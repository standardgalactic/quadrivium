www.ebook3000.com


www.ebook3000.com


www.ebook3000.com


www.ebook3000.com


www.ebook3000.com


Preface to the Second Edition
Without conceding a blemish in the ï¬rst edition, I think I had best come clean
and admit that I embarked on a second edition largely to adopt a more geometric
approach to the detection of signals in white Gaussian noise. Equally rigorous, yet
more intuitive, this approach is not only student-friendly, but also extends more
easily to the detection problem with random parameters and to the radar problem.
The new approach is based on the projection of white Gaussian noise onto a ï¬nite-
dimensional subspace (Section 25.15.2) and on the independence of this projec-
tion and the diï¬€erence between noise and projection; see Theorem 25.15.6 and
Theorem 25.15.7. The latter theorem allows for a simple proof of the suï¬ƒciency
of the matched-ï¬ltersâ€™ outputs without the need to deï¬ne suï¬ƒcient statistics for
continuous-time observables. The key idea is thatâ€”while the receiver cannot re-
cover the observable from its projection onto the subspace spanned by the mean
signalsâ€”it can mimic the performance of any receiver that bases its decision on
the observable using three steps (Figure 26.1 on Page 623): use local randomness
to generate an independent stochastic process whose law is equal to that of the
diï¬€erence between the noise and its projection; add this stochastic process to the
projection; and feed the result to the original receiver.
But the new geometric approach was not the only impetus for a second edition.
I also wanted to increase the bookâ€™s scope. This edition contains new chapters
on the radar problem (Chapter 30), the intersymbol interference (ISI) channel
(Chapter 32), and on the mathematical preliminaries needed for its study (Chap-
ter 31). The treatment of the radar problem is fairly standard with two twists: we
characterize all achievable pairs of false-alarm and missed-detection probabilities
(pFA, pMD) and not just those that are Pareto-optimal. Moreover, we show that
when the observable has a density under both hypotheses, all achievable pairs can
be achieved using deterministic decision rules.
As to ISI channels, I adopted the classic approach of matched ï¬ltering, discrete-
time noise whitening, and running the Viterbi Algorithm. I only allow (bounded-
input/bounded-output) stable whitening ï¬lters, i.e., ï¬lters whose impulse response
is absolutely summable; others often only require that the impulse response be
square summable. While my approach makes it more diï¬ƒcult to prove the exis-
tence of whitening ï¬lters (and I do recommend that the proof be skipped), it is
conceptually much cleaner because the convolution of the noise sequence with the
impulse response exists with probability one. This results in the convolution being
associative, and therefore greatly simpliï¬es the proof that no information is lost in
the whitening process. It is also in line with the bookâ€™s philosophy of obtaining all
xvi
.001
13:55:18, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

Preface to the Second Edition
xvii
results sample-wise.
The chapter on ISI channels also includes Section 32.5, which treats the detec-
tion of QAM signals whereâ€”as in most practical receiversâ€”matched ï¬ltering is
not performed at the carrier frequency but after conversion to baseband (or to
an intermediate frequency). An analysis of the complex stochastic process that
results when a (real) stationary Gaussian passband stochastic process is converted
to baseband can be found in Appendix D.
In addition, some original chapters were expanded. New sections on the opera-
tional power spectral density (in Chapter 15) and a new section on conditionally
independent Gaussians and the zeros of their precision matrix (Section 23.10) are
now included.
Last but not least, I have added over a hundred new exercises. Most reinforce and
test, but some present additional results. Enjoy!
.001
13:55:18, subject to the Cambridge Core terms of use, available at

Preface to the First Edition
Claude Shannon, the father of Information Theory, described the fundamental
problem of point-to-point communications in his classic 1948 paper as â€œthat of
reproducing at one point either exactly or approximately a message selected at
another point.â€
How engineers solve this problem is the subject of this book.
But unlike Shannonâ€™s general problem, where the message can be an image, a
sound clip, or a movie, here we restrict ourselves to bits. We thus envision that
the original message is either a binary sequence to start with, or else that it was
described using bits by a device outside our control and that our job is to reproduce
the describing bits with high reliability. The issue of how images or text ï¬les are
converted eï¬ƒciently into bits is the subject of lossy and lossless data compression
and is addressed in texts on information theory and on quantization.
The engineering solutions to the point-to-point communication problem greatly
depend on the available resources and on the channel between the points. They
typically bring together beautiful techniques from Fourier Analysis, Hilbert Spaces,
Probability Theory, and Decision Theory. The purpose of this book is to introduce
the reader to these techniques and to their interplay.
The book is intended for advanced undergraduates and beginning graduate stu-
dents. The key prerequisites are basic courses in Calculus, Linear Algebra, and
Probability Theory. A course in Linear Systems is a plus but not a must, because
all the results from Linear Systems that are needed for this book are summarized
in Chapters 5 and 6. But more importantly, the book requires a certain mathemat-
ical maturity and patience, because we begin with ï¬rst principles and develop the
theory before discussing its engineering applications. The book is for those who
appreciate the views along the way as much as getting to the destination; who like
to â€œstop and smell the roses;â€ and who prefer fundamentals to acronyms. I ï¬rmly
believe that those with a sound foundation can easily pick up the acronyms and
learn the jargon on the job, but that once one leaves the academic environment,
one rarely has the time or peace of mind to study fundamentals.
In the early stages of the planning of this book I took a decision that greatly
inï¬‚uenced the project. I decided that every key concept should be unambiguously
deï¬ned; that every key result should be stated as a mathematical theorem; and that
every mathematical theorem should be correct. This, I believe, makes for a solid
foundation on which one can build with conï¬dence. But it is also a tall order. It
required that I scrutinize each â€œclassicalâ€ result before I used it in order to be sure
that I knew what the needed qualiï¬ers were, and it forced me to include background
material to which the reader may have already been exposed, because I needed the
xviii
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,
www.ebook3000.com

Preface to the First Edition
xix
results with all the ï¬ne print. Hence Chapters 5 and 6 on Linear Systems and
Fourier Analysis. This is also partly the reason why the book is so long. When I
started out my intention was to write a much shorter book. But I found that to do
justice to the beautiful mathematics on which Digital Communications is based I
had to expand the book.
Most physical-layer communication problems are at their core of a continuous-
time nature. The transmitted physical waveforms are functions of time and not
sequences synchronized to a clock. But most solutions ï¬rst reduce the problem to a
discrete-time setting and then solve the problem in the discrete-time domain. The
reduction to discrete-time often requires great ingenuity, which I try to describe.
It is often taken for granted in courses that open with a discrete-time model from
Lecture 1. I emphasize that most communication problems are of a continuous-
time nature, and that the reduction to discrete-time is not always trivial or even
possible. For example, it is extremely diï¬ƒcult to translate a peak-power constraint
(stating that at no epoch is the magnitude of the transmitted waveform allowed to
exceed a given constant) to a statement about the sequence that is used to represent
the waveform. Similarly, in Wireless Communications it is often very diï¬ƒcult to
reduce the received waveform to a sequence without any loss in performance.
The quest for mathematical precision can be demanding. I have therefore tried to
precede the statement of every key theorem with its gist in plain English. Instruc-
tors may well choose to present the material in class with less rigor and direct the
students to the book for a more mathematical approach. I would rather have text-
books be more mathematical than the lectures than the other way round. Having
a rigorous textbook allows the instructor in class to discuss the intuition knowing
that the students can obtain the technical details from the book at home.
The communication problem comes with a beautiful geometric picture that I try
to emphasize.
To appreciate this picture one needs the deï¬nition of the inner
product between energy-limited signals and some of the geometry of the space of
energy-limited signals. These are therefore introduced early on in Chapters 3 and 4.
Chapters 5 and 6 cover standard material from Linear Systems. But note the early
introduction of the matched ï¬lter as a mechanism for computing inner products
in Section 5.8. Also key is Parsevalâ€™s Theorem in Section 6.2.2 which relates the
geometric pictures in the time domain and in the frequency domain.
Chapter 7 deals with passband signals and their baseband representation. We em-
phasize how the inner product between passband signals is related to the inner
product between their baseband representations. This elegant geometric relation-
ship is often lost in the haze of various trigonometric identities. While this topic is
important in wireless applications, it is not always taught in a ï¬rst course in Digital
Communications. Instructors who prefer to discuss baseband communication only
can skip Chapters 7, 9, 16, 17, 18, 24, 27, and Sections 26.8, 28.5, 30.9, 31.4, 32.4,
32.5. But it would be a shame.
Chapter 8 presents the celebrated Sampling Theorem from a geometric perspective.
It is inessential to the rest of the book but is a striking example of the geometric
approach. Chapter 9 discusses the Sampling Theorem for passband signals.
Chapter 10 discusses modulation.
I have tried to motivate Linear Modulation
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,

xx
Preface to the First Edition
and Pulse Amplitude Modulation and to minimize the use of the â€œthatâ€™s just how
it is doneâ€ argument. The use of the Matched Filter for detecting (here in the
absence of noise) is emphasized. This also motivates the Nyquist Theory, which is
treated in Chapter 11. I stress that the motivation for the Nyquist Theory is not
to avoid intersymbol interference at the sampling points but rather to guarantee
the orthogonality of the time shifts of the pulse shape by integer multiples of the
baud period. This ultimately makes more engineering sense and leads to cleaner
mathematics: compare Theorem 11.3.2 with its corollary, Corollary 11.3.4.
The result of modulating random bits is a stochastic process, a concept which is
ï¬rst encountered in Chapter 10; formally deï¬ned in Chapter 12; and revisited in
Chapters 13, 17, 25, and 31. It is an important concept in Digital Communica-
tions, and I ï¬nd it best to ï¬rst introduce man-made synthesized stochastic processes
(as the waveforms produced by an encoder when fed random bits) and only later
to introduce the nature-made stochastic processes that model noise. Stationary
discrete-time stochastic processes are introduced in Chapter 13 and their complex
counterparts in Chapter 17. These are needed for the analysis in Chapter 14 of the
power in Pulse Amplitude Modulation and for the analysis in Chapter 17 of the
power in Quadrature Amplitude Modulation. They are revisited in Chapter 31,
which presents additional results that are needed in the study of intersymbol in-
terference channels (Chapter 32).
I emphasize that power is a physical quantity that is related to the time-averaged
energy in the continuous-time transmitted power. Its relation to the power in the
discrete-time modulating sequence is a nontrivial result. In deriving this relation
I refrain from adding random timing jitters that are often poorly motivated and
that turn out to be unnecessary. (The transmitted power does not depend on the
realization of the ï¬ctitious jitter.) The Power Spectral Density in Pulse Amplitude
Modulation and Quadrature Amplitude Modulation is discussed in Chapters 15
and 18. The discussion requires a deï¬nition for Power Spectral Density for non-
stationary processes (Deï¬nitions 15.3.1 and 18.4.1) and a proof that this deï¬nition
coincides with the classical deï¬nition when the process is wide-sense stationary
(Theorem 25.14.3).
Chapter 19 opens the second part of the book, which deals with noise and detection.
It introduces the univariate Gaussian distribution and some related distributions.
The principles of Detection Theory are presented in Chapters 20â€“22. I emphasize
the notion of Suï¬ƒcient Statistics, which is central to Detection Theory. Building
on Chapter 19, Chapter 23 introduces the all-important multivariate Gaussian
distribution. Chapter 24 treats the complex case.
Chapter 25 deals with continuous-time stochastic processes with an emphasis on
stationary Gaussian processes, which are often used to model the noise in Digital
Communications. This chapter also introduces white Gaussian noise. My approach
to this topic is perhaps new and is probably where this text diï¬€ers the most from
other textbooks on the subject.
I deï¬ne white Gaussian noise of double-sided power spectral density N0/2
with respect to the bandwidth W as any measurable,1 stationary, Gaussian
1This book does not assume any Measure Theory and does not teach any Measure Theory.
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,
www.ebook3000.com

Preface to the First Edition
xxi
âˆ’W
W
N0/2
f
SNN(f)
Figure 1: The power spectral density of a white Gaussian noise process of double-
sided power spectral density N0/2 with respect to the bandwidth W.
stochastic process whose power spectral density is a nonnegative, symmetric, inte-
grable function of frequency that is equal to N0/2 at all frequencies f satisfying
|f| â‰¤W. The power spectral density at other frequencies can be arbitrary. An
example of the power spectral density of such a process is depicted in Figure 1.
Adopting this deï¬nition has a number of advantages. The ï¬rst is, of course, that
such processes exist. One need not discuss â€œgeneralized processes,â€ Gaussian pro-
cesses with inï¬nite variances (that, by deï¬nition, do not exist), or introduce the
ItË†o calculus to study stochastic integrals. (Stochastic integrals with respect to the
Brownian motion are mathematically intricate and physically unappealing. The
idea of the noise having inï¬nite power is ludicrous.) The above deï¬nition also frees
me from discussing Diracâ€™s Delta, and, in fact, Diracâ€™s Delta is never used in this
book. (A rigorous treatment of Generalized Functions is beyond the engineering
curriculum in most schools, so using Diracâ€™s Delta always gives the reader the
unsettling feeling of being on unsure footing.)
The detection problem in white Gaussian noise is treated in Chapter 26. No course
in Digital Communications should end without Theorem 26.3.1. Roughly speaking,
this theorem states that if the mean-signals are bandlimited to W Hz and if the
noise is white Gaussian noise with respect to the bandwidth W, then there is no
loss of optimality in basing our guess on the projection of the received waveform
onto the subspace spanned by the mean-signals. Numerous examples as well as a
treatment of colored noise are also discussed in this chapter. Extensions to nonco-
herent detection are addressed in Chapter 27 and implications for Pulse Amplitude
Modulation and for Quadrature Amplitude Modulation in Chapter 28.
Coding is introduced in Chapter 29. It emphasizes how the code design inï¬‚uences
the transmitted power, the transmitted power spectral density, the required band-
(I do deï¬ne sets of Lebesgue measure zero in order to be able to state uniqueness theorems.) I
use Measure Theory only in stating theorems that require measurability assumptions. This is
in line with my attempt to state theorems together with all the assumptions that are required
for their validity. I recommend that students ignore measurability issues and just make a mental
note that whenever measurability is mentioned there is a minor technical condition lurking in the
background.
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,

xxii
Preface to the First Edition
width, and the probability of error. The construction of good codes is left to texts
on Coding Theory.
Motivated by the radar problem, Chapter 30 introduces the Neyman-Pearson the-
ory of hypothesis testing as well as the Kullback-Leibler divergence. And after some
mathematical preliminaries in Chapter 31, the book concludes with Chapter 32,
which introduces the intersymbol interference channel and the Viterbi Algorithm.
Basic Latin
Mathematics sometimes reads like a foreign language. I therefore include here a
short glossary for such terms as â€œi.e.,â€ â€œthat is,â€ â€œin particular,â€ â€œa fortiori,â€ â€œfor
example,â€ and â€œe.g.,â€ whose meaning in Mathematics is slightly diï¬€erent from the
deï¬nition you will ï¬nd in your English dictionary. In mathematical contexts these
terms are actually logical statements that the reader should verify. Verifying these
statements is an important way to make sure that you understand the math.
What are these logical statements? First note the synonym â€œi.e.â€ = â€œthat isâ€ and
the synonym â€œe.g.â€ = â€œfor example.â€ Next note that the term â€œthat isâ€ often
indicates that the statement following the term is equivalent to the one preceding
it: â€œWe next show that p is a prime, i.e., that p is a positive integer larger than
one that is not divisible by any positive integer other than one and itself.â€ The
terms â€œin particularâ€ or â€œa fortioriâ€ indicate that the statement following them
is implied by the one preceding them: â€œSince g(Â·) is diï¬€erentiable and, a fortiori,
continuous, it follows from the Mean Value Theorem that the integral of g(Â·) over
the interval [0, 1] is equal to g(Î¾) for some Î¾ âˆˆ[0, 1].â€ The term â€œfor exampleâ€ can
have its regular day-to-day meaning but in mathematical writing it also sometimes
indicates that the statement following it implies the one preceding it: â€œSuppose
that the function g(Â·) is monotonically nondecreasing, e.g., that it is diï¬€erentiable
with a nonnegative derivative.â€
Another important word to look out for is â€œindeed,â€ which in this book typically
signiï¬es that the statement just made is about to be expanded upon and explained.
So when you read something that is unclear to you, be sure to check whether the
next sentence begins with the word â€œindeedâ€ before you panic.
The Latin phrases â€œa prioriâ€ and â€œa posterioriâ€ show up in Probability Theory.
The former is usually associated with the unconditional probability of an event and
the latter with the conditional. Thus, the â€œa prioriâ€ probability that the sun will
shine this Sunday in Zurich is 25%, but now that I know that it is raining today,
my outlook on life changes and I assign this event the a posteriori probability of
15%.
The phrase â€œprima facieâ€ is roughly equivalent to the phrase â€œbefore any further
mathematical arguments have been presented.â€ For example, the deï¬nition of the
projection of a signal v onto the signal u as the vector w that is collinear with u and
for which vâˆ’w is orthogonal to u, may be followed by the sentence: â€œPrima facie,
it is not clear that the projection always exists and that it is unique. Nevertheless,
as we next show, this is the case.â€
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,
www.ebook3000.com

Preface to the First Edition
xxiii
Syllabuses or Syllabi
The book can be used as a textbook for a number of diï¬€erent courses. For a course
that focuses on deterministic signals one could use Chapters 1â€“9 and Chapter 11.
A course that covers Stochastic Processes and Detection Theory could be based
on Chapter 12, Chapters 19â€“26, and Chapter 30 with or without discrete-time
stochastic processes (Chapters 13 and 31) and with or without complex random
variables and processes (Chapters 17 and 24).
For a course on Digital Communications one could use the entire book or, if time
does not permit it, discuss only baseband communication. In the latter case one
could omit Chapters 7, 9, 16, 17, 18, 24, 27, and Sections 26.8, 28.5, 30.9, 31.4,
32.4, 32.5.
The dependencies between the chapters are depicted on Page xxiv. A simpler chart
pertaining only to baseband communication can be found on Page xxv.
The bookâ€™s web page is
www.afidc.ethz.ch
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,

xxiv
Preface to the First Edition
1,2
3
4
5
6
10
11
7
8
12
13
17
14
16
9
15
18
19
23
24
20
25
21
22
26
27
28.
1â€“4
28.
5
30
29
31.
1â€“3
31.
4
32.
1â€“3
32.
4â€“5
A Dependency Diagram.
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,
www.ebook3000.com

Preface to the First Edition
xxv
1,2
3
4
5
6
10
11
8
12
13
14
19
23
15
20
25
21
22
26
28.
1â€“4
30
29
31.
1â€“3
32.
1â€“3
A Dependency Diagram for Baseband Communications.
available at 
.002
13:56:14, subject to the Cambridge Core terms of use,

Chapter 1
Some Essential Notation
Reading a whole chapter about notation can be boring. We have thus chosen to
collect here only the essentials and to introduce the rest when it is ï¬rst used. The
â€œList of Symbolsâ€ on Page 866 is more comprehensive.
We denote the set of complex numbers by C, the set of real numbers by R, the set
of integers by Z, and the set of natural numbers (positive integers) by N. Thus,
N = {n âˆˆZ : n â‰¥1}.
The above equation is not meant to belabor the point. We use it to introduce the
notation
{x âˆˆA : statement}
for the set consisting of all those elements of the set A for which â€œstatementâ€ holds.
In treating real numbers, we use the notation (a, b), [a, b), [a, b], (a, b] to denote
open, half open on the right, closed, and half open on the left intervals of the real
line. Thus, for example,
[a, b) = {x âˆˆR : a â‰¤x < b}.
A statement followed by a comma and a condition indicates that the statement
holds whenever the condition is satisï¬ed. For example,
|an âˆ’a| < Ïµ,
n â‰¥n0
means that |an âˆ’a| < Ïµ whenever n â‰¥n0.
We use I{statement} to denote the indicator of the statement. It is equal to 1, if
the statement is true, and it is equal to 0, if the statement is false. Thus
I{statement} =

1
if statement is true,
0
if statement is false.
In dealing with complex numbers we use i to denote the purely imaginary complex
number whose imaginary part is one
i =
âˆš
âˆ’1.
1
available at 
.003
14:00:46, subject to the Cambridge Core terms of use,
www.ebook3000.com

2
Some Essential Notation
We use zâˆ—to denote the complex conjugate of z, we use Re(z) to denote the real
part of z, we use Im(z) to denote the imaginary part of z, and we use |z| to denote
the absolute value (or â€œmodulusâ€, or â€œcomplex magnitudeâ€) of z. Thus, if z = a+ib,
where a, b âˆˆR, then zâˆ—= a âˆ’ib, Re(z) = a, Im(z) = b, and |z| =
âˆš
a2 + b2.
The notation used to deï¬ne functions is extremely important and is, alas, some-
times confusing to students, so please pay attention. A function or a mapping
associates with each element in its domain a unique element in its codomain. If
a function has a name, the name is often written in bold as in u.1 Alternatively,
we sometimes denote a function u by u(Â·). The notation
u: A â†’B
indicates that u is a function of domain A and codomain B. The rule specifying
for each element of the domain the element in the codomain to which it is mapped
is often written to the right or underneath. Thus, for example,
u: R â†’(âˆ’5, âˆ),
t 	â†’t2
indicates that the domain of the function u is the reals, that its codomain is the
set of real numbers that exceed âˆ’5, and that u associates with t the nonnegative
number t2. We write u(t) for the result of applying the mapping u to t. The range
of a mapping u: A â†’B is the set of all elements of the codomain B to which at
least one element in the domain is mapped by u:
range of

u: A â†’B

=

u(x) : x âˆˆA

.
(1.1)
The range of a mapping is a subset of its codomain. In the above example, the
range of the mapping is the set of nonnegative reals [0, âˆ). A mapping u: A â†’B
is said to be onto (or surjective) if its range is equal to its codomain. Thus,
u: A â†’B is onto if, and only if, for every y âˆˆB there corresponds some x âˆˆA
(not necessarily unique) such that u(x) = y. If the range of g(Â·) is a subset of the
domain of h(Â·), then the composition of g(Â·) and h(Â·) is the mapping x 	â†’h

g(x)

,
which is denoted by h â—¦g. A function u: A â†’B is said to be one-to-one (or
injective) if diï¬€erent elements of A are mapped to diï¬€erent elements of B, i.e., if
x1 Ì¸= x2 implies that u(x1) Ì¸= u(x2) (whenever x1, x2 âˆˆA).
Sometimes we do not specify the domain and codomain of a function if they are
clear from the context.
Thus, we might write u: t 	â†’v(t) cos(2Ï€fct) without
making explicit what the domain and codomain of u are. In fact, if there is no
need to give a function a name, then we will not. For example, we might write t 	â†’
v(t) cos(2Ï€fct) to designate the unnamed function that maps t to v(t) cos(2Ï€fct).
(Here v(Â·) is some other function, which was presumably deï¬ned before.)
If the domain of a function u is R and if the codomain is R, then we sometimes say
that u is a real-valued signal or a real signal, especially if the argument of u
stands for time. Similarly we shall sometimes refer to a function u: R â†’C as a
complex-valued signal or a complex signal. If we refer to u as a signal, then
1But some special functions such as the self-similarity function Rgg, the autocovariance func-
tion KXX, and the power spectral density SXX, which will be introduced in later chapters, are
not in boldface.
available at 
.003
14:00:46, subject to the Cambridge Core terms of use,

Some Essential Notation
3
the question whether it is complex-valued or real-valued should be clear from the
context, or else immaterial to the claim.
We caution the reader that, while u and u(Â·) denote functions, u(t) denotes the
result of applying u to t. If u is a real-valued signal then u(t) is a real number!
Given two signals u and v we deï¬ne their superposition or sum as the signal
t 	â†’u(t) + v(t). We denote this signal by u + v. Also, if Î± âˆˆC and u is any signal,
then we deï¬ne the ampliï¬cation of u by Î± as the signal t 	â†’Î± u(t). We denote
this signal by Î± u. Thus,
Î± u + Î² v
is the signal
t 	â†’Î± u(t) + Î² v(t).
We refer to the function that maps every element in its domain to zero as the all-
zero function and we denote it by 0. The all-zero signal 0 maps every t âˆˆR
to zero. If x: R â†’C is a signal that maps every t âˆˆR to x(t), then its reï¬‚ection
or mirror image is denoted by ~x and is the signal that is deï¬ned by
~x: t 	â†’x(âˆ’t).
Diracâ€™s Delta (which will hardly be mentioned in this book) is not a function.
A probability space is deï¬ned as a triple (Î©, F, P), where the set Î© is the set of
experiment outcomes, the elements of the set F are subsets of Î© and are called
events, and where P : F â†’[0, 1] assigns probabilities to the various events. It is
assumed that F forms a Ïƒ-algebra, i.e., that Î© âˆˆF; that if a set is in F then so
is its complement (with respect to Î©); and that every ï¬nite or countable union of
elements of F is also an element of F. A random variable (RV) X is a mapping
from Î© to R that satisï¬es the technical condition that
{Ï‰ âˆˆÎ© : X(Ï‰) â‰¤Î¾} âˆˆF,
Î¾ âˆˆR.
(1.2)
This condition guarantees that it is always meaningful to evaluate the probability
that the value of X is smaller or equal to Î¾.
available at 
.003
14:00:46, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 2
Signals, Integrals, and Sets of Measure Zero
2.1
Introduction
The purpose of this chapter is not to develop the Lebesgue theory of integration.
Mastering this theory is not essential to understanding Digital Communications.
But some concepts from this theory are needed in order to state the main results
of Digital Communications in a mathematically rigorous way.
In this chapter
we introduce these required concepts and provide references to the mathematical
literature that develops them.
The less mathematically-inclined may gloss over most of this chapter. Readers
who interpret the integrals in this book as Riemann integrals; who interpret â€œmea-
surableâ€ as â€œsatisfying a minor mathematical restrictionâ€; who interpret â€œa set of
Lebesgue measure zeroâ€ as â€œa set that is so small that integrals of functions are
insensitive to the values the integrand takes in this setâ€; and who swap orders of
summations, expectations and integrations fearlessly will not miss any engineering
insights.
But all readers should pay attention to the way the integral of complex-valued
signals is deï¬ned (Section 2.3); to the basic inequality (2.13); and to the notation
introduced in (2.6).
2.2
Integrals
Recall that a real-valued signal u is a function u: R â†’R. The integral of u is
denoted by
 âˆ
âˆ’âˆ
u(t) dt.
(2.1)
For (2.1) to be meaningful some technical conditions must be met. (You may re-
call from your calculus studies, for example, that not every function is Riemann
integrable.) In this book all integrals will be understood to be Lebesgue integrals,
but nothing essential will be lost on readers who interpret them as Riemann inte-
grals. For the Lebesgue integral to be deï¬ned the integrand u must be a Lebesgue
measurable function. Again, do not worry if you have not studied the Lebesgue
integral or the notion of measurable functions. We point this out merely to cover
ourselves when we state various theorems. Also, for the integral in (2.1) to be
4
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,

2.3 Integrating Complex-Valued Signals
5
deï¬ned we insist that
 âˆ
âˆ’âˆ
|u(t)| dt < âˆ.
(2.2)
(There are ways of deï¬ning the integral in (2.1) also when (2.2) is violated, but
they lead to fragile expressions that are diï¬ƒcult to manipulate.)
A function u: R â†’R which is Lebesgue measurable and which satisï¬es (2.2) is
said to be integrable, and we denote the set of all such functions by L1. We shall
refrain from integrating functions that are not elements of L1.
2.3
Integrating Complex-Valued Signals
This section should assuage your fear of integrating complex-valued signals. (Some
of you may have a trauma from your Complex Analysis courses where you dealt
with integrals of functions from the complex plane to the complex plane. Here
things are much simpler because we are dealing only with integrals of functions
from the real line to the complex plane.)
We formally deï¬ne the integral of a
complex-valued function u: R â†’C by
 âˆ
âˆ’âˆ
u(t) dt â‰œ
 âˆ
âˆ’âˆ
Re

u(t)

dt + i
 âˆ
âˆ’âˆ
Im

u(t)

dt.
(2.3)
For this to be meaningful, we require that the real functions t 	â†’Re

u(t)

and
t 	â†’Im

u(t)

both be integrable real functions.
That is, they should both be
Lebesgue measurable and we should have
 âˆ
âˆ’âˆ
Re

u(t)
 dt < âˆ
and
 âˆ
âˆ’âˆ
Im

u(t)
 dt < âˆ.
(2.4)
It is not diï¬ƒcult to show that (2.4) is equivalent to the more compact condition
 âˆ
âˆ’âˆ
u(t)
 dt < âˆ.
(2.5)
We say that a complex signal u: R â†’C is Lebesgue measurable if the mappings
t 	â†’Re

u(t)

and t 	â†’Im

u(t)

are Lebesgue measurable real signals. We say that
a function u: R â†’C is integrable if it is Lebesgue measurable and (2.4) holds.
The set of all Lebesgue measurable integrable complex signals is denoted by L1.
Note that we use the same symbol L1 to denote both the set of integrable real
signals and the set of integrable complex signals. To which of these two sets we
refer should be clear from the context, or else immaterial.
For u âˆˆL1 we deï¬ne âˆ¥uâˆ¥1 as
âˆ¥uâˆ¥1 â‰œ
 âˆ
âˆ’âˆ
u(t)
 dt.
(2.6)
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,
www.ebook3000.com

6
Signals, Integrals, and Sets of Measure Zero
Before summarizing the key properties of the integral of complex signals we remind
the reader that if u and v are complex signals and if Î±, Î² are complex numbers, then
the complex signal Î± u+Î² v is deï¬ned as the complex signal t 	â†’Î± u(t)+Î² v(t). The
intuition for the following proposition comes from thinking about the integrals as
Riemann integrals, which can be approximated by ï¬nite sums and by then invoking
the analogous results about ï¬nite sums.
Proposition 2.3.1 (Properties of Complex Integrals). Let the complex signals u, v
be in L1, and let Î±, Î² be arbitrary complex numbers.
(i) Integration is linear in the sense that Î± u + Î² v âˆˆL1 and
 âˆ
âˆ’âˆ

Î± u(t) + Î² v(t)

dt = Î±
 âˆ
âˆ’âˆ
u(t) dt + Î²
 âˆ
âˆ’âˆ
v(t) dt.
(2.7)
(ii) Integration commutes with complex conjugation
 âˆ
âˆ’âˆ
uâˆ—(t) dt =
	 âˆ
âˆ’âˆ
u(t) dt

âˆ—
.
(2.8)
(iii) Integration commutes with the operation of taking the real part
Re
	 âˆ
âˆ’âˆ
u(t) dt

=
 âˆ
âˆ’âˆ
Re

u(t)

dt.
(2.9)
(iv) Integration commutes with the operation of taking the imaginary part
Im
	 âˆ
âˆ’âˆ
u(t) dt

=
 âˆ
âˆ’âˆ
Im

u(t)

dt.
(2.10)
Proof. For a proof of (i) see, for example, (Rudin, 1987, Theorem 1.32). The rest
of the claims follow easily from the deï¬nition of the integral of a complex-valued
signal (2.3).
2.4
An Inequality for Integrals
Probably the most important inequality for complex numbers is the Triangle
Inequality for Complex Numbers
|w + z| â‰¤|w| + |z|,
w, z âˆˆC.
(2.11)
This inequality extends by induction to ï¬nite sums:

n

j=1
zj
 â‰¤
n

j=1
|zj| ,
z1, . . . , zn âˆˆC.
(2.12)
The extension to integrals is the most important inequality for integrals:
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,

2.5 Sets of Lebesgue Measure Zero
7
Proposition 2.4.1. For every complex-valued or real-valued signal u in L1

 âˆ
âˆ’âˆ
u(t) dt
 â‰¤
 âˆ
âˆ’âˆ
u(t)
 dt.
(2.13)
Proof. See, for example, (Rudin, 1987, Theorem 1.33).
Note that in (2.13) we should interpret | Â· | as the absolute-value function if u is a
real signal, and as the modulus function if u is a complex signal.
Another simple but useful inequality is
âˆ¥u + vâˆ¥1 â‰¤âˆ¥uâˆ¥1 + âˆ¥vâˆ¥1 ,
u, v âˆˆL1,
(2.14)
which can be proved using the calculation
âˆ¥u + vâˆ¥1 =
 âˆ
âˆ’âˆ
|u(t) + v(t)| dt
â‰¤
 âˆ
âˆ’âˆ

|u(t)| + |v(t)|

dt
=
 âˆ
âˆ’âˆ
|u(t)| dt +
 âˆ
âˆ’âˆ
|v(t)| dt
= âˆ¥uâˆ¥1 + âˆ¥vâˆ¥1 ,
where the inequality follows by applying the Triangle Inequality for Complex Num-
bers (2.11) with the substitution of u(t) for w and v(t) for z.
2.5
Sets of Lebesgue Measure Zero
It is one of lifeâ€™s minor grievances that the integral of a nonnegative function can
be zero even if the function is not identically zero. For example, t 	â†’I{t = 17} is a
nonnegative function whose integral is zero and which is nonetheless not identically
zero (it maps 17 to one). In this section we shall derive a necessary and suï¬ƒcient
condition for the integral of a nonzero function to be zero. This condition will
allow us later to state conditions under which various integral inequalities hold
with equality. It will give mathematical meaning to the physical intuition that if
the waveform describing some physical phenomenon (such as voltage over a resistor)
is nonnegative and integrates to zero then â€œfor all practical purposesâ€ the waveform
is zero.
We shall deï¬ne sets of Lebesgue measure zero and then show that a nonnegative
function u: R â†’[0, âˆ) integrates to zero if, and only if, the set {t âˆˆR : u(t) > 0} is
of Lebesgue measure zero. Thus, whether or not a nonnegative function integrates
to zero depends on the set over which it takes on positive values and not on the
actual values. We shall then introduce the notation u â‰¡v to indicate that the set
{t âˆˆR : u(t) Ì¸= v(t)} is of Lebesgue measure zero.
It should be noted that since the integral is unaltered when the integrand is changed
at a ï¬nite (or countable) number of points, it follows that any nonnegative function
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,
www.ebook3000.com

8
Signals, Integrals, and Sets of Measure Zero
that is zero except at a countable number of points integrates to zero. The reverse,
however, is not true. One can ï¬nd nonnegative functions that integrate to zero
and that are nonzero on an uncountable set of points.
The less mathematically inclined readers may skip the mathematical deï¬nition of
sets of measure zero and just think of a subset of the real line as being of Lebesgue
measure zero if it is so â€œsmallâ€ that the integral of any function is unaltered when
the values it takes in the subset are altered. Such readers should then think of the
statement u â‰¡v as indicating that u âˆ’v is just the result of altering the all-zero
signal 0 on a set of Lebesgue measure zero and that, consequently,
 âˆ
âˆ’âˆ
|u(t) âˆ’v(t)| dt = 0.
Deï¬nition 2.5.1 (Sets of Lebesgue Measure Zero). We say that a subset N of
the real line R is a set of Lebesgue measure zero (or a Lebesgue null set)
if for every Ïµ > 0 we can ï¬nd a sequence of intervals [a1, b1], [a2, b2], . . . such that
the total length of the intervals is smaller than or equal to Ïµ
âˆ

j=1
(bj âˆ’aj) â‰¤Ïµ
(2.15a)
and such that the union of the intervals covers N
N âŠ†[a1, b1] âˆª[a2, b2] âˆªÂ· Â· Â· .
(2.15b)
As an example, note that the set {1} is of Lebesgue measure zero. Indeed, it is
covered by the single interval [1 âˆ’Ïµ/2, 1 + Ïµ/2] whose length is Ïµ. Similarly, any
ï¬nite set is of Lebesgue measure zero. Indeed, the set {Î±1, . . . , Î±n} can be covered
by n intervals of total length not exceeding Ïµ as follows:
{Î±1, . . . , Î±n} âŠ‚

Î±1 âˆ’Ïµ/(2n), Î±1 + Ïµ/(2n)

âˆªÂ· Â· Â· âˆª

Î±n âˆ’Ïµ/(2n), Î±n + Ïµ/(2n)

.
This argument can be also extended to show that any countable set is of Lebesgue
measure zero. Indeed the countable set {Î±1, Î±2, . . .} can be covered as
{Î±1, Î±2, . . .} âŠ†
âˆ

j=1

Î±j âˆ’2âˆ’jâˆ’1Ïµ, Î±j + 2âˆ’jâˆ’1Ïµ

where we note that the length of the interval

Î±j âˆ’2âˆ’jâˆ’1Ïµ, Î±j + 2âˆ’jâˆ’1Ïµ

is 2âˆ’jÏµ,
which when summed over j yields Ïµ.
With a similar argument one can show that the union of a countable number of
sets of Lebesgue measure zero is of Lebesgue measure zero.
The above examples notwithstanding, it should be emphasized that there exist sets
of Lebesgue measure zero that are not countable.1 Thus, the concept of a set of
Lebesgue measure zero is diï¬€erent from the concept of a countable set.
1For example, the Cantor set is of Lebesgue measure zero and uncountable; see (Rudin, 1976,
Section 11.11, Remark (f), p. 309).
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,

2.5 Sets of Lebesgue Measure Zero
9
Loosely speaking, we say that two signals are indistinguishable if they agree except
possibly on a set of Lebesgue measure zero. We warn the reader, however, that
this terminology is not standard.
Deï¬nition 2.5.2 (Indistinguishable Functions). We say that the Lebesgue measur-
able functions u, v from R to C (or to R) are indistinguishable and write
u â‰¡v
if the set {t âˆˆR : u(t) Ì¸= v(t)} is of Lebesgue measure zero.
Note that u â‰¡v if, and only if, the signal u âˆ’v is indistinguishable from the
all-zero signal 0

u â‰¡v

â‡â‡’

u âˆ’v â‰¡0

.
(2.16)
The main result of this section is the following:
Proposition 2.5.3.
(i) A nonnegative Lebesgue measurable signal integrates to zero if, and only if,
it is indistinguishable from the all-zero signal 0.
(ii) If u, v are Lebesgue measurable functions from R to C (or to R), then
  âˆ
âˆ’âˆ
|u(t) âˆ’v(t)| dt = 0

â‡â‡’

u â‰¡v

(2.17)
and
  âˆ
âˆ’âˆ
|u(t) âˆ’v(t)|2 dt = 0

â‡â‡’

u â‰¡v

.
(2.18)
(iii) If u and v are integrable and indistinguishable, then their integrals are equal:

u â‰¡v

=â‡’
  âˆ
âˆ’âˆ
u(t) dt =
 âˆ
âˆ’âˆ
v(t) dt

,
u, v âˆˆL1.
(2.19)
Proof. The proof of (i) is not very diï¬ƒcult, but it requires more familiarity with
Measure Theory than we are willing to assume.
The interested reader is thus
referred to (Rudin, 1987, Theorem 1.39).
The equivalence in (2.17) follows by applying Part (i) to the nonnegative function
t 	â†’|u(t) âˆ’v(t)|. Similarly, (2.18) follows by applying Part (i) to the nonnegative
function t 	â†’|u(t)âˆ’v(t)|2 and by noting that the set of tâ€™s for which |u(t)âˆ’v(t)|2 Ì¸= 0
is the same as the set of tâ€™s for which u(t) Ì¸= v(t).
Part (iii) follows from (2.17) by noting that

 âˆ
âˆ’âˆ
u(t) dt âˆ’
 âˆ
âˆ’âˆ
v(t) dt
 =

 âˆ
âˆ’âˆ

u(t) âˆ’v(t)

dt

â‰¤
 âˆ
âˆ’âˆ
u(t) âˆ’v(t)
 dt,
where the ï¬rst equality follows by the linearity of integration, and where the sub-
sequent inequality follows from Proposition 2.4.1.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,
www.ebook3000.com

10
Signals, Integrals, and Sets of Measure Zero
2.6
Swapping Integration, Summation, and Expectation
In numerous places in this text we shall swap the order of integration as in
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
u(Î±, Î²) dÎ±

dÎ² =
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
u(Î±, Î²) dÎ²

dÎ±
(2.20)
or the order of summation as in
âˆ

Î½=1
	 âˆ

Î·=1
aÎ½,Î·

=
âˆ

Î·=1
	 âˆ

Î½=1
aÎ½,Î·

(2.21)
or the order of summation and integration as in
 âˆ
âˆ’âˆ
	 âˆ

Î½=1
aÎ½ uÎ½(t)

dt =
âˆ

Î½=1
	
aÎ½
 âˆ
âˆ’âˆ
uÎ½(t) dt

(2.22)
or the order of integration and expectation as in
E
 âˆ
âˆ’âˆ
X u(t) dt

=
 âˆ
âˆ’âˆ
E[X u(t)] dt = E[X]
 âˆ
âˆ’âˆ
u(t) dt.
These changes of order are usually justiï¬ed using Fubiniâ€™s Theorem, which states
that these changes of order are permissible provided that a very technical measura-
bility condition is satisï¬ed and that, in addition, either the integrand is nonnegative
or that in some order (and hence in all orders) the integrals/summation/expectation
of the absolute value of the integrand is ï¬nite.
For example, to justify (2.20) it suï¬ƒces to verify that the function u: R2 â†’R in
(2.20) is Lebesgue measurable and that, in addition, it is either nonnegative or
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
|u(Î±, Î²)| dÎ±

dÎ² < âˆ
or
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
|u(Î±, Î²)| dÎ²

dÎ± < âˆ.
Similarly, to justify (2.21) it suï¬ƒces to show that aÎ½,Î· â‰¥0 or that
âˆ

Î·=1
	 âˆ

Î½=1
|aÎ½,Î·|

< âˆ
or that
âˆ

Î½=1
	 âˆ

Î·=1
|aÎ½,Î·|

< âˆ.
(No need to worry about measurability which is automatic in this setup.)
As a ï¬nal example, to justify (2.22) it suï¬ƒces that the functions {uÎ½} are all
measurable and that either aÎ½ uÎ½(t) is nonnegative for all Î½ âˆˆN and t âˆˆR or
 âˆ
âˆ’âˆ
	 âˆ

Î½=1
|aÎ½| |uÎ½(t)|

dt < âˆ
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,

2.7 Additional Reading
11
or
âˆ

Î½=1
|aÎ½|
	 âˆ
âˆ’âˆ
|uÎ½(t)| dt

< âˆ.
A precise statement of Fubiniâ€™s Theorem requires some Measure Theory that is
beyond the scope of this book. The reader is referred to (Rudin, 1987, Theorem
8.8) and (Billingsley, 1995, Chapter 3, Section 18) for such a statement and for a
proof.
We shall frequently use the swapping-of-order argument to manipulate the square
of a sum or the square of an integral.
Proposition 2.6.1.
(i) If 
Î½ |aÎ½| < âˆthen
	 âˆ

Î½=1
aÎ½

2
=
âˆ

Î½=1
âˆ

Î½â€²=1
aÎ½aÎ½â€².
(2.23)
(ii) If u is an integrable real-valued or complex-valued signal, then
	 âˆ
âˆ’âˆ
u(Î±) dÎ±

2
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
u(Î±) u(Î±â€²) dÎ± dÎ±â€².
(2.24)
Proof. The proof is a direct application of Fubiniâ€™s Theorem. But ignoring the
technicalities, the intuition is quite clear: it all boils down to the fact that (a + b)2
can be written as (a+b)(a+b), which can in turn be written as aa+ab+ba+bb.
2.7
Additional Reading
Numerous books cover the basics of Lebesgue integration. Classic examples are
(Riesz and Sz.-Nagy, 1990), (Rudin, 1987), and (Royden and Fitzpatrick, 2010).
These texts also cover the notion of sets of Lebesgue measure zero, e.g., (Riesz
and Sz.-Nagy, 1990, Chapter 1, Section 2). For the changing of order of Riemann
integration see (KÂ¨orner, 1988, Chapters 47 & 48).
2.8
Exercises
Exercise 2.1 (Scaling the Integrand). Starting from the deï¬nition of the integral of a
complex signal (2.3), prove that if u is integrable and a is a complex number, then
 âˆ
âˆ’âˆ
a u(t) dt = a
 âˆ
âˆ’âˆ
u(t) dt.
Exercise 2.2 (A Useful Change-of-Variable). Let x be an integrable signal, and let ~x be
its mirror image. Prove that
 âˆ
âˆ’âˆ
~x(t) dt =
 âˆ
âˆ’âˆ
x(t) dt.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,
www.ebook3000.com

12
Signals, Integrals, and Sets of Measure Zero
Exercise 2.3 (Integrating an Exponential). Show that
 âˆ
0
eâˆ’zt dt = 1
z ,
Re(z) > 0.
Exercise 2.4 (The Sinc Is not Integrable). Show that the mapping t â†’sin(Ï€t)/(Ï€t) is
not integrable irrespective of how we deï¬ne it at zero.
Hint: Lower-bound the integral of the absolute value of this function from n to n + 1 by
the integral from n + 1/4 to n + 3/4. Lower-bound the integrand in this region, and use
the fact that the harmonic sum 
kâ‰¥1 1/k diverges.
Exercise 2.5 (Triangle Inequality for Complex Numbers). Prove the Triangle Inequality
for complex numbers (2.11). Under what conditions does it hold with equality?
Exercise 2.6 (When Are Complex Numbers Equal?). Prove that if the complex numbers
w and z are such that Re(Î²z) = Re(Î²w) for all Î² âˆˆC, then w = z.
Exercise 2.7 (Bounding Complex Exponentials). Show that
eiÎ¸ âˆ’1
 â‰¤|Î¸|,
Î¸ âˆˆR.
Exercise 2.8 (An Integral Inequality). Show that if u, v, and w are integrable signals,
then
 âˆ
âˆ’âˆ
u(t) âˆ’w(t)
 dt â‰¤
 âˆ
âˆ’âˆ
u(t) âˆ’v(t)
 dt +
 âˆ
âˆ’âˆ
v(t) âˆ’w(t)
 dt.
Exercise 2.9 (An Integral to Note). Given some f âˆˆR, compute the integral
 âˆ
âˆ’âˆ
I{t = 17} eâˆ’i2Ï€ft dt.
Exercise 2.10 (Subsets of Sets of Lebesgue Measure Zero). Show that a subset of a set
of Lebesgue measure zero must also be of Lebesgue measure zero.
Exercise 2.11 (The Union of Sets of Lebesgue Measure Zero). Show that the union of
two sets that are each of Lebesgue measure zero is also of Lebesgue measure zero.
Exercise 2.12 (Nonuniqueness of the Probability Density Function). We say that the
random variable X is of density fX(Â·) if fX(Â·) is a (Lebesgue measurable) nonnegative
function such that
Pr[X â‰¤x] =
 x
âˆ’âˆ
fX(Î¾) dÎ¾,
x âˆˆR.
Show that if X is of density fX(Â·) and if g(Â·) is a nonnegative function that is indistin-
guishable from fX(Â·), then X is also of density g(Â·). (The reverse is also true: if X is of
density g1(Â·) and also of density g2(Â·), then g1(Â·) and g2(Â·) must be indistinguishable.)
Exercise 2.13 (Indistinguishability). Let Ïˆ : R2 â†’R satisfy Ïˆ(Î±, Î²) â‰¥0, for all Î±, Î² âˆˆR
with equality only if Î± = Î². Let u and v be Lebesgue measurable signals. Show that
 âˆ
âˆ’âˆ
Ïˆ

u(t), v(t)

dt = 0

=â‡’
	
v â‰¡u

.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,

2.8 Exercises
13
Exercise 2.14 (Indistinguishable Signals). Show that if the Lebesgue measurable signals g
and h are indistinguishable, then the set of epochs t âˆˆR where the sums âˆ
j=âˆ’âˆg(t + j)
and âˆ
j=âˆ’âˆh(t + j) are diï¬€erent (in the sense that they both converge but to diï¬€erent
limits or that one converges but the other does not) is of Lebesgue measure zero.
Exercise 2.15 (Continuous Nonnegative Functions). A subset of R containing a nonempty
open interval cannot be of Lebesgue measure zero. Use this fact to show that if a con-
tinuous function g: R â†’R is nonnegative except perhaps on a set of Lebesgue measure
zero, then the exception set is empty and the function is nonnegative.
Exercise 2.16 (Order of Summation Sometimes Matters). For every Î½, Î· âˆˆN deï¬ne
aÎ½,Î· =
â§
âª
â¨
âª
â©
2 âˆ’2âˆ’Î½
if Î½ = Î·
âˆ’2 + 2âˆ’Î½
if Î½ = Î· + 1
0
otherwise.
Show that (2.21) is not satisï¬ed. See (Royden and Fitzpatrick, 2010, Section 20.1, Exer-
cise 5).
Exercise 2.17 (Using Fubiniâ€™s Theorem). Using the relation
1
x =
 âˆ
0
eâˆ’xt dt,
x > 0
and Fubiniâ€™s Theorem, show that
lim
Î±â†’âˆ
 Î±
0
sin x
x
dx = Ï€
2 .
See (Rudin, 1987, Chapter 8, Exercise 12).
Hint: See also Problem 2.3.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.004
Downloaded from https:/www.cambridge.org/core. Columbia University Libraries, on 18 Jun 2017 at 08:54:51, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 3
The Inner Product
3.1
The Inner Product
The inner product is central to Digital Communications, so it is best to introduce
it early. The motivation will have to wait.
Recall that u: A â†’B indicates that u (sometimes denoted u(Â·)) is a function
(or mapping) that maps each element in its domain A to an element in its
codomain B.
If both the domain and the codomain of u are the set of real
numbers R, then we sometimes refer to u as being a real signal, especially if the
argument of u(Â·) stands for time. Similarly, if u: R â†’C where C denotes the set
of complex numbers and the argument of u(Â·) stands for time, then we sometimes
refer to u as a complex signal.
The inner product between two real functions u: R â†’R and v: R â†’R is
denoted âŸ¨u, vâŸ©and is deï¬ned as
âŸ¨u, vâŸ©â‰œ
 âˆ
âˆ’âˆ
u(t) v(t) dt,
(3.1)
whenever the integral is deï¬ned.1(In Section 3.2 we shall study conditions under
which the integral is deï¬ned, i.e., conditions on the functions u and v that guar-
antee that the product function t 	â†’u(t) v(t) is an integrable function.)
The signals that arise in our study of Digital Communications often represent
electric ï¬elds or voltages over resistors. The energy required to generate them is
thus proportional to the integral of their squared magnitude. This motivates us to
deï¬ne the energy of a Lebesgue measurable real-valued function u: R â†’R as
 âˆ
âˆ’âˆ
u2(t) dt.
(If this integral is not ï¬nite, then we say that u is of inï¬nite energy.) We say that
u: R â†’R is of ï¬nite energy if it is Lebesgue measurable and if
 âˆ
âˆ’âˆ
u2(t) dt < âˆ.
1We use the term inner product much more freely than some mathematical texts that reserve
this term for inner product spaces.
14
.005
14:02:50, subject to the Cambridge Core terms of use, available at

3.1 The Inner Product
15
The class of all ï¬nite-energy real-valued functions u: R â†’R is denoted by L2.
Since the energy of u: R â†’R is nonnegative, we can discuss its nonnegative square
root, which we denote2 by âˆ¥uâˆ¥2:
âˆ¥uâˆ¥2 â‰œ
 âˆ
âˆ’âˆ
u2(t) dt.
(3.2)
(Throughout this book we denote by âˆšÎ¾ the nonnegative square root of Î¾ for every
Î¾ â‰¥0.) We can now express the energy in u using the inner product as
âˆ¥uâˆ¥2
2 =
 âˆ
âˆ’âˆ
u2(t) dt
= âŸ¨u, uâŸ©.
(3.3)
In writing âˆ¥uâˆ¥2
2 above we used diï¬€erent fonts for the subscript and the superscript.
The subscript is just a graphical character which is part of the notation âˆ¥Â·âˆ¥2. We
could have replaced it with â™¦and designated the energy by âˆ¥uâˆ¥2
â™¦without any
change in mathematical meaning.3 The superscript, however, indicates that the
quantity âˆ¥uâˆ¥2 is being squared.
For complex-valued functions u: R â†’C and v: R â†’C we deï¬ne the inner product
âŸ¨u, vâŸ©as
âŸ¨u, vâŸ©â‰œ
 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt,
(3.4)
whenever the integral is deï¬ned. Here vâˆ—(t) denotes the complex conjugate of v(t).
The above integral in (3.4) is a complex integral, but that should not worry you:
it can also be written as
âŸ¨u, vâŸ©=
 âˆ
âˆ’âˆ
Re

u(t) vâˆ—(t)

dt + i
 âˆ
âˆ’âˆ
Im

u(t) vâˆ—(t)

dt,
(3.5)
where i = âˆšâˆ’1 and where Re(Â·) and Im(Â·) denote the functions that map a complex
number to its real and imaginary parts: Re(a+ib) = a and Im(a+ib) = b whenever
a, b âˆˆR. Each of the two integrals appearing in (3.5) is the integral of a real signal.
See Section 2.3.
Note that (3.1) and (3.4) are in agreement in the sense that if u and v happen
to take on only real values (i.e., satisfy that u(t), v(t) âˆˆR for every t âˆˆR), then
viewing them as real functions and thus using (3.1) would yield the same inner
product as viewing them as (degenerate) complex functions and using (3.4). Note
also that for complex functions u, v: R â†’C the inner product âŸ¨u, vâŸ©is in general
not the same as âŸ¨v, uâŸ©. One is the complex conjugate of the other.
2The subscript 2 is here to distinguish âˆ¥uâˆ¥2 from âˆ¥uâˆ¥1 , where the latter was deï¬ned in (2.6)
as âˆ¥uâˆ¥1 =
 âˆ
âˆ’âˆ|u(t)| dt.
3We prefer âˆ¥Â·âˆ¥2 to âˆ¥Â·âˆ¥â™¦because it reminds us that in the deï¬nition (3.2) the integrand is
raised to the second power. This should be contrasted with the symbol âˆ¥Â·âˆ¥1 where the magnitude
of the integrand is raised to the ï¬rst power (and where no square root is taken of the result); see
(2.6).
.005
14:02:50, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

16
The Inner Product
Some of the properties of the inner product between complex-valued functions
u, v: R â†’C are given below.
âŸ¨u, vâŸ©= âŸ¨v, uâŸ©âˆ—
(3.6)
âŸ¨Î± u, vâŸ©= Î± âŸ¨u, vâŸ©,
Î± âˆˆC
(3.7)
âŸ¨u, Î± vâŸ©= Î±âˆ—âŸ¨u, vâŸ©,
Î± âˆˆC
(3.8)
âŸ¨u1 + u2, vâŸ©= âŸ¨u1, vâŸ©+ âŸ¨u2, vâŸ©
(3.9)
âŸ¨u, v1 + v2âŸ©= âŸ¨u, v1âŸ©+ âŸ¨u, v2âŸ©.
(3.10)
The above equalities hold whenever the inner products appearing on the right-
hand side (RHS) are deï¬ned. The reader is encouraged to produce a similar list of
properties for the inner product between real-valued functions u, v: R â†’R.
The energy in a Lebesgue measurable complex-valued function u: R â†’C is de-
ï¬ned as
 âˆ
âˆ’âˆ
u(t)
2 dt,
where |Â·| denotes absolute value so |a + ib| =
âˆš
a2 + b2 whenever a, b âˆˆR. This
deï¬nition of energy might seem a bit contrived because there is no such thing
as complex voltage, so prima facie it seems meaningless to deï¬ne the energy of
a complex signal. But this is not the case. Complex signals are used to repre-
sent real passband signals, and the representation is such that the energy in the
real passband signal is proportional to the integral of the squared modulus of the
complex-valued signal representing it; see Section 7.6 ahead.
Deï¬nition 3.1.1 (Energy-Limited Signal). We say that u: R â†’C is energy-
limited or of ï¬nite energy if u is Lebesgue measurable and
 âˆ
âˆ’âˆ
u(t)
2 dt < âˆ.
The set of all energy-limited complex-valued functions u: R â†’C is denoted L2.
Note that whether L2 stands for the class of energy-limited complex-valued or
real-valued functions should be clear from the context, or else immaterial.
For every u âˆˆL2 we deï¬ne âˆ¥uâˆ¥2 as the nonnegative square root of its energy
âˆ¥uâˆ¥2 â‰œ

âŸ¨u, uâŸ©,
(3.11)
so
âˆ¥uâˆ¥2 =
 âˆ
âˆ’âˆ
|u(t)|2 dt.
(3.12)
Again (3.12) and (3.2) are in agreement in the sense that for every u: R â†’R,
computing âˆ¥uâˆ¥2 via (3.2) yields the same result as if we viewed u as mapping
from R to C and computed âˆ¥uâˆ¥2 via (3.12).
.005
14:02:50, subject to the Cambridge Core terms of use, available at

3.2 When Is the Inner Product Deï¬ned?
17
3.2
When Is the Inner Product Deï¬ned?
As noted in Section 2.2, in this book we shall only discuss the integral of integrable
functions, where a function u: R â†’R is integrable if it is Lebesgue measurable
and if
 âˆ
âˆ’âˆ|u(t)| dt < âˆ. (We shall sometimes make an exception for functions
that take on only nonnegative values. If u: R â†’[0, âˆ) is Lebesgue measurable
and if

u(t) dt is not ï¬nite, then we shall say that

u(t) dt = +âˆ.)
Similarly, as in Section 2.3, in integrating complex signals u: R â†’C we limit
ourselves to signals that are integrable in the sense that both t 	â†’Re

u(t)

and
t 	â†’Im

u(t)

are Lebesgue measurable real-valued signals and
 âˆ
âˆ’âˆ|u(t)| dt < âˆ.
Consequently, we shall say that the inner product between u: R â†’C and v: R â†’C
is well-deï¬ned only when they are both Lebesgue measurable (thus implying that
t 	â†’u(t) vâˆ—(t) is Lebesgue measurable) and when
 âˆ
âˆ’âˆ
u(t) v(t)
 dt < âˆ.
(3.13)
We next discuss conditions on the Lebesgue measurable complex signals u and v
that guarantee that (3.13) holds. The simplest case is when one of the functions,
say u, is bounded and the other, say v, is integrable. Indeed, if ÏƒâˆâˆˆR is such
that |u(t)| â‰¤Ïƒâˆfor all t âˆˆR, then |u(t) v(t)| â‰¤Ïƒâˆ|v(t)| and
 âˆ
âˆ’âˆ
u(t) v(t)
 dt â‰¤Ïƒâˆ
 âˆ
âˆ’âˆ
v(t)
 dt = Ïƒâˆâˆ¥vâˆ¥1 ,
where the RHS is ï¬nite by our assumption that v is integrable.
Another case where the inner product is well-deï¬ned is when both u and v are of
ï¬nite energy. To prove that in this case too the mapping t 	â†’u(t) v(t) is integrable
we need the inequality
Î± Î² â‰¤1
2(Î±2 + Î²2),
Î±, Î² âˆˆR,
(3.14)
which follows directly from the inequality (Î± âˆ’Î²)2 â‰¥0 by simple algebra:
0 â‰¤(Î± âˆ’Î²)2
= Î±2 + Î²2 âˆ’2Î±Î².
By substituting |u(t)| for Î± and |v(t)| for Î² in (3.14) we obtain the inequality
|u(t) v(t)| â‰¤(|u(t)|2 + |v(t)|2)/2 and hence
 âˆ
âˆ’âˆ
u(t) v(t)
 dt â‰¤1
2
 âˆ
âˆ’âˆ
u(t)
2 dt + 1
2
 âˆ
âˆ’âˆ
v(t)
2 dt,
(3.15)
thus demonstrating that if both u and v are of ï¬nite energy (so the RHS is ï¬nite),
then the inner product is well-deï¬ned, i.e., t 	â†’u(t) v(t) is integrable.
As a by-product of this proof we can obtain an upper bound on the magnitude of
the inner product in terms of the energies of u and v. All we need is the inequality

 âˆ
âˆ’âˆ
f(Î¾) dÎ¾
 â‰¤
 âˆ
âˆ’âˆ
f(Î¾)
 dÎ¾
.005
14:02:50, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

18
The Inner Product
(see Proposition 2.4.1) to conclude from (3.15) that
|âŸ¨u, vâŸ©| =

 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt

â‰¤
 âˆ
âˆ’âˆ
u(t)
 v(t)
 dt
â‰¤1
2
 âˆ
âˆ’âˆ
u(t)
2 dt + 1
2
 âˆ
âˆ’âˆ
v(t)
2 dt
= 1
2

âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2

.
(3.16)
This inequality will be improved in Theorem 3.3.1, which introduces the Cauchy-
Schwarz Inequality.
We ï¬nally mention here, without proof, a third case where the inner product
between the Lebesgue measurable signals u, v is deï¬ned. The result here is that if
for some numbers 1 < p, q < âˆsatisfying 1/p + 1/q = 1 we have that
 âˆ
âˆ’âˆ
u(t)
p dt < âˆ
and
 âˆ
âˆ’âˆ
v(t)
q dt < âˆ,
then t 	â†’u(t) v(t) is integrable. The proof of this result follows from HÂ¨olderâ€™s
Inequality (Theorem 3.3.2 ahead). Notice that the second case we addressed (where
u and v are both of ï¬nite energy) follows from this case by considering p = q = 2.
3.3
The Cauchy-Schwarz Inequality
The Cauchy-Schwarz Inequality is probably the most important inequality on the
inner product. Its discrete version is attributed to Augustin-Louis Cauchy (1789â€“
1857) and its integral form to Victor Yacovlevich Bunyakovsky (1804â€“1889) who
studied with him in Paris. Its (double) integral form was derived independently by
Hermann Amandus Schwarz (1843â€“1921). See (Steele, 2004, pp. 10â€“12) for more
on the history of this inequality and on how inequalities get their names.
Theorem 3.3.1 (Cauchy-Schwarz Inequality). If the functions u, v: R â†’C are
of ï¬nite energy, then the mapping t 	â†’u(t) vâˆ—(t) is integrable and
âŸ¨u, vâŸ©
 â‰¤âˆ¥uâˆ¥2 âˆ¥vâˆ¥2 .
(3.17)
That is,

 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt
 â‰¤
 âˆ
âˆ’âˆ
u(t)
2 dt
 âˆ
âˆ’âˆ
v(t)
2 dt.
Equality in the Cauchy-Schwarz Inequality is possible, e.g., if u is a scaled version
of v, i.e., if for some constant Î±
u(t) = Î± v(t),
t âˆˆR.
.005
14:02:50, subject to the Cambridge Core terms of use, available at

3.3 The Cauchy-Schwarz Inequality
19
In fact, the Cauchy-Schwarz Inequality holds with equality if, and only if, either v(t)
is zero for all t outside a set of Lebesgue measure zero or for some constant Î± we
have u(t) = Î± v(t) for all t outside a set of Lebesgue measure zero.
There are a number of diï¬€erent proofs of this important inequality. We shall focus
here on one that is based on (3.16) because it demonstrates a general technique for
improving inequalities. The idea is that once one obtains a certain inequalityâ€”in
our case (3.16)â€”one can try to improve it by taking advantage of oneâ€™s under-
standing of how the quantity in question is aï¬€ected by various transformations.
This technique is beautifully illustrated in (Steele, 2004).
Proof. The quantity in question is |âŸ¨u, vâŸ©|. We shall take advantage of our under-
standing of how this quantity behaves when we replace u with its scaled version
Î± u and when we replace v with its scaled version Î² v. Here Î±, Î² âˆˆC are arbitrary.
The quantity in question transforms as
|âŸ¨Î± u, Î² vâŸ©| = |Î±| |Î²| |âŸ¨u, vâŸ©|.
(3.18)
We now use (3.16) to upper-bound the left-hand side (LHS) of the above by sub-
stituting Î± u and Î² v for u and v in (3.16) to obtain
|Î±| |Î²| |âŸ¨u, vâŸ©| = |âŸ¨Î± u, Î² vâŸ©|
â‰¤1
2|Î±|2 âˆ¥uâˆ¥2
2 + 1
2|Î²|2 âˆ¥vâˆ¥2
2 ,
Î±, Î² âˆˆC.
(3.19)
If both âˆ¥uâˆ¥2 and âˆ¥vâˆ¥2 are positive, then (3.17) follows from (3.19) by choosing
Î± = 1/ âˆ¥uâˆ¥2 and Î² = 1/ âˆ¥vâˆ¥2. To conclude the proof it thus remains to show that
(3.17) also holds when either âˆ¥uâˆ¥2 or âˆ¥vâˆ¥2 is zero so the RHS of (3.17) is zero.
That is, we need to show that if either âˆ¥uâˆ¥2 or âˆ¥vâˆ¥2 is zero, then âŸ¨u, vâŸ©must also
be zero. To show this, suppose ï¬rst that âˆ¥uâˆ¥2 is zero. By substituting Î± = 1 in
(3.19) we obtain in this case that
|Î²| |âŸ¨u, vâŸ©| â‰¤1
2|Î²|2 âˆ¥vâˆ¥2
2 ,
which, upon dividing by |Î²|, yields
|âŸ¨u, vâŸ©| â‰¤1
2|Î²| âˆ¥vâˆ¥2
2 ,
Î² Ì¸= 0.
Upon letting |Î²| tend to zero from above this demonstrates that âŸ¨u, vâŸ©must be zero
as we set out to prove. (As an alternative proof of this case one notes that âˆ¥uâˆ¥2 = 0
implies, by Proposition 2.5.3, that the set {t âˆˆR : u(t) Ì¸= 0} is of Lebesgue measure
zero. Consequently, since every zero of t 	â†’u(t) is also a zero of t 	â†’u(t) vâˆ—(t),
it follows that {t âˆˆR : u(t) vâˆ—(t) Ì¸= 0} is included in {t âˆˆR : u(t) Ì¸= 0}, and
must therefore also be of Lebesgue measure zero (Exercise 2.10). Consequently,
by Proposition 2.5.3,
 âˆ
âˆ’âˆ|u(t) vâˆ—(t)| dt must be zero, which, by Proposition 2.4.1,
implies that |âŸ¨u, vâŸ©| must be zero.)
The case where âˆ¥vâˆ¥2 = 0 is very similar: by substituting Î² = 1 in (3.19) we obtain
that (in this case)
|âŸ¨u, vâŸ©| â‰¤1
2|Î±| âˆ¥uâˆ¥2
2 ,
Î± Ì¸= 0
.005
14:02:50, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

20
The Inner Product
and the result follows upon letting |Î±| tend to zero from above.
While we shall not use the following inequality in this book, it is suï¬ƒciently im-
portant that we mention it in passing.
Theorem 3.3.2 (HÂ¨olderâ€™s Inequality). If u: R â†’C and v: R â†’C are Lebesgue
measurable functions satisfying
 âˆ
âˆ’âˆ
u(t)
p dt < âˆ
and
 âˆ
âˆ’âˆ
v(t)
q dt < âˆ
for some 1 < p, q < âˆsatisfying 1/p + 1/q = 1, then the function t 	â†’u(t) vâˆ—(t) is
integrable and

 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt
 â‰¤
	 âˆ
âˆ’âˆ
u(t)
p dt

1/p 	 âˆ
âˆ’âˆ
v(t)
q dt

1/q
.
(3.20)
Note that the Cauchy-Schwarz Inequality corresponds to the case where p = q = 2.
Proof. See, for example, (Rudin, 1987, Theorem 3.5) or (Royden and Fitzpatrick,
2010, Section 7.2, Theorem 1).
3.4
Applications
There are numerous applications of the Cauchy-Schwarz Inequality. Here we only
mention a few. The ï¬rst relates the energy in the superposition of two signals to
the energies of the individual signals. The result holds for both complex-valued and
real-valued functions, andâ€”as is our customâ€”we shall thus not make the codomain
explicit.
Proposition 3.4.1 (Triangle Inequality for L2). If u and v are in L2, then
âˆ¥u + vâˆ¥2 â‰¤âˆ¥uâˆ¥2 + âˆ¥vâˆ¥2 .
(3.21)
Proof. The proof is a straightforward application of the Cauchy-Schwarz Inequality
and the basic properties of the inner product (3.6)â€“(3.10):
âˆ¥u + vâˆ¥2
2 = âŸ¨u + v, u + vâŸ©
= âŸ¨u, uâŸ©+ âŸ¨v, vâŸ©+ âŸ¨u, vâŸ©+ âŸ¨v, uâŸ©
= âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2 + 2 Re(âŸ¨u, vâŸ©)
â‰¤âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2 + 2 |âŸ¨u, vâŸ©|
â‰¤âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2 + 2 âˆ¥uâˆ¥2 âˆ¥vâˆ¥2
=

âˆ¥uâˆ¥2 + âˆ¥vâˆ¥2
2,
from which the result follows by taking square roots. Here the ï¬rst line follows from
the deï¬nition of âˆ¥Â·âˆ¥2 (3.11); the second from (3.9) & (3.10); the third because âŸ¨v, uâŸ©
is the complex conjugate of âŸ¨u, vâŸ©(3.6); the fourth from the inequality |Re(z)| â‰¤|z|,
which holds for every z âˆˆC; the ï¬fth by the Cauchy-Schwarz Inequality; and the
sixth by simple algebra.
.005
14:02:50, subject to the Cambridge Core terms of use, available at

3.4 Applications
21
Another important mathematical consequence of the Cauchy-Schwarz Inequality is
the continuity of the inner product. To state the result we use the notation an â†’a
to indicate that the sequence a1, a2, . . . converges to a, i.e., that limnâ†’âˆan = a.
Proposition 3.4.2 (Continuity of the Inner Product). Let u and v be in L2. If
the sequence u1, u2, . . . of elements of L2 satisï¬es
âˆ¥un âˆ’uâˆ¥2 â†’0,
and if the sequence v1, v2, . . . of elements of L2 satisï¬es
âˆ¥vn âˆ’vâˆ¥2 â†’0,
then
âŸ¨un, vnâŸ©â†’âŸ¨u, vâŸ©.
Proof.
|âŸ¨un, vnâŸ©âˆ’âŸ¨u, vâŸ©|
= |âŸ¨un âˆ’u, vâŸ©+ âŸ¨un âˆ’u, vn âˆ’vâŸ©+ âŸ¨u, vn âˆ’vâŸ©|
â‰¤|âŸ¨un âˆ’u, vâŸ©| + |âŸ¨un âˆ’u, vn âˆ’vâŸ©| + |âŸ¨u, vn âˆ’vâŸ©|
â‰¤âˆ¥un âˆ’uâˆ¥2 âˆ¥vâˆ¥2 + âˆ¥un âˆ’uâˆ¥2 âˆ¥vn âˆ’vâˆ¥2 + âˆ¥uâˆ¥2 âˆ¥vn âˆ’vâˆ¥2
â†’0,
where the ï¬rst equality follows from the basic properties of the inner product (3.6)â€“
(3.10); the subsequent inequality by the Triangle Inequality for Complex Numbers
(2.12); the subsequent inequality from the Cauchy-Schwarz Inequality; and where
the ï¬nal limit follows from the propositionâ€™s hypotheses.
Another useful consequence of the Cauchy-Schwarz Inequality is that if a signal is
energy-limited and is zero outside an interval, then it is also integrable.
Proposition 3.4.3 (Finite-Energy Functions over Finite Intervals Are Integrable).
If for some real numbers a and b satisfying a â‰¤b we have
 b
a
x(Î¾)
2 dÎ¾ < âˆ,
then
 b
a
x(Î¾)
 dÎ¾ â‰¤
âˆš
b âˆ’a
 b
a
x(Î¾)
2 dÎ¾ ,
and, in particular,
 b
a
x(Î¾)
 dÎ¾ < âˆ.
.005
14:02:50, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

22
The Inner Product
Proof.
 b
a
x(Î¾)
 dt =
 âˆ
âˆ’âˆ
I{a â‰¤Î¾ â‰¤b}
x(Î¾)
 dÎ¾
=
 âˆ
âˆ’âˆ
I{a â‰¤Î¾ â‰¤b}



u(Î¾)
I{a â‰¤Î¾ â‰¤b}
x(Î¾)




v(Î¾)
dÎ¾
â‰¤
âˆš
b âˆ’a
 b
a
x(Î¾)
2 dÎ¾,
where the inequality is just an application of the Cauchy-Schwarz Inequality to the
function Î¾ 	â†’I{a â‰¤Î¾ â‰¤b} |x(Î¾)| and the indicator function Î¾ 	â†’I{a â‰¤Î¾ â‰¤b}.
Note that, in general, an energy-limited signal need not be integrable. For example,
the real signal
t 	â†’

0
if t â‰¤1,
1/t
otherwise,
(3.22)
is of ï¬nite energy but is not integrable.
The Cauchy-Schwarz Inequality demonstrates that if both u and v are of ï¬nite
energy, then their inner product âŸ¨u, vâŸ©is well-deï¬ned, i.e., the integrand in (3.4) is
integrable. It can also be used in slightly more sophisticated ways. For example, it
can be used to treat cases where one of the functions, say u, is not of ï¬nite energy
but where the second function decays to zero suï¬ƒciently quickly to compensate for
that. For example:
Proposition 3.4.4. If the Lebesgue measurable functions x: R â†’C and y: R â†’C
satisfy
 âˆ
âˆ’âˆ
|x(t)|2
t2 + 1 dt < âˆ
and
 âˆ
âˆ’âˆ
|y(t)|2 (t2 + 1) dt < âˆ,
then the function t 	â†’x(t) yâˆ—(t) is integrable and

 âˆ
âˆ’âˆ
x(t) yâˆ—(t) dt
 â‰¤
 âˆ
âˆ’âˆ
|x(t)|2
t2 + 1 dt
 âˆ
âˆ’âˆ
|y(t)|2 (t2 + 1) dt.
Proof. This is a simple application of the Cauchy-Schwarz Inequality to the func-
tions t 	â†’x(t)/
âˆš
t2 + 1 and t 	â†’y(t)
âˆš
t2 + 1. Simply write
 âˆ
âˆ’âˆ
x(t) yâˆ—(t) dt =
 âˆ
âˆ’âˆ
x(t)
âˆš
t2 + 1



u(t)

t2 + 1 yâˆ—(t)



vâˆ—(t)
dt
and apply the Cauchy-Schwarz Inequality to the functions u(Â·) and v(Â·).
.005
14:02:50, subject to the Cambridge Core terms of use, available at

3.5 The Cauchy-Schwarz Inequality for Random Variables
23
3.5
The Cauchy-Schwarz Inequality for Random Variables
There is also a version of the Cauchy-Schwarz Inequality for random variables. It is
very similar to Theorem 3.3.1 but with time integrals replaced by expectations. We
denote the expectation of the random variable X by E[X] and remind the reader
that the variance Var[X] of the random variable X is deï¬ned as
Var[X] = E

(X âˆ’E[X])2
.
(3.23)
Theorem 3.5.1 (Cauchy-Schwarz Inequality for Random Variables). Let the ran-
dom variables U and V be of ï¬nite variance. Then
E[U V ]
 â‰¤

E[U 2]

E[V 2],
(3.24)
with equality if, and only if, Pr[Î± U = Î² V ] = 1 for some real Î± and Î² that are not
both equal to zero.
Proof. Use the proof of Theorem 3.3.1 with all time integrals replaced with ex-
pectations. For a diï¬€erent proof and for the conditions for equality see (Grimmett
and Stirzaker, 2001, Chapter 3, Section 3.5, Theorem 9).
For the next corollary we need to recall that the covariance Cov[U, V ] between the
ï¬nite-variance random variables U, V is deï¬ned as
Cov[U, V ] = E

U âˆ’E[U]

V âˆ’E[V ]

.
(3.25)
Corollary 3.5.2 (Covariance Inequality). If the random variables U and V are of
ï¬nite variance Var[U] and Var[V ], then
Cov[U, V ]
 â‰¤

Var[U]

Var[V ].
(3.26)
Proof. Apply Theorem 3.5.1 to the random variables U âˆ’E[U] and V âˆ’E[V ].
Corollary 3.5.2 shows that the correlation coeï¬ƒcient, which is deï¬ned for ran-
dom variables U and V having strictly positive variances as
Ï =
Cov[U, V ]

Var[U]

Var[V ]
,
(3.27)
satisï¬es
âˆ’1 â‰¤Ï â‰¤+1.
(3.28)
3.6
Mathematical Comments
(i) Mathematicians typically consider âŸ¨u, vâŸ©only when both u and v are of ï¬nite
energy. We are more forgiving and simply require that the integral deï¬ning
the inner product be well-deï¬ned, i.e., that the integrand be integrable.
.005
14:02:50, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

24
The Inner Product
(ii) Some refer to âˆ¥uâˆ¥2 as the â€œnorm of uâ€ or the â€œL2 norm of u.â€ We shall
refrain from this usage because mathematicians use the term â€œnormâ€ very
selectively. They require that no function other than the all-zero function be
of zero norm, and this is not the case for âˆ¥Â·âˆ¥2. Indeed, any function u that
is indistinguishable from the all-zero function satisï¬es âˆ¥uâˆ¥2 = 0, and there
are many such functions (e.g., the function t 	â†’I{t is a rational number}
that is equal to one at rational times and that is equal to zero at all other
times). This diï¬ƒculty can be overcome by deï¬ning two functions to be the
same if their diï¬€erence is of zero energy. In this case âˆ¥Â·âˆ¥2 is a norm in the
mathematical sense and is, in fact, what mathematicians call the L2 norm.
This issue is discussed in greater detail in Section 4.7. To stay out of trouble
we shall refrain from giving âˆ¥Â·âˆ¥2 a name.
3.7
Exercises
Exercise 3.1 (Truncated Polynomials). Consider the signals u: t â†’(t + 2) I{0 â‰¤t â‰¤1}
and v: t â†’(t2 âˆ’2t âˆ’3) I{0 â‰¤t â‰¤1}. Compute the energies âˆ¥uâˆ¥2
2 and âˆ¥vâˆ¥2
2 as well as
the inner product âŸ¨u, vâŸ©.
Exercise 3.2 (Inner Products of Mirror Images). Express the inner product âŸ¨~x, ~yâŸ©in
terms of the inner product âŸ¨x, yâŸ©.
Exercise 3.3 (Finite-Energy Signals). Let x be an energy-limited signal.
(i) Show that, for every t0 âˆˆR, the signal t â†’x(t âˆ’t0) must also be energy-limited.
(ii) Show that the reï¬‚ection of x is also energy-limited. I.e., show that the signal ~x
that maps t to x(âˆ’t) is energy-limited.
(iii) How are the energies in t â†’x(t), t â†’x(t âˆ’t0), and t â†’x(âˆ’t) related?
Exercise 3.4 (Manipulating Inner Products). Show that if u, v, and w are energy-limited
complex signals, then
âŸ¨u + v, 3u + v + iwâŸ©= 3 âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2 + âŸ¨u, vâŸ©+ 3 âŸ¨u, vâŸ©âˆ—âˆ’i âŸ¨u, wâŸ©âˆ’i âŸ¨v, wâŸ©.
Exercise 3.5 (A Useful Identity). A classic result in algebra states that a2 âˆ’b2 factorizes
as (a âˆ’b)(a + b) for all a, b âˆˆR.
(i) Prove its integral counterpart for real energy-limited signals:
âˆ¥uâˆ¥2
2 âˆ’âˆ¥vâˆ¥2
2 = âŸ¨u âˆ’v, u + vâŸ©.
(ii) Does this identity also hold for complex signals?
Exercise 3.6 (Orthogonality to All Signals). Let u be an energy-limited signal. Show
that
	
u â‰¡0

â‡â‡’
	
âŸ¨u, vâŸ©= 0,
v âˆˆL2

.
Exercise 3.7 (On the Tightness of the Cauchy-Schwarz Inequality). Show that the bound
obtained from the Cauchy-Schwarz Inequality is at least as tight as (3.16).
.005
14:02:50, subject to the Cambridge Core terms of use, available at

3.7 Exercises
25
Exercise 3.8 (A Generalized Cauchy-Schwarz Inequality). Let h: R â†’R be a nonnegative
function. Prove that if the complex signals u, v are such that the integrals

|u(t)|2 h(t) dt
and

|v(t)|2 h(t) dt are ï¬nite, then

 âˆ
âˆ’âˆ
u(t) vâˆ—(t) h(t) dt

2
â‰¤
 âˆ
âˆ’âˆ
u(t)
2 h(t) dt
 âˆ
âˆ’âˆ
v(t)
2 h(t) dt

.
Exercise 3.9 (Indistinguishability and Inner Products). Let u âˆˆL2 be indistinguishable
from uâ€² âˆˆL2, and let v âˆˆL2 be indistinguishable from vâ€² âˆˆL2. Show that the inner
product âŸ¨uâ€², vâ€²âŸ©is equal to the inner product âŸ¨u, vâŸ©.
Exercise 3.10 (Finite Energy and Integrability). Let x: R â†’C be Lebesgue measurable.
(i) Show that the conditions that x is of ï¬nite energy and that the mapping t â†’t x(t)
is of ï¬nite energy are simultaneously met if, and only if,
 âˆ
âˆ’âˆ
|x(t)|2 
1 + t2
dt < âˆ.
(3.29)
(ii) Show that (3.29) implies that x is integrable.
(iii) Give an example of an integrable signal that does not satisfy (3.29).
Exercise 3.11 (The Variance of a Sum). Show that if the random variables X and Y are
of ï¬nite variance, then
Var[X + Y ] â‰¤

Var[X] +

Var[Y ]
2.
Exercise 3.12 (The Cauchy-Schwarz Inequality for Sequences).
(i) Let the complex sequences a1, a2, . . . and b1, b2, . . . satisfy
âˆ

Î½=1
|aÎ½|2,
âˆ

Î½=1
|bÎ½|2 < âˆ.
Show that

âˆ

Î½=1
aÎ½ bâˆ—
Î½

2
â‰¤
 âˆ

Î½=1
|aÎ½|2
 âˆ

Î½=1
|bÎ½|2

.
(ii) Derive the Cauchy-Schwarz Inequality for d-tuples:

d

Î½=1
aÎ½ bâˆ—
Î½

2
â‰¤

d

Î½=1
|aÎ½|2

d

Î½=1
|bÎ½|2

.
Exercise 3.13 (Cauchy-Schwarz and Matrices). Prove that if A and B are real n Ã— m
matrices then
	
tr

ABT
2
â‰¤

n

k=1
m

â„“=1

a(k,â„“)2

n

k=1
m

â„“=1

b(k,â„“)2

.
Hint: Exercise 3.12 (ii) may be helpful.
.005
14:02:50, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

26
The Inner Product
Exercise 3.14 (Summability and Square Summability). Let a1, a2, . . . be a sequence of
complex numbers. Show that
 âˆ

Î½=1
|aÎ½| < âˆ

=â‡’
 âˆ

Î½=1
|aÎ½|2 < âˆ

.
Exercise 3.15 (A Friendlier GPA). Use the Cauchy-Schwarz Inequality for d-tuples (Prob-
lem 3.12) to show that for any positive integer d,
a1 + Â· Â· Â· + ad
d
â‰¤

a2
1 + Â· Â· Â· + a2
d
d
,
a1, . . . , ad âˆˆR.
.005
14:02:50, subject to the Cambridge Core terms of use, available at

Chapter 4
The Space L2 of Energy-Limited Signals
4.1
Introduction
In this chapter we shall study the space L2 of energy-limited signals in greater
detail. We shall show that its elements can be viewed as vectors in a vector space
and begin developing a geometric intuition for understanding its structure. We
shall focus on the case of complex-valued signals, but with some minor changes the
results are also applicable to real-valued signals. (The main changes that are needed
for translating the results to real-valued signals are replacing C with R, ignoring
the conjugation operation, and interpreting |Â·| as the absolute value function for
real arguments as opposed to the modulus function.)
We remind the reader that the space L2 was deï¬ned in Deï¬nition 3.1.1 as the set
of all Lebesgue measurable complex-valued signals u: R â†’C satisfying
 âˆ
âˆ’âˆ
u(t)
2 dt < âˆ,
(4.1)
and that in (3.12) we deï¬ned for every u âˆˆL2 the quantity âˆ¥uâˆ¥2 as
âˆ¥uâˆ¥2 =
 âˆ
âˆ’âˆ
u(t)
2 dt.
(4.2)
We refer to L2 as the space of energy-limited signals and to its elements as energy-
limited signals or signals of ï¬nite energy.
4.2
L2 as a Vector Space
In this section we shall explain how to view the space L2 as a vector space over
the complex ï¬eld by thinking about signals in L2 as vectors, by interpreting the
superposition u + v of two signals as vector-addition, and by interpreting the
ampliï¬cation of u by Î± as the operation of multiplying the vector u by the scalar
Î± âˆˆC.
We begin by reminding the reader that the superposition of the two signals u
and v is denoted by u + v and is the signal that maps every t âˆˆR to u(t) + v(t).
The ampliï¬cation of u by Î± is denoted by Î± u and is the signal that maps every
27
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

28
The Space L2 of Energy-Limited Signals
t âˆˆR to Î± u(t). More generally, if u and v are signals and if Î± and Î² are complex
numbers, then Î± u + Î² v is the signal t 	â†’Î± u(t) + Î² v(t).
If u âˆˆL2 and Î± âˆˆC, then Î± u is also in L2. Indeed, the measurability of u implies
the measurability of Î± u, and if u is of ï¬nite energy, then Î± u is also of ï¬nite energy,
because the energy in Î± u is the product of |Î±|2 by the energy in u. We thus see
that the operation of ampliï¬cation of u by Î± results in an element of L2 whenever
u âˆˆL2 and Î± âˆˆC.
We next show that if the signals u and v are in L2, then their superposition
u+v must also be in L2. This holds because a standard result in Measure Theory
guarantees that the superposition of two Lebesgue measurable signals is a Lebesgue
measurable signal and because Proposition 3.4.1 guarantees that if both u and v
are of ï¬nite energy, then so is their superposition. Thus the superposition that
maps u and v to u + v results in an element of L2 whenever u, v âˆˆL2.
It can be readily veriï¬ed that the following properties hold:
(i) commutativity:
u + v = v + u,
u, v âˆˆL2;
(ii) associativity:
(u + v) + w = u + (v + w),
u, v, w âˆˆL2,
(Î± Î²) u = Î± (Î² u),

Î±, Î² âˆˆC,
u âˆˆL2

;
(iii) additive identity: the all-zero signal 0: t 	â†’0 satisï¬es
0 + u = u,
u âˆˆL2;
(iv) additive inverse:
to every u âˆˆL2 there corresponds a signal w âˆˆL2
(namely, the signal t 	â†’âˆ’u(t)) such that
u + w = 0;
(v) multiplicative identity:
1 u = u,
u âˆˆL2;
(vi) distributive properties:
Î± (u + v) = Î± u + Î± v,

Î± âˆˆC,
u, v âˆˆL2

,
(Î± + Î²) u = Î± u + Î² u,

Î±, Î² âˆˆC,
u âˆˆL2

.
We conclude that with the operations of superposition and ampliï¬cation the set L2
forms a vector space over the complex ï¬eld (Axler, 2015, Chapter 1). This justiï¬es
referring to the elements of L2 as â€œvectors,â€ to the operation of signal superposition
as â€œvector addition,â€ and to the operation of ampliï¬cation of an element of L2 by
a complex scalar as â€œscalar multiplication.â€
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.3 Subspace, Dimension, and Basis
29
4.3
Subspace, Dimension, and Basis
Once we have noted that L2 together with the operations of superposition and
ampliï¬cation forms a vector space, we can borrow numerous deï¬nitions and results
from the theory of vector spaces. Here we shall focus on the very basic ones.
A linear subspace (or just subspace) of L2 is a nonempty subset U of L2 that
is closed under superposition
u1 + u2 âˆˆU,
u1, u2 âˆˆU
(4.3)
and under ampliï¬cation
Î± u âˆˆU,

Î± âˆˆC,
u âˆˆU

.
(4.4)
Example 4.3.1. Consider the set of all functions of the form
t 	â†’p(t) eâˆ’|t|,
where p(t) is any polynomial of degree no larger than 3. Thus, the set is the set of
all functions of the form
t 	â†’

Î±0 + Î±1 t + Î±2 t2 + Î±3 t3
eâˆ’|t|,
(4.5)
where Î±0, Î±1, Î±2, Î±3 are arbitrary complex numbers.
In spite of the polynomial growth of the function in parentheses in (4.5), all the
functions in this set are in L2 because the exponential decay more than compen-
sates for the polynomial growth. The above set is thus a subset of L2. Moreover,
as we show next, this is a linear subspace of L2.
If u is of the form (4.5), then so is Î± u, because Î± u is the mapping
t 	â†’

Î± Î±0 + Î± Î±1 t + Î± Î±2 t2 + Î± Î±3 t3
eâˆ’|t|,
which is of the same form.
Similarly, if u is as given in (4.5) and
v: t 	â†’

Î²0 + Î²1 t + Î²2 t2 + Î²3 t3
eâˆ’|t|,
then u + v is the mapping
t 	â†’

(Î±0 + Î²0) + (Î±1 + Î²1) t + (Î±2 + Î²2) t2 + (Î±3 + Î²3) t3
eâˆ’|t|,
which is again of this form.
An n-tuple of vectors from L2 is a (possibly empty) ordered list of n vectors
from L2 separated by commas and enclosed in parentheses, e.g., (v1, . . . , vn). Here
n â‰¥0 can be any nonnegative integer, where the case n = 0 corresponds to the
empty list.
A vector v âˆˆL2 is said to be a linear combination of the n-tuple (v1, . . . , vn) if
it is equal to
Î±1 v1 + Â· Â· Â· + Î±n vn,
(4.6)
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

30
The Space L2 of Energy-Limited Signals
which is written more succinctly as
n

Î½=1
Î±Î½ vÎ½,
(4.7)
for some scalars Î±1, . . . , Î±n âˆˆC. The all-zero signal is a linear combination of any
n-tuple including the empty tuple.
The span of an n-tuple (v1, . . . , vn) of vectors in L2 is denoted by
span(v1, . . . , vn)
and is the set of all vectors in L2 that are linear combinations of (v1, . . . , vn):
span(v1, . . . , vn) â‰œ{Î±1 v1 + Â· Â· Â· + Î±n vn : Î±1, . . . , Î±n âˆˆC}.
(4.8)
(The span of the empty tuple is given by the one-element set {0} containing the
all-zero signal only.)
Note that for any n-tuple of vectors (v1, . . . , vn) in L2 we have that span(v1, . . . , vn)
is a linear subspace of L2. Also, if U is a linear subspace of L2 and if the vectors
u1, . . . , un are in U, then span(u1, . . . , un) is a linear subspace which is contained
in U. A subspace U of L2 is said to be ï¬nite-dimensional if there exists an
n-tuple (u1, . . . , un) of vectors in U such that span(u1, . . . , un) = U. Otherwise,
we say that U is inï¬nite-dimensional. For example, the space of all mappings
of the form t 	â†’p(t) eâˆ’|t| for some polynomial p(Â·) can be shown to be inï¬nite-
dimensional, but under the restriction that p(Â·) be of degree smaller than 5, it is
ï¬nite-dimensional. If U is a ï¬nite-dimensional subspace and if Uâ€² is a subspace
contained in U, then Uâ€² must also be ï¬nite-dimensional.
An n-tuple of signals (v1, . . . , vn) in L2 is said to be linearly independent if
whenever the scalars Î±1, . . . , Î±n âˆˆC are such that Î±1 v1 +Â· Â· Â·+Î±n vn = 0, we have
Î±1 = Â· Â· Â· = Î±n = 0. I.e., if
	
n

Î½=1
Î±Î½ vÎ½ = 0

=â‡’

Î±Î½ = 0,
Î½ = 1, . . . , n

.
(4.9)
(By convention, the empty tuple is linearly independent.)
For example, the 3-
tuple consisting of the signals t 	â†’eâˆ’|t|, t 	â†’t eâˆ’|t|, and t 	â†’t2 eâˆ’|t| is linearly
independent. If (v1, . . . , vn) is not linearly independent, then we say that it is
linearly dependent. For example, the 3-tuple consisting of the signals t 	â†’eâˆ’|t|,
t 	â†’t eâˆ’|t|, and t 	â†’

2t + 1

eâˆ’|t| is linearly dependent. The n-tuple (v1, . . . , vn)
is linearly dependent if, and only if, (at least) one of the signals in the tuple can
be written as a linear combination of the others.
The d-tuple (u1, . . . , ud) is said to form a basis for the linear subspace U if it is
linearly independent and if span(u1, . . . , ud) = U. The latter condition is equivalent
to the requirement that every u âˆˆU can be represented as
u = Î±1 u1 + Â· Â· Â· + Î±d ud
(4.10)
for some Î±1, . . . , Î±d âˆˆC.
The former condition that the tuple (u1, . . . , ud) be
linearly independent guarantees that if such a representation exists, then it is
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.4 âˆ¥uâˆ¥2 as the â€œlengthâ€ of the Signal u(Â·)
31
unique. Thus, (u1, . . . , ud) forms a basis for U if u1, . . . , ud âˆˆU (thus guaranteeing
that span(u1, . . . , ud) âŠ†U) and if every u âˆˆU can be written uniquely as in (4.10).
Every ï¬nite-dimensional linear subspace U has a basis, and all bases for U have the
same number of elements. This number is called the dimension of U. Thus, if U
is a ï¬nite-dimensional subspace and if both (u1, . . . , ud) and (uâ€²
1, . . . , uâ€²
dâ€²) form a
basis for U, then d = dâ€² and both are equal to the dimension of U. The dimension
of the subspace {0} is zero. If U is a ï¬nite-dimensional subspace and if Uâ€² is a
subspace contained in U, then the dimension of Uâ€² cannot exceed that of U.
4.4
âˆ¥uâˆ¥2 as the â€œlengthâ€ of the Signal u(Â·)
Having presented the elements of L2 as vectors, we next propose to view âˆ¥uâˆ¥2 as
the â€œlengthâ€ of the vector u âˆˆL2. To motivate this view, we ï¬rst present the key
properties of âˆ¥Â·âˆ¥2.
Proposition 4.4.1 (Properties of âˆ¥Â·âˆ¥2). Let u and v be elements of L2, and let Î±
be some complex number. Then
âˆ¥Î± uâˆ¥2 = |Î±| âˆ¥uâˆ¥2 ,
(4.11)
âˆ¥u + vâˆ¥2 â‰¤âˆ¥uâˆ¥2 + âˆ¥vâˆ¥2 ,
(4.12)
and

âˆ¥uâˆ¥2 = 0

â‡â‡’

u â‰¡0

.
(4.13)
Proof. Identity (4.11) follows directly from the deï¬nition of âˆ¥Â·âˆ¥2; see (4.2). In-
equality (4.12) is a restatement of Proposition 3.4.1. The equivalence of the con-
dition âˆ¥uâˆ¥2 = 0 and the condition that u is indistinguishable from the all-zero
signal 0 follows from Proposition 2.5.3.
Identity (4.11) is in agreement with our intuition that stretching a vector merely
scales its length. Inequality (4.12) is sometimes called the Triangle Inequality
because it is reminiscent of the theorem from planar geometry that states that the
length of no side of a triangle can exceed the sum of the lengths of the others; see
Figure 4.1.
Substituting âˆ’y for u and x + y for v in (4.12) yields âˆ¥xâˆ¥2 â‰¤âˆ¥yâˆ¥2 + âˆ¥x + yâˆ¥2,
i.e., the inequality âˆ¥x + yâˆ¥2 â‰¥âˆ¥xâˆ¥2 âˆ’âˆ¥yâˆ¥2. And substituting âˆ’x for u and x + y
for v in (4.12) yields the inequality âˆ¥yâˆ¥2 â‰¤âˆ¥xâˆ¥2 + âˆ¥x + yâˆ¥2, i.e., the inequality
âˆ¥x + yâˆ¥2 â‰¥âˆ¥yâˆ¥2 âˆ’âˆ¥xâˆ¥2. Combining the two inequalities we obtain the inequality
âˆ¥x + yâˆ¥2 â‰¥
âˆ¥xâˆ¥2 âˆ’âˆ¥yâˆ¥2
. This inequality can be combined with the inequality
âˆ¥x + yâˆ¥2 â‰¤âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2 in the compact form of a double-sided inequality
âˆ¥xâˆ¥2 âˆ’âˆ¥yâˆ¥2
 â‰¤âˆ¥x + yâˆ¥2 â‰¤âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2 ,
x, y âˆˆL2.
(4.14)
Finally, (4.13) â€œalmostâ€ supports the intuition that the only vector of length zero
is the zero-vector. In our case, alas, we can only claim that if a vector is of zero
length, then it is indistinguishable from the all-zero signal, i.e., that all tâ€™s outside
a set of Lebesgue measure zero are mapped by the signal to zero.
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

32
The Space L2 of Energy-Limited Signals
u
v
u + v
Figure 4.1: A geometric interpretation of the Triangle Inequality for energy-limited
signals: âˆ¥u + vâˆ¥2 â‰¤âˆ¥uâˆ¥2 + âˆ¥vâˆ¥2.
A
B
C
v
u
w
w âˆ’v
u âˆ’w
u âˆ’v
Figure 4.2: Illustration of the shortest path property in L2. The shortest path
from A to B is no longer than the sum of the shortest path from A to C and the
shortest path from C to B.
The Triangle Inequality (4.12) can also be stated slightly diï¬€erently. In planar
geometry the sum of the lengths of two sides of a triangle can never be smaller
than the length of the remaining side. Thus, the shortest path from Point A to
Point B cannot exceed the sum of the lengths of the shortest paths from Point A to
Point C, and from Point C to Point B. By applying Inequality (4.12) to the signal
u âˆ’w and w âˆ’v we obtain
âˆ¥u âˆ’vâˆ¥2 â‰¤âˆ¥u âˆ’wâˆ¥2 + âˆ¥w âˆ’vâˆ¥2 ,
u, v, w âˆˆL2,
(4.15)
i.e., that the distance from u to v cannot exceed the sum of distances from u to w
and from w to v. See Figure 4.2.
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.5 Orthogonality and Inner Products
33
4.5
Orthogonality and Inner Products
To further develop our geometric view of L2 we next discuss orthogonality. We
shall motivate its deï¬nition with an attempt to generalize Pythagorasâ€™s Theorem
to L2. As an initial attempt at deï¬ning orthogonality we might deï¬ne two func-
tions u, v âˆˆL2 to be orthogonal if âˆ¥u + vâˆ¥2
2 = âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2.
Recalling the
deï¬nition of âˆ¥Â·âˆ¥2 (4.2) we obtain that this condition is equivalent to the condition
Re

u(t) vâˆ—(t) dt

= 0, because
âˆ¥u + vâˆ¥2
2 =
 âˆ
âˆ’âˆ
|u(t) + v(t)|2 dt
=
 âˆ
âˆ’âˆ

u(t) + v(t)

u(t) + v(t)
âˆ—dt
=
 âˆ
âˆ’âˆ

|u(t)|2 + |v(t)|2 + 2 Re

u(t) vâˆ—(t)

dt
= âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2 + 2 Re
	 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt

,
u, v âˆˆL2,
(4.16)
where we have used the fact that integration commutes with the operation of taking
the real part; see Proposition 2.3.1.
While this approach would work well for real-valued functions, it has some embar-
rassing consequences when it comes to complex-valued functions. It allows for the
possibility that u is orthogonal to v, but that its scaled version Î±u is not. For exam-
ple, with this deï¬nition, the function t 	â†’i I{|t| â‰¤5} is orthogonal to the function
t 	â†’I{|t| â‰¤17} but its scaled (by Î± = i) version t 	â†’i i I{|t| â‰¤5} = âˆ’I{|t| â‰¤5} is
not. To avoid this embarrassment, we deï¬ne u to be orthogonal to v if
âˆ¥Î± u + vâˆ¥2
2 = âˆ¥Î± uâˆ¥2
2 + âˆ¥vâˆ¥2
2 ,
Î± âˆˆC.
This, by (4.16), is equivalent to
Re
	
Î±
 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt

= 0,
Î± âˆˆC,
i.e., to the condition
 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt = 0
(4.17)
(because if z âˆˆC is such that Re(Î± z) = 0 for all Î± âˆˆC, then z = 0). Recalling
the deï¬nition of the inner product âŸ¨u, vâŸ©from (3.4)
âŸ¨u, vâŸ©=
 âˆ
âˆ’âˆ
u(t) vâˆ—(t) dt,
(4.18)
we conclude that (4.17) is equivalent to the condition âŸ¨u, vâŸ©= 0 or, equivalently
(because by (3.6) âŸ¨u, vâŸ©= âŸ¨v, uâŸ©âˆ—) to the condition âŸ¨v, uâŸ©= 0.
Deï¬nition 4.5.1 (Orthogonal Signals in L2). The signals u, v âˆˆL2 are said to
be orthogonal if
âŸ¨u, vâŸ©= 0.
(4.19)
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

34
The Space L2 of Energy-Limited Signals
The n-tuple (u1, . . . , un) of signals in L2 is said to be orthogonal if the signals in
it are pairwise orthogonal
âŸ¨uâ„“, uâ„“â€²âŸ©= 0,

â„“Ì¸= â„“â€²,
â„“, â„“â€² âˆˆ{1, . . . , n}

.
(4.20)
The reader is encouraged to verify that if u is orthogonal to v then so is Î±u. Also,
u is orthogonal to v if, and only if, v is orthogonal to u. Finally every function is
orthogonal to the all-zero function 0.
Having judiciously deï¬ned orthogonality in L2, we can now extend Pythagorasâ€™s
Theorem.
Theorem 4.5.2 (A Pythagorean Theorem). If the n-tuple of vectors (u1, . . . , un)
in L2 is orthogonal, then
âˆ¥u1 + Â· Â· Â· + unâˆ¥2
2 = âˆ¥u1âˆ¥2
2 + Â· Â· Â· + âˆ¥unâˆ¥2
2 .
Proof. This theorem can be proved by induction on n. The case n = 2 follows
from (4.16) using Deï¬nition 4.5.1 and (4.18).
Assume now that the theorem holds for n = Î½, for some Î½ â‰¥2, i.e.,
âˆ¥u1 + Â· Â· Â· + uÎ½âˆ¥2
2 = âˆ¥u1âˆ¥2
2 + Â· Â· Â· + âˆ¥uÎ½âˆ¥2
2 ,
and let us show that this implies that it also holds for n = Î½ + 1, i.e., that
âˆ¥u1 + Â· Â· Â· + uÎ½+1âˆ¥2
2 = âˆ¥u1âˆ¥2
2 + Â· Â· Â· + âˆ¥uÎ½+1âˆ¥2
2 .
To that end, let
v = u1 + Â· Â· Â· + uÎ½.
(4.21)
Since the Î½-tuple (u1, . . . , uÎ½) is orthogonal, our induction hypothesis guarantees
that
âˆ¥vâˆ¥2
2 = âˆ¥u1âˆ¥2
2 + Â· Â· Â· + âˆ¥uÎ½âˆ¥2
2 .
(4.22)
Now v is orthogonal to uÎ½+1 because
âŸ¨v, uÎ½+1âŸ©= âŸ¨u1 + Â· Â· Â· + uÎ½, uÎ½+1âŸ©
= âŸ¨u1, uÎ½+1âŸ©+ Â· Â· Â· + âŸ¨uÎ½, uÎ½+1âŸ©
= 0,
so by the n = 2 case
âˆ¥v + uÎ½+1âˆ¥2
2 = âˆ¥vâˆ¥2
2 + âˆ¥uÎ½+1âˆ¥2
2 .
(4.23)
Combining (4.21), (4.22), and (4.23) we obtain
âˆ¥u1 + Â· Â· Â· + uÎ½+1âˆ¥2
2 = âˆ¥v + uÎ½+1âˆ¥2
2
= âˆ¥vâˆ¥2
2 + âˆ¥uÎ½+1âˆ¥2
2
= âˆ¥u1âˆ¥2
2 + Â· Â· Â· + âˆ¥uÎ½+1âˆ¥2
2 .
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.5 Orthogonality and Inner Products
35
v
u
w
Figure 4.3: The projection w of the vector v onto u.
To derive a geometric interpretation for the inner product âŸ¨u, vâŸ©we next extend
to L2 the notion of the projection of a vector onto another. We ï¬rst recall the
deï¬nition for vectors in R2. Consider two nonzero vectors u and v in the real
plane R2. The projection w of the vector v onto u is a scaled version of u. More
speciï¬cally, it is a scaled version of u and its length is equal to the product of the
length of v multiplied by the cosine of the angle between v and u (see Figure 4.3).
More explicitly,
w = (length of v) cos(angle between v and u)
u
length of u.
(4.24)
This deï¬nition does not seem to have a natural extension to L2 because we have not
deï¬ned the angle between two signals. An alternative deï¬nition of the projection,
and one that is more amenable to extensions to L2, is the following. The vector w
is the projection of the vector v onto u, if w is a scaled version of u, and if v âˆ’w
is orthogonal to u.
This deï¬nition makes perfect sense also in L2, because we have already deï¬ned
what we mean by â€œscaled versionâ€ (i.e., â€œampliï¬cationâ€ or â€œscalar multiplicationâ€)
and â€œorthogonality.â€ We thus have:
Deï¬nition 4.5.3 (Projection of a Signal in L2 onto another). Let u âˆˆL2 have
positive energy. The projection of the signal v âˆˆL2 onto the signal u âˆˆL2
is the signal w that satisï¬es both of the following conditions:
1) w = Î± u for some Î± âˆˆC and
2) v âˆ’w is orthogonal to u.
Note that since L2 is closed with respect to scalar multiplication, Condition 1)
guarantees that the projection w is in L2.
Prima facie it is not clear that a projection always exists and that it is unique.
Nevertheless, this is the case.
We prove this by ï¬nding an explicit expression
for w. We need to ï¬nd some Î± âˆˆC so that Î± u will satisfy the requirements of
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

36
The Space L2 of Energy-Limited Signals
the projection. The scalar Î± is chosen so as to guarantee that v âˆ’w is orthogonal
to u. That is, we seek to solve for Î± âˆˆC satisfying
âŸ¨v âˆ’Î± u, uâŸ©= 0,
i.e.,
âŸ¨v, uâŸ©âˆ’Î± âˆ¥uâˆ¥2
2 = 0.
Recalling our hypothesis that âˆ¥uâˆ¥2 > 0 (strictly), we conclude that Î± is uniquely
given by
Î± = âŸ¨v, uâŸ©
âˆ¥uâˆ¥2
2
,
and the projection w is thus unique and is given by
w = âŸ¨v, uâŸ©
âˆ¥uâˆ¥2
2
u.
(4.25)
Comparing (4.24) and (4.25) we can interpret
âŸ¨v, uâŸ©
âˆ¥uâˆ¥2 âˆ¥vâˆ¥2
(4.26)
as the cosine of the angle between the function v and the function u (provided
that neither u nor v is of zero energy). If the inner product is zero, then we have
said that v and u are orthogonal, which is consistent with the cosine of the angle
between them being zero. Note, however, that this interpretation should be taken
with a grain of salt because in the complex case the inner product in (4.26) is
typically a complex number.
The interpretation of (4.26) as the cosine of the angle between v and u is further
supported by noting that the magnitude of (4.26) is always in the range [0, 1]. This
follows directly from the Cauchy-Schwarz Inequality (Theorem 3.3.1) to which we
next give another (geometric) proof. Let w be the projection of v onto u. Then
starting from (4.25)
|âŸ¨v, uâŸ©|2
âˆ¥uâˆ¥2
2
= âˆ¥wâˆ¥2
2
â‰¤âˆ¥wâˆ¥2
2 + âˆ¥v âˆ’wâˆ¥2
2
= âˆ¥w + (v âˆ’w)âˆ¥2
2
= âˆ¥vâˆ¥2
2 ,
(4.27)
where the ï¬rst equality follows from (4.25); the subsequent inequality from the
nonnegativity of âˆ¥Â·âˆ¥2; and the subsequent equality by the Pythagorean Theorem
because, by its deï¬nition, the projection w of v onto u must satisfy that v âˆ’w is
orthogonal to u and hence also to w, which is a scaled version of u. The Cauchy-
Schwarz Inequality now follows from (4.27) by multiplying each of its sides by âˆ¥uâˆ¥2
2
and then taking the square root of the result.
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.6 Orthonormal Bases
37
4.6
Orthonormal Bases
We next consider orthonormal bases for ï¬nite-dimensional linear subspaces. These
are special bases that are particularly useful for the calculation of projections and
inner products.
4.6.1
Deï¬nition
Deï¬nition 4.6.1 (Orthonormal Tuple). An n-tuple of signals in L2 is said to be
orthonormal if it is orthogonal and if each of the signals in the tuple is of unit
energy.
Thus, the n-tuple (Ï†1, . . . , Ï†n) of signals in L2 is orthonormal, if
âŸ¨Ï†â„“, Ï†â„“â€²âŸ©=

0
if â„“Ì¸= â„“â€²,
1
if â„“= â„“â€²,
â„“, â„“â€² âˆˆ{1, . . . , n}.
(4.28)
Linearly independent tuples need not be orthonormal, but orthonormal tuples must
be linearly independent:
Proposition 4.6.2 (Orthonormal Tuples Are Linearly Independent). If a tuple of
signals in L2 is orthonormal, then it must be linearly independent.
Proof. Let the n-tuple (Ï†1, . . . , Ï†n) of signals in L2 be orthonormal, i.e., satisfy
(4.28). We need to show that if
n

â„“=1
Î±â„“Ï†â„“= 0,
(4.29)
then all the coeï¬ƒcients Î±1, . . . , Î±n must be zero. To that end, assume (4.29). It
then follows that for every â„“â€² âˆˆ{1, . . . , n}
0 = âŸ¨0, Ï†â„“â€²âŸ©
=

n

â„“=1
Î±â„“Ï†â„“, Ï†â„“â€²

=
n

â„“=1
Î±â„“âŸ¨Ï†â„“, Ï†â„“â€²âŸ©
=
n

â„“=1
Î±â„“I{â„“= â„“â€²}
= Î±â„“â€²,
thus demonstrating that (4.29) implies that Î±â„“â€² = 0 for every â„“â€² âˆˆ{1, . . . , n}. Here
the ï¬rst equality follows because 0 is orthogonal to every energy-limited signal
and, a fortiori, to Ï†â„“â€²; the second by (4.29); the third by the linearity of the inner
product in its left argument (3.7) & (3.9); and the fourth by (4.28).
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

38
The Space L2 of Energy-Limited Signals
Deï¬nition 4.6.3 (Orthonormal Basis). A d-tuple of signals in L2 is said to form
an orthonormal basis for the linear subspace U âŠ‚L2 if it is orthonormal and
its span is U.
By convention the empty tuple forms an orthonormal basis for the 0-dimensional
subspace {0} whose sole element is the all-zero signal.
4.6.2
Representing a Signal Using an Orthonormal Basis
Suppose that (Ï†1, . . . , Ï†d) is an orthonormal basis for U âŠ‚L2.
The fact that
(Ï†1, . . . , Ï†d) spans U guarantees that every u âˆˆU can be written as u = 
â„“Î±â„“Ï†â„“
for some coeï¬ƒcients Î±1, . . . , Î±d âˆˆC. The fact that (Ï†1, . . . , Ï†d) is orthonormal
implies, by Proposition 4.6.2, that it is also linearly independent and hence that
the coeï¬ƒcients {Î±â„“} are unique. How does one go about ï¬nding these coeï¬ƒcients?
We next show that the orthonormality of (Ï†1, . . . , Ï†d) also implies a very simple
expression for Î±â„“above. Indeed, as the next proposition demonstrates, Î±â„“is given
explicitly as âŸ¨u, Ï†â„“âŸ©.
Proposition 4.6.4 (Representing a Signal Using an Orthonormal Basis).
(i) If (Ï†1, . . . , Ï†d) is an orthonormal tuple of functions in L2 and if u âˆˆL2 can
be written as u = d
â„“=1 Î±â„“Ï†â„“for some complex numbers Î±1, . . . , Î±d, then
Î±â„“= âŸ¨u, Ï†â„“âŸ©for every â„“âˆˆ{1, . . . , d}:
	
u =
d

â„“=1
Î±â„“Ï†â„“

=â‡’
	
Î±â„“= âŸ¨u, Ï†â„“âŸ©,
â„“âˆˆ{1, . . . , d}

,

(Ï†1, . . . , Ï†d) orthonormal

.
(4.30)
(ii) If (Ï†1, . . . , Ï†d) is an orthonormal basis for the subspace U âŠ‚L2, then
u =
d

â„“=1
âŸ¨u, Ï†â„“âŸ©Ï†â„“,
u âˆˆU.
(4.31)
Proof. We begin by proving Part (i).
If u = d
â„“=1 Î±â„“Ï†â„“, then for every â„“â€² âˆˆ
{1, . . . , d}
âŸ¨u, Ï†â„“â€²âŸ©=

d

â„“=1
Î±â„“Ï†â„“, Ï†â„“â€²

=
d

â„“=1
Î±â„“âŸ¨Ï†â„“, Ï†â„“â€²âŸ©
=
d

â„“=1
Î±â„“I{â„“= â„“â€²}
= Î±â„“â€²,
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.6 Orthonormal Bases
39
thus proving Part (i).
We next prove Part (ii). Let u âˆˆU be arbitrary. Since, by assumption, the tuple
(Ï†1, . . . , Ï†d) forms an orthonormal basis for U it follows a fortiori that its span
is U and, consequently, that there exist coeï¬ƒcients Î±1, . . . , Î±d âˆˆC such that
u =
d

â„“=1
Î±â„“Ï†â„“.
(4.32)
It now follows from Part (i) that for each â„“âˆˆ{1, . . . , d} the coeï¬ƒcient Î±â„“in (4.32)
must be equal to âŸ¨u, Ï†â„“âŸ©, thus establishing (4.31).
This proposition shows that if (Ï†1, . . . , Ï†d) is an orthonormal basis for the sub-
space U and if u âˆˆU, then u is fully determined by the complex constants âŸ¨u, Ï†1âŸ©,
. . . , âŸ¨u, Ï†dâŸ©. Thus, any calculation involving u can be computed from these con-
stants by ï¬rst reconstructing u using the proposition. As we shall see in Proposi-
tion 4.6.9, calculations involving inner products and norms are, however, simpler
than that.
4.6.3
Projection
We next discuss the projection of a signal v âˆˆL2 onto a ï¬nite-dimensional linear
subspace U that has an orthonormal basis (Ï†1, . . . , Ï†d).1 To deï¬ne the projection
we shall extend the approach we adopted in Section 4.5 for the projection of the
vector v onto the vector u. Recall that in that section we deï¬ned the projection
as the vector w that is a scaled version of u and that satisï¬es that (v âˆ’w) is
orthogonal to u. Of course, if (v âˆ’w) is orthogonal to u, then it is orthogonal to
any scaled version of u, i.e., it is orthogonal to every signal in the space span(u).
We would like to adopt this approach and to deï¬ne the projection of v âˆˆL2 onto U
as the element w of U for which (v âˆ’w) is orthogonal to every signal in U. Before
we can adopt this deï¬nition, we must show that such an element of U always exists
and that it is unique.
Lemma 4.6.5. Let (Ï†1, . . . , Ï†d) be an orthonormal basis for the linear subspace
U âŠ‚L2. Let v âˆˆL2 be arbitrary.
(i) The signal v âˆ’d
â„“=1 âŸ¨v, Ï†â„“âŸ©Ï†â„“is orthogonal to every signal in U:

v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“, u

= 0,

v âˆˆL2,
u âˆˆU

.
(4.33)
(ii) If w âˆˆU is such that v âˆ’w is orthogonal to every signal in U, then
w =
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“.
(4.34)
1As we shall see in Section 4.6.5, not every ï¬nite-dimensional linear subspace of L2 has an
orthonormal basis. Here we shall only discuss projections onto subspaces that do.
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

40
The Space L2 of Energy-Limited Signals
Proof. To prove (4.33) we ï¬rst verify that it holds when u = Ï†â„“â€², for some â„“â€² in
the set {1, . . . , d}:

v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“, Ï†â„“â€²

= âŸ¨v, Ï†â„“â€²âŸ©âˆ’

d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“, Ï†â„“â€²

= âŸ¨v, Ï†â„“â€²âŸ©âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©âŸ¨Ï†â„“, Ï†â„“â€²âŸ©
= âŸ¨v, Ï†â„“â€²âŸ©âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©I{â„“= â„“â€²}
= âŸ¨v, Ï†â„“â€²âŸ©âˆ’âŸ¨v, Ï†â„“â€²âŸ©
= 0,
â„“â€² âˆˆ{1, . . . , d}.
(4.35)
Having veriï¬ed (4.33) for u = Ï†â„“â€² we next verify that this implies that it holds
for all u âˆˆU. By Proposition 4.6.4 we obtain that any u âˆˆU can be written as
u = d
â„“â€²=1 Î²â„“â€²Ï†â„“â€², where Î²â„“â€² = âŸ¨u, Ï†â„“â€²âŸ©. Consequently,

v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“, u

=

v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“,
d

â„“â€²=1
Î²â„“â€²Ï†â„“â€²

=
d

â„“â€²=1
Î²âˆ—
â„“â€²

v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“, Ï†â„“â€²

=
d

â„“â€²=1
Î²âˆ—
â„“â€² 0
= 0,
u âˆˆU,
where the third equality follows from (4.35) and the basic properties of the inner
product (3.6)â€“(3.10).
We next prove Part (ii) by showing that if w, wâ€² âˆˆU satisfy
âŸ¨v âˆ’w, uâŸ©= 0,
u âˆˆU
(4.36)
and
âŸ¨v âˆ’wâ€², uâŸ©= 0,
u âˆˆU,
(4.37)
then w = wâ€².
This follows from the calculation:
w âˆ’wâ€² =
d

â„“=1
âŸ¨w, Ï†â„“âŸ©Ï†â„“âˆ’
d

â„“=1
âŸ¨wâ€², Ï†â„“âŸ©Ï†â„“
=
d

â„“=1
âŸ¨w âˆ’wâ€², Ï†â„“âŸ©Ï†â„“
=
d

â„“=1

(v âˆ’wâ€²) âˆ’(v âˆ’w), Ï†â„“

Ï†â„“
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.6 Orthonormal Bases
41
=
d

â„“=1

(v âˆ’wâ€²), Ï†â„“

âˆ’

(v âˆ’w), Ï†â„“

Ï†â„“
=
d

â„“=1

0 âˆ’0

Ï†â„“
= 0,
where the ï¬rst equality follows from Proposition 4.6.4; the second by the linearity of
the inner product in its left argument (3.9); the third by adding and subtracting v;
the fourth by the linearity of the inner product in its left argument (3.9); and the
ï¬fth equality from (4.36) & (4.37) applied by substituting Ï†â„“for u.
With the aid of the above lemma we can now deï¬ne the projection of a signal onto
a ï¬nite-dimensional subspace that has an orthonormal basis.2
Deï¬nition 4.6.6 (Projection of v âˆˆL2 onto U). Let U âŠ‚L2 be a ï¬nite-
dimensional linear subspace of L2 having an orthonormal basis. Let v âˆˆL2 be an
arbitrary energy-limited signal. Then the projection of v onto U is the unique
element w of U such that
âŸ¨v âˆ’w, uâŸ©= 0,
u âˆˆU.
(4.38)
Note 4.6.7. By Lemma 4.6.5 it follows that if (Ï†1, . . . , Ï†d) is an orthonormal basis
for U, then the projection of v âˆˆL2 onto U is given by
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“.
(4.39)
To further develop the geometric picture of L2, we next show that, loosely speaking,
the projection of v âˆˆL2 onto U is the element in U that is closest to v. This result
can also be viewed as an optimal approximation result: if we wish to approximate v
by an element of U, then the optimal approximation is the projection of v onto U,
provided that we measure the quality of our approximation using the energy in the
error signal.
Proposition 4.6.8 (Projection as Best Approximation). Let U âŠ‚L2 be a ï¬nite-
dimensional subspace of L2 having an orthonormal basis. Let v âˆˆL2 be arbitrary.
Then the projection of v onto U is the element w âˆˆU that, among all the elements
of U, is closest to v in the sense that
âˆ¥v âˆ’uâˆ¥2 â‰¥âˆ¥v âˆ’wâˆ¥2 ,
u âˆˆU.
(4.40)
Proof. Let w be the projection of v onto U and let u be an arbitrary signal in U.
Since, by the deï¬nition of projection, w is in U and since U is a linear subspace,
it follows that w âˆ’u âˆˆU. Consequently, since by the deï¬nition of the projection
2A projection can also be deï¬ned if the subspace does not have an orthonormal basis, but in
this case there is a uniqueness issue. There may be numerous vectors w âˆˆU such that v âˆ’w is
orthogonal to all vectors in U. Fortunately, they are all indistinguishable.
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

42
The Space L2 of Energy-Limited Signals
v âˆ’w is orthogonal to every element of U, it follows that v âˆ’w is a fortiori
orthogonal to w âˆ’u. Thus
âˆ¥v âˆ’uâˆ¥2
2 = âˆ¥(v âˆ’w) + (w âˆ’u)âˆ¥2
2
= âˆ¥v âˆ’wâˆ¥2
2 + âˆ¥w âˆ’uâˆ¥2
2
(4.41)
â‰¥âˆ¥v âˆ’wâˆ¥2
2 ,
(4.42)
where the ï¬rst equality follows by subtracting and adding w, the second equality
from the orthogonality of (v âˆ’w) and (w âˆ’u), and the ï¬nal inequality by the
nonnegativity of âˆ¥Â·âˆ¥2. It follows from (4.42) that no signal in U is closer to v
than w is.
And it follows from (4.41) that if u âˆˆU is as close to v as w is,
then u âˆ’w must be an element of U that is of zero energy.
We shall see in
Proposition 4.6.10 that the hypothesis that U has an orthonormal basis implies
that the only zero-energy element of U is 0. Thus u and w must be identical, and
no other element of U is as close to v as w is.
4.6.4
Energy, Inner Products, and Orthonormal Bases
As demonstrated by Proposition 4.6.4, if (Ï†1, . . . , Ï†d) forms an orthonormal basis
for the subspace U âŠ‚L2, then any signal u âˆˆU can be reconstructed from the d
numbers âŸ¨u, Ï†1âŸ©, . . . , âŸ¨u, Ï†dâŸ©. Any quantity that can be computed from u can thus
be computed from âŸ¨u, Ï†1âŸ©, . . . , âŸ¨u, Ï†dâŸ©by ï¬rst reconstructing u and by then per-
forming the calculation on u. But some calculations involving u can be performed
based on âŸ¨u, Ï†1âŸ©, . . . , âŸ¨u, Ï†dâŸ©much more easily.
Proposition 4.6.9. Let (Ï†1, . . . , Ï†d) be an orthonormal basis for the linear subspace
U âŠ‚L2.
(i) The energy âˆ¥uâˆ¥2
2 of every u âˆˆU can be expressed in terms of the d inner
products âŸ¨u, Ï†1âŸ©, . . . , âŸ¨u, Ï†dâŸ©as
âˆ¥uâˆ¥2
2 =
d

â„“=1
âŸ¨u, Ï†â„“âŸ©
2.
(4.43)
(ii) More generally, if v âˆˆL2 (not necessarily in U), then
âˆ¥vâˆ¥2
2 â‰¥
d

â„“=1
âŸ¨v, Ï†â„“âŸ©
2
(4.44)
with equality if, and only if, v is indistinguishable from some signal in U.
(iii) The inner product between any v âˆˆL2 and any u âˆˆU can be expressed in
terms of the inner products {âŸ¨v, Ï†â„“âŸ©} and {âŸ¨u, Ï†â„“âŸ©} as
âŸ¨v, uâŸ©=
d

â„“=1
âŸ¨v, Ï†â„“âŸ©âŸ¨u, Ï†â„“âŸ©âˆ—.
(4.45)
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.6 Orthonormal Bases
43
Proof. Part (i) follows directly from the Pythagorean Theorem (Theorem 4.5.2)
applied to the d-tuple

âŸ¨u, Ï†1âŸ©Ï†1, . . . , âŸ¨u, Ï†dâŸ©Ï†d

.
To prove Part (ii) we expand the energy in v as
âˆ¥vâˆ¥2
2 =


v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“

+
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“

2
2
=
v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“

2
2 +

d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“

2
2
=
v âˆ’
d

â„“=1
âŸ¨v, Ï†â„“âŸ©Ï†â„“

2
2 +
d

â„“=1
âŸ¨v, Ï†â„“âŸ©
2
â‰¥
d

â„“=1
âŸ¨v, Ï†â„“âŸ©
2,
(4.46)
where the ï¬rst equality follows by subtracting and adding the projection of v
onto U; the second from the Pythagorean Theorem and by Lemma 4.6.5, which
guarantees that the diï¬€erence between v and its projection is orthogonal to any
signal in U and hence a fortiori also to the projection itself; the third by Part (i)
applied to the projection of v onto U; and the ï¬nal inequality by the nonnegativity
of energy.
If Inequality (4.46) holds with equality, then the last inequality in its derivation
must hold with equality, so
v âˆ’d
â„“=1 âŸ¨v, Ï†â„“âŸ©Ï†â„“

2 = 0 and hence v must be
indistinguishable from the signal d
â„“=1 âŸ¨v, Ï†â„“âŸ©Ï†â„“, which is in U.
Conversely, if v is indistinguishable from some uâ€² âˆˆU, then
âˆ¥vâˆ¥2
2 = âˆ¥(v âˆ’uâ€²) + uâ€²âˆ¥2
2
= âˆ¥v âˆ’uâ€²âˆ¥2
2 + âˆ¥uâ€²âˆ¥2
2
= âˆ¥uâ€²âˆ¥2
2
=
d

â„“=1
|âŸ¨uâ€², Ï†â„“âŸ©|2
=
d

â„“=1
|âŸ¨v, Ï†â„“âŸ©+ âŸ¨uâ€² âˆ’v, Ï†â„“âŸ©|2
=
d

â„“=1
|âŸ¨v, Ï†â„“âŸ©|2,
where the ï¬rst equality follows by subtracting and adding uâ€²; the second follows
from the Pythagorean Theorem because the fact that âˆ¥v âˆ’uâ€²âˆ¥2 = 0 implies that
âŸ¨v âˆ’uâ€², uâ€²âŸ©= 0 (as can be readily veriï¬ed using the Cauchy-Schwarz Inequality
|âŸ¨v âˆ’uâ€², uâ€²âŸ©| â‰¤âˆ¥v âˆ’uâ€²âˆ¥2 âˆ¥uâ€²âˆ¥2); the third from our assumption that v and uâ€² are
indistinguishable; the fourth from Part (i) applied to the function uâ€² (which is in U);
the ï¬fth by adding and subtracting v; and where the ï¬nal equality follows because
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

44
The Space L2 of Energy-Limited Signals
âŸ¨uâ€² âˆ’v, Ï†â„“âŸ©= 0 (as can be readily veriï¬ed from the Cauchy Schwarz Inequality
|âŸ¨uâ€² âˆ’v, Ï†â„“âŸ©| â‰¤âˆ¥uâ€² âˆ’vâˆ¥2 âˆ¥Ï†â„“âˆ¥2).
To prove Part (iii) we compute âŸ¨v, uâŸ©as
âŸ¨v, uâŸ©=

v,
d

â„“=1
âŸ¨u, Ï†â„“âŸ©Ï†â„“

=
d

â„“=1
âŸ¨u, Ï†â„“âŸ©âˆ—âŸ¨v, Ï†â„“âŸ©,
where the ï¬rst equality holds because u is in U and can therefore be expressed as
in (4.31) of Proposition 4.6.4 (ii); and where the second equality follows from the
sesquilinearity of the inner product ((3.10) and (3.8)).
Proposition 4.6.9 has interesting consequences. It shows that if one thinks of âŸ¨u, Ï†â„“âŸ©
as the â„“-th coordinate of u (with respect to the orthonormal basis (Ï†1, . . . , Ï†d)),
then the energy in u is simply the sum of the squares of the coordinates, and the
inner product between two functions is the sum of the products of each coordinate
of u and the conjugate of the corresponding coordinate of v.
We hope that the properties of orthonormal bases that we presented above have
convinced the reader by now that there are certain advantages to describing func-
tions using an orthonormal basis. A crucial question arises as to whether orthonor-
mal bases always exist. This question is addressed next.
4.6.5
Does an Orthonormal Basis Exist?
Word on the street has it that every ï¬nite-dimensional subspace of L2 has an
orthonormal basis, but this is not true. (It is true for the space L2 that we shall
encounter later.) For example, the set

u âˆˆL2 : u(t) = 0
whenever t Ì¸= 17

of all energy-limited signals that map t to zero whenever t Ì¸= 17 (with the value
to which t = 17 is mapped being unspeciï¬ed) is a one dimensional subspace of L2
that does not have an orthonormal basis. (All the signals in this subspace are of
zero energy, so there are no unit-energy signals in it.)
Proposition 4.6.10. If U is a ï¬nite-dimensional subspace of L2, then the following
two statements are equivalent:
(a) U has an orthonormal basis.
(b) The only element of U of zero energy is the all-zero signal 0.
Proof. The proof has two parts. The ï¬rst consists of showing that (a) â‡’(b), i.e.,
that if U has an orthonormal basis and if u âˆˆU is of zero energy, then u must
be the all-zero signal 0. The second part consists of showing that (b) â‡’(a), i.e.,
that if the only element of zero energy in U is the all-zero signal 0, then U has an
orthonormal basis.
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.6 Orthonormal Bases
45
We begin with the ï¬rst part, namely, (a) â‡’(b). We thus assume that (Ï†1, . . . , Ï†d)
is an orthonormal basis for U and that u âˆˆU satisï¬es âˆ¥uâˆ¥2 = 0 and proceed
to prove that u = 0. We simply note that, by the Cauchy-Schwarz Inequality,
|âŸ¨u, Ï†â„“âŸ©| â‰¤âˆ¥uâˆ¥2 âˆ¥Ï†â„“âˆ¥2 so the condition âˆ¥uâˆ¥2 = 0 implies
âŸ¨u, Ï†â„“âŸ©= 0,
â„“âˆˆ{1, . . . , d},
(4.47)
and hence, by Proposition 4.6.4, that u = 0.
To show (b) â‡’(a) we need to show that if no signal in U other than 0 has zero
energy, then U has an orthonormal basis. The proof is based on the Gram-Schmidt
Procedure, which is presented next. As we shall prove, if the input to this procedure
is a basis for U and if no element of U other than 0 is of energy zero, then the
procedure produces an orthonormal basis for U. The procedure is actually even
more powerful. If it is fed a basis for a subspace that does contain an element other
than 0 of zero-energy, then the procedure produces such an element and halts.
It should be emphasized that the Gram-Schmidt Procedure is not only useful for
proving theorems; it can be quite useful for ï¬nding orthonormal bases for practical
problems.3
4.6.6
The Gram-Schmidt Procedure
The Gram-Schmidt Procedure is named after the mathematicians JÃ¸rgen Pedersen
Gram (1850â€“1916) and Erhard Schmidt (1876â€“1959). However, as pointed out in
(Farebrother, 1988), this procedure was apparently already presented by Pierre-
Simon Laplace (1749â€“1827) and was used by Augustin Louis Cauchy (1789â€“1857).
The input to the Gram-Schmidt Procedure is a basis (u1, . . . , ud) for a d-dimensional
subspace U âŠ‚L2. We assume that d â‰¥1. (The only 0-dimensional subspace of L2
is the subspace {0} containing the all-zero signal only, and for this subspace the
empty tuple is an orthonormal basis; there is not much else to say here.) If U
does not contain a signal of zero energy other than the all-zero signal 0, then the
procedure runs in d steps and produces an orthonormal basis for U (and thus also
proves that U does not contain a zero-energy signal other than 0). Otherwise, the
procedure stops after d or fewer steps and produces an element of U of zero energy
other than 0.
The Gram-Schmidt Procedure:
Step 1: If âˆ¥u1âˆ¥2 = 0, then the procedure declares that there exists a
zero-energy element of U other than 0, it produces u1 as proof, and it
halts. Otherwise, it deï¬nes
Ï†1 =
u1
âˆ¥u1âˆ¥2
and halts with the output (Ï†1) (if d = 1) or proceeds to Step 2 (if
d > 1).
3Numerically, however, it is unstable; see (Golub and van Loan, 1996).
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

46
The Space L2 of Energy-Limited Signals
Assuming that the procedure has run for Î½ âˆ’1 steps without halting
and has deï¬ned the vectors Ï†1, . . . , Ï†Î½âˆ’1, we next describe Step Î½.
Step Î½: Consider the signal
ËœuÎ½ = uÎ½ âˆ’
Î½âˆ’1

â„“=1
âŸ¨uÎ½, Ï†â„“âŸ©Ï†â„“.
(4.48)
If âˆ¥ËœuÎ½âˆ¥2 = 0, then the procedure declares that there exists a zero-
energy element of U other than 0, it produces ËœuÎ½ as proof, and it halts.
Otherwise, the procedure deï¬nes
Ï†Î½ =
ËœuÎ½
âˆ¥ËœuÎ½âˆ¥2
(4.49)
and halts with the output (Ï†1, . . . , Ï†d) (if Î½ is equal to d) or proceeds
to Step Î½ + 1 (if Î½ < d).
We next prove that the procedure behaves as we claim.
Proof. To prove that the procedure behaves as we claim, we shall assume that the
procedure performs Step Î½ (i.e., that it has not halted in the steps preceding Î½)
and prove the following: if at Step Î½ the procedure declares that U contains a
nonzero signal of zero-energy and produces ËœuÎ½ as proof, then this is indeed the
case; otherwise, if it deï¬nes Ï†Î½ as in (4.49), then (Ï†1, . . . , Ï†Î½) is an orthonormal
basis for span(u1, . . . , uÎ½).
We prove this by induction on Î½. For Î½ = 1 this can be veriï¬ed as follows. If
âˆ¥u1âˆ¥2 = 0, then we need to show that u1 âˆˆU and that it is not equal to 0. This
follows from the assumption that the procedureâ€™s input (u1, . . . , ud) forms a basis
for U, so a fortiori the signals u1, . . . , ud must all be elements of U and neither
of them can be the all-zero signal. If âˆ¥u1âˆ¥2 > 0, then Ï†1 is a unit-energy scaled
version of u1 and thus (Ï†1) is an orthonormal basis for span(u1).
We now assume that our claim is true for Î½ âˆ’1 and proceed to prove that it is also
true for Î½. We thus assume that Step Î½ is executed and that (Ï†1, . . . , Ï†Î½âˆ’1) is an
orthonormal basis for span(u1, . . . , uÎ½âˆ’1):
Ï†1, . . . , Ï†Î½âˆ’1 âˆˆU;
(4.50)
span(Ï†1, . . . , Ï†Î½âˆ’1) = span(u1, . . . , uÎ½âˆ’1);
(4.51)
and
âŸ¨Ï†â„“, Ï†â„“â€²âŸ©= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , Î½ âˆ’1}.
(4.52)
We need to prove that if ËœuÎ½ is of zero energy, then it is a nonzero element of U of
zero energy, and that otherwise the Î½-tuple (Ï†1, . . . , Ï†Î½) is an orthonormal basis
for span(u1, . . . , uÎ½). To that end we ï¬rst prove that
ËœuÎ½ âˆˆU
(4.53)
and that
ËœuÎ½ Ì¸= 0.
(4.54)
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.6 Orthonormal Bases
47
We begin with a proof of (4.53). Since (4.48) expresses ËœuÎ½ as a linear combination
of (Ï†1, . . . , Ï†Î½âˆ’1, uÎ½), and since U is by assumption a linear subspace, it suï¬ƒces to
show that Ï†1, . . . , Ï†Î½âˆ’1 âˆˆU and that uÎ½ âˆˆU. The former follows from (4.50) and
the latter from our assumption that (u1, . . . , ud) forms a basis for U.
We next prove (4.54). By (4.48) it suï¬ƒces to show that uÎ½ /âˆˆspan(Ï†1, . . . , Ï†Î½âˆ’1).
By (4.51) this is equivalent to showing that uÎ½ /âˆˆspan(u1, . . . , uÎ½âˆ’1), which fol-
lows from our assumption that (u1, . . . , ud) is a basis for U and a fortiori linearly
independent.
Having established (4.53) and (4.54) it follows that if âˆ¥ËœuÎ½âˆ¥2 = 0, then ËœuÎ½ is a
nonzero element of U which is of zero-energy as we had claimed.
To conclude the proof we now assume âˆ¥ËœuÎ½âˆ¥2 > 0 and prove that (Ï†1, . . . , Ï†Î½) is
an orthonormal basis for span(u1, . . . , uÎ½). That (Ï†1, . . . , Ï†Î½) is orthonormal fol-
lows because (4.52) guarantees that (Ï†1, . . . , Ï†Î½âˆ’1) is orthonormal; because (4.49)
guarantees that Ï†Î½ is of unit energy; and because Lemma 4.6.5 (applied to the lin-
ear subspace span(Ï†1, . . . , Ï†Î½âˆ’1)) guarantees that ËœuÎ½â€”and hence also its scaled
version Ï†Î½â€”is orthogonal to every element of span(Ï†1, . . . , Ï†Î½âˆ’1) and in par-
ticular to Ï†1, . . . , Ï†Î½âˆ’1. It thus only remains to show that span(Ï†1, . . . , Ï†Î½) =
span(u1, . . . , uÎ½). We ï¬rst show that span(Ï†1, . . . , Ï†Î½) âŠ†span(u1, . . . , uÎ½). This
follows because (4.51) implies that
Ï†1, . . . , Ï†Î½âˆ’1 âˆˆspan(u1, . . . , uÎ½âˆ’1);
(4.55)
because (4.55), (4.48) and (4.49) imply that
Ï†Î½ âˆˆspan(u1, . . . , uÎ½);
(4.56)
and because (4.55) and (4.56) imply that Ï†1, . . . , Ï†Î½ âˆˆspan(u1, . . . , uÎ½) and hence
that span(Ï†1, . . . , Ï†Î½) âŠ†span(u1, . . . , uÎ½). The reverse inclusion can be argued
very similarly: by (4.51)
u1, . . . , uÎ½âˆ’1 âˆˆspan(Ï†1, . . . , Ï†Î½âˆ’1);
(4.57)
by (4.48) and (4.49) we can express uÎ½ as a linear combination of (Ï†1, . . . , Ï†Î½)
uÎ½ = âˆ¥ËœuÎ½âˆ¥2 Ï†Î½ +
Î½âˆ’1

â„“=1
âŸ¨uÎ½, Ï†â„“âŸ©Ï†â„“;
(4.58)
and (4.57) & (4.58) combine to prove that u1, . . . , uÎ½ âˆˆspan(Ï†1, . . . , Ï†Î½) and hence
that span(u1, . . . , uÎ½) âŠ†span(Ï†1, . . . , Ï†Î½).
By far the more important scenario for us is when U does not contain a nonzero
element of zero energy. This is because we shall mostly focus on signals that are
bandlimited (see Chapter 6), and the only energy-limited signal that is bandlimited
to W Hz and that has zero-energy is the all-zero signal (Note 6.4.2). For subspaces
not containing zero-energy signals other than 0 the key properties to note about
the signals Ï†1, . . . , Ï†d produced by the Gram-Schmidt procedure are that they
satisfy for each Î½ âˆˆ{1, . . . , d}
span(u1, . . . , uÎ½) = span(Ï†1, . . . , Ï†Î½)
(4.59a)
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

48
The Space L2 of Energy-Limited Signals
and

Ï†1, . . . , Ï†Î½

is an orthonormal basis for span(u1, . . . , uÎ½).
(4.59b)
These properties are, of course, of greatest importance when Î½ = d.
We next provide an example of the Gram-Schmidt procedure.
Example 4.6.11. Consider the following three signals: u1 : t 	â†’I{0 â‰¤t â‰¤1},
u2 : t 	â†’t I{0 â‰¤t â‰¤1}, and u3 : t 	â†’t2 I{0 â‰¤t â‰¤1}. The tuple (u1, u2, u3) forms
a basis for the subspace of all signals of the form t 	â†’p(t) I{0 â‰¤t â‰¤1}, where p(Â·)
is a polynomial of degree smaller than 3. To construct an orthonormal basis for
this subspace with the Gram-Schmidt Procedure, we begin by normalizing u1. To
that end, we compute
âˆ¥u1âˆ¥2
2 =
 âˆ
âˆ’âˆ
|I{0 â‰¤t â‰¤1}|2 dt = 1
and set Ï†1 = u1/ âˆ¥u1âˆ¥2, so
Ï†1 : t 	â†’I{0 â‰¤t â‰¤1}.
(4.60a)
The second function Ï†2 is now obtained by normalizing u2 âˆ’âŸ¨u2, Ï†1âŸ©Ï†1. We ï¬rst
compute the inner product âŸ¨u2, Ï†1âŸ©
âŸ¨u2, Ï†1âŸ©=
 âˆ
âˆ’âˆ
I{0 â‰¤t â‰¤1} t I{0 â‰¤t â‰¤1} dt =
 1
0
t dt = 1
2
to obtain that u2 âˆ’âŸ¨u2, Ï†1âŸ©Ï†1 : t 	â†’(t âˆ’1/2) I{0 â‰¤t â‰¤1}, which is of energy
âˆ¥u2 âˆ’âŸ¨u2, Ï†1âŸ©Ï†1âˆ¥2
2 =
 1
0

t âˆ’1
2
2
dt = 1
12.
Hence,
Ï†2 : t 	â†’
âˆš
12

t âˆ’1
2

I{0 â‰¤t â‰¤1}.
(4.60b)
The third function Ï†3 is the normalized version of u3 âˆ’âŸ¨u3, Ï†1âŸ©Ï†1 âˆ’âŸ¨u3, Ï†2âŸ©Ï†2.
The inner products âŸ¨u3, Ï†1âŸ©and âŸ¨u3, Ï†2âŸ©are respectively
âŸ¨u3, Ï†1âŸ©=
 1
0
t2 dt = 1
3,
âŸ¨u3, Ï†2âŸ©=
 1
0
t2 âˆš
12
	
t âˆ’1
2

dt =
1
âˆš
12.
Consequently
u3 âˆ’âŸ¨u3, Ï†1âŸ©Ï†1 âˆ’âŸ¨u3, Ï†2âŸ©Ï†2 : t 	â†’
	
t2 âˆ’1
3 âˆ’

t âˆ’1
2

I{0 â‰¤t â‰¤1}
with corresponding energy
âˆ¥u3 âˆ’âŸ¨u3, Ï†1âŸ©Ï†1 âˆ’âŸ¨u3, Ï†2âŸ©Ï†2âˆ¥2
2 =
 1
0

t2 âˆ’t + 1
6
2
dt =
1
180.
Hence, the orthonormal basis is completed by the third function
Ï†3 : t 	â†’
âˆš
180

t2 âˆ’t + 1
6

I{0 â‰¤t â‰¤1}.
(4.60c)
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.7 The Space L2
49
4.7
The Space L2
Very informally one can describe the space L2 as the space of all energy-limited
complex-valued signals, where we think of two signals as being diï¬€erent only if they
are distinguishable. This section deï¬nes L2 more precisely. It can be skipped be-
cause we shall have only little to do with L2. Understanding this space is, however,
important for readers who wish to fully understand how the Fourier Transform is
deï¬ned for energy-limited signals that are not integrable (Section 6.2.3). Readers
who continue should recall from Section 2.5 that two energy-limited signals u and v
are said to be indistinguishable if the set {t âˆˆR : u(t) Ì¸= v(t)} is of Lebesgue
measure zero. We write u â‰¡v to indicate that u and v are indistinguishable. By
Proposition 2.5.3, the condition u â‰¡v is equivalent to the condition âˆ¥u âˆ’vâˆ¥2 = 0.
To motivate the deï¬nition of the space L2, we begin by noting that the space L2
of energy-limited signals is â€œalmostâ€ an example of what mathematicians call an
â€œinner product space,â€ but it is not. The problem is that mathematicians insist
that in an inner product space the only vector whose inner product with itself is
zero be the zero vector. This is not the case in L2: it is possible that u âˆˆL2
satisfy âŸ¨u, uâŸ©= 0 (i.e., âˆ¥uâˆ¥2 = 0) and yet not be the all-zero signal 0. From the
condition âˆ¥uâˆ¥2 = 0 we can only infer that u is indistinguishable from 0.
The fact that L2 is not an inner product space is an annoyance because it pre-
cludes us from borrowing from the vast literature on inner product spaces (and
Hilbert spaces, which are special kinds of inner product spaces), and because it
does not allow us to view some of the results about L2 as instances of more gen-
eral principles. For this reason mathematicians prefer to study the space L2, which
is an inner product space (and which is, in fact, a Hilbert space) rather than L2.
Unfortunately, for this luxury they pay a certain price that I am loath to pay.
Consequently, in most of this book I have decided to stick to L2 even though this
precludes me from using the standard results on inner product spaces. The price
one pays for using L2 will become apparent once we deï¬ne it.
To understand how L2 is constructed it is useful to note that the relation â€œu â‰¡vâ€,
i.e., â€œu is indistinguishable from vâ€ is an equivalence relation on L2, i.e., it
satisï¬es
u â‰¡u,
u âˆˆL2;
(reï¬‚exive)

u â‰¡v

â‡â‡’

v â‰¡u

,
u, v âˆˆL2;
(symmetric)
and

u â‰¡v and v â‰¡w

=â‡’

u â‰¡w

,
u, v, w âˆˆL2.
(transitive)
Using these properties one can verify that if for every u âˆˆL2 we deï¬ne its equiv-
alence class [u] as
[u] â‰œ
Ëœu âˆˆL2 : Ëœu â‰¡u},
(4.61)
then two equivalence classes [u] and [v] must be either identical or disjoint. In
fact, the sets [u] âŠ‚L2 and [v] âŠ‚L2 are identical if, and only if, u and v are
indistinguishable

[u] = [v]

â‡â‡’

âˆ¥u âˆ’vâˆ¥2 = 0

,
u, v âˆˆL2,
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

50
The Space L2 of Energy-Limited Signals
and they are disjoint if, and only if, u and v are distinguishable

[u] âˆ©[v] = âˆ…

â‡â‡’

âˆ¥u âˆ’vâˆ¥2 > 0

,
u, v âˆˆL2.
We deï¬ne L2 as the set of all such equivalence classes
L2 â‰œ

[u] : u âˆˆL2}.
(4.62)
Thus, the elements of L2 are not functions, but sets of functions. Each element
of L2 is an equivalence class, i.e., a set of the form [u] for some u âˆˆL2. And for
each u âˆˆL2 the equivalence class [u] is an element of L2.
As we next show, the space L2 can also be viewed as a vector space. To this end
we need to ï¬rst deï¬ne â€œampliï¬cation of an equivalence class by a scalar Î± âˆˆCâ€ and
â€œsuperposition of two equivalence classes.â€ How do we deï¬ne the scaling-by-Î± of
an equivalence class S âˆˆL2? A natural approach is to ï¬nd some function u âˆˆL2
such that S is its equivalence class (i.e., satisfying S = [u]), and to deï¬ne the
scaling-by-Î± of S as the equivalence class of Î± u, i.e., as [Î± u]. Thus we would
deï¬ne Î± S as the equivalence class of the signal t 	â†’Î± u(t). While this turns out to
be a good approach, the careful reader might be concerned by something. Suppose
that S = [u] but that also S = [Ëœu]. Should Î± S be deï¬ned as the equivalence class
of t 	â†’Î± u(t) or of t 	â†’Î± Ëœu(t)? Fortunately, it does not matter because the two
equivalence classes are the same! Indeed, if [u] = [Ëœu], then the equivalence class of
t 	â†’Î± u(t) is equal to the equivalence class of t 	â†’Î± Ëœu(t) (because [u] = [Ëœu] implies
that u and Ëœu agree except on a set of measure zero so Î± u and Î± Ëœu also agree except
on a set of measure zero, which in turn implies that [Î± u] = [Î± Ëœu]).
Similarly, one can show that if S1 âˆˆL2 and S2 âˆˆL2 are two equivalence classes,
then we can deï¬ne their sum (or superposition) S1 + S2 as [u1 + u2] where u1
is any function in L2 such that S1 = [u1] and where u2 is any function in L2
such that S2 = [u2]. Again, to make sure that the result of the superposition of
S1 and S2 does not depend on the choice of u1 and u2 we need to verify that if
S1 = [u1] = [Ëœu1] and if S2 = [u2] = [Ëœu2] then [u1 + u2] = [Ëœu1 + Ëœu2]. This is not
diï¬ƒcult but is omitted.
Using these deï¬nitions and by deï¬ning the zero vector to be the equivalence
class [0], it is not diï¬ƒcult to show that L2 forms a linear space over the com-
plex ï¬eld. To make it into an inner product space we need to deï¬ne the inner
product âŸ¨S1, S2âŸ©between two equivalence classes. If S1 = [u1] and if S2 = [u2]
we deï¬ne the inner product âŸ¨S1, S2âŸ©as the complex number âŸ¨u1, u2âŸ©. Again, we
have to show that our deï¬nition is good in the sense that it does not depend on
the particular choice of u1 and u2. More speciï¬cally, we need to verify that if
S1 = [u1] = [Ëœu1] and if S2 = [u2] = [Ëœu2] then âŸ¨u1, u2âŸ©= âŸ¨Ëœu1, Ëœu2âŸ©. This can be
proved as follows:
âŸ¨u1, u2âŸ©= âŸ¨Ëœu1 + (u1 âˆ’Ëœu1), u2âŸ©
= âŸ¨Ëœu1, u2âŸ©+ âŸ¨u1 âˆ’Ëœu1, u2âŸ©
= âŸ¨Ëœu1, u2âŸ©
= âŸ¨Ëœu1, Ëœu2 + (u2 âˆ’Ëœu2)âŸ©
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.8 Additional Reading
51
= âŸ¨Ëœu1, Ëœu2âŸ©+ âŸ¨Ëœu1, u2 âˆ’Ëœu2âŸ©
= âŸ¨Ëœu1, Ëœu2âŸ©,
where the third equality follows because [u1] = [Ëœu1] implies that âˆ¥u1 âˆ’Ëœu1âˆ¥2 = 0
and hence that âŸ¨u1 âˆ’Ëœu1, u2âŸ©= 0 (Cauchy-Schwarz Inequality), and where the
last equality follows by a similar reasoning about u2 and Ëœu2.
Using the above
deï¬nition of the inner product between equivalence classes one can show that if for
some equivalence class S we have âŸ¨S, SâŸ©= 0, then S is the zero vector, i.e., the
equivalence class [0].
With these deï¬nitions of the scaling of an equivalence class by a scalar, the super-
position of two equivalence classes, and the inner product between two equivalence
classes, the space of equivalence classes L2 becomes an inner product space in the
sense that mathematicians like. In fact, it is a Hilbert space.
What is the price we have to pay for working in an inner product space?
It
is that the elements of L2 are not functions but equivalence classes and that it
is meaningless to talk about the value they take at a given time. For example,
it is meaningless to discuss the supremum (or maximum) of an element of L2.4
To add to the confusion, mathematicians refer to elements of L2 as â€œfunctionsâ€
(even though they are equivalence classes of functions), and they drop the square
brackets. Things get even trickier when one deals with signals contaminated by
noise. If one views the signals as elements of L2, then the result of adding noise to
them is not a stochastic process (Deï¬nition 12.2.1 ahead). We ï¬nd this price too
high, and in this book we shall mostly deal with L2.
4.8
Additional Reading
Most of the results of this chapter follow from basic results on inner product spaces
and can be found, for example, in (Axler, 2015). However, since L2 is not an inner-
product space, we had to introduce some slight modiï¬cations.
More on the deï¬nition of the space L2 can be found in most texts on analysis. See,
for example, (Rudin, 1987, Chapter 3, Remark 3.10) and (Royden and Fitzpatrick,
2010, Section 7.1).
4.9
Exercises
Exercise 4.1 (Subsets that Are not Subspaces).
(i) Provide an example of a set of real energy-limited signals that is closed with respect
to superposition but is not a linear subspace of L2.
(ii) Provide an example of a set of real energy-limited signals that is closed with respect
to ampliï¬cation but is not a linear subspace of L2.
Exercise 4.2 (Linear Subspace). Consider the set of signals u of the form u: t â†’eâˆ’t2p(t),
where p(Â·) is a polynomial whose degree does not exceed d. Is this a linear subspace of L2?
If yes, ï¬nd a basis for this subspace.
4To deal with this, mathematicians deï¬ne the essential supremum.
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

52
The Space L2 of Energy-Limited Signals
Exercise 4.3 (Characterizing Inï¬nite-Dimensional Subspaces). Recall that we say that a
linear subspace is inï¬nite dimensional if it is not of ï¬nite dimension. Show that a linear
subspace U is inï¬nite dimensional if, and only if, there exists a sequence u1, u2, . . . of
elements of U such that for every n âˆˆN the tuple (u1, . . . , un) is linearly independent.
Exercise 4.4 (L2 Is Inï¬nite Dimensional). Show that L2 is inï¬nite dimensional.
Hint: Exercises 4.2 and 4.3 may be useful.
Exercise 4.5 (Separation between Signals). Given u1, u2 âˆˆL2, let V be the set of all
complex signals v that are equidistant to u1 and u2:
V =

v âˆˆL2 : âˆ¥v âˆ’u1âˆ¥2 = âˆ¥v âˆ’u2âˆ¥2

.
(i) Show that
V =

v âˆˆL2 : Re
	
v, u2 âˆ’u1

= âˆ¥u2âˆ¥2
2 âˆ’âˆ¥u1âˆ¥2
2
2

.
(ii) Is V a linear subspace of L2?
(iii) Show that (u1 + u2)/2 âˆˆV.
Exercise 4.6 (Orthogonal Subspace). Given signals v1, . . . , vn âˆˆL2, deï¬ne the set
U =

u âˆˆL2 : âŸ¨u, v1âŸ©= âŸ¨u, v2âŸ©= Â· Â· Â· = âŸ¨u, vnâŸ©= 0

.
Show that U is a linear subspace of L2.
Exercise 4.7 (Projecting a Signal). Let u âˆˆL2 be of positive energy, and let v âˆˆL2 be
arbitrary.
(i) Show that Deï¬nitions 4.6.6 and 4.5.3 agree in the sense that the projection of v
onto span(u) (according to Deï¬nition 4.6.6) is the same as the projection of v onto
the signal u (according to Deï¬nition 4.5.3).
(ii) Show that if the signal u is an element of a ï¬nite-dimensional subspace U having
an orthonormal basis, then the projection of u onto U is u.
Exercise 4.8 (On Projections and Approximations). Let (u1, . . . , un) be an n-tuple in L2,
and let U be its span. Assume that U contains no zero-energy signals other than the all-
zero signal 0. Prove that if v is any signal in L2 and if u is its projection onto U, then
âˆ¥v âˆ’uâˆ¥2 â‰¤âˆ¥v âˆ’u1âˆ¥2. Under what conditions does this hold with equality?
Exercise 4.9 (Computing the Projection). Let (u1, . . . , un) be an n-tuple in L2, and let
U be its span. Assume that U contains no zero-energy signals other than 0.
(i) Prove that the projection of any signal v âˆˆL2 onto U can be computed from
(u1, . . . , un) and the n inner products âŸ¨v, u1âŸ©, . . . , âŸ¨v, unâŸ©.
(ii) Prove that if n
i=1 Î±i ui is the projection of v onto U then
â›
âœ
âœ
âœ
â
âŸ¨u1, u1âŸ©
âŸ¨u2, u1âŸ©
Â· Â· Â·
âŸ¨un, u1âŸ©
âŸ¨u1, u2âŸ©
âŸ¨u2, u2âŸ©
Â· Â· Â·
âŸ¨un, u2âŸ©
...
...
...
...
âŸ¨u1, unâŸ©
âŸ¨u2, unâŸ©
Â· Â· Â·
âŸ¨un, unâŸ©
â
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
â
Î±1
Î±2
...
Î±n
â
âŸ
âŸ
âŸ
â =
â›
âœ
âœ
âœ
â
âŸ¨v, u1âŸ©
âŸ¨v, u2âŸ©
...
âŸ¨v, unâŸ©
â
âŸ
âŸ
âŸ
â .
Does this set of n linear equations in Î±1, . . . , Î±n always have a solution? Can it
have more than one solution?
.006
14:26:19, subject to the Cambridge Core terms of use, available at

4.9 Exercises
53
Exercise 4.10 (Projecting in Two Steps). Let U1 and U2 be ï¬nite-dimensional linear
subspaces of L2 that do not contain signals of zero energy other than the all-zero signal.
Assume that U1 âŠ†U2. Show that if v is any energy-limited signal, then its projection
onto U1 can be computed by ï¬rst projecting v onto U2 and by then projecting the result
onto U1. Is the assumption U1 âŠ†U2 essential?
Exercise 4.11 (Constructing an Orthonormal Basis). Let Ts be a positive constant. Con-
sider the signals s1 : t â†’I{0 â‰¤t â‰¤Ts/2} âˆ’I{Ts/2 < t â‰¤Ts}; s2 : t â†’I{0 â‰¤t â‰¤Ts};
s3 : t â†’I{0 â‰¤t â‰¤Ts/4} + I{3Ts/4 â‰¤t â‰¤Ts}; and s4 : t â†’I{0 â‰¤t â‰¤Ts/4} âˆ’I{3Ts/4 â‰¤
t â‰¤Ts}.
(i) Plot s1, s2, s3, and s4.
(ii) Find an orthonormal basis for span(s1, s2, s3, s4).
(iii) Express each of the signals s1, s2, s3, and s4 as a linear combination of the basis
vectors found in Part (ii).
Exercise 4.12 (Is the L2-Limit Unique?). Show that for signals Î¶, x1, x2, . . . in L2 the
statement
lim
nâ†’âˆâˆ¥xn âˆ’Î¶âˆ¥2 = 0
is equivalent to the statement
	
lim
nâ†’âˆ
xn âˆ’ËœÎ¶

2 = 0

â‡â‡’
	
ËœÎ¶ âˆˆ[Î¶]

.
Exercise 4.13 (Signals of Zero Energy). Given v1, . . . , vn âˆˆL2, show that there exist
integers 1 â‰¤Î½1 < Î½2 < Â· Â· Â· < Î½d â‰¤n such that the following three conditions hold:
the d-tuple

vÎ½1, . . . , vÎ½d

is linearly independent; span(vÎ½1, . . . , vÎ½d) contains no signal
of zero energy other than the all-zero signal 0; and each element of span(v1, . . . , vn) is
indistinguishable from some element of span(vÎ½1, . . . , vÎ½d).
Exercise 4.14 (Orthogonal Subspace). Given v1, . . . , vn âˆˆL2, deï¬ne the set
U =

u âˆˆL2 : âŸ¨u, v1âŸ©= âŸ¨u, v2âŸ©= Â· Â· Â· = âŸ¨u, vnâŸ©= 0

,
and the set of all energy-limited signals that are orthogonal to all the signals in U:
UâŠ¥=
 
w âˆˆL2 :

âŸ¨w, uâŸ©= 0, u âˆˆU
!
.
(i) Show that U âŠ¥is a linear subspace of L2.
(ii) Show that an energy-limited signal is in UâŠ¥if, and only if, it is indistinguishable
from some element of span(v1, . . . , vn).
Hint: For Part (ii) you may ï¬nd Exercise 4.13 useful.
Exercise 4.15 (More on Indistinguishability). Given v1, . . . , vn âˆˆL2 and some w âˆˆL2,
propose an algorithm to check whether there exists an element of span(v1, . . . , vn) that
is indistinguishable from w.
Hint: Exercise 4.13 may be useful.
Exercise 4.16 (Delaying an Equivalence Class). Given some Ï„ âˆˆR and some u âˆˆL2,
the delay-by-Ï„ of the equivalence class [u] âˆˆL2 is deï¬ned as the equivalence class of
the mapping t â†’u(t âˆ’Ï„). Show that the delay-by-Ï„ operator is well-deï¬ned in the sense
that if S = [u] and also S = [Ëœu], then the equivalence class of the mapping t â†’u(t âˆ’Ï„)
is equal to the equivalence class of the mapping t â†’Ëœu(t âˆ’Ï„).
.006
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

Chapter 5
Convolutions and Filters
5.1
Introduction
Convolutions play a central role in the analysis of linear systems, and it is thus
not surprising that they will appear repeatedly in this book. Most of the readers
have probably seen the deï¬nition and key properties in an earlier course on linear
systems, so this chapter can be viewed as a very short review. New perhaps is
the following section on notation and the all-important Section 5.8 on the matched
ï¬lter and its use in calculating inner products.
5.2
Time Shifts and Reï¬‚ections
Suppose that x: R â†’R is a real signal, where we think of the argument as being
time. Such functions are typically plotted on paper with the time arrow pointing
to the right. Take a moment to plot an example of such a function, and on the
same coordinates plot the function
t 	â†’x(t âˆ’t0),
which maps every t âˆˆR to x(t âˆ’t0) for some positive t0. Repeat with t0 being
negative. This may seem like a mindless exercise but there is a point to it. It
will help you understand convolutions graphically and help you visualize mappings
such as t 	â†’
â„“Î±â„“g(t âˆ’â„“Ts), which we will encounter later in our study of Pulse
Amplitude Modulation (PAM). It will also help you visualize the matched ï¬lter.
Given a complex signal x: R â†’C, we denote its reï¬‚ection or mirror image
by ~x, so
~x: t 	â†’x(âˆ’t).
(5.1)
When x is real, the plot of ~x is the mirror image of the plot of x about the vertical
axis. The mirror image of the mirror image of x is always x.
54
.007
14:26:19, subject to the Cambridge Core terms of use, available at

5.3 The Convolution Expression
55
5.3
The Convolution Expression
The convolution x â‹†h between two complex signals x: R â†’C and h: R â†’C is
formally deï¬ned as the complex signal whose time-t value (x â‹†h)(t) is given by
(x â‹†h)(t) =
 âˆ
âˆ’âˆ
x(Ï„) h(t âˆ’Ï„) dÏ„.
(5.2)
Note that the integrand in the above is complex. (Integrals of complex functions
are discussed in Section 2.3.) This deï¬nition also holds for real signals.
We used the term â€œformally deï¬nedâ€ because certain conditions need to be met
for this integral to be deï¬ned. It is conceivable that for some t âˆˆR the integrand
Ï„ 	â†’x(Ï„) h(t âˆ’Ï„) will not be integrable, so the integral will be undeï¬ned. (Recall
that in this book we only allow integrals of the form
 âˆ
âˆ’âˆg(t) dt if the integrand
g(Â·) is in L1 so
 âˆ
âˆ’âˆ|g(t)| dt < âˆ.1) We thus say that x â‹†h is deï¬ned at t âˆˆR if
Ï„ 	â†’x(Ï„) h(t âˆ’Ï„) is integrable.
While (5.2) does not make it apparent, the convolution is in fact symmetric in x
and h. Thus, the integral in (5.2) is deï¬ned for a given t if, and only if, the integral
 âˆ
âˆ’âˆ
h(Ïƒ) x(t âˆ’Ïƒ) dÏƒ
(5.3)
is deï¬ned. And if both are deï¬ned, then their values are identical. This follows
directly by the change of variable Ïƒ â‰œt âˆ’Ï„.
5.4
Thinking About the Convolution
Depending on the application, we can think about the convolution operation in a
number of diï¬€erent ways.
(i) Especially when h(Â·) is nonnegative and integrates to one, one can think of
the convolution as an averaging, or smoothing, operation. Thus, when x is
convolved with h the result at time t0 is not x(t0) but rather a smoothed
version thereof, namely,
 âˆ
âˆ’âˆx(t0 âˆ’Ï„) h(Ï„) dÏ„. For example, if h is the map-
ping t 	â†’I{|t| â‰¤T/2}/T for some T > 0, then the convolution x â‹†h at time
t0 is not x(t0) but rather
1
T
 t0+T/2
t0âˆ’T/2
x(Ï„) dÏ„.
Thus, in this example, we can think of x â‹†h as being a â€œmoving average,â€ or
a â€œsliding-window averageâ€ of x.
(ii) For energy-limited signals it is sometimes beneï¬cial to think about (xâ‹†h)(t0)
as the inner product between the functions Ï„ 	â†’x(Ï„) and Ï„ 	â†’hâˆ—(t0 âˆ’Ï„):
(x â‹†h)(t0) =

Ï„ 	â†’x(Ï„), Ï„ 	â†’hâˆ—(t0 âˆ’Ï„)

.
(5.4)
1On rare occasions we make exceptions for nonnegative integrands whose integral can be +âˆ.
.007
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

56
Convolutions and Filters
(iii) Another useful informal way is to think about x â‹†h as a limit of expressions
of the form

j
(tj+1 âˆ’tj) h(tj) x(t âˆ’tj),
(5.5)
i.e., as a limit of linear combinations of the time shifts of x where the coeï¬ƒ-
cients are determined by h.
5.5
When Is the Convolution Deï¬ned?
There are a number of useful theorems providing suï¬ƒcient conditions for the con-
volutionâ€™s existence. These theorems can be classiï¬ed into two kinds: those that
guarantee that the convolution x â‹†h is deï¬ned at every epoch t âˆˆR and those
that only guarantee that the convolution is deï¬ned for all epochs t outside a set of
Lebesgue measure zero. Both types are useful. We begin with the former.
Convolution deï¬ned for every t âˆˆR:
(i) A particularly simple case where the convolution is deï¬ned at every time
instant t is when both x and h are energy-limited:
x, h âˆˆL2.
(5.6a)
In this case we can use (5.4) and the Cauchy-Schwarz Inequality (Theo-
rem 3.3.1) to conclude that the integral in (5.2) is deï¬ned for every t âˆˆR
and that x â‹†h is a bounded function with
(x â‹†h)(t)
 â‰¤âˆ¥xâˆ¥2 âˆ¥hâˆ¥2 ,
t âˆˆR.
(5.6b)
Indeed,
(x â‹†h)(t)
 =

Ï„ 	â†’x(Ï„), Ï„ 	â†’hâˆ—(t âˆ’Ï„)

â‰¤âˆ¥Ï„ 	â†’x(Ï„)âˆ¥2 âˆ¥Ï„ 	â†’hâˆ—(t âˆ’Ï„)âˆ¥2
= âˆ¥xâˆ¥2 âˆ¥hâˆ¥2 .
In fact, it can be shown that the result of convolving two energy-limited
signals is not only bounded but also uniformly continuous.2 (See, for example,
(Adams and Fournier, 2003, Paragraph 2.23).)
Note that even if both x and h are of ï¬nite energy, the convolution x â‹†h
need not be. However, if x, h are both of ï¬nite energy and if one of them
is additionally also integrable, then the convolution x â‹†h is a ï¬nite-energy
signal. Indeed,
âˆ¥x â‹†hâˆ¥2 â‰¤âˆ¥hâˆ¥1 âˆ¥xâˆ¥2 ,
h âˆˆL1 âˆ©L2,
x âˆˆL2.
(5.7)
For a proof see, for example, (Rudin, 1987, Chapter 8, Exercise 4) or (Stein
and Weiss, 1971, Chapter 1, Section 1, Theorem 1.3).
2A function s: R â†’C is said to be uniformly continuous if for every Ïµ > 0 there corresponds
some positive Î´(Ïµ) such that |s(Î¾â€²) âˆ’s(Î¾â€²â€²)| < Ïµ whenever Î¾â€², Î¾â€²â€² âˆˆR are such that |Î¾â€² âˆ’Î¾â€²â€²| < Î´(Ïµ).
.007
14:26:19, subject to the Cambridge Core terms of use, available at

5.5 When Is the Convolution Deï¬ned?
57
(ii) Another simple case where the convolution is deï¬ned at every epoch t âˆˆR is
when one of the functions is measurable and bounded and when the other is
integrable. For example, if
h âˆˆL1
(5.8a)
and if x is a Lebesgue measurable function that is bounded in the sense that
|x(t)| â‰¤Ïƒâˆ,
t âˆˆR
(5.8b)
for some constant Ïƒâˆ, then for every t âˆˆR the integrand in (5.3) is integrable
because |h(Ïƒ) x(t âˆ’Ïƒ)| â‰¤|h(Ïƒ)| Ïƒâˆ, with the latter being integrable by our
assumption that h is integrable. The result of the convolution is a bounded
function because
|(x â‹†h)(t)| =

 âˆ
âˆ’âˆ
h(Ï„) x(t âˆ’Ï„) dÏ„

â‰¤
 âˆ
âˆ’âˆ
h(Ï„) x(t âˆ’Ï„)
 dÏ„
â‰¤Ïƒâˆâˆ¥hâˆ¥1 ,
t âˆˆR,
(5.8c)
where the ï¬rst inequality follows from Proposition 2.4.1, and where the second
inequality follows from (5.8b).
For this case too one can show that the result of the convolution is not only
bounded but also uniformly continuous.
(iii) Using HÂ¨olderâ€™s Inequality (Theorem 3.3.2), we can generalize the above two
cases to show that whenever x and h satisfy the assumptions of HÂ¨olderâ€™s
Inequality, their convolution is deï¬ned at every epoch t âˆˆR and is, in fact,
a bounded uniformly continuous function.
See, for example, (Adams and
Fournier, 2003, Paragraph 2.23).
(iv) Another important case where the convolution is deï¬ned at every time instant
will be discussed in Proposition 6.2.5. There it is shown that the convolution
between an integrable function (of time) with the Inverse Fourier Transform
of an integrable function (of frequency) is deï¬ned at every time instant and
has a simple representation. This scenario is not as contrived as the reader
might suspect. It arises quite naturally, for example, when discussing the
lowpass ï¬ltering of an integrable signal (Section 6.4.2). The impulse response
of an ideal lowpass ï¬lter (LPF) is not integrable, but it can be represented
as the Inverse Fourier Transform of an integrable function; see (6.35).
Regarding theorems that guarantee that the convolution be deï¬ned for every t
outside a set of Lebesgue measure zero, we mention two.
Convolution deï¬ned for t outside a set of Lebesgue measure zero:
(i) If both x and h are integrable, then one can show (see, for example, (Rudin,
1987, Theorem 8.14), (Katznelson, 2004, Section VI.1), or (Stein and Weiss,
1971, Chapter 1, Section 1, Theorem 1.3)) that, for all t outside a set of
.007
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

58
Convolutions and Filters
Lebesgue measure zero, the mapping Ï„ 	â†’x(Ï„) h(t âˆ’Ï„) is integrable, and
(x â‹†h)(t) is hence deï¬ned. Moreover, irrespective of how we deï¬ne (x â‹†h)(t)
for t inside the set of Lebesgue measure zero
âˆ¥x â‹†hâˆ¥1 â‰¤âˆ¥xâˆ¥1 âˆ¥hâˆ¥1 ,
x, h âˆˆL1.
(5.9)
What is nice about this case is that the result of the convolution stays in
the same class of integrable functions. This makes it meaningful to discuss
associativity and other important properties of the convolution.
(ii) Another case where the convolution is deï¬ned for all t outside a set of
Lebesgue measure zero is when h is integrable and when x is a measur-
able function for which Ï„ 	â†’|x(Ï„)|p is integrable for some 1 â‰¤p < âˆ. In this
case we have (see, for example, (Rudin, 1987, Chapter 8 Exercise 4) or (Stein
and Weiss, 1971, Chapter 1, Section 1, Theorem 1.3)) that for all t outside a
set of Lebesgue measure zero the mapping Ï„ 	â†’x(Ï„) h(t âˆ’Ï„) is integrable so
for such t the convolution (x â‹†h)(t) is well-deï¬ned.3 Moreover, irrespective
of how we deï¬ne (x â‹†h)(t) for t inside the set of Lebesgue measure zero
	 âˆ
âˆ’âˆ
(x â‹†h)(t)
p dt

1/p
â‰¤âˆ¥hâˆ¥1
	 âˆ
âˆ’âˆ
|x(t)|p dt

1/p
.
(5.10)
This inequality is a special case of Youngâ€™s Inequality of Theorem 5.11.1
ahead. It can be written more compactly as
âˆ¥x â‹†hâˆ¥p â‰¤âˆ¥hâˆ¥1 âˆ¥xâˆ¥p ,
p â‰¥1,
(5.11)
where we use the notation that for any measurable function g and p > 0
âˆ¥gâˆ¥p â‰œ
	 âˆ
âˆ’âˆ
|g(t)|p dt

1/p
.
(5.12)
5.6
Basic Properties of the Convolution
The main properties of the convolution are summarized in the following theorem.
Theorem 5.6.1 (Properties of the Convolution). The convolution is
x â‹†h â‰¡h â‹†x,
(commutative)

x â‹†g

â‹†h â‰¡x â‹†

g â‹†h

,
(associative)
x â‹†

g + h

â‰¡x â‹†g + x â‹†h,
(distributive)
and linear in each of its arguments
x â‹†

Î± g + Î² h

â‰¡Î±

x â‹†g

+ Î²

x â‹†h


Î± g + Î² h

â‹†x â‰¡Î±

g â‹†x

+ Î²

h â‹†x

,
where the above hold for all g, h, x âˆˆL1, and Î±, Î² âˆˆC.
Some of these properties hold under more general or diï¬€erent sets of assumptions,
so the reader should focus here on the properties rather than on the restrictions.
3When p = 1 we recover the previous result, namely (i).
.007
14:26:19, subject to the Cambridge Core terms of use, available at

5.7 Filters
59
5.7
Filters
A ï¬lter of impulse response h
is a physical device that when fed the input
waveform x produces the output waveform hâ‹†x. The impulse response h is assumed
to be a real or complex signal, and it is tacitly assumed that we only feed the device
with inputs x for which the convolution x â‹†h is deï¬ned.4
Deï¬nition 5.7.1 (Stable Filter). A ï¬lter is said to be stable if its impulse response
is integrable.
Stable ï¬lters are also called bounded-input/bounded-output stable or BIBO
stable, because, as the next proposition shows, if such ï¬lters are fed a bounded
signal, then their output is also a bounded signal.
Proposition 5.7.2 (BIBO Stability). If h is integrable and if x is a bounded
Lebesgue measurable signal, then the signal x â‹†h is also bounded.
Proof. If the impulse response h is integrable, and if the input x is bounded by
some constant Ïƒâˆ, then (5.8a) and (5.8b) are both satisï¬ed, and the boundedness
of the output then follows from (5.8c).
Deï¬nition 5.7.3 (Causal Filter). A ï¬lter of impulse response h is said to be causal
or nonanticipative if h is zero at negative times, i.e., if
h(t) = 0,
t < 0.
(5.13)
Causal ï¬lters play an important role in engineering because (5.13) guarantees that
the present ï¬lter output be computable from the past ï¬lter inputs. Indeed, the
time-t ï¬lter output can be expressed in the form
(x â‹†h)(t) =
 âˆ
âˆ’âˆ
x(Ï„) h(t âˆ’Ï„) dÏ„
=
 t
âˆ’âˆ
x(Ï„) h(t âˆ’Ï„) dÏ„,
h causal,
where the calculation of the latter integral only requires knowledge of x(Ï„) for
Ï„ < t. Here the ï¬rst equality follows from the deï¬nition of the convolution (5.2),
and the second equality follows from (5.13).
5.8
The Matched Filter
In Digital Communications inner products are often computed using a matched
ï¬lter. In its deï¬nition we shall use the notation (5.1).
4This deï¬nition of a ï¬lter is reminiscent of the concept of a â€œlinear time invariant system.â€
Note, however, that since we do not deal with Diracâ€™s Delta in this book, our deï¬nition is more
restrictive. For example, a device that produces at its output a waveform that is identical to its
input is excluded from our discussion here because we do not allow h to be Diracâ€™s Delta.
.007
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

60
Convolutions and Filters
Deï¬nition 5.8.1 (The Matched Filter). The matched ï¬lter for the signal Ï† is
a ï¬lter whose impulse response is ~Ï†âˆ—, i.e., the mapping
t 	â†’Ï†âˆ—(âˆ’t).
(5.14)
The main use of the matched ï¬lter is for computing inner products:
Theorem 5.8.2 (Computing Inner Products with a Matched Filter). The inner
product âŸ¨u, Ï†âŸ©between the energy-limited signals u and Ï† is given by the output at
time t = 0 of a matched ï¬lter for Ï† that is fed u:
âŸ¨u, Ï†âŸ©=

u â‹†~Ï†âˆ—
(0),
u, Ï† âˆˆL2.
(5.15)
More generally, if g: t 	â†’Ï†(tâˆ’t0), then âŸ¨u, gâŸ©is the time-t0 output of the matched
ï¬lter for Ï† that is fed u:
 âˆ
âˆ’âˆ
u(t) Ï†âˆ—(t âˆ’t0) dt =

u â‹†~Ï†âˆ—
(t0).
(5.16)
Proof. We shall prove the second part of the theorem, i.e., (5.16): the ï¬rst follows
from the second by setting t0 = 0. We express the time-t0 output of the matched
ï¬lter as

u â‹†~Ï†âˆ—
(t0) =
 âˆ
âˆ’âˆ
u(Ï„) ~Ï†âˆ—(t0 âˆ’Ï„) dÏ„
=
 âˆ
âˆ’âˆ
u(Ï„) Ï†âˆ—(Ï„ âˆ’t0) dÏ„,
where the ï¬rst equality follows from the deï¬nition of convolution (5.2) and the
second from the deï¬nition of ~Ï†âˆ—as the conjugated mirror image of Ï†.
From the above theorem we see that if we wish to compute, say, the three inner
products âŸ¨u, g1âŸ©, âŸ¨u, g2âŸ©, and âŸ¨u, g3âŸ©in the very special case where the functions
g1, g2, g3 are all time shifts of the same waveform Ï†, i.e., when g1 : t 	â†’Ï†(t âˆ’t1),
g2 : t 	â†’Ï†(t âˆ’t2), and g3 : t 	â†’Ï†(t âˆ’t3), then we need only one ï¬lter, namely, the
matched ï¬lter for Ï†. Indeed, we can feed u to the matched ï¬lter for Ï† and the
inner products âŸ¨u, g1âŸ©, âŸ¨u, g2âŸ©, and âŸ¨u, g3âŸ©simply correspond to the ï¬lterâ€™s outputs
at times t1, t2, and t3. One circuit computes all three inner products. This is so
exciting that it is worth repeating:
Corollary 5.8.3 (Computing Many Inner Products Using One Filter). If the
energy-limited signals {gj}J
j=1 are all time shifts of the same signal Ï† in the sense
that
gj : t 	â†’Ï†(t âˆ’tj),
j = 1, . . . , J,
and if u is any energy-limited signal, then all J inner products
âŸ¨u, gjâŸ©,
j = 1, . . . , J
.007
14:26:19, subject to the Cambridge Core terms of use, available at

5.9 The Ideal Unit-Gain Lowpass Filter
61
can be computed using one ï¬lter by feeding u to a matched ï¬lter for Ï† and sampling
the output at the appropriate times t1, . . . , tJ:
âŸ¨u, gjâŸ©=

u â‹†~Ï†âˆ—
(tj),
j = 1, . . . , J.
(5.17)
5.9
The Ideal Unit-Gain Lowpass Filter
The impulse response of the ideal unit-gain lowpass ï¬lter of cutoï¬€frequency Wc
is denoted by LPFWc(Â·) and is given for every Wc > 0 by5
LPFWc(t) â‰œ

2Wc
sin(2Ï€Wct)
2Ï€Wct
if t Ì¸= 0,
2Wc
if t = 0,
t âˆˆR.
(5.18)
This can be alternatively written as
LPFWc(t) = 2Wc sinc(2Wct),
t âˆˆR,
(5.19)
where the function sinc(Â·) is deï¬ned by6
sinc(Î¾) â‰œ
 sin(Ï€Î¾)
Ï€Î¾
if Î¾ Ì¸= 0,
1
if Î¾ = 0,
Î¾ âˆˆR.
(5.20)
Notice that the deï¬nition of sinc(0) as being 1 makes sense because, for very small
(but nonzero) values of Î¾ the value of sin(Î¾)/Î¾ is approximately 1. In fact, with
this deï¬nition at zero the function is not only continuous at zero but also inï¬nitely
diï¬€erentiable there. Indeed, the function from C to C
z 	â†’
 sin(Ï€z)
Ï€z
if z Ì¸= 0,
1
otherwise,
is an entire function, i.e., an analytic function throughout the complex plane.
The importance of the ideal unit-gain lowpass ï¬lter will become clearer when we
discuss the ï¬lterâ€™s frequency response in Section 6.3. It is thus named because
the Fourier Transform of LPFWc(Â·) is equal to 1 (hence â€œunit gainâ€), whenever
|f| â‰¤Wc, and is equal to zero, whenever |f| > Wc. See (6.38) ahead.
From a mathematical point of view, working with the ideal unit-gain lowpass ï¬lter
is tricky because the impulse response (5.18) is not an integrable function. (It
decays like 1/t, which does not have a ï¬nite integral from t = 1 to t = âˆ.) This
ï¬lter is thus not a stable ï¬lter. We shall revisit this issue in Section 6.4. Note,
however, that the impulse response (5.18) is of ï¬nite energy. (The square of the
impulse response decays like 1/t2 which does have a ï¬nite integral from one to
inï¬nity.) Consequently, the result of feeding an energy-limited signal to the ideal
unit-gain lowpass ï¬lter is always well-deï¬ned.
Note also that the ideal unit-gain lowpass ï¬lter is not causal.
5For convenience we deï¬ne the impulse response of the ideal unit-gain lowpass ï¬lter of cutoï¬€
frequency zero as the all zero signal. This is in agreement with (5.19).
6Some texts omit the Ï€â€™s in (5.20) and deï¬ne the sinc(Â·) function as sin(Î¾)/Î¾ for Î¾ Ì¸= 0.
.007
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

62
Convolutions and Filters
5.10
The Ideal Unit-Gain Bandpass Filter
The ideal unit-gain bandpass ï¬lter (BPF) of bandwidth W around the carrier
frequency fc, where fc > W/2 > 0 is a ï¬lter of impulse response BPFW,fc(Â·),
where
BPFW,fc(t) â‰œ2W cos(2Ï€fct) sinc(Wt),
t âˆˆR.
(5.21)
This ï¬lter too is nonstable and noncausal. It derives its name from its frequency
response (discussed in Section 6.3 ahead), which is equal to one at frequencies f
satisfying
|f| âˆ’fc
 â‰¤W/2 and which is equal to zero at all other frequencies.
5.11
Youngâ€™s Inequality
Many of the inequalities regarding convolutions are special cases of a result known
as Youngâ€™s Inequality. Recalling (5.12), we can state Youngâ€™s Inequality as follows.
Theorem 5.11.1 (Youngâ€™s Inequality). Let x and h be Lebesgue measurable func-
tions such that âˆ¥xâˆ¥p , âˆ¥hâˆ¥q < âˆfor some 1 â‰¤p, q < âˆsatisfying 1/p + 1/q > 1.
Deï¬ne r through 1/p + 1/q = 1 + 1/r. Then the convolution integral (5.2) is de-
ï¬ned for all t outside a set of Lebesgue measure zero; it is a Lebesgue measurable
function; and
âˆ¥x â‹†hâˆ¥r â‰¤K âˆ¥xâˆ¥p âˆ¥hâˆ¥q ,
(5.22)
where K < 1 is some constant that depends only on p and q.
Proof. See (Adams and Fournier, 2003, Corollary 2.25). Alternatively, see (Stein
and Weiss, 1971, Chapter 5, Section 1) where it is derived from the M. Riesz
Convexity Theorem.
5.12
Additional Reading
For some of the properties of the convolution and its use in the analysis of linear
systems see (Oppenheim and Willsky, 1997) and (Kwakernaak and Sivan, 1991).
5.13
Exercises
Exercise 5.1 (Convolving Brickwall Functions). For a given a > 0, compute the convolu-
tion of the signal t â†’I{|t| â‰¤a} with itself.
Exercise 5.2 (Convolution of Delayed Signals). Let x and h be energy-limited signals.
Let xd : t â†’x(t âˆ’td) be the result of delaying x by some td âˆˆR. Show that

xd â‹†h

(t) =

x â‹†h

(t âˆ’td),
t âˆˆR.
Exercise 5.3 (The Convolution of Reï¬‚ections). Let the signals x, y be such that their
convolution (x â‹†y)(t) is deï¬ned at every t âˆˆR.
Show that the convolution of their
reï¬‚ections is also deï¬ned at every t âˆˆR and that it is equal to the reï¬‚ection of their
convolution:
~x â‹†~y

(t) =

x â‹†y

(âˆ’t),
t âˆˆR.
.007
14:26:19, subject to the Cambridge Core terms of use, available at

5.13 Exercises
63
Exercise 5.4 (The Convolution and Inner Products). Let y and Ï† be energy-limited
complex signals, and let h be an integrable complex signal. Argue that

y, h â‹†Ï†

=

y â‹†~hâˆ—, Ï†

.
Exercise 5.5 (The Real and Imaginary Parts of a Convolution). Let the signals x and h
be energy-limited, and assume that h is real. Prove that
Re

x â‹†h

= Re(x) â‹†h and Im

x â‹†h

= Im(x) â‹†h,
h is real-valued.
Is the assumption that h is real essential?
Exercise 5.6 (The Convolution in Probability Theory).
(i) Let X and Y be independent random variables of probability density functions
fX(Â·) and fY (Â·).
Show that if Z = X + Y then the density fZ(Â·) of Z is the
convolution of fX(Â·) and fY (Â·).
(ii) Convolve the mean-2 exponential density x â†’1/2 eâˆ’x/2 I{x â‰¥0} with itself to
obtain the density of the central Ï‡2 distribution with four degrees of freedom. (See
Section 19.8.1 for more on the central Ï‡2 distribution.)
Exercise 5.7 (The Convolutionâ€™s Derivative). Let the signal g: R â†’C be bounded,
diï¬€erentiable, and with a bounded derivative gâ€². Let h: R â†’C be integrable. Show that
g â‹†h is diï¬€erentiable and that its derivative (g â‹†h)â€² is gâ€² â‹†h.
Exercise 5.8 (Continuity of the Convolution). Show that if the signals x and y are both
in L2 then their convolution is a continuous function.
Hint: Use the Cauchy-Schwarz Inequality and the fact that if x âˆˆL2 and if we deï¬ne
xÎ´ : t â†’x(t âˆ’Î´), then lim
Î´â†’0 âˆ¥x âˆ’xÎ´âˆ¥2 = 0.
Exercise 5.9 (More on the Continuity of the Convolution). Let x and y be in L2. Let the
sequence of energy-limited signals x1, x2, . . . converge to x in the sense that âˆ¥x âˆ’xnâˆ¥2
tends to zero as n tends to inï¬nity. Show that at every epoch t âˆˆR,
lim
nâ†’âˆ

xn â‹†y

(t) =

x â‹†y

(t).
Hint: Use the Cauchy-Schwarz Inequality
Exercise 5.10 (Convolving Bi-Inï¬nite Sequences). The convolution of the bi-inï¬nite se-
quence . . . , aâˆ’1, a0, a1 . . . with the bi-inï¬nite sequence . . . , bâˆ’1, b0, b1 . . . is the bi-inï¬nite
sequence . . . , câˆ’1, c0, c1 . . . formally deï¬ned by
cm =
âˆ

Î½=âˆ’âˆ
aÎ½bmâˆ’Î½,
m âˆˆZ.
(5.23)
Show that if
âˆ

Î½=âˆ’âˆ
|aÎ½| ,
âˆ

Î½=âˆ’âˆ
|bÎ½| < âˆ,
then the sum on the RHS of (5.23) converges for every integer m, and
âˆ

m=âˆ’âˆ
|cm| â‰¤

âˆ

Î½=âˆ’âˆ
|aÎ½|

âˆ

Î½=âˆ’âˆ
|bÎ½|

.
Hint: Recall Problems 3.14 & 3.12 and the Triangle Inequality for Complex Numbers.
.007
14:26:19, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

64
Convolutions and Filters
Exercise 5.11 (Periodic Signals through Stable Filters). Prove that the output of a stable
ï¬lter that is fed a bounded periodic signal is bounded and periodic.
Exercise 5.12 (Stability of the Matched Filter). Let g be an energy-limited signal. Under
what conditions is the matched ï¬lter for g stable?
Exercise 5.13 (Causality of the Matched Filter). Let g be an energy-limited signal.
(i) Under what conditions is the matched ï¬lter for g causal?
(ii) Under what conditions can you ï¬nd a causal ï¬lter of impulse response h and a
sampling time t0 such that

r â‹†h

(t0) = âŸ¨r, gâŸ©,
r âˆˆL2?
(iii) Show that for every Î´ > 0 we can ï¬nd a stable causal ï¬lter of impulse response h
and a sampling epoch t0 such that for every r âˆˆL2

r â‹†h

(t0) âˆ’âŸ¨r, gâŸ©
 â‰¤Î´ âˆ¥râˆ¥2 .
Exercise 5.14 (The Output of the Matched Filter). Compute and plot the output of the
matched ï¬lter for the signal t â†’eâˆ’t I{t â‰¥0} when it is fed the input t â†’I{|t| â‰¤1/2}.
.007
14:26:19, subject to the Cambridge Core terms of use, available at

Chapter 6
The Frequency Response of Filters and
Bandlimited Signals
6.1
Introduction
We begin this chapter with a review of the Fourier Transform and its key properties.
We then use these properties to deï¬ne the frequency response of ï¬lters, to discuss
the ideal unit-gain lowpass ï¬lter, and to deï¬ne bandlimited signals.
6.2
Review of the Fourier Transform
6.2.1
On Hats, 2Ï€â€™s, Ï‰â€™s, and fâ€™s
We denote the Fourier Transform (FT) of a (possibly complex) signal x(Â·) by
Ë†x(Â·). Some other books denote it by X(Â·), but we prefer our notation because,
where possible, we use lowercase letters for deterministic quantities and reserve
uppercase letters for random quantities. In places where convention forces us to
use uppercase letters for deterministic quantities, we try to use a special font, e.g.,
P for power, W for bandwidth, or A for a deterministic matrix.
More importantly, our deï¬nition of the Fourier Transform may be diï¬€erent from
the one you are used to.
Deï¬nition 6.2.1 (Fourier Transform). The Fourier Transform (or the L1-
Fourier Transform) of an integrable signal x: R â†’C is the mapping Ë†x: R â†’C
deï¬ned by
Ë†x: f 	â†’
 âˆ
âˆ’âˆ
x(t) eâˆ’i2Ï€ft dt.
(6.1)
(The FT can also be deï¬ned in more general settings. For example, in Section 6.2.3
it will be deï¬ned via a limiting argument for ï¬nite-energy signals that are not
integrable.)
This deï¬nition should be contrasted with the deï¬nition
X(iÏ‰) =
 âˆ
âˆ’âˆ
x(t) eâˆ’iÏ‰t dt,
(6.2)
65
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

66
The Frequency Response of Filters and Bandlimited Signals
which you may have seen before. Note the 2Ï€, which appears in the exponent in
our deï¬nition (6.1) and not in (6.2). We apologize to readers who are used to (6.2)
for forcing a new deï¬nition, but we have some good reasons:
(i) With our deï¬nition, the transform and its inverse are very similar; see (6.1)
and (6.4) below. If one uses the deï¬nition of (6.2), then the expression for
the Inverse Fourier Transform requires scaling the integral by 1/(2Ï€).
(ii) With our deï¬nition, the Fourier Transform and the Inverse Fourier Transform
of a symmetric function are the same; see (6.6).
This makes it easier to
remember some Fourier pairs.
(iii) As we shall state more precisely in Section 6.2.2 and Section 6.2.3, with our
deï¬nition the Fourier Transform possesses an extremely important property:
it preserves inner products
âŸ¨u, vâŸ©= âŸ¨Ë†u, Ë†vâŸ©
(certain restrictions apply).
Again, no 2Ï€â€™s.
(iv) If x(Â·) models a function of time, then Ë†x(Â·) becomes a function of frequency.
Thus, it is natural to use the generic argument t for such signals x(Â·) and the
generic argument f for their transforms. It is more common these days to
describe tones in terms of their frequencies (i.e., in Hz) and not in terms of
their radial frequency (in radians per second).
(v) It seems that all books on communications use our deï¬nition, perhaps because
people are used to setting their radios in Hz, kHz, or MHz.
Plotting the FT of a signal is tricky, because it is a complex-valued function. This
is generally true even for real signals.
However, for any integrable real signal
x: R â†’R the Fourier Transform Ë†x(Â·) is conjugate-symmetric, i.e.,

Ë†x(âˆ’f) = Ë†xâˆ—(f),
f âˆˆR

,
x âˆˆL1 is real-valued.
(6.3)
Equivalently, the magnitude of the FT of an integrable real signal is symmetric, and
the argument is anti-symmetric.1 (The reverse statement is â€œessentiallyâ€ correct.
If Ë†x is conjugate-symmetric then the set of epochs t for which x(t) is not real is
of Lebesgue measure zero.) Consequently, when plotting the FT of a â€œgenericâ€
real signal we shall plot a symmetric function, but with solid lines for the positive
frequencies and dashed lines for the negative frequencies. This is to remind the
reader that the FT of a real signal is not symmetric but conjugate-symmetric. See,
for example, Figures 7.1 and 7.2 for plots of the Fourier Transforms of real signals.
When plotting the FT of a complex-valued signal, we shall use a generic plot that
is â€œhighly asymmetric,â€ using solid lines. See, for example, Figure 7.4 for the FT
of a complex signal.
1The argument of a nonzero complex number z is deï¬ned as the element Î¸ of [âˆ’Ï€, Ï€) such
that z = |z| eiÎ¸.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.2 Review of the Fourier Transform
67
Deï¬nition 6.2.2 (Inverse Fourier Transform). The Inverse Fourier Transform
(IFT) of an integrable function g: R â†’C is denoted by Ë‡g and is deï¬ned by
Ë‡g: t 	â†’
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df.
(6.4)
We emphasize that the word â€œinverseâ€ here is just part of the name of the transform.
Applying the IFT to the FT of a signal does not always recover the signal.2 (Condi-
tions under which the IFT does recover the signal are explored in Theorem 6.2.13.)
However, if one does not insist on using the IFT, then every integrable signal can
be reconstructed to within indistinguishability from its FT; see Theorem 6.2.12.
Proposition 6.2.3 (Some Properties of the Inverse Fourier Transform).
(i) If g is integrable, then its IFT is the FT of its mirror image
Ë‡g = Ë†~g,
g âˆˆL1.
(6.5)
(ii) If g is integrable and also symmetric in the sense that ~g = g, then the IFT
of g is equal to its FT
Ë†g = Ë‡g,

g âˆˆL1 and ~g = g

.
(6.6)
(iii) If g is integrable and Ë‡g is also integrable, then
Ë†Ë‡g = Ë‡Ë†g.
(6.7)
Proof. Part (i) follows by a simple change of integration variable:
Ë‡g(Î¾) =
 âˆ
âˆ’âˆ
g(Î±) ei2Ï€Î±Î¾ dÎ± = âˆ’
 âˆ’âˆ
âˆ
g(âˆ’Î²) eâˆ’i2Ï€Î²Î¾ dÎ²
=
 âˆ
âˆ’âˆ
~g(Î²) eâˆ’i2Ï€Î²Î¾ dÎ²
= Ë†~g(Î¾),
Î¾ âˆˆR,
where we have changed the integration variable to Î² â‰œâˆ’Î±.
Part (ii) is a special case of Part (i). To prove Part (iii) we compute
Ë†Ë‡g(Î¾) =
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df

eâˆ’i2Ï€Î¾t dt
=
 âˆ
âˆ’âˆ
Ë†g(âˆ’t) eâˆ’i2Ï€Î¾t dt
=
 âˆ
âˆ’âˆ
Ë†g(Ï„) ei2Ï€Î¾Ï„ dÏ„
= Ë‡Ë†g(Î¾),
Î¾ âˆˆR,
2This can be seen by considering the signal t â†’I{t = 17}, which is zero everywhere except
at 17 where it takes on the value 1. Its FT is zero at all frequencies, but if one applies the IFT to
the all-zero function one obtains the all-zero function, which is not the function we started with.
Things could be much worse. The FT of some integrable signals (such as the signal t â†’I{|t| â‰¤1})
is not integrable, so the IFT of their FT is not even deï¬ned.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

68
The Frequency Response of Filters and Bandlimited Signals
where we have changed the integration variable to Ï„ â‰œâˆ’t.
Identity (6.6) will be useful in Section 6.2.5 when we memorize the FT of the
Brickwall function Î¾ 	â†’Î² I{|Î¾| â‰¤Î³}, which is symmetric. Once we succeed we will
also know its IFT.
Table 6.1 summarizes some of the properties of the FT. Note that some of these
properties require additional technical assumptions.
Property
Function
Fourier Transform
linearity
Î± x + Î² y
Î± Ë†x + Î² Ë†y
time shifting
t 	â†’x(t âˆ’t0)
f 	â†’eâˆ’i2Ï€ft0 Ë†x(f)
frequency shifting
t 	â†’ei2Ï€f0t x(t)
f 	â†’Ë†x(f âˆ’f0)
conjugation
t 	â†’xâˆ—(t)
f 	â†’Ë†xâˆ—(âˆ’f)
stretching (Î± âˆˆR, Î± Ì¸= 0)
t 	â†’x(Î±t)
f 	â†’
1
|Î±| Ë†x( f
Î±)
convolution in time
x â‹†y
f 	â†’Ë†x(f) Ë†y(f)
multiplication in time
t 	â†’x(t) y(t)
Ë†x â‹†Ë†y
real part
t 	â†’Re

x(t)

f 	â†’1
2 Ë†x(f) + 1
2 Ë†xâˆ—(âˆ’f)
time reï¬‚ection
~x
Ë‡x
transforming twice
Ë†x
~x
FT of IFT
Ë‡x
x
Table 6.1: Basic properties of the Fourier Transform. Some restrictions apply!
6.2.2
Parseval-like Theorems
A key result on the Fourier Transform is that, subject to some restrictions, it pre-
serves inner products. Thus, if Ë†x1 and Ë†x2 are the Fourier Transforms of x1 and x2,
then the inner product âŸ¨x1, x2âŸ©between x1 and x2 is typically equal to the inner
product âŸ¨Ë†x1, Ë†x2âŸ©between their transforms. In this section we shall describe two
scenarios where this holds. A third scenario, which is described in Theorem 6.2.9,
will have to wait until we discuss the FT of signals that are energy-limited but not
integrable.
To see how the next proposition is related to the preservation of the inner product
under the Fourier Transform, think about g as being a function of frequency and
of its IFT Ë‡g as a function of time.
Proposition 6.2.4. If g: f 	â†’g(f) and x: t 	â†’x(t) are integrable mappings from R
to C, then
 âˆ
âˆ’âˆ
x(t) Ë‡gâˆ—(t) dt =
 âˆ
âˆ’âˆ
Ë†x(f) gâˆ—(f) df,
(6.8)
i.e.,
âŸ¨x, Ë‡gâŸ©= âŸ¨Ë†x, gâŸ©,
g, x âˆˆL1.
(6.9)
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.2 Review of the Fourier Transform
69
Proof. The key to the proof is to use Fubiniâ€™s Theorem to justify changing the
order of integration in the following calculation:
 âˆ
âˆ’âˆ
x(t) Ë‡gâˆ—(t) dt =
 âˆ
âˆ’âˆ
x(t)
	 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df

âˆ—
dt
=
 âˆ
âˆ’âˆ
x(t)
 âˆ
âˆ’âˆ
gâˆ—(f) eâˆ’i2Ï€ft df dt
=
 âˆ
âˆ’âˆ
gâˆ—(f)
 âˆ
âˆ’âˆ
x(t) eâˆ’i2Ï€ft dt df
=
 âˆ
âˆ’âˆ
gâˆ—(f) Ë†x(f) df,
where the ï¬rst equality follows from the deï¬nition of Ë‡g; the second because the
conjugation of an integral is accomplished by conjugating the integrand (Proposi-
tion 2.3.1); the third by changing the order of integration; and the ï¬nal equality
by the deï¬nition of the FT of x.
A related result is that the convolution of an integrable function with the IFT of
an integrable function is always deï¬ned:
Proposition 6.2.5. If the mappings x: t 	â†’x(t) and g: f 	â†’g(f) from R to C are
both integrable, then the convolution x â‹†Ë‡g is deï¬ned at every epoch t âˆˆR and

x â‹†Ë‡g

(t) =
 âˆ
âˆ’âˆ
g(f) Ë†x(f) ei2Ï€ft df,
t âˆˆR.
(6.10)
Proof. Here too the key is in changing the order of integration:

x â‹†Ë‡g

(t) =
 âˆ
âˆ’âˆ
x(Ï„) Ë‡g(t âˆ’Ï„) dÏ„
=
 âˆ
âˆ’âˆ
x(Ï„)
 âˆ
âˆ’âˆ
ei2Ï€f(tâˆ’Ï„)g(f) df dÏ„
=
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft
 âˆ
âˆ’âˆ
x(Ï„) eâˆ’i2Ï€fÏ„ dÏ„ df
=
 âˆ
âˆ’âˆ
g(f) Ë†x(f) ei2Ï€ft df,
where the ï¬rst equality follows from the deï¬nition of the convolution; the second
from the deï¬nition of the IFT; the third by changing the order of integration; and
the ï¬nal equality by the deï¬nition of the FT. The justiï¬cation of the changing of the
order of integration can be argued using Fubiniâ€™s Theorem because, by assumption,
both g and x are integrable.
We next present another useful version of the preservation of inner products under
the FT. It is useful for functions (of time) that are zero outside some interval
[âˆ’T, T ] or for the IFT of functions (of frequency) that are zero outside an interval
[âˆ’W, W ].
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

70
The Frequency Response of Filters and Bandlimited Signals
Proposition 6.2.6 (A Mini Parseval Theorem).
(i) Let the signals x1 and x2 be given by
xÎ½(t) =
 âˆ
âˆ’âˆ
gÎ½(f) ei2Ï€ft df,

t âˆˆR, Î½ = 1, 2

,
(6.11a)
where the functions gÎ½ : f 	â†’gÎ½(f) satisfy
gÎ½(f) = 0,

|f| > W, Î½ = 1, 2

,
(6.11b)
for some W â‰¥0, and
 âˆ
âˆ’âˆ
|gÎ½(f)|2 df < âˆ,
Î½ = 1, 2.
(6.11c)
Then
âŸ¨x1, x2âŸ©= âŸ¨g1, g2âŸ©.
(6.11d)
(ii) Let g1 and g2 be given by
gÎ½(f) =
 âˆ
âˆ’âˆ
xÎ½(t) eâˆ’i2Ï€ft dt,

f âˆˆR, Î½ = 1, 2

,
(6.12a)
where the signals x1, x2 âˆˆL2 are such that for some T â‰¥0
xÎ½(t) = 0,

|t| > T, Î½ = 1, 2

.
(6.12b)
Then
âŸ¨x1, x2âŸ©= âŸ¨g1, g2âŸ©.
(6.12c)
Proof. See the proof of Lemma A.3.6 on Page 837 and its corollary in the appendix.
6.2.3
The L2-Fourier Transform
To appreciate some of the mathematical subtleties of this section, the reader is
encouraged to review Section 4.7 in order to recall the diï¬€erence between the
space L2 and the space L2 and in order to recall the diï¬€erence between an energy-
limited signal x âˆˆL2 and the equivalence class [x] âˆˆL2 to which it belongs. In this
section we shall sketch how the Fourier Transform is deï¬ned for elements of L2.
This section can be skipped provided that you are willing to take on faith that
such a transform exists and that, very roughly speaking, it has some of the same
properties of the Fourier Transform of Deï¬nition 6.2.1. To diï¬€erentiate between
the transform of Deï¬nition 6.2.1 and the transform that we are about to deï¬ne
for elements of L2, we shall refer in this section to the former as the L1-Fourier
Transform and to the latter as the L2-Fourier Transform. Both will be denoted
by a â€œhat.â€ In subsequent sections the Fourier Transform will be understood to be
the L1-Fourier Transform unless explicitly otherwise speciï¬ed.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.2 Review of the Fourier Transform
71
Some readers may have already encountered the L2-Fourier Transform without
even being aware of it. For example, the sinc(Â·) function, which is deï¬ned in (5.20),
is an energy-limited signal that is not integrable.
Consequently, its L1-Fourier
Transform is undeï¬ned. Nevertheless, you may have seen its Fourier Transform
being given as the Brickwall function. As we shall see, this is somewhat in line
with how the L2-Fourier Transform of the sinc(Â·) is deï¬ned.3
For more on the
Fourier Transform of the sinc(Â·) see Section 6.2.5. Another example of an energy-
limited signal that is not integrable is t 	â†’1/(1 + |t|).
We next sketch how the L2-Fourier Transform is deï¬ned and explore some of its
key properties. We begin with the bad news.
(i) There is no explicit simple expression for the L2-Fourier Transform.
(ii) The result of applying the transform is not a function but an equivalence
class of functions.
The L2-Fourier Transform is a mapping
Ë†: L2 â†’L2
that maps elements of L2 to elements of L2. It thus maps equivalence classes
to equivalence classes, not functions.
As long as the operation we perform on
the result of the L2-Fourier Transform does not depend on which member of the
equivalence class it is performed on, there is no need to worry about this issue.
Otherwise, we can end up performing operations that are ill-deï¬ned. For example,
an operation that is ill-deï¬ned is evaluating the result of the transform at a given
frequency, say at f = 17.
An operation you cannot go wrong with is integration, because the integrals of
two functions that diï¬€er on a set of measure zero are equal; see Proposition 2.5.3.
Consequently, inner products, which are deï¬ned via integration, are ï¬ne too. In
this book we shall therefore refrain from applying to the result of the L2-Fourier
Transform any operation other than integration (or related operations such as the
computation of energy or inner product).
In fact, since we ï¬nd the notion of
equivalence classes somewhat abstract we shall try to minimize its use.
Suppose that x âˆˆL2 is an energy-limited signal and that [x] âˆˆL2 is its equivalence
class. How do we deï¬ne the L2-Fourier Transform of [x]? We ï¬rst deï¬ne for every
positive integer n the time-truncated function
xn : t 	â†’x(t) I{|t| â‰¤n}
and note that, by Proposition 3.4.3, xn is integrable. Consequently, its L1-Fourier
Transform Ë†xn is well-deï¬ned and is given by
Ë†xn(f) =
 n
âˆ’n
x(t) eâˆ’i2Ï€ft dt,
f âˆˆR.
3However, as we shall see, the result of the L2 -Fourier Transform is an element of L2 , i.e., an
equivalence class, and not a function.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

72
The Frequency Response of Filters and Bandlimited Signals
We then note that âˆ¥x âˆ’xnâˆ¥2 tends to zero as n tends to inï¬nity, so for every Ïµ > 0
there exists some L(Ïµ) suï¬ƒciently large so that
âˆ¥xn âˆ’xmâˆ¥2 < Ïµ,
n, m > L(Ïµ).
(6.13)
Applying Proposition 6.2.6 (ii) with the substitution of max{n, m} for T and of
xn âˆ’xm for both x1 and x2, we obtain that (6.13) implies
âˆ¥Ë†xn âˆ’Ë†xmâˆ¥2 < Ïµ,
n, m > L(Ïµ).
(6.14)
Because the space of energy-limited signals is complete in the sense of Theo-
rem 8.6.1 ahead, we may infer from (6.14) that there exists some function Î¶ âˆˆL2
such that âˆ¥Ë†xn âˆ’Î¶âˆ¥2 converges to zero.4 We then deï¬ne the L2-Fourier Transform
of the equivalence class [x] to be the equivalence class [Î¶]. In view of Footnote 4
we can deï¬ne the L2-Fourier Transform as follows.
Deï¬nition 6.2.7 (L2-Fourier Transform). The L2-Fourier Transform of the
equivalence class [x] âˆˆL2 is denoted by  
[x] and is given by
 
[x] â‰œ

g âˆˆL2 : lim
nâ†’âˆ
 âˆ
âˆ’âˆ
g(f) âˆ’
 n
âˆ’n
x(t) eâˆ’i2Ï€ft dt

2
df = 0
!
.
The main properties of the L2-Fourier Transform are summarized in the following
theorem.
Theorem 6.2.8 (Properties of the L2-Fourier Transform). The L2-Fourier Trans-
form is a mapping from L2 onto L2 with the following properties:
(i) If x âˆˆL2 âˆ©L1, then the L2-Fourier Transform of [x] is the equivalence class
of the mapping
f 	â†’
 âˆ
âˆ’âˆ
x(t) eâˆ’i2Ï€ft dt.
(ii) The L2-Fourier Transform is linear in the sense that

Î±[x1] + Î²[x2] = Î± "
[x1] + Î² "
[x2],

x1, x2 âˆˆL2,
Î±, Î² âˆˆC

.
(iii) The L2-Fourier Transform is invertible in the sense that to each [g] âˆˆL2
there corresponds a unique equivalence class in L2 whose L2-Fourier Trans-
form is [g]. This equivalence class can be obtained by reï¬‚ecting each of the el-
ements of [g] to obtain the equivalence class [~g] of ~g, and by then applying the
L2-Fourier Transform to it. The result  
[~g]â€”which is called the L2-Inverse
Fourier Transform of [g]â€”then satisï¬es
 
 
~g

= [g],
g âˆˆL2.
(6.15)
4The function Î¶ is not unique. If âˆ¥xn âˆ’Î¶âˆ¥2 â†’0, then also
xn âˆ’ËœÎ¶

2 â†’0 whenever ËœÎ¶ âˆˆ[Î¶].
And conversely, if âˆ¥xn âˆ’Î¶âˆ¥2 â†’0 and
xn âˆ’ËœÎ¶

2 â†’0, then ËœÎ¶ must be in [Î¶].
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.2 Review of the Fourier Transform
73
(iv) Applying the L2-Fourier Transform twice is equivalent to reï¬‚ecting the ele-
ments of the equivalence class
 
 
[x] = [~x],
x âˆˆL2.
(6.16)
(v) The L2-Fourier Transform preserves energies:5
 
[x]

2 =
[x]

2,
x âˆˆL2.
(6.17)
(vi) The L2-Fourier Transform preserves inner products:6

[x], [y]

=
# 
[x],  
[y]
$
,
x, y âˆˆL2.
(6.18)
Proof. This theorem is a restatement of (Rudin, 1987, Chapter 9, Theorem 9.13).
Identity (6.16) appears in this form in (Stein and Weiss, 1971, Chapter 1, Section 2,
Theorem 2.4).
The result that the L2-Fourier Transform preserves energies is sometimes called
Plancherelâ€™s Theorem and the result that it preserves inner products Parsevalâ€™s
Theorem. We shall use â€œParsevalâ€™s Theoremâ€ for both. It is so important that
we repeat it here in the form of a theorem. Following mathematical practice, we
drop the square brackets in the theoremâ€™s statement.
Theorem 6.2.9 (Parsevalâ€™s Theorem). For any x, y âˆˆL2
âŸ¨x, yâŸ©= âŸ¨Ë†x, Ë†yâŸ©
(6.19)
and
âˆ¥xâˆ¥2 = âˆ¥Ë†xâˆ¥2 .
(6.20)
As we mentioned earlier, there is no simple explicit expression for the L2-Fourier
Transform. The following proposition simpliï¬es its calculation under certain as-
sumptions that are, for example, satisï¬ed by the sinc(Â·) function.
Proposition 6.2.10. If x = Ë‡g for some g âˆˆL1 âˆ©L2, then:
(i) x âˆˆL2.
(ii) âˆ¥xâˆ¥2 = âˆ¥gâˆ¥2.
(iii) The L2-Fourier Transform of [x] is the equivalence class [g].
5The energy of an equivalence class was deï¬ned in Section 4.7.
6The inner product between equivalence classes was deï¬ned in Section 4.7.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

74
The Frequency Response of Filters and Bandlimited Signals
Proof. It suï¬ƒces to prove Part (iii) because Parts (i) and (ii) will then follow from
the preservation of energy under the L2-Fourier Transform (Theorem 6.2.8 (v)).
To prove Part (iii) we compute
[g] =  
 
~g

= "
%
Ë†~g
&
=  
[x],
where the ï¬rst equality follows from (6.15); the second from Theorem 6.2.8 (i)
(because the hypothesis g âˆˆL1 âˆ©L2 implies that ~g âˆˆL1 âˆ©L2); and the ï¬nal
equality from Proposition 6.2.3 (i) and from the hypothesis that x = Ë‡g.
6.2.4
More on the Fourier Transform
In this section we present additional results that shed some light on the problem of
reconstructing a signal from its FT. The ï¬rst is a continuity result, which may seem
technical but which has some useful consequences. It can be used to show that the
IFT (of an integrable function) always yields a continuous signal. Consequently,
if one starts with a discontinuous function, takes its FT, and then the IFT, one
does not obtain the original function. It can also be usedâ€”once we deï¬ne the
frequency response of a ï¬lter in Section 6.3â€”to show that no stable ï¬lter can have
a discontinuous frequency response.
Theorem 6.2.11 (Continuity and Boundedness of the Fourier Transform).
(i) If x is integrable, then its FT Ë†x is a uniformly continuous function satisfying
Ë†x(f)
 â‰¤
 âˆ
âˆ’âˆ
|x(t)| dt,
f âˆˆR,
(6.21)
and
lim
|f|â†’âˆË†x(f) = 0.
(6.22)
(ii) If g is integrable, then its IFT Ë‡g is a uniformly continuous function satisfying
Ë‡g(t)
 â‰¤
 âˆ
âˆ’âˆ
|g(f)| df,
t âˆˆR.
(6.23)
Proof. We begin with Part (i). Inequality (6.21) follows directly from the deï¬nition
of the FT and from Proposition 2.4.1. The proof of the uniform continuity of Ë†x is
not very diï¬ƒcult but is omitted. See (Katznelson, 2004, Section VI.1, Theorem 1.2).
A proof of (6.22) can be found in (Katznelson, 2004, Section VI.1, Theorem 1.7).
Part (ii) follows by substituting ~g for x in Part (i) because the IFT of g is the FT
of its mirror image (6.5).
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.2 Review of the Fourier Transform
75
The second result we present is that every integrable signal can be reconstructed
from its FT, but not necessarily via the IFT. The reconstruction formula in (6.25)
ahead works even when the IFT does not do the job.
Theorem 6.2.12 (Reconstructing a Signal from Its Fourier Transform).
(i) If two integrable signals have the same FT, then they are indistinguishable:

Ë†x1(f) = Ë†x2(f),
f âˆˆR

=â‡’

x1 â‰¡x2

,
x1, x2 âˆˆL1.
(6.24)
(ii) Every integrable function x can be reconstructed from its FT in the sense that
lim
Î»â†’âˆ
 âˆ
âˆ’âˆ
x(t) âˆ’
 Î»
âˆ’Î»

1 âˆ’|f|
Î»

Ë†x(f) ei2Ï€ft df
 dt = 0.
(6.25)
Proof. See (Katznelson, 2004, Chapter VI, Section 1.10).
Conditions under which the IFT of the FT of a signal recovers the signal are given
in the following theorem.
Theorem 6.2.13 (The Inversion Theorem).
(i) Suppose that x is integrable and that its FT Ë†x is also integrable. Deï¬ne
Ëœx = Ë‡Ë†x.
(6.26)
Then Ëœx is a continuous function with
lim
|t|â†’âˆËœx(t) = 0,
(6.27)
and the functions x and Ëœx agree except on a set of Lebesgue measure zero.
(ii) Suppose that g is integrable and that its IFT Ë‡g is also integrable. Deï¬ne
Ëœg = Ë†Ë‡g.
(6.28)
Then Ëœg is a continuous function with
lim
|f|â†’âˆËœg(f) = 0
(6.29)
and the functions g and Ëœg agree except on a set of Lebesgue measure zero.
Proof. For a proof of Part (i) see (Rudin, 1987, Theorem 9.11). Part (ii) follows
by substituting g for x in Part (i) and using Proposition 6.2.3 (iii).
Corollary 6.2.14.
(i) If x is a continuous integrable signal whose FT is integrable, then
Ë‡Ë†x = x.
(6.30)
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

76
The Frequency Response of Filters and Bandlimited Signals
(ii) If g is continuous and integrable, and if Ë‡g is also integrable, then
Ë†Ë‡g = g.
(6.31)
Proof. Part (i) follows from Theorem 6.2.13 (i) by noting that if two continuous
functions are equal outside a set of Lebesgue measure zero, then they are identical.
Part (ii) follows similarly from Theorem 6.2.13 (ii).
6.2.5
On the Brickwall and the sinc(Â·) Functions
We next discuss the FT and the IFT of the Brickwall function
Î¾ 	â†’I{|Î¾| â‰¤1},
(6.32)
which derives its name from the shape of its plot. Since it is a symmetric function,
it follows from (6.6) that its FT and IFT are identical. Both are equal to a properly
stretched and scaled sinc(Â·) function (5.20).
More generally, we oï¬€er the reader advice on how to remember that for Î±, Î³ > 0,
t 	â†’Î´ sinc(Î±t) is the IFT of f 	â†’Î² I{|f| â‰¤Î³}
(6.33)
if, and only if,
Î´ = 2Î³Î²
(6.34a)
and
Î³ 1
Î± = 1
2.
(6.34b)
Condition (6.34a) is easily remembered because its LHS is the value at t = 0 of
Î´ sinc(Î±t) and its RHS is the value at t = 0 of the IFT of f 	â†’Î² I{|f| â‰¤Î³}:
 âˆ
âˆ’âˆ
Î² I{|f| â‰¤Î³} ei2Ï€ft df

t=0
=
 âˆ
âˆ’âˆ
Î² I{|f| â‰¤Î³} df = 2Î³Î².
Condition (6.34b) is intimately related to the Sampling Theorem that you may
have already seen and that we shall discuss in Chapter 8. Indeed, in the Sam-
pling Theorem (Theorem 8.4.3) the time between consecutive samples T and the
bandwidth W satisfy
T W = 1
2.
(In this application Î± corresponds to 1/T and Î³ corresponds to the bandwidth W.)
It is tempting to say that Conditions (6.34) also imply that the FT of the func-
tion t 	â†’Î´ sinc(Î±t) is the function f 	â†’Î² I{|f| â‰¤Î³}, but there is a caveat. The
signal t 	â†’Î´ sinc(Î±t) is not integrable. Consequently, its L1-Fourier Transform
(Deï¬nition 6.2.1) is undeï¬ned. However, since it is energy-limited, its L2-Fourier
Transform is deï¬ned (Deï¬nition 6.2.7). Using Proposition 6.2.10 with the substitu-
tion of f 	â†’Î² I{|f| â‰¤Î³} for g, we obtain that, indeed, Conditions (6.34) imply that
the L2-Fourier Transform of the (equivalence class of the) function t 	â†’Î´ sinc(Î±t)
is the (equivalence class of the) function f 	â†’Î² I{|f| â‰¤Î³}.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.3 The Frequency Response of a Filter
77
Î´
ï¬rst zero at
1
Î±
Î²
cutoï¬€Î³
Figure 6.1: The stretched & scaled sinc(Â·) function and the stretched & scaled
Brickwall function above are an L2-Fourier pair if the value of the former at zero
(i.e., Î´) is the integral of the latter (i.e., 2 Ã— Î² Ã— cutoï¬€) and if the product of the
location of the ï¬rst zero of the former by the cutoï¬€of the latter is 1/2.
The relation between the sinc(Â·) and the Brickwall functions is summarized in
Figure 6.1.
The derivation of the result is straightforward: the IFT of the Brickwall function
can be computed for every t Ì¸= 0 as
 âˆ
âˆ’âˆ
Î² I{|f| â‰¤Î³} ei2Ï€ft df = Î²
 Î³
âˆ’Î³
ei2Ï€ft df
=
Î²
i2Ï€t ei2Ï€ft
Î³
âˆ’Î³
=
Î²
i2Ï€t

ei2Ï€Î³t âˆ’eâˆ’i2Ï€Î³t
= Î²
Ï€t sin(2Ï€Î³t)
= 2Î²Î³ sinc(2Î³t).
(6.35)
And since sinc(0) is deï¬ned as 1, (6.35) also holds (by inspection) when t is zero.
6.3
The Frequency Response of a Filter
Recall that in Section 5.7 we deï¬ned a ï¬lter of impulse response h to be a physical
device that when fed the input x produces the output xâ‹†h. Of course, this is only
meaningful if the convolution is deï¬ned. Subject to some technical assumptions
that are made precise in Theorem 6.3.2, the FT of the output waveform xâ‹†h is the
product of the FT of the input waveform x by the FT of the impulse response h.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

78
The Frequency Response of Filters and Bandlimited Signals
Consequently, we can think of a ï¬lter of impulse response h as a physical device
that produces an output signal whose FT is the product of the FT of the input
signal and the FT of the impulse response.
The FT of the impulse response is called the frequency response of the ï¬lter. If
the ï¬lter is stable and its impulse response therefore integrable, then we deï¬ne the
ï¬lterâ€™s frequency response as the Fourier Transform of the impulse response using
Deï¬nition 6.2.1 (the L1-Fourier Transform). If the impulse response is energy-
limited but not integrable, then we deï¬ne the frequency response as the Fourier
Transform of the impulse response using the deï¬nition of the Fourier Transform for
energy-limited signals that are not integrable as in Section 6.2.3 (the L2-Fourier
Transform).
Deï¬nition 6.3.1 (Frequency Response).
(i) The frequency response of a stable ï¬lter is the Fourier Transform of its
impulse response as deï¬ned in Deï¬nition 6.2.1.
(ii) The frequency response of an unstable ï¬lter whose impulse response is
energy-limited is the L2-Fourier Transform of its impulse response as deï¬ned
in Section 6.2.3 (Deï¬nition 6.2.7).
As discussed in Section 5.5, if x, h are both integrable, then x â‹†h is deï¬ned at
all epochs t outside a set of Lebesgue measure zero, and x â‹†h is integrable. In
this case the FT of x â‹†h is the mapping f 	â†’Ë†x(f) Ë†h(f). If x is integrable and
h is of ï¬nite energy, then x â‹†h is also deï¬ned at all epochs t outside a set of
Lebesgue measure zero. But in this case the convolution is only guaranteed to be
of ï¬nite energy; it need not be integrable. We can discuss its Fourier Transform
using the deï¬nition of the L2-Fourier Transform for energy-limited signals that are
not integrable as in Section 6.2.3. In this case, again, the L2-Fourier Transform of
x â‹†h is the (equivalence class of the) mapping f 	â†’Ë†x(f) Ë†h(f):7
Theorem 6.3.2 (The Fourier Transform of a Convolution).
(i) If the signals h and x are both integrable, then the convolution xâ‹†h is deï¬ned
for all t outside a set of Lebesgue measure zero; it is integrable; and its
L1-Fourier Transform 
x â‹†h is given by

x â‹†h(f) = Ë†x(f) Ë†h(f),
f âˆˆR,
(6.36)
where Ë†x and Ë†h are the L1-Fourier Transforms of x and h.
(ii) If the signal x is integrable and if h is of ï¬nite energy, then the convolution
x â‹†h is deï¬ned for all t outside a set of Lebesgue measure zero; it is energy-
limited; and its L2-Fourier Transform 
x â‹†h is also given by (6.36) with Ë†x,
7To be precise we should say that the L2 -Fourier Transform of xâ‹†h is the equivalence class of
the product of the L1 -Fourier Transform of x by any element in the equivalence class consisting
of the L2 -Fourier Transform of [h].
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.3 The Frequency Response of a Filter
79
f
"
LPFWc(f)
Wc
âˆ’Wc
Wc
1
Figure 6.2: The frequency response of the ideal unit-gain lowpass ï¬lter of cutoï¬€
frequency Wc. Notice that Wc is the length of the interval of positive frequencies
where the gain is one.
as before, being the L1-Fourier Transform of x but with Ë†h now being the
L2-Fourier Transform of h.
Proof. For a proof of Part (i) see, for example, (Stein and Weiss, 1971, Chapter 1,
Section 1, Theorem 1.4).
For Part (ii) see (Stein and Weiss, 1971, Chapter 1,
Section 2, Theorem 2.6).
As an example, recall from Section 5.9 that the unit-gain ideal lowpass ï¬lter of
cutoï¬€frequency Wc is a ï¬lter of impulse response
h(t) = 2Wc sinc(2Wct),
t âˆˆR.
(6.37)
This ï¬lter is not causal and not stable, but its impulse response is energy-limited.
The ï¬lterâ€™s frequency response is the L2-Fourier Transform of the impulse response
(6.37), which, using the results from Section 6.2.5, is given by (the equivalence class
of) the mapping
f 	â†’I{|f| â‰¤Wc},
f âˆˆR.
(6.38)
This mapping maps all frequencies f satisfying |f| > Wc to 0 and all frequencies
satisfying |f| â‰¤Wc to one. It is for this reason that we use the adjective â€œunit-gainâ€
in describing this ï¬lter. We denote the mapping in (6.38) by "
LPFWc(Â·) so
"
LPFWc(f) â‰œI{|f| â‰¤Wc},
f âˆˆR.
(6.39)
This mapping is depicted in Figure 6.2. Note that Wc is the length of the interval
of positive frequencies where the response is one.
Turning to the ideal unit-gain bandpass ï¬lter of bandwidth W around the carrier
frequency fc satisfying fc â‰¥W/2, we note that, by (5.21), its time-t impulse
response BPFW,fc(t) is given by
BPFW,fc(t) = 2W cos(2Ï€fct) sinc(Wt)
= 2 Re

LPFW/2(t) ei2Ï€fct
.
(6.40)
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

80
The Frequency Response of Filters and Bandlimited Signals
f

BPFW,fc(f)
fc
âˆ’fc
W
1
Figure 6.3: The frequency response of the ideal unit-gain bandpass ï¬lter of band-
width W around the carrier frequency fc. Notice that, as for the lowpass ï¬lter, W
is the length of the interval of positive frequencies where the gain is one.
This ï¬lter too is noncausal and nonstable. From (6.40) and (6.39) we obtain using
Table 6.1 that its frequency response is (the equivalence class of) the mapping
f 	â†’I
'|f| âˆ’fc
 â‰¤W
2
(
.
We denote this mapping by 
BPFW,fc(Â·) so

BPFW,fc(f) â‰œI
'|f| âˆ’fc
 â‰¤W
2
(
,
f âˆˆR.
(6.41)
This mapping is depicted in Figure 6.3. Note that, as for the lowpass ï¬lter, W is
the length of the interval of positive frequencies where the response is one.
6.4
Bandlimited Signals and Lowpass Filtering
In this section we deï¬ne bandlimited signals and discuss lowpass ï¬ltering.
We
treat energy-limited signals and integrable signals separately. As we shall see, any
integrable signal that is bandlimited to W Hz is also an energy-limited signal that
is bandlimited to W Hz (Note 6.4.12).
6.4.1
Energy-Limited Signals
The main result of this section is that the following three statements are equivalent:
(a) The signal x is an energy-limited signal satisfying
(x â‹†LPFW)(t) = x(t),
t âˆˆR.
(6.42)
(b) The signal x can be expressed in the form
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR,
(6.43a)
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.4 Bandlimited Signals and Lowpass Filtering
81
for some measurable function g: f 	â†’g(f) satisfying
 W
âˆ’W
|g(f)|2 df < âˆ.
(6.43b)
(If (b) holds, then the LHS of (6.43b) must equal the energy in x.)
(c) The signal x is a continuous energy-limited signal whose L2-Fourier Trans-
form Ë†x satisï¬es
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df =
 W
âˆ’W
|Ë†x(f)|2 df.
(6.44)
We can thus deï¬ne x to be an energy-limited signal that is bandlimited to W Hz
if one (and hence all) of the above conditions hold.
In deriving this result we shall take (a) as the deï¬nition. We shall then establish
the equivalence (a) â‡”(b) in Proposition 6.4.5, which also establishes that the
function g in (6.43a) can be taken as any element in the equivalence class of the
L2-Fourier Transform of x, and that the LHS of (6.43b) is then âˆ¥xâˆ¥2
2. Finally, we
shall establish the equivalence (a) â‡”(c) in Proposition 6.4.6.
We conclude the section with a summary of the key properties of the result of
passing an energy-limited signal through an ideal unit-gain lowpass ï¬lter.
We begin by deï¬ning an energy-limited signal to be bandlimited to W Hz if it is
unaltered when it is lowpass ï¬ltered by an ideal unit-gain lowpass ï¬lter of cutoï¬€
frequency W.
Recalling that we are denoting by LPFW(t) the time-t impulse
response of an ideal unit-gain lowpass ï¬lter of cutoï¬€frequency W (see (5.19)), we
have the following deï¬nition.8
Deï¬nition 6.4.1 (Energy-Limited Bandlimited Signals). We say that the signal x
is an energy-limited signal that is bandlimited to W Hz if x is in L2 and
(x â‹†LPFW)(t) = x(t),
t âˆˆR.
(6.45)
Note 6.4.2. If an energy-limited signal that is bandlimited to W Hz is of zero
energy, then it is the all-zero signal 0.
Proof. Let x be an energy-limited signal that is bandlimited to W Hz and that
has zero energy. Then
|x(t)| =
(x â‹†LPFW)(t)

â‰¤âˆ¥xâˆ¥2 âˆ¥LPFWâˆ¥2
= âˆ¥xâˆ¥2
âˆš
2W
= 0,
t âˆˆR,
8Even though the ideal unit-gain lowpass ï¬lter of cutoï¬€frequency W is not stable, its impulse
response LPFW(Â·) is of ï¬nite energy (because it decays like 1/t and the integral of 1/t2 from
one to inï¬nity is ï¬nite). Consequently, we can use the Cauchy-Schwarz Inequality to prove that
if x âˆˆL2 then the mapping Ï„ â†’x(Ï„) LPFW(t âˆ’Ï„) is integrable for every time instant t âˆˆR.
Consequently, the convolution x â‹†LPFW is deï¬ned at every time instant t; see Section 5.5.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

82
The Frequency Response of Filters and Bandlimited Signals
where the ï¬rst equality follows because x is an energy-limited signal that is band-
limited to W Hz and is thus unaltered when it is lowpass ï¬ltered; the subsequent
inequality follows from (5.6b); the subsequent equality by computing âˆ¥LPFWâˆ¥2
using Parsevalâ€™s Theorem and the explicit form of the frequency response of the
ideal unit-gain lowpass ï¬lter of bandwidth W (6.38); and where the ï¬nal equality
follows from the hypothesis that x is of zero energy.
Having deï¬ned what it means for an energy-limited signal to be bandlimited to W
Hz, we can now deï¬ne its bandwidth.9
Deï¬nition 6.4.3 (Bandwidth). The bandwidth of an energy-limited signal x is
the smallest frequency W to which x is bandlimited.
The next lemma shows that the result of passing an energy-limited signal through
an ideal unit-gain lowpass ï¬lter of cutoï¬€frequency W is an energy-limited signal
that is bandlimited to W Hz.
Lemma 6.4.4.
(i) Let y = x â‹†LPFW be the output of an ideal unit-gain lowpass ï¬lter of cutoï¬€
frequency W that is fed the energy-limited input x âˆˆL2. Then y âˆˆL2;
y(t) =
 W
âˆ’W
Ë†x(f) ei2Ï€ft df,
t âˆˆR;
(6.46)
and the L2-Fourier Transform of y is the (equivalence class of the) mapping
f 	â†’Ë†x(f) I{|f| â‰¤W}.
(6.47)
(ii) If g: f 	â†’g(f) is a bounded integrable function and if x is energy-limited,
then x â‹†Ë‡g is in L2; it can be expressed as

x â‹†Ë‡g

(t) =
 âˆ
âˆ’âˆ
Ë†x(f) g(f) ei2Ï€ft df,
t âˆˆR;
(6.48)
and its L2-Fourier Transform is given by (the equivalence class of) the map-
ping f 	â†’Ë†x(f) g(f).
Proof. Even though Part (i) is a special case of Part (ii) corresponding to g being
the mapping f 	â†’I{|f| â‰¤W}, we shall prove the two parts separately. We begin
with a proof of Part (i). The idea of the proof is to express for each t âˆˆR the
time-t output y(t) as an inner product and to then use Parsevalâ€™s Theorem. Thus,
9To be more rigorous we should use in this deï¬nition the term â€œinï¬mumâ€ instead of â€œsmallest,â€
but it turns out that the inï¬mum here is also a minimum.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.4 Bandlimited Signals and Lowpass Filtering
83
(6.46) follows from the calculation
y(t) =

x â‹†LPFW

(t)
=
 âˆ
âˆ’âˆ
x(Ï„) LPFW(t âˆ’Ï„) dÏ„
=

x, Ï„ 	â†’LPFW(t âˆ’Ï„)

=

x, Ï„ 	â†’LPFW(Ï„ âˆ’t)

=
#
Ë†x, f 	â†’eâˆ’i2Ï€ft "
LPFW(f)
$
=
Ë†x, f 	â†’eâˆ’i2Ï€ft I{|f| â‰¤W}

=
 W
âˆ’W
Ë†x(f) ei2Ï€ft df,
where the fourth equality follows from the symmetry of the function LPFW(Â·), and
where the ï¬fth equality follows from Parsevalâ€™s Theorem and the fact that delaying
a function multiplies its FT by a complex exponential. Having established (6.46),
Part (i) now follows from Proposition 6.2.10, because, by Parsevalâ€™s Theorem, the
mapping f 	â†’Ë†x(f) I{|f| â‰¤W} is of ï¬nite energy and hence, by Proposition 3.4.3,
also integrable.
We next turn to Part (ii). We ï¬rst note that the assumption that g is bounded
and integrable implies that it is also energy-limited, because if |g(f)| â‰¤Ïƒâˆfor all
f âˆˆR, then |g(f)|2 â‰¤Ïƒâˆ|g(f)| and

|g(f)|2 df â‰¤Ïƒâˆ

|g(f)| df. Thus,
g âˆˆL1 âˆ©L2.
(6.49)
We next prove (6.48). To that end we express the convolution x â‹†Ë‡g at time t as
an inner product and then use Parsevalâ€™s Theorem to obtain

x â‹†Ë‡g

(t) =
 âˆ
âˆ’âˆ
x(Ï„) Ë‡g(t âˆ’Ï„) dÏ„
= âŸ¨x, Ï„ 	â†’Ë‡gâˆ—(t âˆ’Ï„)âŸ©
=
Ë†x, f 	â†’eâˆ’i2Ï€ftgâˆ—(f)

=
 âˆ
âˆ’âˆ
Ë†x(f) g(f) ei2Ï€ft df,
t âˆˆR,
(6.50)
where the third equality follows from Parsevalâ€™s Theorem and by noting that the
L2-Fourier Transform of the mapping Ï„ 	â†’Ë‡gâˆ—(t âˆ’Ï„) is the equivalence class of
the mapping f 	â†’eâˆ’i2Ï€ftgâˆ—(f), as can be veriï¬ed by expressing the mapping
Ï„ 	â†’Ë‡gâˆ—(t âˆ’Ï„) as the IFT of the mapping f 	â†’eâˆ’i2Ï€ftgâˆ—(f)
Ë‡gâˆ—(t âˆ’Ï„) =
	 âˆ
âˆ’âˆ
g(f) ei2Ï€f(tâˆ’Ï„) df

âˆ—
=
 âˆ
âˆ’âˆ
gâˆ—(f) ei2Ï€f(Ï„âˆ’t) df
=
 âˆ
âˆ’âˆ

gâˆ—(f) eâˆ’i2Ï€ft
ei2Ï€fÏ„ df,
t, Ï„ âˆˆR,
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

84
The Frequency Response of Filters and Bandlimited Signals
and by then applying Proposition 6.2.10 to the mapping f 	â†’gâˆ—(f) eâˆ’i2Ï€ft, which
is in L1 âˆ©L2 by (6.49).
Having established (6.48), we next examine the integrand in (6.48) and note that
if |g(f)| is upper-bounded by Ïƒâˆ, then the modulus of the integrand is upper-
bounded by Ïƒâˆ|Ë†x(f)|, so the assumption that x âˆˆL2 (and hence that Ë†x is of
ï¬nite energy) guarantees that the integrand is square integrable.
Also, by the
Cauchy-Schwarz Inequality, the square integrability of g and of Ë†x implies that the
integrand is integrable. Thus, the integrand in (6.48) is both square integrable and
integrable so, by Proposition 6.2.10, the signal x â‹†Ë‡g is square integrable and its
Fourier Transform is the (equivalence class of the) mapping f 	â†’Ë†x(f) g(f).
With the aid of the above lemma we can now give an equivalent deï¬nition for
energy-limited signals that are bandlimited to W Hz. This deï¬nition is popular
among mathematicians, because it does not involve the L2-Fourier Transform and
because the continuity of the signal is implied.
Proposition 6.4.5 (On the Deï¬nition of Bandlimited Functions in L2).
(i) If x is an energy-limited signal that is bandlimited to W Hz, then it can be
expressed in the form
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR,
(6.51)
where g(Â·) satisï¬es
 W
âˆ’W
|g(f)|2 df < âˆ
(6.52)
and can be taken as (any function in the equivalence class of) Ë†x.
(ii) If a signal x can be expressed as in (6.51) for some function g(Â·) satisfying
(6.52), then x is an energy-limited signal that is bandlimited to W Hz and Ë†x
is (the equivalence class of) the mapping f 	â†’g(f) I{|f| â‰¤W}.
Proof. We ï¬rst prove Part (i). Let x be an energy-limited signal that is band-
limited to W Hz. Then
x(t) = (x â‹†LPFW)(t)
=
 W
âˆ’W
Ë†x(f) ei2Ï€ft df,
t âˆˆR,
where the ï¬rst equality follows from Deï¬nition 6.4.1, and where the second equality
follows from Lemma 6.4.4 (i). Consequently, if we pick g as (any element of the
equivalence class of) f 	â†’Ë†x(f) I{|f| â‰¤W}, then (6.51) will be satisï¬ed and (6.52)
will follow from Parsevalâ€™s Theorem.
To prove Part (ii) deï¬ne Ëœg: f 	â†’g(f) I{|f| â‰¤W}. From the assumption (6.52) and
from Proposition 3.4.3 it then follows that Ëœg âˆˆL1 âˆ©L2. This and (6.51) imply that
x âˆˆL2 and that the L2-Fourier Transform of (the equivalence class of) x is (the
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.4 Bandlimited Signals and Lowpass Filtering
85
equivalence class of) Ëœg; see Proposition 6.2.10. To complete the proof of Part (ii)
it thus remains to show that x â‹†LPFW = x. This follows from the calculation:

x â‹†LPFW

(t) =
 W
âˆ’W
Ë†x(f) ei2Ï€ft df
=
 W
âˆ’W
g(f) ei2Ï€ft df
= x(t),
t âˆˆR,
where the ï¬rst equality follows from Lemma 6.4.4 (i); the second because we have
already established that the L2-Fourier Transform of (the equivalence class of) x is
(the equivalence class of) f 	â†’g(f) I{|f| â‰¤W}; and where the last equality follows
from (6.51).
In the engineering literature a function is often deï¬ned as bandlimited to W Hz
if its FT is zero for frequencies f outside the interval [âˆ’W, W ]. This deï¬nition
is imprecise because the L2-Fourier Transform of a signal is an equivalence class
and its value at a given frequency is technically undeï¬ned. It would be better to
deï¬ne an energy-limited signal as bandlimited to W Hz if âˆ¥xâˆ¥2
2 =
 W
âˆ’W
Ë†x(f)
2 df
so â€œall its energy is contained in the frequency band [âˆ’W, W ].â€ However, this is
not quite equivalent to our deï¬nition. For example, the L2-Fourier Transform of
the discontinuous signal
x(t) =

17
if t = 0,
sinc 2Wt
otherwise,
is (the equivalence class of) the Brickwall (frequency domain) function
1
2W I{|f| â‰¤W},
f âˆˆR
(because the discontinuity at t = 0 does not inï¬‚uence the Fourier integral), but
the signal is altered by the lowpass ï¬lter, which smooths it out to produce the
continuous waveform t 	â†’sinc(2Wt). Readers who have already seen the Sampling
Theorem will note that the above signal x(Â·) provides a counterexample to the
Sampling Theorem as it is often imprecisely stated.
The following proposition clariï¬es the relationship between this deï¬nition and ours.
Proposition 6.4.6 (More on the Deï¬nition of Bandlimited Functions in L2).
(i) If x is an energy-limited signal that is bandlimited to W Hz, then x is a
continuous function and all its energy is contained in the frequency interval
[âˆ’W, W ] in the sense that its L2-Fourier Transform Ë†x satisï¬es
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df =
 W
âˆ’W
|Ë†x(f)|2 df.
(6.53)
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

86
The Frequency Response of Filters and Bandlimited Signals
(ii) If the signal x âˆˆL2 satisï¬es (6.53), then x is indistinguishable from the
signal x â‹†LPFW, which is an energy-limited signal that is bandlimited to W
Hz. If in addition to satisfying (6.53) the signal x is continuous, then x is
an energy-limited signal that is bandlimited to W Hz.
Proof. This propositionâ€™s claims are a subset of those of Proposition 6.4.7, which
summarizes some of the results relating to lowpass ï¬ltering. The proof is therefore
omitted.
Proposition 6.4.7. Let y = x â‹†LPFW be the result of feeding the signal x âˆˆL2 to
an ideal unit-gain lowpass ï¬lter of cutoï¬€frequency W. Then:
(i) y is energy-limited with
âˆ¥yâˆ¥2 â‰¤âˆ¥xâˆ¥2 .
(6.54)
(ii) y is an energy-limited signal that is bandlimited to W Hz.
(iii) Its L2-Fourier Transform Ë†y is given by (the equivalence class of) the mapping
f 	â†’Ë†x(f) I{|f| â‰¤W}.
(iv) All the energy in y is concentrated in the frequency band [âˆ’W, W ] in the
sense that:
 âˆ
âˆ’âˆ
|Ë†y(f)|2 df =
 W
âˆ’W
|Ë†y(f)|2 df.
(v) y can be represented as
y(t) =
 âˆ
âˆ’âˆ
Ë†y(f) ei2Ï€ft df,
t âˆˆR
(6.55)
=
 W
âˆ’W
Ë†x(f) ei2Ï€ft df,
t âˆˆR.
(6.56)
(vi) y is uniformly continuous.
(vii) If x âˆˆL2 has all its energy concentrated in the frequency band [âˆ’W, W ] in
the sense that
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df =
 W
âˆ’W
|Ë†x(f)|2 df,
(6.57)
then x is indistinguishable from the bandlimited signal x â‹†LPFW.
(viii) x is an energy-limited signal that is bandlimited to W if, and only if, it
satisï¬es all three of the following conditions: it is in L2; it is continuous;
and it satisï¬es (6.57).
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.4 Bandlimited Signals and Lowpass Filtering
87
Proof. Part (i) follows from Lemma 6.4.4 (i), which demonstrates that Ë†y is (the
equivalence class of) the mapping f 	â†’Ë†x(f) I{|f| â‰¤W} so, by Parsevalâ€™s Theorem,
âˆ¥yâˆ¥2
2 =
 âˆ
âˆ’âˆ
|Ë†y(f)|2 df
=
 W
âˆ’W
|Ë†x(f)|2 df
â‰¤
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df
= âˆ¥xâˆ¥2
2 .
Part (ii) follows because, by Lemma 6.4.4 (i), the signal y satisï¬es
y(t) =
 W
âˆ’W
Ë†x(f) ei2Ï€ft df
where
 W
âˆ’W
|Ë†x(f)|2 df â‰¤
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df = âˆ¥xâˆ¥2
2 < âˆ,
so, by Proposition 6.4.5, y is an energy-limited signal that is bandlimited to W Hz.
Part (iii) follows directly from Lemma 6.4.4 (i). Part (iv) follows from Part (iii).
Part (v) follows, again, directly from Lemma 6.4.4.
Part (vi) follows from the representation (6.56); from the fact that the IFT of
integrable functions is uniformly continuous (Theorem 6.2.11); and because the
condition âˆ¥xâˆ¥2 < âˆimplies, by Proposition 3.4.3, that f 	â†’Ë†x(f) I{|f| â‰¤W} is
integrable.
To prove Part (vii) we note that by Part (ii) x â‹†LPFW is an energy-limited signal
that is bandlimited to W Hz, and we note that (6.57) implies that x is indistin-
guishable from x â‹†LPFW because
âˆ¥x âˆ’x â‹†LPFWâˆ¥2
2 =
 âˆ
âˆ’âˆ
Ë†x(f) âˆ’

x â‹†LPFW(f)

2
df
=
 âˆ
âˆ’âˆ
Ë†x(f) âˆ’Ë†x(f) I{|f| â‰¤W}
2 df
=

|f|>W
Ë†x(f)
2 df
= 0,
where the ï¬rst equality follows from Parsevalâ€™s Theorem; the second equality from
Lemma 6.4.4 (i); the third equality because the integrand is zero for |f| â‰¤W; and
the ï¬nal equality from (6.57).
To prove Part (viii) deï¬ne y = x â‹†LPFW and note that if x is an energy-limited
signal that is bandlimited to W Hz then, by Deï¬nition 6.4.1, y = x so the continuity
of x and the fact that its energy is concentrated in the interval [âˆ’W, W ] follow from
Parts (iv) and (vi). In the other direction, if x satisï¬es (6.57) then by Part (vii)
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

88
The Frequency Response of Filters and Bandlimited Signals
it is indistinguishable from the signal y, which is continuous by Part (vi).
If,
additionally, x is continuous, then x must be identical to y because two continuous
functions that are indistinguishable must be identical.
6.4.2
Integrable Signals
We next discuss what we mean when we say that x is an integrable signal that is
bandlimited to W Hz. Also important will be Note 6.4.11, which establishes that
if x is such a signal, then x is equal to the IFT of its FT.
Even though the ideal unit-gain lowpass ï¬lter is unstable, its convolution with any
integrable signal is well-deï¬ned. Denoting the cutoï¬€frequency by Wc we have:
Proposition 6.4.8. For any x âˆˆL1 the convolution integral
 âˆ
âˆ’âˆ
x(Ï„) LPFWc(t âˆ’Ï„) dÏ„
is deï¬ned at every epoch t âˆˆR and is given by
 âˆ
âˆ’âˆ
x(Ï„) LPFWc(t âˆ’Ï„) dÏ„ =
 Wc
âˆ’Wc
Ë†x(f) ei2Ï€ft df,
t âˆˆR.
(6.58)
Moreover, x â‹†LPFWc is an energy-limited signal that is bandlimited to Wc Hz.10
Its L2-Fourier Transform is (the equivalence class of) the mapping
f 	â†’Ë†x(f) I{|f| â‰¤Wc}.
Proof. The key to the proof is to note that, although the sinc(Â·) function is not
integrable, it follows from (6.35) that it can be represented as the Inverse Fourier
Transform of an integrable function (of frequency). Consequently, the existence
of the convolution and its representation as (6.58) follow directly from Proposi-
tion 6.2.5 and (6.35).
To prove the remaining assertions of the proposition we note that, since x is inte-
grable, it follows from Theorem 6.2.11 that |Ë†x(f)| â‰¤âˆ¥xâˆ¥1 and hence
 Wc
âˆ’Wc
|Ë†x(f)|2 df < âˆ.
(6.59)
The result now follows from (6.58), (6.59), and Proposition 6.4.5.
With the aid of Proposition 6.4.8 we can now deï¬ne bandlimited integrable signals:
Deï¬nition 6.4.9 (Bandlimited Integrable Signals). We say that the signal x is
an integrable signal that is bandlimited to W Hz if x is integrable and if it
is unaltered when it is lowpass ï¬ltered by an ideal unit-gain lowpass ï¬lter of cutoï¬€
frequency W:
x(t) = (x â‹†LPFW)(t),
t âˆˆR.
10Lowpass-ï¬ltering an integrable signal need not produce an integrable signal (Exercise 6.20).
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.4 Bandlimited Signals and Lowpass Filtering
89
Proposition 6.4.10 (Characterizing Integrable Signals that Are Bandlimited to
W Hz). If x is an integrable signal, then each of the following statements is equiv-
alent to the statement that x is an integrable signal that is bandlimited to W Hz:
(a) The signal x is unaltered when it is lowpass ï¬ltered:
x(t) = (x â‹†LPFW)(t),
t âˆˆR.
(6.60)
(b) The signal x can be expressed as
x(t) =
 W
âˆ’W
Ë†x(f) ei2Ï€ft df,
t âˆˆR.
(6.61)
(c) The signal x is continuous and
Ë†x(f) = 0,
|f| > W.
(6.62)
(d) There exists an integrable function g such that
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR.
(6.63)
Proof. Condition (a) is the condition given in Deï¬nition 6.4.9, so it only remains
to show that the four conditions are equivalent. We proceed to do so by proving
that (a) â‡”(b); that (b) â‡’(d); that (d) â‡’(c); and that (c) â‡’(b).
That (a) â‡”(b) follows directly from Proposition 6.4.8 and, more speciï¬cally, from
the representation (6.58). The implication (b) â‡’(d) is obvious because nothing
precludes us from picking g to be the mapping f 	â†’Ë†x(f) I{|f| â‰¤W}, which is
integrable because Ë†x is bounded by âˆ¥xâˆ¥1 (Theorem 6.2.11).
We next prove that (d) â‡’(c). We thus assume that there exists an integrable
function g such that (6.63) holds and proceed to prove that x is continuous and
that (6.62) holds. To that end we ï¬rst note that the integrability of g implies,
by Theorem 6.2.11, that x (= Ë‡g) is continuous. It thus remains to prove that Ë†x
satisï¬es (6.62). Deï¬ne g0 as the mapping f 	â†’g(f) I{|f| â‰¤W}. By (6.63) it then
follows that x = Ë‡g0. Consequently,
Ë†x = Ë†Ë‡g0.
(6.64)
Employing Theorem 6.2.13 (ii) we conclude that the RHS of (6.64) is equal to g0
outside a set of Lebesgue measure zero, so (6.64) implies that Ë†x is indistinguishable
from g0.
Since both Ë†x and g0 are continuous for |f| > W, this implies that
Ë†x(f) = g0(f) for all frequencies |f| > W.
Since, by its deï¬nition, g0(f) = 0
whenever |f| > W we can conclude that (6.62) holds.
Finally (c) â‡’(b) follows directly from Theorem 6.2.13 (i).
From Proposition 6.4.10 (cf. (b) and (c)) we obtain:
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

90
The Frequency Response of Filters and Bandlimited Signals
Note 6.4.11. If x is an integrable signal that is bandlimited to W Hz, then it is
equal to the IFT of its FT.
By Proposition 6.4.10 it also follows that if x is an integrable signal that is
bandlimited to W Hz, then (6.61) is satisï¬ed.
Since the integrand in (6.61) is
bounded (by âˆ¥xâˆ¥1) it follows that the integrand is square integrable over the in-
terval [âˆ’W, W ]. Consequently, by Proposition 6.4.5, x must be an energy-limited
signal that is bandlimited to W Hz. We have thus proved:
Note 6.4.12. An integrable signal that is bandlimited to W Hz is also an energy-
limited signal that is bandlimited to W Hz.
The reverse statement is not true: the sinc(Â·) is an energy-limited signal that is
bandlimited to 1/2 Hz, but it is not integrable.
The deï¬nition of bandwidth for integrable signals is similar to Deï¬nition 6.4.3.11
Deï¬nition 6.4.13 (Bandwidth). The bandwidth of an integrable signal is the
smallest frequency W to which it is bandlimited.
6.5
Bandlimited Signals Through Stable Filters
In this section we discuss the result of feeding bandlimited signals to stable ï¬lters.
We begin with energy-limited signals. In Theorem 6.3.2 we saw that the convo-
lution of an integrable signal with an energy-limited signal is deï¬ned at all times
outside a set of Lebesgue measure zero. The next proposition shows that if the
energy-limited signal is bandlimited to W Hz, then the convolution is deï¬ned at
every time, and the result is an energy-limited signal that is bandlimited to W Hz.
Proposition 6.5.1. Let x be an energy-limited signal that is bandlimited to W Hz
and let h be integrable. Then xâ‹†h is deï¬ned for every t âˆˆR; it is an energy-limited
signal that is bandlimited to W Hz; and it can be represented as

x â‹†h

(t) =
 W
âˆ’W
Ë†x(f) Ë†h(f) ei2Ï€ft df,
t âˆˆR.
(6.65)
Proof. Since x is an energy-limited signal that is bandlimited to W Hz, it follows
from Proposition 6.4.5 that
x(t) =
 W
âˆ’W
Ë†x(f) ei2Ï€ft df,
t âˆˆR,
(6.66)
with the mapping f 	â†’Ë†x(f) I{|f| â‰¤W} being square integrable and hence, by
Proposition 3.4.3, also integrable. Thus the convolution x â‹†h is the convolution
between the IFT of the integrable mapping f 	â†’Ë†x(f) I{|f| â‰¤W} and the integrable
function h. By Proposition 6.2.5 we thus obtain that the convolution xâ‹†h is deï¬ned
11Again, we omit the proof that the inï¬mum is a minimum.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.6 The Bandwidth of a Product of Two Signals
91
at every time t and has the representation (6.65). The proposition will now follow
from (6.65) and Proposition 6.4.5 once we demonstrate that
 W
âˆ’W
Ë†x(f) Ë†h(f)
2 df < âˆ.
This can be proved by upper-bounding |Ë†h(f)| by âˆ¥hâˆ¥1 (Theorem 6.2.11) and by
then using Parsevalâ€™s Theorem.
We next turn to integrable signals passed through stable ï¬lters.
Proposition 6.5.2 (Integrable Bandlimited Signals through Stable Filters). Let x
be an integrable signal that is bandlimited to W Hz, and let h be integrable. Then
the convolution x â‹†h is deï¬ned for every t âˆˆR; it is an integrable signal that is
bandlimited to W Hz; and it can be represented as

x â‹†h

(t) =
 W
âˆ’W
Ë†x(f) Ë†h(f) ei2Ï€ft df,
t âˆˆR.
(6.67)
Proof. Since every integrable signal that is bandlimited to W Hz is also an energy-
limited signal that is bandlimited to W Hz, it follows from Proposition 6.5.1 that the
convolution xâ‹†h is deï¬ned at every epoch and that it can be represented as (6.65).
Alternatively, one can derive this representation from (6.61) and Proposition 6.2.5
by substituting in that proposition h for x and f 	â†’Ë†x(f) I{|f| â‰¤W} (which is
integrable because Ë†x is bounded by Theorem 6.2.11 (i)) for g. It only remains
to show that x â‹†h is integrable, but this follows because the convolution of two
integrable functions is integrable (5.9).
6.6
The Bandwidth of a Product of Two Signals
In this section we discuss the bandwidth of the product of two bandlimited signals.
The result is a straightforward consequence of the fact that the FT of a product
of two signals is the convolution of their FTs. We begin with the following result
on the FT of a product of signals.
Proposition 6.6.1 (The FT of a Product Is the Convolution of the FTs). If x1
and x2 are energy-limited signals, then their product
t 	â†’x1(t) x2(t)
is an integrable function whose FT is the mapping
f 	â†’
Ë†x1 â‹†Ë†x2

(f).
Proof. Let x1 and x2 be energy-limited signals, and denote their product by y:
y(t) = x1(t) x2(t),
t âˆˆR.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

92
The Frequency Response of Filters and Bandlimited Signals
Since both x1 and x2 are square integrable, it follows from the Cauchy-Schwarz
Inequality that their product y is integrable and that
âˆ¥yâˆ¥1 â‰¤âˆ¥x1âˆ¥2 âˆ¥x2âˆ¥2 .
(6.68)
Having established that the product is integrable, we next derive its FT and show
that
Ë†y(f) =
Ë†x1 â‹†Ë†x2

(f),
f âˆˆR.
(6.69)
This is done by expressing Ë†y(f) as an inner product between two ï¬nite-energy
functions and by then using Parsevalâ€™s Theorem:
Ë†y(f) =
 âˆ
âˆ’âˆ
y(t) eâˆ’i2Ï€ft dt
=
 âˆ
âˆ’âˆ
x1(t) x2(t) eâˆ’i2Ï€ft dt
=

t 	â†’x1(t), t 	â†’xâˆ—
2(t) ei2Ï€ft
=
#
Ëœf 	â†’Ë†x1( Ëœf), Ëœf 	â†’Ë†xâˆ—
2(f âˆ’Ëœf)
$
=
 âˆ
âˆ’âˆ
Ë†x1( Ëœf) Ë†x2(f âˆ’Ëœf) d Ëœf
=
Ë†x1 â‹†Ë†x2

(f),
f âˆˆR.
Proposition 6.6.2. Let x1 and x2 be energy-limited signals that are bandlimited to
W1 Hz and W2 Hz respectively. Then their product is an integrable signal that is
bandlimited to W1 + W2 Hz.
Proof. Since both x1 and x2 are of ï¬nite energy, it follows from the Cauchy-
Schwarz Inequality that their product is integrable. We will show that their product
is an integrable signal that is bandlimited to W1+W2 Hz using Theorem 6.4.10 (d)
by showing that
x1(t) x2(t) =
 W1+W2
âˆ’(W1+W2)
g(f) ei2Ï€ft df,
t âˆˆR,
(6.70)
where the function g(Â·) is bounded and hence integrable
 W1+W2
âˆ’(W1+W2)
|g(f)| df < âˆ.
(6.71)
To establish (6.70) we begin by noting that since x1 is of ï¬nite energy and band-
limited to W1 Hz we have by Proposition 6.4.5
x1(t) =
 W1
âˆ’W1
Ë†x1(f1) ei2Ï€f1t df1,
t âˆˆR.
Similarly,
x2(t) =
 W2
âˆ’W2
Ë†x2(f2) ei2Ï€f2t df2,
t âˆˆR.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.6 The Bandwidth of a Product of Two Signals
93
Consequently,
x1(t) x2(t) =
 W1
âˆ’W1
Ë†x1(f1) ei2Ï€f1t df1
 W2
âˆ’W2
Ë†x2(f2) ei2Ï€f2t df2
=
 W1
âˆ’W1
 W2
âˆ’W2
Ë†x1(f1) Ë†x2(f2) ei2Ï€(f1+f2)t df1 df2
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
Ë†x1(f1) Ë†x2(f2) ei2Ï€(f1+f2)t df1 df2
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
Ë†x1( Ëœf) Ë†x2(f âˆ’Ëœf) ei2Ï€ft d Ëœf df
=
 âˆ
âˆ’âˆ
ei2Ï€ft Ë†x1 â‹†Ë†x2

(f) df
=
 âˆ
âˆ’âˆ
ei2Ï€ftg(f) df,
t âˆˆR,
(6.72)
where
g(f) =
 âˆ
âˆ’âˆ
Ë†x1( Ëœf) Ë†x2(f âˆ’Ëœf) d Ëœf,
f âˆˆR.
(6.73)
Here the second equality follows from Fubiniâ€™s Theorem;12 the third because x1
and x2 are bandlimited to W1 and W2 Hz respectively; and the fourth by intro-
ducing the variables f â‰œf1 + f2 and Ëœf â‰œf1.
To establish (6.70) we now need to show that because x1 and x2 are bandlimited
to W1 and W2 Hz respectively, it follows that
g(f) = 0,
|f| > W1 + W2.
(6.74)
To prove this we note that because x1 and x2 are bandlimited to W1 Hz and W2
Hz respectively, we can rewrite (6.73) as
g(f) =
 âˆ
âˆ’âˆ
Ë†x1( Ëœf) I

| Ëœf| â‰¤W1

Ë†x2(f âˆ’Ëœf) I

|f âˆ’Ëœf| â‰¤W2

d Ëœf,
f âˆˆR,
(6.75)
and the product I

| Ëœf| â‰¤W1

I

|f âˆ’Ëœf| â‰¤W2

is zero for all frequencies Ëœf whenever
|f| > W1 + W2.
Having established (6.70) using (6.72) and (6.74), we now proceed to prove (6.71)
by showing that the integrand in (6.71) is bounded.
We do so by noting that
the integrand in (6.71) is the convolution of two square-integrable functions (Ë†x1
and Ë†x2) so by (5.6b) (with the dummy variable now being f) we have
|g(f)| â‰¤âˆ¥Ë†x1âˆ¥2 âˆ¥Ë†x2âˆ¥2 = âˆ¥x1âˆ¥2 âˆ¥x2âˆ¥2 < âˆ,
f âˆˆR.
12The fact that
 W1
âˆ’W1 |Ë†x1(f)| df is ï¬nite follows from the ï¬niteness of
 W1
âˆ’W1 |Ë†x1(f)|2 df (which
follows from Parsevalâ€™s Theorem) and from Proposition 3.4.3. The same argument applies to x2.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

94
The Frequency Response of Filters and Bandlimited Signals
6.7
Bernsteinâ€™s Inequality
Bernsteinâ€™s Inequality captures the engineering intuition that the rate at which
a bandlimited signal can change is proportional to its bandwidth. The way the
theorem is phrased makes it clear that it is applicable both to integrable signals
that are bandlimited to W Hz and to energy-limited signals that are bandlimited
to W Hz.
Theorem 6.7.1 (Bernsteinâ€™s Inequality). If x can be written as
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR
for some integrable function g, then

dx(t)
dt
 â‰¤4Ï€W sup
Ï„âˆˆR
|x(Ï„)|,
t âˆˆR.
(6.76)
Proof. A proof of a slightly more general version of this theorem can be found in
(Pinsky, 2002, Chapter 2, Section 2.3.8).
6.8
Time-Limited and Bandlimited Signals
In this section we prove that no nonzero signal can be both time-limited and
bandlimited. We shall present two proofs. The ï¬rst is based on Theorem 6.8.1,
which establishes a connection between bandlimited signals and entire functions.
The second is based on the Fourier Series.
We remind the reader that a function Î¾: C â†’C is an entire function if it is
analytic throughout the complex plane.
Theorem 6.8.1. If x is an energy-limited signal that is bandlimited to W Hz, then
there exists an entire function Î¾: C â†’C that agrees with x on the real axis
Î¾(t + i0) = x(t),
t âˆˆR
(6.77)
and that satisï¬es
|Î¾(z)| â‰¤Î³ e2Ï€W|z|,
z âˆˆC,
(6.78)
where Î³ is some constant that can be taken as
âˆš
2W âˆ¥xâˆ¥2.
Proof. Let x be an energy-limited signal that is bandlimited to W Hz. By Propo-
sition 6.4.5 we can express x as
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR
(6.79)
for some square-integrable function g satisfying
 W
âˆ’W
|g(f)|2 df = âˆ¥xâˆ¥2
2 .
(6.80)
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.8 Time-Limited and Bandlimited Signals
95
Consider now the function Î¾: C â†’C deï¬ned by
Î¾(z) =
 W
âˆ’W
g(f) ei2Ï€fz df,
z âˆˆC.
(6.81)
This function is well-deï¬ned for every z âˆˆC because in the region of integration
the integrand can be bounded by
g(f) ei2Ï€fz = |g(f)| eâˆ’2Ï€f Im(z)
â‰¤|g(f)| e2Ï€|f| |Im(z)|
â‰¤|g(f)| e2Ï€ W |z|,
|f| â‰¤W,
(6.82)
and the RHS of (6.82) is integrable over the interval [âˆ’W, W ] by (6.80) and Propo-
sition 3.4.3.
By (6.79) and (6.81) it follows that Î¾ is an extension of the function x in the sense
of (6.77). It is but a technical matter to prove that Î¾ is analytic. One approach is
to prove that it is diï¬€erentiable at every z âˆˆC by verifying that the swapping of
diï¬€erentiation and integration, which leads to
dÎ¾
dz (z) =
 W
âˆ’W
g(f) (i2Ï€f) ei2Ï€fz df,
z âˆˆC
is justiï¬ed. See (Rudin, 1987, Section 19.1) for a diï¬€erent approach.
To prove (6.78) we compute
|Î¾(z)| =

 W
âˆ’W
g(f) ei2Ï€fz df

â‰¤
 W
âˆ’W
g(f) ei2Ï€fz df
â‰¤e2Ï€ W |z|
 W
âˆ’W
|g(f)| df
â‰¤e2Ï€W|z| âˆš
2W
 W
âˆ’W
|g(f)|2 df
=
âˆš
2W âˆ¥xâˆ¥2 e2Ï€W|z|,
where the inequality in the second line follows from Proposition 2.4.1; the inequality
in the third line from (6.82); the inequality in the fourth line from Proposition 3.4.3;
and the ï¬nal equality from (6.80).
Using Theorem 6.8.1 we can now easily prove the main result of this section.
Theorem 6.8.2. Let W and T be ï¬xed nonnegative real numbers. If x is an energy-
limited signal that is bandlimited to W Hz and that is time-limited in the sense that
it is zero for all t /âˆˆ[âˆ’T/2, T/2], then x(t) = 0 for all t âˆˆR.
By Note 6.4.12 this theorem also holds for integrable bandlimited signals.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

96
The Frequency Response of Filters and Bandlimited Signals
Proof. By Theorem 6.8.1 x can be extended to an entire function Î¾. Since x has
inï¬nitely many zeros in a bounded interval (e.g., for all t âˆˆ[T, 2T ]) and since Î¾
agrees with x on the real line, it follows that Î¾ also has inï¬nitely many zeros
in a bounded set (e.g., whenever z âˆˆ{w âˆˆC : Im(w) = 0, Re(w) âˆˆ[T, 2T] }).
Consequently, Î¾ is an entire function that has inï¬nitely many zeros in a bounded
subset of the complex plane and is thus the all-zero function (Rudin, 1987, Theo-
rem 10.18). But since x and Î¾ agree on the real line, it follows that x is also the
all-zero function.
Another proof can be based on the Fourier Series, which is discussed in the ap-
pendix. Starting from (6.79) we obtain that the time-Î·/(2W) sample of x(Â·) satisï¬es
1
âˆš
2W
x
 Î·
2W

=
 W
âˆ’W
g(f)
1
âˆš
2W
ei2Ï€fÎ·/(2W) df,
Î· âˆˆZ,
where we recognize the RHS of the above as the Î·-th Fourier Series Coeï¬ƒcient of
the function f 	â†’g(f) I{|f| â‰¤W} with respect to the interval [âˆ’W, W) (Note A.3.5
on Page 837). But since x(t) = 0 whenever |t| > T/2, it follows that all but a ï¬nite
number of these samples can be nonzero, thus leading us to conclude that all but a
ï¬nite number of the Fourier Series Coeï¬ƒcients of g(Â·) are zero. By the uniqueness
theorem for the Fourier Series (Theorem A.2.3) it follows that g(Â·) is equal to a
trigonometric polynomial (except possibly on a set of measure zero). Thus,
g(f) =
n

Î·=âˆ’n
aÎ· ei2Ï€Î·f/(2W),
f âˆˆ[âˆ’W, W ] \ N,
(6.83)
for some n âˆˆN; for some 2n + 1 complex numbers aâˆ’n, . . . , an; and for some set
N âŠ‚[âˆ’W, W ] of Lebesgue measure zero. Since the integral in (6.79) is insensitive
to the behavior of g on the set N, it follows from (6.79) and (6.83) that
x(t) =
 W
âˆ’W
ei2Ï€ft
n

Î·=âˆ’n
aÎ· ei2Ï€Î·f/(2W) df
=
n

Î·=âˆ’n
aÎ·
 âˆ
âˆ’âˆ
ei2Ï€f(t+
Î·
2W) I

|f| â‰¤W

df
= 2W
n

Î·=âˆ’n
aÎ· sinc(2Wt + Î·),
t âˆˆR,
i.e., that x is a linear combination of a ï¬nite number of time-shifted sinc(Â·) func-
tions. It now remains to show that no linear combination of a ï¬nite number of
time-shifted sinc(Â·) functions can be zero for all t âˆˆ[T, 2T ] unless it is zero for
all t âˆˆR. This can be established by extending the sincs to entire functions so
that the linear combination of the time-shifted sinc(Â·) functions is also an entire
function and by then calling again on the theorem that an entire function that has
inï¬nitely many zeros in a bounded subset of the complex plane must be the all-zero
function.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.9 A Theorem by Paley and Wiener
97
6.9
A Theorem by Paley and Wiener
The theorem of Paley and Wiener that we discuss next is important in the study
of bandlimited functions, but it will not be used in this book.
Theorem 6.8.1 showed that every energy-limited signal x that is bandlimited to W
Hz can be extended to an entire function Î¾ satisfying (6.78) for some constant Î³
by deï¬ning Î¾(z) as
Î¾(z) =
 W
âˆ’W
Ë†x(f) ei2Ï€fz df,
z âˆˆC.
(6.84)
The theorem of Paley and Wiener that we present next can be viewed as the
reverse statement. It demonstrates that if Î¾: C â†’C is an entire function that
satisï¬es (6.78) and whose restriction to the real axis is square integrable, then its
restriction to the real axis is an energy-limited signal that is bandlimited to W Hz
and, moreover, if we denote this restriction by x so x(t) = Î¾(t + i0) for all t âˆˆR,
then Î¾ is given by (6.84). This theorem demonstrates the close connection between
entire functions satisfying (6.78)â€”functions that are called entire functions of
exponential typeâ€”and energy-limited signals that are bandlimited to W Hz.
Theorem 6.9.1 (Paley-Wiener). If for some positive constants W and Î³ the entire
function Î¾: C â†’C satisï¬es
|Î¾(z)| â‰¤Î³ e2Ï€W|z|,
z âˆˆC
(6.85)
and if
 âˆ
âˆ’âˆ
|Î¾(t + i0)|2 dt < âˆ,
(6.86)
then there exists an energy-limited function g: R â†’C such that
Î¾(z) =
 W
âˆ’W
g(f) ei2Ï€fz df,
z âˆˆC.
(6.87)
Proof. See for example, (Rudin, 1987, Theorem 19.3) or (Katznelson, 2004, Sec-
tion VI.7) or (Dym and McKean, 1972, Section 3.3).
6.10
Picket Fences and Poisson Summation
Engineering textbooks often contain a useful expression for the FT of an inï¬nite
series of equally-spaced Diracâ€™s Deltas. Very roughly, the result is that the FT of
the mapping
t 	â†’
âˆ

j=âˆ’âˆ
Î´

t + jTs

is the mapping
f 	â†’1
Ts
âˆ

Î·=âˆ’âˆ
Î´

f + Î·
Ts

,
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

98
The Frequency Response of Filters and Bandlimited Signals
where Î´(Â·) denotes Diracâ€™s Delta. Needless to say, we are being extremely informal
because we said nothing about convergence. This result is sometimes called the
picket-fence miracle, because if we envision the plot of Diracâ€™s Delta as an
upward pointing bold arrow stemming from the origin, then the plot of a sum of
shifted Deltaâ€™s resembles a picket fence. The picket-fence miracle is that the FT
of a picket fence is yet another scaled picket fence; see (Oppenheim and Willsky,
1997, Chapter 4, Example 4.8 and also Chapter 7, Section 7.1.1.) or (Kwakernaak
and Sivan, 1991, Chapter 7, Example 7.4.19(c)).
In the mathematical literature, this result is called â€œthe Poisson summation for-
mula.â€ It states that under certain conditions on the function Ïˆ âˆˆL1,
âˆ

j=âˆ’âˆ
Ïˆ

jTs

= 1
Ts
âˆ

Î·=âˆ’âˆ
Ë†Ïˆ
 Î·
Ts

.
(6.88)
To identify the roots of (6.88) deï¬ne the mapping
Ï†(t) =
âˆ

j=âˆ’âˆ
Ïˆ

t + jTs

,
(6.89)
and note that this function is periodic in the sense that Ï†(t + Ts) = Ï†(t) for every
t âˆˆR. Consequently, it is instructive to study its Fourier Series on the interval
[âˆ’Ts/2, Ts/2] (Note A.3.5 in the appendix). Its Î·-th Fourier Series Coeï¬ƒcient with
respect to the interval [âˆ’Ts/2, Ts/2] is given by
 Ts/2
âˆ’Ts/2
Ï†(t)
1
âˆšTs
eâˆ’i2Ï€Î·t/Ts dt =
1
âˆšTs
 Ts/2
âˆ’Ts/2
âˆ

j=âˆ’âˆ
Ïˆ(t + jTs) eâˆ’i2Ï€Î·t/Ts dt
=
1
âˆšTs
âˆ

j=âˆ’âˆ
 Ts/2+jTs
âˆ’Ts/2+jTs
Ïˆ(Ï„) eâˆ’i2Ï€Î·(Ï„âˆ’jTs)/Ts dÏ„
=
1
âˆšTs
âˆ

j=âˆ’âˆ
 Ts/2+jTs
âˆ’Ts/2+jTs
Ïˆ(Ï„) eâˆ’i2Ï€Î·Ï„/Ts dÏ„
=
1
âˆšTs
 âˆ
âˆ’âˆ
Ïˆ(Ï„) eâˆ’i2Ï€Î·Ï„/Ts dÏ„
=
1
âˆšTs
Ë†Ïˆ
 Î·
Ts

,
Î· âˆˆZ,
where the ï¬rst equality follows from the deï¬nition of Ï†(Â·) (6.89); the second by
swapping the summation and the integration and by deï¬ning Ï„ â‰œt+jTs; the third
by the periodicity of the complex exponential; the fourth because summing the
integrals over disjoint intervals whose union is R is just the integral over R; and
the ï¬nal equality from the deï¬nition of the FT.
We can thus interpret the RHS of (6.88) as the evaluation13 at t = 0 of the Fourier
Series of Ï†(Â·) and the LHS as the evaluation of Ï†(Â·) at t = 0. Having established
13At t = 0 the complex exponentials are all equal to one, and the Fourier Series is thus just
the sum of the Fourier Series Coeï¬ƒcients.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.11 Additional Reading
99
the origin of the Poisson summation formula, we can now readily state conditions
that guarantee that it holds. An example of a set of conditions that guarantees
(6.88) is the following:
1) The function Ïˆ(Â·) is integrable.
2) The RHS of (6.89) converges at t = 0.
3) The Fourier Series of Ï†(Â·) converges at t = 0 to the value of Ï†(Â·) at t = 0.
We draw the readerâ€™s attention to the fact that it is not enough that both sides of
(6.88) converge absolutely and that both Ïˆ(Â·) and Ë†Ïˆ(Â·) be continuous; see (Katznel-
son, 2004, Chapter VI, Section 1, Exercise 15).
A setting where the above conditions are satisï¬ed and where (6.88) thus holds is
given in the following proposition.
Proposition 6.10.1. Let Ïˆ(Â·) be a continuous function satisfying
Ïˆ(t) =

0
if |t| â‰¥T,
 t
âˆ’T Î¾(Ï„) dÏ„
otherwise,
(6.90a)
where
 T
âˆ’T
|Î¾(Ï„)|2 dÏ„ < âˆ,
(6.90b)
and where T > 0 is some constant. Then for any Ts > 0
âˆ

j=âˆ’âˆ
Ïˆ

jTs

= 1
Ts
âˆ

Î·=âˆ’âˆ
Ë†Ïˆ
 Î·
Ts

.
(6.90c)
Proof. The integrability of Ïˆ(Â·) follows because Ïˆ(Â·) is continuous and zero outside
a ï¬nite interval. That the RHS of (6.89) converges at t = 0 follows because the
fact that Ïˆ(Â·) is zero outside the interval [âˆ’T, +T ] implies that only a ï¬nite number
of terms contribute to the sum at t = 0. That the Fourier Series of Ï†(Â·) converges
at t = 0 to the value of Ï†(Â·) at t = 0 follows from (Katznelson, 2004, Chapter I,
Section 6.2, Equation (6.2)) and from the corollary in (Katznelson, 2004, Chapter I,
Section 3.1).
6.11
Additional Reading
There are a number of excellent books on Fourier Analysis.
We mention here
(Katznelson, 2004), (Dym and McKean, 1972), (Pinsky, 2002), and (KÂ¨orner, 1988).
In particular, readers who would like to better understand how the FT is deï¬ned for
energy-limited functions that are not integrable may wish to consult (Katznelson,
2004, Chapter VI, Section 3.1) or (Dym and McKean, 1972, Sections 2.3â€“2.5).
Numerous surprising applications of the FT can be found in (KÂ¨orner, 1988).
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

100
The Frequency Response of Filters and Bandlimited Signals
Engineers often speak of the 2WT degrees of freedom that signals that are band-
limited and time-limited have. A good starting point for the literature on this is
(Slepian, 1976).
Bandlimited functions are intimately related to â€œentire functions of exponential
type.â€ For an accessible introduction to this concept see (Requicha, 1980); for a
more mathematical approach see (Boas, 1954).
6.12
Exercises
Exercise 6.1 (Symmetries of the FT). Let x: R â†’C be integrable, and let Ë†x be its FT.
(i) Show that if x is a real signal, then Ë†x is conjugate-symmetric, i.e., Ë†x(âˆ’f) = Ë†xâˆ—(f),
for every f âˆˆR.
(ii) Show that if x is purely imaginary (i.e., takes on only purely imaginary values),
then Ë†x is conjugate-antisymmetric, i.e., Ë†x(âˆ’f) = âˆ’Ë†xâˆ—(f), for every f âˆˆR.
(iii) Show that Ë†x can be written uniquely as the sum of a conjugate-symmetric function
gcs and a conjugate-antisymmetric function gcas. Express gcs & gcas in terms of Ë†x.
Exercise 6.2 (Diï¬€erentiating the FT). Show that if both x and t â†’t x(t) are integrable,
then Ë†x is diï¬€erentiable and the FT of t â†’t x(t) is
f â†’âˆ’1
i2Ï€
dË†x(f)
df
.
(See, for example, (Katznelson, 2004, Chapter VI, Section 1, Theorem 1.6).)
Exercise 6.3 (The L2-Fourier Transform and Its Inverse). Prove that the Inverse L2-
Fourier Transform is the reï¬‚ection of the L2-Fourier Transform:
|
[x] = ~#
[x].
Hint: Use Theorem 6.2.8.
Exercise 6.4 (The L2-Fourier Transform, Delays, and Frequency Shifts). Let x: t â†’x(t)
and g: f â†’g(f) be energy-limited with the L2-Fourier Transform of [x] being [g].
(i) Let f0 âˆˆR be ï¬xed. Prove that the L2-Fourier Transform of the equivalence class of
the mapping t â†’x(t) ei2Ï€f0t is the equivalence class of the mapping f â†’g(f âˆ’f0).
(ii) Let Ï„ âˆˆR be ï¬xed. Prove that the L2-Fourier Transform of the equivalence class of
the mapping t â†’x(t âˆ’Ï„) is the equivalence class of the mapping f â†’g(f) eâˆ’i2Ï€fÏ„.
Hint: For Part (ii) use Part (i) and Exercise 6.3.
Exercise 6.5 (Reconstructing a Function from Its IFT). Formulate and prove a result
analogous to Theorem 6.2.12 for the Inverse Fourier Transform.
Exercise 6.6 (Eigenfunctions of the FT). Show that if the energy-limited signal x satisï¬es
Ë†x = Î»x for some Î» âˆˆC, then Î» can only be Â±1 or Â±i. (The Hermite functions are such
signals.)
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.12 Exercises
101
Exercise 6.7 (Delaying a Sinc). For W, Î´ > 0 compute
 âˆ
âˆ’âˆ
	
sinc

2Wt

âˆ’sinc

2W(t âˆ’Î´)

2
dt
as a function of W and Î´. Study the result for the cases where Î´ W â‰ª1 and Î´ W â‰«1.
Exercise 6.8 (Existence of a Stable Filter (1)). Let W > 0 be given. Does there exist a
stable ï¬lter whose frequency response is zero for |f| â‰¤W and is one for W < f â‰¤2W ?
Exercise 6.9 (Existence of a Stable Filter (2)). Let W > 0 be given. Does there exist a
stable ï¬lter whose frequency response equals cos(f) for all |f| â‰¥W ?
Exercise 6.10 (Existence of an Energy-Limited Signal). Argue that there exists an energy-
limited signal x whose FT is (the equivalence class of) the mapping f â†’eâˆ’f I{f â‰¥0}.
What is the energy in x? What is the energy in the result of feeding x to an ideal unit-gain
lowpass ï¬lter of cutoï¬€frequency Wc = 1?
Exercise 6.11 (Delaying a Bandlimited Signal). Let x be an energy-limited signal that
is bandlimited to W Hz. For any Î´ âˆˆR, let xÎ´ denote the result of delaying x by Î´, so
xÎ´ : t â†’x(t âˆ’Î´). Prove that
âˆ¥x âˆ’xÎ´âˆ¥2 â‰¤min

2, 2Ï€WÎ´

âˆ¥xâˆ¥2 .
Hint: First argue that, irrespective of W and Î´, we have âˆ¥x âˆ’xÎ´âˆ¥2 â‰¤2 âˆ¥xâˆ¥2. Then use
Parsevalâ€™s Theorem to treat the case WÎ´ < 1/2.
Exercise 6.12 (The Derivative of Bandlimited Signals).
(i) Let the signal x be given by
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR,
where g is integrable. Show that x is diï¬€erentiable and that
dx(t)
dt
=
 W
âˆ’W
i2Ï€f g(f) ei2Ï€ft df,
t âˆˆR.
(ii) Conclude that if x is an energy-limited signal that is bandlimited to W Hz, then so
is its derivative.
Hint: Mathematically inclined readers can prove Part (i) using the Dominated Conver-
gence Theorem and Exercise 2.7.
Exercise 6.13 (The Gram-Schmidt Procedure and Bandwidth). Let U be a d-dimensional
linear subspace of L2 that does not contain any zero-energy signals other than the all-zero
signal. Let (u1, . . . , ud) be a basis for U, where u1, . . . , ud are all energy-limited signals
that are bandlimited to W Hz (with some of the signals possibly being of bandwidth
smaller than W).
The d-tuple (u1, . . . , ud) is fed to the Gram-Schmidt Procedure to
produce the orthonormal basis (Ï†1, . . . , Ï†d).
(i) Prove that Ï†1, . . . , Ï†d are energy-limited signals that are bandlimited to W Hz.
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

102
The Frequency Response of Filters and Bandlimited Signals
(ii) How does the largest of the bandwidths of u1, . . . , ud compare with that of the
bandwidths of Ï†1, . . . , Ï†d?
Exercise 6.14 (Passive Filters). Let h be the impulse response of a stable ï¬lter. Show
that the condition that â€œfor every x âˆˆL2 the energy in x â‹†h does not exceed the energy
in xâ€ is equivalent to the condition
Ë†h(f)
 â‰¤1,
f âˆˆR.
Exercise 6.15 (Real and Imaginary Parts of Bandlimited Signals). Show that if x(Â·) is an
integrable signal that is bandlimited to W Hz, then so are its real and imaginary parts.
Exercise 6.16 (Inner Products and Filtering). Let x be an energy-limited signal that is
bandlimited to W Hz. Show that
âŸ¨x, yâŸ©=

x, y â‹†LPFW

,
y âˆˆL2.
Exercise 6.17 (Squaring a Signal). Show that if x is an energy-limited signal that is
bandlimited to W Hz, then t â†’x2(t) is an integrable signal that is bandlimited to 2W
Hz.
Exercise 6.18 (Squared sinc(Â·)). Find the FT and IFT of the mapping t â†’sinc2(t).
Exercise 6.19 (A Stable Filter). Show that the IFT of the function
g0 : f â†’
â§
âª
â¨
âª
â©
1
if |f| â‰¤a
bâˆ’|f|
bâˆ’a
if a < |f| < b
0
otherwise
is given by
Ë‡g0 : t â†’
1
(Ï€t)2
cos(2Ï€at) âˆ’cos(2Ï€bt)
2(b âˆ’a)
and that this signal is integrable. Here b > a > 0.
Exercise 6.20 (Filtering Integrable Signals). Prove that feeding an integrable signal to
an ideal unit-gain lowpass ï¬lter need not produce an integrable signal.
Hint: Construct an example using Ë‡g0 of Exercise 6.19.
Exercise 6.21 (Multiplying Bandlimited Signals by a Carrier). Let x be an integrable
signal that is bandlimited to W Hz.
(i) Show that if fc > W, then
 âˆ
âˆ’âˆ
x(t) cos(2Ï€fct) dt =
 âˆ
âˆ’âˆ
x(t) sin(2Ï€fct) dt = 0.
(ii) Show that if fc > W/2, then
 âˆ
âˆ’âˆ
x(t) cos2(2Ï€fct) dt = 1
2
 âˆ
âˆ’âˆ
x(t) dt.
.008
14:26:31, subject to the Cambridge Core terms of use, available at

6.12 Exercises
103
Exercise 6.22 (An Identity). Prove that for every W âˆˆR
sinc(2Wt) cos(2Ï€Wt) = sinc(4Wt),
t âˆˆR.
Illustrate the identity in the frequency domain.
Exercise 6.23 (Filtering a Periodic Signal). Let x be a bounded periodic signal with period
T > 0, and let h be integrable. Show that the Î·-th Fourier Series Coeï¬ƒcient of x â‹†h with
respect to the interval [âˆ’T/2, T/2] (Note A.3.5) is the product of the corresponding Î·-th
Fourier Series Coeï¬ƒcient of x and Ë†h(Î·/T).
Exercise 6.24 (A Superposition of Time Shifts of a Pulse). Let g be an integrable signal
of bandwidth W Hz. Let cm, . . . , cn be complex numbers that are not all zero, where
m and n are integers satisfying n â‰¥m.
Prove that, for every positive Ts, the signal
x: t â†’n
â„“=m câ„“g(t âˆ’â„“Ts) is also an integrable signal of bandwidth W Hz.
Hint: There can be at most a ï¬nite number of frequencies where Ë†x is zero but Ë†g is not.
Exercise 6.25 (Picket Fences). If you are familiar with Diracâ€™s Delta, explain how (6.88)
is related to the heuristic statement that the FT of 
jâˆˆZ Î´(t+jTs) is Tâˆ’1
s

Î·âˆˆZ Î´(f +Î·/Ts).
Exercise 6.26 (Bounding the Derivative). Show that if x is an energy-limited signal that
is bandlimited to W Hz, then its time-t derivative satisï¬es

dx(t)
dt
 â‰¤

8
3 Ï€ W 3/2 âˆ¥xâˆ¥2 ,
t âˆˆR.
Hint: Use Proposition 6.4.5 and the Cauchy-Schwarz Inequality
Exercise 6.27 (Another Notion of Bandwidth). Let U denote the set of all energy-limited
signals u such that at least 90% of the energy of u is contained in the band [âˆ’W, W ].
Is U a linear subspace of L2?
.008
14:26:31, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

Chapter 7
Passband Signals and Their Representation
7.1
Introduction
The signals encountered in wireless communications are typically real passband
signals. In this chapter we shall deï¬ne such signals and deï¬ne their bandwidth
around a carrier frequency. We shall then explain how such signals can be rep-
resented using their complex baseband representation. We shall emphasize two
relationships: that between the energy in the passband signal and in its baseband
representation, and that between the bandwidth of the passband signal around the
carrier frequency and the bandwidth of its baseband representation. We ask the
reader to pay special attention to the fact that only real passband signals have a
baseband representation.
Most of the chapter deals with the family of integrable passband signals. As we
shall see in Corollary 7.2.4, an integrable passband signal must have ï¬nite energy,
and this family is thus a subset of the family of energy-limited passband signals.
Restricting ourselves to integrable signalsâ€”while reducing the generality of some of
the resultsâ€”simpliï¬es the exposition because we can discuss the Fourier Transform
without having to resort to the L2-Fourier Transform, which requires all statements
to be phrased in terms of equivalence classes. But most of the derived results will
also hold for the more general family of energy-limited passband signals with only
slight modiï¬cations. The required modiï¬cations are discussed in Section 7.7.
7.2
Baseband and Passband Signals
Integrable signals that are bandlimited to W Hz were deï¬ned in Deï¬nition 6.4.9. By
Proposition 6.4.10, an integrable signal x is bandlimited to W Hz if it is continuous
and if its FT is zero for all frequencies outside the band [âˆ’W, W ]. The bandwidth
of x is the smallest W to which it is bandlimited (Deï¬nition 6.4.13). As an example,
Figure 7.1 depicts the FT Ë†x of a real signal x, which is bandlimited to W Hz.
Since the signal x in this example is real, its FT is conjugate-symmetric, (i.e.,
Ë†x(âˆ’f) = Ë†xâˆ—(f) for all frequencies f âˆˆR). Thus, the magnitude of Ë†x is symmetric
(even), i.e., |Ë†x(f)| = |Ë†x(âˆ’f)|, but its phase is anti-symmetric (odd). In the ï¬gure
dashed lines indicate this conjugate symmetry.
Consider now the real signal y whose FT Ë†y is depicted in Figure 7.2. Again, since
104
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.2 Baseband and Passband Signals
105
W
f
W
âˆ’W
Ë†x(f)
Figure 7.1: The FT Ë†x of a real bandwidth-W baseband signal x.
W
f
fc
âˆ’fc
fc + W
2
fc âˆ’W
2
Ë†y(f)
Figure 7.2: The FT Ë†y of a real passband signal y that is bandlimited to W Hz
around the carrier frequency fc.
the signal is real, its FT is conjugate-symmetric, and hence the dashed lines. This
signal (if continuous) is bandlimited to fc +W/2 Hz. But note that Ë†y(f) = 0 for all
frequencies f in the interval |f| < fcâˆ’W/2. Signals such as y are often encountered
in wireless communications, because in a wireless channel the very-low frequencies
often suï¬€er severe attenuation and are therefore seldom used.
Another reason
is the concurrent use of the wireless spectrum by many systems. If all systems
transmitted in the same frequency band, they would interfere with each other.
Consequently, diï¬€erent systems are often assigned diï¬€erent carrier frequencies so
that their transmitted signals will not overlap in frequency. This is why diï¬€erent
radio stations transmit around diï¬€erent carrier frequencies.
7.2.1
Deï¬nition and Characterization
To describe signals such as y we use the following deï¬nition for passband signals.
We ask the reader to recall the deï¬nition of the impulse response BPFW,fc(Â·) (see
(5.21)) and of the frequency response 
BPFW,fc(Â·) (see (6.41)) of the ideal unit-gain
bandpass ï¬lter of bandwidth W around the carrier frequency fc.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

106
Passband Signals and Their Representation
Deï¬nition 7.2.1 (A Passband Signal). A signal xPB is said to be an integrable
passband signal that is bandlimited to W Hz around the carrier fre-
quency fc if it is integrable
xPB âˆˆL1;
(7.1a)
the carrier frequency fc satisï¬es
fc > W
2 > 0;
(7.1b)
and if xPB is unaltered when it is fed to an ideal unit-gain bandpass ï¬lter of band-
width W around the carrier frequency fc
xPB(t) =

xPB â‹†BPFW,fc

(t),
t âˆˆR.
(7.1c)
An energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc is analogously deï¬ned but with (7.1a) replaced by the
condition
xPB âˆˆL2.
(7.1aâ€™)
(That the convolution in (7.1c) is deï¬ned at every t âˆˆR whenever xPB is integrable
can be shown using Proposition 6.2.5 because BPFW,fc is the Inverse Fourier Trans-
form of the integrable function f 	â†’I
|f| âˆ’fc
 â‰¤W/2

. That the convolution is
deï¬ned at every t âˆˆR also when xPB is of ï¬nite energy can be shown by noting
that BPFW,fc is of ï¬nite energy, and the convolution of two ï¬nite-energy signals is
deï¬ned at every time t âˆˆR; see Section 5.5.)
In analogy to Proposition 6.4.10 we have the following characterization:
Proposition 7.2.2 (Characterizing Integrable Passband Signals). Let fc and W
satisfy fc > W/2 > 0. If xPB is an integrable signal, then each of the following
statements is equivalent to the statement that xPB is an integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc.
(a) The signal xPB is unaltered when it is bandpass ï¬ltered:
xPB(t) =

xPB â‹†BPFW,fc

(t),
t âˆˆR.
(7.2)
(b) The signal xPB can be expressed as
xPB(t) =

||f|âˆ’fc|â‰¤W/2
Ë†xPB(f) ei2Ï€ft df,
t âˆˆR.
(7.3)
(c) The signal xPB is continuous and
Ë†xPB(f) = 0,
|f| âˆ’fc
 > W
2 .
(7.4)
(d) There exists an integrable function g such that
xPB(t) =

||f|âˆ’fc|â‰¤W/2
g(f) ei2Ï€ft df,
t âˆˆR.
(7.5)
Proof. The proof is similar to the proof of Proposition 6.4.10 and is omitted.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.3 Bandwidth around a Carrier Frequency
107
7.2.2
Important Properties
By comparing (7.4) with (6.62) we obtain:
Corollary 7.2.3 (Integrable Passband Signals Are Bandlimited). If xPB is an
integrable passband signal that is bandlimited to W Hz around the carrier frequency
fc, then it is an integrable signal that is bandlimited to fc + W/2 Hz.
Using Corollary 7.2.3 and Note 6.4.12 we obtain:
Corollary 7.2.4 (Integrable Passband Signals Are of Finite Energy). Any inte-
grable passband signal that is bandlimited to W Hz around the carrier frequency fc
is of ï¬nite energy.
Proposition 7.2.5 (Integrable Passband Signals through Stable Filters). If xPB
is an integrable passband signal that is bandlimited to W Hz around the carrier
frequency fc, and if h âˆˆL1 is the impulse response of a stable ï¬lter, then the
convolution xPB â‹†h is deï¬ned at every epoch; it is an integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc; and its FT is the
mapping f 	â†’Ë†xPB(f) Ë†h(f).
Proof. The proof is similar to the proof of the analogous result for bandlimited
signals (Proposition 6.5.2) and is omitted.
7.3
Bandwidth around a Carrier Frequency
Deï¬nition 7.3.1 (The Bandwidth around a Carrier Frequency). The bandwidth
around the carrier fc of an integrable or energy-limited passband signal xPB is
the smallest W for which both (7.1b) and (7.1c) hold.
Note 7.3.2 (The Carrier Frequency Is Critical). The bandwidth of xPB around
the carrier frequency fc is determined not only by the FT of xPB but also by fc.
For example, the real passband signal whose FT is depicted in Figure 7.3 is of
bandwidth W around the carrier frequency fc, but its bandwidth is smaller around
a slightly higher carrier frequency.
At ï¬rst it may seem that the deï¬nition of bandwidth for passband signals is incon-
sistent with the deï¬nition for baseband signals. This, however, is not the case. A
good way to remember the deï¬nitions is to focus on real signals. For such signals
the bandwidth for both baseband and passband signals is deï¬ned as the length of
an interval of positive frequencies where the FT of the signal may be nonzero. For
baseband signals the bandwidth is the length of the smallest interval of positive
frequencies of the form [0, W] containing all positive frequencies where the FT may
be nonzero. For passband signals it is the length of the smallest interval of positive
frequencies that is symmetric around the carrier frequency fc and that contains
all positive frequencies where the signal may be nonzero. (For complex signals we
have to allow for the fact that the zeros of the FT may not be symmetric sets
around the origin.) See also Figures 6.2 and 6.3.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

108
Passband Signals and Their Representation
W
W
W
âˆ’W
f
f
W
fc
âˆ’fc
fc + W
2
fc âˆ’W
2
Figure 7.3: The FT of a complex baseband signal of bandwidth W Hz (above)
and of a real passband signal of bandwidth W Hz around the carrier frequency fc
(below).
We draw the readerâ€™s attention to an important consequence of our deï¬nition of
bandwidth:
Proposition 7.3.3 (Multiplication by a Carrier Doubles the Bandwidth). If x is
an integrable signal of bandwidth W Hz and if fc > W, then t 	â†’x(t) cos(2Ï€fct) is
an integrable passband signal of bandwidth 2W around the carrier frequency fc.
Proof. Deï¬ne y: t 	â†’x(t) cos(2Ï€fct). The proposition is a straightforward conse-
quence of the deï¬nition of the bandwidth of x (Deï¬nition 6.4.13); the deï¬nition of
the bandwidth of y around the carrier frequency fc (Deï¬nition 7.3.1); and the fact
that if x is an integrable signal of FT Ë†x, then y is an integrable signal of FT
Ë†y(f) = 1
2

Ë†x(f âˆ’fc) + Ë†x(f + fc)

,
f âˆˆR,
(7.6)
where (7.6) follows from the calculation
Ë†y(f) =
 âˆ
âˆ’âˆ
y(t) eâˆ’i2Ï€ft dt
=
 âˆ
âˆ’âˆ
x(t) cos(2Ï€fct) eâˆ’i2Ï€ft dt
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.3 Bandwidth around a Carrier Frequency
109
W
f
W
âˆ’W
1
Ë†x(f)
Figure 7.4: The FT of a complex baseband bandwidth-W signal x.
2W
f
fc
fc + W
fc âˆ’W
1
2
Ë†y(f)
Figure 7.5: The FT of y: t 	â†’x(t) cos (2Ï€fct), where Ë†x is as depicted in Figure 7.4.
Note that x is of bandwidth W and that y is of bandwidth 2W around the carrier
frequency fc.
=
 âˆ
âˆ’âˆ
x(t) ei2Ï€fct + eâˆ’i2Ï€fct
2
eâˆ’i2Ï€ft dt
= 1
2
 âˆ
âˆ’âˆ
x(t) eâˆ’i2Ï€(fâˆ’fc)t dt + 1
2
 âˆ
âˆ’âˆ
x(t) eâˆ’i2Ï€(f+fc)t dt
= 1
2

Ë†x(f âˆ’fc) + Ë†x(f + fc)

,
f âˆˆR.
As an illustration of the relation (7.6) note that if x is the complex bandwidth-W
signal whose FT is depicted in Figure 7.4, then the signal y: t 	â†’x(t) cos(2Ï€fct) is
the complex passband signal of bandwidth 2W around fc whose FT is depicted in
Figure 7.5.
Similarly, if x is the real baseband signal of bandwidth W whose FT is depicted
in Figure 7.6, then y: t 	â†’x(t) cos(2Ï€fct) is the real passband signal of bandwidth
2W around fc whose FT is depicted in Figure 7.7.
In wireless applications the bandwidth W of the signals around the carrier frequency
is typically much smaller than the carrier frequency fc, but for most of our results
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

110
Passband Signals and Their Representation
W
f
W
âˆ’W
1
Ë†x(f)
Figure 7.6: The FT of a real baseband bandwidth-W signal x.
2W
f
fc
fc + W
fc âˆ’W
1
2
Ë†y(f)
Figure 7.7: The FT of y: t 	â†’x(t) cos (2Ï€fct), where Ë†x is as depicted in Figure 7.6.
Here x is of bandwidth W and y is of bandwidth 2W around the carrier frequency
fc.
it suï¬ƒces that (7.1b) hold.
The notion of a passband signal is also applied somewhat loosely in instances where
the signals are not bandlimited. Engineers say that an energy-limited signal is a
passband signal around the carrier frequency fc if most of its energy is contained
in frequencies that are close to fc and âˆ’fc. Notice that in this â€œdeï¬nitionâ€ we are
relying heavily on Parsevalâ€™s theorem. I.e., we think about the energy âˆ¥xâˆ¥2
2 of x as
being computed in the frequency domain, i.e., by computing âˆ¥Ë†xâˆ¥2
2 =

|Ë†x(f)|2 df.
By â€œmost of the energy is contained in frequencies that are close to fc and âˆ’fcâ€
we thus mean that most of the contributions to this integral come from small
frequency intervals around fc and âˆ’fc. In other words, we say that x is a passband
signal whose energy is mostly concentrated in a bandwidth W around the carrier
frequency fc if
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df â‰ˆ

||f|âˆ’fc|â‰¤W/2
|Ë†x(f)|2 df.
(7.7)
Similarly, a signal is approximately a baseband signal that is bandlimited to W Hz
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.4 Real Passband Signals
111
if
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df â‰ˆ
 W
âˆ’W
|Ë†x(f)|2 df.
(7.8)
7.4
Real Passband Signals
Before discussing the baseband representation of real passband signals we empha-
size the following.
(i) The passband signals transmitted and received in Digital Communications
are real.
(ii) Only real passband signals have a baseband representation.
(iii) The baseband representation of a real passband signal is typically a complex
signal.
(iv) While the FT of real signals is conjugate-symmetric (6.3), this does not imply
any symmetry with respect to the carrier frequency. Thusâ€”although not
symmetric around fcâ€”the FT depicted in Figure 7.2 is that of a real passband
signal that is bandlimited to W Hz around fc.
We also note that if x is a real integrable signal, then its FT must be conjugate-
symmetric. But if g âˆˆL1 is such that its IFT Ë‡g is real, it does not follow that g
must be conjugate-symmetric.
For example, the conjugate symmetry could be
broken on a set of frequencies of Lebesgue measure zero, a set that does not inï¬‚u-
ence the IFT. As the next proposition shows, this is the only way the conjugate
symmetry can be broken.
Proposition 7.4.1. If x is a real signal and if x = Ë‡g for some integrable function
g: f 	â†’g(f), then:
(i) The signal x can be represented as the IFT of a conjugate-symmetric inte-
grable function.
(ii) The function g and the conjugate-symmetric function f 	â†’

g(f)+gâˆ—(âˆ’f)

/2
agree except on a set of frequencies of Lebesgue measure zero.
Proof. (i) Since x is real and since x = Ë‡g it follows that
x(t) = Re

x(t)

= 1
2x(t) + 1
2xâˆ—(t)
= 1
2
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df + 1
2
	 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df

âˆ—
= 1
2
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df + 1
2
 âˆ
âˆ’âˆ
gâˆ—(f) eâˆ’i2Ï€ft df
= 1
2
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df + 1
2
 âˆ
âˆ’âˆ
gâˆ—(âˆ’Ëœf) ei2Ï€ Ëœ
ft d Ëœf
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

112
Passband Signals and Their Representation
=
 âˆ
âˆ’âˆ
g(f) + gâˆ—(âˆ’f)
2
ei2Ï€ft df,
t âˆˆR,
where the ï¬rst equality follows from the hypothesis that x is a real signal; the second
because for any z âˆˆC we have Re(z) = (z + zâˆ—)/2; the third by the hypothesis
that x = Ë‡g; the fourth because conjugating a complex integral is tantamount
to conjugating the integrand (Proposition 2.3.1 (ii)); the ï¬fth by changing the
integration variable in the second integral to Ëœf â‰œâˆ’f; and the sixth by combining
the integrals. Thus, x is the IFT of the conjugate-symmetric function deï¬ned by
f 	â†’

g(f) + gâˆ—(âˆ’f)

/2, and (i) is established.
As to (ii), since x is the IFT of both g and f 	â†’

g(f) + gâˆ—(âˆ’f)

/2, it follows from
the IFT analog of Theorem 6.2.12 that the two agree outside a set of frequencies
of Lebesgue measure zero.
7.5
The Analytic Signal
In this section we shall deï¬ne the analytic representation of a real passband
signal.
This is also sometimes called the analytic signal associated with the
signal. We shall use the two terms interchangeably. The analytic representation
will serve as a steppingstone to the baseband representation, which is extremely
important in Digital Communications. We emphasize that an analytic signal can
only be associated with a real passband signal. The analytic signal itself, however,
is complex-valued.
7.5.1
Deï¬nition and Characterization
Let xPB be a real integrable passband signal that is bandlimited to W Hz around
the carrier frequency fc. We would have liked to deï¬ne its analytic representation
as the complex signal xA whose FT is the mapping
f 	â†’Ë†xPB(f) I{f â‰¥0},
(7.9)
i.e., as the integrable signal whose FT is equal to zero at negative frequencies and to
Ë†xPB(f) at nonnegative frequencies. While this is often the way we think about xA,
there are two problems with this deï¬nition: an existence problem and a uniqueness
problem. It is not prima facie clear that there exists an integrable signal whose FT
is the mapping (7.9). (We shall soon see that there does.) And, since two signals
that diï¬€er on a set of Lebesgue measure zero have identical Fourier Transforms, the
above deï¬nition would not fully specify xA. This could be remedied by insisting
that xA be continuous, but this would further exacerbate the existence issue. (We
shall see that there does exist a unique integrable continuous signal whose FT is
the mapping (7.9), but this requires proof.) Our approach is to deï¬ne xA as the
IFT of the mapping (7.9) and to then explore the properties of xA.
Deï¬nition 7.5.1 (Analytic Representation of a Real Passband Signal). The an-
alytic representation of a real integrable passband signal xPB that is bandlimited
to W Hz around the carrier frequency fc is the complex signal xA deï¬ned by
xA(t) â‰œ
 âˆ
0
Ë†xPB(f) ei2Ï€ft df,
t âˆˆR.
(7.10)
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.5 The Analytic Signal
113
Note that, by Proposition 7.2.2, Ë†xPB(f) vanishes at frequencies f that satisfy
|f| âˆ’fc
 > W/2, so we can also write (7.10) as
xA(t) =
 fc+ W
2
fcâˆ’W
2
Ë†xPB(f) ei2Ï€ft df,
t âˆˆR.
(7.11)
This latter expression has the advantage that it makes it clear that the integral is
well-deï¬ned for every t âˆˆR, because the integrability of xPB implies that the inte-
grand is bounded, i.e., that |Ë†xPB(f)| â‰¤âˆ¥xPBâˆ¥1 for every f âˆˆR (Theorem 6.2.11)
and hence that the mapping f 	â†’Ë†xPB(f) I{|f âˆ’fc| â‰¤W/2} is integrable.
Also note that our deï¬nition of the analytic signal may be oï¬€by a factor of two
or
âˆš
2 from the one used in some textbooks. (Some textbooks introduce a factor
of
âˆš
2 in order to make the energy in the analytic signal equal that in the passband
signal. We do not do so and hence end up with a factor of two in (7.24) ahead.)
We next show that the analytic signal xA is a continuous and integrable signal and
that its FT is given by the mapping (7.9). In fact, we prove more.
Proposition 7.5.2 (Characterizations of the Analytic Signal). Let xPB be a real
integrable passband signal that is bandlimited to W Hz around the carrier fre-
quency fc. Then each of the following statements is equivalent to the statement
that the complex-valued signal xA is its analytic representation.
(a) The signal xA is given by
xA(t) =
 fc+ W
2
fcâˆ’W
2
Ë†xPB(f) ei2Ï€ft df,
t âˆˆR.
(7.12)
(b) The signal xA is a continuous integrable signal satisfying
Ë†xA(f) =

Ë†xPB(f)
if f â‰¥0,
0
otherwise.
(7.13)
(c) The signal xA is an integrable passband signal that is bandlimited to W Hz
around the carrier frequency fc and that satisï¬es (7.13).
(d) The signal xA is given by
xA = xPB â‹†Ë‡g
(7.14a)
for every integrable mapping g: f 	â†’g(f) satisfying
g(f) = 1,
f âˆ’fc
 â‰¤W
2 ,
(7.14b)
and
g(f) = 0,
f + fc
 â‰¤W
2
(7.14c)
(with g(f) unspeciï¬ed at other frequencies).
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

114
Passband Signals and Their Representation
Proof. That Condition (a) is equivalent to the statement that xA is the analytic
representation of xPB is just a restatement of Deï¬nition 7.5.1. It thus only remains
to show that Conditions (a), (b), (c), and (d) are equivalent. We shall do so by
establishing that (a) â‡”(d); that (b) â‡”(c); that (b) â‡’(a); and that (d) â‡’(c).
To establish (a) â‡”(d), let g: f 	â†’g(f) be any integrable function satisfying (7.14b)
and (7.14c). Since both xPB and g are integrable, we can compute xPB â‹†Ë‡g using
Proposition 6.2.5:

xPB â‹†Ë‡g

(t) =
 âˆ
âˆ’âˆ
Ë†xPB(f) g(f) ei2Ï€ft df
=
 âˆ
0
Ë†xPB(f) g(f) ei2Ï€ft df
=
 fc+ W
2
fcâˆ’W
2
Ë†xPB(f) g(f) ei2Ï€ft df
=
 fc+ W
2
fcâˆ’W
2
Ë†xPB(f) ei2Ï€ft df,
t âˆˆR,
(7.15)
where the ï¬rst equality follows from Proposition 6.2.5; the second because the
assumption that xPB is a passband signal implies, by Proposition 7.2.2 (cf. (c)),
that the only negative frequencies f < 0 where Ë†xPB(f) can be nonzero are those
satisfying | âˆ’f âˆ’fc| â‰¤W/2, and at those frequencies g is zero by (7.14c); the
third by Proposition 7.2.2 (cf. (c)); and the fourth equality by (7.14b). The LHS
of (7.15) is thus equal to xA(t) if, and only if, the RHS is equal to xA(t). This
establishes that (a) â‡”(d).
The equivalence (b) â‡”(c) is an immediate consequence of Proposition 7.2.2. That
(b) â‡’(a) can be proved using Corollary 6.2.14 as follows. If (b) holds, then xA
is a continuous integrable signal whose FT is given by the integrable function on
the RHS of (7.13) and therefore, by Corollary 6.2.14, xA is the IFT of the RHS of
(7.13), thus establishing (a).
We now complete the proof by showing that (d) â‡’(c). To this end let g: f 	â†’g(f)
be a continuous integrable function satisfying (7.14b) & (7.14c) and additionally
satisfying that its IFT Ë‡g is integrable. For example, g could be the function
g(f) =
â§
âª
â¨
âª
â©
1
if |f âˆ’fc| â‰¤W/2,
0
if |f âˆ’fc| â‰¥Wc/2,
Wcâˆ’2|fâˆ’fc|
Wcâˆ’W
otherwise,
(7.16)
where Wc can be chosen arbitrarily in the range
W < Wc < 2fc.
(7.17)
This function is depicted in Figure 7.8. By direct calculation, it can be shown that
its IFT is given by1
Ë‡g(t) = ei2Ï€fct
1
(Ï€t)2
cos(Ï€ W t) âˆ’cos(Ï€ Wc t)
Wc âˆ’W
,
t âˆˆR,
(7.18)
1At t = 0, the RHS of (7.18) should be interpreted as (W + Wc)/2.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.5 The Analytic Signal
115
Wc
f
W
fc
1
g(f)
Figure 7.8: The function g of (7.16), which is used in the proof of Proposition 7.5.2.
which is integrable. Deï¬ne h = Ë‡g, and note that, by Corollary 6.2.14 (ii), Ë†h = g.
If (d) holds, then
xA = xPB â‹†Ë‡g
= xPB â‹†h,
so xA is the result of feeding an integrable passband signal that is bandlimited
to W Hz around the carrier frequency fc (the signal xPB) through a stable ï¬lter
(of impulse response h). Consequently, by Proposition 7.2.5, xA is an integrable
passband signal that is bandlimited to W Hz around the carrier frequency fc and
its FT is given by f 	â†’Ë†xPB(f) Ë†h(f). Hence, as we next justify,
Ë†xA(f) = Ë†xPB(f) Ë†h(f)
= Ë†xPB(f) g(f)
= Ë†xPB(f) g(f) I{f â‰¥0}
= Ë†xPB(f) I{f â‰¥0},
f âˆˆR,
thus establishing (c). Here the third equality is justiï¬ed by noting that the as-
sumption that xPB is a passband signal implies, by Proposition 7.2.2 (cf. (c)),
that the only negative frequencies f < 0 where Ë†xPB(f) can be nonzero are those
satisfying |âˆ’f âˆ’fc| â‰¤W/2, and at those frequencies g is zero by (7.16), (7.17),
and (7.1b). The fourth equality follows by noting that the assumption that xPB
is a passband signal implies, by Proposition 7.2.2 (cf. (c)), that the only positive
frequencies f > 0 where Ë†xPB(f) can be nonzero are those satisfying |f âˆ’fc| â‰¤W/2
and at those frequencies g(f) = 1 by (7.16).
7.5.2
From xA back to xPB
Proposition 7.5.2 describes the analytic representation xA in terms of the real
passband signal xPB. This representation would have been useless if we had not
been able to recover xPB from xA. Fortunately, we can. The key is that, because
xPB is real, its FT is conjugate-symmetric
Ë†xPB(âˆ’f) = Ë†xâˆ—
PB(f),
f âˆˆR.
(7.19)
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

116
Passband Signals and Their Representation
Consequently, since the FT of xA is equal to that of xPB at the positive frequencies
and to zero at the negative frequencies (7.13), we can add to Ë†xA its conjugated
mirror-image to obtain Ë†xPB:
Ë†xPB(f) = Ë†xA(f) + Ë†xâˆ—
A(âˆ’f),
f âˆˆR;
(7.20)
see Figure 7.9 on Page 121. From here it is just a technicality to obtain the time-
domain relationship
xPB(t) = 2 Re

xA(t)

,
t âˆˆR.
(7.21)
These results are summarized in the following proposition.
Proposition 7.5.3 (Recovering xPB from xA). Let xPB be a real integrable pass-
band signal that is bandlimited to W Hz around the carrier frequency fc, and let xA
be its analytic representation. Then,
Ë†xPB(f) = Ë†xA(f) + Ë†xâˆ—
A(âˆ’f),
f âˆˆR,
(7.22a)
and
xPB(t) = 2 Re

xA(t)

,
t âˆˆR.
(7.22b)
Proof. The frequency relation (7.22a) is just a restatement of (7.20), whose deriva-
tion was rigorous. To prove (7.22b) we note that, by Proposition 7.2.2 (cf. (b) &
(c)),
xPB(t) =
 âˆ
âˆ’âˆ
Ë†xPB(f) ei2Ï€ft df
=
 âˆ
0
Ë†xPB(f) ei2Ï€ft df +
 0
âˆ’âˆ
Ë†xPB(f) ei2Ï€ft df
= xA(t) +
 0
âˆ’âˆ
Ë†xPB(f) ei2Ï€ft df
= xA(t) +
 âˆ
0
Ë†xPB(âˆ’Ëœf) eâˆ’i2Ï€ Ëœ
ft d Ëœf
= xA(t) +
 âˆ
0
Ë†xâˆ—
PB( Ëœf) eâˆ’i2Ï€ Ëœ
ft d Ëœf
= xA(t) +
	 âˆ
0
Ë†xPB( Ëœf) ei2Ï€ Ëœ
ft d Ëœf

âˆ—
= xA(t) + xâˆ—
A(t)
= 2 Re

xA(t)

,
t âˆˆR,
where in the second equality we broke the integral into two; in the third we used
Deï¬nition 7.5.1; in the fourth we changed the integration variable to Ëœf â‰œâˆ’f;
in the ï¬fth we used the conjugate symmetry of Ë†xPB (7.19); in the sixth we used
the fact that conjugating the integrand results in the conjugation of the integral
(Proposition 2.3.1); in the seventh we used the deï¬nition of the analytic signal;
and in the last equality we used the fact that a complex number and its conjugate
add up to twice its real part.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.5 The Analytic Signal
117
7.5.3
Relating âŸ¨xPB, yPBâŸ©to âŸ¨xA, yAâŸ©
We next relate the inner product between two real passband signals to the inner
product between their analytic representations.
Proposition 7.5.4 (âŸ¨xPB, yPBâŸ©and âŸ¨xA, yAâŸ©). Let xPB and yPB be real integrable
passband signals that are bandlimited to W Hz around the carrier frequency fc, and
let xA and yA be their analytic representations. Then
âŸ¨xPB, yPBâŸ©= 2 Re

âŸ¨xA, yAâŸ©

,
(7.23)
and
âˆ¥xPBâˆ¥2
2 = 2 âˆ¥xAâˆ¥2
2 .
(7.24)
Note that in (7.23) the inner product appearing on the LHS is the inner product
between real signals whereas the one appearing on the RHS is between complex
signals.
Proof. We ï¬rst note that the inner products and energies are well-deï¬ned because
integrable passband signals are also energy-limited (Corollary 7.2.4). Next, even
though (7.24) is a special case of (7.23), we ï¬rst prove (7.24). The proof is a simple
application of Parsevalâ€™s Theorem. The intuition is as follows. Since xPB is real,
it follows that its FT is conjugate-symmetric (7.19) so the magnitude of Ë†xPB is
symmetric. Consequently, the positive frequencies and the negative frequencies
of Ë†xPB contribute an equal share to the total energy in Ë†xPB. And since the energy
in the analytic representation is equal to the share corresponding to the positive
frequencies only, its energy must be half the energy of xPB.
This can be argued more formally as follows. Because xPB is real-valued, its FT Ë†xPB
is conjugate-symmetric (7.19), so its magnitude is symmetric |Ë†xPB(f)| = |Ë†xPB(âˆ’f)|
for all f âˆˆR and, a fortiori,
 âˆ
0
|Ë†xPB(f)|2 df =
 0
âˆ’âˆ
|Ë†xPB(f)|2 df.
(7.25)
Also, by Parsevalâ€™s Theorem (applied to xPB),
 âˆ
0
|Ë†xPB(f)|2 df +
 0
âˆ’âˆ
|Ë†xPB(f)|2 df = âˆ¥xPBâˆ¥2
2 .
(7.26)
Consequently, by combining (7.25) and (7.26), we obtain
 âˆ
0
|Ë†xPB(f)|2 df = 1
2 âˆ¥xPBâˆ¥2
2 .
(7.27)
We can now establish (7.24) from (7.27) by using Parsevalâ€™s Theorem (applied
to xA) and (7.13) to obtain
âˆ¥xAâˆ¥2
2 = âˆ¥Ë†xAâˆ¥2
2
=
 âˆ
âˆ’âˆ
|Ë†xA(f)|2 df
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

118
Passband Signals and Their Representation
=
 âˆ
0
|Ë†xPB(f)|2 df
= 1
2 âˆ¥xPBâˆ¥2
2 ,
where the last equality follows from (7.27).
We next prove (7.23). We oï¬€er two proofs. The ï¬rst is very similar to our proof
of (7.24): we use Parsevalâ€™s Theorem to express the inner products in the fre-
quency domain, and then argue that the contribution of the negative frequencies
to the inner product is the complex conjugate of the contribution of the positive
frequencies. The second proof uses a trick to relate inner products and energies.
We begin with the ï¬rst proof. Using Proposition 7.5.3 we have
Ë†xPB(f) = Ë†xA(f) + Ë†xâˆ—
A(âˆ’f),
f âˆˆR,
Ë†yPB(f) = Ë†yA(f) + Ë†yâˆ—
A(âˆ’f),
f âˆˆR.
Using Parsevalâ€™s Theorem we now have
âŸ¨xPB, yPBâŸ©= âŸ¨Ë†xPB, Ë†yPBâŸ©
=
 âˆ
âˆ’âˆ
Ë†xPB(f)Ë†yâˆ—
PB(f) df
=
 âˆ
âˆ’âˆ

Ë†xA(f) + Ë†xâˆ—
A(âˆ’f)

Ë†yA(f) + Ë†yâˆ—
A(âˆ’f)
âˆ—
df
=
 âˆ
âˆ’âˆ

Ë†xA(f) + Ë†xâˆ—
A(âˆ’f)

Ë†yâˆ—
A(f) + Ë†yA(âˆ’f)

df
=
 âˆ
âˆ’âˆ
Ë†xA(f) Ë†yâˆ—
A(f) df +
 âˆ
âˆ’âˆ
Ë†xâˆ—
A(âˆ’f) Ë†yA(âˆ’f) df
=
 âˆ
âˆ’âˆ
Ë†xA(f) Ë†yâˆ—
A(f) df +
	 âˆ
âˆ’âˆ
Ë†xA(âˆ’f) Ë†yâˆ—
A(âˆ’f) df

âˆ—
=
 âˆ
âˆ’âˆ
Ë†xA(f) Ë†yâˆ—
A(f) df +
	 âˆ
âˆ’âˆ
Ë†xA( Ëœf) Ë†yâˆ—
A( Ëœf) d Ëœf

âˆ—
= âŸ¨Ë†xA, Ë†yAâŸ©+ âŸ¨Ë†xA, Ë†yAâŸ©âˆ—
= 2 Re

âŸ¨Ë†xA, Ë†yAâŸ©

= 2 Re

âŸ¨xA, yAâŸ©

,
where the ï¬fth equality follows because at all frequencies f âˆˆR the cross-terms
Ë†xA(f) Ë†yA(âˆ’f) and Ë†xâˆ—
A(âˆ’f) Ë†yâˆ—
A(f) are zero, and where the last equality follows from
Parsevalâ€™s Theorem.
The second proof is based on (7.24) and on the identity
2 Re

âŸ¨u, vâŸ©

= âˆ¥u + vâˆ¥2
2 âˆ’âˆ¥uâˆ¥2
2 âˆ’âˆ¥vâˆ¥2
2 ,
u, v âˆˆL2,
(7.28)
which holds for both complex and real signals and which follows by expressing
âˆ¥u + vâˆ¥2
2 as
âˆ¥u + vâˆ¥2
2 = âŸ¨u + v, u + vâŸ©
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.6 Baseband Representation of Real Passband Signals
119
= âŸ¨u, uâŸ©+ âŸ¨u, vâŸ©+ âŸ¨v, uâŸ©+ âŸ¨v, vâŸ©
= âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2 + âŸ¨u, vâŸ©+ âŸ¨u, vâŸ©âˆ—
= âˆ¥uâˆ¥2
2 + âˆ¥vâˆ¥2
2 + 2 Re

âŸ¨u, vâŸ©

.
From Identity (7.28) and from (7.24) we have for the real signals xPB and yPB
2âŸ¨xPB, yPBâŸ©= 2 Re

âŸ¨xPB, yPBâŸ©

= âˆ¥xPB + yPBâˆ¥2
2 âˆ’âˆ¥xPBâˆ¥2
2 âˆ’âˆ¥yPBâˆ¥2
2
= 2

âˆ¥xA + yAâˆ¥2
2 âˆ’âˆ¥xAâˆ¥2
2 âˆ’âˆ¥yAâˆ¥2
2

= 4 Re

âŸ¨xA, yAâŸ©

,
where the ï¬rst equality follows because the passband signals are real; the second
from Identity (7.28) applied to the passband signals xPB and yPB; the third from
the second part of Proposition 7.5.4 and because the analytic representation of
xPB + yPB is xA + yA; and the ï¬nal equality from Identity (7.28) applied to the
analytic signals xA and yA.
7.6
Baseband Representation of Real Passband Signals
Strictly speaking, the baseband representation xBB of a real passband sig-
nal xPB is not a â€œrepresentationâ€ because one cannot recover xPB from xBB alone;
one also needs to know the carrier frequency fc. This may seem like a disadvantage,
but engineers view this as an advantage. Indeed, in some cases, it may illuminate
the fact that certain operations and results do not depend on the carrier frequency.
This decoupling of various operations from the carrier frequency is very useful in
hardware implementation of communication systems that need to work around
selectable carrier frequencies. It allows for some of the processing to be done us-
ing carrier-independent hardware and for only a small part of the communication
system to be tunable to the carrier frequency. Very loosely speaking, engineers
think of xBB as everything about xPB that is not carrier-dependent. Thus, one
does not usually expect the quantity fc to show up in a formula for the baseband
representation. Philosophical thoughts aside, the baseband representation has a
straightforward deï¬nition.
7.6.1
Deï¬nition and Characterization
Deï¬nition 7.6.1 (Baseband Representation). The baseband representation of
a real integrable passband signal xPB that is bandlimited to W Hz around the carrier
frequency fc is the complex signal
xBB(t) â‰œeâˆ’i2Ï€fct xA(t),
t âˆˆR,
(7.29)
where xA is the analytic representation of xPB.
Note that, by (7.29), the magnitudes of xA and xBB are identical
xBB(t)
 =
xA(t)
,
t âˆˆR.
(7.30)
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

120
Passband Signals and Their Representation
Consequently, since xA is integrable we also have:
Proposition 7.6.2 (Integrability of xPB Implies Integrability of xBB). The base-
band representation of a real integrable passband signal that is bandlimited to W
Hz around the carrier frequency fc is integrable.
By (7.29) and (7.13) we obtain that if xPB is a real integrable passband signal that
is bandlimited to W Hz around the carrier frequency fc, then
Ë†xBB(f) = Ë†xA(f + fc) =

Ë†xPB(f + fc)
if |f| â‰¤W/2,
0
otherwise.
(7.31)
Thus, the FT of xBB is the FT of xA but shifted to the left by the carrier fre-
quency fc. The relationship between the Fourier Transforms of xPB, xA, and xBB
is depicted in Figure 7.9.
We have deï¬ned the baseband representation of a passband signal in terms of its
analytic representation, but sometimes it is useful to deï¬ne the baseband represen-
tation directly in terms of the passband signal. This is not very diï¬ƒcult. Rather
than taking the passband signal and passing it through a ï¬lter of frequency re-
sponse g satisfying (7.14) to obtain xA and then multiplying the result by eâˆ’i2Ï€fct
to obtain xBB, we can multiply xPB by t 	â†’eâˆ’i2Ï€fct and then ï¬lter the result to
obtain the baseband representation. This procedure is depicted in the frequency
domain in Figure 7.10 and is made precise in the following proposition.
Proposition 7.6.3 (From xPB to xBB Directly). If xPB is a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc, then its baseband
representation xBB is given by
xBB =

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†Ë‡g0,
(7.32a)
where g0 : f 	â†’g0(f) is any integrable function satisfying
g0(f) = 1,
|f| â‰¤W
2 ,
(7.32b)
and
g0(f) = 0,
|f + 2fc| â‰¤W
2 .
(7.32c)
Proof. The proof is all in Figure 7.10. For an analytic proof we provide the fol-
lowing details. By Deï¬nition 7.6.1 and by Proposition 7.5.2 (cf. (d)) we have for
any integrable function g: f 	â†’g(f) satisfying (7.14b) & (7.14c)
xBB(t) = eâˆ’i2Ï€fct
xPB â‹†Ë‡g

(t)
= eâˆ’i2Ï€fct
 âˆ
âˆ’âˆ
Ë†xPB(f) g(f) ei2Ï€ft df
=
 âˆ
âˆ’âˆ
Ë†xPB(f) g(f) ei2Ï€(fâˆ’fc)t df
=
 âˆ
âˆ’âˆ
Ë†xPB( Ëœf + fc) g( Ëœf + fc) ei2Ï€ Ëœ
ft d Ëœf
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.6 Baseband Representation of Real Passband Signals
121
Ë†xPB(f)
f
fc
âˆ’fc
Ë†xA(f)
f
fc
Ë†xBB(f)
f
Figure 7.9: The Fourier Transforms of the analytic signal xA and of the baseband
representation xBB of a real passband signal xPB.
=
 âˆ
âˆ’âˆ
Ë†xPB( Ëœf + fc) g0( Ëœf) ei2Ï€ Ëœ
ft d Ëœf
=

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†Ë‡g0

(t),
where we deï¬ned
g0(f) = g(f + fc),
f âˆˆR,
(7.33)
and where we use the following justiï¬cation.
The second equality follows from
Proposition 6.2.5; the third by pulling the complex exponential into the integral;
the fourth by the deï¬ning Ëœf â‰œf âˆ’fc; the ï¬fth by deï¬ning the function g0 as in
(7.33); and the ï¬nal equality by Proposition 6.2.5 using the fact that
the FT of t 	â†’eâˆ’i2Ï€fct xPB(t) is f 	â†’Ë†xPB(f + fc).
(7.34)
The proposition now follows by noting that g satisï¬es (7.14b) & (7.14c) if, and
only if, the mapping g0 deï¬ned in (7.33) satisï¬es (7.32b) & (7.32c).
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

122
Passband Signals and Their Representation
Ë†xPB(f)
Ë†xPB(f + fc)
g0(f)
Ë†xBB(f)
W
fc
âˆ’fc
âˆ’fc
âˆ’2fc
W
2
W
2
âˆ’W
2
âˆ’W
2
1
Wc
âˆ’Wc
f
f
f
f
Figure 7.10: A frequency-domain description of the process for deriving xBB di-
rectly from xPB.
From top to bottom: Ë†xPB; the FT of t 	â†’eâˆ’i2Ï€fct xPB(t); a
function g0 satisfying (7.32b) & (7.32c); and Ë†xBB.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.6 Baseband Representation of Real Passband Signals
123
Corollary 7.6.4. If xPB is a real integrable passband signal that is bandlimited to W
Hz around the carrier frequency fc, then its baseband representation xBB is given
by
xBB =

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†LPFWc,
(7.35a)
where the cutoï¬€frequency Wc can be chosen arbitrarily in the range
W
2 â‰¤Wc â‰¤2fc âˆ’W
2 .
(7.35b)
Proof. Let Wc satisfy (7.35b) and deï¬ne g0 as follows: if Wc is strictly smaller
than 2fcâˆ’W/2, deï¬ne g0(f) = I{|f| â‰¤Wc}; otherwise deï¬ne g0(f) = I{|f| < Wc}.
In both cases g0 satisï¬es (7.32b) & (7.32c) and
Ë‡g0 = LPFWc .
(7.36)
The result now follows by applying Proposition 7.6.3 with this choice of g0.
In analogy to Proposition 7.5.2, we can characterize the baseband representation
of passband signals as follows.
Proposition 7.6.5 (Characterizing the Baseband Representation). Let xPB be
a real integrable passband signal that is bandlimited to W Hz around the carrier
frequency fc. Then each of the following statements is equivalent to the statement
that the complex signal xBB is its baseband representation.
(a) The signal xBB is given by
xBB(t) =
 W/2
âˆ’W/2
Ë†xPB(f + fc) ei2Ï€ft df,
t âˆˆR.
(7.37)
(b) The signal xBB is a continuous integrable signal satisfying
Ë†xBB(f) = Ë†xPB(f + fc) I
'
|f| â‰¤W
2
(
,
f âˆˆR.
(7.38)
(c) The signal xBB is an integrable signal that is bandlimited to W/2 Hz and that
satisï¬es (7.38).
(d) The signal xBB is given by (7.32a) for any g0 : f 	â†’g0(f) satisfying (7.32b)
& (7.32c).
Proof. Parts (a), (b), and (c) can be easily deduced from their counterparts in
Proposition 7.5.2 using Deï¬nition 7.6.1 and the fact that (7.30) implies that the
integrability of xBB is equivalent to the integrability of xA. Part (d) is a restatement
of Proposition 7.6.3.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

124
Passband Signals and Their Representation
7.6.2
The In-Phase and Quadrature Components
The convolution in (7.35a) is a convolution between a complex signal (the signal
t 	â†’eâˆ’i2Ï€fctxPB(t)) and a real signal (the signal LPFWc). This should not alarm
you. The convolution of two complex signals evaluated at time t is expressed as an
integral (5.2), and in the case of complex signals this is an integral (over the real
line) of a complex-valued integrand. Such integrals were addressed in Section 2.3.
It should, however, be noted that since the deï¬nition of the convolution of two sig-
nals involves their products, the real part of the convolution of two complex-valued
signals is, in general, not equal to the convolution of their real parts. However, as
we next show, if one of the signals is realâ€”as is the case in (7.35a)â€”then things
become simpler: if x is a complex-valued function of time and if h is a real-valued
function of time, then
Re

x â‹†h

= Re(x) â‹†h and Im

x â‹†h

= Im(x) â‹†h,
h is real-valued.
(7.39)
This follows from the deï¬nition of the convolution,
(x â‹†h)(t) =
 âˆ
âˆ’âˆ
x(Ï„) h(t âˆ’Ï„) dÏ„
and from the basic properties of complex integrals (Proposition 2.3.1) by noting
that if h(Â·) is real-valued, then for all t, Ï„ âˆˆR,
Re

x(Ï„) h(t âˆ’Ï„)

= Re

x(Ï„)

h(t âˆ’Ï„),
Im

x(Ï„) h(t âˆ’Ï„)

= Im

x(Ï„)

h(t âˆ’Ï„).
We next use (7.39) to express the convolution in (7.32a) using real-number oper-
ations. To that end we ï¬rst note that since xPB is real, it follows from Eulerâ€™s
Identity
eiÎ¸ = cos Î¸ + i sin Î¸,
Î¸ âˆˆR
(7.40)
that
Re

xPB(t) eâˆ’i2Ï€fct
= xPB(t) cos(2Ï€fct),
t âˆˆR,
(7.41a)
Im

xPB(t) eâˆ’i2Ï€fct
= âˆ’xPB(t) sin(2Ï€fct),
t âˆˆR,
(7.41b)
so by (7.35a), (7.39), and (7.41)
Re(xBB) =

t 	â†’xPB(t) cos(2Ï€fct)

â‹†LPFWc,
(7.42a)
Im(xBB) = âˆ’

t 	â†’xPB(t) sin(2Ï€fct)

â‹†LPFWc .
(7.42b)
It is common in the engineering literature to refer to the real part of xBB as
the in-phase component of xPB and to the imaginary part as the quadrature
component of xPB.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.6 Baseband Representation of Real Passband Signals
125
cos(2Ï€fct)
90â—¦
Ã—
Ã—
xPB(t)
xPB(t) cos(2Ï€fc)
âˆ’xPB(t) sin(2Ï€fct)
LPFWc
LPFWc
W
2 â‰¤Wc â‰¤2fc âˆ’W
2
Re

xBB(t)

Im

xBB(t)

Figure 7.11: Obtaining the baseband representation of a real passband signal.
Deï¬nition 7.6.6 (In-Phase and Quadrature Components). The in-phase com-
ponent of a real integrable passband signal xPB that is bandlimited to W Hz around
the carrier frequency fc is the real part of its baseband representation, i.e.,
Re(xBB) =

t 	â†’xPB(t) cos(2Ï€fct)

â‹†LPFWc .
(In-Phase)
The quadrature component is the imaginary part of its baseband representation,
i.e.,
Im(xBB) = âˆ’

t 	â†’xPB(t) sin(2Ï€fct)

â‹†LPFWc .
(Quadrature)
Here Wc is any cutoï¬€frequency in the range W/2 â‰¤Wc â‰¤2fc âˆ’W/2.
Figure 7.11 depicts a block diagram of a circuit that produces the baseband rep-
resentation of a real passband signal.
This circuit will play an important role
in Chapter 9 when we discuss the Sampling Theorem for passband signals and
complex sampling.
7.6.3
Bandwidth Considerations
The following is a simple but exceedingly important observation regarding band-
width. Recall that the bandwidth of xPB around the carrier frequency fc is deï¬ned
in Deï¬nition 7.3.1 and that the bandwidth of the baseband signal xBB is deï¬ned
in Deï¬nition 6.4.13.
Proposition 7.6.7 (xPB, xBB, and Bandwidth). If the real integrable passband
signal xPB is of bandwidth W Hz around the carrier frequency fc, then its baseband
representation xBB is an integrable signal of bandwidth W/2 Hz.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

126
Passband Signals and Their Representation
Proof. This can be seen graphically from Figure 7.9 or from Figure 7.10. It can
be deduced analytically from (7.31).
7.6.4
Recovering xPB from xBB
Recovering a real passband signal xPB from its baseband representation xBB is
conceptually simple. We can recover the analytic representation via (7.29) and
then use Proposition 7.5.3 to recover xPB:
Proposition 7.6.8 (From xBB to xPB). Let xPB be a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc, and let xBB be
its baseband representation. Then,
Ë†xPB(f) = Ë†xBB(f âˆ’fc) + Ë†xâˆ—
BB(âˆ’f âˆ’fc),
f âˆˆR,
(7.43a)
and
xPB(t) = 2 Re

xBB(t) ei2Ï€fct
,
t âˆˆR.
(7.43b)
The process of recovering xPB from xBB is depicted in the frequency domain in
Figure 7.12. It can, of course, also be carried out using real-number operations
only by rewriting (7.43b) as
xPB(t) = 2 Re

xBB(t)

cos(2Ï€fct) âˆ’2 Im

xBB(t)

sin(2Ï€fct),
t âˆˆR.
(7.44)
It should be emphasized that (7.43b) does not characterize the baseband represen-
tation of xPB; it is possible that xPB(t) = 2 Re

z(t) ei2Ï€fct
hold at every time t and
that z not be the baseband representation of xPB. However, as the next proposition
shows, this cannot happen if z is bandlimited to W/2 Hz.
Proposition 7.6.9. Let xPB be a real integrable passband signal that is bandlimited
to W Hz around the carrier frequency fc. If the complex signal z satisï¬es
xPB(t) = 2 Re

z(t) ei2Ï€fct
,
t âˆˆR,
(7.45)
and is an integrable signal that is bandlimited to W/2 Hz, then z is the baseband
representation of xPB.
Proof. Since z is bandlimited to W/2 Hz, it follows from Proposition 6.4.10 (cf. (c))
that z must be continuous and that its FT must vanish for |f| > W/2. Conse-
quently, by Proposition 7.6.5 (cf. (b)), all that remains to show in order to establish
that z is the baseband representation of xPB is that
Ë†z(f) = Ë†xPB(f + fc),
|f| â‰¤W/2,
(7.46)
and this is what we proceed to do. By taking the FT of both sides of (7.45) we
obtain that
Ë†xPB(f) = Ë†z(f âˆ’fc) + Ë†zâˆ—(âˆ’f âˆ’fc),
f âˆˆR,
(7.47)
or, upon deï¬ning Ëœf â‰œf âˆ’fc,
Ë†xPB( Ëœf + fc) = Ë†z( Ëœf) + Ë†zâˆ—(âˆ’Ëœf âˆ’2fc),
Ëœf âˆˆR.
(7.48)
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.6 Baseband Representation of Real Passband Signals
127
Ë†xBB(f)
Ë†xBB(f âˆ’fc)
Ë†xâˆ—
BB(âˆ’f)
Ë†xâˆ—
BB(âˆ’f âˆ’fc)
Ë†xPB(f) = Ë†xBB(f âˆ’fc) + Ë†xâˆ—
BB(âˆ’f âˆ’fc)
fc
âˆ’fc
âˆ’fc
âˆ’fc
f
f
f
f
f
Figure 7.12: Recovering a passband signal from its baseband representation. Top
plot of Ë†xBB is the transform of xBB; next is the transform of t 	â†’xBB(t) ei2Ï€fct; the
transform of xâˆ—
BB(t); the transform of t 	â†’xâˆ—
BB(t) eâˆ’i2Ï€fct; and ï¬nally the transform
of t 	â†’xBB(t) ei2Ï€fct + xâˆ—
BB(t) eâˆ’i2Ï€fct = 2 Re

xBB(t) ei2Ï€fct
= xPB(t).
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

128
Passband Signals and Their Representation
By recalling that fc > W/2 and that Ë†z is zero for frequencies f satisfying |f| > W/2,
we obtain that Ë†zâˆ—(âˆ’Ëœf âˆ’2fc) is zero whenever | Ëœf| â‰¤W/2 so
Ë†z( Ëœf) + Ë†zâˆ—(âˆ’Ëœf âˆ’2fc) = Ë†z( Ëœf),
| Ëœf| â‰¤W/2.
(7.49)
Combining (7.48) and (7.49) we obtain
Ë†xPB( Ëœf + fc) = Ë†z( Ëœf),
| Ëœf| â‰¤W/2,
thus establishing (7.46) and hence completing the proof.
Proposition 7.6.9 is more useful than its appearance may suggest. It provides an
alternative way of computing the baseband representation of a signal. It demon-
strates that if we can use algebra to express xPB in the form (7.45) for some signal z,
and if we can verify that z is bandlimited to W/2 Hz, then z must be the baseband
representation of xPB.
Note that the proof would also work if we replaced the assumption that z is an
integrable signal that is bandlimited to W/2 Hz with the assumption that z is an
integrable signal that is bandlimited to fc Hz.
7.6.5
Relating âŸ¨xPB, yPBâŸ©to âŸ¨xBB, yBBâŸ©
If xPB and yPB are integrable real passband signals that are bandlimited to W Hz
around the carrier frequency fc, and if xA, xBB , yA, and yBB are their corre-
sponding analytic and baseband representations, then, by (7.29),
âŸ¨xBB, yBBâŸ©= âŸ¨xA, yAâŸ©,
(7.50)
because
âŸ¨xBB, yBBâŸ©=
 âˆ
âˆ’âˆ
xBB(t) yâˆ—
BB(t) dt
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fct xA(t)

eâˆ’i2Ï€fct yA(t)
âˆ—dt
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fct xA(t) ei2Ï€fct yâˆ—
A(t) dt
= âŸ¨xA, yAâŸ©.
Combining (7.50) with Proposition 7.5.4 we obtain the following relationship be-
tween the inner product between two real passband signals and the inner product
between their corresponding complex baseband representations.
Theorem 7.6.10 (âŸ¨xPB, yPBâŸ©and âŸ¨xBB, yBBâŸ©). Let xPB and yPB be real integrable
passband signals that are bandlimited to W Hz around the carrier frequency fc, and
let xBB and yBB be their corresponding baseband representations. Then
âŸ¨xPB, yPBâŸ©= 2 Re

âŸ¨xBB, yBBâŸ©

,
(7.51)
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.6 Baseband Representation of Real Passband Signals
129
and
âˆ¥xPBâˆ¥2
2 = 2 âˆ¥xBBâˆ¥2
2 .
(7.52)
An extremely important corollary provides a necessary and suï¬ƒcient condition for
the inner product between two real passband signals to be zero, i.e., for two real
passband signals to be orthogonal.
Corollary 7.6.11 (Characterizing Orthogonal Real Passband Signals). Two in-
tegrable real passband signals xPB, yPB that are bandlimited to W Hz around the
carrier frequency fc are orthogonal if, and only if, the inner product between their
baseband representations is purely imaginary (i.e., of zero real part).
Thus, for two such passband signals to be orthogonal their baseband representa-
tions need not be orthogonal: it suï¬ƒces that their inner product be purely imagi-
nary.
7.6.6
The Baseband Representation of xPB â‹†yPB
Proposition 7.6.12 (The Baseband Representation of xPB â‹†yPB Is xBB â‹†yBB).
Let xPB and yPB be real integrable passband signals that are bandlimited to W Hz
around the carrier frequency fc, and let xBB and yBB be their baseband repre-
sentations. Then the convolution xPB â‹†yPB is a real integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc and whose baseband
representation is xBB â‹†yBB.
Proof. The proof is illustrated in Figure 7.13 on Page 130. All that remains is to
add some technical details. We begin by deï¬ning
z = xPB â‹†yPB
and by noting that, by Proposition 7.2.5, z is an integrable real passband signal
that is bandlimited to W Hz around the carrier frequency fc and that its FT is
given by
Ë†z(f) = Ë†xPB(f) Ë†yPB(f),
f âˆˆR.
(7.53)
Thus, it is at least meaningful to discuss the baseband representation of xPB â‹†yPB.
We next note that, by Proposition 7.6.5, both xBB and yBB are integrable signals
that are bandlimited to W/2 Hz. Consequently, by Proposition 6.5.2, the convolu-
tion u = xBB â‹†yBB is deï¬ned at every epoch t and is also an integrable signal that
is bandlimited to W/2 Hz. Its FT is
Ë†u(f) = Ë†xBB(f) Ë†yBB(f),
f âˆˆR.
(7.54)
From Proposition 7.6.5 we infer that to prove that u is the baseband representation
of z it only remains to verify that Ë†u is the mapping f 	â†’Ë†z(f + fc) I{|f| â‰¤W/2},
which, in view of (7.53) and (7.54), is equivalent to showing that
Ë†xBB(f) Ë†yBB(f) = Ë†xPB(f + fc) Ë†yPB(f + fc) I{|f| â‰¤W/2},
f âˆˆR.
(7.55)
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

130
Passband Signals and Their Representation
f
f
f
f
f
f
Ë†xPB(f)
Ë†yPB(f)
Ë†xPB(f) Ë†yPB(f)
Ë†xBB(f)
Ë†yBB(f)
Ë†xBB(f) Ë†yBB(f)
fc
âˆ’fc
1
1.5
1.5
Figure 7.13: The convolution of two real passband signals and its baseband rep-
resentation.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.6 Baseband Representation of Real Passband Signals
131
But this follows because the fact that xBB and yBB are the baseband representa-
tions of xPB and yPB implies that
Ë†xBB(f) = Ë†xPB(f + fc) I{|f| â‰¤W/2},
f âˆˆR,
Ë†yBB(f) = Ë†yPB(f + fc) I{|f| â‰¤W/2},
f âˆˆR,
from which (7.55) follows.
7.6.7
The Baseband Representation of xPB â‹†h
We next study the result of passing a real integrable passband signal xPB that is
bandlimited to W Hz around the carrier frequency fc through a real stable ï¬lter
of impulse response h. Our focus is on the baseband representation of the result.
Proposition 7.6.13 (Baseband Representation of xPBâ‹†h). Let xPB be a real inte-
grable passband signal that is bandlimited to W Hz around the carrier frequency fc,
and let xBB be its baseband representation. Let h âˆˆL1 be real. Then xPB â‹†h is
deï¬ned at every time instant; it is a real integrable passband signal that is band-
limited to W Hz around the carrier frequency fc; and its baseband representation
is of FT
f 	â†’Ë†xBB(f) Ë†h(f + fc),
f âˆˆR
(7.56)
and is given by

xPB â‹†h

BB(t) =
 âˆ
âˆ’âˆ
Ë†xBB(f) Ë†h(f + fc) ei2Ï€ft df,
t âˆˆR.
(7.57)
Proof. That the convolution xPB â‹†h is deï¬ned at every time instant follows from
Proposition 7.2.5. Denote xPB â‹†h by y. By the same proposition, y is a real inte-
grable passband signal that is bandlimited to W Hz around the carrier frequency fc,
and its FT is given by
Ë†y(f) = Ë†xPB(f) Ë†h(f),
f âˆˆR.
(7.58)
From (7.58) (and Proposition 7.6.5 (cf. (b)) applied to the signal y), we obtain
that the baseband representation of y is of FT
f 	â†’Ë†xPB(f + fc) Ë†h(f + fc) I
'
|f| â‰¤W
2
(
,
f âˆˆR.
(7.59)
To establish that the mapping in (7.56) is indeed the FT of (xPB â‹†h)BB it thus
remains to establish that the mappings (7.59) and (7.56) are identical. But this
follows because, by Proposition 7.6.5 (cf. (b)) applied to the signal xPB,
Ë†xBB(f) = Ë†xPB(f + fc) I
'
|f| â‰¤W
2
(
,
f âˆˆR.
To justify the inversion formula (7.57), recall that (xPBâ‹†h)BBâ€”being the baseband
representation of an integrable signal that is bandlimited to W Hz around fcâ€”is an
integrable signal that is bandlimited to W/2 Hz (Proposition 7.6.5 (cf. (c)) applied
to xPB â‹†h), so the inversion is justiï¬ed by Proposition 6.4.10 (cf. (b)).
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

132
Passband Signals and Their Representation
f
f
W
fc
W
2
âˆ’W
2
Ë†h(f)
Figure 7.14: A real ï¬lterâ€™s frequency response (top) and its frequency response
with respect to the bandwidth W around the carrier frequency fc (bottom).
Motivated by Proposition 7.6.13 we put forth the following deï¬nition.
Deï¬nition 7.6.14 (Frequency Response with Respect to a Band). For a stable
real ï¬lter of impulse response h we deï¬ne the frequency response with respect
to the bandwidth W around the carrier frequency fc (satisfying fc > W/2)
as the mapping
f 	â†’Ë†h(f + fc) I
'
|f| â‰¤W
2
(
.
(7.60)
Figure 7.14 illustrates the relationship between the frequency response of a real
ï¬lter and its response with respect to the carrier frequency fc and bandwidth W.
Heuristically, we can think of the frequency response with respect to the band-
width W around the carrier frequency fc of a ï¬lter of real impulse response h as
the FT of the baseband representation of h â‹†BPFW,fc.2
With the aid of Deï¬nition 7.6.14 we can restate Proposition 7.6.13 as stating that
the baseband representation of the result of passing a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc through a
stable real ï¬lter is the product of the FT of the baseband representation of the
signal by the frequency response with respect to the bandwidth W around the
carrier frequency fc of the ï¬lter. This relationship is illustrated in Figures 7.15
2This is mathematically somewhat problematic because hâ‹†BPFW,fc need not be an integrable
signal.
But this can be remedied because h â‹†BPFW,fc is an energy-limited passband signal
that is bandlimited to W Hz around the carrier frequency, and, as such, also has a baseband
representation; see Section 7.7.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.7 Energy-Limited Passband Signals
133
f
f
f
fc
fc
fc
âˆ’fc
âˆ’fc
âˆ’fc
W
1
1
1
Ë†xPB(f)
Ë†h(f)
Ë†xPB(f) Ë†h(f)
Figure 7.15: The FT of a passband signal (top); the frequency response of a real
ï¬lter (middle); and their product (bottom).
and 7.16. The former depicts the product of the FT of a real passband signal xPB
and the frequency response of a real ï¬lter h. The latter depicts the product of the
baseband representation xBB of xPB by the frequency response of h with respect
to the bandwidth W around the carrier frequency fc.
The relationship between some of the properties of xPB, xA, and xBB are summa-
rized in Table 7.1 on Page 146.
7.7
Energy-Limited Passband Signals
We next repeat the results of this chapter under the weaker assumption that the
passband signal is energy-limited and not necessarily integrable. The key results
require only minor adjustments, and most of the derivations are nearly identical
and hence omitted.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

134
Passband Signals and Their Representation
f
f
f
W
2
âˆ’W
2
W
2
âˆ’W
2
W
2
âˆ’W
2
1
1
1
Ë†xBB(f)
Figure 7.16: The FT of the baseband representation of the passband signal xPB of
Figure 7.15 (top); the frequency response with respect to the bandwidth W around
the carrier frequency fc of the ï¬lter of Figure 7.15 (middle); and their product
(bottom).
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.7 Energy-Limited Passband Signals
135
7.7.1
Characterization of Energy-Limited Passband Signals
Recall that energy-limited passband signals were deï¬ned in Deï¬nition 7.2.1 as
energy-limited signals that are unaltered by bandpass ï¬ltering.
In this subsec-
tion we shall describe alternative characterizations. Aiding us in the character-
ization is the following lemma, which can be viewed as the passband analog of
Lemma 6.4.4 (i).
Lemma 7.7.1. Let x be an energy-limited signal, and let fc > W/2 > 0 be given.
Then the signal x â‹†BPFW,fc can be expressed as

x â‹†BPFW,fc

(t) =

||f|âˆ’fc|â‰¤W/2
Ë†x(f) ei2Ï€ft df,
t âˆˆR;
(7.61)
it is of ï¬nite energy; and its L2-Fourier Transform is (the equivalence class of) the
mapping f 	â†’Ë†x(f) I
|f| âˆ’fc
 â‰¤W/2

.
Proof. The lemma follows from Lemma 6.4.4 (ii) by substituting for g the mapping
f 	â†’I
|f| âˆ’fc
 â‰¤W/2

, whose IFT is BPFW,fc.
In analogy to Proposition 6.4.5 we can characterize energy-limited passband signals
as follows.
Proposition 7.7.2 (Characterizations of Passband Signals in L2).
(i) If x is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc, then it can be expressed in the form
x(t) =

||f|âˆ’fc|â‰¤W/2
g(f) ei2Ï€ft df,
t âˆˆR,
(7.62)
for some mapping g: f 	â†’g(f) satisfying

||f|âˆ’fc|â‰¤W/2
|g(f)|2 df < âˆ
(7.63)
that can be taken as (any function in the equivalence class of) Ë†x.
(ii) If a signal x can be expressed as in (7.62) for some function g satisfying
(7.63) with fc > W/2 > 0, then x is an energy-limited passband signal that
is bandlimited to W Hz around the carrier frequency fc and its FT Ë†x is (the
equivalence class of) the mapping f 	â†’g(f) I
|f| âˆ’fc
 â‰¤W/2

.
Proof. The proof of Part (i) follows from Deï¬nition 7.2.1 and from Lemma 7.7.1 in
very much the same way as Part (i) of Proposition 6.4.5 follows from Deï¬nition 6.4.1
and Lemma 6.4.4 (i).
The proof of Part (ii) is analogous to the proof of Part (ii) of Proposition 6.4.5.
As a corollary we obtain the analog of Corollary 7.2.3:
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

136
Passband Signals and Their Representation
Corollary 7.7.3 (Energy-Limited Passband Signals Are Bandlimited). If xPB is
an energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc, then it is an energy-limited signal that is bandlimited to fc +W/2 Hz.
Proof. If xPB is an energy-limited passband signal that is bandlimited to W Hz
around the carrier frequency fc, then, by Proposition 7.7.2 (i), there exists a func-
tion g: f 	â†’g(f) satisfying (7.63) such that xPB is given by (7.62). But this implies
that the function f 	â†’g(f) I
|f| âˆ’fc
 â‰¤W/2

is an energy-limited function such
that
xPB(t) =
 fc+W/2
âˆ’fcâˆ’W/2
g(f) I
|f| âˆ’fc
 â‰¤W/2

ei2Ï€ft df,
t âˆˆR,
(7.64)
so, by Proposition 6.4.5 (ii), xPB is an energy-limited signal that is bandlimited to
fc + W/2 Hz.
The following is the analog of Proposition 6.4.6.
Proposition 7.7.4.
(i) If xPB is an energy-limited passband signal that is bandlimited to W Hz
around the carrier frequency fc, then xPB is a continuous function and all
its energy is contained in the frequencies f satisfying
|f| âˆ’fc
 â‰¤W/2 in the
sense that
 âˆ
âˆ’âˆ
|Ë†xPB(f)|2 df =

||f|âˆ’fc|â‰¤W/2
|Ë†xPB(f)|2 df.
(7.65)
(ii) If xPB âˆˆL2 satisï¬es (7.65), then xPB is indistinguishable from the signal
xPBâ‹†BPFW,fc, which is an energy-limited passband signal that is bandlimited
to W Hz around fc.
If in addition to satisfying (7.65) the signal xPB is
continuous, then xPB is an energy-limited passband signal that is bandlimited
to W Hz around the carrier frequency fc.
Proof. This propositionâ€™s claims are a subset of those of Proposition 7.7.5, which
summarizes some of the results related to bandpass ï¬ltering.
Proposition 7.7.5. Let y = xâ‹†BPFW,fc be the result of feeding the signal x âˆˆL2 to
an ideal unit-gain bandpass ï¬lter of bandwidth W around the carrier frequency fc.
Assume fc > W/2. Then:
(i) y is energy-limited with
âˆ¥yâˆ¥2 â‰¤âˆ¥xâˆ¥2 .
(7.66)
(ii) y is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc.
(iii) The L2-Fourier Transform of y is (the equivalence class of) the mapping
f 	â†’Ë†x(f) I
|f| âˆ’fc
 â‰¤W/2

.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.7 Energy-Limited Passband Signals
137
(iv) All the energy in y is concentrated in the frequencies

f :
|f| âˆ’fc
 â‰¤W/2

in the sense that
 âˆ
âˆ’âˆ
|Ë†y(f)|2 df =

||f|âˆ’fc|â‰¤W/2
|Ë†y(f)|2 df.
(v) y can be represented as
y(t) =
 âˆ
âˆ’âˆ
Ë†y(f) ei2Ï€ft df
(7.67)
=

||f|âˆ’fc|â‰¤W/2
Ë†x(f) ei2Ï€ft df,
t âˆˆR.
(7.68)
(vi) y is uniformly continuous.
(vii) If all the energy of x is concentrated in the frequencies

f :
|f|âˆ’fc
 â‰¤W/2

in the sense that
 âˆ
âˆ’âˆ
|Ë†x(f)|2 df =

||f|âˆ’fc|â‰¤W/2
|Ë†x(f)|2 df,
(7.69)
then x is indistinguishable from the passband signal x â‹†BPFW,fc.
(viii) z is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc if, and only if, it satisï¬es all three of the following
conditions: it is in L2; it is continuous; and all its energy is concentrated in
the passband frequencies

f :
|f| âˆ’fc
 â‰¤W/2

.
Proof. The proof is very similar to the proof of Proposition 6.4.7 and is thus
omitted.
7.7.2
The Analytic Representation
If xPB is a real energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc, then we deï¬ne its analytic representation via (7.11). (Since
xPB âˆˆL2, it follows from Parsevalâ€™s Theorem that Ë†xPB is energy-limited so, by
Proposition 3.4.3, the mapping f 	â†’Ë†xPB(f) I{|f âˆ’fc| â‰¤W/2} is integrable and
the integral (7.11) is deï¬ned for every t âˆˆR. Also, the integral does not depend
on which element of the equivalence class consisting of the L2-Fourier Transform
of xPB it is applied to.)
In analogy to Proposition 7.5.2 we can characterize the analytic representation as
follows.
Proposition 7.7.6 (Characterizing the Analytic Representation of xPB âˆˆL2).
Let xPB be a real energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc. Then each of the following statements is equivalent to the
statement that the complex signal xA is the analytic representation of xPB:
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

138
Passband Signals and Their Representation
(a) The signal xA is given by
xA(t) =
 fc+ W
2
fcâˆ’W
2
Ë†xPB(f) ei2Ï€ft df,
t âˆˆR.
(7.70)
(b) The signal xA is a continuous energy-limited signal whose L2-Fourier Trans-
form Ë†xA is (the equivalence class of) the mapping
f 	â†’Ë†xPB(f) I{f â‰¥0}.
(7.71)
(c) The signal xA is an energy-limited passband signal that is bandlimited to W
Hz around the carrier frequency fc and whose L2-Fourier Transform is (the
equivalence class of) the mapping in (7.71).
(d) The signal xA is given by
xA = xPB â‹†Ë‡g
(7.72)
where g: f 	â†’g(f) is any function in L1 âˆ©L2 satisfying
g(f) = 1,
f âˆ’fc
 â‰¤W/2,
(7.73a)
and
g(f) = 0,
f + fc
 â‰¤W/2.
(7.73b)
Proof. The proof is not very diï¬ƒcult and is omitted.
We note that the reconstruction formula (7.22b) continues to hold also when xPB
is an energy-limited signal that is bandlimited to W Hz around the carrier fre-
quency fc.
7.7.3
The Baseband Representation of xPB âˆˆL2
Having deï¬ned the analytic representation, we now use (7.29) to deï¬ne the base-
band representation.
As in Proposition 7.6.3, we can also describe a procedure for obtaining the base-
band representation of a passband signal without having to go via the analytic
representation.
Proposition 7.7.7 (From xPB âˆˆL2 to xBB Directly). If xPB is a real energy-
limited passband signal that is bandlimited to W Hz around the carrier frequency fc,
then its baseband representation xBB is given by
xBB =

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†Ë‡g0,
(7.74)
where g0 : f 	â†’g0(f) is any function in L1 âˆ©L2 satisfying
g0(f) = 1,
|f| â‰¤W/2,
(7.75a)
and
g0(f) = 0,
|f + 2fc| â‰¤W/2.
(7.75b)
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.7 Energy-Limited Passband Signals
139
Proof. The proof is very similar to the proof of Proposition 7.6.3 and is omitted.
The following proposition, which is the analog of Proposition 7.6.5 characterizes
the baseband representation of energy-limited passband signals.
Proposition 7.7.8 (Characterizing the Baseband Representation of xPB âˆˆL2).
Let xPB be a real energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc. Then each of the following statements is equivalent to the
statement that the complex signal xBB is the baseband representation of xPB.
(a) The signal xBB is given by
xBB(t) =

W
2
âˆ’W
2
Ë†xPB(f + fc) ei2Ï€ft df,
t âˆˆR.
(7.76)
(b) The signal xBB is a continuous energy-limited signal whose L2-Fourier Trans-
form is (the equivalence class of) the mapping
f 	â†’Ë†xPB(f + fc) I{|f| â‰¤W/2}.
(7.77)
(c) The signal xBB is an energy-limited signal that is bandlimited to W/2 Hz
and whose L2-Fourier Transform is (the equivalence class of) the mapping
(7.77).
(d) The signal xBB is given by (7.74) for any mapping g0 : f 	â†’g0(f) satisfying
(7.75).
The in-phase component and the quadrature component of an energy-limited
passband signal are deï¬ned, as in the integrable case, as the real and imaginary
parts of its baseband representation.
Proposition 7.6.7, which asserts that the bandwidth of xBB is half the bandwidth
of xPB continues to hold, as does the reconstruction formula (7.43b). Proposi-
tion 7.6.9 also extends to energy-limited signals. We repeat it (in a slightly more
general way) for future reference.
Proposition 7.7.9.
(i) If z is an energy-limited signal that is bandlimited to W/2 Hz, and if the
signal x is given by
x(t) = 2 Re

z(t) ei2Ï€fct
,
t âˆˆR,
(7.78)
where fc > W/2, then x is a real energy-limited passband signal that is band-
limited to W Hz around fc, and z is its baseband representation.
(ii) If x is an energy-limited passband signal that is bandlimited to W Hz around
the carrier frequency fc and if (7.78) holds for some energy-limited signal z
that is bandlimited to fc Hz, then z is the baseband representation of x and
is, in fact, bandlimited to W/2 Hz.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

140
Passband Signals and Their Representation
Proof. Omitted.
Identity (7.51) relating the inner products âŸ¨xPB, yPBâŸ©and âŸ¨xBB, yBBâŸ©continues to
hold for energy-limited passband signals that are not necessarily integrable.
Proposition 7.6.12 does not hold for energy-limited signals, because the convolution
of two energy-limited signals need not be energy-limited. But if we assume that at
least one of the signals is also integrable, then things sail through. Consequently,
using Corollary 7.2.4 we obtain:
Proposition 7.7.10 (The Baseband Representation of xPB â‹†yPB Is xBB â‹†yBB).
Let xPB be a real integrable passband signal that is bandlimited to W Hz around
the carrier frequency fc, and let yPB be a real energy-limited passband signal that
is bandlimited to W Hz around the carrier frequency fc. Let xBB and yBB be their
corresponding baseband representations. Then xPB â‹†yPB is a real energy-limited
signal that is bandlimited to W Hz around the carrier frequency fc and whose
baseband representation is xBB â‹†yBB.
Proposition 7.6.13 too requires only a slight modiï¬cation to address energy-limited
signals.
Proposition 7.7.11 (Baseband Representation of xPB â‹†h). Let xPB be a real
energy-limited passband signal that is bandlimited to W Hz around the carrier fre-
quency fc, and let h be a real integrable signal. Then xPB â‹†h is deï¬ned at every
time instant; it is a real energy-limited passband signal that is bandlimited to W
Hz around the carrier frequency fc; and its baseband representation is given by

h â‹†xPB

BB = hâ€²
BB â‹†xBB,
(7.79)
where hâ€²
BB is the baseband representation of the energy-limited signal hâ‹†BPFW,fc.
The L2-Fourier Transform of the baseband representation of xPB â‹†h is (the equiv-
alence class of) the mapping
f 	â†’Ë†xBB(f) Ë†h(f + fc),
f âˆˆR,
(7.80)
where xBB is the baseband representation of xPB.
The following theorem summarizes some of the properties of the baseband repre-
sentation of energy-limited passband signals.
Theorem 7.7.12 (Properties of the Baseband Representation).
(i) The mapping xPB 	â†’xBB that maps every real energy-limited passband signal
that is bandlimited to W Hz around the carrier frequency fc to its baseband
representation is a one-to-one mapping onto the space of complex energy-
limited signals that are bandlimited to W/2 Hz.
(ii) The mapping xPB 	â†’xBB is linear in the sense that if xPB and yPB are
real energy-limited passband signals that are bandlimited to W Hz around
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.8 Shifting to Passband and Convolving
141
the carrier frequency fc, and if xBB and yBB are their corresponding base-
band representations, then for every Î±, Î² âˆˆR, the baseband representation of
Î±xPB + Î²yPB is Î±xBB + Î²yBB:

Î±xPB + Î²yPB

BB= Î±xBB + Î²yBB,
Î±, Î² âˆˆR.
(7.81)
(iii) The mapping xPB 	â†’xBB isâ€”to within a factor of twoâ€”energy preserving
in the sense that
âˆ¥xPBâˆ¥2
2 = 2 âˆ¥xBBâˆ¥2
2 .
(7.82)
(iv) Inner products are related via
âŸ¨xPB, yPBâŸ©= 2 Re

âŸ¨xBB, yBBâŸ©

,
(7.83)
for xPB and yPB as in (ii) above.
(v) The (baseband) bandwidth of xBB is half the bandwidth of xPB around the
carrier frequency fc.
(vi) The baseband representation xBB can be expressed in terms of xPB as
xBB =

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†LPFWc
(7.84a)
where Wc is any cutoï¬€frequency satisfying
W/2 â‰¤Wc â‰¤2fc âˆ’W/2.
(7.84b)
(vii) The real passband signal xPB can be expressed in terms of its baseband rep-
resentation xBB as
xPB(t) = 2 Re

xBB(t) ei2Ï€fct
,
t âˆˆR.
(7.85)
(viii) If h is a real integrable signal, and if xPB is as above, then h â‹†xPB is a real
energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc, and its baseband representation is given by

h â‹†xPB

BB = hâ€²
BB â‹†xBB,
(7.86)
where hâ€²
BB is the baseband representation of the energy-limited real signal
h â‹†BPFW,fc.
7.8
Shifting to Passband and Convolving
The following result is almost trivial if you think about its interpretation in the
frequency domain. To that end, it is good to focus on the case where the signal x
is a bandlimited baseband signal and where fc is positive and large. In this case
we can interpret the LHS of (7.87) as the result of taking the baseband signal x,
up-converting it to passband by forming the signal Ï„ 	â†’x(Ï„) ei2Ï€fcÏ„, and then
convolving the result with h. The RHS corresponds to down-converting h to form
the signal Ï„ 	â†’eâˆ’i2Ï€fcÏ„h(Ï„), then convolving this signal with x, and then up-
converting the ï¬nal result.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

142
Passband Signals and Their Representation
Proposition 7.8.1. Suppose that fc âˆˆR and that (at least) one of the following
conditions holds:
1) The signal x is a measurable bounded signal and h âˆˆL1.
2) Both x and h are in L2.
Then, at every epoch t âˆˆR,

Ï„ 	â†’x(Ï„) ei2Ï€fcÏ„
â‹†h

(t) = ei2Ï€fct 
x â‹†

Ï„ 	â†’eâˆ’i2Ï€fcÏ„ h(Ï„)

(t).
(7.87)
Proof. We evaluate the LHS of (7.87) using the deï¬nition of the convolution:

Ï„ 	â†’x(Ï„) ei2Ï€fcÏ„
â‹†h

(t) =
 âˆ
âˆ’âˆ
x(Ï„) ei2Ï€fcÏ„ h(t âˆ’Ï„) dÏ„
= ei2Ï€fct eâˆ’i2Ï€fct
 âˆ
âˆ’âˆ
x(Ï„) ei2Ï€fcÏ„ h(t âˆ’Ï„) dÏ„
= ei2Ï€fct
 âˆ
âˆ’âˆ
x(Ï„) eâˆ’i2Ï€fc(tâˆ’Ï„) h(t âˆ’Ï„) dÏ„
= ei2Ï€fct 
x â‹†

Ï„ 	â†’eâˆ’i2Ï€fcÏ„ h(Ï„)

(t).
7.9
Mathematical Comments
The analytic representation is related to the Hilbert Transform; see, for example,
(Pinsky, 2002, Section 3.4). In our proof that xA is integrable whenever xPB is
integrable we implicitly exploited the fact that the strict inequality fc > W/2
implies that for the class of integrable passband signals that are bandlimited to W
Hz around the carrier frequency fc there exist Hilbert Transform kernels that are
integrable. See, for example, (Logan, 1978, Section 2.5).
7.10
Exercises
Exercise 7.1 (Bandwidth around Diï¬€erent Carrier Frequencies). Let Ë‡g be the IFT of
g: f â†’I{a â‰¤|f| â‰¤b},
where b > a > 0. For which positive values of fc and W is Ë‡g an energy-limited passband
signal that is bandlimited to W Hz around the carrier frequency fc? For those values of
fc and W, express the bandwidth of Ë‡g around fc in terms of a, b and fc.
Exercise 7.2 (On the Deï¬nition of the Analytic Representation). Let xPB be a real
integrable passband signal that is bandlimited to W Hz around the carrier frequency fc.
Show that the signals xPB and t â†’2 Re

z(t)

are identical if, and only if, the diï¬€erence
z âˆ’xA between the signal z and the analytic representation of xPB takes on only purely
imaginary values.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.10 Exercises
143
Exercise 7.3 (Purely Real and Purely Imaginary Baseband Representations). Let xPB
be a real integrable passband signal that is bandlimited to W Hz around the carrier
frequency fc, and let xBB be its baseband representation.
(i) Show that xBB is real if, and only if, Ë†xPB satisï¬es
Ë†xPB(fc âˆ’Î´) = Ë†xâˆ—
PB(fc + Î´),
|Î´| â‰¤W
2 .
(ii) Show that xBB is imaginary if, and only if,
Ë†xPB(fc âˆ’Î´) = âˆ’Ë†xâˆ—
PB(fc + Î´),
|Î´| â‰¤W
2 .
Exercise 7.4 (Symmetry around the Carrier Frequency). Let xPB be a real integrable
passband signal that is bandlimited to W Hz around the carrier frequency fc.
(i) Show that xPB can be written in the form
xPB(t) = w(t) cos(2Ï€fct)
where w(Â·) is a real integrable signal that is bandlimited to W/2 Hz if, and only if,
Ë†xPB(fc + Î´) = Ë†xâˆ—
PB(fc âˆ’Î´),
|Î´| â‰¤W
2 .
(ii) Show that xPB can be written in the form
xPB(t) = w(t) sin(2Ï€fct),
t âˆˆR
for w(Â·) as above if, and only if,
Ë†xPB(fc + Î´) = âˆ’Ë†xâˆ—
PB(fc âˆ’Î´),
|Î´| â‰¤W
2 .
Exercise 7.5 (Delaying a Passband Signal). Let xPB be a real integrable passband signal
that is bandlimited to W Hz around the carrier frequency fc, and let xBB be its baseband
representation. Express the baseband representation of the delayed signal t â†’xPB(t âˆ’Î´)
in terms of xBB, fc, and Î´.
Exercise 7.6 (From Baseband to Passband with Care). Let the complex signal x be the
IFT of g, where
g(f) =
â§
âª
â¨
âª
â©
1
âˆ’B < f â‰¤0,
1 âˆ’f
B
0 < f â‰¤B,
0
otherwise,
f âˆˆR,
and B > 0. For Î± > 0 consider the signal
y(t) = Re

x(t) ei2Ï€Î±t
,
t âˆˆR.
Under what additional conditions on B, Î±, fc, and W is y a real energy-limited passband
signal that is bandlimited to W Hz around fc?
Under these conditions, what is the
bandwidth of y around fc, and what is its baseband representation with respect to fc?
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

144
Passband Signals and Their Representation
Exercise 7.7 (Bandpass Filtering a Passband Signal). Let xPB be a real energy-limited
passband signal of bandwidth W Hz around the carrier frequency fc. Let yPB be the result
of passing xPB through an ideal unit-gain bandpass ï¬lter of bandwidth W/2 around the
frequency fc + W/4
yPB = xPB â‹†BPF W
2 ,fc+ W
4 .
(i) How large can the bandwidth of yPB around fc be?
(ii) How large can the bandwidth of yPB around fc + W/4 be?
(iii) Let xBB be the baseband representation of xPB.
Express the FT of the base-
band representation of yPB in terms of Ë†xBB if you view yPB as a passband signal
around fc. Repeat when you view yPB as a passband signal around fc + W/4.
Exercise 7.8 (Viewing a Baseband Signal as a Passband Signal). Let x be a real integrable
signal that is bandlimited to W Hz. Show that if we had informally allowed equality in
(7.1b) and if we had allowed equality between fc and W/2 in (5.21), then we could have
viewed x also as a real integrable passband signal that is bandlimited to W Hz around the
carrier frequency fc = W/2. Viewed as such, what would have been its complex baseband
representation?
Exercise 7.9 (Bandwidth of the Product of Two Signals). Let x be a real energy-limited
signal that is bandlimited to Wx Hz. Let y be a real energy-limited passband signal that
is bandlimited to Wy Hz around the carrier frequency fc. Show that if fc > Wx + Wy/2,
then the signal t â†’x(t) y(t) is a real integrable passband signal that is bandlimited to
2Wx + Wy Hz around the carrier frequency fc.
Exercise 7.10 (Phase Shift). Let x be a real integrable signal that is bandlimited to
W Hz. Let fc be larger than W.
(i) Express the baseband representation of the real passband signal
zPB(t) = x(t) sin(2Ï€fct + Ï†),
t âˆˆR
in terms of x(Â·) and Ï†.
(ii) Compute the Fourier Transform of zPB.
Exercise 7.11 (Energy of a Passband Signal). Let x âˆˆL2 be of energy âˆ¥xâˆ¥2
2.
(i) What is the approximate energy in t â†’x(t) cos(2Ï€fct) if fc is very large?
(ii) Is your answer exact if x is an energy-limited signal that is bandlimited to W Hz,
where W < fc?
Hint: In Part (i) approximate x as being constant over the periods of t â†’cos (2Ï€fct).
For Part (ii) see also Problem 6.21.
Exercise 7.12 (Diï¬€erences in Passband). Let xPB and yPB be real energy-limited pass-
band signals that are bandlimited to W Hz around the carrier frequency fc. Let xBB and
yBB be their baseband representations. Find the relationship between
 âˆ
âˆ’âˆ

xPB(t) âˆ’yPB(t)
2 dt
and
 âˆ
âˆ’âˆ
xBB(t) âˆ’yBB(t)
2 dt.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

7.10 Exercises
145
Exercise 7.13 (Reï¬‚ection of Passband Signal). Let xPB and yPB be real integrable pass-
band signals that are bandlimited to W Hz around the carrier frequency fc. Let xBB
and yBB be their baseband representations.
(i) Express the baseband representation of ~xPB in terms of xBB.
(ii) Express âŸ¨xPB, ~yPBâŸ©in terms of xBB and yBB.
Exercise 7.14 (Deducing xBB). Let xPB be a real integrable passband signal that is
bandlimited to W Hz around the carrier frequency fc.
Show that it is possible that
xPB(t) be given at every epoch t âˆˆR by 2 Re

z(t) ei2Ï€fct
for some complex signal z and
that z not be the baseband representation of xPB. Does this contradict Proposition 7.6.9?
Exercise 7.15 (Averaging the Instantaneous Power). Let xPB be a real integrable pass-
band signal that is bandlimited to W Hz around the carrier frequency fc, and let xBB be
its baseband representation. The signal xPB is squared, and the result is then fed to a
stable lowpass ï¬lter whose frequency response Ë†h satisï¬es
Ë†h(f) = 0,
|f| â‰¥W0
and
Ë†h(f) = 1,
|f| â‰¤W1,
where W < W1 < W0 < 2fc âˆ’W. Express the ï¬lterâ€™s output in terms of xBB.
.009
14:26:26, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

146
Passband Signals and Their Representation
In terms of xPB
In terms of xA
In terms of xBB
xPB
2 Re(xA)
t 	â†’2 Re

xBB(t) ei2Ï€fct
xPB â‹†

t 	â†’ei2Ï€fct LPFWc(t)

xA
t 	â†’ei2Ï€fct xBB(t)

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†LPFWc
t 	â†’eâˆ’i2Ï€fct xA(t)
xBB
Ë†xPB
f 	â†’Ë†xA(f) + Ë†xâˆ—
A(âˆ’f)
f 	â†’Ë†xBB(f âˆ’fc) + Ë†xâˆ—
BB(âˆ’f âˆ’fc)
f 	â†’Ë†xPB(f) I
f âˆ’fc
 â‰¤Wc

Ë†xA
f 	â†’Ë†xBB(f âˆ’fc)
f 	â†’Ë†xPB(f + fc) I{|f| â‰¤Wc}
f 	â†’Ë†xA(f + fc)
Ë†xBB
BW of xPB around fc
BW of xA around fc
2 Ã— BW of xBB
1
2 Ã— BW of xPB around fc
1
2 Ã— BW of xA around fc
BW of xBB
âˆ¥xPBâˆ¥2
2
2 âˆ¥xAâˆ¥2
2
2 âˆ¥xBBâˆ¥2
2
1
2 âˆ¥xPBâˆ¥2
2
âˆ¥xAâˆ¥2
2
âˆ¥xBBâˆ¥2
2
Table 7.1: Table relating properties of a real integrable passband signal xPB that is bandlimited to W Hz around the carrier
frequency fc to those of its analytic representation xA and its baseband representation xBB. Same-row entries are equal. The cutoï¬€
frequency Wc is assumed to be in the range W/2 â‰¤Wc â‰¤2fc âˆ’W/2, and BW stands for bandwidth. The transformation from xPB
to xA is based on Proposition 7.5.2 with the function g in (d) being chosen as the mapping f 	â†’I{|f âˆ’fc| â‰¤Wc}.
.009
14:26:26, subject to the Cambridge Core terms of use, available at

Chapter 8
Complete Orthonormal Systems and the
Sampling Theorem
8.1
Introduction
Like Chapter 4, this chapter deals with the geometry of the space L2 of energy-
limited signals. Here, however, our focus is on inï¬nite-dimensional linear subspaces
of L2 and on the notion of a complete orthonormal system (CONS). As an
application of this geometric picture, we shall present the Sampling Theorem as
an orthonormal expansion with respect to a CONS for the space of energy-limited
signals that are bandlimited to W Hz.
8.2
Complete Orthonormal System
Recall that L2 is the space of all Lebesgue measurable signals u: R â†’C satisfying
 âˆ
âˆ’âˆ
|u(t)|2 dt < âˆ.
Also recall from Section 4.3 that a subset U of L2 is said to be a linear subspace of
L2 if U is nonempty and if the signal Î±u1 + Î²u2 is in U whenever u1, u2 âˆˆU and
Î±, Î² âˆˆC. A linear subspace is said to be ï¬nite-dimensional if there exists a ï¬nite
number of signals that span it; otherwise, it is said to be inï¬nite-dimensional. The
following are some examples of inï¬nite-dimensional linear subspaces of L2.
(i) The set of all functions of the form t 	â†’p(t) eâˆ’|t|, where p(t) is any polynomial
(of arbitrary degree).
(ii) The set of all energy-limited signals that vanish outside the interval [âˆ’1, 1]
(i.e., that map every t outside this interval to zero).
(iii) The set of all energy-limited signals that vanish outside some unspeciï¬ed
ï¬nite interval (i.e., the set containing all signals u for which there exist some
a, b âˆˆR (depending on u) such that u(t) = 0 whenever t /âˆˆ[a, b]).
(iv) The set of all energy-limited signals that are bandlimited to W Hz.
147
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

148
Complete Orthonormal Systems and the Sampling Theorem
While a basis for an inï¬nite-dimensional subspace can be deï¬ned,1 this notion does
not turn out to be very useful for our purposes. Much more useful to us is the
notion of a complete orthonormal system, which we shall deï¬ne shortly.2
To motivate the deï¬nition, consider a bi-inï¬nite sequence . . . , Ï†âˆ’1, Ï†0, Ï†1, Ï†2, . . .
in L2 satisfying the orthonormality condition
âŸ¨Ï†â„“, Ï†â„“â€²âŸ©= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ,
(8.1)
and let u be an arbitrary element of L2. Deï¬ne the signals
uL â‰œ
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“,
L = 1, 2, . . .
(8.2)
By Note 4.6.7, uL is the projection of the vector u onto the subspace spanned
by (Ï†âˆ’L, . . . , Ï†L).
By the orthonormality (8.1), the tuple (Ï†âˆ’L, . . . , Ï†L) is an
orthonormal basis for this subspace. Consequently, by Proposition 4.6.9,
âˆ¥uâˆ¥2
2 â‰¥
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©
2,
L = 1, 2, . . . ,
(8.3)
with equality if, and only if, u is indistinguishable from some linear combination
of

Ï†âˆ’L, . . . , Ï†L

. This motivates us to explore the situation where (8.3) holds
with equality when L â†’âˆand to hope that it corresponds to u beingâ€”in some
sense that needs to be made preciseâ€”indistinguishable from a limit of ï¬nite linear
combinations of . . . , Ï†âˆ’1, Ï†0, Ï†1, . . .
Deï¬nition 8.2.1 (Complete Orthonormal System). A bi-inï¬nite sequence of sig-
nals . . . , Ï†âˆ’1, Ï†0, Ï†1, . . . is said to form a complete orthonormal system or a
CONS for the linear subspace U of L2 if all three of the following conditions hold:
1) Each element of the sequence is in U
Ï†â„“âˆˆU,
â„“âˆˆZ.
(8.4)
2) The sequence satisï¬es the orthonormality condition
âŸ¨Ï†â„“, Ï†â„“â€²âŸ©= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ.
(8.5)
3) For every u âˆˆU we have
âˆ¥uâˆ¥2
2 =
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©
2,
u âˆˆU.
(8.6)
1A (Hamel) basis for a subspace U is a subset of U such that any function in U is equal to a
linear combination of a ï¬nite number of elements of the subset, and such that any ï¬nite number
of elements of the subset are linearly independent. More useful to us is the notion of a complete
orthonormal system. From a complete orthonormal system we require that each function in U be
approximately equal to a linear combination of a ï¬nite number of functions in the system.
2Mathematicians usually deï¬ne a CONS only for closed subspaces.
Such subspaces are
discussed in Section 8.6.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.2 Complete Orthonormal System
149
The following proposition considers equivalent deï¬nitions of a CONS and demon-
strates that if {Ï†â„“} is a CONS for U, then, indeed, every element of U can be
approximated by a ï¬nite linear combination of the functions {Ï†â„“}.
Proposition 8.2.2. Let U be a subspace of L2 and let the bi-inï¬nite sequence
. . . , Ï†âˆ’2, Ï†âˆ’1, Ï†0, Ï†1, . . . satisfy (8.4) & (8.5).
Then each of the following con-
ditions on {Ï†â„“} is equivalent to the condition that {Ï†â„“} forms a CONS for U:
(a) For every u âˆˆU and every Ïµ > 0 there exists some positive integer L(Ïµ) and
coeï¬ƒcients Î±âˆ’L(Ïµ), . . . , Î±L(Ïµ) âˆˆC such that
u âˆ’
L(Ïµ)

â„“=âˆ’L(Ïµ)
Î±â„“Ï†â„“

2
< Ïµ.
(8.7)
(b) For every u âˆˆU
lim
Lâ†’âˆ
u âˆ’
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

2
= 0.
(8.8)
(c) For every u âˆˆU
âˆ¥uâˆ¥2
2 =
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©
2.
(8.9)
(d) For every u, v âˆˆU
âŸ¨u, vâŸ©=
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©âŸ¨v, Ï†â„“âŸ©âˆ—.
(8.10)
Proof. Since (8.4) & (8.5) hold (by hypothesis), it follows that the additional
condition (c) is, by Deï¬nition 8.2.1, equivalent to {Ï†â„“} being a CONS. It thus only
remains to show that the four conditions are equivalent. We shall prove this by
showing that (a) â‡”(b); that (b) â‡”(c); and that (c) â‡”(d).
That (b) implies (a) is obvious because nothing precludes us from choosing Î±â„“in
(8.7) to be âŸ¨u, Ï†â„“âŸ©. That (a) implies (b) follows because, by Note 4.6.7, the signal
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“,
which we denoted in (8.2) by uL, is the projection of u onto the linear subspace
spanned by (Ï†âˆ’L, . . . , Ï†L) and as such, by Proposition 4.6.8, best approximates u
among all the signals in that subspace. Consequently, replacing Î±â„“by âŸ¨u, Ï†â„“âŸ©can
only reduce the LHS of (8.7).
To prove (b) â‡’(c) we ï¬rst note that by letting L tend to inï¬nity in (8.3) it follows
that
âˆ¥uâˆ¥2
2 â‰¥
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©
2,
u âˆˆL2,
(8.11)
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

150
Complete Orthonormal Systems and the Sampling Theorem
so to establish (c) we only need to show that if u is in U then âˆ¥uâˆ¥2
2 is also upper-
bounded by the RHS of (8.11). To that end we ï¬rst upper-bound âˆ¥uâˆ¥2 as
âˆ¥uâˆ¥2 =

	
u âˆ’
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

+
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

2
â‰¤
u âˆ’
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

2
+

L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

2
=
u âˆ’
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

2
+
	
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©
2

1/2
,
u âˆˆL2,
(8.12)
where the ï¬rst equality follows by adding and subtracting a term; the subsequent in-
equality by the Triangle Inequality (Proposition 3.4.1); and the ï¬nal equality by the
orthonormality assumption (8.5) and the Pythagorean Theorem (Theorem 4.5.2).
If Condition (b) holds and if u is in U, then the RHS of (8.12) converges to the
square root of the inï¬nite sum 
â„“âˆˆZ|âŸ¨u, Ï†â„“âŸ©|2 and thus gives us the desired upper
bound on âˆ¥uâˆ¥2.
We next prove (c) â‡’(b). We assume that (c) holds and that u is in U and set out
to prove (8.8). To that end we ï¬rst note that by the basic properties of the inner
product (3.6)â€“(3.10) and by the orthonormality (8.5) it follows that

u âˆ’
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“



uâ€²
, Ï†â„“â€²

= âŸ¨u, Ï†â„“â€²âŸ©I{|â„“â€²| > L},

â„“â€² âˆˆZ, u âˆˆL2

.
Consequently, if we apply (c) to the under-braced signal uâ€² (which for u âˆˆU is
also in U) we obtain that (c) implies
u âˆ’
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

2
2
=

|â„“|>L
âŸ¨u, Ï†â„“âŸ©
2,
u âˆˆU.
But by applying (c) to u we infer that the RHS of the above tends to zero as L
tends to inï¬nity, thus establishing (8.8) and hence (b).
We next prove (c) â‡”(d). The implication (d) â‡’(c) is obvious because we can
always choose v to be equal to u. We consequently focus on proving (c) â‡’(d).
We do so by assuming that u, v âˆˆU and calculating for every Î² âˆˆC
|Î²|2 âˆ¥uâˆ¥2
2 + 2 Re

Î²âŸ¨u, vâŸ©

+ âˆ¥vâˆ¥2
2
= âˆ¥Î² u + vâˆ¥2
2
=
âˆ

â„“=âˆ’âˆ
âŸ¨Î² u + v, Ï†â„“âŸ©
2
=
âˆ

â„“=âˆ’âˆ
Î²âŸ¨u, Ï†â„“âŸ©+ âŸ¨v, Ï†â„“âŸ©
2
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.3 The Fourier Series
151
= |Î²|2
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©
2 + 2 Re
	
Î²
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©âŸ¨v, Ï†â„“âŸ©âˆ—

+
âˆ

â„“=âˆ’âˆ
âŸ¨v, Ï†â„“âŸ©
2,

u, v âˆˆU, Î² âˆˆC

,
(8.13)
where the ï¬rst equality follows by writing âˆ¥Î²u + vâˆ¥2
2 as âŸ¨Î²u + v, Î²u + vâŸ©and using
the basic properties of the inner product (3.6)â€“(3.10); the second by applying (c)
to Î²u + v (which for u, v âˆˆU is also in U); the third by the basic properties of
the inner product; and the ï¬nal equality by writing the squared magnitude of a
complex number as its product by its conjugate. By applying (c) to u and likewise
to v, we now obtain from (8.13) that
2 Re

Î²âŸ¨u, vâŸ©

= 2 Re
	
Î²
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©âŸ¨v, Ï†â„“âŸ©âˆ—

,

u, v âˆˆU, Î² âˆˆC

,
which can only hold for all Î² âˆˆC (and in particular for both Î² = 1 and Î² = i) if
âŸ¨u, vâŸ©=
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©âŸ¨v, Ï†â„“âŸ©âˆ—,
u, v âˆˆU,
thus establishing (d).
We next describe the two complete orthonormal systems that will be of most in-
terest to us.
8.3
The Fourier Series
A CONS that you have probably already encountered is the one underlying the
Fourier Series representation. You may have encountered the Fourier Series in the
context of periodic functions, but we shall focus on a slightly diï¬€erent view.
Proposition 8.3.1. For every T > 0, the functions {Ï†â„“} deï¬ned for every integer â„“
by
Ï†â„“: t 	â†’
1
âˆš
2T
eiÏ€â„“t/T I{|t| â‰¤T}
(8.14)
form a CONS for the subspace

u âˆˆL2 : u(t) = 0 whenever |t| > T

of energy-limited signals that vanish outside the interval [âˆ’T, T ].
Proof. Follows from Theorem A.3.3 in Appendix A by substituting 2T for S.
Notice that in this case
âŸ¨u, Ï†â„“âŸ©=
1
âˆš
2T
 T
âˆ’T
u(t) eâˆ’iÏ€â„“t/T dt
(8.15)
is the â„“-th Fourier Series Coeï¬ƒcient of u with respect to the interval [âˆ’T, T); see
Note A.3.5 in Appendix A with 2T substituted for S.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

152
Complete Orthonormal Systems and the Sampling Theorem
Note 8.3.2. The dummy argument t is immaterial in Proposition 8.3.1. Indeed, if
we deï¬ne for W > 0 the linear subspace
V =

g âˆˆL2 : g(f) = 0 whenever |f| > W

,
(8.16)
then the functions deï¬ned for every integer â„“by
f 	â†’
1
âˆš
2W
eiÏ€â„“f/W I{|f| â‰¤W}
(8.17)
form a CONS for this subspace.
This note will be crucial when we next discuss a CONS for the space of energy-
limited signals that are bandlimited to W Hz.
8.4
The Sampling Theorem
We next provide a CONS for the space of energy-limited signals that are band-
limited to W Hz. Recall that if x is an energy-limited signal that is bandlimited
to W Hz, then there exists a measurable function3 g: f 	â†’g(f) satisfying
g(f) = 0,
|f| > W
(8.18)
and
 W
âˆ’W
|g(f)|2 df < âˆ,
(8.19)
such that
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR.
(8.20)
Conversely, if g is any function satisfying (8.18) & (8.19), and if we deï¬ne x via
(8.20) as the Inverse Fourier Transform of g, then x is an energy-limited signal that
is bandlimited to W Hz and its L2-Fourier Transform Ë†x is equal to (the equivalence
class of) g.
Thus, if, as in (8.16), we denote by V the set of all functions (of frequency) satisfying
(8.18) & (8.19), then the set of all energy-limited signals that are bandlimited to W
Hz is just the image of V under the IFT, i.e., it is the set Ë‡V, where
Ë‡V â‰œ
Ë‡g : g âˆˆV

.
(8.21)
By the Mini Parseval Theorem (Proposition 6.2.6 (i)), if x1 and x2 are given by
Ë‡g1 and Ë‡g2, where g1, g2 are in V, then
âŸ¨x1, x2âŸ©= âŸ¨g1, g2âŸ©,
(8.22)
i.e.,
âŸ¨Ë‡g1, Ë‡g2âŸ©= âŸ¨g1, g2âŸ©,
g1, g2 âˆˆV.
(8.23)
The following lemma is a simple but very useful consequence of (8.23).
3Loosely speaking, this function is the Fourier Transform of x. But since x is not necessarily
integrable, its FT Ë†x is an equivalence class of signals. Thus, more precisely, the equivalence class
of g is the L2 -Fourier Transform of x. Or, stated diï¬€erently, g can be any one of the signals in
the equivalence class of Ë†x that is zero outside the interval [âˆ’W, W ].
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.4 The Sampling Theorem
153
Lemma 8.4.1. If {Ïˆâ„“} is a CONS for the subspace V, which is deï¬ned in (8.16),
then { Ë‡Ïˆâ„“} is a CONS for the subspace Ë‡V, which is deï¬ned in (8.21).
Proof. Let {Ïˆâ„“} be a CONS for the subspace V. By (8.23),
 Ë‡Ïˆâ„“, Ë‡Ïˆâ„“â€²
= âŸ¨Ïˆâ„“, Ïˆâ„“â€²âŸ©,
â„“, â„“â€² âˆˆZ,
so our assumption that {Ïˆâ„“} is a CONS for V (and hence that, a fortiori, it satisï¬es
âŸ¨Ïˆâ„“, Ïˆâ„“â€²âŸ©= I{â„“= â„“â€²} for all â„“, â„“â€² âˆˆZ) implies that
 Ë‡Ïˆâ„“, Ë‡Ïˆâ„“â€²
= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ.
It remains to verify that for every x âˆˆË‡V
âˆ

â„“=âˆ’âˆ

x, Ë‡Ïˆâ„“
2 = âˆ¥xâˆ¥2
2 .
Equivalently, since every x âˆˆË‡V can be written as Ë‡g for some g âˆˆV, we need to
show that
âˆ

â„“=âˆ’âˆ
Ë‡g, Ë‡Ïˆâ„“
2 = âˆ¥Ë‡gâˆ¥2
2 ,
g âˆˆV.
This follows from (8.23) and from our assumption that {Ïˆâ„“} is a CONS for V
because
âˆ

â„“=âˆ’âˆ
Ë‡g, Ë‡Ïˆâ„“
2 =
âˆ

â„“=âˆ’âˆ
âŸ¨g, Ïˆâ„“âŸ©
2
= âˆ¥gâˆ¥2
2
= âˆ¥Ë‡gâˆ¥2
2 ,
g âˆˆV,
where the ï¬rst equality follows from (8.23) (by substituting g for g1 and by sub-
stituting Ïˆâ„“for g2); the second from the assumption that {Ïˆâ„“} is a CONS for V;
and the ï¬nal equality from (8.23) (by substituting g for g1 and for g2).
Using this lemma and Note 8.3.2 we now derive a CONS for the subspace Ë‡V of
energy-limited signals that are bandlimited to W Hz.
Proposition 8.4.2 (A CONS for the Subspace of Energy-Limited Signals that
Are Bandlimited to W Hz).
(i) The sequence of signals that are deï¬ned for every integer â„“by
t 	â†’
âˆš
2W sinc(2Wt + â„“)
(8.24)
forms a CONS for the space of energy-limited signals that are bandlimited
to W Hz.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

154
Complete Orthonormal Systems and the Sampling Theorem
(ii) If x is an energy-limited signal that is bandlimited to W Hz, then its inner
product with the â„“-th signal is given by its scaled sample at time âˆ’â„“/(2W):
#
x, t 	â†’
âˆš
2W sinc(2Wt + â„“)
$
=
1
âˆš
2W
x

âˆ’â„“
2W

,
â„“âˆˆZ.
(8.25)
Proof. To prove Part (i) we recall that, by Note 8.3.2, the functions deï¬ned for
every â„“âˆˆZ by
Ïˆâ„“: f 	â†’
1
âˆš
2W
eiÏ€â„“f/W I{|f| â‰¤W}
(8.26)
form a CONS for the subspace V. Consequently, by Lemma 8.4.1, their Inverse
Fourier Transforms { Ë‡Ïˆâ„“} form a CONS for Ë‡V.
It just remains to evaluate Ë‡Ïˆâ„“
explicitly in order to verify that it is a scaled shifted sinc(Â·):
Ë‡Ïˆâ„“(t) =
 âˆ
âˆ’âˆ
Ïˆâ„“(f) ei2Ï€ft df
=
 W
âˆ’W
1
âˆš
2W
eiÏ€â„“f/W ei2Ï€ft df
(8.27)
=
âˆš
2W sinc(2Wt + â„“),
(8.28)
where the last calculation can be veriï¬ed by direct computation as in (6.35).
We next prove Part (ii). Since x is an energy-limited signal that is bandlimited
to W Hz, it follows that there exists some g âˆˆV such that
x = Ë‡g,
(8.29)
i.e.,
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR.
(8.30)
Consequently,
#
x, t 	â†’
âˆš
2W sinc(2Wt + â„“)
$
=

x, Ë‡Ïˆâ„“

=
Ë‡g, Ë‡Ïˆâ„“

= âŸ¨g, Ïˆâ„“âŸ©
=
 W
âˆ’W
g(f)

1
âˆš
2W
eiÏ€â„“f/Wâˆ—
df
=
1
âˆš
2W
 W
âˆ’W
g(f) eâˆ’iÏ€â„“f/W df
=
1
âˆš
2W
x

âˆ’â„“
2W

,
â„“âˆˆZ,
where the ï¬rst equality follows from (8.28); the second by (8.29); the third by (8.23)
(with the substitution of g for g1 and Ïˆâ„“for g2); the fourth by the deï¬nition of
the inner product and by (8.26); the ï¬fth by conjugating the complex exponential;
and the ï¬nal equality by substituting âˆ’â„“/(2W) for t in (8.30).
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.4 The Sampling Theorem
155
Using Proposition 8.4.2 and Proposition 8.2.2 we obtain the following L2 version
of the Sampling Theorem.
Theorem 8.4.3 (L2-Sampling Theorem). Let x be an energy-limited signal that
is bandlimited to W Hz, where W > 0, and let
T =
1
2W.
(8.31)
(i) The signal x can be reconstructed from the sequence . . . , x(âˆ’T), x(0), x(T), . . .
of its values at integer multiples of T in the sense that
lim
Lâ†’âˆ
 âˆ
âˆ’âˆ
x(t) âˆ’
L

â„“=âˆ’L
x(âˆ’â„“T) sinc
 t
T + â„“

2
dt = 0.
(ii) The signalâ€™s energy can be reconstructed from its samples via the relation
 âˆ
âˆ’âˆ
|x(t)|2 dt = T
âˆ

â„“=âˆ’âˆ
|x(â„“T)|2.
(iii) If y is another energy-limited signal that is bandlimited to W Hz, then
âŸ¨x, yâŸ©= T
âˆ

â„“=âˆ’âˆ
x(â„“T) yâˆ—(â„“T).
Note 8.4.4. If T â‰¤1/(2W), then any energy-limited signal x that is bandlimited
to W Hz is also bandlimited to 1/(2T) Hz. Consequently, Theorem 8.4.3 continues
to hold if we replace (8.31) with the condition
0 < T â‰¤
1
2W.
(8.32)
Table 8.1 on Page 167 highlights the duality between the Sampling Theorem and
the Fourier Series.
We also mention here without proof a version of the Sampling Theorem that al-
lows one to reconstruct the signal pointwise, i.e., at every epoch t. Thus, while
Theorem 8.4.3 guarantees that, as more and more terms in the sum of the shifted
sinc functions are added, the energy in the error function tends to zero, the follow-
ing theorem demonstrates that at every ï¬xed time t the error tends to zero. The
assumptions are fairly mild and are satisï¬ed by any energy-limited signal that is
bandlimited to W Hz. (For such signals a proof is sketched in Exercise 8.12.)
Theorem 8.4.5 (Pointwise Sampling Theorem). If the signal x can be represented
as
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR
(8.33)
for some function g satisfying
 W
âˆ’W
|g(f)| df < âˆ,
(8.34)
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

156
Complete Orthonormal Systems and the Sampling Theorem
and if 0 < T â‰¤1/(2W), then for every t âˆˆR
x(t) = lim
Lâ†’âˆ
L

â„“=âˆ’L
x(âˆ’â„“T) sinc
	 t
T + â„“

.
(8.35)
Proof. See (Pinsky, 2002, Chapter 4, Section 4.2.3, Theorem 4.2.13).
The Sampling Theorem goes by various names.
It is sometimes attributed to
Claude Elwood Shannon (1916â€“2001), the founder of Information Theory.
But
it also appears in the works of Vladimir Aleksandrovich Kotelnikov (1908â€“2005),
Harry Nyquist (1889â€“1976), and Edmund Taylor Whittaker (1873â€“1956). For fur-
ther references regarding the history of this result and for a survey of many related
results, see (Unser, 2000).
8.5
The Samples of the Convolution
Digital signal processing owes much of its success to the fact that the samples of a
convolution of bandlimited signals can be expressed in terms of the samples of the
signals. This is made precise in the following theorem.
Theorem 8.5.1 (The Samples of a Convolution). Let x and y be energy-limited
signals that are bandlimited to W Hz. Then

x â‹†y
 â„“
2W

=
1
2W
âˆ

Î½=âˆ’âˆ
x
 Î½
2W

y
â„“âˆ’Î½
2W

,
â„“âˆˆZ.
(8.36)
Proof. For every integer â„“,

x â‹†y
 â„“
2W

=
 âˆ
âˆ’âˆ
x(Ï„) y
 â„“
2W âˆ’Ï„

dÏ„
=

x, Ï„ 	â†’yâˆ— â„“
2W âˆ’Ï„

=
1
2W
âˆ

Î½=âˆ’âˆ
x
 Î½
2W

y
â„“âˆ’Î½
2W

,
where the ï¬rst equality follows from the deï¬nition of the convolution; the second
follows by expressing the integral as an inner product; and where in the last step
we have noted that both x and Ï„ 	â†’yâˆ—
â„“/(2W) âˆ’Ï„

are energy-limited signals
that are bandlimited to W Hz, so their inner product can be computed from their
samples using Theorem 8.4.3 (iii).
8.6
Closed Subspaces of L2
Our deï¬nition of a CONS for a subspace U is not quite standard, because we
only assumed that U is a linear subspace; we did not assume that U is closed.
In this section we shall deï¬ne closed linear subspaces and derive a condition for
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.6 Closed Subspaces of L2
157
a sequence {Ï†â„“} to form a CONS for a closed subspace U. (The set of energy-
limited signals that vanish outside the interval [âˆ’T, T ] is closed, as is the class of
energy-limited signals that are bandlimited to W Hz.)
Before proceeding to deï¬ne closed linear subspaces, we pause here to recall that
the space L2 is complete.4
Theorem 8.6.1 (L2 Is Complete). If the sequence u1, u2, . . . of signals in L2 is
such that for any Ïµ > 0 there exists a positive integer L(Ïµ) such that
âˆ¥un âˆ’umâˆ¥2 < Ïµ,
n, m > L(Ïµ),
then there exists some function u âˆˆL2 such that
lim
nâ†’âˆâˆ¥u âˆ’unâˆ¥2 = 0.
Proof. See, for example, (Rudin, 1987, Chapter 3, Theorem 3.11).
Deï¬nition 8.6.2 (Closed Subspace). A linear subspace U of L2 is said to be
closed if for any sequence of signals u1, u2, . . . in U and any u âˆˆL2, the condition
âˆ¥u âˆ’unâˆ¥2 â†’0 implies that u is indistinguishable from some element of U.
Before stating the next theorem we remind the reader that a bi-inï¬nite sequence
of complex numbers . . . , Î±âˆ’1, Î±0, Î±1, . . . is said to be square summable if
âˆ

â„“=âˆ’âˆ
|Î±â„“|2 < âˆ.
Theorem 8.6.3 (Riesz-Fischer). Let U be a closed linear subspace of L2, and let
the bi-inï¬nite sequence . . . , Ï†âˆ’1, Ï†0, Ï†1, . . . satisfy (8.4) & (8.5). Let the bi-inï¬nite
sequence of complex numbers . . . , Î±âˆ’1, Î±0, Î±1, . . . be square summable. Then there
exists an element u in U satisfying
lim
Lâ†’âˆ
u âˆ’
L

â„“=âˆ’L
Î±â„“Ï†â„“

2
= 0;
(8.37a)
âŸ¨u, Ï†â„“âŸ©= Î±â„“,
â„“âˆˆZ;
(8.37b)
and
âˆ¥uâˆ¥2
2 =
âˆ

â„“=âˆ’âˆ
|Î±â„“|2.
(8.37c)
Proof. Deï¬ne for every positive integer L
uL =
L

â„“=âˆ’L
Î±â„“Ï†â„“,
L âˆˆN.
(8.38)
4This property is usually stated about L2 but we prefer to work with L2 .
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

158
Complete Orthonormal Systems and the Sampling Theorem
Since, by hypothesis, U is a linear subspace and the signals {Ï†â„“} are all in U, it fol-
lows that uL âˆˆU. By the orthonormality assumption (8.5) and by the Pythagorean
Theorem (Theorem 4.5.2), it follows that
âˆ¥un âˆ’umâˆ¥2
2 =

min{m,n}<|â„“|â‰¤max{m,n}
|Î±â„“|2
â‰¤

min{m,n}<|â„“|<âˆ
|Î±â„“|2,
n, m âˆˆN.
From this and from the square summability of {Î±â„“}, it follows that for any Ïµ > 0
we have that âˆ¥un âˆ’umâˆ¥2 is smaller than Ïµ whenever both n and m are suï¬ƒciently
large. By the completeness of L2 it thus follows that there exists some uâ€² âˆˆL2
such that
lim
Lâ†’âˆâˆ¥uâ€² âˆ’uLâˆ¥2 = 0.
(8.39)
Since U is closed, and since uL is in U for every L âˆˆN, it follows from (8.39) that uâ€²
is indistinguishable from some element u of U:
âˆ¥u âˆ’uâ€²âˆ¥2 = 0.
(8.40)
It now follows from (8.39) and (8.40) that
lim
Lâ†’âˆâˆ¥u âˆ’uLâˆ¥2 = 0,
(8.41)
as can be veriï¬ed using (4.14) (with the substitution (uâ€² âˆ’uL) for x and (u âˆ’uâ€²)
for y). Combining (8.41) with (8.38) establishes (8.37a).
To establish (8.37b) we use (8.41) and the continuity of the inner product (Propo-
sition 3.4.2) to calculate âŸ¨u, Ï†â„“âŸ©for every ï¬xed â„“âˆˆZ as follows:
âŸ¨u, Ï†â„“âŸ©= lim
Lâ†’âˆâŸ¨uL, Ï†â„“âŸ©
= lim
Lâ†’âˆ

L

â„“â€²=âˆ’L
Î±â„“â€² Ï†â„“â€², Ï†â„“

= lim
Lâ†’âˆÎ±â„“I{|â„“| â‰¤L}
= Î±â„“,
â„“âˆˆZ,
where the ï¬rst equality follows from (8.41) and from the continuity of the inner
product (Proposition 3.4.2); the second by (8.38); the third by the orthonormality
(8.5); and the ï¬nal equality because Î±â„“I{|â„“| â‰¤L} is equal to Î±â„“, whenever L is
large enough (i.e., exceeds |â„“|).
It remains to prove (8.37c). By the orthonormality of {Ï†â„“} and the Pythagorean
Theorem (Theorem 4.5.2)
âˆ¥uLâˆ¥2
2 =
L

â„“=âˆ’L
Î±â„“
2,
L âˆˆN.
(8.42)
Also, by (4.14) (with the substitution of u for x and of (uL âˆ’u) for y) we obtain
âˆ¥uâˆ¥2 âˆ’âˆ¥u âˆ’uLâˆ¥2 â‰¤âˆ¥uLâˆ¥2 â‰¤âˆ¥uâˆ¥2 + âˆ¥u âˆ’uLâˆ¥2 .
(8.43)
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.6 Closed Subspaces of L2
159
It now follows from (8.43), (8.41), and the Sandwich Theorem5 that
lim
Lâ†’âˆâˆ¥uLâˆ¥2 = âˆ¥uâˆ¥2 ,
(8.44)
which combines with (8.42) to prove (8.37c).
By applying Theorem 8.6.3 to the space of energy-limited signals that are band-
limited to W Hz and to the CONS that we derived for that space in Proposi-
tion 8.4.2 we obtain:
Proposition 8.6.4. Any square-summable bi-inï¬nite sequence of complex numbers
corresponds to the samples at integer multiples of T of an energy-limited signal that
is bandlimited to 1/(2T) Hz. Here T > 0 is arbitrary.
Proof. Let . . . , Î²âˆ’1, Î²0, Î²1, . . . be a square-summable bi-inï¬nite sequence of com-
plex numbers, and let W = 1/(2T). We seek a signal u that is an energy-limited
signal that is bandlimited to W Hz and whose samples are given by u(â„“T) = Î²â„“,
for every integer â„“. Since the set of all energy-limited signals that are bandlimited
to W Hz is a closed linear subspace of L2, and since the sequence { Ë‡Ïˆâ„“} (given ex-
plicitly in (8.28) as Ë‡Ïˆâ„“: t 	â†’
âˆš
2W sinc(2Wt+â„“)) is an orthonormal sequence in that
subspace, it follows from Theorem 8.6.3 (with the substitution of Ë‡Ïˆâ„“for Ï†â„“and of
Î²âˆ’â„“/
âˆš
2W for Î±â„“) that there exists an energy-limited signal u that is bandlimited
to W Hz and for which

u, Ë‡Ïˆâ„“

=
1
âˆš
2W
Î²âˆ’â„“,
â„“âˆˆZ.
(8.45)
By Proposition 8.4.2 (ii),

u, Ë‡Ïˆâ„“

=
1
âˆš
2W
u(âˆ’â„“T),
â„“âˆˆZ,
(8.46)
so by (8.45) and (8.46)
u(âˆ’â„“T) = Î²âˆ’â„“,
â„“âˆˆZ.
We now give an alternative characterization of a CONS for a closed subspace of L2.
This result will not be used later in the book.
Proposition 8.6.5 (Characterization of a CONS for a Closed Subspace).
(i) If the bi-inï¬nite sequence {Ï†â„“} is a CONS for the linear subspace U âŠ†L2,
then an element of U whose inner product with Ï†â„“is zero for every integer â„“
must have zero energy:

âŸ¨u, Ï†â„“âŸ©= 0,
â„“âˆˆZ

=â‡’

âˆ¥uâˆ¥2 = 0

,
u âˆˆU.
(8.47)
5The Sandwich Theorem states that if the sequences of real number {an}, {bn} and {cn} are
such that bn â‰¤an â‰¤cn for every n, and if the sequences {bn} and {cn} converge to the same
limit, then {an} also converges to that limit.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

160
Complete Orthonormal Systems and the Sampling Theorem
(ii) If U is a closed subspace of L2 and if the bi-inï¬nite sequence {Ï†â„“} satisï¬es
(8.4) & (8.5), then Condition (8.47) is equivalent to the condition that {Ï†â„“}
forms a CONS for U.
Proof. We begin by proving Part (i). By deï¬nition, if {Ï†â„“} is a CONS for U, then
(8.6) must hold for every u âˆˆU. Consequently, if for some u âˆˆU we have that
âŸ¨u, Ï†â„“âŸ©is zero for all â„“âˆˆZ, then the RHS of (8.6) is zero and hence the LHS must
also be zero, thus showing that u must be of zero energy.
We next turn to Part (ii) and assume that U is closed and that the bi-inï¬nite
sequence {Ï†â„“} satisï¬es (8.4) & (8.5). That the condition that {Ï†â„“} is a CONS
implies Condition (8.47) follows from Part (i). It thus remains to show that if
Condition (8.47) holds, then {Ï†â„“} is a CONS. To prove this we now assume that U
is a closed subspace; that {Ï†â„“} satisï¬es (8.4) & (8.5); and that (8.47) holds and
set out to prove that
âˆ¥uâˆ¥2
2 =
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©
2,
u âˆˆU.
(8.48)
To establish (8.48) ï¬x some arbitrary u âˆˆU. Since U âŠ†L2, the fact that u is
in U implies that it is of ï¬nite energy, which combines with (8.3) to imply that the
bi-inï¬nite sequence . . . , âŸ¨u, Ï†âˆ’1âŸ©, âŸ¨u, Ï†0âŸ©, âŸ¨u, Ï†1âŸ©, . . . is square summable. Since,
by hypothesis, U is closed, this implies, by Theorem 8.6.3 (with the substitution
of âŸ¨u, Ï†â„“âŸ©for Î±â„“), that there exists some element Ëœu âˆˆU such that
lim
Lâ†’âˆ
Ëœu âˆ’
L

â„“=âˆ’L
âŸ¨u, Ï†â„“âŸ©Ï†â„“

2 = 0;
(8.49a)
âŸ¨Ëœu, Ï†â„“âŸ©= âŸ¨u, Ï†â„“âŸ©,
â„“âˆˆZ;
(8.49b)
and
âˆ¥Ëœuâˆ¥2
2 =
âˆ

â„“=âˆ’âˆ
âŸ¨u, Ï†â„“âŸ©
2.
(8.49c)
By (8.49b) it follows that the element u âˆ’Ëœu of U satisï¬es
âŸ¨u âˆ’Ëœu, Ï†â„“âŸ©= 0,
â„“âˆˆZ,
and hence, by Condition (8.47), is of zero energy
âˆ¥u âˆ’Ëœuâˆ¥2 = 0,
(8.50)
so u and Ëœu are indistinguishable and hence
âˆ¥uâˆ¥2 = âˆ¥Ëœuâˆ¥2 .
This combines with (8.49c) to prove (8.48).
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.7 An Isomorphism
161
8.7
An Isomorphism
In this section we collect the results of Theorem 8.4.3 and Proposition 8.6.4 into a
single theorem about the isomorphism between the space of energy-limited signals
that are bandlimited to W Hz and the space of square-summable sequences. This
theorem is at the heart of quantization schemes for bandlimited signals. It demon-
strates that to describe a bandlimited signal one can use discrete-time processing to
quantize its samples and one can then map the quantized samples to a bandlimited
signal. The energy in the error signal corresponding to the diï¬€erence between the
original signal and its description is then proportional to the sum of the squared
diï¬€erences between the samples of the original signal and the quantized version.
Theorem 8.7.1 (Bandlimited Signals and Square-Summable Sequences). Let
T = 1/(2W), where W > 0.
(i) If u is an energy-limited signal that is bandlimited to W Hz, then the bi-
inï¬nite sequence
. . . , u(âˆ’T), u(0), u(T), u(2T), . . .
consisting of its samples taken at integer multiples of T is square summable
and
T
âˆ

â„“=âˆ’âˆ
u(â„“T)
2 = âˆ¥uâˆ¥2
2 .
(ii) More generally, if u and v are energy-limited signals that are bandlimited
to W Hz, then
T
âˆ

â„“=âˆ’âˆ
u(â„“T) vâˆ—(â„“T) = âŸ¨u, vâŸ©.
(iii) If {Î±â„“} is a bi-inï¬nite square-summable sequence, then there exists an energy-
limited signal u that is bandlimited to W Hz such that its samples are given
by
u(â„“T) = Î±â„“,
â„“âˆˆZ.
(iv) The mapping that maps every energy-limited signal that is bandlimited to W
Hz to the square-summable sequence consisting of its samples is linear.
8.8
Prolate Spheroidal Wave Functions
The following result, which is due to Slepian and Pollak, will not be used in this
book; it is included for its sheer beauty.
Theorem 8.8.1. Let the positive constants T > 0 and W > 0 be given.
Then
there exists a sequence of real functions Ï†1, Ï†2, . . . and a corresponding sequence
of positive numbers Î»1 > Î»2 > Â· Â· Â· such that:
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

162
Complete Orthonormal Systems and the Sampling Theorem
(i) The sequence Ï†1, Ï†2, . . . forms a CONS for the space of energy-limited signals
that are bandlimited to W Hz, so, a fortiori,
 âˆ
âˆ’âˆ
Ï†â„“(t) Ï†â„“â€²(t) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆN.
(8.51a)
(ii) The sequence of scaled and time-windowed functions ËœÏ†1,w, ËœÏ†2,w, . . . deï¬ned at
every t âˆˆR by
ËœÏ†â„“,w(t) =
1
âˆšÎ»â„“
Ï†â„“(t) I
'
|t| â‰¤T
2
(
,
â„“âˆˆN
(8.51b)
forms a CONS for the subspace of L2 consisting of all energy-limited signals
that vanish outside the interval [âˆ’T/2, T/2], so, a fortiori,
 T/2
âˆ’T/2
Ï†â„“(t) Ï†â„“â€²(t) dt = Î»â„“I{â„“= â„“â€²},
â„“, â„“â€² âˆˆN.
(8.51c)
(iii) For every t âˆˆR,
 T/2
âˆ’T/2
LPFW(t âˆ’Ï„) Ï†â„“(Ï„) dÏ„ = Î»â„“Ï†â„“(t),
â„“âˆˆN.
(8.51d)
The above functions Ï†1, Ï†2, . . . are related to Prolate Spheroidal Wave Functions.
For a discussion of this connection, a proof of this theorem, and numerous appli-
cations see (Slepian and Pollak, 1961) and (Slepian, 1976).
8.9
Exercises
Exercise 8.1 (Orthogonality with One Exception). Let . . . , Ï†âˆ’1, Ï†0, Ï†1, . . . form a CONS
for the linear subspace U of L2. Show that if u âˆˆU is orthogonal to every Ï†â„“except
possibly to Ï†0, then u is indistinguishable from Î± Ï†0 for some Î± âˆˆC.
Exercise 8.2 (Expansion of a Function). Expand the function t â†’sinc2(t/2) as an or-
thonormal expansion in the functions
. . . , t â†’sinc(t + 2), t â†’sinc(t + 1), t â†’sinc(t), t â†’sinc(t âˆ’1), t â†’sinc(t âˆ’2), . . .
Exercise 8.3 (Expanding a Delayed sinc). Prove that for every W > 0 and t, Ï„ âˆˆR,
sinc

2W(t âˆ’Ï„)

=
âˆ

â„“=âˆ’âˆ
sinc

2WÏ„ + â„“

sinc

2Wt + â„“

.
Exercise 8.4 (The Perils of Sub-Nyquist Sampling).
(i) Show that if T exceeds 1/(2W) then there exists a signal x of positive energy whose
samples x(â„“T) are zero for every integer â„“âˆˆZ and that is nonetheless an energy-
limited signal that is bandlimited to W Hz. This signal and the all-zero signal have
identical samples but are not indistinguishable.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.9 Exercises
163
(ii) Show that if T exceeds 1/(2W) and y is an energy-limited signal that is bandlimited
to W Hz, then there exists a signal Ëœy that is not indistinguishable from y and that
nonetheless is an energy-limited signal that is bandlimited to W Hz and whose
samples satisfy Ëœy(â„“T) = y(â„“T) for every â„“âˆˆZ.
Hint: For Part (i) consider the product of a sinc and a sine wave.
Exercise 8.5 (Inner Product with a Bandlimited Signal). Show that if x is an energy-
limited signal that is bandlimited to W Hz, and if y âˆˆL2, then
âŸ¨x, yâŸ©= Ts
âˆ

â„“=âˆ’âˆ
x(â„“Ts) yâˆ—
LPF(â„“Ts),
where yLPF is the result of passing y through an ideal unit-gain lowpass ï¬lter of bandwidth
W Hz, and where Ts = 1/(2W).
Exercise 8.6 (Approximating a Sinc by Sincs). Find the coeï¬ƒcients {Î±â„“} that minimize
the integral
 âˆ
âˆ’âˆ
	
sinc(3t/2) âˆ’
âˆ

â„“=âˆ’âˆ
Î±â„“sinc(t âˆ’â„“)

2
dt.
What is the value of this integral when the coeï¬ƒcients are chosen as you suggest?
Exercise 8.7 (More on Energy and Samples). Let x be an integrable signal that is band-
limited to W Hz such that the signal t â†’t x(t) is also integrable. Express the energy in
the signal t â†’t x(t) in terms of the samples . . . , x(0), x(1/(2W)), . . . of x.
Hint: Recall Exercise 6.2.
Exercise 8.8 (The Derivative as an Inner Product). Let u be an energy-limited signal
that is bandlimited to W Hz. Show that its derivative at zero can be expressed as the
inner product âŸ¨u, vâŸ©, where
v(t) = âˆ’d
dt2W sinc(2Wt),
t âˆˆR.
Exercise 8.9 (Integrability and Summability). Show that if x is an integrable signal that
is bandlimited to W Hz and if Ts = 1/(2W), then
âˆ

â„“=âˆ’âˆ
x(â„“Ts)
 < âˆ.
Hint: Let h be the IFT of the mapping in (7.16) when we substitute 0 for fc; 2W for W;
and 2W + Î” for Wc, where Î” > 0.
Express x(â„“Ts) as

x â‹†h

(â„“Ts); upper-bound the
convolution integral using Proposition 2.4.1; and use Fubiniâ€™s Theorem to swap the order
of summation and integration.
Exercise 8.10 (More on Integrability and Summability). Use Exercise 8.9 to show that if
x is an integrable signal that is bandlimited to W Hz and if Ts > 0 is arbitrary, then
âˆ

â„“=âˆ’âˆ
|x(â„“Ts)| < âˆ.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

164
Complete Orthonormal Systems and the Sampling Theorem
Exercise 8.11 (Approximating an Integral by a Sum). One often approximates an integral
by a sum, e.g.,
 âˆ
âˆ’âˆ
x(t) dt â‰ˆÎ´
âˆ

â„“=âˆ’âˆ
x(â„“Î´).
(i) Show that if u is an energy-limited signal that is bandlimited to W Hz, then, for
every 0 < Î´ â‰¤1/(2W), the above approximation is exact when we substitute |u(t)|2
for x(t), that is,
 âˆ
âˆ’âˆ
|u(t)|2 dt = Î´
âˆ

â„“=âˆ’âˆ
|u(â„“Î´)|2.
(ii) Show that if x is an integrable signal that is bandlimited to W Hz, then, for every
0 < Î´ â‰¤1/(2W),
 âˆ
âˆ’âˆ
x(t) dt = Î´
âˆ

â„“=âˆ’âˆ
x(â„“Î´).
(iii) Consider the signal u: t â†’sinc(t). Compute âˆ¥uâˆ¥2
2 using Parsevalâ€™s Theorem and
use the result and Part (i) to show that
âˆ

m=0
1
(2m + 1)2 = Ï€2
8 .
For a stronger version of Part (ii), see (Pinsky, 2002, Chapter 4, Proposition 4.2.14).
Exercise 8.12 (On the Pointwise Sampling Theorem).
(i) Let the functions g, g0, g1, . . . be elements of L2 that are zero outside the interval
[âˆ’W, W ]. Show that if âˆ¥g âˆ’gnâˆ¥2 â†’0, then for every t âˆˆR
lim
nâ†’âˆ
 âˆ
âˆ’âˆ
gn(f) ei2Ï€ft df =
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df.
(ii) Use Part (i) to prove the Pointwise Sampling Theorem for energy-limited signals.
Exercise 8.13 (The Error Due to Sub-Nyquist Sampling). Let x be an energy-limited
signal that is bandlimited to W Hz, and let Ts > 0 be possibly larger than 1/(2W). Deï¬ne
Ë†xÏƒ(f) =
âˆ

Î·=âˆ’âˆ
Ë†x
	
f + Î·
Ts

,
f âˆˆR.
(i) Show that for every integer â„“we can express T âˆ’1/2
s
x(âˆ’â„“Ts) as
1
âˆšTs
x(âˆ’â„“Ts) =

1
2Ts
âˆ’
1
2Ts
Ë†xÏƒ(f)
1
âˆšTs
eâˆ’i2Ï€fâ„“Ts df,
i.e., as the â„“-th Fourier Series Coeï¬ƒcient of Ë†xÏƒ w.r.t. the interval
$
âˆ’1/(2Ts), 1/(2Ts)

.
(ii) Conclude that
âˆ

â„“=âˆ’âˆ
x(âˆ’â„“Ts) sinc
	 t
Ts + â„“

=

1
2Ts
âˆ’
1
2Ts
Ë†xÏƒ(f) ei2Ï€ft df,
t âˆˆR.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.9 Exercises
165
(iii) Show that

1
2Ts
âˆ’
1
2Ts
Ë†xÏƒ(f) ei2Ï€ft df =
âˆ

Î·=âˆ’âˆ
eâˆ’i2Ï€tÎ·/Ts

Î·
Ts +
1
2Ts
Î·
Ts âˆ’
1
2Ts
Ë†x(f) ei2Ï€ft df,
t âˆˆR,
and
x(t) =
âˆ

Î·=âˆ’âˆ

Î·
Ts +
1
2Ts
Î·
Ts âˆ’
1
2Ts
Ë†x(f) ei2Ï€ft df,
t âˆˆR.
(iv) Conclude that
x(t) âˆ’
âˆ

â„“=âˆ’âˆ
x(âˆ’â„“Ts) sinc
	 t
Ts + â„“

 â‰¤2

|f|>
1
2Ts
Ë†x(f)
 df,
t âˆˆR.
(See (Pinsky, 2002, Theorem 4.2.13).)
Hint: For Part (ii) you may ï¬nd the Pointwise Sampling Theorem useful.
Exercise 8.14 (Inner Product between Passband Signals). Let xPB and yPB be real
energy-limited passband signals that are bandlimited to W Hz around the carrier fre-
quency fc.
Let xBB and yBB be their corresponding baseband representations.
Let
T = 1/W. Show that
âŸ¨xPB, yPBâŸ©= 2T Re

âˆ

â„“=âˆ’âˆ
xBB(â„“T) yâˆ—
BB(â„“T)

.
Exercise 8.15 (Orthogonality to Time Shifts). Let x and y be energy-limited signals that
are bandlimited to W Hz. Show that x is orthogonal to all the time shifts of y by integer
multiples of 1/(2W) if, and only if, f â†’Ë†x(f) Ë†y(f) is zero outside a set of frequencies of
Lebesgue measure zero. Conclude that if Ë†y(f) is nonzero for all f âˆˆ[âˆ’W, W] then x is
orthogonal to all the time shifts of y by integer multiples of 1/(2W) if, and only if, x is
the all-zero signal.
Exercise 8.16 (Closed Subspaces). Let U denote the set of energy-limited signals that
vanish outside some interval. Thus, u is in U if, and only if, there exist a, b âˆˆR (that may
depend on u) such that u(t) is zero whenever t /âˆˆ[a, b]. Show that U is a linear subspace
of L2, but that it is not closed.
Exercise 8.17 (Projection onto an Inï¬nite-Dimensional Subspace).
(i) Let U âŠ‚L2 be the set of all elements of L2 that are zero outside the interval
[âˆ’1, +1]. Given v âˆˆL2, let w be the signal w: t â†’v(t) I{|t| â‰¤1}. Show that w is
in U and that v âˆ’w is orthogonal to every signal in U.
(ii) Let U be the subspace of energy-limited signals that are bandlimited to W Hz.
Given v âˆˆL2, deï¬ne w = v â‹†LPFW. Show that w is in U and that v âˆ’w is
orthogonal to every signal in U.
Exercise 8.18 (A Maximization Problem). Of all unit-energy real signals that are band-
limited to W Hz, which one has the largest value at t = 0? What is its value at t = 0?
Repeat for t = 17.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

166
Complete Orthonormal Systems and the Sampling Theorem
Exercise 8.19 (Deriving a CONS). Let U be a closed linear subspace of L2, and let the
signals . . . , uâˆ’1, u0, u1, . . . be in U. Let . . . , Ï†âˆ’1, Ï†0, Ï†1, . . . be an orthonormal sequence
in U such that, for every k âˆˆN,
span (uâˆ’k, . . . , uk) = span (Ï†âˆ’k, . . . , Ï†k) .
Show that if no unit-energy element of U is orthogonal to all the signals {uâ„“}, then {Ï†â„“}
is a CONS for U.
Hint: Use Proposition 8.6.5.
Exercise 8.20 (A Complete Subspace). A linear subspace of L2 is said to be complete
if the following statement holds: if a sequence u1, u2, . . . âˆˆU is such that to every Ïµ > 0
there corresponds some L(Ïµ) for which âˆ¥un âˆ’umâˆ¥2 < Ïµ whenever n, m â‰¥L(Ïµ), then the
sequence converges to some u âˆˆU in the sense that âˆ¥un âˆ’uâˆ¥2 tends to 0 as n â†’âˆ.
(i) Use the fact that L2 is complete (Theorem 8.6.1) to prove that the space of energy-
limited signals that are zero outside the interval [âˆ’T, T ] is complete.
(ii) Show that the space of energy-limited signals that are bandlimited to W Hz is
complete. This combined with Note 6.4.2 shows that this space is a Hilbert Space.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,

8.9 Exercises
167
Ë‡V
V
energy-limited signals that
energy-limited functions that
are bandlimited to W Hz
vanish outside the interval [âˆ’W, W)
generic element of Ë‡V
generic element of V
x: t 	â†’x(t)
g: f 	â†’g(f)
a CONS
a CONS
. . . , Ë‡Ïˆâˆ’1, Ë‡Ïˆ0, Ë‡Ïˆ1, . . .
. . . , Ïˆâˆ’1, Ïˆ0, Ïˆ1, . . .
Ë‡Ïˆâ„“(t) =
âˆš
2W sinc

2Wt + â„“

Ïˆâ„“(f) =
1
âˆš
2W
eiÏ€â„“f/W I{âˆ’W â‰¤f < W}
inner product
inner product

x, Ë‡Ïˆâ„“

âŸ¨g, Ïˆâ„“âŸ©
 âˆ
âˆ’âˆ
x(t)
âˆš
2W sinc

2Wt + â„“

dt
 W
âˆ’W
g(f)
1
âˆš
2W
eâˆ’iÏ€â„“f/W df
=
1
âˆš
2W
x

âˆ’â„“
2W

= gâ€™s â„“-th Fourier Series Coeï¬ƒcient (â‰œcâ„“)
Sampling Theorem
Fourier Series
lim
Lâ†’âˆ
x âˆ’
L

â„“=âˆ’L

x, Ë‡Ïˆâ„“
 Ë‡Ïˆâ„“

2
= 0,
lim
Lâ†’âˆ
g âˆ’
L

â„“=âˆ’L
âŸ¨g, Ïˆâ„“âŸ©Ïˆâ„“

2
= 0,
i.e.,
i.e.,
 âˆ
âˆ’âˆ
x(t) âˆ’
L

â„“=âˆ’L
x

âˆ’â„“
2W

sinc

2Wt + â„“

2
dt â†’0
 W
âˆ’W
g(f) âˆ’
L

â„“=âˆ’L
câ„“
1
âˆš
2W
eiÏ€â„“f/W

2
df â†’0
Table 8.1: The duality between the Sampling Theorem and the Fourier Series Representation.
available at 
.010
14:28:40, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 9
Sampling Real Passband Signals
9.1
Introduction
In this chapter we present a procedure for representing a real energy-limited pass-
band signal that is bandlimited to W Hz around a carrier frequency fc using com-
plex numbers that we accumulate at a rate of W complex numbers per second.
Alternatively, since we can represent every complex number as a pair of real num-
bers (its real and imaginary parts), we can view our procedure as allowing us to
represent the signal using real numbers that we accumulate at a rate of 2W real
numbers per second. Thus we propose to accumulate
2W real samples per second,
or
W complex samples per second.
Note that the carrier frequency fc plays no role here (provided, of course, that
fc > W/2): the rate at which we accumulate real numbers to describe the passband
signal does not depend on fc.1
For real baseband signals this feat is easily accomplished using the Sampling The-
orem as follows. A real energy-limited baseband signal that is bandlimited to W
Hz can be reconstructed from its (real) samples that are taken 1/(2W) seconds
apart (Theorem 8.4.3), so the signal can be reconstructed from real numbers (its
samples) that are being accumulated at the rate of 2W real samples per second.
For passband signals we cannot achieve this feat by invoking the Sampling Theorem
directly. Even though, by Corollary 7.7.3, every energy-limited passband signal xPB
that is bandlimited to W Hz around the center frequency fc is also an energy-limited
bandlimited (baseband) signal, we are only guaranteed that xPB be bandlimited
to fc + W/2 Hz. Consequently, if we were to apply the Sampling Theorem directly
1But the carrier frequency fc does play a role in the reconstruction.
168
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,

9.2 Complex Sampling
169
to xPB we would have to sample xPB every 1/(2fc + W) seconds, i.e., we would
have to accumulate 2fc + W real numbers per second, which can be much higher
than 2W, especially in wireless communications where fc â‰«W.
Instead of applying the Sampling Theorem directly to xPB, the idea is to apply it to
xPBâ€™s baseband representation xBB. Suppose that xPB is a real energy-limited pass-
band signal that is bandlimited to W Hz around the carrier frequency fc. By Theo-
rem 7.7.12 (vii), it can be represented using its baseband representation xBB, which
is a complex baseband signal that is bandlimited to W/2 Hz (Theorem 7.7.12 (v)).
Consequently, by the L2-Sampling Theorem (Theorem 8.4.3), xBB can be described
by sampling it at a rate of W samples per second. Since the baseband signal is
complex, its samples are also, in general, complex. Thus, in sampling xBB every
1/W seconds we are accumulating one complex sample every 1/W seconds. Since
we can recover xPB from xBB and fc, it follows that, as we wanted, we have found
a way to describe xPB using complex numbers that are accumulated at a rate of W
complex numbers per second.
9.2
Complex Sampling
Recall from Section 7.7.3 (Theorem 7.7.12) that a real energy-limited passband
signal xPB that is bandlimited to W Hz around a carrier frequency fc can be
represented using its baseband representation xBB as
xPB(t) = 2 Re

ei2Ï€fct xBB(t)

,
t âˆˆR,
(9.1)
where xBB is given by
xBB =

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†LPFWc,
(9.2)
and where the cutoï¬€frequency Wc can be chosen arbitrarily in the range
W
2 â‰¤Wc â‰¤2fc âˆ’W
2 .
(9.3)
The signal xBB is an energy-limited complex baseband signal that is bandlimited
to W/2 Hz. Being bandlimited to W/2 Hz, it follows from the L2-Sampling The-
orem that xBB can be reconstructed from its samples taken 1/(2 (W/2)) = 1/W
seconds apart. We denote these samples by
xBB
 â„“
W

,
â„“âˆˆZ
(9.4)
so, by (9.2),
xBB
 â„“
W

=

t 	â†’eâˆ’i2Ï€fct xPB(t)

â‹†LPFWc
 â„“
W

,
â„“âˆˆZ.
(9.5)
These samples are, in general, complex. Their real part corresponds to the samples
of the in-phase component Re(xBB), which, by (7.42a), is given by
Re(xBB) =

t 	â†’xPB(t) cos(2Ï€fct)

â‹†LPFWc
(9.6)
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,
www.ebook3000.com

170
Sampling Real Passband Signals
cos(2Ï€fct)
90â—¦
Ã—
Ã—
xPB(t)
xPB(t) cos(2Ï€fct)
âˆ’xPB(t) sin(2Ï€fct)
LPFWc
LPFWc
W
2 â‰¤Wc â‰¤2fc âˆ’W
2
Re

xBB(t)

Im

xBB(t)

Re

xBB(â„“/W)

Im

xBB(â„“/W)

â„“/W
â„“/W
Figure 9.1: Sampling of a real passband signal xPB.
(for Wc satisfying (9.3)) and their imaginary part corresponds to the samples of
the quadrature-component Im(xBB), which, by (7.42b), is given by
Im(xBB) = âˆ’

t 	â†’xPB(t) sin(2Ï€fct)

â‹†LPFWc .
(9.7)
Thus,
xBB
 â„“
W

=

t 	â†’xPB(t) cos(2Ï€fct)

â‹†LPFWc
 â„“
W

âˆ’i

t 	â†’xPB(t) sin(2Ï€fct)

â‹†LPFWc
 â„“
W

,
â„“âˆˆZ.
(9.8)
The procedure of taking a real passband signal xPB and sampling its baseband
representation to obtain the samples (9.8) is called complex sampling.
It is
depicted in Figure 9.1.
The passband signal xPB is ï¬rst separately multiplied
by t 	â†’cos(2Ï€fct) and by t 	â†’âˆ’sin(2Ï€fct), which are generated using a local
oscillator and a 90â—¦-phase shifter. Each result is fed to a lowpass ï¬lter with cutoï¬€
frequency Wc to produce the in-phase and quadrature components respectively.
Each component is then sampled at a rate of W real samples per second.
9.3
Reconstructing xPB from its Complex Samples
By the Pointwise Sampling Theorem (Theorem 8.4.5) applied to the energy-limited
signal xBB (which is bandlimited to W/2 Hz) we obtain
xBB(t) =
âˆ

â„“=âˆ’âˆ
xBB
 â„“
W

sinc(Wt âˆ’â„“),
t âˆˆR.
(9.9)
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,

9.3 Reconstructing xPB from its Complex Samples
171
Consequently, by (9.1), xPB can be reconstructed from its complex samples as
xPB(t) = 2 Re
	
ei2Ï€fct
âˆ

â„“=âˆ’âˆ
xBB
 â„“
W

sinc(Wt âˆ’â„“)

,
t âˆˆR.
(9.10a)
Since the sinc (Â·) function is real, this can also be written as
xPB(t) = 2
âˆ

â„“=âˆ’âˆ
Re
	
ei2Ï€fct xBB
 â„“
W

sinc(Wt âˆ’â„“),
t âˆˆR,
(9.10b)
or, using real operations, as
xPB(t) = 2
âˆ

â„“=âˆ’âˆ
Re
	
xBB
 â„“
W

sinc(Wt âˆ’â„“) cos(2Ï€fct)
âˆ’2
âˆ

â„“=âˆ’âˆ
Im
	
xBB
 â„“
W

sinc(Wt âˆ’â„“) sin(2Ï€fct),
t âˆˆR.
(9.10c)
As we next show, we can obtain another form of convergence using the L2-Sampling
Theorem (Theorem 8.4.3). We ï¬rst note that by that theorem
lim
Lâ†’âˆ
t 	â†’xBB(t) âˆ’
L

â„“=âˆ’L
xBB
 â„“
W

sinc(Wt âˆ’â„“)

2
2
= 0.
(9.11)
We next note that xBB is the baseband representation of xPB and thatâ€”as can be
veriï¬ed directly or by using Proposition 7.7.9â€”the mapping
t 	â†’xBB(â„“/W) sinc(Wt âˆ’â„“)
is the baseband representation of the real passband signal
t 	â†’2 Re
	
ei2Ï€fct xBB
 â„“
W

sinc(Wt âˆ’â„“)

.
Consequently, by linearity (Theorem 7.7.12 (ii)), the mapping
t 	â†’xBB(t) âˆ’
L

â„“=âˆ’L
xBB
 â„“
W

sinc(Wt âˆ’â„“)
is the baseband representation of the real passband signal
t 	â†’xPB(t) âˆ’2 Re
	
ei2Ï€fct
L

â„“=âˆ’L
xBB
 â„“
W

sinc(Wt âˆ’â„“)

and hence, by Theorem 7.7.12 (iii),
t 	â†’xPB(t) âˆ’2 Re
	
ei2Ï€fct
L

â„“=âˆ’L
xBB
 â„“
W

sinc(Wt âˆ’â„“)


2
2
= 2
t 	â†’xBB(t) âˆ’
L

â„“=âˆ’L
xBB
 â„“
W

sinc(Wt âˆ’â„“)

2
2
.
(9.12)
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,
www.ebook3000.com

172
Sampling Real Passband Signals
Combining (9.11) with (9.12) yields the L2 convergence
lim
Lâ†’âˆ
t 	â†’xPB(t) âˆ’2 Re
	
ei2Ï€fct
L

â„“=âˆ’L
xBB
 â„“
W

sinc(Wt âˆ’â„“)


2
= 0.
(9.13)
We summarize how a passband signal can be reconstructed from the samples of its
baseband representation in the following theorem.
Theorem 9.3.1 (The Sampling Theorem for Passband Signals). Let xPB be a
real energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc. For every integer â„“, let xBB(â„“/W) denote the time-(â„“/W) sample of
the baseband representation xBB of xPB; see (9.5) and (9.8).
(i) xPB can be pointwise reconstructed from the samples using the relation
xPB(t) = 2 Re
	
ei2Ï€fct
âˆ

â„“=âˆ’âˆ
xBB
 â„“
W

sinc(Wt âˆ’â„“)

,
t âˆˆR.
(ii) xPB can also be reconstructed from the samples in the L2 sense
lim
Lâ†’âˆ
 âˆ
âˆ’âˆ
-
xPB(t) âˆ’2 Re
	
ei2Ï€fct
L

â„“=âˆ’L
xBB
 â„“
W

sinc(Wt âˆ’â„“)

.2
dt = 0.
(iii) The energy in xPB can be reconstructed from the sum of the squared magni-
tudes of the samples via
âˆ¥xPBâˆ¥2
2 = 2
W
âˆ

â„“=âˆ’âˆ
xBB
 â„“
W

2
.
(iv) If yPB is another real energy-limited passband signal that is bandlimited to
W Hz around fc, and if {yBB(â„“/W)} are the samples of its baseband repre-
sentation, then
âŸ¨xPB, yPBâŸ©= 2
W Re
	
âˆ

â„“=âˆ’âˆ
xBB
 â„“
W

yâˆ—
BB
 â„“
W

.
Proof. Part (i) is just a restatement of (9.10b). Part (ii) is a restatement of (9.13).
Part (iii) is a special case of Part (iv) corresponding to yPB being equal to xPB. It
thus only remains to prove Part (iv). This is done by noting that if xBB and yBB
are the baseband representations of xPB and yPB, then, by Theorem 7.7.12 (iv),
âŸ¨xPB, yPBâŸ©= 2 Re

âŸ¨xBB, yBBâŸ©

= 2
W Re
	
âˆ

â„“=âˆ’âˆ
xBB
 â„“
W

yâˆ—
BB
 â„“
W

,
where the second equality follows from Theorem 8.4.3 (iii).
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,

9.4 Exercises
173
Using the isomorphism between the family of complex square-summable sequences
and the family of energy-limited signals that are bandlimited to W Hz (Theo-
rem 8.7.1), and using the relationship between real energy-limited passband signals
and their baseband representation (Theorem 7.7.12), we can readily establish the
following isomorphism between the family of complex square-summable sequences
and the family of real energy-limited passband signals.
Theorem 9.3.2 (Real Passband Signals and Square-Summable Sequences). Let
fc, W, and T be constants satisfying
fc > W/2 > 0,
T = 1/W.
(i) If xPB is a real energy-limited passband signal that is bandlimited to W Hz
around fc, and if xBB is its baseband representation, then the bi-inï¬nite se-
quence consisting of the samples of xBB at integer multiples of T
. . . , xBB(âˆ’T), xBB(0), xBB(T), xBB(2T), . . .
is a square-summable sequence of complex numbers and
2T
âˆ

â„“=âˆ’âˆ
xBB(â„“T)
2 = âˆ¥xPBâˆ¥2
2 .
(ii) More generally, if xPB and yPB are real energy-limited passband signals that
are bandlimited to W Hz around the carrier frequency fc, and if xBB and
yBB are their baseband representations, then
2T Re
	
âˆ

â„“=âˆ’âˆ
xBB(â„“T) yâˆ—
BB(â„“T)

= âŸ¨xPB, yPBâŸ©.
(iii) If . . . , Î±âˆ’1, Î±0, Î±1, . . . is a square-summable bi-inï¬nite sequence of complex
numbers, then there exists a real energy-limited passband signal xPB that is
bandlimited to W Hz around the carrier frequency fc such that the samples
of its baseband representation xBB are given by
xBB(â„“T) = Î±â„“,
â„“âˆˆZ.
(iv) The mapping of every real energy-limited passband signal that is bandlimited
to W Hz around fc to the square-summable sequence consisting of the samples
of its baseband representation is linear (over R).
9.4
Exercises
Exercise 9.1 (A Speciï¬c Signal). Let x be a real energy-limited passband signal that
is bandlimited to W Hz around the carrier frequency fc. Suppose that all its complex
samples are zero except for its zeroth complex sample, which is given by 1+i. What is x?
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,
www.ebook3000.com

174
Sampling Real Passband Signals
Exercise 9.2 (Real Passband Signals whose Complex Samples Are Real). Characterize
the Fourier Transforms of real energy-limited passband signals that are bandlimited to W
Hz around the carrier frequency fc and whose complex samples are real.
Exercise 9.3 (Multiplying by a Carrier). Let x be a real energy-limited signal that is
bandlimited to W/2 Hz, and let fc be larger than W/2. Express the complex samples of
t â†’x(t) cos(2Ï€fct) in terms of x. Repeat for t â†’x(t) sin(2Ï€fct).
Exercise 9.4 (Reï¬‚ecting and Complex Sampling). Let xPB be an integrable signal that
is bandlimited to W Hz around the carrier frequency fc. Relate the complex samples of
~xPB to those of xPB.
Exercise 9.5 (Naively Sampling a Passband Signal).
(i) Consider the signal x: t â†’m(t) sin(2Ï€fct), where m(Â·) is an integrable signal that
is bandlimited to 100 Hz and where fc = 100 MHz. Can x be recovered from its
samples . . . , x(âˆ’T), x(0), x(T), . . . when 1/T = 100 MHz?
(ii) Consider now the general case where x is an integrable real passband signal that is
bandlimited to W Hz around the carrier frequency fc. Find conditions guaranteeing
that x be reconstructible from its samples . . . , x(âˆ’T), x(0), x(T), . . .
Exercise 9.6 (Orthogonal Passband Signals). Let xPB and yPB be real energy-limited
passband signals that are bandlimited to W Hz around the carrier frequency fc. Under
what conditions on their complex samples are they orthogonal?
Exercise 9.7 (Sampling a Baseband Signal as though It Were a Passband Signal). Recall
that, ignoring some technicalities, a real baseband signal x of bandwidth W Hz can be
viewed as a real passband signal of bandwidth W around the carrier frequency fc, where
fc = W/2 (Problem 7.8). Compare the reconstruction formula for x from its samples to
the reconstruction formula for x from its complex samples.
Exercise 9.8 (Multiplying the Complex Samples). Let x be a real energy-limited passband
signal that is bandlimited to W Hz around the carrier frequency fc. Let . . . , xâˆ’1, x0, x1, . . .
denote its complex samples taken 1/W seconds apart. Let y be a real energy-limited
passband signal that is bandlimited to W Hz around the carrier frequency fc and whose
complex samples are like those of x but multiplied by i. Relate the FT of y to the FT
of x.
Exercise 9.9 (Delayed Complex Sampling). Let x and y be real energy-limited passband
signals that are bandlimited to W Hz around the carrier frequency fc. Suppose that the
complex samples of y are the same as those of x, but delayed by one:
yBB
	 â„“
W

= xBB
	â„“âˆ’1
W

,
â„“âˆˆZ.
How are Ë†x and Ë†y related? Is y a delayed version of x?
Exercise 9.10 (On the Family of Real Passband Signals). Is the set of all real energy-
limited passband signals that are bandlimited to W Hz around the carrier frequency fc a
linear subspace of the set of all complex energy-limited signals?
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,

9.4 Exercises
175
Exercise 9.11 (Complex Sampling and Inner Products). Show that the â„“-th complex
sample xBB(â„“/W) of any real energy-limited passband signal that is bandlimited to W Hz
around the carrier frequency fc can be expressed as an inner product
xBB
	 â„“
W

= âŸ¨x, Ï†â„“âŸ©,
â„“âˆˆZ,
where . . . , Ï†âˆ’1, Ï†0, Ï†1, . . . are orthogonal equi-energy complex signals. Is Ï†â„“in general a
delayed version of Ï†0?
Exercise 9.12 (Absolute Summability of the Complex Samples). Show that the complex
samples of a real integrable passband signal that is bandlimited to W Hz around the
carrier frequency fc must be absolutely summable.
Hint: See Exercise 8.9.
Exercise 9.13 (The Convolution Revisited). Let x and y be real integrable passband
signals that are bandlimited to W Hz around the carrier frequency fc.
Express the
complex samples of x â‹†y in terms of those of x and y.
Exercise 9.14 (Complex Sampling and Filtering). Let x be a real integrable passband
signal that is bandlimited to W Hz around the carrier frequency fc, and let h be the
impulse response of a real stable ï¬lter. Relate the complex samples of x â‹†h to those of x
and h â‹†BPFW,fc.
Exercise 9.15 (Two Bands). Let [a1, b1] and [a2, b2] be disjoint positive frequency inter-
vals. Propose a variation on complex sampling that operates at 2(b1 âˆ’a1 + b2 âˆ’a2) real
samples per second and that allows for the perfect reconstruction of every real, continuous,
energy-limited signal whose FT is zero at f whenever |f| /âˆˆ[a1, b1] âˆª[a2, b2].
available at 
.011
14:28:39, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 10
Mapping Bits to Waveforms
10.1
What Is Modulation?
Data bits are mathematical entities that have no physical attributes. To send them
over a channel, one needs to ï¬rst map them into some physical signal, which is
then â€œfedâ€ into a channel to produce a physical signal at the channelâ€™s output. For
example, when we send data over a telephone line, the data bits are ï¬rst converted
to an electrical signal, which then inï¬‚uences the voltage measured at the other
end of the line. (We use the term â€œinï¬‚uencesâ€ because the signal measured at the
other end of the line is usually not identical to the channel input: it is typically
attenuated and also corrupted by thermal noise and other distortions introduced
by various conversions in the telephone exchange system.) Similarly, in a wireless
system, the data bits are mapped to an electromagnetic wave that then inï¬‚uences
the electromagnetic ï¬eld measured at the receiver antenna. In magnetic recording,
data bits are written onto a magnetic medium by a mapping that maps them to
a magnetization pattern, which is then measured (with some distortion and some
noise) by the magnetic head at some later time when the data are read.
In the ï¬rst example the bits are mapped to continuous-time waveforms correspond-
ing to the voltage across an impedance, whereas in the last example the bits are
mapped to a spatial waveform corresponding to diï¬€erent magnetizations at dif-
ferent locations across the magnetic medium. While some of the theory we shall
develop holds for both cases, we shall focus here mainly on channels of the former
type, where the channel input signal is some function of time rather than space.
We shall further focus on cases where the channel input corresponds to a time-
varying voltage across a resistor, a time-varying current through a resistor, or a
time-varying electric ï¬eld, so the energy required to transmit the signal is propor-
tional to the time integral of its square. Thus, if x(t) denotes the channel input at
time t, then we shall refer to
 t+Î”
t
x2(Ï„) dÏ„ as the transmitted energy during the
time interval beginning at time t and ending at time t + Î”.
There are many mappings of bits to waveforms, and our goal is to ï¬nd â€œgoodâ€ ones.
We will, of course, have to deï¬ne some ï¬gures of merit to compare the quality of
diï¬€erent mappings. We shall refer to the mapping of bits to a physical waveform
as modulation and to the part of the system that performs the modulation as the
modulator.
176
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.2 Modulating One Bit
177
Without going into too much detail, we can list a few qualitative requirements of a
modulator. The modulation should be robust with respect to channel impairments,
so that the receiver at the other end of the channel can reliably decode the data bits
from the channel output. Also, the modulator should have reasonable complexity.
Finally, in many applications we require that the transmitted signal be of limited
power so as to preserve the battery. In wireless applications the transmitted signal
may also be subject to spectral restrictions so as to not interfere with other systems.
10.2
Modulating One Bit
One does not typically expect to design a communication system in order to convey
only one data bit. The purpose of the modulator is typically to map an entire bit
stream to a waveform that extends over the entire life of the communication system.
Nevertheless, for pedagogic reasons, it is good to ï¬rst consider the simplest scenario
of modulating a single bit. In this case the modulator is fully characterized by two
functions x0(Â·) and x1(Â·) with the understanding that if the data bit D is equal
to zero, then the modulator produces the waveform x0(Â·) and that otherwise it
produces x1(Â·). Thus, the signal produced by the modulator is given by
X(t) =

x0(t)
if D = 0,
x1(t)
if D = 1,
t âˆˆR.
(10.1)
For example, we could choose
x0(t) =

A eâˆ’t/T
if t/T â‰¥0,
0
otherwise,
t âˆˆR,
and
x1(t) =

A
if 0 â‰¤t/T â‰¤1,
0
otherwise,
t âˆˆR,
where T = 1 sec and where A is a constant such that A2 has units of power.
This may seem like an odd way of writing these waveforms, but we have our
reasons: we typically think of t as having units of time, and we try to avoid
applying transcendental functions (such as the exponential function) to quantities
with units. Also, we think of the squared transmitted waveform as having units
of power, whereas we think of the transcendental functions as returning unit-less
arguments. Hence the introduction of the constant A with the understanding that
A2 has units of power.
We denoted the bit to be sent by an uppercase letter (D) because we like to de-
note random quantities (such as random variables, random vectors, and stochastic
processes) by uppercase letters, and we think of the transmitted bit as a random
quantity.
Indeed, if the transmitted bit were deterministic, there would be no
need to transmit it! This may seem like a statement made in jest, but it is ac-
tually very important. In the ï¬rst half of the twentieth century, engineers often
analyzed the performance of (analog) communication systems by analyzing their
performance in transmitting some particular signal, e.g., a sine wave. Nobody, of
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

178
Mapping Bits to Waveforms
course, transmitted such â€œboringâ€ signals, because those could always be produced
at the receiver using a local oscillator. In the second half of the twentieth century,
especially following the work of Claude Shannon, engineers realized that it is only
meaningful to view the data to be transmitted as random, i.e., as quantities that
are unknown at the receiver and also unknown to the system designer prior to the
systemâ€™s deployment. We thus view the bit to be sent D as a random variable.
Often we will assume that it takes on the values 0 and 1 equiprobably. This is a
good assumption if prior to transmission a data compression algorithm is used.
By the same token, we view the transmitted signal as a random quantity, and
hence the uppercase X. In fact, if we employ the above signaling scheme, then at
every time instant tâ€² âˆˆR the value X(tâ€²) of the transmitted waveform is a random
variable. For example, at time T/2 the value of the transmitted waveform is X(T/2),
which is a random variable that takes on the values A eâˆ’1/2 and A equiprobably.
Similarly, at time 2T the value of the transmitted waveform is X(2T), which is a
random variable taking on the values eâˆ’2 and 0 equiprobably. Mathematicians call
such a waveform a random process or a stochastic process (SP). This will be
deï¬ned formally in Section 12.2.
It is useful to think about a random process as a function of two arguments: time
and â€œluckâ€ or, more precisely, as a function of time and the result of all the random
experiments in the system. For a ï¬xed instant of time t âˆˆR, we have that X(t)
is a random variable, i.e., a real-valued function of the randomness in the system
(in this case the realization of D).
Alternatively, for a ï¬xed realization of the
randomness in the system, the random process is a deterministic function of time.
These two views will be used interchangeably in this book.
10.3
From Bits to Real Numbers
Many of the popular modulation schemes can be viewed as operating in two stages.
In the ï¬rst stage the data bits are mapped to real numbers, and in the second stage
the real numbers are mapped to a continuous-time waveform. If we denote by k the
number of data bits that will be transmitted by the system during its lifetime (or
from the moment it is turned on until it is turned oï¬€), and if we denote the data
bits by D1, D2, . . . , Dk, then the ï¬rst stage can be described as the application of
a mapping Ï•(Â·) that maps length-k sequences of bits to length-n sequences of real
numbers:
Ï•: {0, 1}k â†’Rn
(d1, . . . , dk) 	â†’(x1, . . . , xn).
From an engineering point of view, it makes little sense to allow for the encoding
function to map two diï¬€erent binary k-tuples to the same real n-tuple, because
this would result in the transmitted waveforms corresponding to the two k-tuples
being identical.
This may cause errors even in the absence of noise.
We shall
therefore assume throughout that the mapping Ï•(Â·) is one-to-one (injective) so
no two distinct data k-tuples are mapped to the same n-tuple of real numbers.
An example of a mapping that maps bits to real numbers is the mapping that maps
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.4 Block-Mode Mapping of Bits to Real Numbers
179
each data bit Dj to the real number Xj according to the rule
Xj =

+1
if Dj = 0,
âˆ’1
if Dj = 1,
j = 1, . . . , k.
(10.2)
In this example one real symbol Xj is produced for every data bit, so n = k. For
this reason we say that this mapping has the rate of one bit per real symbol.
As another example consider the case where k is even and the data bits {Dj} are
broken into pairs
(D1, D2), (D3, D4), . . . , (Dkâˆ’1, Dk)
and each pair of data bits is then mapped to a single real number according to the
rule
(D2jâˆ’1, D2j) 	â†’
â§
âª
âª
âª
â¨
âª
âª
âª
â©
+3
if D2jâˆ’1 = D2j = 0,
+1
if D2jâˆ’1 = 0 and D2j = 1,
âˆ’3
if D2jâˆ’1 = D2j = 1,
âˆ’1
if D2jâˆ’1 = 1 and D2j = 0,
j = 1, . . . , k/2.
(10.3)
In this case n = k/2, and we say that the mapping has the rate of two bits per real
symbol.
Note that the rate of the mapping could also be a fraction. Indeed, if each data
bit Dj produces two real numbers according to the repetition law
Dj 	â†’

(+1, +1)
if Dj = 0,
(âˆ’1, âˆ’1)
if Dj = 1,
j = 1, . . . , k,
(10.4)
then n = 2k, and we say that the mapping is of rate half a bit per real symbol.
Since there is a natural correspondence between R2 and C, i.e., between pairs of real
numbers and complex numbers (where a pair of real numbers (x, y) corresponds
to the complex number x + iy), the rate of the above mapping (10.4) can also be
stated as one bit per complex symbol. This may seem like an odd way of stating the
rate, but it has some advantages that will become apparent later when we discuss
the mapping of real (or complex) numbers to waveforms and the Nyquist Criterion.
10.4
Block-Mode Mapping of Bits to Real Numbers
The examples we gave in Section 10.3 of mappings Ï•: {0, 1}k â†’Rn have something
in common. In each of those examples the mapping can be described as follows: the
data bits D1, . . . , Dk are ï¬rst grouped into binary K-tuples; each K-tuple is then
mapped to a real N-tuple by applying some mapping enc: {0, 1}K â†’RN; and the
so-produced real N-tuples are then concatenated to form the sequence X1, . . . , Xn,
where n = (k/K)N.
In the ï¬rst example K = N = 1 and the mapping of K-tuples to N-tuples is the
mapping (10.2). In the second example K = 2 and N = 1 with the mapping (10.3).
And in the third example K = 1 and N = 2 with the repetition mapping (10.4).
To describe such mappings Ï•: {0, 1}k â†’Rn more formally we need the notion of
a binary-to-reals block encoder, which we deï¬ne next.
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

180
Mapping Bits to Waveforms
D1, D2,
. . .
, DK,
enc(Â·)
X1, X2,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
. . .
, D2K,
enc(Â·)
XN+1,
. . .
, X2N,
enc(DK+1, . . . , D2K)
, Dkâˆ’K+1, . . . , Dk
enc(Â·)
, Xnâˆ’N+1,
. . .
, Xn
enc(Dkâˆ’K+1, . . . , Dk)
Figure 10.1: Block-mode encoding.
Deï¬nition 10.4.1 ((K, N) Binary-to-Reals Block Encoder). A (K, N) binary-to-
reals block encoder is a one-to-one mapping from the set of binary K-tuples to
the set of real N-tuples, where K and N are positive integers. The rate of a (K, N)
binary-to-reals block encoder is deï¬ned as
K
N

bit
real symbol

.
Note that we shall sometimes omit the phrase â€œbinary-to-realsâ€ and refer to such
an encoder as a (K, N) block encoder. Also note that â€œone-to-oneâ€ means that
no two distinct binary K-tuples may be mapped to the same real N-tuple.
We say that an encoder Ï•: {0, 1}k â†’Rn operates in block-mode using the
(K, N) binary-to-reals block encoder enc(Â·) if
1) k is divisible by K;
2) n is given by (k/K) N; and
3) Ï•(Â·) maps the binary sequence D1, . . . , Dk to the sequence X1, . . . , Xn by
parsing the sequence D1, . . . , Dk into consecutive length-K binary tuples and
by then concatenating the results of applying enc(Â·) to each such K-tuple as
in Figure 10.1.
If k is not divisible by K, we often introduce zero padding.
In this case we
choose kâ€² to be the smallest integer that is no smaller than k and that is divisible
by K, i.e.,
kâ€² =
/ k
K
0
K,
(where for every Î¾ âˆˆR we use âŒˆÎ¾âŒ‰to denote the smallest integer that is no smaller
than Î¾, e.g., âŒˆ1.24âŒ‰= 2) and map D1, . . . , Dk to the sequence X1, . . . , Xnâ€² where
nâ€² = kâ€²
K N
by applying the (K, N) encoder in block-mode to the kâ€²-length zero-padded binary
tuple
D1, . . . , Dk, 0, . . . , 0
  
kâ€² âˆ’k zeros
(10.5)
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.5 From Real Numbers to Waveforms with Linear Modulation
181
D1, D2,
. . .
, DK,
enc(Â·)
X1, X2,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
. . .
, D2K,
enc(Â·)
XN+1,
. . .
, X2N,
enc(DK+1, . . . , D2K)
, Dkâ€²âˆ’K+1, . . . , Dk, 0, . . . , 0
enc(Â·)
, Xnâ€²âˆ’N+1,
. . .
, Xnâ€²
enc(Dkâˆ’K+1, . . . , Dk, 0, . . . , 0)
Figure 10.2: Block-mode encoding with zero padding.
as in Figure 10.2.
10.5
From Real Numbers to Waveforms with Linear Modulation
There are numerous ways to map a sequence of real numbers X1, . . . , Xn to a real-
valued signal. Here we shall focus on mappings that have a linear structure. This
additional structure simpliï¬es the implementation of the modulator and demodu-
lator. It is described next.
Suppose we wish to modulate the k data bits D1, . . . , Dk, and suppose that we have
mapped these bits to the n real numbers X1, . . . , Xn. Here n can be smaller, equal,
or greater than k. In a linear modulation scheme the transmitted waveform X(Â·),
which we also denote X, is given by
X(t) = A
n

â„“=1
Xâ„“gâ„“(t),
t âˆˆR,
(10.6)
where the deterministic real waveforms g1, . . . , gn are speciï¬ed in advance, and
where A â‰¥0 is a scaling factor.1
The waveform X(Â·) can be thus viewed as
a scaled-by-A linear combination of the tuple

g1, . . . , gn

with the coeï¬ƒcients
X1, . . . , Xn:
X = A
n

â„“=1
Xâ„“gâ„“.
(10.7)
The transmitted energy is a random variable that is given by
âˆ¥Xâˆ¥2
2 =
 âˆ
âˆ’âˆ
X2(t) dt
=
 âˆ
âˆ’âˆ
	
A
n

â„“=1
Xâ„“gâ„“(t)

2
dt
= A2
n

â„“=1
n

â„“â€²=1
Xâ„“Xâ„“â€²
 âˆ
âˆ’âˆ
gâ„“(t) gâ„“â€²(t) dt
1The notation X for the transmitted waveform is discussed in Chapter 12.
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

182
Mapping Bits to Waveforms
= A2
n

â„“=1
n

â„“â€²=1
Xâ„“Xâ„“â€² âŸ¨gâ„“, gâ„“â€²âŸ©.
The transmitted energy takes on a particularly simple form if the waveforms gâ„“(Â·)
are orthonormal, i.e., if
âŸ¨gâ„“, gâ„“â€²âŸ©= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , n},
(10.8)
in which case the energy is given by
âˆ¥Xâˆ¥2
2 = A2
n

â„“=1
X2
â„“,
{gâ„“} orthonormal.
(10.9)
As an exercise, the reader is encouraged to verify that there is no loss in generality
in assuming that the waveforms {gâ„“} are orthonormal. More precisely:
Theorem 10.5.1. Suppose that the waveform X(Â·) is generated from the binary
k-tuple D1, . . . , Dk by applying the mapping Ï•: {0, 1}k â†’Rn and by then linearly
modulating the resulting n-tuple Ï•(D1, . . . , Dk) using the waveforms {gâ„“}n
â„“=1 as in
(10.6).
Then there exist an integer 1 â‰¤nâ€² â‰¤n; a mapping Ï•â€² : {0, 1}k â†’Rnâ€²; and nâ€²
orthonormal signals {Ï†â„“}nâ€²
â„“=1 such that if Xâ€²(Â·) is generated from D1, . . . , Dk by
applying linear modulation to Ï•â€²(D1, . . . , Dk) using the orthonormal waveforms
{Ï†â„“}nâ€²
â„“=1, then Xâ€²(Â·) and X(Â·) are indistinguishable for every k-tuple D1, . . . , Dk.
Proof. The proof is based on the Gram-Schmidt procedure (Section 4.6.6) and is
left as an exercise (Exercise 10.7).
Motivated by this theorem, we shall focus on linear modulation with orthonormal
functions. But please note that even if the transmitted waveform satisï¬es (10.8),
the received waveform might not. For example, the channel might consist of a
linear ï¬lter that could destroy the orthogonality.
10.6
Recovering the Signal Coeï¬ƒcients with a Matched Filter
Suppose now that the binary k-tuple (D1, . . . , Dk) is mapped to the real n-tuple
(X1, . . . , Xn) using the mapping
Ï•: {0, 1}k â†’Rn
(10.10)
and that the n-tuple (X1, . . . , Xn) is then mapped to the waveform
X(t) = A
n

â„“=1
Xâ„“Ï†â„“(t),
t âˆˆR,
(10.11)
where Ï†1, . . . , Ï†n are orthonormal:
âŸ¨Ï†â„“, Ï†â„“â€²âŸ©= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , n}.
(10.12)
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.6 Recovering the Signal Coeï¬ƒcients with a Matched Filter
183
X(Â·)
AXâ„“
~Ï†
â„“Ts
Figure 10.3:
Recovering the symbols from the transmitted waveform using a
matched ï¬lter when (10.14) is satisï¬ed.
How can we recover the k-tuple D1, . . . , Dk from X(Â·)? The decoderâ€™s problem
is, of course, harder, because the decoder usually does not have access to the
transmitted waveform X(Â·) but only to the received waveform, which may be a
noisy and distorted version of X(Â·). Nevertheless, it is instructive to consider the
noiseless and distortionless problem ï¬rst.
If we are able to recover the real numbers {Xâ„“}n
â„“=1 from the received signal X(Â·),
and if the mapping Ï•: {0, 1}k â†’Rn is one-to-one (as we assume), then the data
bits {Dj}k
j=1 can be reconstructed from X(Â·). Thus, the question is how to recover
{Xâ„“}n
â„“=1 from X(Â·). But this is easy if the functions {Ï†â„“}n
â„“=1 are orthonormal,
because in this case, by Proposition 4.6.4 (i), Xâ„“is given by the scaled inner
product between X and Ï†â„“:
Xâ„“= 1
A âŸ¨X, Ï†â„“âŸ©,
â„“= 1, . . . , n.
(10.13)
Consequently, we can compute Xâ„“by feeding X to a matched ï¬lter for Ï†â„“and
scaling the time-0 output by 1/A (Section 5.8). To recover {Xâ„“}n
â„“=1 we thus need n
matched ï¬lters, one matched to each of the waveforms {Ï†â„“}.
The implementation becomes much simpler if the functions {Ï†â„“} have an additional
structure, namely, if they are all time shifts of some function Ï†(Â·):
Ï†â„“(t) = Ï†(t âˆ’â„“Ts),

â„“âˆˆ{1, . . . , n}, t âˆˆR

.
(10.14)
In this case it follows from Corollary 5.8.3 that we can compute all the inner
products {âŸ¨X, Ï†â„“âŸ©} using one matched ï¬lter of impulse response ~Ï† by feeding X
to the ï¬lter and sampling its output at the appropriate times:
Xâ„“= 1
A
 âˆ
âˆ’âˆ
X(Ï„) Ï†â„“(Ï„) dÏ„
= 1
A
 âˆ
âˆ’âˆ
X(Ï„) Ï†(Ï„ âˆ’â„“Ts) dÏ„
= 1
A
 âˆ
âˆ’âˆ
X(Ï„) ~Ï†(â„“Ts âˆ’Ï„) dÏ„
= 1
A

X â‹†~Ï†

(â„“Ts),
â„“= 1, . . . , n.
(10.15)
Figure 10.3 demonstrates how the symbols {Xâ„“} can be recovered from X(Â·) using
a single matched ï¬lter if the pulses {Ï†â„“} satisfy (10.14).
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

184
Mapping Bits to Waveforms
t
t
t
Î±
Î±
Î±
Ts
2
Ts
2
âˆ’Ts
2
âˆ’Ts
2
âˆ’Ts
Ts
Ï†(t)
Ï†(t + Ts)
Ï†(t âˆ’Ts)
Figure 10.4:
A function Ï†(Â·) whose time shifts by integer multiples of Ts are
orthonormal (with the proper choice of Î±).
Figure 10.4 depicts a signal Ï†(Â·) whose time shifts by integer multiples of Ts are
orthonormal (when Î± is chosen so that it be of unit energy).
In fact, for this
signal the time shifts do not overlap in the sense that for â„“Ì¸= 0 the product
Ï†(t) Ï†(t âˆ’â„“Ts) is zero at every epoch t âˆˆR. But, as we shall see in Chapter 11,
we can achieve orthonormality even if the time shifts do overlap. This is plausible
because t 	â†’Ï†(t) Ï†(t âˆ’â„“Ts) can integrate to zero even if it is not identically zero.
10.7
Pulse Amplitude Modulation
Under Assumption (10.14), the transmitted signal X(Â·) in (10.11) is given by
X(t) = A
n

â„“=1
Xâ„“Ï†(t âˆ’â„“Ts),
t âˆˆR,
(10.16)
which is a special case of Pulse Amplitude Modulation (PAM), which we
describe next.
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.8 Constellations
185
In PAM, the data bits D1, . . . , Dk are mapped to real numbers X1, . . . , Xn, which
are then mapped to the waveform
X(t) = A
n

â„“=1
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR,
(10.17)
for some scaling factor A â‰¥0, some function g: R â†’R, and some constant Ts > 0.
The function g (always assumed Borel measurable2) is called the pulse shape; the
constant Ts is called the baud period; and its reciprocal 1/Ts is called the baud
rate.3 The units of Ts are seconds, and one often refers to the units of 1/Ts as real
symbols per second. PAM can thus be viewed as a special case of linear modulation
(10.6) with gâ„“being given for every â„“âˆˆ{1, . . . , n} by the mapping t 	â†’g(t âˆ’â„“Ts).
The signal (10.16) can be viewed as a PAM signal where the pulse shape Ï† satisï¬es
the orthonormality condition

t 	â†’Ï†(t âˆ’â„“Ts), t 	â†’Ï†(t âˆ’â„“â€²Ts)

= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , n},
(10.18)
which is implied by (10.12) and (10.14).
In this book we shall typically denote the PAM pulse shape by g. But we shall
use Ï† if we assume an additional orthonormality condition such as (10.18). In this
case we shall refer to 1/Ts as having units of real dimensions per second:
1
Ts
real dimension
sec

,
Ï† satisï¬es (10.18).
(10.19)
Note that according to Theorem 10.5.1 there is no loss in generality in assuming
that the pulses {Ï†â„“} are orthonormal. There is, however, a loss in generality in
assuming that they satisfy (10.14).
10.8
Constellations
Recall that in PAM the data bits D1, . . . , Dk are ï¬rst mapped to the real n-tuple
X1, . . . , Xn using a one-to-one mapping Ï•: {0, 1}k â†’Rn, and that these real
numbers are then mapped to the waveform X(Â·) via (10.17). Since there are only
2k diï¬€erent binary k-tuples, it follows that each symbol Xâ„“can take on at most
2k diï¬€erent values. The set of values that Xâ„“can take on may, in general, depend
on â„“. The union of all these sets (over â„“âˆˆ{1, . . . , n}) is called the constellation of
the mapping Ï•(Â·). Denoting the constellation of Ï•(Â·) by X, we thus have that a real
number x is in X if, and only if, for some choice of the binary k-tuple (d1, . . . , dk)
and for some â„“âˆˆ{1, . . . , n} the â„“-th component of Ï•

(d1, . . . , dk)

is equal to x.
For example, the constellation corresponding to the mapping (10.2) is the set
{âˆ’1, +1}; the constellation corresponding to (10.3) is the set {âˆ’3, âˆ’1, +1, +3};
2The Borel Ïƒ-algebra is the smallest Ïƒ-algebra containing the open sets. A function g : R â†’
R is Borel measurable if for every Î± âˆˆR the set {Î¾ âˆˆR : g(Î¾) < Î±} is an element of the Borel
Ïƒ-algebra.
3These terms honor the French engineer Jean-Maurice-Â´Emile Baudot (1845â€“1903) who in-
vented a telegraph printing system. Some authors refer to the baud rate as â€œbaud.â€ The term
baud period is also not completely standard.
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

186
Mapping Bits to Waveforms
and the constellation corresponding to (10.4) is the set {âˆ’1, +1}.
In all these
examples, the constellation can be viewed as a special case of the constellation
with 2Î½ symbols

âˆ’(2Î½ âˆ’1), . . . , âˆ’5, âˆ’3, âˆ’1, +1, +3, +5, . . . , +(2Î½ âˆ’1)

(10.20)
for some positive integer Î½. A less prevalent constellation is the constellation
{âˆ’2, âˆ’1, +1, +2}.
(10.21)
The number of points in the constellation X is just # X, i.e., the number of
elements (cardinality) of the set X.
The minimum distance Î´ of a constellation is the Euclidean distance between
the closest distinct elements in the constellation:
Î´ â‰œmin
x,xâ€²âˆˆX
xÌ¸=xâ€²
|x âˆ’xâ€²|.
(10.22)
The scaling of the constellation is arbitrary because of the scaling factor A in the
signalâ€™s description. Thus, the signal A 
â„“Xâ„“g(t âˆ’â„“Ts), where Xâ„“takes values in
the set {Â±1} is of constellation {âˆ’1, +1}, but it can also be expressed in the form
Aâ€² 
â„“Xâ€²
â„“g(t âˆ’â„“Ts), where Aâ€² = 2A and Xâ€²
â„“takes values in the set {âˆ’1/2, +1/2},
i.e., as a PAM signal of constellation {âˆ’1/2, +1/2}.
Diï¬€erent authors choose to normalize the constellation in diï¬€erent ways.
One
common normalization is to express the elements of the constellation as multiples
of the minimum distance. Thus, we would represent the constellation {âˆ’1, +1} as
1
âˆ’1
2Î´ , +1
2Î´
2
,
and the constellation {âˆ’3, âˆ’1, +1, +3} as
1
âˆ’3
2Î´ , âˆ’1
2Î´ , +1
2Î´ , +3
2Î´
2
.
The normalized version of the constellation (10.20) is
1
Â±2Î½ âˆ’1
2
Î´, . . . , Â±5
2Î´, Â±3
2Î´, Â±1
2Î´
2
.
(10.23)
We say that a constellation is centered if

xâˆˆX
x = 0.
(10.24)
The second moment of a constellation X is deï¬ned as
1
# X

xâˆˆX
x2.
(10.25)
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.9 Uncoded Transmission
187
The second moment of the constellation in (10.23) is given by
1
# X

xâˆˆX
x2 = 1
2Î½ 2
Î½

Î·=1
(2Î· âˆ’1)2 Î´2
4
= 1
12

M2 âˆ’1

Î´2,
(10.26a)
where
M = 2Î½
(10.26b)
is the number of points in the constellation, and where (10.26a)â€“(10.26b) can be
veriï¬ed using the identity
Î½

Î·=1
(2Î· âˆ’1)2 = 1
3 Î½ (4Î½2 âˆ’1),
Î½ = 1, 2, . . .
(10.27)
10.9
Uncoded Transmission
Classifying a coding scheme as uncoded sounds like an oxymoron, but it is not.
The term â€œuncoded transmissionâ€ or â€œuncoded communicationâ€ is important and
will be clariï¬ed next. Recall that the mapping Ï•: {0, 1}k â†’Rn maps the data
bits D1, . . . , Dk to the real symbols X1, . . . , Xn. As we have seen in Section 10.8,
the constellation X is the smallest subset of R for whichâ€”irrespective of the data
bitsâ€”each of the symbols X1, . . . , Xn is in X. Thus, the range of Ï• is contained in
X n, and X is the smallest set for which this holds. We emphasize that the range
of Ï• can be a strict subset of X n: some sequences in X n may not be producible
by the encoder irrespective of the data bits fed to it. This is the case, for example,
when the encoder is based on the repetition mapping (10.4) and X = {+1, âˆ’1},
because in this case X1 must equal X2, and sequences beginning with +1, âˆ’1 are
in X n but not in the range of Ï•.
Engineers say that the transmission is uncoded or that the system is uncoded
if the range of Ï• equals X n

Ï•(d) : d âˆˆ{0, 1}k
= X n,
(10.28)
i.e., if every sequence in X n can be produced by the encoder by feeding it the
appropriate data sequence. To see why they use this terminology, it is instructive
to consider some examples.
Examples when (10.28) does hold are when we employ antipodal signaling (10.2)
or the mapping (10.3). More generally, it holds whenever the encoder operates in
block-mode using a (K, 1) block encoder (and k is divisible by K so no zero padding
is required). Such encoders do not rule out any output sequence in X n and they
are therefore error prone (hence â€œuncodedâ€). An example when (10.28) does not
hold is when the encoder operates in block-mode using the (1, 2) (repetition) block
encoder of (10.4). More generally, (10.28) does not hold whenever the encoder
operates in block-mode using a (K, N) block encoder that is nontrivial in the sense
that its range is a strict subset of X N. An uncoded system does not therefore
employ any such nontrivial block codes.
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

188
Mapping Bits to Waveforms
Since we are assuming throughout that Ï• is one-to-one (Section 10.3), the system
is uncoded if, and only if, Ï•: {0, 1}k â†’X n is one-to-one and onto, i.e., a bijection.
10.10
Bandwidth Considerations
Without knowing the data bits that produced the PAM signal, it is very diï¬ƒcult
to say much about it. A notable exception is the bandwidth, which is essentially
determined by the pulse shape: unless A is zero or the symbols x1, . . . , xn are
all zero, the bandwidth of the PAM signal is equal to the bandwidth of the pulse
shape g. One part of the proof of this statement hinges on elementary properties of
the Fourier Transform. The other hinges on the Fundamental Theorem of Algebra
(Rudin, 1987, Chapter 10, Theorem 10.25), which states that the number of roots
of a nonzero polynomial cannot exceed its degree (and, in fact, equals its degree if
we account for the multiplicity of the roots.)
We present the result in a slightly more general form.
Lemma 10.10.1. Let g: R â†’C be an integrable signal of bandwidth W.
Let
cm, . . . , cn be complex numbers that are not all zero, where m and n are integers
satisfying m â‰¤n. Let Ts be positive. Then the signal t 	â†’n
â„“=m câ„“g(t âˆ’â„“Ts) is an
integrable signal of the same bandwidth as g, namely, W.
Proof. Let g, m, n, and cm, . . . , cn be as above, and deï¬ne the signal
h: t 	â†’
n

â„“=m
câ„“g(t âˆ’â„“Ts).
Its Fourier Transform Ë†h is
Ë†h(f) =
n

â„“=m
câ„“eâˆ’i2Ï€fâ„“Ts Ë†g(f)
=
	
n

â„“=m
câ„“eâˆ’i2Ï€fâ„“Ts

Ë†g(f)
= zm
nâˆ’m

â„“=0
câ„“+m zâ„“

z=eâˆ’i2Ï€fTs
Ë†g(f),
f âˆˆR.
(10.29)
It thus follows that at every frequency at which Ë†g is zero so is Ë†h, and hence the
bandwidth of h cannot exceed that of g. It remains to show that the bandwidth
of h is not smaller than that of g. We shall establish this by showing that there
can be at most a ï¬nite number of frequencies where Ë†h is zero but Ë†g is not.
Since Ë†g is zero outside the band [âˆ’W, W ], we need to show that there are at
most a ï¬nite number of frequencies f âˆˆ[âˆ’W, W ] where Ë†h(f) is zero but Ë†g(f) is
not. For f to be such a frequency, the function z 	â†’zm nâˆ’m
â„“=0 câ„“+m zâ„“must be
zero at eâˆ’i2Ï€fTs. Since zm has no roots on the unit circle {z âˆˆC : |z| = 1}, the
only way a frequency f can satisfy this condition is if eâˆ’i2Ï€fTs is a root of the
complex polynomial z 	â†’nâˆ’m
â„“=0 câ„“+m zâ„“, whose degree is at most n âˆ’m. Since, by
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.11 Design Considerations
189
hypothesis, the coeï¬ƒcients cm, . . . , cn are not all zero, this polynomial is not the
zero polynomial. Consequently, by the Fundamental Theorem of Algebra, it has at
most n âˆ’m roots. Of these roots, only some, say d, are on the unit circle. Denote
these
eiÎ¸1, . . . , eiÎ¸d,
where Î¸1, . . . , Î¸d âˆˆ[âˆ’Ï€, Ï€) and d â‰¤n âˆ’m. We conclude that only at frequencies f
for which eâˆ’i2Ï€fTs is equal to eiÎ¸Î½ for some Î½ âˆˆ{1, . . . , d} can Ë†h(f) be zero and
Ë†g(f) not. Since eâˆ’i2Ï€fTs can equal eiÎ¸Î½ only if f has the form
âˆ’Î¸Î½
2Ï€Ts
+ Î·
Ts
for some integer Î·, it follows that there are a ï¬nite number of frequencies in the
band [âˆ’W, W ] where eâˆ’i2Ï€fTs equals eiÎ¸Î½. Consequently, there are at most a ï¬nite
number of frequencies in the band [âˆ’W, W ] where Ë†h is zero and Ë†g is not, and the
bandwidth of h cannot be smaller than that of g.
Corollary 10.10.2 (The Bandwidth of a PAM Is that of the Pulse Shape). If a
PAM signal (with an integrable pulse shape) is not zero, then its bandwidth is equal
to the bandwidth of the pulse shape.
Proof. If the pulse shape is of ï¬nite bandwidth W, then the result follows directly
from the lemma. Otherwise, the result follows by noting that there are a count-
able number of frequencies f where eâˆ’i2Ï€Ts equals eiÎ¸Î½, and the ï¬nite union of a
countable number of frequencies is countable. Consequently, there are at most a
countable number of frequencies where the FT of the PAM signal is zero whereas
that of the pulse shape is not.
10.11
Design Considerations
Designing a communication system employing PAM with a block encoder entails
making choices. We need to choose the PAM parameters A, Ts, and g, and we
need to choose a (K, N) block encoder enc(Â·). These choices greatly inï¬‚uence the
overall system characteristics such as the transmitted power, bandwidth, and the
performance of the system in the presence of noise. To design a system well, we
must understand the eï¬€ect of the design choices on the overall system at three
levels. At the ï¬rst level we must understand which design parameters inï¬‚uence
which overall system characteristics.
At the second level we must understand
how the design parameters inï¬‚uence the system. And at the third level we must
understand how to choose the design parameters so as to optimize the system
characteristics subject to the given constraints.
In this book we focus on the ï¬rst two levels. The third requires tools from Infor-
mation Theory and from Coding Theory that are beyond the scope of this book.
Here we oï¬€er a preview of the ï¬rst level. We thus brieï¬‚y and informally explain
which design choices inï¬‚uence which overall system properties.
To simplify the preview, we shall assume in this section that the time shifts of the
pulse shape by integer multiples of the baud period are orthonormal. Consequently,
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

190
Mapping Bits to Waveforms
we shall denote the pulse shape by Ï† and assume that (10.18) holds. We shall also
assume that k and n tend to inï¬nity as in the bi-inï¬nite block mode discussed in
Section 14.5.2. Roughly speaking this assumption is tantamount to the assumption
that the system has been running since time âˆ’âˆand that it will continue running
until time +âˆ.
Our discussion is extremely informal, and we apologize to the reader for discussing
concepts that we have not yet deï¬ned. Readers who are aggravated by this practice
may choose to skip this section; the issues will be revisited in Chapter 29 after
everything has been deï¬ned and all the claims proved.
The key observation we wish to highlight is that, to a great extent,
the choice of the block encoder enc(Â·) can be decoupled from the
choice of the pulse shape Ï†. The bandwidth and power spectral
density depend hardly at all on enc(Â·) and very much on Ï†, whereas
the probability of error on the white Gaussian noise channel de-
pends very much on enc(Â·) and not at all on Ï†.
This observation greatly simpliï¬es the design problem because it means that, rather
than optimizing over Ï† and enc(Â·) jointly, we can choose each of them separately.
We next brieï¬‚y discuss the diï¬€erent overall system characteristics and which design
choices inï¬‚uence them.
Data Rate:
The data rate Rb that the system supports is determined by the baud
period Ts and by the rate K/N of the encoder. It is given by
Rb = 1
Ts
K
N
 bit
sec

.
Power:
The transmitted power does not depend on the pulse shape Ï† (Theo-
rem 14.5.2). It is determined by the amplitude A, the baud period Ts, and the
block encoder enc(Â·). In fact, if the constellation is centered and the transmission
is uncoded, then the transmitted power is determined by A, Ts, and the second
moment of the constellation.
Power Spectral Density:
If the block encoder enc(Â·) is such that when it is fed
the data bits it produces zero-mean and uncorrelated symbols of equal variance,
then the power spectral density is determined by A, Ts, and Ï† only; it is unaï¬€ected
by enc(Â·) (Section 15.4).
Bandwidth:
The bandwidth of the transmitted waveform is equal to the band-
width of the pulse shape Ï† (Corollary 10.10.2 and Theorem 15.4.1). We will see in
Chapter 11 that for the orthonormality (10.18) to hold (for all n), the bandwidth W
of the pulse shape must satisfy
W â‰¥
1
2Ts
.
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.12 Some Implementation Considerations
191
In Chapter 11 we shall also see how to design Ï† so as to satisfy (10.18) and so as
to have its bandwidth as close as we wish to 1/(2Ts).4
Probability of Error:
It is a remarkable fact that the pulse shape Ï† does not aï¬€ect
the performance of the system on the additive white Gaussian noise channel. Per-
formance is determined only by A, Ts, and the block encoder enc(Â·) (Section 26.4.2).
The preceding discussion focused on PAM, but many of the results also hold for
Quadrature Amplitude Modulation, which is discussed in Chapters 16, 18, and 28.
10.12
Some Implementation Considerations
It is instructive to consider some of the issues related to the generation of a PAM
signal
X(t) = A
n

â„“=1
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR.
(10.30)
Here we focus on delay, causality, and digital implementation.
10.12.1
Delay
To illustrate the delay issue in PAM, suppose that the pulse shape g(Â·) is strictly
positive. In this case we note that, irrespective of which epoch tâ€² âˆˆR we consider,
the calculation of X(tâ€²) requires knowledge of the entire n-tuple X1, . . . , Xn. Since
the sequence X1, . . . , Xn cannot typically be determined in its entirety unless the
entire sequence D1, . . . , Dk is determined ï¬rst, it follows that, when g(Â·) is strictly
positive, the modulator cannot produce X(tâ€²) before observing the entire data
sequence D1, . . . , Dk. And this is true for any tâ€² âˆˆR! Since in the back of our
minds we think about D1, . . . , Dk as the data bits that will be sent during the
entire life of the system or, at least, from the moment it is turned on until it is
shut oï¬€, it is unrealistic to expect the modulator to observe the entire sequence
D1, . . . , Dk before producing any input to the channel.
The engineering solution to this problem is to ï¬nd some positive integer L such
that, for all practical purposes, g(t) is zero whenever |t| > LTs, i.e.,
g(t) â‰ˆ0,
|t| > LTs.
(10.31)
In this case we have that, irrespective of tâ€² âˆˆR, only 2L + 1 terms (approximately)
determine X(tâ€²). Indeed, if Îº is an integer such that
ÎºTs â‰¤tâ€² < (Îº + 1)Ts,
(10.32)
then
X(tâ€²) â‰ˆA
Îº+L

â„“=max{1,Îºâˆ’L}
Xâ„“g(t âˆ’â„“Ts),
ÎºTs â‰¤tâ€² < (Îº + 1)Ts,
(10.33)
4Information-theoretic considerations suggest that this is a good approach.
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

192
Mapping Bits to Waveforms
where the sum is assumed to be zero if Îº + L < 1.
Thus, if (10.31) holds, then the approximate calculation of X(tâ€²) can be performed
without knowledge of the entire sequence X1, . . . , Xn and the modulator can start
producing the waveform X(Â·) as soon as it knows X1, . . . , XL.
10.12.2
Causality
The reader may object to the fact that, even if (10.31) holds, the signal X(Â·) may
be nonzero at negative times. It might therefore seem as though the transmitter
needs to transmit a signal before the system has been turned on and that, worse
still, this signal depends on the data bits that will be fed to the system in the
future when the system is turned on. But this is not really an issue. It all has
to do with how we deï¬ne the epoch t = 0, i.e., to what physical time instant
does t = 0 correspond. We never said it corresponded to the instant when the
system was turned on and, in fact, there is no reason to set the time origin at
that time instant or at the â€œBig Bang.â€ For example, we can set the time origin
at LTs seconds-past-system-turn-on, and the problem disappears. Similarly, if the
transmitted waveform depends on X1, . . . , XL, and if these real numbers can only
be computed once the data bits D1, . . . , DÎº have been fed to the encoder, then it
would make sense to set the time origin to the moment at which the last of these Îº
data bits has been fed to the encoder.
Some problems in Digital Communications that appear like tough causality prob-
lems end up being easily solved by time delays and the redeï¬nition of the time
origin. Others can be much harder. It is sometimes diï¬ƒcult for the novice to de-
termine which causality problem is of the former type and which of the latter. As
a rule of thumb, you should be extra cautious when the system contains feedback
loops.
10.12.3
Digital Implementation
Even when all the symbols among X1, . . . , Xn that are relevant for the calculation
of X(tâ€²) are known, the actual computation may be tricky, particularly if the
formula describing the pulse shape is diï¬ƒcult to implement in hardware. In such
cases one may opt for a digital implementation using look-up tables. The idea is
to compute only samples of X(Â·) and to then interpolate using a digital-to-analog
(D/A) converter and an anti-aliasing ï¬lter. The samples must be computed at a
rate determined by the Sampling Theorem, i.e., at least once every 1/(2W) seconds,
where W is the bandwidth of the pulse shape.
The computation of the values of X(Â·) at its samples can be done by choosing L
suï¬ƒciently large so that (10.31) holds and by then approximating the sum (10.30)
for tâ€² satisfying (10.32) by the sum (10.33). The samples of this latter sum can be
computed with a digital computer orâ€”as is more common if the symbols take on a
ï¬nite (and small) number of valuesâ€”using a pre-programmed look-up table. The
size of the look-up table thus depends on two parameters: the number of samples
one needs to compute every Ts seconds (determined via the bandwidth of g(Â·) and
the Sampling Theorem), and the number of addresses needed (as determined by L
and by the constellation size).
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

10.13 Exercises
193
10.13
Exercises
Exercise 10.1 (Exploiting Orthogonality). Let the energy-limited real signals Ï†1 and Ï†2
be orthogonal, and let A(1) and A(2) be positive constants. Let the waveform X be given
by
X =
	
A(1)X(1) + A(2)X(2)
Ï†1 +
	
A(1)X(1) âˆ’A(2)X(2)
Ï†2,
where X(1) and X(2) are unknown real numbers. How can you recover X(1) and X(2)
from X?
Exercise 10.2 (More Orthogonality). Extend Exercise 10.1 to the case where Ï†1, . . . Ï†Î·
are orthonormal;
X =
	
a(1,1)A(1)X(1) + Â· Â· Â· + a(Î·,1)A(Î·)X(Î·)
Ï†1 + Â· Â· Â·
+
	
a(1,Î·)A(1)X(1) + Â· Â· Â· + a(Î·,Î·)A(Î·)X(Î·)
Ï†Î·;
and where the real numbers a(Î¹,Î½) for Î¹, Î½ âˆˆ{1, . . . , Î·} satisfy the orthogonality condition
Î·

Î½=1
a(Î¹,Î½)a(Î¹â€²,Î½) =

Î·
if Î¹ = Î¹â€²,
0
if Î¹ Ì¸= Î¹â€²,
Î¹, Î¹â€² âˆˆ{1, . . . , Î·}.
Exercise 10.3 (8-PPM). To send three bits D0, D1, and D2 using 8-PPM we send the
signal t â†’A g

tâˆ’(D0+2D1+4D2)Ts

, where g: t â†’I

|t| â‰¤Tc/2

and Tc is some positive
constant. Here PPM stands for Pulse Position Modulation. Show that this mapping can
be represented as a mapping of D0, D1, D2 to real numbers X1, . . . , X8 followed by the
mapping of X1, . . . , X8 to the signal t â†’A 8
â„“=1 Xâ„“Ëœg

t âˆ’â„“Ts

. What is Ëœg?
Exercise 10.4 (A Constellation and its Second Moment). What is the constellation cor-
responding to the (1, 3) binary-to-reals block encoder that maps 0 to (+1, +2, +2) and
maps 1 to (âˆ’1, âˆ’2, âˆ’2)? What is its second moment? Let the real symbols

Xâ„“, â„“âˆˆZ

be generated from IID random bits

Dj, j âˆˆZ

(Deï¬nition 14.5.1) in block mode using
this block encoder. Compute
lim
Lâ†’âˆ
1
2L + 1
L

â„“=âˆ’L
E
$
X2
â„“
%
.
Exercise 10.5 (The Rate and Constellation Size). A PAM encoder Ï•: {0, 1}k â†’Rn has
constellation X of size # X. Show that its rate k/n is upper-bounded by log2(# X).
Exercise 10.6 (Smoothing a PAM Signal). Let

X(t)

be the result of mapping the IID
random bits D1, . . . , DK to the real numbers X1, . . . , XN using enc: {0, 1}K â†’RN and
then mapping these symbols to the waveform
X(t) = A
N

â„“=1
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR,
where A, Ts are positive and g is an energy-limited pulse shape. Deï¬ne
Y (t) = 1
17
 t+17
t
X(Ï„) dÏ„,
t âˆˆR.
Can

Y (t)

be viewed as a PAM signal? If so, of what pulse shape?
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

194
Mapping Bits to Waveforms
Exercise 10.7 (Orthonormal Signal Representation). Prove Theorem 10.5.1.
Hint: Recall the Gram-Schmidt procedure.
Exercise 10.8 (Unbounded PAM Signal). Consider the formal expression
X(t) =
âˆ

â„“=âˆ’âˆ
Xâ„“sinc
	 t
Ts âˆ’â„“

,
t âˆˆR.
(i) Show that even if the Xâ„“â€™s can only take on the values Â±1, the value of X(Ts/2)
can be arbitrarily high. That is, ï¬nd a sequence {xâ„“}âˆ
âˆ’âˆsuch that xâ„“âˆˆ{+1, âˆ’1}
for every â„“âˆˆZ and
lim
Lâ†’âˆ
L

â„“=âˆ’L
xâ„“sinc
	1
2 âˆ’â„“

= âˆ.
(ii) Suppose now that g: R â†’R satisï¬es
g(t)
 â‰¤
Î²
1 + |t/Ts|1+Î± ,
t âˆˆR
for some Î±, Î² > 0. Show that if for some Î³ > 0 we have |xâ„“| â‰¤Î³ for all â„“âˆˆZ, then
the sum
âˆ

â„“=âˆ’âˆ
xâ„“g(t âˆ’â„“Ts)
converges at every t and is a bounded function of t.
Exercise 10.9 (Etymology). Let g be an integrable real signal. Express the frequency
response of the matched ï¬lter for g in terms of the FT of g. Repeat when g is a complex
signal. Can you guess the origin of the term â€œMatched Filterâ€?
Hint: Recall the notion of a â€œmatched impedance.â€
Exercise 10.10 (Recovering the Symbols from a Filtered PAM Signal). Let X(Â·) be the
PAM signal (10.17), where A > 0, and where g(t) is zero for |t| â‰¥Ts/2 and positive for
|t| < Ts/2.
(i) Suppose that X(Â·) is fed to a ï¬lter of impulse response h: t â†’I{|t| â‰¤Ts/2}. Is
it true that for every â„“âˆˆ{1, . . . , n} one can recover Xâ„“from the ï¬lterâ€™s output at
time â„“Ts? If so, how?
(ii) Suppose now that the ï¬lterâ€™s impulse response is h: t â†’I{âˆ’Ts/2 â‰¤t â‰¤3Ts/4}.
Can one always recover Xâ„“from the ï¬lterâ€™s output at time â„“Ts? Can one recover
the sequence (X1, . . . , Xn) from the n samples of the ï¬lterâ€™s output at the times
Ts, . . . , nTs?
Exercise 10.11 (Continuous Phase Modulation). In Continuous Phase Modulation (CPM)
the symbols

Xâ„“

are mapped to the waveform
X(t) = A cos

2Ï€fct + 2Ï€h
âˆ

â„“=âˆ’âˆ
Xâ„“q(t âˆ’â„“Ts)

,
t âˆˆR,
where fc, h > 0 are constants and q is a mapping from R to R. Is CPM a special case of
linear modulation?
available at 
.012
14:28:43, subject to the Cambridge Core terms of use,

Chapter 11
Nyquistâ€™s Criterion
11.1
Introduction
In Section 10.7 we discussed the beneï¬t of choosing the pulse shape Ï† in Pulse
Amplitude Modulation so that its time shifts by integer multiples of the baud
period Ts be orthonormal. We saw that if the real transmitted signal is given by
X(t) = A
n

â„“=1
Xâ„“Ï†(t âˆ’â„“Ts),
t âˆˆR,
where for all integers â„“, â„“â€² âˆˆ{1, . . . , n}
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
then
Xâ„“= 1
A
 âˆ
âˆ’âˆ
X(t) Ï†(t âˆ’â„“Ts) dt,
â„“= 1, . . . , n,
and all the inner products
 âˆ
âˆ’âˆ
X(t) Ï†(t âˆ’â„“Ts) dt,
â„“= 1, . . . , n
can be computed using one circuit by feeding the signal X(Â·) to a matched ï¬lter of
impulse response ~Ï† and sampling the output at the times t = â„“Ts, for â„“= 1, . . . , n.
(In the complex case the matched ï¬lter is of impulse response ~Ï†âˆ—.)
In this chapter we shall address the design of and the limitations on signals that are
orthogonal to their time shifts. While our focus so far has been on real functions Ï†,
for reasons that will become apparent in Chapter 16 when we discuss Quadrature
Amplitude Modulation, we prefer to generalize the discussion and allow Ï† to be
complex. The main results of this chapter are Corollary 11.3.4 and Corollary 11.3.5.
An obvious way of choosing a signal Ï† that is orthogonal to its time shifts by
nonzero integer multiples of Ts is by choosing a pulse that is zero outside some
interval of length Ts, say [âˆ’Ts/2, Ts/2).
This guarantees that the pulse and its
time shifts by nonzero integer multiples of Ts do not overlap in time and that they
195
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

196
Nyquistâ€™s Criterion
are thus orthogonal.
But this choice limits us to pulses of inï¬nite bandwidth,
because no nonzero bandlimited signal can vanish outside a ï¬nite (time) interval
(Theorem 6.8.2).
Fortunately, as we shall see, there exist signals that are orthogonal to their time
shifts and that are also bandlimited.
This does not contradict Theorem 6.8.2
because these signals are not time-limited.
They are orthogonal to their time
shifts in spite of overlapping with them in time.
Since we have in mind using the pulse to send a very large number of symbols n
(where n corresponds to the number of symbols sent during the lifetime of the
system) we shall strengthen the orthonormality requirement to
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†âˆ—(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
for all integers â„“, â„“â€²
(11.1)
and not only to those â„“, â„“â€² in {1, . . . , n}.
We shall refer to Condition (11.1) as
saying that â€œthe time shifts of Ï† by integer multiples of Ts are orthonormal.â€
Condition (11.1) can also be phrased as a condition on Ï†â€™s self-similarity function,
which we introduce next.
11.2
The Self-Similarity Function of Energy-Limited Signals
We next introduce the self-similarity function of energy-limited signals. This
term is not standard; more common in the literature is the term â€œautocorrelation
function.â€ I prefer â€œself-similarity function,â€ which was proposed to me by Jim
Massey, because it reduces the risk of confusion with the autocovariance function
and the autocorrelation function of stochastic processes. There is nothing random
in our current setup.
Deï¬nition 11.2.1 (Self-Similarity Function). The self-similarity function Rvv
of an energy-limited signal v âˆˆL2 is deï¬ned as the mapping
Rvv : Ï„ 	â†’
 âˆ
âˆ’âˆ
v(t + Ï„) vâˆ—(t) dt,
Ï„ âˆˆR.
(11.2)
If v is real, then the self-similarity function has a nice pictorial interpretation: one
plots the original signal and the result of shifting the signal by Ï„ on the same graph,
and one then takes the pointwise product and integrates over time.
The main properties of the self-similarity function are summarized in the following
proposition.
Proposition 11.2.2 (Properties of the Self-Similarity Function). Let Rvv be the
self-similarity function of some energy-limited signal v âˆˆL2.
(i) Value at zero:
Rvv(0) =
 âˆ
âˆ’âˆ
|v(t)|2 dt.
(11.3)
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.2 The Self-Similarity Function of Energy-Limited Signals
197
(ii) Maximum at zero:
|Rvv(Ï„)| â‰¤Rvv(0),
Ï„ âˆˆR.
(11.4)
(iii) Conjugate symmetry:
Rvv(âˆ’Ï„) = Râˆ—
vv(Ï„),
Ï„ âˆˆR.
(11.5)
(iv) Integral representation:
Rvv(Ï„) =
 âˆ
âˆ’âˆ
|Ë†v(f)|2 ei2Ï€fÏ„ df,
Ï„ âˆˆR,
(11.6)
where Ë†v is the L2-Fourier Transform of v.
(v) Uniform Continuity: Rvv is uniformly continuous.
(vi) Convolution Representation:
Rvv(Ï„) = (v â‹†~vâˆ—) (Ï„),
Ï„ âˆˆR.
(11.7)
Proof. Part (i) follows by substituting Ï„ = 0 in (11.2).
Part (ii) follows by noting that Rvv(Ï„) is the inner product between the mapping
t 	â†’v(t + Ï„) and the mapping t 	â†’v(t); by the Cauchy-Schwarz Inequality (Theo-
rem 3.3.1); and by noting that both of the above mappings have the same energy,
namely, the energy of v:
|Rvv(Ï„)| =

 âˆ
âˆ’âˆ
v(t + Ï„) vâˆ—(t) dt

â‰¤
	 âˆ
âˆ’âˆ
|v(t + Ï„)|2 dt

1/2	  âˆ
âˆ’âˆ
|vâˆ—(t)|2 dt

1/2
= âˆ¥vâˆ¥2
2
= Rvv(0),
Ï„ âˆˆR.
Part (iii) follows from the substitution s â‰œt + Ï„ in the following:
Rvv(Ï„) =
 âˆ
âˆ’âˆ
v(t + Ï„) vâˆ—(t) dt
=
 âˆ
âˆ’âˆ
v(s) vâˆ—(s âˆ’Ï„) ds
=
	 âˆ
âˆ’âˆ
v(s âˆ’Ï„) vâˆ—(s) ds

âˆ—
= Râˆ—
vv(âˆ’Ï„),
Ï„ âˆˆR.
Part (iv) follows from the representation of Rvv(Ï„) as the inner product between
the mapping t 	â†’v(t + Ï„) and the mapping t 	â†’v(t); by Parsevalâ€™s Theorem;
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

198
Nyquistâ€™s Criterion
and by noting that the L2-Fourier Transform of the mapping t 	â†’v(t + Ï„) is the
(equivalence class of the) mapping f 	â†’ei2Ï€fÏ„ Ë†v(f):
Rvv(Ï„) =
 âˆ
âˆ’âˆ
v(t + Ï„) vâˆ—(t) dt
=

t 	â†’v(t + Ï„), t 	â†’v(t)

=

f 	â†’ei2Ï€fÏ„ Ë†v(f), f 	â†’Ë†v(f)

=
 âˆ
âˆ’âˆ
ei2Ï€fÏ„|Ë†v(f)|2 df,
Ï„ âˆˆR.
Part (v) follows from the integral representation of Part (iv) and the integrability
of the function f 	â†’|Ë†v(f)|2 using Theorem 6.2.11 (ii).
Part (vi) follows from the substitution s â‰œt + Ï„ and by rearranging terms:
Rvv(Ï„) =
 âˆ
âˆ’âˆ
v(t + Ï„) vâˆ—(t) dt
=
 âˆ
âˆ’âˆ
v(s) vâˆ—(s âˆ’Ï„) ds
=
 âˆ
âˆ’âˆ
v(s)~vâˆ—(Ï„ âˆ’s) ds
= (v â‹†~vâˆ—)(Ï„).
With the above deï¬nition we can restate the orthonormality condition (11.1) in
terms of the self-similarity function RÏ†Ï† of Ï†:
Proposition 11.2.3 (Shift-Orthonormality and Self-Similarity). If Ï† is energy-
limited, then the shift-orthonormality condition
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†âˆ—(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ
(11.8)
is equivalent to the condition
RÏ†Ï†(â„“Ts) = I{â„“= 0},
â„“âˆˆZ.
(11.9)
Proof. The proposition follows by substituting s â‰œt âˆ’â„“â€²Ts in the LHS of (11.8)
to obtain
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†âˆ—(t âˆ’â„“â€²Ts) dt =
 âˆ
âˆ’âˆ
Ï†

s + (â„“â€² âˆ’â„“)Ts

Ï†âˆ—(s) ds
= RÏ†Ï†

(â„“â€² âˆ’â„“)Ts

.
At this point, Proposition 11.2.3 does not seem particularly helpful because Con-
dition (11.9) is not easy to verify. But, as we shall see in the next section, this
condition can be phrased very elegantly in the frequency domain.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.3 Nyquistâ€™s Criterion
199
11.3
Nyquistâ€™s Criterion
Deï¬nition 11.3.1 (Nyquist Pulse). We say that a complex signal v: R 	â†’C is a
Nyquist Pulse of parameter Ts if
v(â„“Ts) = I{â„“= 0},
â„“âˆˆZ.
(11.10)
Theorem 11.3.2 (Nyquistâ€™s Criterion). Let Ts > 0 be given, and let the signal v(Â·)
be given by
v(t) =
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df,
t âˆˆR,
(11.11)
for some integrable function g: f 	â†’g(f). Then v(Â·) is a Nyquist Pulse of param-
eter Ts if, and only if,
lim
Jâ†’âˆ
 1/(2Ts)
âˆ’1/(2Ts)
Ts âˆ’
J

j=âˆ’J
g

f + j
Ts
 df = 0.
(11.12)
Note 11.3.3. Condition (11.12) is sometimes written informally1 in the form
âˆ

j=âˆ’âˆ
g

f + j
Ts

= Ts,
âˆ’1
2Ts
â‰¤f â‰¤
1
2Ts
,
(11.13)
or, in view of the periodicity of the LHS of (11.13), as
âˆ

j=âˆ’âˆ
g

f + j
Ts

= Ts,
f âˆˆR.
(11.14)
Neither form is mathematically precise.
Proof. We will show that v(âˆ’â„“Ts) is the â„“-th Fourier Series Coeï¬ƒcient of the
function2
f 	â†’
1
âˆšTs
âˆ

j=âˆ’âˆ
g

f + j
Ts

,
âˆ’1
2Ts
â‰¤f â‰¤
1
2Ts
.
(11.15)
It will then follow that the condition that v is a Nyquist Pulse of parameter Ts is
equivalent to the condition that the function in (11.15) has Fourier Series Coeï¬ƒ-
cients that are all zero except for the zeroth coeï¬ƒcient, which is one. The theorem
will then follow by noting that a function is indistinguishable from a constant if,
and only if, all but its zeroth Fourier Series Coeï¬ƒcient are zero. (This can be
proved by applying Theorem A.2.3 with g1 chosen as the constant function.) The
1There is no guarantee that the sum converges at every frequency f.
2Since, by hypothesis, g is integrable, it follows that the sum in (11.15) converges in the L1
sense, i.e., that there exists some integrable function sâˆsuch that
lim
Jâ†’âˆ
 1/(2Ts)
âˆ’1/(2Ts)
sâˆ(f) âˆ’
J

j=âˆ’J
g

f + j
Ts
 df = 0.
By writing 	âˆ
j=âˆ’âˆg

f +
j
Ts

we are referring to this function sâˆ.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

200
Nyquistâ€™s Criterion
value of the constant can be computed from the zeroth Fourier Series Coeï¬ƒcient.
To conclude the proof we thus need to relate v(âˆ’â„“Ts) to the â„“-th Fourier Series
Coeï¬ƒcient of the function in (11.15). The calculation is straightforward: for every
integer â„“,
v(âˆ’â„“Ts) =
 âˆ
âˆ’âˆ
g(f) eâˆ’i2Ï€fâ„“Ts df
=
âˆ

j=âˆ’âˆ

j
Ts +
1
2Ts
j
Ts âˆ’
1
2Ts
g(f) eâˆ’i2Ï€fâ„“Ts df
=
âˆ

j=âˆ’âˆ

1
2Ts
âˆ’
1
2Ts
g

Ëœf + j
Ts

eâˆ’i2Ï€( Ëœ
f+ j
Ts )â„“Ts d Ëœf
=
âˆ

j=âˆ’âˆ

1
2Ts
âˆ’
1
2Ts
g

Ëœf + j
Ts

eâˆ’i2Ï€ Ëœ
fâ„“Ts d Ëœf
=

1
2Ts
âˆ’
1
2Ts
âˆ

j=âˆ’âˆ
g

Ëœf + j
Ts

eâˆ’i2Ï€ Ëœ
fâ„“Ts d Ëœf
=

1
2Ts
âˆ’
1
2Ts
	 1
âˆšTs
âˆ

j=âˆ’âˆ
g

Ëœf + j
Ts


Ts eâˆ’i2Ï€ Ëœ
fâ„“Ts d Ëœf,
(11.16)
which is the â„“-th Fourier Series Coeï¬ƒcient of the function in (11.15). Here the ï¬rst
equality follows by substituting âˆ’â„“Ts for t in (11.11); the second by partitioning the
region of integration into intervals of length
1
Ts ; the third by the change of variable
Ëœf â‰œf âˆ’j
Ts ; the fourth by the periodicity of the complex exponentials; the ï¬fth by
Fubiniâ€™s Theorem, which allows us to swap the order summation and integration;
and the ï¬nal equality by multiplying and dividing by âˆšTs.
An example of a function f 	â†’g(f) satisfying (11.12) is plotted in Figure 11.1.
Corollary 11.3.4 (Characterization of Shift-Orthonormal Pulses). Let Ï†: R 	â†’C
be energy-limited and let Ts be positive. Then the condition
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†âˆ—(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ
(11.17)
is equivalent to the condition
âˆ

j=âˆ’âˆ
Ë†Ï†

f + j
Ts

2
â‰¡Ts,
(11.18)
i.e., to the condition that the set of frequencies f âˆˆR for which the LHS of (11.18)
is not equal to Ts is of Lebesgue measure zero.3
3It is a simple technical matter to verify that the question as to whether or not (11.18) is
satisï¬ed outside a set of frequencies of Lebesgue measure zero does not depend on which element
in the equivalence class of the L2 -Fourier Transform of Ï† is considered.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.3 Nyquistâ€™s Criterion
201
f
f
f
f
Ts
Ts
Ts
1
2Ts
1
2Ts
âˆ’1
2Ts
âˆ’1
2Ts
âˆ’1
Ts
âˆ’1
Ts
1
Ts
1
Ts
2
Ts
âˆ’2
Ts
g(f)
g
	
f + 1
Ts

g
	
f âˆ’1
Ts

âˆ

j=âˆ’âˆ
g
	
f + j
Ts

= Ts
Figure 11.1: A function g(Â·) satisfying (11.12).
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

202
Nyquistâ€™s Criterion
Proof. By Proposition 11.2.3, Condition (11.17) can be equivalently expressed in
terms of the self-similarity function as
RÏ†Ï†(mTs) = I{m = 0},
m âˆˆZ.
(11.19)
The result now follows from the integral representation of the self-similarity func-
tion RÏ†Ï† (Proposition 11.2.2 (iv)) and from Theorem 11.3.2 (with the additional
simpliï¬cation that for every j âˆˆZ the function f 	â†’
Ë†Ï†

f + j
Ts
2 is nonnegative, so
the sum on the LHS of (11.18) converges (possibly to +âˆ) for every f âˆˆR).
An extremely important consequence of Corollary 11.3.4 is the following corollary
about the minimum bandwidth of a pulse Ï† satisfying the orthonormality condition
(11.1).
Corollary 11.3.5 (Minimum Bandwidth of Shift-Orthonormal Pulses). Let Ts > 0
be ï¬xed, and let Ï† be an energy-limited signal that is bandlimited to W Hz. If the
time shifts of Ï† by integer multiples of Ts are orthonormal, then
W â‰¥
1
2Ts
.
(11.20)
Equality is achieved if
Ë†Ï†(f)
 =

Ts I
'
|f| â‰¤
1
2Ts
(
,
f âˆˆR
(11.21)
and, in particular, by the sinc(Â·) pulse
Ï†(t) =
1
âˆšTs
sinc
 t
Ts

,
t âˆˆR
(11.22)
or any time shift thereof.
Proof. Figure 11.2 illustrates why Ï† cannot satisfy (11.18) if (11.20) is violated.
The ï¬gure should also convince you of the conditions for equality in (11.20).
For the algebraically-inclined readers we prove the corollary by showing that if
W â‰¤1/(2Ts), then (11.18) can only be satisï¬ed if Ï† satisï¬es (11.21) (outside a set
of frequencies of Lebesgue measure zero).4 To see this, consider the sum
âˆ

j=âˆ’âˆ
Ë†Ï†

f + j
Ts

2
(11.23)
for frequencies f in the open interval

âˆ’1
2Ts , + 1
2Ts

. The key observation in the
proof is that for frequencies in this open interval, if W â‰¤1/(2Ts), then all the terms
in the sum (11.23) are zero, except for the j = 0 term. That is,
âˆ

j=âˆ’âˆ
Ë†Ï†

f + j
Ts

2
=
Ë†Ï†(f)
2,
	
W â‰¤
1
2Ts
, f âˆˆ

âˆ’1
2Ts
, + 1
2Ts

.
(11.24)
4In the remainder of the proof we assume that Ë†Ï†(f) is zero for frequencies f satisfying |f| > W.
The proof can be easily adjusted to account for the fact that, for frequencies |f| > W, it is possible
that Ë†Ï†(Â·) be nonzero on a set of Lebesgue measure zero.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.3 Nyquistâ€™s Criterion
203
To convince yourself of (11.24), consider, for example, the term corresponding to
j = 1, namely, |Ë†Ï†(f + 1/Ts)|2. By the deï¬nition of bandwidth, it is zero whenever
|f + 1/Ts| > W, i.e., whenever f > âˆ’1/Ts + W or f < âˆ’1/Ts âˆ’W. Since the
former category f > âˆ’1/Ts + W includesâ€”by our assumption that W â‰¤1/(2Ts)â€”
all frequencies f > âˆ’1/(2Ts), we conclude that the term corresponding to j = 1
is zero for all the frequencies f in the open interval

âˆ’1
2Ts , + 1
2Ts

. More generally,
the j-th term |Ë†Ï†(f + j/Ts)|2 is zero for all frequencies f satisfying the condition
|f +j/Ts| > W, a condition that is satisï¬edâ€”assuming j Ì¸= 0 and W â‰¤1/(2Ts)â€”by
the frequencies in the open interval that is of interest to us

âˆ’1
2Ts , + 1
2Ts

.
For W â‰¤1/(2Ts) we thus obtain from (11.24) that the condition (11.18) implies
(11.21), and, in particular, that W = 1/(2Ts).
Functions satisfying (11.21) are seldom used in digital communication because they
typically decay like 1/t so that even if the transmitted symbols Xâ„“are bounded,
the signal X(t) may take on very high values (albeit quite rarely). Consequently,
the pulses Ï† that are used in practice have a larger bandwidth than 1/(2Ts).
This leads to the following deï¬nition.
Deï¬nition 11.3.6 (Excess Bandwidth). The excess bandwidth in percent of a
signal Ï† relative to Ts > 0 is deï¬ned as
100%
	bandwidth of Ï†
1/(2Ts)
âˆ’1

.
(11.25)
The following corollary to Corollary 11.3.4 is useful for the understanding of real
signals of excess bandwidth smaller than 100%.
Corollary 11.3.7 (Band-Edge Symmetry). Let Ts be positive, and let Ï† be a real
energy-limited signal that is bandlimited to W Hz, where W < 1/Ts so Ï† is of excess
bandwidth smaller than 100%. Then the time shifts of Ï† by integer multiples of Ts
are orthonormal if, and only if, f 	â†’|Ë†Ï†(f)|2 satisï¬es the band-edge symmetry
condition5
Ë†Ï†
 1
2Ts
âˆ’f

2
+
Ë†Ï†
 1
2Ts
+ f

2
â‰¡Ts,
0 < f â‰¤
1
2Ts
.
(11.26)
Proof. We ï¬rst note that, since we have assumed that W < 1/Ts, only the terms
corresponding to j = âˆ’1, j = 0, and j = 1 contribute to the sum on the LHS of
(11.18) for f âˆˆ

âˆ’1
2Ts , + 1
2Ts

. Moreover, since Ï† is by hypothesis real, it follows
that |Ë†Ï†(âˆ’f)| = |Ë†Ï†(f)|, so the sum on the LHS of (11.18) is a symmetric function
of f. Thus, the sum is equal to Ts on the interval

âˆ’1
2Ts , + 1
2Ts

if, and only if, it is
equal to Ts on the interval

0, + 1
2Ts

. For frequencies in this shorter interval only
two terms in the sum contribute: those corresponding to j = 0 and j = âˆ’1. We
5Condition (11.26) should be understood to indicate that the LHS and RHS of (11.26) are
equal for all frequencies 0 â‰¤f â‰¤1/(2Ts) outside a set of Lebesgue measure zero. Again, we ignore
this issue in the proof and assume that Ë†Ï†(f) is zero for all |f| > W.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

204
Nyquistâ€™s Criterion
f
f
f
f
f
W
W
W
âˆ’W
âˆ’W
âˆ’W
1
Ts
âˆ’1
Ts
1
Ts âˆ’W
âˆ’1
Ts + W
1
2Ts
âˆ’1
2Ts
Ë†Ï†(f)
Ë†Ï†(f)
2
Ë†Ï†

f âˆ’
1
Ts
2
Ë†Ï†

f +
1
Ts
2
Ë†Ï†

f +
1
Ts
2 +
Ë†Ï†(f)
2 +
Ë†Ï†

f âˆ’
1
Ts
2
Figure 11.2: If W < 1/(2Ts), then all the terms of the form
Ë†Ï†

f + j
Ts
2 are zero
over the shaded frequencies W < |f| < 1/(2Ts). Thus, for W < 1/(2Ts) the sum
âˆ
j=âˆ’âˆ
Ë†Ï†

f + j
Ts
2 cannot be equal to Ts at any of the shaded frequencies.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.3 Nyquistâ€™s Criterion
205
f
Ë†Ï†(f)
2
Ts
Ts
2
1
2Ts
1
Ts
f â€²
Ë†Ï†

f â€² +
1
2Ts
2 âˆ’Ts
2
Figure 11.3: An example of a choice for |Ë†Ï†(Â·)|2 satisfying the band-edge symmetry
condition (11.26).
thus conclude that, for real signals of excess bandwidth smaller than 100%, the
condition (11.18) is equivalent to the condition
Ë†Ï†(f)
2 +
Ë†Ï†(f âˆ’1/Ts)
2 â‰¡Ts,
0 â‰¤f <
1
2Ts
.
Substituting f â€² â‰œ
1
2Ts âˆ’f in this condition leads to the condition
Ë†Ï†
 1
2Ts
âˆ’f â€²
2
+
Ë†Ï†

âˆ’f â€² âˆ’1
2Ts

2
â‰¡Ts,
0 < f â€² â‰¤
1
2Ts
,
which, in view of the symmetry of |Ë†Ï†(Â·)|, is equivalent to
Ë†Ï†
 1
2Ts
âˆ’f â€²
2
+
Ë†Ï†

f â€² + 1
2Ts

2
â‰¡Ts,
0 < f â€² â‰¤
1
2Ts
,
i.e., to (11.26).
Note 11.3.8. The band-edge symmetry condition (11.26) has a nice geometric
interpretation. This is best seen by rewriting the condition in the form
Ë†Ï†
 1
2Ts
âˆ’f â€²
2
âˆ’Ts
2



=Ëœg(âˆ’f â€²)
= âˆ’
	Ë†Ï†
 1
2Ts
+ f â€²
2
âˆ’Ts
2




=Ëœg(f â€²)
,
0 < f â€² â‰¤
1
2Ts
,
(11.27)
which demonstrates that the band-edge condition is equivalent to the condition
that the plot of f 	â†’|Ë†Ï†(f)|2 in the interval 0 < f < 1/Ts be invariant with
respect to a 180â—¦-rotation around the point
 1
2Ts , Ts
2

. In other words, the function
Ëœg: f â€² 	â†’
Ë†Ï†
 1
2Ts + f â€²2 âˆ’Ts
2 should be anti-symmetric for 0 < f â€² â‰¤
1
2Ts . I.e., it
should satisfy
Ëœg(âˆ’f â€²) = âˆ’Ëœg(f â€²),
0 < f â€² â‰¤
1
2Ts
.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

206
Nyquistâ€™s Criterion
f
Ë†Ï†(f)
2
Ts
1âˆ’Î²
2Ts
1
2Ts
1+Î²
2Ts
Figure 11.4: A plot of f 	â†’| Ë†Ï†(f)|2 as given in (11.30) with Î² = 0.5.
Figure 11.3 is a plot over the interval [0, 1/Ts) of a mapping f 	â†’|Ë†Ï†(f)|2 that
satisï¬es the band-edge symmetry condition (11.26).
A popular choice of Ï† is based on the raised-cosine family of functions. For every
0 < Î² â‰¤1 and every Ts > 0, the raised-cosine function is given by the mapping
f 	â†’
â§
âª
âª
â¨
âª
âª
â©
Ts
if
0 â‰¤|f| â‰¤1âˆ’Î²
2Ts ,
Ts
2

1 + cos

Ï€Ts
Î² (|f| âˆ’1âˆ’Î²
2Ts )

if
1âˆ’Î²
2Ts < |f| â‰¤1+Î²
2Ts ,
0
if
|f| > 1+Î²
2Ts .
(11.28)
Choosing Ï† so that its Fourier Transform is the square root of the raised-cosine
mapping (11.28)
Ë†Ï†(f) =
â§
âª
âª
âª
â¨
âª
âª
âª
â©
âˆšTs
if
0 â‰¤|f| â‰¤1âˆ’Î²
2Ts ,
3
Ts
2
4
1 + cos

Ï€Ts
Î² (|f| âˆ’1âˆ’Î²
2Ts )

if
1âˆ’Î²
2Ts < |f| â‰¤1+Î²
2Ts ,
0
if
|f| > 1+Î²
2Ts ,
(11.29)
results in Ï† being real with
|Ë†Ï†(f)|2 =
â§
âª
âª
â¨
âª
âª
â©
Ts
if
0 â‰¤|f| â‰¤1âˆ’Î²
2Ts ,
Ts
2

1 + cos

Ï€Ts
Î² (|f| âˆ’1âˆ’Î²
2Ts )

if
1âˆ’Î²
2Ts < |f| â‰¤1+Î²
2Ts ,
0
if
|f| > 1+Î²
2Ts ,
(11.30)
as depicted in Figure 11.4 for Î² = 0.5.
Using (11.29) and using the band-edge symmetry criterion (Corollary 11.3.7), it
can be readily veriï¬ed that the time shifts of Ï† by integer multiples of Ts are
orthonormal. Moreover, by (11.29), Ï† is bandlimited to (1 + Î²)/(2Ts) Hz. It is
thus of excess bandwidth Î² Ã— 100%. For every 0 < Î² â‰¤1 we have thus found a
pulse Ï† of excess bandwidth Î² Ã— 100% whose time shifts by integer multiples of Ts
are orthonormal.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.3 Nyquistâ€™s Criterion
207
t
Ï„
Ï†(t)
RÏ†Ï†(Ï„)
1
1
Ts
2Ts
âˆ’Ts
âˆ’2Ts
Figure 11.5: The pulse Ï†(Â·) of (11.31) with Î² = 0.5 and its self-similarity func-
tion RÏ†Ï†(Â·) of (11.32).
In the time domain
Ï†(t) =
4Î²
Ï€âˆšTs
cos

(1 + Î²)Ï€ t
Ts

+
sin ((1âˆ’Î²)Ï€ t
Ts )
4Î² t
Ts
1 âˆ’(4Î² t
Ts )2
,
t âˆˆR,
(11.31)
with corresponding self-similarity function
RÏ†Ï†(Ï„) = sinc
 Ï„
Ts
 cos(Ï€Î²Ï„/Ts)
1 âˆ’4Î²2Ï„ 2/T2
s
,
Ï„ âˆˆR.
(11.32)
The pulse Ï† of (11.31) is plotted in Figure 11.5 (top) for Î² = 0.5. Its self-similarity
function (11.32) is plotted in the same ï¬gure (bottom). That the time shifts of Ï†
by integer multiples of Ts are orthonormal can be veriï¬ed again by observing that
RÏ†Ï† as given in (11.32) satisï¬es RÏ†Ï†(â„“Ts) = I{â„“= 0} for all â„“âˆˆZ.
Notice also that if Ï†(Â·) is chosen as in (11.31), then for all 0 < Î² â‰¤1, the pulse Ï†(Â·)
decays like 1/t2. This decay property combined with the fact that the inï¬nite sum
âˆ
Î½=1 Î½âˆ’2 converges (Rudin, 1976, Chapter 3, Theorem 3.28) will prove useful in
Section 14.3 when we discuss the power in PAM.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

208
Nyquistâ€™s Criterion
11.4
The Self-Similarity Function of Integrable Signals
This section is a bit technical and can be omitted at ï¬rst reading. In it we deï¬ne
the self-similarity function for integrable signals that are not necessarily energy-
limited, and we then compute the Fourier Transform of the so-deï¬ned self-similarity
function.
Recall that a Lebesgue measurable complex signal v: R â†’C is integrable if
 âˆ
âˆ’âˆ|v(t)| dt < âˆand that the class of integrable signal is denoted by L1. For
such signals there may be Ï„â€™s for which the integral in (11.2) is undeï¬ned. For
example, if v is not energy-limited, then the integral in (11.2) will be inï¬nite at
Ï„ = 0. Nevertheless, we can discuss the self-similarity function of such signals by
adopting the convolution representation of Proposition 11.2.2 (vi) as the deï¬nition.
We thus deï¬ne the self-similarity function Rvv of an integrable signal v âˆˆL1 as
Rvv â‰œv â‹†~vâˆ—,
v âˆˆL1,
(11.33)
but we need some clariï¬cation. Since v is integrable, and since this implies that
its mirror image ~v is also integrable, it follows that the convolution in (11.33) is
a convolution between two integrable signals. As such, we are guaranteed by the
discussion leading to (5.9) that the integral
 âˆ
âˆ’âˆ
v(Ïƒ)~vâˆ—(Ï„ âˆ’Ïƒ) dÏƒ =
 âˆ
âˆ’âˆ
v(t + Ï„) vâˆ—(t) dt
is deï¬ned for all Ï„â€™s outside a set of Lebesgue measure zero. (This set of Lebesgue
measure zero will include the point Ï„ = 0 if v is not of ï¬nite energy.) For Ï„â€™s inside
this set of measure zero we deï¬ne the self-similarity function to be zero. The value
zero is quite arbitrary because, irrespective of the value we choose for such Ï„â€™s, we
are guaranteed by (5.9) that the so-deï¬ned self-similarity function Rvv is integrable
 âˆ
âˆ’âˆ
Rvv(Ï„)
 dÏ„ â‰¤âˆ¥vâˆ¥2
1 ,
v âˆˆL1,
(11.34)
and that its L1-Fourier Transform is given by the product of the L1-Fourier Trans-
form of v and the L1-Fourier Transform of ~vâˆ—, i.e.,
Ë†Rvv(f) = |Ë†v(f)|2,

v âˆˆL1, f âˆˆR

.
(11.35)
11.5
Exercises
Exercise 11.1 (Passband Signaling). Let f0, Ts > 0 be ï¬xed.
(i) Show that a signal x is a Nyquist Pulse of parameter Ts if, and only if, the signal
t â†’ei2Ï€f0t x(t) is such a pulse.
(ii) Show that if x is a Nyquist Pulse of parameter Ts, then so is t â†’cos(2Ï€f0t) x(t).
(iii) If t â†’cos(2Ï€f0t) x(t) is a Nyquist Pulse of parameter Ts, must x also be one?
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.5 Exercises
209
Exercise 11.2 (Self-Similarity of a Nonnegative Signal). Show that the self-similarity
function of a nonnegative signal is nonnegative.
Exercise 11.3 (The Self-Similarity Function of a Delayed Signal). Let u be an energy-
limited signal, and let the signal v be given by v: t â†’u(tâˆ’t0). Express the self-similarity
function of v in terms of the self-similarity of u and t0.
Exercise 11.4 (Transforming an Energy-Limited Signal). Let v be an energy-limited signal
of self-similarity function Rvv, and let p be the signal
p(t) = Av(Î±t),
t âˆˆR,
where A and Î± are real numbers (not necessarily positive) and Î± Ì¸= 0. Is p energy-limited?
If so, relate its self-similarity function Rpp to Rvv.
Exercise 11.5 (Reï¬‚ection and the Self-Similarity Function). Let v be a complex energy-
limited signal. Relate its self-similarity function Rvv to that of its mirror image ~v.
Exercise 11.6 (The Self-Similarity Function of a Frequency Shifted Signal). Let u be
an energy-limited complex signal, and let the signal v be given by v: t â†’u(t) ei2Ï€f0t for
some f0 âˆˆR. Express the self-similarity function of v in terms of f0 and the self-similarity
function of u.
Exercise 11.7 (A Self-Similarity Function). Compute and plot the self-similarity function
of the signal t â†’A

1 âˆ’|t|/T

I

|t| â‰¤T

.
Exercise 11.8 (Symmetry of the FT of the Self-Similarity Function of a Real Signal).
Show that if Ï† is an integrable real signal, then the FT of its self-similarity function is
symmetric:
	
Ë†RÏ†Ï†(f) = Ë†RÏ†Ï†(âˆ’f),
f âˆˆR

,
Ï† âˆˆL1 is real.
Exercise 11.9 (When Is the Convolution a Nyquist Pulse?). Let s and h be energy-
limited.
(i) Show that their convolution can be expressed as

s â‹†h

(t) =
 âˆ
âˆ’âˆ
g(f) ei2Ï€ft df,
t âˆˆR,
where g is some integrable function. Express g in terms of Ë†s and Ë†h.
(ii) Let the L2-Fourier Transform of h be (the equivalence class of)
f â†’4
3

1 âˆ’|f|Ts

I
 
|f| â‰¤
1
2Ts
!
,
f âˆˆR.
What condition on Ë†s is equivalent to the condition that sâ‹†h is a Nyquist pulse with
respect to Ts? Which is the signal s of least bandwidth that satisï¬es this condition?
Hint: For Part (i) adapt the proof of Proposition 6.6.1.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

210
Nyquistâ€™s Criterion
Exercise 11.10 (The Self-Similarity Function is Positive Deï¬nite). Show that if v is an
energy-limited signal, n is a positive integer, Î±1, . . . , Î±n âˆˆC, and t1, . . . , tn âˆˆR, then
n

j=1
n

â„“=1
Î±jÎ±âˆ—
â„“Rvv(tj âˆ’tâ„“) â‰¥0.
Hint: Compute the energy in the signal t â†’n
j=1 Î±j v(t + tj).
Exercise 11.11 (Self-Similarity Functions and Characteristic Functions). Show that Î¦(Â·)
is the characteristic function of some RV having a probability density function if, and
only if, Î¦ = Ruu for some unit-energy (possibly complex) signal u. (This is the Wiener-
Khinchin criterion.)
Hint: If Î¦(Â·) is the characteristic function of the RV X of density fX(Â·), consider the
L2-Fourier Transform of âˆšfX. For the other direction use Proposition 11.2.2 (iv).
Exercise 11.12 (Bounds on the Self-Similarity Function). Let v be an energy-limited
signal that is bandlimited to W Hz, and let Rvv be its self-similarity function. Prove that
Re

Rvv(Ï„)

â‰¥âˆ¥vâˆ¥2
2
	
1 âˆ’min

2, 2Ï€2 W2Ï„ 2
,
Ï„ âˆˆR.
Hint: See Exercise 6.11.
Exercise 11.13 (Relaxing the Orthonormality Condition). What is the minimal bandwidth
of an energy-limited signal whose time shifts by even multiples of Ts are orthonormal?
What is the minimal bandwidth of an energy-limited signal whose time shifts by odd
multiples of Ts are orthonormal?
Exercise 11.14 (Bandwidth and Shift-Orthonormality). Consider the signal
g(t) = Î±
 Î²
âˆ’Î²
&
1 âˆ’|f|
Î² ei2Ï€ft df,
t âˆˆR,
where Î± and Î² are positive numbers.
(i) Is g an energy-limited bandlimited signal? If so, of what bandwidth and energy?
(ii) Given Ts > 0, ï¬nd positive numbers Î± and Î² for which g satisï¬es
 âˆ
âˆ’âˆ
g(t âˆ’â„“Ts) g(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ.
Exercise 11.15 (A Speciï¬c Signal). Let p be the complex energy-limited bandlimited
signal whose FT Ë†p is given by
Ë†p(f) = Ts

1 âˆ’|Tsf âˆ’1|

I
 
0 â‰¤f â‰¤2
Ts
!
,
f âˆˆR.
(i) Plot Ë†p(Â·).
(ii) Is p(Â·) a Nyquist Pulse of parameter Ts?
(iii) Is the real part of p(Â·) a Nyquist Pulse of parameter Ts?
(iv) What about the imaginary part of p(Â·)?
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

11.5 Exercises
211
Exercise 11.16 (Orthogonality of Time Shifts of Diï¬€erent Signals).
(i) Let u and v be energy-limited signals of L2-Fourier Transforms Ë†u and Ë†v. Let Ts be
a positive constant. Show that the condition
 âˆ
âˆ’âˆ
u(t âˆ’â„“Ts) vâˆ—(t âˆ’â„“â€²Ts) dt = 0,
â„“, â„“â€² âˆˆZ
is equivalent to the condition
lim
Jâ†’âˆ

1
2Ts
âˆ’
1
2Ts

J

j=âˆ’J
Ë†u
	
f + j
Ts

Ë†vâˆ—	
f + j
Ts

 df = 0.
If either u or v is bandlimited to W Hz, then the latter condition is equivalent to
f â†’
âˆ

j=âˆ’âˆ
Ë†u
	
f + j
Ts

Ë†vâˆ—	
f + j
Ts

being indistinguishable from the all-zero function.
(ii) Assume that u and v are energy-limited signals that are bandlimited to W Hz and
that for every integer â„“
Ruu(â„“Ts) = Rvv(â„“Ts) = I{â„“= 0}
and
 âˆ
âˆ’âˆ
u(t) vâˆ—(t âˆ’â„“Ts) dt = 0.
Prove that W â‰¥1/Ts.
Exercise 11.17 (Nyquistâ€™s Third Criterion). We say that an energy-limited signal Ïˆ(Â·)
satisï¬es Nyquistâ€™s Third Criterion if
 (2Î½+1)Ts/2
(2Î½âˆ’1)Ts/2
Ïˆ(t) dt =

1
if Î½ = 0,
0
if Î½ âˆˆZ \ {0}.
(11.36)
(i) Express the LHS of (11.36) as an inner product between Ïˆ and some function gÎ½.
(ii) Show that (11.36) is equivalent to
Ts
 âˆ
âˆ’âˆ
Ë†Ïˆ(f) eâˆ’i2Ï€fÎ½Ts sinc(Tsf) df =

1
if Î½ = 0,
0
if Î½ âˆˆZ \ {0}.
(iii) Show that, loosely speaking, Ïˆ satisï¬es Nyquistâ€™s Third Criterion if, and only if,
f â†’
âˆ

j=âˆ’âˆ
Ë†Ïˆ
	
f âˆ’j
Ts

sinc(Tsf âˆ’j)
is indistinguishable from the all-one function. More precisely, if and only if,
lim
Jâ†’âˆ

1
2Ts
âˆ’
1
2Ts
1 âˆ’
J

j=âˆ’J
Ë†Ïˆ
	
f âˆ’j
Ts

sinc(Tsf âˆ’j)
 df = 0.
(iv) What is the FT of the pulse of least bandwidth that satisï¬es Nyquistâ€™s Third
Criterion with respect to the baud Ts? What is its bandwidth?
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,
www.ebook3000.com

212
Nyquistâ€™s Criterion
Exercise 11.18 (Multiplication by a Carrier).
(i) Let u be an energy-limited complex signal that is bandlimited to W Hz, and let
f0 > W be given. Let v be the signal v: t â†’u(t) cos(2Ï€f0t). Express the self-
similarity function of v in terms of f0 and the self-similarity function of u.
(ii) Let the signal Ï† be given by Ï†: t â†’
âˆš
2 cos(2Ï€fct) Ïˆ(t), where fc > W/2 > 0;
where 4fcTs is an odd integer; and where Ïˆ is a real energy-limited signal that
is bandlimited to W/2 Hz and whose time shifts by integer multiples of (2Ts) are
orthonormal. Show that the time shifts of Ï† by integer multiples of Ts are orthonor-
mal.
Exercise 11.19 (The Self-Similarity of a Convolution). Let p and q be integrable signals
of self-similarity functions Rpp and Rqq. Show that the self-similarity function of their
convolution p â‹†q is indistinguishable from Rpp â‹†Rqq.
Exercise 11.20 (The Integral of the Self-Similarity Function). Relate the integral of an
integrable signal x to the integral of its self-similarity function Rxx.
available at 
.013
14:28:43, subject to the Cambridge Core terms of use,

Chapter 12
Stochastic Processes: Deï¬nition
12.1
Introduction and Continuous-Time Heuristics
In this chapter we shall deï¬ne stochastic processes. Our deï¬nition will be general so
as to include the continuous-time stochastic processes of the type we encountered
in Section 10.2 and also discrete-time processes.
In Section 10.2 we saw that since the data bits that we wish to communicate
are random, the transmitted waveform is a stochastic process.
But stochastic
processes play an important role in Digital Communications not only in modeling
the transmitted signals: they are also used to model the noise in the system and
other sources of impairments.
The stochastic processes we encountered in Section 10.2 are continuous-time pro-
cesses. We proposed that you think about such a process as a real-valued function
of two variables: â€œtimeâ€ and â€œluck.â€ By â€œluckâ€ we mean the realization of all the
random components of the system, e.g., the bits to be sent, the realization of the
noise processes (that we shall discuss later), or any other sources of randomness in
the system.
Somewhat more precisely, recall that a probability space is deï¬ned as a triple
(Î©, F, P), where the set Î© is the set of experiment outcomes, the set F is the set
of events, and where P(Â·) assigns probabilities to the various events. A measurable
real-valued function of the outcome is a random variable, and a function of time and
the experiment outcome is a random process or a stochastic process. A continuous-
time stochastic process X is thus a mapping
X: Î© Ã— R â†’R
(Ï‰, t) 	â†’X(Ï‰, t).
If we ï¬x some experiment outcome Ï‰ âˆˆÎ©, then the random process can be regarded
as a function of one argument: time. This function is sometimes called a sample-
path, trajectory, sample-path realization, or a sample function
X(Ï‰, Â·): R â†’R
t 	â†’X(Ï‰, t).
213
available at 
.014
14:28:44, subject to the Cambridge Core terms of use,
www.ebook3000.com

214
Stochastic Processes: Deï¬nition
t
t
Ts
2
âˆ’Ts
2
Ts
âˆ’Ts
g(t)
4
â„“=âˆ’4 xâ„“g(t âˆ’â„“Ts)
Figure 12.1: The pulse shape g: t 	â†’

1 âˆ’4|t|/Ts

I

|t| < Ts/4

, and the sample
function t 	â†’4
â„“=âˆ’4 xâ„“g(t âˆ’â„“Ts) when

xâˆ’4, xâˆ’3, xâˆ’2, xâˆ’1, x0, x1, x2, x3, x4

=
(âˆ’1, âˆ’1, +1, +1, âˆ’1, +1, âˆ’1, âˆ’1, âˆ’1).
Similarly, if we ï¬x an epoch t âˆˆR and view the stochastic process as a function of
â€œluckâ€ only, we obtain a random variable:
X(Â·, t): Î© â†’R
Ï‰ 	â†’X(Ï‰, t).
This random variable is sometimes called the value of the process at time t or
the time-t sample of the process.
Figure 12.1 shows the pulse shape g: t 	â†’

1 âˆ’4|t|/Ts

I{|t| < Ts/4} and a sample-
path of the PAM signal
X(t) =
4

â„“=âˆ’4
Xâ„“g(t âˆ’â„“Ts)
(12.1)
with {Xâ„“} taking values in the set {âˆ’1, +1}.
Notice that in this example the
functions t 	â†’g(t âˆ’â„“Ts) and t 	â†’g(t âˆ’â„“â€²Ts) do not â€œoverlapâ€ if â„“Ì¸= â„“â€².
Figure 12.2 shows the pulse shape
g: t 	â†’

1 âˆ’
4
3Ts |t|
|t| â‰¤3Ts
4 ,
0
|t| > 3Ts
4 ,
t âˆˆR
(12.2)
and a sample-path of the PAM signal (12.1) for {Xâ„“} taking values in the set
{âˆ’1, +1}. In this example the mappings t 	â†’g(t âˆ’â„“Ts) and t 	â†’g(t âˆ’â„“â€²Ts) do
overlap (when â„“â€² âˆˆ{â„“âˆ’1, â„“, â„“+ 1}).
12.2
A Formal Deï¬nition
We next give a formal deï¬nition of a stochastic process, which is also called a
random process, or a random function.
available at 
.014
14:28:44, subject to the Cambridge Core terms of use,

12.2 A Formal Deï¬nition
215
t
t
Ts
2
âˆ’Ts
2
Ts
âˆ’Ts
g(t)
4
â„“=âˆ’4 xâ„“g(t âˆ’â„“Ts)
Figure 12.2: The pulse shape g of (12.2) and the trajectory t 	â†’4
â„“=âˆ’4 xâ„“g(tâˆ’â„“Ts)
for

xâˆ’4, xâˆ’3, xâˆ’2, xâˆ’1, x0, x1, x2, x3, x4

= (âˆ’1, âˆ’1, +1, +1, âˆ’1, +1, âˆ’1, âˆ’1, âˆ’1).
Deï¬nition 12.2.1 (Stochastic Process). A stochastic process

X(t), t âˆˆT

is an
indexed family of random variables that are deï¬ned on a common probability space
(Î©, F, P). Here T denotes the indexing set and X(t) (or sometimes Xt) denotes
the random variable indexed by t.
Thus, X(t) is the random variable to which t âˆˆT is mapped. For each t âˆˆT
we have that X(t) is a random variable, i.e., a measurable mapping from the
experiment outcomes set Î© to the reals.1
A stochastic process

X(t), t âˆˆT

is said to be centered or of zero mean if all
the random variables in the family are of zero mean, i.e., if for every t âˆˆT we have
E[X(t)] = 0. It is said to be of ï¬nite variance if all the random variables in the
family are of ï¬nite variance, i.e., if E

X2(t)

< âˆfor all t âˆˆT .
The case where the indexing set T comprises only one element is not particularly
exciting because in this case the stochastic process is just a random variable with
fancy packaging. Similarly, when T is ï¬nite, the SP is just a random vector or a
tuple of random variables in disguise. The cases that will be of most interest are
enumerated below.
(i) When the indexing set T is the set of integers Z, the stochastic process is
said to be a discrete-time stochastic process and in this case it is simply
a bi-inï¬nite sequence of random variables
. . . , Xâˆ’2, Xâˆ’1, X0, X1, X2, . . .
1Some authors, e.g., (Doob, 1990), allow for X(t) to take on the values Â±âˆprovided that
at each t âˆˆT this occurs with zero probability, but we, following (Lo`eve, 1963), insist that X(t)
only take on ï¬nite values.
available at 
.014
14:28:44, subject to the Cambridge Core terms of use,
www.ebook3000.com

216
Stochastic Processes: Deï¬nition
For discrete-time stochastic processes it is customary to denote the random
variable to which Î½ âˆˆZ is mapped by XÎ½ rather than X(Î½) and to refer to
XÎ½ as the time-Î½ sample of the process

XÎ½, Î½ âˆˆZ

.
(ii) When the indexing set is the set of positive integers N, the stochastic process
is said to be a one-sided discrete-time stochastic process and it is simply
a one-sided sequence of random variables
X1, X2, . . .
Again, we refer to XÎ½ as the time-Î½ sample of

XÎ½, Î½ âˆˆN

.
(iii) When the indexing set T is the real line R, the stochastic process is said to
be a continuous-time stochastic process and the random variable X(t)
is the time-t sample of

X(t), t âˆˆR

.
In dealing with continuous-time stochastic processes we shall usually denote the
process by

X(t), t âˆˆR

, by X, by X(Â·), or by

X(t)

. The random variable to
which t is mapped, i.e., the time-t sample of the process will be denoted by X(t).
Its realization will be denoted by x(t), and the sample-path of the process by x or
x(Â·).
Discrete-time processes will typically be denoted by

XÎ½, Î½ âˆˆZ

or by

XÎ½

.
We shall need only a few results on discrete-time stochastic processes, and those will
be presented in Chapter 13. Continuous-time stochastic processes will be discussed
in Chapter 25.
12.3
Describing Stochastic Processes
The description of a continuous-time stochastic process in terms of a random vari-
able (as in Section 10.2), in terms of a ï¬nite number of random variables (as in
PAM signaling), or in terms of an inï¬nite sequence of random variables (as in the
transmission using PAM signaling of an inï¬nite binary data stream) is particularly
well suited for describing human-generated stochastic processes or stochastic pro-
cesses that are generated using a mechanism that we fully understand. We simply
describe how the stochastic process is synthesized from the random variables. The
method is less useful when the stochastic process denotes a random signal (such
as thermal noise or some other interference of unknown origin) that we observe
rather than generate. In this case we can use measurements and statistical meth-
ods to analyze the process. Often, the best we can hope for is to be informed
of the ï¬nite-dimensional distributions of the process, a concept that will be
introduced in Section 25.2.
12.4
Additional Reading
Classic references on stochastic processes to which we shall frequently refer are
(Doob, 1990) and (Lo`eve, 1963). We also recommend (Gikhman and Skorokhod,
1996), (CramÂ´er and Leadbetter, 2004), and (Grimmett and Stirzaker, 2001). For
discrete-time stochastic processes, see (Pourahmadi, 2001) and (Porat, 2008).
available at 
.014
14:28:44, subject to the Cambridge Core terms of use,

12.5 Exercises
217
12.5
Exercises
Exercise 12.1 (Objects in a Basement). Let T1, T2, . . . be a sequence of positive random
variables, and let N1, N2, . . . be a sequence of random variables taking values in N. Deï¬ne
X(t) =
âˆ

j=1
Nj I

t â‰¥Tj

,
t âˆˆR.
Draw some sample paths of

X(t), t âˆˆR

. Assume that at time zero a basement is empty
and that Nj denotes the number of objects in the j-th box, which is brought down to the
basement at time Tj. Explain why you can think of X(t) as the number of objects in the
basement at time t.
Exercise 12.2 (A Queue). Let S1, S2, . . . be a sequence of positive random variables. A
system is turned on at time zero. The ï¬rst customer arrives at the system at time S1
and the next at time S1 + S2.
More generally, Customer Î· arrives SÎ· minutes after
Customer (Î· âˆ’1). The system serves one customer at a time. It takes the system one
minute to serve each customer, and a customer leaves the system once it has been served.
Let X(t) denote the number of customers in the system at time t. Express X(t) in terms
of S1, S2, . . . Is

X(t), t âˆˆR

a stochastic process? If so, draw a few of its sample paths.
Compute Pr
$
X(0.5) > 0
%
. Express your answer in terms of the distribution of S1, S2, . . .
Exercise 12.3 (A Continuous-Time Markov SP). A particle is in State Zero at time t = 0.
It stays in that state for T (0)
1
seconds and then jumps to State One. It stays in State One
for T (1)
1
seconds and then jumps back to State Zero, where it stays for T (0)
2
seconds. In
general, T (0)
Î½
is the duration of the particleâ€™s stay in State Zero on its Î½-th visit to that
state. Similarly, T (1)
Î½
is the duration of its stay in State One on its Î½-th visit. Assume
that T (0)
1
, T (1)
1
, T (0)
2
, T (1)
2
, T (0)
3
, T (1)
3
, . . . are independent with T (0)
Î½
being a mean-Î¼0
exponential and with T (1)
Î½
being a mean-Î¼1 exponential for all Î½ âˆˆN.
Let X(t) be deterministically equal to zero for t < 0, and equal to the particleâ€™s state for
t â‰¥0.
(i) Plot some sample paths of

X(t), t âˆˆR

.
(ii) What is the probability that the sample path t â†’X(Ï‰, t) is continuous in the
interval [0, t)?
(iii) Conditional on X(t) = 0, where t â‰¥0, what is the distribution of the remaining
duration of the particleâ€™s stay in State Zero?
Hint: An exponential RV X has the memoryless property, i.e., that for every s, t â‰¥0 we
have Pr[X > s + t|X > t] = Pr[X â‰¥s].
Exercise 12.4 (Peak Power). Let the random variables

Dj, j âˆˆZ

be independent and
identically distributed, each taking on the values 0 and 1 equiprobably. Let
X(t) = A
âˆ

â„“=âˆ’âˆ

1 âˆ’2Dâ„“

g(t âˆ’â„“Ts),
t âˆˆR,
where A, Ts > 0 and g: t â†’I{|t| â‰¤3Ts/4}. Find the distribution of the random variable
sup
tâˆˆR
X(t)
.
available at 
.014
14:28:44, subject to the Cambridge Core terms of use,
www.ebook3000.com

218
Stochastic Processes: Deï¬nition
Exercise 12.5 (Sample-Path Continuity). Let the random variables

Dj, j âˆˆZ

be
independent and identically distributed, each taking on the values 0 and 1 equiprobably.
Let
X(t) = A
âˆ

â„“=âˆ’âˆ

1 âˆ’2Dâ„“

g(t âˆ’â„“Ts),
t âˆˆR,
where A, Ts > 0. Suppose that the function g: R â†’R is continuous and is zero outside
some interval, so g(t) = 0 whenever |t| â‰¥T. Show that for every Ï‰ âˆˆÎ©, the sample-path
t â†’X(Ï‰, t) is a continuous function of time.
Exercise 12.6 (Random Sampling Time). Consider the setup of Exercise 12.5, with the
pulse shape g: t â†’

1 âˆ’2|t|/Ts

I

|t| â‰¤Ts/2

. Further assume that the RV T is inde-
pendent of

Dj, j âˆˆZ

and uniformly distributed over the interval [âˆ’Î´, Î´]. Find the
distribution of X(kTs + T) for any integer k.
Exercise 12.7 (A Strange SP). Let T be a mean-one exponential RV, and deï¬ne the SP

X(t), t âˆˆR

by
X(t) =

1
if t = T,
0
otherwise.
Compute the distribution of X(t1) and the joint distribution of X(t1) and X(t2) for
t1, t2 âˆˆR. What is the probability that the sample-path t â†’X(Ï‰, t) is continuous at t1?
What is the probability that the sample-path is a continuous function (everywhere)?
Exercise 12.8 (The Sum of Stochastic Processes: Formalities). Let the stochastic pro-
cesses

X1(t), t âˆˆR

and

X2(t), t âˆˆR

be deï¬ned on the same probability space
(Î©, F, P). Let

Y (t), t âˆˆR

be the SP corresponding to their sum. Express Y as a
mapping from Î© Ã— R to R. What is Y (Ï‰, t) for (Ï‰, t) âˆˆÎ© Ã— R?
Exercise 12.9 (Independent Stochastic Processes). Let the SP

X1(t), t âˆˆR

be de-
ï¬ned on the probability space (Î©1, F1, P1), and let

X2(t), t âˆˆR

be deï¬ned on the
space (Î©2, F2, P2). Deï¬ne a new probability space (Î©, F, P) with two stochastic processes
 Ëœ
X1(t), t âˆˆR

and
 Ëœ
X2(t), t âˆˆR

such that for every Î· âˆˆN and epochs t1, . . . , tÎ· âˆˆR
the following three conditions hold:
1) The joint law of Ëœ
X1(t1), . . . , Ëœ
X1(tÎ·) is the same as the joint law of X1(t1), . . . , X1(tÎ·).
2) The joint law of Ëœ
X2(t1), . . . , Ëœ
X2(tÎ·) is the same as the joint law of X2(t1), . . . , X2(tÎ·).
3) The Î·-tuple Ëœ
X1(t1), . . . , Ëœ
X1(tÎ·) is independent of the Î·-tuple Ëœ
X2(t1), . . . , Ëœ
X2(tÎ·).
Hint: Consider Î© = Î©1 Ã— Î©2.
Exercise 12.10 (Pathwise Integration). Let

Xj, j âˆˆZ

be IID random variables deï¬ned
over the probability space (Î©, F, P), with Xj taking on the values 0 and 1 equiprobably.
Deï¬ne the stochastic process

X(t), t âˆˆR

as
X(t) =
âˆ

j=âˆ’âˆ
Xj I{j â‰¤t < j + 1},
t âˆˆR.
For a given n âˆˆN, compute the distribution of the random variable
Ï‰ â†’
 n
0
X(Ï‰, t) dt.
available at 
.014
14:28:44, subject to the Cambridge Core terms of use,

Chapter 13
Stationary Discrete-Time Stochastic
Processes
13.1
Introduction
This chapter discusses some of the properties of real discrete-time stochastic pro-
cesses. Extensions to complex discrete-time stochastic processes are discussed in
Chapter 17.
13.2
Stationary Processes
A discrete-time stochastic process is said to be stationary if all equal-length tuples
of consecutive samples have the same joint law. Thus:
Deï¬nition 13.2.1 (Stationary Discrete-Time Processes). A discrete-time SP

XÎ½

is said to be stationary or strict sense stationary or strongly stationary
if for every n âˆˆN and all integers Î·, Î·â€² the joint distribution of the n-tuple
(XÎ·, . . . XÎ·+nâˆ’1) is identical to that of the n-tuple (XÎ·â€², . . . , XÎ·â€²+nâˆ’1):

XÎ·, . . . XÎ·+nâˆ’1
 L=

XÎ·â€², . . . XÎ·â€²+nâˆ’1

.
(13.1)
Here
L= denotes equality of distribution (law) so X
L= Y indicates that the random
variables X and Y have the same distribution; (X, Y )
L= (W, Z) indicates that the
pair (X, Y ) and the pair (W, Z) have the same joint distribution; and similarly for
n-tuples.
By considering the case where n = 1 we obtain that if

XÎ½

is stationary, then the
distribution of XÎ· is the same as the distribution of XÎ·â€², for all Î·, Î·â€² âˆˆZ. That
is, if

XÎ½

is stationary, then all the random variables in the family

XÎ½, Î½ âˆˆZ

have the same distribution: the random variable X1 has the same distribution as
the random variable X2, etc. Thus,

XÎ½, Î½ âˆˆZ

stationary

=â‡’

XÎ½
L= X1, Î½ âˆˆZ

.
(13.2)
By considering in the above deï¬nition the case where n = 2 we obtain that for a
stationary process

XÎ½

the joint distribution of X1, X2 is the same as the joint
219
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,
www.ebook3000.com

220
Stationary Discrete-Time Stochastic Processes
distribution of XÎ·, XÎ·+1 for any integer Î·. More, however, is true. If

XÎ½

is
stationary, then the joint distribution of XÎ½, XÎ½â€² is the same as the joint distribution
of XÎ·+Î½, XÎ·+Î½â€²:

XÎ½, Î½ âˆˆZ

stationary

=â‡’

(XÎ½, XÎ½â€²)
L= (XÎ·+Î½, XÎ·+Î½â€²), Î½, Î½â€², Î· âˆˆZ

. (13.3)
To prove (13.3) ï¬rst note that it suï¬ƒces to treat the case where Î½ â‰¥Î½â€² because
(X, Y )
L= (W, Z) if, and only if, (Y, X)
L= (Z, W). Next note that stationarity
implies that

XÎ½â€², . . . , XÎ½
 L=

XÎ·+Î½â€², . . . , XÎ·+Î½

(13.4)
because both are (Î½ âˆ’Î½â€² + 1)-length tuples of consecutive samples of the process.
Finally, (13.4) implies that the joint distribution of (XÎ½â€², XÎ½) is identical to the
joint distribution of (XÎ·+Î½â€², XÎ·+Î½) and (13.3) follows.
The above argument can be generalized to more samples. This yields the following
proposition, which gives an alternative deï¬nition of stationarity, a deï¬nition that
more easily generalizes to continuous-time stochastic processes.
Proposition 13.2.2. A discrete-time SP

XÎ½, Î½ âˆˆZ

is stationary if, and only if,
for every n âˆˆN, all integers Î½1, . . . , Î½n âˆˆZ, and every Î· âˆˆZ

XÎ½1, . . . , XÎ½n
 L=

XÎ·+Î½1, . . . , XÎ·+Î½n

.
(13.5)
Proof. One direction is trivial and simply follows by substituting consecutive in-
tegers for Î½1, . . . , Î½n in (13.5). The proof of the other direction is a straightforward
extension of the argument we used to prove (13.3).
By noting that (W1, . . . , Wn)
L= (Z1, . . . , Zn) if, and only if,1 
j Î±jWj
L= 
j Î±jZj
for all Î±1, . . . , Î±n âˆˆR we obtain the following equivalent characterization of sta-
tionary processes:
Proposition 13.2.3. A discrete-time SP

XÎ½

is stationary if, and only if, for every
n âˆˆN, all Î·, Î½1, . . . , Î½n âˆˆZ, and all Î±1, . . . , Î±n âˆˆR
n

j=1
Î±jXÎ½j
L=
n

j=1
Î±jXÎ½j+Î·.
(13.6)
13.3
Wide-Sense Stationary Stochastic Processes
Deï¬nition 13.3.1 (Wide-Sense Stationary Discrete-Time SP). We say that a
discrete-time SP

XÎ½, Î½ âˆˆZ

is wide-sense stationary (WSS) or weakly
stationary or covariance stationary or second-order stationary or weak-
sense stationary if the following three conditions are satisï¬ed:
1This follows because the multivariate characteristic function determines the joint distribution
(see Proposition 23.4.4 or (Dudley, 2003, Chapter 9, Section 5, Theorem 9.5.1)) and because
the characteristic functions of all the linear combinations of the components of a random vector
determine the multivariate characteristic function of the random vector (Feller, 1971, Chapter XV,
Section 7).
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,

13.4 Stationarity and Wide-Sense Stationarity
221
1) The random variables XÎ½, Î½ âˆˆZ, are all of ï¬nite variance:
Var[XÎ½] < âˆ,
Î½ âˆˆZ.
(13.7a)
2) The random variables XÎ½, Î½ âˆˆZ, have identical means:
E[XÎ½] = E[X1] ,
Î½ âˆˆZ.
(13.7b)
3) The quantity E[XÎ½â€²XÎ½] depends on Î½â€² and Î½ only via Î½ âˆ’Î½â€²:
E[XÎ½â€²XÎ½] = E[XÎ·+Î½â€²XÎ·+Î½] ,
Î½, Î½â€², Î· âˆˆZ.
(13.7c)
Note 13.3.2. By considering (13.7c) when Î½ = Î½â€² we obtain that all the samples
of a WSS SP have identical second moments. And since, by (13.7b), they also all
have identical means, it follows that all the samples of a WSS SP have identical
variances:

XÎ½, Î½ âˆˆZ

WSS

=â‡’

Var[XÎ½] = Var[X1] ,
Î½ âˆˆZ

.
(13.8)
An alternative deï¬nition of a WSS process in terms of the variance of linear func-
tionals of the process is given below.
Proposition 13.3.3. A ï¬nite-variance discrete-time SP

XÎ½

is WSS if, and only
if, for every n âˆˆN, every Î·, Î½1, . . . , Î½n âˆˆZ, and every Î±1, . . . , Î±n âˆˆR
n

j=1
Î±jXÎ½j and
n

j=1
Î±jXÎ½j+Î·
have the same mean & variance.
(13.9)
Proof. The proof is left as an exercise. Alternatively, see the proof of Proposi-
tion 17.5.5.
13.4
Stationarity and Wide-Sense Stationarity
Comparing (13.9) with (13.6) we see that, for ï¬nite-variance stochastic processes,
stationarity implies wide-sense stationarity, which is the content of the following
proposition. This explains why stationary processes are sometimes called strong-
sense stationary and why wide-sense stationary processes are sometimes called
weak-sense stationary.
Proposition 13.4.1 (Finite-Variance Stationary Stochastic Processes Are WSS).
Every ï¬nite-variance discrete-time stationary SP is WSS.
Proof. While this is obvious from (13.9) and (13.6) we shall nevertheless give an
alternative proof because the proof of Proposition 13.3.3 was left as an exercise. The
proof is straightforward and follows directly from (13.2) and (13.3) by noting that if
X
L= Y , then E[X] = E[Y ] and that if (X, Y )
L= (W, Z), then E[XY ] = E[WZ].
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,
www.ebook3000.com

222
Stationary Discrete-Time Stochastic Processes
It is not surprising that not every WSS process is stationary. Indeed, the deï¬nition
of WSS processes only involves means and covariances, so it cannot possibly say
everything regarding the distribution. For example, the process whose samples
are independent with the odd ones taking on the value Â±1 equiprobably and with
the even ones uniformly distributed over the interval [âˆ’
âˆš
3, +
âˆš
3] is WSS but not
stationary.
13.5
The Autocovariance Function
Deï¬nition 13.5.1 (Autocovariance Function). The autocovariance function
KXX : Z â†’R of a WSS discrete-time SP

XÎ½

is deï¬ned by
KXX(Î·) â‰œCov[XÎ½+Î·, XÎ½] ,
Î· âˆˆZ.
(13.10)
Thus, the autocovariance function at Î· is the covariance between two samples of
the process taken Î· units of time apart. Note that because

XÎ½

is WSS, the RHS
of (13.10) does not depend on Î½. Also, for WSS processes all samples are of equal
mean (13.7b), so
KXX(Î·) = Cov[XÎ½+Î·, XÎ½]
= E[XÎ½+Î·XÎ½] âˆ’E[XÎ½+Î·] E[XÎ½]
= E[XÎ½+Î·XÎ½] âˆ’

E[X1]
2,
Î· âˆˆZ.
In some engineering texts the autocovariance function is called â€œautocorrelation
function.â€ We prefer the former because KXX(Î·) does not measure the correlation
coeï¬ƒcient between XÎ½ and XÎ½+Î· but rather the covariance. These concepts are
diï¬€erent also for zero-mean processes. Following (Grimmett and Stirzaker, 2001)
we deï¬ne the autocorrelation function of a WSS process of nonzero variance as
ÏXX(Î·) â‰œCov[XÎ½+Î·, XÎ½]
Var[X1]
,
Î· âˆˆZ,
(13.11)
i.e., as the correlation coeï¬ƒcient between XÎ½+Î· and XÎ½. (Recall that for a WSS
process all samples are of the same variance (13.8), so for such a process the
denominator in (13.11) is equal to

Var[XÎ½] Var[XÎ½+Î·].)
Not every function from the integers to the reals is the autocovariance function of
some WSS SP. For example, the autocovariance function must be symmetric in the
sense that
KXX(âˆ’Î·) = KXX(Î·),
Î· âˆˆZ,
(13.12)
because, by (13.10),
KXX(Î·) = Cov[XÎ½+Î·, XÎ½]
= Cov[XËœÎ½, XËœÎ½âˆ’Î·]
= Cov[XËœÎ½âˆ’Î·, XËœÎ½]
= KXX(âˆ’Î·),
Î· âˆˆZ,
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,

13.5 The Autocovariance Function
223
where in the second equality we deï¬ned ËœÎ½ â‰œÎ½ + Î·, and where in the third equal-
ity we used the fact that for real random variables the covariance is symmetric:
Cov[X, Y ] = Cov[Y, X].
Another property that the autocovariance function must satisfy is that for every
positive integer n
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€² KXX(Î½ âˆ’Î½â€²) â‰¥0,
Î±1, . . . , Î±n âˆˆR,
(13.13)
because
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€² KXX(Î½ âˆ’Î½â€²) =
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€²Cov[XÎ½, XÎ½â€²]
= Cov

n

Î½=1
Î±Î½XÎ½,
n

Î½â€²=1
Î±Î½â€²XÎ½â€²

= Var

n

Î½=1
Î±Î½XÎ½

â‰¥0.
It turns out that (13.12) and (13.13) fully characterize the autocovariance functions
of discrete-time WSS stochastic processes in a sense that is made precise in the
following theorem.
Theorem 13.5.2 (Characterizing Autocovariance Functions).
(i) If KXX is the autocovariance function of some discrete-time WSS SP

XÎ½

,
then KXX must satisfy (13.12) & (13.13) for every positive integer n.
(ii) If K: Z â†’R is some function satisfying
K(âˆ’Î·) = K(Î·),
Î· âˆˆZ
(13.14)
and
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€²K(Î½ âˆ’Î½â€²) â‰¥0,

n âˆˆN, Î±1, . . . , Î±n âˆˆR

,
(13.15)
then there exists a discrete-time WSS SP

XÎ½

whose autocovariance func-
tion KXX is given by KXX(Î·) = K(Î·) for all Î· âˆˆZ.
Proof. We have already proved Part (i). For a proof of Part (ii) see, for example,
(Doob, 1990, Chapter X, Â§ 3, Theorem 3.1) or (Pourahmadi, 2001, Theorem 5.1 in
Section 5.1 and Section 9.7).2
2For the beneï¬t of readers who have already encountered Gaussian stochastic processes, we
mention here that if K(Â·) satisï¬es (13.14) & (13.15) then we can even ï¬nd a Gaussian SP whose
autocovariance function is equal to K(Â·).
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,
www.ebook3000.com

224
Stationary Discrete-Time Stochastic Processes
A function K: Z â†’R satisfying (13.14) & (13.15) is called a positive deï¬nite
function. Such functions have been extensively studied in the literature, and in
Section 13.7 we shall give an alternative characterization of autocovariance func-
tions based on these studies. But ï¬rst we introduce the power spectral density.
13.6
The Power Spectral Density Function
Roughly speaking, the power spectral density (PSD) of a discrete-time WSS
SP (XÎ½) of autocovariance function KXX is the Discrete-Time Fourier Transform
of the bi-inï¬nite sequence . . . , KXX(âˆ’1), KXX(0), KXX(1), . . . (Appendix B). More
precisely, it is an integrable function on the interval [âˆ’1/2, 1/2) whose (âˆ’Î·)-th
Fourier Series Coeï¬ƒcient is equal to KXX(Î·). Such a function does not always
exist. When it does, it is unique in the sense that any two such functions can
only diï¬€er on a subset of the interval [âˆ’1/2, 1/2) of Lebesgue measure zero. (This
follows because integrable functions on the interval [âˆ’1/2, 1/2) that have identical
Fourier Series Coeï¬ƒcients can diï¬€er only on a subset of [âˆ’1/2, 1/2) of Lebesgue
measure zero; see Theorem A.2.3.) Consequently, we shall speak of â€œtheâ€ PSD but
try to remember that this does not always exist and that, when it does, it is only
unique in this restricted sense.
Deï¬nition 13.6.1 (Power Spectral Density). We say that the discrete-time WSS
SP

XÎ½

is of power spectral density
SXX if
SXX is an integrable mapping
from the interval [âˆ’1/2, 1/2) to the reals such that
KXX(Î·) =
 1/2
âˆ’1/2
SXX(Î¸) ei2Ï€Î·Î¸ dÎ¸,
Î· âˆˆZ.
(13.16)
But see also Note 13.6.5 ahead.
Note 13.6.2. We shall sometimes abuse notation and, rather than say that the
stochastic process

XÎ½, Î½ âˆˆZ

is of PSD SXX, we shall say that the autocovariance
function KXX is of PSD SXX.
By considering the special case of Î· = 0 in (13.16) we obtain that
Var[XÎ½] = KXX(0)
=
 1/2
âˆ’1/2
SXX(Î¸) dÎ¸,
Î½ âˆˆZ.
(13.17)
The main result of the following proposition is that power spectral densities are
nonnegative (except possibly on a set of Lebesgue measure zero).
Proposition 13.6.3 (PSDs Are Nonnegative and Symmetric).
(i) If the WSS SP

XÎ½, Î½ âˆˆZ

of autocovariance KXX is of PSD SXX, then,
except on a subset of (âˆ’1/2, 1/2) of Lebesgue measure zero,
SXX(Î¸) â‰¥0
(13.18)
and
SXX(Î¸) = SXX(âˆ’Î¸).
(13.19)
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,

13.6 The Power Spectral Density Function
225
(ii) If the function S: [âˆ’1/2, 1/2) â†’R is integrable, nonnegative, and symmetric
(in the sense that S(Î¸) = S(âˆ’Î¸) for all Î¸ âˆˆ(âˆ’1/2, 1/2)), then there exists a
WSS SP

XÎ½

whose PSD SXX is given by
SXX(Î¸) = S(Î¸),
Î¸ âˆˆ[âˆ’1/2, 1/2).
Proof. The nonnegativity of the PSD (13.18) will be established later in the more
general setting of complex stochastic processes (Proposition 17.5.7 ahead). Here we
only prove the symmetry (13.19) and establish the second half of the proposition.
That (13.19) holds (except on a set of Lebesgue measure zero) follows because KXX
is symmetric. Indeed, for any Î· âˆˆZ we have
 1/2
âˆ’1/2

SXX(Î¸) âˆ’SXX(âˆ’Î¸)

ei2Ï€Î·Î¸ dÎ¸
=
 1/2
âˆ’1/2
SXX(Î¸) ei2Ï€Î·Î¸ dÎ¸ âˆ’
 1/2
âˆ’1/2
SXX(âˆ’Î¸) ei2Ï€Î·Î¸ dÎ¸
= KXX(Î·) âˆ’
 1/2
âˆ’1/2
SXX(ËœÎ¸) ei2Ï€(âˆ’Î·)ËœÎ¸ dËœÎ¸
= KXX(Î·) âˆ’KXX(âˆ’Î·)
= 0,
Î· âˆˆZ,
(13.20)
so all the Fourier Series Coeï¬ƒcients of the function Î¸ 	â†’SXX(Î¸)âˆ’SXX(âˆ’Î¸) are zero,
thus establishing that this function is zero except on a set of Lebesgue measure
zero (Theorem A.2.3).
We next prove that if the function S: [âˆ’1/2, 1/2) â†’R is symmetric, nonnegative,
and integrable, then it is the PSD of some WSS real SP. We cheat a bit because
our proof relies on Theorem 13.5.2, which we never proved. From Theorem 13.5.2
it follows that it suï¬ƒces to establish that the sequence K: Z â†’R deï¬ned by
K(Î·) =
 1/2
âˆ’1/2
S(Î¸) ei2Ï€Î·Î¸ dÎ¸,
Î· âˆˆZ
(13.21)
satisï¬es (13.14) & (13.15).
Verifying (13.14) is straightforward: by hypothesis, S(Â·) is symmetric so
K(âˆ’Î·) =
 1/2
âˆ’1/2
S(Î¸) ei2Ï€(âˆ’Î·)Î¸ dÎ¸
=
 1/2
âˆ’1/2
S(âˆ’ËœÎ¸) ei2Ï€Î·ËœÎ¸ dËœÎ¸
=
 1/2
âˆ’1/2
S(ËœÎ¸) ei2Ï€Î·ËœÎ¸ dËœÎ¸
= K(Î·),
Î· âˆˆZ,
where the ï¬rst equality follows from (13.21); the second from the change of variable
ËœÎ¸ â‰œâˆ’Î¸; the third from the symmetry of S(Â·), which implies that S(âˆ’ËœÎ¸) = S(ËœÎ¸);
and the last equality again from (13.21).
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,
www.ebook3000.com

226
Stationary Discrete-Time Stochastic Processes
We next verify (13.15). To this end we ï¬x arbitrary Î±1, . . . , Î±n âˆˆR and compute
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±Î½â€² K(Î½ âˆ’Î½â€²) =
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±Î½â€²
 1/2
âˆ’1/2
S(Î¸) ei2Ï€(Î½âˆ’Î½â€²)Î¸ dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)
	
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±Î½â€² ei2Ï€(Î½âˆ’Î½â€²)Î¸

dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)
	
n

Î½=1
n

Î½â€²=1
Î±Î½ ei2Ï€Î½Î¸ Î±Î½â€² eâˆ’i2Ï€Î½â€²Î¸

dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)
	
n

Î½=1
Î±Î½ ei2Ï€Î½Î¸

	
n

Î½â€²=1
Î±Î½â€² ei2Ï€Î½â€²Î¸

âˆ—
dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)

n

Î½=1
Î±Î½ ei2Ï€Î½Î¸

2
dÎ¸
â‰¥0,
(13.22)
where the ï¬rst equality follows from (13.21); the subsequent equalities by simple
algebraic manipulation; and the ï¬nal inequality from the nonnegativity of S(Â·).
Corollary 13.6.4. If a discrete-time WSS SP

XÎ½

has a PSD, then it also has a
PSD SXX for which (13.18) holds for every Î¸ âˆˆ[âˆ’1/2, 1/2) and for which (13.19)
holds for every Î¸ âˆˆ(âˆ’1/2, 1/2) (and not only outside a subset of Lebesgue measure
zero).
Proof. Suppose that

XÎ½

is of PSD SXX. Deï¬ne the mapping S: [âˆ’1/2, 1/2) â†’R
by3
S(Î¸) =

1
2

|SXX(Î¸)| + |SXX(âˆ’Î¸)|

if Î¸ âˆˆ(âˆ’1/2, 1/2)
1
if Î¸ = âˆ’1/2.
(13.23)
By the proposition, SXX(Â·) and S(Â·) diï¬€er only on a set of Lebesgue measure zero,
so they must have identical Fourier Series Coeï¬ƒcients. Since the Fourier Series
Coeï¬ƒcients of SXX agree with KXX, it follows that so must those of S(Â·). Thus, S(Â·)
is a PSD for

XÎ½

, and it is by (13.23) nonnegative on [âˆ’1/2, 1/2) and symmetric
on (âˆ’1/2, 1/2).
Note 13.6.5. In view of Corollary 13.6.4 we shall only say that

XÎ½

is of PSD SXX
if the function SXXâ€”in addition to being integrable and to satisfying (13.16)â€”is
also nonnegative and symmetric.
As we have noted, not every WSS SP has a PSD. For example, the SP
XÎ½ = X,
Î½ âˆˆZ,
where X is some zero-mean unit-variance RV has the all-one autocovariance func-
tion KXX(Î·) = 1, Î· âˆˆZ, and this all-one sequence cannot be the Fourier Series
Coeï¬ƒcients sequence of an integrable function because, by the Riemann-Lebesgue
3Our choice of S(âˆ’1/2) as 1 is arbitrary; any nonnegative value would do.
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,

13.6 The Power Spectral Density Function
227
lemma (Theorem A.2.4), the Fourier Series Coeï¬ƒcients of an integrable function
must converge to zero.4
In general, it is very diï¬ƒcult to characterize the autocovariance functions having
a PSD. We know by the Riemann-Lebesgue lemma that such autocovariance func-
tions must tend to zero, but this necessary condition is not suï¬ƒcient. A very useful
suï¬ƒcient (but not necessary) condition is the following:
Proposition 13.6.6 (PSD when KXX Is Absolutely Summable). If the autoco-
variance function KXX is absolutely summable, i.e.,
âˆ

Î·=âˆ’âˆ
KXX(Î·)
 < âˆ,
(13.24)
then its Discrete-Time Fourier Transform
S(Î¸) =
âˆ

Î·=âˆ’âˆ
KXX(Î·) eâˆ’i2Ï€Î·Î¸,
Î¸ âˆˆ[âˆ’1/2, 1/2]
(13.25)
is continuous, symmetric, nonnegative, and satisï¬es
 1/2
âˆ’1/2
S(Î¸) ei2Ï€Î·Î¸ dÎ¸ = KXX(Î·),
Î· âˆˆZ.
(13.26)
Consequently, S(Â·) is a PSD for KXX.
Proof. First note that because |KXX(Î·) eâˆ’i2Ï€Î¸Î·| = |KXX(Î·)|, it follows that (13.24)
guarantees that the sum in (13.25) converges uniformly and absolutely. And since
each term in the sum is a continuous function, the uniform convergence of the
sum guarantees that S(Â·) is continuous (Rudin, 1976, Chapter 7, Theorem 7.12).
Consequently,
 1/2
âˆ’1/2
|S(Î¸)| dÎ¸ < âˆ,
(13.27)
and it is meaningful to discuss the Fourier Series Coeï¬ƒcients of S(Â·).
We next prove that the Fourier Series Coeï¬ƒcients of S(Â·) are equal to KXX, i.e.,
that (13.26) holds. This can be shown by swapping integration and summation
and using the orthonormality property
 1/2
âˆ’1/2
ei2Ï€(Î·âˆ’Î·â€²)Î¸ dÎ¸ = I{Î· = Î·â€²},
Î·, Î·â€² âˆˆZ
(13.28)
4One could say that the PSD of this process is Diracâ€™s Delta, but we shall refrain from doing
so because we do not use Diracâ€™s Delta in this book and because there is not much to be gained
from this. (There exist processes that do not have a PSD even if one allows for Diracâ€™s Deltas.)
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,
www.ebook3000.com

228
Stationary Discrete-Time Stochastic Processes
as follows:
 1/2
âˆ’1/2
S(Î¸) ei2Ï€Î·Î¸ dÎ¸ =
 1/2
âˆ’1/2
	
âˆ

Î·â€²=âˆ’âˆ
KXX(Î·â€²) eâˆ’i2Ï€Î·â€²Î¸

ei2Ï€Î·Î¸ dÎ¸
=
âˆ

Î·â€²=âˆ’âˆ
KXX(Î·â€²)
 1/2
âˆ’1/2
eâˆ’i2Ï€Î·â€²Î¸ ei2Ï€Î·Î¸ dÎ¸
=
âˆ

Î·â€²=âˆ’âˆ
KXX(Î·â€²)
 1/2
âˆ’1/2
ei2Ï€(Î·âˆ’Î·â€²)Î¸ dÎ¸
=
âˆ

Î·â€²=âˆ’âˆ
KXX(Î·â€²) I{Î· = Î·â€²}
= KXX(Î·),
Î· âˆˆZ.
It remains to show that S(Â·) is symmetric, i.e., that S(Î¸) = S(âˆ’Î¸), and that it is
nonnegative. The symmetry of S(Â·) follows directly from its deï¬nition (13.25) and
from the fact that KXX, like every autocovariance function, is symmetric (Theo-
rem 13.5.2 (i)).
We next prove that S(Â·) is nonnegative.
From (13.26) it follows that S(Â·) can
only be negative on a subset of the interval [âˆ’1/2, 1/2) of Lebesgue measure zero
(Proposition 13.6.3 (i)).
And since S(Â·) is continuous, this implies that S(Â·) is
nonnegative.
13.7
The Spectral Distribution Function
We next brieï¬‚y discuss the case where (XÎ½) does not necessarily have a power
spectral density function. We shall see that in this case too we can express the
autocovariance function as the Fourier Series of â€œsomething,â€ but this â€œsomethingâ€
is not an integrable function. (It is, in fact, a measure.) The theorem will also
yield a characterization of positive deï¬nite functions. The proof, which is based on
Herglotzâ€™s Theorem (Feller, 1971, Chapter XIX, Section 6, Theorem 2), (Pourah-
madi, 2001, Theorem 9.22), is omitted. The results of this section will not be used
in subsequent chapters.
Recall that a random variable taking values in the interval [âˆ’Î±, Î±] is said to be
symmetric (or to have a symmetric distribution) if Pr[X â‰¤âˆ’Î¾] = Pr[X â‰¥Î¾] for
all Î¾ âˆˆ[âˆ’Î±, Î±].
Theorem 13.7.1. A function Ï: Z â†’R is the autocorrelation function of a real
WSS SP if, and only if, there exists a symmetric random variable Î˜ taking values
in the interval [âˆ’1/2, 1/2] such that
Ï(Î·) = E

ei2Ï€Î·Î˜
,
Î· âˆˆZ.
(13.29)
The cumulative distribution function of Î˜ is fully determined by Ï.
Proof. See (Doob, 1990, Chapter X, Â§ 3, Theorem 3.2), (Pourahmadi, 2001, The-
orem 9.22), (Shiryaev, 1996, Chapter VI, Â§ 1.1), or (Porat, 2008, Section 2.8).
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,

13.8 Exercises
229
This theorem also characterizes autocovariance functions: a function K: Z â†’R
is the autocovariance function of a real WSS SP if, and only if, there exists a
symmetric random variable Î˜ taking values in the interval [âˆ’1/2, 1/2] and some
constant Î± â‰¥0 such that
K(Î·) = Î± E

ei2Ï€Î·Î˜
,
Î· âˆˆZ.
(13.30)
(By equating (13.30) at Î· = 0 we obtain that Î± = K(0), i.e., the variance of the
stochastic process.)
Equivalently, we can state the theorem as follows. If

XÎ½

is a real WSS SP, then
its autocovariance function KXX can be expressed as
KXX(Î·) = Var[X1] E

ei2Ï€Î·Î˜
,
Î· âˆˆZ
(13.31)
for some random variable Î˜ taking values in the interval [âˆ’1/2, 1/2] according to
some symmetric distribution. If, additionally, Var[X1] > 0, then the cumulative
distribution function FÎ˜(Â·) of Î˜ is uniquely determined by KXX.
Note 13.7.2.
(i) If the random variable Î˜ above has a symmetric density fÎ˜(Â·), then the
process is of PSD Î¸ 	â†’Var[X1] fÎ˜(Î¸). Indeed, by (13.31) we have for every
integer Î·
KXX(Î·) = Var[X1] E

ei2Ï€Î·Î˜
= Var[X1]
 1/2
âˆ’1/2
fÎ˜(Î¸) ei2Ï€Î·Î¸ dÎ¸
=
 1/2
âˆ’1/2

Var[X1] fÎ˜(Î¸)

ei2Ï€Î·Î¸ dÎ¸.
(ii) Some authors, e.g., (Grimmett and Stirzaker, 2001) refer to the cumulative
distribution function FÎ˜(Â·) of Î˜, i.e., to the mapping Î¸ 	â†’Pr[Î˜ â‰¤Î¸], as
the Spectral Distribution Function of

XÎ½

. This, however, is not stan-
dard. It is only in agreement with the more common usage in the case where
Var[X1] = 1.5
13.8
Exercises
Exercise 13.1 (Discrete-Time WSS Stochastic Processes). Prove Proposition 13.3.3.
Exercise 13.2 (Empirical Averages). Let

XÎ½

be a WSS discrete-time SP of autocovari-
ance function KXX.
5The more common deï¬nition is that Î¸ â†’Var[X1] Pr[Î˜ â‰¤Î¸] is the spectral measure or
spectral distribution function. But this is not a distribution function in the probabilistic sense
because its value at Î¸ = âˆis Var[X1] which may be diï¬€erent from one.
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,
www.ebook3000.com

230
Stationary Discrete-Time Stochastic Processes
(i) Prove that
Var
' 1
n
n

Î½=1
XÎ½
(
= 1
n KXX(0) + 1
n
n

m=1
2
	
1 âˆ’m
n

KXX(m).
(ii) Prove that
)
lim
Î·â†’âˆKXX(Î·) = 0
*
=â‡’
)
lim
nâ†’âˆVar
' 1
n
n

Î½=1
XÎ½
(
= 0
*
.
Hint: For Part (ii) recall that if a sequence (aj) converges to a as j tends to inï¬nity, then
nâˆ’1 n
k=1 ak tends to a as n tends to inï¬nity (Rudin, 1976, Ch. 3 Exercise 14).
Exercise 13.3 (Mapping a Discrete-Time Stationary SP). Let

XÎ½

be a stationary
discrete-time SP, and let g: R â†’R be some arbitrary (Borel measurable) function. For
every Î½ âˆˆZ, let YÎ½ = g(XÎ½). Prove that the discrete-time SP

YÎ½

is stationary.
Exercise 13.4 (Mapping a Discrete-Time WSS SP). Let

XÎ½

be a WSS discrete-time
SP, and let g: R â†’R be some arbitrary (Borel measurable) bounded function. For every
Î½ âˆˆZ, let YÎ½ = g(XÎ½). Must the SP

YÎ½

be WSS?
Exercise 13.5 (A Sliding-Window Mapping of a Stationary SP). Let

XÎ½

be a stationary
discrete-time SP, and let g: R2 â†’R be some arbitrary (Borel measurable) function. For
every Î½ âˆˆZ deï¬ne YÎ½ = g(XÎ½âˆ’1, XÎ½). Must

YÎ½

be stationary?
Exercise 13.6 (A Sliding-Window Mapping of a WSS SP). Let

XÎ½

be a WSS discrete-
time SP, and let g: R2 â†’R be some arbitrary bounded (Borel measurable) function. For
every Î½ âˆˆZ deï¬ne YÎ½ = g(XÎ½âˆ’1, XÎ½). Must

YÎ½

be WSS?
Exercise 13.7 (Existence of a SP). For which values of Î±, Î² âˆˆR is the function
KXX(m) =
â§
âª
âª
âª
â¨
âª
âª
âª
â©
1
if m = 0,
Î±
if m = 1,
Î²
if m = âˆ’1,
0
otherwise,
m âˆˆZ
the autocovariance function of some WSS SP

XÎ½, Î½ âˆˆZ

?
Exercise 13.8 (Dilating a Stationary SP). Let

XÎ½

be a stationary discrete-time SP, and
deï¬ne YÎ½ = X2Î½ for every Î½ âˆˆZ. Must

YÎ½

be stationary?
Exercise 13.9 (Inserting Zeros Periodically). Let

XÎ½

be a stationary discrete-time SP,
and let the RV U be independent of it and take on the values 0 and 1 equiprobably. Deï¬ne
for every Î½ âˆˆZ
YÎ½ =

0
if Î½ is odd,
XÎ½/2
if Î½ is even
and
ZÎ½ = YÎ½+U.
(13.32)
Under what conditions is

YÎ½

stationary? Under what conditions is

ZÎ½

stationary?
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,

13.8 Exercises
231
Exercise 13.10 (The Autocovariance Function of a Dilated WSS SP). Let

XÎ½

be a
WSS discrete-time SP of autocovariance function KXX. Deï¬ne YÎ½ = X2Î½ for every Î½ âˆˆZ.
Must

YÎ½

be WSS? If so, express its autocovariance function KYY in terms of KXX.
Exercise 13.11 (Inserting Zeros Periodically: the Autocovariance Function). Let

XÎ½

be
a WSS discrete-time SP of autocovariance function KXX, and let the RV U be independent
of it and take on the values 0 and 1 equiprobably. Deï¬ne

ZÎ½

as in (13.32). Must

ZÎ½

be WSS? If yes, express its autocovariance function in terms of KXX.
Exercise 13.12 (Stationary But Not WSS). Construct a discrete-time stationary SP that
is not WSS.
Exercise 13.13 (Complex Coeï¬ƒcients). Show that (13.13) will hold for complex numbers
Î±1, . . . , Î±n provided that we replace the product Î±Î½ Î±Î½â€² with Î±Î½ Î±âˆ—
Î½â€². That is, show that
if KXX is the autocovariance function of a real discrete-time WSS SP, then
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±âˆ—
Î½â€² KXX(Î½ âˆ’Î½â€²) â‰¥0,
Î±1, . . . , Î±n âˆˆC.
Exercise 13.14 (Existence of a Power Spectral Density). Does there exist a WSS SP

XÎ½, Î½ âˆˆZ

whose autocovariance function KXX is given by KXX(Î·) = (âˆ’1)Î· for all
Î· âˆˆZ? If yes, does it have a power spectral density?
Hint: Recall the Riemann-Lebesgue Lemma (Theorem A.2.4).
available at 
.015
14:31:30, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 14
Energy and Power in PAM
14.1
Introduction
Energy is an important resource in Digital Communications. The rate at which
it is transmittedâ€”the â€œtransmit powerâ€â€”is critical in battery-operated devices.
In satellite applications it is a major consideration in determining the size of the
required solar panels, and in wireless systems it inï¬‚uences the interference that one
system causes to another. In this chapter we shall discuss the power in PAM signals.
To deï¬ne power we shall need some modeling trickery which will allow us to pretend
that the system has been operating since â€œtime âˆ’âˆâ€ and that it will continue
to operate indeï¬nitely.
Our deï¬nitions and derivations will be mathematically
somewhat informal. A more formal account for readers with background in Measure
Theory is provided in Section 14.6.
Before discussing power we begin with a discussion of the expected energy in trans-
mitting a ï¬nite number of bits.
14.2
Energy in PAM
We begin with a seemingly completely artiï¬cial problem. Suppose that K inde-
pendent data bits D1, . . . , DK, each taking on the values 0 and 1 equiprobably,
are mapped by a mapping enc: {0, 1}K â†’RN to an N-tuple of real numbers
(X1, . . . , XN), where Xâ„“is the â„“-th component of the N-tuple enc

D1, . . . , DK

.
Suppose further that the symbols X1, . . . , XN are then mapped to the waveform
X(t) = A
N

â„“=1
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR,
(14.1)
where g âˆˆL2 is an energy-limited real pulse shape, A â‰¥0 is a scaling factor, and
Ts > 0 is the baud period. We seek the expected energy in the waveform X(Â·).
We assume that X(Â·) corresponds to the voltage across a unit-load or to the current
through a unit-load, so the transmitted energy is the time integral of the mapping
t 	â†’X2(t).
Because the data bits are random variables, the signal X(Â·) is a
232
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.2 Energy in PAM
233
stochastic process. Its energy
 âˆ
âˆ’âˆX2(t) dt is thus a random variable.1 If (Î©, F, P)
is the probability space under consideration, then this RV is the mapping from Î©
to R deï¬ned by
Ï‰ 	â†’
 âˆ
âˆ’âˆ
X2(Ï‰, t) dt.
This RVâ€™s expectationâ€”the expected energyâ€”is denoted by E and is given by
E â‰œE
 âˆ
âˆ’âˆ
X2(t) dt

.
(14.2)
Note that even though we are considering the transmission of a ï¬nite number of
symbols (N), the waveform X(Â·) may extend in time from âˆ’âˆto +âˆ.
We next derive an explicit expression for E. Starting from (14.2) and using (14.1),
E = E
 âˆ
âˆ’âˆ
X2(t) dt

= A2E
5 âˆ
âˆ’âˆ
	 N

â„“=1
Xâ„“g(t âˆ’â„“Ts)

2
dt
6
= A2E
5 âˆ
âˆ’âˆ
	 N

â„“=1
Xâ„“g(t âˆ’â„“Ts)

	
N

â„“â€²=1
Xâ„“â€² g(t âˆ’â„“â€²Ts)

dt
6
= A2E
5 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
Xâ„“Xâ„“â€² g(t âˆ’â„“Ts) g(t âˆ’â„“â€²Ts) dt
6
= A2
 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] g(t âˆ’â„“Ts) g(t âˆ’â„“â€²Ts) dt
= A2
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²]
 âˆ
âˆ’âˆ
g(t âˆ’â„“Ts) g(t âˆ’â„“â€²Ts) dt
= A2
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] Rgg

(â„“âˆ’â„“â€²)Ts

,
(14.3)
where Rgg is the self-similarity function of the pulse g(Â·) (Section 11.2). Here the
ï¬rst equality follows from (14.2); the second from (14.1); the third by writing the
square of a number as its product with itself (Î¾2 = Î¾Î¾); the fourth by writing the
product of sums as the double sum of products; the ï¬fth by swapping expectation
with integration and by the linearity of expectation; the sixth by swapping integra-
tion and summation; and the ï¬nal equality by the deï¬nition of the self-similarity
function (Deï¬nition 11.2.1).
Using Proposition 11.2.2 (iv) we can also express Rgg as
Rgg(Ï„) =
 âˆ
âˆ’âˆ
Ë†g(f)
2 ei2Ï€fÏ„ df,
Ï„ âˆˆR
(14.4)
1There are some slight measure-theoretic mathematical technicalities that we are sweeping
under the rug. Those are resolved in Section 14.6.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

234
Energy and Power in PAM
and hence rewrite (14.3) as
E = A2
 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] ei2Ï€f(â„“âˆ’â„“â€²)TsË†g(f)
2 df.
(14.5)
We deï¬ne the energy per bit as
Eb
%energy
bit
&
â‰œE
K
(14.6)
and the energy per real symbol as
Es

energy
real symbol

â‰œE
N.
(14.7)
As we shall see in Section 14.5.2, if inï¬nite data are transmitted using the binary-
to-reals (K, N) block encoder enc(Â·), then the resulting transmitted power P is given
by
P = Es
Ts
.
(14.8)
This result will be proved in Section 14.5.2 after we carefully deï¬ne the average
power. The units work out because if we think of Ts as having units of seconds per
real symbol then:
Es
%
energy
real symbol
&
Ts
%
second
real symbol
& = Es
Ts
%energy
second
&
.
(14.9)
Expression (14.3) for the expected energy E is greatly simpliï¬ed in two cases that
we discuss next. The ï¬rst is when the pulse shape g satisï¬es the orthogonality
condition
 âˆ
âˆ’âˆ
g(t) g(t âˆ’ÎºTs) dt = âˆ¥gâˆ¥2
2 I{Îº = 0},
Îº âˆˆ{0, 1, . . . , N âˆ’1}.
(14.10)
In this case (14.3) simpliï¬es to
E = A2 âˆ¥gâˆ¥2
2
N

â„“=1
E

X2
â„“

,

t 	â†’g(t âˆ’â„“Ts)
Nâˆ’1
â„“=0 orthogonal

.
(14.11)
(In this case one need not even go through the calculation leading to (14.3); the
result simply follows from (14.1) and the Pythagorean Theorem (Theorem 4.5.2).)
The second case for which the computation of E is simpliï¬ed is when the distribu-
tion of D1, . . . , DK and the mapping enc(Â·) result in the real symbols X1, . . . , XN
being of zero mean and uncorrelated:2
E[Xâ„“] = 0,
â„“âˆˆ{1, . . . , N}
(14.12a)
2Actually, it suï¬ƒces that (14.12b) hold; (14.12a) is not needed.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.3 Deï¬ning the Power in PAM
235
and
E[Xâ„“Xâ„“â€²] = E

X2
â„“

I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , N}.
(14.12b)
In this case too (14.3) simpliï¬es to
E = A2 âˆ¥gâˆ¥2
2
N

â„“=1
E

X2
â„“

,

Xâ„“, â„“âˆˆZ

zero-mean & uncorrelated

.
(14.13)
14.3
Deï¬ning the Power in PAM
If

X(t), t âˆˆR

is a continuous-time stochastic process describing the voltage
across a unit-load or the current through a unit-load, then it is reasonable to
deï¬ne the power P in

X(t), t âˆˆR

as the limit
P â‰œlim
Tâ†’âˆ
1
2T E
 T
âˆ’T
X2(t) dt

.
(14.14)
But there is a problem. Over its lifetime, a communication system is only used
to transmit a ï¬nite number of bits, and it only sends a ï¬nite amount of energy.
Consequently, if

X(t), t âˆˆR

corresponds to the transmitted waveform over the
systemâ€™s lifetime, then P as deï¬ned in (14.14) will always end up being zero. The
deï¬nition in (14.14) is thus useless when discussing the transmission of a ï¬nite
number of bits.
To deï¬ne power in a useful way we need some modeling trickery. Instead of thinking
about the encoder as producing a ï¬nite number of symbols, we should now pretend
that the encoder produces an inï¬nite sequence of symbols

Xâ„“, â„“âˆˆZ

, which are
then mapped to the inï¬nite sum
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR.
(14.15)
For the waveform in (14.15), the deï¬nition of P in (14.14) makes perfect sense.
Philosophically speaking, the modeling trickery we employ corresponds to mea-
suring power on a time scale much greater than the signaling period Ts but much
shorter than the systemâ€™s lifetime.
But philosophy aside, there are still two problems we must address: how to model
the generation of the inï¬nite sequence

Xâ„“, â„“âˆˆZ

, and how to guarantee that
the sum in (14.15) converges for every t âˆˆR. We begin with the latter. If g is of
ï¬nite duration, then at every epoch t âˆˆR only a ï¬nite number of terms in (14.15)
are nonzero and convergence is thus guaranteed. But we do not want to restrict
ourselves to ï¬nite-duration pulse shapes because those, by Theorem 6.8.2, cannot
be bandlimited. Instead, to guarantee convergence, we shall assume throughout
that the following conditions both hold:
1) The symbols

Xâ„“, â„“âˆˆZ

are uniformly bounded in the sense that there
exists some constant Î³ such that
|Xâ„“| â‰¤Î³,
â„“âˆˆZ.
(14.16)
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

236
Energy and Power in PAM
Dâˆ’K+1,
. . .
, D0,
enc(Â·)
, Xâˆ’N+1,
. . .
, X0,
enc(Dâˆ’K+1, . . . , D0)
D1, . . .
, DK,
enc(Â·)
X1,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
Â· Â· Â· , D2K
enc(Â·)
XN+1,
Â· Â· Â· , X2N,
enc(DK+1, . . . , D2K)
Figure 14.1: Bi-Inï¬nite Block Encoding.
2) The pulse shape t 	â†’g(t) decays faster than 1/t in the sense that there exist
positive constants Î±, Î² > 0 such that
|g(t)| â‰¤
Î²
1 + |t/Ts|1+Î± ,
t âˆˆR.
(14.17)
Using the fact that the sum 
nâ‰¥1 nâˆ’(1+Î±) converges whenever Î± > 0 (Rudin,
1976, Theorem 3.28), it is not diï¬ƒcult to show that if both (14.16) and (14.17)
hold, then the inï¬nite sum (14.15) converges at every epoch t âˆˆR.
As to the generation of

Xâ„“, â„“âˆˆZ

, we shall consider three scenarios. In the
ï¬rst, which we analyze in Section 14.5.1, we ignore this issue and simply assume
that

Xâ„“, â„“âˆˆZ

is a WSS discrete-time SP of a given autocovariance function.
In the second scenario, which we analyze in Section 14.5.2, we tweak the block-
encoding mode that we introduced in Section 10.4 to account for a bi-inï¬nite data
sequence. We call this tweaked mode bi-inï¬nite block encoding and describe
it more precisely in Section 14.5.2. It is illustrated in Figure 14.1. Finally, the
third scenario, which we analyze in Section 14.5.3, is similar to the ï¬rst except
that we relax some of the statistical assumptions on

Xâ„“, â„“âˆˆZ

. But we only
treat the case where the time shifts of the pulse shape by integer multiples of Ts
are orthonormal.
Except in the third scenario, we shall only analyze the power in the stochastic
process (14.15) assuming that the symbols

Xâ„“, â„“âˆˆZ

are of zero mean
E[Xâ„“] = 0,
â„“âˆˆZ.
(14.18)
This not only simpliï¬es the analysis but also makes engineering sense, because it
guarantees that

X(t), t âˆˆR

is centered
E[X(t)] = 0,
t âˆˆR,
(14.19)
and, for the reasons that we outline in Section 14.4, transmitting zero-mean wave-
forms is usually power eï¬ƒcient.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.4 On the Mean of Transmitted Waveforms
237
+
+
+
TX1
TX2
RX1
RX2
X
X âˆ’c
Y = X âˆ’c + N
X + N
{Dj}
{Dest
j }
âˆ’c
c
N
TX1
RX1
X
Y = X + N
+
N
{Dj}
{Dest
j }
Figure 14.2: The above two systems have identical performance. In the former
the transmitted power is the power in t 	â†’X(t) whereas in the second it is the
power in t 	â†’X(t) âˆ’c(t).
14.4
On the Mean of Transmitted Waveforms
We next explain why the transmitted waveforms in digital communications are
usually designed to be of zero mean.3 We focus on the case where the transmitted
signal suï¬€ers only from an additive disturbance. The key observation is that given
any transmitter that transmits the SP

X(t), t âˆˆR

and any receiver, we can
design a new transmitter that transmits the waveform t 	â†’X(t) âˆ’c(t) and a
new receiver with identical performance.
Here c(Â·) is any deterministic signal.
Indeed, the new receiver can simply add c(Â·) to the received signal and then pass
on the result to the old receiver. That the old and the new systems have identical
performance follows by noting that if

N(t), t âˆˆR

is the added disturbance, then
the received signal on which the old receiver operates is given by t 	â†’X(t) + N(t).
And the received signal in the new system is t 	â†’X(t) âˆ’c(t) + N(t), so after we
add c(Â·) to this signal we obtain the signal X(t) + N(t), which is equal the signal
that the old receiver operated on. Thus, the performance of a system transmitting
X(Â·) can be mimicked on a system transmitting X(Â·) âˆ’c(Â·) by simply adding c(Â·)
at the receiver. See Figure 14.2.
The addition at the receiver of c(Â·) entails no change in the transmitted power.
Therefore, if a system transmits X(Â·), then we might be able to improve its power
eï¬ƒciency without hurting its performance by cleverly choosing c(Â·) so that the
power in X(Â·) âˆ’c(Â·) be smaller than the power in X(Â·) and by then transmitting
t 	â†’X(t) âˆ’c(t) instead of t 	â†’X(t). The only additional change we would need
to make is to add c(Â·) at the receiver.
How should we choose c(Â·)? To answer this we shall need the following lemma.
3This, however, is not the case with some wireless systems that transmit training sequences
to help the receiver learn the channel and acquire timing information.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

238
Energy and Power in PAM
Lemma 14.4.1. If W is a random variable of ï¬nite variance, then
E

(W âˆ’c)2
â‰¥Var[W] ,
c âˆˆR
(14.20)
with equality if, and only if,
c = E[W] .
(14.21)
Proof.
E

(W âˆ’c)2
= E
%
(W âˆ’E[W]) + (E[W] âˆ’c)
2&
= E

(W âˆ’E[W])2
+ 2 E

W âˆ’E[W]




0

E[W] âˆ’c

+

E[W] âˆ’c
2
= E

(W âˆ’E[W])2
+ (E[W] âˆ’c)2
â‰¥E

(W âˆ’E[W])2
= Var[W] ,
with equality if, and only if, c = E[W].
With the aid of Lemma 14.4.1 we can now choose c(Â·) to minimize the power in
t 	â†’X(t) âˆ’c(t) as follows. Keeping the deï¬nition of power (14.14) in mind, we
study
1
2T
 T
âˆ’T
E
%
X(t) âˆ’c(t)
2&
dt
and note that this expression is minimized over all choices of the waveform c(Â·) by
minimizing the integrand, i.e., by choosing at every epoch t the value of c(t) to be
the one that minimizes E
%
X(t) âˆ’c(t)
2&
. By Lemma 14.4.1 this corresponds to
choosing c(t) to be E[X(t)]. It is thus optimal to choose c(Â·) as
c(t) = E[X(t)] ,
t âˆˆR.
(14.22)
This choice results in the transmitted waveform being t 	â†’X(t) âˆ’E[X(t)], i.e., in
the transmitted waveform being of zero mean.
Stated diï¬€erently, if in a given system the transmitted waveform is not of zero
mean, then a new system can be built that transmits a waveform of lower (or
equal) average power and whose performance on any additive noise channel is
identical.
14.5
Computing the Power in PAM
We proceed to compute the power in the signal
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR
(14.23)
under various assumptions on the bi-inï¬nite random sequence

Xâ„“, â„“âˆˆZ

. We
assume throughout that Conditions (14.16) & (14.17) are satisï¬ed so the inï¬nite
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.5 Computing the Power in PAM
239
sum in (14.23) converges at every epoch t âˆˆR.
The power P is deï¬ned as in
(14.14).4
14.5.1

Xâ„“

Is Zero-Mean and WSS
Here we compute the power in the signal (14.23) when

Xâ„“, â„“âˆˆZ

is a centered
WSS SP of autocovariance function KXX:
E[Xâ„“] = 0,
â„“âˆˆZ,
(14.24a)
E[Xâ„“Xâ„“+m] = KXX(m) ,
â„“, m âˆˆZ.
(14.24b)
We further assume that the pulse shape satisï¬es the decay condition (14.17) and
that the process

Xâ„“, â„“âˆˆZ

satisï¬es the boundedness condition (14.16).
We begin by calculating the expected energy of X(Â·) in a half-open interval [Ï„, Ï„+Ts)
of length Ts and in showing that this expected energy does not depend on Ï„, i.e.,
that the expected energy in all intervals of length Ts are identical. We calculate
the energy in the interval [Ï„, Ï„ + Ts) as follows:
E
 Ï„+Ts
Ï„
X2(t) dt

= A2
 Ï„+Ts
Ï„
E
5	
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts)

26
dt
(14.25)
= A2
 Ï„+Ts
Ï„
E

âˆ

â„“=âˆ’âˆ
âˆ

â„“â€²=âˆ’âˆ
Xâ„“Xâ„“â€² g(t âˆ’â„“Ts) g(t âˆ’â„“â€²Ts)

dt
= A2
 Ï„+Ts
Ï„
âˆ

â„“=âˆ’âˆ
âˆ

â„“â€²=âˆ’âˆ
E[Xâ„“Xâ„“â€²] g(t âˆ’â„“Ts) g(t âˆ’â„“â€²Ts) dt
= A2
 Ï„+Ts
Ï„
âˆ

â„“=âˆ’âˆ
âˆ

m=âˆ’âˆ
E[Xâ„“Xâ„“+m] g(t âˆ’â„“Ts) g

t âˆ’(â„“+ m)Ts

dt
= A2
 Ï„+Ts
Ï„
âˆ

m=âˆ’âˆ
KXX(m)
âˆ

â„“=âˆ’âˆ
g(t âˆ’â„“Ts) g

t âˆ’(â„“+ m)Ts

dt
= A2
âˆ

m=âˆ’âˆ
KXX(m)
âˆ

â„“=âˆ’âˆ
 Ï„+Tsâˆ’â„“Ts
Ï„âˆ’â„“Ts
g(tâ€²) g(tâ€² âˆ’mTs) dtâ€²
(14.26)
= A2
âˆ

m=âˆ’âˆ
KXX(m)
 âˆ
âˆ’âˆ
g(tâ€²) g(tâ€² âˆ’mTs) dtâ€²
= A2
âˆ

m=âˆ’âˆ
KXX(m) Rgg(mTs),
Ï„ âˆˆR,
(14.27)
where the ï¬rst equality follows by the structure of X(Â·) (14.15); the second by
writing X2(t) as X(t) X(t) and rearranging terms; the third by the linearity of the
4A general mathematical deï¬nition of the power of a stochastic process is given in Deï¬ni-
tion 14.6.1 ahead.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

240
Energy and Power in PAM
expectation, which allows us to swap the double sum and the expectation and to
take the deterministic term g(tâˆ’â„“Ts)g(tâˆ’â„“â€²Ts) outside the expectation; the fourth
by deï¬ning m â‰œâ„“â€² âˆ’â„“; the ï¬fth by (14.24b); the sixth by deï¬ning tâ€² â‰œt âˆ’â„“Ts; the
seventh by noting that the integrals of a function over all the intervals of the form
[Ï„ âˆ’â„“Ts, Ï„ âˆ’â„“Ts + Ts) where â„“âˆˆZ sum to the integral over the entire real line; and
the ï¬nal by the deï¬nition of the self-similarity function Rgg (Section 11.2).
Note that, indeed, the RHS of (14.27) does not depend on the epoch Ï„ at which
the length-Ts time interval starts. This observation will now help us to compute
the power in X(Â·). Since the interval [âˆ’T, +T) contains âŒŠ(2T)/TsâŒ‹disjoint intervals
of the form [Ï„, Ï„ + Ts), and since it is contained in the union of âŒˆ(2T)/TsâŒ‰such
intervals, it follows that
72T
Ts
8
E
 Ï„+Ts
Ï„
X2(t) dt

â‰¤E
 T
âˆ’T
X2(t) dt

â‰¤
/2T
Ts
0
E
 Ï„+Ts
Ï„
X2(t) dt

, (14.28)
where we use âŒŠÎ¾âŒ‹to denote the greatest integer smaller than or equal to Î¾ (e.g.,
âŒŠ4.2âŒ‹= 4), and where we use âŒˆÎ¾âŒ‰to denote the smallest integer that is greater than
or equal to Î¾ (e.g., âŒˆ4.2âŒ‰= 5) so
Î¾ âˆ’1 < âŒŠÎ¾âŒ‹â‰¤âŒˆÎ¾âŒ‰< Î¾ + 1,
Î¾ âˆˆR.
(14.29)
Note that from (14.29) and the Sandwich Theorem it follows that
lim
Tâ†’âˆ
1
2T
72T
Ts
8
= lim
Tâ†’âˆ
1
2T
/2T
Ts
0
= 1
Ts
,
Ts > 0.
(14.30)
Dividing (14.28) by 2T and using (14.30) we obtain that
lim
Tâ†’âˆ
1
2T E
 T
âˆ’T
X2(t) dt

= 1
Ts
E
 Ï„+Ts
Ï„
X2(t) dt

,
which combines with (14.27) to yield
P = 1
Ts
A2
âˆ

m=âˆ’âˆ
KXX(m) Rgg(mTs).
(14.31)
The power P can be alternatively expressed in the frequency domain using (14.31)
and (14.4) as
P = A2
Ts
 âˆ
âˆ’âˆ
âˆ

m=âˆ’âˆ
KXX(m) ei2Ï€fmTs |Ë†g(f)|2 df.
(14.32)
An important special case of (14.31) is when the symbols

Xâ„“

are zero-mean,
uncorrelated, and of equal variance Ïƒ2
X. In this case KXX(m) = Ïƒ2
X I{m = 0}, and
the only nonzero term in (14.31) is the term corresponding to m = 0 so
P = 1
Ts
A2 âˆ¥gâˆ¥2
2 Ïƒ2
X,

Xâ„“

centered, variance Ïƒ2
X, uncorrelated

.
(14.33)
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.5 Computing the Power in PAM
241
14.5.2
Bi-Inï¬nite Block-Mode
The bi-inï¬nite block-mode with a (K, N) binary-to-reals block encoder
enc: {0, 1}K â†’RN
is depicted in Figure 14.1 and can be described as follows. A bi-inï¬nite sequence
of data bits

Dj, j âˆˆZ

is fed to an encoder. The encoder parses this sequence
into K-tuples and deï¬nes for every integer Î½ âˆˆZ the â€œÎ½-th data blockâ€ DÎ½
DÎ½ â‰œ

DÎ½K+1, . . . , DÎ½K+K

,
Î½ âˆˆZ.
(14.34)
Each data block DÎ½ is then mapped by enc(Â·) to a real N-tuple, which we denote
by XÎ½:
XÎ½ â‰œenc(DÎ½),
Î½ âˆˆZ.
(14.35)
The bi-inï¬nite sequence

Xâ„“, â„“âˆˆZ

produced by the encoder is the concatenation
of these N-tuples so

XÎ½N+1, . . . , XÎ½N+N

= XÎ½,
Î½ âˆˆZ.
(14.36)
Stated diï¬€erently, for every Î½ âˆˆZ and Î· âˆˆ{1, . . . , N}, the symbol XÎ½N+Î· is the
Î·-th component of the N-tuple XÎ½. The transmitted signal X(Â·) is as in (14.15)
with the pulse shape g satisfying the decay condition (14.17) and with Ts > 0 being
arbitrary. (The boundedness condition (14.16) is always guaranteed in bi-inï¬nite
block encoding.)
We next compute the power P in X(Â·) under the assumption that the data bits

Dj, j âˆˆZ

are IID random bits, where we adopt the following deï¬nition.
Deï¬nition 14.5.1 (IID Random Bits). We say that a collection of random variables
comprises IID random bits if the random variables are binary, independent, and
each of them takes on the values 0 and 1 equiprobably.
The assumption that the bi-inï¬nite data sequence

Dj, j âˆˆZ

consists of IID
random bits is equivalent to the assumption that the K-tuples

DÎ½, Î½ âˆˆZ

are
IID with DÎ½ being uniformly distributed over the set of binary K-tuples {0, 1}K.
We shall also assume that the real N-tuple enc(D) is of zero mean whenever the
binary K-tuple is uniformly distributed over {0, 1}K. We will show that, subject to
these assumptions,
P =
1
NTs
E
5 âˆ
âˆ’âˆ
	
A
N

â„“=1
Xâ„“g(t âˆ’â„“Ts)

2
dt
6
.
(14.37)
This expression has an interesting interpretation.
On the LHS is the power in
the transmitted signal in bi-inï¬nite block encoding using the (K, N) binary-to-reals
block encoder enc(Â·). On the RHS is the quantity E/(NTs), where E, as in (14.3), is
the expected energy in the signal that results when only the K-tuple (D1, . . . , DK)
is transmitted from time âˆ’âˆto time +âˆ.
Using the deï¬nition of the energy
per-symbol Es (14.7) we can also rewrite (14.37) as in (14.8). Thus, in bi-inï¬nite
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

242
Energy and Power in PAM
block-mode, the transmitted power is the energy per real symbol Es normalized by
the signaling period Ts. Also, by (14.5), we can rewrite (14.37) as
P = A2
NTs
 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] ei2Ï€f(â„“âˆ’â„“â€²)Ts Ë†g(f)
2 df.
(14.38)
To derive (14.37) we ï¬rst express the transmitted waveform X(Â·) as
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts)
= A
âˆ

Î½=âˆ’âˆ
N

Î·=1
XÎ½N+Î· g

t âˆ’(Î½N + Î·)Ts

= A
âˆ

Î½=âˆ’âˆ
u

XÎ½, t âˆ’Î½NTs

,
t âˆˆR,
(14.39)
where the function u: RN Ã— R â†’R is given by
u: (x1, . . . , xN, t) 	â†’
N

Î·=1
xÎ· g(t âˆ’Î·Ts).
(14.40)
We now make three observations. The ï¬rst is that because the law of DÎ½ does not
depend on Î½, neither does the law of XÎ½ (= enc(DÎ½)):
XÎ½
L= XÎ½â€²,
Î½, Î½â€² âˆˆZ.
(14.41)
The second is that the assumption that enc(D) is of zero mean whenever D is
uniformly distributed over {0, 1}K implies by (14.40) that
E

u

XÎ½, t

= 0,

Î½ âˆˆZ, t âˆˆR

.
(14.42)
The third is that the hypothesis that the data bits

Dj, j âˆˆZ

are IID implies
that

DÎ½, Î½ âˆˆZ

are IID and hence that

XÎ½, Î½ âˆˆZ

are also IID. Consequently,
since the independence of XÎ½ and XÎ½â€² implies the independence of u

XÎ½, t

and
u

XÎ½â€²tâ€²
, it follows from (14.42) that
E

u

XÎ½, t

u

XÎ½â€², tâ€²
= 0,

t, tâ€² âˆˆR, Î½ Ì¸= Î½â€², Î½, Î½â€² âˆˆZ

.
(14.43)
Using (14.39) and these three observations we can now compute for any epoch Ï„ âˆˆR
the expected energy in the time interval [Ï„, Ï„ + NTs) as
 Ï„+NTs
Ï„
E

X2(t)

dt
=
 Ï„+NTs
Ï„
E
5	
A
âˆ

Î½=âˆ’âˆ
u

XÎ½, t âˆ’Î½NTs

26
dt
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.5 Computing the Power in PAM
243
= A2
 Ï„+NTs
Ï„
âˆ

Î½=âˆ’âˆ
âˆ

Î½â€²=âˆ’âˆ
E
%
u

XÎ½, t âˆ’Î½NTs

u

XÎ½â€², t âˆ’Î½â€²NTs
&
dt
= A2
 Ï„+NTs
Ï„
âˆ

Î½=âˆ’âˆ
E

u2
XÎ½, t âˆ’Î½NTs

dt
= A2
 Ï„+NTs
Ï„
âˆ

Î½=âˆ’âˆ
E

u2
X0, t âˆ’Î½NTs

dt
= A2
âˆ

Î½=âˆ’âˆ
 Ï„âˆ’(Î½âˆ’1)NTs
Ï„âˆ’Î½NTs
E

u2
X0, tâ€²
dtâ€²
= A2
 âˆ
âˆ’âˆ
E

u2
X0, tâ€²
dtâ€²
= E
5 âˆ
âˆ’âˆ
	
A
N

â„“=1
Xâ„“g(tâ€² âˆ’â„“Ts)

2
dtâ€²
6
,
Ï„ âˆˆR,
(14.44)
where the ï¬rst equality follows from(14.39); the second by writing the square as
a product and by using the linearity of expectation; the third from (14.43); the
fourth because the law of XÎ½ does not depend on Î½ (14.41); the ï¬fth by changing
the integration variable to tâ€² â‰œt âˆ’NTs; the sixth because the sum of the integrals
is equal to the integral over R; and the seventh by (14.40).
Note that, indeed, the RHS of (14.44) does not depend on the starting epoch Ï„ of
the interval. Because there are âŒŠ2T/(NTs)âŒ‹disjoint length-NTs half-open intervals
contained in the interval [âˆ’T, T) and because âŒˆ2T/(NTs)âŒ‰such intervals suï¬ƒce to
cover the interval [âˆ’T, T), it follows that
7 2T
NTs
8
E
5 âˆ
âˆ’âˆ
	
A
N

â„“=1
Xâ„“g(t âˆ’â„“Ts)

2
dt
6
â‰¤E
 T
âˆ’T
X2(t) dt

â‰¤
/ 2T
NTs
0
E
5 âˆ
âˆ’âˆ
	
A
N

â„“=1
Xâ„“g(t âˆ’â„“Ts)

2
dt
6
.
Dividing by 2T and then letting T tend to inï¬nity establishes (14.37).
14.5.3
Time Shifts of Pulse Shape Are Orthonormal
We next consider the power in PAM when the time shifts of the real pulse shape by
integer multiples of Ts are orthonormal. To remind the reader of this assumption,
we change notation and denote the pulse shape by Ï†(Â·) and express the orthonor-
mality condition as
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ.
(14.45)
The calculation of the power is a bit tricky because (14.45) only guarantees that the
time shifts of the pulse shape are orthogonal over the interval (âˆ’âˆ, âˆ); they need
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

244
Energy and Power in PAM
not be orthogonal over the interval [âˆ’T, +T ] (even for very large T). Nevertheless,
intuition suggests that if â„“Ts and â„“â€²Ts are both much smaller than T, then the
orthogonality of t 	â†’Ï†(t âˆ’â„“Ts) and t 	â†’Ï†(t âˆ’â„“â€²Ts) over the interval (âˆ’âˆ, âˆ)
should imply that they are nearly orthogonal over [âˆ’T, T ]. Making this intuition
rigorous is a bit tricky and the calculation of the energy in the interval [âˆ’T, T ]
requires a fair number of approximations that must be justiï¬ed.
To control these approximations we shall assume a decay condition on the pulse
shape that is identical to (14.17). Thus, we shall assume that there exist positive
constants Î± and Î² such that
Ï†(t)
 â‰¤
Î²
1 + |t/Ts|1+Î± ,
t âˆˆR.
(14.46)
(The pulse shapes used in practice, like those we encountered in (11.31), typically
decay like 1/t2 so this is not a serious restriction.) We shall also continue to assume
the boundedness condition (14.16) but otherwise make no statistical assumptions
on the symbols

Xâ„“, â„“âˆˆZ

.
The main result of this section is the next theorem.
Theorem 14.5.2. Let the continuous-time SP

X(t), t âˆˆR

be given by
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“Ï†(t âˆ’â„“Ts),
t âˆˆR,
(14.47)
where A â‰¥0; Ts > 0; the pulse shape Ï†(Â·) is a Borel measurable function satisfying
the orthonormality condition (14.45) and the decay condition (14.46); and where
the random sequence

Xâ„“, â„“âˆˆZ

satisï¬es the boundedness condition (14.16). Then
lim
Tâ†’âˆ
1
2T E
 T
âˆ’T
X2(t) dt

= A2
Ts
lim
Lâ†’âˆ
1
2L + 1
L

â„“=âˆ’L
E

X2
â„“

,
(14.48)
whenever the limit on the RHS exists.
Proof. The proof is somewhat technical and may be skipped. Both sides of (14.48)
scale like A2, so it suï¬ƒces to prove the theorem, as we shall, for A = 1. We next
argue that it suï¬ƒces to prove the theorem for the case where Ts = 1. To see this,
assume that Ts > 0 is not necessarily equal to 1. Deï¬ne the function
ËœÏ†(t) =

Ts Ï†(Tst),
t âˆˆR,
(14.49)
and note that, by changing the integration variable to Ï„ â‰œtTs,
 âˆ
âˆ’âˆ
ËœÏ†(t âˆ’â„“) ËœÏ†(t âˆ’â„“â€²) dt =
 âˆ
âˆ’âˆ
Ï†(Ï„ âˆ’â„“Ts) Ï†(Ï„ âˆ’â„“â€²Ts) dÏ„
= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ,
(14.50a)
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.5 Computing the Power in PAM
245
where the second equality follows from the theoremâ€™s assumption about the or-
thonormality of the time shifts of Ï† by integer multiples of Ts. Also, by (14.49)
and (14.46) we obtain
|ËœÏ†(t)| =

Ts |Ï†(Tst)|
â‰¤

Ts
Î²
1 + |t|1+Î±
=
Î²â€²
1 + |t|1+Î± ,
t âˆˆR,
(14.50b)
for some Î²â€² > 0 and Î± > 0.
As to the power, by changing the integration variable to Ïƒ â‰œt/Ts we obtain
1
2T
 T
âˆ’T
	
â„“âˆˆZ
Xâ„“Ï†(tâˆ’â„“Ts)

2
dt = 1
Ts
1
2(T/Ts)
 T/Ts
âˆ’T/Ts
	
â„“âˆˆZ
Xâ„“ËœÏ†(Ïƒâˆ’â„“)

2
dÏƒ. (14.50c)
It now follows from (14.50a) & (14.50b) that if we prove the theorem for the pulse
shape ËœÏ† with Ts = 1, it will then follow that the power in âˆ
â„“=âˆ’âˆXâ„“ËœÏ†(Ïƒ âˆ’â„“)
is equal to limLâ†’âˆ(2L + 1)âˆ’1 L
â„“=âˆ’L E

X2
â„“

and that consequently, by (14.50c),
the power in âˆ
â„“=âˆ’âˆXâ„“Ï†(t âˆ’â„“Ts) is Tâˆ’1
s
limLâ†’âˆ(2L + 1)âˆ’1 L
â„“=âˆ’L E

X2
â„“

. In the
remainder of the proof we shall thus assume that Ts = 1 and express the decay
condition (14.46) as
|Ï†(t)| â‰¤
Î²
1 + |t|1+Î± ,
t âˆˆR
(14.51)
for some Î², Î± > 0.
To further simplify notation we shall assume that T is a positive integer. Indeed,
if the limit is proved for positive integers, then the general result follows from the
Sandwich Theorem by noting that for T > 0 (not necessarily an integer)
âŒŠTâŒ‹
T
1
âŒŠTâŒ‹
 âŒŠTâŒ‹
âˆ’âŒŠTâŒ‹
	
â„“âˆˆZ
Xâ„“Ï†(t âˆ’â„“)

2
dt
â‰¤1
T
 T
âˆ’T
	
â„“âˆˆZ
Xâ„“Ï†(t âˆ’â„“)

2
dt â‰¤
âŒˆTâŒ‰
T
1
âŒˆTâŒ‰
 âŒˆTâŒ‰
âˆ’âŒˆTâŒ‰
	
â„“âˆˆZ
Xâ„“Ï†(t âˆ’â„“)

2
dt
(14.52)
and by noting that both âŒŠTâŒ‹/T and âŒˆTâŒ‰/T tend to 1, as T â†’âˆ.
We thus proceed to prove (14.48) for the case where A = 1, where Ts = 1, and
where the limit T â†’âˆis only over positive integers. We begin by introducing
some notation. For every integer â„“we denote the mapping t 	â†’Ï†(t âˆ’â„“) by Ï†â„“, and
for every positive integer T we denote the windowed mapping t 	â†’Ï†(tâˆ’â„“) I{|t| â‰¤T}
by Ï†â„“,w. Finally, we ï¬x some integer Î½ > 0 and deï¬ne for every T > Î½, the random
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

246
Energy and Power in PAM
processes
X0 =

|â„“|â‰¤Tâˆ’Î½
Xâ„“Ï†â„“,w,
(14.53)
X1 =

Tâˆ’Î½<|â„“|â‰¤T+Î½
Xâ„“Ï†â„“,w,
(14.54)
X2 =

T+Î½<|â„“|<âˆ
Xâ„“Ï†â„“,w,
(14.55)
and the unwindowed version of X0
Xu
0 =

|â„“|â‰¤Tâˆ’Î½
Xâ„“Ï†â„“
(14.56)
so
X(t) I{|t| â‰¤T} = X0(t) + X1(t) + X2(t)
= Xu
0 +

X0(t) âˆ’Xu
0 (t)

+ X1(t) + X2(t),
t âˆˆR.
(14.57)
Using arguments very similar to the ones leading to (4.14) (with integration re-
placed by integration and expectation) one can show that (14.57) leads to the
bound
	4
E
%
âˆ¥Xu
0âˆ¥2
2
&
âˆ’
4
E
%
X0 âˆ’Xu
0

+ X1 + X2
2
2
&
2
â‰¤E
 T
âˆ’T
X2(t) dt

â‰¤
	4
E
%
âˆ¥Xu
0âˆ¥2
2
&
+
4
E
%
X0 âˆ’Xu
0

+ X1 + X2
2
2
&
2
.
(14.58)
Note that, by the orthonormality assumption on the time shifts of Ï†,
âˆ¥Xu
0âˆ¥2
2 =

|â„“|â‰¤Tâˆ’Î½
X2
â„“
so
lim
Tâ†’âˆ
1
2T E
%
âˆ¥Xu
0âˆ¥2
2
&
= lim
Lâ†’âˆ
1
2L + 1

|â„“|â‰¤L
E

X2
â„“

.
(14.59)
It follows from (14.58) and (14.59) that to conclude the proof of the theorem it
suï¬ƒces to show that for every ï¬xed Î½ â‰¥2
lim
Tâ†’âˆ
1
2T E
%
âˆ¥X1âˆ¥2
2
&
= 0,
(14.60)
lim
Tâ†’âˆ
1
2T E
%
âˆ¥X0 âˆ’Xu
0âˆ¥2
2
&
= 0,
(14.61)
and that
lim
Î½â†’âˆlim
Tâ†’âˆ
1
2T E
%
âˆ¥X2âˆ¥2
2
&
= 0.
(14.62)
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.5 Computing the Power in PAM
247
We begin with (14.60), which follows directly from the Triangle Inequality,
âˆ¥X1âˆ¥2 â‰¤

Tâˆ’Î½<|â„“|â‰¤T+Î½
|Xâ„“| âˆ¥Ï†â„“,wâˆ¥2
â‰¤4Î½Î³,
where the second inequality follows from the boundedness condition (14.16), from
the fact that Ï†â„“,w is a windowed version of the unit-energy signal Ï†â„“so âˆ¥Ï†â„“,wâˆ¥2 â‰¤
âˆ¥Ï†âˆ¥2 = 1, and because there are 4Î½ terms in the sum (whenever T exceeds Î½).
We next prove (14.62). To that end we upper-bound |X2(t)| for |t| â‰¤T as follows:
|X2(t)| =


T+Î½<|â„“|<âˆ
Xâ„“Ï†(t âˆ’â„“)
,
|t| â‰¤T
â‰¤Î³

T+Î½<|â„“|<âˆ
|Ï†(t âˆ’â„“)|
â‰¤Î³

T+Î½<|â„“|<âˆ
Î²
|t âˆ’â„“|1+Î±
â‰¤Î³

T+Î½<|â„“|<âˆ
Î²
|â„“| âˆ’|t|
1+Î±
â‰¤Î³

T+Î½<|â„“|<âˆ
Î²
(|â„“| âˆ’T)1+Î± ,
|t| â‰¤T
= 2Î³Î²
âˆ

â„“=T+Î½+1
1
(â„“âˆ’T)1+Î±
= 2Î³Î²
âˆ

Ëœâ„“=Î½+1
1
Ëœâ„“1+Î±
â‰¤2Î³Î²
 âˆ
Î½
Î¾âˆ’1âˆ’Î± dÎ¾
= 2Î³Î²
Î± Î½âˆ’Î±,
(14.63)
where the equality in the ï¬rst line follows from the deï¬nition of X2 (14.55) by
noting that for |t| â‰¤T we have Ï†â„“(t) = Ï†â„“,w(t); the inequality in the second line
follows from the boundedness condition (14.16) and from the Triangle Inequality for
Complex Numbers (2.12); the inequality in the third line from the decay condition
(14.51); the inequality in the fourth line because |Î¾ âˆ’Î¶| â‰¥
|Î¾| âˆ’|Î¶|
 whenever
Î¾, Î¶ âˆˆR; the inequality in the ï¬fth line because we are only considering |t| â‰¤T and
because over the range of this summation |â„“| > T + Î½; the equality in the sixth line
from the symmetry of the summand; the equality in the seventh line by deï¬ning
Ëœâ„“â‰œâ„“âˆ’T; the inequality in the eighth line from the monotonicity of the function
Î¾ 	â†’Î¾âˆ’1âˆ’Î± (for Î± > 0), which implies that
1
Ëœâ„“1+Î± â‰¤
 Ëœâ„“
Ëœâ„“âˆ’1
1
Î¾1+Î± dÎ¾;
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

248
Energy and Power in PAM
and where the ï¬nal equality on the ninth line follows by computing the integral.
Using (14.63), which holds for all t âˆˆR because X2(t) is zero for |t| > T, we
conclude that
âˆ¥X2âˆ¥2
2 â‰¤2T
2Î³Î²
Î±
2
Î½âˆ’2Î±,
(14.64)
from which (14.62) follows.
We next turn to proving (14.61). We begin by using the Triangle Inequality and
the boundedness condition (14.16) to obtain
âˆ¥X0 âˆ’Xu
0âˆ¥2
2 =


|â„“|â‰¤Tâˆ’Î½
Xâ„“Ï†â„“,w âˆ’

|â„“|â‰¤Tâˆ’Î½
Xâ„“Ï†â„“

2
2
=


|â„“|â‰¤Tâˆ’Î½
Xâ„“

Ï†â„“,w âˆ’Ï†â„“

2
2
â‰¤Î³2
	

|â„“|â‰¤Tâˆ’Î½
âˆ¥Ï†â„“,w âˆ’Ï†â„“âˆ¥2

2
.
(14.65)
We next proceed to upper-bound the RHS of (14.65) by ï¬rst deï¬ning the function
Ï(Ï„) =

|t|>Ï„
Ï†2(t) dt
(14.66)
and by then using this function to upper-bound âˆ¥Ï†â„“âˆ’Ï†â„“,wâˆ¥2 as
âˆ¥Ï†â„“âˆ’Ï†â„“,wâˆ¥2 â‰¤Ï(T âˆ’|â„“|),
|â„“| â‰¤T,
(14.67)
because
âˆ¥Ï†â„“âˆ’Ï†â„“,wâˆ¥2
2 =
 âˆ’T
âˆ’âˆ
Ï†2(t âˆ’â„“) dt +
 âˆ
T
Ï†2(t âˆ’â„“) dt
=
 âˆ’Tâˆ’â„“
âˆ’âˆ
Ï†2(s) ds +
 âˆ
Tâˆ’â„“
Ï†2(s) ds
â‰¤
 âˆ’T+|â„“|
âˆ’âˆ
Ï†2(s) ds +
 âˆ
Tâˆ’|â„“|
Ï†2(s) ds
=

|s|â‰¥Tâˆ’|â„“|
Ï†2(s) ds,
|â„“| â‰¤T
= Ï2(T âˆ’|â„“|).
It follows from (14.65) and (14.67) that
âˆ¥X0 âˆ’Xu
0âˆ¥2
2 â‰¤Î³2
	 
|â„“|â‰¤Tâˆ’Î½
âˆ¥Ï†â„“,w âˆ’Ï†â„“âˆ¥2

2
â‰¤Î³2
	

|â„“|â‰¤Tâˆ’Î½
Ï(T âˆ’|â„“|)

2
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.6 A More Formal Account
249
â‰¤Î³2
	
2

0â‰¤â„“â‰¤Tâˆ’Î½
Ï(T âˆ’â„“)

2
= 4Î³2
	
T

Î·=Î½
Ï(Î·)

2
.
(14.68)
We next note that the decay condition (14.51) implies that
Ï(Ï„) â‰¤
 2Î²2
1 + 2Î±
1/2
Ï„ âˆ’1
2 âˆ’Î±,
Ï„ > 0,
(14.69)
because for every Ï„ > 0,
Ï2(Ï„) =

|t|>Ï„
Ï†2(t) dt
â‰¤

|t|>Ï„
Î²2
|t|2+2Î± dt
= 2Î²2
 âˆ
Ï„
tâˆ’2âˆ’2Î± dt
=
2Î²2
1 + 2Î±Ï„ âˆ’1âˆ’2Î±.
It now follows from (14.69) that
T

Î·=Î½
Ï(Î·) â‰¤
 2Î²2
1 + 2Î±
1/2
T

Î·=Î½
Î·âˆ’1
2 âˆ’Î±
â‰¤
 2Î²2
1 + 2Î±
1/2  T
Î½âˆ’1
Î¾âˆ’1
2 âˆ’Î± dÎ¾
and hence, by evaluating the integral explicitly, that
lim
Tâ†’âˆ
1
T1/2
T

Î·=Î½
Ï(Î·) = 0.
(14.70)
From (14.68) and (14.70) we thus obtain (14.61).
14.6
A More Formal Account
In this section we present a more formal deï¬nition of power and justify some of
the mathematical steps that we took in deriving the power in PAM signals. This
section is quite mathematical and is recommended for readers who have had some
exposure to Measure Theory.
Let R denote the Ïƒ-algebra generated by the open sets in R, i.e., the Borel Ïƒ-
algebra over the reals. A continuous-time stochastic process

X(t)

deï¬ned over
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

250
Energy and Power in PAM
the probability space (Î©, F, P) is said to be a measurable stochastic process
if the mapping (Ï‰, t) 	â†’X(Ï‰, t) from Î© Ã— R to R is measurable when its codomain
R is endowed with the Ïƒ-algebra R and when its domain Î© Ã— R is endowed with
the product Ïƒ-algebra F Ã— R (Deï¬nition 25.9.1 ahead). Thus,

X(t), t âˆˆR

is
measurable if the mapping (Ï‰, t) 	â†’X(Ï‰, t) is FÃ—R/R measurable.5
From Fubiniâ€™s Theorem it follows that if

X(t), t âˆˆR

is measurable and if T > 0
is deterministic, then:
(i) For every Ï‰ âˆˆÎ©, the mapping t 	â†’X2(Ï‰, t) is Borel measurable;
(ii) the mapping
Ï‰ 	â†’
 T
âˆ’T
X2(Ï‰, t) dt
is a random variable (i.e., F measurable) possibly taking on the value +âˆ;
(iii) and
E
 T
âˆ’T
X2(t) dt

=
 T
âˆ’T
E

X2(t)

dt,
T âˆˆR.
(14.71)
Deï¬nition 14.6.1 (Power of a Stochastic Process). We say that a measurable
stochastic process

X(t), t âˆˆR

is of power P if the limit
lim
Tâ†’âˆ
1
2T E
 T
âˆ’T
X2(t) dt

(14.72)
exists and is equal to P.
Proposition 14.6.2. If the pulse shape g is a Borel measurable function satisfying
the decay condition (14.17) for some positive Î±, Î², Ts, and if the discrete-time SP

Xâ„“, â„“âˆˆZ

satisï¬es the boundedness condition (14.16) for some Î³ â‰¥0, then the
stochastic process
X: (Ï‰, t) 	â†’A
âˆ

â„“=âˆ’âˆ
Xâ„“(Ï‰) g(t âˆ’â„“Ts)
(14.73)
is a measurable stochastic process.
Proof. The mapping (Ï‰, t) 	â†’Xâ„“(Ï‰) is FÃ—R/R measurable because Xâ„“is a ran-
dom variable, so the mapping Ï‰ 	â†’Xâ„“(Ï‰) is F/R measurable.
The mapping
(Ï‰, t) 	â†’A g(t âˆ’â„“Ts) is FÃ—R/R measurable because g is Borel measurable, so
t 	â†’g(t âˆ’â„“Ts) is R/R measurable. Since the product of measurable functions is
measurable (Rudin, 1987, Chapter 1, Section 1.9 (c)), it follows that the mapping
(Ï‰, t) 	â†’AXâ„“(Ï‰) g(t âˆ’â„“Ts) is FÃ—R/R measurable. And since the sum of measur-
able functions is measurable (Rudin, 1987, Chapter 1, Section 1.9 (c)), it follows
that for every positive integer L âˆˆZ, the mapping
(Ï‰, t) 	â†’A
L

â„“=âˆ’L
Xâ„“(Ï‰) g(t âˆ’â„“Ts)
5See (Billingsley, 1995, Section 37, p. 503) or (Lo`eve, 1963, Section 35) on the deï¬nition of a
measurable stochastic process and see (Billingsley, 1995, Section 18) or (Lo`eve, 1963, Section 8.2)
or (Halmos, 1950, Chapter VII) for the deï¬nition of the product Ïƒ-algebra.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.6 A More Formal Account
251
is FÃ—R/R measurable. The proposition now follows by recalling that the pointwise
limit of every pointwise convergent sequence of measurable functions is measurable
(Rudin, 1987, Theorem 1.14).
Having established that the PAM signal (14.73) is a measurable stochastic process
we would next like to justify the calculations leading to (14.31). To justify the
swapping of integration and summations in (14.26) we shall need the following
lemma, which also explains why the sum in (14.27) converges.
Lemma 14.6.3. If g(Â·) is a Borel measurable function satisfying the decay condition
|g(t)| â‰¤
Î²
1 + |t/Ts|1+Î± ,
t âˆˆR
(14.74)
for some positive Î±, Ts, and Î², then
âˆ

m=âˆ’âˆ
 âˆ
âˆ’âˆ
g(t) g(t âˆ’mTs)
 dt < âˆ.
(14.75)
Proof. The decay condition (14.74) guarantees that g is of ï¬nite energy. From the
Cauchy-Schwarz Inequality it thus follows that the terms in (14.75) are all ï¬nite.
Also, by symmetry, the term in (14.75) corresponding to m is the same as the one
corresponding to âˆ’m. Consequently, to establish (14.75), it suï¬ƒces to prove
âˆ

m=2
 âˆ
âˆ’âˆ
g(t) g(t âˆ’mTs)
 dt < âˆ.
(14.76)
Deï¬ne the function
gu(t) â‰œ

1
if |t| â‰¤1,
|t|âˆ’1âˆ’Î±
otherwise,
t âˆˆR.
By (14.74) it follows that |g(t)| â‰¤Î² gu (t/Ts) for all t âˆˆR. Consequently,
 âˆ
âˆ’âˆ
g(t) g(t âˆ’mTs)
 dt â‰¤Î²2
 âˆ
âˆ’âˆ
gu(t/Ts) gu(t/Ts âˆ’m) dt
= Î²2Ts
 âˆ
âˆ’âˆ
gu(Ï„) gu(Ï„ âˆ’m) dÏ„,
and to establish (14.76) it thus suï¬ƒces to prove
âˆ

m=2
 âˆ
âˆ’âˆ
gu(Ï„) gu(Ï„ âˆ’m) dÏ„ < âˆ.
(14.77)
Since the integrand in (14.77) is symmetric around Ï„ = m/2, it follows that
 âˆ
âˆ’âˆ
gu(Ï„) gu(Ï„ âˆ’m) dÏ„ = 2
 âˆ
m/2
gu(Ï„) gu(Ï„ âˆ’m) dÏ„,
(14.78)
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

252
Energy and Power in PAM
and it thus suï¬ƒces to establish
âˆ

m=2
 âˆ
m/2
gu(Ï„) gu(Ï„ âˆ’m) dÏ„ < âˆ.
(14.79)
We next upper-bound the integral in (14.79) for every m â‰¥2 by ï¬rst expressing it
as
 âˆ
m/2
gu(Ï„) gu(Ï„ âˆ’m) dÏ„ = I1 + I2 + I3,
where
I1 â‰œ
 mâˆ’1
m/2
1
Ï„ 1+Î±
1
(m âˆ’Ï„)1+Î± dÏ„,
I2 â‰œ
 m+1
mâˆ’1
1
Ï„ 1+Î± dÏ„,
I3 â‰œ
 âˆ
m+1
1
Ï„ 1+Î±
1
(Ï„ âˆ’m)1+Î± dÏ„.
We next upper-bound each of these terms for m â‰¥2. Starting with I1 we obtain
upon deï¬ning Î¾ â‰œm âˆ’Ï„
I1 =
 mâˆ’1
m/2
1
Ï„ 1+Î±
1
(m âˆ’Ï„)1+Î± dÏ„
=
 m/2
1
1
(m âˆ’Î¾)1+Î±
1
Î¾1+Î± dÎ¾
â‰¤
 m/2
1
1
(m/2)1+Î±
1
Î¾1+Î± dÎ¾
= 1
Î± 21+Î±
1
m1+Î±
	
1 âˆ’2Î±
mÎ±

,
m â‰¥2
â‰¤1
Î± 21+Î±
1
m1+Î± ,
m â‰¥2,
which is summable over m. As to I2 we have
I2 =
 m+1
mâˆ’1
1
Ï„ 1+Î± dÏ„
â‰¤
2
(m âˆ’1)1+Î± ,
m â‰¥2,
which is summable over m. Finally we upper-bound I3 by deï¬ning Î¾ â‰œÏ„ âˆ’m
I3 =
 âˆ
m+1
1
Ï„ 1+Î±
1
(Ï„ âˆ’m)1+Î± dÏ„
=
 âˆ
1
1
(Î¾ + m)1+Î±
1
Î¾1+Î± dÎ¾
=
 m
1
1
(Î¾ + m)1+Î±
1
Î¾1+Î± dÎ¾ +
 âˆ
m
1
(Î¾ + m)1+Î±
1
Î¾1+Î± dÎ¾
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.7 Exercises
253
â‰¤
1
m1+Î±
 m
1
1
Î¾1+Î± dÎ¾ +
 âˆ
m
1
Î¾1+Î±
1
Î¾1+Î± dÎ¾
= 1
Î±
1
m1+Î±
	
1 âˆ’
1
mÎ±

+
1
1 + 2Î±
1
m1+2Î± ,
m â‰¥2
â‰¤1
Î±
1
m1+Î± ,
m â‰¥2,
which is summable over m.
We can now state (14.31) as a theorem.
Theorem 14.6.4. Let the pulse shape g: R â†’R be a Borel measurable function sat-
isfying the decay condition (14.17) for some positive Î±, Î², and Ts. Let

Xâ„“, â„“âˆˆZ

be a centered WSS SP of autocovariance function KXX and satisfying the bound-
edness condition (14.16) for some Î³ â‰¥0. Then the stochastic process (14.73) is
measurable and is of the power P given in (14.31).
Proof. The measurability of

X(t), t âˆˆR

follows from Proposition 14.6.2. The
power can be derived as in the derivation of (14.31) from (14.27) with the derivation
of (14.27) now being justiï¬able by noting that (14.25) follows from (14.71) and by
noting that (14.26) follows from Lemma 14.6.3 and Fubiniâ€™s Theorem.
Similarly, we can state (14.37) as a theorem.
Theorem 14.6.5 (Power in Bi-Inï¬nite Block-Mode PAM). Let

Dj, j âˆˆZ

be
IID random bits. Let the (K, N) binary-to-reals encoder enc: {0, 1}K â†’RN be
such that enc(D1, . . . , DK) is of zero mean whenever the K-tuple (D1, . . . , DK) is
uniformly distributed over {0, 1}K. Let

Xâ„“, â„“âˆˆZ

be generated from

Dj, j âˆˆZ

in bi-inï¬nite block encoding mode using enc(Â·). Assume that the pulse shape g is a
Borel measurable function satisfying the decay condition (14.17) for some positive
Î±, Î², and Ts.
Then the stochastic process (14.73) is measurable and is of the
power P as given in (14.37).
Proof. Measurability follows from Proposition 14.6.2. The derivation of (14.37) is
justiï¬ed using Fubiniâ€™s Theorem.
14.7
Exercises
Exercise 14.1 (Superimposing Independent Transmissions). Let the two PAM signals

X(1)(t)

and

X(2)(t)

be given at every epoch t âˆˆR by
X(1)(t) = A(1)
âˆ

â„“=âˆ’âˆ
X(1)
â„“
g(1)(t âˆ’â„“Ts),
X(2)(t) = A(2)
âˆ

â„“=âˆ’âˆ
X(2)
â„“
g(2)(t âˆ’â„“Ts),
where the zero-mean real symbols

X(1)
â„“

are generated from the data bits

D(1)
j

and
the zero-mean real symbols

X(2)
â„“

from

D(2)
j

. Assume that the bit streams

D(1)
j

and

D(2)
j

are independent and that

X(1)(t)

and

X(2)(t)

are of powers P(1) and P(2).
Find the power in the sum of

X(1)(t)

and

X(2)(t)

.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

254
Energy and Power in PAM
Exercise 14.2 (The Minimum Distance of a Constellation and Power). Consider the
PAM signal (14.47) where the time shifts of the pulse shape Ï† by integer multiples of Ts
are orthonormal, and where the symbols

Xâ„“

are IID and uniformly distributed over the
set

Â± d
2, Â± 3d
2 , . . . , Â±(2Î½ âˆ’1) d
2

. Relate the power in X(Â·) to the minimum distance d
and the constant A.
Exercise 14.3 (PAM with Nonorthogonal Pulses). Let the IID random bits

Dj, j âˆˆZ

be modulated using PAM with the pulse shape g: t â†’I{|t| â‰¤Ts} and the repetition block
encoding map 0 â†’(+1, +1) and 1 â†’(âˆ’1, âˆ’1). Compute the average transmitted power.
Exercise 14.4 (Non-IID Data Bits). Expression (14.37) for the power in bi-inï¬nite block
mode was derived under the assumption that the data bits are IID. Show that it need
not otherwise hold.
Exercise 14.5 (The Power in Nonorthogonal PAM). Consider the PAM signal (14.23)
with the pulse shape g: t â†’I{|t| â‰¤Ts}.
(i) Compute the power in X(Â·) when

Xâ„“

are IID of zero mean and unit variance.
(ii) Repeat when

Xâ„“

is a zero-mean WSS SP of autocovariance function
KXX(m) =
â§
âª
â¨
âª
â©
1
m = 0,
1
2
|m| = 1,
0
otherwise,
m âˆˆZ.
Note that in both parts E[Xâ„“] = 0 and E
$
X2
â„“
%
= 1.
Exercise 14.6 (A Non-PAM Signal). The bi-inï¬nite data sequence

Dj, j âˆˆZ

of IID
random bits is mapped to the waveform
X(t) = A
âˆ

â„“=âˆ’âˆ
	
Dâ„“g1(t âˆ’â„“Ts) + (1 âˆ’Dâ„“) g0(t âˆ’â„“Ts)

,
t âˆˆR,
where g0 and g1 are pulse shapes that satisfy the decay condition (14.17). Compute the
mean E[X(t)]; the energy in a length-Ts interval
E
' Ï„+Ts
Ï„
X2(t) dt
(
;
and the power in the SP

X(t), t âˆˆR

.
Exercise 14.7 (The Power in a Periodic Signal). Let x be a real, Lebesgue measurable,
periodic signal of period Tp > 0. Prove that the power in x is
1
Tp
 Tp
0
x2(t) dt.
Exercise 14.8 (Pre-Encoding). Rather than applying the mapping enc: {0, 1}K â†’RN to
the IID random bits D1, . . . , DK directly, we ï¬rst map the data bits using a one-to-one
mapping Ï†: {0, 1}K â†’{0, 1}K to Dâ€²
1, . . . , Dâ€²
K, and we then map Dâ€²
1, . . . , Dâ€²
K using enc to
X1, . . . , XN. Does this change the transmitted energy?
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

14.7 Exercises
255
Exercise 14.9 (Binary Linear Encoders Producing Pairwise-Independent Symbols). Bi-
nary linear encoders with the antipodal mapping can be described as follows. Using a de-
terministic binary KÃ—N matrix G, the encoder ï¬rst maps the row-vector d = (d1, . . . , dK)
to the row-vector dG, where dG is computed using matrix multiplication over the binary
ï¬eld. (Recall that in the binary ï¬eld multiplication is deï¬ned as 0 Â· 0 = 0 Â· 1 = 1 Â· 0 = 0,
and 1 Â· 1 = 1; and addition is modulo 2, so 0 âŠ•0 = 1 âŠ•1 = 0 and 0 âŠ•1 = 1 âŠ•0 = 1).
Thus, the â„“-th component câ„“of dG is given by
câ„“= d1 Â· g(1,â„“) âŠ•d2 Â· g(2,â„“) âŠ•Â· Â· Â· âŠ•dK Â· g(K,â„“).
The real symbol xâ„“is then computed according to the rule
xâ„“=

+1
if câ„“= 0,
âˆ’1
if câ„“= 1,
â„“= 1, . . . , N.
Let X1, X2, . . . , XN be the symbols produced by the encoder when it is fed IID random
bits D1, D2, . . . , DK. Show that:
(i) Unless all the entries in the â„“-th column of G are zero, E[Xâ„“] = 0.
(ii) The statement â€œXâ„“is independent of Xâ„“â€²â€ is equivalent to the statement â€œthe â„“-th
and â„“â€²-th columns of G diï¬€er or are both all-zero.â€
You may ï¬nd it useful to ï¬rst prove the following.
(i) If a RV E takes value in the set {0, 1}, and if F takes on the values 0 and 1 equiprob-
ably and independently of E, then EâŠ•F is uniform on {0, 1} and independent of E.
(ii) If X and Y are binary random variables and X is uniform, then they are indepen-
dent if, and only if, X âŠ•Y is uniform.
Exercise 14.10 (Zero-Mean Signals for Linearly Dispersive Channels). Suppose that the
transmitted signal X suï¬€ers not only from an additive random disturbance but also
from a deterministic linear distortion. Thus, the received signal Y can be expressed as
Y = X â‹†h + N, where h is a known (deterministic) impulse response, and where N is
an unknown (random) additive disturbance. Show heuristically that transmitting signals
of nonzero mean is power ineï¬ƒcient. How would you mimic the performance of a system
transmitting X(Â·) using a system transmitting X(Â·) âˆ’c(Â·)?
Exercise 14.11 (The Power in Orthogonal Code-Division Multi-Accessing). Suppose that
the data bits

D(1)
j

are mapped to the real symbols

X(1)
â„“

and that the data bits

D(2)
j

are mapped to

X(2)
â„“

. Assume that

A(1)2
Ts
lim
Lâ†’âˆ
1
2L + 1
L

â„“=âˆ’L
E
+
X(1)
â„“
2,
= P(1),
and similarly for P(2). Further assume that the time shifts of Ï† by integer multiples of Ts
are orthonormal and that Ï† satisï¬es the decay condition (14.46). Finally assume that

X(1)
â„“

and

X(2)
â„“

are bounded in the sense of (14.16). Compute the power in the signal
âˆ

â„“=âˆ’âˆ
	
A(1)X(1)
â„“
+ A(2)X(2)
â„“

Ï†

t âˆ’2â„“Ts

+
	
A(1)X(1)
â„“
âˆ’A(2)X(2)
â„“

Ï†

t âˆ’(2â„“+ 1)Ts

.
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,
www.ebook3000.com

256
Energy and Power in PAM
Exercise 14.12 (More on Orthogonal Code-Division Multi-Accessing). Extend the result
of Exercise 14.11 to the case with Î· data streams, where the transmitted signal is given
by
âˆ

â„“=âˆ’âˆ
	
a(1,1)A(1)X(1)
â„“
+ Â· Â· Â· + a(Î·,1)A(Î·)X(Î·)
â„“

Ï†

t âˆ’Î·â„“Ts

+ Â· Â· Â· +
	
a(1,Î·)A(1)X(1)
â„“
+ Â· Â· Â· + a(Î·,Î·)A(Î·)X(Î·)
â„“

Ï†

t âˆ’(Î·â„“+ Î· âˆ’1)Ts

and where the real numbers a(Î¹,Î½) for Î¹, Î½ âˆˆ{1, . . . , Î·} satisfy the orthogonality condition
Î·

Î½=1
a(Î¹,Î½)a(Î¹â€²,Î½) =

Î·
if Î¹ = Î¹â€²,
0
if Î¹ Ì¸= Î¹â€²,
Î¹, Î¹â€² âˆˆ{1, . . . , Î·}.
The sequence a(Î¹,1), . . . , a(Î¹,Î·) is sometimes called the signature of the Î¹-th stream.
Exercise 14.13 (The Samples of the Self-Similarity Function). Let g: R â†’R be of ï¬nite
energy, and let Rgg be its self-similarity function.
(i) Show that there exists an integrable nonnegative function G: [âˆ’1/2, 1/2) â†’[0, âˆ)
such that
Rgg(mTs) =
 1/2
âˆ’1/2
G(Î¸) ei2Ï€mÎ¸ dÎ¸,
m âˆˆZ,
(14.80)
and such that G(âˆ’Î¸) = G(Î¸) for all |Î¸| < 1/2. Express G(Â·) in terms of the FT of g.
(ii) Show that if the samples of the self-similarity function are absolutely summable,
i.e.,

mâˆˆZ
Rgg(mTs)
 < âˆ,
(14.81)
then the function
Î¸ â†’
âˆ

m=âˆ’âˆ
Rgg(mTs) eâˆ’i2Ï€mÎ¸,
Î¸ âˆˆ[âˆ’1/2, 1/2),
(14.82)
is such a function and is continuous.
(iii) Assuming the above summability, show that if

Xâ„“

is of PSD SXX, then the RHS
of (14.31) can be expressed as
1
Ts A2
 1/2
âˆ’1/2
G(Î¸) SXX(Î¸) dÎ¸.
(14.83)
Exercise 14.14 (A Bound on the Power in PAM). Let g be as in Exercise 14.13, and let
its self-similarity function satisfy (14.81). Let G(Â·) be the mapping in (14.82) so that the
RHS of (14.31) can be expressed as in (14.83).
(i) Show that if

Xâ„“

is of zero mean, of unit variance, and has a PSD, then the RHS
of (14.31) is upper-bounded by
1
Ts A2
max
âˆ’1/2â‰¤Î¸<1/2 G(Î¸).
(14.84)
(ii) Show that for every Ïµ > 0, there exists a zero-mean unit-variance SP

Xâ„“

with a
PSD for which the RHS of (14.31) is within Ïµ of (14.84).
available at https:/www.cambridge.org/core/terms. https://doi.org/10.1017/9781316822708.016
Downloaded from https:/www.cambridge.org/core. University of Sydney Library, on 05 Jun 2017 at 02:25:00, subject to the Cambridge Core terms of use,

Chapter 15
Operational Power Spectral Density
15.1
Introduction
The Power Spectral Density of a stochastic process tells us more about the SP than
just its power. It tells us something about how this power is distributed among
the diï¬€erent frequencies that the SP occupies. The purpose of this chapter is to
clarify this statement and to derive the PSD of PAM signals. Most of this chapter
is written informally with an emphasis on ideas and intuition as opposed to math-
ematical rigor. The mathematically-inclined readers will ï¬nd precise statements
of the key results of this chapter in Section 15.5. We emphasize that this chapter
only deals with real continuous-time stochastic processes.
The classical deï¬nition of the PSD of continuous-time stochastic processes (Deï¬ni-
tion 25.7.2 ahead) is only applicable to wide-sense stationary stochastic processes,
and PAM signals are not WSS.1 Consequently, we shall have to introduce a new
concept, which we call the operational power spectral density, or the op-
erational PSD for short.2 This new concept is applicable to a large family of
stochastic processes that includes most WSS processes and most PAM signals.
For WSS stochastic processes, the operational PSD and the classical PSD coin-
cide (Section 25.14). In addition to being more general, the operational PSD is
more intuitive in that it clariï¬es the origin of the words â€œpower spectral density.â€
Moreover, it gives an operational meaning to the concept.
Section 15.2 provides some motivation for our deï¬nition of the operational PSD.
Readers in a rush to get to the deï¬nition can skip it and jump directly to Sec-
tion 15.3.
Section 15.4 derives the operational PSD of PAM signals, and Sec-
tion 15.5 does so a bit more rigorously. Sections 15.6â€“15.8 expand on the oper-
ational PSD. Section 15.6 explores its relationship to the average autocovariance
function; Section 15.7 discusses the eï¬€ect of ï¬ltering on the operational PSD; and
Section 15.8 explores the technical assumptions that are required for the power to
equal the integral of the operational PSD over all the frequencies.
1If the discrete-time symbol sequence is stationary then the PAM signal is cyclostationary.
But this term will not be used in this book.
2These terms are not standard. Most of the literature does not seem to distinguish between
the PSD in the sense of Deï¬nition 25.7.2 and what we call the operational PSD.
257
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

258
Operational Power Spectral Density
function
quantity of interest
per unit of
charge (spatial) density
charge
space
mass (spatial) density
mass
space
mass line density
mass
length
probability (per unit of X) density
probability
unit of X
power spectral density
power
spectrum (Hz)
Table 15.1: Various densities and their units.
15.2
Motivation
To motivate the new deï¬nition we shall ï¬rst brieï¬‚y discuss other â€œdensitiesâ€ such
as charge density, mass density, and probability density.
In electromagnetism one encounters the concept of charge density, which is often
denoted by Ï±(Â·). It measures the amount of charge per unit volume. Since the
charge need not be uniformly distributed, Ï±(Â·) is typically not constant so the charge
density is a function of location. Thus, we usually write Ï±(x, y, z) for the charge
density at the location (x, y, z). This can be deï¬ned diï¬€erentially or integrally.
The diï¬€erential deï¬nition is
Ï±(x, y, z)
= lim
Î”â†“0
Charge in Box

(xâ€², yâ€², zâ€²) : |x âˆ’xâ€²| â‰¤Î”
2 , |y âˆ’yâ€²| â‰¤Î”
2 , |z âˆ’zâ€²| â‰¤Î”
2

Volume of Box

(xâ€², yâ€², zâ€²) : |x âˆ’xâ€²| â‰¤Î”
2 , |y âˆ’yâ€²| â‰¤Î”
2 , |z âˆ’zâ€²| â‰¤Î”
2

= lim
Î”â†“0
Charge in box

(xâ€², yâ€², zâ€²) : |x âˆ’xâ€²| â‰¤Î”
2 , |y âˆ’yâ€²| â‰¤Î”
2 , |z âˆ’zâ€²| â‰¤Î”
2

Î”3
,
and the integral deï¬nition is that a function Ï±(Â·) is the charge density if for every
region D âŠ‚R3
Charge in D =

(x,y,z)âˆˆD
Ï±(x, y, z) dx dy dz,
D âŠ‚R3.
Ignoring some mathematical subtleties, the two deï¬nitions are equivalent. Perhaps
a more appropriate name for charge density is â€œCharge Spatial Density,â€ which
makes it clear that the quantity of interest is charge and that we are interested in
the way it is distributed in space. The units of Ï±(x, y, z) are those of charge per
unit volume.
Mass densityâ€”or as we would prefer to call it, â€œMass Spatial Densityâ€â€”is analo-
gously deï¬ned. Either diï¬€erentially, as
Ï±(x, y, z)
= lim
Î”â†“0
Mass in Box

(xâ€², yâ€², zâ€²) : |x âˆ’xâ€²| â‰¤Î”
2 , |y âˆ’yâ€²| â‰¤Î”
2 , |z âˆ’zâ€²| â‰¤Î”
2

Volume of Box

(xâ€², yâ€², zâ€²) : |x âˆ’xâ€²| â‰¤Î”
2 , |y âˆ’yâ€²| â‰¤Î”
2 , |z âˆ’zâ€²| â‰¤Î”
2

= lim
Î”â†“0
Mass in box

(xâ€², yâ€², zâ€²) : |x âˆ’xâ€²| â‰¤Î”
2 , |y âˆ’yâ€²| â‰¤Î”
2 , |z âˆ’zâ€²| â‰¤Î”
2

Î”3
,
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.2 Motivation
259
or integrally as the function Ï±(x, y, z) such that for every subset D âŠ‚R3
Mass in D =

(x,y,z)âˆˆD
Ï±(x, y, z) dx dy dz,
D âŠ‚R3.
The units are those of mass per unit volume. Since mass is nonnegative, the dif-
ferential deï¬nition of mass density makes it clear that mass density must also be
nonnegative. This is slightly less apparent from the integral deï¬nition, but (exclud-
ing subsets of R3 of Lebesgue measure zero) is true nonetheless. By convention, if
one deï¬nes mass density integrally, then one typically insists that the density be
nonnegative.
Similarly, in discussing mass line density one envisions a one-dimensional object,
and its density with respect to unit length is deï¬ned diï¬€erentially as
Ï±(x) = lim
Î”â†“0
Mass in Interval

xâ€² : |x âˆ’xâ€²| â‰¤Î”
2

Î”
,
or integrally as the nonnegative function Ï±(Â·) such that for every subset D âŠ‚R of
the real line
Mass in D =

xâˆˆD
Ï±(x) dx,
D âŠ‚R.
The units are those of mass per unit length.
In probability theory one encounters the probability density function of a random
variable X. Here the quantity of interest is probability, and we are interested in
how it is distributed on the real line. The units depend on the units of X. Thus, if
X measures the time in days until at least one piece in your new china set breaks,
then the units of the probability density function fX(Â·) of X are those of probability
(unit-less) per day. The probability density function can be deï¬ned diï¬€erentially
as
fX(x) = lim
Î”â†“0
Pr

X âˆˆ

x âˆ’Î”
2 , x + Î”
2

Î”
or integrally by requiring that for every subset E âŠ‚R
Pr[X âˆˆE] =

xâˆˆE
fX(x) dx,
E âŠ‚R.
(15.1)
Again, since probabilities are nonnegative, the diï¬€erential deï¬nition makes it clear
that the probability density function is nonnegative. In the integral deï¬nition we
typically add the nonnegativity as a condition. That is, we say that fX(Â·) is a
density function for the random variable X if fX(Â·) is nonnegative and if (15.1)
holds. (There is a technical uniqueness issue that we are sweeping under the rug
here: if fX(Â·) is a probability density function for X and if Î¾(Â·) is a nonnegative
function that diï¬€ers from fX(Â·) only on a set of Lebesgue measure zero, then Î¾(Â·)
is also a probability density function for X.)
With these examples in mind, it is natural to interpret the power spectral density
of a stochastic process

X(t), t âˆˆR

as the distribution of the power of X(Â·)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

260
Operational Power Spectral Density
among the diï¬€erent frequencies. See Table 15.1 on Page 258. Heuristically, we
would deï¬ne the power spectral density SXX at the frequency f diï¬€erentially as
SXX(f) = lim
Î”â†“0
Power in the frequencies

f âˆ’Î”
2 , f + Î”
2

Î”
or integrally by requiring that for any subset D of the spectrum
Power of X in D =

fâˆˆD
SXX(f) df,
D âŠ‚R.
(15.2)
To make this meaningful we next explain what we mean by â€œthe power of X in
the frequencies D.â€ To that end it is best to envision a ï¬lter of impulse response h
whose frequency response Ë†h is given by
Ë†h(f) =

1
if f âˆˆD,
0
otherwise,
(15.3)
and to think of the power of X(Â·) in the frequencies D as the power at the output
of that ï¬lter when it is fed X(Â·), i.e., the power of the stochastic process X â‹†h.3
We are now almost ready to give a heuristic deï¬nition of the power spectral density.
But there are three more points we would like to discuss ï¬rst. The ï¬rst is that
(15.2) can also be rewritten as
Power of X in D =

all frequencies
I{f âˆˆD} SXX(f) df,
D âŠ‚R.
(15.4)
It turns out that if (15.2) holds for all sets D âŠ‚R of frequencies, then it also holds
for all â€œniceâ€ ï¬lters (of a frequency response that is not necessarily {0, 1} valued):
Power of X â‹†h =

all frequencies
|Ë†h(f)|2 SXX(f) df,
h â€œnice.â€
(15.5)
That (15.4) typically implies (15.5) can be heuristically argued as follows.
By
(15.4) the set of frequency responses Ë†h for which (15.5) holds includes all frequency
responses of the form Ë†h(f) = I{f âˆˆD}. But if (15.5) holds for some frequency
response Ë†h, then it must also hold for Î± Ë†h, where Î± is any complex number, because
scaling the frequency response by Î± merely multiplies the output power by |Î±|2.
Also, if (15.5) holds for two responses Ë†h1 and Ë†h2 for which
Ë†h1(f) Ë†h2(f) = 0,
f âˆˆR,
(15.6)
then it must also hold for h1 + h2, because Parsevalâ€™s Theorem and (15.6) imply
that X â‹†h1 and X â‹†h2 must be orthogonal. Thus, (15.6) implies that the power
in X â‹†(h1 + h2) is the sum of the power in X â‹†h1 and the power in X â‹†h2. It
thus intuitively follows that if (15.4) holds for all subsets D of the spectrum, then
it holds for all step functions Ë†h(f) = 
Î½ Î±Î½ I{f âˆˆDÎ½}, where {DÎ½} are disjoint.
3We are ignoring the fact that the RHS of (15.3) is typically not the frequency response of a
stable ï¬lter. Stable ï¬lters have (uniformly) continuous frequency responses (Theorem 6.2.11 (i)).
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.2 Motivation
261
And since any â€œniceâ€ frequency response Ë†h can be arbitrarily well approximated
by such step functions, we expect that (15.5) would hold for all â€œniceâ€ responses.
Having heuristically established that (15.2) implies (15.5), we prefer to deï¬ne the
PSD as a function SXX for which (15.5) holds, where â€œniceâ€ will be taken to mean
stable.
The second point we would like to make is regarding uniqueness. For real stochastic
processes it is reasonable to require that (15.5) hold only for ï¬lters of real impulse
response. Thus we would require
Power of X â‹†h =

all frequencies
|Ë†h(f)|2 SXX(f) df,
h real and â€œnice.â€
(15.7a)
But since for ï¬lters of real impulse response the mapping f 	â†’|Ë†h(f)|2 is symmetric,
(15.7a) can be rewritten as
 âˆ
0
|Ë†h(f)|2 
SXX(f) + SXX(âˆ’f)

df,
h real and â€œnice.â€
(15.7b)
This form makes it clear that for real stochastic processes, (15.7a) (or its equivalent
form (15.7b)) can only specify the function f 	â†’SXX(f)+SXX(âˆ’f); it cannot fully
specify the mapping f 	â†’SXX(f).
For example, if a symmetric function SXX
satisï¬es (15.7a), then so does
f 	â†’

2 SXX(f)
if f > 0,
0
otherwise,
f âˆˆR.
In fact, if SXX satisï¬es (15.7a), then so does any function ËœS(Â·) such that
ËœS(f) + ËœS(âˆ’f) = SXX(f) + SXX(âˆ’f),
f âˆˆR.
Thus, for the sake of uniqueness, we deï¬ne the power spectral density SXX to be
a function of frequency that satisï¬es (15.7a) and that is additionally symmetric.
It can be shown that this deï¬nes SXX (to within indistinguishability) uniquely.
In fact, once one has identiï¬ed a nonnegative function S(Â·) such that for any real
impulse response h the integral
 âˆ
âˆ’âˆ
S(f) |Ë†h(f)|2 df
corresponds to the power in X â‹†h, then the PSD SXX of X is given by the sym-
metrized version of S(Â·), i.e.,
SXX(f) = 1
2

S(f) + S(âˆ’f)

,
f âˆˆR.
(15.8)
Note that the diï¬€erential deï¬nition of the PSD would not have resolved the unique-
ness issue because a ï¬lter of frequency response f 	â†’I

f âˆˆ

f0 âˆ’Î”
2 , f0 + Î”
2

is
not real.
The ï¬nal point we would like to make is regarding additivity. Apart from some
mathematical details, what makes the deï¬nition of charge density possible is the
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

262
Operational Power Spectral Density
fact that the total charge in the union of two disjoint regions in space is the sum
of charges in the individual regions. The same holds for mass. For the probability
densities the crucial property is that the probability of the union of two disjoint
events is the sum of the probabilities. Consequently, if D1 and D2 are disjoint
subsets of R, then Pr[X âˆˆD1 âˆªD2] = Pr[X âˆˆD1] + Pr[X âˆˆD2].
Does this
hold for power? In general the power in the sum of two signals is not the sum of
the individual powers. But if the signals are orthogonal, then their powers do add.
Thus, while Parsevalâ€™s theorem will not appear explicitly in our analysis of the PSD,
it is really what makes it all possible. It demonstrates that if D1, D2 âŠ‚R are disjoint
frequency bands, then the signals X â‹†h1 and X â‹†h2 that result when X is passed
through the ï¬lters of frequency response Ë†h1(f) = I{f âˆˆD1} and Ë†h2(f) = I{f âˆˆD2}
are orthogonal, so their powers add. We will not bother to formulate this result
precisely, because it does not show up in our analysis explicitly, but it is this result
that allows us to deï¬ne the power spectral density.
15.3
Deï¬ning the Operational PSD
Recall that in (14.14) we deï¬ned the power P in a SP

Y (t), t âˆˆR

as
P = lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
Y 2(t) dt
6
whenever the limit exists. Thus, the power is the limit, as T tends to inï¬nity, of
the ratio of the expected energy in the interval [âˆ’T, T] to the intervalâ€™s duration 2T.
We deï¬ne the operational power spectral density of a stochastic process as follows.
Deï¬nition 15.3.1 (Operational PSD of a Real SP). We say that the continuous-
time real stochastic process

X(t), t âˆˆR

is of operational power spectral
density SXX if

X(t), t âˆˆR

is a measurable SP; the mapping SXX : R â†’R is
integrable and symmetric; and for every stable real ï¬lter of impulse response h âˆˆL1
the power at the ï¬lterâ€™s output when it is fed

X(t), t âˆˆR

is given by
Power in X â‹†h =
 âˆ
âˆ’âˆ
SXX(f) |Ë†h(f)|2 df.
(15.9)
We chose our words very carefully in the above deï¬nition, and, in doing so, we
avoided two issues. The ï¬rst is whether every SP is of some operational PSD. The
answer is â€œno.â€ (But most stochastic processes encountered in Digital Communi-
cations are.) The second issue we avoided is the uniqueness issue. Our wording
did not indicate whether a SP could be of two diï¬€erent operational PSDs. It turns
out that if a SP is of two diï¬€erent operational PSDs, then they must be indistin-
guishable in the sense that they are equal outside a set of frequencies of Lebesgue
measure zero. Consequently, somewhat loosely, we shall speak of the operational
power spectral density of

X(t), t âˆˆR

even though the uniqueness is only to
within indistinguishability. The uniqueness is established in Corollary 15.3.6 af-
ter we discuss nonnegativity, which is a consequence of the following somewhat
technical lemma.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.3 Deï¬ning the Operational PSD
263
Lemma 15.3.2.
(i) If s: R â†’R is an integrable function such that
 âˆ
âˆ’âˆ
s(f) |Ë†h(f)|2 df â‰¥0
(15.10)
for every integrable complex function h: R â†’C, then s(f) is nonnegative
at every frequency f outside a set of frequencies of Lebesgue measure zero.
(ii) If s: R â†’R is a symmetric function (~s = s) such that (15.10) holds for
every integrable real function h: R â†’R, then s(f) is nonnegative at every
frequency f outside a set of frequencies of Lebesgue measure zero.
Proof. We begin with a proof of Part (i). For every Î» > 0 and f0 âˆˆR deï¬ne the
function h: R â†’C by
h(t) =
1
âˆš
Î»
I
'
|t| â‰¤Î»
2
(
ei2Ï€f0t,
t âˆˆR.
(15.11)
This function is in both L1 and L2.
Since it is in L2, its self-similarity func-
tion Rhh(Ï„) is deï¬ned at every Ï„ âˆˆR. In fact,
Rhh(Ï„) =
	
1 âˆ’|Ï„|
Î»

I{|Ï„| â‰¤Î»} ei2Ï€f0Ï„,
Ï„ âˆˆR.
(15.12)
And since h âˆˆL1, it follows from (11.35) that the Fourier Transform of Rhh is the
mapping f 	â†’|Ë†h(f)|2. Consequently, by Proposition 6.2.3 (i) (with the substitution
of ~Rhh for g), the mapping f 	â†’|Ë†h(f)|2 can be expressed as the Inverse Fourier
Transform of ~Rhh. Thus, by (6.8) (with the substitutions of s for x and ~Rhh for g),
 âˆ
âˆ’âˆ
s(f) |Ë†h(f)|2 df =
 âˆ
âˆ’âˆ
Ë†s(f) ~Râˆ—
hh(f) df.
(15.13)
It now follows from (15.10), (15.13), and (15.12) that
 Î»
âˆ’Î»
	
1 âˆ’|f|
Î»

Ë†s(f) ei2Ï€f0f df â‰¥0,
Î» > 0, f0 âˆˆR.
(15.14)
Part (i) now follows from (15.14) and from Theorem 6.2.12 (ii) (with the substitu-
tion of s for x and of f0 for t).
We next turn to Part (ii). For any integrable complex function h: R â†’C, deï¬ne
hR â‰œRe(h) and hI â‰œIm(h) so
Ë†hR(f) =
Ë†h(f) + Ë†hâˆ—(âˆ’f)
2
,
f âˆˆR,
Ë†hI(f) =
Ë†h(f) âˆ’Ë†hâˆ—(âˆ’f)
2i
,
f âˆˆR.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

264
Operational Power Spectral Density
Consequently,
Ë†hR(f)
2 = 1
4
Ë†h(f)
2 +
Ë†h(âˆ’f)
2 + 2 Re
Ë†h(f) Ë†h(âˆ’f)

,
f âˆˆR
Ë†hI(f)
2 = 1
4
Ë†h(f)
2 +
Ë†h(âˆ’f)
2 âˆ’2 Re
Ë†h(f) Ë†h(âˆ’f)

,
f âˆˆR,
and
Ë†hR(f)
2 +
Ë†hI(f)
2 = 1
2
Ë†h(f)
2 +
Ë†h(âˆ’f)
2
,
f âˆˆR.
(15.15)
Applying the lemmaâ€™s hypothesis (15.10) to the real functions hR and hI we obtain
0 â‰¤
 âˆ
âˆ’âˆ
s(f)
Ë†hR(f)
2 df,
0 â‰¤
 âˆ
âˆ’âˆ
s(f)
Ë†hI(f)
2 df,
and thus, upon adding the equations,
0 â‰¤
 âˆ
âˆ’âˆ
s(f)
Ë†hR(f)
2 +
Ë†hI(f)
2
df
= 1
2
 âˆ
âˆ’âˆ
s(f)
Ë†h(f)
2 +
Ë†h(âˆ’f)
2
df
=
 âˆ
âˆ’âˆ
s(f) + s(âˆ’f)
2
Ë†h(f)
2 df
=
 âˆ
âˆ’âˆ
s(f)
Ë†h(f)
2 df,
(15.16)
where the second equality follows from (15.15); the third by writing the integral
of the sum as a sum of integrals and by changing the integration variable in the
integral involving Ë†h(âˆ’f); and the last equality from the hypothesis that s is sym-
metric. Since we have established (15.16) for every complex h: R â†’C, we can
now apply Part (i) to conclude that s(f) is nonnegative at all frequencies f outside
a set of Lebesgue measure zero.
Going back to the deï¬nition of the operational PSD and speciï¬cally to (15.9),
and noting that the power in X â‹†h is nonnegative, we conclude that if X is of
operational PSD SXX, then SXX is symmetric and satisï¬es
 âˆ
âˆ’âˆ
SXX(f) |Ë†h(f)|2 df â‰¥0,
(15.17)
for every integrable h: R â†’R. From this and Lemma 15.3.2 (ii) we conclude:
Corollary 15.3.3 (The Operational PSD Is Nonnegative). If

X(t), t âˆˆR

is of
operational PSD SXX, then SXX(f) must be nonnegative at all frequencies f outside
a set of frequencies of Lebesgue measure zero.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.3 Deï¬ning the Operational PSD
265
It follows from the corollary that if

X(t)

is of operational PSD SXX then the
function
S(f) =

SXX(f)
if SXX(f) â‰¥0,
0
otherwise
(which results when we change the values of SXX to zero at those frequencies where
it is negative) is indistinguishable from SXX. Consequently, (15.9) must also hold
when we replace SXX with S. Thus,

X(t)

is also of operational PSD S, which
is nonnegative at all frequencies.
Consequently, there is no harm in adding to
Deï¬nition 15.3.1 the requirement that SXX be a nonnegative function:
Remark 15.3.4. Henceforth we require from every operational PSD that it be
nonnegative.
Next, as promised, we address the uniqueness of the operational PSD. For that
we will need the following lemma, which is similar to Lemma 15.3.2 but with the
greater-or-equal sign in (15.10) replaced with equality.
Lemma 15.3.5.
(i) If s: R â†’R is an integrable function such that
 âˆ
âˆ’âˆ
s(f) |Ë†h(f)|2 df = 0
(15.18)
for every integrable complex function h: R â†’C, then s(f) is zero at all
frequencies f outside a set of Lebesgue measure zero.
(ii) If s: R â†’R is a symmetric function (~s = s) such that (15.18) holds for
every integrable real function h: R â†’R, then s(f) is zero at all frequencies
f outside a set of Lebesgue measure zero.
Proof. The equality sign in (15.18) implies that if s satisï¬es the hypotheses of the
lemma then both s and âˆ’s satisfy the hypotheses of Lemma 15.3.2. Consequently,
by the latter lemma, s as well as âˆ’s is nonnegative outside an exception set of
frequencies of Lebesgue measure zero. Outside the union of their respective excep-
tion setsâ€”a union which is also of Lebesgue measure zero (Exercise 2.11)â€”both s
and âˆ’s are nonnegative, and s must therefore be zero.
Corollary 15.3.6 (Uniqueness of the Operational PSD). If both SXX and Sâ€²
XX are
operational PSDs of some SP, then the set of frequencies at which they diï¬€er is of
Lebesgue measure zero.
Proof. Apply Lemma 15.3.5 (ii) to the function s: f 	â†’SXX(f) âˆ’Sâ€²
XX(f).
As noted above, we make here no general claims about the existence of opera-
tional PSDs. Under certain restrictions that are made precise in Section 15.5, the
operational PSD is deï¬ned for PAM signals. And by Proposition 25.7.1 and The-
orem 25.14.1, the operational PSD always exists for measurable, centered, WSS
stochastic processes of continuous and integrable autocovariance functions.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

266
Operational Power Spectral Density
Deï¬nition 15.3.7 (Bandlimited Stochastic Processes). A SP

X(t), t âˆˆR

of
operational PSD SXX is said to be bandlimited to W Hz if, except on a set of
frequencies of Lebesgue measure zero, SXX(f) is zero whenever |f| > W.
The smallest W to which

X(t), t âˆˆR

is bandlimited is its bandwidth.
15.4
The Operational PSD of Real PAM Signals
Computing the operational PSD of PAM signals is much easier than one might
expect. This is because, as we next show, passing a PAM signal of pulse shape g
through a stable ï¬lter of impulse response h is tantamount to changing its pulse
shape from g to g â‹†h:
	
Ïƒ 	â†’A

â„“
Xâ„“g(Ïƒ âˆ’â„“Ts)

â‹†h

(t) = A

â„“
Xâ„“(g â‹†h)(t âˆ’â„“Ts),
t âˆˆR. (15.19)
(For a formal statement of this result, see Corollary 18.6.2, which also addresses the
diï¬ƒculty that arises when the sum is inï¬nite.) Consequently, if one can compute
the power in a PAM signal of arbitrary pulse shape (as explained in Chapter 14),
then one can also compute the power in a ï¬ltered PAM signal.
That ï¬ltering a PAM signal is tantamount to convolving its pulse shape with the
impulse response follows from two properties of the convolution: that it is linear
(Î± u + Î² v) â‹†h = Î± u â‹†h + Î² v â‹†h
and that convolving a delayed version of a signal with h is equivalent to convolving
the original signal and delaying the result

Ïƒ 	â†’u(Ïƒ âˆ’t0)

â‹†h

(t) = (u â‹†h)(t âˆ’t0),
t, t0 âˆˆR.
Indeed, if X is the PAM signal
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts),
(15.20)
then (15.19) follows from the calculation

X â‹†h

(t) =
	
Ïƒ 	â†’A
âˆ

â„“=âˆ’âˆ
Xâ„“g(Ïƒ âˆ’â„“Ts)

â‹†h

(t)
= A
âˆ

â„“=âˆ’âˆ
Xâ„“
 âˆ
âˆ’âˆ
h(s) g(t âˆ’s âˆ’â„“Ts) ds
= A
âˆ

â„“=âˆ’âˆ
Xâ„“(g â‹†h)(t âˆ’â„“Ts),
t âˆˆR.
(15.21)
We are now ready to apply the results of Chapter 14 on the power in PAM signals
to study the power in ï¬ltered PAM signals and hence to derive the operational
PSD of PAM signals. We will not treat the case discussed in Section 14.5.3 where
the only assumption is that the time shifts of the pulse shape by integer multiples
of Ts are orthonormal, because this orthonormality is typically lost under ï¬ltering.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.4 The Operational PSD of Real PAM Signals
267
15.4.1
The Symbols Are Centered, Uncorrelated, and of Equal
Variance
We begin with the case where the symbols

Xâ„“, â„“âˆˆZ

are of zero mean, uncor-
related, and of equal variance Ïƒ2
X. As in (15.20) we denote the PAM signal by

X(t), t âˆˆR

and study its operational PSD by studying the power in X â‹†h.
Using (15.21) we obtain that Xâ‹†h is the PAM signal X but with the pulse shape g
replaced by g â‹†h. Consequently, using Expression (14.33) for the power in PAM
with zero-mean, uncorrelated, variance-Ïƒ2
X symbols, we obtain that the power in
X â‹†h is given by
Power in X â‹†h = A2
Ts
Ïƒ2
X âˆ¥g â‹†hâˆ¥2
2
= A2Ïƒ2
X
Ts
 âˆ
âˆ’âˆ
|Ë†g(f)|2 |Ë†h(f)|2 df
=
 âˆ
âˆ’âˆ
	A2Ïƒ2
X
Ts
|Ë†g(f)|2



SXX(f)

|Ë†h(f)|2 df,
(15.22)
where the ï¬rst equality follows from (14.33) applied to the PAM signal of pulse
shape gâ‹†h; the second follows from Parsevalâ€™s Theorem by noting that the Fourier
Transform of a convolution of two signals is the product of their Fourier Transforms;
and where the third equality follows by rearranging terms. From (15.22) and from
the fact that f 	â†’|Ë†g(f)|2 is a symmetric function (because g is real), it follows
that the operational PSD of the PAM signal

X(t), t âˆˆR

when

Xâ„“, â„“âˆˆZ

are
zero-mean, uncorrelated, and of variance Ïƒ2
X is given by
SXX(f) = A2Ïƒ2
X
Ts
|Ë†g(f)|2,
f âˆˆR.
(15.23)
15.4.2

Xâ„“

Is Centered and WSS
The more general case where the symbols

Xâ„“, â„“âˆˆZ

are not necessarily un-
correlated but form a centered, WSS, discrete-time SP can be treated with the
same ease via (14.31) or (14.32). As above, passing X through a ï¬lter of impulse
response h results in a PAM signal with identical symbols but with pulse shape
g â‹†h. Consequently, the resulting power can be computed by substituting g â‹†h
for g in (14.32) to obtain that the power in X â‹†h is given by
Power in X â‹†h =
 âˆ
âˆ’âˆ
	A2
Ts
âˆ

m=âˆ’âˆ
KXX(m) ei2Ï€fmTs |Ë†g(f)|2



SXX(f)

|Ë†h(f)|2 df,
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

268
Operational Power Spectral Density
where again we are using the fact that the FT of g â‹†h is f 	â†’Ë†g(f) Ë†h(f). The
operational PSD is thus
SXX(f) = A2
Ts
âˆ

m=âˆ’âˆ
KXX(m) ei2Ï€fmTs |Ë†g(f)|2,
f âˆˆR,
(15.24)
because, as we next argue, the RHS of the above is a symmetric function of f.
This symmetry follows from the symmetry of |Ë†g(Â·)| (because the pulse shape g
is real) and from the symmetry of the autocovariance function KXX (because the
symbols

Xâ„“, â„“âˆˆZ

are real; see (13.12)). Note that (15.24) reduces to (15.23) if
KXX(m) = Ïƒ2
X I{m = 0}.
15.4.3
The Operational PSD in Bi-Inï¬nite Block-Mode
We now assume, as in Section 14.5.2, that the (K, N) binary-to-reals block encoder
enc: {0, 1}K â†’RN is used in bi-inï¬nite block encoding mode to map the bi-
inï¬nite IID random bits

Dj, j âˆˆZ

to the bi-inï¬nite sequence of real numbers

Xâ„“, â„“âˆˆZ

, and that the transmitted signal is
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts),
(15.25)
where Ts > 0 is the baud period, and where g(Â·) is a pulse shape satisfying the
decay condition (14.17). We do not assume that the time shifts of g(Â·) by integer
multiples of Ts are orthogonal, or that the symbols

Xâ„“, â„“âˆˆZ

are uncorrelated.
We do, however, continue to assume that the N-tuple enc(D1, . . . , DK) is of zero
mean whenever D1, . . . , DK are IID random bits.
We shall determine the operational PSD of X by computing the power of the signal
that results when X is fed to a stable ï¬lter of impulse response h. As before, we note
that feeding X through a ï¬lter of impulse response h is tantamount to replacing
its pulse shape g by g â‹†h. The power of this output signal can be thus computed
from our expression for the power in bi-inï¬nite block encoding with PAM signaling
(14.38) but with the pulse shape being g â‹†h and hence of FT f 	â†’Ë†g(f) Ë†h(f):
Power in X â‹†h =
 âˆ
âˆ’âˆ
	 A2
NTs
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] ei2Ï€f(â„“âˆ’â„“â€²)Ts |Ë†g(f)|2



SXX(f)

|Ë†h(f)|2 df.
As we next show, the underbraced term is a symmetric function of f, and we thus
conclude that the PSD of X is:
SXX(f) = A2
NTs
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] ei2Ï€f(â„“âˆ’â„“â€²)Ts |Ë†g(f)|2,
f âˆˆR.
(15.26)
To see that the RHS of (15.26) is a symmetric function of f, use the identities
N

â„“=1
N

â„“â€²=1
aâ„“,â„“â€² =
N

â„“=1
aâ„“,â„“+
N

â„“=2
â„“âˆ’1

â„“â€²=1
(aâ„“,â„“â€² + aâ„“â€²,â„“)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.4 The Operational PSD of Real PAM Signals
269
and E[Xâ„“Xâ„“â€²] = E[Xâ„“â€²Xâ„“] to rewrite the RHS of (15.26) in the symmetric form
A2
NTs
- N

â„“=1
E

X2
â„“

+
N

â„“=2
â„“âˆ’1

â„“â€²=1
2 E[Xâ„“Xâ„“â€²] cos

2Ï€f(â„“âˆ’â„“â€²)Ts

.
|Ë†g(f)|2.
From (15.26) we obtain:
Theorem 15.4.1 (The Bandwidth of PAM Is that of the Pulse Shape). Suppose
that the operational PSD in bi-inï¬nite block-mode of a PAM signal

X(t)

is as
given in (15.26), e.g., that the conditions of Theorem 15.5.2 ahead are satisï¬ed.
Further assume
A > 0,
N

â„“=1
E

X2
â„“

> 0,
(15.27)
e.g., that

X(t)

is not deterministically zero. Then the bandwidth of the SP

X(t)

is equal to the bandwidth of the pulse shape g.
Proof. The proof is very similar to the proof of Lemma 10.10.1. If g is bandlimited
to W Hz, then so is

X(t)

, because, by (15.26),

Ë†g(f) = 0

=â‡’

SXX(f) = 0

.
We next complete the proof by showing that there are at most a countable number
of frequencies f such that SXX(f) = 0 but Ë†g(f) Ì¸= 0.
From (15.26) it follows
that to show this it suï¬ƒces to show that there are at most a countable number of
frequencies f such that Ïƒ(f) = 0, where
Ïƒ(f) â‰œA2
NTs
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] ei2Ï€f(â„“âˆ’â„“â€²)Ts
=
Nâˆ’1

m=âˆ’N+1
Î³m ei2Ï€fmTs
=
Nâˆ’1

m=âˆ’N+1
Î³m zm 
z=ei2Ï€fTs ,
(15.28)
and
Î³m = A2
NTs
min{N,N+m}

â„“=max{1,m+1}
E[Xâ„“Xâ„“âˆ’m] ,
m âˆˆ{âˆ’N + 1, . . . , N âˆ’1}.
(15.29)
It follows from (15.28) that Ïƒ(f) is zero if, and only if, ei2Ï€fTs is a root of the
mapping
z 	â†’
Nâˆ’1

m=âˆ’N+1
Î³m zm.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

270
Operational Power Spectral Density
Since ei2Ï€fTs is of unit magnitude, it follows that Ïƒ(f) is zero if, and only if, ei2Ï€fTs
is a root of the polynomial
z 	â†’
2Nâˆ’2

Î½=0
Î³Î½âˆ’N+1 zÎ½.
(15.30)
From (15.29) and (15.27) it follows that Î³0 > 0, so the polynomial in (15.30) is
not zero. Consequently, since its degree is at most 2N âˆ’2, it has at most 2N âˆ’2
distinct roots and, a fortiori, at most 2N âˆ’2 distinct roots of unit magnitude.
Denote these roots by
eiÎ¸1, . . . , eiÎ¸d,
where d â‰¤2N âˆ’2 and Î¸1, . . . , Î¸d âˆˆ[âˆ’Ï€, Ï€). Since f satisï¬es ei2Ï€fTs = eiÎ¸ if, and
only if,
f =
Î¸
2Ï€Ts
+ Î·
Ts
for some Î· âˆˆZ, we conclude that the set of frequencies f satisfying Ïƒ(f) = 0 is the
set
1 Î¸1
2Ï€Ts
+ Î·
Ts
: Î· âˆˆZ
2
âˆªÂ· Â· Â· âˆª
1 Î¸d
2Ï€Ts
+ Î·
Ts
: Î· âˆˆZ
2
,
and is thus countable. (The union of a ï¬nite (or countable) number of countable
sets is countable.)
15.5
A More Formal Account
In this section we shall give a more formal account of the power at the output of
a stable ï¬lter that is fed a PAM signal. There are two approaches to this. The
ï¬rst is based on carefully justifying the steps in our informal derivation.4
This
approach is pursued in Section 18.6.5, where the results are generalized to complex
pulse shapes and complex symbols. The second approach is to convert the problem
into one about WSS stochastic processes and to then rely heavily on Sections 25.13
and 25.14 on the ï¬ltering of WSS stochastic processes and, in particular, on the
Wiener-Khinchin Theorem (Theorem 25.14.1). For the beneï¬t of readers who have
already encountered the Wiener-Khinchin Theorem we follow this latter approach
here. We ask the readers to note that the Wiener-Khinchin Theorem is not directly
applicable here because the PAM signal is not WSS. A â€œstationarization argumentâ€
is thus needed.
The key results of this section are the following two theorems.
Theorem 15.5.1. Consider the setup of Theorem 14.6.4 with the additional as-
sumption that the autocovariance function KXX of

Xâ„“

is absolutely summable:
âˆ

m=âˆ’âˆ
KXX(m)
 < âˆ.
(15.31)
Let h âˆˆL1 be the impulse response of a stable real ï¬lter. Then:
4The main diï¬ƒculties in the justiï¬cation are in making (15.19) rigorous and in controlling
the decay of g â‹†h for arbitrary h âˆˆL1 .
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.5 A More Formal Account
271
(i) The PAM signal
X: (Ï‰, t) 	â†’A
âˆ

â„“=âˆ’âˆ
Xâ„“(Ï‰) g(t âˆ’â„“Ts)
(15.32)
is bounded in the sense that there exists a constant Î“ such that
|X(Ï‰, t)| < Î“,

Ï‰ âˆˆÎ©, t âˆˆR

.
(15.33)
(ii) For every Ï‰ âˆˆÎ© the convolution of the sample-path t 	â†’X(Ï‰, t) with h is
deï¬ned at every epoch.
(iii) The stochastic process
(Ï‰, t) 	â†’
 âˆ
âˆ’âˆ
x(Ï‰, Ïƒ) h(t âˆ’Ïƒ) dÏƒ,

Ï‰ âˆˆÎ©, t âˆˆR

(15.34)
that results when the sample-paths of X are convolved with h is a measurable
stochastic process of power
P =
 âˆ
âˆ’âˆ
-
A2
Ts
âˆ

m=âˆ’âˆ
KXX(m) ei2Ï€fmTs |Ë†g(f)|2
.
|Ë†h(f)|2 df.
(15.35)
Theorem 15.5.2. Consider the setup of Theorem 14.6.5. Let h âˆˆL1 be the impulse
response of a real stable ï¬lter. Then:
(i) The sample-paths of the PAM stochastic process
X: (Ï‰, t) 	â†’A
âˆ

â„“=âˆ’âˆ
Xâ„“(Ï‰) g(t âˆ’â„“Ts)
(15.36)
are bounded in the sense of (15.33).
(ii) For every Ï‰ âˆˆÎ© the convolution of the sample-path t 	â†’X(Ï‰, t) and h is
deï¬ned at every epoch.
(iii) The stochastic process

X(t), t âˆˆR

â‹†h that results when the sample-paths
of X are convolved with h is a measurable stochastic process of power
P =
 âˆ
âˆ’âˆ
-
A2
NTs
N

â„“=1
N

â„“â€²=1
E[Xâ„“Xâ„“â€²] ei2Ï€f(â„“âˆ’â„“â€²)Ts |Ë†g(f)|2
.
|Ë†h(f)|2 df,
(15.37)
where

X1, . . . , XN

= enc

D1, . . . , DK

, and where D1, . . . , DK are IID ran-
dom bits.
Proof of Theorem 15.5.1. Part (i) is a consequence of the assumption that

Xâ„“

is bounded in the sense of (14.16) and that the pulse shape g decays faster than 1/t
in the sense of (14.17).
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

272
Operational Power Spectral Density
Part (ii) is a consequence of the fact that the convolution of a bounded function
with an integrable function is deï¬ned at every epoch; see Section 5.5.
We next turn to Part (iii). The proof of the measurability of the convolution of

X(t), t âˆˆR

with h is a bit technical. It is very similar to the proof of Theo-
rem 25.13.2 (i). As in that proof, we ï¬rst note that it suï¬ƒces to prove the result
for functions h that are Borel measurable; the extension to Lebesgue measurable
functions will then follow by approximating h by a Borel measurable function
that diï¬€ers from it on a set of Lebesgue measure zero (Rudin, 1987, Chapter 8,
Lemma 1 or Chapter 2, Exercise 14) and by then noting that the convolution of
t 	â†’X(Ï‰, t) with h is unaltered when h is replaced by a function that diï¬€ers from
it on a set of Lebesgue measure zero. We thus assume that h is Borel measur-
able. Consequently, the mapping from R2 to R deï¬ned by (t, Ïƒ) 	â†’h(t âˆ’Ïƒ) is also
Borel measurable, because it is the composition of the continuous (and hence Borel
measurable) mapping (t, Ïƒ) 	â†’t âˆ’Ïƒ with the Borel measurable mapping t 	â†’h(t).
As in the proof of Theorem 25.13.2, we prove the measurability of the convolution
of

X(t), t âˆˆR

with h by proving the measurability of the mapping deï¬ned by
(Ï‰, t) 	â†’(1 + t2)âˆ’1  âˆ
âˆ’âˆX(Ï‰, Ïƒ) h(t âˆ’Ïƒ) dÏƒ. To this end we study the function

(Ï‰, t), Ïƒ

	â†’X(Ï‰, Ïƒ) h(t âˆ’Ïƒ)
1 + t2
,

(Ï‰, t) âˆˆÎ© Ã— R, Ïƒ âˆˆR

.
(15.38)
This function is measurable because, as noted above, (t, Ïƒ) 	â†’h(t âˆ’Ïƒ) is measur-
able; because, by Proposition 14.6.2,

X(t), t âˆˆR

is measurable; and because the
product of Borel measurable functions is Borel measurable (Rudin, 1987, Chap-
ter 1, Section 1.9 (c)). Moreover, using (15.33) and Fubiniâ€™s Theorem it can be
readily veriï¬ed that this function is integrable. Using Fubiniâ€™s Theorem again, we
conclude that the function
(Ï‰, t) 	â†’
1
1 + t2
 âˆ
âˆ’âˆ
X(Ï‰, Ïƒ) h(t âˆ’Ïƒ) dÏƒ
is measurable. Consequently, so is X â‹†h.
To conclude the proof we now need to compute the power in the measurable (non-
stationary) SP X â‹†h. This will be done in a roundabout way. We shall ï¬rst deï¬ne
a new SP Xâ€². This SP is centered, measurable, and WSS so the power in Xâ€²â‹†h can
be computed using Theorem 25.14.1. We shall then show that the powers of X â‹†h
and Xâ€² â‹†h are equal and hence that from the power in Xâ€² â‹†h we can immediately
obtain the power in X â‹†h.
We begin by deï¬ning the SP

Xâ€²(t), t âˆˆR

as
Xâ€²(t) = X(t + S),
t âˆˆR,
(15.39a)
where S is independent of

X(t)

and uniformly distributed over the interval [0, Ts],
S âˆ¼U ([0, Ts]) .
(15.39b)
That

Xâ€²(t)

is centered follows from the calculation
E[Xâ€²(t)] = E[X(t + S)]
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.5 A More Formal Account
273
=
 Ts
0
1
Ts
E[X(t + s)] ds
= 0,
where the ï¬rst equality follows from the deï¬nition of

Xâ€²(t)

; the second from the
independence of

X(t)

and S and from the speciï¬c form of the density of S; and
the third because

X(t)

is centered. That

Xâ€²(t)

is measurable follows because
the mapping

(Ï‰, s), t

	â†’X(Ï‰, t + s) can be written as the composition of the
mapping

(Ï‰, s), t

	â†’(Ï‰, t + s) with the mapping (Ï‰, t) 	â†’X(Ï‰, t). And that it is
WSS follows from the calculation
E[Xâ€²(t) Xâ€²(t + Ï„)]
= E[X(t + S) X(t + S + Ï„)]
= 1
Ts
 Ts
0
E[X(t + s) X(t + s + Ï„)] ds
= A2
Ts
 Ts
0
E

âˆ

â„“=âˆ’âˆ
Xâ„“g(t + s âˆ’â„“Ts)
âˆ

â„“â€²=âˆ’âˆ
Xâ„“â€² g(t + s + Ï„ âˆ’â„“â€²Ts)

ds
= A2
Ts
âˆ

â„“=âˆ’âˆ
âˆ

â„“â€²=âˆ’âˆ
E[Xâ„“Xâ„“â€²]
 Ts
0
g(t + s âˆ’â„“Ts) g(t + s + Ï„ âˆ’â„“â€²Ts) ds
= A2
Ts
âˆ

â„“=âˆ’âˆ
âˆ

â„“â€²=âˆ’âˆ
KXX(â„“âˆ’â„“â€²)
 Ts
0
g(t + s âˆ’â„“Ts) g(t + s + Ï„ âˆ’â„“â€²Ts) ds
= A2
Ts
âˆ

â„“=âˆ’âˆ
âˆ

m=âˆ’âˆ
KXX(m)
 Ts
0
g

t + s âˆ’â„“Ts

g

t + s + Ï„ âˆ’(â„“âˆ’m)Ts

ds
= A2
Ts
âˆ

m=âˆ’âˆ
KXX(m)
âˆ

â„“=âˆ’âˆ
 âˆ’â„“Ts+Ts+t
âˆ’â„“Ts+t
g(Î¾) g(Î¾ + Ï„ + mTs) dÎ¾
= A2
Ts
âˆ

m=âˆ’âˆ
KXX(m)
 âˆ
âˆ’âˆ
g(Î¾) g(Î¾ + Ï„ + mTs) dÎ¾
= A2
Ts
âˆ

m=âˆ’âˆ
KXX(m) Rgg(mTs + Ï„),
Ï„, t âˆˆR.
(15.40)
Note that (15.40) also shows that

Xâ€²(t)

is of PSD (as deï¬ned in Deï¬nition 25.7.2)
SXâ€²Xâ€²(f) = A2
Ts
âˆ

m=âˆ’âˆ
KXX(m) ei2Ï€fmTs |Ë†g(f)|2,
f âˆˆR,
(15.41)
which is integrable by the absolute summability of KXX.
Deï¬ning

Y â€²(t), t âˆˆR

to be

Xâ€²(t), t âˆˆR

â‹†h we can now use Theorem 25.14.1
to compute the power in

Y â€²(t), t âˆˆR

:
lim
Tâ†’âˆ
1
2TE
5 T
âˆ’T

Y â€²(t)
2 dt
6
=
 âˆ
âˆ’âˆ
	A2
Ts
âˆ

m=âˆ’âˆ
KXX(m) ei2Ï€fmTs|Ë†g(f)|2

|Ë†h(f)|2 df.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

274
Operational Power Spectral Density
To conclude the proof we next show that the power in Y is the same as the power
in Yâ€². To that end we ï¬rst note that from (15.39a) it follows that

Xâ€² â‹†h

(Ï‰, s), t

=

X â‹†h

(Ï‰, t + s),

Ï‰ âˆˆÎ©, 0 â‰¤s â‰¤Ts, t âˆˆR

,
i.e., that
Y â€²
(Ï‰, s), t

= Y (Ï‰, t + s),

Ï‰ âˆˆÎ©, 0 â‰¤s â‰¤Ts, t âˆˆR

.
(15.42)
It thus follows that
 T
âˆ’T
Y 2(Ï‰, t) dt â‰¤
 T
âˆ’Tâˆ’Ts

Y â€²((Ï‰, s), t)
2 dt,

Ï‰ âˆˆÎ©, 0 â‰¤s â‰¤Ts, t âˆˆR

,
(15.43)
because
 T
âˆ’Tâˆ’Ts

Y â€²((Ï‰, s), t)
2 dt =
 T
âˆ’Tâˆ’Ts
Y 2(Ï‰, t + s) dt
=
 T+s
âˆ’Tâˆ’Ts+s
Y 2(Ï‰, Ïƒ) dÏƒ
â‰¥
 T
âˆ’T
Y 2(Ï‰, Ïƒ) dÏƒ,
0 â‰¤s â‰¤Ts,
where the equality in the ï¬rst line follows from (15.42); the equality in the second
line from the substitution Ïƒ â‰œt+s; and the ï¬nal inequality from the nonnegativity
of the integrand and because 0 â‰¤s â‰¤Ts.
Similarly,
 T
âˆ’T
Y 2(Ï‰, t) dt â‰¥
 Tâˆ’Ts
âˆ’T

Y â€²((Ï‰, s), t)
2 dt,

Ï‰ âˆˆÎ©, 0 â‰¤s â‰¤Ts, t âˆˆR

, (15.44)
because
 Tâˆ’Ts
âˆ’T

Y â€²((Ï‰, s), t)
2 dt =
 Tâˆ’Ts
âˆ’T
Y 2(Ï‰, t + s) dt
=
 Tâˆ’Ts+s
âˆ’T+s
Y 2(Ï‰, Ïƒ) dÏƒ
â‰¤
 T
âˆ’T
Y 2(Ï‰, Ïƒ) dÏƒ,
0 â‰¤s â‰¤Ts.
Combining (15.43) and (15.44) and using the nonnegativity of the integrand we
obtain that for every Ï‰ âˆˆÎ© and s âˆˆ[0, Ts]
 Tâˆ’Ts
âˆ’T+Ts

Y â€²((Ï‰, s), t)
2 dt â‰¤
 T
âˆ’T
Y 2(Ï‰, Ïƒ) dÏƒ â‰¤
 T+Ts
âˆ’Tâˆ’Ts

Y â€²((Ï‰, s), t)
2 dt.
(15.45)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.5 A More Formal Account
275
Dividing by 2T and taking expectations we obtain
2T âˆ’2Ts
2T
1
2T âˆ’2Ts
E
5 Tâˆ’Ts
âˆ’T+Ts

Y â€²(t)
2dt
6
â‰¤1
2T E
5 T
âˆ’T
Y 2(Ïƒ) dÏƒ
6
â‰¤
2T + 2Ts
2T
1
2T + 2Ts
E
5 T+Ts
âˆ’Tâˆ’Ts

Y â€²(t)
2 dt
6
,
(15.46)
from which the equality between the power in Yâ€² and in Y follows by letting T
tend to inï¬nity and using the Sandwich Theorem.
Proof of Theorem 15.5.2. The proof of Theorem 15.5.2 is very similar to the proof
of Theorem 15.5.1, so most of the details will be omitted. The main diï¬€erence is
that the process

Xâ€²(t), t âˆˆR

is now deï¬ned as
Xâ€²(t) = X(t + S)
where the random variable S is now uniformly distributed over the interval [0, NTs],
S âˆ¼U ([0, NTs]) .
With this deï¬nition, the autocovariance of

Xâ€²(t), t âˆˆR

can be computed as
KXâ€²Xâ€²(Ï„)
= E

X(t + S) X(t + Ï„ + S)

=
1
NTs
 NTs
0
E

X(t + s) X(t + Ï„ + s)

ds
= A2
NTs
E
5 NTs
0
	
âˆ

Î½=âˆ’âˆ
u

XÎ½, t + s âˆ’Î½NTs

âˆ

Î½â€²=âˆ’âˆ
u

XÎ½â€², t + Ï„ + s âˆ’Î½â€²NTs

ds
6
= A2
NTs
 NTs
0
âˆ

Î½=âˆ’âˆ
âˆ

Î½â€²=âˆ’âˆ
E

u

XÎ½, t + s âˆ’Î½NTs

u

XÎ½â€², t + Ï„ + s âˆ’Î½â€²NTs

ds
= A2
NTs
 NTs
0
âˆ

Î½=âˆ’âˆ
E

u

XÎ½, t + s âˆ’Î½NTs

u

XÎ½, t + Ï„ + s âˆ’Î½NTs

ds
= A2
NTs
 NTs
0
âˆ

Î½=âˆ’âˆ
E

u

X0, t + s âˆ’Î½NTs

u

X0, t + Ï„ + s âˆ’Î½NTs

ds
= A2
NTs
 âˆ
âˆ’âˆ
E

u

X0, Î¾

u

X0, Î¾ + Ï„

dÎ¾
= A2
NTs
 âˆ
âˆ’âˆ
E
5 N

Î·=1
XÎ· g(Î¾ âˆ’Î·Ts)
N

Î·â€²=1
XÎ·â€² g(Î¾ + Ï„ âˆ’Î·â€²Ts)
6
dÎ¾
= A2
NTs
N

Î·=1
N

Î·â€²=1
E

XÎ·XÎ·â€²
Rgg

Ï„ + (Î· âˆ’Î·â€²)Ts

,
t, Ï„ âˆˆR,
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

276
Operational Power Spectral Density
where the third equality follows from (14.36), (14.39), and (14.40); the ï¬fth follows
from (14.43); the sixth because the N-tuples

XÎ·, Î· âˆˆZ

are IID; the seventh
by deï¬ning Î¾ = t + s, swapping the integration and summation, and expressing
the integral over the real line as an inï¬nite sum of the integrals over the disjoint
intervals

t âˆ’Î½ N Ts, t âˆ’(Î½ âˆ’1) N Ts)

with Î½ âˆˆZ; the eighth by the deï¬nition
(14.40) of the function u(Â·); and the ï¬nal equality by swapping the summations
and the expectation.
The process

Xâ€²(t)

is thus a WSS process of PSD (as deï¬ned in Deï¬nition 25.7.2)
SXâ€²Xâ€²(f) = A2
NTs
N

â„“=1
N

â„“â€²=1
E

Xâ„“Xâ„“â€²
ei2Ï€f(â„“âˆ’â„“â€²)Ts |Ë†g(f)|2.
(15.47)
The proof proceeds now along the same lines as the proof of Theorem 15.5.1.
15.6
Operational PSD and Average Autocovariance Function
The operational PSD can often be calculated from the average autocovariance
function, which we deï¬ne and study next. The main result is Theorem 15.6.6,
which provides an operational meaning to the average autocovariance function and
which relates it to the operational PSD. Its proof is somewhat technical and is
recommended for the more advanced readers. Those should read Chapter 25 ï¬rst.
Throughout this section we restrict ourselves to measurable stochastic processes
that are centered and of bounded variance, where a SP

X(t)

is said to be of
bounded variance if there exists some constant Î³ such that at every epoch t âˆˆR
the variance of the RV X(t) is bounded by Î³:
Var[X(t)] â‰¤Î³,
t âˆˆR.
(15.48)
From the Covariance Inequality (Corollary 3.5.2)
Cov[X(t), X(tâ€²)]
 â‰¤

Var[X(t)] Var[X(tâ€²)],
t, tâ€² âˆˆR,
(15.49)
so

Var[X(t)] â‰¤Î³,
t âˆˆR

=â‡’
Cov[X(t), X(tâ€²)]
 â‰¤Î³,
t, tâ€² âˆˆR

.
(15.50)
We focus on stochastic processes for which the limit deï¬ning Â¯KXX(Ï„) for every
Ï„ âˆˆR as
Â¯KXX(Ï„) â‰œlim
Tâ†’âˆ
1
2T
 T
âˆ’T
Cov[X(t), X(t + Ï„)] dt,
Ï„ âˆˆR
(15.51)
exists and is ï¬nite. This is clearly the case when the integrand does not depend
on t,5 but there are many other cases of interest.
5The integrand does not depend on t whenever

X(t)

is wide-sense stationary; see Deï¬ni-
tion 25.4.2 in Chapter 25.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.6 Operational PSD and Average Autocovariance Function
277
Deï¬nition 15.6.1 (Average Autocovariance Function). We say that a SP

X(t)

is of average autocovariance function Â¯KXX if it is measurable, of bounded
variance, and
lim
Tâ†’âˆ
1
2T
 T
âˆ’T
Cov[X(t), X(t + Ï„)] dt = Â¯KXX(Ï„),
Ï„ âˆˆR.
(15.52)
Substituting 0 for Ï„ in (15.52) and recalling the deï¬nition of power (14.14), we
obtain:
Note 15.6.2 (The Power and the Average Autocovariance Function). If

X(t)

is a centered SP of power P and of average autocovariance function Â¯KXX, then
P = Â¯KXX(0).
(15.53)
An example of a SP that has an average autocovariance function is a PAM signal:
Example 15.6.3 (The Average Autocovariance Function of a PAM Signal). Con-
sider the PAM signal
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts)
(15.54)
that we encountered in Section 15.4.1. Here A is a positive constant; {Xâ„“} are cen-
tered, bounded, uncorrelated, and of variance Ïƒ2
X; the pulse shape g is measurable
and satisï¬es the decay condition (14.17); and the baud period Ts is positive. Since
the symbols {Xâ„“} are centered, so is the SP

X(t)

. We claim that

X(t)

has the
average autocovariance function
Â¯KXX(Ï„) = A2Ïƒ2
X
Ts
Rgg(Ï„),
Ï„ âˆˆR.
(15.55)
Proof. For every t, Ï„ âˆˆR
E

X(t) X(t + Ï„)

= E

A
	
â„“
Xâ„“g(t âˆ’â„“Ts)

	
A

â„“â€²
Xâ„“â€² g(t + Ï„ âˆ’â„“â€²Ts)


= A2 
â„“

â„“â€²
E

Xâ„“Xâ„“â€²
g(t âˆ’â„“Ts) g(t + Ï„ âˆ’â„“â€²Ts)
= A2Ïƒ2
X

â„“
g(t âˆ’â„“Ts) g(t + Ï„ âˆ’â„“Ts),
t, Ï„ âˆˆR,
(15.56)
where in the last equality we used the fact that {Xâ„“} are centered, uncorrelated,
and of variance Ïƒ2
X. To study the limit on the RHS of (15.51), we ï¬rst consider
the integral over an arbitrary length-Ts interval [Ïƒ, Ïƒ + Ts]. Using (15.56),
 Ïƒ+Ts
Ïƒ
E

X(t) X(t + Ï„)

dt = A2Ïƒ2
X
 Ïƒ+Ts
Ïƒ

â„“
g(t âˆ’â„“Ts) g(t + Ï„ âˆ’â„“Ts) dt
= A2Ïƒ2
X

â„“
 Ïƒ+Ts
Ïƒ
g(t âˆ’â„“Ts) g(t + Ï„ âˆ’â„“Ts) dt
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

278
Operational Power Spectral Density
= A2Ïƒ2
X

â„“
 Ïƒâˆ’â„“Ts+Ts
Ïƒâˆ’â„“Ts
g(Î±) g(Î± + Ï„) dÎ±
= A2Ïƒ2
X
 âˆ
âˆ’âˆ
g(Î±) g(Î± + Ï„) dÎ±
= A2Ïƒ2
X Rgg(Ï„),
t, Ï„, Ïƒ âˆˆR,
(15.57)
where in the third equality we have substituted Î± for t âˆ’â„“Ts. Since this holds
for every length-Ts interval, we can establish (15.55) using the same argument we
employed in Section 14.5.1 when we computed the power in PAM; see (14.28)â€“
(14.30).
The main result of this section is that if Â¯KXX is integrable, then its FT is the
operational PSD of

X(t)

. This shows that, when

X(t)

has an integrable average
autocovariance function, our deï¬nition of the operational PSD and the deï¬nition in
the literature of the operational PSD as the FT of Â¯KXX (Yaglom, 1986, Chapter 4,
Section 26.6) coincide. It also provides a method for computing the operational
PSD: compute Â¯KXX and take its FT. In the above example, this allows us to recover
the operational PSD of (15.23) by taking the FT of the RHS of (15.55).
To prove this result we shall need a technical lemma and a proposition describing
the key properties of the average autocovariance function. Readers who intend to
skip the theoremâ€™s proof can also skip these results and proceed directly to the
theorem.
The lemma deals with the integration interval in the deï¬nition of Â¯KXX (15.51).
The integral in (15.51) is over the interval [âˆ’T, T ], which is symmetric around the
origin. The lemma shows that a constant oï¬€set (that does not depend on T) in the
range of integration does not alter the limit:
Lemma 15.6.4. If the SP

X(t)

is of average autocovariance function Â¯KXX, then
for every Ï„ âˆˆR and Î± âˆˆR
lim
Tâ†’âˆ
1
2T
 Tâˆ’Î±
âˆ’Tâˆ’Î±
Cov

X(t), X(t + Ï„)

dt = Â¯KXX(Ï„).
(15.58)
Proof. Since

X(t)

is of average autocovariance function Â¯KXX, it is a fortiori of
bounded variance, and there exists some constant Î³ for which (15.48) holds. It
thus follows from (15.50) that
Cov

X(t), X(tâ€²)
 â‰¤Î³,
t, tâ€² âˆˆR.
(15.59)
With the aid of this inequality we are now ready to prove the lemma:
1
2T
 Tâˆ’Î±
âˆ’Tâˆ’Î±
Cov

X(t), X(t + Ï„)

dt
= 1
2T
 T
âˆ’T
Cov

X(t), X(t + Ï„)

dt + 1
2T
- âˆ’T
âˆ’Tâˆ’Î±
+
 Tâˆ’Î±
T
.
Cov

X(t), X(t + Ï„)

dt,
where, as T tends to inï¬nity, the ï¬rst term on the RHS converges to Â¯KXX(Ï„) by
the deï¬nition of Â¯KXX(Ï„) (15.52), and where the second term on the RHS converges
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.6 Operational PSD and Average Autocovariance Function
279
to zero because the integrals over these short intervals (compared to T) can be
bounded by
1
2T

- âˆ’T
âˆ’Tâˆ’Î±
+
 Tâˆ’Î±
T
.
Cov

X(t), X(t + Ï„)

dt
 â‰¤|Î±| Î³
T
using (15.59) (and Proposition 2.4.1).
In the next proposition we shall need the concept of a positive deï¬nite function,
which is discussed in Appendix C.
Proposition 15.6.5 (Properties of Â¯KXX). If the SP

X(t)

is of average autoco-
variance function Â¯KXX, then Â¯KXX is a measurable, real, positive deï¬nite function.
Consequently
Â¯KXX(Ï„)
 â‰¤Â¯KXX(0),
Ï„ âˆˆR;
(15.60)
Â¯KXX(âˆ’Ï„) = Â¯KXX(Ï„),
Ï„ âˆˆR;
(15.61)
and
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±Î½â€² Â¯KXX(tÎ½ âˆ’tÎ½â€²) â‰¥0,
(15.62)
for every n âˆˆN, epochs t1, . . . , tn âˆˆR, and real coeï¬ƒcients Î±1, . . . , Î±n âˆˆR.
Proof. The measurability of Â¯KXX follows (using Fubiniâ€™s Theorem) from the mea-
surability of the SP

X(t)

and from the fact that the limit of measurable functions
is measurable. The details are omitted.
We shall next establish (15.61) and that (15.62) holds for every n âˆˆN, epochs
t1, . . . , tn âˆˆR, and real coeï¬ƒcients Î±1, . . . , Î±n âˆˆR. From this it will then follow
that Â¯KXX is positive deï¬nite using Proposition C.2. Proposition C.3 (iii) will then
establish (15.60).
To prove the symmetry (15.61) we note that for every Ï„ âˆˆR,
Â¯KXX(âˆ’Ï„) = lim
Tâ†’âˆ
1
2T
 T
âˆ’T
Cov

X(t), X(t âˆ’Ï„)

dt
= lim
Tâ†’âˆ
1
2T
 Tâˆ’Ï„
âˆ’Tâˆ’Ï„
Cov

X(s + Ï„), X(s)

ds
= lim
Tâ†’âˆ
1
2T
 Tâˆ’Ï„
âˆ’Tâˆ’Ï„
Cov

X(s), X(s + Ï„)

ds
= Â¯KXX(Ï„),
where the second equality follows by deï¬ning s as tâˆ’Ï„ and the last equality follows
from Lemma 15.6.4 by replacing Î± there with Ï„.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

280
Operational Power Spectral Density
As to (15.62), for all epochs t1, . . . , tn âˆˆR, all real coeï¬ƒcients Î±1, . . . , Î±n âˆˆR and
any T > 0, the nonnegativity of the variance implies
0 â‰¤1
2T
 T
âˆ’T
Var

n

Î½=1
Î±Î½ X(t + tÎ½)

dt
= 1
2T
 T
âˆ’T
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±Î½â€² Cov

X(t + tÎ½â€²), X(t + tÎ½)

dt
=
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±Î½â€² 1
2T
 T
âˆ’T
Cov

X(t + tÎ½â€²), X(t + tÎ½)

dt
=
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±Î½â€² 1
2T
 T+tÎ½â€²
âˆ’T+tÎ½â€²
Cov

X(s), X(s + tÎ½ âˆ’tÎ½â€²)

ds,
where in the last equality we deï¬ned s to be t + tÎ½â€². Letting T tend to inï¬nity
and using Lemma 15.6.4 (by replacing Î± there with âˆ’tÎ½â€² and Ï„ there with tÎ½ âˆ’tÎ½â€²)
establishes (15.62).
The key parts of the next theorem are Part (i), which provides an operational
meaning to the average autocovariance function and Part (iii) that relates it to the
operational PSD; the other parts are technical.
Theorem 15.6.6 (The Operational PSD and Average Autocovariance Function).
Let

X(t)

be a centered SP of average autocovariance function Â¯KXX.
(i) If h is the impulse response of some stable ï¬lter, then
Power in X â‹†h =
 âˆ
âˆ’âˆ
Â¯KXX(Ïƒ) Rhh(Ïƒ) dÏƒ,
h âˆˆL1.
(15.63)
(ii) If Â¯KXX and the IFT of some integrable function S(Â·) diï¬€er on a set of Lebesgue
measure zero, and if h âˆˆL1, then
Power in X â‹†h =
 âˆ
âˆ’âˆ
S(f)
Ë†h(f)
2 df,
h âˆˆL1.
(15.64)
(iii) If Â¯KXX is integrable, then its FT is the operational PSD of

X(t)

:
Ë†Â¯KXX = SXX.
(15.65)
(iv) If

X(t)

is of operational PSD SXX, then, outside a set of Lebesgue measure
zero, Â¯KXX equals the IFT of SXX.
Proof. Since

X(t)

is of average autocovariance function Â¯KXX, it is a fortiori of
bounded variance, and there exists some constant Î³ for which (15.48) holds. It
thus follows from (15.50) and the fact that

X(t)

is centered that
E

X(t) X(tâ€²)
 â‰¤Î³,
t, tâ€² âˆˆR.
(15.66)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.6 Operational PSD and Average Autocovariance Function
281
We now turn to Part (i). Let h âˆˆL1 be any stable impulse response. To compute
the power in Xâ‹†h we use Fubiniâ€™s Theorem to express the expectation of (Xâ‹†h)2(t)
at every epoch t âˆˆR as
E

(X â‹†h)2(t)

= E
	 âˆ
âˆ’âˆ
h(Ï„) X(t âˆ’Ï„) dÏ„

	 âˆ
âˆ’âˆ
h(Ïƒ) X(t âˆ’Ïƒ) dÏƒ


=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
h(Ï„) h(Ïƒ) E

X(t âˆ’Ï„) X(t âˆ’Ïƒ)

dÏ„ dÏƒ.
Consequently, for every T > 0,
1
2T
 T
âˆ’T
E

(X â‹†h)2(t)

dt
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
h(Ï„) h(Ïƒ) 1
2T
 T
âˆ’T
E

X(t âˆ’Ï„) X(t âˆ’Ïƒ)

dt dÏ„ dÏƒ
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
h(Ï„) h(Ïƒ) 1
2T
 Tâˆ’Ï„
âˆ’Tâˆ’Ï„
E

X(s) X(s + Ï„ âˆ’Ïƒ)

ds dÏ„ dÏƒ,
(15.67)
where in the last equality we substituted s for t âˆ’Ï„. Letting T tend to inï¬nity, we
now obtain
lim
Tâ†’âˆ
1
2T
 T
âˆ’T
E

(X â‹†h)2(t)

dt
= lim
Tâ†’âˆ
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
h(Ï„) h(Ïƒ) 1
2T
 Tâˆ’Ï„
âˆ’Tâˆ’Ï„
E

X(s) X(s + Ï„ âˆ’Ïƒ)

ds dÏ„ dÏƒ
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
lim
Tâ†’âˆh(Ï„) h(Ïƒ) 1
2T
 Tâˆ’Ï„
âˆ’Tâˆ’Ï„
E

X(s) X(s + Ï„ âˆ’Ïƒ)

ds dÏ„ dÏƒ
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
h(Ï„) h(Ïƒ) Â¯KXX(Ï„ âˆ’Ïƒ) dÏ„ dÏƒ,
(15.68)
where the swapping of the limit and the (double) integral in the second equality
can be justiï¬ed based on the Dominated Convergence Theorem (using (15.66) and
the integrability of h), and where the last equality follows from Lemma 15.6.4 (by
replacing Î± there with Ï„, and Ï„ there with Ï„ âˆ’Ïƒ). From here the proof of (15.63)
is but a change of variable away:
Power in X â‹†h = lim
Tâ†’âˆ
1
2T
 T
âˆ’T
E

(X â‹†h)2(t)

dt
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
h(Ï„) h(Ïƒ) Â¯KXX(Ï„ âˆ’Ïƒ) dÏ„ dÏƒ
=
 âˆ
âˆ’âˆ
Â¯KXX(Î±)
 âˆ
âˆ’âˆ
h(Ïƒ) h(Ïƒ + Î±) dÏƒ dÎ±
=
 âˆ
âˆ’âˆ
Â¯KXX(Î±) Rhh(Î±) dÎ±,
where the second equality follows from (15.68) and the third by deï¬ning Î± as Ï„ âˆ’Ïƒ.
This concludes the proof of Part (i).
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

282
Operational Power Spectral Density
To prove Part (ii), note that if Ë‡S and Â¯KXX are indistinguishable, then
 âˆ
âˆ’âˆ
Â¯KXX(Ïƒ) Rhh(Ïƒ) dÏƒ =
 âˆ
âˆ’âˆ
Ë‡S(Ïƒ) Rhh(Ïƒ) dÏƒ
=
 âˆ
âˆ’âˆ
S(f) Ë†Rhh(f) df
=
 âˆ
âˆ’âˆ
S(f)
Ë†h(f)
2 df,
from which (15.64) follows using (15.63). Here the second equality follows from
Proposition 6.2.4 and the fact that Ë†Rhh is real, and the last equality holds because
the FT of Rhh is f 	â†’
Ë†h(f)
2; see (11.35).
We next turn to Part (iii). Since Â¯KXX is real and symmetric, its FT Ë†Â¯KXX is real and
symmetric. And since it is positive deï¬nite, its FT is nonnegative (Appendix C,
Corollary C.8 (ii)). To prove that Ë†Â¯KXX equals SXX it thus remains to establish
that
Power in X â‹†h =
 âˆ
âˆ’âˆ
Ë†Â¯KXX(f)
Ë†h(f)
2 df,
h âˆˆL1.
(15.69)
This follows from Part (ii) because Â¯KXX is indistinguishable from the IFT of Ë†Â¯KXX
(Appendix C, Corollary C.8 (i)).
We next prove Part (iv). For every h âˆˆL1 we can express the power in X â‹†h
either in terms of SXX (using the deï¬nition of the operational PSD) or in terms of
the average autocovariance function Â¯KXX (15.63), so
 âˆ
âˆ’âˆ
Â¯KXX(Ïƒ) Rhh(Ïƒ) dÏƒ =
 âˆ
âˆ’âˆ
SXX(f)
Ë†h(f)
2 df
=
 âˆ
âˆ’âˆ
Ë‡SXX(Ïƒ) Rhh(Ïƒ) dÏƒ,
h âˆˆL1,
(15.70)
where the second equality follows from Proposition 6.2.4.
Since Â¯KXX is a measurable positive deï¬nite function (Proposition 15.6.5), it follows
from the Riesz-Crum Theorem (Appendix C, Theorem C.6 (i)), that it can be
expressed as
Â¯KXX = Â¯K
c
XX + Â¯K
s
XX,
(15.71)
where Â¯K
c
XX is a positive deï¬nite function that is continuous, and Â¯K
s
XX is a positive
deï¬nite function that is zero outside a set of Lebesgue measure zero. It follows
from (15.70), (15.71), and from the fact that Â¯K
s
XX is indistinguishable from the
all-zero function that
 âˆ
âˆ’âˆ
Â¯K
c
XX(Ïƒ) Rhh(Ïƒ) dÏƒ =
 âˆ
âˆ’âˆ
Ë‡SXX(Ïƒ) Rhh(Ïƒ) dÏƒ,
h âˆˆL1.
(15.72)
Since both Â¯K
c
XX and Ë‡SXX are continuous, this implies that they are identical
(Lemma 25.14.2). Thus, the IFT of SXX is identical to Â¯K
c
XX, and the latter is
indistinguishable from Â¯KXX (because Â¯K
s
XX is indistinguishable from the all-zero
function), so the IFT of SXX must be indistinguishable from Â¯KXX.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.7 The Operational PSD of a Filtered Stochastic Process
283
15.7
The Operational PSD of a Filtered Stochastic Process
From the operational PSD of a SP X it should not be diï¬ƒcult to compute the
operational PSD of X â‹†r whenever r is deterministic. The intuition is as follows.
To compute the operational PSD of the SP X â‹†r we need to know the power in
(X â‹†r) â‹†h for every h âˆˆL1. But, since convolution is (usually) associative, we
expect that the SP (Xâ‹†r)â‹†h be (usually) identical to the SP Xâ‹†(râ‹†h) and hence
of the same power. The power in the latter is easily computed from SXX: we view
r â‹†h as an impulse response of a ï¬lter; we view X â‹†(r â‹†h) as the result of passing
X through this ï¬lter; and we recall that X is of operational PSD SXX so the power
in X â‹†(r â‹†h)â€”and hence also in (X â‹†r) â‹†hâ€”is
 âˆ
âˆ’âˆ
SXX(f)
Ë†r(f) Ë†h(f)
2 df.
Rewriting this as
 âˆ
âˆ’âˆ

SXX(f)
Ë†r(f)
2 Ë†h(f)
2 df,
we conclude that the operational PSD of X â‹†r ought to be
f 	â†’SXX(f)
Ë†r(f)
2.
As we next show, under some very mild technical conditions, this is indeed the case.
The technical assumptions are needed to justify the claim that the convolution is
associative. This is the content of the next lemma. The proof is rather technical
and can be skipped.
Lemma 15.7.1 (Convolution Is Associative: Stochastic Processes). Let

X(t)

be a measurable SP satisfying
sup
tâˆˆR
E

|X(t)|

< âˆ.
(15.73)
Let r and h be real integrable signals, and let the epoch t âˆˆR be arbitrary but ï¬xed.
Then, with probability one, the two integrals
 âˆ
âˆ’âˆ
X(t âˆ’Ï„)

r â‹†h

(Ï„) dÏ„
and
 âˆ
âˆ’âˆ

X â‹†r

(t âˆ’Ïƒ) h(Ïƒ) dÏƒ
(15.74)
are both deï¬ned and are equal.
Note 15.7.2. Condition (15.73) holds whenever

X(t)

is centered and of bounded
variance, because the nonnegativity of the variance of |X(t)| implies that E[|X(t)|]
is upper-bounded by the square root of E

X2(t)

.
Proof. Since

X(t)

is a measurable SP, and since both r and h are measurable,
the mapping from Î© Ã— (R Ã— R) to the reals

Ï‰, (Ïƒ, Ï„)

	â†’X(Ï‰, t âˆ’Ï„) r(Ï„ âˆ’Ïƒ) h(Ïƒ)
(15.75)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

284
Operational Power Spectral Density
is measurable. It is integrable in the sense that
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
E

|X(t âˆ’Ï„)|

|r(Ï„ âˆ’Ïƒ)| |h(Ïƒ)| dÏƒ dÏ„ < âˆ,
(15.76)
because
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
E

|X(t âˆ’Ï„)|

|r(Ï„ âˆ’Ïƒ)| |h(Ïƒ)| dÏƒ dÏ„
â‰¤sup
tâˆˆR
E

|X(t)|
  âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
|r(Ï„ âˆ’Ïƒ)| |h(Ïƒ)| dÏƒ dÏ„
= sup
tâˆˆR
E

|X(t)|

âˆ¥râˆ¥1 âˆ¥hâˆ¥1 .
Using (15.76) and Fubiniâ€™s Theorem, we now conclude that there exists an event
N of probability zero such that
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
|X(Ï‰, t âˆ’Ï„)| |r(Ï„ âˆ’Ïƒ)| |h(Ïƒ)| dÏƒ dÏ„ < âˆ,
Ï‰ /âˆˆN.
(15.77)
Consequently, by Fubiniâ€™s Theorem,
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
X(Ï‰, t âˆ’Ï„) r(Ï„ âˆ’Ïƒ) h(Ïƒ) dÏƒ

dÏ„ =
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
X(Ï‰, t âˆ’Ï„) r(Ï„ âˆ’Ïƒ) h(Ïƒ) dÏ„

dÏƒ,
Ï‰ /âˆˆN.
(15.78)
The LHS of the above is the integral on the LHS of (15.74), and the RHS is the
integral on the RHS of (15.74).
Having established the associativity of the convolution, we are now ready for the
main result on the operational PSD of ï¬ltered stochastic processes.
Theorem 15.7.3 (The Operational PSD of a Filtered SP). Let the SP

X(t)

satisfy (15.73) and be of operational PSD SXX. Let r be any real integrable deter-
ministic signal. Then the operational PSD of X â‹†r is
f 	â†’SXX(f)
Ë†r(f)
2.
(15.79)
Proof. Since the mapping in (15.79) is symmetric, it only remains to verify that
Power in (X â‹†r) â‹†h =
 âˆ
âˆ’âˆ
SXX(f)
Ë†r(f)
2 Ë†h(f)
2 df,
h âˆˆL1.
(15.80)
This is now straightforward because for any h âˆˆL1
Power in (X â‹†r) â‹†h = Power in X â‹†(r â‹†h)
=
 âˆ
âˆ’âˆ
SXX(f)
Ë†r(f) Ë†h(f)
2 df
=
 âˆ
âˆ’âˆ
SXX(f)
Ë†r(f)
2 Ë†h(f)
2 df,
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.8 The Operational PSD and Power
285
as we next justify. The ï¬rst equality follows from Lemma 15.7.1, which shows that
at every epoch t âˆˆR the random variables

(X â‹†r) â‹†h

(t) and

X â‹†(r â‹†h)

(t) are
equal with probability one and hence of identical second moment:
E
%
(X â‹†r) â‹†h
2(t)
&
= E
%
X â‹†(r â‹†h)
2(t)
&
,
t âˆˆR.
The second equality holds because

X(t)

is of operational PSD SXX.
15.8
The Operational PSD and Power
It would be a sin of omission not to discuss the relationship between the operational
PSD of a stochastic process and its power. Intuition suggests that the integral of
the operational PSD ought to equal the power. To see why, recall that if X is of
operational PSD SXX, then
Power in X â‹†h =
 âˆ
âˆ’âˆ
SXX(f)
Ë†h(f)
2 df,
h âˆˆL1.
(15.81)
Suppose we now substitute for h the impulse response of a ï¬lter whose frequency
response resembles that of an ideal unit-gain lowpass ï¬lter of very large cutoï¬€
frequency W â‰«1. In this case the RHS of (15.81) would resemble the integral of
SXX(f) from âˆ’W to +W, which is approximately the integral from âˆ’âˆto +âˆ
when W is very large. And as to the LHS, if W is very large, then intuition suggests
that X will hardly be altered by the ï¬lter, and the LHS would approximately equal
the power in X.
This intuition is excellent, and for most stochastic processes of interest the opera-
tional PSD indeed integrates to the power. However, as our next example shows,
there are some pathological counter-examples.
Before presenting our example in detail, we begin with the big picture. In our
example the SP X takes on the values Â±1 only, so its power is 1. However, X
changes between the values +1 and âˆ’1 progressively faster the further time is from
the origin. As we next explain, this results in the power in Xâ‹†h being zero for every
stable ï¬lter h, so X is of zero operational PSD. The integral of the operational PSD
is thus zero, while the power is one.
We next oï¬€er some intuition as to why the power in Xâ‹†h is zero. Recall that when h
is stable, its frequency response decays to zero (Theorem 6.2.11 (i)). Consequently,
above some cutoï¬€frequency, the frequency response of the ï¬lter is nearly zero.
Since our SP varies faster and faster the further we are from the origin of time,
when we are suï¬ƒciently far from the origin of time the dynamics of our SP are
much faster than the ï¬lterâ€™s cutoï¬€frequency. Consequently, except for transients
that result from the behavior of our SP near the origin of time, in steady state the
response of h to X will be nearly zero. Since the transients do not inï¬‚uence the
power in X â‹†h, the power in X â‹†h is zero. We next present the example in greater
detail.
Example 15.8.1. Consider the SP

X(t), t âˆˆR

whose value in the time interval
[Î½, Î½ +1) is deï¬ned for every integer Î½ as follows: The interval is divided into |Î½|+1
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

286
Operational Power Spectral Density
nonoverlapping half-open subintervals of length 1/(|Î½| + 1)
%
Î½ +
Îº
|Î½| + 1, Î½ + Îº + 1
|Î½| + 1

,
Îº âˆˆ{0, . . . , |Î½|},
and in each such subinterval the SP is constant and is equal to the RV XÎ½,Îº, which
takes on the values Â±1 equiprobably with
{XÎ½,Îº},
Î½ âˆˆZ, Îº âˆˆ{0, . . . , |Î½|}
being IID. Thus,
X(t) =
âˆ

Î½=âˆ’âˆ
|Î½|

Îº=0
XÎ½,Îº I
'
Î½ +
Îº
|Î½| + 1 â‰¤t < Î½ + Îº + 1
|Î½| + 1
(
,
(15.82a)

XÎ½,Îº} âˆ¼IID U ({Â±1}) .
(15.82b)
This SP is centered, of power P = 1, and yet its operational PSD is zero at all
frequencies. The integral of the operational PSD of X is thus strictly smaller than
the power in X.
Analysis of Example 15.8.1. At every epoch t the RV X(t) takes on the values
Â±1 equiprobably and is thus centered. Moreover, X2(t) is deterministically 1, so
the power in

X(t)

is one. We next show that

X(t)

is of average autocovariance
function
Â¯KXX(Ï„) =

1
if Ï„ = 0,
0
otherwise,
Ï„ âˆˆR.
(15.83)
For Ï„ equal to zero this follows immediately from our observation that X2(t) is
deterministically equal to one. By symmetry, it suï¬ƒces to establish (15.83) for
positive Ï„.
When Ï„ is 1 or larger, the epochs t and t + Ï„ fallâ€”irrespective of
tâ€”in diï¬€erent intervals, so X(t) and X(t + Ï„) are uncorrelated for all t.
For
such Ï„â€™s Â¯KXX(Ï„) is thus zero, in agreement with (15.83). It thus only remains to
establish (15.83) for 0 < Ï„ < 1. In this case t and t + Ï„ are guaranteed to fall in
diï¬€erent subintervals whenever
Ï„ â‰¥
1
âŒŠtâŒ‹
 + 1,
(15.84)
where the RHS is the length of the subintervals to which the interval containing tâ€”
namely the interval [Î½, Î½ + 1), where Î½ is âŒŠtâŒ‹â€”is subdivided. (If this inequality is
not satisï¬ed, then X(t) and X(t + Ï„) may or may not be in diï¬€erent subintervals.)
For Ï„ âˆˆ(0, 1), Inequality (15.84) holds whenever
âŒŠtâŒ‹
 â‰¥Ï„ âˆ’1 âˆ’1. Thus, when t is
outside the ï¬nite interval

tâ€² âˆˆR :
âŒŠtâ€²âŒ‹
 < Ï„ âˆ’1 âˆ’1

the random variables X(t) and X(t + Ï„) are uncorrelated. For t inside this ï¬nite
interval the correlation between X(t) and X(t + Ï„) is upper bounded by 1. Con-
sequently, when we average E[X(t) X(t + Ï„)] over t, the contribution of tâ€™s inside
this interval washes out and the result is zero.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.8 The Operational PSD and Power
287
Having established (15.83), we conclude from Theorem 15.6.6 (i) that the power
in X â‹†h is zero for any stable ï¬lter h. The operational PSD of

X(t)

is thus
zero.
In Example 15.8.1 the power is strictly larger than the integral of the operational
PSD, and the average autocovariance function is discontinuous at the origin. As
we shall next see this is no coincidence.
The integral of the operational PSD
never exceeds the power, and the two are equal whenever the SP has an average
autocovariance function that is continuous at the origin. To emphasize that the
prevalent case is when the two are the same, we shall state the results in two
separate theorems.
Theorem 15.8.2 (The Power and the Integral of the Operational PSD). Let

X(t)

be a centered SP of power P, of operational PSD SXX, and of an average
autocovariance function Â¯KXX that is continuous at the origin. Then
P =
 âˆ
âˆ’âˆ
SXX(f) df.
(15.85)
Proof. For every stable impulse response h âˆˆL1 we can express the power in Xâ‹†h
in two diï¬€erent ways: as in (15.63) of Theorem 15.6.6 (i) and as in the deï¬nition
of the operational PSD (Deï¬nition 15.3.1). Consequently,
 âˆ
âˆ’âˆ
Â¯KXX(Ï„) Rhh(Ï„) dÏ„ =
 âˆ
âˆ’âˆ
SXX(f)
Ë†h(f)
2 df,
h âˆˆL1.
(15.86)
The gist of the proof is to choose h to be a very narrow pulse centered at the origin
with Rhh integrating to 1 (i.e., with
Ë†h(0)
 = 1) so that the LHS of (15.86) would
approximate Â¯KXX(0)â€”which, by Note 15.6.2, is equal to Pâ€”and so that the RHS
would approximate the integral of the operational PSD.
For every positive Î», we choose the impulse response h as
h(t) = 1
Î» I
'
|t| â‰¤Î»
2
(
,
t âˆˆR
(15.87a)
with corresponding Fourier Transform
Ë†h(f) = sinc (Î»f),
f âˆˆR,
(15.87b)
and self-similarity function
Rhh(Ï„) = 1
Î»

1 âˆ’|Ï„|
Î»

I

|Ï„| â‰¤Î»

,
Ï„ âˆˆR.
(15.87c)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

288
Operational Power Spectral Density
We now conclude the proof by justifying the calculation
P = Â¯KXX(0)
(15.88a)
= lim
Î»â†“0
 âˆ
âˆ’âˆ
Â¯KXX(Ï„) Rhh(Ï„) dÏ„
(15.88b)
= lim
Î»â†“0
 âˆ
âˆ’âˆ
SXX(f)
Ë†h(f)
2 df
(15.88c)
=
 âˆ
âˆ’âˆ
SXX(f) df.
(15.88d)
The ï¬rst equality follows from Note 15.6.2; the second from the hypothesis that
Â¯KXX is continuous at the origin and from the explicit form of Rhh (15.87c); and
the third equality follows from (15.86). The ï¬nal equality can be justiï¬ed using
the Dominated Convergence Theorem because the integrand converges to SXX(f)
at every f âˆˆR because
lim
Î»â†“0 SXX(f)
Ë†h(f)
2 = lim
Î»â†“0 SXX(f) sinc2 (Î»f)
= SXX(f),
f âˆˆR,
and the magnitude of the integrand can be upper-bounded as
SXX(f)
Ë†h(f)
2 = SXX(f)
Ë†h(f)
2
= SXX(f) sinc2 (Î»f)
â‰¤SXX(f),
f âˆˆR,
which is integrable.
To prepare for the result that the integral of the operational PSD cannot exceed the
power, we send forward the following lemma, which bounds the power ampliï¬cation
of a ï¬lter. For technical reasons it only deals with impulse responses of compact
support, i.e., impulse responses that are zero outside some interval of the form
[âˆ’Î”, Î”]. This restriction allows us to deal with inputs having ï¬nite power but
inï¬nite energy.
Lemma 15.8.3. Let

X(t)

be a SP of power P, and let h be an integrable function
of compact support. If the power in X â‹†h is deï¬ned, then it is bounded by
Power in X â‹†h â‰¤P max
fâˆˆR
Ë†h(f)
2.
(15.89)
Proof. Denote the SP X â‹†h by Y. Since h is of compact support, we can ï¬nd
some Î” > 0 such that h(t) is zero whenever |t| > Î”. We will show that for any
T > 0 and sample-path Ï‰ âˆˆÎ©,
 T
âˆ’T
Y 2(Ï‰, t) dt â‰¤max
fâˆˆR
Ë†h(f)
2  T+Î”
âˆ’Tâˆ’Î”
X2(Ï‰, t) dt.
(15.90)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.8 The Operational PSD and Power
289
The result will then follow by taking expectations of both sides of the inequality;
dividing by 2T; and letting T tend to inï¬nity.
It thus remains to prove (15.90). This inequality is obvious when its RHS is inï¬nite.
Hereafter we thus focus on the case where
 T+Î”
âˆ’Tâˆ’Î”
X2(Ï‰, t) dt < âˆ.
(15.91)
Starting from the deï¬nition of the convolution and recalling that h(t) is zero when-
ever |t| exceeds Î”, we obtain
Y (t) =
 âˆ
âˆ’âˆ
X(Ï„) h(t âˆ’Ï„) dÏ„
=
 t+Î”
tâˆ’Î”
X(Ï„) h(t âˆ’Ï„) dÏ„,
and thus establish that the value of Y (t) depends only on the values of X at epochs
in the interval [tâˆ’Î”, t+Î”]. Hence, for any T > 0, the values of Y (t) for t âˆˆ[âˆ’T, +T ]
are determined by the values of X at epochs in the interval [âˆ’T âˆ’Î”, T + Î”]. Thus,
if we deï¬ne the time-windowed SP Xw as
Xw(Ï‰, t) = X(Ï‰, t) I

|t| â‰¤T + Î”

,
(15.92)
then, rather than as X â‹†h, we can also express Y for |t| â‰¤T as
Y (t) = (Xw â‹†h)(t),
|t| â‰¤T.
(15.93)
Consequently,
 T
âˆ’T
Y 2(t) dt =
 T
âˆ’T
(Xw â‹†h)2(t) dt
â‰¤âˆ¥Xw â‹†hâˆ¥2
2
=
 âˆ
âˆ’âˆ
 Ë†Xw(f) Ë†h(f)
2 df
â‰¤max
fâˆˆR
Ë†h(f)
2  âˆ
âˆ’âˆ
 Ë†Xw(f)
2 df
= max
fâˆˆR
Ë†h(f)
2 âˆ¥Xwâˆ¥2
2
= max
fâˆˆR
Ë†h(f)
2  T+Î”
âˆ’Tâˆ’Î”
X2(t) dt,
with the following justiï¬cation.
The ï¬rst line follows from (15.93); the second
by extending the region of integration from [âˆ’T, +T ] to the entire real line; the
third by Parsevalâ€™s Theorem; the fourth by upper-bounding the integrand; the
ï¬fth by Parsevalâ€™s Theorem; and the last by the deï¬nition of Xw (15.92). This
establishes (15.90) and hence concludes the proof.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

290
Operational Power Spectral Density
Theorem 15.8.4 (The Integral of the Operational PSD Never Exceeds the
Power). If

X(t)

is of operational PSD SXX and of power P, then
P â‰¥
 âˆ
âˆ’âˆ
SXX(f) df.
(15.94)
Proof. We will show that for every W > 0
P â‰¥
 W
âˆ’W
SXX(f) df,
(15.95)
from which the result follows by letting W tend to inï¬nity. Fix some W > 0 and
some Î´ > 0. Let h âˆˆL1 be the impulse response of some stable ï¬lter that resembles
an ideal unit-gain lowpass ï¬lter of cutoï¬€frequency W in the sense that
Ë†h(f) = 1,
|f| â‰¤W,
(15.96a)
Ë†h(f) = 0,
|f| â‰¥W + Î´,
(15.96b)
and
Ë†h(f)
 â‰¤1,
f âˆˆR.
(15.96c)
(For example, h could be the IFT of the function g in (7.16) when we substitute
2(W + Î´) for Wc, zero for fc, and 2W for W; see also (7.18) and Figure 7.8.) For
this ï¬lter,
Power in X â‹†h =
 âˆ
âˆ’âˆ
SXX(f)
Ë†h(f)
2 df
â‰¥
 W
âˆ’W
SXX(f) df,
(15.97)
where the inequality follows from the nonnegativity of SXX and from (15.96a). The
desired inequality (15.95) will follow from (15.97) once we establish that
Power in X â‹†h â‰¤P.
(15.98)
To this end, ï¬x any (small) Ïµ > 0, and let hc be a function of compact support
that approximates h within Ïµ in the L1 norm. That is,
âˆ¥h âˆ’hcâˆ¥1 â‰¤Ïµ,
(15.99a)
and
hc(t) = 0,
|t| â‰¥Î”,
(15.99b)
where Î” > 0 typically depends on Ïµ. Deï¬ning Ëœh = hâˆ’hc, we obtain from (15.99a)
(using Theorem 6.2.11) that
Ë†Ëœh(f)
 =
Ë†h(f) âˆ’Ë†hc(f)
 â‰¤Ïµ,
f âˆˆR,
(15.100)
and hence, by (15.96c),
Ë†hc(f)
 â‰¤1 + Ïµ,
f âˆˆR.
(15.101)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.8 The Operational PSD and Power
291
By expressing h as hc+Ëœh and using the Triangle Inequality for Stochastic Processes
(Proposition 18.5.1), we obtain for every T > 0,
1
2T E
 T
âˆ’T
(X â‹†h)2(t) dt

â‰¤
-
1
2T E
 T
âˆ’T
(X â‹†hc)2(t) dt

+

1
2T E
 T
âˆ’T
(X â‹†Ëœh)2(t) dt
 .2
.
(15.102)
From this we obtain upon letting T tend to inï¬nity that
Power in X â‹†h â‰¤
-

(1 + Ïµ)2 P
1/2 +
	
Ïµ2
 âˆ
âˆ’âˆ
SXX(f) df

1/2.2
,
(15.103)
because, by Lemma 15.8.3 and (15.101),
Power in X â‹†hc â‰¤(1 + Ïµ)2 P,
(15.104)
and
Power in X â‹†Ëœh =
 âˆ
âˆ’âˆ
SXX(f)
Ë†Ëœh(f)

2
df
â‰¤max
fâˆˆR
Ë†Ëœh(f)

2  âˆ
âˆ’âˆ
SXX(f) df
â‰¤Ïµ2
 âˆ
âˆ’âˆ
SXX(f) df,
(15.105)
where the last inequality follows from (15.100).
The desired inequality (15.98) now follows from (15.103), because the latter holds
for every Ïµ > 0, and the RHS of (15.103) converges to the RHS of (15.98) when Ïµ
tends to zero.
We have seen that the integral of the operational PSD never exceeds the power
(Theorem 15.8.4) and that, for stochastic processes having an average autocovari-
ance function, a suï¬ƒcient condition for equality is that this function be continuous
at the origin (Theorem 15.8.2). The next result shows that for such stochastic
processes this is also a necessary condition. (Recall that in Example 15.8.1 this
condition does not hold and the integral of the operational PSD is strictly smaller
than the power.)
Theorem 15.8.5 (Continuity at the Origin Is Necessary). Let

X(t)

be a cen-
tered SP of power P, of operational PSD SXX, and of average autocovariance func-
tion Â¯KXX that is discontinuous at the origin. Then
P >
 âˆ
âˆ’âˆ
SXX(f) df.
(15.106)
Proof. Since Â¯KXX is a measurable positive deï¬nite function (Proposition 15.6.5),
it follows from the Riesz-Crum Theorem (Appendix C, Theorem C.6), that it can
be expressed as
Â¯KXX = Â¯K
c
XX + Â¯K
s
XX,
(15.107)
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

292
Operational Power Spectral Density
where Â¯K
c
XX is a positive deï¬nite function that is continuous, and Â¯K
s
XX is a positive
deï¬nite function that is zero outside a set of Lebesgue measure zero. Since Â¯KXX is
discontinuous, it follows that Â¯K
s
XX is not identically zero. Consequently its value
at zero must be strictly positive
Â¯K
s
XX(0) > 0
(15.108)
because Â¯K
s
XX is positive deï¬nite and hence achieves its maximum at the origin
(Proposition C.3 (iii)). It follows from (15.107) and (15.108) that
Â¯KXX(0) > Â¯K
c
XX(0).
(15.109)
Using the same choice of h as in (15.87a), we now obtain as in (15.88)
P = Â¯KXX(0)
(15.110a)
> Â¯K
c
XX(0)
(15.110b)
= lim
Î»â†“0
 âˆ
âˆ’âˆ
Â¯K
c
XX(Ï„) Rhh(Ï„) dÏ„
(15.110c)
= lim
Î»â†“0
 âˆ
âˆ’âˆ
Â¯KXX(Ï„) Rhh(Ï„) dÏ„
(15.110d)
= lim
Î»â†“0
 âˆ
âˆ’âˆ
SXX(f)
Ë†h(f)
2 df
(15.110e)
=
 âˆ
âˆ’âˆ
SXX(f) df,
(15.110f)
where (15.110b) follows from (15.109); (15.110c) follows from the continuity of
Â¯K
c
XX at the origin; (15.110d) follows because Â¯K
s
XX is zero outside a set of Lebesgue
measure zero; (15.110e) follows from (15.86); and (15.110f) is justiï¬ed like (15.88d).
15.9
Exercises
Exercise 15.1 (Scaling a SP). Let

Y (t)

be the result of scaling the SP

X(t)

by the
real number Î±. Thus, Y (t) = Î± X(t) for every epoch t âˆˆR. Show that if

X(t)

is of
operational PSD SXX, then

Y (t)

is of operational PSD f â†’Î±2 SXX(f).
Exercise 15.2 (The Operational PSD of a Sum of Independent SPs). Intuition suggests
that if

X(t)

and

Y (t)

are centered independent stochastic processes of operational
PSDs SXX and SYY , then their sum should be of operational PSD f â†’SXX(f) + SYY (f).
Explain why.
Exercise 15.3 (Operational PSD of a Deterministic SP). Let

X(t)

be deterministically
equal to the energy-limited signal g: R â†’R in the sense that, at every epoch t âˆˆR, the
RV X(t) is deterministically equal to g(t). Find the operational PSD of

X(t)

.
Exercise 15.4 (Stretching Time). Let

X(t)

be of operational PSD SXX, and let a > 0
be ï¬xed. Deï¬ne the SP

Y (t)

at every epoch t âˆˆR as Y (t) = X(t/a). Show that

Y (t)

is of operational PSD f â†’a SXX(af).
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

15.9 Exercises
293
Exercise 15.5 (The Operational PSD of PAM). Let

Xâ„“, â„“âˆˆZ

be IID with Xâ„“taking
on the values Â±1 equiprobably. Let
g(t) = I
 
|t| â‰¤Ts
2
!
,
t âˆˆR,
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR,
where A, Ts > 0 are deterministic.
(i) Plot a sample function of X for a realization of

Xâ„“, â„“âˆˆZ

of your choice.
(ii) Compute the operational PSD of X.
(iii) Repeat Parts (i) and (ii) for
Ëœ
X(t) = A
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’2 â„“Ts),
t âˆˆR.
(iv) How do the operational PSDs of X and ËœX compare?
Exercise 15.6 (Spectral Shaping via Precoding). Let

Xâ„“, â„“âˆˆZ

be IID with Xâ„“taking
on the values Â±1 equiprobably. Let Ëœ
Xâ„“= Xâ„“+ Xâ„“âˆ’1 for every â„“âˆˆZ.
(i) Compute the operational PSD of the PAM signal
Ëœ
X(t) =
âˆ

â„“=âˆ’âˆ
Ëœ
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR
for g(Â·) decaying to zero suï¬ƒciently fast as |t| â†’âˆ, e.g., satisfying (14.17).
(ii) Throw mathematical caution to the wind and evaluate your answer for the pulse
shape whose FT is
Ë†g(f) = I
 
|f| â‰¤
1
2Ts
!
,
f âˆˆR.
(Ignore the fact that this pulse shape does not satisfy (14.17).) Plot your answer
and compare it to the operational PSD of the PAM signal
X(t) =
âˆ

â„“=âˆ’âˆ
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR.
(iii) Show that ËœX can also be written as a PAM signal with IID symbols but with a
diï¬€erent pulse shape. That is,
Ëœ
X(t) =
âˆ

â„“=âˆ’âˆ
Xâ„“h(t âˆ’â„“Ts),
t âˆˆR,
h: t â†’g(t) + g(t âˆ’Ts).
Exercise 15.7 (The Operational PSD and Block Codes). PAM is used in block-mode in
conjunction with the (1, 2) binary-to-reals block encoder
0 â†’(+1, âˆ’1),
1 â†’(âˆ’1, +1)
to transmit IID random bits. The pulse shape g(Â·) satisï¬es the decay condition (14.17).
Compute the power and operational PSD of the signal.
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

294
Operational Power Spectral Density
Exercise 15.8 (Repetitions and the Operational PSD). Let

X(t)

be the signal (15.25)
that results when the (1, 2) binary-to-reals block-encoder (10.4) is used in bi-inï¬nite block-
mode. Find the operational PSD of

X(t)

.
Exercise 15.9 (Direct-Sequence Spread-Spectrum Communications). This problem is
motivated by uncoded Direct-Sequence Spread-Spectrum communications with process-
ing gain N. Let the (1, N) binary-to-reals block encoder map 0 to the sequence a1, . . . , aN
and 1 to âˆ’a1, . . . , âˆ’aN. Consider PAM with bi-inï¬nite block encoding using this map-
ping. Express the operational PSD of the resulting PAM signal in terms of the sequence
a1, . . . , aN and the pulse shape g. Calculate explicitly when the pulse shape is the map-
ping t â†’I{|t| â‰¤Ts/2} for two cases: when the sequence a1, . . . , aN is the Barker-7 code
(+1, +1, +1, âˆ’1, âˆ’1, +1, âˆ’1) and when it is the sequence (+1, +1, +1, +1, +1, +1, +1).
Compare the latter case with the case where the mapping is the antipodal mapping
0 â†’+1, and 1 â†’âˆ’1, the baud period is 7Ts, and the pulse shape is t â†’I{|t| â‰¤7Ts/2}
Exercise 15.10 (A Non-PAM Signal). For every t âˆˆR deï¬ne Y (t) = X(t) âˆ’E[X(t)],
where

X(t)

is the SP of Exercise 14.6. Find the operational PSD of

Y (t)

.
Exercise 15.11 (8-PPM). Let

Y (t)

be given at every t âˆˆR by Y (t) = X(t) âˆ’E[X(t)],
where

X(t)

is the result of bi-inï¬nite block encoding IID random bits using 8-PPM; see
Exercise 10.3. Find the operational PSD of

Y (t)

.
Exercise 15.12 (The Power in a Filtered Period Signal). Let x be a periodic signal of
period Tp > 0 and of ï¬nite power. Show that for every h âˆˆL1
Power of x â‹†h = 1
Tp
âˆ

Î·=âˆ’âˆ
|cÎ·|2 Ë†h
	 Î·
Tp


2
,
where cÎ· is the Î·-th Fourier Series Coeï¬ƒcient of x w.r.t. the interval [âˆ’Tp/2, Tp/2].
Hint: Recall Exercises 5.11, 14.7, 6.23, and Theorem A.3.3 (ii).
Exercise 15.13 (Periodicity and the Average Autocovariance Function). Let the SP

X(t)

be centered and of bounded variance.
Show that if for every lag Ï„ âˆˆR the
mapping t â†’Cov[X(t), X(t + Ï„)] is periodic in t with period Tp > 0, then

X(t)

has the
average autocovariance function
Â¯KXX(Ï„) = 1
Tp
 Tp
0
Cov[X(t), X(t + Ï„)] dt,
Ï„ âˆˆR.
Exercise 15.14 (On the Power and Average Autocovariance Function). Let

X(t)

be a
centered, measurable SP of bounded variance, and let Ï„ âˆˆR be any lag. Show that the
existence of the limits deï¬ning the power in the SP

X(t)

and in the SP whose time-t
value is X(t) + X(t + Ï„) guarantees the existence of the limit on the RHS of (15.51).
available at 
.017
14:31:45, subject to the Cambridge Core terms of use,

Chapter 16
Quadrature Amplitude Modulation
16.1
Introduction
We next discuss linear modulation in passband. We envision being allocated band-
width W around the carrier frequency fc, so we can only send real signals whose
Fourier Transform is zero at frequencies f satisfying
|f| âˆ’fc
 > W/2.
That
is, the FT of the transmitted signal is allowed to be nonzero only in the fre-
quency interval [fc âˆ’W/2, fc + W/2] and in its negative frequency counterpart
[âˆ’fc âˆ’W/2, âˆ’fc + W/2] (Deï¬nition 7.3.1). We assume throughout this chapter
that
fc > W
2 .
(16.1)
There are numerous ways to communicate in passband and, to complicate things
further, sometimes seemingly diï¬€erent approaches lead to identical signals. Thus,
while we would like to motivate the scheme we shall focus onâ€”Quadrature Ampli-
tude Modulation (QAM)â€”we cannot prove or claim that it is the only â€œoptimalâ€
solution.1 Nevertheless, we shall try to motivate it by discussing some features
that one would typically like to have and by then showing that QAM has these
features.
From our studies of PAM and of Nyquistâ€™s Criterion we recall that if we are al-
located (baseband) bandwidth W Hz and if Ts â‰¥1/(2W), then we can ï¬nd a
bandwidth-W pulse shape whose time shifts by integer multiples of Ts are or-
thonormal. If Ts = 1/(2W), then such a pulse is the bandwidth-W unit-energy
pulse t 	â†’
âˆš
2W sinc(2Wt). (You may recall that such pulses are rarely used be-
cause they decay to zero too slowly over time, thus rendering the computation
of the PAM signal unstable and the resulting peak power unbounded.)
And if
Ts < 1/(2W), then no such pulse shape exists. (Corollary 11.3.5.)
From a somewhat more abstract perspective, PAM with the above pulse shape (or
with the square root of a raised-cosine pulse shape (11.29) with very small excess
bandwidth) allows us to send symbols arriving at rate
Rs
real symbol
second

â‰œ1
Ts
1There are information theoretic considerations that show that QAM can achieve the capacity
of the bandlimited passband additive white Gaussian noise channel.
295
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

296
Quadrature Amplitude Modulation
as the coeï¬ƒcients in a linear combination of orthonormal signals whose bandwidth
does not exceed (or only slightly exceeds)
Rs
2 [Hz] .
That is, for each spectral sliver of 1 Hz at baseband we obtain 2 real dimensions
per second, i.e., we can communicate at spectral eï¬ƒciency
2 [real dimension/sec]
[baseband Hz]
.
This is an achievement that we would like to replicate for passband signaling:
First Objective: Find a way to transmit real symbols arriving at rate Rs real sym-
bols per second as the coeï¬ƒcients in a linear combination of orthonormal passband
signals occupying a (passband) bandwidth of W Hz around the carrier frequency fc,
where the bandwidth W is equal to (or only slightly exceeds) Rs/2. That is, we
would like to ï¬nd a communication scheme that would allow us to communicate at
2 [real dimension/sec]
[passband Hz]
.
Equivalently, since any stream of real symbols arriving at rate Rs real symbols
per second can be viewed as a stream of complex symbols arriving at rate Rs/2
complex symbols per second (simply by pairing tuples (a, b) of real numbers a, b âˆˆR
into single complex numbers a + ib), we can restate our objective as follows: ï¬nd
a way to transmit complex symbols arriving at rate Rs/2 complex symbols per
second as the coeï¬ƒcients in a linear combination of orthonormal passband signals
occupying a (passband) bandwidth of W Hz around the carrier frequency fc, where
the bandwidth W is equal to, or only slightly exceeds Rs/2. That is, we would like
to ï¬nd a communication scheme that would allow us to communicate at
1 [complex dimension/sec]
[passband Hz]
.
(16.2)
In addition, we would like our modulation scheme to be of reasonable complexity.
One of the beneï¬ts of the baseband PAM scheme is that we can compute all the
inner products required to reconstruct the coeï¬ƒcients (symbols) using the matched
ï¬lter by feeding it with the transmitted signal and sampling its output at the
appropriate times.
A naive approach that does not achieve our objective is to use real baseband PAM
of the type we studied in Chapter 10 and to up-convert the PAM signal to passband
by multiplying it by the mapping t 	â†’cos(2Ï€fct). The problem with this approach
is that the up-conversion doubles the bandwidth (Proposition 7.3.3).
16.2
PAM for Passband?
A natural approach to passband signaling might be to consider PAM directly with-
out any up-conversion. We merely have to look for a pulse shape Ï† whose Fourier
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.3 The QAM Signal
297
Transform is zero outside the band
|f| âˆ’fc
 â‰¤W/2 and whose self-similarity
function RÏ†Ï† is a Nyquist Pulse. It turns out that with this approach we can only
achieve our objective if 4fcTs is an odd integer. Indeed, the reader is encouraged
to use Corollary 11.3.4 to verify that if a pulse Ï† is an energy-limited passband
signal that is bandlimited to W Hz around the carrier frequency fc, and if its time
shifts by integer multiples of Ts are orthonormal, then
Ts â‰¥
1
2W
with equality being achievable only if both
|Ë†Ï†(f)|2 = Ts I
|f| âˆ’fc
 â‰¤W/2

(for all frequencies f âˆˆR outside a set of Lebesgue measure zero) and
4fcTs
is an odd integer.
(16.3)
In fact, it can be shown that if (16.3) is satisï¬ed and if Ïˆ is any energy-limited
signal that is bandlimited to W/2 Hz and whose time shifts by integer multiples
of 2Ts are orthonormal, then the passband signal
Ï†(t) =
âˆš
2 cos(2Ï€fct) Ïˆ(t),
t âˆˆR
is an energy-limited passband signal that is bandlimited to W Hz around the carrier
frequency fc, and its time shifts by integer multiples of Ts are orthonormal.
It would thus seem that if (16.3) is satisï¬ed, then PAM would be a viable solution
to our problem. Nevertheless, this is not the standard solution. The reason may
have to do with implementation. If the above approach is used, then the carrier
frequency inï¬‚uences the choice of the pulse shape. Thus, a radio with a selectable
carrier frequency would require a diï¬€erent pulse shape for each frequency! More-
over, the implementation of the modulator becomes carrier-dependent and fairly
complex. This discussion motivates our second objective:
Second Objective: To allow for ï¬‚exibility in the choice of the carrier, it is desir-
able to decouple the pulse shape selection from the carrier frequency.
16.3
The QAM Signal
Quadrature Amplitude Modulation achieves both our objectives. It achieves our
desired spectral eï¬ƒciency (16.2) and also decouples the signal design from the
carrier frequency. It is easiest to describe QAM by describing the baseband repre-
sentation xBB(Â·) of the transmitted passband signal xPB(Â·). Indeed, the baseband
representation of the transmitted signal has the structure of PAM but with one
important diï¬€erence: we allow for complex symbols and for complex pulse shapes.2
In QAM the encoder
Ï•: {0, 1}k â†’Cn
(16.4)
2Allowing complex pulse shapes is not critical. Crucial is that we allow complex symbols.
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

298
Quadrature Amplitude Modulation
maps k-tuples of data bits (D1, . . . , Dk) to n-tuples of complex symbols (C1, . . . , Cn),
and the baseband representation of the transmitted signal is
XBB(t) = A
n

â„“=1
Câ„“g(t âˆ’â„“Ts),
t âˆˆR,
(16.5a)
where the pulse shape g(Â·) may be complex (though it is often chosen to be real),
A â‰¥0 is a real constant, Ts > 0 is the baud period, and 1/Ts is the baud rate. The
rate of the encoder is given by
k
n

bit
complex symbol

,
(16.5b)
and the transmitted real passband QAM signal XPB(Â·) is given by
XPB(t) = 2 Re

XBB(t) ei2Ï€fct
,
t âˆˆR.
(16.5c)
Using (16.5a) & (16.5c) we can also express the QAM signal as
XPB(t) = 2 Re
	
A
n

â„“=1
Câ„“g(t âˆ’â„“Ts) ei2Ï€fct

,
t âˆˆR.
(16.6)
Alternatively, we can use the identities
Re(wz) = Re(w) Re(z) âˆ’Im(w) Im(z),
w, z âˆˆC,
Im(z) = âˆ’Re(iz),
z âˆˆC
to express the QAM signal as
XPB(t) =
âˆš
2A
n

â„“=1
Re(Câ„“)
gI,â„“(t)



2 Re
-
1
âˆš
2 g(t âˆ’â„“Ts)



gI,â„“,BB(t)
ei2Ï€fct
.
+
âˆš
2A
n

â„“=1
Im(Câ„“)
gQ,â„“(t)



2 Re
-
i 1
âˆš
2 g(t âˆ’â„“Ts)



gQ,â„“,BB(t)
ei2Ï€fct
.
,
t âˆˆR,
(16.7)
where we deï¬ne
gI,â„“(t) â‰œ2 Re
	 1
âˆš
2 g(t âˆ’â„“Ts) ei2Ï€fct

(16.8a)
= 2 Re

gI,â„“,BB(t) ei2Ï€fct
,
t âˆˆR,
and
gQ,â„“(t) â‰œ2 Re
	
i 1
âˆš
2 g(t âˆ’â„“Ts) ei2Ï€fct

(16.8b)
= 2 Re

gQ,â„“,BB(t) ei2Ï€fct
,
t âˆˆR,
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.4 Bandwidth Considerations
299
with corresponding baseband representations:
gI,â„“,BB(t) â‰œ
1
âˆš
2 g(t âˆ’â„“Ts),
t âˆˆR,
(16.9a)
gQ,â„“,BB(t) â‰œi 1
âˆš
2 g(t âˆ’â„“Ts),
t âˆˆR.
(16.9b)
Some comments about the QAM signal:
(i) The representation (16.7) demonstrates that the QAM signal is a linear com-
bination of the waveforms {gI,â„“} and {gQ,â„“}, where the coeï¬ƒcients are pro-
portional to the real parts and the imaginary parts of the symbols {Câ„“}.
(ii) The normalization factor of 1/
âˆš
2 in the deï¬nition of the functions {gI,â„“} and
{gQ,â„“} is for convenience only. Its role will become clearer in Section 16.5,
where the pulse shape is chosen to be of unit energy. In this case the factor of
1/
âˆš
2 guarantees that the functions {gI,â„“} and {gQ,â„“} are also of unit energy.
(iii) We could also view QAM slightly diï¬€erently as a modulation scheme where
data bits D1, . . . , Dk are mapped to 2n real numbers X1, . . . , X2n, which are
then grouped in pairs to form the n complex numbers Câ„“= X2â„“âˆ’1 + iX2â„“
for â„“= 1, . . . , n and where these complex numbers are then mapped into the
passband signal whose baseband representation is given in (16.5a). The two
views are, of course, completely equivalent.
The expression for the QAM signal XPB(Â·) is simpliï¬ed if the pulse shape g is real.
In this case we obtain from (16.6) for every t âˆˆR
XPB(t) = 2A
n

â„“=1
Re(Câ„“) g(t âˆ’â„“Ts) cos(2Ï€fct)
âˆ’2A
n

â„“=1
Im(Câ„“) g(t âˆ’â„“Ts) sin(2Ï€fct),
g real.
(16.10)
Thus, if the pulse shape g is real, then the QAM signal can be viewed as the
sum of two signals: the ï¬rst is the result of feeding {Re(Câ„“)} to a baseband PAM
modulator of pulse shape g and multiplying the result by cos(2Ï€fct), and the second
is the result of feeding {Im(Câ„“)} to a baseband PAM modulator of pulse shape g
and multiplying the result by âˆ’sin(2Ï€fct). Figure 16.1 illustrates the generation
of the QAM signal when the pulse shape g is real.
16.4
Bandwidth Considerations
The baseband representation of the QAM signal XPB(Â·) of (16.5c) is the baseband
PAM signal XBB(Â·) of (16.5a). By Corollary 10.10.2 (which also holds for complex
pulse shapes and symbols), the bandwidth of XBB(Â·) is the bandwidth of the pulse
shape g. Consequently, since the bandwidth of a passband signal around the carrier
frequency is twice the bandwidth of its baseband representation (Proposition 7.6.7
and Theorem 7.7.12 (i)), we conclude:
Note 16.4.1 (Bandwidth around fc of QAM). Unless the QAM signal is zero, its
bandwidth around the carrier frequency is twice the bandwidth of the pulse shape.
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

300
Quadrature Amplitude Modulation
cos(2Ï€fct)
âˆ’sin(2Ï€fct)
90â—¦
Ã—
Ã—
+
{Câ„“}
Re(Â·)
Im(Â·)
Re(Câ„“)
Im(Câ„“)
PAM
PAM
A 
â„“Re(Câ„“)g(t âˆ’â„“Ts)
A 
â„“Im(Câ„“)g(t âˆ’â„“Ts)
A 
â„“Re(Câ„“)g(t âˆ’â„“Ts) cos(2Ï€fct)
âˆ’A 
â„“Im(Câ„“)g(t âˆ’â„“Ts) sin(2Ï€fct)
xPB(t)/2
Figure 16.1: Generating a QAM signal when the pulse shape g is real.
16.5
Orthogonality Considerations
We next study the consequences of choosing the pulse shape g(Â·) so that its time
shifts by integer multiples of Ts be orthonormal. As in our treatment of PAM, we
change notation and denote the pulse shape in this case by Ï†(Â·). The orthonormal-
ity condition is thus
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†âˆ—(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ.
(16.11)
By Corollary 11.3.4, this is equivalent to requiring that
âˆ

â„“=âˆ’âˆ
Ë†Ï†

f + â„“
Ts

2
= Ts,
(16.12)
for all frequencies f outside a set of Lebesgue measure zero.
When the pulse shape satisï¬es the orthogonality condition (16.11) we refer to 1/Ts
as having units of complex dimensions per second. In analogy to Deï¬nition 11.3.6,
we deï¬ne the excess bandwidth as
100%
	bandwidth of Ï†
1/(2Ts)
âˆ’1

.
(16.13)
Proposition 16.5.1. If the energy-limited pulse shape Ï† satisï¬es (16.11), then the
QAM signal XPB(Â·) can be expressed as
XPB =
âˆš
2A
n

â„“=1
Re(Câ„“) ÏˆI,â„“+
âˆš
2A
n

â„“=1
Im(Câ„“) ÏˆQ,â„“
(16.14)
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.5 Orthogonality Considerations
301
where
. . . , ÏˆI,âˆ’1, ÏˆQ,âˆ’1, ÏˆI,0, ÏˆQ,0, ÏˆI,1, ÏˆQ,1, . . .
are orthonormal functions that are given by
ÏˆI,â„“: t 	â†’2 Re
	 1
âˆš
2 Ï†(t âˆ’â„“Ts) ei2Ï€fct

,
â„“âˆˆZ
(16.15a)
ÏˆQ,â„“: t 	â†’2 Re
	
i 1
âˆš
2 Ï†(t âˆ’â„“Ts) ei2Ï€fct

,
â„“âˆˆZ.
(16.15b)
Proof. Substituting Ï† for g in (16.7) we obtain
XPB(t) =
âˆš
2A
n

â„“=1
Re(Câ„“)
ÏˆI,â„“(t)



2 Re
	 1
âˆš
2 Ï†(t âˆ’â„“Ts)



ÏˆI,â„“,BB(t)
ei2Ï€fct

+
âˆš
2A
n

â„“=1
Im(Câ„“)
ÏˆQ,â„“(t)



2 Re
	
i 1
âˆš
2 Ï†(t âˆ’â„“Ts)



ÏˆQ,â„“,BB(t)
ei2Ï€fct

,
t âˆˆR,
where for every t âˆˆR
ÏˆI,â„“(t) â‰œ2 Re
	 1
âˆš
2 Ï†(t âˆ’â„“Ts) ei2Ï€fct

(16.16a)
= 2 Re

ÏˆI,â„“,BB(t) ei2Ï€fct
,
ÏˆQ,â„“(t) â‰œ2 Re
	
i 1
âˆš
2 Ï†(t âˆ’â„“Ts) ei2Ï€fct

(16.16b)
= 2 Re

ÏˆQ,â„“,BB(t) ei2Ï€fct
,
and the baseband representations are given by
ÏˆI,â„“,BB(t) â‰œ
1
âˆš
2 Ï†(t âˆ’â„“Ts)
(16.17a)
and
ÏˆQ,â„“,BB(t) â‰œi 1
âˆš
2 Ï†(t âˆ’â„“Ts).
(16.17b)
We next verify that, when Ï† satisï¬es (16.11), the functions
. . . , ÏˆI,âˆ’1, ÏˆQ,âˆ’1, ÏˆI,0, ÏˆQ,0, ÏˆI,1, ÏˆQ,1, . . .
are orthonormal. To this end we recall that the inner product between two real
passband signals is twice the real part of the inner product between their baseband
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

302
Quadrature Amplitude Modulation
representations (Theorem 7.6.10). For â„“Ì¸= â„“â€² we thus have by (16.11)
âŸ¨ÏˆI,â„“, ÏˆI,â„“â€²âŸ©= 2 Re

ÏˆI,â„“,BB, ÏˆI,â„“â€²,BB

= 2 Re
#
t 	â†’
1
âˆš
2 Ï†(t âˆ’â„“Ts), t 	â†’
1
âˆš
2 Ï†(t âˆ’â„“â€²Ts)
$
= 0,
âŸ¨ÏˆQ,â„“, ÏˆQ,â„“â€²âŸ©= 2 Re

ÏˆQ,â„“,BB, ÏˆQ,â„“â€²,BB

= 2 Re
#
t 	â†’i 1
âˆš
2 Ï†(t âˆ’â„“Ts), t 	â†’i 1
âˆš
2 Ï†(t âˆ’â„“â€²Ts)
$
= 0,
and
âŸ¨ÏˆI,â„“, ÏˆQ,â„“â€²âŸ©= 2 Re
#
t 	â†’
1
âˆš
2 Ï†(t âˆ’â„“Ts), t 	â†’i 1
âˆš
2 Ï†(t âˆ’â„“â€²Ts)
$
= 0.
And for â„“= â„“â€² we have, again by (16.11),
âŸ¨ÏˆI,â„“, ÏˆI,â„“âŸ©= 2 Re
#
t 	â†’
1
âˆš
2 Ï†(t âˆ’â„“Ts), t 	â†’
1
âˆš
2 Ï†(t âˆ’â„“Ts)
$
= 1,
âŸ¨ÏˆI,â„“, ÏˆQ,â„“âŸ©= 2 Re
#
t 	â†’
1
âˆš
2 Ï†(t âˆ’â„“Ts), t 	â†’i 1
âˆš
2 Ï†(t âˆ’â„“Ts)
$
= Re

âˆ’i âˆ¥Ï†âˆ¥2
2

= 0,
and
âŸ¨ÏˆQ,â„“, ÏˆQ,â„“âŸ©= 2 Re
#
t 	â†’i 1
âˆš
2 Ï†(t âˆ’â„“Ts), t 	â†’i 1
âˆš
2 Ï†(t âˆ’â„“Ts)
$
= 1.
Notice that (16.14)â€“(16.15) can be simpliï¬ed when Ï† is real:
Corollary 16.5.2. If, in addition to the assumptions of Proposition 16.5.1, we also
assume that the pulse shape Ï† is real, then the QAM signal can be written as
XPB(t) =
âˆš
2A
n

â„“=1
Re(Câ„“)
âˆš
2 Ï†(t âˆ’â„“Ts) cos(2Ï€fct)
âˆ’
âˆš
2A
n

â„“=1
Im(Câ„“)
âˆš
2 Ï†(t âˆ’â„“Ts) sin(2Ï€fct),
t âˆˆR,
(16.18)
and
'
t 	â†’
âˆš
2 Ï†(t âˆ’â„“Ts) cos(2Ï€fct)
(âˆ
â„“=âˆ’âˆ,
'
t 	â†’
âˆš
2 Ï†(t âˆ’â„“Ts) sin(2Ï€fct)
(âˆ
â„“=âˆ’âˆ
are orthonormal.
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.6 Spectral Eï¬ƒciency
303
16.6
Spectral Eï¬ƒciency
We next show that QAM achieves our spectral eï¬ƒciency objective. We assume
that we are only allowed to transmit signals of bandwidth W around the carrier
frequency fc, so the transmitted signal can only occupy the frequencies f satisfying
|f| âˆ’fc
 â‰¤W/2.
In order for the QAM signal to meet this constraint, we choose a pulse shape Ï†
that is bandlimited to W/2 Hz, because the up-conversion doubles the bandwidth
(Note 16.4.1). Thus, by Corollary 11.3.5, the orthogonality (16.11) can only hold
if the baud period Ts satisï¬es Ts â‰¥1/(2 Ã— W/2) or
Ts â‰¥1
W,
with the RHS being achievable by choosing Ï† to be the bandwidth-W/2 unit-energy
signal t 	â†’
âˆš
W sinc(Wt).
If we choose Ts equal to 1/W (or only slightly larger than that), then our modulation
will support the transmission of complex symbols arriving at a rate of 1/Ts â‰ˆW
complex symbols per second.
And since our QAM signal only occupies W Hz
around the carrier frequency, our scheme achieves a spectral eï¬ƒciency of 1 [complex
dimension per second] per Hz. QAM thus achieves our spectral eï¬ƒciency objective.
This is so exciting that we highlight the achievement:
QAM with the bandwidth-W/2 unit-energy pulse shape given by
t 	â†’
âˆš
W sinc(Wt) transmits a sequence of real symbols arriving at
a rate of 2W real symbols per second as the coeï¬ƒcients in a linear
combination of orthogonal signals, with the resulting waveform
being bandlimited to W Hz around the carrier frequency fc. It
thus achieves a spectral eï¬ƒciency of
2 [real dimension/sec]
[passband Hz]
= 1 [complex dimension/sec]
[passband Hz]
.
16.7
QAM Constellations
In analogy to the deï¬nition of the constellation of a PAM scheme (Section 10.8),
we deï¬ne the constellation of a QAM scheme (or, perhaps more appropriately, of
the mapping Ï•(Â·) in (16.4)) as the smallest subset of C of which Câ„“is an element
for every â„“âˆˆ{1, . . . , n} and for every realization of the data bits.
We denote
the constellation by C. The number of points in the constellation C is just the
number of elements of C.
Important constellations include the square 4-QAM constellation (also known as
QPSK)
{+1 + i, âˆ’1 + i, âˆ’1 âˆ’i, +1 âˆ’i},
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

304
Quadrature Amplitude Modulation
4-QAM
8-PSK
16-QAM
32-QAM
Figure 16.2: Some QAM constellations (drawn to no particular scale).
the square QAM constellation with (2Î½) Ã— (2Î½) points
'
a + ib : a, b âˆˆ

âˆ’(2Î½ âˆ’1), . . . , âˆ’3, âˆ’1, +1, +3, . . . , (2Î½ âˆ’1)
(
,
(16.19)
and the M-PSK (M-ary Phase Shift Keying) constellation comprising the M com-
plex numbers on the unit circle whose M-th power is one, i.e.,
'
1, ei2Ï€/M, ei4Ï€/M, ei6Ï€/M, . . . , ei(Mâˆ’1)2Ï€/M(
.
See Figure 16.2 for some common QAM constellations. Please note that the square
16-QAM and the 16-PSK are just two of many possible constellations with 16
points. However, some engineers omit the word â€œsquareâ€ and write 4-QAM, 16-
QAM, 64-QAM, etc. for the respective square constellations.
We can also deï¬ne the minimum distance Î´ of a constellation C in analogy to
(10.22) as
Î´ â‰œmin
c,câ€²âˆˆC
cÌ¸=câ€²
|c âˆ’câ€²|.
(16.20)
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.8 Recovering the Complex Symbols via Inner Products
305
The constellation is centered if

câˆˆC
c = 0,
(16.21)
and in analogy to (10.25), we deï¬ne the second moment of a constellation C as
1
# C

câˆˆC
|c|2.
(16.22)
Finally, in analogy to our deï¬nition of uncoded PAM transmission (Section 10.9),
we say that the QAM transmission is uncoded when the range of Ï• equals Cn.
16.8
Recovering the Complex Symbols via Inner Products
Recall that, by Proposition 16.5.1, if the time shifts of Ï† by integer multiples of Ts
are orthonormal, then the QAM signal can be written as
XPB =
âˆš
2A
n

â„“=1
Re(Câ„“) ÏˆI,â„“+
âˆš
2A
n

â„“=1
Im(Câ„“) ÏˆQ,â„“,
where the signals . . . , ÏˆI,âˆ’1, ÏˆQ,âˆ’1, ÏˆI,0, ÏˆQ,0, ÏˆI,1, ÏˆQ,1, . . ., which are given in
(16.15), are orthonormal. Consequently, the complex symbols can be recovered
from the QAM signal (in the absence of noise) using the inner products:
Re(Câ„“) =
1
âˆš
2A âŸ¨XPB, ÏˆI,â„“âŸ©,
â„“âˆˆ{1, . . . , n},
(16.23a)
Im(Câ„“) =
1
âˆš
2A âŸ¨XPB, ÏˆQ,â„“âŸ©,
â„“âˆˆ{1, . . . , n}.
(16.23b)
We next describe circuits to compute these inner products. With a view to future
chapters where noise will be present, we shall describe more general circuits that
compute the inner products âŸ¨r, ÏˆI,â„“âŸ©and âŸ¨r, ÏˆQ,â„“âŸ©for an arbitrary (not necessarily
QAM) energy-limited signal r. Moreover, since the calculation of the inner products
will not exploit the orthogonality condition (16.11), we shall describe the more
general setting where the pulse shape is arbitrary and refer to the notation of
(16.7). Thus, we shall present circuits to compute
âŸ¨r, gI,â„“âŸ©, âŸ¨r, gQ,â„“âŸ©,
where gI,â„“and gQ,â„“and their baseband representations are given in (16.8) and
(16.9). Here r is an arbitrary energy-limited signal. We present two approaches:
an approach based on baseband conversion and a direct approach.
16.8.1
Inner Products via Baseband Conversion
We begin by noting that if the pulse shape g is bandlimited to W/2 Hz then both
gI,â„“and gQ,â„“are bandlimited to W Hz around the carrier frequency fc. Conse-
quently, since they contain no energy outside the bands [fc âˆ’W/2, fc + W/2] and
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

306
Quadrature Amplitude Modulation
[âˆ’fcâˆ’W/2, âˆ’fc+W/2], it follows from Parsevalâ€™s Theorem that the Fourier Trans-
form of r outside these bands does not inï¬‚uence the value of the inner products.
Thus, if s is the result of passing r through an ideal unit-gain bandpass ï¬lter of
bandwidth W around the carrier frequency fc, i.e.,
s = r â‹†BPFW,fc,
(16.24)
then
âŸ¨r, gI,â„“âŸ©= âŸ¨s, gI,â„“âŸ©,
(16.25a)
âŸ¨r, gQ,â„“âŸ©= âŸ¨s, gQ,â„“âŸ©.
(16.25b)
If we denote the baseband representation of s by sBB, then
âŸ¨r, gI,â„“âŸ©= âŸ¨s, gI,â„“âŸ©
= 2 Re

âŸ¨sBB, gI,â„“,BBâŸ©

=
âˆš
2 Re

âŸ¨sBB, t 	â†’g(t âˆ’â„“Ts)âŸ©

,
(16.26a)
where the ï¬rst equality follows from (16.25a); the second from Theorem 7.6.10;
and the ï¬nal equality from (16.9a). Similarly,
âŸ¨r, gQ,â„“âŸ©= âŸ¨s, gQ,â„“âŸ©
= 2 Re

âŸ¨sBB, gQ,â„“,BBâŸ©

=
âˆš
2 Re

âŸ¨sBB, t 	â†’i g(t âˆ’â„“Ts)âŸ©

=
âˆš
2 Im

âŸ¨sBB, t 	â†’g(t âˆ’â„“Ts)âŸ©

.
(16.26b)
We next describe circuits to compute the RHS of (16.26a) & (16.26b). The circuit
to produce sBB from s was already discussed in Section 7.6 on the baseband rep-
resentation of passband signals (Figure 7.11). One multiplies s(t) by eâˆ’i2Ï€fct and
then passes the result through a lowpass ï¬lter whose cutoï¬€frequency Wc satisï¬es
W
2 â‰¤Wc â‰¤2fc âˆ’W
2 ,
i.e.,
sBB =

t 	â†’s(t) eâˆ’i2Ï€fct
â‹†LPFWc,
or, in terms of real operations:
Re

sBB

=

t 	â†’s(t) cos(2Ï€fct)

â‹†LPFWc,
Im

sBB

= âˆ’

t 	â†’s(t) sin(2Ï€fct)

â‹†LPFWc .
This circuit is depicted in Figure 16.3. Notice that this circuit depends only on
the carrier frequency fc and on the bandwidth W; it does not depend on the pulse
shape.
Once sBB has been computed, the calculation of the inner products on the RHS of
(16.26a) & (16.26b) is straightforward. For example, to compute the inner product
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.8 Recovering the Complex Symbols via Inner Products
307
cos(2Ï€fct)
90â—¦
Ã—
Ã—
r(t)
s(t)
BPFW,fc
LPFWc
LPFWc
W
2 â‰¤Wc â‰¤2fc âˆ’W
2
Re(sBB)
Im(sBB)
âˆ’sin(2Ï€fct)
Figure 16.3: QAM demodulation: the front-end.
on the RHS of (16.26a) we note that from (16.26a)
âŸ¨r, gI,â„“âŸ©=
âˆš
2 Re
	 âˆ
âˆ’âˆ
sBB(t) gâˆ—(t âˆ’â„“Ts) dt

=
âˆš
2
 âˆ
âˆ’âˆ
Re

sBB(t)

Re

g(t âˆ’â„“Ts)

dt
+
âˆš
2
 âˆ
âˆ’âˆ
Im

sBB(t)

Im

g(t âˆ’â„“Ts)

dt,
(16.27)
where the terms on the RHS can be computed by feeding Re(sBB) to a matched
ï¬lter matched to Re(g) and sampling the ï¬lterâ€™s output at time â„“Ts
 âˆ
âˆ’âˆ
Re

sBB(t)

Re

g(t âˆ’â„“Ts)

dt =

Re(sBB) â‹†Re(~g)

(â„“Ts),
(16.28)
and by feeding Im(sBB) to a matched ï¬lter matched to Im(g) and sampling the
ï¬lterâ€™s output at time â„“Ts
 âˆ
âˆ’âˆ
Im

sBB(t)

Im

g(t âˆ’â„“Ts)

dt =

Im(sBB) â‹†Im(~g)

(â„“Ts).
(16.29)
Similarly, to compute the inner product on the RHS of (16.26b) we note that from
(16.26b)
âŸ¨r, gQ,â„“âŸ©=
âˆš
2 Im
	 âˆ
âˆ’âˆ
sBB(t) gâˆ—(t âˆ’â„“Ts) dt

=
âˆš
2
 âˆ
âˆ’âˆ
Im

sBB(t)

Re

g(t âˆ’â„“Ts)

dt
âˆ’
âˆš
2
 âˆ
âˆ’âˆ
Re

sBB(t)

Im

g(t âˆ’â„“Ts)

dt,
(16.30)
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

308
Quadrature Amplitude Modulation
Im(sBB)
Re(sBB)
1
âˆš
2 âŸ¨r, gQ,â„“âŸ©
1
âˆš
2 âŸ¨r, gI,â„“âŸ©
~g
~g
â„“Ts
â„“Ts
Figure 16.4: QAM demodulation: matched ï¬ltering (g real).
where the inner products can be computed again using a matched ï¬lter:
 âˆ
âˆ’âˆ
Im

sBB(t)

Re

g(t âˆ’â„“Ts)

dt =

Im(sBB) â‹†Re(~g)

(â„“Ts),
 âˆ
âˆ’âˆ
Re

sBB(t)

Im

g(t âˆ’â„“Ts)

dt =

Re(sBB) â‹†Im(~g)

(â„“Ts).
Things become simpler when the pulse shape g is real. In this case (16.27) and
(16.30) simplify to
âŸ¨r, gI,â„“âŸ©=
âˆš
2

Re

sBB(t)

g(t âˆ’â„“Ts) dt,
g real,
(16.31a)
âŸ¨r, gQ,â„“âŸ©=
âˆš
2

Im

sBB(t)

g(t âˆ’â„“Ts) dt,
g real.
(16.31b)
Diagrams demonstrating how these inner products are computed are given in Fig-
ures 16.3 and 16.4. We have already discussed the ï¬rst diagram, which includes the
front-end bandpass ï¬lter and the circuit for producing sBB. The second diagram
includes the matched ï¬ltering needed to compute the RHS of (16.31a) and the
RHS of (16.31b). Notice that we have accomplished our second objective in that
the ï¬rst circuit depends only on the carrier frequency fc (and the bandwidth W)
and the second circuit depends on the pulse shape but not on the carrier frequency.
16.8.2
Computing Inner Products Directly
The astute reader may have noticed that neither the bandpass ï¬ltering of the
signal r nor the image rejection ï¬lters that produce sBB are needed for the com-
putation of the inner products. Indeed, starting from (16.8a)
âŸ¨r, gI,â„“âŸ©=

r, t 	â†’2 Re

gI,â„“,BB(t) ei2Ï€fct
= 2 Re

r, t 	â†’gI,â„“,BB(t) ei2Ï€fct
= 2 Re

t 	â†’r(t) eâˆ’i2Ï€fct, gI,â„“,BB

=
âˆš
2 Re

t 	â†’r(t) eâˆ’i2Ï€fct, t 	â†’g(t âˆ’â„“Ts)

,
(16.32a)
where the second equality follows because r is real and the last equality from
(16.9a). Similarly, starting from (16.8b)
âŸ¨r, gQ,â„“âŸ©=

r, t 	â†’2 Re

gQ,â„“,BB(t) ei2Ï€fct
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.9 Filtering QAM Signals
309
= 2 Re

r, t 	â†’gQ,â„“,BB(t) ei2Ï€fct
= 2 Re

t 	â†’r(t) eâˆ’i2Ï€fct, gQ,â„“,BB

=
âˆš
2 Re

t 	â†’r(t) eâˆ’i2Ï€fct, t 	â†’i g(t âˆ’â„“Ts)

=
âˆš
2 Im

t 	â†’r(t) eâˆ’i2Ï€fct, t 	â†’g(t âˆ’â„“Ts)

,
(16.32b)
where the fourth equality follows from (16.9b). Notice that the RHS of (16.32a)
and the RHS of (16.32b) do not involve any ï¬ltering. To see how to implement
them with real operations we can write them more explicitly as:
âŸ¨r, gI,â„“âŸ©=
âˆš
2 Re
	 âˆ
âˆ’âˆ
r(t) eâˆ’i2Ï€fct gâˆ—(t âˆ’â„“Ts) dt

,
âŸ¨r, gQ,â„“âŸ©=
âˆš
2 Im
	 âˆ
âˆ’âˆ
r(t) eâˆ’i2Ï€fct gâˆ—(t âˆ’â„“Ts) dt

,
or even more explicitly in terms of real operations as:
âŸ¨r, gI,â„“âŸ©=
âˆš
2
 âˆ
âˆ’âˆ
r(t) cos(2Ï€fct) Re

g(t âˆ’â„“Ts)

dt
âˆ’
âˆš
2
 âˆ
âˆ’âˆ
r(t) sin(2Ï€fct) Im

g(t âˆ’â„“Ts)

dt,
(16.33a)
âŸ¨r, gQ,â„“âŸ©= âˆ’
âˆš
2
 âˆ
âˆ’âˆ
r(t) cos(2Ï€fct) Im

g(t âˆ’â„“Ts)

dt
âˆ’
âˆš
2
 âˆ
âˆ’âˆ
r(t) sin(2Ï€fct) Re

g(t âˆ’â„“Ts)

dt.
(16.33b)
The two approaches we discussed for computing the inner products are, of course,
mathematically equivalent. The former makes more engineering sense, because the
bandpass ï¬lter typically guarantees that the energy in s is signiï¬cantly smaller
than in r, thus reducing the dynamic range required from the rest of the receiver.
The latter approach is mathematically cleaner because it requires less mathemat-
ical justiï¬cation. One need not check that the various ï¬lters satisfy the required
integrability conditions.
Moreover, this approach is more useful when r is not
energy-limited and when this is compensated for by the fast decay of the pulse
shape. (See, for example, the situation addressed by Proposition 3.4.4.)
16.9
Filtering QAM Signals
With all the simpliï¬cations aï¬€orded by using a real pulse shape, why use a complex
one?
Why not restrict attention to real pulse shapes?
One reason is that we
sometimes end up with a complex pulse shape through no choice of ours. This can
happen, for example, when the transmitted signal that we have carefully designed is
passed through some ï¬lter over which we have no control. Indeed, as we next show,
passing a QAM signal through a ï¬lter is tantamount to replacing its pulse shape
by a pulse shape that might be complex even when the original one is real. Thus,
a linearly-dispersive channel of the kind we shall study in Chapter 32 transforms
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

310
Quadrature Amplitude Modulation
a QAM signal of a real pulse shape into one of a complex pulse shape (and adds
noise to boot).
Proposition 16.9.1 (QAM Signals through Stable Filters). Let XPB be a QAM
signal of the form (16.6), where the pulse shape g is an integrable signal that is
bandlimited to W/2 Hz and where fc > W/2. Passing XPB through a stable real
ï¬lter of impulse response h âˆˆL1 is tantamount to replacing its pulse shape g by
the pulse shape p, which is deï¬ned as
p(t) =
 âˆ
âˆ’âˆ
Ë†g(f) Ë†h(f + fc) ei2Ï€ft df,
t âˆˆR
(16.34a)
and which is a complex integrable signal that is bandlimited to W/2 Hz and whose
FT is
Ë†p(f) = Ë†g(f) Ë†h(f + fc),
f âˆˆR.
(16.34b)
Proof. To identify the role of p, consider the signal t 	â†’2 Re

g(t) ei2Ï€fct
. Since g
is an integrable signal that is bandlimited to W/2 Hz, this signal is a real integrable
passband signal that is bandlimited to W Hz around fc. Its baseband representa-
tion is g. It follows from Proposition 7.6.13 that the result of convolving this signal
with some real h âˆˆL1 is a real integrable passband signal that is bandlimited to
W Hz around fc and whose baseband representation is p
p =

t 	â†’2 Re

g(t) ei2Ï€fct
â‹†h

BB.
(16.35)
Moreover, by the same proposition, the FT of p is given by (16.34b).
Having identiï¬ed the role of p and having established (16.34b), we next proceed
to study the result of ï¬ltering XPB. From Proposition 7.6.13 we conclude that
XPB â‹†h is an integrable passband signal that is bandlimited to W Hz around fc,
and that its baseband representation (XPB â‹†h)BB is of FT
f 	â†’Ë†XBB(f) Ë†h(f + fc),
f âˆˆR,
(16.36)
where XBB is the baseband representation of XPB and is given in (16.5a). The FT
of XBB can be readily computed from (16.5a) using the basic properties of the FT
(Table 6.1):
Ë†XBB(f) = A
n

â„“=1
Câ„“eâˆ’i2Ï€fâ„“Ts Ë†g(f),
f âˆˆR.
(16.37)
Consequently, the FT of (XPB â‹†h)BB is
f 	â†’A
n

â„“=1
Câ„“eâˆ’i2Ï€fâ„“Ts Ë†g(f) Ë†h(f + fc),
f âˆˆR.
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.10 Exercises
311
Taking the IFT (which recovers (XPB â‹†h)BB by Proposition 7.6.13), we obtain
(XPB â‹†h)BB(t) =
 âˆ
âˆ’âˆ
Ë†XBB(f) Ë†h(f + fc) ei2Ï€ft df
= A
n

â„“=1
Câ„“
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fâ„“Ts Ë†g(f) Ë†h(f + fc) ei2Ï€ft df
= A
n

â„“=1
Câ„“
 âˆ
âˆ’âˆ
Ë†g(f) Ë†h(f + fc) ei2Ï€f(tâˆ’â„“Ts) df
= A
n

â„“=1
Câ„“p(t âˆ’â„“Ts),
t âˆˆR,
(16.38)
where the last equality follows from (16.34a).
From (16.38) we conclude that
XPB â‹†h is also a QAM signal and that it is identical to XPB except that its pulse
shape is not g but p.
16.10
Exercises
Exercise 16.1 (Nyquistâ€™s Criterion and Passband Signals). Corollary 11.3.4 provides con-
ditions under which the time shifts of a signal by integer multiples of Ts are orthonormal.
Discuss how these conditions apply to real passband signals of bandwidth W around the
carrier frequency fc. Speciï¬cally:
(i) Plot the function
f â†’
âˆ

â„“=âˆ’âˆ
Ë†y
	
f + â„“
Ts


2
for the passband signal y of Figure 7.2. Pay attention to how the sum at positive
frequencies is inï¬‚uenced by the signalâ€™s FT at negative frequencies.
(ii) Show that there exists a passband signal Ï†(Â·) whose bandwidth W around the
carrier frequency fc is 1/(2Ts) and whose time shifts by integer multiples of Ts are
orthonormal if, and only if, 4Tsfc is an odd integer. Show that such a signal must
satisfy (outside a set of frequencies of Lebesgue measure zero)
Ë†Ï†(f)
 =
âˆš
Ts I
 |f| âˆ’fc
 â‰¤
1
4Ts
!
,
f âˆˆR.
(iii) Let Ï† be an energy-limited baseband signal of bandwidth W/2 whose FT is a
symmetric function of frequency and whose time shifts by integer multiples of (2Ts)
are orthonormal.
Let the carrier frequency fc be larger than W/2 and satisfy
that 4Tsfc is an odd integer. Show that the (possibly complex) passband signal
t â†’
âˆš
2 cos(2Ï€fct) Ï†(t) is of bandwidth W around the carrier fc, and its time shifts
by integer multiples of Ts are orthonormal.
Exercise 16.2 (How General is QAM?). Under what conditions on A, fc, Ï†, W, and Ts
can we view the signal
t â†’A Re

ei(2Ï€fct+Ï†)
n

â„“=1
Câ„“sinc

W(t âˆ’â„“Ts)

as a QAM signal?
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

312
Quadrature Amplitude Modulation
Exercise 16.3 (M-PSK). Consider a QAM signal XPB of the form (16.6) with the pulse
shape g: t â†’I{âˆ’Ts/2 â‰¤t < Ts/2} and symbols

Câ„“

that are IID and uniformly dis-
tributed over the set
{ei2Ï€/8, ei4Ï€/8, . . . , ei14Ï€/8, 1}.
(i) Plot a sample function of

XPB(t), t âˆˆR

.
(ii) Are the sample paths continuous?
(iii) Express XPB(t) in the form 2A cos

2Ï€fct + Î¦(t)

and describe Î¦(t). Plot a sample
path of

Î¦(t)

.
Exercise 16.4 (Transmission Rate, Encoder Rate, and Bandwidth). Data bits are to be
transmitted at rate Rb bits per second using QAM with a pulse shape Ï† satisfying the
orthonormality condition (16.11).
(i) Let W be the allotted bandwidth around the carrier frequency. What is the minimal
constellation size required for the data bits to be reliably communicated in the
absence of noise?
(ii) Repeat Part (i) if you are required to use a pulse shape of excess bandwidth 15%
or more.
Exercise 16.5 (Synthesis of 16-QAM). Let X1(Â·) and X2(Â·) be 4-QAM (QPSK) signals
that are given for every t âˆˆR by
XÎ½(t) = 2 Re

A
n

â„“=1
C(Î½)
â„“
g(t âˆ’â„“Ts) ei2Ï€fct

,
Î½ = 1, 2,
where the symbols

C(Î½)
â„“

take on the values Â±1 Â± i. Show that for the right choice of the
constant Î± âˆˆR, the signal
X(t) = Î±X1(t) + X2(t),
t âˆˆR
can be viewed as a 16-QAM signal with a square constellation.
Exercise 16.6 (Orthogonality of the In-Phase and Quadrature Components). Let the
pulse shape g be a real integrable signal that is bandlimited to W/2 Hz, and let the
carrier frequency fc be larger than W/2. Show that, even if the time shifts of g by integer
multiples of Ts are not orthonormal, the signals
t â†’g(t âˆ’â„“Ts) cos(2Ï€fct + Ï•) and t â†’g(t âˆ’â„“â€²Ts) sin(2Ï€fct + Ï•)
are orthogonal for all integers â„“, â„“â€² (not necessarily distinct). Here Ï• âˆˆ[âˆ’Ï€, Ï€) is arbitrary.
Exercise 16.7 (The Importance of the Phase). Let x and y be real integrable signals
that are bandlimited to W/2 Hz. Let the transmitted signal s be
s(t) = Re
	
x(t) + iy(t)

ei(2Ï€fct+Ï†T)
= x(t) cos(2Ï€fct + Ï†T) âˆ’y(t) sin(2Ï€fct + Ï†T),
t âˆˆR,
where fc > W/2, and where Ï†T denotes the phase of the transmitted carrier. The receiver
multiplies s(t) by 2 cos(2Ï€fct+Ï†R) (where Ï†R denotes the phase of the receiverâ€™s oscillator)
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,

16.10 Exercises
313
and passes the resulting product through a lowpass ï¬lter of cutoï¬€frequency W/2 to
produce the signal Ëœx:
Ëœx(t) =
	
Ï„ â†’s(Ï„) 2 cos(2Ï€fcÏ„ + Ï†R)

â‹†LPFW/2

(t),
t âˆˆR.
Express Ëœx(Â·) in terms of x(Â·), y(Â·), Ï†T and Ï†R. Evaluate your expression in the following
cases: Ï†T = Ï†R, Ï†T âˆ’Ï†R = Ï€, Ï†T âˆ’Ï†R = Ï€/2, and Ï†T âˆ’Ï†R = Ï€/4.
Exercise 16.8 (Phase Imprecision). Consider QAM with a real pulse shape and a receiver
that performs a conversion to baseband followed by matched ï¬ltering (Section 16.8.1).
Write an expression for the output of the receiver if its oscillator is at the right frequency
but lags the phase of the transmitterâ€™s oscillator by Î”Ï†.
Exercise 16.9 (Rotating a QAM Constellation). Show that rotating a QAM constellation
changes neither its second moment nor its minimum distance.
Exercise 16.10 (Optimal Rectangular Constellation). Consider all rectangular constella-
tions of the form
{a + ib, a âˆ’ib, âˆ’a + ib, âˆ’a âˆ’ib},
where a and b are real. Which of these constellations whose second moment is one has
the largest minimum distance?
Exercise 16.11 (Delaying and Reï¬‚ecting QAM Signals).
(i) Is a delayed QAM signal a QAM signal?
(ii) Is the mirror image of a QAM signal a QAM signal?
available at 
.018
14:31:31, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 17
Complex Random Variables and Processes
17.1
Introduction
We ï¬rst encountered complex random variables in Chapter 16 on QAM. There we
considered an encoder that maps k-tuples of bits into n-tuples of complex numbers,
and we then considered the result of applying this encoder to random bits. The
resulting symbols were therefore random and were taking values in the complex
ï¬eld, i.e., they were complex random variables. Complex random variables are
functions that map â€œluckâ€ into the complex ï¬eld: they map every outcome of the
experiment Ï‰ âˆˆÎ© to a complex number. Thus, they are very much like regular
random variables, except that they take values in the complex ï¬eld. They can
always be considered as pairs of real variables: their real and imaginary parts.
It is perfectly meaningful to discuss their expectation and variance.
If C is a
complex random variable, then
E[C] = E

Re(C)

+ i E

Im(C)

,
E
%
|C|2&
= E
%
Re(C)
2&
+ E
%
Im(C)
2&
,
and
Var[C] = E
%C âˆ’E[C]
2&
= E

|C|2
âˆ’
E[C]
2.
In this chapter we shall make the above deï¬nition of complex random variables
more formal and also discuss complex random vectors and complex stochastic pro-
cesses.
Complex random variables can be avoided if one treats such variables as pairs of
real variables. However, we do not recommend this approach. Many of the complex
random variables and processes encountered in Digital Communications possess ad-
ditional properties that simplify their manipulation, and complex random variables
are better suited to take advantage of these simpliï¬cations.
We begin this chapter with some notation followed by some basic deï¬nitions for
complex random variables.
We next introduce a property that simpliï¬es their
314
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.2 Notation
315
manipulation: properness. (Another such property, circular symmetry, is described
in Chapter 24.) Finally, we extend the discussion to complex random vectors and
conclude with a discussion of complex stochastic processes.
17.2
Notation
The notation we use in this chapter is fairly standard. The only issue that may
need clariï¬cation is the diï¬€erence between three matrix/vector operations: trans-
position, conjugation, and Hermitian conjugation. These operations are described
next.
All vectors in this chapter are column vectors. Thus, a vector a whose components
are a(1), . . . , a(n) is the column vector
a =
â›
âœ
âœ
âœ
â
a(1)
a(2)
...
a(n)
â
âŸ
âŸ
âŸ
â .
(17.1)
We shall sometimes refer to such a vector a as an n-vector to make the number of
its components explicit. For typesetting reasons, we shall usually use the notation
a =

a(1), . . . , a(n)T,
(17.2)
which is more space eï¬ƒcient. Here the operator (Â·)T denotes the matrix trans-
pose. Thus if we think of (a(1), . . . a(n)) as a 1 Ã— n matrix, then (a(1), . . . a(n))T is
this matrixâ€™s transpose, i.e., an n Ã— 1 matrix, or a vector. More generally, if A is
an n Ã— m matrix, then AT is an m Ã— n matrix whose Row-j Column-â„“component
is the Row-â„“Column-j component of A. We say that A is symmetric if AT = A.
We use (Â·)âˆ—to denote componentwise complex conjugation. Thus, if a is as
in (17.1), then
aâˆ—=
â›
âœ
âœ
âœ
â

a(1)âˆ—

a(2)âˆ—
...

a(n)âˆ—
â
âŸ
âŸ
âŸ
â .
(17.3)
We use (Â·)â€  to denote Hermitian conjugation, i.e., the componentwise conjugate
of the transposed matrix. Thus, if a is as in (17.1), then aâ€  is the 1 Ã— n matrix
aâ€  =

a(1)âˆ—, . . . ,

a(n)âˆ—
.
(17.4)
The Hermitian conjugate Aâ€  of an nÃ—m matrix A is an mÃ—n matrix whose Row-j
Column-â„“component is the complex conjugate of the Row-â„“Column-j component
of the matrix A. We say that a matrix A is conjugate-symmetric or self-adjoint
or Hermitian if Aâ€  = A.
Note that if a and b are n-vectors, then aTb is a scalar
aTb =
n

j=1
a(j) b(j),
(17.5)
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

316
Complex Random Variables and Processes
whereas abT is the n Ã— n matrix
abT =
â›
âœ
âœ
âœ
âœ
âœ
âœ
â
a(1)b(1)
a(1)b(2)
. . .
a(1)b(n)
a(2)b(1)
a(2)b(2)
. . .
a(2)b(n)
...
...
...
...
...
...
...
...
a(n)b(1)
a(n)b(2)
. . .
a(n)b(n)
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
17.3
Complex Random Variables
We say that C is a complex random variable (CRV) on the probability space
(Î©, F, P) if C : Î© â†’C is a mapping from Î© to the complex ï¬eld C such that both
Re(C) and Im(C) are random variables on (Î©, F, P).
Any CRV Z can be written in the form Z = X + i Y , where X and Y are real
random variables. But there are some advantages to studying complex random
variables over pairs of real random variables. Those will become apparent when we
discuss analytic functions of complex random variables and when we discuss com-
plex random variables that have special properties such as that of being â€œproperâ€
or that of being â€œcircularly-symmetric.â€
Many of the deï¬nitions related to complex random variables are similar to the
analogous deï¬nitions for pairs of real random variables, but some are not. We
shall try to emphasize the latter.
17.3.1
Distribution and Density
Since it makes no sense to say that one complex number is smaller than another, we
cannot deï¬ne the cumulative distribution function (CDF) of a CRV as in the real
case: an expression like â€œPr[Z â‰¤1 + i]â€ is meaningless. We can, however, discuss
the joint distribution function of the real and imaginary parts of a CRV, which
speciï¬es Pr[Re(Z) â‰¤x, Im(Z) â‰¤y] for all x, y âˆˆR. We say that two complex
random variables W and Z are of equal law (or have the same distribution) and
write W
L= Z, if the joint distribution of the pair (Re(W), Im(W)) is identical to
the joint distribution of the pair (Re(Z), Im(Z)):

W
L= Z

â‡â‡’

Pr

Re(W) â‰¤x, Im(W) â‰¤y

= Pr

Re(Z) â‰¤x, Im(Z) â‰¤y

, x, y âˆˆR

.
(17.6)
Similarly, we can deï¬ne the density function fZ(Â·) (if it exists) of a CRV Z at the
point z âˆˆC as the joint density of the real pair (Re(Z), Im(Z)) at (Re(z), Im(z)):
fZ(z) â‰œfRe(Z),Im(Z)

Re(z), Im(z)

,
z âˆˆC,
(17.7)
which can also be written as
fZ(z) =
âˆ‚2
âˆ‚x âˆ‚y Pr

Re(Z) â‰¤x, Im(Z) â‰¤y

x=Re(z),y=Im(z)
,
z âˆˆC.
(17.8)
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.3 Complex Random Variables
317
The notions of distribution function and density of a CRV extend immediately to
pairs of complex variables and, more generally, to n-tuples.
17.3.2
The Expectation
The expectation of a CRV can be deï¬ned in terms of the expectations of its real
and imaginary parts:
E[Z] = E[Re(Z)] + i E[Im(Z)] ,
(17.9)
provided that the two real expectations E[Re(Z)] and E[Im(Z)] are ï¬nite. With
this deï¬nition one can readily verify that, whenever E[Z] is deï¬ned, conjugation
and expectation commute
E[Zâˆ—] = (E[Z])âˆ—,
(17.10)
and
Re

E[Z]

= E

Re(Z)

,
(17.11a)
Im

E[Z]

= E

Im(Z)

.
(17.11b)
If the CRV Z has a density fZ(Â·), then the expectation E[g(Z)] for some measurable
function g: C â†’C can be formally written as
E

g(Z)

=

zâˆˆC
fZ(z) g(z) dz
(17.12)
or, in terms of real integrals, as
E

g(Z)

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
fZ(x + iy) Re

g(x + iy)

dx dy
+ i
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
fZ(x + iy) Im

g(x + iy)

dx dy.
(17.13)
Thus, rather than computing the distribution of g(Z) and then computing the
expectations of its real and imaginary parts, one can use (17.12).
17.3.3
The Variance
The deï¬nition of the variance of a CRV is not consistent with viewing the CRV as
a pair of real random variables. The variance Var[Z] of a CRV Z is deï¬ned as
Var[Z] â‰œE

|Z âˆ’E[Z]|2
(17.14a)
= E

|Z|2
âˆ’|E[Z]|2
(17.14b)
= Var

Re(Z)

+ Var

Im(Z)

.
(17.14c)
This deï¬nition should be contrasted with the deï¬nition of the covariance matrix
of the pair (Re(Z), Im(Z))
	
Var

Re(Z)

Cov

Re(Z), Im(Z)

Cov

Re(Z), Im(Z)

Var

Im(Z)


.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

318
Complex Random Variables and Processes
One can compute the variance of Z from the covariance matrix of (Re(Z), Im(Z)),
but not the other way around. Indeed, the variance of Z is just the trace of the
covariance matrix of (Re(Z), Im(Z)).
To derive (17.14b) from (17.14a) we note that
E

|Z âˆ’E[Z]|2
= E

(Z âˆ’E[Z])(Z âˆ’E[Z])âˆ—
= E

(Z âˆ’E[Z])(Zâˆ—âˆ’E[Zâˆ—])

= E

(Z âˆ’E[Z])Zâˆ—
âˆ’E

(Z âˆ’E[Z])

E

Zâˆ—
= E

(Z âˆ’E[Z])Zâˆ—
= E[ZZâˆ—] âˆ’E[Z] E[Zâˆ—]
= E

|Z|2
âˆ’|E[Z]|2,
where we only used the linearity of expectation and (17.10). Here the ï¬rst equality
follows by writing |w|2 as wwâˆ—; the second by (17.10); the third by simple algebra;
the fourth because the expectation of Z âˆ’E[Z] is zero; and the ï¬nal by (17.10).
To derive (17.14c) from (17.14b) we write E

|Z|2
as E

(Re(Z))2 + (Im(Z))2
and
express |E[Z]|2 using (17.9) as E[Re(Z)]2 + E[Im(Z)]2.
17.3.4
Proper Complex Random Variables
Many of the complex random variables that appear in Digital Communications
are proper. This is a concept that has no natural counterpart for real random
variables.
Deï¬nition 17.3.1 (Proper CRV). We say that the CRV Z is proper if the following
three conditions are all satisï¬ed: it is of zero-mean; it is of ï¬nite variance; and
E

Z2
= 0.
(17.15)
Notice that the LHS of (17.15) is, in general, a complex number, so (17.15) is
equivalent to two real equations:
E

Re(Z)2
= E

Im(Z)2
(17.16a)
and
E

Re(Z) Im(Z)

= 0.
(17.16b)
This leads to the following characterization of proper complex random variables.
Proposition 17.3.2. A CRV Z is proper if, and only if, all three of the following
conditions are satisï¬ed: Z is of zero mean; Re(Z) & Im(Z) have the same ï¬nite
variance; and Re(Z) & Im(Z) are uncorrelated.
An example of a proper CRV is one taking on the four values {Â±1, Â±i} equiprobably.
We mentioned earlier in Section 17.3.3 that the variance of a CRV is not the
same as the covariance matrix of the tuple consisting of its real and imaginary
parts.
While the covariance matrix determines the variance, the variance does
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.3 Complex Random Variables
319
not uniquely determine the covariance matrix. However, if a CRV is proper, then
its variance uniquely determines the covariance matrix of its real and imaginary
parts. Indeed, by Proposition 17.3.2 and (17.14c), a zero-mean ï¬nite-variance CRV
is proper if, and only if, the covariance matrix of the pair (Re(Z), Im(Z)) is
	 1
2Var[Z]
0
0
1
2Var[Z]

.
17.3.5
The Covariance
The covariance Cov[Z, W] between the complex random variables Z and W is
deï¬ned by
Cov[Z, W] â‰œE
%
Z âˆ’E[Z]

W âˆ’E[W]
âˆ—&
.
(17.17)
Again, this deï¬nition is diï¬€erent from the one for pairs of real random variables:
the covariance between two pairs of real random variables is a real matrix, whereas
the covariance between two CRVs is a complex scalar.
Some of the key properties of the covariance are listed next. They hold whenever
the Î±â€™s and Î²â€™s are deterministic complex numbers and the covariances on the RHS
are deï¬ned.
(i) Conjugate Symmetry:
Cov[Z, W] =

Cov[W, Z]
âˆ—.
(17.18)
(ii) Sesquilinearity:
Cov[Î±Z, W] = Î± Cov[Z, W] ,
(17.19)
Cov[Z1 + Z2, W] = Cov[Z1, W] + Cov[Z2, W] ,
(17.20)
Cov[Z, Î²W] = Î²âˆ—Cov[Z, W] ,
(17.21)
Cov[Z, W1 + W2] = Cov[Z, W1] + Cov[Z, W2] ,
(17.22)
and, more generally,
Cov

n

j=1
Î±jZj,
nâ€²

jâ€²=1
Î²jâ€²Wjâ€²

=
n

j=1
nâ€²

jâ€²=1
Î±j Î²âˆ—
jâ€² Cov[Zj, Wjâ€²] .
(17.23)
(iii) Relation with Variance:
Var[Z] = Cov[Z, Z] .
(17.24)
(iv) Variance of Linear Functionals:
Var

n

j=1
Î±jZj

=
n

j=1
n

jâ€²=1
Î±j Î±âˆ—
jâ€² Cov[Zj, Zjâ€²] .
(17.25)
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

320
Complex Random Variables and Processes
17.3.6
The Characteristic Function
The deï¬nition of the characteristic function of a CRV is consistent with viewing it as
a pair of real random variables. Recall that the characteristic function Î¦X : R â†’C
of a real random variable X is deï¬ned by
Î¦X : Ï– 	â†’E

eiÏ–X
,
Ï– âˆˆR.
(17.26)
For a pair of real random variables X, Y the joint characteristic function is the
mapping Î¦X,Y : R2 â†’C deï¬ned by
Î¦X,Y : (Ï–1, Ï–2) 	â†’E
%
ei(Ï–1X+Ï–2Y )&
,
Ï–1, Ï–2 âˆˆR.
(17.27)
Note that the expectations in (17.26) and (17.27) are always deï¬ned, because the
argument to the expectation operator is of modulus one (|eir| = 1, whenever r is
real). This motivates us to deï¬ne the characteristic function for a complex random
variable as follows.
Deï¬nition 17.3.3 (Characteristic Function of a CRV). The characteristic func-
tion Î¦Z : C â†’C of a complex random variable Z is deï¬ned as
Î¦Z(Ï–) â‰œE
%
ei Re(Ï–âˆ—Z)&
,
Ï– âˆˆC
= E
%
ei( Re(Ï–) Re(Z)+Im(Ï–) Im(Z))&
,
Ï– âˆˆC.
Here we can think of Re(Ï–) and Im(Ï–) as playing the role of Ï–1 and Ï–2 in (17.27).
17.3.7
Transforming Complex Variables
We next calculate the density of the result of applying a (deterministic) transfor-
mation to a CRV. The key to the calculation is to treat the CRV as a pair of real
random variables and to then apply the analogous result regarding the transfor-
mation of a random real tuple. To that end we recall the following basic theorem
regarding the transformation of real random vectors. In the theoremâ€™s statement
we encounter the notion of an open subset of Rn. Loosely speaking, D âŠ†Rn is an
open subset of Rn if to each x âˆˆD there corresponds some Ïµ > 0 such that the
ball of radius Ïµ and center x is fully contained in D.1
Theorem 17.3.4 (Transforming Real Random Vectors). Let g: D â†’R be a one-
to-one mapping from an open subset D of Rn onto a subset R of Rn. Assume
that g has continuous partial derivatives in D and that the Jacobian determinant
det (âˆ‚g(x)/âˆ‚x) is at no point of D zero.
Let the real random n-vector X have
the density function fX(Â·) and satisfy Pr[X âˆˆD] = 1. Then the random n-vector
Y = g(X) is of density
fY(y) =
fX(x)
det âˆ‚g(x)
âˆ‚x


x=gâˆ’1(y)
Â· I{y âˆˆR}.
(17.28)
1Thus, D is an open subset of Rn if D âŠ†Rn and if to each x âˆˆD there corresponds some
Ïµ > 0 such that each y âˆˆRn satisfying (x âˆ’y)T(x âˆ’y) â‰¤Ïµ2 is in D.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.3 Complex Random Variables
321
Using Theorem 17.3.4 we can relate the density of a CRV Z and the joint distri-
bution of its phase and magnitude.
Lemma 17.3.5 (The Joint Density of the Magnitude and Phase of a CRV). Let Z
be a CRV of density fZ(Â·), and let R = |Z| and Î˜ âˆˆ[âˆ’Ï€, Ï€) be the magnitude and
argument of Z:
Z = R eiÎ˜,
R â‰¥0, Î˜ âˆˆ[âˆ’Ï€, Ï€).
Then the joint distribution of the pair (R, Î˜) is of density
fR,Î˜(r, Î¸) = rfZ

r eiÎ¸
,
r > 0, Î¸ âˆˆ[âˆ’Ï€, Ï€).
(17.29)
Proof. This result follows directly from Theorem 17.3.4 by computing the absolute
value of the Jacobian determinant of the transformation2 (x, y) 	â†’(r, Î¸) where
r =

x2 + y2 and Î¸ = arctan(y/x):
det
-
âˆ‚r
âˆ‚x
âˆ‚r
âˆ‚y
âˆ‚Î¸
âˆ‚x
âˆ‚Î¸
âˆ‚y
. =
1

x2 + y2
= 1
r .
For the next change-of-variables result we recall some basic concepts from Complex
Analysis. Given some z0 âˆˆC and some nonnegative real number r â‰¥0, we denote
by D(z0, r) the disc of radius r that is centered at z0:
D(z0, r) â‰œ{z âˆˆC : |z âˆ’z0| < r}.
We say that a subset D of the complex plane is open if to each z âˆˆD there
corresponds some Ïµ > 0 such that D(z0, Ïµ) âŠ†D. Let g: D â†’C be some function
from an open set D âŠ†C to C. Let z0 be in D. We say that g(Â·) is diï¬€erentiable
at z0 âˆˆD and that its derivative at z0 is the complex number gâ€²(z0), if for every
Ïµ > 0 there exists some Î´ > 0 such that

g

z0 + h

âˆ’g

z0

h
âˆ’gâ€²
z0

 â‰¤Ïµ,
(17.30)
whenever the complex number h âˆˆC satisï¬es 0 < |h| â‰¤Î´. It is important to note
that here h is complex. If g is diï¬€erentiable at every z âˆˆD, then we say that g is
holomorphic or analytic in D.3
Deï¬ne the mappings
u, v: {x, y âˆˆR : x + iy âˆˆD} â†’R
(17.31a)
by
u(x, y) = Re

g(x + iy)

,
(17.31b)
and
v(x, y) = Im

g(x + iy)

.
(17.31c)
2Here D is the set R2 without the origin.
3There is some confusion in the literature about the terms analytic, holomorphic, and
regular. We are following here (Rudin, 1987, Chapter 10).
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

322
Complex Random Variables and Processes
Proposition 17.3.6 (The Cauchy-Riemann Equations). Let D âŠ†C be open and
let g: D â†’C be analytic in D. Let u, v be deï¬ned by (17.31). Then u and v
satisfy the Cauchy-Riemann equations
âˆ‚u(x, y)
âˆ‚x
= âˆ‚v(x, y)
âˆ‚y
,
(17.32a)
âˆ‚u(x, y)
âˆ‚y
= âˆ’âˆ‚v(x, y)
âˆ‚x
(17.32b)
at every x, y âˆˆR such that x + iy âˆˆD, and
gâ€²(z) =
	âˆ‚u(x, y)
âˆ‚x
+ iâˆ‚v(x, y)
âˆ‚x


(x,y)=( Re(z),Im(z))
,
z âˆˆD.
(17.33)
Moreover, the partial derivatives in (17.32) are continuous in the subset of R2
deï¬ned by {x, y âˆˆR : x + iy âˆˆD}.
Proof. See (Rudin, 1987, Chapter 11, Theorem 11.2 & Theorem 11.4) or (Nehari,
1975, Chapter II, Section 5 & Chapter III, Section 3).
We can now state the change-of-variables theorem for CRVs.
Theorem 17.3.7 (Transforming Complex Random Variables). Let g: D â†’R be
a one-to-one mapping from an open subset D of C onto a subset R of C. Assume
that g is analytic in D and that at no point of D is the derivative of g zero. Let
the CRV have the density function fZ(Â·) and satisfy Pr[Z âˆˆD] = 1. Then the CRV
deï¬ned by W = g(Z) is of density
fW (w) =
fZ(z)
|gâ€²(z)|2

z=gâˆ’1(w)
I{w âˆˆR}.
(17.34)
Here gâˆ’1(w) denotes the point in D that is mapped by g to w.
Note 17.3.8. The square in (17.34) does not appear in dealing with real random
variables. It appears here because a mapping of complex numbers is essentially
two-dimensional: scaling by Î± âˆˆC translates to a scaling of area by |Î±|2.
Proof. To prove (17.34) we begin by expressing the function g(Â·) as
g(x + iy) = u(x, y) + iv(x, y),

x, y âˆˆR, x + iy âˆˆD

,
where u(x, y) = Re(g(x + iy)) and v(x, y) = Im(g(x + iy)) are deï¬ned in (17.31b)
and (17.31c). The density of g(Z) is, by deï¬nition, the joint density of the pair
u(Re(Z), Im(Z)), v(Re(Z), Im(Z)). And the joint density of the pair(Re(Z), Im(Z))
is just the density of Z.
Thus, if we could relate the joint density of the pair
u(Re(Z), Im(Z)), v(Re(Z), Im(Z)) to the joint density of the pair (Re(Z), Im(Z)),
then we could relate the density of g(Z) to the density of Z.
To relate the joint density of the pair u(Re(Z), Im(Z)), v(Re(Z), Im(Z)) to the
joint density of the pair (Re(Z), Im(Z)) we employ Theorem 17.3.4. To that end
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.4 Complex Random Vectors
323
we need to compute the absolute value of the Jacobian determinant. This we do
as follows:
det
- âˆ‚u
âˆ‚x
âˆ‚u
âˆ‚y
âˆ‚v
âˆ‚x
âˆ‚v
âˆ‚y
. =
det
- âˆ‚u
âˆ‚x
âˆ’âˆ‚v
âˆ‚x
âˆ‚v
âˆ‚x
âˆ‚u
âˆ‚x
.
=
	âˆ‚u
âˆ‚x

2
+
	âˆ‚v
âˆ‚x

2
= |gâ€²(x + iy)|2,
(17.35)
where the ï¬rst equality follows from the Cauchy-Riemann equations (17.32); the
second from a direct calculation of the determinant of a 2 Ã— 2 matrix; and where
the last equality follows from (17.33). The theorem now follows from (17.35) and
Theorem 17.3.4.
17.4
Complex Random Vectors
We say that Z = (Z(1), . . . , Z(n))T is a complex random vector on the probability
space (Î©, F, P) if it is a mapping from the outcome set Î© to Cn such that the real
2n-vector

Re

Z(1)
, Im

Z(1)
, . . . , Re

Z(n)
, Im

Z(n)T
comprising the real and imaginary parts of its components is a real random vector
on (Î©, F, P), i.e., if each of the components of Z is a CRV.
We say that the complex random vector Z = (Z(1), . . . , Z(n))T and the complex
random vector W = (W (1), . . . , W (n))T are of equal law (or have the same distri-
bution) and write Z
L= W, if the real vector taking values in R2n whose components
are the real and imaginary parts of the components of Z has the same distribution
as the analogous vector for W, i.e., if for all x1, . . . , xn, y1, . . . , yn âˆˆR
Pr
%
Re

Z(1)
â‰¤x1, Im

Z(1)
â‰¤y1, . . . , Re

Z(n)
â‰¤xn, Im

Z(n)
â‰¤yn
&
= Pr
%
Re

W (1)
â‰¤x1, Im

W (1)
â‰¤y1, . . . , Re

W (n)
â‰¤xn, Im

W (n)
â‰¤yn
&
.
The expectation of a complex random vector is the vector consisting of the ex-
pectation of each of its components. We say that a complex random vector is of
ï¬nite variance if each of its components is a CRV of ï¬nite variance.
17.4.1
The Covariance Matrix
The discussion in Section 17.3.5 can be generalized to random complex vectors.
The covariance matrix KZZ of a ï¬nite-variance complex random n-vector Z is
deï¬ned as the conjugate-symmetric n Ã— n matrix
KZZ â‰œE

(Z âˆ’E[Z])(Z âˆ’E[Z])â€ 
.
(17.36)
Once again, this deï¬nition is not consistent with viewing the random complex
vector as a vector of length 2n of real random variables. The latter would have a
real symmetric 2n Ã— 2n covariance matrix.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

324
Complex Random Variables and Processes
The reader may wonder why we have chosen to deï¬ne the covariance and the covari-
ance matrix with the conjugation sign. Why not look at E

(Z âˆ’E[Z])(Z âˆ’E[Z])T
?
The reason is that (17.36) is simply much more useful in applications. For example,
for any deterministic Î±1, . . . , Î±n âˆˆC the variance of n
j=1 Î±jZj can be computed
from KZZ (using (17.25)) but not from E

(Z âˆ’E[Z])(Z âˆ’E[Z])T
.
17.4.2
Proper Complex Random Vectors
The notion of proper complex random variables extends to vectors:
Deï¬nition 17.4.1 (Proper Complex Random Vector). A complex random vector Z
is said to be proper if the following three conditions are all met: it is of zero mean;
it is of ï¬nite variance; and
E

ZZT
= 0.
(17.37)
An alternative deï¬nition can be given based on linear functionals:
Proposition 17.4.2. The complex random n-vector Z is proper if, and only if, for
every deterministic vector Î± âˆˆCn the CRV Î±TZ is proper.
Proof. We begin by noting that Z is of zero mean if, and only if, Î±TZ is of zero
mean for all Î± âˆˆCn. This can be seen from the relation
E

Î±TZ

= Î±TE[Z] ,
Î± âˆˆCn.
(17.38)
Indeed, (17.38) demonstrates that if Z is of zero mean then so is Î±TZ for every
Î± âˆˆCn. Conversely, if Î±TZ is of zero mean for all Î± âˆˆCn, then, a fortiori, it must
also be of zero mean for the choice of Î± = E[Z]âˆ—, which yields that 0 = E[Z]â€  E[Z]
and hence that E[Z] must be zero (because E[Z]â€  E[Z] is the sum of the squared
magnitudes of the components of E[Z]).
We next note that Z is of ï¬nite variance if, and only if, Î±TZ is of ï¬nite variance
for every Î± âˆˆCn. The proof is not diï¬ƒcult and is omitted.
We thus continue with the proof under the assumption that Z is of zero mean and
of ï¬nite variance. We note that for any deterministic complex vector Î± âˆˆCn
E

(Î±TZ)2
= E

(Î±TZ)(Î±TZ)

= E

(Î±TZ)(Î±TZ)T
= E

Î±TZZTÎ±

= Î±TE

ZZT
Î±,
Î± âˆˆCn,
(17.39)
where the ï¬rst equality follows by writing the square of a random variable as the
product of the variable by itself; the second because the transpose of a scalar is
the original scalar; the third by the transpose rule
(AB)T = BTAT;
(17.40)
and the ï¬nal equality because Î± is deterministic.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.4 Complex Random Vectors
325
From (17.39) it follows that if Z is proper, then so is Î±TZ for all Î± âˆˆCn. Actually,
(17.39) also proves the reverse implication by substituting A = E

ZZT
in the
following fact from Matrix Theory:

Î±TAÎ± = 0, Î± âˆˆCn
=â‡’

A = 0

,
A symmetric.
(17.41)
To prove this fact from Matrix Theory assume that A is symmetric, i.e., that
a(j,â„“) = a(â„“,j),
j, â„“âˆˆ{1, . . . , n}.
(17.42)
Let Î± = eâ„“where eâ„“is all-zero except for its â„“-th component, which is one. The
equality eT
â„“Aeâ„“= 0 for every â„“âˆˆ{1, . . . , n} is equivalent to
a(â„“,â„“) = 0,
â„“âˆˆ{1, . . . , n}.
(17.43)
Next choose Î± = ej + eâ„“. The equality
(ej + eâ„“)TA(ej + eâ„“) = 0
for every j, â„“âˆˆ{1, . . . , n} is then equivalent to
a(j,â„“) + a(j,j) + a(â„“,j) + a(â„“,â„“) = 0,
j, â„“âˆˆ{1, . . . , n}.
(17.44)
Equations (17.42), (17.43), and (17.44) guarantee that the matrix A is all-zero.
An important observation regarding complex random vectors is that a linearly-
transformed proper vector is also proper:
Proposition 17.4.3 (Linear Transformation of a Proper Random Vector). If the
complex random n-vector Z is proper, then so is the complex random m-vector AZ
for every deterministic m Ã— n complex matrix A.
Proof. We leave it to the reader to verify that the hypothesis that Z is proper
implies that AZ must be of zero mean and of ï¬nite variance. To show that AZ
is proper, it thus remains to show that E

(AZ)(AZ)T
= 0. This we do by direct
calculation:
E

(AZ)(AZ)T
= E

AZZTAT
= AE

ZZT
AT
= 0,
where the ï¬rst equality follows from the rule for the transpose of a product (17.40);
the second because A is deterministic; and the last from the hypothesis that Z is
proper, so E

ZZT
= 0.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

326
Complex Random Variables and Processes
17.4.3
The Characteristic Function
The deï¬nition we gave in Section 17.3.6 for the characteristic function of a CRV
extends naturally to vectors: the characteristic function Î¦Z : Cn â†’C of a complex
random n-vector Z is deï¬ned as
Î¦Z(Ï–) â‰œE
%
ei Re(Ï–â€ Z)&
,
Ï– âˆˆCn.
The connection between the characteristic function of the complex n-vector Z and
that of the (real) random 2n-vector4

Re

Z(1)
, Im

Z(1)
, . . . , Re

Z(n)
, Im

Z(n)T
is revealed once we note that
Re

Ï–â€ Z

= Re

Ï–(1)
Re

Z(1)
+ Im

Ï–(1)
Im

Z(1)
+ Â· Â· Â·
+ Re

Ï–(n)
Re

Z(n)
+ Im

Ï–(n)
Im

Z(n)
.
This connection and the fact that two random 2n-vectors have identical laws if,
and only if, they have identical characteristic functions (Proposition 23.4.4) can be
used to obtain the following theorem:
Theorem 17.4.4. Two complex n-vectors are of equal law if, and only if, their
characteristic functions are identical:

Z
L= W

â‡â‡’

Î¦Z(Ï–) = Î¦W(Ï–), Ï– âˆˆCn
.
(17.45)
Corollary 17.4.5. The complex random n-vectors Z and W are of equal law if, and
only if, for every deterministic vector Î± âˆˆCn the complex random variables Î±TZ
and Î±TW are of equal law:

Z
L= W

â‡â‡’

Î±TZ
L= Î±TW,
Î± âˆˆCn
.
(17.46)
Proof. The direction that needs proof is that equality in law of all linear combi-
nations implies equality in law between the vectors. But this readily follows from
Theorem 17.4.4, because equality in law of the linear combinations implies that the
law of Ï–â€ Z is equal to the law of Ï–â€ W for every Ï– âˆˆCn. This in turn implies
that ei Re(Ï–â€ Z) L= ei Re(Ï–â€ W), from which, upon taking expectations, we obtain that
Z and W have identical characteristic functions. Thus, by the theorem, they are
equal in law.
17.4.4
Transforming Complex Random Vectors
The change of density rule (17.34) can be generalized to analytic multi-variable
mappings (Exercise 17.13). But here we shall only present a version of this result
for linear mappings:
4The characteristic function of a (real) random vector is discussed in Section 23.4.4.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.4 Complex Random Vectors
327
Lemma 17.4.6 (Linearly Transforming Complex Random Vectors). Let the com-
plex random n-vector W be given by
W = AZ,
where A is a nonsingular deterministic complex nÃ—n matrix, and where the complex
random n-vector Z has the density fZ(Â·). Then W is of density
fW(w) =
1
|det A|2 fZ(Aâˆ’1w),
w âˆˆCn.
(17.47)
Proof. The proof is based on viewing the complex n Ã— n linear transformation
from Z to W as a 2nÃ—2n real transformation, and on then applying Theorem 17.3.4.
Stack the real parts of the components of Z on top of the imaginary parts in a real
random 2n-vector S:
S =

Re

Z(1)
, . . . , Re

Z(n)
, Im

Z(1)
, . . . , Im

Z(n)T
.
(17.48)
Similarly, stack the real parts of the components of W on top of the imaginary
parts in a real random 2n-vector T:
T =

Re

W (1)
, . . . , Re

W (n)
, Im

W (1)
, . . . , Im

W (n)T
.
We can then express T as the result of multiplying the random vector S by a
2n Ã— 2n real matrix:
T =
	Re(A)
âˆ’Im(A)
Im(A)
Re(A)

S,
where Re(A) and Im(A) denote the componentwise real and imaginary parts of A.
The result will follow from Theorem 17.3.4 once we show that the absolute value
of the Jacobian determinant of this transformation is |det A|2. Using elementary
row and column operations we compute:
det
	Re(A)
âˆ’Im(A)
Im(A)
Re(A)

= det
	 A
âˆ’Im(A)
âˆ’iA
Re(A)

= det
	A
âˆ’Im(A)
0
Aâˆ—

= (det A) (det Aâˆ—)
= |det A|2,
where the ï¬rst equality follows by the elementary column operations of multiplying
the right columns by (âˆ’i) and adding the result to the left columns; the second
from the elementary row operations of multiplying the top rows by i and adding
the result to the bottom rows; the third from the identity
det
	
B
C
0
D

= (det B) (det D);
and the last by noting that for any square matrix B
det(Bâˆ—) = (det B)âˆ—.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

328
Complex Random Variables and Processes
17.5
Discrete-Time Complex Stochastic Processes
Deï¬nition 12.2.1 of a real stochastic process extends to the complex case as follows.
Deï¬nition 17.5.1 (Complex Stochastic Process). A complex stochastic pro-
cess (CSP)

Z(t), t âˆˆT

is a collection of complex random variables that are
deï¬ned on a common probability space (Î©, F, P) and that are indexed by some
set T .
A CSP

Z(t), t âˆˆT

is said to be centered if for each t âˆˆT the CRV Z(t) is of
zero mean. Similarly, the CSP is said to be of ï¬nite variance if for each t âˆˆT the
CRV Z(t) is of ï¬nite variance. A discrete-time CSP corresponds to the case where
the index set T is the set of integers Z. Discrete-time complex stochastic processes
are not very diï¬€erent from the real-valued ones we encountered in Chapter 13.
Consequently, we shall present the main deï¬nitions and results succinctly with
an emphasis on the issues where the complex and real processes diï¬€er.
As in
Chapter 13, when dealing with a discrete-time CSP we shall use subscripts to
index the complex random variables and denote the process by

ZÎ½, Î½ âˆˆZ

or,
more succinctly, by

ZÎ½

.
A discrete-time CSP

ZÎ½, Î½ âˆˆZ

is said to be stationary, or strict-sense sta-
tionary, or strongly stationary if for every positive integer n and for every
Î·, Î·â€² âˆˆZ, the joint distribution of the n-tuple (ZÎ·, . . . ZÎ·+nâˆ’1) is identical to the
joint distribution of the n-tuple (ZÎ·â€², . . . , ZÎ·â€²+nâˆ’1). This deï¬nition is essentially
identical to the analogous deï¬nition for real processes (Deï¬nition 13.2.1). Similarly,
Proposition 13.2.2 holds verbatim also for complex stochastic processes. Proposi-
tion 13.2.3 also holds for complex stochastic processes with the slight modiï¬cation
that the deterministic coeï¬ƒcients Î±1, . . . , Î±n are now allowed to be arbitrary com-
plex numbers:
Proposition 17.5.2. A discrete-time CSP

ZÎ½

is stationary if, and only if, for
every n âˆˆN, all Î·, Î½1, . . . , Î½n âˆˆZ, and all Î±1, . . . , Î±n âˆˆC,
n

j=1
Î±jZÎ½j
L=
n

j=1
Î±jZÎ½j+Î·.
(17.49)
The deï¬nition of a wide-sense stationary CSP is very similar to the analogous
deï¬nition for real processes (Deï¬nition 13.3.1).
Deï¬nition 17.5.3 (Wide-Sense Stationary Discrete-Time CSP). We say that
a discrete-time CSP

ZÎ½

is wide-sense stationary or weakly stationary or
covariance stationary if the following three conditions all hold:
1) For every Î½ âˆˆZ the CRV ZÎ½ is of ï¬nite variance.
2) The mean of ZÎ½ does not depend on Î½.
3) The expectation E[ZÎ½ Zâˆ—
Î½â€²] depends on Î½â€² and Î½ only via their diï¬€erence Î½âˆ’Î½â€²:
E[ZÎ½ Zâˆ—
Î½â€²] = E

ZÎ½+Î· Zâˆ—
Î½â€²+Î·

,
Î½, Î½â€², Î· âˆˆZ.
(17.50)
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.5 Discrete-Time Complex Stochastic Processes
329
Note the conjugation in (17.50). We do not require that E[ZÎ½â€²ZÎ½] be computable
from Î½ âˆ’Î½â€²; it may or may not be. Thus, we do not require that the matrix
	E[Re(ZÎ½â€²) Re(ZÎ½)]
E[Re(ZÎ½â€²) Im(ZÎ½)]
E[Im(ZÎ½â€²) Re(ZÎ½)]
E[Im(ZÎ½â€²) Im(ZÎ½)]

be computable from Î½ âˆ’Î½â€². This matrix is, however, computable from Î½ âˆ’Î½â€² if the
process is proper:
Deï¬nition 17.5.4 (Proper CSP). A discrete-time CSP

ZÎ½

is said to be proper
if the following three conditions all hold: it is centered; it is of ï¬nite variance; and
E[ZÎ½ ZÎ½â€²] = 0,
Î½, Î½â€² âˆˆZ.
(17.51)
Equivalently, a discrete-time CSP

ZÎ½

is proper if, and only if, for every positive
integer n and all Î½1, . . . , Î½n âˆˆZ the complex random vector (ZÎ½1, . . . , ZÎ½n)T is
proper. Equivalently,

ZÎ½

is proper if, and only if, for every positive integer n, all
Î±1, . . . , Î±n âˆˆC, and all Î½1, . . . , Î½n âˆˆZ
n

j=1
Î±jZÎ½j is proper
(17.52)
(Proposition 17.4.2).
The alternative deï¬nition of WSS real processes in terms of the variance of linear
functionals of the process (Proposition 13.3.3) requires little change:
Proposition 17.5.5. A ï¬nite-variance discrete-time CSP

ZÎ½

is WSS if, and only
if, for every n âˆˆN, all Î·, Î½1, . . . , Î½n âˆˆZ, and all Î±1, . . . , Î±n âˆˆC
n

j=1
Î±jZÎ½j and
n

j=1
Î±jZÎ½j+Î·
have the same mean & variance.
(17.53)
Proof. We begin by assuming that

ZÎ½

is WSS and prove (17.53). The equality
of expectations follows directly from the linearity of expectation and from the fact
that because

ZÎ½

is WSS the mean of ZÎ½ does not depend on Î½. In proving the
equality of the variances we use (17.25):
Var

n

j=1
Î±jZÎ½j+Î·

=
n

j=1
n

jâ€²=1
Î±j Î±âˆ—
jâ€² Cov

ZÎ½j+Î·, ZÎ½jâ€²+Î·

=
n

j=1
n

jâ€²=1
Î±j Î±âˆ—
jâ€² Cov

ZÎ½j, ZÎ½jâ€²

= Var

n

j=1
Î±jZÎ½j

,
where the second equality follows from the wide-sense stationarity of

ZÎ½

, and the
last equality follows again from (17.25).
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

330
Complex Random Variables and Processes
We next turn to proving that (17.53) implies that

ZÎ½

is WSS. Choosing n = 1 and
Î±1 = 1 we obtain, by considering the equality of the means, that E[ZÎ½] = E[ZÎ½+Î·]
for all Î· âˆˆZ, i.e., that the mean of the process is constant. And, by considering
the equality of the variances, we obtain that the random variables

ZÎ½

all have
the same variance
Var[ZÎ½] = Var[ZÎ½+Î·] ,
Î½, Î· âˆˆZ.
(17.54)
Choosing n = 2 and Î±1 = Î±2 = 1 we obtain from the equality of the variances
Var[ZÎ½1 + ZÎ½2] = Var[ZÎ½1+Î· + ZÎ½2+Î·] .
(17.55)
But, by (17.25) and (17.54),
Var[ZÎ½1 + ZÎ½2] = 2 Var[Z1] + 2 Re

Cov[ZÎ½1, ZÎ½2]

(17.56)
and similarly
Var[ZÎ½1+Î· + ZÎ½2+Î·] = 2 Var[Z1] + 2 Re

Cov[ZÎ½1+Î·, ZÎ½2+Î·]

.
(17.57)
By (17.55), (17.56), and (17.57)
Re

Cov[ZÎ½1+Î·, ZÎ½2+Î·]

= Re

Cov[ZÎ½1, ZÎ½2]

,
Î·, Î½1, Î½2 âˆˆZ.
(17.58)
We now repeat the argument with Î±1 = 1 and Î±2 = i:
Var[ZÎ½1 + i ZÎ½2] = Var[ZÎ½1] + Var[ZÎ½2] + 2 Re

Cov[ZÎ½1, i ZÎ½2]

= 2 Var[Z1] + 2 Im

Cov[ZÎ½1, ZÎ½2]

and similarly
Var[ZÎ½1+Î· + i ZÎ½2+Î·] = 2 Var[Z1] + 2 Im

Cov[ZÎ½1+Î·, ZÎ½2+Î·]

,
so the equality of the variances implies
Im

Cov[ZÎ½1+Î·, ZÎ½2+Î·]

= Im

Cov[ZÎ½1, ZÎ½2]

,
Î·, Î½1, Î½2 âˆˆZ,
which combines with (17.58) to prove Cov[ZÎ½1+Î·, ZÎ½2+Î·] = Cov[ZÎ½1, ZÎ½2].
As with real processes, a comparison of Propositions 17.5.5 and 17.5.2 yields that
any ï¬nite-variance stationary CSP is also WSS. The reverse is not true.
Deï¬nition 17.5.6 (Autocovariance Function). We deï¬ne the autocovariance func-
tion KZZ : Z â†’C of a discrete-time WSS CSP

ZÎ½

as5
KZZ(Î·) â‰œCov[ZÎ½+Î·, ZÎ½]
(17.59)
= E
%
ZÎ½+Î· âˆ’E[Z1]

ZÎ½ âˆ’E[Z1]
âˆ—&
,
Î· âˆˆZ.
5Some authors, e.g., (Grimmett and Stirzaker, 2001), deï¬ne KZZ(Î·) as Cov[ZÎ½, ZÎ½+Î·]. Our
deï¬nition follows (Doob, 1990).
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.5 Discrete-Time Complex Stochastic Processes
331
By mimicking the derivations of (13.12) (taking into account the conjugate symme-
try (17.18)) we obtain that the autocovariance function KZZ of every discrete-time
WSS CSP

ZÎ½

satisï¬es the conjugate-symmetry condition
KZZ(âˆ’Î·) = Kâˆ—
ZZ(Î·) ,
Î· âˆˆZ.
(17.60)
Similarly, by mimicking the derivation of (13.13) (i.e., from the nonnegativity of
the variance and from (17.25)), we obtain that the autocovariance function of such
a process satisï¬es, for every positive integer n,
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±âˆ—
Î½â€² KZZ(Î½ âˆ’Î½â€²) â‰¥0,
Î±1, . . . , Î±n âˆˆC.
(17.61)
In analogy to the real case (Theorem 13.5.2), (17.60) and (17.61) fully characterize
the possible autocovariance functions in the sense that any function K: Z â†’C
satisfying
K(âˆ’Î·) = Kâˆ—(Î·),
Î· âˆˆZ
(17.62)
and
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±âˆ—
Î½â€²K(Î½ âˆ’Î½â€²) â‰¥0,
Î±1, . . . , Î±n âˆˆC
(17.63)
is the autocovariance function of some discrete-time WSS CSP.6 If K: Z â†’C
satisï¬es (17.62) and (17.63), then we say that K(Â·) is a positive deï¬nite function
from the integers to the complex ï¬eld.
Deï¬nition 13.6.1 of the power spectral density SZZ requires no change.
We
require that SZZ be integrable on the interval [âˆ’1/2, 1/2) and that
KZZ(Î·) =
 1/2
âˆ’1/2
SZZ(Î¸) ei2Ï€Î·Î¸ dÎ¸,
Î· âˆˆZ.
(17.64)
Proposition 13.6.3 does require some alteration. Indeed, for complex stochastic
processes the PSD need not be a symmetric function. However, the main result
(that the PSD is real and nonnegative) remains true:
Proposition 17.5.7 (PSDs of Complex Processes Are Nonnegative).
(i) If the discrete-time WSS CSP

ZÎ½

is of PSD SZZ, then
SZZ(Î¸) â‰¥0,
(17.65)
except possibly on a subset of the interval [âˆ’1/2, 1/2) of Lebesgue measure
zero.
(ii) If a function S: [âˆ’1/2, 1/2) â†’R is integrable and nonnegative, then there
exists a proper discrete-time WSS CSP7 
ZÎ½

whose PSD SZZ is given by
SZZ(Î¸) = S(Î¸),
Î¸ âˆˆ[âˆ’1/2, 1/2).
6In fact, it is the autocovariance function of some proper Gaussian stochastic process. Com-
plex Gaussian random processes will be discussed in Chapter 24.
7The process can be taken to be Gaussian; see Chapter 24.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

332
Complex Random Variables and Processes
As in the real case, by possibly changing the value of SZZ on the set of Lebesgue
measure zero where (17.65) is violated, we can obtain a power spectral density that
is nonnegative for all Î¸ âˆˆ[âˆ’1/2, 1/2). Consequently, we shall always assume that
the PSD, if it exists, is nonnegative for all Î¸ âˆˆ[âˆ’1/2, 1/2).
Proof of Proposition 17.5.7. We begin with Part (i), where we need to prove the
nonnegativity of the PSD. We shall only sketch the proof. We recommend reading
Appendix A through Theorem A.2.2 before reading this proof.
Let KZZ denote the autocovariance function of the WSS CSP

ZÎ½

.
Applying
(17.61) with
Î±Î½ = eâˆ’i2Ï€Î½Î¸,
Î½ âˆˆ{1, . . . , n}
and thus
Î±Î½ Î±âˆ—
Î½â€² = ei2Ï€(Î½â€²âˆ’Î½)Î¸,
Î½, Î½â€² âˆˆ{1, . . . , n},
we obtain
0 â‰¤
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±âˆ—
Î½â€² KZZ(Î½ âˆ’Î½â€²)
=
n

Î½=1
n

Î½â€²=1
ei2Ï€(Î½â€²âˆ’Î½)Î¸ KZZ(Î½ âˆ’Î½â€²)
=
nâˆ’1

Î·=âˆ’(nâˆ’1)

n âˆ’|Î·|

ei2Ï€Î·Î¸ KZZ(âˆ’Î·),
Î¸ âˆˆ[âˆ’1/2, 1/2).
Dividing by n we obtain
0 â‰¤
nâˆ’1

Î·=âˆ’(nâˆ’1)
	
1 âˆ’|Î·|
n

ei2Ï€Î·Î¸ KZZ(âˆ’Î·)
=
nâˆ’1

Î·=âˆ’(nâˆ’1)
	
1 âˆ’|Î·|
n

ei2Ï€Î·Î¸ Ë†SZZ(Î·)
=

knâˆ’1 â‹†SZZ

(Î¸),
Î¸ âˆˆ[âˆ’1/2, 1/2),
where in the equality on the second line Ë†SZZ(Î·) denotes the Î·-th Fourier Series
Coeï¬ƒcient of SZZ and we use (17.64); and in the subsequent equality on the third
line kn denotes the degree-n FejÂ´er kernel (Deï¬nition A.1.3) and the convolution is
the periodic one of (A.6).
We have thus established that knâˆ’1 â‹†SZZ is nonnegative. The result now follows
from Theorem A.2.2 which guarantees that
lim
nâ†’âˆ
 1/2
âˆ’1/2
SZZ(Î¸) âˆ’

kn â‹†SZZ

(Î¸)
 dÎ¸ = 0.
The proof of Part (ii) is very similar to the proof of the analogous result for real
processes. As in (13.21), we deï¬ne
K(Î·) â‰œ
 1/2
âˆ’1/2
S(Î¸) ei2Ï€Î·Î¸ dÎ¸,
Î· âˆˆZ,
(17.66)
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.5 Discrete-Time Complex Stochastic Processes
333
and we prove that this function satisï¬es (17.62) and (17.63). To prove (17.62) we
compute
K(âˆ’Î·) =
 1/2
âˆ’1/2
S(Î¸) ei2Ï€(âˆ’Î·)Î¸ dÎ¸
=
 1/2
âˆ’1/2
Sâˆ—(Î¸) eâˆ’i2Ï€Î·Î¸ dÎ¸
=
	 1/2
âˆ’1/2
S(Î¸) ei2Ï€Î·Î¸ dÎ¸

âˆ—
= Kâˆ—(Î·),
Î· âˆˆZ,
where the ï¬rst equality follows from the deï¬nition of K(Â·) (17.66); the second
because S(Â·) is, by assumption, real; the third because conjugating the integrand
is equivalent to conjugating the integral; and the ï¬nal equality again by (17.66).
To prove (17.63) we mimic the derivation of (13.22) with the constants Î±1, . . . , Î±n
now being complex:
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±âˆ—
Î½â€²K(Î½ âˆ’Î½â€²) =
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±âˆ—
Î½â€²
 1/2
âˆ’1/2
S(Î¸) ei2Ï€(Î½âˆ’Î½â€²)Î¸ dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)
	
n

Î½=1
n

Î½â€²=1
Î±Î½ Î±âˆ—
Î½â€² ei2Ï€(Î½âˆ’Î½â€²)Î¸

dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)
	
n

Î½=1
n

Î½â€²=1
Î±Î½ ei2Ï€Î½Î¸Î±âˆ—
Î½â€² eâˆ’i2Ï€Î½â€²Î¸

dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)
	
n

Î½=1
Î±Î½ ei2Ï€Î½Î¸

	
n

Î½â€²=1
Î±Î½â€² ei2Ï€Î½â€²Î¸

âˆ—
dÎ¸
=
 1/2
âˆ’1/2
S(Î¸)

n

Î½=1
Î±Î½ ei2Ï€Î½Î¸

2
dÎ¸
â‰¥0,
(17.67)
where the last inequality follows from the assumption in (ii) that S is nonnegative
on [âˆ’1/2, 1/2). The proof is completed in view of the suï¬ƒciency of the conditions
in (17.62) and (17.63) for a function K: Z â†’C to be the autocovariance function
of some discrete-time WSS CSP.
Proposition 13.6.6 needs very little alteration. We only need to drop the symmetry
property:
Proposition 17.5.8 (PSD when KZZ Is Absolutely Summable). If the autocovari-
ance function KZZ of a discrete-time WSS CSP is absolutely summable, i.e.,
âˆ

Î·=âˆ’âˆ
KZZ(Î·)
 < âˆ,
(17.68)
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

334
Complex Random Variables and Processes
then the function
S(Î¸) =
âˆ

Î·=âˆ’âˆ
KZZ(Î·) eâˆ’i2Ï€Î·Î¸
(17.69)
is continuous, nonnegative, and satisï¬es
 1/2
âˆ’1/2
S(Î¸) ei2Ï€Î·Î¸ dÎ¸ = KZZ(Î·),
Î· âˆˆZ.
(17.70)
The Spectral Distribution Function that we encountered in Section 13.7 has a
natural extension to discrete-time WSS CSPs. The main diï¬€erence is that, unlike
in Theorem 13.7.1, here Î˜ need not have a symmetric distribution:
Theorem 17.5.9.
(i) If

ZÎ½

is a WSS CSP of autocovariance function KZZ, then
KZZ(Î·) = KZZ(0) E

ei2Ï€Î·Î˜
,
Î· âˆˆZ,
(17.71)
for some random variable Î˜ taking values in the interval [âˆ’1/2, 1/2).
In
the nontrivial case where KZZ(0) > 0 the distribution function of Î˜ is fully
speciï¬ed by KZZ.
(ii) If Î˜ is any random variable taking values in [âˆ’1/2, 1/2) and if Î± > 0, then
there exists a proper discrete-time WSS CSP

ZÎ½

whose autocovariance func-
tion KZZ is given by
KZZ(Î·) = Î± E

ei2Ï€Î·Î˜
,
Î· âˆˆZ
(17.72)
and whose variance is consequently given by KZZ(0) = Î±.
Proof. See (Shiryaev, 1996, Chapter VI, Section Â§ 1 Theorem 3), (Doob, 1990,
Chapter X Â§ 3 Theorem 3.2), or (Feller, 1971, Chapter XIX, Section 6, Theorem 3).
Some authors refer to the mapping Î¸ 	â†’Pr[Î˜ â‰¤Î¸] as the spectral distribution func-
tion of

ZÎ½

, but others refer to Î¸ 	â†’KZZ(0) Pr[Î˜ â‰¤Î¸] as the spectral distribution
function. The latter is more common.
17.6
Limits of Proper Complex Random Variables
Understanding limits is essential to understanding inï¬nite sums, and those will
appear when we discuss discrete convolutions and the response of discrete-time
ï¬lters to stochastic inputs (Chapter 31). Here we shall establish that the limit of
proper CRVs must also be proper and that this also holds for complex random
vectors. We begin with scalars.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.6 Limits of Proper Complex Random Variables
335
Proposition 17.6.1. If the sequence Z1, Z2, . . . of proper complex random variables
converges to the CRV Z in the sense that
lim
nâ†’âˆE

|Z âˆ’Zn|2
= 0,
(17.73)
then Z must be proper.
Proof. We ï¬rst prove that
E

|Z|2
< âˆ.
(17.74)
The propositionâ€™s hypotheses guarantee that for any n âˆˆN the CRV Zn is proper
and hence, a fortiori,
E

|Zn|2
< âˆ.
(17.75)
Expressing Z as Zn +(Z âˆ’Zn) we obtain from the Triangle Inequality for complex
random variables (Exercise 17.10) that
E

|Z|2
â‰¤

E[|Zn|2] +

E[|Zn âˆ’Z|2]
2
,
which combines with (17.75) and (17.73) to establish (17.74).
We next prove that Z must be of zero mean. For every n âˆˆN the CRV Zn is
proper and hence
E[Zn] = 0.
(17.76)
This in combination with (17.73) implies that E[Z] must be zero, because
E

|Z âˆ’Zn|2
= |E[Z âˆ’Zn]|2 + Var[Z âˆ’Zn]
â‰¥|E[Z âˆ’Zn]|2
= |E[Z]|2,
where in the last equality we used (17.76).
To conclude the proof that Z is proper, we next show that E[Z2] must be zero.
Since Zn is proper for every n âˆˆN,
E

Z2
n

= 0,
n âˆˆN.
(17.77)
We use this to upper-bound |E[Z2]| as follows:
E

Z2 =
E
%
(Z âˆ’Zn) + Zn
2&
=
E

(Z âˆ’Zn)2
+ 2 E

(Z âˆ’Zn)Zn

+ E

Z2
n

=
E

(Z âˆ’Zn)2
+ 2 E

(Z âˆ’Zn)Zn

â‰¤
E

(Z âˆ’Zn)2 + 2
E[(Z âˆ’Zn)Zn]

â‰¤E

|Z âˆ’Zn|2
+ 2
E[(Z âˆ’Zn)Zn]

â‰¤E

|Z âˆ’Zn|2
+ 2

E[|Z âˆ’Zn|2]

E[|Zn|2],
(17.78)
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

336
Complex Random Variables and Processes
where the third line follows from (17.77), the ï¬fth from the inequality |E[Z]| â‰¤
E[|Z|] (Exercise 17.3), and the sixth line from the Cauchy-Schwarz Inequality for
complex random variables (Exercise 17.9).
We will establish that E[Z2] must be zero by showing that the propositionâ€™s hy-
potheses imply that, as n tends to inï¬nity, the RHS of (17.78) tends to zero. In
view of (17.73), we only need to show that E

|Zn|2
must be bounded in n. This
follows from the Triangle Inequality for complex random variables by expressing
Zn as Z + (Zn âˆ’Z) to obtain
E

|Zn|2
â‰¤

E[|Z|2] +

E[|Zn âˆ’Z|2]
2
,
from which the desired boundedness follows using (17.74) and (17.73).
The above proposition can be easily extended to vectors by recalling that if Î±TZ is
proper for every deterministic vector Î±, then Z must be a proper complex random
vector (Proposition 17.4.2):
Proposition 17.6.2 (Limits of Proper Complex Random Vectors). If the sequence
of proper complex random n-vectors Z1, Z2, . . . converges to the complex random
n-vector Z in the sense that
lim
Î½â†’âˆE
%Z(m)
Î½
âˆ’Z(m)2&
= 0,
m âˆˆ{1, . . . , n}
(17.79)
(where Z(m)
Î½
is the m-th component of ZÎ½ and Z(m) is the m-th component of Z),
then Z must be proper.
Proof. To establish that Z is proper it suï¬ƒces to show that Î±TZ is proper for
every Î± âˆˆCn (Proposition 17.4.2), which is what we now do. Let Î± âˆˆCn be any
deterministic n-vector. Since linear functionals of proper complex random vectors
are proper CRVs (Proposition 17.4.2), the sequence Î±TZ1, Î±TZ2, . . . is a sequence
of proper CRVs. We will now argue that (17.79) implies that it converges to Î±TZ
in the sense that
lim
Î½â†’âˆE
%Î±TZÎ½ âˆ’Î±TZ
2&
= 0,
(17.80)
from which it will then follow using Proposition 17.6.1 that Î±TZ is proper.
To see that (17.79) implies (17.80), one can use the Triangle Inequality for complex
random variables (see Exercise 17.10 but extended to the sum of n CRVs) to obtain
that
0 â‰¤E
%Î±TZÎ½ âˆ’Î±TZ
2&
= E
5 
n

j=1
Î±(j)
Z(j)
Î½
âˆ’Z(j)
26
â‰¤
	
n

j=1
Î±(j) E
%Z(j)
Î½
âˆ’Z(j)2& 1
2 
2
.
This inequality demonstrates that (17.79) indeed implies (17.80), because (17.79)
implies that the RHS of the inequality tends to zero as Î½ tends to inï¬nity, and
hence (17.80) must hold by the Sandwich Theorem.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.7 On the Eigenvalues of Large Toeplitz Matrices
337
17.7
On the Eigenvalues of Large Toeplitz Matrices
Although it will not be used in this book, we cannot resist stating the following
classic result, which is sometimes called â€œSzegËoâ€™s Theorem.â€
Let the function
s: [âˆ’1/2, 1/2] â†’[0, âˆ) be Lebesgue integrable. Deï¬ne
cÎ· =
 1/2
âˆ’1/2
s(Î¸) ei2Ï€Î·Î¸ dÎ¸,
Î· âˆˆZ.
(17.81)
(In some applications, see for example (Gray, 2006), s(Â·) is the PSD of a discrete-
time real or complex stochastic process and cÎ· is the value of the corresponding
autocovariance function at Î·.)
The n Ã— n matrix
â›
âœ
âœ
âœ
â
c0
c1
. . .
cnâˆ’1
câˆ’1
c0
. . .
cnâˆ’2
...
...
...
...
câˆ’n+1
. . .
. . .
c0
â
âŸ
âŸ
âŸ
â 
is positive semideï¬nite and conjugate-symmetric. Consequently, it has n nonneg-
ative eigenvalues (counting multiplicity), which we denote by
Î»(1)
n
â‰¤Î»(2)
n
â‰¤Â· Â· Â· â‰¤Î»(n)
n .
(17.82)
As n increases (with s(Â·) ï¬xed), the number of eigenvalues increases. It turns out
that we can say something quite precise about the distribution of these eigenvalues.
Theorem 17.7.1. Let s: [âˆ’1/2, 1/2] â†’[0, âˆ) be integrable, and let Î»(j)
n
be as in
(17.82). Let g: [0, âˆ) â†’R be a continuous function such that the limit limÎ¾â†’âˆ
g(Î¾)
Î¾
exists and is ï¬nite. Then
lim
nâ†’âˆ
1
n
n

j=1
g

Î»(j)
n

=
 1/2
âˆ’1/2
g

s(Î¸)

dÎ¸.
(17.83)
Proof. For a proof of a more general statement of this theorem see (Simon, 2005,
Chapter 2, Section 7, Theorem 2.7.13). More accessible is (Gray, 2006).
17.8
Exercises
Exercise 17.1 (The Distribution of Re(Z) and |Z|). Let the CRV Z be uniformly dis-
tributed over the unit disc {z âˆˆC : |z| â‰¤1}.
(i) What is the density of its real part Re(Z)?
(ii) What is the density of its magnitude |Z|?
Exercise 17.2 (The Variance under Random Rotation). Let Z be a zero-mean CRV of
ï¬nite variance, and let Î˜ be a real random variable that is independent of it. Show that
the variance of ZeiÎ˜ is equal to the variance of Z.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

338
Complex Random Variables and Processes
Exercise 17.3 (Expectation of Modulus). Show that if Z is a CRV and E[|Z|] < âˆthen
E[Z]
 â‰¤E[|Z|]
by justifying the following steps:
E[Z]
 = eiÎ¸ E[Z] = E
$
eiÎ¸Z
%
= E
$
Re

eiÎ¸Z
%
â‰¤E[|Z|] ,
where Î¸ âˆˆ[âˆ’Ï€, Ï€) is such that the ï¬rst equality holds.
Exercise 17.4 (Constructing a Proper CRV). Let the CRV Z be of ï¬nite variance, and
let Î˜ take on the values 0, Ï€/2, Ï€, and 3Ï€/2 equiprobably and independently of Z. Prove
that the CRV Z eiÎ˜ is proper.
Exercise 17.5 (The Density of Z2). Let Z be a CRV of density fZ(Â·). Express the density
of Z2 in terms of fZ(Â·).
Exercise 17.6 (The Conjugate of a Proper CRV). Must the complex conjugate of a proper
CRV be proper?
Exercise 17.7 (Product of Proper CRVs). Show that the product of two independent
complex random variables is proper whenever one is proper and the other is of ï¬nite
variance.
Exercise 17.8 (Sums of Proper CRVs). Show that the sum of independent proper complex
random variables is proper. Is the assumption of independence essential?
Exercise 17.9 (Cauchy-Schwarz Inequality for Complex Random Variables). Prove that
if W and Z are complex random variables of ï¬nite variance then
E[WZâˆ—]
 â‰¤

E[|W|2]

E[|Z|2]
and
Cov[W, Z]
 â‰¤

Var[W]

Var[Z].
Hint: To prove the ï¬rst inequality, recall Exercise 17.3.
Exercise 17.10 (Triangle Inequality for Complex Random Variables). Show that if W
and Z are complex random variables of ï¬nite variance, then

E[|W + Z|2] â‰¤

E[|W|2] +

E[|Z|2].
Hint: Use the Cauchy-Schwarz Inequality for complex random variables (Exercise 17.9).
Exercise 17.11 (On the Characteristic Function of a CRV). Let Z be a CRV. Express
the characteristic functions of Re(Z) and Im(Z) in terms of the characteristic function
of Z.
Exercise 17.12 (A Complex Random Vector of Independent Components). Let V and
W be independent complex random variables. Express the characteristic function of the
complex random vector (V, W)T in terms of the characteristic functions Î¦V (Â·) and Î¦W (Â·).
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

17.8 Exercises
339
Exercise 17.13 (Transforming Complex Random Vectors). Let Z be a complex n-vector
of PDF fZ(Â·). Let W = g(Z), where g: D â†’R is a one-to-one function from an open
subset D of Cn to R âŠ†Cn. Let the mappings u, v: R2n â†’Rn be deï¬ned for x, y âˆˆRn
as
u: (x, y) â†’Re

g(x + iy)

and
v: (x, y) â†’Im

g(x + iy)

.
Assume that g is diï¬€erentiable in D in the sense that for all j, â„“âˆˆ{1, . . . , n} the partial
derivatives
âˆ‚u(j)(x, y)
âˆ‚x(â„“)
, âˆ‚u(j)(x, y)
âˆ‚y(â„“)
, âˆ‚v(j)(x, y)
âˆ‚x(â„“)
, âˆ‚v(j)(x, y)
âˆ‚y(â„“)
exist and are continuous in D, and that they satisfy
âˆ‚u(j)(x, y)
âˆ‚x(â„“)
= âˆ‚v(j)(x, y)
âˆ‚y(â„“)
and
âˆ‚u(j)(x, y)
âˆ‚y(â„“)
= âˆ’âˆ‚v(j)(x, y)
âˆ‚x(â„“)
,
where a(j) denotes the j-th component of the vector a. Further assume that the determi-
nant of the Jacobian matrix
det gâ€²(z) = det
â›
âœ
âœ
âœ
âœ
â
âˆ‚u(1)(x, y)
âˆ‚x(1)
+ iâˆ‚v(1)(x, y)
âˆ‚x(1)
. . .
âˆ‚u(1)(x, y)
âˆ‚x(n)
+ iâˆ‚v(1)(x, y)
âˆ‚x(n)
...
...
...
âˆ‚u(n)(x, y)
âˆ‚x(1)
+ iâˆ‚v(n)(x, y)
âˆ‚x(1)
. . .
âˆ‚u(n)(x, y)
âˆ‚x(n)
+ iâˆ‚v(n)(x, y)
âˆ‚x(n)
â
âŸ
âŸ
âŸ
âŸ
â 
is at no point in D zero. Show that the density fW(Â·) of W is given by
fW(w) =
fZ(z)
|det gâ€²(z)|2

z=gâˆ’1(w)
Â· I{w âˆˆR}.
Exercise 17.14 (The Cauchy-Schwarz Inequality Revisited). Let

Zâ„“

be a discrete-time
WSS CSP. Show that (17.61) implies
Cov[Zâ„“, Zâ„“â€²]
 â‰¤Var[Z1] ,
â„“, â„“â€² âˆˆZ.
Exercise 17.15 (The Real Part of a WSS CSP Need Not Be WSS). Let X and Y
be independent zero-mean random variables of variances Ïƒ2
X and Ïƒ2
Y . Deï¬ne the CRV
Z = X + iY , and deï¬ne ZÎ½ = iÎ½Z, for every integer Î½.
(i) Show that the CSP

ZÎ½, Î½ âˆˆZ

is WSS.
(ii) Show that if Ïƒ2
X Ì¸= Ïƒ2
Y then the real part of

ZÎ½, Î½ âˆˆZ

is not WSS.
Exercise 17.16 (The Real Part of a Proper WSS CSP). Let

ZÎ½, Î½ âˆˆZ

be a proper,
WSS, discrete-time, complex SP of autocovariance function KZZ. Prove that its real part
forms a real WSS SP and express its autocovariance function in terms of KZZ. Repeat
for the imaginary part of

ZÎ½

.
Exercise 17.17 (The Real and Imaginary Parts of a WSS CSP). Show that if the real part
of a WSS CSP is WSS then so is the imaginary part, and the autocovariance functions of
the real and imaginary parts sum to the real part of the autocovariance function of the
CSP.
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,
www.ebook3000.com

340
Complex Random Variables and Processes
Exercise 17.18 (On the Autocovariance Function of a Discrete-Time CSP). Show that
if KZZ is the autocovariance function of a discrete-time WSS CSP, then for every n âˆˆN,
the matrix
â›
âœ
âœ
âœ
â
KZZ(0)
KZZ(1)
. . .
KZZ(n âˆ’1)
KZZ(âˆ’1)
KZZ(0)
. . .
KZZ(n âˆ’2)
...
...
...
...
KZZ(âˆ’n + 1)
KZZ(âˆ’n + 2)
. . .
KZZ(0)
â
âŸ
âŸ
âŸ
â 
is positive semideï¬nite.
Exercise 17.19 (Reversing the Direction of Time). Let KZZ be the autocovariance func-
tion of some discrete-time WSS CSP

ZÎ½

. For every Î½ âˆˆZ deï¬ne YÎ½ = Zâˆ’Î½. Show that
the time-reversed CSP

YÎ½

is also a WSS CSP, and express its autocovariance function
KYY in terms of KZZ.
Exercise 17.20 (The Sum of Autocovariance Functions). Show that the sum of the
autocovariance functions of two discrete-time WSS complex stochastic processes is the
autocovariance function of some discrete-time WSS CSP.
Exercise 17.21 (The Real Part of an Autocovariance Function). Let KZZ be the au-
tocovariance function of some discrete-time WSS CSP

ZÎ½

.
Show that the mapping
m â†’Re

KZZ(m)

is the autocovariance function of some real SP. Is this also true for the
mapping m â†’Im

KZZ(m)

?
Exercise 17.22 (Rotating a WSS CSP). Let

Zâ„“

be a zero-mean WSS discrete-time CSP,
and let Î± âˆˆC be ï¬xed. Deï¬ne the new CSP

Wâ„“

as Wâ„“= Î±â„“Zâ„“for every â„“âˆˆZ.
(i) Show that if |Î±| = 1 then

Wâ„“

is WSS. Compute its autocovariance function.
(ii) Does your answer change if Î± is not of unit magnitude?
available at 
.019
14:31:29, subject to the Cambridge Core terms of use,

Chapter 18
Energy, Power, and PSD in QAM
18.1
Introduction
The calculations of the power and of the operational power spectral density in
QAM are not just repetitions of the analogous PAM calculations with complex
notation.
They contain two new elements that we shall try to highlight.
The
ï¬rst is the relationship between the power (as opposed to energy) in passband and
baseband, and the second is the fact that the energy and power in transmitting
the complex symbols {Câ„“} are only related to expectations of the form E[Câ„“Câˆ—
â„“â€²];
they are uninï¬‚uenced by those of the form E[Câ„“Câ„“â€²].
The signal

X(t), t âˆˆR

(or X for short) that we consider is given by
X(t) = 2 Re

XBB(t) ei2Ï€fct
,
t âˆˆR,
(18.1)
where
XBB(t) = A

â„“
Câ„“g(t âˆ’â„“Ts),
t âˆˆR.
(18.2)
Here A > 0 is real; the symbols {Câ„“} are complex random variables; the pulse
shape g is an integrable complex function that is bandlimited to W/2 Hz; Ts is
positive; and fc > W/2. The range of the summation will depend on the modes
we discuss.
Our focus in this chapter is on Xâ€™s energy, power, and operational PSD. These
quantities are studied in Sections 18.2â€“18.4, albeit without all the ï¬ne mathemat-
ical details. Those are provided in Sections 18.5 & 18.6, which are recommended
for the more mathematical readers. The deï¬nition of the operational PSD of com-
plex stochastic processes is very similar to the one of real stochastic processes
(Deï¬nition 15.3.1). It is given in Section 18.4 (Deï¬nition 18.4.1).
18.2
The Energy in QAM
As in our treatment in Chapter 14 of PAM, we begin with an analysis of the energy
in transmitting K IID random bits D1, . . . , DK.
We assume that the data bits
are mapped to N complex symbols C1, . . . , CN using a (K, N) binary-to-complex
block-encoder
enc: {0, 1}K â†’CN
(18.3)
341
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

342
Energy, Power, and PSD in QAM
of rate
K
N

bit
complex symbol

.
The transmitted signal is then:
X(t) = 2 Re

XBB(t) ei2Ï€fct
(18.4)
= 2 Re
-
A
N

â„“=1
Câ„“g(t âˆ’â„“Ts) ei2Ï€fct
.
,
t âˆˆR,
(18.5)
where the baseband representation of the transmitted signal is
XBB(t) = A
N

â„“=1
Câ„“g(t âˆ’â„“Ts),
t âˆˆR.
(18.6)
Our interest is in the energy E in X, which is deï¬ned by
E â‰œE
 âˆ
âˆ’âˆ
X2(t) dt

.
(18.7)
Our assumption that the pulse shape g is bandlimited to W/2 Hz implies that
for every realization of the symbols {Câ„“}, the signal XBB(Â·) is also bandlimited
to W/2 Hz. And since we assume that fc > W/2, it follows from Theorem 7.6.10
that the energy in the passband signal X(Â·) is twice the energy in its baseband
representation XBB(Â·), i.e.,
E = 2E
 âˆ
âˆ’âˆ
XBB(t)
2 dt

.
(18.8)
We can thus compute the energy in X(Â·) by computing the energy in XBB(Â·) and
doubling the result. The energy of the baseband signal can be computed in much
the same way that the energy was computed in Section 14.2 for PAM. The only
diï¬€erence is that the baseband signal is now complex:
E
 âˆ
âˆ’âˆ
XBB(t)
2 dt

=
 âˆ
âˆ’âˆ
E
5A
N

â„“=1
Câ„“g(t âˆ’â„“Ts)

26
dt
=
 âˆ
âˆ’âˆ
E
5	
A
N

â„“=1
Câ„“g(t âˆ’â„“Ts)

	
A
N

â„“â€²=1
Câ„“â€² g(t âˆ’â„“â€²Ts)

âˆ—6
dt
= A2
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²]
 âˆ
âˆ’âˆ
g(t âˆ’â„“Ts) gâˆ—(t âˆ’â„“â€²Ts) dt
= A2
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] Rgg

(â„“â€² âˆ’â„“)Ts

,
(18.9)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.2 The Energy in QAM
343
where Rgg is the self-similarity function of the pulse shape g (Deï¬nition 11.2.1),
Rgg(Ï„) =
 âˆ
âˆ’âˆ
g(t + Ï„) gâˆ—(t) dt,
Ï„ âˆˆR.
(18.10)
This expression for the energy in XBB(Â·) is greatly simpliï¬ed if the symbols {Câ„“}
are of zero mean and uncorrelated:
E
 âˆ
âˆ’âˆ
XBB(t)
2 dt

= A2 âˆ¥gâˆ¥2
2
N

â„“=1
E

|Câ„“|2
,

E[Câ„“Câˆ—
â„“â€²] = E

|Câ„“|2
I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , N}

,
(18.11)
or if the time shifts of the pulse shape by integer multiples of Ts are orthonormal
E
 âˆ
âˆ’âˆ
XBB(t)
2 dt

= A2
N

â„“=1
E

|Câ„“|2
,
	 âˆ
âˆ’âˆ
g(t âˆ’â„“Ts)gâˆ—(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , N}

.
(18.12)
Since g is an integrable function that is bandlimited to W/2 Hz, it is also energy-
limited (Note 6.4.12). Consequently, by Proposition 11.2.2 (iv), we can express
the self-similarity function Rgg in (18.9) as the Inverse Fourier Transform of the
mapping f 	â†’|Ë†g(f)|2:
Rgg(Ï„) =
 âˆ
âˆ’âˆ
|Ë†g(f)|2 ei2Ï€fÏ„ df,
Ï„ âˆˆR.
(18.13)
With this representation of Rgg we obtain from (18.9) an equivalent representation
of the energy as
E
 âˆ
âˆ’âˆ
XBB(t)
2 dt

= A2
 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] ei2Ï€f(â„“â€²âˆ’â„“)Ts |Ë†g(f)|2 df.
(18.14)
Using (18.8), (18.9), and (18.14) we obtain:
Theorem 18.2.1 (Energy in QAM). Assume that A â‰¥0, that Ts > 0, that g: R â†’
C is an integrable signal that is bandlimited to W/2 Hz, and that fc > W/2. Then
the energy E in the QAM signal X(Â·) of (18.5) is given by
E = 2A2
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] Rgg

(â„“â€² âˆ’â„“)Ts

(18.15)
= 2A2
 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] ei2Ï€f(â„“â€²âˆ’â„“)Ts |Ë†g(f)|2 df,
(18.16)
whenever all the complex random variables C1, . . . , CN are of ï¬nite variance
E

|Câ„“|2
< âˆ,
â„“= 1, . . . , N.
(18.17)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

344
Energy, Power, and PSD in QAM
In analogy to PAM, we deï¬ne the energy per bit Eb by
Eb â‰œE
K
(18.18)
and the energy per complex symbol Es by
Es â‰œE
N.
(18.19)
Using Theorem 18.2.1, we obtain
Es = 2
NA2
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] Rgg

(â„“â€² âˆ’â„“)Ts

(18.20)
= 2
NA2
 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] ei2Ï€f(â„“â€²âˆ’â„“)Ts |Ë†g(f)|2 df.
(18.21)
Notice that, as promised, only terms of the form E[Câ„“Câˆ—
â„“â€²] inï¬‚uence the energy;
terms of the form E[Câ„“Câ„“â€²] do not appear in this analysis.
18.3
The Power in QAM
In order to discuss the power in QAM we must consider the transmission of an
inï¬nite sequence of complex symbols

Câ„“

. To guarantee convergence, we shall
assume that the pulse shape gâ€”in addition to being an integrable signal that is
bandlimited to W/2 Hzâ€”also satisï¬es the decay condition
|g(t)| â‰¤
Î²
1 + |t/Ts|1+Î± ,
t âˆˆR
(18.22)
for some Î±, Î² > 0.
Also, we shall only consider the transmission of bi-inï¬nite
sequences

Câ„“

that are bounded in the sense that there exists some Î³ > 0 such
that every realization of

Câ„“

satisï¬es
Câ„“
 â‰¤Î³,
â„“âˆˆZ.
(18.23)
As for PAM, we shall treat three diï¬€erent scenarios for the generation of

Câ„“

. In
the ï¬rst, we simply ignore the mechanism by which the sequence

Câ„“

is generated
and assume that it forms a wide-sense stationary complex stochastic process. In
the second, we assume bi-inï¬nite block encoding. And in the third we relax the
statistical assumptions and consider the case where the time shifts of g by integer
multiples of Ts are orthonormal. In all these cases the transmitted waveform is
given by
X(t) = 2 Re

XBB(t) ei2Ï€fct
,
t âˆˆR,
(18.24)
where
XBB(t) = A
âˆ

â„“=âˆ’âˆ
Câ„“g(t âˆ’â„“Ts),
t âˆˆR.
(18.25)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.3 The Power in QAM
345
It is tempting to derive the power in X(Â·) by using the complex version of the
PAM results of Section 14.5 to compute the power in XBB(Â·) and then doubling
the result. This turns out to be a valid approach, but its justiï¬cation requires some
work. The diï¬ƒculty is that the powers are deï¬ned as
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
X2(t) dt
6
and
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
XBB(t)
2 dt
6
,
andâ€”Theorem 7.6.10 notwithstandingâ€”
1
2T E
5 T
âˆ’T
X2(t) dt
6
Ì¸= 2 1
2T E
5 T
âˆ’T
XBB(t)
2 dt
6
.
(18.26)
The reason we cannot claim equality in (18.26) is that t 	â†’X(t) I{|t| â‰¤T} is not
bandlimited around fc, so Theorem 7.6.10, which relates energies in passband and
baseband, is not applicable. Nevertheless, it turns out that the limits as T â†’âˆof
the RHS and the LHS of (18.26) do agree:
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
X2(t) dt
6
= 2 lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
XBB(t)
2 dt
6
.
(18.27)
Thus, the power in a QAM signal is, indeed, twice the power in its baseband
representation. This is stated more precisely in Theorem 18.5.2 and is proved in
Section 18.5. With the aid of (18.27) we can now readily compute the power in
QAM.
18.3.1

Câ„“

Is Zero-Mean and WSS
We next ignore the mechanism by which the symbols

Câ„“

are generated and merely
assume that they form a zero-mean WSS discrete-time CSP of autocovariance
function KCC:
E[Câ„“] = 0,
â„“âˆˆZ,
(18.28a)
E[Câ„“+mCâˆ—
â„“] = KCC(m) ,
m, â„“âˆˆZ.
(18.28b)
The calculation of the RHS of (18.27) is very similar to the analogous computation
in Section 14.5.1 for PAM. The only diï¬€erence is that here XBB(Â·) is complex. As
in Section 14.5.1, we begin by computing the energy in a length-Ts interval:
E
 Ï„+Ts
Ï„
XBB(t)
2 dt

= A2
 Ï„+Ts
Ï„
E
5 
âˆ

â„“=âˆ’âˆ
Câ„“g(t âˆ’â„“Ts)

26
dt
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

346
Energy, Power, and PSD in QAM
= A2
 Ï„+Ts
Ï„
E

âˆ

â„“=âˆ’âˆ
âˆ

â„“â€²=âˆ’âˆ
Câ„“Câˆ—
â„“â€² g(t âˆ’â„“Ts) gâˆ—(t âˆ’â„“â€²Ts)

dt
= A2
 Ï„+Ts
Ï„
âˆ

â„“=âˆ’âˆ
âˆ

â„“â€²=âˆ’âˆ
E[Câ„“Câˆ—
â„“â€²] g(t âˆ’â„“Ts) gâˆ—(t âˆ’â„“â€²Ts) dt
= A2
 Ï„+Ts
Ï„
âˆ

m=âˆ’âˆ
âˆ

â„“â€²=âˆ’âˆ
E[Câ„“â€²+mCâˆ—
â„“â€²] g

t âˆ’(â„“â€² + m)Ts

gâˆ—(t âˆ’â„“â€²Ts) dt
= A2
 Ï„+Ts
Ï„
âˆ

m=âˆ’âˆ
KCC(m)
âˆ

â„“â€²=âˆ’âˆ
g

t âˆ’(â„“â€² + m)Ts

gâˆ—(t âˆ’â„“â€²Ts) dt
= A2
âˆ

m=âˆ’âˆ
KCC(m)
âˆ

â„“â€²=âˆ’âˆ
 Ï„+Tsâˆ’â„“â€²Ts
Ï„âˆ’â„“â€²Ts
g(tâ€² âˆ’mTs) gâˆ—(tâ€²) dtâ€²
= A2
âˆ

m=âˆ’âˆ
KCC(m)
 âˆ
âˆ’âˆ
gâˆ—(tâ€²) g(tâ€² âˆ’mTs) dtâ€²
= A2
âˆ

m=âˆ’âˆ
KCC(m) Râˆ—
gg(mTs),
(18.29)
where we have substituted â„“â€² + m for â„“(fourth equality); we have substituted tâ€²
for t âˆ’â„“â€²Ts (sixth equality); and we have used the cojugate-symmetry of the self-
similarity function (11.5) in the last equality.
As in the analogous analysis for real PAM signals, the RHS of (18.29) does not
depend on the starting point Ï„ of the length-Ts interval [Ï„, Ï„ +Ts], and we can hence
lower-bound the energy of XBB(Â·) in the interval [âˆ’T, +T ] by
72T
Ts
8
E
5 Ï„+Ts
Ï„
XBB(t)
2 dt
6
and upper-bound it by
/2T
Ts
0
E
5 Ï„+Ts
Ï„
XBB(t)
2 dt
6
,
so, by the Sandwich Theorem,
lim
Tâ†’âˆ
1
2T E
5 +T
âˆ’T
XBB(t)
2 dt
6
= 1
Ts
E
5 Ï„+Ts
Ï„
XBB(t)
2 dt
6
.
(18.30)
It thus follows from (18.30) and (18.29) that the power PBB in XBB(Â·) is
PBB = A2
Ts
âˆ

m=âˆ’âˆ
KCC(m) Râˆ—
gg(mTs)
(18.31)
= A2
Ts
 âˆ
âˆ’âˆ
âˆ

m=âˆ’âˆ
KCC(m) eâˆ’i2Ï€fmTs |Ë†g(f)|2 df,
(18.32)
where the second equality follows from (18.13).
Since the power in passband is twice the power in baseband (18.27), we conclude:
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.3 The Power in QAM
347
Theorem 18.3.1. Let the QAM SP

X(t)

be given by (18.24) & (18.25), where
A, Ts, g, W, and fc are as in Theorem 18.2.1. Further assume that g satisï¬es
the decay condition (18.22) and that the WSS centered discrete-time CSP

Câ„“

is bounded in the sense of (18.23).
If

Câ„“

satisï¬es (18.28), then

X(t)

is a
measurable SP,
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
X2(t) dt
6
= 2A2
Ts
âˆ

m=âˆ’âˆ
KCC(m) Râˆ—
gg(mTs),
(18.33)
and
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
X2(t) dt
6
= 2A2
Ts
 âˆ
âˆ’âˆ
âˆ

m=âˆ’âˆ
KCC(m) eâˆ’i2Ï€fmTs |Ë†g(f)|2 df.
(18.34)
Proof. Follows by combining (18.27) (Theorem 18.5.2) with (18.31) and (18.32)
(which can be justiï¬ed by extending Theorem 14.6.4 to the case where the pulse
shape and the symbols are complex).
18.3.2
Bi-Inï¬nite Block-Mode
The second scenario we consider is when

Câ„“

is generated, as in Section 14.5.2, by
applying a binary-to-complex block-encoder enc: {0, 1}K â†’CN to bi-inï¬nite IID
random bits

Dj

. As in Section 14.5.2, we assume that the encoder, when fed IID
random bits, produces symbols of zero mean.
By extending the results of Section 14.5.2 to complex pulse shapes and complex
symbols, we obtain that the power in XBB(Â·) is given by:
PBB =
1
NTs
E
5  âˆ
âˆ’âˆ
A
N

â„“=1
Câ„“g(t âˆ’â„“Ts)

26
(18.35)
= A2
NTs
 âˆ
âˆ’âˆ
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] ei2Ï€f(â„“â€²âˆ’â„“)Ts |Ë†g(f)|2 df.
(18.36)
Using the relationship between power in baseband and passband (18.27) and using
the deï¬nitions of E (18.8) and of Es (18.19), we obtain:
Theorem 18.3.2. Under the assumptions of Theorem 18.3.1, if the symbols

Câ„“

are generated from IID random bits

Dj

in bi-inï¬nite block-mode using the encoder
enc(Â·), where enc(Â·) produces zero-mean symbols when fed IID random bits, then

X(t)

is a measurable SP, and
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
X2(t) dt
6
= Es
Ts
,
(18.37)
where the energy per symbol Es is deï¬ned in (18.19) and is given by (18.20) or
(18.21).
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

348
Energy, Power, and PSD in QAM
Proof. Follows from Theorem 18.5.2 and by noting that Theorem 14.6.5 also ex-
tends to the case where the pulse shape and the symbols are complex.
18.3.3
Time Shifts of Pulse Shape Are Orthonormal
We ï¬nally address the third scenario where the time shifts of the pulse shape by
integer multiples of Ts are orthonormal. This situation is very prevalent in Digital
Communications and allows for signiï¬cant simpliï¬cations. In this setting we denote
the pulse shape by Ï†(Â·) and state the orthonormality as
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†âˆ—(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ.
(18.38)
The transmitted signal

X(t), t âˆˆR

is thus given as in (18.24) but with
XBB(t) = A
âˆ

â„“=âˆ’âˆ
Câ„“Ï†(t âˆ’â„“Ts),
t âˆˆR,
(18.39)
where we assume that the discrete-time CSP

Câ„“

satisï¬es the boundedness con-
dition (18.23) and that the complex pulse shape Ï†(Â·) satisï¬es the orthogonality
condition (18.38) and the decay condition
|Ï†(t)| â‰¤
Î²
1 + |t/Ts|1+Î± ,
t âˆˆR,
(18.40)
for some Î±, Î² > 0.
Computing the power in

XBB(t), t âˆˆR

using Theorem 14.5.2, which easily
extends to the complex case, we obtain from (18.27):
Theorem 18.3.3. Let the SP

X(t), t âˆˆR

be given by
X(t) = 2 Re
	
A
âˆ

â„“=âˆ’âˆ
Câ„“Ï†(t âˆ’â„“Ts) ei2Ï€fct

,
t âˆˆR,
(18.41)
where A â‰¥0; Ts > 0; the pulse shape Ï†: R â†’C is an integrable function that is
bandlimited to W/2 Hz, is Borel measurable, satisï¬es the orthogonality condition
(18.38), and satisï¬es the decay condition (18.40); the carrier frequency fc satisï¬es
fc > W/2 > 0; and where the discrete-time CSP

Câ„“

satisï¬es the boundedness
condition (18.23). Then

X(t), t âˆˆR

is a measurable stochastic process, and
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
X2(t) dt
6
= 2A2
Ts
lim
Lâ†’âˆ
1
2L + 1
L

â„“=âˆ’L
E

|Câ„“|2
,
(18.42)
whenever the limit on the RHS exists.
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.4 The Operational PSD of QAM Signals
349
18.4
The Operational PSD of QAM Signals
We shall compute the operational PSD of the QAM signal

X(t), t âˆˆR

(18.24) by
relating it to the operational PSD of the complex signal

XBB(t), t âˆˆR

(18.25)
and by then computing the operational PSD of the latter using techniques similar
to the ones we employed in Chapter 15 in our study of the operational PSD of real
PAM signals. But ï¬rst we must deï¬ne the operational PSD of complex stochastic
processes. The deï¬nition is very similar to that for real stochastic processes (Deï¬-
nition 15.3.1), but there are two issues to note. The ï¬rst is that we do not require
that the operational PSD be a symmetric function, and the second is that we allow
for ï¬lters of complex impulse response.
Deï¬nition 18.4.1 (Operational PSD of a CSP). We say that a CSP

Z(t), t âˆˆR

is of operational power spectral density SZZ if

Z(t), t âˆˆR

is a measurable
CSP;1 the mapping SZZ : R â†’R is integrable; and for every integrable complex-
valued function h: R â†’C the power of the convolution of

Z(t), t âˆˆR

and h is
given by
Power in Z â‹†h =
 âˆ
âˆ’âˆ
SZZ(f) |Ë†h(f)|2 df.
(18.43)
By Lemma 15.3.5 (i) the PSD is unique:
Note 18.4.2 (The Operational PSD Is Unique). The operational PSD of a CSP
is unique in the sense that if a CSP is of two diï¬€erent operational power spectral
densities, then the two must be indistinguishable.
The relationship between the operational PSD of the real QAM signal

X(t)

(18.24) and of the CSP

XBB(t)

(18.25) turns out to be very simple. Indeed,
subject to the conditions that are made precise in Theorem 18.6.6, if the baseband
CSP

XBB(t)

is of operational PSD SBB, then the real QAM SP

X(t)

is of
operational PSD SXX, where
SXX(f) = SBB

|f| âˆ’fc

,
f âˆˆR.
(18.44)
This result is proved in Section 18.6 and relies heavily on the fact that g is band-
limited to W/2 Hz and that fc > W/2. Here we shall only derive it heuristically
and then see how to apply it.
Recalling the deï¬nition of the operational PSD of a real SP (Deï¬nition 15.3.1), we
note that in order to derive (18.44) we need to show that its RHS is an integrable
symmetric function and that
Power in X â‹†h =
 âˆ
âˆ’âˆ
Ë†h(f)
2 SBB

|f| âˆ’fc

df,
(18.45)
whenever h: R â†’R is integrable. The integrability of f 	â†’SBB(|f| âˆ’fc) follows
directly from the integrability of SBB(Â·). The symmetry is obvious because the RHS
1A complex stochastic processes is said to be measurable if its real and imaginary parts are
measurable real stochastic processes.
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

350
Energy, Power, and PSD in QAM
of (18.44) depends on f only via |f|. Our plan for computing the power in X â‹†h
is to ï¬rst use the results of Section 7.6.7 to express the baseband representation
of X â‹†h in the form XBB â‹†hâ€²
BB, where hâ€²
BB is the baseband representation of the
result of passing h through a unit-gain bandpass ï¬lter of bandwidth W around
the carrier frequency fc. Using the relationship between power in passband and
baseband, this will allow us to express the power in X â‹†h as twice the power in
XBB â‹†hâ€²
BB. Expressing the power in the latter using the operational PSD SBB(Â·)
of XBB will allow us to complete the calculation of the power in X â‹†h.
Before executing this plan, we pause here to heuristically argue that, loosely speak-
ing, the condition that g is bandlimited to W/2 Hz implies that we may assume
that
SBB(f) = 0,
|f| > W
2 .
(18.46)
For a precise statement of this result, see Proposition 18.6.3 in Section 18.6.2. The
intuition behind this statement is that, since g is bandlimited to W/2 Hz, in some
loose sense, all the power of the signal XBB is contained in the band |f| â‰¤W/2.
To heuristically justify (18.46), we shall show that if SBB(Â·) is an operational PSD
for

XBB(t)

, then so is the mapping f 	â†’SBB(f) I{|f| â‰¤W/2}. This follows by
noting that for every h: R â†’C in L1
Power in XBB â‹†h = Power in

t 	â†’A

â„“âˆˆZ
Câ„“g(t âˆ’â„“Ts)

â‹†h
= Power in t 	â†’A

â„“âˆˆZ
Câ„“(g â‹†h)(t âˆ’â„“Ts)
= Power in t 	â†’A

â„“âˆˆZ
Câ„“

(g â‹†LPFW/2) â‹†h

(t âˆ’â„“Ts)
= Power in t 	â†’A

â„“âˆˆZ
Câ„“

g â‹†(h â‹†LPFW/2)

(t âˆ’â„“Ts)
= Power in

t 	â†’A

â„“âˆˆZ
Câ„“g(t âˆ’â„“Ts)

â‹†(h â‹†LPFW/2)
=
 âˆ
âˆ’âˆ
SBB(f)
Ë†h(f) I{|f| â‰¤W/2}
2 df
=
 âˆ
âˆ’âˆ

SBB(f) I{|f| â‰¤W/2}
 Ë†h(f)
2 df,
from which the result follows from the uniqueness (to within indistinguishability)
of the operational PSD (Note 18.4.2).
Here the ï¬rst equality follows from the
deï¬nition of XBB (18.25); the second because convolving a PAM signal of pulse
shape g (in our case complex) with h is tantamount to replacing the pulse shape g
with the new pulse shape g â‹†h (see the derivation of (15.19) in Section 15.4 which
extends verbatim to the complex case); the third because, by assumption, g is
bandlimited to W/2 Hz; the fourth by the associativity of convolution (see Theo-
rem 5.6.1, which, strictly speaking, is not applicable here because LPFW/2 is not
integrable); the ï¬fth because replacing the pulse shape g by g â‹†

h â‹†LPFW/2

is
tantamount to convolving the PAM signal with (h â‹†LPFW/2); the sixth from our
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.4 The Operational PSD of QAM Signals
351
assumption that SBB(Â·) is an operational PSD for XBB (and by ignoring the fact
that h â‹†LPFW/2 need not be integrable); and the seventh by trivial algebra.
Having established (18.46), we are now ready to compute the power in X â‹†h.
Using the results of Section 7.6.7 we obtain that for every integrable h: R â†’R,
the baseband representation of X â‹†h is given by XBB â‹†hâ€²
BB where hâ€²
BB : R â†’C is
the baseband representation of the result of passing h through a unit-gain bandpass
ï¬lter of bandwidth W around the carrier frequency fc:
Ë†hâ€²
BB(f) = Ë†h(f + fc) I{|f| â‰¤W/2},
f âˆˆR.
(18.47)
And since the power in passband is twice the power in baseband, we conclude that
Power in X â‹†h = 2 Power in XBB â‹†hâ€²
BB
= 2
 âˆ
âˆ’âˆ
SBB(f)
Ë†hâ€²
BB(f)
2 df
= 2
 âˆ
âˆ’âˆ
SBB(f)
Ë†h(f + fc)
2 I{|f| â‰¤W/2} df
= 2
 âˆ
âˆ’âˆ
SBB(f)
Ë†h(f + fc)
2 df
= 2
 âˆ
âˆ’âˆ
SBB( Ëœf âˆ’fc)
Ë†h( Ëœf)
2 d Ëœf
=
 âˆ
âˆ’âˆ
SBB( Ëœf âˆ’fc)
Ë†h( Ëœf)
2 d Ëœf +
 âˆ
âˆ’âˆ
SBB( Ëœf âˆ’fc)
Ë†h(âˆ’Ëœf)
2 d Ëœf
=
 âˆ
âˆ’âˆ
SBB( Ëœf âˆ’fc)
Ë†h( Ëœf)
2 d Ëœf +
 âˆ
âˆ’âˆ
SBB(âˆ’f â€² âˆ’fc)
Ë†h(f â€²)
2 df â€²
=
 âˆ
âˆ’âˆ

SBB(f âˆ’fc) + SBB(âˆ’f âˆ’fc)
 Ë†h(f)
2 df
=
 âˆ
âˆ’âˆ
SBB(|f| âˆ’fc)
Ë†h(f)
2 df,
where the ï¬rst equality follows from (18.27) because XBB â‹†hâ€²
BB is the baseband
representation of Xâ‹†h; the second holds because XBB is of operational PSD SBB(Â·);
the third by (18.47); the fourth by (18.46); the ï¬fth by changing the integration
variable to Ëœf â‰œf + fc; the sixth because h is real so its Fourier Transform must
be conjugate-symmetric; the seventh by changing the integration variable in the
second integral to f â€² â‰œâˆ’Ëœf; the eighth by the linearity of integration; and the ï¬nal
equality by (18.46) and the assumption that fc > W/2. This establishes (18.45)
and thus concludes the proof of (18.44).
We next apply (18.44) to calculate the operational PSD of QAM in two scenarios:
when the complex symbols

Câ„“

form a bounded, zero-mean, WSS, CSP and when
they are generated in bi-inï¬nite block-mode.
18.4.1

Câ„“

Zero-Mean WSS and Bounded
We next use (18.44) to derive the operational PSD of QAM when the discrete-time
WSS CSP

Câ„“

is of zero mean and of autocovariance function KCC; see (18.28).
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

352
Energy, Power, and PSD in QAM
To use (18.44) we ï¬rst need to compute the operational PSD of the CSP XBB. This
is straightforward. As in Section 15.4.2, we note that XBB â‹†h has the same form
as (18.25) with the pulse shape g replaced by g â‹†h. Consequently, by substituting
the FT of g â‹†h for the FT of g in (18.32),2 we obtain that
Power in XBB â‹†h = A2
Ts
 âˆ
âˆ’âˆ
âˆ

m=âˆ’âˆ
KCC(m) eâˆ’i2Ï€fmTs |Ë†g(f)|2 |Ë†h(f)|2 df
(18.48)
and the operational PSD of XBB is thus
SBB(f) = A2
Ts
âˆ

m=âˆ’âˆ
KCC(m) eâˆ’i2Ï€fmTs |Ë†g(f)|2,
f âˆˆR.
(18.49)
This is the complex analog of (15.24). From (18.49) and (18.44) we now obtain:
Theorem 18.4.3. Under the assumptions of Theorem 18.3.1, the operational PSD
of the QAM signal

X(t), t âˆˆR

is given by
SXX(f) = A2
Ts
âˆ

m=âˆ’âˆ
KCC(m) eâˆ’i2Ï€(|f|âˆ’fc)mTs Ë†g

|f| âˆ’fc
2,
f âˆˆR.
(18.50)
Proof. The justiï¬cation of (18.44) is in Theorem 18.6.6. A formal derivation of
the operational PSD of

XBB(t), t âˆˆR

can be found in Section 18.6.5. We draw
the readerâ€™s attention to the fact that the proof that we gave for the real case in
Section 15.5 is not directly applicable to the complex case because that proof relied
on Theorem 25.14.1 (Wiener-Khinchin), which we prove in Section 25.14 only for
real WSS stochastic processes.3
Figure 18.1 depicts the relationship between the pulse shape g and the operational
PSD of the QAM signal for the case where KCC(m) = I{m = 0} for every m âˆˆZ.
18.4.2
The Operational PSD of QAM in Bi-Inï¬nite Block-Mode
The operational PSD of QAM in bi-inï¬nite block-mode can also be computed
using (18.44).
All we need is the operational PSD of

XBB(t)

, which can be
computed from (18.36) as follows. As in Section 15.4.2, we note that XBB â‹†h has
the same form as (18.25) with the pulse shape g replaced by g â‹†h. Consequently,
by substituting the FT of g â‹†h for the FT of g in (18.36), we obtain that
Power in XBB â‹†h
=
 âˆ
âˆ’âˆ
	 A2
NTs
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] ei2Ï€f(â„“â€²âˆ’â„“)Ts Ë†g(f)
2

 Ë†h(f)
2 df,
(18.51)
2We are ignoring here the fact that g â‹†h need not satisfy the required decay condition.
3The extension to the complex case is not as trivial as one might think because the real and
imaginary parts of a WSS complex SP need not be WSS.
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.4 The Operational PSD of QAM Signals
353
f
f
f
fc
âˆ’fc
Ë†g(f)
|Ë†g(f)|2
A2
Ts
Ë†g

|f| âˆ’fc
2
Figure 18.1: The relationship between the Fourier Transform of the pulse shape
g(Â·) and the operational PSD of a QAM signal. The complex symbols

Câ„“

are
assumed to be of zero mean, of unit variance, and uncorrelated.
and the operational PSD of XBB is thus
SBB(f) = A2
NTs
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] ei2Ï€f(â„“â€²âˆ’â„“)Ts Ë†g(f)
2,
f âˆˆR.
(18.52)
This is the complex analog of (15.26). (But note that, in our present case, SBB(Â·)
need not be a symmetric function.) From (18.52) and (18.44) we now obtain:
Theorem 18.4.4 (Operational PSD of QAM in Bi-Inï¬nite Block-Mode). Under
the assumptions of Theorem 18.3.2, the operational PSD SXX of the QAM signal

X(t), t âˆˆR

is given for every f âˆˆR by
SXX(f) = A2
NTs
N

â„“=1
N

â„“â€²=1
E[Câ„“Câˆ—
â„“â€²] eâˆ’i2Ï€(|f|âˆ’fc)(â„“âˆ’â„“â€²)Ts Ë†g

|f| âˆ’fc
2.
(18.53)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

354
Energy, Power, and PSD in QAM
Proof. The justiï¬cation of (18.44) is in Theorem 18.6.6, and a formal derivation
of the operational PSD of

XBB(t)

is given in Section 18.6.5.
18.5
A Formal Account of Power in Passband and Baseband
In this section we formulate conditions under which (18.27) holds, i.e., under which
the power in passband is twice the power in baseband. We ï¬rst extend the Triangle
Inequality (4.14) to stochastic processes.
Proposition 18.5.1 (Triangle Inequality for Stochastic Processes). Let

X(t)

and

Y (t)

be (real or complex) measurable stochastic processes, and let a < b be
arbitrary real numbers. Suppose further that
E
 b
a
X(t)
2 dt

, E
 b
a
Y (t)
2 dt

< âˆ.
(18.54)
Then
â›
â

E
 b
a
X(t)
2 dt

âˆ’

E
 b
a
Y (t)
2 dt
â
â 
2
â‰¤E
5 b
a
X(t) + Y (t)
2 dt
6
â‰¤
â›
â

E
 b
a
X(t)
2 dt

+

E
 b
a
Y (t)
2 dt
 â
â 
2
.
(18.55)
This also holds when a is replaced with âˆ’âˆand/or b is replaced with +âˆ.
Proof. Replace all integrals in the proof of (4.14) with expectations of integrals.
We can now state the main result of this section relating the power in passband to
the power in baseband.
Theorem 18.5.2. Let Ts, g, W, and fc be as in Theorem 18.2.1 and, addition-
ally, assume that g satisï¬es the decay condition (18.22) and that the CSP

Câ„“

is
bounded in the sense of (18.23). Then the condition
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T


â„“âˆˆZ
Câ„“g(t âˆ’â„“Ts)

2
dt
6
= P
(18.56)
is equivalent to the condition
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T
	
2 Re
	 
â„“âˆˆZ
Câ„“g(t âˆ’â„“Ts) ei2Ï€fct


2
dt
6
= 2P.
(18.57)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.5 A Formal Account of Power in Passband and Baseband
355
The rest of this section is dedicated to proving this theorem.
To simplify the
notation we begin by showing that it suï¬ƒces to prove the result for the case where
Ts = 1. If Ts > 0 is not necessarily equal to 1, then we deï¬ne for every t âˆˆR,
Ëœg(t) = g(tTs),
ËœW = W Ts,
Ëœfc = fcTs,
and note that g is bandlimited to W/2 Hz if, and only if, Ëœg is bandlimited to ËœW/2
Hz; that

fc â‰¥W/2

â‡â‡’
 Ëœfc â‰¥ËœW/2

;
and that g satisï¬es the decay condition (18.22) if, and only if,
|Ëœg(t)| â‰¤
Î²
1 + |t|1+Î± ,
t âˆˆR
for some positive Î± and Î².
By deï¬ning Ï„ â‰œt/Ts we obtain that
1
2T
 T
âˆ’T


â„“âˆˆZ
Câ„“g(t âˆ’â„“Ts)

2
dt =
1
2(T/Ts)
 T/Ts
âˆ’T/Ts


â„“âˆˆZ
Câ„“Ëœg(Ï„ âˆ’â„“)

2
dÏ„
so the power in the mapping t 	â†’ Câ„“g(t âˆ’â„“Ts) is the same as in the mapping
Ï„ 	â†’ Câ„“Ëœg(Ï„ âˆ’â„“). Similarly,
1
2T
 T
âˆ’T
	
2 Re
 
â„“
Câ„“g(t âˆ’â„“Ts) ei2Ï€fct
2
dt
=
1
2(T/Ts)
 T/Ts
âˆ’T/Ts
	
2 Re
 
â„“
Câ„“Ëœg(Ï„ âˆ’â„“) ei2Ï€ Ëœ
fcÏ„
2
dÏ„
so the power in the mapping t 	â†’2 Re

â„“Câ„“g(tâˆ’â„“Ts) ei2Ï€fct
is the same as in the
mapping Ï„ 	â†’2 Re

â„“Câ„“Ëœg(Ï„ âˆ’â„“) ei2Ï€ Ëœ
fcÏ„
. Thus, if we establish that the inequality
Ëœfc > ËœW/2 implies that the power in the baseband signal Ï„ 	â†’ Câ„“Ëœg(Ï„ âˆ’â„“) is equal
to half the power in Ï„ 	â†’2 Re

â„“Câ„“Ëœg(Ï„ âˆ’â„“) ei2Ï€ Ëœ
fcÏ„
, then it will also follow that
the inequality fc > W/2 implies that the power in t 	â†’ Câ„“g(t âˆ’â„“Ts) is equal to
half the power in t 	â†’2 Re

â„“Câ„“g(t âˆ’â„“Ts) ei2Ï€fct
.
Having established that it suï¬ƒces to prove the theorem for Ts = 1, we assume for
the remainder of this section that Ts = 1, so the decay condition (18.22) can be
rewritten as
|g(t)| â‰¤
Î²
1 + |t|1+Î± ,
t âˆˆR.
(18.58)
As in the proof of Theorem 14.5.2, we shall simplify notation and assume thatâ€”in
calculating power as the limiting ratio of the energy in the interval [âˆ’T, T ] to the
length of the intervalâ€”T is restricted to the positive integers. The justiï¬cation is
identical to the one we gave in proving Theorem 14.5.2; see (14.52).
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

356
Energy, Power, and PSD in QAM
We shall ï¬nd it convenient to introduce an additional subscript â€œwâ€ to indicate
â€œwindowing.â€ Thus, if we deï¬ne XBB(Â·) as
XBB(t) =

â„“âˆˆZ
Câ„“g(t âˆ’â„“),
t âˆˆR,
then its windowed version XBB,w(Â·) is given by
XBB,w(t) =

â„“âˆˆZ
Câ„“g(t âˆ’â„“) I{|t| â‰¤T},
t âˆˆR.
Similarly XPB,w(Â·) is the windowed version of the SP
XPB(t) = 2 Re
	 
â„“âˆˆZ
Câ„“g(t âˆ’â„“) ei2Ï€fct

,
t âˆˆR,
and gâ„“,w is the windowed version of
gâ„“: t 	â†’g(t âˆ’â„“),
â„“âˆˆZ.
(18.59)
We can now express the power in baseband as the limit, as T tends to inï¬nity, of
E
%
âˆ¥XBB,wâˆ¥2
2
&
/(2T), and the power in passband as the limit of E
%
âˆ¥XPB,wâˆ¥2
2
&
/(2T).
Note that, since the function I{Â·} is real-valued,
XPB,w(t) = 2 Re

XBB,w(t) ei2Ï€fct
,
t âˆˆR.
(18.60)
But (18.60) notwithstanding, the energy in XPB,w need not be twice the en-
ergy in XBB,w because the signal XBB,wâ€”unlike its unwindowed version XBBâ€”is
not bandlimited. It is time-limited, and as such cannot be bandlimited (Theo-
rem 6.8.2).
The diï¬ƒculty in proving the theorem is in relating the energy in XPB,w to the
energy in XBB,w and, speciï¬cally, in showing that the diï¬€erence between half the
energy in XPB,w and the energy in XBB,w, when normalized by 2T, tends to zero as
T tends to inï¬nity. Aiding us in this is the following lemma relating the energy in
passband to the energy in baseband for signals that are not necessarily bandlimited.
Lemma 18.5.3. Let z be a complex energy-limited signal that is not necessarily
bandlimited, and consider the real signal x: t 	â†’2 Re

z(t) ei2Ï€fct
, where fc > 0 is
arbitrary. Then,

âˆ¥zâˆ¥2 âˆ’
âˆš
2Ïµ
2
â‰¤1
2 âˆ¥xâˆ¥2
2 â‰¤

âˆ¥zâˆ¥2 +
âˆš
2Ïµ
2
,
(18.61)
where
Ïµ2 =
 âˆ’fc
âˆ’âˆ
Ë†z(f)
2 df.
(18.62)
Proof. Expressing the FT of x in terms of the FT of z, we obtain that for every
f âˆˆR outside a set of frequencies of Lebesgue measure zero,
Ë†x(f) I{f â‰¥0}
= Ë†z(f âˆ’fc) I{f â‰¥0} + Ë†zâˆ—(âˆ’f âˆ’fc) I{f â‰¥0}
= Ë†z(f âˆ’fc) + Ë†zâˆ—(âˆ’f âˆ’fc) I{f â‰¥0} âˆ’Ë†z(f âˆ’fc) I{f < 0}.
(18.63)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.5 A Formal Account of Power in Passband and Baseband
357
We next consider the integral over f of the squared magnitude of the LHS and of
the RHS of (18.63). Since x is real, its FT is conjugate-symmetric so, by Parsevalâ€™s
Theorem, the integral of the squared magnitude of the LHS of (18.63) is 1
2 âˆ¥xâˆ¥2
2.
The integral of the squared magnitude of the ï¬rst term on the RHS of (18.63) is
given by âˆ¥zâˆ¥2
2. Finally, the integral of the squared magnitude of each of the last
two terms on the RHS of (18.63) is Ïµ2 and, since they are orthogonal, the integral
of the squared magnitude of their sum is 2Ïµ2. The result now follows from the
Triangle Inequality (4.14).
Applying Lemma 18.5.3 with the substitution of xBB,w for z and of xPB,w for x
we obtain upon noting that fc > W/2 that, in order to establish the theorem, it
suï¬ƒces to show that the â€œout-of-band energyâ€ term
e2 â‰œ

|f|â‰¥W/2
Ë†xBB,w(f)
2 df
(18.64)
satisï¬es
lim
Tâ†’âˆ
1
T e2 = 0,
(18.65)
with the convergence being uniform. That is, we need to show that e2/T is upper-
bounded by some function of Î±, Î², Î³, and T that converges to zero as T tends to
inï¬nity with Î±, Î², Î³ held ï¬xed. Aiding us in the calculation of the out-of-band
energy is the following lemma.
Lemma 18.5.4. Let x be an energy-limited signal and let W â‰¥0.
(i) If u is any energy-limited signal that is bandlimited to W/2 Hz, then

|f|â‰¥W/2
|Ë†x(f)|2 df â‰¤âˆ¥x âˆ’uâˆ¥2
2 .
(18.66)
(ii) In particular,

|f|â‰¥W/2
|Ë†x(f)|2 df â‰¤âˆ¥xâˆ¥2
2 .
(18.67)
Proof. Part (ii) follows from Parsevalâ€™s Theorem. Part (i) follows by noting that
if u is an energy-limited signal that is bandlimited to W/2 Hz, then the Fourier
Transforms of x and x âˆ’u are indistinguishable for frequencies f that satisfy
|f| â‰¥W/2. Consequently,

|f|â‰¥W/2
|Ë†x(f)|2 df =

|f|â‰¥W/2
|Ë†x(f) âˆ’Ë†u(f)|2 df
â‰¤âˆ¥x âˆ’uâˆ¥2
2 ,
where the inequality follows by applying Part (ii) to the signal x âˆ’u.
To prove (18.65) ï¬x some integer Î½ â‰¥2 and express xBB,w as
xBB,w = s0,w + s1,w + s2,w,
(18.68)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

358
Energy, Power, and PSD in QAM
where
s0,w =

0â‰¤|â„“|â‰¤Tâˆ’Î½
câ„“gâ„“,w
(18.69)
s1,w =

Tâˆ’Î½<|â„“|â‰¤T+Î½
câ„“gâ„“,w
(18.70)
s2,w =

T+Î½<|â„“|<âˆ
câ„“gâ„“,w
(18.71)
are of corresponding out-of-band energies
e2
Îº =

|f|â‰¥W/2
Ë†sÎº,w(f)
2 df,
Îº = 0, 1, 2.
(18.72)
Note that by (18.64), (18.68), (18.72), and the Triangle Inequality
e2 â‰¤

e0 + e1 + e2
2.
(18.73)
Since the integer Î½ â‰¥2 is arbitrary, it follows from (18.73) that, to establish (18.65)
and to thus complete the proof of Theorem 18.5.2, it suï¬ƒces to show that for every
ï¬xed integer Î½ â‰¥2,
lim
Tâ†’âˆ
1
T e2
0 = 0,
(18.74)
lim
Tâ†’âˆ
1
T e2
1 = 0,
(18.75)
and that
lim
Î½â†’âˆ

lim
Tâ†’âˆ
1
T e2
2

= 0.
(18.76)
We thus conclude the theoremâ€™s proof by establishing (18.74), (18.75), and (18.76).
We begin with the easiest, namely (18.75).
To establish (18.75) we recall the
deï¬nition of e1 (18.72) & (18.70) and use the Triangle Inequality to obtain
e1 â‰¤

Tâˆ’Î½<|â„“|â‰¤T+Î½
	
|f|â‰¥W/2
câ„“Ë†gâ„“,w(f)
2 df

1/2
â‰¤Î³

Tâˆ’Î½<|â„“|â‰¤T+Î½
âˆ¥gâ„“,wâˆ¥2
â‰¤4Î³Î½ âˆ¥gâˆ¥2 ,
(18.77)
where the second inequality follows from (18.23) and from Lemma 18.5.4 (ii), and
where the ï¬nal inequality holds because windowing cannot increase energy, so
âˆ¥gâ„“,wâˆ¥2 â‰¤âˆ¥gâ„“âˆ¥2, with âˆ¥gâ„“âˆ¥2 being equal to âˆ¥gâˆ¥2 by (18.59). Inequality (18.77)
establishes (18.75).
Having established (18.75), we next turn to proving (18.74). The proof is quite
similar except that, instead of using Part (ii) of Lemma 18.5.4, we use Part (i) with
the substitutions of gâ„“,w for x and of gâ„“for u to obtain

|f|â‰¥W/2
Ë†gâ„“,w(f)
2 df â‰¤âˆ¥gâ„“,w âˆ’gâ„“âˆ¥2
2 ,
â„“âˆˆZ.
(18.78)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.5 A Formal Account of Power in Passband and Baseband
359
We further upper-bound the RHS of (18.78) using the decay condition (18.58) as
âˆ¥gâ„“,w âˆ’gâ„“âˆ¥2
2 =
 âˆ
âˆ’âˆ
gâ„“(t)
2 I{|t| > T} dt
=
 âˆ’T
âˆ’âˆ
|g(t âˆ’â„“)|2 dt +
 âˆ
T
|g(t âˆ’â„“)|2 dt
=
 âˆ’Tâˆ’â„“
âˆ’âˆ
|g(Ï„)|2 dÏ„ +
 âˆ
Tâˆ’â„“
|g(Ï„)|2 dÏ„
â‰¤
 âˆ’Tâˆ’â„“
âˆ’âˆ
Î²2
|Ï„|2+2Î± dÏ„ +
 âˆ
Tâˆ’â„“
Î²2
|Ï„|2+2Î± dÏ„
â‰¤2
 âˆ
Tâˆ’|â„“|
Î²2
|Ï„|2+2Î± dÏ„
=
2Î²2
1 + 2Î±
1
(T âˆ’|â„“|)1+2Î± ,
|â„“| < T,
to obtain
	
|f|â‰¥W/2
Ë†gâ„“,w(f)
2 df

1/2
â‰¤
4
2Î²2
1 + 2Î±
1
(T âˆ’|â„“|)1/2+Î± ,
|â„“| < T.
(18.79)
Using (18.72), (18.69), (18.79), (18.23), and the Triangle Inequality we thus obtain
e0 â‰¤

|â„“|â‰¤Tâˆ’Î½
|câ„“|
	
|f|â‰¥W/2
Ë†gâ„“,w(f)
2 df

1/2
â‰¤
4
2Î³2Î²2
1 + 2Î±

|â„“|â‰¤Tâˆ’Î½
1
(T âˆ’|â„“|)1/2+Î±
â‰¤2
4
2Î³2Î²2
1 + 2Î±
Tâˆ’Î½

â„“=0
1
(T âˆ’â„“)1/2+Î±
= 2
4
2Î³2Î²2
1 + 2Î±
T

Ëœâ„“=Î½
1
Ëœâ„“1/2+Î±
â‰¤2
4
2Î³2Î²2
1 + 2Î±
 T
Î½âˆ’1
1
Î¾1/2+Î± dÎ¾
=
3
2Î³2Î²2
1+2Î±
4
1âˆ’2Î±

T1/2âˆ’Î± âˆ’(Î½ âˆ’1)1/2âˆ’Î±
if Î± Ì¸= 1/2
2Î³Î²

ln T âˆ’ln(Î½ âˆ’1)

if Î± = 1/2
,
(18.80)
where the inequality in the ï¬rst line follows from (18.72) and from the Triangle
Inequality; the inequality in the second line from (18.79) and the boundedness
condition (18.23); the inequality in the third line by counting the term â„“= 0 twice;
the equality in the fourth line by changing the summation variable to Ëœâ„“â‰œT âˆ’â„“;
the inequality in the ï¬fth line from the monotonicity of the function Î¾ 	â†’Î¾âˆ’1/2âˆ’Î±,
which implies that
Ëœâ„“âˆ’1/2âˆ’Î± â‰¤
 Ëœâ„“
Ëœâ„“âˆ’1
1
Î¾1/2+Î± dÎ¾;
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

360
Energy, Power, and PSD in QAM
and where the ï¬nal equality on the sixth line follows by direct calculation. Inequal-
ity (18.80) combines with our assumption that Î± is positive to prove (18.74).
We now conclude the proof of the theorem by establishing (18.76). To that end, we
begin by using Lemma 18.5.4 (ii) and the fact that s2,w is zero outside the interval
[âˆ’T, T ] to obtain
e2
2 â‰¤
 T
âˆ’T
s2,w(t)
2 dt.
(18.81)
We next upper-bound the RHS of (18.81) using the boundedness of the symbols
(18.23) and the decay condition (18.58):
s2,w(t)
 =


T+Î½<|â„“|<âˆ
câ„“gâ„“,w(t)

â‰¤Î³

T+Î½<|â„“|<âˆ
|g(t âˆ’â„“)| I{|t| â‰¤T}
â‰¤Î³

T+Î½<|â„“|<âˆ
Î²
|t âˆ’â„“|1+Î± I{|t| â‰¤T}
â‰¤Î³

T+Î½<|â„“|<âˆ
Î²
|â„“| âˆ’|t|
1+Î± I{|t| â‰¤T}
â‰¤Î³

T+Î½<|â„“|<âˆ
Î²
(|â„“| âˆ’T)1+Î±
= 2Î³Î²
âˆ

â„“=T+Î½+1
1
(â„“âˆ’T)1+Î±
= 2Î³Î²
âˆ

Ëœâ„“=Î½+1
1
Ëœâ„“1+Î±
â‰¤2Î³Î²
 âˆ
Î½
Î¾âˆ’1âˆ’Î± dÎ¾
= 2Î³Î²
Î± Î½âˆ’Î±,
(18.82)
where the equality in the ï¬rst line follows from the deï¬nition of s2,w (18.71); the
inequality in the second line from the Triangle Inequality for Complex Numbers
(2.12), the boundedness of

Câ„“

(18.23), and from the deï¬nition of gâ„“(18.59); the
inequality in the third line from (18.58); the inequality in the fourth line because
|Î¾ âˆ’Î¶| â‰¥
|Î¾| âˆ’|Î¶|
 whenever Î¾, Î¶ âˆˆR; the inequality in the ï¬fth line because for
|t| > T the LHS is zero and the RHS is positive, and because for |t| â‰¤T we have
that |â„“| âˆ’|t| â‰¥|â„“| âˆ’T throughout the range of summation; the equality in the
sixth line from the symmetry of the summand and from the assumption that T is
an integer; the equality in the seventh line by changing the summation variable to
Ëœâ„“= â„“âˆ’T; the inequality in the eighth line from the monotonicity of the function
Î¾ 	â†’Î¾âˆ’1âˆ’Î±, which implies that
1
Ëœâ„“1+Î± â‰¤
 Ëœâ„“
Ëœâ„“âˆ’1
1
Î¾1+Î± dÎ¾;
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.6 A Formal Account of the PSD in Baseband and Passband
361
and the ï¬nal equality in the ninth line by evaluating the integral.
It follows from (18.82) and (18.81) that
e2
2 â‰¤2T 4Î³2Î²2
Î±2
Î½âˆ’2Î±
(18.83)
and hence that
lim
Tâ†’âˆ
1
T e2
2 â‰¤8Î³2Î²2
Î±2
Î½âˆ’2Î±,
Î½ â‰¥2,
which proves (18.76).
18.6
A Formal Account of the PSD in Baseband and Passband
In this section we justify the derivations of Section 18.4.
18.6.1
On Limits of Convolutions
We begin with a lemma that justiï¬es the swapping of inï¬nite summation and
convolution. As a corollary we establish conditions under which feeding a (real or
complex) PAM signal of pulse shape g to a stable ï¬lter of impulse response h is
tantamount to replacing its pulse shape g with the new pulse shape g â‹†h.
Lemma 18.6.1. Let s1, s2, . . . be a sequence of measurable functions from R to C
satisfying the following two conditions:
1) The sequence is uniformly bounded in the sense that there exists some positive
number Ïƒâˆsuch that
sâ„“(t)
 â‰¤Ïƒâˆ,

t âˆˆR,
â„“= 1, 2, . . .

.
(18.84)
2) The sequence converges to some function s uniformly over compact sets in
the sense that for every ï¬xed Î¾ > 0
lim
â„“â†’âˆsup
|t|â‰¤Î¾
s(t) âˆ’sâ„“(t)
 = 0.
(18.85)
Then for every h âˆˆL1,
lim
â„“â†’âˆ

sâ„“â‹†h

(t) =

s â‹†h

(t),
t âˆˆR.
(18.86)
Proof. Fix some epoch t0 âˆˆR and some h âˆˆL1. We will show that for every
Ïµ > 0 there exists some L0 âˆˆN (depending on Ïµ) such that
(sâ„“â‹†h

(t0) âˆ’(s â‹†h

(t0)
 < Ïµ,
â„“â‰¥L0.
(18.87)
To that end note that our assumption that h is integrable implies that there exists
some Î¾ > 0 such that

|Ï„|â‰¥Î¾
|h(Ï„)| dÏ„ <
Ïµ
3Ïƒâˆ
.
(18.88)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

362
Energy, Power, and PSD in QAM
And when we apply our assumption that the sequence s1, s2, . . . converges to s
uniformly over compact sets to the compact interval [t0 âˆ’Î¾, t0 + Î¾], we obtain that
there exists some L0 (depending on Ïµ, t0, and Î¾) such that
âˆ¥hâˆ¥1
sup
t0âˆ’Î¾â‰¤Ï„â‰¤t0+Î¾
s(Ï„) âˆ’sâ„“(Ï„)
 < Ïµ
3,
â„“â‰¥L0.
(18.89)
We can now derive (18.87) as follows:
(sâ„“â‹†h

(t0) âˆ’(s â‹†h

(t0)

=

 âˆ
âˆ’âˆ
sâ„“(t0 âˆ’Ï„) h(Ï„) dÏ„ âˆ’
 âˆ
âˆ’âˆ
s(t0 âˆ’Ï„) h(Ï„) dÏ„

â‰¤

 Î¾
âˆ’Î¾
sâ„“(t0 âˆ’Ï„) h(Ï„) dÏ„ âˆ’
 Î¾
âˆ’Î¾
s(t0 âˆ’Ï„) h(Ï„) dÏ„

+


|Ï„|>Î¾
sâ„“(t0 âˆ’Ï„) h(Ï„) dÏ„
 +


|Ï„|>Î¾
s(t0 âˆ’Ï„) h(Ï„) dÏ„

â‰¤
 Î¾
âˆ’Î¾
sâ„“(t0 âˆ’Ï„) âˆ’s(t0 âˆ’Ï„)
 |h(Ï„)| dÏ„
+

|Ï„|>Î¾
sâ„“(t0 âˆ’Ï„) h(Ï„)
 dÏ„ +

|Ï„|>Î¾
s(t0 âˆ’Ï„) h(Ï„)
 dÏ„
â‰¤âˆ¥hâˆ¥1

sup
t0âˆ’Î¾â‰¤Ï„â‰¤t0+Î¾
s(Ï„) âˆ’sâ„“(Ï„)


+ 2Ïƒâˆ

|Ï„|>Î¾
|h(Ï„)| dÏ„
< Ïµ,
where the last inequality follows from (18.88), from (18.89), and from the inequality
|s(t0 âˆ’Ï„)| â‰¤Ïƒâˆ(which holds by (18.84) and (18.85)).
Corollary 18.6.2. If the sequence

Câ„“

is bounded in the sense of (18.23) and if the
measurable function g satisï¬es the decay condition (18.22), then for every h âˆˆL1
and every epoch t0 âˆˆR
	
t 	â†’

â„“âˆˆZ
Câ„“g(t âˆ’â„“Ts)

â‹†h

(t0) =

â„“âˆˆZ
Câ„“(g â‹†h)(t0 âˆ’â„“Ts).
(18.90)
Proof. Follows by applying Lemma 18.6.1 to the functions
sL : t 	â†’
L

â„“=âˆ’L
Câ„“g(t âˆ’â„“Ts),
L = 1, 2, . . .
and by using the linearity of the convolution.
18.6.2
On the Support of the Operational PSD of XBB
We next prove that if the pulse shape g is bandlimited to W/2 Hz, then the
operational PSD of XBB is zero at frequencies outside the band [âˆ’W/2, W/2].
That is, we justify (18.46).
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.6 A Formal Account of the PSD in Baseband and Passband
363
Proposition 18.6.3. Assume that A, Ts, g, W, and fc are as in Theorem 18.2.1
and, additionally, that g satisï¬es the decay condition (18.22) and that the CSP

Câ„“

is bounded in the sense of (18.23). If the CSP

XBB(t), t âˆˆR

of (18.25) is
of operational PSD SBB(Â·), then SBB(f) is zero for all |f| > W/2 outside a set of
Lebesgue measure zero, and consequently
f 	â†’SBB(f) I
'
|f| â‰¤W
2
(
is also an operational PSD for

XBB(t), t âˆˆR

.
Proof. We shall show that the propositionâ€™s hypotheses imply that if h âˆˆL1 is
such that Ë†h(f) = 0 at all frequencies f satisfying |f| â‰¤W/2, then the power in
XBB â‹†h is zero, irrespective of the values of Ë†h(f) at other frequencies. That is, we
shall show that

Ë†h(f) = 0,
|f| â‰¤W/2

=â‡’

Power in XBB â‹†h = 0

,
h âˆˆL1.
(18.91)
Since XBB is, by assumption, of operational PSD SBB(Â·), it will then follow from
(18.91) that

Ë†h(f) = 0,
|f| â‰¤W/2

=â‡’
	  âˆ
âˆ’âˆ
SBB(f) |Ë†h(f)|2 df = 0

,
h âˆˆL1. (18.92)
From (18.92) it is just a technicality to show that the nonnegative function SBB(Â·)
must be zero at all frequencies |f| > W/2 outside a set of Lebesgue measure
zero. Indeed, if, in order to reach a contradiction, we assume that SBB(Â·) is not
indistinguishable from the all-zero function in some interval [a, b], where a and b
are such that W/2 < a < b, then picking h as an integrable function such that
Ë†h(f) is zero for |f| â‰¤W/2 and such that Ë†h(f) = 1 for a â‰¤f â‰¤b would yield
a contradiction to (18.92). (An example of such a function h is the IFT of the
shifted-trapezoid mapping
f 	â†’
â§
âª
â¨
âª
â©
1
if a â‰¤f â‰¤b,
0
if f â‰¤W/2 or f â‰¥b + (a âˆ’W/2),
1 âˆ’|fâˆ’(a+b)/2|âˆ’(bâˆ’a)/2
aâˆ’W/2
otherwise,
f âˆˆR,
which is a frequency shifted version of the function we encountered in (7.16) and
(7.18).)
The assumption that SBB(Â·) is not indistinguishable from the all-zero
function in some interval [a, b] where a < b < âˆ’W/2 can be similarly contradicted.
To complete the proof we thus need to justify (18.91).
This follows from two
observations. The ï¬rst is that, by Corollary 18.6.2, for every h âˆˆL1
Power in XBB â‹†h = Power in t 	â†’A

â„“âˆˆZ
Câ„“

g â‹†h

(t âˆ’â„“Ts).
(18.93)
The second is that, because g is an integrable function that is bandlimited to W/2
Hz, it follows from Proposition 6.5.2 that

g â‹†h

(t) =
 W/2
âˆ’W/2
Ë†g(f) Ë†h(f) ei2Ï€ft df,
t âˆˆR
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

364
Energy, Power, and PSD in QAM
and, in particular,

Ë†h(f) = 0,
|f| â‰¤W/2

=â‡’

g â‹†h = 0

,
h âˆˆL1.
(18.94)
Combining (18.93) and (18.94) establishes (18.91).
18.6.3
On the Deï¬nition of the Operational PSD
In order to demonstrate that

Z(t), t âˆˆR

is of operational PSD SZZ, one has
to show that (18.43) holds for every function h: R â†’C in L1 (Deï¬nition 18.4.1).
It turns out that it suï¬ƒces to establish (18.43) only for functions that are in a
subset of L1, provided that the subset is suï¬ƒciently rich. This result will allow
us to consider only functions h of compact support. To make this result precise
we need the following deï¬nition. We say that the set H is a dense subset of L1
if H is a subset of L1 such that for every h âˆˆL1 there exists a sequence h1, h2, . . .
of elements of H such that limÎ½â†’âˆâˆ¥h âˆ’hÎ½âˆ¥1 = 0. An example of a dense subset
of L1 is the subset of functions of compact support, where a function h: R â†’C is
said to be of compact support if there exists some Î” > 0 such that
h(t) = 0,
|t| â‰¥Î”.
(18.95)
Lemma 18.6.4 (On Functions of Compact Support).
(i) The set of integrable functions of compact support is a dense subset of L1.
(ii) If h is of compact support and if g satisï¬es the decay condition (18.22) with
parameters Î±, Î², Ts > 0, then g â‹†h also satisï¬es this decay condition with the
same parameters Î± and Ts but with a possibly diï¬€erent parameter Î²â€².
Proof. We begin with Part (i). Given any integrable function h (not necessarily
of compact support) we deï¬ne the sequence of integrable functions of compact
support h1, h2, . . . by hÎ½ : t 	â†’h(t) I{|t| â‰¤Î½} for every Î½ âˆˆN. It is then just a
technicality to show that âˆ¥h âˆ’hÎ½âˆ¥1 converges to zero. (This can be shown using
the Dominated Convergence Theorem, because at every t âˆˆR we have that hÎ½(t)
converges to h(t) with |hÎ½(t)| â‰¤|h(t)| and h being integrable.)
We next prove Part (ii). Let g satisfy the decay condition (18.22) with the positive
parameters Î±, Î², Ts, and let Î” > 0 be such that (18.95) is satisï¬ed. We shall prove
the lemma by showing that
(g â‹†h)(t)
 â‰¤
Î²â€²
1 + (|t|/Ts)1+Î± ,
t âˆˆR,
(18.96)
where
Î²â€² = Î² âˆ¥hâˆ¥1 21+Î±
1 + (2Î”/Ts)1+Î±
.
(18.97)
To that end we shall ï¬rst show that
(g â‹†h)(t)
 â‰¤Î² âˆ¥hâˆ¥1 ,
t âˆˆR
(18.98)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.6 A Formal Account of the PSD in Baseband and Passband
365
and
(g â‹†h)(t)
 â‰¤Î² âˆ¥hâˆ¥1 21+Î±
1
1 + (|t|/Ts)1+Î± ,
|t| â‰¥2Î”.
(18.99)
We shall then proceed to show that the RHS of (18.96) is larger than the RHS of
(18.98) for |t| â‰¤2Î” and that it is larger than the RHS of (18.99) for |t| > 2Î”.
Both (18.98) and (18.99) follow from the bound
|(g â‹†h)(t)| =

 t+Î”
tâˆ’Î”
g(Ï„) h(t âˆ’Ï„) dÏ„

â‰¤
 t+Î”
tâˆ’Î”
|g(Ï„)| |h(t âˆ’Ï„)| dÏ„
â‰¤
 t+Î”
tâˆ’Î”

sup
tâˆ’Î”â‰¤Ïƒâ‰¤t+Î”
|g(Ïƒ)|

|h(t âˆ’Ï„)| dÏ„
= âˆ¥hâˆ¥1
sup
tâˆ’Î”â‰¤Ïƒâ‰¤t+Î”
|g(Ïƒ)|
as follows. Bound (18.98) simply follows by using (18.22) to upper-bound |g(t)|
by Î². And Bound (18.99) follows by using (18.22) to upper-bound |g(t)| for |t| â‰¥Î”
by Î²/

1 + ((|t| âˆ’Î”)/Ts)1+Î±
, and by then upper-bounding this latter expression
in the range |t| â‰¥2Î” by Î²21+Î±/

1 + (|t|/Ts)1+Î±
because in this range
1 +

(|t| âˆ’Î”)/Ts
1+Î± = 1 +
|t|
Ts
1+Î±|t| âˆ’Î”
|t|
1+Î±
â‰¥1 +
|t|
Ts
1+Î±1
2
1+Î±
â‰¥2âˆ’(1+Î±) + 2âˆ’(1+Î±)|t|
Ts
1+Î±
,
|t| â‰¥2Î”.
Having established (18.98) and (18.99), we now complete the proof by showing
that the RHS of (18.96) upper-bounds the RHS of (18.98) whenever |t| â‰¤2Î”, and
that it upper-bounds the RHS of (18.99) for |t| > 2Î”. That the RHS of (18.96)
upper-bounds the RHS of (18.98) whenever |t| â‰¤2Î” follows because
Î² âˆ¥hâˆ¥1 21+Î±
1 + (2Î”/Ts)1+Î±
1 + (|t|/Ts)1+Î±
â‰¥Î² âˆ¥hâˆ¥1 21+Î± â‰¥Î² âˆ¥hâˆ¥1 ,
|t| â‰¤2Î”.
And that the RHS of (18.96) upper-bounds the RHS of (18.99) whenever |t| > 2Î”
follows because the term 1 + (2Î”/Ts)1+Î± is larger than one.
Proposition 18.6.5. Assume that H is a dense subset of L1 and that the (real or
complex) measurable stochastic process

Z(t), t âˆˆR

is bounded in the sense that
for some Ïƒâˆ
|Z(t)| â‰¤Ïƒâˆ,
t âˆˆR.
(18.100)
If S(Â·) is a nonnegative integrable function such that the relation
Power in Z â‹†h =
 âˆ
âˆ’âˆ
S(f)
Ë†h(f)
2 df
(18.101)
holds for every h âˆˆH, then it holds for all h âˆˆL1.
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

366
Energy, Power, and PSD in QAM
Proof. Let h be an element of L1 (but not necessarily of H) for which we would
like to prove (18.101). Since H is a dense subset of L1, there exists a sequence
h1, h2, . . . of elements of H
hÎ½ âˆˆH,
Î½ = 1, 2, . . .
(18.102)
such that
lim
Î½â†’âˆâˆ¥h âˆ’hÎ½âˆ¥1 = 0.
(18.103)
We shall prove that (18.101) holds for h by justifying the calculation
Power in Z â‹†h = lim
Î½â†’âˆPower in Z â‹†hÎ½
(18.104)
= lim
Î½â†’âˆ
 âˆ
âˆ’âˆ
S(f)
Ë†hÎ½(f)
2 df
(18.105)
=
 âˆ
âˆ’âˆ
S(f)
Ë†h(f)
2 df.
(18.106)
The justiï¬cation of (18.105) is that, by (18.102), each of the functions hÎ½ is in H,
and the propositionâ€™s hypothesis guarantees that (18.101) holds for such functions.
The justiï¬cation of (18.106) is a bit technical. It is based on noting that (18.103)
implies (by Theorem 6.2.11 (i) with the substitution of h âˆ’hÎ½ for x) that
lim
Î½â†’âˆ
Ë†hÎ½(f) = Ë†h(f),
f âˆˆR
(18.107)
and by then using the Dominated Convergence Theorem to justify the swapping of
the limit and integral. Indeed, (by Theorem 6.2.11 (i)) for every Î½ âˆˆN, the function
f 	â†’S(f) Ë†hÎ½(f) is bounded by the function f 	â†’

supÎ½ âˆ¥hÎ½âˆ¥1

S(f), which is
integrable because S(Â·) is integrable (by the propositionâ€™s hypothesis) and because
the integrability of h and (18.103) imply that the supremum is ï¬nite as can be
veriï¬ed using the Triangle Inequality by writing hÎ½ as h âˆ’(h âˆ’hÎ½).
We now complete the proof by justifying (18.104). Since Zâ‹†hÎ½ = Zâ‹†hâˆ’Zâ‹†(hâˆ’hÎ½),
it follows from the Triangle Inequality for Stochastic Processes (Proposition 18.5.1)
that for every T > 0


E
 T
âˆ’T
Z â‹†hÎ½(t)
2 dt

âˆ’

E
 T
âˆ’T
Z â‹†h(t)
2 dt

â‰¤

E
 T
âˆ’T

Z â‹†(h âˆ’hÎ½)

(t)
2 dt

â‰¤
âˆš
2T Ïƒâˆâˆ¥h âˆ’hÎ½âˆ¥1 ,
(18.108)
where the second inequality follows from (18.100) using (5.8c). Upon dividing by
âˆš
2T and taking the limit of T â†’âˆ, it now follows from (18.108) that


Power in Z â‹†hÎ½ âˆ’
âˆš
Power in Z â‹†h
 â‰¤Ïƒâˆâˆ¥h âˆ’hÎ½âˆ¥1 ,
from which (18.104) follows by (18.103).
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.6 A Formal Account of the PSD in Baseband and Passband
367
18.6.4
Relating the Operational PSD in Passband and Baseband
We next make the relationship (18.44) between the operational PSD of X and the
operational PSD of XBB formal.
Theorem 18.6.6. Under the assumptions of Proposition 18.6.3, if the complex
stochastic process

XBB(t), t âˆˆR

of (18.25) is of operational PSD SBB(Â·) in
the sense that SBB(Â·) is an integrable function satisfying that for every complex
hc âˆˆL1,
lim
Tâ†’âˆ
1
2T E
5 T
âˆ’T


XBB â‹†hc

(t)

2
dt
6
=
 âˆ
âˆ’âˆ
SBB(f)
Ë†hc(f)
2 df,
(18.109)
then the QAM real SP

X(t), t âˆˆR

of (18.24) is of operational PSD
SPB(f) â‰œSBB

f âˆ’fc

+ SBB

âˆ’f âˆ’fc

,
f âˆˆR
(18.110)
in the sense that SPB(Â·) is an integrable symmetric function such that for every
real hr âˆˆL1
lim
Tâ†’âˆ
1
2T E
 T
âˆ’T


X â‹†hr

(t)

2
dt

=
 âˆ
âˆ’âˆ
SPB(f)
Ë†hr(f)
2 df.
(18.111)
Proof. The hypothesis that SBB(Â·) is integrable clearly implies that SPB(Â·), as
deï¬ned in (18.110), is integrable and symmetric. It remains to show that if (18.109)
holds for every complex hc âˆˆL1, then (18.111) must hold for every real hr âˆˆL1.
Since the set of integrable functions of compact support is a dense subset of L1
(Lemma 18.6.4 (i)), it follows from Proposition 18.6.5 that it suï¬ƒces to establish
(18.111) for real functions hr that are of compact support.
Let hr be such a
function. The following calculation demonstrates that passing the QAM signal X
through a ï¬lter of impulse response hr is tantamount to replacing its pulse shape g
with the pulse shape consisting of the convolution of g with the complex signal
Ï„ 	â†’eâˆ’i2Ï€fcÏ„hr(Ï„):

X â‹†hr

(t) =
	
Ï„ 	â†’2 Re

XBB(Ï„) ei2Ï€fcÏ„
â‹†hr

(t)
= 2 Re
	
Ï„ 	â†’XBB(Ï„) ei2Ï€fcÏ„
â‹†hr

(t)

= 2 Re
	
ei2Ï€fct
XBB â‹†

Ï„ 	â†’eâˆ’i2Ï€fcÏ„ hr(Ï„)

(t)

= 2 Re
	
ei2Ï€fct A
âˆ

â„“=âˆ’âˆ
Câ„“

g â‹†

Ï„ 	â†’eâˆ’i2Ï€fcÏ„ hr(Ï„)

(t âˆ’â„“Ts)

= 2 Re
	
A
âˆ

â„“=âˆ’âˆ
Câ„“

g â‹†hc

(t âˆ’â„“Ts) ei2Ï€fct

,
(18.112)
where the ï¬rst equality follows from the deï¬nition of X in terms of XBB; the second
because hr is real (see (7.39) on the convolution between a real and a complex
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

368
Energy, Power, and PSD in QAM
signal); the third from Proposition 7.8.1; the fourth from Corollary 18.6.2; and
where the ï¬fth equality follows by deï¬ning the mapping
hc : t 	â†’eâˆ’i2Ï€fct hr(t).
(18.113)
Note that by (18.113)
Ë†hc(f) = Ë†hr(f + fc),
f âˆˆR.
(18.114)
It follows from (18.112) that Xâ‹†hr has the form of a QAM signal with pulse shape
gâ‹†hc. We note that, because g (by hypothesis) satisï¬es the decay condition (18.22)
and because the fact that hr is of compact support implies by (18.113) that hc is
also of compact support, it follows from Lemma 18.6.4 (ii) that the pulse shape
g â‹†hc satisï¬es the decay condition
(g â‹†hc)(t)
 â‰¤
Î²â€²
1 + (|t|/Ts)1+Î± ,
t âˆˆR
(18.115)
for some positive Î²â€². Consequently, we can apply Theorem 18.5.2 to obtain that
the power of X â‹†hr is given by
Power in X â‹†hr = 2 Power in t 	â†’A
âˆ

â„“=âˆ’âˆ
Câ„“(g â‹†hc)(t âˆ’â„“Ts)
= 2 Power in

t 	â†’A
âˆ

â„“=âˆ’âˆ
Câ„“g(t âˆ’â„“Ts)

â‹†hc
= 2 Power in (XBB â‹†hc)
= 2
 âˆ
âˆ’âˆ
SBB(f)
Ë†hc(f)
2 df
= 2
 âˆ
âˆ’âˆ
SBB(f)
Ë†hr(f + fc)
2 df
= 2
 âˆ
âˆ’âˆ
SBB( Ëœf âˆ’fc)
Ë†hr( Ëœf)
2 d Ëœf
=
 âˆ
âˆ’âˆ

SBB( Ëœf âˆ’fc) + SBB(âˆ’Ëœf âˆ’fc)
 Ë†hr( Ëœf)
2 d Ëœf,
(18.116)
where the second equality follows from Corollary 18.6.2; the third by the deï¬nition
of XBB; the fourth because, by hypothesis, XBB is of operational PSD SBB(Â·); the
ï¬fth from (18.114); the sixth by changing the integration variable to Ëœf â‰œf + fc;
and the seventh from the conjugate symmetry of Ë†hr(Â·).
Since hr was an arbitrary integrable real function of compact support, (18.116)
establishes (18.111) for all such functions.
Corollary 18.6.7. Under the assumptions of Theorem 18.6.6, the QAM signal

X(t), t âˆˆR

is of operational PSD
SXX(f) = SBB

|f| âˆ’fc

,
f âˆˆR.
(18.117)
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.6 A Formal Account of the PSD in Baseband and Passband
369
Proof. Follows from the theorem by noting that, by Proposition 18.6.3 and by the
assumption that fc > W/2,
SBB

f âˆ’fc

+ SBB

âˆ’f âˆ’fc

= SBB

|f| âˆ’fc

at all frequencies f outside a set of frequencies of Lebesgue measure zero.
18.6.5
On the Operational PSD in Baseband
In the calculation of the operational PSD of the QAM signal

X(t)

via (18.44)
(which is formally stated as Corollary 18.6.7) we needed the operational PSD of
the CSP

XBB(t)

of (18.25). In this section we justify the calculations of this
operational PSD that lead to Theorems 18.4.3 and 18.4.4. Speciï¬cally, we show:
Proposition 18.6.8 (Operational PSD of a Complex PAM Signal). Let the CSP

XBB(t), t âˆˆR

be given by (18.25), where A â‰¥0, Ts > 0, and where g is a
complex Borel measurable function satisfying the decay condition (18.22) for some
constants Î±, Î² > 0.
(i) If

Câ„“

is a bounded, zero-mean, WSS CSP of autocovariance function KCC,
i.e., if it satisï¬es (18.23) and (18.28), then the CSP

XBB(t), t âˆˆR

is of
operational PSD SBB(Â·) as given in (18.49).
(ii) If

Câ„“

is produced in bi-inï¬nite block-mode from IID random bits using an
encoder enc: {0, 1}K â†’CN that produces zero-mean symbols from IID ran-
dom bits, then

XBB(t), t âˆˆR

is of operational PSD SBB(Â·) as given in
(18.52).
Proof. We have all the ingredients that are needed to justify our derivations of
(18.49) and (18.52). All that remains is to piece them together. Let h be any
complex integrable function of compact support. Then
Power in XBB â‹†h = Power in
	
t 	â†’A

â„“âˆˆZ
Câ„“g(t âˆ’â„“Ts)

â‹†h

= Power in
	
t 	â†’A

â„“âˆˆZ
Câ„“(g â‹†h)(t âˆ’â„“Ts)

,
(18.118)
where the ï¬rst equality follows from the deï¬nition of XBB (18.25), and where the
second equality follows from Corollary 18.6.2. Note that by Lemma 18.6.4 (ii) the
function g â‹†h satisï¬es the decay condition (18.96) for some Î²â€² > 0.
To prove Part (i) we substitute g â‹†h for g in (18.32) to obtain from (18.118) that
Power in XBB â‹†h = A2
Ts
 âˆ
âˆ’âˆ
âˆ

m=âˆ’âˆ
KCC(m) eâˆ’i2Ï€fmTs |Ë†g(f)|2 |Ë†h(f)|2 df,
(18.119)
for every integrable complex h of compact support.
It follows from the fact
that the set of integrable functions of compact support is a dense subset of L1
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

370
Energy, Power, and PSD in QAM
(Lemma 18.6.4 (i)) and from Proposition 18.6.5 that (18.119) must hold for ev-
ery h that is integrable. Recalling the deï¬nition of the operational PSD (Deï¬ni-
tion 18.4.1), it follows that

XBB(t), t âˆˆR

is of operational PSD SBB(Â·) as given
in (18.49).
The proof of Part (ii) is very similar except that we compute the RHS of (18.118)
using (18.36) with the substitution of g â‹†h for the pulse shape.
18.7
Exercises
Exercise 18.1 (The Second Moment of the Square QAM Constellation).
(i) Show that picking X and Y IID uniformly over the set in (10.20) results in X + iY
being uniformly distributed over the set in (16.19).
(ii) Compute the second moment of the square 2Î½ Ã— 2Î½ QAM constellation (16.19).
Exercise 18.2 (Optimal Constellations). Let C denote a QAM constellation, and let z âˆˆC
be arbitrary. Deï¬ne the constellation Câ€² = {c âˆ’z : c âˆˆC}.
(i) Relate the minimum distance of Câ€² to that of C.
(ii) Relate the second moment of Câ€² to that of C.
(iii) How would you choose z to minimize the second moment of Câ€²?
Exercise 18.3 (The Power in Baseband Is Real). Show that the RHS of (18.29) is real.
Which properties of the autocovariance function KCC and of the self-similarity func-
tion Rgg are you exploiting?
Exercise 18.4 (The Power in the In-Phase and Quadrature Components). Consider the
setup of Theorem 18.3.1 with the additional assumptions that the real part (and hence,
by Exercise 17.17, also the imaginary part) of

Câ„“, â„“âˆˆZ

is WSS and that the pulse
shape g is real. Compute the power in each of the signals
t â†’2A
âˆ

â„“=âˆ’âˆ
Re(Câ„“) g(t âˆ’â„“Ts) cos(2Ï€fct),
t â†’âˆ’2A
âˆ

â„“=âˆ’âˆ
Im(Câ„“) g(t âˆ’â„“Ts) sin(2Ï€fct),
and show that these powers add up to the power in X(Â·). Give an intuitive explanation
for this result. Do you expect a similar result for the operational PSD?
Hint: To compute their power, express the signals as QAM signals and use Theorem 18.3.1.
Exercise 18.5 (Ï€/4-QPSK). In QPSK or 4-QAM the data bits are mapped to complex
symbols

Câ„“

which take value in the set {Â±1Â±i} and which are then transmitted using the
signal

X(t)

deï¬ned in (18.24). Consider now Ï€/4-QPSK where, prior to transmission,
the complex symbols

Câ„“

are rotated to form the complex symbols
ËœCâ„“= Î±â„“Câ„“,
â„“âˆˆZ,
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

18.7 Exercises
371
where Î± = eiÏ€/4. The transmitted signal is then
2A Re
	
âˆ

â„“=âˆ’âˆ
ËœCâ„“g(t âˆ’â„“Ts) ei2Ï€fct
,
t âˆˆR.
Compute the power and the operational PSD of the Ï€/4-QPSK signal when

Câ„“

is a zero-
mean WSS CSP of autocovariance function KCC. Compare the power and operational
PSD of Ï€/4-QPSK with those of QPSK. How do they compare when the symbols

Câ„“

are IID?
Hint: See Exercise 17.22.
Exercise 18.6 (The Bandwidth of the QAM Signal). Formulate and prove a result anal-
ogous to Theorem 15.4.1 for QAM.
Exercise 18.7 (Bandwidth and Power in PAM and QAM). Data bits

Dj

are generated
at rate Rb bits per second.
(i) The bits are mapped to real symbols using a (K, N) binary-to-reals block-encoder
of rate K/N bits per real symbol. The symbols are mapped to a PAM signal of
pulse shape Ï† whose time shifts by integer multiples of Ts are orthonormal and
whose excess bandwidth is Î·. Find the bandwidth of the transmitted signal (Deï¬-
nition 15.3.7).
(ii) Repeat for the bandwidth around the carrier frequency fc in QAM when the bits
are mapped to complex symbols using a (K, N) binary-to-complex block-encoder of
rate K/N bits per complex symbol. (As in Part (i), the pulse shape is of excess
bandwidth Î·.)
(iii) Show that if we express the rate Ï of the block-encoder in both cases in bits per
complex symbol, then in the former case Ï = 2K/N; in the latter case Ï = K/N;
and in both cases the bandwidth can be expressed as the same function of Rb, Ï,
and Î·.
(iv) Show that for both PAM and QAM the transmitted power is given by
P = EsRb
Ï
provided that the energy per symbol Es and the rate Ï are computed in both cases
per complex symbol.
Hint: Exercise 18.6 is useful for Part (ii).
Exercise 18.8 (Operational PSD of Diï¬€erential PSK). Let the bi-inï¬nite sequence of IID
random bits

Dj, j âˆˆZ

be mapped to the complex symbols

Câ„“, â„“âˆˆZ

as follows:
Câ„“+1 = Câ„“exp
	
i2Ï€
8 (4D3â„“+ 2D3â„“+1 + D3â„“+2)

,
â„“= 0, 1, 2, . . .
Câ„“= Câ„“+1 exp
	
âˆ’i2Ï€
8 (4D3â„“+ 2D3â„“+1 + D3â„“+2)

,
â„“= . . . , âˆ’2, âˆ’1,
where C0 is independent of

Dj

and uniformly distributed over the set
C =
 
1, ei 2Ï€
8 , e2i 2Ï€
8 , e3i 2Ï€
8 , . . . , e7i 2Ï€
8
!
.
Find the operational PSD of the QAM signal under the assumptions of Section 18.3 on
the pulse shape.
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,
www.ebook3000.com

372
Energy, Power, and PSD in QAM
Exercise 18.9 (PAM/QAM). Let D1, . . . , Dk be IID random bits. These bits are mapped
by a mapping Ï•QAM : {0, 1}k â†’Cn to the complex symbols C1, . . . , Cn, which are then
mapped to the QAM signal
XQAM(t; D1, . . . , Dk) = 2A Re

n

â„“=1
Câ„“Ï†QAM

t âˆ’â„“Ts,QAM

ei2Ï€fct

,
t âˆˆR,
where the time shifts of Ï†QAM by integer multiples of Ts,QAM are orthonormal.
Deï¬ne the real symbols X1, . . . , X2n by
X2â„“âˆ’1 = Re(Câ„“),
X2â„“= Im(Câ„“),
â„“âˆˆ{1, . . . , n}
and the corresponding PAM signal
XPAM(t; D1, . . . , Dk) = A
2n

â„“=1
Xâ„“Ï†PAM

t âˆ’â„“Ts,PAM

,
t âˆˆR,
where Ï†PAM is real and its time shifts by integer multiples of Ts,PAM are orthonormal.
(i) Relate the expected energy in XQAM to that in XPAM.
(ii) Relate the minimum squared distance
min
(d1,...,dk)Ì¸=(dâ€²
1,...,dâ€²
k)
 âˆ
âˆ’âˆ
	
XQAM

t; d1, . . . , dk

âˆ’XQAM

t; dâ€²
1, . . . , dâ€²
k

2
dt
to
min
(d1,...,dk)Ì¸=(dâ€²
1,...,dâ€²
k)
 âˆ
âˆ’âˆ
	
XPAM

t; d1, . . . , dk

âˆ’XPAM

t; dâ€²
1, . . . , dâ€²
k

2
dt.
Exercise 18.10 (A Heuristic Derivation of the Operational PSD). Throwing mathematical
caution to the wind, derive the OPSD of QAM (18.50) using Proposition 16.9.1 on ï¬ltered
QAM and Theorem 18.3.1 on the power in QAM.
Hint: Ignore the fact that Proposition 16.9.1 deals with the transmission of a ï¬nite number
of symbols.
Exercise 18.11 (The Operational PSD Is Nonnegative). Show that if the CSP

Z(t)

is
of operational PSD SZZ, then SZZ(f) must be nonnegative outside a set of frequencies of
Lebesgue measure zero.
Hint: Recall Lemma 15.3.2.
Exercise 18.12 (The Operational PSD of the Real Part of a CSP). Show that the
operational PSD of a CSP does not uniquely specify the operational PSD of its real part.
Hint: How does multiplying a CSP by i aï¬€ect its operational PSD?
available at 
.020
14:32:19, subject to the Cambridge Core terms of use,

Chapter 19
The Univariate Gaussian Distribution
19.1
Introduction
In many communication scenarios the noise is modeled as a Gaussian stochastic
process. This is sometimes justiï¬ed by invoking a Central Limit Theorem, which
demonstrates that many small independent disturbances add up to a stochastic
process that is approximately Gaussian.
Another justiï¬cation is mathematical
convenience: while Gaussian processes may seem daunting at ï¬rst, they are actually
well understood and often amenable to analysis. Finally, particularly in wireline
communications, the Gaussian model is justiï¬ed because it leads to robust results
and to good engineering design.
For other scenarios, e.g., fast-moving wireless
mobile communications, more intricate models are needed.
Rather than starting immediately with the deï¬nition and analysis of Gaussian
stochastic processes, we shall take the more moderate approach and start by ï¬rst
discussing Gaussian random variables.
Building on that, we shall later discuss
Gaussian random vectors in Chapter 23, and only then introduce continuous-time
Gaussian stochastic processes in Chapter 25.
19.2
Standard Gaussian Random Variables
We begin with a special kind of Gaussian: the standard Gaussian.
Deï¬nition 19.2.1 (Standard Gaussian). We say that the random variable W is a
standard Gaussian or that it has a standard Gaussian distribution, if its
density function fW (Â·) is given by
fW (w) =
1
âˆš
2Ï€ eâˆ’w2
2 ,
w âˆˆR.
(19.1)
This density is depicted in Figure 19.1. For this deï¬nition to be meaningful, the
RHS of (19.1) had better be a valid density function, i.e., be nonnegative and
integrate to one. This is indeed the case. In fact, the RHS of (19.1) is positive,
and it integrates to one because, as we next show,
 âˆ
âˆ’âˆ
eâˆ’w2/2 dw =
âˆš
2Ï€.
(19.2)
373
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

374
The Univariate Gaussian Distribution
w
fW (w)
Figure 19.1: The standard Gaussian density function.
This integral can be veriï¬ed by computing its square as follows:
	 âˆ
âˆ’âˆ
eâˆ’w2
2 dw

2
=
 âˆ
âˆ’âˆ
eâˆ’w2
2 dw
 âˆ
âˆ’âˆ
eâˆ’v2
2 dv
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’w2+v2
2
dw dv
=
 âˆ
0
 Ï€
âˆ’Ï€
r eâˆ’r2
2 dÏ• dr
= 2Ï€
 âˆ
0
r eâˆ’r2
2 dr
= 2Ï€

âˆ’eâˆ’r2/2
âˆ
0
= 2Ï€,
where the ï¬rst equality follows by writing a2 as a times a; the second by writing
the product of the integrals as a double integral over R2; the third by changing
from Cartesian to polar coordinates:
w = r cos Ï•,
v = r sin Ï•,
r â‰¥0,
âˆ’Ï€ â‰¤Ï• < Ï€,
dw dv = r dr dÏ•;
the fourth because the integrand does not depend on Ï•; the ï¬fth because the
derivative of âˆ’eâˆ’r2/2 is r eâˆ’r2/2; and where the ï¬nal equality follows by direct
evaluation.
Note that the density of a standard Gaussian random variable is symmetric (19.1).
Consequently, if W is a standard Gaussian, then so is âˆ’W. This symmetry also
establishes that the expectation of a standard Gaussian is zero. The variance of a
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.3 Gaussian Random Variables
375
standard Gaussian can be computed using integration by parts:
 âˆ
âˆ’âˆ
w2
1
âˆš
2Ï€ eâˆ’w2
2 dw =
1
âˆš
2Ï€
 âˆ
âˆ’âˆ
w
	
âˆ’d
dw eâˆ’w2
2

dw
=
1
âˆš
2Ï€
	
âˆ’w eâˆ’w2
2

âˆ
âˆ’âˆ+
 âˆ
âˆ’âˆ
eâˆ’w2
2 dw

=
1
âˆš
2Ï€
 âˆ
âˆ’âˆ
eâˆ’w2
2 dw
= 1,
where the last equality follows from (19.2).
19.3
Gaussian Random Variables
We next deï¬ne a Gaussian (not necessarily standard) random variable as the result
of applying an aï¬ƒne transformation to a standard Gaussian.
Deï¬nition 19.3.1 (Centered Gaussians and Gaussians). We say that a random
variable X is a centered Gaussian or that it has a centered Gaussian distri-
bution if it can be written in the form
X = aW
(19.3)
for some deterministic a âˆˆR and for some standard Gaussian W. We say that
the random variable X is Gaussian or that it has a Gaussian distribution if
X = aW + b
(19.4)
for some deterministic a, b âˆˆR and for some standard Gaussian W.
Note 19.3.2. We do not preclude a from being zero. The case a = 0 leads to X
being deterministically equal to b. We thus include the deterministic random vari-
ables in the family of Gaussian random variables.
Note 19.3.3. The family of Gaussian random variables is closed with respect to
aï¬ƒne transformations: if X is Gaussian and Î±, Î² âˆˆR are deterministic, then
Î±X + Î² is also Gaussian.
Proof. Since X is Gaussian, it can be written as X = aW + b, where W is a
standard Gaussian. Consequently
Î±X + Î² = Î±(aW + b) + Î²
= (Î±a)W + (Î±b + Î²),
which has the form aâ€²W + bâ€² for some deterministic aâ€², bâ€² âˆˆR.
If (19.4) holds, then the random variables on its RHS and LHS must have the same
mean. The mean of a standard Gaussian is zero, so the mean of the RHS of (19.4)
is b. The LHS is of mean E[X], and we thus conclude that in the representation
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

376
The Univariate Gaussian Distribution
(19.4) the deterministic constant b is uniquely determined by the mean of X, and
in fact,
b = E[X] .
Similarly, since the variance of a standard Gaussian is one, the variance of the RHS
of (19.4) is a2. And since the variance of the LHS is Var[X], we conclude that
a2 = Var[X] .
Up to its sign, the deterministic constant a in the representation (19.4) is thus also
unique.
Based on the above, one might mistakenly think that for any given mean Î¼ and
variance Ïƒ2 there are two diï¬€erent Gaussian distributions corresponding to
ÏƒW + Î¼,
and
âˆ’ÏƒW + Î¼,
(19.5)
where W is a standard Gaussian. This, however, is not the case:
Note 19.3.4. There is only one Gaussian distribution of a given mean and variance.
Proof. This can be seen in two diï¬€erent ways. The ï¬rst is to note that the two
representations in (19.5) lead to the same distribution, because the standard Gaus-
sian W has a symmetric distribution, so ÏƒW and âˆ’ÏƒW have the same distribution.
The second is based on computing the density of ÏƒW + Î¼ and showing that it is a
symmetric function of Ïƒ; see (19.6) ahead.
Having established that there is only one Gaussian distribution of a given mean Î¼
and variance Ïƒ2, we denote it by
N

Î¼, Ïƒ2
and set out to study its density. Since the distribution does not depend on the
sign of Ïƒ, it is customary to require that Ïƒ be nonnegative and to refer to it as the
standard deviation. Thus, Ïƒ2 is the variance and Ïƒ is the standard deviation.
If Ïƒ2 = 0, then the Gaussian distribution is deterministic with mean Î¼ and has
no density.1
If Ïƒ2 > 0, then the density can be computed from the density of
the standard Gaussian distribution as follows. If X âˆ¼N

Î¼, Ïƒ2
, then X has the
same distribution as Î¼ + ÏƒW, where W is a standard Gaussian, because both X
and Î¼ + ÏƒW are of mean Î¼ and variance Ïƒ2 (W is zero-mean and unit-variance);
both are Gaussian (Note 19.3.3); and Gaussians of identical means and variances
have identical distributions (Note 19.3.4). The density of X is thus identical to the
density of Î¼ + ÏƒW. The density of the latter can be computed from the density
of W (19.1) to obtain that the density of a N

Î¼, Ïƒ2
Gaussian random variable of
positive variance is
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼)2
2Ïƒ2 ,
x âˆˆR.
(19.6)
This density is depicted in Figure 19.2. To derive the density of Î¼ + ÏƒW from
1Some would say that the density of a deterministic random variable is given by Diracâ€™s Delta,
but we prefer not to use generalized functions in this book.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.3 Gaussian Random Variables
377
Î¼
2Ïƒ
Î¼ + Ïƒ
Î¼ âˆ’Ïƒ
eâˆ’1/2
âˆš
2Ï€Ïƒ2
1
âˆš
2Ï€Ïƒ2
Figure 19.2: The Gaussian density function with mean Î¼ and variance Ïƒ2.
that of W, we have used the fact that if X = g(W), where g(Â·) is a deterministic
continuously diï¬€erentiable function whose derivative never vanishes (in our case
g(w) = Î¼ + Ïƒw) and where W is of density fW (Â·) (in our case (19.1)), then the
density fX(Â·) of X is given by:
fX(x) =

0
if for no Î¾ is x = g(Î¾),
1
|gâ€²(Î¾)|fW

Î¾

if Î¾ satisï¬es x = g(Î¾),
(19.7)
where gâ€²(Î¾) denotes the derivative of g(Â·) at Î¾. (For a more formal multivariate
version of this fact see Theorem 17.3.4.)
Since the family of Gaussian random variables is closed under deterministic aï¬ƒne
transformations (Note 19.3.3), it follows that if X âˆ¼N

Î¼, Ïƒ2
with Ïƒ2 > 0, then
(X âˆ’Î¼)/Ïƒ is also a Gaussian random variable. Since it is of zero mean and of
unit variance, it follows that it must be a standard Gaussian, because there is only
one Gaussian distribution of zero mean and unit variance (Note 19.3.4). We thus
conclude that for Ïƒ2 > 0 and arbitrary Î¼ âˆˆR,
	
X âˆ¼N

Î¼, Ïƒ2
=â‡’
	X âˆ’Î¼
Ïƒ
âˆ¼N(0, 1)

.
(19.8)
Recall that the Cumulative Distribution Function FX(Â·) of a RV X is deï¬ned
for x âˆˆR as
FX(x) = Pr[X â‰¤x],
=
 x
âˆ’âˆ
fX(Î¾) dÎ¾,
where the second equality holds if X has a density function fX(Â·).
If W is a
standard Gaussian, then its CDF is thus given by
FW (w) =
 w
âˆ’âˆ
1
âˆš
2Ï€ eâˆ’Î¾2
2 dÎ¾,
w âˆˆR.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

378
The Univariate Gaussian Distribution
Î±
Q(Î±)
Figure 19.3: Q(Î±) is the area to the right of Î± under the standard Gaussian density
plot. Here it is represented by the shaded area.
There is, alas, no closed-form expression for this integral. To handle such expres-
sions we next introduce the Q-function.
19.4
The Q-Function
The Q-function maps every Î± âˆˆR to the probability that a standard Gaussian
exceeds it:
Deï¬nition 19.4.1 (The Q-Function). The Q-function is deï¬ned by
Q(Î±) â‰œ
1
âˆš
2Ï€
 âˆ
Î±
eâˆ’Î¾2/2 dÎ¾,
Î± âˆˆR.
(19.9)
For a graphical interpretation of this integral see Figure 19.3.
Since the Q-function is a well-tabulated function, we are usually happy when we can
express answers to various questions using this function. The CDF of a standard
Gaussian W can be expressed using the Q-function as follows:
FW (w) = Pr[W â‰¤w]
= 1 âˆ’Pr[W â‰¥w]
= 1 âˆ’Q(w),
w âˆˆR,
(19.10)
where the second equality follows because the standard Gaussian has a density,
so Pr[W = w] = 0. Similarly, with the aid of the Q-function we can express the
probability that a standard Gaussian W lies in some given interval [a, b]:
Pr[a â‰¤W â‰¤b] = Pr[W â‰¥a] âˆ’Pr[W â‰¥b]
= Q(a) âˆ’Q(b),
a â‰¤b.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.4 The Q-Function
379
More generally, if X âˆ¼N

Î¼, Ïƒ2
with Ïƒ > 0, then
Pr[a â‰¤X â‰¤b] = Pr[X â‰¥a] âˆ’Pr[X â‰¥b],
a â‰¤b
= Pr
X âˆ’Î¼
Ïƒ
â‰¥a âˆ’Î¼
Ïƒ

âˆ’Pr
X âˆ’Î¼
Ïƒ
â‰¥b âˆ’Î¼
Ïƒ

,
Ïƒ > 0
= Q
a âˆ’Î¼
Ïƒ

âˆ’Q
b âˆ’Î¼
Ïƒ

,

a â‰¤b, Ïƒ > 0

,
(19.11)
where the last equality follows because (X âˆ’Î¼)/Ïƒ is a standard Gaussian; see
(19.8). Letting b tend to +âˆin (19.11), we obtain the probability of a half ray:
Pr[X â‰¥a] = Q
a âˆ’Î¼
Ïƒ

,
Ïƒ > 0.
(19.12a)
And letting a tend to âˆ’âˆwe obtain
Pr[X â‰¤b] = 1 âˆ’Q
b âˆ’Î¼
Ïƒ

,
Ïƒ > 0.
(19.12b)
The Q-function is usually only tabulated for nonnegative arguments, because the
standard Gaussian density (19.1) is symmetric: if W âˆ¼N(0, 1) then, by the sym-
metry of its density,
Pr[W â‰¥âˆ’Î±] = Pr[W â‰¤Î±]
= 1 âˆ’Pr[W â‰¥Î±],
Î± âˆˆR.
Consequently, as illustrated in Figure 19.4,
Q(Î±) + Q(âˆ’Î±) = 1,
Î± âˆˆR,
(19.13)
and it suï¬ƒces to tabulate the Q-function for nonnegative arguments. Note that,
by (19.13),
Q(0) = 1
2.
(19.14)
An alternative expression for the Q-function as an integral with ï¬xed integration
limits is known as Craigâ€™s formula:
Q(Î±) = 1
Ï€
 Ï€/2
0
eâˆ’
Î±2
2 sin2 Ï• dÏ•,
Î± â‰¥0.
(19.15)
This expression can be derived by computing a two-dimensional integral in two
diï¬€erent ways as follows.
Let X âˆ¼N(0, 1) and Y âˆ¼N(0, 1) be independent.
Consider the probability of the event â€œX â‰¥0 and Y â‰¥Î±â€ where Î± â‰¥0. Since the
two random variables are independent, it follows that
Pr[X â‰¥0 and Y â‰¥Î±] = Pr[X â‰¥0] Pr[Y â‰¥Î±]
= 1
2 Q(Î±),
(19.16)
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

380
The Univariate Gaussian Distribution
Î±
âˆ’Î±
âˆ’Î±
âˆ’Î±
Q(Î±)
Q(Î±)
Q(âˆ’Î±)
Q(âˆ’Î±)
Q(Î±)
Figure 19.4: The identity Q(Î±) + Q(âˆ’Î±) = 1.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.4 The Q-Function
381
y
x
Î±
Ï•
Î±
sin Ï•
area of integration
Figure 19.5: Use of polar coordinates to compute 1
2Q(Î±).
where the second equality follows from (19.14). We now proceed to compute the
LHS of the above in polar coordinates centered at the origin (Figure 19.5):
Pr[X â‰¥0 and Y â‰¥Î±] =
 âˆ
0
 âˆ
Î±
1
2Ï€ eâˆ’x2+y2
2
dy dx
=
 Ï€/2
0
 âˆ
Î±
sin Ï•
1
2Ï€ eâˆ’r2/2 r dr dÏ•,
Î± â‰¥0
= 1
2Ï€
 Ï€/2
0
 âˆ
Î±2
2 sin2 Ï•
eâˆ’t dt dÏ•
= 1
2Ï€
 Ï€/2
0
eâˆ’
Î±2
2 sin2 Ï• dÏ•,
Î± â‰¥0,
(19.17)
where we have performed the change of variable t â‰œr2/2. The integral represen-
tation (19.15) now follows from (19.16) & (19.17).
We next describe various approximations for the Q-function. We are particularly
interested in its value for large arguments.2
Since Q(Î±) is the probability that
a standard Gaussian exceeds Î±, it follows that limÎ±â†’âˆQ(Î±) = 0. Thus, large
arguments to the Q-function correspond to small values of the Q-function. The
following bounds justify the approximation
Q(Î±) â‰ˆ
1
âˆš
2Ï€Î±2 eâˆ’Î±2
2 ,
Î± â‰«1.
(19.18)
Proposition 19.4.2 (Estimates for the Q-Function). The Q-function is bounded
by
1
âˆš
2Ï€Î±2 eâˆ’Î±2/2
	
1 âˆ’1
Î±2

< Q(Î±) <
1
âˆš
2Ï€Î±2 eâˆ’Î±2/2,
Î± > 0
(19.19)
2In Digital Communications this corresponds to scenarios with low probability of error.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

382
The Univariate Gaussian Distribution
and
Q(Î±) â‰¤1
2 eâˆ’Î±2/2,
Î± â‰¥0.
(19.20)
Proof. The proof of (19.19) is omitted (but see Exercise 19.3). Inequality (19.20)
is proved by replacing the integrand in (19.15) with its maximal value, namely, its
value at Ï• = Ï€/2. We shall see an alternative proof in Section 20.10.
19.5
Integrals of Exponentiated Quadratics
The fact that (19.6) is a density and hence integrates to one, i.e.,
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼)2
2Ïƒ2
dx = 1,
(19.21)
can be used to compute seemingly complicated integrals. Here we shall show how
(19.21) can be used to derive the identity
 âˆ
âˆ’âˆ
eâˆ’Î±x2Â±Î²x dx =
4Ï€
Î± e
Î²2
4Î± ,
Î² âˆˆR, Î± > 0.
(19.22)
Note that this identity is meaningless when Î± â‰¤0, because in this case the inte-
grand is not integrable. For example, if Î± < 0, then the integrand tends to inï¬nity
as |x| tends to âˆ. If Î± = 0 and Î² Ì¸= 0, then the integrand tends to inï¬nity either
as x tends to +âˆor as x tends to âˆ’âˆ(depending on the sign of Î²). Finally, if
both Î± and Î² are zero, then the integrand is 1, which is not integrable. Note also
that, by considering the change of variable u â‰œâˆ’x, one can verify that the sign
of Î² on the LHS of this identity is immaterial.
The trick to deriving (19.22) is to complete the exponent to a square and to then
apply (19.21):
 âˆ
âˆ’âˆ
eâˆ’Î±x2+Î²x dx =
 âˆ
âˆ’âˆ
exp
	
âˆ’x2 âˆ’Î²
Î±x
2(1/
âˆš
2Î±)2

dx
=
 âˆ
âˆ’âˆ
exp
-
âˆ’

x âˆ’Î²
2Î±
2
2(1/
âˆš
2Î±)2 + Î²2
4Î±
.
dx
= e
Î²2
4Î±
 âˆ
âˆ’âˆ
exp
-
âˆ’

x âˆ’Î²
2Î±
2
2(1/
âˆš
2Î±)2
.
dx
= e
Î²2
4Î±
3
2Ï€

1/
âˆš
2Î±
2 âˆ
âˆ’âˆ
1
3
2Ï€

1/
âˆš
2Î±
2 exp
-
âˆ’

x âˆ’Î²
2Î±
2
2(1/
âˆš
2Î±)2
.
dx
= e
Î²2
4Î±
3
2Ï€

1/
âˆš
2Î±
2
=
4Ï€
Î± e
Î²2
4Î± ,
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.6 The Moment Generating Function
383
where the ï¬rst equality follows by rewriting the integrand so that the term x2 in
the numerator is of coeï¬ƒcient one and so that the denominator has the form 2Ïƒ2
for Ïƒ which turns out here to be given by Ïƒ â‰œ1/
âˆš
2Î±; the second follows by
completing the square; the third by taking the multiplicative constant out of the
integral; the fourth by multiplying and dividing the integral by
âˆš
2Ï€Ïƒ2 so as to
bring the integrand to the form of the density of a Gaussian; the ï¬fth by (19.21);
and the sixth equality by trivial algebra.
19.6
The Moment Generating Function
As an application of (19.22) we next derive the Moment Generating Function
(MGF) of a Gaussian RV. Recall that the MGF of a RV X is denoted by MX(Â·)
and is given by
MX(Î¸) â‰œE

eÎ¸X
(19.23)
for all Î¸ âˆˆR for which this expectation is ï¬nite. If X has density fX(Â·), then its
MGF can be written as
MX(Î¸) =
 âˆ
âˆ’âˆ
fX(x) eÎ¸x dx,
(19.24)
thus highlighting the connection between the MGF of X and the double-sided
Laplace Transform of its density.
If X âˆ¼N

Î¼, Ïƒ2
where Ïƒ2 > 0, then
MX(Î¸) =
 âˆ
âˆ’âˆ
fX(x) eÎ¸x dx
=
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼)2
2Ïƒ2
eÎ¸x dx
=
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 eâˆ’Î¾2
2Ïƒ2 eÎ¸(Î¾+Î¼) dÎ¾
= eÎ¸Î¼
1
âˆš
2Ï€Ïƒ2
 âˆ
âˆ’âˆ
eâˆ’Î¾2
2Ïƒ2 +Î¸Î¾ dÎ¾
= eÎ¸Î¼
1
âˆš
2Ï€Ïƒ2
4
Ï€
1/(2Ïƒ2) e
Î¸2
4/(2Ïƒ2)
= eÎ¸Î¼+ 1
2 Î¸2Ïƒ2,
Î¸ âˆˆR,
where the ï¬rst equality follows from (19.24); the second from (19.6); the third by
changing the integration variable to Î¾ â‰œx âˆ’Î¼; the fourth by rearranging terms;
the ï¬fth from (19.22) with the substitution of 1/(2Ïƒ2) for Î± and of Î¸ for Î²; and the
ï¬nal by simple algebra. This can be veriï¬ed to hold also when Ïƒ2 = 0. Thus,

X âˆ¼N

Î¼, Ïƒ2
=â‡’

MX(Î¸) = eÎ¸Î¼+ 1
2 Î¸2Ïƒ2,
Î¸ âˆˆR

.
(19.25)
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

384
The Univariate Gaussian Distribution
19.7
The Characteristic Function of Gaussians
19.7.1
The Characteristic Function
Recall that the Characteristic Function Î¦X(Â·) of a random variable X is deï¬ned
for every Ï– âˆˆR by
Î¦X(Ï–) = E

eiÏ–X
=
 âˆ
âˆ’âˆ
fX(x) eiÏ–x dx,
where the second equality holds if X has density fX(Â·). The second equality demon-
strates that the characteristic function is related to the Fourier Transform of the
density function but, by convention, there are no 2Ï€â€™s, and the complex exponential
is not conjugated. If we allow for complex arguments to the MGF (by performing
an analytic continuation), then the characteristic function can be viewed as the
MGF evaluated on the imaginary axis:
Î¦X(Ï–) = MX(iÏ–),
Ï– âˆˆR.
(19.26)
Some of the properties of the characteristic function are summarized next.
Proposition 19.7.1 (On the Characteristic Function). Let X be a random variable
of characteristic function Î¦X(Â·).
(i) If E[Xn] < âˆfor some n âˆˆN, then Î¦X(Â·) is diï¬€erentiable n times and the
Î½-th moment of X is related to the Î½-th derivative of Î¦X(Â·) at zero via the
relation
E[XÎ½] = 1
iÎ½
dÎ½Î¦X(Ï–)
dÏ–Î½

Ï–=0
,
Î½ = 1, . . . , n.
(19.27)
(ii) Two random variables of identical characteristic functions must have the
same distribution.
(iii) If X and Y are independent random variables of characteristic functions
Î¦X(Â·) and Î¦Y (Â·), then the characteristic function Î¦X+Y (Â·) of their sum is
given by the product of their characteristic functions:

X & Y independent

=â‡’

Î¦X+Y (Ï–) = Î¦X(Ï–) Î¦Y (Ï–),
Ï– âˆˆR

.
(19.28)
Proof. For a proof of Part (i) see (Shiryaev, 1996, Chapter II, Â§ 12.3, Theorem 1).
For Part (ii) see (Shiryaev, 1996, Chapter II, Â§ 12.4, Theorem 2). For Part (iii) see
(Shiryaev, 1996, Chapter II, Â§ 12.5, Theorem 4).
Recalling the MGF of a N(Î¼, Ïƒ2) RV (19.25), the relationship between the charac-
teristic function and the MGF (19.26), and the fact that the characteristic function
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.7 The Characteristic Function of Gaussians
385
uniquely speciï¬es the distribution (Proposition 19.7.1 (ii)), we obtain that3

X âˆ¼N

Î¼, Ïƒ2
â‡â‡’

Î¦X(Ï–) = eiÏ–Î¼âˆ’1
2 Ï–2Ïƒ2,
Ï– âˆˆR

.
(19.29)
19.7.2
Moments
Since the standard Gaussian density decays faster than exponentially, it possesses
moments of all orders. Those can be computed from the characteristic function
(19.29) using Proposition 19.7.1 (i) by repeated diï¬€erentiation. Using this approach
we obtain that the moments of a standard Gaussian are
E

W Î½
=

1 Ã— 3 Ã— Â· Â· Â· Ã— (Î½ âˆ’1)
if Î½ is even,
0
if Î½ is odd,
W âˆ¼N(0, 1) .
(19.30)
We mention here in passing that4
E

|W|Î½
=

1 Ã— 3 Ã— Â· Â· Â· Ã— (Î½ âˆ’1)
if Î½ is even,
3
2
Ï€ 2(Î½âˆ’1)/2  Î½âˆ’1
2

!
if Î½ is odd,
W âˆ¼N(0, 1)
(19.31)
(Johnson, Kotz, and Balakrishnan, 1994, Chapter 18, Section 3, Equation (18.13)).
19.7.3
Sums of Independent Gaussians
Using the characteristic function we next show:
Proposition 19.7.2 (The Sum of Two Independent Gaussians Is Gaussian). The
sum of two independent Gaussian random variables is a Gaussian RV.5
Proof. Let X âˆ¼N

Î¼x, Ïƒ2
x

and Y âˆ¼N

Î¼y, Ïƒ2
y

be independent. By (19.29),
Î¦X(Ï–) = eiÏ–Î¼xâˆ’1
2 Ï–2Ïƒ2
x,
Ï– âˆˆR,
Î¦Y (Ï–) = eiÏ–Î¼yâˆ’1
2 Ï–2Ïƒ2
y,
Ï– âˆˆR.
Since the characteristic function of the sum of two independent random variables
is equal to the product of their characteristic functions (19.28),
Î¦X+Y (Ï–) = Î¦X(Ï–) Î¦Y (Ï–)
= eiÏ–Î¼xâˆ’1
2 Ï–2Ïƒ2
x eiÏ–Î¼yâˆ’1
2 Ï–2Ïƒ2
y
= eiÏ–(Î¼x+Î¼y)âˆ’1
2 Ï–2(Ïƒ2
x+Ïƒ2
y),
Ï– âˆˆR.
3It does require a (small) leap of faith to accept that (19.25) also holds for complex Î¸. This can
be justiï¬ed using analytic continuation. But there are also direct ways of deriving (19.29); see, for
example, (Williams, 1991, Chapter E, Exercise E16.4) or (Shiryaev, 1996, Chapter II, Section 12,
Paragraph 2, Example 2). Another approach is to express dÎ¦X(Ï–)/ dÏ– as E

iX eiÏ–X
and to
use integration by parts to verify that the latterâ€™s expectation is equal to âˆ’Ï–Î¦X(Ï–) and to then
solve the diï¬€erential equation dÎ¦X(Ï–)/ dÏ– = âˆ’Ï–Î¦X(Ï–) with the condition Î¦X(0) = 1 to obtain
that ln Î¦X(Ï–) = âˆ’1
2 Ï–2.
4The distribution of |W| is sometimes called half-normal. It is the positive square root of
the central chi-squared distribution with one degree of freedom.
5More generally, as we shall see in Chapter 23, X + Y is Gaussian whenever X and Y are
jointly Gaussian. And independent Gaussians are jointly Gaussian.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

386
The Univariate Gaussian Distribution
By (19.29), this is also the characteristic function of a N

Î¼x + Î¼y, Ïƒ2
x + Ïƒ2
y

RV.
Since the characteristic function of a random variable fully determines its law
(Proposition 19.7.1 (ii)), X + Y must be N

Î¼x + Î¼y, Ïƒ2
x + Ïƒ2
y

.
Using induction one can generalize this proposition to any ï¬nite number of ran-
dom variables: if X1, . . . , Xn are independent Gaussian random variables, then
their sum is Gaussian. Applying this to Î±1X1, . . . , Î±nXn, which are independent
Gaussians whenever X1, . . . , Xn are independent Gaussians, we obtain:
Proposition 19.7.3 (Linear Combinations of Independent Gaussians). If the ran-
dom variables X1, . . . , Xn are independent Gaussians, and if Î±1, . . . , Î±n âˆˆR are
deterministic, then the RV Y = n
â„“=1 Î±â„“Xâ„“is Gaussian with mean and variance
E[Y ] =
n

â„“=1
Î±â„“E[Xâ„“] ,
Var[Y ] =
n

â„“=1
Î±2
â„“Var[Xâ„“] .
19.8
Central and Noncentral Chi-Square Random Variables
We summarize here some of the deï¬nitions and main properties of the central and
noncentral Ï‡2 distributions and of some related distributions. We shall only use
three results from this section: that the sum of the squares of two independent
N(0, 1) random variables has a mean-2 exponential distribution; that the distri-
bution of the sum of the squares of n independent Gaussian random variables of
unit-variance and possibly diï¬€erent means depends only on n and on the sum of
the squared means; and that the MGF of this latter sum has a simple explicit form.
These results can be derived quite easily from the MGF of a squared Gaussian RV,
an MGF which, using (19.22), can be shown to be given by

X âˆ¼N

Î¼, Ïƒ2
=â‡’

MX2(Î¸) =
1
âˆš
1 âˆ’2Ïƒ2Î¸
eâˆ’Î¼2
2Ïƒ2 e
Î¼2
2Ïƒ2(1âˆ’2Ïƒ2Î¸) , Î¸ <
1
2Ïƒ2

.
(19.32)
With a small leap of faith we can assume that (19.32) also holds for complex
arguments whose real part is smaller than 1/(2Ïƒ2) so that upon substituting iÏ–
for Î¸ we can obtain the characteristic function

X âˆ¼N

Î¼, Ïƒ2
=â‡’

Î¦X2(Ï–) =
1
âˆš
1 âˆ’i2Ïƒ2Ï–
eâˆ’Î¼2
2Ïƒ2 e
Î¼2
2Ïƒ2(1âˆ’i2Ïƒ2Ï–) , Ï– âˆˆR

.
(19.33)
19.8.1
The Central Ï‡2 Distribution and Related Distributions
The central Ï‡2 distribution with n degrees of freedom is denoted by Ï‡2
n
and is deï¬ned as the distribution of the sum of the squares of n IID zero-mean
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.8 Central and Noncentral Chi-Square Random Variables
387
unit-variance Gaussian random variables:
	
X1, . . . , Xn âˆ¼IID N(0, 1)

=â‡’
	
n

j=1
X2
j âˆ¼Ï‡2
n

.
(19.34)
Using the fact that the MGF of the sum of independent random variables is the
product of their MGFs and using (19.32) with Î¼ = 0 and Ïƒ2 = 1, we obtain that
the MGF of the central Ï‡2 distribution with n degrees of freedom is given by
E
%
eÎ¸Ï‡2
n
&
=
1
(1 âˆ’2Î¸)n/2 ,
Î¸ < 1
2.
(19.35)
Similarly, by (19.33) and the fact that the characteristic function of the sum of
independent random variables is the product of their characteristic functions, (or
by substituting iÏ– for Î¸ in (19.35)), we obtain that the characteristic function of
the central Ï‡2 distribution with n degrees of freedom is given by
E
%
eiÏ–Ï‡2
n
&
=
1
(1 âˆ’2iÏ–)n/2 ,
Ï– âˆˆR.
(19.36)
Notice that for n = 2 this characteristic function is given by Ï– 	â†’1/(1 âˆ’i2Ï–),
which is the characteristic function of the mean-2 exponential density
1
2 eâˆ’x/2 I{x > 0},
x âˆˆR.
Since two random variables of identical characteristic functions must be of equal
law (Proposition 19.7.1 (ii)), we conclude:
Note 19.8.1. The central Ï‡2 distribution with two degrees of freedom Ï‡2
2 is the
mean-2 exponential distribution.
From (19.36) and the relationship between the moments of a distribution and the
derivatives at zero of its characteristic function (19.27), one can verify that the
Î½-th moment of a Ï‡2
n RV is given by
E

Ï‡2
n
Î½
= n Ã— (n + 2) Ã— Â· Â· Â· Ã—

n + 2(Î½ âˆ’1)

,
Î½ âˆˆN,
(19.37)
so the mean is n; the second moment is n(n + 2); and the variance is 2n.
Since the sum of the squares of random variables must be nonnegative, the density
of the Ï‡2
n distribution is zero on the negative numbers. It is given by
fÏ‡2
n(x) =
1
2n/2 Î“(n/2) eâˆ’x/2 x(n/2)âˆ’1 I{x > 0},
(19.38)
where Î“(Â·) is the Gamma function, which is deï¬ned by
Î“(Î¾) â‰œ
 âˆ
0
eâˆ’t tÎ¾âˆ’1 dt,
Î¾ > 0.
(19.39)
If the number of degrees of freedom is even, then the density has a particularly
simple form:
fÏ‡2
2k(x) =
1
2k(k âˆ’1)! eâˆ’x/2 xkâˆ’1 I{x > 0},
k âˆˆN,
(19.40)
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

388
The Univariate Gaussian Distribution
thus demonstrating again that when the number of degrees of freedom is two, the
central Ï‡2 distribution is the mean-2 exponential distribution (Note 19.8.1).
A related distribution is the generalized Rayleigh distribution, which is the
distribution of the square root of a random variable having a Ï‡2
n distribution. The
density of the generalized Rayleigh distribution is given by
fâˆš
Ï‡2
n(x) =
2
2n/2 Î“(n/2)xnâˆ’1 eâˆ’x2/2 I{x > 0},
(19.41)
and its moments are given by
E
%
Ï‡2n
Î½&
= 2Î½/2 Î“

(n + Î½)/2

Î“(n/2)
,
Î½ âˆˆN.
(19.42)
The Rayleigh distribution is the distribution of the square root of a Ï‡2
2 random
variable, i.e., the distribution of the square root of a mean-2 exponential random
variable. The density of the Rayleigh distribution is obtained by setting n = 2 in
(19.41):
fâˆš
Ï‡2
2(x) = x eâˆ’x2/2 I{x > 0}.
(19.43)
The CDF of the Rayleigh distribution (which is only nonzero for positive argu-
ments) can be computed from its density (19.43) by substituting u for Î¾2/2:
Fâˆš
Ï‡2
2(x) =
 x
0
Î¾ eâˆ’Î¾2/2 dÎ¾
= 1 âˆ’exp

âˆ’x2
2

,
x â‰¥0.
(19.44)
19.8.2
The Noncentral Ï‡2 Distribution and Related Distributions
Using (19.32) and the fact that the MGF of the sum of independent random vari-
ables is the product of their MGFs, we obtain that if X1, . . . , Xn are independent
with Xj âˆ¼N

Î¼j, Ïƒ2
, then the MGF of 
j X2
j is given by
	
1
âˆš
1 âˆ’2Ïƒ2 Î¸

n
eâˆ’
n
j=1 Î¼2
j
2Ïƒ2
e
n
j=1 Î¼2
j
2Ïƒ2(1âˆ’2Ïƒ2Î¸) ,
Î¸ <
1
2Ïƒ2 .
(19.45)
Noting that this MGF depends on the individual means Î¼1, . . . , Î¼n only via the
sum of their squares  Î¼2
j, we obtain:
Note 19.8.2. The distribution of the sum of the squares of independent equivari-
ance Gaussians is determined by their number, their common variance, and by the
sum of the squares of their means.
The distribution of the sum of the squares of n independent unit-variance Gaussians
whose squared means sum to Î» is called the noncentral Ï‡2 distribution with
n degrees of freedom and noncentrality parameter Î». This distribution is
denoted by Ï‡2
n,Î». Substituting 1 for Ïƒ2 and Î» for n
j=1 Î¼2
j in (19.45), we obtain
that the MGF of the Ï‡2
n,Î» distribution is
E
%
eÎ¸Ï‡2
n,Î»
&
=
	
1
âˆš
1 âˆ’2Î¸

n
eâˆ’Î»
2 e
Î»
2(1âˆ’2Î¸) ,
Î¸ < 1
2.
(19.46)
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.8 Central and Noncentral Chi-Square Random Variables
389
A special case of this distribution is the central Ï‡2 distribution, which corresponds
to the case where the noncentrality parameter Î» is zero.
Explicit expressions for the density of the noncentral Ï‡2 distribution can be found
in (Johnson, Kotz, and Balakrishnan, 1995, Chapter 29, Equation (29.4)) and in
(Simon, 2002, Chapter 2). An interesting representation of this density in terms
of the density fÏ‡2
Î½,0 of the central Ï‡2 distribution is:
fÏ‡2
n,Î»(x) =
âˆ

j=0
	( 1
2Î»)j
j!
eâˆ’Î»/2

fÏ‡2
n+2j,0(x),
x âˆˆR.
(19.47)
It demonstrates that a Ï‡2
n,Î» random variable X can be generated by picking a
random integer j according to the Poisson distribution of parameter Î»/2 and by
then generating a central Ï‡2 random variable of n + 2j degrees of freedom. That
is, to generate a Ï‡2
n,Î» random variable X, generate some random variable J taking
values in the nonnegative integers according to the law
Pr[J = j] = eâˆ’Î»/2 (Î»/2)j
j!
,
j = 0, 1, . . .
(19.48)
and then generate X according the central Ï‡2 distribution with n + 2j degrees of
freedom, where j is the outcome of J.
The density of the Ï‡2
2,Î» distribution is
fÏ‡2
2,Î»(x) = 1
2 eâˆ’(Î»+x)/2 I0
âˆš
Î»x

I{x > 0},
(19.49)
where I0(Â·) is the modiï¬ed zeroth-order Bessel function, which is deï¬ned in (27.34)
ahead.
The generalized Rice distribution corresponds to the distribution of the square
root of a noncentral Ï‡2 distribution with n degrees of freedom and noncentrality pa-
rameter Î». The case n = 2 is called the Rice distribution. The Rice distribution
is thus the distribution of the square root of a random variable having the noncen-
tral Ï‡2 distribution with 2 degrees of freedom and noncentrality parameter Î». The
density of the Rice distribution is
fâˆš
Ï‡2
2,Î»(x) = x eâˆ’(x2+Î»)/2 I0

x
âˆš
Î»

I{x > 0}.
(19.50)
The cumulative distribution function of the Rice distribution is, of course, zero
for negative arguments. For nonnegative arguments it can be expressed using the
ï¬rst-order Marcum Q-function, as
Fâˆš
Ï‡2
2,Î»(x) = 1 âˆ’Q1
âˆš
Î», x

,
x â‰¥0,
(19.51a)
where the ï¬rst-order Marcum Q-function is deï¬ned as
Q1

Î¼, Î±

=
 âˆ
Î±
Î¾ eâˆ’(Î¾2+Î¼2)/2 I0

Î¼ Î¾

dÎ¾.
(19.51b)
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

390
The Univariate Gaussian Distribution
The following property of the noncentral Ï‡2 is useful in detection theory. In the sta-
tistics literature this property is called the Monotone Likelihood Ratio property
(Lehmann and Romano, 2005, Section 3.4). Alternatively, it is called the Total
Positivity of Order 2 of the function (x, Î») 	â†’fÏ‡2
n,Î»(x).
Proposition 19.8.3 (The Noncentral Ï‡2 Family Has Monotone Likelihood Ratio).
Let fÏ‡2
n,Î»(Î¾) denote the density at Î¾ of the noncentral Ï‡2 distribution with n degrees
of freedom and noncentrality parameter Î» â‰¥0; see (19.47). Then for Î¾0, Î¾1 > 0
and Î»0, Î»1 â‰¥0 we have

Î¾0 < Î¾1 and Î»0 < Î»1

=â‡’

fÏ‡2
n,Î»1 (Î¾0) fÏ‡2
n,Î»0 (Î¾1) â‰¤fÏ‡2
n,Î»0 (Î¾0) fÏ‡2
n,Î»1 (Î¾1)

,
(19.52)
i.e.,

Î»1 > Î»0

=â‡’
	
Î¾ 	â†’
fÏ‡2
n,Î»1 (Î¾)
fÏ‡2
n,Î»0 (Î¾)
is nondecreasing in Î¾ > 0

.
(19.53)
Proof. See, for example, (Finner and Roters, 1997, Proposition 3.8).
19.9
The Limit of Gaussians Is Gaussian
There are a number of useful deï¬nitions of convergence for sequences of random
variables. Here we brieï¬‚y mention a few and show that, under each of these deï¬-
nitions, the convergence of a sequence of Gaussian random variables to a random
variable X implies that X is Gaussian.
Let the random variables X, X1, X2, . . . be deï¬ned over a common probability space
(Î©, F, P). We say that the sequence X1, X2, . . . converges to X with probability
one or almost surely if
Pr
'
Ï‰ âˆˆÎ© : lim
nâ†’âˆXn(Ï‰) = X(Ï‰)
(
= 1.
(19.54)
Thus, the sequence X1, X2, . . . converges to X almost surely if there exists an event
N âˆˆF of probability zero such that for every Ï‰ /âˆˆN the sequence of real numbers
X1(Ï‰), X2(Ï‰), . . . converges to the real number X(Ï‰).
The sequence X1, X2, . . . converges to X in probability if
lim
nâ†’âˆPr

|Xn âˆ’X| â‰¥Ïµ

= 0,
Ïµ > 0.
(19.55)
The sequence X1, X2, . . . converges to X in mean square if
lim
nâ†’âˆE
%
Xn âˆ’X
2&
= 0.
(19.56)
We refer the reader to (Shiryaev, 1996, Ch. II, Section 10, Theorem 2) for a proof
that convergence in mean-square implies convergence in probability and for a proof
that almost-sure convergence implies convergence in probability. Also, if a sequence
converges in probability to X, then it has a subsequence that converges to X with
probability one (Shiryaev, 1996, Ch. II, Section 10, Theorem 5).
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.9 The Limit of Gaussians Is Gaussian
391
Theorem 19.9.1. Let the random variables X, X1, X2, . . . be deï¬ned over a common
probability space (Î©, F, P). Assume that each of the random variables X1, X2, . . .
is Gaussian. If the sequence X1, X2, . . . converges to X in the sense of (19.54) or
(19.55) or (19.56), then X must also be Gaussian.
Proof. Since both mean-square convergence and almost-sure convergence imply
convergence in probability, it suï¬ƒces to prove the theorem in the case where the
sequence X1, X2, . . . converges to X in probability. And since every sequence con-
verging to X in probability has a subsequence converging to X almost surely, it
suï¬ƒces to prove the theorem for almost sure convergence. Our proof for this case
follows (Shiryaev, 1996, Ch. II, Section 13, Paragraph 5).
Since the random variables X1, X2, . . . are all Gaussian, it follows from (19.29) that
E

eiÏ–Xn
= eiÏ–Î¼nâˆ’1
2 Ï–2Ïƒ2
n,
Ï– âˆˆR,
(19.57)
where Î¼n and Ïƒ2
n are the mean and variance of Xn. By the Dominated Convergence
Theorem it follows that the almost sure convergence of X1, X2, . . . to X implies
that
lim
nâ†’âˆE

eiÏ–Xn
= E

eiÏ–X
,
Ï– âˆˆR.
(19.58)
It follows from (19.57) and (19.58) that
lim
nâ†’âˆeiÏ–Î¼nâˆ’1
2 Ï–2Ïƒ2
n = E

eiÏ–X
,
Ï– âˆˆR.
(19.59)
The limit in (19.59) can exist for every Ï– âˆˆR only if there exist Î¼, Ïƒ2 such that
Î¼n â†’Î¼ and Ïƒ2
n â†’Ïƒ2. And in this case, by (19.59),
E

eiÏ–X
= eiÏ–Î¼âˆ’1
2 Ï–2Ïƒ2,
Ï– âˆˆR,
so, by Proposition 19.7.1 (ii) and by (19.29), X is N

Î¼, Ïƒ2
.
As we have noted, convergence in mean-square implies convergence in probability
(Shiryaev, 1996, Ch. II, Section 10, Theorem 2). The reverse is not true. But it is
true for Gaussian sequences:
Theorem 19.9.2. Let X1, X2, . . . be a sequence of Gaussian random variables that
converges in probability to the random variable X. Then X is Gaussian, and
lim
nâ†’âˆE

(Xn âˆ’X)p
= 0,
1 â‰¤p â‰¤âˆ,
where for p = âˆthe above should be interpreted as indicating that for every Ïµ > 0
the absolute diï¬€erence |Xn âˆ’X| is upper-bounded by Ïµ with probability one when-
ever n is suï¬ƒciently large.
Proof. See (Neveu, 1968, Ch. I, Lemma 1.5).
Another type of convergence is convergence in distribution or weak conver-
gence, which is deï¬ned as follows. Let F1, F2, . . . denote the cumulative distri-
bution functions of the sequence of random variables X1, X2, . . . We say that the
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

392
The Univariate Gaussian Distribution
sequence F1, F2, . . . (or sometimes X1, X2, . . .) converges in distribution to the cu-
mulative distribution function F(Â·) if Fn(Î¾) converges to F(Î¾) at every point Î¾ âˆˆR
at which F(Â·) is continuous. That is,

Fn(Î¾) â†’F(Î¾)

,

F(Â·) is continuous at Î¾

.
(19.60)
Theorem 19.9.3. Let the sequence of random variables X1, X2, . . . be such that
Xn âˆ¼N

Î¼n, Ïƒ2
n

, for every n âˆˆN. Then the sequence converges in distribution to
some limiting distribution if, and only if, there exist some Î¼ and Ïƒ2 such that
Î¼n â†’Î¼ and Ïƒ2
n â†’Ïƒ2.
(19.61)
And if the sequence does converge in distribution, then it converges to the mean-Î¼
variance-Ïƒ2 Gaussian distribution.
Proof. See (Gikhman and Skorokhod, 1996, Chapter I, Section 3, Theorem 4)
where this statement is proved in the multivariate case.
For extensions of Theorems 19.9.1 and 19.9.3 to random vectors, see Theorems 23.9.1
and 23.9.2 in Chapter 23 ahead.
19.10
Additional Reading
The Gaussian distribution, its characteristic function, and its moment generating
function appear in almost every basic book on Probability Theory. For more on
the Q-function see (VerdÂ´u, 1998, Section 3.3) and (Simon, 2002). For more on
distributions related to the Gaussian distribution see (Simon, 2002), (Johnson,
Kotz, and Balakrishnan, 1994), and (Johnson, Kotz, and Balakrishnan, 1995).
For more on the central Ï‡2 distribution see (Johnson, Kotz, and Balakrishnan,
1994, Chapter 18) and (Simon, 2002, Chapter 2). For more on the noncentral Ï‡2
distribution see (Johnson, Kotz, and Balakrishnan, 1995, Chapter 29) and (Simon,
2002, Chapter 2). Various characterizations of the Gaussian distribution can be
found in (Bryc, 1995) and (Bogachev, 1998).
19.11
Exercises
Exercise 19.1 (Sums of Independent Gaussians). Let X1 âˆ¼N

0, Ïƒ2
1

and X2 âˆ¼N

0, Ïƒ2
2

be independent. Convolve their densities to show that X1 + X2 is Gaussian.
Exercise 19.2 (Computing Probabilities). Let X âˆ¼N(1, 3) and Y âˆ¼N(âˆ’2, 4) be inde-
pendent. Express the probabilities Pr[X â‰¤2] and Pr[2X +3Y > âˆ’2] using the Q-function
with nonnegative arguments.
Exercise 19.3 (Bounds on the Q-Function). Prove (19.19).
We suggest changing the
integration variable in (19.9) to Î¶ â‰œÎ¾ âˆ’Î± and then proving (19.19) using the inequality
1 âˆ’Î¶2
2 â‰¤exp
	
âˆ’Î¶2
2

â‰¤1,
Î¾ âˆˆR.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

19.11 Exercises
393
Exercise 19.4 (An Application of Craigâ€™s Formula). Let the random variables Z âˆ¼N(0, 1)
and A be independent, where A2 is of MGF MA2(Â·). Show that
Pr
$
Z â‰¥|A|
%
= 1
Ï€
 Ï€/2
0
MA2
	
âˆ’
1
2 sin2 Ï•

dÏ•.
Exercise 19.5 (An Expression for Q2(Î±)). In analogy to (19.15), derive the identity
Q2(Î±) = 1
Ï€
 Ï€/4
0
e
âˆ’
Î±2
2 sin2 Ï• dÏ•,
Î± â‰¥0.
Exercise 19.6 (Expectation of Q(X)). Show that for any RV X
E
$
Q(X)
%
=
1
âˆš
2Ï€
 âˆ
âˆ’âˆ
Pr[X â‰¤Î¾] eâˆ’Î¾2/2 dÎ¾.
(See (VerdÂ´u, 1998, Chapter 3, Section 3.3, Eq. (3.57)).)
Exercise 19.7 (Generating Gaussians from Uniform RVs).
(i) Let W1 and W2 be IID N(0, 1), and let R =

W 2
1 + W 2
2 .
Show that R has
a Rayleigh distribution, i.e., that its density fR(r) is given for every r âˆˆR by
reâˆ’r2
2 I{r â‰¥0}. What is the CDF FR(Â·) of R?
(ii) Prove that if a RV X is of density fX(Â·) and of CDF FX(Â·), then FX(X) âˆ¼U (0, 1).
(iii) Show that if U1 and U2 are IID U (0, 1) and if we deï¬ne R =
-
ln
1
U1 and Î˜ = 2Ï€U2,
then R cos Î˜ and R sin Î˜ are IID N(0, 1/2). (This is the Box-Muller transform. See
also Exercise 19.8.)
Exercise 19.8 (More on Generating Gaussians). Let U1 and U2 be IID U (0, 1). Deï¬ne
SÎ½ = 1 âˆ’2UÎ½ for Î½ = 1, 2, and deï¬ne S =

S2
1 + S2
2. Show that, conditional on S being
smaller than 1, the random variables
S1
S

âˆ’2 ln S2
and
S2
S

âˆ’2 ln S2
are independent standard Gaussians (Grimmett and Stirzaker, 2001, Chapter 4, Sec-
tion 4.11, Exercise 7). This is a variation on the Box-Muller transform (Exercise 19.7).
Exercise 19.9 (Inï¬nite Divisibility). Show that for any Î¼ âˆˆR and Ïƒ2 â‰¥0 there exist IID
random variables X and Y such that X + Y âˆ¼N

Î¼, Ïƒ2
.
Exercise 19.10 (Gaussian Mixtures of Gaussians).
(i) A RV X is drawn N

Î¼x, Ïƒ2
x

. Conditional on X = x, a RV Y is drawn N

x, Ïƒ2
y

.
Find the distribution of Y .
(ii) Prove that
Q(Î±) =
 âˆ
âˆ’âˆ
1
âˆšÏ€ eâˆ’Î¾2Q
	âˆš
2(Î± âˆ’Î¾)

dÎ¾,
Î± âˆˆR.
Hint: The sum of independent Gaussians is Gaussian (Proposition 19.7.2).
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,
www.ebook3000.com

394
The Univariate Gaussian Distribution
Exercise 19.11 (The Sum Is Gaussian). Show that if at least one of two independent
random variables is Gaussian and if their sum is Gaussian, then both are Gaussian.
Exercise 19.12 (MGF of the Square of a Gaussian). Derive (19.32).
Exercise 19.13 (The Distribution of the Magnitude). Show that if a random variable X
is of density fX(Â·) and if Y = |X|, then the density fY (Â·) of Y is
fY (y) =

fX(y) + fX(âˆ’y)

I{y â‰¥0},
y âˆˆR.
Exercise 19.14 (Uniformly Distributed Random Variables). Suppose that X âˆ¼U

[0, 1]

.
(i) Find the characteristic function Î¦X(Â·) of X.
(ii) Show that if X and Y are independent with X as above, then X+Y is not Gaussian.
Exercise 19.15 (More on the Ï‡2 Distribution). For X âˆ¼Ï‡2
2m, where m is a positive
integer, and for Î³ > 0 arbitrary, show that
E
+
Q

Î³X
,
â‰¤1
2
1
(1 + Î³)m .
Hint: Use (19.20) and (19.35).
Exercise 19.16 (Steinâ€™s Characterization of Standard Gaussians).
(i) Let W be a standard Gaussian. Show that for every continuously diï¬€erentiable
function h: R â†’R such that h and its derivative hâ€² grow at most polynomially in
|x| as |x| â†’âˆ
E
$
hâ€²(W)
%
= E
$
W h(W)
%
.
(ii) Argue heuristically that the above characterizes the standard Gaussian distribution.
Hint: For Part (ii) you might want to consider functions that closely approximate the
step-function x â†’I{x â‰¥Î¾}.
available at 
.021
14:35:45, subject to the Cambridge Core terms of use,

Chapter 20
Binary Hypothesis Testing
20.1
Introduction
In Digital Communications the task of the receiver is to observe the channel out-
puts and to use these observations to accurately guess the data bits that were sent
by the transmitter, i.e., the data bits that were fed to the modulator. Ideally, the
guessing would be perfect, i.e., the receiver would make no errors. This, alas, is
typically impossible because of the distortions and noise that the channel intro-
duces. Indeed, while one can usually recover the data bits from the transmitted
waveform (provided that the modulator is a one-to-one mapping), the receiver has
no access to the transmitted waveform but only to the received waveform. And
since the latter is typically a noisy version of the former, some errors are usually
unavoidable.
In this chapter we shall begin our study of how to guess intelligently, i.e., how,
given the channel output, one should guess the data bits with as low a probability
of error as possible. This study will help us not only in the design of receivers but
also in the design of modulators that allow for reliable decoding from the channelâ€™s
output.
In the engineering literature the process of guessing the data bits based on the
channel output is called â€œdecoding.â€
In the statistics literature this process is
called â€œhypothesis testing.â€ We like â€œguessingâ€ because it demystiï¬es the process.
In most applications the channel output is a continuous-time waveform and we seek
to decode a large number of bits. Nevertheless, for pedagogical reasons, we shall
begin our study with the simpler case where we wish to decode only a single data
bit. This corresponds in the statistics literature to â€œbinary hypothesis testing,â€
where the term â€œbinaryâ€ reminds us that in this guessing problem there are only
two alternatives.
Moreover, we shall assume that the observation, rather than
being a continuous-time waveform, is a vector or a scalar. In fact, we shall begin
our study with the simplest case where there are no observations at all.
20.2
Problem Formulation
In choosing a guessing strategy to minimize the probability of error, the labels
of the two alternatives are immaterial. The principles that guide us in guessing
395
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

396
Binary Hypothesis Testing
the outcome of a fair coin toss (where the labels are â€œheadsâ€ or â€œtailsâ€) are the
same as for guessing the value of a random variable that takes on the values +1
and âˆ’1 equiprobably. (These are, of course, extremely simple cases that can be
handled with common sense.) Statisticians typically denote the two alternatives
by H0 and H1 and call them â€œhypotheses.â€ We shall denote the two alternatives
by 0 and 1. We thus envision guessing the value of a random variable H taking
values in the set {0, 1} with probabilities
Ï€0 = Pr[H = 0],
Ï€1 = Pr[H = 1].
(20.1)
The prior is the distribution of H or the pair (Ï€0, Ï€1). It reï¬‚ects the state of our
knowledge about H before having made any observations. We say that the prior
is nondegenerate if
Ï€0, Ï€1 > 0.
(20.2)
(If the prior is degenerate, then H is deterministic and we can determine its value
without any observation. For example if Ï€0 = 0 we always guess 1 and never err.)
The prior is uniform if Ï€0 = Ï€1 = 1/2.
Aiding us in the guess work is the observation Y, which is a random vector
taking values in the observation space Rd.
(When d = 1 the observation is a
random variable and we denote it by Y .) We assume that Y is a column vector,
so, using the notation of Section 17.2,
Y =

Y (1), . . . , Y (d)T.
Typically there is some statistical dependence between Y and H; otherwise, Y
would be useless. If the dependence is so strong that from Y one can deduce H,
then our guess work is very easy: we simply compute from Y the value of H and
declare the result as our guess; we never err. The cases of most interest to us
are therefore those where Y neither determines H nor is statistically independent
of H. Unless otherwise speciï¬ed, we shall assume that, conditional on H = 0,
the observation Y is of density fY|H=0(Â·) and that, conditional on H = 1, it is of
density fY|H=1(Â·). Here fY|H=0(Â·) and fY|H=1(Â·) are nonnegative Borel measurable
functions from Rd to R that integrate to one.1
Our problem is how to use the observation Y to intelligently guess the value of H.
At ï¬rst we shall limit ourselves to deterministic guessing rules. Later we shall show
that no randomized guessing rule can outperform an optimal deterministic rule. A
deterministic guessing rule (or decision rule, or decoding rule) for guessing H
based on Y is a (Borel measurable) mapping from the set of possible observations
Rd to the set {0, 1}. We denote such a mapping by
Ï†Guess : Rd â†’{0, 1}
(20.3)
and say that Ï†Guess(yobs) is the guess we make after having observed that Y = yobs.
1Readers who are familiar with Measure Theory should note that these are densities with
respect to the Lebesgue measure on Rd, but that the reference measure is inessential to our
analysis. We could have also chosen as our reference measure the sum of the probability measures
on Rd corresponding to H = 0 and to H = 1. This would have guaranteed the existence of the
densities.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.3 Guessing in the Absence of Observables
397
The probability of error associated with the guessing rule Ï†Guess(Â·) is
Pr(error) â‰œPr[Ï†Guess(Y) Ì¸= H].
(20.4)
Note that two sources of randomness determine whether the guessing rule Ï†Guess(Â·)
errs or not: the realization of H and the generation of Y conditional on that
realization.
We say that a guessing rule is optimal if no other guessing rule
attains a smaller probability of error. (We shall later see that there always exists
an optimal guessing rule.2) In general, there may be a number of diï¬€erent optimal
guessing rules.
We shall therefore try to refrain from speaking of the optimal
guessing rule. We apologize if this results in cumbersome writing. The probability
of error associated with optimal guessing rules is the optimal probability of
error and is denoted throughout by
pâˆ—(error).
20.3
Guessing in the Absence of Observables
We begin with the simplest case where there are no observables. Common sense
dictates that in this case we should base our guess on the prior (Ï€0, Ï€1) as follows.
If Ï€0 > Ï€1, then we should guess that the value of H is 0; if Ï€0 < Ï€1, then we
should guess the value 1; and if Ï€0 = Ï€1 = 1/2, then it does not really matter what
we guess: the probability of error will be either way 1/2.
To verify that this intuition is correct note that, since there are no observables,
there are only two guessing rules: the rule â€œguess 0â€ and the rule â€œguess 1.â€ The
former results in the probability of error Ï€1 (it is in error whenever H = 1, which
happens with probability Ï€1), and the latter results in the probability of error Ï€0.
Hence the former rule is optimal if Ï€0 â‰¥Ï€1 and the latter is optimal when Ï€1 â‰¥Ï€0.
When Ï€0 = Ï€1 both rules are optimal and we can use either one.
We summarize that, in the absence of observations, an optimal guessing rule is:
Ï†âˆ—
Guess =

0
if Pr[H = 0] â‰¥Pr[H = 1],
1
otherwise.
(20.5)
(Here we guess 0 also when Pr[H = 0] = Pr[H = 1]. An equally good rule would
guess 1 in this case.)
As we next show, the error probability pâˆ—(error) of this rule is
pâˆ—(error) = min

Pr[H = 0], Pr[H = 1]

.
(20.6)
This can be veriï¬ed by considering the case where Pr[H = 0] â‰¥Pr[H = 1] and the
case where Pr[H = 0] < Pr[H = 1] separately. By (20.5), in the former case our
guess is 0 with the associated probability of error Pr[H = 1], whereas in the latter
case our guess is 1 with the associated probability of error Pr[H = 0]. In either
case the probability of error is given by the RHS of (20.6).
2Thus, while there is no such thing as â€œsmallest strictly positive number,â€ i.e., a positive
number that is smaller-or-equal to any other positive number, we shall see that there always
exists a guessing rule that no other guessing rule can outperform. Mathematicians paraphrase
this by saying that â€œthe inï¬mum of the probability of error over all the guessing rules is achievable,
i.e., is a minimum.â€
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

398
Binary Hypothesis Testing
20.4
The Joint Law of H and Y
Before we can extend the results of Section 20.3 to the more interesting case where
we guess H after observing Y, we pause to discuss the joint distribution of H
and Y. This joint distribution is needed in order to derive an optimal decision rule
and in order to analyze its performance. Some care must be exercised in describing
this law because H is discrete (binary) and Y has a density. It is usually simplest
to describe the joint law by describing the prior (the distribution of H), and by
then describing the conditional law of Y given H = 0 and the conditional law of Y
given H = 1.
If, conditional on H = 0, the distribution of Y has the density fY|H=0(Â·) and if,
conditional on H = 1, the distribution of Y has the density fY|H=1(Â·), then the
joint distribution of H and Y can be described using the prior (Ï€0, Ï€1) (20.1) and
the conditional densities
fY|H=0(Â·)
and
fY|H=1(Â·).
(20.7)
From the prior (Ï€0, Ï€1) and the conditional densities fY|H=0(Â·), fY|H=1(Â·) we can
compute the (unconditional) density of Y:
fY(y) = Ï€0fY|H=0(y) + Ï€1fY|H=1(y),
y âˆˆRd.
(20.8)
The conditional distribution of H given Y = yobs is a bit more tricky because
the probability of Y taking on the value yobs (exactly) is zero. There are two
approaches to deï¬ning Pr[H = 0|Y = yobs] in this case: the heuristic one that is
usually used in a ï¬rst course on probability theory and the measure-theoretic one
that was pioneered by Kolmogorov. Our approach is to deï¬ne this quantity in a
way that will be palatable to both mathematicians and engineers and to then give
a heuristic justiï¬cation for our deï¬nition.
We deï¬ne the conditional probability that H = 0 given Y = yobs as
Pr

H = 0
 Y = yobs

â‰œ
 Ï€0fY|H=0(yobs)
fY(yobs)
if fY(yobs) > 0,
1
2
otherwise,
(20.9a)
where fY(Â·) is given in (20.8), and analogously
Pr

H = 1
 Y = yobs

â‰œ
 Ï€1fY|H=1(yobs)
fY(yobs)
if fY(yobs) > 0,
1
2
otherwise.
(20.9b)
Notice that our deï¬nition is meaningful in the sense that the values we assign to
Pr[H = 0|Y = yobs] and Pr[H = 1|Y = yobs] are nonnegative and sum to one:
Pr

H = 0
 Y = yobs

+ Pr

H = 1
 Y = yobs

= 1,
yobs âˆˆRd.
(20.10)
Also note that our deï¬nition of Pr[H = 0|Y = yobs] and Pr[H = 1|Y = yobs]
for those yobs âˆˆRd for which fY(yobs) = 0 is quite arbitrary; we chose 1/2 just
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.4 The Joint Law of H and Y
399
for concreteness.3 Indeed, it is not diï¬ƒcult to verify that the probability that yobs
satisï¬es Ï€0fY|H=0(yobs)+Ï€1fY|H=1(yobs) = 0 is zero, and hence our deï¬nitions in
this eventuality are not important; see (20.12) ahead.
If d = 1, then the observation is a random variable Y and a heuristic way to
motivate (20.9a) is to consider the limit
lim
Î´â†“0
Pr

H = 0, Y âˆˆ

yobs âˆ’Î´, yobs + Î´

Pr

Y âˆˆ

yobs âˆ’Î´, yobs + Î´

.
(20.11)
Assuming some regularity of the conditional densities (e.g., continuity) we can use
the approximations
Pr

H = 0, Y âˆˆ(yobs âˆ’Î´, yobs + Î´)

= Ï€0
 yobs+Î´
yobsâˆ’Î´
fY |H=0(y) dy
â‰ˆ2Ï€0Î´fY |H=0(yobs),
Î´ â‰ª1,
Pr

Y âˆˆ(yobs âˆ’Î´, yobs + Î´)

=
 yobs+Î´
yobsâˆ’Î´
fY (y) dy
â‰ˆ2Î´fY (yobs),
Î´ â‰ª1,
to argue that, under suitable regularity conditions, (20.11) agrees with the RHS of
(20.9a) when fY (yobs) > 0. A similar calculation can be carried out in the vector
case where d > 1.
We next remark on observations yobs at which the density of Y is zero. Accounting
for such observations makes the writing a bit cumbersome as in (20.9). Fortunately,
the probability of such observations is zero:
Note 20.4.1. Let H be drawn according to the prior (Ï€0, Ï€1), and let the con-
ditional densities of Y given H be fY|H=0(Â·) and fY|H=1(Â·) with fY(Â·) given in
(20.8). Then
Pr

Y âˆˆ
Ëœy âˆˆRd : fY(Ëœy) = 0

= 0.
(20.12)
Proof.
Pr

Y âˆˆ
Ëœy âˆˆRd : fY(Ëœy) = 0

=

{ËœyâˆˆRd:fY(Ëœy)=0}
fY(y) dy
=

{ËœyâˆˆRd:fY(Ëœy)=0}
0 dy
= 0,
where the second equality follows because the integrand is zero over the range of
integration.
3In the measure-theoretic probability literature our deï¬nition is just a â€œversionâ€ (among many
others) of the conditional probabilities of the event H = 0 (respectively H = 1), conditional on
the Ïƒ-algebra generated by the random vector Y (Billingsley, 1995, Section 33), (Williams, 1991,
Chapter 9).
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

400
Binary Hypothesis Testing
We conclude this section with two technical remarks which are trivial if you ignore
observations where fY(Â·) is zero:
Note 20.4.2. Consider the setup of Note 20.4.1.
(i) For every y âˆˆRd
min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

= min

Pr[H = 0|Y = y], Pr[H = 1|Y = y]

fY(y).
(20.13)
(ii) For every y âˆˆRd

Ï€0fY|H=0(y) â‰¥Ï€1fY|H=1(y)

â‡â‡’

Pr[H = 0|Y = y] â‰¥Pr[H = 1|Y = y]

.
(20.14)
Proof. Identity (20.13) can be proved using (20.9) and (20.8) by separately con-
sidering the case fY(y) > 0 and the case fY(y) = 0 (where the latter is equivalent,
by (20.8), to Ï€0fY|H=0(y) and Ï€1fY|H=1(y) both being zero).
To prove (20.14) we also separately consider the case fY(y) > 0 and the case
fY(y) = 0. In the former case we note that for c > 0 the condition a â‰¥b is
equivalent to the condition a/c â‰¥b/c so for fY(yobs) > 0

Ï€0fY|H=0(y) â‰¥Ï€1fY|H=1(y)

â‡â‡’
	Ï€0fY|H=0(y)
fY(y)



Pr[H=0|Y=y]
â‰¥Ï€1fY|H=1(y)
fY(y)



Pr[H=1|Y=y]

.
In the latter case where fY(y) = 0 we note that, by (20.8), both Ï€0fY|H=0(y)
and Ï€1fY|H=1(y) are zero, so the condition on the LHS of (20.14) is true (0 â‰¥0).
Fortunately, when fY(y) = 0 the condition on the RHS of (20.14) is also true,
because in this case (20.9) implies that Pr[H = 0|Y = y] and Pr[H = 1|Y = y]
are both equal to 1/2 (and 1/2 â‰¥1/2).
20.5
Guessing after Observing Y
We next derive an optimal rule for guessing H after observing that Y = yobs.
We begin with a heuristic argument. Having observed that Y = yobs, there are
only two possible decision rules: to guess 0 or guess 1. Which should we choose?
The answer now depends on the a posteriori distribution of H. Once it has been
revealed to us that Y = yobs, our outlook changes and we now assign the event
H = 0 the a posteriori probability Pr[H = 0|Y = yobs] and the event H = 1 the
complementary probability Pr[H = 1|Y = yobs]. If the former is greater than the
latter, then we should guess 0, and otherwise we should guess 1. Thus, after it has
been revealed to us that Y = yobs the situation is equivalent to one in which we
need to guess H without any observables and where our distribution on H is not
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.5 Guessing after Observing Y
401
its a priori distribution (prior) but its a posteriori distribution. Using our analysis
from Section 20.3 we conclude that the guessing rule
Ï†âˆ—
Guess(yobs) =

0
if Pr[H = 0|Y = yobs] â‰¥Pr[H = 1|Y = yobs],
1
otherwise,
(20.15)
is optimal. Once again, the way we resolve ties is arbitrary: if the observation
Y = yobs results in the a posteriori distribution of H being uniform, that is, if
Pr[H = 0|Y = yobs] = Pr[H = 1|Y = yobs] = 1/2, then either guess is optimal.
Using Note 20.4.2 (ii) we can also express the decision rule (20.15) as
Ï†âˆ—
Guess(yobs) =

0
if Ï€0fY|H=0(yobs) â‰¥Ï€1fY|H=1(yobs),
1
otherwise.
(20.16)
Conditional on Y = yobs, the probability of error of the optimal decision rule is,
in analogy to (20.6), given by
pâˆ—(error|Y = yobs) = min

Pr[H = 0|Y = yobs], Pr[H = 1|Y = yobs]

,
(20.17)
as can be seen by treating the case Pr[H = 0|Y = yobs] â‰¥Pr[H = 1|Y = yobs] and
the complementary case Pr[H = 0|Y = yobs] < Pr[H = 1|Y = yobs] separately.
The unconditional probability of error associated with the rule (20.15) is thus
pâˆ—(error) = E

min

Pr[H = 0|Y], Pr[H = 1|Y]

(20.18)
=

Rd min

Pr[H = 0|Y = y], Pr[H = 1|Y = y]

fY(y) dy
(20.19)
=

Rd min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

dy,
(20.20)
where the last equality follows from Note 20.4.2 (i).
Before summarizing these conclusions in a theorem, we present the following simple
lemma on the probabilities of error associated with general decision rules.
Lemma 20.5.1. Consider the setup of Note 20.4.1.
Let Ï†Guess(Â·) be an arbi-
trary guessing rule as in (20.3). Then the probabilities of error p(error|H = 0),
p(error|H = 1), and p(error) associated with Ï†Guess(Â·) are given by
p(error|H = 0) =

y/âˆˆD
fY|H=0(y) dy,
(20.21)
p(error|H = 1) =

yâˆˆD
fY|H=1(y) dy,
(20.22)
and
p(error) =

Rd

Ï€0fY|H=0(y) I{y /âˆˆD} + Ï€1fY|H=1(y) I{y âˆˆD}

dy,
(20.23)
where
D = {y âˆˆRd : Ï†Guess(y) = 0}.
(20.24)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

402
Binary Hypothesis Testing
Proof. Conditional on H = 0 the guessing rule makes an error only if Y does not
fall in the set of observations for which Ï†Guess(Â·) produces the guess â€œH = 0.â€ This
establishes (20.21). A similar argument proves (20.22). Finally, (20.23) follows
from (20.21) & (20.22) using the identity
p(error) = Ï€0 p(error|H = 0) + Ï€1 p(error|H = 1).
We next state the key result about binary hypothesis testing. The statement is a
bit cumbersome because, in general, there may be many observations that result
in H being a posteriori uniformly distributed, and an optimal decision rule can
map each such observation to a diï¬€erent guess and still be optimal.
Theorem 20.5.2 (Optimal Binary Hypothesis Testing). Suppose that a guessing
rule Ï†âˆ—
Guess : Rd â†’{0, 1} produces the guess â€œH = 0â€ only when yobs is such that
Ï€0fY|H=0(yobs) â‰¥Ï€1fY|H=1(yobs), i.e.,

Ï†âˆ—
Guess(yobs) = 0

=â‡’

Ï€0fY|H=0(yobs) â‰¥Ï€1fY|H=1(yobs)

,
(20.25a)
and produces the guess â€œH = 1â€ only when Ï€1fY|H=1(yobs) â‰¥Ï€0fY|H=0(yobs),
i.e.,

Ï†âˆ—
Guess(yobs) = 1

=â‡’

Ï€1fY|H=1(yobs) â‰¥Ï€0fY|H=0(yobs)

.
(20.25b)
Then no other guessing rule has a smaller probability of error, and
Pr

Ï†âˆ—
Guess(Y) Ì¸= H

=

Rd min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

dy.
(20.26)
Proof. Let Ï†Guess : Rd â†’{0, 1} be any guessing rule, and let
D = {y âˆˆRd : Ï†Guess(y) = 0}
(20.27)
be the set of observations that result in Ï†Guess(Â·) producing the guess â€œH = 0.â€
Then the probability of error associated with Ï†Guess(Â·) can be lower-bounded by
Pr

Ï†Guess(Y) Ì¸= H

=

Rd

Ï€0fY|H=0(y) I{y /âˆˆD} + Ï€1fY|H=1(y) I{y âˆˆD}

dy
â‰¥

Rd min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

dy,
(20.28)
where the equality follows from Lemma 20.5.1 and where the inequality follows
because for every value of y âˆˆRd
Ï€0fY|H=0(y) I{y /âˆˆD} + Ï€1fY|H=1(y) I{y âˆˆD}
â‰¥min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

,
(20.29)
as can be veriï¬ed by noting that, irrespective of the set D, one of the two terms
I{y âˆˆD} and I{y /âˆˆD} is equal to one and the other is equal to zero, so the LHS of
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.6 Randomized Decision Rules
403
(20.29) is either equal to Ï€0fY|H=0(y) or to Ï€1fY|H=1(y) and hence lower-bounded
by min{Ï€0fY|H=0(y), Ï€1fY|H=1(y)}.
We prove the optimality of Ï†âˆ—
Guess(Â·) by next showing that the probability of error
associated with Ï†âˆ—
Guess(Â·) is equal to the RHS of (20.28). To this end we deï¬ne
Dâˆ—= {y âˆˆRd : Ï†âˆ—
Guess(y) = 0}
(20.30)
and note that if both (20.25a) and (20.25b) hold, then
Ï€0fY|H=0(y) I{y /âˆˆDâˆ—} + Ï€1fY|H=1(y) I{y âˆˆDâˆ—}
= min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

,
y âˆˆRd.
(20.31)
Applying Lemma 20.5.1 to the decoder Ï†âˆ—
Guess(Â·) we obtain
Pr

Ï†âˆ—
Guess(Y) Ì¸= H

=

Rd

Ï€0fY|H=0(y) I{y /âˆˆDâˆ—} + Ï€1fY|H=1(y) I{y âˆˆDâˆ—}

dy
=

Rd min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

dy,
(20.32)
where the second equality follows from (20.31). The theorem now follows from
(20.28) and (20.32).
Referring to a situation where the observation results in the a posteriori distribu-
tion of H being uniform as a tie we have:
Note 20.5.3. The fact that both conditional on H = 0 and conditional on H = 1
the observation Y has a density does not imply that the probability of a tie is zero.
For example, if H takes values in {0, 1} equiprobably, and if the observation Y is
given by Y = H + U, where U is uniformly distributed over the interval [âˆ’2, 2]
independently of H, then the a posteriori distribution of H is uniform whenever
Y âˆˆ[âˆ’1, 2], and this occurs with probability 3/4.
20.6
Randomized Decision Rules
So far we have restricted ourselves to deterministic decision rules, where the guess
is a deterministic function of the observation. We next remove this restriction and
allow for some randomization in the decision rule. As we shall see in this section
and in greater generality in Section 20.11, when properly deï¬ned, randomization
does not help: the lowest probability of error that is achievable with randomized
decision rules can also be achieved with deterministic decision rules.
By a randomized decision rule we mean that, after observing that Y = yobs, the
guesser chooses some bias b(yobs) âˆˆ[0, 1] and then tosses a coin of that bias.
If the result is â€œheadsâ€ it guesses 0 and otherwise it guesses 1.
Note that the
deterministic rules we have considered before are special cases of the randomized
ones: any deterministic decision rule can be viewed as a randomized decision rule
where, depending on yobs, the bias b(yobs) is either zero or one.
Some care must be exercised in deï¬ning the joint distribution of the coin toss with
the other variables (H, Y). We do not want to allow for â€œtelepathic coins.â€ That is,
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

404
Binary Hypothesis Testing
yobs
Bias
Calculator
b(yobs)
Guess
Î˜ âˆ¼U ([0, 1])
Î˜ < b(yobs) â‡’â€œH = 0â€
Î˜ â‰¥b(yobs) â‡’â€œH = 1â€
Random Number
Generator
Figure 20.1: A block diagram of a randomized decision rule.
we want to make sure that once Y = yobs has been observed and the bias b(yobs)
has been accordingly computed, the outcome of the coin toss is random, i.e., has
nothing to do with H. Probabilists would say that we require that, conditional on
Y = yobs, the outcome of the coin toss be independent of H. (We shall discuss
conditional independence in Section 20.11.) We can clarify the setting as follows.
Upon observing the outcome Y = yobs, the guesser computes the bias b(yobs).
Using a local random number generator the guesser then draws a random variable Î˜
uniformly over the interval [0, 1], independently of the pair (H, Y). If the outcome Î¸
is smaller than b(yobs), then it guesses â€œH = 0,â€ and otherwise it guesses â€œH = 1.â€
A randomized decision rule is depicted in Figure 20.1.
We oï¬€er two proofs that randomized decision rules cannot outperform the best
deterministic ones.
The ï¬rst is by straightforward calculation.
Conditional on
Y = yobs, the randomized guesser makes an error either if Î˜ â‰¤b(yobs) (resulting
in the guess â€œH = 0â€) while H = 1, or if Î˜ > b(yobs) (resulting in the guess
â€œH = 1â€) while H = 0. Consequently,
Pr

error
 Y = yobs

= b(yobs) Pr

H = 1
 Y = yobs

+

1 âˆ’b(yobs)

Pr

H = 0
 Y = yobs

.
(20.33)
Thus, Pr(error|Y = yobs) is a weighted average of Pr[H = 0|Y = yobs] and
Pr[H = 1|Y = yobs]. As such, irrespective of the weights, it cannot be smaller
than the minimum of the two. But, by (20.17), the optimal deterministic decision
rule (20.15) achieves just this minimum. We conclude that, irrespective of the bias,
for each outcome Y = yobs the conditional probability of error of the randomized
decoder is lower-bounded by that of the optimal deterministic decoder (20.15).
Since this is the case for every outcome, it must also be the case when we average
over the outcomes. This concludes the ï¬rst proof.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.7 The MAP Decision Rule
405
In the second proof we view the outcome of the local random number generator Î˜
as an additional observation.
Since it is independent of (H, Y) and since it is
uniform over [0, 1],
fY,Î˜|H=0(y, Î¸) = fY|H=0(y) fÎ˜|Y=y,H=0(Î¸)
= fY|H=0(y) fÎ˜(Î¸)
= fY|H=0(y) I{0 â‰¤Î¸ â‰¤1},
(20.34a)
and similarly
fY,Î˜|H=1(y, Î¸) = fY|H=1(y) I{0 â‰¤Î¸ â‰¤1}.
(20.34b)
Since the randomized decision rule can be viewed as a deterministic decision
rule that is based on the pair (Y, Î˜), it cannot outperform any optimal de-
terministic guessing rule based on (Y, Î˜).
But by Theorem 20.5.2 and (20.34)
it follows that the deterministic decision rule that guesses â€œH = 0â€ whenever
Ï€0fY|H=0(y) â‰¥Ï€1fY|H=1(y) is optimal not only for guessing H based on Y but
also for guessing H based on (Y, Î˜), because it produces the guess â€œH = 0â€ only
when Ï€0fY,Î˜|H=0(y, Î¸) â‰¥Ï€1fY,Î˜|H=1(y, Î¸) and it produces the guess â€œH = 1â€
only when Ï€1fY,Î˜|H=1(y, Î¸) â‰¥Ï€0fY,Î˜|H=0(y, Î¸). This concludes the second proof.
Even though randomized decision rules cannot outperform the best deterministic
rules, they may have other advantages. For example, they allow for more symmetric
ways of resolving ties. Suppose, for example, that we have no observations and that
the prior is uniform. In this case guessing â€œH = 0â€ will give rise to a probability of
error of 1/2, with an error occurring whenever H = 1. Similarly guessing â€œH = 1â€
will also result in a probability of error of 1/2, this time with an error occurring
whenever H = 0. If we think about H as being an information bit, then the former
rule makes sending 0 less error prone than sending 1. A randomized test that ï¬‚ips
a fair coin and guesses 0 if â€œheadsâ€ and 1 if â€œtailsâ€ gives rise to the same average
probability of error (i.e., 1/2) and makes sending 0 and sending 1 equally (highly)
error prone.
If Y = yobs results in a tie, i.e., if it yields a uniform a posteriori distribution
on H,
Pr

H = 0
 Y = yobs

= Pr

H = 1
 Y = yobs

= 1
2,
then the probability of error of the randomized decoder (20.33) does not depend on
the bias. In this case there is thus no loss of optimality in choosing b(yobs) = 1/2,
i.e., by employing a fair coin. This makes for a symmetric way of resolving the tie
in the a posteriori distribution of H.
20.7
The MAP Decision Rule
In Section 20.5 we presented an optimal decision rule (20.15). A slight variation
on that decoder is the Maximum A Posteriori (MAP) decision rule. The MAP
rule is identical to (20.15) except in how it resolves ties. Unlike (20.15), which
resolves ties by guessing â€œH = 0,â€ the MAP rule resolves ties by ï¬‚ipping a fair
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

406
Binary Hypothesis Testing
coin. It can thus be summarized as follows:
Ï†MAP(yobs) â‰œ
â§
âª
â¨
âª
â©
0
if Pr[H = 0|Y = yobs] > Pr[H = 1|Y = yobs],
1
if Pr[H = 0|Y = yobs] < Pr[H = 1|Y = yobs],
U

{0, 1}

if Pr[H = 0|Y = yobs] = Pr[H = 1|Y = yobs],
(20.35)
where we use â€œU

{0, 1}

â€ to indicate that we guess the outcome uniformly at
random.
Note that, like the rule in (20.15), the MAP rule is optimal. This follows because
the way ties are resolved does not inï¬‚uence the probability of error, and because
the MAP rule agrees with the rule (20.15) for all observations which do not result
in a tie.
Theorem 20.7.1 (The MAP Rule Is Optimal). The Maximum A Posteriori deci-
sion rule (20.35) is optimal.
Since the MAP decoder is optimal,
pâˆ—(error) = Ï€0 pMAP(error|H = 0) + Ï€1 pMAP(error|H = 1),
(20.36)
where pMAP(error|H = 0) and pMAP(error|H = 1) denote the conditional prob-
abilities of error for the MAP decoder.
Note that one can easily ï¬nd guessing
rules (such as the rule â€œalways guess 0â€) that yield a conditional probability of
error smaller than pMAP(error|H = 0), but one cannot ï¬nd a rule whose average
probability of error outperforms the RHS of (20.36).
Using Note 20.4.2 (ii) we can express the MAP rule in terms of the densities and
the prior as
Ï†MAP(yobs) =
â§
âª
â¨
âª
â©
0
if Ï€0fY|H=0(yobs) > Ï€1fY|H=1(yobs),
1
if Ï€0fY|H=0(yobs) < Ï€1fY|H=1(yobs),
U

{0, 1}

if Ï€0fY|H=0(yobs) = Ï€1fY|H=1(yobs).
(20.37)
Alternatively, the MAP decision rule can be described using the likelihood-ratio
function LR(Â·), which is deï¬ned by
LR(y) â‰œfY|H=0(y)
fY|H=1(y),
y âˆˆRd
(20.38)
using the convention
Î±
0 = âˆ,
Î± > 0

and
0
0 = 1.
(20.39)
Since densities are nonnegative, and since we are deï¬ning the likelihood-ratio func-
tion using the convention (20.39), the codomain of LR(Â·) is the set [0, âˆ] consisting
of the nonnegative reals and the special symbol âˆ:
LR: Rd â†’[0, âˆ].
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.8 The ML Decision Rule
407
Using the likelihood-ratio function and (20.37), we can rewrite the MAP rule for
the case where the prior is nondegenerate (20.2) and where the observation yobs is
such that fY(yobs) > 0 as
Ï†MAP(yobs) =
â§
âª
â¨
âª
â©
0
if LR(yobs) > Ï€1
Ï€0 ,
1
if LR(yobs) < Ï€1
Ï€0 ,
U

{0, 1}

if LR(yobs) = Ï€1
Ï€0 ,

Ï€0, Ï€1 > 0,
fY(yobs) > 0

.
(20.40)
Since many of the densities that are of interest to us have an exponential form, it
is sometimes more convenient to describe the MAP rule using the log likelihood-
ratio function LLR: Rd â†’[âˆ’âˆ, âˆ], which is deï¬ned by
LLR(y) â‰œln fY|H=0(y)
fY|H=1(y),
y âˆˆRd,
(20.41)
using the convention

ln Î±
0 = +âˆ, ln 0
Î± = âˆ’âˆ, Î± > 0

and
ln 0
0 = 0,
(20.42)
where ln(Â·) denotes natural logarithm.
Using the log likelihood-ratio function LLR(Â·) and the monotonicity of the loga-
rithmic function

a > b

â‡â‡’

ln a > ln b

,
a, b > 0,
(20.43)
we can express the MAP rule (20.40) as
Ï†MAP(yobs) =
â§
âª
â¨
âª
â©
0
if LLR(yobs) > ln Ï€1
Ï€0 ,
1
if LLR(yobs) < ln Ï€1
Ï€0 ,
U

{0, 1}

if LLR(yobs) = ln Ï€1
Ï€0 ,

Ï€0, Ï€1 > 0,
fY(yobs) > 0

.
(20.44)
20.8
The ML Decision Rule
A diï¬€erent decision rule, which is typically suboptimal unless H is a priori uniform,
is the Maximum-Likelihood (ML) decision rule. Its structure is similar to that
of the MAP rule except that it ignores the prior. In fact, if Ï€0 = Ï€1, then the two
rules are identical. The ML rule is thus given by
Ï†ML(yobs) â‰œ
â§
âª
â¨
âª
â©
0
if fY|H=0(yobs) > fY|H=1(yobs),
1
if fY|H=0(yobs) < fY|H=1(yobs),
U

{0, 1}

if fY|H=0(yobs) = fY|H=1(yobs).
(20.45)
The ML decision rule can be alternatively described using the likelihood-ratio func-
tion LR(Â·) (20.38) as
Ï†ML(yobs) =
â§
âª
â¨
âª
â©
0
if LR(yobs) > 1,
1
if LR(yobs) < 1,
U

{0, 1}

if LR(yobs) = 1.
(20.46)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

408
Binary Hypothesis Testing
Alternatively, using the log likelihood-ratio function LLR(Â·) (20.41):
Ï†ML(yobs) =
â§
âª
â¨
âª
â©
0
if LLR(yobs) > 0,
1
if LLR(yobs) < 0,
U

{0, 1}

if LLR(yobs) = 0.
(20.47)
20.9
Performance Analysis: the Bhattacharyya Bound
We next derive the Bhattacharyya Bound, which is a useful upper bound on
the optimal probability of error pâˆ—(error).
Starting with the exact expression (20.20) we obtain:
pâˆ—(error) =

Rd min

Ï€0fY|H=0(y), Ï€1fY|H=1(y)

dy
â‰¤

Rd
3
Ï€0fY|H=0(y)Ï€1fY|H=1(y) dy
= âˆšÏ€0Ï€1

Rd
3
fY|H=0(y)fY|H=1(y) dy
â‰¤1
2

Rd
3
fY|H=0(y)fY|H=1(y) dy,
where the equality in the ï¬rst line follows from (20.20); the inequality in the second
line from the inequality
min{a, b} â‰¤
âˆš
ab,
a, b â‰¥0,
(20.48)
(which can be easily veriï¬ed by treating the case a â‰¥b and the case a < b sepa-
rately); the equality in the third line by trivial algebra; and where the inequality
in the fourth line follows by noting that if c, d â‰¥0, then their geometric mean
âˆš
cd
cannot exceed their arithmetic mean (c + d)/2, i.e.,
âˆš
cd â‰¤c + d
2
,
c, d â‰¥0,
(20.49)
and because in our case c = Ï€0 and d = Ï€1, so c + d = 1.
We have thus established the bound
pâˆ—(error) â‰¤1
2

yâˆˆRd
3
fY|H=0(y)fY|H=1(y) dy,
(20.50)
which is known as the Bhattacharyya Bound.
20.10
Example
Consider the problem of guessing H based on the observation Y , where H takes
on the values 0 and 1 equiprobably and where the conditional densities of Y given
H = 0 and H = 1 are
fY |H=0(y) =
1
âˆš
2Ï€Ïƒ2 eâˆ’(yâˆ’A)2/(2Ïƒ2),
y âˆˆR,
(20.51a)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.10 Example
409
fY |H=1(y) =
1
âˆš
2Ï€Ïƒ2 eâˆ’(y+A)2/(2Ïƒ2),
y âˆˆR
(20.51b)
for some deterministic A, Ïƒ > 0. Here the observable is a RV, so d = 1.
For these conditional densities the likelihood-ratio function (20.38) is given by:
LR(y) = fY |H=0(y)
fY |H=1(y)
=
1
âˆš
2Ï€Ïƒ2 eâˆ’(yâˆ’A)2/(2Ïƒ2)
1
âˆš
2Ï€Ïƒ2 eâˆ’(y+A)2/(2Ïƒ2)
= e4yA/(2Ïƒ2),
y âˆˆR.
Since the two hypotheses are a priori equally likely, the MAP rule is equivalent to
the ML rule and both rules guess â€œH = 0â€ or â€œH = 1â€ depending on whether the
likelihood-ratio LR(yobs) is greater or smaller than one. And since
LR(yobs) > 1 â‡â‡’e4yobsA/(2Ïƒ2) > 1
â‡â‡’ln

e4yobsA/(2Ïƒ2)
> ln 1
â‡â‡’4yobsA/(2Ïƒ2) > 0
â‡â‡’yobs > 0,
and
LR(yobs) < 1 â‡â‡’e4yobsA/(2Ïƒ2) < 1
â‡â‡’ln

e4yobsA/(2Ïƒ2)
< ln 1
â‡â‡’4yobsA/(2Ïƒ2) < 0
â‡â‡’yobs < 0,
it follows that the MAP decision rule guesses â€œH = 0,â€ if yobs > 0; guesses â€œH = 1,â€
if yobs < 0; and guesses â€œH = 0â€ or â€œH = 1â€ equiprobably, if yobs = 0 (i.e., in the
case of a tie).
Note that in this example the probability of a tie is zero.
Indeed, under both
hypotheses, the probability that the observed variable Y is exactly equal to zero is
zero:
Pr

Y = 0
 H = 0

= Pr

Y = 0
 H = 1

= Pr

Y = 0

= 0.
(20.52)
Consequently, the way ties are resolved is immaterial.
We next compute the probability of error of the MAP decoder. To this end, let
pMAP(error|H = 0) and pMAP(error|H = 1) denote its conditional probabilities of
error. Its (unconditional) probability of error, which is also the optimal probability
of error, can be expressed as
pâˆ—(error) = Ï€0 pMAP(error|H = 0) + Ï€1 pMAP(error|H = 1).
(20.53)
We proceed to compute the required terms on the RHS. Starting with the term
pMAP(error|H = 0), we note that pMAP(error|H = 0) corresponds to the condi-
tional probability that Y is negative or that Y is equal to zero and the coin toss
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

410
Binary Hypothesis Testing
that the MAP decoder uses to resolve the tie causes the guess to be â€œH = 1.â€ By
(20.52), the conditional probability of a tie is zero, so pMAP(error|H = 0) is, in
fact, just the conditional probability that Y is negative:
pMAP(error|H = 0) = Pr[Y < 0|H = 0]
= Q
A
Ïƒ

,
(20.54)
where the second equality follows because, conditional on H = 0, the random
variable Y is N

A, Ïƒ2
, and the probability that it is smaller than zero can be thus
computed using the Q-function; see (19.12b) and (19.13). Similarly, using (19.12a),
pMAP(error|H = 1) = Pr[Y > 0|H = 1]
= Q
A
Ïƒ

.
(20.55)
Note that in this example the MAP rule is â€œfairâ€ in the sense that the conditional
probability of error given H = 0 is the same as given H = 1. This is a coincidence
(that results from the symmetry in the problem). In general, the MAP rule need
not be fair.
We conclude from (20.53), (20.54), and (20.55) that
pâˆ—(error) = Q
A
Ïƒ

.
(20.56)
Figure 20.2 depicts the conditional densities of y given H = 0 and given H = 1
and the decision regions of the MAP decision rule Ï†MAP(Â·). The area of the shaded
region is the probability of an error conditioned on H = 0.
Note that the optimal decision rule for this example is not unique. Another optimal
decision rule is to guess â€œH = 0â€ if yobs is positive but not equal to 17, and to
guess â€œH = 1â€ otherwise.
Even though we have an exact expression for the probability of error (20.56) it is
instructive to compute the Bhattacharyya Bound too:
pâˆ—(error) â‰¤1
2
 âˆ
âˆ’âˆ
3
fY |H=0(y)fY |H=1(y) dy
= 1
2
 âˆ
âˆ’âˆ

1
âˆš
2Ï€Ïƒ2 eâˆ’(yâˆ’A)2/(2Ïƒ2)
1
âˆš
2Ï€Ïƒ2 eâˆ’(y+A)2/(2Ïƒ2) dy
= 1
2 eâˆ’A2/(2Ïƒ2)
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 eâˆ’y2/(2Ïƒ2) dy
= 1
2 eâˆ’A2/(2Ïƒ2),
(20.57)
where the ï¬rst line follows from (20.50); the second from (20.51); the third by simple
algebra; and the ï¬nal equality because the Gaussian density (like all densities)
integrates to one.
As an aside, we have from (20.57) and (20.56) the bound
Q(Î±) â‰¤1
2 eâˆ’Î±2/2,
Î± â‰¥0,
(20.58)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.11 (Nontelepathic) Processing
411
y
A
âˆ’A
Guess â€œH = 0â€
Guess â€œH = 1â€
fY |H=0(y)
fY |H=1(y)
fY (y)
pMAP(error|H = 0)
Figure 20.2: Binary hypothesis testing with a uniform prior. Conditional on H = 0
the observable Y is N

A, Ïƒ2
and conditional on H = 1 it is N

âˆ’A, Ïƒ2
. The area
of the shaded region is the probability of error of the MAP rule conditional on
H = 0.
which we encountered in Proposition 19.4.2.
20.11
(Nontelepathic) Processing
To further emphasize the optimality of the Maximum A Posteriori decision rule,
and for ulterior motives that have to do with the introduction of conditional inde-
pendence, we shall next show that no processing of the observables can reduce the
probability of a guessing error. To that end we shall have to properly deï¬ne what
we mean by â€œprocessing.â€
The ï¬rst thing that comes to mind is to consider processing as the application of
some deterministic mapping. I.e., we think of mapping the observation yobs using
some deterministic function g(Â·) to g(yobs) and then guessing H based on g(yobs).
That this cannot reduce the probability of error is clear from Figure 20.3, which
demonstrates that mapping yobs to g(yobs) and then guessing H based on g(yobs)
can be viewed as a special case of guessing H based on yobs and, as such, cannot
outperform the MAP decision rule, which is optimal among all decision rules based
on yobs.
A more general kind of processing involves randomization, or â€œdithering.â€ Here we
envision the processor as using a local random number generator to generate a ran-
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

412
Binary Hypothesis Testing
yobs
g(Â·)
g(yobs)
Guess
Guess H based
on g(yobs)
Figure 20.3: No decision rule based on g(yobs) can outperform an optimal decision
rule based on yobs, because computing g(yobs) and then forming the decision based
on the answer can be viewed as a special case of guessing based on yobs.
dom variable Î˜ and then producing an output of the form g(yobs, Î¸obs), where Î¸obs
is the outcome of Î˜, and where g(Â·) is some deterministic function. Here Î˜ is
assumed to be independent of the pair (H, Y), so the processor can generate it
using a local random number generator.
An argument very similar to the one we used in Section 20.6 (in the second proof of
the claim that randomized decision rules cannot outperform optimal deterministic
rules) can be used to show that this type of processing cannot improve our guessing.
The argument is as follows. We view the application of the function g(Â·) to the
pair (Y, Î˜) as deterministic processing of the pair (Y, Î˜), so no decision rule based
on g(Y, Î˜) can outperform a decision rule that is optimal for guessing H based
on (Y, Î˜).
It thus remains to show that the decision rule â€˜Guess â€œH = 0â€ if
Ï€0fY|H=0(yobs) â‰¥Ï€1fY|H=1(yobs)â€™ is also optimal when observing (Y, Î˜) and not
only Y. This follows from Theorem 20.5.2 by noting that the independence of Î˜
and (H, Y), implies that
fY,Î˜|H=0(yobs, Î¸obs) = fY|H=0(yobs) fÎ˜(Î¸obs),
fY,Î˜|H=1(yobs, Î¸obs) = fY|H=1(yobs) fÎ˜(Î¸obs),
and hence that this rule guesses â€œH = 0â€ only when yobs and Î¸obs are such that
Ï€0fY,Î˜|H=0(yobs, Î¸obs) â‰¥Ï€1fY,Î˜|H=1(yobs, Î¸obs) and guesses â€œH = 1â€ only when
Ï€1fY,Î˜|H=1(yobs, Î¸obs) â‰¥Ï€0fY,Î˜|H=0(yobs, Î¸obs).
Fearless readers who are not afraid to divide by zero should note that
LR(yobs, Î¸obs) = fY,Î˜|H=0(yobs, Î¸obs)
fY,Î˜|H=1(yobs, Î¸obs)
= fY|H=0(yobs) fÎ˜(Î¸obs)
fY|H=1(yobs) fÎ˜(Î¸obs)
= fY|H=0(yobs)
fY|H=1(yobs),
fÎ˜(Î¸obs) Ì¸= 0
= LR(yobs),
fÎ˜(Î¸obs) Ì¸= 0,
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.11 (Nontelepathic) Processing
413
so (ignoring some technical issues) the MAP detector based on (yobs, Î¸obs) ig-
nores Î¸obs and is identical to the MAP detector based on yobs only.4
Ostensibly more general is processing Y by mapping it to g(Y, Î˜), where the
distribution of Î˜ is allowed to depend on yobs. This motivates us to further extend
the notion of processing. The cleanest way to deï¬ne processing is to deï¬ne its
outcome rather than the way it is generated.
Before deï¬ning processing we remind the reader of the notion of conditional inde-
pendence. But ï¬rst we recall the deï¬nition of (unconditional) independence. We
do so for discrete random variables using their Probability Mass Function (PMF).
The extension to random variables with a joint density is straightforward. For the
deï¬nition of independence in more general scenarios see, for example, (Billingsley,
1995, Section 20) or (Lo`eve, 1963, Section 15) or (Williams, 1991, Chapter 4).
Deï¬nition 20.11.1 (Independent Discrete Random Variables). We say that the
discrete random variables X and Y of joint PMF PX,Y (Â·, Â·) and marginal PMFs
PX(Â·) and PY (Â·) are independent if PX,Y (Â·, Â·) factors as
PX,Y (x, y) = PX(x) PY (y).
(20.59)
Equivalently, X and Y are independent if, for every outcome y such that PY (y) > 0,
the conditional distribution of X given Y = y is the same as its unconditional
distribution:
PX|Y (x|y) = PX(x),
PY (y) > 0.
(20.60)
Equivalently, X and Y are independent if, for every outcome x such that PX(x) > 0,
the conditional distribution of Y given X = x is the same as its unconditional
distribution:
PY |X(y|x) = PY (y),
PX(x) > 0.
(20.61)
The equivalence of (20.59) and (20.60) follows because, by the deï¬nition of the
conditional probability mass function,
PX|Y (x|y) = PX,Y (x, y)
PY (y)
,
PY (y) > 0.
Similarly, the equivalence of (20.59) and (20.61) follows from
PY |X(y|x) = PX,Y (x, y)
PX(x)
,
PX(x) > 0.
The beauty of (20.59) is that it is symmetric in X, Y . It makes it clear that X
and Y are independent if, and only if, Y and X are independent. This is less
obvious from (20.60) or (20.61).
The deï¬nition of the conditional independence of X and Y given Z is similar, except
that we condition everywhere on Z. Again we only consider the discrete case and
refer the reader to (Lo`eve, 1963, Section 25.3) or (Chung, 2001, Section 9.2) or
(Kallenberg, 2002, Chapter 6) for the general case.
4Technical issues arise when the outcome of Î˜, namely Î¸obs, is such that fÎ˜(Î¸obs) = 0.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

414
Binary Hypothesis Testing
Deï¬nition 20.11.2 (Conditionally Independent Discrete Random Variables). Let
the discrete random variables X, Y, Z have a joint PMF PX,Y,Z(Â·, Â·, Â·). We say that
X and Y are conditionally independent given Z and write
XâŠ¸âˆ’
âˆ’ZâŠ¸âˆ’
âˆ’Y
if
PX,Y |Z(x, y|z) = PX|Z(x|z) PY |Z(y|z),
PZ(z) > 0.
(20.62)
Equivalently, X and Y are conditionally independent given Z if, for any outcome
y, z with PY,Z(y, z) > 0, the conditional distribution of X given that Y = y and
Z = z is the same as the distribution of X when conditioned on Z = z only:
PX|Y,Z(x|y, z) = PX|Z(x|z),
PY,Z(y, z) > 0.
(20.63)
Or, equivalently, X and Y are conditionally independent given Z if
PY |X,Z(y|x, z) = PY |Z(y|z),
PX,Z(x, z) > 0.
(20.64)
The equivalence of (20.62) and (20.63) follows because, by the deï¬nition of the
conditional probability mass function,
PX|Y,Z(x|y, z) = PX,Y,Z(x, y, z)
PY,Z(y, z)
= PX,Y |Z(x, y|z)PZ(z)
PY |Z(y|z)PZ(z)
= PX,Y |Z(x, y|z)
PY |Z(y|z)
,
PY,Z(y, z) > 0,
and similarly the equivalence of (20.62) and (20.64) follows from
PY |X,Z(y|x, z) = PX,Y |Z(x, y|z)
PX|Z(x|z)
,
PX,Z(x, z) > 0.
Again, the beauty of (20.62) is that it is symmetric in X, Y . Thus XâŠ¸âˆ’
âˆ’ZâŠ¸âˆ’
âˆ’Y if,
and only if, Y âŠ¸âˆ’
âˆ’ZâŠ¸âˆ’
âˆ’X. When X and Y are conditionally independent given Z
we sometimes say that XâŠ¸âˆ’
âˆ’ZâŠ¸âˆ’
âˆ’Y forms a Markov chain.
The equivalence between the diï¬€erent deï¬nitions of conditional independence con-
tinues to hold in the general case where the random variables are not necessarily
discrete. We only reluctantly state this as a theorem, because we never deï¬ned
conditional independence in nondiscrete settings.
Theorem 20.11.3 (Equivalent Deï¬nition for Conditional Independence). Let X,
Y, and Z be random vectors. Then the following statements are equivalent:
(a) X and Y are conditionally independent given Z.
(b) The conditional distribution of Y given (X, Z) is equal to its conditional
distribution given Z.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.11 (Nontelepathic) Processing
415
(c) The conditional distribution of X given (Z, Y) is equal to its conditional
distribution given Z.
Proof. For a precise deï¬nition of concepts appearing in this theorem and for a
proof of the equivalence between the statements see (Kallenberg, 2002, Chapter 6)
or (Lo`eve, 1963, Section 25.3) and particularly Theorem 25.3A therein.
We are now ready to deï¬ne the processing of the observation Y with respect to
the hypothesis H.
Deï¬nition 20.11.4 (Processing). We say that Z is the result of processing Y
with respect to H if H and Z are conditionally independent given Y.
As we next show, this deï¬nition of processing extends the previous ones. We ï¬rst
show that if Z = g(Y) for some deterministic Borel measurable function g(Â·) then
HâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’g(Y).
This follows by noting that, conditional on Y, the random
variable g(Y) is deterministic and hence independent of everything and a fortiori
of H.
We next show that if Î˜ is independent of (H, Y), then HâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’g(Y, Î˜). In-
deed, if Z = g(Y, Î˜) with Î˜ being independent of (Y, H), then, conditionally on
Y = y, the distribution of Z is simply the distribution of g(y, Î˜) so (under this
conditioning) Z is independent of H.
We next show that processing the observables cannot help decrease the probability
of error. The proof is conceptually very simple; the neat part is in the deï¬nition.
Theorem 20.11.5 (Processing Is Futile). If Z is the result of processing Y with
respect to H, then no rule for guessing H based on Z can outperform an optimal
guessing rule based on Y.
Proof. Surely no decision rule that guesses H based on Z can outperform an
optimal decision rule based on Z, let alone outperform a decision rule that is
optimal for guessing H based on Z and Y. But an optimal decision rule based on
the pair (Z, Y) is the MAP rule, which compares
Pr[H = 0|Y = y, Z = z]
and
Pr[H = 1|Y = y, Z = z].
And, because HâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’Z, it follows from Theorem 20.11.3 that this is equivalent
to comparing
Pr[H = 0|Y = y]
and
Pr[H = 1|Y = y]
i.e., to an optimal (MAP) decision rule based on Y only.
The above theorem is more powerful than it seems. To demonstrate its strength,
we next use it to show that in testing for a signal in Gaussian noiseâ€”irrespective of
the priorâ€”the optimal probability of error is monotonically nondecreasing in the
noise variance. The setup we consider is one where H is of prior (Ï€0, Ï€1) and aiding
us in guessing H is the observable Y , which, conditional on H = m, is N

Î±m, Ïƒ2
for m âˆˆ{0, 1}. We shall argue that, irrespective of the prior (Ï€0, Ï€1), the optimal
probability of error is monotonically nondecreasing in Ïƒ2.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

416
Binary Hypothesis Testing
The beauty of the argument is that it allows us to prove the monotonicity result
without having to calculate the optimal probability of error explicitly (as we did
in Section 20.10 for the case of a uniform prior with Î±0 = A and Î±1 = âˆ’A). While
we could also compute the optimal probability of error for this more general setup
and then use calculus to derive the monotonicity result, the argument we present
instead has the advantage of also being applicable to multi-dimensional multi-
hypothesis testing scenarios, where there is typically no closed-form expression for
the optimal probability of error.
To prove this result, let pâˆ—
e(Ïƒ2) denote the optimal probability of error as a function
of Ïƒ2. We need to show that pâˆ—
e(Ïƒ2) â‰¤pâˆ—
e(Ïƒ2 + Î´2), for all Î´ âˆˆR. Consider the
low-noise case where the conditional law of Y given H is N

Î±m, Ïƒ2
. Suppose that
the receiver generates W âˆ¼N

0, Î´2
independently of (H, Y ) and adds W to Y
to form Z = Y + W. Since Z is the result of processing Y with respect to H, it
follows that the optimal probability of error based on Y , namely pâˆ—
e(Ïƒ2), is at least
as good as the optimal probability of error based on Z (Theorem 20.11.5). We
now complete the argument by showing that the optimal probability of error based
on Z is pâˆ—
e(Ïƒ2 + Î´2). This follows because, by Proposition 19.7.2, the conditional
law of Z given H is N

Î±m, Ïƒ2 + Î´2
.
Stated diï¬€erently, since using a local random number generator the receiver can
produce from an observation Y of conditional law N

Î±m, Ïƒ2
a random variable Z
whose conditional law is N

Î±m, Ïƒ2 + Î´2
, the minimal probability of error based
on an observation having conditional law N

Î±m, Ïƒ2
cannot be larger than the
optimal probability of error achievable based on an observation having conditional
law N

Î±m, Ïƒ2 + Î´2
. See Figure 20.4 for an illustration of this argument.
20.12
Suï¬ƒcient Statistics
This section aï¬€ords a ï¬rst glance at the notion of suï¬ƒcient statistics, which will be
studied in greater depth and generality in Chapter 22. We begin with the following
example. Consider the hypothesis testing problem with a uniform prior, where the
observation is a tuple of real numbers (Y1, Y2). Conditional on H = 0, the random
variables Y1, Y2 are IID N

0, Ïƒ2
0

, whereas conditional on H = 1 they are IID
N

0, Ïƒ2
1

, where
Ïƒ0 > Ïƒ1 > 0.
(20.65)
(If Ïƒ2
0 = Ïƒ2
1, then the problem is boring in that the conditional law of the observable
given H = 0 is the same as given H = 1, so the two hypotheses cannot be diï¬€er-
entiated. For Ïƒ2
0 Ì¸= Ïƒ2
1 there is no loss in generality in assuming Ïƒ0 > Ïƒ1 because
we can always relabel the hypotheses. And if Ïƒ0 > Ïƒ1 = 0, then the problem is
trivial: we guess â€œH = 1â€ only if Y1 = Y2 = 0.) Thus, the observation space is the
two-dimensional Euclidean space R2 and, using the explicit form of the Gaussian
density (19.6),
fY1,Y2|H=0(y1, y2) =
1
2Ï€Ïƒ2
0
exp

âˆ’1
2Ïƒ2
0
(y2
1 + y2
2)

,
y1, y2 âˆˆR,
(20.66a)
fY1,Y2|H=1(y1, y2) =
1
2Ï€Ïƒ2
1
exp

âˆ’1
2Ïƒ2
1
(y2
1 + y2
2)

,
y1, y2 âˆˆR.
(20.66b)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.12 Suï¬ƒcient Statistics
417
yobs
yobs + W
MAP for testing
N

Î±0, Ïƒ2 + Î´2
vs. N

Î±1, Ïƒ2 + Î´2
with prior (Ï€0, Ï€1)
Gaussian RV
Generator
W âˆ¼N

0, Î´2
W independent of (Y, H).
+
Guess
Local
Randomness
Figure 20.4:
A suboptimal guessing rule (with randomization) for testing
N

Î±0, Ïƒ2
vs. N

Î±1, Ïƒ2
with the given prior (Ï€0, Ï€1).
It attains the optimal
probability of error for guessing N

Î±0, Ïƒ2 + Î´2
vs. N

Î±1, Ïƒ2 + Î´2
(with the given
prior).
Since we assumed a uniform prior, the ML decoding rule for guessing H based on
the tuple (Y1, Y2) is optimal. To derive the ML rule explicitly, we compute the
likelihood-ratio function
LR(y1, y2) = fY1,Y2|H=0(y1, y2)
fY1,Y2|H=1(y1, y2)
=
1
2Ï€Ïƒ2
0 exp

âˆ’
1
2Ïƒ2
0 (y2
1 + y2
2)

1
2Ï€Ïƒ2
1 exp

âˆ’
1
2Ïƒ2
1 (y2
1 + y2
2)

= Ïƒ2
1
Ïƒ2
0
exp
	1
2
 1
Ïƒ2
1
âˆ’1
Ïƒ2
0

(y2
1 + y2
2)

,
y1, y2 âˆˆR.
(20.67)
Thus,
LR(y1, y2) > 1 â‡â‡’exp
	1
2
 1
Ïƒ2
1
âˆ’1
Ïƒ2
0

(y2
1 + y2
2)

> Ïƒ2
0
Ïƒ2
1
â‡â‡’1
2
 1
Ïƒ2
1
âˆ’1
Ïƒ2
0

(y2
1 + y2
2) > ln Ïƒ2
0
Ïƒ2
1
â‡â‡’Ïƒ2
0 âˆ’Ïƒ2
1
2Ïƒ2
0Ïƒ2
1
(y2
1 + y2
2) > ln Ïƒ2
0
Ïƒ2
1
â‡â‡’y2
1 + y2
2 > 2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
ln Ïƒ2
0
Ïƒ2
1
,
(20.68)
where the second equivalence follows from the monotonicity of the logarithm func-
tion (20.43); and where the last equivalence follows by multiplying both sides of
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

418
Binary Hypothesis Testing
the inequality by the constant 2Ïƒ2
0Ïƒ2
1/(Ïƒ2
0 âˆ’Ïƒ2
1) (without the need to change the
inequality direction because this constant is by (20.65) positive).
It follows from (20.68) that the ML decision rule for guessing H based on (Y1, Y2)
computes Y 2
1 + Y 2
2 and then compares the result to a threshold. It is interesting to
note that to implement this decision rule one need not observe Y1 and Y2 directly;
it suï¬ƒces to observe the sum of their squares
T â‰œY 2
1 + Y 2
2 .
(20.69)
Of course, being the result of processing (Y1, Y2) with respect to H, no guess of H
based on T can outperform an optimal guess based on (Y1, Y2) (Section 20.11).
But what is interesting about this example is that, even though one cannot recover
(Y1, Y2) from T (so there are some decision rules based on (Y1, Y2) that cannot
be implemented if one only knows T), the ML rule based on (Y1, Y2) only requires
knowledge of T. Thus, in this example, even though pre-processing the observations
to produce T = Y 2
1 +Y 2
2 is not reversible, basing oneâ€™s decision on T incurs no loss
of optimality. An optimal decision rule based on T is just as good as an optimal
rule based on (Y1, Y2).
The reason for this can be traced to the fact that, in this example, to compute the
likelihood-ratio LR(y1, y2) one need not know the pair (y1, y2); it suï¬ƒces that one
know the sum of their squares y2
1 + y2
2; see (20.67). In this sense T = Y 2
1 + Y 2
2
forms a suï¬ƒcient statistic for guessing H from (Y1, Y2), as we next deï¬ne.
We would like to deï¬ne a mapping T(Â·) from the observation space Rd to Rdâ€² as
being suï¬ƒcient for the densities fY|H=0(Â·) and fY|H=1(Â·) if the likelihood-ratio
LR(yobs) can be computed from T(yobs) for every yobs in Rd. However, for techni-
cal reasons, we require slightly less: we only require that LR(yobs) be computable
from T(yobs) for those observations yobs for which at least one of the densities is
positive (so the likelihood-ratio is not of the form 0/0) and that additionally lie
outside some prespeciï¬ed set Y0 âŠ‚Rd of Lebesgue measure zero.5 Thus, we shall
require that there exist a set Y0 âŠ‚Rd of Lebesgue measure zero and a function
Î¶ : Rdâ€² â†’[0, âˆ] such that Î¶

T(yobs)

is equal to LR(yobs) whenever
yobs /âˆˆY0
and
fY|H=0(yobs) + fY|H=1(yobs) > 0.
(20.70)
Note that the fact that Y0 is of Lebesgue measure zero implies that
Pr[Y âˆˆY0 |H = 0] = Pr[Y âˆˆY0 |H = 1] = 0.
(20.71)
To convince the reader that this really is only â€œslightlyâ€ less, we note:
Note 20.12.1. Both conditional on H = 0 and conditional on H = 1, the proba-
bility that the observable violates (20.70) is zero.
5We allow this exception set so that the question of whether T(Â·) forms a suï¬ƒcient statistic
or not will not depend on our choice of the density function of the conditional distribution of the
observable. (Recall that if a RV has a probability density function, then it has inï¬nitely many
diï¬€erent probability density functions, every two of which diï¬€er on a set of Lebesgue measure
zero.)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.12 Suï¬ƒcient Statistics
419
Proof. We shall show that conditional on H = 0, the probability that the ob-
servable violates (20.70) is zero. The conditional probability given H = 1 can be
analogously shown to be zero. The condition that (20.70) is violated is equivalent
to the condition that either yobs âˆˆY0 or fY|H=0(yobs) + fY|H=1(yobs) = 0. By
(20.71), Pr[Y âˆˆY0 |H = 0] = 0. And, by the nonnegativity of the densities,
Pr

fY|H=0(Y) + fY|H=1(Y) = 0
 H = 0

â‰¤Pr

fY|H=0(Y) = 0
 H = 0

=

{ËœyâˆˆRd:fY|H=0(Ëœy)=0}
fY|H=0(y) dy
=

{ËœyâˆˆRd:fY|H=0(Ëœy)=0}
0 dy
= 0.
Conditionally on H = 0, the probability of the observable violating (20.70) is thus
the probability of the union of two events, each of which is of zero probability, and
is thus of zero probability; see Corollary 21.5.2 ahead.
Deï¬nition 20.12.2 (Suï¬ƒcient Statistic for Two Densities). We say that a map-
ping T : Rd â†’Rdâ€² forms a suï¬ƒcient statistic for the density functions fY|H=0(Â·)
and fY|H=1(Â·) on Rd if it is Borel measurable6 and if there exist a set Y0 âŠ‚Rd of
Lebesgue measure zero and a Borel measurable function Î¶ : Rdâ€² â†’[0, âˆ] such that
for all yobs âˆˆRd satisfying (20.70)
fY|H=0(yobs)
fY|H=1(yobs) = Î¶

T(yobs)

,
(20.72)
where on the LHS of (20.72) we deï¬ne a/0 to be +âˆwhenever a > 0.
In our example the observation (Y1, Y2) takes values in R2 so d = 2; the mapping
T : (y1, y2) 	â†’y2
1 + y2
2 is a mapping from R2 to R so dâ€² = 1; and by, (20.67),
Î¶ : t 	â†’Ïƒ2
1
Ïƒ2
0
exp
	1
2
 1
Ïƒ2
1
âˆ’1
Ïƒ2
0

t

.
Here we can take Y0 to be the empty set.7
We next show that if T(Â·) is a suï¬ƒcient statistic, then there is no loss of opti-
mality in considering decision rules that base their decision on T(Y). This result
6The technical condition that T(Â·) is Borel measurable guarantees that T(Y) is a random
vector. See for example (Billingsley, 1995, Theorem 13.1(ii)) for a discussion of this technical
issue.
The issue is best seen in the scalar case.
Suppose that Y is a RV deï¬ned over the
probability space (Î©, F, P). If T(Â·) is any function, then T(Y ) is a mapping from Î© to the R, but
we are not guaranteed that it be a RV, because for T(Y ) to be a RV we must have that, for every
Î¾ âˆˆR, the set {Ï‰ âˆˆÎ© : T(Y (Ï‰)) â‰¤Î¾} be in F, and this is, in general, not true. However, if T(Â·) is
Borel measurable, then the above cited theorem guarantees that T(X) is, indeed, a RV. Note that
any continuous function is Borel measurable (Billingsley, 1995, Theorem 13.2). In practice, one
never encounters functions that are not Borel measurable. In fact, it is hard work to construct
one.
7We would have needed to choose a nontrivial set Y0 if we had changed the densities (20.66)
at a ï¬nite (or countable) number of points.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

420
Binary Hypothesis Testing
is almost obvious, because the MAP decision rule is optimal (Theorem 20.7.1);
because it can be expressed in terms of the likelihood-ratio function (20.40); and
because the suï¬ƒciency of T(Â·) implies that the likelihood-ratio function LR(yobs)
is computable from T(yobs). Nevertheless, we provide a formal proof because the
result is important.
Proposition 20.12.3. If T : Rd â†’Rdâ€² is a suï¬ƒcient statistic for the densities
fY|H=0(Â·) and fY|H=1(Â·), then, irrespective of the prior of H, there exists a decision
rule that guesses H based on T(Y) and which is as good as any optimal guessing
rule based on Y.
Proof. We need to show that if Ï†âˆ—
Guess(Â·) is an optimal decision rule for guessing H
based on Y, then there exists a guessing rule based on T(Y) that has the same
probability of error. We note that it is enough to prove this result for a nondegen-
erate prior (20.2), because for degenerate priors one can achieve zero probability
of error even without looking at T(Y): if Pr[H = 0] = 1 guess â€œH = 0,â€ and if
Pr[H = 1] = 1 guess â€œH = 1.â€ We thus proceed to assume a nondegenerate prior
(20.2).
Let Ï†MAP(Â·) be the MAP rule for guessing H based on Y. Since this rule is optimal,
it suï¬ƒces to exhibit a decoding rule Ï†T (Â·) based on T(Y) of equal performance.
Since T(Â·) is suï¬ƒcient, it follows that there exists a set of Lebesgue measure zero Y0
and a Borel measurable function Î¶(Â·) such that Î¶

T(yobs)

= LR(yobs), whenever
(20.70) holds. Based upon the observation T(Y) = T(yobs), the desired rule is to
guess
Ï†T

T(yobs)

=
â§
âª
â¨
âª
â©
0
if Î¶

T(yobs)

> Ï€1
Ï€0 ,
1
if Î¶

T(yobs)

< Ï€1
Ï€0 ,
U

{0, 1}

if Î¶

T(yobs)

= Ï€1
Ï€0 .
(20.73)
That Ï†T (Â·) has the same performance as Ï†MAP(Â·) now follows by noting that,
by (20.72), the two decoding rules are in agreement except perhaps for observa-
tions yobs violating (20.70), but those, by Note 20.12.1, occur with probability zero.
The performance of Ï†MAP(Â·) (which is optimal based on Y) and of Ï†T (Â·) (which is
based on T(Y)) are thus identical.
Deï¬nition 20.12.2 is intuitive in that it demonstrates how one typically goes about
identifying a suï¬ƒcient statistic: one computes the likelihood-ratio and checks what
it depends on.
This deï¬nition, however, becomes a bit cumbersome in multi-
hypothesis testing, which we shall discuss in Chapter 21. A deï¬nition that is more
appropriate for that setting is given in Chapter 22 in terms of the computability
of the a posteriori probabilities from T(yobs) (Deï¬nition 22.2.1). The purpose of
the next proposition is to show that the two deï¬nitions coincide in the binary case:
ignoring sets of Lebesgue measure zero, the likelihood-ratio can be computed from
T(yobs) (whenever the ratio is not 0/0), if, and only if, for any prior (Ï€0, Ï€1) one can
compute the a posteriori distribution of H from T(yobs) (whenever fY(yobs) > 0).
We draw the readerâ€™s attention to the following subtle issue. Deï¬nition 20.12.2
makes it clear that the suï¬ƒciency of T(Â·) has nothing to do with the prior; it only
depends on the densities fY|H=0(Â·) and fY|H=1(Â·). The equivalent deï¬nition of
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.12 Suï¬ƒcient Statistics
421
suï¬ƒcient statistics in terms of the computability of the a posteriori distribution
ostensibly depends also on the prior, because it is only meaningful to discuss the a
posteriori distribution if H has a prior. Nevertheless, the deï¬nitions are equivalent
because in the latter deï¬nition we require that the a posteriori distribution be
computable from T(Y) for every prior, and not just for the prior given in the
problemâ€™s formulation.
Proposition 20.12.4 (Computability of the a Posteriori Distribution). Let the
mapping T : Rd â†’Rdâ€² be Borel measurable, and let fY|H=0(Â·) and fY|H=1(Â·) be
densities on Rd. Then the following two conditions are equivalent:
(a) T(Â·) forms a suï¬ƒcient statistic for the densities fY|H=0(Â·) and fY|H=1(Â·).
(b) For some set Y0 âŠ‚Rd of Lebesgue measure zero we have that for every prior
(Ï€0, Ï€1) there exist Borel measurable functions from Rdâ€² to [0, 1]
t 	â†’Ïˆm

Ï€0, Ï€1, t

,
m = 0, 1,
such that the vector

Ïˆ0

Ï€0, Ï€1, T(yobs)

, Ïˆ1

Ï€0, Ï€1, T(yobs)
T
is a probability vector, and this probability vector is equal to the vector

Pr[H = 0|Y = yobs], Pr[H = 1|Y = yobs]
T
,
(20.74)
whenever both the condition yobs /âˆˆY0, and the condition
Ï€0fY|H=0(yobs) + Ï€1fY|H=1(yobs) > 0
(20.75)
are satisï¬ed. Here (20.74) is computed for H having the prior (Ï€0, Ï€1) and
for the conditional densities fY|H=0(Â·) and fY|H=1(Â·).
Proof. We begin by proving that (a) implies (b). That is, we assume that T(Â·)
forms a suï¬ƒcient statistic and proceed to prove the existence of the set Y0 and
of the functions Ïˆ0(Â·), Ïˆ1(Â·). Let Y0 and Î¶ : Rdâ€² â†’[0, âˆ] be as guaranteed by the
deï¬nition of suï¬ƒcient statistics (Deï¬nition 20.12.2) so
fY|H=0(yobs)
fY|H=1(yobs) = Î¶

T(yobs)

,
(20.76)
whenever yobs satisï¬es (20.70).
We next show how to construct for every pair
(Ï€0, Ï€1) the functions Ïˆ0(Â·), Ïˆ1(Â·). We consider three cases separately: the case
Ï€0 = 1 âˆ’Ï€1 = 1, the case Ï€0 = 1 âˆ’Ï€1 = 0, and the case where both Ï€0 and Ï€1 are
strictly positive.
In the ï¬rst case H is deterministically zero, and the functions Ïˆ0(1, 0, t) = 1 and
Ïˆ1(1, 0, t) = 0 meet our requirements. In the second case H is deterministically
one, and the functions Ïˆ0(0, 1, t) = 1 âˆ’Ïˆ1(0, 1, t) = 0 meet our requirements.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

422
Binary Hypothesis Testing
It remains to treat the case where Ï€0, Ï€1 > 0. We shall show that in this case the
functions
Ïˆ0

Ï€0, Ï€1, t

â‰œ
Ï€0 Î¶(t)
Ï€0 Î¶(t) + Ï€1
,
Ïˆ1(Ï€0, Ï€1, t) â‰œ1 âˆ’Ïˆ0

Ï€0, Ï€1, t

,
(20.77)
(where âˆ/(âˆ+ a) is deï¬ned as one for all ï¬nite a) meet our requirements. To
that end we ï¬rst note that Ïˆ0(Ï€0, Ï€1, t) and Ïˆ1(Ï€0, Ï€1, t) are nonnegative and sum
to one.
We next note that, for Ï€0, Ï€1 > 0, the condition (20.75) implies that
fY|H=0(yobs) and fY|H=1(yobs) are not both zero. Consequently, if yobs satisï¬es
(20.75) and also yobs /âˆˆY0, then it satisï¬es (20.70) and LR(yobs) = Î¶

T(yobs)

.
Thus, in the case Ï€0, Ï€1 > 0, we have that, whenever (20.75) and yobs /âˆˆY0 hold,
we have from (20.77)
Ïˆ0

Ï€0, Ï€1, T(yobs)

=
Ï€0 Î¶

T(yobs)

Ï€0 Î¶

T(yobs)

+ Ï€1
=
Ï€0 LR(yobs)
Ï€0 LR(yobs) + Ï€1
=
Ï€0fY|H=0(yobs)/fY|H=1(yobs)
Ï€0fY|H=0(yobs)/fY|H=1(yobs) + Ï€1
=
Ï€0fY|H=0(yobs)
Ï€0fY|H=0(yobs) + Ï€1fY|H=1(yobs)
= Pr[H = 0|Y = yobs]
as required. This implies that, whenever (20.75) and yobs /âˆˆY0 hold, we also have
Ïˆ1

Ï€0, Ï€1, T(yobs)

= Pr[H = 1|Y = yobs], since Ïˆ1(Ï€0, Ï€1, t) = 1 âˆ’Ïˆ0

Ï€0, Ï€1, t

and since Pr[H = 1|Y = yobs] = 1 âˆ’Pr[H = 0|Y = yobs]; see (20.77) and (20.10).
We now prove that (b) implies (a), i.e., that the existence of the set Y0 and of the
functions Ïˆ0(Â·), Ïˆ1(Â·) imply the existence of the function Î¶(Â·) of Deï¬nition 20.12.2.
In fact, we shall prove a stronger statement that if for some nondegenerate prior
the a posteriori distribution of H given Y = yobs is computable from T(yobs)
(whenever (20.75) and yobs /âˆˆY0 hold), then there exists a function Î¶ : Rdâ€² â†’[0, âˆ]
such that LR(yobs) = Î¶(T(yobs)), whenever yobs satisï¬es (20.70).
To construct Î¶(Â·) from Ïˆ0(Â·) and Ïˆ1(Â·), pick some arbitrary strictly positive ËœÏ€0, ËœÏ€1
summing to one (e.g., ËœÏ€0, ËœÏ€1 = 1/2), and deï¬ne Î¶(Â·) by
Î¶

T(yobs)

= ËœÏ€1 Ïˆ0

ËœÏ€0, ËœÏ€1, T(yobs)

ËœÏ€0 Ïˆ1

ËœÏ€0, ËœÏ€1, T(yobs)
,
(20.78)
using the convention that a/0 = âˆfor all a > 0; see (20.39).
We next verify that if yobs satisï¬es (20.70) then Î¶(T(yobs)) = LR(yobs).
To
this end, deï¬ne H to have the law Pr[H = 0] = ËœÏ€0 and Pr[H = 1] = ËœÏ€1,
and let the conditional law of Y given H be as speciï¬ed by the given densities.
Since ËœÏ€0 and ËœÏ€1 are strictly positive, it follows that whenever fY|H=0(yobs) and
fY|H=1(yobs) are not both zero, we also have ËœÏ€0fY|H=0(yobs)+ËœÏ€1fY|H=1(yobs) > 0.
Consequently, for strictly positive ËœÏ€0, ËœÏ€1 we have that (20.70) implies that yobs /âˆˆY0
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.12 Suï¬ƒcient Statistics
423
and ËœÏ€0fY|H=0(yobs)+ËœÏ€1fY|H=1(yobs) > 0 and thus, for observations yobs satisfying
(20.70),
Î¶

T(yobs)

= ËœÏ€1 Ïˆ0

ËœÏ€0, ËœÏ€1, T(yobs)

ËœÏ€0 Ïˆ1

ËœÏ€0, ËœÏ€1, T(yobs)

= Pr[H = 1] Pr[H = 0|Y = yobs]
Pr[H = 0] Pr[H = 1|Y = yobs]
= LR(yobs),
where the last equality follows by dividing the equation
Pr[H = 0|Y = yobs] =
Pr[H = 0] fY|H=0(yobs)
Pr[H = 0] fY|H=0(yobs) + Pr[H = 1] fY|H=1(yobs)
(which is a restatement of (20.9a) for our case) by
Pr[H = 1|Y = yobs] =
Pr[H = 1] fY|H=1(yobs)
Pr[H = 0] fY|H=0(yobs) + Pr[H = 1] fY|H=1(yobs)
(which is a restatement of (20.9b) for our case).
Once we have identiï¬ed a suï¬ƒcient statistic T(Y), we can proceed to derive an
optimal guessing rule using two methods that we describe next. Again, we focus
on nondegenerate priors.
Method 1:
We ignore the fact that T(Y) forms a suï¬ƒcient statistic and simply
use the MAP rule (20.40):
Ï†MAP(yobs) =
â§
âª
â¨
âª
â©
0
if LR(yobs) > Ï€1
Ï€0 ,
1
if LR(yobs) < Ï€1
Ï€0 ,
U

{0, 1}

if LR(yobs) = Ï€1
Ï€0 .
(20.79)
(Because T(Y) is a suï¬ƒcient statistic, the likelihood-ratio function LR(yobs) will
be computable from T(yobs) whenever LR(yobs) does not have the pathological
form 0/0 and does not lie in the exception set Y0. Such pathological observations
occur with probability zero (20.12), so we need not worry about them.)
Method 2:
By Proposition 20.12.3, there is no loss of optimality in forming our
guess based on T(Y). So we can use any optimal rule, e.g., the MAP rule, for
guessing H based on the new dâ€²-dimensional observations tobs = T(yobs). This
method requires computing the conditional distribution of the random dâ€²-vector
T = T(Y) conditional on H = 0 and conditional on H = 1 and deciding according
to the rule:
Ï†Guess(T(yobs)) =
â§
âª
â¨
âª
â©
0
if Ï€0 fT|H=0

T(yobs)

> Ï€1 fT|H=1

T(yobs)

,
1
if Ï€0 fT|H=0

T(yobs)

< Ï€1 fT|H=1

T(yobs)

,
U

{0, 1}

if Ï€0 fT|H=0

T(yobs)

= Ï€1 fT|H=1

T(yobs)

.
(20.80)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

424
Binary Hypothesis Testing
Why would we want to use Method 2 when we have already computed the likelihood-
ratio function to establish the suï¬ƒciency of the statistic? The answer is that some-
times one can demonstrate that T(Y) forms a suï¬ƒcient statistic by methods that
are not based on the computation of the likelihood-ratio. In such cases, Method 2
may be advantageous. Also, sometimes the analysis of the probability of error in
Method 2 is easier. The choice is ours.
Returning to the example of (20.66), we demonstrate Method 2 by calculating
the law of the suï¬ƒcient statistic T = Y 2
1 + Y 2
2 under each of the hypotheses.
Recalling that the sum of the squares of two IID zero-mean Gaussians is exponential
(Note 19.8.1) we obtain:
fT |H=0(t) =
1
2Ïƒ2
0
exp

âˆ’t
2Ïƒ2
0

,
t â‰¥0,
(20.81a)
fT |H=1(t) =
1
2Ïƒ2
1
exp

âˆ’t
2Ïƒ2
1

,
t â‰¥0.
(20.81b)
Consequently, the likelihood-ratio is given by
fT |H=0(t)
fT |H=1(t) = Ïƒ2
1
Ïƒ2
0
exp
	
t
 1
2Ïƒ2
1
âˆ’
1
2Ïƒ2
0

,
t â‰¥0,
and the log likelihood-ratio by
ln fT |H=0(t)
fT |H=1(t) = ln Ïƒ2
1
Ïƒ2
0
+ t
 1
2Ïƒ2
1
âˆ’
1
2Ïƒ2
0

,
t â‰¥0.
We thus guess â€œH = 0â€ if the log likelihood-ratio is positive,
t â‰¥2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
ln Ïƒ2
0
Ïƒ2
1
,
i.e., if
y2
1 + y2
2 â‰¥2Ïƒ2
0Ïƒ2
1
Ïƒ2
0 âˆ’Ïƒ2
1
ln Ïƒ2
0
Ïƒ2
1
.
We similarly guess â€œH = 1â€ if the log likelihood-ratio is negative, and ï¬‚ip a coin if
it is zero. This is the same law we obtained in (20.68) based on Method 1.
20.13
Implications of Optimality
Consider the problem of guessing an a priori uniformly distributed binary ran-
dom variable H based on the observable Y whose conditional law given H = 0
is N

0, Ïƒ2
and whose conditional distribution given H = 1 is N

1, Ïƒ2
. To de-
rive an optimal guessing rule we could derive the MAP rule by computing the
likelihood-ratio function as we did in Section 20.10. But having already carried
out the calculations in Section 20.10 for testing whether an observation was drawn
N

A, Ïƒ2
or N

âˆ’A, Ïƒ2
, there is a better way. Let
T = Y âˆ’1
2.
(20.82)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.14 Multi-Dimensional Binary Gaussian Hypothesis Testing
425
Because there is a one-to-one relationship between Y and T, there is no loss of
optimality in subtracting 1/2 from Y to obtain T and in then applying an optimal
decision rule to T. Indeed, since Y = T + 1/2, it follows that Y is the result of
processing T with respect to H, so no decision rule based on Y can outperform an
optimal decision rule based on T (Theorem 20.11.5). (Of course, no decision rule
based on T can outperform an optimal one based on Y , because T is the result of
processing Y with respect to H.) In fact, using the terminology of Section 20.12,
T : y 	â†’y âˆ’1/2 forms a suï¬ƒcient statistic for guessing H based on Y , because the
likelihood-ratio function LR(yobs) = fY |H=0(yobs)/fY |H=1(yobs) can be expressed
as Î¶

T(yobs)

for the mapping Î¶ : t 	â†’LR(t + 1/2). Consequently, our assertion
that there is no loss of optimality in forming our guess based on T(Y ) is just a
consequence of Proposition 20.12.3.
Conditional on H = 0, the random variable T(Y ) is N

âˆ’0.5, Ïƒ2
, and, conditional
on H = 1, it is N

+0.5, Ïƒ2
. Consequently, using the results of Section 20.10 (with
the substitution of 1/2 for A), we obtain that an optimal rule based on T is to guess
â€œH = 0â€ if T is negative, and to guess â€œH = 1â€ if T is positive. To summarize, the
decision rule we derived is to guess â€œH = 0â€ if Y âˆ’1/2 < 0 and to guess â€œH = 1â€
if Y âˆ’1/2 > 0.
In the terminology of Section 20.12, we used the fact that the transformation in
(20.82) is one-to-one to conclude that T(Â·) forms a suï¬ƒcient statistic, and we then
used Method 2 from that section to derive an optimal decision rule.
20.14
Multi-Dimensional Binary Gaussian Hypothesis Testing
We now come closer to the receiver front end.
The kind of problem we would
eventually like to address is the hypothesis testing problem in which, conditional
on H = 0, the observable is a continuous-time waveform of the form s0(t) + N(t)
whereas, conditional on H = 1, it is of the form s1(t) + N(t), where

N(t), t âˆˆR

is some continuous-time stochastic process modeling the noise. This problem will
be addressed in Chapter 26. For now we only address the discrete-time version of
this problem.
20.14.1
The Setup
We consider the problem of guessing the random variable H that takes on the
values 0 and 1 with positive probabilities Ï€0 and Ï€1. The observable Y âˆˆRJ is
a random vector with J components Y (1), . . . , Y (J).8 Conditional on H = 0, the
components of Y are independent Gaussians with Y (j) âˆ¼N

s(j)
0 , Ïƒ2
, where s0 is
some deterministic vector of J components s(1)
0 , . . . , s(J)
0 , and where Ïƒ2 > 0. Con-
ditional on H = 1, the components of Y are independent with Y (j) âˆ¼N

s(j)
1 , Ïƒ2
,
for some other deterministic vector s1 of J components s(1)
1 , . . . , s(J)
1 . We assume
that s0 and s1 diï¬€er in at least one coordinate. The setup can be described as
H = 0: Y (j) = s(j)
0
+ Z(j),
j = 1, 2, . . . , J,
8We use J rather than d in order to comply with the notation of Section 21.6 ahead.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

426
Binary Hypothesis Testing
H = 1: Y (j) = s(j)
1
+ Z(j),
j = 1, 2, . . . , J,
where Z(1), Z(2), . . . , Z(J) are IID N

0, Ïƒ2
.
For typographical reasons, instead of denoting the observed vector by yobs, we now
denote it by y and its J components by y(1), . . . , y(J).
20.14.2
An Optimal Decision Rule
To ï¬nd an optimal guessing rule we compute the likelihood-ratio function:
LR(y) = fY|H=0(y)
fY|H=1(y)
=
?J
j=1
-
1
âˆš
2Ï€Ïƒ2 exp
	
âˆ’

y(j)âˆ’s(j)
0
2
2Ïƒ2

.
?J
j=1
-
1
âˆš
2Ï€Ïƒ2 exp
	
âˆ’

y(j)âˆ’s(j)
1
2
2Ïƒ2

.
=
J
@
j=1
-
exp
	
âˆ’

y(j) âˆ’s(j)
0
2
2Ïƒ2
+

y(j) âˆ’s(j)
1
2
2Ïƒ2

.
,
y âˆˆRJ.
The log likelihood-ratio function is thus given by
LLR(y) = ln LR(y)
=
1
2Ïƒ2
J

j=1

y(j) âˆ’s(j)
1
2 âˆ’

y(j) âˆ’s(j)
0
2
= 1
Ïƒ2
	
âŸ¨y, s0 âˆ’s1âŸ©E + âˆ¥s1âˆ¥2 âˆ’âˆ¥s0âˆ¥2
2

= 1
Ïƒ2
	
âŸ¨y, s0 âˆ’s1âŸ©E âˆ’âŸ¨s0, s0 âˆ’s1âŸ©E + âŸ¨s1, s0 âˆ’s1âŸ©E
2

= âˆ¥s0 âˆ’s1âˆ¥
Ïƒ2
â›
â

y, s0 âˆ’s1
âˆ¥s0 âˆ’s1âˆ¥

E
âˆ’
#
s0,
s0âˆ’s1
âˆ¥s0âˆ’s1âˆ¥
$
E +
#
s1,
s0âˆ’s1
âˆ¥s0âˆ’s1âˆ¥
$
E
2
â
â 
= âˆ¥s0 âˆ’s1âˆ¥
Ïƒ2

âŸ¨y, Ï†âŸ©E âˆ’1
2

âŸ¨s0, Ï†âŸ©E + âŸ¨s1, Ï†âŸ©E

,
y âˆˆRJ,
(20.83)
where for real vectors u = (u(1), . . . , u(J))T and v = (v(1), . . . , v(J))T taking values
in RJ we deï¬ne9
âŸ¨u, vâŸ©E â‰œ
J

j=1
u(j)v(j),
(20.84)
âˆ¥uâˆ¥â‰œ
3
âŸ¨u, uâŸ©E =
A
B
B
C
J

j=1

u(j)2,
(20.85)
9This is sometimes called the standard inner product on RJ or the inner product between
J-tuples. The subscript â€œEâ€ stands here for â€œEuclidean.â€
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.14 Multi-Dimensional Binary Gaussian Hypothesis Testing
427
s0
s0
s0
s1
s1
s1
Ï†
Ï†
Ï†
guess 0
guess 1
guess 0
guess 1
guess 0
guess 1
Ï€0 < Ï€1
Ï€0 = Ï€1
Ï€0 > Ï€1
Figure 20.5: Eï¬€ect of the ratio Ï€0/Ï€1 on the decision rule.
and where
Ï† =
s0 âˆ’s1
âˆ¥s0 âˆ’s1âˆ¥
(20.86)
is a unit-norm vector pointing from s1 to s0.
An optimal decision rule is to guess â€œH = 0â€ when LLR(y) â‰¥ln Ï€1
Ï€0 , i.e.,
Guess â€œH = 0â€ if
âŸ¨y, Ï†âŸ©E â‰¥âŸ¨s0, Ï†âŸ©E + âŸ¨s1, Ï†âŸ©E
2
+
Ïƒ2
âˆ¥s0 âˆ’s1âˆ¥ln Ï€1
Ï€0
,
(20.87)
and to guess â€œH = 1â€ otherwise. This decision rule is illustrated in Figure 20.5.
Depicted are the cases where Ï€1/Ï€0 is smaller than one, equal to one, and larger
than one.
It is interesting to note that the projection âŸ¨y, Ï†âŸ©E Ï† of y onto the normalized
vector Ï† = (s0 âˆ’s1)/ âˆ¥s0 âˆ’s1âˆ¥forms a suï¬ƒcient statistic for this problem. Indeed,
by (20.83), the log likelihood-ratio (and hence the likelihood-ratio) function is
computable from âŸ¨y, Ï†âŸ©E. The projection is depicted in Figure 20.6.
The rule (20.87) simpliï¬es if H has a uniform prior. In this case the rule is
Guess â€œH = 0â€ if
âŸ¨y, Ï†âŸ©E â‰¥âŸ¨s0, Ï†âŸ©E + âŸ¨s1, Ï†âŸ©E
2
.
(20.88)
Note that in this case the guessing rule can be implemented even if Ïƒ2 is unknown.
20.14.3
Error Probability Analysis
We next ï¬nd the error probability associated with our guessing rule. Denote the
conditional error probabilities associated with our guessing rule by p(error|H = 0)
and p(error|H = 1). Since our rule is optimal, its unconditional probability of error
is pâˆ—(error), and thus
pâˆ—(error) = Ï€0 p(error|H = 0) + Ï€1 p(error|H = 1).
(20.89)
Because in (20.87) we resolved ties by guessing â€œH = 0â€, it follows that to evaluate
p(error|H = 0) we need to evaluate the probability that a random vector Y drawn
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

428
Binary Hypothesis Testing
Ï†
Ï†
s1
s0
y
Figure 20.6: The projection of y onto the normalized vector Ï† = (s0âˆ’s1)/âˆ¥s0âˆ’s1âˆ¥.
according to the density fY|H=0(Â·) is such that the a posteriori probability of H = 0
is strictly smaller than the a posteriori probability of H = 1. Thus, if ties in the a
posteriori distribution of H are resolved in favor of guessing â€œH = 0â€, then
p(error|H = 0) = Pr

Ï€0fY|H=0(Y) < Ï€1fY|H=1(Y)
 H = 0

.
(20.90)
This may seem self-referential, but it is not. Another way to state this is
p(error|H = 0) =

y/âˆˆB1,0
fY|H=0(y) dy,
(20.91)
where
B1,0 =
'
y âˆˆRJ : Ï€0fY|H=0(y) â‰¥Ï€1fY|H=1(y)
(
.
(20.92)
To compute this probability we need the following lemma:
Lemma 20.14.1. Let Ï€0 and Ï€1 be strictly positive but not necessarily sum to one.
Let the vectors s0, s1 âˆˆRJ diï¬€er in at least one component, i.e., âˆ¥s0 âˆ’s1âˆ¥> 0. Let
f0(y) =
	
1
âˆš
2Ï€Ïƒ2

J
exp
	
âˆ’1
2Ïƒ2
J

j=1

y(j) âˆ’s(j)
0
2

,
y âˆˆRJ,
f1(y) =
	
1
âˆš
2Ï€Ïƒ2

J
exp
	
âˆ’1
2Ïƒ2
J

j=1

y(j) âˆ’s(j)
1
2

,
y âˆˆRJ,
where Ïƒ2 > 0. Deï¬ne
B1,0 â‰œ
'
y âˆˆRJ : Ï€0f0(y) â‰¥Ï€1f1(y)
(
.
Then

y/âˆˆB1,0
f0(y) dy = Q
	âˆ¥s0 âˆ’s1âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥s0 âˆ’s1âˆ¥ln Ï€0
Ï€1

.
(20.93)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.14 Multi-Dimensional Binary Gaussian Hypothesis Testing
429
This equality continues to hold if we replace the weak inequality (â‰¥) in the deï¬nition
of B1,0 with a strict inequality (>).
Proof. Using a calculation identical to the one leading to (20.83) we obtain that
the set B1,0 can also be expressed as
B1,0 =
1
y âˆˆRJ : âŸ¨y, Ï†âŸ©E â‰¥âŸ¨s0, Ï†âŸ©E + âŸ¨s1, Ï†âŸ©E
2
+
Ïƒ2
âˆ¥s0 âˆ’s1âˆ¥ln Ï€1
Ï€0
2
,
(20.94)
where Ï† is deï¬ned in (20.86).
The density f0(Â·) is the same as the density of the vector s0 + Z, where the com-
ponents Z(1), . . . , Z(J) of Z are IID N

0, Ïƒ2
. Thus, the LHS of (20.93) can be
expressed as

y/âˆˆB1,0
f0(y) dy = Pr

âŸ¨s0 + Z, Ï†âŸ©E < âŸ¨s0, Ï†âŸ©E + âŸ¨s1, Ï†âŸ©E
2
+
Ïƒ2
âˆ¥s0 âˆ’s1âˆ¥ln Ï€1
Ï€0

= Pr

âŸ¨Z, Ï†âŸ©E < âŸ¨s1, Ï†âŸ©E âˆ’âŸ¨s0, Ï†âŸ©E
2
+
Ïƒ2
âˆ¥s0 âˆ’s1âˆ¥ln Ï€1
Ï€0

= Pr

âˆ’âŸ¨Z, Ï†âŸ©E > âŸ¨s0, Ï†âŸ©E âˆ’âŸ¨s1, Ï†âŸ©E
2
+
Ïƒ2
âˆ¥s0 âˆ’s1âˆ¥ln Ï€0
Ï€1

= Pr

âˆ’âŸ¨Z, Ï†âŸ©E > âŸ¨s0 âˆ’s1, Ï†âŸ©E
2
+
Ïƒ2
âˆ¥s0 âˆ’s1âˆ¥ln Ï€0
Ï€1

= Pr

âŸ¨Z, âˆ’Ï†âŸ©E > âˆ¥s0 âˆ’s1âˆ¥
2
+
Ïƒ2
âˆ¥s0 âˆ’s1âˆ¥ln Ï€0
Ï€1

= Q
	âˆ¥s0 âˆ’s1âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥s0 âˆ’s1âˆ¥ln Ï€0
Ï€1

,
where the ï¬rst equality follows from (20.94) and from the observation that the
density f0(Â·) is the density of s0 + Z; the second because âŸ¨Â·, Â·âŸ©E is linear in the ï¬rst
argument, so âŸ¨s0 + Z, Ï†âŸ©E = âŸ¨s0, Ï†âŸ©E+âŸ¨Z, Ï†âŸ©E; the third by noting that multiplying
both sides of an inequality by (âˆ’1) requires changing the direction of the inequality;
the fourth by the linear relationship âŸ¨s1, Ï†âŸ©Eâˆ’âŸ¨s0, Ï†âŸ©E = âŸ¨s1 âˆ’s0, Ï†âŸ©E; the ï¬fth by
(20.86); and the ï¬nal equality because, as we next argue, âŸ¨Z, âˆ’Ï†âŸ©E âˆ¼N

0, Ïƒ2
, so
we can employ (19.12a). To see that âŸ¨Z, âˆ’Ï†âŸ©E âˆ¼N

0, Ïƒ2
, note that, by (20.86),
âˆ¥âˆ’Ï†âˆ¥= 1 and then employ Proposition 19.7.3.
This establishes the ï¬rst part of the lemma. The result where the weak inequality
is replaced with a strict inequality follows by replacing all the weak inequalities
in the proof with the corresponding strict inequalities and vice versa. (If X has a
density, then Pr[X < Î¾] = Pr[X â‰¤Î¾].)
By applying Lemma 20.14.1 to our problem we obtain
p(error|H = 0) = Q
	âˆ¥s0 âˆ’s1âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥s0 âˆ’s1âˆ¥ln Ï€0
Ï€1

.
(20.95)
Similarly, one can show that
p(error|H = 1) = Q
	âˆ¥s0 âˆ’s1âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥s0 âˆ’s1âˆ¥ln Ï€1
Ï€0

.
(20.96)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

430
Binary Hypothesis Testing
Consequently, by (20.89)
pâˆ—(error) = Ï€0 Q
	âˆ¥s0 âˆ’s1âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥s0 âˆ’s1âˆ¥ln Ï€0
Ï€1

+ Ï€1 Q
	âˆ¥s0 âˆ’s1âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥s0 âˆ’s1âˆ¥ln Ï€1
Ï€0

.
(20.97)
In the special case where the prior is uniform we obtain from (20.95), (20.96), and
(20.97)
pâˆ—(error) = p(error|H = 0) = p(error|H = 1) = Q
	âˆ¥s0 âˆ’s1âˆ¥
2Ïƒ

.
(20.98)
This has a nice geometric interpretation. It is the probability that a N

0, Ïƒ2
RV
exceeds half the distance between the vectors s0 and s1. Stated diï¬€erently, since
âˆ¥s0 âˆ’s1âˆ¥/Ïƒ is the number of standard deviations that separate s0 and s1, we can
express the probability of error as the probability that a standard Gaussian exceeds
half the distance between the vectors as measured in standard deviations of the
noise.
20.14.4
The Bhattacharyya Bound
Finally, we compute the Bhattacharyya Bound for this problem. From (20.50) we
obtain that, irrespective of the values of Ï€0, Ï€1,
pâˆ—(error)
â‰¤1
2

yâˆˆRJ
3
fY|H=0(y)fY|H=1(y) dy
= 1
2

y
A
B
B
C
J
@
j=1
-
1
âˆš
2Ï€Ïƒ2 eâˆ’(y(j)âˆ’s(j)
0 )
2
2Ïƒ2
.
J
@
j=1
-
1
âˆš
2Ï€Ïƒ2 eâˆ’(y(j)âˆ’s(j)
1 )
2
2Ïƒ2
.
dy
= 1
2

y
A
B
B
C
J
@
j=1
-
1
âˆš
2Ï€Ïƒ2 eâˆ’(y(j)âˆ’s(j)
0 )
2
2Ïƒ2
1
âˆš
2Ï€Ïƒ2 eâˆ’(y(j)âˆ’s(j)
1 )
2
2Ïƒ2
.
dy
= 1
2

y
J
@
j=1
-
1
âˆš
2Ï€Ïƒ2 exp
	
âˆ’

y(j) âˆ’s(j)
0
2 +

y(j) âˆ’s(j)
1
2
4Ïƒ2

.
dy
= 1
2
J
@
j=1

y(j)âˆˆR
1
âˆš
2Ï€Ïƒ2 eâˆ’
2(y(j))
2âˆ’2y(j)(s(j)
0
+s(j)
1 )+(s(j)
0 )
2
+(s(j)
1 )
2
4Ïƒ2
dy(j)
= 1
2
J
@
j=1
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 exp
-
âˆ’
y2 âˆ’y

s(j)
0
+ s(j)
1

+ 1
2

s(j)
0
2 +

s(j)
1
2
2Ïƒ2
.
dy
= 1
2
J
@
j=1
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 exp
â›
âœ
âœ
âœ
ââˆ’
	
y âˆ’s(j)
0
+s(j)
1
2

2
+

s(j)
0
âˆ’s(j)
1
2
4
2Ïƒ2
â
âŸ
âŸ
âŸ
â dy
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.15 Guessing in the Presence of a Random Parameter
431
= 1
2
J
@
j=1
exp
-
âˆ’

s(j)
0
âˆ’s(j)
1
2
8Ïƒ2
.
= 1
2 exp
-
âˆ’1
8Ïƒ2
J

j=1

s(j)
0
âˆ’s(j)
1
2
.
= 1
2 exp
-
âˆ’âˆ¥s0 âˆ’s1âˆ¥2
8Ïƒ2
.
,
(20.99)
where the last integral is evaluated using (19.21).
20.15
Guessing in the Presence of a Random Parameter
We now consider the guessing problem when the distribution of the observable Y
depends not only on the hypothesis H but also on a random parameter Î˜, which
is independent of H. Based on the conditional densities fY|Î˜,H=0(Â·), fY|Î˜,H=1(Â·),
the nondegenerate prior Ï€0, Ï€1 > 0, and on the law of Î˜, we seek an optimal rule
for guessing H. We distinguish between two cases depending on whether we must
base our guess on the observed value yobs of Y aloneâ€”random parameter not
observedâ€”or whether we also observe the value Î¸obs of Î˜â€”random parameter
observed. The analysis of both cases is conceptually straightforward.
20.15.1
Random Parameter Not Observed
The guessing problem when the random parameter is not observed is sometimes
called â€œtesting in the presence of a nuisance parameter.â€ Conceptually, the situ-
ation is quite simple. We have only one observation, Y = yobs, and an optimal
decision rule is the MAP rule (Theorem 20.7.1). The MAP rule entails computing
the likelihood-ratio function
LR(yobs) = fY|H=0(yobs)
fY|H=1(yobs),
(20.100)
and comparing the result to the threshold Ï€1/Ï€0; see (20.40).
Often, however, the densities fY|H=0(yobs) and fY|H=1(yobs) appearing in (20.100)
are not given directly. Instead we are given the density of Î˜ and the conditional
density of Y given (H, Î˜). (We shall encounter such a situation in Chapter 27
when we discuss noncoherent communications.) In such cases we can compute the
conditional density fY|H=0(yobs) as follows:
fY|H=0(yobs) =

Î¸
fY,Î˜|H=0(yobs, Î¸) dÎ¸
=

Î¸
fY|Î˜=Î¸,H=0(yobs) fÎ˜|H=0(Î¸) dÎ¸
=

Î¸
fY|Î˜=Î¸,H=0(yobs) fÎ˜(Î¸) dÎ¸,
(20.101)
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

432
Binary Hypothesis Testing
where the ï¬rst equality follows because from the joint density one obtains the
marginal density by integrating out the variable in which we are not interested;
the second by the deï¬nition of the conditional density; and the ï¬nal equality from
our assumption that Î˜ and H are independent. (In computations such as these
it is best to think about the conditioning on H = 0 as deï¬ning a new law on
(Y, Î˜)â€”a new law to which all the regular probabilistic manipulations, such as
marginalization and computation of conditional densities, continue to apply. We
thus simply think of the conditioning on H = 0 as specifying the joint law of (Y, Î˜)
that we have in mind.)
Repeating the calculation under H = 1 we obtain that the likelihood-ratio function
is given by
LR(yobs) =

Î¸ fY|Î˜=Î¸,H=0(yobs) fÎ˜(Î¸) dÎ¸

Î¸ fY|Î˜=Î¸,H=1(yobs) fÎ˜(Î¸) dÎ¸.
(20.102)
The case where Î˜ is discrete can be similarly addressed. An optimal decision rule
can now be derived based on this expression for the likelihood-ratio function and
on the MAP rule (20.40).
20.15.2
Random Parameter Observed
When the random parameter is observed to be Î˜ = Î¸obs, we merely view the
problem as a standard hypothesis testing problem with the observation consisting
of Y and Î˜. That is, we base our decision on the likelihood-ratio function
LR(yobs, Î¸obs) = fY,Î˜|H=0(yobs, Î¸obs)
fY,Î˜|H=1(yobs, Î¸obs).
(20.103)
The additional twist is that because Î˜ is independent of H we have
fY,Î˜|H=0(yobs, Î¸obs) = fÎ˜|H=0(Î¸obs) fY|Î˜=Î¸obs,H=0(yobs)
= fÎ˜(Î¸obs) fY|Î˜=Î¸obs,H=0(yobs),
(20.104)
where the second equality follows from the independence of Î˜ and H. Repeating
for the conditional law of the pair (Y, Î˜) given H = 1 we have
fY,Î˜|H=1(yobs, Î¸obs) = fÎ˜(Î¸obs) fY|Î˜=Î¸obs,H=1(yobs).
(20.105)
Consequently, by (20.103), (20.104), and (20.105), we obtain that for Î¸obs satisfying
fÎ˜(Î¸obs) Ì¸= 0
LR(yobs, Î¸obs) = fY|H=0,Î˜=Î¸obs(yobs)
fY|H=1,Î˜=Î¸obs(yobs).
(20.106)
An optimal decision rule can be again derived based on this expression for the
likelihood-ratio and on the MAP rule (20.40).
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.16 Mathematical Notes
433
20.16
Mathematical Notes
A standard reference on hypothesis testing is (Lehmann and Romano, 2005). It
also contains a measure-theoretic treatment of the subject. For a precise math-
ematical deï¬nition of the condition XâŠ¸âˆ’
âˆ’Y âŠ¸âˆ’
âˆ’Z we refer the reader to (Lo`eve,
1963, Section 25.3). For a measure-theoretic treatment of suï¬ƒcient statistic see
(Lo`eve, 1963, Section 24.4), (Billingsley, 1995, Section 34), (Romano and Siegel,
1986, pp. 154â€“156), and (Halmos and Savage, 1949). For a measure-theoretic treat-
ment of the notion of conditional distribution see, for example, (Billingsley, 1995,
Chapter 6), (Williams, 1991, Chapter 9), or (Lehmann and Romano, 2005, Chap-
ter 2).
20.17
Exercises
Exercise 20.1 (Hypothesis Testing). Let H take on the values 0 and 1 equiprobably.
Conditional on H = 0, the observable Y is equal to a + Z, where Z is independent of H
and has the Laplace distribution
fZ(z) = 1
2 eâˆ’|z|, z âˆˆR,
and a > 0 is a given constant. Conditional on H = 1, the observable Y is given by âˆ’a+Z.
(i) Find and draw the densities fY |H=0(Â·) and fY |H=1(Â·).
(ii) Find an optimal rule for guessing H based on Y .
(iii) Compute the optimal probability of error.
(iv) Compute the Bhattacharyya Bound.
Exercise 20.2 (A Discrete Multi-Dimensional Problem). Let H take on the values 0
and 1 according to the prior (Ï€0, Ï€1). Let the observation Y = (Y1, . . . , Yn)T be an n-
dimensional binary vector. Conditional on H = 0, the components of the vector Y are
IID with
Pr
$
Yâ„“= 1
 H = 0
%
= 1 âˆ’Pr
$
Yâ„“= 0
 H = 0
%
= 1
4,
â„“= 1, . . . , n.
Conditional on H = 1, the components are IID with
Pr
$
Yâ„“= 1
 H = 1
%
= 1 âˆ’Pr
$
Yâ„“= 0
 H = 1
%
= 3
4,
â„“= 1, . . . , n.
(i) Find an optimal rule for guessing H based on Y.
(ii) Compute the optimal probability of error.
(iii) Compute the Bhattacharyya Bound.
Hint: You may need to treat the cases of n even and n odd separately.
Exercise 20.3 (A Multi-Antenna Receiver). Let H take on the values 0 and 1 equiprob-
ably. We wish to guess H based on the random variables Y1 and Y2. Conditional on
H = 0,
Y1 = A + Z1,
Y2 = A + Z2,
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

434
Binary Hypothesis Testing
and conditional on H = 1,
Y1 = âˆ’A + Z1,
Y2 = âˆ’A + Z2.
Here A is a positive constant, and Z1 âˆ¼N

0, Ïƒ2
1

, Z2 âˆ¼N

0, Ïƒ2
2

, and H are independent.
(i) Find an optimal rule for guessing H based on (Y1, Y2).
(ii) Draw the decision regions in the (Y1, Y2)-plane for the special case where Ïƒ1 = 2Ïƒ2.
(iii) Returning to the general case, ï¬nd a one-dimensional suï¬ƒcient statistic.
(iv) Find the optimal probability of error in terms of Ïƒ2
1, Ïƒ2
2, and A.
(v) Consider a suboptimal receiver that declares â€œH = 0â€ if Y1 +Y2 > 0, and otherwise
declares â€œH = 1.â€ Evaluate the probability of error for this decoder as a function
of Ïƒ2
1, Ïƒ2
2, and A.
Exercise 20.4 (Binary Hypothesis Testing with General Costs). Let H take on the values 0
and 1 according to the prior (Ï€0, Ï€1). The observable Y has conditional densities fY|H=0(Â·)
and fY|H=1(Â·). Based on Y, we wish to guess the value of H. Let the guess associated
with Y = yobs be denoted by Ï†Guess(yobs). Guessing â€œH = Î·â€ when H = Î½ costs c(Î·, Î½),
where c(Â·, Â·) is a given function from {0, 1} Ã— {0, 1} to the nonnegative reals.
Find a
decision rule that minimizes the expected cost
E
+
c

Ï†Guess(Y), H
,
=
1

Î½=0
Ï€Î½
1

Î·=0
c(Î·, Î½) Pr
$
Ï†Guess(Y) = Î·
 H = Î½
%
.
Exercise 20.5 (Binary Hypothesis Testing). Let H take on the values 0 and 1 according
to the prior (Ï€0, Ï€1), and let the observation consist of the RV Y . Conditional on H, the
densities of Y are given for every y âˆˆR by
fY |H=0(y) = eâˆ’y I{y â‰¥0},
fY |H=1(y) = Î² eâˆ’y2
2 I{y â‰¥0},
where Î² > 0 is some constant.
(i) Determine Î².
(ii) Find a decision rule that minimizes the probability of error.
(iii) For the rule that you have found, compute Pr(error|H = 0).
Hint: Diï¬€erent priors can lead to dramatically diï¬€erent decision rules.
Exercise 20.6 (On the Bhattacharyya Bound).
(i) Show that the Bhattacharyya Bound never exceeds 1/2.
(ii) When is it equal to 1/2?
Hint: You may ï¬nd the Cauchy-Schwarz Inequality useful.
Exercise 20.7 (When Is the Bhattacharyya Bound Tight?). Under what conditions on the
prior (Ï€0, Ï€1) and the conditional densities fY|H=0(Â·), fY|H=1(Â·) does the Bhattacharyya
Bound coincide with the optimal probability of error?
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.17 Exercises
435
Exercise 20.8 (The Bhattacharyya Bound for Conditionally IID Observations). Consider
a binary hypothesis testing problem where, conditional on H = 0, the J components of
the observed random vector Y are IID with each component of density f0(Â·). Conditional
on H = 1 the components of Y are IID with each component of density f1(Â·). Express
the Bhattacharyya Bound in terms of J and

R

f0(y) f1(y) dy.
Exercise 20.9 (Another Bound on the Optimal Probability of Error).
(i) Prove that if Î± and Î² are nonnegative and sum to one, then min{Î±, Î²} is upper-
bounded by 2Î±Î².
(ii) Prove that
pâˆ—(error) â‰¤2

Rd Pr
$
H = 0
 Y = y
%
Pr
$
H = 1
 Y = y
%
fY(y) dy.
Exercise 20.10 (Conditional Independence and Factorizations of the PMF). For (dis-
crete) random variables X, Y , and Z show that X and Y are conditionally indepen-
dent given Z if, and only if, the joint PMF PX,Y,Z(x, y, z) can be written as a product
g1(x, z) g2(y, z) for some functions g1 and g2.
Exercise 20.11 (Another Characterization of Conditional Independence). Let the random
variables X, Y , and Z take values in ï¬nite subsets of the reals. Show that X and Y are
conditionally independent given Z if, and only if, for all functions g, h: R2 â†’R
E[g(X, Z) h(Y, Z)] = E
$
E[g(X, Z)|Z] E[h(Y, Z)|Z]
%
.
Exercise 20.12 (Independence and Conditional Independence). Let the random variables
X, Y , and Z be of joint PMF PX,Y,Z(Â·, Â·, Â·). Show that if X and Z are independent, Y
and Z are independent, and X and Y are conditionally independent given Z, then X, Y ,
and Z are independent.
Exercise 20.13 (Error Probability and L1-Distance). In the setting of Theorem 20.5.2
with a uniform prior, show that (20.26) can also be written as
Pr
$
Ï†âˆ—
Guess(Y) Ì¸= H
%
= 1
2 âˆ’1
4

Rd
fY |H=0(y) âˆ’fY |H=1(y)
 dy.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

436
Binary Hypothesis Testing
Exercise 20.14 (The Probability of Y Being in a Set). Consider binary hypothesis testing
with a uniform prior.
(i) Suppose that the (Lebesgue measurable) subset A of Rd is such that
Pr[Y âˆˆA|H = 1] âˆ’Pr[Y âˆˆA|H = 0] = Î”,
where Î” is some positive number.
Prove that the optimal probability of error
pâˆ—(error) for guessing H based on Y is upper-bounded by (1 âˆ’Î”)/2.
(ii) Prove that for any (Lebesgue measurable) A âŠ†Rd
Pr[Y âˆˆA|H = 1] âˆ’Pr[Y âˆˆA|H = 0]
 â‰¤1 âˆ’2pâˆ—(error),
A âŠ†Rd.
(iii) Find a set A for which this holds with equality.
Thus, if the optimal probability of error is very close to 1/2, then for every (Lebesgue
measurable) subset A âŠ†Rd the probability that Y is in A hardly depends on H.
Hint: For Part (i) compare an optimal guessing rule to a (possibly suboptimal) rule that
guesses â€œH = 1â€ if Y âˆˆA and â€œH = 0â€ otherwise.
Exercise 20.15 (Multi-Dimensional Binary Hypothesis Testing). The random 2-vector
Z = (Z(1), Z(2)) is uniformly distributed on the unit disc

(x, y) âˆˆR2 : x2 + y2 â‰¤1

,
independently of H, which takes on the values 0 and 1 equiprobably. Conditional on
H = 0, we observe Y = Î±Z, and conditional on H = 1, we observe Y = Î²Z, where Î± and
Î² are real numbers.
(i) Derive an optimal decision rule for guessing H based on Y.
(ii) Find a one-dimensional suï¬ƒcient statistic for guessing H based on Y.
(iii) Compute the optimal probability of error and the Bhattacharyya Bound.
Exercise 20.16 (Optimality and Suï¬ƒciency). Let U1 and U2 be independent and uniformly
distributed on the unit interval [0, 1]. The binary random variable H takes on the values
0 and 1 equiprobably. Conditional on H = 0, we observe the random 2-vector
Y =
Î±U1
Î²U2

,
and conditional on H = 1, we observe
Y =
Î²U1
Î±U2

,
where Î± and Î² are deterministic and Î² > Î± > 0.
(i) Derive an optimal decision rule for guessing H based on Y.
(ii) Compute the optimal probability of error and the Bhattacharyya Bound.
(iii) Exhibit an optimal rule for guessing H based on Y that bases its guess on Y (1).
(iv) Show that, notwithstanding Part (iii), T

y(1), y(2)
= y(1) is not a suï¬ƒcient statistic
for the densities fY|H=0(Â·) and fY|H=1(Â·).
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.17 Exercises
437
Exercise 20.17 (Conditionally Poisson Observations). A RV X is said to have a Poisson
distribution of parameter (â€œintensityâ€) Î», where Î» is some nonnegative real number, if X
takes value in the nonnegative integers and
Pr
$
X = n
%
= eâˆ’Î» Î»n
n! ,
n = 0, 1, 2, . . .
(i) Find the Moment Generating Function of a Poisson RV of intensity Î».
(ii) Show that if X and Y are independent Poisson random variables of intensities Î»x
and Î»y, then their sum X + Y is Poisson with parameter Î»x + Î»y.
(iii) Let H take on the values 0 and 1 according to the prior (Ï€0, Ï€1).
We wish to
guess H based on the RV Y . Conditional on H = 0, the observation Y is Poisson
of intensity Î± + Î», whereas conditional on H = 1 it is Poisson of intensity Î² + Î».
Here Î±, Î², Î» are known nonnegative constants. Show that the optimal probability
of error is monotonically nondecreasing in Î».
Hint: For Part (iii) recall Part (ii) and that no randomized decision rule can outperform
an optimal deterministic rule.
Exercise 20.18 (Optical Communication). Consider an optical communication system
that uses binary on/oï¬€keying at a rate of 108 bits per second. At the beginning of each
time interval of duration 10âˆ’8 seconds a new data bit D enters the transmitter. If D = 0,
the laser is turned oï¬€for the duration of the interval; otherwise, if D = 1, the laser is
turned on. The receiver counts the number Y of photons received during the interval.
Assume that, conditional on D, the observation Y is a Poisson RV whose conditional
PMF is
Pr
$
Y = y
 D = 0
%
= eâˆ’Î¼ Î¼y
y!
,
y = 0, 1, 2, . . . ,
(20.107)
Pr
$
Y = y
 D = 1
%
= eâˆ’Î» Î»y
y!
,
y = 0, 1, 2, . . . ,
(20.108)
where Î» > Î¼ â‰¥0. Further assume that Pr[D = 0] = Pr[D = 1] = 1/2.
(i) Find an optimal guessing rule for guessing D based on Y .
(ii) Compute the optimal probability of error. (Not necessarily in closed-form.)
(iii) Suppose that we now transmit each data bit over two time intervals, each of duration
10âˆ’8 seconds. (The system now supports a data rate of 0.5 Ã— 108 bits per second.)
The receiver produces the photon counts Y1 and Y2 over the two intervals. Assume
that, conditional on D = 0, the counts Y1 & Y2 are IID with the PMF (20.107)
and that, conditional on D = 1, they are IID with the PMF (20.108).
Find a
one-dimensional suï¬ƒcient statistic for the problem and use it to ï¬nd an optimal
decision rule.
Hint: For Part (iii), recall Part (ii) of Exercise 20.17.
Exercise 20.19 (Fun with Randomization). Two random variables U and V are drawn
IID according to some unknown probability density function f(Â·). After observing that
U = uobs, we wish to guess whether it is smaller or larger than V .
(i) What is the probability of success of the rule that, irrespective of uobs, guesses that
U is the larger of the two?
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

438
Binary Hypothesis Testing
(ii) What is the probability of success of the rule that for some a âˆˆR guesses that U
is the larger of the two whenever uobs > a? Express your answer in terms of a and
f(Â·). Under what conditions is this probability of success strictly larger than 1/2 ?
(iii) Can you ï¬nd a randomized guessing rule whose probability of success is strictly
larger than 1/2 irrespective of f(Â·) ?
Exercise 20.20 (Monotone Likelihood Ratio and Log-Concavity). Let H take on the
values 0 and 1 according to the nondegenerate prior (Ï€0, Ï€1). Conditional on H = 0, the
observation Y is given by
Y = Î¾0 + Z,
where Î¾0 âˆˆR is some deterministic number and Z is a RV of PDF fZ(Â·). Conditional on
H = 1, the observation Y is given by
Y = Î¾1 + Z,
where Î¾1 > Î¾0.
(i) Show that if the PDF fZ(Â·) is positive and is such that
fZ(y1 âˆ’Î¾0) fZ(y0 âˆ’Î¾1) â‰¤fZ(y1 âˆ’Î¾1) fZ(y0 âˆ’Î¾0),
	
y1 > y0, Î¾1 > Î¾0

, (20.109)
then an optimal decision rule is to guess â€œH = 0â€ if Y â‰¤yâ‹†and to guess â€œH = 1â€
if Y > yâ‹†for some real number yâ‹†.
(ii) Show that if z â†’log fZ(z) is a concave function, then (20.109) is satisï¬ed.
Mathematicians state this result by saying that if g: R â†’R is positive, then the mapping
(x, y) â†’g(x âˆ’y) has the Total Positivity property of Order 2 if, and only if, g is log-
concave (Marshall, Olkin, and Arnold, 2011, Chapter 18, Section A, Example A.10).
Statisticians state this result by saying that a location family generated by a positive
PDF f(Â·) has monotone likelihood ratios if, and only if, f(Â·) is log-concave. For more on
distributions with monotone likelihood ratios see (Lehmann and Romano, 2005, Chapter
3, Section 3.4).
Hint: For Part (ii) recall that a function g: R â†’R is concave if for any a < b and
0 < Î± < 1 we have g

Î±a + (1 âˆ’Î±)b

â‰¥Î± g(a) + (1 âˆ’Î±) g(b). You may like to proceed as
follows. Show that if g is concave then
g(a âˆ’Î”2) + g(a + Î”2) â‰¤g(a âˆ’Î”1) + g(a + Î”1),
|Î”1| â‰¤|Î”2|.
Deï¬ning g(z) = log fZ(z), show that the logarithm of the LHS of (20.109) can be written
as
g
	
Â¯y âˆ’Â¯Î¾ + 1
2Î”y + 1
2Î”Î¾

+ g
	
Â¯y âˆ’Â¯Î¾ âˆ’1
2Î”y âˆ’1
2Î”Î¾

,
where
Â¯y = (y0 + y1)/2,
Â¯Î¾ = (Î¾0 + Î¾1)/2,
Î”y = y1 âˆ’y0,
Î”Î¾ = Î¾1 âˆ’Î¾0.
Show that the logarithm of the RHS of (20.109) is given by
g
	
Â¯y âˆ’Â¯Î¾ + 1
2Î”y âˆ’1
2Î”Î¾

+ g
	
Â¯y âˆ’Â¯Î¾ + 1
2Î”Î¾ âˆ’1
2Î”y

.
Exercise 20.21 (Worst Prior). Given a guessing rule Ï†Guess : Rd â†’{0, 1}, which prior
maximizes the probability of error?
Hint: You may need to consider several cases separately.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

20.17 Exercises
439
Exercise 20.22 (Is a Uniform Prior the Worst Prior?). Based on an observation Y , we
wish to guess the value of a RV H taking on the values 0 and 1 according to the prior
(Ï€0, Ï€1). Conditional on H = 0, the observation Y is uniform over the interval [0, 1], and,
conditional on H = 1, it is uniform over the interval [0, 1/2].
(i) Find an optimal rule for guessing H based on the observation Y . Note that the
rule may depend on Ï€0.
(ii) Let pâˆ—(error; Ï€0) denote the optimal probability of error. Find pâˆ—(error; Ï€0) and
plot it as a function of Ï€0 in the range 0 â‰¤Ï€0 â‰¤1.
(iii) Which value of Ï€0 maximizes pâˆ—(error; Ï€0)?
Consider now the general problem where the RV Y is of conditional densities fY |H=0(Â·),
fY |H=1(Â·), and H is of prior (Ï€0, Ï€1). Let pâˆ—(error; Ï€0) denote the optimal probability of
error for guessing H based on Y .
(iv) Prove that
pâˆ—	
error; 1
2

â‰¥1
2 pâˆ—(error; Ï€0) + 1
2 pâˆ—(error; 1 âˆ’Ï€0),
Ï€0 âˆˆ[0, 1].
(20.110a)
(v) Show that if the densities fY |H=0(Â·) and fY |H=1(Â·) satisfy
fY |H=0(y) = fY |H=1(âˆ’y),
y âˆˆR,
(20.110b)
then
pâˆ—(error; Ï€0) = pâˆ—(error; 1 âˆ’Ï€0),
Ï€0 âˆˆ[0, 1].
(20.110c)
(vi) Show that if (20.110b) holds, then the uniform prior is the worst prior:
pâˆ—(error; Ï€0) â‰¤pâˆ—(error; 1/2),
Ï€0 âˆˆ[0, 1].
(20.110d)
Hint: For Part (iv) you might like to consider a new setup. In the new setup ËœH = M âŠ•S,
where âŠ•denotes the exclusive-or operation and where the binary random variables M
and S are independent with S taking value in {0, 1} equiprobably and with Pr[M = 0] =
1 âˆ’Pr[M = 1] = Ï€0.
Assume that in the new setup (M, S)âŠ¸âˆ’
âˆ’ËœHâŠ¸âˆ’
âˆ’ËœY and that the
conditional density of ËœY given ËœH = 0 is fY |H=0(Â·) and given ËœH = 1 it is fY |H=1(Â·).
Compare now the performance of an optimal decision rule for guessing ËœH based on ËœY
with the performance of an optimal decision rule for guessing ËœH based on the pair ( ËœY , S).
Express these probabilities of error in terms of the parameters of the original problem.
Exercise 20.23 (Hypothesis Testing with a Random Parameter). Let Y = X + AZ,
where X, A, and Z are independent random variables with X taking on the values Â±1
equiprobably, A taking on the values 2 and 3 equiprobably, and Z âˆ¼N

0, Ïƒ2
.
(i) Find an optimal rule for guessing X based on the pair (Y, A).
(ii) Repeat when you observe only Y .
Exercise 20.24 (Bounding the Conditional Probability of Error). Show that when the
prior is uniform
pMAP(error|H = 0) â‰¤
 -
fY|H=0(y) fY|H=1(y) dy.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,
www.ebook3000.com

440
Binary Hypothesis Testing
Exercise 20.25 (Upper Bounds on the Conditional Probability of Error).
(i) Let H take on the values 0 and 1 according to the nondegenerate prior (Ï€0, Ï€1). Let
the observation Y have the conditional densities fY|H=0(Â·) and fY|H=1(Â·). Show
that for every Ï > 0
pMAP(error|H = 0) â‰¤
	Ï€1
Ï€0

Ï 
f Ï
Y|H=1(y) f 1âˆ’Ï
Y|H=0(y) dy.
(ii) A suboptimal decoder guesses â€œH = 0â€ if q0(y) > q1(y); guesses â€œH = 1â€ if
q0(y) < q1(y); and otherwise tosses a coin.
Here q0(Â·) and q1(Â·) are arbitrary
positive functions. Show that for this decoder
p(error|H = 0) â‰¤
 q1(y)
q0(y)
Ï
fY|H=0(y) dy,
Ï > 0.
Hint: In Part (i) show that you can upper-bound I{Ï€1 fY|H=1(y)/(Ï€0 fY|H=0(y)) â‰¥1} by

Ï€1 fY|H=1(y)/(Ï€0 fY|H=0(y))
Ï.
Exercise 20.26 (The Hellinger Distance). The Hellinger distance between the densities
f(Â·) and g(Â·) is deï¬ned as the square root of
1
2
 	
f(Î¾) âˆ’

g(Î¾)

2
dÎ¾
(though some authors drop the one-half).
(i) Show that the Hellinger distance between f(Â·) and h(Â·) is upper-bounded by the
sum of the Hellinger distances between f(Â·) and g(Â·) and between g(Â·) and h(Â·).
(ii) Relate the Hellinger distance to the Bhattacharyya Bound.
(iii) Show that the Hellinger distance is upper-bounded by one.
Exercise 20.27 (Artifacts of Suboptimality). Let H take on the values 0 and 1 equiprob-
ably. Conditional on H = 0, the observation Y is N

1, Ïƒ2
, and, conditional on H = 1,
it is N

âˆ’1, Ïƒ2
. Alice guesses â€œH = 0â€ if Y > 2 and guesses â€œH = 1â€ otherwise.
(i) Compute the probability that Alice errs as a function of Ïƒ2.
(ii) Show that this probability is not monotonically nondecreasing in Ïƒ2.
(iii) Does her guessing rule minimize the probability of error?
(iv) Show that if you are obliged to use her rule, then adding noise to Y prior to feeding
it to her detector may be beneï¬cial.
Exercise 20.28 (The Bhattacharyya Bound and a Random Parameter). Let Î˜ be inde-
pendent of H and of density fÎ˜(Â·). Express the Bhattacharyya Bound on the probability
of guessing H incorrectly in terms of fÎ˜(Â·), fY|Î˜=Î¸,H=0(Â·) and fY|Î˜=Î¸,H=1(Â·). Treat the
case where Î˜ is not observed and the case where it is observed separately. Show that the
Bhattacharyya Bound in the former case is always at least as large as in the latter case.
available at 
.022
14:36:16, subject to the Cambridge Core terms of use,

Chapter 21
Multi-Hypothesis Testing
21.1
Introduction
In Chapter 20 we discussed how to guess the outcome of a binary random variable.
We now extend the discussion to random variables that take on more than twoâ€”but
still a ï¬niteâ€”number of values. Statisticians call this problem â€œmulti-hypothesis
testingâ€ to indicate that there may be more than two hypotheses. Rather than
using H, we now denote the random variable whose outcome we wish to guess
by M. (In Chapter 20 we used H for â€œhypothesis;â€ now we use M for â€œmessage.â€)
We denote the number of possible values that M can take by M and assume that
M â‰¥2. (The case M = 2 corresponds to binary hypothesis testing.) As before the
â€œlabelsâ€ are not important and there is no loss in generality in assuming that M
takes values in the set M = {1, . . . , M}. (In the binary case we used the traditional
labels 0 and 1 but now we prefer 1, 2, . . . , M.)
21.2
The Setup
A random variable M takes values in the set M = {1, . . . , M}, where M â‰¥2,
according to the prior
Ï€m = Pr[M = m],
m âˆˆM,
(21.1)
where
Ï€m â‰¥0,
m âˆˆM,
(21.2)
and where

mâˆˆM
Ï€m = 1.
(21.3)
We say that the prior is nondegenerate if
Ï€m > 0,
m âˆˆM,
(21.4)
with the inequalities being strict, so M can take on any value in M with positive
probability. We say that the prior is uniform if
Ï€1 = Â· Â· Â· = Ï€M = 1
M.
(21.5)
441
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

442
Multi-Hypothesis Testing
The observation is a random vector Y taking values in Rd. We assume that for
each m âˆˆM the distribution of Y conditional on M = m has the density1
fY|M=m(Â·),
m âˆˆM,
(21.6)
where fY|M=m(Â·) is a nonnegative Borel measurable function that integrates to
one over Rd.
A guessing rule is a Borel measurable function Ï†Guess : Rd â†’M from the space
of possible observations Rd to the set of possible messages M. We think about
Ï†Guess(yobs) as the guess we form after observing that Y = yobs.
The error
probability associated with the guessing rule Ï†Guess(Â·) is given by
Pr

Ï†Guess(Y) Ì¸= M

.
(21.7)
Note that two sources of randomness determine whether we err or not: the real-
ization of M and the generation of Y conditional on that realization. A guessing
rule is said to be optimal if no other guessing rule achieves a lower probability
of error.2 The optimal error probability pâˆ—(error) is the probability of error
associated with an optimal decision rule. In this chapter we shall derive optimal
decision rules and study the optimal probability of error.
21.3
Optimal Guessing
Having observed that Y = yobs, we would like to guess M. An optimal guessing
rule can be derived, as in the binary case, by ï¬rst considering the scenario where
there are no observables.
Its extension to the more interesting case where we
observe Y is straightforward.
21.3.1
Guessing in the Absence of Observables
In this scenario there are only M deterministic decision rules to choose from: the
decision rule â€œguess 1â€, the decision rule â€œguess 2â€, etc. If we employ the â€œguess 1â€
rule, then we are correct if M is indeed equal to 1 and thus with probability of
success Ï€1 and corresponding probability of error of 1âˆ’Ï€1. In general, if we employ
the â€œguess mâ€ rule for some m âˆˆM, then our probability of success is Ï€m. Thus,
of the M diï¬€erent rules at our disposal, the one that has the highest probability
of success is the â€œguess Ëœmâ€ rule, where Ëœm is the outcome that is a priori the most
likely. If this Ëœm is not unique, then guessing any one of the outcomes that have
the highest a priori probability is optimal.
1We feel no remorse for limiting ourselves to conditional distributions possessing a density.
The reason is that, while the reader is encouraged to assume that the densities are with respect to
the Lebesgue measure, this assumption is never used in the text. And using the Radon-Nikodym
Theorem (Billingsley, 1995, Section 32), one can show that even in the most general case there
exists a measure on Rd with respect to which the conditional laws of Y given each of the possible
values of M are absolutely continuous. That measure can be taken, for example, as the sum of
the conditional laws corresponding to each of the possible values that M can take.
2As in the case of binary hypothesis testing, an optimal guessing rule always exists; see
Footnote 2 on Page 397.
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.3 Optimal Guessing
443
We conclude that in the absence of observables, the guessing rule â€œguess Ëœmâ€ is
optimal if, and only if,
Ï€ Ëœm = max
mâ€²âˆˆM Ï€mâ€².
(21.8)
For an optimal guessing rule the probability of success is
pâˆ—(correct) = max
mâ€²âˆˆM Ï€mâ€²
(21.9)
and the optimal error probability is thus
pâˆ—(error) = 1 âˆ’max
mâ€²âˆˆM Ï€mâ€².
(21.10)
21.3.2
The Joint Law of M and Y
Using the prior {Ï€m} and the conditional densities {fY|M=m(Â·)}, we can express
the unconditional density of Y as
fY(y) =

mâˆˆM
Ï€m fY|M=m(y),
y âˆˆRd.
(21.11)
As in Section 20.4, we deï¬ne for every m âˆˆM and for every yobs âˆˆRd the
conditional probability that M equals m given Y = yobs by
Pr[M = m|Y = yobs] â‰œ
â§
âª
â¨
âª
â©
Ï€m fY|M=m(yobs)
fY(yobs)
if fY(yobs) > 0,
1
M
otherwise.
(21.12)
By an argument similar to the one proving (20.12) we have
Pr

Y âˆˆ
Ëœy âˆˆRd : fY(Ëœy) = 0

= 0,
(21.13)
which can also be written as
Pr

fY(Y) = 0

= 0.
21.3.3
Guessing in the Presence of Observables
The problem of guessing in the presence of an observable is very similar to the
one without observables. The intuition is that after observing that Y = yobs, we
associate with each m âˆˆM the a posteriori probability Pr[M = m|Y = yobs] and
then guess M as though there were no observables. Thus, rather than choosing
the message that has the highest a priori probability as we do in the absence of
observables, we should now choose the message that has the highest a posteriori
probability.
After having observed that Y = yobs we should thus guess â€œ Ëœmâ€ where Ëœm is the out-
come in M that has the highest a posteriori probability. If more than one outcome
attains the highest a posteriori probability, then we say that a tie has occurred
and we need to resolve this tie by picking one (it does not matter which) of the
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

444
Multi-Hypothesis Testing
outcomes that attains the maximum a posteriori probability. We thus guess â€œ Ëœm,â€
in analogy to (21.8), only if
Pr[M = Ëœm|Y = yobs] = max
mâ€²âˆˆM

Pr[M = mâ€² |Y = yobs]

.
(We shall later deï¬ne the Maximum A Posteriori guessing rule as a randomized
decision rule that picks uniformly at random from the outcomes that have the
highest a posteriori probability; see Deï¬nition 21.3.2 ahead.)
In analogy with (21.9), for this optimal rule
pâˆ—(correct|Y = yobs) = max
mâ€²âˆˆM

Pr[M = mâ€² |Y = yobs]

,
and, in analogy with (21.10),
pâˆ—(error|Y = yobs) = 1 âˆ’max
mâ€²âˆˆM

Pr[M = mâ€² |Y = yobs]

.
Consequently, the unconditional optimal probability of error can be expressed as
pâˆ—(error) =

Rd

1 âˆ’max
mâ€²âˆˆM

Pr[M = mâ€² |Y = y]

fY(y) dy,
(21.14)
where fY(Â·) is the unconditional density function of Y and is given in (21.11).
We next proceed to make the above intuitive discussion more rigorous. We begin
by deï¬ning for every possible observation yobs âˆˆRd the set of outcomes of maximal
a posteriori probability:
Ëœ
M(yobs) â‰œ
'
Ëœm âˆˆM : Pr[M = Ëœm|Y = yobs] = max
mâ€²âˆˆM Pr[M = mâ€² |Y = yobs]
(
.
(21.15)
As we next argue, this set can also be expressed as
Ëœ
M(yobs) =
'
Ëœm âˆˆM : Ï€ Ëœm fY|M= Ëœm(yobs) = max
mâ€²âˆˆM Ï€mâ€² fY|M=mâ€²(yobs)
(
.
(21.16)
This can be shown by treating the case fY(yobs) > 0 and the case fY(yobs) = 0
separately. In the former case, (21.16) is veriï¬ed by noting that in this case we
have, by (21.12), that Pr[M = mâ€² |Y = yobs] = Ï€mâ€² fY|M=mâ€²(yobs)/fY(yobs), so
the result follows because scaling the scores of all the elements of a set by a positive
number that is common to them all (1/fY(yobs)) does not change the subset of
the elements with the highest score. In the latter case we note that, by (21.12),
we have for all mâ€² âˆˆM that Pr[M = mâ€² |Y = yobs] = 1/M, so the RHS of (21.15)
is M and we also have by (21.11) for all mâ€² âˆˆM that Ï€mâ€² fY|M=mâ€²(yobs) = 0 so
the RHS of (21.16) is also M.
Using the above deï¬nition of Ëœ
M(yobs) we can now state the main theorem regarding
optimal guessing rules.
Theorem 21.3.1 (Optimal Multi-Hypothesis Testing). Let M take values in the
set M = {1, . . . , M} with the prior (21.1), and let the observation Y be a random
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.3 Optimal Guessing
445
vector taking values in Rd with conditional densities fY|M=1(Â·), . . . , fY|M=M(Â·).
Any guessing rule Ï†âˆ—
Guess : Rd â†’M that satisï¬es
Ï†âˆ—
Guess(yobs) âˆˆËœ
M(yobs),
yobs âˆˆRd
(21.17)
is optimal. Here
Ëœ
M(yobs) is the set deï¬ned in (21.15) or (21.16).
Proof. Every (deterministic) guessing rule induces a partitioning of the space of
possible outcomes Rd into M disjoint sets D1, . . . , DM:
M

m=1
Dm = Rd,
(21.18a)
Dm âˆ©Dmâ€² = âˆ…,
m Ì¸= mâ€²,
(21.18b)
where Dm is the set of observations that result in the guessing rule producing
the guess â€œM = m.â€ Conversely, every partition D1, . . . , DM of Rd corresponds
to some deterministic guessing rule that guesses â€œM = mâ€ whenever yobs âˆˆDm.
Searching for an optimal decision rule is thus equivalent to searching for an optimal
way to partition Rd. For every partition D1, . . . , DM the probability of success of
the guessing rule associated with it is given by
Pr(correct) =

mâˆˆM
Ï€m Pr(correct|M = m)
=

mâˆˆM
Ï€m

Dm
fY|M=m(y) dy
=

mâˆˆM
Ï€m

Rd fY|M=m(y) I{y âˆˆDm} dy
=

Rd
	 
mâˆˆM
Ï€m fY|M=m(y) I{y âˆˆDm}

dy.
To minimize the probability of error we maximize the probability of correct deci-
sion. We thus need to ï¬nd a partition D1, . . . , DM that maximizes the last integral.
To maximize the integral we shall maximize the integrand

mâˆˆM
Ï€m fY|M=m(y) I{y âˆˆDm}.
For a ï¬xed value of y, the value of the integrand depends on the set to which we
have assigned y. If y was assigned to D1 (i.e., if y âˆˆD1), then all the terms in the
sum except for the ï¬rst are zero, and the value of the integrand is Ï€1 fY|M=1(y).
More generally, if y was assigned to Dm, then all the terms in the sum except for
the m-th term are zero, and the value of the integrand is Ï€m fY|M=m(y). For a
ï¬xed value of y, the integrand will thus be maximized if we assign y to the set D Ëœm
(and correspondingly guess Ëœm), only if
Ï€ Ëœm fY|M= Ëœm(y) = max
mâ€²âˆˆM

Ï€mâ€² fY|M=mâ€²(y)

.
Thus, if Ï†âˆ—
Guess(Â·) satisï¬es the theoremâ€™s hypotheses, then it maximizes the in-
tegrand for every y âˆˆRd and thus also maximizes the probability of guessing
correctly.
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

446
Multi-Hypothesis Testing
21.3.4
The MAP and ML Rules
As in the binary hypothesis testing case, we can also consider randomized decision
rules. Extending the deï¬nition of a randomized decision rule to our setting, one
can show using arguments very similar to those of Section 20.6 that randomization
does not help: no randomized decision rule can yield a smaller probability of error
than an optimal deterministic rule. But randomized decision rules can yield more
symmetric or more â€œfairâ€ rules.
Indeed, we shall deï¬ne the MAP rule as the
randomized rule that resolves ties by choosing one of the messages that achieves
the highest a posteriori probability uniformly at random:
Deï¬nition 21.3.2 (The M-ary MAP Decision Rule). The Maximum A Poste-
riori decision rule is the guessing rule that, after observing that Y = yobs, forms
a guess by picking uniformly at random an element of the set
Ëœ
M(yobs), which is
deï¬ned in (21.15) or (21.16).
Theorem 21.3.3 (The MAP Rule Is Optimal). For the setting of Theorem 21.3.1
the MAP decision rule is optimal in the sense that it achieves the smallest proba-
bility of error among all deterministic or randomized decision rules. Thus,
pâˆ—(error) =

mâˆˆM
Ï€m pMAP(error|M = m),
(21.19)
where pâˆ—(error) denotes the optimal probability of error and pMAP(error|M = m)
denotes the conditional probability of error of the MAP rule.
Proof. Irrespective of the realization of the randomization that is used to pick
an element of
Ëœ
M(yobs), the resulting decision rule is optimal (Theorem 21.3.1).
Consequently, the average probability of error that results when we average over
this source of randomness must also be optimal.
The Maximum-Likelihood (ML) rule ignores the prior. It is identical to the
MAP rule when the prior is uniform. Having observed that Y = yobs, the ML
decoder produces as its guess a member of the set
'
Ëœm âˆˆM : fY|M= Ëœm(yobs) = max
mâ€²âˆˆM fY|M=mâ€²(yobs)
(
that is drawn uniformly at random.
The ML decoder thus guesses â€œM = Ëœmâ€ only if
fY|M= Ëœm(yobs) = max
mâ€²âˆˆM fY|M=mâ€²(yobs).
(21.20)
(If more than one outcome achieves this maximum, it chooses uniformly at random
one of the outcomes that achieves the maximum.)
21.3.5
Processing
As in Section 20.11, we say that Z is the result of processing Y with respect to M
if
MâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’Z
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.4 Example: Multi-Hypothesis Testing for 2D Signals
447
(a1, b1)
(a2, b2)
(a3, b3)
(a4, b4)
(a5, b5)
(a6, b6)
(a7, b7)
(a8, b8)
A
Figure 21.1: Eight equiprobable hypotheses; the situation corresponds to 8-PSK.
forms a Markov chain. In analogy to Theorem 20.11.5, one can prove that if Z is
the result of processing Y with respect to M, then no decision rule based on Z can
outperform an optimal decision rule based on Y.
21.4
Example: Multi-Hypothesis Testing for 2D Signals
21.4.1
The Setup
Consider the case where M is uniformly distributed over the set M = {1, . . . , M}
and where we would like to guess the outcome of M based on an observation
consisting of a two-dimensional random vector Y of components Y (1) and Y (2).
Conditional on M = m, the random variables Y (1) and Y (2) are independent
with Y (1) âˆ¼N

am, Ïƒ2
and Y (2) âˆ¼N

bm, Ïƒ2
. We assume that Ïƒ2 > 0, so the
conditional densities can be written for every m âˆˆM and every y(1), y(2) âˆˆR as
fY (1),Y (2)|M=m

y(1), y(2)
=
1
2Ï€Ïƒ2 exp
	
âˆ’(y(1) âˆ’am)2 + (y(2) âˆ’bm)2
2Ïƒ2

.
(21.21)
This hypothesis testing problem is related to QAM communication over an additive
white Gaussian noise channel with a pulse shape that is orthogonal to its time shifts
by integer multiples of the baud period. The setup is demonstrated in Figure 21.1
for the special case of M = 8 with
am = A cos
2Ï€m
8

,
bm = A sin
2Ï€m
8

,
m = 1, . . . , 8.
(21.22)
This special case is related to 8-PSK communication, where M-PSK stands for
M-ary Phase Shift Keying.
21.4.2
The â€œNearest-Neighborâ€ Decoding Rule
We shall next derive an optimal decision rule. For typographical reasons we shall
use y rather than yobs to denote the observed vector. To ï¬nd an optimal decoding
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

448
Multi-Hypothesis Testing
rule we note that, since M has a uniform prior, the Maximum-Likelihood rule
(21.20) is optimal. Now Ëœm maximizes the likelihood function if, and only if,
	
fY (1),Y (2)|M= Ëœm(y(1), y(2)) = max
mâ€²âˆˆM

fY (1),Y (2)|M=mâ€²(y(1), y(2))

â‡â‡’
-
1
2Ï€Ïƒ2 eâˆ’(y(1)âˆ’a Ëœ
m)
2+(y(2)âˆ’b Ëœ
m)
2
2Ïƒ2
= max
mâ€²âˆˆM

1
2Ï€Ïƒ2 eâˆ’(y(1)âˆ’amâ€²)
2+(y(2)âˆ’bmâ€²)
2
2Ïƒ2
!.
â‡â‡’
-
eâˆ’(y(1)âˆ’a Ëœ
m)
2+(y(2)âˆ’b Ëœ
m)
2
2Ïƒ2
= max
mâ€²âˆˆM

eâˆ’(y(1)âˆ’amâ€²)
2+(y(2)âˆ’bmâ€²)
2
2Ïƒ2
!.
â‡â‡’
-
âˆ’

y(1) âˆ’a Ëœm
2 +

y(2) âˆ’b Ëœm
2
2Ïƒ2
= max
mâ€²âˆˆM

âˆ’

y(1) âˆ’amâ€²2 +

y(2) âˆ’bmâ€²2
2Ïƒ2
!.
â‡â‡’
-
y(1) âˆ’a Ëœm
2 +

y(2) âˆ’b Ëœm
2
2Ïƒ2
= min
mâ€²âˆˆM

y(1) âˆ’amâ€²2 +

y(2) âˆ’bmâ€²2
2Ïƒ2
!.
â‡â‡’

y(1) âˆ’a Ëœm
2 +

y(2) âˆ’b Ëœm
2 = min
mâ€²âˆˆM
'
y(1) âˆ’amâ€²2 +

y(2) âˆ’bmâ€²2(
â‡â‡’
	
âˆ¥y âˆ’s Ëœmâˆ¥= min
mâ€²âˆˆM
'
âˆ¥y âˆ’smâ€²âˆ¥
(
,
where y = (y(1), y(2))T, sm â‰œ(am, bm)T for m âˆˆM, and âˆ¥Â·âˆ¥denotes the Euclidean
distance (20.85). It is thus seen that the ML rule (which is equivalent to the MAP
rule because the prior is uniform) is equivalent to a â€œnearest-neighborâ€ decoding
rule, which chooses the hypothesis under which the mean vector is closest to the
observed vector (with ties being resolved at random).
Figure 21.2 depicts the
nearest-neighbor decoding rule for 8-PSK. The shaded region corresponds to the
set of observables that result in the guess â€œM = 1,â€ i.e., the set of points that are
nearest to

A cos(2Ï€/8), A sin(2Ï€/8)

.
21.4.3
Exact Error Analysis for 8-PSK
The analysis of the probability of error can be a bit tricky. Here we only present
the analysis for 8-PSK. If nothing else, it will motivate us to seek more easily
computable bounds.
We shall compute the probability of error conditional on M = 4. But there is
nothing special about this choice; the rotational symmetry of the problem implies
that the probability of error does not depend on the hypothesis.
Conditional on M = 4, the observables (Y (1), Y (2))T can be expressed as

Y (1), Y (2)T = (âˆ’A, 0)T +

Z(1), Z(2)T,
where Z(1) and Z(2) are independent N

0, Ïƒ2
random variables:
fZ(1),Z(2)(z(1), z(2)) =
1
2Ï€Ïƒ2 exp
	
âˆ’(z(1))2 + (z(2))2
2Ïƒ2

,
z(1), z(2) âˆˆR.
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.4 Example: Multi-Hypothesis Testing for 2D Signals
449
m = 1
y(1)
y(2)
guess 1
Figure 21.2: The shaded region corresponds to observations leading the ML rule
to guess â€œM = 1.â€
y(1)
y(2)
Figure 21.3: Contour lines of the density fY1,Y2|M=4(Â·). The shaded region corre-
sponds to guessing â€œM = 4â€.
Figure 21.3 depicts the contour lines of the density fY (1),Y (2)|M=4(Â·), which are
centered on the mean (a4, b4) = (âˆ’A, 0). Note that fY (1),Y (2)|M=4(Â·) is symmetric
about the horizontal axis:
fY (1),Y (2)|M=4

y(1), âˆ’y(2)
= fY (1),Y (2)|M=4

y(1), y(2)
,
y(1), y(2) âˆˆR.
(21.23)
The shaded region in the ï¬gure is the set of pairs (y(1), y(2)) that cause the nearest-
neighbor decoder to guess â€œM = 4.â€3 Conditional on M = 4 an error results if
(Y (1), Y (2)) is outside the shaded region.
Referring now to Figure 21.4 we need to compute the probability that the noise
(Z(1), Z(2)) causes the received signal to lie in the union of the shaded areas. The
symmetry of fY (1),Y (2)|M=4(Â·) about the horizontal axis (21.23) implies that the
probability that the received vector lies in the darkly-shaded region is the same as
3It can be shown that the probability that the observation lies exactly on the boundary of
the region is zero; see Proposition 21.6.2 ahead. We shall thus ignore this possibility.
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

450
Multi-Hypothesis Testing
4
5
3
Ïˆ
Î¸
A
Ï(Î¸) =
A sin Ïˆ
sin(Î¸+Ïˆ)
Figure 21.4: Error analysis for 8-PSK.
the probability that it lies in the lightly-shaded region. We shall thus compute the
probability of the latter and double the result.
Let Ïˆ = Ï€/8 denote half the angle between the constellation points. To carry out
the integration we shall use polar coordinates (r, Î¸) centered on the constellation
point (âˆ’A, 0) corresponding to Message 4:
pMAP(error|M = 4) = 2
 Ï€âˆ’Ïˆ
0
 âˆ
Ï(Î¸)
1
2Ï€Ïƒ2 eâˆ’r2
2Ïƒ2 r dr dÎ¸
= 1
Ï€
 Ï€âˆ’Ïˆ
0
 âˆ
Ï2(Î¸)/(2Ïƒ2)
eâˆ’u du dÎ¸
= 1
Ï€
 Ï€âˆ’Ïˆ
0
eâˆ’Ï2(Î¸)
2Ïƒ2 dÎ¸,
(21.24)
where Ï(Î¸) is the distance we travel from the point (âˆ’A, 0) at angle Î¸ until we
reach the lightly-shaded region, and where the second equality follows using the
substitution u â‰œr2/(2Ïƒ2). Using the Law of Sines and the identity sin(Ï€ âˆ’Î¸) =
sin(Î¸), we have
Ï(Î¸) =
A sin Ïˆ
sin(Î¸ + Ïˆ).
(21.25)
Since the symmetry of the problem implies that the conditional probability of error
conditioned on M = m does not depend on m, it follows from (21.24), (21.25), and
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.5 The Union-of-Events Bound
451
(21.19) that
pâˆ—(error) = 1
Ï€
 Ï€âˆ’Ïˆ
0
e
âˆ’
A2 sin2 Ïˆ
2 sin2(Î¸+Ïˆ)Ïƒ2 dÎ¸,
Ïˆ = Ï€
8 .
(21.26)
21.5
The Union-of-Events Bound
Although simple, the Union-of-Events Bound, or Union Bound for short, is an
extremely powerful and useful bound.4 To derive it, recall that one of the axioms
of probability is that the probability of the union of two disjoint events is the sum
of their probabilities.5 Given two not necessarily disjoint events V and W, we can
express the set V as in Figure 21.5 as the union of those elements of V that are not
in W and those that are both in V and in W:
V = (V \ W) âˆª(V âˆ©W).
(21.27)
Because the sets V \ W and V âˆ©W are disjoint, and because their union is V, it
follows that Pr(V) = Pr(V \ W) + Pr(V âˆ©W), which can also be written as
Pr(V \ W) = Pr(V) âˆ’Pr(V âˆ©W).
(21.28)
Writing the union V âˆªW as the union of two disjoint sets
V âˆªW = W âˆª(V \ W)
(21.29)
as in Figure 21.6, we conclude that
Pr(V âˆªW) = Pr(W) + Pr(V \ W),
(21.30)
which combines with (21.28) to prove that
Pr(V âˆªW) = Pr(V) + Pr(W) âˆ’Pr(V âˆ©W).
(21.31)
Since probabilities are nonnegative, it follows from (21.31) that
Pr(V âˆªW) â‰¤Pr(V) + Pr(W),
(21.32)
which is the Union Bound. This bound can also be extended to derive an upper
bound on the union of more sets. For example, we can show that for three events
U, V, W we have Pr(U âˆªV âˆªW) â‰¤Pr(U)+Pr(V)+Pr(W). Indeed, by ï¬rst applying
the Union Bound to the two sets U and (V âˆªW) we obtain
Pr(U âˆªV âˆªW) = Pr

U âˆª(V âˆªW)

â‰¤Pr(U) + Pr(V âˆªW)
â‰¤Pr(U) + Pr(V) + Pr(W),
4It is also sometimes called Booleâ€™s Inequality.
5Actually the axiom is stronger; it states that this holds also for a countably inï¬nite number
of sets.
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

452
Multi-Hypothesis Testing
V
V \ W
V âˆ©W
=
D
Figure 21.5: Diagram of two nondisjoint sets.
V âˆªW
W
V \ W
=
D
Figure 21.6: Diagram of the union of two nondisjoint sets.
where the last inequality follows by applying the Union Bound (21.32) to the two
sets V and W. One can continue the argument by induction for a ï¬nite6 collection
of events to obtain:
Theorem 21.5.1 (Union-of-Events Bound). If V1, V2, . . . , is a ï¬nite or countably
inï¬nite collection of events then
Pr
	
j
Vj

â‰¤

j
Pr(Vj).
(21.33)
We can think about the LHS of (21.33) as the probability that at least one of
the events V1, V2, . . . occurs and of its RHS as the expected number of events that
occur. Indeed, if for each j we deï¬ne the random variables Xj(Ï‰) = I{Ï‰ âˆˆVj} for
all Ï‰ âˆˆÎ©, then the LHS of (21.33) is equal to Pr
 
j Xj > 0

, and the RHS is

j E[Xj], which can also be expressed as E
 
j Xj

.
After the trivial bound that the probability of any event cannot exceed one, the
Union Bound is probably the most important bound in Probability Theory. What
makes it so useful is the fact that the RHS of (21.33) can be computed without
regard to any dependencies between the events.
Corollary 21.5.2.
(i) If each of a ï¬nite (or countably inï¬nite) collection of events occurs with prob-
ability zero, then their union also occurs with probability zero.
6In fact, this claim holds for a countably inï¬nite number of events.
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.5 The Union-of-Events Bound
453
(ii) If each of a ï¬nite (or countably inï¬nite) collection of events occurs with prob-
ability one, then their intersection also occurs with probability one.
Proof. To prove Part (i) we assume that each of the events V1, V2, . . . is of zero
probability and compute
Pr
	 
j
Vj

â‰¤

j
Pr(Vj)
=

j
0
= 0,
where the ï¬rst inequality follows from the Union Bound, and where the subsequent
equality follows from our assumption that Pr(Vj) = 0, for all j.
To prove Part (ii) we assume that each of the events W1, W2, . . . occurs with
probability one and apply Part (i) to the sets V1, V2, . . ., where Vj is the set-
complement of Wj, i.e., Vj = Î© \ Wj:
Pr
	 E
j
Wj

= 1 âˆ’Pr
	 E
j
Wj
c
= 1 âˆ’Pr
	 
j
Vj

= 1,
where the ï¬rst equality follows because the probabilities of an event and its com-
plement sum to one; the second because the complement of an intersection is the
union of the complements; and the ï¬nal equality follows from Part (i) because
the events Wj are, by assumption, of probability one so their complements are of
probability zero.
21.5.1
Applications to Hypothesis Testing
We shall now use the Union Bound to derive an upper bound on the conditional
probability of error pMAP(error|M = m) of the MAP decoding rule. The bound
we derive is applicable to any decision rule that satisï¬es the hypothesis of Theo-
rem 21.3.1 as expressed in (21.17).
Deï¬ne for every mâ€² Ì¸= m the set Bm,mâ€² âŠ‚Rd by
Bm,mâ€² =

y âˆˆRd : Ï€mâ€² fY|M=mâ€²(y) â‰¥Ï€m fY|M=m(y)

.
(21.34)
Notice that y âˆˆBm,mâ€² does not imply that the MAP rule will guess mâ€²: there may
be a third hypothesis that is a posteriori even more likely than either m or mâ€².
Also, since the inequality in (21.34) is not strict, y âˆˆBm,mâ€² does not imply that
the MAP rule will not guess m: there may be a tie, which may be resolved in favor
of m. As we next argue, what is true is that if m was not guessed by the MAP rule,
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

454
Multi-Hypothesis Testing
then some mâ€² which is not equal to m must have had an a posteriori probability
that is at least as high as that of m:

m was not guessed

=â‡’

Y âˆˆ

mâ€²Ì¸=m
Bm,mâ€²

.
(21.35)
Indeed, if m was not guessed by the MAP rule, then some other message was.
Denoting that other message by mâ€², we note that Ï€mâ€² fY|M=mâ€²(y) must be at least
as large as Ï€m fY|M=m(y) (because otherwise mâ€² would not have been guessed),
so y âˆˆBm,mâ€².
Continuing from (21.35), we note that if the occurrence of an event E1 implies the
occurrence of an event E2, then Pr(E1) â‰¤Pr(E2). Consequently, by (21.35),
pMAP(error|M = m) â‰¤Pr

Y âˆˆ

mâ€²Ì¸=m
Bm,mâ€²
 M = m

= Pr
	 
mâ€²Ì¸=m
'
Ï‰ âˆˆÎ© : Y(Ï‰) âˆˆBm,mâ€²
(  M = m

â‰¤

mâ€²Ì¸=m
Pr

{Ï‰ âˆˆÎ© : Y(Ï‰) âˆˆBm,mâ€²}
 M = m

=

mâ€²Ì¸=m
Pr

Y âˆˆBm,mâ€²  M = m

=

mâ€²Ì¸=m

Bm,mâ€²
fY|M=m(y) dy.
We have thus derived:
Proposition 21.5.3. For the setup of Theorem 21.3.1 let pMAP(error|M = m)
denote the conditional probability of error conditional on M = m of the MAP rule
for guessing M based on Y. Then,
pMAP(error|M = m) â‰¤

mâ€²Ì¸=m
Pr

Y âˆˆBm,mâ€²  M = m

(21.36)
=

mâ€²Ì¸=m

Bm,mâ€²
fY|M=m(y) dy,
(21.37)
where
Bm,mâ€² =

y âˆˆRd : Ï€mâ€² fY|M=mâ€²(y) â‰¥Ï€m fY|M=m(y)

.
(21.38)
This bound is applicable to any decision rule satisfying the hypothesis of Theo-
rem 21.3.1 as expressed in (21.17).
The term Pr[Y âˆˆBm,mâ€² |M = m] has an interesting interpretation. If ties occur
with probability zero, then it corresponds to the conditional probability of error
(given that M = m) incurred by a MAP decoder designed for the binary hypothesis
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.5 The Union-of-Events Bound
455
4
4
4
5
5
5
3
3
3
B4,3
B4,5
B4,3 âˆªB4,5
âˆª
=
Figure 21.7: Error events for 8-PSK conditional on M = 4.
testing problem of guessing whether M = m or M = mâ€² when the prior probability
that M = m is Ï€m/(Ï€m + Ï€mâ€²) and that M = mâ€² is Ï€mâ€²/(Ï€m + Ï€mâ€²).
Alternatively, we can write (21.36) as
pMAP(error|M = m) â‰¤

mâ€²Ì¸=m
Pr

Ï€mâ€² fY|M=mâ€²(Y) â‰¥Ï€m fY|M=m(Y)
 M = m

.
(21.39)
21.5.2
Example: The Union Bound for 8-PSK
We next apply the Union Bound to upper-bound the probability of error associated
with maximum-likelihood decoding of 8-PSK. For concreteness we focus on the
conditional probability of error, conditional on M = 4. We shall see that in this
case the RHS of (21.36) is still an upper bound on the probability of error even if
we do not sum over all mâ€² that diï¬€er from m. Indeed, as we next argue, in upper-
bounding the conditional probability of error of the ML decoder given M = 4, it
suï¬ƒces to sum over mâ€² âˆˆ{3, 5} only.
To show this we ï¬rst note that for this problem the set Bm,mâ€² of (21.34) corresponds
to the set of vectors that are at least as close to (amâ€², bmâ€²) as to (am, bm):
Bm,mâ€² =
'
y âˆˆR2 :

y(1) âˆ’amâ€²2 +

y(2) âˆ’bmâ€²2 â‰¤

y(1) âˆ’am
2 +

y(2) âˆ’bm
2(
.
As seen in Figure 21.7, given M = 4, an error will occur only if the observed
vector Y is at least as close to (a3, b3) as to (a4, b4), or if it is at least as close
to (a5, b5) as to (a4, b4). Thus, conditional on M = 4, an error can occur only if
Y âˆˆB4,3 âˆªB4,5. (If Y /âˆˆB4,3 âˆªB4,5, then an error will certainly not occur. If
Y âˆˆB4,3 âˆªB4,5, then an error may or may not occur. It will not occur in the case
of a tieâ€”corresponding to Y being on the boundary of B4,3 âˆªB4,5â€”provided that
the tie is resolved in favor of M = 4.)
Note that the events Y âˆˆB4,5 and Y âˆˆB4,3 are not mutually exclusive, but,
nevertheless, by the Union-of-Events Bound
pMAP(error|M = 4) â‰¤Pr[Y âˆˆB4,3 âˆªB4,5 |M = 4]
â‰¤Pr[Y âˆˆB4,3 |M = 4] + Pr[Y âˆˆB4,5 |M = 4],
(21.40)
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

456
Multi-Hypothesis Testing
where the ï¬rst inequality follows because, conditional on M = 4, an error can
occur only if y âˆˆB4,3 âˆªB4,5; and where the second inequality follows from the
Union-of-Events Bound. In fact, the ï¬rst inequality holds with equality because,
for this problem, the probability of a tie is zero; see Proposition 21.6.2 ahead.
From our analysis of multi-dimensional binary hypothesis testing (Lemma 20.14.1)
we obtain that
Pr

Y âˆˆB4,3
 M = 4

= Q
	
(a4 âˆ’a3)2 + (b4 âˆ’b3)2
2Ïƒ

= Q
	A
Ïƒ sin
Ï€
8

(21.41)
and
Pr

Y âˆˆB4,5
 M = 4

= Q
	
(a4 âˆ’a5)2 + (b4 âˆ’b5)2
2Ïƒ

= Q
	A
Ïƒ sin
Ï€
8

.
(21.42)
Combining (21.40), (21.41), and (21.42) we obtain
pMAP(error|M = 4) â‰¤2Q
	A
Ïƒ sin
Ï€
8

.
(21.43)
This is only an upper bound and not the exact error probability because the sets
B4,3 and B4,5 are not disjoint so the events Y âˆˆB4,3 and Y âˆˆB4,5 are not
mutually exclusive and the Union-Bound is not tight; see Figure 21.7. Nevertheless,
a momentâ€™s reï¬‚ection will convince the reader that the bound is oï¬€by no more
than a factor of two; see also (21.66) ahead.
For this symmetric problem the conditional probability of error conditional on
M = m does not depend on the message m, and we thus also have by (21.19)
pâˆ—(error) â‰¤2Q
	A
Ïƒ sin
Ï€
8

.
(21.44)
21.5.3
Union-Bhattacharyya Bound
We next derive a bound which is looser than the Union Bound but which is of-
ten easier to evaluate in non-Gaussian settings. It is the multi-hypothesis testing
version of the Bhattacharyya Bound (20.50).
Recall that, by Theorem 21.3.1, any guessing rule whose guess after observing that
Y = yobs is in the set
Ëœ
M(yobs) =
'
Ëœm âˆˆM : Ï€ Ëœm fY|M= Ëœm(yobs) = max
mâ€²

Ï€mâ€² fY|M=mâ€²(yobs)
(
is optimal. To analyze the optimal probability of error pâˆ—(error), we shall analyze
one particular optimal decision rule. This rule is not the MAP rule, but it diï¬€ers
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.5 The Union-of-Events Bound
457
from the MAP rule only in the way it resolves ties. Rather than resolving ties at
random, this rule resolves ties according to the index of the hypothesis: it chooses
the message in
Ëœ
M(yobs) of smallest index. For example, if the messages of highest
a posteriori probability are Messages 7, 9, and 17, i.e., if Ëœ
M(yobs) = {7, 9, 17}, then
it guesses â€œ7.â€ This decision rule may not appeal to the readerâ€™s sense of fairness
but, by Theorem 21.3.1, it is nonetheless optimal. Consequently, if we denote the
conditional probability of error of this decoder by p(error|M = m), then
pâˆ—(error) =

mâˆˆM
Ï€m p(error|M = m).
(21.45)
We next analyze the performance of this decision rule. For every mâ€² Ì¸= m let
ËœBm,mâ€² =

y âˆˆRd : Ï€mâ€² fY|M=mâ€²(y) â‰¥Ï€m fY|M=m(y)

if mâ€² < m,

y âˆˆRd : Ï€mâ€² fY|M=mâ€²(y) > Ï€m fY|M=m(y)

if mâ€² > m.
(21.46)
Notice that
ËœBm,mâ€² = ËœBc
mâ€²,m,
m Ì¸= mâ€².
(21.47)
Conditional on M = m, our detector will err if, and only if, yobs âˆˆâˆªmâ€²Ì¸=m ËœBm,mâ€².
Thus
p(error|M = m) = Pr

Y âˆˆ

mâ€²Ì¸=m
ËœBm,mâ€²
 M = m

= Pr
	 
mâ€²Ì¸=m
'
Ï‰ âˆˆÎ© : Y(Ï‰) âˆˆËœBm,mâ€²
(  M = m

â‰¤

mâ€²Ì¸=m
Pr

Ï‰ âˆˆÎ© : Y(Ï‰) âˆˆËœBm,mâ€²  M = m

=

mâ€²Ì¸=m
Pr

Y âˆˆËœBm,mâ€²  M = m

=

mâ€²Ì¸=m

Ëœ
Bm,mâ€²
fY|M=m(y) dy,
(21.48)
where the inequality follows from the Union Bound. To upper-bound pâˆ—(error) we
use (21.45) and (21.48) to obtain
pâˆ—(error) =
M

m=1
Ï€m p(error|M = m)
â‰¤
M

m=1
Ï€m

mâ€²Ì¸=m

Ëœ
Bm,mâ€²
fY|M=m(y) dy
=
M

m=1

mâ€²>m
	
Ï€m

Ëœ
Bm,mâ€²
fY|M=m(y) dy + Ï€mâ€²

Ëœ
Bmâ€²,m
fY|M=mâ€²(y) dy

=
M

m=1

mâ€²>m
	
Ëœ
Bm,mâ€²
Ï€m fY|M=m(y) dy +

Ëœ
Bc
m,mâ€²
Ï€mâ€² fY|M=mâ€²(y) dy

.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

458
Multi-Hypothesis Testing
=
M

m=1

mâ€²>m

Rd min

Ï€m fY|M=m(y), Ï€mâ€² fY|M=mâ€²(y)

dy
â‰¤
M

m=1

mâ€²>m
âˆšÏ€mÏ€mâ€²

Rd
3
fY|M=m(y)fY|M=mâ€²(y) dy
â‰¤
M

m=1

mâ€²>m
Ï€m + Ï€mâ€²
2

Rd
3
fY|M=m(y)fY|M=mâ€²(y) dy
= 1
2

mâˆˆM

mâ€²Ì¸=m
Ï€m + Ï€mâ€²
2

Rd
3
fY|M=m(y)fY|M=mâ€²(y) dy,
where the equality in the ï¬rst line follows from (21.45); the inequality in the second
line from (21.48); the equality in the third line by rearranging the sum; the equality
in the fourth line from (21.47); the equality in the ï¬fth line from the deï¬nition of
the set ËœBm,mâ€²; the inequality in the sixth line from the inequality min{a, b} â‰¤
âˆš
ab,
which holds for all nonnegative a, b âˆˆR (see (20.48)); the inequality in the seventh
line from the Arithmetic-Geometric Inequality
âˆš
cd â‰¤(c + d)/2, which holds for
all c, d â‰¥0 (see (20.49)); and the ï¬nal equality by the symmetry of the summand.
We have thus obtained the Union-Bhattacharyya Bound:
pâˆ—(error) â‰¤1
4

mâˆˆM

mâ€²Ì¸=m
(Ï€m + Ï€mâ€²)

Rd
3
fY|M=m(y)fY|M=mâ€²(y) dy.
(21.49)
For a priori equally likely hypotheses it takes the form
pâˆ—(error) â‰¤
1
2M

mâˆˆM

mâ€²Ì¸=m

Rd
3
fY|M=m(y)fY|M=mâ€²(y) dy,
(21.50)
which is the Union-Bhattacharyya Bound for M-ary hypothesis testing with a
uniform prior.
21.6
Multi-Dimensional M-ary Gaussian Hypothesis Testing
We next use Theorem 21.3.3 to study the multi-hypothesis testing version of the
problem we addressed in Section 20.14. We begin with the problem setup and then
proceed to derive the MAP decision rule. We then assess the performance of this
rule by deriving an upper bound and a lower bound on its probability of error.
21.6.1
Problem Setup
A random variable M takes values in the set M = {1, . . . , M} with a nondegen-
erate prior (21.4).
We wish to guess M based on an observation consisting of
a random column-vector Y taking value in RJ whose components are given by
Y (1), . . . , Y (J).7 For typographical reasons we denote the observed realization of
7Our observation now takes values in RJ and not as before in Rd. My excuse for using J
instead of d is that later, when we refer to this section, d will have a diï¬€erent meaning and
choosing J here reduces the chance of confusion later on.
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.6 Multi-Dimensional M-ary Gaussian Hypothesis Testing
459
Y by y, instead of yobs. For every m âˆˆM we have that, conditional on M = m,
the components of Y are independent Gaussians, with Y (j) âˆ¼N(s(j)
m , Ïƒ2), where
sm is some deterministic vector of J components s(1)
m , . . . , s(J)
m , and where Ïƒ2 > 0.
Recalling the density of the univariate Gaussian distribution (19.6) and using the
conditional independence of the components of Y given M = m, we can express the
conditional density fY|M=m(y) of the vector Y at every point y = (y(1), . . . , y(J))T
in RJ as
fY|M=m(y) =
J
@
j=1
-
1
âˆš
2Ï€Ïƒ2 exp
	
âˆ’

y(j) âˆ’s(j)
m
2
2Ïƒ2

.
.
(21.51)
21.6.2
Optimal Guessing Rule
Using Theorem 21.3.3 we obtain that, having observed y = (y(1), . . . , y(J))T âˆˆRJ,
an optimal decision rule is the MAP rule, which picks uniformly at random an
element from the set
Ëœ
M(y) =
1
Ëœm âˆˆM : Ï€ Ëœm fY|M= Ëœm(y) = max
mâ€²âˆˆM
'
Ï€mâ€² fY|M=mâ€²(y)
(2
=
1
Ëœm âˆˆM : ln

Ï€ Ëœm fY|M= Ëœm(y)

= max
mâ€²âˆˆM
'
ln

Ï€mâ€² fY|M=mâ€²(y)
(2
, (21.52)
where the second equality follows from the strict monotonicity of the logarithm.
We next obtain a more explicit description of
Ëœ
M(y) for our setup. By (21.51),
ln

Ï€m fY|M=m(y)

= ln Ï€m âˆ’J
2 ln(2Ï€Ïƒ2) âˆ’
1
2Ïƒ2
J

j=1

y(j) âˆ’s(j)
m
2.
(21.53)
The term (J/2) ln(2Ï€Ïƒ2) is a constant term that does not depend on the hypothesis.
Consequently, it does not inï¬‚uence the set of messages that attain the highest score.
(The tallest student in the class is the same irrespective of whether the height of all
the students is measured when they are barefoot or when they are all wearing the
one-inch heel school uniform shoes. The heel can only make a diï¬€erence if diï¬€erent
students wear shoes of diï¬€erent heel height.) Thus,
Ëœ
M(y)=

Ëœm âˆˆM: ln Ï€ Ëœmâˆ’
J

j=1

y(j) âˆ’s(j)
Ëœm
2
2Ïƒ2
= max
mâ€²âˆˆM
1
ln Ï€mâ€²âˆ’
J

j=1

y(j) âˆ’s(j)
mâ€²
2
2Ïƒ2
2!
.
The expression for
Ëœ
M(y) can be further simpliï¬ed if M is a priori uniformly
distributed. In this case we have
Ëœ
M(y) =

Ëœm âˆˆM : âˆ’
J

j=1

y(j) âˆ’s(j)
Ëœm
2
2Ïƒ2
= max
mâ€²âˆˆM
1
âˆ’
J

j=1

y(j) âˆ’s(j)
mâ€²
2
2Ïƒ2
2!
=

Ëœm âˆˆM :
J

j=1

y(j) âˆ’s(j)
Ëœm
2 = min
mâ€²âˆˆM
1
J

j=1

y(j) âˆ’s(j)
mâ€²
2
2!
,
M uniform,
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

460
Multi-Hypothesis Testing
where the ï¬rst equality follows because when M is uniform the additive term ln Ï€m
is given by ln(1/M) and hence does not depend on the hypothesis; and where the
second equality follows because changing the sign of all the elements of a set changes
the largest ones to the smallest ones, and by noting that scaling the score by 2Ïƒ2
does not change the highest scoring messages (because we assumed that Ïƒ2 > 0).
If we interpret the quantity
âˆ¥y âˆ’smâˆ¥=
A
B
B
C
J

j=1

y(j) âˆ’s(j)
m
2
as the Euclidean distance between the vector y and the vector sm, then we see that,
for a uniform prior on M, it is optimal to guess the message m whose corresponding
mean vector sm is closest to the observed vector y. Notice that to implement this
â€œnearest-neighborâ€ decision rule we do not need to know the value of Ïƒ2.
We next show that if, in addition to assuming a uniform prior on M, we also
assume that the vectors s1, . . . , sM all have the same norm, i.e.,
âˆ¥s1âˆ¥= âˆ¥s2âˆ¥= Â· Â· Â· = âˆ¥sMâˆ¥,
(21.54)
then
Ëœ
M(y) =

Ëœm âˆˆM :
J

j=1
y(j)s(j)
Ëœm = max
mâ€²âˆˆM
1
J

j=1
y(j)s(j)
mâ€²
2!
,
so the MAP decision rules guesses the message m whose mean vector sm has
the â€œhighest correlationâ€ with the received vector y. To see this, we note that
because M has a uniform prior the â€œnearest-neighborâ€ decoding rule is optimal,
and we then expand
âˆ¥y âˆ’smâˆ¥2 =
J

j=1

y(j) âˆ’s(j)
m
2
=
J

j=1

y(j)2 âˆ’2
J

j=1
y(j)s(j)
m +
J

j=1

s(j)
m
2,
where the ï¬rst term does not depend on the hypothesis and where, by (21.54), the
third term also does not depend on the hypothesis.
We summarize our ï¬ndings in the following proposition.
Proposition 21.6.1. Consider the problem described in Section 21.6.1 of guess-
ing M based on the observation y.
(i) It is optimal to form the guess based on y = (y(1), . . . , y(J))T by choosing
uniformly at random from the set

Ëœm âˆˆM: ln Ï€ Ëœm âˆ’
J

j=1

y(j) âˆ’s(j)
Ëœm
2
2Ïƒ2
= max
mâ€²âˆˆM
1
ln Ï€mâ€² âˆ’
J

j=1

y(j) âˆ’s(j)
mâ€²
2
2Ïƒ2
2!
.
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.6 Multi-Dimensional M-ary Gaussian Hypothesis Testing
461
(ii) If M is uniformly distributed, then this rule is equivalent to the â€œnearest-
neighborâ€ decoding rule of picking uniformly at random an element of the
set
'
Ëœm âˆˆM : âˆ¥y âˆ’s Ëœmâˆ¥= min
mâ€²âˆˆM

âˆ¥y âˆ’smâ€²âˆ¥
(
.
(iii) If, in addition to M being uniform, we also assume that the mean vectors
satisfy (21.54), then this rule is equivalent to the â€œmaximum-correlationâ€ rule
of picking at random an element of the set

Ëœm âˆˆM :
J

j=1
y(j)s(j)
Ëœm = max
mâ€²âˆˆM
1
J

j=1
y(j)s(j)
mâ€²
2!
.
We next show that if the mean vectors s1, . . . , sM are distinct in the sense that for
every pair mâ€² Ì¸= mâ€²â€² in M there exists at least one component where the vectors
smâ€² and smâ€²â€² diï¬€er, i.e.,
âˆ¥smâ€² âˆ’smâ€²â€²âˆ¥> 0,
mâ€² Ì¸= mâ€²â€²,
then the probability of ties is zero. That is, we will show that the probability of
observing a vector y for which the set
Ëœ
M(y) (21.52) has more than one element
is zero. Stated in yet another way, the probability that the observable Y will be
such that the MAP will require randomization is zero. Stated one last time:
Proposition 21.6.2. If the mean vectors s1, . . . , sM in our setup are distinct, then
with probability one the observed vector y is such that there is a unique message of
highest a posteriori probability.
Proof. Conditional on Y = y, associate with each message m âˆˆM the score
ln

Ï€m fY|M=m(y)

. We need to show that the probability of the observation y
being such that at least two messages attain the highest score is zero. Instead, we
shall prove the stronger statement that the probability of two messages attaining
the same score (be it maximal or not) is zero.
We ï¬rst show that it suï¬ƒces to prove that for every m âˆˆM and for every pair of
messages mâ€² Ì¸= mâ€²â€², we have that, conditional on M = m, the probability that mâ€²
and mâ€²â€² attain the same score is zero, i.e.,
Pr

score of Message mâ€² = score of Message mâ€²â€²  M = m

= 0,
mâ€² Ì¸= mâ€²â€².
(21.55)
Indeed, once we show (21.55), it will follow that the unconditional probability that
Message mâ€² attains the same score as Message mâ€²â€² is zero, i.e.,
Pr

score of Message mâ€² = score of Message mâ€²â€²
= 0,
mâ€² Ì¸= mâ€²â€²,
(21.56)
because
Pr

score of Message mâ€² = score of Message mâ€²â€²
=

mâˆˆM
Ï€m Pr

score of Message mâ€² = score of Message mâ€²â€²  M = m

.
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

462
Multi-Hypothesis Testing
But (21.56) implies that the probability that any two or more messages attain the
highest score is zero because
Pr(two or more messages attain the highest score)
= Pr
-

mâ€²,mâ€²â€²âˆˆM
mâ€²Ì¸=mâ€²â€²

mâ€² and mâ€²â€² attain the highest score

.
â‰¤

mâ€²,mâ€²â€²âˆˆM
mâ€²Ì¸=mâ€²â€²
Pr

mâ€² and mâ€²â€² attain the highest score

â‰¤

mâ€²,mâ€²â€²âˆˆM
mâ€²Ì¸=mâ€²â€²
Pr

mâ€² and mâ€²â€² attain the same score

,
where the ï¬rst equality follows because more than one message attains the high-
est score if, and only if, there exist two distinct messages mâ€² and mâ€²â€² that attain
the highest score; the subsequent inequality follows from the Union Bound (Theo-
rem 21.5.1); and the ï¬nal inequality by noting that if mâ€² and mâ€²â€² both attain the
highest score, then they both achieve the same score.
Having established that in order to complete the proof it suï¬ƒces to establish
(21.55), we proceed to do so. By (21.53) we obtain, upon opening the square,
that the observation Y results in Messages mâ€² and mâ€²â€² obtaining the same score if,
and only if,
1
Ïƒ2
J

j=1
Y (j)
s(j)
mâ€² âˆ’s(j)
mâ€²â€²

= ln Ï€mâ€²â€²
Ï€mâ€² +
1
2Ïƒ2

âˆ¥smâ€²âˆ¥2 âˆ’âˆ¥smâ€²â€²âˆ¥2
.
(21.57)
We next show that, conditional on M = m, the probability that Y satisï¬es (21.57)
is zero. To that end we note that, conditional on M = m, the random variables
Y (1), . . . , Y (J) are independent random variables with Y (j) being Gaussian with
mean s(j)
m and variance Ïƒ2; see (21.51). Consequently, by Proposition 19.7.3, we
have that, conditional on M = m, the LHS of (21.57) is a Gaussian random variable
of variance
1
Ïƒ2 âˆ¥smâ€² âˆ’smâ€²â€²âˆ¥2,
which is positive because mâ€² Ì¸= mâ€²â€² and because we assumed that the mean vectors
are distinct.
It follows that, conditional on M = m, the LHS of (21.57) is a
Gaussian random variable of positive variance, and hence has zero probability of
being equal to the deterministic number on the RHS of (21.57). This proves (21.55),
and hence concludes the proof.
21.6.3
The Union Bound
We next use the Union Bound to upper-bound the optimal probability of error
pâˆ—(error). By (21.39)
pMAP(error|M = m) â‰¤

mâ€²Ì¸=m
Pr

Ï€mâ€² fY|M=mâ€²(Y) â‰¥Ï€m fY|M=m(Y)
 M = m

.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.6 Multi-Dimensional M-ary Gaussian Hypothesis Testing
463
=

mâ€²Ì¸=m
Q
	âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥sm âˆ’smâ€²âˆ¥ln Ï€m
Ï€mâ€²

,
(21.58)
where the equality follows from Lemma 20.14.1. From this and from the optimality
of the MAP rule (21.19) we thus obtain
pâˆ—(error) â‰¤

mâˆˆM

mâ€²Ì¸=m
Ï€mQ
	âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥sm âˆ’smâ€²âˆ¥ln Ï€m
Ï€mâ€²

.
(21.59)
If M is uniform, these bounds simplify to:
pMAP(error|M = m) â‰¤

mâ€²Ì¸=m
Q
	âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ

,
M uniform,
(21.60)
pâˆ—(error) â‰¤1
M

mâˆˆM

mâ€²Ì¸=m
Q
	âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ

,
M uniform.
(21.61)
21.6.4
A Lower Bound
We next derive a lower bound on the optimal error probability pâˆ—(error). We do so
by lower-bounding the conditional probability of error pMAP(error|M = m) of the
MAP rule and by then using this lower bound to derive a lower bound on pâˆ—(error)
via (21.19).
We note that if Message mâ€² attains a score that is strictly higher than the one
attained by Message m, then the MAP decoder will surely not guess â€œM = m.â€
(The MAP may or may not guess â€œM = mâ€²â€ depending on the score associated
with messages other than m and mâ€².) Thus, for each message mâ€² Ì¸= m we have
pMAP(error|M = m) â‰¥Pr

Ï€mâ€² fY|M=mâ€²(Y) > Ï€m fY|M=m(Y)
 M = m

(21.62)
= Q
	âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥sm âˆ’smâ€²âˆ¥ln Ï€m
Ï€mâ€²

,
(21.63)
where the equality follows from Lemma 20.14.1.
Noting that (21.63) holds for all mâ€² Ì¸= m, we can choose mâ€² to get the tightest
bound. This yields the lower bound
pMAP(error|M = m) â‰¥
max
mâ€²âˆˆM\{m} Q
	âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥sm âˆ’smâ€²âˆ¥ln Ï€m
Ï€mâ€²

(21.64)
and hence, by (21.19),
pâˆ—(error) â‰¥

mâˆˆM
Ï€m
max
mâ€²âˆˆM\{m} Q
	âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ
+
Ïƒ
âˆ¥sm âˆ’smâ€²âˆ¥ln Ï€m
Ï€mâ€²

.
(21.65)
For uniform M this expression can be simpliï¬ed by noting that the Q-function is
strictly decreasing:
pMAP(error|M = m) â‰¥Q
	
min
mâ€²âˆˆM\{m}
âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ

,
M uniform,
(21.66)
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

464
Multi-Hypothesis Testing
pâˆ—(error) â‰¥1
M

mâˆˆM
Q
	
min
mâ€²âˆˆM\{m}
âˆ¥sm âˆ’smâ€²âˆ¥
2Ïƒ

,
M uniform.
(21.67)
21.7
Additional Reading
For additional reading on multi-hypothesis testing see the recommended reading
for Chapter 20. The problem of assessing the optimal probability of error for the
multi-dimensional M-ary Gaussian hypothesis testing problem of Section 21.6 has
received extensive attention in the coding literature. For a survey of these results
see (Sason and Shamai, 2006).
21.8
Exercises
Exercise 21.1 (Ternary Gaussian Detection). Consider the following special case of the
problem discussed in Section 21.6. Here M is uniformly distributed over the set {1, 2, 3},
and the mean vectors s1, s2, s3 are given by
s1 = 0,
s2 = s,
s3 = âˆ’s,
where s is some deterministic nonzero vector in RJ. Find the conditional probability of
error of the MAP rule conditional on each hypothesis.
Exercise 21.2 (4-PSK Detection). Consider the setup of Section 21.4 with M = 4 and
(a1, b1) = (0, A), (a2, b2) = (âˆ’A, 0), (a3, b3) = (0, âˆ’A), (a4, b4) = (A, 0).
(i) Sketch the decision regions of the MAP decision rule.
(ii) Using the Q-function, express the conditional probabilities of error of this rule
conditional on each hypothesis.
(iii) Compute an upper bound on pMAP(error|M = 1) using Proposition 21.5.3. Indicate
on the ï¬gure which events are summed two or three times. Can you improve the
bound by summing only over a subset of the alternative hypotheses?
Hint: In Part (ii) ï¬rst ï¬nd the probability of correct detection.
Exercise 21.3 (A 7-ary QAM problem). Consider the problem addressed in Section 21.4
in the special case where M = 7 and
am = A cos
	2Ï€m
6

,
bm = A sin
	2Ï€m
6

,
m = 1, . . . , 6,
a7 = 0,
b7 = 0.
(i) Illustrate the decision regions of the MAP (nearest-neighbor) guessing rule.
(ii) Let Z = (Z(1), Z(2))T be a random vector whose components are IID N

0, Ïƒ2
.
Show that for every message m âˆˆ{1, . . . , 7} the conditional probability of error
pMAP(error|M = m) can be upper-bounded by the probability that the Euclidean
norm of Z exceeds A/2. Calculate this probability.
(iii) What is the upper bound on pMAP(error|M = m) that Proposition 21.5.3 yields in
this case? Can you improve it by including fewer terms?
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.8 Exercises
465
(iv) Compare the diï¬€erent bounds.
See (Viterbi and Omura, 1979, Chapter 2, Problem 2.2).
Exercise 21.4 (Orthogonal Mean Vectors). Let M be uniformly distributed over the set
M = {1, . . . , M}. Let the observable Y be a random J-vector. Conditional on M = m,
the observable Y is given by
Y =
âˆš
Es Ï†m + Z,
where Z is a random J-vector whose components are IID N

0, Ïƒ2
, and where Ï†1, . . . , Ï†M
are orthonormal in the sense that
âŸ¨Ï†mâ€², Ï†mâ€²â€²âŸ©E = I{mâ€² = mâ€²â€²},
mâ€², mâ€²â€² âˆˆM.
Show that
pMAP(error|M = m) = 1 âˆ’
1
âˆš
2Ï€
 âˆ
âˆ’âˆ

1 âˆ’Q(Î¾)
Mâˆ’1 eâˆ’(Î¾âˆ’Î±)2
2
dÎ¾,
(21.68)
where Î± = âˆšEs/Ïƒ.
Hint: Requires familiarity with Gaussian vectors (Chapter 23).
Exercise 21.5 (Equi-Energy Constellations). Consider the setup of Section 21.6.1 with a
uniform prior and with âˆ¥s1âˆ¥2 = Â· Â· Â· = âˆ¥sMâˆ¥2 = Es. Show that the optimal probability of
correct decoding is given by
pâˆ—(correct) = 1
M exp
	
âˆ’Es
2Ïƒ2

E
'
exp
	 1
Ïƒ2 max
m âŸ¨V, smâŸ©E

(
,
(21.69)
where V is a random J-vector whose components are IID N

0, Ïƒ2
. We recommend the
following approach. Let D1, . . . , DM be a partition of RJ such that for every m âˆˆM,
	
y âˆˆDm

=â‡’
	
âŸ¨y, smâŸ©E = max
mâ€² âŸ¨y, smâ€²âŸ©E

.
(i) Show that
pâˆ—(correct) = 1
M

mâˆˆM
Pr
$
Y âˆˆDm
 M = m
%
.
(ii) Show that the RHS of the above can be written as
1
M exp
	
âˆ’Es
2Ïƒ2

Â·

RJ
1
(2Ï€Ïƒ2)J/2 exp
	
âˆ’âˆ¥yâˆ¥2
2Ïƒ2

 
mâˆˆM
I{y âˆˆDm} exp
	 1
Ïƒ2 âŸ¨y, smâŸ©E


dy.
(iii) Finally show that

mâˆˆM
I{y âˆˆDm} exp
	 1
Ïƒ2 âŸ¨y, smâŸ©E

= exp
	 1
Ïƒ2 max
mâˆˆM âŸ¨y, smâŸ©E

,
y âˆˆRJ.
See also Problem 23.13.
Exercise 21.6 (When Is the Union Bound Tight?). Under what conditions on the events
V1, V2, . . . is the Union Bound (21.33) tight?
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

466
Multi-Hypothesis Testing
Exercise 21.7 (The Union of Independent Events). Show that if the events V1, V2, . . . , Vn
are independent then
Pr

n
.
j=1
Vj

= 1 âˆ’
n
/
j=1

1 âˆ’Pr(Vj)

.
Exercise 21.8 (A Lower Bound on the Probability of a Union). Show that the probability
of the union of n events V1, . . . , Vn can be lower-bounded by
Pr

n
.
j=1
Vj

â‰¥
n

j=1
Pr

Vj

âˆ’
nâˆ’1

j=1
n

â„“=j+1
Pr

Vj âˆ©Vâ„“

.
Inequalities of this nature are sometimes called Bonferroni-Type Inequalities.
Exercise 21.9 (The Inclusion-Exclusion Formula). Show that the probability of the union
of n events V1, V2, . . . , Vn is given by
Pr

n
.
j=1
Vj

=

j
Pr

Vj

âˆ’
 
j<k
Pr

Vj âˆ©Vk

+
  
j<k<â„“
Pr

Vj âˆ©Vk âˆ©Vâ„“

âˆ’Â· Â· Â· + (âˆ’1)n+1 Pr

V1 âˆ©V2 âˆ©Â· Â· Â· âˆ©Vn

and that (21.31) is a special case of this formula.
Hint: Show that for every Ï‰ âˆˆÎ© we have I{Ï‰ âˆˆV} = 1 âˆ’0n
j=1

1 âˆ’I{Ï‰ âˆˆVj}

, where
V = âˆªjVj.
Expand the product and take expectations.
Alternatively, use (21.31) and
induction.
Exercise 21.10 (de Caenâ€™s Inequality). Let X be a RV taking values in the ï¬nite set X,
and let {Ai}iâˆˆI be a ï¬nite family of subsets (not necessarily disjoint) of X:
Ai âŠ†X,
i âˆˆI.
Deï¬ne
Pr(Ai) â‰œPr[X âˆˆAi],
i âˆˆI,
deg(x) â‰œ#{i âˆˆI : x âˆˆAi},
x âˆˆX,
where # B denotes the cardinality of a set B.
(i) Show that
Pr
 .
iâˆˆI
Ai

=

iâˆˆI

xâˆˆAi
Pr[X = x]
deg(x)
.
(ii) Use the Cauchy-Schwarz Inequality to show that for every i âˆˆI,
) 
xâˆˆAi
Pr[X = x]
deg(x)
*) 
xâˆˆAi
Pr[X = x] deg(x)
*
â‰¥
) 
xâˆˆAi
Pr[X = x]
*2
.
(iii) Use Parts (i) and (ii) to show that
Pr
 .
iâˆˆI
Ai

â‰¥

iâˆˆI
	
xâˆˆAi Pr[X = x]

2

jâˆˆI

xâ€²âˆˆAiâˆ©Aj Pr[X = xâ€²].
.023
14:36:37, subject to the Cambridge Core terms of use, available at

21.8 Exercises
467
(iv) Conclude that
Pr
 .
iâˆˆI
Ai

â‰¥

iâˆˆI
Pr(Ai)2

jâˆˆI Pr(Ai âˆ©Aj).
This is de Caenâ€™s Bound (de Caen, 1997).
Exercise 21.11 (Asymptotic Tightness of the Union Bound). Consider the hypothesis
testing problem of Section 21.6 when the prior is uniform and the mean vectors s1, . . . , sM
are distinct. Show that the Union Bound of (21.60) is asymptotically tight in the sense
that the limiting ratio of the RHS of (21.60) to the LHS tends to one as Ïƒ tends to zero.
Hint: Use Exercise 21.8.
.023
14:36:37, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

Chapter 22
Suï¬ƒcient Statistics
22.1
Introduction
In laymanâ€™s terms, a suï¬ƒcient statistic for guessing M based on the observable Y
is a random variable or a collection of random variables that contains all the infor-
mation in Y that is relevant for guessing M. This is a particularly useful concept
when the suï¬ƒcient statistic is more concise than the observables. For example, if
we observe the results of a thousand coin tosses Y1, . . . , Y1000 and we wish to test
whether the coin is fair or has a bias of 1/4, then a suï¬ƒcient statistic turns out
to be the number of â€œheadsâ€among the outcomes Y1, . . . , Y1000.1 Another example
was encountered in Section 20.12. There the observable was a two-dimensional
random vector, and the suï¬ƒcient statistic summarized the information that was
relevant for guessing H in a scalar random variable; see (20.69).
In this chapter we provide a formal deï¬nition of suï¬ƒcient statistics in the multi-
hypothesis setting and explore the concept in some detail. We shall see that our
deï¬nition is compatible with Deï¬nition 20.12.2, which we gave for the binary case.
We only address the case where the observations take value in the d-dimensional
Euclidean space Rd. Also, we only treat the case of guessing among a ï¬nite number
of alternatives. We thus consider a ï¬nite set of messages
M = {1, . . . , M},
(22.1)
where M â‰¥2, and we assume that associated with each message m âˆˆM is a density
fY|M=m(Â·) on Rd, i.e., a nonnegative Borel measurable function that integrates to
one.
The concept of suï¬ƒcient statistics is deï¬ned for the family of densities
fY|M=m(Â·),
m âˆˆM;
(22.2)
it is unrelated to a prior. But when we wish to use it in the context of hypothesis
testing we need to introduce a probabilistic setting. If, in addition to the family
{fY|M=m(Â·)}mâˆˆM, we introduce a prior {Ï€m}mâˆˆM, then we can discuss the pair
1Testing whether a coin is fair or not is a more complicated hypothesis testing problem of a
kind that we shall not address. It falls under the category of â€œcomposite hypothesis testing.â€
468
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.2 Deï¬nition and Main Consequence
469
(M, Y), where Pr[M = m] = Ï€m, and where, conditionally on M = m, the dis-
tribution of Y is of density fY|M=m(Â·). Thus, once we have introduced a prior
{Ï€m}mâˆˆM we can, for example, discuss the density fY(Â·) of Y as in (21.11)
fY(y) =

mâˆˆM
Ï€m fY|M=m(y),
y âˆˆRd,
(22.3)
and the conditional distribution of M conditional on Y = y as in (21.12)
Pr[M = m|Y = y] â‰œ
â§
âª
â¨
âª
â©
Ï€m fY|M=m(y)
fY(y)
if fY(y) > 0,
1
M
otherwise,
m âˆˆM, y âˆˆRd.
(22.4)
22.2
Deï¬nition and Main Consequence
In this section we shall deï¬ne suï¬ƒcient statistics for a family of densities (22.2).
We shall then state the main result about this notion, namely, that there is no loss
of optimality in basing oneâ€™s guess on a suï¬ƒcient statistic.
Very roughly, T(Â·) (or sometimes T(Y)) forms a suï¬ƒcient statistic for guessing M
based on Y if there exists a black box that, when fed T(yobs) (but not yobs) and
any prior {Ï€m} on M produces the a posteriori distribution of M given Y = yobs.
For technical reasons we make two exceptions. While the black box must always
produce a probability vector, we only require that this vector be the a posteriori
distribution of M given Y = yobs for observations yobs that satisfy

mâˆˆM
Ï€m fY|M=m(yobs) > 0
(22.5)
and that lie outside some prespeciï¬ed set Y0 âŠ‚Rd of Lebesgue measure zero. Thus,
if yobs is in Y0 or if (22.5) is violated, then the output of the black box can be any
probability vector. The exception set Y0 is not allowed to depend on {Ï€m}. Since
it is of Lebesgue measure zero, the conditional probability that the observation Y
lies in Y0 is zero:
Pr

Y âˆˆY0
 M = m

= 0,
m âˆˆM.
(22.6)
Note that the black box need not indicate whether yobs is in Y0 and/or whether
(22.5) holds. Figure 22.1 depicts such a black box.
Deï¬nition 22.2.1 (Suï¬ƒcient Statistics for M Densities). We say that a mapping
T : Rd â†’Rdâ€² forms a suï¬ƒcient statistic for the densities fY|M=1(Â·), . . . , fY|M=M(Â·)
on Rd if it is Borel measurable and if for some Y0 âŠ‚Rd of Lebesgue measure zero we
have that for every prior {Ï€m} there exist M Borel measurable functions from Rdâ€²
to [0, 1]
T(yobs) 	â†’Ïˆm

{Ï€m}, T(yobs)

,
m âˆˆM,
such that the vector

Ïˆ1

{Ï€m}, T(yobs)

, . . . , ÏˆM

{Ï€m}, T(yobs)
T
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

470
Suï¬ƒcient Statistics
yobs
T(Â·)
T(yobs)

Ï€m}mâˆˆM
Black Box
	
Ïˆ1

{Ï€m} , T(yobs)

, . . . , ÏˆM

{Ï€m} , T(yobs)

T
Figure 22.1:
A black box that when fed any prior {Ï€m} and T(yobs) (but
not the observation yobs directly) produces a probability vector that is equal to
(Pr[M = 1|Y = yobs], . . . , Pr[M = M|Y = yobs])T whenever both the condition

mâˆˆM Ï€m fY |M=m(yobs) > 0 and the condition yobs /âˆˆY0 are satisï¬ed.
is a probability vector and such that this probability vector is equal to

Pr[M = 1|Y = yobs], . . . , Pr[M = M|Y = yobs]
T
(22.7)
whenever both the condition yobs /âˆˆY0 and the condition
M

m=1
Ï€m fY|M=m(yobs) > 0
(22.8)
are satisï¬ed. Here (22.7) is computed for M having the prior {Ï€m} and for the
conditional law of Y given M corresponding to the given densities.
The main result regarding suï¬ƒcient statistics is that if T(Â·) forms a suï¬ƒcient
statistic, thenâ€”even if the transformation T(Â·) is not reversibleâ€”there is no loss
of optimality in basing oneâ€™s guess on T(Y).
Proposition 22.2.2 (Guessing Based on T(Y) Is Optimal). If T : Rd â†’Rdâ€²
is a suï¬ƒcient statistic for the M densities {fY|M=m(Â·)}mâˆˆM, then, given any
prior {Ï€m}, there exists an optimal decision rule that bases its decision on T(Y).
Proof. To prove the proposition we shall exhibit a decision rule that is based
on T(Y) and that mimics the MAP rule based on Y. Since the latter is optimal
(Theorem 21.3.3), our proposed rule must also be optimal. Let {Ïˆm(Â·)} be as in
Deï¬nition 22.2.1. Given Y = yobs, the proposed decoder considers the set of all
messages Ëœm satisfying
Ïˆ Ëœm

{Ï€m}, T(yobs)

= max
mâ€²âˆˆM Ïˆmâ€²
{Ï€m}, T(yobs)

(22.9)
and picks uniformly at random from this set.
We next argue that this decision rule is optimal. To that end we shall show that,
with probability one, this guessing rule is the same as the MAP rule for guessing M
based on Y. Indeed, the guess produced by this rule is identical to the one produced
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.3 Equivalent Conditions
471
by the MAP rule whenever yobs satisï¬es (22.8) and lies outside Y0.
Since the
probability that Y satisï¬es (22.8) is, by (21.13), one, and since the probability
that Y is outside Y0 is, by (22.6), also one, it follows from Corollary 21.5.2 that
the probability that Y satisï¬es both (22.8) and the condition Y /âˆˆY0 is also one.
Thus, the proposed guessing rule, which bases its decision only on T(yobs) and
on the prior has the same performance as the (optimal) MAP decision rule for
guessing M based on Y.
22.3
Equivalent Conditions
In this section we derive a number of important equivalent deï¬nitions for suï¬ƒcient
statistics. These will further clarify the concept and will also be useful in identifying
suï¬ƒcient statistics. We shall try to state the theorems rigorously, but our proofs
will be mostly heuristic. Rigorous proofs require some Measure Theory that we
do not wish to assume. For a rigorous measure-theoretic treatment of this topic
see (Halmos and Savage, 1949), (Lehmann and Romano, 2005, Section 2.6), or
(Billingsley, 1995, Section 34).2
22.3.1
The Factorization Theorem
The following characterization is useful because it is purely algebraic. It explores
the form that the densities {fY|M=m(Â·)} must have for T(Y) to form a suï¬ƒcient
statistic. Roughly speaking, T(Â·) is suï¬ƒcient if the densities in the family all have
the form of a product of two functions, where the ï¬rst function depends on the
message and on T(y), and where the second function does not depend on the
message but may depend on y. We allow, however, an exception set Y0 âŠ‚Rd of
Lebesgue measure zero, so we only require that for every m âˆˆM
fY|M=m(y) = gm

T(y)

h(y),
y /âˆˆY0.
(22.10)
Note that if such a factorization exists, then it also exists with the additional
requirement that the functions be nonnegative. Indeed, if (22.10) holds, then by
the nonnegativity of the densities
fY|M=m(y) =
fY|M=m(y)

=
gm

T(y)

h(y)
,
y /âˆˆY0
=
gm

T(y)
 |h(y)|,
y /âˆˆY0,
thus yielding a factorization with the nonnegative functions

y 	â†’
gm

T(y)

mâˆˆM
and
y 	â†’|h(y)|.
Limiting ourselves to nonnegative factorizations, as we henceforth shall, is helpful
in manipulating inequalities where multiplication by negative numbers requires
2Our setting is technically easier because we only consider the case where M is ï¬nite and
because we restrict the observation space to Rd.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

472
Suï¬ƒcient Statistics
changing the direction of the inequality. For our setting the Factorization Theorem
can be stated as follows.3
Theorem 22.3.1 (The Factorization Theorem). A Borel measurable function
T : Rd â†’Rdâ€² forms a suï¬ƒcient statistic for the M densities {fY|M=m(Â·)}mâˆˆM
on Rd if, and only if, there exists a set Y0 âŠ‚Rd of Lebesgue measure zero and non-
negative Borel measurable functions g1, . . . , gM : Rdâ€² â†’[0, âˆ) and h: Rd â†’[0, âˆ)
such that for every m âˆˆM
fY|M=m(y) = gm

T(y)

h(y),
y âˆˆRd \ Y0.
(22.11)
Proof. We begin by showing that if T(Â·) is a suï¬ƒcient statistic then there exists a
factorization of the form (22.11). Let the set Y0 and the functions {Ïˆm(Â·)} be as in
Deï¬nition 22.2.1. Pick some ËœÏ€1, . . . , ËœÏ€M > 0 that sum to one, e.g., ËœÏ€m = 1/M for
all m âˆˆM, and let M be of the prior {ËœÏ€m}, so Pr[M = m] = ËœÏ€m for all m âˆˆM.
Let the conditional law of Y given M be as speciï¬ed by the given densities so, in
particular,
fY(y) =

mâˆˆM
ËœÏ€m fY|M=m(y),
y âˆˆRd.
(22.12)
Since {ËœÏ€m} are strictly positive, it follows from (22.12) that

fY(y) = 0

=â‡’

fY|M=m(y) = 0,
m âˆˆM

.
(22.13)
(The only way the sum of nonnegative numbers can be zero is if they are all zero.
Thus, fY(y) = 0 always implies that all the terms {ËœÏ€m fY|M=m(y)} are zero. But
if {ËœÏ€m} are strictly positive, then this implies that all the terms {fY|M=m(y)} are
zero.)
By the deï¬nition of the functions {Ïˆm(Â·)} and of the conditional probability (22.4),
we have for every m âˆˆM
Ïˆm

ËœÏ€1, . . . , ËœÏ€M, T(yobs)

= ËœÏ€m fY|M=m(yobs)
fY(yobs)
,

yobs /âˆˆY0 and fY(yobs) > 0

.
(22.14)
We next argue that the densities factorize as
fY|M=m(y) =
1
ËœÏ€m
Ïˆm

ËœÏ€1, . . . , ËœÏ€M, T(y)




gm(T (y))
fY(y)
  
h(y)
,
y âˆˆRd \ Y0.
(22.15)
This can be argued as follows. If fY(y) is greater than zero, then (22.15) follows
directly from (22.14). And if fY(y) is equal to zero, then the RHS of (22.15) is
equal to zero and, by (22.13), the LHS is also equal to zero.
3A diï¬€erent, perhaps more elegant, way to state the theorem is in terms of probability dis-
tributions. Let Pm be the probability distribution on Rd corresponding to M = m, where m
is in the ï¬nite set M. Assume that {Pm} are dominated by the Ïƒ-ï¬nite measure Î¼. Then the
Borel measurable mapping T : Rd â†’Rdâ€² forms a suï¬ƒcient statistic for the family {Pm} if, and
only if, there exists a Borel measurable nonnegative function h(Â·) from Rd to R, and M nonneg-
ative, Borel measurable functions gm(Â·) from Rdâ€² to R such that for each m âˆˆM the function
y â†’gm(T(y)) h(y) is a version of the Radon-Nikodym derivative dPm/ dÎ¼ of Pm with respect
to Î¼; see (Billingsley, 1995, Theorem 34.6) and (Lehmann and Romano, 2005, Corollary 2.6.1).
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.3 Equivalent Conditions
473
We next prove that if the densities factorize as in (22.11), then T(Â·) forms a suï¬ƒ-
cient statistic. That is, we show how using the factorization (22.11) we can design
the desired black box. The inputs to the black box are the prior {Ï€m} and T(y).
The black box considers the vector

Ï€1 g1

T(y)

, . . . , Ï€M gM

T(y)
T
.
(22.16)
If all its components are zero, then the black box produces the uniform distribution
(or any other distribution of the readerâ€™s choice). Otherwise, it produces the above
vector but normalized to sum to one. Thus, if we denote by Ïˆm(Ï€1, . . . , Ï€M, T(y))
the probability that the black box assigns to m when fed Ï€1, . . . , Ï€M and T(y),
then
Ïˆm

Ï€1, . . . , Ï€M, T(y)

â‰œ
â§
âª
âª
âª
â¨
âª
âª
âª
â©
1
M
if
M

mâ€²=1
Ï€mâ€² gmâ€²
T(y)

= 0,
Ï€m gm

T(y)


mâ€²âˆˆM Ï€mâ€² gmâ€²
T(y)

otherwise.
(22.17)
To verify that Ïˆm(Ï€1, . . . , Ï€M, T(y)) = Pr[M = m|Y = y] whenever y is such that
y /âˆˆY0 and (22.8) holds, we ï¬rst note that, by the factorization (22.11),

fY(y) > 0 and y /âˆˆY0

=â‡’
	
h(y)
M

mâ€²=1
Ï€mâ€² gmâ€²
T(y)

> 0

,
so

fY(y) > 0 and y /âˆˆY0

=â‡’
	
h(y) > 0
and
M

mâ€²=1
Ï€mâ€² gmâ€²
T(y)

> 0

.
(22.18)
Consequently, if y /âˆˆY0 and if (22.8) holds, then by (22.18) & (22.17)

Ïˆ1

Ï€1, . . . , Ï€M, T(y)

, . . . , ÏˆM

Ï€1, . . . , Ï€M, T(y)
T
is equal to the vector in (22.16) but scaled so that its components add to one. But
the a posteriori probability vector is also a scaled version of (22.16) (scaled by
h(y)/fY(y)) that sums to one. Thus, if y /âˆˆY0 and (22.8) holds, then the vector
produced by the black box is identical to the a posteriori distribution vector.
22.3.2
Pairwise suï¬ƒciency
We next clarify the connection between suï¬ƒcient statistics for binary hypothesis
testing and for multi-hypothesis testing. We show that T(Y) forms a suï¬ƒcient
statistic for the family of densities {fY|M=m(Â·)}mâˆˆM if, and only if, for every pair
of messages mâ€² Ì¸= mâ€²â€² in M we have that T(Y) forms a suï¬ƒcient statistic for the
densities fY|M=mâ€²(Â·) and fY|M=mâ€²â€²(Â·).
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

474
Suï¬ƒcient Statistics
One part of this statement is trivial, namely, that if T(Â·) is suï¬ƒcient for the family
{fY|M=m(Â·)}mâˆˆM then it is also suï¬ƒcient for any pair. Indeed, by the Factoriza-
tion Theorem (Theorem 22.3.1), the suï¬ƒciency of T(Â·) for the family implies the
existence of a set of Lebesgue measure zero Y0 âŠ‚Rd and functions {gm}mâˆˆM, h
such that for all y âˆˆRd \ Y0
fY|M=m(y) = gm

T(y)

h(y),
m âˆˆM.
(22.19)
In particular, if we limit ourselves to mâ€², mâ€²â€² âˆˆM then for y /âˆˆY0
fY|M=mâ€²(y) = gmâ€²
T(y)

h(y),
fY|M=mâ€²â€²(y) = gmâ€²â€²
T(y)

h(y),
which, by the Factorization Theorem, implies the suï¬ƒciency of T(Â·) for the pair of
densities fY|M=mâ€²(Â·), fY|M=mâ€²â€²(Â·).
The nontrivial part of the proposition is that pairwise suï¬ƒciency implies suï¬ƒciency.
Even this is quite easy when the densities are all strictly positive. It is a bit more
tricky without this assumption.4
Proposition 22.3.2 (Pairwise Suï¬ƒciency Implies Suï¬ƒciency). Consider M den-
sities {fY|M=m(Â·)}mâˆˆM on Rd, and assume that T : Rd â†’Rdâ€² forms a suï¬ƒcient
statistic for every pair of densities fY|M=mâ€²(Â·), fY|M=mâ€²â€²(Â·), where mâ€² Ì¸= mâ€²â€² are
both in M. Then T(Â·) is a suï¬ƒcient statistic for the M densities {fY|M=m(Â·)}mâˆˆM.
Proof. To prove that T(Â·) forms a suï¬ƒcient statistic for {fY|M=m(Â·)}M
m=1 we shall
describe an algorithm (black box) that when fed any prior {Ï€m} and T(yobs) (but
not yobs) produces an M-dimensional probability vector that is equal to the a
posteriori probability vector

Pr

M = 1
 Y = yobs

, . . . , Pr

M = M
 Y = yobs
T
(22.20)
whenever yobs âˆˆRd is such that
yobs /âˆˆY0
and
M

m=1
Ï€mfY|M=m(yobs) > 0,
(22.21)
where Y0 is a subset of Rd that does not depend on the prior {Ï€m} and that is of
Lebesgue measure zero.
To describe the algorithm we ï¬rst use the Factorization Theorem (Theorem 22.3.1)
to recast the propositionâ€™s hypothesis as saying that for every pair mâ€² Ì¸= mâ€²â€² in M
there exists a set Y(mâ€²,mâ€²â€²)
0
âŠ‚Rd of Lebesgue measure zero and there exist non-
negative functions g(mâ€²,mâ€²â€²)
mâ€²
, g(mâ€²,mâ€²â€²)
mâ€²â€²
: Rdâ€² â†’R and h(mâ€²,mâ€²â€²) : Rd â†’R such that
fY|M=mâ€²(y) = g(mâ€²,mâ€²â€²)
mâ€²

T(y)

h(mâ€²,mâ€²â€²)(y),
y âˆˆRd \ Y(mâ€²,mâ€²â€²)
0
,
(22.22a)
4This result does not extend to the case where the random variable M can take on inï¬nitely
many values.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.3 Equivalent Conditions
475
fY|M=mâ€²â€²(y) = g(mâ€²,mâ€²â€²)
mâ€²â€²

T(y)

h(mâ€²,mâ€²â€²)(y),
y âˆˆRd \ Y(mâ€²,mâ€²â€²)
0
.
(22.22b)
Let
Y0 =

mâ€²,mâ€²â€²âˆˆM
mâ€²Ì¸=mâ€²â€²
Y(mâ€²,mâ€²â€²)
0
,
(22.23)
and note that, being the union of a ï¬nite number of sets of Lebesgue measure zero,
Y0 is of Lebesgue measure zero.
We now use the above functions g(mâ€²,mâ€²â€²)
mâ€²
, g(mâ€²,mâ€²â€²)
mâ€²â€²
to describe the algorithm. Note
that yobs is never fed directly to the algorithm; only T(yobs) is used. Let the prior
Ï€m = Pr[M = m],
m âˆˆM
(22.24)
be given, and assume without loss of generality that it is nondegenerate in the
sense that
Ï€m > 0,
m âˆˆM.
(22.25)
(If that is not the case, we can set the black box to produce 0 in the coordinates
of the output vector corresponding to messages of prior probability zero and then
proceed to ignore such messages.) Let yobs âˆˆRd be arbitrary.
There are two phases to the algorithm. In the ï¬rst phase the algorithm produces
some mâˆ—âˆˆM whose a posteriori probability is guaranteed to be positive when-
ever (22.21) holds. In fact, if (22.21) holds, then no message has an a posteriori
probability higher than that of mâˆ—(but this is immaterial to us because we are
not content with showing that from T(yobs) we can compute the message that a
posteriori has the highest probability; we want to be able to compute the entire
a posteriori probability vector). In the second phase the algorithm uses mâˆ—to
compute the desired a posteriori probability vector.
The ï¬rst phase of the algorithm runs in M steps. In Step 1 we set m[1] = 1. In
Step 2 we set
m[2] =
â§
âª
â¨
âª
â©
1
if Ï€1 g(1,2)
1

T(yobs)

Ï€2 g(1,2)
2

T(yobs)
 > 1,
2
otherwise.
And in Step Î½ for Î½ âˆˆ{2, . . . , M} we set
m[Î½] =
â§
âª
âª
â¨
âª
âª
â©
m[Î½ âˆ’1]
if
Ï€m[Î½âˆ’1] g(m[Î½âˆ’1],Î½)
m[Î½âˆ’1]

T(yobs)

Ï€Î½ g(m[Î½âˆ’1],Î½)
Î½

T(yobs)

> 1,
Î½
otherwise.
(22.26)
Here we use the convention that a/0 = +âˆwhenever a > 0 and that 0/0 = 1. We
complete the ï¬rst phase by setting
mâˆ—= m[M].
(22.27)
In the second phase we compute the vector
Î±[m] = Ï€m g(m,mâˆ—)
m

T(yobs)

Ï€mâˆ—g(m,mâˆ—)
mâˆ—

T(yobs)
,
m âˆˆM.
(22.28)
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

476
Suï¬ƒcient Statistics
If at least one of the components of Î±[Â·] is +âˆ, then we produce as the algorithmâ€™s
output the uniform distribution on M. (The output corresponding to this case is
immaterial because it will turn out that this case is only possible if yobs is such
that either yobs âˆˆY0 or 
m Ï€mfY|M=m(yobs) = 0, in which case the algorithmâ€™s
output is not required to be equal to the a posteriori distribution.) Otherwise, the
algorithmâ€™s output is the vector
-
Î±[1]
M
Î½=1 Î±[Î½]
, . . . ,
Î±[M]
M
Î½=1 Î±[Î½]
.T
.
(22.29)
Having described the algorithm, we now proceed to prove that it produces the
a posteriori probability vector whenever (22.21) holds. We need to show that if
(22.21) holds then
Pr[M = m|Y = yobs] =
Î±[m]
M
Î½=1 Î±[Î½]
,
m âˆˆM.
(22.30)
Since there is nothing to prove if (22.21) does not hold, we shall henceforth assume
for the rest of the proof that it does. In this case we have by (22.4)
Pr[M = m|Y = yobs] = Ï€m fY|M=m(yobs)
fY(yobs)
,
m âˆˆM.
(22.31)
We shall prove (22.30) in two steps. In the ï¬rst step we show that the result mâˆ—
of the algorithmâ€™s ï¬rst phase satisï¬es
Pr[M = mâˆ—|Y = yobs] > 0.
(22.32)
To establish (22.32) we shall prove the stronger statement that
Pr

M = mâˆ— Y = yobs

= max
mâˆˆM Pr

M = m
 Y = yobs

.
(22.33)
This latter statement follows from the more general claim that for any Î½ âˆˆM (and
not only for Î½ = M) we have, subject to (22.21),
Pr

M = m[Î½]
 Y = yobs

= max
1â‰¤mâ‰¤Î½ Pr

M = m
 Y = yobs

.
(22.34)
For Î½ = 1, Statement (22.34) is trivial. For 2 â‰¤Î½ â‰¤M, (22.34) follows from
Pr

M = m[Î½]
 Y = yobs

=
max
'
Pr

M = Î½
 Y = yobs

, Pr

M = m[Î½ âˆ’1]
 Y = yobs
(
,
(22.35)
which we now prove. We prove (22.35) by considering two cases separately depend-
ing on whether Pr[M = Î½ |Y = yobs] and Pr[M = m[Î½âˆ’1]|Y = yobs] are both zero
or not. In the former case there is nothing to prove because (22.35) holds irrespec-
tive of whether (22.26) results in m[Î½] being set to Î½ or to m[Î½ âˆ’1]. In the latter
case we have by (22.31) and (22.25) that fY|M=Î½(yobs) and fY|M=m[Î½âˆ’1](yobs)
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.3 Equivalent Conditions
477
are not both zero. Consequently, by (22.22), in this case h(m[Î½âˆ’1],Î½)(yobs) is not
only nonnegative but strictly positive. It follows that the choice (22.26) guarantees
(22.35) because
Ï€m[Î½âˆ’1] g(m[Î½âˆ’1],Î½)
m[Î½âˆ’1]

T(yobs)

Ï€Î½ g(m[Î½âˆ’1],Î½)
Î½

T(yobs)

=
Ï€m[Î½âˆ’1] g(m[Î½âˆ’1],Î½)
m[Î½âˆ’1]

T(yobs)

h(m[Î½âˆ’1],Î½)(yobs)
Ï€Î½ g(m[Î½âˆ’1],Î½)
Î½

T(yobs)

h(m[Î½âˆ’1],Î½)(yobs)
=
Ï€m[Î½âˆ’1] g(m[Î½âˆ’1],Î½)
m[Î½âˆ’1]

T(yobs)

h(m[Î½âˆ’1],Î½)(yobs)/fY(yobs)
Ï€Î½ g(m[Î½âˆ’1],Î½)
Î½

T(yobs)

h(m[Î½âˆ’1],Î½)(yobs)/fY(yobs)
= Pr

M = m[Î½ âˆ’1]
 Y = yobs

Pr[M = Î½]|Y = yobs]
,
where the ï¬rst equality follows because h(m[Î½âˆ’1],Î½)(yobs) is strictly positive; the
second because in this part of the proof we are assuming (22.21); and where the
last equality follows from (22.22) and (22.31).
This establishes (22.35), which
implies (22.34), which in turn implies (22.33), which in turn implies (22.32), and
thus concludes the proof of the ï¬rst step.
In the second step of the proof we use (22.32) to establish (22.30).
This is
straightforward because, in view of (22.31), we have that (22.32) implies that
fY|M=mâˆ—(yobs) > 0 so, by (22.22b), we have that
h(m,mâˆ—)(yobs) > 0,
m âˆˆM,
g(m,mâˆ—)
mâˆ—
(yobs) > 0,
m âˆˆM.
Consequently
Î±[m] = Ï€m g(m,mâˆ—)
m

T(yobs)

Ï€mâˆ—g(m,mâˆ—)
mâˆ—

T(yobs)

= Ï€m g(m,mâˆ—)
m

T(yobs)

h(m,mâˆ—)(yobs)
Ï€mâˆ—g(m,mâˆ—)
mâˆ—

T(yobs)

h(m,mâˆ—)(yobs)
= Ï€m g(m,mâˆ—)
m

T(yobs)

h(m,mâˆ—)(yobs)/fY(yobs)
Ï€mâˆ—g(m,mâˆ—)
mâˆ—

T(yobs)

h(m,mâˆ—)(yobs)/fY(yobs)
= Pr[M = m|Y = yobs]
Pr[M = mâˆ—|Y = yobs],
from which (22.30) follows by (22.32).
22.3.3
Markov Condition
We now characterize suï¬ƒcient statistics using Markov chains and conditional in-
dependence. These concepts were introduced in Section 20.11. The key result we
ask the reader to recall is Theorem 20.11.3. We rephrase it for our present setting
as follows.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

478
Suï¬ƒcient Statistics
Proposition 22.3.3. The statement that MâŠ¸âˆ’
âˆ’T(Y)âŠ¸âˆ’
âˆ’Y forms a Markov chain
is equivalent to each of the following statements:
(a) The conditional distribution of M given

T(Y), Y

is the same as given T(Y).
(b) M and Y are conditionally independent given T(Y).
(c) The conditional distribution of Y given

M,T(Y)

is the same as given T(Y).
Statement (a) can also be written as:
(aâ€™) The conditional distribution of M given Y is the same as given T(Y).
Indeed, the conditional distribution of any random variableâ€”in particular Mâ€”
given

T(Y), Y

is the same as given Y only, because T(Y) carries no information
that is not in Y.
Statement (aâ€™) can be rephrased as saying that the conditional distribution of M
given Y can be computed from T(Y). Since this is the key requirement of suï¬ƒcient
statistics, we obtain:
Proposition 22.3.4. A Borel measurable function T : Rd â†’Rdâ€² forms a suï¬ƒcient
statistic for the M densities {fY|M=m(Â·)}mâˆˆM if, and only if, for any prior {Ï€m}
MâŠ¸âˆ’
âˆ’T(Y)âŠ¸âˆ’
âˆ’Y
(22.36)
forms a Markov chain.
Proof. The proof of this proposition is omitted. It is not diï¬ƒcult, but it requires
some measure-theoretic tools.5
Using Proposition 22.3.4 and Proposition 22.3.3 (cf. (b)) we obtain that a Borel
measurable function T(Â·) forms a suï¬ƒcient statistic for guessing M based on Y if,
and only if, for any prior {Ï€m} on M, the message M and the observation Y are
conditionally independent given T(Y).
We next explore the implications of Proposition 22.3.4 and the equivalence of the
Markovity MâŠ¸âˆ’
âˆ’T(Y)âŠ¸âˆ’
âˆ’Y and Statement (c) in Proposition 22.3.3. These imply
that a Borel measurable function T(Â·) forms a suï¬ƒcient statistic if, and only if, the
conditional distribution of Y given

T(Y), M = m

is the same for all m âˆˆM. Or,
in other words, a Borel measurable function T(Â·) forms a suï¬ƒcient statistic if, and
only if, the conditional distribution of Y given T(Y) does not depend on which
of the densities in {fY|M=m(Â·)} governs the law of Y. This characterization has
interesting implications regarding the possibility of simulating observables. These
implications are explored next.
5If T(Â·) forms a suï¬ƒcient statistic, then by Deï¬nition 22.2.1 Ïˆm

{Ï€m}, T(Y)

is a version of
the conditional probability that M = m conditional on the Ïƒ-algebra generated by Y, and it is
also measurable with respect to the Ïƒ-algebra generated by T(Y). The reverse direction follows
from (Lehmann and Romano, 2005, Lemma 2.3.1).
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.3 Equivalent Conditions
479
T(yobs)
PY|T (Y)=T (yobs)
ËœY

T(yobs), Î˜

Given Rule for Guessing
M based on Y
Random Number
Generator
Guess
Î˜
Figure 22.2: If T(Y) forms a suï¬ƒcient statistic for guessing M based on Y, thenâ€”
even though Y cannot typically be recovered from T(Y)â€”the performance of any
given detector based on Y can be achieved based on T(Y) and a local random
number generator as follows. Using T(yobs) and local randomness Î˜, one produces
a ËœY whose conditional law given M = m is the same as that of Y, for each m âˆˆM.
One then feeds ËœY to the given detector.
22.3.4
Simulating Observables
For T(Y) to form a suï¬ƒcient statistic, we do not require that T(Â·) be invertible, i.e.,
that Y be recoverable from T(Y). Indeed, the notion of suï¬ƒcient statistics is most
useful when this transformation is not invertible, in which case T(Â·) â€œsummarizesâ€
the information in the observation Y that is needed for guessing M. Nevertheless,
as we shall next show, if T(Y) forms a suï¬ƒcient statistic, then from T(Y) we
can produce (using a local random number generator) a vector ËœY that appears
statistically like Y in the sense that the conditional law of ËœY given M is identical
to the conditional law of Y given M.
To expand on this, we ï¬rst explain what we mean by â€œwe can produce . . . ËœYâ€ and
then elaborate on the consequences of the vector ËœY having the same conditional
law given M = m as Y. By â€œproducingâ€ ËœY from T(Y) we mean that ËœY is the
result of processing T(Y) with respect to M. Stated diï¬€erently, for every t âˆˆRdâ€²
there corresponds a probability distribution P ËœY|t (not dependent on m) that can
be used to generate ËœY as follows: having observed T(yobs), we use a local random
number generator to generate the vector ËœY according to the distribution P ËœY|t,
where t = T(yobs); see Figure 22.2.
By ËœY appearing statistically the same as Y we mean that the conditional law of ËœY
given M = m is the same as that of Y, i.e., is of density fY|M=m(Â·). Consequently,
anything that can be learned about M from Y can also be learned about M from ËœY.
Also, any guessing device that was designed to guess M based on the input Y will
yield the same probability of error when, instead of being fed Y, it is fed ËœY. Thus,
if p(error|M = m) is the conditional error probability associated with a guessing
device that is fed Y, then it is also the conditional probability of error that will be
incurred by this device if, rather than Y, it is fed ËœY; see Figure 22.2.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

480
Suï¬ƒcient Statistics
Before stating this as a theorem, let us consider the following simple example.
Suppose that our observation consists of d random variables Y1, . . . , Yd and that,
conditional on H = 0, these random variables are IID Bernoulli(p0), i.e., they
each take on the value 1 with probability p0 and the value 0 with probability
1 âˆ’p0. Conditional on H = 1, these d random variables are IID Bernoulli(p1).
Here 0 < p0, p1 < 1 and p0 Ì¸= p1. Consequently, the conditional probability mass
functions are
PY1,...,Yd|H=0(y1, . . . , yd) =
d
@
j=1

pyj
0 (1 âˆ’p0)1âˆ’yj
= p
d
j=1 yj
0
(1 âˆ’p0)dâˆ’d
j=1 yj,
and
PY1,...,Yd|H=1(y1, . . . , yd) = p
d
j=1 yj
1
(1 âˆ’p1)dâˆ’d
j=1 yj,
so T(Y1, . . . , Yd) â‰œd
j=1 Yj forms a suï¬ƒcient statistic by the Factorization The-
orem.6
From T(y1, . . . , yd) one cannot recover the sequence y1, . . . , yd. Indeed,
specifying that T(y1, . . . , yd) = t does not determine which of the random vari-
ables is one; it only determines how many of them are one. There are thus
d
t

possible outcomes (y1, . . . yd) that are consistent with T(y1, . . . , yd) being equal to t.
We leave it to the reader to verify that if we use a local random number genera-
tor to pick one of these outcomes uniformly at random then the result ( ËœY1, . . . ËœYd)
will have the same conditional law given H as (Y1, . . . , Yd). We do not, of course,
guarantee that ( ËœY1, . . . ËœYd) be identical to (Y1, . . . , Yd). (The transformation T(Â·)
is, after all, not reversible.)
For additional insight let us consider our example of (20.66). For T(y1, y2) = y2
1+y2
2
we can generate ËœY from a uniform random variable Î˜ âˆ¼U ([0, 1)) as
ËœY1 =

T(Y) cos

2Ï€Î˜

ËœY2 =

T(Y) sin

2Ï€Î˜

.
That is, after observing T(yobs) = t, we generate
 ËœY1, ËœY2

uniformly over the tuples
that are at radius
âˆš
t from the origin.
This last example also demonstrates the diï¬ƒculty of stating the result. The random
vector Y in this example has a density, both when conditioned on H = 0 and when
conditioned on H = 1. The same applies to the random variable T(Y). However,
the distribution that is used to generate ËœY from T(Y) is neither discrete nor has
a density. All its mass is concentrated on the circle of radius
âˆš
t, so it cannot have
a density, and it is uniformly distributed over that circle, so it cannot be discrete.
Theorem 22.3.5 (Simulating the Observables from the Suï¬ƒcient Statistic). Let
T : Rd â†’Rdâ€² be Borel measurable and let fY|M=1(Â·), . . . , fY|M=M(Â·) be M densities
on Rd. Then the following two statements are equivalent:
(a) T(Â·) forms a suï¬ƒcient statistic for the given densities.
6For illustration purposes we are extending the discussion here to discrete distributions.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.4 Identifying Suï¬ƒcient Statistics
481
(b) To every t in Rdâ€² there corresponds a distribution on Rd such that the fol-
lowing holds: for every m âˆˆ{1, . . . , M}, if Y = yobs is generated according
to the density fY|M=m(Â·) and if the random vector ËœY is then generated ac-
cording to the distribution corresponding to t, where t = T(yobs), then ËœY is
of density fY|M=m(Â·).
Proof. For a measure-theoretic statement and proof see (Lehmann and Romano,
2005, Theorem 2.6.1). Here we only present some intuition. Ignoring some of the
technical details, the proof is very simple. The suï¬ƒciency of T(Â·) is equivalent
to MâŠ¸âˆ’
âˆ’T(Y)âŠ¸âˆ’
âˆ’Y forming a Markov chain for every prior on M. This latter
condition is equivalent by Proposition 22.3.3 (cf. (c)) to the conditional distribution
of Y given

T(Y), M

being the same as given T(Y) only. This latter condition
is equivalent to the conditional distribution of Y given T(Y) not depending on
which density in the family {fY|M=m(Â·)}mâˆˆM was used to generate Y, i.e., to the
existence of a conditional distribution of Y given T(Y) that does not depend on
m âˆˆM.
22.4
Identifying Suï¬ƒcient Statistics
Often a suï¬ƒcient statistic can be identiï¬ed without having to compute and factorize
the conditional densities of the observation. A number of such cases are described
in this section.
22.4.1
Invertible Transformation
We begin by showing that, ignoring some technical details, any invertible transfor-
mation forms a suï¬ƒcient statistic. It may not be a particularly helpful suï¬ƒcient
statistic because it does not â€œsummarizeâ€ the observation, but it is a suï¬ƒcient
statistic nonetheless.
Proposition 22.4.1 (Reversible Transformations Yield Suï¬ƒcient Statistics). If
T : Rd â†’Rdâ€² is Borel measurable with a Borel measurable inverse, then T(Â·) forms
a suï¬ƒcient statistic for guessing M based on Y.
Proof. We provide two proofs. The ï¬rst uses the deï¬nition. We need to verify that
from T(yobs) one can compute the conditional distribution of M given Y = yobs.
This is obvious because if t = T(yobs), then one can compute Pr[M = m|Y = yobs]
from t by ï¬rst applying the inverse T âˆ’1(t) to recover yobs and by then substituting
the result in the expression for Pr[M = m|Y = yobs] (22.4).
A second proof can be based on Proposition 22.3.4. We need to verify that for any
prior {Ï€m}
MâŠ¸âˆ’
âˆ’T(Y)âŠ¸âˆ’
âˆ’Y
forms a Markov chain. To this end we note that, by Proposition 22.3.3, it suï¬ƒces
to verify that M and Y are conditionally independent given T(Y). This is clear
because the invertibility of T(Â·) guarantees that, conditional on T(Y), the random
vector Y is deterministic and hence independent of any random variable and a
fortiori of M.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

482
Suï¬ƒcient Statistics
22.4.2
A Suï¬ƒcient Statistic Is Computable from the Statistic
Intuitively, we think about T(Â·) as forming a suï¬ƒcient statistic if T(Y) contains
all the information about Y that is relevant to guessing M. For this intuition to
make sense it had better be the case that if T(Â·) forms a suï¬ƒcient statistic for
guessing M based on Y, and if T(Y) is computable from S(Y), then S(Â·) also
forms a suï¬ƒcient statistic. Fortunately, this is so:
Proposition 22.4.2. Suppose that a Borel measurable mapping T : Rd â†’Rdâ€² forms
a suï¬ƒcient statistic for the M densities {fY|M=m(Â·)}mâˆˆM on Rd. Let the mapping
S : Rd â†’Rdâ€²â€² be Borel measurable. If T(Â·) can be written as the composition Ïˆ â—¦S
of S with some Borel measurable function Ïˆ: Rdâ€²â€² â†’Rdâ€², then S(Â·) also forms a
suï¬ƒcient statistic for these densities.
Proof. We need to show that Pr[M = m|Y = yobs] is computable from S(yobs).
This follows because, by assumption, T(yobs) is computable from S(yobs) and
because the suï¬ƒciency of T(Â·) implies that Pr[M = m|Y = yobs] is computable
from T(yobs).
22.4.3
Establishing Suï¬ƒciency in Two Steps
It is sometimes convenient to establish suï¬ƒciency in two steps: in the ï¬rst step
we establish that T(Y) is suï¬ƒcient for guessing M based on Y, and in the second
step we establish that S(T) is suï¬ƒcient for guessing M based on T(Y).
The
next proposition demonstrates that it then follows that S(T(Y)) forms a suï¬ƒcient
statistic for guessing M based on Y.
Proposition 22.4.3. If T : Rd â†’Rdâ€² forms a suï¬ƒcient statistic for the M densities
{fY|M=m(Â·)}mâˆˆM and if S : Rdâ€² â†’Rdâ€²â€² forms a suï¬ƒcient statistic for the corre-
sponding family of densities of T(Y), then the composition S â—¦T forms a suï¬ƒcient
statistic for the densities {fY|M=m(Â·)}mâˆˆM.
Proof. We shall establish the suï¬ƒciency of Sâ—¦T by proving that for any prior {Ï€m}
MâŠ¸âˆ’
âˆ’S

T(Y)

âŠ¸âˆ’
âˆ’Y.
This follows because for every m âˆˆM and every yobs âˆˆRd
Pr

M = m
 S

T(Y)

= S

T(yobs)

= Pr

M = m
 T(Y) = T(yobs)

= Pr

M = m
 Y = yobs

,
where the ï¬rst equality follows from the suï¬ƒciency of S(T(Y)) for guessing M
based on T(Y), and where the second equality follows from the suï¬ƒciency of T(Y)
for guessing M based on Y.
22.4.4
Guessing whether M Lies in a Given Subset of M
We motivate the next result with the following example, which arises in the detec-
tion of PAM signals in white Gaussian noise (Section 28.3 ahead). Suppose that
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.4 Identifying Suï¬ƒcient Statistics
483
the distribution of the observable Y is determined by the value of a k-tuple of bits
(D1, . . . , Dk). Thus, to each outcome (d1, . . . , dk) âˆˆ{0, 1}k among the 2k diï¬€er-
ent outcomes of (D1, . . . , Dk) there corresponds a distribution on Y of some given
density fY|D1=d1,...,Dk=dk(Â·). Suppose now that T(Â·) forms a suï¬ƒcient statistic for
this family of M = 2k densities. The result we next describe guarantees that T(Â·)
is also suï¬ƒcient for the binary hypothesis testing problem of guessing whether a
speciï¬c bit Dj is zero or one. More precisely, we shall show that if {Ï€(d1,...,dk)}
is any nondegenerate prior on the 2k diï¬€erent k-tuples of bits, then T(Â·) forms a
suï¬ƒcient statistic for the two densities
Î³0

(d1,...,dk)
dj=0
Ï€(d1,...,dk) fY|D1=d1,...,Dk=dk(y), Î³1

(d1,...,dk)
dj=1
Ï€(d1,...,dk) fY|D1=d1,...,Dk=dk(y)
Î³0 =
-

(d1,...,dk)
dj=0
Ï€(d1,...,dk)
.âˆ’1
,
Î³1 =
-

(d1,...,dk)
dj=1
Ï€(d1,...,dk)
.âˆ’1
.
Proposition 22.4.4 (Guessing whether M Is in K). Let T : Rd â†’Rdâ€² form a
suï¬ƒcient statistic for the M densities {fY|M=m(Â·)}mâˆˆM. Let the set K âŠ‚M be a
nonempty strict subset of M. Let {Ï€m} be a prior on M satisfying
0 <

mâˆˆK
Ï€m < 1.
Then T(Â·) forms a suï¬ƒcient statistic for the two densities
y 	â†’Î³0

mâˆˆK
Ï€m fY|M=m(y)
and
y 	â†’Î³1

m/âˆˆK
Ï€m fY|M=m(y)
(22.37a)
Î³0 =
	 
mâˆˆK
Ï€m

âˆ’1
Î³1 =
	 
m/âˆˆK
Ï€m

âˆ’1
.
(22.37b)
Proof. From the Factorization Theorem it follows that the suï¬ƒciency of T(Â·) for
the family {fY|M=m(Â·)}mâˆˆM is equivalent to the condition that for every m âˆˆM
and for every y /âˆˆY0
fY|M=m(y) = gm

T(y)

h(y),
(22.38)
where the set Y0 âŠ‚Rd is of Lebesgue measure zero; where {gm(Â·)}mâˆˆM are non-
negative Borel measurable functions from Rdâ€²; and where h(Â·) is a nonnegative
Borel measurable function from Rd. Consequently,
Î³0

mâˆˆK
Ï€m fY|M=m(y) = Î³0

mâˆˆK
Ï€m gm

T(y)

h(y)
=
	
Î³0

mâˆˆK
Ï€m gm

T(y)

h(y),
y /âˆˆY0,
(22.39a)
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

484
Suï¬ƒcient Statistics
and
Î³1

m/âˆˆK
Ï€m fY|M=m(y) = Î³1

m/âˆˆK
Ï€m gm

T(y)

h(y)
=
	
Î³1

m/âˆˆK
Ï€m gm

T(y)

h(y),
y /âˆˆY0.
(22.39b)
The factorization (22.39) of the densities in (22.37) proves that T(Â·) is also suï¬ƒcient
for these two densities.
Note 22.4.5. The proposition also extends to more general partitions as follows.
Suppose that T(Â·) is suï¬ƒcient for the family {fY|M=m(Â·)}mâˆˆM. Let K1, . . . , KÎº be
disjoint nonempty subsets of M whose union is equal to M, and let the prior {Ï€m}
be such that

mâˆˆKj
Ï€m > 0,
j âˆˆ{1, . . . , Îº}.
Then T(Â·) is suï¬ƒcient for the Îº densities
y 	â†’Î³1

mâˆˆK1
Ï€m fY|M=m(y), . . . , y 	â†’Î³Îº

mâˆˆKÎº
Ï€m fY|M=m(y)
Î³j =
	 
mâˆˆKj
Ï€m

âˆ’1
,
j âˆˆ{1, . . . , Îº}.
22.4.5
Conditionally Independent Observations
Our next result deals with a situation where we need to guess M based on two
observations: Y1 and Y2. We assume that T1(Y1) forms a suï¬ƒcient statistic for
guessing M when only Y1 is observed, and that T2(Y2) forms a suï¬ƒcient statistic
for guessing M when only Y2 is observed. It is tempting to conjecture that in
this case the pair (T1(Y1), T2(Y2)) must form a suï¬ƒcient statistic for guessing M
when both Y1 and Y2 are observed. But, without additional assumptions, this is
not the case. An example where this fails can be constructed as follows. Let M
and Z be independent with M taking on the values 0 and 1 equiprobably and with
Z âˆ¼N(0, 1). Suppose that Y1 = M +Z and that Y2 = Z. In this case the invertible
mapping T1(Y1) = Y1 forms a suï¬ƒcient statistic for guessing M based on Y1 alone,
and the mapping T2(Y2) = 17 forms a suï¬ƒcient statistic for guessing M based
on Y2 alone (because M and Z are independent). Nevertheless, the pair (Y1, 17) is
not suï¬ƒcient for guessing M based on the pair (Y1, Y2). Basing oneâ€™s guess of M on
(Y1, 17) is not as good as basing it on the pair (Y1, Y2). (The reader is encouraged
to verify that Y1 âˆ’Y2 is suï¬ƒcient for guessing M based on (Y1, Y2) and that M
can be guessed error-free from Y1 âˆ’Y2.)
The additional assumption we need is that Y1 and Y2 be conditionally independent
given M. (It would make no sense to assume that they are independent, because
they are presumably both related to M.) This assumption is valid in many appli-
cations. For example, it occurs when a signal is received at two diï¬€erent antennas
with the additive noises in the two antennas being independent.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.5 Suï¬ƒcient Statistics for the M-ary Gaussian Problem
485
Proposition 22.4.6 (Conditionally Independent Observations). Let the mapping
T1 : Rd1 â†’Rdâ€²
1 form a suï¬ƒcient statistic for guessing M based on the observation
Y1 âˆˆRd1, and let T2 : Rd2 â†’Rdâ€²
2 form a suï¬ƒcient statistic for guessing M based on
the observation Y2 âˆˆRd2. If Y1 and Y2 are conditionally independent given M,
then the pair (T1(Y1), T2(Y2)) forms a suï¬ƒcient statistic for guessing M based on
the pair (Y1, Y2).
Proof. The proof we oï¬€er is based on the Factorization Theorem. The hypothesis
that T1 : Rd1 â†’Rdâ€²
1 forms a suï¬ƒcient statistic for guessing M based on the obser-
vation Y1 implies the existence of nonnegative functions

g(1)
m

mâˆˆM and h(1) and
a subset Y(1)
0
âŠ‚Rd1 of Lebesgue measure zero such that
fY1|M=m(y1) = g(1)
m

T1(y1)

h(1)(y1),
m âˆˆM, y1 /âˆˆY(1)
0 .
(22.40)
Similarly, the hypothesis that T2(Â·) is suï¬ƒcient for guessing M based on Y2 im-
plies the existence of nonnegative functions

g(2)
m

mâˆˆM and h(2) and a subset of
Lebesgue measure zero Y(2)
0
âŠ‚Rd2 such that
fY2|M=m(y2) = g(2)
m

T2(y2)

h(2)(y2),
m âˆˆM, y2 /âˆˆY(2)
0 .
(22.41)
The conditional independence of Y1 and Y2 given M implies7
fY1,Y2|M=m(y1, y2) = fY1|M=m(y1) fY2|M=m(y2),
m âˆˆM, y1 âˆˆRd1, y2 âˆˆRd2.
(22.42)
Combining (22.40), (22.41), and (22.42), we obtain
fY1,Y2|M=m(y1, y2) = g(1)
m

T1(y1)

g(2)
m

T2(y2)




gm

T1(y1),T2(y2)

h(1)(y1) h(2)(y2)



h(y1,y2)
,
m âˆˆM, y1 /âˆˆY(1)
0 , y2 /âˆˆY(2)
0 .
(22.43)
The set of pairs (y1, y2) âˆˆRd1 Ã— Rd2 for which y1 is in Y(1)
0
and/or y2 is in Y(2)
0
is of Lebesgue measure zero, and consequently, the factorization (22.43) implies
that the pair

T1(Y1), T2(Y2)

forms a suï¬ƒcient statistic for guessing M based on
(Y1, Y2).
22.5
Suï¬ƒcient Statistics for the M-ary Gaussian Problem
We next revisit the multi-dimensional M-ary Gaussian hypothesis testing problem
of Section 21.6 with an eye to ï¬nding suï¬ƒcient statistics. Starting from (21.51) we
7Technically speaking, this must only hold outside a set of Lebesgue measure zero, but we do
not want to make things even more cumbersome.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

486
Suï¬ƒcient Statistics
can express the conditional densities at every y âˆˆRJ as
fY|M=m(y)
=
J
@
j=1
1
âˆš
2Ï€Ïƒ2 exp
-
âˆ’

y(j) âˆ’s(j)
m
2
2Ïƒ2
.
= (2Ï€Ïƒ2)âˆ’J
2 exp
-
âˆ’1
2Ïƒ2
J

j=1

y(j) âˆ’s(j)
m
2
.
= (2Ï€Ïƒ2)âˆ’J
2 exp
	
âˆ’1
2Ïƒ2

âˆ¥yâˆ¥2 âˆ’2 âŸ¨y, smâŸ©E + âˆ¥smâˆ¥2
= exp
	 1
Ïƒ2

âŸ¨y, smâŸ©E âˆ’1
2 âˆ¥smâˆ¥2
Â· (2Ï€Ïƒ2)âˆ’J
2 exp
	
âˆ’1
2Ïƒ2 âˆ¥yâˆ¥2

.
(22.44)
If we now deï¬ne the mapping T : RJ â†’RM so that T(Y) will be the M-dimensional
random vector
T(Y) =

âŸ¨Y, s1âŸ©E , . . . , âŸ¨Y, sMâŸ©E
T
(22.45)
whose m-th component T (m)(Y) is
T (m)(Y) = âŸ¨Y, smâŸ©E ,
m âˆˆM,
then we can use (22.44) and the Factorization Theorem (Theorem 22.3.1) to es-
tablish that T(Â·) forms a suï¬ƒcient statistic for guessing M based on Y. Indeed,
we can rewrite (22.44) as
fY|M=m(y) = gm

T(y)

h(y)
where
gm

T(y)

= exp
	 1
Ïƒ2

T (m)(y) âˆ’1
2 âˆ¥smâˆ¥2
and
h(y) = (2Ï€Ïƒ2)âˆ’J
2 exp
	
âˆ’1
2Ïƒ2 âˆ¥yâˆ¥2

.
(The suï¬ƒciency of T(Â·) can also be demonstrated by ï¬rst showing pairwise suï¬ƒ-
ciency and then invoking Proposition 22.3.2. To establish pairwise suï¬ƒciency we
can use the expression for the LLR (20.83) to show that the LLR is computable
from T(y).)
If the vectors {sm} are linearly dependent, then we can ï¬nd a more succinct suf-
ï¬cient statistic. Indeed, if Ëœs1, . . . ,Ëœsn are vectors in RJ whose span includes all the
vectors s1, . . . , sM, then for each m âˆˆM we can ï¬nd coeï¬ƒcients Î±(1)
m , . . . , Î±(n)
m such
that
sm = Î±(1)
m Ëœs1 + Â· Â· Â· + Î±(n)
m Ëœsn,
m âˆˆM.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.6 Irrelevant Data
487
Consequently,
âŸ¨Y, smâŸ©E = Î±(1)
m âŸ¨Y,Ëœs1âŸ©E + Â· Â· Â· + Î±(n)
m âŸ¨Y,ËœsnâŸ©E ,
m âˆˆM,
and T(Y) is computable from
âŸ¨Y,Ëœs1âŸ©E , . . . , âŸ¨Y,ËœsnâŸ©E .
It thus follows from Proposition 22.4.2 that
y 	â†’

âŸ¨y,Ëœs1âŸ©E , . . . , âŸ¨y,ËœsnâŸ©E
T
forms a suï¬ƒcient statistic for guessing M based on Y. We have thus established:
Proposition 22.5.1 (Multi-Dimensional M-ary Gaussian Problem: Suï¬ƒciency).
Consider the setup of Section 21.6.1. If Ëœs1, . . . ,Ëœsn âˆˆRJ are such that
sm âˆˆspan (Ëœs1, . . . ,Ëœsn) ,
m âˆˆM,
then
y 	â†’

âŸ¨y,Ëœs1âŸ©E , . . . , âŸ¨y,ËœsnâŸ©E
T
forms a suï¬ƒcient statistic for guessing M based on Y.
22.6
Irrelevant Data
Closely related to the notion of suï¬ƒcient statistics is the notion of irrelevant data.
This notion is particularly useful when we think about the data as consisting of
two parts.
Heuristically speaking, we say that the second part of the data is
irrelevant for guessing M given the ï¬rst, if it adds no information about M that
is not already contained in the ï¬rst part. In such cases the second part of the
data can be ignored. It should be emphasized that the question whether a part
of the observation is irrelevant depends not only on its dependence on the random
variable to be guessed but also on the other part of the observation.
Deï¬nition 22.6.1 (Irrelevant Data). We say that R is irrelevant for guessing M
given Y, if Y forms a suï¬ƒcient statistic for guessing M based on (Y, R).
Equivalently, R is irrelevant for guessing M given Y, if for any prior {Ï€m} on M
MâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’(Y, R),
(22.46)
i.e.,
MâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’R.
(22.47)
Example 22.6.2. Let H take on the values 0 and 1, and assume that, conditional
on H = 0, the observation Y is N

0, Ïƒ2
0

and that, conditional on H = 1, it is
N

0, Ïƒ2
1

. Rather than thinking of this problem as a decision problem with a single
observation, let us think of it as a decision problem with two observations (Y1, Y2),
where Y1 is the absolute value of Y , and where Y2 is the sign of Y . Thus Y = Y1 Y2,
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

488
Suï¬ƒcient Statistics
where Y1 â‰¥0 and Y2 âˆˆ{+1, âˆ’1}. (The probability that Y = 0 is zero under each
hypothesis, so we need not deï¬ne the sign of zero.) We now show that Y2 (= the
sign of Y ) is irrelevant data for guessing H given Y1 (= the magnitude of Y ). Or,
in other words, the magnitude of Y is a suï¬ƒcient statistic for guessing H based on
(Y1, Y2). Indeed the likelihood-ratio function
LR(y1, y2) = fY1,Y2|H=0(y1, y2)
fY1,Y2|H=1(y1, y2)
=
1
âˆš
2Ï€Ïƒ2
0 exp

âˆ’(y1y2)2
2Ïƒ2
0

1
âˆš
2Ï€Ïƒ2
1 exp

âˆ’(y1y2)2
2Ïƒ2
1

= Ïƒ1
Ïƒ0
exp
 y2
1
2Ïƒ2
1
âˆ’y2
1
2Ïƒ2
0

can be computed from the magnitude y1 only, so Y1 is a suï¬ƒcient statistic for
guessing H based on (Y1, Y2).
The following two notes clarify that the notion of irrelevance is diï¬€erent from that
of statistical independence. Neither implies the other.
Note 22.6.3. A RV can be independent of the RV that we wish to guess and yet
not be irrelevant.
Proof. We provide an example of a RV R that is independent of the RV H that
we wish to guess and that is nonetheless not irrelevant. Suppose that H takes on
the values 0 and 1, and assume that under both hypotheses Y âˆ¼Bernoulli(1/2):
Pr

Y = 1
 H = 0

= Pr

Y = 1
 H = 1

= 1
2.
Further assume that under H = 0 the RV R is given by 0 âŠ•Y = Y , whereas under
H = 1 it is given by 1 âŠ•Y . Here âŠ•denotes the exclusive-or operation or mod-2
addition.
The distribution of R does not depend on the hypothesis; it is Bernoulli(1/2) both
conditional on H = 0 and conditional on H = 1.
But R is not irrelevant for
guessing H given Y . In fact, if we had to guess H based on Y only, our probability
of error would be 1/2. But if we base our decision on Y and R, then our probability
of error is zero because
H = Y âŠ•R.
Note 22.6.4. A RV can be irrelevant even if it is statistically dependent on the
RV that we wish to guess.
Proof. As an example, consider the case where R is equal to Y with probability one
and that Y (and hence also R) is statistically dependent on the RV M that we wish
to guess. Since R is deterministically equal to Y , it follows that, conditional on Y ,
the random variable R is deterministic. Consequently, since a deterministic RV is
independent of every RV, it follows that M and R are conditionally independent
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.7 Testing with Random Parameters
489
given Y , i.e., that (22.47) holds. Thus, even though in this example R is statistically
dependent on M, it is irrelevant for guessing M given Y . The intuitive explanation
is that, in this example, R is irrelevant for guessing M given Y not because it
conveys no information about M (it does!) but because it conveys no information
about M that is not already conveyed by Y .
Condition (22.46) is often diï¬ƒcult to establish directly, especially when the distri-
bution of the pair (R, Y) is speciï¬ed in terms of its conditional density given M,
because in this case the conditional law of (M, R) given Y can be unwieldy. In
some cases the following proposition can be used to establish that R is irrelevant.
Proposition 22.6.5 (A Condition that Implies Irrelevance). Suppose that the con-
ditional law of R given M = m does not depend on m and that, for each m âˆˆM,
we have that, conditionally on M = m, the observations Y and R are independent.
Then R is irrelevant for guessing M given Y.
Proof. We provide the proof for the case where the pair (Y, R) has a conditional
density given M. The discrete case or the mixed case (where one has a conditional
density and the other a conditional PMF) can be treated with the same approach.
To prove this proposition we shall demonstrate that Y is a suï¬ƒcient statistic for
guessing H based on (Y, R) using the Factorization Theorem. To that end, we
express the conditional density of (Y, R) as
fY,R|M=m(y, r) = fY|M=m(y) fR|M=m(r)
= fY|M=m(y) fR(r)
= gm(y) h(y, r),
(22.48)
where the ï¬rst equality follows from the conditional independence of Y and R
given M; the second from the hypothesis that the conditional density of R given
M = m does not depend on m and by denoting this density by fR(Â·); and the
ï¬nal equality follows by deï¬ning gm(y) â‰œfY|M=m(y) and h(y, r) â‰œfR(r). The
factorization (22.48) demonstrates that Y forms a suï¬ƒcient statistic for guessing M
based on (Y, R), i.e., that R is irrelevant for guessing M given Y.
22.7
Testing with Random Parameters
The notions of suï¬ƒcient statistics and irrelevance also apply when testing in the
presence of a random parameter.
If the random parameter Î˜ is not observed,
then T(Y) is suï¬ƒcient if, and only if, for any prior {Ï€m} on M
MâŠ¸âˆ’
âˆ’T(Y)âŠ¸âˆ’
âˆ’Y.
(22.49)
If Î˜ is of density fÎ˜(Â·) and independent of M, then, as in (20.101), we can express
the conditional density of Y given M = m as
fY|M=m(y) =

Î¸
fY|Î˜=Î¸,M=m(y) fÎ˜(Î¸) dÎ¸,
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

490
Suï¬ƒcient Statistics
so T(Â·) forms a suï¬ƒcient statistic if, and only if, it forms a suï¬ƒcient statistic for
the M densities
1
y 	â†’

Î¸
fY|Î˜=Î¸,M=m(y) fÎ˜(Î¸) dÎ¸
2
mâˆˆM
.
Similarly, R is irrelevant for guessing M given Y if, and only if,
MâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’R
forms a Markov chain for every prior {Ï€m} on M.
If the parameter Î˜ is observed, then T(Y, Î˜) is a suï¬ƒcient statistic if, and only if,
for any prior {Ï€m} on M
MâŠ¸âˆ’
âˆ’T(Y, Î˜)âŠ¸âˆ’
âˆ’(Y, Î˜).
If Î˜ is independent of M and is of density fÎ˜(Â·), then the density fY,Î˜|M=m(Â·) can
be expressed, as in (20.104), as
fY,Î˜|M=m(y, Î¸) = fÎ˜(Î¸) fY|Î˜=Î¸,M=m(y),
so T(Â·) forms a suï¬ƒcient statistic if, and only if, it forms a suï¬ƒcient statistic for
the M densities
'
(Î¸, y) 	â†’fÎ˜(Î¸) fY|Î˜=Î¸,M=m(y)
(
mâˆˆM.
Similarly, R is irrelevant for guessing M given (Y, Î˜) if, and only if, for every prior
on M
MâŠ¸âˆ’
âˆ’(Y, Î˜)âŠ¸âˆ’
âˆ’R.
The following lemma provides an easily-veriï¬able condition that guarantees that R
is irrelevant for guessing H based on Y, irrespective of whether the random pa-
rameter is observed or not.
Lemma 22.7.1. If for any prior {Ï€m} on M we have that R is independent of the
triple (M, Î˜, Y),8 then R is irrelevant for guessing M given (Î˜, Y) and also for
guessing M given Y.
Proof. To prove the lemma when Î˜ is observed, we need to show that the inde-
pendence of R and the triple (M, Î˜, Y) implies
MâŠ¸âˆ’
âˆ’(Y, Î˜)âŠ¸âˆ’
âˆ’R,
i.e., that the conditional distribution of R given (Y, Î˜) is the same as given
(M, Y, Î˜).
This is indeed the case because R is independent of (M, Y, Î˜) so
the two conditional distributions are equal to the unconditional distribution of R.
To prove the lemma in the case where Î˜ is unobserved, we need to show that the
independence of R and the triple (M, Î˜, Y) implies that
MâŠ¸âˆ’
âˆ’YâŠ¸âˆ’
âˆ’R.
8Note that being independent of the triple is a stronger condition than being independent of
each of the members of the triple!
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.8 Additional Reading
491
Again, one can do so by noting that the conditional distribution of R given Y is
equal to the conditional distribution of R given (Y, M) because both are equal to
the unconditional distribution of R.
22.8
Additional Reading
The classical deï¬nition of suï¬ƒcient statistic as a mapping T(Â·) such that the dis-
tribution of Y given

T(Y), M = m

does not depend on m is due to R. A. Fisher.
A. N. Kolmogorov deï¬ned T(Â·) to be suï¬ƒcient if for every prior {Ï€m} the a pos-
teriori distribution of M given Y can be computed from T(Y). In our setting
where M takes on a ï¬nite number of values the two deï¬nitions are equivalent. For
an example where the deï¬nitions diï¬€er, see (Blackwell and Ramamoorthi, 1982).
For a discussion of pairwise suï¬ƒciency and its relation to suï¬ƒciency, see (Halmos
and Savage, 1949).
22.9
Exercises
Exercise 22.1 (Another Proof of Proposition 22.4.6). Give an alternative proof of Propo-
sition 22.4.6 using Theorem 22.3.5.
Exercise 22.2 (Hypothesis Testing with Two Observations). Let H take on the values 0
and 1 equiprobably. Let Y1 be a random vector taking values in R2, and let Y2 be a
random variable. Conditional on H = 0,
Y1 = Î¼ + Z1,
Y2 = Î± + Z2,
and, conditional on H = 1,
Y1 = âˆ’Î¼ + Z1,
Y2 = âˆ’Î± + Z2.
Here H, Z1, and Z2 are independent with the components of Z1 being IID N(0, 1),
with Z2 having the mean-one exponential distribution, and with Î¼ âˆˆR2 and Î± âˆˆR being
deterministic.
(i) Find an optimal rule for guessing H based on Y1. Find a one-dimensional suï¬ƒcient
statistic.
(ii) Find an optimal rule for guessing H based on Y2.
(iii) Find a two-dimensional suï¬ƒcient statistic (T1, T2) for guessing H based on (Y1, Y2).
(iv) Find an optimal rule for guessing H based on the pair (T1, T2).
Exercise 22.3 (Suï¬ƒcient Statistics and the Bhattacharyya Bound). Show that if the
mapping T : Rd â†’Rdâ€² is a suï¬ƒcient statistic for the densities fY|H=0(Â·) & fY|H=1(Â·), and
if T = T(Y) is of conditional densities fT|H=0(Â·) and fT|H=1(Â·), then
1
2

Rd
-
fY|H=0(y) fY|H=1(y) dy = 1
2

Rdâ€²
-
fT|H=0(t) fT|H=1(t) dt.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

492
Suï¬ƒcient Statistics
Hint: You may want to ï¬rst derive the identity

Rd
-
fY|H=0(y) fY|H=1(y) dy = E
1fY|H=0(Y)
fY|H=1(Y)
1/2  H = 1
2
.
Exercise 22.4 (Suï¬ƒcient Statistics and Irrelevant Data).
(i) Show that if the hypotheses of Proposition 22.6.5 are satisï¬ed, then the random
variables Y and R must be independent also when one does not condition on M.
(ii) Show that the conditions for irrelevance in that proposition are not necessary.
Exercise 22.5 (Two More Characterizations of Suï¬ƒcient Statistics). Let PY |H=0(Â·) and
PY |H=1(Â·) be probability mass functions on the ï¬nite set Y. We say that T(Y ) forms a
suï¬ƒcient statistic for guessing H based on Y if HâŠ¸âˆ’
âˆ’T(Y )âŠ¸âˆ’
âˆ’Y for every prior on H.
Show that each of the following conditions is equivalent to T(Y ) forming a suï¬ƒcient
statistic for guessing H based on Y :
(a) For every y âˆˆY satisfying PY |H=0(y) + PY |H=1(y) > 0 we have
PY |H=0(y)
PY |H=1(y) = PT |H=0

T(y)

PT |H=1

T(y)
,
where we adopt the convention (20.39).
(b) For every prior (Ï€0, Ï€1) on H there exists a decision rule that bases its decision on
Ï€0, Ï€1, and T(Y ) and that is optimal for guessing H based on Y .
Exercise 22.6 (Pairwise Suï¬ƒciency Implies Suï¬ƒciency). Prove Proposition 22.3.2 in the
case where the conditional densities of the observable given each of the hypotheses are
positive.
Exercise 22.7 (Simulating the Observable). In all the examples we gave in Section 22.3.4
the random vector ËœY was generated from T(yobs) uniformly over the set of vectors Î¾ in Rd
satisfying T(Î¾) = T(yobs). Provide an example where this is not the case.
Hint: The setup of Proposition 22.6.5 might be useful.
Exercise 22.8 (Densities with Zeros). Conditional on H = 0, the d components of Y are
IID and uniformly distributed over the interval [Î±0, Î²0]. Conditional on H = 1, they are
IID and uniformly distributed over the interval [Î±1, Î²1]. Show that the tuple
	
max

Y (1), . . . , Y (d)
, min

Y (1), . . . , Y (d)
forms a suï¬ƒcient statistic for guessing H based on Y.
Exercise 22.9 (Optimality Does Not Imply Suï¬ƒciency). Let H take value in the set
{0, 1}, and let d = 2. Suppose that
Yj = (1 âˆ’2H) + Î˜Zj,
j = 1, . . . , d,
where H, Î˜, Z1, . . . , Zd are independent with Î˜ taking on the distinct positive values Ïƒ0
and Ïƒ1 with probability Ï0 and Ï1 respectively, and with Z1, . . . , Zd being IID N(0, 1).
Let T = 
j Yj.
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,

22.9 Exercises
493
(i) Show that (T, Î˜) forms a suï¬ƒcient statistic for guessing H based on Y1, . . . , Yd
when Î˜ is observed.
(ii) Show that T does not form a suï¬ƒcient statistic for guessing H based on Y1, . . . , Yd
when Î˜ is not observed.
(iii) Show that notwithstanding Part (ii), if H has a uniform prior, then the decision
rule that guesses â€œH = 0â€ whenever T â‰¥0 is optimal both when Î˜ is observed and
when it is not observed.
Exercise 22.10 (Markovity Implies Markovity). Suppose that for every prior on M
(M, A)âŠ¸âˆ’
âˆ’T(Y)âŠ¸âˆ’
âˆ’Y
forms a Markov chain, where M takes value in the set M = {1, . . . , M}, where A and Y
are random vectors, and where T(Â·) is Borel measurable. Does this imply that T(Â·) forms
a suï¬ƒcient statistic for guessing M based on Y?
Exercise 22.11 (Ancillary Statistic). A (Borel measurable) function S(Â·) of the observ-
able Y is said to be an ancillary statistic if the distribution of S(Y) when Y is of density
fY|M=m(Â·) is identical for all m âˆˆM. Must every ancillary statistic be irrelevant?
Exercise 22.12 (Minimal Suï¬ƒcient). A suï¬ƒcient statistic T(Â·) for guessing M based on Y
is said to be minimal suï¬ƒcient if T(Y) can be computed from any suï¬ƒcient statistic
for guessing M based on Y. That is, for every suï¬ƒcient statistic S(Â·) there exists some
function g(Â·) such that
Pr
$
g

S(Y)

= T(Y)
 M = m
%
= 1,
m âˆˆM.
Show that in binary hypothesis testing the mapping yobs â†’LR(yobs) is minimal suï¬ƒcient.
(If you are bothered by the fact that LR(yobs) may be inï¬nite, consider the inverse tangent
yobs â†’tanâˆ’1(LR(yobs)), which takes value in the interval [0, Ï€/2].)
available at 
.024
14:37:12, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 23
The Multivariate Gaussian Distribution
23.1
Introduction
The multivariate Gaussian distribution is arguably the most important multi-
variate distribution in Digital Communications. It is the extension of the univariate
Gaussian distribution from scalars to vectors. A random vector of this distribu-
tion is said to be a Gaussian vector, and its components are said to be jointly
Gaussian. In this chapter we shall deï¬ne this distribution, provide some useful
characterizations, and study some of its key properties.
To emphasize its con-
nection to the univariate distribution, we shall derive it along the same lines we
followed in deriving the univariate Gaussian distribution in Chapter 19.
There are a number of equivalent ways to deï¬ne the multivariate Gaussian distri-
bution, and authors typically pick one deï¬nition and then proceed over the course
of numerous pages to derive alternate characterizations. We shall also proceed in
this way, but to satisfy the impatient readerâ€™s curiosity we shall state the various
equivalent deï¬nitions in this section. The proof of their equivalence will be spread
over the whole chapter.
In the following deï¬nition we use the notation introduced in Section 17.2.
In
particular, all vectors are column vectors, and we denote the components of the
vector a âˆˆRn by a(1), . . . , a(n).
Deï¬nition 23.1.1 (Standard Gaussians, Centered Gaussians, and Gaussians).
(i) A random vector W taking values in Rn is said to be a standard Gaussian
if its n components W (1), . . . , W (n) are independent and each is a zero-mean
unit-variance univariate Gaussian.
(ii) A random vector X taking values in Rn is said to be a centered Gaussian
if there exists some deterministic n Ã— m matrix A such that the distribution
of X is the same as the distribution of AW, i.e.,
X
L= AW,
(23.1)
where W is a standard Gaussian with m components.
494
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.2 Notation and Preliminaries
495
(iii) A random vector X taking values in Rn is said to be Gaussian if there exists
some deterministic n Ã— m matrix A and some deterministic vector Î¼ âˆˆRn
such that the distribution of X is equal to the distribution of AW + Î¼, i.e., if
X
L= AW + Î¼,
(23.2)
where W is a standard Gaussian with m components.
The random vectors AW + Î¼ and X can have identical laws only if they have
identical mean vectors. As we shall see, the linearity of expectation and the fact
that a standard Gaussian is of zero mean imply that the mean vector of AW + Î¼
is equal to Î¼. Thus, AW + Î¼ and X can have identical laws only if Î¼ = E[X].
Consequently, X is a Gaussian random vector if, and only if, for some A and W
as above X
L= AW + E[X]. Stated diï¬€erently, X is a Gaussian random vector if,
and only if, X âˆ’E[X] is a centered Gaussian.
While Deï¬nition 23.1.1 allows for the matrix A to be rectangular, we shall see in
Corollary 23.6.13 that every centered Gaussian can be generated from a standard
Gaussian by multiplication by a square matrix. That is, if X is an n-dimensional
centered Gaussian, then there exists an nÃ—n square matrix A such that X
L= AW,
where W is a standard Gaussian.
In fact, we shall see in Theorem 23.6.14 that we can even limit ourselves to square
matrices that are the product of an orthogonal matrix by a diagonal matrix. Since
multiplying W by a diagonal matrix merely scales its components while leaving
them independent and Gaussian, it follows that X is a centered Gaussian if, and
only if, its law is the same as the law of the result of applying an orthogonal
transformation to a random vector whose components are independent zero-mean
univariate Gaussians (not necessarily of equal variance).
In view of Deï¬nition 23.1.1, it is not surprising that applying a linear transfor-
mation to a Gaussian vector results in a Gaussian vector ((23.49) ahead). The
reverse is perhaps more surprising: X is a Gaussian vector if, and only if, the re-
sult of applying any deterministic linear functional to X has a univariate Gaussian
distribution (Theorem 23.6.17 ahead).
We conclude this section with the following pact with the reader.
(i) Unless preceded by the word â€œrandomâ€ or â€œGaussian,â€ all scalars, vectors,
and matrices in this chapter are deterministic.
(ii) Unless preceded by the word â€œcomplex,â€ all scalars, vectors, and matrices in
this chapter are real.
But, without violating this pact, we shall sometimes get excited and throw in the
words â€œrealâ€ and â€œdeterministicâ€ even when unnecessary.
23.2
Notation and Preliminaries
Our notation in this chapter expands upon the one introduced in Section 17.2. To
minimize page ï¬‚ipping, we repeat here parts of that section.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

496
The Multivariate Gaussian Distribution
Deterministic vectors are denoted by boldface lowercase letters such as w, whereas
random vectors are denoted by boldface uppercase letters such as W. When we
deal with deterministic matrices we make an exception to our rule of trying to
denote deterministic quantities by lowercase letters.1 Thus, deterministic matrices
are denoted by uppercase letters. But to make it clear that we are dealing with
a deterministic matrix and not a scalar random variable, we use special fonts to
distinguish the two. Thus A denotes a deterministic matrix, whereas A denotes a
random variable. Random matrices, which only appear brieï¬‚y in this book, are
denoted by uppercase letters of yet another font, e.g., H.
An n Ã— m deterministic real matrix A is an array of real numbers having n rows
and m columns
A =
â›
âœ
âœ
âœ
â
a(1,1)
a(1,2)
. . .
a(1,m)
a(2,1)
a(2,2)
. . .
a(2,m)
...
...
...
...
a(n,1)
a(n,2)
. . .
a(n,m)
â
âŸ
âŸ
âŸ
â .
The Row-j Column-â„“element of the matrix A is denoted
a(j,â„“)
or
[A]j,â„“.
The transpose of an nÃ—m matrix A is the mÃ—n matrix AT whose Row-j Column-â„“
entry is equal to the Row-â„“Column-j entry of A:
[AT]j,â„“= [A]â„“,j,
j âˆˆ{1, . . . , m}, â„“âˆˆ{1, . . . , n}.
We shall repeatedly use the fact that if the matrix-product AB is deï¬ned (i.e., if
the number of columns of A is the same as the number of rows of B), then the
transpose of the product is the product of the transposes in reverse order
(AB)T = BTAT.
(23.3)
The n Ã— n identity matrix whose diagonal elements are all 1 and whose oï¬€-
diagonal elements are all 0 is denoted In. The all-zero matrix whose components
are all zero is denoted 0.
An n Ã— 1 matrix is an n-vector, or a vector for short.
Thus, unless otherwise
speciï¬ed, all the vectors we shall encounter are column vectors.2 The components
of an n-vector a are denoted by a(1), . . . , a(n) so
a =
â›
âœ
â
a(1)
...
a(n)
â
âŸ
â ,
or, in a typographically more eï¬ƒcient form,
a =

a(1), . . . , a(n)T.
1We have already made some exceptions to this rule when we dealt with deterministic con-
stants that are by convention always denoted using uppercase letters, e.g., bandwidth W, ampli-
tude A, baud period Ts, etc.
2An exception to this rule is in our treatment of linear codes where the tradition of using row
vectors is too strong to change.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.3 Some Results on Matrices
497
The vector whose components are all zero is denoted by 0. The square root of the
sum of the squares of the components of a real n-vector a is denoted by âˆ¥aâˆ¥:
âˆ¥aâˆ¥=
A
B
B
C
n

â„“=1

a(â„“)2,
a âˆˆRn.
(23.4)
If a = (a(1), . . . , a(n))T and b = (b(1), . . . , b(n))T, then3
aTb =
n

â„“=1
a(â„“)b(â„“)
= bTa.
In particular,
âˆ¥aâˆ¥2 =
n

â„“=1

a(â„“)2
= aTa.
(23.5)
Note the diï¬€erence between aTa and aaT: the former is the scalar âˆ¥aâˆ¥2 whereas
the latter is the n Ã— n matrix whose Row-j Column-â„“element is a(j)a(â„“).
The determinant of a square matrix A is denoted by det A. We note that a matrix
and its transpose have equal determinants
det

AT
= det A,
(23.6)
and that the determinant of the product of two square matrices is the product of
the determinants
det (AB) = det (A) det (B).
(23.7)
We say that a square n Ã— n matrix A is singular if its determinant is zero or,
equivalently, if its columns are linearly dependent or, equivalently, if its rows are
linearly dependent or, equivalently, if there exists some nonzero vector Î± âˆˆRn
such that AÎ± = 0.
23.3
Some Results on Matrices
We next survey some of the results from Matrix Theory that we shall be using. Par-
ticularly important to us are results on positive semideï¬nite matrices, because, as
we shall see in Proposition 23.6.1, every covariance matrix is positive semideï¬nite,
and every positive semideï¬nite matrix is the covariance matrix of some random
vector.
3In (20.84) we denoted aTb by âŸ¨a, bâŸ©E.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

498
The Multivariate Gaussian Distribution
23.3.1
Orthogonal Matrices
Deï¬nition 23.3.1 (Orthogonal Matrices). An n Ã— n real matrix U is said to be
orthogonal if its transpose is its inverse
UUT = In.
(23.8)
As proved in (Axler, 2015, Section 7.C, Theorem 7.42), the condition (23.8) is
equivalent to the condition
UTU = In.
(23.9)
Thus, a real matrix is orthogonal if, and only if, its transpose is orthogonal.
If we write an n Ã— n matrix U in terms of its columns as
U =
â›
âœ
âœ
âœ
âœ
â
â†‘
Â· Â· Â·
â†‘
Â· Â· Â·
Ïˆ1
Â· Â· Â·
Ïˆn
Â· Â· Â·
â†“
Â· Â· Â·
â†“
â
âŸ
âŸ
âŸ
âŸ
â 
,
then (23.9) can be expressed as
In = UTU
=
â›
âœ
âœ
âœ
âœ
â
â†
ÏˆT
1
â†’
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
â†
ÏˆT
n
â†’
â
âŸ
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
âœ
â
â†‘
Â· Â· Â·
â†‘
Â· Â· Â·
Ïˆ1
Â· Â· Â·
Ïˆn
Â· Â· Â·
â†“
Â· Â· Â·
â†“
â
âŸ
âŸ
âŸ
âŸ
â 
=
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
ÏˆT
1 Ïˆ1
ÏˆT
1 Ïˆ2
Â· Â· Â·
ÏˆT
1 Ïˆn
ÏˆT
2 Ïˆ1
ÏˆT
2 Ïˆ2
Â· Â· Â·
ÏˆT
2 Ïˆn
...
...
...
...
ÏˆT
nÏˆ1
ÏˆT
nÏˆ2
Â· Â· Â·
ÏˆT
nÏˆn
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
,
thus showing that a real n Ã— n matrix U is orthogonal if, and only if, its n columns
Ïˆ1, . . . , Ïˆn satisfy
ÏˆT
Î½ ÏˆÎ½â€² = I{Î½ = Î½â€²},
Î½, Î½â€² âˆˆ{1, . . . , n}.
(23.10)
Using the same argument but starting with (23.8) we can prove a similar result
about the rows of an orthogonal matrix: if the rows of a real n Ã— n matrix U are
denoted by Ï†T
1 , . . . , Ï†T
n, i.e.,
U =
â›
âœ
âœ
âœ
âœ
â
â†
Ï†T
1
â†’
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
â†
Ï†T
n
â†’
â
âŸ
âŸ
âŸ
âŸ
â 
,
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.3 Some Results on Matrices
499
then U is orthogonal if, and only if,
Ï†T
Î½ Ï†Î½â€² = I{Î½ = Î½â€²},
Î½, Î½â€² âˆˆ{1, . . . , n}.
(23.11)
Recalling that the determinant of a product of square matrices is the product of
the determinants and that the determinant of a matrix is equal to the determinant
of its transpose, we obtain that for every square matrix U
det

UUT
=

det U
2.
(23.12)
Consequently, by taking the determinant of both sides of (23.8) we obtain that the
determinant of an orthogonal matrix must be either +1 or âˆ’1. It should, however,
be noted that there are numerous examples of matrices of unit determinant that
are not orthogonal.
We leave it to the reader to verify that a 2 Ã— 2 matrix is orthogonal if, and only if,
it is equal to one of the following matrices for some choice of âˆ’Ï€ â‰¤Î¸ < Ï€
	
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

,
	
cos Î¸
sin Î¸
sin Î¸
âˆ’cos Î¸

.
(23.13)
The former matrix corresponds to a rotation by Î¸ and has determinant +1, and
the latter to a reï¬‚ection followed by a rotation
	
cos Î¸
sin Î¸
sin Î¸
âˆ’cos Î¸

=
	
cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸

 	
1
0
0
âˆ’1

and has determinant âˆ’1.
23.3.2
Symmetric Matrices
A matrix A is said to be symmetric if it is equal to its transpose:
AT = A.
Only square matrices can be symmetric. A vector Ïˆ âˆˆRn is said to be an eigen-
vector of the matrix A corresponding to the real eigenvalue Î» âˆˆR if Ïˆ is nonzero
and if AÏˆ = Î»Ïˆ. The following is a key result about the eigenvectors of symmetric
real matrices.
Proposition 23.3.2 (Eigenvectors and Eigenvalues of Symmetric Real Matrices).
If A is a symmetric real n Ã— n matrix, then A has n (not necessarily distinct)
real eigenvalues Î»1, . . . , Î»n âˆˆR with corresponding eigenvectors Ïˆ1, . . . , Ïˆn âˆˆRn
satisfying
ÏˆT
Î½ ÏˆÎ½â€² = I{Î½ = Î½â€²},
Î½, Î½â€² âˆˆ{1, . . . , n}.
(23.14)
Proof. See, for example, (Axler, 2015, Section 7.B, Theorem 7.29), or (Herstein,
1975, Section 6.10, pp. 346â€“348), or (Horn and Johnson, 2013, Section 4.1, Theo-
rem 4.1.5).
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

500
The Multivariate Gaussian Distribution
The vectors Ïˆ1, . . . , Ïˆn are eigenvectors of the matrix A corresponding to the eigen-
values Î»1, . . . , Î»n if
AÏˆÎ½ = Î»Î½ÏˆÎ½,
Î½ âˆˆ{1, . . . , n}.
(23.15)
We next express this in an alternative way. We begin by noting that
A
â›
âœ
âœ
âœ
âœ
â
â†‘
Â· Â· Â·
â†‘
Â· Â· Â·
Ïˆ1
Â· Â· Â·
Ïˆn
Â· Â· Â·
â†“
Â· Â· Â·
â†“
â
âŸ
âŸ
âŸ
âŸ
â 
=
â›
âœ
âœ
âœ
âœ
â
â†‘
Â· Â· Â·
â†‘
Â· Â· Â·
AÏˆ1
Â· Â· Â·
AÏˆn
Â· Â· Â·
â†“
Â· Â· Â·
â†“
â
âŸ
âŸ
âŸ
âŸ
â 
and that
â›
âœ
âœ
âœ
âœ
â
â†‘
Â· Â· Â·
â†‘
Â· Â· Â·
Ïˆ1
Â· Â· Â·
Ïˆn
Â· Â· Â·
â†“
Â· Â· Â·
â†“
â
âŸ
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
âœ
â
Î»1
0
Â· Â· Â·
0
0
Î»2
...
...
...
...
...
0
0
Â· Â· Â·
0
Î»n
â
âŸ
âŸ
âŸ
âŸ
â 
=
â›
âœ
âœ
âœ
âœ
â
â†‘
Â· Â· Â·
â†‘
Â· Â· Â·
Î»1Ïˆ1
Â· Â· Â· Î»nÏˆn
Â· Â· Â·
â†“
Â· Â· Â·
â†“
â
âŸ
âŸ
âŸ
âŸ
â 
.
Consequently, Condition (23.15) can be written as
AU = UÎ›,
(23.16)
where
U =
â›
âœ
âœ
âœ
âœ
â
â†‘
Â· Â· Â·
â†‘
Â· Â· Â·
Ïˆ1
Â· Â· Â·
Ïˆn
Â· Â· Â·
â†“
Â· Â· Â·
â†“
â
âŸ
âŸ
âŸ
âŸ
â 
and
Î› =
â›
âœ
âœ
âœ
âœ
â
Î»1
0
Â· Â· Â·
0
0
Î»2
...
...
...
...
...
0
0
Â· Â· Â·
0
Î»n
â
âŸ
âŸ
âŸ
âŸ
â 
.
(23.17)
Condition (23.14) is equivalent to the condition that the above matrix U is orthog-
onal. By multiplying (23.16) from the right by the inverse of U (which, because U
is orthogonal and by (23.8), is UT) we obtain the equivalent form A = UÎ›UT.
Consequently, an equivalent statement of Proposition 23.3.2 is:
Proposition 23.3.3 (Spectral Theorem for Real Symmetric Matrices). A sym-
metric real n Ã— n matrix A can be written in the form
A = UÎ›UT
where, as in (23.17), Î› is a diagonal real n Ã— n matrix whose diagonal elements
are the eigenvalues of A, and where U is a real n Ã— n orthogonal matrix whose Î½-th
column is an eigenvector of A corresponding to the eigenvalue in the Î½-th position
on the diagonal of Î›.
The reverse is also true: if A = UÎ›UT for a real diagonal matrix Î› and for a real
orthogonal matrix U, then A is symmetric, its eigenvalues are the diagonal elements
of Î›, and the Î½-th column of U is an eigenvector of the matrix A corresponding to
the eigenvalue in the Î½-th position on the diagonal of Î›.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.3 Some Results on Matrices
501
23.3.3
Positive Semideï¬nite Matrices
Deï¬nition 23.3.4 (Positive Semideï¬nite and Positive Deï¬nite Matrices).
(i) We say that the n Ã— n real matrix K is positive semideï¬nite or nonneg-
ative deï¬nite and write
K âª°0
if K is symmetric and
Î±TKÎ± â‰¥0,
Î± âˆˆRn.
(ii) We say that the n Ã— n real matrix K is positive deï¬nite and write
K â‰»0
if K is symmetric and
Î±TKÎ± > 0,

Î± Ì¸= 0, Î± âˆˆRn
.
The following two propositions characterize positive semideï¬nite and positive def-
inite matrices. For proofs, see (Axler, 2015, Section 7.C, Theorem 7.35).
Proposition 23.3.5 (Characterizing Positive Semideï¬nite Matrices). Let K be a
real n Ã— n matrix. Then the statement that K is positive semideï¬nite is equivalent
to each of the following statements:
(a) The matrix K can be written in the form
K = STS
(23.18)
for some real n Ã— n matrix S.4
(b) The matrix K is symmetric and all its eigenvalues are nonnegative.
(c) The matrix K can be written in the form
K = UÎ›UT,
(23.19)
where Î› is a real n Ã— n diagonal matrix with nonnegative entries on the
diagonal and where U is a real n Ã— n orthogonal matrix.
Proposition 23.3.6 (Characterizing Positive Deï¬nite Matrices). Let K be a real
n Ã— n matrix. Then the statement that K is positive deï¬nite is equivalent to each
of the following statements.
(a) The matrix K can be written in the form K = STS for some real n Ã— n
nonsingular matrix S.
(b) The matrix K is symmetric and all its eigenvalues are positive.
4Even if S is not a square matrix, STS âª°0.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

502
The Multivariate Gaussian Distribution
(c) The matrix K can be written in the form
K = UÎ›UT,
where Î› is a real n Ã— n diagonal matrix with positive entries on the diagonal
and where U is a real n Ã— n orthogonal matrix.
Given a positive semideï¬nite matrix K, how can we ï¬nd a matrix S satisfying
K = STS? In general, there can be many such matrices. For example, if K is the
identity matrix, then S can be any orthogonal matrix. We mention here two useful
choices. Being symmetric, the matrix K can be written in the form
K = UÎ›UT,
(23.20)
where U and Î› are as in (23.17). Since K is positive semideï¬nite, the diagonal
elements of Î› (which are the eigenvalues of K) are nonnegative. Consequently, we
can deï¬ne the matrix
Î›1/2 =
â›
âœ
âœ
âœ
âœ
â
âˆšÎ»1
0
Â· Â· Â·
0
0 âˆšÎ»2
...
...
...
...
...
0
0
Â· Â· Â·
0 âˆšÎ»n
â
âŸ
âŸ
âŸ
âŸ
â 
.
One choice of the matrix S is
S = Î›1/2UT.
(23.21)
Indeed, with this deï¬nition of S we have
STS =

Î›1/2UTTÎ›1/2UT
= UÎ›1/2Î›1/2UT
= UÎ›UT
= K,
where the ï¬rst equality follows from the deï¬nition of S; the second from the rule
(AB)T = BTAT and from the symmetry of the diagonal matrix Î›1/2; the third from
the deï¬nition of Î›1/2; and where the ï¬nal equality follows from (23.20).
A diï¬€erent choice for S, which will be less useful to us in this chapter, is5
UÎ›1/2UT.
The following lemmas will be used in Section 23.4.3 when we study random vectors
of singular covariance matrices.
Lemma 23.3.7. Let K be a real n Ã— n positive semideï¬nite matrix, and let Î± be a
vector in Rn. Then Î±TKÎ± = 0 if, and only if, KÎ± = 0.
5This is the only choice for S that is positive semideï¬nite (Axler, 2015, Section 7.C, Theo-
rem 7.36), (Horn and Johnson, 2013, Section 7.2, Theorem 7.2.6).
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.4 Random Vectors
503
Proof. One direction is trivial and does not require that K be positive semideï¬nite:
if KÎ± = 0, then Î±TKÎ± must also be equal to zero. Indeed, in this case we have by
the associativity of matrix multiplication Î±TKÎ± = Î±T(KÎ±) = Î±T0 = 0.
To prove the other direction, we ï¬rst note that, since K is positive semideï¬nite,
there exists some n Ã— n matrix S such that K = STS. Hence,
Î±TKÎ± = Î±TSTSÎ±
= (SÎ±)T(SÎ±)
= âˆ¥SÎ±âˆ¥2 ,
Î± âˆˆRn,
where the second equality follows from the rule for transposing a product (23.3),
and where the third equality follows from (23.5). Consequently, if Î±TKÎ± = 0, then
âˆ¥SÎ±âˆ¥2 = 0, so SÎ± = 0, and hence STSÎ± = 0, i.e., KÎ± = 0.
Lemma 23.3.8. If K is a real n Ã— n positive deï¬nite matrix, then Î±TKÎ± = 0 if,
and only if, Î± = 0.
Proof. Follows directly from Deï¬nition 23.3.4 of positive deï¬nite matrices.
23.4
Random Vectors
23.4.1
Deï¬nitions
Recall that an n-dimensional random vector or a random n-vector X de-
ï¬ned over the probability space (Î©, F, P) is a (measurable) mapping from the set
of experiment outcomes Î© to the n-dimensional Euclidean space Rn. A random
vector X is very much like a random variable, except that rather than taking values
in the real line R, it takes values in Rn. In fact, an n-dimensional random vector
can be viewed as an array of n random variables.6
The density of a random vector is the joint density of its components. The density
of a random n-vector is thus a nonnegative (Borel measurable) function from Rn
to the nonnegative reals that integrates to one.
Similarly, an nÃ—m random matrix H is an nÃ—m array of random variables deï¬ned
over a common probability space.
23.4.2
Expectations and Covariance Matrices
The expectation E[X] of a random n-vector X = (X(1), . . . , X(n))T is a vector
whose components are the expectations of the corresponding components of X:7
E[X] â‰œ

E

X(1)
, . . . , E

X(n)T
.
(23.22)
6In dealing with random vectors one often abandons the â€œcoordinate freeâ€ approach and views
vectors in a particular coordinate system. This allows one to speak of the covariance matrix in
more familiar terms.
7The expectation of a random vector is only deï¬ned if the expectation of each of its compo-
nents is deï¬ned.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

504
The Multivariate Gaussian Distribution
The j-th element of E[X] is thus the expectation of the j-th component of X,
namely, E

X(j)
. Similarly, the expectation of a random matrix is the matrix of
expectations.
If all the components of a random n-vector X are of ï¬nite variance, then we say
that X is of ï¬nite variance. We then deï¬ne its n Ã— n covariance matrix KXX
as
KXX â‰œE
%
(X âˆ’E[X]) (X âˆ’E[X])T&
.
(23.23)
That is,
KXX = E
â¡
â¢â¢â¢â¢â£
â›
âœ
âœ
âœ
âœ
â
X(1) âˆ’E

X(1)
...
...
X(n) âˆ’E

X(n)
â
âŸ
âŸ
âŸ
âŸ
â 

X(1) âˆ’E

X(1)
Â· Â· Â·
Â· Â· Â·
X(n) âˆ’E

X(n)
â¤
â¥â¥â¥â¥â¦
=
â›
âœ
âœ
âœ
â
Var

X(1)
Cov

X(1), X(2)
Â· Â· Â·
Cov

X(1), X(n)
Cov

X(2), X(1)
Var

X(2)
Â· Â· Â·
Cov

X(2), X(n)
...
...
...
...
Cov

X(n), X(1)
Cov

X(n), X(2)
Â· Â· Â·
Var

X(n)
â
âŸ
âŸ
âŸ
â .
(23.24)
If n = 1 and the n-dimensional random vector X hence a scalar, then the covariance
matrix KXX is a 1 Ã— 1 matrix whose sole component is the variance of the sole
component of X.
Note that, from the nÃ—n covariance matrix KXX of a random n-vector X, it is easy
to compute the covariance matrix of a subset of Xâ€™s components. For example, if
we are only interested in the 2 Ã— 2 covariance matrix of (X(1), X(2))T, then we just
pick the ï¬rst two columns and the ï¬rst two rows of KXX. More generally, the r Ã— r
covariance matrix of (X(j1), X(j2), . . . , X(jr))T for 1 â‰¤j1 < j2 < Â· Â· Â· < jr â‰¤n is
obtained from KXX by picking Rows and Columns j1, . . . , jr. For example, if
KXX =
â›
âœ
âœ
â
30
31
9
7
31
39
11
13
9
11
9
12
7
13
12
26
â
âŸ
âŸ
â ,
then the covariance matrix of

X(2), X(4)T is
 39 13
13 26

.
We next explore the behavior of the mean vector and the covariance matrix of
a random vector when it is multiplied by a deterministic matrix. Regarding the
mean, we shall show that since matrix multiplication is a linear transformation, it
commutes with the expectation operation. Consequently, if H is a random n Ã— m
matrix and A is a deterministic Î½ Ã— n matrix, then
E[AH] = AE[H] ,
(23.25a)
and similarly if B is a deterministic m Ã— Î½ matrix, then
E[HB] = E[H] B.
(23.25b)
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.4 Random Vectors
505
To prove (23.25a) we write out the Row-j Column-â„“element of the Î½ Ã— m ma-
trix E[AH] and use the linearity of expectation to relate it to the Row-j Column-â„“
element of the matrix AE[H]:

E[AH]

j,â„“= E

n

Îº=1
[A]j,Îº[H]Îº,â„“

=
n

Îº=1
E
%
[A]j,Îº[H]Îº,â„“
&
=
n

Îº=1
[A]j,ÎºE

[H]Îº,â„“

=

AE[H]

j,â„“,
j âˆˆ{1, . . . , Î½}, â„“âˆˆ{1, . . . , m}.
The proof of (23.25b) is almost identical and is omitted.
The transpose operation also commutes with expectation: if H is a random matrix
then
E

HT
= (E[H])T .
(23.26)
As to the covariance matrix, we next show that if A is a deterministic matrix and
if X is a random vector, then the covariance matrix KYY of the random vector
Y = AX can be expressed in terms of the covariance matrix KXX of X as
KYY = A KXX AT,
Y = AX.
(23.27)
Indeed,
KYY â‰œE

(Y âˆ’E[Y])(Y âˆ’E[Y])T
= E

(AX âˆ’E[AX])(AX âˆ’E[AX])T
= E

A(X âˆ’E[X])(A(X âˆ’E[X]))T
= E

A(X âˆ’E[X])(X âˆ’E[X])TAT
= AE

(X âˆ’E[X])(X âˆ’E[X])TAT
= AE

(X âˆ’E[X])(X âˆ’E[X])T
AT
= A KXX AT.
A key property of covariance matrices is that, as we shall next show, they are all
positive semideï¬nite. That is, the covariance matrix KXX of any random vector X
is a symmetric matrix satisfying
Î±T KXX Î± â‰¥0,
Î± âˆˆRn.
(23.28)
(In Proposition 23.6.1 we shall see that this property fully characterizes covari-
ance matrices: every positive semideï¬nite matrix is the covariance matrix of some
random vector.)
To prove (23.28) it suï¬ƒces to consider the case where X is of zero mean because
the covariance matrix of X is the same as the covariance matrix of X âˆ’E[X]. The
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

506
The Multivariate Gaussian Distribution
symmetry of KXX follows from the deï¬nition of the covariance matrix (23.23); from
the fact that expectation and transposition commute (23.26); and from the formula
for the transpose of a product of matrices (23.3):
KT
XX =

E

XXTT
= E
%
XXTT&
= E

XXT
= KXX .
(23.29)
The nonnegativity of Î±T KXX Î± for any deterministic Î± âˆˆRn follows by noting
that by (23.27) (applied with A = Î±T) the term Î±T KXX Î± is the variance of the
scalar random variable Î±TX, i.e.,
Î±T KXX Î± = Var

Î±TX

(23.30)
and, as such, is nonnegative.
23.4.3
Singular Covariance Matrices
A random vector having a singular covariance matrix can be unwieldy because
it cannot have a probability density function. Indeed, as we shall see in Corol-
lary 23.4.2, any such random vector has at least one component that is determined
(with probability one) by the other components. In this section we shall propose
a way of manipulating such vectors. Roughly speaking, the idea is that if X has a
singular covariance matrix, then we choose a subset of its components so that the
covariance matrix of the chosen subset be nonsingular and so that each compo-
nent that was not chosen be equal (with probability one) to a deterministic aï¬ƒne
function of the chosen components. We then manipulate only the chosen compo-
nents and, with some deterministic bookkeeping â€œon the side,â€ take care of the
components that were not chosen. This idea is made precise in Corollary 23.4.3.
To illustrate the idea, suppose that X is a zero-mean random vector of covariance
matrix
KXX =
â›
â
3
5
7
5
9
13
7
13
19
â
â .
An application of Proposition 23.4.1 ahead will show that because the three columns
of KXX satisfy the linear relationship
âˆ’
â›
â
3
5
7
â
â + 2
â›
â
5
9
13
â
â âˆ’
â›
â
7
13
19
â
â = 0,
it follows that
âˆ’X(1) + 2X(2) âˆ’X(3) = 0,
with probability one.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.4 Random Vectors
507
Consequently, in manipulating X we can pick the two components X(2), X(3),
which are of nonsingular covariance matrix
 9 13
13 19

(obtained by picking the last
two rows and the last two columns of KXX), and keep track â€œon the sideâ€ of the
fact that X(1) is equal, with probability one, to 2X(2) âˆ’X(3). We could, of course,
also pick the components X(1), X(2) of nonsingular covariance matrix
 3 5
5 9

and
keep track â€œon the sideâ€ of the relationship X(3) = 2X(2) âˆ’X(1).
To avoid cumbersome language, for the remainder of this section we shall take all
equalities between random variables to stand for equalities with probability one.
Thus, if we write X(1) = 2X(2) âˆ’X(3) we mean that the probability that X(1) is
equal to 2X(2) âˆ’X(3) is one.
The justiï¬cation of the procedure is in the following proposition and its two corol-
laries.
Proposition 23.4.1. Let X be a zero-mean random n-vector of covariance ma-
trix KXX. Then its â„“-th component X(â„“) is equal (with probability one) to a linear
combination of X(â„“1), . . . , X(â„“Î·) if, and only if, the â„“-th column of KXX is a linear
combination of Columns â„“1, . . . , â„“Î·. Here â„“, Î·, â„“1, . . . , â„“Î· âˆˆ{1, . . . , n} are arbitrary.
Proof. If â„“âˆˆ{â„“1, . . . , â„“Î·}, then the result is trivial. We shall therefore present a
proof only for the case where â„“/âˆˆ{â„“1, . . . , â„“Î·}. In this case, the â„“-th component of
the random n-vector X is a linear combination of the Î· components X(â„“1), . . . , X(â„“Î·)
if, and only if, there exists a vector Î± âˆˆRn satisfying
Î±(â„“) = âˆ’1,
(23.31a)
Î±(Îº) = 0,
Îº /âˆˆ{â„“, â„“1, . . . , â„“Î·} ,
(23.31b)
and
Î±TX = 0.
(23.31c)
Since X is of zero mean, the condition Î±TX = 0 is equivalent to the condition
Var

Î±TX

= 0. By (23.30) and Lemma 23.3.7 this latter condition is equivalent
to the condition KXX Î± = 0. Now KXX Î± is a linear combination of the columns
of KXX where the ï¬rst column is multiplied by Î±(1), the second by Î±(2), etc. Con-
sequently, the condition that KXX Î± = 0 for some Î± âˆˆRn satisfying (23.31a) &
(23.31b) is equivalent to the condition that the â„“-th column of KXX is a linear
combination of Columns â„“1, . . . , â„“Î·.
Corollary 23.4.2. The covariance matrix of a zero-mean random n-vector X is
singular if, and only if, some component of X is a linear combination of the other
components.
Proof. Follows from Proposition 23.4.1 by noting that a square matrix is singular
if, and only if, its columns are linearly dependent.
Corollary 23.4.3. Let X be a zero-mean random n-vector of covariance matrix KXX.
If Columns â„“1, . . . , â„“d of KXX form a basis for the subspace of Rn spanned by the
columns of KXX, then every component of X can be written as a linear combination
of the components X(â„“1), . . . , X(â„“d), and the random d-vector

X(â„“1), . . . , X(â„“d)T
has a nonsingular d Ã— d covariance matrix.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

508
The Multivariate Gaussian Distribution
Proof. Since Columns â„“1, . . . , â„“d form a basis for the subspace spanned by the
columns of KXX, every column â„“can be written as a linear combination of these
columns. Consequently, by Proposition 23.4.1, every component of X can be writ-
ten as a linear combination of X(â„“1), . . . , X(â„“d). To prove that the d Ã— d covariance
matrix KËœXËœX of the random d-vector ËœX =

X(â„“1), . . . , X(â„“d)T is nonsingular, we
note that if this were not the case, then by Corollary 23.4.2 applied to ËœX it would
follow that one of the components of ËœX is a linear combination of the other d âˆ’1
components. But by Proposition 23.4.1 applied to X, this would imply that the
columns â„“1, . . . , â„“d of KXX are not linearly independent, in contradiction to the
corollaryâ€™s hypothesis that they form a basis.
23.4.4
The Characteristic Function
If X is a random n-vector, then its characteristic function Î¦X(Â·) is a mapping
from Rn to C that maps each vector Ï– = (Ï–(1), . . . Ï–(n))T in Rn to Î¦X(Ï–), where
Î¦X(Ï–) â‰œE
%
eiÏ–TX&
= E

exp
	
i
n

â„“=1
Ï–(â„“)X(â„“)


,
Ï– âˆˆRn.
If X has the density fX(Â·), then
Î¦X(Ï–) =
 âˆ
âˆ’âˆ
Â· Â· Â·
 âˆ
âˆ’âˆ
fX(x) ei n
â„“=1 Ï–(â„“)x(â„“) dx(1) Â· Â· Â· dx(n),
which is reminiscent of the multi-dimensional Fourier Transform of fX(Â·) (ignoring
2Ï€â€™s and the sign of i).
Proposition 23.4.4 (Identical Distributions and Characteristic Functions). Two
random n-vectors X, Y are of the same distribution if, and only if, they have
identical characteristic functions:

X
L= Y

â‡â‡’

Î¦X(Ï–) = Î¦Y(Ï–),
Ï– âˆˆRn
.
(23.32)
Proof. See (Dudley, 2003, Chapter 9, Section 5, Theorem 9.5.1).
This proposition is extremely useful. We shall demonstrate its power by using it
to show that two random variables X and Y are independent if, and only if,
E
%
ei(Ï–1X+Ï–2Y )&
= E

eiÏ–1X
E

eiÏ–2Y 
,
Ï–1, Ï–2 âˆˆR.
(23.33)
One direction is straightforward. If X and Y are independent, then for any Borel
measurable functions g(Â·) and h(Â·) the random variables g(X) and h(Y ) are also
independent. Thus, the independence of X and Y implies the independence of the
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.5 A Standard Gaussian Vector
509
random variables eiÏ–1X and eiÏ–2Y and hence implies that the expectation of their
product is the product of their expectations:
E
%
ei(Ï–1X+Ï–2Y )&
= E

eiÏ–1X eiÏ–2Y 
= E

eiÏ–1X
E

eiÏ–2Y 
,
Ï–1, Ï–2 âˆˆR.
As to the other direction, suppose that Xâ€² has the same law as X, that Y â€² has the
same law as Y , and that Xâ€² and Y â€² are independent. Since Xâ€² has the same law
as X, it follows that
E
%
eiÏ–1Xâ€²&
= E

eiÏ–1X
,
Ï–1 âˆˆR,
(23.34)
and similarly for Y â€²
E
%
eiÏ–2Y â€²&
= E

eiÏ–2Y 
,
Ï–2 âˆˆR.
(23.35)
Consequently, since Xâ€² and Y â€² are independent
E
%
ei(Ï–1Xâ€²+Ï–2Y â€²)&
= E
%
eiÏ–1Xâ€² eiÏ–2Y â€²&
= E
%
eiÏ–1Xâ€²&
E
%
eiÏ–2Y â€²&
= E

eiÏ–1X
E

eiÏ–2Y 
,
Ï–1, Ï–2 âˆˆR,
where the third equality follows from (23.34) and (23.35).
We thus see that if (23.33) holds, then the characteristic function of the vector
(X, Y )T is identical to the characteristic function of the vector (Xâ€², Y â€²)T.
By
Proposition 23.4.4 the joint distribution of (X, Y ) must then be the same as the
joint distribution of (Xâ€², Y â€²). Since according to the latter distribution the two
components are independent, it follows that the same must be true according to
the former, i.e., X and Y must be independent.
23.5
A Standard Gaussian Vector
Recall Deï¬nition 23.1.1 that a random n-vector W is a standard Gaussian if its n
components are independent zero-mean unit-variance Gaussian random variables.
Its density fW(Â·) is then given by
fW(w) =
n
@
â„“=1
-
1
âˆš
2Ï€ exp
	
âˆ’

w(â„“)2
2

.
=
1
(2Ï€)n/2 exp
	
âˆ’1
2
n

â„“=1

w(â„“)2

= (2Ï€)âˆ’n/2 eâˆ’1
2 âˆ¥wâˆ¥2,
w âˆˆRn.
(23.36)
The deï¬nition of a standard Gaussian random vector is an extension of the deï¬-
nition of a standard Gaussian random variable: the sole component of a standard
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

510
The Multivariate Gaussian Distribution
one-dimensional Gaussian vector is a scalar N(0, 1) random variable. Conversely,
every N(0, 1) random variable can be viewed as a one-dimensional standard Gaus-
sian.
If W is a standard Gaussian random n-vector then, as we next show, its mean
vector and covariance matrix are given by
E[W] = 0,
and
KWW = In.
(23.37)
Indeed, the mean of a random vector is the vector of the means (23.22), so the
fact that E[W] = 0 is a consequence of all the components of W having zero
mean. And using (23.24) it can be easily shown that the covariance matrix of W
is the identity matrix because the components of W are independent and hence, a
fortiori uncorrelated, and because they are each of unit variance.
23.6
Gaussian Random Vectors
Recall Deï¬nition 23.1.1 that a random n-vector X is said to be Gaussian if for some
positive integer m there exists an n Ã— m matrix A; a standard Gaussian random
m-vector W; and a deterministic vector Î¼ âˆˆRn such that
X
L= AW + Î¼.
(23.38)
From (23.38), from the second order properties of standard Gaussians (23.37),
and from the behavior of the mean vector and covariance matrix under linear
transformation (23.25a) & (23.27) we obtain

X
L= AW + Î¼ and W standard

=â‡’

E[X] = Î¼ and KXX = AAT
.
(23.39)
Recall also that X is a centered Gaussian if X
L= AW for A and W as above.
Every standard Gaussian vector is a centered Gaussian because every standard
Gaussian n-vector W is equal to AW when A is the n Ã— n identity matrix In.
The reverse is not true: not every centered Gaussian is a standard Gaussian.
Indeed, standard Gaussians have the identity covariance matrix (23.37), whereas
the centered Gaussian vector AW has, by (23.39), the covariance matrix AAT,
which need not be the identity matrix.
Also, X is a Gaussian vector if, and only if, X âˆ’E[X] is a centered Gaussian
because, by (23.39),

X
L= AW + Î¼ for some Î¼ âˆˆRn and W standard Gaussian

â‡â‡’

X
L= AW + E[X] and W standard Gaussian

â‡â‡’

X âˆ’E[X]
L= AW and W standard Gaussian

.
(23.40)
From (23.40) it also follows that the centered Gaussians are the Gaussian vectors
of zero mean.8
8Thus, the name â€œcentered Gaussian,â€ which we gave in Deï¬nition 23.1.1 was not misleading.
A vector is a â€œcentered Gaussianâ€ if, and only if, it is Gaussian and centered.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.6 Gaussian Random Vectors
511
Using the deï¬nition of a centered Gaussian and using (23.39) we can readily show
that every positive semideï¬nite matrix is the covariance matrix of some centered
Gaussian. In fact, more is true:
Proposition 23.6.1 (Covariance Matrices and Positive Semideï¬nite Matrices).
The covariance matrix of every ï¬nite-variance random vector is positive semideï¬-
nite, and every positive semideï¬nite matrix is the covariance matrix of some cen-
tered Gaussian random vector.
Proof. The covariance matrix of every random vector is positive semideï¬nite be-
cause every covariance matrix is symmetric (23.29) and satisï¬es (23.28). We next
establish the reverse. Given an n Ã— n positive semideï¬nite matrix K we shall con-
struct a centered Gaussian X whose covariance matrix KXX is equal to K. We begin
by noting that, since K is positive semideï¬nite, it follows from Proposition 23.3.5
that there exists some n Ã— n matrix S such that STS = K. Let W be a standard
Gaussian n-vector and consider the vector X = STW. Being the result of a linear
transformation of the standard Gaussian W, this vector is a centered Gaussian. We
complete the proof by showing that its covariance matrix KXX is the prespeciï¬ed
matrix K. This follows from the calculation
KXX = STS
= K,
where the ï¬rst equality follows from (23.39) (by substituting ST for A and 0 for Î¼)
and the second from our choice of S as satisfying STS = K.
23.6.1
Examples and Basic Properties
In this section we provide some examples of Gaussian vectors and some simple
properties that follow from their deï¬nition.
(i) Every univariate N

Î¼, Ïƒ2
random variable, when viewed as a one dimen-
sional random vector, is a Gaussian random vector.
Proof:
Such a univariate random variable has the same law as
ÏƒW + Î¼, when W is a standard univariate Gaussian.
(ii) Any deterministic vector is a Gaussian vector.
Proof: Choose the matrix A as the all-zero matrix 0.
(iii) If the components of X are independent univariate Gaussians (not necessarily
of equal variance), then X is a Gaussian vector.
Proof: Choose A to be an appropriate diagonal matrix.
For the purposes of stating the next proposition we remind the reader that the ran-
dom vectors X =

X(1), . . . , X(nx)T and Y =

Y (1), . . . , Y (ny)T are independent
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

512
The Multivariate Gaussian Distribution
if, for every choice of Î¾1, . . . , Î¾nx âˆˆR and Î·1, . . . , Î·ny âˆˆR,
Pr
%
X(1) â‰¤Î¾1, . . . , X(nx) â‰¤Î¾nx, Y (1) â‰¤Î·1, . . . , Y (ny) â‰¤Î·ny
&
= Pr
%
X(1) â‰¤Î¾1, . . . , X(nx) â‰¤Î¾nx
&
Pr
%
Y (1) â‰¤Î·1, . . . , Y (ny) â‰¤Î·ny
&
.
The following proposition is a consequence of the fact that if X1 & X2 are inde-
pendent, X1
L= Xâ€²
1, X2
L= Xâ€²
2, and Xâ€²
1 & Xâ€²
2 are independent, then
	X1
X2

L=
	Xâ€²
1
Xâ€²
2

.
Proposition 23.6.2 (Stacking Independent Gaussian Vectors). Stacking two in-
dependent Gaussian vectors on top of each other results in a Gaussian vector.
Proof. Let the random n1-vector X1 = (X(1)
1 , . . . , X(n1)
1
)T be Gaussian, and let the
random n2-vector X2 = (X(1)
2 , . . . , X(n2)
2
)T be Gaussian and independent of X1.
We need to show that the (n1 + n2)-vector

X(1)
1 , . . . , X(n1)
1
, X(1)
2 , . . . , X(n2)
2
T
(23.41)
is Gaussian.
Let the n1 Ã— m1 matrix A1 and the vector Î¼1 âˆˆRn1 represent X1 in the sense that
X1
L= A1W1 + Î¼1,
(23.42)
whenever W1 is a standard Gaussian m1-vector. Likewise, let the n2 Ã— m2 ma-
trix A2 and the vector Î¼2 âˆˆRn2 represent X2 in the sense that
X2
L= A2W2 + Î¼2,
(23.43)
whenever W2 is a standard Gaussian m2-vector. Let W be a standard Gaussian
(m1 +m2)-vector and let W1 comprise its ï¬rst m1 components and W2 its last m2
components.
It follows from the deï¬nition of a standard Gaussian vector that
W1 and W2 are independent standard Gaussian vectors. Since they are standard,
both (23.42) and (23.43) hold. And since they are independent, so are their aï¬ƒne
transformations, so
A1W1 + Î¼1 is independent of A2W2 + Î¼2.
(23.44)
We conclude the proof by showing that the vector (23.41) can be represented using
the (n1 +n2)Ã—(m1 +m2) block-diagonal matrix A of diagonal components A1 and
A2, and using the vector Î¼ âˆˆRn1+n2 that results when the vector Î¼1 is stacked on
top of the vector Î¼2:
A =
	A1
0
0
A2

Î¼ =
	
Î¼1
Î¼2

.
(23.45)
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.6 Gaussian Random Vectors
513
Indeed, if W is a standard Gaussian (m1 + m2)-vector and if W1 and W2 are as
above, then
AW + Î¼ =
	
A1
0
0
A2

 	W1
W2

+
	Î¼1
Î¼2

(23.46)
=
	
A1W1 + Î¼1
A2W2 + Î¼2

(23.47)
L=
	
X1
X2

,
(23.48)
where the ï¬rst equality follows from the deï¬nitions of A and Î¼ (23.45) and of W1
and W2; the second by computing the matrix product in blocks; and where the
equality in distribution can be argued as follows: The ï¬rst n1 components of the
vector on the RHS of (23.47) have the same joint law as the ï¬rst n1 components
of the vector on the RHS of (23.48) by (23.42), and likewise the last n2 componets
by (23.43). Additionally, in both vectors the ï¬rst n1 components and the last n2
components are independent: in the former vector this follows from (23.44) and in
the latter from the hypothesis that X1 and X2 are independent.
Proposition 23.6.3 (An Aï¬ƒne Transformation of a Gaussian Is a Gaussian).
Let X be a Gaussian n-vector. If C is a Î½ Ã— n matrix and if d âˆˆRÎ½, then the
random Î½-vector CX + d is Gaussian.
Proof. If X
L= AW + Î¼, where A is a deterministic n Ã— m matrix, W is a standard
Gaussian m-vector, and Î¼ âˆˆRn, then
CX + d
L= C(AW + Î¼) + d
= (CA)W + (CÎ¼ + d),
(23.49)
which demonstrates that CX+d is Gaussian, because (CA) is a deterministic Î½ Ã—m
matrix, W is a standard Gaussian m-vector, and CÎ¼ + d is a deterministic vector
in RÎ½.
This proposition has some important consequences. The ï¬rst is that if we permute
the components of a Gaussian vector then the resulting vector is also Gaussian.
This explains why we sometimes say of random variables that they are jointly Gaus-
sian without specifying an order. Indeed, by the following corollary, the Gaussianity
of (X, Y, Z)T is equivalent to the Gaussianity of (Y, X, Z)T, etc.
Corollary 23.6.4. Permuting the components of a Gaussian vector results in a
Gaussian vector.
Proof. Follows from Proposition 23.6.3 by choosing C to be the appropriate per-
mutation matrix, i.e., the matrix that results from permuting the columns of the
identity matrix. For example,
â›
â
X(3)
X(1)
X(2)
â
â =
â›
â
0
0
1
1
0
0
0
1
0
â
â 
â›
â
X(1)
X(2)
X(3)
â
â .
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

514
The Multivariate Gaussian Distribution
Corollary 23.6.5 (Subsets of Jointly Gaussians Are Jointly Gaussian). Construct-
ing a random p-vector from a Gaussian n-vector by picking p of its components
(allowing for repetition) yields a Gaussian vector.
Proof. Let X be a Gaussian n-vector. For any choice of j1, . . . , jp âˆˆ{1, . . . , n},
we can express the random p-vector (X(j1), . . . , X(jp))T as CX, where C is a deter-
ministic p Ã— n matrix whose Row-Î½ Column-â„“component is given by
[C]Î½,â„“= I{â„“= jÎ½}.
For example
	
X(3)
X(1)

=
	
0
0
1
1
0
0

 â›
â
X(1)
X(2)
X(3)
â
â .
The result thus follows from Proposition 23.6.3.
Proposition 23.6.6. Each component of a Gaussian vector is a univariate Gaus-
sian.
Proof. Let X be a Gaussian n-vector, and let j âˆˆ{1, . . . , n} be arbitrary. We
need to show that X(j) is Gaussian. Since X is Gaussian, there exist an n Ã— m
matrix A, a vector Î¼ âˆˆRn, and a standard Gaussian W such that the vector X
has the same law as the random vector AW + Î¼ (Deï¬nition 23.1.1). In particular,
the j-th component of X has the same law as the j-th component of AW + Î¼, i.e.,
X(j) L=
m

â„“=1
a(j,â„“)W (â„“) + Î¼(j),
j âˆˆ{1, . . . , n}.
The sum on the RHS is a linear combination of the independent univariate Gaus-
sians W (1), . . . , W (m) and is thus, by Proposition 19.7.3, Gaussian. The result of
adding Î¼(j) is still Gaussian.
We caution the reader that while each component of a Gaussian vector has a
univariate Gaussian distribution, there exist random vectors that are not Gaussian
and that yet have Gaussian components.
23.6.2
The Mean and Covariance Determine the Law of a Gaussian
From (23.39) it follows that if X
L= AW + Î¼, where W is a standard Gaussian,
then Î¼ must be equal to E[X]. Thus, the mean of X fully determines the vector Î¼.
The matrix A, however, is not determined by the covariance of X. Indeed, by
(23.39), the covariance matrix KXX of X is equal to AAT, so KXX only determines
the product AAT.
Since there are many diï¬€erent ways to express KXX as the
product of a matrix by its transpose, there are many choices of A (even of diï¬€erent
dimensions) that result in AW + Î¼ having the given covariance matrix. Prima
facie, one might think that these diï¬€erent choices for A yield diï¬€erent Gaussian
distributions. But this is not the case. In this section we shall show that, while the
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.6 Gaussian Random Vectors
515
choice of A is not unique, all choices that result in AAT being equal to the given
covariance matrix KXX give rise to the same distribution.
We shall derive this result by computing the characteristic function Î¦X(Â·) of a
random n-vector X whose law is equal to the law of AW + Î¼, where W, A, and Î¼
are as above and by then showing that Î¦X(Â·) depends on A only via AAT, i.e.,
that Î¦X(Ï–) can be computed for every Ï– âˆˆRn from Ï–, AAT, and Î¼. Since, by
(23.39), AAT is equal to the covariance matrix KXX of X, it will follow that the
characteristic functions of all Gaussian vectors of a given mean vector and a given
covariance matrix are identical. Since random vectors of identical characteristic
functions must have identical distributions (Proposition 23.4.4), it will follow that
all Gaussian vectors of a given mean vector and a given covariance matrix have
identical distributions.
We thus proceed to compute the characteristic function of a random n-vector X
whose law is the law of AW + Î¼, where W is a standard Gaussian m-vector, A
is n Ã— m, and Î¼ âˆˆRn. By (23.39) it follows that KXX = AAT. To that end we
need to compute E

eiÏ–TX
for every Ï– âˆˆRn. From Proposition 23.6.3 (with the
substitution of the 1 Ã— n matrix Ï–T for C and of the scalar zero for d), it follows
that Ï–TX is a Gaussian vector with only one component. By Proposition 23.6.6,
this sole component is a univariate Gaussian. Its mean is, by (23.25a), Ï–TÎ¼ and
its variance is, by (23.30), Ï–T KXX Ï–. Thus,
Ï–TX âˆ¼N

Ï–TÎ¼, Ï–T KXX Ï–

,
Ï– âˆˆRn.
(23.50)
Using the expression (19.29) for the characteristic function of the univariate Gaus-
sian distribution (with the substitution Ï–TÎ¼ for Î¼, the substitution Ï–T KXX Ï–
for Ïƒ2, and the substitution 1 for Ï–), we obtain that the characteristic func-
tion Î¦X(Â·), which is deï¬ned as E

eiÏ–TX
, is given by
Î¦X(Ï–) = eâˆ’1
2 Ï–T KXX Ï–+iÏ–TÎ¼,
Ï– âˆˆRn.
(23.51)
Since this characteristic function is fully determined by the mean vector and the
covariance matrix of X, it follows that the distribution is also determined by the
mean and covariance. We have thus proved:
Theorem 23.6.7 (The Mean and Covariance of a Gaussian Determine its Law).
Two Gaussian vectors of equal mean vectors and of equal covariance matrices have
identical distributions.
Note 23.6.8. Theorem 23.6.7 and Proposition 23.6.1 combine to prove that for
every Î¼ âˆˆRn and every n Ã— n positive semideï¬nite matrix K there exists one, and
only one, Gaussian distribution of mean Î¼ and covariance matrix K. We denote
this Gaussian distribution by N(Î¼, K).
By (23.51) it follows that if X âˆ¼N(Î¼, K) then
Î¦X(Ï–) = eâˆ’1
2 Ï–TKÏ–+iÏ–TÎ¼,
Ï– âˆˆRn.
(23.52)
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

516
The Multivariate Gaussian Distribution
Theorem 23.6.7 has important consequences, one of which has to do with the
properties of independence and uncorrelatedness. Recall that any two independent
random variables (of ï¬nite mean) are also uncorrelated.
The reverse is not in
general true. But for jointly Gaussians it is: if X and Y are jointly Gaussian, then
X and Y are independent if, and only if, they are uncorrelated. More generally:
Corollary 23.6.9. Let X be a centered Gaussian (n1 + n2)-vector. Let the random
n1-vector X1 = (X(1), . . . , X(n1))T correspond to its ï¬rst n1 components, and let
X2 = (X(n1+1), . . . , X(n1+n2))T correspond to the rest of its components. Then the
vectors X1 and X2 are independent if, and only if, they are uncorrelated, i.e., if,
and only if,
E

X1XT
2

= 0.
(23.53)
Proof. The easy direction, which has nothing to do with Gaussianity, is that if
X1 and X2 are centered and independent, then (23.53) holds.
Indeed, by the
independence and the fact that the vectors are of zero mean we have
E

X1XT
2

= E[X1] E

XT
2

= E[X1] (E[X2])T
= 00T
= 0.
We now prove the reverse using the Gaussianity.
We begin by expressing the
covariance matrix of X in terms of the covariance matrices of X1 and X2 as
KXX =
-
E

X1XT
1

E

X1XT
2

E

X2XT
1

E

X2XT
2

.
=
	
KX1X1
0
0
KX2X2

,
(23.54)
where the second equality follows from (23.53).
Next, let Xâ€²
1 and Xâ€²
2 be independent random vectors such that Xâ€²
1
L= X1 and
Xâ€²
2
L= X2. Let Xâ€² be the (n1 + n2)-vector that results from stacking Xâ€²
1 on top
of Xâ€²
2. Since X is Gaussian, it follows from Corollary 23.6.5 that X1 must also
be Gaussian, and since Xâ€²
1 has the same law as X1, it too is Gaussian. Similarly,
Xâ€²
2 is also Gaussian. And since Xâ€²
1 and Xâ€²
2 are, by construction, independent, it
follows from Proposition 23.6.2 that Xâ€² is a centered Gaussian.
Having established that Xâ€² is Gaussian, we next compute its covariance matrix.
Since, by construction, Xâ€²
1 and Xâ€²
2 are independent and centered,
KXâ€²Xâ€² =
	KXâ€²
1Xâ€²
1
0
0
KXâ€²
2Xâ€²
2

=
	KX1X1
0
0
KX2X2

,
(23.55)
where the second equality follows because the equality in law between Xâ€²
1 and X1
implies that KXâ€²
1Xâ€²
1 = KX1X1 and similarly for Xâ€²
2.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.6 Gaussian Random Vectors
517
Comparing (23.55) and (23.54) we conclude that X and Xâ€² are centered Gaussians
of identical covariance matrices. Consequently, by Theorem 23.6.7, Xâ€²
L= X. And
since the ï¬rst n1 components of Xâ€² are independent of its last n2 components, the
same must also be true for X.
Corollary 23.6.10. If the components of a Gaussian random vector are uncorrelated
and its covariance matrix therefore diagonal, then its components are independent.
Proof. Let X be a Gaussian random vector whose components are uncorrelated
and whose covariance matrix KXX therefore diagonal. Denote the diagonal ele-
ments of KXX by Î»1, . . . , Î»n. Let Î¼ be the mean vector of X. Another Gaussian
vector of this mean and of this covariance matrix is the Gaussian vector whose
components are independent N

Î¼(j), Î»j

. Since the mean and covariance deter-
mine the distribution of Gaussian vectors, it follows that the two vectors, in fact,
have identical laws so the components of X are also independent.
Another consequence of the fact that there is only one multivariate Gaussian distri-
bution of a given mean vector and of a given covariance matrix has to do with pair-
wise independence and independence. Recall that the random variables X1, . . . , Xn
are pairwise independent if for each pair of distinct indices Î½â€², Î½â€²â€² âˆˆ{1, . . . , n}
the random variables XÎ½â€² and XÎ½â€²â€² are independent, i.e., if for all such Î½â€², Î½â€²â€² and
all Î¾Î½â€², Î¾Î½â€²â€² âˆˆR
Pr

XÎ½â€² â‰¤Î¾Î½â€², XÎ½â€²â€² â‰¤Î¾Î½â€²â€²
= Pr

XÎ½â€² â‰¤Î¾Î½â€²
Pr[XÎ½â€²â€² â‰¤Î¾Î½â€²â€²].
(23.56)
The random variables X1, . . . , Xn are independent if for all Î¾1, . . . , Î¾n in R
Pr

Xj â‰¤Î¾j, for all j âˆˆ{1, . . . , n}

=
n
@
j=1
Pr

Xj â‰¤Î¾j

.
(23.57)
Independence implies pairwise independence, but the two are not equivalent. One
can ï¬nd triples of random variables that are pairwise independent but not inde-
pendent.9 But if X1, . . . , Xn are jointly Gaussian, then pairwise independence is
equivalent to independence:
Corollary 23.6.11. If the components of a Gaussian random vector are pairwise
independent, then they are independent.
Proof. If the components of a Gaussian vector are pairwise independent, then they
are uncorrelated and hence, by Corollary 23.6.10, independent.
Corollary 23.6.12. If W is a standard Gaussian n-vector, and if U is an n Ã— n
orthogonal matrix, then UW is also a standard Gaussian vector.
9A classical example is the triple X, Y, Z where X and Y are IID each taking on the values
Â±1 equiprobably and where Z is their product.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

518
The Multivariate Gaussian Distribution
Proof. By Deï¬nition 23.1.1 it follows that the random vector UW is a centered
Gaussian. By (23.39) we obtain that the orthogonality of the matrix U implies
that the covariance matrix of this centered Gaussian is the identity matrix, which
is also the covariance matrix of W; see (23.37). Consequently, UW and W are
two centered Gaussian vectors of identical covariance matrices and hence, by The-
orem 23.6.7, of equal law. Since W is standard, this implies that UW must also
be standard.
The next corollary shows that if X is a centered Gaussian n-vector, then X
L= AW
for a standard Gaussian n-vector W and some square matrix A. That is, if the
law of an n-vector X is equal to the law of ËœA Ëœ
W where ËœA is an n Ã— m matrix and
where Ëœ
W is a standard Gaussian m-vector, then the law of X is also identical to
the law of AW, where A is some nÃ—n matrix and where W is a standard Gaussian
n-vector. Consequently, we could have required in Deï¬nition 23.1.1 that the matrix
A be square without changing the set of distributions that we deï¬ne as Gaussian.
Corollary 23.6.13. If X is a centered Gaussian n-vector, then there exists a de-
terministic square n Ã— n matrix A such that X
L= AW, where W is a standard
Gaussian n-vector.
Proof. Let KXX denote the covariance matrix of X. Being a covariance matrix,
KXX must be positive semideï¬nite (Proposition 23.6.1). Consequently, by Propo-
sition 23.3.5, there exists some n Ã— n matrix S such that
KXX = STS.
(23.58)
Consider now the centered Gaussian STW, where W is a standard Gaussian n-
vector.
By (23.39), the covariance matrix of STW is STS, which by (23.58) is
equal to KXX. Thus X and STW are centered Gaussians of the same covariance,
and so they must be of the same law.
We have thus established that the law
of X is the same as the law of the product of a square matrix (ST) by a standard
Gaussian (W).
23.6.3
A Canonical Representation of a Centered Gaussian
The representation of a centered Gaussian vector as the result of the multiplication
of a deterministic matrix by a standard Gaussian vector is not unique. Indeed,
whenever the n Ã— m matrix A satisï¬es AAT = K it follows that if W is a standard
Gaussian m-vector, then AW âˆ¼N(0, K). (This follows because AW is a random
n-vector of covariance matrix AAT (23.39); it is, by Deï¬nition 23.1.1, a centered
Gaussian; and all centered Gaussians of a given covariance matrix have the same
law.) We saw in Corollary 23.6.13 that A can always be chosen as a square matrix.
Thus, to every K âª°0 there exists a square matrix A such that AW âˆ¼N(0, K). In
this section we shall focus on a particular choice of the matrix A that is useful in
the analysis of Gaussian vectors. In this representation A is a square matrix that
can be written as the product of an orthogonal matrix by a diagonal matrix. The
diagonal matrix acts on W by stretching and shrinking its components, and the
orthogonal matrix then rotates (and possibly reï¬‚ects) the result.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.6 Gaussian Random Vectors
519
Theorem 23.6.14 (A Canonical Representation of a Gaussian Vector). Let X be
a centered Gaussian n-vector of covariance matrix KXX. Then
X
L= UÎ›1/2W,
where W is a standard Gaussian n-vector; the n Ã— n matrix U is orthogonal; the
n Ã— n matrix Î› is diagonal; the diagonal elements of Î› are the eigenvalues of KXX;
and the j-th column of U is an eigenvector corresponding to the eigenvalue of KXX
that is equal to the j-th diagonal element of Î›.
Proof. By Proposition 23.6.1, KXX is positive semideï¬nite and a fortiori sym-
metric.
Consequently, by Proposition 23.3.5, there exists a diagonal matrix Î›
whose diagonal elements are the eigenvalues of KXX and there exists an orthogo-
nal matrix U such that KXX U = UÎ›, so the j-th column of U is an eigenvector
corresponding to the eigenvalue given by the j-th diagonal element of Î›. Since
KXX âª°0, it follows that all its eigenvalues are nonnegative, and we can deï¬ne the
matrix Î›1/2 as the matrix whose components are the componentwise nonnegative
square roots of the matrix Î›. As in (23.21), choose S = Î›1/2UT. We then have
that KXX = STS. If W is a standard Gaussian, then STW is a centered Gaussian
of zero mean and covariance STS. Since STS = KXX and since there is only one
centered multivariate Gaussian distribution of a given covariance matrix, it follows
that the law of STW ( = UÎ›1/2W) is the same as the law of X.
Corollary 23.6.15. A centered Gaussian vector can be expressed as the result of
an orthogonal transformation applied to a random vector whose components are
independent centered univariate Gaussians of diï¬€erent variances. These variances
are the eigenvalues of the covariance matrix.
Proof. Because the matrix Î› in the theorem is diagonal, we can write Î›1/2 W as
Î›1/2 W =
â›
âœ
â
âˆšÎ»1 W (1)
...
âˆšÎ»n W (n)
â
âŸ
â ,
where Î»1, . . . , Î»n are the diagonal elements of Î›, i.e., the eigenvalues of KXX. Thus,
the random vector Î›1/2 W has independent components with the Î½-th component
being N(0, Î»Î½).
Figures 23.1 and 23.2 demonstrate this canonical representation. They depict the
contour lines and mesh plots of the density functions of the following four two-
dimensional Gaussian vectors:
X1 =
	
1
0
0
1

W,
KX1X1 = I2,
X2 =
	2
0
0
1

W,
KX2X2 =
	4
0
0
1

,
X3 =
	1
0
0
2

W,
KX3X3 =
	1
0
0
4

,
X4 =
1
âˆš
2
	
1
1
âˆ’1
1

 	
1
0
0
2

W,
KX4X4 = 1
2
	
5
3
3
5

,
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

520
The Multivariate Gaussian Distribution
1
2
3
4
5
6
7
1
2
3
4
5
6
7
1
2
3
4
5
6
7
1
3
5
7
9
11
13
15
âˆ’2
0
2
âˆ’2
0
2
âˆ’2
0
2
âˆ’2
0
2
âˆ’2
0
2
âˆ’2
0
2
âˆ’2
0
2
âˆ’2
0
2
x(1)
3
x(2)
3
x(1)
4
x(2)
4
x(1)
1
x(2)
1
x(1)
2
x(2)
2
Figure 23.1: Contour plot of the density of four diï¬€erent two dimensional Gaussian
random variables: from left to right and top to bottom X1, . . . , X4.
where W is a standard Gaussian vector with two components.
Theorem 23.6.14 can be used to ï¬nd a linear transformation that transforms a
given Gaussian vector to a standard Gaussian. The following is the multivariate
version of the univariate result showing that if X âˆ¼N

Î¼, Ïƒ2
, where Ïƒ2 > 0, then
(X âˆ’Î¼)/Ïƒ has a N(0, 1) distribution (19.8).
Proposition 23.6.16 (From Gaussians to Standard Gaussians). Let the random
n-vector X be N(Î¼, K), where K â‰»0 and Î¼ âˆˆRn. Let the n Ã— n matrices Î› and U
be such that Î› is diagonal, U is orthogonal, and KU = UÎ›. Then
Î›âˆ’1/2UT(X âˆ’Î¼) âˆ¼N(0, In) ,
where Î›âˆ’1/2 is the diagonal matrix whose diagonal entries are the reciprocals of the
square roots of the diagonal elements of Î›.
Proof. Since an aï¬ƒne transformation of a Gaussian vector is Gaussian (Proposi-
tion 23.6.3), it follows that Î›âˆ’1/2UT(X âˆ’Î¼) is a Gaussian vector. And since the
mean and covariance of a Gaussian vector fully specify its law (Theorem 23.6.7),
the result will follow once we show that the mean of Î›âˆ’1/2UT(X âˆ’Î¼) is the zero
vector and its covariance matrix is the identity matrix. This can be readily veriï¬ed
using (23.27).
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.6 Gaussian Random Vectors
521
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
x(1)
3
x(2)
3
x(1)
1
x(2)
1
x(1)
4
x(2)
4
x(1)
2
x(2)
2
Figure 23.2: Mesh plots of the density functions of Gaussian random vectors: from
left to right and top to down X1, . . . , X4.
23.6.4
The Density of a Gaussian Vector
As we saw in Corollary 23.4.2, if the covariance matrix of a centered vector is
singular, then at least one of its components can be expressed as a deterministic
linear combination of its other components. Consequently, random vectors with
singular covariance matrices cannot have a density. If the covariance matrix is
nonsingular, then the vector may or may not have a density. If it is Gaussian, then
it does. In this section we shall derive the density of the multivariate Gaussian
distribution when the covariance matrix is nonsingular, i.e., when it is positive
deï¬nite
We begin with the centered case. To derive the density of a centered Gaussian
n-vector of positive deï¬nite covariance matrix K we shall use Theorem 23.6.14 to
represent the N(0, K) distribution as the distribution of UÎ›1/2W where U is an
orthogonal matrix and Î› is a diagonal matrix satisfying KU = UÎ›. Note that Î›
is nonsingular because its diagonal elements are the eigenvalues of K, which we
assume to be positive deï¬nite.
Let
B = UÎ›1/2,
(23.59)
so the density we are after is the density of BW. Note that, by (23.59),
BBT = UÎ›1/2Î›1/2UT
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
âˆ’5
0
5
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
0.05
0.1
0.15
x(1)
3
x(2)
3
x(1)
1
x(2)
1
x(1)
4
x(2)
4
x(1)
2
x(2)
2
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

522
The Multivariate Gaussian Distribution
= UÎ›UT
= K.
(23.60)
Also, by (23.60),
|det(B)| =

det(B) det(B)
=
3
det(B) det(BT)
=
3
det(BBT)
=

det(K),
(23.61)
where the ï¬rst equality follows by expressing |x| as
âˆš
x2; the second follows because
a square matrix and its transpose have the same determinant; the third because the
determinant of the product of square matrices is the product of the determinants;
and where the last equality follows from (23.60).
Using the formula for computing the density of BW from that of W (Theo-
rem 17.3.4), we have that if X
L= BW, then
fX(x) = fW

Bâˆ’1x

|det(B)|
=
exp

âˆ’1
2

Bâˆ’1x
T
Bâˆ’1x

(2Ï€)n/2|det(B)|
=
exp

âˆ’1
2xT
Bâˆ’1T
Bâˆ’1x

(2Ï€)n/2|det(B)|
=
exp

âˆ’1
2xT
BBTâˆ’1x

(2Ï€)n/2|det(B)|
= exp

âˆ’1
2xTKâˆ’1x

(2Ï€)n/2|det(B)|
= exp

âˆ’1
2xTKâˆ’1x

(2Ï€)n/2
det(K)
,
where the second equality follows from the density of the standard Gaussian (23.36);
the third from the rule for the transpose of the product of matrices (23.3); the fourth
from the representation of the inverse of the product of matrices as the product
of the inverses in reverse order (AB)âˆ’1 = Bâˆ’1Aâˆ’1 and because transposition and
inversion commute; the ï¬fth from (23.60); and the sixth from (23.61). It follows
that if X âˆ¼N(0, K) where K is nonsingular, then
fX(x) = exp

âˆ’1
2xTKâˆ’1x


(2Ï€)ndet(K)
,
x âˆˆRn.
Accounting for the mean, we have that if X âˆ¼N(Î¼, K) where K is nonsingular,
then
fX(x) = exp

âˆ’1
2(x âˆ’Î¼)TKâˆ’1(x âˆ’Î¼)


(2Ï€)ndet(K)
,
x âˆˆRn.
(23.62)
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.7 Jointly Gaussian Vectors
523
23.6.5
Linear Functionals of Gaussian Vectors
A linear functional on Rn is a linear mapping from Rn to R. For example, if Î±
is any ï¬xed vector in Rn, then the mapping
x 	â†’Î±Tx
(23.63)
is a linear functional on Rn. In fact, as we next show, every linear functional on Rn
has this form. This can be proved by using linearity to verify that we can choose
the j-th component of Î± to equal the result of applying the linear functional to
the vector ej whose components are all zero except for its j-th component which
is equal to one.
If X is a Gaussian n-vector and if Î± âˆˆRn, then, by Proposition 23.6.3 (applied
with the substitution of the 1Ã—n matrix Î±T for C), it follows that Î±TX is a Gaus-
sian vector with only one component. By Proposition 23.6.6, this sole component
must have a univariate Gaussian distribution. We thus conclude that the result of
applying a linear functional to a Gaussian vector is a Gaussian random variable.
We next show that the reverse is also true: if X is of mean Î¼ and of covariance
matrix K and if the result of applying every linear functional to X has a univari-
ate Gaussian distribution, then X âˆ¼N(Î¼, K).10 To prove this result we compute
the characteristic function of X. For every Ï– âˆˆRn the mapping x 	â†’Ï–Tx is
a linear functional on Rn. Consequently, our assumption that the result of the
application of every linear functional to X has a univariate Gaussian distribution
implies (23.50). From here we can follow the steps leading to (23.52) to conclude
that the characteristic function of X must be given by the RHS of (23.52). Since
this is also the characteristic function of a N(Î¼, K) random vector, it follows that
X âˆ¼N(Î¼, K), because random vectors of identical characteristic functions must
have identical distributions (Proposition 23.4.4). We have thus proved:
Theorem 23.6.17 (Gaussian Vectors and Linear Functionals). A random vector X
is Gaussian if, and only if, every linear functional of X has a univariate Gaussian
distribution.
23.7
Jointly Gaussian Vectors
Three miracles occur when we compute the conditional distribution of X given
Y = y for jointly Gaussian random vectors X and Y. Before describing these
miracles we need to deï¬ne jointly Gaussian vectors.
Deï¬nition 23.7.1 (Jointly Gaussian Vectors). Two random vectors are said to be
jointly Gaussian if the vector that results when one is stacked on top of the other
is Gaussian.
10It is not diï¬ƒcult to show that the assumption that X is of ï¬nite variance is not necessary.
If every linear functional of X is of ï¬nite variance, then X must be of ï¬nite variance. Thus, we
could have stated the result as follows: if a random vector is such that the result of applying
every linear functional to it is a univariate Gaussian, then it is a multivariate Gaussian.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

524
The Multivariate Gaussian Distribution
That is, the random nx-vector X = (X(1), . . . , X(nx))T and the random ny-vector
Y = (Y (1), . . . , Y (ny))T are jointly Gaussian if the random (nx + ny)-vector

X(1), . . . , X(nx), Y (1), . . . , Y (ny)T
is Gaussian.
By Corollary 23.6.5, the random vectors X and Y can only be jointly Gaussian if
each is Gaussian. But this is not enough: both X and Y can be Gaussian without
them being jointly Gaussian.
However, if X and Y are independent Gaussian
vectors, then, by Proposition 23.6.2, they are jointly Gaussian.
Proposition 23.7.2. Independent Gaussian vectors are jointly Gaussian.
By Corollary 23.6.9 we have:
Proposition 23.7.3. If two jointly Gaussian random vectors are uncorrelated, then
they are independent.
Having deï¬ned jointly Gaussian random vectors we next turn to the main result of
this section. Loosely speaking, it states that if X and Y are jointly Gaussian, then
in computing the conditional distribution of X given Y = y three miracles occur:
(i) the conditional distribution is a multivariate Gaussian;
(ii) its mean vector is an aï¬ƒne function of y;
(iii) and its covariance matrix does not depend on y.
Before stating this more formally, we justify two simplifying assumptions. The ï¬rst
assumption is that the covariance matrix of Y is nonsingular, so
KYY â‰»0.
The reason is that if the covariance matrix of Y is singular, then, by Corol-
lary 23.4.2, some of its components are with probability one aï¬ƒne functions of
the others, and we then have to consider two cases. If the realization y satisï¬es
these aï¬ƒne relations, then we can just pick a subset of the components of Y that
determine all the other components and that have a nonsingular covariance matrix
as in Section 23.4.3 and ignore the other components of y; the ignored components
do not alter the conditional distribution of X given Y = y. The other case where
the realization y does not satisfy the relations that Y satisï¬es with probability one
can be ignored because it occurs with probability zero.
The second assumption we make is that both X and Y are centered. There is no
loss in generality in making this assumption for the following reason. Conditioning
on Y = y when Y has mean Î¼y is equivalent to conditioning on Y âˆ’Î¼y = y âˆ’Î¼y.
And if X has mean Î¼x, then we can compute the conditional distribution of X by
computing the conditional distribution of Xâˆ’Î¼x and by then shifting the resulting
distribution by Î¼x. Thus, the conditional density fX|Y=y(Â·) is given by
fX|Y=y(x) = fXâˆ’Î¼x|Yâˆ’Î¼y=yâˆ’Î¼y(x âˆ’Î¼x),
(23.64)
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.7 Jointly Gaussian Vectors
525
where X âˆ’Î¼x & Y âˆ’Î¼y are jointly Gaussian and centered whenever X & Y are
jointly Gaussian. It is now straightforward to verify that if the miracles hold for
the centered case
x 	â†’fXâˆ’Î¼x|Yâˆ’Î¼y=yâˆ’Î¼y(x)
then they also hold for the general case
x 	â†’fXâˆ’Î¼x|Yâˆ’Î¼y=yâˆ’Î¼y(x âˆ’Î¼x).
Theorem 23.7.4. Let X and Y be centered and jointly Gaussian with covariance
matrices KXX and KYY. Assume that KYY â‰»0. Then the conditional distribution
of X conditional on Y = y is a multivariate Gaussian of mean
E

XYT
Kâˆ’1
YY y
(23.65)
and covariance matrix
KXX âˆ’E

XYT
Kâˆ’1
YY E

YXT
.
(23.66)
Moreover, the deviation of X from its conditional mean is independent of Y:
X âˆ’E

XYT
Kâˆ’1
YY Y is independent of Y.
(23.67)
Proof. Let nx and ny denote the number of components of X and Y. Let D be
any deterministic real nx Ã— ny matrix. Then clearly
X = DY +

X âˆ’DY

.
(23.68)
Since X and Y are jointly Gaussian, the vector

XT, YTT is Gaussian. Conse-
quently, since
	
X âˆ’DY
Y

=
	
Inx
âˆ’D
0
Iny

 	
X
Y

,
it follows from Proposition 23.6.3 that
(X âˆ’DY) and Y are centered and jointly Gaussian.
(23.69)
Suppose now that the matrix D is chosen so that (Xâˆ’DY) and Y are uncorrelated:
E

(X âˆ’DY)YT
= 0.
(23.70)
By (23.69) and Proposition 23.7.3 it then follows that
(X âˆ’DY) is independent of Y.
(23.71)
Consequently, with this choice of D, the decomposition (23.68) expresses X as
the sum of two terms where the ï¬rst, DY, is fully determined by Y and where the
second, (Xâˆ’DY), is independent of Y. It follows that the conditional distribution
of X given Y = y is the same as the distribution of (Xâˆ’DY) but shifted by Dy. By
(23.69) and Corollary 23.6.5, (X âˆ’DY) is a centered Gaussian, so the conditional
distribution of X given Y = y is that of the centered Gaussian (Xâˆ’DY) shifted by
the vector Dy. This already establishes the three â€œmiraclesâ€ we discussed before:
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

526
The Multivariate Gaussian Distribution
the conditional distribution of X given Y = y is Gaussian; its mean Dy is a
linear function of Y; and its covariance matrix, which is the covariance matrix of
(X âˆ’DY), does not depend on the realization y of Y.
That the mean of the conditional distribution is as given in (23.65) and that the co-
variance matrix is as given in (23.66) now follow from straightforward calculations.
Indeed, by solving (23.70) for D we obtain
D = E

XYT
Kâˆ’1
YY,
(23.72)
so Dy is given by (23.65). To show that the covariance of the conditional law of X
given Y = y is as given in (23.66), we note that this covariance is the covariance
of (X âˆ’DY), which is given by
E

(X âˆ’DY)(X âˆ’DY)T
= E

(X âˆ’DY)XT
âˆ’E

(X âˆ’DY)(DY)T
= E

(X âˆ’DY)XT
âˆ’E

(X âˆ’DY)YT
DT
= E

(X âˆ’DY)XT
= KXX âˆ’D E

YXT
= KXX âˆ’E

XYT
Kâˆ’1
YY E

YXT
,
where the ï¬rst equality follows by opening the second set of parentheses; the second
by (23.3) and (23.25b); the third by (23.70); the fourth by opening the parentheses
and using the linearity of the expectation; and the ï¬nal equality by (23.72).
Finally, (23.67) follows from (23.71) and the explicit form of D (23.72).
Theorem 23.7.4 has important consequences in Estimation Theory. A key result
in Estimation Theory is that if after observing that Y = y for some y âˆˆRny we
would like to estimate the random nx-vector X using a (Borel measurable) function
g: Rny â†’Rnx so as to minimize the estimation error
E

âˆ¥X âˆ’g(Y)âˆ¥2
,
(23.73)
then an optimal choice for g(Â·) is the conditional expectation
g(y) = E

X
 Y = y

,
y âˆˆRny.
(23.74)
Theorem 23.7.4 demonstrates that if X and Y are jointly Gaussian and centered,
then E[X|Y = y] is a linear function of y and is explicitly given by (23.65). Thus,
for jointly Gaussian centered random vectors, there is no loss of optimality in
limiting ourselves to linear estimators.
The optimality of choosing g(Â·) as in (23.74) has a simple intuitive explanation. We
ï¬rst note that it suï¬ƒces to establish the result when nx = 1, i.e., when estimating
a random variable rather than a random vector. Indeed, the squared-norm error in
estimating a random vector X with nx components is the sum of the squared errors
in estimating its components. To minimize the sum, one should therefore minimize
each of the terms. And the problem of estimating the j-th component of X based
on the observation Y = y is a problem of estimating a random variable. Stated
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.8 Moments and Wickâ€™s Formula
527
diï¬€erently, to estimate X so as to minimize the error (23.73) we should separately
estimate each of its components.
Having established that it suï¬ƒces to prove the optimality of (23.74) when nx = 1,
we now assume that nx = 1 and denote the random variable to be estimated by X.
To study how to estimate X after observing that Y = y, we ï¬rst consider the
case where there is no observation. In this case, the estimate is a constant, and
by Lemma 14.4.1 the optimal choice of that constant is the mean E[X]. We now
view the general case where we observe Y = y as though there were no observables
but X had the a posteriori distribution given Y = y. Utilizing the result for the
case where there are no observables yields that estimating X by E[X |Y = y] is
optimal.
23.8
Moments and Wickâ€™s Formula
We next describe without proof a technique for computing moments of centered
Gaussian vectors. A sketch of a proof can be found in (Zvonkin, 1997).
Theorem 23.8.1 (Wickâ€™s Formula). Let X be a centered Gaussian n-vector and
let g1, . . . , g2k : Rn â†’R be an even number of (not necessarily diï¬€erent) linear
functionals on Rn. Then
E

g1(X) g2(X) Â· Â· Â· g2k(X)

=

E

gp1(X) gq1(X)

E

gp2(X) gq2(X)

Â· Â· Â· E

gpk(X) gqk(X)

,
(23.75)
where the summation is over all permutations p1, q1, p2, q2, . . . , pk, qk of 1, 2, . . . , 2k
such that
p1 < p2 < Â· Â· Â· < pk
(23.76a)
and
p1 < q1,
p2 < q2,
. . . ,
pk < qk.
(23.76b)
The number of terms on the RHS of (23.75) is 1 Ã— 3 Ã— 5 Ã— Â· Â· Â· Ã— (2k âˆ’1).
Example 23.8.2. Suppose that n = 1, so X is a centered univariate Gaussian.
Let Ïƒ2 be its variance, and suppose we wish to compute E

X4
. We can express this
in the form of Theorem 23.8.1 with k = 2 and g1(x) = g2(x) = g3(x) = g4(x) = x.
By Wickâ€™s Formula
E

X4
= E

g1(X) g2(X)

E

g3(X) g4(X)

+ E

g1(X) g3(X)

E

g2(X) g4(X)

+ E

g1(X) g4(X)

E

g2(X) g3(X)

= 3Ïƒ4,
which is in agreement with (19.31).
Example 23.8.3. Suppose that X is a bivariate centered Gaussian whose com-
ponents are of unit variance and of correlation coeï¬ƒcient Ï âˆˆ[âˆ’1, 1]. We com-
pute E

(X(1))2(X(2))2
using Theorem 23.8.1 by setting k = 2 and by deï¬ning
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

528
The Multivariate Gaussian Distribution
g1(x) = g2(x) = x(1) and g3(x) = g4(x) = x(2). By Wickâ€™s Formula
E
%
X(1)2
X(2)2&
= E

g1(X) g2(X)

E[g3(X) g4(X)] + E

g1(X) g3(X)

E

g2(X) g4(X)

+ E

g1(X) g4(X)

E

g2(X) g3(X)

= E
%
X(1)2&
E
%
X(2)2&
+ E
%
X(1)X(2)&
E
%
X(1)X(2)&
+ E
%
X(1)X(2)&
E
%
X(1)X(2)&
= 1 + 2Ï2.
(23.77)
Similarly,
E
%
X(1)3X(2)&
= 3Ï.
(23.78)
23.9
The Limit of Gaussian Vectors Is a Gaussian Vector
The results of Section 19.9 on limits of Gaussian random variables extend to Gaus-
sian vectors.
In this setting we consider random vectors X, X1, X2, . . . deï¬ned
over the probability space (Î©, F, P). We say that the sequence of random vectors
X1, X2, . . . converges to the random vector X with probability one or almost
surely if
Pr
'
Ï‰ âˆˆÎ© : lim
nâ†’âˆXn(Ï‰) = X(Ï‰)
(
= 1.
(23.79)
The sequence X1, X2, . . . converges to the random vector X in probability if
lim
nâ†’âˆPr

âˆ¥Xn âˆ’Xâˆ¥â‰¥Ïµ

= 0,
Ïµ > 0.
(23.80)
The sequence X1, X2, . . . converges to the random vector X in mean square if
lim
nâ†’âˆE
%
âˆ¥Xn âˆ’Xâˆ¥2&
= 0.
(23.81)
Finally, the sequence of random vectors X1, X2, . . . taking values in Rd converges
to the random vector X weakly or in distribution if
lim
nâ†’âˆPr

X(1)
n
â‰¤Î¾(1), . . . , X(d)
n
â‰¤Î¾(d)
= Pr

X(1) â‰¤Î¾(1), . . . , X(d) â‰¤Î¾(d)
(23.82)
for every vector Î¾ âˆˆRd such that
lim
Ïµâ†“0 Pr

X(1) â‰¤Î¾(1) âˆ’Ïµ, . . . , X(d) â‰¤Î¾(d) âˆ’Ïµ

= Pr

X(1) â‰¤Î¾(1), . . . , X(d) â‰¤Î¾(d)
.
(23.83)
In analogy to Theorem 19.9.1 we next show that, irrespective of which of the above
forms of convergence we consider, if a sequence of Gaussian vectors converges to
some random vector X, then X must be Gaussian.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.10 Conditionally-Independent Gaussian Vectors
529
Theorem 23.9.1. Let the random d-vectors X, X1, X2, . . . be deï¬ned over a com-
mon probability space. Let X1, X2, . . . each be Gaussian (with possibly diï¬€erent
mean vectors and covariance matrices). If the sequence X1, X2, . . . converges to X
in the sense of (23.79) or (23.80) or (23.81), then X must be Gaussian.
Proof. The proof is based on Theorem 23.6.17, which demonstrates that it suï¬ƒces
to consider linear functionals of the vectors in the sequence and on the analogous
result for scalars (Theorem 19.9.1). We demonstrate the idea by considering the
case where the convergence is almost sure. If X1, X2, . . . converges almost surely
to X, then for every Î± âˆˆRd the sequence Î±TX1, Î±TX2, . . . converges almost surely
to Î±TX. Since, by Theorem 23.6.17, linear functionals of Gaussian vectors are
univariate Gaussians, it follows that the sequence Î±TX1, Î±TX2, . . . is a sequence of
Gaussian random variables. And since it converges almost surely to Î±TX, it follows
from Theorem 19.9.1 that Î±TX must be Gaussian. Since this is true for every Î±
in Rd, it follows from Theorem 23.6.17 that X must be a Gaussian vector.
In analogy to Theorem 19.9.3 we have the following result on weakly converging
Gaussian vectors.
Theorem 23.9.2 (Weakly Converging Gaussian Vectors). Let the sequence of
random d-vectors X1, X2, . . . be such that Xn âˆ¼N(Î¼n, Kn) for n = 1, 2, . . . Then
the sequence converges in distribution to some limiting distribution, if, and only if,
there exist some Î¼ âˆˆRd and some d Ã— d matrix K such that
Î¼n â†’Î¼ and Kn â†’K.
(23.84)
And if the sequence does converge in distribution, then it converges to the multi-
variate Gaussian distribution of mean vector Î¼ and covariance matrix K.
Proof. See (Gikhman and Skorokhod, 1996, Chapter I, Section 3, Theorem 4).
23.10
Conditionally-Independent Gaussian Vectors
Given the covariance matrix of a Gaussian vector X, it is easy to check whether two
of its components X(j) and X(â„“) are independent: they are independent if, and only
if, the Row-j Column-â„“component of the covariance matrix is zero. More generally,
the Î·1-vector (X(j1), . . . , X(jÎ·1)) is independent of the Î·2-vector (X(â„“1), . . . , X(â„“Î·2))
if, and only if, for every 1 â‰¤Î½â€² â‰¤Î·1 and 1 â‰¤Î½â€²â€² â‰¤Î·2 the Row-Î½â€² Column-Î½â€²â€²
component of the covariance matrix is zero (Proposition 23.7.3).
Is there an easy check for conditional independence? It turns out that there is,
though not in terms of the covariance matrix but in terms of its inverseâ€”the
precision matrix. Such a check is described in the next theorem, which is the
sectionâ€™s main result.
Theorem 23.10.1 (Conditional Independence and the Precision Matrix). Let X
be a Gaussian n-vector of components X(1), . . . , X(n) and nonsingular covariance
matrix K. Let J , K, and L be disjoint sets whose union is the set {1, . . . , n}. Then
the following statements are equivalent.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

530
The Multivariate Gaussian Distribution
1
2
3
4
5
â›
âœ
âœ
âœ
âœ
â
Â·
Â·
Â·
Â·
0
Â·
Â·
0
Â·
Â·
Â·
0
Â·
Â·
0
Â·
Â·
Â·
Â·
Â·
0
Â·
0
Â·
Â·
â
âŸ
âŸ
âŸ
âŸ
â 
Figure 23.3: The zeros of a precision matrix and the graph associated with it.
(a) The collections of random variables {X(j), j âˆˆJ } and {X(k), k âˆˆK} are
conditionally independent given {X(â„“), â„“âˆˆL}.
(b) For every j âˆˆJ and k âˆˆK the Row-j Column-k entry of Kâˆ’1 is zero.
This theorem shows that the conditional independencies among the components
of a Gaussian vector are determined by the locations of the zeros in the precision
matrix. These locations can be described using a graph that we associate with
the precision matrix and from which the conditional independencies are easily read
oï¬€. The graph is an undirected graph whose nodes are the elements of the set
{1, . . . , n} and where an edge connects Node j with Node k whenever j Ì¸= k and
the Row-j Column-k entry of Kâˆ’1 is nonzero. See Figure 23.3 for an example of a
precision matrix and the graph associated with it. Statement (b) in the theorem is
equivalent to the condition that removing the nodes in L and all the edges touching
them results in a graph with no path connecting a node in J with a node in K. For
example, suppose that n = 5 and the zeros in Kâˆ’1 and the associated graph are as
in Figure 23.3. In this case, X(5) and (X(1), X(3)) are conditionally independent
given (X(2), X(4)). Likewise, (X(2), X(5)) and X(3) are conditionally independent
given (X(1), X(4)).
Before proving the theorem, we ï¬rst argue that it suï¬ƒces to prove it for the special
case where the elements of J , K, and L are consecutive:
J = {1, . . . , # J }, K = {# J + 1, . . . , # J + # K}, L = {n âˆ’# L + 1, . . . , n}.
To see why, let Ï€(Â·) be a permutation on {1, . . . , n} that maps {1, . . . , # J } to J ;
that maps {# J + 1, . . . , # J + # K} to K; and that maps {n âˆ’# L + 1, . . . , n}
to L. Let ËœX be the random vector that results from permuting the components
of X
ËœX(i) = X(Ï€(i)),
i = 1, . . . , n.
(23.85)
Its covariance matrix ËœK is given (using the notation of Section 23.2) by
ËœK

j,k =

K

Ï€(j),Ï€(k),
j, k âˆˆ{1, . . . , n}.
(23.86)
And its precision matrix ËœKâˆ’1 is given by11
ËœKâˆ’1
j,k =

Kâˆ’1
Ï€(j),Ï€(k),
j, k âˆˆ{1, . . . , n}.
(23.87)
11This is easily seen by deï¬ning the permutation matrix P whose Row-i Column-j entry is
I{j = Ï€(i)}; by noting that ËœX = PX; by using (23.27) to conclude that ËœK = PKPT; and by using
the inversion formula for a product of matrices and the fact that P is orthogonal.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.10 Conditionally-Independent Gaussian Vectors
531
With these deï¬nition, Statement (a) in the theorem is equivalent to the statement
that the ï¬rst # J components of ËœX and the next # K components are conditionally
independent given the last # L components, and Statement (b) in the theorem is
equivalent to the statement that the Row-j Column-k entry of ËœKâˆ’1 is zero whenever
j âˆˆ{1, . . . , # J } and k âˆˆ{# J + 1, . . . , n âˆ’# L}. Proving the equivalence of the
statements about ËœX thus also proves the equivalence of the statements about X.
Before proceeding to prove the above special case, we formulate it as a proposition
using slightly diï¬€erent language.
Proposition 23.10.2. Let the centered random vectors X âˆˆRnx, Y âˆˆRny, and
Z âˆˆRnz be jointly Gaussian, and let the vector (XT, YT, ZT)T that results when
they are stacked on top of each other have a nonsingular covariance matrix K.
Represent its inverse using submatrices as
Kâˆ’1 =
â›
â
A
B
C
BT
D
E
CT
ET
F
â
â ,
(23.88)
where A âˆˆRnxÃ—nx, B âˆˆRnxÃ—ny, C âˆˆRnxÃ—nz, D âˆˆRnyÃ—ny, E âˆˆRnyÃ—nz, F âˆˆ
RnzÃ—nz, and where A, D, and F are symmetric.12 Then X and Y are conditionally
independent given Z if, and only if, B is zero.
The signiï¬cance of B being zero can be understood in the following way. Recalling
the density of a centered Gaussian (Section 23.6.4),
fX,Y,Z(x, y, z) = Î± exp

âˆ’1
2

xT
yT
zT
Kâˆ’1 
xT
yT
zTT 
,
(23.89)
where
Î± = (2Ï€)âˆ’
nx+ny+nz
2

det(Kâˆ’1).
Using this and the representation (23.88), we obtain
ln fX,Y,Z(x, y, z) = ln Î± âˆ’1
2

xTAx + 2xTBy + 2xTCz + yTDy + 2yTEz + zTFz

.
(23.90)
We thus see that the logarithm of the Gaussian density function is always the sum
of a constant (ln Î±) and a quadratic form in x, y, and z. The matrix B in the
representation (23.88) is zero if, and only if, the quadratic form associated with
Kâˆ’1 does not have any cross terms involving x and y.
Before we can prove Proposition 23.10.2 and thus conclude the proof of Theo-
rem 23.10.1, we need a factorization result on the joint probability density function
of conditionally independent random variables.
Suppose the random vectors X, Y, and Z have a joint probability density function.
Then X and Y are conditionally independent given Z if, and only if, X, Y, and Z
have a joint probability density function fX,Z,Y satisfying
fX,Y,Z(x, y, z) = fZ(z) fX|Z(x|z) fY|Z(y|z),
fZ(z) > 0.
(23.91)
12These matrices are symmetric because K is symmetric and hence so is its inverse.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

532
The Multivariate Gaussian Distribution
Here fZ denotes the density of Z, and fX|Z and fY|Z denote conditional densities
(cf. Deï¬nition 20.11.2).
Clearly, if fX,Y,Z has the above form, then it can be written as the product of a
function of (x, z) and a function of (y, z). That is, fX,Y,Z factorizes as
fX,Y,Z(x, y, z) = g1(x, z) Â· g2(y, z).
(23.92)
Indeed, g1(x, z) can be chosen as fX,Z(x, z) and g2(y, z) as fY|Z(y|z). The next
proposition shows that the reverse is also true: a factorization of the form (23.92)
implies that X and Y are conditionally independent given Z.
Proposition 23.10.3 (Factorization and Conditional Independence). If X, Y, Z
are random vectors of joint density fX,Y,Z having the form (23.92), were g1(Â·, Â·)
and g2(Â·, Â·) are Borel measurable, then X and Y are conditionally independent
given Z.
Proof. Assume that fX,Y,Z has the form (23.92). Taking the absolute value of
both sides of (23.92), we conclude that if a factorization of the form (23.92) exists,
then it also exists with g1(Â·, Â·) and g2(Â·, Â·) nonnegative. Assume then that g1(Â·, Â·)
and g2(Â·, Â·) are nonnegative. (This will allow us later to use Fubiniâ€™s theorem to
change the orders of integration.)
The density of Z is obtained from the joint density by integrating over x and y
fZ(z) =

fX,Y,Z,(x, y, z) dx dy
=

g1(x, z) g2(y, z) dx dy,
(23.93)
and, for each z with fZ(z) > 0, the conditional density of X and Y given Z is
fX,Y|Z(x, y|z) = fX,Z,Y(x, z, y)
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
.
(23.94)
We conclude the proof by showing that the RHS of (23.94) is fX|Z(x|z) fY|Z(y|z).
For z with fZ(z) > 0,
fX|Z(x|z) fY|Z(y|z) = fX,Z(x, z)
fZ(z)
Â· fY,Z(y, z)
fZ(z)
=

g1(x, z) g2(yâ€², z) dyâ€²
fZ(z)
Â·

g1(xâ€², z) g2(y, z) dxâ€²
fZ(z)
= g1(x, z)

g2(yâ€², z) dyâ€²
fZ(z)
Â· g2(y, z)

g1(xâ€², z) dxâ€²
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
Â·

g2(yâ€², z) dyâ€² 
g1(xâ€², z) dxâ€²
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
Â·

g1(xâ€², z) g2(yâ€², z) dxâ€² dyâ€²
fZ(z)
= g1(x, z) g2(y, z)
fZ(z)
,
(23.95)
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.11 Additional Reading
533
where the last equality follows from (23.93). From (23.94) and (23.95) we conclude
that fX,Y,Z has the form (23.91).
We are now ready to prove Proposition 23.10.2 and to thus conclude the proof of
Theorem 23.10.1.
Proof of Proposition 23.10.2. We begin by proving that if B is zero, then X and
Y are conditionally independent given Z. Substituting 0 for B in (23.90) we obtain
ln fX,Y,Z(x, y, z) = ln Î± âˆ’1
2

xTAx + 2xTCz + yTDy + 2yTEz + zTFz

,
and hence the factorization
fX,Y,Z(x, y, z) = Î± exp

âˆ’1
2(xTAx + 2xTCz + zTFz)

exp

âˆ’1
2(yTDy + 2yTEz)

,
which implies the conditional independence of X and Y given Z (by Proposi-
tion 23.10.3).
To conclude the proof, we now assume that X and Y are conditionally independent
and prove that B must be zero. From the conditional independence and (23.91),
log fX,Y,Z(x, y, z) = log fX,Z(x, z) + log fY|Z(y|z).
(23.96)
The ï¬rst term on RHS, log fX,Z(x, z), is the logarithm of the joint density function
of X, Z and, as such, is a constant plus a quadratic form in (x, z). The second
term, log fY|Z(y|z) is the logarithm of the conditional density of Y given Z, which
is a noncentered Gaussian whose parameters are given in Theorem 23.7.4. Using its
explicit density, we conclude that log fY|Z(y|z) is a constant plus a quadratic form
in (y, z). Thus, log fX,Y,Z is the sum of a constant, a quadratic form in (x, z) and
a quadratic form in (y, z): it contains no cross terms involving x and y. Hence, B
must be zero.13
23.11
Additional Reading
There are numerous books on Matrix Theory that discuss orthogonal matrices and
positive semideï¬nite matrices. We mention here (Zhang, 2011, Section 6.2), (Her-
stein, 1975, Chapter 6, Section 6.10), and (Axler, 2015, Chapter 7) on orthogonal
matrices, and (Zhang, 2011, Chapter 7), (Axler, 2015, Chapter 7), and (Horn and
Johnson, 2013, Chapter 7) on positive semideï¬nite matrices. Much more on the
multivariate Gaussian distribution can be found in (Tong, 1990), (Johnson and
Kotz, 1972, Chapter 35), and (Simon, 2002). For more on estimation and linear
estimation, see (Poor, 1994) and (Kailath, Sayed, and Hassibi, 2000). And for more
on quadratic forms see (Herstein, 1975, Section 6.11, pp. 350â€“354) and (Halmos,
1958).
13If you are worried because, technically speaking, the joint density function is not unique, do
not be: Two joint density functions of (X, Y, Z) can only diï¬€er on a set of Lebesgue measure
zero. Consequently, if they are both continuous, then they must be identical.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

534
The Multivariate Gaussian Distribution
23.12
Exercises
Exercise 23.1 (Covariance Matrices). Which of the following matrices cannot be a co-
variance matrix of some real random vector?
A =
5
0
0
âˆ’1

,
B =
5
1
2
2

,
C =
 2
10
10
1

,
D =
 1
âˆ’1
âˆ’1
1

.
Exercise 23.2 (An Orthogonal Matrix of Determinant 1). Show that in Theorem 23.6.14
the orthogonal matrix U can be chosen to have determinant +1.
Exercise 23.3 (Deriving the Characteristic Function of Sum). Let X be a random 2-
vector of characteristic function Î¦X(Â·). Express the characteristic function of the sum of
its components in terms of Î¦X(Â·).
Exercise 23.4 (More on Independence). Show that the components X(1) and X(2) of a
random 2-vector X are independent if, and only if, the vectorâ€™s characteristic function
Î¦X(Â·) factorizes as
Î¦X(Ï–) = g1

Ï–(1)
g2

Ï–(2)
,
Ï– =

Ï–(1), Ï–(2)T âˆˆR2,
for some functions g1 and g2.
Exercise 23.5 (The Probabilities of Half Spaces). Prove that a probability distribution
on R2 is uniquely determined by the probabilities of all half planes.
(See (Feller, 1971, Chapter XV, Section 7, Footnote 8).)
Hint: You are asked to show that the values of Pr[Î±X + Î²Y â‰¤Î³] for all Î±, Î², Î³ âˆˆR fully
determine the joint distribution of X and Y .
Exercise 23.6 (The Distribution of Linear Functionals). Let X and Y be random n-vectors
of components X(1), . . . , X(n) and Y (1), . . . , Y (n). Assume that for all deterministic co-
eï¬ƒcients Î±1, . . . , Î±n âˆˆR the random variables n
Î½=1 Î±Î½X(Î½) and n
Î½=1 Î±Î½Y (Î½) have the
same distribution, i.e.,

n

j=1
Î±jX(j) L=
n

j=1
Î±jY (j)

,
	
Î±1, . . . , Î±n âˆˆR

.
(i) Show that the characteristic function of X must be equal to that of Y.
(ii) Show that X and Y must have the same distribution.
Exercise 23.7 (Independence and Independence between Linear Functionals). Let the
random n-vector X and the random m-vector Y be such that for every Î± âˆˆRn and
Î² âˆˆRm the random variables Î±TX and Î²TY are independent. Prove that X and Y must
be independent.
Exercise 23.8 (A Mixture of Gaussians). Let X âˆ¼N

Î¼x, Ïƒ2
x

and Y âˆ¼N

Î¼y, Ïƒ2
y

be
Gaussian random variables. Let E take on the values 0 and 1 equiprobably and indepen-
dently of (X, Y ). Deï¬ne the mixture RV
Z =

X
if E = 0,
Y
if E = 1.
Must Z be Gaussian? Can Z be Gaussian? Compute Zâ€™s characteristic function.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.12 Exercises
535
Exercise 23.9 (Multivariate Gaussians). Show that if Z is a univariate Gaussian, then
the random vector (Z, Z)T is a Gaussian vector. What is its canonical representation?
Exercise 23.10 (Manipulating Gaussians). Let W1, W2, . . . , W5 be IID N(0, 1). Deï¬ne
Y = 3W1 + 4W2 âˆ’2W3 + W4 âˆ’W5 and Z = W1 âˆ’4W2 âˆ’2W3 + 3W4 âˆ’W5. What is the
joint distribution of (Y, Z)?
Exercise 23.11 (The Sum of Independent Gaussian Vectors). Let X and Y be indepen-
dent Gaussian random n-vectors, where X âˆ¼N(Î¼X, KXX) and Y âˆ¼N(Î¼Y, KYY).
(i) Compute the mean vector and covariance matrix of the sum X + Y.
(ii) Must X + Y be a Gaussian random vector?
Exercise 23.12 (Largest Eigenvalue). Let X be a zero-mean Gaussian n-vector of covari-
ance matrix K âª°0, and let Î»max denote the maximal eigenvalue of K. Show that for some
random n-vector Z independent of X
X + Z âˆ¼N

0, Î»maxIn

,
where In denotes the n Ã— n identity matrix.
Exercise 23.13 (The Error Probability Revisited). Show that pâˆ—(correct) of (21.69) in
Exercise 21.5 can be rewritten as
pâˆ—(correct) = 1
M exp
	
âˆ’Es
2Ïƒ2

E
+
exp
	 1
Ïƒ max
m

Î(m)
,
,
where (Î(1), . . . , Î(M))T is a centered Gaussian with a covariance matrix whose Row-j
Column-â„“entry is âŸ¨sj, sâ„“âŸ©E.
Exercise 23.14 (Gaussian Marginals). Let X and Z be IID N(0, 1). Let Y = |Z| sgn(X),
where sgn(X) is 1 if X â‰¥0 and is âˆ’1 otherwise. Show that X is Gaussian, that Y is
Gaussian, but that they are not jointly Gaussian. Sketch the contour lines of their joint
probability density function.
Exercise 23.15 (A Linear Functional Is Gaussian). Suppose Î±TX is Gaussian for some
deterministic nonzero Î± âˆˆR2 and some random 2-vector X. Must X be Gaussian?
Exercise 23.16 (Independence, Uncorrelatedness and Gaussianity). Let the random vari-
ables X and H be independent with X âˆ¼N(0, 1) and with H taking on the values Â±1
equiprobably. Let Y = HX denote their product.
(i) Find the density of Y .
(ii) Are X and Y correlated?
(iii) Compute Pr
$
|X| â‰¥1
%
and Pr
$
|Y | â‰¥1
%
.
(iv) Compute the probability that both |X| and |Y | exceed 1.
(v) Are X and Y independent?
(vi) Is the vector (X, Y )T a Gaussian vector?
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

536
The Multivariate Gaussian Distribution
Exercise 23.17 (The Squared Euclidean Norm of Gaussian Vectors). Show that the sum
of the squares of the components of a centered Gaussian vector can be written as a linear
combination of the squares of independent standard Gaussians. What are the coeï¬ƒcients?
Exercise 23.18 (Expected Maximum of Jointly Gaussians).
(i) Let (X1, X2, . . . , Xn, Y ) have an arbitrary joint distribution with E[Y ] = 0. Here
Y need not be independent of (X1, X2, . . . , Xn). Prove that
E
+
max
1â‰¤jâ‰¤n

Xj + Y
,
= E
+
max
1â‰¤jâ‰¤n

Xj
,
.
(ii) Use Part (i) to prove that if (U, V ) are jointly Gaussian and of zero mean, then
E
$
max{U, V }
%
=
&
E
$
(U âˆ’V )2%
2Ï€
.
Exercise 23.19 (The Density of a Bivariate Gaussian). Let X and Y be jointly Gaussian
with means Î¼x and Î¼y and with positive variances Ïƒ2
x and Ïƒ2
y. Let
Ï = Cov[X, Y ]
Ïƒx Ïƒy
be their correlation coeï¬ƒcient. Assume |Ï| < 1.
(i) Find the joint density of X and Y .
(ii) Find the conditional density of X given Y = y.
Exercise 23.20 (A Training Symbol). Conditional on (X1, X2) = (x1, x2), the observable
(Y1, Y2) is given by
YÎ½ = AxÎ½ + ZÎ½,
Î½ = 1, 2,
where Z1, Z2, and A are independent with Z1, Z2 âˆ¼IID N

0, Ïƒ2
and A âˆ¼N(0, 1).
Suppose that X1 = 1 (deterministically) and that X2 takes on the values Â±1 equiprobably.
(i) Derive an optimal rule for guessing X2 based on (Y1, Y2).
(ii) Consider a decoder that operates in two stages.
In the ï¬rst stage the decoder
estimates A from Y1 with an estimator that minimizes the mean squared-error.
In the second stage it uses the ML decoding rule for guessing X2 based on Y2
by pretending that A is given by its estimate from the ï¬rst stage. Compute the
probability of error of this decoder. Is it optimal?
Exercise 23.21 (The Estimation Error Need not Be Independent of the Observable).
Let Y be a uniform binary RV, and let the conditional laws of the RV X given Y = 0 and
Y = 1 be diï¬€erent but both of zero mean. Find the estimator of X based on Y of least
mean squared-error, and show that its error is not independent of the observable Y .
Exercise 23.22 (On Wickâ€™s Formula). Let X be a centered Gaussian n-vector, and let
g1, . . . , g2k+1 : Rn â†’R be an odd number of (not necessarily diï¬€erent) linear functionals
from Rn to R. Show that
E
$
g1(X) g2(X) Â· Â· Â· g2k+1(X)
%
= 0.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.12 Exercises
537
Exercise 23.23 (Jointly Gaussians with Positive Correlation). Let X and Y be jointly
Gaussian with means Î¼x and Î¼y; positive variances Ïƒ2
x and Ïƒ2
y; and correlation coeï¬ƒcient Ï
as in Exercise 23.19 satisfying |Ï| < 1.
(i) Show that, conditional on Y = y, the distribution of X is Gaussian with mean
Î¼x + Ï Ïƒx
Ïƒy (y âˆ’Î¼y) and variance Ïƒ2
x(1 âˆ’Ï2).
(ii) Show that if Ï â‰¥0, then the family fX|Y (x|y) has the monotone likelihood ratio
property that the mapping
x â†’fX|Y (x|y)
fX|Y (x|yâ€²)
is nondecreasing whenever yâ€² â‰¤y. Here fX|Y (Â·|y) is the conditional density of X
given Y = y.
(iii) Show that if Ï â‰¥0, then the joint density fX,Y (Â·) has the Total Positivity of
Order 2 (TP2) property, i.e.,
fX,Y (xâ€², y) fX,Y (x, yâ€²) â‰¤fX,Y (x, y) fX,Y (xâ€², yâ€²),
	
xâ€² < x, yâ€² < y

.
See (Tong, 1990, Chapter 4, Section 4.3.1, Fact 4.3.1 and Theorem 4.3.1).
Exercise 23.24 (Priceâ€™s Theorem). Let X be a centered Gaussian n-vector of covariance
matrix Î›. Let Î»(j,â„“) = E
$
X(j)X(â„“)%
be the Row-j Column-â„“entry of Î›. Let fX(x; Î›)
denote the density of X (when Î› is nonsingular).
(i) Expressing the FT of the partial derivative of a function in terms of the FT of the
original function and using the characteristic function of a Gaussian (23.52), derive
Plackettâ€™s Identities
âˆ‚fX(x; Î›)
âˆ‚Î»(j,j)
= 1
2
âˆ‚2fX(x; Î›)
âˆ‚(x(j))2
,
âˆ‚fX(x; Î›)
âˆ‚Î»(j,â„“)
= âˆ‚2fX(x; Î›)
âˆ‚x(j)âˆ‚x(â„“) ,
j Ì¸= â„“.
(ii) Using integration by parts, derive Priceâ€™s Theorem: if h: Rn â†’R is twice con-
tinuously diï¬€erentiable with h and its ï¬rst and second derivatives growing at most
polynomially in âˆ¥xâˆ¥as âˆ¥xâˆ¥â†’âˆ, then
âˆ‚E
$
h(X)
%
âˆ‚Î»(j,â„“)
=

Rn
âˆ‚2h(x)
âˆ‚x(j)âˆ‚x(â„“) fX(x; Î›) dx,
j Ì¸= â„“.
(See (Adler, 1990, Chapter 2, Section 2.2) for the case where Î› is singular.)
(iii) Show that if in addition to the assumptions of Part (ii) we also assume that for
some j Ì¸= â„“
âˆ‚2h(x)
âˆ‚x(j)âˆ‚x(â„“) â‰¥0,
x âˆˆRn,
(23.97)
then E[h(X)] is a nondecreasing function of Î»(j,â„“).
(iv) Conclude that if h(x) = 0n
Î½=1 gÎ½(x(Î½)), where for each Î½ âˆˆ{1, . . . , n} the function
gÎ½ : R â†’R is nonnegative, nondecreasing, twice continuously diï¬€erentiable, and
satisfying the growth conditions of h in Part (ii), then
E
1
n
/
Î½=1
gÎ½

X(Î½)
2
is monotonically nondecreasing in Î»(j,â„“) whenever j Ì¸= â„“.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

538
The Multivariate Gaussian Distribution
(v) By choosing gÎ½(Â·) to approximate the step function Î± â†’I

Î± â‰¥Î¶(Î½)
for properly
chosen Î¶(Î½), prove Slepianâ€™s Inequality: if X âˆ¼N(Î¼, Î›), then for every choice of
Î¾(1), . . . , Î¾(n) âˆˆR
Pr
+
X(1) â‰¥Î¾(1), . . . , X(n) â‰¥Î¾(n),
is monotonically nondecreasing in Î»(j,â„“) whenever j Ì¸= â„“. See (Tong, 1990, Chap-
ter 5, Section 5.1.4, Theorem 5.1.7).
(vi) Modify the arguments in Parts (iv) and (v) to show that if X âˆ¼N(Î¼, Î›), then for
every choice of Î¾(1), . . . , Î¾(n) âˆˆR
Pr
+
X(1) â‰¤Î¾(1), . . . , X(n) â‰¤Î¾(n),
is monotonically nondecreasing in Î»(j,â„“) whenever j Ì¸= â„“. See (Adler, 1990, Chap-
ter 2, Section 2.2, Corollary 2.4).
Exercise 23.25 (Jointly Gaussians of Equal Sign). Let X and Y be jointly Gaussian and
centered with positive variances and correlation coeï¬ƒcient Ï. Prove that
Pr
$
XY > 0
%
= 1
2 + Ï†
Ï€ ,
where âˆ’Ï€/2 â‰¤Ï† â‰¤Ï€/2 is such that sin Ï† = Ï. We propose the following approach.
(i) Show that it suï¬ƒces to prove the result when X and Y are of unit variance.
(ii) Show that, for such X and Y , if we deï¬ne
W =
1

1 âˆ’Ï2 X âˆ’
Ï

1 âˆ’Ï2 Y,
Z = Y,
then W and Z are IID N(0, 1).
(iii) Show that X and Y can be expressed as
X = R sin(Î˜ + Ï†),
Y = R cos Î˜,
where Ï† is as deï¬ned before, Î˜ is uniformly distributed over the interval [âˆ’Ï€, Ï€),
R is independent of Î˜, and fR(r) = r eâˆ’r2/2 I{r > 0}.
(iv) Justify the calculation
Pr
$
XY > 0
%
= 2 Pr
$
X > 0, Y > 0
%
= 2 Pr
$
sin(Î˜ + Ï†) > 0, cos Î˜ > 0
%
= 1
2 + Ï†
Ï€ .
Hint: Exercise 19.7 may be useful for Part (iii).
Exercise 23.26 (Bussgangâ€™s Theorem). Let X and Y be centered and jointly Gaussian,
and let h: R â†’R be such that h(Y ) has ï¬nite variance. Prove that
E[X h(Y )] =
Î±
Var[Y ] E[XY ] ,
where Î± is given by
Î± = E[Y h(Y )]
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,

23.12 Exercises
539
and is thus determined by Var[Y ] and h(Â·); it does not depend on the correlation coeï¬ƒcient
between X and Y .
Hint: Use Exercise 23.19 (i) and note that
x2
Ïƒ2x
âˆ’2Ïxy
ÏƒxÏƒy + y2
Ïƒ2y
+ Ï2y2
Ïƒ2y
âˆ’Ï2y2
Ïƒ2y
=
 x
Ïƒx âˆ’Ï y
Ïƒy
2
+ y2(1 âˆ’Ï2)
Ïƒ2y
.
Alternatively, express E[X h(Y )] as an integral, diï¬€erentiate with respect to E[XY ] under
the integral, use Plackettâ€™s identity (Exercise 23.24) and integrate over x by parts.
Exercise 23.27 (Guessing the Sign of the Correlation). Let H be a binary RV with
Pr[H = 0] = Ï€0
and
Pr[H = 1] = Ï€1,
where Ï€0, Ï€1 > 0 sum to one. Conditional on H = 0 the observed vector Y is a centered
Gaussian 2-vector of covariance matrix
K0 =
1
Ï
Ï
1

,
where 0 < Ï < 1, whereas conditional on H = 1 it is a centered Gaussian 2-vector of
covariance matrix
K1 =
 1
âˆ’Ï
âˆ’Ï
1

.
(i) Compute the conditional densities fY|H=0(Â·) and fY|H=1(Â·).
(ii) Find a one-dimensional suï¬ƒcient statistic for guessing H based on Y.
(iii) Describe an optimal decision rule for guessing H based on Y.
Sketch the cor-
responding decision regions for each of the three cases: Ï€0 = Ï€1, Ï€0 > Ï€1 and
Ï€0 < Ï€1.
(iv) Compute the Bhattacharyya Bound on the optimal probability of error pâˆ—(error).
(v) Compute limÏâ†‘1 pâˆ—(error).
Hint: For Part (i) note that if
 a b
c d

is invertible, then its inverse is
1
adâˆ’bc
 d
âˆ’b
âˆ’c
a

. For
Part (v) the answer to Part (iv) may be helpful.
Exercise 23.28 (Guessing the Permutation). Let H take on the values 0 and 1 equiprob-
ably and independently of the standard Gaussian 3-vector W. The observable Y is equal
to AW when H = 0 and to BW when H = 1, where
A =
â›
â
1
0
0
0
Î±
0
0
0
Î²
â
â ,
B =
â›
â
0
0
Î²
0
Î±
0
1
0
0
â
â ,
and Î±, Î² > 0 with Î² Ì¸= 1.
(i) If you must guess H based on two of the three components of Y, which two would
you choose in order to minimize the probability of error?
(ii) Determine the conditional densities fY|H=0 and fY|H=1.
(iii) Find a one-dimensional suï¬ƒcient statistic for guessing H based on Y.
(iv) Describe an optimal decision rule for guessing H based on Y.
(v) Compute the Bhattacharyya Bound on the optimal probability of error.
Exercise 23.29 (Independent Binary Random Variables). Show that two zero-one valued
(i.e., binary) random variables are independent if, and only if, they are uncorrelated.
available at 
.025
14:37:28, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 24
Complex Gaussians and Circular Symmetry
24.1
Introduction
This chapter introduces the complex Gaussian distribution and the circular sym-
metry property. We start with the scalar case and then extend these notions to
random vectors. We rely heavily on Chapter 17 for the basic properties of complex
random variables and on Chapter 23 for the properties of the multivariate Gaussian
distribution.
24.2
Scalars
24.2.1
Standard Complex Gaussians
Deï¬nition 24.2.1 (Standard Complex Gaussian). A standard complex Gaus-
sian is a complex random variable whose real and imaginary parts are independent
N(0, 1/2) random variables.
If W is a standard complex Gaussian, then its density is given by
fW (w) = 1
Ï€ eâˆ’|w|2,
w âˆˆC,
(24.1)
because
fW (w) = fRe(W ),Im(W )

Re(w), Im(w)

= fRe(W )

Re(w)

fIm(W )

Im(w)

=
1
âˆšÏ€ eâˆ’Re(w)2
1
âˆšÏ€ eâˆ’Im(w)2
= 1
Ï€ eâˆ’|w|2,
w âˆˆC,
where the ï¬rst equality follows from the deï¬nition of the density fW (w) of a
CRV W at w âˆˆC as the joint density fRe(W ),Im(W ) of its real and imaginary
parts (Re(W), Im(W)) evaluated at

Re(w), Im(w)

(Section 17.3.1); the second
because the real and imaginary parts of a standard complex Gaussian are indepen-
dent; the third because the real and imaginary parts of a standard Gaussian are
540
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.2 Scalars
541
zero-mean variance-1/2 real Gaussians whose density can thus be computed from
(19.6) (by substituting 1/2 for Ïƒ2); and where the ï¬nal equality follows because for
any complex number w we have Re(w)2 + Im(w)2 = |w|2.
Because the real and imaginary parts of a standard complex Gaussian W are of
zero mean, it follows that
E[W] = E[Re(W)] + i E[Im(W)]
= 0.
And because they are each of variance 1/2, it follows from (17.14c) that a standard
complex Gaussian W has unit-variance
Var[W] = E

|W|2
= 1.
(24.2)
Moreover, since a standard complex Gaussian is of zero mean and since its real
and imaginary parts are of equal variance and uncorrelated, a standard Gaussian
is proper (Deï¬nition 17.3.1 and Proposition 17.3.2), i.e.,
E[W] = 0
and
E

W 2
= 0.
(24.3)
Finally note that, by (24.1), the density fW (Â·) of a standard complex Gaussian
is radially-symmetric, i.e., its value at w âˆˆC depends on w only via its mod-
ulus |w|. A CRV whose density is radially-symmetric is said to be circularly-
symmetric, but the deï¬nition of circular symmetry applies also to complex ran-
dom variables that do not have a density. This is the topic of the next section.
24.2.2
Circular Symmetry
Deï¬nition 24.2.2 (Circularly-Symmetric CRV). A CRV Z is said to be circularly-
symmetric if for any deterministic Ï† âˆˆ[âˆ’Ï€, Ï€) the distribution of eiÏ†Z is identical
to the distribution of Z:
eiÏ†Z
L= Z,
Ï† âˆˆ[âˆ’Ï€, Ï€).
(24.4)
Note 24.2.3. If the expectation of a circularly-symmetric CRV is deï¬ned, then it
must be zero.
Proof. Let Z be circularly-symmetric. It then follows from (24.4) that eiÏ†Z and Z
are of equal expectation, so
E[Z] = E

eiÏ†Z

= eiÏ† E[Z] ,
Ï† âˆˆ[âˆ’Ï€, Ï€),
which, by considering a Ï† for which eiÏ† Ì¸= 1, implies that E[Z] must be zero.
To shed some light on the deï¬nition of circular symmetry we shall need Proposi-
tion 24.2.5 ahead, which is highly intuitive but a bit cumbersome to state. Before
stating it we provide its discrete counterpart, which is a bit easier to state: it
makes formal the intuition that if after giving the wheel-of-fortune an arbitrary
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

542
Complex Gaussians and Circular Symmetry
spin, you give it another fair spin, then the combined result is a fair spin that does
not depend on the initial spin. The case Î· = 2 is critical in cryptography. It shows
that taking the mod-2 sum of a binary source sequence with a sequence of IID
random bits results in a sequence that is independent of the source sequence.
Proposition 24.2.4. Fix a positive integer Î·, and deï¬ne the set A = {0, . . . , Î· âˆ’1}.
Let N be a RV taking values in the set A.
Then the following statements are
equivalent:
(a) The RV N is uniformly distributed over the set A.
(b) For any integer-valued RV K that is independent of N, the RV (N+K) mod Î·
is independent of K and uniformly distributed over A.1
Proof. We ï¬rst show (b) â‡’(a). To this end, deï¬ne K to be a RV that takes on
the value zero deterministically. Being deterministic, it is independent of every
RV, and in particular of N. Statement (b) thus guarantees that (N + 0) mod Î· is
uniformly distributed over A. Since we have assumed from the outset that N takes
values in A, it follows that (N +0) mod Î· = N, so the uniformity of (N +0) mod Î·
over A implies the uniformity of N over A.
We next show (a) â‡’(b). To this end, we need to show that if N is uniformly
distributed over A and if K is independent of N, then2
Pr
%
(N + K) mod Î·

= a
 K = k
&
= 1
Î· ,

k âˆˆZ, a âˆˆA

.
(24.5)
By the independence of N and K it follows that
Pr
%
(N +K) mod Î·

= a
 K = k
&
= Pr
%
(N +k) mod Î·

= a
&
,

k âˆˆZ, a âˆˆA

,
so to prove (24.5) it suï¬ƒces to prove
Pr
%
(N + k) mod Î·

= a
&
= 1
Î· ,

k âˆˆZ, a âˆˆA

.
(24.6)
This can be proved as follows. Because N is uniformly distributed over A, it follows
that N+k is uniformly distributed over the set {k, k+1, . . . , k+Î·âˆ’1}. And, because
the mapping m 	â†’(m mod Î·) is a one-to-one mapping from {k, k+1, . . . , k+Î·âˆ’1}
onto A, this implies that (N + k) mod Î· is also uniformly distributed over A, thus
establishing (24.6).
Proposition 24.2.5. Let Î˜ be a RV taking values in [âˆ’Ï€, Ï€). Then the following
statements are equivalent:
(a) The RV Î˜ is uniformly distributed over [âˆ’Ï€, Ï€).
1Here m mod Î· is the remainder of dividing m by Î·, i.e., the unique Î½ âˆˆA such that m âˆ’Î½
is an integer multiple of Î·. E.g., 17 mod 8 = 1.
2Recall that the random variables X and Y are independent if, and only if, the conditional
distribution of X given Y is equal to the marginal distribution of X.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.2 Scalars
543
âˆ’Ï€ + Ï†
Ï€ + Ï†
+Ï€
âˆ’Ï€
Figure 24.1: The function Î¾ 	â†’

Î¾ mod [âˆ’Ï€, +Ï€)

plotted for Î¾ âˆˆ[âˆ’Ï€ + Ï†, Ï€ + Ï†).
(b) For any real RV Î¦ that is independent of Î˜, the RV (Î˜ + Î¦) mod [âˆ’Ï€, Ï€) is
independent of Î¦ and uniformly distributed over the interval [âˆ’Ï€, Ï€).3
Proof. The proof is similar to the proof of Proposition 24.2.4 but with an added
twist. The twist is needed because if X has a uniform density and if a function g
is one-to-one (injective) and onto (surjective), then g(X) need not be uniformly
distributed. (For example, if X âˆ¼U ([0, 1]) and if g: [0, 1] â†’[0, 1] maps Î¾ to Î¾2,
then g(X) is not uniform.)
To prove that (b) implies (a) we simply apply (b) to the deterministic RV Î¦ = 0.
We next prove that (a) implies (b). As in the discrete case, it suï¬ƒces to show that
if Î˜ is uniformly distributed over [âˆ’Ï€, Ï€), then for any deterministic Ï† âˆˆR the
distribution of (Î˜ + Ï†) mod [âˆ’Ï€, Ï€) is uniform over [âˆ’Ï€, Ï€), irrespective of Ï†. To
this end we ï¬rst note that because Î˜ is uniform over [âˆ’Ï€, Ï€) it follows that Î˜+Ï† is
uniform over [Ï† âˆ’Ï€, Ï† + Ï€). Consider now the mapping g: [Ï† âˆ’Ï€, Ï† + Ï€) â†’[âˆ’Ï€, Ï€)
deï¬ned by g: Î¾ 	â†’

Î¾ mod [âˆ’Ï€, Ï€)

. This function is a one-to-one mapping onto
[âˆ’Ï€, Ï€) and is diï¬€erentiable except at the point Î¾âˆ—âˆˆ[Ï† âˆ’Ï€, Ï† + Ï€) satisfying
Î¾âˆ—mod [âˆ’Ï€, Ï€) = Ï€, i.e., the point Î¾âˆ—âˆˆ[Ï†âˆ’Ï€, Ï†+Ï€) of the form Î¾âˆ—= 2Ï€m+Ï€ for
some integer m. At all other points its derivative is 1; see Figure 24.1. (Incidentally,
âˆ’Ï€ + Ï† is mapped to a negative number if Ï† < Î¾âˆ—and to a positive number if
Ï† > Î¾âˆ—. In Figure 24.1 we assume the latter.) Applying the formula for computing
the density of g(X) from the density of X (Theorem 17.3.4) we ï¬nd that if Î˜ + Ï†
is uniform over [Ï† âˆ’Ï€, Ï† + Ï€), then g(Ï† + Î˜) is uniform over [âˆ’Ï€, Ï€).
With the aid of Proposition 24.2.5 we can now give alternative characterizations
of circular symmetry.
3Here x mod [âˆ’Ï€, Ï€) is the unique Î¾ âˆˆ[âˆ’Ï€, Ï€) such that x âˆ’Î¾ is an integer multiple of 2Ï€.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

544
Complex Gaussians and Circular Symmetry
Proposition 24.2.6 (Characterizing Circular Symmetry). Let Z be a CRV with
a density. Then each of the following statements is equivalent to the statement
that Z is circularly-symmetric:
(a) The distribution of eiÏ†Z is identical to the distribution of Z, for any deter-
ministic Ï† âˆˆ[âˆ’Ï€, Ï€).
(b) The CRV Z has a radially-symmetric density function, i.e., a density fZ(Â·)
whose value at z depends on z only via its modulus |z|.
(c) The CRV Z can be written as Z = R eiÎ˜, where R â‰¥0 and Î˜ are independent
real random variables and Î˜ âˆ¼U ([âˆ’Ï€, Ï€)).
Proof. Statement (a) is the deï¬nition of circular symmetry (Deï¬nition 24.2.2).
The proof of (a) â‡’(b) is slightly obtuse because the density of a CRV is not
unique.4
We begin by noting that if Z is of density fZ(Â·), then by (17.34) the
CRV eiÏ†Z is of density w 	â†’fZ(eâˆ’iÏ†w). Thus, if Z
L= eiÏ†Z and if Z is of density
fZ(Â·), then Z is also of density w 	â†’fZ(eâˆ’iÏ†w). Consequently, if Z is circularly-
symmetric, then for every Ï† âˆˆ[âˆ’Ï€, Ï€) the mapping w 	â†’fZ(eâˆ’iÏ†w) is a density
for Z. We can therefore conclude that the mapping
w 	â†’1
2Ï€
 Ï€
âˆ’Ï€
fZ(eâˆ’iÏ†w) dÏ†
is also a density for Z, and this function is radially-symmetric.
The fact that (b) â‡’(c) follows because if we deï¬ne R to be the magnitude of Z
and Î˜ to be its argument, then Z = R eiÎ˜, and
fR,Î˜(r, Î¸) = rfZ(r eiÎ¸)
= rfZ(r)
=

2Ï€rfZ(r)
 1
2Ï€ ,
where the ï¬rst equality follows from (17.29) and the second from our assumption
that fZ(z) depends on z only via its modulus |z|. The joint density of R, Î˜ is thus
of a product form, thereby indicating that R and Î˜ are independent. And it does
not depend on Î¸, thus indicating that its marginal Î˜ is uniformly distributed.
We ï¬nally show that (c) â‡’(a). To that end we assume that R â‰¥0 and Î˜ are
independent with Î˜ being uniformly distributed over [âˆ’Ï€, Ï€) and proceed to show
that R eiÎ˜ is circularly-symmetric, i.e., that
R eiÎ˜ L= R ei(Î˜+Ï†),
Ï† âˆˆ[âˆ’Ï€, Ï€).
(24.7)
To prove (24.7) we note that
ei(Î˜+Ï†) = ei((Î˜+Ï†) mod [âˆ’Ï€,Ï€))
L= eiÎ˜,
(24.8)
4And not all the functions that are densities for a given circularly-symmetric CRV Z are
radially-symmetric. The radial symmetry can be broken on a set of Lebesgue measure zero. We
can therefore only claim that there exists â€œaâ€ radially-symmetric density function for Z.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.2 Scalars
545
where the ï¬rst equality follows from the periodicity of the complex exponentials,
and where the equality in distribution follows from Proposition 24.2.5 because
Î˜ âˆ¼U ([âˆ’Ï€, Ï€)). The proof is now completed by noting that (24.7) follows from
(24.8) and from the independence of R and Î˜. (If X is independent of Y , if X is
independent of Z, and if Y
L= Z, then (X, Y )
L= (X, Z) and hence XY
L= XZ.)
Example 24.2.7. Let the CRV Z be given by Z = eiÎ¦, where Î¦ âˆ¼U ([âˆ’Ï€, Ï€)).
Then Z is uniformly distributed over the unit circle {z : |z| = 1} and is circularly-
symmetric. It does not have a density.
24.2.3
Properness and Circular Symmetry
Proposition 24.2.8. Every ï¬nite-variance circularly-symmetric CRV is proper.
Proof. Let Z be a ï¬nite-variance circularly-symmetric CRV. By Note 24.2.3 it
follows that E[Z] = 0. To conclude the proof it remains to show that E

Z2
= 0.
To this end we note that
E

Z2
= eâˆ’i2Ï† E
%
eiÏ†Z
2&
= eâˆ’i2Ï† E

Z2
,
Ï† âˆˆ[âˆ’Ï€, Ï€),
(24.9)
where the ï¬rst equality follows by rewriting Z2 as eâˆ’i2Ï† 
eiÏ†Z
2, and where the
second equality follows because the circular symmetry of Z guarantees that Z
and eiÏ†Z have the same law, so the expectations of their squares must be equal.
But (24.9) cannot be satisï¬ed for all Ï† âˆˆ[âˆ’Ï€, Ï€) (or for that matter for any Ï† such
that ei2Ï† Ì¸= 1) unless E

Z2
= 0.
Note 24.2.9. Not every proper CRV is circularly-symmetric.
Proof. Consider the CRV Z that takes on the four values 1 + i, 1 âˆ’i, âˆ’1 + i, and
âˆ’1 âˆ’i equiprobably. Its real and imaginary parts are independent, each taking on
the values Â±1 equiprobably. Computing E[Z] and E

Z2
we ï¬nd that they are both
zero, so Z is proper. To see that Z is not circularly-symmetric consider the random
variable eiÏ€/4Z. Its distribution is diï¬€erent from the distribution of Z because Z
takes values in the set {1 + i, âˆ’1 + i, 1 âˆ’i, âˆ’1 âˆ’i}, and eiÏ€/4Z takes values in the
rotated set {
âˆš
2, âˆ’
âˆš
2,
âˆš
2i, âˆ’
âˆš
2i}.
The fact that not every proper CRV is circularly-symmetric is not surprising be-
cause whether a CRV is proper or not is determined solely by its mean and by the
covariance matrix of its real and imaginary parts, whereas circular symmetry has
to do with the entire distribution.
24.2.4
Complex Gaussians
The deï¬nition of a complex Gaussian builds on the deï¬nition of a real Gaussian
vector (Deï¬nition 23.1.1).
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

546
Complex Gaussians and Circular Symmetry
Deï¬nition 24.2.10 (Complex Gaussian). A complex Gaussian is a CRV whose
real and imaginary parts are jointly Gaussian real random variables. A centered
complex Gaussian is a complex Gaussian of zero mean.
An example of a complex Gaussian is the standard complex Gaussian, which we
encountered in Section 24.2.1.
The class of complex Gaussians is closed under multiplication by deterministic
complex numbers. Thus, if Z is a complex Gaussian and if Î± âˆˆC is deterministic,
then Î±Z is also a complex Gaussian. Indeed,
	Re(Î±Z)
Im(Î±Z)

=
	Re(Î±)
âˆ’Im(Î±)
Im(Î±)
Re(Î±)

 	Re(Z)
Im(Z)

,
so the claim follows from the fact that multiplying a real Gaussian vector by a
deterministic real matrix results in a real Gaussian vector (Proposition 23.6.3).
We leave it to the reader to verify that, more generally, if Z is a complex Gaussian
and if Î±, Î² âˆˆC are deterministic, then Î±Z +Î²Zâˆ—is also a complex Gaussian. (This
is a special case of Proposition 24.3.9 ahead.)
Not every centered complex Gaussian can be expressed as the scaling of a standard
complex Gaussian by some complex number. But the following result characterizes
those that can:
Proposition 24.2.11.
(i) For every centered complex Gaussian Z we can ï¬nd coeï¬ƒcients Î±, Î² âˆˆC so
that
Z
L= Î±W + Î²W âˆ—,
(24.10)
where W is a standard complex Gaussian.
(ii) A centered complex Gaussian Z is proper if, and only if, there exists some
Î± âˆˆC such that Z
L= Î±W, where W is a standard complex Gaussian.
Proof. We begin with Part (i). First note that since Z is a complex Gaussian, its
real and imaginary parts are jointly Gaussian, and it follows from Corollary 23.6.13
that there exist deterministic real numbers a(1,1), a(1,2), a(2,1), a(2,2) such that
	Re(Z)
Im(Z)

L=
	a(1,1)
a(1,2)
a(2,1)
a(2,2)

 	
W1
W2

,
(24.11)
where W1 and W2 are independent real standard Gaussians. Next note that by
direct computation
	
Re(Î±W + Î²W âˆ—)
Im(Î±W + Î²W âˆ—)

=
- Re(Î±)+Re(Î²)
âˆš
2
Im(Î²)âˆ’Im(Î±)
âˆš
2
Im(Î²)+Im(Î±)
âˆš
2
Re(Î±)âˆ’Re(Î²)
âˆš
2
. 	âˆš
2 Re(W)
âˆš
2 Im(W)

.
(24.12)
Since, by the deï¬nition of a standard complex Gaussian W,
	
W1
W2

L=
	âˆš
2 Re(W)
âˆš
2 Im(W)

(24.13)
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.2 Scalars
547
it follows from (24.11), (24.12), and (24.13) that if Î± and Î² are chosen so that
- Re(Î±)+Re(Î²)
âˆš
2
Im(Î²)âˆ’Im(Î±)
âˆš
2
Im(Î²)+Im(Î±)
âˆš
2
Re(Î±)âˆ’Re(Î²)
âˆš
2
.
=
	
a(1,1)
a(1,2)
a(2,1)
a(2,2)

,
i.e., if
Î± =
1
âˆš
2

a(1,1) + a(2,2)
+ i

a(2,1) âˆ’a(1,2)
,
Î² =
1
âˆš
2

a(1,1) âˆ’a(2,2)
+ i

a(2,1) + a(1,2)
,
then
	Re(Z)
Im(Z)

L=
	Re(Î±W + Î²W âˆ—)
Im(Î±W + Î²W âˆ—)

,
and (24.10) is satisï¬ed.
We next turn to Part (ii). One direction is straightforward: if Z
L= Î±W, then Z
must be proper because from (24.3) it follows that E[Î±W] = Î±E[W] = 0 and
E

(Î±W)2
= Î±2E

W 2
= 0.
We next prove the other direction that if Z is a proper complex Gaussian, then
Z
L= Î±W for some Î± âˆˆC and some standard complex Gaussian W. Let Z be a
proper complex Gaussian. By Part (i) it follows that there exist Î±, Î² âˆˆC such that
(24.10) is satisï¬ed. Consequently, for this choice of Î± and Î² we have
0 = E

Z2
= E

(Î±W + Î²W âˆ—)2
= Î±2E

W 2
+ 2Î±Î²E[WW âˆ—] + Î²2E

(W âˆ—)2
= 2Î±Î²,
where the ï¬rst equality follows because Z is proper; the second because Î± and Î²
have been chosen so that (24.10) holds; the third by opening the brackets and using
the linearity of expectation; and the fourth by (24.3) and (24.2). It follows that
either Î± or Î² must be zero. Since W
L= W âˆ—, there is no loss in generality in assuming
that Î² = 0, thus establishing the existence of Î± âˆˆC such that Z
L= Î±W.
By Proposition 24.2.11 (ii) we conclude that if Z is a proper complex Gaussian, then
Z
L= Î±W for some Î± âˆˆC and some standard complex Gaussian W. Consequently,
the density of such a CRV Z (that is not deterministically zero) is given by
fZ(z) = fW (z/Î±)
|Î±|2
=
1
Ï€|Î±|2 e
âˆ’|z|2
|Î±|2 ,
z âˆˆC,
where the ï¬rst equality follows from the way the density of a CRV behaves under
linear transformations (Theorem 17.3.7 or Lemma 17.4.6), and where the second
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

548
Complex Gaussians and Circular Symmetry
equality follows from (24.1). We thus conclude that if Z is a proper complex Gaus-
sian, then its density is radially-symmetric, and Z must be circularly-symmetric.
The reverse is also true: since every complex Gaussian is of ï¬nite variance, and since
every ï¬nite-variance circularly-symmetric CRV is also proper (Proposition 24.2.8),
we conclude that every circularly-symmetric complex Gaussian is proper. Thus:
Proposition 24.2.12. A complex Gaussian is circularly-symmetric if, and only if,
it is proper.
The picture that thus emerges is the following.
(i) Every ï¬nite-variance circularly-symmetric CRV is proper.
(ii) Some proper CRVs are not circularly symmetric.
(iii) A Gaussian CRV is circularly-symmetric, if and only if, it is proper.
We shall soon see that these observations extend to vectors too. In fact, the reader
is encouraged to consult Figure 24.2 on Page 554, which holds also for CRVs.
24.3
Vectors
24.3.1
Standard Complex Gaussian Vectors
Deï¬nition 24.3.1 (Standard Complex Gaussian Vector). A standard complex
Gaussian vector is a complex random vector whose components are IID and each
of them is a standard complex Gaussian random variable.
If W is a standard complex Gaussian n-vector, then, by the independence of its n
components and by (24.1), its density is given by
fW(w) = 1
Ï€n eâˆ’wâ€ w,
w âˆˆCn.
(24.14)
By the independence of its components and by (24.3)
E[W] = 0
and
E

WWT
= 0.
(24.15)
Thus, every standard complex Gaussian vector is proper (Section 17.4.2). By the
independence of the components and by (24.2) it also follows that
E

WWâ€ 
= In,
(24.16)
where we remind the reader that In denotes the n Ã— n identity matrix.
24.3.2
Circularly-Symmetric Complex Random Vectors
Deï¬nition 24.3.2 (Circularly-Symmetric Complex Random Vectors). We say that
the complex random vector Z is circularly-symmetric if for every Ï† âˆˆ[âˆ’Ï€, Ï€)
the law of eiÏ†Z is identical to the law of Z.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.3 Vectors
549
An equivalent deï¬nition can be given in terms of linear functionals:
Proposition 24.3.3 (Circular Symmetry and Linear Functionals). Each of the fol-
lowing statements is equivalent to the statement that the complex random n-vector Z
is circularly-symmetric.
(a) For every Ï† âˆˆ[âˆ’Ï€, Ï€) the law of the complex random vector eiÏ†Z is the same
as the law of Z:
eiÏ†Z
L= Z,
Ï† âˆˆ[âˆ’Ï€, Ï€).
(24.17)
(b) For every deterministic vector Î± âˆˆCn, the CRV Î±TZ is circularly-symmetric:
eiÏ†Î±TZ
L= Î±TZ,

Î± âˆˆCn, Ï† âˆˆ[âˆ’Ï€, Ï€)

.
(24.18)
Proof. Statement (a) is just the deï¬nition of circular symmetry. We next show
that the two statements (a) and (b) are equivalent. We begin by proving that (a)
implies (b). This is the easy part because applying the same linear functional to
two random vectors that have the same law results in random variables that have
the same law. Consequently, (24.17) implies (24.18).
We now prove that (b) implies (a). We thus assume (24.18) and set out to prove
(24.17). By Theorem 17.4.4 it follows that to establish (24.17) it suï¬ƒces to show
that the random vectors on the RHS and LHS of (24.17) have the same character-
istic function, i.e., that
E

ei Re

Ï–â€  eiÏ†Z

= E

ei Re(Ï–â€ Z)

,
Ï– âˆˆCn.
(24.19)
But this readily follows from (24.18) because upon substituting Ï–â€  for Î±T in
(24.18) we obtain that
Ï–â€  eiÏ†Z
L= Ï–â€ Z,
Ï– âˆˆCn,
and this implies (24.19), because if Z1
L= Z2, then E[g(Z1)] = E[g(Z2)] for any
measurable function g and, in particular, for the function g: Î¾ 	â†’ei Re(Î¾).
The following proposition demonstrates that circular symmetry is preserved by
linear transformations.
Proposition 24.3.4 (Circular Symmetry and Linear Transformations). Let Z be a
circularly-symmetric complex random n-vector and let A be a deterministic complex
mÃ—n matrix. Then the complex random m-vector AZ is also circularly-symmetric.
Proof. By Proposition 24.3.3 it follows that to establish that AZ is circularly-
symmetric it suï¬ƒces to show that for every deterministic Î± âˆˆCm the random
variable Î±TAZ is circularly-symmetric. To show this, ï¬x some arbitrary Î± âˆˆCm.
Because Z is circularly-symmetric, it follows from Proposition 24.3.3 that for every
deterministic vector Î² âˆˆCn, the random variable Î²TZ is circularly-symmetric.
Choosing Î² = ATÎ± establishes that Î±TAZ is circularly-symmetric.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

550
Complex Gaussians and Circular Symmetry
24.3.3
Proper vs. Circularly-Symmetric Vectors
We now extend the relationship between properness and circular symmetry to
vectors:
Proposition 24.3.5 (Circular Symmetry Implies Properness).
(i) Every ï¬nite-variance circularly-symmetric random vector is proper.
(ii) Some proper random vectors are not circularly-symmetric.
Proof. Part (ii) requires no proof because a CRV can be viewed as a complex
random vector taking values in C1, and we have already seen in Section 24.2.3 an
example of a CRV which is proper but not circularly-symmetric (Note 24.2.9).
We now prove Part (i). Let Z be a ï¬nite-variance circularly-symmetric random
n-vector. To establish that Z is proper we will show that for every Î± âˆˆCn the
CRV Î±TZ is proper (Proposition 17.4.2). To this end, ï¬x an arbitrary Î± âˆˆCn.
By Proposition 24.3.3 it follows that the CRV Î±TZ is circularly-symmetric. And
because Z is of ï¬nite variance, so is Î±TZ. Being a circularly-symmetric CRV of
ï¬nite variance, it follows from Section 24.2.3 that Î±TZ must be proper.
24.3.4
Complex Gaussian Vectors
Deï¬nition 24.3.6 (Complex Gaussian Vectors). A complex random n-vector Z is
said to be a complex Gaussian vector if the real random 2n-vector

Re

Z(1)
, . . . , Re

Z(n)
, Im

Z(1)
, . . . , Im

Z(n)T
(24.20)
consisting of the real and imaginary parts of its components is a real Gaussian
vector. A centered complex Gaussian vector is a zero-mean complex Gaussian
vector.
Note that, Theorem 23.6.7 notwithstanding, the distribution of a centered complex
Gaussian vector is not uniquely speciï¬ed by its covariance matrix. It is uniquely
speciï¬ed by the covariance matrix if the Gaussian vector is additionally known to
be proper. This is a direct consequence of the following proposition.
Proposition 24.3.7. The distribution of a centered complex Gaussian vector Z is
uniquely speciï¬ed by the matrices
K = E

ZZâ€ 
and
L = E

ZZT
.
Proof. Let R be the real 2n-vector that results from stacking the real part of Z on
top of its imaginary part as in (24.20). We will prove the proposition by showing
that the matrices K and L uniquely specify the distribution of R.
Since Z is a complex Gaussian n-vector, R is a real Gaussian 2n-vector. Since Z is
of zero mean, so is R. Consequently, the distribution of R is fully characterized by
its covariance matrix E

RRT
(Theorem 23.6.7). The proof will thus be concluded
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.3 Vectors
551
once we show that the matrices L and K determine the covariance matrix of R.
Indeed, as we next verify,
E

RRT
= 1
2
	
Re(K) + Re(L)
Im(L) âˆ’Im(K)
Im(L) + Im(K)
Re(K) âˆ’Re(L)

.
(24.21)
To verify (24.21) one needs to compute each of the block entries separately. We
shall see how this is done by computing the top-right entry. The rest of the entries
are left for the reader to verify.
E
%
Re(Z) Im(Z)T&
= E
5	Z + Zâˆ—
2

 	Z âˆ’Zâˆ—
2i

T6
= E
	Z + Zâˆ—
2

 	ZT âˆ’Zâ€ 
2i


= 1
2
-
E

ZZT
âˆ’E

Zâˆ—Zâ€ 
2i
âˆ’E

ZZâ€ 
âˆ’E

Zâˆ—ZT
2i
.
= 1
2

Im(L) âˆ’Im(K)

.
Corollary 24.3.8. The distribution of a proper complex Gaussian vector is uniquely
speciï¬ed by its covariance matrix.
Proof. Follows from Proposition 24.3.7 by noting that by specifying that a complex
Gaussian is proper we are specifying that the matrix L is zero (Deï¬nition 17.4.1).
Proposition 24.3.9 (Linear Transformations of Complex Gaussians). If Z is a
complex Gaussian n-vector and if A and B are deterministic m Ã— n complex ma-
trices, then the m-vector
AZ + BZâˆ—
is a complex Gaussian.
Proof. Deï¬ne the complex random m-vector C â‰œAZ + BZâˆ—. To prove that C is
Gaussian we recall that linearly transforming a real Gaussian vector yields a real
Gaussian vector (Proposition 23.6.3), and we note that the real random 2m-vector
whose components are the real and imaginary parts of C can be expressed as the
result of applying a linear transformation to the real Gaussian 2n-vector whose
components are the real and imaginary parts of the components of Z:
	
Re(C)
Im(C)

=
	
Re(A) + Re(B)
Im(B) âˆ’Im(A)
Im(A) + Im(B)
Re(A) âˆ’Re(B)

 	
Re(Z)
Im(Z)

.
Proposition 24.3.10 (Characterizing Complex Gaussian Vectors). Each of the
following statements is equivalent to the statement that Z is a complex Gaussian
n-vector.
(a) The real random vector whose 2n components correspond to the real and
imaginary parts of Z is a real Gaussian vector.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

552
Complex Gaussians and Circular Symmetry
(b) For every deterministic vector Î± âˆˆCn, the CRV Î±TZ is a complex Gaussian
random variable.
(c) There exist complex n Ã— m matrices A and B and a vector Î¼ âˆˆCn such that
Z
L= AW + BWâˆ—+ Î¼
for some standard complex Gaussian random m-vector W.
Proof. Statement (a) is just the deï¬nition of a Gaussian complex random vector.
We next prove the equivalence of (a) and (b). That (a) implies (b) follows from
Proposition 24.3.9 (by substituting Î±T for A and 0 for B).
To prove that (b) â‡’(a) it suï¬ƒces (by Deï¬nition 24.3.6 and Theorem 23.6.17) to
show that (b) implies that any real linear functional of the real random 2n-vector
comprising the real and imaginary parts of Z is a real Gaussian random variable,
i.e., that for every choice of the real constants Î±(1), . . . , Î±(n) and Î²(1), . . . , Î²(n) the
random variable
n

j=1
Î±(j) Re

Z(j)
+
n

j=1
Î²(j) Im

Z(j)
(24.22)
is a Gaussian real random variable. To that end we rewrite (24.22) as
n

j=1
Î±(j) Re

Z(j)
+
n

j=1
Î²(j) Im

Z(j)
= Î±T Re

Z

+ Î²T Im

Z

(24.23)
= Re

(Î± âˆ’iÎ²)TZ

,
(24.24)
where we deï¬ne the real vectors Î± and Î² as Î± â‰œ(Î±(1), . . . , Î±(n))T âˆˆRn and
Î² â‰œ(Î²(1), . . . , Î²(n))T âˆˆRn.
Now (b) implies that (Î± âˆ’iÎ²)TZ is a Gaussian
complex random variable, so its real part Re((Î± âˆ’iÎ²)TZ) must be a real Gaus-
sian random variable (Deï¬nition 24.2.10 and Proposition 23.6.6), thus establishing
that (b) implies that (24.22) is a real Gaussian random variable.
We next turn to proving the equivalence of (a) and (c). That (c) implies (a) follows
directly from Proposition 24.3.9 applied to the Gaussian vector W. The proof of
the implication (a) â‡’(c) is very similar to the proof of its scalar version (24.10).
We ï¬rst note that since we can choose Î¼ = E[Z], it suï¬ƒces to prove the result for
the centered case. Now (a) implies that there exist n Ã— n matrices D, E, F, G such
that
	
Re(Z)
Im(Z)

L=
	
D
E
F
G

 	
W1
W2

,
(24.25)
where W1 and W2 are independent real standard Gaussian n-vectors (Deï¬ni-
tion 23.1.1). On the other hand
	Re(AW + BWâˆ—)
Im(AW + BWâˆ—)

=
- Re(A)+Re(B)
âˆš
2
Im(B)âˆ’Im(A)
âˆš
2
Im(B)+Im(A)
âˆš
2
Re(A)âˆ’Re(B)
âˆš
2
. 	âˆš
2 Re(W)
âˆš
2 Im(W)

.
(24.26)
If W is a standard complex Gaussian, then
	âˆš
2 Re(W)
âˆš
2 Im(W)

L=
	
W1
W2

,
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.3 Vectors
553
where W1 and W2 are as above. Consequently, the representations (24.25) and
(24.26) agree if
	
D
E
F
G

=
- Re(A)+Re(B)
âˆš
2
Im(B)âˆ’Im(A)
âˆš
2
Im(B)+Im(A)
âˆš
2
Re(A)âˆ’Re(B)
âˆš
2
.
,
i.e., if we set
A =
1
âˆš
2

(D + G) + i(F âˆ’E)

,
B =
1
âˆš
2

(D âˆ’G) + i(F + E)

.
24.3.5
Proper Complex Gaussian Vectors
A proper complex Gaussian vector is a complex Gaussian vector that is also proper
(Deï¬nition 17.4.1). Thus, Z is a proper complex Gaussian vector if it is a centered
complex Gaussian vector satisfying E

ZZT
= 0.
Recall that, by Proposition 24.3.5, every ï¬nite-variance circularly-symmetric com-
plex random vector is also proper, but that some random vectors are proper and
not circularly-symmetric. We next show that for Gaussian vectors, circular sym-
metry is equivalent to properness. The relationship between circular symmetry,
properness, and Gaussianity is thus as illustrated in Figure 24.2.
Proposition 24.3.11 (For Complex Gaussians, Proper = Circularly-Symmetric).
A complex Gaussian vector is proper if, and only if, it is circularly-symmetric.
Proof. Every circularly-symmetric complex Gaussian is proper, because every com-
plex Gaussian is of ï¬nite variance, and every ï¬nite-variance circularly-symmetric
complex random vector is proper (Proposition 24.3.5).
We now turn to the reverse implication, i.e., that if a complex Gaussian vector
is proper, then it is circularly-symmetric. Assume that Z is a proper Gaussian
n-vector. We will prove that Z is circularly-symmetric using Proposition 24.3.3 by
showing that for every deterministic vector Î± âˆˆCn the random variable Î±TZ is
circularly-symmetric.
To that end, ï¬x some arbitrary Î± âˆˆCn. Since Z is a Gaussian vector, it follows that
Î±TZ is a Gaussian CRV (Proposition 24.3.9 with the substitution of Î±T for A and
0 for B). Moreover, since Z is proper, so is Î±TZ (Proposition 17.4.2). We have thus
established that Î±TZ is a proper Gaussian CRV and hence, by Proposition 24.2.12,
also circularly-symmetric.
We next address the existence of a proper complex Gaussian of a given covariance
matrix. We say that a complex nÃ—n matrix K is complex positive semideï¬nite
and write K âª°0 if Î±â€ KÎ± is a nonnegative real number for every Î± âˆˆCn. The
covariance matrix KZZ (17.36) of every ï¬nite-variance complex random vector Z
is complex positive semideï¬nite, because Î±â€ KÎ± is the variance of Î±â€ Z and is, as
such, nonnegative. An n Ã— n matrix K is complex positive semideï¬nite if, and only
if, there exists a complex n Ã— n matrix S such that K = SSâ€ ; see (Axler, 2015,
Section 7.C, Theorem 7.35).
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

554
Complex Gaussians and Circular Symmetry
random vectors
ï¬nite-variance
proper
circularly symmetric
Gaussian
Figure 24.2: The relationship between circular symmetry, Gaussianity, and proper-
ness. The outer region corresponds to all complex random vectors. Within that is
the set of all vectors whose components are of ï¬nite variance. Within it is the family
of all proper random vectors. The slanted lines indicate the circularly-symmetric
vectors, and the gray area corresponds to the Gaussian vectors. The same relations
hold for scalars and for stochastic processes.
Proposition 24.3.12 (Existence of Proper Gaussians). Given any n Ã— n complex
positive semideï¬nite matrix K, there exists a proper complex Gaussian n-vector
whose covariance matrix is K.
Proof. Since K is positive semideï¬nite, it follows that there exists an nÃ—n matrix S
such that
K = SSâ€ .
(24.27)
Consider now the vector
Z = SW,
(24.28)
where W is a standard complex Gaussian n-vector. We will show that Z has the
desired properties. First, it must be Gaussian because it is the result of applying
a deterministic linear mapping to the Gaussian vector W (Proposition 24.3.9). It
is centered because W is centered (24.15) and because E[SW] = SE[W]. It is
proper because it is the result of linearly transforming the proper complex random
vector W (Proposition 17.4.3 and (24.15)). Finally, its covariance matrix is
E

(SW)(SW)â€ 
= E

SWWâ€ Sâ€ 
= SE

WWâ€ 
Sâ€ 
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.3 Vectors
555
= SInSâ€ 
= K.
Note 24.3.13. Since the distribution of a proper Gaussian complex vector is fully
speciï¬ed by its covariance matrix (Corollary 24.3.8), we can denote the distribution
of a proper Gaussian complex vector of covariance matrix K by
NC(0, K) .
We next show that linearly transforming a proper Gaussian yields a proper Gaus-
sian.
Proposition 24.3.14 (Linearly Transforming Proper Gaussians). Multiplying a
proper complex Gaussian vector by a deterministic matrix yields another proper
complex Gaussian vector.
Proof. Let Z be a circularly-symmetric complex Gaussian n-vector, and let A be
a deterministic m Ã— n complex matrix. Then, being the result of linearly trans-
forming a complex Gaussian vector, AZ is a complex Gaussian m-vector (Propo-
sition 24.3.9). And since Z is proper, so is AZ (Proposition 17.4.3). Thus, AZ is
a proper complex Gaussian vector and hence also circularly-symmetric (Proposi-
tion 24.3.11).
The characteristic function of a proper Gaussian n-vector has a simple form:

Z âˆ¼NC(0, K)

â‡â‡’

Î¦Z(Ï–) = eâˆ’1
4 Ï–â€ KÏ–,
Ï– âˆˆCn
.
(24.29)
To establish (24.29) we shall compute the characteristic function of a NC(0, K)
complex Gaussian n-vector and then invoke Theorem 17.4.4. But we begin with
the simpler computation of the characteristic function of a scalar standard complex
Gaussian W (Deï¬nition 24.2.1).
Starting from the deï¬nition of the characteristic function (Deï¬nition 17.3.3),
Î¦W (Ï–) = E
%
ei Re(Ï–âˆ—W )&
= E
%
ei( Re(Ï–) Re(W )+Im(Ï–) Im(W ))&
= E
%
ei Re(Ï–) Re(W ) ei Im(Ï–) Im(W )&
= E
%
ei Re(Ï–) Re(W )&
E
%
ei Im(Ï–) Im(W )&
= exp

âˆ’1
4

Re(Ï–)
2
exp

âˆ’1
4

Im(Ï–)
2
= exp

âˆ’1
4|Ï–|2
,
Ï– âˆˆC,
(24.30)
where the fourth equality holds because the real and imaginary parts of a stan-
dard complex Gaussian are independent (Deï¬nition 24.2.1); the ï¬fth because they
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

556
Complex Gaussians and Circular Symmetry
are both N(0, 1/2) and by recalling the characteristic function of real Gaussians
(19.29); and the sixth holds by the deï¬nition of the modulus of a complex number.
This result extends easily to circularly-symmetric Gaussian CRVs of general vari-
ance Ïƒ2. Indeed, if Z âˆ¼NC

0, Ïƒ2
, then Z has the same law as ÏƒW, where W is
NC(0, 1) (Proposition 24.3.14 and Note 24.3.13). Consequently,
Î¦Z(Ï–) = E
%
ei Re(Ï–âˆ—Z)&
= E
%
ei Re(Ï–âˆ—ÏƒW )&
= Î¦W (ÏƒÏ–)
= exp

âˆ’1
4Ïƒ2|Ï–|2
,
Ï– âˆˆC, Z âˆ¼NC

0, Ïƒ2
,
where the second equality holds because Z and ÏƒW are of the same law, and the
fourth by (24.30). We have thus established that
	
Z âˆ¼NC

0, Ïƒ2 
=â‡’
	
Î¦Z(Ï–) = eâˆ’1
4 Ïƒ2|Ï–|2,
Ï– âˆˆC

.
(24.31)
Suppose now that the complex n-vector Z is NC(0, K), where K is some complex
positive semideï¬nite matrix. Let Ï– be any deterministic complex n-vector. By
Proposition 24.3.14, Ï–â€ Z is a proper (scalar) Gaussian CRV. Its variance is
E

|Ï–â€ Z|2
= E

(Ï–â€ Z)(Ï–â€ Z)â€ 
= E

Ï–â€ ZZâ€ Ï–

= Ï–â€ E

ZZâ€ 
Ï–
= Ï–â€ KÏ–.
Thus, Ï–â€ Z âˆ¼NC

0, Ï–â€ KÏ–

, and hence, by (24.31),
E
%
ei Re(Ï–â€ Z)&
= Î¦Ï–â€ Z(1)
= exp

âˆ’1
4Ï–â€ KÏ–

,
which establishes (24.29).
Like their real counterparts (Theorem 23.6.14) proper Gaussians have a natural
canonical representation:
Theorem 24.3.15 (Canonical Representation of Proper Complex Gaussian).
Let Z be a proper complex Gaussian n-vector of covariance matrix KZZ. Then
Z
L= UÎ›1/2W,
where W is a standard complex Gaussian n-vector; the n Ã— n matrix U is unitary;
the n Ã— n matrix Î› is diagonal; the diagonal elements of Î› are the eigenvalues of
KZZ; and the j-th column of U is an eigenvector corresponding to the eigenvalue of
KZZ that is equal to the j-th diagonal element of Î›.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.3 Vectors
557
Proof. Let Z âˆ¼NC(0, K) be a complex n-vector, i.e., a circularly-symmetric com-
plex Gaussian n-vector of covariance matrix K. Being a covariance matrix, K must
be complex positive semideï¬nite. As such, it has a spectral representation analo-
gous to the one described for real matrices in Proposition 23.3.3:
K = UÎ›Uâ€ ,
where U is a unitary n Ã— n matrix (UUâ€  = Uâ€ U = In), and Î› is an n Ã— n diagonal
matrix whose diagonal elements Î»1, . . . , Î»n are nonnegative.
Let W be a standard complex Gaussian n-vector. We next argue that
Z
L= UÎ›1/2W,
where Î›1/2 is a diagonal matrix whose diagonal entries are the square roots of
those of Î›, i.e., Î»1/2
1
, . . . , Î»1/2
n . To this end note that because W is standard, it
is proper (24.15). Consequently, UÎ›1/2W must also be proper because applying
a linear transformation to a proper random vector yields a proper random vector
(Proposition 17.4.3). And UÎ›1/2W is Gaussian by Proposition 24.3.9. Thus, both
Z and UÎ›1/2W are circularly-symmetric complex Gaussian vectors. To establish
that they are of the same law it thus suï¬ƒces to establish that they have iden-
tical covariance matrices (Corollary 24.3.8). This is indeed the case because the
covariance matrix of UÎ›1/2W is given by
E
%
UÎ›1/2W

UÎ›1/2W
â€ &
= E

UÎ›1/2WWâ€ Î›1/2Uâ€ 
= UÎ›1/2 E

WWâ€ 
Î›1/2Uâ€ 
= UÎ›1/2InÎ›1/2Uâ€ 
= UÎ›Uâ€ 
= K.
We conclude by computing the density of proper Gaussians. We only consider those
that have a nonsingular covariance matrix because otherwise the density does not
exist.
Proposition 24.3.16 (The Density of a Proper Gaussian). If Z is a proper complex
Gaussian n-vector of nonsingular covariance matrix K, then its density fZ is
fZ(z) =
1
Ï€n det K eâˆ’zâ€ Kâˆ’1z,
z âˆˆCn.
(24.32)
Proof. To compute the density we use (24.27)â€“(24.28) along with the change-of-
variables formula (Lemma 17.4.6) and the density of a standard Gaussian complex
random vector (24.14) to obtain
fZ(z) =
1
|det S|2 fW(Sâˆ’1z)
=
1
Ï€n det(SSâ€ ) eâˆ’(Sâˆ’1z)â€ Sâˆ’1z
=
1
Ï€n det K eâˆ’zâ€ Kâˆ’1z,
z âˆˆCn.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

558
Complex Gaussians and Circular Symmetry
24.4
Exercises
Exercise 24.1 (The Complex Conjugate of a Circularly-Symmetric CRV). Must the com-
plex conjugate of a circularly-symmetric CRV be circularly-symmetric?
Exercise 24.2 (Scaled Circularly-Symmetric CRV). Show that if Z is circularly-symmetric
and if Î± âˆˆC is deterministic, then the distribution of Î±Z depends on Î± only via its
magnitude |Î±|.
Exercise 24.3 (The n-th Power of a Circularly-Symmetric CRV). Show that if Z is a
circularly-symmetric CRV and if n is a positive integer, then Zn is circularly-symmetric.
Exercise 24.4 (The Characteristic Function of Circularly-Symmetric CRVs). Show that a
CRV Z is circularly-symmetric if, and only if, its characteristic function Î¦Z(Â·) is radially-
symmetric in the sense that Î¦Z(Ï–) depends on Ï– only via its magnitude |Ï–|.
Exercise 24.5 (Multiplying Independent CRVs). Show that the product of two indepen-
dent complex random variables is circularly-symmetric whenever (at least) one of them
is circularly-symmetric.
Exercise 24.6 (The Complex Conjugate of a Gaussian CRV). Must the complex conjugate
of a Gaussian CRV be Gaussian?
Exercise 24.7 (Independent Components). Show that if the complex random variables
W and Z are circularly-symmetric and independent, then the random vector (W, Z)T is
circularly-symmetric.
Exercise 24.8 (Constructing a Circularly-Symmetric Complex Random Vector). Show
that if Z is a complex random vector and if Î˜ is uniformly distributed over the interval
[âˆ’Ï€, Ï€) independently of Z, then eiÎ˜ Z is circularly-symmetric.
Exercise 24.9 (Rotating a Complex Gaussian). Let Z be a centered complex Gaussian
(not necessarily circularly-symmetric), and let Î¦ be independent of it and uniformly
distributed over the interval [âˆ’Ï€, Ï€). Must eiÎ¦Z be a complex Gaussian?
Hint: Consider the case where the imaginary part of Z is deterministically zero, and check
whether the distribution of the squared-magnitude of eiÎ¦Z is the same as for a circularly-
symmetric Gaussian.
Exercise 24.10 (The Squared Euclidean Norm of Complex Gaussian Vectors). Show that
the sum of the squared magnitudes of the components of a NC(0, K) complex random
vector can be written as a sum of independent exponentials of possibly diï¬€erent means.
Hint: Use the canonical representation of Theorem 24.3.15.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,

24.4 Exercises
559
Exercise 24.11 (Jointly Circularly-Symmetric Complex Gaussians). As in Deï¬nition 23.7.1,
we can also deï¬ne jointly complex Gaussians and jointly circularly-symmetric complex
Gaussians. Extend the results of Section 23.7 by showing:
(i) Two centered jointly complex Gaussian vectors Z1 and Z2 are independent if, and
only if, they satisfy
E
$
Z1Zâ€ 
2
%
= 0 and E
$
Z1ZT
2
%
= 0.
(ii) Two jointly circularly-symmetric complex Gaussian vectors Z1 and Z2 are indepen-
dent if, and only if, they satisfy
E
$
Z1Zâ€ 
2
%
= 0.
(iii) If Z1, Z2 are centered jointly complex Gaussians, then, conditional on Z2 = z2, the
complex random vector Z1 is a complex Gaussian such that
E
+
Z1 âˆ’E[Z1 |Z2 = z2]

Z1 âˆ’E[Z1 |Z2 = z2]
â€   Z2 = z2
,
and
E
+
Z1 âˆ’E[Z1 |Z2 = z2]

Z1 âˆ’E[Z1 |Z2 = z2]
T  Z2 = z2
,
do not depend on z2 and such that the conditional mean E[Z1 |Z2 = z2] can be
expressed as Az2 + Bzâˆ—
2 for some matrices A and B that do not depend on z2.
(iv) If Z1, Z2 are jointly circularly-symmetric complex Gaussians, then, conditional on
Z2 = z2, the complex random vector Z1 is a circularly-symmetric complex Gaussian
of a covariance matrix that does not depend on z2 and of a mean that can be
expressed as Az2 for some matrix A that does not depend on z2.
Exercise 24.12 (Limits of Complex Gaussians). Extend the deï¬nition of almost-sure con-
vergence (23.79) to complex random vectors, and show that if the complex Gaussian
d-vectors Z1, Z2, . . . converge to Z almost surely, then Z must be a complex Gaussian.
Exercise 24.13 (Limits of Circularly-Symmetric Complex Random Variables). Consider
a sequence Z1, Z2, . . . of circularly-symmetric complex random variables that converges
almost surely to the CRV Z. Show that Z must be circularly-symmetric. Extend this
result to complex random vectors.
Hint: Consider the characteristic functions of Z, Z1, Z2, . . ., and recall the proof of Theo-
rem 19.9.1.
Exercise 24.14 (Limits of Circularly-Symmetric Complex Gaussians). Let Z1, Z2, . . . be
a sequence of circularly-symmetric complex Gaussians that converges almost surely to
the CRV Z. Show that Z must be a circularly-symmetric Gaussian. Extend to complex
random vectors.
Hint: Either combine Exercises 24.12 & 24.13 or prove directly using the characteristic
function as in the proof of Theorem 19.9.1.
available at 
.026
14:53:37, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 25
Continuous-Time Stochastic Processes
25.1
Notation
Recall from Section 12.2 that a continuous-time stochastic process

X(t), t âˆˆR

is a family of random variables that are deï¬ned on a common probability space
(Î©, F, P) and that are indexed by the real line (time). We denote by X(t) the
time-t sample of

X(t), t âˆˆR

, i.e., the random variable to which t is mapped
(the RV indexed by t). This RV is sometimes also called the state at time t.
Rather than writing

X(t), t âˆˆR

, we sometimes denote the SP by

X(t)

or
by X. Perhaps the clearest way to denote the process is as a mapping:
X: Î© Ã— R â†’R,
(Ï‰, t) 	â†’X(Ï‰, t).
For a ï¬xed t âˆˆR, the time-t sample X(t) is the mapping X(Â·, t) from Î© to the real
line, i.e., the RV Ï‰ 	â†’X(Ï‰, t) indexed by t. If we ï¬x Ï‰ âˆˆÎ© and view X(Ï‰, Â·) as a
mapping t 	â†’X(Ï‰, t), then we obtain a function of time. This function is called a
trajectory, sample-path, path, sample-function, or realization.
Ï‰ 	â†’X(Ï‰, t)
time-t sample for a ï¬xed t âˆˆR
(random variable)
t 	â†’X(Ï‰, t)
trajectory for a ï¬xed Ï‰ âˆˆÎ©
(function of time)
Recall also from Section 12.2 that the process is centered if for every t âˆˆR the
RV X(t) is of zero mean. It is of ï¬nite variance if for every t âˆˆR the RV X(t)
is of ï¬nite variance.
25.2
The Finite-Dimensional Distributions
The ï¬nite-dimensional distributions (FDDs) of a continuous-time SP

X(t)

are all
the joint distributions of n-tuples of the form (X(t1), . . . , X(tn)), where n can be
any positive integer and t1, . . . , tn âˆˆR are arbitrary epochs. To specify the FDDs
of a SP

X(t)

one must thus specify, for every n âˆˆN and for every choice of
the epochs t1, . . . , tn âˆˆR, the distribution of the n-tuple

X(t1), . . . , X(tn)

. This
560
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.2 The Finite-Dimensional Distributions
561
is a conceptually clear if formidable task. We denote the cumulative distribution
function of the n-tuple (X(t1), . . . , X(tn)) by
Fn

Î¾1, . . . , Î¾n; t1, . . . , tn

â‰œPr

X(t1) â‰¤Î¾1, . . . , X(tn) â‰¤Î¾n

.
We next show that the FDDs of every SP

X(t)

must satisfy two key properties:
the symmetry property and the consistency property. The symmetry property
is that Fn(Â·; Â·) is unaltered when we simultaneously permute its right arguments
(the tâ€™s) and its left arguments (the Î¾â€™s) by the same permutation. That is, for
every n âˆˆN; every choice of the epochs t1, . . . , tn âˆˆR; every Î¾1, . . . , Î¾n âˆˆR; and
every permutation Ï€ on {1, . . . , n}
Fn

Î¾Ï€(1), . . . , Î¾Ï€(n); tÏ€(1), . . . , tÏ€(n)

= Fn

Î¾1, . . . , Î¾n; t1, . . . , tn

.
(25.1)
This property is a generalization to n-tuples of the obvious fact that if X and Y are
random variables, then Pr[X â‰¤x, Y â‰¤y] = Pr[Y â‰¤y, X â‰¤x] for every x, y âˆˆR.
The consistency property is that whenever n âˆˆN and t1, . . . , tn, Î¾1, . . . , Î¾n âˆˆR,
lim
Î¾nâ†’âˆFn

Î¾1, . . . , Î¾nâˆ’1, Î¾n; t1, . . . , tnâˆ’1, tn

= Fnâˆ’1

Î¾1, . . . , Î¾nâˆ’1; t1, . . . , tnâˆ’1

.
(25.2)
This property is a consequence of the fact that the set

Ï‰ âˆˆÎ© : X(Ï‰, t1) â‰¤Î¾1, . . . , X(Ï‰, tnâˆ’1) â‰¤Î¾nâˆ’1, X(Ï‰, tn) â‰¤Î¾n

is increasing in Î¾n and converges as Î¾n tends to inï¬nity to the set

Ï‰ âˆˆÎ© : X(Ï‰, t1) â‰¤Î¾1, . . . , X(Ï‰, tnâˆ’1) â‰¤Î¾nâˆ’1

.
The key result on the existence of stochastic processes of given FDDs is Kol-
mogorovâ€™s Existence Theorem, which states that the symmetry and consistency
properties suï¬ƒce for a family of ï¬nite-dimensional distributions to correspond to
the FDDs of some SP.
Theorem 25.2.1 (Kolmogorovâ€™s Existence Theorem). Let G1(Â·; Â·), G2(Â·; Â·), . . . be
a sequence of functions Gn : Rn Ã— Rn â†’[0, 1] satisfying the following properties:
1) For every n â‰¥1 and every t1, . . . , tn âˆˆR, the function Gn(Â·; t1, . . . , tn) is a
valid joint distribution function.1
2) The symmetry property
Gn

Î¾Ï€(1), . . . , Î¾Ï€(n); tÏ€(1), . . . , tÏ€(n)

= Gn

Î¾1, . . . , Î¾n; t1, . . . , tn

,
t1, . . . , tn, Î¾1, . . . , Î¾n âˆˆR, Ï€ a permutation on {1, . . . , n}.
(25.3)
1 A function F : Rn â†’[0, 1] is a valid joint distribution function if there exist random variables
X1, . . . , Xn whose joint distribution function is F(Â·), i.e.,
Pr[X1 â‰¤Î¾1, . . . , Xn â‰¤Î¾n] = F(Î¾1, . . . , Î¾n),
Î¾1, . . . , Î¾n âˆˆR.
Not every function F : Rn â†’[0, 1] is a valid joint distribution function. For example, a valid joint
distribution function must be monotonic in each variable. See, for example, (Billingsley, 1995,
Theorem 12.5) for a characterization of joint distribution functions.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

562
Continuous-Time Stochastic Processes
3) The consistency property
lim
Î¾nâ†’âˆGn(Î¾1, . . . , Î¾nâˆ’1, Î¾n; t1, . . . , tnâˆ’1, tn)
= Gnâˆ’1(Î¾1, . . . , Î¾nâˆ’1; t1, . . . , tnâˆ’1),
t1, . . . , tn, Î¾1, . . . , Î¾nâˆ’1 âˆˆR.
(25.4)
Then there exists a SP

X(t)

whose FDDs are given by {Gn(Â·; Â·)} in the sense
that
Pr

X(t1) â‰¤Î¾1, . . . , X(tn) â‰¤Î¾n

= Gn

Î¾1, . . . , Î¾n; t1, . . . , tn

for every n âˆˆN, all t1, . . . , tn âˆˆR, and all Î¾1, . . . , Î¾n âˆˆR.
Proof. See, for example, (Billingsley, 1995, Chapter 7, Section 36), (CramÂ´er and
Leadbetter, 2004, Section 3.3), (Grimmett and Stirzaker, 2001, Section 8.6), or
(Doob, 1990, Chapter I Â§ 5).
In the study of n-tuples of random variables we can use the joint distribution
function to answer, at least in principle, most of our probability questions. When it
comes to stochastic processes, however, there are interesting questions that cannot
be answered using the FDDs. For example, it can be shown that the probability
of the event that the SP

X(t)

produces a sample-path that is continuous at time
zero cannot be computed from the FDDs. This is not due to our limited analytic
capabilities but rather because there exist two stochastic processes of identical
FDDs where for one process this event is of zero probability whereas for the other
it is of probability one (CramÂ´er and Leadbetter, 2004, Section 3.6). Fortunately,
most of the questions of interest to us in Digital Communications can be answered
based on the FDDs.
Another example is a very subtle question related to measurability. From the FDDs
alone one cannot determine whether the trajectories are measurable functions of
time, i.e., whether it makes sense to talk about integrals of the form
 âˆ
âˆ’âˆx(Ï‰, t) dt.
This issue will be revisited in Section 25.9.
The above discussion motivates us to deï¬ne the set of events whose probability
can be determined from the FDDs using the axioms of probability, i.e., using the
rules that the probability of the set of all possible outcomes Î© is one and that
the probability of a countable union of disjoint events is the inï¬nite sum of the
probabilities of the events. In the mathematical literature what we are deï¬ning is
called the Ïƒ-algebra generated by

X(t), t âˆˆR

or the Ïƒ-algebra generated
by the cylindrical sets of

X(t), t âˆˆR

.2 For the classical deï¬nition see, for
example, (Billingsley, 1995, Section 36).
Deï¬nition 25.2.2 (Ïƒ-Algebra Generated by a SP). The Ïƒ-algebra generated
by a SP

X(t), t âˆˆR

which is deï¬ned over the probability space (Î©, F, P) is
the set of events (i.e., elements of F) whose probability can be computed from the
FDDs of

X(t)

using only the axioms of probability.
2It is the smallest Ïƒ-algebra with respect to which all the random variables

X(t), t âˆˆR

are
measurable.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.3 Deï¬nition of a Gaussian SP
563
We now rephrase our previous statement about continuity as saying that the set
of Ï‰ âˆˆÎ© for which the function t 	â†’X(Ï‰, t) is continuous at t = 0 is not in the
Ïƒ-algebra generated by

X(t)

. The probability of such sets cannot be inferred
from the FDDs alone. If such sets are assigned a probability, it must be based on
some additional information that is not captured by the FDDs.
The FDDs provide a natural way to deï¬ne independence between stochastic pro-
cesses.
Deï¬nition 25.2.3 (Independent Stochastic Processes). Two stochastic processes

X(t)

and

Y (t)

, deï¬ned on the same probability space (Î©, F, P), are said to
be independent stochastic processes if for every n âˆˆN and any choice of the
epochs t1, . . . , tn âˆˆR, the n-tuples (X(t1), . . . , X(tn)) and (Y (t1), . . . , Y (tn)) are
independent.
25.3
Deï¬nition of a Gaussian SP
By far the most important processes for modeling noise in Digital Communications
are the Gaussian processes. Fortunately, these processes are among the mathemat-
ically most tractable. The deï¬nition of a Gaussian SP builds on that of a Gaussian
vector (Deï¬nition 23.1.1).
Deï¬nition 25.3.1 (Gaussian Stochastic Processes). A SP

X(t)

is said to be a
Gaussian stochastic process if, for every n âˆˆN and every choice of the epochs
t1, . . . , tn âˆˆR, the random vector (X(t1), . . . , X(tn))T is Gaussian.
Note 25.3.2. Gaussian stochastic processes are of ï¬nite variance.
Proof. If

X(t)

is a Gaussian process, then a fortiori at each epoch t âˆˆR, the
random variable X(t) is a univariate Gaussian (choose n = 1 in the above deï¬ni-
tion) and hence, by the deï¬nition of the univariate distribution (Deï¬nition 19.3.1),
of ï¬nite variance.
One of the things that make Gaussian processes tractable is the ease with which
their FDDs can be speciï¬ed.
Proposition 25.3.3 (The FDDs of a Gaussian SP). If

X(t)

is a centered Gaus-
sian SP, then all its FDDs are determined by the mapping that speciï¬es the covari-
ance between any two of its samples:
(t1, t2) 	â†’Cov

X(t1), X(t2)

,
t1, t2 âˆˆR.
(25.5)
Proof. Let

X(t)

be a centered Gaussian SP. We shall show that, for any choice of
the epochs t1, . . . , tn âˆˆR, we can compute the joint distribution of X(t1), . . . X(tn)
from the mapping (25.5). To this end we note that since

X(t)

is a Gaussian
SP, the random vector (X(t1), . . . X(tn))T is Gaussian (Deï¬nition 25.3.1). Conse-
quently, its distribution is fully speciï¬ed by its mean vector and covariance matrix
(Theorem 23.6.7). Its mean vector is zero, because we assumed that

X(t)

is cen-
tered. To conclude the proof we thus only need to show that the covariance matrix
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

564
Continuous-Time Stochastic Processes
of (X(t1), . . . X(tn))T is determined by the mapping (25.5). But this is obvious
because the covariance matrix of (X(t1), . . . X(tn))T is the n Ã— n matrix
â›
âœ
âœ
âœ
âœ
â
Cov[X(t1), X(t1)]
Cov[X(t1), X(t2)]
Â· Â· Â·
Cov[X(t1), X(tn)]
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Cov[X(tn), X(t1)]
Cov[X(tn), X(t2)]
Â· Â· Â·
Cov[X(tn), X(tn)]
â
âŸ
âŸ
âŸ
âŸ
â 
,
(25.6)
and each of the entries in this matrix is speciï¬ed by the mapping (25.5).
Things become even simpler if the Gaussian process is wide-sense stationary
(Deï¬nition 25.4.2 ahead). In this case the RHS of (25.5) is determined by t1 âˆ’t2,
so the mapping (25.5) (and hence all the FDDs) is determined by the mapping
Ï„ 	â†’Cov[X(t), X(t + Ï„)]. But before discussing wide-sense stationary Gaussian
stochastic processes in Section 25.5, we ï¬rst deï¬ne stationarity and wide-sense
stationarity for general processes that are not necessarily Gaussian.
25.4
Stationary Continuous-Time Processes
With an eye to deï¬ning stationarity for continuous-time stochastic processes, we
deï¬ne the time shift by Ï„ of a SP X: (Ï‰, t) 	â†’X(Ï‰, t) as the SP
(Ï‰, t) 	â†’X(Ï‰, t âˆ’Ï„).
(25.7)
A SP is stationary if its time shifts all have the same FDDs. More formally, we
have the following continuous-time analogue of Deï¬nition 13.2.1:
Deï¬nition 25.4.1 (Stationary Continuous-Time SP). We say that a continuous-
time SP

X(t)

is stationary (or strict sense stationary, or strongly sta-
tionary) if for every n âˆˆN, any epochs t1, . . . , tn âˆˆR, and every Ï„ âˆˆR,

X(t1 + Ï„), . . . , X(tn + Ï„)
 L=

X(t1), . . . , X(tn)

.
(25.8)
By considering the case where n = 1 we obtain that if

X(t)

is stationary, then
all its samples have the same distribution
X(t)
L= X(t + Ï„),
t, Ï„ âˆˆR.
(25.9)
That is, the distribution of the random variable X(t) does not depend on t. By
considering n = 2 we obtain that if

X(t)

is stationary, then the joint distribution
of any two of its samples depends on how far apart they are and not on the absolute
time at which they are taken

X(t1), X(t2)
 L=

X(t1 + Ï„), X(t2 + Ï„)

,
t1, t2, Ï„ âˆˆR.
(25.10)
That is, the joint distribution of

X(t1), X(t2)

can be computed from t2 âˆ’t1.
As we did for discrete-time processes (Deï¬nition 13.3.1), we can also deï¬ne wide-
sense stationarity of continuous-time processes. Recall that a process

X(t)

is
said to be of ï¬nite variance if at every time t âˆˆR the random variable X(t) is of
ï¬nite variance.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.4 Stationary Continuous-Time Processes
565
Deï¬nition 25.4.2 (Wide-Sense Stationary Continuous-Time SP). A continuous-
time SP

X(t)

is said to be wide-sense stationary (or weakly stationary or
second-order stationary) if the following three conditions are met:
1) It is of ï¬nite variance.
2) Its mean is constant
E

X(t)

= E

X(t + Ï„)

,
t, Ï„ âˆˆR.
(25.11)
3) The covariance between its samples satisï¬es
Cov

X(t1), X(t2)

= Cov

X(t1 + Ï„), X(t2 + Ï„)

,
t1, t2, Ï„ âˆˆR.
(25.12)
By considering the case where t1 = t2 in (25.12), we obtain that all the samples of
a WSS SP have the same variance:
Var

X(t)

= Var

X(0)

,
t âˆˆR.
(25.13)
Note 25.4.3. Every ï¬nite-variance stationary SP is WSS.
Proof. This follows because (25.9) implies (25.11), and because (25.10) implies
(25.12).
The reverse is not true: some WSS processes are not stationary.
(Wide-sense
stationarity concerns only means and covariances, whereas stationarity has to do
with distributions.)
The following deï¬nition of the autocovariance function of a continuous-time WSS
SP is the analogue of Deï¬nition 13.5.1.
Deï¬nition 25.4.4 (Autocovariance Function). The autocovariance function
KXX : R â†’R of a WSS continuous-time SP

X(t)

is deï¬ned for every Ï„ âˆˆR by
KXX(Ï„) â‰œCov

X(t + Ï„), X(t)

,
(25.14)
where the RHS does not depend on t because

X(t)

is assumed to be WSS.
By evaluating (25.14) at Ï„ = 0 and using (25.13), we can express the variance
of X(t) in terms of the autocovariance function KXX as
Var

X(t)

= KXX(0),
t âˆˆR.
(25.15)
We end this section with a few simple inequalities related to WSS stochastic pro-
cesses and their autocovariance functions.
Lemma 25.4.5. Let

X(t)

be a WSS SP of autocovariance function KXX. Then
KXX(Ï„)
 â‰¤KXX(0),
Ï„ âˆˆR,
(25.16)
E

|X(t)|

â‰¤
3
KXX(0) + E[X(0)]2,
t âˆˆR,
(25.17)
and
E
X(t) X(tâ€²)

â‰¤KXX(0) + E[X(0)]2 ,
t, tâ€² âˆˆR.
(25.18)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

566
Continuous-Time Stochastic Processes
Proof. Inequality (25.16) follows from the Covariance Inequality (Corollary 3.5.2):
|KXX(Ï„)| =
Cov[X(t + Ï„), X(t)]

â‰¤

Var[X(t + Ï„)]

Var[X(t)]
= KXX(0),
where the last equality follows from (25.15).
Inequality (25.17) follows from the nonnegativity of the variance of |X(t)| and the
assumption that

X(t)

is WSS:
0 â‰¤Var[|X(t)|]
= E

X2(t)

âˆ’

E[|X(t)|]
2
= Var[X(t)] +

E[X(t)]
2 âˆ’

E[|X(t)|]
2
= KXX(0) +

E[X(0)]
2 âˆ’

E[|X(t)|]
2.
Finally, Inequality (25.18) follows from the Cauchy-Schwarz Inequality for random
variables (Theorem 3.5.1)
E[UV ]
 â‰¤

E[U 2] E[V 2]
by substituting |X(t)| for U and |X(tâ€²)| for V and by noting that
E

|X(t)|2
= E

X2(t)

= Var

X(t)

+

E[X(t)]
2
= KXX(0) +

E[X(0)]
2,
t âˆˆR.
25.5
Stationary Gaussian Stochastic Processes
For Gaussian stochastic processes we do not distinguish between stationarity and
wide-sense stationarity. The reason is that, while for general processes the two
concepts are diï¬€erent (in that every ï¬nite-variance stationary SP is WSS, but not
every WSS SP is stationary), for Gaussian stochastic processes the two concepts are
equivalent. These relationships between stationarity and wide-sense stationarity for
general stochastic processes and for Gaussian stochastic processes are illustrated
in Figure 25.1.
Proposition 25.5.1 (Stationary Gaussian Stochastic Processes).
(i) A Gaussian SP is stationary if, and only if, it is WSS.
(ii) The FDDs of a centered stationary Gaussian SP are fully speciï¬ed by its
autocovariance function.
Proof. We begin by proving (i). One direction has only little to do with Gaus-
sianity. Since every Gaussian SP is of ï¬nite variance (Note 25.3.2), and since every
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.5 Stationary Gaussian Stochastic Processes
567
stochastic processes
ï¬nite-variance
WSS
(strictly) stationary
Gaussian
Figure 25.1: The relationship between wide-sense stationarity, Gaussianity, and
strict-sense stationarity. The outer region corresponds to all stochastic processes.
Within it is the set of all ï¬nite-variance processes and within that the set of all wide-
sense stationary processes. The slanted lines indicate the strict-sense stationary
processes, and the gray area corresponds to the Gaussian stochastic processes. Not
all strict-sense stationary SPs are WSS, but those of ï¬nite variance are.
ï¬nite-variance stationary SP is WSS (Note 25.4.3), it follows that every stationary
Gaussian SP is WSS.
Gaussianity plays a much more important role in the proof of the reverse direction,
namely, that every WSS Gaussian SP is stationary. We prove this by showing that
if

X(t)

is Gaussian and WSS, then for every n âˆˆN and any t1, . . . , tn, Ï„ âˆˆR
the joint distribution of X(t1), . . . , X(tn) is identical to the joint distribution of
X(t1 + Ï„), . . . , X(tn + Ï„). To this end, let n âˆˆN and t1, . . . , tn, Ï„ âˆˆR be ï¬xed.
Because

X(t)

is Gaussian, (X(t1), . . . , X(tn))T and (X(t1 + Ï„), . . . , X(tn + Ï„))T
are both Gaussian vectors (Deï¬nition 25.3.1). And since

X(t)

is WSS, the two
are of the same mean vector (see (25.11)). The formerâ€™s covariance matrix is
â›
âœ
âœ
âœ
âœ
â
Cov[X(t1), X(t1)]
Â· Â· Â·
Cov[X(t1), X(tn)]
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Cov[X(tn), X(t1)]
Â· Â· Â·
Cov[X(tn), X(tn)]
â
âŸ
âŸ
âŸ
âŸ
â 
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

568
Continuous-Time Stochastic Processes
and the latterâ€™s is
â›
âœ
âœ
âœ
âœ
â
Cov[X(t1 + Ï„), X(t1 + Ï„)]
Â· Â· Â·
Cov[X(t1 + Ï„), X(tn + Ï„)]
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Cov[X(tn + Ï„), X(t1 + Ï„)]
Â· Â· Â·
Cov[X(tn + Ï„), X(tn + Ï„)]
â
âŸ
âŸ
âŸ
âŸ
â 
.
Since

X(t)

is WSS, the two covariance matrices are identical (see (25.12)). But
two Gaussian vectors of equal mean vectors and of equal covariance matrices have
identical distributions (Theorem 23.6.7), so the distribution of (X(t1), . . . , X(tn))T
is identical to that of (X(t1+Ï„), . . . , X(tn+Ï„))T. Since this has been established for
all choices of n âˆˆN and all choices of t1, . . . , tn, Ï„ âˆˆR, the SP

X(t)

is stationary.
Part (ii) follows from Proposition 25.3.3 and the deï¬nition of wide-sense stationar-
ity. Indeed, by Proposition 25.3.3, all the FDDs of a centered Gaussian SP

X(t)

are determined by the mapping (25.5). If

X(t)

is additionally WSS, then the
RHS of (25.5) can be computed from t1 âˆ’t2 and is given by KXX(t1 âˆ’t2), so the
mapping (25.5) is fully speciï¬ed by the autocovariance function KXX.
25.6
Properties of the Autocovariance Function
Many of the deï¬nitions and results on continuous-time WSS stochastic processes
have analogous discrete-time counterparts. But some technical issues are encoun-
tered only in continuous time. For example, most results on continuous-time WSS
stochastic processes require that the autocovariance function of the process be
continuous at the origin, i.e., satisfy
lim
Î´â†’0 KXX(Î´) = KXX(0),
(25.19)
and this condition has no discrete-time counterpart. As we next show, this condi-
tion is equivalent to the condition
lim
Î´â†’0 E
%
X(t + Î´) âˆ’X(t)
2&
= 0,
t âˆˆR.
(25.20)
This equivalence follows from the identity
E
%
X(t) âˆ’X(t + Î´)
2&
= 2

KXX(0) âˆ’KXX(Î´)

,
t, Î´ âˆˆR,
(25.21)
which can be proved as follows. We ï¬rst note that it suï¬ƒces to prove it for centered
processes, and for such processes we then compute:
E
%
X(t) âˆ’X(t + Î´)
2&
= E

X2(t) âˆ’2X(t) X(t + Î´) + X2(t + Î´)

= E

X2(t)

âˆ’2 E

X(t) X(t + Î´)

+ E

X2(t + Î´)

= KXX(0) âˆ’2 KXX(Î´) + KXX(0)
= 2

KXX(0) âˆ’KXX(Î´)

,
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.6 Properties of the Autocovariance Function
569
where the ï¬rst equality follows by opening the square; the second by the linearity of
expectation; the third by the deï¬nition of KXX; and the ï¬nal equality by collecting
terms.
We note here that if the autocovariance function of a WSS process is continuous
at the origin, then it is continuous everywhere. In fact, it is uniformly continuous:
Lemma 25.6.1. If the autocovariance function of a WSS continuous-time SP is
continuous at the origin, then it is a uniformly continuous function.
Proof. We ï¬rst note that it suï¬ƒces to prove the lemma for centered processes.
Let

X(t)

be such a process. For every Ï„, Î´ âˆˆR we then have3
KXX(Ï„ + Î´) âˆ’KXX(Ï„)
 =
E[X(Ï„ + Î´) X(0)] âˆ’E[X(Ï„) X(0)]

=
E

X(Ï„ + Î´) âˆ’X(Ï„)

X(0)

=
Cov

X(Ï„ + Î´) âˆ’X(Ï„), X(0)

â‰¤
4
E
%
X(Ï„ + Î´) âˆ’X(Ï„)
2& 
E[X2(0)]
=
3
2

KXX(0) âˆ’KXX(Î´)
 
KXX(0)
=
3
2 KXX(0)

KXX(0) âˆ’KXX(Î´)

,
(25.22)
where the equality in the ï¬rst line follows from the deï¬nition of the autocovariance
function because

X(t)

is centered; the equality in the second line by the linearity
of expectation; the equality in the third line by the deï¬nition of the covariance
between two zero-mean random variables; the inequality in the fourth line by the
Covariance Inequality (Corollary 3.5.2); the equality in the ï¬fth line by (25.21);
and the ï¬nal equality by trivial algebra.
The uniform continuity of KXX now
follows from (25.22) by noting that its RHS does not depend on Ï„ and that, by our
assumption about the continuity of KXX at zero, it tends to zero as Î´ â†’0.
We next derive two important properties of autocovariance functions and then
demonstrate in Theorem 25.6.2 that these properties characterize those functions
that can arise as the autocovariance functions of a WSS SP. These properties are
the continuous-time analogues of (13.12) & (13.13), and the proofs are almost
identical. We ï¬rst state the properties and then proceed to prove them.
The ï¬rst property is that the autocovariance function KXX of any continuous-time
WSS process

X(t)

is a symmetric function
KXX(âˆ’Ï„) = KXX(Ï„),
Ï„ âˆˆR.
(25.23)
The second is that it is a positive deï¬nite function in the sense that for every
n âˆˆN, and for every choice of the coeï¬ƒcients Î±1, . . . , Î±n âˆˆR and of the epochs
t1, . . . , tn âˆˆR
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€² KXX(tÎ½ âˆ’tÎ½â€²) â‰¥0.
(25.24)
3This calculation is, in fact, also valid when

X(t)

is WSS but not centered.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

570
Continuous-Time Stochastic Processes
To prove (25.23) we calculate
KXX(Ï„) = Cov

X(t + Ï„), X(t)

= Cov

X(tâ€²), X(tâ€² âˆ’Ï„)

= Cov

X(tâ€² âˆ’Ï„), X(tâ€²)

= KXX(âˆ’Ï„),
Ï„ âˆˆR,
where the ï¬rst equality follows from the deï¬nition of KXX(Ï„) (25.14); the second
by deï¬ning tâ€² â‰œt + Ï„; the third because Cov[X, Y ] = Cov[Y, X] (for real random
variables); and the ï¬nal equality by the deï¬nition of KXX(âˆ’Ï„) (25.14).
To prove (25.24) we compute
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€² KXX(tÎ½ âˆ’tÎ½â€²) =
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€²Cov[X(tÎ½), X(tÎ½â€²)]
= Cov

n

Î½=1
Î±Î½X(tÎ½),
n

Î½â€²=1
Î±Î½â€²X(tÎ½â€²)

= Var

n

Î½=1
Î±Î½X(tÎ½)

(25.25)
â‰¥0.
The next theorem demonstrates that Properties (25.23) and (25.24) characterize
the autocovariance functions of WSS stochastic processes (cf. Theorem 13.5.2).
Theorem 25.6.2. Every symmetric positive deï¬nite function is the autocovariance
function of some stationary Gaussian SP.
Proof. The proof is based on Kolmogorovâ€™s Existence Theorem (Theorem 25.2.1)
and is only sketched here. Let K(Â·) be a symmetric and positive deï¬nite function
from R to R. The idea is to consider, for every n âˆˆN and for every choice of the
epochs t1, . . . , tn âˆˆR, the joint distribution function Gn(Â·; t1, . . . , tn) corresponding
to the centered multivariate Gaussian distribution of covariance matrix
â›
âœ
âœ
âœ
âœ
â
K(t1 âˆ’t1)
K(t1 âˆ’t2)
Â· Â· Â·
K(t1 âˆ’tn)
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
K(tn âˆ’t1)
K(tn âˆ’t2)
Â· Â· Â·
K(tn âˆ’tn)
â
âŸ
âŸ
âŸ
âŸ
â 
and to verify that the sequence {Gn(Â·; Â·)} satisï¬es the symmetry and consistency
requirements of Kolmogorovâ€™s Existence Theorem. The details, which can be found
in (Doob, 1990, Chapter II, Section Â§ 3, Theorem 3.1), are omitted.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.7 The Power Spectral Density of a Continuous-Time SP
571
25.7
The Power Spectral Density of a Continuous-Time SP
Under suitable conditions, engineers usually deï¬ne the power spectral density of a
WSS SP as the Fourier Transform of its autocovariance function. There is nothing
wrong with this deï¬nition, and we encourage the reader to think about the PSD
in this way.4 We, however, prefer a slightly more general deï¬nition that allows us
also to consider discontinuous spectra and, more importantly, allows us to infer
that any integrable, nonnegative, symmetric function is the PSD of some Gaus-
sian SP (Proposition 25.7.3). Fortunately, the two deï¬nitions agree whenever the
autocovariance function is continuous and integrable.
Before deï¬ning the PSD, we pause to discuss the Fourier Transform of the auto-
covariance function. If the autocovariance function KXX of a WSS SP

X(t)

is
integrable, i.e., if
 âˆ
âˆ’âˆ
KXX(Ï„)
 dÏ„ < âˆ,
(25.26)
then we can discuss its FT Ë†KXX. The following proposition summarizes the main
properties of the FT of continuous integrable autocovariance functions.
Proposition 25.7.1. If the autocovariance function KXX is continuous at the origin
and integrable, then its Fourier Transform Ë†KXX is nonnegative
Ë†KXX(f) â‰¥0,
f âˆˆR
(25.27)
and symmetric
Ë†KXX(âˆ’f) = Ë†KXX(f),
f âˆˆR.
(25.28)
Moreover, the Inverse Fourier Transform recovers KXX in the sense that5
KXX(Ï„) =
 âˆ
âˆ’âˆ
Ë†KXX(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR.
(25.29)
Proof. This result can be deduced from three results in (Feller, 1971, Chap-
ter XIX): the theorem in Section 3, Bochnerâ€™s Theorem in Section 2, and Lemma 1
in Section 2.
Deï¬nition 25.7.2 (The PSD of a Continuous-Time WSS SP). We say that the
WSS continuous-time SP

X(t)

is of power spectral density (PSD) SXX if SXX
is a nonnegative, symmetric, integrable function from R to R whose Inverse Fourier
Transform is the autocovariance function KXX of

X(t)

:
KXX(Ï„) =
 âˆ
âˆ’âˆ
SXX(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR.
(25.30)
4Engineers can, however, be a bit sloppy in that they sometimes speak of a SP whose PSD
is discontinuous, e.g., the Brickwall function f â†’I{|f| â‰¤W}. This is inconsistent with their
deï¬nition because the FT of an integrable function must be continuous (Theorem 6.2.11), and
consequently if the autocovariance function is integrable then its FT cannot be discontinuous.
Our more general deï¬nition does not suï¬€er from this problem and allows for discontinuous PSDs.
5Recall that without additional assumptions one is not guaranteed that the Inverse Fourier
Transform of the Fourier Transform of a function will be identical to the original function. Here we
need not make any additional assumptions because we already assumed that the autocovariance
function is continuous and because autocovariance functions are positive deï¬nite.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

572
Continuous-Time Stochastic Processes
A few remarks regarding this deï¬nition:
(i) By the uniqueness of the IFT (the analogue of Theorem 6.2.12 for the IFT) it
follows that if two functions are PSDs of the same WSS SP, then they must be
equal except on a set of frequencies of Lebesgue measure zero. Consequently,
we shall often speak of â€œtheâ€ PSD as though it were unique.
(ii) By Proposition 25.7.1, if KXX is continuous and integrable, then

X(t)

has
a PSD in the sense of Deï¬nition 25.7.2, and this PSD is the FT of KXX.
There are, however, autocovariance functions that are not integrable and
that nonetheless have a PSD in the sense of Deï¬nition 25.7.2. For example,
Ï„ 	â†’sinc(Ï„).
Thus, every continuous autocovariance function that has a PSD in the en-
gineersâ€™ sense (i.e., that is integrable) also has the same PSD according to
our deï¬nition, but our deï¬nition is more general in that some autocovariance
functions that have a PSD according to our deï¬nition are not integrable and
therefore do not have a PSD in the engineersâ€™ sense.
(iii) By substituting Ï„ = 0 in (25.30) and using (25.15) we can express the variance
of X(t) in terms of the PSD SXX as
Var

X(t)

= KXX(0) =
 âˆ
âˆ’âˆ
SXX(f) df,
t âˆˆR.
(25.31)
(iv) Only processes with continuous autocovariance functions have PSDs, because
the RHS of (25.30), being the IFT of an integrable function, must be contin-
uous (Theorem 6.2.11 (ii)).
(v) It can be shown that if the autocovariance function can be written as the
IFT of some integrable function, then this latter function must be nonneg-
ative (except on a set of frequencies of Lebesgue measure zero). This is the
continuous-time analogue of Proposition 13.6.3.
The nonnegativity, symmetry, and integrability conditions characterize PSDs in
the following sense:
Proposition 25.7.3. Every nonnegative, symmetric, integrable function is the PSD
of some stationary Gaussian SP whose autocovariance function is continuous.
Proof. Let S(Â·) be some integrable, nonnegative, and symmetric function from R
to the nonnegative reals. Deï¬ne K(Â·) to be its IFT
K(Ï„) =
 âˆ
âˆ’âˆ
S(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR.
(25.32)
We shall verify that K(Â·) satisï¬es the hypotheses of Theorem 25.6.2, namely, that
it is symmetric and positive deï¬nite. It will then follow from Theorem 25.6.2 that
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.8 The Spectral Distribution Function
573
there exists a stationary Gaussian SP

X(t)

whose autocovariance function KXX
is equal to K(Â·) and is thus given by
KXX(Ï„) =
 âˆ
âˆ’âˆ
S(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR.
(25.33)
This will establish that

X(t)

is of PSD S(Â·). The continuity of KXX will follow
from the continuity of the IFT of integrable functions (Theorem 6.2.11).
To conclude the proof we need to show that the function K(Â·) deï¬ned in (25.32)
is symmetric and positive deï¬nite. The symmetry follows from our assumption
that S(Â·) is symmetric:
K(âˆ’Ï„) =
 âˆ
âˆ’âˆ
S(f) ei2Ï€f(âˆ’Ï„) df
=
 âˆ
âˆ’âˆ
S(âˆ’Ëœf) ei2Ï€ Ëœ
fÏ„ d Ëœf
=
 âˆ
âˆ’âˆ
S( Ëœf) ei2Ï€ Ëœ
fÏ„ d Ëœf
= K(Ï„),
Ï„ âˆˆR,
where the ï¬rst equality follows from (25.32); the second from the change of variable
Ëœf â‰œâˆ’f; the third by the symmetry of S(Â·); and the ï¬nal equality again by (25.32).
We next prove that K(Â·) is positive deï¬nite. To that end we ï¬x some n âˆˆN, some
constants Î±1, . . . , Î±n âˆˆR, and some epochs t1, . . . , tn âˆˆR and compute:
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€²K(tÎ½ âˆ’tÎ½â€²) =
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€²
 âˆ
âˆ’âˆ
S(f) ei2Ï€f(tÎ½âˆ’tÎ½â€²) df
=
 âˆ
âˆ’âˆ
S(f)
	
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€² ei2Ï€f(tÎ½âˆ’tÎ½â€²)

df
=
 âˆ
âˆ’âˆ
S(f)
	
n

Î½=1
n

Î½â€²=1
Î±Î½ ei2Ï€ftÎ½Î±Î½â€² eâˆ’i2Ï€ftÎ½â€²

df
=
 âˆ
âˆ’âˆ
S(f)
	
n

Î½=1
Î±Î½ ei2Ï€ftÎ½

	
n

Î½â€²=1
Î±Î½â€² ei2Ï€ftÎ½â€²

âˆ—
df
=
 âˆ
âˆ’âˆ
S(f)

n

Î½=1
Î±Î½ ei2Ï€ftÎ½

2
df
â‰¥0,
where the ï¬rst equality follows from (25.32); the subsequent equalities by simple
algebra; and the last inequality from our assumption that S(Â·) is nonnegative.
25.8
The Spectral Distribution Function
In this section we shall state, without proof, Bochnerâ€™s Theorem on continuous
positive deï¬nite functions and discuss its application to continuous autocovariance
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

574
Continuous-Time Stochastic Processes
functions. We shall then deï¬ne the spectral distribution function of WSS stochastic
processes. The concept of a spectral distribution function is more general than
that of a PSD, because every WSS with a continuous autocovariance function has
a spectral distribution function, but only some have a PSD. Nevertheless, for our
purposes, the notion of PSD will suï¬ƒce, and the results of this section will not be
used in the rest of the book.
Recall that the characteristic function Î¦X(Â·) of a RV X is the mapping from R
to C deï¬ned by
Ï– 	â†’E

eiÏ–X
,
Ï– âˆˆR.
(25.34)
If X is symmetric (i.e., has a symmetric distribution) in the sense that
Pr[X â‰¥x] = Pr[X â‰¤âˆ’x],
x âˆˆR,
(25.35)
then Î¦X(Â·) only takes on real values and is a symmetric function, as the following
argument shows. The symmetry of the distribution of X implies that X and âˆ’X
have the same distribution, which implies that their exponentiations have the same
law
eiÏ–X
L= eâˆ’iÏ–X,
Ï– âˆˆR,
(25.36)
and a fortiori that the expectation of the two exponentials are equal
E

eiÏ–X
= E

eâˆ’iÏ–X
,
Ï– âˆˆR.
(25.37)
The LHS of (25.37) is Î¦X(Ï–), and the RHS is Î¦X(âˆ’Ï–), thus demonstrating the
symmetry of Î¦X(Â·). To establish that (25.35) also implies that Î¦X(Â·) is real, we
note that, by (25.37),
Î¦X(Ï–) = E

eiÏ–X
= 1
2

E

eiÏ–X
+ E

eâˆ’iÏ–X
= E
eiÏ–X + eâˆ’iÏ–X
2

= E

cos(Ï–X)

,
Ï– âˆˆR,
which is real. Here the ï¬rst equality follows from (25.34); the second from (25.37);
and the third from the linearity of expectation.
Bochnerâ€™s Theorem establishes a correspondence between continuous, symmetric,
positive deï¬nite functions and characteristic functions.
Theorem 25.8.1 (Bochnerâ€™s Theorem). Let the mapping Î¦(Â·) from R to R be
continuous. Then the following two conditions are equivalent:
a) Î¦(Â·) is the characteristic function of some RV having a symmetric distribu-
tion.
b) Î¦(Â·) is a symmetric positive deï¬nite function satisfying Î¦(0) = 1.
Proof. See (Feller, 1971, Chapter XIX, Section 2) or (Lo`eve, 1963, Chapter IV,
Section 14) or (Katznelson, 2004, Chapter VI, Section 2.8).
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.8 The Spectral Distribution Function
575
Bochnerâ€™s Theorem is the key to understanding autocovariance functions:
Proposition 25.8.2. Let

X(t)

be a WSS SP whose autocovariance function KXX
is continuous. Then:
(i) There exists a symmetric RV S such that
KXX(Ï„) = KXX(0) E

ei2Ï€Ï„S
,
Ï„ âˆˆR.
(25.38)
(ii) If KXX(0) > 0, then the distribution of S in (25.38) is uniquely determined
by KXX, and

X(t)

has a PSD if, and only if, S has a density.
Proof. If KXX(0) = 0, then

X(t)

is deterministic in the sense that for every
epoch t âˆˆR the variance of X(t) is zero. By the inequality |KXX(Ï„)| â‰¤KXX(0)
(Lemma 25.4.5, (25.16)) it follows that if KXX(0) = 0 then KXX(Ï„) = 0 for all
Ï„ âˆˆR, and (25.38) holds in this case for any choice of S and there is nothing else
to prove.
Consider now the case KXX(0) > 0. To prove Part (i) we note that because KXX is
by assumption continuous, and because all autocovariance functions are symmetric
and positive deï¬nite (see (25.23) and (25.24)), it follows that the mapping
Ï„ 	â†’KXX(Ï„)
KXX(0),
Ï„ âˆˆR
is a continuous, symmetric, positive deï¬nite mapping that takes on the value one
at Ï„ = 0. Consequently, by Bochnerâ€™s Theorem, there exists a RV R of a symmetric
distribution such that
KXX(Ï„)
KXX(0) = E

eiÏ„R
,
Ï„ âˆˆR.
It follows that if we deï¬ne S as R/(2Ï€) then (25.38) will hold, and Part (i) is thus
also established for the case where KXX(0) > 0.
We conclude the treatment of the case KXX(0) > 0 by proving Part (ii). That the
distribution of S is unique follows because (25.38) implies that
E

eiÏ–S
= KXX(Ï–/(2Ï€))
KXX(0)
,
Ï– âˆˆR,
so KXX determines the characteristic function of S and hence also its distribution
(Theorem 17.4.4).
Because the distribution of S is symmetric, if S has a density then it also has a
symmetric density. Denote by fS(Â·) a symmetric density function for S. In terms
of fS(Â·) we can rewrite (25.38) as
KXX(Ï„) =
 âˆ
âˆ’âˆ
KXX(0) fS(s) ei2Ï€sÏ„ ds,
Ï„ âˆˆR,
so the nonnegative symmetric function KXX(0) fS(Â·) is a PSD of

X(t)

. Con-
versely, if

X(t)

has PSD SXX, then
KXX(Ï„) =
 âˆ
âˆ’âˆ
SXX(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR,
(25.39)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

576
Continuous-Time Stochastic Processes
and (25.38) holds with S having the density
fS(s) = SXX(s)
KXX(0),
s âˆˆR.
(25.40)
(The RHS of (25.40) is symmetric, nonnegative, and integrates to 1 by (25.31).)
Proposition 25.8.2 motivates us to deï¬ne the spectral distribution function of a
continuous autocovariance function (or of a WSS SP having such an autocovariance
function) as follows.
Deï¬nition 25.8.3 (Spectral Distribution Function). The spectral distribution
function of a continuous autocovariance function KXX is the mapping
Î¾ 	â†’KXX(0) Pr[S â‰¤Î¾],
(25.41)
where S is a random variable for which (25.38) holds.
25.9
The Average Power
We next address the average power in the sample-paths of a SP. We would like to
better understand formal expressions of the form
1
T
 T/2
âˆ’T/2
X2(Ï‰, t) dt
for a SP

X(t)

deï¬ned on the probability space (Î©, F, P). Recalling that if we ï¬x
Ï‰ âˆˆÎ© then we can view the trajectory t 	â†’X(Ï‰, t) as a function of time, we would
like to think about the integral above as the time-integral of the square of the
trajectory t 	â†’X(Ï‰, t). Since the result of this integral is a (nonnegative) number
that depends on Ï‰, we would like to view this result as a nonnegative RV
Ï‰ 	â†’1
T
 T/2
âˆ’T/2
X2(Ï‰, t) dt,
Ï‰ âˆˆÎ©.
Mathematicians, however, would object to our naive approach on two grounds. The
ï¬rst is that it is prima facie unclear whether for every ï¬xed Ï‰ âˆˆÎ© the mapping
t 	â†’X2(Ï‰, t) is suï¬ƒciently well-behaved to allow us to discuss its integral. (It may
not be Lebesgue measurable.) The second is that, even if this integral could be
carried out for every Ï‰ âˆˆÎ©, it is prima facie unclear that the result would be a
RV. While it would certainly be a mapping from Î© to the extended reals (allowing
for +âˆ), it is not clear that it would satisfy the technical measurability conditions
that random variables must meet.6
To address these objections we shall assume that

X(t)

is a â€œmeasurable stochastic
process.â€ This is a technical condition that will be foreign to most readers and
6By â€œX is a random variable possibly taking on the value +âˆâ€ we mean that X is a mapping
from Î© to R âˆª{+âˆ} with the set {Ï‰ âˆˆÎ© : X(Ï‰) â‰¤Î¾} being an event for every Î¾ âˆˆR and with
the set {Ï‰ âˆˆÎ© : X(Ï‰) = +âˆ} also being an event.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.9 The Average Power
577
that will be inessential to the rest of this book. We mention it here because, in
order to be mathematically honest, we shall have to slip this attribute into some
of the theorems that we shall later state. Nothing will be lost on readers who
replace â€œmeasurable stochastic processâ€ with â€œstochastic process satisfying a mild
technical condition.â€
Fortunately, this technical condition is, indeed, very mild. For example, Propo-
sition 25.7.3 still holds if we slip in the attribute â€œmeasurableâ€ before the words
â€œGaussian process.â€ Similarly, in Theorem 25.6.2, if we add the hypothesis that
the given function is continuous at the origin, then we can slip in the attribute
â€œmeasurableâ€ before the words â€œstationary Gaussian stochastic process.â€7
For the beneï¬t of readers who are familiar with Measure Theory, we provide the
following deï¬nition.
Deï¬nition 25.9.1 (Measurable SP). Let

X(t), t âˆˆR

be a SP deï¬ned over the
probability space (Î©, F, P). We say that the process is a measurable stochastic
process if the mapping (Ï‰, t) 	â†’X(Ï‰, t) is a measurable mapping from Î© Ã— R
to R when the codomain R is endowed with the Borel Ïƒ-algebra on R and when the
domain Î© Ã— R is endowed with the Ïƒ-algebra deï¬ned by the product of F on Î© and
the Borel Ïƒ-algebra on R.
The nice thing about measurable stochastic processes is that if

X(t)

is a measur-
able SP, then for every Ï‰ âˆˆÎ© the trajectory t 	â†’X(Ï‰, t) is a Borel (and hence also
Lebesgue) measurable function of time; see (Halmos, 1950, Chapter 7, Section 34,
Theorem B) or (Billingsley, 1995, Chapter 3, Section 18, Theorem 18.1 (ii)). More-
over, for such processes we can sometimes use Fubiniâ€™s Theorem to swap the order
in which we compute time-integrals and expectations; see (Halmos, 1950, Chap-
ter 7, Section 36) or (Billingsley, 1995, Chapter 3, Section 18, Theorem 18.3 (ii)).
We can now state the main result of this section regarding the average power in a
WSS SP.
Proposition 25.9.2 (Power in a Centered WSS SP). If

X(t)

is a measurable,
centered, WSS SP deï¬ned over the probability space (Î©, F, P) and having the au-
tocovariance function KXX, then, for every a, b âˆˆR satisfying a < b, the mapping
Ï‰ 	â†’
1
b âˆ’a
 b
a
X2(Ï‰, t) dt
(25.42)
deï¬nes a RV (possibly taking on the value +âˆ) satisfying
1
b âˆ’a E
 b
a
X2(t) dt

= KXX(0).
(25.43)
Proof. The proof of (25.43) is straightforward and merely requires swapping the
order of integration and expectation. This swap can be justiï¬ed using Fubiniâ€™s The-
orem. Heuristically, the swapping of expectation and integration can be justiï¬ed by
7These are but very special cases of a much more general result that states that given FDDs
corresponding to a WSS SP of an autocovariance that is continuous at the origin, there exists
a SP of the given FDDs that is also measurable. See, for example, (Doob, 1990, Chapter II,
Section Â§ 2, Theorem 2.6). (Replacing the values Â±âˆwith zero may ruin the separability but
not the measurability.)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

578
Continuous-Time Stochastic Processes
thinking about the integral as being a Riemann integral that can be approximated
by ï¬nite sums and by then recalling the linearity of expectation that guarantees
that the expectation of a ï¬nite sum is the sum of the expectations. We then have
E
 b
a
X2(t) dt

=
 b
a
E

X2(t)

dt
=
 b
a
KXX(0) dt
= (b âˆ’a) KXX(0),
where the ï¬rst equality follows by swapping the integration with the expectation;
the second equality holds because our assumption that

X(t)

is centered implies
that, for every t âˆˆR, the RV X(t) is centered and by (25.14); and the ï¬nal equality
holds because the integrand is constant.
That (25.42) is a RV (possibly taking on the value +âˆ) follows from Fubiniâ€™s
Theorem.
Recalling Deï¬nition 14.6.1 of the power in a SP as
lim
Tâ†’âˆE
1
T
 T/2
âˆ’T/2
X2(t) dt

,
we conclude:
Corollary 25.9.3. The power in a centered, measurable, WSS SP

X(t)

of auto-
covariance function KXX is equal to KXX(0).
25.10
Stochastic Integrals and Linear Functionals
For the problem of detecting continuous-time signals corrupted by noise, we shall
be interested in stochastic integrals of the form
 âˆ
âˆ’âˆ
X(t) s(t) dt
(25.44)
for WSS stochastic processes

X(t)

deï¬ned over a probability space (Î©, F, P)
and for properly well-behaved deterministic functions s(Â·). We would like to think
about the result of such an integral as a RV
Ï‰ 	â†’
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt
(25.45)
that maps each Ï‰ âˆˆÎ© to the real number that is the result of the integration
over time of the product of the trajectory t 	â†’X(Ï‰, t) corresponding to Ï‰ by the
deterministic function t 	â†’s(t). That is, each Ï‰ is mapped to the inner product
between its trajectory t 	â†’X(Ï‰, t) and the function s(Â·).
This is an excellent way of thinking about such integrals, but we do run into
some mathematical objections similar to those we encountered in Section 25.9.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.10 Stochastic Integrals and Linear Functionals
579
For example, it is not obvious that for each Ï‰ âˆˆÎ© the mapping t 	â†’X(Ï‰, t) s(t)
is a suï¬ƒciently well-behaved function for the time-integral to be deï¬ned. As we
shall see, for this reason we must impose certain restrictions on s(Â·), and we will
not claim that t 	â†’X(Ï‰, t) s(t) is integrable for every Ï‰ âˆˆÎ© but only for Ï‰â€™s in
some subset of Î© having probability one. Also, even if this issue is addressed, it is
unclear whether the mapping of Ï‰ to the result of the integration is a RV: while it
is clearly a mapping from Î© to the reals, it is unclear that it satisï¬es the additional
mathematical requirement of measurability, i.e., that for every Î¾ âˆˆR the set
1
Ï‰ âˆˆÎ© :
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt â‰¤Î¾
2
be an event, i.e., an element of F.
We ask the reader to take it on faith that these issues can be resolved and to focus
on the relatively straightforward computation of the mean and variance of (25.45).
The resolution of the measurability issues is provided in Proposition 25.10.1, whose
proof is recommended only to readers with background in Measure Theory.
We shall assume throughout that

X(t)

is WSS and that the deterministic function
s: R â†’R is integrable. We begin by heuristically deriving the mean:
E
 âˆ
âˆ’âˆ
X(t) s(t) dt

=
 âˆ
âˆ’âˆ
E

X(t) s(t)

dt
=
 âˆ
âˆ’âˆ
E

X(t)

s(t) dt
= E

X(0)
  âˆ
âˆ’âˆ
s(t) dt,
(25.46)
with the following informal justiï¬cation. The ï¬rst equality follows by swapping
the expectation with the time-integration; the second because s(Â·) is deterministic;
and the last equality from our assumption that

X(t)

is WSS, which implies that

X(t)

is of constant mean: E[X(t)] = E[X(0)] for all t âˆˆR.
We next heuristically derive the variance of the integral in terms of the autocovari-
ance function KXX of the process

X(t)

. We begin by considering the case where

X(t)

is of zero mean. In this case we have
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

= E
5	 âˆ
âˆ’âˆ
X(t) s(t) dt

26
= E
	 âˆ
âˆ’âˆ
X(t) s(t) dt

 	 âˆ
âˆ’âˆ
X(Ï„) s(Ï„) dÏ„


= E
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
X(t) s(t) X(Ï„) s(Ï„) dt dÏ„

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) s(Ï„) E

X(t) X(Ï„)

dt dÏ„
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) KXX(t âˆ’Ï„) s(Ï„) dt dÏ„,
(25.47)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

580
Continuous-Time Stochastic Processes
where the ï¬rst equality follows because (25.46) and our assumption that

X(t)

is centered combine to guarantee that

X(t) s(t) dt is of zero mean; the second
by writing a2 as a times a; the third by writing the product of integrals over R
as a double integral (i.e., as an integral over R2); the fourth by swapping the
double-integral with the expectation; and the ï¬nal equality by the deï¬nition of the
autocovariance function (Deï¬nition 25.4.4) and because

X(t)

is centered.
There are two equivalent ways of writing the RHS of (25.47) that we wish to point
out. The ï¬rst is obtained from (25.47) by changing the integration variables from
(t, Ï„) to (Ïƒ, Ï„), where Ïƒ â‰œt âˆ’Ï„ and by performing the integration ï¬rst over Ï„ and
then over Ïƒ:
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) KXX(t âˆ’Ï„) s(Ï„) dt dÏ„
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(Ïƒ + Ï„) KXX(Ïƒ) s(Ï„) dÏƒ dÏ„
=
 âˆ
âˆ’âˆ
KXX(Ïƒ)
 âˆ
âˆ’âˆ
s(Ïƒ + Ï„) s(Ï„) dÏ„ dÏƒ
=
 âˆ
âˆ’âˆ
KXX(Ïƒ) Rss(Ïƒ) dÏƒ,
(25.48)
where Rss is the self-similarity function of s (Deï¬nition 11.2.1 and Section 11.4).
The second equivalent way of writing (25.47) can be derived from (25.48) when

X(t)

is of PSD SXX. Since (25.48) has the form of an inner product, we can use
Proposition 6.2.4 to write this inner product in the frequency domain by noting
that the FT of Rss is f 	â†’|Ë†s(f)|2 (see (11.35)) and that KXX is the IFT of its
PSD SXX. The result is that
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

=
 âˆ
âˆ’âˆ
SXX(f)
Ë†s(f)
2 df.
(25.49)
We next show that (25.47) (and hence also (25.48) & (25.49), which are equivalent
ways of writing (25.47)) remains valid also when

X(t)

is of mean Î¼ (not neces-
sarily zero). To see this we can consider the zero-mean SP
 ËœX(t)

deï¬ned at every
epoch t âˆˆR by ËœX(t) = X(t) âˆ’Î¼ and formally compute
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

= Var
 âˆ
âˆ’âˆ
 ËœX(t) + Î¼

s(t) dt

= Var
 âˆ
âˆ’âˆ
ËœX(t) s(t) dt + Î¼
 âˆ
âˆ’âˆ
s(t) dt

= Var
 âˆ
âˆ’âˆ
ËœX(t) s(t) dt

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) K Ëœ
X Ëœ
X(t âˆ’Ï„) s(Ï„) dt dÏ„
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) KXX(t âˆ’Ï„) s(Ï„) dt dÏ„,
(25.50)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.10 Stochastic Integrals and Linear Functionals
581
where the ï¬rst equality follows from the deï¬nition of ËœX(t) as X(t) âˆ’Î¼; the second
by the linearity of integration; the third because adding a deterministic quantity
to a RV does not change its covariance; the fourth by (25.47) applied to the zero-
mean process
 ËœX(t)

; and the ï¬nal equality because the autocovariance function
of
 ËœX(t)

is the same as the autocovariance function of

X(t)

(Deï¬nition 25.4.4).
As above, once a result is proved for centered stochastic processes, its extension
to WSS stochastic processes with a mean can be straightforward. Consequently,
we shall often derive our results for centered WSS stochastic processes and leave
it to the reader to extend them to mean-Î¼ stochastic processes by expressing such
stochastic processes as the sum of a zero-mean SP and the deterministic constant Î¼.
As promised, we now state the results about the mean and variance of (25.45) in
a mathematically defensible proposition.
Proposition 25.10.1 (Mean and Variance of Stochastic Integrals). Let

X(t)

be a measurable WSS SP deï¬ned over the probability space (Î©, F, P) and having
the autocovariance function KXX. Let s: R â†’R be some deterministic integrable
function. Then:
(i) For every Ï‰ âˆˆÎ©, the mapping t 	â†’X(Ï‰, t) s(t) is Lebesgue measurable.
(ii) The set
N â‰œ
1
Ï‰ âˆˆÎ© :
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t)
 dt = âˆ
2
(25.51)
is an event and its probability is zero.
(iii) The mapping from Î© \ N to R deï¬ned by
Ï‰ 	â†’
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt
(25.52)
is measurable with respect to F.
(iv) The mapping from Î© to R deï¬ned by
Ï‰ 	â†’
â§
â¨
â©
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt
if Ï‰ /âˆˆN,
0
otherwise,
(25.53)
deï¬nes a random variable.
(v) The mean of this RV is
E

X(0)
  âˆ
âˆ’âˆ
s(t) dt.
(25.54)
(vi) Its variance is
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) KXX(t âˆ’Ï„) s(Ï„) dÏ„ dt,
(25.55)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

582
Continuous-Time Stochastic Processes
which can also be expressed as
 âˆ
âˆ’âˆ
KXX(Ïƒ) Rss(Ïƒ) dÏƒ,
(25.56)
where Rss is the self-similarity function of s.
(vii) If

X(t)

is of PSD SXX, then the variance of this RV can be expressed as
 âˆ
âˆ’âˆ
SXX(f)
Ë†s(f)
2 df.
(25.57)
Proof. Part (i) follows because the measurability of the process

X(t)

guarantees
that for every Ï‰ âˆˆÎ© the mapping t 	â†’X(Ï‰, t) is Borel measurable and hence a
fortiori Lebesgue measurable; see (Billingsley, 1995, Chapter 3, Section 18, Theo-
rem 18.1 (ii)).
If s happens to be Borel measurable, then Parts (ii)â€“(v) follow directly by Fubiniâ€™s
Theorem (Billingsley, 1995, Chapter 3, Section 18, Theorem 18.3) because in this
case the mapping (Ï‰, t) 	â†’X(Ï‰, t) s(t) is measurable (with respect to the product
of F by the Borel Ïƒ-algebra on the real line) and because
 âˆ
âˆ’âˆ
E
X(t) s(t)

dt =
 âˆ
âˆ’âˆ
E[|X(t)|] |s(t)| dt
â‰¤

E[X2(0)]
 âˆ
âˆ’âˆ
|s(t)| dt
< âˆ,
where the ï¬rst inequality follows from (25.17), and where the second inequality
follows from our assumption that s is integrable.
To prove Parts (i)â€“(v) for the case where s is Lebesgue measurable but not Borel
measurable, recall that every Lebesgue measurable function is equal (except on
a set of Lebesgue measure zero) to a Borel measurable function (Rudin, 1987,
Chapter 8, Lemma 1 or Chapter 2, Exercise 14), and note that the RHS of (25.51)
and the mappings in (25.52) and (25.53) are unaltered when s is replaced with a
function that is identical to it outside a set of Lebesgue measure zero.
We next prove Part (vi) under the assumption that

X(t)

is centered. The more
general case then follows from the argument leading to (25.50). To prove Part (vi)
we need to justify the steps leading to (25.47). For the readerâ€™s convenience we
repeat these steps here and then proceed to justify them.
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

= E
5	 âˆ
âˆ’âˆ
X(t) s(t) dt

26
= E
	 âˆ
âˆ’âˆ
X(t) s(t) dt

 	 âˆ
âˆ’âˆ
X(Ï„) s(Ï„) dÏ„


= E
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
X(t) s(t)X(Ï„) s(Ï„) dt dÏ„

.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.10 Stochastic Integrals and Linear Functionals
583
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) s(Ï„) E[X(t)X(Ï„)] dt dÏ„
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) KXX(t âˆ’Ï„) s(Ï„) dt dÏ„.
The ï¬rst equality follows from Part (v) and the assumption that

X(t)

is centered;
the second follows by writing a2 as a times a; the third follows because for Ï‰â€™s sat-
isfying

|X(Ï‰, t) s(t)| dt < âˆwe can use Fubiniâ€™s Theorem to replace the iterated
integrals with a double integral and because other Ï‰â€™s occur with zero probability
and therefore do not inï¬‚uence the expectation; the fourth equality entails swap-
ping the expectation with the integration over R2 and can be justiï¬ed by Fubiniâ€™s
Theorem because, by (25.18),
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
s(t) s(Ï„)
 E
X(t)X(Ï„)

dt dÏ„ â‰¤KXX(0)
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
|s(t)| |s(Ï„)| dt dÏ„
= KXX(0) âˆ¥sâˆ¥2
1
< âˆ;
and the ï¬nal equality follows from the deï¬nition of the autocovariance function
(Deï¬nition 25.4.4).
Having derived (25.55) we can derive (25.56) by following the steps leading to
(25.48). The only issue that needs clariï¬cation is the justiï¬cation for replacing
the integral over R2 with the iterated integrals. This is justiï¬ed using Fubiniâ€™s
Theorem by noting that, by (25.16), | KXX(Ïƒ)| â‰¤KXX(0) and that s is integrable:
 âˆ
âˆ’âˆ
|s(Ï„)|
 âˆ
âˆ’âˆ
s(Ïƒ + Ï„) KXX(Ïƒ)
 dÏƒ dÏ„ â‰¤KXX(0)
 âˆ
âˆ’âˆ
|s(Ï„)|
 âˆ
âˆ’âˆ
|s(Ïƒ + Ï„)| dÏƒ dÏ„
= KXX(0) âˆ¥sâˆ¥2
1
< âˆ.
Finally, Part (vii) follows from (25.56) and from Proposition 6.2.4 by noting that,
by (11.34) & (11.35), Rss is integrable and of FT
Ë†Rss(f) =
Ë†s(f)
2,
f âˆˆR,
and that, by Deï¬nition 25.7.2, if SXX is the PSD of

X(t)

, then SXX is integrable
and its IFT is KXX, i.e.,
KXX(Ïƒ) =
 âˆ
âˆ’âˆ
SXX(f) ei2Ï€fÏƒ df.
Note 25.10.2.
(i) In the future we shall sometimes write
 âˆ
âˆ’âˆ
X(t) s(t) dt
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

584
Continuous-Time Stochastic Processes
instead of the mathematically more explicit (25.53) & (25.51). Sometimes,
however, we shall make the argument Ï‰ âˆˆÎ© more explicit:
	 âˆ
âˆ’âˆ
X(t) s(t) dt

(Ï‰) =
â§
â¨
â©
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt
if
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t)
 dt < âˆ,
0
otherwise.
(ii) If s1 and s2 are indistinguishable integrable real signals (Deï¬nition 2.5.2),
then the random variables
 âˆ
âˆ’âˆX(t)s1(t) dt and
 âˆ
âˆ’âˆX(t)s2(t) dt are identi-
cal.
(iii) For every Î± âˆˆR
 âˆ
âˆ’âˆ
X(t)

Î± s(t)

dt = Î±
 âˆ
âˆ’âˆ
X(t) s(t) dt.
(25.58)
(iv) We caution the very careful readers that if s1 and s2 are integrable func-
tions, then there may be some Ï‰â€™s in Î© for which the stochastic integral
 âˆ
âˆ’âˆX(t) (s1(t) + s2(t)) dt

(Ï‰) is not equal to the sum of the stochastic
integrals
 âˆ
âˆ’âˆX(t) s1(t) dt

(Ï‰) and
 âˆ
âˆ’âˆX(t) s2(t) dt

(Ï‰). This can hap-
pen, for example, if the trajectory t 	â†’X(Ï‰, t) corresponding to Ï‰ is such
that either

|X(Ï‰, t) s1(t)| dt or

|X(Ï‰, t) s2(t)| dt is inï¬nite, but not both.
Fortunately, as we shall see in Lemma 25.10.3, such Ï‰â€™s occur with zero prob-
ability.
(v) The value that we have chosen to assign to the integral in (25.53) when Ï‰ is
in N is immaterial. Such Ï‰â€™s occur with zero probability, so this value does
not inï¬‚uence the distribution of the integral.8
Lemma 25.10.3 (â€œAlmostâ€ Linearity of Stochastic Integration). Let

X(t)

be
a measurable WSS SP, let s1, . . . , sm : R â†’R be integrable, and let Î³1, . . . , Î³m be
real. Then the random variables
Ï‰ 	â†’
- âˆ
âˆ’âˆ
X(t)
	 m

j=1
Î³j sj(t)

dt
.
(Ï‰)
(25.59)
and
Ï‰ 	â†’
m

j=1
Î³j
-	 âˆ
âˆ’âˆ
X(t) sj(t) dt

(Ï‰)
.
(25.60)
diï¬€er on at most a set of Ï‰â€™s of probability zero. In particular, the two random
variables have the same distribution.
Note 25.10.4. In view of this lemma we shall write, somewhat imprecisely,
 âˆ
âˆ’âˆ
X(t)

Î±1 s1(t) + Î±2 s2(t)

dt = Î±1
 âˆ
âˆ’âˆ
X(t) s1(t) dt + Î±2
 âˆ
âˆ’âˆ
X(t) s2(t) dt.
8The value zero is convenient because it guarantees that (25.58) holds even for Ï‰â€™s for which
the mapping t â†’X(Ï‰, t) s(t) is not integrable.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.11 Linear Functionals of Gaussian Processes
585
Proof of Lemma 25.10.3. Let (Î©, F, P) be the probability space over which the
SP

X(t)

is deï¬ned. Deï¬ne the function
s0 : t 	â†’
m

j=1
Î³j sj(t)
(25.61)
and the sets
Nj =
1
Ï‰ âˆˆÎ© :
 âˆ
âˆ’âˆ
X(Ï‰, t) sj(t)
 dt = âˆ
2
,
j = 0, 1, . . . , m.
By (25.61) and the Triangle Inequality (2.12)
X(Ï‰, t) s0(t)
 â‰¤
m

j=1
|Î³j| |X(Ï‰, t) sj(t)|,
Ï‰ âˆˆÎ©, t âˆˆR,
which implies that
N0 âŠ†
m

j=1
Nj.
By the Union Bound (or more speciï¬cally by Corollary 21.5.2 (i)), the set on the
RHS is of probability zero. The proof is concluded by noting that, outside this
set, the random variables (25.59) and (25.60) are identical. This follows because,
for Ï‰â€™s outside this set, all the integrals are ï¬nite so linearity holds.
In view of the linearity of stochastic integration (Lemma 25.10.3), we shall adopt
the following terminology.9
Deï¬nition 25.10.5 (Linear Functional of a SP). A linear functional of a SP

X(t)

is an expression of the form
 âˆ
âˆ’âˆ
X(t) s(t) dt +
n

Î½=1
Î±Î½ X(tÎ½),
(25.62)
where s: R â†’R is some deterministic integrable function; n is a nonnegative
integer; the constants Î±1, . . . , Î±n are real; and so are the epochs t1, . . . , tn. When n
is zero the linear functional reduces to a stochastic integral.
25.11
Linear Functionals of Gaussian Processes
We continue our discussion of linear functionals of stochastic processes, but this
time with the additional assumption that the underlying SP is Gaussian.
The
main result of this section is Proposition 25.11.1, which states that, subject to
some technical conditions, linear functionals of Gaussian stochastic processes are
Gaussian random variables. That is, if

X(t)

is a stationary Gaussian SP, then
 âˆ
âˆ’âˆ
X(t) s(t) dt +
n

Î½=1
Î±Î½ X(tÎ½)
(25.63)
9Our terminology is not standard: not every linear mapping of stochastic processes to the
reals has the form (25.62).
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

586
Continuous-Time Stochastic Processes
is a Gaussian RV. Consequently, the distribution of such a functional is fully speci-
ï¬ed by its mean and variance, which, as we shall see, can be easily computed from
the autocovariance function KXX of

X(t)

.
The proof of the Gaussianity of (25.63) (Proposition 25.11.1 ahead) is technical,
so we encourage the reader to focus on the following heuristic argument. Suppose
that the integral is a Riemann integral and that we can therefore approximate it
with a ï¬nite sum
 âˆ
âˆ’âˆ
X(t) s(t) dt â‰ˆ
K

k=âˆ’K
Î´ X(Î´k) s(Î´k)
for some large enough K and small enough Î´ > 0. (Do not bother trying to sort
out the exact sense in which this approximation holds. This is, after all, a heuristic
argument.) Consequently, we can approximate (25.63) by
 âˆ
âˆ’âˆ
X(t) s(t) dt +
n

Î½=1
Î±Î½ X(tÎ½) â‰ˆ
K

k=âˆ’K
Î´ s(Î´k) X(Î´k) +
n

Î½=1
Î±Î½ X(tÎ½).
(25.64)
But the RHS of the above is just a linear combination of the random variables
X(âˆ’KÎ´), . . . , X(KÎ´), X(t1), . . . , X(tn),
which are jointly Gaussian because

X(t)

is a Gaussian SP. Since a linear func-
tional of jointly Gaussian random variables is Gaussian (Theorem 23.6.17), the
RHS of (25.64) is Gaussian, thus making it plausible that its LHS is also Gaussian.
Before stating the main result of this section in a mathematically defensible way,
we now proceed to compute the mean and variance of (25.63). We assume that s(Â·)
is integrable and that

X(t)

is measurable and WSS. (Gaussianity is inessential
for the computation of the mean and variance.) The computation is very similar
to the one leading to (25.46) and (25.47). For the mean we have:
E
 âˆ
âˆ’âˆ
X(t) s(t) dt +
n

Î½=1
Î±Î½X(tÎ½)

= E
 âˆ
âˆ’âˆ
X(t) s(t) dt

+
n

Î½=1
Î±Î½E

X(tÎ½)

= E[X(0)]
	 âˆ
âˆ’âˆ
s(t) dt +
n

Î½=1
Î±Î½

,
(25.65)
where the ï¬rst equality follows from the linearity of expectation and where the
second equality follows from (25.46) and from the wide-sense stationarity of

X(t)

,
which implies that E[X(t)] = E[X(0)], for all t âˆˆR.
For the purpose of computing the variance of (25.63), we assume that

X(t)

is
centered. The result continues to hold if

X(t)

has a nonzero mean, because the
mean of

X(t)

does not inï¬‚uence the variance of (25.63). We begin by expanding
the variance as
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt +
n

Î½=1
Î±Î½X(tÎ½)

= Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

+ Var

n

Î½=1
Î±Î½X(tÎ½)

+ 2
n

Î½=1
Î±Î½Cov
 âˆ
âˆ’âˆ
X(t) s(t) dt, X(tÎ½)

(25.66)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.11 Linear Functionals of Gaussian Processes
587
and by noting that, by (25.48),
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

=
 âˆ
âˆ’âˆ
KXX(Ïƒ) Rss(Ïƒ) dÏƒ
(25.67)
and that, by (25.25),
Var

n

Î½=1
Î±Î½X(tÎ½)

=
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€² KXX(tÎ½ âˆ’tÎ½â€²).
(25.68)
To complete the computation of the variance of (25.63) it remains to compute the
covariance in the last term in (25.66):
E

X(tÎ½)
 âˆ
âˆ’âˆ
X(t) s(t) dt

= E
 âˆ
âˆ’âˆ
X(t) X(tÎ½) s(t) dt

=
 âˆ
âˆ’âˆ
s(t) E[X(t) X(tÎ½)] dt
=
 âˆ
âˆ’âˆ
s(t) KXX(t âˆ’tÎ½) dt.
(25.69)
Combining (25.66) with (25.67)â€“(25.69) we obtain
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt +
n

Î½=1
Î±Î½X(tÎ½)

=
 âˆ
âˆ’âˆ
KXX(Ïƒ) Rss(Ïƒ) dÏƒ
+
n

Î½=1
n

Î½â€²=1
Î±Î½Î±Î½â€² KXX(tÎ½ âˆ’tÎ½â€²) + 2
n

Î½=1
Î±Î½
 âˆ
âˆ’âˆ
s(t) KXX(t âˆ’tÎ½) dt.
(25.70)
We now state the main result about linear functionals of Gaussian stochastic pro-
cesses. The proof is recommended for mathematically-inclined readers only.
Proposition 25.11.1 (Linear Functional of a Stationary Gaussian SP). Consider
the setup of Proposition 25.10.1 with the additional assumption that

X(t)

is a
Gaussian SP. Additionally introduce the coeï¬ƒcients Î±1, . . . , Î±n âˆˆR and the epochs
t1, . . . , tn âˆˆR for some n âˆˆN. Then there exists an event N âˆˆF of zero probability
such that for all Ï‰ /âˆˆN the mapping t 	â†’X(Ï‰, t) s(t) is a Lebesgue integrable
function:

the mapping t 	â†’X(Ï‰, t) s(t) is in L1

,
Ï‰ /âˆˆN,
(25.71a)
and the mapping from Î© to R
Ï‰ 	â†’
â§
âª
â¨
âª
â©
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt +
n

Î½=1
Î±Î½X(Ï‰, tÎ½)
if Ï‰ /âˆˆN,
0
otherwise
(25.71b)
is a Gaussian RV whose mean and variance are given in (25.65) and (25.70).
Moreover, there is a RV that is measurable with respect to the Ïƒ-algebra generated
by

X(t)

and that is equal to the RV in (25.71b) with probability one.10
10In the future, when we write (25.63) we shall refer to such a RV.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

588
Continuous-Time Stochastic Processes
Proof. We prove this result when

X(t)

is centered. The extension to the more
general case follows by noting that adding a deterministic constant to a zero-mean
Gaussian results in a Gaussian. We also assume that s(Â·) is Borel measurable,
because once the theorem is established for this case it immediately also extends
to the case where s(Â·) is only Lebesgue measurable by noting that every Lebesgue
measurable function is equal almost everywhere to a Borel measurable function.
The existence of the event N and the fact that the mapping (25.71b) is a RV follow
from Proposition 25.10.1. We next show that the RV
Y (Ï‰) â‰œ
â§
âª
â¨
âª
â©
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt +
n

Î½=1
Î±Î½X(Ï‰, tÎ½)
if Ï‰ /âˆˆN,
0
otherwise,
(25.72)
is Gaussian.
To that end, deï¬ne for every k âˆˆN the function
sk(t) =

s(t)
if |t| â‰¤k and |s(t)| â‰¤
âˆš
k,
0
otherwise.
t âˆˆR.
(25.73)
Note that for every Ï‰ âˆˆÎ©
lim
kâ†’âˆX(Ï‰, t) sk(t) = X(Ï‰, t) s(t),
t âˆˆR,
and
X(Ï‰, t) sk(t)
 â‰¤
X(Ï‰, t) s(t)
,
t âˆˆR,
so, by the Dominated Convergence Theorem and (25.71a),
lim
kâ†’âˆ
 âˆ
âˆ’âˆ
X(Ï‰, t) sk(t) dt =
 âˆ
âˆ’âˆ
X(Ï‰, t) s(t) dt,
Ï‰ /âˆˆN.
(25.74)
Deï¬ne now for every k âˆˆN the RV
Yk(Ï‰) =
â§
âª
â¨
âª
â©
 âˆ
âˆ’âˆ
X(Ï‰, t) sk(t) dt +
n

Î½=1
Î±Î½X(Ï‰, tÎ½)
if Ï‰ /âˆˆN,
0
otherwise.
(25.75)
It follows from (25.74) that the sequence Y1, Y2, . . . converges almost surely to Y .
To prove that Y is Gaussian, it thus suï¬ƒces to prove that for every k âˆˆN the RV
Yk is Gaussian (Theorem 19.9.1).
To prove that Yk is Gaussian, we begin by showing that it is of ï¬nite variance. To
that end, it suï¬ƒces to show that the RV
ËœYk(Ï‰) â‰œ
 âˆ
âˆ’âˆX(Ï‰, t) sk(t) dt
if Ï‰ /âˆˆN,
0
otherwise
(25.76)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.11 Linear Functionals of Gaussian Processes
589
is of ï¬nite variance. We prove this by using the deï¬nition of sk(Â·) (25.73) and by
using the Cauchy-Schwarz Inequality to show that for every Ï‰ /âˆˆN
ËœY 2
k (Ï‰) =
	 âˆ
âˆ’âˆ
X(Ï‰, t) sk(t) dt

2
=
	 k
âˆ’k
X(Ï‰, t) sk(t) dt

2
â‰¤
 k
âˆ’k
X2(Ï‰, t) dt
 k
âˆ’k
s2
k(t) dt
â‰¤
	 k
âˆ’k
X2(Ï‰, t) dt

2k2,
where the equality in the ï¬rst line follows from the deï¬nition of ËœYk (25.76); the
equality in the second line from the deï¬nition of sk(Â·) (25.73); the inequality in the
third line from the Cauchy-Schwarz Inequality; and the ï¬nal inequality again by
(25.73). Since N is an event of probability zero, it follows from this inequality that
E
 ËœY 2
k

â‰¤4k3 KXX(0) < âˆ,
thus establishing that ËœYk, and hence also Yk, is of ï¬nite variance.
To prove that Yk is Gaussian, we shall use some results about the Hilbert space
L2(Î©, F, P) of (the equivalence classes of) the random variables that are deï¬ned
over (Î©, F, P) and that have a ï¬nite second moment; see, for example, (Shiryaev,
1996, Chapter II, Section 11). Let G denote the closed linear subspace of L2(Î©, F, P)
that is generated by the random variables

X(t), t âˆˆR

. Thus, G contains all ï¬-
nite linear combinations of the random variables

X(t), t âˆˆR

as well as the
mean-square limits of such linear combinations. Since the process

X(t), t âˆˆR

is
Gaussian, all such linear combinations are Gaussian. And since mean-square lim-
its of Gaussian random variables are Gaussian (Theorem 19.9.1), it follows that G
contains only random variables that have a Gaussian distribution (Shiryaev, 1996,
Chapter II, Section 13, Paragraph 6). To prove that Yk is Gaussian, it thus suï¬ƒces
to show that it is an element of G.
To prove that Yk is an element of G, decompose Yk as
Yk = Y G
k + Y âŠ¥
k ,
(25.77)
where Y G
k is the projection of Yk onto G and where Y âŠ¥
k is consequently perpendic-
ular to every element of G and a fortiori to all the random variables

X(t), t âˆˆR

:
E

X(t)Y âŠ¥
k

= 0,
t âˆˆR.
(25.78)
Since Yk is of ï¬nite variance, this decomposition is possible and
E
%
Y G
k
2&
, E
%
Y âŠ¥
k
2&
< âˆ.
(25.79)
To prove that Yk is an element of G, we shall next show that
E
%
Y âŠ¥
k
2&
= 0,
(25.80)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

590
Continuous-Time Stochastic Processes
or, equivalently (in view of (25.77)), that
E

YkY âŠ¥
k

= 0.
(25.81)
To establish (25.81), we evaluate its LHS as follows:
E

YkY âŠ¥
k

= E
	 âˆ
âˆ’âˆ
X(t) sk(t) dt +
n

Î½=1
Î±Î½X(tÎ½)

Y âŠ¥
k

= E
	  âˆ
âˆ’âˆ
X(t) sk(t) dt

Y âŠ¥
k

+
n

Î½=1
Î±Î½ E

X(tÎ½) Y âŠ¥
k




0
= E
	 âˆ
âˆ’âˆ
X(t) sk(t) dt

Y âŠ¥
k

=
 âˆ
âˆ’âˆ
E

X(t) sk(t) Y âŠ¥
k

dt
=
 âˆ
âˆ’âˆ
E

X(t) Y âŠ¥
k




0
sk(t) dt
= 0,
where the ï¬rst equality follows from the deï¬nition of Yk (25.75); the second from
the linearity of expectation; the third from the orthogonality (25.78); the fourth by
an application of Fubiniâ€™s Theorem that we shall justify shortly; the ï¬fth because
sk(Â·) is a deterministic function; and the ï¬nal equality again by (25.78).
This
establishes (25.81) subject to a veriï¬cation that the conditions of Fubiniâ€™s Theorem
are satisï¬ed, a veriï¬cation we conduct now. That (Ï‰, t) 	â†’X(Ï‰, t) Y âŠ¥
k (Ï‰) sk(t) is
measurable follows because

X(t), t âˆˆR

is a measurable SP; Y âŠ¥
k , being a RV,
is measurable with respect to F; and because the Borel measurability of s(Â·) also
implies the Borel measurability of sk(Â·). The integrability of this function follows
from the Cauchy-Schwarz Inequality for random variables
 âˆ
âˆ’âˆ
E
%X(t) Y âŠ¥
k

&
|sk(t)| dt â‰¤
 âˆ
âˆ’âˆ

E[X2(t)]
4
E
%
Y âŠ¥
k
2&
|sk(t)| dt
â‰¤

KXX(0)
4
E
%
Y âŠ¥
k
2&
2k
âˆš
k
< âˆ,
where the second inequality follows from the deï¬nition of sk(Â·) (25.73), and where
the third inequality follows from (25.79). This justiï¬es the use of Fubiniâ€™s Theorem
in the proof of (25.81). We have thus demonstrated that Yk is in G, and hence, like
all elements of G, is Gaussian. This concludes the proof of the Gaussianity of Yk
for every k âˆˆN and hence the Gaussianity of Y .
We next verify that the mean and variance of Y are as stated in the theorem. The
only part of the derivation of (25.70) that we have not yet justiï¬ed is the derivation
of (25.69) and, in particular, the swapping of the expectation and integration. But
this is easily justiï¬ed using Fubiniâ€™s Theorem because, by (25.18),
 âˆ
âˆ’âˆ
E
X(tÎ½) X(t)

|s(t)| dt â‰¤

KXX(0) + E[X(0)]2
âˆ¥sâˆ¥1 < âˆ.
(25.82)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.12 The Joint Distribution of Linear Functionals
591
We now conclude the proof by showing that there exists a RV that is measurable
with respect to the Ïƒ-algebra generated by

X(t)

and that is equal to Y with
probability one. The elements of G are all (equivalence classes of) random variables
that are measurable with respect to the Ïƒ-algebra generated by

X(t)

, because
they are all linear combinations of the random variables

X(t), t âˆˆR

or limits
thereof. Consequently, by (25.77) and (25.80), the RV Yk is equal with probability
one to a RV that is measurable with respect to the Ïƒ-algebra generated by

X(t)

.
Since Yk converges almost surely to Y , the same must be true of Y .
Proposition 25.11.1 is extremely powerful because it allows us to determine the
distribution of a linear functional of a Gaussian SP from its mean and variance.
In the next section we shall extend this result and show that any ï¬nite number of
linear functionals of a Gaussian SP are jointly Gaussian. Their joint distribution
is thus fully determined by the mean vector and the covariance matrix, which, as
we shall see, can be readily computed from the autocovariance function.
25.12
The Joint Distribution of Linear Functionals
Let us now shift our focus from the distribution of a single linear functional to the
joint distribution of a collection of such functionals. Speciï¬cally, we consider m
functionals
 âˆ
âˆ’âˆ
X(t) sj(t) dt +
nj

Î½=1
Î±j,Î½X

tj,Î½

,
j = 1, . . . , m
(25.83)
of the measurable, stationary Gaussian SP

X(t)

. Here the m real-valued sig-
nals s1, . . . , sm are integrable, n1, . . . , nm are in N, and Î±j,Î½, tj,Î½ are deterministic
constants for all Î½ âˆˆ{1, . . . , nj}.
The main result of this section is that if

X(t)

is a Gaussian SP, then the random
variables in (25.83) are jointly Gaussian.
Theorem 25.12.1 (Linear Functionals of a Gaussian SP Are Jointly Gaussian).
The m linear functionals
 âˆ
âˆ’âˆ
X(t) sj(t) dt +
nj

Î½=1
Î±j,Î½X

tj,Î½

,
j = 1, . . . , m
of a measurable, stationary, Gaussian SP

X(t)

are jointly Gaussian, whenever
m âˆˆN; the m functions {sj}m
j=1 are integrable functions from R to R; the inte-
gers {nj} are nonnegative; and the coeï¬ƒcients {Î±j,Î½} and the epochs {tj,Î½} are
deterministic real numbers for all j âˆˆ{1, . . . , m} and all Î½ âˆˆ{1, . . . , nj}.
Proof. It suï¬ƒces to show that any linear combination of these linear function-
als has a univariate Gaussian distribution (Theorem 23.6.17). This follows from
Proposition 25.11.1 and Lemma 25.10.3 because, by Lemma 25.10.3, for any choice
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

592
Continuous-Time Stochastic Processes
of the coeï¬ƒcients Î³1, . . . , Î³m âˆˆR the linear combination
Î³1
	 âˆ
âˆ’âˆ
X(t) s1(t) dt +
n1

Î½=1
Î±1,Î½X

t1,Î½

+ Â· Â· Â·
+ Î³m
	 âˆ
âˆ’âˆ
X(t) sm(t) dt +
nm

Î½=1
Î±m,Î½X

tm,Î½

has the same distribution as the linear functional
 âˆ
âˆ’âˆ
X(t)
	 m

j=1
Î³j sj(t)

dt +
m

j=1
nj

Î½=1
Î³jÎ±j,Î½X

tj,Î½

,
which, by Proposition 25.11.1, has a univariate Gaussian distribution.
It follows from Theorem 25.12.1 that if

X(t)

is a measurable, stationary, Gaussian
SP, then the joint distribution of the random variables in (25.83) is fully speciï¬ed
by their means and their covariance matrix. If

X(t)

is centered, then by (25.65)
these random variables are centered, so their joint distribution is determined by
their covariance matrix. We next show how this covariance matrix can be computed
from the autocovariance function KXX.
To this end we assume that

X(t)

is
centered, and expand the covariance between any two such functionals as follows:
Cov
5 âˆ
âˆ’âˆ
X(t) sj(t) dt +
nj

Î½=1
Î±j,Î½X(tj,Î½),
 âˆ
âˆ’âˆ
X(t) sk(t) dt +
nk

Î½â€²=1
Î±k,Î½â€²X(tk,Î½â€²)
6
= Cov
 âˆ
âˆ’âˆ
X(t) sj(t) dt,
 âˆ
âˆ’âˆ
X(t) sk(t) dt

+
nj

Î½=1
Î±j,Î½Cov

X(tj,Î½),
 âˆ
âˆ’âˆ
X(t) sk(t) dt

+
nk

Î½â€²=1
Î±k,Î½â€²Cov

X(tk,Î½â€²),
 âˆ
âˆ’âˆ
X(t) sj(t) dt

+
nj

Î½=1
nk

Î½â€²=1
Î±j,Î½Î±k,Î½â€²Cov

X(tj,Î½), X(tk,Î½â€²)

,
j, k âˆˆ{1, . . . , m}. (25.84)
The second and third terms on the RHS can be computed from the autocovariance
function KXX using (25.69). The fourth term can be computed from KXX by noting
that Cov[X(tj,Î½), X(tk,Î½â€²)] = KXX(tj,Î½ âˆ’tk,Î½â€²) (Deï¬nition 25.4.4). We now evaluate
the ï¬rst term:
Cov
 âˆ
âˆ’âˆ
X(t) sj(t) dt,
 âˆ
âˆ’âˆ
X(t) sk(t) dt

= E
 âˆ
âˆ’âˆ
X(t) sj(t) dt
 âˆ
âˆ’âˆ
X(Ï„) sk(Ï„) dÏ„

= E
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
X(t) sj(t) X(Ï„) sk(Ï„) dt dÏ„

.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.12 The Joint Distribution of Linear Functionals
593
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
E[X(t) X(Ï„)] sj(t) sk(Ï„) dt dÏ„
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
KXX(t âˆ’Ï„) sj(t) sk(Ï„) dt dÏ„,
(25.85)
which is the generalization of (25.55). By changing variables from (t, Ï„) to (t, Ïƒ),
where Ïƒ â‰œtâˆ’Ï„, we can obtain the generalization of (25.56). Starting from (25.85)
Cov
 âˆ
âˆ’âˆ
X(t) sj(t) dt,
 âˆ
âˆ’âˆ
X(t) sk(t) dt

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
KXX(t âˆ’Ï„) sj(t) sk(Ï„) dt dÏ„
=
 âˆ
âˆ’âˆ
KXX(Ïƒ)
 âˆ
âˆ’âˆ
sj(t) sk(t âˆ’Ïƒ) dt dÏƒ
=
 âˆ
âˆ’âˆ
KXX(Ïƒ)
 âˆ
âˆ’âˆ
sj(t)~sk(Ïƒ âˆ’t) dt dÏƒ
=
 âˆ
âˆ’âˆ
KXX(Ïƒ)

sj â‹†~sk

(Ïƒ) dÏƒ.
(25.86)
If

X(t)

is of PSD SXX, then we can rewrite (25.86) in the frequency domain
using Proposition 6.2.4 in much the same way that we rewrote (25.47) in the form
(25.49):
Cov
 âˆ
âˆ’âˆ
X(t) sj(t) dt,
 âˆ
âˆ’âˆ
X(t) sk(t) dt

=
 âˆ
âˆ’âˆ
SXX(f) Ë†sj(f) Ë†sâˆ—
k(f) df, (25.87)
where we have used the fact that the FT of sj â‹†~sk is the product of the FT of sj
and the FT of ~sk, and that the FT of ~sk is f 	â†’Ë†sk(âˆ’f), which, because sk is real,
is also given by f 	â†’Ë†sâˆ—
k(f).
The key second-order properties of linear functionals of measurable WSS stochastic
processes are summarized in the following theorem. Using these properties and
(25.84) we can compute the covariance matrix of the linear functionals in (25.83),
a matrix which fully speciï¬es their joint distribution whenever

X(t)

is a centered
Gaussian SP.
Theorem 25.12.2 (Covariance Properties of Linear Functionals of a WSS SP).
Let

X(t)

be a measurable WSS SP.
(i) If the real signal s is integrable, then
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

=
 âˆ
âˆ’âˆ
KXX(Ïƒ) Rss(Ïƒ) dÏƒ,
(25.88)
where Rss is the self-similarity function of s. Furthermore, for every ï¬xed
epoch Ï„ âˆˆR
Cov
 âˆ
âˆ’âˆ
X(t) s(t) dt, X(Ï„)

=
 âˆ
âˆ’âˆ
s(t) KXX(Ï„ âˆ’t) dt,
Ï„ âˆˆR.
(25.89)
If s1, s2 are real-valued integrable signals, then
Cov
 âˆ
âˆ’âˆ
X(t) s1(t) dt,
 âˆ
âˆ’âˆ
X(t) s2(t) dt

=
 âˆ
âˆ’âˆ
KXX(Ïƒ)

s1 â‹†~s2

(Ïƒ) dÏƒ.
(25.90)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

594
Continuous-Time Stochastic Processes
(ii) If

X(t)

is of PSD SXX, then for s, s1, s2, and Ï„ as above
Var
 âˆ
âˆ’âˆ
X(t) s(t) dt

=
 âˆ
âˆ’âˆ
SXX(f)
Ë†s(f)
2 df,
(25.91)
Cov
 âˆ
âˆ’âˆ
X(t) s(t) dt, X(Ï„)

=
 âˆ
âˆ’âˆ
SXX(f) Ë†s(f) ei2Ï€fÏ„ df,
(25.92)
and
Cov
 âˆ
âˆ’âˆ
X(t) s1(t) dt,
 âˆ
âˆ’âˆ
X(t) s2(t) dt

=
 âˆ
âˆ’âˆ
SXX(f) Ë†s1(f) Ë†sâˆ—
2(f) df.
(25.93)
Proof. Most of these claims have already been proved. Indeed, (25.88) was proved
in Proposition 25.10.1 (vi), and (25.89) was proved in Proposition 25.11.1 using
Fubiniâ€™s Theorem and (25.82).
However, (25.90) was only derived heuristically
in (25.85) and (25.86). To rigorously justify this derivation one can use Fubiniâ€™s
Theorem, or use the relation
Cov[X, Y ] = 1
2

Var[X + Y ] âˆ’Var[X] âˆ’Var[Y ]

and the result for the variance, namely, (25.88).
All the results in Part (ii) of this theorem follow from the corresponding results in
Part (i) using the deï¬nition of the PSD and Proposition 6.2.4.
25.13
Filtering WSS Processes
We next discuss the result of passing a WSS SP through a stable ï¬lter, i.e., the
convolution of a SP with a deterministic integrable function. Our main result is
that, subject to some technical conditions, the following hold:
(i) Passing a WSS SP through a stable ï¬lter produces a WSS SP.
(ii) If the input to the ï¬lter is of PSD SXX, then the output of the ï¬lter is of
PSD f 	â†’SXX(f) |Ë†h(f)|2, where Ë†h(Â·) is the ï¬lterâ€™s frequency response.
(iii) If the input to the ï¬lter is a Gaussian SP, then so is the output.
We state this result in Theorem 25.13.2. But ï¬rst we must deï¬ne the convolution
of a SP with an integrable deterministic signal. Our approach is to build on our
deï¬nition of linear functionals of WSS stochastic processes (Section 25.10) and to
deï¬ne the convolution of

X(t)

with h(Â·) as the SP that maps every epoch t âˆˆR
to the RV
 âˆ
âˆ’âˆ
X(Ïƒ) h(t âˆ’Ïƒ) dÏƒ,
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.13 Filtering WSS Processes
595
h(Â·)

X(t)


X(t)

â‹†h
Figure 25.2: Passing a SP

X(t)

through a stable ï¬lter of impulse response h(Â·).
If

X(t)

is measurable and WSS, then so is the output

X(t)

â‹†h. If, additionally,

X(t)

is of PSD SXX, then the output is of PSD f 	â†’SXX(f) |Ë†h(f)|2. If

X(t)

is
additionally Gaussian, then so is the output.
where the above integral is the linear functional
 âˆ
âˆ’âˆ
X(Ïƒ) s(Ïƒ) dÏƒ
with
s: Ïƒ 	â†’h(t âˆ’Ïƒ).
With this approach the key results will follow by applying Theorem 25.12.2 with
the proper substitutions.
Deï¬nition 25.13.1 (Filtering a Stochastic Process). The convolution of a mea-
surable, WSS SP

X(t)

with an integrable function h: R â†’R is denoted by

X(t)

â‹†h
and is deï¬ned as the SP that maps every t âˆˆR to the RV
 âˆ
âˆ’âˆ
X(Ïƒ) h(t âˆ’Ïƒ) dÏƒ,
(25.94)
where the stochastic integral in (25.94) is the stochastic integral that was deï¬ned
in Note 25.10.2

X(t)

â‹†h: (Ï‰, t) 	â†’
â§
â¨
â©
 âˆ
âˆ’âˆ
X(Ï‰, Ïƒ) h(t âˆ’Ïƒ) dÏƒ
if
 âˆ
âˆ’âˆ
X(Ï‰, Ïƒ) h(t âˆ’Ïƒ)
 dÏƒ < âˆ,
0
otherwise.
Theorem 25.13.2. Let

Y (t)

be the result of convolving the measurable, cen-
tered, WSS SP

X(t)

of autocovariance function KXX with the integrable function
h: R â†’R.
(i) The SP

Y (t)

is centered, measurable, and WSS with autocovariance func-
tion
KYY = KXX â‹†Rhh,
(25.95)
where Rhh is the self-similarity function of h (Section 11.4).
(ii) If

X(t)

is of PSD SXX, then

Y (t)

is of PSD
SYY (f) =
Ë†h(f)
2 SXX(f),
f âˆˆR.
(25.96)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

596
Continuous-Time Stochastic Processes
(iii) For every t, Ï„ âˆˆR,
E

X(t) Y (t + Ï„)

=

KXX â‹†h

(Ï„),
(25.97)
where the RHS does not depend on t.11
(iv) If

X(t)

is Gaussian, then so is

Y (t)

.
Moreover, for every choice of
n, m âˆˆN and for every choice of the epochs t1, . . . , tn, tn+1, . . . , tn+m âˆˆR,
the random variables
X(t1), . . . , X(tn), Y (tn+1), . . . , Y (tn+m)
(25.98)
are jointly Gaussian.12
Proof. For ï¬xed t, Ï„ âˆˆR we use Deï¬nition 25.13.1 to express Y (t) and Y (t + Ï„) as
Y (t) =
 âˆ
âˆ’âˆ
X(Ïƒ) s1(Ïƒ) dÏƒ,
(25.99)
and
Y (t + Ï„) =
 âˆ
âˆ’âˆ
X(Ïƒ) s2(Ïƒ) dÏƒ,
(25.100)
where
s1 : Ïƒ 	â†’h(t âˆ’Ïƒ),
(25.101)
s2 : Ïƒ 	â†’h(t + Ï„ âˆ’Ïƒ).
(25.102)
We are now ready to prove Part (i).
That

Y (t)

is centered follows from the
representation of Y (t) in (25.99) & (25.101) as a linear functional of

X(t)

and
from the hypothesis that

X(t)

is centered (Proposition 25.10.1).
To establish that

Y (t)

is WSS we use the representations (25.99)â€“(25.102) and
Theorem 25.12.2 regarding the covariance between two linear functionals as follows.
Cov

Y (t + Ï„), Y (t)

= Cov
 âˆ
âˆ’âˆ
X(Ïƒ) s2(Ïƒ) dÏƒ,
 âˆ
âˆ’âˆ
X(Ïƒ) s1(Ïƒ) dÏƒ

=
 âˆ
âˆ’âˆ
KXX(Ïƒ)

s2 â‹†~s1

(Ïƒ) dÏƒ,
(25.103)
where the convolution can be evaluated as

s2 â‹†~s1

(Ïƒ) =
 âˆ
âˆ’âˆ
s2(Î¼)~s1(Ïƒ âˆ’Î¼) dÎ¼
=
 âˆ
âˆ’âˆ
h(t + Ï„ âˆ’Î¼) h(t + Ïƒ âˆ’Î¼) dÎ¼
=
 âˆ
âˆ’âˆ
h(ËœÎ¼ + Ï„ âˆ’Ïƒ) h(ËœÎ¼) dËœÎ¼
= Rhh(Ï„ âˆ’Ïƒ),
(25.104)
11Two stochastic processes

X(t)

and

Y (t)

are said to be jointly wide-sense stationary
if each is WSS and if E[X(t)Y (t + Ï„)] does not depend on t.
12That is,

X(t)

and

Y (t)

are jointly Gaussian stochastic processes.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.13 Filtering WSS Processes
597
where ËœÎ¼ â‰œt + Ïƒ âˆ’Î¼. Combining (25.103) with (25.104) yields
Cov

Y (t + Ï„), Y (t)

=

KXX â‹†Rhh

(Ï„),
t, Ï„ âˆˆR,
(25.105)
where the RHS does not depend on t. This establishes that

Y (t)

is WSS and
proves (25.95).13
To conclude the proof of Part (i) we now show that

Y (t)

is measurable. The proof
is technical and requires background in Measure Theory. Readers are encouraged
to skip it and move on to the proof of Part (ii).
We ï¬rst note that, as in the proof of Proposition 25.10.1, it suï¬ƒces to prove the
result for impulse response functions h that are Borel measurable; the extension
to Lebesgue measurable functions will then follow by approximating h by a Borel
measurable function that diï¬€ers from it on a set of Lebesgue measure zero (Rudin,
1987, Chapter 8, Lemma 1 or Chapter 2, Exercise 14) and by then applying Part (ii)
of Note 25.10.2. We hence now assume that h is Borel measurable.
We shall prove that

Y (t)

is measurable by proving that the (nonstationary)
process (Ï‰, t) 	â†’Y (Ï‰, t)/(1 + t2) is measurable. This we shall prove using Fubiniâ€™s
Theorem applied to the function from (Î© Ã— R) Ã— R to R deï¬ned by

(Ï‰, t), Ïƒ

	â†’X(Ï‰, Ïƒ) h(t âˆ’Ïƒ)
1 + t2
,

(Ï‰, t) âˆˆÎ© Ã— R, Ïƒ âˆˆR

.
(25.106)
This function is measurable because, by assumption,

X(t)

is measurable and
because the measurability of the function h(Â·) implies the measurability of the
function (t, Ïƒ) 	â†’h(t âˆ’Ïƒ) (as proved, for example, in (Rudin, 1987, p. 171)). We
next verify that this function is integrable.
To that end, we ï¬rst integrate its
absolute value over (Ï‰, t) and then over Ïƒ. The integral over (Ï‰, t) is given by
 âˆ
t=âˆ’âˆ
E[|X(Ïƒ)|] |h(t âˆ’Ïƒ)|
1 + t2
dt â‰¤

KXX(0)
 âˆ
t=âˆ’âˆ
|h(t âˆ’Ïƒ)|
1 + t2
dt,
where the inequality follows from (25.17) and from our assumption that

X(t)

is
centered. We next need to integrate the RHS over Ïƒ. Invoking Fubiniâ€™s Theorem
to exchange the order of integration over t and Ïƒ we obtain that the integral of the
absolute value of the function deï¬ned in (25.106) is upper-bounded by
 âˆ
Ïƒ=âˆ’âˆ

KXX(0)
 âˆ
t=âˆ’âˆ
|h(t âˆ’Ïƒ)|
1 + t2
dt dÏƒ =

KXX(0)
 âˆ
t=âˆ’âˆ
 âˆ
Ïƒ=âˆ’âˆ
|h(t âˆ’Ïƒ)|
1 + t2
dÏƒ dt
=

KXX(0)
 âˆ
t=âˆ’âˆ
âˆ¥hâˆ¥1
1 + t2 dt
= Ï€

KXX(0) âˆ¥hâˆ¥1
< âˆ.
Having established that the function in (25.106) is measurable and integrable, we
can now use Fubiniâ€™s Theorem to deduce that its integral over Ïƒ is measurable as
13That

Y (t)

is of ï¬nite variance follows from (25.105) by setting Ï„ = 0 and noting that
the convolution on the RHS of (25.105) is between a bounded function (KXX) and an integrable
function (Rhh) and is thus deï¬ned and ï¬nite at every Ï„ âˆˆR and a fortiori at Ï„ = 0.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

598
Continuous-Time Stochastic Processes
a mapping of (Ï‰, t), i.e., that the mapping
(Ï‰, t) 	â†’
 âˆ
Ïƒ=âˆ’âˆ
X(Ï‰, Ïƒ) h(t âˆ’Ïƒ)
1 + t2
dÏƒ
(25.107)
is measurable. This mapping is no other than the mapping (Ï‰, t) 	â†’Y (Ï‰, t)/(1+t2),
so the latter must also be measurable, and hence also (Ï‰, t) 	â†’Y (Ï‰, t).
We next prove Part (ii) using (25.95) and Proposition 6.2.5. Because h is integrable,
its self-similarity function Rhh is integrable and of FT
Ë†Rhh(f) =
Ë†h(f)
2,
f âˆˆR
(25.108)
(Section 11.4). And since, by assumption,

X(t)

is of PSD SXX, it follows that SXX
is integrable and that its IFT is KXX:
KXX(Ï„) =
 âˆ
âˆ’âˆ
SXX(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR.
(25.109)
Consequently, by Proposition 6.2.5,

KXX â‹†Rhh

(Ï„) =
 âˆ
âˆ’âˆ
Ë†h(f)
2 SXX(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR.
Combining this with (25.95) yields
KYY (Ï„) =
 âˆ
âˆ’âˆ
Ë†h(f)
2 SXX(f) ei2Ï€fÏ„ df,
Ï„ âˆˆR,
and thus establishes that the PSD of

Y (t)

is as given in (25.96).
We next turn to Part (iii). To establish (25.97) we use the representation (25.100)
& (25.102) and Theorem 25.12.2:
E

X(t) Y (t + Ï„)

= Cov

X(t),
 âˆ
âˆ’âˆ
X(Ïƒ) s2(Ïƒ) dÏƒ

=
 âˆ
âˆ’âˆ
s2(Ïƒ) KXX(t âˆ’Ïƒ) dÏƒ
=
 âˆ
âˆ’âˆ
h(t + Ï„ âˆ’Ïƒ) KXX(t âˆ’Ïƒ) dÏƒ
=
 âˆ
âˆ’âˆ
KXX(âˆ’Î¼) h(Ï„ âˆ’Î¼) dÎ¼
=
 âˆ
âˆ’âˆ
KXX(Î¼) h(Ï„ âˆ’Î¼) dÎ¼
=

KXX â‹†h

(Ï„),
Ï„ âˆˆR,
where Î¼ â‰œÏƒ âˆ’t, and where we have used the symmetry of the autocovariance
function.
Finally, we prove Part (iv). The proof is a simple application of Theorem 25.12.1.
To prove that

Y (t)

is a Gaussian process we need to show that, for every pos-
itive integer n and for every choice of the epochs t1, . . . , tn, the random variables
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.13 Filtering WSS Processes
599
Y (t1), . . . , Y (tn) are jointly Gaussian. This follows directly from Theorem 25.12.1
because Y (tÎ½) can be expressed as
Y (tÎ½) =
 âˆ
âˆ’âˆ
X(Ïƒ) h(tÎ½ âˆ’Ïƒ) dÏƒ
=
 âˆ
âˆ’âˆ
X(Ïƒ) sÎ½(Ïƒ) dÏƒ,
Î½ = 1, . . . , n,
where
sÎ½ : Ïƒ 	â†’h(tÎ½ âˆ’Ïƒ),
Î½ = 1, . . . , n
are all integrable.
The joint Gaussianity of the random variables in (25.98) can also be deduced from
Theorem 25.12.1. Indeed, X(tÎ½) can be trivially expressed as the functional
X(tÎ½) =
 âˆ
âˆ’âˆ
X(Ïƒ) sÎ½(Ïƒ) dÏƒ + Î±Î½X(tÎ½),
Î½ = 1, . . . , n
when sÎ½ is chosen to be the zero function and when Î±Î½ is chosen as 1, and Y (tÎ½)
can be similarly expressed as
Y (tÎ½) =
 âˆ
âˆ’âˆ
X(Ïƒ) sÎ½(Ïƒ) dÏƒ + Î±Î½X(tÎ½),
Î½ = n + 1, . . . , n + m
when sÎ½ : Ïƒ 	â†’h(tÎ½ âˆ’Ïƒ) and Î±Î½ = 0.
The mathematically astute reader may have noted that, in deï¬ning the result of
passing a WSS SP through a stable ï¬lter of impulse response h, we did not preclude
the possibility that for every Ï‰ there may be some epochs t for which the mapping
Ïƒ 	â†’X(Ï‰, Ïƒ) h(t âˆ’Ïƒ) is not integrable.
So far, we have only established that
for every epoch t the set Nt of Ï‰â€™s for which this mapping is not integrable is of
probability zero.
We next show that if h is well-behaved in the sense that it is not only integrable
but also satisï¬es
 âˆ
âˆ’âˆ
h2(t) (1 + t2) dt < âˆ,
(25.110)
then whenever Ï‰ is outside some set N âŠ‚Î© of probability zero, the mapping
Ïƒ 	â†’X(Ï‰, Ïƒ) h(t âˆ’Ïƒ) is integrable for all t âˆˆR. Thus, for Ï‰â€™s outside this set of
probability zero, we can think of the response of the ï¬lter as being the convolution
of the trajectory t 	â†’X(Ï‰, t) and the impulse response t 	â†’h(t). For such Ï‰â€™s this
convolution never blows up.
To prove this note that if h satisï¬es (25.110) and if the trajectory t 	â†’X(Ï‰, t)
satisï¬es
 âˆ
âˆ’âˆ
X2(Ï‰, t)
1 + t2
dt < âˆ,
(25.111)
then the function Ïƒ 	â†’X(Ï‰, Ïƒ) h(t âˆ’Ïƒ) is integrable for every t âˆˆR (Proposi-
tion 3.4.4). To prove the claim it thus remains to prove that outside a set of Ï‰â€™s of
probability zero, all the trajectories t 	â†’X(Ï‰, t) satisfy (25.111):
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

600
Continuous-Time Stochastic Processes
Lemma 25.13.3. Let

X(t)

be a WSS measurable SP deï¬ned over the probability
space (Î©, F, P). Then
E
 âˆ
âˆ’âˆ
X2(t)
1 + t2 dt

< âˆ,
(25.112)
and the set
1
Ï‰ âˆˆÎ© :
 âˆ
âˆ’âˆ
X2(Ï‰, t)
1 + t2
dt < âˆ
2
(25.113)
is an event of probability one.
Proof. Since

X(t)

is measurable, the mapping
(Ï‰, t) 	â†’X2(Ï‰, t)
1 + t2
(25.114)
is nonnegative and measurable. By Fubiniâ€™s Theorem it follows that if we deï¬ne
W(Ï‰) â‰œ
 âˆ
âˆ’âˆ
X2(Ï‰, t)
1 + t2
dt,
Ï‰ âˆˆÎ©,
(25.115)
then W is a nonnegative RV taking values in the interval [0, âˆ]. Consequently, the
set {Ï‰ âˆˆÎ© : W(Ï‰) < âˆ} is measurable. Moreover, by Fubiniâ€™s Theorem,
E[W] =
 âˆ
âˆ’âˆ
E
X2(t)
1 + t2

dt
=
 âˆ
âˆ’âˆ
E

X2(t)

1 + t2
dt
= E

X2(0)
  âˆ
âˆ’âˆ
1
1 + t2 dt
= Ï€ E

X2(0)

< âˆ.
Thus, W is a RV taking values in the interval [0, âˆ] and having ï¬nite expectation,
so the event {Ï‰ âˆˆÎ© : W(Ï‰) < âˆ} must be of probability one.
25.14
The PSD Revisited
Theorem 25.13.2 describes the PSD of the output of a stable ï¬lter that is fed a
WSS SP

X(t)

. By integrating this PSD, we obtain the value at the origin of the
autocovariance function of the ï¬lterâ€™s output (see (25.31)). Since the latter is the
power of the ï¬lterâ€™s output (Corollary 25.9.3), we have:
Theorem 25.14.1 (Wiener-Khinchin). If a measurable, centered, WSS SP

X(t)

of autocovariance function KXX is passed through a stable ï¬lter of impulse response
h: R â†’R, then the average power of the ï¬lterâ€™s output is given by
Power of X â‹†h = âŸ¨KXX, RhhâŸ©.
(25.116)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.14 The PSD Revisited
601
If, additionally,

X(t)

is of PSD SXX, then this power is given by
Power of X â‹†h =
 âˆ
âˆ’âˆ
SXX(f)
Ë†h(f)
2 df.
(25.117)
Proof. To prove (25.116), we note that by (25.95) the autocovariance function of
the ï¬ltered process is KXX â‹†Rhh, which evaluates at the origin to (25.116). The
result thus follows from Proposition 25.9.2, which shows that the power in the
ï¬ltered process is given by its autocovariance function evaluated at the origin.
To prove (25.117), we note that KXX is the IFT of SXX and that, by (11.35),
Ë†Rhh(f) = |Ë†h(f)|2, so the RHS of (25.117) is equal to the RHS of (25.116) by
Proposition 6.2.4.
We next show that for WSS stochastic processes, the operational PSD (Deï¬ni-
tion 15.3.1) and the PSD (Deï¬nition 25.7.2) are equivalent. That is, a WSS SP
has an operational PSD if, and only if, it has a PSD, and if the two exist, then they
are equal (outside a set of frequencies of Lebesgue measure zero). Before stating
this as a theorem, we present a lemma that will be needed in the proof. It is very
much in the spirit of Lemma 15.3.5.
Lemma 25.14.2. Let g: R â†’R be a symmetric continuous function satisfying the
condition that for every integrable real signal h: R â†’R
 âˆ
âˆ’âˆ
g(t) Rhh(t) dt = 0.
(25.118)
Then g is the all-zero function.
Proof. For every a > 0 consider the function
h(t) =
1
âˆša I{|t| â‰¤a/2},
t âˆˆR
whose self-similarity function is
Rhh(t) =
	
1 âˆ’|t|
a

I{|t| â‰¤a},
t âˆˆR.
(25.119)
Since h is integrable, it follows from (25.118) that
0 =
 âˆ
âˆ’âˆ
g(t) Rhh(t) dt
= 2
 âˆ
0
g(t) Rhh(t) dt
= 2
 a
0
g(t)

1 âˆ’t
a

dt,
a > 0,
(25.120)
where the second equality follows from the hypothesis that g(Â·) is symmetric and
from the symmetry of Rhh, and where the third equality follows from (25.119).
Deï¬ning
G(t) =
 t
0
g(Î¾) dÎ¾,
t â‰¥0,
(25.121)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

602
Continuous-Time Stochastic Processes
and using integration by parts, we obtain from (25.120) that
0 = G(Î¾)

1 âˆ’Î¾
a

a
0
+ 1
a
 a
0
G(Î¾) dÎ¾,
a > 0,
from which we obtain
aG(0) =
 a
0
G(Î¾) dÎ¾,
a > 0.
Diï¬€erentiating with respect to a yields
G(0) = G(a),
a â‰¥0,
which combines with (25.121) to yield
 a
0
g(t) dt = 0,
a â‰¥0.
(25.122)
Diï¬€erentiating with respect to a and using the continuity of g (Rudin, 1976, Chap-
ter 6, Theorem 6.20) yields that g(a) is zero for all a â‰¥0 and hence, by its
symmetry, for all a âˆˆR.
Theorem 25.14.3 (The PSD and Operational PSD of a WSS SP). Let

X(t)

be a measurable, centered, WSS SP of a continuous autocovariance function KXX.
Let S(Â·) be a nonnegative, symmetric, integrable function. Then the following two
conditions are equivalent:
(a) KXX is the Inverse Fourier Transform of S(Â·).
(b) For every integrable h: R â†’R, the power in X â‹†h is given by
Power of X â‹†h =
 âˆ
âˆ’âˆ
S(f) |Ë†h(f)|2 df.
(25.123)
Proof. That (a) implies (b) follows from the Wiener-Khinchin Theorem because
(a) implies that

X(t)

is of PSD S(Â·). It remains to prove that (b) implies (a). We
thus assume that Condition (b) is satisï¬ed and proceed to prove that KXX must
then be equal to the IFT of S(Â·). By Theorem 25.14.1, the power in X â‹†h is given
by (25.116). Consequently, Condition (b) implies that
 âˆ
âˆ’âˆ
S(f) |Ë†h(f)|2 df =
 âˆ
âˆ’âˆ
KXX(Ï„) Rhh(Ï„) dÏ„,
(25.124)
for every integrable h: R â†’R.
If h is integrable, then the FT of Rhh is the mapping f 	â†’|Ë†h(f)|2 (see (11.35)). If,
in addition, h is a real signal, then Rhh is a symmetric function, and its IFT is thus
identical to its FT (Proposition 6.2.3 (ii)). Thus, if h is real and integrable, then
the IFT of Rhh is the mapping f 	â†’|Ë†h(f)|2. (Using the dummy variable f for the
IFT is unusual but legitimate.) Consequently, by Proposition 6.2.4 (applied with
the substitution of S(Â·) for x and of Rhh for g),
 âˆ
âˆ’âˆ
S(f) |Ë†h(f)|2 df =
 âˆ
âˆ’âˆ
Ë†S(Ï„) Rhh(Ï„) dÏ„.
(25.125)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.15 White Gaussian Noise
603
By (25.124) & (25.125) and by the symmetry of S(Â·) (which implies that Ë†S = Ë‡S)
we obtain that
 âˆ
âˆ’âˆ
Ë‡S(Ï„) âˆ’KXX(Ï„)

Rhh(Ï„) dÏ„ = 0,
h âˆˆL1.
(25.126)
It thus follows from Lemma 15.3.5 that the mapping Ï„ 	â†’Ë‡S(Ï„) âˆ’KXX(Ï„) is the
all-zero function, and Condition (a) is established.
25.15
White Gaussian Noise
The most important continuous-time SP in Digital Communications is white
Gaussian noise (WGN), which is often used to model the additive noise in com-
munication systems. In this section we deï¬ne this process and study its key proper-
ties. Our deï¬nition diï¬€ers from the one in most textbooks, most notably in that we
deï¬ne WGN only with respect to some given bandwidth W. We give our reasons
and comment on the implications in Section 25.15.3 after providing our deï¬nition
and deriving the key results.
25.15.1
Deï¬nition and Main Properties
The parameters deï¬ning WGN are the bandwidth W with respect to which the
process is white and the double-sided power spectral density N0/2.
Deï¬nition 25.15.1 (White Gaussian Noise). We say that

N(t)

is white Gaus-
sian noise of double-sided power spectral density N0/2 with respect to
the bandwidth W if

N(t)

is a measurable, stationary, centered, Gaussian SP
that has a PSD SNN satisfying
SNN(f) = N0
2 ,
f âˆˆ[âˆ’W, W ].
(25.127)
An example of the PSD of white Gaussian noise of double-sided PSD N0/2 with
respect to the bandwidth W is depicted in Figure 25.3. Note that our deï¬nition
of WGN only speciï¬es the PSD for frequencies f satisfying |f| â‰¤W. We leave the
value of the PSD at other frequencies unspeciï¬ed. But the PSD should, of course,
be a valid PSD, i.e., it must be nonnegative, symmetric, and integrable (Deï¬ni-
tion 25.7.2). Recall also that by Proposition 25.7.3 every nonnegative, symmetric,
integrable function is the PSD of some measurable stationary Gaussian SP.14
The following proposition summarizes the key properties of WGN. The reader is
encouraged to recall the deï¬nition of an integrable function that is bandlimited to
W Hz (Deï¬nition 6.4.9); the deï¬nition of the inner product between two energy-
limited real signals (3.1); the deï¬nition of âˆ¥sâˆ¥2 as

âŸ¨s, sâŸ©; and the deï¬nition of
orthonormality of the functions Ï†1, . . . , Ï†m (Deï¬nition 4.6.1).
14As we have noted in the paragraph preceding Deï¬nition 25.9.1, Proposition 25.7.3 can be
strengthened to also guarantee measurability.
Every nonnegative, symmetric, and integrable
function is the PSD of some measurable, stationary, and Gaussian SP whose autocovariance
function is continuous.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

604
Continuous-Time Stochastic Processes
âˆ’W
W
N0/2
f
SNN(f)
Figure 25.3: The PSD of a SP

N(t)

which is of double-sided power spectral
density N0/2 with respect to the bandwidth W.
Proposition 25.15.2 (Key Properties of White Gaussian Noise). Let

N(t)

be
WGN of double-sided PSD N0/2 with respect to the bandwidth W.
(i) If s is any integrable function that is bandlimited to W Hz, then
 âˆ
âˆ’âˆ
N(t) s(t) dt âˆ¼N
	
0, N0
2 âˆ¥sâˆ¥2
2

.
(ii) If s1, . . . , sm are integrable functions that are bandlimited to W Hz, then the
m random variables
 âˆ
âˆ’âˆ
N(t) s1(t) dt, . . . ,
 âˆ
âˆ’âˆ
N(t) sm(t) dt
are jointly Gaussian centered random variables of covariance matrix
N0
2
â›
âœ
âœ
âœ
â
âŸ¨s1, s1âŸ©
âŸ¨s1, s2âŸ©
Â· Â· Â·
âŸ¨s1, smâŸ©
âŸ¨s2, s1âŸ©
âŸ¨s2, s2âŸ©
Â· Â· Â·
âŸ¨s2, smâŸ©
...
...
...
...
âŸ¨sm, s1âŸ©
âŸ¨sm, s2âŸ©
Â· Â· Â·
âŸ¨sm, smâŸ©
â
âŸ
âŸ
âŸ
â .
(iii) If Ï†1, . . . , Ï†m are integrable functions that are bandlimited to W Hz and are
orthonormal, then the random variables
 âˆ
âˆ’âˆ
N(t) Ï†1(t) dt, . . . ,
 âˆ
âˆ’âˆ
N(t) Ï†m(t) dt
are IID N(0, N0/2).
(iv) If s is any integrable function that is bandlimited to W Hz, and if KNN is the
autocovariance function of

N(t)

, then
KNN â‹†s = N0
2 s.
(25.128)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.15 White Gaussian Noise
605
(v) If s is an integrable function that is bandlimited to W Hz, then for every
epoch t âˆˆR
Cov
 âˆ
âˆ’âˆ
N(Ïƒ) s(Ïƒ) dÏƒ, N(t)

= N0
2 s(t).
(25.129)
Proof. Parts (i) and (iii) are special cases of Part (ii), so it suï¬ƒces to prove
Parts (ii), (iv), and (v). We begin with Part (ii), which, as we next show, fol-
lows from Theorem 25.12.1.
We ï¬rst note that since {sj} are assumed to be
integrable and bandlimited to W Hz, and since Note 6.4.12 guarantees that every
bandlimited integrable signal is also of ï¬nite energy, it follows that the functions
{sj} are energy-limited and the inner products âŸ¨sj, skâŸ©are well-deï¬ned. To ap-
ply Theorem 25.12.1 it remains to calculate the covariance between the diï¬€erent
functionals. By (25.93),
Cov
 âˆ
âˆ’âˆ
N(t) sj(t) dt,
 âˆ
âˆ’âˆ
N(t) sk(t) dt

=
 âˆ
âˆ’âˆ
SNN(f) Ë†sj(f) Ë†sâˆ—
k(f) df
=
 W
âˆ’W
SNN(f) Ë†sj(f) Ë†sâˆ—
k(f) df
= N0
2
 W
âˆ’W
Ë†sj(f) Ë†sâˆ—
k(f) df
= N0
2 âŸ¨sj, skâŸ©,
j, k âˆˆ{1, . . . , m},
where the second equality follows because sj and sk are bandlimited to W Hz; the
third from (25.127); and the ï¬nal equality from Parsevalâ€™s Theorem.
To prove Part (iv), we start with the deï¬nition of the convolution and compute

KNN â‹†s

(t) =
 âˆ
âˆ’âˆ
s(Ï„) KNN(t âˆ’Ï„) dÏ„
=
 âˆ
âˆ’âˆ
s(Ï„)
 âˆ
âˆ’âˆ
SNN(f) ei2Ï€f(tâˆ’Ï„) df dÏ„
=
 âˆ
âˆ’âˆ
SNN(f) Ë†s(f) ei2Ï€ft df
=
 W
âˆ’W
SNN(f) Ë†s(f) ei2Ï€ft df
= N0
2
 W
âˆ’W
Ë†s(f) ei2Ï€ft df
= N0
2 s(t),
t âˆˆR,
where the second equality follows from the deï¬nition of the PSD of

N(t)

(Deï¬ni-
tion 25.7.2); the third by Proposition 6.2.5; the fourth because s is, by assumption,
bandlimited to W Hz (Proposition 6.4.10, cf. (c)); the ï¬fth from our assumption
that

N(t)

is white with respect to the bandwidth W (25.127); and the ï¬nal
equality from Proposition 6.4.10 (cf. (b)).
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

606
Continuous-Time Stochastic Processes
Part (v) now follows from (25.89) and Part (iv). Alternatively, it can be proved
using (25.92) and (25.127) as follows:
Cov
 âˆ
âˆ’âˆ
N(Ïƒ) s(Ïƒ) dÏƒ, N(t)

=
 âˆ
âˆ’âˆ
SNN(f) Ë†s(f) ei2Ï€ft df
=
 W
âˆ’W
SNN(f) Ë†s(f) ei2Ï€ft df
= N0
2
 W
âˆ’W
Ë†s(f) ei2Ï€ft df
= N0
2 s(t),
t âˆˆR,
where the ï¬rst equality follows from (25.92); the second because s is bandlimited
to W Hz (Proposition 6.4.10, cf. (c)); the third from (25.127); and the last from
Proposition 6.4.10 (cf. (b)).
25.15.2
Projecting White Gaussian Noise
A natural way to deï¬ne the projection of a stochastic process is pathwise. Ignoring
integrability issues, we would deï¬ne the projection as follows: Suppose we are
given orthonormal deterministic signals Ï†1, . . . , Ï†d and a SP X that we would like
to project onto span(Ï†1, . . . , Ï†d). For each Ï‰ âˆˆÎ© we could consider the sample-
path t 	â†’X(Ï‰, t) and deï¬ne its projection, like we did for deterministic signals (see
Deï¬nition 4.6.6 and Note 4.6.7), as the signal
t 	â†’
d

â„“=1
	 âˆ
âˆ’âˆ
X(Ï‰, Ï„) Ï†â„“(Ï„) dÏ„

Ï†â„“(t),
(25.130a)
i.e., as
t 	â†’
d

â„“=1
âŸ¨X, Ï†â„“âŸ©(Ï‰) Ï†â„“(t).
(25.130b)
Diï¬€erent sample-paths will, of course, have diï¬€erent projections, thus resulting in
the projection being a stochastic process
(Ï‰, t) 	â†’
d

â„“=1
âŸ¨X, Ï†â„“âŸ©(Ï‰) Ï†â„“(t).
(25.130c)
For this to make mathematical sense we need to make sure that the stochastic
integrals in (25.130a) exist (with probability one). We shall therefore limit ourselves
to scenarios where Ï†1, . . . , Ï†d are integrable so that the existence of the integrals
be guaranteed by Proposition 25.10.1. And, of course, for orthonormality to make
sense we shall also require that they be energy-limited. (The latter condition is
superï¬‚uous when they are bandlimited; see Note 6.4.12.)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.15 White Gaussian Noise
607
Deï¬nition 25.15.3 (Projecting a Stochastic Process). If X is a measurable WSS
SP, and if Ï†1, . . . , Ï†d âˆˆL1 âˆ©L2 are orthonormal, then the projection of X onto
span(Ï†1, . . . , Ï†d) is the SP
(Ï‰, t) 	â†’
d

â„“=1
âŸ¨X, Ï†â„“âŸ©(Ï‰) Ï†â„“(t),
which we write more succinctly as
d

â„“=1
âŸ¨X, Ï†â„“âŸ©Ï†â„“.
(25.131)
We refer to
X âˆ’
d

â„“=1
âŸ¨X, Ï†â„“âŸ©Ï†â„“
(25.132)
as the part of X that is orthogonal to span(Ï†1, . . . , Ï†d).
Note 25.15.4. If ËœÏ†1, . . . , ËœÏ†d âˆˆL1 âˆ©L2 are orthonormal and have the same linear
span as Ï†1, . . . , Ï†d, then one can show using the â€œalmostâ€ linearity of stochastic
integration (Lemma 25.10.3) that outside a set of probability zero, the stochastic
processes d
â„“=1âŸ¨X, Ï†â„“âŸ©Ï†â„“and d
â„“=1âŸ¨X, ËœÏ†â„“âŸ©ËœÏ†â„“are identical.
Hence there is no
harm in referring to (25.131) as â€œtheâ€ projection.
Note 25.15.5. Even if X is stationary, its projection onto span(Ï†1, . . . , Ï†d) is
typically not stationary. Nor is its part that is orthogonal to span(Ï†1, . . . , Ï†d).
The following result on the projection of WGN is of paramount importance in dig-
ital communications. It, and its restatement in Theorem 25.15.7, are fundamental
to the design of optimal receivers for communication in the presence of WGN.
Theorem 25.15.6 (Projecting White Gaussian Noise). Let N be WGN of double-
sided PSD N0/2 with respect to the bandwidth W, and let Ï†1, . . . , Ï†d be orthonormal
integrable signals that are bandlimited to W Hz. Then
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“
and
N âˆ’
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“
are independent Gaussian stochastic processes.
Proof. Denote the projection of N onto span(Ï†1, . . . , Ï†d) by N1,
N1 =
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“,
(25.133a)
and the part of N that is orthogonal to span(Ï†1, . . . , Ï†d) by N2,
N2 = N âˆ’
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“.
(25.133b)
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

608
Continuous-Time Stochastic Processes
We need to show that for every n âˆˆN and epochs t1, . . . , tn âˆˆR the random vectors

N1(t1), . . . , N1(tn)
T
and

N2(t1), . . . , N2(tn)
T
(25.134)
are independent Gaussian vectors. To that end, we ï¬rst note that the 2n-vector

N1(t1), . . . , N1(tn), N2(t1), . . . , N2(tn)
T
is Gaussian because its components are linear functionals of the Gaussian SP N
(Theorem 25.12.1). This establishes that the random vectors in (25.134) are Gaus-
sian (Corollary 23.6.5) and are, in fact, jointly Gaussian (Deï¬nition 23.7.1). It
also establishes that to prove their independence it suï¬ƒces to prove that they are
uncorrelated (Proposition 23.7.3). To conclude the proof it thus remains to show
that
Cov

N1(tÎ½), N2(tÎ½â€²)

= 0,
Î½, Î½â€² âˆˆ{1, . . . , n}.
(25.135)
Since both N1(tÎ½) and N2(tÎ½â€²) are linear functionals of the centered SP N, they
are centered (Proposition 25.10.1), and their covariance is thus E[N1(tÎ½) N2(tÎ½â€²)].
To show (25.135) we thus need to show that
E

N1(tÎ½) N2(tÎ½â€²)

= 0,
Î½, Î½â€² âˆˆ{1, . . . , n}
(25.136)
or, more generally, that
E

N1(t) N2(tâ€²)

= 0,
t, tâ€² âˆˆR.
(25.137)
This we proceed to do next:
E

N1(t) N2(tâ€²)

= E
	
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“(t)

	
N(tâ€²) âˆ’
d

â„“â€²=1
âŸ¨N, Ï†â„“â€²âŸ©Ï†â„“â€²(tâ€²)


=
d

â„“=1
Ï†â„“(t) E

âŸ¨N, Ï†â„“âŸ©N(tâ€²)




= N0
2 Ï†â„“(tâ€²)
âˆ’
d

â„“=1
d

â„“â€²=1
Ï†â„“(t) Ï†â„“â€²(tâ€²) E

âŸ¨N, Ï†â„“âŸ©âŸ¨N, Ï†â„“â€²âŸ©




= N0
2
I{â„“=â„“â€²}
= N0
2
d

â„“=1
Ï†â„“(t) Ï†â„“(tâ€²) âˆ’N0
2
d

â„“=1
Ï†â„“(t) Ï†â„“(tâ€²)
= 0,
t, tâ€² âˆˆR,
where the ï¬rst equality follows from the deï¬nitions of N1 and N2 (25.133); the
second by swapping expectations and summations; and the third by the basic
properties of WGN (Proposition 25.15.2).
Theorem 25.15.6 has far-reaching consequences in digital communications. But
before we can reap its beneï¬ts, we must restate it slightly diï¬€erently. To motivate
the restatement, let us write N as the sum of its projection onto span(Ï†1, . . . , Ï†d)
and the part of N that is orthogonal to span(Ï†1, . . . , Ï†d):
N =
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“+
	
N âˆ’
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“

.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.15 White Gaussian Noise
609
In this representation, N is the sum of two independent stochastic processes,
each of which is computed from N: the ï¬rst is computed by projecting N onto
span(Ï†1, . . . , Ï†d), and the second by computing the part of N that is orthogonal
to this projection. To get a sense of the implications of this decomposition, let
us imagine a simpler scenario where stochastic processes are replaced by random
variables and projections by functions. Thus, imagine that we are told that a RV
X is such that X = g(X) + h(X), with g(X) and h(X) being independent. (In
this cartoon X plays the role of N; the mapping g: R â†’R plays the role of the
projection; and h(X) plays the role of the part of N that is orthogonal to the
subspace.) We could then infer that if we generate U independently of g(X) ac-
cording to the distribution of h(X), then the sum g(X) + U would have the same
law as X. How could we generate such a U? One way would be to compute the
distribution of h(X) (from that of X) and to then generate U independently of X
(and hence also independently of g(X)) according to this distribution. But there
is another way: we could generate some Xâ€² of the same law as X but independent
of it, and then set U to equal h(Xâ€²). Since Xâ€² is independent of X, so is h(Xâ€²),
and, because Xâ€² is of the same law as X, it follows that h(Xâ€²) has the same law as
h(X). Adopting this approach we could conclude that g(X) + h(Xâ€²) has the same
law as X whenever Xâ€² is generated independently of X but with same law.
Going back to stochastic processes and our WGN, we would expect the same: if
Nâ€² is generated independently of N but with the same FDDs, then
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“+
	
Nâ€² âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“

should have the same FDDs as N. This is the restatement of the theorem that we
are after:
Theorem 25.15.7 (Simulating WGN of a Given Projection). Let the SP N be
WGN of double-sided PSD N0/2 with respect to the bandwidth W. Let the SP Nâ€²
be of the same law as N but independent of it. Let Ï†1, . . . , Ï†d be orthonormal
integrable signals that are bandlimited to W Hz. Then the SP
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“+ Nâ€² âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“
is a measurable SP of the same FDDs as N.
The heuristic argument we gave above nearly proves that this theorem follows
directly from Theorem 25.15.6.
Some of the missing subtle details are related
to the deï¬nition of independence for stochastic processes (Deï¬nition 25.2.3). For
example, it is prima facie not clear from this deï¬nition that linear functionals of
independent stochastic processes are independent random variables. (They are; see
Proposition 25.15.8 ahead.) And it is therefore not obvious that if Nâ€² and N are
independent, then so are
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“
and
Nâ€² âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

610
Continuous-Time Stochastic Processes
(They are.) The rest of this subsection is dedicated to a rigorous proof of this
theorem.
We begin with the basic result on independent stochastic processes that we need
in order to prove Theorem 25.15.7. The proof is technical, and readers without
some background in Measure Theory should probably skip it.
Proposition 25.15.8 (Linear Functionals of Independent SPs are Independent).
Let

X(t)

and

Y (t)

be independent WSS measurable stochastic processes. Let
sx and sy be integrable functions, and let Î±1, . . . , Î±n, Î²1, . . . , Î²n and t1, . . . , tn be
real. Then
 âˆ
âˆ’âˆ
X(t) sx(t) dt +
n

Î½=1
Î±Î½X(tÎ½)
and
 âˆ
âˆ’âˆ
Y (t) sy(t) dt +
n

Î½=1
Î²Î½Y (tÎ½)
are independent random variables.
More generally, a random vector whose components are such linear functionals of

X(t)

is independent of any random vector whose components are such linear
functionals of

Y (t)

.
Proof. This result is reminiscent of the result that if the random variables U
and V are independent then so are g(U) and h(V ) for any (Borel measurable)
functions g, h: R â†’R. Here things are a bit trickier because we are dealing with
stochastic processes, which are uncountable collections of random variables. The
proof therefore requires some Measure Theory.
The independence of the stochastic processes implies the independence of the Ïƒ-
algebras generated by their cylindrical sets (Billingsley, 1995, Chapter 1, Section 4,
Theorem 4.2). By Proposition 25.10.1 (iii) the above linear functionals are mea-
surable with respect to the respective Ïƒ-algebras. The result now follows by noting
that if a RV U is measurable with respect to a Ïƒ-algebra Fu, if a RV V is measurable
with respect to a Ïƒ-algebra Fv, and if the Ïƒ-algebras Fu and Fv are independent,
then U and V are independent (Billingsley, 1995, Chapter 4, Section 20).
Proof of Theorem 25.15.7. Denote the autocovariance function of N by KNN.
Since Nâ€² has the same FDDs as N, its autocovariance function is also KNN. Denote
the projection of N onto span(Ï†1, . . . , Ï†d) by N1,
N1 =
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“,
(25.138)
and the part of Nâ€² that is orthogonal to span(Ï†1, . . . , Ï†d) by Nâ€²
2,
Nâ€²
2 = Nâ€² âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“.
(25.139)
Because the FDDs of a centered stationary Gaussian SP are fully determined by its
autocovariance function (Proposition 25.5.1 (ii)), to prove the theorem it suï¬ƒces
to establish that, like N, the SP N1 + Nâ€²
2 is a measurable Gaussian stochastic
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.15 White Gaussian Noise
611
process of autocovariance function KNN. We ï¬rst show that N1 + Nâ€²
2 is Gaussian,
by establishing that N1 and Nâ€²
2 are independent Gaussian stochastic processes,
i.e., by arguing that for every n âˆˆN and epochs t1, . . . , tn âˆˆR the random vectors

N1(t1), . . . , N1(tn)
T
and

N â€²
2(t1), . . . , N â€²
2(tn)
T
(25.140)
are independent Gaussian vectors.
Indeed, independence follows from Proposi-
tion 25.15.8 (because the components of the ï¬rst vector are linear functionals of
N, whereas those of the second are linear functionals of Nâ€²), and Gaussianity fol-
lows from Theorem 25.12.1 (because the components of each of the vectors are
linear functionals of a Gaussian SP). Having established that N1 + Nâ€²
2 is a Gaus-
sian SP, it only remains to show that it is stationary with autocovariance function
KNN.
Denote N1 + Nâ€²
2 by ËœN. Since N1 and Nâ€²
2 are independent and centered, it follows
that for all t, Ï„ âˆˆR
E
 ËœN(t + Ï„) ËœN(t)

= E

N1(t + Ï„) N1(t)

+ E

N â€²
2(t + Ï„) N â€²
2(t)

= E
	 d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“(t + Ï„)

	
d

â„“â€²=1
âŸ¨N, Ï†â„“â€²âŸ©Ï†â„“â€²(t)


+ E
	
N â€²(t + Ï„) âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“(t + Ï„)

	
N â€²(t) âˆ’
d

â„“â€²=1
âŸ¨Nâ€², Ï†â„“â€²âŸ©Ï†â„“â€²(t)


.
(25.141)
We next compute the above two terms separately, starting with the ï¬rst:
E
	 d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“(t + Ï„)

	
d

â„“â€²=1
âŸ¨N, Ï†â„“â€²âŸ©Ï†â„“â€²(t)


=
d

â„“=1
d

â„“â€²=1
E
%
âŸ¨N, Ï†â„“âŸ©Ï†â„“(t + Ï„) âŸ¨N, Ï†â„“â€²âŸ©Ï†â„“â€²(t)
&
=
d

â„“=1
d

â„“â€²=1
E
%
âŸ¨N, Ï†â„“âŸ©âŸ¨N, Ï†â„“â€²âŸ©
&



= N0
2
I{â„“=â„“â€²}
Ï†â„“(t + Ï„) Ï†â„“â€²(t)
= N0
2
d

â„“=1
Ï†â„“(t + Ï„) Ï†â„“(t),
t, Ï„ âˆˆR,
(25.142)
where the last equality follows from our assumptions about N and the orthonor-
mality of (Ï†1, . . . , Ï†d) (Proposition 25.15.2 (iii)).
As to the second term on the RHS of (25.141)
E
	
N â€²(t + Ï„) âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“(t + Ï„)

	
N â€²(t) âˆ’
d

â„“â€²=1
âŸ¨Nâ€², Ï†â„“â€²âŸ©Ï†â„“â€²(t)


.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

612
Continuous-Time Stochastic Processes
= E

N â€²(t + Ï„)N â€²(t)

+ E
 	 d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“(t + Ï„)

	
d

â„“â€²=1
âŸ¨Nâ€², Ï†â„“â€²âŸ©Ï†â„“â€²(t)


âˆ’E

N â€²(t + Ï„)
d

â„“â€²=1
âŸ¨Nâ€², Ï†â„“â€²âŸ©Ï†â„“â€²(t)

âˆ’E

N â€²(t)
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“(t + Ï„)

(25.143)
= KNN(Ï„) + N0
2
d

â„“=1
Ï†â„“(t + Ï„) Ï†â„“(t)
âˆ’
d

â„“â€²=1
E
%
N â€²(t + Ï„) âŸ¨Nâ€², Ï†â„“â€²âŸ©
&



= N0
2 Ï†â„“â€²(t+Ï„)
Ï†â„“â€²(t) âˆ’
d

â„“=1
E
%
N â€²(t) âŸ¨Nâ€², Ï†â„“âŸ©
&



= N0
2 Ï†â„“(t)
Ï†â„“(t + Ï„) (25.144)
= KNN(Ï„) âˆ’N0
2
d

â„“=1
Ï†â„“(t + Ï„) Ï†â„“(t),
t, Ï„ âˆˆR.
(25.145)
Here the ï¬rst term on the RHS of (25.143) is computed using the deï¬nition of
the autocovariance function (Deï¬nition 25.4.4), and the second term is computed
as in (25.142) but with N replaced by Nâ€². The last two terms in (25.144) are
computed using Proposition 25.15.2 (v).
It follows from (25.141), (25.142), and (25.145) that the autocovariance function
of ËœN is identical to that of N.
25.15.3
Other Deï¬nitions
As we noted earlier, our deï¬nition of WGN is diï¬€erent from the one given in most
textbooks on Digital Communications. The key diï¬€erence is that we deï¬ne white-
ness with respect to a certain bandwidth W, whereas most textbooks do not add this
qualiï¬er. Thus, while we require that the PSD SNN(f) be equal to N0/2 only for
frequencies f satisfying |f| â‰¤W (leaving SNN(f) unspeciï¬ed at other frequencies),
other textbooks require that SNN(f) be equal to N0/2 for all frequencies f âˆˆR.
With our deï¬nition of WGN we can only prove that (25.128) holds for integrable
signals that are bandlimited to W Hz, whereas with the other textbooksâ€™ deï¬nition
one could presumably derive this relationship for all integrable functions.
We prefer our deï¬nition because there does not exist a Gaussian SP

N(t)

whose
PSD is equal to N0/2 at all frequencies. Indeed, the function of frequency that is
equal to N0/2 at all frequencies is not integrable and therefore does not qualify
as a PSD (Deï¬nition 25.7.2). Were such a PSD to exist, we would obtain from
(25.31) that such a process would have inï¬nite variance and thus be neither WSS
(Deï¬nition 25.4.2) nor Gaussian (Note 25.3.2).
Requiring that (25.128) hold for all integrable (continuous) signals would require
that KNN be given by the product of N0/2 and Diracâ€™s delta, which opens a whole
can of worms. Nevertheless, the reader should be aware that in some books WGN
is deï¬ned as a centered, stationary Gaussian noise whose autocovariance function
is given by Diracâ€™s Delta scaled by N0/2 or, equivalently, whose PSD is equal to
N0/2 at all frequencies.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.16 Exercises
613
25.15.4
White Gaussian Noise in Passband
Deï¬nition 25.15.9 (White Gaussian Noise in Passband). We say that

N(t)

is
white Gaussian noise of double-sided power spectral density N0/2 with
respect to the bandwidth W around the carrier frequency fc if

N(t)

is a
centered, measurable, stationary, Gaussian stochastic process that has a PSD SNN
satisfying
SNN(f) = N0
2 ,
|f| âˆ’fc
 â‰¤W
2 ,
(25.146)
and if
fc > W/2.
(25.147)
Note 25.15.10. For WGN with respect to the bandwidth W around the carrier
frequency fc, all the claims of Proposition 25.15.2 hold provided that we replace the
requirement that the functions s, {sj}, and {Ï†j} be integrable functions that are
bandlimited to W Hz with the requirement that they be integrable functions that
are bandlimited to W Hz around the carrier frequency fc. Likewise for Theorems
25.15.6 and 25.15.7.
25.16
Exercises
Exercise 25.1 (Constructing a SP from a RV). Let W be a standard Gaussian RV. Deï¬ne
the continuous-time SP

X(t)

by
X(t) = eâˆ’|t| W,
t âˆˆR.
(i) Is

X(t)

a stationary SP?
(ii) Is

X(t)

a Gaussian SP?
Exercise 25.2 (A SP from Jointly Gaussian Random Variables). Let X1, . . . , Xd be jointly
Gaussian, and let the real signals g1, . . . , gd be deterministic. Prove that the SP
X(t) =
d

j=1
Xj gj(t),
t âˆˆR
is Gaussian.
Exercise 25.3 (The Sum of Independent Gaussian Stochastic Processes). Prove that the
sum of two independent Gaussian stochastic processes is a Gaussian SP.
Exercise 25.4 (Delaying and Adding). Let

X(t)

be a stationary Gaussian SP of mean Î¼x
and autocovariance function KXX. Deï¬ne
Y (t) = X(t) + X(t âˆ’tD),
t âˆˆR,
where tD âˆˆR is deterministic.
(i) Is

Y (t)

a Gaussian SP?
(ii) Compute the mean and the autocovariance function of

Y (t)

.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

614
Continuous-Time Stochastic Processes
(iii) Is

Y (t)

stationary?
Exercise 25.5 (Random Variables and Stochastic Processes). Let the random variables X
and Y be IID N

0, Ïƒ2
, and let
Z(t) = X cos(2Ï€t) + Y sin(2Ï€t),
t âˆˆR.
(i) Is Z(0.2) Gaussian?
(ii) Is

Z(t)

a Gaussian SP?
(iii) Is it stationary?
Exercise 25.6 (Sampling a SP). Let

X(t), t âˆˆR

be a WSS continuous-time SP of
PSD SXX. Let T be ï¬xed, and deï¬ne the discrete-time SP

YÎ½, Î½ âˆˆZ

by
YÎ½ = X(Î½T),
Î½ âˆˆZ.
Express the PSD of

YÎ½, Î½ âˆˆZ

in terms of SXX and T.
Exercise 25.7 (Sample-and-Hold). A centered and bounded WSS SP

X(t), t âˆˆR

of
autocovariance function KXX(Â·) is fed to a sample-and-hold circuit that produces the SP
Y (t) = X
	3 t
T
4
T

,
t âˆˆR,
where T > 0 is the holding duration. Express the operational PSD of

Y (t)

in terms of
KXX(Â·) and T as follows:
(i) Show that the discrete-time SP

Xâ„“, â„“âˆˆZ

deï¬ned for every â„“âˆˆZ as Xâ„“= X(â„“T)
is centered and WSS, and express its autocovariance function in terms of KXX(Â·).
(ii) Express

Y (t)

as a PAM signal with real symbols

Xâ„“, â„“âˆˆZ

, baud period T, and
appropriate pulse shape.
(iii) Compute the operational PSD of this PAM signal.
Exercise 25.8 (More on Covariances). Let

X(t), t âˆˆR

and

Y (t), t âˆˆR

be centered
WSS stochastic processes of autocovariance functions KXX and KYY . Prove that
E
$
X(t + Ï„) Y (t)
% â‰¤

KXX(0) KYY (0),
t, Ï„ âˆˆR.
Exercise 25.9 (The Product of WSS Stochastic Processes). Show that the product of
two independent WSS stochastic processes is WSS. Express the PSD of the product in
terms of the PSDs of the individual processes and their means.
Exercise 25.10 (Stochastic Processes through Nonlinearities).
(i) Let

X(t)

be a stationary SP and let
Y (t) = g(X(t)),
t âˆˆR,
where g: R â†’R is some (Borel measurable) deterministic function. Show that the
SP

Y (t)

is stationary. Under what conditions is

Y (t)

WSS?
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.16 Exercises
615
(ii) Let

X(t)

be a centered stationary Gaussian SP of autocovariance function KXX.
Let Y (t) = sgn(X(t)), where sgn(Î¾) is equal to +1 whenever Î¾ â‰¥0 and is equal
to âˆ’1 otherwise. Is

Y (t)

centered? Is it WSS? If so, what is its autocovariance
function?
Hint: For Part (ii) recall Exercise 23.25.
Exercise 25.11 (A Memoryless Nonlinearity). Let

X(t)

be a centered stationary Gaus-
sian SP of autocovariance function KXX.
Let h: R â†’R be such that h

X(t)

is of
ï¬nite variance. Deï¬ne Z(t) for every t âˆˆR as Z(t) = h

X(t)

. Show that the mapping
Ï„ â†’E[X(t) Z(t + Ï„)] does not depend on t and is proportional to KXX.
Hint: Recall Bussgangâ€™s Theorem (Exercise 23.26).
Exercise 25.12 (The Transform Method). We wish to study the SP

h

X(t)

, t âˆˆR

when

X(t), t âˆˆR

is some given SP and h is some deterministic function, which we
assume to be of the form h = Ë‡g for some g âˆˆL1.
(i) Show that for all epochs t1, t2 âˆˆR,
E
+
h

X(t1)

h

X(t2)
,
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
g(f1) g(f2) E
+
ei(2Ï€f1X(t1)+2Ï€f2X(t2)),
df1 df2.
(ii) Show that if

X(t)

is a centered stationary Gaussian SP of autocovariance func-
tion KXX, then
E
+
h

X(t1)

h

X(t2)
,
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
g(f1) g(f2)
Â· exp
	
âˆ’2Ï€2
KXX(0)f 2
1 + 2 KXX(t1 âˆ’t2)f1f2 + KXX(0)f 2
2

df1 df2.
These identities are the core of the Transform Method (Davenport and Root, 1987, Chap-
ter 13).
Exercise 25.13 (WSS Stochastic Processes). Let A and B be IID random variables taking
on the values Â±1 equiprobably. Deï¬ne the SP
Z(t) = A cos(2Ï€t) + B sin(2Ï€t),
t âˆˆR.
(i) Is the SP

Z(t)

WSS?
(ii) Deï¬ne the SP

W(t)

by W(t) = Z2(t). Is

W(t)

WSS?
Exercise 25.14 (Valid Autocovariance Functions). Let KXX and KYY be the autocovari-
ance functions of some WSS stochastic processes

X(t)

and

Y (t)

.
(i) Show that Ï„ â†’KXX(Ï„) + KYY (Ï„) is the autocovariance function of some WSS SP.
(ii) Repeat for Ï„ â†’KXX(Ï„) KYY (Ï„).
Exercise 25.15 (Time Reversal). Let KXX be the autocovariance function of some WSS
SP

X(t), t âˆˆR

. Is the time-reversed SP (Ï‰, t) â†’X(Ï‰, âˆ’t) WSS? If so, express its
autocovariance function in terms of KXX.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

616
Continuous-Time Stochastic Processes
Exercise 25.16 (Classifying Stochastic Processes). Let

X(t)

and

Y (t)

be independent
centered stationary Gaussian stochastic processes of unit variance and autocovariance
functions KXX and KYY . Deï¬ne the stochastic processes

S(t)

,

T(t)

,

U(t)

,

V (t)

,
and

W(t)

at every t âˆˆR as
S(t) = X(t) + Y (t + Ï„1),
T(t) =X(t) Y (t + Ï„2),
U(t) = X(t) + X(t + Ï„3),
V (t)=X(t) X(t + Ï„4),
W(t) = X(t) + X(âˆ’t),
where Ï„1, Ï„2, Ï„3, Ï„4 âˆˆR are deterministic. Which of these stochastic processes is Gaussian?
Which is WSS? Which is stationary?
Exercise 25.17 (Another Interpretation of the PSD). Let

X(t)

be a centered WSS
measurable SP of PSD SXX. Given any T > 0 and f0 âˆˆR, consider the integral
 T
âˆ’T
X(t) eâˆ’i2Ï€f0t dt =
 T
âˆ’T
X(t) cos(2Ï€f0t) dt âˆ’i
 T
âˆ’T
X(t) sin(2Ï€f0t) dt.
(i) Show that for every such T and f0,
E
1
1
âˆš
2T
 T
âˆ’T
X(t) eâˆ’i2Ï€f0t dt

22
= 2T
 âˆ
âˆ’âˆ
SXX(f) sinc2
2T(f âˆ’f0)

df.
(ii) Argue that if SXX is continuous at f0, then the RHS of the above (and hence also
the LHS) tends to SXX(f0) as T tends to inï¬nity.
Exercise 25.18 (On the Spectral Distribution Function). Let Ts be some positive constant;
let p(Â·) be some real trigonometric polynomial
p(t) =
n

Î·=âˆ’n
aÎ· ei2Ï€Î·t/Ts,
t âˆˆR,
where aÎ· = aâˆ—
âˆ’Î· for all Î· âˆˆ{âˆ’n, . . . , n}; let T be a RV that is uniformly distributed over
the interval [âˆ’Ts/2, Ts/2); and let X(t) = p(t + T) for all t âˆˆR.
(i) Show that

X(t)

is WSS.
(ii) Find the distribution of a RV S for which (25.38) holds.
Exercise 25.19 (Midpoint-Approximation). Suppose that

X(t)

is a measurable, station-
ary, centered, Gaussian SP of autocovariance function KXX(Â·) and that we approximate
its integral over the interval [a, b] by the product of its value at the intervalâ€™s midpoint
and the intervalâ€™s length. Express the variance of the estimation error in terms of KXX(Â·),
a, and b.
Exercise 25.20 (A Linear Functional of a Gaussian SP). Let

X(t)

be a measurable
stationary Gaussian SP of mean 2 and of autocovariance function KXX : Ï„ â†’exp (âˆ’|Ï„|).
Compute
Pr
' 2
0
X(t) dt â‰¥2
(
.
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.16 Exercises
617
Exercise 25.21 (An Integral). Let the SP

Z(t)

be given by
Z(t) = X eâˆ’t I{t â‰¥0} + Y eâˆ’2t I{t â‰¥0},
t âˆˆR,
where X and Y are jointly Gaussian with means Î¼x and Î¼y and covariance matrix

Ïƒ2
x
ÏÏƒxÏƒy
ÏÏƒxÏƒy
Ïƒ2
y

.
Is

Z(t)

a Gaussian SP? Is it stationary? Compute
Pr
' âˆ
âˆ’âˆ
Z(t) dt â‰¥0
(
.
Exercise 25.22 (Integrating a Gaussian SP). Let

X(t)

be a measurable stationary
Gaussian SP of nonzero mean. Is the SP

Y (t)

deï¬ned at every epoch t âˆˆR by
Y (t) =
 |t|
0
X(Ï„) dÏ„
a Gaussian SP? Is it WSS? Is it stationary?
Exercise 25.23 (Two Filters). Let

X(t)

be a centered stationary Gaussian SP of auto-
covariance function KXX and PSD SXX. Deï¬ne

Y (t)

=

X(t)

â‹†hy,

Z(t)

=

X(t)

â‹†hz,
where hy, hz âˆˆL1. Thus,

Y (t)

is the result of passing

X(t)

through a stable ï¬lter of
impulse response hy and similarly

Z(t)

.
(i) What is the joint distribution of Y (t1) and Z(t2) for given epochs t1, t2 âˆˆR?
(ii) Give a necessary and suï¬ƒcient condition on Ë†hy, Ë†hz, and SXX for Y (17) to be
independent of Z(17).
(iii) Give a necessary and suï¬ƒcient condition on Ë†hy, Ë†hz, and SXX for

Z(t)

to be
independent of

Y (t)

.
Exercise 25.24 (On the Covariance of Linear Functionals). Let

X(t)

be a measurable
WSS SP, let g1 be real-valued, integrable, and symmetric, and let g2 be real-valued,
integrable, and anti-symmetric. Show that if
s1(t) = g1(t âˆ’t0),
t âˆˆR,
and
s2(t) = g2(t âˆ’t0),
t âˆˆR,
for some real number t0, then
Cov
' âˆ
âˆ’âˆ
X(t) s1(t) dt,
 âˆ
âˆ’âˆ
X(t) s2(t) dt
(
= 0.
Exercise 25.25 (Functionals of Diï¬€erent SPs). Let

X(t)

and

Y (t)

be centered, WSS,
measurable stochastic processes. Assume that E[X(t + Ï„) Y (t)] does not depend on t and
deï¬ne
KXY (Ï„) = E
$
X(t + Ï„) Y (t)
%
,
Ï„ âˆˆR.
Let sx, sy âˆˆL1 be deterministic. Prove that
Cov
' âˆ
âˆ’âˆ
X(t) sx(t) dt,
 âˆ
âˆ’âˆ
Y (t) sy(t) dt
(
=
 âˆ
âˆ’âˆ
KXY (Ïƒ)

sx â‹†~sy

(Ïƒ) dÏƒ.
Why can this be viewed as a generalization of Proposition 25.10.1 (vi) and of (25.90)?
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

618
Continuous-Time Stochastic Processes
Exercise 25.26 (On Jointly Gaussian Stochastic Processes). We say that

X(t)

and

Y (t)

are jointly Gaussian stochastic processes if for every n âˆˆN and every choice
of the epochs t1, . . . , tn âˆˆR the random 2n-vector (X(t1), . . . , X(tn), Y (t1), . . . , Y (tn))T
is Gaussian. Let

X(t)

and

Y (t)

be stationary and jointly Gaussian, and let sx and
sy be integrable real signals.
Either using a heuristic argument similar to the one leading to (25.64) or by extending
the proof of Proposition 25.11.1 prove that
 âˆ
âˆ’âˆ
X(t) sx(t) dt +
 âˆ
âˆ’âˆ
Y (t) sy(t) dt
is a Gaussian random variable. Use this to prove the joint Gaussianity of
 âˆ
âˆ’âˆ
X(t) sx(t) dt
and
 âˆ
âˆ’âˆ
Y (t) sy(t) dt.
Exercise 25.27 (Filters and Power). Let

X(t)

be a centered measurable WSS SP of
PSD SXX. Show that when

X(t)

is passed through a stable ï¬lter of impulse response
h âˆˆL1, its power is scaled by at most
max
fâˆˆR
Ë†h(f)
2.
Exercise 25.28 (Linear Functionals of White Gaussian Noise). Find the distribution of
 Ts
0
N(t) dt
and of
 âˆ
0
eâˆ’tN(t) dt
when

N(t), t âˆˆR

is white Gaussian noise of double-sided PSD N0/2 with respect to
the bandwidth of interest. (Ignore the fact that the mappings t â†’I{0 â‰¤t â‰¤Ts} and
t â†’eâˆ’t I{t â‰¥0} are not bandlimited.)
Exercise 25.29 (More on Independence). Let

X(t)

and

Y (t)

be independent cen-
tered WSS stochastic processes, and let hx and hy be integrable signals. Prove that the
stochastic processes X â‹†hx and Y â‹†hy are independent.
Hint: Recall Exercise 23.7 and Proposition 25.15.8.
Exercise 25.30 (Approximately White SP). Let

X(t), t âˆˆR

be a measurable, centered,
stationary, Gaussian SP of autocovariance function
KXX(Ï„) = BN0
4
eâˆ’B|Ï„|,
Ï„ âˆˆR,
where N0, B > 0 are given constants. Throughout this problem N0 is ï¬xed.
(i) Plot KXX for several values of B. What does KXX look like when B â‰«1? Show
that KXX(Ï„) > 0 for all Ï„ âˆˆR; that
 âˆ
âˆ’âˆ
KXX(Ï„) dÏ„ = N0
2 ;
and that for every Î´ > 0,
lim
Bâ†’âˆ
 Î´
âˆ’Î´
KXX(Ï„) dÏ„ = N0
2 .
(In this sense, KXX approximates Diracâ€™s Delta scaled by N0/2 when B is large.)
.027
14:53:20, subject to the Cambridge Core terms of use, available at

25.16 Exercises
619
(ii) Compute E
$
X(t)2%
. Plot this as a function of B, with N0 held ï¬xed. What happens
when B â‰«1?
(iii) Compute the PSD SXX. Plot it for several values of B. What does it look like when
B â‰«1?
(iv) For the orthonormal signals deï¬ned for every t âˆˆR by
Ï†1(t) =

1
if 0 â‰¤t â‰¤1,
0
otherwise,
Ï†2(t) =
â§
âª
â¨
âª
â©
1
if 0 â‰¤t â‰¤1
2,
âˆ’1
if 1
2 < t â‰¤1,
0
otherwise
compute E
$
âŸ¨X, Ï†1âŸ©âŸ¨X, Ï†2âŸ©
%
. What happens to this expression when B â‰«1?
Exercise 25.31 (Multiplying a SP by a Periodic Signal). Let

X(t)

be a centered WSS
SP of autocovariance function KXX. Let r be a real, deterministic, ï¬nite-power, periodic
signal of period Tp > 0. Let

Y (t)

be the product of

X(t)

and r.
(i) Show that

Y (t)

is of average autocovariance function
Â¯KYY (Ï„) = KXX(Ï„) 1
Tp
 Tp
0
r(t) r(t + Ï„) dt,
Ï„ âˆˆR.
(ii) Denoting by Ë†r(Î·) the Î·-th Fourier Series Coeï¬ƒcient of r with respect to the interval
[âˆ’Tp/2, Tp/2), show that if

X(t)

is of PSD SXX then

Y (t)

is of operational PSD
f â†’1
Tp
âˆ

Î·=âˆ’âˆ
Ë†r(Î·)
2 SXX
	
f + Î·
Tp

.
Hint: For Part (i) recall Exercise 15.13. For Part (ii) recall Theorem A.3.3.
.027
14:53:20, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

Chapter 26
Detection in White Gaussian Noise
26.1
Introduction
In this chapter we ï¬nally address the detection problem in continuous time. The
setup is described in Section 26.2. The key result of this chapter is thatâ€”although
the observation in this setup is a stochastic process, i.e., a continuum of ran-
dom variablesâ€”the problem can be reduced without loss of optimality to a ï¬nite-
dimensional problem where the observation is a random vector.
This result is
presented in Section 26.3. In Section 26.4 we analyze the conditional law of the
random vector under each of the hypotheses.
This analysis enables us in Sec-
tion 26.5 to derive an optimal guessing rule and in Section 26.6 to analyze its
performance. Section 26.7 addresses the front-end ï¬lter, which is a critical element
of any practical implementation of the decision rule. Extensions to passband detec-
tion are then described in Section 26.8, followed by some examples in Section 26.9.
Section 26.10 treats the problem of detection in â€œcoloredâ€ noise, and Section 26.11
treats systems with multiple antennas. The chapter concludes with a discussion of
the detection problem for mean signals that are not bandlimited.
26.2
Setup
A discrete random variable M (â€œmessageâ€) takes values in the set M = {1, . . . , M},
where M â‰¥2, according to the a priori probabilities
Ï€m = Pr[M = m],
m âˆˆM,
(26.1)
where Ï€1, . . . , Ï€M are positive1
Ï€m > 0,
m âˆˆM
(26.2)
and sum to one

mâˆˆM
Ï€m = 1.
(26.3)
1There is no loss in generality in addressing the detection problem only for strictly positive
priors. Hypotheses that have a zero prior can be ignored at the receiver without loss in optimality.
620
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.3 From a Stochastic Process to a Random Vector
621
We observe a continuous-time SP

Y (t), t âˆˆR

, which, conditional on M = m,
can be expressed as
Y (t) = sm(t) + N(t),
t âˆˆR,
(26.4)
i.e.,
Y = sm + N,
(26.5)
where the â€œmean signalsâ€ s1, . . . , sM are real, deterministic, integrable signals that
are bandlimited to W Hz (Deï¬nition 6.4.9), and where the â€œnoiseâ€

N(t)

is inde-
pendent of M and is WGN of double-sided PSD N0/2 with respect to the band-
width W (Deï¬nition 25.15.1). Based on the observation

Y (t)

we wish to guess M
with the smallest possible probability of error.2
26.3
From a Stochastic Process to a Random Vector
The next theorem allows us to reduce the detection problem from one where the
observation is a SP to one where it is a random vector. The reduction is done
â€œwithout loss in optimalityâ€ in the sense that to every measurable decision rule
that is based on the SP there corresponds a (randomized) decision rule that is
based on the random vector and that is of identical performance: for every message
m âˆˆM, the conditional probabilities of error given M = m of the two decision
rules are identical and, consequently, so are the (unconditional) probabilities of
error, because
p(error) =

mâˆˆM
Ï€m p(error|M = m).
(26.6)
Before stating this key result we recall that the linear subspace span(s1, . . . , sM)
is the collection of all linear combinations of the mean signals s1, . . . , sM (Sec-
tion 4.3); that this subspace is ï¬nite dimensional; that its elements are all inte-
grable signals that are bandlimited to W Hz (because s1, . . . , sM are); and that
it has an orthonormal basis (Deï¬nition 4.6.3, Note 6.4.12, Note 6.4.2, and Propo-
sition 4.6.10). Such an orthonormal basis can be found using the Gram-Schmidt
Procedure (Section 4.6.6). If (Ï†1, . . . , Ï†d) is an orthonormal basis for the subspace
span(s1, . . . , sM), then this subspace is of dimension d; by Proposition 4.6.4
sm =
d

â„“=1
âŸ¨sm, Ï†â„“âŸ©Ï†â„“,
m âˆˆM;
(26.7)
and, by Proposition 4.6.9,
âˆ¥smâˆ¥2
2 =
d

â„“=1
âŸ¨sm, Ï†â„“âŸ©2,
m âˆˆM.
(26.8)
2In mathematical terms we are looking for a mapping from Î© to M that is measurable with
respect to the Ïƒ-algebra generated by

Y (t)

and that minimizes the probability of error among
all such functions.
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

622
Detection in White Gaussian Noise
Theorem 26.3.1 (From a SP to a Random Vector). Consider the setup of Sec-
tion 26.2.
(i) If (Ï†1, . . . , Ï†d) is an orthonormal basis for span(s1, . . . , sM), then to every
decision rule based on Y that is measurable with respect to the Ïƒ-algebra
generated by Y there corresponds a randomized decision rule based on the
random d-vector

âŸ¨Y, Ï†1âŸ©, . . . , âŸ¨Y, Ï†dâŸ©
T
(26.9)
of identical performance in the sense that the conditional probabilities of error
given M of the two rules are identical. Consequently, no measurable decision
rule for guessing M based on Y can have a probability of error that is smaller
than that of an optimal rule for guessing M based on the random vector
(26.9).
(ii) If Ëœs1, . . . ,Ëœsn are integrable signals satisfying
span(s1, . . . , sM) âŠ†span(Ëœs1, . . . ,Ëœsn),
(26.10)
then, in the above sense, it is optimal to guess M based on

âŸ¨Y,Ëœs1âŸ©, . . . , âŸ¨Y,ËœsnâŸ©
T.
(26.11)
(iii) In the above sense there is also no loss of optimality in guessing M based on

âŸ¨Y, s1âŸ©, . . . , âŸ¨Y, sMâŸ©
T.
(26.12)
Proof. The main result of the theorem is Part (i), with which we begin. The other
parts are simple corollaries. We begin with a technicality. Since the mean signals
s1, . . . , sM are integrable signals that are bandlimited to W Hz, it follows that so
are all the signals in span(s1, . . . , sM), including Ï†1, . . . , Ï†d. Consequently, the
stochastic integrals in (26.9) are well-deï¬ned.
Having gotten this out of the way, we now proceed to the main part of the proof,
which is illustrated in Figure 26.1 on Page 623. At the top of the ï¬gure is some
arbitrary given (measurable) decision rule Ï†Guess that produces the guess Ï†Guess(Y)
based on the observed SP Y. We next construct a randomized decision rule of
identical performance but that is based on the vector of inner products (26.9). The
randomized rule ï¬rst uses its local randomness and the vector of inner products
to produce a new continuous-time SP Yâ€², and it then feeds Yâ€² to the given rule
Ï†Guess to produce the guess Ï†Guess(Yâ€²). The SP Yâ€² is constructed so that, for
every m âˆˆM, its conditional law given M = m be identical to the conditional law
of Y given M = m. Consequently,
Pr

Ï†Guess(Y) Ì¸= m
 M = m

= Pr

Ï†Guess(Yâ€²) Ì¸= m
 M = m

,
m âˆˆM, (26.13)
with the result that the two rules have identical performance. It remains to con-
struct Yâ€² and to demonstrate that its conditional law is identical to the conditional
law of Y.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.3 From a Stochastic Process to a Random Vector
623
Decision
Device
Decision
Device
Guess
Guess
Projection
Reconstruction

â„“âŸ¨Y, Ï†â„“âŸ©Ï†â„“
âŸ¨Y, Ï†1âŸ©
âŸ¨Y, Ï†2âŸ©
âŸ¨Y, Ï†dâŸ©

Y (t), t âˆˆR

+
Projection
Subtraction
Generate

N â€²(t)

of same FDDs as

N(t)

Local
Randomness
Nâ€²
Nâ€² âˆ’
â„“âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“

Y â€²(t)

Figure 26.1: The top ï¬gure is of a generic decision rule that bases its decision
on the received SP

Y (t)

.
Using the above rule as one of its inner modules,
the (randomized) decision rule in the lower ï¬gure bases its decision on the d inner
products âŸ¨Y, Ï†1âŸ©, . . . , âŸ¨Y, Ï†dâŸ©and achieves the same performance: for any message
m âˆˆM, the conditional FDDs of Y and Yâ€² given M = m are identical, and
consequently, so are the conditional probabilities of error.
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

624
Detection in White Gaussian Noise
The construction of Yâ€² is depicted at the bottom of Figure 26.1.
We use the
local randomness to produce a measurable SP Nâ€² whose law is identical to that
of the noise N. Thus, Nâ€² is a measurable, centered, stationary, Gaussian SP of
autocovariance function KNNâ€”the autocovariance function of the WGN on the
channel. The SP Yâ€² is
Yâ€² =
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©Ï†â„“+ Nâ€² âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“.
(26.14)
The ï¬rst summand, which is based on the inner products, reconstructs the projec-
tion of Y onto span(Ï†1, . . . , Ï†d)â€”the linear subspace spanned by the mean signals.
The sum of the remaining terms is of the same law as the part of N that is or-
thogonal to span(Ï†1, . . . , Ï†d) but, of course, independent of it. (It was generated
from Nâ€², which was generated from the local randomness, which is independent
of N.)
We next verify that, conditional on M = m, the stochastic processes Y and Yâ€² have
identical laws (FDDs) for every m âˆˆM. We begin by verifying that the conditional
means are identical. It is here that we use the hypothesis that (Ï†1, . . . , Ï†d) is an
orthonormal basis for span(s1, . . . , sM):
E

Yâ€²  M = m

=
d

â„“=1
E

âŸ¨Y, Ï†â„“âŸ©
 M = m

Ï†â„“
=
d

â„“=1
E

âŸ¨sm + N, Ï†â„“âŸ©

Ï†â„“
=
d

â„“=1
âŸ¨sm, Ï†â„“âŸ©Ï†â„“
= sm,
m âˆˆM.
Here the ï¬rst equality holds because Nâ€² is centered; the second because, conditional
on M = m, the received waveform Y is sm+N; the third by writing âŸ¨sm + N, Ï†â„“âŸ©as
âŸ¨sm, Ï†â„“âŸ©+âŸ¨N, Ï†â„“âŸ©and by noting that âŸ¨N, Ï†â„“âŸ©is of zero mean because N is centered;
and the last equality follows from (26.7), which holds because (Ï†1, . . . , Ï†d) is an
orthonormal basis for span(s1, . . . , sM).
If we subtract the conditional mean from Y, we obtain N, whereas if we subtract
the conditional mean from Yâ€² we obtain
d

â„“=1
âŸ¨N, Ï†â„“âŸ©Ï†â„“+ Nâ€² âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“.
The two have the same conditional laws given M = m by Theorem 25.15.7.
We next prove Part (ii) by showing that (26.10) implies that, if Ï†1, . . . , Ï†d are
as above, then from the inner-products vector (26.11) we can compute the inner-
products vector (26.9). To see this we note that since Ï†1, . . . , Ï†d is an orthonormal
basis for span(s1, . . . , sM),
Ï†â„“âˆˆspan(s1, . . . , sM),
â„“âˆˆ{1, . . . , d},
(26.15)
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.3 From a Stochastic Process to a Random Vector
625
and hence, by the assumption (26.10),
Ï†â„“âˆˆspan(Ëœs1, . . . ,Ëœsn),
â„“âˆˆ{1, . . . , d}.
(26.16)
Consequently, for each such â„“there exist n constants Î±(1)
â„“, . . . , Î±(n)
â„“
âˆˆR such that
Ï†â„“=
n

j=1
Î±(j)
â„“
Ëœsj,
â„“âˆˆ{1, . . . , d}.
(26.17)
This and the â€œalmostâ€ linearity of stochastic integration (Lemma 25.10.3) implies
that with probability one
âŸ¨Y, Ï†â„“âŸ©=
n

j=1
Î±(j)
â„“
âŸ¨Y,ËœsjâŸ©,
â„“âˆˆ{1, . . . , d},
(26.18)
and the inner-products vector (26.9) is indeed computable from the inner-products
vector (26.11).
Part (iii) follows from Part (ii) by substituting M for n and s1, . . . , sM for Ëœs1, . . . ,Ëœsn.
The fact that there is no loss of optimality in basing our guess of M on the vector of
inner products (26.9) has a beautiful geometric interpretation. In order to present
it, we ï¬rst note that there is a one-to-one relationship between the inner-products
vector and the projection
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©Ï†â„“
(26.19)
of the received SP Y onto the subspace spanned by the mean vectors (Deï¬ni-
tion 25.15.3).
Indeed, we can compute this projection from the inner-products
vector using (26.19), and from this projection we can compute the inner-products
vector via the relation
âŸ¨Y, Ï†â„“âŸ©=
L
d

â„“â€²=1
âŸ¨Y, Ï†â„“â€²âŸ©Ï†â„“â€², Ï†â„“
M
,
â„“âˆˆ{1, . . . , d}.
(26.20)
By Theorem 26.3.1 (i), it is optimal to guess M based on the inner-products vector,
and since this vector is computable from the projection of the received SP onto
the linear subspace spanned by the mean signals, we conclude that it is optimal
to guess M based on this projection. This is one of the most important results in
Digital Communications, so we repeat:
When guessing which of M integrable signals that are bandlimited
to W Hz is being observed in Gaussian noise that is white with
respect to the bandwidth W, there is no loss of optimality in basing
the guess on the projection of the observed waveform onto the
linear subspace spanned by the signals.
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

626
Detection in White Gaussian Noise
26.4
The Random Vector of Inner Products
26.4.1
The Conditional Law
Theorem 26.3.1 reduces our detection problem from one where the observable is a
SP to one where it is a random vector. But to solve the latter using the techniques
of Chapter 21 we need the conditional law of the random vector given each of the
hypotheses. Obtaining these conditional laws is, fortunately, straightforward:
Theorem 26.4.1 (The Conditional Laws of the Inner-Products Vector). Consider
the setup of Theorem 26.3.1, and let m be any message in M.
(i) Conditional on M = m, the random vector

âŸ¨Y, Ï†1âŸ©, . . . , âŸ¨Y, Ï†dâŸ©
T
(26.21a)
is Gaussian with mean vector

âŸ¨sm, Ï†1âŸ©, . . . , âŸ¨sm, Ï†dâŸ©
T
(26.21b)
and d Ã— d covariance matrix
N0
2 Id.
(26.21c)
(ii) Conditional on M = m, the random vector

âŸ¨Y,Ëœs1âŸ©, . . . , âŸ¨Y,ËœsnâŸ©
T
(26.22a)
is Gaussian with mean vector

âŸ¨sm,Ëœs1âŸ©, . . . , âŸ¨sm,ËœsnâŸ©
T
(26.22b)
and n Ã— n covariance matrix
N0
2
â›
âœ
âœ
âœ
â
âŸ¨Ëœs1,Ëœs1âŸ©
âŸ¨Ëœs1,Ëœs2âŸ©
Â· Â· Â·
âŸ¨Ëœs1,ËœsnâŸ©
âŸ¨Ëœs2,Ëœs1âŸ©
âŸ¨Ëœs2,Ëœs2âŸ©
Â· Â· Â·
âŸ¨Ëœs2,ËœsnâŸ©
...
...
...
...
âŸ¨Ëœsn,Ëœs1âŸ©
âŸ¨Ëœsn,Ëœs2âŸ©
Â· Â· Â·
âŸ¨Ëœsn,ËœsnâŸ©
â
âŸ
âŸ
âŸ
â .
(26.22c)
(iii) Conditional on M = m, the random vector

âŸ¨Y, s1âŸ©, . . . , âŸ¨Y, sMâŸ©
T
(26.23a)
is Gaussian with mean vector

âŸ¨sm, s1âŸ©, . . . , âŸ¨sm, sMâŸ©
T
(26.23b)
and M Ã— M covariance matrix
N0
2
â›
âœ
âœ
âœ
â
âŸ¨s1, s1âŸ©
âŸ¨s1, s2âŸ©
Â· Â· Â·
âŸ¨s1, sMâŸ©
âŸ¨s2, s1âŸ©
âŸ¨s2, s2âŸ©
Â· Â· Â·
âŸ¨s2, sMâŸ©
...
...
...
...
âŸ¨sM, s1âŸ©
âŸ¨sM, s2âŸ©
Â· Â· Â·
âŸ¨sM, sMâŸ©
â
âŸ
âŸ
âŸ
â .
(26.23c)
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.4 The Random Vector of Inner Products
627

Y (t)

~Ï†1
~Ï†2
~Ï†d
...
âŸ¨Y, Ï†1âŸ©
âŸ¨Y, Ï†2âŸ©
âŸ¨Y, Ï†dâŸ©
Guess
Decision Rule
sample at t = 0
Figure 26.2: Computing the inner products âŸ¨Y, Ï†1âŸ©, . . . , âŸ¨Y, Ï†dâŸ©from the received
waveform using d matched ï¬lters and feeding the result to a guessing device.
Proof. We shall only prove Part (ii); the other parts are special cases. Conditional
on M = m, the SP Y is sm + N, and the inner-products vector in (26.22a) is thus

âŸ¨sm,Ëœs1âŸ©, . . . , âŸ¨sm,ËœsnâŸ©
T +

âŸ¨N,Ëœs1âŸ©, . . . , âŸ¨N,ËœsnâŸ©
T.
(26.24)
Here the ï¬rst vector is deterministic, and the second is a centered Gaussian because
its components are linear functionals of WGN (Proposition 25.15.2 (ii)).
This
establishes that the vector is Gaussian (Proposition 23.6.3). Since the second vector
is centered, the mean vector equals the ï¬rst vector, i.e., is as given in (26.22b).
It remains to compute the covariance matrix, which is the covariance matrix of

âŸ¨N,Ëœs1âŸ©, . . . , âŸ¨N,ËœsnâŸ©
T.
This matrix is the one in (26.22c) by Proposition 25.15.2 (ii) on linear functionals
of WGN.
Theorems 26.3.1 and 26.4.1 suggest two diï¬€erent (equivalent) receiver architec-
tures: the one depicted in Figure 26.2, where the inner-products vector (26.21a) is
computed using d matched ï¬lters, and the result is then fed to a guessing device,
and the one depicted in Figure 26.3 where we ï¬rst compute the M inner products
between Y and each of the mean signals and we then feed the result to a (diï¬€erent)
decision device.
For the purpose of deriving an optimal decision rule, it is usually easier to adopt the
ï¬rst architecture and to work with the inner-products vector (26.21a), because its
conditional law is so simple. Moreover, this architecture requires only d matched
ï¬lters, and d â‰¤M. In contrast, the vector (26.23a) can be cumbersome because its
covariance matrix may be singular, so it need not have a density. Nevertheless, it
does have the advantage that it is â€œcoordinate free,â€ in the sense that its deï¬nition
does not depend on the choice of the orthonormal basis. This makes some of the
results more transparent. With an eye to this, we note the following immediate
consequence of Theorem 26.4.1 (iii):
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

628
Detection in White Gaussian Noise

Y (t)

~s1
~s2
~sM
...
âŸ¨Y, s1âŸ©
âŸ¨Y, s2âŸ©
âŸ¨Y, sMâŸ©
Guess
Decision Rule
sample at t = 0
Figure 26.3: Computing the inner product between the observed SP and each of
the mean signals and basing the guess on these inner products.
Note 26.4.2. The conditional distribution of the random vector

âŸ¨Y, s1âŸ©, . . . , âŸ¨Y, sMâŸ©
T
(26.25)
given each of the hypotheses is determined by N0 and by the pairwise inner prod-
ucts between the mean signals
{âŸ¨smâ€², smâ€²â€²âŸ©}mâ€²,mâ€²â€²âˆˆM.
(26.26)
The PSD of the noise at frequencies outside the band [âˆ’W, W ] is immaterial.
26.4.2
It Is all in the Geometry!
By Theorem 26.3.1 (iii), the performance of the best decision rule based on Y is the
same as the performance of the best rule based on the inner-products vector (26.25).
The performance of the latter is determined by the prior and by the conditional
laws of the vector given the diï¬€erent messages. By Note 26.4.2, these conditional
laws are determined by the PSD N0/2 of the noise in the band [âˆ’W, W ] and
by the pairwise inner products between the mean signals (26.26). Other than in
determining these inner products, the nature of the mean signals is immaterial
(provided that they are integrable signals that are bandlimited to W Hz). The
PSD of the noise outside the band [âˆ’W, W ] is also immaterial. We thus conclude:
Proposition 26.4.3. For the setup of Section 26.2, the minimal probability of error
that can be achieved in guessing M based on

Y (t)

is determined by N0, the inner
products (26.26), and the prior {Ï€m}mâˆˆM.
26.5
Optimal Guessing Rule
26.5.1
The Decision Rule in Terms of (Ï†1, . . . , Ï†d)
We are now ready to derive an optimal guessing rule. For this purpose we start
with an orthonormal basis (Ï†1, . . . , Ï†d) for span(s1, . . . , sM); it does not really
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.5 Optimal Guessing Rule
629
matter which. Without loss in optimality we can then base our guess of M on the
vector of inner products (26.9) (Theorem 26.3.1 (i)). Denote this random d-vector
by T,
T =

âŸ¨Y, Ï†1âŸ©, . . . , âŸ¨Y, Ï†dâŸ©
T,
(26.27)
and its â„“-th component by T (â„“),
T (â„“) =
 âˆ
âˆ’âˆ
Y (t) Ï†â„“(t) dt
= âŸ¨Y, Ï†â„“âŸ©,
â„“= 1, . . . , d.
(26.28)
It follows from Theorem 26.4.1 (i) that for every m âˆˆM the conditional distribu-
tion of T given M = m is Gaussian with mean
E

T
 M = m

=

âŸ¨sm, Ï†1âŸ©, . . . , âŸ¨sm, Ï†dâŸ©
T
(26.29)
and covariance matrix (N0/2) Id, where Id denotes the d Ã— d identity matrix. The
components of T are thus conditionally independent and of equal variance N0/2
(but not of equal mean). Consequently, we can express the conditional density
of T, conditional on M = m, at every point t = (t(1), . . . , t(d))T âˆˆRd using
this conditional independence and the explicit form of the univariate Gaussian
density (19.6) as
fT|M=m(t) =
d
@
â„“=1
1

2Ï€N0/2
exp
-
âˆ’

t(â„“) âˆ’âŸ¨sm, Ï†â„“âŸ©
2
2N0/2
.
=
1
(Ï€N0)d/2 exp
	
âˆ’1
N0
d

â„“=1

t(â„“) âˆ’âŸ¨sm, Ï†â„“âŸ©
2

,
t âˆˆRd.
(26.30)
Note that with proper translation (Table 26.1, Page 630) the conditional distri-
bution of T is very similar to the one we addressed in Section 21.6; see (21.51).
In fact, it is a special case of the distribution studied in Section 21.6: Y there
corresponds to T here; J there corresponds to d here; Ïƒ2 there corresponds to N0/2
here; and the mean vector sm associated with Message m there corresponds to the
vector

âŸ¨sm, Ï†1âŸ©, . . . , âŸ¨sm, Ï†dâŸ©
T
(26.31)
here. Consequently, we can use the results from Section 21.6 and Proposition 21.6.1
in particular, to obtain an optimal decision rule for guessing M based on T:
Theorem 26.5.1. Consider the setup of Section 26.2, and let (Ï†1, . . . , Ï†d) be an
orthonormal basis for span(s1, . . . , sM).
(i) The decision rule that guesses uniformly at random from among all the mes-
sages Ëœm âˆˆM for which
ln Ï€ Ëœm âˆ’
d
â„“=1

âŸ¨Y, Ï†â„“âŸ©âˆ’âŸ¨s Ëœm, Ï†â„“âŸ©
2
N0
= max
mâ€²âˆˆM

ln Ï€mâ€² âˆ’
d
â„“=1

âŸ¨Y, Ï†â„“âŸ©âˆ’âŸ¨smâ€², Ï†â„“âŸ©
2
N0
!
(26.32)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

630
Detection in White Gaussian Noise
In Section 21.6
Here
number of components of
observed vector
J
d
variance of noise added to
each component
Ïƒ2
N0/2
number of hypotheses
M
M
conditional mean of observa-
tion given M = m

s(1)
m , . . . , s(J)
m
T

âŸ¨sm, Ï†1âŸ©, . . . , âŸ¨sm, Ï†dâŸ©
T
sum of squared components
of mean vector
J

j=1

s(j)
m
2
d

â„“=1

âŸ¨sm, Ï†â„“âŸ©
2 =
 âˆ
âˆ’âˆ
s2
m(t) dt
Table 26.1: The setup in Section 21.6 and here.
minimizes the probability of a guessing error.
(ii) If M has a uniform distribution, then this rule does not depend on the value
of N0. It chooses uniformly at random from among all the messages Ëœm âˆˆM
for which
d

â„“=1

âŸ¨Y, Ï†â„“âŸ©âˆ’âŸ¨s Ëœm, Ï†â„“âŸ©
2 =
min
mâ€²âˆˆM
 d

â„“=1

âŸ¨Y, Ï†â„“âŸ©âˆ’âŸ¨smâ€², Ï†â„“âŸ©
2
!
.
(26.33)
(iii) If M has a uniform distribution and, in addition, the mean signals are of
equal energy, i.e,
âˆ¥s1âˆ¥2
2 = âˆ¥s2âˆ¥2
2 = Â· Â· Â· = âˆ¥sMâˆ¥2
2 ,
(26.34)
then these decision rules are equivalent to the maximum-correlation rule that
guesses uniformly from among all the messages Ëœm âˆˆM for which
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©âŸ¨s Ëœm, Ï†â„“âŸ©= max
mâ€²âˆˆM
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©âŸ¨smâ€², Ï†â„“âŸ©.
(26.35)
Proof. The theorem follows directly from Proposition 21.6.1. For Part (iii) we
need to note that, by (26.8), Condition (26.34) is equivalent to the condition
d

â„“=1
âŸ¨s1, Ï†â„“âŸ©2 =
d

â„“=1
âŸ¨s2, Ï†â„“âŸ©2 = Â· Â· Â· =
d

â„“=1
âŸ¨sM, Ï†â„“âŸ©2,
(26.36)
and hence, upon taking square roots, to the condition that the Euclidean norm of
the vector in (26.31) not depend on the message m. This latter condition is the
additional hypothesis in Proposition 21.6.1 (iii).
Note that, because (Ï†1, . . . , Ï†d) is an orthonormal basis for span(s1, . . . , sM), the
signals smâ€² and smâ€²â€² diï¬€er, if, and only if, the vectors (âŸ¨smâ€², Ï†1âŸ©, . . . , âŸ¨smâ€², Ï†dâŸ©)T
and (âŸ¨smâ€²â€², Ï†1âŸ©, . . . , âŸ¨smâ€²â€², Ï†dâŸ©)T diï¬€er.3 Consequently, by Proposition 21.6.2:
3Readers who expected to see the clause â€œoutside a set of Lebesgue measure zero,â€ should
recall Notes 6.4.2 and 6.4.12.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.5 Optimal Guessing Rule
631
Note 26.5.2. If the mean signals s1, . . . , sM are distinct, then the probability of a
tie, i.e., that more than one message Ëœm âˆˆM satisï¬es (26.32), is zero.
26.5.2
The Decision Rule without Reference to a Basis
We next derive a â€œcoordinate-freeâ€ representation of our decision rule, i.e., a rep-
resentation that does not refer to a speciï¬c orthonormal basis.
Theorem 26.5.3. Consider the setup of Section 26.2.
(i) The decision rule that guesses uniformly at random from among all the mes-
sages Ëœm âˆˆM for which
ln Ï€ Ëœm + 2
N0
	 âˆ
âˆ’âˆ
Y (t) s Ëœm(t) dt âˆ’1
2
 âˆ
âˆ’âˆ
s2
Ëœm(t) dt

= max
mâ€²âˆˆM
1
ln Ï€mâ€² + 2
N0
	 âˆ
âˆ’âˆ
Y (t) smâ€²(t) dt âˆ’1
2
 âˆ
âˆ’âˆ
s2
mâ€²(t) dt

2
(26.37)
minimizes the probability of error.
(ii) If M has a uniform distribution, then this rule does not depend on the value
of N0. It chooses uniformly at random from among all the messages Ëœm âˆˆM
for which
 âˆ
âˆ’âˆ
Y (t) s Ëœm(t) dt âˆ’1
2
 âˆ
âˆ’âˆ
s2
Ëœm(t) dt
= max
mâ€²âˆˆM
1 âˆ
âˆ’âˆ
Y (t) smâ€²(t) dt âˆ’1
2
 âˆ
âˆ’âˆ
s2
mâ€²(t) dt
2
.
(26.38)
(iii) If M has a uniform distribution and, in addition, the mean signals are of
equal energy, i.e.,
âˆ¥s1âˆ¥2
2 = âˆ¥s2âˆ¥2
2 = Â· Â· Â· = âˆ¥sMâˆ¥2
2 ,
then these decision rules are equivalent to the maximum-correlation rule that
guesses uniformly from among all the messages Ëœm âˆˆM for which
 âˆ
âˆ’âˆ
Y (t) s Ëœm(t) dt = max
mâ€²âˆˆM
1 âˆ
âˆ’âˆ
Y (t) smâ€²(t) dt
2
.
(26.39)
Proof. We shall prove Part (i) using Theorem 26.5.1 (i). To this end we begin by
noting that
ln Ï€mâ€² âˆ’
d
â„“=1

âŸ¨Y, Ï†â„“âŸ©âˆ’âŸ¨smâ€², Ï†â„“âŸ©
2
N0
can be expressed by opening the square as
ln Ï€mâ€² âˆ’1
N0
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©2 + 2
N0
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©âŸ¨smâ€², Ï†â„“âŸ©âˆ’1
N0
d

â„“=1
âŸ¨smâ€², Ï†â„“âŸ©2.
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

632
Detection in White Gaussian Noise
Since the term
âˆ’1
N0
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©2
does not depend on the hypothesis, it is optimal to choose a message at random
from among all the messages Ëœm satisfying
ln Ï€ Ëœm + 2
N0
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©âŸ¨s Ëœm, Ï†â„“âŸ©âˆ’1
N0
d

â„“=1
âŸ¨s Ëœm, Ï†â„“âŸ©2
= max
mâ€²âˆˆM

ln Ï€mâ€² + 2
N0
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©âŸ¨smâ€², Ï†â„“âŸ©âˆ’1
N0
d

â„“=1
âŸ¨smâ€², Ï†â„“âŸ©2
!
.
Part (i) of the theorem now follows from this rule by substituting mâ€² for m in (26.8)
and by noting that
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©âŸ¨smâ€², Ï†â„“âŸ©=

Y,
d

â„“=1
âŸ¨smâ€², Ï†â„“âŸ©Ï†â„“

= âŸ¨Y, smâ€²âŸ©,
mâ€² âˆˆM,
and likewise for Ëœm. Here the ï¬rst equality follows by linearity (Lemma 25.10.3)
and the second by substituting mâ€² for m in (26.7).
Part (ii) follows by noting that if M is uniform, then ln Ï€mâ€² does not depend on
the hypothesis mâ€².
Part (iii) follows from Part (ii) because if all the mean signals are of equal energy,
then the term
 âˆ
âˆ’âˆ
s2
mâ€²(t) dt
does not depend on the hypothesis.
By Note 26.5.2 we have:
Note 26.5.4. If the mean signals are distinct, then the probability of a tie is zero.
26.6
Performance Analysis
The decision rule we derived in Section 26.5.1 uses the observed SP

Y (t)

to
compute the vector T of inner products with an orthonormal basis (Ï†1, . . . , Ï†d)
via (26.28), with the result that the vector T has the conditional law speciï¬ed
in (26.30).
Our decision rule then performs MAP decoding of M based on T.
Consequently, the performance of our decoding rule is identical to the performance
of the MAP rule for guessing M based on a vector T having the conditional law
(26.30). The performance of this latter decoding rule was studied in Section 21.6.
All that remains is to translate the results from that section in order to obtain
performance bounds on our decoder.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.6 Performance Analysis
633
To translate the results from Section 21.6 we need to substitute N0/2 for Ïƒ2 there;
d for J there; and (26.31) for the mean vectors there (see Table 26.1). But there is
one more translation we need: the bounds in Section 21.6 are expressed in terms
of the Euclidean distance between the mean vectors, and here we prefer to express
the bounds in terms of the â€œdistanceâ€ between the mean signals.
Fortunately,
as we next show, the translation is straightforward. Because (Ï†1, . . . , Ï†d) is an
orthonormal basis for span(s1, . . . , sM), it follows from Proposition 4.6.9 that
d

â„“=1
âŸ¨v, Ï†â„“âŸ©2 = âˆ¥vâˆ¥2
2 ,
v âˆˆspan(s1, . . . , sM).
(26.40)
Substituting smâ€² âˆ’smâ€²â€² for v and hence âŸ¨smâ€², Ï†â„“âŸ©âˆ’âŸ¨smâ€²â€², Ï†â„“âŸ©for âŸ¨v, Ï†â„“âŸ©in this
identity yields
d

â„“=1

âŸ¨smâ€², Ï†â„“âŸ©âˆ’âŸ¨smâ€²â€², Ï†â„“âŸ©
2 = âˆ¥smâ€² âˆ’smâ€²â€²âˆ¥2
2
(26.41)
=
 âˆ
âˆ’âˆ

smâ€²(t) âˆ’smâ€²â€²(t)
2 dt.
(26.42)
Thus, the squared Euclidean distance between two mean vectors in Section 21.6
is equal to the energy in the diï¬€erence between the corresponding mean signals in
our setup.
Denoting by pMAP(error|M = m) the conditional probability of error of our decoder
conditional on M = m, and denoting by pâˆ—(error) its unconditional probability of
error (which is the optimal probability of error)
pâˆ—(error) =

mâˆˆM
Ï€m pMAP(error|M = m),
(26.43)
we obtain from (21.58)
pMAP(error|M = m) â‰¤

mâ€²Ì¸=m
Q
	âˆ¥sm âˆ’smâ€²âˆ¥2
âˆš2N0
+

N0/2
âˆ¥sm âˆ’smâ€²âˆ¥2
ln Ï€m
Ï€mâ€²

(26.44)
and hence by, (26.43),
pâˆ—(error) â‰¤

mâˆˆM
Ï€m

mâ€²Ì¸=m
Q
	âˆ¥sm âˆ’smâ€²âˆ¥2
âˆš2N0
+

N0/2
âˆ¥sm âˆ’smâ€²âˆ¥2
ln Ï€m
Ï€mâ€²

.
(26.45)
When M is uniform these bounds simplify to
pMAP(error|M = m) â‰¤

mâ€²Ì¸=m
Q
â›
â

âˆ¥sm âˆ’smâ€²âˆ¥2
2
2N0
â
â ,
M uniform
(26.46)
and
pâˆ—(error) â‰¤1
M

mâˆˆM

mâ€²Ì¸=m
Q
â›
â

âˆ¥sm âˆ’smâ€²âˆ¥2
2
2N0
â
â ,
M uniform.
(26.47)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

634
Detection in White Gaussian Noise
Similarly, we can use the results from Section 21.6 to lower-bound the probability
of a guessing error. Indeed, using (21.64) we obtain
pMAP(error|M = m) â‰¥max
mâ€²Ì¸=m Q
	âˆ¥sm âˆ’smâ€²âˆ¥2
âˆš2N0
+

N0/2
âˆ¥sm âˆ’smâ€²âˆ¥2
ln Ï€m
Ï€mâ€²

,
(26.48)
pâˆ—(error) â‰¥

mâˆˆM
Ï€m max
mâ€²Ì¸=m Q
	âˆ¥sm âˆ’smâ€²âˆ¥2
âˆš2N0
+

N0/2
âˆ¥sm âˆ’smâ€²âˆ¥2
ln Ï€m
Ï€mâ€²

.
(26.49)
For a uniform prior these bounds simplify to
pMAP(error|M = m) â‰¥max
mâ€²Ì¸=m Q
â›
â

âˆ¥sm âˆ’smâ€²âˆ¥2
2
2N0
â
â ,
M uniform,
(26.50)
pâˆ—(error) â‰¥1
M

mâˆˆM
max
mâ€²Ì¸=m Q
â›
â

âˆ¥sm âˆ’smâ€²âˆ¥2
2
2N0
â
â ,
M uniform.
(26.51)
26.7
The Front-End Filter
Receivers in practice rarely have the structure depicted in Figure 26.3 becauseâ€”
although mathematically optimalâ€”its hardware implementation is challenging.
The diï¬ƒculty is related to the â€œdynamic rangeâ€ problem in implementing the
matched ï¬lter: it is very diï¬ƒcult to design a perfectly-linear system to exact speciï¬-
cation. Linearity is usually only guaranteed for a certain range of input amplitudes.
Once the amplitude of the signal exceeds a certain level, the circuit often â€œclipsâ€
the input waveform and no longer behaves linearly. Similarly, input signals that
are too small might be below the sensitivity of the circuit and might therefore
produce no output, thus violating linearity. This is certainly the case with circuits
that employ analog-to-digital conversion followed by digital processing, because
analog-to-digital converters can only represent the input with ï¬nite precision. The
problem with the structure depicted in Figure 26.3 is that the noise

N(t)

is typ-
ically much larger than the mean signal, so it becomes very diï¬ƒcult to design a
circuit to exact speciï¬cations that will be linear enough to guarantee that its action
on the received waveform (consisting of the weak transmitted waveform and the
strong additive noise) be the sum of the required responses to the mean signal and
to the noise-signal. (That the noise is typically much larger than the mean signals
can be seen from the heuristic plot of its PSD in Figure 25.3. White Gaussian
noise is often of PSD N0/2 over frequency bands that are much larger than the
band [âˆ’W, W ] so, by (25.31), the variance of the noise can be extremely large.)
The engineering solution to the dynamic range problem is to pass the received
waveform through a â€œfront-end ï¬lterâ€ and to then feed this ï¬lterâ€™s output to the
matched ï¬lter as in Figure 26.4. Except for a few very stringent requirements,
the speciï¬cations of the front-end ï¬lter are relatively lax. The ï¬rst requirement
is that the ï¬lter be linear over a very large range of input levels. This is usually
accomplished by using only passive elements in its implementation. The second
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.7 The Front-End Filter
635

Y (t)

~s1
~s2
~sM
...
Guess
Decision Rule
sample at t = 0
Front-End
Figure 26.4: Feeding the signal to a front-end ï¬lter and then computing the inner
products with the mean signals.
1
f
Ë†hFE(f)
âˆ’W
W
Figure 26.5: An example of the frequency response of a front-end ï¬lter.
requirement is that the front-end ï¬lterâ€™s frequency response be of unit gain over
the mean signalsâ€™ frequency band [âˆ’W, W ] so that it will not distort the mean
signals.4 Additionally, we require that the ï¬lter be stable and that its frequency
response decay to zero sharply for frequencies outside the band [âˆ’W, W ]. This
latter condition guarantees that the ï¬lterâ€™s response to the noise be of small variance
so that the dynamic range of the signal at the ï¬lterâ€™s output be moderate. If we
denote the front-end ï¬lterâ€™s impulse response by hFE, then the key mathematical
requirements are linearity; stability, i.e.,
 âˆ
âˆ’âˆ
hFE(t)
 dt < âˆ;
(26.52)
and the unit-gain requirement
Ë†hFE(f) = 1,
|f| â‰¤W.
(26.53)
An example of the frequency response of a front-end ï¬lter is depicted in Figure 26.5.
4Imprecisions here can often be corrected using signal processing.
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

636
Detection in White Gaussian Noise
In the rest of this section we shall prove that, as long as these assumptions are met,
there is no loss of optimality in introducing the front-end ï¬lter as in Figure 26.4.
(In the ideal mathematical world there is, of course, nothing to be gained from this
ï¬lter, because the architecture depicted in Figure 26.3 is optimal.)
The crux of the proof is in showing thatâ€”like

Y (t)

â€”the front-end ï¬lterâ€™s output
is the sum of the transmitted signal and WGN of PSD N0/2 with respect to the
bandwidth W. Once this is established, the result follows by recalling that the
conditional joint distribution of the matched ï¬ltersâ€™ outputs does not depend on
the PSD of the noise outside the band [âˆ’W, W ] (Note 26.4.2).
We thus proceed to analyze the front-end ï¬lterâ€™s output, which we denote by
 ËœY (t)

:
 ËœY (t)

=

Y (t)

â‹†hFE.
(26.54)
We ï¬rst note that (26.53) and the assumption that sm is an integrable signal that
is bandlimited to W Hz guarantee that
sm â‹†hFE = sm,
m âˆˆM
(26.55)
(Proposition 6.5.2 and Proposition 6.4.10 cf. (b)). By (26.55) and by the linearity
of the ï¬lter we can thus express the ï¬lterâ€™s output (conditional on M = m) as
 ËœY (t)

=

Y (t)

â‹†hFE
= sm â‹†hFE +

N(t)

â‹†hFE
= sm +

N(t)

â‹†hFE.
(26.56)
We next show that the SP

N(t)

â‹†hFE on the RHS of (26.56) is WGN of PSD N0/2
with respect to the bandwidth W. This follows from Theorem 25.13.2. Indeed,
being the result of passing a measurable centered stationary Gaussian SP through
a stable ï¬lter, it is a measurable centered stationary Gaussian SP. And its PSD is
f 	â†’SNN(f)
Ë†hFE(f)
2,
(26.57)
which is equal to N0/2 for all frequencies f âˆˆ[âˆ’W, W ], because for these frequen-
cies SNN(f) is equal to N0/2 and Ë†hFE(f) is equal to one. At frequencies outside
the band [âˆ’W, W ] the PSD of

N(t)

â‹†hFE may diï¬€er from that of

N(t)

.
We thus conclude that the front-end ï¬lterâ€™s output, like its input, can be expressed
as the transmitted signal corrupted by WGN of PSD N0/2 with respect to the
bandwidth W. Note 26.4.2 now guarantees that for every m âˆˆM we have that,
conditional on M = m, the distribution of
	 âˆ
âˆ’âˆ
ËœY (t) s1(t) dt, . . . ,
 âˆ
âˆ’âˆ
ËœY (t) sM(t) dt

T
is identical to the conditional distribution of the random vector in (26.12).
The advantage of the front-end ï¬lter becomes apparent when we re-examine the
PSD of the noise at its output. If the front-end ï¬lterâ€™s frequency response decays
very sharply to zero for frequencies outside the band [âˆ’W, W ], then, by (26.57),
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.8 Detection in Passband
637
this PSD will be nearly zero outside this band. Consequently, the variance of the
noise at the front-end ï¬lterâ€™s outputâ€”which is the integral of this PSDâ€”will be
greatly reduced. This will guarantee that the dynamic range at the ï¬lterâ€™s output
be much smaller than at its input, thus simplifying the implementation of the
matched ï¬lters.
26.8
Detection in Passband
The detection problem in passband is very similar to the one in baseband. The
diï¬€erence is that the mean signals {sm} are now assumed to be integrable signals
that are bandlimited to W Hz around the carrier frequency fc (Deï¬nition 7.2.1)
and that the noise is now assumed to be WGN of PSD N0/2 with respect to the
bandwidth W around fc (Deï¬nition 25.15.9).
Here too, there is no loss of optimality in basing our guess on the vector

âŸ¨Y,Ëœs1âŸ©, . . . , âŸ¨Y,ËœsnâŸ©
T
(26.58)
whenever the signals Ëœs1, . . . ,Ëœsn are integrable signals that are bandlimited to W
Hz around fc and satisfy
sm âˆˆspan(Ëœs1, . . . ,Ëœsn),
m âˆˆM.
Conditional on M = m, this vector is Gaussian with mean vector (26.22b) and
covariance matrix (26.22c). The latter covariance matrix can also be written in
terms of the baseband representation of the mean signals using the relation
âŸ¨Ëœsjâ€²,Ëœsjâ€²â€²âŸ©= 2 Re

âŸ¨Ëœsjâ€²,BB,Ëœsjâ€²â€²,BBâŸ©

,
(26.59)
where Ëœsjâ€²,BB and Ëœsjâ€²â€²,BB are the baseband representations of Ëœsjâ€² and Ëœsjâ€²â€² (Theo-
rem 7.6.10).
The computation of the inner products (26.11) can be performed in passband
by feeding the signal Y directly to ï¬lters that are matched to the passband sig-
nals {Ëœsj}, or in baseband by expressing the inner product âŸ¨Y,ËœsjâŸ©in terms of the
baseband representation Ëœsj,BB of Ëœsj as follows:
âŸ¨Y,ËœsjâŸ©=

Y, t 	â†’2 Re

Ëœsj,BB(t) ei2Ï€fct
= 2
 âˆ
âˆ’âˆ

Y (t) cos(2Ï€fct)

Re

Ëœsj,BB(t)

dt
âˆ’2
 âˆ
âˆ’âˆ

Y (t) sin(2Ï€fct)

Im

Ëœsj,BB(t)

dt.
This expression suggests computing the inner product âŸ¨Y,ËœsjâŸ©using two baseband
matched ï¬lters: one that is matched to Re(Ëœsj,BB) and that is fed the product
of

Y (t)

and cos(2Ï€fct), and one that is matched to Im(Ëœsj,BB) and that is fed the
product of

Y (t)

and sin(2Ï€fct).5
5Since the baseband representation of an integrable passband signal that is bandlimited to
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

638
Detection in White Gaussian Noise
As discussed in Section 26.7, in practice one typically ï¬rst feeds the received sig-
nal

Y (t)

to a stable highly-linear bandpass ï¬lter of frequency response Ë†hPB-FE(Â·)
satisfying
Ë†hPB-FE(f) = 1,
|f| âˆ’fc
 â‰¤W/2,
(26.60)
with the frequency response decaying drastically at other frequencies to guarantee
that the ï¬lterâ€™s output be of small dynamic range. Moreover, we typically prefer to
use matched ï¬lters that do not depend on the carrier frequency in order to allow
it to be tunable. This issue is discussed in greater detail in Section 32.5.
26.9
Some Examples
26.9.1
Binary Hypothesis Testing
Before treating the general binary hypothesis testing problem we begin with the
case of antipodal signaling with a uniform prior. In this case
s0 = âˆ’s1 = s,
(26.61)
where s is some integrable signal that is bandlimited to W Hz.
We denote its
energy by Es, i.e.,
Es = âˆ¥sâˆ¥2
2
(26.62)
and assume that it is strictly positive. In this case the dimension of the linear
subspace spanned by the mean signals is one, and this subspace is spanned by the
unit-norm signal
Ï† =
s
âˆ¥sâˆ¥2
.
(26.63)
Depending on the outcome of a fair coin toss, either s or âˆ’s is sent over the channel.
We observe the SP

Y (t)

given by the sum of the transmitted signal and WGN
of PSD N0/2 with respect to the bandwidth W, and we wish to guess which signal
was sent. How should we form our guess?
By Theorem 26.3.1 there is no loss of optimality in forming our guess based on
the RV T = âŸ¨Y, Ï†âŸ©. Conditional on H = 0, we have T âˆ¼N
âˆšEs, N0/2

, whereas,
conditional on H = 1, we have T âˆ¼N

âˆ’âˆšEs, N0/2

. How to guess H based on T
is the problem we addressed in Section 20.10. There we showed that it is optimal
to guess â€œH = 0â€ if T â‰¥0 and to guess â€œH = 1â€ if T < 0. (The case T = 0 occurs
with probability zero, so we need not worry about it.) An optimal decision rule for
guessing H based on

Y (t)

is thus:
Guess â€œH = 0â€ if
 âˆ
âˆ’âˆ
Y (t) s(t) dt â‰¥0.
(26.64)
W Hz around the carrier frequency fc is integrable (Proposition 7.6.2), it follows that our as-
sumption that Ëœsj is an integrable function that is bandlimited to W Hz around the carrier fre-
quency fc guarantees that both t â†’cos(2Ï€fct) Re

Ëœsj,BB(t)

and t â†’sin(2Ï€fct) Im

Ëœsj,BB(t)

are
integrable. Hence, with probability one, both the integrals
 âˆ
âˆ’âˆ

Y (t) cos(2Ï€fct)

Re

Ëœsj,BB(t)

dt
and
 âˆ
âˆ’âˆ

Y (t) sin(2Ï€fct)

Im

Ëœsj,BB(t)

dt exist.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.9 Some Examples
639
Let pMAP(error|s) denote the conditional probability of error of this decision rule
given that s was sent; let pMAP(error|âˆ’s) be similarly deï¬ned; and let pâˆ—(error)
denote the optimal probability of error of this problem. By the optimality of our
rule,
pâˆ—(error) = 1
2

pMAP(error|s) + pMAP(error|âˆ’s)

.
Using the expression for the error probability derived in Section 20.10 we obtain
pâˆ—(error) = Q
â›
â

2 âˆ¥sâˆ¥2
2
N0
â
â ,
(26.65)
which, in view of (26.62), can also be written as
pâˆ—(error) = Q
-4
2Es
N0
.
.
(26.66)
Note that, as expected from Section 26.4.2 and in particular from Proposition 26.4.3,
the probability of error is determined by the â€œgeometryâ€ of the problem, which in
this case is summarized by the energy in s.
There is also a nice geometric interpretation to (26.66). The distance between the
mean signals s and âˆ’s is âˆ¥s âˆ’(âˆ’s)âˆ¥2 = 2âˆšEs. Half the distance is âˆšEs. The
inner product between the noise and the unit-length vector Ï† pointing from âˆ’s
to s is N(0, N0/2). Half the distance thus corresponds to âˆšEs/

N0/2 standard
deviations of this inner product. The probability of error is thus the probability
that a standard Gaussian is greater than half the distance between the signals as
measured by standard deviations of the inner product between the noise and the
unit-length vector pointing from âˆ’s towards s.
Consider now the more general binary hypothesis testing problem where both hy-
potheses are still equally likely, but where the mean signals s0 and s1 are not
antipodal, i.e., they do not sum to zero. Our approach to this problem is to reduce
it to the antipodal case we already treated. We begin by forming the signal
 ËœY (t)

by subtracting (s0 + s1)/2 from the received signal, so
ËœY (t) = Y (t) âˆ’1
2

s0(t) + s1(t)

,
t âˆˆR.
(26.67)
Since Y (t) can be recovered from ËœY (t) by adding

s0(t) + s1(t)

/2, the smallest
probability of a guessing error that can be achieved based on
 ËœY (t)

is no larger
than that which can be achieved based on

Y (t)

. (The two are, in fact, the same
because
 ËœY (t)

can be computed from

Y (t)

.)
The advantage of using
 ËœY (t)

becomes apparent once we compute its conditional
law given H. Conditional on H = 0, we have ËœY (t) = (s0(t) âˆ’s1(t))/2 + N(t),
whereas conditional on H = 1, we have ËœY (t) = âˆ’(s0(t)âˆ’s1(t))/2+N(t). Thus, the
guessing problem given
 ËœY (t)

is exactly the problem we addressed in the antipodal
case with (s0 âˆ’s1)/2 playing the role of s. We thus obtain that an optimal decision
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

640
Detection in White Gaussian Noise
rule is to guess â€œH = 0â€ if
 ËœY (t)

s0(t) âˆ’s1(t)

/2 dt is nonnegative. Or stated in
terms of

Y (t)

using (26.67):
Guess â€œH = 0â€ if
 âˆ
âˆ’âˆ
	
Y (t) âˆ’s0(t) + s1(t)
2

s0(t) âˆ’s1(t)
2
dt â‰¥0.
(26.68)
The error probability associated with this decision rule is obtained from (26.65) by
substituting (s0 âˆ’s1)/2 for s:
pâˆ—(error) = Q
â›
â

âˆ¥s0 âˆ’s1âˆ¥2
2
2N0
â
â .
(26.69)
This expression too has a nice geometric interpretation. The inner product between
the noise and the unit-norm signal that is pointing from s1 to s0 is N(0, N0/2). The
â€œdistanceâ€ between the signals is âˆ¥s0 âˆ’s1âˆ¥2. Half the distance is âˆ¥s0 âˆ’s1âˆ¥2 /2,
which corresponds to
âˆ¥s0 âˆ’s1âˆ¥2 /2

N0/2
standard deviations of a N(0, N0/2) random variable. The probability of error
(26.69) is thus the probability that the inner product between the noise and the
unit-norm signal that is pointing from s1 to s0 exceeds half the distance between
the signals.
26.9.2
8-PSK
We next present an example of detection in passband. For concreteness we consider
8-PSK, which stands for â€œ8-ary Phase Shift Keying.â€ Here the number of hypothe-
ses is eight, so M = {1, 2, . . . , 8} and M = 8. We assume that M is uniformly
distributed over M. Conditional on M = m, the received signal is given by
Y (t) = sm(t) + N(t),
t âˆˆR,
(26.70)
where
sm(t) = 2 Re

cmsBB(t) ei2Ï€fct
,
t âˆˆR;
(26.71)
cm = Î± eim 2Ï€
8
(26.72)
for some positive real Î±; the baseband signal sBB is an integrable complex signal
that is bandlimited to W/2 Hz and of unit energy
âˆ¥sBBâˆ¥2 = 1;
(26.73)
the carrier frequency fc satisï¬es fc > W/2; and

N(t)

is WGN of PSD N0/2 with
respect to the bandwidth W around the carrier frequency fc (Deï¬nition 25.15.9).
Irrespective of M, the transmitted energy Es is given by
Es = âˆ¥smâˆ¥2
2
=
 âˆ
âˆ’âˆ

2 Re

cmsBB(t) ei2Ï€fct2
dt
= 2Î±2,
(26.74)
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.9 Some Examples
641
m = 1
t(1)
t(2)
guess 1
Figure 26.6: Region of points (t(1), t(2)) resulting in guessing â€œM = 1.â€
as can be veriï¬ed using the relationship between energy in passband and baseband
(Theorem 7.6.10) and using (26.73).
The transmitted waveform sm can also be written in a form that is highly suggestive
of a choice of an orthonormal basis for span(s1, . . . , sM):
sm(t) =
âˆš
2 Re(cm)
âˆš
2 Re

sBB(t) ei2Ï€fct



Ï†1(t)
+
âˆš
2 Im(cm)
âˆš
2 Re

i sBB(t) ei2Ï€fct



Ï†2(t)
=
âˆš
2 Re(cm) Ï†1(t) +
âˆš
2 Im(cm) Ï†2(t),
where
Ï†1(t) â‰œ
âˆš
2 Re

sBB(t) ei2Ï€fct
,
t âˆˆR,
Ï†2(t) â‰œ
âˆš
2 Re

i sBB(t) ei2Ï€fct
,
t âˆˆR.
From Theorem 7.6.10 on inner products in passband and baseband, it follows that
Ï†1 and Ï†2 are orthogonal. Also, from that theorem and from (26.73), it follows
that they are of unit energy. Thus, the tuple (Ï†1, Ï†2) is an orthonormal basis for
span(s1, . . . , sM). Consequently, there is no loss of optimality in basing our guess
on the random vector T = (âŸ¨Y, Ï†1âŸ©, âŸ¨Y, Ï†2âŸ©)T, and conditional on M = m, the
components of T are independent with T (1) âˆ¼N
âˆš
2Î± cos(2Ï€m/8), N0/2

and with
T (2) âˆ¼N
âˆš
2Î± sin(2Ï€m/8), N0/2

. We have thus reduced the guessing problem to
that of guessing M based on a two-dimensional vector T.
The problem of guessing M based on T was studied in Section 21.4. To lift the
results from that section, we need to substitute
âˆš
2Î± for A and to substitute N0/2
for Ïƒ2. For example, the region where we guess â€œM = 1â€ is given in Figure 26.6.
For the scenario we described, some engineers prefer to use complex random vari-
ables (Chapter 17).
Rather than viewing T as a two-dimensional real random
vector, they prefer to view it as a (scalar) complex random variable whose real
part is âŸ¨Y, Ï†1âŸ©and whose imaginary part is âŸ¨Y, Ï†2âŸ©. Conditional on M = m, this
CRV has the form
âˆš
2cm + Z,
Z âˆ¼NC(0, N0) ,
(26.75)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

642
Detection in White Gaussian Noise
where NC(0, N0) denotes the circularly-symmetric variance-N0 complex Gaussian
distribution (Note 24.3.13).
The expression for the probability of error of our detector can also be lifted from
Section 21.4.
Substituting, as above,
âˆš
2Î± for A and N0/2 for Ïƒ2, we obtain
from (21.26) that the conditional probability of error pMAP(error|M = m) of our
proposed decision rule is given for every m âˆˆM by
pMAP(error|M = m) = 1
Ï€
 Ï€âˆ’Ïˆ
0
e
âˆ’
2Î±2 sin2 Ïˆ
N0 sin2(Î¸+Ïˆ) dÎ¸,
Ïˆ = Ï€
8 .
The conditional probability of error can also be expressed in terms of the trans-
mitted energy Es using (26.74).
Doing that and recalling that the conditional
probability of error does not depend on the transmitted message, we obtain that
the average probability of error pâˆ—(error) is given by
pâˆ—(error) = 1
Ï€
 Ï€âˆ’Ïˆ
0
e
âˆ’
Es sin2 Ïˆ
N0 sin2(Î¸+Ïˆ) dÎ¸,
Ïˆ = Ï€
8 .
(26.76)
Note 26.9.1. The expression (26.76) continues to hold also for M-PSK where
cm = Î± ei2Ï€m/M for M â‰¥2 not necessarily equal to 8, provided that we deï¬ne
Ïˆ = Ï€/M in (26.76).
26.9.3
Orthogonal Keying
We next consider M-ary orthogonal keying. We assume that the RV M that we
wish to guess is uniformly distributed over the set M = {1, . . . , M}, where M â‰¥2.
The mean signals are assumed to be orthogonal and of equal (strictly) positive
energy Es:
âŸ¨smâ€², smâ€²â€²âŸ©= Es I{mâ€² = mâ€²â€²},
mâ€², mâ€²â€² âˆˆM.
(26.77)
Since M is uniform, and since the mean signals are of equal energy, it follows
from Theorem 26.5.3 that to minimize the probability of guessing incorrectly, it is
optimal to correlate the received waveform

Y (t)

with each of the mean signals
and to pick the message whose mean signal gives the highest correlation:
Guess â€œmâ€ if âŸ¨Y, smâŸ©= max
mâ€²âˆˆM âŸ¨Y, smâ€²âŸ©
(26.78)
with ties (which occur with probability zero) being resolved by picking a random
message among those that attain the highest correlation.
We next address the probability of error of this optimal decision rule. We ï¬rst
deï¬ne the vector (T (1), . . . , T (M))T by
T (â„“) =
 âˆ
âˆ’âˆ
Y (t) sâ„“(t)
âˆšEs
dt,
â„“âˆˆ{1, . . . , M}
and recast the decision rule as guessing â€œM = mâ€ if T (m) = maxmâ€²âˆˆM T (mâ€²), with
ties being resolved at random among the components of T that are maximal.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.9 Some Examples
643
Let pMAP(error|M = m) denote the conditional probability of error of this decoding
rule, conditional on M = m. Conditional on M = m, an error occurs in two cases:
when m does not attain the highest score or when m attains the highest score but
this score is also attained by some other message and the tie is not resolved in mâ€™s
favor. Since the probability of a tie is zero (Note 26.5.4), we may ignore the second
case and only compute the probability that an incorrect message is assigned a score
that is (strictly) higher than the one associated with m. Thus,
pMAP(error|M = m)
= Pr

max

T (1), . . . , T (mâˆ’1), T (m+1), . . . , T (M)
> T (m)  M = m

.
(26.79)
From (26.30) and the orthogonality of the signals (26.77) we have that, condi-
tional on M = m, the random vector T is Gaussian with the mean of its m-th
component being âˆšEs, the mean of its other components being zero, and with all
the components being of variance N0/2 and independent of each other. Thus, the
conditional probability of error given M = m is â€œthe probability that at least one
of M âˆ’1 IID N(0, N0/2) random variables exceeds the value of a N
âˆšEs, N0/2

random variable that is independent of them.â€ Having recast the probability of
error conditional on M = m in a way that does not involve m (the clause in quotes
makes no reference to m), we conclude that the conditional probability of error
given that M = m does not depend on m:
pMAP(error|M = m) = pMAP(error|M = 1),
m âˆˆM.
(26.80)
This conditional probability of error can be computed starting from (26.79) as:
pMAP(error|M = 1)
= Pr

max

T (2), . . . , T (M)
> T (1)  M = 1

= 1 âˆ’Pr

max

T (2), . . . , T (M)
â‰¤T (1)  M = 1

= 1 âˆ’
 âˆ
âˆ’âˆ
fT (1)|M=1(t) Pr

max

T (2), . . . , T (M)
â‰¤t
 M = 1, T (1) = t

dt
= 1 âˆ’
 âˆ
âˆ’âˆ
fT (1)|M=1(t) Pr

max

T (2), . . . , T (M)
â‰¤t
 M = 1

dt
= 1 âˆ’
 âˆ
âˆ’âˆ
fT (1)|M=1(t) Pr

T (2) â‰¤t, . . . , T (M) â‰¤t
 M = 1

dt
= 1 âˆ’
 âˆ
âˆ’âˆ
fT (1)|M=1(t)

Pr

T (2) â‰¤t
 M = 1
Mâˆ’1
dt
= 1 âˆ’
 âˆ
âˆ’âˆ
fT (1)|M=1(t)
-
1 âˆ’Q
	
t

N0/2

.Mâˆ’1
dt
= 1 âˆ’
 âˆ
âˆ’âˆ
1
âˆšÏ€N0
eâˆ’
(tâˆ’âˆš
Es)2
N0
-
1 âˆ’Q
	
t

N0/2

.Mâˆ’1
dt
= 1 âˆ’
1
âˆš
2Ï€
 âˆ
âˆ’âˆ
eâˆ’Ï„ 2/2
-
1 âˆ’Q
	
Ï„ +
4
2Es
N0

.Mâˆ’1
dÏ„,
(26.81)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

644
Detection in White Gaussian Noise
where the ï¬rst equality follows from (26.79); the second because the conditional
probability of an event and its complement add to one; the third by conditioning
on T (1) = t and integrating it out, i.e., by noting that for any random variable X
of density fX(Â·) and for any random variable Y ,
Pr

Y â‰¤X

=
 âˆ
âˆ’âˆ
fX(x) Pr

Y â‰¤x
 X = x

dx,
(26.82)
with X here being equal to T (1) and with Y here being max{T (2), . . . , T (M)}; the
fourth from the conditional independence of T (1) and (T (2), . . . , T (M)) given M = 1,
which implies the conditional independence of T (1) and max{T (2), . . . , T (M)} given
M = 1; the ï¬fth because the maximum of random variables does not exceed t if,
and only if, none of them exceeds t

max{T (2), . . . , T (M)} â‰¤t

â‡â‡’

T (2) â‰¤t, . . . , T (M) â‰¤t

;
the sixth because, conditional on M = 1, the random variables T (2), . . . , T (M) are
IID so
Pr

T (2) â‰¤t, . . . , T (M) â‰¤t
 M = 1

=

Pr

T (2) â‰¤t
 M = 1
Mâˆ’1
;
the seventh because, conditional on M = 1, we have T (2) âˆ¼N(0, N0/2) and using
(19.12b); the eighth because, conditional on M = 1, we have T (1) âˆ¼N
âˆšEs, N0/2

so its conditional density can be written explicitly using (19.6); and the ï¬nal equal-
ity using the change of variable
Ï„ â‰œt âˆ’âˆšEs

N0/2
.
(26.83)
Using (26.80) and (26.81) we obtain that if pâˆ—(error) denotes the unconditional
probability of error, then pâˆ—(error) = pMAP(error|M = 1) and
pâˆ—(error) = 1 âˆ’
1
âˆš
2Ï€
 âˆ
âˆ’âˆ
eâˆ’Ï„ 2/2
-
1 âˆ’Q
	
Ï„ +
4
2Es
N0

.Mâˆ’1
dÏ„.
(26.84)
An alternative expression for the probability of error can be derived using the
Binomial Expansion
(a + b)n =
n

j=0
	n
j

anâˆ’j bj,

n âˆˆN, a, b âˆˆR

.
(26.85)
Substituting
a = 1,
b = âˆ’Q
	
Ï„ +
4
2Es
N0

,
n = M âˆ’1,
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.9 Some Examples
645
in (26.85) yields
-
1 âˆ’Q
	
Ï„ +
4
2Es
N0

.Mâˆ’1
=
Mâˆ’1

j=0
(âˆ’1)j
	M âˆ’1
j

-
Q
	
Ï„ +
4
2Es
N0

.j
= 1 +
Mâˆ’1

j=1
(âˆ’1)j
	M âˆ’1
j

-
Q
	
Ï„ +
4
2Es
N0

.j
,
from which we obtain from (26.84) (using the linearity of integration and the fact
that the Gaussian density integrates to one)
pâˆ—(error) =
Mâˆ’1

j=1
(âˆ’1)j+1
	M âˆ’1
j

  âˆ
âˆ’âˆ
1
âˆš
2Ï€ eâˆ’Ï„ 2/2
-
Q
	
Ï„ +
4
2Es
N0

.j
dÏ„.
(26.86)
For the case where M = 2 the expression (26.84) for the probability of error can
be simpliï¬ed to
pâˆ—(error) = Q
-4
Es
N0
.
,
M = 2,
(26.87)
as we proceed to show in two diï¬€erent ways. The ï¬rst way is to note that for
M = 2 the probability of error can be expressed, using (26.79) and (26.80), as
pMAP(error|M = 1) = Pr

T (2) > T (1)  M = 1

= Pr

T (2) âˆ’T (1) > 0
 M = 1

= Q
-4
Es
N0
.
,
where the last equality follows because, conditional on M = 1, the random vari-
ables T (1) and T (2) are independent Gaussians of variance N0/2 with the ï¬rst
having mean âˆšEs and the second having zero mean, so their diï¬€erence T (2) âˆ’T (1)
is N

âˆ’âˆšEs, N0

. (The probability that a N

âˆ’âˆšEs, N0

RV exceeds zero can be
computed using (19.12a).) The second way of showing (26.87) it to use (26.69) and
to note that the orthogonality of s1 and s2 implies âˆ¥s1 âˆ’s2âˆ¥2
2 = âˆ¥s1âˆ¥2
2 + âˆ¥s2âˆ¥2
2 =
2Es.
26.9.4
The M-ary Simplex
We next describe a detection problem that is intimately related to the problem we
addressed in Section 26.9.3. To motivate the problem we ï¬rst note:
Proposition 26.9.2. Consider the setup described in Section 26.2.
If s is any
integrable signal that is bandlimited to W Hz, then the probability of error associated
with the mean signals {s1, . . . , sM} and the prior {Ï€m} is the same as with the mean
signals {s1 âˆ’s, . . . , sM âˆ’s} and the same prior.
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

646
Detection in White Gaussian Noise
Proof. We have essentially given a proof of this result in Section 14.3 and also
in Section 26.9.1 in our analysis of nonantipodal signaling. The idea is that, by
subtracting the signal s from the received waveform, the receiver can make the
problem with mean signals {s1, . . . , sM} appear as though it were the problem
with mean signals {s1 âˆ’s, . . . , sM âˆ’s}.
Conversely, by adding s, the receiver
can make the latter appear as though it were the former. Consequently, the best
performance achievable in the two settings must be identical.
The expected transmitted energy when employing the mean signals {s1, . . . , sM}
may be diï¬€erent than when employing the mean signals {s1 âˆ’s, . . . , sM âˆ’s}. In
subtracting the signal s one can change the average transmitted energy for better
or worse. As we argued in Section 14.4, to minimize the expected transmitted
energy, one should choose s to correspond to the â€œcenter of gravityâ€ of the mean
signals:
Proposition 26.9.3. Let the prior {Ï€m} and mean signals {sm} be given. Let
sâˆ—=

mâˆˆM
Ï€m sm.
(26.88)
Then, for any energy-limited signal s

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ—âˆ¥2
2 â‰¤

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ¥2
2 ,
(26.89)
with equality if, and only if, s is indistinguishable from sâˆ—.
Proof. Writing sm âˆ’s as (sm âˆ’sâˆ—) + (sâˆ—âˆ’s) we have

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ¥2
2
=

mâˆˆM
Ï€m âˆ¥(sm âˆ’sâˆ—) + (sâˆ—âˆ’s)âˆ¥2
2
=

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ—âˆ¥2
2 +

mâˆˆM
Ï€m âˆ¥sâˆ—âˆ’sâˆ¥2
2 + 2

mâˆˆM
Ï€m âŸ¨sm âˆ’sâˆ—, sâˆ—âˆ’sâŸ©
=

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ—âˆ¥2
2 + âˆ¥sâˆ—âˆ’sâˆ¥2
2 + 2
 
mâˆˆM
Ï€m(sm âˆ’sâˆ—), sâˆ—âˆ’s

=

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ—âˆ¥2
2 + âˆ¥sâˆ—âˆ’sâˆ¥2
2 + 2 âŸ¨sâˆ—âˆ’sâˆ—, sâˆ—âˆ’sâŸ©
=

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ—âˆ¥2
2 + âˆ¥sâˆ—âˆ’sâˆ¥2
2
â‰¥

mâˆˆM
Ï€m âˆ¥sm âˆ’sâˆ—âˆ¥2
2 ,
with the inequality being an equality if, and only if, âˆ¥sâˆ—âˆ’sâˆ¥2
2 = 0.
We can now construct the simplex signals as follows. We start with M orthonormal
waveforms Ï†1, . . . , Ï†M
âŸ¨Ï†mâ€², Ï†mâ€²â€²âŸ©= I{mâ€² = mâ€²â€²},
mâ€², mâ€²â€² âˆˆM
(26.90)
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.9 Some Examples
647
Ï†1
Ï†2
Â¯Ï†
Ï†2 âˆ’Â¯Ï†
Ï†1 âˆ’Â¯Ï†
Figure 26.7: Starting with two orthonormal signals and subtracting the â€œcenter of
gravityâ€ from each we obtain two antipodal signals. Scaling these antipodal signals
results in the simplex constellation with two signals.
that are integrable and bandlimited to W Hz. We set Â¯Ï† to be their â€œcenter of
gravityâ€ with respect to the uniform prior
Â¯Ï† = 1
M

mâˆˆM
Ï†m.
(26.91)
Using (26.90), (26.91), and the basic properties of the inner product (3.6)â€“(3.10)
it is easily veriï¬ed that

Ï†mâ€² âˆ’Â¯Ï†, Ï†mâ€²â€² âˆ’Â¯Ï†

= I

mâ€² = mâ€²â€²
âˆ’1
M,
mâ€², mâ€²â€² âˆˆM.
(26.92)
We now deï¬ne the M-ary simplex constellation with energy Es by
sm =

Es
4
M
M âˆ’1

Ï†m âˆ’Â¯Ï†

,
m âˆˆM.
(26.93)
The construction for the case where M = 2 is depicted in Figure 26.7. It yields
the binary antipodal signaling scheme. The construction for M = 3 is depicted in
Figure 26.8.
From (26.93) and (26.92) we obtain for distinct mâ€², mâ€²â€² âˆˆM
âˆ¥smâˆ¥2
2 = Es
and
âŸ¨smâ€², smâ€²â€²âŸ©= âˆ’
Es
M âˆ’1.
(26.94)
Also, from (26.93) we see that {sm} can be viewed as the result of subtracting the
center of gravity from orthogonal signals of energy Es M/(M âˆ’1). Consequently,
the least error probability that can be achieved in detecting simplex signals of
energy Es is the same as the least error probability that can be achieved in detecting
orthogonal signals of energy
M
M âˆ’1Es
(26.95)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

648
Detection in White Gaussian Noise
Figure 26.8: Constructing the simplex constellation with three points from three
orthonormal signals. Left ï¬gure depicts the orthonormal constellation and its cen-
ter of gravity; middle ï¬gure depicts the result of subtracting the center of gravity,
and the right ï¬gure depicts the result of scaling (from a diï¬€erent perspective).
(Proposition 26.9.2). From the expression for the error probability in orthogonal
signaling (26.84) we obtain for the simplex signals with a uniform prior
pâˆ—(error) = 1 âˆ’
1
âˆš
2Ï€
 âˆ
âˆ’âˆ
eâˆ’Ï„ 2/2
-
1 âˆ’Q
	
Ï„ +
4
M
M âˆ’1
2Es
N0

.Mâˆ’1
dÏ„.
(26.96)
The decision rule for the simplex constellation can also be derived by exploiting the
relationship to orthogonal keying. For example, if Ïˆ is a unit-energy integrable sig-
nal that is bandlimited to W Hz and that is orthogonal to the signals {s1, . . . , sM},
then, by (26.94), the waveforms
1
sm +
1
âˆš
M âˆ’1

Es Ïˆ
2
mâˆˆM
(26.97)
are orthogonal, each of energy Es M/(M âˆ’1). (See Figure 26.9 for a demonstra-
tion of the process of obtaining an orthogonal constellation with M = 2 signals by
adding a signal Ïˆ to each of the signals in a binary simplex constellation.) Conse-
quently, in order to decode the simplex signals contaminated by WGN with respect
to the bandwidth W, we can add
1
âˆšMâˆ’1
âˆšEs Ïˆ to the received waveform and then
feed the result to an optimal detector for orthogonal keying.
26.9.5
Bi-Orthogonal Keying
Starting with an orthogonal constellation, we can double the number of signals
without reducing the minimum distance. This construction, which results in the
â€œbi-orthogonal signal setâ€ is the topic of this section. To construct the bi-orthogonal
signal set with 2Îº signals, we start with Îº â‰¥1 orthonormal signals (Ï†1, . . . , Ï†Îº)
and deï¬ne the 2Îº bi-orthogonal signal set {s1,u, s1,d, . . . , sÎº,u, sÎº,d} by
sÎ½,u = +

Es Ï†Î½
and
sÎ½,d = âˆ’

Es Ï†Î½,
Î½ âˆˆ{1, . . . , Îº}.
(26.98)
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.9 Some Examples
649
s1
s2
âˆšEs Ïˆ
s1 + âˆšEs Ïˆ
s2 + âˆšEs Ïˆ
Figure 26.9:
Adding a properly scaled signal Ïˆ that is orthogonal to all the
elements of a simplex constellation results in an orthogonal constellation.
Figure 26.10: A bi-orthogonal constellation with six signals.
We can think of â€œuâ€ as standing for â€œupâ€ and of â€œdâ€ as standing for â€œdown,â€ so to
each signal Ï†Î½ there correspond two signals in the bi-orthogonal constellation: the
â€œup signalâ€ that corresponds to multiplying âˆšEsÏ†Î½ by +1 and the â€œdown signalâ€
that corresponds to multiplying âˆšEsÏ†Î½ by âˆ’1. Only bi-orthogonal signal sets with
an even number of signals are deï¬ned. The constructed signals are all of energy Es:
âˆ¥sÎ½,uâˆ¥2 = âˆ¥sÎ½,dâˆ¥2 =

Es,
Î½ âˆˆ{1, . . . , Îº}.
(26.99)
A bi-orthogonal constellation with six points (Îº = 3) is depicted in Figure 26.10.
Consider now the case where each of the signals Ï†1, . . . , Ï†Îº is an integrable sig-
nal that is bandlimited to W Hz and, consequently, so are all the signals in the
constructed bi-orthogonal signal set. A signal is picked uniformly at random from
the signal set and is sent over a channel. We observe the stochastic process

Y (t)

given by the sum of the transmitted signal and WGN of PSD N0/2 with respect
to the bandwidth W. How should we guess which signal was sent?
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

650
Detection in White Gaussian Noise
Since the signal was chosen equiprobably, and since all the signals in the signal set
are of the same energy, it is optimal to consider the inner products
âŸ¨Y, s1,uâŸ©, âŸ¨Y, s1,dâŸ©, . . . , âŸ¨Y, sÎº,uâŸ©, âŸ¨Y, sÎº,dâŸ©
(26.100)
and to pick the signal in the signal set corresponding to the largest of these in-
ner products. By (26.98) we have for every Î½ âˆˆ{1, . . . , Îº} that sÎ½,u = âˆ’sÎ½,d so
âŸ¨Y, sÎ½,uâŸ©= âˆ’âŸ¨Y, sÎ½,dâŸ©and hence
max

âŸ¨Y, sÎ½,uâŸ©, âŸ¨Y, sÎ½,dâŸ©

=
âŸ¨Y, sÎ½,uâŸ©
,
Î½ âˆˆ{1, . . . , Îº}.
(26.101)
Equivalently, by (26.98),
max

âŸ¨Y, sÎ½,uâŸ©, âŸ¨Y, sÎ½,dâŸ©

=

Es
âŸ¨Y, Ï†Î½âŸ©
,
Î½ âˆˆ{1, . . . , Îº}.
To ï¬nd the maximum of the 2Îº terms in (26.100) we can ï¬rst compute for each
Î½ âˆˆ{1, . . . , Îº} the maximum between âŸ¨Y, sÎ½,uâŸ©and âŸ¨Y, sÎ½,dâŸ©and then compute the
maximum of the Îº results:
max
'
âŸ¨Y, s1,uâŸ©, âŸ¨Y, s1,dâŸ©, . . . , âŸ¨Y, sÎº,uâŸ©, âŸ¨Y, sÎº,dâŸ©
(
= max
'
max

âŸ¨Y, s1,uâŸ©, âŸ¨Y, s1,dâŸ©

, . . . , max

âŸ¨Y, sÎº,uâŸ©, âŸ¨Y, sÎº,dâŸ©
(
.
Using this approach, we obtain from (26.101) the following optimal two-step proce-
dure: ï¬rst ï¬nd which Î½âˆ—in {1, . . . , Îº} attains the maximum of the absolute values
of the inner products
max
Î½âˆˆ{1,...,Îº}
âŸ¨Y, Ï†Î½âŸ©

and then, after you have found Î½âˆ—, guess â€œsÎ½âˆ—,uâ€ if âŸ¨Y, Ï†Î½âˆ—âŸ©> 0 and guess â€œsÎ½âˆ—,dâ€
if âŸ¨Y, Ï†Î½âˆ—âŸ©â‰¤0.
We next compute the probability of error of this optimal guessing rule. It is not
diï¬ƒcult to see that the conditional probability of error does not depend on the
message we condition on. For concreteness, we shall analyze the probability of
error associated with the message corresponding to the signal s1,u, a probability
that we denote by pMAP(error|s1,u), with the corresponding conditional probability
of correct decoding pMAP(correct|s1,u) = 1 âˆ’pMAP(error|s1,u).
To simplify the
typesetting, we shall denote the conditional probability of the event A given that
s1,u is sent by Pr(A|s1,u).
Since the probability of ties in the likelihood function is zero (Note 26.5.4),
pMAP(correct|s1,u)
= Pr

âˆ’âŸ¨Y, Ï†1âŸ©â‰¤âŸ¨Y, Ï†1âŸ©and max
2â‰¤Î½â‰¤Îº
âŸ¨Y, Ï†Î½âŸ©

â‰¤âŸ¨Y, Ï†1âŸ©
 s1,u

= Pr

âŸ¨Y, Ï†1âŸ©â‰¥0 and max
2â‰¤Î½â‰¤Îº
âŸ¨Y, Ï†Î½âŸ©

â‰¤âŸ¨Y, Ï†1âŸ©
 s1,u

=
 âˆ
0
fâŸ¨Y,Ï†1âŸ©|s1,u(t) Pr
%
max
2â‰¤Î½â‰¤Îº
âŸ¨Y, Ï†Î½âŸ©

â‰¤t
 s1,u, âŸ¨Y, Ï†1âŸ©= t
&
dt
=
 âˆ
0
fâŸ¨Y,Ï†1âŸ©|s1,u(t) Pr
%
max
2â‰¤Î½â‰¤Îº
âŸ¨Y, Ï†Î½âŸ©

â‰¤t
 s1,u
&
dt
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.9 Some Examples
651
=
 âˆ
0
fâŸ¨Y,Ï†1âŸ©|s1,u(t)

Pr

|âŸ¨Y, Ï†2âŸ©| â‰¤t
 s1,u
Îºâˆ’1
dt
=
 âˆ
0
1
âˆšÏ€N0
eâˆ’
(tâˆ’âˆš
Es)2
N0
-
1 âˆ’2Q
	
t

N0/2

.Îºâˆ’1
dt
=
1
âˆš
2Ï€
 âˆ
âˆ’

2Es
N0
eâˆ’Ï„ 2/2
-
1 âˆ’2Q
	
Ï„ +
4
2Es
N0

.Îºâˆ’1
dÏ„,
(26.102)
with the following justiï¬cation. The ï¬rst equality follows from the deï¬nition of
our optimal decoder and from the fact that ties occur with probability zero. The
second equality follows by trivial algebra (âˆ’Î¾ â‰¤Î¾ if, and only if, Î¾ â‰¥0). The third
equality follows by conditioning on âŸ¨Y, s1,uâŸ©being equal to t and integrating t
out while noting that a correct decision can only be made if t â‰¥0, in which case
the condition âŸ¨Y, Ï†1âŸ©â‰¥0 is satisï¬ed automatically. The fourth equality follows
because, conditional on the signal s1,u being sent, the random variable âŸ¨Y, s1,uâŸ©is
independent of the random variables {|âŸ¨Y, Ï†Î½âŸ©|}2â‰¤Î½â‰¤Îº. The ï¬fth equality follows
because, conditional on s1,u being sent, the random variables {|âŸ¨Y, Ï†Î½âŸ©|}2â‰¤Î½â‰¤Îº are
IID. The sixth equality follows because, conditional on s1,u being sent, we have
âŸ¨Y, Ï†1âŸ©âˆ¼N
âˆšEs, N0/2

and âŸ¨Y, Ï†2âŸ©âˆ¼N(0, N0/2), so
Pr
% âŸ¨Y, Ï†2âŸ©
 â‰¤t
 s1,u
&
= Pr
 |âŸ¨Y, Ï†2âŸ©|

N0/2
â‰¤
t

N0/2
 s1,u

= 1 âˆ’Pr
 |âŸ¨Y, Ï†2âŸ©|

N0/2
â‰¥
t

N0/2
 s1,u

= 1 âˆ’Pr
 âŸ¨Y, Ï†2âŸ©

N0/2
â‰¥
t

N0/2
 s1,u

âˆ’Pr
 âŸ¨Y, Ï†2âŸ©

N0/2
â‰¤
âˆ’t

N0/2
 s1,u

= 1 âˆ’2Q
	
t

N0/2

.
Finally, (26.102) follows from the substitution Ï„ â‰œ(tâˆ’âˆšEs)/

N0/2 as in (26.83).
Since the conditional probability of error does not depend on the message, it follows
that all conditional probabilities of error are equal to the average probability of
error pâˆ—(error) and
pâˆ—(error) = 1 âˆ’
1
âˆš
2Ï€
 âˆ
âˆ’

2Es
N0
eâˆ’Ï„ 2/2
-
1 âˆ’2Q
	
Ï„ +
4
2Es
N0

.Îºâˆ’1
dÏ„,
(26.103)
or, using the Binomial Expansion (26.85) with the substitution of âˆ’Q

Ï„ +
3
2Es
N0

for b, of 1 for a, and of Îº âˆ’1 for n
pâˆ—(error) =
Îºâˆ’1

j=1
(âˆ’1)j+12j
	Îº âˆ’1
j

1
âˆš
2Ï€
 âˆ
âˆ’

2Es
N0
eâˆ’Ï„ 2/2
-
Q
	
Ï„ +
4
2Es
N0

.j
dÏ„.
(26.104)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

652
Detection in White Gaussian Noise
The probability of error associated with an orthogonal constellation with Îº signals
is better than that of the bi-orthogonal constellation with 2Îº signals and equal
average energy. But the comparison is not quite fair because the bi-orthogonal
constellation is richer.
26.10
Detection in Colored Gaussian Noise
26.10.1
Overview
We next consider the problem of detecting known signals in noise that is Gaussian
but possibly â€œcolored,â€ i.e., not white. Our setup is thus as in Section 26.2 except
that the noise N, while still stationary and Gaussian, is no longer assumed to be
white. Instead, we assume a general PSD SNN.
The solution is based on a â€œwhitening ï¬lterâ€ for the PSD SNN with respect to the
bandwidth W. This is a stable ï¬lter that when fed Gaussian noise of PSD SNN
produces WGN of double-sided PSD 1 with respect to the bandwidth W (Def-
inition 26.10.1 and Proposition 26.10.4 ahead).
It turns out that when such a
ï¬lter existsâ€”and it usually does (Proposition 26.10.12)â€”an optimal solution to
our problem is to feed the observed SP Y to a whitening ï¬lter and to then base
our guess on the ï¬lterâ€™s output.
The proof that there is no loss of optimality in basing our guess on the ï¬lterâ€™s
output is nontrivial (Theorem 26.10.9). But how to guess based on the ï¬lterâ€™s
output is straightforward. Indeed, as we next argue, this guessing problem is of
the kind we already studied, i.e., that of detecting known signals (the response of
the ï¬lter to the mean signals) in WGN. To see this, let h be the impulse response of
a whitening ï¬lter. Conditional on M = m, we can then express the ï¬lterâ€™s output
Y â‹†h as
Y â‹†h =

sm + N) â‹†h
= sm â‹†h + N â‹†h.
(26.105)
Here, because the ï¬lter is whitening, Nâ‹†h is WGN of double-sided PSD 1 with re-
spect to the bandwidth W (Proposition 26.10.4 ahead). Thus, the guessing problem
based on the ï¬lterâ€™s output is that of guessing which of the signals s1â‹†h, . . . , sMâ‹†h
is being observed in WGN of double-sided PSD 1 with respect to the band-
width W. This is the setting addressed in Theorem 26.3.1 (with the substitution
of s1 â‹†h, . . . , sM â‹†h for the mean signals and of 1 for N0/2). From this theorem
it thus follows that for guessing M based on the whitening ï¬lterâ€™s output Y â‹†h,
there is no loss of optimality in basing the decision on the inner-products vector

Y â‹†h, Ï†1

, . . . ,

Y â‹†h, Ï†d
T
,
(26.106)
whenever (Ï†1, . . . , Ï†d) is any orthonormal basis for
span(s1 â‹†h, . . . , sM â‹†h).
(26.107)
The solution is thus conceptually very simple. Most of the work will go into showing
that this procedure is optimal, i.e., that there is indeed no loss of optimality in
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.10 Detection in Colored Gaussian Noise
653
guessing M based on the whitening ï¬lterâ€™s output. This is prima facie not obvious
because the whitening ï¬lter need not be invertible. (But see Lemma 26.10.8 ahead.)
26.10.2
Whitening Filters
Deï¬nition 26.10.1 (Whitening Filter for SNN with respect to W). A ï¬lter of
impulse response h: R â†’R is said to be a whitening ï¬lter for SNN (or for N)
with respect to the bandwidth W if it is stable and its frequency response Ë†h
satisï¬es
SNN(f)
Ë†h(f)
2 = 1,
|f| â‰¤W.
(26.108)
Only the magnitude of the frequency response of the whitening ï¬lter is speciï¬ed in
(26.108) and only for frequencies in the band [âˆ’W, W ]. The response is unspeciï¬ed
outside this band. Consequently:
Note 26.10.2. There may be many diï¬€erent whitening ï¬lters for SNN with respect
to the bandwidth W.
If SNN is zero at some frequencies in [âˆ’W, W ], then there is no whitening ï¬lter
for SNN with respect to W. Likewise, a whitening ï¬lter for SNN does not exist
if SNN is discontinuous in [âˆ’W, W ] (because the frequency response of a stable
ï¬lter must be continuous (Theorem 6.2.11), and if SNN is discontinuous, then so is
f 	â†’1/

|SNN(f)| ). Thus:
Note 26.10.3. A whitening ï¬lter for SNN with respect to W need not exist.
We shall see, however, that a whitening ï¬lter exists whenever throughout the inter-
val [âˆ’W, W ] the PSD SNN is strictly positive and is twice continuously diï¬€erentiable
(Proposition 26.10.12).
The ï¬lter is called â€œwhiteningâ€ because, by (26.108) and Theorem 25.13.2, we have:
Proposition 26.10.4. If

N(t), t âˆˆR

is a measurable, stationary, Gaussian SP
of PSD SNN, and if
h is the impulse response of a whitening ï¬lter for SNN with
respect to W, then

N(t)

â‹†h is WGN of PSD 1 with respect to the bandwidth W.
From this proposition and (26.105) we obtain:
Corollary 26.10.5 (The Output of the Whitening Filter). If N and h are as above,
and Y â‹†h is the whitening ï¬lterâ€™s response to the observed SP Y, then, conditional
on M = m,
Y â‹†h = sm â‹†h + ËœN,
(26.109)
where ËœN is WGN of double-sided PSD 1 with respect to the bandwidth W.
26.10.3
Optimality
Rather than proving the optimality of basing our guess on the whitening ï¬lterâ€™s
output and then invoking Theorem 26.3.1 to prove the optimality of basing our
guess on the vector (26.106), we shall adopt a more direct approach and prove the
optimality of basing our guess on this vector directly. But ï¬rst we shall rewrite
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

654
Detection in White Gaussian Noise
this vector in a form that accentuates that its components are linear functionals
of Y: we will show that, with probability one, the tuples

Y â‹†h, Ï†1

, . . . ,

Y â‹†h, Ï†d

and

Y, ~h â‹†Ï†1

, . . . ,

Y, ~h â‹†Ï†d

are equal. It will then follow that to prove optimality it suï¬ƒces to prove that there
is no loss of optimality in basing our guess on

Y, ~h â‹†Ï†1

, . . . ,

Y, ~h â‹†Ï†d

.
(26.110)
More generally, we will show that if q1, . . . , qn are integrable signals (not necessarily
orthonormal), then, with probability one,

Y â‹†h, q1

, . . . ,

Y â‹†h, qn

=

Y, ~h â‹†q1

, . . . ,

Y, ~h â‹†qn

.
(26.111)
We begin with a deterministic setting.
Lemma 26.10.6 (The Adjoint of the Convolution: Deterministic Signals). Let h
be an integrable signal, and let ~h: t 	â†’h(âˆ’t) be its mirror image. Then

u â‹†h, v

=

u, ~hâˆ—â‹†v

,
u, v âˆˆL2,
(26.112)
where h, u, and v may be complex.
Proof. The proof is based on changing the order of integration:

u â‹†h, v

=
 âˆ
t=âˆ’âˆ
	 âˆ
Ï„=âˆ’âˆ
h(t âˆ’Ï„) u(Ï„) dÏ„

vâˆ—(t) dt
=
 âˆ
Ï„=âˆ’âˆ
u(Ï„)
	 âˆ
t=âˆ’âˆ
h(t âˆ’Ï„) vâˆ—(t) dt

dÏ„
=
 âˆ
Ï„=âˆ’âˆ
u(Ï„)
	 âˆ
t=âˆ’âˆ
hâˆ—(t âˆ’Ï„) v(t) dt

âˆ—
dÏ„
=
 âˆ
Ï„=âˆ’âˆ
u(Ï„)
~hâˆ—â‹†v
âˆ—(Ï„) dÏ„
=

u, ~hâˆ—â‹†v

.
The change can be justiï¬ed using Fubiniâ€™s Theorem: we integrate |h(tâˆ’Ï„) u(Ï„) vâˆ—(t)|
ï¬rst over Ï„ to obtain (|h| â‹†|u|)(t) |v(t)|; we use Inequality (5.11) to conclude that
|h|â‹†|u| is energy-limited; and we then use this and the hypothesis that v is energy-
limited to argue that the integral of t 	â†’(|h|â‹†|u|)(t) |v(t)| is ï¬nite (Cauchy-Schwarz
Inequality).
The result above extends to stochastic processes. Since we only deal here with real
stochastic processes and real whitening ï¬lters, we consider the real case only. The
next lemma allows us to replace u in Lemma 26.10.6 with a SP X. We only need
the lemma for WSS stochastic processes, but we shall state it a bit more generally.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.10 Detection in Colored Gaussian Noise
655
Lemma 26.10.7 (The Adjoint of the Convolution: Stochastic Processes). Let X
be a measurable SP for which the function t 	â†’E[|X(t)|] is bounded. Let h and v
be real, deterministic, integrable signals. Then
âŸ¨X â‹†h, vâŸ©= âŸ¨X, ~h â‹†vâŸ©
with probability one.
(26.113)
Proof. The inner product âŸ¨X â‹†h, vâŸ©can also be expressed as

(X â‹†h) â‹†~v

(0).
Likewise, âŸ¨X, ~h â‹†vâŸ©can be expressed as the convolution, evaluated at zero, of X
with the mirror image of (~hâ‹†v), i.e., as

Xâ‹†(hâ‹†~v)

(0). The result thus follows from
the associativity of the convolution for stochastic processes (Lemma 15.7.1).
Using Lemma 26.10.7 and the fact that the union of a ï¬nite number of events
having probability zero also has probability zero (Corollary 21.5.2 (i)), we conclude
that (26.111) holds with probability one. Hence, upon substituting Ï†â€™s for qâ€™s,

Y â‹†h, Ï†1

, . . . ,

Y â‹†h, Ï†d

=

Y, ~h â‹†Ï†1

, . . . ,

Y, ~h â‹†Ï†d

(26.114)
with probability one. It thus suï¬ƒces to prove the optimality of basing our guess
on

Y, ~h â‹†Ï†1

, . . . ,

Y, ~h â‹†Ï†d

.
(26.115)
To prove this, we begin by exploring conditions that guarantee that the input to the
whitening ï¬lter be recoverable from its output. Such conditions are needed because
the ï¬lterâ€™s frequency response could be zero outside the band [âˆ’W, W ], in which
case all input signals whose FT vanishes in this band produce the same all-zero
output, and their recovery from the ï¬lterâ€™s output is thus impossible. However, if
the input is known to be bandlimited to W Hz, then recovery is possible:
Lemma 26.10.8 (Recovering the Input to a Whitening Filter from its Output).
Let h be the impulse response of a whitening ï¬lter for SNN with respect to the
bandwidth W. Let q be the ï¬lterâ€™s response to the input s. If s is an integrable
signal that is bandlimited to W Hz, then it can be recovered from q via the relation
s(t) =
 W
âˆ’W
Ë†q(f)
Ë†h(f)
ei2Ï€ft df,
t âˆˆR.
(26.116)
Proof. Since q equals s â‹†h, where s is an integrable signal that is bandlimited
to W Hz and h is integrable, it follows that q is an integrable signal that is band-
limited to W Hz (Proposition 6.5.2) whose FT Ë†q(f) is Ë†s(f) Ë†h(f) for all f âˆˆR
(Theorem 6.3.2). Since Ë†h(f) is nonzero for all |f| â‰¤W (because h is a whitening
ï¬lter for SNN with respect to the bandwidth W) this impliesâ€”upon dividing by
Ë†h(f) and restricting f to |f| â‰¤Wâ€”that
Ë†s(f) = Ë†q(f)
Ë†h(f)
,
|f| â‰¤W.
(26.117)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

656
Detection in White Gaussian Noise
Expressing s as the IFT of its FT (Proposition 6.4.10), we obtain
s(t) =
 W
âˆ’W
Ë†s(f) ei2Ï€ft df
=
 W
âˆ’W
Ë†q(f)
Ë†h(f)
ei2Ï€ft df,
t âˆˆR.
We now come to the main result of this section.
Theorem 26.10.9 (Detecting Known Signals in Colored Gaussian Noise). Let M
take values in the ï¬nite set M = {1, . . . , M}, and let the mean signals s1, . . . , sM be
integrable signals that are bandlimited to W Hz. Let the conditional law of

Y (t)

given M = m be that of sm(t) + N(t), where

N(t)

is a stationary, measurable,
Gaussian SP of PSD SNN that can be whitened with respect to the bandwidth W.
Let h be the impulse response of a whitening ï¬lter for

N(t)

.
(i) If Ï†1, . . . , Ï†d is an orthonormal basis for span(s1 â‹†h, . . . , sM â‹†h), then no
measurable decision rule for guessing M based on Y can outperform the best
rule that is based on the d-vector

Y, ~h â‹†Ï†1

, . . . ,

Y, ~h â‹†Ï†d
T
.
(26.118)
Conditional on M = m, this vector is a Gaussian vector whose mean is

âŸ¨sm â‹†h, Ï†1âŸ©, . . . , âŸ¨sm â‹†h, Ï†dâŸ©
T
(26.119)
and whose covariance matrix is the d Ã— d identity matrix Id.
(ii) If Ëœq1, . . . , Ëœqn are integrable signals for which
span(s1 â‹†h, . . . , sM â‹†h) âŠ†span(Ëœq1, . . . , Ëœqn),
(26.120)
then no measurable decision rule for guessing M based on Y can outperform
the best rule based on

Y, ~h â‹†Ëœq1

, . . . ,

Y, ~h â‹†Ëœqn
T
.
(26.121)
(iii) If each of the mean signals can be written as a linear combination of the
integrable signals Ëœs1, . . . ,Ëœsn, then it is optimal to guess M based on the vector

Y, ~h â‹†Ëœs1 â‹†h

, . . . ,

Y, ~h â‹†Ëœsn â‹†h
T
.
(26.122)
(iv) No measurable decision rule for guessing M based on Y can outperform the
best decision rule based on the vector

Y, ~h â‹†q1

, . . . ,

Y, ~h â‹†qM
T
,
(26.123a)
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.10 Detection in Colored Gaussian Noise
657
where
qm = sm â‹†h,
m âˆˆM.
(26.123b)
Conditional on M = m, this vector is Gaussian with mean

âŸ¨qm, q1âŸ©, . . . , âŸ¨qm, qMâŸ©
T
(26.124)
and M Ã— M covariance matrix
â›
âœ
âœ
âœ
â
âŸ¨q1, q1âŸ©
âŸ¨q1, q2âŸ©
Â· Â· Â·
âŸ¨q1, qMâŸ©
âŸ¨q2, q1âŸ©
âŸ¨q2, q2âŸ©
Â· Â· Â·
âŸ¨q2, qMâŸ©
...
...
...
...
âŸ¨qM, q1âŸ©
âŸ¨qM, q2âŸ©
Â· Â· Â·
âŸ¨qM, qMâŸ©
â
âŸ
âŸ
âŸ
â ,
(26.125)
where the inner product âŸ¨qmâ€², qmâ€²â€²âŸ©can be expressed as
âŸ¨qmâ€², qmâ€²â€²âŸ©=
 W
âˆ’W
Ë†smâ€²(f) Ë†sâˆ—
mâ€²â€²(f)
1
SNN(f) df,
mâ€², mâ€²â€² âˆˆM.
(26.126)
Proof. We begin with Part (i). Let qm be the response of the whitening ï¬lter to
the mean signal sm:
qm = sm â‹†h,
m âˆˆM.
(26.127)
Since, by assumption, the mean signals are integrable signals that are bandlimited
to W Hz, it follows that so are the signals q1, . . . , qM (Proposition 6.5.2) and hence
also all the signals in span(q1, . . . , qM) including Ï†1, . . . , Ï†d.
Since h is a whitening ï¬lter for SNN with respect to the bandwidth W, it is stable
h âˆˆL1,
(26.128a)
and
1
|Ë†h(f)|2 = SNN(f),
|f| â‰¤W.
(26.128b)
The latter implies that
 W
âˆ’W
1
Ë†h(f)
2 df < âˆ
(26.128c)
because SNN, being a PSD, is integrable.
Since (Ï†1, . . . , Ï†d) is an orthonormal basis for span(q1, . . . , qM),
qm =
d

â„“=1
âŸ¨qm, Ï†â„“âŸ©Ï†â„“,
m âˆˆM.
(26.129)
Deï¬ne the signals g1, . . . , gd for every t âˆˆR as
gâ„“(t) =
 W
âˆ’W
Ë†Ï†â„“(f)
Ë†h(f)
ei2Ï€ft df,
â„“âˆˆ{1, . . . , d}.
(26.130)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

658
Detection in White Gaussian Noise
The integrability of Ï†â„“implies that Ë†Ï†â„“is bounded (Theorem 6.2.11), which com-
bines with (26.128c) to show that the integrand in (26.130) is square integrable.
Consequently, the signals g1, . . . , gd are energy-limited signals that are bandlimited
to W Hz (Proposition 6.4.5 (ii)). By (26.130) and Proposition 6.5.1
gâ„“â‹†h = Ï†â„“,
â„“âˆˆ{1, . . . , d}.
(26.131)
Let Nâ€² be a measurable SP of the same law as N but independent of it. Thus,

N â€²(t)

is independent of

N(t)

and is a measurable, centered, stationary, Gaus-
sian SP of PSD SNN. Deï¬ne the SP
Yâ€² =
d

â„“=1

Y, ~h â‹†Ï†â„“

gâ„“+ Nâ€² âˆ’
d

â„“=1

Nâ€², ~h â‹†Ï†â„“

gâ„“.
(26.132)
We will show that, conditional on every message m âˆˆM, the FDDs of Y and Yâ€²
are identical. This will show that there is no loss of optimality in basing our guess
of M on the vector (26.118) because the decoder can use its local randomness to
generate
Nâ€² âˆ’
d

â„“=1

Nâ€², ~h â‹†Ï†â„“

gâ„“;
add this SP to
d

â„“=1

Y, ~h â‹†Ï†â„“

gâ„“
(which it can synthesize because the vector (26.118) is at its disposal) to form
Yâ€²; and feed Yâ€² to any guessing rule that was designed to base its decision on Y.
Since Y and Yâ€² have the same conditional FDDs, this would result in the same
performance as if Y and not Yâ€² were fed to the guessing device.
Conditional on M = m, the mean of Y is sm whereas the mean of Yâ€² is
d

â„“=1

sm, ~h â‹†Ï†â„“

gâ„“.
We next show that the two are identical. Indeed, by (26.127) and Lemma 26.10.8
sm(t) =
 W
âˆ’W
Ë†qm(f)
Ë†h(f)
ei2Ï€ft df
=
 W
âˆ’W

â„“âŸ¨qm, Ï†â„“âŸ©Ë†Ï†â„“(f)
Ë†h(f)
ei2Ï€ft df
=
d

â„“=1
âŸ¨qm, Ï†â„“âŸ©
 W
âˆ’W
Ë†Ï†â„“(f)
Ë†h(f)
ei2Ï€ft df
=
d

â„“=1
âŸ¨qm, Ï†â„“âŸ©gâ„“(t)
=
d

â„“=1

sm, ~h â‹†Ï†â„“

gâ„“(t),
t âˆˆR,
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.10 Detection in Colored Gaussian Noise
659
where the second equality follows by taking the FT of both sides of (26.129);
the third by swapping summation and integration; the fourth by the deï¬nition of
gâ„“(26.130); and the ï¬fth by (26.127) and Lemma 26.10.6.
Having established that Y and Yâ€² have identical conditional means, we next show
that, conditional on M = m, they also have identical autocovariance functions.
Since both Y and Yâ€² are Gaussian (conditional on M = m), this will establish
that Y and Yâ€² have identical FDDs given M = m. Subtracting the means, we
need to show that the SP
d

â„“=1

N, ~h â‹†Ï†â„“

gâ„“+ Nâ€² âˆ’
d

â„“=1

Nâ€², ~h â‹†Ï†â„“

gâ„“
has the same autocovariance function as N, namely, KNN. Starting from the deï¬-
nition of Yâ€² (26.132),
Cov

Y â€²(t + Ï„), Y â€²(t)
 M = m

= E
	
d

â„“=1

N, ~h â‹†Ï†â„“

gâ„“(t + Ï„) + N â€²(t + Ï„) âˆ’
d

â„“=1

Nâ€², ~h â‹†Ï†â„“

gâ„“(t + Ï„)

	
d

â„“â€²=1

N, ~h â‹†Ï†â„“â€²
gâ„“â€²(t) + N â€²(t) âˆ’
d

â„“â€²=1

Nâ€², ~h â‹†Ï†â„“â€²
gâ„“â€²(t)


=
d

â„“=1
d

â„“â€²=1
E

N, ~h â‹†Ï†â„“

N, ~h â‹†Ï†â„“â€²
gâ„“(t + Ï„) gâ„“â€²(t) + E

N â€²(t + Ï„)N â€²(t)

âˆ’
d

â„“â€²=1
E

N â€²(t + Ï„)

Nâ€², ~h â‹†Ï†â„“â€²
gâ„“â€²(t) âˆ’
d

â„“=1
E

N â€²(t)

Nâ€², ~h â‹†Ï†â„“

gâ„“(t + Ï„)
+
d

â„“=1
d

â„“â€²=1
E

Nâ€², ~h â‹†Ï†â„“

Nâ€², ~h â‹†Ï†â„“â€²
gâ„“(t + Ï„) gâ„“â€²(t)
= 2
d

â„“=1
d

â„“â€²=1
E

N, ~h â‹†Ï†â„“

N, ~h â‹†Ï†â„“â€²
gâ„“(t + Ï„) gâ„“â€²(t) + KNN(Ï„)
âˆ’
d

â„“=1
E

N(t + Ï„)

N, ~h â‹†Ï†â„“

gâ„“(t) âˆ’
d

â„“=1
E

N(t)

N, ~h â‹†Ï†â„“

gâ„“(t + Ï„),
(26.133)
where the second equality follows because N and Nâ€² are independent and con-
sequently so are linear functionals of them (Proposition 25.15.8), and the third
because N and Nâ€² have identical laws.
We next study the diï¬€erent terms on the RHS of (26.133). We begin with the ï¬rst.
From Theorem 25.12.2 (ii) and the fact that h is real,
E
%
N, ~h â‹†Ï†â„“

N, ~h â‹†Ï†â„“â€²&
=
 âˆ
âˆ’âˆ
SNN(f) Ë†hâˆ—(f) Ë†Ï†â„“(f) Ë†h(f) Ë†Ï†âˆ—
â„“â€²(f) df
=
 W
âˆ’W
SNN(f)
Ë†h(f)
2 Ë†Ï†â„“(f) Ë†Ï†âˆ—
â„“â€²(f) df
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

660
Detection in White Gaussian Noise
=
 W
âˆ’W
Ë†Ï†â„“(f) Ë†Ï†âˆ—
â„“â€²(f) df
(26.134)
=

Ï†â„“, Ï†â„“â€²
= I{â„“= â„“â€²},
â„“, â„“â€² âˆˆ{1, . . . , d},
(26.135)
where the second equality holds because Ï†1, . . . , Ï†d are integrable signals that are
bandlimited to W Hz; the third by (26.128b); the fourth by Parsevalâ€™s Theorem;
and the ï¬fth by the orthonormality of (Ï†1, . . . , Ï†d). Consequently, for the ï¬rst
term on the RHS of (26.133)
2
d

â„“=1
d

â„“â€²=1
E

N, ~h â‹†Ï†â„“

N, ~h â‹†Ï†â„“â€²
gâ„“(t + Ï„) gâ„“â€²(t) = 2
d

â„“=1
gâ„“(t) gâ„“(t + Ï„).
(26.136a)
As to the third term, we invoke Theorem 25.12.2 (ii) again to obtain
E
%
N(t + Ï„)

N, ~h â‹†Ï†â„“
&
=
 âˆ
âˆ’âˆ
SNN(f) Ë†hâˆ—(f) Ë†Ï†â„“(f) ei2Ï€f(t+Ï„) df
=
 W
âˆ’W
SNN(f) Ë†hâˆ—(f) Ë†Ï†â„“(f) ei2Ï€f(t+Ï„) df
=
 W
âˆ’W
Ë†Ï†â„“(f)
Ë†h(f)
ei2Ï€f(t+Ï„) df
= gâ„“(t + Ï„),
where the second equality follows because Ï†1, . . . , Ï†d are integrable signals that
are bandlimited to W Hz; the third follows from (26.128b); and the last equality
from (26.130). Consequently, for the third term on the RHS of (26.133)
d

â„“=1
E

N(t + Ï„)

N, ~h â‹†Ï†â„“

gâ„“(t) =
d

â„“=1
gâ„“(t + Ï„) gâ„“(t).
(26.136b)
For the fourth term we invoke similar arguments to obtain
E

N(t)

N, ~h â‹†Ï†â„“

= gâ„“(t),
and to thus conclude that
d

â„“=1
E

N(t)

N, ~h â‹†Ï†â„“

gâ„“(t + Ï„) =
d

â„“=1
gâ„“(t) gâ„“(t + Ï„).
(26.136c)
It now follows from (26.136) that the RHS of (26.133) equals KNN(Ï„), thus demon-
strating that conditional on M the autocovariance function of Yâ€² is KNN, and
hence concluding the proof that the conditional laws of Y and Yâ€² given M = m
are identical.
To conclude the proof of Part (i) it remains to compute the conditional law of
the vector (26.118) given M = m. Since (26.114) holds with probability one, this
conditional law is equal to the conditional law of

Y â‹†h, Ï†1

, . . . ,

Y â‹†h, Ï†d
T
.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.10 Detection in Colored Gaussian Noise
661
The conditional law of the latter can be easily computed, because the conditional
law of Y â‹†h given M = m is that of sm â‹†h + ËœN where ËœN is WGN of double-sided
PSD 1 with respect to W (Corollary 26.10.5), so the result follows directly from
Proposition 25.15.2 on linear functionals of WGN (with the substitution of 1 for
the double-sided PSD N0/2).
Part (ii) is a consequence of the linearity of the stochastic integral (Lemma 25.10.3).
It can be proved as follows. Let (Ï†1, . . . , Ï†d) be, as before, an orthonormal ba-
sis for span(s1 â‹†h, . . . , sM â‹†h). Since, by assumption, each signal in the latter
subspace can be written as a linear combination of the signals Ëœq1, . . . , Ëœqn so can
the signals Ï†1, . . . , Ï†d. Consequently, by the linearity of stochastic integration, the
vector (26.118) can be computed from (26.121). And since it is optimal to base our
guess of M on the former, it must also be optimal to base our guess on the latter.
Part (iii) follows from Part (ii) by deï¬ning Ëœqj as Ëœsj â‹†h for every j âˆˆ{1, . . . , n}.
As to Part (iv), the optimality of basing our guess on the vector (26.123) follows
from Part (iii), and its conditional law can be computed as follows: by Theo-
rem 25.12.1 it is conditionally Gaussian, and its conditional mean (26.124) and
conditional covariance (26.125) are readily derived using Theorem 25.12.2. To de-
rive (26.126), we note that by (26.123b) and Proposition 6.5.2 we have for every
m âˆˆM
qm(t) =
 W
âˆ’W
Ë†sm(f) Ë†h(f) ei2Ï€ft df,
t âˆˆR.
(26.137)
This, the Mini Parseval Theorem (Proposition 6.2.6 (i)), and (26.108) combine to
establish (26.126).
A diï¬€erent way to compute the conditional distribution is to use (26.111): we
ï¬rst substitute M for n and q1, . . . , qM for q1, . . . , qn in (26.111) to infer that the
conditional law of the vector (26.123) is the same as the conditional law of

Y â‹†h, q1

, . . . ,

Y â‹†h, qM
T
.
(26.138)
We then compute the conditional law of the latter by noting that, conditional on
M = m, the law of Y â‹†h is that of sm â‹†h + ËœN, where ËœN is WGN of double-
sided PSD 1 with respect to W (Corollary 26.10.5), and by then invoking Propo-
sition 25.15.2 on linear functionals of WGN (with the substitution of 1 for the
double-sided PSD N0/2).
Since the conditional law of the random vector (26.123) is fully determined by the
inner products âŸ¨qmâ€², qmâ€²â€²âŸ©for mâ€², mâ€²â€² âˆˆM (see (26.124) and (26.125)), and since,
by (26.126), the inner product âŸ¨qmâ€², qmâ€²â€²âŸ©does not depend on the choice of the
whitening ï¬lter we obtain:
Note 26.10.10 (The Choice of the Whitening Filter Is Immaterial). Neither the
conditional distribution of the random vector in (26.123) nor the optimal proba-
bility of error depends on the choice of the whitening ï¬lter.
Using Theorem 26.10.9 we can now derive an optimal rule for guessing M. Indeed,
in analogy to Theorem 26.5.1 we have:
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

662
Detection in White Gaussian Noise
Theorem 26.10.11. Consider the setting of Theorem 26.10.9 with M of prior {Ï€m}.
The decision rule that guesses uniformly at random from among all the messages
Ëœm âˆˆM for which
ln Ï€ Ëœm âˆ’1
2
d

â„“=1

Y, ~h â‹†Ï†â„“

âˆ’

s Ëœm, ~h â‹†Ï†â„“
2
= max
mâ€²âˆˆM
1
ln Ï€mâ€² âˆ’1
2
d

â„“=1

Y, ~h â‹†Ï†â„“

âˆ’

smâ€², ~h â‹†Ï†â„“
22
(26.139)
minimizes the probability of error whenever h is a whitening ï¬lter and the tuple
(Ï†1, . . . , Ï†d) forms an orthonormal basis for span(s1 â‹†h, . . . , sM â‹†h).
26.10.4
Existence of A Whitening Filter
Before concluding our discussion of detection in the presence of colored noise we
derive here a suï¬ƒcient condition for the existence of a whitening ï¬lter.
Proposition 26.10.12 (Existence of a Whitening Filter). Let W > 0 be ï¬xed. If
throughout the interval [âˆ’W, W ] the PSD SNN is strictly positive and twice con-
tinuously diï¬€erentiable, then there exists a whitening ï¬lter for SNN with respect to
the bandwidth W.
Proof. The proof hinges on the following basic result from harmonic analysis
(Katznelson, 2004, Chapter VI, Section 1, Exercise 7): if a function f 	â†’g(f)
is twice continuously diï¬€erentiable and is zero outside some interval [âˆ’Î”, Î”], then
it is the FT of some integrable function.
To prove the proposition using this result we begin by picking some Î” > W. We
now deï¬ne a function g: R â†’R as follows.
For f â‰¥Î”, we deï¬ne g(f) = 0.
For f in the interval [0, W], we deï¬ne g(f) = 1/

SNN(f). And for f âˆˆ(W, Î”),
we deï¬ne g(f) so that g be twice continuously diï¬€erentiable in [0, âˆ). We can
thus think of g in [W, Î”] as an interpolation function whose values and ï¬rst two
derivatives are speciï¬ed at the endpoints of the interval. Finally, for f < 0, we
deï¬ne g(f) as g(âˆ’f). Figure 26.11 depicts SNN, g, W, and Î”.
A whitening ï¬lter for SNN with respect to the bandwidth W is the integrable
function whose FT is g and whose existence is guaranteed by the quoted result.
26.11
Multiple Antennas
In some applications the receiver can guess the message based on more than one
received waveform. For example, the receiver may employ a number of antennas,
each of which receives the desired signal contaminated by diï¬€erent noise. More
generally, we consider nR received waveforms Y1, . . . , YnR and assume that con-
ditional on M = m, they are given by
YÎ· = sm,Î· + NÎ·,
Î· = 1, . . . , nR,
(26.140)
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.11 Multiple Antennas
663
W
Î”
âˆ’W
âˆ’Î”
f
g(f)
1

SNN(f)
SNN(f)
Figure 26.11: The frequency response of a whitening ï¬lter for the PSD SNN with
respect to the bandwidth W.
where the nR stochastic processes N1, . . . , NnR are independent stochastic pro-
cesses, each of which is WGN of spectral density N0/2 with respect to the band-
width W, and where sm,Î·â€”which is assumed to be an integrable signal that is
bandlimited to W Hzâ€”is the mean-signal in the Î·-th antenna corresponding to the
m-th message.
Theorem 26.3.1 extends to this setup as follows:
Theorem 26.11.1 (Detection with Multiple Antennas). Consider the setup above
and assume that for each antenna Î· âˆˆ{1, . . . , nR}, the dÎ· signals Ï†1,Î·, . . . , Ï†dÎ·,Î·
form an orthonormal basis for
span(s1,Î·, . . . , sM,Î·).
Then, in the same sense as in Theorem 26.3.1, there is no loss of optimality in
guessing M based on the d1 + Â· Â· Â· + dnR inner products
'
YÎ·, Ï†â„“,Î·
(
,
â„“âˆˆ{1, . . . , dÎ·}, Î· âˆˆ{1, . . . , nR}.
Conditional on M = m, these inner products are independent Gaussians with
âŸ¨YÎ·, Ï†â„“,Î·âŸ©being
N
	
sm,Î·, Ï†â„“,Î·

, N0
2

.
(26.141)
Proof. We only sketch the proof because it is just a small variation on that of
Theorem 26.3.1. Given any (measurable) decoder that is based on Y1, . . . , YÎ· we
propose a randomized decision rule of identical performance that is based on the
inner products only. The proposed decoder uses its local randomness to produce
nR independent WGN processes Nâ€²
1, . . . , Nâ€²
nR, where for each Î· âˆˆ{1, . . . , nR} the
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

664
Detection in White Gaussian Noise
law of Nâ€²
Î· is identical to the law of NÎ·. Based on the inner products and on these
locally-generated WGN processes, it then produces the nR waveforms
Yâ€²
Î· =
dÎ·

â„“=1
âŸ¨YÎ·, Ï†â„“,Î·âŸ©Ï†â„“,Î· + Nâ€²
Î· âˆ’
dÎ·

â„“=1

Nâ€²
Î·, Ï†â„“,Î·

Ï†â„“,Î·,
Î· âˆˆ{1, . . . , nR}, (26.142)
which it then feeds to the given rule.
26.12
Detecting Signals of Inï¬nite Bandwidth
So far we have only dealt with the detection problem when the mean signals are
bandlimited. What if they are not? The diï¬ƒculty in this case is that we can no
longer assume that the noise PSD is constant over the bandwidth occupied by the
mean signals, or that the noise can be whitened with respect to this bandwidth.
We can address this issue in three diï¬€erent ways. In the ï¬rst we can try to ï¬nd the
optimal detector by studying this more complicated hypothesis testing problem. It
will no longer be optimal to base our guess on the inner-products vector (26.12).
The optimal detector will greatly depend on the relationship between the rate of
decay of the PSD of the noise as the frequency tends to Â±âˆand the rate of decay of
the FT of the mean signals. This approach will often lead to bad designs, because
the structure of the receiver will depend greatly on how we model the noise, and
inaccuracies in our modeling of the noise PSD at ultra-high frequencies might lead
us completely astray in our design.
A more level-headed approach that is valid if the noise PSD is â€œessentially ï¬‚at
over the bandwidth of interestâ€ is to ignore the fact that the mean signals are not
bandlimited and to base our decision on the inner-products vector, even if this is
not fully justiï¬ed mathematically. This approach leads to robust designs that are
insensitive to inaccuracies in our modeling of the noise process. If the PSD is not
essentially ï¬‚at, we can whiten it with respect to a suï¬ƒciently large band [âˆ’W, W ]
that contains most of the energy of the mean signals.
The third approach is to use very complicated mathematical machinery involving
the ItË†o Calculus (Karatzas and Shreve, 1991) to model the noise in a way that will
result in the optimality of basing our decision on the inner-products vector. We
have chosen not to pursue this approach because it requires modeling the noise as
a process of inï¬nite power, which is physically unappealing. This approach just
swaps one diï¬ƒculty for another: the ItË†o Calculus can now prove for us that it is
optimal to base our decision on the inner-products vector, but we need a leap of
faith in modeling the noise as a process of inï¬nite power.
In the future, in dealing with mean signals that are not bandlimited, we shall refer
to the â€œwhite noise paradigmâ€ as the paradigm under which the receiver forms
its decision based on the inner-products vector (26.12) and under which these inner
products have the conditional law derived in Section 26.4.1.
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.13 Exercises
665
26.13
Exercises
Exercise 26.1 (Monotonicity in the Noise). In the setup of Section 26.2 show that the
minimal probability of error that can be achieved in guessing M based on

Y (t)

is
nondecreasing in N0.
Exercise 26.2 (Reducing the Number of Matched Filters). Consider the setup of Sec-
tion 26.2, and let s0 be any integrable signal that is bandlimited to W Hz. Deï¬ne dâ€² as the
dimension of span(s1 âˆ’s0, . . . , sM âˆ’s0). Exhibit a random dâ€²-vector that is computable
from Y and on which we can base our guess without any loss of optimality. Show that dâ€²
is sometimes smaller than d (the dimension of the subspace spanned by the mean signals).
Exercise 26.3 (Nearest-Neighbor Decoding Revisited). The form of the decoder in The-
orem 26.5.3 (ii) is diï¬€erent from the nearest-neighbor rule of Proposition 21.6.1 (ii).
Why does minimizing âˆ¥Y âˆ’smâˆ¥2 not make mathematical sense in the setting of The-
orem 26.5.3?
Exercise 26.4 (Minimum Shift Keying). Let the signals s0, s1 be given at every t âˆˆR by
s0(t) =

2Es
Ts cos(2Ï€f0t) I{0 â‰¤t â‰¤Ts},
s1(t) =

2Es
Ts cos(2Ï€f1t) I{0 â‰¤t â‰¤Ts}.
(i) Compute the energies âˆ¥s0âˆ¥2
2, âˆ¥s1âˆ¥2
2. You may assume that f0Ts â‰«1 and f1Ts â‰«1.
(ii) Under what conditions on f0, f1, and Ts are s0 and s1 orthogonal?
(iii) Assume that the parameters are chosen as in Part (ii). Let H take on the values 0
and 1 equiprobably, and assume that, conditional on H = Î½, the time-t received
waveform is sÎ½(t) + N(t) where

N(t)

is WGN of double-sided PSD N0/2 with
respect to the bandwidth of interest, and Î½ âˆˆ{0, 1}.
Find an optimal rule for
guessing H based on the received waveform.
(iv) Compute the optimal probability of error.
Exercise 26.5 (Signaling in White Gaussian Noise). Let the RV M take value in the set
M = {1, 2, 3, 4} uniformly. Conditional on M = m, the observed waveform

Y (t)

is
given at every time t âˆˆR by sm(t) + N(t), where the signals s1, s2, s3, s4 are given by
s1(t) = A I{0 â‰¤t â‰¤T},
s2(t) = A I{0 â‰¤t â‰¤T/2} âˆ’A I{T/2 < t â‰¤T},
s3(t) = 2A I{0 â‰¤t â‰¤T/2},
s4(t) = âˆ’A I{0 â‰¤t â‰¤T/2} + A I{T/2 < t â‰¤T},
and where

N(t)

is WGN of double-sided PSD N0/2 over the bandwidth of interest.
(Ignore the fact that the signals are not bandlimited.)
(i) Derive the MAP rule for guessing M based on

Y (t)

.
(ii) Use the Union-of-Events Bound to upper-bound pMAP(error|M = 3). Are all the
terms in the bound needed?
(iii) Compute pMAP(error|M = 3) exactly.
(iv) Show that by subtracting a waveform sâˆ—from each of the signals s1, s2, s3, s4, we
can reduce the average transmitted energy without degrading performance. What
waveform sâˆ—should be subtracted to minimize the transmitted energy?
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

666
Detection in White Gaussian Noise
Exercise 26.6 (QPSK). Let the IID random bits D1 and D2 be mapped to the symbols
X1, X2 according to the rule
(0, 0) â†’(1, 0),
(0, 1) â†’(âˆ’1, 0),
(1, 0) â†’(0, 1),
(1, 1) â†’(0, âˆ’1).
The received waveform

Y (t)

is given by
Y (t) = AX1 Ï†1(t) + AX2 Ï†2(t) + N(t),
t âˆˆR,
where A > 0, the signals Ï†1, Ï†2 are orthonormal integrable signals that are bandlimited
to W Hz, and the SP

N(t)

is independent of (D1, D2) and is WGN of double-sided PSD
N0/2 with respect to the bandwidth W.
(i) Find an optimal rule for guessing (D1, D2) based on

Y (t)

.
(ii) Find an optimal rule for guessing D1 based on

Y (t)

.
(iii) Compare the rule that you have found in Part (ii) with the rule that guesses that D1
is the ï¬rst component of the tuple produced by the decoder that you have found
in Part (i). Evaluate the probability of error for both rules.
(iv) Repeat when (D1, D2) are mapped to (X1, X2) according to the rule
(0, 0) â†’(1, 0),
(0, 1) â†’(0, 1),
(1, 0) â†’(âˆ’1, 0),
(1, 1) â†’(0, âˆ’1).
Exercise 26.7 (Mismatched Decoding of Antipodal Signaling). Let the received wave-
form

Y (t)

be given at every t âˆˆR by (1âˆ’2H) s(t)+N(t), where s is an integrable signal
that is bandlimited to W Hz,

N(t)

is WGN of double-sided PSD N0/2 with respect to
the bandwidth W, and H takes on the values 0 and 1 equiprobably and independently
of

N(t)

. Let sâ€² be an integrable signal that is bandlimited to W Hz. A suboptimal
detector feeds the received waveform to a matched ï¬lter for sâ€² and guesses according to
the ï¬lterâ€™s time-0 output: if it is positive, it guesses â€œH = 0,â€ and if it is negative, it
guesses â€œH = 1.â€ Express this detectorâ€™s probability of error in terms of s, sâ€², and N0.
Exercise 26.8 (Imperfect Automatic Gain Control). Let the received signal

Y (t)

be
given by
Y (t) = AX s(t) + N(t),
t âˆˆR,
where A > 0 is some deterministic positive constant, X is a RV that takes value in the
set {âˆ’3, âˆ’1, +1, +3} uniformly, s is an integrable signal that is bandlimited to W Hz, and

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W.
(i) Find an optimal rule for guessing X based on

Y (t)

.
(ii) Using the Q-function compute the optimal probability of error.
(iii) Suppose you use the rule you have found in Part (i), but the received signal is
Y (t) = 3
4AX s(t) + N(t),
t âˆˆR.
(You were misinformed about the amplitude of the signal.) What is the probability
of error now?
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.13 Exercises
667
Exercise 26.9 (Early-Late Gate). Let H take on the values 0 and 1 equiprobably, and let
Y (t) = (1 âˆ’2H) s(t) + N(t),
t âˆˆR,
where s: t â†’A I{0 â‰¤t â‰¤T}, where A and T are positive numbers, and where

N(t)

is
WGN of double-sided PSD N0/2 with respect to the bandwidth of interest (white noise
paradigm). The observation

Y (t)

is fed to a matched ï¬lter for s, and the output of the
ï¬lter is sampled at times âˆ’T/2 and T/2. If the sum of the two samples is positive, then
the receiver guesses â€œH = 0â€; otherwise it guesses â€œH = 1â€.
(i) Express the probability of error for the above guessing rule using the Q-function in
terms of A, T, and N0. (Ignore the fact that the signal s is not bandlimited.)
(ii) Suppose that the synchronization between the transmitter and the receiver is not
perfect, and instead of sampling at times âˆ’T/2 and T/2, the output of the matched
ï¬lter is sampled at times âˆ’T/2 + Ï„ and T/2 + Ï„, where âˆ’T/2 â‰¤Ï„ â‰¤T/2. How does
this inï¬‚uence the probability of error?
(iii) Compute the optimal probability of error for guessing H based on

Y (t)

and
compare it to the result of Part (i).
Exercise 26.10 (More General Front-End Filters). Let the impulse response h âˆˆL1 be
invertible with respect to the band [âˆ’W, W ] in the sense that there exists some g âˆˆL1
such that Ë†g(f) = 1/Ë†h(f) for all |f| â‰¤W. Show that in the setup of Section 26.2 there is
no loss in optimality in basing our guess of M on the result of convolving

Y (t)

with h.
Exercise 26.11 (Positive Semideï¬nite Matrices).
(i) Let s1, . . . , sM be of ï¬nite energy.
Show that the M Ã— M matrix whose Row-j
Column-â„“entry is âŸ¨sj, sâ„“âŸ©is positive semideï¬nite.
(ii) Show that any M Ã— M positive semideï¬nite matrix can be expressed in this form
with a proper choice of the signals s1, . . . , sM.
Exercise 26.12 (A Lower Bound on the Minimum Distance). Let s1, . . . , sM be equi-
energy signals of energy Es. Let
Â¯d2 â‰œ
1
M(M âˆ’1)

mâ€²

mâ€²â€²Ì¸=mâ€²
âˆ¥smâ€² âˆ’smâ€²â€²âˆ¥2
2
denote the average squared-distance between the signals.
(i) Justify the following bound on Â¯d:
Â¯d2 =
1
M(M âˆ’1)
M

mâ€²=1
M

mâ€²â€²=1
âˆ¥smâ€² âˆ’smâ€²â€²âˆ¥2
2
=
2M
M âˆ’1Es âˆ’
2M
M âˆ’1
1
M2
M

mâ€²=1
M

mâ€²â€²=1
âŸ¨smâ€², smâ€²â€²âŸ©
=
2M
M âˆ’1Es âˆ’
2M
M âˆ’1

1
M
M

m=1
sm

2
2
â‰¤
2M
M âˆ’1Es.
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

668
Detection in White Gaussian Noise
(ii) Show that if, in addition, âŸ¨smâ€², smâ€²â€²âŸ©= ÏEs for all mâ€² Ì¸= mâ€²â€² in {1, . . . , M}, then
âˆ’
1
M âˆ’1 â‰¤Ï â‰¤1.
(iii) Are equalities possible in the above bounds?
Exercise 26.13 (Generalizations of the Simplex). Let pâˆ—(error; Es; Ï; M; N0) denote the
optimal probability of error for the setup of Section 26.2 for the case where the prior
on M is uniform and where
âŸ¨smâ€², smâ€²â€²âŸ©=

Es
if mâ€² = mâ€²â€²,
ÏEs
otherwise,
mâ€², mâ€²â€² âˆˆ{1, . . . , M}.
Show that
pâˆ—
error; Es; Ï; M; N0

= pâˆ—
error; Es(1 âˆ’Ï); 0; M; N0

,
âˆ’
1
M âˆ’1 â‰¤Ï â‰¤1.
Hint: You may need a diï¬€erent proof depending on the sign of Ï.
Exercise 26.14 (Decoding the Simplex without Gain Control). Let the simplex con-
stellation s1, . . . , sM be constructed from the orthonormal signals Ï†1, . . . , Ï†M as in Sec-
tion 26.9.4. In that section we proposed to decode by adding
1
âˆš
M âˆ’1
âˆš
EsÏˆ
to the received signal Y and then feeding the result to a decoder that was designed for
the orthogonal signals
s1 +
1
âˆš
M âˆ’1
âˆš
EsÏˆ, . . . , sM +
1
âˆš
M âˆ’1
âˆš
EsÏˆ.
Here Ïˆ is any signal that is orthogonal to the signals {s1, . . . , sM}. Show that feeding the
signal Y+Î±Ïˆ to the above orthogonal-keying decoder also results in an optimal decoding
rule, irrespective of the value of Î± âˆˆR.
Exercise 26.15 (Pretending the Noise Is White). Let H take on the values 0 and 1
equiprobably, and let the received waveform

Y (t)

be given at time t by
Y (t) = (1 âˆ’2H) s(t) + N(t),
where s: t â†’I{0 â‰¤t â‰¤1}, and where the SP

N(t)

is independent of H and is a
measurable, centered, stationary, Gaussian SP of autocovariance function
KNN(Ï„) = 1
4Î±eâˆ’|Ï„|/Î±,
Ï„ âˆˆR,
where 0 < Î± < âˆis some deterministic real parameter. Compute the probability of error
of a detector that guesses â€œH = 0â€ whenever
 1
0
Y (t) dt â‰¥0.
To what does this probability of error converge when Î± tends to zero?
.028
14:52:52, subject to the Cambridge Core terms of use, available at

26.13 Exercises
669
Exercise 26.16 (Antipodal Signaling in Colored Noise). Let s be an integrable signal that
is bandlimited to W Hz, and let H take on the values 0 and 1 equiprobably. Let the time-t
value of the received signal

Y (t)

be given by (1 âˆ’2H) s(t) + N(t), where

N(t)

is a
measurable, centered, stationary, Gaussian SP of PSD SNN. Assume that H and

N(t)

are independent, and that SNN can be whitened with respect to the bandwidth W. Find
the optimal probability of error in guessing H based on

Y (t)

.
Exercise 26.17 (Modeling Artifacts). Let H take on the values 0 and 1 equiprobably, and
let the received signal

Y (t)

be given by
Y (t) = (1 âˆ’2H) s(t) + N(t),
t âˆˆR,
where s: t â†’I{0 â‰¤t â‰¤1} and the SP

N(t)

is independent of H and is a measurable,
centered, stationary, Gaussian SP of autocovariance function
KNN(Ï„) = Î± eâˆ’Ï„2/Î²,
Ï„ âˆˆR,
for some Î±, Î² > 0.
Argue heuristically thatâ€”irrespective of the values of Î± and Î²â€”for any Ïµ > 0 we can ï¬nd
a rule for guessing H based on

Y (t)

whose probability of error is smaller than Ïµ.
Hint: Study Ë†s(f) and SNN(f) at high frequencies f.
Exercise 26.18 (Comparing Noises). Consider two channels of the kind considered in
Section 26.10. In the ï¬rst the PSD of the noise is S1 and in the second S2. Show that if
S1(f) â‰¥S2(f) for every f âˆˆR, then any performance that can be achieved in the ï¬rst
can also be achieved in the second.
Exercise 26.19 (Measurability in Theorem 26.3.1).
(i) Let

N(t)

be WGN of double-sided PSD N0/2 with respect to the bandwidth W.
Let R be a unit-mean exponential RV that is independent of

N(t)

. Deï¬ne the SP
Ëœ
N(t) = N(t) I{t Ì¸= R},
t âˆˆR.
Show that
 Ëœ
N(t)

is WGN of double-sided PSD N0/2 with respect to the band-
width W.
(ii) Let s be a nonzero integrable signal that is bandlimited to W Hz. To be concrete,
s(t) = sinc2(Wt),
t âˆˆR.
Suppose that the SP

N(t)

is as above and that for every Ï‰ âˆˆÎ© the sample-path
t â†’N(Ï‰, t) is continuous. Construct
 Ëœ
N(t)

as above. Suppose you wish to test
whether you are observing s or âˆ’s in the additive noise
 Ëœ
N(t)

. Show that you can
guess with zero probability of error by ï¬nding an epoch where the observed SP is
discontinuous and by comparing the value of the received signal at that epoch to
the value of s. (This does not violate Theorem 26.3.1 because this decision rule is
not measurable with respect to the Borel Ïƒ-algebra generated by the observed SP.)
.028
14:52:52, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

Chapter 27
Noncoherent Detection and Nuisance
Parameters
27.1
Introduction and Motivation
This chapter discusses a detection problem that arises in noncoherent communi-
cations.
For motivation, consider a transmitter that sends one of two diï¬€erent
passband waveforms
t 	â†’2 Re

s0,BB(t) ei2Ï€fct
or
t 	â†’2 Re

s1,BB(t) ei2Ï€fct
,
where s0,BB and s1,BB are integrable baseband signals that are bandlimited to W/2
Hz, and where the carrier frequency fc satisï¬es fc > W/2. To motivate our problem
it is instructive to consider the case where
fc â‰«W.
(27.1)
(In wireless communications it is common for fc to be orders-of-magnitude larger
than W.)
Let X(t) denote the transmitted waveform at time t.
Suppose that
the received waveform

Y (t)

is a delayed version of the transmitted waveform
corrupted by WGN of double-sided PSD N0/2 with respect to the bandwidth W
around the carrier frequency fc (Deï¬nition 25.15.9):
Y (t) = X(t âˆ’tD) + N(t),
t âˆˆR,
where tD denotes the delay (typically proportional to the distance between the
transmitter and the receiver) and

N(t)

is the additive noise. Suppose further
that the receiver estimates the delay to be tâ€²
D and moves its clock back by deï¬ning
tâ€² â‰œt âˆ’tâ€²
D.
(27.2)
If ËœY (tâ€²) is what the receiver receives when its clock shows tâ€², then by (27.2)
ËœY (tâ€²) = Y (tâ€² + tâ€²
D)
= X(tâ€² + tâ€²
D âˆ’tD) + N(tâ€² + tâ€²
D)
= X(tâ€² + tâ€²
D âˆ’tD) + ËœN(tâ€²),
tâ€² âˆˆR,
670
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.1 Introduction and Motivation
671
where ËœN(tâ€²) â‰œN(tâ€² + tâ€²
D) and is thus, by the stationarity of

N(t)

, also WGN of
double-sided PSD N0/2 with respect to the bandwidth W around fc. The term
X(tâ€² + tâ€²
D âˆ’tD) can be more explicitly written for every tâ€² âˆˆR as
X(tâ€² + tâ€²
D âˆ’tD) = 2 Re

sÎ½,BB(tâ€² + tâ€²
D âˆ’tD) ei2Ï€fc(tâ€²+tâ€²
Dâˆ’tD)
,
(27.3)
where Î½ is either zero or one, depending on which waveform is sent.
We next argue that if
tâ€²
D âˆ’tD
 â‰ª1
W,
(27.4)
then
sÎ½,BB(tâ€² + tâ€²
D âˆ’tD) â‰ˆsÎ½,BB(tâ€²),
tâ€² âˆˆR.
(27.5)
This can be seen by considering a Taylor Series expansion for sÎ½,BB(Â·) around tâ€²
sÎ½,BB(tâ€² + tâ€²
D âˆ’tD) â‰ˆsÎ½,BB(tâ€²) + dsÎ½,BB(Ï„)
dÏ„

Ï„=tâ€²

tâ€²
D âˆ’tD

and by then using Bernsteinâ€™s Inequality (Theorem 6.7.1) to heuristically argue
that the magnitude of the derivative of the baseband signal is of the same order of
magnitude as W, so its product by the timing error is, by (27.4), negligible.
From (27.3) and (27.5) we obtain that, as long as (27.4) holds,
X(tâ€² + tâ€²
D âˆ’tD) â‰ˆ2 Re

sÎ½,BB(tâ€²) ei2Ï€fc(tâ€²+tâ€²
Dâˆ’tD)
= 2 Re

sÎ½,BB(tâ€²) ei(2Ï€fctâ€²+Î¸)
,
tâ€² âˆˆR,
(27.6a)
where
Î¸ = 2Ï€fc(tâ€²
D âˆ’tD)
mod [âˆ’Ï€, Ï€).
(27.6b)
(Recall that Î¾ mod [âˆ’Ï€, Ï€) is the element in the interval [âˆ’Ï€, Ï€) that diï¬€ers from Î¾
by an integer multiple of 2Ï€.) Note that even if (27.4) holds, the term 2Ï€fc(tâ€²
Dâˆ’tD)
may be much larger than 1 when fc â‰«W.
We conclude that if the error in estimating the delay is negligible compared to the
reciprocal of the signal bandwidth but signiï¬cantly larger than the reciprocal of
the carrier frequency, then the received waveform can be modeled as
ËœY (tâ€²) = 2 Re

sÎ½,BB(tâ€²) ei(2Ï€fctâ€²+Î¸)
+ ËœN(tâ€²),
tâ€² âˆˆR,
(27.7)
where the receiver needs to determine whether Î½ is equal to zero or one;
 ËœN(tâ€²)

is additive WGN of double-sided PSD N0/2 with respect to the bandwidth W
around fc; and where the phase Î¸ is unknown to the receiver. Since the phase is
unknown to the receiver, the detection is said to be noncoherent. In the statistics
literature an unknown parameter such as Î¸ is called a nuisance parameter.
It would make engineering sense to ask for a decision rule for guessing Î½ based
on
 ËœY (tâ€²)

that would work well irrespective of the value of Î¸, but this is not the
question we shall ask. This question is related to â€œcomposite hypothesis testing,â€
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

672
Noncoherent Detection and Nuisance Parameters
which is not treated in this book.1 Instead we shall adopt a probabilistic approach.
We shall assume that Î¸ is a random variableâ€”and therefore henceforth denote it
by Î˜ and its realization by Î¸â€”that is uniformly distributed over the interval [âˆ’Ï€, Ï€)
independently of the noise and the message, and we shall seek a decision rule that
has the smallest average probability of error. Thus, if we denote the probability
of error conditional on Î˜ = Î¸ by p(error|Î¸), then we seek a decision rule based
on
 ËœY (t)

that minimizes
1
2Ï€
 Ï€
âˆ’Ï€
p(error|Î¸) dÎ¸.
(27.8)
The conservative reader may prefer to minimize the probability of error on the
â€œworst case Î¸â€
sup
Î¸âˆˆ[âˆ’Ï€,Ï€)
p(error|Î¸)
(27.9)
but, miraculously, it will turn out that the decoder we shall derive to minimize (27.8)
has a conditional probability of error p(error|Î¸) that does not depend on the real-
ization Î¸ so, as we shall see in Section 27.7, our decoder also minimizes (27.9).
27.2
The Setup
We next deï¬ne our hypothesis testing problem.
We denote time by t and the
received waveform by

Y (t)

(even though in the scenario we described in Sec-
tion 27.1 these correspond to tâ€² and
 ËœY (tâ€²)

, i.e., to the time coordinate and to the
corresponding signal at the receiver). We denote the RV we wish to guess by H
and assume a uniform prior:
Pr[H = 0] = Pr[H = 1] = 1
2.
(27.10)
For each Î½ âˆˆ{0, 1} the observation

Y (t)

is, conditionally on H = Î½, of the form
Y (t) = SÎ½(t) + N(t),
t âˆˆR,
(27.11)
where

N(t)

is WGN of positive double-sided PSD N0/2 with respect to the
bandwidth W around the carrier frequency fc (Deï¬nition 25.15.9), and where SÎ½(t)
can be described as
SÎ½(t) = 2 Re

sÎ½,BB(t) ei(2Ï€fct+Î˜)
= 2 Re

sÎ½,BB(t) ei2Ï€fct
cos Î˜ âˆ’2 Im

sÎ½,BB(t) ei2Ï€fct
sin Î˜
= 2 Re

sÎ½,BB(t) ei2Ï€fct
cos Î˜ + 2 Re

i sÎ½,BB(t) ei2Ï€fct
sin Î˜
= sÎ½,c(t) cos Î˜ + sÎ½,s(t) sin Î˜,
t âˆˆR,
(27.12)
where Î˜ is a RV that is uniformly distributed over the interval [âˆ’Ï€, Ï€) indepen-
dently of

H,

N(t)

, and where we deï¬ne for Î½ âˆˆ{0, 1}
sÎ½,c(t) â‰œ2 Re

sÎ½,BB(t) ei2Ï€fct
,
t âˆˆR,
(27.13a)
sÎ½,s(t) â‰œ2 Re

i sÎ½,BB(t) ei2Ï€fct
,
t âˆˆR.
(27.13b)
1See, for example, (Lehmann and Romano, 2005, Chapter 3).
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.3 From a SP to a Random Vector
673
Notice that by (27.13) and by the relationship between inner products in baseband
and passband (Theorem 7.6.10),
âŸ¨sÎ½,c, sÎ½,sâŸ©= 0,
Î½ = 0, 1.
(27.14)
We assume that the baseband signals s0,BB, s1,BB are integrable complex signals
that are bandlimited to W/2 Hz and that they are orthogonal:
âŸ¨s0,BB, s1,BBâŸ©= 0.
(27.15)
Consequently, by (27.13) and Theorem 7.6.10,
âŸ¨s0,c, s1,câŸ©= âŸ¨s0,s, s1,câŸ©= âŸ¨s0,c, s1,sâŸ©= âŸ¨s0,s, s1,sâŸ©= 0.
(27.16)
We ï¬nally assume that the baseband signals s0,BB and s1,BB are of equal positive
energy:
âˆ¥s0,BBâˆ¥2
2 = âˆ¥s1,BBâˆ¥2
2 > 0.
(27.17)
Deï¬ning2
Es = 2 âˆ¥s0,BBâˆ¥2
2
(27.18)
we have by the relationship between energy in baseband and passband (Theo-
rem 7.6.10)
Es = âˆ¥S0âˆ¥2
2 = âˆ¥S1âˆ¥2
2 = âˆ¥s0,sâˆ¥2
2 = âˆ¥s0,câˆ¥2
2 = âˆ¥s1,sâˆ¥2
2 = âˆ¥s1,câˆ¥2
2 .
(27.19)
By (27.14), (27.16), and (27.18)
1
âˆšEs

s0,c, s0,s, s1,c, s1,s

is an orthonormal 4-tuple.
(27.20)
Our problem is to guess H based on the observation

Y (t)

.
27.3
From a SP to a Random Vector
To derive an optimal guessing rule, we begin by exhibiting a random 4-vector, which
is computable from the observed SP

Y (t)

and on which we can base our decision
without loss of optimality. This will transform the problem from one where the
observation is a SP to one where it is a random 4-vector. (For the latter problem
we shall later exhibit a suï¬ƒcient statistic vector with only two components.) We
denote random vector by T and its four components by T0,c, T0,s, T1,c, and T1,s:
T =

T0,c, T0,s, T1,c, T1,s
T.
We denote its realization by t with corresponding components
t =

t0,c, t0,s, t1,c, t1,s
T.
2The â€œsâ€ in Es stands for â€œsignal,â€ whereas the â€œsâ€ in s0,s and s1,s stands for â€œsine.â€
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

674
Noncoherent Detection and Nuisance Parameters
The vector T is deï¬ned by
T â‰œ
	 #
Y, s0,c
âˆšEs
$
,
#
Y, s0,s
âˆšEs
$
,
#
Y, s1,c
âˆšEs
$
,
#
Y, s1,s
âˆšEs
$ 
T
(27.21)
=
1
âˆšEs
	  âˆ
âˆ’âˆ
Y (t) s0,c(t) dt, . . . ,
 âˆ
âˆ’âˆ
Y (t) s1,s(t) dt

T
.
(27.22)
We now prove that there is no loss of optimality in guessing H based on T. It is
interesting to note that this holds for Î˜ of arbitrary distribution (not necessarily
uniform) provided that the pair (H, Î˜) is independent of the additive noise. More-
over, it holds even if the baseband signals s0,BB and s1,BB are not orthogonal. The
key is that, irrespective of the realization of Î˜ and of the value of Î½, the signal SÎ½
lies in the four-dimensional subspace spanned by the signals s0,c, s0,s, s1,c, and s1,s.
The result thus follows from a generalization of Theorem 26.3.1 that we state next.
Theorem 27.3.1 (White Gaussian Noise with Nuisance Parameters). Let V be
a d-dimensional subspace of the space of all integrable signals that are bandlimited
to W Hz, and let (Ï†1, . . . , Ï†d) be an orthonormal basis for V. Let the RV M take
values in a ï¬nite set M. Suppose that, conditional on M = m, the SP

Y (t)

is
given by
Y (t) =
d

â„“=1
A(â„“)Ï†â„“(t) + N(t),
t âˆˆR,
(27.23)
where A = (A(1), . . . , A(d))T is a random d-vector whose law typically depends
on m, and where the SP

N(t)

is WGN with respect to the bandwidth W and
independent of the pair (M, A). Then no measurable rule for guessing M based on

Y (t)

can outperform an optimal rule for guessing M based on the d-vector
T =

âŸ¨Y, Ï†1âŸ©, . . . , âŸ¨Y, Ï†dâŸ©
T.
(27.24)
The theorem also holds in passband, i.e., if V is a d-dimensional subspace of the
space of all integrable signals that are bandlimited to W Hz around the carrier
frequency fc and if

N(t)

is WGN with respect to the bandwidth W around fc.
Note 27.3.2. Theorem 27.3.1 continues to hold even if (Ï†1, . . . , Ï†d) is not or-
thonormal; it suï¬ƒces that it be a basis for V. Indeed, if (u1, . . . , ud) is a basis
(not necessarily orthonormal) for V and if (v1, . . . , vd) is another basis (possibly
orthonormal) for V, then the inner products {âŸ¨Y, vâ„“âŸ©}d
â„“=1 are computable from the
inner products {âŸ¨Y, uâ„“âŸ©}d
â„“=1 (Lemma 25.10.3).
Before presenting the proof of Theorem 27.3.1 we show how it can be applied to
the noncoherent detection problem at hand. Here d = 4 and
V = span(s0,c, s0,s, s1,c, s1,s),
(27.25)
with Ï†1 â‰œs0,c/âˆšEs, Ï†2 â‰œs0,s/âˆšEs, Ï†3 â‰œs1,c/âˆšEs, and Ï†4 â‰œs1,s/âˆšEs. We note
that, conditional on H = 0, the received waveform

Y (t)

can be written in the
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.4 The Conditional Law of the Random Vector
675
form (27.23) where A(3) & A(4) are deterministically zero and the pair

A(1), A(2)
is uniformly distributed over the unit circle:

A(1)2 +

A(2)2 = 1.
Similarly, conditional on H = 1, the random variables A(1) and A(2) are deter-
ministically zero and the pair

A(3), A(4)
is uniformly distributed over the unit
circle. Thus, once we prove Theorem 27.3.1, it will follow that there is no loss of
optimality in forming our guess based on the 4-vector in (27.22).
Proof of Theorem 27.3.1. The proof is similar to the proof of Theorem 26.3.1.
We prove the result by showing that, given any measurable guessing rule Ï†Guess(Â·)
that is based on Y, we can ï¬nd a randomized decision rule of identical performance
that is based on T. As in the proof of Theorem 26.3.1, our decision rule uses local
randomness to generate a SP Nâ€² of the same FDDs as N and independent of
(M, A, N). From T and Nâ€² it then generates
Yâ€² =
d

â„“=1
âŸ¨Y, Ï†â„“âŸ©Ï†â„“+ Nâ€² âˆ’
d

â„“=1
âŸ¨Nâ€², Ï†â„“âŸ©Ï†â„“
(27.26)
and produces Ï†Guess(Yâ€²). To see that our rule has the same performance as the rule
that guesses Ï†Guess(Y), we note that, as in the proof of Theorem 26.3.1, Y and Yâ€²
have identical conditional FDDs given (M = m, A = a).
Consequently, upon
taking the expectation over A, we obtain that they also have the same conditional
FDDs given M = m. Hence the guesses Ï†Guess(Y) and Ï†Guess(Yâ€²) have identical
performance.
27.4
The Conditional Law of the Random Vector
Having established in the previous section that there is no loss of optimality in
basing our guess on the vector T in (27.21), we next proceed to calculate its
conditional distribution given H. This will allow us to compute the likelihood-
ratio fT|H=0(t)/fT|H=1(t) and to thus obtain an optimal guessing rule.
Rather than computing the conditional distribution directly, we begin with the
simpler conditional distribution of T given (H, Î˜).
Conditional on (H, Î˜), the
vector T is Gaussian (Theorem 25.12.1). Consequently, to compute its conditional
distribution we only need to compute its conditional mean vector and covariance
matrix, which we proceed to do.
Conditional on (H, Î˜) = (Î½, Î¸), the observed
process

Y (t)

can be expressed as
Y (t) = sÎ½,c(t) cos Î¸ + sÎ½,s(t) sin Î¸ + N(t),
t âˆˆR.
(27.27)
Hence, since

N(t)

is of zero mean, we have from (27.22) and (27.20)
E

T
 (H, Î˜) = (0, Î¸)

=

Es

cos Î¸, sin Î¸, 0, 0
T
,
(27.28a)
E

T
 (H, Î˜) = (1, Î¸)

=

Es

0, 0, cos Î¸, sin Î¸
T
,
(27.28b)
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

676
Noncoherent Detection and Nuisance Parameters
as we next calculate. The calculation is a bit tedious because we need to compute
the conditional mean of each of four random variables conditional on each of two
hypotheses, thus requiring eight calculations, which are all very similar but not
identical. We shall carry out only one calculation:
E

T0,c
 (H, Î˜) = (0, Î¸)

=
1
âˆšEs

âŸ¨s0,c cos Î¸ + s0,s sin Î¸, s0,câŸ©+ E[âŸ¨N, s0,câŸ©]

=
1
âˆšEs
âŸ¨s0,c cos Î¸ + s0,s sin Î¸, s0,câŸ©
=
1
âˆšEs

âˆ¥s0,câˆ¥2
2 cos Î¸ + âŸ¨s0,s, s0,câŸ©sin Î¸

=

Es cos Î¸,
where the ï¬rst equality follows from (27.27); the second because

N(t)

is of zero
mean (Proposition 25.10.1); the third from the linearity of the inner product and
by writing âŸ¨s0,c, s0,câŸ©as âˆ¥s0,câˆ¥2
2; and the ï¬nal equality from (27.20).
We next compute the conditional covariance matrix of T given (H, Î˜) = (Î½, Î¸). By
the orthonormality (27.20) and the whiteness of the noise (Proposition 25.15.2) we
have that, irrespective of Î½ and Î¸, this conditional covariance matrix is given by
the 4 Ã— 4 matrix (N0/2)I4, where I4 is the 4 Ã— 4 identity matrix.
Using the explicit form of the Gaussian distribution (19.6) and deï¬ning
Ïƒ2 â‰œN0
2 ,
(27.29)
we can thus write the conditional density as
fT|H=0,Î˜=Î¸(t)
=
1
(2Ï€Ïƒ2)2 exp
	
âˆ’1
2Ïƒ2

t0,c âˆ’

Es cos Î¸
2 +

t0,s âˆ’

Es sin Î¸
2 + t2
1,c + t2
1,s

=
1
(2Ï€Ïƒ2)2 exp

âˆ’Es
2Ïƒ2 âˆ’t0 + t1
2

Ã— exp
 1
Ïƒ2

Es t0,c cos Î¸ + 1
Ïƒ2

Es t0,s sin Î¸

,
t âˆˆR4,
(27.30)
where the second equality follows by opening the squares, by using the identity
cos2 Î¸ + sin2 Î¸ = 1, and by deï¬ning
T0 â‰œT 2
0,c + T 2
0,s
Ïƒ2
,
t0 â‰œt2
0,c + t2
0,s
Ïƒ2
,
(27.31a)
T1 â‰œT 2
1,c + T 2
1,s
Ïƒ2
,
t1 â‰œt2
1,c + t2
1,s
Ïƒ2
.
(27.31b)
(We deï¬ne T0 and T1 not only to simplify the typesetting: it will turn out that the
vector (T0, T1)T forms a suï¬ƒcient statistic for guessing H based on T.)
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.4 The Conditional Law of the Random Vector
677
To derive fT|H=0(t) (unconditioned on Î˜) we can integrate out Î˜. Thus, for every
t =

t0,c, t0,s, t1,c, t1,s
T in R4
fT|H=0(t) =
 Ï€
âˆ’Ï€
fÎ˜|H=0(Î¸) fT|H=0,Î˜=Î¸(t) dÎ¸
=
 Ï€
âˆ’Ï€
fÎ˜(Î¸) fT|H=0,Î˜=Î¸(t) dÎ¸
= 1
2Ï€
 Ï€
âˆ’Ï€
fT|H=0,Î˜=Î¸(t) dÎ¸
=
1
(2Ï€Ïƒ2)2 eâˆ’Es/(2Ïƒ2) eâˆ’(t1+t2)/2
Ã— 1
2Ï€
 Ï€
âˆ’Ï€
exp
	 1
Ïƒ2

Es t0,c cos Î¸ + 1
Ïƒ2

Es t0,s sin Î¸

dÎ¸
=
1
(2Ï€Ïƒ2)2 eâˆ’Es/(2Ïƒ2) eâˆ’(t0+t1)/2
Ã— 1
2Ï€
 Ï€
âˆ’Ï€
exp
-4
Es
Ïƒ2
âˆšt0 cos

Î¸ âˆ’arctan(t0,s/t0,c)
.
dÎ¸
=
1
(2Ï€Ïƒ2)2 eâˆ’Es/(2Ïƒ2) eâˆ’(t0+t1)/2
Ã— 1
2Ï€
 Ï€âˆ’arctan(t0,s/t0,c)
âˆ’Ï€âˆ’arctan(t0,s/t0,c)
exp
-4
Es
Ïƒ2
âˆšt0 cos Ïˆ
.
dÏˆ
=
1
(2Ï€Ïƒ2)2 eâˆ’Es/(2Ïƒ2) eâˆ’(t0+t1)/2 1
2Ï€
 Ï€
âˆ’Ï€
exp
-4
Es
Ïƒ2
âˆšt0 cos Ïˆ
.
dÏˆ
=
1
(2Ï€Ïƒ2)2 eâˆ’Es/(2Ïƒ2) eâˆ’(t1+t0)/2 I0
-4
Es
Ïƒ2
âˆšt0
.
,
(27.32)
where the ï¬rst equality follows by averaging out Î˜; the second because Î˜ and H
are independent; the third because Î˜ is uniform; the fourth by the explicit form of
fT|H=0,Î˜=Î¸(t) (27.30); the ï¬fth by the trigonometric identity
Î± cos Î¸ + Î² sin Î¸ =

Î±2 + Î²2 cos

Î¸ âˆ’arctan(Î²/Î±)

;
(27.33)
the sixth by the change of variable Ïˆ â‰œÎ¸ âˆ’arctan(t0,s/t0,c); the seventh from
the periodicity of the cosine function; and the ï¬nal equality by recalling that the
zeroth-order modiï¬ed Bessel function I0(Â·) is deï¬ned by
I0(Î¾) â‰œ1
2Ï€
 Ï€
âˆ’Ï€
eÎ¾ cos Ï† dÏ†
(27.34)
= 1
Ï€
 Ï€
0
eÎ¾ cos Ï† dÏ†
= 1
Ï€
 Ï€/2
0

eÎ¾ cos Ï† + eâˆ’Î¾ cos Ï†
dÏ†,
Î¾ âˆˆR.
(27.35)
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

678
Noncoherent Detection and Nuisance Parameters
By symmetry,
fT|H=1(t) =
1
(2Ï€Ïƒ2)2 eâˆ’Es/(2Ïƒ2) eâˆ’(t0+t1)/2 I0
-4
Es
Ïƒ2
âˆšt1
.
,
t âˆˆR4.
(27.36)
27.5
An Optimal Detector
By (27.32) and (27.36), the likelihood-ratio is given by
fT|H=0(t)
fT|H=1(t) =
I0
	3
Es
Ïƒ2
âˆšt0

I0
	3
Es
Ïƒ2
âˆšt1

,
t âˆˆR4,
(27.37)
which is computable from t0 and t1. This proves that the pair (T0, T1) deï¬ned in
(27.31) forms a suï¬ƒcient statistic for guessing H based on T (Deï¬nition 20.12.2).
Having identiï¬ed (T0, T1) as a suï¬ƒcient statistic, we now proceed to derive an
optimal decision rule using two diï¬€erent methods.
The ï¬rst method, which is
summarized in (20.79), ignores the fact that (T0, T1) is suï¬ƒcient and proceeds to
base the decision on the likelihood-ratio of T (27.37). The second method, which
is summarized in (20.80), bases the decision on the likelihood-ratio of the pair
(T0, T1).
Method 1:
Since we assumed a uniform prior (27.10), an optimal decision rule
is to guess â€œH = 0â€ whenever fT|H=0(t)/fT|H=1(t) â‰¥1, which, by (27.37) is
equivalent to
Guess â€œH = 0â€ if
I0
-4
Es
Ïƒ2
âˆšt0
.
â‰¥I0
-4
Es
Ïƒ2
âˆšt1
.
.
(27.38)
This rule can be further simpliï¬ed by noting that I0(Î¾) is (strictly) increasing in Î¾
for Î¾ â‰¥0. (This can be veriï¬ed by computing the derivative from (27.35)
d I0(Î¾)
dÎ¾
= 1
Ï€
 Ï€/2
0
cos Ï†

eÎ¾ cos Ï† âˆ’eâˆ’Î¾ cos Ï†
dÏ†
and by noting that for Î¾ > 0 the integrand is positive for all Ï† âˆˆ(0, Ï€/2).) Conse-
quently, the function Î¾ 	â†’I0
âˆšÎ¾

is also (strictly) increasing and the guessing rule
(27.38) is thus equivalent to the rule
Guess â€œH = 0â€ if t0 â‰¥t1.
(27.39)
In terms of the observable

Y (t)

this can be paraphrased using (27.31) and (27.22)
as guessing â€œH = 0â€ whenever
	 âˆ
âˆ’âˆ
Y (t) Re

s0,BB(t) ei2Ï€fct
dt

2
+
	 âˆ
âˆ’âˆ
Y (t) Re

i s0,BB(t) ei2Ï€fct
dt

2
â‰¥
	 âˆ
âˆ’âˆ
Y (t) Re

s1,BB(t) ei2Ï€fct
dt

2
+
	 âˆ
âˆ’âˆ
Y (t) Re

i s1,BB(t) ei2Ï€fct
dt

2
.
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.5 An Optimal Detector
679
Method 2:
We can obtain the same result by considering the likelihood-ratio
function of the suï¬ƒcient statistic (T0, T1)
fT0,T1|H=0(t0, t1)
fT0,T1|H=1(t0, t1).
We begin by arguing that, conditional on H = 0, the random variables T0, T1,
and Î˜ are independent with
fT0,T1,Î˜|H=0(t0, t1, Î¸) = 1
2Ï€ fÏ‡2
2,Î»1(t0) fÏ‡2
2,Î»0(t1),
(27.40)
where fÏ‡2
n,Î»(x) denotes the density at x of the noncentral Ï‡2 distribution with n
degrees of freedom and noncentrality parameter Î» (Section 19.8.2), and where
Î»0 = 0
and
Î»1 = Es
Ïƒ2 .
(27.41)
To prove (27.40) we compute for every t0, t1 âˆˆR and Î¸ âˆˆ[âˆ’Ï€, Ï€)
fT0,T1,Î˜|H=0(t0, t1, Î¸) = fÎ˜|H=0(Î¸) fT0,T1|H=0,Î˜=Î¸(t0, t1)
= 1
2Ï€ fT0,T1|H=0,Î˜=Î¸(t0, t1)
= 1
2Ï€ fT0|H=0,Î˜=Î¸(t0) fT1|H=0,Î˜=Î¸(t1)
= 1
2Ï€ fÏ‡2
2,Î»1(t0) fÏ‡2
2,Î»0(t1),
where the ï¬rst equality follows from the deï¬nition of the conditional density; the
second because Î˜ is independent of H and is uniformly distributed over the interval
[âˆ’Ï€, Ï€); the third because, conditional on (H, Î˜) = (0, Î¸), the random variables
T0,c, T0,s, T1,c, T1,s are independent (Section 27.4), and because T0 is a function
of (T0,c, T0,s) whereas T1 is a function of (T1,c, T1,s) (see (27.31)); and the ï¬nal
equality follows because, conditional on (H, Î˜) = (0, Î¸), the random variables
T0,c, T0,s, T1,c, T1,s are variance-Ïƒ2 Gaussians with means speciï¬ed in (27.28a) (Sec-
tion 19.8.2).
Integrating out Î¸ in (27.40) we obtain that, conditional on H, the random variables
T0 and T1 are independent with
fT0,T1|H=0(t0, t1) = fÏ‡2
2,Î»1(t0) fÏ‡2
2,Î»0(t1)
(27.42a)
fT0,T1|H=1(t0, t1) = fÏ‡2
2,Î»0(t0) fÏ‡2
2,Î»1(t1),
(27.42b)
where the expression for fT0,T1|H=1(t0, t1) is obtained using analogous steps.
Since H has a uniform prior, an optimal decision rule is thus to guess â€œH = 0â€
whenever
fÏ‡2
2,Î»1(t0) fÏ‡2
2,Î»0(t1) â‰¥fÏ‡2
2,Î»0(t0) fÏ‡2
2,Î»1(t1).
Since Î»1 > Î»0, this will hold, by Proposition 19.8.3, whenever t0 â‰¥t1. And by the
same proposition the inequality
fÏ‡2
2,Î»1(t0) fÏ‡2
2,Î»0(t1) â‰¤fÏ‡2
2,Î»0(t0) fÏ‡2
2,Î»1(t1)
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

680
Noncoherent Detection and Nuisance Parameters
will hold whenever t0 â‰¤t1. It is thus optimal to guess â€œH = 0â€ whenever t0 â‰¥t1
and to guess â€œH = 1â€ whenever t0 < t1. (It does not matter how we guess when
t0 = t1.) The decision rule (27.39) has thus been recovered.
27.6
The Probability of Error
In this section we compute the probability of error for the optimal guessing rule
(27.39). Since the probability of a tie (i.e., of T0 = T1) is zero both conditional on
H = 0 and conditional on H = 1, we shall analyze a slightly simpler guessing rule
that guesses â€œH = 0â€ if T0 > T1, and guesses â€œH = 1â€ if T1 > T0.
We begin with the conditional probability of error given that H = 0, i.e., with
Pr[T1 â‰¥T0 |H = 0]. Conditional on H = 0, the question of whether our decoder
errs depends prima facie not only on the realization of the additive noise

N(t)

but also on the realization of Î˜. But this is not the case because, conditionally on
H = 0, the pair (T0, T1) is independent of Î˜ (see (27.40)), so the realization of Î˜
does not play a role in the sense that for every Î¸ âˆˆ[âˆ’Ï€, Ï€)
Pr

T1 â‰¥T0
 H = 0, Î˜ = Î¸

= Pr

T1 â‰¥T0
 H = 0, Î˜ = 0

.
(27.43)
Conditional on (H, Î˜) = (0, Î¸) we have by (27.40) that T0 and T1 are independent
with T0 âˆ¼Ï‡2
2,Î»1 and with T1 âˆ¼Ï‡2
2,Î»0, i.e., with T1 having a mean-2 exponential
distribution (Note 19.8.1)
fT1|H=0,Î˜=Î¸(t1) = 1
2 eâˆ’t1/2,
t1 â‰¥0.
Consequently, for every Î¸ âˆˆ[âˆ’Ï€, Ï€) and Î¾ â‰¥0,
Pr

T1 â‰¥Î¾
 H = 0, Î˜ = Î¸

=
 âˆ
Î¾
1
2 eâˆ’t/2 dt = eâˆ’Î¾/2.
(27.44)
Starting with (27.43) we now have for every Î¸ âˆˆ[âˆ’Ï€, Ï€)
Pr

T1 â‰¥T0
 H = 0, Î˜ = Î¸

= Pr

T1 â‰¥T0
 H = 0, Î˜ = 0

=
 âˆ
0
fT0|H=0,Î˜=0(t0) Pr

T1 â‰¥t0
 H = 0, Î˜ = 0, T0 = t0

dt0
=
 âˆ
0
fT0|H=0,Î˜=0(t0) Pr

T1 â‰¥t0
 H = 0, Î˜ = 0

dt0
=
 âˆ
0
fT0|H=0,Î˜=0(t0) eâˆ’t0/2 dt0
= E
%
esT0
 H = 0, Î˜ = 0
&
s=âˆ’1/2
= MÏ‡2
2,Es/Ïƒ2 (s)

s=âˆ’1/2
= 1
2 eâˆ’Es
4Ïƒ2 ,
(27.45)
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.7 Discussion
681
where the ï¬rst equality follows from (27.43); the second from (26.82); the third
because conditional on H = 0 (and Î˜ = 0) the random variables T0 and T1
are independent; the fourth from (27.44); the ï¬fth by expressing

fZ(z) g(z) dz as
E[g(Z)] (with g(Â·) the exponential function); the sixth by the deï¬nition of the MGF
(19.23) and because, conditional on H = 0 and Î˜ = 0, we have that T0 âˆ¼Ï‡2
2,Es/Ïƒ2;
and the ï¬nal equality from the explicit expression for the MGF of a Ï‡2
2,Es/Ïƒ2 RV,
i.e., from (19.46) with the substitution n = 2 for the number of degrees of freedom,
Î» = Es/Ïƒ2 for the noncentrality parameter, and s = âˆ’1/2.
By symmetry we also have for every Î¸ âˆˆ[âˆ’Ï€, Ï€)
Pr

T0 â‰¥T1
 H = 1, Î˜ = Î¸

= 1
2 eâˆ’Es
4Ïƒ2 .
(27.46)
Thus, if we denote by pMAP(error|Î˜ = Î¸) the conditional probability of error of
our decoder conditional on Î˜ = Î¸, then by the uniformity of the prior (27.10) and
by (27.45) & (27.46)
pMAP(error|Î˜ = Î¸)
= Pr[H = 0] pMAP(error|H = 0, Î˜ = Î¸) + Pr[H = 1] pMAP(error|H = 1, Î˜ = Î¸)
= 1
2 Pr

T1 â‰¥T0
 H = 0, Î˜ = Î¸

+ 1
2 Pr

T0 â‰¥T1
 H = 1, Î˜ = Î¸

= 1
2 eâˆ’Es
4Ïƒ2 ,
Î¸ âˆˆ[âˆ’Ï€, Ï€).
(27.47)
Integrating (27.47) over Î¸ yields the optimal unconditional probability of error
pâˆ—(error) = 1
2 eâˆ’Es
4Ïƒ2 .
(27.48)
Using (27.29), this can also be expressed as
pâˆ—(error) = 1
2 eâˆ’Es
2N0 .
(27.49)
27.7
Discussion
The detector we derived has the property that its error probability does not depend
on the realization of the nuisance parameter Î˜; see (27.47). This property makes
the detector robust with respect to the distribution of Î˜: since the conditional
probability of error does not depend on the realization of Î˜, neither does the
average performance depend on the distribution of Î˜.
(Of course, if Î˜ is not
uniform, then our decoder need not be optimal.)
We next show that our guessing rule is also conservative in the sense that it mini-
mizes the worst-case performance:
sup
Î¸âˆˆ[âˆ’Ï€,Ï€)
p(error|Î˜ = Î¸).
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

682
Noncoherent Detection and Nuisance Parameters
That is, for any guessing rule of conditional error probability pâ€²(error|Î˜ = Î¸)
sup
Î¸âˆˆ[âˆ’Ï€,Ï€)
pâ€²(error|Î˜ = Î¸) â‰¥
sup
Î¸âˆˆ[âˆ’Ï€,Ï€)
pMAP(error|Î˜ = Î¸) = 1
2 eâˆ’Es
4Ïƒ2 .
(27.50)
Thus, while other decoders may outperform our decoder for some realizations of Î˜,
for other realizations their probability of error will be at least as high. Indeed, if
pâ€²(error|Î˜ = Î¸) is the conditional probability of error associated with any guessing
rule, then
sup
Î¸âˆˆ[âˆ’Ï€,Ï€)
pâ€²(error|Î˜ = Î¸) â‰¥1
2Ï€
 Ï€
âˆ’Ï€
pâ€²(error|Î˜ = Î¸) dÎ¸
â‰¥1
2Ï€
 Ï€
âˆ’Ï€
pMAP(error|Î˜ = Î¸) dÎ¸
=
sup
Î¸âˆˆ[âˆ’Ï€,Ï€)
pMAP(error|Î˜ = Î¸) dÎ¸
= 1
2 eâˆ’Es
4Ïƒ2 ,
where the ï¬rst inequality follows because the average (over Î¸) cannot exceed the
supremum; the second inequality because the decoder we designed minimizes the
unconditional probability of error; and the last two equalities follow from (27.47),
i.e., from the fact that the conditional probability of error pMAP(error|Î˜ = Î¸) of
our decoder does not depend on Î¸ and is equal to the RHS of (27.47).
It is interesting to assess the degradation in performance due to our ignorance
of Î˜. To that end we now compare the performance of our decoder with that of
the â€œcoherent decoder.â€ The coherent decoder is an optimal decoder for the setting
where the realization of Î˜ is known to the receiver, i.e., when the receiver can form
its guess based on both

Y (t)

and Î˜. If the receiver knows Î˜, it can compute
S0 and S1, and the problem reduces to the problem of guessing which of the two
known signals is being observed in WGN. Irrespective of Î˜, the signals S0 and S1
are orthogonal and of energy Es (by (27.12), (27.16), and (27.19)). Consequently,
the coherent guessing problem is the binary version of the problem we discussed in
Section 26.9.3. An optimal coherent guessing rule is thus
guess â€œH = 0â€ if
 âˆ
âˆ’âˆ
Y (t) S0(t) dt >
 âˆ
âˆ’âˆ
Y (t) S1(t) dt
with optimal probability of error (see (26.87))
pâˆ—
coherent(error|Î˜ = Î¸) = Q
-4
Es
2Ïƒ2
.
â‰ˆ
1

Ï€Es/Ïƒ2 exp

âˆ’Es
4Ïƒ2

,
Es
Ïƒ2 â‰«1,
(27.51)
where the approximation follows from (19.18). Integrating over Î¸ we obtain
pâˆ—
coherent(error) â‰ˆ
1

Ï€Es/Ïƒ2 exp

âˆ’Es
4Ïƒ2

,
Es
Ïƒ2 â‰«1.
(27.52)
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.8 Extension to M â‰¥2 Signals
683
Comparing (27.52) with (27.48) we see that if Es/Ïƒ2 is large, then we pay only
a small penalty for not knowing the phase.3 Of course, if the phase were known
precisely we might have used antipodal signaling with the resulting probability of
error being lower; see (26.66).4
27.8
Extension to M â‰¥2 Signals
We next brieï¬‚y address the M-ary version of the problem of noncoherent detec-
tion of orthogonal signals. We now denote the RV to be guessed by M and re-
place (27.10) with the assumption that M is uniformly distributed over the set
M = {1, . . . , M}, where M â‰¥2.
We wish to guess the value of M based on
the observation

Y (t)

(27.11), where Î½ now takes values in M and where the
orthogonality conditions (27.15) & (27.18) are now written as
âŸ¨sÎ½â€²,BB, sÎ½â€²â€²,BBâŸ©= 1
2 Es I{Î½â€² = Î½â€²â€²},
Î½â€², Î½â€²â€² âˆˆM.
(27.53)
We ï¬rst argue that no measurable decision rule that is based on Y can outperform
an optimal rule for guessing M based on the vector

T1, . . . , TM
T,
(27.54)
where, in analogy to (27.31), we deï¬ne
TÎ½ = T 2
Î½,c + T 2
Î½,s
Ïƒ2
,
Î½ âˆˆM,
and where
TÎ½,c =
#
Y, sÎ½,c
âˆšEs
$
and
TÎ½,s =
#
Y, sÎ½,s
âˆšEs
$
,
Î½ âˆˆM.
To this end, we ï¬rst note that, by Theorem 27.3.1, we can base our decision on the
collection
'
(TÎ½,c, TÎ½,s)
(
Î½âˆˆM.
(27.55)
To show that (T1, . . . , TM)T forms a suï¬ƒcient statistic for guessing M based on this
collection, it is enough to show pairwise suï¬ƒciency (Proposition 22.3.2). Pairwise
suï¬ƒciency can be proved using Proposition 22.4.2 because for every mâ€² Ì¸= mâ€²â€²
in M our analysis of the binary problem shows that the tuple (Tmâ€², Tmâ€²â€²) forms
a suï¬ƒcient statistic for testing between mâ€² and mâ€²â€², and this tuple is computable
from the vector in (27.54).
Our analysis of the binary case shows that, after observing

Y (t)

, the a posteriori
probability of the event M = m is larger than the a posteriori distribution of the
3Although pâˆ—(error)/pâˆ—
coherent(error) tends to inï¬nity as Es/Ïƒ2 â†’âˆ, it does so only subex-
ponentially.
4Comparing (26.87) and (26.66) we see that, to achieve the same probability of error, binary
orthogonal keying requires twice as much energy as antipodal signaling.
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

684
Noncoherent Detection and Nuisance Parameters
event M = mâ€² whenever Tm > Tmâ€². Consequently, Message m has the highest a
posteriori probability if Tm = maxmâ€²âˆˆM Tmâ€². Thus, the decision rule
Guess â€œM = mâ€ if Tm = max
mâ€²âˆˆM Tmâ€²
(27.56)
is optimal. The probability of a tie is zero, so it does not matter how ties are
resolved.
We next turn to the analysis of the probability of error. We shall assume that
a tie results in an error, so, conditional on M = m, an error occurs whenever
max{T1, . . . , Tmâˆ’1, Tm+1, . . . , TM} â‰¥Tm.
We ï¬rst show that, as in the binary
case, the probability of error associated with this guessing rule depends neither on
the realization of Î˜ nor on the message, i.e., that for every m âˆˆM and Î¸ âˆˆ[âˆ’Ï€, Ï€)
pMAP(error|M = m, Î˜ = Î¸) = pMAP(error|M = 1, Î˜ = 0).
(27.57)
To see this note that, conditional on (M, Î˜) = (m, Î¸), the components of the vec-
tor (27.54) are independent, with the m-th component being Ï‡2
2,Es/Ïƒ2 and with the
other components being Ï‡2
2,0. Consequently, irrespective of Î¸ and m, the condi-
tional probability of error is the probability that a Ï‡2
2,Es/Ïƒ2 RV is exceeded by, or
is equal to, at least one of M âˆ’1 IID Ï‡2
2,0 random variables that are independent
of it. In the analysis of the probability of error we shall thus assume that M = 1
and that Î¸ = 0.
The probability that the maximum among the random variables T2, . . . , TM exceeds
or is equal to Î¾ is given for every Î¾ â‰¥0 by
Pr

max{T2, . . . , TM} â‰¥Î¾
 M = 1, Î˜ = 0

= 1 âˆ’Pr

max{T2, . . . , TM} < Î¾
 M = 1, Î˜ = 0

= 1 âˆ’Pr

T2 < Î¾, . . . , TM < Î¾
 M = 1, Î˜ = 0

= 1 âˆ’

Pr

T2 < Î¾
 M = 1, Î˜ = 0
Mâˆ’1
= 1 âˆ’

1 âˆ’eâˆ’Î¾/2Mâˆ’1
= 1 âˆ’
Mâˆ’1

j=0
(âˆ’1)j
	M âˆ’1
j

eâˆ’jÎ¾/2,
(27.58)
where the ï¬rst equality follows because the probabilities of an event and of its
complement sum to one; the second because the maximum is smaller than Î¾ if,
and only if, all the random variables are smaller than Î¾; the third because, con-
ditionally on M = 1 and Î˜ = 0, the random variables T2, . . . , TM are IID; the
fourth because conditional on M = 1 and Î˜ = 0, the RV T2 is a mean-2 exponen-
tial (Note 19.8.1); and the ï¬nal equality follows from the binomial formula (26.85)
with the substitution a = 1, b = âˆ’eâˆ’Î¾/2, and n = M âˆ’1. The probability of error
is thus:
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.9 Exercises
685
Pr

max{T2, . . . , TM} â‰¥T1
 M = 1, Î˜ = Î¸

= Pr

max{T2, . . . , TM} â‰¥T1
 M = 1, Î˜ = 0

=
 âˆ
0
fT1|M=1,Î˜=0(t1) Pr

max{T2, . . . , TM} â‰¥t1
 M = 1, Î˜ = 0, T1 = t1

dt1
=
 âˆ
0
fT1|M=1,Î˜=0(t1) Pr

max{T2, . . . , TM} â‰¥t1
 M = 1, Î˜ = 0

dt1
=
 âˆ
0
fT1|M=1,Î˜=0(t1)
	
1 âˆ’
Mâˆ’1

j=0
(âˆ’1)j
	M âˆ’1
j

eâˆ’jt1/2

dt1
= 1 âˆ’
Mâˆ’1

j=0
(âˆ’1)j
	M âˆ’1
j

  âˆ
0
fT1|M=1,Î˜=0(t1) eâˆ’jt1/2 dt1
= 1 âˆ’
Mâˆ’1

j=0
(âˆ’1)j
	M âˆ’1
j

E
%
esT1
 M = 1, Î˜ = 0
&
s=âˆ’j/2
= 1 âˆ’
Mâˆ’1

j=0
(âˆ’1)j
	M âˆ’1
j

MÏ‡2
2,Es/Ïƒ2 (s)

s=âˆ’j/2
= 1 âˆ’
Mâˆ’1

j=0
(âˆ’1)j
	M âˆ’1
j

1
j + 1 eâˆ’
j
j+1
Es
2Ïƒ2 ,
where the justiï¬cations are very similar to the justiï¬cations of (27.45) except that
we use (27.58) instead of (27.44). Denoting the probability of error by pâˆ—(error)
and noting that for j = 0 the summand is 1, we have
pâˆ—(error) =
Mâˆ’1

j=1
(âˆ’1)j+1
	M âˆ’1
j

1
j + 1 eâˆ’
j
j+1
Es
2Ïƒ2 ,
(27.59)
or, upon recalling that Ïƒ2 was deï¬ned in (27.29) as N0/2,
pâˆ—(error) =
Mâˆ’1

j=1
(âˆ’1)j+1
	M âˆ’1
j

1
j + 1 eâˆ’
j
j+1
Es
N0 .
(27.60)
27.9
Exercises
Exercise 27.1 (The Conditional Law of the Random Vector). Conditional on M = m,
are the components of the random vector T in Theorem 27.3.1 independent? What about
conditional on (M, A) = (m, a) for m âˆˆM and a âˆˆRd?
Exercise 27.2 (A Silly Design Criterion). Let Ëœp(error|Î˜ = Î¸) denote the conditional
probability of error given Î˜ = Î¸ of some decision rule for the setup of Section 27.2. Show
that
inf
âˆ’Ï€â‰¤Î¸<Ï€ Ëœp(error|Î˜ = Î¸) â‰¥Q
)
Es
N0
*
.
Can you think of a detector that achieves this bound with equality? Would you recom-
mend using it?
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

686
Noncoherent Detection and Nuisance Parameters
Exercise 27.3 (A Coherent Detector for an Incoherent Channel). Alice designs a coherent
detector for the setup of Section 27.2 by pretending that Î˜ is deterministically equal to
zero and by then using the results on the detection of known signals in white Gaussian
noise. Show that if her detector is used over our channel where Î˜ âˆ¼U

[âˆ’Ï€, Ï€)

, then the
resulting average probability of error (averaged over Î˜) is 1/2.
Exercise 27.4 (Noncoherent Antipodal Signaling). Show that if in the setup of Sec-
tion 27.2 the baseband signals s0,BB and s1,BBâ€”rather than orthogonalâ€”are antipodal
in the sense that s0,BB = âˆ’s1,BB, then the optimal probability of error is 1/2.
Exercise 27.5 (A Fading Scenario). Consider the setup of Section 27.2 but with (27.11)
replaced by Y (t) = ASÎ½(t) + N(t), where A is a Rayleigh RV that is independent of

H, Î˜,

N(t)

. Find an optimal detector and the associated probability of error when A
is observed by the receiver. Repeat when A is unobserved.
Exercise 27.6 (Uniform Phase Noise Is the Worst Phase Noise). Consider the setup of
Section 27.2 but with Î˜ not necessarily uniformly distributed over [âˆ’Ï€, Ï€). Show that
the optimal probability of error is upper-bounded by the optimal probability of error
corresponding to the case where Î˜ âˆ¼U

[âˆ’Ï€, Ï€)

.
Exercise 27.7 (Unknown Frequency-Selective Channel). Let H take on the values 0 and 1
equiprobably, and let s be an integrable signal that is bandlimited to W Hz. When H = 0
the transmitted signal is s, and when H = 1 it is âˆ’s. Let U take on the values {up, down}
equiprobably and independently of H. When U = up the transmitted signal is passed
through a stable ï¬lter of impulse response hu; when U = down it is passed through
a stable ï¬lter of impulse response hd. At the receiver, white Gaussian noise

N(t)

of
PSD N0/2 over the bandwidth W is added to the received signal. The noise is independent
of (H, U). Based on the received waveform

Y (t)

, the receiver wishes to guess H. The
receiver has no knowledge of the realization of the switch U.
(i) Find a two-dimensional suï¬ƒcient statistic vector (T1, T2)T for this problem.
(ii) Find a decision rule that minimizes the probability of error.
Express your rule
using the function Ï†(x, y; Ïƒ2
x, Ïƒ2
y, Ï), which is the value at the point (x, y) of the
joint density of the zero-mean jointly Gaussian random variables X, Y of variances
Ïƒ2
x and Ïƒ2
y and covariance E[XY ] = ÏƒxÏƒyÏ.
Exercise 27.8 (Noncoherent Detection with Two Antennas). Consider the setup of Sec-
tion 27.2 but with the signal now received at two antennas. Denote the received signals
by

Y1(t)

and

Y2(t)

Y1(t) = 2 Re
	
sÎ½,BB(t) ei(2Ï€fct+Î˜1)
+ N1(t),
t âˆˆR,
Y2(t) = 2 Re
	
sÎ½,BB(t) ei(2Ï€fct+Î˜2)
+ N2(t),
t âˆˆR,
where the additive white noises

N1(t)

and

N2(t)

at the two antennas are independent.
(i) Suppose that the random phase at the two antennas Î˜1 and Î˜2 are unknown but
identical. Find an optimal detector and the optimal probability of error.
(ii) Assume now that Î˜1 and Î˜2 are independent. Find an optimal guessing rule for H.
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,

27.9 Exercises
687
Exercise 27.9 (Antenna Selection). Let X take on the values Â±1 equiprobably, and let the
transmitted signal be X s, where s is an energy-Es integrable signal that is bandlimited
to W Hz. The transmitted signal is received with two antennas: the ï¬rst receives Y1 =
A1X s + N1 and the second Y2 = A2X s + N2.
Here N1 and N2 are independent
white Gaussian noise processes of double-sided PSD N0/2 with respect to W; the positive
random variables A1 and A2 are IID, each taking values in a ï¬nite subset A of the positive
reals according to the probability mass function PA(Â·) of cumulative distribution function
FA(Â·); and the pair (A1, A2) is independent of (N1, N2). Deï¬ne T1 =

Y1, s/ âˆ¥sâˆ¥2

and
T2 =

Y2, s/ âˆ¥sâˆ¥2

. The receiver wishes to guess X based on (A1, A2, T1, T2).
(i) Find an optimal rule for guessing X based on (A1, A2, T1, T2). What is its prob-
ability of error? For which Î² does eâˆ’Î²Es/N0 best approximate this probability of
error when Es/N0 â‰«1?
(ii) The antenna-selection rule is to decide according to the sign of T1 when A1 â‰¥A2
and according to the sign of T2 otherwise. What is the probability of error of this
rule, and for which Î² does eâˆ’Î²Es/N0 best approximate it when Es/N0 â‰«1?
(iii) Repeat for the decision rule that ignores A1, A2 and decides according to the sign
of T1 + T2.
Exercise 27.10 (Unknown Polarity). Consider the setup of Section 27.2 but with Î˜ now
taking on the values âˆ’Ï€ and 0 equiprobably.
(i) Find an optimal decision rule for guessing H.
(ii) Bob suggests accounting for the random phase as follows. Pretend that the trans-
mitted signal is drawn uniformly from the set {Â±s0,c, Â±s1,c} and that it is observed
in white Gaussian noise. Feed the received signal to an optimal receiver for guessing
which of these four signals is being observed in white Gaussian noise, and if the
receiver produces the guess â€œs0,câ€ or â€œâˆ’s0,câ€, declare â€œH = 0â€; otherwise declare
â€œH = 1â€. Is Bobâ€™s receiver optimal?
Exercise 27.11 (Additional Channel Randomness). Consider the setup of Section 27.2
but when the observed SP

Y (t), t âˆˆR

, rather than being given by (27.11), is now given
by
Y (t) = SÎ½(t) + AN(t),
t âˆˆR,
where A is a positive RV that is independent of

H, Î˜,

N(t)

. Find an optimal decision
rule when A is observed. Repeat when A is not observed.
Exercise 27.12 (Mismatched Noncoherent Detection). Suppose that the signal fed to
the detector of Section 27.5 is
2 Re
	
uBB(t) ei(2Ï€fct+Î˜)
+ N(t),
t âˆˆR,
where uBB is an integrable signal that is bandlimited to W/2 Hz and that is orthogonal
to s0,BB, and where the other quantities are as deï¬ned in Section 27.2. Compute the
probability that the detector produces the guess â€œH = 0.â€ Express your answer in terms
of the inner product âŸ¨uBB, s1,BBâŸ©, the energy in uBB, and N0.
available at 
.029
15:13:41, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 28
Detecting PAM and QAM Signals in White
Gaussian Noise
28.1
Introduction and Setup
In Chapter 26 we addressed the problem of detecting one of M bandwidth-W sig-
nals corrupted by additive Gaussian noise that is white with respect to the band-
width W. Except for assuming that the mean signals are integrable signals that
are bandlimited to W Hz, we made no assumptions about their structure. In this
chapter we study the implication of the results of Chapter 26 for Pulse Amplitude
Modulation, where the mean signals correspond to diï¬€erent possible outputs of a
PAM modulator. The conclusions we shall draw are extremely important for the
design of receivers for systems employing PAM.
The key result of this chapter is that when PAM signals are observed in WGN, there
is no loss of optimality in the receiver basing its guess on the inner products between
the received waveform and the time shifts of the pulse shape by integer multiples of
the baud period Ts. These inner products can be computed by feeding the received
waveform to a matched ï¬lter that is matched to the pulse shape deï¬ning the PAM
signals and by then sampling the ï¬lterâ€™s output at integer multiples of the baud
period Ts (Corollary 5.8.3).
Using this result we can reduce the guessing problem from one where the observa-
tion is a continuous-time stochastic process to one where it is a discrete-time SP.
In fact, since we shall only consider the problem of detecting a ï¬nite number of
data bits, the reduction will be to a ï¬nite number of random variables. This will
justify the canonical structure of a PAM receiver where the received continuous-
time waveform is fed to a matched ï¬lter whose sampled output is then used by the
decision circuitry to produce its guess. We shall derive the results ï¬rst for PAM
and then brieï¬‚y describe their extension to QAM in Section 28.5.
In our setup k data bits D1, . . . , Dk are mapped by an encoder Ï•: {0, 1}k â†’Rn to
the real symbols X1, . . . , Xn, which are then mapped to the transmitted waveform
X(t) = A
n

â„“=1
Xâ„“g(t âˆ’â„“Ts),
t âˆˆR,
(28.1)
where A > 0 is a scaling constant; Ts > 0 is the baud period; and g(Â·) is the pulse
688
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.2 A Random Vector and Its Conditional Law
689
shape, which is assumed to be a real integrable signal that is bandlimited to W
Hz. The received waveform

Y (t)

is
Y (t) = X(t) + N(t)
= A
n

â„“=1
Xâ„“g(t âˆ’â„“Ts) + N(t),
t âˆˆR,
(28.2)
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W
and is independent of the data bits D1, . . . , Dk and hence also of

X(t)

. Based
on the received waveform

Y (t)

we wish to guess the data bits D1, . . . , Dk.
To simplify the typesetting we stack the k data bits D1, . . . , Dk in a binary vector
D = (D1, . . . , Dk)T,
(28.3)
the n symbols X1, . . . , Xn in a vector
X = (X1, . . . , Xn)T,
(28.4)
and we write
X = Ï•(D).
(28.5)
We express the n-tuple Ï•(d) that the encoder produces when D = d as
Ï•(d) =

x1(d), . . . , xn(d)
T,
(28.6)
where xÎ½(d) is the Î½-th real symbol that the encoder produces when it is fed d.
We denote the waveform that is transmitted to convey d by x(Â·; d), so
x(t; d) = A
n

â„“=1
xâ„“(d) g(t âˆ’â„“Ts),
t âˆˆR.
(28.7)
Conditional on D = d, the received waveform is thus
Y (t) = x(t; d) + N(t),
t âˆˆR.
(28.8)
28.2
A Random Vector and Its Conditional Law
We can view the vector D = (D1, . . . , Dk)T as a message and view the 2k diï¬€erent
values it can take as the set of messages. To promote this view we deï¬ne
D â‰œ{0, 1}k
(28.9)
to be the set of all 2k binary k-tuples, and we view D as the set of possible mes-
sages. In Chapter 21 on multi-hypothesis testing we denoted the set of hypotheses
(messages) by M and we labeled them 1, . . . , M, but we never attached a meaning
to the labels. So there is no harm in now labeling the hypotheses (messages) by
the binary k-tuples.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

690
Detecting PAM and QAM Signals in White Gaussian Noise
Associated with every message d âˆˆD is its prior Ï€d
Ï€d = Pr[D = d]
= Pr[D1 = d1, . . . , Dk = dk],
d âˆˆD.
(28.10)
If we assume that the data bits are IID random bits (Deï¬nition 14.5.1), then
Ï€d = 2âˆ’k for every k-tuple d âˆˆD, but this assumption is inessential to our
derivation.
Conditional on D = d, the transmitted waveform is x(Â·; d) of (28.7). Thus, the
problem of guessing D is equivalent to guessing which of the 2k signals
'
t 	â†’x(t; d)
(
dâˆˆD
(28.11)
is being observed in WGN of double-sided PSD N0/2 with respect to the band-
width W. From (28.7) it follows that for every message d âˆˆD the transmitted
waveform t 	â†’x(t; d) is a (deterministic) linear combination of the n functions
{t 	â†’g(t âˆ’â„“Ts)}n
â„“=1. Moreover, if the pulse shape g(Â·) is an integrable function
that is bandlimited to W Hz, then so is each waveform t 	â†’x(t; d) . Consequently,
from Theorem 26.3.1 (ii) and Theorem 26.4.1 (ii) we obtain:
Proposition 28.2.1 (Reducing the Output to a Random Vector: PAM in WGN).
Let the conditional law of

Y (t)

given D = d be given by (28.5), (28.7), and (28.8),
where the pulse shape g is a real integrable signal that is bandlimited to W Hz, and
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W.
Then no measurable rule for guessing D based on

Y (t)

can outperform an optimal
rule for guessing D based on the n inner products
T (â„“) =
 âˆ
âˆ’âˆ
Y (t) g(t âˆ’â„“Ts) dt,
â„“âˆˆ{1, . . . , n}.
(28.12)
Moreover, conditional on D = d, the vector T = (T (1), . . . , T (n))T is a Gaussian
n-vector whose â„“-th component T (â„“) is of conditional mean
E

T (â„“)  D = d

= A
n

â„“â€²=1
xâ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“âˆˆ{1, . . . , n}
(28.13)
and whose conditional covariance matrix is
N0
2
â›
âœ
âœ
âœ
âœ
â
Rgg(0)
Rgg(Ts)
Â· Â· Â·
Rgg

(n âˆ’1)Ts

Rgg(Ts)
Rgg(0)
Â· Â· Â·
Rgg

(n âˆ’2)Ts

Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
Rgg

(n âˆ’1)Ts

Rgg

(n âˆ’2)Ts

Â· Â· Â·
Rgg(0)
â
âŸ
âŸ
âŸ
âŸ
â 
,
(28.14)
i.e.,
Cov

T (â„“â€²), T (â„“â€²â€²)  D = d

= N0
2 Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

,
â„“â€², â„“â€²â€² âˆˆ{1, . . . , n}.
(28.15)
Here Rgg is the self-similarity function of the real pulse shape g (Deï¬nition 11.2.1),
and (x1(d), . . . , xn(d))T = Ï•(d) is the real n-tuple to which d is encoded.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.3 Other Optimality Criteria
691
Proof. This follows directly from Theorem 26.3.1 (ii) and Theorem 26.4.1 (ii) upon
substituting the mapping t 	â†’g(t âˆ’â„“Ts) for Ëœsj and upon computing the inner
product

t 	â†’g(t âˆ’â„“Ts), t 	â†’g(t âˆ’â„“â€²Ts)

= Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“, â„“â€² âˆˆZ.
28.3
Other Optimality Criteria
Proposition 28.2.1 establishes that if our objective is to minimize the probability of
a message error, then there is no loss of optimality in basing our guess on the vector
of inner products T = (T (1), . . . , T (n))T. But what if our objective is diï¬€erent? We
shall next consider other design criteria and show that, for those too, there is no
loss of optimality in basing our guess on T. Readers who can recall the proof of
Theorem 26.3.1 on which Proposition 28.2.1 is based will surely not be surprised.
We ï¬rst elaborate on what a message error is. If we denote our guess by
Ëœd =
 Ëœd1, . . . , Ëœdk
T,
then a message error occurs if our guess diï¬€ers from the message d in at least one
component, i.e., if Ëœdâ„“Ì¸= dâ„“for some â„“âˆˆ{1, . . . , k}. The probability of a message
error is thus
Pr
 ËœD Ì¸= D

.
(28.16)
Designing the receiver to minimize the probability of a message error is reason-
able, for example, when the k data bits constitute a computer ï¬le, and we wish to
minimize the probability that the ï¬le is corrupted. In such applications the userâ€™s
primary concern is whether the ï¬le was successfully received (no error occurred)
or whether it was corrupted (at least one error occurred). Minimizing the proba-
bility of a message error corresponds to minimizing the probability that the ï¬le is
corrupted.
In other applications, engineers are more interested in the average probability
of a bit error or bit error rate (BER). That is, they may wish to minimize
1
k
k

j=1
Pr
 ËœDj Ì¸= Dj

.
(28.17)
To better appreciate the diï¬€erence between the BER (28.17) and the probability
of a message error (28.16), deï¬ne the RV
Ej = I
 ËœDj Ì¸= Dj

,
j âˆˆ{1, . . . , k},
which indicates whether the j-th bit was incorrectly decoded.
Minimizing the
probability of a message error minimizes
Pr

k

j=1
Ej > 0

,
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

692
Detecting PAM and QAM Signals in White Gaussian Noise
whereas minimizing the BER minimizes
1
k E

k

j=1
Ej

.
(28.18)
Thus, minimizing the probability of a message error is equivalent to minimizing
the probability that one or more of the data bits are in error, whereas minimizing
the BER is equivalent to minimizing the expected number of data bits in error.
We next argue that there is no loss of optimality in basing our guess on T also
when we wish to minimize the BER (28.17). We ï¬rst note that to minimize (28.17)
we should choose for each j âˆˆ{1, . . . , k} our guess ËœDj to minimize
Pr
 ËœDj Ì¸= Dj

.
That is, we should consider the binary hypothesis testing problem of guessing
whether Dj is equal to zero or one, and we should guess ËœDj to minimize the
probability of error associated with this problem. To conclude our argument we
next show that for the purpose of minimizing Pr
 ËœDj Ì¸= Dj

, there is no loss of
optimality in basing our decision on T.
To prove this we will show that for every measurable guessing rule Ï†Guess(Â·) for
guessing Dj based on Y, we can ï¬nd a randomized decision rule based on T of
identical probability of error. As in the proof of Theorem 26.3.1, the randomized
decision rule uses its local randomness and T to generate a SP Yâ€² whose conditional
FDDs given D are identical to those of Y. It then produces the guess Ï†Guess(Yâ€²).
Since the conditional FDDs of Y and Yâ€² given D = d are identical for every d âˆˆD,
Pr
%
Ï†Guess(Y) Ì¸= Dj
 D = d
&
= Pr
%
Ï†Guess(Yâ€²) Ì¸= Dj
 D = d
&
,
d âˆˆD.
(28.19)
Since this is true for every d âˆˆD, we can take the expectation over D to obtain
Pr
%
Ï†Guess(Y) Ì¸= Dj
&
= Pr
%
Ï†Guess(Yâ€²) Ì¸= Dj
&
,
(28.20)
i.e., that the randomized decoder that is based on T has the same performance as
Ï†Guess(Â·).
This argument can be applied also to more general performance measures:
Proposition 28.3.1. Consider the setup of Proposition 28.2.1. Let Ïˆ: d 	â†’Ïˆ(d)
be any function of the data bits, and let D have an arbitrary prior.
Then no
measurable guessing rule for guessing Ïˆ(D) based on

Y (t)

can outperform an
optimal rule for guessing Ïˆ(D) based on

T (1), . . . , T (n)
.
Proof. The proof is almost identical to the one we used above to treat the bit-
error-rate criterion. Once again we consider any measurable decision rule Ï†Guess(Â·)
for guessing Ïˆ(D), and we mimic its performance by guessing Ï†Guess(Yâ€²), where Yâ€²
is generated using local randomness from T. Analogously to (28.19),
Pr
%
Ï†Guess(Y) Ì¸= Ïˆ(D)
 D = d
&
= Pr
%
Ï†Guess(Yâ€²) Ì¸= Ïˆ(D)
 D = d
&
,
d âˆˆD.
(28.21)
from which the result follows by taking expectation over D.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.4 Consequences of Orthonormality
693
D1, D2,
. . .
, DK,
enc(Â·)
X1, X2,
. . .
, XN,
enc(D1, . . . , DK)
DK+1,
. . .
, D2K,
enc(Â·)
XN+1,
. . .
, X2N,
enc(DK+1, . . . , D2K)
, Dkâˆ’K+1, . . . , Dk
enc(Â·)
, Xnâˆ’N+1,
. . .
, Xn
enc(Dkâˆ’K+1, . . . , Dk)
Figure 28.1: Block-mode encoding.
The examples we have seen so far correspond to the case where Ïˆ: d 	â†’d (with the
probability of guessing Ïˆ(D) incorrectly corresponding to a message error) and the
case Ïˆ: d 	â†’dj (with the probability of guessing Ïˆ(D) incorrectly corresponding
to the probability that the j-th bit Dj is incorrectly decoded). Another useful
example is when Ïˆ: d 	â†’

dÎ½, . . . , dÎ½â€²
for some given Î½, Î½â€² âˆˆN satisfying Î½â€² â‰¥Î½.
This situation corresponds to the case where (DÎ½, . . . , DÎ½â€²) constitutes a packet
and we are interested in the probability that the packet is erroneously decoded.
Yet another example arises in block-mode transmissionâ€”which is described in Sec-
tion 10.4 and which is depicted in Figure 28.1â€”where the data bits D1, . . . , Dk are
mapped to the symbols X1, . . . , Xn using a (K, N) binary-to-reals block encoder
enc: {0, 1}K â†’RN.
Here we assume that k is divisible by K and that n = N k/K.
If we wish to guess the K-tuple

D(Î½âˆ’1)K+1, . . . , D(Î½âˆ’1)K+K

with the smallest
probability of error, then there is no loss of optimality in basing our guess on
T (1), . . . , T (n).
This follows by applying Proposition 28.3.1 with the function
Ïˆ(d) =

d(Î½âˆ’1)K+1, . . . , d(Î½âˆ’1)K+K

.
28.4
Consequences of Orthonormality
The conditional distribution of the inner products in (28.12) is simpler when the
time shifts of the pulse shape by integer multiples of Ts are orthonormal. In this
case we denote the pulse shape by Ï†(Â·) and state the orthonormality condition as
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ,
(28.22)
or, equivalently, as
RÏ†Ï†(â„“Ts) =

1
if â„“= 0,
0
if â„“Ì¸= 0,
â„“âˆˆZ.
(28.23)
28.4.1
The Conditional Law of the Inner-Products Vector
From Proposition 28.2.1 we obtain a key result on PAM communication in WGN:
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

694
Detecting PAM and QAM Signals in White Gaussian Noise
Corollary 28.4.1. Consider PAM where data bits D1, . . . , Dk are mapped by an
encoder to the real symbols X1, . . . , Xn, which are then mapped to the waveform
X(t) = A
n

â„“=1
Xâ„“Ï†(t âˆ’â„“Ts),
t âˆˆR,
(28.24)
where the pulse shape Ï†(Â·) is an integrable signal that is bandlimited to W Hz and
whose time shifts by integer multiples of the baud period Ts are orthonormal. Let
the observed waveform

Y (t)

be given by
Y (t) = X(t) + N(t),
t âˆˆR,
where

N(t)

is independent of the data bits and is WGN of double-sided PSD N0/2
with respect to the bandwidth W.
(i) No measurable rule for guessing D1, . . . , Dk based on

Y (t)

can outperform
an optimal rule for guessing those bits based on the n inner products
T (â„“) =
 âˆ
âˆ’âˆ
Y (t) Ï†(t âˆ’â„“Ts) dt,
â„“âˆˆ{1, . . . , n}.
(28.25)
(ii) Conditional on D = d with corresponding encoder outputs (X1, . . . , Xn) =

x1(d), . . . , xn(d)

, the inner products (28.25) are independent with
T (â„“) âˆ¼N
	
A xâ„“(d), N0
2

,
â„“âˆˆ{1, . . . , n}.
(28.26)
(iii) The conditional distribution of these inner products can also be expressed as
T (â„“) = A xâ„“(d) + Zâ„“,
â„“âˆˆ{1, . . . , n},
(28.27a)
where
Z1, . . . , Zn âˆ¼IID N
	
0, N0
2

.
(28.27b)
From Proposition 28.3.1 we obtain that basing our guess on

T (1), . . . , T (n)
is
optimal also for guessing the value of any function of the data bits (D1, . . . , Dk).
28.4.2
A Further Reduction of Dimensionality
In block-mode transmission (with the pulse shape Ï†(Â·) still satisfying (28.23)) we
can typically base our decision on N inner products instead of n. For this to hold
we need to assume that the data bits are independent or that the k/K tuples

D1, . . . , DK

,

DK+1, . . . , D2K

, . . . ,

Dkâˆ’K+1, . . . , Dk

(28.28)
are independent.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.4 Consequences of Orthonormality
695
Proposition 28.4.2. In addition to the assumptions of Corollary 28.4.1, assume
that X1, . . . , Xn are generated from D1, . . . , Dk in block-mode using a (K, N) binary-
to-reals block encoder. Further assume that the K-tuples in (28.28) are independent.
Then for every Î½ âˆˆ{1, . . . , k/K}, the N-tuple

T((Î½âˆ’1)N+1), . . . , T(Î½N)
(28.29)
forms a suï¬ƒcient statistic for guessing the K-tuple

D(Î½âˆ’1)K+1, . . . , DÎ½K

(28.30)
based on (T (1), . . . , T (n)).
Proof. Fix some Î½ âˆˆ{1, . . . , k/K}.
The statement that the N-tuple (28.29) is
suï¬ƒcient for guessing the K-tuple (28.30) based on the n-tuple (T (1), . . . , T (n)) is
equivalent to the irrelevancy of
R â‰œ
	
T(1), . . . , T(N)
, . . . ,

T((Î½âˆ’2)N+1), . . . , T((Î½âˆ’1)N)
,

T(Î½N+1), . . . , T((Î½+1)N)
, . . . ,

T(nâˆ’N+1), . . . , T(n)
T
for guessing the K-tuple (28.30) based on the N-tuple (28.29). To prove this irrele-
vancy, it suï¬ƒces to prove two claims: that R is independent of the K-tuple (28.30)
and that, conditionally on this K-tuple, R is independent of the N-tuple (28.29)
(Proposition 22.6.5). These claims follow from three observations: that, by the
orthonormality assumption (28.23), R is determined by the data bits
D1, . . . , D(Î½âˆ’1)K, DÎ½K+1, . . . , Dk
(28.31)
and by the random variables
Z1, . . . , Z(Î½âˆ’1)N, ZÎ½N+1, . . . , Zn;
(28.32)
that the N-tuple (28.29) is determined by the K-tuple (28.30) and by the random
variables
Z(Î½âˆ’1)N+1, . . . , ZÎ½N;
(28.33)
and that the tuples in (28.30), (28.31), (28.32), and (28.33) are independent.
Having established that the N-tuple (28.29) forms a suï¬ƒcient statistic for guessing
the K-tuple (28.30), it now follows, using arguments very similar to those employed
in proving Proposition 28.3.1, that the N-tuple (28.29) is also suï¬ƒcient for guessing
the value of any function Ïˆ(Â·) of the K-tuple (28.30).
In combination with Corollary 28.4.1 and Exercise 28.10, Proposition 28.4.2 im-
plies:
Corollary 28.4.3. Under the assumptions of Proposition 28.4.2, no measurable rule
for guessing the value of a function of the K-tuple in (28.30) can outperform an
optimal rule for guessing that value based on the N-tuple in (28.29).
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

696
Detecting PAM and QAM Signals in White Gaussian Noise
28.4.3
The Discrete-Time Single-Block Model
Corollary 28.4.3 is the starting point of much of the literature on block codes, upon
which we shall touch in Chapter 29. In Coding Theory N is usually called the
blocklength, and K/N is called the rate in bits per dimension. Coding theorists
envision that the function enc(Â·) is used to map k bits to n real numbers using
the block-encoding rule of Figure 10.1 (with k being divisible by K) and that the
resulting real symbols are then transmitted over a WGN channel using PAM with
a pulse shape satisfying the orthogonality condition (28.23). Assuming that the
data tuples are independent, and by then resorting to Corollary 28.4.3, they then
focus on the problem of decoding the K-tuple (28.30) from the N matched ï¬lter
outputs (28.29).
In this problem the index Î½ of the block is immaterial, and coding theorists re-
label the data bits of the K tuple (28.30) as D1, . . . , DK; they re-label the symbols
to which they are mapped as X1, . . . , XN; and they re-label the corresponding
observations as Y1, . . . , YN.
The resulting model is the discrete-time single-
block model where

X1, . . . , XN

= enc

D1, . . . , DK

,
(28.34a)
YÎ· = AXÎ· + ZÎ·,
Î· âˆˆ{1, . . . , N},
(28.34b)
ZÎ· âˆ¼N
	
0, N0
2

,
Î· âˆˆ{1, . . . , N},
(28.34c)
and where Z1, . . . , ZN are IID and independent of D1, . . . , DK.
We recall that
this model is appropriate when the pulse shape Ï† satisï¬es the orthonormality
condition (28.22); the data bits are â€œblock IIDâ€ in the sense that the k/K tuples in
(28.28) are independent; and the additive noise is WGN of double-sided PSD N0/2
with respect to the bandwidth occupied by the pulse shape Ï†. It is customary to
additionally assume that D1, . . . , DK are IID random bits (Deï¬nition 14.5.1). This
is a good assumption if, prior to transmission, the data bits are compressed using
an eï¬ƒcient data compression algorithm.
28.5
Extension to QAM Communications
28.5.1
Introduction and Setup
We next extend our discussion to the detection of QAM signals. We assume that
an encoding function
Ï•: {0, 1}k â†’Cn
is used to map the k data bits D = (D1, . . . , Dk)T to the n complex symbols
C = (C1, . . . , Cn)T, and we denote by
Ï•(d) =

c1(d), . . . , cn(d)
T
(28.35)
the result of encoding the data bits d. The complex symbols are mapped to the
passband signal
XPB(t) = 2 Re

XBB(t) ei2Ï€fct
,
t âˆˆR,
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.5 Extension to QAM Communications
697
where
XBB(t) = A
n

â„“=1
Câ„“g(t âˆ’â„“Ts),
t âˆˆR;
the pulse shape g(Â·) is a complex integrable signal that is bandlimited to W/2 Hz;
A > 0 is a real constant; and fc > W/2. Conditional on D = d, we denote the
transmitted signal by x(Â·; d), so
x(t; d) = 2A Re
	
n

â„“=1
câ„“(d) g(t âˆ’â„“Ts) ei2Ï€fct

(28.36)
=
âˆš
2A
n

â„“=1
Re

câ„“(d)

gI,â„“(t)



2 Re
	 1
âˆš
2 g(t âˆ’â„“Ts)



gI,â„“,BB(t)
ei2Ï€fct

+
âˆš
2A
n

â„“=1
Im

câ„“(d)

gQ,â„“(t)



2 Re
	
i 1
âˆš
2 g(t âˆ’â„“Ts)



gQ,â„“,BB(t)
ei2Ï€fct

,
t âˆˆR,
(28.37)
where (28.37) follows from (16.7); and where {gI,â„“}, {gQ,â„“}, {gI,â„“,BB}, {gQ,â„“,BB}
are as indicated in (28.37) and as deï¬ned in (16.8) and (16.9).
Conditional on D = d, the received waveform is
Y (t) = x(t; d) + N(t),
t âˆˆR,
(28.38)
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W
around the carrier frequency fc (Deï¬nition 25.15.9).
28.5.2
From a SP to a Random Vector
The representation (28.37) makes it clear that for every d âˆˆ{0, 1}k the signal
t 	â†’x(t; d) can be expressed as a linear combination of the 2n real-valued signals
{gI,â„“}n
â„“=1,
{gQ,â„“}n
â„“=1.
(28.39)
Since these signals are integrable signals that are bandlimited to W Hz around
the carrier frequency fc, it follows (see Section 26.8) that no measurable rule for
guessing D based on

Y (t)

can outperform an optimal rule for guessing D based
on the 2n inner products
T (â„“)
I
=
 âˆ
âˆ’âˆ
Y (t) gI,â„“(t) dt,
â„“âˆˆ{1, . . . , n},
(28.40a)
T (â„“)
Q
=
 âˆ
âˆ’âˆ
Y (t) gQ,â„“(t) dt,
â„“âˆˆ{1, . . . , n}.
(28.40b)
To describe the conditional joint distribution of these inner products conditional on
each of the hypotheses, we next express the inner products between the functions
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

698
Detecting PAM and QAM Signals in White Gaussian Noise
in (28.39) in terms of the self-similarity function Rgg of the complex pulse shape g
Rgg(Ï„) =
 âˆ
âˆ’âˆ
g(t + Ï„) gâˆ—(t) dt,
Ï„ âˆˆR
(28.41)
(Deï¬nition 11.2.1). Key to these calculations is the relationship between the inner
product between real passband signals and the inner product between their complex
baseband representations (Theorem 7.6.10). Thus,
âŸ¨gI,â„“â€², gI,â„“âŸ©= 2 Re

âŸ¨gI,â„“â€²,BB, gI,â„“,BBâŸ©

= Re

t 	â†’g(t âˆ’â„“â€²Ts), t 	â†’g(t âˆ’â„“Ts)

= Re
	 âˆ
âˆ’âˆ
g(t âˆ’â„“â€²Ts) gâˆ—(t âˆ’â„“Ts) dt

= Re

Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“, â„“â€² âˆˆZ,
(28.42a)
where the ï¬rst equality follows by relating the inner product in passband to the
inner product in baseband; the second from the expressions for the corresponding
baseband representations (16.9a); the third from the deï¬nition of the inner product
for complex-valued signals (3.4); and the ï¬nal equality from the deï¬nition of the
self-similarity function (28.41). Similarly,
âŸ¨gQ,â„“â€², gQ,â„“âŸ©= 2 Re

âŸ¨gQ,â„“â€²,BB, gQ,â„“,BBâŸ©

= Re

t 	â†’ig(t âˆ’â„“â€²Ts), t 	â†’ig(t âˆ’â„“Ts)

= Re
	 âˆ
âˆ’âˆ
ig(t âˆ’â„“â€²Ts) (âˆ’i) gâˆ—(t âˆ’â„“Ts) dt

= Re
	 âˆ
âˆ’âˆ
g(t âˆ’â„“â€²Ts) gâˆ—(t âˆ’â„“Ts) dt

= Re

Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“, â„“â€² âˆˆZ,
(28.42b)
and
âŸ¨gQ,â„“â€², gI,â„“âŸ©= 2 Re

âŸ¨gQ,â„“â€²,BB, gI,â„“,BBâŸ©

= Re

t 	â†’ig(t âˆ’â„“â€²Ts), t 	â†’g(t âˆ’â„“Ts)

= Re
	
i
 âˆ
âˆ’âˆ
g(t âˆ’â„“â€²Ts) gâˆ—(t âˆ’â„“Ts) dt

= âˆ’Im
	 âˆ
âˆ’âˆ
g(t âˆ’â„“â€²Ts) gâˆ—(t âˆ’â„“Ts) dt

= âˆ’Im

Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“, â„“â€² âˆˆZ,
(28.42c)
where the ï¬rst equality leading to (28.42c) follows from the relationship between
the inner product between real passband signals and the inner product between
their baseband representations (Theorem 7.6.10); the second from the expressions
for the corresponding baseband representations (16.9); the third from the deï¬nition
of the inner product between complex signals (3.4); the fourth from the identity
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.5 Extension to QAM Communications
699
Re(iz) = âˆ’Im(z); and where the last equality follows from the deï¬nition of the
self-similarity function of complex signals (28.41).
We are now ready to compute the conditional laws of the inner products given
each of the hypotheses. Conditional on D = d with corresponding encoder output
Ï•(d) of (28.35), the 2n random variables

T (â„“)
I
, T (â„“)
Q
n
â„“=1 are jointly Gaussian (Sec-
tion 26.8). Their conditional law is thus fully speciï¬ed by the conditional mean
vector and by the conditional covariance matrix. We begin with the former:
E
%
T (â„“)
I
 D = d
&
=

t 	â†’x(d; t), gI,â„“

=
âˆš
2A
n

â„“â€²=1
Re

câ„“â€²(d)

gI,â„“â€² +
âˆš
2A
n

â„“â€²=1
Im

câ„“â€²(d)

gQ,â„“â€², gI,â„“

=
âˆš
2A
n

â„“â€²=1

Re

câ„“â€²(d)

âŸ¨gI,â„“â€², gI,â„“âŸ©+ Im

câ„“â€²(d)

âŸ¨gQ,â„“â€², gI,â„“âŸ©

=
âˆš
2A
n

â„“â€²=1
	
Re

câ„“â€²(d)

Re

Rgg

(â„“âˆ’â„“â€²)Ts

âˆ’Im

câ„“â€²(d)

Im

Rgg

(â„“âˆ’â„“â€²)Ts

=
âˆš
2A
n

â„“â€²=1
Re

câ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

=
âˆš
2A Re
	
n

â„“â€²=1
câ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“âˆˆ{1, . . . , n},
(28.43a)
where the ï¬rst equality follows from the deï¬nition of T (â„“)
I
(28.40a), from (28.38),
and from our assumption that the noise

N(t)

is of zero mean; the second from
(28.37); the third from the linearity of the inner product; the fourth by express-
ing the inner products using the self-similarity function, i.e., using (28.42a) and
(28.42c); the ï¬fth by the complex-numbers identity Re(wz) = Re(w) Re(z) âˆ’
Im(w) Im(z); and the ï¬nal equality because the sum of the real parts is the real
part of the sum. Similarly,
E
%
T (â„“)
Q
 D = d
&
=

t 	â†’x(d; t), gQ,â„“

=
âˆš
2A
n

â„“â€²=1
Re

câ„“â€²(d)

gI,â„“â€² +
âˆš
2A
n

â„“â€²=1
Im

câ„“â€²(d)

gQ,â„“â€², gQ,â„“

=
âˆš
2A
n

â„“â€²=1

Re

câ„“â€²(d)

âŸ¨gI,â„“â€², gQ,â„“âŸ©+ Im

câ„“â€²(d)

âŸ¨gQ,â„“â€², gQ,â„“âŸ©

=
âˆš
2A
n

â„“â€²=1
-
Re

câ„“â€²(d)
	
âˆ’Im

Rgg

(â„“â€² âˆ’â„“)Ts

+ Im

câ„“â€²(d)

Re

Rgg

(â„“âˆ’â„“â€²)Ts
.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

700
Detecting PAM and QAM Signals in White Gaussian Noise
=
âˆš
2A
n

â„“â€²=1
	
Re

câ„“â€²(d)

Im

Rgg

(â„“âˆ’â„“â€²)Ts

+ Im

câ„“â€²(d)

Re

Rgg

(â„“âˆ’â„“â€²)Ts

=
âˆš
2A
n

â„“â€²=1
Im

câ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

=
âˆš
2A Im
	
n

â„“â€²=1
câ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“âˆˆ{1, . . . , n},
(28.43b)
where the ï¬rst equality follows from the deï¬nition of T (â„“)
Q
(28.40b), from (28.38),
and from our assumption that the noise

N(t)

is of zero mean; the second from
(28.37); the third from the linearity of the inner product; the fourth by express-
ing the inner products using the self-similarity function, i.e., using (28.42c) and
(28.42b); the ï¬fth by the conjugate symmetry of the self-similarity function (Propo-
sition 11.2.2); the sixth by the complex-numbers identity Im(wz) = Re(w) Im(z) +
Im(w) Re(z); and the ï¬nal equality by noting that the sum of the imaginary parts
is equal to the imaginary part of the sum.
The conditional covariances are easily computed using Note 25.15.10. Using the
inner products expressions (28.42), we obtain:
Cov
%
T (â„“â€²)
I
, T (â„“â€²â€²)
I
 D = d
&
= N0
2

gI,â„“â€², gI,â„“â€²â€²
= N0
2 Re

Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

,
(28.44a)
Cov
%
T (â„“â€²)
Q
, T (â„“â€²â€²)
Q
 D = d
&
= N0
2

gQ,â„“â€², gQ,â„“â€²â€²
= N0
2 Re

Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

,
(28.44b)
and
Cov
%
T (â„“â€²)
I
, T (â„“â€²â€²)
Q
 D = d
&
= N0
2

gI,â„“â€², gQ,â„“â€²â€²
= âˆ’N0
2 Im

Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

.
(28.44c)
We summarize our results on QAM detection in WGN as follows.
Proposition 28.5.1 (Reducing the Output to a Random Vector: QAM in WGN).
Let a QAM signal (28.37) of an integrable pulse shape g(Â·) that is bandlimited
to W/2 Hz be observed in WGN of double-sided PSD N0/2 with respect to the
bandwidth W around the carrier frequency fc. Then:
(i) No measurable rule for guessing D based on

Y (t)

can outperform an optimal
rule for guessing D based on the 2n inner products
T (â„“)
I
=
 âˆ
âˆ’âˆ
Y (t) gI,â„“(t) dt,
â„“âˆˆ{1, . . . , n},
(28.45a)
T (â„“)
Q
=
 âˆ
âˆ’âˆ
Y (t) gQ,â„“(t) dt,
â„“âˆˆ{1, . . . , n},
(28.45b)
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.5 Extension to QAM Communications
701
where
gI,â„“(t) = 2 Re
 1
âˆš
2 g(t âˆ’â„“Ts) ei2Ï€fct
,
t âˆˆR,
gQ,â„“(t) = 2 Re
 1
âˆš
2i g(t âˆ’â„“Ts) ei2Ï€fct
,
t âˆˆR.
(ii) Conditional on D = d with corresponding transmitted symbols as is (28.35),
these 2n real random variables are jointly Gaussian with conditional means as
speciï¬ed by (28.43) and with conditional covariances as speciï¬ed by (28.44).
28.5.3
From a SP to a Complex Random Vector
The notation is simpler if we introduce the n complex random variables
T (â„“) â‰œT (â„“)
I
+ i T (â„“)
Q
=
 âˆ
âˆ’âˆ
Y (t) gI,â„“(t) dt + i
 âˆ
âˆ’âˆ
Y (t) gQ,â„“(t) dt,
â„“âˆˆ{1, . . . , n}.
(28.46)
There is, of course, a one-to-one relationship between these n complex random
variables and the 2n real inner products, so it is also optimal to base our guess on
the former. Using (28.43) we obtain
E
%
T (â„“)  D = d
&
= E
%
T (â„“)
I
 D = d
&
+ i E
%
T (â„“)
Q
 D = d
&
=
âˆš
2A Re
	
n

â„“â€²=1
câ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

+ i
âˆš
2A Im
	
n

â„“â€²=1
câ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

=
âˆš
2A
n

â„“â€²=1
câ„“â€²(d) Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“âˆˆ{1, . . . , n}.
(28.47)
The advantage of the complex notation is thatâ€”as we shall see in Proposition 28.5.2
aheadâ€”conditional on D = d, the random vector Tâˆ’E[T|D = d] is proper (Deï¬-
nition 17.4.1). And since conditionally on D = d it is also Gaussian, it follows from
Proposition 24.3.11 that, conditional on D = d, the random vector Tâˆ’E[T|D = d]
is a circularly-symmetric complex Gaussian (Deï¬nition 24.3.2). Its conditional law
is thus determined by its conditional covariance matrix (Corollary 24.3.8). This
covariance matrix is an n Ã— n (complex) matrix, whereas the covariance matrix for
the 2n real variables in Proposition 28.5.1 is a (2n) Ã— (2n) (real) matrix.
We summarize our results for QAM detection with complex notation as follows.
Proposition 28.5.2 (A Complex Random Vector for QAM in WGN). Consider
the setup of Proposition 28.5.1.
(i) No measurable rule for guessing D based on

Y (t)

can outperform an optimal
rule for guessing D based on the complex random vector T = (T (1), . . . , T (n))T
deï¬ned by
T (â„“) =
 âˆ
âˆ’âˆ
Y (t) gI,â„“(t) dt + i
 âˆ
âˆ’âˆ
Y (t) gQ,â„“(t) dt,
â„“âˆˆ{1, . . . , n}.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

702
Detecting PAM and QAM Signals in White Gaussian Noise
(ii) The â„“-th component of T can be expressed as
T (â„“) =
âˆš
2A
n

â„“â€²=1
Câ„“â€² Rgg

(â„“âˆ’â„“â€²)Ts

+ Z(â„“),
â„“âˆˆ{1, . . . , n},
where Rgg is the self-similarity function of the pulse shape g(Â·) (28.41), and
where the random vector Z = (Z(1), . . . , Z(n))T is independent of D and is a
circularly-symmetric complex Gaussian of covariance
Cov
%
Z(â„“â€²), Z(â„“â€²â€²)&
= E
%
Z(â„“â€²)
Z(â„“â€²â€²)âˆ—&
= N0 Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

,
â„“â€², â„“â€²â€² âˆˆ{1, . . . , n}.
(28.48)
(iii) If the time shifts of the pulse shape by integer multiples of Ts are orthonormal,
then
T (â„“) =
âˆš
2ACâ„“+ Z(â„“),
â„“âˆˆ{1, . . . , n, },
(28.49)
where the complex random variables {Z(â„“)} are independent of D and are IID
circularly-symmetric complex Gaussians of variance N0.
Proof. Part (i) follows directly from Proposition 28.5.1 because there is a one-to-
one relationship between the complex n-vector and the 2n (real) inner products.
To prove Part (ii) deï¬ne
Z(â„“) â‰œT (â„“) âˆ’
âˆš
2A
n

â„“â€²=1
Câ„“â€² Rgg

(â„“âˆ’â„“â€²)Ts

,
â„“âˆˆ{1, . . . , n},
(28.50)
and note that by (28.47) the conditional distribution of Z given D = d is of zero
mean. Moreover, from Proposition 28.5.1 and from the deï¬nition of a complex
Gaussian random vector as one whose real and imaginary parts are jointly Gaussian
(Deï¬nition 24.3.6), it follows that, conditional on D = d, the vector Z is Gaussian.
To prove that it is proper we compute
E
%
Z(â„“â€²)Z(â„“â€²â€²)  D = d
&
= E
%
Re

Z(â„“â€²)
Re

Z(â„“â€²â€²)
âˆ’Im

Z(â„“â€²)
Im

Z(â„“â€²â€²)  D = d
&
+ i E
%
Re

Z(â„“â€²)
Im

Z(â„“â€²â€²)
+ Im

Z(â„“â€²)
Re

Z(â„“â€²â€²)  D = d
&
= Cov
%
T (â„“â€²)
I
, T (â„“â€²â€²)
I
 D = d
&
âˆ’Cov
%
T (â„“â€²)
Q
, T (â„“â€²â€²)
Q
 D = d
&
+ i

Cov
%
T (â„“â€²)
I
, T (â„“â€²â€²)
Q
 D = d
&
+ Cov
%
T (â„“â€²)
Q
, T (â„“â€²â€²)
I
 D = d
&
= 0,
â„“â€², â„“â€²â€² âˆˆ{1, . . . , n},
where the second equality follows from (28.50) and the last equality from (28.44).
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.6 Additional Reading
703
The calculation of the conditional covariance matrix is very similar except that Z(â„“â€²â€²)
is now conjugated:
Cov
%
Z(â„“â€²), Z(â„“â€²â€²) D = d
&
= E
%
Re

Z(â„“â€²)
Re

Z(â„“â€²â€²)
+ Im

Z(â„“â€²)
Im

Z(â„“â€²â€²)  D = d
&
+ i E
%
âˆ’Re

Z(â„“â€²)
Im

Z(â„“â€²â€²)
+ Im

Z(â„“â€²)
Re

Z(â„“â€²â€²)  D = d
&
= Cov
%
T (â„“â€²)
I
, T (â„“â€²â€²)
I
 D = d
&
+ Cov
%
T (â„“â€²)
Q
, T (â„“â€²â€²)
Q
 D = d
&
+ i

âˆ’Cov
%
T (â„“â€²)
I
, T (â„“â€²â€²)
Q
 D = d
&
+ Cov
%
T (â„“â€²)
Q
, T (â„“â€²â€²)
I
 D = d
&
= N0
2 Re

Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

+ N0
2 Re

Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

+ i
-
N0
2 Im

Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

âˆ’N0
2 Im

Rgg

(â„“â€²â€² âˆ’â„“â€²)Ts
.
= N0 Rgg

(â„“â€² âˆ’â„“â€²â€²)Ts

,
â„“â€², â„“â€²â€² âˆˆ{1, . . . , n},
(28.51)
where the ï¬rst equality follows from the deï¬nition of the covariance between cen-
tered complex random variables (17.17); the second by (28.50); the third by (28.44);
and the last equality by the conjugate-symmetry of the self-similarity function
(Proposition 11.2.2 (iii)).
Conditional on D = d, the complex n-vector Z is thus a proper Gaussian, and its
conditional law is thus fully speciï¬ed by its conditional covariance matrix (Corol-
lary 24.3.8). By (28.51), this conditional covariance matrix does not depend on d,
and we thus conclude that the conditional law of Z conditional on D = d does not
depend on d, i.e., that Z is independent of D.
Part (iii) follows from Part (ii).
28.6
Additional Reading
Proposition 28.2.1 and Proposition 28.5.2 are the starting points of much of the
literature on equalization and on the use of the Viterbi Algorithm for channels
with intersymbol interference (ISI). More on this in Chapter 32. Additional re-
sources include, for example, (Proakis and Salehi, 2007, Chapter 9), (Viterbi and
Omura, 1979, Chapter 4, Section 4.9), and (Barry, Lee, and Messerschmitt, 2004,
Chapter 8).
28.7
Exercises
Exercise 28.1 (A Dispersive Channel). Let the transmitted signal

X(t)

be as in (28.1),
and let the received signal

Y (t)

be given by
Y (t) =

X â‹†h

(t) + N(t),
t âˆˆR,
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W, and
where h is the impulse response of some stable real ï¬lter.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

704
Detecting PAM and QAM Signals in White Gaussian Noise
(i) Show that no measurable rule for guessing D based on

Y (t)

can outperform an
optimal rule for guessing D based on the n inner products
 âˆ
âˆ’âˆ
Y (t)

g â‹†h

(t âˆ’â„“Ts) dt,
â„“âˆˆ{1, . . . , n}.
(ii) Compute their conditional law.
Exercise 28.2 (PAM in Colored Noise). Let the transmitted signal

X(t)

be as in (28.1),
and let the received signal

Y (t)

be given by
Y (t) = X(t) + N(t),
t âˆˆR,
where

N(t)

is a centered, stationary, measurable, Gaussian SP of PSD SNN that can be
whitened with respect to the bandwidth W. Let h be the impulse response of a whitening
ï¬lter for

N(t)

with respect to W.
(i) Show that no measurable rule for guessing D based on

Y (t)

can outperform an
optimal rule for guessing D based on the n inner products
 âˆ
âˆ’âˆ
Y (t)

g â‹†h â‹†~h

(t âˆ’â„“Ts) dt,
â„“âˆˆ{1, . . . , n}.
(ii) Compute their conditional law.
Exercise 28.3 (A Channel with an Echo). Bits D1, . . . , Dk are mapped to real symbols
X1, . . . , Xk using the antipodal mapping, so Xâ„“= 1 âˆ’2Dâ„“, for every â„“âˆˆ{1, . . . , k}. The
transmitted signal

X(t)

is given by X(t) = A 
â„“Xâ„“Ï†(tâˆ’â„“Ts), where Ï† is an integrable
signal that is bandlimited to W Hz and that satisï¬es the orthonormality condition (28.22).
The received signal

Y (t)

is
Y (t) = X(t) + Î±X(t âˆ’Ts) + N(t),
t âˆˆR,
where

N(t)

is WGN of double-sided PSD N0/2 with respect to the bandwidth W, and Î±
is a real constant. Let Yâ„“be the time-(â„“Ts) output of a ï¬lter that is matched to Ï† and
that is fed

Y (t)

.
(i) Is there a loss of optimality in guessing D1, . . . , Dk based on Y1, . . . , Yk+1 instead
of

Y (t)

?
(ii) Consider a suboptimal rule that guesses â€œDj = 0â€ if Yj â‰¥0, and otherwise guesses
â€œDj = 1.â€ Express the probability that this rule guesses Dj incorrectly in terms
of j, Î±, A, and N0. To what does this probability converge as N0 tends to zero?
Exercise 28.4 (Another Channel with an Echo). Consider the setup of Exercise 28.3 but
where the echo is delayed by a noninteger multiple of the baud period. Thus,
Y (t) = X(t) + Î±X(t âˆ’Ï„) + N(t),
t âˆˆR,
where 0 < Ï„ < Ts. Show that no measurable rule for guessing D1, . . . , Dk based on

Y (t)

can outperform an optimal rule for guessing these bits based on the 2k inner products
 âˆ
âˆ’âˆ
Y (t) Ï†(t âˆ’â„“Ts) dt,
 âˆ
âˆ’âˆ
Y (t) Ï†(t âˆ’â„“Ts âˆ’Ï„) dt,
â„“âˆˆ{1, . . . , k}.
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

28.7 Exercises
705
Exercise 28.5 (A Multiple-Access Scenario). Two transmitters communicate with a single
receiver. The receiver observes the signal
Y (t) = A1X1 Ï†1(t) + A2X2 Ï†2(t) + N(t),
t âˆˆR,
where A1, A2 > 0; Ï†1 and Ï†2 are orthonormal integrable signals that are bandlimited
to W Hz; the pair (X1, X2) takes value in the set {(+1, +1), (+1, âˆ’1), (âˆ’1, +1), (âˆ’1, âˆ’1)}
equiprobably; and where

N(t)

is WGN of double-sided PSD N0/2 with respect to the
bandwidth W.
(i) Can you recover (X1, X2) from A1X1Ï†1 + A2X2Ï†2?
(ii) Find an optimal receiver for guessing (X1, X2) based on

Y (t)

.
(iii) Compute the optimal probability of error for guessing (X1, X2) based on

Y (t)

.
(iv) Suppose that a genie informs the receiver of the value of X2.
How should the
receiver then guess X1 based on

Y (t)

and the information provided by the genie?
(v) A receiver guesses â€œX1 = +1â€ if âŸ¨Y, Ï†1âŸ©> 0 and guesses â€œX1 = âˆ’1â€ otherwise. Is
this receiver optimal for guessing X1?
Exercise 28.6 (Two Receiver Antennas). Consider the setup of (28.1). We observe two
signals

Y1(t)

,

Y2(t)

that are given at every epoch t âˆˆR by
Y1(t) =

X â‹†h1

(t) + N1(t),
Y2(t) =

X â‹†h2

(t) + N2(t),
where h1 and h2 are the impulse responses of two real stable ï¬lters, and where the
stochastic processes

N1(t)

and

N2(t)

are independent white Gaussian noise processes
of double-sided PSD N0/2 with respect to the bandwidth W. Show that no measurable
rule for guessing D based on (Y1, Y2) can outperform an optimal rule for guessing D
based on the 2n inner products
 âˆ
âˆ’âˆ
Y1(t)

g â‹†h1

(t âˆ’â„“Ts) dt,
 âˆ
âˆ’âˆ
Y2(t)

g â‹†h2

(t âˆ’â„“Ts) dt,
â„“âˆˆ{1, . . . , n}.
Exercise 28.7 (Bits of Unequal Importance). Consider the setup of Section 28.3 but
where some data bits are more important than others. We therefore wish to minimize the
weighted average
k

j=1
Î±j Pr
$ Ë†Dj Ì¸= Dj
%
,
for some positive Î±1, . . . , Î±k that sum to one.
(i) Is it still optimal to base our guess of D1, . . . , Dk on the inner products in (28.12)?
(ii) Does this criterion lead to a diï¬€erent receiver design than the bit error rate?
Exercise 28.8 (Sandwiching the Probability of a Message Error). In the notation of
Section 28.3, show that
1
k
k

j=1
Pr[ ËœDj Ì¸= Dj] â‰¤max
1â‰¤jâ‰¤k

Pr[ ËœDj Ì¸= Dj]

â‰¤Pr
$ ËœD Ì¸= D
%
â‰¤
k

j=1
Pr[ ËœDj Ì¸= Dj].
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,
www.ebook3000.com

706
Detecting PAM and QAM Signals in White Gaussian Noise
Exercise 28.9 (Sandwiching the Bit Error Rate). In the notation of Section 28.3, show
that
1
k Pr
$ ËœD Ì¸= D
%
â‰¤1
k
k

j=1
Pr[ ËœDj Ì¸= Dj] â‰¤Pr
$ ËœD Ì¸= D
%
.
Exercise 28.10 (Guessing Ïˆ(M)). Show that if T : Rd â†’Rdâ€² forms a suï¬ƒcient statistic
for guessing the message M âˆˆ{1, . . . , M} based on the random d-vector Y, and if Ïˆ(Â·)
is any function of M, then no rule for guessing Ïˆ(M) based on Y can outperform an
optimal rule for guessing Ïˆ(M) based on T(Y).
Hint: Recall Theorem 22.3.5.
Exercise 28.11 (Transmission via an Unknown Dispersive Channel). A random switch
outside our control whose state we cannot observe determines whether our observation Y
is
X â‹†h1 + N
or
X â‹†h2 + N,
where

X(t)

is the transmitted signal of (28.1);

N(t)

is WGN of double-sided PSD N0/2
with respect to the bandwidth W; and h1 & h2 are the impulse responses of two stable
real ï¬lters. Show that no measurable rule for guessing D1, . . . , Dk based on

Y (t)

can
outperform an optimal rule for guessing these bits based on the 2n inner products
 âˆ
âˆ’âˆ
Y (t)

g â‹†h1

(t âˆ’â„“Ts) dt,
 âˆ
âˆ’âˆ
Y (t)

g â‹†h2

(t âˆ’â„“Ts) dt,
â„“âˆˆ{1, . . . , n}.
Exercise 28.12 (The Matched-Filter Bound). Consider the setting of Proposition 28.2.1
with the additional assumptions that the data bits are IID random bits and that the
encoder in (28.5) is the one of (10.2). Show that for every j âˆˆ{1, . . . , k} the probability
of guessing Dj incorrectly based on

Y (t)

is lower-bounded by
Q
â›
â
&
2A2 âˆ¥gâˆ¥2
2
N0
â
â .
Can this bound hold with equality? Would the bound continue to hold if we relaxed the
assumption that the data are IID random bits and only required that Dj âˆ¼U ({0, 1})?
Hint: Knowing the other data bits cannot hurt.
Exercise 28.13 (More Inner Products than Necessary). Consider the setting of Proposi-
tion 28.5.2 but where we deï¬ne T (â„“) not only for â„“âˆˆ{1, . . . , n} but for all â„“âˆˆZ. Deï¬ne
Z(â„“) as in (28.50) but, again, not just for â„“âˆˆ{1, . . . , n} but for all â„“âˆˆZ. Prove that
the bi-inï¬nite sequence . . . , Z(âˆ’1), Z(0), Z(1), . . . forms a circularly-symmetric, stationary,
Gaussian, discrete-time CSP. What is its autocovariance function? What is its PSD?
available at 
.030
15:36:13, subject to the Cambridge Core terms of use,

Chapter 29
Linear Binary Block Codes with Antipodal
Signaling
29.1
Introduction and Setup
We have thus far said very little about the design of good encoders. We mentioned
block encoders but, apart from deï¬ning and studying some of their basic prop-
erties (such as rate and energy per symbol), we have said very little about how
to design such encoders. The design of block encoders falls under the heading of
Coding Theory and is the subject of numerous books such as (MacWilliams and
Sloane, 1977), (van Lint, 1998), (Blahut, 2003), (Roth, 2006) and (Richardson and
Urbanke, 2008). Here we provide only a glimpse of this theory for one class of such
encoders: the class of binary linear block encoders with antipodal pulse amplitude
modulation.
Such encoders map the data bits D1, . . . , DK to the real symbols
X1, . . . , XN by ï¬rst applying a one-to-one linear mapping of binary K-tuples to
binary N-tuples and by then applying the antipodal mapping
0 	â†’+1
1 	â†’âˆ’1
to each component of the binary N-tuple to produce the {Â±1}-valued symbols
X1, . . . , XN.
Our emphasis in this chapter is not on the design of such encoders, but on how
their properties inï¬‚uence the performance of communication systems that employ
them in combination with Pulse Amplitude Modulation. We thus assume that the
transmitted waveform is given by
A

â„“
Xâ„“Ï†(t âˆ’â„“Ts),
t âˆˆR,
(29.1)
where A > 0 is a scaling factor, Ts > 0 is the baud period, Ï†(Â·) is a real integrable
signal that is bandlimited to W Hz, and where the time shifts of Ï†(Â·) by integer
multiples of Ts are orthonormal
 âˆ
âˆ’âˆ
Ï†(t âˆ’â„“Ts) Ï†(t âˆ’â„“â€²Ts) dt = I{â„“= â„“â€²},
â„“, â„“â€² âˆˆZ.
(29.2)
707
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

708
Linear Binary Block Codes with Antipodal Signaling
The summation in (29.1) can be ï¬nite, as in the block-mode that we discussed
in Section 10.4, or inï¬nite, as in the bi-inï¬nite block-mode that we discussed in
Section 14.5.2. We shall further assume that the PAM signal is transmitted over
an additive noise channel where the transmitted signal is corrupted by Gaussian
noise that is white with respect to the bandwidth W. We also assume that the
data are IID random bits (Deï¬nition 14.5.1).
In Section 29.2 we brieï¬‚y discuss the binary ï¬eld F2 and discuss some of the basic
properties of the set of all binary Îº-tuples when it is viewed as a vector space
over this ï¬eld. This allows us in Section 29.3 to deï¬ne linear binary encoders and
codes. Section 29.4 introduces binary encoders with antipodal signaling, and Sec-
tion 29.5 discusses the power and power spectral density when they are employed
in conjunction with PAM. Section 29.6 begins the study of decoding with a dis-
cussion of two performance criteria: the probability of a block error (also called
message error) and the probability of a bit error. It also recalls the discrete-
time single-block channel model (Section 28.4.3). Section 29.7 contains the design
and performance analysis of the guessing rule that minimizes the probability of a
block error, and Section 29.8 contains a similar analysis for the guessing rule that
minimizes the probability of a bit error. Section 29.9 explains why performance
analysis and simulation are often done under the assumption that the transmitted
data are the all-zero data. Section 29.10 discusses how the encoder and the PAM
parameters inï¬‚uence the overall system performance. The chapter concludes with
a discussion of the (suboptimal) Hard Decision decoding rule in Section 29.11 and
of bounds on the minimum distance of a code in Section 29.12.
29.2
The Binary Field F2 and the Vector Space FÎº
2
29.2.1
The Binary Field F2
The binary ï¬eld F2 consists of two elements that we denote by 0 and 1.
An
operation that we denote by âŠ•is deï¬ned between any two elements of F2 through
the relation
0 âŠ•0 = 0,
0 âŠ•1 = 1,
1 âŠ•0 = 1,
1 âŠ•1 = 0.
(29.3)
This operation is sometimes called â€œmod 2 additionâ€ or â€œexclusive-orâ€ or â€œGF(2)
addition.â€ (Here GF(2) stands for the Galois Field of two elements after the French
mathematician Â´Evariste Galois (1811â€“1832) who did ground-breaking work on ï¬nite
ï¬elds and groups.) Another operationâ€”â€œGF(2) multiplicationâ€â€”is denoted by a
dot and is deï¬ned via the relation
0 Â· 0 = 0,
0 Â· 1 = 0,
1 Â· 0 = 0,
1 Â· 1 = 1.
(29.4)
Combined with these operations, the set F2 forms a ï¬eld, which is sometimes
called the Galois Field of size two. We leave it to the reader to verify that the âŠ•
operation satisï¬es
a âŠ•b = b âŠ•a,
a, b âˆˆF2,
(a âŠ•b) âŠ•c = a âŠ•(b âŠ•c),
a, b, c âˆˆF2,
a âŠ•0 = 0 âŠ•a = a,
a âˆˆF2,
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.2 The Binary Field F2 and the Vector Space FÎº
2
709
a âŠ•a = 0,
a âˆˆF2;
and that the operations âŠ•and Â· satisfy the distributive law
(a âŠ•b) Â· c = (a Â· c) âŠ•(b Â· c),
a, b, c âˆˆF2.
29.2.2
The Vector Field FÎº
2
We denote the set of all binary Îº-tuples by FÎº
2 and deï¬ne the componentwise-âŠ•
operation between Îº-tuples u =

u1, . . . , uÎº

âˆˆFÎº
2 and v =

v1, . . . , vÎº

âˆˆFÎº
2 as
u âŠ•v â‰œ

u1 âŠ•v1, . . . , uÎº âŠ•vÎº

,
u, v âˆˆFÎº
2.
(29.5)
We deï¬ne the product between a scalar Î± âˆˆF2 and a Îº-tuple u =

u1, . . . , uÎº

âˆˆFÎº
2
by
Î± Â· u â‰œ

Î± Â· u1, . . . , Î± Â· uÎº

.
(29.6)
With these operations the set FÎº
2 forms a vector space over the ï¬eld F2. The all-zero
Îº-tuple is denoted by 0.
29.2.3
Linear Mappings
A mapping T: FÎº
2 â†’FÎ·
2 is said to be linear if
T(Î± Â· u âŠ•Î² Â· v) = Î± Â· T(u) âŠ•Î² Â· T(v),

Î±, Î² âˆˆF2, u, v âˆˆFÎº
2

.
(29.7)
The kernel of a linear mapping T: FÎº
2 â†’FÎ·
2 is denoted by ker(T) and is the set of
all Îº-tuples in FÎº
2 that are mapped by T(Â·) to the all-zero Î·-tuple 0:
ker(T) =

u âˆˆFÎº
2 : T(u) = 0

.
(29.8)
The kernel of every linear mapping contains the all-zero tuple 0.
The range of T: FÎº
2 â†’FÎ·
2 is denoted by range(T) and consists of those elements
of FÎ·
2 to which some element of FÎº
2 is mapped by T(Â·):
range(T) =

T(u) : u âˆˆFÎº
2

.
(29.9)
The key results from Linear Algebra that we need are summarized in the following
proposition.
Proposition 29.2.1. Let T: FÎº
2 â†’FÎ·
2 be linear.
(i) The kernel of T(Â·) is a linear subspace of FÎº
2.
(ii) The mapping T(Â·) is one-to-one if, and only if, ker(T) = {0}.
(iii) The range of T(Â·) is a linear subspace of FÎ·
2.
(iv) The sum of the dimension of the kernel and the dimension of the range space
is equal to the dimension of the domain:
dim

ker(T)

+ dim

range(T)

= Îº.
(29.10)
(v) If U is a linear subspace of FÎ·
2 of dimension Îº, then there exists a one-to-one
linear mapping from FÎº
2 to FÎ·
2 whose range is U.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

710
Linear Binary Block Codes with Antipodal Signaling
29.2.4
Hamming Distance and Hamming Weight
The Hamming distance dH(u, v) between two binary Îº-tuples u and v is deï¬ned
as the number of components in which they diï¬€er. For example, the Hamming
distance between the tuples (1, 0, 1, 0) and (0, 0, 1, 1) is two. It is easy to prove
that for u, v, w âˆˆFÎº
2:
dH(u, v) â‰¥0 with equality if, and only if, u = v;
(29.11a)
dH(u, v) = dH(v, u);
(29.11b)
dH(u, w) â‰¤dH(u, v) + dH(v, w).
(29.11c)
The Hamming weight wH(u) of a binary Îº-tuple u is deï¬ned as the number of
its nonzero components. Thus,
wH(u) = dH(u, 0),
u âˆˆFÎº
2,
(29.12)
and
dH(u, v) = wH(u âŠ•v),
u, v âˆˆFÎº
2.
(29.13)
29.2.5
The Componentwise Antipodal Mapping
The antipodal mapping Î¥: F2 â†’{âˆ’1, +1} maps the zero element of F2 to the
real number +1 and the unit element of F2 to âˆ’1:
Î¥(0) = +1,
Î¥(1) = âˆ’1.
(29.14)
This rule is not as arbitrary as it may seem. Although one might be somewhat
surprised that we do not map 1 âˆˆF2 to +1, we have our reasons: we prefer the
mapping (29.14) because it maps mod-2 sums to real products. Thus,
Î¥(a âŠ•b) = Î¥(a)Î¥(b),
a, b âˆˆF2,
(29.15)
where the operation on the RHS between Î¥(a) and Î¥(b) is the regular real-numbers
multiplication. This extends by induction to any ï¬nite number of elements of F2:
Î¥

c1 âŠ•c2 âŠ•Â· Â· Â· âŠ•cÎ½

=
Î½
@
â„“=1
Î¥(câ„“),
c1, . . . , cÎ½ âˆˆF2.
(29.16)
The componentwise antipodal mapping Î¥Î· : FÎ·
2 â†’{âˆ’1, +1}Î· maps elements
of FÎ·
2 to elements of {âˆ’1, +1}Î· by applying the mapping (29.14) to each component:
Î¥Î· :

c1, . . . , cÎ·

	â†’

Î¥(c1), . . . , Î¥(cÎ·)

.
(29.17)
For example, Î¥3 maps the triple (0, 0, 1) to (+1, +1, âˆ’1).
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.3 Binary Linear Encoders and Codes
711
29.2.6
Hamming Distance and Euclidean Distance
We next relate the Hamming distance dH(u, v) between any two binary Î·-tuples
u = (u1, . . . , uÎ·) and v = (v1, . . . , vÎ·) to the squared Euclidean distance between
the results of applying the componentwise antipodal mapping Î¥Î· to them. We
argue that
d2
E

Î¥Î·(u), Î¥Î·(v)

= 4 dH(u, v),
u, v âˆˆFÎ·
2,
(29.18)
where dE(Â·, Â·) denotes the Euclidean distance, so
d2
E

Î¥Î·(u), Î¥Î·(v)

=
Î·

Î½=1

Î¥(uÎ½) âˆ’Î¥(vÎ½)
2.
(29.19)
To prove (29.18) it suï¬ƒces to consider the case where Î· = 1, because the Hamming
distance is the sum of the Hamming distances between the respective components,
and likewise for the squared Euclidean distance. To prove this result for Î· = 1 we
note that if the Hamming distance is zero, then u and v are identical and hence so
are Î¥(u) and Î¥(v), so the Euclidean distance between them must be zero. And if
the Hamming distance is one, then u Ì¸= v, and hence Î¥(u) and Î¥(v) are of opposite
sign but of equal unit magnitude, so the squared Euclidean distance between them
is four.
29.3
Binary Linear Encoders and Codes
Deï¬nition 29.3.1 (Linear (K, N) F2 Encoder and Code). Let N and K be positive
integers.
(i) A linear (K, N) F2 encoder is a one-to-one linear mapping from FK
2 to FN
2 .
(ii) A linear (K, N) F2 code is a linear subspace of FN
2 of dimension K.1
In both deï¬nitions N is called the blocklength and K is called the dimension.
For example, the (K, K + 1) systematic single parity check encoder is the
mapping

d1, . . . , dK

	â†’

d1, . . . , dK, d1 âŠ•d2 âŠ•Â· Â· Â· âŠ•dK

.
(29.20)
It appends to the data tuple a single bit that is chosen so that the resulting (K+1)-
tuple be of even Hamming weight. The (K, K+1) single parity check code is the
subset of FK+1
2
consisting of those binary (K + 1)-tuples whose Hamming weight is
even.
Recall that the range of a mapping g: A â†’B is the subset of B comprising those
elements y âˆˆB to which there corresponds some x âˆˆA such that g(x) = y.
1The terminology here is not standard. In the Coding Theory literature a linear (K, N) F2
code is often called a â€œbinary linear [N, K] code.â€
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

712
Linear Binary Block Codes with Antipodal Signaling
Proposition 29.3.2 (F2 Encoders and Codes).
(i) If T: FK
2 â†’FN
2 is a linear (K, N) F2 encoder, then its range is a linear (K, N)
F2 code.
(ii) Every linear (K, N) F2 code is the range of some (nonunique) linear (K, N)
F2 encoder.
Proof. We begin with Part (i). Let T: FK
2 â†’FN
2 be a linear (K, N) F2 encoder.
That its range is a linear subspace of FN
2
follows from Proposition 29.2.1 (iii).
That its dimension must be K follows from Proposition 29.2.1 (iv) (see (29.10))
because the fact that T(Â·) is one-to-one implies, by Proposition 29.2.1 (ii), that
ker(T) = {0} so dim

ker(T)

= 0.
To prove Part (ii) we note that FK
2 is of dimension K and that, by deï¬nition, every
linear (K, N) F2 code is also of dimension K. The result now follows by noting
that there exists a one-to-one linear mapping between any two subspaces of equal
dimensions over the same ï¬eld (Proposition 29.2.1 (v)).
Any linear transformation from a ï¬nite-dimensional space to a ï¬nite-dimensional
space can be represented as matrix multiplication. A linear (K, N) F2 encoder is
no exception. What is perhaps unusual is that coding theorists use row vectors
to denote the data K-tuples and the N-tuples to which they are mapped. They
consequently use matrix multiplication from the left. This tradition is so ingrained
that we shall begrudgingly adopt it.
Deï¬nition 29.3.3 (Matrix Representation of an Encoder). We say that the linear
(K, N) F2 encoder T: FK
2 â†’FN
2 is represented by the generator matrix G if G
is a K Ã— N matrix whose elements are in F2 and
T(d) = dG,
d âˆˆFK
2 .
(29.21)
Note that in the matrix multiplication in (29.21) we use F2 arithmetic, so the Î·-th
component of dG is d(1)Â·g(1,Î·)âŠ•Â· Â· Â·âŠ•d(K)Â·g(K,Î·), where g(Îº,Î·) is the Row-Îº Column-Î·
component of the generator matrix G, and where d(Îº) is the Îº-th component of d.
For example, the (K, K + 1) F2 systematic single parity check encoder (29.20) is
represented by the K Ã— (K + 1) generator matrix
G =
â›
âœ
âœ
âœ
âœ
âœ
â
1
0
0
Â· Â· Â·
0
1
0
1
0
Â· Â· Â·
0
1
0
0
1
Â· Â· Â·
0
1
...
...
...
...
0
1
0
0
0
Â· Â· Â·
1
1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
(29.22)
The generator matrix G in (29.21) is uniquely speciï¬ed by the linear transformation
T(Â·): its Î·-th row is the result of applying T(Â·) to the K-tuple (0, . . . , 0, 1, 0, . . . , 0)
(the K-tuple whose components are all zero except for the Î·-th, which is one).
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.3 Binary Linear Encoders and Codes
713
Moreover, every K Ã— N binary matrix G deï¬nes a linear transformation T(Â·) via
(29.21), but this linear transformation need not be one-to-one. It is one-to-one if,
and only if, the subspace of FN
2 spanned by the rows of G is of dimension K.
Deï¬nition 29.3.4 (Generator Matrix). A matrix G is a generator matrix for a
given linear (K, N) F2 code if G is a binary K Ã— N matrix such that the range of
the mapping d 	â†’dG is the given code.
Note that there may be numerous generator matrices for a given code. For example,
the matrix (29.22) is a generator matrix for the single parity check code.
But
there are others. Indeed, replacing any row of the above matrix by the sum of
that row and another diï¬€erent row results in another generator matrix for this
code. Likewise, swapping two rows of a generator matrix for a code yields another
generator matrix for the code.
Coding theorists like to distinguish between a code property and an encoder
property.
Code properties are properties that are common to all encoders of
the same range. Encoder properties are speciï¬c to an encoder. Examples of code
properties are the blocklength and dimension. We shall soon encounter more. An
example of an encoder property is the property of being systematic:
Deï¬nition 29.3.5 (Systematic Encoder). A linear (K, N) F2 encoder T: FK
2 â†’FN
2
is said to be systematic (or strictly systematic) if, for every K-tuple

d1, . . . , dK

in FK
2 , the ï¬rst K components of T

(d1, . . . , dK)

are equal to d1, . . . , dK.
For example, the encoder (29.20) is systematic. An encoder whose range is the
single-parity check code and which is not systematic is the encoder

d1, . . . , dK

	â†’

d1, d1 âŠ•d2, d2 âŠ•d3, . . . , dKâˆ’1 âŠ•dK, dK

.
(29.23)
The reader is encouraged to verify that if a linear (K, N) F2 encoder T: FK
2 â†’FN
2
is represented by the matrix G, then T(Â·) is systematic if, and only if, the K Ã— K
matrix that results from deleting the last N âˆ’K columns of G is the K Ã— K identity
matrix.
Deï¬nition 29.3.6 (Parity-Check Matrix). A parity-check matrix for a given
linear (K, N) F2 code is a binary matrix H with N columns such that a (row) N-
tuple c is in the code if, and only if, cHT is the all-zero (row) vector.
For example, a parity-check matrix for the (K, K + 1) single-parity check code is
the 1 Ã— (K + 1) matrix
H = (1, 1, . . . , 1).
(Codes typically have numerous diï¬€erent parity-check matrices, but the single-
parity check code is an exception.)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

714
Linear Binary Block Codes with Antipodal Signaling
29.4
Binary Encoders with Antipodal Signaling
Deï¬nition 29.4.1.
(i) We say that a (K, N) binary-to-reals block encoder enc: {0, 1}K â†’RN is a
linear binary (K, N) block encoder with antipodal signaling if
enc(d) = Î¥N

T(d)

,
d âˆˆFK
2 ,
(29.24)
where T: FK
2 â†’FN
2 is a linear (K, N) F2 encoder, and where Î¥N(Â·) is the
componentwise antipodal mapping (29.17). Thus, if (X1, . . . , XN) denotes
the N-tuple produced by enc(Â·) when fed the data K-tuple (D1, . . . , DK), then
XÎ· =

+1
if the Î·-th components of T

(D1, . . . , DK)

is zero,
âˆ’1
otherwise.
(29.25)
(ii) A linear binary (K, N) block code with antipodal signaling is the range
of some linear binary (K, N) block encoder with antipodal signaling.
In analogy to Proposition 29.3.2, the range of every linear binary (K, N) block
encoder with antipodal signaling is a linear binary (K, N) block code with antipodal
signaling.
If enc(Â·) can be represented by the application of T(Â·) to the data K-tuple followed
by the application of the componentwise antipodal mapping Î¥N, then we shall
write
enc = Î¥N â—¦T.
(29.26)
Since Î¥N is invertible, there is a one-to-one correspondence between T and enc.
An important code property is the distribution of the result of applying an encoder
to IID random bits.
Proposition 29.4.2. Let T: FK
2 â†’FN
2 be a linear (K, N) F2 encoder.
(i) Applying T to a K-tuple of IID random bits results in a random N-tuple that
is uniformly distributed over range(T).
(ii) Applying Î¥N â—¦T to IID random bits produces an N-tuple that is uniformly
distributed over the range of range(T) under the componentwise antipodal
mapping Î¥N.
Proof. Part (i) follows from the fact that the mapping T(Â·) is one-to-one. Part (ii)
follows from Part (i) and from the fact that Î¥N(Â·) is one-to-one.
For example, it follows from Proposition 29.4.2 (ii) and from (29.16) that if we
feed IID random bits to any encoder (be it systematic or not) whose range is the
(K, K + 1) single parity check code and then employ the componentwise antipodal
mapping Î¥N(Â·), then the resulting random (K + 1)-tuple (X1, . . . , XK+1) will be
uniformly distributed over the set
1
Î¾1, . . . , Î¾K+1

âˆˆ{âˆ’1, +1}K+1 :
K+1
@
Î·=1
Î¾Î· = +1
2
.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.5 Power and Operational Power Spectral Density
715
Corollary 29.4.3. Any property that is determined by the joint distribution of the
result of applying the encoder to IID random bits is a code property.
Examples of such properties are the power and operational power spectral density,
which are discussed next.
29.5
Power and Operational Power Spectral Density
To discuss the transmitted power and the operational power spectral density we
shall consider bi-inï¬nite block encoding (Section 14.5.2). We shall then use the
results of Section 14.5.2 and Section 15.4.3 to compute the power and operational
PSD of the transmitted signal in this mode.
The impatient reader who is only interested in the transmitted power for pulse
shapes satisfying the orthogonality condition (29.2) can apply the results of Sec-
tion 14.5.3 directly to obtain that, subject to the decay condition (14.46), the
transmitted power P is given by
P = A2
Ts
.
(29.27)
We next extend the discussion to general pulse shapes and to the operational PSD.
To remind the reader that we no longer assume the orthogonality condition (29.2),
we shall now denote the pulse shape by g(Â·) and assume that it is bandlimited
to W Hz and that it satisï¬es the decay condition (14.17). Before proceeding with
the analysis of the power and PSD, we wish to characterize linear binary (K, N)
block encoders with antipodal signaling that map IID random bits to zero-mean
N-tuples. Note that by Corollary 29.4.3 this is, in fact, a code property. Thus, if
enc = Î¥N â—¦T, then the question of whether enc(Â·) maps IID random bits to zero-
mean N-tuples depends only on the range of T. Aiding us in this characterization
is the following lemma on linear functionals. A linear functional on FÎº
2 is a linear
mapping from FÎº
2 to F2. The zero functional maps every Îº-tuple in FÎº
2 to zero.
Lemma 29.5.1. Let L: FK
2 â†’F2 be a linear functional that is not the zero func-
tional. Then the RV X deï¬ned by
X =

+1
if L

(D1, . . . , DK)

= 0,
âˆ’1
if L

(D1, . . . , DK)

= 1
(29.28)
is of zero mean whenever D1, . . . , DK are IID random bits.
Proof. We begin by expressing the expectation of X as
E[X] =

dâˆˆFK
2
Pr[D = d] Î¥

L(d)

= 2âˆ’K 
dâˆˆFK
2
Î¥

L(d)

available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

716
Linear Binary Block Codes with Antipodal Signaling
= 2âˆ’K

dâˆˆFK
2 : L(d)=0
(+1) + 2âˆ’K

dâˆˆFK
2 : L(d)=1
(âˆ’1)
= 2âˆ’K
# Lâˆ’1(0) âˆ’# Lâˆ’1(1)

,
where
Lâˆ’1(0) =

d âˆˆFK
2 : L(d) = 0

is the set of all K-tuples in FK
2 that are mapped by L(Â·) to 0, where Lâˆ’1(1) is anal-
ogously deï¬ned, and where # A denotes the number of elements in the set A. It
follows that to prove that E[X] = 0 it suï¬ƒces to show that if L(Â·) is not determin-
istically zero, then the sets Lâˆ’1(0) and Lâˆ’1(1) have the same number of elements.
We prove this by exhibiting a one-to-one mapping from Lâˆ’1(0) onto Lâˆ’1(1). (If
there is a one-to-one mapping from a ï¬nite set A onto a ï¬nite set B, then A and B
must have the same number of elements.) To exhibit this mapping, note that the
assumption that L(Â·) is not the zero transformation implies that the set Lâˆ’1(1) is
not empty. Let dâˆ—be an element of this set, so
L(dâˆ—) = 1.
(29.29)
The required mapping maps each d0 âˆˆLâˆ’1(0) to d0 âŠ•dâˆ—:
Lâˆ’1(0) âˆ‹d0 	â†’d0 âŠ•dâˆ—.
(29.30)
We next verify that it is a one-to-one mapping from Lâˆ’1(0) onto Lâˆ’1(1). That it is
one-to-one follows because if d0 âŠ•dâˆ—= dâ€²
0 âŠ•dâˆ—then by adding dâˆ—to both sides we
obtain d0 âŠ•dâˆ—âŠ•dâˆ—= dâ€²
0 âŠ•dâˆ—âŠ•dâˆ—, i.e., that d0 = dâ€²
0 (because dâˆ—âŠ•dâˆ—= 0). That
this mapping maps each element of Lâˆ’1(0) to an element of Lâˆ’1(1) follows because,
as we next show, if d0 âˆˆLâˆ’1(0), then L(d0 âŠ•dâˆ—) = 1. Indeed, if d0 âˆˆLâˆ’1(0), then
L(d0) = 0,
(29.31)
and consequently,
L(d0 âŠ•dâˆ—) = L(d0) âŠ•L(dâˆ—)
= 0 âŠ•1
= 1,
where the ï¬rst equality follows from the linearity of L(Â·), and where the second
equality follows from (29.29) and (29.31). That the mapping is onto follows by
noting that if d1 is any element of Lâˆ’1(1), then d1 âŠ•dâˆ—is in Lâˆ’1(0) and it is
mapped by this mapping to d1.
Using this lemma we can show:
Proposition 29.5.2. Let (X1, . . . , XN) be the result of applying a linear binary
(K, N) block encoder with antipodal signaling to a binary K-tuple comprising IID
random bits.
(i) For every Î· âˆˆ{1, . . . , N}, the RV XÎ· is either deterministically equal to +1,
or else of zero mean.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.5 Power and Operational Power Spectral Density
717
(ii) For every Î·, Î·â€² âˆˆ{1, . . . , N}, the random variables XÎ· and XÎ·â€² are either
deterministically equal to each other or else E[XÎ·XÎ·â€²] = 0.
Proof. Let the linear binary (K, N) block encoder with antipodal signaling enc(Â·)
be given by enc = Î¥N â—¦T, where T: FK
2 â†’FN
2 is one-to-one and linear. Let
(X1, . . . , XN) be the result of applying enc to the K-tuple D = (D1, . . . , DK),
where D1, . . . , DK are IID random bits.
To prove Part (i), ï¬x some Î· âˆˆ{1, . . . , N}, and let L(Â·) be the linear functional that
maps d to the Î·-th component of T(d), so XÎ· = Î¥

L(D)

, where D denotes the
row vector comprising the K IID random bits. If L(Â·) maps all data K-tuples to zero,
then XÎ· is deterministically equal to +1. Otherwise, E[XÎ·] = 0 by Lemma 29.5.1.
To prove Part (ii), let the matrix G represent the mapping T(Â·), so XÎ· = Î¥

DG(Â·,Î·)
,
where G(Â·,Î·) denotes the Î·-th column of G. Expressing XÎ·â€² in a similar way, we
obtain from (29.15)
XÎ·XÎ·â€² = Î¥

DG(Â·,Î·)
Î¥

DG(Â·,Î·â€²)
= Î¥

DG(Â·,Î·) âŠ•DG(Â·,Î·â€²)
= Î¥

D

G(Â·,Î·) âŠ•G(Â·,Î·â€²)
.
(29.32)
Consequently, if we deï¬ne the linear functional L: d 	â†’d

G(Â·,Î·) âŠ•G(Â·,Î·â€²)
, then
XÎ·XÎ·â€² = Î¥

L(D)

. This linear functional is the zero functional if the Î·-th column
of G is identical to its Î·â€²-th column, i.e., if XÎ· is deterministically equal to XÎ·â€².
Otherwise, it is not the zero functional, and E[XÎ·XÎ·â€²]

= E

Î¥

L(D)

must be
zero (Lemma 29.5.1).
Proposition 29.5.3 (Producing Zero-Mean Uncorrelated Symbols). A linear bi-
nary (K, N) block encoder with antipodal signaling enc = Î¥N â—¦T produces zero-
mean uncorrelated symbols when fed IID random bits if, and only if, the columns
of the matrix G representing T(Â·) are distinct and neither of these columns is the
all-zero column.
Proof. The Î·-th symbol XÎ· produced by enc = Î¥N â—¦T when fed the K-tuple of
IID random bits D = (D1, . . . , DK) is given by
XÎ· = Î¥

DG(Â·,Î·)
= Î¥

D1 Â· G(1,Î·) âŠ•Â· Â· Â· âŠ•DK Â· G(K,Î·)
where G(Â·,Î·) is the Î·-th column of the K Ã— N generator matrix of T(Â·). Since the
linear functional
d 	â†’d1 Â· G(1,Î·) âŠ•Â· Â· Â· âŠ•dK Â· G(K,Î·)
is the zero functional if, and only if,
G(1,Î·) = Â· Â· Â· = G(K,Î·) = 0,
(29.33)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

718
Linear Binary Block Codes with Antipodal Signaling
it follows that XÎ· is deterministically zero if, and only if, the Î·-th column of G is
zero. From this and Lemma 29.5.1 it follows that all the symbols produced by enc
are of zero mean if, and only if, none of the columns of G is zero.
A similar argument shows that the product XÎ·XÎ·â€², which by (29.32) is given by
Î¥

D

G(Â·,Î·) âŠ•G(Â·,Î·â€²)
,
is deterministically zero if, and only if, the functional
d 	â†’d1 Â· (G(1,Î·) âŠ•G(1,Î·â€²)) âŠ•Â· Â· Â· âŠ•dK Â· (G(K,Î·) âŠ•G(K,Î·â€²))
is zero, i.e., if, and only if, the Î·-th and Î·â€²-th columns of G are equal. Otherwise,
by Lemma 29.5.1, we have E[XÎ·XÎ·â€²] = 0.
Note 29.5.4. By Corollary 29.4.3 the property of producing zero-mean uncorre-
lated symbols is a code property.
Proposition 29.5.5 (Power and PSD). Let the linear binary (K, N) block encoder
with antipodal signaling enc = Î¥N â—¦T produce zero-mean uncorrelated symbols
when fed IID random bits, and let the pulse shape g satisfy the decay condition
(14.17). Then the transmitted power P in bi-inï¬nite block-encoding mode is given
by
P = A2
Ts
âˆ¥gâˆ¥2
2
(29.34)
and the operational PSD is
SXX(f) = A2
Ts
Ë†g(f)
2,
f âˆˆR.
(29.35)
Proof. The expression (29.34) for the power follows either from (14.33) or (14.38).
The expression for the operational PSD follows either from (15.23) or from (15.26).
Engineers rarely check whether an encoder produces uncorrelated symbols when
fed IID random bits. The reason may be that they usually deal with pulse shapes
Ï† satisfying the orthogonality condition (29.2) and the decay condition (14.46).
For such pulse shapes, the power is given by (29.27) without any additional as-
sumptions. Also, by Theorem 15.4.1, the bandwidth of the PAM signal is typically
equal to the bandwidth of the pulse shape. In fact, by that theorem, for linear
binary (K, N) block encoders with antipodal signaling
bandwidth of PAM signal = bandwidth of pulse shape,
(29.36)
whenever A Ì¸= 0; the pulse shape g is a Borel measurable function satisfying the
decay condition (14.17) for some Î±, Î² > 0; and the encoder produces zero-mean
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.6 Performance Criteria
719
symbols when fed IID random bits. Thus, if one is not interested in the exact form
of the operational PSD but only in its support, then one need not check whether
the encoder produces uncorrelated symbols when fed IID random bits.
29.6
Performance Criteria
Designing an optimal decoder for linear binary block encoders with antipodal sig-
naling is conceptually very simple but algorithmically very diï¬ƒcult. The structure
of the decoder depends on what we mean by â€œoptimal.â€ In this chapter we focus
on two notions of optimality: minimizing the probability of a block errorâ€”also
called message errorâ€”and minimizing the probability of a bit error. Referring
to Figure 28.1, we say that a block error occurred in decoding the Î½-th block if
at least one of the data bits

D(Î½âˆ’1)K+1, . . . , D(Î½âˆ’1)K+K

was incorrectly decoded.
We say that a bit error occurred in decoding the j-th bit if Dj was incorrectly
decoded.
We consider the case where IID random bits are transmitted in block-mode, and
the transmitted waveform is corrupted by additive Gaussian noise that is white
with respect to the bandwidth W of the pulse shape. The pulse shape is assumed
to satisfy the orthonormality condition (29.2) and the decay condition (14.17).
From Proposition 28.3.1 it follows that for both optimality criteria, there is no
loss of optimality in feeding the received waveform to a matched ï¬lter for Ï† and
in basing the decision on the ï¬lterâ€™s output sampled at integer multiples of Ts.
Moreover, for the purposes of decoding a given message, it suï¬ƒces to consider only
the samples corresponding to the symbols that were produced when the encoder
encoded the given message (Proposition 28.4.2). Similarly, for decoding a given
data bit it suï¬ƒces to consider only the samples corresponding to the symbols that
were produced when the encoder encoded the message of which the given bit is part.
These observations lead us (as in Section 28.4.3) to the discrete-time single-block
model (28.34). For convenience, we repeat this model here (with the additional
assumption that the data are IID random bits):

X1, . . . , XN

= enc

D1, . . . , DK

;
(29.37a)
YÎ· = AXÎ· + ZÎ·,
Î· âˆˆ{1, . . . , N};
(29.37b)
Z1, . . . , ZN âˆ¼IID N
	
0, N0
2

;
(29.37c)
D1, . . . , DK âˆ¼IID U ({0, 1}) ,
(29.37d)
where (Z1, . . . , ZN) are independent of (D1, . . . , DK).
We also introduce some
additional notation. We use xÎ·(d) for the Î·-th component of the N-tuple to which
the binary K-tuple d is mapped by enc(Â·):
xÎ·(d) â‰œÎ·-th component of enc(d),

Î· âˆˆ{1, . . . , N}, d âˆˆFK
2

.
(29.38)
Denoting the conditional density of (Y1, . . . , YN) given (X1, . . . , XN) by fY|X(Â·),
we have for every y âˆˆRN of components y1, . . . , yN and for every x âˆˆ{âˆ’1, +1}N
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

720
Linear Binary Block Codes with Antipodal Signaling
Parameter
In Section 21.6
In Section 29.7
number of observations
J
N
number of hypotheses
M
2K
set of hypotheses
{1, . . . , M}
FK
2
dummy hypothesis variable
m
d
prior
{Ï€m}
uniform
conditional mean tuple

s(1)
m , . . . , s(J)
m


Ax1(d), . . . , AxN(d)

conditional variance
Ïƒ2
N0/2
Table 29.1: A conversion table for the setups of Section 21.6 and of Section 29.7.
of components x1, . . . , xN
fY|X=x(y) = (Ï€N0)âˆ’N/2
N
@
Î·=1
exp
	
âˆ’(yÎ· âˆ’AxÎ·)2
N0

.
(29.39)
Likewise, for every y âˆˆRN and every data tuple d âˆˆFK
2 ,
fY|D=d(y) = (Ï€N0)âˆ’N/2
N
@
Î·=1
exp
-
âˆ’

yÎ· âˆ’AxÎ·(d)
2
N0
.
.
(29.40)
29.7
Minimizing the Block Error Rate
29.7.1
Optimal Decoding
To minimize the probability of a block error, we need to use the random N-vector
Y = (Y1, . . . , YN) to guess the K-tuple D =

D1, . . . , DK

. This is the type of
problem we addressed in Section 21.6. The translation between the setup of that
section and our current setup is summarized in Table 29.1: the number of observa-
tions, which was there J, is here N; the number of hypotheses, which was there M,
is here 2K; the set of possible messages, which was there M = {1, . . . , M}, is here
the set of binary K-tuples FK
2 ; the dummy variable for a generic message, which
was there m, is here the binary K-tuple d; the prior, which was there {Ï€m}, is
here uniform; the mean tuple corresponding to the m-th message, which was there

s(1)
m , . . . , s(J)
m

is here

Ax1(d), . . . , AxN(d)

(see (29.38)); and the conditional
variance of each observation, which was there Ïƒ2, is here N0/2.
Because all the symbols produced by the encoder take value in {âˆ’1, +1}, it follows
that
N

Î·=1

AxÎ·(d)
2 = A2N,
d âˆˆFK
2 ,
so all the mean tuples are of equal Euclidean norm. From Proposition 21.6.1 (iii)
we thus obtain that, to minimize the probability of a block error, our guess should
be the K-tuple dâˆ—that satisï¬es
N

Î·=1
xÎ·(dâˆ—) YÎ· = max
dâˆˆFK
2
N

Î·=1
xÎ·(d) YÎ·,
(29.41)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.7 Minimizing the Block Error Rate
721
with ties being resolved uniformly at random among the data tuples that achieve
the maximum. Our guess should thus be the data sequence that when fed to the
encoder produces the {Â±1}-valued N-tuple of highest correlation with the observed
tuple Y.
Note that, by deï¬nition, all block encoders are one-to-one mappings
and thus the mean tuples are distinct. Consequently, by Proposition 21.6.2, the
probability that more than one tuple dâˆ—satisï¬es (29.41) is zero.
Since guessing the data tuple is equivalent to guessing the N-tuple to which it is
mapped, we can also describe the optimal decision rule in terms of the encoderâ€™s
output.
Proposition 29.7.1 (The Max-Correlation Decision Rule). Consider the problem
of guessing D based on Y for the setup of Section 29.6.
(i) Picking at random a message from the set

Ëœd âˆˆFK
2 :
N

Î·=1
xÎ·(Ëœd) YÎ· = max
dâˆˆFK
2
N

Î·=1
xÎ·(d) YÎ·
!
(29.42)
minimizes the probability of incorrectly guessing D.
(ii) The probability that the above set contains more than one element is zero.
(iii) For the problem of guessing the encoderâ€™s output, picking at random an N-
tuple from the set

Ëœx âˆˆrange(enc) :
N

Î·=1
ËœxÎ· YÎ· =
max
xâˆˆrange(enc)
N

Î·=1
xÎ· YÎ·
!
(29.43)
minimizes the probability of error. This set contains more than one element
with probability zero.
Conceptually, the problem of ï¬nding an N-tuple that has the highest correlation
with (Y1, . . . , YN) among all the N-tuples in the range of enc(Â·) is very simple: one
goes over the list of all the 2K N-tuples that are in the range of enc(Â·) and picks
the one that has the highest correlation with (Y1, . . . , YN). But algorithmically
this is very diï¬ƒcult because 2K is in most applications a huge number. It is one of
the challenges of Coding Theory to come up with encoders for which the decoding
does not require an exhaustive search over all 2K tuples.
As we shall see, the
single parity check code is an example of such a code. But the performance of this
encoder is, alas, not stellar.
29.7.2
Wagnerâ€™s Rule
For the (K, K + 1) systematic single parity check encoder (29.20), the decoding can
be performed very eï¬ƒciently using a decision algorithm that is called Wagnerâ€™s
Rule in honor of C.A. Wagner. Unlike the brute-force approach that considers all
possible data tuples and which thus has a complexity which is exponential in K,
the complexity of Wagnerâ€™s Rule is linear in K.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

722
Linear Binary Block Codes with Antipodal Signaling
Wagnerâ€™s Rule can be summarized as follows. Consider the (K + 1) tuple
Î¾Î· â‰œ

+1
if YÎ· â‰¥0,
âˆ’1
otherwise,
Î· = 1, . . . , K + 1.
(29.44)
If this tuple has an even number of negative components, then guess that the en-
coderâ€™s output is (Î¾1, . . . , Î¾K+1) and that the data sequence is thus the inverse of
(Î¾1, . . . , Î¾K) under the componentwise antipodal mapping Î¥K, i.e., that the data
tuple is (1 âˆ’Î¾1)/2, . . . , (1 âˆ’Î¾K)/2. Otherwise, ï¬‚ip the sign of Î¾Î·âˆ—corresponding to
the YÎ·âˆ—of smallest magnitude. I.e., guess that the encoderâ€™s output is
Î¾1, . . . , Î¾Î·âˆ—âˆ’1, âˆ’Î¾Î·âˆ—, Î¾Î·âˆ—+1 . . . , Î¾K+1,
(29.45)
and that the data bits are
1 âˆ’Î¾1
2
, . . . , 1 âˆ’Î¾Î·âˆ—âˆ’1
2
, 1 + Î¾Î·âˆ—
2
, 1 âˆ’Î¾Î·âˆ—+1
2
. . . , 1 âˆ’Î¾K
2
,
(29.46)
where Î·âˆ—is the element of {1, . . . , K + 1} satisfying
|YÎ·âˆ—| =
min
1â‰¤Î·â‰¤K+1 |YÎ·|.
(29.47)
Proof that Wagnerâ€™s Rule is Optimal. Recall that the (K, K + 1) single parity
check code with antipodal signaling consists of all Â±1-valued (K + 1)-tuples having
an even number of âˆ’1â€™s. We seek to ï¬nd the tuple that among all such tuples max-
imizes the correlation with the received tuple (Y1, . . . , YK+1). The tuple deï¬ned in
(29.44) is the tuple that among all tuples in {âˆ’1, +1}K+1 has the highest correla-
tion with (Y1, . . . , YK+1). Since ï¬‚ipping the sign of Î¾Î· reduces the correlation by
2|YÎ·|, the tuple (29.45) has the second-highest correlation among all the tuples in
{âˆ’1, +1}K+1. Since the tuples (29.44) and (29.45) diï¬€er in one component, exactly
one of them has an even number of negative components. That tuple thus maxi-
mizes the correlation among all tuples in {âˆ’1, +1}K+1 that have an even number
of negative components and is thus the tuple we are after.
Since the encoder is systematic, the data tuple that generates a given encoder
output is easily found by considering the ï¬rst K components of the encoder output
and by then applying the mapping +1 	â†’0 and âˆ’1 	â†’1, i.e., Î¾ 	â†’(1 âˆ’Î¾)/2.
29.7.3
The Probability of a Block Error
We next address the performance of the detector that we designed in Section 29.7.1
when we sought to minimize the probability of a block error. We continue to assume
that the encoder is a linear binary (K, N) block encoder with antipodal signaling,
so the encoder function enc(Â·) can be written as enc = Î¥N â—¦T where T: FK
2 â†’FN
2
is a linear one-to-one mapping and Î¥N(Â·) is the componentwise antipodal mapping.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.7 Minimizing the Block Error Rate
723
An Upper Bound
It is usually very diï¬ƒcult to precisely evaluate the probability of a block error. A
very useful bound is the Union Bound, which we encountered in Section 21.6.3.
Denoting by pMAP(error|D = d) the probability of error of our guessing rule con-
ditional on the binary K-tuple D = d being fed to the encoder, we can use (21.60),
Table 29.1, and (29.18) to obtain
pMAP(error|D = d) â‰¤

dâ€²âˆˆFK
2 \{d}
Q
â›
â

2A2dH

T(dâ€²), T(d)

N0
â
â .
(29.48)
It is customary to group all the equal terms on the RHS of (29.48) and to write
the bound in the equivalent form
pMAP(error|D = d) â‰¤
N

Î½=1
#
'
dâ€² âˆˆFK
2 : dH

T(dâ€²), T(d)

= Î½
(
Q
â›
â

2A2Î½
N0
â
â ,
(29.49)
where
#
'
dâ€² âˆˆFK
2 : dH

T(dâ€²), T(d)

= Î½
(
(29.50)
is the number of data tuples that are mapped by T(Â·) to a binary N-tuple that
is at Hamming distance Î½ from T(d), and where the sum excludes Î½ = 0 because
the fact that T(Â·) is one-to-one implies that if dâ€² Ì¸= d then the Hamming distance
between T(dâ€²) and T(d) must be at least one.
We next show that the linearity of T(Â·) implies that the RHS of (29.49) does not
depend on d. (In Section 29.9 we show that this is also true of the LHS.) To this
end we show that for every Î½ âˆˆ{1, . . . , N} and for every d âˆˆFK
2 ,
#
'
dâ€² âˆˆFK
2 : dH

T(dâ€²), T(d)

= Î½
(
= #
'
Ëœd âˆˆFK
2 : wH

T(Ëœd)

= Î½
(
(29.51)
= #

c âˆˆrange(T) : wH(c) = Î½

,
(29.52)
where the RHS of (29.51) is the evaluation of the LHS at d = 0. To prove (29.51)
we note that the mapping dâ€² 	â†’dâ€² âŠ•d is a one-to-one mapping from the set whose
cardinality is written on the LHS to the set whose cardinality is written on the
RHS, because

dH

T(dâ€²), T(d)

= Î½

â‡â‡’

wH

T(d) âŠ•T(dâ€²)

= Î½

â‡â‡’

wH

T(d âŠ•dâ€²)

= Î½

,
where the ï¬rst equivalence follows from (29.13), and where the second equivalence
follows from the linearity of T(Â·). To prove (29.52) we merely substitute c for T(Ëœd)
in (29.51) and use the fact that T(Â·) is one-to-one.
Combining (29.49) with (29.52) we obtain the union bound
pMAP(error|D = d) â‰¤
N

Î½=1
#

c âˆˆrange(T) : wH(c) = Î½

Q
â›
â

2A2Î½
N0
â
â . (29.53)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

724
Linear Binary Block Codes with Antipodal Signaling
The list of N + 1 nonnegative integers

#

c âˆˆrange(T) : wH(c) = 0

, . . . , #

c âˆˆrange(T) : wH(c) = N

(whose ï¬rst term is equal to one and whose terms sum to 2K) is called the weight
enumerator of the code.
For example, for the (K, K + 1) single parity check code
#
'
Ëœd âˆˆFK
2 : wH

T(Ëœd)

= Î½
(
=

0
if Î½ is odd,
K+1
Î½

if Î½ is even,
Î½ = 0, . . . , K + 1
because this code consists of all (K + 1)-tuples of even Hamming weight. Conse-
quently, this codeâ€™s weight enumerator is
-
1, 0,
	K + 1
2

, 0,
	K + 1
4

, 0, . . . , 0,
	K + 1
K + 1

.
,
if K is odd;
-
1, 0,
	K + 1
2

, 0,
	K + 1
4

, 0, . . . ,
	K + 1
K

, 0
.
,
if K is even.
The minimum Hamming distance dmin,H of a linear (K, N) F2 code is the
smallest Hamming distance between distinct elements of the code. (If K = 0, i.e.,
if the only codeword is the all-zero codeword, then, by convention, the minimum
distance is said to be inï¬nite.) By (29.52) it follows that (for K > 0) the minimum
Hamming distance of a code is also the smallest weight that a nonzero codeword
can have
dmin,H =
min
câˆˆrange(T)\{0} wH(c).
(29.54)
With this deï¬nition we can rewrite (29.53) as
pMAP(error|D = d) â‰¤
N

Î½=dmin,H
#

c âˆˆrange(T) : wH(c) = Î½

Q
â›
â

2A2Î½
N0
â
â .
(29.55)
Engineers sometimes approximate the RHS of (29.55) by its ï¬rst term:
#

c âˆˆrange(T) : wH(c) = dmin,H

Q
â›
â

2A2dmin,H
N0
â
â .
(29.56)
This is reasonable when A2/N0 â‰«1 because the Q(Â·) function decays very rapidly;
see (19.18).
The term
#

c âˆˆrange(T) : wH(c) = dmin,H

(29.57)
is sometimes called the number of nearest neighbors.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.8 Minimizing the Bit Error Rate
725
A Lower Bound
Using the results of Section 21.6.4, we can obtain a lower bound on the probability
of a block error. Indeed, by (21.66), Table 29.1, (29.18), the monotonicity of Q(Â·),
and the deï¬nition of dmin,H
pMAP(error|D = d) â‰¥Q
â›
â

2A2dmin,H
N0
â
â .
(29.58)
29.8
Minimizing the Bit Error Rate
In some applications we want to minimize the number of data bits that are incor-
rectly decoded. This performance criterion leads to a diï¬€erent guessing rule, which
we derive and analyze in this section.
29.8.1
Optimal Decoding
We next derive the guessing rule that minimizes the average probability of a bit
error, or the Bit Error Rate. Conceptually, this is simple. For each Îº âˆˆ{1, . . . , K}
our guess of the Îº-th data bit DÎº should minimize the probability of error. This
problem falls under the category of binary hypothesis testing, and, since DÎº is a
priori equally likely to be 0 or 1, the Maximum-Likelihood rule of Section 20.8 is
optimal. To compute the likelihood-ratio function, we treat the other data bits
D1, . . . , DÎºâˆ’1, DÎº+1, . . . , DK as unobserved random parameters (Section 20.15.1).
Thus, using (20.101) with the random parameter Î˜ now corresponding to the tuple
(D1, . . . , DÎºâˆ’1, DÎº+1, . . . , DK) we obtain2
fY|DÎº=0(y1, . . . , yN)
= 2âˆ’(Kâˆ’1)

dâˆˆAÎº,0
fY1,...,YN|D=d(y1, . . . , yN)
(29.59)
= 2âˆ’(Kâˆ’1)(Ï€N0)âˆ’N/2

dâˆˆAÎº,0
N
@
Î·=1
exp
-
âˆ’

yÎ· âˆ’AxÎ·(d)
2
N0
.
,
(29.60)
where the set AÎº,0 consists of those tuples in FK
2 whose Îº-th component is zero
AÎº,0 =
'
(d1, . . . , dK) âˆˆFK
2 : dÎº = 0
(
.
(29.61)
Likewise,
fY|DÎº=1(y1, . . . , yN)
= 2âˆ’(Kâˆ’1)

dâˆˆAÎº,1
fY1,...,YN|D=d(y1, . . . , yN)
(29.62)
= 2âˆ’(Kâˆ’1)(Ï€N0)âˆ’N/2

dâˆˆAÎº,1
N
@
Î·=1
exp
-
âˆ’

yÎ· âˆ’AxÎ·(d)
2
N0
.
,
(29.63)
2Our assumption that the data are IID random bits guarantees that the random parameter
Î˜ â‰œ(D1, . . . , DÎºâˆ’1, DÎº+1, . . . , DK) is independent of the RV DÎº that we wish to guess.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

726
Linear Binary Block Codes with Antipodal Signaling
where we similarly deï¬ne
AÎº,1 =
'
(d1, . . . , dK) âˆˆFK
2 : dÎº = 1
(
.
(29.64)
Using Theorem 20.7.1 and (29.60) & (29.63) we obtain the following.
Proposition 29.8.1 (Minimizing the BER). Consider the problem of guessing DÎº
based on Y for the setup of Section 29.6, where Îº âˆˆ{1, . . . , K}. The decision rule
that guesses â€œDÎº = 0â€ if

dâˆˆAÎº,0
N
@
Î·=1
exp
-
âˆ’

yÎ· âˆ’AxÎ·(d)
2
N0
.
>

dâˆˆAÎº,1
N
@
Î·=1
exp
-
âˆ’

yÎ· âˆ’AxÎ·(d)
2
N0
.
;
that guesses â€œDÎº = 1â€ if

dâˆˆAÎº,0
N
@
Î·=1
exp
-
âˆ’

yÎ· âˆ’AxÎ·(d)
2
N0
.
<

dâˆˆAÎº,1
N
@
Î·=1
exp
-
âˆ’

yÎ· âˆ’AxÎ·(d)
2
N0
.
;
and that guesses at random in case of equality minimizes the probability of guessing
the data bit DÎº incorrectly.
The diï¬ƒculty in implementing this decision rule is that, unless we exploit some
algebraic structure, the computation of the sums above has exponential complexity
in K, because the number of terms in each sum is 2Kâˆ’1.
It is interesting to note that, unlike the decision rule that minimizes the probability
of a block error, the above decision rule depends on the value of N0/2.
29.8.2
The Probability of a Bit Error
We next obtain bounds on the probability that the detector of Proposition 29.8.1
errs in guessing the Îº-th data bit DÎº. We denote this probability by pâˆ—
Îº.
An Upper Bound
Since the detector of Proposition 29.8.1 is optimal, the probability that it errs
in decoding the Îº-th data bit DÎº cannot exceed the probability of error of the
suboptimal rule whose guess for DÎº is the Îº-th bit of the message produced by the
detector of Section 29.7. Thus, if Ï†MAP(Â·) denotes the decision rule of Section 29.7,
then
pâˆ—
Îº â‰¤Pr

D âŠ•Ï†MAP(Y) âˆˆAÎº,1

,
Îº âˆˆ{1, . . . , K},
(29.65)
where the set AÎº,1 was deï¬ned in (29.64) as the set of messages whose Îº-th com-
ponent is equal to one, and where Y is the observed N-tuple whose components
are given in (29.37b).
Since the data are IID random bits, we can rewrite (29.65) as
pâˆ—
Îº â‰¤1
2K

dâˆˆFK
2

ËœdâˆˆAÎº,1
Pr

Ï†MAP(Y) = d âŠ•Ëœd
 D = d

,
Îº âˆˆ{1, . . . , K}.
(29.66)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.8 Minimizing the Bit Error Rate
727
Since Ï†MAP(Y) can only equal d âŠ•Ëœd if Y is at least as close in Euclidean distance
to enc(d âŠ•Ëœd) as it is to enc(d), it follows from Lemma 20.14.1, Table 29.1, and
(29.18) that
Pr

Ï†MAP(Y) = d âŠ•Ëœd
 D = d

â‰¤Q
â›
â
AdE

Î¥N

T(d âŠ•Ëœd)

, Î¥N

T(d)

2
3
N0
2
â
â 
= Q
â›
âœ
âœ
â
A
B
B
CA2d2
E

Î¥N

T(d âŠ•Ëœd)

, Î¥N

T(d)

2N0
â
âŸ
âŸ
â 
= Q
â›
â

2A2dH

T(d âŠ•Ëœd), T(d)

N0
â
â 
= Q
â›
â

2A2wH

T(d âŠ•Ëœd) âŠ•T(d)

N0
â
â 
= Q
â›
â

2A2wH

T(Ëœd)

N0
â
â .
(29.67)
It follows from (29.66) and (29.67) upon noting that RHS of (29.67) does not
depend on the transmitted message d that
pâˆ—
Îº â‰¤

ËœdâˆˆAÎº,1
Q
â›
â

2A2wH

T(Ëœd)

N0
â
â ,
Îº âˆˆ{1, . . . , K}.
(29.68)
This bound is sometimes written as
pâˆ—
Îº â‰¤
N

Î½=dmin,H
Î³(Î½, Îº) Q
â›
â

2A2Î½
N0
â
â ,
Îº âˆˆ{1, . . . , K},
(29.69a)
where Î³(Î½, Îº) denotes the number of elements Ëœd of FK
2 whose Îº-th component is
equal to one and for which T(Ëœd) is of Hamming weight Î½, i.e.,
Î³(Î½, Îº) = #
'
Ëœd âˆˆAÎº,1 : wH

T(Ëœd)

= Î½
(
,
(29.69b)
and where the minimum Hamming distance dmin,H is deï¬ned in (29.54).
Sometimes one is more interested in the arithmetic average of pâˆ—
Îº
1
K
K

Îº=1
pâˆ—
Îº,
(29.70)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

728
Linear Binary Block Codes with Antipodal Signaling
which is the optimal bit error rate. We next show that (29.68) leads to the
upper bound
1
K
K

Îº=1
pâˆ—
Îº â‰¤1
K

dâˆˆFK
2
wH(d) Q
â›
â

2A2wH

T(d)

N0
â
â .
(29.71)
This follows from the calculation
K

Îº=1
pâˆ—
Îº â‰¤
K

Îº=1

dâˆˆAÎº,1
Q
â›
â

2A2wH

T(d)

N0
â
â 
=
K

Îº=1

dâˆˆFK
2
Q
â›
â

2A2wH

T(d)

N0
â
â I{d âˆˆAÎº,1}
=

dâˆˆFK
2
Q
â›
â

2A2wH

T(d)

N0
â
â 
K

Îº=1
I{d âˆˆAÎº,1}
=

dâˆˆFK
2
Q
â›
â

2A2wH

T(d)

N0
â
â wH(d),
where the inequality in the ï¬rst line follows from (29.68); the equality in the second
by introducing the indicator function for the set AÎº,1 and extending the summa-
tion; the equality in the third line by changing the order of summation; and the
equality in the last line by noting that every d âˆˆFK
2 is in exactly wH(d) of the sets
A1,1, . . . , AK,1.
A Lower Bound
We next show that, for every Îº âˆˆ{1, . . . , K}, the probability pâˆ—
Îº that the optimal
detector for guessing the Îº-th data bit errs is lower-bounded by
pâˆ—
Îº â‰¥max
dâˆˆAÎº,1 Q
â›
â

2A2wH

T(d)

N0
â
â ,
(29.72)
where AÎº,1 denotes the set of binary K-tuples whose Îº-th component is equal to
one (29.64). To derive (29.72), ï¬x some d âˆˆAÎº,1 and note that for every dâ€² âˆˆFK
2

dâ€² âˆˆAÎº,0

â‡â‡’

dâ€² âŠ•d âˆˆAÎº,1

.
(29.73)
This allows us to express fY|DÎº=1(y) for every y âˆˆRN as
fY|DÎº=1(y) = 2âˆ’(Kâˆ’1)

ËœdâˆˆAÎº,1
fY|D=Ëœd(y)
= 2âˆ’(Kâˆ’1)

dâ€²âˆˆAÎº,0
fY|D=dâŠ•dâ€²(y),
(29.74)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.9 Assuming the All-Zero Codeword
729
where the ï¬rst equality follows from (29.62) and the second from (29.73).
Using the exact expression for the probability of error in binary hypothesis testing
(20.20) we have:
pâˆ—
Îº = 1
2

yâˆˆRN min
'
fY|DÎº=0(y), fY|DÎº=1(y)
(
dy
= 1
2

min
1
2âˆ’(Kâˆ’1)

dâ€²âˆˆAÎº,0
fY|D=dâ€²(y), 2âˆ’(Kâˆ’1)

dâ€²âˆˆAÎº,0
fY|D=dâŠ•dâ€²(y)
2
dy
= 1
2 2âˆ’(Kâˆ’1)

min
1

dâ€²âˆˆAÎº,0
fY|D=dâ€²(y),

dâ€²âˆˆAÎº,0
fY|D=dâŠ•dâ€²(y)
2
dy
â‰¥1
2 2âˆ’(Kâˆ’1)


dâ€²âˆˆAÎº,0
min
'
fY|D=dâ€²(y), fY|D=dâŠ•dâ€²(y)
(
dy
= 2âˆ’(Kâˆ’1)

dâ€²âˆˆAÎº,0
 1
2 min
'
fY|D=dâ€²(y), fY|D=dâŠ•dâ€²(y)
(
dy
= 2âˆ’(Kâˆ’1)

dâ€²âˆˆAÎº,0
Q
â›
â

2A2dH

T(dâ€²), T(dâ€² âŠ•d)

N0
â
â 
= 2âˆ’(Kâˆ’1)

dâ€²âˆˆAÎº,0
Q
â›
â

2A2wH

T(d)

N0
â
â 
= Q
â›
â

2A2wH

T(d)

N0
â
â ,
d âˆˆAÎº,1,
where the ï¬rst line follows from (20.20); the second by the explicit forms (29.59) &
(29.74) of the conditional densities fY|DÎº=0(Â·) and fY|DÎº=1(Â·); the third by pulling
the common term 2âˆ’(Kâˆ’1) outside the minimum; the fourth because the minimum
between two sums with an equal number of terms is lower-bounded by the sum of
the minima between the corresponding terms; the ï¬fth by swapping the summation
and integration; the sixth by Expression (20.20) for the optimal probability of error
for the binary hypothesis testing between D = dâ€² and D = d âŠ•dâ€²; the seventh by
the linearity of T(Â·); and the ï¬nal line because the cardinality of AÎº,0 is 2(Kâˆ’1).
Since the above derivation holds for every d âˆˆAÎº,1, we may choose d to yield the
tightest bound, thus establishing (29.72).
29.9
Assuming the All-Zero Codeword
When simulating linear binary block encoders with antipodal signaling over the
Gaussian channel we rarely simulate the data as IID random bits.
Instead we
assume that the message that is fed to the encoder is the all-zero message and that
the encoderâ€™s output is hence the N-tuple whose components are all +1. In this
section we shall explain why it is correct to do so. More speciï¬cally, we shall show
that pMAP(error|D = d) does not depend on the message d and is thus equal to
pMAP(error|D = 0). We shall also prove an analogous result for the decoder that
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

730
Linear Binary Block Codes with Antipodal Signaling
minimizes the probability of a bit error. The proofs are based on two features of
our setup: the encoder is linear and the Gaussian channel with antipodal inputs is
symmetric in the sense that
fY |X=âˆ’1(y) = fY |X=+1(âˆ’y),
y âˆˆR.
(29.75)
Indeed, by (29.37b),
fY |X=âˆ’1(y) =
1
âˆšÏ€N0
eâˆ’(y+A)2
N0
=
1
âˆšÏ€N0
eâˆ’(âˆ’yâˆ’A)2
N0
= fY |X=+1(âˆ’y),
y âˆˆR.
Deï¬nition 29.9.1 (Memoryless Binary-Input/Output-Symmetric Channel). We
say that the conditional distribution of Y = (Y1, . . . , YN) given X = (X1, . . . , XN)
corresponds to a memoryless binary-input/output-symmetric channel if
fY|X=x(y) =
N
@
Î·=1
fY |X=xÎ·(yÎ·),
x âˆˆ{âˆ’1, +1}N,
(29.76a)
where
fY |X=âˆ’1(y) = fY |X=+1(âˆ’y),
y âˆˆR.
(29.76b)
For every d âˆˆFK
2 deï¬ne the mapping Ïˆd : RN â†’RN as
Ïˆd :

y1, . . . , yN

	â†’

y1x1(d), . . . , yNxN(d)

.
(29.77)
The function Ïˆd(Â·) thus changes the sign of those components of its argument
that correspond to the negative components of enc(d). The key properties of this
mapping are summarized in the following lemma.
Lemma 29.9.2. As in (29.38), let xÎ·(d) denote the result of applying the antipodal
mapping Î¥ to the Î·-th component of T(d), where T: FK
2 â†’FN
2 is some one-to-one
linear mapping. Let the conditional law of (Y1, . . . , YN) given D = d be given by
?N
Î·=1 fY |X=xÎ·(d)(yÎ·), where fY |X(Â·) satisï¬es the symmetry property (29.75). Let
Ïˆd(Â·) be deï¬ned as in (29.77). Then
(i) Ïˆ0(Â·) maps each y âˆˆRN to itself.
(ii) For any d, dâ€² âˆˆFK
2 the composition of Ïˆdâ€² with Ïˆd is given by ÏˆdâŠ•dâ€²:
Ïˆd â—¦Ïˆdâ€² = ÏˆdâŠ•dâ€².
(29.78)
(iii) Ïˆd is equal to its inverse
Ïˆd

Ïˆd(y)

= y,
y âˆˆRN.
(29.79)
(iv) For every d âˆˆFK
2 , the Jacobian of the mapping Ïˆd(Â·) is one.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.9 Assuming the All-Zero Codeword
731
(v) For every d âˆˆFK
2 and every y âˆˆRN,
fY|D=d(y) = fY|D=0

Ïˆd(y)

.
(29.80)
(vi) For any d, dâ€² âˆˆFK
2 and every y âˆˆRN,
fY|D=dâ€²
Ïˆd(y)

= fY|D=dâ€²âŠ•d(y).
(29.81)
Proof. Part (i) follows from the deï¬nition (29.77) because the linearity of T(Â·) and
the deï¬nition of Î¥N guarantee that xÎ·(0) = +1, for all Î· âˆˆ{1, . . . , N}. Part (ii)
follows by linearity and from (29.15):
(Ïˆd â—¦Ïˆdâ€²)(y1, . . . , yN) = Ïˆd

y1x1(dâ€²), . . . , yNxN(dâ€²)

=

y1x1(dâ€²)x1(d), . . . , yNxN(dâ€²)xN(d)

=

y1x1(dâ€² âŠ•d), . . . , yNxN(dâ€² âŠ•d)

= ÏˆdâŠ•dâ€²(y1, . . . , yN),
where in the third equality we used (29.15) and the linearity of the encoder.
Part (iii) follows from Parts (i) and (ii). Part (iv) follows from Part (iii) or di-
rectly by computing the partial derivative matrix and noting that it is diagonal
with the diagonal elements being Â±1 only. Part (v) follows from (29.75). To prove
Part (vi) we substitute dâ€² for d and Ïˆd(y) for y in Part (v) to obtain
fY|D=dâ€²
Ïˆd(y)

= fY|D=0

Ïˆdâ€²
Ïˆd(y)

= fY|D=0

ÏˆdâŠ•dâ€²(y)

= fY|D=dâŠ•dâ€²(y),
where the second equality follows from Part (ii), and where the third equality
follows from Part (v).
With the aid of this lemma we can now justify the all-zero assumption in the
analysis of the probability of a block error. We shall state the result not only for
the Gaussian setup but also for the more general case where the conditional den-
sity fY|X(Â·) corresponds to a memoryless binary-input/output-symmetric channel.
Theorem 29.9.3. Consider the setup of Section 29.6 with the conditional den-
sity fY|X(Â·) corresponding to a memoryless binary-input/output-symmetric chan-
nel. Let pMAP(error|D = d) denote the conditional probability of a block error for
the detector of Proposition 29.7.1, conditional on the data tuple being d. Then,
pMAP(error|D = d) = pMAP(error|D = 0),
d âˆˆFK
2 .
(29.82)
Proof. The proof of this result is not very diï¬ƒcult, but there is a slight technicality
that arises from the way ties are resolved. Since on the Gaussian channel ties occur
with probability zero (Proposition 21.6.2), this issue could be ignored. But we
prefer not to ignore it because we would like the proof to apply also to channels
satisfying (29.76) that are not necessarily Gaussian.
To address ties, we shall
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

732
Linear Binary Block Codes with Antipodal Signaling
assume that they are resolved at random as in Proposition 29.7.1 (i.e., as in the
Deï¬nition 21.3.2 of the MAP rule).
For every d âˆˆFK
2 and every Î½ âˆˆ{1, . . . , 2K}, deï¬ne the set Dd,Î½ âŠ‚RN to contain
those y âˆˆRN for which the following two conditions hold:
fY|D=d(y) = max
dâ€²âˆˆFK
2
fY|D=dâ€²(y),
(29.83a)
#
'
Ëœd âˆˆFK
2 : fY|D=Ëœd(y) = fY|D=d(y)
(
= Î½.
(29.83b)
Whenever y âˆˆDd,Î½, the MAP rule guesses â€œD = dâ€ with probability 1/Î½. Thus,
pMAP(error|D = d) = 1 âˆ’
2K

Î½=1
1
Î½

yâˆˆRN I{y âˆˆDd,Î½} fY|D=d(y) dy.
(29.84)
The key is to note that, by Lemma 29.9.2 (v), for every d âˆˆFK
2 and Î½ âˆˆ{1, . . . , 2K}

y âˆˆDd,Î½

â‡â‡’

Ïˆd(y) âˆˆD0,Î½

.
(29.85)
(Please pause to verify this.) Consequently, by (29.84),
pMAP(error|D = d) = 1 âˆ’
2K

Î½=1
1
Î½

yâˆˆRN I{y âˆˆDd,Î½} fY|D=d(y) dy
= 1 âˆ’
2K

Î½=1
1
Î½

yâˆˆRN I{y âˆˆDd,Î½} fY|D=0

Ïˆd(y)

dy
= 1 âˆ’
2K

Î½=1
1
Î½

ËœyâˆˆRN I{Ïˆd(Ëœy) âˆˆDd,Î½} fY|D=0(Ëœy) dËœy
= 1 âˆ’
2K

Î½=1
1
Î½

ËœyâˆˆRN I{Ëœy âˆˆD0,Î½} fY|D=0(Ëœy) dËœy
= pMAP(error|D = 0),
where the ï¬rst equality follows from (29.84); the second by Lemma 29.9.2 (v); the
third by deï¬ning Ëœy â‰œÏˆd(y) and using Parts (iv) and (iii) of Lemma 29.9.2; the
fourth by (29.85); and the ï¬nal equality by (29.84).
We now formulate a similar result for the detector of Proposition 29.8.1.
Let
pâˆ—
Îº(error|D = d) denote the conditional probability that the decoder of Proposi-
tion 29.8.1 incorrectly decodes the Îº-th data bit, conditional on the tuple d being
fed to the encoder. Since the data are IID random bits,
pâˆ—
Îº = 2âˆ’K 
dâˆˆFK
2
pâˆ—
Îº(error|D = d).
(29.86)
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.9 Assuming the All-Zero Codeword
733
Since ties are resolved at random
pâˆ—
Îº(error|D = d)
= Pr


dâ€²âˆˆAÎº,0
fY|D=dâ€²(Y) >

dâ€²âˆˆAÎº,1
fY|D=dâ€²(Y)
 D = d

+ 1
2 Pr


dâ€²âˆˆAÎº,0
fY|D=dâ€²(Y) =

dâ€²âˆˆAÎº,1
fY|D=dâ€²(Y)
 D = d

, d âˆˆAÎº,1, (29.87)
and
pâˆ—
Îº(error|D = d)
= Pr


dâ€²âˆˆAÎº,0
fY|D=dâ€²(Y) <

dâ€²âˆˆAÎº,1
fY|D=dâ€²(Y)
 D = d

+ 1
2 Pr


dâ€²âˆˆAÎº,0
fY|D=dâ€²(Y) =

dâ€²âˆˆAÎº,1
fY|D=dâ€²(Y)
 D = d

, d âˆˆAÎº,0.(29.88)
Theorem 29.9.4. Under the assumptions of Theorem 29.9.3, we have for every
Îº âˆˆ{1, . . . , K}
pâˆ—
Îº(error|D = d) = pâˆ—
Îº(error|D = 0),
d âˆˆFK
2 ,
(29.89)
and consequently
pâˆ—
Îº = pâˆ—
Îº(error|D = 0).
(29.90)
Proof. It suï¬ƒces to prove (29.89) because (29.90) will then follow by (29.86). To
prove (29.89) we begin by deï¬ning e(d) for d âˆˆFK
2 as follows. If d is in AÎº,1, then
we deï¬ne e(d) as
e(d) â‰œPr


dâ€²âˆˆAÎº,0
fY|D=dâ€²(Y) >

dâ€²âˆˆAÎº,1
fY|D=dâ€²(Y)
 D = d

,
d âˆˆAÎº,1.
Otherwise, if d is in AÎº,0, then we deï¬ne e(d) as
e(d) â‰œPr


dâ€²âˆˆAÎº,0
fY|D=dâ€²(Y) <

dâ€²âˆˆAÎº,1
fY|D=dâ€²(Y)
 D = d

,
d âˆˆAÎº,0.
We shall prove (29.89) for the case where
d âˆˆAÎº,1.
(29.91)
The proof for the case where d âˆˆAÎº,0 is almost identical and is omitted. For d
satisfying (29.91) we shall prove that e(d) does not depend on d. The second term
in (29.87) which accounts for the random resolution of ties can be treated very
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

734
Linear Binary Block Codes with Antipodal Signaling
similarly. To show that e(d) does not depend on d we compute:
e(d)
=

yâˆˆRN I
1

dâ€²âˆˆAÎº,0
fY|D=dâ€²(y) >

dâ€²âˆˆAÎº,1
fY|D=dâ€²(y)
2
fY|D=d(y) dy
=

yâˆˆRN I
1

dâ€²âˆˆAÎº,0
fY|D=dâ€²(y) >

dâ€²âˆˆAÎº,1
fY|D=dâ€²(y)
2
fY|D=0

Ïˆd(y)

dy
=

ËœyâˆˆRN I
1

dâ€²âˆˆAÎº,0
fY|D=dâ€²
Ïˆd(Ëœy)

>

dâ€²âˆˆAÎº,1
fY|D=dâ€²
Ïˆd(Ëœy)
2
fY|D=0(Ëœy) dËœy
=

ËœyâˆˆRN I
1

dâ€²âˆˆAÎº,0
fY|D=dâ€²âŠ•d(Ëœy) >

dâ€²âˆˆAÎº,1
fY|D=dâ€²âŠ•d(Ëœy)
2
fY|D=0(Ëœy) dËœy
=

ËœyâˆˆRN I
1

ËœdâˆˆAÎº,1
fY|D=Ëœd(Ëœy) >

ËœdâˆˆAÎº,0
fY|D=Ëœd(Ëœy)
2
fY|D=0(Ëœy) dËœy
= e(0),
where the second equality follows from Lemma 29.9.2 (v); the third by deï¬ning
the vector Ëœy as Ëœy â‰œÏˆd(y) and by Parts (iv) and (iii) of Lemma 29.9.2; the fourth
by Lemma 29.9.2 (vi); and the ï¬fth equality by deï¬ning Ëœd â‰œd âŠ•dâ€² and using
(29.73).
29.10
System Parameters
We next summarize how the system parameters such as power, bandwidth, and
block error rate are related to the parameters of the encoder. We only address the
case where the pulse shape Ï† satisï¬es the orthonormality condition (29.2). As we
next show, in this case the bandwidth W in Hz of the pulse shape can be expressed
as
W = 1
2 Rb
N
K (1 + excess bandwidth),
(29.92)
where Rb is the bit rate at which the data are fed to the modem in bits per
second, and where the excess bandwidth, which is deï¬ned in Deï¬nition 11.3.6, is
nonnegative. To verify (29.92) note that if the data arrive at the encoder at the
rate of Rb bits per second and if the encoder produces N real symbols for every K
bits that are fed to it, then the encoder produces real symbols at a rate
Rs = N
K Rb
real symbol
second

,
(29.93)
so the baud period must be
Ts = K
N
1
Rb
.
(29.94)
It then follows from Deï¬nition 11.3.6 that the bandwidth of Ï† is given by (29.92)
with the excess bandwidth being nonnegative by Corollary 11.3.5.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.11 Hard vs. Soft Decisions
735
As to the transmitted power P, by (29.27) and (29.94) it is given by
P = Eb Rb,
(29.95)
where Eb denotes the energy per data bit and is given by
Eb = N
K A2.
(29.96)
It is customary to describe the error probability by which one measures performance
as a function of the energy-per-bit Eb.3 Thus, for example, one typically writes the
upper bound (29.55) on the probability of a block error using (29.96) as
pMAP(error|D = d)
â‰¤
N

Î½=dmin,H
#

c âˆˆrange(T) : wH(c) = Î½

Q
â›
â

2Eb(K/N)Î½
N0
â
â .
(29.97)
29.11
Hard vs. Soft Decisions
In Section 29.7 we derived the decision rule that minimizes the probability of a block
error. We saw that, in general, its complexity is exponential in the dimension K of
the code because a brute-force implementation of this rule requires correlating the
N-tuple Y with each of the 2K tuples in range(enc). For the single parity check
rule we found a much simpler implementation of this rule, but for general codes
the decoding problem can be very diï¬ƒcult.
A suboptimal decoding rule that is sometimes implemented is the Hard Decision
decoding rule, which has two steps. In the ï¬rst step one uses the observed real-
valued N-tuple (Y1, . . . , YN) to form the binary tuple ( Ë†C1, . . . , Ë†CN) according to
the rule
Ë†CÎ· =

0
if YÎ· â‰¥0,
1
if YÎ· < 0,
Î· = 1, . . . , N,
and in the second step one searches for the message d for which T(d) is closest in
Hamming distance to ( Ë†C1, . . . , Ë†CN). The advantage of this decoding rule is that
the ï¬rst step is very simple and that the second step can be often performed very
eï¬ƒciently if the code has a strong algebraic structure.
29.12
The Varshamov and Singleton Bounds
Motivated by the approximation (29.56) and by (29.58), a fair bit of eï¬€ort in
Coding Theory has been invested in ï¬nding (K, N) codes that have a large minimum
Hamming weight and reasonable decoding complexity. One of the key existence
3The terms â€œenergy-per-bit,â€ â€œenergy-per-data-bit,â€ and â€œenergy-per-information-bitâ€ are
used interchangeably.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

736
Linear Binary Block Codes with Antipodal Signaling
results in this area is the Varshamov Bound. We state here a special case of this
bound pertaining to our binary setting.
Theorem 29.12.1 (The Varshamov Bound). Let K and N be positive integers,
and let d be an integer in the range 2 â‰¤d â‰¤N âˆ’K + 1. If
dâˆ’2

Î½=0
	N âˆ’1
Î½

< 2Nâˆ’K,
(29.98)
then there exists a linear (K, N) F2 code whose minimum distance dmin,H satisï¬es
dmin,H â‰¥d.
Proof. See, for example, (MacWilliams and Sloane, 1977, Chapter 1, Section 10,
Theorem 12) or (Blahut, 2003, Chapter 12, Section 3, Theorem 12.3.3).
A key upper bound on dmin,H is given by the Singleton Bound.
Theorem 29.12.2 (The Singleton Bound). If N and K are positive integers, then
the minimum Hamming distance dmin,H of any linear (K, N) F2 code must satisfy
dmin,H â‰¤N âˆ’K + 1.
(29.99)
Proof. See, for example, (Blahut, 2003, Chapter 3, Section 3, Theorem 3.2.6) or
(van Lint, 1998, Chapter 5, Section 2, Corollary 5.2.2) or Exercise 29.11.
29.13
Additional Reading
We have only had a glimpse of Coding Theory.
A good starting point for the
literature on Algebraic Coding Theory is (Roth, 2006). For more on the modern
coding techniques such as low-density parity-check codes (LDPC) and turbo-codes,
see (Richardson and Urbanke, 2008) and (Ryan and Lin, 2009).
For more bounds on the error probabilities, see (Sason and Shamai, 2006).
The degradation resulting from hard decisions is addressed, e.g., in (Viterbi and
Omura, 1979, Chapter 3, Section 3.4).
The results of Section 29.9 can be extended also to nonbinary codes with other
mappings. See, for example, (Loeliger, 1991) and (Forney, 1991).
For some of the literature on the minimum distance and its asymptotic behavior
in the block length, see, for example, (Roth, 2006, Chapter 4)
For more on the decoding complexity see the notes on Section 2.4 in Chapter 2 of
(Roth, 2006).
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.14 Exercises
737
29.14
Exercises
Exercise 29.1 (Orthogonality of Signals). Recall that, given a binary K-tuple d âˆˆFK
2
and a linear (K, N) F2 encoder T(Â·), we use xÎ·(d) to denote the result of applying the
antipodal mapping Î¥(Â·) to the Î·-th component of T(d). Let the pulse shape Ï† be such
that its time shifts by integer multiples of the baud period Ts are orthonormal. Show that
5
t â†’
N

Î·=1
xÎ·(d) Ï†(t âˆ’Î·Ts), t â†’
N

Î·=1
xÎ·(dâ€²) Ï†(t âˆ’Î·Ts)
6
= 0,
if, and only if, dH

T(d), T(dâ€²)

= N/2.
Exercise 29.2 (Permuting the Components of a Linear Code). Suppose we swap the ï¬rst
and second components of each of the N-tuples of a given linear (K, N) F2 code. Is the
resulting collection of N-tuples a linear (K, N) F2 code?
Exercise 29.3 (How Many Encoders Does a Code Have?). Let the linear (K, N) F2
encoder T: FK
2 â†’FN
2 be represented by the K Ã— N matrix G. Show that any linear (K, N)
F2 encoder whose image is equal to the image of T can be written in the form
d â†’dAG,
where A is a K Ã— K invertible matrix whose entries are in F2. How many such matrices A
are there?
Exercise 29.4 (The (4,7) Hamming Code). A systematic encoder for the linear (4, 7) F2
Hamming code maps the four data bits d1, d2, d3, d4 to the 7-tuple

d1, d2, d3, d4, d1 âŠ•d3 âŠ•d4, d1 âŠ•d2 âŠ•d4, d2 âŠ•d3 âŠ•d4

.
Suppose that this encoder is used in conjunction with the componentwise antipodal map-
ping Î¥7(Â·) over the white Gaussian noise channel with PAM of pulse shape whose time
shifts by integer multiples of the baud period are orthonormal.
(i) Write out the 16 binary codewords and compute the codeâ€™s weight enumerator.
(ii) Assuming that the codewords are equally likely and that the decoding minimizes the
probability of a message error, use the Union Bound to upper-bound the probability
of codeword error. Express your bound using the transmitted energy per bit Eb.
(iii) Find a lower bound on the probability that the ï¬rst bit D1 is incorrectly decoded.
Express your bound in terms of the energy per bit. Compare with the exact ex-
pression in uncoded communication.
Exercise 29.5 (The Repetition Code). Consider the linear (1, N) F2 repetition code
consisting of the all-zero and all-one N-tuples (0, . . . , 0) and (1, . . . , 1).
(i) Find its weight enumerator.
(ii) Find an optimal decoder for a system employing this code with the componentwise
antipodal mapping Î¥N(Â·) over the white Gaussian noise channel in conjunction
with PAM with a pulse shape whose times shifts by integer multiples of the baud
period are orthonormal.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

738
Linear Binary Block Codes with Antipodal Signaling
(iii) Find the optimal probability of error. Express your answer using the energy per
bit Eb. Compare with uncoded antipodal signaling.
(iv) Describe the hard decision rule for this setup. Find its performance in terms of Eb.
Exercise 29.6 (The Dual Code). We say that two binary Îº-tuples u = (u1, . . . , uÎº) and
v = (v1, . . . , vÎº) are orthogonal if
u1 Â· v1 âŠ•u2 Â· v2 âŠ•Â· Â· Â· âŠ•uÎº Â· vÎº = 0.
(Beware! A nonzero Îº-tuple may be orthogonal to itself.) Consider the set of all N-tuples
that are orthogonal to every codeword of some given linear (K, N) F2 code. Show that
this set is a linear (N âˆ’K, N) F2 code. This code is called the dual code. What is the
dual code of the (K, K + 1) single parity check code?
Exercise 29.7 (Hadamard Code). For a positive integer N which is a power of two, deï¬ne
the N Ã— N binary matrix HN recursively as
H2 =
0
0
0
1

,
HN =
HN/2
HN/2
HN/2
Â¯HN/2

,
N = 4, 8, 16, . . . ,
(29.100)
where Â¯H denotes the componentwise negation of the matrix H, that is, the matrix whose
Row-j Column-â„“element is given by 1âŠ•[H]j,â„“, where [H]j,â„“is the Row-j Column-â„“element
of H. Consider the set of all rows of HN.
(i) Show that this collection of N binary N-tuples forms a linear (log2 N, N) F2 code.
This code is called the Hadamard code. Find this codeâ€™s weight enumerator.
(ii) Suppose that, as in Section 29.6, this code is used in conjunction with PAM over
the white Gaussian noise channel and that Y1, . . . , YN are as deï¬ned there. Show
that the following rule minimizes the probability of a message error: compute the
vector
ËœHN
â›
âœ
âœ
âœ
â
Y1
Y2
...
YN
â
âŸ
âŸ
âŸ
â 
(29.101)
and guess that the m-th message was sent if the m-th component of this vector is
largest. Here ËœHN is the N Ã— N matrix whose Row-j Column-â„“entry is the result of
applying Î¥(Â·) to the Row-j Column-â„“entry of HN.
(iii) A brute-force computation of the vector in (29.101) requires N2 additions, which
translates to N2/ log2 N additions per information bit. Use the structure of HN that
is given in (29.100) to show that this can be done with N log2 N additions (or N
additions per information bit).
Hint: For Part (iii) provide an algorithm for which c(N) = 2c(N/2) + N, where c(n)
denotes the number of additions needed to compute this vector when the matrix is n Ã— n.
Show that the solution to this recursion for c(2) = 2 is c(n) = n log2 n.
Exercise 29.8 (Bi-Orthogonal Code). Referring to the notation introduced in Exer-
cise 29.7, consider the 2N Ã— N matrix
HN
Â¯HN

,
where N is some positive power of two.
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,

29.14 Exercises
739
(i) Show that the rows of this matrix form a linear

log2(2N), N

F2 code.
(ii) Compute the codeâ€™s weight enumerator.
(iii) Explain why we chose the title â€œBi-Orthogonal Codeâ€ for this exercise.
(iv) Find an eï¬ƒcient decoding algorithm for the setup of Section 29.6.
Exercise 29.9 (Non-IID Data). How would you modify the decision rule of Section 29.8 if
the data bits (D1, . . . , DK) are not necessarily IID but have the general joint probability
mass function PD(Â·)?
Exercise 29.10 (Asymmetric Channels). Show that Theorem 29.9.3 will no longer hold if
we drop the hypothesis that the channel is symmetric.
Exercise 29.11 (A Proof of the Singleton Bound). Use the following steps to prove the
Singleton Bound.
(i) Consider a linear (K, N) F2 code. Let Ï€ : FN
2 â†’FKâˆ’1
2
map each N-tuple to the
(K âˆ’1)-tuple consisting of its ï¬rst K âˆ’1 components. By comparing the number of
codewords with the cardinality of the range of Ï€, argue that there must exist two
codewords whose ï¬rst K âˆ’1 components are identical.
(ii) Show that these two codewords are at Hamming distance of at most N âˆ’K + 1.
(iii) Show that the minimum Hamming distance of the code is at most N âˆ’K + 1.
(iv) Does linearity play a role in the proof?
Exercise 29.12 (Binary MDS Codes). Codes that satisfy the Singleton Bound with equal-
ity are called Maximum Distance Separable (MDS). Show that the linear (K, K + 1) F2
single parity check code is MDS. Can you think of other binary MDS codes?
Exercise 29.13 (Existence via the Varshamov Bound). Can the existence of a linear (4, 7)
F2 code of minimum Hamming distance 3 be deduced from the Varshamov Bound?
available at 
.031
15:00:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 30
The Radar Problem
The radar problem is to guess whether an observed waveform is noise or a signal
corrupted by noise.
In its simplest form, the signalâ€”which corresponds to the
reï¬‚ection from a targetâ€”is deterministic. In the more general setting of a moving
target at an unknown distance or velocity, some of the signalâ€™s parameters (e.g.,
delay or phase) are either unknown or random.
Unlike the hypothesis-testing problem that we encountered in Chapter 20, here
there is no prior. Consequently, it is meaningless to discuss the probability of er-
ror, and a new optimality criterion must be introduced. Typically one wishes to
minimize the probability of missed-detection (guessing â€œno targetâ€ in its presence)
subject to a constraint on the maximal probability of false alarm (guessing â€œtar-
getâ€ when none is present). More generally, one studies the trade-oï¬€between the
probability of false alarm and the probability of missed-detection.
There are many other scenarios where one needs to guess in the absence of a prior,
e.g., in guessing whether a drug is helpful against some ailment or in guessing
whether there is a housing bubble. Consequently, although we shall refer to our
problem as â€œthe radar problem,â€ we shall pose it in greater generality.
The radar problem is closely related to the Knapsack Problem in Computer
Science. This relation is explored in Section 30.2.
Readers who prefer to work on their jigsaw puzzle after peeking at the picture on
the box shouldâ€”as recommendedâ€”glance at Section 30.11 (without the proofs and
referring to Deï¬nition 30.5.1 if they must) after reading about the setup and the
connection with the Knapsack Problem (Sections 30.1â€“30.2) and before proceeding
to Section 30.3. Others can read in the order in which the results are derived.
30.1
The Setup
Two probability density functions f0(Â·) and f1(Â·) on the d-dimensional Euclidean
space Rd are given. A random vector Y is drawn according to one of them, and
our task is to guess according to which. We refer to Y as the observation and to
the space Rd, which we denote by Y, as the observation space. We refer to the
hypothesis that Y was drawn according to f0(Â·) as the null hypothesis. In the
radar problem this corresponds to the hypothesis that no target is present and that
740
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.1 The Setup
741
we are therefore observing only noise; the other hypothesisâ€”the alternativeâ€”
corresponds to the hypothesis that a target is present.
A (deterministic) guessing rule is speciï¬ed by a (Borel measurable) mapping
Ï†Guess : Y â†’{0, 1}
(30.1)
as follows. Having observed that Y = yobs, our guess is determined by Ï†Guess(yobs):
if Ï†Guess(yobs) is 0, then we guess that Y was drawn according to f0(Â·), and oth-
erwise we guess that it was drawn according to f1(Â·). We refer to guessing that Y
was drawn according to f0(Â·) as guessing â€œH = 0,â€ and to guessing that it was
drawn according to f1(Â·) as guessing â€œH = 1.â€ The mapping Ï†Guess(Â·)â€”and hence
also the guessâ€”can also be speciï¬ed in terms of the subset D âŠ†Y that it maps to
zero
D =

y âˆˆY : Ï†Guess(y) = 0

.
(30.2)
The event that we guess that Y was drawn according to f1(Â·) when it was in fact
drawn according to f0(Â·) is called a false alarm, because it corresponds to the
event that we erroneously guess that a target is present. The probability pFA of
this event is
pFA =

{yâˆˆY : Ï†Guess(y)=1}
f0(y) dy
(30.3a)
=

Dc f0(y) dy
(30.3b)
=

Y
f0(y) I{y âˆˆDc} dy,
(30.3c)
where in the second equality we have used (30.2).
The other error event of guessing that Y was drawn according to f0(Â·) when it was
in fact drawn according to f1(Â·) is called a missed-detection, because it corre-
sponds to the event that we fail to detect that a target is present. Its probability
pMD is
pMD =

{yâˆˆY : Ï†Guess(y)=0}
f1(y) dy
(30.4a)
=

D
f1(y) dy
(30.4b)
=

Y
f1(y) I{y âˆˆD} dy.
(30.4c)
Rather than specifying the missed-detection probability, it is more common in the
radar community to specify the probability of detection pD, which is deï¬ned as
pD = 1 âˆ’pMD.
(30.5)
This probability can be expressed as
pD =

Y
f1(y) I{y âˆˆDc} dy.
(30.6)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

742
The Radar Problem
We would like the probability of detection to be high and the probability of false
alarm to be small, but there is a tension between these objectives. This tension is
best seen by comparing (30.6) with (30.3c): for pD to be large, the set Dc should
be â€œlargeâ€ so that the integral in (30.6) be large; but for pFA to be small, the same
set Dc should be â€œsmallâ€ so that the integral in (30.3c) be small. Stated diï¬€erently,
by including an inï¬nitesimal region around y in Dc we increase pD by f1(y) dy,
but at the cost of increasing pFA by f0(y) dy. The radar problem is to study the
trade-oï¬€between pD and pFA and to identify the guessing rules that achieve the
best trade-oï¬€.
We say that a pair (pFA, pMD) is achievable if there exists a decision rule of these
probabilities of false alarm and missed-detection. The set of all achievable pairs is
denoted R. We say that a pair (pâˆ—
FA, pâˆ—
MD) is Pareto-optimal if it is achievable
and there does not exist a â€œbetterâ€ achievable pair. More precisely, (pâˆ—
FA, pâˆ—
MD)
is Pareto-optimal if it is achievable and there does not exist an achievable pair
(pâ€²
FA, pâ€²
MD) satisfying both pâ€²
FA < pâˆ—
FA and pâ€²
MD â‰¤pâˆ—
MD nor an achievable pair
(pâ€²
FA, pâ€²
MD) satisfying both pâ€²
FA â‰¤pâˆ—
FA and pâ€²
MD < pâˆ—
MD. Thus, (pâˆ—
FA, pâˆ—
MD) is Pareto-
optimal if it is achievable and if for every other achievable pair (pâ€²
FA, pâ€²
MD)

pâ€²
FA â‰¤pâˆ—
FA and pâ€²
MD â‰¤pâˆ—
MD

=â‡’

pâ€²
FA = pâˆ—
FA and pâ€²
MD = pâˆ—
MD

.
(30.7)
The radar problem is to identify the Pareto-optimal pairs and the guessing rules
that achieve them.
Examples of various achievable regions (shadowed) and their corresponding Pareto-
optimal pairs (solid lines) can be found in Figure 30.1. In (b) the support of f0 is
strictly contained in that of f1, soâ€”by guessing â€œH = 0â€ whenever the observation
falls in the support set of f0â€” we can achieve zero pFA with pMD strictly smaller
than 1. In (c) the support of f1 is strictly contained in that of f0, soâ€”by guessing
â€œH = 1â€ whenever the observation falls in the support set of f1â€” we can achieve
zero pMD with pFA strictly smaller than 1. In (d) the support sets are disjoint. In
this case all probability pairs (pFA, pMD) are achievable, and the sole Pareto-optimal
pair is (0, 0).
In the radar literature the problem is often formulated slightly diï¬€erently. For a
given maximal tolerated false-alarm probability Ï€FA, we seek the highest probabil-
ity of detection pâˆ—
D(Ï€FA). Thus,1
pâˆ—
D(Ï€FA) = max

Y
f1(y) I{y âˆˆDc} dy,
(30.8a)
where the maximization is over all (Borel measurable) subsets D for which

Y
f0(y) I{y âˆˆDc} dy â‰¤Ï€FA.
(30.8b)
1Strictly speaking, it is prima facie not clear that the supremum is achieved, and it would
have been more appropriate to replace the maximum with a supremum. However, the maximum
is achieved: see Lemma 30.4.1 and Proposition 30.5.5 ahead.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.1 The Setup
743
1
1
1
1
1
1
1
1
pâˆ—
MD(0)
pâˆ—
FA(0)
pMD
pMD
pMD
pMD
pFA
pFA
pFA
pFA
(a)
(b)
(c)
(d)
Figure 30.1:
Some achievable regions and their corresponding Pareto-optimal
pairs. In (b) the support of f0(Â·) is contained in that of f1(Â·). In (c) the sup-
port of f1(Â·) is contained in that of f0(Â·). And in (d) the supports are disjoint.
Alternatively, for a given maximal tolerated false-alarm probability Ï€FA, we seek
the smallest probability of missed-detection pâˆ—
MD(Ï€FA):
pâˆ—
MD(Ï€FA) = min

Y
f1(y) I{y âˆˆD} dy = 1 âˆ’pâˆ—
D(Ï€FA),
(30.9)
where the minimization is over sets D as above. The receiver operating char-
acteristic (ROC) is a plot of the function Ï€FA 	â†’pâˆ—
D(Ï€FA).
Notice the inequality sign in (30.8b). Usually, pâˆ—
D(Ï€FA) and pâˆ—
MD(Ï€FA) are achieved
by some D for which (30.8b) holds with equality, but this is not a requirement. It is
because of this inequality sign that the pair (Ï€FA, pâˆ—
MD(Ï€FA)) is not always Pareto-
optimal: the deï¬nition of pâˆ—
MD(Ï€FA) precludes any achievable pair (pâ€²
FA, pâ€²
MD) with
pâ€²
FA â‰¤Ï€FA and pâ€²
MD smaller than pâˆ—
MD(Ï€FA), but it does not preclude a pair
(pâ€²
FA, pâ€²
MD) with pâ€²
MD equal to pâˆ—
MD(Ï€FA) and pâ€²
FA strictly smaller than Ï€FA. In
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

744
The Radar Problem
1
1
1
1
pâˆ—
MD(0)
pâˆ—
MD(0)
pâˆ—
FA(0)
pâˆ—
FA(0)
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.2: A generic achievable region. In (a) the solid line depicts the Pareto-
optimal pairs. In (b) the dashed line is the graph of pâˆ—
MD(Â·).
other words, (Ï€FA, pâˆ—
MD(Ï€FA)) need not be Pareto-optimal because it is conceiv-
able that the same missed-detection probability could be achievable with a smaller
false-alarm probability.
The reverse, however, does hold: as we next argue,

(pâˆ—
FA, pâˆ—
MD) is Pareto-optimal

=â‡’

pâˆ—
MD(pâˆ—
FA) = pâˆ—
MD

.
(30.10)
To see this, assume that (pâˆ—
FA, pâˆ—
MD) is Pareto-optimal.
In this case (pâˆ—
FA, pâˆ—
MD)
is a fortiori achievable, and pâˆ—
MD(pâˆ—
FA) cannot be larger than pâˆ—
MD because the
rule that achieves (pâˆ—
FA, pâˆ—
MD) is in the feasible set of the minimization (30.9) that
deï¬nes pâˆ—
MD(pâˆ—
FA). It cannot be smaller than pâˆ—
MD because if it were, then the rule
that achieves pâˆ—
MD(pâˆ—
FA) would have false-alarm and missed-detection probabilities
that are â€œbetterâ€ than (pâˆ—
FA, pâˆ—
MD) in contradiction to the Pareto-optimality of
(pâˆ—
FA, pâˆ—
MD).
The relationship between Pareto-optimality and the function pFA 	â†’pâˆ—
MD(pFA) is
illustrated in Figure 30.2. The shaded region corresponds to pairs (pFA, pMD) that
are achievable. The solid line corresponds to those achievable pairs that are also
Pareto-optimal. And the dashed line is the graph of the function pFA 	â†’pâˆ—
MD(pFA).
In analogy to (30.9), we deï¬ne pâˆ—
FA(Ï€MD) as the least false-alarm probability that
is achievable by guessing rules whose missed-detection probability does not exceed
Ï€MD:
pâˆ—
FA(Ï€MD) = min

Y
f0(y) I{y âˆˆDc} dy,
(30.11a)
where the minimization is over all (Borel measurable) sets D satisfying

Y
f1(y) I{y âˆˆD} dy â‰¤Ï€MD.
(30.11b)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.1 The Setup
745
As in Section 20.6, we can extend the class of decision rules to allow randomization.
A randomized decision rule is speciï¬ed by a mapping b: Y â†’[0, 1] with the
understanding that if we observe yobs then we guess â€œH = 0â€ with probability
b(yobs) and we guess â€œH = 1â€ with probability 1 âˆ’b(yobs); cf. Figure 20.1 on
Page 404. For such a rule the probability of false alarm pFA(b) is
pFA(b) =

Y

1 âˆ’b(y)

f0(y) dy;
(30.12)
the probability of missed-detection pMD(b) is
pMD(b) =

Y
b(y) f1(y) dy,
(30.13)
and the probability of detection is
pD(b) = 1 âˆ’pMD(b) =

Y

1 âˆ’b(y)

f1(y) dy.
(30.14)
The set of all pairs that are achievable by some randomized guessing rule is de-
noted Rrnd. Since every deterministic rule that guesses â€œH = 0â€ when y âˆˆD can
be viewed as a randomized rule with b(y) = I{y âˆˆD},
R âŠ†Rrnd.
(30.15)
A pair (pâˆ—
FA, pâˆ—
MD) is said to be Pareto-optimal with respect to the class
of randomized guessing rules if it is achievable by some randomized guessing
rule and if (30.7) holds for all pairs (pâ€²
FA, pâ€²
MD) that are achievable with a possibly-
randomized guessing rule. A guessing rule is said to be Pareto-optimal with respect
to the class of randomized guessing rules if the error probabilities (pFA, pMD) asso-
ciated with it are Pareto-optimal with respect to the class of randomized guessing
rules.
In analogy to pâˆ—
MD(Ï€FA), we deï¬ne pâˆ—
MD,rnd(Ï€FA) as the minimal2 probability of
missed-detection that can be achieved by a randomized rule whose probability of
false alarm does not exceed Ï€FA:
pâˆ—
MD,rnd(Ï€FA) =
min
b: pFA(b)â‰¤Ï€FA pMD(b).
(30.16)
Using arguments similar to those justifying (30.10) we conclude:
Proposition 30.1.1. If (pâˆ—
FA, pâˆ—
MD) is Pareto-optimal with respect to the class of
randomized guessing rules, then pâˆ—
MD,rnd(pâˆ—
FA) = pâˆ—
MD.
Since every pair (pFA, pMD) that is achievable with a deterministic guessing rule is
also achievable with a randomized decision rule (30.15),
pâˆ—
MD,rnd(Ï€FA) â‰¤pâˆ—
MD(Ï€FA).
(30.17)
Later we shall see (Corollary 30.5.6) that, because the observation has a density
under both hypotheses, this inequality is in fact an equality.
2Again, prima facie the minimum need not exist, and we should replace the minimum
in (30.16) with an inï¬mum.
However, it turns out that the inï¬mum can be achieved:
see
Lemma 30.4.1 and Lemma 30.5.4.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

746
The Radar Problem
30.2
The Radar and the Knapsack Problems
Among all guessing rules whose false-alarm probability does not exceed Ï€FA, which
one has the highest probability of detection? For some intuition, let us think about
each y in Y as an object of weight f0(y) and of monetary value f1(y). (The set of
objects is uncountable, but let us ignore this.) Rather than focusing on the set D,
which deï¬nes the guessing rule via (30.2), let us focus on its complement Dc, which
also deï¬nes the guessing rule. And let us think of Dc as a bag of objects. The
problem of choosing Dcâ€”and hence a guessing ruleâ€”is then the problem of picking
which objects to place in the bag. The total weight of the bag is the sum of the
weights of the objects in the bag and, as such, corresponds to the RHS of (30.3c).
The weight of the bag is thus the probability of false alarm of the rule it deï¬nes.
Similarly, the total monetary value of the bag corresponds to the RHS of (30.6)
and is thus the probability of detection of the rule it deï¬nes. Viewed this way,
our problem is similar to the one encountered by a robber who breaks into a home
containing many objects of various weights and values. The robber must decide
which objects to put in the bag with the goal of maximizing the bagâ€™s value subject
to the maximal weight the robber can carry.
A natural approach is the greedy one, according to which the robber ï¬rst places
in the bag the object whose value-to-weight ratio is highest (diamond?), followed
by the one whose value-to-weight ratio is the second highest, etc. The robber thus
continues to place objects in the bag in descending order of value-to-weight ratio
until the bag reaches the maximal weight the robber can carry. Stated diï¬€erently,
in the greedy approach the robber judiciously chooses a threshold to which the
robber compares the value-to-weight ratios of the diï¬€erent objects: any object
having a higher ratio is placed in the bag, and any object of a lower ratio is not.
While perfectly reasonable, in some scenarios this approach is suboptimal. For
example, suppose there are three objects in the home, the ï¬rst of which weighs 6
kilograms and is worth $1, 200, whereas each of the other two weighs 5 kilograms
and is worth $900. If the maximal weight the robber can carry is 10 kilograms, then
the greedy approach is suboptimal: after placing in the bag the object of highest
value-to-weight ratio (the ï¬rst) the bag is worth $1, 200 and weighs 6 kilograms;
nothing more will ï¬t in the bag, and the robberâ€™s loot is $1, 200. The robber could
do better by forgoing the ï¬rst object and placing the other two objects in the bag
with a combined value of $1, 800.
Of course, an even better approach is to place the ï¬rst object in the bag and to
then place 80% of the second (whatever this may mean). This would result in a bag
of weight 6+0.8Ã—5 (= 10) kilograms and of value $1, 200+0.8Ã—$900 (= $1, 920).
In the Computer Science literature the robberâ€™s problem is called the Knapsack
Problem. If the robber can take a fraction of each object, the problem is called
the Fractional Knapsack Problem; otherwise it is called the 0-1 Knapsack
Problem.3
The Fractional Knapsack Problem can indeed be solved using the
greedy approach; the 0-1 Knapsack Problem is harder and is often solved using
Dynamic Programming techniques.
3Computer scientists also assume that the number of objects is ï¬nite and that the weights
and values are positive integers.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.3 Pareto-Optimality and Linear Functionals
747
The radar problem with deterministic decision rules is reminiscent of the 0-1 Knap-
sack Problem, because for each â€œobjectâ€ (y) we must decide whether to place it
in the bag (Dc) or not (i.e., place it in D). When randomized rules are allowed
the problem is reminiscent of the Fractional Knapsack Problem, because specifying
some b(y) is tantamount to contributing

1 âˆ’b(y)

f0(y) to the weight of the bag
(cf. (30.12)) and (1 âˆ’b(y)) f1(y) to its value (cf. (30.14)).
Nevertheless, as we shall see, in the radar problem the greedy approach is optimal
even when we do not allow randomization. This is because we have assumed a
continuum of objects and densities for the observation (under both hypotheses).
The greedy approach is optimal even if we do not assume densities, provided we
allow for randomization. Either way, randomization makes the solution a bit more
elegant in allowing us to describe the guessing rule in terms of the likelihood-ratio
function, LR: Y â†’[0, âˆ], which we encountered in (20.38)â€“(20.39):
LR(y) = f0(y)
f1(y),
(30.18a)
with the convention
Î±
0 = âˆ,
Î± > 0

and
0
0 = 1.
(30.18b)
30.3
Pareto-Optimality and Linear Functionals
Some of the properties of Pareto-optimal pairs have less to do with the radar prob-
lem and more with the fact that if (pâˆ—
FA, pâˆ—
MD) is Pareto-optimal then it is achievable
and no other achievable pair can be â€œbetterâ€ than it. For example, there cannot
be two diï¬€erent Pareto-optimal pairs of equal false-alarm probabilities, because
otherwise the pair of smaller missed-detection probability would be â€œbetterâ€ than
the other and thus contradict the latterâ€™s Pareto-optimality. Hence:
Note 30.3.1. Two Pareto-optimal pairs of equal false-alarm probabilities must also
have equal missed-detection probabilities.
Another such property is related to the minimization of linear functionals with
positive coeï¬ƒcients. If (pâ€²
FA, pâ€²
MD) is â€œbetterâ€ than (pFA, pMD), and if Î± and Î² are
both positive, then Î± pâ€²
FA + Î² pâ€²
MD must be smaller than Î± pFA + Î² pMD. Turn-
ing this around, if Î± and Î² are positive and for no achievable pair (pâ€²
FA, pâ€²
MD) is
Î± pâ€²
FA +Î² pâ€²
MD smaller than Î± pFA +Î² pMD, then no achievable pair is â€œbetterâ€ than
(pFA, pMD). Thus, if (pâˆ—
FA, pâˆ—
MD) minimizes Î± pFA + Î² pMD among all achievable
pairs (pFA, pMD), then (pâˆ—
FA, pâˆ—
MD) must be Pareto-optimal. We have thus proved:
Proposition 30.3.2 (Minimizers of Positive Functionals Are Pareto-Optimal).
(i) If, among all guessing rules, a given guessing rule minimizes Î± pFA + Î² pMD
for some positive Î± and Î², then this ruleâ€™s probabilities of false alarm and
missed-detection are Pareto-optimal.
(ii) If, among all randomized guessing rules, a given guessing rule minimizes
Î± pFA +Î² pMD for some positive Î± and Î², then this ruleâ€™s probabilities of false
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

748
The Radar Problem
alarm and missed-detection are Pareto-optimal with respect to the class of
randomized guessing rules.
Proof. It is probably easier to prove this on oneâ€™s own than to follow someone
elseâ€™s proof. But we provide a proof nonetheless. We begin with the deterministic
case, i.e., with Part (i). Let (pFA, pMD) be the probabilities of error associated with
the given guessing rule. This partâ€™s hypothesis guarantees that if (pâ€²
FA, pâ€²
MD) are
the probabilities of error associated with any other guessing rule, then
Î± pFA + Î² pMD â‰¤Î± pâ€²
FA + Î² pâ€²
MD.
(30.19)
Equivalently,
Î±(pâ€²
FA âˆ’pFA) + Î²(pâ€²
MD âˆ’pMD) â‰¥0.
(30.20)
Consequently, if pâ€²
FA â‰¤pFA then pâ€²
MD cannot be smaller than pMD because in
this case the ï¬rst term on the LHS of (30.20) is nonpositive, so for (30.20) to
hold Î²(pâ€²
MD âˆ’pMD) cannot be negative and hence, since Î² is strictly positive,
(pâ€²
MD âˆ’pMD) cannot be negative. Similarly, if pâ€²
MD â‰¤pMD then pâ€²
FA cannot be
smaller than pFA because in this case the second term on the LHS of (30.20) is
nonpositive, so for (30.20) to hold Î±(pâ€²
FA âˆ’pFA) cannot be negative and hence,
since Î± is strictly positive, (pâ€²
FA âˆ’pFA) cannot be negative. This concludes the
proof of Part (i).
The proof of Part (ii) is nearly identical.
Simply replace â€œguessing ruleâ€ with
â€œpossibly-randomized guessing rule.â€
30.4
One Type of Error Is Not Allowed
Suppose no false alarms are allowed. What is then the least probability of missed-
detection pâˆ—
MD(0) we can achieve? In this section we compute this probability and
provide a guessing rule that achieves it. The analogous problem when no missed-
detections are allowed and we seek the least probability of false alarm pâˆ—
FA(0) has
a similar solution.
At ï¬rst sight one might erroneously think that to preclude false alarms we must
never guess â€œH = 1,â€ i.e., that we must choose Dc to be the empty set. But this is
not always the case: it suï¬ƒces that we only guess â€œH = 1â€ when f0(yobs) is zero,
i.e., that Dc only contain observations yobs for which f0(yobs) is zero. Indeed,

Dc âŠ†

y âˆˆY : f0(y) = 0

=â‡’

pFA = 0

,
(30.21)
because if the LHS of (30.21) holds, then the integrand in (30.3b) is zero over
the range of integration, and pFA is zero. To minimize the probability of missed-
detection, we choose Dc to be the largest we can while satisfying the condition on
the LHS of (30.21). Thus, we choose
Dc =

y âˆˆY : f0(y) = 0

(30.22)
with the result that
pâˆ—
MD(0) =

{yâˆˆY : f0(y)>0}
f1(y) dy.
(30.23)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.4 One Type of Error Is Not Allowed
749
For example, in the one-dimensional case with the densities f0(y) = I{0 â‰¤y â‰¤1}
and f1(y) = I{0.25 â‰¤y â‰¤1.25} a rule that achieves pâˆ—
MD(0) is to guess â€œH = 0â€
whenever yobs is in the interval [0, 1], i.e., to choose D to be the interval [0, 1] with
the result that pâˆ—
MD(0) = 0.75.
The optimality of (30.22) is almost self-evident, but not quite. The slight technical
diï¬ƒculty is that the LHS of (30.21) is suï¬ƒcient to guarantee no false alarms, but
it is not necessary. Are there other decision rules that also guarantee zero false
alarms? As we shall see in the next lemma, there are, but they diï¬€er from (30.22)
only on sets of Lebesgue measure zero, which do not inï¬‚uence the probability of
missed-detection. The lemma also shows that randomized guessing rules are no
better for this problem than the best deterministic ones.
Lemma 30.4.1 (Optimal Rule with No False Alarms). Among all randomized
guessing rules whose probability of false alarm is zero, the deterministic guessing
rule (30.22) minimizes the probability of missed-detection and yields the missed-
detection probability in (30.23). Thus,
pâˆ—
MD(0) = pâˆ—
MD,rnd(0) =

{yâˆˆY : f0(y)>0}
f1(y) dy.
(30.24)
Moreover,

0, pâˆ—
MD(0)

is Pareto-optimal
(30.25)
both with respect to the class of deterministic rules and of randomized rules.
Proof. Consider any randomized decision rule that is determined by a function
b: Y â†’[0, 1] and whose false-alarm probability is zero, i.e., (see (30.12))

Y

1 âˆ’b(y)

f0(y) dy = 0.
(30.26)
The integral of a nonnegative function can be zero only if the set over which the
function is positive is of Lebesgue measure zero (Proposition 2.5.3 (i)).
Thus,
(30.26) implies that the set

y âˆˆY : 1 âˆ’b(y) > 0 and f0(y) > 0

(30.27)
must be of Lebesgue measure zero. Consequently, the integral of any functionâ€”
and in particular the function y 	â†’

1 âˆ’b(y)

f1(y)â€”over this setâ€™s complement is
identical to the integral over all Y, and

Y

1 âˆ’b(y)

f1(y) dy =

{yâˆˆY : 1 âˆ’b(y) = 0 or f0(y) = 0}

1 âˆ’b(y)

f1(y) dy
=

{yâˆˆY : 1 âˆ’b(y) > 0 and f0(y) = 0}

1 âˆ’b(y)

f1(y) dy
â‰¤

{yâˆˆY : f0(y)=0}

1 âˆ’b(y)

f1(y) dy
â‰¤

{yâˆˆY : f0(y)=0}
f1(y) dy,
(30.28)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

750
The Radar Problem
where the second line holds because when 1 âˆ’b(y) is zero so is the integrand;
the third line holds because the integrand is nonnegative, so inï¬‚ating the set over
which we integrate cannot decrease the integral; and the fourth line follows by
upper-bounding 1 âˆ’b(y) by 1.
Since the LHS of (30.28) is the probability of
detection of the rule deï¬ned by b(Â·), if we subtract both sides of (30.28) from 1 we
obtain
pMD(b) â‰¥1 âˆ’

{yâˆˆY : f0(y)=0}
f1(y) dy
=

{yâˆˆY : f0(y)>0}
f1(y) dy.
(30.29)
Since this inequality holds for every randomized rule b(Â·) whose false-alarm prob-
ability is zero, it establishes the optimality of the guessing rule (30.22) whose
false-alarm probability is zero and whose missed-detection probability is the RHS
of (30.29).
The Pareto-optimality of

0, pâˆ—
MD(0)

is almost self-evident. To prove it for deter-
ministic rules using (30.7), we consider any achievable pair (pâ€²
FA, pâ€²
MD) satisfying
pâ€²
FA â‰¤0 and pâ€²
MD â‰¤pâˆ—
MD(0) and prove that both inequalities must hold with equal-
ity. Clearly pâ€²
FA â‰¤0 implies equality, i.e., pâ€²
FA = 0, because probabilities are non-
negative. And the achievability of (pâ€²
FA, pâ€²
MD)â€”which by the above is (0, pâ€²
MD)â€”
implies (by the deï¬nition of pâˆ—
MD(0) as the lowest missed-detection probability that
can be achieved by a deterministic guessing rule of zero false-alarm probability)
that pâ€²
MD â‰¥pâˆ—
MD(0), which, in combination with the assumption pâ€²
MD â‰¤pâˆ—
MD(0),
implies equality.
The proof for randomized rules is almost identical: simply replace â€œdeterministicâ€
with â€œrandomizedâ€ and pâˆ—
MD(0) with pâˆ—
MD,rnd(0) (which, by (30.24), is identical to
it).
We can analyze the case where no missed-detections are allowed in a similar way.
In this case it is optimal to guess â€œH = 0â€ whenever f1(yobs) is zero. This leads to
D =

y âˆˆY : f1(y) = 0

(30.30)
with the result that
pâˆ—
FA(0) =

{yâˆˆY : f1(y)>0}
f0(y) dy.
(30.31)
And like Lemma 30.4.1 we have:
Lemma 30.4.2 (Optimal Rule with No Missed-Detections). Among all random-
ized guessing rules whose probability of missed-detection is zero, the deterministic
guessing rule (30.30) minimizes the probability of false alarm and yields the false-
alarm probability in (30.31). Thus,
pâˆ—
FA(0) = pâˆ—
FA,rnd(0) =

{yâˆˆY : f1(y)>0}
f0(y) dy.
(30.32)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.5 Likelihood-Ratio Tests
751
Moreover,

pâˆ—
FA(0), 0

is Pareto-optimal
(30.33)
both with respect to the class of deterministic rules and of randomized rules.
Note 30.4.3. In view of Lemma 30.4.1, we shall no longer distinguish between
pâˆ—
MD(0) and pâˆ—
MD,rnd(0). Likewise, in view of Lemma 30.4.2, we shall no longer
distinguish between pâˆ—
FA(0) and pâˆ—
FA,rnd(0).
The achievability of the pair (pâˆ—
FA(0), 0) using a deterministic guessing rule shows
that pâˆ—
MD(Ï€FA) is zero whenever Ï€FA â‰¥pâˆ—
FA(0).
And since pâˆ—
MD,rnd(Ï€FA) never
exceeds pâˆ—
MD(Ï€FA), we conclude:
Corollary 30.4.4. If Ï€FA is larger than pâˆ—
FA(0), then both pâˆ—
MD(Ï€FA) and pâˆ—
MD,rnd(Ï€FA)
are zero:
pâˆ—
MD(Ï€FA) = pâˆ—
MD,rnd(Ï€FA) = 0,
Ï€FA â‰¥pâˆ—
FA(0).
(30.34)
Corollary 30.4.5. There are no Pareto-optimal pairs (pâˆ—
FA, pâˆ—
MD) (be it with respect
to deterministic or randomized rules) with pâˆ—
FA larger than pâˆ—
FA(0).
Proof. Let (pâˆ—
FA, pâˆ—
MD) be Pareto-optimal.
Then the achievability of (pâˆ—
FA(0), 0)
using a deterministic guessing rule implies that if both pâˆ—
FA(0) â‰¤pâˆ—
FA and 0 â‰¤pâˆ—
MD
then both must hold with equality (cf. (30.7)). But the inequality 0 â‰¤pâˆ—
MD always
holds. We thus conclude that if the inequality pâˆ—
FA(0) â‰¤pâˆ—
FA holds, then it must
hold with equality. Thus pâˆ—
FA cannot exceed pâˆ—
FA(0).
30.5
Likelihood-Ratio Tests
A key role in the radar problem is played by likelihood-ratio tests, which we deï¬ne
next.
Deï¬nition 30.5.1 (Likelihood-Ratio Test). We say that a deterministic guessing
rule Ï†Guess(Â·) speciï¬ed by the set D is a likelihood-ratio test with the threshold
0 â‰¤Î¥ â‰¤âˆif it guesses â€œH = 0â€ whenever LR(yobs) is strictly larger than Î¥, and
it guesses â€œH = 1â€ whenever LR(yobs) is strictly smaller than Î¥, i.e, if

LR(y) > Î¥

=â‡’

y âˆˆD

(30.35a)

LR(y) < Î¥

=â‡’

y /âˆˆD

.
(30.35b)
(There are no restrictions on Ï†Guess(yobs) when LR(yobs) is equal to Î¥.)
A similar deï¬nition holds for randomized guessing rules with (30.35) replaced by

LR(y) > Î¥

=â‡’

b(y) = 1

(30.36a)

LR(y) < Î¥

=â‡’

b(y) = 0

.
(30.36b)
Although there is no prior in the radar problem, from every radar problem and
every Î± âˆˆ(0, 1) we can construct a binary hypothesis-testing problem with a
nondegenerate prior (of the kind we encountered in Chapter 20) by setting
Pr[H = 0] = Î±,
Pr[H = 1] = 1 âˆ’Î±,
(30.37a)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

752
The Radar Problem
and by setting for every y âˆˆY
fY|H=0(y) = f0(y),
fY|H=1(y) = f1(y).
(30.37b)
Any guessing rule for the radar problem can also be used for the hypothesis-
testing problem (30.37) and vice versa.
If Ï†Guess(Â·) is a guessing rule that on
the hypothesis-testing problem yields the error probabilities p(error|H = 0) and
p(error|H = 1), then on the radar problem it will yield
pMD = p(error|H = 1),
pFA = p(error|H = 0).
(30.37c)
As we saw in Chapter 20 (see Theorem 20.5.2 and the discussion of randomized
decision rules in Section 20.6), if the prior is nondegenerate, then
Pr[H = 0] p(error|H = 0) + Pr[H = 1] p(error|H = 1)
is minimized by a guessing rule that compares the likelihood-ratio function LR(yobs)
to the ratio Pr[H = 1]/ Pr[H = 0] and guesses H accordingly. This rule minimizes
the probability of error irrespective of how it resolves ties. In view of (30.37a), the
ratio Pr[H = 1]/ Pr[H = 0] can be expressed as (1 âˆ’Î±)/Î±. We can also turn this
around: a likelihood-ratio test of threshold Î¥ âˆˆ(0, âˆ) minimizes
Î± p(error|H = 0) + (1 âˆ’Î±) p(error|H = 1)
whenever Î± is such that
1 âˆ’Î±
Î±
= Î¥.
(30.38)
Solving (30.38) for Î± in terms of Î¥, we conclude that among all guessing rules
(deterministic or randomized) any likelihood-ratio test of threshold Î¥ âˆˆ(0, âˆ)
minimizes
1
Î¥ + 1 p(error|H = 0) +
Î¥
Î¥ + 1 p(error|H = 1).
Stated in terms of the radar problem using (30.37c): irrespective of how it resolves
ties, a likelihood-ratio test of threshold 0 < Î¥ < âˆminimizes
1
Î¥ + 1 pFA +
Î¥
Î¥ + 1 pMD
(30.39)
among all randomized guessing rules. It thus follows from Proposition 30.3.2 (ii)
that likelihood-ratio tests are Pareto-optimal. This result, which we state next,
is sometimes called the Neyman-Pearson Lemma after the statisticians Jerzy
Neyman (1894â€“1981) and Egon Pearson (1895â€“1980).
Proposition 30.5.2 (Likelihood-Ratio Tests Are Pareto-Optimal). Irrespective of
how it resolves ties, any likelihood-ratio test of ï¬nite positive threshold Î¥ minimizes
1
Î¥ + 1 pFA +
Î¥
Î¥ + 1 pMD
(30.40)
among all randomized guessing rules and is thus Pareto-optimal with respect to the
class of randomized guessing rules.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.5 Likelihood-Ratio Tests
753
1
1
1
1
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.3: The ï¬gure on the left depicts a generic achievable region and lines of
slope âˆ’1/Î¥. The lines that do not contain achievable pairs are dashed, and those
that do are solid. In this example there is a unique achievable pair that minimizes
1
Î¥+1 pFA +
Î¥
Î¥+1 pMD among all achievable pairs. It is Pareto-optimal and marked
by a bullet. The line through it is thick solid. In the ï¬gure on the right the point
marked by a bullet is Pareto-optimal, and there is a unique value of Î¥ for which
it minimizes
1
Î¥+1 pFA +
Î¥
Î¥+1 pMD among all achievable pairs. But for this value of
Î¥ it is not the unique minimizer: all the (Pareto-optimal) points that lie on the
solid line segment going through it are also minimizers. For no value of Î¥ is the
point marked by a bullet the unique minimizer. Incidentally, the point marked by
a square is Pareto-optimal, and it is the unique minimizer of
1
Î¥+1 pFA +
Î¥
Î¥+1 pMD
for a whole range of values of Î¥.
To gain some geometric insight into the minimization of (30.40), recall that given
any number c, the loci of pairs (pFA, pMD) satisfying
1
Î¥ + 1 pFA +
Î¥
Î¥ + 1 pMD = c
is a straight line of slope âˆ’1/Î¥. Depending on the value of c, the line may or
may not contain pairs (pFA, pMD) that are achievable. For example, if c is zero,
then the line contains an achievable pair only if we can simultaneously achieve
zero false alarms and zero missed-detections. For larger values of c, the line may
contain achievable pairs even if (0, 0) is not achievable. Figure 30.3 (a) depicts an
achievable region and some such lines. The lines that do not contain achievable
pairs are dashed, and those that do are solid. Of all the lines that contain achievable
pairs, the one whose value of c is smallest is drawn thick. This line intersects (in
this example) the achievable region at a single point, which is marked by a bullet
and at which the line is tangent to the curve of Pareto-optimal pairs. But, as
Figure 30.3 (b) shows, the minimizing line may intersect the achievable region at
more than one point. Moreover, the curve of Pareto-optimal pairs may contain
â€œcornersâ€ at which the tangent is undeï¬ned.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

754
The Radar Problem
Having shown that every likelihood-ratio test of ï¬nite positive threshold yields a
Pareto-optimal pair, we now ask the reverse question: is every Pareto-optimal pair
(pâˆ—
FA, pâˆ—
MD) achievable by some likelihood-ratio test of some ï¬nite threshold? As
we shall seeâ€”in the range 0 < pâˆ—
FA < pâˆ—
FA(0)â€”the answer is â€œyes.â€ To prove this
requires a bit more than just showing that every Pareto-optimal pair minimizes
some functional of the form Î± pFA + Î² pMD with Î±, Î² > 0: it is possible that a pair
that we wish to achieve minimizes Î± pFA + Î² pMD but that another undesired pair
also minimizes it. In this case we need to show that a likelihood-ratio test exists
that achieves the desired and not the undesired pair. This will require a judicious
choice of the mechanism for resolving ties.
Figure 30.3 (b) depicts a situation
where there is a unique value of Î¥ for which
1
Î¥+1 pFA +
Î¥
Î¥+1 pMD is minimized at
the (desired) point marked by a bullet, but for this value of Î¥ all the points on
the thick line segment containing the bullet are also minimizers. Incidentally, this
ï¬gure also shows that a given point (in this example the one marked by a square)
may be the minimizer of (30.40) for a whole range of values of Î¥.
To overcome these diï¬ƒculties, we shall adopt a somewhat less geometric and more
algebraic approach.
To this end we ï¬rst introduce the cumulative distribution
function of LR(Y) under the null hypothesis and study some of its properties. This
function is key to the study of the performance of likelihood-ratio tests. Deï¬ne
P0

LR(Y) â‰¤Î¾

=

Y
I

LR(y) â‰¤Î¾

f0(y) dy,
0 â‰¤Î¾ â‰¤âˆ,
(30.41a)
P0

LR(Y) < Î¾

=

Y
I

LR(y) < Î¾

f0(y) dy,
0 â‰¤Î¾ â‰¤âˆ,
(30.41b)
P0

LR(Y) = Î¾

=

Y
I

LR(y) = Î¾

f0(y) dy,
0 â‰¤Î¾ â‰¤âˆ,
(30.41c)
and, more generally,
P0

Y âˆˆB

=

Y
I

y âˆˆB

f0(y) dy.
(30.41d)
Lemma 30.5.3 (On the Distribution of LR(Y) under the Null Hypothesis). The
function Î¾ 	â†’P0[LR(Y) â‰¤Î¾] is monotonically nondecreasing and continuous from
the right. Moreover,
P0

LR(Y) = 0

= 0
(30.42)
and
P0

LR(Y) < âˆ

= pâˆ—
FA(0).
(30.43)
Proof. Being the distribution function of the random variable LR(Y) (when Y
is drawn according to f0(Â·)), the function Î¾ 	â†’P0[LR(Y) â‰¤Î¾] is monotonically
nondecreasing and continuous from the right (Billingsley, 1995, Chapter 2, Sec-
tion 14).4
4Strictly speaking, LR(Y) is not a random variable, because it can take on the value +âˆ. As
such, it is a â€œgeneralized random variable,â€ but the distribution function of a generalized random
variable is also nondecreasing and continuous from the right.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.5 Likelihood-Ratio Tests
755
To establish (30.42) we note that the convention 0/0 = 1â€”which we adopted when
we deï¬ned the likelihood-ratio function in (20.39) and (30.18)â€”implies that

y : LR(y) = 0} =

y : f0(y) = 0

\

y : f0(y) = f1(y) = 0

.
(30.44)
Consequently,
P0

LR(Y) = 0

= P0

Y âˆˆ{y : LR(y) = 0}

= P0

Y âˆˆ{y : f0(y) = 0} \ {y : f0(y) = f1(y) = 0}

â‰¤P0

Y âˆˆ{y : f0(y) = 0}

= 0.
We next turn to proving (30.43). Our convention (30.18b) implies that

y : LR(y) < âˆ} =

y : f1(y) > 0

âˆª

y : f0(y) = f1(y) = 0

.
(30.45)
Since
P0

Y âˆˆ{y : f0(y) = f1(y) = 0}

â‰¤P0

Y âˆˆ{y : f0(y) = 0}

= 0,
(30.46)
it follows from (30.45) and (30.46) that
P0

LR(Y) < âˆ

= P0

Y âˆˆ{y : LR(y) < âˆ}

= P0

Y âˆˆ{y : f1(y) > 0} âˆª{y : f0(y) = f1(y) = 0}

= P0

Y âˆˆ{y : f1(y) > 0}

= pâˆ—
FA(0),
where the last equality follows from (30.32).
With the aid of Lemma 30.5.3 we can now analyze likelihood-ratio tests and prove
that any Pareto-optimal pair whose false-alarm probability is (strictly) between 0
and pâˆ—
FA(0) can be achieved by such a test.
Lemma 30.5.4. For every 0 < Ï€FA < pâˆ—
FA(0), the pair

Ï€FA, pâˆ—
MD,rnd(Ï€FA)

is
Pareto-optimal with respect to the class of randomized guessing rules, and it can be
achieved using a randomized likelihood-ratio test that bases its guess on LR(yobs)
only, i.e., for which
b(yobs) =
â§
âª
â¨
âª
â©
1
if LR(yobs) > Î¥,
Î³
if LR(yobs) = Î¥,
0
if LR(yobs) < Î¥,
(30.47)
for some threshold 0 < Î¥ < âˆand for some Î³ âˆˆ[0, 1].
Proof. We will show that for every Ï€FA satisfying
0 < Ï€FA < pâˆ—
FA(0)
(30.48)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

756
The Radar Problem
there is a choice of the threshold Î¥ âˆˆ(0, âˆ) and of Î³ âˆˆ[0, 1] that results in
the test (30.47) having false-alarm probability Ï€FA. Denoting this testâ€™s missed-
detection probability by pâˆ—
MD, it will then follow from the Pareto-optimality of
all likelihood-ratio tests (Proposition 30.5.2) that the pair (Ï€FA, pâˆ—
MD) is Pareto-
optimal with respect to the class of randomized guessing rules and hence that pâˆ—
MD
equals pâˆ—
MD,rnd(Ï€FA) (Proposition 30.1.1).
Using the notation we introduced in (30.41), the probability of false alarm of the
likelihood-ratio test (30.47) can be expressed as
P0

LR(Y) < Î¥

+ (1 âˆ’Î³) P0

LR(Y) = Î¥

.
(30.49)
We shall conclude the proof by showing that for every Ï€FA in the range (30.48)
there exist Î¥ > 0 and Î³ âˆˆ[0, 1] for which this expression equals Ï€FA.
We begin with Î¥, which we would like to choose so that (30.52) and (30.54) ahead
will hold. To this end, we deï¬ne Î¥ as
Î¥ = min
'
Î¾ âˆˆR : P0

LR(Y) â‰¤Î¾

â‰¥Ï€FA
(
.
(30.50)
Here the set whose smallest element we seek is not empty because, by (30.43),
lim
Î¾â†’âˆP0

LR(Y) â‰¤Î¾

= P0

LR(Y) < âˆ

= pâˆ—
FA(0),
(30.51)
and we are only considering the case (30.48) where Ï€FA is smaller than pâˆ—
FA(0). This
set has a smallest element because Î¾ 	â†’P0[LR(Y) â‰¤Î¾] is continuous from the right
(Lemma 30.5.3). The minimum is thus deï¬ned and ï¬nite. Also, since the smallest
element is in the set, it cannot be zero, becauseâ€”by (30.42) and (30.48)â€”zero is
not in the set. Thus, indeed, 0 < Î¥ < âˆ.
With this deï¬nition, Î¥ has the following properties: Since Î¥ is in the set,
P0

LR(Y) â‰¤Î¥

â‰¥Ï€FA.
(30.52)
And since it is the smallest element in the set, no number smaller than Î¥ is in the
set, so
P0

LR(Y) â‰¤Î¥ âˆ’Ïµ

< Ï€FA,
Ïµ > 0.
(30.53)
From (30.53) we obtain upon letting Ïµ tend to zero from above that
P0

LR(Y) < Î¥

â‰¤Ï€FA.
(30.54)
Having deï¬ned Î¥ and derived its salient properties, we next describe our choice of Î³.
It depends on whether or not P0[LR(Y) = Î¥] is zero. If P0[LR(Y) = Î¥] is zero,
then the left-hand sides of (30.52) and (30.54) are equal, so one can be smaller-or-
equal Ï€FA and the other greater-or-equal Ï€FA only if they are both equal to Ï€FA. In
this case, the likelihood-ratio test with threshold Î¥ has false-alarm probability Ï€FA
irrespective of how we resolve ties (see (30.49)).
We can choose Î³ as zero for
concreteness.
Suppose now that P0[LR(Y) = Î¥] is not zero. In this case we choose 1 âˆ’Î³ as
1 âˆ’Î³ = Ï€FA âˆ’P0

LR(Y) < Î¥

P0

LR(Y) = Î¥

.
(30.55)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.5 Likelihood-Ratio Tests
757
The number on the RHS is nonnegative by (30.54), and it does not exceed one
because
Ï€FA âˆ’P0

LR(Y) < Î¥

= Ï€FA âˆ’P0

LR(Y) â‰¤Î¥

+ P0

LR(Y) = Î¥

â‰¤P0

LR(Y) = Î¥

,
(30.56)
where the inequality follows from (30.52). Substituting this choice of Î³ in (30.49)
shows that the false-alarm probability of the likelihood-ratio test deï¬ned by Î¥ and
by Î³ is Ï€FA.
We next show that randomization is not essential for obtaining the Pareto-optimal
pairs: if we do not insist that the likelihood-ratio test depend on the observable yobs
only via LR(yobs), then randomization is not needed. This result relies heavily on
the assumption that under both hypotheses Y has a density. Had we allowed for
point masses, this result would not have held.
Proposition 30.5.5. For every 0 < Ï€FA < pâˆ—
FA(0), the pair

Ï€FA, pâˆ—
MD,rnd(Ï€FA)

can be achieved using a deterministic likelihood-ratio test.
Proof. The randomized likelihood-ratio test in (30.47) is not deterministic when Î³
is strictly between 0 and 1. Inspecting the proof of Lemma 30.5.4 we see that this
is the case whenever P0

LR(Y) = Î¥

is nonzero and the RHS of (30.55) is strictly
between 0 and 1, i.e., whenever
0 < Ï€FA âˆ’P0

LR(Y) < Î¥

< P0

LR(Y) = Î¥

,
(30.57)
where the threshold Î¥ is deï¬ned in (30.50).
We shall thus conclude the proof
by showing that when (30.57) holds, we can ï¬nd an alternative deterministic
likelihood-ratio test of false-alarm probability Ï€FA.
Our proposed deterministic likelihood-ratio test will also be of threshold Î¥. But it
will not treat all the observations yobs for which LR(yobs) equals Î¥ in the same way:
some such observations will result in the guess â€œH = 0,â€ and others in â€œH = 1.â€
To be more speciï¬c, denote the ï¬rst component of the observed vector Y by Y (1)
and the ï¬rst component of its realization yobs by y(1)
obs. The deterministic rule we
propose guesses â€œH = 0â€ whenever yobs is in the set
D =
'
yobs : LR(yobs) > Î¥
(
âˆª
'
yobs : LR(yobs) = Î¥ and y(1)
obs â‰¥Î¾
(
(30.58)
for some judicious choice of Î¾ that will guarantee a false-alarm probability Ï€FA.
The false-alarm probability of this rule is
P0

LR(Y) < Î¥

+ P0

LR(Y) = Î¥ and Y (1) < Î¾

.
(30.59)
For this to equal Ï€FA, we need to choose Î¾ so that
P0

LR(Y) = Î¥ and Y (1) < Î¾

= Ï€FA âˆ’P0

LR(Y) < Î¥

.
(30.60)
We next show that such a Î¾ exists. To this end, deï¬ne the mapping g: R â†’[0, 1]
g: Î± 	â†’P0

LR(Y) = Î¥ and Y (1) < Î±

.
(30.61)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

758
The Radar Problem
This function is monotonically nondecreasing;
lim
Î±â†’âˆg(Î±) = P0

LR(Y) = Î¥

(30.62a)
(as can be proved using the Monotone Convergence Theorem); and
lim
Î±â†’âˆ’âˆg(Î±) = 0
(30.62b)
(as can be proved by noting that g(Â·) is bounded from above by the cumulative
distribution function of Y (1), which tends to zero as its argument tends to âˆ’âˆ).
Once we show that g is continuous, the existence of a Î¾ satisfying (30.60) will
follow from (30.62) and (30.57) using the Intermediate Value Theorem (Rudin,
1976, Theorem 4.23, p. 93), (Royden and Fitzpatrick, 2010, Section 1.6).
It thus only remains to prove that g is continuous.
Since g is monotonically
nondecreasing, |g(Î± + h) âˆ’g(Î±)| is upper-bounded by g(Î± + |h|) âˆ’g(Î± âˆ’|h|),
and to prove continuity at Î± it suï¬ƒces to prove that g(Î± + h) âˆ’g(Î± âˆ’h) tends
to zero when h tends to zero from above. To establish this, we note that for any
positive h,
0 â‰¤g(Î± + h) âˆ’g(Î± âˆ’h)
= P0

LR(Y) = Î¥ and Y (1) < Î± + h

âˆ’P0

LR(Y) = Î¥ and Y (1) < Î± âˆ’h

= P0

LR(Y) = Î¥ and Î± âˆ’h â‰¤Y (1) < Î± + h

â‰¤P0

Î± âˆ’h â‰¤Y (1) < Î± + h

,
and the RHS tends to zero as h â†“0 because the hypothesis that Y has a density
implies that so does its ï¬rst component Y (1), and consequently the cumulative
distribution function of Y (1) is continuous.
Combining this result with (30.24) and (30.34) we obtain:
Corollary 30.5.6. The functions pâˆ—
MD(Â·) and pâˆ—
MD,rnd(Â·) are identical:
pâˆ—
MD(Ï€FA) = pâˆ—
MD,rnd(Ï€FA),
0 â‰¤Ï€FA â‰¤1.
(30.63)
Corollary 30.5.7. Every Pareto-optimal pair with respect to the class of randomized
guessing rules can be also achieved using a deterministic guessing rule.
Proof. The pair

0, pâˆ—
MD(0)

is achievable by a deterministic rule by Lemma 30.4.1;
all pairs of the form

Ï€FA, pâˆ—
MD,rnd(Ï€FA)

for 0 < Ï€FA < pâˆ—
FA(0) are achievable with
deterministic rules by Proposition 30.5.5; and the pair

pâˆ—
FA(0), 0

is achievable
with a deterministic rule by Lemma 30.4.2. There are no more Pareto-optimal
pairs (Corollary 30.4.5).
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.6 A Gaussian Example
759
30.6
A Gaussian Example
How do we guess whether an observed random variable Y was generated from a
centered or from a mean-A Gaussian distribution? Here we answer this question
for the case where the observed random variable has the same variance under the
two hypotheses. We denote this variance by Ïƒ2, and we assume that Ïƒ > 0 so
f0(y) =
1
âˆš
2Ï€Ïƒ2 eâˆ’y2
2Ïƒ2 ,
y âˆˆR,
(30.64a)
f1(y) =
1
âˆš
2Ï€Ïƒ2 eâˆ’(yâˆ’A)2
2Ïƒ2
,
y âˆˆR.
(30.64b)
We further assume that A is positive. (If it is zero, then f0(Â·) and f1(Â·) are identical
and the observation is of no use. And if it is negative, we can consider âˆ’Y as our
observation.)
Since f0(Â·) is positive, it follows from Lemma 30.4.1 that pâˆ—
MD(0) = 1. Likewise,
since f1(Â·) is positive, it follows from Lemma 30.4.2 that pâˆ—
FA(0) = 1. The pairs
(0, 1) and (1, 0) are thus Pareto-optimal. We next ï¬nd the more interesting pairs.
Any guessing rule that guesses â€œH = 0â€ whenever LR(yobs) â‰¥Î¥ is Pareto-optimal
for every value of Î¥ âˆˆ(0, âˆ) (Proposition 30.5.2). Consequently, so is the rule
that guesses â€œH = 0â€ whenever LLR(yobs) â‰¥log Î¥ (because the logarithm is a
monotonic function, so the two rules are, in fact, identical). As Î¥ varies over the
positive reals, its logarithm varies over all the reals. In other words, every Ï… in R is
the logarithm of some positive Î¥. Hence, the rule that guesses â€œH = 0â€ whenever
LLR(yobs) â‰¥Ï… is Pareto-optimal for every Ï… âˆˆR (because, it is identical to the rule
that guesses â€œH = 0â€ whenever LR(yobs) â‰¥eÏ…, and the latter is Pareto-optimal
because it is a likelihood-ratio test with positive ï¬nite threshold). We next explore
such rules and show that with a judicious choice of Ï… âˆˆR they can achieve any
prespeciï¬ed false-alarm probability that is strictly between zero and one. There is
no need to randomize or scrutinize ties.
For the densities in (30.64) the log likelihood-ratio function is
LLR(y) = âˆ’Ay
Ïƒ2 + A2
2Ïƒ2 ,
y âˆˆR,
(30.65)
so our guessing rule can be described as
Guess â€œH = 0â€ â‡â‡’LLR(yobs) â‰¥Ï…
â‡â‡’âˆ’Ayobs
Ïƒ2
+ A2
2Ïƒ2 â‰¥Ï…
â‡â‡’Ayobs
Ïƒ2
â‰¤A2
2Ïƒ2 âˆ’Ï…
â‡â‡’yobs â‰¤Ïƒ2
A
	 A2
2Ïƒ2 âˆ’Ï…

â‡â‡’yobs â‰¤ËœÏ…,
(30.66)
where in the fourth line we have used the hypothesis that A is positive, and where
in the ï¬fth line we deï¬ned
ËœÏ… = Ïƒ2
A
	 A2
2Ïƒ2 âˆ’Ï…

.
(30.67)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

760
The Radar Problem
We thus see that to every Ï… âˆˆR there corresponds some ËœÏ… (which is given in (30.67))
such that the rule of guessing â€œH = 0â€ whenever yobs â‰¤ËœÏ… is Pareto-optimal. But
we can strengthen the statement and claim that this rule is Pareto-optimal for every
ËœÏ… âˆˆR, because to every ËœÏ… âˆˆR there corresponds some Ï… âˆˆR for which (30.67)
holds.
The probability of false alarm of the rule (30.66) is
pFA = P0

Y â‰¥ËœÏ…

= Q
 ËœÏ…
Ïƒ

,
(30.68)
where in the second line we have noted that under the null hypothesis Y has a
N(0, Ïƒ) distribution and hence exceeds ËœÏ… with the probability speciï¬ed in (19.12a).
Since the Q-function maps R onto (0, 1), for each prespeciï¬ed false-alarm probabil-
ity pFA (strictly) between 0 and 1 we can ï¬nd some ËœÏ… âˆˆR for which (30.68) holds.
For this choice of ËœÏ… âˆˆR the decision rule (30.66) is not only Pareto-optimal, but
also of the desired false-alarm probability. Its missed-detection probability is
pMD = 1 âˆ’Q
 ËœÏ… âˆ’A
Ïƒ

,
(30.69)
because, under the alternative hypothesis the observation Y has a N(A, Ïƒ) distribu-
tion, and the probability that it is smaller than ËœÏ… can be computed using (19.12b).
We can now characterize the Pareto-optimal pairs of error probabilities. Those
comprise the pair (0, 1), the pair (1, 0), and all the pairs that have the form

pâˆ—
FA, pâˆ—
MD

=
	
Q
 ËœÏ…
Ïƒ

, 1 âˆ’Q
 ËœÏ… âˆ’A
Ïƒ

(30.70)
for some ËœÏ… âˆˆR. Those are depicted in Figure 30.4.
30.7
Detecting a Signal in White Gaussian Noise
In our next example the observation consists of a stochastic process

Y (t), t âˆˆR

.
Under the null hypothesis, it is the realization of white Gaussian noise N of double-
sided PSD N0/2 with respect to the bandwidth W, whereas under the alternative
hypothesis it is the sum of such noise N and some integrable signal s that is
bandlimited to W Hz. Given the signal s and the PSD SNN of the noise, our task
is to design a guessing rule that bases its decision on the observed SP.5
We ï¬rst argue that there is no loss of optimality in basing our guess on âŸ¨Y, Ï†âŸ©,
where
Ï† =
s
âˆ¥sâˆ¥2
(30.71)
is a unit-vector collinear with s. We do so by showing that, given any guessing
rule that bases its guess on Y, we can construct a randomized rule of identical
performance that bases its decision on âŸ¨Y, Ï†âŸ©. The argument is very similar to the
5We limit ourselves to guessing rules that are measurable with respect to the Ïƒ-algebra gen-
erated by the cylindrical sets of Y (Deï¬nition 25.2.2).
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.7 Detecting a Signal in White Gaussian Noise
761
1
1
1
1
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.4: The Pareto-optimal pairs (30.70) for the Gaussian problem. In (a)
A/Ïƒ is 1, and in (b) it is 2.
one we employed in proving Theorem 26.3.1 and which was illustrated in Figure 26.1
on Page 623.
The rule we construct uses âŸ¨Y, Ï†âŸ©and local randomness to generate a SP Yâ€², which
is then fed to the given guessing rule. Since Yâ€² will be generated in a way that will
guarantee that its FDDs equal those of Y under both hypotheses, the constructed
rule and the given rule will have identical performance.
To generate Yâ€², the rule we construct ï¬rst uses the local randomness to generate
a SP Nâ€² of the same law as N but independent of it. In terms of âŸ¨Y, Ï†âŸ©and Nâ€²,
we can express Yâ€² as
Yâ€² = âŸ¨Y, Ï†âŸ©Ï† + Nâ€² âˆ’âŸ¨Nâ€², Ï†âŸ©Ï†
(30.72)
=

âŸ¨N, Ï†âŸ©Ï† + Nâ€² âˆ’âŸ¨Nâ€², Ï†âŸ©Ï†
if Y = N,
âŸ¨N, Ï†âŸ©Ï† + Nâ€² âˆ’âŸ¨Nâ€², Ï†âŸ©Ï† + s
if Y = s + N,
(30.73)
where in the last line we have used the fact that âŸ¨s + N, Ï†âŸ©= âŸ¨s, Ï†âŸ©+ âŸ¨N, Ï†âŸ©
and âŸ¨s, Ï†âŸ©Ï† = s. We see from (30.73) that, under both hypotheses, Yâ€² and Y
have identical laws, because N and âŸ¨N, Ï†âŸ©Ï† + Nâ€² âˆ’âŸ¨Nâ€², Ï†âŸ©Ï† have identical laws
(Theorem 25.15.7).
Having established that there is no loss of optimality in basing our guess on âŸ¨Y, Ï†âŸ©,
we next proceed to study the guessing problem when this is our observation. Us-
ing the properties of WGN (Proposition 25.15.2), we conclude that âŸ¨Y, Ï†âŸ©has a
N

0, Ïƒ2
distribution under H = 0 and a N

âˆ¥sâˆ¥2, Ïƒ2
distribution under H = 1,
where
Ïƒ2 = N0
2 .
(30.74a)
The problem of guessing based on âŸ¨Y, Ï†âŸ©is thus exactly the problem we studied
in Section 30.6. Using the results from that section (with âˆ¥sâˆ¥2 substituted for A),
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

762
The Radar Problem
we obtain that the Pareto-optimal pairs are the trivial pairs (0, 1) and (1, 0) and
all the pairs that can be expressed as

pâˆ—
FA, pâˆ—
MD

=
-
Q
	 ËœÏ…
Ïƒ

, 1 âˆ’Q
	 ËœÏ… âˆ’âˆ¥sâˆ¥2
Ïƒ

.
(30.74b)
for some ËœÏ… âˆˆR.
30.8
Suï¬ƒcient Statistics
We have seen that, when randomized guessing rules are allowed, any Pareto-
optimal pair (pâˆ—
FA, pâˆ—
MD) with 0 < pâˆ—
FA < pâˆ—
FA(0) can be achieved by a (random-
ized) likelihood-ratio test that bases its decision on the likelihood ratio LR(yobs)
(Lemma 30.5.4). It therefore stands to reason that if T(Â·) is a suï¬ƒcient statistic for
f0(Â·) and f1(Â·) so LR(yobs) is computable from T(yobs) (Deï¬nition 20.12.2), then
basing our (randomized) guess on T(yobs) should incur no loss of optimality. This
is true, but before we can prove this we need to show that also the Pareto-optimal
pairs (0, pâˆ—
MD(0)) and (pâˆ—
FA(0), 0) can be achieved using guessing rules that depend
only on LR(yobs). (The rules in Lemma 30.4.1 and Lemma 30.4.2 are not quite of
this kind because of the convention we adopted to handle the case of 0/0 in the
deï¬nition of the likelihood-ratio function (30.18).)
Proposition 30.8.1 (Rules Based on LR(yobs) for pFA = 0 and for pMD = 0).
(i) Guessing â€œH = 0â€ whenever LR(yobs) > 0 achieves the pair (0, pâˆ—
MD(0)).
(ii) Guessing â€œH = 0â€ whenever LR(yobs) = âˆachieves the pair (pâˆ—
FA(0), 0).
Proof. We begin with Part (i). In view of our convention that 0/0 is 1,

y : LR(y) > 0

=

y : f0(y) > 0

âˆª

y : f0(y) = f1(y) = 0

.
(30.75)
Consequently, since

Y
f0(y) I
'
y âˆˆ

yâ€² : f0(yâ€²) = f1(yâ€²) = 0
(
dy = 0
(30.76a)
and

Y
f1(y) I
'
y âˆˆ

yâ€² : f0(yâ€²) = f1(yâ€²) = 0
(
dy = 0,
(30.76b)
we conclude that the events Y âˆˆ{y : LR(y) > 0} and Y âˆˆ{y : f0(y) > 0} have
identical probabilities under both hypotheses. Consequently, the rule of guessing
â€œH = 0â€ when yobs âˆˆ{y : f0(y) > 0} and the rule of guessing â€œH = 0â€ when
yobs âˆˆ{y : LR(y) > 0} have identical performance. And since the former achieves
(0, pâˆ—
MD(0)) (Lemma 30.4.1), so does the latter.
The proof of Part (ii) follows similarly from

y : LR(y) = âˆ

=

y : f1(y) = 0

\

y : f0(y) = f1(y) = 0

,
(30.77)
from (30.76), and from Lemma 30.4.2.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.9 A Noncoherent Detection Problem
763
We are now ready to state the main result on suï¬ƒcient statistics.
Proposition 30.8.2 (Suï¬ƒcient Statistics and the Radar Problem). If the mapping
T : Rd â†’Rdâ€² forms a suï¬ƒcient statistic for the densities f0(Â·) and f1(Â·), then any
Pareto-optimal pair that can be achieved by a possibly-randomized guessing rule
that is based on Y can also be achieved by a randomized guessing rule that is based
on T(Y).
Proof. Let (pâˆ—
FA, pâˆ—
MD) be Pareto-optimal with respect to the class of randomized
guessing rules that are based on Y.
By Corollary 30.4.5, Lemma 30.5.4, and
Proposition 30.8.1 the pair (pâˆ—
FA, pâˆ—
MD) is achievable by a guessing rule that forms
its guess based on local randomness and the value of LR(yobs) that it is fed. Since
T : Rd â†’Rdâ€² is a suï¬ƒcient statistic for the densities f0(Â·) and f1(Â·), there exists a
set Y0 âŠ‚Rd of Lebesgue measure zero and a Borel measurable function Î¶ : Rdâ€² â†’
[0, âˆ] such that for all yobs âˆˆRd satisfying
yobs /âˆˆY0
and
f0(yobs) + f1(yobs) > 0
(30.78)
we can express LR(yobs) in terms of T(yobs) as
LR(yobs) = Î¶

T(yobs)

.
(30.79)
Suppose now that instead of LR(yobs) we feed Î¶

T(yobs)

to the above rule. This
would result in a randomized rule that bases its decision on T(yobs). We claim
that this rule has the same performance as the one that is fed LR(yobs) and hence
also achieves (pâˆ—
FA, pâˆ—
MD). Indeed, under both hypotheses, the probability that the
observable yobs violates (30.78) is zero (cf. Note 20.12.1), so under both hypotheses
Î¶

T(Y)

and LR(Y) are equal with probability one.
30.9
A Noncoherent Detection Problem
Our next example is a variation on a theme from Chapter 27 on noncoherent
detection. The observation is a SP

Y (t)

, and we wish to guess whether it is noise
(the â€œnull hypothesisâ€) or the sum of noise and a passband signal of random phase
(the â€œalternative hypothesisâ€). We model the random passband signal S as
S(t) = 2 Re

sBB(t) ei(2Ï€fct+Î˜)
= 2 Re

sBB(t) ei2Ï€fct
cos Î˜ âˆ’2 Im

sBB(t) ei2Ï€fct
sin Î˜
= 2 Re

sBB(t) ei2Ï€fct
cos Î˜ + 2 Re

i sBB(t) ei2Ï€fct
sin Î˜
= sc(t) cos Î˜ + ss(t) sin Î˜,
t âˆˆR,
(30.80)
where sBB is a deterministic (known), possibly-complex, integrable signal that
is bandlimited to W/2 Hz; fc is the carrier frequency, which is assumed to be
deterministic (known) and to satisfy fc > W/2; the RV Î˜ (unknown) is uniformly
distributed over the interval [âˆ’Ï€, Ï€) independently of the noise; and where we
deï¬ned
sc(t) â‰œ2 Re

sBB(t) ei2Ï€fct
,
t âˆˆR,
(30.81a)
ss(t) â‰œ2 Re

i sBB(t) ei2Ï€fct
,
t âˆˆR.
(30.81b)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

764
The Radar Problem
The noise is assumed to be WGN of double-sided PSD N0/2 with respect to the
bandwidth W around the carrier frequency fc (Deï¬nition 25.15.9). Notice that, by
Theorem 7.6.10,
âŸ¨sc, ssâŸ©= 0.
(30.82)
Also, if we deï¬ne Es as
Es = 2 âˆ¥sBBâˆ¥2
2 ,
(30.83)
then, by Theorem 7.6.10 on the energy in baseband and passband,
Es = âˆ¥Sâˆ¥2
2 = âˆ¥scâˆ¥2
2 = âˆ¥ssâˆ¥2
2
(30.84)
for every realization of Î˜. The signals sc/âˆšEs and ss/âˆšEs are thus orthonormal.
The motivation for studying this problem is similar to the one in Section 27.1: We
envision a scenario where the distance to the target is known only approximately,
and we therefore only have an estimate of the arrival time of the reï¬‚ected signal.
Our model is plausible when our estimation error is small compared to 1/W but
large compared to 1/fc.
As in Section 30.7, here too we can reduce the problem to a ï¬nite-dimensional one:
there is no loss of optimality in basing our guess on the tuple (Tc, Ts), where
Tc â‰œ
#
Y, sc
âˆšEs
$
,
Ts â‰œ
#
Y, ss
âˆšEs
$
.
(30.85)
Indeed, the performance of any given guessing rule that bases its decision on the
waveform Y can also be achieved by a randomized decision rule that bases its
decision on (Tc, Ts) in the following way.
It uses local randomness to generate
noise Nâ€² of the same law as N; it uses (Tc, Ts) and Nâ€² to synthesize Yâ€², where
Yâ€² = Tc
sc
âˆšEs
+ Ts
ss
âˆšEs
+ Nâ€² âˆ’
#
Nâ€², sc
âˆšEs
$ sc
âˆšEs
âˆ’
#
Nâ€², ss
âˆšEs
$ ss
âˆšEs
;
(30.86)
and it then feeds Yâ€² to the given rule. Using arguments very similar to those in
Section 30.7, it can be shown that the laws of Yâ€² and Y are identical both under
the null hypothesis and under the alternative hypothesis.
To derive Pareto-optimal rules based on the tuple (Tc, Ts), we next derive the
tupleâ€™s densities under the two hypotheses.
To simplify the typesetting, let us
deï¬ne
Ïƒ2 â‰œN0
2
(30.87)
and
t â‰œt2
c + t2
s
Ïƒ2
.
(30.88)
(We deï¬ne t not only to simplify the typesetting but also for ulterior motives that
have to do with the further reduction of the suï¬ƒcient statistic from a random
vector of two components (Tc, Ts) to a scalar.)
Since the signals tuple

sc/âˆšEs, ss/âˆšEs

is orthonormal (by (30.82) and (30.84)),
it follows that under the null hypothesis Tc and Ts are independent variance-Ïƒ2
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.9 A Noncoherent Detection Problem
765
centered Gaussians (Note 25.15.10), and hence
fTc,Ts|H=0(tc, ts) =
1
2Ï€Ïƒ2 exp

âˆ’1
2Ïƒ2

t2
c + t2
s

=
1
2Ï€Ïƒ2 exp

âˆ’t
2

.
(30.89)
To compute the density under the alternative hypothesis, we next condition on Î˜
and then integrate it out like we did in Section 27.4. Conditional on Î˜ = Î¸ we
have under the alternative hypothesis that Tc and Ts are independent variance-Ïƒ2
Gaussians with Tc having mean âˆšEs cos Î¸ and with Ts having mean âˆšEs sin Î¸. Thus,
fTc,Ts|H=1,Î˜=Î¸(tc, ts)
=
1
2Ï€Ïƒ2 exp
	
âˆ’1
2Ïƒ2

tc âˆ’

Es cos Î¸
2 +

ts âˆ’

Es sin Î¸
2
=
1
2Ï€Ïƒ2 exp

âˆ’Es
2Ïƒ2 âˆ’t
2

exp
 1
Ïƒ2

Es tc cos Î¸ + 1
Ïƒ2

Es ts sin Î¸

.
(30.90)
As in (27.32), we now integrate Î˜ out:
fTc,Ts|H=1(tc, ts) =
 Ï€
âˆ’Ï€
fÎ˜|H=1(Î¸) fTc,Ts|H=1,Î˜=Î¸(tc, ts) dÎ¸
=
 Ï€
âˆ’Ï€
fÎ˜(Î¸) fTc,Ts|H=1,Î˜=Î¸(tc, ts) dÎ¸
= 1
2Ï€
 Ï€
âˆ’Ï€
fTc,Ts|H=1,Î˜=Î¸(tc, ts) dÎ¸
=
1
2Ï€Ïƒ2 eâˆ’Es/(2Ïƒ2) eâˆ’t/2
Ã— 1
2Ï€
 Ï€
âˆ’Ï€
exp
	 1
Ïƒ2

Es tc cos Î¸ + 1
Ïƒ2

Es ts sin Î¸

dÎ¸
=
1
2Ï€Ïƒ2 eâˆ’Es/(2Ïƒ2) eâˆ’t/2
Ã— 1
2Ï€
 Ï€
âˆ’Ï€
exp
-4
Es
Ïƒ2
âˆš
t cos

Î¸ âˆ’arctan(ts/tc)
.
dÎ¸
=
1
2Ï€Ïƒ2 eâˆ’Es/(2Ïƒ2) eâˆ’t/2
Ã— 1
2Ï€
 Ï€âˆ’arctan(ts/tc)
âˆ’Ï€âˆ’arctan(ts/tc)
exp
-4
Es
Ïƒ2
âˆš
t cos Ïˆ
.
dÏˆ
=
1
2Ï€Ïƒ2 eâˆ’Es/(2Ïƒ2) eâˆ’t/2 1
2Ï€
 Ï€
âˆ’Ï€
exp
-4
Es
Ïƒ2
âˆš
t cos Ïˆ
.
dÏˆ
=
1
2Ï€Ïƒ2 eâˆ’Es/(2Ïƒ2) eâˆ’t/2 I0
-4
Es
Ïƒ2
âˆš
t
.
,
(30.91)
where the ï¬rst equality follows by averaging out Î˜; the second because the law of Î˜
does not depend on the hypothesis; the third because Î˜ is uniform; the fourth by
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

766
The Radar Problem
the explicit form of fTc,Ts|Î˜=Î¸,H=1(Â·) (30.90); the ï¬fth by the trigonometric identity
Î± cos Î¸ + Î² sin Î¸ =

Î±2 + Î²2 cos

Î¸ âˆ’arctan(Î²/Î±)

(30.92)
and the deï¬nition of t (30.88); the sixth by the change of variable Ïˆ â‰œÎ¸ âˆ’
arctan(ts/tc); the seventh from the periodicity of the cosine function; and the ï¬nal
equality by recalling the deï¬nition of the zeroth-order modiï¬ed Bessel function
I0(Â·) (27.34).
Having computed the density of (Tc, Ts) under both the null hypothesis (30.89) and
the alternative hypothesis (30.91), we can now derive the Pareto-optimal decision
rules. But before rushing to do so, we pause to note that both fTc,Ts|H=0(tc, ts)
and fTc,Ts|H=1(tc, ts) are computable from t (30.88). Consequently, so is their ratio,
and we conclude that

Tc, Ts

	â†’T 2
c + T 2
s
Ïƒ2
(30.93)
is a suï¬ƒcient statistic for the two densities. By Proposition 30.8.2 there is therefore
no loss of optimality in basing our decision on
T â‰œT 2
c + T 2
s
Ïƒ2
.
(30.94)
In analogy to the two methods we presented in Section 27.5, we can now proceed to
derive the Pareto-optimal pairs using two diï¬€erent methods. The ï¬rst is to ignore
the suï¬ƒciency of T and to work with the likelihood-ratio function
LR(tc, ts) = fTc,Ts|H=0(tc, ts)
fTc,Ts|H=1(tc, ts)
=
1
2Ï€Ïƒ2 exp

âˆ’t
2

1
2Ï€Ïƒ2 eâˆ’Es/(2Ïƒ2) exp

âˆ’t
2

I0
3
Es
Ïƒ2
âˆš
t

=
eEs/(2Ïƒ2)
I0
3
Es
Ïƒ2
âˆš
t
.
(30.95)
The second method is to compute the density of T under the two hypotheses and de-
rive the Pareto-optimal pairs via the likelihood-ratio function fT |H=0(t)/fT |H=1(t).
We shall use the ï¬rst method, but we shall nonetheless ï¬rst compute the densities
fT |H=0(Â·), fT |H=1(Â·), because those will be needed when we analyze the perfor-
mance of the proposed guessing rules. Under the null hypothesis, T is the sum of
the squares of two independent standard Gaussians (Tc/Ïƒ and Tc/Ïƒ). As such, it
has a central Ï‡2 distribution with two degrees of freedom (Section 19.8.1),
T âˆ¼Ï‡2
2
(the null hypothesis),
(30.96)
i.e., a mean-2 exponential distribution (Note 19.8.1):
fT |H=0(t) = 1
2 eâˆ’t/2 I{t â‰¥0},
t âˆˆR.
(30.97)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.9 A Noncoherent Detection Problem
767
To compute the distribution of T under the alternative hypothesis, we ï¬rst condi-
tion on Î˜ = Î¸, with the result that T is distributed as the sum of the squares
of two independent unit-variance Gaussians, where the ï¬rst, Tc/Ïƒ, is of mean
Ïƒâˆ’1âˆšEs cos Î¸ and the second, Ts/Ïƒ, is of mean Ïƒâˆ’1âˆšEs sin Î¸.
Thus, under the
alternative hypothesis, the conditional distribution of T given Î˜ = Î¸ is a non-
central Ï‡2 distribution with two degrees of freedom and noncentrality parameter
Es/Ïƒ2 (Note 19.8.2). Since the parameters of this distribution do not depend on Î¸,
we conclude that, under the alternative hypothesis, T is independent of Î˜ and
T âˆ¼Ï‡2
2,Es/Ïƒ2
(the alternative hypothesis).
(30.98)
This concludes the derivation of the distribution of T under the two hypotheses.
We next derive the Pareto-optimal pairs using the ï¬rst method, i.e., by ignoring
the suï¬ƒciency of T and by considering our observation as being the pair (Tc, Ts)
and using the likelihood-ratio function (30.95).
Under both hypotheses the joint density of (Tc, Ts) is positive ((30.89) and (30.91)),
so pâˆ—
MD(0) = 1 (Lemma 30.4.1) and pâˆ—
FA(0) = 1 (Lemma 30.4.2). The pairs (0, 1)
and (1, 0) are thus Pareto-optimal. Any guessing rule that guesses â€œH = 0â€ when-
ever LR(y) â‰¥Î¥ is Pareto-optimal, irrespective of the value of Î¥ âˆˆ(0, âˆ) (Propo-
sition 30.5.2). Using the explicit form of the likelihood-ratio function (30.95), we
thus conclude that the rule
guess â€œH = 0â€ whenever
eEs/(2Ïƒ2)
I0
3
Es
Ïƒ2
âˆš
t
 â‰¥Î¥
(30.99a)
is Pareto-optimal for every Î¥ âˆˆ(0, âˆ). Deï¬ning Î¥â€² = eEs/(2Ïƒ2)/Î¥, noting that
every Î¥â€² âˆˆ(0, âˆ) corresponds to some Î¥ âˆˆ(0, âˆ) (i.e., that the mapping Î¥ 	â†’Î¥â€²
is onto (0, âˆ)), and noting that I0(Â·) is positive (27.34), we conclude that the rule
guess â€œH = 0â€ whenever
I0
	4
Es
Ïƒ2
âˆš
t

â‰¤Î¥â€²
(30.99b)
is Pareto-optimal for every choice of Î¥â€² âˆˆ(0, âˆ). Given any Î¥â€²â€² âˆˆ(0, âˆ), we can
deï¬ne Î¥â€² to be I0(Î¥â€²â€²) with the result that Î¥â€² âˆˆ(1, âˆ) and that the rule (30.99b)
is thus Pareto-optimal. Substituting I0(Î¥â€²â€²) for Î¥â€² in (30.99b) thus shows that the
rule
guess â€œH = 0â€ whenever
I0
	4
Es
Ïƒ2
âˆš
t

â‰¤I0(Î¥â€²â€²)
(30.99c)
is Pareto-optimal for every Î¥â€²â€² âˆˆ(0, âˆ). Since the zeroth-order modiï¬ed Bessel
function I0(Â·) is strictly increasing on the nonnegative reals, the rule (30.99c) is
equivalent to the rule
guess â€œH = 0â€ whenever
4
Es
Ïƒ2
âˆš
t â‰¤Î¥â€²â€²,
(30.99d)
which is thus also Pareto-optimal for every Î¥â€²â€² âˆˆ(0, âˆ). Deï¬ning Ï… to be the
square of Î¥â€²â€²/

Es/Ïƒ2 (and noting that the mapping from Î¥â€²â€² to Ï… is onto the
positive reals) we conclude that
guess â€œH = 0â€ whenever
t â‰¤Ï…
(30.99e)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

768
The Radar Problem
is Pareto-optimal for every choice of Ï… âˆˆ(0, âˆ).
The false-alarm probability of this rule is, by (30.96), the probability that a mean-2
exponential exceeds Ï…, so
pFA = exp

âˆ’Ï…
2

,
Ï… âˆˆ(0, âˆ).
(30.100)
To each prespeciï¬ed pFA âˆˆ(0, 1), we can thus ï¬nd a corresponding Ï… âˆˆ(0, âˆ)
so that the Pareto-optimal rule (30.99e) will be of this false-alarm probability.
Consequently, as we vary Ï… over (0, âˆ) we swipe over all Pareto-optimal rules of
nontrivial false alarm.
The probability of missed-detection of the rule (30.99e) is, by (30.98), the proba-
bility that a Ï‡2
2,Es/Ïƒ2 RV does not exceed Ï…. Equivalently, it is the probability that
the square root of a Ï‡2
2,Es/Ïƒ2 RV (which has a Rice distribution) does not exceed
âˆšÏ…. Using (19.51) (with the substitution of Es/Ïƒ2 for Î» and of âˆšÏ… for x) we thus
obtain
pMD = 1 âˆ’Q1
	4
Es
Ïƒ2 , âˆšÏ…

,
Ï… âˆˆ(0, âˆ),
(30.101)
where Q1(Â·) is the Marcum Q-function.
To conclude, the Pareto-optimal pairs are the pairs (0, 1), (1, 0), and all the pairs
that have the form

pâˆ—
FA, pâˆ—
MD

=
-
exp

âˆ’Ï…
2

, 1 âˆ’Q1
	4
Es
Ïƒ2 , âˆšÏ…

.
(30.102)
for Ï… âˆˆ(0, âˆ). Pairs of the form (30.102) are achieved by the guessing rule (30.99e).
30.10
Randomization Is Not Needed
We have seen that all the pairs that are Pareto-optimal with respect to the class
of randomized guessing rules can also be achieved by deterministic rules (Corol-
lary 30.5.7). Here we present a stronger result: any pairâ€”be it Pareto-optimal
or notâ€”that is achievable by a randomized guessing rule can also be achieved by
a deterministic one. Like the aforementioned result on Pareto-optimal pairs, this
result too relies heavily on the assumption that the observable Y has a density
under both hypotheses. This sectionâ€™s main result can be stated as follows:
Proposition 30.10.1 (Rrnd = R). Every pair (pFA, pMD) that can be achieved by
some randomized rule can also be achieved by a deterministic rule:
Rrnd = R.
(30.103)
Since Rrnd âŠ‡R (30.15), to prove (30.103) it remains to prove that
Rrnd âŠ†R,
(30.104)
i.e., that every pair that is achievable by a randomized rule can also be achieved
by a deterministic rule. The proof of this result relies on an important theorem
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.10 Randomization Is Not Needed
769
due to Alexey Liapunov (1911â€“1973) on the convexity of vector-valued measures.
To state the theorem we ï¬rst recall that a subset K of R2 is a convex set if for
every 0 < Î» < 1 and every pair of tuples r1, r2 in K, the tuple Î»r1 + (1 âˆ’Î»)r2 is
also in K. We can now state Liapunovâ€™s theorem for our setting as follows:
Theorem 30.10.2 (Liapunov). Let f0(Â·) and f1(Â·) be two densities on Rn. The
subset of R2 comprising all pairs that can be expressed as
	
A
f0(y) dy,

A
f1(y) dy

for some (Lebesgue measurable) subset A of Rn is convex.
If a set K âŠ†R2 is convex, then so is the set
'
1 âˆ’r(1), r(2)
:

r(1), r(2)
âˆˆK
(
.
By using this observation, by substituting in Liapunovâ€™s Theorem the set D (the
subset of Rn where we guess â€œH = 0â€) for A, and by recalling the representa-
tions (30.3b) and (30.4b) of pFA and pMD we obtain:
Corollary 30.10.3 (R Is Convex). The set of pairs (pFA, pMD) that can be achieved
by deterministic guessing rules is convex.
Before we can prove the inclusion (30.104) using Corollary 30.10.3, we need some
more groundwork. Recall that in (30.9) we deï¬ned pâˆ—
MD(Ï€FA) to be the least proba-
bility of missed-detection that can be achieved by a deterministic rule whose false-
alarm probability does not exceed Ï€FA. We did not require that (Ï€FA, pâˆ—
MD(Ï€FA))
be achievable. But we did show that (Ï€FA, pâˆ—
MD(Ï€FA)) is in R (i.e., is achievable)
whenever Ï€FA is in the range 0 â‰¤Ï€FA â‰¤pâˆ—
FA(0). (For Ï€FA = 0 the result fol-
lows from Lemma 30.4.1; for 0 < Ï€FA < pâˆ—
FA(0) it follows from Proposition 30.5.5;
and for Ï€FA = pâˆ—
FA(0) it follows from Lemma 30.4.2.) We next use the convexity
of R to show that this also holds for pâˆ—
FA(0) < Ï€FA â‰¤1, i.e., that it holds for all
0 â‰¤Ï€FA â‰¤1.6
Lemma 30.10.4. For every 0 â‰¤Ï€FA â‰¤1, the pair

Ï€FA, pâˆ—
MD(Ï€FA)

is achievable
by a deterministic guessing rule:

Ï€FA, pâˆ—
MD(Ï€FA)

âˆˆR,
0 â‰¤Ï€FA â‰¤1.
(30.105)
Proof. For Ï€FA = 0 the result follows from Lemma 30.4.1; for 0 < Ï€FA < pâˆ—
FA(0) it
follows from Proposition 30.5.5; and for Ï€FA = pâˆ—
FA(0) it follows from Lemma 30.4.2.
The remaining case is when Ï€FA is in the range
pâˆ—
FA(0) < Ï€FA â‰¤1.
(30.106)
For such Ï€FA we can express (Ï€FA, 0) as a convex combination of (pâˆ—
FA(0), 0) and
(1, 0), both of which are in R (the former by Lemma 30.4.2 and the latter by
6This result does not need the full power of Liapunovâ€™s Theorem. We could have also proved
it using the technique we used in the proof of Proposition 30.5.5.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

770
The Radar Problem
considering the deterministic guessing rule of always guessing â€œH = 1â€).
The
convexity of R thus implies that (Ï€FA, 0) is in R:
(Ï€FA, 0) âˆˆR,
pâˆ—
FA(0) < Ï€FA â‰¤1.
(30.107)
For Ï€FA in the range (30.106), pâˆ—
MD(Ï€FA) is zero (30.34), so (30.107) is equivalent
to

Ï€FA, pâˆ—
MD(Ï€FA)

âˆˆR,
pâˆ—
FA(0) < Ï€FA â‰¤1,
(30.108)
thus verifying the lemma in this range too.
Before we can prove (30.104) we also need the following symmetry result:
Proposition 30.10.5. Both R and Rrnd are invariant under rotation by 180 degrees
around the pair (1/2, 1/2):

(pFA, pMD) âˆˆR

â‡â‡’

(1 âˆ’pFA, 1 âˆ’pMD) âˆˆR

,
(30.109)

(pFA, pMD) âˆˆRrnd

â‡â‡’

(1 âˆ’pFA, 1 âˆ’pMD) âˆˆRrnd

.
(30.110)
Proof. Consider ï¬rst Rrnd. To establish (30.110) it suï¬ƒces to prove that

(pFA, pMD) âˆˆRrnd

=â‡’

(1 âˆ’pFA, 1 âˆ’pMD) âˆˆRrnd

,
(30.111)
because the reverse implication will then follow by applying (30.111) to the pair
(1 âˆ’pFA, 1 âˆ’pMD). To prove (30.111) suppose that (pFA, pMD) is achieved by the
randomized guessing rule determined by the function b(Â·). Consider now a new
rule that is determined by the function Â¯b(Â·), where
Â¯b(y) = 1 âˆ’b(y),
y âˆˆY.
Denoting the probability of false alarm of this new guessing rule by Â¯pFA,
Â¯pFA =

Y

1 âˆ’Â¯b(y)

f0(y) dy
=

Y
f0(y) dy âˆ’

Y
Â¯b(y) f0(y) dy
= 1 âˆ’

Y

1 âˆ’b(y)

f0(y) dy
= 1 âˆ’pFA.
Likewise, denoting the probability of missed-detection of the new guessing rule by
Â¯pMD,
Â¯pMD =

Y
Â¯b(y) f1(y) dy
=

Y
f1(y) dy âˆ’

Y

1 âˆ’Â¯b(y)

f1(y) dy
= 1 âˆ’

Y
b(y) f1(y) dy
= 1 âˆ’pMD.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.10 Randomization Is Not Needed
771
1
1
1
1
pâˆ—
MD(0)
pâˆ—
MD(0)
pâˆ—
FA(0)
pâˆ—
FA(0)
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.5: A generic achievable region. In (a) the dashed line is the graph of
pâˆ—
MD,rnd(Â·). In (b), we have added the graph of pFA 	â†’1 âˆ’pâˆ—
MD,rnd(1 âˆ’pFA), which
is obtained from the graph of pâˆ—
MD,rnd(Â·) by rotating it around the point (1/2, 1/2)
by 180 degrees.
Thus, the probabilities of error of the new rule are (1âˆ’pFA, 1âˆ’pMD), and the pair
is thus achievable. This concludes the proof of (30.111) and hence of (30.110).
The proof for R is very similar. The only diï¬€erence is that the function b(Â·) and
hence also Â¯b(Â·) are zero-one valued.
We next use Proposition 30.10.5 to show that if (pFA, pMD) âˆˆRrnd then we can
bound pMD from below and from above using the function pâˆ—
MD,rnd(Â·) as follows:
pâˆ—
MD,rnd(pFA) â‰¤pMD â‰¤1 âˆ’pâˆ—
MD,rnd(1 âˆ’pFA),
(pFA, pMD) âˆˆRrnd.
(30.112)
The lower bound pMD â‰¥pâˆ—
MD,rnd(pFA) follows directly from the deï¬nition (30.16)
of pâˆ—
MD,rnd(pFA), which implies that no achievable pair of false alarm pFA can have
a missed-detection probability smaller than pâˆ—
MD,rnd(pFA). More interesting is the
upper bound, i.e., the statement that

(pFA, pMD) âˆˆRrnd

=â‡’

pMD â‰¤1 âˆ’pâˆ—
MD,rnd(1 âˆ’pFA)

.
(30.113)
This relation is illustrated in Figure 30.5. The shadowed region corresponds to
the achievable region Rrnd. The dashed line is the graph of pâˆ—
MD,rnd(Â·), and the
irregularly-dashed line is the graph of pFA 	â†’1 âˆ’pâˆ—
MD,rnd(1 âˆ’pFA). (It is obtained
from the dashed line by rotating it by 180 degrees around the point (1/2, 1/2).)
No achievable pair lies above the irregularly-dashed line.
To prove (30.113), suppose that (pFA, pMD) âˆˆRrnd.
Proposition 30.10.5 then
implies that also (1 âˆ’pFA, 1 âˆ’pMD) âˆˆRrnd. This and the deï¬nition of pâˆ—
MD,rnd(1 âˆ’
pFA) imply that pâˆ—
MD,rnd(1âˆ’pFA) â‰¤1âˆ’pMD and hence pMD â‰¤1âˆ’pâˆ—
MD,rnd(1âˆ’pFA).
Having established (30.112), we now recall that the functions pâˆ—
MD,rnd(Â·) and pâˆ—
MD(Â·)
are identical (Corollary 30.5.6), and we can thus substitute pâˆ—
MD(Â·) for pâˆ—
MD,rnd(Â·)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

772
The Radar Problem
in (30.112) to obtain
pâˆ—
MD(pFA) â‰¤pMD â‰¤1 âˆ’pâˆ—
MD(1 âˆ’pFA),
(pFA, pMD) âˆˆRrnd.
(30.114)
The inequalities in (30.114) imply that every (pFA, pMD) âˆˆRrnd can be written as
a convex combination of the pairs

pFA, pâˆ—
MD(pFA)

and

pFA, 1 âˆ’pâˆ—
MD(1 âˆ’pFA)

.
Once we show that these two pairs are both in R, it will follow from the convexity
of R (Corollary 30.10.3) that this convex combination is in R, and (30.104) will
be established.
Beginning with the ï¬rst pair, we note that

pFA, pâˆ—
MD(pFA)

âˆˆR,
0 â‰¤Ï€FA â‰¤1,
(30.115)
by Lemma 30.10.4, i.e., by (30.105). As to the second pair, we note that substi-
tuting 1 âˆ’pFA for pFA in (30.115) yields

1 âˆ’pFA, pâˆ—
MD(1 âˆ’pFA)

âˆˆR,
0 â‰¤Ï€FA â‰¤1,
(30.116)
from which the achievability of the second pair, namely,

pFA, 1 âˆ’pâˆ—
MD(1 âˆ’pFA)

âˆˆR,
0 â‰¤Ï€FA â‰¤1
(30.117)
follows using Proposition 30.10.5. We have thus established the achievability of
the two pairs using deterministic guessing rules and have thus concluded the proof
of (30.104).
As we have noted, the assumption that Y has a density under both hypotheses is
essential for R to equal Rrnd. (We used this assumption in the proof of Proposi-
tion 30.5.5, and it is also key in Liapunovâ€™s Theorem.) To see this, consider the
case where under both hypotheses the observation is deterministically equal to 17.
In this case the observation is clearly useless. Depending on whether or not we
guess â€œH = 0â€ after observing 17, we either achieve (pFA, pMD) equal to (1, 0) or
to (0, 1). There are no other pairs that are achievable with a deterministic rule.
However, with randomized guessing rules we can guess â€œH = 0â€ with probability b
and thus achieve all pairs of the form (1 âˆ’b, b) where b is any number satisfying
0 â‰¤b â‰¤1.
30.11
The Big Picture
To see the big picture, we collect here the key results on Pareto-optimal pairs
and the rules that achieve them. In some places we also expand. For example,
Theorem 30.11.1 (ii) explains how we drew the achievable regions in the diï¬€erent
ï¬gures in this chapter. As noted earlier, some of the results rely heavily on the
assumption that the observable has a density under both hypotheses.
Theorem 30.11.1 (Achievable Pairs and Pareto-Optimal Pairs).
(i) Every pair of error probabilities that is achievable by a randomized guessing
rule can also be achieved by a deterministic one:
Rrnd = R.
(30.118)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.11 The Big Picture
773
(ii) The set R is convex and comprises all probability pairs (pFA, pMD) satisfying
pâˆ—
MD(pFA) â‰¤pMD â‰¤1 âˆ’pâˆ—
MD(1 âˆ’pFA).
(30.119)
(iii) The set of Pareto-optimal pairs comprises all pairs of the form

pFA, pâˆ—
MD(pFA)

,
0 â‰¤pFA â‰¤pâˆ—
FA(0).
(30.120)
(iv) Every likelihood-ratio test of positive ï¬nite threshold is Pareto-optimal.
(v) Every Pareto-optimal pair can be achieved by a deterministic likelihood-ratio
test. It can also be achieved by a randomized likelihood-ratio test that bases
its decision on LR(yobs) only.
Proof. Part (i) is from Proposition 30.10.1. As to Part (ii), the convexity result
is from Corollary 30.10.3. That every pair (pFA, pMD) in R must satisfy (30.119)
follows from (30.114). And that every pair (pFA, pMD) satisfying (30.119) must be
in R follows from the convexity of R, because every such pair can be written as
a convex combination of the pairs

pFA, pâˆ—
MD(pFA)

and

pFA, 1 âˆ’pâˆ—
MD(1 âˆ’pFA)

,
both of which are in R by (30.115) and (30.117).
As to Part (iii), all the pairs in (30.120) are Pareto-optimal by Lemma 30.4.1,
Lemma 30.5.4, and Lemma 30.4.2. There are no additional Pareto-optimal pairs
of false alarm in the interval [0, pâˆ—
FA(0)] by Note 30.3.1, and there are none outside
this interval by Corollary 30.4.5.
Part (iv) is from Proposition 30.5.2. We ï¬nally prove Part (v). The achievability of
every Pareto-optimal pair using a deterministic likelihood-ratio test follows from
Part (iii), Proposition 30.5.5, and Proposition 30.8.1. The achievability using a
randomized likelihood-ratio test that bases its decision only on LR(yobs) follows
from Part (iii), Lemma 30.5.4, and Proposition 30.8.1.
Theorem 30.11.2 (The Function pâˆ—
MD(Â·)).
(i) The functions pâˆ—
MD(Â·) and pâˆ—
MD,rnd(Â·) are identical:
pâˆ—
MD(Ï€FA) = pâˆ—
MD,rnd(Ï€FA),
0 â‰¤Ï€FA â‰¤1.
(30.121)
(ii) For every Ï€FA âˆˆ[0, 1], the pair

Ï€FA, pâˆ—
MD(Ï€FA)

is achievable:

Ï€FA, pâˆ—
MD(Ï€FA)

âˆˆR,
0 â‰¤Ï€FA â‰¤1.
(30.122)
(iii) For Ï€FA = 0,
pâˆ—
MD(0) =

{yâˆˆY : f0(y)>0}
f1(y) dy.
(30.123)
It is achieved by guessing â€œH = 0â€ whenever f0(yobs) is positive.
(iv) For 0 < Ï€FA < pâˆ—
FA(0), the pair

Ï€FA, pâˆ—
MD(Ï€FA)

is achievable by a determin-
istic likelihood-ratio test of positive ï¬nite threshold. It is also achievable by a
randomized likelihood-ratio test of the same threshold that bases its decision
only on LR(yobs).
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

774
The Radar Problem
(v) For pâˆ—
FA(0) â‰¤Ï€FA â‰¤1,
pâˆ—
MD(Ï€FA) = 0.
(30.124)
(vi) The function pâˆ—
MD(Â·) is monotonically nonincreasing, convex, and continuous
over the closed interval [0, 1].
Proof. Part (i) is a restatement of Corollary 30.5.6; see (30.63).
Part (ii) is
a restatement of Lemma 30.10.4; see (30.105).
Part (iii) is a restatement of
Lemma 30.4.1; see (30.24). Part (iv) is a combination of Lemma 30.5.4 and Propo-
sition 30.5.5. Part (v) is a restatement of Corollary 30.4.4; see (30.34).
We next turn to Part (vi). Monotonicity is proved by noting that increasing the
maximal-allowed false-alarm probability enlarges (or leaves unchanged) the class
of allowed decision rules in (30.8b) and hence can only decrease (or not change) the
least probability of missed-detection in the class (30.9). Convexity is a consequence
of the convexity of R and the achievability of (Ï€FA, pâˆ—
MD(Ï€FA)) (Part (ii)): If Ï€(1)
FA
and Ï€(2)
FA are two diï¬€erent false-alarm probabilities, then Part (ii) implies that

Ï€(Î½)
FA, pâˆ—
MD

Ï€(Î½)
FA

âˆˆR,
Î½ âˆˆ{1, 2},
(30.125)
and the convexity of R (Corollary 30.10.3) thus implies

Î» Ï€(1)
FA+(1âˆ’Î») Ï€(2)
FA, Î» pâˆ—
MD

Ï€(1)
FA

+(1âˆ’Î») pâˆ—
MD

Ï€(2)
FA

âˆˆR,
0 â‰¤Î» â‰¤1. (30.126)
The latter implies
pâˆ—
MD

Î» Ï€(1)
FA+(1âˆ’Î») Ï€(2)
FA

â‰¤Î» pâˆ—
MD

Ï€(1)
FA

+(1âˆ’Î») pâˆ—
MD

Ï€(2)
FA

,
0 â‰¤Î» â‰¤1, (30.127)
and thus establishes convexity.
The proof of continuity is a bit technical, so we will be terse.
Convexity over
the closed interval [0, 1] implies continuity over the open interval (0, 1) (see, for
example, (Royden and Fitzpatrick, 2010, Section 6.6, Corollary 17), so it remains
to establish continuity at the end points. We ï¬rst consider the endpoint Ï€FA = 1,
where continuity follows from monotonicity and convexity: Monotonicity implies
that
lim
Ï€FAâ†‘1 pâˆ—
MD(Ï€FA) â‰¥pâˆ—
MD(1).
(30.128)
And convexity implies that (30.128) must hold with the reverse inequality, because
if we substitute in (30.127) Ï€FA for Ï€(1)
FA, the value 1 for Ï€(2)
FA, and 1/2 for Î» we
obtain
pâˆ—
MD
Ï€FA + 1
2

â‰¤1
2 pâˆ—
MD(Ï€FA) + 1
2 pâˆ—
MD(1),
and hence
2 pâˆ—
MD
Ï€FA + 1
2

âˆ’pâˆ—
MD(Ï€FA) â‰¤pâˆ—
MD(1),
(30.129)
from which it follows upon taking the limit Ï€FA â†‘1 that
lim
Ï€FAâ†‘1 pâˆ—
MD(Ï€FA) â‰¤pâˆ—
MD(1).
(30.130)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.11 The Big Picture
775
This combines with (30.128) to establish continuity at Ï€FA = 1.
It thus remains to establish continuity at the endpoint Ï€FA = 0, i.e., to prove
lim
Ï€FAâ†“0 pâˆ—
MD(Ï€FA) = pâˆ—
MD(0).
(30.131)
Monotonicity implies that the above limit exists, and we now proceed to show
that it equals pâˆ—
MD(0). To this end, consider the likelihood-ratio test that guesses
â€œH = 0â€ whenever LR(yobs) > Ïµ, and denote its false-alarm and missed-detection
probabilities by pFA(Ïµ) and pMD(Ïµ):
pFA(Ïµ) = P0

0 â‰¤LR(Y) â‰¤Ïµ

,
(30.132)
pMD(Ïµ) = P1

LR(Y) > Ïµ

,
(30.133)
where P1[Â·] is deï¬ned as in (30.41d) but with the density f0 replaced by f1. Since
this is a likelihood-ratio test, the pair

pFA(Ïµ), pMD(Ïµ)

is Pareto-optimal (Propo-
sition 30.5.2), and hence, by (30.10),
pâˆ—
MD

pFA(Ïµ)

= pMD(Ïµ),
Ïµ > 0.
(30.134)
We next study the limits of pFA(Ïµ) and pMD(Ïµ) as Ïµ â†“0. Starting with the former,
lim
Ïµâ†“0 pFA(Ïµ) = lim
Ïµâ†“0 P0

0 â‰¤LR(Y) â‰¤Ïµ

= P0

LR(Y) = 0

= 0,
(30.135)
where the last equality follows from (30.42). As to the latter,
lim
Ïµâ†“0 pMD(Ïµ) = lim
Ïµâ†“0 P1

LR(Y) > Ïµ

= P1

LR(Y) > 0

= pâˆ—
MD(0),
(30.136)
where the last equality follows because the rule of guessing â€œH = 0â€ whenever
LR(yobs) > 0 achieves the pair

0, pâˆ—
MD(0)

(Proposition 30.8.1).
To conclude the proof of (30.131) we now compute:
lim
Ï€FAâ†“0 pâˆ—
MD(Ï€FA) = lim
Ïµâ†“0 pâˆ—
MD

pFA(Ïµ)

(30.137)
= lim
Ïµâ†“0 pMD(Ïµ)
(30.138)
= pâˆ—
MD(0),
(30.139)
where the ï¬rst equality follows from the existence of the limit on the LHS and
from (30.135); the second equality from (30.134); and where the last equality follows
from (30.136).
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

776
The Radar Problem
30.12
Relative Entropy
Any useful lower bound on the missed-detection probability must also take into
account the false-alarm probability, because we can always achieve zero pMD by
ignoring the observation and guessing that the target is present. In this section we
shall derive a necessary condition that all achievable pairs (pFA, pMD) must satisfy.
The condition will help us to understand the trade-oï¬€between the two types of
error, and it will often allow us to rule out the possibility that both are very low.
The condition will be expressed in terms of the relative entropy between the
densities of the observable under the two hypotheses.
Relative entropy, also known as Kullback-Leibler divergence or divergence,
is central to Statistics, Probability, and Information Theory. It can be deï¬ned
between any pair of probability distributions, but we will focus on two special
cases only: when both distributions have densities, and when both are deï¬ned over
a ï¬nite set. In the next deï¬nition and, in fact, throughout, we adopt the convention

0 ln 0
Î² = 0,
Î² â‰¥0

and

Î± ln Î±
0 = +âˆ,
Î± > 0

.
(30.140)
Deï¬nition 30.12.1 (Relative Entropy).
(i) Let p = (p1, . . . , pn) and q = (q1, . . . , qn) be n-tuples having nonnegative
entries that sum to one. The relative entropy is deï¬ned as
D(pâˆ¥q) â‰œ
n

j=1
pj ln pj
qj
.
(30.141)
(ii) Let f(Â·) and g(Â·) be probability density functions on Rn. The relative entropy
is deï¬ned as7
D(fâˆ¥g) â‰œ

f(x) ln f(x)
g(x) dx.
(30.142)
Relative entropy is not symmetric: D(pâˆ¥q) is typically diï¬€erent from D(qâˆ¥p).
Example 30.12.2 (The Relative Entropy between Equi-Variance Gaussians).
Suppose that f(Â·) is the density of the N

Î¼1, Ïƒ2
distribution, and g(Â·) is the
density of the N

Î¼2, Ïƒ2
distribution, where Ïƒ > 0 and Î¼1, Î¼2 âˆˆR. Then
D(fâˆ¥g) =
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼1)2
2Ïƒ2
ln
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼1)2
2Ïƒ2
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼2)2
2Ïƒ2
dx
7Some clariï¬cation is needed, because the integrand could be +âˆ(though not âˆ’âˆ). More
generally, the integral

A
f(x) ln f(x)
g(x) dx
should be interpreted as follows: If the integrand is +âˆon a subset of A of positive Lebesgue
measure, then the integral is deï¬ned as being +âˆ. (This is reasonable, because the contribution
of the other points in A to the integral is lower-bounded by âˆ’1; see Exercise 30.14.) Otherwise,
the integral is computed by excluding from A the points where the integrand is inï¬nite.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.12 Relative Entropy
777
=
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼1)2
2Ïƒ2
1
2Ïƒ2

(x âˆ’Î¼2)2 âˆ’(x âˆ’Î¼1)2
dx
=
1
2Ïƒ2
 âˆ
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 eâˆ’(xâˆ’Î¼1)2
2Ïƒ2

2x(Î¼1 âˆ’Î¼2) + Î¼2
2 âˆ’Î¼2
1

dx
=
1
2Ïƒ2

2Î¼1(Î¼1 âˆ’Î¼2) + Î¼2
2 âˆ’Î¼2
1

.
Thus,
D

N(Î¼1, Ïƒ2)
N(Î¼2, Ïƒ2)

= (Î¼1 âˆ’Î¼2)2
2Ïƒ2
,
Ïƒ > 0.
(30.143)
A key property of D(pâˆ¥q) is that it is nonnegative, and that it is zero if, and only
if, the tuples p and q are identical. Likewise for densities, D(fâˆ¥g) is nonnegative
and is equal to zero if, and only if, the densities f(Â·) and g(Â·) diï¬€er on a set of
Lebesgue measure zero. We shall only prove nonnegativity:
Theorem 30.12.3 (Relative Entropy Is Nonnegative).
(i) If p and q are as in Deï¬nition 30.12.1, then
D(pâˆ¥q) â‰¥0.
(30.144)
(ii) If f(Â·) and g(Â·) are probability densities on Rn, then
D(fâˆ¥g) â‰¥0.
(30.145)
Proof. We shall only prove Part (ii); the proof of the discrete version merely re-
quires replacing the integrals with sums. The key is the inequality
ln Î¾ â‰¤Î¾ âˆ’1,
Î¾ > 0,
(30.146)
which also holds when Î¾ is zero provided that we interpret ln 0 as âˆ’âˆ. Starting
from (30.142) and using the convention (30.140),
âˆ’D(fâˆ¥g) = âˆ’

{x: f(x)>0}
f(x) ln f(x)
g(x) dx
=

{x: f(x)>0}
f(x) ln g(x)
f(x) dx
â‰¤

{x: f(x)>0}
f(x)
 g(x)
f(x) âˆ’1

dx
=

{x: f(x)>0}
g(x) dx âˆ’

{x: f(x)>0}
f(x) dx
=

{x: f(x)>0}
g(x) dx âˆ’

Rn f(x) dx
â‰¤

Rn g(x) dx âˆ’1
= 0,
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

778
The Radar Problem
where the second line follows from the identity âˆ’ln(a/b) = ln(b/a); the third
from (30.146) by substituting g(x)/f(x) for Î¾; the fourth from the linearity of
integration; the ï¬fth because extending the domain of integration does not change
the integral if we only add points where the integrand is zero; the sixth because
f(Â·) is a density (and hence integrates to one) and because g(Â·) is nonnegative so
extending the domain of integration cannot decrease the integral; and the ï¬nal
equality because, being a density, g(Â·) integrates to one.
We call the next inequality the â€œLog-Integral Inequalityâ€ because the standard
name for its discrete counterpart is the â€œLog-Sum Inequalityâ€ (Cover and Thomas,
2006, Chapter 2, Section 7, Theorem 2.7.1).
Theorem 30.12.4 (Log-Integral Inequality). Let f(Â·) and g(Â·) be densities on Rn,
and let A be any (Lebesgue measurable) subset of Rn. Then,

A
f(x) ln f(x)
g(x) dx â‰¥
	
A
f(x) dx

ln

A f(x) dx


A g(x) dx
 .
(30.147)
Proof. Deï¬ne the constants
a =

A
f(x) dx,
b =

A
g(x) dx.
(30.148)
We shall prove the inequality for the case where both a and b are positive. (If a is
zero, then both sides of the inequality are zero, and if a is positive and b is zero,
then both sides are inï¬nite.) Deï¬ne the two densities
Ëœf(x) = 1
a f(x) I

x âˆˆA

,
Ëœg(x) = 1
b g(x) I

x âˆˆA

.
(30.149)
By the nonnegativity of D(Ëœfâˆ¥Ëœg) (Theorem 30.12.3),
0 â‰¤D(Ëœfâˆ¥Ëœg)
=

A
1
a f(x) ln aâˆ’1 f(x)
bâˆ’1 g(x) dx
=

A
1
a f(x) ln b
a dx +

A
1
a f(x) ln f(x)
g(x) dx
= ln b
a +

A
1
a f(x) ln f(x)
g(x) dx
= 1
a

A
f(x) ln f(x)
g(x) dx âˆ’ln a
b ,
from which the result follows by multiplying both sides of the inequality by a.
We next apply the Log-Integral Inequality to the radar problem. We ï¬rst consider
deterministic decision rules8 and derive two necessary conditions that their error
8Since R = Rrnd (Proposition 30.10.1), we need not consider randomized rules but, for the
beneï¬t of readers who have skipped the propositionâ€™s proof, those will be considered later too.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.12 Relative Entropy
779
probabilities (pFA, pMD) must satisfy irrespective of the set D that deï¬nes them.
One necessary condition is in terms of D(f0âˆ¥f1) and the other in terms of D(f1âˆ¥f0).
To derive the ï¬rst necessary condition we apply the Log-Integral Inequality twice.
In both applications we substitute in (30.147) the densities f0(Â·) for f(Â·) and f1(Â·)
for g(Â·). In the ï¬rst application, we substitute the decision region D for A to obtain

D
f0(y) ln f0(y)
f1(y) dy â‰¥

D
f0(y) dy

ln

D f0(y) dy


D f1(y) dy

=

1 âˆ’pFA

ln 1 âˆ’pFA
pMD
,
(30.150)
where the equality follows from the expressions (30.3b) and (30.4b) for pFA and pMD.
In the second application of the Log-Integral Inequality, we substitute Dc for A to
obtain

Dc f0(y) ln f0(y)
f1(y) dy â‰¥

Dc f0(y) dy

ln

Dc f0(y) dy


Dc f1(y) dy

= pFA ln
pFA
1 âˆ’pMD
.
(30.151)
Summing (30.150) and (30.151) yields the ï¬rst necessary condition
D(f0âˆ¥f1) â‰¥

1 âˆ’pFA

ln 1 âˆ’pFA
pMD
+ pFA ln
pFA
1 âˆ’pMD
.
(30.152)
Notice that the RHS of the above has the form of a relative entropy between two
discrete probability vectors. We can thus also express the condition as
D

f0
f1

â‰¥D

(1 âˆ’pFA, pFA)
(pMD, 1 âˆ’pMD)

.
(30.153)
Before turning to the second necessary condition, we next argue that (30.153)
also holds for randomized guessing rules. As we have seen in Section 20.6, any
randomized guessing rule that guesses â€œH = 0â€ with probability b(yobs) can be
viewed as a deterministic rule that bases its decision on an observation consisting of
the pair (yobs, Î¸obs) âˆˆRnÃ—[0, 1] and that guesses â€œH = 0â€ whenever Î¸obs â‰¤b(yobs),
where Î˜ âˆ¼U

[0, 1]

is the outcome of a uniform random-number generator that is
independent of the observation so the corresponding two densities are
f0(y) I{0 â‰¤Î¸ â‰¤1},
f1(y) I{0 â‰¤Î¸ â‰¤1}.
(30.154)
The relative entropy between these two densities is
 1
0

Rn f0(y) I{0 â‰¤Î¸ â‰¤1} ln f0(y) I{0 â‰¤Î¸ â‰¤1}
f1(y) I{0 â‰¤Î¸ â‰¤1} dy dÎ¸
=
 1
0

Rn f0(y) ln f0(y)
f1(y) dy dÎ¸
=
 1
0
D(f0âˆ¥f1) dÎ¸
= D(f0âˆ¥f1).
(30.155)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

780
The Radar Problem
Applying the result on deterministic rules to the setting where the densities are
those in (30.154) and using (30.155) establishes the bound for randomized rules.
Condition (30.152) was derived from the Log-Integral Inequality by substituting
the densities f0(Â·) for f(Â·) and f1(Â·) for g(Â·). A diï¬€erent condition can be derived
in a similar way by swapping f0(Â·) and f1(Â·), i.e., by substituting in (30.147) f1(Â·)
for f(Â·) and f0(Â·) for g(Â·). If we substitute the decision region D for A we obtain

D
f1(y) ln f1(y)
f0(y) dy â‰¥

D
f1(y) dy

ln

D f1(y) dy


D f0(y) dy

= pMD ln
pMD
1 âˆ’pFA
,
(30.156)
and if we substitute Dc for A we obtain

Dc f1(y) ln f1(y)
f0(y) dy â‰¥

Dc f1(y) dy

ln

Dc f1(y) dy


Dc f0(y) dy

= (1 âˆ’pMD) ln 1 âˆ’pMD
pFA
.
(30.157)
Summing (30.156) and (30.157) leads to our second necessary condition
D(f1âˆ¥f0) â‰¥pMD ln
pMD
1 âˆ’pFA
+ (1 âˆ’pMD) ln 1 âˆ’pMD
pFA
= D

(pMD, 1 âˆ’pMD)
(1 âˆ’pFA, pFA)

.
(30.158)
Like (30.152) this bound too can be extended to randomized rules. To summarize:
Theorem 30.12.5 (Necessary Conditions on (pFA, pMD)). If (pFA, pMD) is achiev-
able by some possibly-randomized decision rule, then

1 âˆ’pFA

ln 1 âˆ’pFA
pMD
+ pFA ln
pFA
1 âˆ’pMD
â‰¤D(f0âˆ¥f1),
(30.159)
and
pMD ln
pMD
1 âˆ’pFA
+ (1 âˆ’pMD) ln 1 âˆ’pMD
pFA
â‰¤D(f1âˆ¥f0).
(30.160)
As an example of the use of Theorem 30.12.5, we next revisit the Gaussian problem
we discussed in Section 30.6. Using the expression for the relative entropy between
two Gaussians (30.143) we conclude that in this example D(f0âˆ¥f1) happens to equal
D(f1âˆ¥f0) and both are equal to A2/(2Ïƒ2). We thus conclude from the theorem that,
for this problem,

1 âˆ’pFA

ln 1 âˆ’pFA
pMD
+ pFA ln
pFA
1 âˆ’pMD
â‰¤A2
2Ïƒ2
(30.161a)
and
pMD ln
pMD
1 âˆ’pFA
+ (1 âˆ’pMD) ln 1 âˆ’pMD
pFA
â‰¤A2
2Ïƒ2 .
(30.161b)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.13 Additional Reading
781
1
1
1
1
pMD
pMD
pFA
pFA
(a)
(b)
Figure 30.6: The achievable pairs for the Gaussian problem of Section 30.6. Lightly
shaded in (a) are the pairs that satisfy (30.161a), and lightly shaded in (b) are those
that satisfy (30.161b).
These necessary conditions are depicted in Figure 30.6. The darkly-shaded area cor-
responds to the achievable pairs. Lightly shaded are the pairs that satisfy (30.161a)
(Figure 30.6 (a)) and those that satisfy (30.161b) (Figure 30.6 (b)). In this very
special case, D(f1âˆ¥f0) is equal to D(f0âˆ¥f1), so Condition (30.161b) is identical to
the result of swapping pMD and pFA in (30.161a). The lightly-shaded regions in
Figure 30.6 can thus be obtained from each other by reï¬‚ection with respect to the
slope-1 line passing through the origin.
30.13
Additional Reading
We have not considered the radar problem for signals with unknown parameters.
Good starting points for the literature on this are (Poor, 1994) and (Helstrom,
1995). For more on the Knapsack Problem see (Cormen, Leiserson, Rivest, and
Stein, 2009, Chapter 16, Section 2). For various extensions of Liapunovâ€™s Theorem
see (Karlin and Studden, 1966, Chapter VIII, Section 12), and for a simpliï¬ed
proof see (Ross, 2005). An accessible account of the role of relative entropy in
Information Theory, Statistics, and Probability Theory can be found in (Cover
and Thomas, 2006).
30.14
Exercises
Exercise 30.1 (A Degenerate Radar Problem). Consider a degenerate radar problem
where the two densities f0(Â·) and f1(Â·) are identical.
Find all the achievable pairs
(pFA, pMD) and all the Pareto-optimal pairs.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

782
The Radar Problem
Exercise 30.2 (Unimpressive Achievable Pairs). Show thatâ€”irrespective of the densi-
ties f0(Â·) and f1(Â·)â€”any pair (pFA, pMD) satisfying pFA + pMD = 1 is achievable by a
randomized guessing rule.
Exercise 30.3 (Testing between Exponentials). Find all the Pareto-optimal pairs for the
one-dimensional radar problem with the exponential densities
f0(y) = eâˆ’y I

y â‰¥0

,
f1(y) = 2eâˆ’2y I

y â‰¥0

.
Exercise 30.4 (A Diï¬€erent Optimality Criterion). For given positive numbers Î±, Î² > 0,
which guessing rule minimizes Î± pFA + Î² pMD?
Exercise 30.5 (An Odd Radar Problem). Consider the one-dimensional radar problem
with the uniform densities
f0(y) = I

0 < y < 1

,
f1(y) = 2 I
 1
2 < y < 1
!
.
(i) Find all achievable pairs (pFA, pMD).
(ii) Find all Pareto-optimal pairs (pâˆ—
FA, pâˆ—
MD).
(iii) Compute pâˆ—
MD(0.75).
(iv) Is the pair

0.75, pâˆ—
MD(0.75)

Pareto optimal?
Illustrate your answers pictorially.
Hint: How do pFA and pMD depend on the set D?
Exercise 30.6 (On the Optimality Criteria). Prove that the RHS of (30.10) does not
imply its LHS by providing an example of a radar problem where for some Ï€FA, say 0.1,
the pair

Ï€FA, pâˆ—
MD(Ï€FA)

is not Pareto optimal.
Hint: Consider a case where f0(Â·) and f1(Â·) are both uniform, but on disjoint subsets of R.
Exercise 30.7 (Convex Combinations of Achievable Pairs). Let Ï†(1)
Guess(Â·) and Ï†(2)
Guess(Â·)
be deterministic guessing rules achieving the pairs (p(1)
FA, p(1)
MD) and (p(2)
FA, p(2)
MD), and let
Î» âˆˆ(0, 1) be arbitrary. Find a randomized guessing rule achieving the pair
	
Î» p(1)
FA + (1 âˆ’Î») p(2)
FA, Î» p(1)
MD + (1 âˆ’Î») p(2)
MD

.
Repeat when Ï†(1)
Guess and Ï†(2)
Guess are randomized guessing rules determined by b(1)(Â·) and
b(2)(Â·).
Exercise 30.8 (Two Pairs Minimize A Linear Functional). Suppose that Î± and Î² are posi-
tive and that both

p(1)
FA, p(1)
MD

and

p(2)
FA, p(2)
MD

minimize Î± pFA +Î² pMD over all achievable
pairs (pFA, pMD). Prove that both pairs are Pareto optimal and that so is every pair on
the line segment connecting them, i.e., every pair of the form
	
Î» p(1)
FA + (1 âˆ’Î») p(2)
FA, Î» p(1)
MD + (1 âˆ’Î») p(2)
MD

,
Î» âˆˆ(0, 1).
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.14 Exercises
783
Exercise 30.9 (Achievable Regions with Corners). Find a one-dimensional radar problem
with f0(y) = I{0 â‰¤y â‰¤1} whose achievable region is the one depicted in Figure 30.3 (b).
Assume that the point marked by a square is (0.375, 0.25) or, more generally, (Î±, Î²). Are
there other (correct) solutions?
Exercise 30.10 (The Minimax Criterion). Show that if (pâ‹†, pâ‹†) is a Pareto-optimal pair
of equal false-alarm and missed-detection probabilities, then it minimizes max{pFA, pMD}
over all achievable pairs in the sense that for every achievable pair (pFA, pMD)
pâ‹†= max{pâ‹†, pâ‹†} â‰¤max{pFA, pMD}.
Argue that there exists a likelihood-ratio test that minimizes max{pFA, pMD} over all
achievable pairs and that (pâ‹†, pâ‹†) is the intersection of the curve (Ï€FA, pâˆ—
MD(Ï€FA)) with
the unit-slope line through the origin.
Exercise 30.11 (Giving Up). Suppose we allow guessing rules that can guess â€œH = 0,â€
â€œH = 1â€, or â€œI give up,â€ where â€œI give upâ€ counts as an error irrespective of whether or
not the target is present. Characterize the achievable pairs (pFA, pMD).
Exercise 30.12 (The Relative Entropy between Exponentials). Compute the relative
entropy between the mean-Î¼1 and the mean-Î¼2 exponential distributions.
Exercise 30.13 (Invariance under Rotation). The set of achievable pairs (pFA, pMD) is
invariant under rotation by 180 degrees around the point (1/2, 1/2) (Proposition 30.10.5).
Is the same true about the pairs that satisfy the necessary conditions of Theorem 30.12.5?
Exercise 30.14 (A Technicality Related to Relative Entropy). Prove that if f(Â·) and g(Â·)
are densities on Rn and A âŠ‚Rn is measurable, then

A
f(x) ln f(x)
g(x) dx â‰¥âˆ’1.
In fact, the above integral is lower-bounded by âˆ’1/e.
Exercise 30.15 (Relative RÂ´enyi Entropy Is Nonnegative). Let Î± Ì¸= 1 be positive. The
relative RÂ´enyi entropy of order Î± between two probability vectors p and q (as in Deï¬ni-
tion 30.12.1) is
DÎ±(pâˆ¥q) â‰œ
1
Î± âˆ’1 log

n

j=1
pÎ±
j q1âˆ’Î±
j

.
(30.162)
Between two densities f(Â·) and g(Â·) it is
DÎ±(fâˆ¥g) â‰œ
1
Î± âˆ’1 log

f(x)Î± g(x)1âˆ’Î± dx

.
(30.163)
Prove that both are always nonnegative.
Hint: Treat the cases 0 < Î± < 1 and Î± > 1 separately using HÂ¨olderâ€™s Inequality.
Exercise 30.16 (Relative RÂ´enyi Entropy and Relative Entropy). Show that, as Î± tends
to 1, the relative RÂ´enyi entropy of order Î± (30.162) converges to the relative entropy:
lim
Î±â†’1 DÎ±(pâˆ¥q) = D(pâˆ¥q).
(30.164)
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

784
The Radar Problem
Exercise 30.17 (Log-Integral Inequality for Relative RÂ´enyi Entropy). Let f(Â·) and g(Â·) be
densities on Rn, and let Î± be positive but not equal to 1. Prove that for any (Lebesgue
measurable) A âŠ†Rn
1
Î± âˆ’1 log

A
f(x)Î± g(x)1âˆ’Î± dx

â‰¥
1
Î± âˆ’1 log
	
A
f(x) dx

Î±	
A
g(x) dx

1âˆ’Î±
.
(30.165)
Hint: Recall Exercise 30.15 and the proof of Theorem 30.12.4.
Exercise 30.18 (Relative RÂ´enyi Entropy and the Radar Problem). Show that if (pFA, pMD)
is achievable then for every positive Î± Ì¸= 1,
DÎ±

f0
f1

â‰¥DÎ±

(1 âˆ’pFA, pFA)
(pMD, 1 âˆ’pMD)

(30.166)
and
DÎ±(f1âˆ¥f0) â‰¥DÎ±

(pMD, 1 âˆ’pMD)
(1 âˆ’pFA, pFA)

.
(30.167)
Hint: Recall Exercise 30.17 and mimic the derivations of (30.153) and (30.158).
Exercise 30.19 (Errors and Erasures). Consider the multi-hypothesis testing setting of
Theorem 21.3.1 but where the guessing rule is a mapping Ï†Guess : Rd â†’M âˆª{0} with
the understanding that if Ï†Guess(yobs) is some m âˆˆM then the guess is â€œM = mâ€,
whereas if Ï†Guess(yobs) is zero then the decoder declares â€œerasureâ€. Given such a guessing
rule, deï¬ne for every m âˆˆM the set Dm = {y âˆˆRd : Ï†Guess(y) = m}, and deï¬ne
D0 = {y âˆˆRd : Ï†Guess(y) = 0}.
(i) Show that
Pr(erasure) =

mâˆˆM
Ï€m

Rd fY|M=m(y) I

y âˆˆD0

dy,
and
Pr(error) = 1 âˆ’

mâˆˆM
Ï€m

Rd fY|M=m(y) I

y âˆˆDm

dy
âˆ’

mâˆˆM
Ï€m

Rd fY|M=m(y) I

y âˆˆD0

dy.
(ii) Fix some Î» â‰¥0, and let â€œM = mâ€ be the guess produced by the MAP rule
(Section 21.3.4). Show that a rule that minimizes Pr(error) + Î» Pr(erasure) is to
guess like the MAP if
Ï€mfY|M=m(y)
Ï€mfY|M=m(y) + 
mâ€²Ì¸=m Ï€mâ€²fY|M=mâ€²(y) â‰¥1 âˆ’Î»
and to declare â€œerasureâ€ otherwise.
Exercise 30.20 (The Radar Problem and Multi-Hypothesis Testing). Consider the multi-
hypothesis testing setup of Section 21.2. Let Ï†Guess be any guessing rule and Dm comprise
the observables that result in the guess â€œM = mâ€, i.e., Dm =

y âˆˆRd : Ï†Guess(y) = m

.
Let q(Â·) be any probability density function on Rd.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,

30.14 Exercises
785
(i) Show that for some Ëœm âˆˆM

D Ëœ
m
q(y) dy â‰¤1
M.
(ii) Let Pr(error|M = Ëœm) be the error probability associated with Ï†Guess and Ëœm. Con-
sider now the radar problem with f0(Â·) being q(Â·) and with f1(Â·) being fY|M= Ëœ
m.
Show that, for this radar problem, the guessing rule that guesses â€œH = 1â€ whenever
y âˆˆD Ëœ
m and guesses â€œH = 0â€ otherwise has
pFA =

D Ëœ
m
q(y) dy,
pMD = Pr(error|M = Ëœm).
(iii) Conclude that
Pr(error|M = Ëœm) â‰¥pâˆ—
MD
 1
M

,
where pâˆ—
MD is calculated for the above radar problem.
available at 
.032
15:01:42, subject to the Cambridge Core terms of use,
www.ebook3000.com

Chapter 31
A Glimpse at Discrete-Time Signal
Processing
Once we have reduced the detection problem from one where the observable is a
continuous-time SP to one where it is a random vector, we can harness the power of
digital signal processing to form our guess. This approach is particularly suitable
for intersymbol interference channelsâ€”â€œISI channelsâ€â€”which we shall encounter in
Chapter 32, and where the required digital signal processing is quite sophisticated.
The more basic and more general elements of discrete-time signal processing are
treated here. The main concepts are similar to their continuous-time counterparts,
so the reading should be fairly easy.
31.1
Discrete-Time Filters
In Chapter 5 we discussed the convolution between waveforms and the result of ï¬l-
tering them. Here we discuss the analogous results for bi-inï¬nite sequences. In view
of the notation we adopted for discrete-time stochastic processes (Section 12.2), we
denote the bi-inï¬nite sequence . . . , aâˆ’2, aâˆ’1, a0, a1, a2, . . . by

aÎ½, Î½ âˆˆZ

or simply
by (aÎ½). But sometimes we use Î½ 	â†’aÎ½ because a bi-inï¬nite sequence is a mapping
from the integers to the real or complex ï¬eld. For this reason we shall also some-
times denote the sequence by a. In that case the sequence ~a maps Î½ to aâˆ’Î½ and is
thus the â€œmirror imageâ€ of a.
We say that a sequence (aÎ½) is absolutely summable if
âˆ

Î½=âˆ’âˆ
|aÎ½| < âˆ.
Here |Â·| denotes the absolute value for real sequences and the modulus for complex
sequences. The class of all absolutely summable sequences is denoted â„“1. This
notation does not make it explicit whether the sequences are real or complex. A
sequence (aÎ½) is square summable if
âˆ

Î½=âˆ’âˆ
|aÎ½|2 < âˆ,
786
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.1 Discrete-Time Filters
787
and the class of square-summable sequences is denoted â„“2. Note that
â„“1 âŠ‚â„“2,
(31.1)
because if a sequence is in â„“1 then all but a ï¬nite number of its elements are of
modulus smaller than one, and if a modulus is smaller than one, then it cannot
exceed its square:
|Î¾|2 â‰¤|Î¾|,

|Î¾| < 1

.
The inclusion in (31.1) is strict: there are sequences, such as Î½ 	â†’1/(|Î½|+1), which
are in â„“2 but not in â„“1.
The convolution of the bi-inï¬nite sequences (aÎ½) and (bÎ½) is a bi-inï¬nite sequence
that we denote (aÎ½) â‹†(bÎ½) or a â‹†b. Its Î·-th symbol is denoted

(aÎ½) â‹†(bÎ½)

Î·
or
(a â‹†b)Î·
and is formally given by
(a â‹†b)Î· =
âˆ

Î½=âˆ’âˆ
aÎ½ bÎ·âˆ’Î½.
(31.2)
The convolution of two square-summable sequences (aÎ½), (bÎ½) âˆˆâ„“2 is deï¬ned at
every Î· âˆˆZ and is a bounded sequence. Indeed, the Cauchy-Schwarz Inequality
for sequences (Exercise 3.12) shows that for sequences (aÎ½), (bÎ½) âˆˆâ„“2 and any Î· âˆˆZ
the sequence Î½ 	â†’aÎ½ bÎ·âˆ’Î½ is absolutely summable and its sum, which is (a â‹†b)Î·,
can be bounded by
(a â‹†b)Î·
 â‰¤
	 
Î½âˆˆZ
|aÎ½|2

1/2	 
Î½âˆˆZ
|bÎ½|2

1/2
,
Î· âˆˆZ.
(31.3)
Since â„“1 âŠ‚â„“2, the convolution of two absolutely summable sequences is also always
deï¬ned. In fact, as we next show, the result is absolutely summable with

Î·âˆˆZ
(a â‹†b)Î·
 â‰¤
	 
Î½âˆˆZ
|aÎ½|

	 
Î½âˆˆZ
|bÎ½|

.
(31.4)
This can be proved using Fubiniâ€™s Theorem, because

Î·âˆˆZ
(a â‹†b)Î·
 =

Î·âˆˆZ


Î½âˆˆZ
aÎ½ bÎ·âˆ’Î½

â‰¤

Î·âˆˆZ

Î½âˆˆZ
|aÎ½| |bÎ·âˆ’Î½|
=

Î½âˆˆZ
|aÎ½|

Î·âˆˆZ
|bÎ·âˆ’Î½|
=
	 
Î½âˆˆZ
|aÎ½|

	 
Î·â€²âˆˆZ
|bÎ·â€²|

,
where in the last equality we deï¬ned Î·â€² â‰œÎ· âˆ’Î½.
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

788
A Glimpse at Discrete-Time Signal Processing
All the properties listed in Theorem 5.6.1 hold whenever the sequences involved
are in â„“1. To those we add that if (eÎ½) is the sequence deï¬ned by
eÎ½ â‰œI{Î½ = 0},
Î½ âˆˆZ,
(31.5)
then
(aÎ½) â‹†(eÎ½) = (aÎ½).
(31.6)
In analogy to Deï¬nition 11.2.1, we deï¬ne the self-similarity sequence as follows:
Deï¬nition 31.1.1 (Self-Similarity Sequence). The self-similarity sequence Raa(Â·)
of a (possibly-complex) square-summable sequence a is deï¬ned as
Raa(Î·) â‰œ

Î½âˆˆZ
aÎ½+Î· aâˆ—
Î½,
Î· âˆˆZ.
(31.7)
A
discrete-time ï¬lter of impulse response (hÎ½) is a device that when fed
the input sequence (aÎ½) produces the output sequence (aÎ½) â‹†(hÎ½). A discrete-time
ï¬lter is said to be stable if its impulse response is absolutely summable. It is said
to be causal if hÎ½ is zero whenever Î½ < 0.
The Discrete-Time Fourier Transform (DTFT) of an absolutely summable
sequence a âˆˆâ„“1 is discussed in Appendix B. It is denoted ua and is the mapping
from R to C
ua: Î¸ 	â†’

Î½âˆˆZ
aÎ½ eâˆ’i2Ï€Î½Î¸.
The frequency response of a stable discrete-time ï¬lter of impulse response (hÎ½)
is the Discrete-Time Fourier Transform of its impulse response: it is the mapping
from R to C
uh: Î¸ 	â†’

Î½âˆˆZ
hÎ½ eâˆ’i2Ï€Î½Î¸.
(31.8)
This function is continuous and periodic with its value at Î¸ + 1 being equal to its
value at Î¸. Consequently, it is often restricted to the interval I, where, as in (A.1),
I â‰œ
'
Î¸ âˆˆR : âˆ’1
2 â‰¤Î¸ < 1
2
(
.
(31.9)
From its restriction to I we can, of course, recover its value everywhere via the
periodic extension: The periodic extension SP of a function S: I â†’C, is a
function that agrees with S on I and that satisï¬es SP(Î¸ + 1) = SP(Î¸) for every
Î¸ âˆˆR. Thus,
SP(Î½ + Î¸) = S(Î¸),

Î¸ âˆˆI, Î½ âˆˆZ

.
(31.10)
We also deï¬ne
Â¯I â‰œ
'
Î¸ âˆˆR : âˆ’1
2 â‰¤Î¸ â‰¤1
2
(
.
(31.11)
A stable discrete-time ï¬lter of impulse response (hÎ½) is said to have a stable
inverse if there exists a sequence (aÎ½) âˆˆâ„“1 such that (hÎ½) â‹†(aÎ½) =

eÎ½

, i.e.,

(hÎ½) â‹†(aÎ½)

Î· = I{Î· = 0},
Î· âˆˆZ.
The following theorem on stable inverses is due to Norbert Wiener (1894â€“1964):
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.2 Processing Discrete-Time Stochastic Processes
789
Theorem 31.1.2 (Existence of a Stable Inverse). If the frequency response of a
stable discrete-time ï¬lter is never zero, then the ï¬lter has a stable inverse.
Proof. See (Rudin, 1987, Chapter 18, Theorem 18.21).
31.2
Processing Discrete-Time Stochastic Processes
We next discuss linear functionals of discrete-time stochastic processes as well as the
result of ï¬ltering such processes. We treat the real and complex cases separately,
with the latter addressed in Section 31.4.
31.2.1
Linear Functionals of Discrete-Time SPs
If an absolutely summable sequence is multiplied term-by-term by a bounded se-
quence, then the result is also absolutely summable. But what if the latter is only
bounded in some probabilistic sense? The next proposition shows that this is often
enough. In this proposition we require that the SP (XÎ½) have a bounded second
moment, i.e., that there exist some Î³ > 0 that upper-bounds E[X2
Î½] for every
Î½ âˆˆZ:
E

X2
Î½

â‰¤Î³,
Î½ âˆˆZ.
(31.12)
Note 31.2.1. Every WSS SP is also of bounded second moment.
Proof. If

XÎ½

is WSS, then
E

X2
Î½

=

E[XÎ½]
2 + Var[XÎ½]
=

E[X0]
2 + Var[X0] ,
and (31.12) thus holds (with equality) with
Î³ =

E[X0]
2 + Var[X0] .
The following proposition is the discrete-time counterpart of Proposition 25.10.1.
Proposition 31.2.2 (Linear Functional of a Discrete-Time SP). Let (XÎ½) be a SP
of bounded second moment, and let (aÎ½) be an absolutely summable real sequence.
(i) The set
N =
1
Ï‰ âˆˆÎ© :

Î½âˆˆZ
aÎ½ XÎ½(Ï‰)
 = âˆ
2
is an event of probability zero.
(ii) The mapping from Î© to R
Y (Ï‰) =
â§
â¨
â©

Î½âˆˆZ
aÎ½ XÎ½(Ï‰)
if Ï‰ /âˆˆN,
0
otherwise
(31.13)
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

790
A Glimpse at Discrete-Time Signal Processing
is a random variable of mean
E[Y ] =
âˆ

Î½=âˆ’âˆ
aÎ½ E[XÎ½]
(31.14)
and of second moment
E

Y 2
=

Î½â€²âˆˆZ

Î½â€²â€²âˆˆZ
aÎ½â€² aÎ½â€²â€² E[XÎ½â€²XÎ½â€²â€²] .
(31.15)
(iii) If

XÎ½

is WSS and of autocovariance function KXX(Â·), then the mean and
variance of the RV Y above are given by
E
 
Î½âˆˆZ
aÎ½ XÎ½

= E[X0]

Î½âˆˆZ
aÎ½,
(31.16)
Var
 
Î½âˆˆZ
aÎ½ XÎ½

=

Î½â€²âˆˆZ

Î½â€²â€²âˆˆZ
aÎ½â€² aÎ½â€²â€²KXX(Î½â€² âˆ’Î½â€²â€²)
(31.17)
=

Î·âˆˆZ
KXX(Î·) Raa(Î·),
(31.18)
where Raa is the self-similarity sequence of (aÎ½) (Deï¬nition 31.1.1).
(iv) If

XÎ½

is WSS and of PSD SXX(Â·) (Deï¬nition 13.6.1), then the variance
can be expressed as
Var
 
Î½âˆˆZ
aÎ½XÎ½

=

1
2
âˆ’1
2
SXX(Î¸)

âˆ

Î½=âˆ’âˆ
aÎ½ eâˆ’i2Ï€Î¸Î½

2
dÎ¸
(31.19)
=

1
2
âˆ’1
2
SXX(Î¸)
ua(Î¸)
2 dÎ¸.
(31.20)
Proof. The proof is almost identical to the proof of Proposition 25.10.1 and mostly
requires replacing integrals with sums. It is thus omitted.
We will need the following corollary when we discuss the time-Î· value

(XÎ½)â‹†(hÎ½)

Î·
of the convolution (XÎ½) â‹†(hÎ½). It shows that we can ï¬nd an event of probability
zero outside of which the convolution sum converges at all (integer) epochs: â€œone
null event ï¬ts all.â€ It has no continuous-time counterpart because it relies heavily
on the fact that, when time is discrete, the set of epochs is countable.
Corollary 31.2.3. If

XÎ½

is of bounded second moment, and if h âˆˆâ„“1, then

Î·âˆˆZ
1
Ï‰ âˆˆÎ© :

Î½âˆˆZ
XÎ½(Ï‰) hÎ·âˆ’Î½
 = âˆ
2
(31.21)
is an event of probability zero.
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.2 Processing Discrete-Time Stochastic Processes
791
Proof. For any Î· âˆˆZ we can substitute Î½ 	â†’hÎ·âˆ’Î½ for

aÎ½

in Proposition 31.2.2
to obtain from Part (i) that the event
1
Ï‰ âˆˆÎ© :

Î½âˆˆZ
XÎ½(Ï‰) hÎ·âˆ’Î½
 = âˆ
2
has probability zero. And since the union of a countable collection of events having
probability zero is also of probability zero (Corollary 21.5.2 (i)), the event in (31.21)
must also have probability zero.
31.2.2
Filtering Discrete-Time SPs
With the aid of Corollary 31.2.3, we can now deï¬ne the convolution between a WSS
SP and a deterministic absolutely summable sequence, i.e., the response of a stable
ï¬lter to a WSS input. This is the discrete-time counterpart to Deï¬nition 25.13.1.
Deï¬nition 31.2.4 (Filtering a Discrete-Time SP). The convolution (XÎ½) â‹†(hÎ½)
between a discrete-time SP of bounded second moment (XÎ½) and an absolutely
summable sequence (hÎ½) is the SP (YÎ½) that is deï¬ned as
YÎ·(Ï‰) =
â§
â¨
â©

Î½âˆˆZ
XÎ½(Ï‰) hÎ·âˆ’Î½
if Ï‰ /âˆˆN,
0
otherwise,
(31.22a)
where
N =

Î·âˆˆZ
1
Ï‰ âˆˆÎ© :

Î½âˆˆZ
XÎ½(Ï‰) hÎ·âˆ’Î½
 = âˆ
2
.
(31.22b)
As in Deï¬nition 25.13.1, the deï¬nition of the convolution for Ï‰ in N is arbitrary
and was chosen for concreteness. Ignoring the minor technical issue of how the
convolution is deï¬ned when Ï‰ âˆˆN, the convolution is â€œalmost linearâ€ in the sense
of Lemma 25.10.3. This minor technicality will be ignored in the future.
As we next argue, the convolution is â€œalmostâ€ associative. By this we mean that for
Ï‰â€™s outside an event whose probability is zero, the following two calculations yield
identical sequences. In the ï¬rst calculation we convolve the sequence Î½ 	â†’XÎ½(Ï‰)
with (hÎ½) and then convolve the result with the sequence (gÎ½).
In the second
we convolve Î½ 	â†’XÎ½(Ï‰) with the result of convolving (hÎ½) with (gÎ½), i.e., with
(hÎ½) â‹†(gÎ½).
Proposition 31.2.5 (The Convolution Is â€œalmostâ€ Associative). Let

XÎ½

be of
bounded second moment, and let the deterministic sequences h and g be absolutely
summable. Then there exists an event N of probability zero such that for every
Ï‰ /âˆˆN

Î½ 	â†’XÎ½(Ï‰)

â‹†h

â‹†g =

Î½ 	â†’XÎ½(Ï‰)

â‹†(h â‹†g).
(31.23)
Proof. Since h and g are absolutely summable, so are the sequences Î½ 	â†’|hÎ½| and
Î½ 	â†’|gÎ½|, which we denote |h| and |g|. Consequently, their convolution is also
absolutely summable, i.e.,
|h| â‹†|g| âˆˆâ„“1,
(31.24)
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

792
A Glimpse at Discrete-Time Signal Processing
(because the convolution of absolutely summable sequences is absolutely summable;
see (31.4)). It therefore follows from Corollary 31.2.3 (upon substituting |h| â‹†|g|
for h there) that the event
N =

Î·âˆˆZ
1
Ï‰ âˆˆÎ© :

Î½âˆˆZ
XÎ½(Ï‰)



|h| â‹†|g|

Î·âˆ’Î½
 = âˆ
2
(31.25)
is of probability zero.1 For Ï‰ /âˆˆN we have

Î½âˆˆZ
XÎ½(Ï‰)
 
|h| â‹†|g|

Î·âˆ’Î½ < âˆ,
Î· âˆˆZ,
i.e.,

Î½âˆˆZ
XÎ½(Ï‰)
 
mâˆˆZ
hm
gÎ·âˆ’Î½âˆ’m
 < âˆ,

Î· âˆˆZ, Ï‰ /âˆˆN

,
or

Î½âˆˆZ

mâˆˆZ
XÎ½(Ï‰) hm gÎ·âˆ’Î½âˆ’m
 < âˆ,

Î· âˆˆZ, Ï‰ /âˆˆN

.
Replacing the summation variable m with k, where k = m + Î½, we obtain

Î½âˆˆZ

kâˆˆZ
XÎ½(Ï‰) hkâˆ’Î½ gÎ·âˆ’k
 < âˆ,

Î· âˆˆZ, Ï‰ /âˆˆN

.
(31.26)
This and Fubiniâ€™s Theorem justify swapping the sums in the following calculation:
	
Î½ 	â†’XÎ½(Ï‰)

â‹†h

â‹†g

Î·
=

kâˆˆZ

Î½ 	â†’XÎ½(Ï‰)

â‹†h

k gÎ·âˆ’k
=

kâˆˆZ
	
Î½âˆˆZ
XÎ½(Ï‰) hkâˆ’Î½

gÎ·âˆ’k
=

Î½âˆˆZ

kâˆˆZ
XÎ½(Ï‰) hkâˆ’Î½ gÎ·âˆ’k
=

Î½âˆˆZ
XÎ½(Ï‰)

kâˆˆZ
hkâˆ’Î½ gÎ·âˆ’k
=

Î½âˆˆZ
XÎ½(Ï‰)

mâˆˆZ
hm gÎ·âˆ’Î½âˆ’m
=

Î½âˆˆZ
XÎ½(Ï‰)

h â‹†g

Î·âˆ’Î½
=

Î½ 	â†’XÎ½(Ï‰)

â‹†(h â‹†g)

Î·,

Î· âˆˆZ, Ï‰ /âˆˆN

,
where we have changed the summation argument by deï¬ning m â‰œk âˆ’Î½.
The following corollary is very important. It says that passing a SP through a
stable ï¬lter that has a stable inverse is essentially a lossless operation.
1The convolution of nonnegative sequences is nonnegative, so taking the absolute value of

|h| â‹†|g|

Î·âˆ’Î½ in (31.25) is superï¬‚uous.
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.2 Processing Discrete-Time Stochastic Processes
793
Corollary 31.2.6. If (XÎ½) is of bounded second moment and (hÎ½) is the impulse
response of a stable ï¬lter that has a stable inverse, then (XÎ½) can be recovered from
(XÎ½) â‹†(hÎ½) with probability one.
Proof. Apply Proposition 31.2.5 with (gÎ½) being the impulse response of the inverse
ï¬lter, i.e., with (gÎ½) chosen so (hÎ½) â‹†(gÎ½) = (eÎ½).
31.2.3
Discrete-Time Gaussian SPs
Discrete-time Gaussian stochastic processes are deï¬ned like their continuous-time
counterparts (Deï¬nition 25.3.1):
Deï¬nition 31.2.7 (Discrete-Time Gaussian SP). A discrete-time SP

XÎ½

is said
to be Gaussian if for every positive integer m and every choice of the integers
Î½1, . . . , Î½m, the random vector

XÎ½1, . . . , XÎ½m
T
is a Gaussian vector.
In analogy to Proposition 25.11.1 and Theorem 25.12.1 we have:
Theorem 31.2.8 (Linear Functionals of a Discrete-Time Gaussian SP). Let

XÎ½

be a Gaussian SP of bounded second moment.
(i) If (aÎ½) is an absolutely summable real sequence, then

Î½âˆˆZ
aÎ½ XÎ½
is a Gaussian RV.
(ii) If the m bi-inï¬nite real sequences

a1,Î½

, . . . ,

am,Î½

are absolutely summable,
then the m random variables

Î½âˆˆZ
a1,Î½ XÎ½, . . . ,

Î½âˆˆZ
am,Î½ XÎ½
are jointly Gaussian.
Proof. To prove Part (i) we could mimic the proof of Proposition 25.11.1, but
there is a simpler proof: For any n âˆˆN deï¬ne
Sn â‰œ
n

Î½=âˆ’n
aÎ½ XÎ½.
Since

XÎ½

is a Gaussian SP, the vector (Xâˆ’n, . . . , Xn)T is Gaussian (Deï¬ni-
tion 31.2.7), and consequently Snâ€”being a linear functional of a Gaussian vectorâ€”
is a Gaussian RV (Theorem 23.6.17). Since, by Proposition 31.2.2, Sn converges
(almost surely) to

Î½âˆˆZ
aÎ½ XÎ½,
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

794
A Glimpse at Discrete-Time Signal Processing
the latter must also be Gaussian (Theorem 19.9.1).
Part (ii) can be proved from Part (i) in much the same way that Theorem 25.12.1
was proved using Proposition 25.11.1. Alternatively it can be proved as we proved
Part (i) but using Proposition 23.6.3 (instead of Theorem 23.6.17) and Theo-
rem 23.9.1 (instead of Theorem 19.9.1).
Theorem 31.2.9 (Filtering a Discrete-Time SP). Let (XÎ½) be a centered WSS
discrete-time SP of autocovariance function KXX, and let (hÎ½) be an absolutely
summable real bi-inï¬nite sequence. Let

YÎ½

=

XÎ½

â‹†

hÎ½

.
(31.27)
(i) The discrete-time SP (YÎ½) is a centered WSS SP whose autocovariance func-
tion KYY is
KYY = KXX â‹†Rhh,
(31.28)
where Rhh is the self-similarity sequence of

hÎ½

.
(ii) If (XÎ½) is of PSD SXX, then (YÎ½) is of PSD
SYY (Î¸) = SXX(Î¸)

âˆ

Î½=âˆ’âˆ
hÎ½ eâˆ’i2Ï€Î½Î¸

2
(31.29)
= SXX(Î¸)
uh(Î¸)
2,
Î¸ âˆˆR.
(31.30)
(iii) If (XÎ½) is a Gaussian SP, then so is (YÎ½).
Proof. The proof is similar to the proof of Theorem 25.13.2 with all integrals
replaced by sums. It is thus omitted.
31.3
Discrete-Time Whitening Filters
A whitening ï¬lter is lossless in the sense of Corollary 31.2.6 and allows us to trans-
form a Gaussian SP with memory to one that is memoryless (IID). For this to
hold, the magnitude of the ï¬lterâ€™s frequency response must be inversely propor-
tional to the square root of the SPâ€™s PSD; the phase is immaterial. Consequently,
the whitening ï¬lter, when it exists, is not unique.
Deï¬nition 31.3.1 (Discrete-Time Whitening Filter). A discrete-time whiten-
ing ï¬lter for a WSS SP (XÎ½) of PSD SXX is a stable ï¬lter whose frequency re-
sponse uh satisï¬es
uh(Î¸)
 = SXX(Î¸)âˆ’1/2
(31.31)
(outside a set of Î¸â€™s of Lebesgue measure zero) and that has a stable inverse.
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.3 Discrete-Time Whitening Filters
795
The existence of a whitening ï¬lter is a technical issue. It is natural to assume that
the PSD of the SP be positive so that the square root of its reciprocal be ï¬nite.
But we typically require a bit more, because we need to guarantee that this square
root correspond to the magnitude of some stable ï¬lter.
The next theorem provides suï¬ƒcient conditions for the existence of a whitening
ï¬lter. In its statement we shall require that some function be absolutely continuous
on Â¯I.2 Most readers have probably not encountered this notion before, and it is
inessential here. Interested readers can read up on this topic, for example, in (Roy-
den and Fitzpatrick, 2010, Section 6.4). Here we merely note that every function
that is continuously diï¬€erentiable on [a, b] is also absolutely continuous on [a, b].
And to deal with functions that are continuous but only piecewise continuously
diï¬€erentiable, we note that if a < b < c and if g(Â·) is absolutely continuous on both
[a, b] and [b, c], then g(Â·) is absolutely continuous on [a, c]. This extends to any
ï¬nite number of intervals [a, b1], [b1, b2], . . . , [bn, c].
Finally, we note that if g(Â·) is absolutely continuous on [a, b] then it is continuous
on [a, b] and that, excluding a set of Lebesgue measure zero, it is diï¬€erentiable at
every point in [a, b]. Moreover, irrespective of how we deï¬ne its derivative gâ€² in the
exception set
g(Î¾) = g(a) +
 Î¾
a
gâ€²(Î±) dÎ±,
Î¾ âˆˆ[a, b].
Theorem 31.3.2 (Existence of a Discrete-Time Whitening Filter). Suppose the
mapping S: I â†’R is positive
S(Î¸) > 0,
Î¸ âˆˆI;
its periodic extension is absolutely continuous on Â¯I; and its derivative Sâ€²(Â·) satisï¬es

1
2
âˆ’1
2

Sâ€²(Î¸)
2 dÎ¸ < âˆ.
Then there exists a stable discrete-time ï¬lter with a stable inverse whose impulse
response

hÎ½

satisï¬es

âˆ

Î½=âˆ’âˆ
hÎ½ eâˆ’i2Ï€Î½Î¸
 =
1

S(Î¸)
,
Î¸ âˆˆI.
If S is symmetric in the sense that S(âˆ’Î¸) = S(Î¸) whenever |Î¸| < 1/2, then such a
ï¬lter exists that is additionally real.
2A real-valued function g(Â·) is said to be absolutely continuous on the interval [a, b] if it is
deï¬ned on [a, b], and for every Ïµ > 0 there exists some Î´ > 0 such that

n

i=1
(bi âˆ’ai) < Î´

=â‡’

n

i=1
g(bi) âˆ’g(ai)
 < Ïµ

,
for every ï¬nite number of pairwise-disjoint subintervals (a1, b1), . . . , (an, bn) of [a, b].
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

796
A Glimpse at Discrete-Time Signal Processing
Proof. Before presenting the details, we give an overview of the proof. Our ap-
proach is to deï¬ne hÎ½ to be the (âˆ’Î½)-th Fourier Series Coeï¬ƒcient of the mapping
Î¸ 	â†’
1

S(Î¸)
,
(31.32)
so
hÎ½ =

1
2
âˆ’1
2
1

S(Î¸)
ei2Ï€Î½Î¸ dÎ¸,
Î½ âˆˆZ.
(31.33)
(When S is symmetric the ï¬lter is thus real.) From this it will then be merely a
technicality to show that this ï¬lter has the desired discrete-time frequency response,
i.e., that
âˆ

Î½=âˆ’âˆ
hÎ½ eâˆ’i2Ï€Î½Î¸ =
1

S(Î¸)
,
Î¸ âˆˆI.
(31.34)
(Some technical assumptions are needed for (31.33) to imply (31.34).)
The other technical issue will be to show that the ï¬lter is stable, i.e., that
âˆ

Î½=âˆ’âˆ
|hÎ½| < âˆ.
(31.35)
The existence of a stable inverse will then follow from (31.34) using Theorem 31.1.2,
because the RHS of (31.34) is never zero.
To address some of the above technicalities, we begin by showing that the theoremâ€™s
hypotheses guarantee that the mapping (31.32) is absolutely continuous on Â¯I. Since
S(Î¸) is positive and continuous on Â¯I, we can ï¬nd constants smin and smax such that
0 < smin â‰¤S(Î¸) â‰¤smax,
Î¸ âˆˆÂ¯I.
(31.36)
Since smin is positive, the function Î¾ 	â†’Î¾âˆ’1
2 is continuously diï¬€erentiable on
[smin, smax]. And S(Â·) is absolutely continuous on Â¯I by assumption. Consequently,
since the composition of a Lipschitz function with an absolutely continuous function
is absolutely continuous (Royden and Fitzpatrick, 2010, Section 6.4, Exercise 44),
(Bogachev, 2007, Section 5.3, Lemma 5.3.2), the mapping (31.32) is absolutely
continuous on Â¯I.
Having established that the mapping (31.32) is absolutely continuous on Â¯I, we next
show that its derivative is square integrable. By the chain rule for diï¬€erentiation
(Rudin, 1976, Chapter 5, Theorem 5.5), the derivative of the mapping (31.32) is
âˆ’1
2 S(Î¸)âˆ’3
2 dS(Î¸)
dÎ¸
(31.37)
at every Î¸ at which S(Â·) is diï¬€erentiable. Since, by assumption, S(Â·) is absolutely
continuous, it is diï¬€erentiable for all Î¸ âˆˆÂ¯I outside a set of Lebesgue measure zero,
and the derivative of the mapping (31.32) is thus given by (31.37) for all Î¸ âˆˆÂ¯I
outside a set of Lebesgue measure zero.
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.4 Processing Discrete-Time Complex Processes
797
Using (31.36) we thus conclude that, outside a subset of Â¯I of Lebesgue measure
zero,

d
dÎ¸
1

S(Î¸)
 â‰¤1
2 s
âˆ’3
2
min

dS(Î¸)
dÎ¸
.
This, combined with the theoremâ€™s hypothesis that the derivative of S(Â·) is square
integrable over Â¯I implies that the derivative of the mapping (31.32) is also square
integrable.
We are now ready to tackle the technicalities. Since the mapping (31.32) is abso-
lutely continuous and its derivative square integrable, its Fourier Series Coeï¬ƒcients
are absolutely summable (Katznelson, 2004, the theorem in Chapter I, Section 6.2)
and (31.35) is thus established.
As to (31.34), it simply follows from the absolute continuity of the mapping (31.32)
(Katznelson, 2004, the corollary in Chapter II, Section 2.2).
The existence of a stable inverse follows from Theorem 31.1.2.
31.4
Processing Discrete-Time Complex Processes
Linear functionals of complex stochastic processes are very similar to their real
counterparts. But proper complex stochastic processes have no real counterparts
and thus require some attention. The same applies to the ï¬ltering of such complex
stochastic processes.
As in the real case, we shall need an assumption on the second moment of the
CSP. We say that a CSP (ZÎ½) is of bounded second moment if there exists
some Î³ > 0 that upper-bounds E

|ZÎ½|2
for every Î½ âˆˆZ:
E

|ZÎ½|2
â‰¤Î³,
Î½ âˆˆZ.
(31.38)
Every WSS CSP satisï¬es this condition because if (ZÎ½) is WSS then
E

|ZÎ½|2
=
E[ZÎ½]
2 + Var[ZÎ½]
=
E[Z0]
2 + Var[Z0] ,
and (31.38) thus holds with
Î³ =
E[Z0]
2 + Var[Z0] .
Note also that (31.38) implies
E

(Re(ZÎ½))2
, E

(Im(ZÎ½))2
â‰¤Î³,
(31.39)
so the real and imaginary parts of

ZÎ½

satisfy our underlying assumption for real
stochastic processes (31.12).
Finally note that, by the Cauchy-Schwarz Inequality for complex random variables
(Exercise 17.9), Inequality (31.38) implies
E[ZÎ½Zâˆ—
Î½â€²]
 â‰¤Î³,
Î½, Î½â€² âˆˆZ.
(31.40)
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

798
A Glimpse at Discrete-Time Signal Processing
The deï¬nition of a Gaussian SP requires little change. It builds on the notion of a
complex Gaussian vector (Deï¬nition 24.3.6):
Deï¬nition 31.4.1 (Discrete-Time Gaussian CSP). A discrete-time CSP (ZÎ½) is
said to be Gaussian if for every positive integer m and every choice of the integers
Î½1, . . . , Î½m, the random vector

ZÎ½1, . . . , ZÎ½m
T
is a complex Gaussian vector.
Also recall the deï¬nitions of a wide-sense stationary (Deï¬nition 17.5.3) and of a
proper (Deï¬nition 17.5.4) discrete-time CSP.
The following theorem is the complex counterpart of Proposition 31.2.2 and The-
orem 31.2.8. Note the additional part pertaining to proper CSPs.
Theorem 31.4.2 (Linear Functional of a CSP). Let (aÎ½) be an absolutely summable
complex sequence, and let the CSP (ZÎ½) be of bounded second moment.
(i) The set
N =
1
Ï‰ âˆˆÎ© :

Î½âˆˆZ
aÎ½ ZÎ½(Ï‰)
 = âˆ
!
is an event of probability zero.
(ii) The mapping
W(Ï‰) =
â§
â¨
â©

Î½âˆˆZ
aÎ½ ZÎ½(Ï‰)
if Ï‰ /âˆˆN,
0
otherwise
(31.41)
is a complex random variable of mean
E[W] =
âˆ

Î½=âˆ’âˆ
aÎ½ E[ZÎ½]
(31.42)
and of second moment
E

|W|2
=

Î½â€²âˆˆZ

Î½â€²â€²âˆˆZ
aÎ½â€² aâˆ—
Î½â€²â€² E

ZÎ½â€²Zâˆ—
Î½â€²â€²

.
(31.43)
(iii) The partial sums converge to W in the sense that
lim
nâ†’âˆE
5 W âˆ’
n

Î½=âˆ’n
aÎ½ ZÎ½

26
= 0.
(iv) If

ZÎ½

is a proper CSP, then W is a proper CRV.
(v) If

ZÎ½

is a Gaussian CSP, then W is a Gaussian CRV.
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.4 Processing Discrete-Time Complex Processes
799
(vi) If

ZÎ½

is WSS and of autocovariance function KZZ(Â·), then the mean and
variance of the CRV W above are given by
E
 
Î½âˆˆZ
aÎ½ZÎ½

= E[Z0]

Î½âˆˆZ
aÎ½,
(31.44)
Var
 
Î½âˆˆZ
aÎ½ZÎ½

=

Î½â€²âˆˆZ

Î½â€²â€²âˆˆZ
aÎ½â€² aâˆ—
Î½â€²â€² KZZ(Î½â€² âˆ’Î½â€²â€²)
(31.45)
=

Î·âˆˆZ
KZZ(Î·) Raa(Î·),
(31.46)
where Raa is the self-similarity sequence of (aÎ½) (Deï¬nition 31.1.1).
(vii) If

ZÎ½

is WSS and of PSD SZZ(Â·), then the variance of W is also given by
Var[W] =

1
2
âˆ’1
2
SZZ(Î¸)

âˆ

Î½=âˆ’âˆ
aÎ½ eâˆ’i2Ï€Î¸Î½

2
dÎ¸
=

1
2
âˆ’1
2
SZZ(Î¸) |ua(Î¸)|2 dÎ¸.
(31.47)
Proof. The proof is almost identical to the proof of Proposition 31.2.2. New are
Parts (iii), (iv), and (v), so we focus on those. Let Î³ > 0 be such that (31.38) and
hence also (31.39) hold. To prove (iii) we note that for Ï‰ /âˆˆN we have by (31.41)
W âˆ’
n

Î½=âˆ’n
aÎ½ ZÎ½ =

|Î½|>n
aÎ½ ZÎ½
=

Î½âˆˆZ
bÎ½ ZÎ½,
where
bÎ½ =

0
if |Î½| â‰¤n,
aÎ½
otherwise,
Î½ âˆˆZ.
Substituting the sequence

bÎ½

for

aÎ½

in Part (ii) we obtain
E
5W âˆ’
n

Î½=âˆ’n
aÎ½ ZÎ½

26
=

|Î½â€²|>n

|Î½â€²â€²|>n
aÎ½â€² aâˆ—
Î½â€²â€² E[ZÎ½â€²Zâˆ—
Î½â€²â€²]
â‰¤

|Î½â€²|>n

|Î½â€²â€²|>n
|aÎ½â€² aÎ½â€²â€²|
E[ZÎ½â€²Zâˆ—
Î½â€²â€²]

â‰¤

|Î½â€²|>n

|Î½â€²â€²|>n
|aÎ½â€² aÎ½â€²â€²| Î³
= Î³

|Î½â€²|>n
|aÎ½â€²|

|Î½â€²â€²|>n
|aÎ½â€²â€²|,
(31.48)
where the second inequality follows from (31.40). The RHS of (31.48) tends to
zero, because (aÎ½) was assumed to be absolutely summable. Consequently, since
its LHS is nonnegative, it too must tend to zero as n tends to inï¬nity.
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

800
A Glimpse at Discrete-Time Signal Processing
Part (iv) can be proved as follows: Since (ZÎ½) is proper, the vector

Zâˆ’n, . . . , Zâˆ’1, Z0, Z1, . . . , Zn
T
(31.49)
is a proper complex random vector (Deï¬nition 17.5.4). Consequently, if we de-
ï¬ne Wn as the partial sum
Wn =
n

Î½=âˆ’n
aÎ½ ZÎ½,
then, being a linear functional of the above proper vector, Wn must be a proper
CRV (Proposition 17.4.2). This combines with the fact that E

|W âˆ’Wn|2
â†’0
(Part (iii)) to imply that W must also be proper (Proposition 17.6.1).
As to (v), we note that since (ZÎ½) is a Gaussian CSP, the real SP
. . . , Re(Zâˆ’1), Im(Zâˆ’1), Re(Z0), Im(Z0), Re(Z1), Im(Z1), . . .
(31.50)
must be Gaussian. By (31.39), it is of bounded second moment. And since Re(Z)
and Im(Z) are linear functionals of this Gaussian SP, they must be jointly Gaussian
by Theorem 31.2.8 (ii).
We can deï¬ne the ï¬ltering of a discrete-time CSP like we did in the real case in
Deï¬nition 31.2.4. In analogy to Theorem 31.2.9 we have:
Theorem 31.4.3 (Filtering a Discrete-Time CSP). Let (ZÎ½) be a centered WSS
discrete-time CSP of autocovariance function KZZ, and let (hÎ½) be an absolutely
summable complex bi-inï¬nite sequence. Let

YÎ½

=

ZÎ½

â‹†

hÎ½

.
(31.51)
(i) The discrete-time CSP (YÎ½) is a centered WSS CSP whose autocovariance
function KYY is
KYY = KZZ â‹†Rhh,
(31.52)
where Rhh is the self-similarity sequence of

hÎ½

.
(ii) If (ZÎ½) is of PSD SZZ, then (YÎ½) is of PSD
SYY (Î¸) = SZZ(Î¸)

âˆ

Î½=âˆ’âˆ
hÎ½ eâˆ’i2Ï€Î½Î¸

2
(31.53)
= SZZ(Î¸)
uh(Î¸)
2,
Î¸ âˆˆR.
(31.54)
(iii) If (ZÎ½) is a Gaussian CSP, then so is (YÎ½).
(iv) If (ZÎ½) is a proper CSP, then so is (YÎ½).
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

31.5 Additional Reading
801
Proof. Parts (i), (ii), and (iii) are similar to those in Theorem 31.2.9, so we shall
only prove Part (iv). Recalling Deï¬nition 17.5.4, we set out to prove that E[YÎ½ YÎ½â€²]
is zero for all integers Î½ and Î½â€². Starting from the deï¬nition of the convolution,
E

YÎ½ YÎ½â€²
= E
 
mâˆˆZ
Zm hÎ½âˆ’m

Î·âˆˆZ
ZÎ· hÎ½â€²âˆ’Î·

=

mâˆˆZ

Î·âˆˆZ
hÎ½âˆ’m hÎ½â€²âˆ’Î· E

Zm ZÎ·

= 0,
Î½, Î½â€² âˆˆZ,
where the last equality follows from the assumption that (ZÎ½) is proper, which
implies that E[Zm ZÎ·] is zero for all integers m and Î·.
31.5
Additional Reading
Digital Signal Processing is a huge ï¬eld, and we have only presented the very
basics that we shall need in Chapter 32 to study the Linearly-Dispersive Channel.
Interested readers can consult (Porat, 2008) and (Pourahmadi, 2001) for much
more. Our treatment of the whitening ï¬lter is a bit diï¬€erent from the standard
one because we require that the ï¬lter be stable, whereas other authors often only
require that its impulse response be square summable.
31.6
Exercises
Exercise 31.1 (Time- and Frequency-Limited Sequences). Prove the discrete-time coun-
terpart of Theorem 6.8.2: If a sequence (aÎ½) is such that aÎ½ is zero whenever |Î½| > Î·,
where Î· is some positive integer, and if its DTFT ua(Î¸) is zero whenever Î¾ â‰¤|Î¸| < 1/2,
where Î¾ âˆˆ[0, 1/2), then the sequence must be the all-zero sequence.
Hint: Recall the proof of Theorem 15.4.1.
Exercise 31.2 (On the Convolution of Causal Sequences). Suppose that (aÎ½), (bÎ½) âˆˆâ„“1
are both causal and their convolution is (eÎ½) of (31.5). Show that a0 cannot be zero.
Exercise 31.3 (Uniqueness of the Inverse Filter). Show that if a stable ï¬lter has a stable
inverse then this stable inverse is unique.
Exercise 31.4 (Verifying the Inverse Filter). Consider the discrete-time ï¬lter whose im-
pulse response (hÎ½) is given for every Î½ âˆˆZ by hÎ½ = Î±Î½ I{Î½ â‰¥0}, where Î± is some real
number.
(i) Under what conditions on Î± is this ï¬lter stable?
(ii) Show that, for such values of Î±, the sequence (bÎ½) with b0 = 1, with b1 = âˆ’Î±, and
with bÎ½ being zero for all other integers Î½ is the inverse of (hÎ½).
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,
www.ebook3000.com

802
A Glimpse at Discrete-Time Signal Processing
Exercise 31.5 (The Inverse of an FIR Filter). A ï¬nite impulse response (FIR) ï¬lter
is one whose impulse response (hÎ½) is such that there exists some Î· âˆˆN for which hÎ½ is
zero whenever |Î½| > Î·. Show that if (hÎ½) is the impulse response of an FIR ï¬lter and hÎ½
is nonzero for at least two values of Î½, then the ï¬lter cannot have an inverse that is FIR.
Exercise 31.6 (Is the Convolution always Associative?). Consider the bi-inï¬nite sequences
aÎ½ =
â§
âª
â¨
âª
â©
1
if Î½ = 0,
âˆ’1
if Î½ = 1,
0
otherwise,
bÎ½ = I{Î½ â‰¥0},
cÎ½ = âˆ’I{Î½ â‰¤âˆ’1},
Î½ âˆˆZ.
Compare (b â‹†a) â‹†c with b â‹†(a â‹†c) and reï¬‚ect on Proposition 31.2.5.
Exercise 31.7 (Reï¬‚ecting a Whitening Filter). Show that if (hÎ½) is the impulse response
of a whitening ï¬lter for (XÎ½) then so is Î½ â†’hâˆ—
âˆ’Î½.
Exercise 31.8 (Gauss-Markov). Let (UÎ½) be IID N

0, Ïƒ2
u

, where Ïƒu > 0. Let Î± âˆˆ(0, 1)
be a constant, and deï¬ne the SP (XÎ½) as
XÎ½ =
âˆ

Î·=0
Î±Î· UÎ½âˆ’Î·.
(31.55)
Show that (XÎ½) is a WSS Gaussian SP satisfying the recursion
XÎ½+1 = Î± XÎ½ + UÎ½+1,
Î½ âˆˆZ.
(31.56)
This is the Gauss-Markov SP. Under what conditions on Ïƒu and Î± is XÎ½ of unit variance
for every integer Î½?
Exercise 31.9 (Delayless Causal Whitening Filters). Show that if there exists a causal
whitening ï¬lter for (XÎ½), then there also exists one whose impulse response at time zero
is nonzero.
Exercise 31.10 (Causal Whitening Filters and Prediction). Let (XÎ½) be a centered Gaus-
sian SP having a whitening ï¬lter whose impulse (hÎ½) is causal and nonzero at time zero.
(i) Show that (XÎ½) can be represented as
XÎ½ =
âˆ

Î·=1
aÎ· XÎ½âˆ’Î· + Ïƒ UÎ½,
Î½ âˆˆZ,
(31.57)
where (UÎ½) are IID N(0, 1); where Ïƒ2 = (1/h0)2; and where
aÎ· = âˆ’hÎ·
h0 ,
Î· âˆˆZ.
(31.58)
(ii) Suppose now additionally that the stable inverse of (hÎ½) is causal. Prove that for
every Î½ âˆˆZ the RV XÎ½ is independent of all future Uâ€™s, i.e., of all the random
variables UÎ½â€² with Î½â€² > Î½.
(iii) Argue that, subject to the above conditions, the inï¬nite sum on the RHS of (31.57)
is the predictor of XÎ½ based on XÎ½âˆ’1, XÎ½âˆ’2, . . . of least mean squared-error.
Hint: For Part (ii) express (XÎ½) as the convolution of (UÎ½) with the inverse of (hÎ½).
Exercise 31.11 (From a CSP to a SP). Let (ZÎ½) be a WSS CSP. Show that the SP in
(31.50) need not be WSS, even if (ZÎ½) is also proper (Deï¬nition 17.5.4).
available at 
.033
15:01:48, subject to the Cambridge Core terms of use,

Chapter 32
Intersymbol Interference
32.1
The Linearly-Dispersive Channel
The linearly-dispersive channelâ€”also known as the intersymbol interfer-
ence channel or ISI channelâ€”ï¬lters its input and adds white Gaussian noise to
the result. It serves, inter alia, as a model for communication over copper wires
as in communications over telephone lines.
Such a channel is depicted in Fig-
ure 32.1. In this chapter we shall discuss PAM (Sections 32.2 and 32.3) and QAM
(Section 32.4) over such channels with special emphasis on the receiver structure.
In Figure 32.1 and throughout this chapter we use the subscript â€œcâ€ for continuous-
time waveforms, e.g., xc for the channel input and hc for the ï¬lterâ€™s impulse re-
sponse. This is to avoid confusion with a related discrete-time channel that we
shall encounter later. We assume throughout that the ï¬lter is stable, i.e., that hc
is integrable:
hc âˆˆL1.
(32.1)
xc(t)
hc
Yc(t)
Nc(t)
(xc â‹†hc)(t)
Figure 32.1: A linearly-dispersive channel Yc(t) = (xc â‹†hc)(t) + Nc(t).
32.2
PAM on the ISI Channel
Consider k data bits D1, . . . , Dk, which are stacked in the vector D as in (28.3), and
that are mapped by the encoding function Ï•(Â·) to the n real symbols X1, . . . , Xn,
which are stacked in a vector X as in (28.4), so X = Ï•(D) as in (28.5).
The
803
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

804
Intersymbol Interference
transmitted PAM signal Xc is
Xc(t) = A
n

â„“=1
Xâ„“gc(t âˆ’â„“Ts),
t âˆˆR,
(32.2)
where gc is the pulse shape, which is assumed to be a real integrable signal that is
bandlimited to W Hz. Since ï¬ltering a PAM signal is tantamount to ï¬ltering its
pulse shape (Section 15.4 and particularly (15.21)), the received signal Yc can be
written as
Yc(t) = A
n

â„“=1
Xâ„“pc(t âˆ’â„“Ts) + Nc(t),
t âˆˆR,
(32.3)
where
pc = gc â‹†hc
(32.4)
is a real integrable signal that is bandlimited to W Hz (Proposition 6.5.2), and
the stochastic process

Nc(t)

is WGN of double-sided PSD N0/2 with respect to
the bandwidth W. Rather than guessing D based on

Yc(t)

, there is no loss of
optimality in guessing it based on the inner products between

Yc(t)

and the time
shifts of pc by Ts, 2Ts, . . . , nTs (Proposition 28.2.1). We prefer to scale the inner
products and deï¬ne
Tâ„“=

1
N0/2

Yc, t 	â†’pc(t âˆ’â„“Ts)

(32.5)
=

2A2
N0
n

â„“â€²=1
Xâ„“â€² Rpcpc

(â„“âˆ’â„“â€²)Ts

+ ËœZâ„“,
â„“âˆˆ{1, . . . , n},
(32.6)
where Rpcpc is the self-similarity function of pc, and where ( ËœZ1, . . . , ËœZn)T is a
centered Gaussian vector that is independent of D and that has the covariance
matrix
Cov
 ËœZâ„“â€², ËœZâ„“â€²â€²
= Rpcpc

(â„“â€² âˆ’â„“â€²â€²)Ts

,
â„“â€², â„“â€²â€² âˆˆ{1, . . . , n}.
(32.7)
To avoid edge eï¬€ects and to harness the machinery of WSS SPs, it is convenient
to extend the symbols to a bi-inï¬nite sequence by deï¬ning Xâ„“as zero for every
integer â„“outside the range {1, . . . , n}:
Xâ„“= 0,
â„“âˆˆZ \ {1, . . . , n}.
(32.8)
And it is convenient to deï¬ne Tâ„“also for â„“/âˆˆ{1, . . . , n} by extending (32.5) also to
such â„“â€™s:
Tâ„“=

1
N0/2

Yc, t 	â†’pc(t âˆ’â„“Ts)

,
â„“âˆˆZ,
(32.9)
so
Tâ„“=

2A2
N0
n

â„“â€²=1
Xâ„“â€² Rpcpc

(â„“âˆ’â„“â€²)Ts

+ ËœZâ„“,
â„“âˆˆZ,
(32.10)
=

2A2
N0
âˆ

â„“â€²=âˆ’âˆ
Xâ„“â€² Rpcpc

(â„“âˆ’â„“â€²)Ts

+ ËœZâ„“,
â„“âˆˆZ,
(32.11)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.2 PAM on the ISI Channel
805
where
 ËœZâ„“, â„“âˆˆZ

is now a centered discrete-time WSS Gaussian SP that is
independent of D and that has the autocovariance function
K Ëœ
Z Ëœ
Z(Î·) = Rpcpc(Î·Ts),
Î· âˆˆZ.
(32.12)
As a technical matter, we note here that
âˆ

Î·=âˆ’âˆ
Rpcpc(Î·Ts)
 < âˆ.
(32.13)
This can be seen by noting that pc is an integrable signal that is bandlimited to
W Hz, so Rpcpcâ€”which equals pc â‹†~pc (Section 11.4)â€”must also be an integrable
signal that is bandlimited to W Hz (Proposition 6.5.2), and Î· 	â†’Rpcpc(Î·Ts) must
therefore be absolutely summable (Exercise 8.10).
The PSD of
 ËœZâ„“

(Deï¬nition 13.6.1) can be readily computed from its autocovari-
ance function (32.12) by expressing K Ëœ
Z Ëœ
Z(Î·) as
K Ëœ
Z Ëœ
Z(Î·) = Rpcpc(Î·Ts)
=
 âˆ
âˆ’âˆ
|Ë†pc(f)|2 ei2Ï€fÎ·Ts df
=
âˆ

j=âˆ’âˆ

j
Ts +
1
2Ts
j
Ts âˆ’
1
2Ts
|Ë†pc(f)|2 ei2Ï€fÎ·Ts df
= 1
Ts
âˆ

j=âˆ’âˆ

1
2
âˆ’1
2
Ë†pc
 Î¸
Ts
+ j
Ts

2
ei2Ï€(Î¸+j)Î· dÎ¸
= 1
Ts
âˆ

j=âˆ’âˆ

1
2
âˆ’1
2
Ë†pc
 Î¸
Ts
+ j
Ts

2
ei2Ï€Î¸Î· dÎ¸
=

1
2
âˆ’1
2
	 1
Ts
âˆ

j=âˆ’âˆ
Ë†pc
 Î¸
Ts
+ j
Ts

2
ei2Ï€Î¸Î· dÎ¸
=

1
2
âˆ’1
2
	 1
Ts
âˆ

j=âˆ’âˆ
Ë†gc
 Î¸
Ts
+ j
Ts

Ë†hc
 Î¸
Ts
+ j
Ts

2
ei2Ï€Î¸Î· dÎ¸,
Î· âˆˆZ, (32.14)
where the ï¬rst equality follows from (32.12); the second because Rpcpc is of FT
f 	â†’|Ë†pc(f)|2 (Section 11.4) and is an integrable signal that is bandlimited to W
Hz; the third by replacing the integral over the reals with the sum of integrals
over the nonoverlapping intervals of the form

j/Ts âˆ’1/(2Ts), j/Ts + 1/(2Ts)

; the
fourth by changing the integration variable from f to Î¸ â‰œTs(f âˆ’j/Ts); the ï¬fth
because exp (i2Ï€jÎ·) is one; the sixth by swapping the summation and the integra-
tion; and the seventh by (32.4). From (32.14) and the symmetry of the integrand,
we conclude that the PSD of
 ËœZâ„“

is
SËœ
ZËœ
Z(Î¸) = 1
Ts
âˆ

j=âˆ’âˆ
Ë†gc
 Î¸
Ts
+ j
Ts

Ë†hc
 Î¸
Ts
+ j
Ts

2
,
Î¸ âˆˆ
%
âˆ’1
2, 1
2

.
(32.15)
Note that the sum in (32.15) is actually ï¬nite, because gc is bandlimited to W Hz.
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

806
Intersymbol Interference
We shall now assume that
 ËœZâ„“

has a whitening ï¬lter, e.g., that SËœ
ZËœ
Z of (32.15)
satisï¬es the assumptions of Theorem 31.3.2. To whiten the noise, let hDT-Wh be
the impulse response of a whitening ï¬lter for ( ËœZâ„“) (Section 31.3). Rather than
basing our guess of D on (Tâ„“), let us base it on the result

Yâ„“

of feeding

Tâ„“

through the whitening ï¬lter
(Yâ„“) = (Tâ„“) â‹†hDT-Wh.
(32.16)
This incurs no loss of optimality, because the ï¬lter is stable with a stable inverse so
(Tâ„“) can be recovered from (Yâ„“) with probability one (Corollary 31.2.6). Starting
with (32.16), we can express Yâ„“as follows:
Yâ„“=
âˆ

Î½=âˆ’âˆ
TÎ½ hDT-Wh
â„“âˆ’Î½
(32.17a)
=
âˆ

Î½=âˆ’âˆ
Î±
n

â„“â€²=1
Xâ„“â€² Rpcpc

(Î½ âˆ’â„“â€²)Ts

hDT-Wh
â„“âˆ’Î½
+

( ËœZÎ½) â‹†(hDT-Wh
Î½
)

(â„“)
(32.17b)
= Î±
n

â„“â€²=1
Xâ„“â€²
âˆ

Î½=âˆ’âˆ
Rpcpc

(Î½ âˆ’â„“â€²)Ts

hDT-Wh
â„“âˆ’Î½
+ Zâ„“
(32.17c)
= Î±
n

â„“â€²=1
Xâ„“â€²
âˆ

Î·=âˆ’âˆ
Rpcpc

(â„“âˆ’â„“â€² âˆ’Î·)Ts

hDT-Wh
Î·
+ Zâ„“
(32.17d)
= Î±
n

â„“â€²=1
Xâ„“â€² hâ„“âˆ’â„“â€² + Zâ„“,
(32.17e)
= Î±
âˆ

â„“â€²=âˆ’âˆ
Xâ„“â€² hâ„“âˆ’â„“â€² + Zâ„“,
(32.17f)
where
Î± =

2A2
N0
,
(32.18)
. . . , Zâˆ’1, Z0, Z1, . . . âˆ¼IID N(0, 1),
(32.19)
and
hÎº =
âˆ

Î½=âˆ’âˆ
hDT-Wh
Î½
Rpcpc

(Îº âˆ’Î½)Ts

,
Îº âˆˆZ.
(32.20)
Here (32.17b) follows from the expression (32.10) for Tâ„“and from the deï¬nition
of the convolution; (32.17c) follows by swapping the sums and because the ï¬lter
whitens the noise; (32.17d) follows by deï¬ning Î· as â„“âˆ’Î½; (32.17e) follows from the
deï¬nition (32.20); and (32.17f) follows from (32.8).
Notice that the sequence (hÎº) in (32.20) is absolutely summable
(hÎº) âˆˆâ„“1,
(32.21)
because it is the result of convolving the sequence Î· 	â†’Rpcpc(Î·Ts), which is abso-
lutely summable by (32.13), with the sequence Î· 	â†’hDT-Wh
Î·
, which is the impulse
response of a stable ï¬lter, namely the whitening ï¬lter.
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.3 Guessing the Data Bits
807
We have thus reduced the continuous-time linearly-dispersive channel with PAM
signaling to a discrete-time ISI channel whose time-â„“output Yâ„“is
Yâ„“= Î±
âˆ

â„“â€²=âˆ’âˆ
Xâ„“â€² hâ„“âˆ’â„“â€² + Zâ„“,
â„“âˆˆZ,
(32.22)
where Î± is a positive constant (32.18); the sequence (hk) (32.20) is absolutely
summable; the noise sequence (Zâ„“) is IID N(0, 1) and independent of D; the sym-
bol Xâ„“is zero whenever â„“âˆˆZ \ {1, . . . , n}; and all the quantities are real. We
refer to hÎº as the Îº-th ISI coeï¬ƒcient, and to the sequence (hÎº) as the ISI
coeï¬ƒcients.
We emphasize that the ISI coeï¬ƒcients (hÎº) of (32.20) are determined not only
by the pulse shape gc and the channel response hc (through pc): they are also
inï¬‚uenced by the choice of the whitening ï¬lter, which is not unique.
Diï¬€erent
choices of the whitening ï¬lter lead to diï¬€erent ISI coeï¬ƒcients.
The coeï¬ƒcients (hÎº) corresponding to positive values of Îº are called postcursor
coeï¬ƒcients because through them the symbol Xâ„“â€² inï¬‚uences Yâ„“for â„“> â„“â€², i.e.,
for values of â„“that are in the â€œfutureâ€ relative to â„“â€². For example, if h1 is nonzero,
then Xâ„“â€² inï¬‚uences the future output Yâ„“â€²+1 because, by (32.22),
Î±âˆ’1
Yâ„“â€²+1 âˆ’Zâ„“â€²+1

= Â· Â· Â· + h1Xâ„“â€² + h0Xâ„“â€²+1 + Â· Â· Â· .
(32.23)
Similarly, the coeï¬ƒcients (hÎº) corresponding to negative values of Îº are called
precursor coeï¬ƒcients because through them the symbol Xâ„“â€² inï¬‚uences Yâ„“for
â„“< â„“â€², i.e., for values of â„“that are in the â€œpastâ€ relative to â„“â€². For example, if hâˆ’1
is nonzero, then Xâ„“â€² inï¬‚uences the past output Yâ„“â€²âˆ’1 because, by (32.22),
Î±âˆ’1
Yâ„“â€²âˆ’1 âˆ’Zâ„“â€²âˆ’1

= Â· Â· Â· + h0Xâ„“â€²âˆ’1 + hâˆ’1Xâ„“â€² + Â· Â· Â· .
(32.24)
These terms can also be described using the time frame with respect to which â„“
corresponds to the present by rewriting (32.22) as
Î±âˆ’1
Yâ„“âˆ’Zâ„“

=
Â· Â· Â· + h2Xâ„“âˆ’2 + h1Xâ„“âˆ’1



contribution of past symbols due to postcursor coeï¬ƒcients
+ h0Xâ„“+
hâˆ’2Xâ„“+2 + hâˆ’1Xâ„“+1 + Â· Â· Â· .



contribution of future symbols due to precursor coeï¬ƒcients
(32.25)
32.3
Guessing the Data Bits
32.3.1
Postulating a Model
Engineers are sometimes reluctant to implement the MAP rule, either because the
implementation is too complex or because they seek a one-for-all solution that
works in many diï¬€erent scenarios. To ï¬nd a reasonable guessing rule, they some-
times postulate a diï¬€erent set of densities { ËœfY|M=m(Â·)} (perhaps simpler) and
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

808
Intersymbol Interference
a diï¬€erent prior {ËœÏ€m}, and they then implement the MAP for the â€œpostulated
model,â€ i.e., the guessing rule that guesses â€œM = Ëœmâ€ whenever Ëœm achieves
max
mâ€²âˆˆM ËœÏ€mâ€² ËœfY|M=mâ€²(yobs)
(32.26)
(with ties being resolved arbitrarily). In other words, they pretend that the densi-
ties and prior are { ËœfY|M=m(Â·)} and {ËœÏ€m}â€”knowing full-well that they are notâ€”
and they then implement the MAP for this â€œpretend model.â€
For example, if ËœfY|M=m(Â·) is the same as fY|M=m(Â·) for every m âˆˆM and if {ËœÏ€m}
is uniform, then this approach leads to the ML rule (Sections 20.8 and 21.3.4).
More interesting, of course, is when the postulated densities are diï¬€erent from the
true ones.
We shall refer to the postulated densities and prior as the postulated model and
to the true densities {fY|M=m(Â·)} and the true prior {Ï€m} as the true model. We
emphasize that the postulated model need not be consistent with the mathematical
assumptions we made in deriving the true model, and it may not resemble the
truth. But we hope that if it does, then using the MAP rule corresponding to the
postulated model may yield good results on the true model.
Instead of providing the densities explicitly, we sometimes describe them by how
the observables are generated. For example, instead of saying that we pretend that
â€œunder H = 0 the observable is N(A, Ïƒ2) and that under H = 1 it is N(âˆ’A, Ïƒ2),â€
we could say that we pretend that â€œthe observable Y is given by Y = (1âˆ’2H)A+Z,
where Z âˆ¼N(0, Ïƒ2) is independent of H.â€
For the ISI channel we shall postulate a model that is also an ISI channel, but
one whose ISI coeï¬ƒcients

â„Îº

are simpler in the sense that â„Îº is zero for every
integer Îº that is either negative or exceeds some Îºmax âˆˆN:
â„Îº = 0,
Îº /âˆˆ{0, . . . , Îºmax}.
(32.27)
The ISI coeï¬ƒcients (hÎº) of the true model, which are given in (32.20), do not
typically have this simple form. However, as we next show, if Îºmax is allowed to
be suï¬ƒciently large, then a judicious choice of the phase of the whitening ï¬lter
can lead to the true coeï¬ƒcients being â€œnearlyâ€ of this simple form: hÎº will be
approximately zero whenever Îº is negative or exceeds Îºmax.
To this end, we ï¬rst note that the sequence (hÎº) of ISI coeï¬ƒcients corresponding
to any whitening ï¬lter hDT-Wh is absolutely summable (32.21), so it tends to
zero as Îº tends to Â±âˆ. Consequently, hÎº is approximately zero whenever Îº /âˆˆ
{Îº1, . . . , Îº2}, where Îº1, Îº2 are appropriately chosen integers. If Îº1 is zero, we set
Îºmax to be Îº2 and we are done. Otherwise, we change our whitening ï¬lter so that
the new ISI coeï¬ƒcients be equal to the result of shifting (hÎº) by Îº1. To be more
explicit, if hDT-Wh is the original whitening ï¬lter that yielded (hÎº), we consider
the new whitening ï¬lter Î½ 	â†’hDT-Wh
Î½+Îº1
. (This is also a whitening ï¬lter because the
Discrete-Time Fourier Transform of Î½ 	â†’hDT-Wh
Î½+Îº1
has the same magnitude as that
of hDT-Wh.) Substituting Î½ 	â†’hDT-Wh
Î½+Îº1
for hDT-Wh in (32.20) shows that employing
Î½ 	â†’hDT-Wh
Î½+Îº1
instead of hDT-Wh yields the ISI coeï¬ƒcients Îº 	â†’hÎº+Îº1, which are
approximately zero whenever Îº is negative or exceeds Îºmax, where Îºmax = Îº2 âˆ’Îº1.
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.3 Guessing the Data Bits
809
To complete the description of the postulated model we need to address the prior.
Recall that X = Ï•(D), where Ï•: {0, 1}k â†’Rn is a one-to-one mapping that maps
the k data bits to the n real symbols; see (28.3), (28.4), and (28.5). Let X denote
the constellation of Ï• (Section 10.8), so Ï•(D) âˆˆX n. For the purpose of deriving
our decision rule, we shall postulate that
D1, . . . , Dk âˆ¼IID random bits
(32.28a)
and that the system is uncoded (Section 10.9), i.e., that the range of Ï• is X n

Ï•(d) : d âˆˆ{0, 1}k
= X n.
(32.28b)
Since we are assuming throughout that Ï• is one-to-one, postulating that it is also
onto is equivalent to postulating that it is a one-to-one mapping of {0, 1}k onto X n
(Section 10.9). Such a mapping maps uniform inputs to uniform outputs. And
since (32.28a) is equivalent to (D1, . . . , Dk) being uniform over {0, 1}k, we can
conclude that (32.28) (together with the assumption that Ï• is one-to-one) implies
that
(X1, . . . , Xn) âˆ¼U (X n) ,
(32.29)
i.e., that the symbols X1, . . . , Xn are independent with each being uniformly dis-
tributed over X:
X1, . . . , Xn âˆ¼IID U (X).
(32.30)
(In fact, since Ï• is one-to-one, the conditions (32.30), (32.29), and (32.28) are
equivalent.)
32.3.2
States and Dynamic Programming
We next derive a guessing rule for the postulated model. We pretend that our
observations came from this model, and we ï¬nd the guessing rule that minimizes
the probability of a message error (28.16).
We begin by noting that, since the postulated ISI coeï¬ƒcients (â„Î½) satisfy (32.27),
the outputs . . . , Yâˆ’1, Y0 and the outputs Yn+Îºmax+1, . . . are not inï¬‚uenced by the
symbols (X1, . . . , Xn), and they are merely independent noise. Under the postu-
lated model we can therefore ignore these outputs and base our guess of D only on
the observed y1, . . . , yn+Îºmax. For typographical reasons, it is convenient to deï¬ne
nâ€² = n + Îºmax.
(32.31)
Since X is a deterministic function of the data D,
ËœfY1,...,Ynâ€²|D=d(y1, . . . , ynâ€²) = ËœfY1,...,Ynâ€²|X=Ï•(d)(y1, . . . , ynâ€²).
Hence, given y1, . . . , ynâ€², the Maximum-Likelihood decoder for the postulated model
searches over all d âˆˆ{0, 1}k for the data vector that maximizes the value of
ËœfY1,...,Ynâ€²|X=Ï•(d)(y1, . . . , ynâ€²). Since the range of Ï• is X n (32.28b), we can per-
form this search in two steps: ï¬rst search over all sequences in X n for some Ëœx for
which
ËœfY1,...,Ynâ€²|X=Ëœx(y1, . . . , ynâ€²) = max
xâ€²âˆˆX n ËœfY1,...,Ynâ€²|X=xâ€²(y1, . . . , ynâ€²)
(32.32)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

810
Intersymbol Interference
(resolving ties arbitrarily), and then produce the data sequence Ï•âˆ’1(Ëœx) that Ï•
maps to it.
To ï¬nd a maximizer Ëœx, we need the postulated densities in an explicit form. Sub-
stituting in (32.22) the postulated ISI coeï¬ƒcients (â„Î½) for the true ones (hÎ½) and
recalling that the Zâ„“â€™s are IID N(0, 1), we obtain the explicit form
ËœfY1,...,Ynâ€²|X1=x1,...,Xn=xn(y1, . . . , ynâ€²)
= (2Ï€)âˆ’nâ€²/2 exp
-
âˆ’1
2
nâ€²

â„“=1
	
yâ„“âˆ’Î± â„0 xâ„“âˆ’Î±
Îºmax

â„“â€²=1
â„â„“â€² xâ„“âˆ’â„“â€²

2.
with xâ„“taken as zero if â„“/âˆˆ{1, . . . n}.
(32.33)
Taking logarithms, discarding the term âˆ’nâ€²
2 log(2Ï€) (which is common to all the
hypotheses), and scaling by âˆ’2Î±âˆ’2, we conclude that the maximization of the
likelihood under the postulated model is equivalent to the minimization of Î¥(x; y),
where x and y stand for x1, . . . , xn and y1, . . . , ynâ€², and
Î¥(x; y) =
nâ€²

â„“=1
	
Î±âˆ’1yâ„“âˆ’â„0 xâ„“âˆ’
Îºmax

â„“â€²=1
â„â„“â€² xâ„“âˆ’â„“â€²

2
,
(32.34)
with xâ„“taken as zero if â„“/âˆˆ{1, . . . n}. We refer to Î¥(x; y) as the â€œcost of x1, . . . , xn.â€
(More precise but too cumbersome would have been â€œcost of x1, . . . , xn given
y1, . . . , ynâ€².â€)
The diï¬ƒculty in minimizing the cost Î¥(x; y) over x1, . . . , xn is that each symbol xâ„“
inï¬‚uences a number of diï¬€erent terms in the sum, so we cannot minimize the sum
term-by-term. To address this issue, we introduce the notion of a state. We will
deï¬ne the states so that the time-â„“state sâ„“will be a deterministic function of the
time-(â„“âˆ’1) state sâ„“âˆ’1 and the symbol xâ„“
sâ„“= Ïˆ(sâ„“âˆ’1, xâ„“)
(32.35a)
(where the time-zero state s0 is chosen arbitrarily) and so that the cost Î¥(x; y)
will be expressible as
Î¥(x; y) =
n

â„“=1
Î³â„“(xâ„“, sâ„“âˆ’1; yâ„“) + Î³n+1(sn; yn+1, . . . , ynâ€²)
(32.35b)
for some functions Î³1, . . . , Î³n+1. Once we have cast the problem in this form, we
will be able to use Dynamic Programming or the Viterbi Algorithm to minimize
the cost.
Ideally, we would like the time-(â„“âˆ’1) state sâ„“âˆ’1 to correspond to the symbols
xâ„“âˆ’1, . . . , xâ„“âˆ’Îºmax, which are needed in addition to xâ„“in order to compute the â„“-th
term in (32.34) (for 1 â‰¤â„“â‰¤n). This would allow us to express this term as a
function of xâ„“and sâ„“âˆ’1 (and yâ„“). But there are some slight technicalities stemming
from edge eï¬€ects. We must therefore be a bit more formal.
A state s is a Îºmax-tuple in X Îºmax of components s(1), s(2), . . . , s(Îºmax):
s =

s(1), s(2), . . . , s(Îºmax)
âˆˆX Îºmax.
(32.36)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.3 Guessing the Data Bits
811
The collection of all states, namely X Îºmax, is denoted S:
S = X Îºmax.
(32.37)
We choose from S some arbitrary state s0 âˆˆS and call it â€œthe state at time zero.â€
We next deï¬ne the state transition law. As in a shift-register, feeding the symbol x1
at time 1 results in the transition of the state from s0 to the time-1 state
s1 =

x1, s(1)
0 , s(2)
0 , . . . , s(Îºmaxâˆ’1)
0

.
(32.38)
We express this by writing
s0
x1
â‡s1.
(32.39)
Feeding x2 after x1 leads to the time-2 state
s2 =

x2, s(1)
1 , s(2)
1 , . . . , s(Îºmaxâˆ’1)
1

=

x2, x1, s(1)
0 , . . . , s(Îºmaxâˆ’2)
0

.
Thus,
s0
x1
â‡s1
x2
â‡s2.
In general, if the time-(â„“âˆ’1) state is sâ„“âˆ’1, then feeding xâ„“leads to the time-â„“state
sâ„“= Ïˆ(sâ„“âˆ’1, xâ„“),
(32.40a)
where
Ïˆ(sâ„“âˆ’1, xâ„“) =

xâ„“, s(1)
â„“âˆ’1, s(2)
â„“âˆ’1, . . . , s(Îºmaxâˆ’1)
â„“âˆ’1

.
(32.40b)
Note that feeding the inputs x1, . . . , xâ„“consecutively results in the time-â„“state sâ„“
being
sâ„“=
â§
â¨
â©

xâ„“, xâ„“âˆ’1, . . . , xâ„“âˆ’Îºmax+1

if â„“âˆˆ{Îºmax, . . . , n},

xâ„“, xâ„“âˆ’1, . . . , x1, s(1)
0 , . . . , s(Îºmaxâˆ’â„“)
0

if â„“âˆˆ{1, . . . , Îºmax âˆ’1}.
(32.41)
(The second case accounts for the edge eï¬€ect we mentioned earlier.) From sâ„“we
can thus recover xâ„“, xâ„“âˆ’1, . . . , xâ„“âˆ’min{Îºmaxâˆ’1,â„“âˆ’1}:
xâ„“âˆ’Î½ = s(Î½+1)
â„“
,
Î½ âˆˆ

0, . . . , min{Îºmax âˆ’1, â„“âˆ’1}

,
â„“âˆˆ{1, . . . , n}.
(32.42)
And from sâ„“âˆ’1 we can recover xâ„“âˆ’1, . . . , xâ„“âˆ’min{Îºmax,â„“âˆ’1}:
xâ„“âˆ’Î½ = s(Î½)
â„“âˆ’1,
Î½ âˆˆ

1, . . . , min{Îºmax, â„“âˆ’1}

,
â„“âˆˆ{2, . . . , n + 1}.
(32.43)
Finally, by substituting n for â„“in (32.42) we obtain that from sn we can recover
xn, xnâˆ’1, . . . , xnâˆ’min{Îºmaxâˆ’1,nâˆ’1}:
xnâˆ’Î½ = s(Î½+1)
n
,
Î½ âˆˆ

0, . . . , min{Îºmax âˆ’1, n âˆ’1}

.
(32.44)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

812
Intersymbol Interference
We next express the cost Î¥(x, y) (32.34) as in (32.35b) and without the need for
the interpretation of xâ„“as being zero when â„“/âˆˆ{1, . . . , n}. To avoid this need,
we rewrite (32.34) in a somewhat more cumbersome form, but one that does not
involve any xâ„“with â„“/âˆˆ{1, . . . n}:
Î¥(x; y) =
n

â„“=1
	
Î±âˆ’1yâ„“âˆ’â„0 xâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=1
â„â„“â€² xâ„“âˆ’â„“â€²

2
+
nâ€²

â„“=n+1
	
Î±âˆ’1yâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=â„“âˆ’n
â„â„“â€² xâ„“âˆ’â„“â€²

2
,
(32.45)
where we have set the upper limits in the two sums over â„“â€² above to min{Îºmax, â„“âˆ’1}
to avoid â„“âˆ’â„“â€² being nonpositive; and in the last line of (32.45) we discarded the
term â„0 xâ„“(because â„“exceeds n), and we set the lower limit on â„“â€² to â„“âˆ’n so as to
guarantee that â„“âˆ’â„“â€² not exceed n.
To express the RHS of (32.45) as in (32.35b) in terms of the states, we can
use (32.43) and (32.44) to obtain
Î¥(x; y) =
n

â„“=1
	
Î±âˆ’1yâ„“âˆ’â„0 xâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=1
â„â„“â€² s(â„“â€²)
â„“âˆ’1

2



Î³â„“(xâ„“,sâ„“âˆ’1;yâ„“)
+
nâ€²

â„“=n+1
	
Î±âˆ’1yâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=â„“âˆ’n
â„â„“â€² s(nâˆ’â„“+â„“â€²+1)
n

2



Î³n+1(sn;yn+1,...,yn+Îºmax)
(32.46)
=
n

â„“=1
Î³â„“(xâ„“, sâ„“âˆ’1; yâ„“) + Î³n+1(sn; yn+1, . . . , ynâ€²),
(32.47)
where for every â„“âˆˆ{1, . . . , n} we deï¬ne the â„“-th intermediate cost
Î³â„“(xâ„“, sâ„“âˆ’1; yâ„“) =
	
Î±âˆ’1yâ„“âˆ’â„0 xâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=1
â„â„“â€² s(â„“â€²)
â„“âˆ’1

2
,
(32.48)
and we deï¬ne the ï¬nal cost as
Î³n+1(sn; yn+1, . . . , ynâ€²) =
nâ€²

â„“=n+1
	
Î±âˆ’1yâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=â„“âˆ’n
â„â„“â€² s(nâˆ’â„“+â„“â€²+1)
n

2
. (32.49)
Note that (32.43) and (32.44) also provide alternative forms for the costs (in terms
of the symbols instead of the states):
Î³â„“(xâ„“, sâ„“âˆ’1; yâ„“) =
	
Î±âˆ’1yâ„“âˆ’â„0 xâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=1
â„â„“â€² xâ„“âˆ’â„“â€²

2
,
â„“âˆˆ{1, . . . , n},
(32.50)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.3 Guessing the Data Bits
813
and
Î³n+1(sn; yn+1, . . . , ynâ€²) =
nâ€²

â„“=n+1
	
Î±âˆ’1yâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=â„“âˆ’n
â„â„“â€² xâ„“âˆ’â„“â€²

2
.
(32.51)
Having established that our problem of minimizing (32.34) can be cast as in (32.35),
we next propose an algorithm to solve it. For typographical reasons we shall no
longer make the dependence of the cost on y1, . . . , ynâ€² explicit as in (32.35b), and
we shall write the cost instead as
n

â„“=1
Î³â„“(xâ„“, sâ„“âˆ’1) + Î³n+1(sn).
Let us deï¬ne the cost-to-go of xj, . . . , xn from the state s at time j âˆ’1 as
n

â„“=j
Î³â„“(xâ„“, sâ„“âˆ’1) + Î³n+1(sn),
(32.52)
where the state evolves according to (32.40a) with sjâˆ’1 taken to be s. Let us denote
by cjâˆ’1(s) the minimum of (32.52) over all choices of xj, xj+1, . . . , xn. We refer to
cjâˆ’1(s) as the minimum cost-to-go from s at time j âˆ’1. With this notation the
minimum of (32.35b) that we seek can be expressed as c0(s0)
min
xâˆˆX n Î¥(x; y) = c0(s0).
(32.53)
Moreover,
cn(s) = Î³n+1(s),
s âˆˆS.
(32.54)
To be formal, we say that cn(s) is achieved by the empty string of symbols.
To get a sense of what the next proposition is about, let us imagine that we did not
know that the shortest path between two points is a straight line. Even without
knowing this, it would be fairly easy to convince ourselves that if the shortest
path from A to B goes through C, then its segment from C to B ought to be along
the shortest path from C to B. This is the analog of the following proposition,
which can also be stated extremely informally as follows: â€œWhether or not a path
achieving cÎ½(Ëœs) goes at time j through the state s we do not know. But if it does,
then it must continue from there along a path that achieves cj(s), and any path
achieving cj(s) will do.â€
Proposition 32.3.1 (Principle of Optimality: Cost-to-Go). Let s,Ëœs âˆˆS be ar-
bitrary states, and let Î½, j âˆˆ{0, . . . , n} be intermediate times with Î½ < j.
Let
xÎ½+1, . . . , xn achieve cÎ½(Ëœs) and go through the state s at time j
Ëœs
xÎ½+1
â‡Â· Â· Â·
xj
â‡s
xj+1
â‡Â· Â· Â·
xn
â‡.
(32.55)
Then:
(i) xj+1, . . . , xn must achieve cj(s).
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

814
Intersymbol Interference
(ii) If xâ‹†
j+1, . . . , xâ‹†
n achieves cj(s), then xÎ½+1, . . . , xj, xâ‹†
j+1, . . . , xâ‹†
n achieves cÎ½(Ëœs).
Proof. Let xâ‹†
j+1, . . . , xâ‹†
n achieve cj(s). The proof is based on a comparison between
the cost-to-go of xÎ½+1, . . . , xn and the cost-to-go of xÎ½+1, . . . , xj, xâ‹†
j+1, . . . , xâ‹†
n. De-
note by sÎ½+1, . . . , sn the states through which xÎ½+1, . . . , xn goes, and deï¬ne sÎ½
as Ëœs:
Ëœs = sÎ½
xÎ½+1
â‡sÎ½+1
xÎ½+2
â‡Â· Â· Â·
xn
â‡sn.
The assumption (32.55) that xÎ½+1, . . . , xn goes through s at time j implies that
n

â„“=j+1
Î³â„“(xâ„“, sâ„“âˆ’1) + Î³n+1(sn) â‰¥cj(s).
(32.56)
Consequently, the cost-to-go of xÎ½+1, . . . , xn from Ëœs at time Î½ can be lower-bounded
as follows:
j

â„“=Î½+1
Î³â„“(xâ„“, sâ„“âˆ’1) +
n

â„“=j+1
Î³â„“(xâ„“, sâ„“âˆ’1) + Î³n+1(sn) â‰¥
j

â„“=Î½+1
Î³â„“(xâ„“, sâ„“âˆ’1) + cj(s).
(32.57)
In comparison, the cost-to-go of xÎ½+1, . . . , xj, xâ‹†
j+1, . . . , xâ‹†
n from Ëœs at time Î½ is
j

â„“=Î½+1
Î³â„“(xâ„“, sâ„“âˆ’1) +
n

â„“=j+1
Î³â„“(xâ‹†
â„“, sâ„“âˆ’1) + Î³n+1(sn) =
j

â„“=Î½+1
Î³â„“(xâ„“, sâ„“âˆ’1) + cj(s).
(32.58)
Our assumption that xÎ½+1, . . . , xn minimizes the cost-to-go from Ëœs at time Î½ rules
out the possibility that xÎ½+1, . . . , xjâˆ’1, xâ‹†
j, . . . , xâ‹†
n has a lower cost, and (32.56)
must hence hold with equality, thus proving that xj+1, . . . , xn must achieve cj(s)
and hence establishing Part (i). Moreover, the fact that (32.56) holds with equality
implies that the cost-to-go of xÎ½+1, . . . , xj, xâ‹†
j+1, . . . , xâ‹†
n from Ëœs at time Î½ is the same
as that of xÎ½+1, . . . , xn, so it too achieves cÎ½(Ëœs), thus establishing Part (ii).
We have already seen in (32.54) how to compute cn(s) for every state s âˆˆS. We
will now use Proposition 32.3.1 to show that knowing cj(s) for every state s âˆˆS
allows us to compute cjâˆ’1(Ëœs) eï¬ƒciently for every state Ëœs âˆˆS. This will allow us to
compute c0(s0) as follows: start by computing cn(s) for every state s âˆˆS; proceed
to compute cnâˆ’1(s) for every state s âˆˆS; and continue in this fashion until you
have computed c0(s0). We will also see that with little additional work we can also
ï¬nd inputs x1, . . . , xn that achieve c0(s0).
Assume then that we have computed cj(s) for every state s âˆˆS, and ï¬x some
state Ëœs for which we would like to compute cjâˆ’1(Ëœs).
Here j âˆˆ{1, . . . , n}.
To
compute cjâˆ’1(Ëœs) we need to ï¬nd some sequence xâ€²
j, . . . , xâ€²
n that achieves it. There
are # X nâˆ’j+1 such sequences, but our search turns out to be easier: once we have
found xâ€²
j, the rest is easy, and there are only # X possible choices for xâ€²
j. To see
why xâ€²
j holds the key, suppose that xâ€²
j, . . . , xâ€²
n achieves cjâˆ’1(Ëœs). Since
Ëœs
xâ€²
j
â‡Ïˆ(Ëœs, xâ€²
j),
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.3 Guessing the Data Bits
815
we can infer from Proposition 32.3.1 (with the substitution of j âˆ’1 for Î½ and of
Ïˆ(Ëœs, xâ€²
j) for s) that xâ€²
j+1, . . . , xâ€²
n must achieve cj

Ïˆ(Ëœs, xâ€²
j)

. It thus follows that the
cost-to-go of xâ€²
j, . . . , xâ€²
n from Ëœs at time j âˆ’1 is Î³j(xâ€²
j,Ëœs) + cj

Ïˆ(Ëœs, xâ€²
j)

. And since
xâ€²
j, . . . , xâ€²
n minimizes the cost-to-go, xâ€²
j must minimize this cost. Hence
cjâˆ’1(Ëœs) = min
xâˆˆX
'
Î³j(x,Ëœs) + cj

Ïˆ(Ëœs, x)
(
.
(32.59)
As promised, the minimization in (32.59) is over x in X, and the latter contains
only # X elements.
The algorithm for ï¬nding c0(s0) can be thus described as
follows: For each s âˆˆS compute cn(s). Then use (32.59) to compute cnâˆ’1(Ëœs) for
every Ëœs âˆˆS. Continue using (32.59) until you have computed c0(s0). (In the last
step one need not compute c0(s) for every state: it suï¬ƒces to compute c0(s0).)
To guess the symbols, we also need to ï¬nd a sequence achieving c0(s0) and not
just the latterâ€™s value. This requires just a simple change to our algorithm. Again
we start out with the calculation of cn(s) for every s âˆˆS. But we also keep track
of the sequence that achieves it: in this case the empty sequence. Suppose now
that we have computed cj

s

for every s âˆˆS and that we have found a sequence
xâ‹†
j+1, . . . , xâ‹†
n that achieves it. For each Ëœs âˆˆS we next calculate cjâˆ’1(Ëœs) according
to (32.59). If the minimum in (32.59) is achieved by some xâ‹†, then we record that
cjâˆ’1(Ëœs) is achieved by the concatenation of xâ‹†with the sequence that achieves
cj

Ïˆ(Ëœs, xâ‹†)

.
We continue until we have computed c0(s0) and a sequence that
achieves it.
The complexity of this procedure is roughly proportional to n (# S) (# X), i.e., to
n (# X)Îºmax+1,
(32.60)
because in going from j to j âˆ’1 we use (32.59) for every Ëœs âˆˆS, and in each use we
search over a set of cardinality # X.
This algorithm is not very suitable for real-time applications, because it is a â€œback-
ward algorithmâ€ that can only begin once all the symbols y1, . . . , ynâ€² have been
received. Instead, engineers usually implement the Viterbi Algorithm, which is a
â€œforward algorithm,â€ that we describe next. It is very similar to the algorithm we
described above, except that, instead of working down from n, it works up to n.
32.3.3
The Viterbi Algorithm
Instead of studying the cost-to-go, we shall study the cost-to-reach, which we
deï¬ne next. The cost-to-reach of x1, . . . , xj (from s0) is
j

â„“=1
Î³â„“(xâ„“, sâ„“âˆ’1) + Î³n+1(sn) I{j = n},
(32.61)
where the state evolves according to (32.40a), and the last term Î³n+1(sn) is added
only if j is equal to n. Let us denote by Ëœcj(s) the minimum cost-to-reach over
all sequences x1, . . . , xj reaching s at time j, i.e., satisfying s0
x1
â‡Â· Â· Â·
xj
â‡s. If no
such sequence exists, then we say that s cannot be reached from s0 in j steps, and
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

816
Intersymbol Interference
we set Ëœcj(s) to be +âˆ. For j = 0 we deï¬ne
Ëœc0(s) =

0
if s = s0,
+âˆ
otherwise,
(32.62)
and we say that Ëœc0(s0) is achieved by the empty string of inputs.
In terms of the minimal cost-to-reach, the minimum of (32.35b) that we seek is
min
xâˆˆX n Î¥(x; y) = min
sâˆˆS Ëœcn(s),
(32.63)
and our task is to ï¬nd this minimum and a sequence x1, . . . , xn that achieves it.
The following proposition is the analog of Proposition 32.3.1 for the cost-to-reach.
Informally, it can be stated as follows: â€œWhether or not a path achieving ËœcÎ½(Ëœs)
goes at time j < Î½ through the state s we do not know. But if it does, then it does
so along a path that achieves Ëœcj(s), and any path achieving Ëœcj(s) will do.â€ The
formal version is as follows:
Proposition 32.3.2 (Principle of Optimality: Cost-to-Reach). Let s,Ëœs âˆˆS be
arbitrary states, and let Î½, j âˆˆ{0, . . . , n} be intermediate times with j < Î½. Let
x1, . . . , xÎ½ achieve ËœcÎ½(Ëœs) and go through the state s at time j
s0
x1
â‡Â· Â· Â·
xj
â‡s
xj+1
â‡Â· Â· Â·
xÎ½
â‡Ëœs.
(32.64)
Then:
(i) x1, . . . , xj must achieve Ëœcj(s).
(ii) If xâ‹†
1, . . . , xâ‹†
j achieves Ëœcj(s), then xâ‹†
1, . . . , xâ‹†
j, xj+1, . . . , xÎ½ achieves ËœcÎ½(Ëœs).
Proof. Let xâ‹†
1, . . . , xâ‹†
j achieve Ëœcj(s).
The proof is very similar to the proof of
Proposition 32.3.1 and compares the cost-to-reach of x1, . . . , xÎ½ with the cost-to-
reach of xâ‹†
1, . . . , xâ‹†
j, xj+1, . . . , xÎ½. The details are omitted.
With the aid of Proposition 32.3.2 (with the substitution of j + 1 for Î½) we shall
next show how knowing Ëœcj(s) for every state s âˆˆS allows us to compute Ëœcj+1(Ëœs)
eï¬ƒciently. We ï¬rst show that
Ëœcj+1(Ëœs) =
min
s,x: s
x
â‡Ëœs
'
Ëœcj(s) + Î³j+1(x, s) + Î³n+1(Ëœs) I{j + 1 = n}
(
,
(32.65)
where the minimum is over all states s âˆˆS and symbols x âˆˆX such that s
xâ‡Ëœs.
Later we will characterize all such s and x.
To see why (32.65) holds, let xâ€²
1, . . . , xâ€²
j+1 be a sequence achieving Ëœcj+1(Ëœs), and
let sâ€² be the state it reaches at time j, so
s0
xâ€²
1
â‡s1
xâ€²
2
â‡Â· Â· Â·
xâ€²
j
â‡sâ€² xâ€²
j+1
â‡Ëœs.
(32.66)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.3 Guessing the Data Bits
817
We do not know sâ€², but we do know from (32.66) that
sâ€² xâ€²
j+1
â‡Ëœs.
(32.67)
Since the inputs xâ€²
1, . . . , xâ€²
j+1 achieve Ëœcj+1(Ëœs) and go through sâ€² at time j, it follows
from Proposition 32.3.2 (i) that the cost-to-reach of xâ€²
1, . . . , xâ€²
j is Ëœcj(sâ€²). Conse-
quently, Ëœcj+1(Ëœs), which is the cost-to-reach of xâ€²
1, . . . , xâ€²
j+1, can also be expressed
as
Ëœcj+1(Ëœs) = Ëœcj(sâ€²) + Î³j+1(xâ€²
j+1, sâ€²) + Î³n+1(Ëœs) I{j + 1 = n}.
(32.68)
By (32.67), the pair sâ€², xâ€²
j+1 are in the feasible set of (32.65), and we thus conclude
that Ëœcj+1(Ëœs) is lower-bounded by the RHS of (32.65). (The RHS of (32.65), which
is the minimum over all choices of s and x satisfying s
xâ‡Ëœs cannot be higher than
when we make a particular choice for s to be sâ€² and of x to be xâ€²
j+1.) However,
equality must hold, because if sâ‹†and xâ‹†achieve the minimum in (32.65), and if
xâ‹†
1, . . . , xâ‹†
j achieve Ëœcj(sâ‹†), then the concatenated sequence xâ‹†
1, . . . , xâ‹†
j, xâ‹†reaches Ëœs
at time j + 1 and is of cost-to-reach Ëœcj

sâ‹†
+ Î³j+1(xâ‹†, sâ‹†) + Î³n+1(Ëœs) I{j + 1 = n},
which is the RHS of (32.65).
The minimization in (32.65) is over all states s and symbols x satisfying s
xâ‡Ëœs.
Given some state Ëœs âˆˆS, under what conditions on x âˆˆX and s âˆˆS does this hold?
In other words, given Ëœs âˆˆS, from which states s and with which inputs x can we
go from s to Ëœs? Using (32.40b) it is readily seen that s
xâ‡Ëœs holds if, and only if,
the following two conditions hold:
1) The state s has the form
s =

Ëœs(2), Ëœs(3), . . . Ëœs(Îºmaxâˆ’1), Î¾

(32.69a)
for some Î¾ âˆˆX.
2) The input x is given by
x = Ëœs(1).
(32.69b)
Here Î¾ can be viewed as the right-most entry in the shift register s, i.e., as the
entry that is â€œforgottenâ€ as x enters the shift-register from the left.
Thus, in the minimization on the RHS of (32.65) we can ï¬x x to be Ëœs(1), and we
can restrict the minimization over s to states s that are of the form (32.69a), of
which there are only # X.
We can thus describe the Viterbi Algorithm as follows: Initialize using (32.62).
Assume now that you have computed Ëœcj(s) for every state s âˆˆS. Compute Ëœcj+1(Ëœs)
for every Ëœs âˆˆS as follows: for each Î¾ âˆˆX compute
Ëœcj(s) + Î³j+1(x, s) + Î³n+1(Ëœs) I{j + 1 = n},
where x and s are given in (32.69), and ï¬nd one, say Î¾â‹†, that among all those Î¾â€™s
minimizes this expression. If sâ‹†is the result of substituting Î¾â‹†for Î¾ in (32.69a),
then
Ëœcj+1(Ëœs) = Ëœcj(sâ‹†) + Î³j+1

Ëœs(1), sâ‹†
+ Î³n+1(Ëœs) I{j + 1 = n}.
(32.70)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

818
Intersymbol Interference
Continue until Ëœcn(s) has been computed for every state s and then take its minimum
over all s. This is the minimum of (32.35b).
A minor alteration to the algorithm allows us to ï¬nd not only the minimum of Ëœcn(s)
but also a path that achieves it. The idea is to compute not only Ëœcj(s) but also
a path that achieves it. Such a path is called a survivor for the state s at
time j. The survivor for the state s0 at time 0 is the empty path. Suppose now
that for some j âˆˆ{0, . . . , n âˆ’1} we have found both Ëœcj(s) and a survivor achieving
it for every s âˆˆS. To compute Ëœcj+1(Ëœs) we use (32.65). If the minimum in (32.65)
is achieved by sâ‹†and xâ‹†(with the latter being equal to Ëœs(1) by (32.69b)), then a
survivor for the state Ëœs at time j + 1 is the concatenation of the survivor for the
state sâ‹†at time j and the symbol xâ‹†.
At the heart of the Viterbi Algorithm is (32.65), which engineers sometimes call
the add-compare-select step: For each choice of s and x satisfying s
xâ‡Ëœs, we
add to Ëœcj

s

the term Î³j+1(x, s) + Î³n+1(Ëœs) I{j + 1 = n} and we compare the
results. We then select the pair sâ‹†and xâ‹†for which the result is minimal. We
ï¬nally set Ëœcj+1(Ëœs) to be this minimum, and we set the survivor of Ëœcj+1(Ëœs) to be the
concatenation of the survivor of Ëœcj

sâ‹†
with xâ‹†.
We have chosen to derive the Viterbi Algorithm via Dynamic Programming. A
diï¬€erent approach is graph theoretic. We imagine a directed graph with weights
whose vertices are pairs of times and states (j, s). More speciï¬cally, it has the
vertex (0, s0) and all the vertices of the form (j, s) where j âˆˆ{1, . . . , n} and s âˆˆS.
There is an edge from the vertex (j, s) to the vertex (j + 1,Ëœs) if s
xâ‡Ëœs, in which
case its weight is
Î³j+1(x, s) + Î³n+1(Ëœs) I{j + 1 = n},
(32.71)
i.e.,
Î³j+1

Ëœs(1), s

+ Î³n+1(Ëœs) I{j + 1 = n}.
(32.72)
Note that the graph contains no cycles. In this setting the Viterbi Algorithm ï¬nds
the path of least weight from s0 to each of the vertices that has the form (n, s)
for some s âˆˆS (Cormen, Leiserson, Rivest, and Stein, 2009, Section 24.2). The
graph, without the weights, is sometimes called a trellis. An example of a trellis
is depicted in Figure 32.2. (Only the vertices (j, s) that are reachable from (0, s0)
are depicted.)
Our description of the Viterbi Algorithm has us decode only after n steps. Since n
is typically very large (corresponding to the total number of symbols transmitted
during the transmitterâ€™s lifetime), this delay is intolerable. Instead engineers often
decode Xâ„“after running the Viterbi Algorithm for â„“+ j steps, where j is chosen to
be â€œsuï¬ƒciently large.â€ They pick some arbitrary state s and look at its time-(â„“+j)
survivor. This survivorâ€™s time-â„“symbol is then their guess of Xâ„“.
32.4
QAM on the ISI Channel
We next consider QAM for the ISI channel. As in Section 28.5, we envision that k
data bits D1, . . . , Dk, which we stack in a vector D, are mapped by Ï•: {0, 1}k â†’Cn
to n complex symbols C1, . . . , Cn, which we stack in a vector C, so Ï•(D) = C.
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.4 QAM on the ISI Channel
819
(+1, +1, +1)
(+1, +1, âˆ’1)
(+1, âˆ’1, +1)
(+1, âˆ’1, âˆ’1)
(âˆ’1, +1, +1)
(âˆ’1, +1, âˆ’1)
(âˆ’1, âˆ’1, +1)
(âˆ’1, âˆ’1, âˆ’1)
Figure 32.2: An example of a trellis. Here Îºmax = 3, the constellation is {Â±1},
and only the vertices that are reachable from (0, s0) are depicted. The time-zero
state s0 here is (+1, âˆ’1, +1).
The transmitted signal is
XPB(t) = 2 Re

XBB(t) ei2Ï€fct
,
t âˆˆR,
(32.73)
where its baseband representation XBB(Â·) is
XBB(t) = A
n

â„“=1
Câ„“gc(t âˆ’â„“Ts),
t âˆˆR.
(32.74)
Here A > 0, the pulse shape gc is some integrable complex signal that is band-
limited to W/2 Hz, and the carrier frequency fc exceeds W/2. For convenience
(cf. (32.8)), we deï¬ne Câ„“as zero whenever â„“/âˆˆ{1, . . . , n}
Câ„“= 0,
â„“âˆˆZ \ {1, . . . , n}.
(32.75)
As we have seen in Section 16.9 (Proposition 16.9.1), passing the QAM signal XPB
through the real ï¬lter of impulse response hc is tantamount to replacing its pulse
shape gc with the pulse shape pc, where
pc(t) =

W
2
âˆ’W
2
Ë†gc(f) Ë†hc(f + fc) ei2Ï€fct df,
t âˆˆR
(32.76)
is an integrable complex signal that is bandlimited to W/2 Hz and whose FT is
Ë†pc(f) = Ë†gc(f) Ë†hc(f + fc),
f âˆˆR.
(32.77)
Thus, the baseband representation of XPB â‹†hc is
(XPB â‹†hc)BB(t) = A
n

â„“=1
Câ„“pc(t âˆ’â„“Ts),
t âˆˆR.
(32.78)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

820
Intersymbol Interference
We can hence view the received waveform as the sum of a QAM signal (of pulse
shape pc) and WGN. This is the setting we studied in Section 28.5, where we
also identiï¬ed a real random vector (Section 28.5.2) and a complex random vector
(Section 28.5.3) on which we could base our guess without any loss of optimality.
Here we shall use the latter, but we shall scale the inner products. We ï¬rst deï¬ne
pc,I,â„“(t) = 2 Re
 1
âˆš
2 pc(t âˆ’â„“Ts) ei2Ï€fct
,
t âˆˆR,
(32.79a)
pc,Q,â„“(t) = 2 Re
 1
âˆš
2 i pc(t âˆ’â„“Ts) ei2Ï€fct
,
t âˆˆR,
(32.79b)
and then1
Tâ„“= T (â„“)
I
+ i T (â„“)
Q ,
â„“âˆˆZ,
(32.80a)
where
T (â„“)
I
=
4
1
N0
 âˆ
âˆ’âˆ
Yc(t) pc,I,â„“(t) dt,
â„“âˆˆZ,
(32.80b)
T (â„“)
Q
=
4
1
N0
 âˆ
âˆ’âˆ
Yc(t) pc,Q,â„“(t) dt,
â„“âˆˆZ.
(32.80c)
It follows from Proposition 28.5.2 that there is no loss of optimality in guessing D
based on T1, . . . , Tn, and that Tâ„“can be expressed as
Tâ„“=

2A2
N0
n

â„“â€²=1
Câ„“â€² Rpcpc

(â„“âˆ’â„“â€²)Ts

+ ËœZâ„“,
â„“âˆˆ{1, . . . , n},
(32.81)
where Rpcpc is the self-similarity function of pc, and where ( ËœZ1, . . . , ËœZn)T is a
circularly-symmetric complex Gaussian vector that is independent of D and that
has the covariance matrix
Cov
 ËœZâ„“â€², ËœZâ„“â€²â€²
= Rpcpc

(â„“â€² âˆ’â„“â€²â€²)Ts

,
â„“â€², â„“â€²â€² âˆˆ{1, . . . , n}.
(32.82)
If we also consider â„“/âˆˆ{1, . . . , n}, then we can express Tâ„“as
Tâ„“=

2A2
N0
n

â„“â€²=1
Câ„“â€² Rpcpc

(â„“âˆ’â„“â€²)Ts

+ ËœZâ„“,
â„“âˆˆZ,
(32.83)
where
 ËœZâ„“

is a circularly-symmetric complex Gaussian SP that is independent
of D and that has the autocovariance function
K Ëœ
Z Ëœ
Z(Î·) = Rpcpc(Î·Ts),
Î· âˆˆZ.
(32.84)
To whiten the noise, we can proceed as in Section 32.2: Let hDT-Wh be the (com-
plex) impulse response of a whitening ï¬lter for
 ËœZâ„“, â„“âˆˆZ

(Section 31.3). And
1In Section 28.5.3 we denoted T (â„“)
I
+ i T (â„“)
Q
by T (â„“), but here we prefer Tâ„“in order to be
consistent with our notation for sequences.
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.4 QAM on the ISI Channel
821
rather than basing our guess of D on

Tâ„“, â„“âˆˆZ

, let us base it on the result

Yâ„“

of feeding

Tâ„“

through the whitening ï¬lter
(Yâ„“) = (Tâ„“) â‹†hDT-Wh.
(32.85)
Proceeding as in (32.17), this can be written in view of (32.83) as
Yâ„“= Î±
âˆ

â„“â€²=âˆ’âˆ
Câ„“â€² hâ„“âˆ’â„“â€² + Zâ„“,
â„“âˆˆZ,
(32.86a)
where
Câ„“= 0,
â„“âˆˆZ \ {1, . . . , n},
(32.86b)
. . . , Zâˆ’1, Z0, Z1, . . . âˆ¼IID NC(0, 1),
(32.86c)
Î± =

2A2
N0
,
(32.86d)
and
hk =
âˆ

Î½=âˆ’âˆ
hDT-Wh
Î½
Rpcpc

(k âˆ’Î½)Ts

,
k âˆˆZ.
(32.86e)
The similarities between the discrete-time model for PAM (32.22) and for QAM
(32.86) are striking. The only diï¬€erence is that in the former the symbols (Xâ„“)
and ISI coeï¬ƒcients (hÎº) are real, whereas in the latter the symbols (Câ„“) and ISI
coeï¬ƒcients (hÎº) are complex, and that the Gaussians in the former are real and in
the latter complex.
As we did in the real case, we shall now assume that the data D1, . . . , Dk are IID
random bits and that transmission is uncoded, so
(C1, . . . , Cn) âˆ¼U (Cn) ,
(32.87)
where C denotes the constellation (Section 16.7). We shall also postulate simpliï¬ed
complex ISI coeï¬ƒcients

â„Îº

for which
â„Îº = 0,
Îº /âˆˆ{0, . . . , Îºmax}.
(32.88)
Under the postulated model the outputs Yj for j < 0 or j > n + Îºmax can thus
be ignored, and the decision can be based on Y1, . . . , Ynâ€², where nâ€² = n + Îºmax.
Since the Gaussians in (32.86c) are standard complex Gaussians (Section 24.2.1),
the postulated densities are
ËœfY1,...,Ynâ€²|C1=c1,...,Cn=cn(y1, . . . , ynâ€²)
= Ï€âˆ’nâ€² exp
-
âˆ’
nâ€²

â„“=1
yâ„“âˆ’Î± â„0 câ„“âˆ’Î±
Îºmax

â„“â€²=1
â„â„“â€² câ„“âˆ’â„“â€²

2.
with câ„“taken as zero if â„“/âˆˆ{1, . . . n}.
(32.89)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

822
Intersymbol Interference
Taking logarithms, discarding the term âˆ’nâ€² log Ï€ (which is common to all the
hypotheses), and scaling by âˆ’Î±âˆ’2, we conclude that the maximization of the like-
lihood under the postulated model is equivalent to the minimization of Î¥(c; y),
where c and y stand for c1, . . . , cn and y1, . . . , ynâ€², and
Î¥(c; y) =
nâ€²

â„“=1
Î±âˆ’1yâ„“âˆ’â„0 câ„“âˆ’
Îºmax

â„“â€²=1
â„â„“â€² câ„“âˆ’â„“â€²

2
,
(32.90)
with câ„“taken as zero if â„“/âˆˆ{1, . . . n}. We refer to this quantity as the â€œcost of
c1, . . . , cn.â€ Notice that Î¥(c; y) is very similar to its PAM counterpart (32.34):
the only diï¬€erence is that the symbols are denoted câ„“instead of xâ„“and that the
squared parentheses are replaced with the squared magnitude.
As for PAM, we next deï¬ne states so that the time-â„“state sâ„“will be a deterministic
function of the time-(â„“âˆ’1) state sâ„“âˆ’1 and the symbol câ„“
sâ„“= Ïˆ(sâ„“âˆ’1, câ„“)
(32.91a)
(where the time-zero state s0 is chosen arbitrarily); and so that the cost (32.90)
will be expressible as
Î¥(c; y) =
n

â„“=1
Î³â„“(câ„“, sâ„“âˆ’1; yâ„“) + Î³n+1(sn; yn+1, . . . , ynâ€²).
(32.91b)
Once we have cast the problem in this form, we will be able to use Dynamic
Programming or the Viterbi Algorithm to minimize the cost just as we did for
PAM.
The states are now elements of CÎºmax (instead of elements of X Îºmax in the PAM
case), and the transition law analogous to (32.40) is
sâ„“= Ïˆ(sâ„“âˆ’1, câ„“),
(32.92a)
where
Ïˆ(sâ„“âˆ’1, câ„“) =

câ„“, s(1)
â„“âˆ’1, s(2)
â„“âˆ’1, . . . , s(Îºmaxâˆ’1)
â„“âˆ’1

.
(32.92b)
The deï¬nitions of Î³â„“(câ„“, sâ„“âˆ’1; yâ„“) and Î³n+1(sn; yn+1, . . . , ynâ€²) are nearly identical to
their real counterparts, except that the symbols are denoted cj instead of xj and
the squared parentheses are replaced with the squared magnitude:
Î³â„“(câ„“, sâ„“âˆ’1; yâ„“) =
Î±âˆ’1yâ„“âˆ’â„0 câ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=1
â„â„“â€² câ„“âˆ’â„“â€²

2
(32.93)
=
Î±âˆ’1yâ„“âˆ’â„0 câ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=1
â„â„“â€² s(â„“â€²)
â„“âˆ’1

2
,
â„“âˆˆ{1, . . . , n}, (32.94)
and
Î³n+1(sn; yn+1, . . . , ynâ€²) =
nâ€²

â„“=n+1
Î±âˆ’1yâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=â„“âˆ’n
â„â„“â€² câ„“âˆ’â„“â€²

2
(32.95)
=
nâ€²

â„“=n+1
Î±âˆ’1yâ„“âˆ’
min{Îºmax,â„“âˆ’1}

â„“â€²=â„“âˆ’n
â„â„“â€² s(nâˆ’â„“+â„“â€²+1)
n

2
.
(32.96)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.5 From Passband to Baseband
823
With these new deï¬nitions, the Dynamic Programming algorithm and the Viterbi
Algorithm require no change.
32.5
From Passband to Baseband
The QAM receiver that we described in Section 32.4 is mathematically elegant but
not very practical. The diï¬ƒculty is in computing T (â„“)
I
and T (â„“)
Q , which are needed to
calculate Tâ„“(32.80). In principle, we could compute them using matched ï¬lters, but
the impulse responses of these ï¬lters would depend on the carrier frequency. For
this frequency to be tunable, we would need a whole set of matched ï¬lters for all the
diï¬€erent carrier frequencies, and this would have dire implications on the required
hardware. Also, designing such ï¬lters can be very tricky when fc is large. Instead,
engineers prefer to convert the received waveform to baseband (Section 7.6) or to
some intermediate frequency, and to then perform the required signal processing
on the converted signal, which does not depend on the carrier frequency.2 There
are numerous diï¬€erent designs along these lines, and we cannot possibly analyze
them all. Instead, we shall focus on one and present the mathematical tools that
are needed to analyze it and many of its ilk.
eâˆ’i2Ï€fct
Front-End
(nonideal BPF)
hFE
Baseband
(nonideal LPF)
h
Yc(t)
ËœYc(t)
Figure 32.3: Converting the received signal to baseband. The diagram uses com-
plex notation; if h is real (which it usually is), then a real implementation would
be reminiscent of Figure 7.11.
Our design is depicted with complex notation in Figure 32.3. The received signal
is denoted Yc and is assumed to be the sum of a mean signal sc and centered
noise Nc.
For the ISI channel the mean signal is xc â‹†hc, but for the sake of
generality we prefer to denote it sc.
We shall assume that sc is an integrable
signal that is bandlimited to W Hz around the carrier frequency fc. The received
signal Yc is fed to a stable real nonideal bandpass ï¬lter of impulse response hFE.
This ï¬lter serves two purposes. The ï¬rst is to serve as a front-end ï¬lter of the
kind we discussed in Section 26.7 and brieï¬‚y in Section 26.8. For this purpose it
2Applying the results of Chapter 7 directly is, however, mathematically dubious, because the
noise component of the received signal is neither energy-limited nor integrable. Nevertheless, the
results of Chapter 7 do provide the right intuition; see Appendix D.
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

824
Intersymbol Interference
is typically required to satisfy
Ë†hFE(f) = 1,
|f| âˆ’fc
 â‰¤W
2
(32.97)
so as to pass the mean signal sc unaltered; cf. (26.60). The second is to convert the
received waveform Yc into a (random) passband signal that can be then converted
to baseband. For this purpose we typically require that
Ë†hFE(f) = 0,
|f| âˆ’fc
 > WFE
2
(32.98a)
for some
0 < WFE
2
< fc.
(32.98b)
The front-end ï¬lter does depend on the carrier frequency, but only very coarsely.
And it is typically not required to reject all the out-of-band noise, so its impulse
response need not be very precise.
The output Yc â‹†hFE of the front-end ï¬lter is next converted to baseband using
the following procedure: it is multiplied by t 	â†’eâˆ’i2Ï€fct, and the result is then fed
to a stable ï¬lterâ€”possibly-complex, but usually realâ€” of impulse response h. We
denote the CSP that the procedure produces ËœYc so
ËœYc =

t 	â†’eâˆ’i2Ï€fct(Yc â‹†hFE)(t)

â‹†h.
(32.99)
This is reminiscent of the procedure described in Proposition 7.6.3 with Ë‡g0 replaced
by h. (In practice, engineers often choose h to be the impulse response of a stable
ï¬lter that closely resembles an ideal unit-gain lowpass ï¬lter of cutoï¬€frequency
W/2. The ideal unit-gain lowpass ï¬lter itself is, alas, not stable.) For the ï¬lter to
â€œreject the image,â€ we shall assume that
Ë†h(f) = 0,
|f| > WBB,
(32.100a)
where WBB, in analogy to (7.35b), satisï¬es
0 < WBB â‰¤2fc âˆ’WFE
2
.
(32.100b)
And for the procedure to produce the baseband representation of sc in the absence
of noise, we require that
Ë†h(f) = 1,
|f| â‰¤W
2 .
(32.101)
Our main result is that from the baseband ï¬lterâ€™s output ËœYc we can recover Tâ„“using
a (complex) matched ï¬lter. More precisely, if the front-end ï¬lter satisï¬es (32.97)
and the baseband lowpass ï¬lter satisï¬es (32.101), then
Tâ„“=
4
2
N0
#
ËœYc, t 	â†’pc(t âˆ’â„“Ts)
$
,
â„“âˆˆZ.
(32.102)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.5 From Passband to Baseband
825
32.5.1
Computing Inner Products in Baseband
To derive (32.102) we shall begin by showing thatâ€”as long as the front-end ï¬lter
is of unit gain over the frequency band of interest (32.97)â€” we can compute the
passband inner products T (â„“)
I
and T (â„“)
Q
of (32.80) from Yc â‹†hFE.
Proposition 32.5.1 (

Y, gPB

and

Y â‹†hFE, gPB

). Let gPB be a real integrable
signal that is bandlimited to W Hz around the carrier frequency fc, and let hFE be
the real impulse response of a stable ï¬lter satisfying (32.97). If Y is a SP satisfying
sup
tâˆˆR
E

|Y (t)|

< âˆ,
(32.103)
then

Y, gPB

=

Y â‹†hFE, gPB

with probability 1.
(32.104)
Proof. Using Lemma 15.7.1 on the associativity of the convolution with the sub-
stitution of hFE for r; of Y for

X(t)

; of ~gPB for h; and of 0 for t, we obtain that
with probability one, the two integrals
 âˆ
âˆ’âˆ
Y (âˆ’Ï„)

hFE â‹†~gPB

(Ï„) dÏ„
and
 âˆ
âˆ’âˆ

Y â‹†hFE

(âˆ’Ïƒ)~gPB(Ïƒ) dÏƒ
(32.105)
are both deï¬ned and are equal. The integral on the right in (32.105) is the inner
product on the right in (32.104). And since the front-end ï¬lter has unit gain over
the bandwidth of interest (32.97), hFE â‹†~gPB equals ~gPB, and the integral on the
left in (32.105) is the inner product on the left in (32.104).
Applying Proposition 32.5.1 twiceâ€”once by substituting pc,I,â„“for gPB and once
by substituting pc,Q,â„“for gPBâ€”we obtain that, with probability one, T (â„“)
I
and T (â„“)
Q
of (32.80) can be expressed as
T (â„“)
I
=
4
1
N0
#
Yc â‹†hFE, pc,I,â„“
$
,
â„“âˆˆZ,
(32.106a)
T (â„“)
Q
=
4
1
N0
#
Yc â‹†hFE, pc,Q,â„“
$
,
â„“âˆˆZ,
(32.106b)
where pc,I,â„“and pc,Q,â„“are deï¬ned in (32.79). We now proceed to relate these inner
products to the inner products in baseband.
Proposition 32.5.2. Let gBB be the baseband representation of some real integrable
passband signal gPB that is bandlimited to W Hz around the carrier frequency fc.
Let the SP YPB satisfy the condition
sup
tâˆˆR
E
YPB(t)

< âˆ,
(32.107)
and let the CSP YBB be given by
YBB =

t 	â†’eâˆ’i2Ï€fct YPB(t)

â‹†h,
(32.108)
where h âˆˆL1 is possibly complex. Then, with probability one,
2 Re

âŸ¨YBB, gBBâŸ©

=
#
YPB, t 	â†’2 Re

ei2Ï€fct
gBB â‹†~hâˆ—
(t)
$
.
(32.109)
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

826
Intersymbol Interference
In particular, since gBB â‹†~hâˆ—is of FT f 	â†’Ë†gBB(f) Ë†hâˆ—(f),
	
Ë†h(f) = 1,
|f| â‰¤W
2

=â‡’
	
âŸ¨YPB, gPBâŸ©= 2 Re

âŸ¨YBB, gBBâŸ©

.
(32.110)
Proof. To prove this result we shall justify the following formal calculation
âŸ¨YBB, gBBâŸ©=
#
t 	â†’eâˆ’i2Ï€fct YPB(t)

â‹†h, gBB
$
(32.111)
=
#
t 	â†’eâˆ’i2Ï€fct YPB(t), gBB â‹†~hâˆ—$
(32.112)
=
#
YPB, t 	â†’ei2Ï€fct
gBB â‹†~hâˆ—
(t)
$
,
(32.113)
which implies that
2 Re

âŸ¨YBB, gBBâŸ©

= 2 Re
#
YPB, t 	â†’ei2Ï€fct
gBB â‹†~hâˆ—
(t)
$
=
#
YPB, t 	â†’2 Re

ei2Ï€fct
gBB â‹†~hâˆ—
(t)
$
,
where the second equality holds because YPB is real.
In the above calculation (32.111) follows from the deï¬nition of YBB (32.108),
and (32.113) follows from the deï¬nition of the inner product. The second equal-
ity (32.112) follows from the analogue of Lemma 26.10.7 for complex stochastic
processes and can be justiï¬ed as follows:
#
t 	â†’eâˆ’i2Ï€fct YPB(t)

â‹†h, gBB
$
=
 âˆ
âˆ’âˆ
	 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ YPB(Ï„) h(t âˆ’Ï„) dÏ„

gâˆ—
BB(t) dt
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ YPB(Ï„)
 âˆ
âˆ’âˆ
h(t âˆ’Ï„) gâˆ—
BB(t) dt dÏ„
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ YPB(Ï„)
	 âˆ
âˆ’âˆ
hâˆ—(t âˆ’Ï„) gBB(t) dt

âˆ—
dÏ„
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ YPB(Ï„)
	 âˆ
âˆ’âˆ
~h
âˆ—(Ï„ âˆ’t) gBB(t) dt

âˆ—
dÏ„
=
#
Ï„ 	â†’eâˆ’i2Ï€fcÏ„ YPB(Ï„), gBB â‹†~hâˆ—$
,
where the swapping of the integration order can be justiï¬ed using Fubiniâ€™s The-
orem, becauseâ€”using arguments similar to those that we used in the proof of
Proposition 25.10.1 (ii)â€”one can show that, with probability one,
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
YPB(Ï„) h(t âˆ’Ï„) gBB(t)
 dÏ„ dt < âˆ.
We next apply Proposition 32.5.2 with Ycâ‹†hFE corresponding to YPB and with ËœYc
therefore corresponding to YBB. Starting from (32.106) and using the explicit form
(32.79) of the baseband representations of pc,I,â„“and pc,Q,â„“(namely, the mapping
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.6 Additional Reading
827
t 	â†’pc(tâˆ’â„“Ts)/
âˆš
2 for the former and t 	â†’i pc(tâˆ’â„“Ts)/
âˆš
2 for the latter), we obtain
that, if (32.97) and (32.101) hold, then with probability one
T (â„“)
I
=
4
1
N0
2 Re
#
ËœYc, t 	â†’
1
âˆš
2 pc(t âˆ’â„“Ts)
$
,
â„“âˆˆZ,
(32.114)
and
T (â„“)
Q
=
4
1
N0
2 Re
#
ËœYc, t 	â†’
1
âˆš
2 i pc(t âˆ’â„“Ts)
$
(32.115)
=
4
1
N0
2 Im
#
ËœYc, t 	â†’
1
âˆš
2 pc(t âˆ’â„“Ts)
$
,
(32.116)
so
Tâ„“=
4
2
N0
#
ËœYc, t 	â†’pc(t âˆ’â„“Ts)
$
,
â„“âˆˆZ.
(32.117)
This expression for Tâ„“is the one we promised in (32.102).
We state this as a
theorem:
Theorem 32.5.3. Let pc be some integrable complex signal that is bandlimited to
W/2 Hz, and let the passband signals pc,I,â„“and pc,Q,â„“be given in (32.79). Let Yc
be some SP satisfying
sup
tâˆˆR
E

|Yc(t)|

< âˆ,
and let ËœYc be given by (32.99), where hFE âˆˆL1 is real and satisï¬es (32.97) and
h âˆˆL1 is possibly complex and satisï¬es (32.101). Then Tâ„“of (32.80) is given
by (32.102).
In some applications the baseband signal ËœYc is used not only to compute inner
products but also for timing recovery or for estimating other parameters. For such
applications it is important to characterize the distribution of the baseband noise.
This is explored in Appendix D.
32.6
Additional Reading
The literature on communication over linearly-dispersive channels is vast. Good
starting points are (Proakis and Salehi, 2007) and (Barry, Lee, and Messerschmitt,
2004). These texts deal inter alia with suboptimal receiver designs for scenarios
where Îºmax is too large for the implementation of the Viterbi Algorithm. They
also deal with additional properties of the whitening ï¬lter (such as minimum-
phase) that are sometimes desirable. But please note that some of the results in
the literature do not guarantee the existence of a stable ï¬lter with these additional
properties but only one whose impulse response is square summable.
For more on Dynamic Programming, see (Bertsekas, 2005) and (Cormen, Leiserson,
Rivest, and Stein, 2009).
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

828
Intersymbol Interference
32.7
Exercises
Exercise 32.1 (The Matched-Filter Bound). Consider uncoded PAM communication with
antipodal signaling over an ISI channel (32.2), (32.3), (32.4). Prove that if Ë†
Xj is any guess
of Xj based on the received waveform Yc, then
Pr
$ Ë†
Xj Ì¸= Xj
%
â‰¥Q
â›
â
&
2A2 âˆ¥gc â‹†hcâˆ¥2
2
N0
â
â .
(32.118)
Exercise 32.2 (Edges of the Trellis). Consider the trellis of Section 32.3.3 when n > 4,
the constellation X is {Â±1}, and Îºmax = 3. Which edges are incident from (leave) the
node (j, s), where j = 4 and s = (+1, âˆ’1, âˆ’1)? Which edges are incident to (enter) this
node?
Exercise 32.3 (A Known Symbol). Just before you are about to implement the ML
guessing rule for the postulated model of Section 32.3.1, a genie reveals to you that the
symbol Xjâ‹†is equal to xjâ‹†. How would you guess the remaining symbols?
Exercise 32.4 (Terminating Symbols). How would you modify the Viterbi Algorithm for
the case where the data D1, . . . , Dk are mapped to X1, . . . , Xk using antipodal signaling
(10.2) and the symbols Xk+1, . . . , Xk+Îºmax are all +1?
Exercise 32.5 (A Known Data Bit). Consider uncoded PAM over an ISI channel where
the data bits are mapped to symbols using the mapping (10.3) (4-PAM). How would you
modify the guessing rule of Section 32.3 if just before you started decoding you were told
that the ï¬rst data bit is zero?
Exercise 32.6 (No Cursor). We never assumed that in the postulated model â„0 is nonzero.
Does the Viterbi Algorithm also work when â„0 is zero? How does this aï¬€ect the algorithm?
Exercise 32.7 (A LPF instead of a Matched Filter). Consider PAM communication over
an ISI channel as in (32.2), (32.3), (32.4) but with a diï¬€erent receiver structure: the
received waveform Yc is fed to a stable ï¬lter whose frequency response closely resembles
"
LPFW (see (6.39) and Figure 6.2), and the ï¬lterâ€™s output is sampled at all integer multiples
of Ts to form the sequence
 ËœTâ„“

. Describe the discrete-time channel from the symbols (Xâ„“)
to
 ËœTâ„“

.
Exercise 32.8 (Zero-Forcing Equalization). Consider uncoded antipodal PAM commu-
nication over an ISI channel as in (32.2), (32.3), (32.4) with the additional assumption
that the time shifts of the pulse shape gc by integer multiples of Ts are orthonormal. A
zero-forcing equalizer passes the received signal Yc through a stable ï¬lter whose frequency
response over the band [âˆ’W, W ] is the reciprocal of Ë†hc. It then guesses Xâ„“by comparing
to zero the inner product between the ï¬lterâ€™s output and t â†’gc(t âˆ’â„“Ts). Calculate the
probability of error of the zero-forcing equalizer. Explain why this approach is unsuitable
when the frequency response of hc has a small magnitude at some frequencies inside the
band [âˆ’W, W ].
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,

32.7 Exercises
829
Exercise 32.9 (ISI with Colored Noise). Propose a decoder for uncoded PAM communi-
cation over an ISI channel that is similar to the one described in Section 32.1 but where
the noise

Nc(t)

is not white: it is a centered measurable stationary Gaussian SP of
PSD SNN that can be whitened with respect to W (Deï¬nition 26.10.1).
Exercise 32.10 (Two ISI Channels). Suppose the waveform xc is transmitted over two
linearly-dispersive channels, each of which is as depicted in Figure 32.1. The noises on the
two channels are independent, and the two ï¬lters are diï¬€erent: their impulse responses
are hc,1 and hc,2. Based on the two outputs Yc,1, Yc,2 we wish to guess the data. How
would you adapt Section 32.3 to this setting?
Exercise 32.11 (Allpass Channels). For the setting of Section 32.2, show that if the
frequency response of the channel ï¬lter is of unit magnitude over the bandwidth of interest,
i.e.,
Ë†hc(f)
 = 1,
|f| â‰¤W,
then any performance that can be achieved on the ISI channel can also be achieved on
the channel where Yc = xc + Nc and vice versa.
Hint: Consider ï¬ltering Yc.
available at 
.034
15:01:47, subject to the Cambridge Core terms of use,
www.ebook3000.com

Appendix A
On the Fourier Series
A.1
Introduction and Preliminaries
We survey here some of the results on the Fourier Series that are used in the book.
The Fourier Series has numerous other applications that we do not touch upon.
For those we refer the reader to (Katznelson, 2004), (Dym and McKean, 1972),
and (KÂ¨orner, 1988).
To simplify typography, we denote the half-open interval [âˆ’1/2, 1/2) by I:
I â‰œ
'
Î¸ âˆˆR : âˆ’1
2 â‰¤Î¸ < 1
2
(
.
(A.1)
Deï¬nition A.1.1 (Fourier Series Coeï¬ƒcient). The Î·-th Fourier Series Coef-
ï¬cient of an integrable function g: I â†’C is denoted by Ë†g(Î·) and is deï¬ned for
every integer Î· by
Ë†g(Î·) â‰œ

I
g(Î¸) eâˆ’i2Ï€Î·Î¸ dÎ¸.
(A.2)
The periodic extension of the function g: I â†’C is denoted by gP : R â†’C and
is deï¬ned as
gP(n + Î¸) = g(Î¸),

n âˆˆZ, Î¸ âˆˆI

.
(A.3)
We say that g: I â†’C is periodically continuous if its periodic extension gP is
continuous, i.e., if g(Â·) is continuous in I and if, additionally,
lim
Î¸â†‘1/2 g(Î¸) = g(âˆ’1/2).
(A.4)
A degree-n trigonometric polynomial is a function of the form
Î¸ 	â†’
n

Î·=âˆ’n
aÎ· ei2Ï€Î·Î¸,
Î¸ âˆˆR,
(A.5)
where an and aâˆ’n are not both zero. Note that if p(Â·) is a trigonometric polynomial,
then p(Î¸ + 1) = p(Î¸) for all Î¸ âˆˆR.
830
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,

A.1 Introduction and Preliminaries
831
If g: I â†’C is integrable, and if p(Â·) is a trigonometric polynomial, then we deï¬ne
the convolution g â‹†p at every Î¸ âˆˆR as
(g â‹†p)(Î¸) =

I
g(Ï‘) p(Î¸ âˆ’Ï‘) dÏ‘
(A.6)
=

I
p(Ï‘) gP(Î¸ âˆ’Ï‘) dÏ‘.
(A.7)
Lemma A.1.2 (Convolution with a Trigonometric Polynomial). The convolution
of an integrable function g: I â†’C with the trigonometric polynomial
Î¸ 	â†’
n

Î·=âˆ’n
aÎ· ei2Ï€Î·Î¸
(A.8)
is the trigonometric polynomial
Î¸ 	â†’
n

Î·=âˆ’n
Ë†g(Î·) aÎ· ei2Ï€Î·Î¸,
Î¸ âˆˆR.
(A.9)
Proof. Denote the trigonometric polynomial in (A.8) by p(Â·). By swapping sum-
mation and integration we obtain
(g â‹†p)(Î¸) =

I
g(Ï‘) p(Î¸ âˆ’Ï‘) dÏ‘
=

I
g(Ï‘)
n

Î·=âˆ’n
aÎ· ei2Ï€Î·(Î¸âˆ’Ï‘) dÏ‘
=
n

Î·=âˆ’n

I
g(Ï‘) aÎ· ei2Ï€Î·(Î¸âˆ’Ï‘) dÏ‘
=
n

Î·=âˆ’n
aÎ· ei2Ï€Î·Î¸

I
g(Ï‘) eâˆ’i2Ï€Î·Ï‘ dÏ‘
=
n

Î·=âˆ’n
aÎ· ei2Ï€Î·Î¸Ë†g(Î·),
Î¸ âˆˆR.
Deï¬nition A.1.3 (FejÂ´erâ€™s Kernel). FejÂ´erâ€™s degree-n kernel kn is the trigono-
metric polynomial
kn(Î¸) =
n

Î·=âˆ’n
	
1 âˆ’
|Î·|
n + 1

ei2Ï€Î·Î¸
(A.10a)
=
â§
âª
â¨
âª
â©
n + 1
if Î¸ âˆˆZ,
1
n+1
	
sin((n+1)Ï€Î¸)
sin(Ï€Î¸)

2
if Î¸ âˆˆR \ Z.
(A.10b)
The key properties of FejÂ´erâ€™s kernel are that it is nonnegative
kn(Î¸) â‰¥0,
Î¸ âˆˆR;
(A.11a)
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,
www.ebook3000.com

832
On the Fourier Series
that it integrates over I to one

I
kn(Î¸) dÎ¸ = 1;
(A.11b)
and that for every ï¬xed 0 < Î´ < 1/2
lim
nâ†’âˆ

Î´<|Î¸|< 1
2
kn(Î¸) dÎ¸ = 0.
(A.11c)
Here (A.11a) follows from (A.10b); (A.11b) follows from (A.10a) by term-by-term
integration over I; and (A.11c) follows from the inequality
kn(Î¸) â‰¤
1
n + 1
	
1
sin Ï€Î´

2
,
Î´ â‰¤|Î¸| â‰¤1
2,
which follows from (A.10b) by upper-bounding the numerator by 1 and by using
the monotonicity of sin2(Ï€Î¸) in |Î¸| âˆˆ[0, 1/2].
For an integrable function g: I â†’C, we deï¬ne for every n âˆˆN and Î¸ âˆˆR
Ïƒn(g, Î¸) â‰œ(g â‹†kn)(Î¸)
(A.12)
=
n

Î·=âˆ’n
	
1 âˆ’
|Î·|
n + 1

Ë†g(Î·) ei2Ï€Î·Î¸,
(A.13)
where the second equality follows from (A.10a) and Lemma A.1.2. We also deï¬ne
for g: R â†’C or g: I â†’C
âˆ¥gâˆ¥I,1 =

I
|g(Î¸)| dÎ¸.
(A.14)
Finally, for every function h: I â†’C and Ï‘ âˆˆR we deï¬ne the mapping hÏ‘ : R â†’C
as
hÏ‘ : Î¸ 	â†’hP(Î¸ âˆ’Ï‘).
(A.15)
A.2
Reconstruction in L1
Lemma A.2.1. If g: R â†’C is integrable over I and g(Î¸ + 1) = g(Î¸) for every
Î¸ âˆˆR, then
lim
Ï‘â†’0

I
g(Î¸) âˆ’g(Î¸ âˆ’Ï‘)
 dÎ¸ = 0.
(A.16)
Proof. This is easy to see if g is continuous, because in this case it is also uniformly
continuous (due to its periodicity). The general result follows from this case by
picking a periodic continuous function h that approximates g in the sense that
âˆ¥g âˆ’hâˆ¥I,1 < Ïµ/2; by computing
âˆ¥g âˆ’gÏ‘âˆ¥I,1 = âˆ¥g âˆ’h + h âˆ’gÏ‘âˆ¥I,1
= âˆ¥g âˆ’h + h âˆ’hÏ‘ + hÏ‘ âˆ’gÏ‘âˆ¥I,1
â‰¤âˆ¥g âˆ’hâˆ¥I,1 + âˆ¥h âˆ’hÏ‘âˆ¥I,1 + âˆ¥hÏ‘ âˆ’gÏ‘âˆ¥I,1
= âˆ¥g âˆ’hâˆ¥I,1 + âˆ¥h âˆ’hÏ‘âˆ¥I,1 + âˆ¥h âˆ’gâˆ¥I,1
â‰¤Ïµ + âˆ¥h âˆ’hÏ‘âˆ¥I,1 ;
(A.17)
and by then applying the result to h, which is continuous.
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,

A.2 Reconstruction in L1
833
Theorem A.2.2 (Reconstruction in L1). If g: I â†’C is integrable, then
lim
nâ†’âˆ

I
g(Î¸) âˆ’Ïƒn(g, Î¸)
 dÎ¸ = 0.
(A.18)
Proof. Let gP be the periodic extension of g. Then for every Î´ âˆˆ(0, 1/2),

I
g(Î¸) âˆ’Ïƒn(g, Î¸)
 dÎ¸ =

I
gP(Î¸) âˆ’

I
gP(Î¸ âˆ’Ï‘) kn(Ï‘) dÏ‘
 dÎ¸
=

I


I
kn(Ï‘)

gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)

dÏ‘
 dÎ¸
=

I

 Î´
âˆ’Î´
+

Î´<|Ï‘|< 1
2
kn(Ï‘)

gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)

dÏ‘
 dÎ¸, (A.19)
where the ï¬rst equality follows from the deï¬nition of Ïƒn(g, Î¸) (A.12), and where the
second equality follows from (A.11b). We now bound the two integrals in (A.19)
separately:

I

 Î´
âˆ’Î´
kn(Ï‘)

gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)

dÏ‘
 dÎ¸
â‰¤

I
 Î´
âˆ’Î´
kn(Ï‘)
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÏ‘ dÎ¸
(A.20)
=
 Î´
âˆ’Î´

I
kn(Ï‘)
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÎ¸ dÏ‘
=
 Î´
âˆ’Î´
kn(Ï‘)

I
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÎ¸ dÏ‘
â‰¤
 Î´
âˆ’Î´
kn(Ï‘) max
|Ï‘â€²|â‰¤Î´
1
I
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘â€²)
 dÎ¸
2
dÏ‘
â‰¤

I
kn(Ï‘) max
|Ï‘â€²|â‰¤Î´
1
I
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘â€²)
 dÎ¸
2
dÏ‘
= max
|Ï‘|â‰¤Î´

I
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÎ¸,
(A.21)
where the ï¬rst inequality follows from the Triangle Inequality for Integrals (Propo-
sition 2.4.1) and the nonnegativity of kn(Â·) (A.11a), and where the last equality
follows because kn(Â·) integrates to one (A.11b).
The second integral in (A.19) is bounded as follows:

I

Î´<|Ï‘|< 1
2
kn(Ï‘)
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÏ‘ dÎ¸
=

Î´<|Ï‘|< 1
2
kn(Ï‘)

I
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÎ¸ dÏ‘
â‰¤max
Ï‘â€²âˆˆI
1
I
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘â€²)
 dÎ¸
2 
Î´<|Ï‘|< 1
2
kn(Ï‘) dÏ‘
â‰¤2 âˆ¥gâˆ¥I,1

Î´<|Ï‘|< 1
2
kn(Ï‘) dÏ‘.
(A.22)
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,
www.ebook3000.com

834
On the Fourier Series
From (A.19), (A.21), and (A.22) we obtain

I
g(Î¸) âˆ’Ïƒn(g, Î¸)
 dÎ¸ â‰¤max
|Ï‘|â‰¤Î´

I
gP(Î¸ âˆ’Ï‘) âˆ’gP(Î¸)
 dÎ¸
+ 2 âˆ¥gâˆ¥I,1

Î´<|Ï‘|< 1
2
kn(Ï‘) dÏ‘.
(A.23)
Inequality (A.23) establishes the theorem as follows. For every Ïµ > 0 we can ï¬nd
by Lemma A.2.1 some Î´ > 0 such that
max
|Ï‘|â‰¤Î´

I
gP(Î¸ âˆ’Ï‘) âˆ’gP(Î¸)
 dÎ¸ < Ïµ,
(A.24)
and keeping this Î´ > 0 ï¬xed we have by (A.11c)
lim
nâ†’âˆ2 âˆ¥gâˆ¥I,1

Î´<|Ï‘|< 1
2
kn(Ï‘) dÏ‘ = 0.
(A.25)
It thus follows from (A.23), (A.24), and (A.25) that
lim
nâ†’âˆ

I
g(Î¸) âˆ’Ïƒn(g, Î¸)
 dÎ¸ < Ïµ,
(A.26)
from which the theorem follows because Ïµ > 0 was arbitrary.
From Theorem A.2.2 we obtain:
Theorem A.2.3 (Uniqueness Theorem). Let g1, g2 : I â†’C be integrable. If
Ë†g1(Î·) = Ë†g2(Î·),
Î· âˆˆZ,
(A.27)
then g1 and g2 are equal except on a set of Lebesgue measure zero.
Proof. Let g = g1 âˆ’g2. By (A.27)
Ë†g(Î·) = 0,
Î· âˆˆZ,
(A.28)
and consequently, by (A.13), Ïƒn(g, Î¸) = 0 for every n âˆˆN and Î¸ âˆˆI. By Theo-
rem A.2.2
lim
nâ†’âˆ

I
g(Î¸) âˆ’Ïƒn(g, Î¸)
 dÎ¸ = 0,
(A.29)
which combines with (A.28) to establish that

I
|g(Î¸)| dÎ¸ = 0.
Thus, g is zero except on a set of Lebesgue measure zero (Proposition 2.5.3 (i)),
and the result follows by recalling that g = g1 âˆ’g2.
Theorem A.2.4 (Riemann-Lebesgue Lemma). If g: I â†’C is integrable, then
lim
|Î·|â†’âˆË†g(Î·) = 0.
(A.30)
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,

A.3 Geometric Considerations
835
Proof. Given any Ïµ > 0, let p be a degree-n trigonometric polynomial satisfying

I
g(Î¸) âˆ’p(Î¸)
 dÎ¸ < Ïµ.
(A.31)
(Such a trigonometric polynomial exists for some n âˆˆN by Theorem A.2.2). Ex-
pressing g as (g âˆ’p) + p and using the linearity of the Fourier Series Coeï¬ƒcients
we obtain for every integer Î· whose magnitude exceeds the degree n of p
Ë†g(Î·)
 =
 
(g âˆ’p)(Î·) + Ë†p(Î·)

=
 
(g âˆ’p)(Î·)

â‰¤

I
g(Î¸) âˆ’p(Î¸)
 dÎ¸
< Ïµ,
(A.32)
where the equality in the ï¬rst line follows from the linearity of the Fourier Series
Coeï¬ƒcient; the equality in the second line holds because |Î·| is larger than the
degree n of p; the inequality in the third line holds because for every integrable
h: I â†’C we have
Ë†h(Î·)
 =


I
h(Î¸) eâˆ’i2Ï€Î·Î¸ dÎ¸

â‰¤

I
h(Î¸) eâˆ’i2Ï€Î·Î¸ dÎ¸
=

I
|h(Î¸)| dÎ¸,
Î· âˆˆZ;
and where the inequality in the last line of (A.32) follows from (A.31).
A.3
Geometric Considerations
Every square-integrable function that is zero outside the interval [âˆ’1/2, 1/2] is also
integrable (Proposition 3.4.3). For such functions we can discuss the inner product
and some of the related geometry. The main result is the following.
Theorem A.3.1 (Complete Orthonormal System). The bi-inï¬nite sequence of
functions . . . , Ï†âˆ’1, Ï†0, Ï†1, . . . deï¬ned for every Î· âˆˆZ by
Ï†Î·(Î¸) = ei2Ï€Î·Î¸ I

Î¸ âˆˆI

,
Î¸ âˆˆR
forms a complete orthonormal system (CONS) for the subspace of L2 consisting of
those energy-limited functions that are zero outside the interval I.
Proof. The orthonormality follows by direct calculation

I
ei2Ï€Î·Î¸ eâˆ’i2Ï€Î·â€²Î¸ dÎ¸ = I{Î· = Î·â€²},
Î·, Î·â€² âˆˆZ.
(A.33)
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,
www.ebook3000.com

836
On the Fourier Series
To show completeness it suï¬ƒces by Proposition 8.6.5 (ii) to show that a square-
integrable function g: I â†’C that satisï¬es
âŸ¨g, Ï†Î·âŸ©= 0,
Î· âˆˆZ
(A.34)
must be equal to the all-zero function except on a subset of I of Lebesgue measure
zero. To show this, we note that
âŸ¨g, Ï†Î·âŸ©= Ë†g(Î·),
Î· âˆˆZ,
(A.35)
so (A.34) is equivalent to
Ë†g(Î·) = 0,
Î· âˆˆZ,
(A.36)
and hence, by Theorem A.2.3, g must be zero except on a set of Lebesgue measure
zero.
Recalling Deï¬nition 8.2.1 and Proposition 8.2.2 (d) we obtain that, because the
functions . . . , Ï†âˆ’1, Ï†0, Ï†1, . . . form a CONS and because âŸ¨g, Ï†Î·âŸ©= Ë†g(Î·), the fol-
lowing holds:
Theorem A.3.2. Let g, h: I â†’C be square integrable. Then

I
|g(Î¸)|2 dÎ¸ =
âˆ

Î·=âˆ’âˆ
|Ë†g(Î·)|2
(A.37)
and

I
g(Î¸) hâˆ—(Î¸) dÎ¸ =
âˆ

Î·=âˆ’âˆ
Ë†g(Î·) Ë†hâˆ—(Î·).
(A.38)
There is nothing special about the interval I, and, indeed, by scaling we obtain:
Theorem A.3.3. Let S be positive.
(i) The bi-inï¬nite sequence of functions deï¬ned for every Î· âˆˆZ by
s 	â†’
1
âˆš
S
ei2Ï€Î·s/S I
1
âˆ’S
2 â‰¤s < S
2
2
,
s âˆˆR
(A.39)
forms a CONS for the class of square-integrable functions that are zero out-
side the interval [âˆ’S/2, S/2).
(ii) If g is square integrable and zero outside the interval [âˆ’S/2, S/2), then
 S/2
âˆ’S/2
|g(Î¾)|2 dÎ¾ =
âˆ

Î·=âˆ’âˆ

 S/2
âˆ’S/2
g(s) 1
âˆš
S
eâˆ’i2Ï€Î·s/S ds

2
.
(A.40)
(iii) If g, h: R â†’C are square integrable and zero outside the interval [âˆ’S/2, S/2),
then
 S/2
âˆ’S/2
g(Î¾) hâˆ—(Î¾) dÎ¾
=
âˆ

Î·=âˆ’âˆ
	 S/2
âˆ’S/2
g(s) 1
âˆš
S
eâˆ’i2Ï€Î·s/S ds

	 S/2
âˆ’S/2
h(s) 1
âˆš
S
eâˆ’i2Ï€Î·s/S ds

âˆ—
.
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,

A.3 Geometric Considerations
837
Note A.3.4. The theorem continues to hold if we replace the half-open interval
with the open interval (âˆ’S/2, S/2) or with the closed interval [âˆ’S/2, S/2], because
the integrals are insensitive to these replacements.
Note A.3.5. We refer to
 S/2
âˆ’S/2
g(s) 1
âˆš
S
eâˆ’i2Ï€Î·s/S ds
as the Î·-th Fourier Series Coeï¬ƒcient of g with respect to the interval
[âˆ’S/2, S/2).
Lemma A.3.6 (A Mini Parseval Theorem).
(i) If
x(t) =
 W
âˆ’W
g(f) ei2Ï€ft df,
t âˆˆR,
(A.41)
where g: R â†’C satisï¬es
 W
âˆ’W
|g(f)|2 df < âˆ,
(A.42)
then
 âˆ
âˆ’âˆ
|x(t)|2 dt =
 W
âˆ’W
|g(f)|2 df.
(A.43)
(ii) If for both Î½ = 1 and Î½ = 2
xÎ½(t) =
 W
âˆ’W
gÎ½(f) ei2Ï€ft df,
t âˆˆR,
(A.44)
where the functions g1, g2 : R â†’C satisfy
 W
âˆ’W
|gÎ½(f)|2 df < âˆ,
Î½ = 1, 2,
(A.45)
then
 âˆ
âˆ’âˆ
x1(t) xâˆ—
2(t) dt =
 W
âˆ’W
g1(f) gâˆ—
2(f) df.
(A.46)
Proof. We ï¬rst prove Part (i). We begin by expressing the energy in x in the form
 âˆ
âˆ’âˆ
|x(t)|2 dt =
âˆ

â„“=âˆ’âˆ
 âˆ’
â„“
2W +
1
2W
âˆ’
â„“
2W
|x(t)|2 dt
=
âˆ

â„“=âˆ’âˆ

1
2W
0
x

Î± âˆ’
â„“
2W

2
dÎ±
=

1
2W
0
âˆ

â„“=âˆ’âˆ
x

Î± âˆ’
â„“
2W

2
dÎ±,
(A.47)
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,
www.ebook3000.com

838
On the Fourier Series
where in the second equality we changed the integration variable to Î± â‰œt+â„“/(2W);
and where the third equality follows from Fubiniâ€™s Theorem and the nonnegativity
of the integrand. The proof of Part (i) will follow from (A.47) once we show that
for every Î± âˆˆR
âˆ

â„“=âˆ’âˆ
x

Î± âˆ’
â„“
2W

2
= 2W
 W
âˆ’W
|g(f)|2 df.
(A.48)
This can be shown by noting that by (A.41)
1
âˆš
2W
x

Î± âˆ’
â„“
2W

=
 W
âˆ’W
1
âˆš
2W
eâˆ’i2Ï€f
â„“
2W ei2Ï€fÎ±g(f) df,
so (2W)âˆ’1/2x

Î± âˆ’â„“/(2W)

is the â„“-th Fourier Series Coeï¬ƒcient of the mapping
f 	â†’ei2Ï€fÎ±g(f) with respect to the interval [âˆ’W, W) and consequently
âˆ

â„“=âˆ’âˆ

1
âˆš
2W
x

Î± âˆ’
â„“
2W

2
=
 W
âˆ’W
ei2Ï€fÎ±g(f)
2 df
=
 W
âˆ’W
|g(f)|2 df,
where the ï¬rst equality follows from Theorem A.3.3 (ii) and the second because
the magnitude of ei2Ï€fÎ± is one.
To prove Part (ii) we note that by opening the square and then applying Part (i)
to the function Î²x1 + x2 we obtain for every Î² âˆˆC
|Î²|2
 âˆ
âˆ’âˆ
x1(t)
2 dt +
 âˆ
âˆ’âˆ
x2(t)
2 dt + 2 Re
	
Î²
 âˆ
âˆ’âˆ
x1(t) xâˆ—
2(t) dt

=
 âˆ
âˆ’âˆ
Î²x1(t) + x2(t)
2 dt
=
 W
âˆ’W
Î²g1(f) + g2(f)
2 df
= |Î²|2
 W
âˆ’W
g1(f)
2 df +
 W
âˆ’W
g2(f)
2 df + 2 Re
	
Î²
 W
âˆ’W
g1(f) gâˆ—
2(f) df

.
Consequently, upon applying Part (i) to x1 and to x2, we obtain
Re
	
Î²
 âˆ
âˆ’âˆ
x1(t) xâˆ—
2(t) dt

= Re
	
Î²
 W
âˆ’W
g1(f) gâˆ—
2(f) df

,
Î² âˆˆC,
which implies that
 âˆ
âˆ’âˆ
x1(t) xâˆ—
2(t) dt =
 W
âˆ’W
g1(f) gâˆ—
2(f) df.
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,

A.4 Pointwise Reconstruction
839
Corollary A.3.7.
(i) Let y: R â†’C be of ï¬nite energy, and let T > 0 be arbitrary. Let
Ëœg(f) =
 T
âˆ’T
y(t) eâˆ’i2Ï€ft dt,
f âˆˆR.
Then
 T
âˆ’T
|y(t)|2 dt =
 âˆ
âˆ’âˆ
|Ëœg(f)|2 df.
(ii) Let the signals x1, x2 : R â†’C be of ï¬nite energy, and let T > 0. Deï¬ne
gÎ½(f) =
 T
âˆ’T
xÎ½(t) eâˆ’i2Ï€ft dt,

Î½ = 1, 2,
f âˆˆR

.
Then
 T
âˆ’T
x1(t) xâˆ—
2(t) dt =
 âˆ
âˆ’âˆ
g1(f) gâˆ—
2(f) df.
Proof. Part (i) follows from Lemma A.3.6 (i) by substituting T for W; by substi-
tuting ~y for g; and by swapping the dummy variables f and t. Part (ii) follows
analogously.
A.4
Pointwise Reconstruction
If g: I â†’C is periodically continuous, then we can reconstruct its value at every
point from its Fourier Series Coeï¬ƒcients:
Theorem A.4.1 (Reconstructing Periodically Continuous Functions). Let the
function g: I â†’C be periodically continuous. Then
lim
nâ†’âˆmax
Î¸âˆˆI
g(Î¸) âˆ’Ïƒn(g, Î¸)

= 0.
(A.49)
Proof. Let gP denote the periodic extension of g. Then for every Î¸ âˆˆI,
g(Î¸) âˆ’Ïƒn(g, Î¸) = g(Î¸) âˆ’

I
kn(Ï‘) gP(Î¸ âˆ’Ï‘) dÏ‘
=

I
kn(Ï‘)

gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)

dÏ‘,
(A.50)
where the ï¬rst equality follows from the deï¬nition of Ïƒn(g, Î¸) (A.12) and the second
from (A.11b). Consequently, for every Î¸ âˆˆI,
g(Î¸) âˆ’Ïƒn(g, Î¸)

â‰¤

I
kn(Ï‘)
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÏ‘
=
 Î´
âˆ’Î´
+

Î´<|Ï‘|< 1
2
kn(Ï‘)
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÏ‘,
0 â‰¤Î´ < 1
2.
(A.51)
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,
www.ebook3000.com

840
On the Fourier Series
We next treat the two integrals separately. For the ï¬rst we have for every Î¸ âˆˆI
and every 0 â‰¤Î´ < 1/2,
 Î´
âˆ’Î´
kn(Ï‘)
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÏ‘ â‰¤max
|Ï‘â€²|â‰¤Î´
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘â€²)
  Î´
âˆ’Î´
kn(Ï‘) dÏ‘
â‰¤max
|Ï‘|â‰¤Î´
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)

,
(A.52)
where the ï¬rst inequality follows from the the nonnegativity of kn(Â·) (A.11a), and
where the second inequality follows because kn(Â·) is nonnegative and integrates
over I to one (A.11b). For the second integral in (A.51) we have for every Î¸ âˆˆI
and every 0 â‰¤Î´ < 1/2,

Î´<|Ï‘|< 1
2
kn(Ï‘)
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 dÏ‘ â‰¤2 max
Î¸â€²âˆˆI {|g(Î¸â€²)|}

Î´<|Ï‘|< 1
2
kn(Ï‘) dÏ‘, (A.53)
where the maximum on the RHS is ï¬nite because g is periodically continuous.
Combining (A.51), (A.52), and (A.53) we obtain for every 0 â‰¤Î´ < 1/2
max
Î¸âˆˆI
g(Î¸) âˆ’Ïƒn(g, Î¸)

â‰¤max
Î¸âˆˆI max
|Ï‘|â‰¤Î´
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)

+ 2 max
Î¸â€²âˆˆI
g(Î¸â€²)
 
Î´<|Ï‘|< 1
2
kn(Ï‘) dÏ‘.
(A.54)
Because g(Â·) is periodically continuous, it follows that its periodic extension gP is
uniformly continuous. Consequently, for every Ïµ > 0 we can ï¬nd some Î´ > 0 such
that
max
|Ï‘|â‰¤Î´
gP(Î¸) âˆ’gP(Î¸ âˆ’Ï‘)
 < Ïµ,
Î¸ âˆˆI.
(A.55)
By letting n tend to inï¬nity in (A.54), we obtain from (A.11c) and (A.55)
lim
nâ†’âˆmax
Î¸âˆˆI
g(Î¸) âˆ’Ïƒn(g, Î¸)

< Ïµ,
which establishes the result because Ïµ > 0 was arbitrary.
As a corollary we obtain:
Corollary A.4.2 (Weierstrassâ€™s Approximation Theorem). Every periodically con-
tinuous function from I to C can be approximated uniformly using trigonometric
polynomials.
available at 
.035
15:03:35, subject to the Cambridge Core terms of use,

Appendix B
On the Discrete-Time Fourier Transform
The Discrete-Time Fourier Transform (DTFT) is to absolutely summable bi-inï¬nite
sequences what the Fourier Transform is to integrable functions. Here we summa-
rize its basic properties.
Recall that a bi-inï¬nite sequence . . . , aâˆ’1, a0, a1, . . . is absolutely summable if
âˆ

Î½=âˆ’âˆ
|aÎ½| < âˆ,
and that the set containing all such sequences is denoted â„“1. Recall also that I
denotes the half-closed interval
I =
'
Î¸ âˆˆR : âˆ’1
2 â‰¤Î¸ < 1
2
(
and that its closure Â¯I is thus
Â¯I =
'
Î¸ âˆˆR : âˆ’1
2 â‰¤Î¸ â‰¤1
2
(
.
Deï¬nition B.1 (The Discrete-Time Fourier Transform). The Discrete-Time
Fourier Transform (DTFT) of a complex absolutely summable bi-inï¬nite se-
quence (aÎ½) is the mapping ua: R â†’C deï¬ned by
ua: Î¸ 	â†’
âˆ

Î½=âˆ’âˆ
aÎ½ eâˆ’i2Ï€Î½Î¸.
(B.1)
The key properties of the DTFT are summarized below.
Theorem B.2 (Properties of the DTFT). Let (aÎ½) be a complex absolutely summable
bi-inï¬nite sequence.
(i) The sum in (B.1) converges uniformly on R.
(ii) The function ua(Â·) is continuous and periodic with
ua(Î¸ + 1) = ua(Î¸),
Î¸ âˆˆR.
841
.036
15:03:43, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

842
On the Discrete-Time Fourier Transform
(iii) The sequence (aÎ½) can be recovered from its DTFT ua via
aÎ½ =

I
ua(Î¸) ei2Ï€Î½Î¸ dÎ¸,
Î½ âˆˆZ.
(B.2)
Thus, aâˆ’Î½ is the Î½-th Fourier Series Coeï¬ƒcient of ua(Â·)
aâˆ’Î½ = Ë†ua(Î½),
Î½ âˆˆZ.
(B.3)
(iv) If (bÎ½) is another absolutely summable bi-inï¬nite sequence, then
âˆ

Î½=âˆ’âˆ
aÎ½ bâˆ—
Î½ =

I
ua(Î¸) ubâˆ—(Î¸) dÎ¸
(B.4)
and, in particular,
âˆ

Î½=âˆ’âˆ
|aÎ½|2 =

I
ua(Î¸)
2 dÎ¸.
(B.5)
(v) If (bÎ½) is as above, then the DTFT of (aÎ½)â‹†(bÎ½) is the mapping Î¸ 	â†’ua(Î¸)ub(Î¸):
Å
a â‹†b(Î¸) = ua(Î¸) ub(Î¸),
Î¸ âˆˆR.
(B.6)
Proof. Part (i) follows by observing that, since the complex exponentials are of
modulus one,
aÎ½ eâˆ’i2Ï€Î½Î¸ â‰¤|aÎ½|,
Î¸ âˆˆR
and, since the RHS of the above is summable, the uniform convergence of the sum
follows from Weierstrassâ€™s test (Rudin, 1976, Ch. 7, Theorem 7.10).
As to Part (ii), the continuity of ua follows from the continuity of the complex
exponentials and from Part (i) as follows: the former implies that the partial sums
are continuous, the latter implies that they converge uniformly, and if a sequence
of continuous functions converges uniformly, then the limit must be a continuous
function (Rudin, 1976, Theorem 7.12). The periodicity follows from the periodicity
of the complex exponentials:
eâˆ’i2Ï€Î½(Î¸+1) = eâˆ’i2Ï€Î½Î¸,
(Î¸ âˆˆR, Î½ âˆˆZ).
To verify Part (iii) we calculate:

I
ua(Î¸) ei2Ï€Î½Î¸ dÎ¸ =

I
	
âˆ

Î½â€²=âˆ’âˆ
aÎ½â€² eâˆ’i2Ï€Î½â€²Î¸

ei2Ï€Î½Î¸ dÎ¸
=

I
âˆ

Î½â€²=âˆ’âˆ
aÎ½â€² ei2Ï€(Î½âˆ’Î½â€²)Î¸ dÎ¸
=
âˆ

Î½â€²=âˆ’âˆ
aÎ½â€²

I
ei2Ï€(Î½âˆ’Î½â€²)Î¸ dÎ¸
=
âˆ

Î½â€²=âˆ’âˆ
aÎ½â€² I{Î½â€² = Î½}
= aÎ½,
Î½ âˆˆZ,
.036
15:03:43, subject to the Cambridge Core terms of use, available at

On the Discrete-Time Fourier Transform
843
where the term-by-term integration is justiï¬ed by the uniform convergence (Rudin,
1976, Corollary to Theorem 7.16), and where we have also used (A.33).
We next turn to Part (iv). By Part (iii) we can express aâˆ’Î½ as the Î½-th Fourier
Series Coeï¬ƒcient of ua, i.e.,
aâˆ’Î½ = Ë†ua(Î½),
Î½ âˆˆZ.
Likewise,
bâˆ’Î½ = Ë†ub(Î½),
Î½ âˆˆZ,
so Part (iv) follows from Theorem A.3.2.
As to Part (v), we calculate the DTFT of the convolution as follows:
âˆ

Î·=âˆ’âˆ

(aÎ½) â‹†(bÎ½)

Î· eâˆ’i2Ï€Î·Î¸ =
âˆ

Î·=âˆ’âˆ
	
âˆ

Î½=âˆ’âˆ
aÎ½bÎ·âˆ’Î½

eâˆ’i2Ï€Î·Î¸
=
âˆ

m=âˆ’âˆ
bm
âˆ

Î½=âˆ’âˆ
aÎ½ eâˆ’i2Ï€(m+Î½)Î¸
=
âˆ

m=âˆ’âˆ
bm eâˆ’i2Ï€mÎ¸
âˆ

Î½=âˆ’âˆ
aÎ½ eâˆ’i2Ï€Î½Î¸
= ub(Î¸) ua(Î¸),
Î¸ âˆˆR,
where we have deï¬ned m as Î· âˆ’Î½.
Every absolutely summable sequence is square summable (31.1), but some se-
quences are square summable but not absolutely summable.1 The next theorem
allows us to deï¬ne the discrete-time analog of the L2-Fourier Transform (Sec-
tion 6.2.3): it allows us to deï¬ne the DTFT of any square-summable sequence,
even if is not absolutely summable.
Theorem B.3 (L2-DTFT).
(i) If (aÎ½) is a square-summable bi-inï¬nite complex sequence, then there exists a
function g: R â†’C such that
g(Î¸ + 1) = g(Î¸),
Î¸ âˆˆR,
(B.7a)

I
|g(Î¸)|2 dÎ¸ < âˆ,
(B.7b)
and
aÎ½ =

I
g(Î¸) ei2Ï€Î½Î¸ dÎ¸,
Î½ âˆˆZ.
(B.7c)
Moreover,
lim
nâ†’âˆ

I

n

Î½=âˆ’n
aÎ½ eâˆ’i2Ï€Î½Î¸ âˆ’g(Î¸)

2
dÎ¸ = 0.
(B.7d)
1An example of a sequence in â„“2 but not â„“1 is Î½ â†’1/(|Î½| + 1).
.036
15:03:43, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

844
On the Discrete-Time Fourier Transform
The function g is unique in the sense that two functions that satisfy (B.7)
can diï¬€er only on a set of Lebesgue measure zero. Consequently, if (aÎ½) is
additionally absolutely summable, then g(Â·) must be indistinguishable from
the mapping
Î¸ 	â†’
âˆ

Î½=âˆ’âˆ
aÎ½ eâˆ’i2Ï€Î½Î¸.
The equivalence class of g(Â·) is called the L2-Discrete-Time Fourier Trans-
form or L2-DTFT of (aÎ½).
(ii) If g: R â†’C is any function satisfying (B.7a) and (B.7b), and if we deï¬ne
the sequence (aÎ½) as in (B.7c), then (aÎ½) is square summable.
(iii) If g(Â·) is the L2-DTFT of the square-summable bi-inï¬nite sequence (aÎ½), and
if h(Â·) is the L2-DTFT of the square-summable bi-inï¬nite sequence (bÎ½), then
âˆ

Î½=âˆ’âˆ
aÎ½bâˆ—
Î½ =

I
g(Î¸) hâˆ—(Î¸) dÎ¸.
Proof. See (Katznelson, 2004, Chapter I, Section 5.5).
.036
15:03:43, subject to the Cambridge Core terms of use, available at

Appendix C
Positive Deï¬nite Functions
Complex positive deï¬nite functions appear inter alia as the characteristic func-
tions of random variables (Sections 19.7.1 and 17.3.6) and as the autocovariance
functions of complex stochastic processes. Real positive deï¬nite functions appear
as the characteristic functions of symmetric RVs or as the autocovariance functions
of (real) SPs.
Deï¬nition C.1 (Positive Deï¬nite Function). A function h: R â†’C is said to be
positive deï¬nite if for every positive integer n and all choices of x1, . . . , xn âˆˆR
and Î±1, . . . , Î±n âˆˆC
n

j=1
n

k=1
Î±j Î±âˆ—
k h(xj âˆ’xk) â‰¥0.
(C.1)
Allowing the coeï¬ƒcients Î±1, . . . , Î±n in (C.1) to be complex is not very natural when
h(Â·) is real. The following proposition shows that if h(Â·) is real and symmetric, then
it suï¬ƒces to establish (C.1) for real Î±â€™s.
Proposition C.2 (Real Positive Deï¬nite Functions). A real function h: R â†’R
is positive deï¬nite if, and only if, it satisï¬es the following two conditions: it is
symmetric (i.e., ~h = h) and (C.1) holds for all x1, . . . , xn âˆˆR and Î±1, . . . , Î±n âˆˆR.
The basic properties of positive deï¬nite functions are summarized below (Feller,
1971, Chapter XIX, Section 2, Lemma 3).
Proposition C.3 (Basic Properties). If h: R â†’C is positive deï¬nite, then
(i) it is conjugate symmetric
h(âˆ’x) = hâˆ—(x),
x âˆˆR;
(ii) its value at zero h(0) is nonnegative (and hence real)
h(0) â‰¥0;
(iii) it attains its maximum at zero in the sense that
|h(x)| â‰¤h(0),
x âˆˆR;
845
available at 
.037
15:04:57, subject to the Cambridge Core terms of use,
www.ebook3000.com

846
Positive Deï¬nite Functions
(iv) if h(Â·) is continuous at the origin, then it is continuous everywhere; and
(v) it satisï¬es
h(x) âˆ’h(y)
2 â‰¤2 h(0)

h(0) âˆ’Re(h(x âˆ’y))

,
x, y âˆˆR.
Theorem C.4 (Bochner). If h: R â†’C is a continuous positive deï¬nite function,
then h is proportional to a characteristic function of some RV V in the sense that
h(x) = h(0) Â· E

ei2Ï€xV 
,
x âˆˆR.
If additionally h is real-valued, then it can be expressed as above with the RV V
having a symmetric distribution.
Since the Inverse Fourier-Stieltjes Transform is injective (Rudin, 1962, Chapter 1,
Section 1.3, Theorem 1.3.6), Bochnerâ€™s Theorem has the following corollary.
Corollary C.5. If h: R â†’C is a positive deï¬nite function that is equal to the IFT
of some integrable function S: R â†’C, then S(Â·) must be nonnegative outside a set
of Lebesgue measure zero.
Theorem C.6 (Riesz-Crum).
(i) Every Lebesgue measurable positive deï¬nite function h: R â†’C can be ex-
pressed uniquely as
h(x) = hc(x) + hs(x),
x âˆˆR,
where hc(Â·) is a positive deï¬nite function that is continuous, and hs(Â·) is a
positive deï¬nite function that is zero outside a set of Lebesgue measure zero.
(ii) If h is real valued, then so are hc and hs; if h is integrable, then so is hc
(and hs); if h is symmetric (i.e., ~h = h) then so are hc and hs.
Proof. See Crum (1956).
If a nonzero positive deï¬nite function is continuous and also integrable, then it is
proportional to the characteristic function of a RV with a density, and the density
can be computed using the Fourier Transform (Feller, 1971, Chapter XV, Section 3,
Theorem 3, and Chapter XIX, Section 2, Lemma 1). Hence:
Proposition C.7 (Integrable Positive Deï¬nite Functions). A continuous integrable
positive deï¬nite function is equal to the IFT of its FT.
Dropping the continuity assumption yields a slightly weaker conclusion:
available at 
.037
15:04:57, subject to the Cambridge Core terms of use,

Positive Deï¬nite Functions
847
Corollary C.8. Let h: R â†’C be a positive deï¬nite integrable function, and let hc
and hs be as in Theorem C.6 (i).
(i) The IFT of the FT of h is equal to hc
Ë‡Ë†h = hc
(C.2)
and is thus indistinguishable from h
Ë‡Ë†h â‰¡h.
(C.3)
(ii) The FT of h is nonnegative
Ë†h(f) â‰¥0,
f âˆˆR.
(C.4)
Proof. Since h = hc+hs and hs is zero outside a set of Lebesgue measure zero, the
functions h and hc are indistinguishable and thus of identical Fourier Transforms
Ë†h = Ë†hc.
(C.5)
To prove (C.2) we note that (C.5) implies that the IFT of the FT of h is equal to
the IFT of the FT of hc, and the latter is equal to hc by Proposition C.7.
From (C.5) we infer that to establish (C.4) it suï¬ƒces to establish that
Ë†hc(f) â‰¥0,
f âˆˆR.
(C.6)
This is trivial when hc is zero. When it is not, we can establish (C.6) by applying
Bochnerâ€™s Theorem to hc and by using the inversion formula for characteristic func-
tions (Shiryaev, 1996, Chapter II, Â§ 12, Theorem 3) or (Feller, 1971, Chapter XV,
Section 3, Theorem 3).
available at 
.037
15:04:57, subject to the Cambridge Core terms of use,
www.ebook3000.com

Appendix D
The Baseband Representation of Passband
Stochastic Processes
Many of the results of Chapter 7 on the baseband representation of passband signals
also apply to stationary stochastic processes. But the proofs must be altered to
avoid applying the Fourier Transform to sample-paths, because those are typically
neither integrable nor of ï¬nite energy.
To state the results we need to extend the deï¬nition of a Gaussian CSP (Deï¬ni-
tion 31.4.1) and of a proper CSP (Deï¬nition 17.5.4) from discrete time to contin-
uous time. The extensions are obvious but are provided nonetheless.
Deï¬nition D.1 (Gaussian, Proper, and Circularly-Symmetric CSP). A continuous-
time CSP

Z(t), t âˆˆR

is said to be Gaussian if every complex random vector
of the form

Z(t1), . . . Z(tn)
T,
where n âˆˆN and t1, . . . , tn âˆˆR, is a complex Gaussian vector (Deï¬nition 24.3.6).
It is said to be proper if every such complex random vector is proper (Deï¬ni-
tion 17.4.1). And it is said to be circularly-symmetric if every such complex
random vector is circularly-symmetric (Deï¬nition 24.3.2).
Since a complex Gaussian vector is proper if, and only if, it is circularly-symmetric
(Proposition 24.3.11) we have:
Note D.2. A Gaussian CSP is proper if, and only if, it is circularly-symmetric.
We shall also need the deï¬nition of a WSS CSP in continuous time (the counterpart
to Deï¬nition 17.5.6).
Deï¬nition D.3 (WSS Continuous-Time CSP). A continuous-time CSP

Z(t)

is
said to be wide-sense stationary if it is of ï¬nite variance; the mean E[Z(t)] does
not depend on t; and Cov[Z(t + Ï„), Z(t)] does not depend on t. Its autocovariance
function KZZ is the mapping Ï„ 	â†’Cov[Z(t + Ï„), Z(t)]. The CSP or the autocovari-
ance function is said to be of PSD SZZ if SZZ is nonnegative, integrable, and its
IFT is KZZ.
Using Proposition 24.3.7 we can mimick the proof of Proposition 25.5.1 to conclude:
848
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,

The Baseband Representation of Passband Stochastic Processes
849
Note D.4 (For Proper Gaussians, WSS = stationary). A proper Gaussian CSP
is wide-sense stationary if, and only if, it is stationary.
The ï¬rst result is on the synthesis of a passband SP from a baseband CSP. It is
particularly useful for computer simulations of passband systems.
Theorem D.5 (Synthesizing a Passband SP from a Baseband CSP). Let

XBB(t)

be a proper WSS CSP of autocovariance function KBB, and let the SP

X(t)

be
given by
X(t) = 2 Re

XBB(t) ei2Ï€fct
,
t âˆˆR,
(D.1)
where fc > 0 is arbitrary.
(i) The SP

X(t)

is centered and WSS with autocovariance function
Ï„ 	â†’2 Re

KBB(Ï„) ei2Ï€fcÏ„
.
(D.2)
(ii) If

XBB(t)

is of PSD SBB, then

X(t)

is of PSD
f 	â†’SBB(f âˆ’fc) + SBB(âˆ’f âˆ’fc).
(D.3)
(iii) If

XBB(t)

is Gaussian, then so is

X(t)

.
Proof. We begin by rewriting (D.1) in the form
X(t) = XBB(t) ei2Ï€fct + Xâˆ—
BB(t) eâˆ’i2Ï€fct,
t âˆˆR
(D.4)
to emphasize that X(t) is a linear functional of the pair XBB(t) and Xâˆ—
BB(t). We
now proceed to prove Part (i). Since

XBB(t)

is WSS, it is of ï¬nite variance and
therefore so is

X(t)

, because (D.1) implies that
X(t)
 â‰¤2
XBB(t) ei2Ï€fct
and hence that
E

X2(t)

â‰¤4 E

|XBB(t)|2
.
Since

XBB(t)

is proper, it is centered, and therefore so is

X(t)

(see (D.4)).
To complete the proof of Part (i) we next compute E[X(t + Ï„) X(t)].
We ï¬rst
substitute t + Ï„ for t in (D.4), to obtain
X(t + Ï„) = XBB(t + Ï„) ei2Ï€fc(t+Ï„) + Xâˆ—
BB(t + Ï„) eâˆ’i2Ï€fc(t+Ï„).
(D.5)
Using (D.5) and (D.4) we can now compute
E

X(t + Ï„) X(t)

= E

XBB(t + Ï„) XBB(t)

ei2Ï€fc(2t+Ï„) + E

XBB(t + Ï„) Xâˆ—
BB(t)

ei2Ï€fcÏ„
+ E

Xâˆ—
BB(t + Ï„) XBB(t)

eâˆ’i2Ï€fcÏ„ + E

Xâˆ—
BB(t + Ï„) Xâˆ—
BB(t)

eâˆ’i2Ï€(2fc+Ï„)
= E

XBB(t + Ï„) Xâˆ—
BB(t)

ei2Ï€fcÏ„ + E

Xâˆ—
BB(t + Ï„) XBB(t)

eâˆ’i2Ï€fcÏ„
= E

XBB(t + Ï„) Xâˆ—
BB(t)

ei2Ï€fcÏ„ +

E

XBB(t + Ï„) Xâˆ—
BB(t)

ei2Ï€fcÏ„âˆ—
= KBB(Ï„) ei2Ï€fcÏ„ +

KBB(Ï„) ei2Ï€fcÏ„âˆ—
= 2 Re

KBB(Ï„) ei2Ï€fcÏ„
,
t, Ï„ âˆˆR,
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

850
The Baseband Representation of Passband Stochastic Processes
where the second equality follows from the hypothesis that

XBB(t)

is proper.
To prove Part (ii) we note that since KBB is the IFT of SBB, the mapping in (D.2)
is the IFT of the mapping f 	â†’SBB(f âˆ’fc)+Sâˆ—
BB(âˆ’f âˆ’fc), and the latter is equal
to the mapping in (D.3) because SBB is real.
Part (iii) follows from (D.4) and the fact that linear transformations map Gaussian
vectors to Gaussian vectors (Proposition 24.3.9).
The second result is on the conversion from passband to baseband.
Theorem D.6 (From Passband to Baseband (and back)). Let

XPB(t)

be a
measurable centered WSS SP of PSD SPB that is â€œpassbandâ€ in the sense that
SPB(f) = 0,
|f| âˆ’fc
 > W
2 ,
(D.6)
where
fc > W
2 > 0.
(D.7)
Let the (possibly complex) impulse response h be integrable
h âˆˆL1;
(D.8a)
of unit gain in the band [âˆ’W/2, W/2]
Ë†h(f) = 1,
|f| â‰¤W
2 ;
(D.8b)
and image rejecting in the sense that
Ë†h(f) = 0,
|f| â‰¥2fc âˆ’W
2 .
(D.8c)
Let the CSP

XBB(t)

be given by (cf. Proposition 7.6.3)
XBB =

t 	â†’eâˆ’i2Ï€fctXPB(t)

â‹†h.
(D.9)
(i) The CSP XBB is proper, WSS, and of PSD
f 	â†’SPB(f + fc) I
'
|f| â‰¤W
2
(
.
(D.10)
(ii) The power in

XPB(t)

is twice the power in

XBB(t)

, and
E

X2
PB(t)

= 2 E

|XBB(t)|2
,
t âˆˆR.
(D.11)
(iii) At every ï¬xed epoch t âˆˆR
XPB(t) = 2 Re

XBB(t) ei2Ï€fct
,
with probability one.
(D.12)
(iv) If vPB is an integrable passband signal that is bandlimited to W Hz around
the carrier frequency fc, and if vBB is its baseband representation, then

XPB, vPB

= 2 Re

XBB, vBB

,
with probability one.
(D.13)
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,

The Baseband Representation of Passband Stochastic Processes
851
(v) If

XPB(t)

is Gaussian, then so is

XBB(t)

.
(vi) If

XPB(t)

is Gaussian and if its PSD SPB, in addition to satisfying (D.6),
is also symmetric around the carrier frequency fc in the sense that
SPB

fc âˆ’Ëœf

= SPB

fc + Ëœf

,
 Ëœf
 â‰¤W
2 ,
(D.14)
then the stochastic processes

Re(XBB(t)), t âˆˆR

and

Im(XBB(t)), t âˆˆR

are independent centered stationary Gaussian stochastic processes, each of
which is of PSD
f 	â†’1
2 SPB(f + fc) I
'
|f| â‰¤W
2
(
.
(D.15)
Proof. Let KPB be the autocovariance function of XPB. To establish that XBB is
proper and WSS, we need to show inter alia that it is of bounded variance. This
can be established using Fubiniâ€™s Theorem as in the proof of Proposition 25.10.1.
The details are therefore omitted.
In the course of the proof we shall repeatedly use the fact that the theoremâ€™s
hypotheses about SPB and Ë†h imply that for every f âˆˆR,
SPB(f + fc) Ë†h(f) = SPB(f + fc) Ë†h(f) I
'
|f| â‰¤W
2
(
(D.16)
= SPB(f + fc) I
'
|f| â‰¤W
2
(
,
(D.17)
where the ï¬rst equality follows from (D.6) and (D.8c), and where the second equal-
ity follows from (D.8b).
To prove Part (i) we begin by showing that XBB is proper. It is clearly centered,
because XPB is centered and by (D.9)
XBB(t) =
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏƒXPB(Ïƒ) h(t âˆ’Ïƒ) dÏƒ,
t âˆˆR.
(D.18)
To show that E

XBB(t1) XBB(t2)

is zero for all t1, t2 âˆˆR we ï¬rst note that,
by (D.18),
XBB(t1) XBB(t2)
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏƒXPB(Ïƒ) h(t1 âˆ’Ïƒ) dÏƒ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÎ¼XPB(Î¼) h(t2 âˆ’Î¼) dÎ¼
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fc(Ïƒ+Î¼) h(t1 âˆ’Ïƒ) h(t2 âˆ’Î¼) XPB(Ïƒ) XPB(Î¼) dÏƒ dÎ¼,
(D.19)
so
E

XBB(t1) XBB(t2)

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fc(Ïƒ+Î¼) h(t1 âˆ’Ïƒ) h(t2 âˆ’Î¼) E

XPB(Ïƒ)XPB(Î¼)

dÏƒ dÎ¼
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fc(Ïƒ+Î¼) h(t1 âˆ’Ïƒ) h(t2 âˆ’Î¼) KPB(Ïƒ âˆ’Î¼) dÏƒ dÎ¼.
(D.20)
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

852
The Baseband Representation of Passband Stochastic Processes
Replacing the integration variables (Ïƒ, Î¼) with (Ïƒ, Ï„) where Ï„ = Ïƒ âˆ’Î¼ we obtain
E

XBB(t1) XBB(t2)

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fc(2Ïƒâˆ’Ï„) h(t1 âˆ’Ïƒ) h(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„ dÏƒ
=
 âˆ
âˆ’âˆ
eâˆ’i4Ï€fcÏƒ h(t1 âˆ’Ïƒ)
 âˆ
âˆ’âˆ
ei2Ï€fcÏ„ h(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„ dÏƒ.
(D.21)
We next use Proposition 6.2.4 (

xË‡gâˆ—=

Ë†xgâˆ—) to compute the inner integral
 âˆ
âˆ’âˆ
ei2Ï€fcÏ„ h(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„.
(D.22)
Indeed, Ï„ 	â†’ei2Ï€fcÏ„ KPB(Ï„) is the complex conjugate of Ï„ 	â†’eâˆ’i2Ï€fcÏ„ KPB(Ï„), and
the latter is the IFT of f 	â†’SPB(f + fc), which thus plays the role of g. And the
mapping Ï„ 	â†’h(t2 + Ï„ âˆ’Ïƒ), which plays the role of x is of FT f 	â†’ei2Ï€(t2âˆ’Ïƒ) Ë†h(f).
Hence, by Proposition 6.2.4 (and the fact that SPB is real)
 âˆ
âˆ’âˆ
ei2Ï€fcÏ„ h(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„
=
 âˆ
âˆ’âˆ
SPB(f + fc) ei2Ï€(t2âˆ’Ïƒ)f Ë†h(f) df
=

W
2
âˆ’W
2
SPB(f + fc) ei2Ï€(t2âˆ’Ïƒ)f df,
(D.23)
where the second equality follows from (D.17). Substituting (D.23) in (D.21) yields
E

XBB(t1) XBB(t2)

=
 âˆ
âˆ’âˆ
eâˆ’i4Ï€fcÏƒ h(t1 âˆ’Ïƒ)

W
2
âˆ’W
2
SPB(f + fc) ei2Ï€(t2âˆ’Ïƒ)f df dÏƒ
=

W
2
âˆ’W
2
ei2Ï€t2f SPB(f + fc)
 âˆ
âˆ’âˆ
eâˆ’i2Ï€Ïƒ(2fc+f) h(t1 âˆ’Ïƒ) dÏƒ df.
(D.24)
The inner integral is the FT of Ïƒ 	â†’h(t1 âˆ’Ïƒ) evaluated at 2fc + f. Since the FT
of Ïƒ 	â†’h(t1 âˆ’Ïƒ) is Ëœf 	â†’eâˆ’i2Ï€ Ëœ
ft1 Ë†h(âˆ’Ëœf) we obtain, upon substituting 2fc + f for Ëœf,
 âˆ
âˆ’âˆ
eâˆ’i2Ï€Ïƒ(2fc+f) h(t1 âˆ’Ïƒ) dÏƒ = eâˆ’i2Ï€(2fc+f)t1 Ë†h(âˆ’2fc âˆ’f).
(D.25)
Substituting (D.25) in (D.24) we obtain
E

XBB(t1) XBB(t2)

=

W
2
âˆ’W
2
ei2Ï€t2f SPB(f + fc) eâˆ’i2Ï€(2fc+f)t1 Ë†h(âˆ’2fc âˆ’f) df
= 0,
(D.26)
where the second equality holds because over the range of integration, i.e., for
|f| â‰¤W/2, the integrand is zero because, by (D.8c), Ë†h(âˆ’2fc âˆ’f) is zero. Since
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,

The Baseband Representation of Passband Stochastic Processes
853
we have already established that XBB is centered, and since (D.26) holds for all
t1, t2 âˆˆR, it follows that XBB is proper.
We next show that XBB is WSS. To this end, we compute E[XBB(t1) Xâˆ—
BB(t2)] for
arbitrary epochs t1, t2 âˆˆR. As in (D.19) and (D.20),
XBB(t1) Xâˆ—
BB(t2)
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏƒXPB(Ïƒ) h(t1 âˆ’Ïƒ) dÏƒ Â·
	 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÎ¼XPB(Î¼) h(t2 âˆ’Î¼) dÎ¼

âˆ—
=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fc(Ïƒâˆ’Î¼) h(t1 âˆ’Ïƒ) hâˆ—(t2 âˆ’Î¼) XPB(Ïƒ) XPB(Î¼) dÏƒ dÎ¼,
(D.27)
so
E

XBB(t1) Xâˆ—
BB(t2)

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fc(Ïƒâˆ’Î¼) h(t1 âˆ’Ïƒ) hâˆ—(t2 âˆ’Î¼) KPB(Ïƒ âˆ’Î¼) dÏƒ dÎ¼.
(D.28)
Replacing the integration variables (Ïƒ, Î¼) with (Ïƒ, Ï„) where Ï„ = Ïƒ âˆ’Î¼ we obtain
E

XBB(t1) Xâˆ—
BB(t2)

=
 âˆ
âˆ’âˆ
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ h(t1 âˆ’Ïƒ) hâˆ—(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„ dÏƒ
=
 âˆ
âˆ’âˆ
h(t1 âˆ’Ïƒ)
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ hâˆ—(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„ dÏƒ.
(D.29)
We can now use Proposition 6.2.4 (

xË‡gâˆ—=

Ë†xgâˆ—or, upon conjugation,

xâˆ—Ë‡g =

Ë†xâˆ—g) to compute the inner integral
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ hâˆ—(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„.
Indeed, Ï„ 	â†’eâˆ’i2Ï€fcÏ„ KPB(Ï„) is the IFT of f 	â†’SPB(f + fc) and Ï„ 	â†’h(t2 + Ï„ âˆ’Ïƒ)
is of FT f 	â†’ei2Ï€(t2âˆ’Ïƒ)f Ë†h(f), so
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏ„ hâˆ—(t2 + Ï„ âˆ’Ïƒ) KPB(Ï„) dÏ„
=
 âˆ
âˆ’âˆ
eâˆ’i2Ï€(t2âˆ’Ïƒ)f Ë†hâˆ—(f) SPB(f + fc) df
=

W
2
âˆ’W
2
eâˆ’i2Ï€(t2âˆ’Ïƒ)f SPB(f + fc) df,
(D.30)
where the second equality follows from (D.17). Substituting (D.30) in (D.29) yields
E

XBB(t1) Xâˆ—
BB(t2)

=
 âˆ
âˆ’âˆ
h(t1 âˆ’Ïƒ)

W
2
âˆ’W
2
eâˆ’i2Ï€(t2âˆ’Ïƒ)f SPB(f + fc) df dÏƒ
=

W
2
âˆ’W
2
eâˆ’i2Ï€t2f SPB(f + fc)
 âˆ
âˆ’âˆ
ei2Ï€Ïƒf h(t1 âˆ’Ïƒ) dÏƒ df.
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

854
The Baseband Representation of Passband Stochastic Processes
The inner integral is the FT of Ïƒ 	â†’h(t1 âˆ’Ïƒ) evaluated at âˆ’f. Since this FT is
Ëœf 	â†’eâˆ’i2Ï€ Ëœ
ft1 Ë†h(âˆ’Ëœf), the inner integral evaluates to
 âˆ
âˆ’âˆ
ei2Ï€Ïƒf h(t1 âˆ’Ïƒ) dÏƒ = ei2Ï€ft1 Ë†h(f),
and thus
E

XBB(t1) Xâˆ—
BB(t2)

=

W
2
âˆ’W
2
eâˆ’i2Ï€t2f SPB(f + fc) ei2Ï€ft1 Ë†h(f) df
=

W
2
âˆ’W
2
ei2Ï€f(t1âˆ’t2) SPB(f + fc) df,
(D.31)
where the last equality follows from (D.8b). The RHS of (D.31) depends on t1 and
on t2 only via their diï¬€erence t1 âˆ’t2, so XBB is WSS. From (D.31) we also see
that the autocovariance function of XBB is the IFT of the mapping in (D.10), so
the latter is the PSD of XBB.
To prove Part (ii) we shall establish (D.11), which implies that the power in XPB
is twice the power in XBB (Proposition 25.9.2 and its complex version). The LHS
of (D.11) is the integral of SPB over all the frequencies f âˆˆR. The RHS is twice
the integral of the mapping in (D.10) (because this mapping is by Part (i) the PSD
of XBB). The two are equal because XPB is real and its PSD is hence symmetric.
We next turn to Part (iii). For every epoch t âˆˆR we deï¬ne the error
e(t) = E

XPB(t) âˆ’2 Re

XBB(t) ei2Ï€fct2
(D.32)
and proceed to show that it is zero. Using the identity
2 Re

XBB(t) ei2Ï€fct
= XBB(t) ei2Ï€fct + Xâˆ—
BB(t) eâˆ’i2Ï€fct
(D.33)
and opening the square, we obtain that
e(t) = E

X2
PB(t)

âˆ’2 E

XPB(t) XBB(t)

ei2Ï€fct âˆ’2 E

XPB(t) Xâˆ—
BB(t)

eâˆ’i2Ï€fct
+ E

X2
BB(t)

ei4Ï€fct + E

(Xâˆ—
BB(t))2
eâˆ’i4Ï€fct + 2 E

|XBB(t)|2
= E

X2
PB(t)

âˆ’2 E

XPB(t) XBB(t)

ei2Ï€fct âˆ’2 E

XPB(t) Xâˆ—
BB(t)

eâˆ’i2Ï€fct
+ 2 E

|XBB(t)|2
= E

X2
PB(t)

+ 2 E

|XBB(t)|2
âˆ’4 Re

E

XPB(t) XBB(t)

ei2Ï€fct
= 4 E

|XBB(t)|2
âˆ’4 Re

E

XPB(t) XBB(t)

ei2Ï€fct
,
(D.34)
where the second equality holds because XBB is proper (Part (i)), and the ï¬nal
equality follows from (D.11). To conclude the proof that e(t) is zero, it thus remains
to establish that
Re

E

XPB(t) XBB(t)

ei2Ï€fct
= E

|XBB(t)|2
,
(D.35)
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,

The Baseband Representation of Passband Stochastic Processes
855
which is what we proceed to do. Beginning with (D.18),
E

XPB(t) XBB(t)

= E

XPB(t)
 âˆ
âˆ’âˆ
eâˆ’i2Ï€fcÏƒXPB(Ïƒ) h(t âˆ’Ïƒ) dÏƒ

=
 âˆ
âˆ’âˆ
E

XPB(t) XPB(Ïƒ)

eâˆ’i2Ï€fcÏƒ h(t âˆ’Ïƒ) dÏƒ
=
 âˆ
âˆ’âˆ
KPB(t âˆ’Ïƒ) eâˆ’i2Ï€fcÏƒ h(t âˆ’Ïƒ) dÏƒ
= eâˆ’i2Ï€fct
 âˆ
âˆ’âˆ
KPB(t âˆ’Ïƒ) ei2Ï€fc(tâˆ’Ïƒ) h(t âˆ’Ïƒ) dÏƒ
= eâˆ’i2Ï€fct
 âˆ
âˆ’âˆ
KPB(Ï„) ei2Ï€fcÏ„ h(Ï„) dÏ„
= eâˆ’i2Ï€fct
 âˆ
âˆ’âˆ
SPB(f) Ë†h(f âˆ’fc) df
= eâˆ’i2Ï€fct
 âˆ
âˆ’âˆ
SPB( Ëœf + fc) Ë†h( Ëœf) d Ëœf
= eâˆ’i2Ï€fct

W
2
âˆ’W
2
SPB( Ëœf + fc) d Ëœf
= eâˆ’i2Ï€fct E

|XBB(t)|2
,
(D.36)
from which (D.35) follows. Here the sixth equality follows from Proposition 6.2.4,
the eighth from (D.17), and the last because the power in XBB is the integral of
its PSD, and the latter is the mapping in (D.10).
The proof of Part (iv) is based on the identity
âŸ¨Z â‹†h, vâŸ©=

Z, ~hâˆ—â‹†v

with probability one,
(D.37)
which is the complex counterpart of (26.113).
Here

Z(t)

is a CSP, and the
signals h and v are integrable complex signals. Using this identity we have with
probability one
2 Re

XBB, vBB

= 2 Re

t 	â†’XPB(t) eâˆ’i2Ï€fct
â‹†h, vBB

= 2 Re

t 	â†’XPB(t) eâˆ’i2Ï€fct, ~hâˆ—â‹†vBB

= 2 Re

t 	â†’XPB(t) eâˆ’i2Ï€fct, vBB

= 2 Re
	 âˆ
âˆ’âˆ
XPB(t)

vBB(t) ei2Ï€fctâˆ—dt

=
 âˆ
âˆ’âˆ
XPB(t) 2 Re

vBB(t) ei2Ï€fctâˆ—
dt
=

XPB, vPB

,
where the ï¬rst equality follows from the deï¬nition of XBB (D.9); the second from
the identity (D.37); the third holds because vBB is bandlimited to W/2 Hz, the FT
of ~hâˆ—is the complex conjugate of Ë†h, and h is of unit-gain in the band [âˆ’W/2, W/2]
(D.8b); the fourth follows from the deï¬nition of the inner product; the ï¬fth holds
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

856
The Baseband Representation of Passband Stochastic Processes
because XPB is real; and the sixth holds because Re(zâˆ—) equals Re(z) and because
vBB is the baseband representation of vPB.
To establish Part (v) note that XPB is a Gaussian SP so

eâˆ’i2Ï€fct XPB(t), t âˆˆR

is a Gaussian CSP. Consequently, XBB, which is the result of ï¬ltering this CSP, is
also a Gaussian CSP.1
We ï¬nally proceed to Part (vi). To establish independence and Gaussianity we need
to show that, for every n âˆˆN and epochs t1, . . . , tn âˆˆR, the random n-vectors

Re

XBB(t1)

, . . . , Re

XBB(tn)
T
and

Im

XBB(t1)

, . . . , Im

XBB(tn)
T
are independent Gaussian vectors (Deï¬nitions 25.2.3 and 25.3.1). They are jointly
Gaussian (Deï¬nition 23.7.1) because, by Part (v) and Deï¬nition 24.3.6, the random
2n-vector

Re

XBB(t1)

, . . . , Re

XBB(tn)

, Im

XBB(t1)

, . . . , Im

XBB(tn)
T
is Gaussian. Being jointly Gaussian, each of them is Gaussian (Corollary 23.6.5).
Moreover, to establish their independence it suï¬ƒces to establish that they are
uncorrelated (Proposition 23.7.3). Since XPB is centered so is XBB, and to establish
independence it remains to show that
E
%
Re

XBB(t)

Im

XBB(tâ€²)
&
= 0,
t, tâ€² âˆˆR.
(D.38)
Using the identities
Re

XBB(t)

= XBB(t) + Xâˆ—
BB(t)
2
,
Im

XBB(tâ€²)

= XBB(tâ€²) âˆ’Xâˆ—
BB(tâ€²)
2i
,
and the fact that XBB is proper we obtain
E
%
Re

XBB(t)

Im

XBB(tâ€²)
&
= E

Xâˆ—
BB(t) XBB(tâ€²)

âˆ’E

Xâˆ—
BB(tâ€²) XBB(t)

4i
= KBB(tâ€² âˆ’t) âˆ’Kâˆ—
BB(tâ€² âˆ’t)
4i
= 1
2 Im

KBB(tâ€² âˆ’t)

= 0,
t, tâ€² âˆˆR,
where KBB is the autocovariance function of XBB, and where the last equality holds
because the symmetry hypothesis (D.14) on SPB implies that the mapping (D.10)
(which is real) is also symmetric and hence that its IFT, namely KBB, is real. We
1We are relying here on the complex version of Theorem 25.13.2 (iv). To prove this it suï¬ƒces
to notice that the proof of Proposition 25.11.1 goes through almost verbatim also for the complex
case.
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,

The Baseband Representation of Passband Stochastic Processes
857
ï¬nally turn to the autocovariance functions.
E
%
Re

XBB(t)

Re

XBB(tâ€²)
&
= E

XBB(t) Xâˆ—
BB(tâ€²)

+ E

Xâˆ—
BB(t) XBB(tâ€²)

4
= KBB(t âˆ’tâ€²) + Kâˆ—
BB(t âˆ’tâ€²)
4
= 1
2 Re

KBB(t âˆ’tâ€²)

= 1
2 KBB(t âˆ’tâ€²),
t, tâ€² âˆˆR,
where the last equality holds because, as we have seen, the symmetry assumption
(D.14) implies that KBB is real. The real part of XBB is thus WSS, and its PSD
is thus the mapping in (D.15). The calculation for the imaginary part of XBB is
nearly identical and is omitted.
available at 
.038
15:04:59, subject to the Cambridge Core terms of use,
www.ebook3000.com

Bibliography
Adams, R. A., and J. J. F. Fournier (2003): Sobolev Spaces. Elsevier B.V.,
Amsterdam, The Netherlands, second edn.
Adler, R. J. (1990): An Introduction to Continuity, Extrema, and Related Topics
for General Gaussian Processes. Institute of Mathematical Statistics, Hayward,
CA.
Axler, S. (2015): Linear Algebra Done Right. Springer-Verlag New York, Inc.,
New York, third edn.
Barry, J. R., E. A. Lee, and D. G. Messerschmitt (2004): Digital Commu-
nication. Springer-Verlag New York, Inc., New York, third edn.
Bertsekas, D. P. (2005): Dynamic Programming and Optimal Control, vol. 1.
Athena Scientiï¬c, Nashua, NH, third edn.
Billingsley, P. (1995): Probability and Measure. John Wiley & Sons, Inc., New
York, third edn.
Blackwell, D., and R. V. Ramamoorthi (1982): â€œA Bayes but not classically
suï¬ƒcient statistic,â€ The Annals of Statistics, 10(3), 1025â€“1026.
Blahut, R. E. (2003): Algebraic Codes for Data Transmission. Cambridge Uni-
versity Press, New York.
Boas, Jr., R. P. (1954): Entire Functions. Academic Press Inc., New York.
Bogachev, V. I. (1998): Gaussian Measures. American Mathematical Society,
Providence, RI.
(2007): Measure Theory: Volume I. Springer-Verlag, Berlin, Germany.
Bryc, W. (1995): The Normal Distribution: Characterizations with Applications.
Springer-Verlag New York, Inc., New York.
Chung, K. L. (2001): A Course in Probability Theory. Academic Press, San Diego,
CA, third edn.
Cormen, T. H., C. E. Leiserson, R. L. Rivest, and C. Stein (2009): Intro-
duction to Algorithms. The MIT Press, Cambridge, MA, third edn.
858
.039
15:06:02, subject to the Cambridge Core terms of use, available at

Bibliography
859
Cover, T. M., and J. A. Thomas (2006): Elements of Information Theory.
Wiley, Hoboken, NJ, second edn.
CramÂ´er, H., and M. R. Leadbetter (2004): Stationary and Related Stochastic
Processes: Sample Function Properties and Their Applications. Dover Publica-
tions, Inc., Mineola, NY.
Crum, M. M. (1956): â€œOn positive deï¬nite functions,â€ Proc. London Math. Soc.,
6(3), 548â€“560.
Davenport, Jr., W. B., and W. L. Root (1987): An Introduction to the Theory
of Random Signals and Noise. IEEE Press, New York.
de Caen, D. (1997): â€œA lower bound on the probability of a union,â€ Discrete
Mathematics, 169, 217â€“220.
Doob, J. L. (1990): Stochastic Processes. John Wiley & Sons, Inc., New York.
Dudley, R. M. (2003): Real Analysis and Probability. Cambridge University
Press, New York, second edn.
Dym, H., and H. P. McKean (1972): Fourier Series and Integrals. Academic
Press, Inc., San Diego.
Farebrother, R. W. (1988):
Linear Least Squares Computations. Marcel
Dekker, Inc., New York.
Feller, W. (1971): An Introduction to Probability Theory and Its Applications,
vol. II. John Wiley & Sons, Inc., New York, second edn.
Finner, H., and M. Roters (1997): â€œLog-concavity and inequalities for chi-
square, F and beta distributions with applications in multiple comparisons,â€
Statistica Sinica, 7(3), 771â€“787.
Forney, Jr., G. D. (1991): â€œGeometrically uniform codes,â€ IEEE Transactions
on Information Theory, 37(5), 1241â€“1260.
Gallager, R. G. (2008): Principles of Digital Communication. Cambridge Uni-
versity Press, New York.
Gikhman, I. I., and A. V. Skorokhod (1996): Introduction to the Theory of
Random Processes. Dover Publications, Inc., Mineola, NY.
Golub, G. H., and C. F. van Loan (1996): Matrix Computations. The John
Hopkins University Press, Baltimore, MD, third edn.
Gray, R. M. (2006): â€œToeplitz and circulant matrices: a review,â€ Foundations
and Trends in Communications and Information Theory, 2(3), 155â€“239.
Grimmett, G., and D. Stirzaker (2001): Probability and Random Processes.
Oxford University Press, New York, third edn.
Halmos, P. R. (1950): Measure Theory. D. Van Nostrand Company, Inc., Prince-
ton, NJ.
.039
15:06:02, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

860
Bibliography
(1958): Finite-Dimensional Vector Spaces. D. Van Nostrand Company,
Inc., Princeton, NJ.
Halmos, P. R., and L. J. Savage (1949): â€œApplication of the Radon-Nikodym
theorem to the theory of suï¬ƒcient statistics,â€ The Annals of Mathematical Sta-
tistics, 20(2), 225â€“241.
Helstrom, C. W. (1995): Elements of Signal Detection and Estimation. Prentice
Hall, Englewood Cliï¬€s, NJ.
Herstein, I. N. (1975): Topics in Algebra. John Wiley & Sons, Inc., New York,
second edn.
Horn, R. A., and C. R. Johnson (2013): Matrix Analysis. Cambridge University
Press, New York, second edn.
Johnson, N. L., and S. Kotz (1972): Distributions in Statistics: Continuous
Multivariate Distributions. John Wiley & Sons, Inc., New York.
Johnson, N. L., S. Kotz, and N. Balakrishnan (1994): Continuous Univari-
ate Distributions, vol. 1. John Wiley & Sons, Inc., New York, second edn.
(1995): Continuous Univariate Distributions, vol. 2. John Wiley & Sons,
Inc., New York, second edn.
Kailath, T., A. H. Sayed, and B. Hassibi (2000): Linear Estimation. Prentice
Hall, Inc., Upper Saddle River, NJ.
Kallenberg, O. (2002): Foundations of Modern Probability. Springer, New York,
second edn.
Karatzas, I., and S. E. Shreve (1991): Brownian Motion and Stochastic Cal-
culus. Springer-Verlag New York, Inc., New York, second edn.
Karlin, S., and W. J. Studden (1966): Tchebycheï¬€Systems: With Applications
in Analysis and Statistics. John Wiley & Sons, Inc., New York.
Katznelson, Y. (2004): An Introduction to Harmonic Analysis. Cambridge Uni-
versity Press, New York, NY, third edn.
KÂ¨orner, T. W. (1988): Fourier Analysis. Cambridge University Press, New York,
NY.
Kwakernaak, H., and R. Sivan (1991): Modern Signals and Systems. Prentice
Hall, Inc., Englewood Cliï¬€s, NJ.
Lehmann, E. L., and J. P. Romano (2005): Testing Statistical Hypotheses.
Springer-Verlag New York, Inc., New York, third edn.
Loeliger, H.-A. (1991): â€œSignal sets matched to groups,â€ IEEE Transactions on
Information Theory, 37(6), 1675â€“1682.
Lo`eve, M. (1963): Probability Theory. D. Van Nostrand Company, Inc., Prince-
ton, NJ, third edn.
.039
15:06:02, subject to the Cambridge Core terms of use, available at

Bibliography
861
Logan, Jr., B. F. (1978): â€œTheory of analytic modulation systems,â€ The Bell
System Technical Journal, 57(3), 491â€“576.
MacWilliams, F. J., and N. J. A. Sloane (1977): The Theory of Error-
Correcting Codes. North-Holland, Amsterdam, The Netherlands.
Marshall, A. W., I. Olkin, and B. C. Arnold (2011): Inequalities: Theory
of Majorization and Its Applications. Springer, New York, NY, second edn.
Nehari, Z. (1975): Conformal Mapping. Dover Publications, Inc., Mineola, NY.
Neveu, J. (1968): Processus AlÂ´eatoires Gaussiens. Les Presses de lâ€™UniversitÂ´e de
MontrÂ´eal.
Oppenheim, A. V., and A. S. Willsky (1997): Signals & Systems. Prentice
Hall, Inc., Upper Saddle River, NJ, second edn.
Pinsky,
M.
A. (2002):
Introduction to Fourier Analysis and Wavelets.
Brooks/Cole, Paciï¬c Grove, CA.
Poor, H. V. (1994):
An Introduction to Signal Detection and Estimation.
Springer-Verlag New York, Inc., New York, second edn.
Porat, B. (2008): Digital Processing of Random Signals: Theory and Methods.
Dover Publications, Inc., Mineola, NY.
Pourahmadi, M. (2001): Foundations of Time Series Analysis and Prediction
Theory. John Wiley & Sons, Inc., New York.
Proakis, J., and M. Salehi (2007): Digital Communications. McGraw-Hill Ed-
ucation, 5th edn.
Requicha, A. A. G. (1980): â€œThe zeros of entire functions: theory and engineer-
ing applications,â€ Proceedings of the IEEE, 68(3), 308â€“328.
Richardson, T., and R. Urbanke (2008): Modern Coding Theory. Cambridge
University Press, New York.
Riesz, F., and B. Sz.-Nagy (1990): Functional Analysis. Dover Publications,
Inc., Mineola, NY.
Romano, J. P., and A. F. Siegel (1986): Counterexamples in Probability and
Statistics. Chapman & Hall, New York.
Ross, D. A. (2005): â€œAn elementary proof of Lyapunovâ€™s theorem,â€ The American
Mathematical Monthly, 112(7), 651â€“653.
Roth, R. M. (2006): Introduction to Coding Theory. Cambridge University Press,
New York.
Royden, H. L., and P. M. Fitzpatrick (2010): Real Analysis. Prentice Hall,
Inc., Upper Saddle River, NJ, fourth edn.
Rudin, W. (1962): Fourier Analysis on Groups. John Wiley & Sons, New York.
.039
15:06:02, subject to the Cambridge Core terms of use, available at
www.ebook3000.com

862
Bibliography
(1976): Principles of Mathematical Analysis. McGraw-Hill, Inc., New
York, third edn.
(1987): Real & Complex Analysis. McGraw-Hill, Inc., New York, third
edn.
Ryan, W., and S. Lin (2009): Channel Codes: Classical and Modern. Cambridge
University Press.
Sason, I., and S. Shamai (2006): â€œPerformance analysis of linear codes under
maximum-likelihood decoding: a tutorial,â€ Foundations and Trends in Commu-
nications and Information Theory, 3(1/2), 1â€“225.
Shiryaev, A. N. (1996): Probability. Springer-Verlag New York, Inc., New York,
second edn.
Simon, B. (2005): Orthogonal Polynomials on the Unit Circle. Part 1: Classical
Theory. American Mathematical Society, Providence RI.
Simon, M. K. (2002): Probability Distributions Involving Gaussian Random Vari-
ables: A Handbook for Engineers and Scientists. Kluwer Academic Publishers,
Norwell, MA.
Slepian, D. (1976): â€œOn bandwidth,â€ Proceedings of the IEEE, 64(3), 292â€“300.
Slepian, D., and H. O. Pollak (1961): â€œProlate spheroidal wave functions,
Fourier analysis and uncertaintyâ€”I,â€ The Bell System Technical Journal, 40(1),
43â€“63.
Steele, J. M. (2004): The Cauchy-Schwarz Master Class: An Introduction to the
Art of Mathematical Inequalities. Cambridge University Press, New York.
Stein, E. M., and G. Weiss (1971): Introduction to Fourier Analysis on Eu-
clidean Spaces. Princeton University Press, Princeton, NJ.
Tong, Y. L. (1990): The Multivariate Normal Distribution. Springer-Verlag New
York, Inc., New York.
Unser, M. (2000): â€œSamplingâ€”50 years after Shannon,â€ Proceedings of the IEEE,
88(4), 569â€“587.
van Lint, J. H. (1998): Introduction to Coding Theory. Springer-Verlag New
York, Inc., New York, third edn.
VerdÂ´u, S. (1998): Multiuser Detection. Cambridge University Press, New York.
Viterbi, A. J., and J. K. Omura (1979): Principles of Digital Communication
and Coding. McGraw-Hill, Inc., New York.
Williams, D. (1991): Probability with Martingales. Cambridge University Press,
New York.
Yaglom, A. (1986): Correlation Theory of Stationary and Related Random Func-
tions: Volume I: Basic Results. Springer-Verlag.
.039
15:06:02, subject to the Cambridge Core terms of use, available at

Bibliography
863
Zhang, F. (2011): Matrix Theory: Basic Results and Techniques. Springer-Verlag
New York, Inc., New York, second edn.
Zvonkin, A. (1997): â€œMatrix integrals and map enumeration: an accessible in-
troduction,â€ Mathematical and Computer Modelling, 26(8â€“10), 281â€“304.
.039
15:06:02, subject to the Cambridge Core terms of use, available at
www.ebook3000.com


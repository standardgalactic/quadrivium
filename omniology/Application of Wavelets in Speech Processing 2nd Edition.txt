123
SPRINGER BRIEFS IN ELECTRICAL AND 
COMPUTER ENGINEERING    SPEECH TECHNOLOGY
Mohamed Hesham Farouk
Application 
of Wavelets 
in Speech 
Processing
  Second Edition  

SpringerBriefs in Electrical and Computer 
Engineering
Series editor
Amy Neustein, Fort Lee, NJ, USA
Speech Technology

Editor’s Note
The authors of this series have been hand-selected. They comprise some of the most 
outstanding scientists—drawn from academia and private industry—whose research 
is marked by its novelty, applicability, and practicality in providing broad based 
speech solutions. The SpringerBriefs in Speech Technology series provides the 
latest findings in speech technology gleaned from comprehensive literature reviews 
and empirical investigations that are performed in both laboratory and real life 
settings. Some of the topics covered in this series include the presentation of real 
life commercial deployment of spoken dialog systems, contemporary methods of 
speech parameterization, developments in information security for automated 
speech, forensic speaker recognition, use of sophisticated speech analytics in call 
centers, and an exploration of new methods of soft computing for improving human-
computer interaction. Those in academia, the private sector, the self service industry, 
law enforcement, and government intelligence, are among the principal audience 
for this series, which is designed to serve as an important and essential reference 
guide for speech developers, system designers, speech engineers, linguists and 
others. In particular, a major audience of readers will consist of researchers and 
technical experts in the automated call center industry where speech processing is a 
key component to the functioning of customer care contact centers.
Amy Neustein, Ph.D., serves as Editor-in-Chief of the International Journal of 
Speech Technology (Springer). She edited the recently published book “Advances in 
Speech Recognition: Mobile Environments, Call Centers and Clinics” (Springer 
2010), and serves as quest columnist on speech processing for Womensenews. 
Dr. Neustein is Founder and CEO of Linguistic Technology Systems, a NJ-based 
think tank for intelligent design of advanced natural language based emotion-
detection software to improve human response in monitoring recorded conversations 
of terror suspects and helpline calls. Dr. Neustein’s work appears in the peer review 
literature and in industry and mass media publications. Her academic books, which 
cover a range of political, social and legal topics, have been cited in the Chronicles 
of Higher Education, and have won her a pro Humanitate Literary Award. She 
serves on the visiting faculty of the National Judicial College and as a plenary 
speaker at conferences in artificial intelligence and computing. Dr. Neustein is a 
member of MIR (machine intelligence research) Labs, which does advanced work in 
computer technology to assist underdeveloped countries in improving their ability 
to cope with famine, disease/illness, and political and social affliction. She is a 
founding member of the New York City Speech Processing Consortium, a newly 
formed group of NY-based companies, publishing houses, and researchers dedicated 
to advancing speech technology research and development.
More information about this series at http://www.springer.com/series/10043

Mohamed Hesham Farouk
Application of Wavelets  
in Speech Processing
Second Edition

ISSN 2191-8112	
        ISSN 2191-8120  (electronic)
SpringerBriefs in Electrical and Computer Engineering
ISSN 2191-737X	                 ISSN 2191-7388  (electronic)
SpringerBriefs in Speech Technology
ISBN 978-3-319-69001-8        ISBN 978-3-319-69002-5  (eBook)
https://doi.org/10.1007/978-3-319-69002-5
Library of Congress Control Number: 2017958884
© The Author(s) 2014, 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of 
the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, 
broadcasting, reproduction on microfilms or in any other physical way, and transmission or information 
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology 
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a specific statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the 
editors give a warranty, express or implied, with respect to the material contained herein or for any errors 
or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims 
in published maps and institutional affiliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
Mohamed Hesham Farouk
Department of Engineering, Math and Physics
Cairo University, Faculty of Engineering
Giza, Egypt

To the soul of my mother

vii
Preface
This is a new edition of the title Application of Wavelets in Speech Processing 
published in 2014. All chapters of previous edition have been revised in this work. 
The sequence of topics is rearranged taking into consideration the dependency 
among topics such that the information flow through book chapters becomes 
smoother. More subjects have been added in the current edition reflecting advances 
in using wavelet analysis for speech processing and its widespread applications.
Organization of the Book
The chapters of this book have been structured such that each one is self-contained 
and can be read separately. Each chapter is concerned with a specific application of 
wavelets in speech technology. Every module in a chapter surveys the literature in 
this topic such that the use of wavelets in the work is explained and experimental 
results of proposed method are then discussed. Chapter 1 introduces the topic of 
speech processing, while Chap. 2 discusses processes for speech production and 
different approaches in modeling of a speech signal. Chapter 3, thereafter, explains 
how wavelets can describe and model many features of a speech signal. Applications 
of wavelet transform (WT) in speech processing are the subjects of subsequent 
chapters. Collectively, the power of WT in estimating spectral characteristics of 
speech is explained in Chap. 4 showing how elements of such spectrum can be 
derived like pitch and formants. Chapter 5 confers the problem of speech activity 
detection and signal separation based on features extracted from WT. Enhancement 
and noise cancellation is revised in Chap. 6 showing how WT improves the process. 
The problem of speech recognition is discussed in Chap. 7 in view of the provided 
powerful features obtained by a wavelet analysis. Another recognition problem is 
considered in Chap. 8 discussing the identification of a speaker from his voice. 
Additionally, a similar topic on emotion recognition through wavelet features in an 
utterance is elucidated in Chap. 9. Another key application of speech is discussed in 
Chap. 10 showing how speech signal can be decoded and synthesized using a 

viii
low-dimensional features domain. Also, the assessment of speech quality based on 
WT coefficients is surveyed and explained in Chap. 11. Chapter 12 has been added 
in this edition to examine how nonlinear features can be extracted from a speech 
signal by WT. Furthermore, Chap. 13 hits critical application of speech signal in 
security and steganography. More arguments on clinical diagnosis are discussed in 
Chap. 14 through wavelet features of speech recorded from a patient.
Acknowledgment
The author would like to thank the editorial board of the SpringerBriefs series for 
letting him prepare this monograph and for their continuous cooperation during the 
preparation of the work. Thanks should also go to my colleagues at the Engineering 
Physics Department, Bahira Elsebelgy, Ph.D., and M. El-Gohary, M.Sc., for helping 
in proofreading.
Giza, Egypt
Mohamed Hesham Farouk
Preface

ix
Contents
	 1	 Introduction.............................................................................................	
1
	1.1	 History and Definition of Speech Processing..................................	
1
	1.2	 Applications of Speech Processing..................................................	
2
	1.3	 Recent Progress in Speech Processing.............................................	
2
	1.4	 Wavelet Analysis as an Efficient Tool for Speech Processing.........	
3
References.................................................................................................	
4
	 2	 Speech Production and Perception........................................................	
5
	2.1	 Speech Production Process..............................................................	
5
	2.2	 Classification of Speech Sounds......................................................	
6
	2.3	 Speech Production Modeling...........................................................	
7
	2.4	 Speech Perception Modeling...........................................................	
8
	2.5	 Intelligibility and Speech Quality Measures....................................	
9
References.................................................................................................	
10
	 3	 Wavelets, Wavelet Filters, and Wavelet Transforms............................	
11
	3.1	 Short-Time Fourier Transform (STFT)............................................	
11
	3.2	 Multiresolution Analysis and Wavelet Transform............................	
12
	3.3	 Wavelets and Bank of Filters...........................................................	
14
	3.4	 Wavelet Families..............................................................................	
15
	3.5	 Wavelet Packets................................................................................	
16
	3.6	 Undecimated Wavelet Transform.....................................................	
18
	3.7	 The Continuous Wavelet Transform (CWT)....................................	
18
	3.8	 Wavelet Scalogram...........................................................................	
19
	3.9	 Empirical Wavelets..........................................................................	
19
References.................................................................................................	
20
	 4	 Spectral Analysis of Speech Signal and Pitch Estimation...................	
23
	4.1	 Spectral Analysis..............................................................................	
23
	4.2	 Formant Tracking and Estimation....................................................	
24
	4.3	 Pitch Estimation...............................................................................	
25
References.................................................................................................	
27

x
	 5	 Speech Detection and Separation..........................................................	   29
	5.1	 Voice Activity Detection..................................................................	
29
	5.2	 Segmentation of Speech Signal.......................................................	
30
	5.3	 Source Separation of Speech...........................................................	
31
References.................................................................................................	
  33
	 6	 Speech Enhancement and Noise Suppression......................................	   35
	6.1	 Thresholding Schemes.....................................................................	
36
	6.2	 Thresholding on Wavelet Packet Coefficients.................................	
37
	6.3	 Enhancement on Multitaper Spectrum.............................................	
38
References.................................................................................................	
  39
	 7	 Speech Recognition.................................................................................	   41
	
7.1	 Signal Enhancement and Noise  
Cancellation for Robust Recognition...............................................	
41
	7.2	 Wavelet-Based Features for Better Recognition..............................	
42
	7.3	 Hybrid Approach..............................................................................	
43
	
7.4	 Wavelet as an Activation Function for Neural  
Networks in ASR.............................................................................	
44
References.................................................................................................	
  45
	 8	 Speaker Identification.............................................................................	   47
	8.1	 Wavelet-Based Features for Speaker Identification.........................	
48
	8.2	 Hybrid Feature Sets for Speaker Identification................................	
49
References.................................................................................................	
  49
	 9	 Emotion Recognition from Speech........................................................	   51
	9.1	 Wavelet-Based Features for Emotion Recognition..........................	
51
	9.2	 Combined Feature Set for Better Emotion Recognition..................	
53
	9.3	 WNN for Emotion Recognition.......................................................	
54
References.................................................................................................	
  54
	10	 Speech Coding, Synthesis, and Compression.......................................	   57
	10.1	 Speech Synthesis............................................................................	
57
	10.2	 Speech Coding and Compression..................................................	
58
	
10.3	 Real-Time Implementation of DWT-Based  
Speech Compression......................................................................	
58
References	...............................................................................................	
  59
	11	 Speech Quality Assessment....................................................................	   61
	11.1	 Wavelet-Packet Analysis................................................................	
61
	11.2	 Discrete Wavelet Transform...........................................................	
63
References	...............................................................................................	
  64
	12	 Scalogram and Nonlinear Analysis of Speech......................................	   65
	12.1	 Wavelet-Based Nonlinear Features................................................	
65
	12.2	 Wavelet Scalogram Analysis..........................................................	
66
	12.3	 Nonlinear and Chaotic Components in Speech Signal..................	
67
References	...............................................................................................	
  69
Contents

xi
	13	 Steganography, Forensics, and Security of Speech Signal..................	   71
	13.1	 Secure Communication of Speech.................................................	
71
	13.2	 Watermarking of Speech................................................................	
73
	13.3	 Watermarking in Sparse Representation........................................	
73
	13.4	 Forensic Analysis of Speech..........................................................	
74
References	...............................................................................................	
  75
	14	 Clinical Diagnosis and Assessment of Speech Pathology....................	   77
References	...............................................................................................	
79
Index.................................................................................................................	
81
Contents

xiii
ANN	
Artificial neural network
ANFIS	
Adaptive network-based fuzzy inference system
ASR	
Automatic speech recognition
AWP	
Admissible wavelet packet
BP	
Backpropagation algorithm for neural network training
c.s.a	
Cross-sectional area
CDS	
Cosine distance scoring
CODEC	
Coding-decoding system
CWT	
Continuous wavelet transform
DCT	
Discrete cosine transform
DFT	
Discrete Fourier transform
DLSDWT	
Daubechies lifting scheme discrete wavelet transform
DSP	
Digital signal processor
DWFT	
Discrete wavelet-Fourier transform
DWT	
Discrete wavelet transform
ERB	
Equivalent rectangular bandwidth
EM	
Expectation maximization
EW	
Empirical wavelet
EWT	
Empirical wavelet transform
FD	
Fractal dimension
FPGA	
Field programmable gate array
FT	
Fourier transform
FWENN	
Formants, wavelet entropy, and neural networks
GMM	
Gaussian mixture model
HMM	
Hidden Markov model
ICA	
Independent component analysis
IDWT	
Inverse DWT
IIR	
Infinite impulse response filter
LDA	
Linear discriminant analysis
LLE	
Largest Lyapunov exponent
LMM	
Laplacian mixture model
Abbreviations

xiv
LPC	
Linear predictive coding
LPCC	
Linear prediction cepstral coefficients
LSB	
Least significant bit
MELP	
Mixed-excitation linear prediction
MFCC	
Mel-frequency cepstral coefficients
MOS	
Mean-opinion score
MRA	
Multiresolution analysis
MRAM	
Multiresolution auditory model
MRTSC	
Multiresolution sinusoidal transform coding
MTS	
Multitaper spectrum
NLD	
Nonlinear dynamics
NN	
Neural network
PWF	
Perceptual wavelet filterbank
PWP	
Predefined wavelet packet
PWPD	
Perceptual wavelet packet decomposition
RBFNN	
Radial basis function neural network
RP	
Recurrence plot
RPS	
Reconstructed phase space
SNR	
Signal-to-noise ratio
SOM	
Self-organizing map
SS	
Spread spectrum
SSNR	
Segmental SNR
STFT	
Short-time Fourier transform
SURE	
Stein’s unbiased risk estimate
SVM	
Support vector machine
UMB	
Universal background model
UWPD	
Undecimated wavelet packet decomposition
UWPT	
Undecimated wavelet packet transform
UWT	
Undecimated wavelet transform
VAD	
Voice activity detection
V/U	
Voiced/unvoiced
WBCF	
Wavelet-bark coherence function
WCC	
Wavelet cepstral coefficient
WNN	
Neural network with wavelet-based activation function
WOCOR	
Wavelet octave coefficients of residues of linear prediction
WP	
Wavelet packet
WPD	
Wp decomposition
WPT	
Wp transform
WT	
Wavelet transform
WBCF	
Wavelet-based bark coherence function
ZCR	
Zero-crossing rate
Abbreviations

1
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_1
Chapter 1
Introduction
In this monograph, we discuss many proposed algorithms which employ wavelet 
transform (WT) for different applications in speech technology. A survey will be 
conducted through recent works which use WT in speech processing realm. This 
survey will cover both the use of wavelets in enhancing both previously proposed 
algorithms and new algorithms based, principally, on wavelet analysis. In general, 
wavelet analysis can serve through many ways in speech processing since it can 
provide new enhanced spectral analysis approach, basis expansion for signals, and 
identification features and can help in noise cancelation.
1.1  History and Definition of Speech Processing
First trials for speech processing through machines may be dated since ancient 
Egyptians who built statutes producing sounds. During the eighteenth century, there 
are documents on attempts for building speaking machines [1] .
In the human speech processing system, several transformations may be included, 
such as thought to articulation, articulator’s movement to acoustical signal, propa-
gation of the speech signal, electronic transmission/storage, loudspeaker to the lis-
tener’s ears, acoustic to electrical in the inner ear, and interpretation by the listener’s 
brain. These transformations are modeled through many mathematical algorithms. 
In most of the speech processing algorithms, a feature space is built based on a 
transformation kernel to a space of lower dimension, which allows a post-­processing 
stage, readily, resulting in more useful information.
Accordingly, speech processing is discussing the methods and algorithms used in 
analyzing and manipulating speech signals. Since signals are usually processed in a 
digital representation, speech processing can be regarded as a special case of digital 
signal processing, applied to speech signal. The main topics of speech processing 
are recognition, coding, synthesis, and enhancement. The processing for speech 

2
recognition concentrates on extracting the best features which can achieve the high-
est recognition rate using a certain classifier. For speech coding and synthesis, the 
coding parameters from speech signals should result in a low-dimensional set which 
gives the closest matching between the original and the reconstructed signals. For 
speech enhancement, efforts are directed toward discovering analysis components 
which may comprise sources of signal degradation.
1.2  Applications of Speech Processing
Applications of speech processing techniques may include compression and 
enhancement of human speech, clinical diagnosis of speech disorders, man-machine 
interfacing through voice, security systems for speech communications, machine 
translation of speech, reading machines, and understanding based on voice com-
munication. In these applications, the speech signal is, customarily, transformed 
from time domain to another domain in which efficient features can be extracted to 
express a functional character of such signal. As an example, spectral features can 
be exploited in identifying the meaning or type of a speech utterance in the field of 
speech and speaker recognition. For other applications, the features can be used for 
reconstructing the signal again after analysis and enhancement.
Nowadays, many real-world applications are available based on research results 
in speech processing. They may range from the use of speech-based dictation 
machines to speech-based command and control supporting interactions with 
machines. Speech recognition can serve as an alternative interface to traditional 
ones or as a complement modality speeding up the analysis process and increasing 
its fidelity and convenience. Many applications for critical society services are con-
tinuously being improved and made easily accessible through a speech-based inter-
face. Cellular phones and multimedia systems, also, employ speech coding 
algorithms. Moreover, diagnosis of speech disorder can benefit from results obtained 
by research in speech processing. Speech processing also has applications in secu-
rity fields [2] .
1.3  Recent Progress in Speech Processing
Most applications of speech processing have emerged many years ago. However, 
recent years have seen widespread deployment of smartphones and other portable 
devices with the ability to make good quality recordings of speech and even video. 
Such recordings can be processed locally or transmitted for processing on remote 
stations having more computational power and storage. More computational power 
and storage increase rapidly every day as computational technology advances.
1  Introduction

3
The current state of speech processing systems is still far from human perfor-
mance. A major problem for most speech-based applications is robustness, which 
refers to the fact that they may be insufficiently general. As an example, a truly 
robust automatic speech recognition (ASR) system should be independent from any 
speaker, in reasonable environments. Environmental noise from natural sources or 
machines, as well as communication channel distortions, all tend to degrade the 
system’s performance, often severely. Human listeners, by contrast, often can adapt 
rapidly to these difficulties, which suggests that there remains significant enhance-
ment needed. However, much of what we know about human speech production and 
perception needs to be integrated into research efforts in the near future.
As a result, the main objective of research in speech processing is directed toward 
finding techniques for robust speech processing. This concern has been motivated 
by the increase in the need of lower-complexity and more efficient methods for 
speech feature extraction, which are needed for enhancing the naturalness, accept-
ability, and intelligibility of the reconstructed speech signal corrupted by environ-
mental noise, and the necessity of reducing noise for robust speech recognition 
systems to achieve high recognition rate in harsh environments [3]. New algorithms 
are continuously developed for enhancing the performance of speech processing for 
different applications. Most of the improvements are founded on the growth of 
research infrastructure in speech area and the inspiration from related biological 
systems. The technology of powerful computation and communication systems 
admit more sophisticated and efficient algorithms to be employed for more reliable 
and robust applications of speech processing. In addition to such systems, larger 
speech corpora become available, and in general, the infrastructure of research in 
speech area is growing continuously.
1.4  Wavelet Analysis as an Efficient Tool for Speech 
Processing
As the use of wavelets started in the field of digital signal processing since the 
1990s, it finds wide applications in speech processing. Wavelet analysis has contin-
ued to serve many speech-based applications since that time till now. Unlimited 
algorithms and hardware implementations have been developed employing wavelet 
analysis as an efficient spectral analysis tool compensating limitations of Fourier-­
based algorithms [4]. Eventually, different merits of WT can support efficient fea-
ture extraction in approximately most of the research concerns especially with 
newer versions of speech corpora emerging continuously. Moreover, WT represents 
an economic analysis tool from a processing time perspective since it can be 
obtained in O(L), whereas the short-time Fourier transform (STFT) representation 
requires O(L log M), where L is the length of the discretized speech signal and 
M denotes the subframe length of the used window [5].
1.4  Wavelet Analysis as an Efficient Tool for Speech Processing

4
References
	1.	 J. Benesty, M. Sondhi, Y. Huang, Springer Handbook of Speech Processing (Springer-Verlag 
New York, Inc., Secaucus, 2007)
	2.	 R. Hu, S. Zhu, J. Feng, A. Sears, Use of Speech Technology in Real Life Environment, Lecture 
Notes in Computer Science, vol 6768 (Springer, New York, 2011), pp. 62–71
	3.	 V.T. Pham, Wavelet analysis for robust speech processing and applications, Ph.D. dissertation, 
Ph. D. thesis, (VDM Verlag, Saarbrücken, Germany, 2007), http://theses.eurasip.org/media/
theses/documents/pham-van-tuan-wavelet-analysis-for-robust-speech-processing-and-applica-
tions.pdf
	4.	 J.I. Agbinya, Discrete wavelet transform techniques in speech processing. TENCON '96. Proc. 
1996 IEEE TENCON. Digit. Signal Process. Appl. Perth, WA 2, 514–519 (1996)
	5.	 B. Wieland, Speech signal noise reduction with wavelets. Diplomarbeit an der Universität Ulm , 
October 2009
1  Introduction

5
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_2
Chapter 2
Speech Production and Perception
Speech sounds are produced due to the movement of organs constituting the vocal 
tract (glottis, velum, tongue, lips) acting on the air from the respiratory passages 
(trachea, larynx, pharynx, mouth, nose). The vocal organs generate a local distur-
bance on the air at several positions in the vocal tract creating the sources for speech 
production. The acoustic waves generated by such sources are then modulated dur-
ing the propagation through the vocal tract with a specific shape. Accordingly, 
speech sounds are generated by the combined effect of sound sources and vocal 
tract characteristics. The source-filter model of speech production assumes that the 
spectrum of source excitation at the glottis is shaped according to filtering proper-
ties of the vocal tract. Such filtering properties change continuously with time. 
Continuous changes in the shape of the vocal tract and excitation either through 
glottis or tract constriction make the produced sounds at the lips nonstationary. 
Wavelet analysis is one of the best methods for extracting spectral features from 
nonstationary signals, since it employs multiresolution measures both in time and 
frequency.
2.1  Speech Production Process
The speech production process takes place inside the vocal tract extending from the 
glottis to the lips. The process is energized from air-filled lungs. The vocal tract is a 
chamber of extremely complicated geometrical shape whose dimensions and con-
figuration may vary continuously with time and whose walls are composed of tis-
sues having widely ranging properties.
Figure 2.1 shows the anatomical structure of the vocal tract. The glottis is a slit-­
like orifice between the vocal cords (at the top of the trachea). The cartilages around 
the cords support them and facilitate adjustment of their tension. The flexible 

6
­structure of the vocal cords makes them oscillate easily. These oscillations are 
­responsible for periodic excitation of vowels. The excitation of other sounds may 
be through a jet of air through a constriction within the vocal tract or a combination 
of the periodic excitation and what is produced by that jet of air. The nasal tract 
constitutes an ancillary path for sound transmission. It begins at the velum and 
terminates at the nostrils.
2.2  Classification of Speech Sounds
Speech sounds are classified according to the type and place of excitation. Voiced 
sounds and vowels are characterized by a periodic excitation at the glottis. For 
voiced sounds and vowels, the expelled air from the lungs causes the vocal cords to 
vibrate as a relaxation oscillator, and the airstream is modulated into discrete puffs. 
These oscillations start when the subglottal pressure is increased sufficiently to 
Nasal 
Cavity
Lips
Tongue
Nasal 
pharynx
Soft 
palate
Epiglottis
Larynx
Vocal folds
Oral 
pharynx
Pharynx
False Vocal 
folds
Laryngeal 
ventricle
Thyroid 
cartilage 
Trachea
Esophagus
Velum
Fig. 2.1  Anatomical structure of the vocal tract
2  Speech Production and Perception

7
force the initially abducted cords apart with lateral acceleration. As the air flow 
builds up in the orifice, the local pressure is reduced and a force acts to return the 
cords to a proximate position. Consequently, the pressure approaches the subglottal 
value as the flow decreases with the decrease in the orifice (glottal area). The relax-
ation cycle is then repeated. The mass and compliance of the cords and the subglot-
tal pressure determine the oscillation frequency (pitch) [1].
Unvoiced sounds are generated by passing the airstream through a constriction in 
the tract. The pressure perturbations due to these excitation mechanisms provide an 
acoustic wave which propagates along the vocal tract toward the lips. The source of 
voiced sounds is a combined effect of both types of excitation.
If the nasal tract is coupled to the vocal cavity through the velum, the radiated 
sound is the resultant of the radiation at both the lips and the nostrils and it is called 
nasalized sounds (as in /m/ and /n/). The distinctive sounds of any language (pho-
nemes) are uniquely determined by describing the excitation source and the vocal 
tract configuration.
The variation of the cross-sectional area (c.s.a.) along the vocal tract is called the 
area function according to the articulators positions. The area function of the vowels 
is determined primarily by the position of the tongue, but the positions of the jaw, 
lips, and, to a small extent, the velum also influence the resulting sound. The area 
function with the excitation type can uniquely define the produced sound.
2.3  Speech Production Modeling
As discussed in the previous section, the process of speech production can be 
divided into three stages: sound source generation, articulation by vocal tract, and 
radiation from the lips and/or nostrils. The resulted speech signal s(t), in time 
domain, can be written as a convolution of excitation e(t) and vocal tract impulse 
response h(t):
	
s t
e t
h t
( ) = ( )
( )
∗
	
(2.1)
Specifically, sound source e(t) is either voiced or unvoiced. A voiced source can 
be modeled, in the simplest case, by a generator of periodic pulses or asymmetrical 
triangular waves which are repeated at every fundamental period (pitch). The peak 
value of the source wave corresponds to the loudness of the voiced sound. On the 
other hand, an unvoiced sound source can be modeled by a white noise generator, 
while the mean energy corresponds to the loudness [2].
For many speech applications such as recognition, coding, and synthesis, good 
performance can be achieved with a speech model that reflects broad characteristics 
of timing and articulatory patterns as well as varying frequency properties [3] and 
[4]. In such a model, a scheme is designed to perform some spectral shaping on a 
2.3  Speech Production Modeling

8
certain excitation wave so that it matches the natural spectrum (i.e., the vocal tract 
tube looks as a spectral shaper of the excitation). This approach is called “terminal 
analog” since its output is analogous to the natural process at the terminals only. The 
main interest in such an approach is centered on resonance frequencies (formants or 
system poles) and their bandwidths. The two widely used methods of this approach 
are the formant model [5] and the linear prediction (LP) model [6]. These models 
provide simpler implementation schemes for both hardware and software. Many 
commercial products now adopt such models in their operation [7]. The adoption of 
terminal analog models affords sufficient intelligibility for many applications along 
with fast response due to its simplicity and amenability to implementation through 
many available media. Apparently, the extracted features using such models can be 
considered as different forms of resonances or spectral content of a speech signal. 
As wavelets are considered one of the efficient methods for representing the spec-
trum of speech signals, WT can efficiently help in implementing such models [8].
2.4  Speech Perception Modeling
The ear is the main organ in the process of speech perception. It consists of an outer 
part, a middle part, and an inner part. Figure 2.2 shows the structure of the human 
auditory system. The main function of the outer ear is to catch sound waves which 
is done by the pinna. The pinna is pointed forward and has a number of curves to be 
able to catch the sound and determine its direction. After the sound reaches the 
pinna, it is guided to the middle ear using the external auditory canal until it reaches 
Pinna
Middle Ear 
Inner Ear 
Cochlea 
Auditory Nerve
Eardrum
Fig. 2.2  Human auditory system
2  Speech Production and Perception

9
the ear drum. The main function of the middle ear is to magnify the sound pressure 
because the inner ear transfers sound through fluid not air as in the middle and outer 
ears. Thereafter, the inner ear starts with the cochlea, the most important organ in 
the human ear. The cochlea performs the spectral analysis of the speech signal 
through splitting it into several frequency bands which are called critical bands [9]. 
The ear averages the energies of the frequencies within each critical band and thus 
forms a compressed representation of the original stimulus.
Studies have shown that human perception of the frequency content of sounds, 
either for pure tones or for speech signals, does not follow a linear scale. The major-
ity of the speech and speaker recognition systems have used the feature vectors 
derived from a filter bank that has been designed according to the model of auditory 
system. There are a number of forms used for these filters, but all of them are based 
on a frequency scale that is approximately linear below 1 kHz and approximately 
logarithmic above this point. Wavelet multiresolution analysis can provide accurate 
localization in both time and frequency domains which can emulate the operation of 
the human auditory system [8].
2.5  Intelligibility and Speech Quality Measures
The terms intelligibility and quality of speech are used interchangeably. The degra-
dation of speech quality is, mainly, a result of background noise either through a 
communication channel or environment [3]. The evaluation of speech quality is 
highly important in many speech applications. Subjective listening or conversation 
tests are the most reliable measure of speech quality; however, these tests are often 
fairly expensive, time consuming, labor-intensive, and difficult to reproduce. 
However, for some applications, like the assessment of alternative coding or 
enhancement algorithms, an objective measure is more economic to give the 
designer an immediate and reliable estimate of the anticipated perceptual quality of 
a particular algorithm. Traditional objective quality measures which rely on wave-
form matching like signal-to-noise ratio (SNR) or its variants like segmental SNR 
(SSNR) are examples of straightforward measures. Perceptual quality measures are 
better candidate for fast assessment with more accurate results. The motivation for 
this perception-based approach is to create estimators which resemble that of the 
human hearing system as described by the psychoacoustic models. In a psycho-
acoustic model of the human hearing, the whole spectrum bandwidth of the speech 
signal is divided into the critical bands of hearing. WT can contribute to a quality 
evaluation of speech in the context of critical band decomposition and auditory 
masking [10], [11], [12], and [13]. Moreover, wavelet analysis can reduce the com-
putational effort associated with the mapping of speech signals into an auditory 
scale [10].
2.5  Intelligibility and Speech Quality Measures

10
References
	 1.	J. Flanagan, Speech Analysis; Synthesis and Perception (Springer, New York, 1972)
	 2.	M.  Hesham, Vocal tract modeling. Ph. D.  Dissertation, Faculty of Engineering, Cairo 
University, 1994
	 3.	J. Deller, J. Proakis, J. Hansen, Discrete-Time Processing of Speech Signals (IEEE PRESS, 
New York, 2000)
	 4.	M. Hesham, M.H. Kamel, A unified mathematical analysis for acoustic wave propagation 
inside the vocal tract. J. Eng. Appl. Sci. 48(6), 1099–1114 (2001)
	 5.	D. Klatt, Review of text-to-speech conversion for English. J. Acoust. Soc. Am 82(3), 737–793 
(1987)
	 6.	J.D. Markel, A.H.J. Gray, Linear Prediction of Speech (Springer, New York, 1976), ch.4
	 7.	D. O’Shaughnessy, Invited paper: automatic speech recognition: history, methods and chal-
lenges. Pattern Recogn. 41(10), 2965–2979 (2008)
	 8.	S. Ayat, A new method for threshold selection in speech enhancement by wavelet thresholding, 
in International Conference on Computer Communication and Management (ICCCM 2011), 
(Sydney, May 2011)
	 9.	J. Benesty, M. Sondhi, Y. Huang, Springer Handbook of Speech Processing (Springer-Verlag 
New York, Inc., Secaucus, 2007)
	10.	M. Hesham, A predefined wavelet packet for speech quality assessment. J. Eng. Appl. Sci. 
53(5), 637–652 (2006)
	11.	A. Karmakar, A. Kumar, R.K. Patney, A multiresolution model of auditory excitation pattern 
and its application to objective evaluation of perceived speech quality. IEEE Trans. Audio 
Speech Lang. Process. 14(6), 1912–1923 (2006)
	12.	L. Rabiner, R. Schafer, Theory and Applications of Digital Speech Processing (Pearson, New 
Jersey, 2011)
	13.	W. Dobson, J. Yang, K. Smart, F. Guo, High quality low complexity scalable wavelet audio 
coding, in Proceedings of IEEE International Conference Acoustics, Speech, and Signal 
Processing (ICASSP’97), Apr 1997, pp. 327–330
2  Speech Production and Perception

11
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_3
Chapter 3
Wavelets, Wavelet Filters, and Wavelet 
Transforms
Multiresolution analysis based on the wavelet theory permits the introduction of the 
concepts of signal filtering with different bandwidths or frequency resolutions. The 
WT provides a framework to decompose a signal into a number of new signals, each 
one with a different degree of resolution. While the Fourier transform (FT) gives an 
idea on the frequency content in a signal, the wavelet representation is an intermedi-
ate representation between the frequency and the time representations, and it can 
provide good localization in both frequency and time domains. Fast variation in 
both domains can be detected by inspecting the coefficients of the WT. Because 
of  the difficult nature of speech signals and their fast variation with time, the  
WT is used. In this part, we will review the properties of different approaches for 
obtaining a WT.
3.1  Short-Time Fourier Transform (STFT)
In general, any mathematical transform for a signal or a function of time s(t) takes 
the form:
	
S
K
dt
α
α
( ) =
( )
(
)
−∞
∞
∫s t
,t
	
(3.1)
where S(α) is the transform of s(t) with respect to the kernel K(α, t) and α is the 
transform variable. In the Fourier transform, the kernel is K(ω, t) = e-jω t where ω is 
the angular frequency and is equal to 2π f while f is the frequency.
FT is the main tool for spectral analysis of different signals. The STFT cuts out 
a signal in short-time intervals (frames) and performs the FT in order to capture 

12
time-dependent fluctuation of the frequency components in a nonstationary signal. 
The STFT can be expressed as:
	
S
w
dt
ω β
β
ω
,
t
e j t
(
) =
( )
(
)
−∞
∞
∫s t
−
−
	
(3.2)
where s(t) is a signal, S(ω) is its STFT, and w(t-β) is a window function centered 
around β in time. The window function is then shifted in time, and the Fourier trans-
form (FT) of the product is, thereafter, computed again. So, for a fixed shift β of the 
window w(t), the window captures the features of the signal s(t) around different 
locations defined by β. The window helps to localize the time domain data within a 
limited period in time, before obtaining the frequency domain information. The 
signal has been assumed to be quasi-stationary during the period of w(t). The STFT 
can be viewed as a convolution of the signal s(t) with a filter having an impulse 
response of the form h(t) = w(t ‐ β) e‐jωt. The STFT can be also interpreted as a bank 
of narrow, slightly overlapping band pass filters with additional phase information 
for each one. Alternatively, it can be seen as a special case of a family of transforms 
that use basis functions.
In order to improve the accuracy with respect to time-dependent variation for 
STFT, it is necessary to shorten the frame period. The frequency resolution becomes 
worse with decreasing frame length. In other words, the requirements in the time 
localization and frequency resolution are conflicting. So, the major drawback of the 
STFT is that it uses a fixed window width. Alternatively, the WT provides a better 
time-frequency representation of the signal than any other existing transform. The 
WT solves the above problem to a certain extent. In contrast to STFT, which uses a 
single analysis window, the WT uses short windows at high frequencies and long 
windows at low frequencies.
3.2  Multiresolution Analysis and Wavelet Transform
In FT, a fixed window is used uniformly on a spread of frequencies. On the contrary, 
WT uses short windows at high frequencies and long windows at low frequencies. 
In this way, the characteristics of nonstationary speech signals can be more closely 
examined. Accordingly, WT coefficients are localized in both time and frequency 
domains. This localization is constrained with Heisenberg’s uncertainty principle 
which affirms that no transform can provide high resolution in both time and fre-
quency domains at the same time. The useful locality property is exploited in this 
context. Because the wavelet basis functions are generated by scaling from a mother 
wavelet, they are well localized in time and scale domains. This behavior of wavelet 
decomposition is suitable for the processing of speech signals which requires 
high-­frequency resolution to analyze low-frequency components (voiced sounds, 
3  Wavelets, Wavelet Filters, and Wavelet Transforms

13
formant frequencies) and high temporal resolution to analyze high-frequency com-
ponents (mostly unvoiced sounds).
As a mathematical transform, the WT takes a kernel based on a function ψ(t) as 
follows:
	
S a b
s t
a
t
b
a
dt
,
(
) =
( )
−




−∞
∞
∫
1 ψ
	
(3.3)
where a is a scale parameter and b is another parameter for translation.
As in Eq. (3.3), the wavelet analysis is done similar to the STFT analysis except 
that the kernel function is not sinusoidal. The wavelet function ψ(t) is a member in 
a wavelet family. A wavelet family is generated from what is called the mother 
wavelet. All wavelets of a family share the same properties and their collection con-
stitutes a complete basis.
The semi-discrete WT of the function s(t), s ∈ L2(ℜ) is defined as follows [1]:
Analysis equation:
	
S j k
s t
t
kT
dt
j
j
s
,
(
) =
( )
−
(
)
−∞
∞
−
−
∫
2
2
2
/
.
ψ
	
(3.4)
Synthesis or inverse transform equation:
	
s t
S j k
t
kT
j
k
j
j
s
( ) =
(
)
∗
−
(
)
=−∞
∞
=−∞
∞
−
−
∑∑
,
2
2
2
/
.
ψ
	
(3.5)
where j and k are indices indicating scale and location of a particular wavelet while 
Ts is the sampling time of s(t). Without loss of generality, Ts = 1 can be considered 
in a discrete case while ψ* is the complex conjugate of ψ.
The wavelet theory would immediately allow us to obtain line frequency analysis 
and synthesis with the possibility of capturing both long-lasting (low-frequency) 
components and localizing short irregularities, spikes, and other singularities with 
high-frequency content. The former objectives can be approached by wavelets at 
low scales, while the latter are successfully performed by wavelets at high scales 
and appropriate locations. Wavelet localization follows Heisenberg uncertainty 
principle in both the time and frequency domains that for any given wavelet Δt 
Δf ≥1/2π.
As in (3.3), the pure wavelet expansion requests an infinite number of scales or 
resolutions k to represent the signal s(t) completely. This is impractical if the expan-
sion is known, only for certain scales k < M, we need a complement component to 
present information of expansion for k > M. This is done by introducing a scaling 
function φ(t) such that [2]:
	
ϕ
ϕ
j k
j
j
t
t
k
,
/
( ) =
−
(
)
−
−
2
2
2
	
(3.6)
3.2  Multiresolution Analysis and Wavelet Transform

14
where the set φj,k(t) is an orthonormal basis for subspace of L2(ℜ). With the intro-
duced component, the signal s(t) can be represented as a limit of successive approxi-
mations corresponding to different resolutions. This formulation is called a 
multiresolution analysis (MRA).
Consequently, the signal s(t) can be set as the sum of approximations plus M 
details at the Mth decomposed resolution or level. Equation (3.5) can be rewritten 
after including approximations as follows:
	
s t
a
t
d
t
k
M k
M k
j
M
k
j k
j k
( ) =
( ) +
( )
∑
∑∑
=
,
,
,
,
φ
ψ
1
	
(3.7)
where M represents the number of scales. aM,k are the approximations or scaling 
coefficients and dj,k are the details or wavelet coefficients.
As a result, WT can be obtained in O(L) whereas the STFT representation 
requires O(L log2M) where L is the length of discretized signal and M denotes the 
subframe length of the used window [3].
3.3  Wavelets and Bank of Filters
The WT can be viewed as a convolution of the signal and a wavelet function. In 
discrete-time domain, the set of discrete-time scaling and wavelet functions can 
be constructed from filters bank. The signal can be split into frequency bands 
through such bank of filters. As an example, a two-channel filter bank is shown 
in Fig. 3.1. The filters are therefore a low-pass L(z) and a high-pass H(z) filter. 
The outputs of the analysis bank are called subbands. This technique is also 
called subband coding [4].
h(n)
l(n)
h’ (n)
l’ (n)
s (n)
2
2
2
2
s~(n)
d1,1
a1,1
Analysis bank
Synthesis/Reconstruction bank
Fig. 3.1  Analysis and reconstruction of a signal using DWT through two-channel filter bank
3  Wavelets, Wavelet Filters, and Wavelet Transforms

15
The original signal can be reconstructed using this bank of filters. In the synthe-
sis phase, the signals are upsampled and passed through the synthesis filters. The 
output of the filters in the synthesis bank is summed to get the reconstructed 
signal.
3.4  Wavelet Families
A wavelet function must be oscillatory in some way to capture a frequency band 
from the analyzed signal. The wavelet function comprises both the analyzing func-
tion and a short-time window. A wavelet family is generated from a mother function 
ψjk which can be defined as follows:
	
ψ
ψ
jk
j
j
t
t
k
( ) =
−
(
)
2
2
2
/
, 	
(3.8)
where j and k are indices indicating scale and location of a particular wavelet.
Accordingly, the wavelet family is a collection of wavelet functions ψjk(t) which 
are translated along the time axis t and then dilated by 2j times, and the new dilated 
wavelet is translated along the time axis again. The wavelets of a family share the 
same properties and their collection constitutes a complete basis. The basic wavelet 
function must have local (or almost local) support in both a real dimension (time in 
case of speech signals) and a frequency domain. Several kinds of wavelet functions 
were developed and all of them have specific properties [4] as follows:
	1.	 A wavelet function has finite energy [5]:
	
−∞
∞
∫
( )
< ∞
ψ t
dt
2
	
(3.9)
	2.	 Similar condition must hold for Ψ(f)/f if Ψ(f) is the Fourier transform of the 
wavelet function and it has zero meanΨ(0)=0. This condition can be formulated 
as follows:
	
0
2
∞
∫
( )
< ∞
ψ
f
f
df
	
(3.10)
Another important property is that the wavelet function is compactly supported. 
The speed of convergence to zero, as the time t or the frequency goes to infinity, 
quantifies both time and frequency localizations. The symmetry is useful in avoid-
ing dephasing. The number of vanishing moments for a wavelet function is useful 
for regularity which serves the compression purposes. The regularity can achieve 
smoothness of the reconstructed signal. Additional properties, namely, the existence 
of a scaling function and orthogonality or biorthogonality, allow to get faster algo-
rithm and space-saving coding.
3.4  Wavelet Families

16
There are a number of basis functions that can be used as the mother wavelet for 
wavelet transformation. Since the mother wavelet produces all wavelet functions 
used in the transformation through translation and scaling, it determines the charac-
teristics of the resulting transform. Therefore, the appropriate mother wavelet 
should be chosen in order to use the wavelet analysis effectively for a specific 
application.
Figure 3.2 shows an example of the simplest wavelet, Haar. Haar wavelet is one 
of the oldest and simplest types. The Haar scaling function acts as a low-pass filter 
through an averaging effect on the signal while its wavelet counterpart acts as a 
high-pass filter.
Daubechies wavelets are the most popular. They represent the foundations of 
wavelet signal processing and are used in numerous applications. The Haar, 
Daubechies, Symlets, and Coiflets are compactly supported orthogonal wavelets. 
These wavelets along with Meyer wavelets can provide a perfect reconstruction of 
a signal. The Meyer, Morlet, and Mexican Hat wavelets are symmetric in shape [5]. 
The discrete form of a scale function is the impulse response of a low-pass filter, 
while the wavelet is the impulse response of a high-pass filter.
3.5  Wavelet Packets
Wavelet packet basis consists of a set of multiscale functions derived from the shift 
and dilation of a basic wavelet function as in (3.8). The wavelet packet (WP) basis 
space is generated from the decomposition of both the low-pass filter function space 
and the corresponding basic high-pass filter function space. The conventional wave-
let basis space can be considered as a special case of the WP space when the decom-
position takes place only in the low-pass filter function space [6]. Assuming that the 
discrete form of a scale function is l(n) and the wavelet one is h(n), WP basis can be 
expressed as:
t
f (t)
1
1
(a)
t
y (t)
1
1
-1
(b)
Fig. 3.2  Haar Scaling (a) and wavelet (b) functions
3  Wavelets, Wavelet Filters, and Wavelet Transforms

17
	
ψ
ψ
01
00
2
n
k
l
n
k
( ) =
( )
−
(
)
Σ
k
and
	
	
ψ
ψ
11
00
2
n
k
h
n
k
( ) =
( )
−
(
)
Σ
k
	
(3.11)
where ψ00(k) is the wavelet basis function with the finest time resolution. The func-
tions in the next scale become coarser in time resolution and finer in spectral resolu-
tion through filtering and down-sampling in the level 11, i.e. ψ11. The same 
procedure can be applied recursively to the outputs of level 11 into subsequent 
scales. In other words, a complete and orthogonal WP basis can be generated from 
a frequency decomposition tree which starts by using recursive two-channel filter-
ing and down-sampling from an initial function with the finest time resolution. It 
can be shown that the decomposed functions at the outermost branches of the tree 
satisfy orthogonality and completeness for any decomposition tree and, thus, con-
stitute a WP basis set [7].
Figure 3.3 (a) and (b) show, respectively, an example of a conventional wavelet 
decomposition tree and a predefined wavelet packet (PWP) decomposition tree. The 
former always zooms in along the low-frequency branch while the latter zooms in 
along a preselected frequency.
The WP analysis provides much better frequency resolution than WT. Subbands 
with finer bandwidths across the whole spectrum can be attained using WP analysis 
as shown in Fig. 3.4.
First, the tree structure of WP decomposition can be chosen in a way to closely 
mimic the critical bands in a psychoacoustic model [8]. Several WP audio algo-
rithms have successfully employed time-invariant WP tree structures that mimic the 
frequency resolution properties of the ear’s critical bands for perceptual quality 
assessment of speech [9] and [10] .
In order to achieve a critical band resolution using fast Fourier transform (FFT), 
it requires (N log2N) processes, and the whole process becomes computational 
intensive as N becomes larger where N is the number of samples per frame. WP can 
directly calculate the signal energy in the wavelet domain, and in turn, the complex-
ity is greatly reduced [8].
(a)
(b)
s(n)
Details of level 1
Details of level 2
Level 3
Level 4
Level 4
Level 1
Level 2
Level 3.1
Level 4
Level 4
Level 3.0
s(n)
Fig. 3.3  A comparison between (a) conventional and (b) WP trees
3.5  Wavelet Packets

18
3.6  Undecimated Wavelet Transform
Undecimated wavelet transform (UWT) involves the elimination of the down-­
sampling involved in Fig. 3.1 and Fig. 3.4 to render the transform a translational 
invariance property. This translation invariant version of the DWT is known by 
other names such as stationary WT, redundant WT, algorithm à trous, quasi-­
continuous WT, shift invariant wavelet transform, cycle spinning, and maximal 
overlap WT. Although undecimated wavelet packet transform (UWPT) employs a 
higher-density filter bank without decimation, this structure has a better time-­
frequency localization and accordingly becomes less shift sensitive [11] than those 
of critically sampled wavelet packet decomposition (WPD). While the nondecima-
tion gives more freedom in designing filter banks, iterative undecimated wavelet 
analysis can improve the multiscale denoising as confirmed in [12] .
3.7  The Continuous Wavelet Transform (CWT)
The continuous wavelet transform (CWT) achieves the MRA of WT by having a 
continuously varying window width. The variation of window width is controlled 
through the scale parameter. Continuous version of mother wavelets can be 
employed as a transform basis in analogous with that used in DWT as long as they 
satisfy the predefined mathematical criteria [7] and [13]. The WT of a continuous 
time signal, s(t), is defined as:
s(n)
h(n)
h(n)
h(n)
h(n)
h(n)
h(n)
h(n)
l(n)
l(n)
l(n)
l(n)
l(n)
l(n)
l(n)
Fig. 3.4  Wavelet packet decomposition over three levels. l[n] are the low-pass approximation 
coefficients; h[n] are the high-pass detail coefficients
3  Wavelets, Wavelet Filters, and Wavelet Transforms

19
	
S a b
a
s t
w
t
b
a
dt
a b
,
(
) =
( )⋅
−




−∞
∞
∗
∫
1
,
	
(3.12)
where wa,b
*(t) is the complex conjugate of the wavelet basis function w(t), a is the 
scale parameter, and b is the translation parameter of the wavelet. The numerical 
implementation of CWT can be viewed as a convolution of the input data sequence 
with a set of functions generated by the mother wavelet. The output S(a,b) is a real 
valued function when the mother wavelet is real.
Since the majority of real-world signals are available as discrete-time samples, 
the analytical form of S(a,b) is typically inaccessible. Moreover, a closed-form solu-
tion of the integration in (3.12) does not exist except for very special cases. 
Therefore, a large set of discrete values are assumed for the scales a and shifts b and 
the CWT can be, thereafter, computed numerically.
3.8  Wavelet Scalogram
A scalogram is a time-scale distribution of WT on a signal to approximate its spec-
tral properties. It provides a three-dimensional representation of the signal spec-
trum, with time represented on the abscissa and frequency on the ordinate axes. 
Gray scales are used to represent the spectral amplitude on the third dimension. The 
wavelet scalogram of a speech signal is obtained using CWT analysis in the form 
representing the difference between the speech and the background noise in terms 
of dissimilar perceptions on human beings.
3.9  Empirical Wavelets
The available wavelet functions have structures which limit the adaptability to spe-
cific signals. An appropriate wavelet filter bank has been designed such that it adapts 
to the processed signal and called empirical wavelet transform (EWT) [14]. So, 
EWT did not use any prescribed function basis but it is self-adapting to the analyzed 
signal. Eventually, EWT has shown its advantages to many applications in the 
speech processing.
The adaptability is achieved through assuming that the spectrum of an analyzed 
signal has modes or maxima of compact support and centered around a certain fre-
quency [15] and [16]. In analogy to Eq. 3.6, the scaling function in EW may be 
defined in frequency domain as follows:
	
Φ j k
j
j
j
j
j
k
k
k
k
,
cos
ω
ω
ω
π β
ω
ω
ω
ω
( ) =
≤
−
−
+
(
)














−
≤
1
2
1
2
j
j
≤
+







ωj
k
otherwise
j
0

(3.13)
3.9  Empirical Wavelets

20
While the wavelet function definition might take the following form:
	
Ψ j k
j
j
j
j
j
k
k
k
k
,
cos
ω
ω
ω
ω
π β
ω
ω
ω
( ) =
+
≤
≤
−
−
+
(
)














1
2
1
2
j
j
j
j
j
j
−
≤
≤
+
−
+
(
)














−
≤
≤
k
k
k
k
k
j
j
j
j
j
j
ω
ω
π β
ω
ω
ω
ω
ω
sin 2
1
2
+











k
otherwise
j
0

(3.14)
where β(x) is an arbitrary function vanishes for x≤0 and equals 1 for x≥1 while 
β(x)+ β(1-x) = 1 ∀ x ∈[0,1]. Additionally, kj possibly be taken proportional to ωj [14].
References
	 1.	T.K.  Sarkar, C.  Su, R.  Adve, M.  Salazar-Palma, L.  Garcia Castillo, R.R.  Boix, A tutorial 
on wavelets from an electrical engineering perspective, part II; The continuous case. IEEE 
Antennas Propag. Mag. 40(6), 36–48 (1998)
	 2.	V.T. Pham, Wavelet analysis for robust speech processing and applications, Ph.D. dissertation, 
Ph. D. thesis, (VDM Verlag, Saarbrücken, Germany, 2007), http://theses.eurasip.org/media/
theses/documents/pham-van-tuan-wavelet-analysis-for-robust-speech-processing-and-appli-
cations.pdf
	 3.	B. Wieland, Speech signal noise reduction with wavelets. Diplomarbeit an der Universität Ulm 
, October 2009
	 4.	R.J.E. Merry, Wavelet Theory and Applications: A Literature Study (Technische. Universiteit 
Eindhoven, (Eindhoven), DCT, 2005)
	 5.	D. Sripathi, Efficient implementations of discrete wavelet transforms using FPGAs, M.Sc. the-
sis, Florida State University, 2003
	 6.	R.R. Coifman, Y. Meyer, M.V. Wickerhauser, Size Properties of Wavelet Packets, in Wavelets 
and their Applications (Jones Bartlett, Boston, 1992)
	 7.	M.V.  Wickerhauser, Adapted Wavelet Analysis from Theory to Software (IEEE Press, 
New York, 1994)
	 8.	W. Dobson, J. Yang, K. Smart, F. Guo, High quality low complexity scalable wavelet audio 
coding, in Proceedings of IEEE International Conference Acoustics, Speech, and Signal 
Processing (ICASSP’97), Apr 1997, pp. 327–330
	 9.	M. Hesham, M.H. Kamel, A unified mathematical analysis for acoustic wave propagation 
inside the vocal tract. J. Eng. Appl. Sci. 48(6), 1099–1114 (2001)
	10.	A. Karmakar, A. Kumar, R.K. Patney, A multiresolution model of auditory excitation pattern 
and its application to objective evaluation of perceived speech quality. IEEE Trans. Audio 
Speech Lang. Process. 14(6), 1912–1923 (2006)
	11.	H.R. Tohidypour, A. Banitalebi-Dehkordi, Speech frame recognition based on less shift sensi-
tive wavelet filter banks. SIViP 10(4), 633–637 (2016)
	12.	J.L. Starck, J. Fadili, F. Murtagh, The undecimated wavelet decomposition and its reconstruc-
tion. IEEE Trans. Image Process. 16(2), 297–309 (2007)
3  Wavelets, Wavelet Filters, and Wavelet Transforms

21
	13.	P.S.  Addison, Wavelet transforms and the ECG: a review. Physiol. Meas. 26, R155–R199 
(2005)
	14.	Y. Li, B. Xue, H. Hong, X. Zhu, Instantaneous pitch estimation based on empirical wavelet 
transform. in 2014 19th International Conference on Digital Signal Processing, Hong Kong, 
2014, pp. 250–253
	15.	J.  Gilles, Empirical wavelet transform. IEEE Trans. Signal Process. 61(16), 3999–4010 
(Aug.15, 2013)
	16.	A. Francis, C. Muruganantham, Article: an adaptive denoising method using empirical wavelet 
transform. Int. J. Comput. Appl. 117(21), 18–20 (2015)
References

23
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_4
Chapter 4
Spectral Analysis of Speech Signal  
and Pitch Estimation
Spectral analysis of speech takes many forms in the context of speech processing 
algorithms. Short-time Fourier transform (STFT), mel-frequency cepstral coeffi-
cients (MFCC), linear predictive code (LPC), and cepstral analysis are examples of 
such forms. MRA based on the wavelet theory permits the introduction of the con-
cepts of signal filtering with different bandwidths or frequency resolutions. Since 
that the speech signal is nonstationary, the WT provides a framework to obtain a 
more elegant spectral analysis through partitioning of sound in time according to its 
spectral properties [1].
4.1  Spectral Analysis
Spectral analysis using WT is visualized through a scalogram which is the equiva-
lent of a spectrogram in the time-frequency domain. WT can decompose the speech 
signal into a number of new signals, each one with different resolutions both in time 
and frequency. The tree structure of WP analysis can be customized to mimic the 
critical bands of human hearing giving better spectral estimation for speech signal 
than other methods [2], [3] and [4].
The DWT is used for spectral analysis and to create a segmentation profile for a 
speech signal in [1]. The efficiency of the segmentation results is tested against the 
hand-annotated speech corpus.
Fast variation of a speech signal in both time and frequency domains can be 
detected by inspecting the decomposed coefficients of WT and, accordingly, obtain-
ing more insight into signal spectrum. Moreover, abrupt changes of speech can be 
tracked by wavelet analysis. Therefore, WT has been used successfully for pitch 
estimation of a speech signal and for voiced/unvoiced (V/U) classification [5]. The 
V/U classification has been also achieved using EWT in [6]. Another work applied 

24
a data fusion method including wavelet features in [7] for both pitch estimation and 
speech segmentation.
4.2  Formant Tracking and Estimation
On another level, some sort of spectral analysis takes the form of tracking and esti-
mation of resonance frequencies in the vocal tract. Such resonances, which are 
called formants, may be detected as peaks of the frequency response of the vocal 
tract after the source excitation has been de-convolved from the speech signal. 
Formant tracks are used to identify vowels and other voiced sounds and in formant-­
based synthesizers. In some cases, the formants can be exploited as supplementary 
data to provide better speech recognition. Automatic formant tracking has wide 
range of applications, and it still has many open research areas [8].
The work in [9] tackles the problem of formant estimation through investigating 
the spectral envelope obtained using WT analysis. The results are similar to those 
obtained using traditional cepstral analysis.
In [8] the peaks are detected in the time-frequency domain through two 
approaches, namely, Fourier-based spectrogram and scalogram. The detected for-
mant peaks are then analyzed using continuity constraint as shown in Fig.  4.1. 
Formant trajectories labeled by hand are compared to the identified peaks. The 
Computation of formant alternatives
Formant track
(freq 
time)
Speech signal
Preprocessing
Time-Frequency analysis
Peak detection
Formant selection based on Continuity constraint
Track history
0.0
0.2
0.4
0.6
0.8
1.0
600
700
Fig. 4.1  Formant tracking based on time-frequency analysis [8]
4  Spectral Analysis of Speech Signal and Pitch Estimation

25
­comparison shows the adequacy of the Fourier-based approach rather than the 
WT-based one.
Combined approach was proposed in [10] to extract formants including multi-
resolution analysis of wavelet with linear predictive coding (LPC). Another com-
bined method is also introduced in [10] using WT with cepstral analysis as explained 
in Fig. 4.2. The experimental results show that the proposed techniques give better 
results when compared to those extracted using conventional methods like LPC and 
cepstrum.
4.3  Pitch Estimation
For human speech, the pitch characterizes voiced sounds which have periodic exci-
tations. It is defined as the fundamental frequency of sound excitation to vocal tract 
entrance at the glottis. Spectral wise, it may be identified at the lowest frequency 
having maximum peak with respect to its neighboring frequencies.
It is very difficult to estimate the pitch period due to the inherent large variability 
in natural speech [5]. Traditional algorithms for pitch detection can be classified 
into two types, spectral-domain (nonevent)-based and time-domain (event)-based 
pitch detectors. The spectral-based pitch detectors, such as the autocorrelation and 
the cepstrum methods, estimate the average pitch period over a fixed-length window 
of a speech signal. The time-based pitch detectors estimate the pitch period by mea-
suring the period between two successive instants of glottal closures. Pitch period 
Formants F1, F2 and F3
LPC
LPC
LPC
LPC
A3
D2
D3
D1
3 level DWT
Detection of spectral peaks
Preprocessing and framing
Speech signal
Fig. 4.2  Formant 
estimation using combined 
DWT and LPC noting that 
another approach in [10] 
uses cepstrum analysis in 
place of LPC
4.3  Pitch Estimation

26
can, in some sense, be related to finding the local maximum in wavelet representation 
of sound [5]. Several pitch determination algorithms have been presented based on 
wavelet analysis in [11], [12], [13] and [14].
Early work on pitch evaluation is presented in [15] using WT. In [16], WT is 
applied to the excitation part in the cepstrum of a speech signal. The local maximum 
is then searched within WT coefficients in order to extract the global peak index 
which represents the pitch. The experimental results prove the advantage of this 
method than other classical approaches for pitch estimation. The local maxima are 
also searched in [17] through WT coefficients after pre-filtering for pitch detection. 
Such an approach exhibits superior performance compared to other wavelet meth-
ods in both clean and noisy environments.
Another event-based detection of pitch is developed in [18], and the local maxi-
mum is, also, searched through dyadic WT coefficients. The candidate pitches 
according to such maxima at each scale are then averaged. An optimal scale is cho-
sen at the minimum average of consecutive pitches, and the optimal value of the 
pitch period is estimated at such optimum scale. Experiments using this method 
show superior performance in comparison with other event-based detectors and 
classical ones that use the autocorrelation and cepstrum-based methods.
The method of modified higher-order moment can replace the autocorrelation 
function which is used traditionally in pitch detection. In this method the average of 
the product of k samples are taken instead of two samples. The higher-order moment 
is applied to the transformed signal by dyadic WT, and then the local maxima are, 
also, obtained. Pitch detection is achieved through searching within such maxima. 
It was shown that the modified higher-order moment method is better compared to 
traditional pitch detection techniques [19]. In [19] the results in [19] show an 
improvement over conventional dyadic WT method even after adding noise.
Pitch estimation approach which assumes that the glottal closures are correlated 
with the maxima in the adjacent scales of the WT is often prone to error especially 
in the case of noisy signals. In [20], an optimization scheme is proposed in the 
wavelet framework for pitch detection using a multipulse excitation model for the 
speech signal. Experimental results on both clean and noisy conditions show that 
the proposed optimization works better than widely used heuristic approach of max-
ima detection.
A different technique in [21] uses the DWT to extract the wavelet parameters of 
noisy speech in the fundamental frequency band. Therefore, variance analysis is 
performed to generate a variance statistical distribution function of the wavelet 
parameters. The peak detection approach is then applied to extract the pitch period. 
Due to the combination of the contribution of both WT and variance analysis (VA), 
the proposed method can be effectively applied to pitch period estimation of speech 
with noise. The simulation results show that the proposed method can give a supe-
rior accuracy and robustness against noise relative to some of the existing methods 
from high to very low SNR levels.
In [22] a method is proposed for estimating the pitch of mixed signal using 
WT. After separating the signals, periodicity detection is performed. WPT is then 
applied to analyze the signal into bands, and cepstrum is obtained for each signal in 
4  Spectral Analysis of Speech Signal and Pitch Estimation

27
a specific band. The cepstrum functions, subsequently, lead to the extraction of pitch 
for each signal separately.
On any way, it becomes clear that neither the classic frame-based nor event-­
based approach can extract the real instantaneous pitch. However, EWT can provide 
a reliable and accurate algorithm for extracting the real instantaneous pitch. In [23] 
an estimator of pitch is developed based on EW and provides accurate estimate for 
the time variation of pitch. Algorithmic calculations in this estimator are summa-
rized in Fig. 4.3. Tests of the estimator on both Chinese and English speech reveal 
outperforming results when compared to other methods.
References
	 1.	J.  Galka, M.  Ziolko, Wavelets in speech segmentation, in The 14th IEEE Mediterranean 
Electrotechnical Conference, MELECON2008, 2008, pp. 876–879
	 2.	M. Hesham, A predefined wavelet packet for speech quality assessment. J. Eng. Appl. Sci. 
53(5), 637–652 (2006)
	 3.	A. Karmakar, A. Kumar, R.K. Patney, A multiresolution model of auditory excitation pattern 
and its application to objective evaluation of perceived speech quality. IEEE Trans. Audio 
Speech Lang. Process. 14(6), 1912–1923 (2006)
	 4.	W. Dobson, J. Yang, K. Smart, F. Guo, High quality low complexity scalable wavelet audio 
coding, in Proceedings of IEEE International Conference Acoustics, Speech, and Signal 
Processing (ICASSP’97), Apr 1997, pp. 327–330
	 5.	J.F. Wang, S.H. Chen, J.S. Shyuu, Wavelet transforms for speech signal processing. J. Chin. 
Inst. Eng. 22(5), 549–560 (1999)
	 6.	T.S. Kumar, M.A. Hussain V. Kanhangad, Classification of voiced and non-voiced speech 
signals using empirical wavelet transform and multi-level local patterns, in 2015 IEEE 
International Conference on Digital Signal Processing (DSP), Singapore, 2015, pp. 163–167
	 7.	D. Charalampidis, V.B. Kura, Novel wavelet-based pitch estimation and segmentation of non-­
stationary speech. 8th Int. Conf. Inf. Fusion 2, 1383–1387 (2005)
	 8.	I. Jemaa, K. Ouni, Y. Laprie, Automatic formant tracking method by Fourier and multiresolu-
tion analysis. IFAC Proceedings 43(8), 347–352 (2010)
	 9.	M. Bensaid, J. Schoentgen, S. Ciocea, Estimation of formant frequencies by means of a wave-
let transform of the speech spectrum, in Proceeding of the ProRISC Workshop on Circuits, 
Systems and Signal Processing, 1997, pp. 42–46
	10.	D.Y.  Loni, S.  Subbaraman, Formant estimation of speech and singing voice by combin-
ing wavelet with LPC and Cepstrum techniques, in 2014 9th International Conference on 
Industrial and Information Systems (ICIIS), Gwalior, 2014, pp. 1–7
	11.	M. Obaidat, C. Lee, B. Sadoun, D. Neslon, Estimation of pitch period of speech signal using a 
new dyadic wavelet transform. J. Inf. Sci. 119, 21–39 (1999)
Separation of 
Voiced Sounds
Band-
Filtering
EWT
Speech Signal
Pitch
Hilbert 
Transform
Smoothing
Fig. 4.3  Block diagram of pitch estimation using EWT [23]
References

28
	12.	M. Obaidat, A. Bradzik, B. Sadoun, A performance evaluation study of four wavelet algo-
rithms for the pitch period estimation of speech signals. J. Inf. Sci. 112, 213–221 (1998)
	13.	E. Ercelebi, Second generation wavelet transform based pitch period estimation and voiced/
unvoiced decision for speech signals. Appl. Acoust. 64, 25–41 (2003)
	14.	S. Kadambe, F. Boudreaux-Bartels, Application of the wavelet transform for pitch detection of 
speech signals. IEEE Trans. Inf. Theory 38(2), 917–924 (1992)
	15.	J.I. Agbinya, Discrete wavelet transform techniques in speech processing. TENCON '96. Proc. 
1996 IEEE TENCON. Digit. Signal Process. Appl. Perth, WA 2, 514–519 (1996)
	16.	F. Bahja, E.-H. Ibn Elhaj, J. Di Martino, On the use of wavelets and cepstrum excitation for 
pitch determination in real-time, in International Conference on Multimedia Computing and 
Systems (ICMCS), Tangier, Morocco, 2012, pp. 150–153
	17.	C. Runshen, Z. Yaoting, S. Shaoqiang, A modified pitch detection method based on wavelet 
transform, in Second International Conference on Multimedia and Information Technology 
(MMIT), 2010, Kaifeng, China, vol. 2, 2010, pp. 246–249
	18.	S. Bing, G. Chuan-qing, J. Zhang, A new pitch detection algorithm based on wavelet trans-
form. J. Shanghai University (English Edition) 9(4), 309–313 (2005)
	19.	J. Choupan, S. Ghorshi, M. Mortazavi, F. Sepehrband, Pitch extraction using dyadic wave-
let transform and modified higher order moment, in 12th IEEE International Conference on 
Communication Technology (ICCT), 2010, Nanjing, China, 2010, pp. 833–836
	20.	P. Ghosh, A. Ortega, S. Narayanan, Pitch period estimation using multipulse model and wave-
let transform, in Proceedings of INTERSPEECH, ICSLP2007, Antwerp, Belgium, August 
2007, pp. 2761–2764
	21.	X. Wei, L. Zhao, Q. Zhang, J. Dong, Robust pitch estimation using a wavelet variance analysis 
model. Signal Process. 89(6), 1216–1223 (2009)
	22.	T.K. Muhaseena, M.S. Lekshmi, A model for pitch estimation using wavelet packet transform 
based Cepstrum method. Procedia Technol. 24, 1061–1067 (2016)
	23.	Y. Li, B. Xue, H. Hong, X. Zhu, Instantaneous pitch estimation based on empirical wavelet 
transform. in 2014 19th International Conference on Digital Signal Processing, Hong Kong, 
2014, pp. 250–253
4  Spectral Analysis of Speech Signal and Pitch Estimation

29
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_5
Chapter 5
Speech Detection and Separation
5.1  Voice Activity Detection
Speech detection means the localization of the speech part in a signal containing 
other signals or noise. Many parameters are employed for speech detection and 
separation from other superimposed signals. Some of such parameters are the time 
energy (the magnitude in time domain), zero-crossing rate (ZCR), cepstral coeffi-
cients, pitch information, and time-frequency parameters. Methods employing these 
parameters usually fail to detect speech when SNR is low [1]. Detection in the pres-
ence of noise with varying level is more challenging than in the presence of impulse 
noise or fixed level noise. WT can reduce the influences of different types of noise 
at different levels. A robust speech detection method in the presence of different 
types of noise with various levels is necessary for many practical applications [1].
The wavelet energy is used in [1], [2], and [3] to detect voice activity. A recurrent 
fuzzy neural network (NN) is applied in [1] and [3] to discriminate WT coefficients 
of voice and noisy silence. Results are obtained in these works with different types 
of noise and various SNRs. Comparing the results with other robust detection meth-
ods has verified the robust performance of WT-based methods.
In a different study in [4], WT is used to implement a voice activity detector 
(VAD) for European Telecommunication Standards Institution (ETSI) adaptive 
multi-rate (AMR) narrow-band (NB) (ETSI AMR-NB) and wide-band (WB) speech 
codecs. The original IIR filter bank and pitch/tone detector in the codec are reimple-
mented, respectively, via the wavelet filter bank and the wavelet-based pitch/tone 
detection algorithm. The wavelet filter bank can divide the speech signal into sev-
eral frequency bands. The background noise level can, also, be estimated in each 
subband by using the wavelet thresholding method. The wavelet filter bank is also 
derived to detect correlated complex signals like music. A support vector machine 
(SVM) is then used to train the VAD decision rule involving the subband power, 
noise level, pitch period, tone flag, and complex signals warning flag of input speech 

30
signals. Experimental results show that the proposed algorithm in [4] gives consid-
erable VAD performance compared to the standard one. SVM is used again in [5] 
for building a VAD based on MFCC of multiresolution spectrum via WT. Experiments 
on this VAD achieve robustness for all SNRs compared to VAD of G.729b while 
computational time delay satisfies the needs of real-time transmission on G.729b.
WP is, also, used in [6] to obtain features across frequency and time for VAD. The 
feature extraction is based on observations of the angles between the vectors in 
feature space. These WPT-based features also help to distinctly discriminate the 
voiced, unvoiced, and transient components of speech. Experimental results show 
that the proposed WPT-based approach in [6] is sufficiently robust such that it can 
extract the speech activity under poor SNR conditions and that it is also insensitive 
to variable level of noise.
5.2  Segmentation of Speech Signal
Speech signals typically need to be decomposed into small segments in many 
applications, namely, speech recognition, analysis, or coding. Segmentation of 
speech can be easily performed for a limited number of classes like vowel-conso-
nant detection based on simple rules on amplitude and ZCR. Difficulties arise if the 
segment contains the end of one phoneme and the beginning of another caused by 
phonetic coarticulation. That is why segmentation of speech for many classes is so 
hard. However, WT is well localized in the time-frequency domain, and boundaries 
of speech segments can be willingly detected in a process like that in Fig. 5.1. 
Preprocessing in 
time domain
Onset identification
Identification of Speech 
unit boundaries
Segmentation
Spectral/WT analysis
Input speech signal
Identified speech segments
Fig. 5.1  Speech 
segmentation using WT
5  Speech Detection and Separation

31
As an example, the work in [7] presented results on successful segmentation of 
speech signal using WT into four distinct classes, namely, voiced, plosive, frica-
tive, and silent.
Nonuniform segmentation of speech signal has been achieved by finding pho-
neme boundaries in a speech signal for recognition using that WPD in [8]. The work 
in [8] used perceptual scale for decomposition of speech via Meyer wavelet in the 
WP structure. The use of Mayer wavelets results in the separation of the frequency 
bands with a better resolution when compared to other wavelets. The mean best 
basis algorithm is then applied for choosing 11 subbands. Localization of boundar-
ies is decided by analysis of the energy flows among such WPD levels.
Artificial neural network (ANN) is employed in [9] to accomplish the segmenta-
tion and recognition of vowel phonemes in Assamese language. A self-organizing 
map (SOM) has been trained with a various number of iterations to segment the 
word into its constituent phonemes. The DWT-based segmentation is then com-
pared with the developed algorithm with a result in favor of ANN.
5.3  Source Separation of Speech
WT is applied for the decomposition of speech signal space into two orthogonal 
subspaces in [10] achieving well separation of the sources. Experimental results on 
speech recognition of two simultaneous speakers are obtained using such decompo-
sition approach. Very high separation performance has been attained in the most 
difficult mixing situations with high interference. Experiments prove such results, 
both for the measured phoneme recognition accuracy of a speech recognition sys-
tem and for the signal-to-interference ratio before and after the wavelet-based 
separation.
As independent component analysis (ICA) can identify independent sources 
using only sensor observation data, it requires at least as many sensors as sources 
[11]. Therefore, the work in [12] applies speech source separation using ICA on WT 
decomposition of multisensor data. Mixed speech signals are decomposed into dif-
ferent levels using DWT, and the subbands of each level are, thereafter, separated 
using ICA. The source signals are finally reconstructed from separated subbands. 
The presented algorithm in [12] achieves improved separation for multiple sources 
of real-room-recorded data. That method achieves higher SNR when compared to 
ICA-based separation.
A similar work in [13] tackles the problem of speech source separation using 
ICA but using the undecimated wavelet packet decomposition (UWPD). UWPD 
improves the non-Gaussianity distribution of independent components in the speech 
signal which is a pre-requirement for ICA. Also UWPD increases independency of 
such components. Figure 5.2 describes the process for source separation in speech 
signal based on UWPD and ICA. In the filtering block, the filter bank frequencies 
and bandwidths are designed based on critical bands (CB) of human hearing while 
the optimized coefficients are chosen through applying best selection algorithm. 
5.3  Source Separation of Speech

32
The mixing process may occur naturally through real-room recording or arithmeti-
cally during simulation. The results show that UWPD-based method gives a consid-
erable improvement when compared to other techniques.
Since the estimation of mixing matrix is a key issue in the blind source separa-
tion, an algorithm has been proposed in [11] for estimating it after the separation of 
speech signals from noise. In that study, the phase difference between the two mix-
tures is defined in the WP domain, and histograms of phase differences are obtained 
for every wavelet packet. The Laplacian mixture model (LMM) is applied to each 
histogram of packets. Moreover, expectation maximization (EM) algorithm is used 
to train the LMM and evaluate the model parameters. The best wavelet packet node 
is then used to find source directions on scatter plots using variance calculations. 
Figure 5.3 shows the sequence of processes followed in [11] for estimating the mix-
ing matrix. The performance of different mother wavelets is evaluated for the esti-
mation, and the best wavelet has been determined. The study reveals that the discrete 
Perceptual 
Filterbank
(CB-UWPD)
ICA
Best Selection 
Estimate 
sources
Mixing
s1(t)
s2(t)
UW coeff
Opt. coeff
s^1(t)
s^2(t)
Fig. 5.2  UWPD-based ICA separation of speech sources
s2(t)
s1(t)
WPD on each mixture s1 and s2
Phase difference calculation, j
LMM-EM training for j 
Mixing matrix
Fig. 5.3  The sequence of 
processes followed in [11] 
for estimating mixing 
matrix
5  Speech Detection and Separation

33
Meyer wavelet gives better results. At last, a cost function is defined for achieving 
minimum correlation between sources incorporating an adaptive algorithm in the 
wavelet packet domain. The presented algorithms are evaluated for mixing matrix 
estimation and source separation, and the results indicate that estimation of the mix-
ing matrix is highly accurate, and separated sources comprise an excellent SNR.
References
	 1.	C. Juang, C. Cheng, T. Chen, Speech detection in noisy environments by wavelet energy-based 
recurrent neural fuzzy network. Expert Syst. Appl. 36(1), 321–332 (2009)
	 2.	S.M. Joseph, A.P. Babu, Wavelet energy based voice activity detection and adaptive threshold-
ing for efficient speech coding. Int. J. Speech Technol. 19(3), 537–550 (2016)
	 3.	C.C. Tu, C. Juang, Recurrent type-2 fuzzy neural network using Haar wavelet energy and 
entropy features for speech detection in noisy environments. Expert Syst. Appl. 39(3), 2479–
2488 (2012)
	 4.	S.H. Chen, R. Guido, T.K. Truong, Y. Chang, Improved voice activity detection algorithm 
using wavelet and support vector machine. Comput. Speech Lang. 24(3), 531–543 (2010)
	 5.	W.  Xue, S.  Du, C.  Fang, Y.  Ye, Voice activity detection using wavelet-based multiresolu-
tion Spectrum and support vector machines and audio mixing algorithm, computer vision in 
human-computer interaction, lecture notes in computer science. Spring 3979, 78–88 (2006)
	 6.	M. Eshaghi, M. Mollaei, Voice activity detection based on using wavelet packet. Digit.Signal 
Process. 20(4), 1102–1115 (2010)
	 7.	B. Tan, R. Lang, H. Schroder, A. Spray, P. Dermody. Applying wavelet analysis to speech seg-
mentation and classification. In H. H. Szu, Wavelet Appl. Proc. SPIE 2242, 750–761, (1994)
	 8.	M. Ziolko, J. Galka, B. Ziolko, T. Drwiega, Perceptual wavelet decomposition for speech seg-
mentation, in 11th Annual Conference of the International Speech Communication Association 
2010 (INTERSPEECH 2010), Vols. 3 and 4, 2234–2237
	 9.	M.  Sarma, K.K.  Sarma, Segmentation and classification of vowel phonemes of assamese 
speech using a hybrid neural framework. Appl. Comput. Intell. Soft Comput. 2012, 8 (2012)
	10.	Α. Koutras, E. Dermatas, G. Kokkinakis, Blind speech separation using wavelet decomposi-
tion, in 6th International Workshop on Speech and Computers, Moscow, Russia, Oct 2001, 
pp. 146–149
	11.	B. Mozaffari, M.A. Tinati, Blind source separation of speech sources in wavelet packet domains 
using Laplacian mixture model expectation maximization estimation in over-­complete- cases. 
J. Stat. Mech. Theory Exp. An IOP and SISSA J. 1–31 (2007)
	12.	X. Wu, J. He, S. Jin, A. Xu, W. Wang, Blind separation of speech signals based on wave-
let transform and independent component analysis. Trans. Tianjin University 16(2), 123–128 
(2010)
	13.	I. Missaoui, Z. Lachiri, Undecimated wavelet packet for blind speech separation using inde-
pendent component analysis, in Advances in Computing and Communications. ACC 2011. 
Communications in Computer and Information Science, ed. by A. Abraham, J. L. Mauri, J. F. 
Buford, J. Suzuki, S. M. Thampi, vol. 193, (Springer, Berlin, Heidelberg, 2011), pp. 318–328
References

35
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_6
Chapter 6
Speech Enhancement and Noise Suppression
Speech enhancement aims to improve the quality and intelligibility of a speech 
signal, as perceived by the human hearing process. Figure 6.1 shows the main pro-
cesses included during speech enhancement. Given that a noisy speech signal has 
additive noise, the signal can be assumed as
	
s m
y m
n m
( ) = ( ) + ( ) 	
(6.1)
where n(m) is the noise component in the signal and y(m) is the original free com-
ponent at a discrete time instant m; S(ω) and N(ω) are the spectral transformation of 
the signal s and noise n, respectively.
As customarily expected, WT can be efficiently used as a spectral analysis tool. 
After completing the enhancement process, it will be claimed that the estimated 
clean signal s^(m) approaches y(m) in an optimal way. On another hand, the wavelet 
transform (WT) has been used for blind adaptive filtering of speech signals from 
unknown colored noise when neither speech nor noise is separately accessible [1]. 
Moreover, a different approach is proposed in [2] to detect and classify background 
noise in speech sentences based on the negative selection algorithm and the dual-­
tree complex wavelet transform. Next this algorithm judges any noise existence, and 
then the type of the background noise is identified among the real noise types 
considered.
In general, wavelets have been widely used for noise suppression in speech sig-
nals since the inception of wavelet analysis. The ideas of removing noise by WT are 
based on the singularity information analysis in [3] and the thresholding of the 
wavelet coefficients. The seminal works on signal denoising via wavelet threshold-
ing or shrinkage in [4] and [5] have shown that various wavelet thresholding schemes 
for denoising have near-optimal properties in the minimax sense. The wavelet 
denoising starts with WT using a specific wavelet basis. Only few coefficients in the 
lower bands can be used for approximating the main features of the clean signal. 

36
Hence, by setting the smaller details, up to a predetermined threshold value, to zero, 
we can reach nearly optimal elimination of noise while preserving the important 
information of the clean signal.
The thresholding (shrink) approach comprises the following steps:
	1.	 A forward WT of the observed data
	2.	 Thresholding the wavelet coefficients
	3.	 Inverse WT of the thresholded coefficients
6.1  Thresholding Schemes
There are two thresholding schemes which are frequently used: the hard-­thresholding 
function which keeps the input if it is larger than the threshold; otherwise, it is set to 
zero as in Eq. (6.2).
	
Hard thresholding
,
,
if
,
if
,
S
a b
S a b
S a b
S a b
^
,
(
) =
(
)
(
) >
(
) ≤



λ
λ
0
	
(6.2)
The other alternative is the soft thresholding which takes the argument and 
shrinks it by a threshold if its modulus is larger than that threshold or shrinks it 
toward zero for the other case as described in (6.3) [6].
Noisy Speech s(m)
Spectral Analysis
Power-density 
estimation of Noise
Thresholding and Noise Filtering
Clean Signal Synthesis
Threshold estimation 
Enhanced Speech s^(m)
S(w)
S ^ (w)
N(w)
Fig. 6.1  Speech 
enhancement process
6  Speech Enhancement and Noise Suppression

37
	
Soft thresholding
,
,
if
,
if
,
S
a b
S a b
s
S a b
S a b
sg
^
.
,
(
) =
(
) −
(
) >
(
) ≤
λ
λ
0
λ



=
(
)
(
)
while
,
s
S a b
sg
sgn
	
(6.3)
where S(a, b) is the thresholded wavelet coefficient at wavelet level of scale a and 
time shift b and sgn is the function of arithmetic sign.
The estimation of the threshold seeks to minimize the maximum error over all 
possible samples of a signal. A more advanced strategy based on Stein’s unbiased 
risk estimate (SURE) is considered to get a threshold value as discussed in [5] and 
[6]. Additionally, a universal threshold is proposed based on the standard deviation 
of the signal. This threshold is shown to be asymptotically optimal in the minimax 
sense when employed as a hard threshold [7].
Hard thresholding maintains the scale of the signal but introduces ringing and arti-
facts after reconstruction due to a discontinuity in the wavelet coefficients. Alternatively, 
soft thresholding eliminates this discontinuity resulting in smoother signals, but it 
slightly decreases the magnitude of the reconstructed signal [7]. The soft-thresholding 
rule is preferred over the hard one for several reasons as discussed in [8].
Semisoft shrinking is introduced to improve the shrinkage approach for denois-
ing with selected threshold for unvoiced regions [9] while a smooth hard-­thresholding 
function is presented in [10], [11], and [9] based on μ-law. The combination of soft 
and hard thresholding is applied to deal with the different properties of the speech 
signal in [12]. Wavelet thresholding methods are also integrated with other tech-
niques such as the Teager energy operator and the masked adaptive threshold as 
discussed in [13] .
In literature, the method which employs a universal threshold may be called 
VisuShrink [7]. SureShrink is a hybrid of employing universal threshold with a 
SURE-based threshold when soft thresholding is used [14]. BayesShrink is an 
adaptive wavelet thresholding method proposed in [8] using a Bayesian estimate of 
the risk.
An updated work in [15] considers undecimated wavelet transform (UWT) to 
avoid the drawback of the DWT which is not shift invariant. Also, in [6] two new 
approaches were proposed, namely, sophisticated thresholding and biased risk 
thresholding. The results show that both methods need improved approach for 
threshold estimation.
6.2  Thresholding on Wavelet Packet Coefficients
Other works consider WPT instead of DWT in [16], [17], and [18]. Recently some 
works treat the wavelet packet tree in a perceptual manner [19], in which the percep-
tual wavelet filter bank (PWF) is built to approximate the critical-band responses of 
the human ear. Critical-band wavelet decomposition is used with noise masking 
6.2  Thresholding on Wavelet Packet Coefficients

38
threshold in [20]. Another perceptual wavelet packet decomposition (PWPD) which 
simulates the critical bands of the psychoacoustic model is proposed in [21] and 
[17]. In [22], an adaptive threshold is statistically determined and applied to WP 
coefficients of noisy speech through a hard-thresholding function. Several standard 
objective measures and subjective observations show that the proposed method per-
forms well from high to low level SNRs. Yet in [23], a semisoft mask filter with 
exponential increase is introduced to the method employing wavelet packet decom-
position and mask filtering. The semisoft mask is proposed to minimize signal loss 
resulting from traditional binary mask, and the exponential filter removes residual 
noise. The proposed semisoft filtering approach can be summarized in the following 
equation:
	
S
a b
S a b
S a b
e
Avg s m
L m
f b
^
,
.
(
:
,
,
if
,
if
otherwi
(
) =
(
)
(
) >
⋅
−
(
) >
( )
λ
σ
0 5
0
sse






	
(6.4)
where f(b) is a function of the coefficient sequence in the defined packet and L is the 
width of the considered mask. This relationship estimates the speech signal on dif-
ferent scales or bands using the variance (σ) of the wavelet coefficients [23].
In [24] a wavelet packet filter bank is optimized to match critical bands of human 
ear such that the associated noise with input speech may be minimized. The result, 
therefore, shows a better quality when compared to the related work described in [25]. 
A similar approach in [26] employs a noise estimator based on nonnegative matrix 
factorization (NMF). All of the subband signals obtained using WP analysis are indi-
vidually compensated using NMF-estimated noise in a specific subband. At last, these 
updated subband signals are concatenated for signal reconstruction. Instead, in [27] 
noise estimation is obtained for each subband of a specific filter bank matching the 
hearing critical bands using WP analysis. The estimated noise spectrum is then calcu-
lated by STFT, and spectral over-subtraction is applied.
Alternatively, a two-stage dual-tree complex WPT is implemented in [28] for 
speech enhancement. The drawback of signal distortions caused by down sampling 
of WPT has been overcome through the two-stage analytic decomposition which 
concatenates UWPT and decimated WPT. Noise estimation is conducted through 
estimators based on a speech presence probability and a generalized minimum 
mean squared error. The results show that the proposed method gives better objec-
tive quality scores at low SNR when compared to other state-of-the-art measures.
6.3  Enhancement on Multitaper Spectrum
Another approach considers wavelet denoising with multitaper spectrum (MTS) 
estimation. The MTS estimate can be obtained as in (6.5)
6  Speech Enhancement and Noise Suppression

39
	
S
L
S
mt
k
L
K
mt
^
^
ω
ω
( ) =
( )
=
−
∑
1
0
1
	
(6.5)
where S
L
a
m s m e
k
mt
m
L
k
j m
^
)
ω
ω
( ) =
( ) ( )
=
−
−
∑
1
0
1
2
 such that ak is the kth tapping coefficient, 
L is the analysis window length, and ω is the angular frequency. The tapers are cho-
sen as a set of orthogonal basis which may be obtained from wavelet repository or 
any harmonic sinusoids. Since the wavelet shrinkage approach has not been fully 
optimized for denoising, the MTS of noisy speech signals is obtained in [24] through 
a wavelet packet filter bank which is optimized to match critical bands. As a result, 
musical noise residual after spectral subtraction has been trimmed down through 
selection in log MTS. Similarly, a two-stage wavelet denoising algorithm is pro-
posed in [14] for estimating the speech power spectrum. The wavelet transform is 
applied to the periodogram of a noisy speech signal, and the resulting wavelet coef-
ficients are searched to indicate the approximate locations of the noise floor in the 
periodogram. The wavelet coefficients of the noise floor are then selectively removed 
in the log MTS of the noisy speech. The wavelet coefficients that remain are then 
used to reconstruct a denoised MTS. Simulation results outperform the traditional 
shrinking approaches and improve both the quality and intelligibility of the enhanced 
speech [14].
References
	 1.	D. Veselinovic, D. Graupe, A wavelet transform approach to blind adaptive filtering of speech 
from unknown noises. IEEE Trans. Circuits Syst. II, Analog Digit. Signal Process. 50(3), 150–
154 (2003)
	 2.	C.C.E. de Abreu, M.A.Q. Duarte, F. Villarreal, An immunological approach based on the nega-
tive selection algorithm for real noise classification in speech signals. AEU-Int. J. Electron.  
C. 72, 125–133 (2017)
	 3.	S. Mallat, W. Hwang, Singularity detection and processing with wavelets. IEEE Trans. Inf. 
Theory 38(2), 617–643 (1992)
	 4.	D.L.  Donoho, I.M.  Johnstone, Ideal spatial adaptation via wavelet shrinkage. Biometrika 
81(3), 425–455 (1994)
	 5.	D.L. Donoho, I.M. Johnstone, Adapting to unknown smoothness via wavelet shrinkage. J. Am. 
Stat. Assoc. 90(432), 1200–1224 (1995)
	 6.	B. Wieland, Speech signal noise reduction with wavelets. Diplomarbeit an der Universität Ulm 
, October 2009
	 7.	V.  Balakrishnan, N.  Borges, L.  Parchment, Wavelet Denoising and Speech Enhancement 
(Department of Electrical and Computer Engineering, The Johns Hopkins University, 
Baltimore, Spring 2006)
	 8.	G. Chang, B. Yu, M. Vetterli, Adaptive wavelet thresholding for image denoising and compres-
sion. IEEE Trans. Image Process. 9(9), 1532–1546 (2000)
	 9.	V.T. Pham, Wavelet analysis for robust speech processing and applications, Ph.D. dissertation, 
Ph. D. thesis, (VDM Verlag, Saarbrücken, Germany, 2007), http://theses.eurasip.org/media/theses/
documents/pham-van-tuan-wavelet-analysis-for-robust-speech-processing-and-applications.pdf
References

40
	10.	H. Sheikhzadeh, H.R. Abutalebi, An improved wavelet-based speech enhancement system, in 
Proceedings of Eurospeech, 2001, pp. 1855–1858
	11.	S. Chang, Y. Kwon, S. Yang, I. Kim, Speech enhancement for nonstationary noise environment 
by adaptive wavelet packet. Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP 2002) 1, 
561–564 (2002)
	12.	A. Lallouani, M. Gabrea, C.S. Gargour, Wavelet based speech enhancement using two dif-
ferent threshold-based denoising algorithms, in Proceedings of the Canadian Conference on 
Electrical and Computer Engineering, 2004, pp. 315–318
	13.	M. Bahoura, J. Rouat, Wavelet speech enhancement based on the Teager energy operator. 
IEEE Signal Process. Lett. 8(1), 10–12 (2001)
	14.	D. Pak-Kong Lun, T.-W. Shen, T.-C. Hsung, D.K.C. Ho, Wavelet based speech presence prob-
ability estimator for speech enhancement. Digit. Signal Process. 22(6), 1161–1173 (2012)
	15.	M.A Hassanein, M. El-Barawy, N.P.A. Seif, M.T. Hanna, Trimmed thresholding with SURE 
for denoising signals, circuits and systems (MWSCAS), in 2012 IEEE 55th International 
Midwest Symposium on, 5–8 Aug 2012, pp. 1024–1027
	16.	S. Chang, Y. Kwon, S.-i. Yang, I.-J. Kim, Speech enhancement for non-stationary noise envi-
ronment by adaptive wavelet packet. Proc. IEEE Int. Conf. Acoust. Speech Signal Process. 
(ICASSP’2002) 1, I-561–I-564 (2002)
	17.	S.-H. Chen, J.-F. Wang, Speech enhancement using perceptual wavelet packet decomposition 
and Teager energy operator. J. VLSI Signal Process. Syst. Signal Image Video Technol. 36, 
125–139 (2004)
	18.	Y. Ghanbari, M.R.K. Mollaei, A new approach for speech enhancement based on the adaptive 
thresholding of the wavelet packets. Speech Comm. 48, 927–940 (2006)
	19.	S. Yu, C.-H. Chang, Bayesian separation with Sparsity promotion in perceptual wavelet domain 
for speech enhancement and hybrid speech recognition. IEEE Trans. Syst. Man Cybern. Part 
A Syst. Hum. 41(2), 284–293 (2011)
	20.	C. Lu, H.C. Wang, Enhancement of single channel speech based on masking property and 
wavelet transform. Speech Comm. 41(3), 409–427 (2003)
	21.	E. Jafer, A.E. Mahdi, Wavelet-based perceptual speech enhancement using adaptive threshold 
estimation, in Proceedings of Eurospeech, 2003, pp. 569–572
	22.	T. Sanam, C. Shahnaz, Noisy speech enhancement based on an adaptive threshold and a modi-
fied hard thresholding function in wavelet packet domain. Digit. Signal Process. 23(3), 941–
951 (2013)
	23.	G. Lee, S.D. Na, K.W. Seong, J.-H. Cho, M.N. Kim, Wavelet speech enhancement algorithm 
using exponential semi-soft mask filtering. Bioengineered 7(5) (2016)
	24.	E.  Jayakumar, P.  Sathidevi, Speech Enhancement Based on Noise Type and Wavelet 
Thresholding the Multitaper Spectrum, in Advances in Machine Learning and Signal 
Processing. Lecture Notes in Electrical Engineering, ed. by P. Soh, W. Woo, H. Sulaiman, 
M. Othman, M. Saat, vol. 387, (Springer, Cham, 2016), pp. 187–200
	25.	Y. Hu, P.C. Loizou, Speech enhancement based on wavelet thresholding the multitaper spec-
trum. IEEE Trans. Speech Audio Process. 12(1), 59–67 (2004)
	26.	S.S. Wang et al., Wavelet speech enhancement based on nonnegative matrix factorization. 
IEEE Signal Process. Lett. 23(8), 1101–1105 (2016)
	27.	N.  Upadhyay, A.  Karmakar, A perceptually motivated stationary wavelet packet filterbank 
using improved spectral over-subtraction for enhancement of speech in various noise environ-
ments. Int J Speech Technol. 17(2), 117–132 (2014)
	28.	P. Sun, J. Qin, Speech enhancement via two-stage dual tree complex wavelet packet transform 
with a speech presence probability estimator. J. Acoust. Soc. Am. 141(2), 808–817 (2017)
6  Speech Enhancement and Noise Suppression

41
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_7
Chapter 7
Speech Recognition
Automatic speech recognition (ASR) systems generally carry out some kind of 
­classification/recognition based upon speech features which are usually obtained 
via time-frequency representations. Accordingly, a speech waveform is mapped into 
features space in the form of numerical vectors. A common set of feature vectors are 
variants of spectral representation of speech signal obtained by Fourier or cepstral 
analysis. Acoustic and linguistic models are then used with the extracted features to 
recognize the content of an utterance.
7.1  Signal Enhancement and Noise Cancellation for Robust 
Recognition
While real-world applications require that speech recognition systems be robust to 
interfering noise, the performance of a speech recognition system drops dramati-
cally when there is a mismatch between training and testing conditions. Many dif-
ferent approaches have been studied to decrease the effect of noise on the recognition 
process [1]. Wavelet denoising can be applied as a preprocessing stage before fea-
ture extraction to compensate noise effects [2]. WT has become a frequently used 
method for improvement of speech recognition in recent years as proved by denois-
ing theory and practice [3].
Dealing with enhancement and feature extraction for robust ASR, several param-
eterization methods which are based on the DWT and WPD have been proposed in [3]. 
More sophisticated shrinking functions with better characteristics than soft and hard 
thresholding are optimized for speech enhancement and speech recognition in [4]. 
Another approach in [5] relies on the detection and classification of background 
noise in speech sentences based on the negative selection algorithm and dual-tree 
complex WT. The identification of noise type can help in the development of new 

42
speech enhancement or automatic speech recognition systems. A schematic of 
­optimizing wavelet parameters for noise and speech representation is shown in 
Fig. 7.1.
Improved speech enhancement method is presented in [6] using Wiener filtering 
on wavelet coefficients. The gain of Wiener filter is optimized to compensate for 
contamination by the effects of late reflection and background noise. Obtained rec-
ognition rate is better when compared to conventional methods.
7.2  Wavelet-Based Features for Better Recognition
The most widely used speech representation is based on MFCC inspired from mod-
els of human perception. However, they provide limited robustness, as evidenced by 
the difficulty to adapt to noise and distortions [3]. Recent advances have been made 
with the introduction of wavelet-based representations, which improves the classifi-
cation performance. The WT overcomes some of the limitations faced by other fea-
tures since it can be used to analyze a speech signal directly through selective 
wavelet packet structure into the critical bands defined by a psychoacoustic model. 
Rather than conventional wavelets, the effect of gamma-tone wavelet was explored 
in [7] getting features toward more robust ASR.
Eventually, many works employ wavelet features in an ASR, while such features 
are computed based on critically sampled filter bands using WT or WP analysis [1, 
8–10, 11–13, 14]. Although most of the works on WP considered a fixed selective 
WP tree structure, e.g., [9] based on the MEL scale, the work in [13] examines a 
specific critical-band frequency partition by means of adopting rational and dyadic 
WP filter banks. In [15] the problem of WP filter-bank selection is considered to 
obtain adaptive and nearly optimal energy-based filter-bank signatures for an ASR 
WT
IWT
Is 
Optimized?
Adjust wavelet 
scale and shift
Speech DB
Noise DB
No
Optimized 
for Speech
Optimized 
for Noise
Yes
Fig. 7.1  Optimized detection and classification of background noise in speech based on WT [5]
7  Speech Recognition

43
system. The filter-bank selection was addressed again by a complexity regularized 
criterion and feature discrimination. The usage of wavelet-based features extracted from 
selective sampled WPD leads to improvement of recognition rate compared with the 
well-known conventional MFCC features [16, 17].
However, WT features are not time invariant. Time variance property decreases the 
rate of speech recognition systems. On the other hand, due to the nature of discrete 
wavelet, exact auditory band width as in Mel scale cannot be achieved [9]. Another 
perspective for optimizing wavelet speech recognition is presented in [18, 19], and it 
is based on the frequency aspect of CWT. The features are chosen based on Bark 
scale, and it is known as Bark wavelet. The performance of ASR with features based 
on wavelet analysis is discussed and compared in [18, 19].
On another level, down-sampling in DWT results in time-variant features of 
speech which means a small signal displacement with time and that the exact match-
ing with auditory critical bands and associated bandwidths cannot be achieved as in 
[9]. So, critically sampled WT or WPD can be replaced by multichannel filter banks 
which afford more degrees of freedom. Thereafter, it becomes possible to obtain 
both symmetry and orthogonality. Proper redundant filter banks are presented in 
[20] such that the used WT expands a C point input signal into a D point output, 
where D > C. Accordingly, redundant wavelet filter banks (RWFB) are proposed to 
provide features in a neural network based speech recognizer [20]. Although RWFB 
employs higher-density filter bank, however, this structure has better time-frequency 
localization and accordingly becomes less shift sensitive [21] than those of critically 
sampled WPD. The proposed structure outperforms that of critically sampled wave-
let filter bank but at the expense of higher redundancy.
As the missing of features based on signal phase may affect the recognition accu-
racy, the work in [22] adds phase-based features extracted from WT on nonlinear 
representation of speech signal. 	
Moreover, WP adaptive network based fuzzy 
inference system (ANFIS) is developed in [23] utilizing WP features for more 
robustness in ASR.
Since a wavelet based representation should be searched for each particular prob-
lem, a genetic algorithm is employed in [24]. The representation search is based on a 
non-orthogonal wavelet decomposition, for phoneme classification. The results, 
obtained for a set of Spanish phonemes, show that the proposed genetic algorithm is 
able to find a representation that improves speech recognition results [24].
7.3  Hybrid Approach
Although the removal of noise can improve the speech recognition accuracy, errors 
in the estimated signal components can obscure the recognition. To overcome this, 
a wavelet-based framework is realized in [25] by implementing speech enhance-
ment preprocessing, feature extraction, and a hybrid speech recognizer in the wave-
let domain. A Bayesian scheme is applied in a wavelet domain to separate the speech 
and noise components in an iterative speech enhancement algorithm. The denoised 
7.3  Hybrid Approach

44
wavelet features are then fed to a classifier. The intrinsic limitation of the used 
­classifier is overcome by augmenting it with a support vector machine (SVM) [26]. 
This hybrid and hierarchical design paradigm improves the recognition perfor-
mance at a low SNR [25].
In [27] a frontend is introduced to achieve noise robustness for ASR using wave-
lets in both enhancement stage and then in the feature extraction. The methodology 
in that work includes a time-adapted hybrid wavelet speech enhancement. Thereafter, 
Teager energy operators and dynamic perceptual wavelet packet features are fed to 
hidden Markov model (HMM)-based recognizer. The experiments show that the 
proposed work in [27] gives better recognition rate than other ASR employing 
MFCC features treated by HMM in noisy environment.
The typical block diagram shown in Fig. 7.2 illustrates the processes included in 
a hybrid system comprising enhancement stage and feature extraction as a front end 
of ASR [27].
7.4  Wavelet as an Activation Function for Neural Networks 
in ASR
An HMM is a stochastic process that can estimate the probability of an observed 
sequence generated through a specific speech unit. The HMM is one of the most 
widely used and successful classifiers for speech recognition. Alternatively, NNs 
can replace HMM with better adaptability. Yet, a wavelet function can replace tradi-
tional activation function in NN (WNN) when used as a recognizer. Thereafter, 
wavelet functions substitute the traditional activation functions of NN in a number 
of works [28, 29, 30]. In [28] simple ZCR and peak-amplitude features are fed to a 
WNN for recognition of limited number of isolated words. Similarly, an NN is pre-
sented in [29] for recognition of Arabic digits using MFCC features with wavelet 
activation function in the hidden layers of an NN. Another approach is proposed in 
[30] for speech recognition based on a hybrid classifier employing WNN.  The 
hybrid WNN-based systems benefit from the ability of wavelet analysis to resist 
noise and the adaptability of NN on the other side. The obtained results in [30] show 
that an ASR system based on WNN is competitive with HMM-based systems.
Raw 
speech 
signal
Preprocessing 
& framing
Perceptual WP 
coefficients
InverseWPT
Time adaptive 
WT shrinkage
WPD
D and DD of 
coefficients 
Combined feature set
HMM-based Recognizer
Fig. 7.2  Block diagram of hybrid enhancement and WT feature extraction as a front end of ASR
7  Speech Recognition

45
References
	 1.	Z. Tufekci, J.N. Gowdy, S. Gurbuz, E. Patterson, Applied mel-frequency discrete wavelet coef-
ficients and parallel model compensation for noise-robust speech recognition. Speech Comm. 
48(10), 1294–1307 (2006)
	 2.	O. Farooq, S. Datta, Wavelet-based denoising for robust feature extraction for speech recogni-
tion. Electron. Lett. 39(1), 163–165 (2003)
	 3.	M. Gupta, A. Gilbert, Robust speech recognition using wavelet coefficient features, Automatic 
speech recognition and understanding, IEEE automatic speech recognition and understanding 
workshop 2001 (ASRU'01), Madonna di Campiglio, Italy, 2001, pp. 445–448
	 4.	B. Kotnik, Z. Kacic, B. Horvat, The usage of wavelet packet transformation in automatic noisy 
speech recognition systems. Int. Conf. Comput. Tool 2, 131–134 (2003)
	 5.	C.C.E. de Abreu, M.A.Q.  Duarte, F.  Villarreal, An immunological approach based on the 
­negative selection algorithm for real noise classification in speech signals. AEU-Int. J. Electron. 
C. 72, 125–133 (2017)
	 6.	R. Gomez, T. Kawahara, K. Nakadai, Optimized wavelet-domain filtering under noisy and 
reverberant conditions. APSIPA Trans. Signal Inf. Process 4(e3), 1–12 (2015)
	 7.	A. Adiga, M. Magimai, C.S. Seelamantula, Gammatone wavelet cepstral coefficients for robust 
speech recognition, in 2013 IEEE International Conference of IEEE Region 10 (TENCON 
2013), Xi’an, 2013, pp. 1–4
	 8.	Z. Xueying, J. Zhiping, Speech recognition based on auditory wavelet packet filter. Proc. 7th 
Int. Conf. Signal Process. 2004 (ICSP '04) 1, 695–698 (2004)
	 9.	O. Farooq, S. Datta, Mel filter-like admissible wavelet packet structure for speech recognition. 
IEEE Signal Process. Lett. 8(7), 196–198 (2001)
	10.	J.N. Gowdy, Z. Tufekci, Mel-scaled discrete wavelet coefficients for speech recognition. Proc. 
IEEE Int. Conf. Acoust Speech Signal Process (ICASSP 2000) 3, 1351–1354 (2000)
	11.	P.K.  Sahu, A.  Biswas, A.  Bhowmick, M.  Chandra, Auditory ERB like admissible wavelet 
packet features for TIMIT phoneme recognition. Eng. Sci. Technol. Int. J. 17(3), 145–151 
(2014)
	12.	P.K.  Astik Biswas, A.B.  Sahu, M.  Chandra, Feature extraction technique using ERB like 
wavelet sub-band periodic and aperiodic decomposition for TIMIT phoneme recognition. Int. 
J. Speech Technol. 17(4), 389–399 (2014)
	13.	G. Choueiter, J. Glass, An implementation of rational wavelets and filter design for phonetic 
classification. IEEE Trans. Audio Speech Lang. Process. 15(3), 939–948 (2007)
	14.	B. Rehmam, Z. Halim, G. Abbas, T. Muhammad, Artificial neural network- based speech 
recognition using DWT analysis applied on isolated words from oriental language. Malaysian 
J. Comput. Sci. 28(3), 242–262 (2015)
	15.	E. Pavez, J.F. Silva, Analysis and design of Wavelet-Packet Cepstral coefficients for automatic 
speech recognition. Speech Comm. 54(6), 814–835 (2012)
	16.	V.T. Pham, Wavelet analysis for robust speech processing and applications, Ph.D. dissertation, Ph. 
D. thesis, (VDM Verlag, Saarbrücken, Germany, 2007), http://theses.eurasip.org/media/theses/ 
documents/pham-van-tuan-wavelet-analysis-for-robust-speech-processing-and-applications.
pdf
	17.	S. Chang, Y. Kwon, S. Yang, I. Kim, Speech enhancement for nonstationary noise environment 
by adaptive wavelet packet. Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP 2002) 1, 
561–564 (2002)
	18.	H.R. Tohidypour, S.A. Seyyedsalehi, H. Behbood, Comparison between wavelet packet trans-
form, bark wavelet & MFCC for robust speech recognition tasks. Proc. 2nd Int. Conf. Ind. 
Mechatron. Autom. (ICIMA2010) 2, 329–332 (2010)
	19.	Z. Jie, L. Guo-liang, Z. Yu-zheng, L. Xiao-ying, A novel noise-robust speech recognition sys-
tem based on adaptively enhanced bark wavelet MFCC. Proc. Sixth Int Conf Fuzzy Syst. 
Knowl. Discov., 2009 (FSKD '09) 4, 443–447 (2009)
References

46
	20.	H.R. Tohidypour, S.A. Seyyedsalehi, H. Behbood, H. Roshandel, A new representation for 
speech frame recognition based on redundant wavelet filter banks. Speech Comm. 54(2), 
256–271 (2012)
	21.	H.R. Tohidypour, A. Banitalebi-Dehkordi, Speech frame recognition based on less shift sensi-
tive wavelet filter banks. SIViP 10(4), 633–637 (2016)
	22.	S.G. Firooz, F. Almasganj, Y. Shekofteh, Improvement of automatic speech recognition sys-
tems via nonlinear dynamical features evaluated from the recurrence plot of speech signals. 
Comput. Electr. Eng. 58, 215–226 (2017)
	23.	E. Avci, Z.H. Akpolat, Speech recognition using a wavelet packet adaptive network based 
fuzzy inference system. Expert Syst. Appl. 31(3), 495–503 (2006)
	24.	L.D. Vignolo, D.H. Milone, H.L. Rufiner, Genetic wavelet packets for speech recognition. 
Expert Syst. Appl. 40(6), 2350–2359 (2013)
	25.	Y. Shao, C.H. Chang, A generalized time-frequency subtraction method for robust speech 
enhancement based on wavelet filter bank modeling of human auditory system. IEEE Trans. 
Syst. Man Cybern. B Cybern. 37(4), 877–889 (2007)
	26.	S. Yu, C.-H. Chang, Bayesian separation with Sparsity promotion in perceptual wavelet domain 
for speech enhancement and hybrid speech recognition. IEEE Trans. Syst. Man Cybern. Part 
A Syst. Hum. 41(2), 284–293 (2011)
	27.	P.N. Rajeswari, V. Sathyanarayana, Robust speech recognition using wavelet domain front 
end and Hidden Markov Models, in Emerging Research in Electronics, Computer Science 
and Technology. Lecture Notes in Electrical Engineering, ed. by V. Sridhar, H. Sheshadri, 
M. Padma, vol. 248, (Springer, New Delhi, 2014), pp. 435–442
	28.	Y. Wang, Z. Zhao, A noise-robust speech recognition system based on wavelet neural net-
work, in Artificial Intelligence and Computational Intelligence. AICI 2011. Lecture Notes in 
Computer Science, ed. by H. Deng, D. Miao, J. Lei, F. L. Wang, vol. 7004, (Springer, Berlin, 
Heidelberg, 2011), pp. 392–397
	29.	X. Hu, L. Zhan, Y. Xue, W. Zhou, L. Zhang, Spoken arabic digits recognition based on wavelet 
neural networks, in 2011 IEEE International Conference on Systems, Man, and Cybernetics, 
(Anchorage, 2011), pp. 1481–1485
	30.	R. Ejbali, M. Zaied, C. Ben Amar, Wavelet network for recognition system of Arabic word. Int. 
J. Speech Technol. 13(3), 163–174 (2010)
7  Speech Recognition

47
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_8
Chapter 8
Speaker Identification
The recognition of a speaker is quite different from speech recognition since in case 
of speech, the linguistic information is emphasized and speaker’s individual infor-
mation is suppressed. Speaker recognition requires different types of features which 
will contain more speaker-specific information. Specifically, a speaker recognizer 
deals with the problem of identifying the talker from others, while speaker verifica-
tion is concerned with ascertaining the identity of a speaker. Both techniques have 
many useful applications for security. Speaker recognition systems are composed of 
a feature extraction stage and a subsequent classification one. In follows, these 
terms can be used interchangeably since the same features may be used. Feature 
extraction is concerned with extracting the speaker’s characteristics while avoiding 
any sources of adverse variability. The resulting feature vector makes use of infor-
mation from all spectrum bands. Therefore, any inaccuracy of representation and 
any distortion induced in any part of the spectrum may spread to all features.
Speaker identification system is very difficult to be realized due to variations 
associated with differences in individual speaker characteristics, mood variations, 
and intermingled noise disturbances. The recognition accuracy of current systems is 
high under controlled conditions. However, in practical situations, many negative 
factors are encountered including mismatched handsets in training and testing, lim-
ited training data, unbalanced text, background noise, and noncooperative users. 
The techniques of robust feature extraction, feature normalization, model-domain 
compensation, and score normalization methods are necessary.
While MFCC features are used in many speaker recognition systems, however 
one of the weaknesses of MFCC is that the speech signal is assumed to be stationary 
within the given time frame [1]. So far wavelets were employed in successful speech 
recognition systems; the use of wavelet filter banks is a better choice in many works 
for speaker recognition [2]. The classification stage identifies the feature vector 
within a certain class based on the probability density function of the acoustic vec-
tors which is seriously confused in case of impaired features.

48
8.1  Wavelet-Based Features for Speaker Identification
The applicability of nine wavelet functions is evaluated to obtain WPD-based 
­features for speaker verification in [3]. These features are computed such that they 
match the critical bands of human hearing. The experimental results, obtained on 
two well-known speaker recognition databases, show that the Battle-Lemarié wave-
let function is the most advantageous one when compared to the baseline MFCC. 
Additionally, comparative experimental results confirm in [4, 5] that features ext­
racted from WPD matched with critical bands also outperform MFCC on the task of 
speaker verification.
Another work in [6] describes a speaker verification system that includes wavelet 
octave coefficients of linear-prediction residues (WOCOR) as features. The pro-
posed features capture the spectro-temporal source excitation characteristics embed-
ded in the linear predictive residual signal of pitch-synchronous windowing on 
voiced parts of speech. Speaker verification experiments, in that work, reveal an 
equal error rate of 7.67% using the wavelet-based method, in comparison to 9.30% 
of the conventional MFCC-based system on NIST database.
A WP filter structure is fine tuned in [7] to some of the frequency bands which 
are more important for speaker discrimination. This structure does not follow the 
human auditory band structure since it has been obtained through a systematic and 
application-oriented search through a reasonable set of admissible wavelet packet 
(AWP) trees. Next, discrete cosine transform (DCT) is performed on the logarith-
mic AWP subband energies to reduce the dimensionality as indicated in Fig.8.1. The 
study shows improved identification performance compared to other commonly 
used Mel scale based filter structures.
Alternatively [8] makes use of DWT in extracting features for efficient speaker 
recognition. A comprehensive comparison is conducted in [9] on DWP, DWT, and 
DFT for speaker recognition using Gaussian mixture model (GMM) classifier. The 
study reveals that WPD introduces marginal improvement in accuracy with respect 
to the DFT, while WPD mirrors DWT in terms of the order of GMM and can per-
form as well as the DWT under certain conditions.
In another approach, optimized features are obtained based on entropy criterion 
in [10, 11]. Optimum WP features are selected using wavelet packet entropy using 
genetic algorithm in [11] for speaker recognition. Nevertheless, SURE entropy is 
used for adaptive feature selection at the terminal nodes of DWT on speech signal 
in [10] for speaker identification.
Preprocessing
AWP Filter 
bank
Log
Speech Signal
GMM 
classifier
DCT
Speaker ID
Fig. 8.1  AWP-based speaker identification system [7]
8  Speaker Identification

49
8.2  Hybrid Feature Sets for Speaker Identification
A hybrid technique is proposed in [1] for extracting features by combining the 
AM-­FM modulation/demodulation approach with WT analysis. Features are 
extracted from the envelope of the signal and then passed through a wavelet filter 
bank. It is observed from results that the features extracted with envelope of the 
signal are more robust against noise. Experimental results in [1] show that the pro-
posed hybrid method gives better efficiency for speaker identification as compared 
to AM-based method, MFCC, WP, and wavelet filter banks.
Discrete wavelet-Fourier transform (DWFT) can help in the analysis of quasi-­
harmonic signals, like speech signal. The DWFT reveals some spectrum irregularities 
with particular behavior in the time domain. This behavior determines specific proper-
ties of a sound for speaker recognition. Consequently, a hybrid technique is presented 
in [12] combining DWFT and MFCC-based classifiers for speaker recognition. The 
results of such hybrid system outperform that based on MFCC features only.
An approach for speaker feature extraction is presented using formants, wavelet 
entropy, and neural networks (FWENN) in [13]. Five formants and seven WP coef-
ficients based on Shannon entropy are extracted for speaker vowels. The extracted 
features are then used as inputs to feed-forward NN for classification. Experimental 
results show that FWENN achieves a higher classification rate when compared to 
well-known classical algorithms for speaker verification and identification tasks.
In forensic cases, the robustness of a speaker recognition model is very crucial; 
accordingly, sophisticated combination of features has been introduced in [14] 
based on wavelet cepstral coefficient (WCC), i-vector, and cosine distance scoring 
(CDS). The model uses the WCC to transform the speech into spectral features and 
then train, on such features, the i-vectors that represent speech frames having differ-
ent durations.
Nevertheless similar to speech recognition, wavelet-based enhancement of 
speech has been applied toward robust speaker recognition. Adaptive wavelet 
shrinkage method is used in [15] for noise suppression in speech of a specific 
speaker. Wavelet subband coefficient thresholds are computed proportional to the 
noise contamination. Speaker recognition is achieved using modified MFCC of 
overlapped voice signal segments. The method exhibits great robustness in various 
noise conditions. The improvement is significant especially when noise dominates 
the underlying speech.
References
	 1.	V.  Tiwari, J.  Singhai, Wavelet based noise robust features for speaker recognition. Signal 
Process. Int. J. (SPIJ) 5(2), 52–64 (2011)
	 2.	T. Kinnunen, H. Li, An overview of text-independent speaker recognition: from features to 
supervectors. Speech Comm. 52(1), 12–40 (2010)
	 3.	T. Ganchev, M. Siafarikas, I. Mporas, T. Stoyanova, Wavelet basis selection for enhanced 
speech parametrization in speaker verification. Int. J. Speech Technol. 17(1), 27–36 (2014)
References

50
	 4.	T.  Ganchev, M.  Siafarikas, N.  Fakotakis, Speaker Verification Based on Wavelet Packets, 
Lecture Notes in Computer Science, vol LNAI 3206/2004 (Springer, Heidelberg, 2004), 
pp. 299–306
	 5.	M. Siafarikas, T. Ganchev, N. Fakotakis, G. Kokkinakis, Wavelet packet approximation of 
critical bands for speaker verification. Int. J. Speech Technol. 10(4), 197–218 (2007)
	 6.	N. Zheng, T. Lee, P. Ching, Integration of complementary acoustic features for speaker recog-
nition. IEEE Signal Process. Lett. 14(3), 181–184 (2007)
	 7.	S.M. Deshpande, R.S. Holambe, Speaker identification using admissible wavelet packet based 
decomposition. Int. J. Inf. Commun. Eng 6(1), 20–23 (2010)
	 8.	K.D. Returi, Y. Radhika, An artificial neural networks model by using wavelet analysis for 
speaker recognition, in Information Systems Design and Intelligent Applications. Advances 
in Intelligent Systems and Computing, ed. by J.  Mandal, S.  Satapathy, M.  Kumar Sanyal, 
P. Sarkar, A. Mukhopadhyay, vol. 340, (Springer, New Delhi, 2015), pp. 859–874
	 9.	C. Turner, A. Joseph, A wavelet packet and Mel-frequency Cepstral coefficients-based feature 
extraction method for speaker identification. Procedia Comput. Sci. 61, 416–421 (2015)
	10.	D. Avci, An expert system for speaker identification using adaptive wavelet SURE entropy. 
Expert Syst. Appl. 36(3), Part 2, 6295–6300 (2009)
	11.	E. Avci, A new optimum feature extraction and classification method for speaker recognition: 
GWPNN. Expert Syst. Appl. 32(2), 485–498 (2007)
	12.	B.  Ziółko, W.  Kozłowski, M.  Ziółko, R.  Samborski, D.  Sierra, J.  Gałka, Hybrid wavelet-­
Fourier-­HMM speaker recognition. Int. J. Hybrid Inf. Technol. 4(4), 25–42 (2011)
	13.	K. Daqrouq, T.A. Tutunji, Speaker identification using vowels features through a combined 
method of formants, wavelets, and neural network classifiers. Appl. Soft Comput. 27(C), 231–
239 (2015)
	14.	L. Lei, S. Kun, Speaker recognition using wavelet cepstral coefficient, i-vector, and cosine 
distance scoring and its application for forensics. J. Electr. Comput. Eng. 2016, 11 (2016)
	15.	S.M. Govindan, P. Duraisamy, X. Yuan, Adaptive wavelet shrinkage for noise robust speaker 
recognition. Digit. Signal Process. 33, 180–190 (2014)
8  Speaker Identification

51
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_9
Chapter 9
Emotion Recognition from Speech
The emotion detection from human speech has various applications. Detecting 
­emotions can be considered as a classification process of an utterance in terms of a 
predetermined set of emotional conditions, like happiness, anger, fear, or sadness.
9.1  Wavelet-Based Features for Emotion Recognition
A survey on speech emotion recognition in [1] addressed the issues associated with 
such systems. One of these issues is the appropriate features which can achieve such 
recognition. Wavelet-based features are discussed in that work as one of successful 
choices. Subsequently, it becomes apparent after experiments on different features 
responsible for emotion that WPT is emotion specific. The experiments in [2] 
include the comparison of WPT coefficients with a threshold in different bands. In 
another experiment, energy ratios based on WPT were compared in different bands. 
The results are then compared to conventional methods which employs MFCC. 
Based on WPT features, a model is also proposed for emotion conversion, namely, 
neutral-to-angry and neutral-to-happy.
In [3] phenomenal features were proposed based on selective sampling in WP 
domain extending the conventional MFCC. This extension adapts the filter-bank 
structure according to selecting the most crucial frequency bands where the most 
discriminative emotion information is located. This discriminative information is 
captured through comparing different filter-bank structures toward an optimal one. 
The optimal filter-bank structure after the WP tree pruning procedure is then 
included in the feature extraction stage to confine emotion-discriminative informa-
tion. GMM is thereafter employed for emotion classification. Figure 9.1 shows dif-
ferent stages included for training of discriminative bands for recognition. The 
results, then, outperform those of systems employing MFCC. More work in [4, 5] 
has been done based on this model incorporating other wavelet families and model 

52
improvements. One of these improvements is introduced in [6]. In that work, 
­subband energies of WP filter-bank structure are combined with subband spectral 
centroids via a weighting scheme for noise-robust acoustic features in SVM-based 
emotion recognition.
In [3] two different WP filter-bank structures are proposed to extract features 
based on Bark scale and equivalent rectangular bandwidth (ERB) scale for multi-
style classification of speech under stress. A classifier based on linear discriminant 
analysis (LDA) is then used for emotion recognition. The experimental results 
reveal a classification accuracy of more than 90%. Another work in [7] examines 
wavelet packet energy and entropy features applied to Mel, Bark, and ERB scale 
through an HMM classifier. The results show that ERB scale features give better 
performance in comparison with other studied features with recognition accuracy of 
78.75% for acted context and 50.06% for spontaneous context.
Alternatively, in the work in [8], DWT coefficients are used as features, while an 
NN was used for pattern classification. Daubechies type of mother wavelet is used 
for DWT.  Overall recognition accuracies are 72.05%, 66.05%, and 71.25% for 
male, female, and combined male and female databases, respectively. A similar 
work applied DWT with NN for emotion recognition with voice transmitted via 
non-idle channels in [9].
In another work [10], wavelet features are employed in two classifiers: one for a 
video part and another for the audio part of an emotional database. Features are 
extracted using multi-resolution analysis based on the DWT. The dimensionality of 
the obtained wavelet coefficients vector is then reduced. The two feature sets are 
combined, and a fusion algorithm is applied for multiple emotions: happiness, sad-
ness, anger, surprise, fear, and dislike. The recognition accuracies are 98.3% for 
female speakers and 95% for male speakers.
Pre-processing
Post-processing
GMM 
classification
GMM training
Feature extraction 
WP tree pruning
Post-processing
Pre-processing
Feature extraction 
Pruning 
Dataset
Training 
Dataset
Test Dataset
GMM parameters
Emotion label
WP structure
Fig. 9.1  Emotion recognition system with discriminative WP bands
9  Emotion Recognition from Speech

53
9.2  Combined Feature Set for Better Emotion Recognition
Traditional features of speech signal are combined with wavelet-based features for 
enhanced performance of emotion recognition. In [11], WT is used to get emotion-­
specific features using frequency-based decomposition from speech signal. These fea-
tures are then combined with MFCC features and linear prediction cepstral coefficients 
(LPCC) derived from wavelets aiming at capturing proper emotion identification. The 
features dimension is also reduced using the vector quantization method. The reduced 
feature set is classified using a neural network with radial basis function (RBFNN). 
Five emotion classes are studied as angry, fear, happy, disgust, and neutral on different 
databases. The proposed frequency-based decomposition and combination choice of 
features show an improvement in classification with respect to other features combi-
nation in earlier works. Additionally, feature reduction ensures less complexity and 
processing time.
In [12] nonlinear dynamics of WPT are considered for extracting representative 
features for emotion content in speech. In that study the voiced and unvoiced seg-
ments of speech are separated, and then WPT is performed on each segment sepa-
rately. Four nonlinear dynamics (NLD) features are calculated for each subband as 
explained in Fig. 9.2. The difference in the features estimated for both voiced and 
unvoiced segments is based on the fact that features estimated for voiced segments 
are related to perturbation in the fundamental frequency of the excitation source, 
while such features cannot be estimated for unvoiced segments. Thereafter, the deci-
sion on emotion is taken by the combination of the posterior probabilities produced 
by GMM and the universal-background model (UMB). The classification action is 
applied on both voiced and unvoiced feature vectors. The results prove the efficacy 
of WPT-based NLD features in discriminating fear-type emotions. These results are 
compared in [13] versus different types of features with and without WPD.
More wavelet-based features in [13] have been also derived from the WPT and 
different wavelet time-frequency representations such as the bionic wavelet trans-
form and the synchro-squeezed wavelet transform. These features are therefore 
used in classifying high versus low arousal emotions.
Voiced/Unvoiced 
Segmentation
Speech Input
GMM-
UMB
Decision
GMM-
UMB
WPT
Voiced 
Segments
WPT
UnVoiced 
Segments
NLD 
features 
for voiced
NLD 
features 
for 
Unvoiced
Emotion Type
Fig. 9.2  Emotion-type identification using WPT-based NLD features
9.2  Combined Feature Set for Better Emotion Recognition

54
9.3  WNN for Emotion Recognition
As a recognition task, emotion recognition employs in [14] a WNN on features 
relevant to energy, speech rate, pitch, and formants. WNN is then used as the classi-
fier for five emotions including anger, calmness, happiness, sadness, and boredom. 
Compared to the traditional back-propagation (BP) NN, the results of experiments 
show that the WNN has faster convergence speed and higher recognition rate.
Although there are not many works on this approach, all NN-based classifiers 
discussed in previous sections for emotion recognition can benefit from the use of 
wavelets as an activation function. Hence, more works are still needed on the effect 
of diverse varieties in wavelet families when used as an activation function in NN.
References
	 1.	M.  El-Ayadi, M.-S.  Kamel, F.  Karray, Survey on speech emotion recognition: features, 
­classification schemes, and databases. Pattern Recogn. 44(3), 572–587 (2011)
	 2.	V.N. Degaonkar, S.D. Apte, Emotion modeling from speech signal based on wavelet packet 
transform. Int. J. Speech Technol. 16(1), 1–5 (2013)
	 3.	N.A. Johari, M. Hariharan, A. Saidatul, S. Yaacob, Multistyle classification of speech under 
stress using wavelet packet energy and entropy features, in Proceedings of IEEE Conference 
on Sustainable Utilization and Development in Engineering and Technology (STUDENT 
2011), 2011, pp. 74–78
	 4.	H.K. Vydana, P.P. Kumar, K.S.R. Krishna, A.K. Vuppala, Improved emotion recognition using 
GMM-UBMs, in 2015 International Conference on Signal Processing and Communication 
Engineering Systems, Guntur, 2015, pp. 53–57
	 5.	Y. Huang, G. Zhang, Y. Li, A. Wu, Improved emotion recognition with novel task-oriented 
wavelet packet features, in Intelligent Computing Theory. ICIC 2014. Lecture Notes in 
Computer Science, ed. by D. S. Huang, V. Bevilacqua, P. Premaratne, vol. 8588, (Springer, 
Cham, 2014), pp. 706–714
	 6.	Huang, Y., Ao, W., G.  Zhang, Novel sub-band spectral centroid weighted wavelet packet 
­features with importance-weighted support vector machines for robust speech emotion recog-
nition, Wireless Personal Communications (2017), pp. 1–16
	 7.	F.  Chenchah, Z.  Lachiri, Speech emotion recognition in acted and spontaneous context. 
Procedia Comput. Sci. 39, 139–145 (2014)
	 8.	F. Shah, A.R. Sukumar, A.P. Babu, Discrete wavelet transforms and artificial neural networks 
for speech emotion recognition. Int. J. Comput Theory Eng. 2(3), 1793–8201 (2010)
	 9.	D. Campo, O.L. Quintero, M. Bastidas, Multiresolution analysis (discrete wavelet transform) 
through Daubechies family for emotion recognition in speech. J. Phys. Conf. Ser. 705(1), 
012034. IOP Publishing, (2016)
	10.	H. Go, K. Kwak, D. Lee, M. Chun, Emotion recognition from the facial image and speech 
signal. Proc. SICE Annu. Conf. 3, 2890–2895 (2003)
	11.	H.K. Palo, M.N. Mohanty, Wavelet based feature combination for recognition of emotions, Ain 
Shams Eng. J. Available online 28 January 2017., https://doi.org/10.1016/j.asej.2016.11.001
	12.	J.C. Vásquez-Correa, J.R. Orozco-Arroyave, J.D. Arias-Londoño, JF Vargas-Bonilla and Elmar 
Nöth, non-linear dynamics characterization from wavelet packet transform for automatic recog-
nition of emotional speech. Recent Adv. Nonlin. Speech Process. 48, 199–207 (2016)
9  Emotion Recognition from Speech

55
	13.	J.C.  Vásquez-Correa, Emotion recognition from speech with acoustic, non-linear and 
wavelet-­based features extracted in different acoustic conditions, M. Sc. Thesis, University of 
Antioquia, Coulombia, 2016
	14.	Y. Huang, G. Zhang, X. Xu, Speech emotion recognition research based on wavelet neural 
network for robot pet, in Proceedings of the Intelligent computing 5th international con-
ference on Emerging intelligent computing technology and applications (ICIC'09), ed. by 
D.-S. Huang, K.-H. Jo, H.-H. Lee, H.-J. Kang, V. Bevilacqua, (Springer, Berlin/Heidelberg, 
2009), pp. 993–1000
References

57
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_10
Chapter 10
Speech Coding, Synthesis, and Compression
The application of WT in speech coding is studied in [1] for speech analysis, ­coding, 
and synthesis. Optimum wavelets for speech compression are also searched in [1]. 
The study shows that WT concentrates speech energy into bands which differentiate 
between voiced and unvoiced speech. It has been also shown that optimum wavelets 
can be selected based on energy conservation properties in the approximation part 
of the wavelet coefficients. For example, the Battle-Lemarié wavelet concentrates 
more than 97.5% of the signal energy into the approximation part of the coefficients, 
followed closely by Daubechies families. Daubechies family with ten vanishing 
moments preserves perceptual content better than other Daubechies wavelets and 
other orthogonal wavelet families.
10.1  Speech Synthesis
Different wavelet families are evaluated in [2] for speech coding. Eventually, Haar, 
Daubechies, Discrete approximation of Meyer wavelet, and Coiflets are tested for 
coding and decoding (CODEC) of speech signal. The Daubechies wavelet proves its 
superiority over other families for speech coding in terms of compression rate and 
SNR value. The study also shows that the average MOS of wavelet-based speech 
CODEC using such families is in the range of 3.9–4.5 which is suitable for the VoIP 
applications. Experimental results additionally demonstrate that the performance of 
WT-based CODEC remains unaffected with change in language or speakers.
The quality and naturalness of synthesized speech are improved in [3] after add-
ing WT processing to LPC-based synthesizer. WT has been used in voiced/unvoiced 
classification for selecting the appropriate excitation and pitch detection. Wavelet 
coefficients help to adjust the high-frequency components in the synthesized speech. 
On a complementary area, prosody in synthesized speech still lacks naturalness 
especially in conversational scenarios, where speech needs to be more expressive. 

58
As prosody is inherently a suprasegmental property, a dynamic wavelet-based 
decomposition approach is proposed in [4]. The CWT can describe the pitch signal 
for prosody enhancement, continuously, in terms of various transformations of a 
mother wavelet since it is better for synthesis to use a continuous pitch signal as 
described in [5].
In [6] an undecimated wavelet-based speech coding strategy is explained for 
speech synthesis in cochlear implants. The undecimated wavelet-packet transform 
(UWPT) is computed like WPT but without down-sampling at each level. This 
approach gives better SSNR in synthesized speech, while the MOS is reported twice 
in comparison with that of the infinite impulse response (IIR) filter bank.
10.2  Speech Coding and Compression
Speech coding is a major issue in the area of digital processing of speech signal. 
Speech coding is the act of transforming the speech signal to a more compact form, 
which can then be used in signal compression or communication. Speech compres-
sion aims at reducing the required bandwidth for communication or accordingly the 
storage size. Therefore, there is a need to code and compress speech signals. Speech 
compression is required for long-distance communication, high-quality speech stor-
age, and message encryption. Lossy type of speech compression maintains that the 
perceived quality is kept as close as possible to the original quality.
Several techniques of speech coding, such as LPC, waveform coding, and sub-
band coding, have been used since many years for speech synthesis, analysis, and 
compression. However, there are many trials to apply the wavelet analysis for 
speech coding and compression.
In fact, various methods have been developed based on wavelet or wavelet-­
packet analysis for compressing speech signal as in [7–11]. These works also 
include analysis-by-synthesis for speech signal through wavelets [9, 11]. Most of 
such trials exploit the sparseness of speech signal representation in the wavelet 
domain. WT can concentrate speech information into a few neighboring coeffi-
cients. Therefore such coefficients will either be zero or will have negligible magni-
tudes [12, 13]. In [13], the use of WT ensures lower complexity at arbitrary rates 
with acceptable quality when compared to other traditional techniques. The WT is 
also used in compressing residuals of linear prediction (LP) analysis, and it achieved 
good performance as reported in [14, 15].
10.3  Real-Time Implementation of DWT-Based Speech 
Compression
A major issue in the design of a real-time wavelet-based speech coder is choosing 
an optimal wavelet family for analysis. The performance of different wavelet fami-
lies on speech compression is evaluated and compared in [16].
10  Speech Coding, Synthesis, and Compression

59
As it was recommended in [1], the adequate decomposition level for speech 
compression should be less or equal to five, without further advantage in scales 
more than five. So, real-time implementation of DWT-based compression in [17] 
considers only five decomposition levels from an optimal wavelet family chosen 
based on the average energy concentrated in the approximation part of the wavelet 
coefficients. The obtained wavelet coefficients (subbands) are then thresholded for 
data reduction. Different thresholding schemes were also tried in [17]. The reduced 
coefficients are quantized after that in few possible discrete values. Entropy encod-
ing is applied, afterward, to eliminate residual redundancy in WT coefficients result-
ing in the compressed bit stream, as shown in Fig. 10.1. Adding a module for voice 
activity detection prevents from performing DWT during the inactive voice signal. 
Finally, a real-time implementation of the prescribed compression algorithm is 
implemented on a fixed-point digital signal processor (DSP). Eventually, the pro-
posed speech compression algorithm proves low complexity and low bit rate, while 
it achieves high speech coding efficiency.
Another real-time implementation makes use of the field programmable gate 
array (FPGA) exploiting its significant advantages over traditional DSPs such as 
configurable datapath, low cost, and high performance. A FPGA-based design of 
speech compression is presented in [18] where different discrete DWT families 
including the Daubechies and Daubechies Lifting Scheme DWT (DLSDWT) are 
employed. The implementation includes an audio CODEC chip as a front end to 
convert analog speech into a digital format. The digital streams are then compressed 
in real time by using DLSDWT. The compressed data can be decompressed, using 
IDWT, and upsampled. The upsampled signal is thereafter converted to the analog 
format.
References
	 1.	J.I. Agbinya, Discrete wavelet transform techniques in speech processing. TENCON '96. Proc. 
1996 IEEE TENCON. Digit. Signal Process. Appl. Perth, WA 2, 514–519 (1996)
	 2.	M. Ray, M. Chandra, Evaluation of wavelet-based speech codecs for VOIP applications, in 
Proceedings of the International Conference on Nano-Electronics, Circuits & Communication 
Systems, (Springer, Singapore, 2017), pp. 29–37
	 3.	M. Ataş, S. Baykut, T. Akgül, A wavelet-based technique towards a more natural sounding 
synthesized speech, Workshop on Non Linear Speech Processing – NOLISP07, Paris, 22–25 
May 2007
Thresholding &
Encoding coeff.
Entropy 
Encoding
Quantization
Speech signal
Bit stream
DWT
Fig. 10.1  Optimized DWT-based speech compression
References

60
	 4.	M.S.  Ribeiro, O.  Watts, J.  Yamagishi, R.A.J.  Clark, Wavelet-based decomposition of f0 
as a secondary task for DNN-based speech synthesis with multi-task learning, 2016 IEEE 
International Conference on Acoustics (Shanghai, Speech and Signal Processing (ICASSP), 
2016), pp. 5525–5529
	 5.	J. Latorre et al., Continuous F0 in the source-excitation generation for HMM-based TTS: do 
we need voiced/unvoiced classification? 2011 IEEE International Conference on Acoustics 
(Prague, Speech and Signal Processing (ICASSP), 2011) pp. 4724–4727
	 6.	F.  Hajiaghababa, S.  Kermani, H.R.  Marateb, An Undecimated wavelet-based method for 
cochlear implant speech processing. J Med Signals Sens. 4(4), 247–255 (2014)
	 7.	E.B. Fgee, W.J. Philips, W. Robertson, Comparing audio compression using wavelet with other 
audio compression schemes. Proc. IEEE Elect. Comput. Eng. 2, 698–701 (1999)
	 8.	S.  Dusan, J.L.  Flanagan, A.  Karve, M.  Balaraman, Speech compression using polynomial 
approximation. IEEE Trans. Audio Speech Lang. Process. 15(2), 387–397 (2007)
	 9.	S.M.  Joseph, Spoken digit compression using wavelet packet, in IEEE International 
Conference on Signal and Image Processing (ICSIP-2010), 2010, pp. 255–259
	10.	A. Kumar, G.K. Singh, G. Rajesh, K. Ranjeet, The optimized wavelet filters for speech com-
pression. Int. J. Speech Technol. 16(2), 171–179 (2013)
	11.	S. Moriai, I. Hanazaki, Application of the wavelet transform to the low-bit-rate speech coding 
system. Electr. Eng. Japan 148(3), 62–71 (2004)
	12.	S.M. Joseph, P. Babu Anto, Speech compression using wavelet transform, in International 
Conference on Recent Trends in Information Technology (ICRTIT2011), 2011, pp. 754–758
	13.	M.  Abo-Zahhad, A.  Al-Smadi, S.M.  Ahmed, High-quality low-complexity wavelet-based 
compression algorithm for audio signals. Electr. Eng. 86(4), 219–227 (2004)
	14.	M. Deriche, D. Ning, A novel audio coding scheme using warped linear prediction model and 
the discrete wavelet transform. IEEE Trans. Audio Speech Lang. Process. 14(6), 2039–2048 
(2006)
	15.	T. Shimizu, M. Kimoto, H. Yoshimura, N. Isu, K. Sugata, A method of coding LSP residual 
signals using wavelets for speech synthesis. Electr. Eng. Japan 148(3), 54–60 (2004)
	16.	S.  Joseph, P.  Anto, in The Optimal Wavelet for Speech Compression, Communications in 
Computer and Information Science, ed. by A. Abraham et al. ACC 2011, Part III, vol. CCIS 
192, (Springer, 2011), pp. 406–414
	17.	N. Aloui, S. Bousselmi, A. Cherif, Optimized speech compression algorithm based on wave-
lets techniques and its real time implementation on DSP. Int. J. Inf. Technol. Comput. Sci. 
(IJITCS) 7(3), 33–41 (2015)
	18.	J. Pang, S. Chauhan, J.M. Bhlodia, Speech compression FPGA design by using different dis-
crete wavelet transform schemes, in Advances in Electrical and Electronics Engineering – 
IAENG Special Edition of the World Congress on Engineering and Computer Science (San 
Francisco, 2008), pp. 21–29
10  Speech Coding, Synthesis, and Compression

61
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_11
Chapter 11
Speech Quality Assessment
Subjective listening tests are accurate for speech quality assessment, but these tests 
may be slow and expensive. Alternatively, an objective measure is more economic 
to give an immediate and reliable estimate of the anticipated perceptual quality of 
speech. Recent objective measures employ perception-based assessment features. 
An objective measure of perceived speech quality should have two essential compo-
nents: a perceptual transformation and a distance measure. A perceptual transforma-
tion may represent a speech signal as it is perceived through the human hearing 
system. The distance measure estimates the perceived contrast between two speech 
signals. Most of objective measures use STFT as a spectrum estimation tool which 
represents a part of the perceptual transformation of the quality measure. However, 
the loudness perception has nonlinear nature and nonuniform frequency resolution 
for a time-varying speech signal [1]. Accordingly, wavelet analysis can replace the 
conventional spectrum evaluation techniques which are used in such tests. Wavelet 
multiresolution analysis provides an accurate localization in both time and fre-
quency domains. This localization emulates the operation of human auditory sys-
tem. Moreover, wavelet analysis can reduce the computational load and complexity 
associated with the mapping of speech signals into an auditory scale [2].
11.1  Wavelet-Packet Analysis
The WP analysis can provide two merits for an efficient objective quality measure 
[3]. First, the tree structure of WP decomposition may be chosen in a way to closely 
mimic the critical bands of the ear in a psychoacoustic model [4]. Several algo-
rithms have successfully employed time-invariant WP tree structures that mimic the 
frequency resolution properties of the critical bands for perceptual quality assess-
ment of speech [5, 6, 7, 8]. In [6], a predefined structure along the wavelet-packet 

62
tree is proposed to approximate the critical bands of the human hearing. As this 
predefined wavelet-packet (PWP)-based structure represents a filter bank with non-
uniform frequency resolution, it replaces the required filter bank for speech quality 
assessment. In this structure, each tree leaf stands for a filter which gives a band 
energy extracted from the speech signal for the PWP-equivalent critical band.
Table 11.1 shows an example of approximated critical bands and associated 
bandwidths (BW) for PWP filters as defined in [6]. While Fig. 11.1 illustrates how 
the PWP decomposition tree is constructed for a speech frame, every node in the 
Table. 11.1  Approximated critical bands for PWP filters as defined in [6]
Center freq. (Hz)
Approx. center freq.
BW (Hz)
Approx. BW (Hz)
50
62.5
100
125
150
187.5
100
125
250
312.5
100
125
350
437.5
100
125
450
562.5
110
125
570
687.5
120
125
700
812.5
140
125
840
937.5
150
125
1170
1125
190
250
1370
1375
210
250
1600
1625
240
250
1850
1875
280
250
2150
2250
320
500
2500
2750
380
500
0.0
1.0
2.2
2.0
2.1
1.1
2.3
3.0
Level 4
3.1
3.2
3.3
3.8
Level 5
Fig. 11.1  Design of the five levels of PWP decomposition tree
11  Speech Quality Assessment

63
tree is labeled with a number that denotes the layer of the node and the sequential 
number of this node within the layer as in Eq. (3.11). PWP analysis leads to 
­approximated center frequencies of the filters corresponding to the nodes as follows:
Node 0,0: fc = fs/2     Node 1,0: fc = fs/4
Node 1,1: fc = 3fs/4   Node 1,0: fc = fs/4
Node 2,0: fc = fs/8     Node 2,1: fc = 3fs/8
And so on where fc is the center frequency of the filter and fs is the sampling 
frequency.
The choice of the wavelet family and its order of vanishing moments are impor-
tant for the frequency selectivity and the time resolution of the filter bank used  
for analysis. The wavelet filter length should be chosen as small as possible in order 
to guarantee high time resolution and the maximum of possible up- and down-­
switching positions along the signal spectrum. Additionally, the wavelet family 
should have a maximum number of vanishing moments as possible. A compromise 
between the requirement for high-frequency separation between adjacent bands and 
high temporal resolution is turned out by [9].
The energy in each defined frequency band in a signal frame may be considered 
as the features vector which in turn is included in the computation of the distance 
measure. A quality judgment is then taken based on the calculated distance mea-
sures. Yet, another work in [8] employs an NN for predicting the quality measure.
In [10] WP selective coefficients matching the human auditory model are also 
employed within a combined features vector. The other features include the mean, 
variance, skewness, and kurtosis of such coefficients. These features are obtained 
through what is called multiresolution auditory model (MRAM). PCA is then 
applied to reduce the MRAM features vector dimensionality preserving most of 
the total energy of the original features. The MRAM results are used to estimate the 
objective mean opinion scores (MOS). Then such scores are compared to what are 
obtained using traditional features like MFCC and line spectral frequencies (LSF). 
The work finally concluded that a combination of MRAM features, MFCC, and 
LSF feature vectors has a better performance compared to traditional features.
11.2  Discrete Wavelet Transform
The work in [11] uses DWT and then applies MFCC analysis to separate DWT 
details into three levels. As a trial, a cognition model of the perceptual quality is 
proposed in [12] where the wavelet-based bark coherence function (WBCF) com-
putes a coherence function with perceptually weighted speech after wavelet series 
expansion. By using the WBCF, it is possible to alleviate the effects of the variable 
delay of packet-based end-to-end system and the linear distortion caused by the 
analog interface of the communication systems [12].
11.2  Discrete Wavelet Transform

64
References
	 1.	S. Voran, Objective estimation of perceived speech quality – Par I: development of measuring 
normalizing block technique. IEEE Trans. Speech and Audio Process. 7(4), 371–382 (1999)
	 2.	B. Carnero, A. Drgajlo, Perceptual speech coding and enhancement using frame synchronized 
fast wavelet packet transform algorithms. IEEE Trans. Signal Process. 47(6), 1622–1635 
(1999)
	 3.	M. Siafarikas, T. Ganchev, N. Fakotakis, Objective wavelet packet features for speaker verifi-
cation, in Proceedings of Interspeech 2004 – ICSLP, (Jeju, Oct 2004), pp. 2365–2368
	 4.	W. Dobson, J. Yang, K. Smart, F. Guo, High quality low complexity scalable wavelet audio 
coding, in Proceedings of IEEE International Conference Acoustics, Speech, and Signal 
Processing (ICASSP’97), Apr 1997, pp. 327–330
	 5.	A. Karmakar, A. Kumar, R.K. Patney, A multiresolution model of auditory excitation pattern 
and its application to objective evaluation of perceived speech quality. IEEE Trans. Audio 
Speech Lang. Process. 14(6), 1912–1923 (2006)
	 6.	M. Hesham, A predefined wavelet packet for speech quality assessment. J. Eng. Appl. Sci. 
53(5), 637–652 (2006)
	 7.	N.  Upadhyay, A.  Karmakar, A perceptually motivated stationary wavelet packet filterbank 
using improved spectral over-subtraction for enhancement of speech in various noise environ-
ments. Int J Speech Technol. 17(2), 117–132 (2014)
	 8.	Y.F.  Xing, Y.S.  Wang, L.  Shi, H.  Guo, H.  Chen, Sound quality recognition using optimal 
wavelet-­packet transform and artificial neural network methods. Mech. Syst. Signal Process. 
66–67, 875–892 (2016)
	 9.	M. Eme, G. Moschytz, C. Faller, Best wavelet-packet bases for audio coding using perceptual 
and rate-distortion criteria. Proc. IEEE ICASSP’99 2, 909–912 (1999)
	10.	R.K.  Dubey, A.  Kumar, Non-intrusive speech quality assessment using multi-resolution 
­auditory model features for degraded narrowband speech. IET Signal Process. 9(9), 638–646 
(2015)
	11.	F. Rahdari, R. Mousavi, M. Eftekhari, An ensemble learning model for single-ended speech 
quality assessment using multiple-level signal decomposition method, in 2014 4th International 
Conference on Computer and Knowledge Engineering (ICCKE), Mashhad, 2014, pp. 189–193
	12.	S.-W. Park, Y.-C. Park, D. Youn, Speech quality measure for VoIP using wavelet based bark 
coherence function. Proc. INTERSPEECH 2001, 2491–2494 (2001)
11  Speech Quality Assessment

65
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_12
Chapter 12
Scalogram and Nonlinear Analysis of Speech
As a voice source generally interacts with the vocal tract in a nonlinear way, the 
structure of speech sounds is generated by the combined effect of sound sources and 
vocal tract characteristics. Chaotic components are in turn introduced to the speech 
signal due to nonlinear processes occurring during the generation and propagation 
of acoustic waves through voicing system. Among these processes are the interac-
tion of the vocal tract with the nasal tract, the presence of other sources of acoustic 
energy at constrictions within the vocal tract, the differences in vocal fold confi­
gurations during the opening and closing phases of the vibratory cycle, and many 
others interactions [1].
12.1  Wavelet-Based Nonlinear Features
Nonlinear features are obtained in [2] through the calculation of fractal dimension 
(FD) of each band in the three-level DWT as a multiresolution transform. Classi­
fication results show that the FD of DWT details efficiently detains the transient 
behavior of the disordered speech and leads to a successful characterization of the 
signal using SVM classifier into normal and pathological classes. The achieved 
accuracy of the diagnosis is slightly higher than comparable works.
As the phase-based features of a speech signal are neglected in most of the ASR 
systems, the inclusion of such features may improve ASR performance. As the 
reconstructed phase space (RPS) is an essential tool for nonlinear analysis, which 
preserves phase information of a signal, RPS can provide useful information. The 
recurrence plot (RP) of the embedded speech signal in the RPS provides new fea-
tures as described in [3] via applying a two-dimensional WT on the resulting plot. 
These features are examined in an ASR task and achieve significant improvement in 
the classification accuracy of the phoneme recognition accuracy against the usage of 

66
MFCC features. Figure  12.1 shows how nonlinear features are included into a 
speech recognition system.
Another approach is proposed in [4] for emotion recognition based on nonlinear 
features derived from WPT coefficients. The results in that work are comparable to 
those obtained using traditional features indicating that wavelet-based nonlinear 
features are useful to represent the emotional content in speech.
12.2  Wavelet Scalogram Analysis
The wavelet scalogram is a time-scale distribution to approximate spectral properties 
of a signal. It provides a three-dimensional representation of the signal spectrum 
with time represented on one dimension and the frequency, represented in a scale 
parameter, on the other dimension. Gray scales are used to represent the spectral 
amplitude on the third dimension. A simulation study in [5] demonstrates that a 
perceptive scalogram based on WT is very robust to additive white Gaussian noise 
for different languages. Another work in [6] reports the use of scalogram as an 
instantaneous frequency estimator which is important for signals contaminated with 
additive white Gaussian noise.
A scalogram obtained from a complex WT is used in developing a formant track-
ing algorithm in [7]. The formant frequencies are estimated based on constrained 
detection of local maxima in the scalogram representation. The results of the 
­proposed algorithm are compared to those of manually edited ones. The comparison 
Fig.12.1  A schematic of a speech recognition system employing wavelet-based nonlinear 
features
12  Scalogram and Nonlinear Analysis of Speech

67
shows the adequacy of the complex Morlet wavelet in achieving successful tracking 
of the first three formants in a local Arabic database.
On another hand, a speaker verification system is discussed in [8] based on a 
scalogram image of speech utterance from a checked person. Features of a scalo-
gram image are obtained through a wavelet-like filter. The dimensionality features 
extracted from the scalogram are used for training of a GMM. Deciding whether to 
accept or reject the claimed identity relies on the log-likelihood ratio test of the 
speaker features versus the trained GMM. Figure 12.2 illustrates the different stages 
included into such a system.
12.3  Nonlinear and Chaotic Components in Speech Signal
For nonlinear signal analysis, a signal is usually considered chaotic if it shows 
­sensitive dependence on the initial conditions and if it is nonperiodic or it does not 
converge to a periodic orbit in RPS. While the sensitivity condition can be tested 
through the largest Lyapunov exponent (LLE), the periodicity condition may be 
detected through a harmonic analysis [9]. Whereas many measurements in [4, 9, 10] 
are discussed for detecting chaotic activity in a speech signal, the condition on 
periodicity is still questionable. Eventually, the use of wavelets is suitable for the 
analysis of nonstationary periodic signals like speech [11]. Actually, the wavelets 
have been successfully used in the analysis of many chaotic systems [12, 13, 14]. 
Wavelet spectral analysis has been used to measure the nonlinear content of a speech 
signal in [15] through a wavelet scalogram.
The periodicity state may be also studied through scalogram analysis using CWT 
by calculating a scale index [15, 12]. The periodicity analysis of a signal can be con-
ducted through the inner scalogram ξ of a function s at a scale a which is defined by:
	
ξ
ζ
a
W
a b
s
a
( ) =
(
)
( )


,
	
(12.1)
where Ws(a,b) is the continuous wavelet transform of signal s(t) and ζ(a) is the 
maximal subinterval for which the support by a and b is considered.
Framing & 
Segmentation
Wavelet 
Scalogram
Wavelet 
features
Speech Signal
Image
Speaker 
matching
Trained 
Speaker GMM
Decision Accept/Reject 
Claimed speaker features
Fig. 12.2  Scalogram-based speaker verification system
12.3  Nonlinear and Chaotic Components in Speech Signal

68
Since the length of the support interval of wavelets depends on the scale a and 
the scale is continuously varying, it should be normalized to that length ‖ζ(a)‖ as 
follows:
	
ξ
ζ
a
W
a b
a
x
( ) =
(
)
( )
,
	
(12.2)
In this sense, the CWT provides a scalogram with a better resolution than the 
DWT with dyadic scales [16].
In [12], the scale index is used to get information on the degree of nonperiodicity 
of a signal. So, it has been concluded that a signal with details at every scale must 
be nonperiodic. For detecting that a signal is numerically periodic, we have to ana-
lyze its scalogram throughout a relatively wide time range. Additionally, since the 
scalogram of a T-periodic signal vanishes at all 2kT scales, it is sufficient to analyze 
only scales greater than a fundamental scale s0. Thus, a signal which has details at 
an arbitrarily large scale is nonperiodic. The most representative scale of a signal s, 
on a finite scale interval [a0,a1], will be the scale amax for which the scalogram 
reaches its maximum value.
The scale index is then defined in [12] as:
	
i
a
a
scale =
(
)
(
)
ξ
ξ
min
max
	
(12.3)
From this definition, the scale index will be numerically close to zero for peri-
odic signals and close to one for highly nonperiodic signals. In other words, it can 
be considered as a measure of nonperiodicity of a signal.
The periodicity condition for strongly periodic voices, which is the case in speech 
vowels, is tested in [15] through a scale index based on scalogram analysis on a set of 
recordings for Arabic vowels. In that study about 300 frames for each Arabic vowel 
are collected. These frames are extracted sequentially without overlapping along each 
segment which is manually labeled at different instants. The scalogram index is then 
calculated for each frame with wavelet family of Symlets having length of 20 
(“sym20”) and with 20 scales. The Symlets are nearly symmetrical, orthogonal, and 
biorthogonal wavelets proposed by Daubechies as modifications to the Daubechies 
family. The properties of the two wavelet families are similar. When Symlets are 
applied on a signal, the SNR of the reconstructed signal is improved [17, 18]. The 
order of 20 has been chosen for this wavelet to ensure that it is continuously differen-
tiable with compactly supported scaling function This order as well ensure high order 
of vanishing moments. The decomposition into 20 levels also provides finer resolution 
in the obtained scalogram such that it can approximate the critical bands of human 
hearing [19]. Figure 12.3 shows the histogram distribution of the scale index obtained 
in [15] normalized by the maximum for the three Arabic vowels where the abscissa is 
the frame number. Moreover, the vertical dimension is the scale index, while the third 
dimension is for vowel symbol as a category.
12  Scalogram and Nonlinear Analysis of Speech

69
The results in Fig. 12.3 prove the value of scale index for confirming chaotic 
behavior even for highly periodic waveforms like speech vowels. This conclusion is 
valuable since the vowel sounds have strongly periodic behavior which means van-
ishing chaotic content or scale index. Also, Fig. 12.3 exhibits the discrimination 
effect of the scale index for speech vowels. In other words, the scale index would 
exhibit classification ability for Arabic vowels as indicated through the experiments. 
The analysis of both LLE and scale index of Arabic vowels in [15] as well shows a 
correlation in their behavior on the way to confirm chaotic behavior of speech 
vowels.
References
	 1.	A. Esposito, M. Marinaro, Some notes on nonlinearities of speech, lecture notes in computer 
science, nonlinear speech modeling and applications. Spring 3445, 1–14 (2005)
	 2.	Z. Ali, I. Elamvazuthi, M. Alsulaiman, G. Muhammad, Detection of voice pathology using 
fractal dimension in a multiresolution analysis of normal and disordered speech signals. 
J. Med. Syst. 40(1), 20, 10 pages (2016)
	 3.	S.G. Firooz, F. Almasganj, Y. Shekofteh, Improvement of automatic speech recognition sys-
tems via nonlinear dynamical features evaluated from the recurrence plot of speech signals. 
Comput. Electr. Eng. 58, 215–226 (2017)
	 4.	M. Faúndez-Zanuy, S. McLaughlin, A. Esposito, A. Hussain, J. Schoentgen, G. Kubin, W.B. 
Kleijn, P. Maragos, Non-linear speech processing: overview and applications. Control Intell. 
Syst. ACTA Press 30(1), 1–10 (2002)
a
w
y
0
100
200
300
0
0.2
0.4
0.6
0.8
1
Distribution of scale-index with frame number
Frame number
Fig. 12.3  Distribution of the scale index with frame number for Arabic vowels /a/, /w/, and /y/
References

70
	 5.	J.C. Vásquez-Correa, J.R. Orozco-Arroyave, J.D. Arias-Londoño, J.F. Vargas-Bonilla, E. Nöth, 
Non-linear dynamics characterization from wavelet packet transform for automatic recogni-
tion of emotional speech. Recent Adv. Nonlin. Speech Process. 48, 199–207 (2016, Springer, 
Cham)
	 6.	E. Sejdic, I. Djurovic, L. Stankovic, Quantitative performance analysis of scalogram as instan-
taneous frequency estimator. IEEE Trans. Signal Process. 56(8), 3837–3845 (2008)
	 7.	I. Jemaa, K. Ouni, Y. Laprie, S. Ouni, J.-P. Haton, A new automatic formant tracking approach 
based on scalogram maxima detection using complex wavelets, in CEIT  – International 
Conference on Control, Engineering & Information Technology – 2013, Jun 2013, Sousse, 
Tunisia, 2013
	 8.	P. Anju, P. Shanmugapriya, Speaker verification using scalogram and Gaussian mixture model, 
in International Conference on Engineering and Technology, Bofring, 2013, pp. 22–25
	 9.	J.J. Jiang, Y. Zhang, C. McGilligan, Chaos in voice, from modeling to measurement. J. Voice 
20(1), 2–17 (2006)
	10.	G. Vaziri, F. Almasganj, R. Behroozmand, Pathological assessment of patients’speech signals 
using nonlinear dynamical analysis. Comput. Biol. Med. 40, 54–63 (2010)
	11.	Y. Hou, A compactly supported, symmetrical and quasi-orthogonal wavelet. Int. J. Wavelets 
Multiresolution Inf. Process. 8(6), 931–940 (2010)
	12.	R.  Benítez, V.J.  Bolós, M.E.  Ramírez, A wavelet-based tool for studying non-periodicity. 
Comput. Math. Appl. 60(3), 634–641 (2010)
	13.	E. Campos Cantón, J.S. Murguía, Wavelet analysis of chaotic time series. Revista Mexicana de 
Física 52(2), 155–162 (2006)
	14.	G. Chen, S. Hsu, Y. Huang, M. Roque-Sol, The spectrum of chaotic time series (II): wavelet 
analysis. Int. J. Bifurcation Chaos 21(5), 1457–1467 (2011)
	15.	M. Hesham, Wavelet-scalogram based study of non-periodicity in speech signals as a comple-
mentary measure of chaotic content. Int. J Speech Technol 16(3), 353–361 (2013)
	16.	S. Mallat, A Wavelet Tour of Signal Processing (Academic Press, London, 1999)
	17.	M.S.  Chavan, N.  Mastorakis, M.N.  Chavan, M.S.  Gaikwad, Implementation of SYMLET 
wavelets to removal of Gaussian additive noise from speech signal, in Proceeding of 10th 
WSEAS International Conference on Signal Processing, Robotics and Automation, Wisconsin, 
USA, 2011, pp. 37–41
	18.	Y. Long, L. Gang, G. Jun, Selection of the best wavelet base for speech signal, intelligent mul-
timedia, video and speech processing, 2004, in Proceedings of 2004 International Symposium 
on, 20–22 Oct 2004, pp. 218–221
	19.	M. Hesham, A predefined wavelet packet for speech quality assessment. J. Eng. Appl. Sci. 
53(5), 637–652 (2006)
12  Scalogram and Nonlinear Analysis of Speech

71
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_13
Chapter 13
Steganography, Forensics, and Security 
of Speech Signal
There are multiple approaches for secured handling of speech data. One of them 
is  through encryption. Another one hides the signal within another medium. 
Watermarking is used for authentication and protecting copyright via adding indis-
tinguishable key. Steganography, cryptography, and watermarking have the same 
purpose in securing information. Figure 13.1 summarizes the taxonomy of these 
methods. If the information is naturally hidden, forensic analysis should extract 
such data from speech signal.
13.1  Secure Communication of Speech
There are two classical approaches for secure transmission of a speech signal: 
encrypting or hiding the signal (steganography) [1]. In the first case, the speech 
signal is scrambled through a key, while in the second, it is hidden into a host 
medium, and the resulting signal should be highly similar to the host one. The secret 
signal, which may be called stegano signal, is hidden in the redundant portion of the 
cover or host medium [2]. In steganography, no one, apart from the owners of the 
hidden message, realizes its existence.
Common techniques for audio steganography are either in time domain or trans-
form domain. WT as a mathematical transform is preferred because of its multireso-
lution properties which can provide access to both the most significant parts and the 
details of a spectrum.
In [3], the audio signal is transformed into the wavelet domain, and then the 
wavelet coefficients are scaled using the maximum value within all subbands. The 
data bits of the stegano signal are embedded within the least significant bits (LSB) 
of the samples in the host signal. The process might be called LSB substitution. At 
the receiver side, the secret bits are retrieved from these LSB portions. The stegano 
signal can be reconstructed because of the perfect reconstruction properties in the 

72
wavelet filter banks. The use of Haar wavelet, as in [3], shows large advantages, in 
terms of hiding capacity and transparency, compared to other methods.
In [4], the frequency masking in the human auditory system is used in hiding a 
speech signal into another audio content. The wavelet coefficients of the secret mes-
sages are sorted, and indirect LSB substitution is then applied. This approach proves 
to have a higher hiding capacity, and the statistical transparency is higher than that by 
other mechanisms. Moreover, the transparency is not dependent on the host signal 
chosen because the wavelet sorting guarantees the adaptation of the secret message 
to the host signal. Similarly as described in [1], the masking property through wave-
let coefficients helps in achieving real-time hiding of secret speech into another one.
Mixed-excitation linear prediction (MELP) algorithm is used in [5] to code the 
stegano speech into binary parameter bits. The cover speech is then divided into 
voiced and unvoiced frames using auditory WT. For voiced frames, the auditory WT 
is used to detect pitch, and the pitch is utilized to localize the embedding position in 
the cover medium. The information hiding procedure is completed by modifying 
relevant wavelet coefficients. Based on the same pitch detection method, the embed-
ding position is found and the hiding bit is recovered. Thereafter, the stegano speech 
can be retrieved after MELP decoding. The experiments show that the method is 
robust to many attacks such as compression, filtering, and so on.
A different approach is presented in [6], in which, the high-frequency compo-
nents of a secret speech are separated from the low-frequency components using 
the DWT. In a second phase, FT is applied to the high-frequency components to 
get deeper spectral components. Finally, low-pass spectral components of stegano 
speech are hidden into the low-amplitude high-frequency regions of the cover 
speech signal. This method allows the hiding of a large amount of stegano informa-
tion with slight degradation in the quality.
In [7], chaotic logistic mapping is used to encrypt the speech signal. In order to 
hide the characteristics of information signal, the chaotic mapping is applied to the 
WT of secrete speech. The results show that the chaotic encrypting system can 
improve the security and can resist the decipher analysis with less complexity.
DWT is also applied for speech encryption in [8]. The WT coefficients are 
scrambled according to a predefined scheme such that the sampling rate of the 
Speech Data Hiding
Cryptography
Steganography
Watermarking
Visible
Invisible (copyright) 
Fig. 13.1  Taxonomy of speech hiding
13  Steganography, Forensics, and Security of Speech Signal

73
encrypted signal is equal to the original one. The proposed method ensures ­real-­time 
implementation. The framework is then tested to measure the performance and 
complexity. The results show that the proposed method has a better performance 
and lower processing time than the other comparable algorithms for different 
lengths of the encrypted signal.
13.2  Watermarking of Speech
Watermarking of speech is the process of embedding extra information in the host 
speech such that its presence is not remarkable. Watermarking is used for authenti-
cation and protection of copyright. Multiple factors need to be considered during 
the watermarking process such as capacity, imperceptibility, and robustness. While 
the capacity is the amount of data after watermarking, imperceptibility relates to the 
speech quality and robustness means the ability of extraction after several process-
ing on watermarked speech. An additional issue is the security of watermark. Time-­
domain methods for watermarking have been used efficiently, and they have simple 
implementation, whereas methods depending on mathematical transformation have 
the advantage of high robustness. DWT is one of the widely used transforms for 
watermarking of speech [9].
An approach to speech watermarking hides speech data in LSB of the wavelet 
coefficients, but the imperceptibility of hidden data becomes low. As well, integer 
quan­tization of wavelet coefficients can speed up the process of data hiding. How­
ever signal extraction is not always accurate. The effect of quantization may be 
reduced through applying a hearing threshold [10].
Another approach for digital watermarking of speech is introduced in [11] based 
on eigenvalue quantization in the transform domain of DWT. In this method, SVD 
is applied to DWT approximations of each frame in the speech signal. The obtained 
eigenvalues from SVD are then quantized and embedded into such approximations. 
Results of that approach prove its robustness against different attacks such as filter-
ing, additive noise, resampling, and cropping. The work also shows that embedding 
the watermark within detail coefficients of DWT can improve the imperceptibility 
but degrades the robustness. A similar approach has been followed for watermark-
ing of audio signals in [9].
13.3  Watermarking in Sparse Representation
As audio signals are usually produced by resonant-like systems, such sounds have a 
limited number of frequency components. This allows a sparse representation of the 
signal in the frequency domain obtained by WT. Hidden data can be readily embed-
ded in such sparse representation for audio steganography.
13.3  Watermarking in Sparse Representation

74
Sparse representation of a signal s can be expressed as follows:
	
s
d
D
k
m
k
k
T
≅
=
≡
……
(
)
=∑
1
α
α
α
α
α
1
m
, 
,
..
	
(13.1)
where the signal snx1 has been expressed approximately as a linear combination of m 
predefined atoms of what is called the learned dictionary D with the atoms dk are the 
columns in Dnxm. Dictionary learning algorithms search for D which satisfies a spar-
sity constraint on the nth norm of each atom as ‖α‖n<<m.
In [12] ℓ0 norm is used within an algorithm adapted to audio signals for sparse 
representation. The proposed algorithm for speech steganography makes use of the 
sparse representation on signal’s WT coefficients to embed secret messages, result-
ing in increased undetectability. The proposed method also improves both the 
resulting signal quality and the embedding capacity. Experimental results show that 
secret data are perceptually indistinguishable from the original cover signal as 
­measured by objective quality measures. The results are compared with other steg-
anography algorithms and confirm better performance in terms of imperceptibility, 
undetectability, and capacity.
13.4  Forensic Analysis of Speech
Forensic voice is considered as a branch within forensic sciences. Subdisciplines of 
forensic voice meet the relevant needs of criminal justice and intelligence agencies. 
For instance, speech may be analyzed to determine the authorship, the intent of an 
individual, deception, emotion, and so on. The analysis is mainly concerned with 
speech decoding and speaker’s voice recognition [13]. Speaker recognition requires 
a different type of features which will contain more speaker-specific information. 
Specifically, speaker recognition deals with the problem of identifying the talker 
from others, while speaker verification is concerned with ascertaining the identity of 
a speaker.
Speaker recognition in forensic cases is highly affected by what is called recording 
session variability. This variability in terms of mismatched environment, critically, 
degrades the identification performance. Examples of environmental mismatching 
may be ambient noise, variable channel, and different duration of speech data [14]. 
Another issue is the variation of different utterances from the same speaker typically 
due to factors such as transmission channel, speaking style, or the speaker emotional 
state. Accordingly, the design of a robust speaker identification system is a challenge 
for obtaining forensic evidence. This challenge has been discussed in [15, 16] and in 
terms of new features in [17, 10].
To identify a speaker’s voice, the employed features should contain more 
speaker-specific information. These features may span most of the frequency ranges 
since the information of the glottis is mainly encoded in a low-frequency band 
13  Steganography, Forensics, and Security of Speech Signal

75
(between 100 and 400 Hz), the information of the piriform fossa in a high-frequency 
band (between 4 and 5 kHz), while the constriction of the consonants may be in a 
higher-frequency region around 7.5 kHz. The role of different frequency regions for 
speaker recognition is extensively investigated in [17]. The contribution of the dif-
ferent frequency bands can be explored with more speaker-specific information 
through either assigning weighting coefficients for different frequency bands or 
changing frequency resolutions. A multiresolution filter bank is proposed in [17] 
using WPD with nonuniform subbands for evaluating the contribution of every 
band. Experimental results demonstrate the value of higher-frequency components 
as speaker-specific information which improves the speaker identification rate with 
remarkable robustness in [17].
An enhanced speaker recognition model is proposed in [18] based on wavelet 
cepstral coefficients (WCC). This model employs the WCC to map the speech sig-
nal into spectral features which are used to train the i-vectors that efficiently 
describes the signal with different durations. The i-vectors are then compared to 
confirm the speaker identity. In general, WT analysis serves well in all issues related 
to forensic voice which have been already discussed in [13].
Source identification of the speech signal is studied in [19] for forensic investiga-
tion. Voice features are obtained using uniform WPD and the Teager energy opera-
tor to identify the recorder. The results reveal that the proposed system can effectively 
identify the correct model and brand of the source recorder. The work also show that 
a new feature vector can be composed of traditional speech features with subband 
Teager energy cepstral parameters together with short time frame energy. The pro-
posed feature vector improves the identification accuracy under both silent and 
noisy recording conditions.
References
	 1.	D.M. Ballesteros, J.M. Moreno, Real-time, speech-in-speech hiding scheme based on least 
significant bit substitution and adaptive key. Comput. Electr. Eng. 39(4). Available online 5 
March (2013)
	 2.	J.  Antony, C.  Sobin, A.  Sherly, Audio steganography in wavelet domain  – A survey. Int. 
J. Comput. Appl. 52(13), 33–37 (2012)
	 3.	N. Cvejic, T. Seppanen, A wavelet domain LSB insertion algorithm for high capacity audio 
steganography, in Proceedings of 2002 IEEE 10th Digital Signal Processing Workshop, 2002 
and the IEEE 2nd Signal Processing Education Workshop, 2002
	 4.	D.M. Ballesteros, J.M. Moreno, Highly transparent steganography model of speech signals 
using efficient wavelet masking. Expert Syst. Appl. 39(10), 9141–9149 (2012)
	 5.	L. Shen, X. Li, H. Wang, R. Zhang, Speech hiding based on auditory wavelet, in Proceedings 
of International Conference on Computational Science and Its Applications – ICCSA 2004, 
Part IV, Assisi, Italy, 2004, pp. 414–420
	 6.	R. Siwar, G. Driss, S. Sid-Ahmed, H. Habib, Speech Steganography using wavelet and Fourier 
transforms. EURASIP J. Audio Speech Music Process. 2012(20), 1–14 (2012)
	 7.	D. Que, L. Lu, H. Wang, Y. Ding, A digital voice secure communication system based on 
logistic-mapping in wavelet domain chaotic modulation. 7th Int. Conf Signal Process. 2004, 
ICSP '04 3, 2397–2400 (2004)
References

76
	 8.	A.  Azzazi, A real time secured voice framework using the totally reconstructed discrete 
­wavelet transformation. Br. J. Math. Comput. Sci. 5(2), 190–203 (2014)
	 9.	A. Al-Haj, An imperceptible and robust audio watermarking algorithm. EURASIP J. Audio 
Speech Music Process. 2014(1), 37 (2014)
	10.	V.  Balakrishnan, N.  Borges, L.  Parchment, Wavelet Denoising and Speech Enhancement 
(Department of Electrical and Computer Engineering, The Johns Hopkins University, 
Baltimore, Spring 2006)
	11.	S.A.R.  Mohammad Ali Nematollahi, F.Z.  Al-Haddad, Blind digital speech watermarking 
based on Eigen-value quantization in DWT. J. King Saud University – Comput. Inf. Sci. 27(1), 
(2015)
	12.	S.  Ahani, S.  Ghaemmaghami, Z.J.  Wang, A sparse representation-based wavelet domain 
speech Steganography method. IEEE/ACM Trans Audio Speech Lang. Process. 23(1), 80–91 
(2015)
	13.	H. Hollien, R.H. Bahr, J.D. Harnsberger, Issues in forensic voice. J. Voice 28(2), 170–184 
(2014)
	14.	L.A. Khan, F. Iqbal, M.S. Baig, Speaker verification from partially encrypted compressed 
speech for forensic investigation. Digit. Investig. 7(1–2), 74–80 (2010)
	15.	P. Kenny, G. Boulianne, P. Ouellet, P. Dumouchel, Speaker and session variability in GMM-­
based speaker verification. IEEE Trans Audio Speech Signal Process. 15(4), 1448–1460 
(2007)
	16.	R.  Vogt, S.  Sridharan, Explicit modelling of session variability for speaker verification. 
Comput. Speech Lang. 22(1), 17–38 (2007)
	17.	R.S. Holambe, M.S. Deshpande, Noise robust speaker identification: using nonlinear modeling 
techniques, in Forensic speaker recognition: law enforcement and counter-terrorism, ed. by 
H. Patil, A. Neustein, (Springer, New York, 2012), pp. 153–182
	18.	L. Lei, S. Kun, Speaker recognition using wavelet cepstral coefficient, i-vector, and cosine 
distance scoring and its application for forensics. J. Electr. Comput. Eng. 2016, 11 (2016)
	19.	Ömer Eskidere, Source digital voice recorder identification by wavelet analysis. Int. J Artif. 
Intell. Tools 25(3), 1–19 (2016)
13  Steganography, Forensics, and Security of Speech Signal

77
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5_14
Chapter 14
Clinical Diagnosis and Assessment of Speech 
Pathology
Clinical diagnostic symptoms of pathological speech include hypernasality, breathiness, 
diplophonia, audible nasal emissions, short phrases, pitch breaks, monopitch, 
monoloudness, reduced loudness, harshness, low pitch, slow rate, short rushes of 
speech and reduced stress, hypotonicity, atrophy, facial myokymia, fasciculation, 
occasional nasal regurgitation, dysphagia, and drooling. Additionally, impairments 
in the vocal folds can result in disorders in expressing words. Most of these disor-
ders can be analyzed and graded from audio recordings of speech [1, 2]. In such a 
pathological speech signal, strong high-frequency components can be detected as 
abnormal noise through spectral analysis and may be separated within a wavelet 
spectrum [3]. In other words, inspecting WT coefficients for a normal voice signal 
and a pathological one shows that the amplitude of wavelet coefficients for patho-
logical signals is considerably decreased and disturbed [3]. Accordingly, CWT- and 
wavelet-like transforms provide more efficient features for diagnosis [4]. The CWT-­
based analysis yields sharp and well-defined visual patterns which facilitate the 
diagnosis of speech disorders as discussed in [5].
In case of vocal fold-related voice disorders, there are common attributes like 
irregular distribution in the spectrum, loss of periodicity, and decrease in the inten-
sity of speech waveform. Energy distribution has strong irregularity under the dif-
ferent subbands for pathological speech when compared to normal ones due to the 
presence of noise-like elements. Therefore, WPT is suitable for judging such disor-
ders because of its fractal shape and its ability to discriminate patterns. The work in 
[3] compared WPT performance with STFT and CWT for discriminating normal 
voices and that with vocal fold-related voice disorder.
An extensive comparison is presented in [3] on different WPT structures for 
identification of different voice disorders due to problems in the vocal folds. The 
performance of each WPT structure is evaluated in terms of the accuracy, sensitiv-
ity, specificity, and area under the receiver operating curve. Eventually, entropy fea-
tures in the sixth level of WPT decomposition along with feature dimension 
reduction followed by an SVM-based classification give the best results. WPT-based 

78
results show better performance than other works in both perspectives of accuracy 
and reduction of residues. This leads to full accuracy and high-speed diagnosis 
procedure.
In [6] an objective evaluation is proposed for the diagnosis of speech dysfluen-
cies and stuttering based on the WPT with sample entropy features. Dysfluent 
speech signals are decomposed into six levels using WPT. Sample entropy features 
are extracted at every level of decomposition and then fed to a classifier. Experimental 
results show that the WPT-based features affirm very promising classification accu-
racy. A study is also explained in [6] showing the effect of different wavelet families 
for better classification performance.
Another algorithm to discriminate voice disorders is introduced in [7] using 
adaptive growth of WPT. A Mel-scaled WPT provides additional robust features for 
automatic voice disorder diagnosis as illustrated in [8, 9]. A further set of features is 
based on WPT and SVD for the detection of vocal fold pathology in [2]. The exp­
erimental results show very promising classification accuracy with WPT-based 
features.
As the voice source generally interacts with the vocal tract in a nonlinear way, an 
MRA is applied on speech signal in [10] for detecting irregular vibration in vocal 
folds through a noninvasive way. Nonlinear assumption is implemented through 
calculating the fractal dimension (FD) of each band in three-level DWT as a multi-
resolution transform. Figure  14.1 shows a schematic of implementation of that 
approach with SVM classifier. Experimental results in [10] show that the FD of 
DWT details efficiently detains the transient behavior of the disordered speech and 
leads to a successful characterization of the signal into normal and pathological 
classes. The achieved diagnosis accuracy is slightly higher than that reported in 
relevant works.
Wavelet-based orthogonal filter banks are implemented using a more efficient 
lattice structure in [11]. The lattice structure can parameterize the filter bank into 
fewer parameters to distinguish between normal and disordered voices. Optimum 
features are then searched within such parameters using a genetic algorithm with a 
fitness function corresponding to the classification error. Next, these optimized fea-
tures are fed to an SVM classifier achieving better classification rate as described in 
[11]. The work in [11] defines a filter bank using DWT to assess the pathological 
voice. The filter bank is represented by a polyphase matrix constructed as a product 
of simple matrices seeking a mother wavelet which achieves the best classification 
Diagnosis
DWT
Speech Signal
SVM
classifier
Details of level 1
Details of level 3
Details of level 2
Residual details
FD
FD
FD
FD
Fig. 14.1  Diagnosis of vocal fold irregular vibration using nonlinear fractal dimension of DWT
14  Clinical Diagnosis and Assessment of Speech Pathology

79
of voice quality. Accordingly, the structure of each filter is optimized subject to 
maximum classification rate.
The characteristics of normal and pathological voices are also separated in [12] 
through energy and Shannon entropy extracted from the coefficients in the output 
nodes of the best WPD with eight decomposition levels. The obtained results from 
the confusion matrix and cross-validation tests prove that the proposed system is 
capable of significant classification improvement. This improvement is character-
ized with lower complexity specifically for early detection of laryngeal pathology 
and assessment of vocal fold improvement following voice therapy. A subsequent 
work in [13] applies new features for diagnosis of laryngeal pathology. These fea-
tures are extracted from residues of LP analysis on DWT coefficients. Experimental 
results point out that these features are highly reliable especially for assessing voice 
quality and the effectiveness of the prescribed rehabilitation medicine.
As the multiresolution sinusoidal transform coding (MRSTC) method is a 
wavelet-­like analysis with sinusoidal bases, MRSTC coefficients are used in [14] 
for evaluating voice quality degradation associated with neck and laryngeal cancer. 
The use of MRSTC features achieves significant improvement in error rate and 
accuracy of diagnosis.
For pitch detection, normal speech estimators are applied to pathological speech 
in [15]. A comparison is conducted between autocorrelation, harmonic product 
spectrum, and wavelet methods. The assessment accuracy of roughness, hoarseness, 
and breathiness is the best with wavelets [1].
References
	 1.	L. Baghai-Ravary, W. Steve, Beet, Automatic Speech Signal Analysis for Clinical Diagnosis 
and Assessment of Speech Disorders, (SpringerBriefs in Electrical and Computer Engineering / 
SpringerBriefs in Speech Technology) (Springer, New York, 2013)
	 2.	M. Hariharan, K. Polat, S. Yaacob, A new feature constituting approach to detection of vocal 
fold pathology. Int. J. Systs. Sci. 45(8), (2014). www.tandfonline, 14-5-3013
	 3.	M. Arjmandi, M. Pooyan, An optimum algorithm in pathological voice quality assessment 
using wavelet-packet-based features, linear discriminant analysis and support vector machine. 
Biomed. Signal Process. Control 7(1), 3–19 (2012)
	 4.	P. Kukharchik, D. Martynov, I. Kheidorov, O. Kotov, Vocal fold pathology detection using 
modified wavelet-like features and support vector machines, in 15th European Signal 
Processing Conference (EUSIPCO 2007), 2007, pp. 2214–2218
	 5.	B.  Ziółko, W.  Kozłowski, M.  Ziółko, R.  Samborski, D.  Sierra, J.  Gałka, Hybrid wavelet-­
Fourier-­HMM speaker recognition. Int. J. Hybrid Inf. Technol. 4(4), 25–42 (2011)
	 6.	M. Hariharan, C.Y. Fook, R. Sindhu, A.H. Adom, S. Yaacob, Objective evaluation of speech 
dysfluencies using wavelet packet transform with sample entropy. Digi. Signal Process. 23(3), 
952–959 (2013)
	 7.	P.T. Hosseini, F. Almasganj, T. Emami, R. Behroozmand, S. Gharibzade, F. Torabinezhad, 
Local discriminant wavelet packet basis for voice pathology classification, in The 2nd 
International Conference on Bioinformatics and Biomedical Engineering, 2008. ICBBE, 
2008, pp. 2052–2055
References

80
	 8.	P. Murugesapandian, S. Yaacob, M. Hariharan, Feature extraction based on Mel-scaled ­wavelet 
packet transform for the diagnosis of voice disorders. 4th Kuala Lumpur Int. Conf. Biomed. 
Eng. 2008, BIOMED 2008 21, 790–793 (2008)
	 9.	M.P. Paulraj, Y. Sazali, M. Hariharan, Diagnosis of voice disorders using Mel scaled WPT and 
functional link neural network, biomedical soft computing and human. Sciences 14(2), 55–60 
(2009)
	10.	Z. Ali, I. Elamvazuthi, M. Alsulaiman, G. Muhammad, Detection of voice pathology using 
fractal dimension in a multiresolution analysis of normal and disordered speech signals. 
J. Med. Syst. 40(1), 20, 10 pages (2016)
	11.	N.E. Saeedi, F. Almasganj, F. Torabinejad, Support vector wavelet adaptation for pathological 
voice assessment. Comput. Biol. Med. 41(9), 822–828 (2011)
	12.	A. Akbari, M.K. Arjmandi, An efficient voice pathology classification scheme based on apply-
ing multi-layer linear discriminant analysis to wavelet packet-based features. Biomed. Signal 
Process. Control 10, 209–223 (2014)
	13.	A. Akbari, M.K. Arjmandi, Employing linear prediction residual signal of wavelet sub-bands 
in automatic detection of laryngeal pathology. Biomed. Signal Process Control 18, 293–302 
(2015)
	14.	J.C. Kim, H. Rao, M.A. Clements, Speech intelligibility estimation using multi-resolution 
spectral features for speakers undergoing cancer treatment. J. Acoust. Soc. Am. Express Lett. 
136(4), EL315–EL321 (2014)
	15.	C. Llerena, L. Alvarez, D. Ayllon, Pitch detection in pathological voices driven by three tailored 
classical pitch detection algorithms, Recent Advances in Signal Processing, in Computational 
Geometry and System Theory: Proc ISCGAV ‘11 and ISTASC ‘11, 2011, pp. 113–118
14  Clinical Diagnosis and Assessment of Speech Pathology

81
© The Author(s) 2018 
M.H. Farouk, Application of Wavelets in Speech Processing,  
SpringerBriefs in Electrical and Computer Engineering,  
https://doi.org/10.1007/978-3-319-69002-5
A
Adaptive network based fuzzy inference 
system (ANFIS), 43
Admissible wavelet packet (AWP) trees, 48
Air-filled lungs, 5
AM-based method, 49
AM-FM modulation/demodulation approach, 
49
Applications in speech technology, 2
Arabic digits, 44
Area function, 7
Artificial neural network (ANN), 31
Audio coding, 59
Audio recordings of speech, 77
Audio signal, 71
Audio steganography, 71
Automatic formant tracking, 24
Automatic speech recognition (ASR)
classification/recognition, 41
DWT and WPD, 41
filter-bank selection, 43
hybrid enhancement, 44
MFCC features, 44
neural networks, 44
noise robustness, 44
performance, 43
stage and feature extraction, 44
systems, 3, 65
wavelet features, 42
WP features, 43
WT feature extraction, 44
AWP-based speaker identification system, 48
B
Back-propagation (BP) NN, 54
Bandwidths (BW), 62
Bark scale, 43, 52
Bark wavelet, 43
Battle-Lemarié wavelet concentrates, 57
Battle-Lemarié wavelet function, 48
Bayesian scheme, 43
BayesShrink, 37
Biased risk thresholding, 37
C
Candidate pitches, 26
Chaotic components in speech signal, 67–69
Chaotic logistic mapping, 72
Classification error, 78
Coding-decoding (CODEC) of speech, 57, 59
Communication systems, 63
Confusion matrix, 79
Continuous wavelet transform (CWT), 19, 43, 
58, 67, 68
Cosine distance scoring (CDS), 49
Critical-band (CB), 8, 31
Critical-band wavelet decomposition, 37
Cross-validation tests, 79
D
Data hiding, 73
Daubechies and Daubechies Lifting Scheme 
DWT (DLSDWT), 59
Index

82
Daubechies family, 68
Daubechies wavelets, 16, 57
DFT, 48
Diagnose vocal pathologies
audio recordings of speech, 77
characteristics of normal and pathological 
voices, 79
clinical symptoms, 77
CWT, 77
dysfluent speech signals, 78
energy distribution, 77
FD, 78
Mel-scaled WPT, 78
MRSTC method, 79
nonlinear fractal dimension of DWT, 78
objective evaluation, 78
optimum features, 78
pitch detection, 79
speech signal, 77
STFT, 77
strong high-frequency components, 77
SVM-based classification, 77
SVM classifier, 78
voice disorders, 77, 78
voice source, 78
wavelet families, 78
Wavelet-based orthogonal filter banks, 78
WPT, 77
WT coefficients, 77
Digital signal processing, 3
Digital signal processor (DSP), 59
Digital watermarking, 73
Discrete cosine transform (DCT), 48
Discrete wavelet-Fourier transform  
(DWFT), 49
Discrete wavelet transform (DWT), 14, 18, 
23, 25, 26, 48, 52, 63, 65, 72
Discrete-time domain, 14
Dual-tree complex WT, 41
DWP, 48
DWT-based speech compression, 58–59
Dysfluent speech signals, 78
E
Emotion recognition, 66
classification, 51, 53
detecting, 51
frequency-based decomposition, 53
LPCC, 53
MFCC, 53
NLD features, 53
RBFNN, 53
speech signal, 53
synchro-squeezed wavelet transform, 53
UMB, 53
voiced and unvoiced segments, 53
wavelet-based features, 51–52
WNN, 54
WPT, 53
WT, 53
Emotional database, 52
Empirical wavelets, 19–20
Empirical wavelet transform (EWT), 19
Encryption, 71, 72
Energy distribution, 77
Enhanced speaker recognition model, 75
Equivalent rectangular bandwidth (ERB) 
scale, 52
Estimating mixing matrix, 32
European Telecommunication Standards 
Institution (ETSI) adaptive 
multi-rate (AMR) narrow-band 
(NB) (ETSI AMR-NB), 29
Event-based detection of pitch, 26
Expectation maximization (EM) algorithm, 32
F
Fast Fourier transform (FFT), 18
Field programmable gate array (FPGA), 59
Filter-bank structure, 51
Forensic analysis, 74–75
Forensic sciences, 74
Forensic voice, 74
Formant estimation, 24, 25
Formant frequencies, 66
Formant model, 8
Formant tracking, 24, 25, 66
Formants, 49
Formants, wavelet entropy and neural 
networks (FWENN), 49
Fourier-based algorithms, 3
Fourier-based spectrogram, 24
Fourier transform (FT), 11
FPGA-based design, 59
Fractal dimension (FD), 65, 78
Frequency-based decomposition, 53
Fundamental frequency estimation, 25, 26
G
Gamma-tone wavelet, 42
Gaussian mixture model (GMM),  
48, 51, 53, 67
Gaussian noise, 66
Index

83
Genetic algorithm, 48, 78
Gray scales, 66
H
Haar scaling, 16
Harmonic analysis, 67
Heisenberg uncertainty principle, 12, 13
Hidden data, 73
Hidden Markov model (HMM), 44
Hiding a speech signal, 72
Higher-frequency components, 75
High-frequency band, 75
Human auditory system, 8, 9, 72
Hybrid technique, speaker identification, 49
I
IDWT, 59
Independent component analysis (ICA), 31, 32
i-vectors, 75
L
Laplacian mixture model (LMM), 32
Largest Lyapunov exponent (LLE), 67, 69
Learned dictionary, 74
Least significant bits (LSB), 71
Line spectral frequencies (LSF), 63
Linear discriminant analysis (LDA), 52
Linear prediction (LP) analysis, 58
Linear prediction cepstral coefficients 
(LPCC), 53
Linear prediction (LP) model, 8
Linear predictive code (LPC), 23, 25, 58
Low-frequency band, 74
LPC-based synthesizer, 57
LSB substitution, 71
M
Mean opinion scores (MOS), 63
MEL scale, 42
Mel-frequency cepstral coefficients (MFCC), 
23, 42–44, 47–49, 51, 53
analysis, 63
features, 66
Mel-scaled WPT, 78
Mel scale based filter structures, 48
Meyer wavelets, 16, 57
Mixed-excitation linear prediction (MELP) 
algorithm, 72
Morlet wavelet, 67
Mother wavelet, 13, 52
Multiresolution analysis (MRA), 11–14, 52
Multiresolution auditory model (MRAM), 63
Multiresolution filter bank, 75
Multiresolution sinusoidal transform coding 
(MRSTC) method, 79
Multitaper spectrum (MTS), 38, 39
N
Nasalized sounds, 7
Nasal tract, 7
Negative selection algorithm, 41
Neural network (NN), 29, 44
Neural network with radial basis function 
(RBFNN), 53
NMF-estimated noise, 38
Noise cancellation, 41–42
Noise suppression, 35
Nonlinear components in speech signal, 67–69
Nonlinear dynamics (NLD) features, 53
Nonlinear features, 65–66
Nonlinear signal analysis, 67
Nonnegative matrix factorization (NMF), 38
Non-orthogonal wavelet decomposition, 43
Nonuniform subbands, 75
O
Optimizing wavelet parameters, 42
Optimum wavelets, 57
Optimum WP features, 48
P
Packet-based end-to-end system, 63
Peak detection approach, 26
Perception-based approach, 9
Perceptual quality measures, 9
Perceptual wavelet filter bank (PWF), 37
Perceptual wavelet packet decomposition 
(PWPD), 38
Phase-based features, 65
Pitch detection, 25, 26, 79
Pitch estimation, 23, 25–27
Predefined wavelet-packet (PWP)
analysis, 63
decomposition tree, 17, 62
equivalent critical band, 62
filter bank, 62
Psychoacoustic models, 9, 42
Q
Quasi-continuous WT, 18
Index

84
R
Real-time implementation of DWT-based 
speech compression, 58, 59
Reconstructed phase space (RPS), 65
Recording session variability, 74
Recurrence plot (RP), 65
Redundant wavelet filter banks (RWFB), 43
Redundant WT, 18
Resonant-like systems, 73
S
Scale index, 67–69
Scale parameter, 66
Scalogram, 19, 23, 24
Scalogram analysis, 66–67
Scalogram-based speaker verification 
system, 67
Secret signal, 71
Secret speech, 72
Secure communication of speech, 71–73
Segmental SNR (SSNR), 9
Segmentation of speech, 30
Self-organizing map (SOM), 31
Semi-discrete WT, 13
Semisoft mask filter, 38
Semisoft shrinking, 37
Shannon entropy, 49, 79
Short-time Fourier transform (STFT), 3, 
11–13, 23
Signal enhancement, 41–42
Signal-to-noise ratio (SNR), 9, 29
Sophisticated thresholding, 37
Sound source generation, 7
Source-filter model, 5
Source separation, 31–33
Sparse representation, 73–74
Speaker identification
characteristics, 47
hybrid feature sets, 49
MFCC features, 47
recognition accuracy, 47
speaker recognition, 47
speaker verification, 47
speech recognition, 47
wavelet-based features, 48
Speaker recognition, 47–49, 74
Speaker-specific information, 74, 75
Speaker’s voice, 74
Speaker verification, 47–49, 67
Spectral analysis, 8, 23, 24
Spectral-domain (nonevent)-based pitch 
detectors, 25, 26
Spectrogram, 23
Speech analysis, 57
Speech coding, 58
Speech compression
and coding, 58
optimum wavelets, 57
real-time implementation of DWT-based, 
58, 59
Speech detection, 29
Speech enhancement
MTS, 38, 39
noise suppression, 35
noisy speech signal, 35
process, 35, 36
quality and intelligibility of speech 
signal, 35
speech sentences, 35
thresholding (shrink) approach, 36
thresholding schemes, 36, 37
wavelet thresholding, 35
WT, 35
Speech hiding, 71, 72
Speech perception, 8, 9
Speech processing
algorithms, 3
applications, 2
ASR system, 3
computation and communication 
systems, 3
definition, 1–2
history, 1–2
objective of research, 3
recognition concentrates, 2
recordings, 2
wavelet analysis, 3
WT, 1, 3
Speech production
intelligibility and quality, 9
modeling, 7–8
process, 5, 6
sounds, 6–7
source-filter model, 5
vocal organs, 5
vocal tract, 5, 6
Speech quality, 9
Speech quality assessment
DWT, 63
objective measure, 61
perception-based assessment features, 61
perceptual transformation and distance 
measure, 61
wavelet analysis, 61
WP analysis, 61–63
Speech recognition, 44
accuracy and errors, 43
Index

85
ASR (see Automatic speech recognition 
(ASR))
Bayesian scheme, 43
HMM, 44
hybrid system, 44
noise cancellation, 41–42
signal enhancement, 41–42
SVM, 44
system, 66
time-adapted hybrid wavelet speech 
enhancement, 44
wavelet-based features, 42–43
WNN, 44
Speech segmentation, 30, 31
Speech sentences, 35
Speech signal, 30–31
Speech sounds, 6–7
Speech synthesis, 57, 58
Stationary WT, 18
Stegano signal, 71
Stegano speech, 72
Steganography, 71, 73, 74
Stein’s unbiased risk estimate (SURE), 37
Subbands, 15, 58, 59
Support vector machine (SVM), 29, 30, 44
SURE entropy, 48
SureShrink, 37
SVD, 73
SVM-based classification, 77
SVM-based emotion recognition, 52
SVM classifier, 65, 78
Synchro-squeezed wavelet transform, 53
T
Teager energy operator, 37, 75
Terminal analog, 8
Thresholding (shrink) approach, 36
Thresholding schemes
BayesShrink, 37
hard-thresholding function, 36, 37
semisoft shrinking, 37
soft thresholding, 36, 37
SURE, 37
SureShrink, 37
Teager energy operator, 37
UWT, 37
VisuShrink, 37
wavelet coefficient, 37
wavelet packet coefficients, 37–38
Time-domain (event)-based pitch  
detectors, 25, 26
Time-domain methods, 71, 73
Time-frequency analysis, 12, 13, 16, 18, 29, 30
Time-frequency domain, 23, 24
Time-scale distribution, 66
Time variance property, 43
Traditional cepstral analysis, 24
Transform domain, 71
Two-channel filter bank, 14
U
Undecimated wavelet-based speech coding 
strategy, 58
Undecimated wavelet packet decomposition 
(UWPD), 31, 32
Undecimated wavelet-packet transform 
(UWPT), 18, 58
Undecimated wavelet transform (UWT), 18, 37
Uniform WPD, 75
Universal-background model (UMB), 53
Unvoiced sounds, 7
V
Variance analysis (VA), 26
VisuShrink, 37
Vocal fold-related voice disorders, 77
Vocal organs, 5
Vocal tract
anatomical structure, 5, 6
characteristics, 65
cross-sectional area (c.s.a.), 7
Voice activity detection (VAD), 29–30
Voice disorders, 78
Voiced sounds
source, 7
and vowels, 6
Voice features, 75
Voice source, 65, 78
Voiced/unvoiced (V/U) classification, 23
VoIP applications, 57
W
Watermarking
authentication and protecting copyright, 71
in sparse representation, 73–74
speech, 73
Waveform coding, 58
Wavelet
emotion recognition, 51–52
neural networks in ASR, 44
scalogram analysis, 66–67
Wavelet analysis, 5, 9, 23, 26
Wavelet-based bark coherence function 
(WBCF), 63
Index

86
Wavelet-based features, 42–43
Wavelet-based nonlinear features, 66
Wavelet-based orthogonal filter banks, 78
Wavelet cepstral coefficients (WCC), 49, 75
Wavelet coefficients, 71, 72
Wavelet denoising, 35, 38, 39
Wavelet energy, 29
Wavelet family, 15, 16, 78
Wavelet filter, 14, 15, 19
Wavelet multiresolution analysis, 8
Wavelet octave coefficients of linear-­prediction 
residues (WOCOR), 48
Wavelet packet decomposition (WPD), 18
Wavelet packets (WP)
analysis, 42
frequency band, 63
MRAM, 63
objective quality measure, 61
PWP, 62
tree structure, 61
wavelet family, 63
coefficients, 37–38
conventional wavelet decomposition tree, 17
decomposition, 17, 18
entropy, 48
FFT, 18
filter bank, 38
filter-bank selection, 42
filter-bank structures, 52
filter structure, 48
finest time resolution, 17
frequency and time, 30
frequency resolution, 17
low-pass filter function space and 
high-pass filter function space, 17
multiscale functions, 16
PWP decomposition tree, 17
structure, 17
Wavelet-packet transform (WPT), 51, 53
Wavelets
and bank of filters, 14, 15
Daubechies, 16
empirical, 19–20
FT, 11
Meyer, 16
mother, 13
multiresolution analysis, 11
scalogram, 19
WP, 16–18
WT (see Wavelet transform (WT))
Wavelet shrinkage method, 49
Wavelet spectral analysis, 3
Wavelet theory, 11, 13
Wavelet thresholding, 37–38
Wavelet transform (WT), 1, 3
coefficients, 11
coding, 57
CWT, 19
framework, 11
and MRA, 12–14
optimized detection and classification, 42
pitch estimation, 25–27
quality evaluation of speech, 9
spectral analysis, 23–24
speech segmentation, 30
STFT, 11–13
UWT, 18
and VA, 26
voiced/unvoiced classification, 57
Wavelet/wavelet-packet analysis, 58
Wide-band (WB) speech codecs, 29
Wiener filtering, 42
WNN
ASR, 44
emotion recognition, 54
WPT-based NLD features, 53
WPT coefficients, 66
WT-based speech compression, 59
Z
Zero-crossing rate (ZCR), 29
Index


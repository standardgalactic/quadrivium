
Classical Numerical Analysis
Numerical analysis is a broad ﬁeld, and coming to grips with all of it may seem like
a daunting task. This text provides a thorough and comprehensive exposition of all
the topics contained in a classical graduate sequence in numerical analysis. With an
emphasis on theory and connections with linear algebra and analysis, the book shows
all the rigor of numerical analysis. Its high level and exhaustive coverage will prepare
students for research in the ﬁeld and will become a valuable reference as they continue
their career. Students will appreciate the simple notation and clear assumptions and
arguments, as well as the many examples and classroom-tested exercises ranging
from simple veriﬁcation to qualifying exam-level problems. In addition to the many
examples with hand calculations, readers will also be able to translate theory into
practical computational codes by running sample MATLAB codes as they try out new
concepts.
Abner J. Salgado is Professor of Mathematics at the University of Tennessee,
Knoxville. He obtained his PhD in Mathematics in 2010 from Texas A&M University.
His main area of research is the numerical analysis of nonlinear partial differential
equations, and related questions.
Steven M. Wise is Professor of Mathematics at the University of Tennessee, Knoxville.
He obtained his PhD in 2003 from the University of Virginia. His main area of research
interest is the numerical analysis of partial differential equations that describe phys-
ical phenomena, and the efﬁcient solution of the ensuing nonlinear systems. He has
authored more than 80 publications.


Classical Numerical Analysis
A Comprehensive Course
ABNER J. SALGADO
University of Tennessee, Knoxville
STEVEN M. WISE
University of Tennessee, Knoxville

Shaftesbury Road, Cambridge CB2 8EA, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre,
New Delhi – 110025, India
103 Penang Road, #05–06/07, Visioncrest Commercial, Singapore 238467
Cambridge University Press is part of Cambridge University Press & Assessment,
a department of the University of Cambridge.
We share the University’s mission to contribute to society through the pursuit of
education, learning and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781108837705
DOI: 10.1017/9781108942607
© Abner J. Salgado and Steven M. Wise 2023
This publication is in copyright. Subject to statutory exception and to the provisions
of relevant collective licensing agreements, no reproduction of any part may take
place without the written permission of Cambridge University Press & Assessment.
First published 2023
Printed in the United Kingdom by TJ Books Limited, Padstow Cornwall
A catalogue record for this publication is available from the British Library
Library of Congress Cataloging-in-Publication Data
Names: Salgado, Abner J., author. | Wise, Steven M. (Mathematician), author.
Title: Classical numerical analysis : a comprehensive course / Abner J. Salgado,
University of Tennessee, Knoxville, Steven M. Wise, University of Tennessee, Knoxville.
Description: Cambridge, United Kingdom ; New York, NY : Cambridge University Press, 2023. |
Includes bibliographical references and index.
Identiﬁers: LCCN 2022022842 (print) | LCCN 2022022843 (ebook) |
ISBN 9781108837705 (hardback) | ISBN 9781108942607 (epub)
Subjects: LCSH: Numerical analysis–Textbooks. |
BISAC: MATHEMATICS / Mathematical Analysis
Classiﬁcation: LCC QA297 .S25 2023 (print) | LCC QA297 (ebook) |
DDC 518–dc23/eng20220823
LC record available at https://lccn.loc.gov/2022022842
LC ebook record available at https://lccn.loc.gov/2022022843
ISBN 978-1-108-83770-5
Hardback
Cambridge University Press & Assessment has no responsibility for the persistence
or accuracy of URLs for external or third-party internet websites referred to in this
publication and does not guarantee that any content on such websites is, or will
remain, accurate or appropriate.

Contents
Preface
page xiii
Acknowledgments
xvii
List of Symbols
xix
Part I
Numerical Linear Algebra
1
1
Linear Operators and Matrices
3
1.1
Linear Operators and Matrices
3
1.2
Matrix Norms
9
1.3
Eigenvalues and Spectral Decomposition
12
Problems
15
2
The Singular Value Decomposition
20
2.1
Reduced and Full Singular Value Decompositions
21
2.2
Existence and Uniqueness of the SVD
22
2.3
Further Properties of the SVD
25
2.4
Low Rank Approximations
27
Problems
29
3
Systems of Linear Equations
31
3.1
Solution of Simple Systems
32
3.2
LU Factorization
35
3.3
Gaussian Elimination with Column Pivoting
43
3.4
Implementation of the LU Factorization
50
3.5
Special Matrices
51
Problems
65
Listings
67
4
Norms and Matrix Conditioning
73
4.1
The Spectral Radius
73
4.2
Condition Number
80
4.3
Perturbations and Matrix Conditioning
82
Problems
86

vi
Contents
5
Linear Least Squares Problem
88
5.1
Linear Least Squares: Full Rank Setting
89
5.2
Projection Matrices
93
5.3
Linear Least Squares: The Rank-Deﬁcient Case
98
5.4
The QR Factorization and the Gram–Schmidt Algorithm
101
5.5
The Moore–Penrose Pseudo-inverse
106
5.6
The Modiﬁed Gram–Schmidt Process
107
5.7
Householder Reﬂectors
110
Problems
115
Listings
119
6
Linear Iterative Methods
121
6.1
Linear Iterative Methods
122
6.2
Spectral Convergence Theory
124
6.3
Matrix Splitting Methods
125
6.4
Richardson’s Method
133
6.5
Relaxation Methods
135
6.6
The Householder–John Criterion
137
6.7
Symmetrization and Symmetric Relaxation
138
6.8
Convergence in the Energy Norm
140
6.9
A Special Matrix
143
6.10
Nonstationary Two-Layer Methods
145
Problems
149
Listings
154
7
Variational and Krylov Subspace Methods
156
7.1
Basic Facts about HPD Matrices
156
7.2
Gradient Descent Methods
161
7.3
The Steepest Descent Method
163
7.4
The Conjugate Gradient Method
169
7.5
The Conjugate Gradient Method as a Three-Layer Scheme
183
7.6
Krylov Subspace Methods for Non-HPD Problems
186
Problems
191
Listings
195
8
Eigenvalue Problems
197
8.1
Estimating Eigenvalues Using Gershgorin Disks
200
8.2
Stability
203
8.3
The Rayleigh Quotient for Hermitian Matrices
205
8.4
Power Iteration Methods
207
8.5
Reduction to Hessenberg Form
211
8.6
The QR Method
214
8.7
Computation of the SVD
221
Problems
223
Listings
225

Contents
vii
Part II
Constructive Approximation Theory
229
9
Polynomial Interpolation
231
9.1
The Vandermonde Matrix and the Vandermonde Construction
232
9.2
Lagrange Interpolation and the Lagrange Nodal Basis
235
9.3
The Runge Phenomenon
240
9.4
Hermite Interpolation
242
9.5
Complex Polynomial Interpolation
243
9.6
Divided Diﬀerences and the Newton Construction
249
9.7
Extended Divided Diﬀerences
257
Problems
263
10
Minimax Polynomial Approximation
266
10.1
Minimax: Best Approximation in the ∞-Norm
267
10.2
Interpolation Error and the Lebesgue Constant
277
10.3
Chebyshev Polynomials
278
10.4
Interpolation at Chebyshev Nodes
282
10.5
Bernstein Polynomials and the Weierstrass Approximation Theorem
286
Problems
297
11
Polynomial Least Squares Approximation
300
11.1
Least Squares Polynomial Approximations
301
11.2
Orthogonal Polynomials
301
11.3
Existence and Uniqueness of the Least Squares Approximation
302
11.4
Properties of Orthogonal Polynomials
305
11.5
Convergence of Least Squares Approximations
307
11.6
Uniform Convergence of Least Squares Approximations
313
Problems
319
12
Fourier Series
320
12.1
Least Squares Trigonometric Approximations
321
12.2
Density of Trigonometric Polynomials in the Space Cp(0, 1; C)
324
12.3
Convergence of Fourier Series in the Quadratic Mean
328
12.4
Uniform Convergence of Fourier Series
331
12.5
Convergence of Fourier Series in Sobolev Spaces
340
Problems
343
13
Trigonometric Interpolation and the Fast Fourier Transform
345
13.1
Periodic Interpolation and Periodic Grid Functions
347
13.2
The Discrete Fourier Transform
350
13.3
Existence and Uniqueness of the Interpolant
354
13.4
Alias Error and Convergence of Trigonometric Interpolation
356
13.5
Numerical Integration of Periodic Functions
361
13.6
The Fast Fourier Transform (FFT)
363

viii
Contents
13.7
Fourier Matrices, Least Squares Approximation, and Basic
Signal Processing
366
Problems
371
14
Numerical Quadrature
372
14.1
Quadrature Rules for Weighted Integrals
373
14.2
Simple Estimates for Interpolatory Quadrature
376
14.3
The Peano Kernel Theorem
378
14.4
Proper Scaling and an Error Estimate
Via a Scaling Argument
383
14.5
Newton–Cotes Formulas
385
14.6
Peano Error Formulas for Trapezoidal, Midpoint,
and Simpson’s Rules
392
14.7
Composite Quadrature Rules
395
14.8
Bernoulli Numbers and Euler–Maclaurin Error Formulas
400
14.9
Gaussian Quadrature Rules
408
Problems
414
Part III
Nonlinear Equations and Optimization
417
15
Solution of Nonlinear Equations
419
15.1
Methods of Bisection and False Position
421
15.2
Fixed Points and Contraction Mappings
423
15.3
Newton’s Method in One Space Dimension
428
15.4
Quasi-Newton Methods
433
15.5
Newton’s Method in Several Dimensions
440
Problems
444
Listings
449
16
Convex Optimization
451
16.1
Some Tools from Functional Analysis
451
16.2
Existence and Uniqueness of a Minimizer
460
16.3
The Euler Equation
463
16.4
Preconditioners and Gradient Descent Methods
469
16.5
The Golden Key
471
16.6
Preconditioned Steepest Descent Method
473
16.7
PSD with Approximate Line Search
479
16.8
Newton’s Method
482
16.9
Accelerated Gradient Descent Methods
489
16.10 Numerical Illustrations
496
Problems
498
Listings
499

Contents
ix
Part IV
Initial Value Problems for Ordinary Diﬀerential Equations
507
17
Initial Value Problems for Ordinary Diﬀerential Equations
509
17.1
Existence of Solutions
510
17.2
Uniqueness and Regularity of Solutions
515
17.3
The Flow Map and the Alekseev–Gr¨obner Lemma
518
17.4
Dissipative Equations
520
17.5
Lyapunov Stability
521
Problems
524
18
Single-Step Methods
525
18.1
Single-Step Approximation Methods
526
18.2
Consistency and Convergence
527
18.3
Linear Slope Functions
532
Problems
534
19
Runge–Kutta Methods
536
19.1
Simple Two-Stage Methods
537
19.2
General Deﬁnition and Basic Properties
539
19.3
Collocation Methods
544
19.4
Dissipative Methods
550
Problems
554
20
Linear Multi-step Methods
555
20.1
Consistency of Linear Multi-step Methods
555
20.2
Adams–Bashforth and Adams–Moulton Methods
562
20.3
Backward Diﬀerentiation Formula Methods
565
20.4
Zero Stability
568
20.5
Convergence of Linear Multi-step Methods
574
20.6
Dahlquist Theorems
577
Problems
578
Listings
580
21
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
581
21.1
The Linear Stability Domain and A-Stability
584
21.2
A-Stability of Runge–Kutta Methods
585
21.3
A-Stability of Linear Multi-step Methods
589
21.4
The Boundary Locus Method
590
Problems
593
Listings
595
22
Galerkin Methods for Initial Value Problems
596
22.1
Assumptions and Basic Deﬁnitions
596
22.2
Coercive Operators: The Discontinuous Galerkin Method
599

x
Contents
22.3
Monotone Operators: The Continuous Petrov–Galerkin Method
605
Problems
607
Part V
Boundary and Initial Boundary Value Problems
609
23
Boundary and Initial Boundary Value Problems for Partial Diﬀerential
Equations
611
23.1
Heuristic Derivation of the Common Partial Diﬀerential Equations
611
23.2
Elliptic Equations
627
23.3
Parabolic Equations
636
23.4
Hyperbolic Equations
647
Problems
660
24
Finite Diﬀerence Methods for Elliptic Problems
664
24.1
Grid Functions and Finite Diﬀerence Operators
665
24.2
Consistency and Stability of Finite Diﬀerence Methods
674
24.3
The Poisson Problem in One Dimension
678
24.4
Elliptic Problems in One Dimension
684
24.5
The Poisson Problem in Two Dimensions
691
Problems
696
25
Finite Element Methods for Elliptic Problems
700
25.1
The Galerkin Method
701
25.2
The Finite Element Method in One Dimension
704
25.3
The Finite Element Method in Two Dimensions
710
Problems
718
26
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
721
26.1
Periodic Diﬀerential Equations
721
26.2
Finite Diﬀerence Approximation
722
26.3
The Spectral Galerkin Method
727
26.4
The Pseudo-Spectral Method
732
Problems
735
Listings
739
27
Collocation Methods for Elliptic Equations
742
27.1
Weighted Sobolev Spaces and Weak Formulation
743
27.2
Weighted Spectral Galerkin Approximations
747
27.3
The Chebyshev Projection and the Finite Chebyshev Transform
749
27.4
Chebyshev–Gauss–Lobatto Quadrature and Interpolation
751
27.5
The Discrete Cosine Transform
756
27.6
The Chebyshev Collocation Method
762
27.7
Error Analysis of the Chebyshev Collocation Method
766
27.8
Practical Computation of the Collocation Approximation
769
Problems
770

Contents
xi
28
Finite Diﬀerence Methods for Parabolic Problems
774
28.1
Space–Time Grid Functions
775
28.2
The Initial Boundary Value Problem for the Heat Equation
776
28.3
Stability and Convergence in the L∞
τ (L∞
h )-Norm
778
28.4
Stability and Convergence in the L∞
τ (L2
h)-Norm
784
28.5
Stability by Energy Techniques
788
28.6
Advection–Diﬀusion and Upwinding
790
28.7
The Initial Value Problem for the Heat Equation
in One Dimension
792
Problems
802
Listings
807
29
Finite Diﬀerence Methods for Hyperbolic Problems
811
29.1
The Initial Value Problem for the Transport Equation
812
29.2
Positivity and Max-Norm Dissipativity
816
29.3
The Transport Equation in a Periodic Spatial Domain
819
29.4
Dispersion Relations
822
29.5
The Initial Boundary Value Problem for the Wave Equation
823
29.6
Finite Diﬀerence Methods for Hyperbolic Systems
827
Problems
830
Listings
833
Appendix A
Linear Algebra Review
837
A.1
The Field of Complex Numbers
837
A.2
Vector Spaces
839
A.3
Normed Spaces
843
A.4
Inner Product Spaces
847
A.5
Gram–Schmidt Orthogonalization Process
849
Problems
851
Listings
852
Appendix B
Basic Analysis Review
854
B.1
Sequences and Compactness in Cd and Rd
854
B.2
Functions of a Single Real Variable
858
B.3
Functions of Several Variables
866
B.4
Sequences of Functions
871
Problems
872
Appendix C
Banach Fixed Point Theorem
874
C.1
Contractions and Fixed Points in Banach Spaces
874
C.2
The Contraction Mapping Principle in Metric Spaces
876
Problems
879

xii
Contents
Appendix D
A (Petting) Zoo of Function Spaces
880
D.1
Spaces of Smooth Functions
880
D.2
Spaces of Integrable Functions
885
D.3
Sobolev Spaces
887
Problems
894
References
896
Index
901

Preface
This book on numerical analysis grew out of an ever expanding set of lecture notes
that, over the years, the authors developed, corrected, used, and misused, while
teaching the year-long sequence on this topic at the introductory graduate level at
the University of Tennessee, Knoxville (UTK). The purpose of this sequence can
be simply stated: prepare students for the PhD subject examination in numerical
analysis and equip them for research in a rich, active, and expanding ﬁeld.
The prerequisites for the book are (i) a solid understanding of linear algebra, at
the level of Horn and Johnson [44], for example, and (ii) a working knowledge of
advanced calculus, at the level of Rudin [76] or Bartel and Sherbert [6]. Both of
these topics are thoroughly reviewed in the appendices. Those comfortable with
the material in Appendices A and B should be well prepared for the book. Some
important topics from diﬀerential equations, functional analysis, and measure and
integration theory are also used. But, these are reviewed, as needed, and are not
treated as prerequisites.
Our mission while writing this book was to present a spartan, but thoroughly
understandable text in numerical analysis that students can use to pass PhD exams
and get quickly started on research, similar to the philosophy behind the well-
received graduate-level analysis text by Bass [7]. As a competing goal, we have
designed the text so that no important topic should be left out; the student using
this book should have all the details at their ﬁngertips, to the extent possible.
We make a concerted eﬀort to use good notation; make deﬁnitions clear and
concise; make hypotheses of theorems, lemmas, etc., apparent, perhaps to the
point of being too pedantic; use the simplest versions of proofs; and keep proofs
of theorems, lemmas, etc., where they are most natural for quick reference, after
the respective results are stated. Important facts that students will need later are
never buried deep in the exercises; they are front and center in the presentation.
We may have fallen short in some or all of these goals, but they were our goals,
nonetheless.
The end result is, we believe, and hope, a text that has a very broad coverage, but
with a simple, modern, and easy-to-read style, and, importantly, with notation made
as clear as possible, even if that means breaking with tradition. We deliberately
choose not to be overly expository or conversational with our readers. The reader
will ﬁnd few long paragraphs of explanation. While some instructors will lament the
the loss of those paragraphs, to be honest, in our experience, most students do

xiv
Preface
not read them. They want core principles, easily locatable facts, and clear proofs.
Thus, we focus on illustrative examples, good problems, and clean presentation.
Some other texts do a much better job of using eloquent language to present
the ideas and their historical signiﬁcance. Speciﬁcally, the books by Trefethen [95],
Scott [83], and S¨uli and Mayers [89] include fascinating historical accounts of the
development of numerical analysis and are must-reads. In our book, whenever a
named concept or result is ﬁrst introduced, we have included, as a footnote, the
name and some minimal background on this person. This is done to convey that
mathematics in general, and numerical analysis in particular, is a lively subject,
made by and for people. We have relied on Wikipedia [101] and the MacTutor
History of Mathematics archive [66] as references.
We emphasize theory over implementation in the book, and there are many
good, classroom-tested problems that involve proofs and theoretical insight. This
is a book that uses a lot of linear algebra and analysis, subjects that are near and
dear to our hearts. The disciplined student will sharpen their theoretical abilities
in those subjects with this book. It is our realization, however, that, at least in
universities in the United States, advanced undergraduate or beginning graduate
students sometimes do not have an adequate preparation in linear algebra and
basic analysis. For this reason, we have included rather substantial reviews of
the necessary results from linear algebra and analysis, in Appendices A and B,
respectively. In addition, many of the ideas and techniques of Part V rely on
the theory of partial diﬀerential equations. Chapter 23 and Appendix D provide
a cooking-recipe list of facts. Some other background material is recalled as it is
needed, and references to the literature are provided. To the extent possible, the
presentation was designed to be self-contained.
We believe that the subject of numerical analysis does not need to be hard —
or harder than it actually is — but, on the other hand, there is deep, beautiful,
and, yes, sometimes diﬃcult mathematics under the hood. One of our goals is
to dispel the myth that numerical analysis is an ad hoc mixture of whatever
seems to work, requiring little to no theory, proofs, or analysis. Our subject is
a fundamental discipline of mathematics, a core sub-ﬁeld of analysis, and many of
the great mathematicians, from antiquity to the present — including Erd˝os and
von Neumann, for example — have contributed to its advance. The biographical
footnotes that we have added reﬂect this fact.
This book is not merely a catalog of numerical methods, though most of the
important ones are contained herein. In almost every case, methods are developed
and supported by rigorous theory. We give plenty of examples with hand calculations
and output from numerical simulations. We include several sample codes, so that
students can get their hands dirty. The codes are written in MATLAB® and were
tested in its open source counterpart GNU Octave for compatibility. These codes are
designed to help students learn the subject. To this end, we emphasize clarity over
eﬃciency in our programming. Still, we adopt the point of view, shared by many
others, that performing computational experiments can be both fun and deeply
enlightening. Students are encouraged to code the methods and try them out. The
interested student could use this text as a starting point for learning implementation

Preface
xv
issues which, we admit, are highly nontrivial and almost completely ignored in this
text. The active researcher could use it to learn how to design a better algorithm
or to understand why a particular algorithm works, or does not work. The codes
listed in the text — in addition to those not listed but used to generate examples
and ﬁgures — can be obtained from GitHub:
https://github.com/stevenmwise/ClassicalNumericalAnalysis
While this text covers more than what can be presented in a year-long course,
we have undoubtedly omitted several topics that can be found in other books
on the subject. Most of these omissions were deliberate, made either because we
believed that the topics were more advanced or because we deemed the topics to be
nonessential. For instance, many numerical analysis books begin the discussion by
addressing rounding errors and ﬂoating point arithmetic. We chose not to discuss
these points, as we believe that, while important, they skew the student’s perception
about what is numerical analysis. This, then, begs the question: What is numerical
analysis? For an answer, we defer to one of the classics [41]:
. . . we shall mean by numerical analysis the theory of constructive methods of mathe-
matical analysis. The emphasis is on the word “constructive.”
We urge the reader to examine the Introduction to this reference for a very insightful
deﬁnition of what numerical analysis is, and what it is not, one that was established
in the early days of our beloved discipline. For an update on this viewpoint, which
only reinforces our beliefs, we refer to the appendix of [96].
Regarding usage, the graduate-level numerical analysis sequence at UTK focuses
heavily on numerical linear algebra, nonlinear equations, and numerical diﬀerential
equations. These topics are well represented in the text, speciﬁcally in Parts I, III, IV,
and V, and students using this book will get a thorough and modern introduction in
those areas. However, we are well aware that other universities choose to emphasize
other topics of numerical analysis. While the subjects of interpolation, constructive
approximation, and quadrature theory, the focus of Part II, are not speciﬁcally in
the catalog description of the introductory graduate sequence at UTK, these are
included to support the theory needed to analyze numerical methods in the other
parts of the book, to broaden the background knowledge of our students when they
study these subjects in other classes, and for audiences outside of our university.
We have taught the material from Part II in a separate, single-semester, graduate
course on classical approximation theory.
We envision, and hope, that this text will be used in several year-long numerical
analysis sequences. For this reason the dependency of all chapters is linear, with
the exception of the appendices. Some sample course plans and ways we have used
this text to teach are as follows:
• UTK, numerical algebra: (1 semester) Part I and Chapter 15.
• UTK, numerical diﬀerential equations: (1 semester) Part IV and Chapters 23 —
25, 28 and 29, referencing results from Part II, as needed.
• UTK, classical approximation theory: (1 semester) Part II, with Chapters 26
and 27 as applications.

xvi
Preface
• Classical numerical analysis: (1 semester) Chapters 3, 4, 6, 7, 9, 12, 14, 15, and
Part IV.
• Topics in numerical analysis: (1 semester) Part II, Chapters 2 and 5, Part IV,
Chapters 6 and 7 (using Chapter 24 as motivation), and Chapters 15 and 16.

Acknowledgments
The authors thank the many Numerical Analysis students at the University of
Tennessee who endured countless versions of this text. They provided valuable
feedback on the stylistic structure of the book and also found numerous typos,
errors, and blunders. We hope that most of the inaccuracies have been amended.
For those that remain, we are solely responsible.
We acknowledge support from the National Science Foundation, which supported
us in our research during the writing of the book. This project represents an attempt
on our part to make the subject of numerical analysis more accessible, from the
theoretical point of view, and more interesting to a broader mathematics audience,
especially those who might have thought that the subject lacks rigor and beauty.
At the same time, much of what we have learned at the frontiers of research in
numerical partial diﬀerential equations and scientiﬁc computing has made it into
the textbook.
AJS: I wish to thank all the numerical analysis instructors that I have had
throughout the years. In particular, and in chronological order, my MSc advisor,
V.G. Korneev; my PhD advisor J.-L. Guermond; V. Girault; and my postdoc mentor,
R.H. Nochetto. All the good things I contributed to this book are because of the
ideas and “way of doing business” that they taught me. All the bad ideas that
remain are due only to the fact that, as everyone knows, I am very stubborn. I also
wish to thank all the other numerical analysts who had the misfortune to cross
paths with me at some point. I thank also Steve M. Wise for bringing me along
on this journey, and for his patience. I am glad we were able to stay friends after
completing this project. Finally, I thank my parents for supporting me in every step
of my professional formation. Without their support I would not have made it to
where I am today.
SMW: I must thank my numerical analysis teachers and advisors, Chris Beattie
and Layne Watson, at Virginia Tech. I got a really great foundation from them.
From my PhD advisor, Bill Johnson, a materials scientist by training, I learned to
appreciate the power of computing in scientiﬁc exploration. The equations that I
learned from Bill still motivate and drive my numerical analysis research 20 years
later. My postdoctoral advisor, long-time collaborator, and friend, John Lowengrub,
taught me a lot about the intersection of practical scientiﬁc computing, modeling,
and numerical analysis. I fortunately learned a valuable lesson early in my career;
namely, that I should not be afraid to admit that I do not know something. This
enabled me to learn from those around me and helped to ﬁll in the gaps in my

xviii
Acknowledgments
education, which are still considerable. Speciﬁcally, my colleagues and collaborators,
Xiaobing Feng, Ohannes Karakashian, Cheng Wang, and, of course, Abner J.
Salgado, have given me a great on-the-job education. I am so grateful that Abner
agreed to go on this journey with me. He is the smartest person I know, and I have
learned so much from him. It has been a pleasure. I thank my wife, Nicole, and
children, Jude and Cece, for giving up a lot of family time while I worked on this
project over the last few years. I love you so much. Finally, I dedicate my work on
this book to the memory of my dear mother, Mary Ann Wise. I love you, miss you,
and think of you every day.

Symbols
This list describes several symbols that will be commonly used within the text. The
page number indicates where its deﬁnition, or ﬁrst appearance, is found.
(·, ·)2
The Euclidean inner product, page 849
(·, ·)L2
h
The discrete inner product on V0(¯Ωh), page 673
[x]i
The ith component of the n-vector x, page 840
[·, ·]L2
h
The discrete inner product on V(¯Ωh), page 673
[A]i,j
The element in the ith row and jth column of the matrix A, page 4
¯z
The complex conjugate of z ∈C, page 838
¯δh
Backward diﬀerence operator, page 669
¯Ωh
[0, 1]d ∩Zd
h, with d ∈N, page 666
u ⊗v
The exterior product of u and v, also denoted uv H, page 27
C
In the context of parabolic equations, this is the space–time cylinder,
Ω× (0, T), page 646
Dj
hv
For a periodic grid function v and j ∈{1, 2}, this denotes its pseudo-
spectral derivative of order j, page 732
Fn[·]
For n ∈N, this denotes the Discrete Fourier Transform (DFT),
page 350
GB
h
For a grid domain Gh, these are the boundary points (with respect
to a ﬁnite diﬀerence operator), page 670
GI
h
For a grid domain Gh, these are the interior points (with respect to
a ﬁnite diﬀerence operator), page 670
IX
For a nodal set X ⊂[a, b] ⊂R, this denotes the interpolation
operator subordinate to X, page 234
Km(A, q)
The Krylov subspace of the matrix A of degree m, page 169
LX
For a nodal set X, this denotes the Lagrange nodal basis, page 235
R(a, b)
The same as R([a, b]), page 862
R(a, b; C)
The collection of complex-valued Riemann integrable functions,
page 865
R(I)
For I a ﬁnite interval, this denotes the collection of functions that
are Riemann integrable, page 862
V(Gh)
For a grid domain Gh ⊆Zd
h with h = 1/(N +1), this is the collection
of grid functions, page 666
V(C)
The space of functions Z →C, page 348
V0(¯Ωh)
The collection of functions in V(¯Ωh) that vanish on the discrete
boundary ∂Ωh, page 667

xx
List of Symbols
VM,p(C)
For M ∈N, this denotes the space of complex-valued grid functions
that are, in addition, periodic, page 722
Z[·]
The Fourier-Z, or Discrete Fourier, transform on grid functions in
L2
h(Zh; C), page 794
χA
The characteristic polynomial of the matrix A, page 12
clos(K)
For a subset K of a Hilbert space, this denotes its closure, page 455
clow(K)
For a subset K of a Hilbert space, this denotes its weak closure,
page 455
col(A)
The column space of the matrix A, page 6
C
The set of complex numbers, page 838
Cn
The vector space of complex n-vectors, page 840
Cn
⋆
The collection of nonzero vectors in Cn, page 9
Cn×n
Her
The space of Hermitian matrices of size n, page 57
∆φ
For a smooth scalar-valued function φ, this denotes its Laplacian,
page 618
δn,p
The n-periodic grid delta function, page 349
∆h
The discrete Laplacian, page 669
δh
Forward diﬀerence operator, page 669
δ♦
h
The discrete mixed derivative, page 669
∆□
h
The two-dimensional skew Laplacian, page 670
δi,j
The Kronecker delta, page 842
ℓ2(Z; C)
The collection of all sequences {aj}j∈Z
⊂
C that are square
summable, page 330
B(V)
The same as B(V, V), page 457
B(V, W)
For normed spaces V and W, this denotes the vector space of
bounded linear operators V →W, page 457
L(V)
The same as L(V, V), page 3
L(V, W)
The set of linear operators from V to W, page 3
ℑz
The imaginary part of the complex number z, i.e., ℑz = b, if z =
a + ib, page 838
im(A)
The image (or range) of the matrix A, also denoted R(A), page 6
i
The imaginary unit, page 838
κ(A)
The condition number of the matrix A, page 80
κ2(A)
The spectral condition number of the matrix A, page 80
ker(A)
The kernel (or null space) of the matrix A, also denoted N(A), page 6
⟨·, ·⟩
The duality pairing between a Hilbert space H and its dual H′,
page 464
[u ⋆v]n,p
For u, v ∈Vn,p(C), this denotes their discrete periodic convolution,
page 352
≤
For a vector space V and W ⊆V, W ≤V denotes that W is a
subspace of V. If W ̸= V, then we denote W < V, page 841
Bα(−1, 1)
For α the Chebyshev weight function, this denotes the subspace
of C1([−1, 1]) of functions such that α(x)g(x) →0 as x →±1,
page 743
Fm(S)
For m ∈N, this is F 1(S) ∩Cm(S; Rd), page 517

List of Symbols
xxi
Vn,p(C)
For n ∈N, this denotes the space of n-periodic grid functions,
page 348
˚VM,p(C)
For M ∈N, this denotes the space of mean-zero, complex-valued,
periodic grid functions, page 723
˚δh
Centered diﬀerence operator, page 669
˚
SN(0, 1; C)
For N ∈N, this is the space of complex-valued, mean-zero, trigono-
metric polynomials of degree at most N, page 730
˚C∞
p (0, 1; C)
The space inﬁnitely diﬀerentiable, complex-valued, periodic functions
that have mean-zero, page 884
˚Cm
p (0, 1; C)
For m ∈N0, this denotes the space functions in Cm
p (0, 1; C) that
have mean-zero, page 884
˚
Hm
p (0, L; C)
For m ∈N0, this is the space of functions in Hm
p (0, L; C) that have
mean-zero, page 892
A ≍B
The matrix A is similar to the matrix B, page 13
A(S)
For A ∈Cn×n and S ⊆{1, . . . , n}, this denotes the sub-matrix
obtained by deleting the rows and columns whose indices are not in
S, page 36
AH
The conjugate transpose of the matrix A, page 7
A†
The Moore–Penrose pseudo-inverse of A, page 30
A⊺
The transpose of the matrix A, page 7
A−1
The inverse of the matrix A, page 8
∇v
For a smooth scalar-valued function, this denotes its gradient,
page 612
∇· u
For a smooth vector-valued function u, this denotes its divergence,
page 612
∥A∥max
The matrix max-norm of the matrix A, page 9
∥· ∥H1
h
The discrete H1
h-norm on the space of grid functions on V(¯Ωh),
page 686
∥· ∥Lp
h
For p ∈[1, ∞], this denotes the discrete Lp
h-norm on the spaces of
grid functions on (0, 1)d, page 671
∥A∥F
The Frobenius norm of the matrix A, page 9
∥A∥p
The induced p-norm of the matrix A, page 10
∥f ∥Lp(Ω;C)
For a function f : Ω→C, this denotes its Lp-norm, p ∈[1, ∞],
page 881
∥f ∥Lp
w (a,b;C)
For a function f : [a, b] →C, this denotes its weighted Lp-norm,
p ∈[1, ∞), with weight w, page 882
Ωh
(0, 1)d ∩Zd
h, with d ∈N, page 666
∂Ωh
¯Ωh\Ωh, page 666
∂pC
The parabolic boundary of C, page 646
H∗
For a complex Hilbert space H, this denotes the anti-dual, page 736
Kn
The vector space of n-vectors, page 840
Km×n
The set of matrices with m rows and n columns with coeﬃcients in
K, page 4
Pn
This is, typically, Pn(R) or Pn(C), depending upon the context,
page 840

xxii
List of Symbols
Pn(K)
The vector space of polynomials of degree no larger than n with
coeﬃcients in K, page 840
Pm/n
For m, n ∈N0, this is the set of rational polynomials whose numerator
and denominator lie in Pm and Pn, respectively, page 588
Q
The set of rational numbers, page 838
Tn
For n ∈N0, this denotes the space of all one-periodic trigonometric
polynomials, page 321
Zd
h
The collection of vectors in Rd of the form hz with z ∈Zd, page 665
ℜz
The real part of the complex number z, i.e., ℜz = a, if z = a + ib,
page 838
R
The set of real numbers, page 838
Rn
The vector space of real n-vectors, page 840
Rn
⋆
The collection of nonzero vectors in Rn, page 9
Rn×n
sym
The space of real symmetric matrices of size n, page 57
ρ(A)
The spectral radius of matrix A ∈Cn×n, page 73
row(A)
The row space of the matrix A, page 6
σ(A)
The spectrum of the square matrix A, page 12
σ(A)
The spectrum of the linear operator A, page 15
S 1,0(Th)
The space of continuous piecewise linear functions subject to the
triangulation Th, page 705
S 1,0
0
(Th)
This is S 1,0(Th) ∩H1
0(Ω), page 705
S p,0(Th)
For a one-dimensional mesh Th, with #Th = N and p ∈NN+1, this
is the space of functions that are continuous, and for every Ii ∈Th
their restriction to Ii is a polynomial of degree pi+1, page 710
S p,0
0
(Th)
This is S p,0(Th) ∩H1
0(0, 1), page 710
S p,−1(τ; H) For a Hilbert space H, this is the space of H-valued piecewise
polynomials of degree at most p over the partition τ, page 599
S p,0(τ; H)
This is S p,−1(τ; H) ∩C([0, T]; H), page 605
S p,0(Th)
For p ∈N, this is the space of functions that are continuous and
piecewise polynomials, of degree p, subject to the triangulation Th,
page 710
S p,0
0
(Th)
This is S p,0(Th) ∩H1
0(Ω), page 710
S p,r(τ; H)
This is S p,−1(τ; H) ∩Cr([0, T]; H), page 605
SN,0(−1, 1) For N ∈N, this denotes the set of polynomials of degree at most N
that vanish at x = ±1, page 747
span(S)
The span of the set S, also denoted ⟨S⟩, page 840
supp g
The support of the function g, page 705
supp(φ)
For a function φ, this denotes its support, page 887
˜δn,p
The singular n-periodic grid delta function, page 353
˜Eτ
h(ξ)
The symbol of a two-layer, matrix-valued, ﬁnite diﬀerence method,
page 828
˜Eτ
h (ξ)
The symbol of a two-layer ﬁnite diﬀerence method, page 795
Th
A mesh with mesh size h > 0, page 705
{xk}∞
k=1
A sequence of vectors in either Cd or Rd, page 854
A∗
The adjoint of the linear operator A, page 7

List of Symbols
xxiii
C(A; B)
The vector space of continuous functions with domain A and range
in B, page 842
C(I)
For I and interval this denotes the set of functions f : I →R that
are continuous, page 858
C0(I)
The same as C(I), page 859
Cm(I)
For m ∈N and I an interval, this denotes the collection of functions
f : I →R whose derivatives up to and including mth order exist and
are continuous on I, page 859
Cm
p (0, 1; C)
For m ∈N0, this denotes the space of complex-valued, m-times
continuously diﬀerentiable periodic functions, page 884
C0,1(I)
For I an interval, this denotes the collection of functions f : I →R
that are Lipschitz continuous, page 859
C0,α([0, 1])
For α > 0, this denotes the set of functions v : [0, 1] →R that are
H¨older continuous of order α, page 895
C0,α(I)
For I an interval and α ∈(0, 1], this denotes the collection of
functions f : I →R that are H¨older continuous of order α, page 859
Cb(Rd)
The space of continuous functions Rd →R that, in addition, are
bounded on Rd, page 639
Cm
b (I)
For m ∈N and I an interval, this denotes the collection of functions
in Cm(I) such that, in addition, the function and all its derivatives
up to and including order m are bounded on I, page 860
f = O(g)
The Landau symbol. Whenever f and g are two related quantities,
this is used to denote that f is, asymptotically, of the order of g,
page 856
F 1(S)
The class of slope functions that are continuously diﬀerentiable on
S and whose partial u-derivatives are bounded, page 517
H1(Ω)
For a bounded domain Ω∈Rd, with d ∈N this denotes the Sobolev
space of functions v ∈L2(Ω) such that ∇v ∈L2(Ω; Rd), page 888
H1
0(Ω)
The subspace of H1(Ω) of functions that vanish on the boundary,
page 888
H1
α,0(−1, 1)
The subspace of H1
α(−1, 1) of functions that vanish at x = ±1,
page 744
Hm
α (−1, 1)
For m ∈N0 and α the Chebyshev weight function, this denotes the
Chebyshev weighted Sobolev space of order m, page 743
Hm
p (0, L; C)
For L > 0 and m ∈N0, this denotes the space of L-periodic Sobolev
functions, page 892
L2
h(Zh)
The collection of grid functions V(Zh) that are square summable,
page 793
L2
p(0, 1; C)
The set of all one-periodic, locally square integrable functions,
page 886
Lℓ
For a nodal set X of size n + 1 and 0 ≤ℓ≤n, this denotes the ℓth
element of the Lagrange nodal basis, page 235
S1 + S2
For S1, S2 ≤Cn, this denotes their sum, page 94
S1 ⊕S2
For S1, S2 ≤Cn, this means that they are complementary subspaces,
i.e., S1 + S2 = Cn, page 94

xxiv
List of Symbols
S1
⊥⊕S2
For S1, S2 ≤Cn, this means that they are complementary, and
orthogonal, subspaces, i.e., S1 + S2 = Cn and s1 ∈S1 s2 ∈S2
implies sH
2 s1 = 0, page 95
W ⊥
The orthogonal complement of the set W, page 849
x ⊥y
The vector x is orthogonal to y, page 849
X ,→Y
For normed spaces X and Y , this means that X is continuously
embedded in Y , page 706
A(k)
The leading principal sub-matrix of order k of A, page 36
∥x∥p
The p-norm of a complex n-vector x. Also denoted ∥x∥ℓp(Cn),
page 844
Hm(Ω)
For m ∈N, this denotes the collection of functions v ∈L2(Ω) whose
weak derivatives up to order m belong to L2(Ω) as well, page 890
Lp(0, T; V)
For a Banach space V, this denotes the space of functions such that
the mapping t 7→∥v(t)∥V belongs to Lp(0, T), page 644
#S
The cardinality of the set S, page 841
|z|
The modulus of the complex number z, page 839

Part I
Numerical Linear Algebra


1
Linear Operators and Matrices
We begin our disussion by presenting several facts about the natural transformations
between vector spaces and their representations, i.e., matrices. These form the
foundation for our study of numerical linear algebra. Herein, we will set in place
much of our notation, especially for matrices, that will be used not just for Part I
but for the entirety of the book. Every student using this text should master the
material from Appendix A and this chapter before moving on. The book by Horn
and Johnson [44] is an excellent external reference.
Why is linear algebra so important to numerical analysis? That is a fair question.
The answer is that many algorithms in numerical analysis — for a broad range of
problem types, interpolation, approximation of functions, approximating solutions
to diﬀerential or integral equations — require, at some stage in the algorithm, the
investigation of a system of linear equations:













a1,1x1 + a1,2x2 + · · · + a1,nxn = f1,
a2,1x1 + a2,2x2 + · · · + a2,nxn = f2,
...
am,1x1 + am,2x2 + · · · + am,nxn = fm.
Many algorithms will require the solution of such systems. Others, by contrast, may
need some or all of the eigenvalues or singular values of the associated coeﬃcient
matrix for the system.
Before we jump into the topic of how to practically solve such a system of
equations, which we will cover in Chapter 3 — or how to compute singular values
(Chapter 2) and/or eigenvalues (Chapter 8) of the coeﬃcient matrix — we need
to understand the properties of such systems. This will be the topic of this chapter.
Let us get started.
1.1
Linear Operators and Matrices
We study the natural mappings between vector spaces, i.e., those that preserve
the vector space structure.
Deﬁnition 1.1 (linear operator). Let V and W be complex vector spaces. The
mapping A: V →W is called a linear operator if and only if
A(αx + βy) = αAx + βAy,
∀α, β ∈C, ∀x, y ∈V.

4
Linear Operators and Matrices
The set of all linear operators from V to W is denoted by L(V, W). For simplicity,
we denote by L(V) the set of linear operators from V to itself. Suppose that
A, B ∈L(V, W) and α, β ∈C are arbitrary. We deﬁne, in a natural way, the object
αA + βB via
(αA + βB)x = αAx + βBx,
∀x ∈V.
It is straightforward to prove that αA + βB is a linear operator and we get the
following result.
Proposition 1.2 (properties of L(V, W)). Let V and W be complex vector spaces.
The set L(V, W) is a vector space using the natural deﬁnitions of addition and
scalar multiplication given in the last deﬁnition. If dim(V) = m and dim(W) = n,
then dim(L(V, W)) = mn.
Proof. See Problem 1.2.
Deﬁnition 1.3 (m × n matrices). Let K be a ﬁeld. We deﬁne, for any m, n ∈N,
Km×n = {A = [ai,j] | ai,j ∈K, i = 1, . . . , m, j = 1, . . . , n} .
The object A is called a matrix and the elements ai,j ∈K are called its components
or entries. We call Cm×n the set of complex m × n matrices and Rm×n the set
of real m × n matrices.
To extract the entry in the ith row and jth column of the m×n matrix A ∈Km×n,
we use the notation
[A]i,j = ai,j ∈K.
The convention is that the entries of a matrix are denoted by the respective
lowercase roman symbol. For example, the matrix C has entries ci,j. We often
make this identiﬁcation explicit, as in writing A = [ai,j] ∈Cm×n. We say that there
are m rows and n columns in an m × n matrix A. We naturally deﬁne m × n matrix
addition and scalar multiplication component-wise via
[A + B]i,j = ai,j + bi,j,
[αA]i,j = αai,j,
i = 1, . . . , m,
j = 1, . . . , n,
where A, B ∈Km×n are arbitrary m × n matrices and α ∈K is an arbitrary scalar.
Proposition 1.4 (Km×n is a vector space). With addition and scalar multiplication
deﬁned as above, Km×n is a vector space over K and dim(Km×n) = m · n.
Proof. See Problem 1.3.
Of course, the reader will remember that matrices can be combined in more
exotic ways.
Deﬁnition 1.5 (matrix product). Let A = [ai,k] ∈Km×p and B = [bk,j] ∈Kp×n.
The matrix product C = AB is a matrix in Km×n whose entries are computed
according to the formula
[C]i,j = ci,j =
p
X
k=1
ai,kbk,j,
i = 1, . . . , m,
j = 1, . . . , n.

1.1 Linear Operators and Matrices
5
Next, we deﬁne a matrix–vector product, which, the reader will see, is similar to
the last deﬁnition.
Deﬁnition 1.6 (matrix–vector product). Suppose that x = [xs] ∈Kn and A =
[ak,s] ∈Km×n. Then the matrix–vector product y = Ax is a vector in Km whose
components are computed via the formula
[y]k = yk =
n
X
s=1
ak,sxs,
k = 1, . . . , m.
Remark 1.7 (identiﬁcation). Suppose that A ∈Cm×n. Then the (canonical)
mapping A: Cn →Cm deﬁned by y = Ax — where x ∈Cn, so that the matrix–
vector product y is in Cm — is linear. Mimicking the identiﬁcation process outlined
in Theorem A.24, we can also identify L(Cn, Cm) with the space Cm×n of matrices
having m rows and n columns of complex entries. This says that all linear mappings
from Cn to Cm are, essentially, matrices. This result can be generalized to identify
L(Kn, Km) with Km×n for a generic ﬁeld K.
Remark 1.8 (notation). It will be helpful from this point on to always view Ck as a
vector space of column k-vectors, i.e., Ck×1. When we consider x ∈Ck, we think
x =


|
x
|

=


x1
...
xk

.
Upon introducing the transpose operation ·⊺: Ck×1 →C1×k as mapping column
k-vectors to row k-vectors, we will often express x ∈Ck inline as x = [x1, . . . , xk]⊺,
i.e., as the transpose of a row vector. In a related way, given a matrix A ∈Cm×n we
commonly wish to represent it in a column-wise format (as a collection of column
vectors) via
A =


|
|
c1
· · ·
cn
|
|

,
cj ∈Cm, j = 1, . . . , n,
or in a row-wise format (as a collection of row vectors) via
A =


−
r ⊺
1
−
...
−
r ⊺
m
−

,
r i ∈Cn, i = 1, . . . , m.
As a further shorthand, we will often write (inline) A = [c1, . . . , cn] and A =
[r 1, . . . , r m]⊺. It is important to notice that if we view the matrix A in column-wise
format, then the matrix–vector product y = Ax ∈Cm is precisely
y =
n
X
k=1
xkck.
In other words, the column vector y is a linear combination of the columns of A.

6
Linear Operators and Matrices
Thinking about A ∈Cm×n as a mapping from Cn to Cm, the following deﬁnitions
are natural.
Deﬁnition 1.9 (range and kernel). Let A ∈Cm×n. The image (or range) of A is
deﬁned as
im(A) = R(A) = {y ∈Cm | ∃x ∈Cn, y = Ax} ⊆Cm.
The kernel (or null space) of A is
ker(A) = N(A) = {x ∈Cn | Ax = 0} ⊆Cn.
Deﬁnition 1.10 (row and column space). Suppose that the matrix A ∈Cm×n is
expressed column-wise as A = [c1, . . . , cn] and row-wise as A = [r 1, . . . , r m]⊺. The
row space of A is
row(A) = span({r 1, . . . , r m}) ≤Cn
and the column space of A is
col(A) = span({c1, . . . , cn}) ≤Cm.
The row rank of A is the dimension of row(A); similarly, the column rank is the
dimension of col(A).
A very important result in linear algebra states that the row and column ranks
coincide. For a proof, see, for example, [44].
Theorem 1.11 (row and column rank). Suppose that A ∈Cm×n. The row and
column ranks of A are equal.
Since this is an important invariant between the domain and range of an operator,
we give it a name.
Deﬁnition 1.12 (rank). The rank of a matrix A ∈Cm×n is the dimension of its
row/column space. We denote it by the symbol rank(A).
Theorem 1.13 (range and column space). Let A ∈Cm×n be represented column-
wise as A = [c1, . . . , cn]. Then
im(A) = span({c1, . . . , cn}) = col(A).
In other words, the range of A coincides with its column space.
Proof. (⊆) Let y ∈im(A) ⊆Cm. Then, by deﬁnition, there is an x ∈Cn for which
y = Ax, or
y =
n
X
k=1
xkck,
which implies that y ∈col(A).
(⊇) On the other hand, if y ∈col(A), this implies that there are αi ∈C,
i = 1, . . . , n such that
y =
n
X
i=1
αici.

1.1 Linear Operators and Matrices
7
Deﬁne x = [α1, . . . , αn]⊺∈Cn. The previous identity shows that y = Ax, so that
y ∈im(A).
Corollary 1.14 (range and rank). For any A ∈Cm×n,
dim(im(A)) = rank(A).
Deﬁnition 1.15 (nullity). Suppose that A ∈Cm×n. The nullity of A is the
dimension of ker(A):
nullity(A) = dim(ker(A)).
Theorem 1.16 (properties of the rank). Let A ∈Cm×n. Then
1. rank(A) ≤min{m, n}.
2. rank(A) + nullity(A) = n.
3. For any B ∈Cn×p, we have rank(AB) ≥rank(A) + rank(B) −n.
4. For any C ∈Cm×m with rank(C) = m and any B ∈Cn×n with rank(B) = n, it
holds that
rank(CA) = rank(A) = rank(AB).
5. rank(AB) ≤min{rank(A), rank(B)}.
6. rank(A + B) ≤rank(A) + rank(B).
Proof. Some of these are given as exercises. Otherwise, see, for example, [44].
Deﬁnition 1.17 (adjoint). Suppose that (V, ( · , · )V) and (W, ( · , · )W) are inner
product spaces over C. Let A ∈L(V, W). The adjoint of A is a linear operator
A∗∈L(W, V) that satisﬁes
(Ax, y)W = (x, A∗y)V,
∀x ∈V, y ∈W.
A linear operator A ∈L(V) = L(V, V) is called self-adjoint if and only if A = A∗.
For matrices, the adjoint has a familiar deﬁnition.
Deﬁnition 1.18 (matrix adjoint, conjugate transpose). Let A = [ai,j] ∈Cm×n.
The matrix adjoint (or conjugate transpose) of A is the matrix AH ∈Cn×m with
entries

AH
i,j = ¯aj,i.
The transpose of A is the matrix A⊺∈Cn×m with entries
[A⊺]i,j = aj,i.
A matrix A ∈Cn×n is called Hermitian1 if and only if A = AH. A is called skew-
Hermitian if and only if A = −AH. A matrix A ∈Rn×n is called symmetric if and
only if A = A⊺and skew-symmetric if A = −A⊺.
Simple calculations yield the following results.
1 Named in honor of the French mathematician Charles Hermite (1822–1901).

8
Linear Operators and Matrices
Proposition 1.19 (properties of matrix adjoints). Let A ∈Cm×p and B ∈Cp×n.
Then (AB)H = BHAH and (AH)H = A.
Proof. See Problem 1.8.
Remark 1.20 (notation). Observe that, above, we have naturally extended the
domain of deﬁnition of the operator ·⊺. Let x = [x1, . . . , xn]⊺∈Cn. The conjugate
transpose of x is deﬁned as the row vector xH = [¯x1, . . . , ¯xn]. This conforms to
the deﬁnition above, provided that we view any column n-vector as a matrix with n
rows and one column. A direct computation shows that (xH)H = x for all x ∈Cn.
Moreover, upon identifying C1×1 with C, if x, y ∈Cm,
(x, y)ℓ2(Cm) = (x, y)2 = y Hx = xHy = (y, x)2 = (y, x)ℓ2(Cm).
Furthermore, if A ∈Cm×n, x ∈Cn and y ∈Cm, then it follows that
(Ax, y)ℓ2(Cm) = y HAx =
 AHy
H x = (x, AHy)ℓ2(Cn),
where ( · , · )ℓ2(Cm) is the Euclidean inner product on Cm. For any x ∈Rn, xH = x⊺,
and for A ∈Rm×n, the conjugate transpose coincides with the transpose, A⊺.
Theorem 1.21 (properties of the conjugate transpose). Let A ∈Cm×n. Then
1. rank(A) = rank(AH) = rank(A⊺).
2. ker(A) = im(AH)⊥.
3. im(A)⊥= ker(AH).
Proof. We prove the second result and leave the ﬁrst and last to exercises; see
Problem 1.10.
(⊆) Let x ∈ker(A). By deﬁnition, Ax = 0 ∈Cm. Let z ∈im(AH), i.e., ∃y ∈Cm
for which z = AHy. Now compute
(z, x)2 = (AHy, x)2 = (y, Ax)2 = 0,
which shows that x ∈im(AH)⊥.
(⊇) Conversely, if x ∈im(AH)⊥, then 0 = (x, AHy)2 = (Ax, y)2 for every
y ∈Cm. Thus, Ax = 0.
Deﬁnition 1.22 (identity). The matrix In ∈Cn×n, deﬁned by
[In]i,j = δi,j,
is known as the matrix identity of order n.
Deﬁnition 1.23 (inverse). Let A ∈Cn×n. If there is B ∈Cn×n such that AB =
BA = In, then we say that A is invertible and call the matrix B an inverse of A.
In light of Problem 1.13, we denote the inverse of A by A−1.
Theorem 1.24 (properties of the inverse). Let A ∈Cn×n. Then A is invertible if
and only if rank(A) = n. Moreover, if A is invertible,
1. A−1 is invertible and (A−1)−1 = A.

1.2 Matrix Norms
9
2. AH is invertible and (AH)−1 = (A−1)H. In this case, we write
A−H = (AH)−1.
3. A⊺is invertible and (A⊺)−1 = (A−1)⊺. In this case, we write
A−⊺= (A⊺)−1.
4. For all α ∈C⋆= C\{0}, αA is invertible and (αA)−1 = 1
αA−1.
5. If B ∈Cn×n is also invertible, then the product AB is invertible and (AB)−1 =
B−1A−1.
Proof. See Problem 1.14.
Deﬁnition 1.25 (unitary matrices). Let A ∈Rm×m. We say that A is orthogonal
if and only if A−1 = A⊺. Similarly, for A ∈Cm×m, we say that A is unitary if and
only if AH = A−1.
1.2
Matrix Norms
Since, for any two vector spaces V and W, the set L(V, W) is a vector space itself,
we can think of ways of norming it. An immediate way of doing so is by simply
considering elements of Cm×n as a collection of mn numbers, i.e., by identifying
Cm×n with Cm·n.
Deﬁnition 1.26 (Frobenius norm2). Let A = [ai,j] ∈Cm×n. The Frobenius norm
is deﬁned via
∥A∥2
F =
m
X
i=1
n
X
j=1
|ai,j|2.
Deﬁnition 1.27 (max norm). The matrix max norm is deﬁned via
∥A∥max = max
1≤i≤m
1≤j≤n
|ai,j|
for all A = [ai,j] ∈Cm×n.
However, it turns out that it is often more useful when the norms on L(V, W)
are, in a sense, compatible with those of V and W.
Deﬁnition 1.28 (induced norm). Let (V, ∥· ∥V) and (W, ∥· ∥W) be complex, ﬁnite-
dimensional normed vector spaces. The induced norm on L(V, W) is
∥A∥L(V,W) = sup
x∈V⋆
∥Ax∥W
∥x∥V
,
∀A ∈L(V, W),
where V⋆= V\{0}. When V = W it is understood that ∥· ∥V = ∥· ∥W as well.
2 Named in honor of the German mathematician Ferdinand Georg Frobenius (1849–1917).

10
Linear Operators and Matrices
Remark 1.29 (convention). Regarding the last point, in our presentation, the
following object would not deﬁne an induced matrix norm:
∥A∥L(ℓp(Cn),ℓq(Cn)) = sup
x∈Cn⋆
∥Ax∥ℓq(Cn)
∥x∥ℓp(Cn)
,
∀A ∈Cn×n
for p ̸= q. While this deﬁnition is meaningful for every p, q ∈[1, ∞], and it indeed
deﬁnes a norm, we will only consider it to be an induced norm for p = q.
Deﬁnition 1.30 (matrix p-norm). Let A ∈Cm×n be given and p ∈[1, ∞]. The
induced L(ℓp(Cn), ℓp(Cm)) norm, called simply the induced matrix p-norm, is
denoted ∥A∥p and is deﬁned as
∥A∥p = sup
x∈Cn⋆
∥Ax∥ℓp(Cm)
∥x∥ℓp(Cn)
.
Proposition 1.31 (matrix 1-norm). Let A = [ai,j] = [ai,j] = [c1, . . . , cn] ∈Cm×n
be arbitrary. The induced matrix 1-norm, which is, by deﬁnition,
∥A∥1 = sup
x∈Cn⋆
∥Ax∥ℓ1(Cm)
∥x∥ℓ1(Cn)
,
may be calculated via the following formula:
∥A∥1 =
n
max
j=1
 m
X
i=1
|ai,j|
!
.
Proof. Given any x = [x1, . . . , xn]⊺∈Cn,
∥Ax∥ℓ1(Cm) =

n
X
j=1
xjcj

ℓ1(Cm)
≤
n
X
j=1
|xj|∥cj∥ℓ1(Cm)
≤
n
max
j=1 ∥cj∥ℓ1(Cm)
n
X
j=1
|xj|
=
n
max
j=1 ∥cj∥ℓ1(Cm)∥x∥ℓ1(Cn).
This shows that
∥A∥1 ≤
n
max
j=1 ∥cj∥ℓ1(Cm) =
n
max
j=1
 m
X
i=1
|ai,j|
!
.
On the other hand, there must be an index j0 where the maximum in the previous
inequality is attained. Choose x = ej0, the j0th canonical basis vector, and notice
then that
∥Ax∥ℓ1(Cm) = ∥cj0∥ℓ1(Cm).

1.2 Matrix Norms
11
It is not diﬃcult to see that the supremum in the deﬁnition of induced norm
is attained at this vector. This implies that the norm is the maximum absolute
column sum, i.e.,
∥A∥1 =
n
max
j=1
 m
X
i=1
|ai,j|
!
.
Deﬁnition 1.32 (sub-multiplicativity). Suppose that ∥· ∥: Cn×n →R is a matrix
norm, i.e., a norm on the vector space L(Cn). We say that the norm is sub-
multiplicative if and only if
∥AB∥≤∥A∥∥B∥,
∀A, B ∈Cn×n.
Deﬁnition 1.33 (consistency). Suppose that ∥· ∥Cn : Cn →R and ∥· ∥Cm : Cm →
R are norms, and ∥· ∥: Cm×n →R is a matrix norm. We say that ∥· ∥is consistent
with respect to the norms ∥· ∥Cn and ∥· ∥Cm if and only if
∥Ax∥Cm ≤∥A∥∥x∥Cn
for all A ∈Cm×n and x ∈Cn.
Proposition 1.34 (property of induced norms). Suppose that ∥· ∥Cn : Cn →R is
a norm on Cn and ∥· ∥: Cn×n →R is the induced matrix norm
∥A∥= sup
x∈Cn⋆
∥Ax∥Cn
∥x∥Cn ,
∀A ∈Cn×n.
Then ∥· ∥is a sub-multiplicative norm, and it is consistent with respect to ∥· ∥Cn.
Proof. See Problem 1.27.
Example 1.1
Let A ∈C1×n, i.e., A = aH for some a ∈Cn. Then Ax = (x, a)2,
so that
|Ax| = |(x, a)2| ≤∥x∥2∥a∥2.
In addition,
|Aa| = |(a, a)2| = ∥a∥2
2,
from which we may conclude that ∥A∥2 = ∥a∥2. This matrix A: Cn →C is a
prototype of an object called a linear functional.
Proposition 1.35 (norm of a unitary matrix). Let A ∈Cm×n be arbitrary and
Q ∈Cm×m be unitary. Then we have
∥QA∥2 = ∥A∥2.
Proof. Recall that, owing to Problem 1.16, for any unitary matrix we have ∥Qx∥2 =
∥x∥2. The result follows from this fact.

12
Linear Operators and Matrices
1.3
Eigenvalues and Spectral Decomposition
As a ﬁnal topic in this chapter we discuss eigenvalues and spectral decomposition
of square matrices. We begin with a deﬁnition.
Deﬁnition 1.36 (spectrum). Let A ∈Cn×n. We say that λ ∈C is an eigenvalue
of A if and only if there exists a vector x ∈Cn
⋆= Cn\{0} such that
Ax = λx.
This vector is called an eigenvector of A associated with λ. The spectrum of A,
denoted by σ(A), is the collection of all eigenvalues of A. The pair (λ, x) is called
an eigenpair of A.
Theorem 1.37 (properties of the spectrum). Let A ∈Cn×n. Then
1. λ ∈σ(A) if and only if ¯λ ∈σ(AH).
2. A is invertible if and only if 0 ̸∈σ(A).
3. The eigenvectors corresponding to distinct eigenvalues are linearly independent.
4. λ ∈σ(A) if and only if χA(λ) = 0, where χA is a polynomial of degree n, deﬁned
via
χA(λ) = det(λIn −A).
χA is called the characteristic polynomial.
5. There are at most n distinct complex-valued eigenvalues of A.
Proof. See Problem 1.28.
Since we are dealing with matrices with complex entries, the fundamental
theorem of algebra (see [18, Section 2.8]) implies that the characteristic polynomial
can be written as a product of factors, i.e.,
χA(λ) =
L
Y
i=1
(λ −λi)mi
(1.1)
with n = PL
i=1 mi.
Deﬁnition 1.38 (algebraic multiplicity). Let A ∈Cn×n be given. The number mi
in (1.1) is called the algebraic multiplicity of the eigenvalue λi.
Deﬁnition 1.39 (geometric multiplicity). Let A ∈Cn×n and λ ∈σ(A). Deﬁne the
eigenspace
E(λ, A) = {x ∈Cn | Ax = λx} .
This is a vector subspace of Cn; its dimension dim(E(λ, A)) is called the geometric
multiplicity of λ.
The following result gives a relation between the algebraic and geometric
multiplicities of an eigenvalue. For a proof of this result, see [44].
Theorem 1.40 (relation between multiplicities). Let A ∈Cn×n and λ ∈σ(A). The
geometric multiplicity of λ is not larger than the algebraic multiplicity of λ.

1.3 Eigenvalues and Spectral Decomposition
13
Deﬁnition 1.41 (triangular matrices). The square matrix A = [ai,j] ∈Cn×n is called
upper triangular if and only if ai,j = 0 for all i > j. A is called lower triangular
if and only if ai,j = 0 for all i < j. A matrix is called triangular if and only if it
is either upper or lower triangular. A is called diagonal if and only if ai,j = 0 for
all i ̸= j. A matrix A = [ai,j] ∈Cn×n is called unit lower triangular (unit upper
triangular) if and only if it is lower (upper) triangular and ai,i = 1, i = 1, . . . , n.
Deﬁnition 1.42 (similarity). Let A, B ∈Cn×n. We say that A and B are similar,
denoted by A ≍B, if and only if there is an invertible matrix S such that
A = S−1BS.
We say that matrix A is diagonalizable if it is similar to a diagonal matrix.
Proposition 1.43 (spectrum of similar matrices). Let A, B ∈Cn×n be such that
A ≍B. Then χA = χB and, consequently, σ(A) = σ(B). Furthermore, det(A) =
det(B) and tr(A) = tr(B).
Proof. See Problem 1.34.
Deﬁnition 1.44 (defective matrix). A matrix A ∈Cn×n is called defective if and
only if there is an eigenvalue λk with geometric multiplicity strictly smaller than
the algebraic multiplicity. Otherwise, the matrix is called nondefective.
One of the main results in the spectral theory of matrices is the following.
Theorem 1.45 (diagonalizability criterion). Let A ∈Cn×n be nondefective. Then
it is diagonalizable.
Proof. Let σ(A) = {λk}L
k=1, where λk ̸= λj, k ̸= j. For each k,
E(λk, A) = span({x(k)
1 , . . . , x(k)
mk }) = span(Sk),
where the set Sk = {x(k)
1 , . . . , x(k)
mk } is linearly independent. Then S = ∪L
k=1Sk is
a basis of Cn. Indeed, item 3 of Theorem 1.37 shows that the set S is linearly
independent. Moreover, #(S) = PL
k=1 mk = n, since the matrix A is nondefective.
Now set D = diag (λ1, . . . , λ1, . . . , λL, . . . , λL) and
X =


|
|
|
|
x(1)
1
· · ·
x(1)
m1
· · ·
x(L)
1
· · ·
x(L)
mL
|
|
|
|

,
where in D each eigenvalue λk appears exactly mk times. Notice now that, since
all the columns of X are linearly independent, we have rank(X) = n and this implies
that X is invertible.
Since, for all j = 1, . . . , mk, we have Ax(k)
j
= λkx(k)
j
, we see that
AX = A [x1, . . . , xn] = [Ax1, . . . , Axn]
and
XD = [λ1x1, . . . , λnxn] .
This implies that AX = XD, or, since X is invertible, A = XDX−1. In conclusion, A
is diagonalizable.

14
Linear Operators and Matrices
An important class of nondefective matrices are those that are self-adjoint, or
Hermitian. To investigate these, we use the Schur factorization. For a proof, again,
we refer to [44].
Lemma 1.46 (Schur normal form3). Let A ∈Cn×n. There are, not necessarily
unique, matrices U, R ∈Cn×n, with U unitary and R upper triangular, such that
A = URUH.
Notice that, in the setting of Lemma 1.46, we have that A ≍R and that, since
R is upper triangular, its diagonal entries coincide with its spectrum.
Proposition 1.47 (Spectral Decomposition Theorem). Let A ∈Cn×n be self-
adjoint (Hermitian), i.e., AH = A. Then σ(A) ⊆R and there is a unitary U ∈Cn×n
such that
A = UDUH,
where the matrix D = diag(λ1, . . . , λn). Furthermore, there exists an orthonormal
basis B = {u1, . . . , un} of eigenvectors of A for the space Cn and Aui = λiui,
i = 1, . . . , n.
Proof. From Lemma 1.46 we are guaranteed that there is a unitary matrix U ∈
Cn×n and an upper triangular matrix D ∈Cn×n such that
A = UDUH.
But, since A is self-adjoint,
AH = UDHUH = UDUH = A.
This implies that DH = D, i.e., D is self-adjoint. Since D is triangular, it must
be diagonal. Furthermore, the diagonal elements of D must be real. Otherwise, D
could not be self-adjoint. Therefore, we have the desired factorization.
Now the eigenvalues of a diagonal matrix are precisely its diagonal entries. Since
A is similar to the diagonal matrix D, the eigenvalues of A are precisely λi = di,i ∈R,
i = 1, . . . , n.
Finally, observe that the columns of U form an orthonormal basis for Cn. Indeed,
suppose that the kth column of U is denoted uk. Then AU = UD if and only if
Auk = dk,kuk = λkuk.
Thus, the eigenvectors of A, namely uk, k = 1, . . . , n, form an orthonormal basis
for Cn: (uk, uj)2 = uH
j uk = δk,j, k, j = 1, . . . , n.
Notice that the previous result shows that, for A self-adjoint, there exists an
orthonormal basis of Cn consisting of eigenvectors of A. This is a result that is
used countless times in the text.
There are numerous generalizations of the last theorem. We will be interested in
one that is rather straightforward to establish. First, we need what is perhaps an
obvious deﬁnition.
3 Named in honor of the Russian-born German–Israeli mathematician Issai Schur (1875–1941).

Problems
15
Deﬁnition 1.48 (eigenvalue). Suppose that V is a complex vector space and A ∈
L(V). The scalar λ ∈C for which there is w ∈V \ {0} such that
Aw = λw,
is called an eigenvalue of A and w is a corresponding eigenvector. The spectrum
of A, σ(A), is the set of all eigenvalues of A. The pair (λ, w) is called an
eigenpair of A.
For self-adjoint operators we have the following general result.
Theorem 1.49 (Spectral Decomposition Theorem). Suppose that (V, ( · , · )) is
an n-dimensional complex inner product space and A ∈L(V) is self-adjoint. Then
there are precisely n eigenvalues, counting multiplicities, and σ(A) ⊆R. Moreover,
there is an orthonormal basis B = {w1, . . . , wn} of eigenvectors of A for the space
V: (wi, wj) = δi,j, i, j = 1, . . . , n.
Proof. A proof for this is, for instance, furnished by the theory developed in
Chapter 7.
Finally, the class of normal matrices, which contains as a proper subset the class
of Hermitian matrices, is sometimes important.
Deﬁnition 1.50 (normal matrix). The square matrix A ∈Cn×n is called normal if
and only if AHA = AAH.
We will need the following technical lemma.
Lemma 1.51 (normal and triangular). Suppose that A ∈Cn×n is normal and upper
triangular. Then it must be diagonal.
Proof. See Problem 1.45.
Theorem 1.52 (diagonalization of normal matrices). Suppose that A ∈Cn×n is
normal. Then A is unitarily diagonalizable, i.e., there is a unitary matrix U ∈Cn×n
and a diagonal matrix D ∈Cn×n such that
A = UDUH.
Proof. Use the Schur factorization and Lemma 1.51. See Problem 1.46.
Corollary 1.53 (orthonormal basis). Suppose that A ∈Cn×n is normal. There is
an orthonormal basis of eigenvectors of A for Cn.
Proof. Repeat the construction of Proposition 1.47.
Problems
1.1
Let (V, ∥· ∥) be a ﬁnite-dimensional normed space and A ∈L(V). Does
∥· ∥A = ∥A · ∥: V →R
deﬁne a norm? Why or why not?
1.2
Prove Proposition 1.2.

16
Linear Operators and Matrices
1.3
Prove Proposition 1.4.
1.4
For A ∈Cm×n, prove that im(A) ≤Cm (i.e., im(A) is a vector subspace
of Cm) and ker(A) ≤Cn (i.e., ker(A) is a vector subspace of Cn).
1.5
Suppose that A ∈Cm×n. Prove that im(A) = CA, where im(A) is the range
of A and CA is its column space.
1.6
Suppose that A ∈Cm×n with m ≥n. Prove that the following are equivalent:
a)
rank(A) = n.
b)
A maps no two distinct vectors in Cn to the same vector in Cm.
c)
ker(A) = {0}.
1.7
Let A ∈Cm×n. Prove that im(A)⊥= ker(AH).
1.8
Prove Proposition 1.19.
1.9
Show that the deﬁnitions “adjoint” and the “conjugate transpose” coincide
for matrices when we use the canonical inner product
(x, y)ℓ2(Cm) = (x, y)2 = y Hx
for Cm.
1.10
Complete the proof of Theorem 1.21.
1.11
Show that In ∈Cn×n acts as multiplicative identity with respect to matrix
multiplication. In other words, for every A ∈Cn×n, we have
AIn = InA = A.
1.12
Suppose that C ∈Cn×n is invertible and the set S = {w 1, . . . , w k} ⊆
Cn is linearly independent. Prove that CS = {Cw 1, . . . , Cw k} ⊆Cn is linearly
independent.
1.13
Suppose that A ∈Cn×n is invertible. Prove that its inverse must be unique.
1.14
Prove Theorem 1.24.
1.15
Let A ∈Cm×n. Prove that rank(A) = rank(AB) for any B ∈Cn×n that is
invertible.
1.16
Let U ∈Cn×n be unitary. Show that, for any x, y ∈Cn, we have (Ux, Uy)2 =
(x, y)2, so that ∥Ux∥2 = ∥x∥2.
1.17
Show that the Frobenius and matrix max norms are indeed norms on the
vector space L(Cn, Cm).
1.18
Show that
∥A∥2
F = tr(AHA) = tr(AAH),
where, for any square matrix, M = [mi,j] ∈Cn×n, tr(M) = Pn
i mi,i denotes its
trace.
1.19
Let V and W be ﬁnite-dimensional complex-normed vector spaces. Show
that the induced norm is indeed a norm on the vector space L(V, W). Prove that
∥A∥L(V,W) = sup {∥Ax∥W | x ∈V, ∥x∥V = 1} .
1.20
Let, for a, b ∈R,
A =
a
b
b
a

.
Show that ∥A∥1 = ∥A∥2 = ∥A∥∞.

Problems
17
1.21
Let, for a, b ∈R,
A =
a
b
b
−a

.
Show that ∥A∥2 = (a2 + b2)1/2.
1.22
Show that
∥A∥∞= max
1≤i≤n
n
X
j=1
|aij|,
∀A ∈Cn×n,
and also that ∥A∥1 =
AH
∞.
1.23
Show that, for every A ∈Cn×n,
1
√n∥A∥2 ≤∥A∥∞≤√n∥A∥2.
1.24
Show that
∥A∥2
2 ≤∥A∥1∥A∥∞,
∀A ∈Cn×n.
1.25
Show that, for every A ∈Cn×n,
∥A∥max ≤∥A∥∞≤n ∥A∥max ,
(1.2)
where ∥· ∥∞is the induced matrix ∞-norm, and recall that
∥A∥max = max
1≤i,j≤n |ai,j|
is the matrix max norm.
1.26
Let A ∈Rn×n be such that A⊺= A and tr A = 0. Show that
∥A∥2
2 ≤n −1
n
∥A∥2
F .
Is the assumption that tr A = 0 essential? You may justify your answer with an
example or counterexample.
1.27
Prove Proposition 1.34.
1.28
Prove Theorem 1.37.
1.29
Show that, for every A ∈Cn×n,
∥A∥2 =
max
λ∈σ(AHA)
√
λ.
Hint: You need some facts about the eigenvalues and eigenvectors of Hermitian
matrices.
1.30
Suppose that ∥· ∥: Cm×n →R is the induced norm with respect to the
vector norms ∥· ∥Cm and ∥· ∥Cn and that A ∈Cm×n. Prove that the function
∥A( · )∥Cm : Cn →R is uniformly continuous. Use this fact to prove that there is a
vector x ∈Sn−1
Cn
such that
∥A∥= ∥Ax∥Cm .

18
Linear Operators and Matrices
1.31
Suppose that ∥· ∥: Cn×n →R is the induced norm with respect to the
vector norm ∥· ∥: Cn →R. Let A ∈Cn×n be invertible. Prove that
1
∥A−1∥= min
y∈Cn⋆
∥Ay∥
∥y∥.
1.32
Let Tk, T ∈Cn×n, k = 1, 2, be lower triangular matrices.
a)
Show that T1T2 is lower triangular.
b)
If T1 and T2 are unit lower triangular, show that T1T2 is unit lower triangular.
c)
If [T]i,i ̸= 0, show that T is invertible and T−1 is lower triangular.
d)
If T is unit lower triangular, prove that it is invertible and T−1 is unit lower
triangular.
e)
If [T]i,i > 0, show that

T−1
i,i =
1
[T]i,i > 0.
1.33
Show that if A ∈Cn×n is both unitary (i.e., AAH = AHA = In) and triangular,
then it is diagonal.
Hint: You need a fact about the inverse of a triangular matrix.
1.34
Prove Proposition 1.43.
1.35
Let A ∈Cn×n. Suppose that λ1, . . . , λk are distinct eigenvalues of A and
suppose that x1, . . . , xk are eigenvectors associated with the respective eigenvalues.
Prove that {x1, . . . , xk} is linearly independent.
1.36
Let A ∈Cn×n. Prove that if A has n distinct eigenvalues, then A is
diagonalizable.
1.37
Let A ∈Cn×n be Hermitian, i.e., AH = A.
a)
Prove directly that all eigenvalues of A are real.
b)
Prove that if x and y are eigenvectors associated with distinct eigenvalues,
then they are orthogonal, i.e., xHy = 0.
1.38
Let A ∈Cm×n and B ∈Cn×m. Show that σ(AB)\{0} = σ(BA)\{0}, i.e.,
the nonzero eigenvalues of AB and BA coincide.
1.39
Let A ∈Cm×n. Show that σ(AHA) ∪σ(AAH) ⊆[0, ∞).
1.40
Let A ∈Cn×n and λ ∈σ(A). The vector y ∈Cn
⋆is called a left eigenvector
associated with λ if and only if y HA = λy H. Now suppose that λ, µ ∈σ(A) are
distinct. Let y be a left eigenvector associated with λ and x be a right (usual)
eigenvector associated with µ. Prove that y Hx = 0.
1.41
Let A ∈Cn×n be skew-Hermitian.
a)
Prove directly that the eigenvalues of A are purely imaginary.
b)
Prove that if x and y are eigenvectors associated with distinct eigenvalues,
then they are orthogonal, i.e., xHy = 0.
c)
Show that I −A is nonsingular.
d)
Prove that Q = (I −A)−1(I + A) is unitary.
1.42
Let u, v ∈Cn. Set A = In + uv H ∈Cn×n.
a)
Suppose that A is invertible. Prove that A−1 = In + αuv H for some α ∈C.
Give an expression for α.
b)
For what u and v is A singular, i.e., not invertible?
c)
Suppose that A is singular. What is the kernel space of A, ker(A), in this case?
1.43
Suppose that q ∈Cn, ∥q∥2 = 1. Set P = I −qqH.
a)
Find im(P).

Problems
19
b)
Find ker(P).
c)
Find the eigenvalues of P.
1.44
Characterize the eigenvalues of a unitary matrix.
1.45
Prove Lemma 1.51.
1.46
Prove Theorem 1.52.

2
The Singular Value Decomposition
In Chapter 1, we saw that if A ∈Cn×n is Hermitian, then it is unitarily
diagonalizable, i.e., there is a diagonal matrix D ∈Rn×n and a unitary matrix
U ∈Cn×n such that
A = UDUH.
This gives us, at least for this class of matrices, a nice geometric interpretation of
the action of a matrix on a vector.
1. Since UH is unitary and
UHx

2 = ∥x∥2, the action of UH is essentially that of
a rotation/reﬂection (it does not change the magnitude of the vector).
2. The matrix D is diagonal; its action is a (signed) dilation in each coordinate
direction.
3. Finally, U =
 UH−1 reverses the rotation/reﬂection implemented by UH.
Now suppose that the elements of D = diag(λ1, . . . , λn), i.e., the eigenvalues of
A, are ordered by magnitude:
|λ1| ≥|λ2| ≥· · · ≥|λn| ≥0.
It is not hard to show that A is the sum of rank-one matrices (matrices of the type
µuv H):
A =
n
X
i=1
λiuiuH
i ,
where U = [u1, . . . , un]. Thus, the action of A in matrix–vector multiplication is
quite simple: for any x ∈Cn,
Ax =
n
X
i=1
λi (x, ui)2 ui.
Next, suppose that, after some index r ∈{1, 2, . . . , n−1}, the eigenvalues are either
very small in magnitude relative to |λr| > 0 or zero. Then it seems reasonable that
A ≈
r
X
i=1
λiuiuH
i
=⇒
Ax ≈
r
X
i=1
λi (x, ui)2 ui.
In some special circumstances, we have r ≪n, so that the last approximations
are very inexpensive to assemble (once we have the spectral decomposition, of
course). This is the idea of data compression and low-rank approximation, which

2.1 Reduced and Full Singular Value Decompositions
21
are related to principal component analysis (PCA). One tries to reduce matrices to
their primary (or principal) components. For some matrices with special structure,
r can be quite small, and the information of that matrix can be signiﬁcantly
compressed into just a handful of numbers and vectors. We will make these
arguments above rigorous, and, in fact, generalize them, with the proof of the
Eckart–Young Theorem (2.15) later in the chapter.
Of course, not all matrices are Hermitian. We would like to have an analogue of
unitary diagonalization for generic, nonsquare matrices. This is the purpose of the
so-called singular value decomposition (SVD).
2.1
Reduced and Full Singular Value Decompositions
Deﬁnition 2.1 (SVD). Let A ∈Cm×n. A singular value decomposition (SVD)
of the matrix A is a factorization of the form
A = UΣVH,
where U ∈Cm×m and V ∈Cn×n are unitary, Σ ∈Rm×n is diagonal — meaning that
[Σ]i,j = 0, for i ̸= j — and the diagonal entries [Σ]i,i = σi are nonnegative and in
nonincreasing order: σ1 ≥σ2 ≥· · · ≥σp ≥0 with p = min(m, n). The elements
of the Σ are called the singular values of A. The columns of U and V are called
the left and right singular vectors, respectively.
Remark 2.2 (reduced SVD). Let us, for the sake of deﬁniteness, assume that
m ≥n. If A ∈Cm×n has an SVD, then we can write
Av j = σjuj,
j = 1, . . . , n,
or, equivalently,
A[v 1, . . . , v n] = [u1, . . . , un]


σ1
...
σn

.
In other words, we have obtained the representation
AV = ˆUˆΣ,
where:
1. ˆΣ ∈Rn×n is square and diagonal with nonnegative diagonal entries.
2. ˆU ∈Cm×n has orthonormal columns.
3. V ∈Cn×n is unitary.
Writing this another way, we have
A = ˆUˆΣVH.
This is the so-called reduced SVD of a matrix. The standard SVD implies that
the existence of the reduced SVD. Conversely, if one wants to obtain the full SVD

22
The Singular Value Decomposition
from the reduced SVD, we observe that the columns of ˆU can be completed to a
full orthonormal basis of Cm (using the Gram–Schmidt process described in Section
A.5) to obtain
U = [u1, . . . , un, un+1, . . . , um] ∈Cm×m,
which is square and unitary. Next, we deﬁne
Σ =
ˆΣ
O

,
where O ∈C(m−n)×n is a matrix of zeros. It is easy to see that the reduced SVD
is equivalent to the representation
A = UΣVH.
A similar conclusion can be reached if m < n, where in this case the zero padding
on the matrix Σ occurs to the right of ˆΣ.
2.2
Existence and Uniqueness of the SVD
Let us now show that every matrix has an SVD.
Theorem 2.3 (existence of SVD). Every matrix A ∈Cm×n has a singular value
decomposition. The singular values are unique and

σ2
i
	p
i=1 =
(
σ(AHA)
if
m ≥n,
σ(AAH)
if
m ≤n,
where p = min(m, n). Recall that the symbol σ(B) stands for the spectrum of the
square matrix B.
Proof. (existence) Let us set
σ1 = ∥A∥2 =
sup
∥x∥ℓ2(Cn)=1
∥Ax∥ℓ2(Cm) .
Arguing by compactness, and using Theorem B.47, there is a vector v 1 ∈Cn with
∥v 1∥ℓ2(Cn) = 1 such that ∥A∥2 = ∥Av 1∥ℓ2(Cm) = σ1. Deﬁne u1 = ∥A∥−1
2 Av 1. Then
Av 1 = σ1u1,
∥u1∥ℓ2(Cm) = 1.
Using the Gram–Schmidt orthogonalization process described in Section A.5, we
can extend {v 1} to an orthonormal basis of Cn and {u1} to an orthonormal basis
of Cm. In doing so, we obtain matrices
U1 = [u1, . . . , um] ∈Cm×m,
V1 = [v 1, . . . , v n] ∈Cn×n,
which are unitary and, more importantly, satisfy
AV1 = U1
σ1
w H
0
B

= U1S
for some w ∈Cn−1 and B ∈C(m−1)×(n−1).

2.2 Existence and Uniqueness of the SVD
23
Notice that
S
σ1
w

=
σ1
w H
0
B
 σ1
w

=
σ2
1 + w Hw
Bw

.
Therefore,
S
σ1
w

ℓ2(Cm)
=
q
(σ2
1 + w Hw)2 + ∥Bw∥2
ℓ2(Cm−1)
≥σ2
1 + w Hw
=
 σ2
1 + w Hw
1/2

σ1
w

ℓ2(Cm)
,
which shows the lower bound ∥S∥2 ≥
 σ2
1 + w Hw
1/2. On the other hand, since
UH
1 AV1 = S, and U1 and V1 are unitary, then we must have that σ1 = ∥A∥2 = ∥S∥2,
which forces us to conclude that w = 0. This shows the result if n = 1 or m = 1.
Otherwise, we proceed by induction.
For the induction step, we factorize A ∈Cm×n as above,
AV1 = U1
σ1
0
0
B

,
and assume that B has an SVD, say B = U2Σ2VH
2 . Therefore, we have
A = U1
1
O
O
U2
 σ1
0
0
Σ2
 1
O
O
VH
2

VH
1 ,
which is the sought-after SVD for A with
U = U1
1
O
O
U2

,
Σ =
σ1
O
O
Σ2

,
V = V1
1
O
O
V2

.
The existence part is ﬁnished by induction.
(uniqueness) Next, notice that
AHA =
 UΣVHHUΣVH = VΣ⊺UHUΣVH = VΣ⊺ΣVH
and
AAH = UΣVH UΣVHH = UΣΣ⊺UH,
where Σ⊺Σ ∈Rn×n and ΣΣ⊺∈Rm×m are diagonal matrices with the diagonal
entries σ2
1, . . . , σ2
p, plus zeros for padding, as needed. Thus, AHA ≍Σ⊺Σ and
AAH ≍ΣΣ⊺. Using this fact and the fact that eigenvalues are uniquely determined
proves the result.
The uniqueness results for the right and left singular vectors are a little more
subtle. We have for instance the following result.
Theorem 2.4 (uniqueness of singular vectors). Suppose that A ∈Cm×n with
m ≥n. If
A = U1Σ1VH
1 = U2Σ2VH
2

24
The Singular Value Decomposition
are two SVDs for A, then Σ1 = Σ2, the columns of V1 and V2 form an orthonormal
basis of eigenvectors of AHA, and if AHA has n distinct eigenvalues, then
V1 = V2D
for some D = diag[eiθ1, . . . , eiθn] with angles θi ∈R, i = 1, . . . , n. Finally, if
rank(A) = n (A has full rank) and
A = U1ΣVH = U2ΣVH
are two SVDs for A, then the ﬁrst n columns of U1 and U2 are equal.
Proof. The fact that Σ1 = Σ2 was already proved in Theorem 2.3. Suppose now
that AHA has n distinct eigenvalues and
A = U1ΣVH
1 = U2ΣVH
2 ,
where we have used the fact that the singular values are uniquely determined. Also
from Theorem 2.3,
AHA = V1Σ⊺ΣVH
1 = V2Σ⊺ΣVH
2 ,
which proves that the columns of V1 and V2 are eigenvectors of AHA.
Now, if the eigenvalues λi = σ2
i
of AHA are all simple, the corresponding
eigenspaces E(λi, AHA) are all one dimensional and
E(λi, AHA) = span{v 1,i} = span{v 2,i},
i = 1, . . . , n,
with
∥v 1,i∥ℓ2(Cn) = ∥v 2,i∥ℓ2(Cn) = 1,
i = 1, . . . , n,
where Vk = [v k,1, . . . , v k,n], k = 1, 2. The only possibility is that v 1,i = γiv 2,i,
where |γi| = 1, for i = 1, . . . , n. This proves that V1 = V2D, where D is a diagonal
matrix with the required structure.
Finally, if
A = U1ΣVH = U2ΣVH
are two SVDs for A, then we have the following family of equations:
Av i = σiuk,i,
k = 1, 2,
i = 1, . . . , n,
where Uk = [uk,1, . . . , uk,m], k = 1, 2. Since A has full rank and σi > 0, u1,i = u2,i,
i = 1, . . . , n is the only possibility.
Remark 2.5 (geometric interpretation of the SVD). Let b ∈Cm and expand it in
the basis of left singular vectors U. This gives the coordinate vector b′ = UHb. Do
the same for x ∈Cn to obtain its coordinate vector x′ = VHx. Once we have this,
notice that, if b = Ax, we can proceed as follows:
b′ = UHb = UHAx = UHUΣVHx = ΣVHx = Σx′.
In other words, the SVD is essentially saying that every matrix, once proper bases
for the domain and target spaces are chosen, may be viewed as a diagonal matrix.

2.3 Further Properties of the SVD
25
2.3
Further Properties of the SVD
The main motivation for the SVD was to try to construct an analogue of the
spectral decomposition, which we know is only valid for square, nondefective
matrices. Let us study now the relation between these two constructions, which in
principle are not related to each other.
Theorem 2.6 (SVD and rank). Let A ∈Cm×n. Then rank(A) coincides with the
number of nonzero singular values.
Proof. We write the SVD: A = UΣVH. Since U and V are unitary, they are full
rank. By Theorem 1.16, rank(A) = rank(Σ) and since Σ is diagonal, the assertion
follows.
Theorem 2.7 (range and kernel through SVD). Let A ∈Cm×n with rank(A) = r.
Suppose that an SVD for A is given by A = UΣVH, where u1, . . . , um denote the
columns of U and v 1, . . . , v n denote the columns of V. Then
⟨u1, . . . , ur⟩= im(A)
and
⟨v r+1, . . . , v n⟩= ker(A).
Proof. (⊆) From the SVD one can easily write Av i = σiui, for i = 1, . . . , r. This
proves immediately that ui ∈im(A), for i = 1, . . . , r. Since im(A) is a subspace of
Cm, any linear combination of u1, . . . , ur is in im(A). Hence, ⟨u1, . . . , ur⟩⊆im(A).
(⊇) Let y ∈im(A). Then there exists x ∈Cn such that Ax = y. This implies that
UΣVHx = y for some x. Let x′ = VHx. Then, for some x′ ∈Cn, UΣx′ = y. Set
x′′ = Σx′. Note that x′′ ∈Cm and x′′
r+1 = · · · = x′′
m = 0. Hence, for some x′′ ∈Cm,
Ux′′ = y. Now we write
y = Ux′′ =
m
X
j=1
x′′
j uj =
r
X
j=1
x′′
j uj ∈⟨u1, . . . , ur⟩.
This proves that ⟨u1, . . . , ur⟩⊇im(A), and we are done.
(⊆) From the SVD one can easily write Av i = 0, for i = r + 1, . . . , n. This proves
immediately that v i ∈ker(A), for i = r + 1, . . . , n. Since ker(A) is subspace of
Cn, any linear combination of v r+1, . . . , v n is in ker(A). Hence, ⟨v r+1, . . . , v n⟩⊆
ker(A).
(⊇) Let x ∈ker(A). Then Ax = 0. This implies that UΣVHx = 0. Let x′ = VHx.
This implies that x = Vx′ and UΣx′ = 0. Since U is invertible, this implies that
Σx′ = 0. This homogeneous system always has a solution of the form
x′ =


x′
1 = 0
...
x′
r = 0
x′
r+1 = αr+1
...
x′
n = αn


,

26
The Singular Value Decomposition
where αr+1, . . . , αn are arbitrary. But this shows that
x = Vx′ =
n
X
j=1
x′
j v j =
n
X
j=r+1
αjv j ∈⟨v r+1, . . . , v n⟩.
This proves that ⟨v r+1, . . . , v n⟩⊇ker(A), and we are done.
The last result gives another proof of the famous Rank-Plus-Nullity Theorem;
see Theorem 1.16.2.
Corollary 2.8 (Rank-Plus-Nullity Theorem). Suppose that A ∈Cm×n. Then
rank(A) + nullity(A) = dim(im(A)) + dim(ker(A)) = n.
Theorem 2.9 (AHA is nonsingular). Suppose that A ∈Cm×n with m ≥n. AHA is
nonsingular if and only if rank(A) = n.
Proof. Let A = UΣVH be an SVD for A. Then AHA = VΣ⊺ΣVH yields a unitary
diagonalization of AHA. Note that
Σ⊺Σ = diag

σ2
1, . . . , σ2
n

,
where σ1, . . . , σn are the singular values of A, some of which may be zero.
It is clear that rank(A) = r, where r is the number of nonzero singular values.
Of course, it must be that r ≤n. Likewise, rank(AHA) is the number of nonzero
elements on the diagonal of ΣHΣ. This number must also be r. In other words,
rank(A) = r = rank(AHA),
which proves the result.
Theorem 2.10 (SVD and norms). Let A ∈Cm×n. Then ∥A∥2 = σ1 and ∥A∥2
F =
Pr
i=1 σ2
i .
Proof. The ﬁrst statement is by construction. The second follows from Problem
1.18 and the fact that U and V are unitary. In this case,
UΣVH
F =
ΣVH
F =
∥Σ∥F .
Theorem 2.11 (SVD and self-adjoint matrices). If A is Hermitian, i.e., AH = A,
then the singular values of A are the absolute values of its eigenvalues. If A is
Hermitian with nonnegative eigenvalues,1 then the eigenvalues and the singular
values coincide.
Proof. Since A is self-adjoint, it is orthogonally diagonalizable and σ(A) ⊂R, i.e.,
A = QΛQH = Q|Λ| sgn(Λ)QH,
where the notation has the obvious meaning. Deﬁne U = Q and VH = sgn(Λ)QH
to obtain the SVD of A. Conclude by uniqueness.
1 We will see in Chapter 3 that this is a special class of matrices called Hermitian positive
semi-deﬁnite (HPSD) matrices.

2.4 Low Rank Approximations
27
Theorem 2.12 (SVD and determinants). Let A ∈Cn×n. Then
| det(A)| =
n
Y
i=1
σi.
Proof. By the usual rules for determinants
| det(A)| = | det(UΣVH)| = | det(U)|| det(Σ)|| det(VH)| = | det(Σ)| =
n
Y
i=1
σi,
where we have used the facts that the determinant of a unitary matrix has modulus
one, the determinant of a diagonal matrix is the product of its diagonals, and Σ is
diagonal with nonnegative entries.
2.4
Low Rank Approximations
Now this is where the SVD really shines. If we can ﬁnd an SVD for a matrix A, we
can analyze the singular values and use the SVD to compress the information in A
in an optimal way.
Deﬁnition 2.13 (rank-one matrix). Given u ∈Cm
⋆, v ∈Cn
⋆, and σ ∈C⋆, the matrix
A ∈Cm×n deﬁned via
A = σuv H = σu ⊗v
is called a rank-one matrix.
For every x ∈Cn, the rank-one matrix A, deﬁned above, acts as
Ax = σ(x, v)2u ∈span{u}.
This shows, as a consequence of Corollary 1.14, that the rank of this matrix is
exactly equal to one. This justiﬁes the name. The question we want to address
now is whether every matrix can be represented (or at least approximated) by
linear combinations of rank-one matrices.
Theorem 2.14 (Rank-One Decomposition Theorem). Let A ∈Cm×n be such that
r = rank(A) and A = UΣVH is an SVD. Then A is a linear combination of r
rank-one matrices
A =
r
X
j=1
σjuj ⊗v j.
(2.1)
Proof. It suﬃces to write Σ as the sum of matrices of the form
Σj = diag[0, . . . , 0, σj, 0, . . . , 0],
where the element σj is in the jth entry. The rest of the details are left to the
reader as an exercise; see Problem 2.6.

28
The Singular Value Decomposition
Let us now prove the so-called Eckart–Young low rank approximation theorem,
which states that truncating the SVD of a matrix gives, in a sense, the best low
rank approximation to it.
Theorem 2.15 (Eckart–Young Theorem2). Let A ∈Cm×n be such that r =
rank(A). Let A = UΣVH be an SVD of A. For k < r deﬁne Ak = Pk
j=1 σjuj ⊗v j.
Let us denote by Ck the collection of all matrices B ∈Cm×n such that rank(B) ≤k.
Then Ak is of rank k and
∥A −Ak∥2 = σk+1 = inf
B∈Ck ∥A −B∥2.
Furthermore,
∥A −Ak∥F =
v
u
u
t
r
X
j=k+1
σ2
j = inf
B∈Ck ∥A −B∥F .
Proof. The values of the norms of the diﬀerence A−Ak follow from the representa-
tion (2.1) and Theorem 2.10. Next, it is straightforward to see that rank(Ak) = k.
To show that the ﬁrst inﬁmum is attained at Ak, we use a contradiction
argument. Namely, let us assume that there is a matrix B ∈Ck such that
∥A −B∥2 < ∥A −Ak∥2 = σk+1.
Since rank(B) ≤k, by Theorem 1.16, dim ker(B) ≥n −k. There is a subspace
W1 ≤ker(B) such that dim(W1) = n −k and, for every w ∈W1, Bw = 0.
For any w ∈W1, it follows that Aw = (A −B)w and
∥Aw∥2 = ∥(A −B)w∥2 ≤∥A −B∥2∥w∥2 < σk+1∥w∥2.
Next, deﬁne W2 = span{v 1, . . . , v k+1}. We claim that, for every w ∈W2, ∥Aw∥2 ≥
σk+1∥w∥2. To see this, observe that w ∈W2 can be written as w = Pk+1
i=1 βiv i,
for some β1, . . . , βk+1 ∈C. By orthonormality, it follows that
∥w∥2 =
v
u
u
t
k+1
X
i=1
|βi|2.
Using orthonormality again, we see that
∥Aw∥2 =

k+1
X
i=1
σiβiui

2
=
v
u
u
t
k+1
X
i=1
σ2
i |βi|2 ≥σk+1
v
u
u
t
k+1
X
i=1
|βi|2 = σk+1 ∥w∥2 .
Thus, W1 ≤Cn and W2 ≤Cn, and the sum of the dimensions of these subspaces
exceeds n. Therefore, there must be a nonzero vector in their intersection. But this
yields a contradiction, because such a vector z ∈W1 ∩W2\{0} would satisfy
σk+1 ∥z∥2 ≤∥Az∥2 < σk+1∥z∥2.
The proof for the second case involving the Frobenius norm is left to the reader
as an exercise; see Problem 2.7.
2 Named in honor of the American physicist Carl Henry Eckart (1902–1973) and the American
engineer Gale J. Young (1912–1990).

Problems
29
Example 2.1
Consider the matrix
A =


2.905 7
6.545 7
3.858 7
4.673 7
2.417 1
3.870 3
0.992 2
2.646 0
0.888 9
3.254 2
1.757 4
6.809 9
1.435 7
6.211 4
2.126 8
3.427 0
2.625 5
2.603 0
3.022 8
2.109 3
2.245 9
2.168 6
2.677 8
3.865 3
3.641 6
3.691 7
3.355 3
2.268 9
3.892 7
6.630 1
6.486 3
6.819 2
5.028 1
6.091 0
2.552 1
4.046 1
6.279 8
3.171 3
2.788 0
6.163 2
1.645 6
5.812 0
2.961 2
1.982 0
6.101 8
5.175 4
6.464 6
5.031 6
4.203 1


.
The singular values are, to 13 decimal digits of precision,
σ1 = 27.775 450 511 276 4,
σ2 = 08.024 842 310 514 9,
σ3 = 05.224 556 262 211 5,
σ4 = 00.000 196 585 865 6,
σ5 = 00.000 085 666 006 1,
σ6 = 00.000 062 891 962 9,
σ7 = 00.000 007 169 799 2.
This matrix is very nearly singular and is well approximated by the rank-three
(compressed) matrix
A3 =
3
X
i=1
σiuiv ⊺
i ,
where ui, v ∈R7 are the singular vectors, which are suppressed for brevity. In other
words, to a good approximation, there are really only three important components
of A — namely, σiuiv ⊺
i , i = 1, 2, 3 — that express its action. In particular, according
to the Eckart–Young Theorem 2.15, the relative error in the compressed matrix is
relatively small,
∥A −A3∥2
∥A∥2
= σ4
σ1
= 7.077 68 × 10−6.
Problems
2.1
Let A, B ∈Cn×n be unitarily equivalent. Prove that they have the same
singular values.
2.2
Show that if A is real, then it has a real SVD (U ∈Rm×m and V ∈Rn×n).
2.3
Show that any matrix in Cm×n is the limit of a sequence of matrices of full
rank.
2.4
Suppose that A ∈Cm×m has an SVD A = UΣVH. Find an eigenvalue
decomposition of the matrix
O
AH
A
O

.

30
The Singular Value Decomposition
2.5
Let A ∈Cm×n with rank(A) = r. If A has the SVD A = UΣVH, the Moore–
Penrose pseudo-inverse of A is deﬁned by
A† = VΣ†UH,
where Σ† = diag[σ−1
1 , . . . , σ−1
r , 0, . . . , 0] ∈Rn×m. Show the following:
a)
If A is square and A−1 exists, then A† = A−1.
b)
If m ≥n and A has full rank, then A† = (AHA)−1AH.
c)
AA†A = A.
d)
A†AA† = A†.
2.6
Complete the proof of Theorem 2.14.
2.7
Complete the proof of Theorem 2.15.
2.8
Let A ∈Cm×n. Use the SVD to prove the following:
a)
rank
 AHA

= rank
 AAH
= rank(A) = rank
 AH
.
b)
AHA and AAH have the same nonzero eigenvalues.
c)
If the eigenvectors w1 and w2 of AHA are orthogonal, then Aw1 and Aw2 are
orthogonal.
2.9
The purpose of this problem is to provide another proof of the existence of an
SVD for any matrix A ∈Cm×n. Assume, for the sake of deﬁniteness, that m ≥n.
a)
Show that AAH = UΣΣ⊺UH, where U ∈Cm×m is unitary and Σ ∈Rm×n is
diagonal with nonnegative diagonal entries.
Hint: Recall Problem 1.39.
b)
Show that AHA = VΣ⊺ΣVH, where V ∈Cn×n is unitary and Σ is the same
matrix from the previous item.
Hint: Recall Problem 1.38.
c)
Show that A = UΣVH.
Hint: Take V = [v 1, . . . , v n] as given in part (b). Deﬁne ˆui = Av i/σi, i =
1, . . . , r = rank(A). Show that these newly deﬁned vectors can be identiﬁed
as the ﬁrst r vectors of the matrix U from part (a).

3
Systems of Linear Equations
In this chapter, we will be concerned with the following problem: Given the matrix
A = [ai,j] ∈Cn×n and the vector f = [fi] ∈Cn, ﬁnd x = [xi] ∈Cn such that
Ax = f .
(3.1)
Of course, this is shorthand for the following system of linear equations:













a1,1x1 + a1,2x2 + · · · + a1,nxn = f1,
a2,1x1 + a2,2x2 + · · · + a2,nxn = f2,
...
an,1x1 + an,2x2 + · · · + an,nxn = fn.
We call A the coeﬃcient matrix. First of all, we need to make sure that a
solution exists and is unique. The following result is nothing but a recapitulation of
statements that the reader will have encountered before.
Theorem. The system of linear equations (3.1) has a unique solution if and only if
det(A) ̸= 0 if and only if Ax = 0 has only the trivial solution if and only if A−1 exists.
This theorem gives necessary and suﬃcient conditions for the inverse of the
coeﬃcient matrix, A−1, to exist. If it does, then the solution is x = A−1f , but it is
not usually computationally tractable to calculate the inverse, as we discuss later.
Thus, we will look for ways of computing x without ﬁrst explicitly ﬁnding A−1. Most
of us, in a course on linear algebra, learned of a method called Gaussian elimination.
This simple, powerful, and sometimes mysterious technique will be the basis of most
of what we do in this chapter. In particular, Gaussian elimination is the foundation
for some well-known factorization techniques, such as the LU decomposition
method and the Cholesky factorization method for positive deﬁnite matrices.
Why is Gaussian elimination mysterious? The reason for this is that one of the
big open questions of numerical linear algebra is
Why is Gaussian elimination (with partial pivoting) usually so numerically stable in
practice?
Except for certain classes of truly pathological matrices, our best generic estimates
for the growth of roundoﬀerror in the algorithm tend to be overly pessimistic. In
other words, this simple algorithm usually performs much better than the worst-
case scenario for the average matrix. Gaussian elimination is much more reliable

32
Systems of Linear Equations
than numerical analysts would expect. A discussion of this topic is beyond the scope
of our text, but see [34, 96].
3.1
Solution of Simple Systems
Before we describe the general case, in this section, we develop some algorithms
to ﬁnd the solution to (3.1) for some simple cases, all of which avoid the direct
construction of A−1.
3.1.1
Diagonal Matrices
If the coeﬃcient matrix A is diagonal, i.e., A = diag(a1, . . . , an) with ak ̸= 0,
k = 1, . . . , n, then the solution can be easily found by xk = fk/ak.
3.1.2
Triangular Matrices
Let us, to be deﬁnite, consider the case when A is upper triangular. The system of
equations reads













a1,1x1 + a1,2x2 + · · · + a1,nxn = f1,
a2,2x2 + · · · + a2,nxn = f2,
...
an,nxn = fn.
A unique solution exists if and only if ai,i ̸= 0 for all i = 1, . . . , n. In this case, the
solution can be easily found by ﬁrst computing the value of the last variable,
xn = fn/an,n,
and, after that, recursively computing
xk =
1
ak,k

fk −
n
X
j=k+1
ak,jxj

,
k = n −1, n −2, . . . , 2, 1.
The order of execution of this algorithm is vital: one must start with k = n −1
and proceed in reverse order, ﬁnishing with k = 1. This algorithm is known as back
substitution.
Remark 3.1 (forward substitution). A similar procedure, known as forward
substitution, can be applied to lower triangular matrices.
3.1.3
Tridiagonal Matrices
We begin with a deﬁnition.

3.1 Solution of Simple Systems
33
Deﬁnition 3.2 (tridiagonal matrix). Let A = [ai,j] ∈Cn×n. We say that A is
tridiagonal if and only if when i, j ∈{1, . . . , n} and |i −j| > 1 then ai,j = 0.
A generic system of equations with a tridiagonal coeﬃcient matrix can be
conveniently expressed as
akxk−1 + bkxk + ckxk+1 = fk,
k = 1, . . . , n,
(3.2)
with a1 = cn = 0. This can be visualized as


b1
c1
0
· · ·
0
0
a2
b2
c2
0
0
0
a3
b3
...
...
...
...
0
...
...
...
0
0
...
...
bn−1
cn−1
0
0
· · ·
0
an
bn




x1
x2
x3
...
xn−2
xn−1
xn


=


f1
f2
f3
...
fn−2
fn−1
fn


.
To ﬁnd the solution — assuming that a unique solution exists — we begin by
assuming that it has the following form:
xk = αkxk+1 + βk.
This seems reasonable since, for k = 1, we have
x1 = −c1
b1
x2 + f1
b1
,
which conforms to our solution ansatz with
α1 = −c1
b1
,
β1 = f1
b1
.
Substituting our solution expression into the general form of the equations gives
ak(αk−1xk + βk−1) + bkxk + ckxk+1 = fk,
from which we get
xk = −
ck
akαk−1 + bk
xk+1 + fk −akβk−1
akαk−1 + bk
= αkxk+1 + βk.
(3.3)
Then, since cn = 0,
xn = fn −anβn−1
bn + anαn−1
.
Then, for k = n −1, . . . , 1, we can use (3.3) to ﬁnd the remaining components of
the solution.
An implementation of the just described algorithm is presented in Listing 3.1.
The reader can easily verify that the obtained x is indeed a solution to system
(3.2). In the literature, this algorithm is sometimes called the Thomas algorithm. 1
1 Named in honor of the British physicist and applied mathematician Llewellyn Hilleth Thomas
(1903–1992).

34
Systems of Linear Equations
Remark 3.3 (structure). The reader may wonder how useful the algorithm that
we just devised may be, as it requires a very special structure on the system matrix,
namely that it is tridiagonal. Later, in Chapters 24 and 28, we will see that many
schemes for the solution of one-dimensional boundary and initial boundary problems
entail solving (a sequence of) systems of linear equations with tridiagonal matrices.
3.1.4
Cyclically Tridiagonal Matrices
Consider a system of the form





b1x1 + c1x2 + a1xn = f1,
akxk−1 + bkxk + ckxk+1 = fk,
k = 2, . . . , n −1,
cnx1 + anxn−1 + bnxn = fn.
(3.4)
The coeﬃcient matrix for this system is said to be cyclically tridiagonal. Equation
(3.4) can also be visualized as


b1
c1
0
· · ·
0
a1
a2
b2
c2
0
0
0
a3
b3
...
...
...
...
0
...
...
...
0
0
...
...
bn−1
cn−1
cn
0
· · ·
0
an
bn




x1
x2
x3
...
xn−2
xn−1
xn


=


f1
f2
f3
...
fn−2
fn−1
fn


.
Notice that the coeﬃcient matrix diﬀers from a tridiagonal one, only in the (1, n)
and (n, 1) entries. This hints at the fact that, to solve (3.4), we will make use of
the solution of systems with tridiagonal matrices.
Indeed, to ﬁnd the solution of (3.4), we will ﬁrst solve the tridiagonal systems













b2u2 + c2u3 = f2,
a3u2 + b3u3 + c3u4 = f3,
...
anun−1 + bnun = fn
(3.5)
and













b2v2 + c2v3 = −a2,
a3v2 + b3v3 + c3v4 = 0,
...
anvn−1 + bnvn = −cn.
(3.6)
Then we set
xk = uk + x1vk,
k = 1, . . . , n,
(3.7)
with u1 = 0 and v1 = 1. Substituting this representation in the ﬁrst equation yields
b1x1 + c1(u2 + x1v2) + a1(un + x1vn) = f1,

3.2 LU Factorization
35
which implies that
x1 = f1 −c1u2 −a1un
b1 + c1v2 + a1vn
.
(3.8)
Let us verify that (3.7) and (3.8) are indeed the solution to (3.4). To do so,
multiply the ﬁrst equation of system (3.6) by x1 and add it to the ﬁrst equation of
system (3.5) to obtain
a2x1 + b2(u2 + x1v2) + c2(u3 + x1v3) = f2,
which implies that
a2x1 + b2x2 + c2x3 = f2.
A similar calculation can be made for the remaining equations, up until the last
one, where we get
cnx1 + an(un−1 + x1vn−1) + bn(un + x1vn) = fn,
which implies that
cnx1 + anxn−1 + bnxn = fn.
An implementation of this procedure is presented in Listing 3.2. Once again,
the reader may wonder how often one encounters cyclically tridiagonal matrices.
Many discretization schemes for one-dimensional boundary and initial boundary
value problems with periodic boundary conditions entail the solution of (a collection
of) systems of linear equations with cyclically tridiagonal matrices; see Chapters
24–28 for more details.
3.2
LU Factorization
In this section, we give a practical and theoretical description of the method of LU
factorization for solving a square system of linear equations. The idea is based upon
the very familiar concept of Gaussian elimination. We need the following preliminary
results.
Theorem 3.4 (properties of triangular matrices). Let the matrices T, Tk ∈Cn×n,
for k = 1, 2, be lower (upper) triangular. Then the following are true.
1. The product T1T2 is lower (upper) triangular.
2. If, in addition [Tk]i,i = 1, for k = 1, 2 and i = 1, . . . , n — i.e., T1, T2 ∈Cn×n
are unit lower (unit upper) triangular — then the product T1T2 is unit lower
(unit upper) triangular.
3. The matrix T is nonsingular if and only if [T]i,i ̸= 0 for all i = 1, . . . , n.
4. If T is nonsingular, T−1 ∈Cn×n is lower (upper) triangular.
5. If T is unit lower (unit upper) triangular, then it is invertible and T−1 is unit
lower (unit upper) triangular.
6. If [T]i,i > 0, then [T−1
k ]i,i =
1
[T]i,i > 0.
Proof. See Problem 1.32.

36
Systems of Linear Equations
Deﬁnition 3.5 (sub-matrix). Suppose that A ∈Cn×n and S ⊆{1, 2, . . . , n} is
nonempty with cardinality k = #(S) > 0. The sub-matrix A(S) ∈Ck×k is that
matrix obtained by deleting the columns and rows of A whose indices are not in S.
In symbols,
[A(S)]i,j = [A]mi,mj,
i, j = 1, . . . , k,
where
S = {m1, . . . , mk}
and
1 ≤m1 < m2 < · · · < mk ≤n.
Example 3.1
Suppose that
A =


1
−7
12
4
6
9
−3
−4
1
−6
8
9
4
4
−11
17

,
S = {2, 4}.
Then m1 = 2 and m2 = 4 and
A(S) =
9
−4
4
17

.
Deﬁnition 3.6 (leading principal sub-matrix). Let A ∈Cn×n and S = {1, 2, . . . , k}
with k ≤n. Then we deﬁne
A(k) = A(S) ∈Ck×k
and we call A(k) the leading principal sub-matrix of A of order k.
Typically, the LU factorization is produced via Gaussian elimination. But the
proof of the following theorem obscures this fact and guarantees, independently of
Gaussian elimination, the existence and uniqueness of the LU factorization.
Theorem 3.7 (LU factorization). Let n ≥2 and A ∈Cn×n. Suppose that all
the leading principal sub-matrices of A are nonsingular, i.e., det(A(k)) ̸= 0 for all
k = 1, . . . , n −1. Then there exists a unit lower triangular matrix L ∈Cn×n and an
upper triangular matrix U ∈Cn×n such that
A = LU.
Proof. The proof is by induction on n, the size of the matrix.
(n = 2) Consider
A =
a
b
c
d

,

3.2 LU Factorization
37
with a ̸= 0, by assumption. Deﬁne
L =
 1
0
m
1

,
U =
u
v
0
η

,
where
u = a,
v = b,
m = c
a ,
η = d −bc
a .
Then
mu = c,
mv + η = d,
and consequently A = LU, as is easily conﬁrmed.
(n = m) The induction hypothesis is as follows: suppose that the result is valid for
any A ∈Cm×m, provided that A(k) is nonsingular for all k = 1, . . . , m −1.
(n = m + 1) Suppose that A(k) is nonsingular for k = 1, . . . , m. Set
A =
A(m)
b
c⊺
d

∈C(m+1)×(m+1).
From the induction hypothesis, there is a unit lower triangular matrix L(m) and an
upper triangular matrix U(m) such that A(m) = L(m)U(m), where A(m) is the leading
principal sub-matrix of A of order m. Deﬁne
L =
L(m)
0
m⊺
1

,
U =
U(m)
v
0⊺
η

,
where b, c, m, v, 0 ∈Cm. Then
LU =
L(m)U(m)
L(m)v
m⊺U(m)
m⊺v + η

.
Let us set this equal to A and determine whether or not the resulting equations
are solvable. It is easy to see that A = LU if and only if
L(m)U(m) = A(m),
L(m)v = b,
m⊺U(m) = c⊺,
m⊺v + η = d.
The last three equations are uniquely solvable, as we now show: since L(m) is
invertible,
v =

L(m)−1
b.
The matrix U(m) is invertible since
0 ̸= det(A(m)) = det(L(m)U(m)) = det(U(m)).
Hence,
m⊺= c⊺
U(m)−1
or
m =

U(m)−⊺
c.
Finally,
η = d −m⊺v.
The proof by induction is complete.

38
Systems of Linear Equations
Before we go any further, we ought to say why it is that an LU factorization of
a matrix is useful. Suppose that we want to solve the indexed family of problems
Ax(k) = f (k),
k = 1, . . . , K,
and that there exists a unit lower triangular matrix L ∈Cn×n and an upper triangular
matrix U ∈Cn×n such that A = LU. To ﬁnd the solutions x(k), we solve the
following equivalent family:
Ly (k) = f (k),
Ux(k) = y (k),
k = 1, . . . , K.
The vector y (k) can be obtained easily and cheaply via forward substitution. Sub-
sequently, the vector x(k) can be obtained by back substitution; see Section 3.1.2.
Now we show a practical connection between the LU factorization and what
is commonly called Gaussian elimination. We use an example to motivate our
discussion.
Example 3.2
Consider the following system of linear equations:





x1+
x2+
x3 =
6,
2x1+
4x2+
2x3 =
16,
−x1+
5x2−
4x3 =
−3.
Of course, we can represent this as a matrix–vector equation Ax = f . We write
this as an augmented matrix and perform Gaussian elimination to put the system
into so-callled row echelon form,
[A|f ] =


1
1
1
6
2
4
2
16
−1
5
−4
−3


−2R1+R2→R2
−−−−−−−−−→
1R1+R3→R3
−−−−−−−−→


1
1
1
6
0
2
0
4
0
6
−3
3


−3R2+R3→R3
−−−−−−−−−→


1
1
1
6
0
2
0
4
0
0
−3
−9

.
The boxed entries indicate the so-called pivot elements. The values of the pivot
elements help to determine the row multipliers in the algorithm. As long as these are
nonzero, the algorithm can run to completion. Gaussian elimination uses elementary
row operations to produce an equivalent upper triangular system of linear equations,
Ux = b. By equivalent, we mean that the solution sets are exactly the same, even
though the coeﬃcient matrix and right-hand-side vector are changed.
Let us focus on the left-hand side of the augmented system, as this will be the
important part with respect to the LU factorization. We have
L(3,2)L(3,1)L(2,1)A = U =


1
1
1
0
2
0
0
0
−3

,
where L(2,1), L(3,1), L(3,2) are elementary matrices encoding the elementary row
operations performed in our Gaussian elimination process. To produce the matrix

3.2 LU Factorization
39
representations of these operations, recall that we need only to apply the
corresponding elementary row operations on the identity matrix:


1
0
0
0
1
0
0
0
1

−2R1+R2→R2
−−−−−−−−−→


1
0
0
−2
1
0
0
0
1

= L(2,1).
Likewise,


1
0
0
0
1
0
0
0
1

1R1+R3→R3
−−−−−−−−→


1
0
0
0
1
0
1
0
1

= L(3,1)
and


1
0
0
0
1
0
0
0
1

−3R2+R3→R3
−−−−−−−−−→


1
0
0
0
1
0
0
−3
1

= L(3,2).
Then it is easy to see that
L(3,1)L(2,1)A =


1
1
1
0
2
0
0
6
−3


and
L(3,2)L(3,1)L(2,1)A =


1
1
1
0
2
0
0
0
−3

.
Observe that the order of application of L(2,1) and L(3,1) does not matter:
L(3,1)L(2,1) =


1
0
0
−2
1
0
1
0
1

= L(2,1)L(3,1).
This will be shown to be true in general.
Suppose that A ∈Cn×n, where n ≥2. If Gaussian elimination for A proceeds to
completion without encountering any zero pivots, one gets
L(n,n−1) · · · L(n,2) · · · L(3,2)L(n,1) · · · L(2,1)A = U,
where U is square and upper triangular. Moreover, since we assume that no zero
pivot entries are encountered, [U]i,i ̸= 0, for i = 1, · · · , n−1. However, it is possible
that [U]n,n = 0. We can group the elementary operations into column operations
as follows:
(column 1):
L1 = L(n,1) · · · L(2,1),
(column 2):
L2 = L(n,2) · · · L(3,2),
...
(column n −2):
Ln−2 = L(n,n−2)L(n−1,n−2),
(column n −1):
Ln−1 = L(n,n−1),
so that
Ln−1Ln−2 · · · L2 L1A = U.

40
Systems of Linear Equations
Furthermore, we can prove that the matrices deﬁning the Li matrices commute
and can be multiplied in any order, as we will show. The matrices Li are examples
of what we will call column-i complete elementary matrices.
In any case,
A = L−1
1 · · · L−1
n−1U = LU.
It only remains to show that L is unit lower triangular. If so, we have connected
Gaussian elimination to the LU factorization.
Now consider the implications of the following example.
Example 3.3
What is the inverse of an elementary matrix? Suppose that
L =


1
0
0
−1
2
1
0
0
0
1


and
L′ =


1
0
0
1
2
1
0
0
0
1

.
Then it is easy to see that L′L = I3. Or, in other words, L′ = L−1.
Deﬁnition 3.8 (elementary matrix). A matrix E ∈Cn×n is called elementary if
and only if E = I+µr,sM(r,s) for some µr,s ∈C and for some 1 ≤s < r ≤n, where
M(r,s) = ere⊺
s ,
i.e.,
h
M(r,s)i
i,j = δi,rδj,s.
Remark 3.9 (generality). We remark that our last deﬁnition is more restrictive
than might be found in some other references, speciﬁcally in our requirement that
r is strictly greater than s. But this is all that is needed for our purposes.
Proposition 3.10 (properties of elementary matrices). Suppose that
Ek = I + µrk,sM(rk,s),
k = 1, 2
are two elementary matrices with r1 ̸= r2. Then both matrices are invertible, the
inverses are elementary, and the matrices commute. Furthermore,
E−1
k
= I −µrk,sM(rk,s),
(E1E2)−1 = E−1
2 E−1
1
= E−1
1 E−1
2
= I −µr1,sM(r1,s) −µr2,sM(r2,s),
and
E1E2 = E2E1 = I + µr1,sM(r1,s) + µr2,sM(r2,s),
Proof. See Problem 3.5.

3.2 LU Factorization
41
Deﬁnition 3.11 (complete elementary matrix). Suppose that n ≥2. Let the index
s ∈{1, 2, . . . , n−1} be given. The matrix F ∈Cn×n is called a column-s complete
elementary matrix if and only if
F = I +
n
X
r=s+1
µr,sM(r,s)
for some scalars µr,s ∈C, r = s + 1, . . . , n. In other words, F is a unit lower
triangular matrix of the form
F =


1
...
1
µs+1,s
1
...
...
µn,s
1


.
Deﬁnition 3.12 (Gaussian elimination2). Let A ∈Cn×n be given with n ≥2. We
deﬁne the Gaussian elimination algorithm recursively as follows. Suppose that k
stages of Gaussian elimination have been completed, where k ∈{0, . . . , n −1},
such that no zero pivots have been encountered, producing the matrix factorization
Lk · · · L1A = A(k),
k = 1, . . . , n −1,
where A(0) = A and, for k = 1, . . . , n −1,
A(k) =


a(0)
1,1
a(0)
1,2
a(0)
1,3
a(0)
1,4
· · ·
a(0)
1,k+1
· · ·
a(0)
1,n
0
a(1)
2,2
a(1)
2,3
a(1)
2,4
· · ·
a(1)
2,k+1
· · ·
a(1)
2,n
0
0
a(2)
3,3
a(2)
3,4
· · ·
a(2)
3,k+1
· · ·
a(2)
3,n
...
...
...
...
...
...
0
0
a(k−1)
k,k
a(k−1)
k,k+1
· · ·
a(k−1)
k,n
0
0
0
a(k)
k+1,k+1
· · ·
a(k)
k+1,n
...
...
...
...
...
0
0
· · ·
0
0
a(k)
n,k+1
· · ·
a(k)
n,n


.
If k = n −1, we are done and we set U = A(n−1). Otherwise, if the (k + 1)st pivot
entry, a(k)
k+1,k+1, is not equal to zero, the algorithm may proceed. Construct the
column-(k + 1) complete elementary matrix
Lk+1 = I +
n
X
r=k+2
µr,k+1M(r,k+1),
where
µr,k+1 = −a(k)
r,k+1
a(k)
k+1,k+1
,
r = k + 2, . . . , n.
2 Named in honor of the German mathematician and physicist Johann Carl Friedrich Gauss
(1777–1855).

42
Systems of Linear Equations
Then set
Lk+1A(k) = A(k+1),
obtaining
A(k+1) =


a(0)
1,1
a(0)
1,2
a(0)
1,3
a(0)
1,4
· · ·
a(0)
1,k+1
a(0)
1,k+2
· · ·
a(0)
1,n
0
a(1)
2,2
a(1)
2,3
a(1)
2,4
· · ·
a(1)
2,k+1
a(1)
2,k+2
· · ·
a(1)
2,n
0
0
a(2)
3,3
a(2)
3,4
· · ·
a(2)
3,k+1
a(2)
3,k+2
· · ·
a(2)
3,n
...
...
...
...
...
...
...
0
0
a(k−1)
k,k
a(k−1)
k,k+1
a(k−1)
k,k+2
· · ·
a(k−1)
k,n
0
0
0
a(k)
k+1,k+1
a(k)
k+1,k+2
· · ·
a(k)
k+1,n
0
0
0
0
a(k+1)
k+2,k+2
· · ·
a(k+1)
k+2,n
...
...
...
...
...
...
0
0
· · ·
0
0
0
a(k+1)
n,k+2
· · ·
a(k+1)
n,n


.
Based on our previous computations, the following result should be clear.
Theorem 3.13 (Gaussian elimination). Let A ∈Cn×n. If Gaussian elimination
proceeds to completion without encountering any zero pivots, then there are
column-k complete elementary matrices Lk ∈Cn×n, for k = 1, . . . , n −1, such
that
Ln−1 · · · L2L1A = U,
where U ∈Cn×n is upper triangular and
[U]i,i ̸= 0,
i = 1, . . . , n −1,
since no zero pivots are encountered. Furthermore,
A = L−1
1 · · · L−1
n−1U = LU,
where L is unit lower triangular. Writing
Lk = I +
n
X
r=k+1
µr,sM(r,k),
it follows that
L = I −
n−1
X
k=1
n
X
r=k+1
µr,kM(r,k).
In other words,
L =


1
−µ2,1
1
−µ3,1
−µ3,2
1
...
...
...
...
−µn,1
−µn,2
· · ·
−µn,n−1
1


.
Proof. See Problem 3.7.

3.3 Gaussian Elimination with Column Pivoting
43
Theorem 3.14 (uniqueness). Suppose that n ≥2 and A ∈Cn×n is invertible.
Suppose that there is a unit lower triangular matrix L ∈Cn×n and an upper
triangular matrix U ∈Cn×n such that A = LU. Then this LU factorization is
unique.
Proof. Suppose that there are two factorizations with the desired properties:
L1U1 = A = L2U2.
Since A is invertible, U1 and U2 must be invertible, i.e., there are no zeros on their
diagonals. Furthermore,
L−1
2 L1 = U2U−1
1
= D,
where D is by necessity diagonal. Therefore,
L1 = L2D.
But it must be that D = In, since the diagonal elements of L1 and L2 are all
ones.
Listing 3.3 provides a more streamlined, computable version of the LU factoriza-
tion algorithm. The algorithm proceeds to completion provided that no zero pivots
are encountered. This listing can also be used to estimate the complexity of the
LU factorization algorithm.
Proposition 3.15 (complexity of LU). Let A ∈Cn×n. Then the LU factorization
algorithm requires, to leading order, 2
3n3 operations.
Proof. We only care about the leading order of operations, which, from Listing
3.3, can easily be seen to be roughly
n−1
X
k=1
n
X
j=k
n
X
t=k
2 = 2
n−1
X
k=1
(n −k)
n
X
j=k+1
1
≈2
n−1
X
k=1
(n −k)2
= 1
3(n −1)n(2n −1)
≈2
3n3.
3.3
Gaussian Elimination with Column Pivoting
In the last section, we did not consider what one should do if a zero pivot is
encountered. Let us examine a simple situation where zero pivots appear.

44
Systems of Linear Equations
Example 3.4
Suppose that A ∈C3×3 is given by
A =


0
1
5
−2
1
1
4
−2
6

.
We want to use Gaussian elimination to obtain an LU factorization of A. However,
we notice from the beginning that there is a zero in the ﬁrst pivot location. But a
simple row interchange operation will ﬁx this situation. In the following algorithm,
let us agree to interchange rows, so that the element with largest modulus in the
column at or below the pivot position moves into the pivot position. This is called
Gaussian elimination with maximal column pivoting:
A =


0
1
5
−2
1
1
4
−2
6


R1⇆R3
−−−−→


4
−2
6
−2
1
1
0
1
5


1
2 R1+R2→R2
−−−−−−−−→
0R1+R3→R3
−−−−−−−−→


4
−2
6
0
0
4
0
1
5


R2⇆R3
−−−−→


4
−2
6
0
1
5
0
0
4


0R2+R3→R3
−−−−−−−−→


4
−2
6
0
1
5
0
0
4

.
Our procedure may be expressed as
L2P2L1P1A = U,
where P1 and P2 are simple permutation matrices and L1 and L2 are column-1 and
column-2 complete elementary matrices, respectively.
Deﬁnition 3.16 (permutation). A matrix P ∈Cn×n is called a simple permutation
matrix if and only if it is obtained from the n × n identity matrix I by interchanging
exactly two rows of I. P is called a regular permutation (or just a permutation)
matrix if and only if P is the product of simple permutation matrices.
Proposition 3.17 (action of permutations). Let n ≥2. Suppose that A ∈Cn×n is
any matrix and P ∈Cn×n is a simple permutation matrix obtained by interchanging
rows r and s of the identity matrix with 1 ≤r < s ≤n. Then PA is identical to A,
except with rows r and s interchanged. Furthermore, AP is identical to A, except
with columns r and s interchanged.
Proof. See Problem 3.8.

3.3 Gaussian Elimination with Column Pivoting
45
Lemma 3.18 (properties of permutations). Suppose that P, Q ∈Cn×n, with n ≥2,
are permutation matrices. Then
1. The product PQ is a permutation matrix.
2. det(P) = ±1 according to whether P is the product of an even (det(P) = 1) or
an odd (det(P) = −1) number of simple permutation matrices.
3. The inverse of a simple permutation matrix is itself. Any regular permutation
matrix P is invertible, and, if
P = P1P2 · · · Pk,
where Pi is a simple permutation matrix, for 1 ≤i ≤k, then
P−1 = Pk · · · P2P1 = P⊺.
Proof. See Problem 3.9.
Example 3.5
Let us continue with our 3 × 3 example, but in general terms.
We have
L2P2L1P1A = U,
where Pj, j = 1, 2 are simple permutation matrices or the identity matrix (in the
case that no row interchange took place) and Lj are column-j complete elementary
matrices, j = 1, 2. Now observe that
L2P2L1P2P2P1A = U.
Therefore,
ˆL2ˆL1PA = U,
where
ˆL2 = L2,
ˆL1 = P2L1P2,
P = P2P1.
Example 3.6
Suppose that Gaussian elimination with maximal column pivoting
is applied to A ∈C4×4. Then it should be clear that one obtains
L3P3L2P2L1P1A = U,
which can be rewritten as
ˆL3ˆL2ˆL1PA = U,
where
ˆL3 = L3,
ˆL2 = P3L2P3,
ˆL1 = P3P2L1P2P3,
P = P3P2P1.
It turns out — and it is probably not so hard to see — that the ˆLk matrices
constructed above are still column-k complete elementary matrices.

46
Systems of Linear Equations
Proposition 3.19 (permutations and elementary matrices). Suppose that Lk ∈
Cn×n is a column-k complete elementary matrix,
Lk = In +
n
X
r=k+1
µr,kM(r,k),
for some constants µr,k ∈C, for k = k + 1, . . . , n. Assume that Q ∈Cn×n is a
simple permutation matrix encoding the interchange of rows r ′ and s′, where k <
r ′ < s′ ≤n. Then the matrix QLkQ is a column-k complete matrix. In particular,
QLkQ is identical to Lk, except that entries µr ′,k and µs′,k are interchanged.
Proof. It follows that
QLkQ = QInQ +
n
X
r=k+1
µr,kQere⊺
kQ.
But observe that e⊺
kQ = e⊺
k and
Qer =





es′,
r = r ′,
er ′,
r = s′,
er,
r ∈{1, . . . , n}\{r ′, s′}.
Therefore,
QLkQ = In +
n
X
r=k+1
r̸=r ′,s′
µr,kere⊺
k + µs′,ker ′e⊺
k + µr ′,kes′e⊺
k.
In other words, QLkQ is a column-k complete elementary matrix that is identical
to Lk, except that the positions of µr ′,k and µs′,k are swapped.
Deﬁnition 3.20 (Gaussian elimination with maximal column pivoting). Let A ∈
Cn×n be given with n ≥2. We deﬁne the Gaussian elimination with maximal
column pivoting algorithm recursively as follows. Suppose that k stages of
Gaussian elimination with maximal column pivoting have been completed, where
k = 0, . . . , n −1, producing the matrix decomposition
LkPk · · · L1P1A = A(k),
k = 1, . . . , n −1,
where A(0) = A and, for k = 1, . . . , n −1,
A(k) =


a(0,1)
1,1
a(0,1)
1,2
a(0,1)
1,3
a(0,1)
1,4
· · ·
a(0,1)
1,k+1
· · ·
a(0,1)
1,n
0
a(1,1)
2,2
a(1,1)
2,3
a(1,1)
2,4
· · ·
a(1,1)
2,k+1
· · ·
a(1,1)
2,n
0
0
a(2,1)
3,3
a(2,1)
3,4
· · ·
a(2,1)
3,k+1
· · ·
a(2,1)
3,n
...
...
...
...
...
...
0
0
a(k−1,1)
k,k
a(k−1,1)
k,k+1
· · ·
a(k−1,1)
k,n
0
0
0
a(k)
k+1,k+1
· · ·
a(k)
k+1,n
...
...
...
...
...
0
0
· · ·
0
0
a(k)
n,k+1
· · ·
a(k)
n,n


.

3.3 Gaussian Elimination with Column Pivoting
47
If k = n −1, we are done, and we set U = A(n−1). Otherwise, use a simple permu-
tation matrix to interchange rows k + 1 and r, with r ≥k + 1 and
a(k)
r,k+1
 ≥
a(k)
j,k+1
 ,
j = k + 1, . . . , n,
with the understanding that the simple permutation is the identity matrix if
r = k + 1, obtaining
Pk+1A(k) = A(k,1),
where
A(k,1) =


a(0,1)
1,1
a(0,1)
1,2
a(0,1)
1,3
a(0,1)
1,4
· · ·
a(0,1)
1,k+1
· · ·
a(0,1)
1,n
0
a(1,1)
2,2
a(1,1)
2,3
a(1,1)
2,4
· · ·
a(1,1)
2,k+1
· · ·
a(1,1)
2,n
0
0
a(2,1)
3,3
a(2,1)
3,4
· · ·
a(2,1)
3,k+1
· · ·
a(2,1)
3,n
...
...
...
...
...
...
0
0
a(k−1,1)
k,k
a(k−1,1)
k,k+1
· · ·
a(k−1,1)
k,n
0
0
0
a(k,1)
k+1,k+1
· · ·
a(k,1)
k+1,n
...
...
...
...
...
0
0
· · ·
0
0
a(k,1)
n,k+1
· · ·
a(k,1)
n,n


.
If the updated (k + 1)st pivot entry, a(k,1)
k+1,k+1, is equal to zero, we set Lk+1 = In.
Otherwise, construct the column-(k + 1) complete elementary matrix
Lk+1 = In +
n
X
r=k+2
µr,k+1M(r,k+1),
where
µr,k+1 = −a(k,1)
r,k+1
a(k,1)
k+1,k+1
.
Then set
Lk+1A(k,1) = A(k+1),
obtaining
A(k+1) =


a(0,1)
1,1
a(0,1)
1,2
a(0,1)
1,3
a(0,1)
1,4
· · ·
a(0,1)
1,k+1
a(0,1)
1,k+2
· · ·
a(0,1)
1,n
0
a(1,1)
2,2
a(1,1)
2,3
a(1,1)
2,4
· · ·
a(1,1)
2,k+1
a(1,1)
2,k+2
· · ·
a(1,1)
2,n
0
0
a(2,1)
3,3
a(2,1)
3,4
· · ·
a(2,1)
3,k+1
a(2,1)
3,k+2
· · ·
a(2,1)
3,n
...
...
...
...
...
...
...
0
0
a(k−1,1)
k,k
a(k−1,1)
k,k+1
a(k−1,1)
k,k+2
· · ·
a(k−1,1)
k,n
0
0
0
a(k,1)
k+1,k+1
a(k,1)
k+1,k+2
· · ·
a(k,1)
k+1,n
0
0
0
0
a(k+1)
k+2,k+2
· · ·
a(k+1)
k+2,n
...
...
...
...
...
...
0
0
· · ·
0
0
0
a(k+1)
n,k+2
· · ·
a(k+1)
n,n


.

48
Systems of Linear Equations
Theorem 3.21 (LU factorization with pivoting). Suppose that n ≥2 and A ∈
Cn×n. The Gaussian elimination with maximal column pivoting algorithm always
proceeds to completion to yield an upper triangular matrix U. In particular, there
are matrices Lj, Pj ∈Cn×n, j = 1, . . . , n −1 such that
Ln−1Pn−1 · · · L2P2L1P1A = U,
where Lj is a column-j complete elementary matrix and Pj is either the n × n
identity or a simple permutation matrix. Furthermore, there are column-j complete
elementary matrices ˆLj, for j = 1, . . . , n −1, and a permutation matrix P such that
ˆLn−1 · · · ˆL1PA = U,
where
P = Pn−1 · · · P1,
ˆLj = Pn−1 · · · Pj+1LjPj+1 · · · Pn−1,
j = 1, . . . , n −2,
and
ˆLn−1 = Ln−1.
Finally, there is a unit lower triangular matrix L such that
PA = LU.
Proof. This is nothing but an exercise in applying our previous results and
deﬁnitions; see Problem 3.10.
Listing 3.4 computes the LU factorization with pivoting. As before, and for a
diﬀerent perspective, it is also possible to prove the existence of the factorization
A = P⊺LU independent of the consideration of the Gaussian elimination algorithm.
Here, we follow the presentation in [89].
Theorem 3.22 (LU factorization with pivoting). Suppose that n ≥2. Let A ∈Cn×n
be given. There exists a permutation matrix P ∈Rn×n, a unit lower triangular matrix
L ∈Cn×n, and an upper triangular matrix U ∈Cn×n such that
PA = LU.
Proof. We proceed by induction.
(n = 2) Consider
A =
a
b
c
d

.
If a ̸= 0, then set P = I2,
L =
1
0
c
a
1

,
U =
a
b
0
d −c
ab

.

3.3 Gaussian Elimination with Column Pivoting
49
Clearly, PA = LU. On the other hand, if a = 0, but c ̸= 0, set
P =
0
1
1
0

,
L = I2,
U =
c
d
0
b

,
and again observe that PA = LU. Finally, if a = 0 = c, there is not much to do:
set
P = I2 = L,
U =
0
b
0
d

,
and conclude the result.
(n = m) The induction hypothesis is to suppose that the result is true for every
matrix A ∈Ck×k for all k = 2, . . . , m.
(n = m + 1) Suppose that A ∈C(m+1)×(m+1) is arbitrary. Suppose that, in the ﬁrst
column of A, the largest element by modulus is contained in row r. Set P1 as the
simple permutation matrix interchanging rows 1 and r. Then the result is, for some
p, w ∈Cm and B ∈Cm×m,
P1A =
α
w ⊺
p
B

,
where
|α| ≥|[p]i|,
i = 1, . . . , m.
Observe that it is possible that α = 0, in which case p = 0. Next, we seek a
solution, if possible, to the following intermediate problem:
P1A =
α
w ⊺
p
B

=
 1
0⊺
m
Im
 α
v ⊺
0
C

,
where
C ∈Cm×m,
m, v, 0 ∈Cm.
The block matrix equation is satisﬁed if and only if
v = w,
αm = p,
C = B −mv ⊺.
Recall that, if α = 0, p = 0. In this case,
m = 0,
v = w,
C = B
is one possible solution. If α ̸= 0, then
m = 1
αp,
v = w,
C = B −1
αpw ⊺.
Observe that
|[m]i| ≤1,
i = 1, . . . , m.
Now, from the induction hypothesis, there is a permutation matrix ˜P ∈Cm×m, a
unit upper triangular matrix ˜L ∈Cm×m, and an upper triangular matrix ˜U ∈Cm×m
such that
˜PC = ˜L˜U.

50
Systems of Linear Equations
Finally, the reader will observe that
1
0⊺
0
˜P⊺
  1
0⊺
˜Pm
˜L
 α
v ⊺
0
˜U

= P1A,
using the fact that ˜P⊺= ˜P−1. Therefore,
LU = PA,
where
P =
1
0⊺
0
˜P

P1,
L =
 1
0⊺
˜Pm
˜L

,
U =
α
v ⊺
0
˜U

,
and the matrices are of the required types.
The LU factorization can be used to eﬃciently compute the solution to (3.1).
Let us suppose that A is nonsingular. The algorithm is as follows: if PA = LU, then
we have the equivalent system
Ly = Pf = q,
Ux = y.
(3.9)
We use the forward substitution algorithm to solve Ly = q for y and, then, we use
the backward substitution algorithm to solve Ux = y for x. These algorithms were
covered in Section 3.1.2.
3.4
Implementation of the LU Factorization
We conclude the discussion of Gaussian elimination with some practical consider-
ations. First of all, one does not need to store the permutation matrix P. Instead
one only needs to remember which rows were swapped. This means that we only
need to store a vector s ∈Nn of indices which is used to indirectly reference the
entries of the matrix. Second, since the matrices L and U are lower and upper
triangular, respectively, and the diagonal entries of L are always equal to one, both
of these matrices can be conveniently stored in the already allocated array for A.
This is a convenient and eﬃcient way of storing the LU factorization of the system
matrix A. Listing 3.5 provides an implementation of this idea.
Once this factorization has taken place, we can use it, together with the swap
vector s, to perform a back substitution and ﬁnd the solution to system (3.1),
as described by (3.9). Listing 3.6 provides the implementation details for this. It
is important to note that the LU factorization, the most expensive part of this
procedure, needs to only be called once. After that, solving several systems of the
form (3.1) where the system matrix A does not change, but the right-hand-side f
does, is rather eﬃcient, as it only requires n2 operations. See Problem 3.2. This is
a situation that is very common in practice. See, for instance, Chapters 24–28.

3.5 Special Matrices
51
3.5
Special Matrices
While Theorems 3.21 and 3.22 provide, in general, the existence and uniqueness
of an LU factorization with pivoting, it is of interest to study this process for some
special kinds of matrix that often appear in applications.
3.5.1
Diagonally Dominant Matrices
Deﬁnition 3.23 (diagonal dominance). A matrix A = [ai,j] ∈Cn×n is called
diagonally dominant if and only if
|ai,i| ≥
n
X
k=1
k̸=i
|ai,k| ,
∀i = 1, . . . , n.
A is strictly diagonally dominant (SDD) if and only if
|ai,i| >
n
X
k=1
k̸=i
|ai,k| ,
∀i = 1, . . . , n.
A is called strictly diagonally dominant of dominance δ if and only if there is a
δ > 0 such that
|ai,i| ≥δ +
n
X
k=1
k̸=i
|ai,k| ,
∀i = 1, . . . , n.
The reader should verify that, essentially, the last two deﬁnitions are equivalent.
It turns out that SDD matrices are always invertible, and we have an easy bound
on the norm of its inverse.
Theorem 3.24 (properties of an SDD matrix). If A ∈Cn×n is SDD, then A is
invertible. If A is SDD of dominance δ > 0, then
A−1
∞< 1
δ .
Proof. Suppose that A is singular. If that is the case there is an x = [xi] ∈Cn
⋆
such that Ax = 0. Suppose that k ∈{1, . . . , n} is an index for which |xk| = ∥x∥∞.
Since Ax = 0, we must have that, for each i = 1, . . . , n,
n
X
j=1
ai,jxj = 0.
In particular, Pn
j=1 ak,jxj = 0. Then, from the triangle inequality,
|ak,k| · ∥x∥∞= |ak,kxk| =

−
n
X
j=1
j̸=k
ak,jxj

≤
n
X
j=1
j̸=k
|ak,j| · |xj| ≤∥x∥∞
n
X
j=1
j̸=k
|ak,j|.

52
Systems of Linear Equations
Since ∥x∥∞> 0, we have
|ak,k| ≤
n
X
j=1
j̸=k
|ak,j|.
This proves that A is not SDD, a contradiction.
Next, suppose that A has dominance δ > 0. Let x be arbitrary. Set Ax = f .
Assume that ∥x∥∞= |xk|, for some k = 1, . . . , n. Then
ak,1x1 + · · · + ak,kxk + · · · + ak,nxn = fk
and, using the reverse triangle inequality,
|fk| ≥|ak,k||xk| −
n
X
j=1
j̸=k
|aj,k||xj| ≥



|ak,k| −
n
X
j=1
j̸=k
|aj,k|



|xk| ≥δ∥x∥∞.
This shows that
∥Ax∥∞
∥x∥∞
≥δ,
∀x ∈Cn,
which is equivalent to
1
δ ≥
A−1w

∞
∥w∥∞
,
∀w ∈Cn
⋆.
This, in turn, implies that
1
δ ≥sup
w∈Cn⋆
A−1w

∞
∥w∥∞
=
A−1
∞,
as we intended to show.
Theorem 3.25 (Gaussian elimination and SDD). Let A = [ai,j] ∈Cn×n be SDD
and assume that it is represented as
A =
α
v ⊺
p
ˆA

,
where α ∈C, p, v ∈Cn−1, and ˆA = [ˆai,j] ∈C(n−1)×(n−1). After one step of
Gaussian elimination (without pivoting), A will be reduced to the matrix
α
v ⊺
0
B

,
where B = [bi,j] ∈C(n−1)×(n−1) is SDD.
Proof. Let us construct a matrix L ∈Cn×n such that
LA =
α
v ⊺
0
B

,

3.5 Special Matrices
53
if possible. Consider
L =
 1
0⊺
m
In−1

.
Then
LA =
 1
0⊺
m
In−1
 α
v ⊺
p
ˆA

=

α
v ⊺
αm + p
mv ⊺+ ˆA

.
Note that, since A is SDD, α ̸= 0 and ˆA is SDD. Choosing m = −α−1p, we have
LA =
 α
v ⊺
0
ˆA −α−1pv ⊺

.
Having successfully constructed L, we ﬁnd B = ˆA −α−1pv ⊺.
All that remains is to show that B is SDD. To see that this is the case, consider
for row i of B,
n−1
X
j=1
j̸=i
|bi,j| =
n−1
X
j=1
j̸=i
ˆai,j −α−1pivj

≤
n−1
X
j=1
j̸=i
|ˆai,j| +
n−1
X
j=1
j̸=i
α−1pivj

=
n−1
X
j=1
j̸=i
|ˆai,j| + |pi|
|α|
n−1
X
j=1
j̸=i
|vj|
=
n−1
X
j=1
j̸=i
|ai+1,j+1| + |ai+1,1|
|a1,1|
n−1
X
j=1
j̸=i
|a1,j+1|
=
n
X
j=2
j̸=i+1
|ai+1,j| + |ai+1,1|
|a1,1|
n
X
j=2
j̸=i+1
|a1,j|
=
n
X
j=1
j̸=i+1
|ai+1,j| −|ai+1,1| + |ai+1,1|
|a1,1|
n
X
j=2
|a1,j| −|ai+1,1| · |a1,i+1|
|a1,1|
.
Now, since A is SDD, we can continue this string of inequalities to obtain
n−1
X
j=1
j̸=i
|bi,j| < |ai+1,i+1| −|ai+1,1| + |ai+1,1| −|ai+1,1| · |a1,i+1|
|a1,1|
= |ai+1,i+1| −|ai+1,1| · |a1,i+1|
|a1,1|
≤
ai+1,i+1 −ai+1,1a1,i+1
a1,1

= |bi,i|,
where we used the reverse triangle inequality. Thus, as claimed, B is SDD.

54
Systems of Linear Equations
Corollary 3.26 (SDD of magnitude δ). If A ∈Cn×n is SDD of magnitude δ > 0,
then B ∈C(n−1)×(n−1), introduced above, is SDD of magnitude δ.
Proof. The details of our last proof still hold up to the point where we apply the
fact that A is SDD. Thus,
n−1
X
j=1
j̸=i
|bi,j| ≤
n
X
j=1
j̸=i+1
|ai+1,j| −|ai+1,1| + |ai+1,1|
|a1,1|
n
X
j=2
|a1,j| −|ai+1,1| · |a1,i+1|
|a1,1|
≤|ai+1,i+1| −δ −|ai+1,1| + |ai+1,1|
|a1,1| (|a1,1| −δ) −|ai+1,1| · |a1,i+1|
|a1,1|
≤|ai+1,i+1| −δ −|ai+1,1| + |ai+1,1| −|ai+1,1| · |a1,i+1|
|a1,1|
= |ai+1,i+1| −|ai+1,1| · |a1,i+1|
|a1,1|
−δ
≤
ai+1,i+1 −ai+1,1a1,i+1
a1,1
 −δ
= |bi,i| −δ.
Thus, B is SDD of magnitude δ.
Corollary 3.27 (Gaussian elimination and SDD). If A ∈Cn×n is SDD, then
Gaussian elimination without pivoting applied to A proceeds to completion without
encountering any zero pivot elements.
Proof. We only need to proceed recursively using the previous two results.
Remark 3.28 (modiﬁed Gaussian elimination). In a variant of Gaussian elimination,
the pivot entry is normalized to one. Namely, given the system of linear equations













a1,1x1 + a1,2x2 + · · · + a1,nxn = f1,
a2,1x1 + a2,2x2 + · · · + a2,nxn = f2,
...
an,1x1 + an,2x2 + · · · + an,nxn = fn,
we could proceed as follows.
1. Using the ﬁrst equation, express x1 in terms of all the other variables to
obtain
x1 = a(1)
1,2x2 + · · · + a(1)
1,nxn + f (1)
1
.
Eliminate x1 from equations indexed 2 through n.
2. Using the second equation, express x2 only in terms of x3, . . . , xn:
x2 = a(2)
2,3x3 + · · · + a(2)
2,nxn + f (2)
2
.
Eliminate x2 from equations indexed 3 through n.

3.5 Special Matrices
55
3. Using the third equation, express x3 only in terms of x4, . . . , xn:
x3 = a(3)
3,4x4 + · · · + a(3)
3,nxn + f (3)
3
.
Eliminate x3 from equations indexed 4 through n.
i. For i = 4, . . . , n −1, using the ith equation, express xi only in terms of the
variables xi+1, . . . , xn:
xi = a(i)
i,i+1xi+1 + · · · + a(i)
i,nxn + f (i)
i
.
(3.10)
Eliminate xi from equations indexed i + 1 through n.
n. Using the last remaining equation, express xn as
xn = f (n)
n
.
n + 1. Use back substitution to solve for x.
We will call this the modiﬁed Gaussian elimination process. It works as long as
no zero pivots are encountered. For systems whose coeﬃcient matrix is SDD,
this process yields an interesting and desirable property. The following two results
address this case and give a new perspective to our methodology.
Lemma 3.29 (modiﬁed Gaussian elimination). Let n ≥2 and Ax = f be a system
of n equations with n unknowns, where the coeﬃcient matrix A = [ai,j] ∈Cn×n is
SDD of magnitude δ > 0. Then one may reduce the ﬁrst equation to the form
x1 = a(1)
1,2x2 + · · · + a(1)
1,nxn + f (1)
1
(3.11)
for some coeﬃcients a(1)
1,j , j = 2, . . . , n, and f (1)
1
, with no division by zero. Moreover,
n
X
j=2
|a(1)
1,j | < 1.
Finally, x1 can be eliminated from equations indexed 2 through n to obtain a sub-
system, A(1)x(1) = f (1), of n −1 equations with n −1 unknowns. The coeﬃcient
matrix A(1) has diagonal dominance of magnitude δ > 0.
Proof. Since the matrix A has diagonal dominance of magnitude δ > 0, we observe
that
|a1,1| ≥δ + |a1,2| + · · · + |a1,n| > 0,
so that a1,1 ̸= 0, and we can deﬁne a(1)
1,j = −a1,j/a1,1, for j = 2, . . . , n, and
f (1)
1
= f1/a1,1. Notice also that
n
X
j=2
|a(1)
1,j | = |a1,2| + · · · + |a1,n|
|a1,1|
< 1.
Now, to eliminate x1 from the system, we substitute (3.11) into all the remaining
equations. The ith equation, for i = 2, . . . , n, reads
ai,1x1 + ai,2x2 + · · · + ai,nxn = fi,

56
Systems of Linear Equations
and, when we substitute for x1, we ﬁnd, for i = 2, . . . , n,
(ai,2 + ai,1a(1)
1,2)x2 + (ai,3 + ai,1a(1)
1,3)x3 + · · · + (ai,n + ai,1a(1)
1,n)xn = f (1)
i
,
where
f (1)
i
= fi −ai,1f (1)
1
,
i = 2, . . . , n.
It is convenient to index the matrix A(1) starting at row and column 2 and ending
at row and column n. In this case, the entries of A(1) are
h
A(1)i
i,j = ai,j + ai,1a(1)
1,j ,
i, j = 2, . . . , n.
The proof that A(1) is SDD of magnitude δ > 0 follows as before.
We can now provide a suﬃcient condition for our modiﬁed Gaussian elimination
process to proceed to completion and include some further results.
Theorem 3.30 (modiﬁed Gaussian elimination, SDD case). Let n ≥2. Suppose
that A ∈Cn×n has diagonal dominance of magnitude δ > 0 and f ∈Cn is given.
Then the modiﬁed Gaussian elimination process (without pivoting) used to solve
Ax = f does not fail. Moreover, we have that, at every step,
n
X
j=i+1
|a(i)
i,j | < 1,
i = 1, . . . , n −1,
and
|f (i)
i
| ≤2
δ ∥f ∥∞,
i = 1, . . . , n.
Proof. To prove the ﬁrst part, proceed by induction, using the previous lemma
to show that the process does not fail. Invoking the fact that A(i−1) is SDD of
magnitude δ > 0, we can always conclude that
n
X
j=i+1
|a(i)
i,j | < 1,
i = 1, . . . , n −1.
For the second part, notice that, since A has diagonal dominance of magnitude
δ > 0, we can recall Theorem 3.24, which indicates
∥x∥∞= ∥A−1f ∥∞≤∥A−1∥∞∥f ∥∞≤1
δ ∥f ∥∞.

3.5 Special Matrices
57
Now, from (3.10), we see that
|f (i)
i
| =

xi −
n
X
j=i+1
a(i)
i,j xj

≤|xi| +
n
X
j=i+1
|a(i)
i,j ||xj|
≤

1 +
n
X
j=i+1
|a(i)
i,j |

∥x∥∞
≤2∥x∥∞
≤2
δ ∥f ∥∞,
as we intended to show.
3.5.2
Positive Deﬁnite Matrices
Deﬁnition 3.31 (HPD matrices). A matrix A ∈Cn×n is called Hermitian positive
semi-deﬁnite (HPSD) if and only if A = AH and
xHAx ≥0,
∀x ∈Cn.
A is called Hermitian positive deﬁnite (HPD) if and only if A = AH and
xHAx > 0,
∀x ∈Cn
⋆.
Remark 3.32 (notation). We often use the symbol Cn×n
Her to denote the vector
space of Hermitian matrices. For real, symmetric matrices we use the notation
Rn×n
sym .
Theorem 3.33 (properties of HPD matrices). Suppose that A = [ai,j] ∈Cn×n is
HPD. Then
1. ai,i > 0 for all i = 1, . . . , n.
2. σ(A) ⊂(0, ∞).
3. det(A) > 0 and tr(A) = Pn
i=1 ai,i > 0.
4. For all ∅̸= S ⊆{1, . . . , n}, we have that A(S) is HPD.
5. |ai,j|2 ≤ai,iaj,j for all i ̸= j.
6. max1≤i,j≤n |ai,j| ≤max1≤i≤n |ai,i|.
Proof. We will prove statements 4–6 and leave the others for exercises; see
Problem 3.13.
4: Given S ⊆{1, . . . , n} nonempty. Suppose that y ∈C#(S)
⋆
is arbitrary. Deﬁne
x ∈Cn
⋆such that xi = 0 for all i ̸∈S; otherwise,
xik = yk,
k = 1, . . . , #(S),

58
Systems of Linear Equations
where
S = {i1, . . . , i#(S)}
and
ik < ik+1,
k = 1, . . . , #(S) −1.
Then A(S) = A(S)H and
y HA(S)y = xHAx > 0,
y ∈C#(S)
⋆
.
5: Let S = {r, s}, r < s. Then
A(S) =
ar,r
ar,s
as,r
as,s

;
consequently,
0 < det(A(S)) = ar,ras,s −|ar,s|2.
6: We argue by contradiction. Suppose that the element with largest modulus is
oﬀ-diagonal in, say, row r and column s, r ̸= s. Then
|ar,s| > ar,r
and
|ar,s| > as,s.
Thus,
|ar,s|2 > ar,ras,s,
which is a contradiction.
Theorem 3.34 (factorization of HPD matrices). Let n ≥2. Suppose that A ∈
Cn×n is HPD. There exists a unit lower triangular matrix L ∈Cn×n and a diagonal
matrix D ∈Rn×n with positive diagonal entries such that
A = LDLH.
Proof. Since A is HPD, for k = 1, . . . , n−1 the principal sub-matrices A(k) ∈Ck×k
are invertible. By Theorem 3.7 there exists a unit lower triangular matrix L and an
upper triangular matrix U such that A = LU.
Next, we claim that all of the diagonal elements of U are real and positive. This
follows by an induction argument, as in the proof of Theorem 3.7, and the fact
that det(A) = det(U). The details are left to the reader see Problem 3.14.
Now set D = diag(u1,1, . . . , un,n) and ˜U = D−1U. Then
A = LDD−1U = LD˜U.
It follows that, for i = 1, . . . , n, ˜ui,i = 1. Taking the conjugate transpose, we have
˜UHDLH = AH = A = LD˜U.
It follows that
L−1˜UHD = D˜UL−H.
(3.12)

3.5 Special Matrices
59
Recall that, by Theorem 3.4, L−1˜UH must be unit lower triangular and ˜UL−H must
be unit upper triangular. The only way for (3.12) to hold is for L−1˜UH and ˜UL−H
to be diagonal. But, as these products must be unit triangular, they are both equal
to the identity. In other words,
L = ˜UH,
and we have proven that
A = LDLH.
Corollary 3.35 (Cholesky factorization3). Let n ≥2. Suppose that A ∈Cn×n is
HPD. Then there is a lower triangular matrix L ∈Cn×n such that
A = LLH.
This is known as the Cholesky factorization.
Proof. From the last theorem, there is a unit lower triangular matrix ˜L and a
diagonal matrix D with positive real diagonal entries such that
A = ˜LD˜LH.
Suppose that D = diag(d1, . . . , dn). Deﬁne ˜D = diag(√d1, . . . , √dn). Then, setting
L = ˜L˜D, we see that
A = LLH.
The proof is complete.
Theorem 3.36 (uniqueness of Cholesky factorization). Let n ≥2. Suppose that
A ∈Cn×n is HPD. Then there is a unique lower triangular matrix L such that the
diagonal entries of L are positive real numbers and
A = LLH.
In other words, the Cholesky factorization is unique.
Proof. Suppose that there are two lower triangular matrices L1 and L2 with positive
real diagonal entries and
L1LH
1 = A = L2LH
2 .
Then
L−1
2 L1 = LH
2 L−H
1 ;
by Theorem 3.4, L−1
2 L1 is lower triangular and LH
2 L−H
1
is upper triangular. Thus,
there is a diagonal matrix D such that
L−1
2 L1 = D = LH
2 L−H
1 .
Therefore,
L1 = L2D
(3.13)
3 Named in honor of the French military oﬃcer and mathematician Andr´e-Louis Cholesky
(1875–1918).

60
Systems of Linear Equations
and
DLH
1 = LH
2 ,
or, equivalently,
L2 = L1DH.
(3.14)
Combining (3.13) and (3.14), we have
L1 = L2D = L1DHD.
Since L1 is invertible, the cancellation property holds and DHD = In. But since L1
and L2 have positive diagonal entries, so must D have positive diagonal entries. It
follows that D = In, which implies that L1 = L2.
Theorem 3.37 (HPD and spectrum). Suppose that A ∈Cn×n is Hermitian. Then
A is HPD if and only if σ (A) ⊂(0, ∞).
Proof. We only prove one direction here, as the other has already been proven.
Since A ∈Cn×n
Her , there exists a unitary matrix U and a diagonal matrix D =
diag (λ1, . . . , λn), where λi ∈σ(A), for i = 1, . . . , n, such that A = UHDU. Let
x ∈Cn
⋆. Set y = Ux and note that y ̸= 0. Then
xHAx = (Ux)HDUx = y HDy =
n
X
i=1
λi |yi|2 ≥min
1≤i≤n λiy Hy = min
1≤i≤n λi ∥y∥2
2 > 0.
This proves that A is HPD.
The matrix encountered in the next result comes up repeatedly in the text. It is
an example of an HPD matrix, but with all real entries. Such matrices are called
symmetric positive deﬁnite (SPD). This matrix is also an example of a Toeplitz
symmetric tridiagonal4 (TST) matrix.
Theorem 3.38 (TST matrix). Deﬁne A ∈R(n−1)×(n−1) via
A =


2
−1
0
· · ·
0
−1
2
...
...
...
0
...
...
−1
0
...
...
−1
2
−1
0
· · ·
0
−1
2


.
Then A is SPD. Let h = 1/n. The set
S = {w 1, w 2, . . . , w n−1} ,
where the ith component of w k is deﬁned via
[w k]i = sin(kπih) ,
is an orthogonal set of eigenvectors of A.
4 Named in honor of the German mathematician Otto Toeplitz (1881–1940).

3.5 Special Matrices
61
Proof. Note that for k = 1, . . . , n −1,
[Aw k]i = −sin(kπ(i −1)h) + 2 sin(kπih) −sin(kπ(i + 1)h)
= 2 sin(kπih) −2 cos(kπh) sin(kπih)
= (2 −2 cos(kπh)) sin(kπih)
= 2(1 −cos(kπh))[w k]i.
Hence, the distinct eigenvalues are λk = 2 −2 cos(kπh). To see that these are
strictly positive for k = 1, . . . , n −1, note that
1 > cos(kπh) > −1,
which implies that
−2 < −2 cos(kπh) < 2,
which implies that
0 = 2 −2 < 2 −2 cos(kπh) < 2 + 2 = 4.
Since A is symmetric, the eigenvectors associated with distinct eigenvalues are
orthogonal. A is SPD since its eigenvalues are strictly positive.
Proposition 3.39 (HPD and similarity transformations). Let A ∈Cm×m be HPD
and X ∈Cm×n with m ≥n have full rank. Then XHAX ∈Cn×n is HPD.
Proof. Notice that
(XHAX)H = XHAHX = XHAX.
Suppose that x ∈Cn
⋆is arbitrary. Since X is full rank, it follows that y = Xx ̸= 0,
i.e., y ∈Cm
⋆. Then
xHXHAXx = y HAy > 0,
since A is HPD.
Theorem 3.40 (HPD and Gaussian elimination). Let A ∈Cn×n be HPD and
represented as
A =
α
pH
p
ˆA

,
where α ∈C, p ∈Cn−1, and ˆA ∈C(n−1)×(n−1). After one step of Gaussian
elimination (without pivoting), A will be reduced to the matrix
α
pH
0
B

,
where B ∈C(n−1)×(n−1). Then B is HPD and the corresponding diagonal elements
of B are smaller than those of ˆA.

62
Systems of Linear Equations
Proof. Let us construct a matrix L ∈Cn×n such that
LA =
α
pH
0
B

,
if possible. Consider
L =
 1
0⊺
m
In−1

.
Then
LA =
 1
0⊺
m
In−1
 α
pH
p
ˆA

=

α
pH
αm + p
mpH + ˆA

.
Note that, since A is HPD, α > 0 and ˆA is HPD. Choosing m = −α−1p, we have
LA =
α
pH
0
ˆA −α−1ppH

.
Having successfully constructed L, we ﬁnd B = ˆA −α−1ppH. Notice that this is
not the only way to ﬁnd the matrix B.
Now let x ∈Cn−1
⋆
be arbitrary. Deﬁne y ∈Cn
⋆via
y =
γ
x

,
where γ ∈C is arbitrary. Then, since A is HPD,
0 < y HAy
=

γ
xH α
pH
p
ˆA
 γ
x

=

γ
xH αγ + pHx
γp + ˆAx

= α|γ|2 + γpHx + γxHp + xHˆAx.
Now we set γ = −α−1pHx. From the last calculation
0 < y HAy
= α−1|pHx|2 −α−1|pHx|2 −α−1|pHx|2 + xHˆAx
= xHˆAx −α−1|pHx|2
= xHBx.
This proves that B is HPD.
Now the diagonal elements of B, which must be positive since B is HPD, are
precisely [B]i,i = [ˆA]i,i −α−1|[p]i|2. Hence, 0 < [B]i,i ≤[ˆA]ii, since α−1|[p]i|2 ≥0.
Corollary 3.41 (HPD and Gaussian elimination). Suppose that A ∈Cn×n is HPD.
Then Gaussian elimination without pivoting proceeds to completion to produce a
unit lower triangular matrix L ∈Cn×n and an upper triangular matrix U ∈Cn×n
with positive diagonal elements such that A = LU.

3.5 Special Matrices
63
Proof. Apply recursively the previous result.
Theorem 3.42 (HPD criterion). A ∈Cn×n is HPD if and only if A = LLH, where
L ∈Cn×n is invertible.
Proof. Suppose that A = LLH, where L is invertible. Let x ∈Cn be arbitrary. Set
y = LHx. Since L is invertible, LH is invertible; and y = 0 if and only if x = 0.
Then
xHAx = xHLLHx =
 LHx
H LHx = y Hy = ∥y∥2
2 ≥0,
with equality if and only if x = 0. This proves that A is HPD.
The converse direction follows from the Cholesky factorization provided in
Corollary 3.35.
Theorem 3.43 (block matrices). Let k, m ∈N. Set n = k + m. Suppose that
A ∈Cn×n has the decomposition
A =
B
CH
C
D

,
where B ∈Ck×k, C ∈Cm×k, and D ∈Cm×m.
1. If A is HPD, then B, D, and S = D −CB−1CH are HPD. S is called the Schur
complement of B in A.
2. If A is HPD, the Cholesky factorization of A may be expressed in terms of the
matrix C and the Cholesky factorizations of B and S.
Proof. We prove each statement separately.
1. Since A is HPD, xHAx > 0 for any x ∈Cn
⋆. Let y ∈Ck
⋆and w ∈Cm
⋆be
arbitrary. Setting x =

y H, 0HH ∈Cn
⋆, we have
0 < xHAx = y HBy;
on the other hand, setting x =

0H, w HH ∈Cn
⋆, we have
0 < xHAx = w HDw.
Thus, B and D are HPD. More generally, suppose that x =

xH
1 , xH
2
H ∈Cn
⋆.
Then
0 < xHAx = xH
1 Bx1 + xH
1 CHx2 + xH
2 Cx1 + xH
2 Dx2.
Now pick x1 = −B−1CHx2 and suppose that x2 ̸= 0. Then
0 < xHAx
= xH
2 CB−HBB−1CHx2 −xH
2 CB−HCHx2 −xH
2 CB−1CHx2 + xH
2 Dx2
= xH
2
 D −CB−1CH
x2,
where we used the fact that B−1 = B−H — since B is HPD — on the last step.
It follows that S is HPD.

64
Systems of Linear Equations
2. Since A is HPD, there is a unique lower triangular matrix LA ∈Cn×n with positive
diagonal entries, such that A = LALH
A. Likewise there are unique lower triangular
matrices LB ∈Ck×k and LS ∈Cm×m, both with positive diagonal entries, such
that B = LBLH
B and S = LSLH
S . Suppose that
LA =
L1
O
M
L2

,
where L1 ∈Ck×k is lower triangular with positive diagonal entries; M ∈Cm×k;
and L2 ∈Cm×m is lower triangular with positive diagonal entries. Then
LALH
A =
L1
O
M
L2
 LH
1
MH
OH
LH
2

=
L1LH
1
L1MH
MLH
1
MMH + L2LH
2

=
B
CH
C
D

.
Comparing entries we conclude that
L1LH
1 = B
=⇒
L1 = LB,
MLH
1 = C
=⇒
M = CL−H
B ,
MMH + L2LH
2 = D
=⇒
L2LH
2 = D −CL−H
B L−1
B CH
=⇒
L2LH
2 = S
=⇒
L2 = LS,
and the result is proven.
3.5.3
Cholesky Factorization, Revisited
In this section, our aim is to produce a practical, computable algorithm for the
Cholesky factorization. Let us ﬁrst recall how Gaussian elimination acts on an
HPD matrix. Let us consider a simpliﬁed HPD matrix A ∈Cn×n:
A =
 1
w H
w
K

→L1A =
1
w H
0
ˆK

with
L1 =
 1
0⊺
m
In−1

,
so that
L1A =
 1
0⊺
m
In−1
  1
w H
w
K

=

1
w H
m + w
K + mw H

.
In other words, we must have m = −w and ˆK = K −ww H. Therefore,
A = L−1
1
1
w H
0
ˆK

=
 1
0⊺
w
In−1
 1
w H
0
K −ww H

,
so that, in the process, we lost that the matrix is Hermitian.
But being Hermitian is an important feature, and it would be desirable to keep
it throughout the Gaussian elimination process. To do so, we will introduce zeros

Problems
65
on the ﬁrst row to match the ones we introduced on the ﬁrst column. To achieve
this, notice that
1
w H
0
K −ww H

=
1
0⊺
0
K −ww H
 1
w H
0
In−1

.
Notice that the factor on the right is nothing but L−H
1 . In combining these two
operations, we get
A =
 1
0⊺
w
In−1
 1
0⊺
0
K −ww H
 1
w H
0
In−1

.
This is the main idea behind the algorithmic version of the Cholesky factorization.
In general, since α2 = a1,1 > 0, we can proceed as follows:
A =
α2
w H
w
K

=
 α
0⊺
1
αw
In−1
 "
1
0⊺
0
K −
1
a1,1 ww H
# α
1
αw H
0
In−1

= RH
1 A1R1,
where R1 is an upper triangular matrix. Applying this repeatedly we obtain a
computable version of the Cholesky factorization, yielding an upper triangular
matrix R with positive diagonal entries such that
A = RHInR = RHR.
Remark 3.44 (terminology). Since αk = √ak,k uniquely determines the algorithm,
this Cholesky factorization is sometimes also referred to as the square root method.
Listing 3.7 describes how to compute this factorization. Once this factorization is
computed, system (3.1), in the case that A is HPD, can be solved rather eﬃciently.
Proposition 3.45 (complexity of Cholesky factorization). Let A ∈Cn×n be HPD.
The Cholesky factorization algorithm requires of the order of 1
3n3 operations.
Proof. We just need to notice that the innermost loop of Listing 3.7 requires two
(2) operations. Thus, to leading order, the total complexity is
n
X
i=2
i−1
X
j=1
j−1
X
k=1
2 ≈1
3n3.
Problems
3.1
Write the computational formulas used to solve a system of equations with a
lower triangular coeﬃcient matrix. In other words, describe the forward substitution
algorithm.
3.2
Show that when the generic back (forward) substitution algorithm is applied
to solve a system of n ∈N unknowns, there are n divisions, n2−n
2
multiplications,
and n2−n
2
additions/subtractions. Use this to show that complexity, i.e., the number
of operations of this algorithm, is
n + n2 −n
2
+ n2 −n
2
= n2.

66
Systems of Linear Equations
3.3
Find the complexity of the algorithm presented in Listing 3.1 to solve a system
with a tridiagonal coeﬃcient matrix.
3.4
Given a nonsingular matrix A ∈Cn×n propose an algorithm, based on
Gaussian elimination, to ﬁnd A−1.
3.5
Prove Proposition 3.10.
3.6
Show that a column-s complete elementary matrix is invertible and ﬁnd its
inverse.
3.7
Prove Theorem 3.13.
3.8
Prove Proposition 3.17.
3.9
Prove Lemma 3.18.
3.10
Complete the proof of Theorem 3.21.
3.11
Suppose that A ∈Rn×n is a nonsingular matrix whose leading principal
sub-matrices are all nonsingular. Partition A as
A =
A11
A12
A21
A22

,
where A11 ∈Rk×k.
a)
Show that there is a matrix M such that
 I
O
−M
I
 A11
A12
A21
A22

=
A11
A12
O
eA22

and write out the explicit expresssions for M and eA22.
b)
Show that
A11
A12
A21
A22

=
 I
O
M
I
 A11
A12
O
eA22

.
c)
The leading principal sub-matrices of A11 are, of course, all nonsingular. Prove
that eA22 is also nonsingular.
d)
By the previous statement, both A11 and eA22 have LU decompositions, say
A11 = L1U1 and eA22 = L2U2. Show that
A =
 L1
O
ML1
L2
 U1
L−1
1 A12
O
U2

,
which is the LU decomposition of A.
3.12
Suppose that A
∈
Rn×n is SDD with positive diagonal entries and
nonpositive oﬀ-diagonal entries. Show that, in this case, A−1 exists and contains
only nonnegative elements.
Hint: Use the procedure for inverting a matrix using Gaussian elimination.
3.13
Complete the proof of Theorem 3.33.
3.14
Complete the proof of Theorem 3.34.
3.15
Suppose that A ∈Cn×n is Hermitian, has nonnegative (real) diagonal
elements, and is SDD. Prove that it is HPD.

Listings
67
Listings
1
function [x, err] = TriDiagonal( a, b, c, f )
2
% Solution of a linear system of equations with a tridiagonal
3
% matrix.
4
%
5
%
a(k) x(k-1) + b(k) x(k) + c(k) x(k+1) = f(k)
6
%
7
% with a(1) = c(n) = 0.
8
%
9
% Input
10
%
a(1:n), b(1:n), c(1:n) : the coefficients of the system
11
%
matrix
12
%
f(1:n) : the right hand side vector
13
%
14
% Output
15
%
x(1:n) : the solution to the linear system of equations, if
16
%
no division by zero occurs
17
%
err : = 0, if no division by zero occurs
18
%
= 1, if division by zero is encountered
19
n = length(f);
20
alpha = zeros(n,1);
21
beta = zeros(n,1);
22
err = 0;
23
if abs(b(1)) > eps( b(1) )
24
alpha(1) = -c(1)/b(1);
25
beta(1) = f(1)/b(1);
26
else
27
err = 1;
28
return;
29
end
30
for k=2:n
31
denominator = a(k)*alpha(k-1) + b(k);
32
if abs(denominator) > eps( denominator )
33
alpha(k) = - c(k)/denominator;
34
beta(k) = ( f(k) - a(k)*beta(k-1) )/denominator;
35
else
36
err = 1;
37
return;
38
end
39
end
40
if abs(a(n)*alpha(n-1) + b(n)) > eps( b(n) )
41
x(n) = ( f(n) - a(n)*beta(n-1) )/( a(n)*alpha(n-1) + b(n) );
42
else
43
err = 1;
44
return;
45
end
46
for k=n-1:-1:1
47
x(k) = alpha(k)*x(k+1) + beta(k);
48
end
49
end
Listing 3.1 Solution of a system of equations with a tridiagonal coeﬃcient matrix.

68
Systems of Linear Equations
1
function [x, err] = CyclicallyTriDiagonal( a, b, c, f )
2
% Solution of a cyclically tridiagonal linear system of equations:
3
%
4
%
b(1) x(
1) + c(1) x(
2) + a(1) x(
n) = f(1),
5
%
a(k) x(k-1) + b(k) x(
k) + c(k) x(k+1) = f(k),
6
%
c(n) x(
1) + a(n) x(n-1) + b(n) x(
n) = f(n).
7
%
8
% Input
9
%
a(1:n), b(1:n), c(1:n) : the coefficients of the system
10
%
matrix
11
%
f(1:n) : the right hand side vector
12
%
13
% Output
14
%
x(1:n) : the solution to the linear system of equations, if
15
%
no division by zero is encountered
16
%
err : = 0, if division by zero does not occur
17
%
= 1, if division by zero occurs
18
n = length(f);
19
err = 0;
20
newa = a(2:n);
21
newa(1) = 0;
22
newb = b(2:n);
23
newc = c(2:n);
24
newc(n-1) = 0;
25
newf = f(2:n);
26
[u, err] = TriDiagonal( newa, newb, newc, newf );
27
if err == 1
28
return;
29
end
30
newf = zeros(n-1, 1);
31
newf(1) = - a(2);
32
newf(n-1) = -c(n);
33
[v, err] = TriDiagonal( newa, newb, newc, newf );
34
if err == 1
35
return;
36
end
37
x = zeros(n,1);
38
denmoniator = b(1) + c(1)*v(1) + a(1)*v(n-1);
39
if abs(denominator) > eps( denorminator )
40
x(1) =
( f(1) - c(1)*u(1) - a(1)*u(n-1) )/denominator;
41
else
42
err = 1;
43
return;
44
end
45
for k=2:n
46
x(k) = u(k-1) + x(1)*v(k-1);
47
end
48
end
Listing 3.2 Solution of a system of equations with a cyclically tridiagonal
coeﬃcient matrix.

Listings
69
1
function [L, U, err] = LUFactSimple( A )
2
% LU factorization of a square matrix.
3
%
4
% Input
5
%
A(1:n,1:n) : the matrix to be factorized
6
%
7
% Output
8
%
L(1:n,1:n), U(1:n,1:n) : the factors in A = LU, if Gaussian
9
%
elimination proceeds to completion
10
%
err : = 0 if Gaussian elimination proceeds to completion
11
%
= 1 if a zero pivot is encountered
12
n = size(A,1);
13
U = A;
14
L = eye(n);
15
err = 0;
16
for k=1:n-1
17
for j=k+1:n
18
if abs( U(k,k) ) > eps( U(k,k) )
19
L(j,k) = U(j,k)/U(k,k);
20
else
21
err = 1;
22
return;
23
end
24
for t = k:n
25
U(j,t) = U(j,t)-L(j,k)*U(k,t);
26
end
27
end
28
end
29
end
Listing 3.3 LU factorization.
1
function [L, U, P, err] = LUFactPivot( A )
2
% LU factorization with pivoting of a square matrix.
3
%
4
% Input
5
%
A(1:n,1:n) : the matrix to be factorized
6
%
7
% Output
8
%
L(1:n,1:n), U(1:n,1:n), P(1:n,1:n) : the factors in A = P'LU
9
%
err : = 0 if no error was encountered
10
%
= 1 if a division by zero occurred
11
n = size(A,1);
12
U = A;
13
L = eye(n);
14
P = eye(n);
15
err = 0;
16
for k=1:n-1
17
i=k;
18
for t=k:n
19
if abs( U(t,k) ) > abs( U(i,k) )
20
i=t;
21
end
22
end

70
Systems of Linear Equations
23
for t=k:n
24
temp = U(k,t);
25
U(k,t) = U(i,t);
26
U(i,t) = temp;
27
end
28
for t=1:k-1
29
temp = L(k,t);
30
L(k,t) = L(i,t);
31
L(i,t) = temp;
32
end
33
for t=1:n
34
temp = P(k,t);
35
P(k,t) = P(i,t);
36
P(i,t) = temp;
37
end
38
if abs( U(k,k) ) > eps( U(k,k) )
39
for j=k+1:n
40
L(j,k) = U(j,k)/U(k,k);
41
for t=k:n
42
U(j,t) = U(j,t) - L(j,k)*U(k,t);
43
end
44
end
45
else
46
err = 1;
47
return;
48
end
49
end
50
end
Listing 3.4 LU factorization with pivoting.
1
function [Afact, swaps, err] = LUFactEfficient( A )
2
% Efficient implementation of LU factorization with partial
3
% pivoting.
4
%
5
% Input
6
%
A(1:n,1:n) : the matrix to be factorized into A = P'LU
7
%
8
% Output
9
%
Afact(1:n,1:n) : a matrix containing the matrices L and U
10
%
below and above the diagonal, respectively
11
%
swaps : the indices that indicate the permutations
12
%
(row swaps)
13
%
err : = 0 if no the factorization finished successfully
14
%
= 1 if a division by zero was encountered
15
n=size(A,1);
16
swaps = 1:n;
17
err = 0;
18
for k=1:n-1
19
i = k;
20
for t=k:n
21
if abs( A(t,k) ) > abs( A(i,k) )
22
i=t;
23
end
24
end

Listings
71
25
tt = swaps(k);
26
swaps(k) = swaps(i);
27
swaps(i) = tt;
28
if abs( A( swaps(k), k ) <= eps( A( swaps(k), k) )
29
err = 1;
30
return;
31
end
32
for i=k+1:n
33
xmult = A(swaps(i),k)/A(swaps(k),k);
34
A( swaps(i), k ) = xmult;
35
for j=k+1:n
36
A(swaps(i),j) = A(swaps(i),j) - xmult*A(swaps(k),j);
37
end
38
end
39
end
40
Afact = A;
41
end
Listing 3.5 Eﬃcient implementation of LU factorization with pivoting.
1
function [x, err] = Solve( A, f )
2
% Solves the system Ax = f.
3
%
4
% Input
5
%
A(1:n,1:n) : the coefficient matrix
6
%
f(1:n) : the RHS vector
7
%
8
% Output:
9
%
x(1:n) : the solution vector
10
%
err : = 0 if the solution was found successfully
11
%
= 1 if an error occurred during the process
12
n = length(f);
13
err = 0;
14
[AA, swaps, err] = LUFactEfficient( A );
15
if err == 1
16
return
17
end
18
for k=1:n-1
19
for i=k+1:n
20
f( swaps(i) ) = f( swaps(i) ) - AA( swaps(i), k ) ...
21
*f( swaps(k) );
22
end
23
end
24
if abs( AA( swaps(n), n ) ) > eps( AA(swaps(n),n) )
25
x(n) = f( swaps(n) )/AA( swaps(n), n );
26
else
27
err = 1;
28
return;
29
end
30
for i=n-1:-1:1
31
xsum = f( swaps(i) );
32
for j=i+1:n
33
xsum = xsum - AA( swaps(i), j )*x(j);
34
end
35
if abs( AA( swaps(i), i ) ) > eps( AA( swaps(i), i ) )

72
Systems of Linear Equations
36
x(i) = xsum/AA( swaps(i), i );
37
else
38
err = 1;
39
return;
40
end
41
end
42
end
Listing 3.6 Solution of (3.1) after LU factorization.
1
function [R, err] = CholeskyDecomposition( A )
2
% Choleksy factorization of the HPD matrix A:
3
%
A = R'*R
4
%
5
% Input:
6
%
A(1:n,1:n) : an HPD matrix
7
%
8
% Output:
9
%
R(1:n,1:n) : The upper triangular matrix satisfying A = R'*R
10
%
err : = 0 if the decomposition finished successfully
11
%
= 1 if an error occurred
12
err = 0;
13
n = size(A,1);
14
R = zeros(n,n);
15
R(1,1) = sqrt( A(1,1) );
16
for i=2:n
17
for j=1:i-1
18
sum = A(i,j);
19
for k=1:j-1
20
sum = sum - R(k,i)* conj( R(k,j) );
21
end
22
if abs( R(j,j) ) > eps( R(j,j) )
23
R(j,i) = sum/R(j,j);
24
else
25
err = 1;
26
return;
27
end
28
end
29
sum = A(i,i);
30
for k=1:i-1
31
sum = sum - R(k,i)*conj( R(k,i) );
32
end
33
R(i,i) = sqrt( sum );
34
end
35
end
Listing 3.7 Computation of the Cholesky factorization for an HPD matrix.

4
Norms and Matrix Conditioning
In this chapter, we look at some qualitative aspects of the solution to (3.1), which
we studied in Chapter 3. While it is usually assumed that the coeﬃcient matrix
A ∈Cn×n and the right-hand-side vector f ∈Cn are known, ﬁxed, and perfectly
represented in our computational device, here we are interested to see what happens
to the solution to our linear system if these are somehow perturbed.
To ﬁx ideas, suppose, for example, that A is invertible and that x ∈Cn is the
solution to the (ideal) system
Ax = f .
(4.1)
Now suppose that, in some hypothetical computing device, A and f are perturbed
in storage: A →A+δA and f →f +δf , where δA ∈Cn×n and δf ∈Cn. Assuming
that A + δA is invertible, there is some δx ∈Cn such that x + δx ∈Cn is the
solution to the perturbed system
(A + δA) (x + δx) = f + δf .
(4.2)
Clearly, δx measures the error resulting from the perturbations to our data. How
large is this error vector? How large is the relative error, ∥δx∥
∥x∥? How do the error
vector and relative error relate to the sizes of the perturbations? It turns out that
the answers to our questions depend upon the so-called condition number of the
matrix A, deﬁned as
κ(A) = ∥A∥
A−1 .
Of course, it is not practical to compute κ(A) according to the formula above, as
it involves the inverse of the coeﬃcient matrix. But, often, the condition number
can be accurately estimated. We will see that, if the condition number is large, the
relative error can be quite large, even when other measures of error are actually
small.
Now, since all the ideas in this chapter depend heavily upon the notions of vector
norms on Cn and induced matrix norms on Cn×n, we urge the reader to, if necessary,
review these concepts in Sections A.3 and 1.2.
4.1
The Spectral Radius
We begin with a deﬁnition.

74
Norms and Matrix Conditioning
Deﬁnition 4.1 (spectral radius). Suppose that A ∈L(V), where V is a complex
n-dimensional vector space. The spectral radius of A is
ρ(A) = max {|λ| | λ ∈σ(A)} .
An analogous deﬁnition is made for any matrix A ∈Cn×n.
For self-adjoint operators, the spectral radius has a very precise meaning.
Theorem 4.2 (self-adjoint operator). Let V be an n-dimensional complex inner
product space. Suppose that ∥x∥= (x, x)1/2 is the Euclidean norm. Let A ∈L(V)
be self-adjoint. Then the induced norm satisﬁes
∥A∥= ρ(A).
Proof. Since A: V →V is self-adjoint, there exists an orthonormal basis of
eigenvectors S = {e1, . . . , en}, i.e., (ei, ej) = δi,j, V = span(S), and Aei = λiei.
Expanding x ∈V in this basis, i.e., x = Pn
i=1 xiei with xi ∈C, we see that
Ax =
n
X
i=1
λixiei.
Since this basis is orthonormal,
∥x∥2 =
n
X
i=1
|xi|2
and
∥Ax∥2 =
n
X
i=1
|λi|2|xi|2.
With this at hand we notice that
∥Ax∥≤max {|λ| |λ ∈σ(A)} ∥x∥,
which implies that
∥A∥≤ρ(A).
Problem 4.2 gives the reverse inequality, and this concludes the proof.
For more general operators and norms, all that can be established is the following.
Theorem 4.3 (norms and spectral radius). Suppose that ∥· ∥: Cn×n →R is any
induced matrix norm. Then there is a constant C > 0 such that
ρ(A) ≤∥A∥≤C
p
ρ(AHA),
∀A ∈Cn×n.
Proof. Let ∥· ∥Cn be the vector norm that induces ∥· ∥. Owing to Problem A.12,
this norm is equivalent to ∥· ∥2, i.e., there are constants 0 < C1 ≤C2 for which
C1 ∥x∥Cn ≤∥x∥2 ≤C2 ∥x∥Cn ,
∀x ∈Cn.
This, in turn, implies that, if x ∈Cn
⋆,
∥Ax∥Cn
∥x∥Cn
≤C2
C1
∥Ax∥2
∥x∥2
≤C ∥A∥2 ≤C
p
ρ(AHA),
where we denoted C = C2/C1 and used Problem 1.29. Taking supremum over x ∈
Cn
⋆implies the upper bound. The lower bound is an exercise; see Problem 4.2.

4.1 The Spectral Radius
75
While the spectral radius may not necessarily be a norm it is almost one, as the
following, rather technical, result shows.
Theorem 4.4 (spectral radius and norms). For every matrix A ∈Cn×n and any
ε > 0, there is a norm ∥· ∥A,ε : Cn →R such that the induced matrix norm
∥M∥A,ε = sup
x∈Cn⋆
∥Mx∥A,ε
∥x∥A,ε
=
sup
∥x∥A,ε=1
∥Mx∥A,ε ,
∀M ∈Cn×n,
satisﬁes
∥A∥A,ε ≤ρ(A) + ε.
Proof. Appealing to the Schur factorization, Lemma 1.46, there is a unitary matrix
P ∈Cn×n and an upper triangular matrix B ∈Cn×n such that
A = PHBP.
The diagonal elements of B are the eigenvalues of A. Let us write
B = Λ + U,
where Λ = diag(λ1, . . . , λn) with
σ(A) = {λ1, . . . , λn} = σ(B),
and U = [ui,j] ∈Cn×n is strictly upper triangular. Let δ > 0 be arbitrary. Deﬁne
D = diag
 1, δ−1, . . . , δ1−n
.
Next, deﬁne
C = DBD−1 = Λ + E,
where
E = DUD−1.
Now observe that E, like U, must be strictly upper triangular, and the elements of
E = [ei,j] must satisfy
ei,j =
(
0,
j ≤i,
δj−iui,j,
j > i.
Consequently, the elements of E can be made arbitrarily small in modulus, depending
on our choice of δ.
Now notice that
A = P−1D−1CDP,
and, since DP is nonsingular, the following deﬁnes a norm and an induced matrix
norm: for any x ∈Cn,
∥x∥⋆= ∥DPx∥2 ,
and, for any M ∈Cn×n,
∥M∥⋆= sup
∥x∥⋆=1
∥Mx∥⋆.

76
Norms and Matrix Conditioning
Observe that
∥Ay∥⋆= ∥DPAy∥2 = ∥CDPy∥2 .
Deﬁne z = DPy. Then
∥Ay∥⋆= ∥Cz∥2 =
√
zHCHCz.
But
CHC =
 ΛH + EH
(Λ + E) = ΛHΛ + M(δ),
where
M(δ) = EHΛ + ΛHE + EHE.
As an exercise, the reader should prove that, for a given matrix A, there is a
constant K1 > 0 such that
∥M(δ)∥2 ≤K1δ
for all 0 < δ ≤1. Thus, using the deﬁnition of the spectral radius, the Cauchy–
Schwarz inequality, and induced norm consistency,
zHCHCz = zHΛHΛz + zHM(δ)z
≤
max
k=1,...,n |λk|2zHz + ∥z∥2 ∥M(δ)z∥2
≤
 ρ(A)2 + ∥M(δ)∥2

∥z∥2
2
≤
 ρ(A)2 + K1δ

∥z∥2
2 .
To ﬁnish up, note that
∥y∥⋆=
(DP)−1z

⋆= ∥z∥2 .
Hence, ∥y∥⋆= 1 if and only if ∥z∥2 = 1 and
{∥Ay∥⋆| ∥y∥⋆= 1} = {∥Cz∥⋆| ∥z∥2 = 1} .
Consequently,
∥A∥⋆= sup
∥y∥⋆=1
∥Ay∥⋆
= sup
∥z∥2=1
∥Cz∥2
≤sup
∥z∥2=1
p
ρ(A)2 + K1δ ∥z∥2
=
p
ρ(A)2 + K1δ ≤ρ(A) + K2δ,
for some K2 > 0, for all 0 < δ ≤1. The result follows on choosing
δ ≤min (1, ε/K2) .
As a consequence of this result we can provide an extension of Theorem 4.2 for
a broader class of matrices.

4.1 The Spectral Radius
77
Corollary 4.5 (equality). Suppose that A ∈Cn×n is diagonalizable. Then there
exists a norm ∥· ∥⋆: Cn →R such that the induced matrix norm satisﬁes
∥A∥⋆= ρ(A).
Proof. See Problem 4.3.
We provide now a notion of convergence for matrices and, with the aid of the
spectral radius, provide necessary and suﬃcient conditions for convergence.
Deﬁnition 4.6 (convergence). We say that the square matrix A ∈Cn×n is
convergent to zero if and only if Ak →O ∈Cn×n, i.e., if and only if
lim
k→∞∥Ak∥→0
for any matrix norm ∥· ∥: Cn×n →R.
Remark 4.7 (norm equivalence). We recall that, owing to Theorem A.29, all norms
on Cm×n, whether induced or not, are equivalent. For this reason, the norm in this
last deﬁnition does not matter.
Theorem 4.8 (convergence criteria). Let A ∈Cn×n. The following are equivalent.
1. A is convergent to zero.
2. ρ(A) < 1.
3. For all x ∈Cn,
lim
k→∞Akx = 0.
Proof. (1 =⇒2) We recall two facts. First, if λ ∈σ(A), then λk ∈σ(Ak). This
follows from the Schur factorization: if A = UTUH, where T is upper triangular and
U is unitary, then
Ak = UTkUH.
Second, ρ(A) ≤∥A∥, for any induced matrix norm. Therefore,
0 ≤ρk(A) = ρ(Ak) ≤
Ak .
Thus, if
Ak →0, it follows that
ρk(A) →0.
This implies that ρ(A) < 1.
(2 =⇒1) By Theorem 4.4, there is an induced matrix norm ∥· ∥⋆such that
∥A∥⋆≤ρ(A) + ε
for any ε > 0. Recall that the choice of ∥· ∥⋆depends upon A and ε > 0. Since, by
assumption, ρ(A) < 1, there is an ε > 0 such that ρ(A) + ε < 1, and, therefore,
an induced norm ∥· ∥⋆such that
∥A∥⋆≤ρ(A) + ε < 1.

78
Norms and Matrix Conditioning
Then, using sub-multiplicativity,
Ak
⋆≤∥A∥k
⋆≤(ρ(A) + ε)k →0.
Consequently,
lim
k→∞
Ak
⋆= 0.
(1 =⇒3) Suppose that limk→∞
Ak
∞= 0. Let x ∈Cn be arbitrary. Then
Akx

∞≤
Ak
∞∥x∥∞→0
since
Ak
∞→0. Hence,
Akx

∞→0. This implies that
lim
k→∞Akx = 0.
(3 =⇒1) Suppose that, for any x ∈Cn,
lim
k→∞Akx = 0.
Then it follows that, for all x, y ∈Cn,
y HAkx →0.
Now suppose that y = ei and x = ej, then, since
y HAkx = eH
i Akej =

Ak
i,j ,
it follows that
lim
k→∞

Ak
i,j = 0.
This implies that
lim
k→∞
Ak
max = 0.
Hence, A is convergent to zero.
As an easy consequence we obtain a suﬃcient criterion for convergence to zero.
Corollary 4.9 (convergence condition). Let M ∈Cn×n. Assume that, for some
induced matrix norm ∥· ∥: Cn×n →R,
∥M∥< 1,
then M is convergent to zero.
Proof. Recall that, for each and every induced matrix norm ∥· ∥: Cn×n →R,
ρ(M) ≤∥M∥,
where ρ(M) is the spectral radius of M.
The spectral radius of a matrix can also be estimated via powers of this matrix.

4.1 The Spectral Radius
79
Proposition 4.10 (upper bound). Suppose that ∥· ∥: Cn×n →R is an induced
matrix norm. Then, for all A ∈Cn×n and k ∈N, we have
ρ(A) ≤∥Ak∥1/k.
Proof. See Problem 4.4.
In fact, for large values of k the previous upper bound is tight.
Theorem 4.11 (Gelfand1). Suppose that ∥· ∥: Cn×n →R is an induced matrix
norm. Then, for all A ∈Cn×n, we have
ρ(A) = lim
k→∞∥Ak∥1/k.
Proof. Let 0 < ε < ρ(A)/2. We deﬁne two matrices
A± =
1
ρ(A) ± εA.
Clearly,
ρ(A±) =
ρ(A)
ρ(A) ± ε,
which implies that
ρ(A+) < 1 < ρ(A−).
Therefore, A+ is convergent to zero; consequently, there is a number K+ ∈N such
that, for k ≥K+, we have
∥Ak
+∥< 1
=⇒
1
(ρ(A) + ε)k ∥Ak∥< 1
=⇒
∥Ak∥< (ρ(A) + ε)k.
On the other hand, the bound of Proposition 4.10 implies that
1 < ρ(A−)k ≤∥Ak
−∥=
1
(ρ(A) −ε)k ∥Ak∥.
In conclusion, for suﬃciently large k, we have shown that
ρ(A) −ε < ∥Ak∥1/k < ρ(A) + ε
and the result follows.
Corollary 4.12 (product of matrices). Suppose that ∥· ∥: Cn×n →R is an induced
matrix norm. Let {Ai}k
i=1 ⊂Cn×n be a family of matrices that commute, i.e.,
AiAj = AjAi,
∀i, j = 1, . . . , k.
Then
ρ
 kY
i=1
Ai
!
≤
kY
i=1
ρ(Ai).
Proof. See Problem 4.6.
1 This result is due to the Ukrainian–American mathematician Izrail Moiseevic Gelfand
(1913–2009).

80
Norms and Matrix Conditioning
4.2
Condition Number
We can now introduce the notion of the condition number of a matrix.
Deﬁnition 4.13 (condition number). Suppose that A ∈Cn×n is invertible. The
condition number of A with respect to the matrix norm ∥· ∥: Cn×n →R is
κ(A) = ∥A∥
A−1 .
Before we get on to the meaning and utility of the condition number, let us
present some elementary properties of this quantity.
Proposition 4.14 (properties of κ). Suppose that ∥· ∥: Cn×n →R is an induced
matrix norm and that A ∈Cn×n is invertible. Then
κ(A) = ∥A∥
A−1 ≥1.
Furthermore,
1
A−1 ≤∥A −B∥
for any B ∈Cn×n that is singular. Consequently,
1
κ(A) ≤
inf
det(B)=0
∥A −B∥
∥A∥
.
(4.3)
Proof. See Problem 4.10.
Remark 4.15 (interpretation of κ(A)). Estimate (4.3) is useful in a couple of ways.
First, it says that if A is close in norm to a singular matrix B, then κ(A) will be
very large. Thus, nearly singular matrices are ill-conditioned. Second, this formula
gives an upper bound on κ(A)−1.
There are some nice formulas for and estimates of the condition number with
respect to the induced matrix 2-norm, which is usually called the spectral condition
number and denoted κ2.
Proposition 4.16 (spectral condition number). Suppose that A ∈Cn×n is invertible
and ∥· ∥2 : Cn×n →R is the induced matrix 2-norm.
1. If the singular values of A are σ1 ≥σ2 ≥· · · ≥σn > 0,
κ2(A) = ∥A∥2
A−1
2 = σ1
σn
.
2. If the eigenvalues of B = AHA are 0 < µ1 ≤µ2 ≤· · · ≤µn, then
κ2(A) =
rµn
µ1
.
(4.4)
3. Let, for p ∈[1, ∞], κp(A) = ∥A∥p ·
A−1
p, where ∥· ∥p is the induced matrix
norm with respect to the p-norm. We have
κ2(A) ≤
p
κ1(A)κ∞(A).

4.2 Condition Number
81
4.
1
κ2(A) =
inf
det(B)=0
∥A −B∥2
∥A∥2
.
5. If A is Hermitian, then
κ2(A) = maxλ∈σ(A) |λ|
minλ∈σ(A) |λ| .
6. If A is Hermitian positive deﬁnite with eigenvalues 0 < λ1 ≤λ2 ≤· · · ≤λn,
then
κ2(A) = λn
λ1
.
Proof. See Problem 4.12.
The ﬁrst and last items in Proposition 4.16 give an easy geometric interpretation
of the spectral condition number. It is the ratio of the maximal stretching to the
minimal stretching under the action of the matrix A.
Example 4.1
It is well known that A ∈Cn×n is singular if and only if det(A) = 0.
Thus, it may be thought that, similar to item 4 of Proposition 4.16, the quantity
| det(A)| may also be used to quantify how close to singular a matrix can be. The
following example shows that this is not necessarily the case.
Let A ∈Rn×n have the singular value decomposition
A = UΣVH,
σj = 1
j ,
j = 1, . . . , n.
Then
| det(A)| =
n
Y
j=1
1
j = 1
n!,
but, owing to the ﬁrst item in Proposition 4.16,
κ2(A) = σ1
σn
=
1
1/n = n.
Deﬁnition 4.17 (error and residual). Given a matrix A ∈Cn×n and a vector f ∈Cn
with A nonsingular, let x ∈Cn solve (3.1). The residual vector with respect to
x′ ∈Cn is deﬁned as
r = r(x′) = f −Ax′ = A(x −x′).
The error vector with respect to x′ is deﬁned as
e = e(x′) = x −x′.
Consequently,
Ae = r.

82
Norms and Matrix Conditioning
It often happens that we have obtained an approximate solution x′ ∈Cn. We
would like to have some measure of the error, but a direct measurement of the
error would require the exact solution x. The next best thing is the residual, which
is an indirect measurement of the error, as the last deﬁnition suggests. The next
theorem tells us how useful the residual is in determining the relative size of the
error.
Theorem 4.18 (relative error estimate). Let A ∈Cn×n be invertible, f ∈Cn
⋆, and
x solves (3.1). Assume that ∥· ∥: Cn×n →R is the induced matrix norm with
respect to the vector norm ∥· ∥: Cn →R. Then
1
κ(A)
∥r∥
∥f ∥≤∥e∥
∥x∥≤κ(A) ∥r∥
∥f ∥.
Proof. Since e = A−1r, using consistency of the induced norm
∥e∥=
A−1r
 ≤
A−1 ∥r∥.
Likewise,
∥f ∥= ∥Ax∥≤∥A∥∥x∥,
which implies that
1
∥x∥≤∥A∥1
∥f ∥.
Combining the ﬁrst and third inequalities, we obtain the claimed upper bound,
∥e∥
∥x∥≤∥A∥
A−1 ∥r∥
∥f ∥= κ(A) ∥r∥
∥f ∥.
The rest of the proof is left to the reader as an exercise; see Problem 4.13.
4.3
Perturbations and Matrix Conditioning
Let us now return to the motivating problem with which we began the chapter,
i.e., trying to estimate how much the solution to (3.1) changes under perturbations
to the data A and f . It is not diﬃcult to imagine a scenario where the data are
perturbed. Perturbations may come from measurement errors, and so they are not
exactly known. Or, perhaps, perturbations may be introduced when numbers are
stored in ﬁnite precision in the computer. Let δA ∈Cn×n and δf ∈Cn be known
(or estimable) perturbations of the data. The problem that is actually solved then is
(A + δA) (x + δx) = f + δf .
We then wish to provide an estimate for how large is the relative error ∥δx∥
∥x∥. Formally,
the perturbation δx ∈Cn is
δx = (A + δA)−1 (f + δf ) −x,

4.3 Perturbations and Matrix Conditioning
83
provided that A + δA is invertible. Observe that, since A is invertible, we have
(A + δA)−1 = (A(In + A−1δA))−1 = (In + A−1δA)−1A−1.
Therefore, we have reduced the question of the invertibility of A + δA to a more
general question: Given M ∈Cn×n, when is In ± M invertible?
Theorem 4.19 (Neumann series). Suppose that ∥· ∥: Cn×n →R is an induced
matrix norm with respect to the vector norm ∥· ∥: Cn →R. Let M ∈Cn×n with
∥M∥< 1. Then In −M is invertible,
(In −M)−1 ≤
1
1 −∥M∥
and
(In −M)−1 =
∞
X
k=0
Mk.
The series P∞
k=0 Mk is known as the Neumann series.2
Proof. Using the reverse triangle inequality and consistency, since ∥M∥< 1, for
any x ∈Cn,
∥(In −M)x∥≥
∥x∥−∥Mx∥
 ≥(1 −∥M∥)∥x∥.
This inequality implies that if (In −M)x = 0, then x = 0. Therefore, In −M is
invertible.
To obtain the norm estimate, notice that
1 = ∥In∥
=
(In −M)(In −M)−1
=
(In −M)−1 −M(In −M)−1
≥
(In −M)−1 −∥M∥
(In −M)−1 ,
where we have used the reverse triangle inequality and sub-multiplicativity. The
upper bound of the quantity
(In −M)−1 now follows.
Finally, for N ∈N, deﬁne
RN =
N
X
k=0
Mk.
Let us show that RN(In −M) →In as N →∞. Indeed,
RN(In −M) =
N
X
k=0
Mk(In −M) =
N
X
k=0
Mk −
N
X
k=0
Mk+1 = In −MN+1,
which shows that, as N →∞,
∥RN(In −M) −In∥=
MN+1 ≤∥M∥N+1 →0,
using the sub-multiplicativity of the induced norm and the fact that ∥M∥< 1.
2 Named in honor of the German mathematician Carl Gottfried Neumann (1832–1925).

84
Norms and Matrix Conditioning
A consequence of Theorem 4.19 is that the set of invertible matrices is open. In
this context, this means that any matrix that is suﬃciently close to an invertible
one will also be invertible.
Corollary 4.20 (inverse of a perturbation). Suppose that ∥· ∥: Cn×n →R is an
induced matrix norm with respect to the vector norm ∥· ∥: Cn →R. If R ∈Cn×n
is invertible and T ∈Cn×n satisﬁes
R−1 ∥R −T∥< 1,
then T is invertible.
Proof. Notice that
T = R(In −(In −R−1T));
therefore, T will be invertible provided that In −(In −R−1T) is invertible. Deﬁne
M = In −R−1T to conclude that, according to Theorem 4.19, we need ∥M∥< 1.
Observe that
∥M∥=
In −R−1T
 =
R−1(R −T)
 ≤
R−1 ∥R −T∥< 1,
and so T is invertible.
With these results at hand we can give an estimate for the relative size of the
error in the problem we were originally interested in. Let us ﬁrst begin by assuming
that δf = 0.
Theorem 4.21 (relative error estimate, case δf = 0). Let A ∈Cn×n be invertible,
f ∈Cn, and x ∈Cn solves (3.1). Suppose that ∥· ∥: Cn×n →R is the induced
matrix norm with respect to the vector norm ∥· ∥: Cn →R. Assume that δA ∈
Cn×n satisﬁes ∥A−1δA∥< 1 and that x + δx ∈Cn solves the perturbed problem
(A + δA)(x + δx) = f .
Then δx is uniquely determined and
∥δx∥
∥x∥≤
κ(A)
1 −κ(A) ∥δA∥
∥A∥
∥δA∥
∥A∥.
Proof. Let us begin by repeating a previous computation. Since A is invertible, we
can write A+δA = A(In +A−1δA). Deﬁne M = −A−1δA, which satisﬁes ∥M∥< 1.
Invoking Theorem 4.19 we conclude that A + δA is invertible. Therefore, δx exists
and is unique. In addition, we have
(A + δA)−1 = (In −M)−1A−1
and
(In −M)−1 ≤
1
1−∥M∥. Moreover, the obvious estimate
∥M∥≤
A−1 ∥δA∥
implies that
1
1 −∥M∥≤
1
1 −
A−1∥δA∥.

4.3 Perturbations and Matrix Conditioning
85
Now
δx = (A + δA)−1f −A−1f
= (In −M)−1A−1f −A−1f
= (In −M)−1(A−1f −(In −M)A−1f )
= (In −M)−1MA−1f
= (In −M)−1Mx.
Consequently,
∥δx∥≤
(In −M)−1 ∥M∥∥x∥≤
A−1 ∥δA∥
1 −∥A−1∥∥δA∥∥x∥=
κ(A)
1 −κ(A) ∥δA∥
∥A∥
∥δA∥
∥A∥∥x∥.
The result follows.
To conclude our discussion, let us see what happens when we perturb both A
and f .
Theorem 4.22 (relative error estimate, general case). Let A ∈Cn×n be invertible,
f ∈Cn, and x ∈Cn solves (3.1). Suppose that ∥· ∥: Cn×n →R is an induced
matrix norm with respect to the vector norm ∥· ∥: Cn →R. Assume that δA ∈
Cn×n satisﬁes
A−1δA
 < 1, δf ∈Cn is given, and x + δx ∈Cn satisﬁes the
perturbed problem
(A + δA)(x + δx) = f + δf .
Then δx is uniquely determined and
∥δx∥
∥x∥≤
κ(A)
1 −κ(A) ∥δA∥
∥A∥
∥δf ∥
∥f ∥+ ∥δA∥
∥A∥

.
Proof. Let M = −A−1δA. We then have that x = A−1f and x + δx = (In −
M)−1A−1(f + δf ). Therefore,
δx = (In −M)−1A−1(f + δf ) −A−1f
= (In −M)−1  A−1f + A−1δf −(In −M)A−1f

= (In −M)−1(A−1δf + MA−1f ).
This shows that
∥δx∥≤
1
1 −κ(A) ∥δA∥
∥A∥
 A−1δf
 +
MA−1f

.
Notice also that
MA−1f
 = ∥Mx∥≤∥M∥∥x∥≤
A−1 ∥δA∥∥x∥= κ(A)∥δA∥
∥A∥∥x∥
and
A−1δf
 ≤
A−1 ∥δf ∥∥Ax∥
∥Ax∥≤κ(A)∥δf ∥
∥f ∥∥x∥.

86
Norms and Matrix Conditioning
The previous three inequalities, when combined, yield
∥δx∥≤
κ(A)
1 −κ(A) ∥δA∥
∥A∥
∥δf ∥
∥f ∥+ ∥δA∥
∥A∥

∥x∥,
as we intended to show.
Problems
4.1
Does the spectral radius, introduced in Deﬁnition 4.1, deﬁne a norm?
4.2
Suppose that V is a ﬁnite-dimensional complex normed vector space. Show
that if ∥· ∥is any induced operator norm, then ρ(A) ≤∥A∥for all A ∈L(V).
4.3
Prove Corollary 4.5.
4.4
Prove Proposition 4.10.
4.5
Prove, using induction on n ∈N, that if A, B ∈Cn×n commute, then they
are simultaneously triangularizable, i.e., there is a nonsingular P ∈Cn×n for which
P−1AP and P−1BP are upper triangular.
Hint: Show that if (λ, x) is an eigenpair of A, then so is (λ, Bx).
4.6
Prove Corollary 4.12.
Hint: See the previous problem.
4.7
Let A ∈Cn×n and µ ∈C be such that |µ| > ρ(A). Show that the series
∞
X
k=0
1
µk Ak
converges to (In −µ−1A)−1.
4.8
Let A ∈Cn×n. Deﬁne
Sk = In + A + · · · + Ak.
a)
Prove that the sequence {Sk}∞
k=0 converges if and only if A is convergent to
zero.
b)
Prove that if A is convergent to zero, then I −A is nonsingular and
lim
k→∞Sk = (I −A)−1 .
4.9
Show that if ∥A∥< 1 for some induced matrix norm, then I−A is nonsingular
and
1
1 + ∥A∥≤
(I −A)−1 ≤
1
1 −∥A∥.
4.10
Prove Proposition 4.14.
4.11
Suppose that ∥· ∥: Cn×n →R is the induced norm with respect to the vector
norm ∥· ∥: Cn →R. Show that if λ is an eigenvalue of AHA, where A ∈Cn×n,
then
0 ≤λ ≤
AH ∥A∥.
4.12
Prove Proposition 4.16.
4.13
Complete the proof of Theorem 4.18.
4.14
Let A ∈Cn×n be nonsingular. Show that the condition numbers κ∞(A) and
κ1(A) will not change after permutation of rows or columns.

Problems
87
4.15
Suppose that ∥· ∥: Cn×n →R is a matrix norm and κ is the condition
number deﬁned with respect to it. Let A ∈Cn×n be nonsingular and 0 ̸= α ∈C.
Show that κ(αA) = κ(A).
4.16
Show that if Q ∈Cn is unitary, then κ2(Q) = 1.
4.17
Suppose that ∥· ∥: Cn×n →R is the induced norm with respect to the vector
norm ∥· ∥: Cn →R and κ is the condition number deﬁned with respect to this
norm. Let A ∈Cn be invertible. Show that
κ(A) ≥maxλ∈σ(A) |λ|
minλ∈σ(A) |λ| .
4.18
Let A = RHR with R ∈Cn×n nonsingular. Give an expression for κ2(A) in
terms of κ2(R).
4.19
Let A ∈Cn×n be invertible, f ∈Cn
⋆, and x ∈Cn solves (3.1). Suppose that
∥· ∥: Cn×n →R is the induced norm with respect to the vector norm ∥· ∥: Cn →
R. Let the perturbations δx, δf ∈Cn satisfy Aδx = δf , so that A (x + δx) =
f + δf .
a)
Prove the error (or perturbation) estimate
1
κ(A)
∥δf ∥
∥f ∥≤∥δx∥
∥x∥≤κ(A)∥δf ∥
∥f ∥.
b)
Show that, for any invertible matrix A, the upper bound for ∥δx∥
∥x∥above can be
attained for suitable choices of f and δf .
4.20
Show that, for every nonsingular A ∈Cn×n, we have
1
n ≤κ∞(A)
κ2(A) ≤n.
4.21
Let
A =
1.000 0
2.000 0
1.000 1
2.000 0

.
a)
Calculate κ1 (A) and κ∞(A).
b)
Use (4.3) to obtain upper bounds on κ1(A)−1 and κ∞(A)−1.
c)
Suppose that you wish to solve Ax = f , where f =
 3.000 0
3.000 1

. Instead of x
you obtain the approximation x′ = x + δx =
 0.000 0
1.500 0

. For this approximation
you discover f ′ = f + δf =
 3.000 0
3.000 0

, where Ax′ = f ′. Calculate ∥δx∥1/∥x∥1
exactly. (You will need the exact solution, of course.) Then use the general
estimate
∥δx∥
∥x∥≤κ(A)∥δf ∥
∥f ∥
to obtain an upper bound for ∥δx∥1/∥x∥1. How good is ∥δf ∥1/∥f ∥1 as indicator
of the size of ∥δx∥1 / ∥x∥1?

5
Linear Least Squares Problem
We begin by providing some motivation for what we want to accomplish in this
chapter — namely, the solution of problems like the following. Suppose that we
are given a table of values
(xk, yk),
k = 1, . . . , n,
that is obtained, say, by a series of measurements. We wish to ﬁnd a function
y = y(x) that, in some sense, best represents the data of this table. In particular,
we may have some reason to believe that, apart from some measurement error,
there really is a such a function y — for example, y(x) = sin(2πx) — that would
otherwise generate the observed data in the table.
We immediately see two possible solutions.
1. Find a function, say a polynomial p, that interpolates the points (xk, yk), i.e.,
yk = p(xk) for all k = 1, . . . , n. There are suﬃcient conditions that indicate
when such a polynomial exists and is unique, and there are algorithms to
eﬃciently ﬁnd it. (We shall examine these later in Chapter 9.) However, this
procedure can be unstable. But more importantly, since we are dealing with
measurements, this approach does not take into account the fact that we might
have errors and/or redundancy in the data.
2. Find a simple function — a linear function, for example y = c1x + c0, where
c0, c1 ∈C — that ﬁts the data in some exact or approximate sense. If we
demand that it matches the data exactly, then
yk = c1xk + c0,
k = 1, . . . , n.
This can also be expressed in vector form as


y1
...
yn

=


x1
1
...
...
xn
1


c1
c0

,
so that we obtain a system of linear equations of the form Ac = y with
A =


x1
1
...
...
xn
1

∈Cn×2,
c =
c1
c0

∈C2,
y =


y1
...
yn

∈Cn.

5.1 Linear Least Squares: Full Rank Setting
89
Usually, n is much larger than 2, so that we end up with an overdetermined
system of equations, i.e., there are more equations than unknowns. There is no
solution in general.
On the other hand, if we only approximately enforce the matching conditions
yk ≈c1xk + c0,
k = 1, . . . , n,
then it is not clear how to proceed. There may be an inﬁnite number of ways
that we can reasonably satisfy the approximation. The purpose of this chapter is
to provide a way to tackle this problem, i.e., we will give a precise way to enforce
the matching conditions and show how the numbers c0, c1 may be computed.
5.1
Linear Least Squares: Full Rank Setting
In general, it will be important to consider systems of the form Ax = f with a
rectangular matrix A ∈Cm×n, m > n. The question is this: When can we expect
that this system has a solution? The answer is almost never!
The following result is standard in the theory of linear systems of equations.
Theorem 5.1 (existence). Let A ∈Cm×n and f ∈Cm. If rank(A) coincides with
the rank of the augmented matrix [A|f ], then the system of equations Ax = f has
at least one solution.
Proof. Let A = [c1, . . . , cn] for ci ∈Cm. If rank(A) = rank([A|f ]), then f ∈
col(A). Thus, there are xi ∈C, for i = 1, . . . , n, such that
n
X
i=1
xici = f .
Setting x = [x1, . . . , xn]⊺∈Cn, we immediately see that Ax = f .
Systems of equations Ax = f that have solutions are called consistent, and the
previous theorem, essentially, is telling us that a system is consistent provided that
f ∈im(A). But, in general, there is no reason to expect that the data f are in the
range of the coeﬃcient matrix A. What do we do in this case? For a given vector
z ∈Cn, we can always compute its residual vector
r(z) = f −Az.
The vector z ∈Cn is a solution of Ax = f if and only if r(z) = 0. If it is not
possible to ﬁnd a solution, the next best thing may be to ﬁnd z that gives the
smallest residual vector, measured in some norm.
Deﬁnition 5.2 (generalized solution). Suppose that ∥· ∥: Cm →R is a norm.
Given A ∈Cm×n, m ≥n, and f ∈Cm, we say that x ∈Cn is a weak or generalized
solution of the system Ax = f if and only if
x ∈argmin
w∈Cn ∥r(w)∥= argmin
w∈Cn ∥f −Aw∥.

90
Linear Least Squares Problem
We say that x ∈Cn is a least squares solution of the system Ax = f if and only if
x ∈argmin
w∈Cn ∥r(w)∥2
ℓ2(Cm) = argmin
w∈Cn ∥f −Aw∥2
ℓ2(Cm).
When these minima exist and are unique, we replace ∈with =.
Remark 5.3 (squaring). Observe that, for the special case of the 2-norm, we square
the norm. This is a cosmetic alteration, since x minimizes ∥r(w)∥2
ℓ2(Cm) if and only
if it minimizes ∥r(w)∥ℓ2(Cm) over all w ∈Cn.
Using the 2-norm leads to a relatively simple theory, and deriving the equations
that determine the approximations we seek is straightforward. To this end, let us
deﬁne
Φ(z) = ∥r(z)∥2
ℓ2(Cm),
r(z) = f −Az,
∀z ∈Cn.
(5.1)
We wish to ﬁnd conditions that lead to a minimum of Φ(x). Let δx ∈Cn be
arbitrary. The variation of Φ can be computed via
Φ(x + δx) −Φ(x) = (f −A(x + δx), f −A(x + δx))2 −(f −Ax, f −Ax)2
= (r −Aδx, r −Aδx)2 −(r, r)2
= (r, r)2 −(Aδx, r)2 −(r, Aδx)2 + (Aδx, Aδx)2 −(r, r)2
= −(r, Aδx)2 −(r, Aδx)2 + (Aδx, Aδx)2
= −2ℜ
 (AHr, δx)2

+ (Aδx, Aδx)2.
Thus, if x ∈Cn satisﬁes
0 = AHr = AH (f −Ax) ,
(5.2)
then x is a least squares solution to Ax = f (and a minimizer of Φ(x)). Note that
(5.2) is equivalent to
AHAx = AHf ,
which is called the normal equation. The following result provides necessary and
suﬃcient conditions for when it has a unique solution.
Lemma 5.4 (full rank). Let A ∈Cm×n with m ≥n. Then AHA is Hermitian positive
deﬁnite (HPD) if and only if A is full rank, i.e., rank(A) = n.
Proof. Clearly the matrix AHA is Hermitian. By construction, this matrix is also
nonnegative deﬁnite since, for any x ∈Cn,
(AHAx, x)2 = (Ax, Ax)2 = ∥Ax∥2 ≥0.
( =⇒) Suppose that AHA is HPD and, to reach a contradiction, that rank(A) < n.
This is equivalent to saying that there is a nonzero x ∈Cn such that Ax = 0. But
then we must have AHAx = 0 and (AHAx, x)2 = 0, contradicting the assumption
that AHA is positive deﬁnite.
( ⇐= ) Let us now assume that A is of full rank. To reach a contradiction, suppose
that AHA is not positive deﬁnite. There must be a nonzero x ∈Cn
⋆for which
0 = (AHAx, x)2 = (Ax, Ax)2 = ∥Ax∥2
2,

5.1 Linear Least Squares: Full Rank Setting
91
which implies that
Ax = 0 =⇒x ∈ker(A).
This contradicts the assumption that A has full rank.
Before we tackle the general case, let us consider the case that the data are
composed of real numbers.
Theorem 5.5 (least squares: real case). Let A ∈Rm×n with m ≥n and rank(A) =
n and let f ∈Rm. The vector x ∈Rn is the unique least squares solution to
Ax = f , i.e.,
x = argmin
w∈Rn ∥f −Aw∥2
ℓ2(Rm) ,
(5.3)
if and only if x is the unique solution to the normal equation
A⊺Ax = A⊺f .
(5.4)
Proof. ( =⇒) Suppose that x ∈Rn solves the least squares problem. Set r =
f −Ax. Now ﬁx y ∈Rn and deﬁne
g(s) = Φ(x + sy) = ∥r −sAy∥2
2
for all s ∈R, where Φ( · ) is as deﬁned in (5.1). Then, for any s ∈R,
g(0) = Φ(x) ≤Φ(x + sy) = g(s),
since x is a least squares solution. Calculating, we ﬁnd
g(s) = (r −sAy)⊺(r −sAy)
= r ⊺r −sy ⊺A⊺r −sr ⊺Ay + s2y ⊺A⊺Ay
= g(0) −2s r ⊺Ay + s2y ⊺A⊺Ay.
Since A⊺A is symmetric positive deﬁnite (SPD), g is a positive quadratic function
of one variable with a global minimum at s = 0. Hence,
0 = dg
ds

s=0
= −2r ⊺Ay
for arbitrary y ∈Rn. From this condition, we conclude that r ⊺A = 0⊺. This is
equivalent to (5.4).
( ⇐= ) Now suppose that x ∈Rn solves the normal equation (5.4). Set r = f −Ax.
This implies that r ⊺Ay = 0 for all y ∈Rn, or, equivalently, r ∈im(A)⊥. Then
Φ(x + y) = (r −Ay)⊺(r −Ay)
= Φ(x) −r ⊺Ay −yA⊺r + y ⊺A⊺Ay
= Φ(x) + y ⊺A⊺Ay
≥Φ(x),
since y ⊺A⊺Ay ≥0 for any y. More importantly, since A⊺A is SPD,
Φ(x + y) > Φ(x),
∀y ∈Rn
⋆.
Thus, x ∈Rn is a least squares solution.

92
Linear Least Squares Problem
Now let us treat the complex case. Note the slight diﬀerence in the approach.
Theorem 5.6 (least squares: complex case). Let A ∈Cm×n with m ≥n and
rank(A) = n and let f ∈Cm. The vector x ∈Cn is the unique least squares
solution to Ax = f , i.e.,
x = argmin
w∈Cn ∥f −Aw∥2
ℓ2(Cm) ,
(5.5)
if and only if x is the unique solution to the normal equation
AHAx = AHf .
(5.6)
Proof. ( =⇒) Suppose that x ∈Cn solves the least squares problem. Set r =
f −Ax. Now ﬁx y ∈Cn and deﬁne
g(s, t) = Φ(x + zy) = ∥r −zAy∥2
2
for all z = s + it ∈C with s, t ∈R, i = √−1. Then, for any s, t ∈R,
g(0, 0) = Φ(x) ≤Φ(x + zy) = g(s, t),
since x is a least squares solution. Calculating, we ﬁnd
g(s, t) = (r −zAy)H (r −zAy)
= r Hr −¯zy HAHr −zr HAy + |z|2y HAHAy
= g(0, 0) −2 ℜ
 zr HAy

+
 s2 + t2
y HAHAy
= g(0, 0) −2s ℜ
 r HAy

+ 2t ℑ
 r HAy

+
 s2 + t2
y HAHAy.
Since AHA is HPD, g is a positive quadratic function of two variables. Hence,
0 = ∂g
∂s

s,t=0
= −2 ℜ
 r HAy

,
0 = ∂g
∂t

s,t=0
= 2 ℑ
 r HAy

for arbitrary y ∈Cn. From these two conditions, we conclude that r HA = 0⊺.
Hence, r ∈im(A)⊥, as desired.
( ⇐= ) This step is more or less the same as in the real case. Suppose that x ∈Cn
satisﬁes r ∈im(A)⊥. Set r = f −Ax. Then
Φ(x + y) = (r + Ay)H (r + Ay)
= Φ(x) + r HAy + y HAHr + y HAHAy
= Φ(x) + y HAHAy
≥Φ(x),
since y HAHAy ≥0 for any y. And, since AHA is HPD,
Φ(x + y) > Φ(x),
∀y ∈Cn
⋆.
Thus, x ∈Cn is a least squares solution.

5.2 Projection Matrices
93
Remark 5.7 (weighted least squares). Instead of doing our least squares compu-
tations using the norm ∥· ∥ℓ2(Cm), we could, in fact, use any norm on Cn that arises
from an inner product. For example, suppose that ( · , · )w : Cn×Cn →C is an inner
product and ∥· ∥w is the norm induced by this inner product. Then we can use the
inner product ( · , · )w to construct a slightly diﬀerent least squares theory, a kind
of w-weighted least squares theory, if you like. Instead of the normal equation,
we will get an interesting analogue. For instance, suppose that B ∈Cn×n is HPD.
Consider the inner product ( · , · )B, deﬁned by
(x, y)B = y HBx,
∀x, y ∈Cn.
Deﬁne
ΦB(z) = ∥r(z)∥2
B,
r(z) = f −Az,
∀z ∈Cn.
(5.7)
What is the analogue of the normal equations for this case? See Problem 5.5.
5.2
Projection Matrices
The previous section presented the basic theory for the linear least squares problem
for the full rank case. To get a more detailed picture, and, in particular, to analyze
the rank-deﬁcient case, we need to develop some more tools. The ﬁrst one of these
is projection matrices.
Deﬁnition 5.8 (projection matrix). The square matrix P ∈Cn×n is called a
projection matrix if and only if it is idempotent, i.e.,
P2 = P.
Proposition 5.9 (image of P). Let P ∈Cn×n be a projection matrix and v ∈im(P).
Then
Pv = v.
Proof. If v ∈im(P), then there is a vector w ∈Cn such that Pw = v. Then
Pv = P(Pw) = P2w = Pw = v.
Theorem 5.10 (properties of a projection matrix). Suppose that P ∈Cn×n is a
projection matrix. Then In −P is also a projection matrix and
1. im(In −P) = ker(P).
2. ker(In −P) = im(P).
3. im(P) ∩ker(P) = {0}.
4. im(In −P) ∩ker(In −P) = {0}.
Proof. We will prove the ﬁrst property and leave the remaining ones as an exercise;
see Problem 5.2. Suppose that x ∈im(In −P). Then there is a vector y ∈Cn such
that (In −P)y = x. So,
Px = P(In −P)y = Py −P2y = 0.

94
Linear Least Squares Problem
Consequently Px = 0 and x ∈ker(P).
Suppose now that x ∈ker(P). Then Px = 0. Therefore,
(In −P)x = x,
which implies that x ∈im(In −P).
Deﬁnition 5.11 (sum of subspaces). Let S1, S2 ⊆Cn be subspaces. Recall that
we write S1, S2 ≤Cn for short. Then
S1 + S2 = {w ∈Cn | w = v 1 + v 2, ∃v i ∈Si, i = 1, 2} .
Proposition 5.12 (property of the sum). Let S1, S2 ≤Cn. Then S1 + S2 ≤Cn.
Proof. See Problem 5.6.
Deﬁnition 5.13 (complementary subspaces). Suppose that S1, S2 ≤Cn. If S1 +
S2 = Cn and S1 ∩S2 = {0}, then we call S1 and S2 complementary subspaces
and we write S1 ⊕S2 = Cn.
Theorem 5.14 (decomposition). Suppose that P ∈Cn×n is a projection matrix.
Then im(P) ⊕ker(P) = Cn, i.e., im(P) and ker(P) are complementary.
Proof. This follows from Theorem 5.10 and the simple decomposition
v = Pv + v −Pv.
Theorem 5.15 (existence). Let S1, S2 ≤Cn be complementary subspaces. Then
there is a projection matrix P ∈Cn×n such that
S1 = im(P)
and
S2 = ker(P).
Proof. Suppose that
Bi = {w (i)
1 , . . . , w (i)
ki } ⊂Si
is a basis for Si, i = 1, 2. Then it is left to the reader to prove that the set
B = B1 ∪B2
is a basis for Cn = S1 ⊕S2. Now deﬁne a mapping P: Cn →Cn such that
P(w (1)
j
) = w (1)
j
,
j = 1, . . . , k1,
and
P(w (2)
j
) = 0,
j = 1, . . . , k2.
We require P to be linear, so that
P


k1
X
j=1
c(1)
j
w (1)
j
+
k2
X
j=1
c(2)
j
w (2)
j

=
k1
X
j=1
c(1)
j
P(w (1)
j
) +
k2
X
j=1
c(2)
j
P(w (2)
j
)
=
k1
X
j=1
c(1)
j
w (1)
j
.

5.2 Projection Matrices
95
It is now straightforward to prove that P2 = P and im(P) = S1 and ker(P) = S2,
as required.
Deﬁnition 5.16 (orthogonal subspaces). Two subspaces S1, S2 ≤Cn are called
orthogonal if and only if
(v 1, v 2)ℓ2(Cn) = (v 1, v 2)2 = v H
2 v 1 = 0
for all v 1 ∈S1 and v 2 ∈S2.
Proposition 5.17 (orthogonality). If S1, S2 ≤Cn are orthogonal subspaces, then
S1 ∩S2 = {0}.
Proof. See Problem 5.7.
Proposition 5.18 (orthogonal decomposition). Suppose that L ≤Cn has dimen-
sion 1 ≤k < n. Then L⊥is a complementary subspace of dimension n −k,
L ⊕L⊥= Cn.
Furthermore, the decomposition of any w ∈Cn into
w = x + y,
x ∈L,
y ∈L⊥
is unique.
Proof. See Problem 5.8.
Remark 5.19 (notation). In light of Propositions 5.17 and 5.18, we see that
whenever S1, S2 ≤Cn are orthogonal, then they are complementary. Moreover,
whenever s1 ∈S1 and s2 ∈S2 we must necessarily have that sH
2 s1 = 0. We will
indicate this fact by
Cn = S1
⊥⊕S2.
As the concepts of complementary and orthogonal subspaces can be naturally
extended to any vector space that has an inner product, a similar notation will be
used in this case.
Deﬁnition 5.20 (orthogonal projection). The matrix P ∈Cn×n is called an
orthogonal projection if and only if P = P2 and im(P) and ker(P) are orthogonal
subspaces.
Theorem 5.21 (characterization of orthogonal projection). Let P ∈Cn×n be a
projection matrix. Then P is an orthogonal projection if and only if P = PH.
Proof. Suppose that PH = P. Let v 1 ∈ker(P) and v 2 ∈im(P) be arbitrary. Then
Pv 2 = v 2,
(In −P)v 1 = v 1,
and
v H
2 v 1 = (Pv 2)H (In −P)v 1 = v H
2 PH(In −P)v 1 = v H
2 (P −P2)v 1 = 0.
Thus, im(P) and ker(P) are orthogonal subspaces, which implies that P is an
orthogonal projection.

96
Linear Least Squares Problem
Suppose now that P is an orthogonal projection. Set
S1 = im(P),
S2 = ker(P)
with
dim(S1) = k < n,
dim(S2) = n −k.
We want to prove that PH = P, using the fact that S1 and S2 are orthogonal
subspaces of Cn. Let
B1 = {q1, . . . , qk} ⊂S1,
B2 = {qk+1, . . . , qn} ⊂S2
be orthonormal bases for the respective spaces. This is always possible owing to
the Gram–Schmidt algorithm, as described in Theorem A.38. We leave it as an
exercise for the reader to prove that B = B1 ∪B2 is an orthonormal basis for Cn.
(Use the fact that S1 and S2 are complementary orthogonal subspaces.)
By our construction,
Pqj =
(
qj,
j = 1, . . . , k,
0,
j = k + 1, . . . , n.
Set
Q =


|
|
q1
· · ·
qn
|
|

∈Cn×n.
Then
PQ =


|
|
|
|
q1
· · ·
qk
0
· · ·
0
|
|
|
|

∈Cn×n
and
QHPQ =
Ik
O
O
On−k

= Σ ∈Cn×n.
Consequently,
P = QΣQH
and PH = P.
Remark 5.22 (representation). In the notation of the proof of Theorem 5.21, let
us observe that P = ˆQˆQH, where
ˆQ =


|
|
q1
· · ·
qk
|
|

∈Cn×k,
n > k.
Theorem 5.23 (special projectors). Let k
∈{1, . . . , n}. Suppose that the
collection of vectors {q1, . . . , qk} ⊂Cn is orthonormal. Deﬁne
ˆQ =


|
|
q1
· · ·
qk
|
|

∈Cn×k,

5.2 Projection Matrices
97
then the matrices
P = ˆQˆQH,
In −P = In −ˆQˆQH
are orthogonal projectors.
Deﬁnition 5.24 (rank-one projection). Suppose that q ∈Cn, ∥q∥2 = 1. The
matrix P = qqH is called a rank-one orthogonal projection. The complement,
In −qqH, is called a rank-(n −1) orthogonal projection.
Theorem 5.25 (norm of a projection). Let P ∈Cn×n be a nonzero projection
matrix. Then ∥P∥2 ≥1. Moreover, ∥P∥2 = 1 if and only if P is an orthogonal
projection, i.e., PH = P.
Proof. Suppose that P is a projection matrix, i.e., P2 = P. Then, using the sub-
multiplicativity of the 2-norm,
∥P∥2 =
P2
2 ≤∥P∥2 ∥P∥2 .
Since P is not the zero matrix, ∥P∥2 > 0 and ∥P∥2 ≥1.
Now, for the second part, we have two directions to prove. Assume ﬁrst that
P2 = P and PH = P. In the proof of Theorem 5.21, we showed that
P = QΣQH,
where Q = [q1, . . . , qn] ∈Cn×n is unitary and
Σ =
Ik
O
O
On−k

for some 1 ≤k < n. In other words, Σ = diag[σ1, . . . , σk, 0, . . . , 0], where σi = 1,
1 ≤i ≤k. Recall that {q1, . . . , qk} is a basis for im(P) and {qk+1, . . . , qn}
is a basis for ker(P). In any case, P is Hermitian positive semi-deﬁnite (HPSD),
P = QΣQH is a unitary diagonalization of P, and
∥P∥2 = σ1 = ρ(P) = 1.
The other implication is proved by the contrapositive. Let us assume that P2 = P,
but PH ̸= P. We want to show that this implies that
∥P∥2 > 1.
Since PH ̸= P, ker(P) ∩im(P) = {0}, but
ker(P) ̸⊥im(P).
So, there is some nonzero vector v 1 ∈im(P) and some nonzero vector v 2 ∈ker(P)
such that
(v 1, v 2)2 = v H
2 v 1 ̸= 0.
Set
v = v 1 + αv 2,
α ∈C.

98
Linear Least Squares Problem
Then Pv = v 1. Now we want to choose α ∈C, so that
∥Pv∥2 > ∥v∥2 > 0.
Indeed, if such an α ∈C exists, then
∥P∥2 = sup
x∈Cn⋆
∥Px∥2
∥x∥2
≥∥Pv∥2
∥v∥2
> 1.
Since
Pv = P(v 1 + αv 2) = Pv 1 + αPv 2 = v 1,
it follows that
∥v 1∥2
2 = ∥Pv∥2
2
and
∥v∥2
2 = ∥v 1 + αv 2∥2
2 = ∥Pv∥2
2 + 2ℜ
 αv H
1 v 2

+ |α|2 ∥v 2∥2
2 = ∥Pv∥2
2 + T,
where
T = 2ℜ
 αv H
1 v 2

+ |α|2 ∥v 2∥2
2 .
Therefore, it suﬃces to choose α ∈C, so that T < 0, for, in that case,
∥v∥2
2 < ∥Pv∥2
2 .
The key is to choose α ∈C such that v ⊥v2, i.e., v H
2 v = 0. This is equivalent to
v H
2 (v 1 + αv 2) = 0
⇐⇒
v H
2 v 1 = −αv H
2 v 2
⇐⇒
¯αv H
2 v 1 = −|α|2 ∥v 2∥2
2 ∈R
⇐⇒
¯αv H
1 v 2 = −|α|2 ∥v 2∥2
2 ∈R
⇐⇒
αv H
1 v 2 = −|α|2 ∥v 2∥2
2 ∈R.
In this case,
T = −|α|2 ∥v 2∥2
2 ∈R.
Thus, the result follows upon choosing
α = −v H
2 v 1
∥v 2∥2
2
.
5.3
Linear Least Squares: The Rank-Deﬁcient Case
In this section, we address the the rank-deﬁcient case, i.e., the case when A ∈
Cm×n, m > n, and rank(A) < n. In this setting, it is not clear, at ﬁrst glance, that
the normal equations will have a solution.

5.3 Linear Least Squares: The Rank-Deﬁcient Case
99
Theorem 5.26 (general least squares). Suppose that m ≥n and the matrix A ∈
Cm×n is such that rank(A) ≤n, i.e., A may be rank deﬁcient. Let f ∈Cm be given.
Then the normal equations,
AHAx = AHf ,
(5.8)
always have at least one solution; for any two solutions, x1, x2 ∈Cn — in the case
that there are multiple solutions — we ﬁnd
r(x1) = r(x2),
where, for w ∈Cn, we deﬁned
r(w) = f −Aw.
In other words, the residual is always unique. Furthermore, the following are
equivalent.
1. xo ∈Cn is a solution to
x ∈argmin
w∈Cn Φ(w),
Φ(w) = ∥r(w)∥2
2 .
(5.9)
2. xo ∈Cn is a solution to the normal equations (5.8).
3. xo ∈Cn has the property that
r(xo) ⊥im(A).
Proof. First, let us prove that the normal equations (5.8) have a solution. Set
L = im(A). Then L and L⊥are complementary, orthogonal subspaces of Cm:
Cm = L ⊕L⊥.
Therefore, the decomposition
f = s + r,
s ∈L = im(A),
r ∈L⊥
is unique. Since s ∈im(A), there is at least one vector xo ∈Cn such that
Axo = s.
Since r ∈L⊥, r HAx = 0 for all x ∈Cn. This implies that
AHr = 0 ∈Cn.
Recall that
f = Axo + r,
which implies that
r = f −Axo = r(xo).
Hence,
AHr(xo) = 0 ∈Cn,
which is equivalent to the normal equations. Thus, the normal equations have
at least one solution. This solution is not necessarily unique. However, since the

100
Linear Least Squares Problem
decomposition f = s + r is unique, this is enough to prove that the residual is
uniquely determined. Nevertheless, we show this explicitly.
Suppose that x1, x2 ∈Cn are solutions to the normal equations (5.8). Recall
that the decomposition
f = s + r,
s ∈L = im(A),
r ∈L⊥
is unique. But
f = Ax1 + r(x1) = Ax2 + r(x2),
so s = Ax1 = Ax2 and r = r(x1) = r(x2).
Let us now prove the equivalences.
(2 ⇐⇒3) This follows from the calculations we carried out above.
(2 =⇒1) This argument is similar to previous ones. Suppose that xo ∈Cn is a
solution to the normal equation (5.8). Let w ∈Cn be arbitrary. Then
Φ(xo + w) = (r(xo) −Aw)H (r(xo) −Aw)
= Φ(xo) −(r(xo))HAw −w HAHr(xo) + w HAHAw
= Φ(xo) + w HAHAw ≥Φ(xo),
since AHA is HPSD. Note that we cannot claim that Φ(xo + w) > Φ(xo) for all
w ∈Cn
⋆, since we do not know that AHA is HPD. However, we can still assert that
xo is a minimizer, though it might not be unique.
(1 =⇒3) Suppose that xo ∈Cn is a solution to (5.9). We want to show that
r(xo) ⊥im(A). To get a contradiction, suppose that r(xo) ̸⊥im(A). If this is the
case, there is some q ∈im(A), q ̸= 0, such that qHr(xo) ̸= 0. Since q ∈im(A),
there is a vector w ∈Cn such that Aw = q. Since xo is a minimizer of Φ, for any
α ∈C,
∥r(xo)∥2
2 = Φ(xo)
≤Φ(xo + αw)
= Φ(xo) −α(r(xo))HAw −¯αw HAHr(xo) + |α|2w HAHAw
= ∥r(xo)∥2
2 −2ℜ
 ¯αqHr(xo)

+ |α|2qHq.
Thus, for all α ∈C,
2ℜ
 ¯αqHr(xo)

≤|α|2qHq.
Now set
α = qHr(xo)
qHq
to get a contradiction.

5.4 The QR Factorization and the Gram–Schmidt Algorithm
101
5.4
The QR Factorization and the Gram–Schmidt Algorithm
In this section, we construct a new factorization for the coeﬃcient matrix A that
will aid us in the practical solution of the least squares problem.
Theorem 5.27 (QR factorization). Suppose that A ∈Cm×n, m ≥n. There is an
upper triangular matrix ˆR = [ˆri,j] ∈Cn×n with nonnegative (real) diagonal elements
and a matrix ˆQ ∈Cm×n satisfying ˆQH ˆQ = In such that
A = ˆQˆR.
If rank(A) = n, then ˆri,i > 0 for all i = 1, . . . , n.
Proof. The proof is by induction on the number of columns of A, n ≥1.
(n = 1) A = a ∈Cm. If a ̸= 0, then
ˆQ =
1
∥a∥2
a,
ˆR = [∥a∥2]
gives the result. If a = 0, pick any q ∈Cm with the property that ∥q∥2 = 1. Set
ˆQ = q,
ˆR = [0]
and observe that the result is still satisﬁed.
(n = k < m) Suppose that the result is true for any A ∈Cm×k, i.e., there is a
matrix ˆQ ∈Cm×k with ˆQH ˆQ = Ik and an upper triangular matrix ˆR ∈Ck×k such
that
A = ˆQˆR.
(n = k + 1 ≤m) Suppose that A ∈Cm×(k+1). Write
A = [Ak a] ,
Ak ∈Cm×k,
a ∈Cm.
There exist ˆQk ∈Cm×k and ˆRk ∈Ck×k, as above, such that Ak = ˆQk ˆRk. Deﬁne
ˆR =
ˆRk
r
0⊺
α

,
ˆQ =
ˆQk
q

,
where r ∈Ck, q ∈Cm, and α ∈R are to be determined. Now we observe that
A = ˆQˆR,
ˆQH ˆQ = Ik+1
(5.10)
has a solution if and only if the following are satisﬁed:
Ak = ˆQk ˆRk,
(5.11)
a = ˆQkr + αq,
(5.12)
ˆQH
k ˆQk = Ik,
(5.13)
qH ˆQk = 0⊺,
(5.14)
qHq = 1.
(5.15)

102
Linear Least Squares Problem
Equations (5.11) and (5.13) are satisﬁed as part of the induction hypothesis.
Next, set
α =
a −ˆQk ˆQH
k a

2 .
If α > 0, (5.11)–(5.15) have a solution, namely,
r = ˆQH
k a ∈Ck,
q = 1
α
 a −ˆQk ˆQH
k a

.
It is easy to see that qHq = 1; also, notice that
qH ˆQk = 1
α
 a −ˆQk ˆQH
k a
H ˆQk = 1
α
 aH ˆQk −aH ˆQk ˆQH
k ˆQk

= 0⊺.
If α = 0, which is possible, the construction fails. In this case, pick any q ∈Cm
⋆
that satisﬁes (5.14) and (5.15). Then set α = 0 and
r = ˆQH
k a.
The proof of the existence of the factorization is completed.
Finally, suppose that rank(A) = n. We want to prove that the diagonal elements
of ˆR must all be positive. To get a contradiction, suppose that ˆR has a zero diagonal
element and is, therefore, singular. In this case, there is a vector x ∈Cn
⋆such that
ˆRx = 0 ∈Cn.
This implies that
Ax = ˆQˆRx = 0 ∈Cm,
which, in turn, implies that A is rank deﬁcient. This is a contradiction.
Deﬁnition 5.28 (reduced QR factorization). Suppose that A ∈Cm×n, m ≥n. A
factorization of the form
A = ˆQˆR,
where ˆR
=
[ˆri,j]
∈
Cn×n is an upper triangular matrix with nonnegative
(real) diagonal elements and ˆQ ∈Cm×n is a matrix satisfying ˆQH ˆQ = In, is
called a reduced QR factorization, or sometimes a reduced nonnegative QR
factorization to emphasize the fact that the diagonal entries are nonnegative.
Theorem 5.29 (uniqueness). Suppose that A ∈Cm×n, m ≥n, and rank(A) = n.
The reduced nonnegative QR factorization is unique.
Proof. See Problem 5.14.
Recall the Gram–Schmidt process that was introduced in Section A.5. We will
now show that this process is related to the reduced QR factorization; in so doing,
we give an alternate proof of the existence of the reduced QR factorization in the
full rank case.

5.4 The QR Factorization and the Gram–Schmidt Algorithm
103
Lemma 5.30 (reduced QR factorization via Gram–Schmidt). Let A ∈Cm×n with
m ≥n = rank(A). Then there exists a unique factorization
A = ˆQˆR,
where ˆQ ∈Cm×n has orthonormal columns and ˆR = [ˆri,j] ∈Cn×n is upper triangular
with positive real diagonal entries.
Proof. Let A = [a1, . . . , an], where ai ∈Cm are the columns of A. We will
inductively construct the columns of ˆQ = [q1, . . . , qn] with qi ∈Cm from the
columns of A as follows.
1. Since rank(A) = n, we know that the columns of A are linearly independent.
This, in particular, implies that a1 ̸= 0. Deﬁne
q1 =
1
∥a1∥2
a1
and
ˆrj,1 =
(
∥a1∥2 > 0,
j = 1,
0,
j = 2, . . . , n.
2. Assume that, for k ≤n −1, we have found q1, . . . , qk that are orthonormal
and, moreover, that
Lk = span{a1, . . . , ak} = span{q1, . . . , qk}.
3. Since A is full rank, we know that ak+1 ̸= 0 and ak+1 ̸∈Lk. Therefore, the
vector
v k+1 = ak+1 −
k
X
j=1
(ak+1, qj)2qj ̸= 0.
We deﬁne
qk+1 =
1
∥v k+1∥2
v k+1
and
ˆrj,k+1 =





(ak+1, qj)2,
j = 1, . . . , k,
∥v k+1∥2 > 0,
j = k + 1,
0,
j = k + 2, . . . , n.
By construction, ˆQˆR = A, as the reader should conﬁrm. Furthermore, the matrix
ˆQ has orthonormal columns and
span ({a1, . . . , ak}) = span ({q1, . . . , qk}) ,
k = 1, . . . , n.
The matrix ˆR is upper triangular with positive diagonal entries. The uniqueness of
this decomposition follows from Theorem 5.29.

104
Linear Least Squares Problem
Remark 5.31 (instability of Gram–Schmidt). As mentioned in Section A.5, the
classical Gram–Schmidt process used in the construction of the reduced QR
factorization in the proof of Lemma 5.30 suﬀers from numerical instabilities. For
this reason, it is never used in practice. There exists a variant that is numerically
stable, which we discuss in Section 5.6.
The existence of the so-called QR factorization, given in Lemma 5.30, allows us
to provide a solution for the normal equations.
Theorem 5.32 (solution of the normal equations). Let A ∈Cm×n be such that
m ≥n = rank(A). Suppose that f ∈Cm is given. The vector ˜x ∈Cn is the unique
least squares solution to Ax = f if and only if
˜x = ˆR−1 ˆQHf ,
where ˆQ and ˆR are the matrices from Lemma 5.30.
Proof. See Problem 5.21.
Oftentimes it is convenient to have what is known as the full QR factorization,
as opposed to the reduced version.
Theorem 5.33 (full QR factorization). Let A ∈Cm×n, m > n. There exists a
unitary matrix Q ∈Cm×m and an upper triangular matrix R = [ri,j] ∈Cm×n (i.e.,
ri,j = 0, if i > j) with nonnegative diagonal entries (ri,i ≥0, for each i = 1, . . . , n)
such that
A = QR.
Such a factorization is known as a full QR factorization.
Proof. By Theorem 5.27, there is an upper triangular matrix ˆR = [ˆri,j] ∈Cn×n with
nonnegative (real) diagonal elements and a matrix ˆQ ∈Cm×n satisfying ˆQH ˆQ = In
such that
A = ˆQˆR.
Suppose that ˆQ = [q1, . . . , qn]. The columns of ˆQ form an orthonormal set. We
create the square matrix
Q =
ˆQ ˜Q

∈Cm×m,
where the columns of ˜Q = [qn+1, . . . , qm] ∈Cm×m−n are orthonormal and chosen,
so that the set {q1, . . . , qm}, i.e., the set of columns of Q, is an orthonormal basis
for Cn. This can always be accomplished by a combination of the basis extension
(Theorem A.22) and the Gram–Schmidt process. Next, we deﬁne
R =
ˆR
O

∈Cm×n,
where O ∈Cm−n×n is a zero matrix used for padding. Then just note that A = QR
and the proof is complete.
The following is a simple consequence of the full QR factorization of a square
matrix.

5.4 The QR Factorization and the Gram–Schmidt Algorithm
105
im(A)
f
Axo
Figure 5.1 Finding the least squares solution to Ax = f amounts to ﬁnding the
orthogonal projection of f ∈Cm onto the subspace im(A).
Theorem 5.34 (Hadamard inequality1). Suppose that A ∈Cn×n. Denote the
columns of A by aj, j = 1, . . . , n. Then
| det(A)| ≤
n
Y
j=1
∥aj∥2 .
Proof. See Problem 5.23.
Remark 5.35 (range of A). Let A ∈Cm×n, m ≥n have full rank, i.e., rank(A) = n.
The orthogonal projection onto the range of A is completely determined by the
reduced QR factorization. Problem 5.22 shows that this projection is precisely
P = ˆQˆQH. If A is rank deﬁcient, the orthogonal projection onto the range of A is
still well deﬁned, but its form is not as simple.
In the next result, we use the orthogonal projection onto the range of A, but we
will not assume that A is of full rank. An illustration of this result is given in Figure
5.1.
Theorem 5.36 (least squares and projection). Let A ∈Cm×n, m ≥n, and f ∈Cm.
The vector xo ∈Cn is a least squares solution to Ax = f if and only if Axo = Pf ,
where P ∈Cm×m is the orthogonal projection onto im(A).
Proof. See Problem 5.24.
1 Named in honor of the French mathematician Jacques Salomon Hadamard (1865–1963).

106
Linear Least Squares Problem
5.5
The Moore–Penrose Pseudo-inverse
If A ∈Cm×n with m ≥n = rank(A), then there is a unique least squares solution
to the system Ax = f , which can be found by solving the normal equations or by
computing the reduced QR factorization of A.
The question we want to address now is: What happens if m ≥n > r = rank(A)?
In this case, we say that A is rank deﬁcient and we know that ker(A) ̸= {0}. We
have shown the existence of a least squares solution, even in this case. However, the
solution will not be unique. Indeed, if x is a least squares solution and ξ ∈ker(A)
is nonzero, then
AHf = AHAx = AHA(x + ξ),
so that x + ξ is also a least squares solution. In other words, for every ξ ∈ker(A)
we have
Φ(x + ξ) = ∥f −A(x + ξ)∥2
2 = ∥f −Ax∥2
2 = Φ(x).
To be able to remove the nonuniqueness, we will require the solution to be, in a
sense, the smallest.
Deﬁnition 5.37 (minimum norm least squares solution). Let A ∈Cm×n with m ≥
n > r = rank(A) and f ∈Cm. Deﬁne
Φ(x) = ∥f −Ax∥2
2.
The minimum norm least squares solution of Ax = f is ˆx ∈Cn that satisﬁes:
1. ˆx is a least squares solution, i.e., Φ(ˆx) ≤Φ(x) for all x ∈Cn.
2. If Φ(ˆx) = Φ(x), then ∥ˆx∥2 ≤∥x∥2.
We ﬁnd this minimum norm solution using the so-called pseudo-inverse of A,
which was introduced in Problem 2.5. We recall here its deﬁnition and basic
properties for convenience.
Deﬁnition 5.38 (Moore–Penrose pseudo-inverse2). Let the matrix A ∈Cm×n with
min(m, n) ≥r = rank(A). Assume that A has the SVD A = UΣVH with
Σ = diag
m×n (σ1, . . . , σr, 0, . . . , 0),
where σ1 ≥· · · ≥σr > 0. Deﬁne
Σ† = diag
n×m
 σ−1
1 , . . . , σ−1
r , 0, . . . , 0

.
Then the matrix
A† = VΣ†UH
is called the Moore–Penrose pseudo-inverse of A.
2 Named in honor of the American mathematician Eliakim Hastings Moore (1862–1932) and
the British mathematician and physicist Sir Roger Penrose (1931–).

5.6 The Modiﬁed Gram–Schmidt Process
107
Theorem 5.39 (properties of A†). In the notation of Deﬁnition 5.38, the Moore–
Penrose pseudo-inverse satisﬁes:
1. If m = n and A−1 exists, then A† = A−1.
2. If m ≥n and A has full rank, then A† = (AHA)−1AH.
3. AA†A = A.
4. A†AA† = A†.
Using the Moore–Penrose pseudo-inverse we can show the existence and
uniqueness of a minimal norm least squares solution.
Theorem 5.40 (minimal norm least squares solution). Let A ∈Cm×n with m ≥
n > r = rank(A). Then there is a unique minimum norm least squares solution ˆx,
which is given by ˆx = A†f .
Proof. Let A = UΣVH. For a ﬁxed x ∈Cn, set w x = VHx. Then
Φ(x) = ∥f −UΣVHx∥2
2 = ∥f −UΣw x∥2
2 = ∥UHf −Σw x∥2
2.
In other words, ﬁnding a minimal norm least squares solution is equivalent to ﬁnding
ˆw ∈Cn such that
∥UHf −Σ ˆw∥2
2 ≤∥UHf −Σw∥2
2,
∀w ∈Cn,
which, since ∥ˆx∥2 = ∥ˆw∥2, also has a minimal norm.
Given that r = rank(A), there are exactly r nonzero diagonal entries in Σ.
Therefore,
∥UHf −Σw∥2
2 =
r
X
i=1
|σiwi −(UHf )i|2 +
m
X
j=r+1
|(UHf )i|2.
The second sum does not depend on w, so to minimize the norm of w = [wi] we
will set wi = 0 for i = r + 1, . . . , n. In addition, since σi > 0 for i ≤r, we can set
wi = 1
σi
(UHf )i,
i = 1, . . . , r
to make the ﬁrst sum vanish. We have thus constructed the vector ˆw = Σ†UHf
that minimizes Φ and has a minimal norm. As a consequence,
ˆx = VΣ†UHf
is the minimal norm least squares solution.
5.6
The Modiﬁed Gram–Schmidt Process
Let us redeﬁne the Gram–Schmidt process using the language of orthogonal
projection matrices. To do so, for k = 1, . . . , n, we deﬁne the matrix Pk ∈Cn×n
by its action on a vector w ∈Cn,
Pkw = w −
k−1
X
j=1
(w, qj)2qj,
(5.16)
where {q1, . . . , qk−1} is an orthonormal set.

108
Linear Least Squares Problem
Deﬁnition 5.41 (modiﬁed Gram–Schmidt). Suppose that S = {a1, . . . , ak} ⊂Cn
⋆
with k ≤n. The modiﬁed Gram–Schmidt process is an algorithm for generating
the set of vectors Q = {q1, . . . , qk} recursively as follows: for m = 1,
q1 =
1
∥a1∥2
a1.
For 2 ≤m ≤k, suppose that {q1, . . . , qm−1} have been computed. Set
v 1
m = am,
v 2
m = Pq⊥
1 v 1
m,
...
v m = v m
m = Pq⊥
m−1v m−1
m
.
If v m = 0, the process terminates. Otherwise, the process continues with
qm =
1
∥v m∥2
v m.
As noted in Remark 5.31 the classical Gram–Schmidt algorithm is unstable. But
it turns out that the modiﬁed Gram–Schmidt process is stable and is the one that
is used in practical computations. Since it results from only cosmetic changes to
the deﬁnition of the original Gram–Schmidt process, we have the following result.
Proposition 5.42 (modiﬁed Gram–Schmidt). Let {ak}n
k=1 ⊂Cm with m ≥n
be linearly independent. The sequence {qk}n
k=1 ⊂Cm obtained by the modiﬁed
Gram–Schmidt process is orthonormal and, for every k ∈{1, . . . , n}, it holds that
span ({a1, . . . , ak}) = span ({q1, . . . , qk}).
Proof. See Problem 5.25.
We conclude the discussion on the modiﬁed Gram–Schmidt process by addressing
its complexity. The algorithm is described in Listing 5.1
When looking at Listing 5.1, we notice that every step in the outermost loop can
be realized as the right multiplication by a square upper triangular matrix. Indeed,
what we are eﬀectively doing is multiplying the ﬁrst column of A = [a1, . . . , an] =
[v 1, . . . , v n] by
1
r1,1 and subtracting this r1, j times from the other columns. This is


|
|
v 1
· · ·
v n
|
|




1
r1,1
−r1,2
r1,1
−r1,3
r1,1
· · ·
−r1,n
r1,1
0
1
0
· · ·
0
0
0
...
...
...
...
...
...
1
0
0
0
· · ·
0
1


=


|
|
|
q1
v 2
2
· · ·
v 2
n
|
|
|

.
This observation lends itself to the following generalization: at step i of the
modiﬁed Gram–Schmidt process we subtract ri, j/ri,i times the column i from the

5.6 The Modiﬁed Gram–Schmidt Process
109
columns j > i, which is then replaced by r −1
i,i times the column itself. This operation
can be encoded in the following upper triangular matrix:
Ri =


1
0
· · ·
0
0
0
0
· · ·
0
0
1
...
...
...
...
...
...
...
...
0
0
0
0
· · ·
0
0
· · ·
0
1
0
0
0
· · ·
0
0
· · ·
0
0
1
ri,i
−ri,i+1
ri,i
−ri,i+2
ri,i
· · ·
−ri,n
ri,i
0
· · ·
0
0
0
1
0
· · ·
0
0
0
0
0
0
...
...
...
...
...
...
...
...
...
1
0
0
· · ·
0
0
0
0
· · ·
0
1


,
where the highlighted row occupies the ith position.
With these matrices, we ﬁnd
AR1R2 · · · Rn = ˆQ ⇐⇒AˆR−1 = ˆQ
(5.17)
with
ˆR−1 = R1R2 · · · Rn,
where each Ri is upper triangular.
This shows that the QR factorization can be understood as a triangular
orthogonalization process.
Theorem 5.43 (complexity of triangular orthogonalization). Let A ∈Cm×n with
m ≥n = rank(A). Then the modiﬁed Gram–Schmidt process requires O(2mn2)
ﬂoating point operations to compute the reduced QR factorization of A.
Proof. We begin by noticing that, in the innermost part of the loop, we must
compute rij = (v j, qi)2 and this requires m multiplications and m −1 additions (as
v j, qi ∈Cm). Once this is done we need to compute v j = v j −rijqj, which needs
m multiplications and m subtractions. In total, this means that we need
m + (m −1) + m + m ≈4m
operations.
This loop is performed for m = i + 1, . . . , n inside a loop of size i = 1, . . . , n, so
the total number of operations must be about
n
X
i=1
n
X
m=i+1
4m ≈4m
n
X
i=1
i ≈2mn2,
as we intended to show.

110
Linear Least Squares Problem
L
w
x
y = Hwx
Figure 5.2 The reﬂection of the point x about the subspace L = span{w}⊥.
5.7
Householder Reﬂectors
Let us develop now a dual idea to triangular orthogonalization, i.e., we will construct
a sequence Q1, . . . , Qn of unitary matrices such that
Qn · · · Q1A = R
(5.18)
with R upper triangular. If we construct this, the QR factorization of the matrix A
is given by QH = Qn · · · Q1 and the R matrix above. This process will be obtained
with the help of the so-called Householder reﬂectors.
Remark 5.44 (full QR). Notice that in (5.18) we obtain a matrix Q that is unitary,
i.e., we are computing a full QR factorization.
Deﬁnition 5.45 (Householder reﬂector3). Suppose that w ∈Cn with ∥w∥2 = 1.
The Householder reﬂector with respect to span{w}⊥is the matrix
Hw = In −2ww H.
As illustrated in Figure 5.2, the action of Hw on a point x ̸∈span{w}⊥is the
mirror reﬂection of x about span{w}⊥.
Proposition 5.46 (properties of Hw). Suppose that w ∈Cn with ∥w∥2 = 1. The
reﬂector Hw satisﬁes the following properties:
3 Named in honor of the American mathematician Alston Scott Householder (1904–1993).
Householder was a former professor at the University of Tennessee and a researcher at Oak
Ridge National Laboratory in Oak Ridge, Tennessee.

5.7 Householder Reﬂectors
111
1. Hw is Hermitian.
2. Hw is unitary.
3. Hw is involutory, i.e., H2
w = In.
4. For any x ∈Cn, ∥Hwx∥2 = ∥x∥2.
Proof. Clearly,
HH
w = IH
n −2
 ww HH = In −2ww H = Hw,
which shows that Hw is Hermitian. To see the second and third properties, observe
that
HH
wHw = H2
w =
 In −2ww H  In −2ww H
= In −4ww H + 4ww Hww H = In.
The fourth property follows from the fact that Hw is unitary:
∥Hwx∥2
2 = xHHH
wHwx = xHx = ∥x∥2
2 .
Let us now see how Householder reﬂectors can be used to obtain the full QR
factorization. The following two results essentially show that, for any vector with a
nonzero jth entry, there is a reﬂection that sends it to (a multiple of) the canonical
vector ej.
Lemma 5.47 (action of a reﬂector). Let x = [xj] ∈Cn
⋆with
xj = |xj|eiαj,
αj ∈[0, 2π).
Suppose that w ∈Cn, ∥w∥2 = 1, has the property that
Hwx = k ∥x∥2 ej,
where k ∈C and ej is the jth canonical basis vector. Then it must be that
k = ±eiαj.
Proof. From the properties of the Householder reﬂector Hw, we have that
∥x∥2 = ∥Hwx∥2 = |k| ∥x∥2 ∥ej∥,
which necessarily implies that |k| = 1. In other words, there is β ∈R such that
k = eiβ.
Next, observe that, since Hw is Hermitian,
xHHwx = xHk ∥x∥2 ej = k ¯xj ∥x∥2 ∈R.
As a consequence, we ﬁnd
k ¯xj ∥x∥2 ∈R ⇐⇒k ¯xj ∈R ⇐⇒eiβ|xj|e−iαj ∈R ⇐⇒ei(αj−β) ∈R,
but this is only possible if there is m ∈Z for which
β = αj + mπ
or, equivalently, k = ±eiαj as we had claimed.

112
Linear Least Squares Problem
Theorem 5.48 (existence of reﬂector). Let x = [xj] ∈Cn
⋆with
xj = |xj|eiαj,
αj ∈[0, 2π).
Then, provided that
x ̸= ±eiαj ∥x∥2 ej,
there is a vector w ∈Cn, ∥w∥2 = 1 such that
Hwx = ±eiαj ∥x∥2 ej.
Proof. Let σ = ±1. Deﬁne
v = x −σeiαj ∥x∥2 ej;
since by assumption v ̸= 0, we can deﬁne
w =
1
∥v∥2
v.
Let us now show that
Hwx = σeiαj ∥x∥2 ej.
A straightforward computation reveals that
v Hx = ∥x∥2
2 −σe−iαj ∥x∥2 xj = ∥x∥2
2 −σ|xj| ∥x∥2 ∈R
and
v Hv = ∥x∥2
2 −2σ|xj| ∥x∥2 + σ2 ∥x∥2
2 = 2

∥x∥2
2 −σ|xj| ∥x∥2

= 2v Hx.
With these computations, it follows that
Hwx = x −2v v Hx
v Hv = x −v = σeiαj ∥x∥2 ej,
as claimed.
Remark 5.49 (real coordinates). Let x = [xj] ∈Cn
⋆with
xj = |xj|eiαj,
∃αj ∈[0, 2π).
We can ﬁnd a vector w ∈Cn, ∥w∥2 = 1 such that
Hwx = ± ∥x∥2 ej ∈Rn
only if xj ∈R, i.e., αj = 0 or π.
Lemma 5.50 (construction of reﬂectors). Assume that m > k. Suppose that
x = [xj] ∈Ck
⋆with
x1 = |x1|eiα1,
∃α1 ∈[0, 2π)
satisﬁes the property that
x ̸= ±eiα1 ∥x∥ℓ2(Ck) be1,

5.7 Householder Reﬂectors
113
where be1 ∈Ck is the ﬁrst canonical basis vector. Let Hw
∈Ck×k be the
Householder reﬂector that satisﬁes
Hwx = σeiα1 ∥x∥ℓ2(Ck) be1,
where σ = ±1. Then the matrix
H =
Im−k
O
O⊺
Hw

∈Cm×m
is a Householder reﬂector in the following sense: there is a vector z ∈Cm with
∥z∥ℓ2(Cm) = 1 such that
H = In −2zzH.
Furthermore, if c ∈Cm−k is any arbitrary vector, then
H
c
x

=
Im−k
O
O⊺
Hw
 c
x

=
 c
rbe1

,
where
r = σeiα1 ∥x∥ℓ2(Ck) .
Proof. See Problem 5.30.
The previous construction suggests, given a matrix A, that a triangular matrix
R may be obtained from it using reﬂections by ﬁrst multiplying by a matrix that
sends the ﬁrst column of A to a multiple of e1 and then proceeding similarly on
the sub-matrix obtained by dropping the ﬁrst row and column of A. This algorithm
is called the Householder triangularization and is described below.
Deﬁnition 5.51 (Householder triangularization). Suppose that
A = A(0) =
h
a(0)
1 , . . . , a(0)
n
i
∈Cm×n
with m ≥n is given. The Householder triangularization process is a recursive
algorithm for converting A into an upper triangular matrix R ∈Cm×n by applying
a sequence of Householder reﬂections; it is deﬁned as follows: for s = 1, suppose
that a(0)
1
∈Cm has as its ﬁrst element
a(0)
1,1 =
a(0)
1,1
 eiα1,
α1 ∈[0, 2π).
If
a(0)
1
̸= ±eiα1
a(0)
1

s2(Cm) be1,
(5.19)
where be1 ∈Cm is the ﬁrst canonical basis vector, there is a vector w 0 ∈Cm with
∥w 0∥ℓ2(Cm) = 1 such that
Hw 0a(0)
1
= r1,1be1,
where |r1,1| =
a(0)
1

ℓ2(Cm). Set H1 = Hw 0. If one or both of the conditions in (5.19)
fails, set H1 = Im. In either case, we have
H1A(0) =
r1,1
f ⊺
0
A(1)

,
where A(1) =
h
a(1)
1 , . . . , a(1)
n−1
i
∈C(m−1)×(n−1).

114
Linear Least Squares Problem
For step s = k + 1 with 1 ≤k ≤n −1, suppose that k Householder reﬂections
have been applied resulting in the decomposition
Hk · · · H1A =
R(k)
B(k)
O(k)
A(k)

,
where R(k) ∈Ck×k is an upper triangular matrix with diagonal elements
r1,1, . . . , rk,k ∈C,
B(k) ∈Ck×(m−k), O(k) ∈C(m−k)×k is a zero matrix, and
A(k) =
h
a(k)
1 , . . . , a(k)
n−k
i
∈C(m−k)×(n−k).
Suppose that a(k)
1
∈Cm−k has as its ﬁrst element
a(k)
1,1 =
a(k)
1,1
 eiα1,
α1 ∈[0, 2π).
If
a(k)
1
̸= ±eiα1
a(k)
1

ℓ2(Cm−k) be1,
(5.20)
where be1 ∈Cm−k is the ﬁrst canonical basis vector, there is a vector w k ∈Cm−k
with ∥w k∥ℓ2(Cm−k) = 1 such that
Hw ka(k)
1
= rk+1,k+1ˆe1,
where |rk+1,k+1| =
a(k)
1

ℓ2(Cm−k). Set
Hk+1 =
 Ik
O
O⊺
Hw k

.
If one or both of the conditions in (5.20) fails, set Hk+1 = Im. In either case,
Hk+1Hk · · · H1A =
R(k+1)
B(k+1)
O(k+1)
A(k+1)

,
where R(k+1) ∈C(k+1)×(k+1) is an upper triangular matrix with diagonal elements
r1,1, . . . , rk,k, rk+1,k+1 ∈C,
B(k+1) ∈C(k+1)×(m−k−1), O(k+1) ∈C(m−k−1)×(k+1) is a zero matrix, and
A(k+1) =
h
a(k+1)
1
, . . . , a(k+1)
n−k−1
i
∈C(m−k−1)×(n−k−1).
The following result follows directly from our last deﬁnition.
Theorem 5.52 (triangularization). Let A ∈Cm×n with m ≥n be given. The
Householder triangulation procedure always proceeds to completion. In other words,
there exists a sequence of Householder reﬂectors H1, . . . , Hn ∈Cm×m such that
Hn · · · H1A = R,

Problems
115
where R ∈Cm×n is upper triangular. Moreover, the product
QH = Hn · · · H1 ∈Cm×m
is a unitary matrix. Consequently, there is a unitary matrix
Q = H1 · · · Hn
and an upper triangular matrix R such that A = QR.
A more practical version of the algorithm is presented in Listing 5.2. Notice that
this procedure does not necessarily compute the matrix Q in the QR factorization.
This is controlled by the parameter computeQ.
Let us now study the complexity of the Householder algorithm without computing
the matrix Q.
Proposition 5.53 (complexity of Householder). The Householder algorithm requires,
approximately, 2mn2 −2
3n3 operations.
Proof. Let us look at the innermost step, i.e.,
A(k : m, k : n) = A(k : m, k : n) −2w kw H
k A(k : m, k : n),
where the vector w k is of length t = m−k+1 and the matrix A(k : m, k : n) ∈Ct×s
with s = n −k + 1. Thus,
1. The product w H
k A(k : m, k : n) requires about 2t · s operations, thus yielding a
row vector of length s.
2. Once this is obtained, the multiplication w k(w H
k A(k : m, k : n)) requires t · s
operations and yields a matrix of size t × s.
3. This is subtracted from A(k : m, k : n) at a cost of t · s operations.
This is done in the innermost part of the loop, which is done n times. Thus,
n
X
k=1
4t · s = 4
n
X
k=1
(m −k + 1)(n −k + 1)
≈4
 n
X
k=1
k2 −(n + m)
n
X
k=1
k + n2m
!
≈4
n3
3 −(n + m)n2
2 + n2m

= 2mn2 −2
3n3,
as claimed.
Problems
5.1
In this problem we will show the existence of a least squares solution using
a method diﬀerent from the one presented in Theorem 5.5. Let A ∈Rm×n with
m ≥n and rank(A) = n and let f ∈Rm. Prove the following:

116
Linear Least Squares Problem
a)
For any λ > 0, the set
Kλ =

x ∈Rn  ∥f −Ax∥ℓ2(Rm) ≤λ
	
is closed and bounded. In other words, the sub-level sets of ∥f −Ax∥ℓ2(Rm) are
compact.
b)
If λ = 2∥f ∥ℓ2(Rm), then
inf

∥f −Ax∥ℓ2(Rm)
 x ∈Rn	
= inf

∥f −Ax∥ℓ2(Rm)
 x ∈Kλ
	
.
c)
The two items above allow us to conclude the result.
5.2
Complete the proof of Theorem 5.10.
5.3
Let m = n = 2 and consider the following rank-one system:
Ax = f ,
A =
0
0
1
0

,
f =
1
1

.
Consider also a perturbed version of this system that has rank-two, where the
perturbation is assumed small ε ≪1:
Aεxε = f ,
Aε =
0
ε
1
0

,
f =
1
1

.
Find the least squares solution to both systems and characterize the sensitivity of
the solution to the perturbation ε.
Hint: Consider
U =
0
1
1
0

,
V = I2
as factors in the SVD of A.
5.4
Give an example of two vectors x, y ∈C2 such that (x, y)2 ̸= 0 and
∥x + y∥2
2 = ∥x∥2
2 + ∥y∥2
2.
What if x, y ∈R2?
5.5
Assume that A ∈Cm×n with m ≥n is of full rank. What is the analogue of
the normal equation that arises from using ΦB( · ), deﬁned in (5.7), where B ∈Cn×n
is HPD?
5.6
Prove Proposition 5.12.
5.7
Prove Proposition 5.17.
5.8
Prove Proposition 5.18.
5.9
Prove Theorem 5.23.
5.10
Show that if P is an orthogonal projector, then I −2P is unitary.
5.11
Determine the eigenvalues, determinant, and singular values of an orthogo-
nal projector. Give algebraic proofs for your conclusions.
5.12
Suppose that q ∈Cn, ∥q∥2 = 1. Set P = I −qqH.
a)
Find im(P).
b)
Find ker(P).
c)
Find the eigenvalues of P.

Problems
117
5.13
Suppose that A ∈Cn×n is invertible. Let A = QR and AHA = UHU be the
QR factorization of A and Cholesky factorization of AHA, respectively, with the
normalizations rj, j, uj, j > 0. Prove that R = U.
5.14
Prove Theorem 5.29.
Hint: Consider the Cholesky factorization of AHA.
5.15
Let A ∈Cm×n (m ≥n). Let A = ˆQˆR be a reduced QR factorization.
a)
Show that A has full rank if and only if all the diagonal entries of ˆR are nonzero.
b)
Suppose that ˆR has k nonzero diagonal entries for some k with 0 ≤k < n.
What does this imply about the rank of A? Is it exactly k? At least k? At most
k?
5.16
Let A ∈Cn×n be a normal matrix with spectrum σ(A) = {λ1, . . . , λn}. Let
A = QR be a QR factorization of A. Prove that
min
1≤j≤n |λj| ≤|ri,i| ≤max
1≤j≤n |λj|
for all i = 1, . . . , n.
5.17
Let W ⊆Cn be a vector subspace. Prove that W has an orthonormal basis.
Hint: Use Gram–Schmidt.
5.18
Suppose that A ∈Cn×n is HPD. Deﬁne the inner product (x, y)A = y HAx
for all x, y ∈Cn. We say that vectors x, y ∈Cn are A-orthogonal if and only if
(x, y)A = 0. Let W ⊆Cn be a subspace. Generalize the Gram–Schmidt process to
show that there is an A-orthonormal basis for W.
5.19
Suppose that A, U ∈Cn×n and U is unitary. Prove that
∥UA∥2 = ∥AU∥2 = ∥A∥2 .
5.20
Let A ∈Rm×m be SPD. Suppose that P ∈Rm×n with m ≥n is full rank.
a)
Show that AC = P⊺AP is invertible.
b)
Deﬁne QA = PA−1
C P⊺A. Show that QAu ∈im(P) is the best approximation of
u ∈Rm with respect to the A-norm, ∥· ∥A, which is deﬁned as follows:
(v, w)A = v ⊺Aw, ∀v, w ∈Rm,
∥w∥A =
p
(w, w)A, ∀w ∈Rm.
In other words, prove that
QAu = argmin
n
∥z −u∥2
A
 z ∈im(P)
o
.
5.21
Prove Theorem 5.32.
5.22
Let A ∈Cm×n with m ≥n. Suppose that A has full rank and A = ˆQˆR is a
reduced QR factorization of A. Prove that P = ˆQˆQH is an orthogonal projection
onto im(A).
5.23
Prove Theorem 5.34.
5.24
Prove Theorem 5.36.
5.25
Prove Proposition 5.42.
5.26
Determine the eigenvalues, determinant, and singular values of a House-
holder reﬂector. Give algebraic proofs for your conclusions.

118
Linear Least Squares Problem
5.27
Suppose that {q1, . . . , qk−1} is an orthonormal set. Let Pk be deﬁned as in
(5.16). Show that this matrix is an orthogonal projection. Verify, in addition, that
if we deﬁne ˆQk = [q1, . . . , qk−1], then
Pk = I −ˆQk ˆQH
k .
Finally, show that, for k > 1,
Pk = Pq⊥
k−1 · · · Pq⊥
1 ,
where Pa⊥denotes the orthogonal projection onto span{a}⊥.
5.28
Given x, y ∈Cn
⋆= Cn\{0}, determine a vector v ∈Cn
⋆such that the
associated Householder reﬂector H satisﬁes Hx = γy for some γ.
5.29
Let u, v ∈Rm and σ ∈R. Deﬁne H(u, v, σ) := Im −σuv ⊺, where Im is the
m × m identity matrix.
a)
Find all nonzero values of σ for which H(u, u, σ) is orthogonal. For such σ,
determine all the eigenvalues and the corresponding eigenvectors of H(u, u, σ).
b)
Let x ∈Rm and x ̸= 0. Describe how to choose a vector u ∈Rm such that
H = H(u, u, σ) has the property that Hx is a multiple of ˆe1 = (1, 0, 0, . . . , 0)⊺,
where σ is as deﬁned in the previous item.
5.30
Prove Lemma 5.50.
5.31
Let Q ∈Rn×n be orthogonal, i.e., Q⊺= Q−1. Recall that a Householder
reﬂector is a matrix of the form
H = In −2vv H
v Hv ,
where v ∈Rn
⋆. Prove that Q can be written as the product of n standard
Householder reﬂectors or less.
5.32
The conclusion of the previous Problem does not necessarily hold in the
complex case. In other words, if U ∈Un×n is unitary, i.e., UH = U−1, then it may
not be possible to represent it as the product of Householder reﬂectors. Instead,
show that
U = H1 · · · HkD
for some k ∈{1, . . . , n −1}, where Hi is a Householder matrix for each i ∈
{1, . . . , k} and D is a diagonal matrix whose diagonal entries each have modulus
one.
5.33
Let v ∈Cm be a unit vector. Deﬁne Aα = I + αvv H for α ∈C⋆.
a)
Prove that the Householder matrix H = A−2 is the unique unitary matrix in
the set {Aα | α ∈R⋆}.
b)
Is the statement still true for the set {Aα | α ∈C⋆}? If so, prove it. If not,
give a counterexample.
5.34
Prove Theorem 5.52.
5.35
Consider the 2 × 2 orthogonal matrices
F =
−c
s
s
c

,
J =
 c
s
−s
c

,

Listings
119
where s = sin θ and c = cos θ for some θ. The ﬁrst matrix has det(F) = −1 and is a
reﬂector. The second has det(J) = 1 and eﬀects a rotation instead of a reﬂection.
Such a matrix is called a Givens rotation.4
a)
Describe exactly what geometric eﬀect left-multiplications by F and J have on
points in the plane.
b)
Describe an algorithm for QR factorization based on Givens rotations.
Listings
1
function [Q, R, err] = ModifiedGramSchmidt( A )
2
% The modified Gram-Schmidt orthogonalization process.
3
%
4
% Input
5
%
A(1:m,1:n) : a matrix representing a collection of n column
6
%
vectors of dimension m
7
%
8
% Output
9
%
Q(1:m,1:n) : a collection of k orthonormal vectors of
10
%
dimension n
11
%
R(1:n,1:n) : an upper triangular square matrix such that
12
%
A = QR
13
%
err : = 0, if the columns of A are linearly independent
14
%
= 1, if an error has occurred
15
m = size(A)(1);
16
n = size(A)(2);
17
Q = zeros(m,n);
18
R = zeros(n,n);
19
err = 0;
20
if n>m
21
err = 1;
22
return;
23
end
24
V = A;
25
for i=1:n
26
R(i,i) = norm( V(:,i) );
27
if R(i,i) > eps( R(i,i) );
28
Q(:,i) = (1./R(i,i)) * V(:,i);
29
else
30
err = 1;
31
return;
32
end
33
for j = i+1:n
34
R(i,j) = V(:,j)'*Q(:,i);
35
V(:,j) = V(:,j) - R(i,j)*Q(:,i);
36
end
37
end
38
end
Listing 5.1 The modiﬁed Gram–Schmidt process.
4 Named in honor of the American mathematician and computer scientist James Wallace
Givens (1910–1993). Givens was, for a time, a professor at the University of Tennessee and
he worked at Oak Ridge National Laboratory in Oak Ridge, Tennessee.

120
Linear Least Squares Problem
1
function [Q, R, err] = QRFact( A, computeQ )
2
% The QR factorization using Householder reflectors.
3
%
4
% Input
5
%
A(1:m,1:n) : a rectangular matrix representing a collection
6
%
of n column vectors of dimension m
7
%
computeQ : = 1, the matrix Q is computed
8
%
= 0, the matrix Q is not computed. The algorithm
9
%
returns the identity
10
%
11
% Output
12
%
Q(1:m,1:m) : a unitary matrix of dimension n
13
%
R(1:m,1:n) : an rectangular upper triangular square matrix
14
%
such that A = QR
15
%
err : = 0, if the columns of A are linearly independent
16
%
= 1, if an error has occurred
17
m = size(A)(1);
18
n = size(A)(2);
19
Q = eye(m,m);
20
R = A;
21
err = 0;
22
if n>m
23
err = 1;
24
return;
25
end
26
for k=1:n
27
nn = norm( R(k:m,k) );
28
e = zeros(m-k+1,1);
29
e(1) = 1;
30
v = nn*e + R(k:m,k);
31
norm v = norm( v );
32
if norm v > eps( norm v )
33
w = (1.0/norm v)*v;
34
else
35
err = 1;
36
return;
37
end
38
R(k:m,k:n) = R(k:m,k:n) - 2.0*(w*w')*R(k:m,k:n);
39
if computeQ > 0
40
for t=1:m
41
Q(k:m,1:m) = Q(k:m,1:m) - 2.0*(w*w')*Q(k:m,1:m);
42
end
43
end
44
end
45
if computeQ > 0
46
Q = Q';
47
end
48
end
Listing 5.2 QR factorization using reﬂectors.

6
Linear Iterative Methods
In Chapter 3 we learned that, using direct methods such as Gaussian elimination,
one could obtain an exact solution to the linear system of equations Ax = f with
A ∈Cn×n and f ∈Cn. (Of course, we are ignoring the eﬀects of roundoﬀ.)
Unfortunately, these algorithms require O(n3) operations, which are frequently
too expensive in practice. The high cost begs the following questions: Are there
lower cost options? Is an approximation of x good enough? How would such an
approximation be generated? As we will see, oftentimes we can ﬁnd methods that
have a much lower cost of computing a good approximate solution to x.
As an alternate to the direct methods that we studied in the previous chapters,
in the present chapter we will describe the so-called linear iteration methods for
constructing sequences, {xk}∞
k=1 ⊂Cn, with the desire that xk →x = A−1f , as
k →∞. The idea is that, given some ε > 0, we look for a k ∈N such that
∥x −xk∥≤ε
with respect to some norm. In this context, ε is called the stopping tolerance.
In other words, we want to make certain the error is small in norm. But a word
of caution. Usually, we do not have a direct way of approximating the error. The
residual is more readily available. Suppose that xk is an approximation of x = A−1f .
The error is ek = x −xk and the residual is r k = f −Axk = Aek. Recall that
∥ek∥
∥x∥≤κ(A)∥r k∥
∥f ∥.
Thus, when κ(A) is large, ∥r k∥
∥f ∥, which is easily computable, may not be a good
indicator of the size of the relative error ∥ek∥
∥x∥, which is not directly computable.
One must be careful when measuring the error.
The material of this section — containing topics such as the Gauss–Seidel
method and the (successive) over-relaxation (SOR) method — does not, for the
most part, represent the leading edge of research in iterative solvers. We call such
methods classical, though not in the sense of a pejorative. Indeed, while workers are
not typically applying Gauss–Seidel methods to solve industrial strength problems,
understanding such methods is vital to our investigation of more modern methods,
like multigrid and conjugate gradient methods and also eﬀective preconditioning
strategies. Excellent references for the classical material of this section may be
found in the books by Hageman and Young [36] and Young [103]. Another good
reference is [81].

122
Linear Iterative Methods
6.1
Linear Iterative Methods
Deﬁnition 6.1 (iterative method). Let A ∈Cn×n with det(A) ̸= 0 and f ∈Cn.
An iterative method to ﬁnd an approximate solution to Ax = f is a process to
generate a sequence of approximations {xk}∞
k=1 via an iteration of the form
xk = ϕ(A, f , xk−1, . . . , xk−r),
given the starting values x0, . . . , xr−1 ∈Cn. Here,
ϕ(·, ·, . . . , ·): Cn×n × Cn × · · · × Cn →Cn
is called the iteration function. If r = 1, we say that the process is a two-layer
method; otherwise, we say it is a multilayer method.
Deﬁnition 6.2 (linear iterative method). Let A ∈Cn×n with det(A) ̸= 0 and
f ∈Cn. Set x = A−1f . The two-layer iterative method
xk = ϕ(A, f , xk−1)
is said to be consistent if and only if x = ϕ(A, f , x), i.e., x = A−1f is a ﬁxed point
of ϕ(A, f , · ). The method is linear if and only if
ϕ(A, αf 1 + βf 2, αx1 + βx2) = αϕ(A, f 1, x1) + βϕ(A, f 2, x2)
for all α, β ∈C and f 1, f 2, x1, x2 ∈Cn.
Proposition 6.3 (general form). Let A ∈Cn×n with det(A) ̸= 0 and f ∈Cn. Any
two-layer, linear, and consistent method can be written in the form
xk+1 = xk + Cr(xk) = xk + C(f −Axk)
(6.1)
for some matrix C ∈Cn×n, where r(z) = f −Az is the residual vector.
Proof. A two-layer method is deﬁned by an iteration function
ϕ( · , · , · ): Cn×n × Cn × Cn →Cn.
Given ϕ, deﬁne the operator
Cz = ϕ(A, z, 0).
This is a linear operator owing to the assumed linearity of the iteration function.
Consequently, C can be identiﬁed as a square matrix. It follows from this deﬁnition,
using the consistency and linearity of ϕ, that
(In −CA)w = w −ϕ(A, Aw, 0) = ϕ(A, Aw, w) −ϕ(A, Aw, 0) = ϕ(A, 0, w).
Furthermore, by linearity, we can write
xk+1 = ϕ(A, f + 0, 0 + xk)
= ϕ(A, f , 0) + ϕ(A, 0, xk)
= Cf + (In −CA)xk
= xk + C(f −Axk),
as we intended to show.

6.1 Linear Iterative Methods
123
If C is invertible, we can, if we like, write
C−1 (xk+1 −xk) + Axk = f .
Motivated by this form, we have the following generalization.
Deﬁnition 6.4 (two-layer methods). Let A ∈Cn×n with det(A) ̸= 0 and f ∈Cn.
A method of the form
Bk+1 (xk+1 −xk) + Axk = f ,
where Bk+1 ∈Cn×n is invertible, is called an adaptive two-layer method. If Bk+1 =
B, where B is invertible and independent of k, then the method is called a stationary
two-layer method. If Bk+1 =
1
αk+1 In, where αk+1 ∈C⋆, then we say that the
adaptive two-layer method is explicit.
Remark 6.5 (iterator). Consider a stationary two-layer method and assume that
B is invertible, then
xk+1 = xk + B−1(f −Axk);
(6.2)
from this it follows that if {xk}k≥0 converges, then it must converge to x = A−1f .
Of course, this form is equivalent to (6.1) with C = B−1. The matrix B in the
stationary two-layer method is called the iterator.
Remark 6.6 (choice of B). Let us now consider an extreme case, namely B = A.
In this case, we obtain
xk+1 = xk + B−1(f −Axk) = xk + A−1(f −Axk) = A−1f ,
i.e., we get the exact solution after one step.
The previous observation shows that the choice of an iterator comes with two
conﬂicting requirements:
1. The iterator B should be easy/cheap to invert.
2. The iterator B should “approximate” the matrix A well.
In essence, the art of iterative methods is concerned with ﬁnding good iterators.
Deﬁnition 6.7 (error transfer). Let A ∈Cn×n be invertible and f ∈Cn. Suppose
that x = A−1f and consider the stationary two-layer method (6.2) deﬁned by the
invertible matrix B ∈Cn×n. The matrix T = In −B−1A is called the error transfer
matrix and satisﬁes
ek+1 = Tek,
where ek = x −xk is the error at step k.
We need to ﬁnd conditions on T to guarantee that {ek}∞
k=0 converges to zero.

124
Linear Iterative Methods
6.2
Spectral Convergence Theory
The theory in this section is based on properties of the spectrum σ(T) of the
error transfer matrix. In particular, with the tools developed in Section 4.1, we can
provide conditions for a linear method to be convergent.
Theorem 6.8 (convergence of linear methods). Suppose that A, B ∈Cn×n are
invertible, f , x0 ∈Cn are given, and x = A−1f .
1. The sequence {xk}∞
k=1 deﬁned by the linear two-layer stationary iterative method
(6.2) converges to x for any starting point x0 if and only if ρ(T) < 1, where T
is the error transfer matrix T = In −B−1A.
2. A suﬃcient condition for the convergence of {xk}∞
k=1 for any starting point x0
is the condition that ∥T∥< 1 for some induced matrix norm.
Proof. Before we begin the proof, observe that
ek = Tek−1 = T2ek−2 = · · · = Tke0.
Also, observe that xk →x = A−1f , as k →∞, if and only if ek →0, as k →∞.
Suppose that xk →x = A−1f , as k →∞, for any x0. Then ek →0, as k →∞,
for any e0. Set e0 = w, where (λ, w) is any eigenpair of T, with ∥w∥∞= 1. Then
ek = λke0
and
|λ|k = |λ|k∥w∥∞= ∥ek∥∞→0.
It follows that |λ| < 1. Since λ was arbitrary, ρ(T) < 1.
If ρ(T) < 1, appealing to Theorem 4.8,
lim
k→∞ek = lim
k→∞Tke0 = 0
for any e0. Hence, xk →x = A−1f , as k →∞, for any x0.
Suppose now that ∥T∥< 1 for some induced matrix norm. Since, for any induced
matrix norm,
ρ(T) ≤∥T∥,
it follows that ρ(T) < 1. Again, by Theorem 4.8,
lim
k→∞ek = lim
k→∞Tke0 = 0
for any e0.
We can also provide an error estimate.
Theorem 6.9 (error estimate). Let A, B ∈Cn×n be invertible, x0, f ∈Cn are
given, and x = A−1f . Let {xk}∞
k=1 be the sequence generated by the linear two-

6.3 Matrix Splitting Methods
125
layer stationary method (6.2). Assume that, for some induced norm, ∥T∥< 1.
Then the following estimates hold:
∥x −xk∥≤∥T∥k ∥x −x0∥,
∥x −xk∥≤
∥T∥k
1 −∥T∥∥x1 −x0∥.
Proof. It follows that ek = Tke0. By using the consistency and sub-multiplicativity
of the induced matrix norm, we ﬁnd
∥ek∥≤
Tk ∥e0∥≤∥T∥k ∥e0∥,
which proves the ﬁrst estimate. To see the second, observe that ek = Tk−1e1,
and thus Tek = Tke1. Subtracting the last expression from ek = Tke0, we ﬁnd
(In −T) ek = Tk (x1 −x0). Since ∥T∥< 1, Theorem 4.19 guarantees that In −T
is invertible, and
(In −T)−1 ≤
1
1 −∥T∥.
Hence,
ek = (In −T)−1 Tk (x1 −x0)
and, using the consistency and sub-multiplicativity of the norm, we get
∥ek∥≤
(In −T)−1 ∥T∥k ∥x1 −x0∥≤
1
1 −∥T∥∥T∥k ∥x1 −x0∥.
The result is proven.
6.3
Matrix Splitting Methods
Here, we present some methods that are based on the idea of matrix splitting.
Namely, we assume that we can split the coeﬃcient matrix A as
A = M + N,
where M is invertible and, hopefully, easy to invert. Since Ax = f if and only if
Mx + Nx = f , the strategy that we follow is then to construct a method of the
form
Mxk+1 + Nxk = f .
6.3.1
Jacobi Method
Let A = [ai,j] ∈Cn×n have nonzero diagonal elements and consider the following
splitting of A:
A = L + D + U,

126
Linear Iterative Methods
where D = diag (a1,1, . . . , an,n) is the diagonal part of A, L is the strictly lower
triangular part of A, and U is its strictly upper triangular part. Choosing the iterator
to be
B = BJ = D
yields the so-called Jacobi method.1 In this case, the error transfer matrix is
T = TJ = In −D−1A,
(6.3)
so that
TJ = −


0
a1,2
a1,1
· · ·
a1,n
a1,1
a2,1
a2,2
0
...
...
...
...
...
an−1,n
an−1,n−1
an,1
an,n
· · ·
an,n−1
an,n
0


.
Alternately, we may write the Jacobi method in component form via
xi,k+1 = [xk+1]i = [TJxk]i +

D−1f

i = −1
ai,i
n
X
j=1
j̸=i
ai,jxj,k + 1
ai,i
fi,
where xi,k = [xk]i. Equivalently,
ai,ixi,k+1 = −
n
X
j=1
j̸=i
ai,jxj,k + fi.
Theorem 6.10 (convergence). Let A = [ai,j] ∈Cn×n be strictly diagonally
dominant (SDD) of magnitude δ > 0, and f ∈Cn. Then the Jacobi iteration
method for approximating the solution to Ax = f is convergent.
Proof. Since A is SDD of magnitude δ > 0, it follows that D is invertible, and TJ,
given in (6.3), is well deﬁned. Then
1 Named in honor of the German mathematician Carl Gustav Jacob Jacobi (1804–1851).

6.3 Matrix Splitting Methods
127
∥TJ∥∞= max
1≤i≤n
n
X
j=1
[TJ]i,j

= max
1≤i≤n
n
X
j=1
δi,j −1
ai,i
ai,j

= max
1≤i≤n
n
X
j=1
j̸=i

ai,j
ai,i

= max
1≤i≤n
1
|ai,i|
n
X
j=1
j̸=i
|ai,j|
≤max
1≤i≤n
1
|ai,i| (|ai,i| −δ)
= 1 −δ min
1≤i≤n
1
|ai,i|.
Hence, ∥TJ∥∞< 1. By Theorem 6.8 the method converges.
Theorem 6.11 (convergence). Suppose that A ∈Cn×n is column-wise SDD of
magnitude δ > 0, i.e.,
|aj,j| −δ ≥
n
X
i=1
i̸=j
|ai,j|,
j = 1, . . . , n,
and f ∈Cn is given. Then the sequence {xk}∞
k=1 generated by the Jacobi iteration
method converges, for any starting value x0, to the vector x = A−1f .
Proof. It will suﬃce to prove that
ρ(TJ) < 1,
where TJ is given by (6.3). Since A ∈Cn×n is column-wise SDD of magnitude
δ > 0, AH is row-wise SDD of magnitude δ > 0. Therefore, by Theorem 6.10,
ρ
 In −D−HAH
≤
In −D−HAH
∞< 1.
Deﬁne ˜T = In −D−HAH. Then
˜TH = In −AD−1
and
D−1˜THD = In −D−1A = TJ.
Therefore,
σ(TJ) = σ(˜TH) = σ(˜T)

128
Linear Iterative Methods
and
ρ(TJ) = ρ(˜T) < 1.
6.3.2
Gauss–Seidel Method
Recall that the Jacobi method can be written in the form
i−1
X
j=1
ai,jxj,k + ai,ixi,k+1 +
n
X
j=i+1
ai,jxj,k = fi.
However, at this stage, we have already computed new approximations for
components xm, m = 1, . . . , i −1, which we are not using. The Gauss–Seidel
method uses these newly computed approximations to obtain the method
iX
j=1
ai,jxj,k+1 +
n
X
j=i+1
ai,jxj,k = fi.
As before, we require ai,i ̸= 0 for all i = 1, . . . , n, so that the method is well deﬁned.
Recall the splitting A = L + D + U. Choosing the iterator matrix as
B = BGS = L + D
results in the so-called Gauss–Seidel method.2 The linear iteration process for the
Gauss–Seidel method may be expressed as
(L + D)xk+1 + Uxk = f .
Therefore, assuming that D is invertible,
xk+1 = −(L + D)−1Uxk + (L + D)−1f .
The error transfer matrix may be expressed as
TGS = In −B−1
GSA = −(L + D)−1U = −(A −U)−1U.
(6.4)
We have the following convergence criterion.
Theorem 6.12 (convergence). Suppose that A = [ai,j] ∈Cn×n is SDD and f ∈
Cn. Then, for any starting value x0, the sequence generated by the Gauss–Seidel
method converges to x = A−1f .
Proof. Let us deﬁne
γ =
n
max
i=1
(
Pn
j=i+1 |ai,j|
|ai,i| −Pi−1
j=1 |ai,j|
)
.
2 Named in honor of the German mathematician and physicist Johann Carl Friedrich Gauss
(1777–1855) and the German mathematician Philipp Ludwig von Seidel (1821–1896).

6.3 Matrix Splitting Methods
129
Owing to the fact that A is SDD,
|ai,i| >
n
X
j=1
j̸=i
|ai,j| =
i−1
X
j=1
|ai,j| +
n
X
j=i+1
|ai,j|,
which implies that
|ai,i| −
i−1
X
j=1
|ai,j| >
n
X
j=i+1
|ai,j|,
and thus γ ∈[0, 1). We will show convergence of the Gauss–Seidel method by
proving that ∥TGS∥∞≤γ.
Let A = L + D + U be the usual decomposition into lower triangular, diagonal,
and upper triangular parts, respectively. Set y = TGSx, i.e., (L + D)y = −Ux. We
have
i−1
X
j=1
ai,jyj + ai,iyi = −
n
X
j=i+1
ai,jxj,
1 ≤i ≤n.
Let i be such that |yi| = ∥y∥∞. By the triangle inequality,

i−1
X
j=1
ai,jyj + ai,iyi

≥|ai,i||yi| −
i−1
X
j=1
|ai,j||yj| ≥

|ai,i| −
i−1
X
j=1
|ai,j|

∥y∥∞.
Also, we have

n
X
j+1
ai,jxj

≤
n
X
j+1
|ai,j|∥x∥∞.
Consequently,
∥TGSx∥∞= ∥y∥∞≤
Pn
j=i+1 |ai,j|
|ai,i| −Pi−1
j=1 |ai,j|
∥x∥∞≤γ∥x∥∞.
This implies that, for all x ∈Cn
⋆,
∥TGSx∥∞
∥x∥∞
≤γ,
which shows that
∥TGS∥∞≤γ < 1.
In fact, we can prove more than what the last result suggests.
Theorem 6.13 (convergence). Suppose that A = [ai,j] ∈Cn×n is SDD of
magnitude δ > 0 and f ∈Cn. Then
∥TGS∥∞≤∥TJ∥∞< 1,
where TGS and TJ denote, respectively, the error transfer matrices of the Gauss–
Seidel and Jacobi methods. In particular, for any starting value x0, the sequences
generated by the Jacobi and the Gauss–Seidel methods both converge to x = A−1f .

130
Linear Iterative Methods
Proof. Set
ηGS = ∥TGS∥∞,
ηJ = ∥TJ∥∞,
and 1 = [1, . . . , 1]⊺.
Our argument will simplify signiﬁcantly with some new notation, which may not
be widely used outside of this proof. Deﬁne, for any matrix C = [ci,j] ∈Cn×n, its
absolute value
|C| = [|ci,j|]n
i,j=1 ∈Cn×n.
It follows that
|C| 1 =


Pn
j=1 |c1,j|
...
Pn
j=1 |cn,j|


∥|C|1 ∥∞= ∥|C| ∥∞.
Similarly, for any vector c = [ci] ∈Cn, |c| = [|ci|]n
i=1 ∈Cn. For a = [ai], b =
[bi] ∈Cn, we write a ⪯b if and only if ai ≤bi for all i = 1, . . . , n. Similarly, for
matrices, A = [ai,j], B = [bi,j] ∈Cn×n, we write A ⪯B if and only if ai,j ≤bi,j for
all i, j = 1, . . . , n. As usual, a ≺b means a ⪯b and a ̸= b.
Under the assumption that ηJ = ∥TJ∥∞< 1, it follows that
|TJ| 1 ⪯ηJ1 ≺1.
Let A = L + D + U be the usual decomposition into lower triangular, diagonal, and
upper triangular parts, respectively. Then
TJ = In −D−1A = −D−1 (L + U) = −˜L −˜U,
where ˜L = D−1L is strictly lower triangular and ˜U = D−1U is strictly upper
triangular. Hence,
|TJ| =
˜L
 +
˜U
 .
From this it follows that
˜U
 1 = |TJ|1 −
˜L
 1 ⪯ηJ1 −
˜L
 1 =
 ηJIn −
˜L

1.
Since
˜L
 and ˜L are strictly lower triangular, it follows that
˜Ln =
˜L
n = O ∈Cn×n.
Furthermore, In + ˜L is invertible and
 In + ˜L
−1 = In −˜L + ˜L2 −˜L3 + · · · +
 −˜L
n−1 .
Then it is left to the reader as an exercise (Problem 6.1) to show that
O ⪯
(In + ˜L)−1
=
In −˜L + ˜L2 −˜L3 + · · · +
 −˜L
n−1
⪯In +
˜L
 +
˜L
2 + · · · +
˜L
n−1
=
 In −
˜L
−1 .

6.3 Matrix Splitting Methods
131
Now recall and observe that
TGS = In −(L + D)−1 A
= −(L + D)−1 U
= −(L + D)−1 DD−1U
= −
 In + ˜L
−1 ˜U.
Thus,
˜U
 1 ⪯
 ηJIn −
˜L

1
and
 In −
˜L
−1 ˜U
 1 ⪯
 In −
˜L
−1  ηJIn −
˜L

1,
since
O ⪯
 In −
˜L
−1 .
Finally,
|TGS| 1 ⪯

 In + ˜L
−1
˜U
 1
⪯
 In −
˜L
−1 ˜U
 1
⪯
 In −
˜L
−1  ηJIn −
˜L

1
=
 In −
˜L
−1  In −
˜L
 + (ηJ −1)In

1
=
n
In + (ηJ −1)
 In −
˜L
−1o
1.
Now
In ⪯
 In −
˜L
−1 = In +
˜L
 +
˜L
2 + · · · +
˜L
n−1
and ηJ < 1 from Theorem 6.10, so that
|TGS| 1 ⪯ηJ1,
which implies that
ηGS = ∥TGS∥∞≤ηJ.
Let us conclude this discussion by, for a special class of matrices, linking the
convergence of the Jacobi method to that of the Gauss–Seidel method.
Theorem 6.14 (tridiagonal matrices). Let A ∈Cn×n be tridiagonal with nonzero
diagonal elements. Denote by TJ and TGS the error transfer matrices of the Jacobi
and Gauss–Seidel methods, respectively. Then we have
ρ(TGS) = ρ(TJ)2.
In particular, one method converges if and only if the other method converges.

132
Linear Iterative Methods
Proof. We begin with a preliminary statement about tridiagonal matrices. Suppose
that
A =


b1
c2
0
· · ·
0
a2
b2
...
...
...
0
...
...
cn−1
0
...
...
an−1
bn−1
cn
0
· · ·
0
an
bn


,
where bi ̸= 0 for all i = 1, . . . , n. Let 0 ̸= µ ∈C. Deﬁne
M(µ) = SAS−1,
where S = diag
 µ, µ2, . . . , µn
. Then
M(µ) =


b1
µ−1c2
0
· · ·
0
µa2
b2
...
...
...
0
...
...
µ−1cn−1
0
...
...
µan−1
bn−1
µ−1cn
0
· · ·
0
µan
bn


.
It is left to the reader as an exercise to show that
det(M(µ)) = det(M(1)) = det(A).
Now let A = L + D + U, where, as usual, D is diagonal and L and U are,
respectively, strictly lower and strictly upper triangular. From (6.3) we have that
TJ = In −D−1A; therefore, the eigenvalues of TJ are the zeros of the characteristic
polynomial
χJ(λ) = det(TJ −λIn)
= det
 −D−1(L + U) −λIn

= det

−D−1(L + U + λD)

= det(−D−1)qJ(λ),
where we deﬁned the polynomial
qJ(λ) = det(L + λD + U).
On the other hand, from (6.4) we have that TGS = −(L + D)−1U, so its
eigenvalues are the zeros of
χGS(λ) = det(TGS −λIn) = det
 −(L + D)−1
qGS(λ),
where
qGS(λ) = det(λL + λD + U).

6.4 Richardson’s Method
133
Notice that both matrices involved in the deﬁnitions of qJ and qGS, respectively, are
tridiagonal. By the previous statement about determinants of tridiagonal matrices
we have that, if λ ̸= 0,
qGS(λ2) = det(λ2L + λ2D + U)
= λn det(λL + λD + λ−1U)
= λn det(L + λD + U)
= λnqJ(λ),
and so, by continuity, this holds for all λ ∈C.
The previous relation shows that
λ ∈σ(TGS)
=⇒
λ1/2, −λ1/2 ∈σ(TJ)
and
(λ ∈σ(TJ) ⇐⇒−λ ∈σ(TJ))
=⇒λ2 ∈σ(TGS),
as we needed to show.
6.4
Richardson’s Method
Let A ∈Cn×n be invertible and f ∈Cn be given. Let us consider now what is known
as Richardson’s method:3
xk+1 = xk + α (f −Axk).
Clearly, this is a stationary two-layer method that results from choosing
B = BR = 1
αIn,
where α ∈C⋆. In this case, TR = In −αA.
Theorem 6.15 (convergence). Let A ∈Cn×n be Hermitian positive deﬁnite (HPD),
σ(A) = {λ1, . . . , λn} with 0 < λ1 ≤λ2 ≤· · · ≤λn. Assume that α ∈R⋆. Then
Richardson’s method converges if and only if α ∈(0, 2/λn). In this case, we have
the estimate
∥ek∥2 ≤ρk∥e0∥2,
ρ = ρ(α) = max{|1 −αλn|, |1 −αλ1|}.
From this, it follows that, by setting
α = αopt =
2
λ1 + λn
,
one obtains the smallest possible value of ρ and
ρopt = ρ(αopt) = λn −λ1
λn + λ1
= κ2(A) −1
κ2(A) + 1.
3 Named in honor of the British mathematician, physicist, and meteorologist Lewis Fry
Richardson (1881–1953).

134
Linear Iterative Methods
Proof. Since A is HPD, we know that the eigenvalues of A are positive real numbers
and
λn = ∥A∥2.
Notice also that TR = In −αA = TH
R, which implies that the eigenvalues of TR are
real. Observe that (λi, w i) is an eigenpair of A if and only if (νi = 1 −αλi, w i) is
an eigenpair of TR. Assume that 0 < α < 2/λn. Then
0 < λiα < 2 λi
λn
,
i = 1, . . . , n,
which implies that
1 > 1 −λiα > 1 −2 λi
λn
≥−1,
i = 1, . . . , n.
It follows that
1 > ν1 ≥· · · ≥νn > −1,
νi = 1 −αλi.
This guarantees that ∥TR∥2 = ρ(TR) < 1, which implies convergence. Conversely,
if α ̸∈(0, 2/λn), then ρ(TR) ≥1 and the method does not converge.
By consistency,
∥ek+1∥2 = ∥Tk
Re0∥2 ≤ρk∥e0∥2.
Of course, it is easy to see that
ρ = ρ(TR) = max{|ν1|, |νn|} = max{|1 −αλn|, |1 −αλ1|}.
Finally, showing optimality amounts to minimizing ρ; see Figure 6.1. From this
we see that the minimum of ρ is attained when
|1 −αλ1| = |1 −αλn|
or
1 −αλn = αλ1 −1,
which implies that
αopt =
2
λ1 + λn
.
Therefore,
ρopt = 1 −αoptλ1 = λ1 + λn −2λ1
λ1 + λn
= λn −λ1
λ1 + λn
= κ2(A) −1
κ2(A) + 1.
Remark 6.16 (optimal α). Notice that the rate of convergence for the optimal
choice αopt degrades as κ2(A) gets larger, since ρopt →1. We will see that this is
a major problem for classical iterative methods.

6.5 Relaxation Methods
135
α
ρ
αopt
1
1
λ1
|1 −αλ1|
1
λi
|1 −αλi|
1
λn
|1 −αλn|
Figure 6.1 The curve ρ(TR) (solid line) as a function of α.
6.5
Relaxation Methods
Let us consider one last classical method, namely the relaxation method. Recall
that Gauss–Seidel reads
Lxk+1 + Dxk+1 + Uxk = f ,
where A = L + D + U with the usual assumptions and notation. We will weight the
contribution of the diagonal D by introducing the parameter ω > 0:
Lxk+1 + ω−1Dxk+1 + (1 −ω−1)Dxk + Uxk = f .
In doing that, we obtain the method
(L + ω−1D)xk+1 =
 (ω−1 −1)D −U

xk + f .
If we choose ω > 1, the method is termed a (successive) over-relaxation (SOR)
method; if 0 < ω < 1, the method is called an under-relaxation method. From the
previous identity, we also ﬁnd that the error transfer matrix is
Tω = (L + ω−1D)−1  (ω−1 −1)D −U

and the iterator is
Bω = L + ω−1D.
Theorem 6.17 (convergence). Let A ∈Cn×n have nonzero diagonal entries.
A necessary condition for convergence of the relaxation method is that ω ∈(0, 2).

136
Linear Iterative Methods
Proof. Since A ∈Cn×n has nonzero diagonal entries, the relaxation method is
well deﬁned. We know that a necessary and suﬃcient condition for convergence is
ρ(Tω) < 1. Since the eigenvalues are roots of the characteristic polynomial,
χT(λ) = det(Tω −λIn) = (−1)n
n
Y
i=1
(λ −λi),
setting λ = 0 we obtain χT (0) = det(Tω) = Qn
i=1 λi. Therefore, if we have that
| det(Tω)| ≥1 this means that there must be at least one eigenvalue that satisﬁes
|λi| ≥1 and the method cannot converge. Consequently, a necessary condition for
unconditional convergence — i.e., convergence for any starting point — is that
| det(Tω)| < 1.
In the case of the relaxation method, we have
det(Tω) = det((ω−1 −1)D −U)
det(L + ω−1D)
=
Qn
i=1(ω−1 −1)di,i
Qn
i=1 ω−1di,i
= (ω−1 −1)n
ω−n
= ωn(ω−1 −1)n
= (1 −ω)n.
If ω ̸∈(0, 2), then | det(Tω)| ≥1. In other words, if ω ̸∈(0, 2) the method cannot
converge unconditionally, meaning that ω ∈(0, 2) is a necessary condition for
unconditional convergence.
Let us show that, in a particular case, this condition is also suﬃcient.
Theorem 6.18 (convergence). Let A ∈Cn×n be HPD and ω ∈(0, 2). Then the
relaxation method converges. In particular, the Gauss–Seidel method converges,
since Tω=1 = TGS.
Proof. Recall that
Tω = B−1
ω
 (ω−1 −1)D −U

and
In −Tω = In −B−1
ω
 (ω−1 −1)D −U

= B−1
ω
 Bω −(ω−1 −1)D + U

= B−1
ω A.
Now suppose that (λ, w) is an eigenpair of the matrix Tω. Set
y = (In −Tω)w = (1 −λ)w.

6.6 The Householder–John Criterion
137
The previous computation implies that Bωy = Aw. For this reason,
(Bω −A)y = (Bω −A)B−1
ω Aw
= (BωB−1
ω A −AB−1
ω A)w
= (A −AB−1
ω A)w
= A(In −B−1
ω A)w
= ATωw
= λAw.
Taking the inner product with y, and using the fact that A is Hermitian, we ﬁnd
(Bωy, y)2 = (Aw, y)2 = (1 −¯λ)(w, Aw)2,
which implies, using the explicit form of Bω, that
(Ly, y)2 + ω−1(Dy, y)2 = (1 −¯λ)(w, Aw)2.
(6.5)
Similarly,
(y, (Bω −A)y)2 = ¯λ(y, Aw)2 = ¯λ(1 −λ)(w, Aw)2,
which implies that
(ω−1 −1)(Dy, y)2 −(y, Uy)2 = ¯λ(1 −λ)(w, Aw)2.
(6.6)
Adding (6.5) and (6.6) — and observing that, since A is Hermitian, (Ly, y)2 =
(y, Uy)2 — we obtain
(2ω−1 −1)(Dy, y)2 = (1 −|λ|2)(w, Aw)2.
Recall that, since A is HPD, so is its diagonal D. The expression on the left is
positive, provided that ω ∈(0, 2). This means that
1 −|λ|2 = (2ω−1 −1)(Dy, y)2
(w, Aw)2
> 0,
which implies that |λ| < 1. It follows that ρ(Tω) < 1.
6.6
The Householder–John Criterion
This next result can be used to prove the convergence of a class of methods based
on a veriﬁable criterion.
Theorem 6.19 (Householder–John criterion4). Suppose that A
∈
Cn×n is
nonsingular and Hermitian and B ∈Cn×n is nonsingular. Assume that
Q = B + BH −A
is HPD. Then the two-layer stationary linear iteration method with error transfer
matrix T = In −B−1A converges unconditionally if and only if A is HPD.
4 Named in honor of the American mathematician Alston Scott Householder (1904–1993) and
the German–American mathematician Fritz John (1910–1994).

138
Linear Iterative Methods
Proof. For this proof, we use the standard spectral theory. Another method of
proof is demonstrated in the next section.
Suppose that A is HPD. Let (λ, w) be an arbitrary eigenpair of T. We want to
show that |λ| < 1. Using the deﬁnition of T, we observe that
(1 −λ)Bw = Aw.
It follows that λ ̸= 1. Otherwise, A would be singular. It follows that
w HBw =
1
1 −λw HAw.
Taking the conjugate transpose of this equation, we have
w HBHw =
1
1 −¯λw HAw,
using the fact that A is Hermitian. Combining the last two equations,
w H  BH + B −A

w =

1
1 −λ +
1
1 −¯λ −1

w HAw = 1 −|λ|2
|1 −λ|2 w HAw.
Since
w HAw > 0,
w H  BH + B −A

w > 0,
it must be that
1 −|λ|2
|1 −λ|2 > 0,
which implies that
|λ| < 1.
Using our previous calculations, but only assuming that A is nonsingular and
Hermitian, if
1 −|λ|2
|1 −λ|2 > 0
and
w H  BH + B −A

w > 0,
it is easy to see that
w HAw > 0
for every eigenvector w ∈Cn
⋆. This proves that A must be HPD.
6.7
Symmetrization and Symmetric Relaxation
When the coeﬃcient matrix, A, is symmetric, it is often desirable that the iterator
matrix, B, is as well. For the standard Gauss–Seidel method, in particular, this is
not the case. However, there is a simple way to symmetrize the iterator.

6.7 Symmetrization and Symmetric Relaxation
139
Deﬁnition 6.20 (symmetrized method). Let A ∈Cn×n be invertible and f ∈Cn.
Suppose that x = A−1f and consider the stationary two-layer method (6.2) deﬁned
by the invertible iterator matrix B ∈Cn×n. The symmetrized stationary two-layer
method is deﬁned as follows:
xk+ 1
2 = xk + B−1 (f −Axk) ,
xk+1 = xk+ 1
2 + B−H 
f −Axk+ 1
2

.
(6.7)
Lemma 6.21 (standard form). Let A ∈Cn×n be invertible, f ∈Cn, and x = A−1f .
Suppose that B ∈Cn×n is invertible and consider the symmetrized stationary two-
layer method (6.7). Then
xk+1 = xk + CS (f −Axk) ,
where
CS = B−H  B + BH −A

B−1.
If A is Hermitian, then CS is as well.
Proof. See Problem 6.4.
Deﬁnition 6.22 (symmetric relaxation). Let A ∈Cn×n be invertible with nonzero
diagonal entries and consider the relaxation method with the iterator
Bω = L + ω−1D,
ω > 0,
where A = L + D + U is the standard splitting of A into lower triangular, diagonal,
and upper triangular parts, respectively. Note that Bω is invertible. The symmetric
relaxation method is the symmetrized stationary two-layer method with respect
to Bω, i.e.,
Cω,S = B−H
ω
 Bω + BH
ω −A

B−1
ω .
When ω = 1, the method is called the symmetric Gauss–Seidel method.
Notice that if A is Hermitian, then Cω,S is as well, according to Lemma 6.21; in
particular,
Cω,S = B−H
ω
 L + ω−1D + U + ω−1D −A

B−1
ω
= B−H
ω
 (2ω−1 −1)D

B−1
ω .
Theorem 6.23 (convergence). Let A ∈Cn×n be HPD and ω ∈(0, 2). Then the
symmetric relaxation method converges. In particular, the symmetric Gauss–Seidel
method, obtained by setting ω = 1, converges.
Proof. Since A is HPD, it has positive diagonal entries. Using this fact and the
fact that 0 < ω < 2, it follows that Cω,S is invertible. Then
C−1
ω,S (xk+1 −xk) = f −Axk.
In other words, the iterator matrix for the symmetric relaxation method is precisely
C−1
ω,S =
 2
ω −1
−1
BωD−1BH
ω.

140
Linear Iterative Methods
One can show that
Q = C−1
ω,S + C−H
ω,S −A
is HPD, since A is HPD. Applying the Householder–John criterion, Theorem 6.19,
we see that the method converges. The details are left to the reader as an exercise;
see Problem 6.5.
6.8
Convergence in the Energy Norm
In this section, we provide a powerful alternate method for proving convergence
of some methods. This technique is called the energy method. Recall that, if A is
HPD, we can deﬁne the so-called energy norm of the matrix by
∥x∥2
A = (x, x)A = (Ax, x)2.
We will give suﬃcient conditions for convergence in this norm. Before we do that,
though, we need a technical result.
Lemma 6.24 (positive deﬁnite). Suppose that Q ∈Cn×n is positive deﬁnite in the
sense that
ℜ((Qy, y)2) > 0,
∀y ∈Cn
⋆,
but Q is not necessarily Hermitian. Then
∥w∥Q =
q
ℜ((Qw, w)2),
∀w ∈Cn
deﬁnes a norm.
Proof. Suppose that Q is not Hermitian to avoid the simple case. Then
Q = QH + QA,
where
QH = 1
2
 Q + QH
,
QA = 1
2
 Q −QH
are the Hermitian and anti-Hermitian parts, respectively. Observe that QH
H = QH
and QH
A = −QA. It follows that (QAy, y)2 is purely imaginary for any y ∈Cn,
because
(QAy, y)2 = y HQAy = y HQH
Ay = −y HQAy = −(QAy, y)2.
Therefore, for all y ∈Cn
⋆,
0 < ℜ((Qy, y)2) = ℜ((QHy, y)2) + ℜ((QAy, y)2) = (QHy, y)2,
since (QHy, y)2 is real. Therefore, QH is HPD. Since
∥w∥QH =
q
(QHw, w)2,
∀w ∈Cn
deﬁnes a norm, the result follows.

6.8 Convergence in the Energy Norm
141
Theorem 6.25 (convergence in energy). Let A be HPD. Suppose that B ∈Cn×n
is the invertible iterator describing a two-layer stationary linear iteration method. If
Q = B −1
2A is positive deﬁnite in the sense that
ℜ((Qy, y)2) > 0,
∀y ∈Cn
⋆,
but is not necessarily Hermitian, then method (6.2) converges.
Proof. Deﬁne the error ek = x −xk, as usual, and notice that for any stationary
two-layer linear iteration method we have
B (ek+1 −ek) + Aek = 0,
which is equivalent to
Bqk+1 + Aek = 0,
where qk+1 = ek+1 −ek. Taking the inner product of this identity with qk+1 and
using the fact that
ek = 1
2(ek+1 + ek) −1
2(ek+1 −ek),
we obtain
0 = (Bqk+1, qk+1)2 + (Aek, qk+1)2
=

B −1
2A

qk+1, qk+1

2
+ 1
2(Aek+1, ek+1)2 −1
2(Aek, ek)2
= (Qqk+1, qk+1)2 + 1
2∥ek+1∥2
A −1
2∥ek∥2
A.
Consequently,
ℜ((Qqk+1, qk+1)2) + 1
2∥ek+1∥2
A = 1
2∥ek∥2
A;
in other words,
1
2∥ek+1∥2
A + ∥qk+1∥2
Q = 1
2∥ek∥2
A.
This allows us to conclude that the sequence ∥ek∥A is nonincreasing. Since it
is bounded below, by the monotone convergence theorem, it must have a limit,
limk→∞∥ek∥A = α, say. Passing to the limit in this identity then tells us that
∥qk+1∥Q →0,
as k →∞.
But, since A is invertible,
ek = −A−1Bqk+1
and this implies that ek →0. Therefore, the method converges.
Let us apply this result to obtain, by diﬀerent means, convergence of two methods
that we have previously studied.

142
Linear Iterative Methods
Example 6.1
Convergence of Richardson’s method. For Richardson’s method we
have Q = 1
αIn −1
2A, α > 0, and
(Qx, x)2 = 1
α∥x∥2
2 −1
2(Ax, x)2 ≥∥x∥2
2
 1
α −∥A∥2
2

.
A suﬃcient condition for Q to be positive deﬁnite, and, therefore, for the method
to converge, is that
0 < α <
2
∥A∥2
.
But, since A is HPD, ∥A∥2 = λn, the largest eigenvalue. If 0 < αλn < 2,
Richardson’s method converges by Theorem 6.25.
Example 6.2
Convergence of the relaxation method. In the relaxation method,
the iterator is
Bω = L + 1
ω D,
where A = L + D + U is the standard matrix splitting and ω > 0. Therefore,
Qω = Bω −1
2A = 1
ω D + L −1
2(L + D + LH) =
 1
ω −1
2

D + 1
2
 L −LH
and
ℜ((Qωx, x)2) =
 1
ω −1
2

(Dx, x)2 + ℜ
1
2((L −LH)x, x)2

=
 1
ω −1
2

(Dx, x)2,
where we used the fact that 1
2(L−LH) is anti-Hermitian. If 1
ω−1
2 > 0 or, equivalently,
ω < 2, Qω is positive deﬁnite — though not Hermitian — and the relaxation
method converges.
We now state another theorem — which we have seen before and which is similar
to the previous one — that can be proven using energy methods.
Theorem 6.26 (Householder–John). Suppose that A ∈Cn×n is nonsingular and
Hermitian and B ∈Cn×n is nonsingular. Assume that
Q = B + BH −A
is HPD. Then the two-layer stationary linear iteration method with error transfer
matrix T = In −B−1A converges unconditionally if and only if A is HPD.
Proof. Let us prove one direction and save the other for an exercise; see
Problem 6.6.

6.9 A Special Matrix
143
Suppose that A is HPD. Recall that ∥· ∥A deﬁnes a norm on Cn. The error
equation is precisely
ek+1 =
 I −B−1A

ek.
Then
∥ek+1∥2
A =
  I −B−1A

ek
H A
  I −B−1A

ek

=
 ek
H  I −AB−H
A
  I −B−1A

ek

=
 ek
H −ek
HAB−H  Aek −AB−1Aek

= ek
HAek −ek
HAB−HAek −ek
HAB−1Aek + ek
HAB−HAB−1Aek
= ∥ek∥2
A −ek
HA
 B−H + B−1 −B−HAB−1
Aek
= ∥ek∥2
A −ek
HAB−H  B + BH −A

B−1Aek
= ∥ek∥2
A −
 B−1Aek
H  B + BH −A

B−1Aek.
Since B + BH −A is HPD and B−1Aek ̸= 0, in general, it follows that
∥ek+1∥2
A +
B−1Aek
2
Q = ∥ek∥2
A
and ∥ek∥A is a decreasing sequence. Therefore, by the monotone convergence
theorem, ∥ek∥A converges, i.e., there is some α ∈[0, ∞) such that
lim
k→∞∥ek∥A = α = lim
k→∞∥ek+1∥A .
This implies that
lim
k→∞
B−1Aek

Q = 0,
which, in turn, implies that ek →0, as k →∞.
Example 6.3
Let us provide yet another proof of convergence of the relaxation
method. Suppose that A ∈Cn×n is HPD. We once again recall that the iterator
matrix for the relaxation method is
Bω = L + 1
ω D,
where A = L + D + U is the standard matrix splitting and ω > 0. Therefore,
Qω = Bω + BH
ω −A = 2
ω D + L + LH −(L + D + LH) =
 2
ω −1

D.
If 0 < ω < 2, Qω is HPD and the method converges.
6.9
A Special Matrix
We end the discussion of classical, linear, stationary, iterative methods by studying
their eﬀect on the TST matrix deﬁned in Theorem 3.38.

144
Linear Iterative Methods
Theorem 6.27 (convergence). Let A be the TST matrix deﬁned in Theorem 3.38.
Denote by TJ, TGS ∈R(n−1)×(n−1) the error transfer matrices of the Jacobi and
Gauss–Seidel methods, respectively. Then ρ(TJ) < 1 and ρ(TGS) < 1.
Proof. From the proof of Theorem 3.38, we observe that the distinct eigenvalues
of A are λk = 2 −2 cos(kπh), where h = 1/n, for k = 1, . . . , n −1. This implies
that
0 < λ1 < · · · < λn−1 < 4.
In addition, we recall that the kth eigenvector of A, associated with λk, can be
deﬁned as
[w k]i = sin(kπih).
The error transfer matrix of the Jacobi method is
TJ = D−1(D −A) = 1
2(2In−1 −A) = 1
2


0
1
0
· · ·
0
1
0
...
...
...
0
...
...
1
0
...
...
1
0
1
0
· · ·
0
1
0


.
Then
TJw k =

1 −1
2λk

w k.
In other words, the eigenvalues of TJ are µk = cos(kπh), k = 1, . . . , n −1. Hence,
ρ(TJ) = µ1 = −µn−1 = cos(πh) < 1.
By Theorem 6.14, since A is symmetric positive deﬁnite (SPD) tridiagonal,
ρ(TGS) = ρ2(TJ) = cos2(πh).
Theorem 6.28 (tridiagonal matrices). Suppose that A ∈Cn×n is HPD and
tridiagonal. Then
ρ(TGS) = ρ2(TJ) < 1
and the optimal choice for ω in the relaxation method is
ωopt =
2
1 +
p
1 −ρ(TGS)
.
With this choice,
ωopt −1 = ρ(Tωopt) =
min
ω∈(0,2) ρ(Tω).
Proof. See Problem 6.7.
Example 6.4
Theorem 6.28 shows that, using the optimal choice for the
relaxation method, this method can converge much faster than the Gauss–Seidel

6.10 Nonstationary Two-Layer Methods
145
method. Suppose that A is the TST matrix deﬁned in Theorem 3.38. Then we
proved that
ρ(TGS) = ρ2(TJ) = cos2(πh).
Suppose that n = 32, so that h = 1/32. Then
ρ(TGS) = cos2(π/32) ≈(0.995 184 72)2 ≈0.990 392 64.
But
ωopt =
2
1 +
p
1 −cos2(π/32)
≈1.821 465 19.
Therefore,
ρ(Tωopt) = ωopt −1 ≈0.821 465 19.
This might not seem like a big deal, but it is. The convergence rate of the relaxation
method with the optimal parameter ωopt is much better than that for Gauss–Seidel.
6.10
Nonstationary Two-Layer Methods
We now consider a class of nonstationary methods that are usually obtained from
a minimization condition.
6.10.1
Chebyshev’s Method
Let us consider the explicit method:
xk+1 −xk
αk+1
+ Axk = f ,
where αk > 0. This is a nonstationary method deﬁned by setting
Bk = BC,k = 1
αk
In
and is known as Chebyshev’s method.5 It is like Richardson’s method, except that
we will choose the parameter αk adaptively, in such a way that, after m steps, the
quantity ∥em∥2 = ∥x −xm∥2 is as small as possible.
Theorem 6.29 (convergence of Chebyshev’s method). Let A ∈Cn×n be HPD
with spectrum σ(A) = {λi}n
i=1, 0 < λ1 ≤λ2 ≤· · · ≤λn. For a given m ∈N, the
quantity ∥em∥2 is minimized if
αk =
α0
1 + ρ0tk
,
k = 1, . . . , m,
where
α0 =
2
λ1 + λn
,
ρ0 = κ2(A) −1
κ2(A) + 1,
tk = cos
(2k −1)π
2m

.
5 Named in honor of the Russian mathematician Pafnuty Lvovich Chebyshev (1821–1894).

146
Linear Iterative Methods
In this case, we have
∥em∥2 ≤
2ρm
1
1 + ρ2m
1
∥e0∥2,
ρ1 =
p
κ2(A) −1
p
κ2(A) + 1
.
Proof. Let us merely sketch the proof. The error is governed by
ek+1 −ek + αk+1Aek = 0,
so that
ek+1 = (In −αk+1A)ek = · · · = (In −αk+1A) · · · (In −α1A)e0.
This implies that
em = Tme0,
Tm = (In −αmA)(In −αm−1A) · · · (In −α1A).
Since A is Hermitian, so is Tm; therefore, ∥Tm∥2 = ρ(Tm). It follows that
∥em∥2 ≤ρ(Tm)∥e0∥2
and it suﬃces to minimize ρ(Tm).
Notice that ν ∈σ(Tm) if and only if ν = Qm
i=1(1 −αiλ) for some λ ∈σ(A).
Now, since A is HPD,
ρ(Tm) =
n
max
i=1

m
Y
j=1
(1 −αjλi)

≤
max
ζ∈[λ1,λn] |p(ζ)|,
where
p(ζ) =
m
Y
j=1
(1 −αjζ).
Solving this problem is beyond the scope of our present discussion. For now, let
us just say that this is possible to do. The solution is given by shifted and rescaled
roots of the Chebyshev polynomials Tm, which will imply all the formulas given
above; see Section 10.3.
Chebyshev’s method shows that, if we have bounds for the spectrum of A, it is
possible to make the error as small as possible after a ﬁxed number of iterations.
What can we do if such bounds are not known? This gives rise to the following
methods.
6.10.2
The Method of Minimal Residuals
Let A ∈Cn×n be HPD. Given an arbitrary guess xk, the residual is deﬁned by
r k = f −Axk. Notice that
Aek = A(x −xk) = f −Axk = r k.
Let us again consider the nonstationary method
xk+1 = xk + αk+1(f −Axk) = xk + αk+1r k,

6.10 Nonstationary Two-Layer Methods
147
where we will choose the iteration parameter to minimize ∥r k+1∥2. Let us apply A
to the method to get
Axk+1 = Axk + αk+1Ar k,
which is equivalent to
r k+1 = f −Axk+1 = f −Axk −αk+1Ar k.
To sum up,
r k+1 = Tk+1r k,
Tk+1 = In −αk+1A.
Notice that Tk+1 has a familiar form. Computing the 2-norm of the residual, we
ﬁnd
∥r k+1∥2
2 = (r k −αk+1Ar k, r k −αk+1Ar k)2
= ∥r k∥2
2 + α2
k+1∥Ar k∥2
2 −2αk+1(Ar k, r k)2,
which, being a positive quadratic in αk+1, is clearly minimized by setting
αk+1 = (Ar k, r k)2
∥Ar k∥2
2
.
Thus, we arrive at the so-called method of minimal residuals. This method is
presented in Listing 6.1. It will converge faster than Richardson’s method, as we
show in the following result.
Theorem 6.30 (convergence). Let A be HPD with spectrum σ(A) = {λi}n
i=1,
0 < λ1 ≤λ2 ≤· · · ≤λn. Then
∥Aek∥2 ≤ρk
⋆∥Ae0∥2,
ρ⋆= κ2(A) −1
κ2(A) + 1.
Proof. Since r k+1 = Tk+1r k in the 2-norm is minimized by the given choice of
αk+1, we must have that
∥r k+1∥2 = ∥Tk+1r k∥2 ≤∥Tr k∥2,
where T = In −αA and any choice of α ∈C. In particular, we can set α = α⋆=
2
λ1+λn to get the error transfer matrix of Richardson’s method with its optimal
choice of parameter. In this case, ∥T∥2 = ρ⋆< 1. Since Aek = r k, the result
follows.
6.10.3
The Method of Minimal Corrections
Let us consider now a nonstationary two-layer method of the form
1
αk+1
S(xk+1 −xk) + Axk = f ,
where αk+1 > 0 and S ∈Cn×n is invertible. This conforms to our standard
nonstationary iteration framework by setting Bk+1
=
1
αk+1 S. We deﬁne the
correction to be w k = S−1r k and notice that
xk+1 = xk + αk+1S−1(f −Axk) = xk + αk+1S−1r k = xk + αk+1w k.

148
Linear Iterative Methods
Let us now assume that S is HPD, so that it has an associated energy norm.
The method of minimal corrections then chooses αk+1 so as to minimize ∥w k+1∥2
S.
Theorem 6.31 (convergence). Let A, S ∈Cn×n be HPD. Then
σ(S−1A) = {µi}n
i=1 ⊂(0, ∞).
Suppose that
0 < µ1 ≤· · · ≤µn
and
κ = µn
µ1
.
Then
∥Aek∥S−1 ≤ρk
0∥Ae0∥S−1,
ρ0 = κ −1
κ + 1.
Proof. Since S is HPD, the object (x, y)S = (Sx, y)2 deﬁnes an inner product and
∥x∥S =
p
(x, x)S deﬁnes a norm. Since the matrix S−1A is self-adjoint and positive
deﬁnite with respect to the inner product ( · , · )S, we leave it to the reader as an
exercise to show that the eigenvalues of S−1A are all real and positive.
Furthermore, we can deﬁne the square root S1/2. In particular, since S is HPD,
there exist a unitary matrix U ∈Cn×n and a diagonal matrix D = diag(ν1, . . . , νn),
with positive real diagonal entries, such that
S = UDUH.
Consequently, we deﬁne D1/2 = diag
 √ν1, . . . , √νn

. Set
S1/2 = UD1/2UH.
Clearly, S1/2 is HPD and S1/2S1/2 = S. Let us introduce then the change of variables
v k = S1/2w k and notice that, from Problem 6.8, it follows that
1
αk+1
(v k+1 −v k) + Cv k = 0
(6.8)
with C = S−1/2AS−1/2. Finally, note that we must choose αk+1 in order to minimize
∥w k+1∥2
S = (Sw k+1, w k+1)2 = (S1/2w k+1, S1/2w k+1)2 = ∥v k+1∥2
2.
We can repeat some steps from the proof of Theorem 6.30 to obtain
αk+1 = (Cv k, v k)2
∥Cv k∥2
2
.
Next, we observe the following identities:
(Cv k, v k)2 = (S−1/2AS−1/2w k, S−1/2w k)2 = (Aw k, w k)2,
∥Cv k∥2
2 = (S−1/2Aw k, S−1/2Aw k)2 = ∥Aw k∥2
S−1,
∥v k+1∥2 = (Sw k+1, w k+1)2 = ∥Aek+1∥2
S−1.
The desired error estimate follows by using these.

Problems
149
Problems
6.1
Complete the proof of Theorem 6.13.
6.2
Let A, B ∈Cn×n with A and A −BA−1B being invertible. Consider solving
the linear system
Ax + By = f 1,
Bx + Ay = f 2
for the unknowns x and y.
a)
Use the Schur complement to show that this system has a unique solution.
b)
Show that ρ(A−1B) < 1 is a necessary and suﬃcient condition for convergence
of the iteration method:
Axk+1 = f 1 −By k,
Ay k+1 = f 2 −Bxk
with an arbitrary initial guess.
c)
Consider the following slightly modiﬁed iteration method:
Axk+1 = f 1 −By k,
Ay k+1 = f 2 −Bxk+1.
Does the conclusion of the previous part still hold? Why or why not?
6.3
Use the Householder–John criterion to prove that the Gauss–Seidel method
converges for any initial guess if A is HPD. Can the criterion be used to establish
convergence for the relaxation method?
6.4
Prove Lemma 6.21.
6.5
Prove Theorem 6.23.
6.6
Complete the proof of Theorem 6.26.
6.7
Prove Theorem 6.28.
6.8
Consider a nonstationary two-layer method of the form
1
αk+1
S (xk+1 −xk) + Axk = f ,
where αk+1 > 0 and S ∈Cn×n is invertible. Show that w k = S−1r k satisﬁes
1
αk+1
S (w k+1 −w k) + Aw k = 0.
6.9
Deﬁne A ∈Rn×n via
A =


2
−1
0
· · ·
0
−1
2
...
...
0
...
...
−1
0
...
−1
2
−1
0
· · ·
0
−1
2


.
a)
Find the eigenvalues and eigenvectors of A.
b)
Suppose that A = D + U + U⊺, where D is the diagonal part of A and U
is the strict upper triangular part. Find the eigenvalues and eigenvectors of
TJ = In −D−1A, the error transfer matrix for the Jacobi method.

150
Linear Iterative Methods
c)
The damped Jacobi method has the error transfer matrix
T(ω) = −ωD−1(U + U⊺) + (1 −ω)In,
0 < ω < 2,
so that T(1) = TJ. Write out the corresponding linear stationary two-layer
method.
d)
Find the eigenvalues and eigenvectors of T(ω) ∈Cn×n.
e)
Let µk(ω) be the kth eigenvalue of T(ω) with the ordering
µ1 ≤µ2 ≤· · · ≤µn.
(You will observe in this problem that we may extend the deﬁnition of the
eigenvalues in a natural way, so that µ0 = 1 and µn+1 = 1 −2ω.) Assume
that n + 1 is even. Prove that the quantity
S(ω) =
max
n+1
2 ≤k≤n+1 |µk(ω)|
is minimized by ω = ω0 = 2
3 and that
|µk(ω0)| ≤1
3
for all n+1
2
≤k ≤n + 1.
6.10
Let A ∈Cn×n be HPD and f ∈Cn. Consider solving Ax = f using the linear
iterative method (6.2), where B ∈Cn×n is invertible. Suppose that Q = B+BH −A
is positive deﬁnite. Let ek = x −xk be the error of the kth iteration. Show that
each step of this method reduces the A-norm of ek, whenever ek ̸= 0. Prove that
this linear iteration method converges.
6.11
Let A ∈Cn×n be Hermitian and f ∈Cn. Consider solving Ax = f using the
linear iterative method (6.2), where B is invertible and x0 ∈Cn is arbitrary.
a)
If A and B + BH −A are HPD, prove that the method is convergent.
b)
Conversely, suppose that B + BH −A is HPD and the linear iterative method
above is convergent. Prove that A is HPD.
c)
Prove that the Gauss–Seidel method converges when A is HPD.
d)
Prove that the Jacobi method converges when A and 2D −A are HPD, where
D is the diagonal part of A.
6.12
Suppose that A ∈Cn×n is an upper triangular nonsingular matrix and f ∈
Cn. Show that both the Jacobi and Gauss–Seidel iteration methods always converge
when used to solve Ax = f , and, moreover, that they will converge in ﬁnitely many
steps.
6.13
Let A ∈Cn×n have the property that there is q ∈(0, 1) for which
q|ai,i| >
n
X
j=1
j̸=i
|ai,j|,
1 ≤i ≤n.
Show that the Gauss–Seidel method converges and that we have the error estimate
∥ek∥∞≤qk∥e0∥∞.

Problems
151
6.14
Given the matrix A ∈R100×100 assume that its eigenvalues are known via
the formula
λk = k2,
k = 1, . . . , 100.
The system Ax = f is to be solved by the following nonstationary Richardson
method:
xk+1 = (I100 −τkA)xk + τkf
for some positive parameters τk. Find a particular set of iteration parameters
{τ0, . . . , τ99} such that x100 = x, the exact solution of Ax = f .
Hint: Verify that ek+1 = (In −τkA)ek, then expand the initial error
e0 = α1u1 + · · · + α100u100,
where the ui are the eigenvectors of A. Choose the iteration parameters to eliminate
exactly one term of the expansion of e0.
6.15
Show that if A ∈Cn×n is HPD, then the relaxation method converges for
0 < ω < 2.
6.16
Let X be a ﬁnite-dimensional vector space, f ∈X, and A: X →X a linear
operator. A general two-layer iterative process for the solution of Ax = f can be
understood as a map J : X × X →X as follows: given x0 ∈X the sequence of
iterates is deﬁned by
xk+1 = J (xk, f ).
a)
The map J is said to be consistent (with Ax = f ) if and only if the solution
x is a ﬁxed point, i.e., x = J (x, f ).
b)
The map J is linear if and only if
J (αx + βy, αf + βg) = αJ (x, f ) + βJ (y, g).
Show that any linear and consistent iterative method can be written in the form
xk+1 = xk + B(f −Axk),
where B : X →X is a linear operator.
Hint: Following Proposition 6.3, deﬁne Bf = J (0, f ) and (I −BA)x = J (x, 0).
6.17
Suppose that A ∈Cn×n is HPD and f ∈Cn. Consider Richardson’s method
with an optimal choice of iterative parameter. How many iterations are needed to
reduce the error by a predetermined factor? In other words, given σ > 0, how many
iterations are needed to guarantee that
∥ek∥2 ≤σ∥e0∥2.
6.18
Suppose that
A1 =
 1
−1
2
−1
2
1

,
A2 =
 1
−3
4
−1
12
1

.
Let T1 and T2 be the associated error transfer matrices for Jacobi’s method for
the respective coeﬃcient matrices. Show that ρ(T1) > ρ(T2), thereby refuting the
claim that greater diagonal dominance implies faster convergence.

152
Linear Iterative Methods
6.19
This problem deals with an acceleration procedure called extrapolation.
Consider a linear stationary method
xk+1 = Txk + c,
(6.9)
which can be embedded in a one-parameter family of methods
xk+1 = γ(Txk + c) + (1 −γ)xk = Tγxk + γc
with Tγ = γT + (1 −γ)I and γ ̸= 0.
a)
Show that any ﬁxed point of Tγ is a ﬁxed point of T.
b)
Assume that we know that σ(T) ⊂[a, b]. Show that σ(Tγ) ⊂[γa+1−γ, γb+
1 −γ].
c)
Show that, from the previous item, it follows that
ρ(Tγ) ≤max
a≤λ≤b |γλ + 1 −γ|.
d)
Show that if 1 /∈[a, b], then the choice γ⋆= 2/(2 −a −b) minimizes ρ(Tγ)
and that, in this case,
ρ(Tγ⋆) = 1 −|γ⋆|d,
where d is the distance between 1 and [a, b]. This shows that, using
extrapolation, we can turn a nonconvergent method into a convergent one.
e)
Suppose that A is HPD. Consider Richardson’s method with α = 1. If σ(A) ⊂
[m, M] ⊂(0, ∞), show that choosing γ = 2/(m + M) we obtain
ρ(Tγ) = M −m
M + m;
thus, Richardson’s method with extrapolation converges.
6.20
This problem deals with Chebyshev acceleration. Consider, again, the linear
stationary method (6.9) and assume that we have computed x0, . . . , xk. We will
compute a linear combination of them,
uk =
k
X
i=0
ak,ixi,
in the hope that uk is a better approximation to x.
a)
Show that if x0 = x, then xk = x. This tells us that we want Pk
i=0 ak,i = 1.
Why?
b)
Show that
uk −x = p(T)e0,
where p(z) = Pk
i=0 ak,izi. Consequently, ∥uk −x∥≤∥p(T)∥∥e0∥.
c)
Show that, if σ(T) ⊂[a, b],
ρ(p(T)) ≤max
z∈[a,b] |p(z)|.

Problems
153
d)
Thus, we need to ﬁnd a polynomial of degree k such that p(1) = 1 (Why?)
and minimizes
max
z∈[a,b] |p(z)|.
It can be shown that shifted Chebyshev polynomials solve this problem and
that, moreover,
u1 = γ[Tu0 + c] + (1 −γ)u0,
γ =
2
2 −b −a,
and
uk = ρk[γ(Tuk−1 + c) + (1 −γ)uk−1] + (1 −ρk)uk−2,
with ρ1 = 2, ρk = (1 −αρk−1)−1, and α = (2(2 −b −a)/(b −a))−2.
6.21
Show that, for the method of minimal corrections,
αk+1 =
(Aw k, w k)2
(S−1Aw k, w k)2
.
6.22
Suppose that A ∈Rn×n is invertible and f ∈Rn. Consider the following
iterative method to solve Ax = f :
xk+1 = xk + αr k,
where r k = f −Axk is the residual, x0 ̸= A−1f is arbitrary, and α is a scalar
parameter to be determined.
a)
Show that if all the eigenvalues of A have positive real part, then there will be
some real α such that the method converges for any starting vector x0.
b)
Show how to choose α optimally in the case that A is SPD, and estimate the
rate of convergence in the 2-norm.
c)
Show that if some eigenvalues of A have negative real part and some have
positive real part, then there is no real α for which the iterations converge.
6.23
Let A = B+P ∈Cn×n be invertible. To approximate the solution of Ax = f ,
starting from an arbitrary x0 ∈Cn, we apply the following iterative method (where
we are implicitly assuming that B is invertible):
Bxk+1 + Pxk = f .
Show that this iterative method is convergent if and only if all the roots of
det(κB + P) = 0,
κ ∈C,
are of modulus strictly less than one. Use this to show that if A is HPD, then the
Gauss–Seidel method converges.
Hint: You may use without proof that if M is skew-Hermitian, then for every
w ∈Cn we have
ℜ
 w HMw

= 0.

154
Linear Iterative Methods
6.24
Let A ∈Rn×n be SPD with eigenvalues 0 < λ1 ≤λ2 ≤· · · ≤λn. To
approximate the solution x of Ax = f , we consider the following iterative method:
xk+1 = xk + αkB(f −Axk),
where αk > 0 and B is some nonsingular matrix.
a)
For what choices of αk and B does the above method reduce to:
i)
The Jacobi method?
ii)
The Gauss–Seidel method?
b)
Let B = In in the following and ek = x −xk. Show that there is a polynomial
pk such that
ek = pk(A)e0.
What are the zeros of the polynomial pk?
c)
Prove the inequality
∥pk(A)∥2 ≤
max
λ1≤t≤λn |pk(t)|.
d)
For the case where αk = α > 0 independent of k, what conditions does α
have to satisfy in order to ensure convergence?
6.25
Let A ∈Cn×n be HPD. Deﬁne A1 = 1
2D + L and A2 = 1
2D + U, where
A = D+L+U is the usual decomposition of A into diagonal, strictly lower triangular,
and strictly upper triangular parts, respectively. For the solution of Ax = f , consider
a linear stationary method with iterator
B = (In + αA1)(In + αA2),
α > 0.
a)
Show that B is HPD.
b)
Show that, for all α ≥1
2, this method converges.
c)
Why is this method eﬃcient?
Listings
1
function [x, its, err] = MinRes( A, x0, f, maxit, tol )
2
% The method of minimal residuals to approximate the solution to
3
%
4
%
Ax = f
5
%
6
% with A HPD.
7
%
8
% Input
9
%
A(1:n,1:n) : the system matrix,
10
%
x0(1:n) : the initial guess
11
%
f(1:n) : the right hand side vector
12
%
maxit : the maximal number of iterations
13
%
tol : the tolerance
14
%
15
% Output

Listings
155
16
%
x(1:n) : the approximate solution to the linear system of
17
%
equations
18
%
its : the number of iterations
19
%
err : = 0, if the tolerance is reached in less than maxit
20
%
iterations
21
%
= 1, if the tolerance is not reached
22
err = 0;
23
x = x0;
24
25
for its=1:maxit
26
r = f-A*x;
27
p = A*r;
28
p norm = norm( p );
29
if p norm < tol
30
return;
31
end
32
alpha = r'*p/(p norm*p norm);
33
x = x + alpha*r;
34
end
35
err = 1;
36
end
Listing 6.1 The method of minimal residuals.

7
Variational and Krylov Subspace
Methods
In this chapter, we introduce gradient-type methods for solving the system of
equations Ax = f , where A ∈Cn×n is Hermitian positive deﬁnite (HPD). These are
iterative methods that include the steepest descent and conjugate gradient (CG)
methods. We will take advantage of the fact that solving this equation is equivalent
to minimizing the quadratic function
EA(z) = 1
2zHAz −ℜ
 zHf

over Cn, and will utilize some simple ideas from the theory of convex optimization,
which we will study in more detail in Chapter 16.
7.1
Basic Facts about HPD Matrices
Let us collect some basic facts about HPD matrices and some connections between
these and inner products. Many of these properties have been covered previously
and will be familiar.
Theorem 7.1 (properties of HPD matrices). Suppose that A ∈Cn×n is HPD.
Then the following are true.
1. The expression
(x, y)A = (Ax, y)2 = y HAx,
∀x, y ∈Cn
deﬁnes an inner product on Cn.
2. The object ∥x∥A =
√
xHAx, where x ∈Cn deﬁnes a norm on Cn.
3. Let the eigenvalues of A be ordered, so that 0 < λ1 ≤λ2 ≤· · · ≤λn. Then
p
λ1 ∥x∥2 ≤∥x∥A ≤
p
λn ∥x∥2
for any x ∈Cn.
4. Let f ∈Cn be given. Then x = A−1f if and only if x minimizes the quadratic
function EA : Cn →R deﬁned by
EA(z) = 1
2zHAz −ℜ
 zHf

.
Proof. The ﬁrst and second parts are left to the reader as an exercise.

7.1 Basic Facts about HPD Matrices
157
(3) Suppose that {w 1, . . . , w n} is an orthonormal basis of Cn consisting of
eigenvectors of A with the ordering Aw i = λiw i. Let x ∈Cn be arbitrary. Then
there exist unique constants c1, . . . , cn ∈C such that x = Pn
i=1 ciw i. Hence,
∥x∥2
A = xHAx
= xH
n
X
i=1
ciλiw i
=
n
X
i=1
ciλi
 xHw i

=
n
X
i=1
ciλi


n
X
j=1
¯cjw H
j w i


=
n
X
i=1
ciλi


n
X
j=1
¯cjδij


=
n
X
i=1
|ci|2λi
≤λn
m
X
i=1
|ci|2
= λn ∥x∥2
2 .
The inequality λ1 ∥x∥2
2 ≤∥x∥2
A is obtained similarly and the result follows upon
taking square roots.
(4: =⇒) Suppose that x = A−1f . Let y ∈Cn be arbitrary and consider
EA(x + y) = 1
2 (x + y)H A (x + y) −ℜ
 xHf

−ℜ
 y Hf

= 1
2xHAx + 1
2y HAy + ℜ
 y H (Ax −f )

−ℜ
 xHf

= EA(x) + 1
2y HAy
≥EA(x),
where, in the last step, we used that A is HPD. Notice also that we have equality
if and only if y = 0. Hence, x minimizes EA.
An alternate approach, one that needs some inspiration perhaps, is to establish
the following equality (see Problem 7.7):
EA(z) = 1
2
z −A−1f
2
A −1
2f HA−1f
(7.1)
for arbitrary z ∈Cn. The right-hand side is clearly strictly convex (as a function of
z) and has the minimizer z = x = A−1f . Since the right- and left-hand sides must
have the same minimizer, we are done.

158
Variational and Krylov Subspace Methods
(4:
⇐= ) Now suppose that x minimizes the function EA. Let u ∈Cn be an
arbitrary unit vector. Now deﬁne g(s, t) = EA(x +αu), where α = s +it, s, t ∈R.
Then
g(s, t) = 1
2 (x + αu)H A (x + αu) −ℜ

(x + αu)H f

= 1
2xHAx + ℜ
 ¯αuHAx

+ |α|2
2 uHAu −ℜ
 xHf

−ℜ
 ¯αuHf

= EA(x) + ℜ
 ¯αuH (Ax −f )

+ |α|2
2 uHAu
= EA(x) + ℜ(¯α) ℜ
 uH (Ax −f )

−ℑ(¯α) ℑ
 uH (Ax −f )

+ s2 + t2
2
uHAu
= EA(x) + sℜ
 uH (Ax −f )

+ tℑ
 uH (Ax −f )

+ s2 + t2
2
uHAu.
Clearly, g is a strictly convex, quadratic function on R2. Moreover, g is minimized
at (s, t) = (0, 0). Hence,
0 = ∂g
∂s (0, 0) = ℜ
 uH (Ax −f )

and
0 = ∂g
∂t (0, 0) = ℑ
 uH (Ax −f )

hold for any vector u. It follows then that Ax = f .
In the previous result we saw that an HPD matrix deﬁnes an inner product. It
turns out that the converse is also true.
Proposition 7.2 (inner products and HPD matrices). Suppose that ( · , · ) : Cn ×
Cn →C is an inner product. There exists a unique HPD matrix A ∈Cn×n such
that
(x, y) = (Ax, y)2 = (x, y)A ,
∀x, y ∈Cn.
Proof. Let, for j = 1, . . . , n, ej denote the canonical unit basis vectors of Cn.
Deﬁne the matrix A = [ai,j] ∈Cn×n via
ai,j = (ej, ei).
The reader must show that A has the desired properties; see Problem 7.2.
Deﬁnition 7.3 (A-conjugate). Suppose that ( · , · ) : Cn × Cn →C is an inner
product and A ∈Cn×n is its associated HPD matrix. We say that B ∈Cn×n is
self-adjoint with respect to this inner product if and only if
(x, By) = (x, By)A = (Bx, y)A = (Bx, y),
∀x, y ∈Cn.
We say that B is self-adjoint positive deﬁnite with respect to the inner product if
and only if B is self-adjoint and satisﬁes
(x, Bx) = (x, Bx)A > 0,
∀x ∈Cn
⋆.

7.1 Basic Facts about HPD Matrices
159
We say that two vectors x, y ∈Cn are A-orthogonal (or A-conjugate) if and
only if
(x, y) = (x, y)A = 0.
We say that a set S ⊂Cn of nonzero vectors is called A-orthogonal (or A-
conjugate) if and only if whenever x, y ∈S and x ̸= y, then
(x, y) = (x, y)A = 0.
We say that S ⊂Cn is A-orthonormal if and only if S is A-orthogonal and
∥x∥A = 1, ∀x ∈S.
During the course of the proof of Theorem 6.31, we encountered the square root
of an HPD matrix. Let us review this idea.
Proposition 7.4 (square root). Suppose that A ∈Cn×n is HPD. Then A is invertible
and A−1 is HPD. Furthermore, there exists a unique HPD matrix B ∈Cn×n with
the property that BB = A.
Proof. Since A is HPD, it is unitarily diagonalizable, i.e., there are a unitary matrix
U ∈Cn×n and a diagonal matrix with positive diagonal entries D = diag(λ1, . . . , λn)
such that
A = UDUH.
Set
B = UD1/2UH
and
D1/2 = diag
p
λ1, . . . ,
p
λn

.
Then B has the desired properties. Furthermore, it is easy to see that
A−1 = UD−1UH,
D−1 = diag
 λ−1
1 , . . . , λ−1
n

.
We leave it to the reader to check the details.
Proposition 7.5 (product of HPD matrices). Suppose that A, B ∈Cn×n are HPD
matrices. Then the product BA is self-adjoint and positive deﬁnite with respect to
( · , · )A, i.e.,
(x, BAy)A = (BAx, y)A ,
∀x, y ∈Cn,
and
(BAx, x)A > 0,
∀x ∈Cn
⋆.
Proof. See Problem 7.3.
Proposition 7.6 (similarity). Suppose that A ∈Cn×n is HPD and B ∈Cn×n is
self-adjoint with respect to (·, ·)A, i.e.,
(x, By)A = (Bx, y)A ,
∀x, y ∈Cn.
Then B is similar to a matrix that is Hermitian, i.e., a matrix that is self-adjoint
with respect to the Euclidean inner product ( · , · )2.

160
Variational and Krylov Subspace Methods
Proof. Since B ∈Cn×n is self-adjoint with respect to (·, ·)A, then the reader should
conﬁrm that
BHA = AB.
Since A is HPD, there is an invertible matrix L ∈Cn×n such that A = LLH. Deﬁne
C = LHBL−H.
Then
CH = L−1BHL
= L−1BHLLHL−H
= L−1BHAL−H
= L−1ABL−H
= L−1LLHBL−H
= LHBL−H
= C.
Since C is similar to B, the result follows.
Knowing that the spectral decomposition, Theorem 1.47, is valid in the Euclidean
case, the last result can be used to prove the validity of the more general version,
Theorem 1.49.
Theorem 7.7 (spectral decomposition). Suppose that A ∈Cn×n is HPD and B ∈
Cn×n is self-adjoint with respect to ( · , · )A. Then all of the eigenvalues of B are
real and there is an A-orthonormal basis of Cn consisting of eigenvectors of B.
Proof. Applying the Euclidean spectral decomposition, Theorem 1.47, to the
Hermitian matrix
C = LHBL−H,
where the invertible matrix L is taken from the Cholesky-type decomposition A =
LLH of A, there are a unitary matrix U ∈Cn×n and a diagonal matrix with real
entries D = diag(λ1, . . . , λn) such that
LHBL−H = C = UDU−1.
Hence, B is similar to a diagonal matrix with real entries:
B =
 L−HU

D
 L−HU
−1.
Moreover, setting M = L−HU, we see that
BM = MD,
which implies that the columns of the invertible matrix M are eigenvectors of B. It
only remains to check that the columns of M form an A-orthonormal set. This is
left to the reader as an exercise; see Problem 7.5.

7.2 Gradient Descent Methods
161
Corollary 7.8 (eigenvalues). Suppose that A ∈Cn×n is HPD and B ∈Cn×n is
self-adjoint and positive deﬁnite with respect to ( · , · )A. Then the eigenvalues of
B are all real and positive.
Proof. See Problem 7.6.
7.2
Gradient Descent Methods
Gradient descent methods are related to optimization methods. The basic idea
is to solve, successively, several one-dimensional optimization problems called line
searches.
Deﬁnition 7.9 (gradient descent). Suppose that A ∈Cn×n is HPD and f ∈Cn.
Deﬁne the quadratic function EA : Cn →R via
EA(z) = 1
2zHAz −ℜ
 zHf

.
A gradient descent method is a two-layer iterative scheme to approximate x =
A−1f . Starting from an arbitrary initial guess x0, the iterations proceed as
xk = xk−1 + αkd k−1,
k = 1, 2, 3, . . . ,
where d k−1 ∈Cn is the (k −1)st search direction, supplied by the algorithm, and
αk ∈C is the step size given by the condition
αk = argmin
α∈C
EA (xk−1 + αd k−1),
which is called a line search.
Before attempting the more general case, let us ﬁrst compute a formula for αk
in the case where real numbers are used exclusively.
Theorem 7.10 (gradient descent, real case). Suppose that A ∈Rn×n is symmetric
positive deﬁnite (SPD) and f ∈Rn. Deﬁne the quadratic function EA : Rn →R
EA(z) = 1
2z⊺Az −z⊺f .
Suppose that the search direction d k−1 ∈Rn
⋆and previous iterate xk−1 ∈Rn in a
gradient descent method are given. Deﬁne r k−1 = f −Axk−1. Then the step size
can be computed exactly via the formula
αk = argmin
α∈R
EA (xk−1 + αd k−1) =
d ⊺
k−1r k−1
d ⊺
k−1Ad k−1
.
In other words, a gradient descent method is well deﬁned once a nontrivial search
direction is speciﬁed.
Proof. Consider the quadratic
g(α) = EA (xk−1 + αd k−1) = EA (xk−1) −αd ⊺
k−1r k−1 + α2 1
2d ⊺
k−1Ad k−1.

162
Variational and Krylov Subspace Methods
A calculation with the ﬁrst derivative locates the extremum:
0 = g′(αk) = −d ⊺
k−1r k−1 + αkd ⊺
k−1Ad k−1,
which implies that
αk =
d ⊺
k−1r k−1
d ⊺
k−1Ad k−1
.
The second derivative indicates that this is a minimum:
g′′(α) = d ⊺
k−1Ad k−1 > 0,
provided that d k−1 ̸= 0.
The more general case in the complex setting requires a bit more care.
Theorem 7.11 (gradient descent, complex case). Suppose that A ∈Cn×n is HPD
and f ∈Cn. Deﬁne the quadratic function EA : Cn →R
EA(z) = 1
2zHAz −ℜ
 zHf

.
Suppose that the search direction d k−1 ∈Cn
⋆and previous iterate xk−1 ∈Cn in a
gradient descent method are given. Deﬁne r k−1 = f −Axk−1. Then the step size
can be computed exactly via the formula
αk = argmin
α∈C
EA (xk−1 + αd k−1) =
d H
k−1r k−1
d H
k−1Ad k−1
.
Proof. Let α = s + it, where s, t ∈R. Deﬁne the function
g(s, t) = EA (xk−1 + αd k−1)
= EA (xk−1) −ℜ
 ¯αd H
k−1r k−1

+ |α|2
2 d H
k−1Ad k−1
= EA (xk−1) −sℜ
 d H
k−1r k−1

−tℑ
 d H
k−1r k−1

+ s2 + t2
2
d H
k−1Ad k−1.
This is a strictly convex quadratic function of two variables. Setting the ﬁrst
derivatives equal to zero, we ﬁnd
0 = ∂g
∂s (sk, tk) = −ℜ
 d H
k−1r k−1

+ skd H
k−1Ad k−1,
0 = ∂g
∂t (sk, tk) = −ℑ
 d H
k−1r k−1

+ tkd H
k−1Ad k−1,
which implies that
αk = sk + itk =
d H
k−1r k−1
d H
k−1Ad k−1
.

7.3 The Steepest Descent Method
163
7.3
The Steepest Descent Method
Deﬁnition 7.12 (steepest descent). Suppose that A ∈Cn×n is HPD and f ∈Cn.
The steepest descent method is a gradient descent method for which the search
direction d k−1 is deﬁned to be the residual, i.e.,
d k−1 = r k−1 = f −Axk−1,
so that the step size is precisely
αk = r H
k−1r k−1
r H
k−1Ar k−1
.
If B ∈Cn×n is an HPD matrix, the B-preconditioned steepest descent method
is a gradient descent method with search direction
d k−1 = B−1r k−1,
(7.2)
so that the step size is precisely
αk =
r H
k−1B−1r k−1
r H
k−1B−1AB−1r k−1
.
Remark 7.13 (preconditioning). The idea with preconditioning is that the precon-
ditioner, B, should be like A, but easier to invert. In fact, if we choose B = A, which
is not practical, we would converge in a single iteration, because (7.2) would yield
the error vector as the search direction. One way of realizing the preconditioned
steepest descent, theoretically, is to observe that it is just the normal steepest
descent applied to solve the equation
B−1Ax = B−1f .
(7.3)
Proposition 7.14 (orthogonality). Suppose that A ∈Cn×n is HPD and f ∈Cn.
Suppose that {xk}∞
k=1 is computed using the steepest descent method with the
starting vector x0. Then the sequence of residual vectors {r k}∞
k=1, r k = f −Axk,
has the property that
(r k, r k+1)2 = r H
k+1r k = 0
for k = 0, 1, 2, . . ..
Proof. See Problem 7.9.
Remark 7.15 (orthogonality). Proposition 7.14 shows that the steepest descent
method has the property that its next search direction is always orthogonal to the
last. We will see that this can lead to some bad outcomes.
Theorem 7.16 (error equation). Suppose that A ∈Cn×n is HPD, f ∈Cn, and
x = A−1f . Suppose that {xk}∞
k=1 is computed using the steepest descent method
with the starting value x0 ∈Cn. Then the error ek = x −xk satisﬁes
∥ek+1∥2
A = γk ∥ek∥2
A ,
where
γk = 1 −
 r H
k r k
2
 r H
k Ar k
  r H
k A−1r k
.

164
Variational and Krylov Subspace Methods
Proof. Suppose that x = A−1f . Then, for any z ∈Cn,
EA(z) = EA(x) + 1
2 ∥z −x∥2
A ,
(7.4)
where, as usual,
EA(z) = 1
2zHAz −ℜ
 zHf

.
Since r k = f −Axk and, for the standard steepest descent method,
αk+1 = r H
k r k
r H
k Ar k
,
a brief calculation shows that
EA(xk+1) = EA(xk + αk+1r k) = EA(xk) −1
2
 r H
k r k
2
r H
k Ar k
.
(7.5)
Combining (7.4) and (7.5), we get
∥ek+1∥2
A = ∥ek∥2
A −
 r H
k r k
2
r H
k Ar k
.
(7.6)
Since r k = Aek, we have
∥ek∥2
A = r H
k A−1r k.
(7.7)
Combining (7.6) and(7.7), we get the desired result.
The following technical lemma allows us to make some sense of the error equation
derived in Theorem 7.16.
Lemma 7.17 (Kantorovich inequality1). Let the matrix A ∈Cn×n be HPD with
spectrum σ(A) = {λi}n
i=1, with 0 < λ1 ≤λ2 ≤· · · ≤λn, and spectral condition
number
κ = κ2(A) = λn
λ1
.
Then, for any x ∈Cn
⋆,
 xHAx
  xHA−1x

(xHx)2
≤1
4
√κ +
√
κ−1
2
.
Proof. Deﬁne µ = √λ1λn. Then
κ−1/2 ≤λi
µ ≤κ1/2
(7.8)
and
κ−1/2 ≤µ
λi
≤κ1/2.
Therefore, for all i = 1, . . . , n,
2κ−1/2 ≤λi
µ + µ
λi
≤2κ1/2.
1 Named in honor of the Russian mathematician Leonid Vitalyevich Kantorovich (1912–1986).

7.3 The Steepest Descent Method
165
Next, observe that the function
f (x) = x + 1
x
is strictly decreasing on (0, 1) and strictly increasing on (1, ∞). Set x = λi
µ . Using
(7.8), if
1 ≤x = λi
µ ≤κ1/2,
then
2 = f (1) ≤f
λi
µ

≤f (κ1/2).
This implies that
2 ≤λi
µ + µ
λi
≤κ1/2 + κ−1/2.
On the other hand, if
κ−1/2 ≤x = λi
µ ≤1,
then
f (κ−1/2) ≥f
λi
µ

≥f (1) = 2,
which implies that
κ−1/2 + κ1/2 ≥λi
µ + µ
λi
≥2.
Therefore, it is always true that
2 ≤λi
µ + µ
λi
≤κ1/2 + κ−1/2.
(7.9)
Now suppose that (λi, w i) is an eigenpair of A, where {w}n
i=1 is an orthonormal
basis for Cn. Then
 µ−1A + µA−1
w i =
λi
µ + µ
λi

w i.
Let x ∈Cn
⋆be arbitrary. There exist unique constants ci ∈C such that
x =
n
X
i=1
ciw i.
Then
1
µxHAx + µxHA−1x =
n
X
i=1
|ci|2
λi
µ + µ
λi

≤

κ1/2 + κ−1/2
∥x∥2
2 .
We recall the standard inequality
|ab| ≤1
2|a|2 + 1
2|b|2

166
Variational and Krylov Subspace Methods
for any a, b ∈R. From this, it follows that
ab ≤|ab| ≤1
4 (|a| + |b|)2 .
Using the last inequality with
a = 1
µxHAx,
b = µxHA−1x,
we obtain
 xHAx
  xHA−1x

≤1
4
 1
µxHAx + µxHA−1x
2
≤1
4

κ1/2 + κ−1/22  xHx
2
and the proof is complete.
Theorem 7.18 (convergence). Suppose that A ∈Cn×n is HPD, f ∈Cn, and
x = A−1f . Suppose that {xk}∞
k=1 is computed using the steepest descent method
with the starting value x0 ∈Cn. Then the following estimate holds:
∥ek∥A ≤
κ −1
κ + 1
k
∥e0∥A ,
where κ = κ2(A).
Proof. Using the Kantorovich inequality from Lemma 7.17 and the result of
Theorem 7.16,
γk ≤1 −
4
 κ−1/2 + κ1/22 =
κ −1
κ + 1
2
,
which implies the result.
Example 7.1
Let us show, by means of an example, that the convergence rate
for the steepest descent method, presented in the previous result, is sharp. Let
A = diag(a, b) ∈R2×2, a > b > 0, f = 0, and x = 0. We will prove the following: if
xi = ci [b, sa]⊺∈V1 = ⟨[b, sa]⊺⟩
for some ci ∈R⋆, where s = ±1, then
xi+1 = ci+1 [b, −sa]⊺∈V2 = ⟨[b, −sa]⊺⟩
for some coeﬃcient ci+1 ∈R⋆. In general, xk ∈V1 ∪V2. First r i = −ci [ab, sab]⊺
and Ar i = −ci

a2b, sab2⊺. Then
αi+1 = r ⊺
i r i
r ⊺
i Ar i
=
2c2
i a2b2
c2
i (a3b2 + a2b3) =
2
a + b .

7.3 The Steepest Descent Method
167
Finally,
xi+1 = xi + αir i
=
"
cib −2ci
a+bab
cisa −2ci
a+bsab
#
=
ci
a + b
"
b(a + b) −2ab
sa(a + b) −2sab
#
=
ci
a + b
 b2 −ab
sa2 −sab

= ci(b −a)
a + b
 b
−sa

.
Hence,
ci+1 = ci(b −a)
a + b
= −ci
κ −1
κ + 1,
where κ =
a
b is precisely the 2-norm condition number of the matrix A. From
this, one can see that the convergence rate is the worst possible, according to our
convergence theory, because
ck = (−1)kc0
κ −1
κ + 1
k
.
Remark 7.19 (ill-conditioning). For the steepest descent method, with large
spectral condition number κ, we observe that
κ −1
κ + 1 ≈1 −2
κ.
In other words, the convergence rate deteriorates as κ →∞, i.e., as the system
matrix becomes increasingly ill-conditioned. We will see, when we study numerical
methods for certain types of diﬀerential equations, how a family of matrices can
become more and more ill-conditioned as the size of the matrices increases.
Let us now study the convergence of the B-preconditioned steepest descent
method deﬁned in (7.2).
Theorem 7.20 (error equation). Suppose that A, B ∈Cn×n are HPD, f ∈Cn, and
x = A−1f . Suppose that {xk}∞
k=1 is computed using the B-preconditioned steepest
descent method with the starting value x0 ∈Cn. Then the error ek = x −xk
satisﬁes
∥ek+1∥2
A = βk ∥ek∥2
A ,
where
βk = 1 −
 d H
k r k
2
 d H
k Ad k
  r H
k A−1r k


168
Variational and Krylov Subspace Methods
and
r k = Bd k.
Setting B = LHL, for some invertible matrix L ∈Cn×n,
gk = Ld k,
C = L−HAL−1,
the error equation may be expressed using
βk = 1 −
 gH
k gk
2
 gH
k Cgk
  gH
k C−1gk
.
Proof. See Problem 7.10.
Using the Kantorovich inequality, we get the the following error estimate for the
preconditioned steepest descent method.
Theorem 7.21 (convergence). Suppose that A, B ∈Cn×n are HPD, f ∈Cn, and
x = A−1f . Suppose that {xk}∞
k=1 is computed using the B-preconditioned steepest
descent method with the starting value x0 ∈Cn. Suppose that B has the Cholesky-
type factorization B = LHL, where L ∈Cn×n is invertible. Deﬁne C = L−HAL−1.
Then the following error estimate holds:
∥ek∥A ≤
κC −1
κC + 1
k
∥e0∥A ,
where
κC = κ2(C) = µn
µ1
,
σ(C) = {µi}n
i=1 ,
0 < µ1 ≤· · · ≤µn.
Proof. Observe that C = L−HAL−1 is HPD. The result now follows by applying
the Kantorovich inequality to estimate the size βk, the contraction factor from
Theorem 7.20.
Remark 7.22 (generalized condition number). Consider the preconditioned system
(7.3) and observe that B−1A is self-adjoint and positive deﬁnite with respect to
( · , · )A, since B−1 is HPD. It, therefore, has positive real eigenvalues. Furthermore,
one will ﬁnd that B−1A is similar to C = L−HAL−1. In particular,
L
 B−1A

L−1 = L−HAL−1 = C.
Therefore, the eigenvalues of C and the preconditioned coeﬃcient matrix B−1A
are the same. Therefore, we will often write the result of the last theorem in the
following way:
∥ek∥A ≤
κ −1
κ + 1
k
∥e0∥A ,
where
κ = κB−1A = µn
µ1
,
σ(B−1A) = {µi}n
i=1 ,
0 < µ1 ≤· · · ≤µn.
The number κB−1A ≥1 is called the condition number of the preconditioned
coeﬃcient matrix B−1A.

7.4 The Conjugate Gradient Method
169
Remark 7.23 (eﬀect of preconditioning). The result of this last theorem is quite
important. It shows that, if we select the preconditioner B in a careful way, it is
possible to dramatically improve the convergence rate. In particular, it is possible,
theoretically, to ﬁnd B such that κC is nearly one. The closer κC is to one, the
faster is the convergence rate.
7.4
The Conjugate Gradient Method
We have arrived at the so-called conjugate gradient (CG) method. Let A ∈Cn×n
be HPD and f ∈Cn. Recall that we are interested in ﬁnding a solution to
Ax = f ,
or, equivalently,
x = argmin
z∈Cn
EA(z),
EA(z) = 1
2zHAz −ℜ
 zHf

.
To solve Ax = f , we will, instead, try to solve the minimization problem in a clever
way; namely, by doing it over an increasing sequence of subspaces of Cn.
Deﬁnition 7.24 (Krylov subspace2). Given A ∈Cn×n and 0 ̸= q ∈Cn, the Krylov
subspace of degree m is
Km(A, q) = span

Akq
 k = 0, . . . , m −1
	
.
Many students, when they learn the CG method for the ﬁrst time, come away
with the notion that the method is quite complicated. On the contrary, the idea is
quite simple and it is easy to remember. Here, it is in its most natural form.
Deﬁnition 7.25 (CG). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆, and x = A−1f .
The zero-start conjugate gradient method is an iterative scheme for producing a
sequence of approximations {xk}∞
k=1 from the starting point x0 = 0 ∈Cn according
to the following prescription: setting Kk = Kk(A, f ), the kth iterate is obtained by
xk = argmin
z∈Kk
EA(z).
(7.10)
Notice that, by construction, Km(A, q) ⊆Km+1(A, q). Thus, we are minimizing
over a nondecreasing family of nested subspaces of Cn.
Remark 7.26 (nonzero starting vector). Later on, we will mention the important
case of starting the CG method with a nonzero starting vector x0. This is a very
important issue, since, in many cases, the user may have some insight on how to
start the CG method so as to obtain faster convergence.
2 Named in honor of the Russian mathematician Aleksei Nikolaevich Krylov (1863–1945).

170
Variational and Krylov Subspace Methods
Deﬁnition 7.27 (Galerkin approximation3). Suppose that A ∈Cn×n is HPD, f ∈
Cn, x = A−1f , and W is a subspace of Cn. The vector xW ∈W is called the
Galerkin approximation of x in W if and only if
(AxW , w)2 = (f , w)2,
∀w ∈W.
(7.11)
Theorem 7.28 (existence and uniqueness). Suppose that A ∈Cn×n is HPD, f ∈
Cn, x = A−1f , and W is a subspace of Cn. The Galerkin approximation xW ∈W
exists and is unique.
Proof. Let B = {w 1, . . . , w k} be an A-orthonormal basis for W, i.e.,
(w i, w j)A = (Aw i, w j)2 = δi,j
for all 1 ≤i, j ≤k ≤n. Then (7.11) holds if and only if
(AxW , w i)2 = (f , w i)2,
i = 1, . . . , k.
(7.12)
Since B is a basis, there are unique constants c1, . . . , ck ∈C such that xW =
Pk
j=1 cjw j. Plugging this into (7.12) we get the trivial diagonal system
ci =
k
X
j=1
cj(w j, w i)A = (f , w i)2,
i = 1, . . . , k.
(7.13)
This proves existence and uniqueness.
Galerkin approximations have important properties that we will ﬁnd useful.
Proposition 7.29 (properties of Galerkin approximations). Suppose that A ∈Cn×n
is HPD, f ∈Cn, x = A−1f , W is a subspace of Cn, and xW is the Galerkin
approximation to x in W.
1. The residual is orthogonal to W. That is, if r = f −AxW , we have
(r, w)2 = 0,
∀w ∈W.
2. Galerkin orthogonality: Deﬁne the error e = x −xW . Then we have
(Ae, w)2 = 0,
∀w ∈W.
3. Optimality:
(Ae, e)2 ≤(A(x −w), x −w)2,
∀w ∈W.
Proof. We only prove optimality, leaving the ﬁrst two for the reader; see Problem
7.11. Let w ∈W be arbitrary. Using Galerkin orthogonality, and the Cauchy–
Schwarz inequality for the A-norm,
3 Named in honor of the Russian mathematician Boris Grigorievich Galerkin (1871–1945).

7.4 The Conjugate Gradient Method
171
∥e∥2
A = (Ae, e)2
= (Ae, x −xW )2
= (Ae, x −xW )2 + (Ae, xW −w)2
= (Ae, x −xW + xW −w)2
= (Ae, x −w)2
≤∥e∥A ∥x −w∥A .
If ∥e∥A = 0, the result is trivial. If ∥e∥A > 0,
∥e∥A ≤∥x −w∥A ,
as we claimed.
Theorem 7.30 (characterization of Galerkin approximations). Suppose that A ∈
Cn×n is HPD, f ∈Cn, and W is a subspace of Cn. Then the following are equivalent.
1. The vector xW ∈W is a minimizer of EA over W:
xW = argmin
z∈W
EA(z).
2. The vector xW ∈W is a Galerkin approximation of x = A−1f :
(AxW , w)2 = (f , w)2,
∀w ∈W.
Proof. See Problem 7.12.
With these deﬁnitions at hand we can actually show that the CG method will
converge in a ﬁnite number of steps.
Theorem 7.31 (convergence). Let A be HPD, f ∈Cn
⋆, and x = A−1f . We have
that dim Km(A, f ) = m and, as a consequence, the sequence {xk}∞
k=1, generated
by the zero-start CG method, is such that there is an integer m⋆∈{1, . . . , n} for
which
xk ̸= x,
k = 1, . . . , m⋆−1,
xk = x,
k ≥m⋆.
Proof. Let Km = Km(A, f ). Notice that dim Km ≤m, so that, if we show that
equality actually holds, all the statements will follow.
We will proceed by induction. Set m = 1 and notice that, since f ̸= 0,
K1 = span{f }
=⇒
dim K1 = 1.
Assume now that, for all m = 1, . . . , k with k < n −1, we have dim Kk = k and
xk ̸= x. Therefore, the residual r k = f −Axk ̸= 0 and r k ∈Kk+1.
Notice that, using the characterization of Galerkin approximations given in
Theorem 7.30, we have that xk ∈Kk is the Galerkin approximation of x in Kk.
Thus, the residual r k must be orthogonal to Kk, i.e.,
(r k, w)2 = 0,
∀w ∈Kk.

172
Variational and Krylov Subspace Methods
In other words, we have shown that 0 ̸= r k ∈Kk+1\Kk. This is only possible if
dim Kk+1 > dim Kk. In other words, dim Kk+1 = k + 1 and the result follows.
While the previous result shows that CG obtains the exact solution in at most n
steps, we will consider this as an iterative scheme and study its properties as such.
The reason for this is twofold. First, we want to consider the case when n is
very large; therefore, we may wish to stop the iterations before the exact solution
is found. Second, while in theory the exact solution can be obtained with CG,
experience shows that rounding errors make this not possible.
We will begin then by rephrasing the CG method in an equivalent form, which is
more convenient for practical computations. Indeed, as stated in Deﬁnition 7.25,
it seems that at step k we need to store k vectors — a basis of Kk(A, f ) — as
we need to minimize over it. The following equivalent formulation shows that this
is not necessary.
Theorem 7.32 (equivalence). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆, and
x = A−1f . The sequence generated by the zero-start CG method, {xk}m⋆
k=1, is
the same sequence as that generated by the following recursive algorithm: given
x0 = 0, deﬁne r 0 = f −Ax0 = f and p0 = r 0 = f . For k = 0, . . . , m⋆−1,
compute:
1. Update the iterate:
xk+1 = xk + λk+1pk,
λk+1 = (r k, pk)2
(Apk, pk)2
.
2. Update the residual:
r k+1 = r k −λk+1Apk.
3. Update the search direction:
pk+1 = r k+1 −µk+1pk,
µk+1 = (Ar k+1, pk)2
(Apk, pk)2
.
4. If k = m⋆−1, stop. Otherwise, index k and go to step 1.
Clearly,
r m⋆= 0 = pm⋆,
but it is guaranteed that
0 ̸= r k ∈Kk+1\Kk,
0 ̸= pk ∈Kk+1\Kk,
k = 0, . . . , m⋆−1,
and the following orthogonalities hold:
(r j, r i)2 =
 pj, pi

A = 0
for all 0 ≤j < i ≤m⋆−1.
Proof. The proof is by induction on k, terminating at step m⋆−1.
We leave the reader to check that the base cases, k ≤2, are true; see Problem
7.13.

7.4 The Conjugate Gradient Method
173
For the induction hypothesis we assume that the formulas and properties above
are true for 0 ≤k ≤m −1.
Now let k ≤m ≤m⋆−1. Let us assume that {xk}m+1
k=1 is generated by the
zero-start CG algorithm. We know that xm ̸= x and r m = f −Axm ̸= 0. The
vector xm+1 ∈Km+1 is obtained by solving the optimization problem (7.10) over
the Krylov space Km+1. Notice that
r m = f −Axm ∈Km+1 = Km+1(A, f ) = span

Akf
 k = 0, . . . , m
	
,
because, by assumption, xm ∈Km. Since xm is a Galerkin approximation to x, by
Galerkin orthogonality, we observe that for all y ∈Km,
0 = (Aem, y)2 = (r m, y)2.
In other words,
r m ∈Km+1\Km.
The induction hypothesis guarantees that {r 0, . . . , r m−1} is an orthogonal set and
a basis for Km. Galerkin orthogonality implies that the vector r m is orthogonal to
{r 0, . . . , r m−1}; therefore, {r 0, . . . , r m−1, r m} is an orthogonal basis for Km+1.
Let K⊥A
m
denote the orthogonal complement of Km in Km+1 in the A-inner
product, i.e.,
K⊥A
m = {w ∈Km+1 | (Aw, y)2 = 0, ∀y ∈Km} .
It follows that K⊥A
m
̸= {0}; moreover, it is not diﬃcult to see that K⊥A
m
is one
dimensional. Since
Km+1 = Km
⊥⊕K⊥A
m ,
any element ξm+1 ∈Km+1 can be written as
ξm+1 = ξm + µpm
for some µ ∈C, pm ∈K⊥A
m ∩Cn
⋆, and ξm ∈Km. Now consider the element
w = xm + λm+1pm,
λm+1 = (r m, pm)2
(Apm, pm)2
.
Then
(Aw −f , ξm+1)2 = (A(xm + λm+1pm) −f , ξm + µpm)2
= (Axm −f , ξm)2 + ¯µ (Axm −f , pm)2 + λm+1(Apm, ξm)2
+ ¯µλm+1(Apm, pm)2
= (Axm −f , ξm)2 −¯µ [(r m, pm)2 −λm+1(Apm, pm)2]
= −¯µ [(r m, pm)2 −λm+1(Apm, pm)2]
= 0,
where we used that (Apm, ξm)2 = 0 for every ξm ∈Km, the fact that xm minimizes
over Km and so (Axm −f , ξm)2 = 0 for every ξm ∈Km, and, ﬁnally, the deﬁnition
of λm+1. Therefore,
(Aw −f , ξm+1)2 = 0,
∀ξm+1 ∈Km+1.

174
Variational and Krylov Subspace Methods
But xm+1 ∈Km+1, the Galerkin approximation, is the unique element that has
the property exhibited in the last equation. Therefore, w = xm+1. In other words,
xm+1 ∈Km+1 is the Galerkin approximation deﬁning the (m + 1)st iterate in the
zero-start CG algorithm if and only if
xm+1 = xm + λm+1pm,
λm+1 = (r m, pm)2
(Apm, pm)2
.
Furthermore, a simple computation involving the last equation then yields the
(m + 1)st residual:
r m+1 = r m −λm+1Apm.
By the induction hypothesis, {p0, . . . , pm−1} is an A-orthogonal set; hence, it
forms a basis for Km. Since r m has a component in K⊥A
m , its A-orthogonal projection,
q, into K⊥A
m is nonzero:
q = r m −
m−1
X
i=0
(Ar m, pi)2
(Api, pi)2
pi = r m −
m−1
X
i=0
(r m, pi)A
(pi, pi)A
pi.
But if 0 ≤i ≤m −2, then Api ∈Km and so (Api, r m)2 = 0. We are thus left with
q = r m −
(Ar m, pm−1)2
(Apm−1, pm−1)2
pm−1 ∈K⊥A
m .
Finally, pm ∈K⊥A
m
is not yet completely determined. We are free to pick it to
suit our purposes. We take
pm = q = r m −
(Ar m, pm−1)2
(Apm−1, pm−1)2
pm−1 ∈K⊥A
m .
It follows that {p0, . . . , pm−1, pm} is an A-orthogonal set.
Corollary 7.33 (iterates). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆, and x = A−1f .
The sequence generated by the zero-start CG method, {xk}m⋆
k=1, has the following
property: for all k ∈{1, . . . , m⋆},
xk ∈Kk\Kk−1,
which implies that
⟨x1, . . . , xk⟩= Kk.
Proof. See Problem 7.14.
Corollary 7.34 (spanning properties). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆, and
x = A−1f . If the zero-start CG algorithm is employed to produce the approximation
sequence {xj}m⋆
j=1, then, for all 1 ≤i ≤m⋆,
Ki(A, f ) =

f , Af , . . . , Ai−1f

= ⟨x1, . . . , xi⟩= ⟨p0, . . . , pi−1⟩= ⟨r 0, . . . , r i−1⟩.
Proof. See Problem 7.15.
A slightly more computationally eﬃcient, but entirely equivalent, version of the
CG algorithm is possible.

7.4 The Conjugate Gradient Method
175
Corollary 7.35 (equivalent formulation). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆,
and x = A−1f . The sequence generated by the zero-start CG method, {xk}m⋆
k=1,
is the same sequence as that generated by the following recursive algorithm: given
x0 = 0, deﬁne r 0 = f −Ax0 = f and p0 = r 0 = f . For 0 ≤k ≤m⋆−1, compute:
1. Update the iterate:
xk+1 = xk + λk+1pk,
λk+1 =
(r k, r k)2
(Apk, pk)2
.
2. Update the residual:
r k+1 = r k −λk+1Apk.
3. Update the search direction:
pk+1 = r k+1 + βk+1pk,
βk+1 = (r k+1, r k+1)2
(r k, r k)2
.
4. If k = m⋆−1, stop. Otherwise, index k and go to step 1.
It follows that
r m⋆= 0 = pm⋆,
but it is guaranteed that
0 ̸= r k ∈Kk+1\Kk,
0 ̸= pk ∈Kk+1\Kk,
k = 0, . . . , m⋆−1.
The following orthogonalities hold:
(r j, r i)2 =
 pj, pi

A = 0
for all 0 ≤j < i ≤m⋆−1.
Proof. See Problem 7.16.
We have deﬁned the CG method via minimization over certain subspaces of
increasing size. But we could alternately deﬁne the algorithm via one of the recursive
algorithms above. In this case, we could derive the minimization property as a
consequence. In other words, there are a few equivalent ways of deﬁning the method
of CGs.
Theorem 7.36 (minimization). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆is given,
and x = A−1f . Let, for some m ∈{1, . . . , n}, {xi}m
i=0 denote any sequence of
vectors with x0 = 0 — with associated residual vectors r j = f −Axj — that
satisﬁes
Kj = Kj(A, f ) = ⟨f , Af , . . . , Aj−1f ⟩= ⟨x1, . . . , xj⟩= ⟨r 0, . . . , r j−1⟩,
r j−1 ̸= 0
for all j = 1, . . . , n, with the orthogonality relations
r H
k r i = 0

176
Variational and Krylov Subspace Methods
for all 0 ≤k < i ≤m. Then the jth iterate xj is the unique vector in Kj that min-
imizes the error function φ(y) = ∥x −y∥A. Furthermore, φ is monotonically
decreasing:
∥ej∥A = ∥x −xj∥A = φ(xj) ≤φ(xj−1) = ∥x −xj−1∥A = ∥ej−1∥A .
Proof. Let z ∈Kj be arbitrary. Deﬁne w = xj −z ∈Kj. Then
φ2(z) = ∥x −z∥2
A
= ∥x −xj + xj −z∥2
A
= ∥ej + w∥2
A
= (ej + w)HA(ej + w)
= ∥ej∥2
A + 2w HAej + ∥w∥2
A
= ∥ej∥2
A + 2w Hr j + ∥w∥2
A.
Since w ∈Kj = ⟨r 0, . . . , r j−1⟩, there exist unique α0, . . . , αj−1 ∈C such that
w =
j−1
X
i=0
= αir i.
Using the orthogonality r i Hr j = 0 for all 0 ≤i < j, it is clear that w Hr j = 0.
Hence,
φ2(z) = ∥ej∥2
A + ∥w∥2
A ≥∥ej∥2
A,
with equality in the last relation if and only if w = 0, or, equivalently, if and only if
z = xj. Hence, xj is the unique minimizer of φ over Kj.
Since we have the space containment Kj−1(A, f ) ⊆Kj(A, f ), we must have
∥ej∥A = φ(xj)
= inf {φ(z) | z ∈Kj(A, f )}
≤inf {φ(z) | z ∈Kj−1(A, f )}
= φ(xj−1)
= ∥ej−1∥A .
The convergence of the CG method is given in the following few results.
Theorem 7.37 (polynomial bound). Suppose that the zero-start CG algorithm is
applied to solve Ax = f , where A ∈Cn×n is HPD and f ∈Cn
⋆. Then, if the iteration
has not already converged (r i−1 ̸= 0), there is a unique polynomial
pi ∈Pi,⋆= {p ∈Pi | p(0) = 1}
that minimizes ∥p(A)e0∥A over all p ∈Pi,⋆. The iterate xi has the error ei =
pi(A)e0 and, consequently,
∥ei∥A
∥e0∥A
≤inf
p∈Pi,⋆max
λ∈σ(A) |p(λ)| .

7.4 The Conjugate Gradient Method
177
Proof. Recall that
EA(z) = 1
2 ∥z −x∥2
A −1
2f HA−1f ,
where x = A−1f is the exact solution. Therefore, by deﬁnition of the zero-start
CG algorithm, we have
xi = argmin
z∈Ki
∥x −z∥A ,
min
z∈Ki ∥x −z∥A = ∥ei∥A
and xi ∈Ki is uniquely determined. Now observe that e0 = x, since x0 = 0, and,
consequently, r 0 = f . Thus, for any z ∈Ki, there are constants cj ∈C, 1 ≤j ≤i,
such that z = Pi
j=1(−cj)Aj−1f . Consequently,
x −z = x +
iX
j=1
cjAj−1f = e0 +
iX
j=1
cjAj−1r 0 = e0 +
iX
j=1
cjAje0 = p(A)e0,
where p(t) = 1 + Pi
j=1 cjtj ∈Pi,⋆. It follows, then, that the minimization problem
above is equivalent to
pi = argmin
p∈Pi,⋆
∥p(A)e0∥A ,
min
p∈Pi,⋆∥p(A)e0∥A = ∥ei∥A
and pi ∈Pi,⋆is, of course, uniquely determined. It, therefore, follows that
∥ei∥A = inf
p∈Pi,⋆∥p(A)e0∥A ≤inf
p∈Pi,⋆∥p(A)∥A ∥e0∥A
and, consequently,
∥ei∥A
∥e0∥A
≤inf
p∈Pi,⋆∥p(A)∥A .
(7.14)
To ﬁnish up, suppose that z ∈Cn
⋆. Let {w 1, . . . , w n} be an orthonormal basis
of eigenvectors of A. Set σ(A) = {λ1, . . . , λn} ⊂(0, ∞), with Aw j = λjw j for
j = 1, . . . , n. There exist constants αj ∈C, j = 1, . . . , n such that z = Pn
j=1 αjw j
and
∥z∥2
A = zHAz =
n
X
j=1
|αj|2λj.
Furthermore,
∥p(A)z∥2
A = zHp(A)HAp(A)z =
n
X
j=1
|αj|2λj|p(λj)|2.
As a consequence,
∥p(A)z∥2
A
∥z∥2
A
=
Pn
j=1 |αj|2λj|p(λj)|2
Pn
j=1 |αj|2λj
≤max
λ∈σ(A) |p(λ)|2,
which implies that
∥p(A)∥A ≤max
λ∈σ(A) |p(λ)| .

178
Variational and Krylov Subspace Methods
Putting this estimate together with that in (7.14), we have
∥ei∥A
∥e0∥A
≤inf
p∈Pi,⋆max
λ∈σ(A) |p(λ)| ,
which is the desired result.
Theorem 7.38 (convergence). Suppose that the zero-start CG algorithm is applied
to solve Ax = f , where A ∈Cn×n is HPD and f ∈Cn
⋆. If A has only k distinct
eigenvalues, k < n, then the algorithm converges in at most k steps.
Proof. Let σ(A) = {λj}k
j=1 denote the set of k distinct eigenvalues of A. From the
last theorem, for i = 1, . . . , k,
∥ei∥A
∥e0∥A
≤max
λ∈σ(A) |qi(λ)|
for any polynomial qi of degree at most i with the property that qi(0) = 1. Let us
deﬁne, for any i = 1, . . . , k,
qi(x) =
iY
j=1

1 −x
λj

.
Clearly, qi(0) = 1 and for all j = 1, . . . , i,
qi(λj) = 0.
But, on the other hand, if j = i + 1, . . . , k,
qi(λj) ̸= 0.
Now, once i = k,
qi(λ) = 0,
∀λ ∈σ(A).
Thus, ∥ek∥A = 0. Of course, convergence could happen at an earlier stage if, by
chance, x ∈Ki for some i < k.
Theorem 7.39 (convergence of CG). Let A ∈Cn×n be HPD and f ∈Cn
⋆. The
error for the zero-start CG method satisﬁes
∥ek∥A ≤2
 p
κ2(A) −1
p
κ2(A) + 1
!k
∥e0∥A.
Proof. Suppose that σ(A) = {λ1, . . . , λn}, 0 < λ1 ≤· · · ≤λn. From Theorem
7.37, it follows that
∥ek∥A ≤
max
λ∈[λ1,λn] |qk(λ)|∥e0∥A
for any polynomial qk of degree at most k such that qk(0) = 1. Since this
polynomial is arbitrary, we may choose it to minimize the right-hand side of this
expression. It turns out, as in the proof of Theorem 6.29, that shifted and rescaled

7.4 The Conjugate Gradient Method
179
versions of the classical Chebyshev polynomials minimize this choice; see Section
10.3. Namely, we set
qk(t) =
1
Tk(1/ρ)Tk
1
ρ

1 −
2
λ1 + λn
t

,
ρ = λn −λ1
λn + λ1
,
and
Tk(t) =
(
cos(k arccos(t)),
|t| ≤1,
cosh(k cosh−1(t)),
|t| > 1.
With this choice, we obtain the bound
max
λ∈[λ1,λn] |qk(λ)| ≤
1
Tk(1/ρ).
Now, since ρ < 1, we set σ = cosh−1(1/ρ) to see that
Tk(1/ρ) = 1
2(ekσ + e−kσ) ≥1
2ekσ.
But, using that σ = cosh−1(1/ρ) = ln

1/ρ +
p
1/ρ2 −1

, we get
ekσ =
1
ρ

1 +
p
1 −ρ2
k
.
Using that ρ = κ2(A)−1
κ2(A)+1, we then obtain
1
Tk(1/ρ) ≤2e−kσ = 2
 p
κ2(A) −1
p
κ2(A) + 1
!k
and the result follows.
7.4.1
Nonzero Starting Vectors
In the absence of any knowledge about the solution to a linear system, it makes no
diﬀerence what is chosen as the starting vector for an iterative scheme. Thus, x0 =
0 seems like a perfect candidate. There are situations, however, when something
is known about the solution; therefore, we wish to use a starting vector other than
the trivial one. In this case, we use the following algorithm.
Deﬁnition 7.40 (standard CG). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆, and
x = A−1f . The (standard) conjugate gradient method is an iterative scheme for
producing a sequence of approximations {xk}∞
k=1 from the starting point x0 ∈Cn
according to the following recursive formula: setting Kk = Kk(A, r 0), where r 0 =
f −Ax0, the kth iterate is obtained by
xk = x′
k + x0,
x′
k = argmin
z∈Kk
EA(z + x0).
(7.15)
The following result shows that the standard CG method essentially reduced to
the zero-start one for a modiﬁed equation.

180
Variational and Krylov Subspace Methods
Proposition 7.41 (equivalence). Suppose that A ∈Cn×n is HPD, f ∈Cn
⋆, x =
A−1f , and x0 ∈Cn. Set
x′ = x −x0,
r 0 = f −Ax0.
The sequence {x′
k}∞
k=1 generated by the zero-start CG algorithm to approximate
the solution to Ax′ = r 0 is equivalent to the sequence {xk}∞
k=1 generated by the
(standard) CG algorithm with the starting vector x0 to approximate the solution
to Ax = f in the sense that
xk = x0 + x′
k,
x −xk = x′ −x′
k.
Furthermore, as long as x0 ̸= x, there is an integer m⋆∈{1, . . . , n} such that
xk ̸= x,
k = 1, . . . , m⋆−1,
xk = x,
k ≥m⋆.
Proof. Recall that
EA(z) = 1
2(Az, z)2 −ℜ((f , z)2),
∀z ∈Cn.
Deﬁne
˜EA(z) = 1
2(Az, z)2 −ℜ((r 0, z)2),
∀z ∈Cn,
and observe that
EA(z + x0) = ˜EA(z) + EA(x0).
It follows that
x′
k = argmin
z∈Kk
EA(z + x0) = argmin
z∈Kk
˜EA(z).
The rest of the details are left to the reader as an exercise; see Problem 7.21.
Remark 7.42 (equivalence). This result states that approximating Ax = f using
the (standard) CG algorithm is exactly equivalent to approximating Ax′ = r 0 using
the zero-start CG algorithm. In short, their convergence properties are the same.
Of course, often it is advantageous to use a nonzero starting vector, especially in
the case where one already has a good approximation to the exact solution of an
equation of interest.
7.4.2
Preconditioned Conjugate Gradient Method
As was the case for the steepest descent method, we can discuss a preconditioning
strategy for the CG method, though the details are somewhat more complicated.
We begin by collecting some simple facts.
Proposition 7.43 (some useful facts). Suppose that A, B ∈Cn×n are HPD, f ∈Cn
⋆,
and x = A−1f ∈Cn
⋆. Let B = LHL be a Cholesky-type factorization for B, where
L ∈Cn×n is invertible. Deﬁne
C = L−HAL−1.

7.4 The Conjugate Gradient Method
181
Then C is HPD and B−1A is similar to C. Consequently,
σ(B−1A) = σ(C) ⊂(0, ∞).
Furthermore, the following problems are equivalent.
1. Find x ∈Cn such that
Ax = f .
2. Find x ∈Cn such that
B−1Ax = B−1f .
3. Find x ∈Cn such that
Lx = y,
Cy = q,
where q = L−Hf .
The equation Cy = q is called the preconditioned system.
Proof. See Problem 7.22.
The preconditioned conjugate gradient (PCG) method is a method for solving
Ax = f by utilizing the preconditioned system Cy = q.
Deﬁnition 7.44 (PCG). Suppose that A, B ∈Cn×n are HPD, f ∈Cn
⋆, and x =
A−1f . Assume that C, L, q, and y are as deﬁned in Proposition 7.43. The zero-start
B-preconditioned conjugate gradient (PCG) method is an iterative scheme for
producing the sequence {y k}∞
k=1 by applying the standard zero-start CG method
to approximate the solution to the preconditioned system, Cy = q. The sequence
of approximations for the solution of interest, x, denoted {xk}∞
k=1, is deﬁned by
xk = L−1y k.
In exact arithmetic, the B-PCG algorithm terminates in a ﬁnite number of steps.
We will address this point momentarily. Using Corollary 7.35 and Theorem 7.39,
we immediately get an equivalent formulation of the PCG method.
Proposition 7.45 (equivalence I). Suppose that A, B ∈Cn×n are HPD, f ∈Cn
⋆,
and x = A−1f . Assume that C, L, q, and y are as deﬁned in Proposition 7.43.
The sequence {y k}∞
k=1, generated by the zero-start B-PCG method, is the same
sequence as that generated by the following recursive algorithm: given y 0 = 0,
deﬁne s0 = q −Cy 0 = q and d 0 = s0 = q. For k ≥0, compute:
1. Update the iterate:
y k+1 = y k + θk+1d k,
θk+1 =
(sk, sk)2
(Cd k, d k)2
.
2. Update the residual:
sk+1 = sk −θk+1Cd k.
3. Update the search direction:
d k+1 = sk+1 + νk+1d k,
νk+1 = (sk+1, sk+1)2
(sk, sk)2
.

182
Variational and Krylov Subspace Methods
4. If sk+1 = 0, stop. Otherwise, index k and go to step 1.
Deﬁne Mk = Kk(C, q). Then there is an integer mC
⋆∈{1, . . . , n} such that
smC⋆= 0 = d mC⋆
and
0 ̸= sk ∈Mk+1\Mk,
0 ̸= d k ∈Mk+1\Mk,
k = 0, . . . , mC
⋆−1.
Furthermore, the following orthogonalities hold:
(sj, si)2 = (d j, d i)C = 0
for all 0 ≤j < i ≤mC
⋆−1. Finally, the following convergence estimate holds:
∥y −y k∥C ≤2
 p
κ2(C) −1
p
κ2(C) + 1
!k
∥y −y 0∥C.
Proof. See Problem 7.23.
Corollary 7.46 (equivalence II). With the same assumptions and notation as in
Proposition 7.45, deﬁne, for 0 ≤k ≤mC
⋆,
xk = L−1y k,
r k = LHsk,
pk = L−1d k.
These vectors may be generated directly by the following recursive algorithm:
x0 = 0, r 0 = f , and p0 = B−1f . For 0 ≤k ≤mC
⋆−1,
1. Update the iterate:
xk+1 = xk + θk+1pk,
θk+1 = (B−1r k, r k)2
(Apk, pk)2
.
2. Update the residual:
r k+1 = r k −θk+1Apk.
3. Update the search direction:
pk+1 = B−1r k+1 + νkpk,
νk+1 = (B−1r k+1, r k+1)2
(B−1r k, r k)2
.
4. If k = mC
⋆−1, stop. Otherwise, index k and go to step 1.
The following error estimate is valid:
∥x −xk∥A ≤2
 p
κ2(C) −1
p
κ2(C) + 1
!k
∥x −x0∥A.
Proof. See Problem 7.24.

7.5 The Conjugate Gradient Method as a Three-Layer Scheme
183
Remark 7.47 (equivalences). To summarize, the practical version of the B-
PCG algorithm is deﬁned by the recursive algorithm given in Corollary 7.46. It
is equivalent, as we have seen, to applying the zero-start CG algorithm to the
preconditioned system Cy = q. As with the preconditioned steepest decent method,
the rate of convergence can increase if we can ﬁnd a preconditioner matrix B ≈A
such that the condition number of B−1A or, equivalently, the condition number
of C is suﬃciently small. An implementation of the algorithm of Corollary 7.46 is
given in Listing 7.1.
7.5
The Conjugate Gradient Method as a Three-Layer Scheme
To conclude the discussion of the CG method we begin with a very important
observation. Since xk −x0, according to Proposition 7.41, is the Galerkin
approximation to x −x0 over Kk, to compute xk we would need to have at hand a
basis for Kk, i.e., we need to store k vectors; see Theorem 7.31. It is remarkable,
however, that owing to the equivalences described in Corollary 7.35 one only needs
to remember the previous two residuals and search directions. This means that
the CG method can be seen as a three-layer scheme. We will explore this in more
detail. The presentation in this section closely follows [80] and, for simplicity, is
done under the assumption that we are operating over the reals.
Consider
B(xk+1 −xk) + (1 −αk+1)(xk −xk−1)
τk+1αk+1
+ Axk = f ,
where we will choose the iterative parameters αk+1 and τk+1. To start this
procedure we need x0, which can be chosen arbitrarily, and x1, which we will
compute by
Bx1 −x0
τ1
+ Ax0 = f .
From these formulas we can see that the error ek = x −xk satisﬁes
ek+1 = αk+1(I −τk+1B−1A)ek + (1 −αk+1)ek−1,
e1 = (I −τ1B−1A)e0.
Let us now introduce the change of variables v k = A1/2ek, so that ∥v k∥2 = ∥ek∥A.
Then, upon deﬁning C = A1/2B−1A1/2, we see that
v k+1 = αk+1(I −τk+1C)v k + (1 −αk+1)v k−1,
v 1 = (I −τ1C)v 0.
Using these formulas recursively, we conclude that the error satisﬁes
v k = pk(C)v 0
for some polynomial pk of degree k with pk(0) = 1.
We will now choose the iterative parameters αk+1 and τk+1 in order to minimize
∥v k∥2 = ∥ek∥A. Notice that this is diﬀerent than the Chebyshev iterations described

184
Variational and Krylov Subspace Methods
in Section 6.10.1, in that there the error is minimized only after m iterations. By
contrast, here, we are minimizing it in every iteration. From this condition, it
immediately follows that we must choose τ1 so as to minimize ∥v 1∥2; thus,
τ1 = (Cv 0, v 0)2
∥Cv 0∥2
2
= (B−1r 0, r 0)2
∥B−1r 0∥2
A
.
In doing so, we obtain
(Cv 1, v 0)2 = (C(I −τ1C)v 0, v 0)2 = (Cv 0, v 0)2 −(Cv 0, v 0)2
∥Cv 0∥2
2
(C2v 0, v 0)2 = 0,
or that v 1 and v 0 are orthogonal in the C-inner product.
Now, for k > 1, we will write
pk(C) = In +
k
X
i=1
ˆak,iCi,
where the coeﬃcients ˆak,i are deﬁned by the parameters αi and τi for i = 1, . . . , k.
Therefore,
v k = v 0 +
k
X
i=1
ˆak,iCiv 0,
which implies that
∥v k∥2
2 = ∥v 0∥2
2 + 2
k
X
j=1
ˆak,j(Cjv 0, v 0)2 +
k
X
i,j=1
ˆak,i ˆak,j(Civ 0, Cjv 0)2.
To minimize the error, we choose the coeﬃcients of pk from the conditions
1
2
∂∥v k∥2
2
∂ˆak,j
= (Cjv 0, v 0)2 +
k
X
i=1
ˆak,i(Civ 0, Cjv 0)2 = 0,
j = 1, . . . , k.
(7.16)
Thus, from now on, we will study the solvability of the system (7.16). We begin
by observing that this equation is equivalent to the orthogonality condition
(Cmv 0, v k)2 = 0,
m = 1, . . . , k.
(7.17)
Lemma 7.48 (orthogonality). Condition (7.17) is equivalent to the condition
(Cv m, v k)2 = 0,
m = 0, . . . , k −1.
(7.18)
Proof. Since v m = pm(C)v 0, we can write
Cv m = Cv 0 +
m
X
i=1
ˆam,iCi+1v 0

7.5 The Conjugate Gradient Method as a Three-Layer Scheme
185
and then
(Cv m, v k)2 = (Cv 0, v k)2 +
m
X
i=1
ˆam,i(Ci+1v 0, v k)2
= (Cv 0, v k)2 +
m+1
X
i=2
ˆam,i−1(Civ 0, v k)2.
From this, it follows that if (7.17) holds, then (7.18) must follow.
To show the reverse implication, we proceed by induction on m. For m = 1,
condition (7.18) coincides with condition (7.17), so that there is nothing to
prove. Now, if (7.17) holds for all j ≤m, let us show that this implies that
(Cm+1v 0, v k)2 = 0. Since, by (7.18),
0 = (Cv m, v k)2
= (Cpm(C)v 0, v k)2
=
 
Cv 0 +
m
X
i=1
ˆam,iCi+1v 0, v k
!
2
= (Cv 0, v k)2 +
m
X
i=2
ˆam,i−1(Civ 0, v k)2 + ˆam,m(Cm+1v 0, v k)2
= ˆam,m(Cm+1v 0, v k)2,
where in the last step we used (7.17). But ˆam,m ̸= 0, since deg pm = m.
We must remark that the number k in Lemma 7.48 is ﬁxed, while we need ∥v k∥2
to be minimized for every k. In light of the equivalence given in Lemma 7.48, we
will choose the iteration parameters using the condition
(Cv m, v k)2 = 0,
k = 1, 2, . . . ,
m = 0, 1, . . . , k −1.
However, this shows that if we are able to ﬁnd parameters then we are constructing
a system of vectors that is orthogonal in the C-inner product. For this reason, we
necessarily have that, after at most n steps, v n = 0 and so we have found the
exact solution to the system.
After this observation, we can construct the iteration parameters. As mentioned
before,
α1 = 1,
τ1 = (Cv 0, v 0)2
∥Cv 0∥2
2
.
If we have already found α1, . . . , αk and τ1, . . . , τk, then from the orthogonality
condition and the error representation we must have, for m ≤k −2,
(v k+1, Cv m)2 = αk+1(v k, Cv m)2 −αk+1τk+1(Cv k, Cv m)2
+ (1 −αk+1)(v k−1, Cv m)2
= −αk+1τk+1(Cv k, Cv m)2.

186
Variational and Krylov Subspace Methods
But, since
Cv m =
1
τm+1
v m −
1
αm+1τm+1
[v m+1 −v m−1],
it follows that
(Cv k, Cv m)2 = 0.
It remains then to verify the orthogonality condition for m = k −1, k. For
m = k −1, we obtain
0 = (v k+1, Cv k−1)2
= αk+1(v k, Cv k−1)2 −αk+1τk+1(Cv k, Cv k−1)2 + (1 −αk+1)(v k−1, Cv k−1)2
= −αk+1τk+1(Cv k, Cv k−1)2 + (1 −αk+1)(v k−1, Cv k−1)2
and, for m = k, we obtain
0 = (v k+1, Cv k)2
= αk+1(v k, Cv k)2 −αk+1τk+1(Cv k, Cv k)2 + (1 −αk+1)(v k−1, Cv k)2
= αk+1(v k, Cv k)2 −αk+1τk+1(Cv k, Cv k)2.
Solving this system yields
τk+1 = (Cv k, v k)2
∥Cv k∥2
2
= (B−1r k, r k)2
∥B−1r k∥2
A
(7.19)
and
αk+1 =

1 −τk+1
τk
1
αk
(Cv k, v k)2
(Cv k−1, v k−1)2
−1
=

1 −τk+1
τk
1
αk
(B−1r k, r k)2
(B−1r k−1, r k−1)2
−1
.
(7.20)
7.6
Krylov Subspace Methods for Non-HPD Problems
As a ﬁnal section in our discussion of variational and Krylov subspace methods we
consider the linear system Ax = f in the case that the system matrix A is not
necessarily HPD. In this case, the solution of the linear system may not minimize
EA. In fact, this quadratic function may not have a minimum! We must, then,
consider diﬀerent properties of the solution.
7.6.1
Methods Based on the Normal Equation
A ﬁrst attempt at solving a system with a non-HPD matrix is to transform it into a
problem with a matrix that is HPD and apply to it the CG method that we already
discussed. We begin by recalling that, since A is assumed to be nonsingular, the
matrix AHA is HPD and x solves Ax = f if and only if it solves
AHAx = AHf .
(7.21)

7.6 Krylov Subspace Methods for Non-HPD Problems
187
This gives rise to the following method.
Deﬁnition 7.49 (CGNR). Let A ∈Cn×n be nonsingular, f ∈Cn
⋆, and x0 ∈Cn be
arbitrary. The sequence {xk}∞
k=1 obtained by applying the CG method of Deﬁnition
7.40 to the system (7.21) is called the conjugate gradient normal equation
residual (CGNR) method.
The following is an immediate consequence of Theorem 7.39.
Corollary 7.50 (convergence of CGNR). Let A ∈Cn×n be nonsingular, f ∈Cn
⋆,
and x0 ∈Cn be arbitrary. The sequence {xk}∞
k=1 obtained by the CGNR method
is such that the error ek = x −xk satisﬁes
∥ek∥2 ≤2κ2(A)
κ2(A) −1
κ2(A) + 1
k
∥ek∥2 ,
where κ2(A) is the spectral condition number of A.
Proof. See Problem 7.31.
A motivation for the naming of this method is explored in Problem 7.32.
We can consider another system with an HPD coeﬃcient matrix. Since A is
nonsingular, the vector y ∈Cn solves
AAHy = f
(7.22)
if and only if x = AHy solves Ax = f . The matrix of the previous system is HPD,
and so we can apply to it the CG method, giving rise to the so-called CGNE scheme.
Deﬁnition 7.51 (CGNE). Let A ∈Cn×n be nonsingular, f ∈Cn
⋆, and y 0 ∈Cn
be arbitrary. Let {y k}∞
k=1 be the sequence obtained by applying the CG method of
Deﬁnition 7.40 to system (7.22). Set xk = AHy k. This gives rise to the conjugate
gradient normal equation error (CGNE) method.
The reason for the naming of this method is explored in Problem 7.33. The
following convergence result is, again, an immediate consequence of Theorem 7.39.
Corollary 7.52 (convergence of CGNE). Let A ∈Cn×n be nonsingular, f ∈Cn
⋆,
and y 0 ∈Cn be arbitrary. The sequence {xk}∞
k=1 obtained by the CGNE method
is such that the error ek = x −xk satisﬁes
∥ek∥2 ≤2
κ2(A) −1
κ2(A) + 1
k
∥ek∥2 ,
where κ2(A) is the spectral condition number of A.
Proof. See Problem 7.34.

188
Variational and Krylov Subspace Methods
7.6.2
The Generalized Minimization of the Residual (GMRES) Method
As we observe from Corollaries 7.50 and 7.52, although methods based on the
normal equation are rather convenient, they suﬀer from a slower convergence rate.
The condition number of the system is eﬀectively squared. Here, we explore the
so-called generalized minimization of the residual (GMRES) method, which does
not suﬀer from this issue.
Deﬁnition 7.53 (GMRES). Let A ∈Cn×n be nonsingular, f ∈Cn
⋆, and x0 ∈Cn
be arbitrary. The sequence {xk}∞
k=1, obtained by minimizing
∥r(z)∥2 = ∥f −Az∥2
over
x0 + Kk(A, f −Ax0),
gives rise to the generalized minimization of the residual (GMRES) method.
Before we embark on the study of the properties of this method, we must
make some observations regarding its practical implementation. Recall that CG
also minimizes over a translation of the Krylov subspace Kk; see Proposition 7.41.
Thus, a priori it seemed necessary to construct and store a basis for this space.
However, owing to the fact that the system matrix was HPD, this method can
be reduced to a three-layer scheme, as shown in Proposition 7.45. For GMRES,
however, this property no longer holds, and so an orthonormal basis of Kk must be
computed and stored.
This is a major bottleneck of this method when n is large. For this reason, it is
common practice to consider the GMRES method with restarts. This means that
a number m ≪n is chosen, and the GMRES algorithm with arbitrary initial guess
x0 is run for at most m iterations. If the solution, or a suitable approximation, has
not been found, the Krylov subspace and its basis are discarded and the GMRES
algorithm is run again, but this time the initial guess is xm, the last approximate
solution. This procedure is repeated until a suitable tolerance is reached.
Let us now discuss how an orthonormal basis ⟨q1, . . . , qm⟩= Km(A, r 0), with
r 0 = f −Ax0, can be eﬃciently constructed and how xk can be eﬃciently found. A
clever application of the modiﬁed Gram–Schmidt algorithm, presented in Section
5.6, can be used to compute an orthonormal basis of Km. This is known as the
Arnoldi algorithm4 and is presented in Listing 7.2. We observe that the algorithm
may break down if, at some point, [H]j+1,j = 0. This means that the Krylov
subspace is maximal, i.e., invariant under A. For this reason, the exact solution
satisﬁes x −x0 ∈Kk. In the literature, this unlikely occurrence is known as a lucky
breakdown.
Having found a way to compute an orthonormal basis of the Krylov subspaces,
we observe that, setting Qk = [q1, . . . , qk], by construction we have
AQk = Qk+1Hk,
(7.23)
4 Named in honor of the American engineer Walter Edwin Arnoldi (1917–1995).

7.6 Krylov Subspace Methods for Non-HPD Problems
189
where Hk ∈Ck+1,k is the matrix with entries corresponding to the inner products
in the algorithm. Consider now how xk is deﬁned. According to Deﬁnition 7.53, we
have that
xk ∈argmin
z∈Kk
∥f −A(x0 + z)∥2 = argmin
z∈Kk
∥r 0 −Az∥2.
Now, since z ∈Kk, we can write it as z = Pk
i=1 yiqi = Qky for some y ∈Ck.
Substituting, using (7.23) and recalling that r 0 = ∥r 0∥2q1, we obtain that
∥r 0 −Az∥2 = ∥∥r 0∥2q1 −AQky∥2
= ∥∥r 0∥2q1 −Qk+1Hky∥2
= ∥Qk+1 (∥r 0∥2e1 −Hky)∥2
= ∥∥r 0∥2e1 −Hky∥ℓ2(Ck+1) ,
where e1 denotes the ﬁrst canonical basis vector in Ck+1 and, in the last step,
we used that Qk+1 has orthonormal columns. In conclusion, ﬁnding xk ∈Kk is
equivalent to solving the minimization problem
y ∈argmin
˜y∈Ck
∥∥r 0∥2e1 −Hk˜y∥ℓ2(Ck+1)
(7.24)
and setting xk = x0 + Qky.
Equation (7.24) deﬁnes a linear least squares problem, which we studied in
Chapter 5. As Problem 7.36 shows, the matrix Hk is full rank and, consequently,
there is a unique y ∈Ck that solves (7.24). This can be found by computing the
QR factorization of the matrix Hk using either Householder reﬂectors (Section 5.7)
or Givens rotations (Problem 5.35).
Having brieﬂy discussed the implementation of the GMRES method, we now
turn to its convergence properties. As we are minimizing over subspaces of
nondecreasing dimension, we must converge in a ﬁnite number of iterations.
Proposition 7.54 (minimization). Let A ∈Cn×n be nonsingular and f ∈Cn
⋆be
given. Set x = A−1f . For any x0 ∈Cn, there is an integer m⋆∈{1, . . . , n} such
that the sequence {xk}∞
k=1 generated by the GMRES method of Deﬁnition 7.53
satisﬁes
xk ̸= x,
k = 1, . . . , m⋆−1,
xk = x,
k ≥m⋆.
Proof. Clearly, dim Kk ≤k. Moreover, if the Arnoldi process does not break down
after k steps, then we must have equality. Thus, if the Arnoldi process can reach
n steps, Kn = Cn. On the other hand, if the Arnoldi process breaks down for some
m⋆< n, then AKm⋆= Km⋆. Thus, since
A(x −x0) = r 0 ∈Km⋆,
it follows that x −x0 ∈Km⋆and the minimization procedure yields the exact
solution.
As was the case with CG, we consider GMRES as an iterative scheme.
The following result shows its convergence properties.

190
Variational and Krylov Subspace Methods
Theorem 7.55 (convergence of GMRES). Let A ∈Cn×n be nonsingular and f ∈Cn
⋆
be given. Set x = A−1f . Let x0 be arbitrary and let the sequence {xk}∞
k=1 be
generated by the GMRES method of Deﬁnition 7.53. The residual r k = f −Axk
satisﬁes
∥r k∥2 ≤inf
p∈Pk,⋆∥p(A)∥2∥r 0∥2,
where Pk,⋆was deﬁned in Theorem 7.37. In addition, if A is diagonalizable
A = VΛV−1, then
∥r k∥2 ≤κ2(V) inf
p∈Pk,⋆max
λ∈σ(A) |p(λ)|∥r 0∥2.
Proof. Notice that xk −x0 ∈Kk, so, as in Theorem 7.37, there is a polynomial
qk−1 ∈Pk−1 for which xk −x0 = qk−1(A)r 0. Therefore,
r k = f −Axk = r 0 −A(xk −x0) = (I −Aqk−1(A)) r 0 = pk(A)r 0,
where pk ∈Pk,⋆. The minimization property of the residual then is equivalent to
minimizing over p ∈Pk,⋆, i.e.,
∥r k∥2 = inf
p∈Pk,⋆∥p(A)r 0∥2,
and the ﬁrst assertion follows.
To show the second estimate, we begin by observing that if A = VΛV−1 and p
is a polynomial, then
p(A) = Vp(Λ)V−1.
Since A ≍Λ and Λ is diagonal, the second estimate follows from the ﬁrst one.
If knowledge about the location of the spectrum of A is available, then the
estimate on the convergence rate can be reﬁned as we now discuss. Since A is not
assumed to be Hermitian, σ(A) ⊂C in general. Thus, to quantify the location of
the eigenvalues, we deﬁne, for c ∈R, the set
E(c, d, a) ⊂C
of points contained in the ellipse with center c, major semiaxis a, and focal distance
d. Moreover, we assume that the major semiaxis is parallel to one of the coordinate
axes.
The proof of the following result is beyond our scope here, as it requires properties
of Chebyshev polynomials which we will only consider in Part II of our text. We
refer the reader to [78, Corollary 6.1] for a proof.
Corollary 7.56 (convergence rate). Let A ∈Cn×n be nonsingular and diagonaliz-
able A = VΛV−1 and f ∈Cn
⋆be given. Set x = A−1f . Let x0 be arbitrary and
let the sequence {xk}∞
k=1 be generated by the GMRES method of Deﬁnition 7.53.
Assume that σ(A) ⊆E(c, d, a) and 0 /∈E(c, d, a). Then the residual r k = f −Axk
satisﬁes
∥r k∥2 ≤κ2(V) Ck
  a
d

Ck
  c
d
∥r 0∥2,

Problems
191
where
Ck(z) =

z +
p
z2 −1
k
+

z +
p
z2 −1
−k
.
Problems
7.1
Complete the proof of Theorem 7.1.
7.2
Complete the proof of Proposition 7.2.
7.3
Prove Proposition 7.5.
7.4
Let A ∈Cn×n be HPD. Deﬁne ∥x∥A : Cn →R via ∥x∥A =
√
xHAx. Prove
that this is a norm and satisﬁes the estimates
p
λ1 ∥x∥2 ≤∥x∥A ≤
p
λn ∥x∥2,
where 0 < λ1 ≤· · · ≤λn are the eigenvalues of A, with both inequalities attainable
(though perhaps not simultaneously) for suitable choices of x.
7.5
Complete the proof of Theorem 7.7.
7.6
Prove Corollary 7.8.
7.7
Prove formula (7.1).
7.8
Suppose that A ∈Cn×n is HPD, f ∈Cn is given, and x = A−1f . Consider
the quadratic function EA : Cn →R deﬁned by
EA(z) = 1
2zHAz −ℜ(zHf ).
Given xk ∈Cn and a search direction d k ∈Cn, deﬁne
αk+1 = argmin
α∈R
EA(xk + αd k),
xk+1 = xk + αk+1d k.
Consider a method in which the ﬁrst n search directions d 0, . . . , d n−1 are taken to
be the standard unit vectors e1, . . . , en, the next n search directions d n, . . . , d 2n−1
are again taken to be e1, . . . , en, and so on.
a)
Show that each group of n steps constitutes one complete iteration of the
Gauss–Seidel method.
b)
Show that it can happen that EA(xk+1) = EA(xk) even if xk ̸= x.
c)
Prove that the situation described in the previous part cannot persist for
n consecutive steps. In other words, show that, for some k with xk ̸= x,
EA(xk+1) < EA(xk). Thus, each complete Gauss–Seidel iteration reduces the
function EA. This idea leads to yet another proof that the Gauss–Seidel method
applied to a HPD system matrix always converges.
7.9
Prove Proposition 7.14.
7.10
Prove Theorem 7.20.
7.11
Complete the proof of Proposition 7.29.
7.12
Prove Theorem 7.30.
Hint: Follow the ideas in the proof of Theorem 7.1.
7.13
Complete the proof of Theorem 7.32.
7.14
Prove Corollary 7.33.
7.15
Prove Corollary 7.34.
7.16
Prove Corollary 7.35.

192
Variational and Krylov Subspace Methods
7.17
Show that, for the CG method, the following error estimate holds:
∥x −xk∥2 ≤2κ(A)
 p
κ(A) −1
p
κ(A) + 1
!k
∥x −x0∥2.
7.18
Show that if λ ∈σ(A) and p ∈Pn, then p(λ) ∈σ(p(A)).
7.19
Suppose that A ∈Cn×n is HPD and f ∈Cn. For any z, deﬁne the residual
as r(z) = f −Az and the error as e(z) = A−1r(z). Show that (e(z), r(z))2 > 0
unless z = A−1f .
7.20
Suppose that A = I −B ∈Rn×n is SPD and rank(B) = r < n. Show that
when the CG algorithm is applied to solve Ax = f , where f ∈Rn, the method
converges in at most r + 1 steps.
7.21
Complete the proof of Proposition 7.41.
7.22
Prove Proposition 7.43.
7.23
Prove Proposition 7.45.
7.24
Prove Corollary 7.46.
7.25
This problem is about preconditioning for the CG method. Let
A =
A1
A2
A⊺
2
A3

,
S =
A1
O
O
A3

,
where A1, A2, A3, O ∈Rn×n and O is the zero matrix. Suppose that A is SPD.
a)
Prove that A1, A3, and S are also SPD.
b)
Let L1L⊺
1 = A1, L3L⊺
3 = A3, and BB⊺= S be the respective Cholesky
factorizations. Writing B in terms of L1 and L3, prove that
C = B−1AB−⊺=
 In
F
F⊺
In

,
where F = L−1
1 A2L−⊺
3 .
Hint: Start by writing
A = BB⊺+
 O
A2
A⊺
2
O

.
c)
Let λi, i = 1, . . . , 2n, and µj, j = 1, . . . , n, be the eigenvalues of C and FF⊺,
respectively. Prove that, with the appropriate numbering,
λk = 1 −√µk,
λk+d = 1 + √µk,
k = 1, . . . , n.
Hint: Calculate (C −I)2 and ﬁnd the eigenvalues of this matrix.
d)
Let rank(A2) = r < n. Prove that rank(FF⊺) = r.
e)
Deduce that C has at most 2r + 1 distinct eigenvalues. In what number of
iterations is the CG algorithm (with exact arithmetic) guaranteed to converge
if applied to solve Cx = f ?
7.26
Suppose that A, B ∈Cn×n are HPD. Assume that there are positive
constants γ1, γ2 > 0 such that
γ1(Bx, x)2 ≤(Ax, x)2 ≤γ2(Bx, x)2
for all x ∈Cn. Prove the following.

Problems
193
a)
The matrix B−1A is self-adjoint and positive deﬁnite with respect to the B
inner product, which is deﬁned via
(x, y)B = (Bx, y)2,
∀x, y ∈Cn.
b)
The matrix B−1A has positive real eigenvalues:
σ(B−1A) = {µ1, . . . , µn},
0 < µ1 ≤· · · ≤µn.
c)
The condition number of the preconditioned matrix B−1A with respect to the
B-norm satisﬁes
κB(B−1A) =
B−1A

B

 B−1A
−1
B = µn
µ1
.
d)
Finally, the following condition number estimate holds:
κB(B−1A) ≤γ2
γ1
.
7.27
Prove that if A ∈Cn×n is HPD with eigenvalues λ1, . . . , λn, and p is a
polynomial, then
∥p(A)∥A = max
1≤j≤n |p(λj)|.
7.28
Suppose that A ∈Cn×n is HPD and f ∈Cn
⋆is given. To solve the system
Ax = f , we employ the CG method, starting from the initial guess x0 = 0.
a)
Suppose that A has only two distinct eigenvalues, 0 < λ1 < λ2. Prove that,
for every λ ∈(λ1, λ2),
∥e1∥A ≤q(λ)∥e0∥A,
q(λ) =
2
max
i=1
1 −λi
λ
 .
b)
Prove that
1
2 (λ1 + λ2) = argmin
λ∈(λ1,λ2)
q(λ).
Hint: max{|a|, |b|} = 1
2|a + b| + 1
2|a −b|.
c)
Use the last fact to show that
∥e1∥A ≤κ2 −1
κ2 + 1∥e0∥A,
where κ2 is the spectral condition number of A.
7.29
Let A ∈Rn×n be SPD and σ(A) = {λi}n
i=1. Assume that 0 < λ1 ≤λ2 ≤
· · · ≤λn, so that κ2(A) = λn/λ1. As in the proof of Theorem 7.37, denote by Pk,⋆
the set of polynomials of degree at most k that have the value one at zero and set
˜Pk,⋆= {p ∈Pk,⋆|p(λn) = 0} .
Show that:
a)
The error {ek}k≥0 in CG satisﬁes
∥ek∥A ≤∥e0∥A inf
p∈˜Pk,⋆
max
λ∈[λ1,λn−1] |p(λ)|.

194
Variational and Krylov Subspace Methods
b)
Use the previous estimate to show that the error in CG also satisﬁes
∥ek∥A ≤∥e0∥A
λn −λ1
λn

inf
p∈Pk−1,⋆
max
λ∈[λ1,λn−1] |p(λ)|.
c)
Deﬁne ˜κ2(A) = λn−1/λ1. From the previous estimate, deduce that
∥ek∥A ≤2∥e0∥A
 p
˜κ2(A) −1
p
˜κ2(A) + 1
!k−1
.
7.30
When deriving the CG method as a three-layer iterative scheme, we obtained
the following formula:
xk+1 = αk+1xk + (1 −αk+1)xk−1 + αk+1τk+1w k,
where w k = B−1r k and r k = f −Axk. By contrast, when deriving it as the Galerkin
solution to Ax = f over Krylov subspaces, we arrived at
xk+1 = xk + θk+1pk,
pk = w k + νkpk−1, k > 0,
p0 = w 0.
Show that, indeed, these two methods are equivalent.
7.31
Prove Corollary 7.50.
7.32
Show that the sequence {xk}k≥0 generated by the CGNR method of
Deﬁnition 7.49 minimizes
φ1(z) = 1
2zHAHAz −ℜ(zHAHf )
over x0 + Kk(AHA, f −AHAx0). Moreover,
φ1(z) + 1
2ℜ(f Hf ) = 1
2∥f −Az∥2
2.
7.33
Show that the sequence {y k}k≥0 generated by the CGNE method of
Deﬁnition 7.51 minimizes EAAH over y 0 + Kk(AAH, f −AAHy 0).
7.34
Prove Corollary 7.52.
7.35
Let {y k}k≥0 be generated by the CGNE method of Deﬁnition 7.51. Deﬁne
xk = AHy k. Show that the sequence {xk}k≥0 minimizes
1
2∥z −A−1f ∥2
2 + 1
2∥A−1f ∥2
2
over x0 + Kk(AHA, AH(f −Ax0)).
7.36
Show that the matrix Hk, deﬁned in (7.23), is full rank.

Listings
195
Listings
1
function [x, its, err] = PCG( A, x0, f, Binv, maxit, tol )
2
% The preconditioned conjugate gradient method to approximate
3
% the solution to
4
%
5
%
Ax = f
6
%
7
% with A HPD.
8
%
9
% Input
10
%
A(1:n,1:n) : the system matrix
11
%
x0(1:n) : the initial guess
12
%
f(1:n) : the right hand side vector
13
%
Binv(1:n,1:n) : the inverse of the preconditioner
14
%
maxit : the maximal number of iterations
15
%
tol : the tolerance
16
%
17
% Output
18
%
x(1:n) : the approximate solution to the linear system of
19
%
equations
20
%
its : the number of iterations
21
%
err : = 0, if the tolerance is reached in less than maxit
22
%
iterations
23
%
= 1, if the tolerance is not reached
24
err = 0;
25
x = x0;
26
r = f - A*x;
27
p = Binv*r;
28
z = p;
29
initerror = sqrt( r'*p );
30
for its=1:maxit
31
Ap = A*p;
32
denom = p'*Ap;
33
if denom < tol
34
err = 0;
35
return;
36
end
37
theta = (1./denom)*(z'*p);
38
x = x + theta*p;
39
r = r - theta*Ap;
40
z = Binv*r;
41
nu = (1./denom)*(z'*p);
42
p = z + nu*p;
43
if sqrt( r'*z )/initerror < tol
44
err = 0;
45
return;
46
end
47
end
48
err = 1;
49
end
Listing 7.1 The preconditioned conjugate gradient method.

196
Variational and Krylov Subspace Methods
1
function [Q, H, err] = ArnoldiGMRES( A, r, m )
2
% The Arnoldi algorithm to compute an orthonormal basis of
3
%
4
%
K k(A, r) = span{ r, Ar, ..., Aˆ{m-1} r }
5
%
6
% the Krylov subspace of order m. This will be eventually used
7
% in GMRES
8
%
9
% Input:
10
%
A(1:n,1:n) : the system matrix
11
%
r(1:n) : the initial residual
12
%
m : the size of the Krylov subspace
13
%
14
% Output:
15
%
Q(1:n, 1:m) : the columns of this matrix are the orthonormal
16
%
basis
17
%
H(1:m, 1:m-1) : the upper Hessenberg matrix that is defined
18
%
by
19
%
20
%
AQ k = Q {k+1} H
21
%
22
%
with Q i being the first columns of Q
23
%
err : = 0, if the algorithm proceeded to completion
24
%
= 1, if H(j+1,j) = 0 at some point
25
err = 0;
26
n = size(A,1);
27
Q = zeros(n,m);
28
H = zeros(m,m-1);
29
norm r = norm( r );
30
if norm r < eps( norm r )
31
err = 1;
32
return;
33
end
34
Q(:,1) = r/norm r;
35
for j=1:m-1
36
Q(:,j+1) = A*Q(:,j);
37
for i=1:j
38
H(i,j) = Q(:,i)'*Q(:,j+1);
39
Q(:,j+1) = Q(:,j+1) - H(i,j)*Q(:,i);
40
end
41
norm r = norm( Q(:,j+1) );
42
if norm r < eps( norm r )
43
err = 1;
44
return;
45
end
46
H(j+1,j) = norm r;
47
Q(:,j+1) = Q(:,j+1)/norm r;
48
end
49
end
Listing 7.2 The Arnoldi algorithm.

8
Eigenvalue Problems
The focus of this chapter will be the eigenvalue problem: given A ∈Cn×n, we will
be interested in ﬁnding pairs (λ, x) ∈C × Cn
⋆such that
Ax = λx.
Standard references for this topic are the classic texts [70, 102]. The possible
applications of this problem are so vast that any attempt at listing them here will
force us to misrepresent them.
One might argue that this chapter is in the wrong part of the book. Indeed,
ﬁnding eigenvalues requires nonlinear methods, and it is a bit misleading to place
it in the numerical linear algebra part of our discussion. However, this is done for
historical reasons.
Why nonlinear rather than linear? Since, for a x ̸= 0, we have that (A−λI)x = 0,
we necessarily have that det(A −λI) = 0. Recall that the characteristic polynomial
of the matrix A is deﬁned by
χA(t) = det(A −tI).
This shows that λ ∈σ(A) if and only if χA(λ) = 0. This suggests a naive approach
to the problem at hand: to ﬁnd the eigenvalues of a matrix, it is enough to ﬁnd the
roots of the characteristic polynomial, a nonlinear process. We now immediately
see two issues. To ﬁnd eigenvalues, following this approach we must:
1. Compute the characteristic polynomial χA.
2. Find its roots.
Even if we forget about the complexity, which in general is O(n3), of computing
the determinant of a square matrix of size n, the ﬁrst step in this approach is
plagued with numerical diﬃculties, as the following examples illustrate.
Example 8.1
Instability of the determinant: Suppose that
A =


1
20
2
20
...
...
9
20
10


.

198
Eigenvalue Problems
There are zeros everywhere that there is not a whole number above. It follows that
σ(A) = {1, . . . , 10}. Now, for 0 < ε ≪1, consider also
Aε =


1
20
2
20
...
...
9
20
ε
10


.
In other words, we have replaced one zero, in the bottom left corner, with a very
small number. In any reasonable norm, ∥Aε∥will be close to ∥A∥. However, we can
choose ε, so that 0 ∈σ(Aε). To see this, consider
det(Aε) = det


2
20
...
...
9
20
10

−ε det


20
2
20
...
...
9
20
10
20


= 10! −209ε.
Therefore, if
ε = 10!
209 ≈3 × 106
5 × 1011 ≈7 × 10−6,
the matrix Aε has zero as an eigenvalue.
Example 8.2
The famous Wilkinson polynomial1 is deﬁned as
pW (t) =
20
Y
i=1
(t −i).
The roots of this polynomial are, clearly, ti = i, for i = 1, . . . , 20. However, if there
is a nondiagonal matrix W such that χW = pW , a numerical procedure to compute
χW will never produce it in the factored form presented above. In addition, the
computations will be aﬀected by roundoﬀ. Let us illustrate how catastrophic this
can be. Figure 8.1 compares pW with a perturbation,
˜pW (t) = pW (t) −10−9t19.
We clearly see that, although the size of the perturbation in front of the coeﬃcient
t19 is rather small, namely 10−9, several of the roots of this polynomial have become
complex.
Even if we choose to ignore the inherent complexity and stability issues in
computing a determinant that we have just described, we are still faced with an
insurmountable diﬃculty, which we now detail.
1 Named in honor of the British mathematician James Hardy Wilkinson (1919–1986).

Eigenvalue Problems
199
0
5
10
15
20
25
−2e+16
−1.5e+16
−1e+16
−5e+15
0
5e+15
t
p(t)
roots
pW
˜pW
Figure 8.1 The Wilkinson polynomial pW and its perturbation ˜pw(x) = pW (x) −10−9x19.
Notice that the perturbation of one of the coeﬃcients is rather small; the plot shows
that we are missing several of the roots.
For a linear system Ax = f , or an even least squares problem, we showed that
there are always direct methods for ﬁnding the solution vector x. These may be
quite expensive, requiring, say, O(n3) operations; but, nonetheless, direct methods
were available. However, direct methods are not typically available for ﬁnding
eigenpairs. This fundamental diﬃculty can be seen from the following example,
the details of which are left for Problem 8.1. Consider the matrix
F =


a1
a2
· · ·
an−1
an
1
0
· · ·
0
0
0
...
...
...
...
...
...
1
0
0
0
· · ·
0
1
0


.
F has the characteristic polynomial
χF(t) = (−1)n tn −a1tn−1 −· · · −an

.
In other words, to any polynomial we can associate an eigenvalue problem, and
vice versa. One of the most celebrated results of Galois theory2 is the so-called
2 Named in honor of the French mathematician ´Evariste Galois (1811–1832).

200
Eigenvalue Problems
Abel’s impossibility theorem,3 which, in particular, states that there is no general
direct solution method for ﬁnding roots of polynomials (of degree larger than or
equal to 5); see, for example, [45, Proposition V.9.8]. There cannot be a direct
method — an algorithmic method that completes in a ﬁnite number of steps — to
ﬁnd the eigenvalues of a matrix. In conclusion, the mantra of eigenvalue problems
is the following:
Every practical eigenvalue approximation algorithm is iterative!
Now you may argue that there are matrices for which the spectrum is easy to
compute, namely diagonal and triangular matrices. This is true: for such matrices,
we can simply read the eigenvalues from the diagonal entries.
An alternate approach to our problem then may be proposed: since similarity
transformations preserve the spectrum, transform a generic matrix A into a
triangular matrix T via a similarity transformation. As guaranteed by the Schur
factorization theorem, there exists a unitary matrix U ∈Cn×n such that A = UTUH,
where T is triangular.
So the question reduces to this one: Can we ﬁnd U using a direct method? The
answer again is a resounding no. If there was a such a method, then we could ﬁnd
the roots to any polynomial of any degree in a ﬁnite number of steps. Since this is
impossible, in general, we get a contradiction.
Before we describe our iterative methods for ﬁnding eigenpairs, let us introduce
some coarse approximation techniques.
8.1
Estimating Eigenvalues Using Gershgorin Disks
If all that is wanted are coarse approximations of the eigenvalues, this can be
obtained easily using so-called Gershgorin disks.
Deﬁnition 8.1 (Gershgorin disks4). Let n ≥2 and A ∈Cn×n. The Gershgorin
disks Di of A are
Di = {z ∈C | |z −ai,i| ≤Ri} ,
Ri =
n
X
j=1
j̸=i
|ai,j|,
i = 1, . . . , n.
Theorem 8.2 (Gershgorin Circle Theorem). Let n ≥2 and A = [ai,j] ∈Cn×n.
Then
σ(A) ⊂
n[
i=1
Di.
3 Named in honor of the Norwegian mathematician Niels Henrik Abel (1802–1829).
4 Named in honor of the Belarussian mathematician Semyon Aranovich Gershgorin
(1901–1933).

8.1 Estimating Eigenvalues Using Gershgorin Disks
201
Proof. Suppose that (λ, w) is an eigenpair of A. Then
n
X
j=1
ai,jwj = λwi,
i = 1, 2, . . . , n.
Suppose that
|wk| = ∥w∥∞=
n
max
i=1 |wi|.
Observe that wk ̸= 0, since w ̸= 0. Then
|λ −ak,k| · |wk| = |λwk −ak,kwk|
=

n
X
j=1
ak,jwj −ak,kwk

=

n
X
j=1
j̸=k
ak,jwj

≤
n
X
j=1
j̸=k
|ak,jwj|
≤
n
X
j=1
j̸=k
|ak,j| · |wk|
= Rk|wk|.
Thus, λ ∈Dk.
Theorem 8.3 (Gershgorin Second Theorem). Let n ≥2 and A ∈Cn×n. Suppose
that 1 ≤p ≤n −1 and that the Gershgorin disks of the matrix A can be divided
into disjoint subsets D(p) and D(q) containing p and q = n −p disks, respectively.
Then the union of the disks in D(p) contains p eigenvalues, and the union of the
disks in D(q) contains q eigenvalues, counting multiplicities. In particular, if one
disk is disjoint from all the others, it contains exactly one eigenvalue. And, if all of
the disks are disjoint, then each contains exactly one eigenvalue.
Proof. We will proceed with a technique that is commonly known as a homotopy
argument. In particular, we construct a family of matrices B(ε), parameterized by
ε ∈[0, 1], such that we know the spectrum at ε = 0 completely. As ε increases
from 0 to 1, we can follow the eigenvalue trajectories as they continuously deform.
From this we will try to extract information for ε = 1. If we properly construct this
family, then B(1) = A and we will have proved the result.
Deﬁne the matrix B(ε) = [bi,j(ε)]n
i,j=1 as follows:
bi,j(ε) =
(
ai,i,
i = j,
εai,j,
i ̸= j.

202
Eigenvalue Problems
Then B(1) = A and B(0) = diag(a1,1, . . . , an,n). Each eigenvalue of B(0) is the
center of one of the Gershgorin disks of A. Thus, exactly p of the eigenvalues of
B(0) lie in the union of the disks in D(p).
The eigenvalues of the matrix B(ε) are the zeros of the characteristic polynomial
of B(ε). The coeﬃcients of this characteristic polynomial are continuous functions
of the parameter ε, and, in turn, the zeros of the polynomial are continuous
functions of ε. We will accept this rather deep fact without proof; see, for example,
[38].
As ε increases from 0 to 1, the eigenvalues of B(ε) move in continuous paths
in the complex plane. Since the degree of the characteristic polynomial of B(ε)
is always exactly n, none of the zeros of the characteristic polynomial diverge to
inﬁnity. At the same time, the radii of the Gershgorin disks increase from 0 to Ri,
respectively. In particular, it is easy to see that they increase as
Rε,i = εRi,
0 ≤ε ≤1,
i = 1, . . . , n.
Since p of the eigenvalues of B(ε) lie in the union of the disks in D(p) when
ε = 0, and these disks are disjoint from those in D(q), these p eigenvalues will stay
within the union of the disks in D(p) for all values of ε, 0 ≤ε ≤1.
Theorem 8.4 (almost diagonal). Let n ≥2. Suppose that in the matrix A ∈Cn×n
all oﬀ-diagonal elements are smaller in modulus than ε > 0, i.e.,
|ai,j| < ε,
1 ≤i, j ≤n,
i ̸= j.
Suppose that there is δ > 0 such that, for some r ∈{1, . . . , n}, the diagonal
element ar,r satisﬁes
|ar,r −ai,i| > δ,
∀i ̸= r.
Then, provided that
ε <
δ
2(n −1),
there is an eigenvalue λ ∈σ(A) such that
|λ −ar,r| < 2(n −1)
δ
ε2 <
δ
2(n −1).
Proof. Let κ > 0. Deﬁne K = [ki,j] ∈Cn×n via
ki,j =
(
κ,
i = j = r,
δi,j,
otherwise.
Deﬁne the similar matrix Aκ = KAK−1. For example, if r = 3, n = 4,
A =


a1,1
a1,2
a1,3
a1,4
a2,1
a2,2
a2,3
a2,4
a3,1
a3,2
a3,3
a3,4
a4,1
a4,2
a4,3
a4,4

∈C4×4,

8.2 Stability
203
then
Aκ =


a1,1
a1,2
κ−1a1,3
a1,4
a2,1
a2,2
κ−1a2,3
a2,4
κa3,1
κa3,2
a3,3
κa3,4
a4,1
a4,2
κ−1a4,3
a4,4

.
The Gershgorin disk of Aκ, with respect to row r, has its center at ar,r, and its
radius is
Rκ,r =
n
X
j=1
j̸=r
|κai,j| = κ
n
X
j=1
j̸=r
|ai,j| < κ
n
X
j=1
j̸=r
ε = κ(n −1)ε.
The disk of Aκ corresponding to row i ̸= r has ai,i as its center, and its radius is
Rκ,i =
n
X
j=1
j̸=i
j̸=r
|ai,j| + κ−1|ai,r| < (n −2)ε + ε
κ.
Now pick κ = 2ε
δ . Then
Rκ,r < 2ε2(n −1)
δ
,
Rκ,i < δ
2 + (n −2)ε,
i ̸= r.
Therefore, for i ̸= r,
Rκ,r + Rκ,i < 2ε2(n −1)
δ
+ δ
2 + (n −2)ε < ε + δ
2 + (n −2)ε < δ
on the assumption that ε <
δ
2(n−1). Finally, we have
δ < |ar,r −ai,i|,
but Rκ,r +Rκ,i < δ. Therefore, the two disks Dκ,r and Dκ,i must be disjoint. Hence,
there is an eigenvalue λ ∈σ(Aκ) = σ(A) with λ ∈Dκ,r. In other words,
|λ −ar,r| ≤Rκ,r < 2ε2(n −1)
δ
<
δ
2(n −1).
8.2
Stability
In this section, we give a couple of results that are concerned with the stability of
eigenvalue computation.
Theorem 8.5 (Bauer–Fike5). Suppose that A ∈Cn×n is diagonalizable, i.e., there
is an invertible matrix P ∈Cn×n and a diagonal matrix D = diag(λ1, . . . , λn) such
that A = PDP−1. Suppose that λ ̸∈σ(A) = {λi}n
i=1 is an eigenvalue of the
perturbed matrix A + δA with δA ∈Cn×n. Then, for any p ∈[1, ∞], we have
n
min
i=1 |λ −λi| ≤∥P∥p
P−1
p ∥δA∥p = κp(P) ∥δA∥p .
5 Named in honor of the German mathematician Friedrich Ludwig Bauer (1924–2015) and the
American mathematician and computer scientist Charles Theodore Fike (1933–2021).

204
Eigenvalue Problems
Proof. There is some x ∈Cn
⋆such that
(A + δA)x = λx.
It follows that
(λIn −A)x = δAx
and
P(λIn −D)
 P−1x

= δAP(P−1x).
Note that (λIn −D) is invertible, since λ ̸∈σ(A). Hence,
P−1x

p ≤
(λIn −D)−1
p
P−1δAP

p
P−1x

p .
Observe that, for any p ∈[1, ∞],
(λIn −D)−1
p =
n
max
i=1
1
|λ −λi| =
1
minn
i=1 |λ −λi|.
Therefore,
n
min
i=1 |λ −λi| ≤∥P∥p
P−1
p ∥δA∥p ,
as desired.
Corollary 8.6 (Hermitian matrix I). If A ∈Cn×n is Hermitian, then
n
min
i=1 |λ −λi| ≤∥δA∥2 .
Proof. If A is Hermitian, it is unitarily diagonalizable. Since the 2-norm of a unitary
matrix equals one, the result follows.
Corollary 8.7 (Hermitian matrix II). Suppose that A ∈Cn×n is Hermitian, λ ∈R,
and λ is closest to the eigenvalue λr, i.e.,
r = argmin
1≤i≤n
|λi −λ|.
Then, setting λ = λr + δλr,
|δλr| ≤∥δA∥2 .
Proof. See Problem 8.6
Remark 8.8 (stability). In other words, for Hermitian matrices, the eigenvalue
problem is stable to perturbations in the coeﬃcient matrix A. In the general case,
the eigenvalue problem is stable, but a type of condition number, ∥P∥p
P−1
p,
appears on the left-hand side.
Theorem 8.9 (eigenvector perturbation). Suppose that A ∈Cn×n is Hermitian
and σ(A) = {λ1, . . . , λn} ⊂R. Suppose that x ∈Cn
⋆and λ ∈C are given. Deﬁne
w = Ax −λx.

8.3 The Rayleigh Quotient for Hermitian Matrices
205
Then
min
1≤i≤n |λi −λ| ≤∥w∥2
∥x∥2
.
Proof. See Problem 8.7.
Remark 8.10 (stability). The last result can be interpreted as follows: eigenvalue
calculations are stable with respect to perturbations in the associated eigenvector.
8.3
The Rayleigh Quotient for Hermitian Matrices
In light of the stability results presented above, in this section, let us assume that
A is Hermitian.
Deﬁnition 8.11 (Rayleigh quotient6). Suppose that A ∈Cn×n is Hermitian. The
Rayleigh quotient of x ∈Cn
⋆is
R(x) = (Ax, x)2
(x, x)2
.
Proposition 8.12 (properties of the Rayleigh quotient). Suppose that A ∈Cn×n
is Hermitian with spectrum σ(A) = {λi}n
i=1 ⊂R, where the following ordering is
imposed:
λ1 ≤λ2 ≤· · · ≤λn−1 ≤λn.
Then
1. R(x) ∈R for all x ∈Cn
⋆.
2. For all x ∈Cn
⋆,
n
min
j=1 λj ≤R(x) ≤
n
max
j=1 λj.
3. R(x) = λk if and only if x is an eigenvector associated with λk.
4. For a ﬁxed x ∈Cn
⋆, the function
Q(α) = ∥Ax −αx∥2
2,
∀α ∈C
has a unique global minimum; in fact, the Rayleigh quotient is the unique
minimizer
R(x) = argmin
α∈C
∥Ax −αx∥2
2.
Proof. See Problem 8.8.
To gain insight into what the previous result is saying, let us suppose that
A ∈Rn×n is symmetric. In light of the minimality property described in the last
6 Named in honor of the British physicist John William Strutt (Lord Rayleigh) (1842–1919).

206
Eigenvalue Problems
result, let us consider how the Rayleigh quotient behaves under perturbations of
the vector x. It is not diﬃcult to show that
∇R(x) =
2
(x, x)2
(Ax −R(x)x).
Thus, if (λ, w) is an eigenpair of A, we have
∇R(w) =
2
(w, w)2
(Aw −R(w)w)2 = 0,
so that eigenvectors are stationary points of the Rayleigh quotient. In light of this,
using Taylor’s Theorem (B.54), we observe that
R(x) −R(w) = (∇R(w), x −w)2 + O(∥x −w∥2
2) = O(∥x −w∥2
2),
x →w,
so that the Rayleigh quotient is a quadratically accurate approximation of λ.
Let us make the last statement rigorous before moving on.
Theorem 8.13 (eigenvalue estimate). Suppose that A ∈Cn×n is Hermitian with
spectrum σ(A) = {λi}n
i=1 ⊂R and an associated orthonormal basis of eigenvectors
{w i}n
i=1. Suppose that x ∈Cn
⋆is a vector with the property that
∥x −w k∥2 < ε,
∥x∥2 = 1
for some positive number ε. Then
|R(x) −λk| < 2ρ(A)ε2.
Proof. Suppose that x ̸= w k. There are unique constants αi ∈C such that
x =
n
X
j=1
αjw j,
∥x∥2
2 =
n
X
j=1
|αj|2 = 1.
It follows from orthonormality that
0 < ∥x −w k∥2
2 =
n
X
j=1
j̸=k
|αj|2 + |αk −1|2.
Therefore, using our assumptions,
0 <
n
X
j=1
j̸=k
|αj|2 + |αk −1|2 = ∥x −w k∥2
2 < ε2.
(8.1)
Since ∥x∥2
2 = 1, it follows from (8.1) that
0 ≤
n
X
j=1
j̸=k
|αj|2 = 1 −|αk|2 < ε2.
(8.2)
Finally,
R(x) =
Pn
j=1 λj|αj|2
Pn
j=1 |αj|2
= λk|αk|2 +
n
X
j=1
j̸=k
λj|αj|2,

8.4 Power Iteration Methods
207
so that
|R(x) −λk| =

λk
 |αk|2 −1

+
n
X
j=1
j̸=k
λj|αj|2

≤|λk|
 1 −|αk|2
+
n
X
j=1
j̸=k
|λj| · |αj|2
≤
n
max
j=1 |λj|
 1 −|αk|2
+
n
max
j=1 |λj|
n
X
j=1
j̸=k
|αj|2
≤ρ(A)ε2 + ρ(A)ε2
= 2ρ(A)ε2,
where we used (8.2).
This last property motivates the so-called power iterations to ﬁnd the largest
eigenvalue, which will be our next topic of discussion.
8.4
Power Iteration Methods
Deﬁnition 8.14 (power iteration). Suppose that A ∈Cn×n and q ∈Cn
⋆is given.
The power method is an algorithm that generates a sequence of vectors {v k}∞
k=0 ⊂
Cn
⋆according to the following recursive formula:
v 0 =
q
∥q∥2
.
For k ≥1,
qk = Av k−1,
and, provided that qk ̸= 0,
v k =
qk
∥qk∥2
.
If qk = 0, the algorithm terminates at step k.
Remark 8.15 (terminology). The name of the method becomes clear once we
realize that
v k =
1
∥qk∥2
qk =
1
∥qk∥2
Av k−1 = · · · = ckAkv 0.
The convergence of the method is as follows.
Theorem 8.16 (convergence of the power iteration). Let A ∈Cn×n be Hermitian
and q ∈Cn
⋆be given. Suppose that the spectrum σ(A) = {λi}n
i=1 has the ordering
0 ≤|λ1| ≤|λ2| ≤· · · ≤|λn−1| < |λn|.

208
Eigenvalue Problems
Assume that {w i}n
i=1 is the associated basis of orthonormal eigenvectors of A.
If (q, w n)2 ̸= 0, where w n is the eigenvector corresponding to the dominant
eigenvalue λn, then, for some constant C > 0,
∥v k −skw n∥2 ≤C

λn−1
λn

k
,
|R(v k) −λn| ≤2|λn|C2

λn−1
λn

2k
,
when k is suﬃciently large. Here, sk is a sequence of modulus one complex numbers.
Proof. The vector v 0 is obtained by normalizing q. Let us then expand v 0 in the
basis of eigenvectors: there exist unique constants αj ∈C such that
v 0 =
n
X
j=1
αjw j.
From our assumptions, αn ̸= 0 and Pn
j=1 |αj|2 = 1. Now
q1 = Av 0 =
n
X
j=1
λjαjw j.
From this, it follows that
v 1 =
1
qPn
j=1 |αj|2λ2
j
n
X
j=1
λjαjw j.
Proceeding this way, we observe that
v k =
1
qPn
j=1 |αj|2λ2k
j
n
X
j=1
λk
j αjw j
=
1
qPn
j=1 |αj|2λ2k
j

λk
nαnw n +
n−1
X
j=1
λk
j αjw j


= αnλk
n
|αnλkn|
1
r
1 + Pn−1
j=1
 αj
αn

2 
λj
λn
2k

w n +
n−1
X
j=1
αj
αn
 λj
λn
k
w j

.
Deﬁne sk =
αnλk
n
|αnλkn|. Given the assumptions on σ(A), we observe, for j = 1, . . . , n−1,
that
 λj
λn
k
→0,
k →∞.
It is clear that
1
sk
v k −w n →0,
k →∞.
We leave it to the reader as an exercise to prove the estimate
∥v k −skw n∥2 ≤C

λn−1
λn

k

8.4 Power Iteration Methods
209
for some C > 0, provided that k is suﬃciently large. If this holds, Theorem 8.13
yields the second-order convergence rate of the eigenvalue approximations.
8.4.1
Inverse Iteration
We just showed how, using the power iteration method, we can approximate the
dominant eigenvalue, provided that it is isolated. A small modiﬁcation of this idea
allows us to approximate any other isolated eigenvalue. Let us begin with an auxiliary
result, whose proof is straightforward.
Proposition 8.17 (shifts). Suppose that A ∈Cn×n and µ ∈C has the the property
that µ ̸∈σ(A). Then (A −µI)−1 exists, and (λ, w) is an eigenpair of A if and only
if
 (λ −µ)−1, w

is an eigenpair of (A −µI)−1.
Proof. See Problem 8.11.
This motivates the following observation: if µ is close to λr ∈σ(A), then (λr −
µ)−1 is much larger in modulus than (λj −µ)−1, for any other eigenvalue. Thus,
to approximate λr, we can apply power iterations to the matrix (A −µI)−1. This
is the idea of the inverse iteration method.
Deﬁnition 8.18 (inverse iteration method). Suppose that A ∈Cn×n, q ∈Cn
⋆, and
µ ∈C are given. Assume that A −µI is invertible. The inverse iteration method is
an algorithm that generates a sequence of vectors {v k}∞
k=0 ∈Cn
⋆according to the
following recursive formula:
v 0 =
q
∥q∥2
.
For k ≥1,
qk = (A −µI)−1 v k−1,
and, provided that qk ̸= 0,
v k =
qk
∥qk∥2
.
If qk = 0, the algorithm terminates at step k.
The convergence properties of this algorithm are as follows.
Theorem 8.19 (convergence of the inverse iteration). Suppose that A ∈Cn×n
is Hermitian with spectrum σ(A) = {λi}n
i=1 ⊂R. Let {w i}n
i=1 be an associated
orthonormal basis of eigenvectors of A. Suppose that µ ∈C is given with the
property that µ ̸∈σ(A). Deﬁne
λr =
n
argmin
j=1
|λj −µ|,
λs =
n
argmin
j=1
j̸=r
|λj −µ|.
Assume that
|λr −µ| < |λs −µ| ≤|λj −µ|

210
Eigenvalue Problems
for all j ∈{1, . . . , n}\{r, s}. If q ∈Cn
⋆is given with the property that (q, w r)2 ̸= 0,
then the inverse iteration converges. Moreover, we have the following convergence
estimates: there is some C > 0 such that
∥v k −skw r∥2 ≤C

µ −λr
µ −λs

k
,
|R(v k) −λr| ≤2ρ(A)C2

µ −λr
µ −λs

2k
,
provided that k is suﬃciently large. As before, sk is a sequence of modulus one
complex numbers.
Proof. See Problem 8.12.
8.4.2
Deﬂation
The next result addresses the issue of computing a next-to-dominant eigenvalue.
Theorem 8.20 (deﬂation). Suppose that A ∈Cn×n is Hermitian and its spectrum
is denoted σ(A) = {λ1, . . . , λn} ⊂R. Let {w 1, . . . , w n} be an orthonormal basis of
eigenvectors of A with Aw k = λkw k, for k = 1, . . . , n. Assume that the eigenvalues
of A are ordered as follows:
0 ≤|λ1| ≤|λ2| ≤· · · ≤|λn−2| < |λn−1| < |λn|.
Assume that λn is known and that v 0 ∈Cn
⋆is such that w H
n−1v 0 ̸= 0. Then the
sequence
v k+1 = Av k,
y k+1 = v k+1 −λnv k
satisﬁes
y H
k+1y k
y H
k y k
→λn−1,
k →∞.
Proof. There exist unique constants αj ∈C, j = 1, 2, . . . , n such that v 0 =
Pn
j=1 αjw j. By assumption, αn−1 ̸= 0. We observe that v k
= Akv 0 and,
consequently,
y k = Akv 0 −λnAk−1v 0 = Ak−1(A −λnIn) v 0 =
n−1
X
j=1
αjλk−1
j
(λj −λn) w j.
Therefore,
y H
k+1y k
y H
k y k
=
Pn−1
j=1 |αj|2λ2k−1
j
(λj −λn)2
Pn−1
j=1 |αj|2λ2(k−1)
j
(λj −λn)2
=
Pn−1
j=1
 αj
αn−1

2
λj

λj
λn−1
2(k−1)
λj−λn
λn−1−λn
2
Pn−1
j=1
 αj
αn−1

2
λj
λn−1
2(k−1)
λj−λn
λn−1−λn
2
=
λn−1 + Pn−2
j=1
 αj
αn−1

2
λj

λj
λn−1
2(k−1)
λj−λn
λn−1−λn
2
1 + Pn−2
j=1
 αj
αn−1

2
λj
λn−1
2(k−1)
λj−λn
λn−1−λn
2
→λn−1,
as k →∞, since

λj
λn−1
2
< 1 for each j = 1, 2, . . . , n −2.

8.5 Reduction to Hessenberg Form
211
8.5
Reduction to Hessenberg Form
We have seen that it is impossible, in general, to reduce a square matrix to a
triangular or diagonal matrix through similarity transformations in a ﬁnite number
of steps (via a direct method). We will now see that it is possible to reduce a matrix
to a so-called Hessenberg matrix, one that has zeros below the ﬁrst sub-diagonal.
Deﬁnition 8.21 (Hessenberg matrix7). The square matrix A = [ai,j] ∈Cn×n is
called a lower Hessenberg matrix or is said to have lower Hessenberg form if
and only if ai,j = 0 for all 1 ≤i, j ≤n satisfying i ≥j + 2. A is said to be upper
Hessenberg if and only if A⊺is lower Hessenberg.
We want to come up with a similarity transformation that eﬀects the following
change, where by × we denote nonzero entries of the result:
A →QHAQ =


×
×
×
· · ·
×
×
×
×
...
...
0
×
×
...
×
...
...
...
...
×
0
· · ·
0
×
×


.
We now explain how this is achieved.
Suppose that Hn−1 is the Householder matrix that leaves the ﬁrst row of A
unchanged and introduces zeros below the second row of the ﬁrst column:
A →HH
n−1A =


×
×
×
· · ·
×
⊗
⊗
⊗
· · ·
⊗
0
⊗
⊗
...
...
...
...
...
...
⊗
0
⊗
⊗
· · ·
⊗


,
where, as before, by × we denote entries of the matrix A that did not change
and by ⊗those that did change. When we calculate HH
n−1AHn−1 it leaves the ﬁrst
column unchanged, so that we obtain
A →HH
n−1A =


×
×
×
· · ·
×
⊗
⊗
⊗
· · ·
⊗
0
⊗
⊗
...
...
...
...
...
...
⊗
0
⊗
⊗
· · ·
⊗


→HH
n−1AHn−1


×
⊗
⊗
· · ·
⊗
⊗
⊕
⊕
· · ·
⊕
0
⊕
⊕
...
...
...
...
...
...
⊕
0
⊕
⊕
· · ·
⊕


.
7 Named in honor of the German mathematician and engineer Karl Adolf Hessenberg
(1904–1959).

212
Eigenvalue Problems
The symbol ⊕indicates that the entry has been changed twice. Repeating this
idea we can reduce any matrix to Hessenberg form. The algorithm is presented in
Listing 8.1.
If A ∈Cn×n is Hermitian, then the Hessenberg transformation results in a
tridiagonal Hermitian matrix similar to A. We present a result on the existence
of this transformation for the Hermitian case. We leave it to the reader to prove
the existence in the more general setting.
Theorem 8.22 (existence). Suppose that A ∈Cn×n is Hermitian. There exists a
unitary matrix Q ∈Cn×n, the product of n −2 Householder matrices,
Q = Hn−1 · · · H2,
where Hk ∈Cn×n is a Householder matrix such that
QHAQ = T,
where T ∈Cn×n is a Hermitian tridiagonal matrix.
Proof. Let us express A as
A =
α
bH
b
C

,
where α ∈C, b ∈Cn−1, and C ∈C(n−1)×(n−1) is Hermitian. To simplify notation,
we deﬁne
Ek
1 = span{e1} ⊂Ck.
If b ∈Cn−1
⋆
, there is a Householder matrix Hn−1 ∈C(n−1)×(n−1) such that
Hn−1b ∈En−1
1
.
If b = 0, then Hn−1b = 0 ∈En−1
1
, regardless of our choice for Hn−1, and so for the
sake of deﬁniteness we take Hn−1 = In−1.
Deﬁne
Hn,n−1 =
1
0H
0
Hn−1

∈Cn×n.
(8.3)
This is also a Householder matrix, as we have seen before. It follows that
HH
n,n−1AHn,n−1 =
1
0H
0
HH
n−1
 α
bH
b
C
 1
0H
0
Hn−1

=
α
d H
d
D

,
where d = HH
n−1b = Hn−1b ∈En−1
1
and
D = HH
n−1CHn−1 ∈C(n−1)×(n−1),
which is clearly a Hermitian matrix. The proof from this point is by induction.
(k = 3) Suppose that A ∈C3×3. The matrix
HH
3,2AH3,2 ∈C3×3
is already tridiagonal, since
H3,2b = d ∈E2
1.

8.5 Reduction to Hessenberg Form
213
This case follows upon taking Q3 = H3,2. Observe that, for any f ∈E3
1, i.e.,
f = [λ, 0, 0]⊺, we have
QH
3 f = HH
3,2f = H3,2f = f .
(k = m −1) For the induction hypothesis, let us assume that, for any D ∈
C(m−1)×(m−1) that is Hermitian, there is a unitary matrix Qm−1 ∈C(m−1)×(m−1),
the product of Householder matrices
Qm−1 = Hm−1,m−2 · · · Hm−1,2,
such that the product
QH
m−1DQm−1 = Tm−1 ∈C(m−1)×(m−1)
is Hermitian and tridiagonal. Furthermore, let us assume that
QH
m−1f ∈Em−1
1
,
∀f ∈Em−1
1
.
(k = m) Suppose that A ∈Cm×m is Hermitian. Let us inﬂate each Hm−1,k ∈
C(m−1)×(m−1) from the induction hypothesis step by
Hm,k =
1
0H
0
Hm−1,k

∈Cm×m
for k = 2, . . . , m −2. For k = m −1, deﬁne Hm,m−1 as in (8.3). Next, set
Qm = Hm,m−1Hm,m−2 · · · Hm,2.
It follows that
QH
mAQm = Hm,2 · · · Hm,m−2Hm,m−1AHm,m−1Hm,m−2 · · · Hm,2
=
1
0H
0
QH
m−1
 α
d H
d
D
 1
0H
0
Qm−1

=

α
d HQm−1
QH
m−1d
Tm−1

= Tm.
Since QH
m−1d ∈Em−1
1
, Tm must be tridiagonal.
Finally, for any f ∈Em
1 , we have QH
mf ∈Em
1 , since the (1, 1) entry of Qm is 1 and
the remaining entries in the ﬁrst column of Qn are zeros. The proof by induction
is complete.
Next, we have a simple, and quite amazing, criterion for a Hermitian tridiagonal
matrix to have distinct eigenvalues.
Theorem 8.23 (distinct eigenvalues). Suppose that A ∈Cn×n is Hermitian and
tridiagonal. Assume that its super- and sub-diagonal entries are all nonzero. Then
the eigenvalues of A must be distinct.
Proof. See Problem 8.13.
Of course, it is easy to show that the converse is not true. Can you think of a
quick example that shows the converse is false?

214
Eigenvalue Problems
8.6
The QR Method
Deﬁnition 8.24 (QR iteration method). Let A ∈Cn×n. The QR iteration method
is a recursive algorithm for computing the sequence {Ak}∞
k=0 ⊂Cn×n according to
the following rules.
1. Set A0 = A.
2. For k = 0, 1, 2, . . ., given Ak,
a. Compute the factorization
Qk+1Rk+1 = Ak,
where Qk+1 ∈Cn×n is unitary and Rk+1 ∈Cn×n is upper triangular.
b. Deﬁne the next iterate as
Ak+1 = Rk+1Qk+1.
Listing 8.2 presents an implementation of the QR iteration method.
Recall that every square matrix has a (not necessarily unique) Schur decompo-
sition
A = QHUQ,
where Q is unitary and U is upper triangular. We will show that, under suitable
assumptions, the QR iteration method computes the factor U in a Schur
decomposition of A as the limit of the sequence {Ak}∞
k=0. To minimize the cost
of the QR iteration, ﬁrst one converts the matrix A to Hessenberg form. This
preserves the spectrum, and the QR iteration will preserve this form, among other
properties.
Proposition 8.25 (invariance). Let A ∈Cn×n. Suppose that {Ak}∞
k=0 is computed
according to the QR iteration method. Then we have:
1. The matrix Ak is similar to Ak−1.
2. If Ak−1 is Hermitian, so is Ak.
3. If Ak−1 is Hessenberg, so is Ak.
4. If Ak−1 is tridiagonal, so is Ak.
Proof. Since Rk = QH
k Ak−1 and Ak = RkQk, it follows that
Ak = RkQk = QH
k Ak−1Qk.
Since Qk is unitary, this shows that Ak is similar to Ak−1, and that Ak is Hermitian
if Ak−1 is Hermitian as well.
The last two statements are left for the reader as an exercise; see Problem
8.14.

8.6 The QR Method
215
Example 8.3
Consider the symmetric, tridiagonal matrix
A =


9
17
0
0
0
17
3
18
0
0
0
18
20
2
0
0
0
2
1
8
0
0
0
8
16


.
Observe that the QR factorization, QR = A, has factors
Q =


−0.467 888
0.533 293
0.700 794
−0.008 036
−0.074 181
−0.883 788
−0.282 332
−0.371 009
0.004 254
0.039 272
0
0.797 425
−0.600 026
0.006 880
0.063 515
0
0
−0.105 874
−0.107 091
−0.988 596
0
0
0
−0.994 184
0.107 696


and
R =


−19.235 384
−10.605 455
−15.908 182
0
0
0
22.572 645
10.866 537
1.594 851
0
0
0
−18.890 423
−1.305 926
−0.846 990
0
0
0
−8.046 802
−16.763 666
0
0
0
0
−6.185 635


to six decimal digits of precision. Notice that we have not demanded that R
has positive diagonal components in this factorization, though that can be easily
remedied. In any case, note the placement of the zeros in the factor matrices. Can
you show that these entries are generically zero, based on the structure of A?
Recall that, in the QR algorithm, A1 = RQ, and we obtain
A1 =


18.372 973
−19.949 431
0
0
0
−19.949 431
2.292 279
−15.063 703
0
0
0
−15.063 703
11.473 011
0.851 945
0
0
0
0.851 945
17.527 905
6.149 659
0
0
0
6.149 659
−0.666 167


,
again showing six decimal digits of precision. The structure of A is preserved in
this ﬁrst step of the QR algorithm; namely, A1 is symmetric and tridiagonal, as
predicted by Proposition 8.25.
This leads us to the main result of convergence. Under suitable assumptions, for
a generic matrix A, the sequence {Ak}∞
k=1 generated by the QR iteration converges
to an upper triangular matrix that is similar to A.
Theorem 8.26 (convergence I). Suppose that A ∈Cn×n is invertible and all its
eigenvalues are distinct in modulus, i.e.,
|λ1| > |λ2| > · · · > |λn| > 0.

216
Eigenvalue Problems
Let P ∈Cn×n be an invertible matrix such that
A = PDP−1,
where D = diag(λ1, λ2, . . . , λn). If P−1 has an LU factorization — i.e., there exists
a unit lower triangular matrix L ∈Cn×n and an upper triangular matrix U ∈Cn×n
such that P−1 = LU (Theorem 3.7) — then the sequence of matrices {Ak}∞
k=1
produced by the QR iteration method is such that
lim
k→∞[Ak]i,i = λi,
1 ≤i ≤n,
and
lim
k→∞[Ak]i,j = 0,
1 ≤j < i ≤n.
Proof. Let us deﬁne, for k ≥1,
Qk = Q1 · · · Qk,
Rk = Rk · · · R1.
With this notation we observe that
Ak+1 = QH
k AkQk
= QH
k QH
k−1Ak−1Qk−1Qk
...
= QH
k · · · QH
1 A0Q1 · · · Qk
= QH
k AQk.
In addition,
A = A0 = Q1R1 = Q1R1,
A2 = Q1R1Q1R1 = Q1(R1Q1)R1 = Q1A1R1 = Q1(Q2R2)R1 = Q2R2,
and
A3 = Q1(R1Q1)R1Q1R1
= Q1A1R1Q1R1
= Q1Q2R2(R1Q1)R1
= Q1Q2R2A1R1
= Q1Q2R2(R1Q1)R1
= Q1Q2(R2Q2)R2R1
= Q1Q2A2R2R1
= Q1Q2Q3R3R2R1
= Q3R3.
Continuing in this fashion, we have
Ak = Q1R1 · · · Q1R1 = · · · = Q1 · · · QkRk · · · R1 = QkRk.

8.6 The QR Method
217
Now, since A has distinct eigenvalues, it is diagonalizable. Therefore, there is an
invertible matrix P such that A = PDP−1. Assume that P−1 has an LU factorization:
P−1 = LU.
According to Theorem 3.14, this factorization is unique, if it exists. Every matrix
has a unique QR factorization of the following form:
P = QR,
where Q ∈Cn×n is unitary and R ∈Cn×n is upper triangular with positive diagonal
entries. Thus,
Ak = PDkP−1 = QR
 DkLD−k
DkU.
Observe that, since L = [ℓi,j] is unit lower triangular,

DkLD−k
i,j =







0,
i < j,
1,
i = j,

λi
λj
k
ℓi,j,
i > j.
Since limk→∞

λi
λj
k
= 0, it follows that
lim
k→∞
 DkLD−k
= I.
Now let us set
DkLD−k = I + Fk,
where
lim
k→∞Fk = O.
It follows that
R
 DkLD−k
=
 I + RFkR−1
R.
Since Fk →O, there is some K ∈N such that, for every k ≥K,
RFkR−1
∞< 1.
Thus, owing to Theorem 4.19, if k ≥K, I + RFkR−1 is invertible, and therefore
admits a unique QR factorization:
I + RFkR−1 = ˜Qk ˜Rk,
where ˜Qk ∈Cn×n is unitary and ˜Rk ∈Cn×n is upper triangular with positive diagonal
entries. Since the sequence
˜Qk
	∞
k=1 is bounded — in particular,
˜Qk

2 = 1 —
owing to Theorem B.9, there is a convergent subsequence
˜Qkm
	∞
m=1 ⊆
˜Qk
	∞
k=1.
Set
˜Q = lim
m→∞
˜Qkm.
It is not diﬃcult to see that ˜Q must be unitary. Next, we observe that
˜Rkm = ˜QH
km
 I + RFkmR−1
→˜R,
where ˜R must be upper triangular. Its diagonal elements must be nonnegative.

218
Eigenvalue Problems
Taking the limit of the subsequence that we constructed, we ﬁnd
I = ˜Q˜R,
which implies that the diagonal elements of ˜R must be positive. By uniqueness of
the QR factorization, I = ˜Q = ˜R. We leave it to the reader to prove that, in fact,
the original sequences
˜Qk
	
k≥1 and
˜Rk
	
k≥1 actually converge and
˜Qk, →I,
˜Rk →I,
k →∞.
Taking into account all the diﬀerent factorizations we have performed we observe
that we have
Ak = QkRk =
 Q˜Qk
  ˜RkRDkU

.
Note that Q˜Qk is unitary and ˜RkRDkU is upper triangular. Furthermore, Qk is
unitary and Rk is upper triangular. We do not assert here that the upper triangular
matrices have positive diagonal entries. Therefore, there is a diagonal matrix Sk =
diag

s(k)
1 , . . . , s(k)
n

with |s(k)
i
| = 1, for i = 1, . . . , n, such that
Qk = Q˜QkSk.
This matrix, eﬀectively, accounts for the loss of uniqueness.
Finally, some simple manipulations reveal that
Ak+1 = QH
k AQk
= QH
k PDP−1Qk
= QH
k QRDR−1QHQk
= SH
k ˜QH
k QHQRDR−1QHQ˜QkSk
= SH
k ˜QH
k RDR−1 ˜QkSk.
Since limk→∞˜Qk = I,
lim
k→∞
 ˜QH
k RDR−1 ˜Qk

= RDR−1 =


λ1
×
· · ·
×
0
λ2
...
...
...
...
...
×
0
· · ·
0
λn


.
Deﬁne
Dk = ˜QH
k RDR−1 ˜Qk.
Since the matrix Sk is diagonal,
[Ak+1]i,j = s(k)
i
s(k)
j
[Dk]i,j .
Consequently,
[Ak+1]i,i = [Dk]i,i →λi,
k →∞.
The proof is thus complete.

8.6 The QR Method
219
Remark 8.27 (convergence). Observe that the previous result says nothing about
the convergence of [Ak]i,j for j > i. Notice also that the order of the eigenvalues
is preserved.
When the matrix is Hermitian, the previous result can be reﬁned.
Theorem 8.28 (convergence II). Suppose that the QR iteration method is applied
to a Hermitian matrix A ∈Cn×n, whose eigenvalues satisfy
|λ1| > |λ2| > · · · > |λn|
and whose corresponding unitary eigenvector matrix Q has all nonsingular leading
principal sub-matrices. Then, as k →∞, the matrices Ak converge linearly, with
constant
max
j

λj+1
λj
 ,
to Λ = diag(λ1, λ2, . . . , λn) and Qk converges, with the same rate, to Q.
Proof. Extend the previous proof by using the fact that the matrix A is unitarily
diagonalizable, i.e., A = QΛQH, and Q has an LU factorization. The details are left
for Problem 8.15.
We can give a slightly diﬀerent proof in the case that A is Hermitian positive
deﬁnite (HPD), by taking advantage of the Cholesky factorization.
Theorem 8.29 (convergence III). Suppose that A ∈Cn×n is HPD. Then the
sequence of matrices {Ak}∞
k=1 produced by the QR iteration converges to a diagonal
matrix D = diag(λn, . . . , λ1), where
0 < λ1 ≤· · · ≤λn
are the eigenvalues of A.
Proof. Suppose that B = A2, so that B is HPD. Consider the following sequence:
set B0 = B and, for all k ≥0, if Bk = UH
k+1Uk+1 is the unique Cholesky factorization
of Bk, then
Bk+1 = Uk+1UH
k+1.
It is left to the reader as a simple exercise to show that if Bk is HPD, then Bk+1
is always HPD; therefore, it will possess a unique Cholesky factorization. Here,
Uk ∈Cn×n is an upper triangular matrix with strictly positive real diagonal entries.
For the QR iteration method, recall that A0 = A and, for k ≥1, if Ak−1 = QkRk,
then
Ak = RkQk.
Each Ak is unitarily equivalent to A and is HPD. Furthermore,
A2
k = AH
k Ak = RH
k+1QH
k+1Qk+1Rk+1 = RH
k+1Rk+1
is a Cholesky decomposition of A2
k and
A2
k+1 = Ak+1AH
k+1 = Rk+1Qk+1QH
k+1RH
k+1 = Rk+1RH
k+1.

220
Eigenvalue Problems
It follows by uniqueness that
Bk = A2
k,
Uk = Rk,
∀k = 1, 2, . . . .
Also, it must be that each Bk is unitarily equivalent to B = A2. In particular, since
Rk+1 = QH
k+1RkQk,
Bk+1 = QH
k+1RkQkQH
k RH
k Qk+1 = QH
k+1BkQk+1,
k = 0, 1, . . . .
Now, since each Rk is upper triangular,
[Bk]i,j =
iX
m=1
[Rk+1]m,i [Rk+1]m,j .
Similarly,
[Bk+1]i,j =
n
X
m=i
[Rk+1]i,m [Rk+1]j,m.
With these formulas we observe that, for 1 ≤m ≤n,
m
X
p=1
[Bk]p,p =
m
X
i=1
m
X
j=1
[Rk+1]i,j
2
and
m
X
p=1
[Bk+1]p,p =
m
X
i=1
n
X
j=1
[Rk+1]i,j
2 .
Thus, for 1 ≤m < n,
m
X
p=1
[Bk+1]p,p −
m
X
p=1
[Bk]p,p =
m
X
i=1
n
X
j=m+1
[Rk+1]i,j
2 ,
(8.4)
and, for m = n,
n
X
p=1
[Bk+1]p,p −
n
X
p=1
[Bk]p,p = 0,
since the traces of the matrices Bk and Bk+1 are equal.
Thus, we deduce that the sequences
( m
X
p=1
[Bk]p,p
)∞
k=1
(8.5)
are increasing for each m = 1, . . . , n. Since each Bk is unitarily equivalent to
B = A2,
∥Bk∥2 =
A2
2 ;
consequently, the elements of Bk must be bounded. Therefore, the sequences
in (8.5) must remain bounded. By the monotone convergence theorem (see

8.7 Computation of the SVD
221
Theorem B.7), the sequence of (8.5) converges for each m = 1, . . . , n. It must be
that the sequences

[Bk]p,p
	∞
k=1
(8.6)
converge for each p = 1, . . . , n. We denote their limits as the diagonal elements of
the matrix B∞, i.e.,
lim
k→∞[Bk]p,p = [B∞]p,p .
Since this is the case, (8.4) implies that all of the oﬀ-diagonal elements of Rk must
tend to zero. Thus, this is also the case for the oﬀ-diagonal elements of Bk. This
implies that B∞= D is a diagonal matrix. The theorem is proven.
8.6.1
QR with Shifts
We conclude by commenting that there is a variant of the QR iteration method,
presented in Listing 8.3, that is known as QR with shifts, which, in practice,
converges faster than the standard QR iteration. This topic is beyond the scope of
our text, and we refer the reader to [26, 34, 70, 96, 102] for more information.
8.7
Computation of the SVD
In this ﬁnal section concerning the computation of eigenvalues and eigenvectors,
we will study the computation of the singular value decomposition (SVD), as the
techniques are closely related to the QR iteration method described in the previous
section. The approaches rely upon certain properties of the SVD, which we will
recall as needed.
As a ﬁrst approach, we see that if A ∈Cm×n with m ≥n and A = UΣVH is an
SVD of A, then
AHA = VΣ⊺ΣVH
is a Schur (eigenvalue) decomposition of the matrix AHA; see Theorem 2.3. This
suggests the following approach.
1. Form the Hermitian matrix B = AHA.
2. Use the QR iteration method to ﬁnd a decomposition of the form VHBV = D =
diag(σ2
1, . . . , σ2
n), where {σi}n
i=1 are the singular values of A and V ∈Cn×n is
unitary.
3. Deﬁne Σ = diag(σ1, . . . , σn) ∈Rm×n.
4. Find a QR decomposition of AV ∈Cm×n to obtain UR = AV with U ∈Cm×m
unitary and R ∈Cm×n upper triangular.
5. From the computations above, we see that A = UΣVH is the desired SVD.
This approach is not satisfactory in practice, as it is rather sensitive to
perturbations. A more computationally robust approach is motivated by Problem
2.4. Let A ∈Cm×n with m ≥n and A = ˆUˆΣVH be a reduced SVD of A. We recall

222
Eigenvalue Problems
that the matrix V ∈Cn×n is unitary, ˆΣ = diag(σ1, . . . , σn) ∈Rn×n, and ˆU ∈Cm×n
has orthonormal columns. Let ˜U ∈Cm×(m−n) be such that
U = [ˆU ˜U] ∈Cm×m
is unitary. Deﬁne
C =
On×n
AH
A
Om×m

,
Q =
1
√
2
"
V
V
On×(m−n)
ˆU
−ˆU
√
2˜U
#
∈C(m+n)×(m+n),
where Ok×p ∈Rk×p is the zero matrix of size k×p. An easy and direct computation
shows that Q is unitary and that
QHCQ = diag
 ˆΣ, −ˆΣ, O(m−n)×(m−n)

.
Thus, the computation of the SVD of A can be obtained from the eigenvalue
decomposition of the Hermitian matrix C.
The last method is stable to perturbations, but dealing with the large matrix C is
problematical. To circumvent the need for computing with C, we use an algorithm
that is now known as the Golub–Kahan method.8
The Golub–Kahan algorithm proceeds in two stages.
1. Bidiagonalize A: Put A into upper bidiagonal form. This is obtained by applying
diﬀerent unitary transformations from the left and the right to A, so that
UH
1 AV1 = T =

˜T
O(m−n)×n

,
˜T =


×
×
×
×
×
...
...
×
×


∈Cn×n.
(8.7)
Notice that, unlike in a reduction to Hessenberg form, we are not applying
a similarity transformation to this matrix. This allows more ﬂexibility and the
possibility to reduce the matrix to upper bidiagonal form. The matrices U1 ∈
Cm×m and V1 ∈Cn×n are unitary and can be computed, for instance, as products
of Householder reﬂectors.
2. Compute the SVD of T: Find the SVD of the upper bidiagonal matrix
T = U2ΣVH
2 . From the previous computations,
A = U1U2ΣVH
2 VH
1 = UΣVH,
with U = U1U2, and V = V1V2 is an SVD for A.
Evidently, to realize the Golub–Kahan method, one must be able to (i)
bidiagonalize A and (ii) compute the SVD of the bidiagonal matrix T. The next
result guarantees that any matrix A can be put into upper bidiagonal form using
unitary matrices.
8 Named in honor of American mathematician Gene Howard Golub (1932–2007) and the
Canadian mathematician and computer scientist William Morton Kahan (1933–).

Problems
223
Lemma 8.30 (upper bidiagonal form). Suppose that A ∈Cm×n with m ≥n.
There exist unitary matrices U1 ∈Cm×m and V1 ∈Cn×n such that UH
1 AV1 is
upper bidiagonal, as in (8.7). In particular, U1 and V1 are products of Householder
reﬂectors.
Proof. The idea is to mimic the similarity transformation of a square matrix to
upper Hessenberg form. However, we have the added ﬂexibility that we are allowed
to use diﬀerent alternating Householder matrices on the right and left; see Problem
8.16.
The following result gives a way to reduce the computation of the SVD of
an upper bidiagonal matrix to the computation of the eigenpairs of a Hermitian
tridiagonal matrix.
Lemma 8.31 (reduction to eigenvalues). Let T ∈Cm×n with m ≥n be bidiagonal.
The matrix Z = THT is tridiagonal and Hermitian. Moreover, the singular values
of T are the square roots of the eigenvalues of Z. The right singular vectors of T
are the eigenvectors of Z.
Proof. See Problem 8.17.
The previous result implies that, by applying the QR iteration method to Z =
THT, one can compute the factors Σ and V2 in step 2 above. The left singular
vectors can then be computed by a QR factorization of TV2. There are variants of
this idea that do not require to form the matrix Z.
We refer the reader to [26, 34] for many more practical details, variants, and
convergence properties on the Golub–Kahan method.
Problems
8.1
A Frobenius matrix is a matrix that has the form
F =


a1
a2
· · ·
an−1
an
1
0
· · ·
0
0
0
1
...
...
...
...
...
...
0
0
0
· · ·
0
1
0


.
Find χF for a Frobenius matrix.
8.2
Let A ∈Cn×n have eigenvalues σ(A) = {λ1, . . . , λn}. Deﬁne
Ri =
n
X
j=1
j̸=i
|ai,j| ,
Cj =
n
X
i=1
i̸=j
|ai,j| .
a)
Prove that
σ(A) ⊆
n[
i=1
{z ∈C | |z −ai,i| ≤Ri} .

224
Eigenvalue Problems
b)
Prove that
σ(A) ⊆
n[
j=1
{z ∈C | |z −aj,j| ≤Cj} .
8.3
Let A = [ai,j] ∈Cn×n. Let
Ri =
n
X
j=1
j̸=i
|ai,j| .
Deﬁne the Cassini ovals9 as
Ci,j = {z ∈C | |z −ai,i||z −aj,j| < RiRj} .
a)
Show that
n[
i,j=1
Ci,j ⊆
n[
i=1
Di,
where D1, . . . , Dn denotes the Gershgorin disks.
b)
Show that
σ(A) ⊂
n[
i,j=1
Ci,j.
c)
Let 0 < ε ≪1 and consider the 2 × 2 matrix
A =
1
−ε
ε
−1

.
Find the eigenvalues of A and compare the predictions given by the Gershgorin
disks and Cassini ovals. Which one seems more accurate?
8.4
Let A = [ai,j] ∈Cn×n. Let
Ri =
n
X
j=1
j̸=i
|ai,j| .
Show that if
|ai,iaj,j| > RiRj,
then the matrix A is nonsingular.
8.5
Let A = [ai,j] ∈Cn×n have the property that, for some k and all i,
|ak,k −ai,i| >
X
j̸=k
|ak,j| +
X
j̸=i
|ai,j| .
Show that the region
D =


β ∈C

|β −ak,k| ≤
X
j̸=k
|ak,j|



9 Named in honor of the Italian (naturalized French) mathematician, astronomer, and engineer
Giovanni Domenico Cassini (1625–1712).

Listings
225
contains one, and only one, eigenvalue.
8.6
Prove Corollary 8.7.
8.7
Prove Theorem 8.9.
8.8
Prove Proposition 8.12.
8.9
Complete the proof of Theorem 8.16.
8.10
Suppose that A ∈Cn×n is diagonalizable (not necessarily Hermitian) and
x0 ∈Cn. Deﬁne the sequence of vectors {xk}∞
k=0 ⊂Cn via
xk =
Akx0
∥Akx0∥∞
,
k = 1, 2, 3, . . . .
Suppose that the spectrum σ(A) = {λi}n
i=1 ⊂C has the ordering
0 ≤|λ1| ≤|λ2| ≤· · · ≤|λn−1| < |λn|.
Suppose that v n is the eigenvector associated with the dominant eigenvector λn.
Assuming that x0 has a nonzero component in the direction of v n, prove that xk
converges to a multiple of v n.
8.11
Prove Proposition 8.17.
8.12
Prove Theorem 8.19.
8.13
Prove Theorem 8.23.
8.14
Complete the proof of Proposition 8.25.
8.15
Complete the proof of Theorem 8.28.
8.16
Prove Lemma 8.30.
8.17
Prove Lemma 8.31.
Listings
1
function [M, Q, err] = Hessenberg(A)
2
% The following algorithm reduces the matrix A to Hessenberg
3
% form
4
%
5
% M = Q'AQ
6
%
7
% Input
8
%
A(1:n,1:n) : a square matrix
9
%
10
% Output
11
%
M(1:n,1:n) : a lower Hessenberg matrix that is similar to A
12
%
Q(1:n,1:n) : the unitary matrix that realizes the
13
%
transformation
14
%
err : = 0, if no error was encountered
15
%
= 1, if there was an error
16
[m,n] = size(A);
17
M = A;
18
err = 0;
19
if m ~= n
20
err = 1;
21
return;

226
Eigenvalue Problems
22
end
23
Q = eye(n);
24
for k=1:n-2
25
nn = norm( M(k+1:n,k) );
26
e = zeros(n-k,1);
27
e(1) = 1;
28
v = nn*e + M(k+1:n,k);
29
norm v = norm(v);
30
if norm v < eps( norm v )
31
err = 1;
32
return;
33
end
34
v = (1.0/norm v)*v;
35
M(k+1:n,k:n) = M(k+1:n,k:n) - 2.0*(v*v')*M(k+1:n,k:n);
36
M(1:n,k+1:n) = M(1:n,k+1:n) - 2.0*M(1:n,k+1:n)*(v*v');
37
Q(1:n,k+1:n) = Q(1:n,k+1:n) - 2.0*Q(1:n,k+1:n)*(v*v');
38
end
39
end
Listing 8.1 Reduction of a matrix to lower Hessenberg form via similarity
transformations.
1
function [U, EigVecs, err] = QRIter( A, maxits )
2
% The QR iteration method to find eigenvalues.
3
%
4
% Input
5
%
A(1:n,1:n) : a square matrix
6
%
maxits : the maximal number of iterations
7
%
8
% Output
9
%
U(1:n,1:n) : an approximation to an upper triangular matrix
10
%
that is similar to A
11
%
EigVecs(1:n,1:n) : A unitary matrix whose columns are
12
%
eigenvectors of A
13
%
err : = 0, if no error was encountered
14
%
= 1, if there was an error
15
err = 0;
16
[U, EigVecs, err] = Hessenberg(A);
17
if err == 1
18
return;
19
end
20
for i=1:maxits
21
[Q, R, err] = QRFact( U, 1 );
22
if err == 1
23
return;
24
end
25
U = R*Q;
26
EigVecs = EigVecs*Q;
27
end
28
end
Listing 8.2 The QR iteration to ﬁnd eigenvalues.

Listings
227
1
function [U, EigVecs, err] = QRIterShifts( A, maxits )
2
% The QR iteration method with shifts to find eigenvalues.
3
%
4
% Input
5
%
A(1:n,1:n) : a square matrix
6
%
maxits : the maximal number of iterations
7
%
8
% Output
9
%
U(1:n,1:n) : an approximation to an upper triangular matrix
10
%
that is similar to A
11
%
err : = 0, if no error was encountered
12
%
= 1, if there was an error
13
err = 0;
14
[U, EigVecs, err] = Hessenberg(A);
15
if err == 1
16
return;
17
end
18
n = size(A,1);
19
id = eye(n,n);
20
for i=1:maxits
21
mu = U(n,n);
22
[Q, R, err] = QRFact( U - mu*id, 1 );
23
if err == 1
24
return;
25
end
26
U = R*Q + mu*id;
27
28
EigVecs = EigVecs*Q;
29
end
30
end
Listing 8.3 The QR iteration with shifts to ﬁnd eigenvalues.


Part II
Constructive Approximation
Theory


9
Polynomial Interpolation
In this chapter, we begin the study of constructive approximation theory, which, as
its name suggests, is concerned with methods to approximate a function (which may
only be known approximately) by a simpler one. One of the recurring features in our
discussion will be the interplay between the smoothness of a function, measured in
an appropriate sense, and the quality of approximation that we are able to produce.
Applications of approximation theory are plentiful, and we will see several of these
throughout this book. For instance, in Chapter 14, we will see how this is used to
approximate the value of integrals. It will also play a central role in Part V, where
the performance of a numerical scheme for the approximation of the solution to a
boundary value problem depends in a fundamental way not only on the method of
choice but also on the smoothness of the solution.
We begin approximation theory with the topic of polynomial approximation. Given
a, usually continuous, function we wish to construct a polynomial satisfying certain
properties that, in a very deﬁnite sense, approximates the given function. In fact,
most of this part of the book is about generating approximations with polynomials,
of both the ordinary and the trigonometric kind.
Let us immediately remark that, since our discussion is now concerned with
functions and their properties, the reader should be familiar with some basic facts
in real analysis: continuity, compactness, etc. We refer to Appendix B for a review
and guide to notation. Some facts about spaces of smooth functions will also be
necessary, and Appendix D provides an overview. With this in mind, our discussion
is started by ﬁrst presenting a cornerstone approximation result, which the reader
may have seen before. For the moment, this is stated without proof.
Theorem (Weierstrass Approximation Theorem1). Let [a, b] ⊂R be a compact
interval and f ∈C([a, b]). For every ε > 0, there exists an n ∈N and a polynomial
pn ∈Pn such that
∥f −pn∥L∞(a,b) ≤ε.
In other words, there is a sequence of polynomials, {pn}∞
n=0 with pn ∈Pn, that
converges uniformly to f as n →∞.
This theorem tells us that a continuous function on a compact interval can be
well approximated by polynomials, but it does not tell us how to construct such a
polynomial and it does not tell us the convergence rate.
1 Named in honor of the German mathematician Karl Theodor Wilhelm Weierstrass
(1815–1897).

232
Polynomial Interpolation
On the other hand, Taylor’s Theorem B.31 gives a precise way to construct a
polynomial approximation, but requires the function f to be n-times diﬀerentiable
to build an approximating polynomial of degree n. Moreover, we have to know the
values of all the derivatives at a certain point, which is not always practical.
Suppose, for example, that one only knows function values at a ﬁnite collection of
points in the domain. What can we do? The celebrated theorems mentioned above
do not give any indication. For constructive approximation, another place to start is
interpolation. Interpolation works by demanding that a simple function — usually,
but not always, a polynomial — exactly matches the values of a function of interest
at a given number of points. Typically, we only require that the function of interest is
continuous, or piecewise continuous, in its domain of deﬁnition. Interpolation is
simple and often works well. But sometimes it can go badly wrong. Paradoxically,
this can happen when we try to interpolate a large number of points. One must be
careful, as we will see.
9.1
The Vandermonde Matrix and the Vandermonde Construction
To understand interpolation, we need some basic deﬁnitions.
Deﬁnition 9.1 (nodal set). Let [a, b] ⊂R be a compact interval. X is called a
nodal set of size n + 1 ∈N in [a, b] if and only if X = {xi}n
i=0 ⊂[a, b] is a set of
distinct elements. The elements of X, xi are called nodes.
Observe that we usually enumerate the nodes in a very particular way, starting
with 0 and ending with n. Also note that the points of any nodal set are always, by
design, distinct. Not all ﬁnite sets that we shall introduce will have this property.
Typically the nodes are numbered in increasing order, for convenience; but this is
not always the case.
Deﬁnition 9.2 (interpolating polynomial). Suppose that X = {xi}n
i=0 is a nodal set
of size n + 1 ∈N contained in the compact interval [a, b] ⊂R and f : [a, b] →R
is a function. The function I : [a, b] →R is called an interpolant of f subordinate
to X if and only if I(xi) = f (xi), i = 0, . . . , n. In this case, we write I(X) = f (X),
for short. Suppose that Y = {yi}n
i=0 ⊂R is a set of not necessarily distinct points.
Deﬁne the set of ordered pairs
O = {(xi, yi) | xi ∈X, yi ∈Y, i = 0, . . . , n} .
We say that I is an interpolant of O if and only if I(xi) = yi, i = 0, . . . , n. Often,
we write I(X) = Y as a shorthand. If the interpolant I is a polynomial, it is called
an interpolating polynomial.
In short, an interpolant is a function that agrees with another, given, function
at a ﬁnite number of points in its domain.
Deﬁnition 9.3 (Vandermonde matrix2). Suppose that X = {xi}n
i=0 is a nodal set
2 Named in honor of the French mathematician, musician and chemist Alexandre-Th´eophile
Vandermonde (1735–1796).

9.1 The Vandermonde Matrix and the Vandermonde Construction
233
of size n + 1 ∈N in the compact interval [a, b] ⊂R. The Vandermonde matrix
subordinate to the nodal set X, denoted V = [vi,j] ∈R(n+1)×(n+1), is the matrix
with entries
vi,j = xj−1
i−1 ,
i, j = 1, . . . , n + 1,
where the superscript represents exponentiation. In other words,
V =


1
x0
x2
0
· · ·
xn
0
1
x1
x2
1
· · ·
xn
1
...
...
...
...
1
xn
x2
n
· · ·
xn
n


.
Theorem 9.4 (Vandermonde). Suppose that X = {xi}n
i=0 is a nodal set in the
compact interval [a, b] ⊂R and V = [vi,j] ∈R(n+1)×(n+1) is the Vandermonde
matrix subordinate to X. Then V is invertible and, in particular,
det(V) =
Y
0≤i<j≤n
(xj −xi) ̸= 0.
Proof. See Problem 9.1. Of course, if one can establish the determinant formula,
the invertibility of V follows because nodes are distinct.
With this at hand we can establish the existence and uniqueness of an
interpolating polynomial.
Proposition 9.5 (existence and uniqueness). Suppose that X = {xi}n
i=0 is a nodal
set in the compact interval [a, b] ⊂R and Y = {yi}n
i=0 ⊂R. There is a unique
polynomial p ∈Pn with the property that p(X) = Y .
Proof. We can set this problem up in the following way. Express p as p(x) =
Pn
j=0 cjxj, where the coeﬃcients ci ∈R must be determined. Observe that p
satisﬁes p(X) = Y if and only if, for all i = 0, . . . , n,
p(xi) =
n
X
j=0
cjxj
i = yi.
In matrix form, this can be rewritten as


1
x0
x2
0
· · ·
xn
0
1
x1
x2
1
· · ·
xn
1
...
...
...
...
1
xn
x2
n
· · ·
xn
n




c0
c1
...
cn

=


y0
y1
...
yn

,
(9.1)
where the vector c = [c0, . . . , cn]⊺∈Rn+1 is unknown. Since the coeﬃcient matrix
is the Vandermonde matrix, which is invertible, the result follows.
The interpolating polynomial construction represented by the linear system (9.1)
is called the Vandermonde construction.

234
Polynomial Interpolation
Example 9.1
While, in theory, all that is needed to construct an interpolating
polynomial is to solve system (9.1), it turns out that the Vandermonde matrix tends
to be severely ill-conditioned. For example, deﬁne Xn = {i/n}n
i=0, a set of uniform
nodes in [0, 1]. The following table shows how the spectral condition number of V
grows with n:
n
κ2(V)
4
6.86 × 1002
8
2.01 × 1006
16
2.42 × 1013
Clearly, one should avoid using the Vandermonde matrix in practical applications
when n is large. We will look for other more practical ways of constructing
interpolants.
Interpolation, from the theoretical perspective, can be thought of as a linear
projection operator.
Deﬁnition 9.6 (interpolation operator). Suppose that X = {xi}n
i=0 is a nodal set
in the compact interval [a, b] ⊂R. The interpolation operator subordinate to X,
denoted
IX : C([a, b]) →Pn,
is deﬁned as follows: for f ∈C([a, b]), IX[f ] ∈Pn is the unique interpolating
polynomial satisfying IX[f ](X) = f (X).
The reader can easily prove the following result.
Proposition 9.7 (projection). Suppose that X = {xi}n
i=0 is a nodal set in the
compact interval [a, b] ⊂R and IX : C([a, b]) →Pn is the interpolation operator
subordinate to X. IX is a linear projection operator, meaning that
IX[αf + βg] = αIX[f ] + βIX[g],
∀f , g ∈C([a, b]),
∀α, β ∈R
and
IX[p] = p,
∀p ∈Pn.
Proof. See Problem 9.2.
The norm of the interpolation operator has a special name.
Deﬁnition 9.8 (Lebesgue constant3). Suppose that X = {xi}n
i=0 is a nodal set
in the compact interval [a, b] ⊂R and IX : C([a, b]) →Pn is the interpolation
3 Named in honor of the French mathematician Henri L´eon Lebesgue (1875–1941).

9.2 Lagrange Interpolation and the Lagrange Nodal Basis
235
operator subordinate to X. The Lebesgue constant subordinate to X, denoted
Λ(X), is the operator norm of IX, i.e., Λ(X) = ∥IX∥∞, where
∥IX∥∞=
sup
0̸=f ∈C([a,b])
∥IX[f ]∥L∞(a,b)
∥f ∥L∞(a,b)
=
sup
f ∈C([a,b])
∥f ∥L∞(a,b)=1
∥IX[f ]∥L∞(a,b) .
(9.2)
In the next section, we will show how one might compute the Lebesgue constant.
At this point, just note that the constant depends upon our choice of the nodal
set.
9.2
Lagrange Interpolation and the Lagrange Nodal Basis
It is sometimes more convenient, from the viewpoint of representation and
construction, to use the so-called Lagrange basis to build our interpolating
polynomials.
Deﬁnition 9.9 (Lagrange nodal basis4). Suppose that X = {xi}n
i=0 is a nodal set
of size n + 1 ∈N in the compact interval [a, b] ⊂R. The Lagrange nodal basis
subordinate to X is the set of polynomials LX = {Lℓ}n
ℓ=0 ⊂Pn deﬁned via
Lℓ(x) =
n
Y
i=0
i̸=ℓ
x −xi
xℓ−xi
.
(9.3)
Proposition 9.10 (properties of LX). Suppose that X = {xi}n
i=0 is a nodal set of
size n + 1 ∈N in the compact interval [a, b] ⊂R and LX = {Li}n
i=0 ⊂Pn is the
Lagrange basis subordinate to X. Then LX is a basis of Pn with the properties
Li(xj) = δi,j,
i, j = 0, . . . , n
(9.4)
and
n
X
i=0
Li(x) = 1,
∀x ∈R.
(9.5)
Proof. See Problem 9.3
Example 9.2
Figure 9.1 shows the Lagrange nodal basis subordinate to the nodal
set X = {0, 0.30, 0.42, 0.71, 1.00} ⊂[0, 1]. The ﬁgure also shows the sum of the
basis functions, P4
i=0 Li(x), which conﬁrms (9.5) for this case.
Theorem 9.11 (interpolation polynomial). Suppose that X = {xi}n
i=0 is a nodal set
in the compact interval [a, b] ⊂R, and Y = {yi}n
i=0 ⊂R. Let LX = {Li}n
i=0 ⊂Pn
4 Named in honor of the Italian, later naturalized French, mathematician and astronomer
Joseph-Louis Lagrange (1736–1813).

236
Polynomial Interpolation
0
0.2
0.4
0.6
0.8
1.0
−1
−0.5
0
0.5
1.0
1.5
x
Figure 9.1 The Lagrange nodal basis of degree n = 4, subordinate to the nodal set
X = {0, 0.30, 0.42, 0.71, 1.00} ⊂[0, 1]. We also plot the sum of the basis functions,
P4
i=0 Li(x), which conﬁrms (9.5) for this case.
be the Lagrange nodal basis subordinate to X. The unique polynomial p ∈Pn, with
the property that p(X) = Y , has the form
p(x) =
n
X
i=0
yiLi(x) ∈Pn.
(9.6)
Proof. From deﬁnition (9.6), we have, using property (9.4), that p(xi) = yi.
Deﬁnition 9.12 (Lagrange interpolating polynomial). Suppose that X = {xi}n
i=0 is
a nodal set in the compact interval [a, b] ⊂R, LX = {Li}n
i=0 ⊂Pn is the Lagrange
nodal basis subordinate to X, and f : [a, b] →R. The Lagrange interpolating
polynomial of the function f , subordinate to the nodal set X, is the polynomial
p(x) =
n
X
i=0
f (xi)Li(x) ∈Pn.
(9.7)
Observe that, by uniqueness, the Lagrange interpolating polynomial coincides
with the interpolant we obtained in Proposition 9.5 via the Vandermonde construc-
tion. For historical reasons, the interpolating polynomial constructed by matching

9.2 Lagrange Interpolation and the Lagrange Nodal Basis
237
point values of the given function is called the Lagrange interpolating polynomial.
We will give one more construction in Section 9.6 based on Newton’s basis.
Deﬁnition 9.13 (nodal polynomial). Suppose that X = {xi}n
i=0 is a nodal set in
the compact interval [a, b] ⊂R. The polynomial ωn+1 ∈Pn+1, deﬁned by
ωn+1(x) =
n
Y
i=0
(x −xi),
(9.8)
is called the nodal polynomial subordinate to X.
The following alternate formula for the elements of the Lagrange nodal basis is
often useful.
Proposition 9.14 (Lagrange nodal basis). Suppose that X = {xi}n
i=0 is a nodal set
in the compact interval [a, b] ⊂R, LX = {Li}n
i=0 ⊂Pn is the Lagrange nodal basis
subordinate to X, ωn+1 is the nodal polynomial subordinate to X, and f : [a, b] →R.
Then, for all i = 0, . . . , n, we have
Li(x) =
ωn+1(x)
(x −xi)ω′
n+1(xi).
(9.9)
Consequently, the Lagrange interpolating polynomial, p ∈Pn, of the function f
subordinate to the nodal set X is
p(x) =
n
X
i=0
f (xi)
ωn+1(x)
(x −xi)ω′
n+1(xi).
(9.10)
Proof. One can show that
ω′
n+1(xi) =
n
Y
j=0
j̸=i
(xi −xj).
This and further details are left to the reader as an exercise; see Problem 9.5.
Before we move on to the error analysis for Lagrange interpolation, let us
compute the Lebesgue constant.
Theorem 9.15 (Λ(X)). Suppose that X = {xi}n
i=0 is a nodal set in the compact
interval [a, b] ⊂R. The Lebesgue constant satisﬁes
Λ(X) = max
a≤x≤b λX(x),
λX(x) =
n
X
i=0
|Li(x)| ≥1,
where LX = {Li}n
i=0 ⊂Pn is the Lagrange nodal basis subordinate to X. The
function λX is called the Lebesgue function subordinate to X.
Proof. Suppose that f ∈C([a, b]) is arbitrary. Then
|IX[f ](x)| =

n
X
i=0
f (xi)Li(x)
 ≤max
0≤i≤n |f (xi)|
n
X
i=0
|Li(x)| ≤∥f ∥L∞(a,b)
n
X
i=0
|Li(x)| .

238
Polynomial Interpolation
0
0.2
0.4
0.6
0.8
1.0
1.0
1.5
2.0
2.5
3.0
3.5
x
Figure 9.2 The Lebesgue function, λX(x) = P4
i=0 |Li(x)|, subordinate to the nodal set
X = {0, 0.30, 0.42, 0.71, 1.00} ⊂[0, 1]. The basis functions, Li, i = 0, . . . , 4, are plotted
in Figure 9.1, for comparison.
Thus,
∥IX∥∞≤max
a≤x≤b
n
X
i=0
|Li(x)| .
Now, on the other hand, there is a function f ∈C([a, b]) with ∥f ∥L∞(a,b) = 1
satisfying
max
a≤x≤b |IX[f ](x)| = max
a≤x≤b

n
X
i=0
f (xi)Li(x)
 = max
a≤x≤b
n
X
i=0
|Li(x)| ,
which proves that
∥IX∥∞= max
a≤x≤b
n
X
i=0
|Li(x)| .
We leave it to the reader as an exercise to ﬁnd the appropriate function f ; see
Problem 9.6.
Example 9.3
The Lebesgue function, λX(x) = P4
i=0 |Li(x)|, subordinate to the
nodal set X = {0, 0.30, 0.42, 0.71, 1.00} ⊂[0, 1] is shown in Figure 9.2. The basis
functions, Li, i = 0, . . . , 4, are plotted in Figure 9.1, for comparison.

9.2 Lagrange Interpolation and the Lagrange Nodal Basis
239
In Chapter 10, we will give an error estimate for Lagrange interpolation that
involves the Lebesgue constant and an object known as the minimax polynomial.
For now, we give a more standard error formula.
Theorem 9.16 (Lagrange interpolation error). Suppose that X = {xi}n
i=0 is a nodal
set in the compact interval [a, b] ⊂R, f ∈Cn+1([a, b]), and p ∈Pn is the Lagrange
interpolating polynomial of f subordinate to X. Then, for every x ∈[a, b]\X, there
is a point ξ = ξ(x) ∈(a, b) with
min{x0, . . . , xn, x} < ξ < max{x0, . . . , xn, x}
such that
f (x) −p(x) = f (n+1)(ξ)
(n + 1)! ωn+1(x),
(9.11)
where ωn+1 is the nodal polynomial introduced in Deﬁnition 9.13.
Proof. Fix x ∈[a, b]\X. Let us consider cases.
(n = 0) In this case, p(x) = f (x0) for all x ∈[a, b]. By the Mean Value Theorem
B.30, there is a point ξ between x and x0 such that
f (x) −p(x) = f (x) −f (x0) = f ′(ξ)(x −x0),
which is the same as (9.11), since, in this case, (n + 1)! = 1 and ω1(x) = x −x0.
(n ≥1) Deﬁne, for any s ∈[a, b],
e(s) = f (s) −p(s) −f (x) −p(x)
ωn+1(x)
ωn+1(s).
Observe that, for all i = 0, . . . , n,
e(xi) = 0 −f (x) −p(x)
ωn+1(x)
· 0 = 0.
Furthermore, e(x) = 0. Thus, e(s) is zero at at least n +2 distinct points in [a, b].
By Rolle’s Theorem B.29, there are n+1 distinct points in (a, b) where e′ vanishes.
Applying Rolle’s Theorem B.29 again, there are n distinct points in (a, b) where
e′′ vanishes. Continuing in this fashion, there is one point (at least) ξ in (a, b) for
which e(n+1)(ξ) = 0. Now observe that
0 = e(n+1)(ξ) = f (n+1)(ξ) −f (x) −p(x)
ωn+1(x)
(n + 1)!.
Rearranging terms we get (9.11).
The interpolating polynomial, p, can become a surrogate for the original function
f . If what is wanted is a derivative of f , we can approximate that with the derivative
of p. If what is wanted is the integral of f , we can integrate p instead. This will
be much further explored in Chapter 14, where we discuss numerical integration.

240
Polynomial Interpolation
For the present, with a technique similar to that used in the last proof, we can also
establish an error formula for derivatives.
Theorem 9.17 (error formula for derivatives). Suppose that X = {xi}n
i=0 is a
nodal set in the compact interval [a, b] ⊂R, f ∈Cn+1([a, b]), and p ∈Pn is the
Lagrange interpolating polynomial of f subordinate to X. Then there are distinct
points ζi ∈(a, b), i = 1, . . . , n, whose values depend on f and X, such that, for
every x ∈[a, b]\X, there is a point ξ = ξ(x) ∈(a, b) with
min{x0, . . . , xn, x} < ξ < max{x0, . . . , xn, x}
for which
f ′(x) −p′(x) = f (n+1)(ξ)
n!
ψn(x),
(9.12)
where
ψn(x) =
n
Y
i=1
(x −ζi).
(9.13)
Proof. By Rolle’s Theorem B.29, there are points ζi ∈(xi−1, xi), i = 1, . . . , n such
that f ′(ζi) −p′(ζi) = 0. Clearly, all these ζi are distinct and only depend on f and
the distribution of the nodes, xi.
Suppose that x ∈[a, b]\{ζi}n
i=1. Deﬁne, for all s ∈[a, b],
e(s) = f ′(s) −p′(s) −f ′(x) −p′(x)
ψn(x)
ψn(s),
where ψn(x) = Qn
i=1(x −ζi). Clearly, e vanishes at n + 1 distinct points: ζi,
i = 1, . . . , n, and x. Repeated application of Rolle’s Theorem B.29 gives the result,
as before.
9.3
The Runge Phenomenon
It is natural to think that, by increasing the size of the nodal set and, therefore, the
degree of the interpolating polynomial, we should be able to get better and better
approximations of the function of interest. But this is not always the case.
Proposition 9.18 (conditional convergence). Suppose that n ∈N, [a, b] is a
compact interval, h = b−a
n , and X = {xi}n
i=0 is the uniformly spaced nodal set
xi = a + ih,
i = 0, 1, . . . , n.
Let f ∈C∞([a, b]) and pn ∈Pn be the Lagrange interpolating polynomial of f
subordinate to X. Then
∥f −pn∥L∞(a,b) = max
a≤x≤b |f (x) −pn(x)| →0,
n →∞,
provided that
lim
n→∞
Fn+1Ωn+1
(n + 1)! = 0,

9.3 The Runge Phenomenon
241
−1.0
−0.5
0
0.5
1.0
−1.0
−0.5
0
0.5
1.0
x
f (x)
p4(x)
p8(x)
p16(x)
interpolation points
Figure 9.3 The Runge phenomenon: with uniformly spaced interpolation nodes,
interpolation polynomials can oscillate wildly with increasing n, yielding inaccurate
approximations.
where
Fn+1 =
f (n+1)
L∞(a,b) ,
Ωn+1 = n!
4 hn+1.
In other words, the sequence {pn}n∈N converges uniformly to f , provided that
lim
n→∞
Fn+1hn+1
4(n + 1) = 0.
Proof. It suﬃces to use (9.11) and prove that
max
a≤x≤b |ωn+1(x)| ≤Ωn+1.
We leave the remaining steps to the reader as an exercise; see Problem 9.8.
The convergence condition of the previous result is not a trivial one, as the
following example shows.

242
Polynomial Interpolation
Example 9.4
The following is commonly referred to as the Runge phenomenon.5
Suppose that n ∈N, h = 2
n, and Xn = {xi}n
i=0 is the uniformly spaced nodal set
xi = −1 + ih,
i = 0, 1, . . . , n.
Suppose that pn ∈Pn is the Lagrange interpolating polynomial subordinate to Xn
of the function
f (x) =
1
1 + 25x2 ,
x ∈[−1, 1].
Figure 9.3 shows the interpolating polynomials p4, p8, and p16. It appears that the
error is growing in the L∞(−1, 1) norm as n is getting larger. In particular, the
error in the tails is increasing. In fact, it is possible to prove that, for this problem,
∥f −pn∥L∞(−1,1) →∞,
n →∞.
This is due to the fact that, in this case, the growth in Fn+1 cannot be controlled
by
hn+1
4(n+1). This has to do with the structure of f and the fact that our points are
uniformly spaced. However, if we modify the spacing of the points in a smart way,
we can do better, as we will show when we discuss Chebyshev interpolation.
9.4
Hermite Interpolation
One way to generalize Lagrange interpolation is to match not only the values
of a given function at a nodal set but also the values of a certain number of
derivatives. Matching point values and ﬁrst derivatives of a function leads to
Hermite interpolation.
Deﬁnition 9.19 (Hermite interpolating polynomial6). Suppose that X = {xi}n
i=0 is
a nodal set of size n + 1 ∈N in the compact interval [a, b] ⊂R and f ∈C1([a, b]).
The polynomial p ∈P2n+1 is called a Hermite interpolating polynomial of f if
and only if
p(xi) = f (xi),
p′(xi) = f ′(xi),
i = 0, . . . , n.
Theorem 9.20 (existence and uniqueness). Suppose that X = {xi}n
i=0 is a nodal
set of size n + 1 ∈N in the compact interval [a, b] ⊂R and f ∈C1([a, b]). Then
the Hermite interpolating polynomial is well deﬁned, i.e., it exists and is unique.
Proof. Since the case n = 0 is trivial, let us suppose that n ≥1. Deﬁne, for
0 ≤ℓ≤n,
F0,ℓ(x) = (Lℓ(x))2(1 −2L′
ℓ(xℓ)(x −xℓ)),
F1,ℓ= (Lℓ(x))2(x −xℓ),
(9.14)
5 Named in honor of the German mathematician, physicist, and spectroscopist Carl David
Tolm´e Runge (1856–1927).
6 Named in honor of the French mathematician Charles Hermite (1822–1901).

9.5 Complex Polynomial Interpolation
243
where Lℓis as in (9.3), and
p(x) =
n
X
ℓ=0
[F0,ℓ(x)f (xℓ) + F1,ℓ(x)f ′(xℓ)] .
(9.15)
We leave it to the reader as an exercise to prove that p has the desired properties
and is unique; see Problem 9.9.
Theorem 9.21 (Hermite interpolation error). Suppose that X = {xi}n
i=0 is a nodal
set of size n + 1 ∈N in the compact interval [a, b] ⊂R, f ∈C(2n+2)([a, b]), and
p ∈P2n+1 is the unique Hermite interpolating polynomial of f . Then, for every
x ∈[a, b]\X, there is a point ξ = ξ(x) ∈(a, b), with
min{x0, . . . , xn, x} < ξ < max{x0, . . . , xn, x}
such that
f (x) −p(x) = f (2n+2)(ξ)
(2n + 2)! (ωn+1(x))2,
(9.16)
where ωn+1 is given in 9.8.
Proof. Fix x ∈[a, b]\X. Deﬁne, for all s ∈[a, b],
e(s) = f (s) −p(s) −f (x) −p(x)
(ωn+1(x))2 (ωn+1(s))2.
As before, e vanishes at n + 2 distinct points in [a, b]: the n + 1 nodes xi and the
point x. By Rolle’s Theorem B.29, e′ vanishes at n + 1 distinct points, which are
distinct from the n+2 points in X ∪{x}. Furthermore, by construction, e′ vanishes
at the n + 1 points of X. Thus, e′ vanishes at a total of (at least) 2n + 2 distinct
points in [a, b]. This is the key fact we need.
By repeated application of Rolle’s Theorem B.29, the function e(2n+2), which
exists and is continuous by assumption, vanishes at (at least) one point ξ ∈(a, b).
Of course, since p ∈P2n+1, p(2n+2) ≡0 and, therefore,
0 = e(2n+2)(ξ) = f (2n+2)(ξ) −f (x) −p(x)
(ωn+1(x))2 (2n + 2)!,
which proves the result.
9.5
Complex Polynomial Interpolation
We now discuss the interpolation of functions that are holomorphic in a region
of the complex plane. In doing so, we gain some further insight into the curious
example provided by Runge; see Example 9.4.

244
Polynomial Interpolation
9.5.1
Some Facts of Complex Analysis
In what follows, we will make use of some tools from complex analysis to make
the discussion rigorous, but we supply these as needed. For further results, see, for
example, [3, 24, 35, 60, 77].
Deﬁnition 9.22 (holomorphic function). Let D ⊂C be a simply connected, open
region and g : D →C. We say that g is complex diﬀerentiable at a ∈D if the limit
lim
z→a
g(z) −g(a)
z −a
exists. In this case, we denote by g′(a) the value of this limit and call it the
derivative of g at a. If g is complex diﬀerentiable at every point of D, then we say
that g is holomorphic in D.
As the reader may be aware, being complex diﬀerentiable is a very strong
condition. It implies, in particular, that if g is complex diﬀerentiable in a
neighborhood of a point a ∈C, then it is inﬁnitely diﬀerentiable (again in the
complex sense) at this point. Moreover, this also implies that this function is
analytic, i.e., its power series representation
∞
X
k=0
g(k)(a)(z −a)k
k!
converges uniformly in a neighborhood of a. This can be proved with the help of
Cauchy’s integral theorem, which we discuss next. We will ﬁrst need to deﬁne path
integrals, and for that we need several deﬁnitions.
Deﬁnition 9.23 (path). Let [a, b] ⊂R be a compact interval. A function γ ∈
C([a, b]; C) is called a path or curve. We say that a path is simple or a Jordan
path7 if
γ(t1) ̸= γ(t2),
∀t1, t2 ∈(a, b).
We say that a path is closed, or a contour, if γ(a) = γ(b).
A powerful result known as the Jordan Curve Theorem (see [39, Section 2.B]),
states that a simple contour divides the complex plane C into two parts: an
“interior” region bounded by the path and an “exterior,” so that every other path
that goes from a point of the interior to the exterior must intersect the given
contour. With the help of this, we can talk about points inside or outside a simple
closed path.
Deﬁnition 9.24 (smooth path). Let γ ∈C([a, b]; C) be a path. We say that this
path is piecewise smooth if there is S ⊂[a, b] of ﬁnite cardinality such that
γ ∈C1([a, b]\S; C).
7 Named in honor of the French mathematician Marie Ennemond Camille Jordan (1838–1922).

9.5 Complex Polynomial Interpolation
245
For piecewise smooth paths, the derivative exists at all, but possibly a ﬁnite
number of, points. Thus, we can deﬁne the orientation of a path. This will be
mostly used for contours, so we only deﬁne them this way.
Deﬁnition 9.25 (orientation). Let γ ∈C([a, b]; C) be a simple, piecewise smooth
contour and let Γ = γ([a, b]). Denote by D ⊂C the collection of points inside γ,
i.e., ∂D = Γ. Let nD(t) = [n1(t), n2(t)]⊺, t ∈[a, b] be the unit exterior normal to
D at the point γ(t). We say that γ is traversed counterclockwise if
det
 n1(t)
n2(t)
ℜγ′(t)
ℑγ′(t)

> 0
for all t ∈[a, b].
Finally, we can deﬁne integrals over a path.
Deﬁnition 9.26 (path integral). Let γ ∈C([a, b]; C) be a simple, piecewise smooth
path; and let Γ = γ([a, b]). For g : Γ →C, the path integral of g over γ is
deﬁned as
Z
γ
g(z)dz =
Z b
a
g(γ(t))dγ(t)
dt
dt.
It turns out that values of a holomorphic function can be obtained using path
integrals.
Theorem 9.27 (Cauchy Integral Theorem8). Suppose that D ⊂C is a simply
connected, open region and g : D →C is holomorphic. Suppose that γ : [0, 2π] →
D is a simple closed contour traversed counterclockwise and the point z0 ∈D is
inside γ. Then
g(z0) =
1
2πi
Z
γ
g(z)
z −z0
dz.
Let us now brieﬂy discuss what can be said about functions that are holomorphic
except at an isolated point.
Deﬁnition 9.28 (Laurent expansion9). Suppose that D ⊂C is a simply connected,
open region and z0 ∈D. Assume that g : D\{z0} →C is holomorphic, i.e., g is
holomorphic on D, except for the isolated singularity at z0. In this setting, g admits
the Laurent expansion
g(z) =
∞
X
k=1
bk
(z −z0)k +
∞
X
k=0
ak(z −z0)k
= · · · +
b2
(z −z0)2 +
b1
(z −z0) + a0 + a1(z −z0) + a2(z −z0)2 + · · · ,
where bj ∈C, j = 1, 2, . . ., (not all of which are zero) and aj ∈C, j = 0, 1, . . ..
8 Named in honor of the French mathematician Augustin-Louis Cauchy (1789–1857).
9 Named in honor of the French mathematician and engineer Pierre Alphonse Laurent
(1813–1854).

246
Polynomial Interpolation
Points like z0 in the previous deﬁnition are called isolated singularities of a
function.
Deﬁnition 9.29 (isolated singularity). Suppose that D ⊂C is a simply connected,
open region and z0 ∈D. Assume that g : D\{z0} →C is holomorphic. Then we
call z0 an isolated singularity of g. Moreover, we say that z0 is a removable
singularity if the limit
lim
z→z0 g(z)
(9.17)
exists and it is ﬁnite. If the limit (9.17) exists but equals ∞, then we say that
the point z0 is a pole. Finally, if (9.17) does not exist, we call z0 an essential
singularity.
It turns out that poles and Laurent expansions are closely related.
Deﬁnition 9.30 (poles). Suppose that D ⊂C is a simply connected, open region
and z0 ∈D. Assume that g : D\{z0} →C is holomorphic and that z0 is a pole.
Let g admit a Laurent expansion. If b1 ̸= 0, and bj = 0 for j > 1, then we say
that g has a simple pole at the singularity z = z0. Similarly, if there is k ∈N such
that bk ̸= 0, but bj = 0 for j > k, then we say that g has a pole of degree k at
the singularity z = z0. Finally, the residue of g at z0, denoted Res(g, z0), is the
coeﬃcient b1, i.e.,
Res(g, z0) = b1.
We comment that a function that is holomorphic in an open region except at a
ﬁnite number of poles is called a meromorphic in this region.
Theorem 9.31 (Residue Theorem). Let D ⊂C be a simply connected, open,
bounded region and Z = {zi}n
i=0 ⊂D be a set of n + 1 distinct points. Assume
that g : D\Z →C is holomorphic, i.e., g is holomorphic on D, except for isolated
singularities at z0, . . . , zn. Suppose that γ : [0, 2π] →D is a simple closed contour
traversed counterclockwise and the points of Z are inside γ. Then
Z
γ
g(z) dz = 2πi
n
X
i=0
Res(g, zi),
where Res(g, zi) is the residue of g at zi.
The following result is useful for calculating residues.
Proposition 9.32 (computation of residues). Suppose that D ⊂C is a simply
connected, open region and z0 ∈D. Assume that g, h: D →C are holomorphic,
g(z0) ̸= 0, h(z0) = 0, and h′(z0) ̸= 0. Then the function f (z) = g(z)
h(z) is holomorphic
in D\{z0} and has a simple pole at z0. Furthermore,
Res(f , z0) = g(z0)
h′(z0).
We conclude our slight detour into complex analysis by stating a result that
will be used in Chapter 21 and is commonly referred to as the Maximum Modulus
Principle.

9.5 Complex Polynomial Interpolation
247
Theorem 9.33 (Maximum Modulus Principle). Let D ⊂C be an open and
connected region and g : D →C be holomorphic. If there is a point z0 ∈D
such that z 7→|g(z)| attains a (local) maximum at z0, then g is constant.
In other words, if g ∈C( ¯D; C) is holomorphic in D, then it attains its maximum
at some point of the boundary ∂D.
9.5.2
Lagrange Interpolation
We are now ready to study the problem of Lagrange interpolation in the complex
case. The setting is similar to the real case: we let D ⊂C be a simply connected,
open region; n ∈N; and the nodal set is Z = {zi}n
i=0 ⊂D, where all the nodes
are distinct. Given f : D →C, we need to ﬁnd a (complex-valued) polynomial
p ∈Pn(C) such that
p(zi) = f (zi),
i = 0, . . . , n.
(9.18)
The following result is obtained by small variations of the real case.
Theorem 9.34 (Lagrange interpolation error). Suppose that D ⊂C is a simply
connected, open region, f : D →C, and Z = {zi}n
i=0 ⊂D is a set of n + 1 ∈N
distinct points. There exists a unique polynomial p ∈Pn(C) that satisﬁes (9.18).
Suppose, in addition, that f is holomorphic, γ : [0, 2π] →D is a simple closed
contour traversed counterclockwise, and the points of Z are inside γ. Then the
error formula may be expressed as
f (ζ) −p(ζ) =
1
2πi
Z
γ
f (z)
z −ζ
n
Y
i=0
ζ −zi
z −zi
dz
(9.19)
for all ζ inside γ, but ζ /∈Z.
Proof. The existence and uniqueness of the interpolating polynomial p ∈Pn(C)
follows from similar arguments to those used for the real case. One can either appeal
to a Vandermonde-type construction, in which case the Vandermonde matrix is
invertible; or one can appeal to a Lagrange-type construction. In fact, the complex
analogue of the Lagrange interpolation formula (9.10) is valid, i.e., the interpolating
polynomial is
p(z) =
n
X
i=0
f (zi)
ωn+1(z)
(z −zi)ω′
n+1(zi),
(9.20)
where
ωn+1(z) =
n
Y
i=0
(z −zi).
To get the error formula, consider now the function
g(z) = f (z)
z −ζ
n
Y
i=0
ζ −zi
z −zi
= f (z)
z −ζ · ωn+1(ζ)
ωn+1(z),

248
Polynomial Interpolation
which is the integrand in (9.19). Observe that the function g is meromorphic on
D, i.e., it is holomorpic on D except at the isolated singularities Z ∪{ζ}. Thus, by
the residue Theorem 9.31, the integral
1
2πi
Z
γ
g(z)dz =
1
2πi
Z
γ
f (z)
z −ζ
n
Y
i=0
ζ −zi
z −zi
dz
is the sum of the residues of g contained inside of γ. Using Proposition 9.32, we ﬁnd
Res(g, zi) = f (zi)
zi −ζ · ωn+1(ζ)
ω′
n+1(zi),
i = 0, . . . , n,
and
Res(g, ζ) = f (ζ) · ωn+1(ζ)
ωn+1(ζ) = f (ζ).
Thus,
1
2πi
Z
γ
f (z)
z −ζ
n
Y
i=0
ζ −zi
z −zi
dz = f (ζ) −
n
X
i=0
f (zi)
ζ −zi
· ωn+1(ζ)
ω′
n+1(zi) = f (ζ) −p(ζ),
where in the last step we used (9.20).
This last result provides a useful means to estimate errors.
Theorem 9.35 (error estimate). Let [a, b] ⊂R be a compact interval and ε > 0
be ﬁxed. Deﬁne
r = b −a + ε,
Cε = {z ∈C | dist(z, [a, b]) = r} .
Suppose that γ : [0, 2π] →Cε is the simple closed contour traversing the set Cε
in a counterclockwise fashion. Deﬁne Dε as the open, simply connected set of all
points inside γ. Suppose that f : Dε →C is holomorphic on Dε and there is M > 0
such that
|f (z)| ≤M,
∀x ∈Cε = ∂Dε.
Let n ∈N0, Xn = {xi}n
i=0 be a nodal set in the interval [a, b], and pn ∈Pn(C)
be the interpolating polynomial of f subordinate to the nodal set Xn. Then, for all
x ∈[a, b]\Xn,
|f (x) −pn(x)| ≤(b −a + πr)M
π
b −a
r
n+1
.
Consequently, pn →f uniformly on [a, b], as n →∞, no matter how the nodes in
Xn are chosen.
Proof. Observe that the length of γ is precisely 2(b −a) + 2πr and apply the last
theorem. The details are left to the reader as an exercise; see Problem 9.10.

9.6 Divided Diﬀerences and the Newton Construction
249
Example 9.5
Let us revisit the Runge phenomenon. The function
f (x) =
1
1 + 25x2 ,
x ∈[−1, 1],
which we have previously considered, and the uniformly spaced nodal set
Xn =

−1 + 2i
n
n
i=0
⊂[−1, 1],
n ∈N.
As we mentioned in Example 9.4, if pn ∈Pn is the Lagrange interpolating
polynomial of f subordinate to Xn, then one can show that ∥f −pn∥L∞(−1,1) →∞,
as n →∞. In light of Theorem 9.35, why do we fail to obtain uniform convergence?
Clearly, one or more of the hypotheses of Theorem 9.35 must fail. In fact, as a
complex function, f is not holomorphic, for any ε > 0, in the region Dε deﬁned in
the hypotheses of Theorem 9.35. In fact, writing
f (z) =
1
1 + 25z2 =
1
(1 + 5iz)(1 −5iz),
we observe that f has isolated singularities and simple poles at z = ± i
5. This
explains is why Theorem 9.35 cannot be applied to guarantee uniform convergence
in [−1, 1] with the uniformly spaced nodes Xn.
9.6
Divided Diﬀerences and the Newton Construction
In this section, we give another method for computing interpolating polynomials,
using the so-called Newton construction. This method is based on an alternate
basis for Pn, namely the Newton basis.
Deﬁnition 9.36 (Newton basis10). Let n ∈N0 and X = {xi}n
i=0 be a nodal set
in the compact interval [a, b] ⊂R. The polynomial set Bn = {ωj}n
j=0, deﬁned as
ω0 ≡1 and, if n ≥1,
ωj(x) =
j−1
Y
k=0
(x −xk),
j = 1, . . . , n
is called the Newton basis. The polynomial ωj ∈Pj is called the nodal polynomial
of order j with respect to X.
The following result shows that Bn is indeed a basis.
Proposition 9.37 (basis). For any compact interval [a, b] ⊂R, any n ∈N0, and
all nodal sets X in [a, b], the Newton basis Bn is a basis of Pn.
Proof. See Problem 9.16.
10 Named in honor of the British mathematician, physicist, astronomer, theologian, and natural
philosopher Sir Isaac Newton (1642–1726/27).

250
Polynomial Interpolation
Since, as the previous result shows, Bn is a basis we can expand any interpolating
polynomial with respect to it. It turns out that the coeﬃcients in this basis
expansion carry very useful information.
Deﬁnition 9.38 (divided diﬀerences). Suppose that n ∈N0 and X = {xi}n
i=0 is
a nodal set in the compact interval [a, b]. Let f ∈C([a, b]) and pn ∈Pn be the
(unique) interpolating polynomial of f subordinate to X. Set
pn(x) =
n
X
k=0
akωk(x).
(9.21)
The coeﬃcient,
ak = f [x0, x1, . . . , xk],
k = 0, . . . , n,
in the expansion with respect to the Newton basis Bn = {ωj}n
j=0 is called the kth
order divided diﬀerence.
The choice of the name, divided diﬀerence, will be clear only later when we
discuss the properties of these objects.
Example 9.6
The Newton form of the interpolating polynomial lends itself to a
very eﬃcient algorithm for computation. Suppose, for example, that n = 4 and we
wish to evaluate p4 in (9.21) at x. Then, once the coeﬃcients {ak}4
k=0 are known,
we can express p4(x) as
p4(x) = a0 + (x −x0) {a1 + (x −x1) [a2 + (x −x2) {a3 + (x −x3) [a4]}]} .
This method of evaluation is known as Horner’s method.11
While this representation is, in theory, completely equivalent to (9.21), its
evaluation is much cheaper. In fact, it can be shown that evaluating a polynomial
of degree n using Horner’s method only requires O(n) arithmetic operations, which
is optimal.
Proposition 9.39 (recursion). Suppose that n ∈N0 and Xn = {xi}n
i=0 is a nodal
set in the compact interval [a, b] ⊂R. Let f ∈C([a, b]) and, for k ∈{0, . . . , n},
pk ∈Pk be the (unique) interpolating polynomial of f subordinate to Xk = {xi}k
i=0.
Then
pk(x) = pk−1(x) + bkωk,
k = 1, . . . , n,
(9.22)
holds if and only if
bk = f (xk) −pk−1(xk)
ωk(xk)
,
k = 1, . . . , n.
(9.23)
11 Named in honor of the Britsh mathematician William George Horner (1786–1837).

9.6 Divided Diﬀerences and the Newton Construction
251
As a consequence, it follows that, using the Newton basis representation (9.21),
we have f [x0] = f (x0) and
f [x0, x1, . . . , xk] = ak = bk = f (xk) −pk−1(xk)
ωk(xk)
,
k = 1, . . . , n.
Proof. We will sketch the proof and leave the details to the reader as an exercise;
see Problem 9.17. Suppose that 1 ≤k ≤n. Then
pk−1(xj) = f (xj),
j = 0, . . . , k −1,
and
pk(xj) = f (xj),
j = 0, . . . , k.
In general, however, pk(xk) ̸= pk−1(xk).
( =⇒) Suppose that (9.22) holds. Then
f (xj) = pk(xj) = pk−1(xj) + bkωk(xj),
j = 0, . . . , k.
This last equation holds identically for j = 0, . . . , k −1, since ωk(xj) = 0, for
j = 0, . . . , k −1. We get new information only from j = k. Thus,
f (xk) = pk(xk) = pk−1(xk) + bkωk(xk),
which implies, since ωk(xk) ̸= 0, that
bk = f (xk) −pk−1(xk)
ωk(xk)
.
( ⇐= ) Suppose that (9.23) holds. Then, it is easy to see that (9.22) holds.
This last result shows that the construction of the interpolating polynomial
pn ∈Pn can be done in a particular order. First, set p0 ≡f (x0). Then p1, the
interpolant using the points x0 and x1, is constructed from p0 by computing b1. p2,
the interpolant using the points x0, x1, and x2, is constructed from p1 by computing
b2, and so on. The construction proceeds according to our labeling of the nodes:
x0, x1, . . . , xn. But, as we have noted, there is no special numbering assigned to
Xn. Thus, we could choose another order for our construction; for example, reverse
order construction: xn, xn−1, . . . , x0.
Proposition 9.40 (divided diﬀerences). Suppose that n ∈N0, Xn = {xi}n
i=0 is a
nodal set in the compact interval [a, b] ⊂R, and f : [a, b] →R. Then
an = f [x0, x1, . . . , xn] =
n
X
j=0
f (xj)
n
Y
k=0
k̸=j
(xj −xk)
.
(9.24)
Proof. This follows by equating the Lagrange, Newton, and canonical forms of
the interpolating polynomial. Suppose that pn ∈Pn is the unique interpolating
polynomial of f subordinate to Xn = {xi}n
i=0. Then
pn(x) =
n
X
j=0
Lj(x)f (xj) =
n
X
k=0
akωk(x) =
n
X
j=0
cjxj,

252
Polynomial Interpolation
where Lj ∈Pn is the jth Lagrange basis element; see (9.3). Let us ﬁgure out what
are the coeﬃcients of xn when the Lagrange and Newton forms are expanded to
canonical form. For the Newton form, clearly the coeﬃcient of xn is
cn = an = f [x0, x1, . . . , xn].
On the other hand, for the Lagrange form,
cn =
n
X
j=0
f (xj)
n
Y
k=0
k̸=j
(xj −xk)
.
Corollary 9.41 (invariance). Suppose that n ∈N0, Xn = {xi}n
i=0 is a nodal set in
the compact interval [a, b] ⊂R, and f : [a, b] →R. Assume that (i0, i1, . . . , in) is
a permutation of (0, 1, . . . , n). Deﬁne ˜ω0 ≡1 and, if n ≥1,
˜ωk(x) =
k−1
Y
j=0
(x −xij),
k = 1, . . . , n.
Suppose that pn ∈Pn is the interpolating polynomial of f subordinate to Xn, so
that
pn =
n
X
j=0
ajωj(x) =
n
X
j=0
˜aj ˜ωj(x).
(9.25)
Then
an = ˜an
and
f [x0, x1, . . . , xn] = f [xi0, xi1, . . . , xin] .
Proof. This follows directly from the right-hand side of (9.24), since the sum
and product can rearranged arbitrarily. Alternately, one can simply expand the
representations in (9.25) and consider the coeﬃcient of xn.
Remark 9.42 (invariance). Let us consider what the expansions in (9.25) represent.
Pn
j=0 ajωj represents the construction of pn in the order x0, x1, . . . , xn, whereas
Pn
j=0 ˜aj ˜ωj represents the construction of pn in the permuted order xi0, xi1, . . . , xin.
Though the order of the constructions may diﬀer, the resulting interpolating
polynomial must be the same if all of the same interpolation points are ultimately
used.
Theorem 9.43 (divided diﬀerence formula). Suppose that n ∈N, Xn = {xi}n
i=0
is a nodal set in the compact interval [a, b] ⊂R, and f : [a, b] →R. Assume that
pn ∈Pn is the interpolating polynomial of f subordinate to Xn. When constructed
in the order x0, x1, . . . , xn, let us write, as in (9.21),
pn(x) =
n
X
j=0
ajωj(x).

9.6 Divided Diﬀerences and the Newton Construction
253
When pn is constructed in the reverse order xn, xn−1, . . . , x0, let us write
pn(x) =
n
X
j=0
ˆaj ˆωj(x),
where ˆω0 ≡1 and, if n ≥1,
ˆωj(x) =
j−1
Y
k=0
(x −xn−k),
j = 1, . . . , n,
and where, by deﬁnition,
ˆaj = f [xn, xn−1, . . . , xn−j],
j = 0, . . . , n.
Then it follows that
an = ˆan = an−1 −ˆan−1
x0 −xn
,
which, in turn, implies that
f [x0, x1, . . . , xn] = f [x0, x1, . . . , xn−1] −f [x1, x2, . . . , xn]
x0 −xn
,
∀n ∈N.
(9.26)
Proof. Clearly, for all x ∈R,
∆(x) =
n
X
j=0
ajωj(x) −
n
X
j=0
ˆaj ˆωj(x) = 0
and, by Corollary 9.41, an = ˆan. Using an = ˆan, the function ∆may be re-expressed
as
0 = ∆(x) = an[(x −x0) −(x −xn)](x −x1) · · · (x −xn−1) + (an−1 −ˆan−1)xn−1
+ pn−2(x),
where pn−2 ∈Pn−2. The coeﬃcient of xn−1 in the polynomial ∆is precisely
(xn −x0)an + (an−1 −ˆan−1),
which must be zero since ∆≡0:
(xn −x0)an + (an−1 −ˆan−1) = 0.
Equation 9.26 then follows immediately from the last equation and another
application of Corollary 9.41, which guarantees that
ˆan−1 = f [xn, xn−1, . . . , x1] = f [x1, x2, . . . , xn].
We have the following, as a simple consequence of the last result.
Corollary 9.44 (divided diﬀerence formula). Let n ∈N. Suppose X = {xj, . . . , xj+n}
is a nodal set in the compact interval [a, b] ⊂R. If f : [a, b] →R, then
f [xj, . . . , xj+n] = f [xj+1, . . . , xj+n] −f [xj, . . . , xj+n−1]
xj+n −xj
.
(9.27)

254
Polynomial Interpolation
Proof. See Problem 9.18.
xi
f [xi]
f [xi, xi+1]
f [xi, xi+1, xi+2]
f [xi, . . . , xi+3]
f [xi, . . . , xi+4]
x0
f [x0]
x1
f [x1]
f [x0, x1]
x2
f [x2]
f [x1, x2]
f [x0, x1, x2]
x3
f [x3]
f [x2, x3]
f [x1, x2, x3]
f [x0, x1, x2, x3]
x4
f [x4]
f [x3, x4]
f [x2, x3, x4]
f [x1, x2, x3, x4]
f [x0, x1, x2, x3, x4]
Table 9.1 A table of divided diﬀerences for computing up to quartic (fourth degree)
interpolating polynomials.
Example 9.7
Suppose that [a, b] ⊂R is a compact interval, f : [a, b] →R, and
we wish to compute a quartic interpolating polynomial for f with the nodes X4 =
{x0, . . . , x4} ⊂[a, b]. Table 9.1 depicts the construction of the divided diﬀerences,
where the order is assumed to proceed according to the numbering of the nodes:
0, 1, . . . , 4. The construction can be accomplished by recursively computing all of
the divided diﬀerences in the table. For example, using 9.27, we compute
f [x1, x2, x3] = f [x2, x3] −f [x1, x2]
x3 −x1
.
Note that all of the values in the table are needed to compute the last entry, namely
f [x0, x1, x2, x3, x4].
When all of the values are computed, the interpolating polynomial is
p4(x) = f [x0]
+ (x −x0)f [x0, x1]
+ (x −x0)(x −x1)f [x0, x1, x2]
+ (x −x0)(x −x1)(x −x2)f [x0, x1, x2, x3]
+ (x −x0)(x −x1)(x −x2)(x −x3)f [x0, x1, x2, x3, x4].
If, on the other hand, one wished to stop with the cubic interpolating polynomial,
for example, the last row of Table 9.1 need not be calculated. One will obtain
p3(x) = f [x0]
+ (x −x0)f [x0, x1]
+ (x −x0)(x −x1)f [x0, x1, x2]
+ (x −x0)(x −x1)(x −x2)f [x0, x1, x2, x3].
Example 9.8
As a concrete example, consider the data in Table 9.2. There are
ﬁve interpolation nodes, which allows us to construct an interpolating polynomial of

9.6 Divided Diﬀerences and the Newton Construction
255
degree up to four (4) for the function of interest, f . The interpolating polynomials
of degrees three (3) and four (4) are plotted in Figure 9.4. Speciﬁcally,
p3(x) = IX3[f ](x) = 5
+ (x −0.1)(−30)
+ (x −0.1)(x −0.3)
400
3

+ (x −0.1)(x −0.3)(x −0.4)

−2125
9

and
p4(x) = IX4[f ](x) = p3(x) + (x −0.1)(x −0.3)(x −0.4)(x −0.7)
5125
18

.
xi
f [xi]
f [xi, xi+1]
f [xi, xi+1, xi+2]
f [xi, . . . , xi+3]
f [xi, . . . , xi+4]
0.1
5
0.3
−1
−30
0.4
0
10
400
3
0.7
2
20
3
−25
3
−2125
9
0.9
2
0
−40
3
−25
3
5125
18
Table 9.2 A table of divided diﬀerences for computing up to quartic (fourth degree)
interpolating polynomials with real data. See Figure 9.4 for the interpolating polynomials
of degrees three (3) and four (4) constructed from these data.
It turns out that divided diﬀerences are not only useful in practical computation,
They can also be used to obtain an alternate error formula for interpolation.
Theorem 9.45 (Newton–Lagrange interpolation error). Suppose that n ∈N, X =
{xi}n
i=0 is a nodal set in the compact interval [a, b] ⊂R, and f : [a, b] →R. Let
pn ∈Pn be the interpolating polynomial of f subordinate to X. Then, for any
x ∈[a, b]\X,
f (x) −pn(x) = ωn+1(x)f [x0, x1, . . . , xn, x].
(9.28)
Proof. Since x ̸∈X, then X′ = X ∪{x} is a nodal set in [a, b]. The unique
interpolating polynomial of f subordinate to X′, which we label pn+1 ∈Pn+1, is
pn+1(t) = pn(t) + ωn+1(t)f [x0, x1, . . . , xn, x],
∀t ∈[a, b],
according to Proposition 9.39. Since x ∈[a, b] is an interpolation node,
f (x) = pn+1(x)
and the result follows.
Comparing the Lagrange and Newton error formulas, we get the following result.

256
Polynomial Interpolation
0
0.2
0.4
0.6
0.8
1.0
−30
−20
−10
0
10
20
x
f (xi)
IX3[f ](x)
IX4[f ](x)
Figure 9.4 The third- and fourth-order interpolating polynomials constructed using
Newton’s divided diﬀerences and the data from Table 9.2.
Corollary 9.46 (divided diﬀerences). Suppose that n ∈N, X = {xi}n
i=0 is a nodal
set in the compact interval [a, b] ⊂R, and f ∈Cn+1([a, b]). Let x ∈[a, b]\X be
arbitrary. Then there is a point ξ = ξ(x0, . . . , xn, x) ∈(a, b) with
min{x0, x1, . . . , xn, x} < ξ < max{x0, x1, . . . , xn, x}
such that
f [x0, x1, . . . , xn, x] = f (n+1)(ξ)
(n + 1)! .
Proof. Compare the error representations in 9.28 from Theorem 9.45 and 9.11
from Theorem 9.16.
The following is just a cosmetic, but quite useful, reformulation of the last result.
Corollary 9.47 (divided diﬀerences). Suppose that n ∈N and X = {xi}n
i=0 is a
nodal set in the compact interval [a, b] ⊂R. Let f ∈Cn([a, b]). Then there is a
point ξ = ξ(x0, . . . , xn) ∈(a, b) with
min{x0, x1, . . . , xn} < ξ < max{x0, x1, . . . , xn}

9.7 Extended Divided Diﬀerences
257
such that
f [x0, x1, . . . , xn] = f (n)(ξ)
n!
.
Proof. See Problem 9.19.
9.7
Extended Divided Diﬀerences
In this section, we give an alternate characterization of the divided diﬀerence
function that will help us to extend its deﬁnition to the case where the points
are not distinct, as well as to yield some other useful properties. First, we need a
deﬁnition.
Deﬁnition 9.48 (n-simplex). Suppose that n ∈N. The canonical n-simplex is
the set
Tn = {τ ∈Rn | τ · 1 ≤1, τ · ei ≥0, i = 1, . . . , n} ,
(9.29)
where we recall that {ej}n
j=1 is the canonical basis of Rn and 1 = [1, . . . , 1]⊺∈Rn.
Lemma 9.49 (volume). Suppose that n ∈N. The n-dimensional volume of the
canonical n-simplex is
vol(Tn) =
Z
Tn
dnτ = 1
n!,
(9.30)
where dnτ = dτ1dτ2 · · · dτn.
Proof. The proof proceeds by induction. The crucial step is to show that, for
2 ≤k ≤n,
Z
Tk
dkτ =
Z
Tk−1
Z τk=1−Pk−1
j=1 τj
τk=0
dτkdk−1τ.
The details are left for the reader as an exercise; see Problem 9.22.
Theorem 9.50 (Hermite–Genocchi Theorem12). Suppose that n ∈N, X = {xi}n
i=0
is a nodal set in the compact interval [a, b] ⊂R, and f ∈Cn([a, b]). We have
f [x0, x1, . . . , xn] =
Z
Tn
f (n)(τ0x0 + τ1x1 + · · · + τnxn)dnτ,
(9.31)
where τ = [τ1, . . . , τn]⊺∈Tn, τ0 = 1 −τ · 1, and dnτ = dτ1dτ2 · · · dτn.
Proof. The proof is by induction on n.
12 Named in honor of the French mathematician Charles Hermite (1822–1901) and the Italian
mathematician Angelo Genocchi (1817–1889).

258
Polynomial Interpolation
(n = 1) Suppose that f ∈C1([a, b]). Clearly, T1 = [0, 1] and
Z
T1
f ′(τ0x0 + τ1x1)dτ1 =
Z 1
0
f ′(x0 + τ1(x1 −x0))dτ1
=
1
x1 −x0
f (x0 + τ1(x1 −x0))|τ1=1
τ1=0
= f (x1) −f (x0)
x1 −x0
= f [x0, x1].
(n = 2) Suppose that f ∈C2([a, b]). The integration region T2 is the triangle in
(τ1, τ2) ∈R2 with the vertices (0, 0), (1, 0), and (0, 1). Then
Z
T2
f ′′(τ0x0 + τ1x1 + τ2x2)d2τ
=
Z 1
0
Z 1−τ1
0
f ′′(x0 + τ1(x1 −x0) + τ2(x2 −x0))dτ2dτ1
=
1
x2 −x0
Z 1
0
[f ′(x0 + τ1(x1 −x0) + τ2(x2 −x0))]|τ2=1−τ1
τ2=0
dτ1
=
1
x2 −x0
Z 1
0
f ′(x2 + τ1(x1 −x2))dτ1 −
Z 1
0
f ′(x0 + τ1(x1 −x0))dτ1

=
1
x2 −x0
[f [x2, x1] −f [x0, x1]]
=
1
x2 −x0
[f [x1, x2] −f [x0, x1]]
= f [x0, x1, x2].
(n = k −1) Let k ∈N, k ≥2 be arbitrary. Suppose that f ∈Ck−1([a, b]).
Assume that {z0, z1, . . . , zk−1} is an arbitrary set of k distinct points in the compact
interval [a, b]. For the induction hypothesis, let us assume that the formula holds
for n = k −1:
f [z0, z1, . . . , zk−1] =
Z
Tk−1
f (k−1)(τ0z0 + τ1z1 + · · · + τk−1zk−1)dk−1τ.
(n = k) Suppose that f ∈Ck([a, b]). Using the induction hypothesis, Corollary
9.41, and 9.26 from Theorem 9.43, we have

9.7 Extended Divided Diﬀerences
259
Z
Tk
f (k)(τ0x0 + τ1x1 + · · · + τkxk)dkτ
=
Z
Tk−1
Z τk=1−Pk−1
j=1 τj
τk=0
f (k)

x0 +
k
X
j=1
τj(xj −x0)

dτkdk−1τ
=
1
xk −x0
Z
Tk−1

f (k−1)

x0 +
k
X
j=1
τj(xj −x0)





τn=1−Pk−1
j=1 τj
τn=0
dk−1τ
=
1
xk −x0


Z
Tk−1
f (k−1)

xk +
k−1
X
j=1
τj(xj −xk)

dk−1τ
−
Z
Tk−1
f (k−1)

x0 +
k−1
X
j=1
τj(xj −x0)

dk−1τ


=
1
xk −x0
[f [xk, x1, x2, . . . , xk−1] −f [x0, x1, . . . , xk−1]]
=
1
xk −x0
[f [x1, x2, . . . , xk] −f [x0, x1, . . . , xk−1]]
= f [x0, x1, . . . , xk].
The proof is complete.
Let us now extend the deﬁnition of the divided diﬀerence function to the case
where the nodes are not necessarily distinct.
Deﬁnition 9.51 (extended divided diﬀerences). Suppose that n ∈N and [a, b] ⊂R
is a compact interval. Set
[a, b]n+1 =

[z0, z1, . . . , zn]⊺∈Rn+1  zj ∈[a, b], j = 0, . . . , n
	
.
Let f ∈Cn([a, b]). For every point z ∈[a, b]n+1, we deﬁne
f JzK = f Jz0, z1, . . . , znK =
Z
Tn
f (n)(τ0z0 + τ1z1 + · · · + τnzn)dnτ,
(9.32)
where Tn is the canonical n-simplex deﬁned in (9.29), τ = [τ1, . . . , τn]⊺∈Tn,
τ0 = 1 −τ · 1, and dnτ = dτ1dτ2 · · · dτn. The function f J · K: [a, b]n+1 →R is
called the extended divided diﬀerence of order n.
Theorem 9.52 (continuity). Suppose that n ∈N, [a, b] is a compact interval, and
f ∈Cn([a, b]). The extended ﬁnite diﬀerence function deﬁned by (9.32) satisﬁes
f J · K ∈C([a, b]n+1).
Proof. Let τ ∈Tn and τ0 = 1 −τ · 1. Deﬁne ⃗τ = [τ0, τ]⊺∈Tn+1.

260
Polynomial Interpolation
Notice now that f (n) ∈C([a, b]) and, consequently, it is uniformly continuous on
[a, b]. In other words, given ε > 0, there is a δ > 0, such that, if x and y are any
points in [a, b] satisfying |x −y| ≤δ, then it follows that
f (n)(x) −f (n)(y)
 ≤εn!.
Now suppose that z1, z2 ∈[a, b]n+1 satisfy ∥z1 −z2∥∞≤δ. Then
|⃗τ · z1 −⃗τ · z2| = |⃗τ · (z1 −z2)| ≤∥⃗τ∥1 ∥z1 −z2∥∞≤δ.
Therefore, uniformly with respect to ⃗τ, we have
f (n)(⃗τ · z1) −f (n)(⃗τ · z2)
 ≤εn!,
as long as z1, z2 ∈[a, b]n+1 satisfy ∥z1 −z2∥∞≤δ. Then,
|f Jz1K −f Jz2K| =

Z
Tn
f (n)(⃗τ · z1)dnτ −
Z
Tn
f (n)(⃗τ · z2)dnτ

≤
Z
Tn
f (n)(⃗τ · z1) −f (n)(⃗τ · z2)
 dnτ
≤
Z
Tn
εn!dnτ
= ε.
This proves the result.
We now prove a result for this extended deﬁnition, which is analogous to Corollary
9.47, but does not require the nodes to be distinct.
Proposition 9.53 (extended ﬁnite diﬀerences). Let n ∈N, [a, b] ⊂R be a compact
interval, and f ∈Cn([a, b]). Suppose that
{xj}n
j=0 ⊂[a, b].
Then there is a point ξ = ξ(x0, . . . , xn) ∈(a, b) with
min{x0, x1, . . . , xn} < ξ < max{x0, x1, . . . , xn}
such that
f Jx0, x1, . . . , xnK = f (n)(ξ)
n!
.
Proof. Set
ˆa = min{x0, x1, . . . , xn},
ˆb = max{x0, x1, . . . , xn},
and
m = min
ˆa≤x≤ˆb
f (n)(x),
M = max
ˆa≤x≤ˆb
f (n)(x).
Then, using (9.30), we deduce
m
n! ≤f Jx0, x1, . . . , xnK ≤M
n! ,

9.7 Extended Divided Diﬀerences
261
where we have again used vol(Tn) =
R
Tn dnτ = 1
n!. In other words,
m ≤f Jx0, x1, . . . , xnKn! ≤M.
By the Intermediate Value Theorem B.27, there is a point ξ ∈[ˆa, ˆb] such that
f (n)(ξ) = f Jx0, x1, . . . , xnKn!.
The following is a simple consequence.
Corollary 9.54 (extended divided diﬀerences). Suppose that [a, b] is a compact
interval and f ∈Cn([a, b]; R). Suppose that x ∈[a, b]. Then
f Jx, x, . . . , x
|
{z
}
n+1
K = f (n)(x)
n!
.
Proof. See Problem 9.23.
We showed that divided diﬀerences are invariant to permutations. Let us show
that this is the case for extended ﬁnite diﬀerences as well. First, we need an
approximation result.
Lemma 9.55 (density). Let n ∈N. Suppose that z ∈[a, b]n+1. There exists a
sequence {zℓ}∞
ℓ=1 ⊂[a, b]n+1 with the following properties.
1. For each ℓ∈N the coordinates of zℓare all distinct.
2. The sequence converges to z as ℓ→∞.
Proof. See Problem 9.24.
Proposition 9.56 (invariance). Suppose that n ∈N, [a, b] ⊂R is a compact
interval, and f ∈Cn([a, b]). Let
z = [z0, z1, . . . , zn]⊺∈[a, b]n+1
be arbitrary, and (i0, i1, . . . , in) be a permutation of (0, 1, . . . , n). Then
f Jzi0, zi1, . . . , zinK = f Jz0, z1, . . . , znK.
Proof. If the coordinates of z, i.e., z0, z1, . . . , zn, are all distinct, there is nothing
to prove.
Assume, then, that the coordinates of z are not distinct. Let {zj}∞
j=1 be the
sequence of Lemma 9.55. Since the divided diﬀerence function and the extended
divided diﬀerence function agree for distinct points, by Corollary 9.41,
f Jzℓ,i0, zℓ,i1, . . . , zℓ,inK = f Jzℓ,0, zℓ,1, . . . , zℓ,nK,
∀ℓ∈N,
where
zℓ= [zℓ,0, zℓ,1, . . . , zℓ,n]⊺.
Now
Theorem
9.52
showed
that
f J · K
is
continuous, i.e.,
f Jzi0, zi1, . . . , zinK = lim
ℓ→∞f Jzi0,ℓ, zi1,ℓ, . . . , zin,ℓK
= lim
ℓ→∞f Jz0,ℓ, z1,ℓ, . . . , zn,ℓK
= f Jz0, z1, . . . , znK,
and the claimed invariance follows.

262
Polynomial Interpolation
The following result will be needed for our analysis of numerical integration
methods in Chapter 14.
Theorem 9.57 (diﬀerentiation). Suppose that n ∈N, [a, b] ⊂R is a compact
interval, and f ∈Cn+1([a, b]). Let X = {xi}n
i=0 ⊂[a, b] be a nodal set in [a, b].
Then f Jx0, x1, . . . , xnK is continuously diﬀerentiable with respect to xn and
∂
∂xn
f Jx0, x1, . . . , xnK = f Jx0, x1, . . . , xn, xnK.
(9.33)
Proof. Using (9.27),
∂
∂xn
f Jx0, x1, . . . , xnK = lim
h→0
f Jx0, x1, . . . , xn + hK −f Jx0, x1, . . . , xnK
h
= lim
h→0
f Jx0, x1, . . . , xn + hK −f Jxn, x0, x1, . . . , xn−1K
h
= lim
h→0 f Jxn, x0, x1, . . . , xn + hK
= f Jxn, x0, x1, . . . , xnK
= f Jx0, x1, . . . , xn, xnK,
where the last equality came from Proposition 9.56. Now observe that
f Jx0, x1, . . . , xn, xnK =
Z
Tn+1
f (n+1)(τ0z0 + τ1z1 + · · · + τnzn + τn+1zn)dn+1τ.
As long as f ∈Cn+1([a, b]), f Jx0, x1, . . . , xn, xnK is a continuous function of its
arguments.
We conclude this chapter with alternate formulas for the Hermite interpolating
polynomial and its error in terms of divided diﬀerences.
Theorem 9.58 (Newton–Hermite interpolation error). Suppose that n ∈N, X =
{xj}n
j=0 is a nodal set in the compact interval [a, b] ⊂R, and f ∈C2n+2([a, b]).
Then p ∈P2n+1, deﬁned by
p(x) = f (x0) + (x −x0)f Jx0, x0K + (x −x0)2f Jx0, x0, x1K
+ (x −x0)2(x −x1)f Jx0, x0, x1, x1K
+ (x −x0)2(x −x1)2f Jx0, x0, x1, x1, x2K
+ · · · + (x −x0)2 · · · (x −xn−1)2(x −xn)f Jx0, x0, x1, x1, . . . , xn, xnK,
is the unique Hermite interpolating polynomial satisfying
p(xj) = f (xj),
p′(xj) = f ′(xj),
j = 0, . . . , n.
For any x ∈[a, b], the interpolation error has the representation
f (x) −p(x) = ω2
n+1(x)f Jx0, x0, x1, x1, . . . , xn, xn, xK.
(9.34)

Problems
263
Proof. Suppose that Z = {z0, . . . , z2n+1} is a nodal set in [a, b]. Then p ∈P2n+1,
deﬁned by
p(z) = f (z0) + (z −z0)f Jz0, z1K + (z −z0)(z −z1)f Jz0, z1, z2K
+ (z −z0)(z −z1)(z −z2)f Jz0, z1, z2, z3K
+ · · · + (z −z0) · · · (z −z2n)f Jz0, z1, . . . , z2n+1K,
is the unique Lagrange interpolating polynomial satisfying
p(zj) = f (zj),
j = 0, . . . , 2n + 1.
Owing to Theorem 9.45, if f ∈C2n+2([a, b]), the error may be expressed as follows:
for any x ∈[a, b],
f (x) −p(x) = (x −z0) · · · (x −z2n+1)f Jz0, z1, . . . , z2n+1, xK.
Use now the continuity of the extended divided diﬀerences to take the limits
z0, z1 →x0,
z2, z3 →x1,
. . . ,
z2n, z2n+1 →xn
and show that the desired properties hold. The details are left to the reader as an
exercise; see Problem 9.25.
Problems
9.1
Prove Theorem 9.4. In fact, prove the following more general result. Suppose
that Z = {zi}n
i=0 ⊂C is a set of n + 1 ∈N distinct points. Then the matrix
V =


1
z0
z2
0
· · ·
zn
0
1
z1
z2
1
· · ·
zn
1
...
...
...
...
1
zn
z2
n
· · ·
zn
n

∈C(n+1)×(n+1)
is invertible and, in particular,
det(V) =
Y
0≤i<j≤n
(zj −zi) ̸= 0.
9.2
Prove Proposition 9.7.
9.3
Prove Proposition 9.10.
9.4
Complete the proof of Theorem 9.11.
9.5
Prove Proposition 9.14.
9.6
Complete the proof of Theorem 9.15.
9.7
Suppose that X = {xi}n
i=0 is a nodal set of size n + 1 ∈N in the compact
interval [a, b] ⊂R and LX = {Li}n
i=0 ⊂Pn is the Lagrange basis subordinate to X.
Prove that if 0 ≤m ≤n, then
n
X
i=0
xm
i Li(x) = xm,
∀x ∈R.

264
Polynomial Interpolation
9.8
Prove Proposition 9.18 by showing the following. Let n ∈N. Deﬁne
h = b −a
n
,
and, for i = 0, . . . , n, set xi = a + ih. Show that the nodal polynomial ωn+1,
introduced in Deﬁnition 9.13, satisﬁes
max
a≤x≤b |ωn+1(x)| ≤Ωn+1,
where
Ωn+1 = n!
4 hn+1.
9.9
Prove Theorem 9.20 by the following steps. Let n ∈N. Suppose that X =
{xi}n
i=0 is a nodal set in the compact interval [a, b] ⊂R and f ∈C1([a, b]). Deﬁne,
for 0 ≤ℓ≤n,
F0,ℓ(x) = (Lℓ(x))2(1 −2L′
ℓ(xℓ)(x −xℓ)),
F1,ℓ= (Lℓ(x))2(x −xℓ),
where Lℓis the Lagrange basis polynomial of order ℓ, deﬁned in (9.3), and
p(x) =
n
X
ℓ=0
[F0,ℓ(x)f (xℓ) + F1,ℓ(x)f ′(xℓ)] .
a)
Prove that
F0,ℓ(xk) = δk,ℓ,
F ′
0,ℓ(xk) = 0,
F1,ℓ(xk) = 0,
F ′
1,ℓ(xk) = δk,ℓ,
and, therefore, that, for all i = 0, . . . , n, we have
f (xi) = p(xi),
f ′(xi) = p′(xi).
b)
Prove that p is the unique polynomial in P2n+1 with the property above.
9.10
Prove Theorem 9.35.
9.11
Let f ∈C([−1, 1]). Construct the Lagrange interpolation polynomial p1 ∈
P1 for f using the interpolation nodes x0 = −1 and x1 = 1. Show that if f ∈
C2([−1, 1]), then
|f (x) −p1(x)| ≤F2
2
 1 −x2
≤F2
2 ,
∀x ∈[−1, 1],
where F2 = ∥f ′′∥L∞(−1,1). Find a function f and a point x for which equality is
achieved in the estimates above.
9.12
This problem is concerned with the estimate provided in Theorem 9.16.
a)
Compute the Lagrange interpolating polynomial of degree one for the function
f (x) = x3 using the interpolation nodes x0 = 0 and x1 = a. Verify Theorem
9.16 by a direct calculation, showing that, in this case, ξ has the unique value
ξ = (x + a)/3.
b)
Repeat the calculation for the function f (x) = (2x −a)4. Show that, in this
case, there are two possible values for ξ and give their values.

Problems
265
9.13
Let n ∈N0. Given the nodal set X = {xi}n+1
i=0 and Y = {yi}n+1
i=0 ⊂R, let
qn, rn ∈Pn be the Lagrange interpolating polynomials for the coordinate sets
Q = {(xi, yi) | i = 0, 1, . . . , n} ,
R = {(xi, yi) | i = 1, 2, . . . , n + 1} ,
respectively. Deﬁne
pn+1(x) = (x −x0)rn(x) −(x −xn+1)qn(x)
xn+1 −x0
.
Show that pn+1 is the Lagrange interpolating polynomial of degree n + 1 for the
coordinate set P = {(xi, yi) | i = 0, 1, . . . , n + 1}.
9.14
Construct the Hermite interpolating polynomial of degree three (3) for the
function f (x) = x5, using the points x0 = 0 and x1 = a, and show that it has the
form p3(x) = 3a2x3 −2a3x2. Verify Theorem 9.21 by direct calculation, showing
that, in this case, ξ has the unique value ξ = (x + 2a)/5.
9.15
Let n ∈N0 and X = {xi}n
i=0 be a nodal set in the compact interval [a, b] ⊂
R. Let f ∈C1([a, b]) and p ∈P2n+1 be the Hermite interpolating polynomial of f
subject to X. State and prove an error estimate for f ′(x)−p′(x) with x ∈[a, b]\X.
9.16
Prove Proposition 9.37.
9.17
Complete the proof of Proposition 9.39.
9.18
Prove Corollary 9.44.
9.19
Prove Corollary 9.47.
9.20
Let [a, b] ⊂R be a compact interval, n ∈N0, and f ∈C([a, b]). Show that
f ∈Pn if and only if for every nodal set X = {xj}n
j=0 ⊂[a, b] we have
f [x0, . . . , xn] = 0.
9.21
Let [a, b] ⊂R be a compact interval, n ∈N0, and g, h ∈C([a, b]). Deﬁne
f (x) = g(x)h(x). Show that, for every nodal set X = {xj}n
j=0 ⊂[a, b], we have
f [x0, . . . , xn] =
n
X
j=0
g[x0, . . . , xj]h[xj+1, . . . , xn].
9.22
Prove Lemma 9.49.
9.23
Prove Corollary 9.54.
9.24
Prove Lemma 9.55.
9.25
Prove Theorem 9.58.

10
Minimax Polynomial Approximation
In this chapter — which is inﬂuenced by the presentations in the excellent books
by Davis [24], Isaacson and Keller [46], S¨uli and Mayers [89], Trefethen [95], and
Powell [71] — we look for best approximations of a given function of interest
by polynomials in the max norm. This is called the minimax problem. For further
insight, we also refer the reader to [1, 73].
While, the max norm is a simple-to-understand object, it turns out that ﬁnding
minimax approximations is a highly nontrivial task, except for very simple cases.
There is a way to characterize minimax approximations, based on a type of
oscillation property, but this requires some complicated (and beautiful) theory. We
will ﬁnd an interesting connection between this characterization and what are called
Chebyshev orthogonal polynomials and Chebyshev interpolation. This connection
will give us some further insight into the Runge1 phenomenon that we encountered
in the last chapter.
To motivate this topic, let us consider a simple function of interest,
f (x) = x2,
0 ≤x ≤1.
Now let us search for the linear (n = 1) polynomial p1(x) = a1x + a0 that yields
the best approximation of f , where the error is measured as
∥f −p1∥L∞(0,1) = max
0≤x≤1 |f (x) −p1(x)|.
The best approximation will minimize the max norm of the error, hence the term
minimax. The minimizer, as we see from Problem 10.11, is p1(x) = x −1
8, and
min
p1∈P1 ∥f −p1∥L∞(0,1) = 1
8.
Interestingly, the error function, e(x) = f (x) −p1(x) = x2 −x + 1
8 equi-oscillates,
namely,
e(0) = 1
8,
e
1
2

= −1
8,
e(1) = 1
8.
In other words, with precisely alternating signs, the error function takes its largest
values at exactly three (n + 2) points. We will see that this curious property is
always present in the minimax approximations.
Before we get into the theory, we refer the reader to Appendix D for important
notation and all of the properties of spaces of functions that we shall use in the
the chapter.
1 Named in honor of the German mathematician Carl David Tolm´e Runge (1856–1927).

10.1 Minimax: Best Approximation in the ∞-Norm
267
10.1
Minimax: Best Approximation in the ∞-Norm
Let us deﬁne our problem of interest and give a detailed study of it.
Deﬁnition 10.1 (minimax). Let f ∈C([a, b]) and n ∈N0. We say that the
polynomial p ∈Pn is a best polynomial approximation of f in the L∞-norm
or a minimax polynomial if and only if
∥f −p∥L∞(a,b) = inf
q∈Pn ∥f −q∥L∞(a,b) .
If such a polynomial exists, we write
p ∈argmin
q∈Pn
∥f −q∥L∞(a,b) .
If, additionally, p is unique, we write
p = argmin
q∈Pn
∥f −q∥L∞(a,b) .
Let us immediately show that a minimax polynomial always exists.
Theorem 10.2 (existence). Let f ∈C([a, b]) and n ∈N0. There exists at least
one minimax polynomial approximation to f , i.e., there is a polynomial, p ∈Pn,
with
p ∈argmin
q∈Pn
∥f −q∥L∞(a,b) .
Proof. We follow the proofs in [24] and [89]; see also Problem 5.1. Deﬁne the
function E : Rn+1 →[0, ∞) via
E(c0, . . . , cn) = ∥f −q∥L∞(a,b) ,
q(x) =
n
X
i=0
cixi.
We leave it to the reader to prove that E ∈C
 Rn+1; [0, ∞)

, i.e., it is continuous;
see Problem 10.1. Now deﬁne
S =
n
[c0, . . . , cn]⊺∈Rn+1  E(c0, . . . , cn) ≤∥f ∥L∞(a,b) + 1
o
.
The set S is nonempty: 0 ∈S, since E(0, . . . , 0) = ∥f ∥L∞(a,b). In addition, the
set S is closed and bounded in Rn+1. In other words, S is a compact subset of
Rn+1. Therefore, Theorem B.47 guarantees that E attains its minimum on S. Let
us label a point of attainment c⋆= [c⋆
0, . . . , c⋆
n]⊺, and deﬁne
p⋆(x) =
n
X
i=0
c⋆
i xi ∈Pn.
Then
m = E(c⋆
0, . . . , c⋆
n) = ∥f −p⋆∥L∞(a,b) .
It turns out that this is the minimum of E over all of Rn+1, not just S. To see this,
observe that
m = E(c⋆
0, . . . , c⋆
n) ≤E(0, . . . , 0) = ∥f ∥L∞(a,b) .

268
Minimax Polynomial Approximation
Suppose now that [c0, . . . , cn]⊺∈Rn+1\S. By deﬁnition,
E(c0, . . . , cn) > ∥f ∥L∞(a,b) + 1 > ∥f ∥L∞(a,b) ≥m.
This shows that
m ≤E(c0, . . . , cn),
∀[c0, . . . , cn]⊺∈Rn+1,
and, in fact,
m = ∥f −p⋆∥L∞(a,b) =
inf
[c0,...,cn]⊺∈Rn+1 E(c0, . . . , cn) = inf
q∈Pn ∥f −q∥L∞(a,b) .
In other words,
p⋆∈argmin
q∈Pn
∥f −q∥L∞(a,b) .
We shall only go into depth regarding best approximations in the L∞-norm, but
the following is of theoretical interest.
Deﬁnition 10.3 (best approximation). Suppose that w is a weight function on [a, b]
and 1 ≤p < ∞. Let f ∈C([a, b]) and n ∈N0. We say that the polynomial q ∈Pn
is a best polynomial approximation of f in the Lp
w(a, b)-norm if and only if
∥f −q∥Lp
w (a,b) = inf
r∈Pn ∥f −r∥Lp
w (a,b) .
If such a polynomial exists, we write
q ∈argmin
r∈Pn
∥f −r∥Lp
w (a,b) ,
and if q is unique, we write
q = argmin
r∈Pn
∥f −r∥Lp
w (a,b) .
A proof of the following can be obtained by adapting the proof of Theorem 10.2.
Theorem 10.4 (existence). Suppose that w is a weight function on [a, b] and
1 ≤p < ∞. Let f ∈C([a, b]) and n ∈N0 be ﬁxed. There exists at least one
polynomial, q ∈Pn, satisfying
q ∈argmin
r∈Pn
∥f −r∥Lp
w (a,b) .
Proof. See Problem 10.2.
We note that the previous two results mention the existence of a best polynomial
approximation, but say nothing about their uniqueness. It turns out that this is a
delicate issue which heavily depends on the norm in question, as the following simple
ﬁnite-dimensional examples show.
Example 10.1
The essence of best polynomial approximation is that we are trying
to ﬁnd the best (in some norm) approximation to an element of a vector space
by elements of a subspace. If the ambient space is Rn, for some n ∈N, with the

10.1 Minimax: Best Approximation in the ∞-Norm
269
x
y
f
Figure 10.1 The best approximation in the ∞-norm of the vector f = [0, 1]⊺by vectors
of the form [x, 0]⊺is not unique.
2-norm then the results of Section 5.2 show that the best approximation is unique
and realized by a projection matrix.
Example 10.2
Consider now R2 under the ∞-norm. Assume that we wish to
approximate the vector f = [0, 1]⊺by elements of the subspace
S =

[x, y]⊺∈R2  y = 0
	
.
In other words, we are trying to ﬁnd
inf
v∈S ∥f −v∥∞.
Figure 10.1 shows that all vectors of the form [x, 0]⊺with |x| ≤1 realize this
inﬁmum. On the other hand, we leave it to the reader, see Problem 10.3, to show
that if the subspace is
W =

[x, y]⊺∈R2  x = y
	
,
then the inﬁmum is realized by a unique element.
The previous two examples show that the issue of uniqueness of best approxima-
tions is related to convexity of norms. It turns out that the 2-norm is strictly convex,
whereas the ∞-norm is not. We will deal more in depth with the minimization of
convex and strictly convex functions in Chapter 16.
Before we move on, we point out that, for all p ∈[1, ∞), the best polynomial
approximation to a continuous function in the Lp
w norm — whose existence is
guaranteed by Theorem 10.4 — is, in fact, unique. For a proof, we refer the reader
to [24].

270
Minimax Polynomial Approximation
Let us focus here on the issue of uniqueness for the case of the L∞-norm. We
begin with an important technical lemma.
Lemma 10.5 (comparison). Suppose that a, b, c ∈R satisfy
|a| > |b| ≥0,
c = a −b.
Then
1. The numbers a and c are nonzero.
2. a > 0 if and only if c > 0.
3. a < 0 if and only if c < 0.
Observe that properties 2 and 3 may be summarized by saying that a and c must
have the same sign.
Proof. We begin by proving property 2.
( ⇐= ) Suppose that c > 0. Then a > b. Since |a| > |b|,
a > b > −|a|,
which implies that
0 < a + |a| =
(
0,
a < 0,
2a,
a > 0.
The only possibility is that a > 0.
( =⇒) Suppose that a > 0. Then c > −b. If b < 0, we get the conclusion we
want, namely c > 0. On the other hand, if b ≥0, then, by assumption,
a = |a| > |b| = b ≥0.
So a −b > 0. But c = a −b > 0.
Next, we prove property 3.
( ⇐= ) Suppose that c < 0. Then b > a. Since |a| > |b|,
a < b < |a|,
which implies that
0 < |a| −a =
(
0,
a > 0,
−2a,
a < 0.
The only possibility is that a < 0.
( =⇒) Suppose that a < 0. Then c < −b. If b > 0, we get the result we want,
namely c < 0. If b ≤0, then, by assumption,
−a = |a| > |b| = −b ≥0.
So a −b < 0. But c = a −b < 0.
Finally, to prove property 1, observe that |a| > |b| ≥0 implies that a cannot be
zero. The equivalences above imply that c cannot be zero either.

10.1 Minimax: Best Approximation in the ∞-Norm
271
Theorem 10.6 (de la Vall´ee Poussin2). Suppose that f ∈C([a, b]), n ∈N0, and
q ∈Pn. Assume that there are n + 2 distinct points in [a, b], denoted
a ≤x0 < x1 < · · · < xn < xn+1 ≤b
such that {f (xi) −q(xi)}n+1
i=0
is a strictly alternating sequence, none of the
diﬀerences vanishing. Let p ∈Pn be a minimax polynomial. Then
∥f −p∥L∞(a,b) ≥
min
0≤i≤n+1 |f (xi) −q(xi)| = χ > 0.
(10.1)
Proof. To get a contradiction suppose that, in (10.1), χ > ∥f −p∥L∞(a,b). This
implies that, for all i = 0, . . . , n + 1,
|f (xi) −q(xi)| > |f (xi) −p(xi)| ≥0,
i = 0, 1, . . . , n + 1,
(10.2)
where p ∈Pn is the minimax polynomial. Now write, for i = 0, 1, . . . , n + 1,
q(xi) −p(xi) = [q(xi) −f (xi)] −[p(xi) −f (xi)] .
(10.3)
Observe now that, for all i = 0, . . . , n + 1, q(xi) −p(xi) and q(xi) −f (xi) have
the same signs, and neither are zero. This follows from the technical Lemma 10.5.
This implies that q −p ∈Pn changes sign n + 1 times; in other words, q −p has
n + 1 distinct zeros in [a, b]. This, in turn, implies that the diﬀerence is the zero
polynomial, i.e., q ≡p. But, if that is the case, (10.2) cannot be true. We have a
contradiction. Thus, (10.1) must be true.
We have then arrived at the celebrated Chebyshev Oscillation Theorem, which
is a key to proving the uniqueness of the minimax polynomial [89, 95]. We remark
that Davis [24] uses a convexity argument to prove uniqueness, a diﬀerent tack
from that used here.
Deﬁnition 10.7 (equi-oscillation). Suppose that f ∈C([a, b]) and n ∈N0. We say
that a polynomial q ∈Pn has the equi-oscillation property with respect to f if
and only if there is a sequence of (at least) n + 2 distinct points in [a, b], called
critical points and labeled
a ≤x0 < x1 < · · · < xn < xn+1 ≤b
such that
|f (xi) −q(xi)| = ∥f −q∥L∞(a,b) = R(f , q),
i = 0, . . . , n + 1,
(10.4)
and
f (xi) −q(xi) = ±(−1)iR(f , q),
i = 0, . . . , n + 1.
(10.5)
Let k ∈N0 with k ≤n. We say that q ∈Pn has a defective equi-oscillation
property of degree k if and only if there are at most k critical points
a ≤x0 < x1 < · · · < xk−1 < xk ≤b
2 Named in honor of the Belgian mathematician Charles Jean de la Vall´ee Poussin
(1866–1962).

272
Minimax Polynomial Approximation
such that
|f (xi) −q(xi)| = R(f , q),
f (xi) −q(xi) = ±(−1)iR(f , q),
i = 0, . . . , k.
Remark 10.8 (defective equi-oscillation). The defective equi-oscillation property
is essentially the negation of the equi-oscillation property. Generically, since f −p
is continuous, there will always be at least one point where ±R(f , q) is attained. In
other words, there will always be one critical point. The polynomial is defective if
and only if there are not enough critical points where the polynomial equi-oscillates.
The following is the Chevyshev Oscillation Theorem.3
Theorem 10.9 (Chebyshev Oscillation Theorem). Suppose that f ∈C([a, b]) and
n ∈N0. p ∈Pn is a minimax polynomial approximation of f if and only if p has the
equi-oscillation property with respect to f .
Proof. To avoid a trivial case, we assume that f /∈Pn.
( ⇐= ) Suppose that p ∈Pn has the equi-oscillation property, i.e., that there is a
sequence of n + 2 distinct points, denoted
a ≤x0 < x1 < · · · < xn < xn+1 ≤b,
such that (10.4) and (10.5) hold. Deﬁne
E(f ) = inf
q∈Pn ∥f −q∥L∞(a,b) = min
q∈Pn ∥f −q∥L∞(a,b) .
(10.6)
This inﬁmum is achieved as we have seen in Theorem 10.2, which justiﬁes the use
of the minimum. By Theorem 10.6, it must be that E(f ) ≥R(f , p) with R(f , p)
as deﬁned in (10.4). On the other hand, since E(f ) is deﬁned as a minimum,
E(f ) ≤R(f , p). Hence, E(f ) = R(f , p) and p is a minimax polynomial for f .
( =⇒) Suppose that p ∈Pn is a minimax polynomial with respect to f . We want
to prove that the equi-oscillation property holds.
(n = 0) First, let us take care of the n = 0 case, for which we need two critical
points. For this case, we can directly construct these points. Note that |f (x)−p(x)|
is continuous on [a, b] and so it attains its maximum value, which is, of course,
R(f , p). This shows then that
S0 = {x ∈[a, b] | |f (x) −p(x)| = R(f , p)} ̸= ∅.
Therefore, for the ﬁrst critical point, we choose
x0 = inf S0.
Observe now that, since S0 ⊆[a, b] and [a, b] is closed, we have that x0 ∈[a, b].
However, x0 ̸= b, as the contrary will yield a contradiction. To see this, let us
suppose that x0 = b. Observe that this implies that there can be no point x ∈[a, b)
such that |f (x) −p(x)| = R(f , p) > 0. There are then two sub-cases.
3 Named in honor of the Russian mathematician Pafnuty Lvovich Chebyshev (1821–1894).

10.1 Minimax: Best Approximation in the ∞-Norm
273
1. If f (x0) −p(x0) = R(f , p) > 0, then
−R(f , p) < f (x) −p(x) ≤R(f , p),
∀x ∈[a, b].
(10.7)
2. If f (x0) −p(x0) = −R(f , p) < 0, then
−R(f , p) ≤f (x) −p(x) < R(f , p),
∀x ∈[a, b].
Let us work with the ﬁrst sub-case, as the second one can be treated in a similar
fashion. If (10.7) holds, by continuity, there is δ ∈(0, R(f , p)) such that
−R(f , p) + δ ≤f (x) −p(x) ≤R(f , p),
∀x ∈[a, b].
Subtracting δ/2,
−(R(f , p) −δ/2) ≤f (x) −(p(x) + δ/2) ≤R(f , p) −δ/2,
∀x ∈[a, b].
But this proves that, for p + δ/2 ∈Pn, we have R(f , p + δ/2) < R(f , p). In other
words, p ∈Pn is not the minimax polynomial. This is the sought-after contradiction
and, consequently, it must be that a ≤x0 < b.
Without loss of generality, let us assume that f (x0) −p(x0) = R(f , p) > 0.
Next, we need to show that there is a point x1 ∈(x0, b] such that f (x1) −p(x1) =
−R(f , p). To get a contradiction, suppose that no such point exists. If there is no
such point, then, since f (x0) −p(x0) = R(f , p),
−R(f , p) < f (x) −p(x) ≤R(f , p),
∀x ∈[a, b].
Since f −p is continuous, there is δ ∈(0, R(f , p)) such that
−R(f , p) + δ ≤f (x) −p(x) ≤R(f , p),
∀x ∈[a, b].
Subtracting δ/2,
−(R(f , p) −δ/2) ≤f (x) −(p(x) + δ/2) ≤R(f , p) −δ/2,
∀x ∈[a, b].
This, once again, shows that, for p+δ/2 ∈Pn, we have R(f , p+δ/2) < R(f , p). In
other words, p ∈Pn is not the minimax polynomial, a contradiction. So x1 ∈(x0, b]
must exist. In fact, we can deﬁne
x1 = inf S1,
S1 = {x ∈(x0, b] | f (x) −p(x) = −R(f , p)} ,
so that f (x1) −p(x1) = −R(f , p), x1 ∈(x0, b]. The proof is ﬁnished for the case
n = 0.
(n ≥1) We deﬁne the critical points recursively as follows. Assume that x0 ∈[a, b)
has been found, as above, such that |f (x0) −p(x0)| = R(f , p). Without loss of
generality, let us assume that f (x0) −p(x0) = −R(f , p). Then deﬁne
xi = inf Si,
Si =

x ∈(xi−1, b]
 f (x) −p(x) = −(−1)iR(f , p)
	
for i = 1, . . . , k, where k is maximal. If k ≥n + 1, the proof is complete. If
1 ≤k ≤n, then p has a defective equi-oscillation property. To get a contradiction,
let us assume that 1 ≤k ≤n, i.e., p is defective.

274
Minimax Polynomial Approximation
0
0.5
1.0
1.5
2.0
−1.0
−0.5
0
0.5
1.0
(x, f (x) −p(x))
(xi, f (xi) −p(xi))
xi
(ξi, f (ξi) −p(ξi))
ξi
(x, s(x))
Figure 10.2 The equi–oscillation property. The notation used in the ﬁgure is deﬁned in
the proof of Theorem 10.9.
We have
f (xi) −p(xi) = −(−1)iR(f , p),
i = 0, . . . , k ≤n;
see Figure 10.2. Let us deﬁne a sequence of points, {ξi}k−1
i=0 , as follows:
ξ0 = 1
2 (η0 + ζ0),
where
η0 = inf {η ∈[a, x1] | f (η) −p(η) = R(f , p)}
and
ζ0 = sup {ζ ∈[a, x1] | f (ζ) −p(ζ) = −R(f , p)} .
We claim that x0 ≤ζ0 < η0 ≤x1. If this were not true, then f −p would equi-
oscillate at another pair of points previously unaccounted for. Next, deﬁne
ξ1 = 1
2 (η1 + ζ1),
where
η1 = inf {η ∈[ζ0, x2] | f (η) −p(η) = −R(f , p)}
and
ζ1 = sup {ζ ∈[ζ0, x2] | f (ζ) −p(ζ) = R(f , p)} .

10.1 Minimax: Best Approximation in the ∞-Norm
275
We claim that x1 ≤ζ1 < η1 ≤x2. The argument is the same as before. Likewise,
deﬁne
ξ2 = 1
2 (η2 + ζ2),
where
η2 = inf {η ∈[ζ1, x3] | f (η) −p(η) = R(f , p)}
and
ζ2 = sup {ζ ∈[ζ1, x3] | f (ζ) −p(ζ) = −R(f , p)} .
We claim that x2 ≤ζ2 < η2 ≤x3. The rest of the points can be deﬁned similarly,
so that the sequence {ξi}k−1
i=0 separates {xi}k
i=0:
x0 < ξ0 < x1 < ξ1 < · · · < xk−1 < ξk−1 < xk.
Again, see Figure 10.2.
Deﬁne the sets
S−=
(
(a, ξ0) ∪(ξ1, ξ2) ∪(ξ3, ξ4) ∪· · · ∪(ξk−1, b),
k = 2m, m ∈N,
(a, ξ0) ∪(ξ1, ξ2) ∪(ξ3, ξ4) ∪· · · ∪(ξk−2, ξk−1),
k = 2m −1, m ∈N,
and
S+ =
(
(ξ0, ξ1) ∪(ξ2, ξ3) ∪(ξ4, ξ5) ∪· · · ∪(ξk−2, ξk−1),
k = 2m, m ∈N,
(ξ0, ξ1) ∪(ξ2, ξ3) ∪(ξ4, ξ5) ∪· · · ∪(ξk−1, b),
k = 2m −1, m ∈N.
Because of the separation property, for each 0 ≤i ≤k −1,
|f (ξi) −p(ξi)| < R(f , p)
and there is a number ε > 0 such that
−R(f , p) ≤f (x) −p(x) ≤R(f , p) −ε,
x ∈S−
(10.8)
and
−R(f , p) + ε ≤f (x) −p(x) ≤R(f , p),
x ∈S+.
(10.9)
Thus, for each of the k + 1 intervals
[a, ξ0], [ξ0, ξ1], . . . , [ξk−2, ξk−1], [ξk−1, b],
the error function f (x) −p(x) takes on one and only one of the extreme values
+R(f , p) or −R(f , p) and is bounded well away from the other extreme value.
Again, take a careful look at Figure 10.2.
Now deﬁne
r(x) = (x −ξ0)(x −ξ1) · · · (x −ξk−1) ∈Pk ⊆Pn.
This polynomial has one sign in each of the k + 1 separating intervals constructed
above. In fact, the sign will clearly oscillate. Deﬁne
M = max
a≤x≤b |r(x)|

276
Minimax Polynomial Approximation
and
q(x) = p(x) + εs(x),
s(x) = (−1)k+1 r(x)
2M ,
x ∈R
for ε > 0 suﬃciently small. Clearly, q ∈Pn, as long as 1 ≤k ≤n.
Notice that, by construction,
−1
2 ≤s(x) < 0,
∀x ∈S−,
0 < s(x) ≤1
2,
∀x ∈S+.
(10.10)
Using inequalities (10.8) and (10.9) and the deﬁnition of q, we ﬁnd
−R(f , p) −εs(x) ≤f (x) −q(x) ≤R(f , p) −ε[1 + s(x)],
∀x ∈S−
and
−R(f , p) + ε[1 −s(x)] ≤f (x) −q(x) ≤R(f , p) −εs(x),
∀x ∈S+.
Thus, applying (10.10) in the last two inequalities, we observe that
−R(f , p) < f (x) −q(x) < R(f , p),
∀x ∈[a, b].
It is straightforward now to see that R(f , q) < R(f , p). Since q ∈Pn, this is a
contradiction to the fact that p is a minimax polynomial. It must be that k ≥n +1
and, therefore, p has the equi-oscillation property.
As a corollary of Chebyshev Oscillation Theorem, we can obtain uniqueness of a
minimax polynomial.
Corollary 10.10 (uniqueness). For every n ∈N0 the minimax polynomial p ∈Pn
of f ∈C([a, b]) is unique.
Proof. Suppose that there are two minimax polynomials of degree n, labeled
p1, p2 ∈Pn. Consistent with previous notation, deﬁne
E(f ) = inf
q∈Pn ∥f −q∥L∞(a,b) ,
∥f −pi∥L∞(a,b) = R(f , pi),
i = 1, 2.
Then
E(f ) = R(f , p1) = R(f , p2).
Set q = 1
2(p1 + p2). Then, for all x ∈[a, b],
|f (x) −q(x)| ≤1
2|f (x) −p1(x)| + 1
2|f (x) −p2(x)| ≤E(f ).
(10.11)
Therefore, q is yet another minimax polynomial of degree n. By the last theorem,
q has the equi-oscillation property, i.e., there are n + 2 distinct critical points
a ≤x0 < x1 < · · · < xn < xn+1 ≤b
such that (10.4) and (10.5) hold for q. From (10.11), it follows that, at the critical
points of q,

10.2 Interpolation Error and the Lebesgue Constant
277
E(f ) = |f (xi) −q(xi)|
≤

1
2f (xi) + 1
2p1(xi) + 1
2f (xi) + 1
2p2(xi)

≤1
2|f (xi) −p1(xi)| + 1
2|f (xi) −p2(xi)|
≤E(f ).
The only possibility is that
|f (xi) −p1(xi)| = E(f ) = |f (xi) −p2(xi)|,
i = 0, . . . , n + 1,
or, equivalently,
f (xi) −p1(xi) = ±E(f ) = f (xi) −p2(xi),
i = 0, . . . , n + 1,
such that the alternation pattern is in sync. Therefore,
p1(xi) −p2(xi) = f (xi) −p2(xi) −[f (xi) −p1(xi)] = 0,
i = 0, . . . , n + 1.
It follows that p1 ≡p2 in Pn.
With the help of the equi-oscillation property, at least for smooth functions, we
can relate the minimax and interpolating polynomials.
Corollary 10.11 (error). Suppose that n ∈N0 and f ∈Cn+1([a, b]). Let p ∈Pn
be the unique minimax polynomial of f . There exist n + 1 distinct points x0, . . . , xn
in the interval [a, b] such that f (xi) = p(xi), i = 0, . . . , n. Furthermore, for every
x ∈[a, b]\{x0, . . . , xn}, there is a point ξ = ξ(x) ∈(a, b) such that
f (x) −p(x) = (x −x0) · · · (x −xn)
(n + 1)!
f (n+1)(ξ).
Proof. Because of equi-oscillation, there are (n + 1) distinct points X = {xi}n
i=0 ⊂
[a, b], such that f (xi) = p(xi). (Note that these points are not the same as the
critical points of equi-oscillation.) Thus, p is the unique interpolating polynomial
for f that agrees with f at these n + 1 distinct points. Now, we apply Theorem
9.16.
10.2
Interpolation Error and the Lebesgue Constant
In this section, we state and prove a fundamental error estimate for interpolation
that involves the Lebesgue constant and the minimax polynomial approximation.
Theorem 10.12 (interpolation versus minimax). Suppose that n ∈N0, X = {xi}n
i=0
is a nodal set in the compact interval [a, b] ⊂R, f
∈C([a, b]), p ∈Pn is
the Lagrange interpolating polynomial for f subordinate to X, and p⋆∈Pn is
the minimax polynomial approximation of f . Then
∥f −p∥L∞(a,b) ≤(1 + Λ(X)) ∥f −p⋆∥L∞(a,b) ,
where Λ(X) is the Lebesgue constant subordinate to X, deﬁned in (9.2).

278
Minimax Polynomial Approximation
Proof. We begin by writing f −p = (f −p⋆) −(p −p⋆). Observe that p −p⋆is
the Lagrange interpolating polynomial for f −p⋆subordinate to X. Thus,
IX[f −p⋆] = p −p⋆.
Thus,
∥f −p∥L∞(a,b) = ∥(f −p⋆) −(p −p⋆)∥L∞(a,b)
= ∥(f −p⋆) −IX[f −p⋆]∥L∞(a,b)
≤∥(f −p⋆)∥L∞(a,b) + ∥IX[f −p⋆]∥L∞(a,b)
≤∥(f −p⋆)∥L∞(a,b) + ∥IX∥∞∥f −p⋆∥L∞(a,b)
= (1 + Λ(X)) ∥f −p⋆∥L∞(a,b) .
This theorem guarantees that if the Lebesgue constant is not large, then
the interpolant subordinate to X is almost as good as the best approximation.
Interestingly, for uniformly spaced nodes, the Lebesgue constant can be quite large,
for large n. We will say more about this after we have described the so-called
Chebyshev–Lagrange interpolation.
10.3
Chebyshev Polynomials
In light of the uniqueness result of Corollary 10.10, we can properly talk about the
best polynomial approximation to a (continuous) function in the ∞-norm. For a few
very particular cases, we can construct the minimax by using the equi-oscillation
property guaranteed by Theorem 10.9.
Example 10.3
Let n ∈N0, f (x) = xn+1, and p ∈Pn be its minimax approxima-
tion on [−1, 1]. Then there must be X = {xj}n+1
j=0 ⊂[−1, 1] distinct critical points
where
|f (xj) −p(xj)| = R(f , p) = E(f ) > 0,
j = 0, . . . , n + 1,
where R(f , p) is deﬁned in (10.4) and E(f ) is deﬁned in (10.6). As usual, we
assume that the critical points are ordered in an increasing fashion. Notice now
that e = f −p ∈Pn+1 and, setting E = E(f ),
E2 −e2 ∈P2n+2,
E2 −e2(xj) = 0,
j = 0, . . . , n + 1.
Of course, e2(x) ≤E2 for all x ∈[−1, 1]. This shows that the polynomial e2 has
a relative maximum at all the critical points; consequently, if xj ∈(−1, 1),
de2
dx (xj) = 0,
which implies that
d(E2 −e2)
dx
(xj) = 0,
∀xj ∈(−1, 1).

10.3 Chebyshev Polynomials
279
Observe, in addition, that each xj ∈(−1, 1) is a zero of multiplicity (at least) two
of the polynomial E2 −e2 ∈P2n+2. By simple counting, there can only be n such
interior double roots. But this implies that x0 = −1 and xn+1 = 1 are the remaining
critical points, and these are simple zeros. (You may have to think about that for
a moment.)
On the other hand, the polynomial
(1 −x2)[e′(x)]2 ∈P2n+2
has simple zeros at x0 = −1 and xn+1 = 1 and zeros of multiplicity two for all xj
with j = 1, . . . , n (interior critical points). Thus, necessarily, we must have that
(1 −x2)[e′(x)]2 = c(E2 −e2)
for some constant c ∈R. The value of the constant can be easily identiﬁed by
looking at the leading-order term. Thus, we have
(1 −x2)[e′(x)]2 = (n + 1)2(E2 −e(x)2).
The previous diﬀerential equation can be solved on each interval for which e′
does not change sign. If we assume that e′(x) ≥0 for x ∈[−1, x1], then
e′(x)
p
E2 −e2(x)
=
n + 1
√
1 −x2 .
This is a separable equation with solution
arccos
 e
E

= (n + 1) arccos(x) + c,
which implies that
e(x) = E cos[(n + 1)θ + c],
where x = cos θ and c is an arbitrary constant. Now, since we assumed that
e′(−1) ≥0, we must have e(−1) = −E and, consequently, c = mπ for m ∈Z.
Therefore,
e(x) = ±E cos[(n + 1)θ].
Example 10.3 motivates the following deﬁnition.
Deﬁnition 10.13 (Chebyshev polynomial4). Let x ∈[−1, 1] and x = cos θ. The
polynomial
Tk(x) = cos(kθ),
k ∈N0
(10.12)
is called the Chebyshev polynomial of degree k.
From Deﬁnition 10.13, it is not immediately clear that the functions Tk are indeed
polynomials. This can be proved by showing that the Chebyshev polynomials satisfy
the following three-term recurrence relation.
4 Named in honor of the Russian mathematician Pafnuty Lvovich Chebyshev (1821–1894).

280
Minimax Polynomial Approximation
n
Tn(x)
0
1
1
x
2
2x2 −1
3
4x3 −3x
4
8x4 −8x2 + 1
5
16x5 −20x3 + 5x
6
32x6 −48x4 + 18x2 −1
7
64x7 −112x5 + 56x3 −7x
8
128x8 −256x6 + 160x4 −32x2 + 1
9
256x9 −576x7 + 432x5 −120x3 + 9x
10
512x10 −1 280x8 + 1 120x6 −400x4 + 50x2 −1
Table 10.1 The ﬁrst 11 Chebyshev polynomials.
Proposition 10.14 (recurrence). The functions Tk, k ∈N0, deﬁned as in (10.12),
satisfy the three-term recurrence relation
Tn+1(x) = 2xTn(x) −Tn−1(x),
n = 1, 2, . . . ,
with T0(x) = 1 and T1(x) = x for all x ∈[−1, 1]. Thus, Tk ∈Pk, k ∈N0.
Furthermore, Tk(1) = 1 and Tk(−1) = (−1)k.
Proof. Consider the trigonometric formula
cos ((n ± 1)θ) = cos(nθ) cos(θ) ∓sin(nθ) sin(θ).
Then
Tn+1(x) + Tn−1(x) = cos((n + 1)θ) + cos((n −1)θ)
= 2 cos(nθ) cos(θ)
= 2Tn(x)x,
which proves the ﬁrst result. The rest of the details are left to the reader as an
exercise; see Problem 10.4.
The ﬁrst 11 Chebyshev polynomials are shown in Table 10.1. Let us now explore
some further properties of these polynomials.
Proposition 10.15 (orthogonality). Let w(x) =
1
√
1−x2 for x ∈(−1, 1). The
Chebyshev polynomials Tk ∈Pk, k ∈N0, deﬁned in (10.12) are L2
w-orthogonal,
i.e.,
(Tk, Tm)L2w (−1,1) =
Z 1
−1
Tk(x)Tm(x)
dx
√
1 −x2 =







0,
k ̸= m,
π,
k = m = 0,
π
2 ,
k = m > 0.

10.3 Chebyshev Polynomials
281
Proof. Let us make the change of variables θ = cos(x). Then
Z 1
−1
Tk(x)Tm(x)
dx
√
1 −x2 =
Z π
0
cos(kθ) cos(mθ)dθ.
The rest of the details are left to the reader as an exercise; see Problem 10.5.
We will discuss orthogonal polynomials more in the next chapter. In the
meantime, we can also prove the following.
Proposition 10.16 (diﬀerential equation). The Chebyshev polynomials Tk ∈Pk,
k ∈N0, deﬁned in (10.12), are solutions of the diﬀerential equation
 1 −x2
y ′′ −xy ′ + k2y = 0.
Proof. See Problem 10.6.
Theorem 10.17 (further properties). The Chebyshev polynomials have the follow-
ing further properties.
1. For n ∈N, deg(Tn) = n with leading coeﬃcient 2n−1.
2. Tn is even when n ∈N0 is even, and odd otherwise.
3. For n ∈N, the zeros of the polynomial Tn are simple and given by the formula
xj = cos
(2j −1)π
2n

,
j = 1, . . . , n.
In particular, the zeros are real, distinct, and lie in the interval (−1, 1).
4. |Tn(x)| ≤1, for all x ∈[−1, 1], and n ∈N0.
5. For n ∈N, the extreme values of Tn satisfy
Tn(xk) = (−1)n+k,
xk = cos
kπ
n

,
k = 0, . . . , n.
Proof. See Problem 10.7.
Let us now give a full proof of the fact that our motivating calculations of
Example 10.3 were pointing us toward.
Theorem 10.18 (minimax). Suppose that n ∈N0 and f (x) = xn+1 on the interval
[−1, 1]. Then the polynomial
p(x) = xn+1 −2−nTn+1(x) ∈Pn
is the minimax polynomial for f on [−1, 1].
Proof. By Theorem 10.17, item 1, we are justiﬁed in our claim that p ∈Pn. Since
f (x) −p(x) = 2−nTn+1(x),
by Theorem 10.17, items 4 and 5, f −p achieves its maximum value, 2−n, in
[−1, 1], with alternating signs at the n + 2 critical points xk = cos
  kπ
n+1

, k = 0,
1, . . . , n + 1. By the Chebyshev Oscillation Theorem 10.9, p ∈Pn is the unique
minimax polynomial for f (x) = xn+1 in the interval [−1, 1].

282
Minimax Polynomial Approximation
Theorem 10.19 (least deviation). Suppose that n ∈N0. Among all polynomials
in Pn+1 whose leading coeﬃcient is equal to one, the polynomial p(x) = 2−nTn+1
is the one with the smallest L∞(−1, 1) norm, i.e., it is the one that has the least
deviation from zero.
Proof. We can write any polynomial e ∈Pn+1, whose leading coeﬃcient is equal to
one, as the diﬀerence between f (x) = xn+1 and a polynomial q ∈Pn. By Theorem
10.18,
min
q∈Pn ∥f −q∥L∞(−1,1) =
2−nTn+1

L∞(−1,1) .
Therefore, among all polynomials in Pn+1 whose leading coeﬃcient equals one, the
polynomial p(x) = 2−nTn+1 has the smallest deviation from zero, and the deviation
is precisely 2−n.
10.4
Interpolation at Chebyshev Nodes
As motivation for the construction of this section, let us recall the result of Theorem
9.16. If n ∈N0, X = {xi}n
i=0 is a nodal set in the compact interval [a, b] ⊂R, f ∈
Cn+1([a, b]), and p ∈Pn is the Lagrange interpolating polynomial of f subordinate
to X. then, for every x ∈[a, b]\X, there is a point ξ ∈(a, b) such that
f (x) −p(x) = f (n+1)(ξ)
(n + 1)! ωn+1(x),
where
ωn+1(x) =
n
Y
i=0
(x −xi).
Clearly, ωn+1 ∈Pn+1 and has leading coeﬃcient equal to one. We saw in Example
9.4 that a major problem with uniform interpolation is that ∥ωn+1∥L∞(a,b) may get
too large. One way to control this is to adjust the values of the nodes using facts
we have just learned about Chebyshev polynomials.
Deﬁnition 10.20 (Chebyshev nodes). Let n ∈N0 be given. Suppose that [a, b] ⊂R
is a compact interval. We call the nodal set X = {ξj}n
j=0 the Chebyshev nodal
points in [a, b] of order n if and only if
ξj = 1
2(b −a) cos
 
(j + 1
2)π
n + 1
!
+ 1
2(b + a),
j = 0, 1, 2, . . . , n.
(10.13)
When a = −1, b = 1, these are just the n+1 distinct zeros of the Chebyshev poly-
nomial Tn+1. The interpolating polynomial that results from using the Chebyshev
nodal points is called the Chebyshev–Lagrange interpolant or just the Chebyshev
interpolant.
This next result should be compared with Proposition 9.18, where uniformly
spaced interpolation nodes are utilized. Note that a much better convergence rate
is achieved.

10.4 Interpolation at Chebyshev Nodes
283
Theorem 10.21 (interpolation error). Let n ∈N0 and f ∈Cn+1([a, b]). Suppose
that p ∈Pn is the unique Chebyshev–Lagrange interpolating polynomial of f . In
other words, p is the Lagrange interpolant subject to the Chebyshev nodal points
X = {ξj}n
j=0, which were deﬁned in (10.13). Then
∥f −p∥L∞(a,b) ≤
(b −a)n+1
22n+1(n + 1)!
f (n+1)
L∞(a,b) .
Proof. By
ζj = cos
 
(j + 1
2)π
n + 1
!
,
j = 0, . . . , n,
we denote the n + 1 distinct roots of Tn+1 in the interval (−1, 1). Thus,
n
Y
j=0
(t −ζj) = 2−nTn+1(t).
Deﬁne the aﬃne function
x(t) = 1
2(b −a)t + 1
2(b + a).
Then x([−1, 1]) = [a, b] with x(−1) = a and x(0) = 1
2(b + a), and x(1) = b.
Furthermore,
x(ζj) = ξj,
j = 0, . . . , n.
As with any aﬃne map, x is invertible, and the inverse is precisely
t(x) = 2x −a −b
b −a
.
Therefore,
n
Y
j=0
(x −ξj) =
b −a
2
n+1
2−nTn+1(t(x)).
Since |Tn+1(t(x))| ≤1 for all x ∈[a, b], the proof now follows from Theorem 9.16
and the fact that
|ωn+1(x)| =

n
Y
j=0
(x −ξj)

≤(b −a)n+1
22n+1
.
Example 10.4
(Runge phenomenon re-revisited) Let us now revisit (again) the
Runge’s function from Examples 9.4 and 9.5. See Figure 10.3, where, instead
of uniformly spaced nodes, we use Chebyshev interpolation points. Note that the

284
Minimax Polynomial Approximation
−1
−0.5
0
0.5
1.0
−1.0
−0.5
0
0.5
1.0
f (x)
p4(x)
interpolation points
p8(x)
interpolation points
p16(x)
interpolation points
Figure 10.3 The Runge Phenomenon revisited. Using Chebyshev interpolation nodes,
interpolation polynomials can behave better, i.e., oscillate less, than when uniformly
spaced nodes are used. See Figure 9.3 for comparison.
interpolation polynomials oscillate less wildly at the tails, as n gets larger. The
reason for this is explained in the result of Theorem 10.21 and in Figure 10.4.
Example 10.5
In Figure 10.5, we plot the Lagrange nodal bases for both
uniformly spaced (a) and Chebyshev (b) nodes, for degrees n = 4, 8, 16. Observe
that, for the uniformly spaced nodes, the absolute maxima are exploding in value,
as n gets large, especially at the tails. By contrast, the basis functions for the
Chebyshev nodes remain relatively well behaved, even as n becomes large. The
Lebesgue functions, Pn
i=0 |Li(x)|, for uniformly spaced and Chebyshev nodes, for
n = 4, 8, 16, are plotted in Figure 10.6. Not surprisingly, for the uniformly spaced
nodes, these functions are becoming very large at the boundaries, as n increases.
Example 10.6
Recall that the Lebesgue constant is computed via
Λ(X) = max
a≤x≤b
n
X
i=0
|Li(x)| .
One can prove that the Lebesgue constant for the Chebyshev nodes has the
asymptotic behavior

10.4 Interpolation at Chebyshev Nodes
285
−1.0
−0.5
0
0.5
1.0
−0.02
−0.01
0
0.01
0.02
ω9(x) (Chebyshev)
ω9(x) (Uniform)
2−8
−2−8
Figure 10.4 Plots of ωn+1, for n = 8, using Chebyshev and uniformly spaced interpolation
nodes. Observe that, as expected, the deviation from zero in ω9(x) over [−1, 1], using
Chebyshev points, remains bounded in absolute value by 2−8. This deviation grows for
ω9 when uniformly spaced nodes are used.
Λ(Xn,Cheb) ∼2
π ln n,
n →∞.
The asymptotic behavior for uniformly spaced nodes is
Λ(Xn,Unif) ∼
2n+1
exp(1)n ln n,
n →∞.
These rates seem to be consistent with what we have observed in Figure 10.6.
While it is beyond the scope of our text to prove these relations [83, 95], it is clear
that, as n →∞, the slow logarithmic growth of the Lebesgue constant for Cheby-
shev nodes, Xn,Cheb, is dominated by the exponential growth of the constant for
the uniform nodal set, Xn,Unif. This gives another explanation for why interpolation
with Chebyshev nodes is generally much better than that using uniformly spaced
nodes.

286
Minimax Polynomial Approximation
(a)
(b)
0
0.2
0.4
0.6
0.8
1
−0.5
0
0.5
1
Lagrange Basis Functions of Degree 4: Uniform Nodes
x
0
0.2
0.4
0.6
0.8
1
0
0.5
1
x
Lagrange Basis Functions of Degree 4: Chebyshev Nodes
0
0.2
0.4
0.6
0.8
1
−2
−1
0
1
2
x
Lagrange Basis Functions of Degree 8: Uniform Nodes
0
0.2
0.4
0.6
0.8
1
0
0.5
1
Lagrange Basis Functions of Degree 8: Chebyshev Nodes
x
0
0.2
0.4
0.6
0.8
1
−150
−100
−50
0
50
100
150
Lagrange Basis Functions of Degree 16: Uniform Nodes
x
0
0.2
0.4
0.6
0.8
1
0
0.5
1
x
Lagrange Basis Functions of Degree 16: Chebyshev Nodes
Figure 10.5 Lagrange nodal basis functions for uniformly spaced (a) and Chebyshev (b)
nodes in [0, 1]. Observe that, for the uniformly spaced nodes, the absolute maxima are
exploding in value, as n gets large, especially at the tails. By contrast, the basis functions
for the Chebyshev nodes remain relatively well behaved, even as n becomes large.
10.5
Bernstein Polynomials and the Weierstrass
Approximation Theorem
In this section, we give a constructive proof of the Weierstrass Approximation
Theorem using so-called Bernstein polynomials. In the section after this one, we
will use the Weierstrass Approximation Theorem to give a proof of convergence of
the least squares approximation to a function in the weighted quadratic mean.
Before we introduce Bernstein polynomials, we need some deﬁnitions.

10.5 Bernstein Polynomials and the Weierstrass Approximation Theorem
287
(a)
(b)
0
0.2
0.4
0.6
0.8
1
1
1.2
1.4
1.6
1.8
2
2.2
x
Lebesgue Function of Degree 4: Uniform Nodes
0
0.2
0.4
0.6
0.8
1
1
1.2
1.4
1.6
1.8
x
Lebesgue Function of Degree 4: Chebyshev Nodes
0
0.2
0.4
0.6
0.8
1
2
4
6
8
10
x
Lebesgue Function of Degree 8: Uniform Nodes
0
0.2
0.4
0.6
0.8
1
1
1.2
1.4
1.6
1.8
2
2.2
x
Lebesgue Function of Degree 8: Chebyshev Nodes
0
0.2
0.4
0.6
0.8
1
200
400
600
800
x
Lebesgue Function of Degree 16: Uniform Nodes
0
0.2
0.4
0.6
0.8
1
1
1.5
2
2.5
x
Lebesgue Function of Degree 16: Chebyshev Nodes
Figure 10.6 The Lebesgue function, λX(x), for uniformly spaced (a) and Chebyshev
(b) nodes in [0, 1].
Deﬁnition 10.22 (modulus of continuity). Suppose that I ⊆R is an interval and
f ∈C(I; C). The (uniform) modulus of continuity of f is deﬁned, for each
h ≥0, as
ωf (h) =
sup
x,y∈I
|x−y|≤h
|f (x) −f (y)|.
Example 10.7
Suppose that f (x) = x2 on the interval I = (0, 1). Then ωf (h) =
2h −h2. See Problem 10.14.

288
Minimax Polynomial Approximation
Example 10.8
If f (x) =
1
x and I = (0, 1), then ωf (h) = +∞. See Problem
10.15.
Lemma 10.23 (properties of ωf ). Suppose that [a, b] ⊂R is a compact interval
and f ∈C([a, b]; C) is continuous. The modulus of continuity has the following
properties.
1. ωf (0) = 0.
2. Monotonicity: If 0 < h1 < h2, then ωf (h1) ≤ωf (h2).
3. Subadditivity: ωf (h1 + h2) ≤ωf (h1) + ωf (h2).
4. For all n ∈N, ωf (nh) ≤nωf (h).
5.
lim
h↓0 ωf (h) = 0.
6. ωf ∈C([0, b −a]; [0, ∞)).
7. If f ∈C0,1([a, b]; C) with Lipschitz constant L > 0, i.e.,
|f (x) −f (y)| ≤L|x −y|,
∀x, y ∈[a, b],
then ωf (h) ≤Lh.
Proof. We give some details and leave some points to the reader as an exercise;
see Problem 10.13.
1: This one is obvious.
2: Suppose that 0 < h1 < h2. Deﬁne
Si = {|f (x) −f (y)| | x, y ∈[a, b], |x −y| ≤hi} ,
i = 1, 2.
If |x −y| ≤h1, then also |x −y| ≤h2. This implies that
S1 ⊆S2,
which implies that
ωf (h1) = sup S1 ≤sup S2 = ωf (h2).
3: Let h1 ≥0 and h2 ≥0 be given. Suppose that 0 ≤y −x ≤h1. Then,
|f (x) −f (y)| ≤ωf (h1) ≤ωf (h1) + ωf (h2).
(10.14)
Now, suppose that h1 < y −x ≤h1 + h2. Then
x + h1 < y,
0 < y −(x + h1) < h2.
Consequently,
|f (x) −f (y)| ≤|f (x) −f (x + h1)| + |f (x + h1) −f (y)|
≤ωf (h1) + ωf (y −(x + h1))
≤ωf (h1) + ωf (h2),
(10.15)

10.5 Bernstein Polynomials and the Weierstrass Approximation Theorem
289
where we have used the result of property 2 in the last estimate. If we swap the roles
of x and y, then inequalities (10.14) and (10.15) still hold. Thus, if |x−y| ≤h1+h2,
we have
|f (x) −f (y)| ≤ωf (h1) + ωf (h2).
Taking the supremum over all x, y ∈I such that |x −y| ≤h1 + h2, we get
ωf (h1 + h2) =
sup
x,y∈I
|x−y|≤h1+h2
|f (x) −f (y)| ≤ωf (h1) + ωf (h2).
4: From property 3,
ωf (2h) = ωf (h + h) ≤ωf (h) + ωf (h) = 2ωf (h).
Property 4 then follows by a simple induction argument.
5: Since f is continuous on the compact interval [a, b], f is uniformly continuous
on [a, b]; see ]Theorem B.20. Thus, given any ε > 0, there is a δ = δ(ε) > 0 such
that if x and y are any points in [a, b] satisfying |x −y| < δ, then it follows that
|f (x) −f (y)| < ε.
Hence,
ωf (δ) ≤ε.
(10.16)
We know from 2 that ωf (h) is an increasing function of h. Suppose that hn ↓0,
as n →∞. Then
0 ≤ωf (hn+1) ≤ωf (hn) ≤ωf (hn−1) ≤· · · ≤ωf (h2) ≤ωf (h1).
By the Monotone Convergence Theorem B.7, the sequence ωf (hn) has a limit, say,
α and
ωf (hn) ↓α ≥0.
Suppose that α > 0. Then by the argument above, namely (10.16), there is a
δ > 0 such that
ωf (δ) ≤α
2 .
Since hn ↓0, there is some N ∈N such that hN < δ. Hence,
ωf (hN) ≤ω(δ) ≤α
2 < α.
This is a contradiction.
6: From property 2, for any h ≥0 and hn > 0,
ωf (h) ≤ωf (h + hn).
Thus,
0 ≤ωf (h + hn) −ωf (h) ≤ωf (h) + ωf (hn) −ωf (h) = ωf (hn),

290
Minimax Polynomial Approximation
using property 3 in the last estimate. Now suppose that hn ↓0. Then ωf (hn) ↓0,
from property 5. Hence, by the last estimate,
ωf (h + hn) ↓ωf (h),
as n →∞.
This holds for any h ≥0.
Next, suppose that h > 0. Assume that h ≥hn ↓0, as n →∞. By similar
arguments,
ωf (h −hf ) ↑ωf (h),
as n →∞.
7: This is left to the reader as an exercise.
The deﬁnition of the Bernstein polynomials is simpliﬁed if we restrict our focus
to the interval [0, 1]. The general deﬁnition can be obtained by a translation and
dilation of the interval.
Deﬁnition 10.24 (Bernstein polynomial5). Suppose that n ∈N0. The Bernstein
basis polynomials, with respect to the interval [0, 1], are the n + 1 polynomials
βn,j(x) =
n
j

xj(1 −x)n−j ∈Pn,
0 ≤j ≤n,
(10.17)
where
 n
j

is the binomial coeﬃcient
n
j

=
n!
j!(n −j)!.
We have the following simple properties of these polynomials.
Proposition 10.25 (properties of βn,j). The Bernstein basis polynomials form a
basis for Pn and a partition of unity over R, i.e.,
n
X
j=0
βn,j(x) = 1,
∀x ∈R.
(10.18)
Each basis polynomial is nonnegative on the interval [0, 1]. Furthermore,
n
X
j=0
j
nβn,j(x) = x,
∀x ∈R,
(10.19)
and
n
X
j=0
j2
n2 βn,j(x) =

1 −1
n

x2 + 1
nx,
∀x ∈R.
(10.20)
5 Named in honor of the Russian and Soviet mathematician Sergei Natanovich Bernstein
(1880–1968).

10.5 Bernstein Polynomials and the Weierstrass Approximation Theorem
291
Proof. The nonnegativity on [0, 1] is immediate from the deﬁnition. Recall the
binomial theorem
(a + b)n =
n
X
j=0
n
j

ajbn−j.
Setting a = x and b = 1 −x, we have the partition of unity (10.18). The proofs of
the other properties are left to the reader as an exercise; see Problem 10.20.
Deﬁnition 10.26 (Bernstein approximation). Suppose that f ∈C([0, 1]). The
Bernstein polynomial approximation of f is deﬁned by
Bn[f ](x) =
n
X
j=0
f
 j
n

βn,j(x).
(10.21)
Deﬁnition 10.27 (monotonicity). A linear operator A: C([a, b]) →C([a, b]) is
called monotone if and only if for any f , g ∈C([a, b]), if f ≤g, then A[f ] ≤A[g].
Proposition 10.28 (properties of Bn). Bn : C([0, 1]) →Pn is a linear, monotone
operator. Unlike the interpolation operator, it is not a projection operator. In other
words, there exists a polynomial p ∈Pn such that Bn[p] ̸= p.
Proof. The linearity is straightforward: if f , g ∈C([0, 1]), then
Bn[αf + βg] = αBn[f ] + βBn[g].
The reader can check the details. To show that Bn is monotone, since Bn is linear,
it suﬃces to show that if f ≥0 on [0, 1], then Bn[f ] ≥0 on [0, 1]. But this is clear
from the fact that each βn,j is nonnegative on [0, 1]. That Bn is not a projection
follows from Proposition 10.25.
Cheney [15] and others use the theory of monotone operators to prove the
Weierstrass Approximation Theorem using Bernstein operators. We will take a more
direct approach. This next theorem, due to Bernstein, will yield the Weierstrass
Approximation Theorem as a corollary.
Theorem 10.29 (Bernstein). Suppose that n ∈N0 and f ∈C([0, 1]) with modulus
of continuity ωf . Then
∥f −Bn[f ]∥L∞(0,1) ≤9
4ωf
 1
√n

.
Proof. Using (10.18), the error function can be written as
en[f ](x) = f (x) −Bn[f ](x) =
n
X
j=0
βn,j(x)

f (x) −f
 j
n

.
Using the nonnegativity of the Bernstein basis polynomials and the triangle
inequality, we get
|en[f ](x)| ≤
n
X
j=0
βn,j(x)
f (x) −f
 j
n
 .

292
Minimax Polynomial Approximation
Fix x ∈[0, 1] and let δ > 0 be arbitrary. If |x −j/n| ≤δ, then observe that
f (x) −f
 j
n
 ≤ωf (δ).
Deﬁne the sets
J1,n(x, δ) =

j ∈{0, . . . , n}

x −j
n
 ≤δ

,
J2,n(x, δ) = {0, . . . , n}\J1,n(x; δ) =

j ∈{0, . . . , n}

x −j
n
 > δ

.
Then, since βn,j ≥0 on [0, 1] and Pn
j=0 βn,j = 1,
|en[f ](x)| ≤
X
j∈J1,n
βn,j(x)
f (x) −f
 j
n
 +
X
j∈J2,n
βn,j(x)
f (x) −f
 j
n

≤
X
j∈J1,n
βn,j(x)ωf (δ) +
X
j∈J2,n
βn,j(x)
f (x) −f
 j
n

≤ωf (δ) +
X
j∈J2,n
βn,j(x)
f (x) −f
 j
n
 .
Now, for each j ∈J2,n, let p ∈N be the unique number with the property that
p < 1
δ
x −j
n
 ≤p + 1.
(10.22)
Deﬁne the sequence {yk}p+1
k=0 ⊂[0, 1] via
yk = x +
k
p + 1
 j
n −x

,
k = 0, . . . , p + 1.
Notice that
y0 = x,
yp+1 = j
n,
and yk is strictly increasing from x to j
n, if x < j
n, and strictly decreasing from x
to j
n, if x > j
n. Furthermore, by construction,
|yk+1 −yk| =
1
p + 1
x −j
n
 ≤δ.
Then, by the triangle inequality,
f (x) −f
 j
n
 ≤|f (x) −f (y1)| + · · · +
f (yp) −f
 j
n

≤(p + 1)ωf (δ).

10.5 Bernstein Polynomials and the Weierstrass Approximation Theorem
293
For each j ∈J2,n, using (10.22), we have
f (x) −f
 j
n
 < ωf (δ)

1 + 1
δ
x −j
n


< ωf (δ)
 
1 + 1
δ2

x −j
n
2!
,
the second inequality resulting from the fact that
x −j
n
 > δ. Therefore, using
(10.18)–(10.20), we have
X
j∈J2,n
βn,j(x)
f (x) −f
 j
n
 ≤
X
j∈J2,n
βn,j(x)ωf (δ)
 
1 + 1
δ2

x −j
n
2!
= ωf (δ)
X
j∈J2,n
βn,j(x)
 
1 + 1
δ2

x −j
n
2!
= ωf (δ)

1 + 1
δ2
x(1 −x)
n

≤ωf (δ)

1 +
1
4nδ2

.
We now have
|en[f ](x)| ≤ωf (δ) +
X
j∈J2,n
βn,j(x)
f (x) −f
 j
n

≤ωf (δ)

2 +
1
4nδ2

(10.23)
for any δ > 0 that we may choose. It follows, upon setting δ = n−1/2, that
|en[f ](x)| ≤9
4ωf

n−1/2
.
Since x ∈[0, 1] was chosen arbitrarily in [0, 1], we have the desired result.
Our Bernstein construction was for real-valued functions. But it is not hard to
see that we can generalize to complex-valued functions by treating the real and
imaginary parts separately. Thus, we have the following corollary.
Corollary 10.30 (Bernstein). Suppose that n ∈N0 and f ∈C([0, 1]; C). Set
g = ℜ(f ) and h = ℑ(f ). Deﬁne the Bernstein approximation of f via
Bn[f ] = Bn[g] + iBn[h] ∈Pn(C).
(10.24)
Then
∥f −Bn[f ]∥L∞(0,1;C) ≤9
4

ωg
 1
√n

+ ωh
 1
√n

.
Proof. See Problem 10.21.

294
Minimax Polynomial Approximation
Corollary 10.31 (Lipschitz function). Suppose that f
∈C0,1([0, 1]; C) with
Lipschitz constant L > 0, i.e.,
|f (x) −f (y)| ≤L|x −y|,
∀x, y ∈[0, 1].
Then, for any n ∈N0,
∥f −Bn[f ]∥L∞(0,1;C) ≤2L
r
2
n,
where Bn[f ] is as deﬁned in (10.24).
Proof. Set g = ℜ(f ) and h = ℑ(f ). Suppose that ωf and ωg are the moduli of
continuity for the real and imaginary parts of f , respectively. Since f is Lipschitz, it
follows that g and h are each Lipschitz continuous with the same Lipschitz
constants, L. From Lemma 10.23.7 it follows that
ωg(δ) ≤Lδ,
ωh(δ) ≤Lδ,
and from (10.23) we can obtain the estimate
|f (x) −Bn[f ](x)| ≤2Lδ

2 +
1
4nδ2

for any x ∈[0, 1] and all δ > 0. The choice δ =
1
2
√
2n yields the desired result.
The Weierstrass Approximation Theorem6 follows easily from Theorem 10.29;
it is essentially a corollary.
Theorem 10.32 (Weierstrass Approximation Theorem). Let [a, b] ⊂R be a
compact interval and f ∈C([a, b]; C). For every ε > 0, there exists an n ∈N
and a polynomial pn ∈Pn(C) such that
∥f −pn∥L∞(a,b;C) ≤ε.
In other words, there is a sequence of polynomial functions {pn}∞
n=0, with pn ∈
Pn(C), that converge uniformly to f as n →∞.
Proof. First, we need to make an aﬃne change of coordinates from [a, b] to [0, 1].
In particular, consider
G(x) = x −a
b −a,
x ∈[a, b],
whose inverse is, of course,
F(t) = (1 −t)a + tb,
t ∈[0, 1].
Use the aﬃne change of coordinates to map f to the interval [0, 1]; approximate
this composite function by the appropriate Bernstein polynomial; then map the
result back to the interval [a, b]. Finally, use the fact that ωq (h) →0 as h →0,
from Lemma 10.23, for any continuous function q deﬁned on a compact interval.
The details are left to the reader as an exercise; see Problem 10.22.
6 Named in honor of the German mathematician Karl Theodor Wilhelm Weierstrass
(1815–1897).

10.5 Bernstein Polynomials and the Weierstrass Approximation Theorem
295
One may wonder how sharp the conclusion of Theorem 10.29 is. The following
example, taken from [73], shows that using Bernstein polynomials, this is asymp-
totically sharp.
Example 10.9
Let f (x) = |x −1
2|. It is not diﬃcult to see that f ∈C0,1([0, 1])
with Lipschitz constant L = 1. Thus, owing to Corollary 10.31, we have
∥f −Bn[f ]∥L∞(0,1) ≤C
√n,
∀n ∈N
for some constant C > 0 that is independent of n.
Notice, however, that at the point x = 1
2 we have
Bn[f ](x) −f (x) = Bn[f ](x) =
1
2
n
n
X
j=0

j
n −1
2

n
j

.
Then, if n is even, we have
n
X
j=0

j
n −1
2

n
j

=
n/2
X
j=0
1
2 −j
n
 n
j

= 1
2
 n
n/2

.
Consequently,
Bn[f ]
1
2

−f
1
2
 =
1
2n+1
 n
n/2

> 1
2n−1/2,
where the ﬁnal estimate is a consequence of the well-known Stirling Formula7
√
2πmmme−m < m! <
√
2πmmme−m

1 + 1
4m

,
∀m ∈N.
In conclusion, we have shown that
∥f −Bn[f ]∥L∞(0,1) > 1
2n−1/2;
therefore, we cannot expect to do better than as Theorem 10.29 states, at least
using Bernstein polynomials.
Example 10.10
Although we do not discuss it here, Jackson’s Theorem8 shows
that, for every compact interval [a, b] ⊂R, any f ∈C([a, b]), and all n ∈N0,
inf
p∈Pn ∥f −p∥L∞(a,b) ≤6ωf
b −a
2n

.
Interestingly, the so-called Bernstein Theorem shows that this is asymptotically
sharp. The reader is referred to [73, 15] for details.
7 Named in honor of the British mathematician James Stirling (1692–1770).
8 Named in honor of the American mathematician Dunham Jackson (1888–1946).

296
Minimax Polynomial Approximation
10.5.1
Moduli of Smoothness
We conclude our discussion by an observation motivated by Problem 10.18. While
the modulus of continuity proved rather useful, it is of no help in characterizing the
smoothness of a function beyond Lipschitz continuity. To aid with this, moduli of
smoothness are introduced.
Deﬁnition 10.33 (moduli of smoothness). Let [a, b] ⊂R be a compact interval
and f ∈C([a, b]; C). For k ∈N and h ∈[0, b−a
k ], we deﬁne the modulus of
smoothness of order k to be
ω(k)
f
(h) = sup
∆k
t f (x)
  |t| ≤h, x, x + kt ∈[a, b]
	
,
where
∆k
t f (x) =
k
X
i=0
(−1)k+i
k
i

f (x + it).
Some of the basic properties of the modulus of smoothness are as follows.
Proposition 10.34 (properties of ω(k)
f
). Let [a, b] ⊂R be a compact interval
and f ∈C([a, b]; C). For k ∈N, the modulus of smoothness has the following
properties.
1. ω(1)
f
= ωf , i.e., the modulus of smoothness of order one coincides with the
modulus of continuity.
2. Monotonicity: If 0 < h1 < h2, then ω(k)
f
(h1) ≤ω(k)
f
(h2).
3. ω(k)
f
(h) ≤2ω(k−1)
f
(h).
4. For all n ∈N, ω(k)
f
(nh) ≤nkω(k)
f
(h).
5. If f ∈Ck([a, b]; C), then
ω(k)
f
(h) ≤hk f (k)
L∞(a,b;C) .
6. ω(k)
f
(h) ≡0 if and only if f ∈Pk−1.
7. If f is diﬀerentiable on [a, b] and f ′ is bounded, then ω(k)
f
(h) ≤hω(k−1)
f ′
(h).
Proof. Most of the proofs are similar to those for the modulus of continuity, and
so we leave them to the reader as an exercise; see Problem 10.19. We will only
prove the last one. For |t| ≤h, we have

Problems
297
|∆k
t f (x)| =
∆k−1
t
[f (x + t) −f (x)]

=
∆k−1
t
Z x+t
x
f ′(y)dy

=
∆k−1
t
Z t
0
f ′(x + y)dy

=

Z t
0
∆k−1
t
f ′(x + y)dy

≤
Z max{0,t}
min{0,t}
|∆k−1
t
f ′(x + y)|dy
≤
Z max{0,t}
min{0,t}
ω(k−1)
f ′
(h)dy
≤hω(k−1)
f ′
(h).
Taking supremum over |t| ≤h on this inequality, the claim follows.
With the help of moduli of smoothness of order larger than one, we can, for
instance, provide a ﬁner characterization of the error in Lagrange interpolation.
The proof of the following result is beyond the scope of our text, but for context it
should be compared with Theorems 9.16 and 9.45. The interested reader is referred
to, for instance, [84].
Theorem 10.35 (Sendov9). Let f ∈C([0, 1]). For every n ∈N, deﬁne the nodal
set X = {
i
n+1}n
i=1. Let p ∈Pn−1 be the Lagrange interpolating polynomial subject
to X. Then we have
∥f −p∥L∞(0,1) ≤6ω(n)
f

1
n + 1

.
Problems
10.1
Complete the proof of Theorem 10.2.
10.2
Prove Theorem 10.4.
10.3
Provide all the details for Example 10.2.
10.4
Complete the proof of Proposition 10.14.
10.5
Complete the proof of Proposition 10.15.
10.6
Prove Proposition 10.16.
10.7
Prove Theorem 10.17.
10.8
Construct the minimax polynomial p ∈P1 on the interval [−2, 1] for the
function f (x) = |x|.
10.9
Find the minimax polynomial p ∈Pn on the interval [−1, 1] for the function
f (x) = Pn+1
j=0 ajxj, where aj ∈R and an+1 ̸= 0.
9 Named after the Bulgarian mathematician, diplomat, and politician Blagovest Hristov Sendov
(1932–2020).

298
Minimax Polynomial Approximation
10.10
Let n ∈N be given. Show that, for any set of coeﬃcients aj ∈R, j =
0, . . . , n −1,
max
−1≤x≤1
xn + an−1xn−1 + · · · + a1x + a0
 ≥21−n.
10.11
Suppose that f ∈C2([a, b]) and f ′′(x) > 0 for all x ∈[a, b]. Suppose that
p ∈P1 is the minimax polynomial approximation of f and p(x) = a1x + a0. Prove
that
a1 = f (b) −f (a)
b −a
and
a0 = 1
2(f (a) + f (c)) −f (b) −f (a)
b −a
· a + c
2
,
where c ∈(a, b) is the unique solution to
f ′(c) = f (b) −f (a)
b −a
.
10.12
Suppose that, for a > 0, f ∈C([−a, a]) is even. Suppose that p ∈Pn is
the minimax polynomial approximation of f on [−a, a]. What can you say about
the parity of p? Is p an even function? What can one say about the critical points
in [−a, a]? How are they distributed? Answer the analogous questions in the case
that f is odd. Prove your assertions.
10.13
Fill in any missing details in the proof of Lemma 10.23.
10.14
Suppose that f (x) = x2 on the interval I = (0, 1). Show that ωf (h) =
2h −h2.
10.15
If f (x) = 1
x and I = (0, 1), show that ωf (h) = +∞.
10.16
Show that if α ∈(0, 1], and f ∈C0,α([0, 1]), then there is a constant
C > 0, for which
ωf (h) ≤Chα.
10.17
We say that a function f ∈LogLip([0, 1]) if there is a constant M > 0
such that
ωf (h) ≤Mh| log(h)|.
a)
Show that, for every α < 1, f ∈LogLip([0, 1]) implies that f ∈C0,α([0, 1]).
b)
Show that LogLip([0, 1]) ̸⊂C0,1([0, 1]).
Hint: Consider f (x) = x log x.
10.18
Show that if h−1ωf (h) →0 as h ↓0, then f is constant.
10.19
Complete the proof of Proposition 10.34.
10.20
Recall that the Bernstein basis polynomials, with respect to the interval
[0, 1], are the n + 1 polynomials
βn,j(x) =
n
j

xj(1 −x)n−j ∈Pn,
0 ≤j ≤n.
Prove the following facts; in doing so, prove Proposition 10.25.
a)
βn,j(0) = δ0,j and βn,j(1) = δn,j.
b)
βn,j(1 −x) = βn,n−j(x).
c)
β′
n,j(x) = n (βn−1,j−1(x) −βn−1,j(x)).
d)
For 0 < j ≤n, βn,j has a root at x = 0 of multiplicity j.

Problems
299
e)
For 0 ≤j < n, βn,j has a root at x = 1 of multiplicity n −j.
f)
R 1
0 βn,j(x)dx =
1
n+1 for all 0 ≤j ≤n.
g)
The set of Bernstein basis polynomials forms a basis for Pn.
h)
The Bernstein basis polynomials form a partition of unity over R, i.e.,
n
X
j=0
βn,j(x) = 1,
∀x ∈R.
i)
Each basis polynomial is nonnegative on the interval [0, 1].
j)
The polynomials satisfy the identities
n
X
j=0
j
nβn,j(x) = x,
∀x ∈R,
n
X
j=0
j −1
n −1 · j
nβn,j(x) = x2,
∀x ∈R,
n
X
j=0
j2
n2 βn,j(x) =

1 −1
n

x2 + 1
nx,
∀x ∈R.
Hint: Take the ﬁrst and second derivatives of (x + y)n, with respect to x,
assuming y is constant, and then set y = 1 −x.
10.21
Prove Corollary 10.30.
10.22
Complete the proof of Theorem 10.32.

11
Polynomial Least Squares
Approximation
In this chapter, we again want to approximate a function of interest with a
polynomial. This time we will seek a polynomial that minimizes the error in the
L2-norm. This problem, known as the the polynomial least squares problem, is
closely related to classical Fourier expansions. We saw in Chapter 10 that the
minimax approximations were rather complicated objects. Polynomial least squares
approximations, by contrast, are relatively easy to construct.
To motivate our work in this chapter, let us do some formal calculations. Suppose
that J is a set of indices, of ﬁnite or inﬁnite cardinality, and that Ψ = {ψj}j∈J is an
orthonormal set of functions, i.e., (ψi, ψj)⋆= δi,j, where ( · , · )⋆is an inner product
on an appropriate function space V . Assume that f ∈V can be expanded in the
functions of Ψ:
f =
X
j∈J
cjψj,
where {cj}j∈J is a sequence of scalars that we wish to determine. Using orthonor-
mality, we ﬁnd
(f , ψi)⋆=

X
j∈J
cjψj, ψi


⋆
=
X
j∈J
cj(ψj, ψi)⋆=
X
j∈J
cjδi,j = ci.
In other words,
f =
X
j∈J
(f , ψj)⋆ψj.
Now, if the set J is ﬁnite, the series above are ﬁnite, and there would be no
question about bringing the summation outside of the inner product. In fact, all
of our calculations would be perfectly valid. However, if J is inﬁnite, we need to
rigorously justify our work.
Do our formal calculations make any sense if J is inﬁnite? In what sense do the
series above converge? If J is inﬁnite, is Ψ = {ψj}j∈J some kind of basis for our
space of functions, V ? What properties should f possess for the calculations to
work? Can all functions in V be represented in our “basis” Ψ? And, in what way
are the previous calculations related to least squares approximation? In this and
the next chapter, we seek to answer these questions.

11.1 Least Squares Polynomial Approximations
301
11.1
Least Squares Polynomial Approximations
Let us begin by deﬁning what we mean by least squares polynomial approximation.
Deﬁnition 11.1 (least squares polynomial). Let n ∈N0, [a, b] ⊂R be a compact
interval, and w be a weight function on [a, b]. Given f ∈L2
w(a, b; C), we say that
the polynomial p ∈Pn(C) is a least squares polynomial approximation of f of
order n if and only if
∥f −p∥L2w (a,b;C) =
inf
q∈Pn(C) ∥f −q∥L2w (a,b;C) .
As usual, if such a p ∈Pn(C) exists, we write
p ∈argmin
q∈Pn
∥f −q∥L2w (a,b;C) ;
if it is unique, we write
p = argmin
q∈Pn(C)
∥f −q∥L2w (a,b;C) .
11.2
Orthogonal Polynomials
An essential concept related to least squares polynomial approximation is that of
an orthogonal polynomial.
Deﬁnition 11.2 (orthogonal polynomial). Let [a, b] ⊂R be a compact interval.
Suppose that w is a weight function on [a, b]. The set Ψ = {ψj}∞
j=0 is called an
orthogonal polynomial system on [a, b] with respect to the weight w if and only
if, for each j ∈N, we have ψj ∈Pj(C) with deg ψj = j and
(ψk, ψℓ)L2w (a,b;C) = δk,ℓ∥ψℓ∥2
L2w (a,b;C) ,
∀k, ℓ∈N0.
The set is called an orthonormal polynomial system if and only if in addition
∥ψℓ∥L2w (a,b;C) = 1 for every ℓ∈N0.
The following result shows that for every weight function there is an orthonormal
polynomial system.
Theorem 11.3 (existence). Let [a, b] ⊂R be a compact interval and w be a weight
function on [a, b]. There exists an orthonormal polynomial system on [a, b] with
respect to the weight w. Moreover, the polynomials in the system can be taken to
be real valued.
Proof. We know that {xn}∞
n=0 is a linearly independent set of continuous functions
on [a, b] such that, for all n ∈N0, xn ∈Pn(C) and deg xn = n. The existence
of the orthonormal system now follows from the Gram–Schmidt process, since
( · , · )L2w (a,b;C) is an inner product on C([a, b]; C). That the resulting polynomials
are real valued is due to starting with {xn}∞
n=0 and the details of the Gram–Schmidt
process.

302
Polynomial Least Squares Approximation
Example 11.1
The Chebyshev polynomial system deﬁned in Deﬁnition 10.13 (see
also Proposition 10.15 and Table 10.1) is an orthogonal polynomial system with
respect to the weight function w(x) =
1
√
1−x2 on the interval [−1, 1]. Note that
this polynomial system, as deﬁned, is orthogonal but not orthonormal.
11.3
Existence and Uniqueness of the Least
Squares Approximation
With the help of an orthonormal polynomial system, we can establish the existence
and uniqueness of a least squares polynomial approximation.
Theorem 11.4 (existence and uniqueness). Let [a, b] ⊂R be a compact interval
and n ∈N0. Suppose that w is a weight function on [a, b] and f ∈L2
w(a, b; C).
There exists a unique least squares polynomial approximation of order n of f on
[a, b]. In particular, if Ψ = {ψj}∞
j=0 is an orthonormal polynomial system with
respect to the weight function w on [a, b], then the unique least squares polynomial
is given by
p(x) =
n
X
j=0
(f , ψj)L2w (a,b;C)ψj(x) ∈Pn(C).
Proof. By construction, the system Ψn = {ψj}n
j=0 is a basis of Pn(C); conse-
quently, each q ∈Pn(C) can be expressed on this basis: there are unique constants
c0, . . . , cn ∈C such that
q(x) =
n
X
j=0
cjψj(x).
This allows us to deﬁne a function E : Cn+1 →[0, ∞) via
E(c0, . . . , cn) = ∥f −q∥2
L2w (a,b;C) .
In particular,
E(c0, . . . , cn) = (f −q, f −q)L2w (a,b;C)
= ∥f ∥2
L2w (a,b;C) −(f , q)L2w (a,b;C) −(q, f )L2w (a,b;C) + (q, q)L2w (a,b;C)
= ∥f ∥2
L2w (a,b;C) −2ℜ
 (f , q)L2w (a,b;C)

+
n
X
j=0
|cj|2
=
n
X
j=0
cj −(f , ψj)L2w (a,b;C)
2 + ∥f ∥2
L2w (a,b;C) −
n
X
j=0
(f , ψj)L2w (a,b;C)
2 .
Clearly, E is minimized if and only if
cj = c⋆
j = (f , ψj)L2w (a,b;C),
j = 0, . . . , n.

11.3 Existence and Uniqueness of the Least Squares Approximation
303
Thus,
p(x) =
n
X
j=0
c⋆
j ψj(x) ∈Pn(C)
is the unique minimizer, i.e., the unique least squares polynomial approximation
to f .
As a consequence of the previous result, we obtain the following corollary, which
is commonly known as Bessel’s inequality.
Corollary 11.5 (Bessel’s inequality1). Let [a, b] ⊂R be a compact interval, w be
a weight function on [a, b], and f ∈L2
w(a, b; C). If Ψ = {ψj}∞
j=0 is an orthonormal
polynomial system with respect to the weight function w on [a, b], then Bessel’s
inequality,
n
X
j=0
(f , ψj)L2w (a,b;C)
2 ≤∥f ∥2
L2w (a,b;C) ,
(11.1)
holds for any n ∈N0. As a consequence, since ∥f ∥2
L2w (a,b;C) < ∞, the series
∞
X
j=0
(f , ψj)L2w (a,b;C)
2
converges absolutely.
Proof. In the proof of Theorem 11.4 we learned that if p is the least squares
polynomial of order n ∈N0, then
0 ≤∥f −p∥2
L2w (a,b;C) = ∥f ∥2
L2w (a,b;C) −
n
X
j=0
(f , ψj)L2w (a,b;C)
2 .
(11.2)
The result easily follows from this equation.
Least squares polynomial approximations can be characterized by a fundamental
orthogonality relation.
Theorem 11.6 (characterization). Let [a, b] ⊂R be a compact interval, w be a
weight function on [a, b], and f ∈L2
w(a, b; C). The polynomial p ∈Pn(C) is the
least squares approximation to f of order n ∈N0 if and only if
(f −p, q)L2w (a,b;C) = 0,
∀q ∈Pn(C).
(11.3)
1 Named in honor of the German astronomer, mathematician, physicist, and geodesist Friedrich
Wilhelm Bessel (1784–1846).

304
Polynomial Least Squares Approximation
Proof. ( ⇐= ) Suppose that (11.3) holds for some p ∈Pn(C). We want to show
that this p is the least squares approximation to f . To this end, since p−q ∈Pn(C),
for all q ∈Pn(C), we ﬁnd
∥f −p∥2
L2w (a,b;C) = (f −p, f −p)L2w (a,b;C)
= (f −p, f −p)L2w (a,b;C) + (f −p, p −q)L2w (a,b;C)
= (f −p, f −q)L2w (a,b;C)
≤∥f −p∥L2w (a,b;C) ∥f −q∥L2w (a,b;C) ,
where the Cauchy–Schwarz inequality was used in the last step. Thus,
∥f −p∥L2w (a,b;C) ≤∥f −q∥L2w (a,b;C) ,
∀q ∈Pn(C),
which proves that p is the least squares approximation of f .
( =⇒) Now suppose that p ∈Pn(C) is the least squares approximation of f . Then
p(x) =
n
X
j=0
c⋆
j ψj(x) ∈Pn,
where Ψ = {ψj}∞
j=0 is an orthonormal polynomial system with respect to the weight
function w on [a, b] and
c⋆
j = (f , ψj)L2w (a,b;C),
j = 0, . . . , n.
This implies, using orthonormality, that if 0 ≤j ≤n, then
(f −p, ψj)L2w (a,b;C) = (f , ψj)L2w (a,b;C) −(p, ψj)L2w (a,b;C) = c⋆
j −c⋆
j = 0.
Finally, if q ∈Pn(C), there is the unique representation
q =
n
X
j=0
ajψj ∈Pn(C)
and
(f −p, q)L2w (a,b;C) =
n
X
j=0
aj (f −p, ψj)L2w (a,b;C) = 0.
Since q ∈Pn(C) is arbitrary, the result is proven.
Deﬁnition 11.7 (least squares projection). Let [a, b] ⊂R be a compact interval,
w be a weight function on [a, b], and Ψ = {ψj}∞
j=0 be an orthonormal polynomial
system with respect to w on [a, b]. For every n ∈N0, the mapping
Pw,n : L2
w(a, b; C) →Pn(C),
f 7→Pw,n[f ] =
n
X
j=0
(f , ψj)L2w (a,b;C)ψj
is called the polynomial least squares projection or, sometimes, the L2
w-
projection of degree n.
The following result shows that this mapping is indeed a projection.

11.4 Properties of Orthogonal Polynomials
305
Pn(C)
f ∈L2
w(a, b; C)
Pw,n[f ]
Figure 11.1 The fundamental orthogonality of the least squares projection.
Proposition 11.8 (projection). For every compact interval [a, b] ⊂R, weight
function w on [a, b], and n ∈N0, the polynomial least squares projection is a
linear projection operator from L2
w(a, b; C) onto Pn(C), i.e.,
Pw,n[αf + βg] = αPw,n[f ] + βPw,n[g],
∀f , g ∈L2
w(a, b; C),
∀α, β ∈C,
and
Pw,n[p] = p,
∀p ∈Pn(C).
Proof. See Problem 11.1.
Remark 11.9 (orthogonality). Theorem 11.6 guarantees that the error between f
and its projection Pw,n[f ] is orthogonal to the subspace Pn(C), i.e.,
(f −Pw,n[f ], q)L2w (a,b;C) = 0,
∀q ∈Pn(C).
(11.4)
Property (11.4) is sometimes referred to as the fundamental orthogonality of the
projection. For a visual representation, see Figure 11.1, which can be compared
with Figure 5.1. We will revisit this idea when we talk about Galerkin projections
in the chapter on ﬁnite element methods, Chapter 25.
11.4
Properties of Orthogonal Polynomials
In this section we give some general properties of orthogonal polynomials with
respect to a compact interval [a, b] ⊂R. The next result is really quite amazing.
Take a moment to ponder it.

306
Polynomial Least Squares Approximation
Theorem 11.10 (zeros). Let [a, b] ⊂R be a compact interval. Suppose that w is a
weight function on [a, b] and Ψ = {ψj}∞
j=0 is an orthogonal polynomial system with
respect to the weight function w on [a, b]. For all j ≥1, the polynomial ψj ∈Pj
has j real simple roots, all of which lie in the ﬁnite interval (a, b).
Proof. Recall that, by deﬁnition, w is continuous on (a, b); w is nonnegative; and
w is Riemann integrable on (a, b). Without loss of generality, let us assume that
the coeﬃcients of the polynomials are real. Fix j ≥1 and observe that, using
orthogonality,
(ψj, 1)L2w (a,b) =
Z b
a
w(x)ψj(x)dx = 0.
The integrand is continuous and not identically zero on (a, b). Moreover, since w
is nonnegative on (a, b), there must be at least one point ξ ∈(a, b) such that
ψj(ξ) = 0. In addition, ψj(x) must change sign at this point. In other words, there
must be δ > 0 such that
ψj(xL)ψj(xR) < 0,
∀xL ∈(ξ −δ, ξ),
∀xR ∈(ξ, ξ + δ).
Let us now suppose that there are exactly k ∈N such points where ψj changes
sign and denote them by {ξj}k
j=1. Deﬁne
sk(x) = (x −ξ1) · · · (x −ξk) ∈Pk.
The polynomial ψjsk does not change sign in the interval (a, b). Thus,
(ψj, sk)L2w (a,b) =
Z b
a
w(x)ψj(x)sk(x)dx ̸= 0.
Now, if k < j, this is a contradiction because ψj is orthogonal to every polynomial
of strictly lesser degree. Thus, it must be that k ≥j. But k cannot be strictly
greater than j either, since ψj cannot change signs more than j times. Thus, k = j
is the only option. Thus, all of the zeros lie in (a, b) and they are distinct.
A similar argument yields an equally fascinating fact.
Theorem 11.11 (zeros of least squares). Let [a, b] ⊂R be a compact interval and
n ∈N0. Suppose that w is a weight function on [a, b] and f ∈L2
w(a, b)\Pn. If the
polynomial p ∈Pn is the least squares approximation to f , then the function f −p
changes sign at no less than n + 1 distinct points in (a, b).
Proof. Using the fundamental orthogonality property (for the error f −p), we have
0 = (f −p, 1)L2w (a,b) =
Z b
a
w(x)(f (x) −p(x))dx = 0.
Once again, the integrand is not identically zero on (a, b); therefore, there are
distinct points ξi ∈(a, b), i = 1, . . . , k, such that f −p changes sign at these
points. Suppose that there are exactly k such points. Deﬁne, as before,
sk(x) = (x −ξ1) · · · (x −ξk) ∈Pk.

11.5 Convergence of Least Squares Approximations
307
We want to show that k ≥n + 1. To see this, note that the function (f −p)skw
does not change sign in (a, b); consequently,
(f −p, sk)L2w (a,b) =
Z b
a
w(x)(f (x) −p(x))sk(s)dx ̸= 0.
It is not possible that 0 ≤k ≤n, because of the fundamental orthogonality property
and the fact that sk ∈Pk. The only possibility is that k > n.
As a consquence of these results, it turns out that orthonormal polynomial
systems must satisfy certain recurrence relations.
Theorem 11.12 (recurrence). Let [a, b] ⊂R be a compact interval. Suppose that
w is a weight function on [a, b] and Ψ = {ψj}∞
j=0 is an orthonormal polynomial
system with respect to the weight function w on [a, b]. Then the polynomials satisfy
the following three-term recurrence relation:
ψn+1 = (Anx + Bn) ψn −Cnψn−1,
n = 1, 2, . . . ,
for some sequences of constants {An}n∈N, {Bn}n∈N, and {Cn}n∈N.
Proof. See Problem 11.2.
11.5
Convergence of Least Squares Approximations
In this section, we prove the convergence, in the weighted quadratic mean, of
the least squares polynomial to the continuous function, f , that it approximates.
We will make use of the Bernstein polynomial approximation of f established in
Chapter 10.
Theorem 11.13 (convergence). Suppose that Ψ = {ψj}∞
j=0 is an orthonormal
polynomial system with respect to the weight function w on the compact interval
[a, b] ⊂R. Assume that f ∈C([a, b]; C) and, for n ∈N0, pn ∈Pn(C) is the least
squares polynomial approximation of f . Then
lim
n→∞∥f −pn∥L2w (a,b;C) = 0.
Furthermore, Parseval’s relation,2
∞
X
j=0
(f , ψj)L2w (a,b;C)

2
=
Z b
a
|f (x)|2 w(x)dx,
holds and
lim
j→0 (f , ψj)L2w (a,b;C) = 0.
2 Named in honor of the French mathematician Marc-Antoine Parseval des Chˆenes
(1755–1836).

308
Polynomial Least Squares Approximation
Proof. By deﬁnition, for any rn ∈Pn(C),
∥f −pn∥2
L2w (a,b;C) ≤∥f −rn∥2
L2w (a,b;C) .
By the Weierstrass Approximation Theorem 10.32, there is a sequence {qn}∞
n=1,
qn ∈Pn(C) such that
lim
n→∞∥f −qn∥L∞(a,b;C) = 0.
Using (D.1) with p = 2, we have
∥f −pn∥L2w (a,b;C) ≤∥f −qn∥L2w (a,b;C) ≤∥f −qn∥L∞(a,b)
q
∥w∥L1(a,b) →0.
Thus, ∥f −pn∥L2w (a,b;C) →0, as n →∞. The ﬁrst part is proven.
Using (11.2), we observe that
∥f −pn∥2
L2w (a,b;C) = ∥f ∥2
L2w (a,b;C) −
n
X
j=0
(f , ψj)L2w (a,b;C)

2
or, equivalently,
n
X
j=0
(f , ψj)L2w (a,b;C)

2
= ∥f ∥2
L2w (a,b;C) −∥f −pn∥2
L2w (a,b;C) ≥0.
By Bessel’s inequality (11.1),
0 ≤∥f ∥2
L2w (a,b;C) −∥f −pn∥2
L2w (a,b;C) =
n
X
j=0
(f , ψj)L2w (a,b;C)

2
≤∥f ∥2
L2w (a,b;C) .
By the Squeeze Theorem B.6, since ∥f −pn∥L2w (a,b;C) →0,
n
X
j=0
(f , ψj)L2w (a,b;C)

2
→∥f ∥2
L2w (a,b;C)
and Parseval’s relation follows.
Theorem 11.13 can be generalized for functions f ∈L2
w(a, b; C). We ﬁrst need
a well-known result that can be found in a good book on integration theory; for
example, [7] or [77].
Theorem 11.14 (density). Let w be a weight function on the compact interval
[a, b] ⊂R and f ∈L2
w(a, b; C). For any ε > 0, there is a g ∈C([a, b]; C) such that
∥f −g∥L2w (a,b;C) < ε.
In other words, C([a, b]; C) is dense in L2
w(a, b; C) with respect to the norm
∥· ∥L2w (a,b;C).
Then we can prove the following result, without any diﬃculty.

11.5 Convergence of Least Squares Approximations
309
Theorem 11.15 (convergence). Suppose that Ψ = {ψj}∞
j=0 is an orthonormal
polynomial system with respect to the weight function w on the compact interval
[a, b] ⊂R. Assume that f ∈L2
w(a, b; C) and, for n ∈N0, pn ∈Pn(C) is the least
squares polynomial approximation of f . Then
lim
n→∞∥f −pn∥L2w (a,b;C) = 0.
Furthermore, Parseval’s relation,
∞
X
j=0
(f , ψj)L2w (a,b;C)

2
=
Z b
a
|f (x)|2 w(x)dx,
holds.
Proof. Let ε > 0 be arbitrary. By Theorem 11.14, there is g ∈C([a, b]; C) such
that
∥f −g∥L2w (a,b) < ε
2.
Since pn ∈Pn(C) is the best approximation of f , for any rn ∈Pn(C),
∥f −pn∥L2w (a,b;C) ≤∥f −rn∥L2w (a,b;C)
= ∥f −g + g −rn∥L2w (a,b;C)
≤∥f −g∥L2w (a,b;C) + ∥g −rn∥L2w (a,b;C)
< ε
2 + ∥g −rn∥L2w (a,b;C) .
Now use the Weierstrass Approximation Theorem 10.32. The remaining details are
left to the reader as an exercise; see Problem 11.3.
It is important to note what this result is saying, as well as what it is not. We
have, as n →∞,
pn =
n
X
j=0
(f , ψj)L2w (a,b;C) ψj −→f ,
in the sense of L2
w(a, b; C), i.e., convergence in the weighted quadratic mean. This
is a strange sort of convergence. With L2
w(a, b; C) convergence, it is possible that
there are points x ∈[a, b] such that pn(x) does not converge to f (x), as n →∞.
It does not imply convergence in the sense of L∞(a, b; C); in other words, uniform
convergence. That topic is addressed in Section 11.6.
For those who have studied Fourier series, this topic should be familiar. In fact,
we make the following deﬁnition.
Deﬁnition 11.16 (Fourier coeﬃcients). Suppose that Ψ
=
{ψj}∞
j=0 is an
orthonormal polynomial system with respect to the weight function w on the
compact interval [a, b] ⊂R and f ∈L2
w(a, b; C). The numbers
cj = (f , ψj)L2w (a,b;C) ,
j ∈N0

310
Polynomial Least Squares Approximation
n
Pn(x)
0
1
1
x
2
1
2
 3x2 −1

3
1
2
 5x3 −3x

4
1
8
 35x4 −30x2 + 3

5
1
8
 63x5 −70x3 + 15x

6
1
16
 231x6 −315x4 + 105x2 −5

7
1
16
 429x7 −693x5 + 315x3 −35x

8
1
128
 6 435x8 −12 012x6 + 6 930x4 −1 260x2 + 35

9
1
128
 12 155x9 −25 740x7 + 18 018x5 −4 620x3 + 315x

10
1
256
 46 189x10 −109 395x8 + 90 090x6 −30 030x4 + 3 465x2 −63

Table 11.1 The ﬁrst 11 Legendre polynomials.
are called the generalized Fourier coeﬃcients of f .3 The series
∞
X
j=0
cjψj
is called the generalized Fourier series. We say that this series converges in
L2
w(a, b; C) if and only if there is a function g ∈L2
w(a, b; C) such that the sequence
of partial sums,
pn =
n
X
j=0
cjψj ∈Pn,
converges to g in the L2
w(a, b; C) norm, i.e.,
∥pn −g∥L2w (a,b;C) →0.
In Chapter 12 we will discuss the classical Fourier series involving expansions of
trigonometric functions. Here, we will focus our attention on another polynomial
system.
Deﬁnition 11.17 (Legendre polynomials). Suppose that w ≡1. The orthogonal
system of polynomials on the interval [−1, 1], denoted {Pn}∞
n=0 and satisfying the
normalization
Z 1
−1
Pk(x)Pℓ(x)dx =
2
2k + 1δk,ℓ,
(11.5)
is called the Legendre polynomial system.4
The ﬁrst few members of this system are given in Table 11.1. Let us now present
some of the properties of these polynomials.
3 Named in honor of the French mathematician and physicist Jean-Baptiste Joseph Fourier
(1768–1830).
4 Named in honor of the French mathematician Adrien-Marie Legendre (1752–1833).

11.5 Convergence of Least Squares Approximations
311
Theorem 11.18 (properties of {Pn}). Suppose that {Pn}∞
n=0 is the Legendre
polynomial system given in Deﬁnition 11.17. Then the following properties hold.
1. Rodrigues formula:5 For n ∈N0,
Pn(x) =
1
2nn!
dn
dxn
 x2 −1
n .
2. Recurrence relation: For n ∈N,
(n + 1)Pn+1(x) = (2n + 1)xPn(x) −nPn−1(x).
3. Recurrence relation for derivatives: For n ∈N,
(2n + 1)Pn(x) = d
dx (Pn+1(x) −Pn−1(x)).
4. Pn(1) = 1 and Pn(−1) = (−1)n for n ∈N0.
5. Pn satisﬁes the diﬀerential equation
d
dx

(1 −x2)dPn
dx (x)

+ n(n + 1)Pn(x) = 0.
6. |Pn(x)| ≤1 for all x ∈[−1, 1] and n ∈N0.
Proof. See Problem 11.4.
Example 11.2
Suppose that
f (x) =
(
0,
−1 ≤x < 0,
1,
0 ≤x ≤1.
In this example, we will use the weight function w ≡1, so that the appropriate
polynomial system is the Legendre polynomial system. The function f is not
continuous, but f ∈L2
w(−1, 1). The least squares polynomial approximations of f
in this setting are called Fourier–Legendre expansions. It is easy to see that the
generalized Fourier coeﬃcients (i.e., the Fourier–Legendre coeﬃcients) satisfy
c0 = 1
2,
c2k = 0,
k ∈N,
c2k−1 ̸= 0,
k ∈N.
We plot the approximations of degrees n = 21, 31, and 41 in Figure 11.2. One can
prove that pn(0) does not converge to f (0) = 1, as n →∞. In other words, pn does
not converge uniformly to f , even though it must converge in the quadratic mean,
according to Theorem 11.15. The overshoot phenomenon near the discontinuity in
5 Named in honor of the French banker, mathematician, and social reformer Benjamin Olinde
Rodrigues (1795–1851).

312
Polynomial Least Squares Approximation
−1.0
−0.5
0
0.5
1.0
0
0.2
0.4
0.6
0.8
1.0
x
Fourier–Legendre Expansion of Degree 21
−1.0
−0.5
0
0.5
1.0
0
0.2
0.4
0.6
0.8
1.0
x
Fourier–Legendre Expansion of Degree 31
−1.0
−0.5
0
0.5
1.0
0
0.2
0.4
0.6
0.8
1.0
x
Fourier–Legendre Expansion of Degree 41
Figure 11.2 Fourier–Legendre expansions of the Heaviside function; see Example 11.2.

11.6 Uniform Convergence of Least Squares Approximations
313
f that prevents uniform convergence is known as the Gibbs phenomenon.6 We refer
the interested reader to [42, 93, 104] for an in-depth analysis of this phenomenon.
11.6
Uniform Convergence of Least Squares Approximations
In this section, we will prove that, for suitable functions f , if pn is its polynomial
least squares approximation, then we have
pn =
n
X
j=0
(f , ψj)L2w (a,b) ψj −→f ,
n →∞,
in the sense of L∞(a, b), i.e., uniform convergence, for two common weight
functions on the interval [−1, 1]. This is usually accomplished by adding some
further conditions, such as f ∈C2([a, b]; C), to the function of interest.
Let us begin with the simplest case, w ≡1, so that the orthonormal polynomial
system is related to the Legendre polynomial system.
We ﬁrst state and prove a useful, but simple, technical lemma that will be used
several times in this chapter and Chapter 12.
Lemma 11.19 (convergent series). Suppose that p ∈(1, ∞). Then there is a
constant C > 0, only depending on p, such that
∞
X
j=n+1
1
jp ≤
C
np−1 .
In particular, C =
1
p−1.
Proof. First we note that, since p > 1, the series
∞
X
j=1
1
jp
converges; therefore, the series in question converge by comparison. We further
observe that
∞
X
j=n+1
1
jp ≤
Z ∞
n
1
xp dx =
1
1 −p x1−p

∞
n
=
C
np−1 .
For Fourier–Legendre expansions, we have the following result.
6 Although originally discovered by the British mathematician Henry Wilbraham (1825–1883),
this is named in honor of the American scientist who made signiﬁcant theoretical
contributions to physics, chemistry, and mathematics, Josiah Willard Gibbs (1839–1903). For
a fascinating historical account behind the controversy surrounding this phenomenon, we
refer the interested reader to [42].

314
Polynomial Least Squares Approximation
Theorem 11.20 (uniform convergence). Suppose that Ψ = {ψj}∞
j=0 is an
orthonormal polynomial system with respect to the weight function w ≡1 on the
compact interval [−1, 1] ⊂R. Assume that f ∈C2([−1, 1]) with f (−1) = f (1) = 0
and f ′(−1) = f ′(1) = 0. Suppose that, for every n ∈N0, pn ∈Pn is the least
squares polynomial approximation of f on [−1, 1]. Then
lim
n→∞∥f −pn∥L∞(−1,1) = 0.
In other words, pn converges uniformly to f on [−1, 1]. More precisely, for n ≥1,
∥f −pn∥L∞(−1,1) ≤
√
6C1
√n ,
|cn| ≤C1
n2 ,
where cn = (f , ψn)L2(−1,1) is the nth generalized Fourier coeﬃcient of f for some
constant C1 > 0 that is independent of n.
Proof. In consideration of (11.5), the appropriate orthonormal system Ψ =
{ψj}∞
j=0 is obtained by setting
ψj =
r
2j + 1
2
Pj,
in which case we are guaranteed that (ψi, ψj)L2(−1,1) = δi,j. The least squares
polynomial approximation of degree n is
pn(x) =
n
X
j=0
cjψj(x),
where
cj = (f , ψj)L2(−1,1).
Now we deﬁne, for j ∈N0,
c(1)
j
= (f ′, ψj)L2(−1,1),
c(2)
j
= (f ′′, ψj)L2(−1,1).
Using integration by parts, and the fact that f (−1) = f (1) = 0, we ﬁnd
c(1)
j±1 = −(f , ψ′
j±1)L2(−1,1).
Using the derivative recurrence relation obtained in Theorem 11.18, it follows that,
for j ∈N,
1
√2j + 3ψ′
j+1(x) −
1
√2j −1ψ′
j−1(x) =
2j + 1
√2j + 1ψj(x).

11.6 Uniform Convergence of Least Squares Approximations
315
From this, we deduce that
−
1
√2j + 3c(1)
j+1 +
1
√2j −1c(1)
j−1 =
2j + 1
√2j + 1cj
for j ∈N. Rearranging terms, we get
jcj = −Ajc(1)
j+1 + Bjc(1)
j−1,
(11.6)
where, for j ∈N,
Aj =
j
2j + 1
s
2j + 1
2j + 3,
Bj =
j
2j + 1
s
2j + 1
2j −1.
Now applying Theorem 11.13 for f ′, which is a continuous function on [−1, 1], it
follows that c(1)
j
→0, as j →∞. Since Aj, Bj →1
2, it must be that
lim
j→∞jcj = 0.
We can apply the same arguments above to the derivative function, f ′, concluding
that
jc(1)
j
= −Ajc(2)
j+1 + Bjc(2)
j−1
(11.7)
and, since c(2)
j
→0, as j →∞, that
lim
j→∞jc(1)
j
= 0.
But, appealing to (11.6), we see that
lim
j→∞j2cj = 0.
Since {j2cj}∞
j=0 converges to 0, it must be bounded. Thus, there is a constant
C1 > 0 independent of n such that, for all j ∈N,
|cj| ≤C1
j2 .

316
Polynomial Least Squares Approximation
Next, we observe that, if 1 ≤m < n,
∥pm −pn∥L∞(−1,1) =

n
X
j=m+1
cjψj

L∞(−1,1)
≤C1

n
X
j=m+1
ψj
j2

L∞(−1,1)
≤C1
n
X
j=m+1
1
j2 ∥ψj∥L∞(−1,1)
= C1
n
X
j=m+1
1
j2
r
2j + 1
2
∥Pj∥L∞(−1,1)
≤C1
n
X
j=m+1
1
j2
r
2j + 1
2
≤C1
∞
X
j=m+1
1
j2
r
2j + 1
2
≤
r
3
2C1
∞
X
j=m+1
1
j3/2
≤2
r
3
2
C1
√m,
where we have used the fact that ∥Pj∥L∞(−1,1) ≤1, from Theorem 11.18, and
Lemma 11.19 with p = 3
2. Thus, if 1 ≤m < n,
∥pm −pn∥L∞(−1,1) ≤
√
6C1
√m .
From this, we can conclude that {pn}∞
n=0 is uniformly Cauchy. From Theorems
B.61 and B.62, there is a continuous function g ∈C([−1, 1]) such that pn →g
uniformly on [−1, 1]. By Theorems B.65 and 11.13,
0 = lim
n→∞
Z 1
−1
|f (x) −pn(x)|2 dx =
Z 1
−1
|f (x) −g(x)|2 dx.
This implies that f ≡g. In other words, given any ε > 0, there is a number N ∈N
such that if n > N, then
∥f −pn∥L∞(−1,1) < ε.
Finally, if 1 ≤m < n and n > N, then
∥f −pm∥L∞(−1,1) ≤∥f −pn∥L∞(−1,1) + ∥pn −pm∥L∞(−1,1) < ε +
√
6C1
√m .

11.6 Uniform Convergence of Least Squares Approximations
317
Since ε can be made arbitrarily small,
∥f −pm∥L∞(−1,1) ≤
√
6C1
√m ,
provided that m ≥1.
Next, we remove the restrictions at the endpoints on f and its derivatives.
Corollary 11.21 (uniform convergence). Suppose that Ψ
=
{ψj}∞
j=0 is an
orthonormal polynomial system with respect to the weight function w ≡1 on
the compact interval [−1, 1] ⊂R. Assume that f ∈C2([−1, 1]) and pn ∈Pn,
n ∈N0, is the least squares polynomial approximation of f on [−1, 1]. Then
lim
n→∞∥f −pn∥L∞(−1,1) = 0.
More precisely, if n ≥3, then
∥f −pn∥L∞(−1,1) ≤
√
6C1
√n ,
|cn| ≤C1
n2 ,
where cn = (f , ψn)L2(−1,1) is the nth generalized Fourier coeﬃcient of f for some
constant C1 > 0 that is independent of n.
Proof. Let g ∈P3 be the Hermite interpolating polynomial satisfying g(±1) =
f (±1), g′(±1) = f ′(±1). Then f −g satisﬁes the hypotheses of Theorem 11.20.
Thus, if n ≥3, then
∥f −g −Pw,n[f −g]∥L∞(−1,1) ≤
√
6C1
√n .
But, since Pw,n is a linear projection and n ≥3,
Pw,n[f −g] = Pw,n[f ] −Pw,n[g] = Pw,n[f ] −g.
Thus,
∥f −Pw,n[f ]∥L∞(−1,1) = ∥f −g −Pw,n[f −g]∥L∞(−1,1) ≤
√
6C1
√n .
Regarding the coeﬃcients, if n ≥3,
(f −g, ψn)L2(−1,1) = (f , ψn)L2(−1,1)
and the result follows.
A similar result can be proven for Fourier–Chebyshev expansions, i.e., least
squares approximations involving Chebyshev polynomials. Interestingly, we are able
to prove a better rate of convergence for Fourier–Chebyshev expansions.
Theorem 11.22 (uniform convergence). Suppose that Ψ = {ψj}∞
j=0 is an orthonor-
mal polynomial system with respect to the weight function w(x) =
1
√
1−x2 on the
interval [−1, 1]. Assume that f ∈C2([−1, 1]) and pn ∈Pn, n ∈N0, is the least
squares polynomial approximation of f on [−1, 1]. Then
lim
n→∞∥f −pn∥L∞(−1,1) = 0.

318
Polynomial Least Squares Approximation
More precisely, if n > 1, then
∥f −pn∥L∞(−1,1) ≤C2
n ,
|cn| ≤C2
n2 ,
for some constant C2 > 0 that is independent of n.
Proof. Recall that the Chebyshev polynomials {Tj}∞
j=0 were introduced in Deﬁnition
10.13. They have the normalization
(Tk, Tm)L2w (−1,1) =
Z 1
−1
Tk(x)Tm(x)
dx
√
1 −x2 =







0,
k ̸= m,
π,
k = m = 0,
π
2 ,
k = m > 0,
where w(x) =
1
√
1−x2 , x ∈(−1, 1). Thus, we set ψ0 =
1
√πT0 and ψj =
q
2
πTj, j =
1, 2, . . . to obtain the appropriate orthonormal system. Therefore, pn = Pn
j=0 cjψj,
where, for j ∈N,
cj =
Z 1
−1
f (x)ψj(x)
dx
√
1 −x2 =
r
2
π
Z 1
−1
f (x)Tj(x)
dx
√
1 −x2 .
Making the change of variable, x = cos(θ), for 0 ≤θ ≤π, we ﬁnd
cj =
r
2
π
Z π
0
g(θ) cos(jθ)dθ,
where g(θ) = f (cos(θ)). Now, integrating by parts, we obtain, for j ∈N,
r
π
2 cj = g(θ)1
j sin(jθ)

π
0
−
Z π
0
g′(θ)1
j sin(jθ)dθ
= −
Z π
0
g′(θ)1
j sin(jθ)dθ
= −g′(θ) 1
j2 cos(jθ)

π
0
+
Z π
0
g′′(θ) 1
j2 cos(jθ)dθ
= −f ′(cos(θ)) sin(θ) 1
j2 cos(jθ)

π
0
+
Z π
0
g′′(θ) 1
j2 cos(jθ)dθ
=
Z π
0
g′′(θ) 1
j2 cos(jθ)dθ.
Since f ∈C2([−1, 1]), it follows that
|cj| ≤
r
π
2
1
j2
Z π
0
|g′′(θ)| | cos(jθ)| dθ ≤
r
π
2
1
j2
Z π
0
Mdθ =
r
π
2
Mπ
j2 ,
where
M = max
0≤θ≤π |g′′(θ)|.
Recall that, by Theorem 10.17, ∥Tj∥L∞(−1,1) ≤1. By the Weierstrass M-Test,
Theorem B.63, since

Problems
319
|cjψj| ≤
r
π
2
Mπ
j2 |ψj| =
r
π
2
Mπ
j2
r
2
π |Tj| ≤Mπ
j2
and P∞
j=0
1
j2 converges absolutely, it follows that the series
∞
X
j=1
cjψj
converges uniformly and absolutely to g ∈C([−1, 1]). In other words, pn →g
uniformly on [−1, 1]. To see that f ≡g on [−1, 1], observe that, by Theorems
B.65 and 11.13,
0 = lim
n→∞
Z 1
−1
|f (x) −pn(x)|2
dx
√
1 −x2 =
Z 1
−1
|f (x) −g(x)|2
dx
√
1 −x2 .
This implies that ∥f −g∥L2w (−1,1) = 0, which implies the ﬁrst part of the result.
We leave the details of the second part to the reader as an exercise; see Problem
11.5.
Problems
11.1
Prove Proposition 11.8.
11.2
Prove Theorem 11.12.
11.3
Prove Theorem 11.15.
11.4
Prove Theorem 11.18.
11.5
Complete the proof of Theorem 11.22.
11.6
Suppose that cj ∈C, j = 0, . . . , n are given. Find a polynomial q ∈Pn(C)
such that
Z b
a
xjq(x) = cj,
j = 0, . . . , n.
11.7
Suppose that Ψ = {ψj}m
j=0 is an orthonormal polynomial system with respect
to the weight function w on the compact interval [a, b] ⊂R and q ∈Pm satisﬁes
(q, ψj)L2w (a,b) = 0,
j = 0, . . . , m.
Prove that q ≡0.

12
Fourier Series
This chapter is a close companion to Chapter 11. Here, we will discuss least
squares best approximation by trigonometric polynomials. Many topics from our
discussion in the last chapter will reappear, but in a slightly diﬀerent form. The
big changes are that functions of interest in this chapter will be assumed periodic,
and the orthonormal systems will be trigonometric functions. The topics covered
here come under the classical subject of Fourier Analysis, which many students have
encountered in their PDE or Physics courses when studying separation of variables.
Now we want to make some details of that discussion rigorous. However, as
numerical analysts, we have a slightly diﬀerent perspective than, say, a physicist or a
PDE analyst. In particular, we want to rigorously quantify the error in approximation
of a function of interest by trigonometric functions, and this will be our ultimate
goal in the chapter.
The subject was founded by Jean-Baptiste Joseph Fourier, who discovered what
we would recognize as the basics of Fourier Analysis in his studies of heat ﬂow in
the 1820s. At its heart, the theory indicates that almost every signal is a linear
combination of Fourier modes, i.e., the sum of trigonometric functions. Take,
for example, the one-periodic square wave, f , as shown in Figure 12.1. We show
approximations of the form
Sn[f ](x) =
n
X
j=1
cj sin(2πxj)
using n = 15, 17, and 19, where the coeﬃcients, cj, are chosen according to
a rule that we will shortly describe. Notice that the square wave is qualitatively
well approximated by our sums of sine waves. But there is a glaring problem.
Near the discontinuities of the square wave, the approximations overshoot the
function by a considerable amount. Will this overshoot diminish as n becomes
larger? Of course, one would hope that this is the case. How are these Fourier
approximations computed theoretically? How are they computed practically? When
can we expect that Fourier approximations converge uniformly, as n →∞? What
are the rates of convergence? Many of these and other questions will be answered
in this chapter.

12.1 Least Squares Trigonometric Approximations
321
0
0.5
1.0
1.5
2.0
−1.5
−1.0
−0.5
0
0.5
1.0
1.5
x
Sn[f ](x)
n = 15
n = 17
n = 19
Figure 12.1 Fourier approximations of a square wave.
12.1
Least Squares Trigonometric Approximations
First, we want to precisely deﬁne the objects we will use to approximate periodic
functions. The reader is referred to Appendix D for an overview of periodic functions
and spaces of periodic functions. To simplify the description of general one-periodic
signals, we use the family of complex exponential functions.
Deﬁnition 12.1 (trigonometric polynomial). Let n ∈N0. The function
pn(x) =
n
X
j=−n
cj exp(2πijx),
(12.1)
where cj ∈C, j = −n, . . . , n is called a trigonometric polynomial of degree at
most n. The set of all such polynomials is labeled Tn(0, 1), or just Tn when the
period is understood. Under the usual function operations, this is a vector space
over the ﬁeld of complex numbers.
Deﬁnition 12.2 (least squares approximation). Let f ∈L2
p(0, 1; C) and n ∈N0.
The trigonometric polynomial p ∈Tn, if it exists, is called a least squares
trigonometric polynomial approximation of f of order n if and only if
∥f −p∥L2p(0,1;C) = inf
q∈Tn ∥f −q∥L2p(0,1;C) .

322
Fourier Series
As usual, if such a p ∈Tn exists, we write
p ∈argmin
q∈Tn
∥f −q∥L2p(0,1;C) ,
and, if it is unique, we write
p = argmin
q∈Tn
∥f −q∥L2p(0,1;C) .
Recall, from Euler’s formula,1 that, for any x ∈R,
exp(2πikx) = cos(2πkx) + i sin(2πkx),
which shows that the function
x 7→exp(2πikx)
belongs to C∞
p (0, 1; C).
Proposition 12.3 (orthonormality). Let n ∈N0. The set
Ψ = {ψj = exp(2πij · )}n
j=−n
is an orthonormal system with respect to the inner product ( · , · )L2(0,1;C), i.e.,
(ψk, ψℓ)L2(0,1;C) = δk,ℓ,
−n ≤k, ℓ≤n.
Proof. Evaluate
(ψk, ψℓ)L2(0,1;C) =
Z 1
0
exp(2πikx)exp(2πiℓx)dx.
The simple details are left to the reader as an exercise; see Problem 12.1
Using this orthonormal system, which we will call standard, it is rather easy to
establish the existence and uniqueness of least squares trigonometric polynomial
approximations.
Theorem 12.4 (existence and uniqueness). Let n ∈N0 and f ∈L2
p(0, 1; C).
There exists a unique least squares trigonometric polynomial approximation of f . If
Ψ = {ψj = exp(2πij · )}n
j=−n is the standard orthonormal trigonometric polynomial
system on [0, 1], then the least squares approximation is given by
p =
n
X
j=−n
ˆfjψj,
where ˆfj is the jth Fourier coeﬃcient of f , deﬁned as
ˆfj = (f , ψj)L2(0,1;C) =
Z 1
0
f (x) exp(−2πijx)dx.
(12.2)
1 Named in honor of the Swiss mathematician, physicist, astronomer, geographer, logician, and
engineer Leonhard Euler (1707–1783).

12.1 Least Squares Trigonometric Approximations
323
Furthermore, Bessel’s inequality,2
n
X
j=−n
ˆfj
2 ≤∥f ∥2
L2(0,1;C) ,
(12.3)
holds for any n ∈N0. Since ∥f ∥2
L2(0,1;C) < ∞, the series
∞
X
j=−∞
ˆfj
2 = lim
n→∞
n
X
j=−n
ˆfj
2
converges absolutely.
Proof. The existence and uniqueness proof is identical to the proof of Theorem
11.4. In going through the details, we learn that
0 ≤∥f −p∥2
L2(0,1;C) = ∥f ∥2
L2(0,1;C) −
n
X
j=−n
ˆfj
2 ,
(12.4)
where p is the least squares approximation. Bessel’s inequality follows easily from
this. We leave the rest of the details to the reader as an exercise; see Problem
12.2.
By following the techniques in Chapter 11, the following results can be easily
obtained.
Theorem 12.5 (characterization). Let f ∈L2
p(0, 1; C). The trigonometric polyno-
mial p ∈Tn is the least squares approximation to f if and only if
(f −p, q)L2(0,1;C) = 0,
∀q ∈Tn.
(12.5)
Proof. See Problem 12.3.
Deﬁnition 12.6 (Fourier projection). Let n ∈N0 and Ψ = {ψj = exp(2πij · )}n
j=−n
be the standard orthonormal trigonometric polynomial system on [0, 1]. The
mapping
Sn : L2
p(0, 1; C) →Tn,
f 7→Sn[f ] =
n
X
j=−n
ˆfjψj,
(12.6)
where ˆfj = (f , ψj)L2(0,1;C) is the jth Fourier coeﬃcient of f , is called the Fourier
projection of degree n.
Proposition 12.7 (projection). For every n ∈N0, the Fourier projection is a linear
projection operator from L2
p(0, 1; C) onto Tn, i.e.,
Sn[αf + βg] = αSn[f ] + βSn[g],
∀f , g ∈L2
p(0, 1; C),
∀α, β ∈C,
and
Sn[p] = p,
∀p ∈Tn.
2 Named in honor of the German astronomer, mathematician, physicist, and geodesist Friedrich
Wilhelm Bessel (1784–1846).

324
Fourier Series
−2
−1
0
1
2
0
2
4
6
8
x
P20(x)/
R 1
0 P20(x)dx
Figure 12.2 An approximation of the periodic Dirac delta function, the so-called Dirac
comb, via (12.8) with n = 20.
Proof. See Problem 12.4.
Remark 12.8 (orthogonality). Theorem 12.5 guarantees that the error between f
and its Fourier projection, Sn[f ], is orthogonal to the subspace Tn, i.e.,
(f −Sn[f ], q)L2(0,1;C) = 0,
∀q ∈Tn.
(12.7)
12.2
Density of Trigonometric Polynomials in the Space Cp(0, 1; C)
Next, we want to prove the density of the trigonometric polynomials in the space
Cp(0, 1; C) with respect to the norm ∥· ∥L∞(0,1;C). This result is a sort of periodic
analogue to the Weierstrass Approximation Theorem 10.32. To prove our result, we
need a trigonometric approximation of the Dirac delta function; in fact, a periodic
Dirac delta function, which is sometimes called a Dirac comb.
Lemma 12.9 (Dirac comb3). For each n ∈N0 and x ∈R, deﬁne
Qn(x) =
Pn(x)
R 1
0 Pn(x)dx
,
Pn(x) = (1 + cos(2πx))n.
(12.8)
3 Named in honor of the British theoretical physicist Paul Adrien Maurice Dirac (1902–1984).

12.2 Density of Trigonometric Polynomials in the Space Cp(0, 1; C)
325
Then Qn ∈Tn; Qn(x) ≥0 for all x ∈R;
R 1
0 Qn(x)dx = 1; and
lim
n→∞
Z 1−δ
δ
Qn(x)dx = 0
(12.9)
for every δ ∈(0, 1
2).
Proof. The proof is left to the reader as an exercise; see Problem 12.5. It may help
to plot Qn for a few values of n to get an idea of what one is trying to prove; see,
for example, Figure 12.2.
Deﬁnition 12.10 (periodic convolution). Let f , g ∈Cp(0, 1; C). The periodic
convolution [f ⋆g]p is deﬁned as the function
[f ⋆g]p(x) =
Z 1
0
f (x −y)g(y)dy,
∀x ∈R.
Lemma 12.11 (properties of convolution). Suppose that f , g ∈Cp(0, 1; C). Then
the periodic convolution [f ⋆g]p is well deﬁned, linear in both entries, commutative,
i.e.,
[f ⋆g]p(x) = [g ⋆f ]p(x),
∀x ∈R,
and an element of Cp(0, 1; C). Furthermore, if g ∈Tn for some n ∈N0, then
[f ⋆g]p ∈Tn.
Proof. Fix x ∈R. The function Cx(y) = f (x −y)g(y) is continuous and one-
periodic. Indeed,
Cx(y +m) = f (x −y −m)g(y +m) = f (x −y)g(y) = Cx(y),
∀y ∈R,
∀m ∈Z.
Therefore,
I(x) =
Z 1
0
Cx(y)dy =
Z a+1
a
Cx(y)dy,
∀a ∈R.
The function I is certainly well deﬁned. With the change of variable t = x −y for
integration, we have
[f ⋆g]p(x) = I(x) =
Z 1
0
f (x −y)g(y)dy
= −
Z x−1
x
f (t)g(x −t)dt
=
Z x
x−1
f (t)g(x −t)dt
=
Z 1
0
f (t)g(x −t)dt
= [g ⋆f ]p(x).
Thus, we have shown that periodic convolution is commutative. Linearity is a
straightforward property to show, and we skip the proof.

326
Fourier Series
Next, we want to show that I is periodic. Let ℓ∈Z be arbitrary. Then
I(x + ℓ) =
Z 1
0
Cx+ℓ(y)dy
=
Z 1
0
f (x + ℓ−y)g(y)dy
=
Z 1
0
f (x −y)g(y)dy
=
Z 1
0
Cx(y)dy
= I(x).
To see that I is continuous, consider, for any x1, x2 ∈R,
|I(x1) −I(x2)| =

Z 1
0
[f (x1 −y) −f (x2 −y)]g(y)dy

≤
Z 1
0
|f (x1 −y) −f (x2 −y)| · |g(y)| dy
≤max
0≤y≤1 |f (x1 −y) −f (x2 −y)|
Z 1
0
|g(y)| dy.
Let ε > 0 be given. Then, since f is continuous and one-periodic, it is uniformly
continuous on any compact subset of R. Therefore, there is a δ > 0 such that if
|x1 −x2| = |(x1 −y) −(x2 −y)| < δ,
then, uniformly with respect to y,
|f (x1 −y) −f (x2 −y)| < ε.
Hence,
|I(x1) −I(x2)| ≤ε
Z 1
0
|g(y)| dy,
which proves that I is uniformly continuous. Thus, I = [f ⋆g]p ∈Cp(0, 1; C).
Finally, suppose that g ∈Tn. By linearity, it suﬃces to prove our result with
g(x) = exp(2πikx) for some k ∈Z, |k| ≤n. Thus,
[f ⋆g]p(x) = [g ⋆f ]p(x) =
Z 1
0
g(x −y)f (y)dy
= exp(2πikx)
Z 1
0
exp(−2πiky)f (y)dy
= exp(2πikx)ˆfk,
which proves that [f ⋆g]p ∈Tn, whenever g ∈Tn.
We now come to the main result of the section, an analogue of the Weierstrass
Approximation Theorem 10.32.

12.2 Density of Trigonometric Polynomials in the Space Cp(0, 1; C)
327
Theorem 12.12 (density). The trigonometric polynomials of period one are dense
in Cp(0, 1; C) in the following sense: given any f ∈Cp(0, 1; C) and any ε > 0, there
is an n ∈N0 and a trigonometric polynomial fn ∈Tn such that
∥f −fn∥L∞(0,1;C) ≤ε.
Proof. For n ∈N0, deﬁne fn = [f ⋆Qn]p, where Qn is deﬁned in (12.8). From
Lemma 12.11, we know that fn ∈Tn. Fix x ∈R. Computing the diﬀerence f −fn,
we ﬁnd
|f (x) −fn(x)| =

Z
1
2
−1
2
(f (x) −f (x −y)) Qn(y)dy

≤
Z
1
2
−1
2
|f (x) −f (x −y)| Qn(y)dy
≤
Z
1
2
−1
2
|f (x) −f (x −y)| Qn(y)dy
= I1(x) + I2(x),
where
I1(x) =
Z δ
−δ
|f (x) −f (x −y)| Qn(y)dy,
I2(x) =
Z 1−δ
δ
|f (x) −f (x −y)| Qn(y)dy.
Let ε > 0 be given. Since f is uniformly continuous, we can choose δ > 0 small
enough, so that if −δ ≤y ≤δ, then
|f (x) −f (x −y)| ≤ε
2
uniformly with respect to x. Thus,
I1(x) =
Z δ
−δ
|f (x) −f (x −y)| Qn(y)dy ≤
Z δ
−δ
ε
2Qn(y)dy ≤ε
2.
We now consider δ > 0 to be ﬁxed. Turning to I2, let us assume, using (12.9),
that n is suﬃciently large, so that
Z 1−δ
δ
Qn(y)dy ≤
ε
4 ∥f ∥L∞(0,1;C)
.

328
Fourier Series
If this is the case,
I2(x) =
Z 1−δ
δ
|f (x) −f (x −y)| Qn(y)dy
≤2 ∥f ∥L∞(0,1;C)
Z 1−δ
δ
Qn(y)dy
≤2 ∥f ∥L∞(0,1;C)
ε
4 ∥f ∥L∞(0,1;C)
= ε
2.
Putting both of our estimates together, we ﬁnd, for every x ∈[0, 1],
|f (x) −fn(x)| ≤I1(x) + I2(x) ≤ε,
provided that n ∈N0 is suﬃciently large. Since ε > 0 is arbitrary and independent
of x, we have the desired result.
12.3
Convergence of Fourier Series in the Quadratic Mean
We now focus on the trigonometric analogue of Section 11.5. In other words, we
will show convergence of the least squares trigonometric polynomial.
Deﬁnition 12.13 (Fourier series). Suppose that f ∈L2
p(0, 1; C). Let
Ψ = {ψj = exp(2πij · )}∞
j=−∞
be the standard orthonormal trigonometric polynomial system on [0, 1] and ˆfj
denote the jth Fourier coeﬃcient of f , i.e., ˆfj = (f , ψj)L2(0,1;C). The Fourier series
of f is deﬁned as
∞
X
j=−∞
ˆfjψj = lim
n→∞Sn[f ],
where Sn is the Fourier projection deﬁned in (12.6). We say that Sn[f ] converges
to f in the quadratic mean if and only if
lim
n→∞∥f −Sn[f ]∥L2(0,1;C) = 0
and we write Sn[f ] →f in L2
p(0, 1; C).
Remark 12.14 (convergence). We are justiﬁed in writing
f =
∞
X
j=−∞
ˆfjψj
whenever the series on the right-hand side converges to f in the quadratic mean.
However, as we saw in Chapter 11, this may not mean the series converges to f
point-wise on [0, 1].

12.3 Convergence of Fourier Series in the Quadratic Mean
329
Theorem 12.15 (convergence). Suppose that f
∈Cp(0, 1; C). Then Sn[f ]
converges to f in the quadratic mean, i.e.,
lim
n→∞∥f −Sn[f ]∥L2(0,1;C) = 0.
Furthermore, Parseval’s relation,4
Z 1
0
|f (x)|2 dx =
∞
X
j=−∞
|ˆfj|2 = lim
n→∞
n
X
j=−n
|ˆfj|2,
holds.
Proof. For any n ∈N0, set fn = [f ⋆Qn]p ∈Tn. We know that Sn[f ] is the least
squares trigonometric polynomial approximation for f . Therefore,
∥f −Sn[f ]∥2
L2(0,1;C) ≤∥f −fn∥2
L2(0,1;C) .
Since fn →f uniformly,
lim
n→∞∥f −fn∥L∞(0,1;C) = 0.
But recall, from inequality (D.1) (with [a, b] = [0, 1], w ≡1, and p = 2), that
∥f −fn∥L2(0,1;C) ≤∥f −fn∥L∞(0,1;C) .
This implies the convergence result. For Parseval’s relation, we recall (12.4):
0 ≤∥f −Sn[f ]∥2
L2(0,1;C) = ∥f ∥2
L2(0,1;C) −
n
X
j=−n
ˆfj
2 .
A little manipulation gives the result.
The following result is a simple consequence of Theorem 11.14.
Theorem 12.16 (density). Suppose that f ∈L2
p(0, 1; C). For every ε > 0, there is
a function g ∈Cp(0, 1; C) such that
∥f −g∥L2(0,1;C) ≤ε.
In other words, continuous one-periodic functions are dense in L2
p(0, 1; C).
From this, we easily get the following classical result by following the proof of
Theorem 11.15.
Theorem 12.17 (Riesz–Fischer Theorem5). Suppose that f ∈L2
p(0, 1; C). Then
Sn[f ] converges to f in the quadratic mean, i.e.,
lim
n→∞∥f −Sn[f ]∥L2(0,1;C) = 0.
Conversely, let {aj}j∈Z ⊂C. If the series P∞
j=−∞|aj|2 converges, there is a function
f ∈L2
p(0, 1; C) such that aj = ˆfj for all j ∈Z.
4 Named in honor of the French mathematician Marc-Antoine Parseval des Chˆenes
(1755–1836).
5 Named in honor of the Hungarian mathematician Frigyes Riesz (1880–1956) and the Austrian
mathematician Ernst Sigismund Fischer (1875–1954).

330
Fourier Series
If f ∈L2
p(0, 1; C), Parseval’s relation,
Z 1
0
|f (x)|2 dx =
∞
X
j=−∞
|ˆfj|2 = lim
n→∞
n
X
j=−n
|ˆfj|2,
holds.
Proof. See Problem 12.6.
The previous result established a unique correspondence between one-periodic
square integrable functions and square summable sequences. Let us explore this in
more detail. For that, we introduce the following classical sequence space.
Deﬁnition 12.18 (ℓ2(Z; C)). We deﬁne
ℓ2(Z; C) =


a: Z →C

∞
X
j=−∞
|aj|2 < ∞


.
This is a vector space. Moreover, this is a complex Hilbert space with inner product
(a, b)ℓ2(Z;C) =
∞
X
j=−∞
ajbj
and norm
∥a∥ℓ2(Z;C) =
q
(a, a)ℓ2(Z;C).
Deﬁnition 12.19 (Fourier transform). For v ∈L2
p(0, 1; C) its ﬁnite Fourier
transform is deﬁned as
Fp[v] ∈ℓ2(Z; C)
with
Fp[v]j = ˆvj =
Z 1
0
f (x) exp(−2πijx)dx.
Thus, Fp : L2
p(0, 1; C)
→
ℓ2(Z; C). The inverse ﬁnite Fourier transform,
F−1
p
: ℓ2(Z; C) →L2
p(0, 1; C), is deﬁned via
F−1
p [c](x) =
∞
X
j=−∞
cj exp(2πijx).
Proposition 12.20 (isomorphism). The ﬁnite Fourier transform is an isometric
isomorphism between L2
p(0, 1; C) and ℓ2(Z; C).
Proof. This immediately follows from Theorem 12.17.
The ﬁnite Fourier transform is one of many transforms associated with the
surname Fourier. For comparisons, see Deﬁnition 13.8, Proposition 23.16, and
Deﬁnition 28.28.

12.4 Uniform Convergence of Fourier Series
331
12.4
Uniform Convergence of Fourier Series
When does the Fourier series converge uniformly to the function that it is designed
to approximate? Let us answer this question for a rather simple case. Pay attention
to the techniques of the following proof as they will be used several times.
Theorem 12.21 (uniform convergence). Suppose that f ∈C2
p(0, 1; C). Then
lim
n→∞∥f −Sn[f ]∥L∞(0,1;C) = 0.
In other words, Sn[f ] →f , uniformly on [0, 1]. In particular,
|ˆfj| ≤C1
j2 ,
j ∈Z⋆,
and
∥f −Sn[f ]∥L∞(0,1;C) ≤2C1
n ,
∀n ∈N
for some constant C1 > 0 that is independent of n.
Proof. By ˆf (2)
j
∈C, we denote the Fourier coeﬃcients of f ′′:
ˆf (2)
j
= (f ′′, ψj)L2(0,1;C) =
Z 1
0
f ′′(x) exp(−2πijx)dx,
j ∈Z.
Using integration by parts and the periodicity of the functions, we ﬁnd
ˆf (2)
j
=
Z 1
0
f ′′(x) exp(−2πijx)dx
= −(−2πij)
Z 1
0
f ′(x)exp (−2πijx)dx
= (−2πij)2
Z 1
0
f (x)exp (−2πijx)dx.
Thus,
ˆfj = −
ˆf (2)
j
4π2j2 ,
j ∈Z⋆.
We know that |ˆf (2)
j
|, |ˆf (2)
−j | →0, as j →∞, by Parseval’s relation for f ′′. Since
every convergent sequence is also bounded, there is a constant C1 > 0 such that
ˆfj
 =
ˆf (2)
j

4π2j2 ≤C1
j2 ,
j ∈Z⋆.
By the Weierstrass M-Test, Theorem B.63, since
ˆfj exp(2πijx)
 ≤C1
j2 ,
j ∈Z⋆,
and
X
j∈Z⋆
1
j2 =
∞
X
j=−∞
j̸=0
1
j2 = π2
3

332
Fourier Series
is convergent, Sn[f ] converges uniformly to a continuous one-periodic function. Let
us label the uniform limit g ∈Cp(0, 1; C).
Now we have Sn[f ] →g uniformly on R and Sn[f ] →f in L2(0, 1; C). We want
to show that f ≡g. By Theorem B.65,
0 = lim
n→∞
Z 1
0
|f (x) −Sn[f ](x)|2 dx
=
Z 1
0
lim
n→∞|f (x) −Sn[f ](x)|2 dx
=
Z 1
0
|f (x) −g(x)|2 dx
= ∥f −g∥2
L2(0,1;C) .
Hence, f −g ≡0 since ∥· ∥L2(0,1;C) is a norm on Cp(0, 1; C).
Finally, we want to prove the convergence rate estimate. Suppose that n ∈N is
ﬁxed and m > n. Then, for any x ∈[0, 1],
|Sm[f ](x) −Sn[f ](x)| ≤
m
X
j=n+1
 |ˆfj| + |ˆf−j|

≤2C1
m
X
j=n+1
1
j2
≤2C1
∞
X
j=n+1
1
j2
≤2C1
n ,
using Lemma 11.19 with p = 2. Now since Sm[f ] →f uniformly on R, as m →∞,
given any ε > 0, there is an N ∈N such that if m ≥N, then
|f (x) −Sm[f ](x)| < ε
for all x ∈R. Therefore, if m ≥N and m > n, then, for all x ∈R,
|f (x) −Sn[f ](x)| = |f (x) −Sm[f ](x) + Sm[f ](x) −Sn[f ](x)|
≤|f (x) −Sm[f ](x)| + |Sm[f ](x) −Sn[f ](x)|
< ε + 2C1
n .
Now since we can make ε > 0 as small as we like, we get
|f (x) −Sn[f ](x)| ≤2C1
n
for all x ∈R and for all n ∈N. Since x is arbitrary, we get our result:
∥f −Sn[f ]∥L∞(0,1;C) ≤2C1
n .
With similar techniques we can establish an improved rate of convergence for
smoother functions.

12.4 Uniform Convergence of Fourier Series
333
Corollary 12.22 (uniform convergence). Suppose that r ≥2 and f ∈Cr
p(0, 1; C).
Then
|ˆfj| ≤C2
|j|r ,
j ∈Z⋆,
and
∥f −Sn[f ]∥L∞(0,1;C) ≤
2C2
(r −1)nr−1 ,
n ∈N,
for some constant C2 > 0 that is independent of n.
Proof. See Problem 12.8.
What if the function of interest, f , is not quite smooth enough so that Theorem
12.21 applies? Does its Fourier series converge uniformly? Let us answer a simpler
question for the moment. The proof of the following results should be obvious after
reading the proof of Theorem 12.21.
Theorem 12.23 (uniform limit). Suppose that f ∈Cp(0, 1; C) and Sn[f ] converges
uniformly in R to some function g : R →C. Then it must be that g ∈Cp(0, 1; C)
and, in fact, g ≡f .
Proof. See Problem 12.9.
Corollary 12.24 (uniform convergence). Suppose that f ∈Cp(0, 1; C) is such that
P∞
j=−∞
ˆfj
 < ∞. Then Sn[f ] converges uniformly and absolutely in R to f .
Proof. Use the Weierstrass M-Test, Theorem B.63. The details are left to the
reader as an exercise; see Problem 12.10.
To make further progress, we need a weakened deﬁnition of the derivative.
Deﬁnition 12.25 (piecewise continuity). Let I ⊆R be an interval, ﬁnite or inﬁnite
in length. Suppose that g : I →C. We say that g is piecewise continuous on I if
and only if:
1. There is X ⊂I of at most countable cardinality such that g ∈C(I\X;C).
2. For any compact interval [c, d] ⊆I, X ∩[c, d] has ﬁnite cardinality.
3. For every x ∈X, the right and left limits exist and are ﬁnite, i.e., there exist
numbers AR, AL ∈C such that
lim
y↓x g(y) = AR,
lim
y↑x g(y) = AL.
Example 12.1
Consider the functions f , g : R →R deﬁned by
f (x) =



1
x ,
x ∈(−∞, 0) ∪(0, ∞),
0,
x = 0,
and
g(x) =
(
x3 + 1,
x ∈(−∞, 0),
x2,
x ∈[0, ∞).

334
Fourier Series
The function f is not piecewise continuous, but the function g is piecewise
continuous.
Deﬁnition 12.26 (piecewise diﬀerentiability). Let I ⊆R be an interval, ﬁnite
or inﬁnite in length, and f : I →C. We say that f is piecewise continuously
diﬀerentiable on I if and only if:
1. f ∈C(I; C).
2. There is a set X ⊂I, of at most countable cardinality, such that f is classically
diﬀerentiable in I\X and fails to be classically diﬀerentiable at the points of X.
3. For any compact interval [c, d] ⊆I, X ∩[c, d] has ﬁnite cardinality.
4. There is a piecewise continuous function g : I →C, whose points of discontinuity
are precisely the points of X, and g(x) = f ′(x) at all points x ∈I\X.
In this case, we write f ′(x) = g(x) for all x ∈I, and we call f ′ a piecewise
derivative of f .
Example 12.2
We observe that, if f is piecewise continuously diﬀerentiable, the
derivative is not unique. For example, suppose that f : R →[0, ∞) is deﬁned by
f (x) = |x|. Of course, f is not diﬀerentiable at x = 0 in the classical sense.
However, both
g1(x) =
(
−1,
−∞< x ≤0,
1,
0 < x < ∞
and
g2(x) =
(
−1,
−∞< x < 0,
1,
0 ≤x < ∞
are candidates for our piecewise derivative. Which one should we choose? In fact,
it does not matter for our purposes, as they diﬀer only on a set of measure zero.
Further, observe that if [c, d] is any ﬁnite interval with c < 0 < d, then g1, g2 ∈
L2(c, d) and g1 ≡g2 in the L2-sense. In particular, for any q ∈C([c, d]; C),
(g1, q)L2(c,d;C) =
Z d
c
g1(x)q(x) dx =
Z d
c
g2(x)q(x) dx = (g2, q)L2(c,d;C).
In fact, the last equality holds for any q ∈L2(c, d; C).
We will make use of the following important integration by parts property for
piecewise continuously diﬀerentiable functions.

12.4 Uniform Convergence of Fourier Series
335
Proposition 12.27 (integration by parts). Let I ⊆R be an open interval, ﬁnite or
inﬁnite in length. Suppose that f ∈C(I; C) is piecewise continuously diﬀerentiable
in I. If [c, d] is any compact subset of I, and g ∈C1([c, d]; C), then
Z d
c
f (x)g′(x)dx = f (x)g(x)|d
c −
Z d
c
f ′(x)g(x)dx.
Proof. See Problem 12.11.
With these notions at hand we can provide a — more general — suﬃcient
condition for uniform convergence of Fourier series.
Theorem 12.28 (uniform convergence). Suppose that f ∈Cp(0, 1; C) is piecewise
continuously diﬀerentiable in R and its piecewise derivative f ′ : R →C is one-
periodic. Then Sn[f ] →f uniformly and absolutely in R.
Proof. We ﬁrst observe that f ′ ∈L2
p(0, 1; C). Therefore, Parseval’s relation holds
for f ′:
∞
X
j=−∞
bf ′j

2
= ∥f ′∥2
L2(0,1;C) < ∞.
Using integration by parts,
ˆfj =
Z 1
0
f (x) exp(−2πijx)dx
= −1
2πij
Z 1
0
f ′(x) exp(−2πijx)dx
= −
bf ′j
2πij ,
j ∈Z⋆.
Now for all n ∈N, using the Cauchy–Schwarz inequality for ﬁnite sums,
n
X
j=−n
ˆfj
 = |ˆf0| + 1
2π
n
X
j=−n
j̸=0
1
j
bf ′j

≤|ˆf0| + 1
2π




n
X
j=−n
j̸=0
1
j2




1/2 



n
X
j=−n
j̸=0
bf ′j

2




1/2
≤|ˆf0| + 1
2π




∞
X
j=−∞
j̸=0
1
j2




1/2 

∞
X
j=−∞
bf ′j

2


1/2
= |ˆf0| + 1
2π
π2
3
1/2
∥f ′∥L2(0,1;C) .

336
Fourier Series
Since the right-hand side does not depend on n,
∞
X
j=−∞
ˆfj
 ≤|ˆf0| +
1
2
√
3 ∥f ′∥L2(0,1;C) .
By Corollary 12.24, Sn[f ] →f uniformly and absolutely on R.
Unfortunately, our (now standard) trick for extracting a rate of uniform
convergence using Lemma 11.19 will not work in the proof of the last theorem. This
leaves us to wonder if a rate of convergence can be extracted for functions that are
merely piecewise continuously diﬀerentiable. In numerical analysis, we always want,
if possible, to get to the best, or optimal, rate of convergence. But this can be
challenging in the present setting, and so we refer the reader to the classic work [48]
to get a more complete picture. In the next example, we will see that a rate of
convergence can be extracted for a particular piecewise continuously diﬀerentiable
function, the triangle wave.
Example 12.3
Let us construct a one-periodic triangle wave. Suppose that
fo : [0, 1) →R is deﬁned via
fo(x) =





x,
0 ≤x ≤1
2,
1 −x,
1
2 < x < 1.
Now let us extend fo to a one-periodic function that we label f :
f (x) = fo(x −⌊x⌋),
x ∈R,
where ⌊· ⌋is the ﬂoor function; see Figure 12.3. Then f
∈Cp(0, 1) and f
is piecewise continuously diﬀerentiable. However, f
̸∈C1
p(0, 1). The Fourier
coeﬃcients of f are
ˆf0 = 1
4,
ˆfj = −(exp(πij) −1)2
4π2j2
,
j ∈Z⋆.
It is easy to see that the even coeﬃcients vanish:
ˆfj = 0,
j = ±2, ±4, ±6, . . . .
The Fourier projections Sn[f ], for n = 3, 5, and 9, are plotted in Figure 12.3.
Observe that Sn[f ] is a real-valued function. Uniform convergence is guaranteed
by Theorem 12.28, but with no rate of convergence. On the other hand, since we
can ﬁnd (directly)
ˆfj
 ≤
1
π2j2 ,
j ∈Z⋆,
we can prove, using our usual technique, that
∥f −Sn[f ]∥L∞(0,1) ≤
2
π2n,
n ∈N.
For the present example, we can show a rate of uniform convergence, even though

12.4 Uniform Convergence of Fourier Series
337
0
0.5
1.0
1.5
2.0
0
0.1
0.2
0.3
0.4
0.5
x
Sn[f ](x)
n = 3
n = 5
n = 9
Figure 12.3 Fourier projections of a triangle wave. Uniform convergence is guaranteed by
Theorem 12.28, but with no rate of convergence. However, in this case, we can show
that ∥f −Sn[f ]∥L∞(0,1) →0 with rate O(n−1), as n →∞; see Example 12.3.
it is not covered by our limited theory. The errors f −Sn[f ] are plotted in Figure
12.4, for n = 3, 5, and 9.
There is still a bit more that we can do using our simple set of tools. We can
slightly weaken the assumption that f ∈C2
p(0, 1) and still get a rate of convergence.
Theorem 12.29 (H¨older continuous derivative). Let α ∈(0, 1]. Suppose that
f ∈C1
p(0, 1; C). Assume, additionally, that f ′ ∈Cα(R; C), i.e., there is a constant
C > 0 such that
|f ′(x) −f ′(y)| ≤C|x −y|α
for all x, y ∈R. Then Sn[f ] →f uniformly and absolutely in R. Furthermore,
|ˆfj| ≤
C3
|j|1+α ,
j ∈Z⋆,

338
Fourier Series
0
0.5
1.0
1.5
2.0
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
x
f (x) −Sn[f ](x)
n = 3
n = 5
n = 9
Figure 12.4 Errors in the Fourier projections of a triangle wave. We can show that
∥f −Sn[f ]∥L∞(0,1) →0 with rate O(n−1), as n →∞; see Example 12.3. Note that the
error function oscillates about zero, though it does not “equi-oscillate.”
and
∥f −Sn[f ]∥L∞(0,1;C) ≤2C3
αnα ,
n ∈N,
for some constant C3 > 0 that is independent of n.
Proof. Using integration by parts and periodicity of the complex exponential, for
j ∈Z⋆, we have
ˆfj =
Z 1
0
f (x) exp(−2πijx)dx
= −1
2πij
Z 1
0
f ′(x) exp(−2πijx)dx
= −1
2πij
Z 1
0
f ′

x + 1
2j

exp

−2πij

x + 1
2j

dx
=
1
2πij
Z 1
0
f ′

x + 1
2j

exp(−2πijx)dx.

12.4 Uniform Convergence of Fourier Series
339
Therefore, for j ∈Z⋆,
|ˆfj| =
1
4π|j|

Z 1
0

f ′

x + 1
2j

−f ′(x)

exp(−2πijx)dx

≤
1
4π|j|
Z 1
0
f ′

x + 1
2j

−f ′(x)
 dx
≤
C
4π|j|
Z 1
0

1
2j

α
dx
=
C
4π2α|j|1+α
=
C3
|j|1+α ,
where C3 =
C
4π2α .
From here, the details should be quite familiar to the reader. By the Weierstrass
M-Test, Theorem B.63, since
X
j∈Z⋆
1
|j|1+α
is convergent, Sn[f ] converges uniformly to a continuous one-periodic function. Let
us label the uniform limit g ∈Cp(0, 1; C). But, since Sn[f ] →g uniformly on R
and Sn[f ] →f in L2(0, 1; C), it follows that f ≡g.
Finally, we need to prove the convergence rate estimate. Suppose that n ∈N is
ﬁxed and m > n. Then, for any x ∈[0, 1],
|Sn[f ](x) −Sm[f ](x)| ≤
m
X
j=n+1
 |ˆfj| + |ˆf−j|

≤2C3
m
X
j=n+1
1
j1+α
≤2C3
∞
X
j=n+1
1
j1+α
≤2C3
αnα ,
using Lemma 11.19 with p = 1 + α. Since the right-hand side is independent of m
and x, we have
∥f −Sn[f ]∥L∞(0,1;C) ≤2C3
αnα .
Remark 12.30 (Lipschitz continuity). This last result shows that we do not actually
need f ∈C2
p(0, 1) in order to get a rate of convergence of order O( 1
n). In Theorem
12.29, if α = 1, in which case f ′ is Lipschitz continuous, we still get a rate of
convergence of order O( 1
n). Compare Theorem 12.29 with Theorem 12.21.

340
Fourier Series
12.5
Convergence of Fourier Series in Sobolev Spaces
Next, let us discuss the convergence of Fourier series in spaces of so-called weakly
diﬀerentiable functions, also known as Sobolev spaces; see Appendix D for an
overview. These spaces are important in the study of partial diﬀerential equations.
Our main result regarding approximation in periodic Sobolev spaces reads as
follows.
Theorem 12.31 (approximation). Let r ∈N and f ∈Hr
p(0, 1; C). For all n ∈N
and any s ∈N0, s < r, we have
∥f −Sn[f ]∥Hs(0,1;C) ≤C
1
nr−s |f |Hr (0,1;C)
for some constant C > 0 that is independent of f and n. If f ∈C∞
p (0, 1; C), then,
for all n, m ∈N and for any s ∈N0,
∥f −Sn[f ]∥Hs(0,1;C) ≤C 1
nm
(12.10)
for some constant C > 0 that is independent of n.
Proof. Suppose that f ∈L2
p(0, 1; C). Let
Ψ = {ψj = exp(2πij · )}∞
j=−∞
be the standard orthonormal trigonometric polynomial system on [0, 1] and ˆfj
denote the jth Fourier coeﬃcient of f . Then, from the Riesz–Fischer Theorem
12.17,
f = lim
n→∞
n
X
j=−n
ˆfjψj,
in the sense of L2
p(0, 1; C). We will prove the ﬁrst part for the special case s = 0
and leave the general case to the reader. For any r ∈N, since the numbers γk and
γ−k, k ∈N, deﬁned in (D.3), are always strictly positive, and since f −Sn[f ] ∈
L2
p(0, 1; C),
f −Sn[f ] =
∞
X
k=n+1
 ˆf−kψ−k + ˆfkψk

=
∞
X
k=n+1

γ−r/2
−k

γ
r/2
−k ˆf−k

ψ−k + γ−r/2
k

γ
r/2
k ˆfk

ψk

.
Since, for k ≥n + 1,
0 ≤γ−r
−k = γ−r
k
=
1
(4π2k2)r ≤
1
(4π2(n + 1)2)r = γ−r
n+1,

12.5 Convergence of Fourier Series in Sobolev Spaces
341
using Parseval’s relation, we have
∥f −Sn[f ]∥2
L2(0,1;C) =
∞
X
k=n+1
γ−r/2
−k

γ
r/2
−k ˆf−k

2
+
∞
X
k=n+1
γ−r/2
k

γ
r/2
k ˆfk

2
≤γ−r
n+1
 
∞
X
k=n+1
γr
−k
ˆf−k
2 +
∞
X
k=n+1
γr
k
ˆfk
2
!
≤γ−r
n+1|f |2
Hr (0,1;C),
where, in the last step, we have used the fact that
|f |2
Hr (0,1;C) =
∞
X
k=−∞
γr
k
ˆfk
2 ≥
∞
X
k=n+1
γr
−k
ˆf−k
2 +
∞
X
k=n+1
γr
k
ˆfk
2 ,
which follows from (D.4) Theorem D.36. We have proven that
∥f −Sn[f ]∥L2(0,1;C) ≤γ−r/2
n+1|f |Hr (0,1;C) =
1
2rπr(n + 1)r |f |Hr (0,1;C).
The other cases can be proven in a similar fashion and are left to the reader as an
exercise; see Problem 12.15.
Remark 12.32 (spectral convergence). The convergence property expressed in
estimate (12.10) of Theorem 12.31 is known as spectral convergence.
In the periodic setting, we can, using Fourier series, provide a more ﬁne-grained
characterization of smoothness.
Deﬁnition 12.33 (fractional Sobolev space). Suppose that f ∈L2
p(0, 1; C) and
α ∈(0, ∞). Let
Ψ = {ψj}∞
j=−∞,
ψj = exp(2πij · )
be the standard orthonormal trigonometric polynomial system on [0, 1] and ˆfj
denote the jth Fourier coeﬃcient of f . We say that f is in the periodic Sobolev6
space of fractional order α, denoted by Hα
p (0, 1; C), if and only if the series
∞
X
j=−∞
γα
j |ˆfj|2 = lim
n→∞
n
X
j=−n
γα
j |ˆfj|2
converges, where
γj = 4π2j2,
j ∈Z.
If f ∈Hα
p (0, 1; C), then, in the quadratic mean, as n →∞,
n
X
j=−n
ˆfj(2πij)α exp(2πij · ) −→gα
for some function gα ∈L2
p(0, 1; C), and we are justiﬁed in writing
gα = dαf
dxα =
∞
X
j=−∞
ˆfj(2πij)α exp(2πij · ).
6 Named in honor of the Soviet mathematician Sergei Lvovich Sobolev (1908–1989).

342
Fourier Series
The function dαf
dxα is called the weak derivative of f of fractional order α. The
Sobolev seminorm of f of fractional order α is deﬁned via
|f |2
Hα(0,1;C) =

dαf
dxα

2
L2(0,1;C)
=
∞
X
j=−∞
γα
j |ˆfj|2.
The following fundamental result connects spaces of continuous functions to
fractional Sobolev spaces. This is a particular case of so-called Sobolev embedding
theorems.
Theorem 12.34 (embedding). Let m ∈N0. Suppose that α > m + 1
2. Then
Hα
p (0, 1; C) ⊂Cm
p (0, 1; C). Furthermore, there is a constant C > 0, which depends
upon α, such that, for all f ∈Hα
p (0, 1; C), we have
f (m)
L∞(0,1;C) ≤C

∥f ∥L2(0,1;C) + |f |Hα(0,1;C)

.
Finally, the Fourier series of f ∈Hα
p (0, 1; C) converges uniformly and absolutely
to f , provided that α > 1
2.
Proof. Suppose that f ∈Hα
p (0, 1; C). Let
Ψ = {ψj = exp(2πij · )}∞
j=−∞
be the standard orthonormal trigonometric polynomial system on [0, 1] and ˆfj
denote the jth Fourier coeﬃcient of f .
We will prove the case m = 0 and leave the general case to the reader as
an exercise; see Problem 12.16. Our goal is to show that the Fourier series of f
converges uniformly and absolutely on R. Using the Cauchy–Schwarz inequality, we
ﬁnd
n
X
j=−n
|ˆfj| = |ˆf0| +
n
X
j=−n
j̸=0
|ˆfj|
= |ˆf0| +
n
X
j=−n
j̸=0
γ−α/2
j
γα/2
j
|ˆfj|
≤|ˆf0| +
v
u
u
u
t
n
X
j=−n
j̸=0
1
γα
j
v
u
u
u
t
n
X
j=−n
j̸=0
γα
j |ˆfj|2
≤|ˆf0| +
v
u
u
u
t
∞
X
j=−∞
j̸=0
1
γα
j
v
u
u
t
∞
X
j=−∞
γα
j |ˆfj|2
≤|ˆf0| +
v
u
u
t
2
(4π2)α
∞
X
j=1
1
j2α |f |Hα(0,1;C)
≤|ˆf0| +
s
2
(4π2)α ζ(2α) |f |Hα(0,1;C),

Problems
343
where
ζ(s) =
∞
X
j=1
1
js ,
s > 1
is the Riemann zeta function.7 Since the right-hand side is independent of n, it
follows that
∞
X
j=−∞
|ˆfj| ≤|ˆf0| +
s
2
(4π2)α ζ(2α) |f |Hα(0,1;C) < ∞.
By Corollary 12.24, the Fourier series of f converges uniformly and absolutely to
f and f ∈Cp(0, 1; C). Finally, it follows that, for all x ∈R,
|f (x)| ≤
∞
X
j=−∞
|ˆfj|
≤|ˆf0| +
s
2
(4π2)α ζ(2α) |f |Hα(0,1;C)
≤∥f ∥L2(0,1) +
s
2
(4π2)α ζ(2α) |f |Hα(0,1;C)
≤
 
1 +
s
2
(4π2)α ζ(2α)
! 
∥f ∥L2(0,1;C) + |f |Hα(0,1;C)

.
Since the right-hand side of the last estimate is independent of x, the result follows.
The following fractional version of Theorem 12.31 is easily proven. In fact, the
proof is basically the same as that of Theorem 12.31.
Theorem 12.35 (approximation). Let α > 0 and f ∈Hα
p (0, 1; C). Then, for all
n ∈N,
∥f −Sn[f ]∥L2(0,1;C) ≤C 1
nα |f |Hα(0,1;C)
for some constant C > 0 that is independent of f and n.
Proof. See Problem 12.17.
Problems
12.1
Prove Proposition 12.3.
12.2
Complete the proof of Theorem 12.4.
12.3
Prove Theorem 12.5.
12.4
Prove Proposition 12.7.
12.5
Prove Lemma 12.9.
12.6
Prove Theorem 12.17.
7 Named in honor of the German mathematician Georg Friedrich Bernhard Riemann
(1826–1866).

344
Fourier Series
12.7
Prove that ℓ2(Z; C) is a complex Hilbert space.
12.8
Prove Corollary 12.22.
12.9
Prove Theorem 12.23.
12.10
Prove Corollary 12.24.
12.11
Prove Proposition 12.27.
12.12
Suppose that fo : [0, 1) →R is deﬁned via
fo(x) =





−1,
0 ≤x ≤1
2,
1,
1
2 < x < 1.
Extend fo to a one-periodic function f :
f (x) = fo(x −⌊x⌋),
∀x ∈R,
where ⌊· ⌋is the ﬂoor function. Compute the Fourier series of f and determine its
convergence properties. Does the series converge uniformly?
12.13
Provide all the details for Example 12.3.
12.14
Suppose that a > 1. Prove that the function φ: (−a, a) →R, deﬁned via
φ(x) =





exp

−
1
1 −x2

,
x ∈(−1, 1),
0,
x ∈(−a, −1] ∪[1, a),
is a test function and supp(φ) = [−1, 1].
12.15
Complete the proof of Theorem 12.31.
12.16
Complete the proof of Theorem 12.34.
12.17
Prove Theorem 12.35.

13
Trigonometric Interpolation and the
Fast Fourier Transform
In this chapter, we discuss trigonometric interpolation. To keep the discussion
simple, we will only treat the case of uniformly spaced points. Surprisingly, we will
see an interesting and deep connection between trigonometric interpolation and
something called the Discrete Fourier Transform (DFT).
Let us think about how interpolation would work in the periodic setting with an
example. Consider the one-periodic function f shown in Figure 13.1. This function
might perhaps describe the waveform of a piece of (very simple) music that repeats
in time every one second. In this case, x will represent time in seconds. We wish
to interpolate this function — which is often referred to as the the signal — at
equally spaced points in [0, 1) with a linear combination of trigonometric functions
of the form sin(2πkx) and cos(2πkx). How can we do this? Suppose that n ∈N
is odd and n ≥3. We use the interpolation points xj = j/n, j = 0, . . . , n −1.
We could equally well use the interpolation points xj = j/n, j = 1, . . . , n, but the
former is the usual convention. Let us assume that the interpolant has the form
q(x) = a0 +
K
X
k=1
(ak cos(2πkx) + bk sin(2πkx)).
Of course, there are 2K + 1 unknown coeﬃcients to determine, and we should
probably set K = (n −1)/2. To determine the coeﬃcients, we use the n = 2K + 1
interpolation conditions
q(xj) = f (xj),
j = 0, . . . , n −1.
Clearly, this yields a square linear system of n equations in n unknowns:
a0 +
K
X
k=1
(ak cos(2πkxj) + bk sin(2πkxj)) = f (xj),
j = 0, . . . , n −1.
But the three tools that we introduced in earlier chapters for solving linear systems
resulting from interpolation problems — the Vandermonde matrix, the Lagrange
basis, and the Newton basis — are not directly applicable in this case. Is the system
always uniquely solvable? How easy is it to solve this system? While the present
system appears to be quite complicated, it turns out that we can exploit some
hidden symmetry to get a unique solution to this system very rapidly; in particular,
using an algorithm called the Fast Fourier Transform (FFT).
One other thing that we should mention before going on is the interesting
situation associated with the case that n is even. Do you see the problem? Every

346
Trigonometric Interpolation and the Fast Fourier Transform
0
0.5
1.0
1.5
2.0
0.5
1.0
1.5
2.0
2.5
x
f (x)
f (xj)
interpolant
Figure 13.1 Trigonometric interpolation of a one-periodic function at six equally spaced
points, xj = j/6, j = 0, . . . , 5.
time we try to balance the number of sine and cosine terms in the interpolant,
we end up with an odd number of them. (The sine term of index k = 0 is always
identically zero.) So what should we do about this? The resolution to this issue is
discussed in Example 13.1.
Before we get stuck worrying too much about practical matters associated with
solving this system, let us address some other important issues. For example, how
well does the interpolant approximate f , the one-periodic function of interest? In
the present case, if f ∈Hm
p (0, 1), then the approximation is always reliably good.
We indicated previously that one must be quite careful about using equally —
spaced points. Remember? Again, if f is suﬃciently nice, say f ∈Hm
p (0, 1), we
need not worry about wild oscillations in the interpolant as n is increased. They do
not appear. As in Chapter 12, we obtain approximations that are spectrally accurate
for “nice” functions. For example, consider Figures 13.1–13.3, where we use n = 6
(Figure 13.1), n = 8 (Figure 13.2), and n = 16 (Figure 13.3). Notice that the
interpolant is a better and better approximation to the one-periodic function f as n
increases. For n = 16, the interpolant and the one-periodic function of interest f
are essentially indistinguishable to the eye; see Example 13.1.
Finally, what if the signal f is corrupted by some noise? For example, suppose
that the recorded waveform f was stored on an analogue cassette tape which has
degraded over time. The signal that we now have can be written as

13.1 Periodic Interpolation and Periodic Grid Functions
347
0
0.5
1.0
1.5
2.0
0.5
1.0
1.5
2.0
2.5
x
f (x)
f (xj)
interpolant
Figure 13.2 Trigonometric interpolation of a one-periodic function at eight equally
spaced points, xj = j/8, j = 0, . . . , 7.
g(x) = f (x) + χ(x),
where χ represents random distortion of f . Using only the noisy data, how could we
recover a waveform (a kind of pseudo-interpolant) that approximates the original
signal f well? This is a complicated issue and a topic of the rich subject of signal
processing and ﬁltering, which we discuss very brieﬂy in the ﬁnal section of the
chapter.
13.1
Periodic Interpolation and Periodic Grid Functions
To get started, let us be precise about what we want to obtain. We have the
following deﬁnition.
Deﬁnition 13.1 (trigonometric interpolation). Let f ∈Cp(0, 1; C) and n ∈N.
Suppose that X = {xj}n−1
j=0 is a uniformly spaced nodal set:
xj = j
n,
j = 0, . . . , n −1.

348
Trigonometric Interpolation and the Fast Fourier Transform
0
0.5
1.0
1.5
2.0
0.5
1.0
1.5
2.0
2.5
x
f (x)
f (xj)
interpolant
Figure 13.3 Trigonometric interpolation of a one-periodic function at 16 equally spaced
points, xj = j/16, j = 0, . . . , 15.
Let K ∈N satisfy n = 2K + 1 if n is odd and n = 2K if n is even. The function
q ∈TK = TK(0, 1) =


p(x) =
K
X
j=−K
cj exp(2πijx)

∃cj ∈C, j = −K, . . . , K



is called a trigonometric interpolating polynomial of f subordinate to X if and
only if
q(xj) = f (xj),
j = 0, . . . , n −1.
Remark 13.2 (parity). As we mentioned above, in the odd case, there is exactly
the same number of free coeﬃcients in the trigonometric polynomial (2K + 1) as
there is interpolation nodes (n = 2K + 1). For the even case, there are still 2K + 1
coeﬃcients, but only n = 2K nodes. How should we deal with this problem?
Before we answer that last question and continue with the discussion of
trigonometric interpolation, we need some machinery. Some of this involves grid
functions and their properties, a topic that will be developed much further in Part
V of this text. When we sample the one-periodic function f at the nodal points in
X, we obtain a discrete function that has a kind of discrete periodicity.
Deﬁnition 13.3 (grid function). Suppose that n ∈N. Deﬁne
V(C) = {w | w : Z →C} .

13.1 Periodic Interpolation and Periodic Grid Functions
349
Elements of this set are called grid functions. For grid functions, we use the
notation wi = w(i), for i ∈Z. The subspace of n-periodic grid functions is
deﬁned as
Vn,p(C) = {w ∈V(C) | wi+mn = wi, ∀i, m ∈Z} .
One of the most important periodic functions is introduced in the following
deﬁnition.
Deﬁnition 13.4 (periodic grid delta function). Suppose that n ∈N. The periodic
grid delta function, denoted δn,p ∈V(C), is deﬁned via
δn,p
j
=
(
1,
∃m ∈Z | j = mn,
0,
∀m ∈Z j ̸= mn
for all j ∈Z.
We have the following simple, but useful, property involving the periodic grid
delta function.
Proposition 13.5 (convolution). Suppose that n ∈N. Then δn,p ∈Vn,p(C).
Furthermore, for any u ∈Vn,p(C), it follows that
n−1
X
j=0
ujδn,p
j−k = uk
(13.1)
for all k ∈Z.
Proof. See Problem 13.1.
Deﬁnition 13.6 (roots of unity). Let n ∈N. Suppose that X =

xj = j
n
	n−1
j=0 is
the standard uniformly spaced nodal set. The grid function
ωn,k = exp(2πixk),
k ∈Z
is called the nth root of unity. To reference the nth root of unity grid function as
a whole, independent of its individual components, we write ωn ∈V(C).
The following elementary properties should be clear.
Proposition 13.7 (properties of ωn). Let n ∈N. Then ωn ∈Vn,p(C) and
ωn
n,k = 1,
∀k ∈Z,
where the superscript is an exponent. Furthermore, the functions ωj
n ∈V(C), j ∈Z,
that are deﬁned (as one would expect) by
ωj
n,k = exp(2πijxk),
k ∈Z,
are periodic, i.e.,
ωj
n,k+mn = ωj
n,k,
∀j, k, m ∈Z
and

ωj
n,k
n
= 1,
∀j, k ∈Z.

350
Trigonometric Interpolation and the Fast Fourier Transform
Proof. See Problem 13.2.
13.2
The Discrete Fourier Transform
Now we come to one of the most important tools in discrete Fourier analysis.
Deﬁnition 13.8 (DFT). Suppose that n ∈N. For all u ∈Vn,p(C) and all k ∈Z,
deﬁne
ˆuk = 1
n
n−1
X
ℓ=0
uℓe−2πikℓ/n.
The grid function ˆu ∈V(C) is called the Discrete Fourier Transform (DFT) of
u, and we write ˆu = Fn[u].
Proposition 13.9 (periodicity). Let n ∈N. For all u ∈Vn,p(C), we have ˆu ∈
Vn,p(C), i.e., the DFT is a periodic grid function.
Proof. See Problem 13.3.
Proposition 13.10 (periodic grid delta). Suppose that n ∈N. For any k ∈Z,
δn,p
k
= 1
n
n−1
X
ℓ=0
e2πikℓ/n.
(13.2)
Proof. Suppose that k ̸= mn for any m ∈Z. Then
e2πik/n 1
n
n−1
X
ℓ=0
e2πikℓ/n = 1
n
n−1
X
ℓ=0
e2πik(ℓ+1)/n = 1
n
n
X
ℓ=1
e2πikℓ/n = 1
n
n−1
X
ℓ=0
e2πikℓ/n,
where the last equality follows from the periodicity of e2πikℓ/n with respect to ℓ.
Thus,
e2πik/n 1
n
n−1
X
ℓ=0
e2πikℓ/n = 1
n
n−1
X
ℓ=0
e2πikℓ/n
⇐⇒

e2πik/n −1
 1
n
n−1
X
ℓ=0
e2πikℓ/n = 0.
But, because it is assumed that k ̸= mn,
e2πik/n ̸= 1.
Therefore, it follows that the sum must equal zero. On the other hand, when
k = mn, for some m ∈Z,
e2πikℓ/n = e2πimnℓ/n = e2πmℓ= 1.
Thus,
1
n
n−1
X
ℓ=0
e2πikℓ/n = 1
n
n−1
X
ℓ=0
e2πikℓ/n = 1
n
n−1
X
ℓ=0
1 = 1
nn = 1.
The claim then follows.
With this property at hand we can prove that the DFT is a bijection.

13.2 The Discrete Fourier Transform
351
Theorem 13.11 (bijection). Let n ∈N. The DFT mapping Fn[u] = ˆu is a linear,
one-to-one, onto mapping from Vn,p(C) to Vn,p(C). Therefore, it has an inverse.
Proof. Clearly, Fn is linear and maps into Vn,p(C). To show that the mapping is
one to one, suppose that u, w ∈Vn,p(C) have the same DFT, i.e., for every k ∈Z,
1
n
n−1
X
ℓ=0
uℓe−2πikℓ/n = 1
n
n−1
X
ℓ=0
wℓe−2πikℓ/n.
Thus, for all k ∈Z,
1
n
n−1
X
ℓ=0
(uℓ−wℓ)e−2πikℓ/n = 0.
Then, for any j ∈Z,
0 = 1
n
n−1
X
k=0
e2πijk/n
n−1
X
ℓ=0
(uℓ−wℓ)e−2πikℓ/n
= 1
n
n−1
X
ℓ=0
(uℓ−wℓ)
n−1
X
k=0
e2πi(j−ℓ)k/n
=
n−1
X
ℓ=0
(uℓ−wℓ)δn,p
j−ℓ
=
n−1
X
ℓ=0
(uℓ−wℓ)δn,p
ℓ−j
= uj −wj,
where we have used (13.1) and (13.2). Since j ∈Z is arbitrary, u = w, which
proves that Fn is one to one.
To show that Fn is onto, suppose that v ∈Vn,p(C) is arbitrary. We want to ﬁnd
some w ∈Vn,p(C) such that ˆw = Fn[w] = v. Deﬁne
wk =
n−1
X
ℓ=0
vℓe2πikℓ/n.

352
Trigonometric Interpolation and the Fast Fourier Transform
Then
ˆwj = Fn[w]j = 1
n
n−1
X
k=0
wke−2πijk/n
= 1
n
n−1
X
k=0
 n−1
X
ℓ=0
vℓe2πikℓ/n
!
e−2πijk/n
= 1
n
n−1
X
k=0
n−1
X
ℓ=0
vℓe2πi(ℓ−j)k/n
=
n−1
X
ℓ=0
vℓ
1
n
n−1
X
k=0
e2πi(ℓ−j)k/n
=
n−1
X
ℓ=0
vℓδn,p
ℓ−j
= vj.
We have again used the identities (13.1) and (13.2). The proof is complete.
Deﬁnition 13.12 (IDFT). The Inverse Discrete Fourier Transform (IDFT) is
deﬁned via
F−1
n [w]k =
n−1
X
ℓ=0
wℓe2πikℓ/n
for all w ∈Vn,p(C).
From Theorem 13.11, it immediately follows that the IDFT is a bijection as well.
Corollary 13.13 (bijection). Let n ∈N. The IDFT mapping F−1
n
is a linear, one-
to-one, onto mapping from Vn,p(C) to Vn,p(C), and it is the inverse of the DFT
mapping Fn:
w = Fn

F−1
n [w]

= F−1
n
[Fn[w]]
for all w ∈Vn,p(C).
Proof. It suﬃces to examine the proof of Theorem 13.11.
Theorem 13.14 (discrete Parseval1). Let n ∈N. Suppose that w ∈Vn,p(C) and
ˆw ∈Vn,p(C) is its DFT. Then
1
n
n−1
X
k=0
wkwk =
n−1
X
k=0
ˆwk ˆwk.
(13.3)
Proof. See Problem 13.4.
1 Named in honor of the French mathematician Marc-Antoine Parseval des Cheˆenes
(1755–1836).

13.2 The Discrete Fourier Transform
353
Deﬁnition 13.15 (discrete convolution). Suppose that n ∈N and u, v ∈Vn,p(C).
Deﬁne, for all k ∈Z,
[u ⋆v]n,p
k
= 1
n
n−1
X
j=0
uk−jvj.
(13.4)
The operator [ · ⋆· ]n,p : Vn,p(C)×Vn,p(C) →V(C) is called the discrete periodic
convolution.
Let us establish a couple of properties of the discrete convolution.
Proposition 13.16 (properties of discrete convolution). Suppose that n ∈N and
u, v ∈Vn,p(C) are arbitrary. Then [u ⋆v]n,p ∈Vn,p(C). Furthermore, the discrete
convolution is commutative, i.e., [u ⋆v]n,p = [v ⋆u]n,p, and
Fn

[u ⋆v]n,p 
= Fn[u]Fn[v].
Proof. See Problem 13.5.
In addition to the standard grid delta function, we also need a singular version
of this function.
Deﬁnition 13.17 (singular periodic grid delta). Suppose that n ∈N. The singular
periodic grid delta function, denoted ˜δn,p ∈Vn,p(C), is deﬁned via
˜δn,p
j
= nδn,p
j
.
The following is a simple consequence of (13.1).
Proposition 13.18 (convolution). Suppose that n ∈N and u ∈Vn,p(C) is arbitrary.
Then

u ⋆˜δn,pn,p = u.
Proof. See Problem 13.6.
Now let us relate periodic functions to periodic grid functions via a new type of
projection.
Deﬁnition 13.19 (grid projection). Suppose that n ∈N. The periodic grid
projection operator, Gn,p : Cp(0, 1; C) →Vn,p(C), is deﬁned as follows: for any
f ∈Cp(0, 1; C), Gn,p[f ] ∈Vn,p(C) is the grid function
Gn,p[f ]j = f (j/n),
∀j ∈Z.
Next, we can extend the DFT to continuous one-periodic functions in a natural
way.
Deﬁnition 13.20 (DFT of continuous functions). Let n ∈N and f ∈Cp(0, 1; C).
For j ∈Z, the numbers
ˆfn,j = 1
n
n−1
X
ℓ=0
Gn,p[f ]ℓexp(−2πijℓ/n) = 1
n
n−1
X
ℓ=0
f (ℓ/n) exp(−2πijℓ/n)
(13.5)

354
Trigonometric Interpolation and the Fast Fourier Transform
are called the discrete Fourier coeﬃcients of f . The grid function
Fn[Gn,p[f ]] ∈Vn,p(C)
is called the Discrete Fourier Transform of f .
To summarize, in order to compute the DFT of a continuous one-periodic
function, we ﬁrst project it into the space of periodic grid functions and then
we apply the DFT. We can then take the IDFT of that object but, in general, we
will not get back the original function, only its grid projection.
13.3
Existence and Uniqueness of the Interpolant
We have all of the necessary machinery in place. Let us get back to the subject of
trigonometric interpolation.
Theorem 13.21 (existence and uniqueness). Let f ∈Cp(0, 1; C) and n ∈N.
Suppose that
X = {xj}n−1
j=0 ,
xj = j
n,
j = 0, . . . , n −1
is a uniformly spaced nodal set. Let K ∈N satisfy n = 2K + 1 if n is odd and
n = 2K if n is even. If n is odd, there exists a unique trigonometric polynomial
q ∈TK of the form
q(x) =
K
X
j=−K
cj exp(2πijx),
(13.6)
which interpolates f at the nodes X. If n is even, there exists a unique interpolating
polynomial q ∈TK, subordinate to X, of the form
q(x) = cK
2 (exp(2πiKx) + exp(−2πiKx)) +
K−1
X
j=−K+1
cj exp(2πijx)
= cK cos(2πKx) +
K−1
X
j=−K+1
cj exp(2πijx).
(13.7)
Proof. Using the grid projection operator, set F = Gn,p[f ] ∈Vn,p(C). Then Fℓ=
f (xℓ) = f (ℓ/n) for ℓ= 0, . . . , n −1. We require, in the odd case (n = 2K + 1),
that
Fℓ= q(xℓ) =
K
X
j=−K
cj exp(2πijxℓ) ,
ℓ= 0, . . . , n −1.
(13.8)

13.3 Existence and Uniqueness of the Interpolant
355
In the even case (n = 2K), we have, for ℓ= 0, . . . , n −1,
q(xℓ) = cK
2 (exp(2πiKxℓ) + exp(−2πiKxℓ)) +
K−1
X
j=−K+1
cj exp(2πijxℓ)
= cK exp(2πiKxℓ) +
K−1
X
j=−K+1
cj exp(2πijxℓ)
=
K
X
j=−K+1
cj exp(2πijxℓ).
(13.9)
Next, we can view the coeﬃcients, cj, as the discrete values of a grid function,
which we label c. We can naturally extend c, so that it is also periodic, i.e.,
c ∈Vn,p(C). These facts allow us to shift the summation indices in (13.8) and
(13.9), so that we may write both expressions, regardless of parity, as
f (ℓ/n) = Fℓ=
n−1
X
j=0
cj exp(2πijxℓ),
ℓ= 0, . . . , n −1.
Finally, using the DFT, the unique solution, regardless of parity, is given by
cj = ˆfn,j = 1
n
n−1
X
ℓ=0
f (ℓ/n) exp(−2πijxℓ),
j = 0, . . . , n −1;
see Theorem 13.11. To get the coeﬃcients with the desired indices, we can use
the periodicity of the grid function c.
Example 13.1
Consider the one-periodic function
f (x) = exp(sin(2πx) −0.1 cos(2πx) + 0.2 cos(4πx) + 0.2 sin(6πx)),
which is shown in Figures 13.1, 13.2, and 13.3. We interpolate this function using
n = 6 (Figure 13.1), n = 8 (Figure 13.2), and n = 16 (Figure 13.3) points,
respectively. Note that it is straightforward to convert the coeﬃcients from (13.7)
to those for the expansion
q(x) = a0 +
K−1
X
k=1
(ak cos(2πkx) + bk sin(2πkx)) + aK cos(2πKx),
n = 2K.
We leave it to the reader to derive the appropriate conversion. In any case, notice
that the interpolant gets better as n gets larger. Of course, one might worry that
adding more points might lead to some kind of undesired Runge phenomenon,
where the interpolant oscillates more, as larger numbers of equally spaced points
are sampled. In the periodic case, however, this does not happen for well-behaved
functions f , as we will show in the next section.

356
Trigonometric Interpolation and the Fast Fourier Transform
We have demonstrated an interesting connection between trigonometric inter-
polation at evenly spaced points and the DFT, which we utilize later in Chapter
26 to solve certain diﬀerential equations. In the meantime, a number of standard
questions arise: How accurate is this interpolation? Does it converge as n →∞?
In what norms can we expect convergence? We will answer these questions in the
next section.
13.4
Alias Error and Convergence of Trigonometric Interpolation
We can, as usual, think of the process of interpolation as an operation, mapping a
function from Cp(0, 1; C) to a function in TK. Let us make a useful deﬁnition.
Deﬁnition 13.22 (trigonometric interpolation). Let n ∈N. Suppose that K ∈N
satisﬁes n = 2K when n is even and n = 2K +1 when n is odd. The trigonometric
interpolation operator, denoted In,p, is deﬁned as follows:
In,p : Cp(0, 1; C) →TK,
f 7→In,p[f ] = qn,
(13.10)
where qn
∈TK is the unique trigonometric interpolating polynomial for f ,
subordinate to X = {j/n}n−1
j=0 , satisfying (13.7) when n is even and (13.6) when n
is odd.
Proposition 13.23 (properties of In,p). Let n ∈N. Suppose that K ∈N satisﬁes
n = 2K when n is even and n = 2K + 1 when n is odd. In,p is a linear projection
operator, i.e.,
In,p[f + g] = In,p[f ] + In,p[g],
∀f , g ∈Cp(0, 1; C),
and
In,p[q] = q,
∀q ∈TK.
Proof. See Problem 13.7.
The goal of this section is to estimate the size of ∥f −In,p[f ]∥L2(0,1;C).
Suppose that n ∈N is ﬁxed. Let K ∈N satisfy n = 2K + 1 if n is odd and
n = 2K if n is even. It follows from the proof of Theorem 13.21 and Deﬁnition
13.20 that, for f ∈Cp(0, 1; C), if n is odd,
In,p[f ](x) =
K
X
j=−K
ˆfn,j exp(2πijx),
∀x ∈R,
(13.11)
and, if n is even,
In,p[f ](x) = ˆfn,K cos(2πKx) +
K−1
X
j=−(K−1)
ˆfn,j exp(2πijx),
∀x ∈R.
(13.12)

13.4 Alias Error and Convergence of Trigonometric Interpolation
357
Suppose that n ∈N is ﬁxed. To properly measure the accuracy of trigonometric
interpolation, we need to compare ˆfn,j and ˆfj. Recall that
ˆfn,j = 1
n
n−1
X
ℓ=0
f (ℓ/n) exp(−2πijℓ/n),
ˆfj =
Z 1
0
f (x) exp(−2πijx)dx.
The ﬁrst object is just a particular Riemann sum approximation2 — a trapezoidal
rule approximation, in particular — of the second. Thus, we may be justiﬁed in
writing ˆfn,j ≈ˆfj. But how good an approximation is this?
Before we answer that last question completely, let us compute a useful
expression for the diﬀerence ˆfn,j −ˆfj, called the alias error.
Lemma 13.24 (alias error). Let n ∈N. Suppose that f ∈Hα
p (0, 1; C) with α > 1
2.
Then
ˆfn,j = ˆfj +
X
ℓ∈Z⋆
ˆfj+ℓδn,p
ℓ,
or, equivalently,
ˆfn,j −ˆfj =
X
m∈Z⋆
ˆfj+mn.
Proof. By Theorem 12.34, the Fourier series of f
converges uniformly and
absolutely to f , and we may write
f (x) =
X
k∈Z
ˆfk exp(2πikx),
∀x ∈R.
By deﬁnition, i.e., 13.5, for all j ∈Z,
ˆfn,j = 1
n
n−1
X
ℓ=0
f (ℓ/n) exp(−2πijℓ/n)
= 1
n
n−1
X
ℓ=0
X
k∈Z
ˆfk exp(2πikℓ/n) exp(−2πijℓ/n)
=
X
k∈Z
ˆfk
1
n
n−1
X
ℓ=0
exp(2πi(k −j)ℓ/n)
=
X
k∈Z
ˆfkδn,p
k−j,
using (13.2). Our manipulations above are justiﬁed because the Fourier series of f
converges uniformly and absolutely. Shifting indices, we have
ˆfn,j = ˆfj +
X
ℓ∈Z⋆
ˆfj+ℓδn,p
ℓ.
Using the deﬁnition of the periodic grid delta function, we have, alternately,
ˆfn,j = ˆfj +
X
m∈Z⋆
ˆfj+m·n.
2 Named in honor of the German mathematician Georg Friedrich Bernhard Riemann
(1826–1866).

358
Trigonometric Interpolation and the Fast Fourier Transform
Now we come to the main result of the present section.
Theorem 13.25 (convergence). Let n ∈N. Suppose that K ∈N satisﬁes n = 2K
when n is even and n = 2K + 1 when n is odd. Let In,p denote the trigonometric
interpolation operator deﬁned in (13.10). Assume that α > 1
2 and f ∈Hα
p (0, 1; C).
There is a constant C > 0 independent of f and n such that
∥f −In,p[f ]∥L2(0,1;C) ≤C
nα |f |Hα(0,1;C).
Proof. Suppose that n is odd. Using Lemma 13.24 and the Cauchy–Schwarz
inequality, we have
∥SK[f ] −In,p[f ]∥2
L2(0,1;C) =
K
X
j=−K
ˆfj −ˆfn,j
2
=
K
X
j=−K

X
m∈Z⋆
ˆfj+mn

2
=
K
X
j=−K

X
m∈Z⋆
γ−α/2
j+mnγα/2
j+mn ˆfj+mn

2
≤
K
X
j=−K


s X
m∈Z⋆
γ−α
j+mn
s X
m∈Z⋆
γα
j+mn
ˆfj+mn
2


2
=
K
X
j=−K
" X
m∈Z⋆
γ−α
j+mn
X
m∈Z⋆
γα
j+mn
ˆfj+mn
2
#
≤
max
−K ≤j ≤K
( X
m∈Z⋆
γ−α
j+mn
)
K
X
j=−K
X
m∈Z⋆
γα
j+mn
ˆfj+mn
2 .
Observe now that
max
−K ≤j ≤K
( X
m∈Z⋆
γ−α
j+mn
)
=
1
(4π2)α
max
−K ≤j ≤K
X
m∈Z⋆
1
(j + mn)2α
=
1
(4π2)α
1
n2α
max
−K ≤j ≤K
X
m∈Z⋆
1
(j/n + m)2α
≤
1
(4π2)α
1
n2α
X
m∈Z⋆
1
(|m| −1/2)2α

13.4 Alias Error and Convergence of Trigonometric Interpolation
359
≤
2
(4π2)α
1
n2α
∞
X
m=1
1
(m −1/2)2α
= 22α+1
(4π2)α
1
n2α
∞
X
m=1
1
(2m −1)2α
≤22α+1
(4π2)α
1
n2α
∞
X
m=1
1
m2α
= 22α+1ζ(2α)
(4π2)α
1
n2α .
Next, we estimate the second term:
K
X
j=−K
X
m∈Z⋆
γα
j+mn
ˆfj+mn
2 ≤
X
ℓ∈Z⋆
γα
ℓ
ˆfℓ
2 = |f |2
Hα(0,1;C).
Putting these last two estimates together, we have
∥SK[f ] −In,p[f ]∥L2(0,1;C) ≤Cn−α|f |Hα(0,1;C)
(13.13)
for some constant C > 0 that depends upon α but is independent of n.
Finally, using the triangle inequality and Theorem 12.35,
∥f −In,p[f ]∥L2(0,1;C) = ∥f −SK[f ] + SK[f ] −In,p[f ]∥L2(0,1;C)
≤∥f −SK[f ]∥L2(0,1;C) + ∥SK[f ] −In,p[f ]∥L2(0,1;C)
≤C 1
Kα |f |Hα(0,1;C) + C 1
nα |f |Hα(0,1;C)
= C 1
nα |f |Hα(0,1;C)
for some constant C > 0 that depends upon α but is independent of f and n.
The case for which n is even is only a little more tedious and is left to the reader
as an exercise; see Problem 13.8.
Before we end this section, let us give another estimate of the interpolation error.
This estimate is not sharp, but it is very easy to produce.
Theorem 13.26 (uniform convergence). Let n ∈N. Suppose that K ∈N satisﬁes
n = 2K when n is even and n = 2K + 1 when n is odd. Let In,p denote the
trigonometric interpolation operator deﬁned in (13.10). If f ∈Cr
p(0, 1; C) for some

360
Trigonometric Interpolation and the Fast Fourier Transform
r ≥2, then there is a constant C > 0 that depends on f and r but is independent
of n such that
∥f −In,p[f ]∥L∞(0,1;C) ≤
C
nr−1 .
Proof. Again, we only prove the estimate for the case that n is odd and leave the
even case to the reader as an exercise; see Problem 13.9. Since f ∈Cr
p(0, 1; C) ⊂
Hr
p(0, 1; C), r ≥2 > 1
2, Theorem 13.25 applies. More speciﬁcally, we use estimate
(13.13) from Theorem 13.25 to get
∥SK[f ] −In,p[f ]∥L∞(0,1;C) ≤
K
X
j=−K
ˆfj −ˆfn,j

≤
v
u
u
t
K
X
j=−K
12
v
u
u
t
K
X
j=−K
ˆfj −ˆfn,j
2
= √n ∥SK[f ] −In,p[f ]∥L2(0,1;C)
≤Cn−r+1/2|f |Hα(0,1;C)
= Cn−r+1/2.
Using the triangle inequality and Corollary 12.22,
∥f −In,p[f ]∥L∞(0,1;C) = ∥f −SK[f ] + SK[f ] −In,p[f ]∥L∞(0,1;C)
≤∥f −SK[f ]∥L∞(0,1;C) + ∥SK[f ] −In,p[f ]∥L∞(0,1;C)
≤2C2
r −1
1
Kr−1 + C
1
nr−1/2
≤C
1
nr−1
for some constant C > 0 that depends upon r and f but is independent of n.
While the proof is too lengthy and complicated to be included here, the following,
due to Jackson [48], gives the sharpest result known regarding uniform convergence
of trigonometric interpolation.
Theorem 13.27 (Jackson3). Let n ∈N. Suppose that K ∈N satisﬁes n = 2K
when n is even and n = 2K + 1 when n is odd. Let In,p denote the trigonometric
interpolation operator deﬁned in (13.10). If f ∈Cr
p(0, 1; C) for some r ≥1, then
there is a constant C > 0 that depends on r but is independent of f and n such
that
∥f −In,p[f ]∥L∞(0,1;C) ≤C log(n)
nr
f (r)
L∞(0,1;C) .
3 Named in honor of the American mathematician Dunham Jackson (1888–1946).

13.5 Numerical Integration of Periodic Functions
361
13.5
Numerical Integration of Periodic Functions
Next, let us introduce a couple of results regarding the numerical integration, i.e.,
the numerical quadrature, of periodic functions. In particular, we show that a certain
quadrature rule is spectrally accurate. This will nicely set the stage for the topic of
the next chapter, which is numerical quadrature in general.
Theorem 13.28 (trapezoidal rule). Let n ∈N. Suppose that, for some r ≥1,
g ∈Cr
p(0, 1; C). Let
X = {xj}n−1
j=0 ,
xj = j
n,
j = 0, . . . , n −1
be a uniformly spaced nodal set. Then

Z 1
0
g(x)dx −1
n
n−1
X
j=0
g(xj)

≤C
nr
for some constant C = C(g, r) > 0 that is independent of n but is dependent upon
g and r. If g ∈C∞
p (0, 1; C), then

Z 1
0
g(x)dx −1
n
n−1
X
j=0
g(xj)

≤1
nm C(g, m)
for any m ∈N, m ≥2.
Proof. To begin, let us compute the error for numerically integrating a trigono-
metric monomial: deﬁne, for any k ∈Z,
En,k =
Z 1
0
exp(2πikx)dx −1
n
n−1
X
j=0
exp(2πikxj).
From (13.2), we have
1
n
n−1
X
j=0
exp(2πikxj) = δn,p
k .
On the other hand,
Z 1
0
exp(2πikx)dx = δ0,k,
k ∈Z,
where δi,j is a standard Kronecker delta function4. Therefore,
En,k =
(
−1,
∃m ∈Z⋆| k = mn,
0,
∀m ∈Z⋆k ̸= mn.
Thus, if q ∈TK, say
q(x) =
K
X
ℓ=−K
cℓexp(2πiℓx),
4 Named in honor of the German mathematician Leopold Kronecker (1823–1891).

362
Trigonometric Interpolation and the Fast Fourier Transform
then
Z 1
0
q(x)dx −1
n
n−1
X
j=0
q(xj) =
K
X
ℓ=−K
cℓEn,ℓ= −
X
ℓ∈JK,n
cℓn,
where
JK,n = {ℓ∈Z | 0 < |ℓn| ≤K} .
Now if g ∈Cr
p(0, 1; C) for r ≥2, by Theorem 12.21, the Fourier partial sums
Sk[f ] converge uniformly to f , as k →∞. This allows us, by Theorem B.65, to
interchange the limits of integration and summation below:
Z 1
0
g(x)dx −1
n
n−1
X
j=0
g(xj) =
Z 1
0
X
k∈Z
ˆgk exp(2πikx)dx −1
n
n−1
X
j=0
X
k∈Z
ˆgk exp(2πikxj)
=
X
k∈Z
ˆgkEn,k
= −
X
k∈Z⋆
ˆgkn.
By Corollary 12.22, since g ∈Cr
p(0, 1; C),

Z 1
0
g(x)dx −1
n
n−1
X
j=0
g(xj)

≤
X
k∈Z⋆
|ˆgkn| ≤2C2
nr
∞
X
k=1
1
kr = 2C2
nr ζ(r).
To ﬁnish this section, we give a complementary result to the last, which requires
a bit less regularity.
Theorem 13.29 (trapezoidal rule). Let n ∈N. Suppose that
X = {xj}n−1
j=0 ,
xj = j
n,
j = 0, . . . , n −1
is a uniformly spaced nodal set. If g ∈Hα
p (0, 1; C) for some α > 1
2, then

Z 1
0
g(x)dx −1
n
n−1
X
j=0
g(xj)

≤C
nα |g|Hα(0,1;C)
for some constant C > 0 that is independent of n and g but is dependent upon α.
Proof. By Theorem 12.34, since α > 1
2, the Fourier series of g converges uniformly
and absolutely to g. Up to the last step, the computations in the proof of the last
theorem hold. By the Cauchy–Schwarz inequality,

13.6 The Fast Fourier Transform (FFT)
363

Z 1
0
g(x)dx −1
n
n−1
X
j=0
g(xj)

≤
X
k∈Z⋆
|ˆgkn|
=
X
k∈Z⋆
γ−α/2
kn
γα/2
kn |ˆgkn|
≤
sX
k∈Z⋆
γ−α
kn
sX
k∈Z⋆
γα
kn |ˆgkn|2
=
s
1
(4π2)αn2α
X
k∈Z⋆
1
k2α
sX
k∈Z⋆
γα
kn |ˆgkn|2
=
1
(2π)αnα
p
2ζ(2α)
sX
k∈Z⋆
γα
kn |ˆgkn|2
=
p
2ζ(2α)
(2π)αnα |g|Hα(0,1;C).
The proof is complete.
The previous two remarkable results show that the composite trapezoidal rule
(14.16) is spectrally accurate; see Problem 14.17 for an alternate proof of Theorem
13.28 using the Euler–Maclaurin Theorem 14.40.
13.6
The Fast Fourier Transform (FFT)
Now let us discuss how the DFT can be computed eﬃciently — actually, very
eﬃciently — in practice. In fact, what makes the DFT such a useful, ubiquitous
tool in modern science, statistics, and mathematics is that it can be executed so
eﬃciently in code. So it is hard to tell the story of the DFT without talking a bit
about the fast algorithm used to compute it.
The DFT/IDFT is a very old object, known even before Fourier had his concept
of trigonometric series. In fact, it was known to Gauss5, who was interested
in trigonometric interpolation applied to the prediction of the periodic motion
of heavenly bodies [40]. The Fast Fourier Transform (FFT) is an algorithm for
computing the DFT/IDFT using O(n log2(n)) operations, rather than O(n2)
operations, the cost one would expect from straightforward application of Deﬁnition
13.8. The essence of the FFT algorithm was, it seems, invented by Gauss in 1805
in his work on trigonometric interpolation, though Gauss himself did not publish
the discovery, it being to him such a trivial matter. It was rediscovered, in various
forms, a couple of times over the years but more or less forgotten [40]. These days,
the modern discovery of the FFT is usually attributed to James Cooley6 and John
Tukey7 in a paper published in 1965 [20, 40].
5 Johann Carl Friedrich Gauss (1777–1855) was a German mathematician.
6 James William Cooley (1926–2016) was an American mathematician.
7 John Wilder Tukey (1915–2000) was an American mathematician and statistician.

364
Trigonometric Interpolation and the Fast Fourier Transform
Cost of FFT: (13.20)
Cost of standard DFT: (13.14)
n = 2r
r
Tn = 3rn + 2n
2n2
64
6
1 280
8 192
128
7
2 944
32 768
256
8
6 656
131 072
512
9
14 848
524 288
1024
10
32 768
2 097 152
2048
11
71 680
8 388 608
Table 13.1 A comparison of the cost of the FFT algorithm and standard DFT for the
case n = 2r.
In this section, let us explain some of the details of the Cooley–Tukey FFT
algorithm for the simplest case, i.e., when n = 2r for some r ∈N. This is sometimes
called the radix-2 case. The algorithm is based on a divide and conquer strategy,
and it involves rewriting the DFT as two separate DFTs of half the size. According
to Deﬁnition 13.8, to compute the DFT, we must calculate the components
ˆuk = 1
n
n−1
X
ℓ=0
uℓe−2πikℓ/n
for k = 0, . . . , n −1 = 2r −1. This can be construed as a matrix–vector
multiplication. Speciﬁcally, deﬁne the matrix Wn = [wk,ℓ]n−1
k,ℓ=0 via
wk,ℓ= ω−k
n,ℓ= e−2πikℓ/n,
k, ℓ= 0, . . . , n −1,
where ωn ∈Vn,p is the nth root of the unity grid function. We use the slightly
unusual convention that column and row indexing start at 0 rather than 1. Then
ˆuk = 1
n [Wnu]k ,
where u = [u0, u1, . . . , un−1]⊺; again, indexing starting with 0. The total cost of
computing the DFT according to the straightforward formula above is, clearly,
n(n + 1)
multiplications
+
n(n −1)
additions/subtractions
= 2n2 = O(n2).
(13.14)
This brute force approach, however, does not take any advantage of underlying
symmetries. Exploiting the structure of the problem, we can lower the cost of
computing the DFT dramatically, as is emphatically demonstrated in Table 13.1.
Using linearity, let us split this computation into two parts, one summing over
the even indices and one over the odd indices:
ˆuk = 1
n
n/2−1
X
ℓ=0
u2ℓe−2πik(2ℓ)/n + 1
n
n/2−1
X
ℓ=0
u2ℓ+1e−2πik(2ℓ+1)/n.

13.6 The Fast Fourier Transform (FFT)
365
We can extract a common factor out of the second summation, and, with just a
little rewriting, we have
ˆuk = 1
2
1
n/2
n/2−1
X
ℓ=0
u2ℓe−2πi kℓ
n/2
DFT even part of u
+e−2πik/n 1
2
1
n/2
n/2−1
X
ℓ=0
u2ℓ+1e−2πi kℓ
n/2
DFT odd part of u
for k = 0, . . . , n −1. Now deﬁne
ˆue
k =
1
n/2
n/2−1
X
ℓ=0
u2ℓe−2πi kℓ
n/2
and
ˆuo
k =
1
n/2
n/2−1
X
ℓ=0
u2ℓ+1e−2πi kℓ
n/2
for k = 0, . . . , n −1, so that
ˆuk = 1
2 ˆue
k + 1
2e−2πik/n ˆuo
k,
k = 0, . . . , n −1.
(13.15)
But observe that, for k = 0, . . . , n/2 −1,
ˆuk+n/2 = 1
2 ˆue
k −1
2e−2πik/n ˆuo
k.
(13.16)
We leave it to the reader to prove the last equality using the periodicity of u and
simple properties of the roots of unity; see Problem 13.11. Finally, we can conclude
from (13.15) and (13.16) that
ˆuk = 1
2

ˆue
k + e−2πik/n ˆuo
k

,
k = 0, . . . , n/2 −1,
ˆuk+n/2 = 1
2

ˆue
k −e−2πik/n ˆuo
k

,
k = 0, . . . , n/2 −1.
(13.17)
In other words, the DFT of u can be constructed from the two DFTs of the even
and odd parts of u, which are exactly half the size of the original. This is the
basis of the FFT. It is computed recursively, using half-sized pieces at each level
of recursion. In other words, we can compute ˆue
k and ˆuo
k using quarter-sized pieces,
and so on.
Using (13.17), let us compute the number of multiplications, Mn, required for
the FFT algorithm at level n. Suppose that the number of multiplications required
to compute the half-sized transforms, ˆue
k and ˆuo
k, via FFT is Mn/2. Then
Mn = 2Mn/2 + 2n.
Of course, by recursion, we can assume that
Mn/2 = 2Mn/4 + 2n/2,
and so on, down to
M2 = 2M1 + 4,
M1 = 2.

366
Trigonometric Interpolation and the Fast Fourier Transform
Naturally, we assume that factors such as e−2πik/n are pre-computed and bring no
extra cost to the algorithm. By induction, we can show that, if n = 2r, r ∈N, the
number of multiplications for the FFT at stage n is precisely
Mn = 2rn + 2n;
(13.18)
see Problem 13.12. Regarding the number of additions/subtractions, An, we clearly
have
An = 2An/2 + n.
By recursion, it follows that
An/2 = 2An/4 + n,
and so on, down to
A2 = 2A1 + 2,
A1 = 0.
By induction, we can prove that, if n = 2r, r ∈N,
An = rn;
(13.19)
see Problem 13.13. Thus, the total cost of the FFT is
Tn = Mn + An = 3n + 2n = O(rn) = O(n log2(n)).
(13.20)
A comparison of the cost of the FFT algorithm and standard DFT is presented in
Table 13.1.
13.7
Fourier Matrices, Least Squares Approximation,
and Basic Signal Processing
As we have indicated, the DFT can be computed via a matrix–vector multiplication
procedure with the appropriate matrix. In this section, we want to explore the details
of that matrix. Along the way, we will see that the FFT algorithm can be interpreted
via an interesting matrix factorization. We will conclude with an interesting result
about discrete least squares approximation using trigonometric polynomials and
interpret the result in the context of signal processing.
Consider the following deﬁnition.
Deﬁnition 13.30 (Fourier matrices). Suppose that n ∈N. Deﬁne the matrices
Fn = [fkℓ]n−1
k,ℓ=0 and Wn = [wkℓ]n−1
k,ℓ=0 via
fk,ℓ= ωk
n,ℓ= e2πikℓ/n,
k, ℓ= 0, . . . , n −1,
(13.21)
wk,ℓ= ω−k
n,ℓ= e−2πikℓ/n,
k, ℓ= 0, . . . , n −1,
(13.22)
respectively, where ωn ∈Vn,p is the nth root of the unity grid function and we
use the convention that the row and column indices begin with 0 rather than 1.
Fn ∈Cn×n is called the Fourier matrix. Wn ∈Cn×n is called the DFT matrix.

13.7 Fourier Matrices, Least Squares Approximation, and Basic Signal Processing
367
Of course, we met the DFT matrix in the last section. These two matrices are
nearly unitary. We have the following.
Proposition 13.31 (properties of the Fourier matrices). Suppose that n ∈N. Then
1. F⊺
n = Fn and W⊺
n = Wn.
2. Fn = Wn and Wn = Fn, where the overline indicates entry-wise complex
conjugation.
3. FH
n = Wn and WH
n = Fn.
4. FnWn = WnFn = n In.
5.
1
√nFn and
1
√nWn are unitary matrices.
Proof. These are all straightforward to show. We will prove property 4 and leave
the others to the reader as an exercise; see Problem 13.14.
4: Using the identity (13.2), we ﬁnd
[FnWn]k,ℓ=
n−1
X
j=0
e2πikj/ne−2πijℓ/n
=
n−1
X
j=0
e2πij(k−ℓ)/n
= nδn,p
k−ℓ
= n [In]k,ℓ.
Example 13.2
Suppose that n = 4. Then, since e2πi/4 = i,
F4 =


1
1
1
1
1
i
i2
i3
1
i2
i4
i6
1
i3
i6
i9

=


1
1
1
1
1
i
−1
−i
1
−1
1
−1
1
−i
−1
i


and
W4 =


1
1
1
1
1
−i
(−i)2
(−i)3
1
(−i)2
(−i)4
(−i)6
1
(−i)3
(−i)6
(−i)9

=


1
1
1
1
1
−i
−1
i
1
−1
1
−1
1
i
−1
−i

.
It is clear that these matrices have the properties outlined in Proposition 13.31.
Example 13.3
Observe that, in analogy with the work of the last section, we can
write
F4 =
I2
D2
I2
−D2
 F2
O2
O2
F2

P
(13.23)
and
W4 =
I2
˜D2
I2
−˜D2
 W2
O2
O2
W2

P,
(13.24)

368
Trigonometric Interpolation and the Fast Fourier Transform
where I2 is the 2 × 2 identity; O2 is the 2 × 2 zero matrix; F2 is the 2 × 2 Fourier
matrix,
F2 =
1
1
1
eπi

=
1
1
1
−1

;
W2 is the 2 × 2 DFT matrix,
W2 =
1
1
1
e−πi

=
1
1
1
−1

;
D2 and ˜D2 are diagonal matrices,
D2 = diag

1, e2πi/4
= diag (1, i) ,
˜D2 = diag

1, e−2πi/4
= diag (1, −i) ;
and P is a permutation matrix,
P =


1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1

.
These factorizations are just another way of expressing the essence of the FFT
algorithm. The reader is tasked with proving these results in Problem 13.15.
Example 13.4
The last example can be generalized. For any n ∈N that is even,
we have
Fn =
In/2
Dn/2
In/2
−Dn/2
 Fn/2
On/2
On/2
Fn/2

P
(13.25)
and
Wn =
In/2
˜Dn/2
In/2
−˜Dn/2
 Wn/2
On/2
On/2
Wn/2

P,
(13.26)
where Dn/2 = diag
 1, ω, ω2, . . . , ωn/2
, ˜Dn/2 = diag
 1, ω−1, ω−2, . . . , ω−n/2
, ω =
e2πi/n; and P is an n × n permutation matrix.
Now we come to a rather surprising result. We will deal with only the even case.
The odd case is similar.
Theorem 13.32 (trigonometric least squares approximation). Let f ∈Cp(0, 1; C)
and n ∈N. Suppose that
X = {xj}n−1
j=0 ,
xj = j
n,
j = 0, . . . , n −1
is a uniformly spaced nodal set. Assume that n is even. Let K ∈N satisfy n = 2K.
Suppose that qK ∈TK, expressed as
qK(x) = cK cos(2πKx) +
K−1
X
j=−K+1
cj exp(2πijx),
(13.27)

13.7 Fourier Matrices, Least Squares Approximation, and Basic Signal Processing
369
is the unique interpolating polynomial of f , subordinate to X. Suppose that M ∈
N, M ≤K, and m = 2M. Then, with the same coeﬃcients as in (13.27), the
polynomial qM ∈TM, expressed as
qM(x) = cM cos(2πMx) +
M−1
X
j=−M+1
cj exp(2πijx),
(13.28)
is the unique trigonometric polynomial that minimizes ∥r∥2
2, where the vector r =
[r0, . . . , rn−1]⊺is deﬁned as
rℓ= f (ℓ/n)−cM cos(2πMxℓ)−
M−1
X
j=−M+1
cj exp(2πijxℓ),
ℓ= 0, . . . , n−1. (13.29)
Proof. Recall from the proof of Theorem 13.21, that we showed that, for the case
of interpolation,
f (ℓ/n) =
K
X
j=−K+1
cj exp(2πijxℓ),
ℓ= 0, . . . , n −1,
where cj is viewed as an n-periodic grid function. Equivalently, in matrix–vector
format,
f = ˜Fnc,
where c = [cj]K
j=−K+1 , f = [f (ℓ/n)]n−1
ℓ=0 ∈Cn. Here, ˜Fn is merely a shifted version
of Fn. In particular,
˜Fn

ℓ,j = ωℓ
n,j = e2πijℓ/n,
ℓ= 0, . . . , n −1,
j = −K + 1, . . . , K.
Since the columns of the Fourier matrix are orthogonal with uniform normalization,
the solution for the interpolation problem is
c = 1
n
˜FH
n f .
(13.30)
For the least squares problem, the residual, deﬁned in (13.29), can be expressed
in matrix–vector form as
r = f −ˆFn,mˆc,
where ˆFn,m ∈Cn×m is the matrix
ˆFn,m

ℓ,j = ωℓ
n,j = e2πijℓ/n,
ℓ= 0, . . . , n −1,
j = −M + 1, . . . , M,
and ˆc = [cj]M
j=−M+1. The solution to the standard least squares problem, of course,
satisﬁes the normal equation (Theorem 5.6):
ˆFH
n,mˆFn,mˆc = ˆFH
n,mf .
Since the columns of Fn (and ˆFn,m) are orthogonal with uniform normalization
(Proposition 13.31), we have
Imˆc = ˆc = 1
n
ˆFH
n,mf .
(13.31)

370
Trigonometric Interpolation and the Fast Fourier Transform
0
0.2
0.4
0.6
0.8
1.0
0.5
1.0
1.5
2.0
2.5
x
f (x)
f (xj) + χ(xj)
interpolant
least squares
Figure 13.4 Trigonometric least squares approximation of corrupted data. f is the pure
one-periodic uncorrupted signal and f (xj) + χ(xj) are the 32 sampled points of the noisy
data. We use m = 8 to obtain a less oscillatory least squares approximation via Theorem
13.32; see also Figures 13.1, 13.2, and 13.3.
It is easy to see that entries j = −M +1, . . . , M of the solution c derived in (13.30)
agree with the entries of the least squares solution ˆc derived in (13.31).
This result is fascinating, because it indicates that a least squares approximation
can be trivially obtained from an interpolant. Now one might argue that using the
interpolant is always better than a lower–order least squares approximation. But
this may not always be the case.
Example 13.5
(a simple ﬁlter) Consider the pure one-periodic signal
f (x) = exp[sin(2πx) −0.1 cos(2πx) + 0.2 cos(4πx) + 0.2 sin(6πx)].
Suppose that the data f have been corrupted by some random noise, χ, that
we want to ﬁlter out. In signal processing, we commonly have to work with the
noisy signal g(x) = f (x) + χ(x), but we really want to know something about f .
One simple tool for trying to remove the noise is to use Theorem 13.32. In other
words, we interpolate the noisy signal using a large number of points, and then we
truncate the interpolant, removing the highly oscillatory terms. What is left, the

Problems
371
least squares approximation of lower order, is often a much better approximation
of the original, uncorrupted signal, f , than the noisy interpolant; see Figure 13.4.
The pure signal f is smooth, but the corrupted signal g, of which we have sampled
n = 32 points, is rough. If we interpolate the sampled points, f (xj)+χ(xj), we get
get an oscillatory interpolant. However, the least squares approximation ( m = 8)
is much less rough and arguably a better approximation of the uncorrupted signal,
at least to the naked eye; see also Figures 13.1, 13.2, and 13.3 and Example 13.1.
Problems
13.1
Prove Proposition 13.5.
13.2
Prove Proposition 13.7.
13.3
Prove Proposition 13.9.
13.4
Prove Theorem 13.14.
13.5
Prove Proposition 13.16.
13.6
Prove Proposition 13.18.
13.7
Prove Proposition 13.23.
13.8
Complete the proof of Theorem 13.25.
13.9
Complete the proof of Theorem 13.26.
13.10
Suppose that the Fourier series for f : R →R,
f (x) =
∞
X
j=−∞
ˆfje2πijx,
is absolutely convergent and
ψ(x) =
n
X
j=−n
cje2πijx ∈Tn
satisﬁes
ψ(xk) = f (xk),
xk =
k
2n + 1,
k = 0, . . . , 2n.
Prove that
ck = ˆfk +
∞
X
j=−∞
ˆfj(2n+1)+k + ˆfj(2n+1)−k

,
−n ≤k ≤n.
13.11
Prove that (13.15) and (13.16) hold.
13.12
Prove that, if n = 2r, r ∈N, the number of multiplications for the FFT
at stage n is precisely given by (13.18).
13.13
Prove that, if n = 2r, r ∈N, the number of additions/subtractions for the
FFT at stage n is precisely given by (13.19).
13.14
Prove Proposition 13.31.
13.15
Prove that the expressions in (13.23) and (13.24) are valid.

14
Numerical Quadrature
In the ﬁnal chapter of this part, we want to approximate the value of a deﬁnite
integral. Given, for example, f ∈C([a, b]), we wish to compute an approximation of
I(a,b)[f ] =
Z b
a
f (x)dx.
If the antiderivative of f is not readily available, then an approximation of the
integral may be a good alternate. Every calculus student already knows how to do
this using Riemann sums. A small, and probably dwindling, number have learned
Simpson’s rule and the trapezoidal rule in calculus for approximating integrals. In
this chapter, we want to estimate the sizes of the errors for such approximation
schemes in a systematic way. The strategy we will adopt to accomplish this is as
follows. Suppose that g ∈C([a, b]), whose antiderivative is simply obtained, and
∥f −g∥L∞(a,b) < ε. Then

Z b
a
f (x)dx −
Z b
a
g(x)dx
 ≤ε(b −a).
This estimate is the basis of most numerical integration methods.
In particular, suppose that X = {xi}n
i=0 ⊂[a, b] is a nodal set and p ∈Pn is the
unique Lagrange interpolating polynomial of f subordinate to X. Then
f (xi) = p(xi),
i = 0, . . . , n,
and
f (x) = p(x) + E(x),
∀x ∈[a, b],
where E is an expression of the interpolation error. Then
Z b
a
f (x)dx =
Z b
a
p(x)dx +
Z b
a
E(x)dx.
But
Z b
a
p(x)dx =
Z b
a
n
X
i=0
Li(x)f (xi)dx =
n
X
i=0
f (xi)
Z b
a
Li(x)dx =
n
X
i=0
f (xi)βi,
where Li ∈Pn is the ith Lagrange nodal basis element and βi is its deﬁnite integral:
βi =
Z b
a
Li(x)dx.

14.1 Quadrature Rules for Weighted Integrals
373
The expression Pn
i=0 f (xi)βi is a typical numerical integration formula, requiring
only certain point values of the integrand f . Regarding the error, we have

Z b
a
f (x)dx −
n
X
i=0
f (xi)βi
 =

Z b
a
E(x)dx
 ≤
Z b
a
|E(x)| dx.
In other words, we can generate an error formula or an error estimate for our
quadrature rule, Pn
i=0 f (xi)βi, by working with the term
R b
a E(x)dx, and much of
this chapter will be devoted to it.
The quadrature weights, βi, depend only on the positions of the nodes within
[a, b], as well as the interval [a, b] itself. Suppose that n = 1, x0 = a, and x1 = b.
Then
p(x) = f (a)x −b
a −b + f (b)x −a
b −a
and
Z b
a
p(x)dx = f (a)
a −b
Z b
a
(x −b)dx + f (b)
b −a
Z b
a
(x −a)dx = b −a
2
(f (a) + f (b)).
This is, essentially, the trapezoidal rule (or trapezium rule).
Now suppose that n = 2, x0 = a, x1 = a+b
2 , and x2 = b. It follows that
p(x) = f (a)
 x −a+b
2

(x −b)
 a −a+b
2

(a −b) + f
a + b
2

(x −a) (x −b)
  a+b
2
−a
   a+b
2
−b

+ f (b)(x −a)
 x −a+b
2

(b −a)
 b −a+b
2
.
It follows that
Z b
a
p(x)dx = b −a
6

f (a) + 4f
a + b
2

+ f (b)

,
which is Simpson’s rule.1
14.1
Quadrature Rules for Weighted Integrals
To keep our discussion general, and because this also appears in applications, we
will consider the approximation of a weighted integral
I(a,b)
w
[f ] =
Z b
a
f (x)w(x)dx.
Here, w is a weight function on the compact interval [a, b] ⊂R. To avoid the
trivial case, we always assume that [a, b] has positive length.
1 Named in honor of the British mathematician Thomas Simpson (1710–1761).

374
Numerical Quadrature
Deﬁnition 14.1 (quadrature rule). Suppose that n, r ∈N0, w is a weight function
on the compact interval [a, b] ⊂R, h = b −a > 0, and f ∈Cr([a, b]). The
expression
Q(a,b)
w,r [f ] =
r
X
i=0
n
X
j=0
βi,jf (i)(xj)
=
n
X
j=0

β0,jf (xj) + β1,jf ′(xj) + · · · + βr,jf (r)(xj)

,
(14.1)
where
βi,j = hi+1 ˆβi,j,
i = 0, . . . , r,
j = 0, . . . , n
(14.2)
and
xj = a + h · ˆxj,
j = 0, . . . , n,
(14.3)
is called a quadrature rule of degree r with intrinsic nodes ˆX = {ˆxj} ⊂[0, 1]
and intrinsic weights {ˆβi,j} ⊂R. The sets X = {xj} ⊂[a, b] and {βi,j} ⊂R are
called the eﬀective nodes and eﬀective weights, respectively. A quadrature rule
of degree r = 0 is called a simple quadrature rule, and we simplify the notation
by writing βj = β0,j and
Q(a,b)
w
[f ] =
n
X
j=0
βjf (xj).
(14.4)
The quadrature error is deﬁned as
EQ[f ] = I(a,b)
w
[f ] −Q(a,b)
w,r [f ].
Deﬁnition 14.2 (consistency). The quadrature rule (14.1) is consistent of order
at least m ∈N0 if and only if EQ[q] = 0 for all q ∈Pm. The quadrature rule (14.1)
is consistent of order exactly m if and only if EQ[q] = 0 for all q ∈Pm; however,
for some r ∈Pm+1, EQ[r] ̸= 0.
Deﬁnition 14.3 (interpolatory quadrature rule). Assume that n ∈N0, w is a
weight function on the compact interval [a, b] ⊂R, and f ∈C([a, b]). Suppose
that X = {xi}n
i=0 ⊂[a, b] is a nodal set and p ∈Pn is the unique Lagrange
interpolating polynomial of f subordinate to X, with
p(x) =
n
X
j=0
f (xj)Lj(x),
where Lj ∈Pn is the jth Lagrange nodal basis element deﬁned in (9.3). The
expression
Q(a,b)
w
[f ] =
n
X
j=0
f (xj)βj,
(14.5)

14.1 Quadrature Rules for Weighted Integrals
375
where
βj =
Z b
a
Lj(x)w(x)dx,
is called an interpolatory quadrature rule subordinate to X of Lagrange2 type
for approximating I(a,b)
w
[f ]. Suppose that f ∈C1([a, b]) and p ∈P2n+1 is the unique
Hermite interpolating polynomial of f subordinate to X, with
p(x) =
n
X
ℓ=0
[F0,ℓ(x)f (xℓ) + F1,ℓ(x)f ′(xℓ)],
where F0,ℓ, F1,ℓ(x) ∈P2n+1 are deﬁned in (9.14). The formula
Q(a,b)
w,1 [f ] =
n
X
j=0
(f (xj)β0,j + f ′(xj)β1,j),
(14.6)
where
β0,j =
Z b
a
F0,j(x)w(x)dx,
β1,j =
Z b
a
F1,j(x)w(x)dx,
is called an interpolatory quadrature rule subordinate to X of Hermite3 type.
Clearly, we can use Lagrange and Hermite interpolation to construct quadrature
rules of degree 0 and degree 1, respectively. Now suppose we look at the problem
from a diﬀerent perspective. Suppose that we want to construct a quadrature rule
such that it is consistent to a certain order. What sort of quadrature rule will one
obtain?
In the case of simple quadrature rules, we have the following result.
Proposition 14.4 (existence and uniqueness). Suppose that X = {xj}n
j=0 is a nodal
set in the compact interval [a, b] ⊂R. There exist unique weights {βj}n
j=0 such
that
Z b
a
q(x)w(x)dx =
n
X
j=0
βjq(xj),
∀q ∈Pn,
(14.7)
or, equivalently,
EQ[q] = 0,
∀q ∈Pn.
Moreover, these weights are given by
βj =
Z b
a
Lj(x)w(x)dx,
j = 0, . . . , n,
where Lj is the jth Lagrange nodal basis polynomial subject to X, which is deﬁned
in (9.3).
Proof. Recall that the Lagrange nodal basis polynomials satisfy
Lj(xk) = δj,k,
j, k ∈{0, 1, . . . , n}.
2 Named in honor of the Italian, later naturalized French, mathematician and astronomer
Joseph-Louis Lagrange (1736–1813).
3 Named in honor of the French mathematician Charles Hermite (1822–1901).

376
Numerical Quadrature
Furthermore, any polynomial q ∈Pn can be written uniquely as
q(x) =
n
X
j=0
q(xj)Lj(x).
Therefore,
Z b
a
q(x)w(x)dx =
n
X
j=0
Z b
a
Lj(x)w(x)dx

q(xj) =
n
X
j=0
βjq(xj)
if and only if
βj =
Z b
a
Lj(x)w(x)dx,
j = 0, . . . , n.
To prove that the weights are unique, suppose that
Z b
a
q(x)w(x)dx =
n
X
j=0
β(1)
j
q(xj) =
n
X
j=0
β(2)
j
q(xj).
Then
n
X
k=0

β(1)
k
−β(2)
k

q(xk) = 0,
∀q ∈Pn.
In particular, suppose that q = Lj. Then
0 =
n
X
k=0

β(1)
k
−β(2)
k

Lj(xk) =
n
X
k=0

β(1)
k
−β(2)
k

δj,k = β(1)
j
−β(2)
j
.
This proves uniqueness.
Remark 14.5 (consistency). The last result shows that the simple quadrature rule,
Q(a,b)
w
[f ] =
n
X
i=0
βif (xi),
is consistent of order at least n if and only if it is a quadrature rule of Lagrange
type. An analogous result could be shown for a quadrature rule of Hermite type;
see Problem 14.2.
It is important to note that, whether a quadrature rule of Lagrange or Hermite
type, once the nodes are chosen the quadrature weights can be easily computed
oﬄine and tabulated. These form an important class of quadrature rules that we
will investigate here.
14.2
Simple Estimates for Interpolatory Quadrature
In this short section, we give two easy estimates for the errors in interpolatory
quadrature rules. It turns out that these estimates can be quite pessimistic in
certain important cases, but they are easily derived.

14.2 Simple Estimates for Interpolatory Quadrature
377
Theorem 14.6 (error estimate). Suppose that n ∈N0, w is a weight function on
the compact interval [a, b] ⊂R, f ∈Cn+1([a, b]), and X = {xi}n
i=0 ⊂[a, b] is a
nodal set. Suppose that Q(a,b)
w
[f ] is the interpolatory quadrature rule subordinate
to X of Lagrange type. Then
|EQ[f ]| ≤
Mn+1
(n + 1)!
Z b
a
|ωn+1(x)| w(x)dx,
(14.8)
where
ωn+1(x) =
n
Y
j=0
(x −xj)
and
Mn+1 =
f (n+1)
L∞(a,b) .
Consequently, an interpolatory quadrature rule subordinate to X of Lagrange type
is consistent of order at least n.
Proof. Suppose that p ∈Pn is the unique interpolating polynomial of f subordinate
to X. From Theorem 9.16, we have, for x ∈[a, b]\X,
f (x) −p(x) = f (n+1)(ξ)
(n + 1)! ωn+1(x)
for some ξ = ξ(x) ∈(a, b) satisfying
min{x0, x1, . . . , xn, x} < ξ < max{x0, x1, . . . , xn, x}.
Then
|EQ[f ]| =

Z b
a
f (n+1)(ξ)
(n + 1)! ωn+1(x)w(x)dx

≤
1
(n + 1)!
Z b
a
f (n+1)(ξ)ωn+1(x)
 w(x)dx
≤
f (n+1)
L∞(a,b)
(n + 1)!
Z b
a
|ωn+1(x)| w(x)dx,
using Theorem B.36 in the last step.
The proof of the following result is similar, using Theorem 9.21.
Theorem 14.7 (error estimate). Suppose that n ∈N0, w is a weight function on
the compact interval [a, b] ⊂R, f ∈C2n+2([a, b]), and X = {xi}n
i=0 ⊂[a, b] is a
nodal set. Suppose that Q(a,b)
w,1 [f ] is the interpolatory quadrature rule subordinate
to X of Hermite type. Then
|EQ[f ]| ≤
M2n+2
(2n + 2)!
Z b
a
|ωn+1(x)|2 w(x)dx,
(14.9)
where
M2n+2 =
f (2n+2)
L∞(a,b) .

378
Numerical Quadrature
Consequently, an interpolatory quadrature rule subordinate to X of Hermite type
is consistent of order at least 2n + 1.
Proof. See Problem 14.3.
14.3
The Peano Kernel Theorem
The Peano Kernel Theorem is an important tool for analyzing quadrature rules.
Before introducing it, let us remind the reader of a standard deﬁnition and state a
needed technical lemma.
Deﬁnition 14.8 (characteristic function). Suppose that B ⊆R. The characteristic
function of B is the function
χB(t) =
(
1,
t ∈B,
0,
t ∈R\B.
Lemma 14.9 (kernel). Suppose that r ∈N0 and m ∈N, with m > r. Deﬁne the
function km : [a, b] × [a, b] →R via
km(x, y) = (x −y)mχ[a,x](y) =
(
(x −y)m,
a ≤y ≤x ≤b,
0,
a ≤x < y ≤b.
(14.10)
Then, for each i ∈{0, 1, . . . , r},
∂ikm
∂xi
∈C([a, b] × [a, b])
and
∂ikm(x, y)
∂xi
=







i−1
Y
k=0
(m −k)(x −y)m−i,
a ≤y ≤x ≤b,
0,
a ≤x < y ≤b.
Proof. See Problem 14.4.
Theorem 14.10 (Peano Kernel Theorem). Suppose that r ∈N0, m ∈N, with m >
r, w is a weight function on the compact interval [a, b] ⊂R, and f ∈Cm+1([a, b]).
Assume that Q(a,b)
w,r [f ] is a quadrature rule of degree r, (14.1), that is consistent of
order at least m. Let the function km : [a, b] × [a, b] →R be deﬁned as in (14.10).
Set
Km(y) = EQ [km( · , y)] =
Z b
a
km(x, y)w(x)dx −
n
X
j=0
r
X
i=0
βi,j
∂ikm(xj, y)
∂xi
. (14.11)
Then the quadrature error satisﬁes
EQ[f ] = 1
m!
Z b
a
f (m+1)(y)Km(y)dy.

14.3 The Peano Kernel Theorem
379
The function Km(y) is called the Peano Kernel.4
Proof. Using Taylor’s Theorem with an integral remainder, Theorem B.39,
f (x) = P(x) + R(x),
where
P(x) =
m
X
j=0
f (j)(a)
j!
(x −a)j,
R(x) = 1
m!
Z x
a
f (m+1)(y)(x −y)m dy.
Observe that we can rewrite the remainder as
R(x) = 1
m!
Z b
a
f (m+1)(y)km(x, y)dy.
By assumption, the quadrature rule is of order at least m, so that
EQ[P] = 0.
As integration and quadrature are linear functionals,
EQ[f ] = EQ[P] + EQ[R] = EQ[R].
Therefore, we need to evaluate the integral of R:
Z b
a
R(x)w(x)dx =
Z b
a
 1
m!
Z b
a
f (m+1)(y)km(x, y)dy

w(x)dx
= 1
m!
Z b
a
Z b
a
km(x, y)w(x)dx

f (m+1)(y)dy.
We also must apply the quadrature rule to R: using Lemma 14.9,
n
X
j=0
r
X
i=0
βi,j
diR(xj)
dxi
= 1
m!
n
X
j=0
r
X
i=0
βi,j
di
dxi
Z b
a
f (m+1)(y)km(x, y)dy

x=xj
= 1
m!
Z b
a


n
X
j=0
r
X
i=0
βi,j
∂ikm(xj, y)
∂xi

f (m+1)(y)dy.
The result now follows:
EQ[f ] = EQ[R] = 1
m!
Z b
a
f (m+1)(y)Km(y)dy,
where Km is given in (14.11).
Remark 14.11 (Peano Kernel). Recall that, for y ∈[a, b], the Peano Kernel is
deﬁned by
Km(y) = EQ [km( · , y)].
4 Named in honor of the Italian mathematician and glottologist Giuseppe Peano (1858–1932).

380
Numerical Quadrature
In other words, the Peano Kernel itself is deﬁned by a quadrature error. Further-
more, we have
EQ[f ] = 1
m!
Z b
a
f (m+1)(y)EQ [km( · , y)]dy.
The following result is obvious.
Corollary 14.12 (quadrature error stability). With the same hypotheses as for
Theorem 14.10, we have
|EQ[f ]| ≤1
m!
f (m+1)
L∞(a,b) ∥Km∥L1(a,b) .
Since ∥Km∥L1(a,b) < ∞, there is a constant C > 0 that may depend on the size of
the interval but is independent of f such that
|EQ[f ]| ≤C
f (m+1)
L∞(a,b) .
(14.12)
Proof. See Problem 14.5.
Corollary 14.13 (constant sign). With the same hypotheses as for Theorem 14.10,
if it is additionally known that Km does not change sign on [a, b], then
EQ[f ] = f (m+1)(ξ)
m!
Z b
a
Km(y)dy
for some ξ ∈(a, b). Furthermore, we have the simple representation for the error
EQ[f ] = EQ

xm+1
(m + 1)! f (m+1)(ξ)
for some ξ ∈(a, b), where EQ

xm+1
is the (computable) quadrature error for the
function x 7→xm+1.
Proof. Theorem 14.10 and the Integral Mean Value Theorem B.41 yield
EQ[f ] = 1
m!
Z b
a
f (m+1)(y)Km(y)dy = 1
m!f (m+1)(ξ)
Z b
a
Km(y)dy
for some ξ ∈(a, b).
Now notice that if Km does not change sign on [a, b], then we have a means for
ﬁnding the quantity
R b
a Km(y)dy, which does not depend upon f . Namely, suppose
that f (x) = xm+1. Then, by the last result,
EQ[f ] = EQ

xm+1
= (m + 1)!
m!
Z b
a
Km(y)dy.
Consequently,
Z b
a
Km(y)dy = EQ

xm+1
m + 1
.
Therefore, in general,
EQ[f ] = 1
m!f (m+1)(ξ)EQ

xm+1
m + 1
,
which gives the desired representation.

14.3 The Peano Kernel Theorem
381
14.3.1
Integral Representation of the Moduli of Smoothness
Let us, as a further application of the ideas presented above, derive an integral
representation of the moduli of smoothness, introduced in Deﬁnition 10.33, which
sometimes is important in applications and can be used to derive many deep results.
We begin by deﬁning the so-called Peano Kernel of ∆k
t .
Deﬁnition 14.14 (Peano Kernel). Let M1 = χ[0,1]. For k ∈N with k ≥2, set
Mk(x) =
Z
R
Mk−1(x −y)M1(y)dy.
For k ∈N and h > 0, we deﬁne
Mk(x, h) = 1
hMk
x
h

.
The following properties of the kernel Mk are immediate.
Lemma 14.15 (properties of Mk). Let k ∈N and Mk be as in Deﬁnition 14.14.
We have:
1. For all h > 0,
supp Mk(·, h) = [0, kh].
2. For all x ∈R and h > 0, we have 0 ≤Mk(x, h) ≤1.
3. For all h > 0,
Z
R
Mk(x, h)dx = 1.
Proof. See Problem 14.6.
We can now give an integral representation of the modulus of smoothness.
Proposition 14.16 (integral representation). Let k ∈N. If f ∈Ck([a, b]; C), then,
for all x[a, b] and t ∈R such that x + kt ∈[a, b], we have
∆k
t f (x) = tk
Z
R
f (k)(y)Mk(y −x, t)dy.
Proof. We prove the result by induction. If k = 1, we have
∆1
t f (x) = f (x + t) −f (x) =
Z x+t
x
f ′(y)dy = t
Z
R
f ′(y)M1(y −x, t)dy.

382
Numerical Quadrature
Assume now that the representation holds for k −1. Then
t−k∆k
t f (x) = t−1∆1
t

t−k+1∆k−1
t
f (x)

= t−1∆1
t
Z
R
f (k−1)(y)Mk−1(y −x, t)dy
=
Z
R
d
dz
Z
R
f (k−1)(y)Mk−1(y −z, t)dyM1(z −x, t)dz
=
Z
R
d
dz
Z
R
f (k−1)(ζ + z)Mk−1(ζ, t)dζM1(z −x, t)dz,
where, in the last step, we applied the change of variable ζ = y −z. Consider now
the term
d
dz
Z
R
f (k−1)(ζ + z)Mk−1(ζ, t)dζ.
Since f (k−1) is diﬀerentiable, and Mk−1 is bounded and of compact support, we
can write
d
dz
Z
R
f (k−1)(ζ + z)Mk−1(ζ, t)dζ =
Z
R
f (k)(ζ + z)Mk−1(ζ, t)dζ
=
Z
R
f (k)(y)Mk−1(y −z, t)dy.
With this representation, we can continue our derivations and write
t−k∆k
t f (x) =
Z
R
Z
R
f (k)(y)Mk−1(y −z, t)dyM1(z −x, t)dz
=
Z
R
f (k)(y)
Z
R
Mk−1(y −z, t)M1(z −x, t)dzdy
=
Z
R
f (k)(y)Mk(y −x, t)dy,
where we exchanged the order of integration, carried out a change of variables, and
used the deﬁnition of Mk. The continuity of f (k) and boundedness of the kernels
justify these steps and imply the result.
As a simple corollary of the last result, we provide a simple proof of two important
properties of the modulus of smoothness.
Corollary 14.17 (properties of the modulus of smoothness). Let [a, b] ⊂R be a
compact interval, k ∈N, and f ∈Ck([a, b]). Then, for all h > 0, we have
ω(k)
f
(h) ≤hk f (k)
L∞(a,b) ,
and, for all m ∈N0,
ω(k+m)
f
(h) ≤hkω(m)
f (k) (h).
Proof. From the previous result we have, for any t,
∆k
t f (x)
 ≤|t|k
Z
R
|f (k)(y)|Mk(y −x, t)dy ≤|t|k f (k)
L∞(a,b) ,

14.4 Proper Scaling and an Error EstimateVia a Scaling Argument
383
where we used the support and integration properties of Mk. Taking supremum
over |t| ≤h, the ﬁrst result follows.
For the second result, we can write
∆k+m
t
f (x) = tk∆m
t
Z
R
f (k)(y)Mk(y −x, t)dy = tk∆m
t
Z
R
f (k)(x + y)Mk(y, t)dy.
Therefore, if |t| ≤h,
∆k+m
t
f (x)
 ≤|t|k
Z
R
∆m
t f (k)(x + y)
 Mk(y, t)dy ≤|t|kω(m)
f (k) (h),
where we used the integration properties of Mk. Taking supremum over |t| ≤h,
the second estimate follows.
14.4
Proper Scaling and an Error Estimate
Via a Scaling Argument
Proper scaling is related to the units of various quantities. Let us ﬁrst consider
what are the units of the integral of f ,
Z b
a
f (x)dx.
The type of units is not important; what is important is how units are transferred.
For example, let us assume that the units of x are those of length and the units of
f are those of a mass density, speciﬁcally mass per unit length. We express units
in the following way:
[x] = m,
[f ] = kg
m ,
where m means meters and kg stands for kilograms. Therefore, the integral has
units of mass:
Z b
a
f (x)dx

= kg
m · m = kg.
Now consider a simple quadrature rule approximating the integral:
Z b
a
f (x)dx ≈
n
X
j=0
βjf (xj).
For the approximation to make sense, it must be that the quadrature rule has the
same units as the integral. This implies that
[βj] = m.
If one uses a quadrature rule of degree r to approximate the integral, we conclude
that
kg =


n
X
j=0
r
X
i=0
βi,jf (i)(xj)

= [βi,j] ·
h
f (i)(xj)
i
= [βi,j] ·
kg
mi+1 ,

384
Numerical Quadrature
so that
[βi,j] = mi+1.
Now our intrinsic weights are deﬁned so that they are unitless. This is usually
expressed as
ˆβi,j

= 1.
And, naturally, [h] = [b −a] = m. Thus, the expression (14.2) makes sense with
respect to units:
mi+1 = [βi,j] =

hi+1 ˆβi,j

= [h]i+1 ·
ˆβi,j

= mi+1.
Furthermore, both numerical values of the eﬀective quadrature weights (14.2)
and eﬀective quadrature nodes (14.3) agree with their intrinsic counterparts when
[a, b] = [0, 1].
With the stability result, Corollary 14.12, at hand, one can prove a type of
convergence. This proof uses what is known as a scaling argument.
Theorem 14.18 (scaling argument). Suppose that w is a weight function on the
compact interval [a, b] ⊂R, h = b −a > 0, m ∈N0, f ∈Cm+1([a, b]), and
Q(a,b)
w,r [f ] is a quadrature rule of degree r < m (14.1) that is consistent of order at
least m. Then
|EQ[f ]| ≤C0hm+2 f (m+1)
L∞(a,b) ,
(14.13)
where C0 > 0 is independent of h and f .
Proof. Let us use an aﬃne transformation and calculate the integral on the
reference interval [0, 1]. To this end, deﬁne
ˆf (ζ) = f (a + ζh),
ˆw(ζ) = w(a + ζh),
ζ ∈[0, 1].
The simple change of variables x = a + hζ and the fact that ˆf (ζ) = f (x) reveals
that
di ˆf
dζi (ˆxj) = hi dif
dxi (xj),
i = 0, . . . , m + 1,
j = 0, . . . , n.
Now on the reference interval [0, 1], by the stability estimate (14.12) for ˆf , there
is a constant C0 > 0 such that

Z 1
0
ˆf (ζ)ˆω(ζ) dζ −
n
X
j=0
r
X
i=1
ˆβi,j ˆf (i)(ˆxj)

≤C0

dm+1 ˆf
dζm+1

L∞(0,1)
,
and C0 must be independent of h, since we are working on the reference interval.

14.5 Newton–Cotes Formulas
385
Using (14.2) and the stability estimate for ˆf , we have

Z b
a
f (x)w(x)dx −
n
X
j=0
r
X
i=1
βi,jf (i)(xj)

= h

Z 1
0
ˆf (ζ)ˆω(ζ)dζ −
n
X
j=0
r
X
i=1
ˆβi,j ˆf (i)(ˆxj)

≤hC0

dm+1 ˆf
dζm+1

L∞(0,1)
≤C0hm+2

dm+1f
dxm+1

L∞(a,b)
,
where C0 > 0 is independent of h and f . The desired result is proven.
In other words, if the interval is small, i.e., h is small, we can realize a small error.
14.5
Newton–Cotes Formulas
A natural, although possibly not optimal, choice for the nodes is to set them
equidistant. In the context of quadrature rules of Lagrange type, this gives rise to
the so-called Newton–Cotes formulas; see, for example, [25, 46, 54].
Deﬁnition 14.19 (Newton–Cotes rules). Suppose that w is a weight function on
the compact interval [a, b] ⊂R and n ∈N. Set h = b −a > 0 and ℏ= h
n. Suppose
that, for the simple quadrature rule (14.4), the nodal set X = {xj}n
j=0 ⊂[a, b] is
deﬁned by
xj = a + jℏ,
j = 0, 1, . . . , n,
(14.14)
and the weights are deﬁned via
βj =
Z b
a
Lj(x)w(x)dx,
Lj(x) =
n
Y
k=0
k̸=j
x −xk
xj −xk
,
j = 0, . . . , n.
The resulting method, denoted Qn[f ], is called a closed Newton–Cotes quadra-
ture rule of order n.5
There are open Newton–Cotes rules as well, though we will not cover these in
the text. For a description, see, for example, [46].
Example 14.1
In Table 14.1, we list closed Newton–Cotes quadrature rules of
order n = 1, 2, 3, 4, assuming a weight function w ≡1 on the compact interval
[a, b] ⊂R. Only the intrinsic nodes and weights are listed. To obtain the eﬀective
versions, use
xj = a + hˆxj,
βj = h ˆβj,
h = b −a.
5 Named in honor of the British mathematician, physicist, astronomer, theologian, and natural
philosopher Sir Isaac Newton (1642–1726/27) and the British mathematician Roger Cotes
(1682–1716).

386
Numerical Quadrature
n
rule
ˆxj
ˆβj
Error formula
1
Trapezoidal
0,1
1
2, 1
2
−1
12ℏ3f (2)(ξ)
2
Simpson’s
0, 1
2, 1
1
6, 4
6, 1
6
−1
90ℏ5f (4)(ξ)
3
Simpson’s 3
8
0, 1
3, 2
3, 1
1
8, 3
8, 3
8, 1
8
−3
80ℏ5f (4)(ξ)
4
Boole’sa
0, 1
4, 1
2, 3
4, 1
7
90, 32
90, 12
90, 32
90,
1
90
−8
945ℏ7f (6)(ξ)
Table 14.1 Common closed Newton–Cotes formulas with integration weight function
w ≡1 on a compact interval [a, b] ⊂R of length h = b −a. Recall, ℏ= h
n; see, for
example, [46, 86]. Notice that, for n = 2 and 4, we observe the phenomenon of
super-convergence, i.e., a higher than expected convergence considering, say,
Theorem 14.20.
a Named in honor of the British mathematician, philosopher, and logician George Boole
(1815–1864).
For example, consider the case n = 3, Simpson’s 3
8 rule, on the reference interval
[0, 1]. The second Lagrange nodal basis element is
ˆL1(x) = x
 x −2
3

(x −1)
1
3
  1
3 −2
3
   1
3 −1
 = 27
2

x3 −5
3x2 + 2
3x

.
Then
ˆβ1 =
Z 1
0
ˆL1(x)dx = 27
2
1
4x4 −5
9x3 + 1
3x2

x=1
x=0
= 3
8.
Luckily, these weights need to be tabulated only once, as they clearly do not depend
upon the function whose integral is being approximated.
Theorem 14.20 (error estimate). Let [a, b] ⊂R be a compact interval. Suppose
that (14.4) is a closed Newton–Cotes quadrature rule of order n ∈N. Then the
order of the quadrature rule is consistent of order at least n. Consequently, if
f ∈Cn+1([a, b]), then
|EQn[f ]| ≤Chn+2 f (n+1)
L∞(a,b) ,
where h = b −a and C > 0 is independent of h and f .
Proof. This follows from Theorem 14.18 and the fact that the quadrature rule was
designed to be exact for all polynomials of degree at most n; see also Theorem
14.6.
This last theorem only gives an error estimate. But, for even numbered rules,
there is an interesting super-convergence that occurs. Speciﬁcally, the order of
convergence is one higher than what one might expect from, say, Theorem 14.20;
see Table 14.1.

14.5 Newton–Cotes Formulas
387
Example 14.2
The reason for the super-convergence observed in Table 14.1 is
that the even degree Newton–Cotes rules are exact for polynomials of degree n+1.
For example, Simpson’s rule is exact for cubics. To see this, it suﬃces to show that
it is exact for f (x) = x3 on the reference interval [0, 1]. Of course,
Z 1
0
x3 dx = 1
4x4

x=1
x=0
= 1
4.
Approximating with Simpson’s rule,
1
6f (0) + 4
6f (1/2) + 1
6f (1) = 4
6(1/2)3 + 1
6 = 1
4.
This remarkable fact is due to the spacing of the nodes.
We wish to prove the error estimates appearing in Table 14.1, and even more
general results, but we need some technical lemmas ﬁrst.
Lemma 14.21 (symmetry). Suppose that n ∈N is even, [a, b] ⊂R is a compact
interval, and the nodal set X = {xj}n
j=0 ⊂[a, b] is deﬁned by (14.14). Set
ωn+1(x) = Qn
j=0(x −xj). Then
ωn+1(a + z) = −ωn+1(b −z),
∀z ∈R,
and
ωn+1(µ + z) = −ωn+1(µ −z),
∀z ∈R,
where µ = a+b
2 .
Proof. The proof is an exercise; see Figure 14.1 and Problem 14.8.
Lemma 14.22 (positivity). Suppose that n ∈N is even, [a, b] ⊂R is a compact
interval, and the nodal set X = {xj}n
j=0 ⊂[a, b] is deﬁned by (14.14). Set
ωn+1(x) = Qn
j=0(x −xj), as usual, and
Ωn+1(x) =
Z x
a
ωn+1(t)dt.
(14.15)
Then Ωn+1(a) = 0, Ωn+1(b) = 0, and
Ωn+1(x) > 0,
∀x ∈(a, b).
Proof. The proof is an exercise; see Figures 14.1 and 14.2 and Problem 14.9.
The following general and remarkable result can be found in [46] and in [54].
Our proof is inspired by those references.
Theorem 14.23 (error estimate for even order). Let [a, b] ⊂R be a compact
interval. Suppose that (14.4) is a closed Newton–Cotes quadrature rule of order
n ∈N and w ≡1. If n is even and f ∈Cn+2([a, b]), then for some ξ ∈(a, b),

388
Numerical Quadrature
0
0.2
0.4
0.6
0.8
1.0
−4e−05
−2e−05
0
2e−05
4e−05
x
ω9(x)
Figure 14.1 The nodal polynomial ω9 for nine equally spaced nodes in [0, 1], determined
according to (14.14). The regions that are light shaded are positive-area regions,
whereas the dark shaded regions are negative-area regions. From left to right, the ﬁrst
light shaded region is greater in area, in absolute value, than the ﬁrst dark shaded
region, which, in turn, is greater in area, in absolute value, than the second light shaded
region, etc. This suggests that Ω9(x), deﬁned in (14.15), is positive for x ∈(a, b); see
Figure 14.2.
EQn[f ] = Cn
f (n+2)(ξ)
(n + 2)! ,
where
Cn =
Z b
a
xωn+1(x)dx < 0.
Proof. By Theorem 9.45,
EQn[f ] =
Z b
a
ωn+1(x)f Jx0, x1, . . . , xn, xKdx.
Since, from Lemma 14.22, Ωn+1 does not change sign on [a, b], using integration
by parts and the Integral Mean Value Theorem B.41, we have

14.5 Newton–Cotes Formulas
389
0
0.2
0.4
0.6
0.8
1.0
0
5e−07
1.0e−06
1.5e−06
2.0e−06
2.5e−06
3.0e−06
x
Ω9(x)
Figure 14.2 The indeﬁnite integral Ω9(x) =
R x
0 ω9(t)dt, where ω9 is as shown in Figure
14.1; see (14.15). The light and dark shaded regions, respectively, correspond to those
in Figure 14.1, where positive- or negative-area contributions are being added to the
integral and the value of Ω9 is increasing or decreasing.
EQn[f ] = Ωn+1(x)f Jx0, x1, . . . , xn, xK|x=b
x=a −
Z b
a
∂
∂x f Jx0, x1, . . . , xn, xKΩn+1(x)dx
= −
Z b
a
f Jx0, x1, . . . , xn, x, xKΩn+1(x)dx
= −f Jx0, x1, . . . , xn, η, ηK
Z b
a
Ωn+1(x)dx,
where η is some number in (a, b). Using Proposition 9.53,
f Jx0, x1, . . . , xn, η, ηK = f (n+2)(ξ)
(n + 2)!
for some ξ ∈(a, b) satisfying
min{x0, x1, . . . , xn, η} < ξ < max{x0, x1, . . . , xn, η}.

390
Numerical Quadrature
Finally, since Ωn+1(x) > 0 in (a, b), using integration by parts again, we ﬁnd
0 <
Z b
a
Ωn+1(x)dx = xΩn+1(x)|x=b
x=a −
Z b
a
xωn+1(x)dx = −
Z b
a
xωn+1(x)dx.
Thus, Cn < 0 and the result is proven.
Thus, it follows that, for n even, the Newton–Cotes quadrature rule is consistent
to order n+2, which is one larger than what we would have expected from Theorem
14.20.
Theorem 14.24 (error estimate for odd order). Let [a, b] ⊂R be a compact
interval. Suppose that (14.4) is a closed Newton–Cotes quadrature rule of order
n ∈N and w ≡1. If n is odd and f ∈Cn+1([a, b]), then for some ξ ∈(a, b),
EQn[f ] = Cn
f (n+1)(ξ)
(n + 1)! ,
where
Cn =
Z b
a
ωn+1(x)dx < 0.
Proof. First note that ωn+1(x) < 0 on the interval (b −ℏ, b) = (xn−1, xn). This
implies, by Theorem 9.45 and the Integral Mean Value Theorem B.41, that
EQn[f ] =
Z b
a
ωn+1(x)f Jx0, x1, . . . , xn, xKdx
=
Z b−ℏ
a
ωn+1(x)f Jx0, x1, . . . , xn, xKdx +
Z b
b−ℏ
ωn+1(x)f Jx0, x1, . . . , xn, xKdx
=
Z b−ℏ
a
ωn+1(x)f Jx0, x1, . . . , xn, xKdx + f Jx0, x1, . . . , xn, ηK
Z b
b−ℏ
ωn+1(x)dx
for some η ∈(a, b). To simplify the ﬁrst integral, let us write
ωn+1(x) = ωn(x)(x −xn) = ωn(x)(x −b)
and deﬁne
Ωn(x) =
Z x
a
ωn(x)dx.
Using Corollary 9.44, we have
I1 =
Z b−ℏ
a
ωn+1(x)f Jx0, x1, . . . , xn, xKdx
=
Z b−ℏ
a
ωn(x)(x −xn)f Jx0, x1, . . . , xn−1, xK −f Jx0, x1, . . . , xnK
(x −xn)
dx
=
Z b−ℏ
a
Ω′
n(x) (f Jx0, x1, . . . , xn−1, xK −f Jx0, x1, . . . , xnK)dx
=
Z b−ℏ
a
Ω′
n(x)f Jx0, x1, . . . , xn−1, xKdx −f Jx0, x1, . . . , xnK
Z b−ℏ
a
Ω′
n(x)dx.

14.5 Newton–Cotes Formulas
391
Now n −1 is even and, by Lemma 14.22, Ωn(x) > 0 for x ∈(a, b −ℏ), and
Ωn(a) = Ω(b −ℏ) = 0. Therefore,
Z b−ℏ
a
Ω′
n(x)dx = 0,
and, using integration by parts and the Integral Mean Value Theorem, again,
I1 =
Z b−ℏ
a
Ω′
n(x)f Jx0, x1, . . . , xn−1, xKdx
= Ωn(x)f Jx0, x1, . . . , xn−1, xK|x=b−ℏ
x=a
−
Z b−ℏ
a
Ωn(x)f Jx0, x1, . . . , xn−1, x, xKdx
= −f Jx0, x1, . . . , xn−1, ζ, ζK
Z b−ℏ
a
Ωn(x)dx
for some ζ ∈(a, b). Using Proposition 9.53, for some ξ1, ξ2 ∈(a, b),
EQn[f ] = −f Jx0, x1, . . . , xn−1, ζ, ζK
Z b−ℏ
a
Ωn(x)dx
+ f Jx0, x1, . . . , xn, ηK
Z b
b−ℏ
ωn+1(x)dx
= −f (n+1)(ξ1)
(n + 1)!
Z b−ℏ
a
Ωn(x)dx + f (n+1)(ξ2)
(n + 1)!
Z b
b−ℏ
ωn+1(x)dx.
Finally, by Lemma 14.22, since n −1 is even,
0 <
Z b−ℏ
a
Ωn(x)dx
= (x −b)Ωn(x)|x=b−ℏ
x=a
−
Z b−ℏ
a
(x −b)ωn(x)dx
= −
Z b−ℏ
a
(x −b)ωn(x)dx.
Furthermore, since ωn+1 < 0 on (b −ℏ, b),
0 < −
Z b
b−ℏ
ωn+1(x)dx.
Using the Summation Mean Value Theorem B.40, there is a point ξ ∈(a, b) such
that
EQn[f ] = −f (n+1)(ξ)
(n + 1)!

−
Z b−ℏ
a
(x −b)ωn(x)dx −
Z b
b−ℏ
ωn+1(x)dx

= f (n+1)(ξ)
(n + 1)!
Z b
a
ωn+1(x)dx.
The proof is complete.

392
Numerical Quadrature
Example 14.3
In Table 14.1, we list error formulas for the closed Newton–Cotes
quadrature rules of order n = 1, 2, 3, 4, assuming a weight function w ≡1 on the
compact interval [a, b] ⊂R. Consider again Simpson’s 3
8 rule. Following Theorem
14.24, for some ξ ∈[a, b],
EQn[f ] = C3
f (4)(ξ)
4!
,
where
C3 =
Z b
a
ω4(x)dx
and
ω4(x) = (x −a)(x −a −ℏ)(x −a −2ℏ)(x −a −3ℏ),
ℏ= h
3,
h = b −a.
Make a change of variables
x = a + hz.
Thus,
C3 = h
Z 1
0
ω4(a + hz)dz
= h5
Z 1
0
z

z −1
3
 
z −2
3

(z −1)dz
= −h5
270.
Therefore,
EQn[f ] = −h5
270
f (4)(ξ)
24
= −ℏ5 3
80f (4)(ξ).
14.6
Peano Error Formulas for Trapezoidal, Midpoint,
and Simpson’s Rules
In this section, we show how to use the Peano Kernel Theorem 14.10, to compute
errors for the trapezoidal rule and Simpson’s rule, assuming, for simplicity, that the
weight function is trivial, i.e., w ≡1 on the integration interval [a, b]. The error
relations we derive are exactly the same as those found in Table 14.1; only the
procedure for calculating the relations is diﬀerent. Speciﬁcally, this procedure does
not use any details of polynomial interpolation error or divided diﬀerences.
Recall that, if a method is exact for polynomials of degree m or less, the
quadrature error satisﬁes
EQn[f ] = 1
m!
Z b
a
f (m+1)(y)Km(y)dy,

14.6 Peano Error Formulas for Trapezoidal, Midpoint, and Simpson’s Rules
393
where Km is the Peano Kernel, deﬁned as
Km(y) = EQn [km( · , y)] =
Z b
a
km(x, y)dx −
n
X
j=0
βjkm(xj, y).
We will show that, in both cases, Km does not change sign on [a, b], allowing us
to use Corollary 14.13.
Theorem 14.25 (error estimates). Suppose that [a, b] ⊂R is a compact interval,
the integration weight function is trivial, w ≡1, and n = 1 or 2. Set h = [a, b] and
ℏ= h
n. If f ∈C2([a, b]), then the error for the trapezoidal rule is
EQ1[f ] = −1
12ℏ3f (2)(ξ)
for some ξ ∈[a, b]. If f ∈C4([a, b]), then the error for Simpson’s rule is
EQ2[f ] = −1
90ℏ5f (4)(ξ)
for some ξ ∈[a, b].
Proof. The trapezoidal rule is exact for polynomials of degree one. Invoking the
Peano Kernel Theorem 14.10, we see that the error satisﬁes
EQ1[f ] =
Z b
a
f (2)(y)K1(y)dy,
where
K1(y) =
Z b
a
k1(x, y)dx −h
2 (k1(a, y) + k1(b, y))
and
k1(x, y) =
(
x −y,
a ≤y ≤x ≤b,
0,
a ≤x < y ≤b.
It follows that
K1(y) = 1
2(b −y)2 −1
2(b −y)(b −a),
which is nonpositive for y ∈[a, b]. Integrating, we ﬁnd
Z b
a
K1(y)dy =
1
6(y −b)3 + 1
4(y −b)2(b −a)
y=b
y=a
= −
 2
12(−h)3 + 3
12(−h)2h

= −h3
12
= −ℏ3
12.

394
Numerical Quadrature
Applying Corollary 14.13, we have, for some ξ ∈[a, b],
EQ1[f ] = −1
12ℏ3f (2)(ξ).
Simpson’s rule, as we have seen in Example 14.2, is exact for polynomials of
degree three. The Peano Kernel Theorem 14.10 guarantees that the error satisﬁes
EQ2[f ] =
Z b
a
f (4)(y)K3(y)dy,
where
K3(y) =
Z b
a
k3(x, y)dx −h
6 (k3(a, y) + 4k3(µ, y) + k3(b, y)),
with µ = a+b
2 , and
k3(x, y) =
(
(x −y)3,
a ≤y ≤x ≤b,
0,
a ≤x < y ≤b.
It follows that
K3(y) =





1
72 (y −a)3 (3y −a −2b),
a ≤y ≤µ,
1
72 (b −y)3 (b + 2a −3t),
µ < y ≤b,
which, again, is nonpositive for y ∈[a, b]. Integrating, we ﬁnd
Z b
a
K3(y)dy = −h5
5760 −
h5
5760 = −h5
2880 = −ℏ5
90.
Using Corollary 14.13, we see that, for some ξ ∈[a, b],
EQ2[f ] = −1
90ℏ5f (4)(ξ).
Now let us introduce a method that is not a closed Newton–Cotes rule.
Deﬁnition 14.26 (midpoint rule). Suppose that [a, b] ⊂R is a compact interval
of positive length. Set µ = a+b
2
and h = b −a. Suppose that f ∈C([a, b]). The
midpoint rule is deﬁned as
Qµ[f ] = hf (µ).
It is clear that the midpoint rule is consistent of order exactly one. Therefore,
we have the following.
Theorem 14.27 (error estimate). Suppose that [a, b] ⊂R is a compact interval,
and the integration weight function is trivial, w ≡1. Set h = b−a. If f ∈C2([a, b]),
then the error for the midpoint rule is
EQµ[f ] = 1
24h3f (2)(ξ)
for some ξ ∈[a, b].

14.7 Composite Quadrature Rules
395
Proof. Since the midpoint rule is exact for polynomials of degree one, invoking the
Peano Kernel Theorem 14.10, we have
EQµ[f ] =
Z b
a
f (2)(y)K1(y)dy,
where
K1(y) =
Z b
a
k1(x, y)dx −hk1(µ, y),
with µ = a+b
2 , and
k1(x, y) =
(
x −y,
a ≤y ≤x ≤b,
0,
a ≤x < y ≤b.
It follows that
K1(y) =





1
2(b −y)2 −(b −a)(µ −y),
a ≤y ≤µ < b,
1
2(b −y)2,
a < µ < y ≤b,
which is nonnegative for y ∈[a, b]. Integrating, we ﬁnd
Z b
a
K1(y)dy = 2
Z µ
a
K1(y)dy
= 2
Z µ
a
1
2(y −b)2 + (b −a)(y −µ)

dy
=
1
3(y −b)3 + (b −a)(y −µ)2
y=µ
y=a
= 1
24h3.
Applying Corollary 14.13, we have, for some ξ ∈[a, b],
EQ1[f ] = 1
24h3f (2)(ξ).
14.7
Composite Quadrature Rules
The typical way of using Newton–Cotes quadrature rules is to partition an interval
into several smaller subintervals of equal size and apply low-order Newton–Cotes
rules on each subinterval. In the case of the composite trapezoidal rule, this is
equivalent to forming a piecewise linear interpolant of the function and integrating
this interpolant. For Simpson’s rule, the composite version of the rule may be
obtained by interpolating the integrand by a piecewise quadratic function and
integrating; see Figures 14.3 and 14.4.
We will assume everywhere in this section, for simplicity, that w ≡1.

396
Numerical Quadrature
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Trapezoidal Rule: m = 1
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Trapezoidal Rule: m = 2
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Trapezoidal Rule: m = 4
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Trapezoidal Rule: m = 8
Figure 14.3 Composite trapezoidal rule approximation of
R π
0 ex cos(x)dx. This rule is
obtained by integrating the piecewise linear approximations of ex cos(x), which are
shown by dashed curves.
Deﬁnition 14.28 (composite Newton–Cotes rules). Let [c, d] ⊂R be a compact
interval, of positive length L = d −c > 0, and m, n ∈N. Suppose that f ∈
C([c, d]). Set h = L
m and ℏ= h
n =
L
mn. Deﬁne
xi,j = c + (i −1)h + jℏ,
i = 1, . . . , m,
j = 0, . . . , n.
The composite closed Newton–Cotes quadrature rule of order n is the rule
Q(c,d)
n,m [f ] =
m
X
i=1
n
X
j=1
βjf (xi,j),
where
βj =
Z h
0
Lj(x)dx,
Lj(x) =
n
Y
k=0
k̸=j
x −kℏ
(j −k)ℏ,
j = 0, . . . , n.
Remark 14.29 (nodes). Note that certain nodes repeat:
xi−1,n = xi,0,
i = 2, . . . , m.
The following are the two most important composite rules.

14.7 Composite Quadrature Rules
397
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Simpson’s Rule: m = 1
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Simpson’s Rule: m = 2
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Simpson’s Rule: m = 4
0
0.5
1.0
1.5
2.0
2.5
3.0
−20
−15
−10
−5
0
x
Composite Simpson’s Rule: m = 8
Figure 14.4 Composite Simpson’s rule approximation of
R π
0 ex cos(x)dx. This rule is
obtained by integrating a piecewise quadratic approximation of ex cos(x).
Example 14.4
Suppose that n = 1. The composite closed Newton–Cotes
quadrature rule of order n = 1 is the composite trapezoidal rule. The rule is
Q(c,d)
1,m [f ] =
m
X
i=1
h
2f (xi,0) + h
2f (xi,1)

.
Let us set
ξk = c + kℏ,
fk = f (ξk),
k = 0, . . . , m.
Then
Q(c,d)
1,m [f ] = ℏ
1
2f0 + f1 + · · · + fm−1 + 1
2fm

.
(14.16)
Example 14.5
Suppose that n = 2 and w ≡1. The composite closed Newton–
Cotes quadrature rule of order n = 2 is the composite Simpson’s rule. The rule is
Q(c,d)
2,m [f ] =
m
X
i=1
h
6f (xi,0) + 4h
6 f (xi,1) + h
6f (xi,2)

.

398
Numerical Quadrature
Let us set
ξk = c + kℏ,
fk = f (ξk),
k = 0, . . . , 2m.
Then
Q(c,d)
2,m [f ] = ℏ
3 (f0 + 4f1 + 2f2 + · · · + 2f2m−2 + 4f2m−1 + f2m) .
(14.17)
The composite midpoint rule cannot be realized as a composite closed Newton–
Cotes rule but is closely related.
Deﬁnition 14.30 (composite midpoint rule). Let [c, d] ⊂R be a compact interval,
of positive length L = d −c > 0, and m ∈N. Suppose that f ∈C([c, d]). Set
h = L
m. Deﬁne
µk = c +

k −1
2

h,
fk = f (µk),
k = 1, . . . , m.
The composite midpoint rule is
Q(c,d)
µ,m [f ] = h
m
X
k=1
fk = h (f1 + · · · + fm).
(14.18)
The error formulas for the composite trapezoidal, Simpson’s, and midpoint rules
are easily obtained.
Theorem 14.31 (error formulas). Suppose that [c, d] is a compact interval of
positive length L = b −c > 0, m ∈N, and h = L
m. If f ∈C2([c, d]), then
EQ1,m[f ] =
Z d
c
f (x)dx −Q(c,d)
1,m [f ] = −Lh2
12 f ′′(η1)
for some η1 ∈[c, d] and
EQµ,m[f ] =
Z d
c
f (x)dx −Q(c,d)
µ,m [f ] = Lh2
24 f ′′(ηµ)
for some ηµ ∈[c, d]. If f ∈C4([c, d]) and n = 2, then
EQ2,m[f ] =
Z d
c
f (x)dx −Q(c,d)
2,m [f ] = −Lℏ4
180f (4)(η2)
for some η2 ∈[c, d], where ℏ= h
2 =
L
2m.
Proof. For the composite trapezoidal rule, there are points ηi,1 ∈[xi,0, xi,1],
i = 1, . . . , m such that
EQ1,m[f ] =
m
X
i=1

−1
12h3f ′′(ηi,1)

.

14.7 Composite Quadrature Rules
399
m
Q1,m
EQ1,m
Rate
16
−12.148 004 099 896 829
7.765 778 350 719 543e−02
–
32
−12.089 742 117 014 199
1.939 580 062 456 514e−02
2.001 386
64
−12.075 194 099 202 140
4.847 782 812 506 196e−03
2.000 347
128
−12.071 558 189 102 351
1.211 872 712 717 721e−03
2.000 086
256
−12.070 649 280 005 421
3.029 636 157 876 325e−04
2.000 021
512
−12.070 422 057 008 418
7.574 061 878 479 199e−05
2.000 005
Table 14.2 Approximation of the integral (14.19) by the composite trapezoidal rule.
The rate of convergence is computed according to (14.20).
m
Q2,m
EQ2,m
Rate
16
−12.070 321 456 053 321
−2.486 033 631 221 574e−05
–
32
−12.070 344 759 931 452
−1.556 458 181 894 982e−06
3.997 507
64
−12.070 346 219 069 089
−9.732 054 451 205 840e−08
3.999 378
128
−12.070 346 310 306 443
−6.083 190 839 945 019e−09
3.999 844
256
−12.070 346 316 009 422
−3.802 114 179 052 296e−10
3.999 954
512
−12.070 346 316 365 873
−2.376 054 908 381 775e−11
4.000 161
Table 14.3 Approximation of the integral (14.19) by the composite Simpson’s rule. The
rate of convergence is computed according to (14.20).
Applying the Summation Mean Value Theorem B.40, there is a point η1 ∈[c, d]
such that
EQ1,m[f ] = −f ′′(η)
m
X
i=1
1
12h3 = −f ′′(η) 1
12mh3 = −f ′′(η1) 1
12Lh2.
The two other results are derived similarly; see Problem 14.11.
Example 14.6
In this example, let us apply the composite trapezoidal and
Simpson’s rule to approximate the value of the deﬁnite integral
Z π
0
ex cos(x)dx = −eπ + 1
2
≈−12.070 346 316 389 633.
(14.19)
Approximations by the composite trapezoidal rule are shown in Table 14.2. The
rate of convergence is computed by the formula
log2
 |EQn,m|
|EQn,2m|

.
(14.20)
Approximations by the composite Simpson’s rule are shown in Table 14.3; see
Figures 14.3 and 14.4.

400
Numerical Quadrature
14.8
Bernoulli Numbers and Euler–Maclaurin Error Formulas
The purpose of this section is to introduce error formulas for composite quadrature
rules in terms of expansions in powers of h. To begin, we need to introduce special
sequences of numbers and polynomials.
Theorem 14.32 (exponential generating function). The function g : C →C,
deﬁned by
g(z) =
z
ez −1,
is holomorphic in the disk
D = {z ∈C | |z| < 2π} ,
and an expansion of the form
g(z) =
z
ez −1 =
∞
X
j=0
Bn
n! zn,
|z| < 2π
(14.21)
is, therefore, valid.
Proof. The point z = 0 is a removable singularity. However, the points z = 2kπi,
for k ∈Z\0 are essential singularities; see, for example, [60, 77].
The coeﬃcients in the expansion of the function of the previous result bear an
important name.
Deﬁnition 14.33 (Bernoulli numbers). The sequence {Bn}∞
n=0 ⊂C, appearing in
(14.21), is called the sequence of Bernoulli numbers.6
Proposition 14.34 (properties of Bernoulli numbers). The Bernoulli numbers
satisfy B0 = 1 and
n
X
j=0
n!
j!(n −j)!Bj = Bn,
n ∈N.
(14.22)
Furthermore,
B2j+1 = 0,
j ∈N.
(14.23)
Proof. Since
z = g(z) (ez −1),
using the expansion of the exponential, it follows that
 ∞
X
k=1
1
k!zk
! 

∞
X
j=0
Bj
j! zj

= z,
|z| < 2π.
6 Named in honor of the Swiss mathematician Jacob Bernoulli (1654/5–1705).

14.8 Bernoulli Numbers and Euler–Maclaurin Error Formulas
401
n
Bn(x)
Bn
0
1
1
1
x −1
2
−1
2
2
x2 −x + 1
6
1
6
3
x3 −3
2x2 + 1
2x
0
4
x4 −2x3 + x2 −
1
30
−1
30
5
x5 −5
2x4 + 5
3x3 −1
6x
0
6
x6 −3x5 + 5
2x4 −1
2x2 +
1
42
1
42
7
x7 −7
2x6 + 7
2x5 −7
6x3 + 1
6x
0
8
x8 −4x7 + 14
3 x6 −7
3x4 + 2
3x2 −
1
30
−1
30
9
x9 −9
2x8 + 6x7 −21
5 x5 + 2x3 −
3
10x
0
10
x10 −5x9 + 15
2 x8 −7x6 + 5x4 −3
2x2 +
5
66
5
66
Table 14.4 The ﬁrst 11 Bernoulli polynomials and Bernoulli numbers.
This can be used to prove (14.22). To prove (14.23), we use the fact that, for
|z| < 2π,
g(−z) =
−z
e−z −1 =
∞
X
j=0
(−1)nBn
n!
zn = z +
∞
X
j=0
Bn
n! zn.
The details are left to the reader as an exercise; see Problem 14.12.
Theorem 14.35 (generating function). The function G : R × C →C, deﬁned by
G(t, z) = etzg(z),
is, for each t ∈R, holomorphic in the disk D = {z ∈C | |z| < 2π}. Consequently,
an expansion of the form
G(t, z) = etz
z
ez −1 =
∞
X
j=0
Bn(t)
n!
zn,
|z| < 2π
(14.24)
is, therefore, valid.
Proof. This follows because etz is entire (holomporphic in C), for every t ∈R, and
g(z) is holomorphic in D.
Deﬁnition 14.36 (Bernoulli polynomials). The coeﬃcients Bn(t) in (14.24) are
called the Bernoulli polynomials.
Table 14.4 shows the ﬁrst 11 Bernoulli numbers and polynomials. From the def-
inition, however, it is not evident that these are indeed polynomials. The following
result establishes this.
Proposition 14.37 (properties of Bernoulli polynomials). The Bernoulli polynomi-
als are indeed real-valued polynomials of degree exactly n. They satisfy B0(t) = 1,
Bn(t) =
n
X
j=0
n!
j!(n −j)!Bn−jtj,
n ∈N,

402
Numerical Quadrature
and
Bn(t) = Bn + n
Z t
0
Bn−1(t)dt,
n ∈N.
Consequently, Bn(0) = Bn and B′(t) = nBn−1(t).
Proof. See Problem 14.13.
Some further properties of the Bernoulli polynomials are important for us.
Proposition 14.38 (further properties). The Bernoulli polynomials {Bn(t)}∞
n=0
satisfy:
1.
Bj(t + 1) −Bj(t) = jtj−1,
j ∈N.
2.
Bj(1) = Bj(0) = Bj,
j ∈N,
j > 1.
3.
Z 1
0
Bj(t) dt = 0,
j ∈N.
4.
(−1)j (B2j(t) −B2j) ≥0,
∀t ∈[0, 1],
j ∈N.
Proof. See Problem 14.14.
With the help of Bernoulli numbers and polynomials, we can establish error
representations for quadrature rules.
Theorem 14.39 (error representation). Suppose that k ∈N and f ∈C2k+1([0, 1]).
Then
Z 1
0
f (x)dx −1
2 [f (0) + f (1)] = −
k
X
j=1
B2j
(2j)!
h
f (2j−1)(1) −f (2j−1)(0)
i
+ R2k[f ],
where
R2k[f ] =
−1
(2k + 1)!
Z 1
0
f (2k+1)(x)B2k+1(x)dx.
If f ∈C2k+2([0, 1]), then
R2k[f ] =
1
(2k + 2)!
Z 1
0
f (2k+2)(x) (B2k+2(x) −B2k+2)dx
=
1
(2k + 2)!f (2k+2)(η)
Z 1
0
(B2k+2(x) −B2k+2)dx
= −
1
(2k + 2)!f (2k+2)(η)B2k+2
for some η ∈[0, 1].

14.8 Bernoulli Numbers and Euler–Maclaurin Error Formulas
403
Proof. Using integration by parts and properties of the Bernoulli polynomials, we
have
Z 1
0
f (x)dx =
Z 1
0
f (x)B′
1(x)dx
= B1(1)f (1) −B1(0)f (0) −
Z 1
0
f ′(x)B1(x)dx
= 1
2 [f (1) + f (0)] −
Z 1
0
f ′(x)1
2B′
2(x)dx.
To shorten notation, set
E =
Z 1
0
f (x)dx −1
2 [f (1) + f (0)].
The previous computations have then shown that
E = −
Z 1
0
f ′(x)1
2B′
2(x)dx.
Using integration by parts twice more, we obtain
E = −1
2 [B2(1)f ′(1) −B2(0)f ′(0)] +
Z 1
0
f ′′(x)1
2B2(x)dx
= −B2
2 [f ′(1) −f ′(0)] +
Z 1
0
f ′′(x) 1
3!B′
3(x)dx
= −B2
2 [f ′(1) −f ′(0)] −
Z 1
0
f ′′′(x) 1
3!B3(x)dx
= −B2
2 [f ′(1) −f ′(0)] +
Z 1
0
f ′′′(x) 1
4!B′
4(x)dx,
where we have used the fact that
B3(0) = B3(1) = B3 = 0.
Integrating by parts twice more gives
E = −
2
X
j=1
B2j
(2j)!
h
f (2j−1)(1) −f (2j−1)(0)
i
−1
5!
Z 1
0
f (5)(x)B5(x)dx,
using
B5(0) = B5(1) = B5 = 0.
Continuing in this fashion, we obtain the result with the remainder
R2k[f ] =
−1
(2k + 1)!
Z 1
0
f (2k+1)(x)B2k+1(x)dx.

404
Numerical Quadrature
Now if f ∈C2k+2([0, 1]), using integration by parts once more, we have
R2k[f ] =
−1
(2k + 2)!
Z 1
0
f (2k+1)(x) d
dx (B2k+2(x) −B2k+2)dx
=
1
(2k + 2)!
Z 1
0
f (2k+2)(x) (B2k+2(x) −B2k+2)dx
−
1
(2k + 2)!
h
f (2k+1)(x) (B2k+2(x) −B2k+2)
ix=1
x=0
=
1
(2k + 2)!
Z 1
0
f (2k+2)(x) (B2k+2(x) −B2k+2)dx.
Since B2k+2(x) −B2k+2 does not change sign in [0, 1], using the integral mean
value theorem, we have, for some η ∈[0, 1],
R2k[f ] =
1
(2k + 2)!f (2k+2)(η)
Z 1
0
(B2k+2(x) −B2k+2)dx
= −
1
(2k + 2)!f (2k+2)(η)B2k+2,
since
R 1
0 B2k+2(x)dx = 0. The proof is complete.
The previous result was a special case of the so-called Euler–Maclaurin Formula.7
Theorem 14.40 (Euler–Maclaurin). Suppose that m, k ∈N are positive integers,
[c, d] ⊂R is a compact interval of positive length, and f ∈C2k+1([c, d]). Deﬁne
h = d−c
m
and
Bn(t) = Bn(t −⌊t⌋).
Then
EQ1,m[f ] = −
k
X
j=1
B2jh2j
(2j)!
h
f (2j−1)(d) −f (2j−1)(c)
i
−
h2k+1
(2k + 1)!
Z d
c
f (2k+1)(x)B2k+1
x −c
h

dx.
7 Named in honor of the Swiss mathematician, physicist, astronomer, geographer, logician, and
engineer Leonhard Euler (1707–1783) and the British mathematician Colin Maclaurin
(1698–1746).

14.8 Bernoulli Numbers and Euler–Maclaurin Error Formulas
405
If f ∈C2k+2([c, d]), then
EQ1,m[f ] = −
k
X
j=1
B2jh2j
(2j)!
h
f (2j−1)(d) −f (2j−1)(c)
i
+
h2k+2
(2k + 2)!
Z d
c
f (2k+2)(x)

B2k+2
x −c
h

−B2k+2

dx
= −
k
X
j=1
B2jh2j
(2j)!
h
f (2j−1)(d) −f (2j−1)(c)
i
−h2k+2B2k+2
(2k + 2)! (d −c)f (2k+2)(η)
(14.25)
for some η ∈[c, d].
Proof. Deﬁne
ξi = c + ih,
i = 0, . . . , m.
Now ﬁx i and set
x = ξi + th,
0 ≤t ≤1,
and
g(t) = f (x) = f (ξi + th),
0 ≤t ≤1.
Then
drg
dtr (t) = hr drf
dxr (x).
By Theorem 14.39,
Z 1
0
g(t)dt −1
2 [g(0) + g(1)] = −
k
X
j=1
B2j
(2j)!
h
g(2j−1)(1) −g(2j−1)(0)
i
−
1
(2k + 1)!
Z 1
0
g(2k+1)(t)B2k+1(t)dt.
Using our change of variable,
h−1
Z ξi
ξi−1
f (x)dx −1
2 [f (ξi−1) + f (ξi)]
= −
k
X
j=1
B2jh2j−1
(2j)!
h
f (2j−1)(ξi) −f (2j−1)(ξi−1)
i
−
h2k
(2k + 1)!
Z ξi
ξi−1
f (2k+1)(x)B2k+1
x −ξi
h

dx.

406
Numerical Quadrature
Thus, for each i = 1, . . . , m,
Z ξi
ξi−1
f (x)dx −h
2 [f (ξi−1) + f (ξi)]
= −
k
X
j=1
B2jh2j
(2j)!
h
f (2j−1)(ξi) −f (2j−1)(ξi−1)
i
−
h2k+1
(2k + 1)!
Z ξi
ξi−1
f (2k+1)(x)B2k+1
x −ξi
h

dx.
Summing, from i = 1 to i = m, we have
Z d
c
f (x)dx −h
2 [f (c) + f (d)] −h
m−1
X
i=1
f (ξi)
= −
k
X
j=1
B2jh2j
(2j)!
h
f (2j−1)(d) −f (2j−1)(c)
i
−
h2k+1
(2k + 1)!
Z d
c
f (2k+1)(x)B2k+1
x −c
h

dx.
The remaining details are left to the reader as an exercise; see Problem 14.15.
Using simple algebraic manipulations, we can formulate Euler–Maclaurin–type
expansions for other composite quadrature rules.
Lemma 14.41 (Simpson’s rule). Suppose that [c, d] ⊂R is a compact interval of
positive length and m ∈N. Assume that f ∈C([c, d]). Then
−1
3EQ1,m[f ] + 4
3EQ1,2m[f ] = EQ2,m[f ].
Proof. Set L = d−c
m . Then
EQ1,m[f ] =
Z d
c
f (x)dx −L
m
1
2f0 + f2 + f4 + · · · + 1
2f2m

,
EQ1,2m[f ] =
Z d
c
f (x)dx −L
2m
1
2f0 + f1 + f2 + · · · + 1
2f2m

,
where
fi = f (ξi),
ξi = c + L
2mi,
i = 1, . . . , 2m.
Therefore,
−1
3EQ1,m[f ] = −1
3
Z d
c
f (x)dx + 2
L
3(2m)
1
2f0 + f2 + f4 + · · · + 1
2f2m

,
4
3EQ1,2m[f ] = 4
3
Z d
c
f (x)dx −4
L
3(2m)
1
2f0 + f1 + f2 + · · · + 1
2f2m

.

14.8 Bernoulli Numbers and Euler–Maclaurin Error Formulas
407
Adding, we have
−1
3EQ1,m[f ] + 4
3EQ1,2m[f ] =
Z d
c
f (x)dx −Q(c,d)
2,m [f ] = EQ2,m[f ].
Using the last results, we get the following Euler–Maclaurin–type formula for the
composite Simpson’s rule.
Theorem 14.42 (Euler–Maclaurin). Suppose that m, k ∈N, [c, d] ⊂R is a
compact interval of positive length, and f ∈C2k+2([c, d]). Deﬁne, as usual,
ℏ= d −c
2m .
Then
EQ2,m[f ] = −
k
X
j=2
4 −22j
3
B2jℏ2j
(2j)!
h
f (2j−1)(d) −f (2j−1)(c)
i
−4 −22k+2
3
ℏ2k+2B2k+2
(2k + 2)! (d −c)f (2k+2)(η)
for some η ∈[c, d].
Proof. See Problem 14.18.
Example 14.7
Using the Euler–Maclaurin Formula (14.25) in Theorem 14.40,
we have
EQ1,m[f ] = −h2
12 [f ′(d) −f ′(c)] + O(h4),
h = d −c
m
.
This formula prompts the deﬁnition of the corrected composite trapezoidal rule
Qcorr
1,m[f ] = Q(c,d)
1,m [f ] −h2
12 [f ′(d) −f ′(c)].
(14.26)
Clearly,
Z d
c
f (x)dx −Qcorr
1,m[f ] = O(h4).
Similarly, from Theorem 14.42, we ﬁnd
EQ2,m[f ] = −ℏ4
180
h
f (3)(d) −f (3)(c)
i
+ O(ℏ6),
ℏ= d −c
2m ,
which motivates the deﬁnition of the corrected composite Simpson’s rule
Qcorr
2,m[f ] = Q(c,d)
2,m [f ] −ℏ4
180 [f ′(d) −f ′(c)].
(14.27)
Clearly,
Z d
c
f (x)dx −Qcorr
2,m[f ] = O(ℏ6).

408
Numerical Quadrature
m
Qcorr
1,n
EQcorr
1,m
Rate
8
−12.071 929 244 529 246
1.582 928 139 612 250e−03
–
16
−12.070 445 803 590 246
9.948 720 061 281 335e−05
3.991 941
32
−12.070 352 542 937 552
6.226 547 919 041 536e−06
3.998 006
64
−12.070 346 705 682 978
3.892 933 442 273 261e−07
3.999 502
128
−12.070 346 340 722 560
2.433 292 678 460 930e−08
3.999 875
256
−12.070 346 317 910 474
1.520 840 342 550 400e−09
3.999 969
Table 14.5 Approximation of the integral (14.19) by the corrected composite
trapezoidal rule (14.26). The rate of convergence is computed according to (14.20) and
is, as predicted, 4.
m
Qcorr
2,m
EQcorr
2,m
Rate
8
−12.070 350 005 413 999
3.689 024 365 982 618e−06
–
16
−12.070 346 373 686 869
5.729 723 540 071 063e−08
6.008 629
32
−12.070 346 317 283 548
8.939 142 759 345 486e−10
6.002 185
64
−12.070 346 316 403 596
1.396 216 475 768 597e−11
6.000 541
128
−12.070 346 316 389 848
2.149 391 775 674 303e−13
6.021 450
256
−12.070 346 316 389 635
1.776 356 839 400 250e−15
6.918 863
Table 14.6 Approximation of the integral (14.19) by the corrected composite Simpson’s
rule (14.27). The rate of convergence is computed according to (14.20) and is, as
predicted, 6.
We report the approximations and errors of computing the integral (14.19) by
the corrected composite trapezoidal and Simpson’s rules in Tables 14.5 and 14.6.
Compare the results with those in Tables 14.2 and 14.3.
Example 14.8
Another application of the Euler–Maclaurin Formula (14.25) in
Theorem 14.40 is an alternate proof of Theorem 13.28, i.e., showing that, in
the numerical quadrature of periodic functions, the composite trapezoidal rule is
spectrally accurate; see Problem 14.17.
14.9
Gaussian Quadrature Rules
Gaussian quadrature rules are based on certain polynomial orthogonality relations.
We will use the following simple property as we develop these methods.
Proposition 14.43 (orthogonality). Let [a, b] ⊂R be a compact interval and
suppose that w is a weight function on [a, b]. Suppose that {pm}∞
m=0, where pm ∈
Pm with deg pm = m, is a system of orthogonal polynomials on [a, b] with respect
to the weight function w. Then, for any k ∈N,

14.9 Gaussian Quadrature Rules
409
Z b
a
pk(x)q(x)w(x)dx = 0,
∀q ∈Pk−1.
Proof. See Problem 14.19.
The existence of orthogonal polynomials allows us to construct so-called
Gaussian quadrature rules.
Deﬁnition 14.44 (Gaussian rule). Let n ∈N0 and pn+1 ∈Pn+1 be an orthogonal
polynomial of order n + 1 with respect to the weight function w on the compact
interval [a, b] ⊂R. Then the simple quadrature rule (14.4) is called a Gaussian
quadrature rule8 of degree n if and only if:
1. The eﬀective nodes X = {xj}n
j=0 are the zeros of pn+1 in (a, b). This choice is
possible because of Theorem 11.10.
2. The eﬀective weights {βj}n
j=0 are chosen to satisfy the exactness condition
(14.7), i.e.,
βj =
Z b
a
Lj(x)w(x)dx,
Lj(x) =
n
Y
k=0
k̸=j
x −xk
xj −xk
,
j = 0, . . . , n.
Gaussian quadrature rules possess a higher order of consistency, as the following
result shows.
Theorem 14.45 (order of Gaussian rules). A Gaussian quadrature rule of degree
n ∈N (n + 1 nodes) is consistent of order at least 2n + 1.
Proof. Let pn+1 ∈Pn+1 be an orthogonal polynomial of degree n + 1 with respect
to the weight function w on the compact interval [a, b]. Suppose that ˆp ∈P2n+1
is arbitrary. Since 2n + 1 ≥n + 1, there are polynomials q, r ∈Pn such that
ˆp = pn+1q + r.
If that is the case, then we have
Z b
a
ˆp(x)w(x)dx =
Z b
a
pn+1(x)q(x)w(x)dx +
Z b
a
r(x)w(x)dx
=
Z b
a
r(x)w(x)dx,
where we used the orthogonality of pn+1. In addition, by the choice of the nodes,
n
X
j=0
βj ˆp(xj) =
n
X
j=0
βjpn+1(xj)q(xj) +
n
X
j=0
βjr(xj) =
n
X
j=0
βjr(xj).
8 Named in honor of the German mathematician and physicist Johann Carl Friedrich Gauss
(1777–1855).

410
Numerical Quadrature
Finally, since r ∈Pn, the choice of the weights guarantees that
n
X
j=0
βjr(xj) =
Z b
a
r(x)w(x)dx.
Putting everything together, we have
Z b
a
ˆp(x)w(x)dx =
n
X
j=0
βj ˆp(xj)
for any ˆp ∈P2n+1, which shows that this formula is consistent of order 2n +1.
This is remarkable. Remember that we are using only n + 1 nodes to construct
the method. One might only expect that the method will be consistent of order n.
But our nodes are special; this is the key point. Consequently, we obtain a method
that is actually of order 2n + 1. It turns out that one cannot do better than this;
see Theorem 14.46.
Theorem 14.46 (impossibility). Let w be a weight function on the compact interval
[a, b] ⊂R. There is no simple quadrature rule of Lagrange type constructed with
n + 1 nodes in [a, b] that is consistent of order 2n + 2 or higher. Consequently, a
Gaussian quadrature rule of degree n is consistent to exactly order 2n + 1.
Proof. Suppose that there is a simple quadrature rule (14.4) constructed with n+1
nodes, {xj}n
j=0 ⊂[a, b], that is consistent of order at least 2n + 2. This rule would
be exact for any polynomial of degree not exceeding 2n + 2. Deﬁne
ˆp(x) =
n
Y
j=0
(x −xj)2 ∈P2n+2.
Then since ˆp is strictly positive, except at the nodes where it is zero, we have
Z b
a
ˆp(x)w(x)dx > 0.
But since ˆp(xj) = 0, for all j = 0, . . . , n,
n
X
j=1
βj ˆp(xj) = 0.
Consequently,
0 =
n
X
j=1
βj ˆp(xj) ̸=
Z b
a
ˆp(x)w(x)dx > 0,
which is a contradiction to our assumption that the rule is consistent of order at
least 2n + 2.
Modifying the previous arguments slightly, we can also prove the following.
Corollary 14.47 (improved order). Suppose that w is a weight function on the
compact interval [a, b] ⊂R. Let r ∈Pn+1 be such that

14.9 Gaussian Quadrature Rules
411
1. It has n + 1 real, distinct roots in [a, b].
2. There is an integer m ∈{1, . . . , n, n + 1} for which
(r, q)L2w (a,b) = 0,
∀q ∈Pm−1,
but there is ˜q ∈Pm for which
(r, ˜q)L2w (a,b) ̸= 0.
Let the nodes {xj}n
j=0 ⊂[a, b] be chosen as the roots of r and the weights {βj}n
j=0
be chosen to satisfy the exactness condition (14.7). Then the resulting simple
quadrature rule (14.4) is consistent to exactly order n + m.
Proof. See Problem 14.20.
The proof of Theorem 14.48 makes an interesting connection between Gaussian
quadrature and Hermite interpolation.
Theorem 14.48 (error representation). Suppose that w is a weight function on
the compact interval [a, b] ⊂R and (14.4) is a Gaussian quadrature of degree
n ∈N0, i.e., it has n + 1 nodes. Assume that f ∈C2n+2([a, b]). Then there exists
a point ζ ∈(a, b) such that
EQ[f ] = f (2n+2)(ζ)
(2n + 2)!
Z b
a
w(x) (ωn+1(x))2 dx,
where, as usual,
ωn+1(x) =
n
Y
i=0
(x −xi)
and {xi}n
i=0 ⊂[a, b] are the nodes for the quadrature rule.
Proof. Suppose that p ∈P2n+1 is the Hermite interpolating polynomial with respect
to the nodes {xi}n
i=0 ⊂[a, b]. This is the unique polynomial in P2n+1 with the
property that
f (xj) = p(xj),
f ′(xj) = p′(xj),
j = 0, . . . , n.
Recall that, to construct p, we deﬁne, for 0 ≤ℓ≤n, the following polynomials
from P2n+1:
F0,ℓ(x) = (Lℓ(x))2 (1 −2L′
ℓ(xℓ)(x −xℓ)) ,
F1,ℓ= (Lℓ(x))2(x −xℓ),
where Lℓis the Lagrange basis element (9.3). Then the Hermite interpolating
polynomial is
p(x) =
n
X
ℓ=0
[F0,ℓ(x)f (xℓ) + F1,ℓ(x)f ′(xℓ)].

412
Numerical Quadrature
By consistency,
Z b
a
w(x)p(x)dx =
n
X
j=0
βjp(xj)
=
n
X
j=0
βj
 n
X
ℓ=0
[F0,ℓ(xj)f (xℓ) + F1,ℓ(xj)f ′(xℓ)]
!
=
n
X
ℓ=0
Aℓf (xℓ) +
n
X
ℓ=0
Bℓf ′(xℓ),
where
Aℓ=
n
X
j=0
βjF0,ℓ(xj),
Bℓ=
n
X
j=0
βjF1,ℓ(xj),
j = 0, . . . , n.
Now we prove the following remarkable facts:
Aℓ= βℓ,
Bℓ= 0,
ℓ= 0, . . . , n.
To see that Bℓ= 0, note that, again by consistency, and using the deﬁnition of Lℓ,
Bℓ=
n
X
j=0
βjF1,ℓ(xj) =
Z b
a
w(x)F1,ℓ(x)dx
=
Z b
a
w(x)Lℓ(x)(x −xℓ)Lℓ(x)dx
= Cn
Z b
a
w(x)ωn+1(x)Lℓ(x)dx,
where Cn is a constant and ωn+1 ∈Pn+1 is an orthogonal polynomial of degree
n + 1, since the xj were chosen as the roots of such an orthogonal polynomial.
Since Lℓ∈Pn,
Z b
a
w(x)ωn+1(x)Lℓ(x)dx = 0,
ℓ= 0, . . . , n.
Thus, Bℓ= 0 for ℓ= 0, . . . , n. It is not hard to see that Aℓ= βℓfor ℓ= 0, . . . , n,
and we leave that to the reader as a short exercise; see Problem 14.21.
Regarding Hermite interpolation error, Theorem 9.21 guarantees that, for every
x ∈[a, b]\X, there is a point ξ = ξ(x) ∈(a, b) such that
f (x) −p(x) = f (2n+2)(ξ(x))
(2n + 2)!
(ωn+1(x))2.
(14.28)
Alternately according to Theorem 9.58, for any x ∈[a, b],
f (x) −p(x) = (ωn+1(x))2 f Jx0, x0, x1, x1, . . . , xn, xn, xK.
(14.29)
Of course, comparing (14.28) and (14.29), we see that
f Jx0, x0, x1, x1, . . . , xn, xn, xK = f (2n+2)(ξ(x))
(2n + 2)!
.

14.9 Gaussian Quadrature Rules
413
The extended divided diﬀerence function f Jx0, x0, x1, x1, . . . , xn, xn, xK is a contin-
uous function of its arguments, speciﬁcally of x. Using what we have just learned,
EQ[f ] =
Z b
a
w(x)f (x)dx −
n
X
j=0
βjf (xj)
=
Z b
a
w(x)f (x)dx −
n
X
j=0
βjp(xj)
=
Z b
a
w(x) (f (x) −p(x))dx
=
Z b
a
w(x)f Jx0, x0, x1, x1, . . . , xn, xn, xK(ωn+1(x))2 dx.
Using the Integral Mean Value Theorem B.41, there is a point η ∈(a, b) such that
EQ[f ] = f Jx0, x0, x1, x1, . . . , xn, xn, ηK
Z b
a
w(x) (ωn+1(x))2dx
= f (2n+2)(ξ(η))
(2n + 2)!
Z b
a
w(x) (ωn+1(x))2dx
= f (2n+2)(ζ)
(2n + 2)!
Z b
a
w(x) (ωn+1(x))2dx.
To ﬁnish this section and chapter, let us state an interesting property of Gaussian
quadratures.
Proposition 14.49 (positivity). Suppose that w is a weight function on the
compact interval [a, b] ⊂R and (14.4) is the Gaussian quadrature rule of degree
n ∈N0 with respect to w. Then each of the weights is positive, i.e.,
βj > 0,
j = 0, . . . , n.
Proof. Recall that a Gaussian quadrature of degree n has n + 1 nodes and is
consistent of exactly order 2n + 1. Therefore, the rule will be exact for the
polynomials qj ∈P2n, deﬁned by
qj(x) = ω2
n+1(x)
(x −xj)2 ,
j = 0, . . . , n,
where, as usual,
ωn+1(x) = (x −x0)(x −x1) · · · (x −xn).
Thus,
Z b
a
qj(x)w(x)dx =
n
X
k=0
βkqj(xk).

414
Numerical Quadrature
But notice that
qj(xk) =









0,
k ̸= j,
n
Y
i=0
i̸=j
(xj −xi)2 > 0,
k = j.
Therefore, for j = 0, . . . , n,
βj =
n
X
k=0
βkδj,k
=
n
X
k=0
βk
qj(xk)
qj(xj)
=
1
qj(xj)
Z b
a
qj(x)w(x)dx
=
1
qj(xj)
Z b
a
n
Y
i=0
i̸=j
(x −xi)2w(x)dx > 0.
Problems
14.1
Show that if a quadrature rule on [a, b] for the weight w ≡1 is consistent
of order at least one, then
n
X
j=1
βj = b −a.
What does one obtain for a general weight?
14.2
State and prove an analogue of Proposition 14.4 for quadrature rules of
Hermite type.
14.3
Prove Theorem 14.7.
14.4
Prove Lemma 14.9.
14.5
Prove Corollary 14.12.
14.6
Prove Lemma 14.15.
14.7
Conﬁrm the intrinsic weights for the Newton–Cotes rules listed in Table
14.1.
14.8
Prove Lemma 14.21.
14.9
Prove Lemma 14.22.
14.10
The closed Newton–Cotes quadrature rule of order n is designed with n+1
equally spaced nodes such that, when applied to any polynomial of degree at most
n, there is no quadrature error. Show directly that, when n is even, there is no
quadrature error for polynomials in Pn+1.
14.11
Complete the proof of Theorem 14.31.
14.12
Complete the proof of Proposition 14.34.
14.13
Prove Proposition 14.37.
14.14
Prove Proposition 14.38.
14.15
Complete the proof of Theorem 14.40.

Problems
415
14.16
Derive an Euler–Maclaurin–type error formula for the composite midpoint
rule (14.18).
14.17
Use Theorem 14.40 to give an alternate proof of Theorem 13.28.
14.18
Prove Theorem 14.42.
14.19
Prove Proposition 14.43.
14.20
Prove Corollary 14.47.
14.21
Complete the details of the proof of Theorem 14.48 by proving the
following result. Let [a, b] ⊂R be a compact interval, w be a weight on [a, b], and
f ∈C1([a, b]). Use the Hermite interpolating polynomial to design a quadrature
method for approximating
Z b
a
f (x)w(x)dx
using n+1 nodes that is consistent of order 2n+1. Show that one obtains precisely
the Gaussian quadrature rule of order n.
14.22
Deﬁne the sequence of functions {qm}∞
m=1 as follows.
a)
q1(x) = −x.
b)
q′
m+1 = qm, for each m ∈N.
c)
qm is an odd function if m is odd.
d)
qm is an even function if m is even.
e)
if m > 1 is odd, qm(−1) = 0 and qm(1) = 0.
Prove that the sequence {qm}∞
m=1 is well deﬁned and, in particular, that qm ∈Pm,
for each m ∈N0. Show that the ﬁrst ﬁve members are
q1(x) = −x;
q2(x) = −1
2x2 + 1
6;
q3(x) = −1
6x3 + 1
6x;
q4(x) = −1
24x4 + 1
12x2 −
7
360;
q5(x) = −1
120x5 + 1
36x3 −
7
360x.
14.23
Suppose that, for k ∈N, f ∈C2k([−1, 1]). Denote
E =
Z 1
−1
f (x)dx −[f (−1) + f (1)].
Show that
E =
Z 1
−1
q1(x)f (x)dx,
(14.30)
E =
k
X
m=1
q2m(1)
h
f (2m−1)(1) −f (2m−1)(−1)
i
−
Z 1
−1
q2k(x)f (2k)(x)dx,
(14.31)
where {qm}∞
m=1 is as in Problem 14.22.

416
Numerical Quadrature
14.24
Use the previous two problems to give an alternate proof of the Euler–
Maclaurin Formula of Theorem 14.40.
14.25
Use Hermite interpolation to derive the corrected trapezoidal rule,
Q[f ] = b −a
2
(f (a) + f (b)) −(b −a)2
12
(f ′(b) −f ′(a)) ,
∀f ∈C1([a, b]).
Derive the corrected composite trapezoidal rule using this result and show that
the error for this rule (in approximating
R d
c f (x)dx) is exactly as predicted by the
Euler–Maclaurin Formula, provided that f is suﬃciently smooth on [c, d].
14.26
Derive a composite form of Boole’s rule and obtain an error estimate
analogous to those derived in Theorem 14.31; see Table 14.1.
14.27
Recall that
(f , g)L2(−1,1;C) =
Z 1
−1
f (x)g(x) dx,
∀f , g ∈L2(−1, 1; C).
Suppose that the elements of X = {xi}n
i=0 are chosen as the roots of the (n + 1)st
Legendre polynomial Pn+1 ∈Pn+1; see Deﬁnition 11.17. Deﬁne
βj =
Z 1
−1
Lj(x)dx,
where Lj ∈Pn is the jth Lagrange nodal basis polynomial subordinate to X. Prove
that
(p, q)L2(−1,1;C) =
n
X
j=0
βjp(xj)q(xj),
∀p, q ∈Pn(C).

Part III
Nonlinear Equations and
Optimization


15
Solution of Nonlinear Equations
In this chapter, we depart from linear algebra problems and concentrate on the
study of nonlinear problems. We will focus on methods for solving a nonlinear
system of equations. In other words, given m, n ∈N and f : Rn →Rm, we wish to
ﬁnd a point ξ ∈Rn such that
f (ξ) = 0.
(15.1)
Such a point ξ, if it exists, is called a root of f . Of course, if m = n and f so
happens to be aﬃne then this problem reduces to (3.1), and can be treated either
by the direct methods of Chapter 3 or by the iterative ones of Chapters 6 and 7.
If m ̸= n, but the function f is still aﬃne, then the least squares methods of
Chapter 5 apply. The function f in this chapter, however, is not assumed to be
aﬃne. The importance of (15.1) cannot be overstated. Similar to (3.1), many
problems can be reduced to this. We already saw an instance of this in Chapter 8:
although this may not be the preferred choice, ﬁnding eigenvalues can be reduced
to solving a problem like (15.1) with m = n = 1 and the function is a polynomial.
Other examples will be seen in future chapters.
We must immediately remark that any method that attempts to ﬁnd a solution
to (15.1) must be iterative, unless a very special structure is assumed on the
function f . The general strategy that we will follow can be simply stated as:
• Show that the problem has at least one solution.
• Isolate a root, i.e., ﬁnd an open region D ⊂Rn for which there is ξ ∈D that
solves (15.1) and f (x) ̸= 0 for all x ∈D\{ξ}.
• Iterate.
Unfortunately, there is no general strategy to treat the ﬁrst two points, as these
usually require analytical methods or additional knowledge about the problem at
hand.
Before we begin, we must give a word of caution regarding iterations. Much as in
Chapter 6, starting from some x0 ∈Rn, we will construct sequences {xk}∞
k=1 ⊂Rn
which, hopefully, converge xk →ξ as fast as possible. We will, as usual, stop the
iteration when a prescribed tolerance ε > 0 is reached, i.e.,
∥xk −ξ∥< ε.
How do we know when to stop the iterations? One might be tempted to say that,
since f (ξ) = 0, we can stop them whenever
∥f (xk)∥< Cε

420
Solution of Nonlinear Equations
for some suitable constant C. The following two examples, however, show that this
is not always a viable strategy.
Example 15.1
The function f (x) = ex does not have a root in R. However, for
any ε > 0, there is xε such that
0 < f (xε) = exε < ε.
Example 15.2
The previous example may be misleading in the sense that the
problem did not have a solution, and we were considering a function deﬁned on an
unbounded interval. This can be easily ﬁxed. Let δ ∈(0, 1) and consider
fδ(x) =







δ,
x ∈[0, 1
2),
8(1 −δ)(x −1
2) + δ,
x ∈[ 1
2, 5
8),
−16
3 (x −5
8) + 1,
x ∈[ 5
8, 1].
This function is continuous and it has a unique root ξ = 13
16 > 1
2. However, no
matter what ε > 0 is, we can choose δ < ε, so that any point x ∈[0, 1
2] satisﬁes
0 < fδ(x) < ε.
Let us now quickly and not very rigorously discuss the sensitivity of this problem
to perturbations. To do so, we will assume that m = n = 1, and the function f is
k + 1 times continuously diﬀerentiable in a neighborhood of its unique root ξ ∈R.
Moreover, we will assume that f (p)(ξ) = 0 for p < k, but f (k)(ξ) ̸= 0. Then, given
a perturbation δx, we let η be such that
f (ξ + δx) = η.
Taylor’s Theorem then shows that
η = f (ξ + δx)
= f (ξ) + f ′(ξ)δx + 1
2f ′′(ξ)δx2 + · · · + 1
k!f (k)(ξ)δxk + O(|δx|k+1)
= 1
k!f (k)(ξ)δxk + O(|δx|k+1).
In other words, at least intuitively, the allowed relative size of the perturbation δx,
to obtain an output of size η, is

δx
η
 ≈
η1−k
k!
f (k)(ξ)

1/k
.
From this, we learn two things: the smaller the value of the ﬁrst nonzero derivative,
the larger δx can be; and, the higher the order of the ﬁrst nonzero derivative, the
larger δx can be. For this reason, of importance to us will be so-called simple roots.
Deﬁnition 15.1 (simple root). Let f : R →R have a root ξ ∈R, and assume that
f is diﬀerentiable at ξ. We say that ξ is a simple root if f ′(ξ) ̸= 0. If this is not
the case, we say that the root is nonsimple.

15.1 Methods of Bisection and False Position
421
In this chapter, we will ﬁrst present simple methods to tackle the case m =
n = 1. As a rule, their convergence will be no better than linear (see Appendix
B for deﬁnitions). We will then move on to Newton’s method, for which we will
show quadratic convergence. This method, and some of its variants, will be ﬁrst
presented in the one-dimensional case, and then for the multidimensional case, i.e.,
m = n = d > 1.
As always, we are barely scratching the surface of the subject. We refer the
reader, for instance, to [28, 51, 68] for many more details.
15.1
Methods of Bisection and False Position
Here, we consider the case m = n = 1 and f ∈C([a, b]) for some −∞< a <
b < ∞. The following result is a simple consequence of the Intermediate Value
Theorem B.27.
Corollary 15.2 (existence). Let −∞< a < b < ∞and f ∈C([a, b]). If f (a)f (b) <
0, then f has a (not necessarily unique) root in ξ ∈(a, b).
Proof. From the condition f (a)f (b) < 0 we infer that
inf
x∈[a,b] f (x) ≤min{f (a), f (b)} < 0
and
sup
x∈[a,b]
f (x) ≥max{f (a), f (b)} > 0.
The Intermediate Value Theorem B.27 then implies the result.
The idea of the bisection method, as its name suggests, is that we will
successively subdivide the interval [a, b] into two subintervals of equal length and
check on which of the subintervals there must be a root.
Deﬁnition 15.3 (bisection method). Let −∞< a < b < ∞and f ∈C([a, b]).
Assume that f (a)f (b) < 0. The bisection method is an algorithm for generating
an approximation sequence, {xk}∞
k=0 ⊂[a, b], via the following recursive procedure.
Deﬁne a0 = a, b0 = b. For k ≥0, set
xk = 1
2(ak + bk).
If f (xk) = 0, the algorithm terminates. Otherwise,
(ak+1, bk+1) =
(
(xk, bk),
if f (xk)f (bk) < 0,
(ak, xk),
if f (xk)f (bk) > 0.
The convergence properties of this method are stated in the following result.
Theorem 15.4 (convergence). Let −∞< a < b < ∞and f ∈C([a, b]) be
such that f (a)f (b) < 0. Then the sequence {xk}∞
k=0 generated by the bisection
method converges to a point ξ ∈[a, b] such that f (ξ) = 0. Moreover, this method
converges linearly, with the following rate of convergence:
|xk −ξ| ≤
1
2k+1 (b −a).

422
Solution of Nonlinear Equations
Proof. The bisection method generates the sequences {ak}∞
k=0, {bk}∞
k=0, {xk}∞
k=0,
which satisfy
xk ∈(ak, bk),
[ak+1, bk+1] ⊊[ak, bk],
bk+1 −ak+1 = 1
2(bk −ak).
Observe that {ak}∞
k=0 is a bounded increasing sequence, and {bk}∞
k=0 is a bounded
decreasing sequence. By the Monotone Convergence Theorem (Theorem B.7),
there exist limit points ξa, ξb ∈[a, b] such that
an ↑ξa ≤ξb ↓bn.
But
bn −an = 1
2n (b0 −a0) ↓0,
which implies that ξa = ξb. Let us call the common point ξ. By the Squeeze
Theorem B.6, since xk ∈(ak, bk), we have also that xk →ξ. Moreover, since xk is
the midpoint of each interval, we observe that
|xk −ξ| ≤1
2(bk −ak) = · · · =
1
2k+1 (b0 −a0) =
1
2k+1 (b −a).
It remains then to show that f (ξ) = 0, and this follows from the continuity of f ,
since
0 ≤f (ξ)2 = lim
k→∞f (ak) lim
k→∞f (bk) = lim
k→∞f (ak)f (bk) ≤0.
The bisection method requires very little from the function at hand, just
continuity and that it takes values of diﬀerent signs on the given interval. In this
setting, it is always guaranteed to converge. The convergence, however, is only
linear. Let us present a small modiﬁcation, known as the false position method,
which may improve the rate of convergence. The idea is simple. There is no reason
why, in the bisection method, the approximation of the root must be the midpoint.
Instead, suppose the update is chosen to be the zero of the line that connects
(ak, f (ak)) and (bk, f (bk)).
Deﬁnition 15.5 (false position). Let −∞< a < b < ∞and f ∈C([a, b]). The
sequence {xk}∞
k=0 ⊂[a, b] obtained by the following procedure deﬁnes the false
position method. Deﬁne a0 = a, b0 = b. For k ≥0, set
xk = akf (bk) −bkf (ak)
f (bk) −f (ak)
.
If f (xk) = 0, the algorithm terminates. Otherwise,
(ak+1, bk+1) =
(
(xk, bk),
if f (xk)f (bk) < 0,
(ak, xk),
if f (xk)f (bk) > 0.
The convergence of this method easily follows from that of the bisection method.
Theorem 15.6 (convergence). Let −∞< a < b < ∞and f ∈C([a, b]) be such
that f (a)f (b) < 0. Then the sequence {xk}∞
k=0 generated by the false position
method converges to a point ξ ∈[a, b] such that f (ξ) = 0.

15.2 Fixed Points and Contraction Mappings
423
Proof. The details of the proof are left to the reader as an exercise; see Problem
15.2. Here, we merely sketch why xk ∈[ak, bk]. Consider,
xk −ak = −f (ak)
bk −ak
f (bk) −f (ak).
Now if f (ak) > 0, then the numerator of this expression is negative. If this is the
case, by construction, f (bk) < 0 and the denominator of the expression above is
negative. Thus, xk −ak > 0. The remaining cases are treated similarly.
From this, it follows that we are constructing, again, a sequence {[ak, bk]}∞
k=0
of nested intervals whose lengths are strictly decreasing.
We will not provide an analysis of the false position method. We will just mention
that, in general, it is possible (Problem 15.3) for one of the endpoints of the interval
to “get stuck,” i.e., we can have, for all k ≥0,
ak = a0 = a
or
bk = b0 = b.
Because of this, the rate of convergence of this method is no better than linear. In
practice, however, this method seems to perform better than bisection. A partial
explanation for this fact will be given in the analysis of the secant method provided
in Section 15.4.4.
15.2
Fixed Points and Contraction Mappings
In this section, we will relate root ﬁnding to contraction mappings and ﬁxed point
iteration schemes. The reader should examine Appendix C and Theorem C.4 for
the more general setting.
Deﬁnition 15.7 (ﬁxed point iteration). Suppose that −∞< a < b < ∞, g ∈
C([a, b]), and g(x) ∈[a, b] for all x ∈[a, b]. In other words, g([a, b]) ⊆[a, b].
Given x0 ∈[a, b], the algorithm for constructing the recursive sequence {xk}∞
k=0 via
xk+1 = g(xk),
k ≥0
(15.2)
is called a simple iteration scheme or, sometimes, a ﬁxed point iteration scheme.
The following fact follows easily from continuity.
Proposition 15.8 (ﬁxed point). Suppose that −∞< a < b < ∞, g ∈C([a, b]),
and g([a, b]) ⊆[a, b]. Assume that the sequence {xk}∞
k=0 obtained by a simple
iteration scheme converges to a limit ξ ∈[a, b]. Then ξ is a ﬁxed point of g, i.e.,
g(ξ) = ξ.
Proof. Indeed, by continuity,
g(ξ) = g

lim
k→∞xk

= lim
k→∞g(xk) = lim
k→∞xk+1 = ξ.
The following result provides suﬃcient conditions for the existence of a ﬁxed point.
Theorem 15.9 (existence). Suppose that −∞< a < b < ∞, g ∈C([a, b]), and
g([a, b]) ⊆[a, b]. Then there exists at least one ﬁxed point ξ ∈[a, b] of g.

424
Solution of Nonlinear Equations
Proof. Deﬁne
f (x) = x −g(x),
∀x ∈[a, b].
Then, since g([a, b]) ⊆[a, b],
f (b) = b −g(b) ≥0
and
f (a) = a −g(a) ≤0.
By the Intermediate Value Theorem B.27, since f (a) ≤0 ≤f (b), there is a point
ξ ∈[a, b] such that f (ξ) = 0. For this point, ξ = g(ξ).
In practice, it is very diﬃcult to verify the condition that g([a, b]) ⊆[a, b]. The
following deﬁnition provides a suﬃciently large class of functions for which this
condition is almost automatically satisﬁed.
Deﬁnition 15.10 (contraction). Suppose that −∞< a < b < ∞and g ∈
C([a, b]). We say that g is Lipschitz continuous1 on [a, b] if and only if there
exists a constant L > 0 such that
|g(x) −g(y)| ≤L|x −y|,
∀x, y ∈[a, b],
and the associated L is called the Lipschitz constant. The function g is called
a contraction on [a, b] if and only if it is Lipschitz on [a, b] and its associated
Lipschitz constant, L, satisﬁes L ∈(0, 1).
Proposition 15.11 (translation). Let −∞< a < b < ∞and g ∈C([a, b])
be a contraction on [a, b]. There is a constant m ∈R such that the function
˜g : [a, b] →R, where
˜g(x) = g(x) + m,
∀x ∈[a, b]
is a contraction on [a, b]; moreover, ˜g([a, b]) ⊆[a, b].
Proof. See Problem 15.5.
We see also that, although Theorem 15.9 provides existence of ﬁxed points,
it says nothing about uniqueness. It turns out that, for contractions, ﬁxed points
must be unique.
Theorem 15.12 (uniqueness). Suppose that −∞< a < b < ∞, g ∈C([a, b]),
and g([a, b]) ⊆[a, b]. If g is a contraction on [a, b], then g has a unique ﬁxed point
ξ ∈[a, b]. Furthermore, the sequence {xk}∞
k=0 generated by (15.2) converges to ξ
for any starting value x0 ∈[a, b].
Proof. Theorem 15.9 guarantees the existence of at least one ﬁxed point ξ ∈[a, b].
Suppose that η ∈[a, b] is another ﬁxed point. Since g is a contraction,
|ξ −η| = |g(ξ) −g(η)| ≤L|ξ −η|.
Therefore,
0 ≤(1 −L)|ξ −η| ≤0,
which proves that ξ = η. Hence, the ﬁxed point ξ ∈[a, b] is unique.
1 Named in honor of the German mathematician Rudolf Otto Sigismund Lipschitz
(1832–1903).

15.2 Fixed Points and Contraction Mappings
425
Now suppose that {xk}∞
k=0 is generated by (15.2) for x0 ∈[a, b]. Then
|ξ −xk| = |g(ξ) −g(xk−1)| ≤L|ξ −xk−1|.
By induction, for any k ∈N,
|ξ −xk| ≤Lk|ξ −x0|.
By the Squeeze Theorem B.6, since L ∈(0, 1), we must have xk →ξ.
Theorem 15.13 (local [at least linear] convergence). Suppose that −∞< a <
b < ∞, g ∈C([a, b]), and g([a, b]) ⊆[a, b]. Let ξ ∈[a, b] be a ﬁxed point of g, i.e.,
ξ = g(ξ). Suppose that there is a constant δ > 0 such that Iδ = (ξ −δ, ξ + δ) ⊂
[a, b], g ∈C1(Iδ), and |g′(ξ)| < 1. Then the sequence {xk}∞
k=0 generated by (15.2)
converges at least linearly to ξ, provided that x0 is suﬃciently close to ξ.
Proof. Suppose that ξ ∈(a, b), i.e., ξ is in the interior of the set [a, b]. The reader
can examine the other cases. Since |g′(ξ)| < 1, there is an h ∈(0, δ) and an
L ∈(0, 1) such that, for all x ∈Ih = (ξ −h, ξ + h),
|g′(x)| ≤L < 1.
The proof of this last fact is left to the reader as an exercise; see Problem 15.6.
Suppose that xk ∈Ih. Then, using the Mean Value Theorem B.30,
|ξ −xk+1| = |g(ξ) −g(xk)| = |g′(ηk)| · |ξ −xk| ≤L|ξ −xk|
for some ηk ∈Ih between ξ and xk. This proves that if xk ∈Ih, then xk+1 ∈Ih.
Using induction, if x0 ∈Ih,
|ξ −xk| ≤Lk|ξ −x0|.
By the Squeeze Theorem, since Lk →0 as k →∞, we have xk →ξ as k →∞.
Furthermore, the convergence is at least linear; see Deﬁnition B.10.
We will now consider how simple ﬁxed point iterations can be used to ﬁnd roots.
A general strategy is to ﬁnd some function α that does not vanish and to consider
a ﬁxed point iteration scheme for
g(x) = x −α(x)f (x).
The particular choice of α gives rise to the various methods we now consider.
15.2.1
Relaxation Method
Deﬁnition 15.14 (relaxation). Let I ⊆R be an interval, f ∈C(I), and x0 ∈I
be given. The relaxation method is an algorithm for computing the terms of the
sequence {xk}∞
k=0 via the recursive formula
xk+1 = xk −λf (xk),
(15.3)
where λ ̸= 0. The method is well deﬁned if and only if xk ∈I for all k = 1, 2,
3, . . ., and the relaxation method converges if and only if there is a ξ ∈I, with
f (ξ) = 0, such that xk →ξ as k →∞.

426
Solution of Nonlinear Equations
Notice that the relaxation method is a simple ﬁxed point iteration scheme with
g(x) = x −λf (x). From this deﬁnition, it necessarily follows that a ﬁxed point of
g must be a root of f . Thus, the following result is just a translation of the theory
of ﬁxed point iterations.
Theorem 15.15 (convergence). Let I ⊂R be an interval. Suppose that f : I →R
and, for some ξ ∈I, f (ξ) = 0, but f ′(ξ) ̸= 0. Assume that, for some δ > 0,
f ∈C1(Iδ), where Iδ = [ξ −δ, ξ + δ] ⊆I. Then there exists positive real numbers
λ and h ∈(0, δ) such that the sequence {xk}∞
k=0 deﬁned by the relaxation scheme
(15.3) converges to ξ for any x0 ∈Ih = [ξ −h, ξ + h].
Proof. Suppose that f ′(ξ) = α > 0. The case α < 0 is analogous and left to the
reader. By continuity, we may assume that
0 < 1
2α ≤f ′(x) ≤3
2α,
∀x ∈Iδ.
If this is not the case, we can just choose a smaller δ and redeﬁne Iδ. Set
M = max
x∈Iδ f ′(x).
Thus, 1
2α ≤M ≤3
2α. For any λ > 0, it follows that
1 −λM ≤1 −λf ′(x) ≤1 −1
2λα,
∀x ∈Iδ.
We now choose, if possible, λ > 0 such that
1 −λM = −θ
and
1 −1
2λα = θ.
These equations are satisﬁed if and only if
λM −1 = 1 −1
2λα,
θ = 1 −1
2λα
if and only if
λ =
4
2M + α,
θ = 2M −α
2M + α.
Now deﬁne the iteration function g via
g(x) = x −λf (x) = x −
4f (x)
2M + α.
The rest of the details are left to the reader as an exercise; see Problem 15.7.
Use Theorem 15.13 to conclude that xk →ξ, provided that x0 is suﬃciently
close to ξ.
15.2.2
Stationary Slope Approximation Methods
Let I ⊂R be an interval and f ∈C1(I). Assume that there is ξ ∈I, which is a
simple root of f . Then, by Taylor’s Theorem B.31, we obtain that, for any x ∈I,
0 = f (ξ) = f (x) + f ′(θ)(ξ −x)

15.2 Fixed Points and Contraction Mappings
427
for some θ between x and ξ. Thus, if f ′(θ) ̸= 0, the root must satisfy
ξ = x −[f ′(θ)]−1f (x).
This motivates the construction of a family of schemes. Let s0 ̸= 0 be a slope
approximation and x0 ∈I an initial guess. Then, for k ≥0,
xk+1 = xk −s−1
k f (xk),
(15.4)
with some rule to compute sk+1 ̸= 0. Notice that if sk = s0 =
1
λ, then this
reduces to the relaxation method of Deﬁnition 15.14. Let us consider two particular
examples.
Deﬁnition 15.16 (chord method). Let I = [a, b] be an interval and x0 ∈I. The
sequence {xk}∞
k=0 is obtained by the chord method if is constructed via (15.4)
with
sk = f (b) −f (a)
b −a
.
Theorem 15.17 (convergence). Let I = [a, b] ⊂R be an interval and f
∈
C1([a, b]) is such that it has a unique simple root ξ ∈[a, b]. If b −a is suﬃciently
small, then the sequence {xk}∞
k=1 obtained by the chord method of Deﬁnition 15.16
converges linearly to ξ as k →∞.
Proof. See Problem 15.8.
Deﬁnition 15.18 (simpliﬁed Newton). Let I be an interval and x0 ∈I be such that
f ′(x0) ̸= 0. The sequence {xk}∞
k=0 is obtained by the simpliﬁed Newton method2
if it is constructed via (15.4) with
sk = f ′(x0).
This method can be analyzed in two diﬀerent ways. Here, we analyze it using
the theory of ﬁxed point iterations.
Proposition 15.19 (convergence). Let I ⊂R be an interval and f ∈C(I) be such
that there is ξ ∈I for which f (ξ) = 0. Deﬁne, for δ > 0, Iδ = [ξ −δ, ξ + δ] ⊆I.
Assume that there is δ > 0 such that f ∈C1(Iδ). Finally, assume that f ′(ξ) = α >
0. If x0 is suﬃciently close to ξ, then the simpliﬁed Newton method of Deﬁnition
15.18 converges linearly to ξ.
Proof. See Problem 15.9.
15.2.3
Fixed Point Iterations in Several Dimensions
Let us quickly comment that the idea of ﬁxed point iterations can be generalized
to several dimensions. For this, we consider a closed ball B ⊂Rd and g : B →Rd.
The ﬁxed point iteration scheme, in this setting, starts from x0 ∈B and proceeds,
for k ≥0, as
xk+1 = g(xk).
2 Named in honor of the English mathematician, physicist, astronomer, theologian, and natural
philosopher Sir Isaac Newton (1643–1726/27).

428
Solution of Nonlinear Equations
As in the one-dimensional case, we say that the ﬁxed point iteration scheme is
well deﬁned if xk ∈B for all k. The convergence theory of this approach and its
application to the solution of systems of nonlinear equations follow the same ideas
we have presented here. We leave the details to the reader.
15.3
Newton’s Method in One Space Dimension
We have now arrived at our preferred method of choice: Newton’s method.
Throughout our discussion, we will assume that the function f , for which we are
trying to ﬁnd a root, is at least twice continuously diﬀerentiable. One can prove
these results using less regularity than this, but this assumption will greatly simplify
our arguments.
Deﬁnition 15.20 (Newton’s method3). Let I ⊆R be an interval, f ∈C1(I), and
x0 ∈I, with f ′(x0) ̸= 0, be given. Newton’s method is an algorithm for computing
the terms of the sequence {xk}∞
k=0 via the recursive formula
xk+1 = xk −f (xk)
f ′(xk).
(15.5)
We say that the method is well deﬁned if and only if xk ∈I and f ′(xk) ̸= 0 for all
k = 1, 2, 3, . . .. We say that Newton’s method converges if and only if there is a
ξ ∈I, with f (ξ) = 0, such that xk →ξ as k →∞.
Newton’s method can be studied with the theory of ﬁxed point iterations
presented in Section 15.2. Indeed, from the deﬁnition, it is clear that Newton’s
method is a ﬁxed point iteration for the function
g(x) = x −f (x)
f ′(x).
(15.6)
Thus, for a simple root, one immediately obtains a linear convergence for Newton’s
method provided the initial guess, x0, is suﬃciently close to the root.
Proposition 15.21 (linear convergence). Let I ⊆R be an interval and f ∈C2(I)
with |f ′(x)| ≥α > 0 for all x ∈I. Assume that there is ξ ∈I for which f (ξ) = 0.
There is a constant h > 0 such that, if x0 ∈I and |x0 −ξ| < h, Newton’s method
converges at least linearly to ξ.
Proof. For this proof, let us apply the theory of ﬁxed points, as detailed in Section
15.2, to the function g deﬁned in (15.6). Notice that since f ∈C2(I), then
g′(x) = f (x)f ′′(x)
[f ′(x)]2
=⇒
g′(ξ) = 0.
Thus, by continuity, there is a constant δ > 0 such that if x ∈Iδ = [ξ −δ, ξ + δ],
then
|g′(x)| < 1.
3 Named in honor of the British mathematician, physicist, astronomer, theologian, and natural
philosopher Sir Isaac Newton (1642–1726/27).

15.3 Newton’s Method in One Space Dimension
429
In addition, notice that, by Taylor’s Theorem B.32, we have, for any x ∈I,
0 = f (ξ) = f (x) + f ′(x)(ξ −x) + 1
2f ′′(η)(ξ −x)2
for some η between x and ξ. Let us deﬁne
A = maxx∈I |f ′′(x)|
α
,
h = min

δ , 1
A

.
Then, for any x ∈Ih = [ξ −h, ξ + h], we have |g′(x)| < 1 and
|g(x) −ξ| =
x −ξ −f (x)
f ′(x)

= 1
2

f ′′(η)
f ′(x) (x −ξ)2

≤1
2Ah2
≤1
2h,
so that g(x) ∈Ih. Theorem 15.13 then allows us to conclude the (at least linear)
convergence of the ﬁxed point iteration sequence deﬁned in (15.2).
It turns out that, under normal, reasonable circumstances, Newton’s method
converges quadratically.
Theorem 15.22 (quadratic convergence). Let I ⊂R be an interval. Suppose that
f : I →R and, for some ξ ∈I, f (ξ) = 0, but f ′(ξ) ̸= 0 and f ′′(ξ) ̸= 0. Assume
that, for some δ > 0, f ∈C2(Iδ), where Iδ = [ξ−δ, ξ+δ] ⊆I, and 0 < α ≤|f ′(x)|
for all x ∈Iδ. Set
A = maxx∈Iδ |f ′′(x)|
α
,
h = min

δ , 1
A

.
(15.7)
If |ξ −x0| ≤h, then the sequence {xk}∞
k=0 deﬁned by Newton’s method (15.5)
converges quadratically, as k →∞, to the root ξ.
Proof. We could use the result of Proposition 15.21 as our starting point for this
proof. To give some variety, let us repeat the proof of linear convergence, but this
time using a direct approach.
(Well-posedness) Suppose that xk ∈Iδ. Then, by Taylor’s Theorem B.32,
0 = f (ξ) = f (xk) + (ξ −xk)f ′(xk) + (ξ −xk)2
2
f ′′(ηk)
for some ηk between xk and ξ. Note that f ′(xk) ̸= 0, and we have, using (15.5)
(the deﬁnition of Newton’s method),
xk+1 −ξ = (ξ −xk)2f ′′(ηk)
2f ′(xk)
.
(15.8)

430
Solution of Nonlinear Equations
Now, if xk ∈Ih = [ξ −h, ξ + h],
|ξ −xk+1| = 1
2
|f ′′(ηk)|
|f ′(xk)| · |ξ −xk| · |ξ −xk| ≤A
2 · h · |ξ −xk| = 1
2|ξ −xk|,
and xk+1 ∈Ih as well. The algorithm is, therefore, well deﬁned, since xk+1 ∈Ih and
f ′(xk+1) ̸= 0.
(Linear convergence) By induction, it is clear from the contraction estimate that
|ξ −xk| ≤h
2k ,
which proves that the sequence converges to the root ξ as k →∞, at least linearly;
see Deﬁnition B.10.
(Quadratic convergence) From (15.8) we obtain
|ξ −xk+1|
|ξ −xk|2 = 1
2
|f ′′(ηk)|
|f ′(xk)| .
Since xk →ξ, by the Squeeze Theorem B.6, ηk →ξ as k →∞as well. Passing to
limits, we have
lim
k→∞
|ξ −xk+1|
|ξ −xk|2 = 1
2
|f ′′(ξ)|
|f ′(ξ)| = σ ∈(0, ∞),
which establishes quadratic convergence; see Deﬁnition B.10.
Remark 15.23 (faster convergence). We see immediately that it is possible for the
convergence to be faster than quadratic if and only if f ′′(ξ) = 0.
One glaring fact about the last result for Newton’s method is that it guarantees
convergence only in a local region. To improve this to a more global result, we need
additional assumptions.
Theorem 15.24 (global convergence). Let [a, b] ⊂R be an interval and f ∈
C2([a, b]) be such that, for some ξ ∈[a, b], f (ξ) = 0. Assume further that f ′ and
f ′′ are strictly positive on the interval [a, b]. For any starting value x0 ∈(ξ, b], the
sequence {xk}∞
k=0 deﬁned by Newton’s method (15.5) converges quadratically to
the root ξ as k →∞. Moreover, xk > ξ for all k ∈N.
Proof. Since f is monotonically increasing on [a, b], ξ is the only root in [a, b].
Otherwise, by Rolle’s Theorem B.29, one could ﬁnd a point where f ′ is zero,
contradicting the assumptions. Also note that f (x) > 0 for all x ∈(ξ, b]; likewise,
f (x) < 0 for all x ∈[a, ξ).
Assume that xk > ξ. Employing Newton’s method (15.5) and using the positivity
of f (xk) and f ′(xk), we immediately obtain that
xk+1 = xk −f (xk)
f ′(xk) < xk.
Furthermore, from the error equation (15.8), using the positivity of f ′′(ηk) and
f ′(xk), we ﬁnd out that ξ−xk+1 < 0. In other words, ξ < xk+1 < xk. Thus, {xk}∞
k=0
is a bounded, monotonically decreasing sequence in [a, b]. By Theorem B.7, it must,

15.3 Newton’s Method in One Space Dimension
431
therefore, have a limit point in [a, b], call it η, as k →∞. But this limit must be
a ﬁxed point of the function g, deﬁned in (15.6). Therefore, f (η) = 0. But since
ξ is the unique root of f in [a, b], it must be that ξ = η. This shows that xk →ξ
as k →∞. Quadratic convergence follows as in the proof of Theorem 15.22.
Remark 15.25 (other initial guesses). If in the setting of the last theorem one
assumes that x0 < ξ to begin, one encounters some problems. One will conclude
from (15.5) that x1 > x0. But from (15.8) one will conclude that x1 > ξ. Thus,
there is no “squeezing” action as before. However, one might recover if, by chance,
x1 ≤b. For then we could restart the argument in the last proof to guarantee
convergence. Of course, if the interval in question is (−∞, +∞) there is no problem
at all.
Example 15.3
Let us use Newton’s method to compute the square root of a
positive real number. Suppose that we want to compute
√
5. Deﬁne f (x) = x2 −5.
There are two solutions to f (x) = 0, namely ξ± = ±
√
5. Let us pick x0 = 5.
Theorem 15.24 guarantees that this is a suitable choice if we want to compute the
zero ξ+ =
√
5. The sequence of approximations for Newton’s method is deﬁned by
xk+1 = xk −x2
k −5
2xk
,
k = 0, 1, . . . .
(15.9)
Below, we show the result of using the code presented in Listing 15.1. The correct
digits are indicated using boldface.
k
xk
0
5.000 000 000 000 000
1
3.000 000 000 000 000
2
2.333 333 333 333 333
3
2.238 095 238 095 238
4
2.236 068 895 643 363
5
2.236 067 977 499 978
6
2.236 067 977 499 790
This example illustrates an empirical fact that is a consequence of quadratic
convergence: Newton’s method doubles the number of correct digits with each
iteration. A partial explanation for this fact is as follows: from the proof of Theorem
15.22, we see that
|ξ −xk+1| ≤C|ξ −xk|2,
so that, by taking base-10 logarithms (which essentially counts the number of
correct digits), we have
log10 |ξ −xk+1| ≤2 log10 |ξ −xk| + log10 C.

432
Solution of Nonlinear Equations
Example 15.4
Deﬁne f : [1, 5] →R via f (x) = (x −3)3. Observe that, for
this simple example, ξ = 3 is a nonsimple root, i.e., f (3) = f ′(3) = 0, which is
something that the theory (up to this point) cannot handle. Nevertheless, Newton’s
method will still work. In particular, if Newton’s method is employed with the
starting point x0 = 4 to approximate the root ξ = 3, then one can show directly
that the convergence is exactly linear; see Problem 15.16.
15.3.1
Nonsimple roots
The next result describes what may happen when a certain number of derivatives
vanish at the zero of interest, as in the Example 15.4.
Theorem 15.26 (nonsimple roots). Let m be a positive integer and I be a closed
and bounded interval. Suppose that f ∈Cm(I) is such that there is ξ ∈I, for which
f (ξ) = f ′(ξ) = · · · = f (m−1)(ξ) = 0, but f (m)(ξ) ̸= 0. If |ξ −x0| is suﬃciently
small, the sequence {xk}∞
k=0 deﬁned by Newton’s method (15.5) is well deﬁned
and converges to ξ exactly linearly with
lim
k→∞
|xk+1 −ξ|
|xk −ξ|
= m −1
m
= σ ∈(0, 1).
Proof. We give a sketch of the proof. The details of well-deﬁnedness and
convergence, in particular, are left to the reader as an exercise; see Problems 15.17
and 15.18. By Taylor’s Theorem,
f (xk) = f (ξ) + f ′(ξ)(xk −ξ) + · · · + f (m−1)(ξ)(xk −ξ)m−1
(m −1)!
+ f (m)(ηk)(xk −ξ)m
m!
= f (m)(ηk)(xk −ξ)m
m!
for some ηk between xk and ξ. Another application of Taylor’s Theorem gives
f ′(xk) = f ′(ξ) + f ′′(ξ)(xk −ξ) + · · · + f (m−1)(ξ)(xk −ξ)m−2
(m −2)!
+ f (m)(ζk)(xk −ξ)m−1
(m −1)!
= f (m)(ζk)(xk −ξ)m−1
(m −1)!
for some ζk between xk and ξ. Thus, assuming xk ̸= ξ,
xk+1 −ξ = xk −ξ −f (xk)
f ′(xk) = xk −ξ −f (m)(ηk) (xk−ξ)m
m!
f (m)(ζk) (xk−ξ)m−1
(m−1)!
or
xk+1 −ξ
xk −ξ
= 1 −
f (m)(ηk)
m · f (m)(ζk).
Since ηk, ζk →ξ as k →∞,
lim
k→∞
xk+1 −ξ
xk −ξ
= 1 −1
m = m −1
m
.

15.4 Quasi-Newton Methods
433
The fact that, in Theorem 15.26, the constant σ depends only on the multiplicity
m of the root hints at the fact that quadratic convergence for Newton’s method
can be recovered by a small modiﬁcation. This is explored in Problem 15.19.
15.4
Quasi-Newton Methods
In the previous section, we developed the analysis of Newton’s method of (15.5).
We showed that, under suitable assumptions, this method converges quadratically.
There are, however, two major drawbacks to Newton’s method: it requires a
suﬃciently close initial approximation to the root, and it requires evaluating the
derivative at every iteration. Requiring a good initial guess is mostly unavoidable,
but such an approximation may be obtained by some other method. The evaluation
of the derivative, on the other hand, may be an issue. In applications, this may be
very costly, or not even at all possible. For this reason, here we propose several
important variants.
The common feature of all these methods is that they take the form (15.4),
where the slope approximation sk will, in general, change on every iteration. Its
construction will depend for instance, for r ≥0, on xk−r, . . . , xk, and the values
of the function at these points, but not require evaluation of the derivative at
these points. Another possibility is that we require fewer derivative evaluations
than Newton does, i.e., not at every iteration.
15.4.1
Simpliﬁed Newton’s Method
We already saw the simpliﬁed Newton method in Deﬁnition 15.18. Here, we provide
an analysis for it. We recall that this method has the form (15.4) with sk = s =
f ′(x0).
Theorem 15.27 (convergence). Let I ⊆R be an interval. Assume that f ∈C(I)
is such that there is an ξ ∈I for which f (ξ) = 0, but f ′(ξ) ̸= 0 and f ′′(ξ) ̸= 0.
Set, for δ > 0, Iδ = [ξ −δ, ξ + δ] ⊆I. Assume that, for some δ > 0, f ∈C2(Iδ)
and 0 < α ≤|f ′(x)| for all x ∈Iδ. Set
A = maxx∈Iδ |f ′′(x)|
α
,
h = min

δ,
1
3A

.
(15.10)
If |ξ−x0| ≤h, then the sequence {xk}∞
k=0 deﬁned by the simpliﬁed Newton method
of Deﬁnition 15.18 converges linearly to the zero ξ of f as k →∞.
Proof. Suppose that x0, xk ∈[ξ −h, ξ + h]. Then we have
xk+1 −ξ =
1
f ′(x0)
h
f ′(x0) (xk −ξ) −f (xk)
i
=
1
f ′(x0)
h f ′(x0) −f ′(ξ)

(xk −ξ) −f (xk) + f ′(ξ) (xk −ξ)
i
.

434
Solution of Nonlinear Equations
By the Mean Value Theorem B.30, there is β ∈[ξ −h, ξ + h] between x0 and ξ,
for which
f ′′(β)(x0 −ξ) = f ′(x0) −f ′(ξ).
Furthermore, for some ηk ∈[ξ −h, ξ + h] between ξ and xk, we have
f (xk) = f (ξ) + f ′(ξ)(xk −ξ) + f ′′(ηk)
2
(xk −ξ)2
from Taylor’s Theorem. Rearranging terms and using f (ξ) = 0 yields
−f (xk) + f ′(ξ)(xk −ξ) = −f ′′(ηk)
2
(xk −ξ)2.
Putting things together, we ﬁnd
xk+1 −ξ =
1
f ′(x0)

f ′′(β)(x0 −ξ) (xk −ξ) −f ′′(ηk)
2
(xk −ξ)2

=
1
f ′(x0)

f ′′(β)(x0 −ξ) −f ′′(ηk)
2
(xk −ξ)

(xk −ξ).
Taking absolute values,
|xk+1 −ξ| =
1
|f ′(x0)|
f ′′(β)(x0 −ξ) −f ′′(ηk)
2
(xk −ξ)
 · |xk −ξ|
≤
1
|f ′(x0)|

|f ′′(β)| · |x0 −ξ| + 1
2|f ′′(ηk)| · |xk −ξ|

|xk −ξ|
≤

A|x0 −ξ| + 1
2|xk −ξ|A

|xk −ξ|
≤

A · 1
3A + 1
2
1
3AA

|xk −ξ| = 1
2|xk −ξ|.
Hence, xk+1 ∈[ξ −h, ξ + h] for any k ∈N, as long as x0, xk ∈[ξ −h, ξ + h]. The
simpliﬁed Newton algorithm is well deﬁned. Furthermore, it is clear that
|xk −ξ| ≤h
2k ,
which proves that xk →ξ as k →∞.
The convergence is exactly linear, as can be seen from the error equation:
lim
k→∞
|xk+1 −ξ|
|xk −ξ|
= |f ′′(β)| · |x0 −ξ|
|f ′(x0)|
= µ.
By our assumptions, 0 < µ ≤1/3 < 1.
15.4.2
Steﬀensen’s Method
In the simpliﬁed Newton method, only one derivative evaluation is required. The
trade-oﬀis that the order of convergence is reduced. In the following, we can
eliminate derivative evaluation altogether and still retain quadratic convergence.

15.4 Quasi-Newton Methods
435
Deﬁnition 15.28 (Steﬀensen’s method4). Let I ⊆R be an interval, f ∈C(I),
and x0 ∈I. Steﬀensen’s method is an algorithm for computing the terms of the
sequence {xk}∞
k=0 via (15.4) with
sk = f (xk + f (xk)) −f (xk)
f (xk)
.
We say that this method is well deﬁned if and only if x0 ∈I implies that xk ∈I for
all k = 1, 2, . . .. We say that this method converges if and only if there is ξ ∈I,
with f (ξ) = 0, such that xk →ξ as k →∞.
Before studying the convergence of this method, let us provide some intuition
behind this slope approximation. If the method is to converge, then h = f (xk) →
f (ξ) = 0, so that
sk = f (xk + h) −f (xk)
h
is indeed a good approximation of the derivative f ′(xk).
Let us now study the convergence of this method.
Theorem 15.29 (convergence). Let I ⊆R be an interval and f ∈C(I) be such
that, for some ξ ∈I, f (ξ) = 0. Deﬁne, for δ > 0, Iδ = [ξ −δ, ξ + δ] ⊆I. Assume
that there is δ > 0 for which f ∈C2(Iδ), f ′(ξ) ̸= 0, and f ′′(ξ) ̸= 0. If |ξ −x0|
is suﬃciently small, then the sequence {xk}∞
k=0 deﬁned by Steﬀensen’s method
of Deﬁnition 15.28 is well deﬁned and converges quadratically to the zero ξ as
k →∞.
Proof. First observe that, using Taylor’s Theorem B.32, there are points ηk and
γk between xk and xk + f (xk) such that
sk = f ′(xk)f (xk) + 1
2f ′′(ηk)f 2(xk)
f (xk)
= f ′(xk) + 1
2f ′′(ηk)f (xk) = f ′(γk).
(15.11)
From the deﬁnition of the scheme, we have
xk+1 −ξ = xk −ξ −f (xk)
sk
= (xk −ξ)
 f ′(xk) + 1
2f ′′(ηk)f (xk)

−f (xk)
f ′(xk) + 1
2f ′′(ηk)f (xk)
= −[f (xk) + f ′(xk)(ξ −xk)] + 1
2f ′′(ηk)f (xk)(xk −ξ)
f ′(xk) + 1
2f ′′(ηk)f (xk)
.
By Taylor’s Theorem B.32, there is a point βk between xk and ξ such that
0 = f (ξ) = f (xk) + f ′(xk)(ξ −xk) + 1
2f ′′(βk)(ξ −xk)2,
so that
1
2f ′′(βk)(ξ −xk)2 = −[f (xk) + f ′(xk)(ξ −xk)] .
4 Named in honor of the Danish mathematician and statistician Johan Frederik Steﬀensen
(1873–1961).

436
Solution of Nonlinear Equations
Hence,
xk+1 −ξ =
1
2f ′′(βk)(ξ −xk)2 + 1
2f ′′(ηk)f (xk)(xk −ξ)
f ′(xk) + 1
2f ′′(ηk)f (xk)
=
1
2f ′′(βk)(ξ −xk)2 + 1
2f ′′(ηk)(f (xk) −f (ξ))(xk −ξ)
f ′(xk) + 1
2f ′′(ηk)f (xk)
=
1
2f ′′(βk)(ξ −xk)2 + 1
2f ′′(ηk)f ′(αk)(xk −ξ)2
f ′(xk) + 1
2f ′′(ηk)f (xk)
=
"
1
2f ′′(βk) + 1
2f ′′(ηk)f ′(αk)
f ′(xk) + 1
2f ′′(ηk)f (xk)
#
(xk −ξ)2,
where αk is some point between xk and ξ.
Taking absolute values, using the triangle inequality, and using (15.11), we get
|xk+1 −ξ| =

1
2f ′′(βk) + 1
2f ′′(ηk)f ′(αk)
f ′(γk)
 · |xk −ξ| · |xk −ξ|
≤
1
2|f ′′(βk)| + 1
2|f ′′(ηk)| · |f ′(αk)|
|f ′(γk)|
· |xk −ξ| · |xk −ξ|.
Notice now that the points ηk and γk are between xk and xk +f (xk). By continuity,
there is an h ∈(0, δ/2) such that
|f (x)| ≤δ/2,
∀x ∈Ih = [ξ −h, ξ + h].
Therefore, if xk ∈Ih, it easily follows that
|ξ −ηk| ≤δ,
|ξ −γk| ≤δ.
We assume, as usual, that there are constants m2 ≥m1 > 0 such that
m1 ≤|f ′(x)| ≤m2,
∀x ∈Iδ,
and constants m4 ≥m3 > 0 such that
m3 ≤|f ′′(x)| ≤m4,
∀x ∈Iδ.
Therefore, if xk ∈Is = [ξ −s, ξ + s], where
s < min

m1
m4 + m4m2
, h

,
|xk+1 −ξ| ≤
1
2|f ′′(βk)| + 1
2|f ′′(ηk)| · |f ′(αk)|
|f ′(γk)|
· |xk −ξ| · |xk −ξ|
≤
1
2m4 + 1
2m4m2
m1
·
m1
m4 + m4m2
· |xk −ξ| ≤1
2|xk −ξ|.
Thus, the method is well deﬁned and it converges at least linearly.
The proof of quadratic convergence follows from the fact that
lim
k→∞
|xk+1 −ξ|
|xk −ξ|2 = 1
2

f ′′(ξ) + f ′′(ξ)f ′(ξ)
f ′(ξ)
 ̸= 0,

15.4 Quasi-Newton Methods
437
where we used the fact that
αk, βk, γk, ηk →ξ,
k →∞.
15.4.3
Two-Step Newton’s Method
The next method is a variant of Newton’s method which exhibits convergence that
may be faster than quadratic.
Deﬁnition 15.30 (two-step Newton). Let I ⊆R be an interval and f ∈C1(I). For
x0 ∈I, with f ′(x0) ̸= 0, the sequence {xk}∞
k=0 deﬁned by
yk = xk −f (xk)
f ′(xk),
xk+1 = yk −f (yk)
f ′(xk)
(15.12)
is called the two-step Newton method. We say that the method is well deﬁned
if xk ∈I and f ′(xk) ̸= 0 for all k ≥0. We say that the method converges if there
is ξ ∈I such that f (ξ) = 0 and xk →ξ as k →∞.
Theorem 15.31 (convergence). Let I ⊆R be an interval, f ∈C(I) is such that
there is ξ ∈I for which f (ξ) = 0, but f ′(ξ) ̸= 0, and f ′′(ξ) ̸= 0. Set, for δ > 0,
Iδ = [ξ−δ, ξ+δ] ⊆I. Assume that is δ > 0 for which f ∈C2(Iδ) and 0 < α ≤|f ′(x)|
for all x ∈Iδ. Set
A = maxx∈Iδ |f ′′(x)|
α
,
h = min

δ, 1
A

.
If |ξ−x0| ≤h, then the sequence {xk}∞
k=0 deﬁned by the two-step Newton method
converges exactly cubically to the zero ξ as k →∞.
Proof. Suppose that xk ∈[ξ −h, ξ + h] ⊆Iδ. Then, by Taylor’s Theorem,
0 = f (ξ) = f (xk) + f ′(xk)(ξ −xk) + f ′′(ηk)
2
(ξ −xk)2
for some ηk between xk and ξ. Note that f ′(xk) ̸= 0 and, using the ﬁrst equation
in (15.12), we have that
ξ −yk = −(ξ −xk)2
2
f ′′(ηk)
f ′(xk).
(15.13)
Hence,
|ξ −yk| = 1
2
|f ′′(ηk)|
|f ′(xk)| · |ξ −xk| · |ξ −xk| ≤A
2 · h · |ξ −xk| ≤1
2|ξ −xk| ≤h
2.
We can conclude that if xk ∈[ξ −h, ξ + h], then yk ∈[ξ −h, ξ + h] as well.
Now, using the second equation in (15.12), we have
xk+1 −ξ =
1
f ′(xk)
h
f ′(xk) (yk −ξ) −f (yk)
i
=
1
f ′(xk)
h f ′(xk) −f ′(ξ)

(yk −ξ) −f (yk) + f ′(ξ) (yk −ξ)
i
.

438
Solution of Nonlinear Equations
By the Mean Value Theorem B.30, there is βk ∈[ξ −h, ξ + h] between xk and ξ,
for which
f ′′(βk)(xk −ξ) = f ′(xk) −f ′(ξ).
Furthermore, for some γk ∈[ξ −h, ξ + h] between ξ and yk, we have
f (yk) = f (ξ) + f ′(ξ)(yk −ξ) + f ′′(γk)
2
(yk −ξ)2
from Taylor’s Theorem B.32. Rearranging terms and using f (ξ) = 0 yields
−f (yk) + f ′(ξ)(yk −ξ) = −f ′′(γk)
2
(yk −ξ)2.
Putting things together, we ﬁnd
xk+1 −ξ =
1
f ′(xk)

f ′′(βk)(xk −ξ) (yk −ξ) −f ′′(γk)
2
(yk −ξ)2

.
(15.14)
Taking absolute values,
|xk+1 −ξ| =
1
|f ′(xk)|
f ′′(βk) (xk −ξ) (yk −ξ) −f ′′(γk)
2
(yk −ξ)2

≤A|xk −ξ| · |yk −ξ| + 1
2|yk −ξ| · |yk −ξ|A
≤A|xk −ξ|h
2 + 1
2 · h
2 · 1
2|xk −ξ|A
≤1
2|xk −ξ| + 1
8|xk −ξ|
= 5
8|xk −ξ|.
We can conclude that if xk ∈[ξ −h, ξ + h], then xk+1 ∈[ξ −h, ξ + h] as well. More
importantly, we see by induction that
|ξ −xk| ≤
5
8
k
h,
which proves that xk →ξ as k →∞. Using this fact, it is easy to see that
yk, βk, γk, ηk →ξ as k →∞as well.
Now, using (15.14), we see that
xk+1 −ξ
(xk −ξ) (yk −ξ) = f ′′(βk)
f ′(xk) −(yk −ξ)
2(xk −ξ)
f ′′(γk)
f ′(xk) .
Making use of (15.13),
(xk+1 −ξ)
(xk −ξ)3 = f ′′(βk)f ′′(ηk)
2(f ′(xk))2
−1
8(xk −ξ)(f ′′(ηk))2
(f ′(xk))2
f ′′(γk)
f ′(xk) .
Taking limits, we have
lim
k→∞
|xk+1 −ξ|
|xk −ξ|3 = 1
2

f ′′(ξ)
f ′(ξ)

2
= σ ∈(0, ∞).
This shows that the convergence is exactly cubic.

15.4 Quasi-Newton Methods
439
15.4.4
The Secant Method
Deﬁnition 15.32 (secant method). Let I ⊆R be an interval, f ∈C(I), and x0 ∈I.
The secant method is an algorithm for computing the terms of the sequence
{xk}∞
k=0 via (15.4) with
sk = f (xk) −f (xk−1)
xk −xk−1
,
k ≥1.
(15.15)
We say that this method is well deﬁned if and only if x0, x1 ∈I implies that xk ∈I
for all k = 2, 3, . . .. We say that this method converges if and only if there is ξ ∈I,
with f (ξ) = 0, such that xk →ξ as k →∞.
Theorem 15.33 (convergence). Let I ⊆R be an interval and f ∈C(I) be such
that there is ξ ∈I for which f (ξ) = 0. Set, for δ > 0, Iδ = [ξ −δ, ξ + δ] ⊆I.
Assume that there is δ > 0 for which f ∈C1(Iδ) and, for simplicity, f ′(ξ) > 0.
The sequence {xk}∞
k=0 deﬁned by the secant method converges (at least) linearly
to the root ξ as k →∞, provided that x0 and x1 are suﬃciently close to ξ.
Proof. Set f ′(ξ) = α > 0. By continuity, there is no loss in generality in assuming
that, for all x ∈Iδ,
0 < 3α
4 ≤f ′(x) ≤5α
4 .
Suppose now that xk, xk−1 ∈Iδ. By the Mean Value Theorem B.30, there is ηk
between xk and xk−1 such that sk = f ′(ηk). Then
xk+1 −ξ = xk −ξ −f (xk)
f ′(ηk).
By Taylor’s Theorem, there is a γk between xk and ξ such that
f (xk) = f (ξ) + f ′(γk)(xk −ξ) = f ′(γk)(xk −ξ).
Thus,
xk+1 −ξ = xk −ξ −f ′(γk)(xk −ξ)
f ′(ηk)
= (xk −ξ)

1 −f ′(γk)
f ′(ηk)

≤2
5(xk −ξ).
If |x0 −ξ| ≤δ and |x1 −ξ| ≤δ, then, by induction, we see that, for k ≥2,
|xk −ξ| ≤
2
5
k−1
δ.
This proves that the method is well deﬁned and that xk →ξ at least linearly.
It turns out that the convergence of the secant method is super-linear.
Theorem 15.34 (super-linear convergence). Let I ⊆R be an interval and f ∈
C(I). In the setting of Theorem 15.33, assume, in addition, that f ∈C2(Iδ) and
f ′′(ξ) > 0. Then the sequence {xk}∞
k=0 generated by the secant method converges
to ξ at the rate q = 1+
√
5
2
.
Proof. See Problem 15.21.

440
Solution of Nonlinear Equations
15.5
Newton’s Method in Several Dimensions
In this section, we develop and analyze Newton’s method for the solution of (15.1)
in the case that n = m = d > 1. We will need several facts about basic calculus in
several variables, and we refer the reader to Appendix B for a review.
Deﬁnition 15.35 (Newton’s method). Suppose that d ∈N, Ω⊆Rd is an open,
convex set, x0 ∈Ωis given, and f
∈C1(Ω; Rd). Newton’s method in d-
dimensions is an algorithm for computing the terms of the sequence {xk}∞
k=0
via the recursive iteration
Jf (xk) (xk+1 −xk) = −f (xk),
(15.16)
where Jf is the Jacobian matrix of f . We say that the method is well deﬁned if
and only if xk ∈Ωand Jf (xk) is nonsingular, for all k ∈N. We say that Newton’s
method converges if and only if there is a ξ ∈Ω, with f (ξ) = 0, such that xk →ξ
as k →∞.
As in the one-dimensional case, Newton’s method converges quadratically to the
root.
Theorem 15.36 (convergence). Let ξ ∈Rd and r > 0 be given. Suppose that
f ∈C2(B(ξ, r); Rd),
f (ξ) = 0, and for every x ∈B(ξ, r) the Jacobian matrix Jf (x) is invertible, with
the estimate
[J(x)]−1
2 ≤β.
Then the sequence {xk}∞
k=0 deﬁned by Newton’s method (15.16) converges (at
least) quadratically to the root ξ as k →∞, provided that x0 is suﬃciently close
to ξ.
Proof. By Taylor’s Theorem B.51, for each i = 1, . . . , d, there is a point ηk,i ∈
B(ξ, r) such that
0 = f i(ξ) = f i(xk) + ∇f i(xk)⊺(ξ −xk) + ck,i,
where ck,i = 1
2 (ξ −xk)⊺Hi(ηk,i) (ξ −xk) and Hi is the Hessian matrix of f i. To
simplify notation in the proof, let us set ck = [ck,1, . . . , ck,d]⊺, Jk = Jf (xk), and
H(k,i) = Hi(ηk,i). Using the deﬁnition of Newton’s method together with our Taylor
expansion, we get
∇f i(xk)⊺(xk+1 −xk) = −f i(xk) = ∇f i(xk)⊺(ξ −xk) + ck,i,
which simpliﬁes to
ξ −xk+1 = −J−1
k ck.

15.5 Newton’s Method in Several Dimensions
441
Using the Cauchy–Schwarz and other basic inequalities,
J−1
k ck

2 ≤
J−1
k

2 ∥ck∥2
≤β
v
u
u
t
d
X
i=1
c2
k,i
= β
2
v
u
u
t
d
X
i=1
(ξ −xk)⊺H(k,i) (ξ −xk)
2
≤β
2
v
u
u
t
d
X
i=1
∥ξ −xk∥2
2
H(k,i) (ξ −xk)
2
2
= β
2 ∥ξ −xk∥2
v
u
u
t
d
X
i=1
H(k,i) (ξ −xk)
2
2.
Another application of Cauchy–Schwarz gives
H(k,i) (ξ −xk)

2
2 =
d
X
j=1

d
X
m=1
∂2f i
∂xj∂xm
(ηk,i)(ξm −xk,m)

2
≤
d
X
j=1
"
d
X
m=1

∂2f i
∂xj∂xm
(ηk,i)

2
∥ξ −xk∥2
2
#
≤∥ξ −xk∥2
2
d
X
j=1
"
d
X
m=1
A2
#
= ∥ξ −xk∥2
2 A2d2,
where A is an upper bound on the absolute values of the second derivatives of f ,
which is available because f ∈C2  B(ξ, r); Rd
. We ﬁnally get the fundamental
error estimate:
∥ξ −xk+1∥2 ≤βAd3/2
2
∥ξ −xk∥2
2 .
Therefore, if ∥ξ −x0∥2 ≤
1
βAd3/2 = h, then ∥ξ −x1∥2 ≤1
2 ∥ξ −x0∥2. By induction,
it follows that
∥ξ −xk∥2 ≤h
1
2
2k−1
= εk.
Thus, {xk}∞
k=0 is well deﬁned, xk →ξ, and the order is at least quadratic.
The next result, due to Kantorovich, is interesting in that it requires less regularity
than what we have assumed in the previous results. Additionally, the existence of the
zero point is not a required assumption, but is a consequence of the convergence.
The following proof is similar to that in [86].

442
Solution of Nonlinear Equations
Theorem 15.37 (Kantorovich5). Let d ∈N. Suppose that Ω⊂Rd is an open,
bounded, convex set, x0 ∈Ω, and f ∈C1  Ω; Rd
. Assume, additionally, with Jf
denoting the Jacobian matrix of f , that there is γ > 0 such that
∥Jf (x) −Jf (y)∥2 ≤γ ∥x −y∥2 ,
∀x, y ∈Ω.
Furthermore, let us assume the following.
a. For all x ∈Ω, the Jacobian matrix Jf (x) is invertible and there is β > 0 such
that
[Jf (x)]−1
2 ≤β,
∀x ∈Ω.
b. The initial iterate, x0 ∈Ω, satisﬁes
[Jf (x0)]−1 f (x0)

2 ≤α.
c. The parameters satisfy
h = αβγ
2
< 1.
d. The initial iterate is well inside Ω, in the sense that
B(x0, r) ⊆Ω,
where r =
α
1−h.
In this setting, the sequence xk deﬁned by Newton’s method (15.16) is well deﬁned;
in particular, xk ∈B(x0, r) for each k ∈N. Moreover, there exists a point ξ ∈
B(x0, r) such that limk→∞xk = ξ, with the convergence estimate
∥xk −ξ∥2 ≤α h2k−1
1 −h2k ,
∀k ∈N.
Since 0 < h < 1, convergence is at least quadratic. Finally, the point ξ is a zero of
the function f , i.e., f (ξ) = 0.
Proof. We split the proof into several steps.
1. Since [Jf (x)]−1 exists for all x ∈Ω, we will have that xk+1 is deﬁned if xk ∈
B(x0, r). Suppose that, for all j = 0, 1, . . . , k, xj ∈B(x0, r). Then
∥xk+1 −xk∥2 =
[Jf (xk)]−1 f (xk)

2
≤
[J(xk)]−1
2 ∥f (xk)∥2
≤β ∥f (xk)∥2
= β ∥f (xk) −f (xk−1) −J(xk−1)(xk −xk−1)∥2
≤βγ
2 ∥xk −xk−1∥2
2 ,
(15.17)
5 Named in honor of the Soviet mathematician Leonid Vitalyevich Kantorovich (1912–1986).

15.5 Newton’s Method in Several Dimensions
443
using the result of Theorem B.56 in the last step. We claim that (15.17) implies
that, for all k ≥0,
∥xk+1 −xk∥2 ≤αh2k−1.
(15.18)
The proof is by induction. The case k = 0 holds because of assumption b:
∥x1 −x0∥2 =
[Jf (x0)]−1 f (x0)

2 ≤α.
For the induction step, we suppose that (15.18) is valid for k = j −1:
∥xj −xj−1∥2 ≤αh2j−1−1.
Let k = j now. Using (15.17) and the induction hypothesis,
∥xj+1 −xj∥2 ≤βγ
2 ∥xj −xj−1∥2
2 ≤βγ
2 α2 
h2j−1−12
= βγ
2 α2h2j−2
= αβγ
2
αh2j−2 = αh2j−1.
Hence, estimate (15.18) follows by induction.
Now, by the triangle inequality,
∥xk+1 −x0∥2 ≤∥xk+1 −xk∥2 + · · · + ∥x1 −x0∥2
≤α

1 + h + h3 + h7 + · · · + h2k−1
< α
 1 + h + h2 + · · ·

=
α
1 −h
= r.
Thus, xk+1 ∈B(x0, r). By induction, xk ∈B(x0, r) for all k ∈N.
2. Using (15.18), we can prove that {xk}∞
k=0 is a Cauchy sequence. Suppose that
m > n ≥0. Then
∥xm −xn∥2 ≤∥xm −xm−1∥2 + · · · + ∥xn+1 −xn∥2
≤αh2n−1  1 + h2n + h3·2n + h5·2n + · · ·

< αh2n−1
1 −h2n
< ε,
(15.19)
provided that n is suﬃciently large. Since xk is Cauchy, it converges to a unique
limit point ξ ∈B(x0, r), appealing to Theorem B.8 and the fact that B(x0, r)
is closed. It follows on taking m →∞in (15.19) that
∥ξ −xn∥2 < αh2n−1
1 −h2n .
From this estimate, it follows that convergence is at least quadratic.
3. Finally, we prove that f (ξ) = 0. Since xk ∈B(x0, r),
∥Jf (xk) −Jf (x0)∥2 ≤γ ∥x0 −xk∥2 ≤γr.

444
Solution of Nonlinear Equations
Thus,
∥Jf (xk)∥2 = ∥Jf (xk) −Jf (x0) + Jf (x0)∥2 ≤γr + ∥Jf (x0)∥2 = R.
As a consequence,
∥f (xk)∥2 = ∥−Jf (xk) (xk+1 −xk)∥2 ≤R ∥xk+1 −xk∥2 ,
which implies that limk→∞∥f (xk)∥2 = 0. It follows that f (ξ) = 0.
The proof is complete.
Problems
15.1
Suppose that, for every k ≥0,
|ξ −xk| ≤α h2k−1
1 −h2k ,
where α > 0 and 0 < h < 1. Show that the sequence {xk}∞
k=0 converges to ξ at
least quadratically.
15.2
Complete the proof of Theorem 15.6.
15.3
Let
f (x) = ex −2x −1,
a = 1,
b = 2.
Show that the false position method in this setting will converge to a root of f .
Show, in addition, that, for all k, we will have bk = 2.
15.4
Can you generalize the example of the previous problem? In other words,
let −∞< a < b < ∞and f ∈C2([a, b]) with f (a)f (b) < 0. Can you provide
suﬃcient conditions on f , f ′, and f ′′, so that bk = b for all k?
15.5
Prove Proposition 15.11.
15.6
Complete the proof of Theorem 15.13.
15.7
Complete the proof of Theorem 15.15.
15.8
Prove Theorem 15.17.
15.9
Prove Proposition 15.19.
15.10
Let {xk}∞
k=0 be the sequence generated, for some g ∈C([a, b]), by the
ﬁxed point iteration scheme. In the setting of Theorem 15.12, assume, in addition,
that g′(x) < 0 for all x ∈[a, b] and that x0 < ξ, where ξ is the (unique) ﬁxed point
of g. Show that, for any k ≥0,
x2k < ξ < x2k+1.
15.11
Let a ∈R and, for some r > 0, I = [a −r, a + r] be an interval. Let
g ∈C(I) be such that there is q ∈(0, 1) for which
|g′(x)| ≤q,
∀x ∈I,
and
|g(a) −a| ≤(1 −q)r.
Show that g has a unique ﬁxed point ξ ∈I and that the ﬁxed point iteration scheme
converges for any starting value x0. Moreover,
|xk −ξ| ≤qk|x0 −ξ|.

Problems
445
15.12
Consider the relaxation method (15.3). Show that if f : R →R is such
that f ′(x) < 0 and |f ′(x)| ∈[m, M] ⊂(0, ∞) for all x ∈R, then the choice
λ =
2
m + M
is optimal for the relaxation parameter.
15.13
Let f : Rd →Rd be given by
f (x) = Ax + g(x),
where A ∈Rd×d is invertible and g ∈C1(Rd; Rd). Assume that there is ξ ∈Rd
for which f (ξ) = 0. To approximate it, consider the following Picard-like iteration
method: given x0 ∈Rd, ﬁnd xk+1, for k ≥0, via
Axk+1 + g(xk) = 0.
Provide suﬃcient conditions for the convergence of this approach.
Hint: Find an expression for the error ek = ξ−xk. Then, use a version of the Mean
Value Theorem in multiple dimensions.
15.14
Assume that f ∈C2(R) with f ′(x) > 0 and f ′′(x) > 0 for all x ∈R.
a)
Exhibit a function that satisﬁes these assumptions, but has no root.
b)
Show that, if a root ξ ∈R exists, it is unique.
c)
Prove that, for any starting guess x0 ∈R, Newton’s method converges and
the convergence is quadratic.
15.15
Show that (15.26) will converge, for any x0 > 0, to
√
5.
15.16
Deﬁne f : [1, 5] →R via f (x) = (x −3)3. Use Newton’s method with
the starting point x0 = 4 to approximate the root ξ = 3. Show directly that
convergence is linear.
Hint: Show that, for k ≥0,
xk = 3 +
2
3
k
.
15.17
Suppose that f ∈C2(I), where I is an interval. Let ξ ∈I be such that
f (ξ) = f ′(ξ) = 0, but f ′′(ξ) ̸= 0.
a)
Show that the sequence {xk}∞
k=0 deﬁned by Newton’s method satisﬁes the
relation
ξ −xk+1 = −1
2
(ξ −xk)2f ′′(ηk)
f ′(xk)
= 1
2(ξ −xk) f ′′(ηk)
f ′′(χk),
where ηk and χk lie between ξ and xk.
b)
Suppose that 0 < m ≤|f ′′(x)| ≤M for all x ∈[ξ −δ, ξ + δ] ⊂I for some
δ > 0, where 0 < M < 2m. Prove that if x0 ∈[ξ −δ, ξ + δ], then xk →ξ.
15.18
Complete the proof of Theorem 15.26.
15.19
Suppose that f ∈C2(R) with f ′′ Lipschitz continuous and f (ζ) = f ′(ζ) =
0, but f ′′(ζ) ̸= 0.

446
Solution of Nonlinear Equations
a)
Prove that the iterative method
xk+1 = xk −2 f (xk)
f ′(xk)
converges at least quadratically to ζ provided that x0 is suﬃciently near, but
not equal, to ζ.
b)
Can one extend the last result for the case
f (ζ) = f ′(ζ) = f ′′(ζ) = 0,
but
f ′′′(ζ) ̸= 0?
What method, if any, would still give quadratic convergence?
15.20
Use the secant method to show that the sequence, whose recursive
deﬁnition is given below, converges to √Q, where Q > 0, given “good” starting
values x0 and x1:
xk+1 = xkxk−1 + Q
xk + xk−1
.
Come up with a similar recursion for approximating Q1/3 using the secant method.
15.21
Prove Theorem 15.34. To do so, proceed as follows.
a)
Prove the iterations are well deﬁned and converge.
b)
Show that the secant method may be written in the equivalent form,
xk+1 = xkf (xk−1) −xk−1f (xk)
f (xk−1) −f (xk)
,
k ≥1.
c)
Deﬁne
φ(xk, xk−1) =
xk+1 −ξ
(xk −ξ)(xk−1 −ξ),
where xk+1 is expressed in terms of xk and xk−1 through the recursive formula
above. Find an expression for
ψ(xk−1) = lim
xk→ξ φ(xk, xk−1)
and then determine the value of
lim
xk−1→ξ ψ(xk−1).
d)
Deduce that
lim
xk,xk−1→ξ φ(xk, xk−1) = f ′′(ξ)
2f ′(ξ).
e)
Next, suppose that
lim
k→∞
|xk+1 −ξ|
|xk −ξ|q = A > 0.
Prove that it must be that q −1 −1/q = 0 and, therefore, q = (1 +
√
5)/2.
f)
Finally, deduce that
lim
k→∞
|xk+1 −ξ|
|xk −ξ|q =

f ′′(ξ)
2f ′(ξ)

q/(1+q)
.

Problems
447
15.22
Consider the nonlinear equation exp(x) = sin(x).
a)
With pencil and paper only, argue that there is one, and only one, solution
ξ ∈
 −3
2π, −π

.
b)
Show, in fact, that there is one, and only one, solution ξ ∈
 −5
4π, −π

.
c)
Consider the following iterative methods:
xk+1 = ln(sin(xk))
and
xk+1 = sin−1(exp(xk)),
where the inverse of the sine function is appropriately, and carefully, deﬁned.
What can you say about the local convergence of each of these methods to ξ
and their convergence orders?
d)
For estimating ξ, provide a method that is quadratically convergent. Will the
method converge for any starting value in the interval x0 ∈
 −5
4π, −π

? Why
or why not?
15.23
In this method, we will explore a variant of the simpliﬁed Newton method.
Let f ∈C(R). Let ξ ∈R be such that f (ξ) = 0 and f ′(ξ) > 0. Show that there is
an ε > 0 such that if |x0 −ξ| < ε, then the following iteration converges to ξ:
xk+1 = xk −f (xk)
f ′(xm),
where m ≤k is chosen such that |f ′(xm)| = maxj≤k |f ′(xj)|.
15.24
In this problem, we will construct iterative schemes to ﬁnd a solution of
f (ξ) = 0 that converges with orders q = 2 and q = 3, respectively. Assume that
f (ξ) = 0 can be rewritten as the ﬁxed point of g, i.e., ξ = g(ξ), so that we consider
the iterative scheme
xk+1 = g(xk).
a)
Deﬁne the error ek = ξ −xk. Show that
|g(ξ −ek−1) −g′(ξ)ek−1| ≤C|ek−1|2
for some constant C > 0.
b)
Show that if g′(ξ) ̸= 0, then the method converges linearly.
c)
Show that if g′(ξ) = 0, but g′′(ξ) ̸= 0, the method converges quadratically.
d)
Consider, for a1 and a2, nonvanishing functions
g(x) = x + a1(x)f (x) + a2(x)[f (x)]2.
(15.20)
Show that x = g(x) if and only if f (x) = 0.
e)
Let g be given by (15.20). Evaluate the ﬁrst and second derivatives of g
with respect to x. From them, show that if q = 2 (the method converges
quadratically), then we obtain Newton’s method.

448
Solution of Nonlinear Equations
f)
Let g be given by (15.20). Show that if the method converges cubically, i.e.,
q = 3, then
a1(x) = −
1
f ′(x),
a2(x) = −f ′′(x)
2[f ′(x)]3 .
Unfortunately, it turns out that this method is unstable. It is only used to
accelerate the convergence once a good guess is already available.
15.25
Let f : Ω⊂Rn →Rn be twice continuously diﬀerentiable. Suppose that
ξ ∈Ωis a solution of f (x) = 0 and the Jacobian matrix of f , denoted Jf , is
invertible at ξ. Prove that if x0 ∈Ωis suﬃciently close to ξ, then the following
iteration converges to ξ:
xk+1 = xk −Jf (x0)−1 f (xk).
15.26
Suppose that the function f : R2 →R2 is deﬁned via
f (x1, x2) =
16 −x2
1 −x2
2
x2
1 −1

.
How many real-valued (vector) solutions does the system f (x) = 0 have? Use
Newton’s method to obtain the approximations x1 and x2 when x0 = [1, 1]⊺.
15.27
Suppose that the function f : R2 →R2 is deﬁned via
f (x1, x2) =
x2
1 −2x1 + x2
2
x2
1 + x2
2 −1

.
How many real-valued (vector) solutions does the system f (x) = 0 have? Use
Newton’s method to obtain the approximations x1 and x2 when x0 = [0, −1]⊺.
15.28
By B(x, r) ⊂R2, denote the open ball of radius r > 0 centered at x.
Suppose that, for some r > 0, f , g : B(ξ, r) →R are nonlinear, twice continuously
diﬀerentiable functions with
f (ξ) = 0,
g(ξ) = 0.
Consider the Gauss–Seidel-like iterative scheme: given
xk = [x1,k, x2,k]⊺∈B(ξ, r),
ﬁnd xk+1 = [x1,k+1, x2,k+1]⊺∈R2 such that
f (x1,k+1, x2,k) = 0,
g(x1,k+1, x2,k+1) = 0.
a)
Let ek = ξ −xk be the error.
b)
Establish an iteration error equation of the form
" ∂f
∂x1 (ξ)
∂f
∂x2 (ξ)
∂g
∂x1 (ξ)
∂g
∂x2 (ξ)
#
ek+1 =
" ∂f
∂x1 (ξ)e1,k+1 + ∂f
∂x2 (ξ)e2,k+1
∂g
∂x1 (ξ)e1,k+1 + ∂g
∂x2 (ξ)e2,k+1
#
= r k+1.
Give a precise expression for the remainder term, r k+1.
c)
Give suﬃcient conditions for the convergence of the scheme.

Listings
449
15.29
Let f : R2 →R2 be deﬁned via
f (x1, x2) =
x2
1 −2x1 + x2
2x1 −x2
2 −1

.
Observe that f has the zero
ξ =
 1
1

.
Consider the iteration
xn+1 = xn −Af (xn),
A =
1
1/2
1
0

.
(15.21)
a)
Prove that xn →ξ, provided that x0 is suﬃciently close to ξ.
b)
Show that the convergence is at least quadratic.
c)
Is the iteration (15.21) equivalent to Newton’s method?
Listings
1
function [root,count,err] = NewtonRoot(xin, p, q, tol, maxits)
2
%
3
% This function calculates the pth root of q ,
4
%
5
%
qˆ(1/p)
6
%
7
% using Newton's method. For simplicity, we assume that p is a
8
% positive integer and q is a positive real number
9
%
10
% Input:
11
%
xin : initial guess
12
%
p : the positive integer degree of the root
13
%
q : the positive number whose pth root is to estimated
14
%
tol : stopping tolerance
15
%
maxits : the maximal number of iterations
16
%
17
% Output:
18
%
root : approximation of the root
19
%
count : number of newton iterations required to compute the
20
%
root
21
%
err: = 0, if the algorithm proceeded to completion
22
%
= 1, if an error was encountered
23
%
24
root = NaN;
25
count = 0;
26
err = 0;
27
if int32(p) ~= p
| |
p < 0
28
disp('Error: p must be a positive integer');
29
err = 1;
30
return
31
end
32
if q < 0

450
Solution of Nonlinear Equations
33
disp('Error: q must be positive');
34
err = 1;
35
return
36
end
37
diff = 1.0;
38
x = xin;
39
while diff > tol && count < maxits
40
xo = x;
41
x = xo - fn(xo,p,q)/dfn(xo,p,q);
42
diff = abs(x-xo);
43
count = count+1;
44
end
45
if count >= maxits
46
err = 1;
47
end
48
root = x;
49
end
50
51
function y = fn(x,p,q)
52
y = xˆp-q;
53
end
54
55
function y = dfn(x,p,q)
56
y = p*xˆ(p-1);
57
end
Listing 15.1 Newton’s method for computing
p√q.

16
Convex Optimization
Given an energy (or cost function, or objective function) E deﬁned on a real Hilbert
space H, we consider the following minimization (or optimization) problem:
u = argmin
v∈H
E(v).
(16.1)
For mathematicians, the ﬁrst thoughts must always be these: Does this problem
have a solution? Does the problem make sense as it is written?
Such problems, called unconstrained optimization problems, are ubiquitous in
science and engineering. For example, many physical problems have as their solution
the minimization of some energy. Nature works by ﬁnding the minimum energy path
through space and time. In other applications, E may be the cost to manufacture
an item. To make the most proﬁt, we will try to minimize the production cost.
In this chapter, we will work with the simplest version of this problem, by assuming
that E is strongly convex, with some other nice properties as well. With our
assumptions about E, we will be guaranteed that a unique minimizer always exists,
an important ﬁrst step for a numerical analyst who wants to build an algorithm to
approximate the minimizer. In many real-world problems, much less may be known
or assumed about E, and things can get messy, beyond the scope of our simple
introduction.
A word of warning. This chapter may be challenging for readers not familiar
with the beautiful theory of functional analysis and calculus on inﬁnite-dimensional
vector spaces. In fact, this chapter may serve as an introduction to the subject
for many. Be prepared to read and think carefully. We use this theory not for the
sake of pure abstraction, but because it is powerful, it will quickly lead us to deep
insights about this problem, and it will allow us to cover many practical applications
at once. For further reading about functional analysis and diﬀerential calculus in
inﬁnite-dimensional vector spaces, we refer the reader to [5, 18]. See [14] for a
similar treatment of convex optimization.
16.1
Some Tools from Functional Analysis
Throughout this chapter, H will be a real Hilbert space, ﬁnite or inﬁnite dimensional,
which is nothing but an inner product space, in the sense of Section A.4, with an
additional property that we now deﬁne.
Deﬁnition 16.1 (completeness). Let V be a normed vector space with norm ∥·∥V.
We say that this space is complete if every Cauchy sequence converges. In other

452
Convex Optimization
words, if {xn}n∈N ⊂V is a sequence that satisﬁes: for every ε > 0, there is N ∈N
such that for all m, n ≥N, we have
∥xm −xn∥V < ε,
then this sequence must converge, meaning that there is x ∈V such that
∥xn −x∥V →0
as n →∞.
Notice that, in this deﬁnition, we combine two, at ﬁrst glance disparate, notions:
linear algebra (through the notion of vector space) and analysis (through the notion
of convergence). This is a recurring feature in functional analysis.
Now we are ready to deﬁne Hilbert spaces.
Deﬁnition 16.2 (Hilbert space1). A Hilbert space is an inner product space H
with inner product ( · , · )H that is complete under the canonical norm ∥· ∥H, i.e.,
∥v∥H =
p
(v, v)H,
∀v ∈H.
We have already dealt with several examples of ﬁnite-dimensional Hilbert spaces.
Example 16.1
For n ∈N, Rn with the (·, ·)2 inner product is a Hilbert space.
Example 16.2
For n ∈N, Pn with the inner product
(p, q)L2(−1,1) =
Z 1
0
p(x)q(x)dx,
∀p, q ∈Pn
is a Hilbert space.
Example 16.3
Let us now show an example of an inner product space that is
not complete and, hence, is not a Hilbert space. Consider the vector space of
continuous real-valued functions on [−1, 1] and deﬁne, on C([−1, 1]), the inner
product
(f , g)L2(−1,1) =
Z 1
−1
f (x)g(x)dx,
∀f , g ∈C([−1, 1]).
It turns out that this is not a complete space. To see this, consider the following
sequence:
fn(x) =













1,
x ≤0,
1 −nx,
x ∈

0, 1
n

,
0,
x ∈
1
n, 1

.
1 Named in honor of the German mathematician David Hilbert (1862–1943).

16.1 Some Tools from Functional Analysis
453
We now show that this sequence is Cauchy. Choose ε > 0. Notice that 0 ≤fn(x) ≤
1 for all x ∈[−1, 1], Thus, provided that N > 4
ε2 , we have for any m, n ≥N,
∥fn −fm∥L2(−1,1) ≤∥fn∥L2(−1,1) + ∥fm∥L2(−1,1) ≤2
 Z 1/N
0
1dx
!1/2
=
2
√
N < ε.
However, this sequence does not converge to a continuous function, as the only
possible limit is
fn(x) =
(
1,
x ≤0,
0,
x ∈[0, 1], /∈C([−1, 1]).
Notice that what made the diﬀerence in the previous two examples was the
dimension of the space. While dim Pn = n + 1, we have that C([−1, 1]) is inﬁnite
dimensional. To see this, observe that P ≤C([−1, 1]), where P is introduced in
Example A.5.
Before moving forward, we quickly comment that complete normed spaces
without an inner product can also be studied. These are called Banach spaces.2
However, discussing them would stray too far from where we want to go. For this
reason, for the remainder of this chapter, we assume that H is a real Hilbert space.
Because we are working in a fairly general setting — in particular, we are not
assuming that H is ﬁnite dimensional — we will need some basic tools and facts
from functional analysis.
The ﬁrst one comes from realizing that the proof of Proposition A.31 never
assumed that the inner product space was either ﬁnite dimensional or complete.
Thus, the Cauchy–Schwarz inequality,
|(u, v)H| ≤∥u∥H ∥v∥H ,
holds in a Hilbert space. Another identity, which shall prove useful in the sequel, is
2(u, v)H = ∥u∥2
H −∥v −u∥2
H + ∥v∥2
H,
∀u, v ∈H;
(16.2)
see Problem 16.1.
With the aid of a norm, we can introduce the notions of convergence and
continuity. In inﬁnite-dimensional Hilbert spaces, there are multiple notions of
convergence that are important.
Deﬁnition 16.3 (modes of convergence). Let {un}∞
n=1 be a sequence in the Hilbert
space H. We say that {un}∞
n=1 is bounded if and only if there is a constant C > 0
such that
∥un∥H ≤C
for all n ≥1. We say that {un}∞
n=1 (strongly) converges if and only if there is an
element u ∈H such that
∥un −u∥H →0,
n →∞,
2 Named in honor of the Polish mathematician Stefan Banach (1892–1945).

454
Convex Optimization
and we write un →u. We say that {un}∞
n=1 converges weakly if and only if there
is an element u ∈H such that
(un, w)H →(u, w)H,
∀w ∈H,
and we write un ⇀u.
The following result shows that weak convergence is indeed a weaker notion.
Proposition 16.4 (strong =⇒weak). Suppose that {un}∞
n=1 is a sequence in the
Hilbert space H. Then un →u implies that un ⇀u. The converse is not necessarily
true.
Proof. See Problem 16.2.
However, there are cases when from weak convergence, strong can be implied.
Theorem 16.5 (weak =⇒strong). Suppose that {un}∞
n=1 is a sequence in the
Hilbert space H. If un ⇀u then {un}∞
n=1 is bounded. Furthermore, if un ⇀u and
∥un∥H →∥u∥H, then un →u.
Proof. The ﬁrst result requires tools beyond the scope of our introduction. The
second result follows from the following polarization identity:
∥u −un∥2
H = 2(u −un, u)H + ∥un∥2
H −∥u∥2
H .
In ﬁnite dimensions, the Heine–Borel Theorem, see Corollary B.15, ensures that
every closed and bounded set is compact. But, in inﬁnite dimensions, closed and
bounded sets need not be compact. To further muddy the waters, as there are
several notions of convergence, there are multiple notions on closedness.
Deﬁnition 16.6 (closed). Suppose that K is a subset of the Hilbert space H. We
say that K is closed if and only if whenever {un}∞
n=1 ⊂K is such that un →u,
then we must have u ∈K. We say that K is weakly closed if and only if whenever
{un}∞
n=1 ⊂K is such that un ⇀u, then this implies that u ∈K.
Deﬁnition 16.7 (closure). Suppose that K is a subset of the Hilbert space H.
Deﬁne
Ls = {u ∈H | ∃{un}∞
n=1 ⊂K : un →u}
and
Lw = {u ∈H | ∃{un}∞
n=1 ⊂K : un ⇀u} .
The set
clos(K) = K ∪Ls
is called the closure of K and the set
clow(K) = K ∪Lw
is called the weak closure of K.

16.1 Some Tools from Functional Analysis
455
These are the smallest closed and weakly closed sets, respectively, that
contain K, as you would expect.
Next, let us remind the reader about subsequences, as these play an important
role in the concept of compactness and the coming new concept of weak
compactness.
Deﬁnition 16.8 (subsequence). Suppose that {nk}∞
k=1 ⊆N is a strictly increasing
sequence of numbers and {un}∞
n=1 ⊂H. The sequence {unk}∞
k=1 is called a
subsequence of {un}∞
n=1 and we write {unk}∞
k=1 ⊆{un}∞
n=1.
Deﬁnition 16.9 (sequential compactness). We say that a set K
⊂
H is
sequentially compact if and only if every sequence {un}∞
n=1 has a subsequence
{unk}∞
k=1 ⊆{un}∞
n=1 that converges to a point u ∈K. K ⊂H is called weakly
sequentially compact if and only if every sequence {un}∞
n=1 has a subsequence
{unk}∞
k=1 ⊆{un}∞
n=1 that converges weakly to a point u ∈K.
Now we can state the so-called weak compactness property, a fundamental
attribute of Hilbert spaces. The proof of this result is beyond the scope of this
text.
Theorem 16.10 (weak compactness). Suppose that H is a Hilbert space. Then
every bounded sequence {un}∞
n=1 ⊂H has a weakly convergent subsequence, i.e.,
there is an element u ∈H and a sequence of numbers {nk}∞
k=1 ⊂N such that
unk ⇀u as k →∞.
16.1.1
The Dual Space
Functional analysis gets its name from the following deﬁnition.
Deﬁnition 16.11 (linear functional). A function f : H →R is called a bounded
linear functional if and only if it is:
1. Linear, i.e.,
f (αv + βw) = αf (v) + βf (w),
∀α, β ∈R,
∀v, w ∈H.
2. Bounded, i.e., there is some constant C > 0 such that for all w ∈H,
|f (w)| ≤C ∥w∥H .
The set of all bounded linear functionals on H is called the dual space and is
denoted H′. Under point-wise addition and scalar multiplication this is a vector
space.
The space of bounded linear functionals can be normed, as follows.
Deﬁnition 16.12 (operator norm). Suppose that f ∈H′. The operator norm of
f is deﬁned as
∥f ∥H′ =
sup
v∈H
∥v∥H=1
|f (v)| =
sup
v∈H\{0}
|f (v)|
∥v∥H
.
(16.3)

456
Convex Optimization
Proposition 16.13 (completeness). The function ∥· ∥H′ : H′ →R deﬁnes a norm
on H′ and, with this norm, the dual space H′ of a Hilbert space H is in fact a
Banach space. Furthermore, if f ∈H′, then
|f (v)| ≤∥f ∥H′ ∥v∥H ,
∀v ∈H.
Proof. The proof of the fact that ∥· ∥H′ is indeed a norm and the estimate on
|f (v)| are left to the reader as an exercise; see Problem 16.3.
To show completeness, let {fn}n∈N ⊂H′ be Cauchy, i.e., for every ε > 0, there
is N ∈N such that, whenever m, n ≥N, we have
∥fn −fm∥H′ < ε.
By deﬁnition, this implies that, for any v ∈H and all m, n ≥N, we have
|fn(v) −fm(v)| ≤∥fn −fm∥H′∥v∥H < ε∥v∥H.
In other words, the sequence {fn(v)}n∈N ⊂R is Cauchy, and so it must converge.
Denote the limit by f (v).
By uniqueness of the limit, we have deﬁned f : H →R via
f (v) = lim
n→∞fn(v).
This mapping is linear, since, whenever α, β ∈R and v, w ∈H, we have
f (αv + βw) = lim
n→∞fn(αv + βw) = α lim
n→∞fn(v) + β lim
n→∞fn(w) = αf (v) + βf (w).
We now show that f ∈H′, i.e., that it is bounded. First, it is left to the reader
as an exercise to show that {∥fn∥H′}n∈N is bounded, i.e., there is M > 0 such that
for all n ∈N,
∥fn∥H′ ≤M.
With this at hand, let v ∈H be arbitrary and observe that
|f (v)| =
 lim
n→∞fn(v)
 = lim
n→∞|fn(v)| ≤lim
n→∞∥fn∥H′∥v∥H ≤lim
n→∞M∥v∥H ≤M∥v∥H.
Finally, we show that ∥fn −f ∥H′ →0. For this, let v ∈H be arbitrary and
m, n ≥N, so that
|fn(v) −fm(v)| ≤∥fn −fm∥H′∥v∥H < ε
2∥v∥H.
Passing to the limit m →∞, this implies that
∥f −fn∥H′ =
sup
0̸=v∈H
|fn(v) −f (v)|
∥v∥H
≤ε
2 < ε,
and so H′ is complete.
The next result — commonly known as the Riesz Representation Theorem3 —
is classical, powerful, and famous. Its proof is beyond our scope here.
3 Named in honor of the Hungarian mathematician Frigyes Riesz (1880–1956).

16.1 Some Tools from Functional Analysis
457
Theorem 16.14 (Riesz Representation Theorem). Suppose that f ∈H′. There
exists a unique element Rf ∈H such that
f (v) = (Rf , v)H,
∀v ∈H.
Furthermore,
∥Rf ∥H = ∥f ∥H′ .
The map R: H′ →H is called the canonical Riesz map.
The previous result is important in two respects. First, it shows that un ⇀u if
and only if f (un) →f (u) for all f ∈H′. In addition, it gives a way to deﬁne an
inner product on H′ as well, so that H′ can actually be viewed as a Hilbert space.
Deﬁnition 16.15 (canonical inner product). For all f , g ∈H′, deﬁne
(f , g)H′ = (Rf , Rg)H .
The object (·, ·)H′ : H′ × H′ →R is called the canonical inner product on H′.
Theorem 16.16 (H′ is Hilbert). Let H be a Hilbert space and H′ be its dual. Then
(·, ·)H′ : H′ × H′ →R is an inner product on H′, and, with this inner product, H′
is a Hilbert space. Moreover,
∥f ∥H′ =
q
(f , f )H′,
∀f ∈H′,
where ∥·∥H′ is the operator norm deﬁned in (16.3).
Proof. See Problem 16.5.
Remark 16.17 (ﬁnite dimensions). Note that, as shown in Example 1.1, the
content of the Riesz Representation Theorem 16.14 is essentially trivial in ﬁnite
dimensions and that (Rn)′ can be easily identiﬁed with Rn itself.
16.1.2
Bounded Linear Operators
We now introduce some notions regarding operators on a Hilbert space. We recall
that for normed spaces V and W, in Deﬁnition 1.1, we introduced L(V, W).
Deﬁnition 16.18 (bounded operator). Let V and W be normed spaces. We say
that T ∈L(V, W) is bounded if and only if there is a constant C > 0 such that,
for every v ∈V,
∥Tv∥W ≤C∥v∥V.
We denote the set of all bounded linear operators V →W by B(V, W). If V = W,
then we denote this by B(V).
The point of introducing this new notion is that we are not assuming the normed
spaces to be ﬁnite dimensional. Otherwise, we could have applied Proposition 1.2
and Deﬁnition 1.28 to conclude that B(V, W) = L(V, W) is a normed space with
a very special norm. In inﬁnite dimensions, however, not every linear operator is
bounded.

458
Convex Optimization
Example 16.4
The space of (equivalence classes of) functions f : (0, 1) →R,
for which
Z 1
0
|f (x)|2dx < ∞
is a vector space, which we denote L2(0, 1). We can endow this space with the
inner product
(f , g)L2(0,1) =
Z 1
0
f (x)g(x)dx,
∀f , g ∈L2(0, 1),
under which this is a Hilbert space. It is not diﬃcult to show that P ⊂L2(0, 1), so
that L2(0, 1) is inﬁnite dimensional. Let us deﬁne, at least for f ∈P,
(Tf )(x) = f ′(x).
This is a linear operator; however, it is not bounded. To see this, notice that we
can set, for n ∈N, f (x) = xn to obtain
∥f ∥2
L2(0,1) =
Z 1
0
|x|2ndx =
1
2n + 1,
∥f ′∥2
L2(0,1) = n2
Z 1
0
|x|2(n−1)dx =
n2
2n −1.
Since n can be arbitrarily large, there is no constant C > 0 for which
n2
2n −1 ≤
C2
2n + 1,
∀n ∈N.
Even in inﬁnite dimensions, for linear operators, boundedness and continuity are
equivalent notions.
Proposition 16.19 (equivalence). Let V and W be normed spaces and T ∈
L(V, W). The following are equivalent.
1. The operator T is bounded.
2. The operator T is continuous, i.e., whenever {vn}n∈N ⊂V is such that vn →v,
then
∥Tvn −Tv∥W →0.
3. The operator T is continuous at zero, meaning that if vn →0, then
Tvn →0.
Proof. Let us show that boundedness implies continuity. To do so, consider a
sequence vn →v. By linearity and boundedness,
∥Tvn −Tv∥W = ∥T(vn −v)∥W ≤C∥vn −v∥V →0,
so that the operator is continuous.
Obviously, continuity implies that the operator T is continuous at a particular
point, and by linearity T0 = 0.

16.1 Some Tools from Functional Analysis
459
To close the argument, assume that T is continuous at zero. This necessarily
means that there is δ > 0 such that, whenever ∥v∥V < δ, we must have ∥Tv∥W < 1.
Let now w ∈V be nonzero, but otherwise arbitrary. Deﬁne
v =
δ
2∥w∥V
w ∈V
=⇒
∥v∥V < δ.
Thus, by assumption, ∥Tv∥W < 1, which by linearity and properties of the norm
implies that
δ
2∥w∥V
∥Tw∥W = ∥Tv∥W < 1
⇐⇒
∥Tw∥W < 2
δ ∥w∥V,
so that C = 2/δ is the needed constant.
It turns out that B(V, W) is a vector space, that it can be normed, and that,
if W is complete, this space is complete under this norm. Hence, it is a Banach
space.
Deﬁnition 16.20 (operator norm). Let V, W be normed spaces and T ∈B(V, W).
We deﬁne the operator norm
∥T∥B(V,W) = sup
0̸=v∈V
∥Tv∥W
∥v∥V
.
Proposition 16.21 (completeness). Let V, W be normed spaces. The set B(V, W)
under the same rules of addition and scalar multiplication of L(V, W) is a vector
space. Moreover, the quantity ∥·∥B(V,W) of Deﬁnition 16.20 is a norm on B(V, W).
Finally, if W is complete, then so is B(V, W) under the operator norm.
Proof. The fact that B(V, W) is a vector space and that the operator norm
is indeed a norm are immediate and left to the reader. For completeness, one,
essentially, needs to repeat the argument of completeness of H′ presented in
Proposition 16.13. The only diﬀerence worth noting is that, instead of the
completeness of R, we must invoke that of W; see Problem 16.6.
Even in ﬁnite dimensions, not every bounded operator is invertible. In inﬁnite
dimensions, one new issue arises; namely, even when an inverse exists and is linear,
this may not be bounded. Here, we will only state a suﬃcient condition for an
operator to have a bounded inverse. The proof of this fact is beyond the scope of
our discussion here. Let us only mention that it follows from the celebrated Banach
Open Mapping Theorem.
Proposition 16.22 (bounded inverse). Let H be a Hilbert space and T ∈B(H)
be such that there is m > 0 such that
∥Tv∥H ≥m∥v∥H,
∀v ∈H.
Then the operator T has a bounded inverse, in the sense that there is a (necessarily)
unique mapping T −1 ∈B(H) such that
T(T −1v) = T −1(Tv) = v,
∀v ∈H.

460
Convex Optimization
Moreover,
∥T −1∥B(H) ≤1
m.
16.1.3
Convex Sets and Convex Functions
Deﬁnition 16.23 (convex set). A subset K ⊆H is called convex if and only if
u, v ∈K implies that
tu + (1 −t)v ∈K,
∀t ∈[0, 1].
Deﬁnition 16.24 (convex function). Let K be a convex subset of the Hilbert space
H. We say that the function E : K →R is convex if and only if
E(tu + (1 −t)v) ≤tE(u) + (1 −t)E(v),
∀u, v ∈K,
∀t ∈[0, 1].
We say that E is strictly convex if and only if it is convex and for every u, v ∈K,
u ̸= v,
E(tu + (1 −t)v) < tE(u) + (1 −t)E(v),
∀t ∈(0, 1).
16.2
Existence and Uniqueness of a Minimizer
Let us now use some of the functional analysis tools we previously developed to
discuss convex optimization.
Deﬁnition 16.25 (lower semi-continuity). Suppose that K is a subset of the Hilbert
space H and {un}∞
n=1 ⊂K is a sequence. We say that E : K →R is lower semi-
continuous if and only if un →u ∈K implies that
E(u) ≤lim inf
n→∞E(un).
(16.4)
We say that E is weakly lower semi-continuous if and only if un ⇀u ∈K implies
(16.4).
The notions of lower semi-continuity are related as follows.
Proposition 16.26 (semi-continuity). Suppose that K is a subset of the Hilbert
space H and E : K →R is weakly lower semi-continuous. Then E is lower semi-
continuous. If E is continuous, then it is lower semi-continuous.
Proof. See Problem 16.7.
Example 16.5
Let H be a Hilbert space. The norm ∥· ∥H is weakly lower semi-
continuous. To see this, suppose that un ⇀u ̸= 0 and observe that
∥u∥2
H = (u, u)H = lim
n→∞(un, u)H ≤lim inf
n→∞∥un∥H ∥u∥H .

16.2 Existence and Uniqueness of a Minimizer
461
The following two, very general, results are commonly known as the direct
method of calculus of variations.
Theorem 16.27 (existence I). Suppose that K ⊂H is bounded and weakly closed
and H is a Hilbert space. Assume that E : K →R is weakly lower semi-continuous.
Then there is at least one element u such that
E(u) = inf
v∈K E(v).
(16.5)
Proof. Set α = infv∈K E(v). Assume that α ∈R. There exists a sequence
{un}∞
n=1 ⊂K such that
E(un) →α.
The sequence {un}∞
n=1 ⊂K is called an inﬁmizing or minimizing sequence. For
completeness, let us prove its existence. Since α is the greatest lower bound, we
know that, for each n ∈N, there must be an element yn ∈E(K), where E(K) ⊆R
denotes the image of K under E such that
α < yn < α + 1
n.
Otherwise, α could not be the greatest lower bound. Now take un ∈K, so that
yn = E(un). This proves the existence of the minimizing sequence.
Having shown the existence of the inﬁmizing sequence, we observe that, since
K is bounded, {un}∞
n=1 is bounded. Therefore, by Theorem 16.10, there is a
subsequence {unk}∞
k=1 ⊆{un}∞
n=1 and a point u ∈H such that unk ⇀u as
k →∞. Since K is weakly closed, u ∈K. Since E is weakly sequentially lower
semi-continuous,
E(u) ≤lim inf
k→∞E(unk).
The only possibility is that α = E(u) and the proof is complete.
Incidentally, we point out that the proof implies that −∞< α.
Deﬁnition 16.28 (coercivity). Suppose that E : H →R. E is called coercive if
and only if
lim
n→∞E(vn) →∞
whenever ∥vn∥H →∞.
Theorem 16.29 (existence II). Assume that E : H →R is weakly lower semi-
continuous and coercive on the Hilbert space H. Then there is at least one element
u such that
E(u) = inf
v∈H E(v).
(16.6)
Proof. Deﬁne
K0 = {v ∈H | E(v) ≤E(0)} .
Since E is coercive, K0 is bounded. Otherwise, there is a sequence {vn}∞
n=1 ⊂K
such that ∥vn∥H →∞as n →∞. But then limn→∞E(vn) →∞. So there is an
N ∈N such that, when m ≥N,

462
Convex Optimization
E(vm) > E(0).
This is a contradiction and so K0 must be bounded.
Now, since E is weakly lower semi-continuous, we claim that K0 is weakly closed.
To see this, suppose that {vn}∞
n=1 ⊂K0 is such that there is v ∈H for which
vn ⇀v. We want to prove that v ∈K0. Since E is weakly lower semi-continuous,
E(v) ≤lim inf
n→∞E(vn) ≤E(0).
Thus, v ∈K0.
Problem (16.6) then is equivalent to
E(u) = inf
v∈K0 E(v).
By Theorem 16.27, there is at least one solution to (16.6).
It is not easy to work directly with the concepts of weakly closed sets and weakly
lower semi-continuous functions. Thankfully, we have the following result, whose
proof is beyond our scope. It is commonly known as Mazur’s Lemma.
Proposition 16.30 (Mazur’s Lemma4). Let H be a Hilbert space. Suppose that
K ⊂H is convex and closed. Then it is weakly closed. If E : H →R is convex and
continuous (or lower semi-continuous), then it is weakly lower semi-continuous.
Finally, we have this, the main result.
Theorem 16.31 (existence and uniqueness). Assume that E : H →R is strictly
convex, continuous, and coercive on the Hilbert space H. Then there is a unique
element u ∈H such that
E(u) = inf
v∈H E(v).
(16.7)
This allows us to write
u = argmin
v∈H
E(v).
(16.8)
Proof. Deﬁne, as before,
K0 = {v ∈H | E(v) ≤E(0)} .
Since E is continuous and convex, it is weakly lower semi-continuous on H. The
set K0 is convex, since E is convex, and it is bounded, since E is coercive. Since
E is continuous, if vn →v, with {vn}∞
n=1 ⊂K0, we have
E(v) = lim
n→∞E(vn) ≤E(0).
Thus, v ∈K0 and K0 is closed. Since K0 is closed and convex, it is weakly closed.
The proof of existence now follows by the same argument as in the proof of
Theorem 16.29.
4 Named in honor of the Polish mathematician Stanis law Mieczys law Mazur (1905–1981).

16.3 The Euler Equation
463
Uniqueness follows from the strict convexity of E. Suppose that u1, u2 ∈H,
u1 ̸= u2 are minimizers. Deﬁne
α = min
v∈H E(v).
Since E is strictly convex,
E(tu1 + (1 −t)u2) < tE(u1) + (1 −t)E(u2) = α,
∀t ∈(0, 1).
This implies that u1 and u2 are not minimizers, unless perhaps u1 = u2, the only
possibility that does not lead to contradiction.
16.3
The Euler Equation
Deﬁnition 16.32 (Fr´echet derivative5). Suppose that H is a Hilbert space and
E : H →R. We say that E is Fr´echet diﬀerentiable at the point v ∈H if and
only if there is a bounded linear functional A ∈H′ for which
lim
∥h∥H→0
|E(v + h) −E(v) −A(h)|
∥h∥H
= 0.
A is called a Fr´echet derivative at v. If E is Fr´echet diﬀerentiable at every point
v in a particular set B ⊆H, we say that E is Fr´echet diﬀerentiable on B.
Proposition 16.33 (uniqueness). Suppose that E : H →R is Fr´echet diﬀerentiable
at the point v ∈H. Then the derivative is unique. The (uniquely deﬁned) derivative
is denoted by E′[v] ∈H′.
Proof. See Problem 16.8.
Remark 16.34 (arguments). We observe that if E is Fr´echet diﬀerentiable on a set
B, then the bounded linear functional E′[v] changes as v ∈B changes. Speciﬁcally,
if v and w are two diﬀerent points of B, then E′[v] and E′[w] are generally diﬀerent
linear functionals. Now, once v ∈B is ﬁxed, we have E′[v] ∈H′, and the action of
the functional on an element w ∈H of the Hilbert space is denoted by E′[v](w).
Of course, E′[v](·) is linear, so that
E′[v]
 N
X
i=1
αiwi
!
=
N
X
i=1
αiE′[v](wi),
∀αi ∈R, wi ∈H, i = 1, . . . , N.
Remark 16.35 (notation). We will use the duality pairing notation throughout this
chapter; in particular, for any f ∈H′,
⟨f , w⟩= f (w),
∀w ∈H.
5 Named in honor of the French mathematician Maurice Ren´e Fr´echet (1878–1973).

464
Convex Optimization
The object on the left, ⟨·, ·⟩, is not an inner product. It is called the duality pairing
between H′ and H. The bounded linear functional always appears in the left-hand
slot of ⟨·, ·⟩. Thus, if E is Fr´echet diﬀerentiable at a point v ∈H, we write
⟨E′[v], w⟩= E′[v](w),
∀w ∈H.
This usage will save some writing.
Now let us give a weaker version of the deﬁnition of diﬀerentiability.
Deﬁnition 16.36 (Gateaux derivative6). We say that E : H →R is Gateaux
diﬀerentiable at the point v ∈H if and only if there is a bounded linear functional
A ∈H′ such that
lim
s→0
1
s (E(v + sw) −E(v)) = A(w)
for all w ∈H. A is called a Gateaux derivative at v. As before, if E is Gateaux
diﬀerentiable at every point v of a set B ⊆H, then we say that E is Gateaux
diﬀerentiable on B.
The reader should be able to prove the following without too much trouble.
Proposition 16.37 (uniqueness). Suppose that E : H →R is Gateaux diﬀeren-
tiable at the point v ∈H. Then the derivative is unique. The (uniquely deﬁned)
Gateaux derivative is denoted by E′[v] ∈H′, the same symbol as before.
Proof. See Problem 16.9.
Proposition 16.38 (Fr´echet implies Gateaux). If E : H →R is Fr´echet diﬀeren-
tiable at a point v ∈H, then it is Gateaux diﬀerentiable at v. The converse is not
true, however.
Proof. See Problem 16.10.
For diﬀerentiable functions, we have various ways to express convexity.
Proposition 16.39 (convexity equivalences). Let K ⊆H be nonempty and convex.
Assume that E : H →R is Fr´echet or Gateaux diﬀerentiable on K. The following
are equivalent.
1. E is convex on K.
2. E satisﬁes
E(v) ≥E(u) + ⟨E′[u], v −u⟩,
∀u, v ∈K.
3. E′ satisﬁes the monotonicity condition
⟨E′[v] −E′[u], v −u⟩≥0,
∀u, v ∈K.
Proof. See Problem 16.11.
There is a similar result for strict convexity.
6 Named in honor of the French mathematician Ren´e Eug`ene Gateaux (1889–1914).

16.3 The Euler Equation
465
Proposition 16.40 (strict convexity equivalences). Let K ⊆H be nonempty and
convex. Assume that E : H →R is Fr´echet or Gateaux diﬀerentiable on K. The
following are equivalent.
1. E is strictly convex on K.
2. E satisﬁes
E(v) > E(u) + ⟨E′[u], v −u⟩,
∀u, v ∈K,
u ̸= v.
3. E′ satisﬁes the strict monotonicity condition
⟨E′[v] −E′[u], v −u⟩> 0,
∀u, v ∈K,
u ̸= v.
Proof. See Problem 16.12.
Deﬁnition 16.41 (strong convexity). Suppose that E : H →R is Fr´echet or
Gateaux diﬀerentiable on the Hilbert space H. We say that E is strongly convex
if and only if there is a constant µ > 0 such that
µ ∥w −v∥2
H ≤⟨E′[w] −E′[v], w −v⟩
(16.9)
for all v, w ∈H, where ⟨· , · ⟩is the dual pairing between H′ and H. The positive
number µ is called the convexity constant. To emphasize the value of the convexity
constant in connection with the strong convexity property, we say that E is µ-
strongly convex on H.
Remark 16.42 (terminology). Other authors, for example Ciarlet [16], use the
term elliptic in place of strongly convex. The strong convexity property is also
equivalent to the property that the derivative is strongly monotone [5], another
common term.
Deﬁnition 16.43 (local Lipschitz smoothness). Suppose that E : H →R is Fr´echet
or Gateaux diﬀerentiable on the Hilbert space H. We say that E is locally —
Lipschitz smooth on H if and only if for every bounded convex set B, there is a
constant LB > 0 such that, for all w, v ∈B,
∥E′[w] −E′[v]∥H′ ≤LB∥w −v∥H.
(16.10)
The positive number LB is called the (local) Lipschitz constant. We say that E is
LB-Lipschitz smooth on B when we wish to emphasize the value of the Lipschitz
constant on the convex set B.
For µ-strongly convex functions, much more can be said about the solution of a
minimization problem.
Theorem 16.44 (Euler equation7). If E is µ–strongly convex on H, then, for all
w, v ∈H,
E(w) −E(v) ≥⟨E′[v], w −v⟩+ µ
2 ∥w −v∥2
H .
(16.11)
7 Named in honor of the Swiss mathematician, physicist, astronomer, geographer, logician, and
engineer Leonhard Euler (1707–1783).

466
Convex Optimization
Consequently, E is strictly convex and coercive. Furthermore, there is a unique
element u ∈H with the property that
E(u) ≤E(v),
∀v ∈H,
E(u) < E(v),
∀v ̸= u,
and this global minimizer satisﬁes Euler’s equation
⟨E′[u], w⟩= 0,
∀w ∈H.
(16.12)
Proof. Using Taylor’s Theorem with integral remainder (see Problem 16.13)
guarantees that
E(w) −E(v) =
Z 1
0
⟨E′[v + t(w −v)], w −v⟩dt
= ⟨E′[v], w −v⟩+
Z 1
0
⟨E′[v + t(w −v)] −E′[v], t(w −v)⟩
t
dt
≥⟨E′[v], w −v⟩+
Z 1
0
µ ∥t(w −v)∥2
H
t
dt
= ⟨E′[v], w −v⟩+ µ
2 ∥w −v∥2
H ,
which proves (16.11).
Inequality (16.11) implies that, for any w, v ∈H, w ̸= v,
E(w) > E(v) + ⟨E′[v], w −v⟩,
which is an equivalent deﬁnition of strict convexity.
We say that E is quadratically coercive if and only if there are constants C1 ∈R
and C2 > 0 such that
E(w) ≥C1 + C2 ∥w∥2
H ,
∀w ∈H.
Setting v = 0 in (16.11), we have
E(w) ≥E(0) + ⟨E′[0], w⟩+ µ
2 ∥w∥2
H
≥E(0) −∥E′[0]∥H′ ∥w∥H + µ
2 ∥w∥2
H
≥E(0) −1
µ ∥E′[0]∥2
H′ −µ
4 ∥w∥2
H + µ
2 ∥w∥2
H
= C1 + C2 ∥w∥2
H ,
where
C1 = E(0) −1
µ ∥E′[0]∥2
H′ ,
C2 = µ
4 .
Of course, it is clear that quadratic coercivity implies coercivity.
The existence and uniqueness of a minimizer follows from Theorem 16.31.
Having shown the existence and uniqueness of a minimizer, let us now prove that
this is equivalent to solving the Euler equation (16.12). First, suppose that u ∈H
satisﬁes Euler’s equation (16.12). Strict convexity implies that
E(w) > E(u) + ⟨E′[u], w −u⟩= E(u)

16.3 The Euler Equation
467
for all w ∈H, w ̸= u, and so it is a minimizer. On the other hand, suppose that
u ∈H is the unique global minimizer of E. Let v ∈H be arbitrary. Then, for s ̸= 0,
E(u + sv) −E(u) > 0.
Thus, the directional derivatives satisfy
lim
s↓0
1
s (E(u + sv) −E(u)) = ⟨E′[u], v⟩≥0
and
lim
s↑0
1
s (E(u + sv) −E(u)) = ⟨E′[u], v⟩≤0.
The only possibility is that ⟨E′[u], v⟩= 0. Since v ∈H was arbitrary, u solves the
Euler equation.
Deﬁnition 16.45 (bounded energy set). Fix u0 ∈H. The set
E0 = {v ∈H | E(v) ≤E(u0)}
(16.13)
is called the bounded energy set.
Proposition 16.46 (convexity). If E is convex on H, the bounded energy set E0 is
convex.
Proof. Suppose that v, w ∈E0. Then E(w) ≤E(u0) and E(v) ≤E(u0). Since E
is convex, for any t ∈[0, 1],
E(u0) ≥(1 −t)E(w) + tE(v) ≥E((1 −t)w + tv).
Thus, (1 −t)w + tv ∈E0 for any t ∈[0, 1].
If E is strongly convex and the Lipschitz smooth, then very useful estimates can
be shown.
Lemma 16.47 (two-sided estimate). Suppose that E is µ-strongly convex on H
and L-Lipschitz smooth on the bounded energy set E0. Then, for all v, w ∈E0,
µ ∥w −v∥2
H ≤⟨E′[w] −E′[v], w −v⟩≤L ∥w −v∥2
H .
Furthermore, the lower bound holds for all v, w ∈H.
Proof. The lower bound is just µ-strong convexity. To get the upper bound, observe
that, for all w, v ∈E0 and for any z ∈H,
|⟨E′[w] −E′[v], z⟩| ≤∥E′[w] −E′[v]∥H′ ∥z∥H ≤L ∥w −v∥H ∥z∥H .
Setting z = w −v gives the desired inequality.
Now we consider the relation between the energy and the norm centered at the
minimizer. The following estimates can be easily proved using Taylor’s Theorem
with integral remainder, as above; see, for example, [16].

468
Convex Optimization
Lemma 16.48 (quadratic energy trap). Suppose that E is µ-strongly convex on
H and L-Lipschitz smooth on the bounded energy set E0. Then, for all v, w ∈E0,
µ
2 ∥w −v∥2
H + ⟨E′[v], w −v⟩≤E(w) −E(v)
≤⟨E′[v], w −v⟩+ L
2 ∥w −v∥2
H .
(16.14)
Furthermore, the lower bound holds for all v, w ∈H. In addition, suppose that
u ∈E0 is the minimizer of E. Then, for all w ∈E0,
µ
2 ∥w −u∥2
H ≤E(w) −E(u) ≤L
2 ∥w −u∥2
H .
(16.15)
Again, the lower bound holds for all w ∈H.
Proof. The lower bound for (16.14) has been established in Theorem 16.44. For
the upper bound, we again apply Taylor’s Theorem with integral remainder and
use the fact that E0 is convex. The remaining details are left to the reader as an
exercise; see Problem 16.14.
If E is strongly convex, we can produce an upper bound for the energy that
consists of the norm of the Fr´echet (or Gateaux) derivative.
Lemma 16.49 (upper bound). Suppose that E is µ-strongly convex on H and
u ∈H is the minimizer of E. Then, for all v ∈H, we have
0 ≤E(v) −E(u) ≤1
2µ∥E′[v]∥2
H′.
(16.16)
Proof. Fix the point v ∈H. Now, for any w ∈H, using the lower bound of (16.14),
we have
E(w) ≥E(v) + ⟨E′[v], w −v⟩+ µ
2 ∥w −v∥2
H = g(w).
For ﬁxed v ∈H, the minimizer of g is w ∗= v −1
µRE′[v], where RE′[v] is the
Riesz representation in H of E′[v]. Therefore,
E(w) ≥g(w) ≥g(w ∗) = E(v) −1
2µ∥RE′[v]∥2
H = E(v) −1
2µ∥E′[v]∥2
H′.
Then (16.16) is obtained by letting w = u in the above inequality.
We shall often use the following simple variant of Lemma 16.48.
Lemma 16.50 (convexity of energy sections). Suppose that E is µ-strongly convex
on H and L-Lipschitz smooth on E0. Let ξ ∈E0 be arbitrary. Assume that W ⊆H
is a subspace. Deﬁne the function
J(w) = E(ξ + w),
∀w ∈W,
which we refer to as an energy section (or energy slice) of E. Then J : W →R is
diﬀerentiable and strongly convex, and there exists a unique element η ∈W such
that ξ + η ∈E0, η is the unique global minimizer of J, and
⟨E′[ξ + η], w⟩= ⟨J′[η], w⟩= 0,
∀w ∈W.

16.4 Preconditioners and Gradient Descent Methods
469
Furthermore, for all w ∈W with w + ξ ∈E0,
µ
2 ∥w −η∥2
H ≤J(w) −J(η) = E(ξ + w) −E(ξ + η) ≤L
2 ∥w −η∥2
H .
The lower bound holds for any w ∈W, without restriction.
Proof. See Problem 16.15.
16.4
Preconditioners and Gradient Descent Methods
Next, we introduce an important operator, the preconditioner, and we use this to
deﬁne a class of gradient descent methods.
Lemma 16.51 (preconditioner). Suppose that L: H →H′ is a linear and
symmetric operator, in the sense that
⟨L[v], w⟩= L[v](w) = L[w](v) = ⟨L[w], v⟩,
∀v, w ∈H.
Assume further that L is:
1. Coercive, i.e., there exists a constant µ1 such that, for all v, w ∈H,
⟨L[w], w⟩≥µ1∥w∥2
H.
2. Continuous, i.e., there exists a constant L1 > 0 such that, for all v, w ∈H,
∥L[w]∥H′ ≤L1∥w∥H.
Then, for any w ∈H,
µ1 ∥w∥2
H ≤⟨L[w], w⟩≤L1 ∥w∥2
H ,
and, for all v, w ∈H,
µ1
2 ∥w −v∥2
H + ⟨L[v], w −v⟩≤E1(w) −E1(v) ≤⟨L[v], w −v⟩+ L1
2 ∥w −v∥2
H ,
where E1 is the quadratic energy
E1(v) = 1
2⟨L[v], v⟩.
Proof. As before, the proof of the second set of estimates uses Taylor’s Theorem
with integral remainder; see Problem 16.16.
Deﬁnition 16.52 (preconditioner). A linear operator L: H →H′ that satisﬁes the
hypotheses of the lemma is called a preconditioner or a preconditioning operator.
When we wish to emphasize the values of µ1 and L1, we say that L is a (µ1, L1)-
preconditioner.
Lemma 16.53 (properties of L). Let L: H →H′ be a preconditioner. The object
(u, v)L = ⟨L[u], v⟩,
∀u, v ∈H
is an inner product on the Hilbert space H, and the norm that it induces is equivalent
to the canonical norm ∥·∥H. Consequently, L is invertible.

470
Convex Optimization
Proof. The proof of the ﬁrst fact is left to the reader; see Problem 16.17. Now,
since (·, ·)L is an inner product on H, by the Riesz Representation Theorem 16.14,
there is a unique element RLf ∈H such that
(RLf , w)L = ⟨f , w⟩,
∀w ∈H.
The map RL : H′ →H is called the Riesz map subordinate to L, and we are
justiﬁed in writing L−1 = RL.
Deﬁnition 16.54 (L−1-inner product). Suppose that L: H →H′ is a precondi-
tioner. The object
(f , g)L−1 = (RLf , RLg)L ,
∀f , g ∈H′
is called the L−1-inner product. The associated L−1-norm is deﬁned as
∥f ∥L−1 =
q
(f , f )L−1,
∀f ∈H′.
Proposition 16.55 (L−1-inner product). Suppose that L: H →H′ is a precondi-
tioner. Then (·, ·)L−1 is an inner product on H′.
Proof. See Problem 16.18.
Deﬁnition 16.56 (gradient descent). Suppose that E : H →R is Fr´echet
diﬀerentiable on a convex open subset B of the Hilbert space H. Let u0 ∈B
be given. A gradient descent method is an algorithm for the construction of the
sequence {uk}∞
k=0 ⊂B according to the following rule: given uk ∈B, compute
uk+1 via
uk+1 = uk −αkL−1
k E′[uk],
where αk > 0 is the step size and Lk : H →H′ is a family of symmetric, coercive,
and continuous operators, i.e., a family of preconditioners. We also write
uk+1 = uk + εk,
where
εk = αksk ∈H
is called the correction;
sk = L−1
k rk ∈H
is called the search direction; and
rk = −E′[uk] ∈H′
is called the residual. We say that the algorithm is well deﬁned if and only if
uk+1 ∈B whenever uk ∈B. The algorithm is convergent if and only if it is well
deﬁned, there is a point u ∈B such that uk →u, and u is at least a stationary
point of E in B, i.e., E′[u] = 0.

16.5 The Golden Key
471
Example 16.6
We can take L−1
k
= R: H′ →H, the canonical Riesz map. This
means that we precondition with whatever canonical metric we have chosen for the
Hilbert space. The reader can check that Lk satisﬁes all of the requirements of a
preconditioner. In this case, µ1 = L1 = 1, as the reader can conﬁrm.
16.5
The Golden Key
We will now study the convergence of the gradient descent method of Deﬁnition
16.56. To do so, we will utilize the following simple result in the proofs of
convergence.
Theorem 16.57 (linear convergence). Suppose that {dk}∞
k=0, {δk}∞
k=0, {βk}∞
k=0
are sequences of nonnegative real numbers, the ﬁrst two having the relationship
δk = dk −dk+1,
k = 0, 1, 2, . . . .
Assume that there are constants CL, CU > 0, independent of k, such that
CLβk ≤δk
and
dk+1 ≤CUβk.
Then
dk+1 ≤
CU
CL + CU
dk,
k = 0, 1, 2, . . . .
(16.17)
Consequently, {dk}∞
k=0 converges monotonically and (at least) linearly to zero.
Proof. Observe that
dk+1 ≤CUβk = CU
CL
CLβk ≤CU
CL
δk = CU
CL
(dk −dk+1),
which implies (16.17). Since ρ =
CU
CL+CU ∈(0, 1), the sequence {dk}∞
k=0 is strictly
decreasing. Moreover, it is bounded from below by zero. Thus, the Monotone
Convergence Theorem B.7 implies that dk →γ ≥0. Suppose that γ > 0, to get
a contradiction. There is a positive integer K such that, if k ≥K,
γ ≤dk < dK ≤γ + (1 −ρ)γ
2ρ
.
Therefore,
dk+1 ≤ρdk ≤ργ + 1
2(1 −ρ)γ = 1
2(ρ + 1)γ < γ.
This is a contradiction. The only possibility is that γ = 0.
We will apply the last result with the following deﬁnitions:
dk = E(uk) −E(u),
δk = E(uk) −E(uk+1).
(16.18)

472
Convex Optimization
v
E(v)
u
uk+1
uk
E(u)
E(uk+1)
E(uk)
dk+1
dk
δk
Figure 16.1 The sequences {dk}∞
k=0 and {δk}∞
k=0 used in the proof of the golden key;
Corollary 16.58.
The quantity dk is the diﬀerence between the current energy and the minimum
energy, and δk is the energy decrease associated with the (k + 1)st iteration. They
are connected, as desired, by the trivial identity
δk = dk −dk+1.
We deﬁne αk in terms of the subspace corrections via
βk = ∥εk∥2
H,
and we assume the following upper and lower bounds.
• Lower bound on corrections. There exists a positive constant CL such that, for
any k = 0, 1, 2, . . .,
E(uk) −E(uk+1) = δk ≥CLβk = CL∥εk∥2
H.
(16.19)
• Upper bound on corrections. There exists a positive constant CU such that, for
any k = 0, 1, 2, . . .,
E(uk+1) −E(u) = dk+1 ≤CUβk = CU∥εk∥2
H.
(16.20)
These sequences, with the respective bounds, are depicted in Figure 16.1. If
these bounds hold, then, as a corollary to Theorem 16.57, we have the following
convergence result.
Corollary 16.58 (golden key). Assume that the lower bound (16.19) and upper
bound (16.20) hold with positive constants CL and CU, respectively. We then have
E(uk+1) −E(u) ≤ρ (E(uk) −E(u)) ,
ρ =
CU
CL + CU
,

16.6 Preconditioned Steepest Descent Method
473
and E(uk) converges monotonically and (at least) linearly to E(u), at the linear
rate ρ. Furthermore, {uk}∞
k=0 converges at least linearly to u.
Proof. The linear convergence of E(uk) to E(u) at the rate ρ is guaranteed by
Theorem 16.57. Using (16.15), with w = uk, we have
µ
2 ∥uk −u∥2
H ≤E(uk) −E(u),
which guarantees the linear convergence of uk to u.
As we see, the essence of the analysis reduces to ﬁnding suitable bounds (16.19)
and (16.20). Let us now obtain these for several particular instances.
16.6
Preconditioned Steepest Descent Method
The ﬁrst gradient descent method that we examine is the preconditioned steepest
descent (PSD) method, which is listed in Deﬁnition 16.59. There are two
characteristics that make the PSD method a gradient descent method, as follows.
Deﬁnition 16.59 (PSD). Suppose that B is an open and convex subset of
the Hilbert space H, E : H →R is Fr´echet or Gateaux diﬀerentiable on B, and
L: H →H′ is a ﬁxed, invariant preconditioner. The preconditioned steepest
descent (PSD) algorithm is a gradient descent method for which Lk = L, i.e.,
the search direction is computed as the solution to
⟨sk, w⟩L = L[sk](w) = ⟨rk, w⟩,
∀w ∈H,
(16.21)
and the step size is computed via line search along the search direction, i.e.,
εk = α∗
ksk,
(16.22)
where
α∗
k = argmin
α∈R
E(uk + αsk) = argzero
α∈R
⟨E′[uk + αsk], sk⟩.
(16.23)
The correction εk is then applied via
uk+1 = uk + εk.
(16.24)
Remark 16.60 (PSD). We ﬁrst observe that the PSD method of Deﬁnition 16.59
is a gradient descent algorithm where the preconditioner is invariant. The “steepest
descent” part of the name refers to the orthogonalization step (16.23).
A ﬁrst step in the analysis of PSD is to show that it is well deﬁned. We have the
following result.
Theorem 16.61 (well deﬁned). Suppose that E : H →R is µ-strongly convex
on the Hilbert space H and L: H →H′ is a ﬁxed, invariant preconditioner. Let
u0 ∈H be arbitrary. Suppose that E0 is the (convex) bounded energy set deﬁned
in (16.13). Then the PSD algorithm in Deﬁnition 16.59 is well deﬁned on E0.

474
Convex Optimization
Proof. See Problem 16.19.
To prove convergence of the PSD algorithm, we use the golden key of Corollary
16.58, which requires a certain lower bound and a certain upper bound. The lower
bound is quite easy.
Theorem 16.62 (lower bound). Suppose that E is µ-strongly convex on H and L
is a (µ1, L1)-preconditioner. Let uk be the kth iteration in the PSD algorithm of
Deﬁnition 16.59. Then
δk = E(uk) −E(uk+1) ≥µ
2 ∥εk∥2
H.
Proof. Owing to the line search (steepest descent step), we have the orthogonality
property
⟨E′[uk+1], w⟩= 0,
w ∈span{sk};
(16.25)
see Problem 16.20. Applying Lemma 16.50, with the subspace W = span{sk}, and
noting that
uk+1 −uk = εk = α∗
ksk ∈span{sk},
we have
E(uk) −E(uk+1) ≥µ
2 ∥uk −uk+1∥2
H = µ
2 ∥εk∥2
H.
The upper bound requires some preliminary estimates for α∗
k.
Lemma 16.63 (descent). Let sk be computed as in Deﬁnition 16.59. Suppose that
L is a (µ1, L1)-preconditioner. Then sk is a descent direction in the sense that
⟨rk, sk⟩= ⟨−E′[uk], sk⟩≥µ1∥sk∥2
H > 0.
Proof. Recall the search direction problem: ﬁnd sk ∈H such that
⟨L[sk], w⟩= −⟨E′[uk], w⟩,
∀w ∈H.
(16.26)
Choosing w = sk, we obtain the inequality
⟨−E′[uk], sk⟩= ⟨L[sk], sk⟩≥µ1∥sk∥2
H > 0.
In order to better understand the choice of the step size, we introduce the scalar
function fk; see Figure 16.2.
Proposition 16.64 (energy section). Suppose that E is µ-strongly convex on H
and L is a (µ1, L1)-preconditioner. Deﬁne the one-dimensional energy section
fk(α) = E(uk + αsk).
(16.27)
Then
f ′
k(0) = ⟨E′[uk], sk⟩≤−µ1 ∥sk∥2
H .
Furthermore, α∗
k > 0 and, for all α ∈(0, α∗
k], fk(α) < fk(0).

16.6 Preconditioned Steepest Descent Method
475
α
fk(α)
αo
L,k
fk(αo
L,k)
α∗
k
fk(0)
fk(α∗
k)
αL,k
Figure 16.2 The function fk is a one-dimensional energy section, a slice of the energy
along the search direction sk. It is straightforward to prove that its minimizer, α∗
k, is
positive.
Proof. Lemma 16.63 implies that f ′
k(0) < 0. As f ′
k is continuous, we conclude that
the minimizing point is positive, α∗
k > 0, and, for all α ∈(0, α∗
k], fk(α) < fk(0) =
E(uk).
It turns out that the energy sections fk inherit many properties from the original
energy E, as the following result shows.
Lemma 16.65 (properties of fk). Assume that E is µ-strongly convex on H and
L-Lipschitz smooth on the bounded energy set E0. Then fk, deﬁned in (16.27), is
diﬀerentiable and strongly convex in the following sense: for all α, β ∈R,
(f ′
k(α) −f ′
k(β))(α −β) ≥(α −β)2µ∥sk∥2
H.
Furthermore, f ′
k is Lipschitz in the following sense: for all α, β ∈[0, αL,k],
|f ′
k(α) −f ′
k(β)| ≤L∥sk∥2
H|α −β|,
where αL,k = (1 +
p
µ/L)α∗
k.
Proof. The proof is based on the following identity:
f ′
k(α) −f ′
k(β) = ⟨E′[uk + αsk] −E′[uk + βsk], sk⟩.
Then, by µ-strong convexity,
(f ′
k(α) −f ′
k(β))(α −β) = ⟨E′[uk + αsk] −E′[uk + βsk], αsk −βsk⟩
≥µ∥(α −β)sk∥2
H.
To use the Lipschitz inequality involving E′, we need to ensure that the points
of evaluation are inside the set E0, which imposes an upper bound on α and β.

476
Convex Optimization
As f ′
k(0) < 0 and f ′
k(α∗
k) = 0, by coercivity, there exists αo
L,k > α∗
k such that
fk(0) = fk(αo
L,k) and, for all α ∈(0, αo
L,k), fk(α) < fk(0); see Figure 16.2.
This implies that uk + αsk ∈E0 for all α ∈(0, αo
L,k), since the energy E is
decreased for such values of α. We now estimate αo
L,k. As f ′
k(α∗
k) = 0 and f ′
k is
Lipschitz in (0, αo
L,k), we have, from Lemma 16.50, the bound
0 < fk(αo
L,k) −fk(α∗
i ) = E(uk + αo
L,ksk) −E(uk + α∗
ksk) ≤L
2
αo
L,ksk −α∗
ksk
2
H
= (αo
L,k −α∗
k)2 L
2 ∥sk∥2
H.
On the other hand, and again from Lemma 16.50,
fk(αo
L,k) −fk(α∗
k) = fk(0) −fk(α∗
k) ≥µ(α∗
k)2
2
∥sk∥2
H.
The desired bound,
αo
L,k ≥αL,k =

1 +
r
µ
L

α∗
k > α∗
k > 0,
then follows. To ﬁnish up, since E′ is Lipschitz, f ′
k is Lipschitz with constant L∥sk∥2
H
on the interval [0, αo
L,k]. Indeed, for all α, β ∈(0, αo
L,k), α ̸= β,
|f ′
k(α) −f ′
k(β)| = |⟨E′[uk + αsk] −E′[uk + βsk], sk⟩|
=
1
|α −β||⟨E′[uk + αsk] −E′[uk + βsk], (α −β)sk⟩|
≤
1
|α −β|L∥(α −β)sk∥2
H = L∥sk∥2
H|α −β|.
It is also Lipschitz with the same constant on the smaller interval [0, αL,k] ⊆
[0, αo
L,k]. The proof is complete.
Remark 16.66 (computability). The importance of using the right-hand endpoint
αL,k =

1 +
r
µ
L

α∗
k,
rather than αo
L,k, is that αL,k is computable, provided that µ, L, and α∗
k are known.
We know that the optimal value α∗
k is positive. Let us provide a reﬁned lower
bound; see Figure 16.2.
Lemma 16.67 (reﬁned lower bound). Assume that the energy E is µ-strongly
convex on H and L-Lipschitz smooth on the bounded energy set E0 and L is a
(µ1, L1)-preconditioner. Then we have the lower bound
µ1
L ≤α∗
k.
Consequently,
αo
L,k ≥αL,k =

1 +
r
µ
L

α∗
k > α∗
k ≥µ1
L > 0.

16.6 Preconditioned Steepest Descent Method
477
Proof. Recall that εk = α∗
ksk ∈span{sk} and, owing to the line search, we still
have the orthogonality property (16.25). Thus, E′[uk + εk] = 0 in the dual of
span{sk}. The choice of search direction in Deﬁnition 16.59 implies that, in H′,
we have
−E′[uk] = L[sk].
The lower bound is obtained by the coercivity of L and Lipschitz continuity of E′:
α∗
kL∥sk∥2
H = 1
α∗
k
L∥εk∥2
H
≥1
α∗
k
⟨E′[uk + εk] −E′[uk], εk⟩
= ⟨E′[uk + εk] −E′[uk], sk⟩
= −⟨E′[uk], sk⟩
= ⟨L[sk], sk⟩
≥µ1∥sk∥2
H.
Note that uk + εk ∈E0 by Lemma 16.65, so that we can use Lipschitz continuity
of E′.
Next, we present a reﬁned upper bound on the step size α∗
k.
Lemma 16.68 (reﬁned upper bound). Assume that the energy E is µ-strongly
convex on H and L-Lipschitz smooth on E0 and L is a (µ1, L1)-preconditioner.
Then we have the upper bound
α∗
k ≤L1
µ .
Proof. Since E′[uk + εk] = 0 in the dual space of span{sk},
µ∥εk∥2
H ≤⟨E′[uk + εk] −E′[uk], εk⟩= ⟨L[sk], εk⟩≤L1
α∗
k
∥εk∥2
H.
With our estimates of α∗
k in place, we are now ready to establish the upper bound
for the PSD algorithm of Deﬁnition 16.59 needed by the golden key of Corollary
16.58.
Theorem 16.69 (upper bound). Suppose that the energy E is µ-strongly convex
on H and L-Lipschitz smooth on the set E0 and L is a (µ1, L1)-preconditioner.
Then we have the upper bound
E(uk+1) −E(u) ≤CU∥εk∥2
H,
where
CU = [L (1 + L1/µ1)]2
2µ
.
Proof. Note that, for any w ∈H,
⟨E′[uk+1], w⟩= ⟨E′[uk+1] −E′[uk], w⟩+ ⟨E′[uk], w⟩= I1 + I2,

478
Convex Optimization
where
I1 = ⟨E′[uk+1] −E′[uk], w⟩,
I2 = ⟨E′[uk], w⟩.
Since we know a priori that uk, uk+1 ∈E0, we can use the Lipschitz smoothness of
E to get
I1 ≤L ∥εk∥H∥w∥H.
For I2, we have
I2 = −⟨L[sk], w⟩
≤|⟨L[sk], w⟩|
≤∥L[sk]∥H′ ∥w∥H
≤L1 ∥sk∥H ∥w∥H
≤L1
α∗
k
∥εk∥H ∥w∥H
≤L1L
µ1
∥εk∥H ∥w∥H .
In the last estimate, we used the relation sk = α∗
k
−1εk and the lower bound of α∗
k
given in Lemma 16.67.
Putting the estimates together, we have, for any w ∈H,
⟨E′[uk+1], w⟩≤L

1 + L1
µ1

∥εk∥H∥w∥H,
which implies that
∥E′[uk+1]∥2
H′ ≤L2

1 + L1
µ1
2
∥εk∥2
H.
Using inequality (16.16) in Lemma 16.49 with v = uk+1, the result follows.
Using Theorems 16.62 and 16.69, and the golden key of Corollary 16.58, we
obtain the following linear convergence result.
Corollary 16.70 (linear convergence). Let uk be the kth iteration and uk+1 the next
iteration in the PSD algorithm. Suppose that the energy E is µ-strongly convex on
H and L-Lipschitz smooth on the set E0 and L is a (µ1, L1)-preconditioner. Then
E(uk+1) −E(u) ≤ρ(E(uk) −E(u)),
with
ρ =
L2 (1 + L1/µ1)2
L2 (1 + L1/µ1)2 + µ2 .
If we take L = R−1, then µ1 = L1 = 1 and
ρ =
4L2
µ2 + 4L2 .
Proof. See Problem 16.21.

16.7 PSD with Approximate Line Search
479
fk(α)
qk(α)
α
αo
L,k
fk(αo
L,k)
α∗
k
αq
k
fk(0)
fk(α∗
k)
qk(αq
k)
αL,k
Figure 16.3 The function fk, deﬁned in (16.27), and its quadratic approximation qk,
which is deﬁned in (16.28). The quadratic minimizer, αq
k, is always to the left of α∗
k by
construction.
16.7
PSD with Approximate Line Search
Next, we introduce another preconditioned gradient descent method. This one,
which we will call the preconditioned steepest descent method with approximate
line search (PSD-ALS) — a real mouthful — is pretty much as the name suggests.
For this, we need a quadratic approximation of the function fk, which was deﬁned
in (16.27). Before we go any further, we should point out that, in the optimization
literature, this method is usually just called the gradient descent method or
preconditioned gradient descent method; see, for example, [64].
From its deﬁnition, we see that fk(0) = E(uk), f ′
k(0) = ⟨E′[uk], sk⟩< 0. Using
fk(0) and f ′
k(0), we deﬁne the quadratic function
qk(α) = fk(0) + f ′
k(0)α + L∥sk∥2
H
2
α2;
(16.28)
see Figure 16.3. The optimal step size for PSD is, of course, α∗
k = argminα∈R fk(α).
Our choice for this new algorithm is
αq
k = argmin
α∈R
qk(α) = −⟨E′[uk], sk⟩
L∥sk∥2
H
= −f ′
k(0)
L∥sk∥2
H
,
(16.29)
which satisﬁes the following estimate.

480
Convex Optimization
Lemma 16.71 (two-sided bound). Assume that the energy E is L-Lipschitz smooth
on the bounded energy set E0 and L is a (µ1, L1)-preconditioner. Then
µ1
L ≤αq
k ≤α∗
k.
Proof. The lower bound is obtained by the deﬁnition of αq
k and Lemma 16.63. To
prove the upper bound, we notice that, owing to line search,
f ′
k(α∗
k) = ⟨E′[uk + α∗
ksk], sk⟩= 0
and, thus,
αq
kL∥sk∥2
H = −⟨E′[uk], sk⟩= ⟨E′[uk + α∗
ksk] −E′[uk], sk⟩≤α∗
kL∥sk∥2
H.
Deﬁnition 16.72 (PSD-ALS). Suppose that B is an open convex subset of the
Hilbert space H, the energy E : H →R is Fr´echet or Gateaux diﬀerentiable on
B, and L: H →H′ is a ﬁxed, invariant preconditioner. The preconditioned
steepest descent with approximate line search (PSD-ALS) algorithm is a
gradient descent method for which Lk = L, and the step size is computed via
the quadratic approximation (16.29), i.e.,
εk = αq
ksk,
αq
k = ⟨rk, sk⟩
L∥sk∥2
H
.
(16.30)
Thus,
uk+1 = uk + εk,
εk = αq
ksk.
Let us now elucidate the properties of the PSD-ALS algorithm. We will show
that it is well deﬁned and convergent.
Remark 16.73. In the literature, the PSD-ALS method is also called the
preconditioned gradient descent method (PGD); see [64].
Theorem 16.74 (well deﬁned). Suppose that E : H →R is µ-strongly convex
on the Hilbert space H and L: H →H′ is a ﬁxed, invariant preconditioner. Let
u0 ∈H be arbitrary. Suppose that E0 is the (convex) bounded energy set deﬁned
in (16.13). Then the PSD-ALS algorithm of Deﬁnition 16.72 is well deﬁned on E0.
Proof. See Problem 16.22.
Let us now show that, as Figure 16.3 suggests, the quadratic approximation qk
of fk is larger than the energy section.
Lemma 16.75 (upper approximation). Suppose that the energy E is L-Lipschitz
on the bounded energy set E0. Then
fk(α) ≤qk(α),
∀α ∈[0, αL,k].

16.7 PSD with Approximate Line Search
481
Proof. By Lemma 16.65, for α ∈[0, αL,k], f ′
k is Lipschitz continuous with constant
L∥sk∥2
H. We recall a classical result: since f ′
k is Lipschitz continuous with constant
L∥sk∥2
H on α ∈[0, αL,k], then
|fk(α) −fk(β) −(α −β)f ′
k(β)| ≤L∥sk∥2
H
2
|α −β|2,
∀α, β ∈[0, αL,k];
(16.31)
see Problem 16.23. Setting β = 0 above, we get, for all α ∈[0, αL,k],
fk(α) −qk(α) = fk(α) −fk(0) −αf ′
k(0) −L∥sk∥2
H
2
α2
≤|fk(α) −fk(0) −αf ′
k(0)| −L∥sk∥2
H
2
α2
≤L∥sk∥2
H
2
α2 −L∥sk∥2
H
2
α2
= 0,
which implies the result.
Now, since the optimal linear search procedure is broken, the orthogonality
condition (16.25) with respect to the corrections is broken. Thus, establishing
the lower bound is a little more complicated.
Theorem 16.76 (lower bound). Let {uk}∞
k=0 be computed by the PSD-ALS
algorithm given in Deﬁnition 16.72. Suppose that E is µ-strongly convex on H
and L-Lipschitz smooth on E0 and L is a (µ1, L1)-preconditioner. Then we have
E(uk) −E(uk+1) ≥CL∥αq
ksk∥2
H,
CL = L
2 .
Proof. It suﬃces to prove that
E(uk) −E(uk+1) = fk(0) −fk(αq
k) ≥L
2 ∥αq
ksk∥2.
By Lemma 16.75, fk(α) ≤qk(α) for all α ∈[0, αL,k]. As αq
k = argminα∈R qk(α)
and αq
k ≤α∗
k, we get
fk(αq
k) ≤qk(αq
k) = min
α∈R qk(α) = fk(0) −
1
2L∥sk∥2
H
|f ′
k(0)|2 = fk(0) −L
2 ∥αq
ksk∥2
H.
In the last step, we have used the deﬁnition of αq
k and this completes the proof.
Since αq
k has the same lower bound as α∗
k, we can derive the upper bound in
exactly the same way as the proof of Theorem 16.69, only replacing εk = α∗
ksk by
αq
ksk and using the lower bound from Lemma 16.71.
Theorem 16.77 (upper bound). Let uk, uk+1 ∈H be two consecutive iterations of
the PSD-ALS algorithm of Deﬁnition 16.72. Suppose that E is µ-strongly convex
on H and L-Lipschitz smooth on E0 and L is a (µ1, L1)-preconditioner. Then we
have the upper bound
E(uk+1) −E(u) ≤CU∥αq
ksk∥2
H,
CU = L2 (1 + L1/µ1)2
2µ
.

482
Convex Optimization
Proof. See Problem 16.24.
Finally, using the golden key of Corollary 16.58, we can easily compute the linear
rate of convergence.
Corollary 16.78 (convergence). Let uk, uk+1 ∈H be two consecutive iterations of
the PSD-ALS algorithm of Deﬁnition 16.72. Suppose that E is µ-strongly convex
on H and L-Lipschitz smooth on E0 and L is a (µ1, L1)-preconditioner. Then we
have
E(uk+1) −E(u) ≤ρ(E(uk) −E(u))
with
ρ =
L2 (1 + L1/µ1)2
L2 (1 + L1/µ1)2 + Lµ
.
Proof. See Problem 16.25.
16.8
Newton’s Method
In Section 15.5, we considered Newton’s method with the purpose of solving a
nonlinear system of d ∈N equations in d unknowns. Given f : Rd →Rd, we wish
to ﬁnd a solution to
f (x) = 0.
Here, we will explore how we can use Newton’s method to solve optimization
problems as well. The idea is to think about solving the Euler equation,
⟨E′[v], w⟩= 0,
∀w ∈H,
with the method. This might not be a ﬁnite-dimensional problem anymore, but
this is not a problem. Newton’s method continues to make sense for an inﬁnite-
dimensional Hilbert space H. To see this, we observe that the Euler equation
is actually an equation on H′. Indeed, we deﬁne the function F : H →H′ as
F(v) = E′[v], so that we are seeking
F(v) = 0 ∈H′.
Thus, Newton’s method, at this formal stage, reads
F ′[uk](uk+1 −uk) = −F(uk).
However, here we see two issues. First, we must make sense of F ′, where F maps
an inﬁnite-dimensional normed space to another. Second, since the function F itself
is the derivative of a functional, we must deﬁne higher order derivatives. Let us
then present some facts about this theory.
First, we generalize the concept of Fr´echet diﬀerentiability, introduced in
Deﬁnition 16.32 for mappings between normed spaces.

16.8 Newton’s Method
483
Deﬁnition 16.79 (Fr´echet derivative). Let V, W be normed spaces and F : V →W.
We say that F if Fr´echet diﬀerentiable at the point v ∈V if and only if there
exists T ∈B(V, W) such that
lim
∥h∥V→0
∥F(v + h) −F(v) −T(h)∥W
∥h∥V
→0.
If F is Fr´echet diﬀerentiable at every point of a set B ⊆V, then we say that F is
Fr´echet diﬀerentiable on B.
As in Proposition 16.33, we can show that, if it exists, the Fr´echet derivative
is unique, and so we denote this as F ′[v] ∈B(V, W). Observe now that, if H
is a Hilbert space and E : H →R is a convex energy that, in addition, is Fr´echet
diﬀerentiable, we can then deﬁne F : H →H′ via F(v) = E′[v]. With the deﬁnition
given above, we can then speak about the diﬀerentiability of F, which in turn will
give us the second derivatives of the energy E.
Deﬁnition 16.80 (second derivative). Let H be a Hilbert space and E : H →R
be an energy that is Fr´echet diﬀerentiable on H. If the mapping
E′ : H →H′,
v ∈H 7→E′[v] ∈H′
is Fr´echet diﬀerentiable at v ∈H, then its derivative
E′′[v] ∈B(H, B(H, R)) = B(H, H′)
is called the second Fr´echet derivative of E at v, and E is said to be twice Fr´echet
diﬀerentiable at the point v. If the energy E is twice diﬀerentiable at every point
of a set B ⊆H, then we say that E is twice Fr´echet diﬀerentiable on B.
Remark 16.81 (notation). The deﬁnition of the second derivative and what it
represents requires some explanation. First, we noted that B(H, R) = H′, a fact
that easily follows from the deﬁnition. More importantly, we noted that, for v ∈H,
we have
E′′[v] ∈B(H, H′).
This means that, given w ∈H, we can deﬁne the continuous linear functional
E′′[v](w) ∈H′, so that
E′′[v](w, x) = ⟨E′′[v](w), x⟩,
∀x ∈H
is meaningful and it satisﬁes
|E′′[v](w, x)| ≤∥E′′[v](w)∥H′∥x∥H ≤∥E′′[v]∥B(H,H′)∥w∥H∥x∥H.
Finally, we note that, a priori, the order of the arguments in E′′[v](·, ·) matters.
We will see below, however, that this is not the case.
Before we deﬁne Newton’s method, it is useful to have some assumptions on
the second derivative.

484
Convex Optimization
Proposition 16.82 (second derivative). Suppose that E : H →R is twice Fr´echet
diﬀerentiable on H. Assume that:
1. E′′ is µ-coercive on H, i.e., for all v ∈H, there is a constant µ > 0 such that
µ ∥w∥2
H ≤E′′[v](w, w),
∀w ∈H.
2. E′′ is locally uniformly continuous on H, i.e., given any bounded convex subset
B ⊂H, there is a constant LB > 0 such that, for all v ∈B,
E′′[v](w, x) ≤LB ∥w∥H ∥x∥H ,
∀w, x ∈H.
Then E′′ is symmetric, in the sense that
E′′[v](w, x) = E′′[v](x, w),
∀v, w, x ∈H,
and, for a ﬁxed v ∈H, the second derivative E′′[v](·, ·) is bilinear. Furthermore, E
is µ-strongly convex and locally Lipschitz smooth on H.
Proof. The symmetry and bilinearity are standard properties of the second Fr´echet
derivative and can be found in the literature under the name Schwarz Lemma.8
The last two properties follow from Problem 16.26. First, let us ﬁx w ∈H. For
every v ∈H, there is a point x ∈H on the line segment between w and v such
that
E(v) = E(w) + E′[w](v −w) + 1
2E′′[x](v −w, v −w).
(16.32)
Thus, for any v, w ∈H,
E(v) −E(w) −E′[w](v −w) = 1
2E′′[x](v −w, v −w) ≥µ
2 ∥v −w∥2
H .
Reversing the roles of v and w above, we have
E(w) −E(v) −E′[v](w −v) ≥µ
2 ∥v −w∥2
H .
Adding these inequalities gives
⟨E′[v] −E′[w], v −w⟩≥µ ∥v −w∥2
H ,
which implies that E is µ-strongly convex.
For the upper bound, let us use the following version of Taylor’s Theorem: ﬁx
w ∈B. For every v ∈B, there is a point x ∈B on the line segment between w
and v such that
⟨E′[v], z⟩= ⟨E′[w], z⟩+ E′′[x](w −v, z),
∀z ∈H;
(16.33)
see Problem 16.27. Thus,
⟨E′[v] −E′[w], z⟩= E′′[x](w −v, z) ≤LB ∥w −v∥H ∥z∥H ,
∀z ∈H.
8 Named in honor of the German mathematician Karl Hermann Amandus Schwarz
(1843–1921).

16.8 Newton’s Method
485
Consequently,
⟨E′[v] −E′[w], z⟩
∥z∥H
≤LB ∥w −v∥H ,
∀z ∈H,
which implies that, for any v, w ∈B,
∥E′[v] −E′[w]∥H′ ≤LB ∥w −v∥H .
We are now ready to deﬁne Newton’s method.
Deﬁnition 16.83 (Newton’s method). Suppose that B is an open convex subset of
the Hilbert space H and E : H →R is twice Fr´echet diﬀerentiable on B. Newton’s
method is a type of gradient descent method for which the preconditioner Lk
changes at every iteration. It is given by
Lk[v](·) = E′′[uk](v, ·),
∀v ∈H,
i.e., the search direction is computed as the solution to
E′′[uk](sk, w) = ⟨rk, w⟩,
∀w ∈H,
(16.34)
and the step size is αk = 1, which implies that the correction and the search
direction are the same: εk = sk. To sum up, Newton’s method can be expressed as
E′′[uk](uk+1 −uk, w) = −E′[uk](w),
∀w ∈H.
We immediately notice that, for Newton’s method to be well deﬁned, we require
E′′[uk](·, ·) to be invertible. We will see that this is the case for a certain class of
problems.
Lemma 16.84 (well deﬁned). Suppose that E : H →R is twice Fr´echet diﬀeren-
tiable on H, E′′ is µ-coercive on H, and E′′ is locally uniformly continuous on H.
Let u0 ∈H be arbitrary. Suppose that E0 is the (convex) bounded energy set
deﬁned in (16.13). Assume that uniform continuity constant for E′′ on E0 is L > 0.
Then, for any uk ∈E0, (16.34) always has a unique solution.
Proof. Because of the assumptions on E′′, we can use the Riesz Representation
Theorem to prove the unique existence of the solution sk ∈H to the problem
deﬁned in (16.34). This is because E′′[uk](·, ·) deﬁnes an inner product on H,
equivalent to the canonical inner product.
The previous statement is far from giving a complete answer to the properties
of Newton’s method. It only says that we can advance one iteration. The complete
analysis of Newton’s method is more involved and will be given in the following
sections.
16.8.1
Aﬃne Invariance
Before we embark on the discussion of the convergence of Newton’s method, let
us show an important feature of Newton’s method in this context. Namely, that
of aﬃne invariance. Simply put, this means that this method is independent of a
linear (or aﬃne) change of coordinates.

486
Convex Optimization
Let T ∈B(H) be such that there is mT > 0 for which ∥Tv∥H ≥mT ∥v∥H for
all v ∈H. Proposition 16.22 shows that this operator has a bounded inverse and
provides a bound for the norm of the inverse. The action of T can be understood
as a change of coordinates in T. Given an energy E : H →R, we deﬁne
ET (v) = E(Tv).
(16.35)
It turns out that many of the properties of E are inherited by ET .
Lemma 16.85 (diﬀerentiability). Let the energy ET : H →R be deﬁned as in
(16.35). Then we have that ET is twice Fr´echet diﬀerentiable, with
⟨E′
T [v], w⟩= ⟨E′[Tv], Tw⟩,
E′′
T [v](w, x) = E′′[Tv](Tw, Tx)
for all v, w, x ∈H. Moreover, if E satisﬁes all the hypotheses of Proposition 16.82,
then ET is µm2
T -coercive on H and locally uniformly continuous with constant
LT (B)∥T∥2
B(H), where we denoted by T(B) the image of B under the mapping T.
Finally,
u = argmin
v∈H
E(v)
⇐⇒
T −1u = argmin
v∈H
ET (v).
Proof. See Problem 16.28.
The aﬃne invariance of Newton’s method is the content of the next result.
Theorem 16.86 (aﬃne invariance). Let H be a Hilbert space and E : H →R
satisfy all the hypotheses of Proposition 16.82. Let ET : H →R be deﬁned as in
(16.35). Let u0 ∈H. Denote by {uk}k∈N the result of applying Newton’s method
of Deﬁnition 16.83 to ET . Starting from Tu0, denote by {˜uk}k∈N the result of
Newton’s method, when applied to E. Then, for all k ∈N0, we have
˜uk = Tuk.
Proof. We will proceed by induction on k, with the obvious starting point ˜u0 = Tu0.
Assume then that, for all k ≤n, we have the thesis. If this is the case, then
un+1 = un + sn with
E′′
T [un](sn, w) = −E′
T [un](w),
∀w ∈H,
which by Lemma 16.85 is equivalent to, for all w ∈H,
E′′[Tun](Tsn, Tw) = −E′[Tun](Tw)
⇐⇒E′′[Tun](Tsn, w) = −E′[Tun](w),
where we used that T is invertible. On the other hand, the Newton iterations for
the energy E yield that ˜un+1 = ˜un + ˜sn with
E′′[Tun](˜sn, w) = E′′[˜un](˜sn, w) = −E′[˜un](w) = −E′[Tun](w),
∀w ∈H,
where we used the induction hypothesis. By uniqueness, we must then have that
˜sn = Tsn, so that
˜un+1 = ˜un + ˜sn = Tun + Tsn = T(un + sn) = Tun+1,
as we claimed.

16.8 Newton’s Method
487
16.8.2
Local Convergence
Recall that, with Newton’s method, we are, essentially, looking for the zero of a
function. Thus, the analysis of local convergence of Newton’s method presented
in Section 15.5 can be generalized to this context.
Theorem 16.87 (local convergence). Let H be a Hilbert space and E : H →R be
a strongly convex and locally Lipschitz smooth energy. Assume that:
1. E is twice Fr´echet diﬀerentiable on H.
2. E′′ is µ-coercive on H.
3. E′′ is locally uniformly continuous on H.
4. E′′ is locally Lipschitz continuous on H; i.e., for every B ⊆H that is bounded,
there is γB such that
∥E′′[v] −E′′[w]∥B(H,H′) ≤γB∥v −w∥H.
Let u ∈H be the (necessarily unique) minimum of E and, for r > 0, deﬁne
B(u, r) = {v ∈H | ∥v −u∥H ≤r} .
Denote by γr and Lr, respectively, the local Lipschitz and uniform continuity
constants of E′′ on B(u, r). Assume that:
r ≤λLr
γr
,
λ < 1
4.
Then, for every u0 ∈B(u, r), Newton’s method of Deﬁnition 16.83 is well deﬁned,
{uk}∞
k=1 ⊆B(u, r), and, ﬁnally, Newton’s method converges at least linearly.
Proof. The proof is nothing but a restatement of the results in Section 15.5 by
carefully extending the results from Rd to this inﬁnite-dimensional setting. We leave
the results to the reader; see [16] or Problem 16.29.
16.8.3
Damped Newton’s Method. Forcing Global Convergence
We conclude our discussion of Newton’s method by presenting a variant of this
method that is globally convergent. We will only present the method and sketch
the main results detailing its properties. For further details, we refer to [8, 27, 68].
Deﬁnition 16.88 (damped Newton). Suppose that B is an open convex subset
of the Hilbert space H and E : H →R is twice Fr´echet diﬀerentiable on B. The
damped Newton’s method is a type of gradient descent method for which the
search direction is computed as the solution to (16.34) and the step size is obtained
by a backtracking line search algorithm, i.e., αk ∈(0, 1] is the largest step size
for which
E(uk + αksk) ≤E(uk) + αk
3 ⟨E′[uk], sk⟩.

488
Convex Optimization
Remark 16.89 (implementation). In the backtracking line search step the factor 1
3
is somewhat arbitrary; any number in the interval (0, 1
2) is suitable. In addition, the
backtracking is usually implemented as follows. One chooses a parameter β ∈(0, 1)
and sets α = 1. While
E(uk + αsk) > E(uk) + α
3 ⟨E′[uk], sk⟩,
one replaces α ←βα.
The idea behind backtracking is that one performs an approximate line search in
the direction sk, and the step size is reduced until the exit condition is satisﬁed.
Intuitively, the factor 1
3 is by how much we allow the line search to be inexact.
Let us state and sketch the proof of the global convergence of the damped
Newton’s method.
Theorem 16.90 (global convergence). Let H be a Hilbert space and E : H →R
be a strongly convex and locally Lipschitz smooth energy. Assume that:
1. E is twice Fr´echet diﬀerentiable on H.
2. E′′ is µ-coercive on H.
3. E′′ is uniformly continuous on E0 with constant M.
4. E′′ is Lipschitz continuous on E0 with constant L.
Then, for any u0, the sequence {uk}∞
k=1 obtained by the damped Newton’s method
of Deﬁnition 16.88 is well deﬁned, i.e., {uk}∞
k=1 ⊂E0. Moreover, we have that
uk →u,
u = argmin
v∈H
E(v).
Finally, there is a K ≥0 such that, for k ≥K, the convergence of the damped
Newton method is quadratic.
Proof. (sketch) It is possible to show that there are numbers γ > 0 and η ∈
(0, µ2/L) such that if
∥E′[uk]∥H′ ≥η,
(16.36)
then
E(uk+1) −E(uk) ≤−γ.
Otherwise, i.e., if ∥E′[uk]∥H′ < η, the step size is chosen as αk = 1 and, moreover,
L
2µ2 ∥E′[uk+1]∥H′ ≤
 L
2µ2 ∥E′[uk]∥H′
2
.
Notice that if (16.36) is false, then we also have that ∥E′[uk+1]∥H′ < η and so,
whenever ℓ≥k, we must have
L
2µ2 ∥E′[uℓ]∥H′ ≤
 L
2µ2 ∥E′[uk]∥H′
2ℓ−k
≤
1
2
2ℓ−k
.

16.9 Accelerated Gradient Descent Methods
489
By Lemma 16.49, this implies that
E(uℓ) −E(u) ≤1
2µ∥E′[uℓ]∥2
H′ ≤2µ2
L2
1
2
2ℓ−k+1
,
from which the quadratic convergence follows.
We ﬁnish by commenting that the backtracking guarantees the existence of
γ > 0, so that, by continuity, since E′[u] = 0 we will eventually violate condition
(16.36).
Remark 16.91 (quasi-Newton methods). The most diﬃcult step in either Newton
or damped Newton methods is ﬁnding the search direction via (16.34), since the
preconditioner changes at every iteration. There are variants, known as quasi-
Newton methods, in which this problem is replaced by simpler ones. We will not
discuss these here.
16.9
Accelerated Gradient Descent Methods
We have so far presented two classes of methods: steepest descent, and its variants,
and Newton. We also developed a theory regarding the convergence, both global
and local, of each of these schemes. At this stage, the avid reader may have
noticed a fundamental diﬀerence between these two methods, one that is of utmost
relevance in practice. Namely, steepest descent is a ﬁrst-order method, whereas
Newton is a second-order one. By this, we mean that the order of derivatives of
the objective that we assume known and computable for each one of these schemes
is one and two, respectively.
In simple problems, computing derivatives may not be an issue. However, in
practice, the second derivative of the objective function may not be available or it
may be extremely diﬃcult to compute. Even if the second derivative is available, as
Remark 16.91 has pointed out, the computation of the search direction, essentially,
entails the solution of a large linear system of equations. The reasons outlined
above show that, for large-scale optimization problems, ﬁrst-order methods may
be preferable.
Having set our attention to ﬁrst-order methods, we must now think about
their performance. To make matters concrete, we will consider schemes without
preconditioning and operate under the assumptions that our objective is µ-strongly
convex and globally Lipschitz smooth with constant L. We must note, however,
that, while strong convexity is a realistic assumption, and many objectives are
locally Lipschitz smooth, making this a global assumption is rather unrealistic. In
fact, most of the objectives that we encounter in practice will not satisfy this
condition. Nevertheless, let us assume this for the sake of argument.
Motivated by Problem 16.30, to quantify the performance of our schemes we
will introduce the nonlinear condition number
ˆκ(E) = L
µ ≥1.

490
Convex Optimization
With
this quantity
at
hand, we
observe that
both
Corollary
16.70
and
Corollary 16.78 show that
E(uk+1) −E(u) = O
 
1 −
1
ˆκ(E)
k!
.
Notice also that this is consistent with the convergence rates we obtained for
gradient descent methods when we discussed the iterative solution of linear systems
with a symmetric positive deﬁnite (SPD) matrix; see Theorems 7.18 and 7.21. We
also refer to Remark 7.19 for a discussion on the meaning of this estimate, and its
limitation, particularly in the case of a large ˆκ(E).
This was one of the motivations, and the major advantage, of the conjugate
gradient method; see Deﬁnitions 7.25 and 7.40 and Theorem 7.39. The conver-
gence rate depends not on the condition number itself, but rather on its square
root. This motivates the search for general ﬁrst-order optimization schemes whose
rate of convergence depends not on the nonlinear condition number, but on some
other, smaller, quantity. In this regard, we mention that there are, for instance,
nonlinear variants of the conjugate gradient method; see [65, Section 5.2] and [16,
Section 8.5].
We begin with a sort of threshold on what can be expected from a ﬁrst-order
scheme. The following result can be found in [64, Theorem 2.1.13].
Theorem 16.92 (lower bound). Let H be a Hilbert space. For any u0 ∈H and
constants 0 < µ < L, there is an objective E : H →R that is µ-strongly convex and
globally Lipschitz smooth with constant L such that, for any ﬁrst-order optimization
scheme satisfying, for k ≥0,
uk+1 −u0 ∈span {RE′[u0], . . . , RE′[uk]} ,
(16.37)
we have
E(uk) −E(u) ≥µ
2
 p
ˆκ(E) −1
p
ˆκ(E) + 1
!2k
∥u0 −u∥2
H,
where u ∈H is the unique minimizer of E.
Proof. (sketch) Consider the space
ℓ2 =

v ∈ℓ2(Z; C)
[v]i ∈R, ∀i ∈Z,
[v]i = 0, ∀i ≤0
	
,
where ℓ2(Z; C) was introduced in Deﬁnition 12.18, and we made an obvious
extension of notation from the ﬁnite-dimensional case. Given positive constants
µ < L, deﬁne the objective Eµ,L : ℓ2 →R via
Eµ,L(v) = L −µ
8
"
[v]2
1 +
∞
X
i=1
([v]i+1 −[v]i)2 −2[v]1
#
+ µ
2 ∥v∥2
ℓ2.

16.9 Accelerated Gradient Descent Methods
491
Let us introduce the (inﬁnite) matrix
A =


2
−1
0
0
· · ·
−1
2
−1
0
· · ·
0
−1
2
−1
· · ·
0
0
...
...
...
...
...
...
...
...


,
which is an inﬁnite-dimensional analogue of the Toeplitz symmetric tridiagonal
(TST) matrix introduced in Theorem 3.38. It is possible to show that
E′
µ,L[v](w) = v ⊺
L −µ
4
A + µI

w −L −µ
4
[w]1,
E′′
µ,L[x](v, w) = L −µ
4
v ⊺Aw + µv ⊺w.
Therefore, an extension of Theorem 3.38 reveals that the objective Eµ,L is µ-
strongly convex and globally Lipschitz smooth with constant L.
Let ˆκ = L/µ. The Euler equation for this objective then reads

A +
4
ˆκ −1I

u = e1,
or, in coordinate form,
2 ˆκ + 1
ˆκ −1[u]1 −[u]2 = 1,
[u]i+1 −2 ˆκ + 1
ˆκ −1[u]i + [u]i+1 = 0,
i = 2, 3, . . . .
Setting λ =
√
ˆκ−1
√
ˆκ+1 ∈(0, 1), we see that [u]i = λi solves the system and, therefore,
is the minimizer of Eµ,L.
Let now {uk}∞
k=0 be a sequence generated by a ﬁrst-order optimization scheme
that satisﬁes (16.37). Without loss of generality, we can assume that u0 = 0, so
that
∥u0 −u∥2
ℓ2 =
∞
X
i=1
λi2 =
∞
X
i=1
λ2i =
λ2
1 −λ2 .
Notice also that
u1 ∈span{RE′
µ,L[u0]} = span{RE′
µ,L[0]}
=⇒
[u1]i = 0, ∀i > 1,
and by induction,
[uk]j = 0, ∀j > k.
Consequently,
∥uk −u∥2
ℓ2 ≥
∞
X
i=k+1
|[u]i|2 =
∞
X
i=k+1
λ2i = λ2(k+1)
1 −λ2 ≥Cλ2k.
The lower quadratic energy trap (16.14) then implies the result.

492
Convex Optimization
Notice that (16.37) is satisﬁed by both PSD and PSD-ALS and that a variation
of this result for their preconditioned versions is not diﬃcult to show. Having
obtained a lower bound on the rate of convergence for certain ﬁrst-order schemes,
the following question arises: Are there any optimal methods? If the objective is
quadratic, then the conjugate gradient method gives exactly this result. In the
general case, however, this is much more complicated.
We now present the so-called accelerated gradient descent scheme devised by
Nesterov9 [63]. To simplify the discussion, we present our results under the restric-
tive assumption that the objective is globally Lipschitz smooth with constant L.
The extension to locally Lipschitz smooth objectives is presented in [69].
Deﬁnition 16.93 (PAGD). Let H be a Hilbert space; E : H →R be a µ-strongly
convex and globally L-smooth objective; and L: H →H′ be a ﬁxed, invariant
preconditioner. The preconditioned accelerated gradient descent (PAGD) or
Nesterov method is an algorithm that constructs a sequence {uk}∞
k=0 ⊂H via
the following rules: u0 is given and we set λ > 1 and y0 = u0. Then, for k ≥0,
uk+1 = yk −αL−1E′[yk],
yk+1 = uk+1 + λ −1
λ + 1 (uk+1 −uk) ,
where α > 0 is the step size.
Notice that the ﬁrst part of the scheme is rather similar to a gradient descent
scheme. The diﬀerence lies in the second step, which we call an extrapolation step.
Indeed, since λ−1
λ+1 > 0, we have that yk+1 lies on the line that passes through uk and
uk+1, but it is not on the segment with endpoints uk and uk+1. This hints at the
fact that the decay of the energy may not be monotone, as we will later see. It also
shows why the analysis of this method for a merely locally L-smooth objective is
rather involved. Since we are extrapolating, we may leave any bounded energy set we
prescribe beforehand, and thus we cannot assume that the constant L remains ﬁxed.
We will now show that AGD, i.e., PAGD with L−1 being the Riesz map, is optimal
under the assumption that the objective is globally L-smooth. Our presentation will
closely follow [98]. The proofs in the preconditioned case are similar. We begin with
a simple algebraic identity.
Lemma 16.94 (auxiliary variable). Let x, y, u ∈H and λ > 1. Deﬁne
ˆz = (λ + 1)y −λx −u,
z = λy −(λ −1)x −u.
Then we have

1 −1
λ2

∥z∥2
H =

1 −1
λ

∥ˆz∥2
H + λ −1
λ2
∥x −u∥2
H −

λ −1
λ

∥y −x∥2
H.
Proof. See Problem 16.31.
Next, we relate the value of the objective at the approximations {uk}∞
k=0 to
the value at any other point via the extrapolations {yk}∞
k=0. Notice that, since we
9 Yurii Evgenevich Nesterov is a Russian mathematician (1956–). He is currently a professor at
the Universit´e catholique de Louvain, Belgium.

16.9 Accelerated Gradient Descent Methods
493
assume that the objective E is globally L-smooth, the second condition in this
lemma holds whenever τ ≤1/L.
Lemma 16.95 (descent condition). Let E : H →R be µ-strongly convex and
globally L-smooth. For any v, w ∈H and τ > 0 that satisfy
v = w −τRE′[w],
E(v) ≤E(w) + E′[w](v −w) + 1
2τ ∥v −w∥2
H , (16.38)
we have that
τ
2 ∥E′[w]∥2
H′ ≤E(w) −E(v)
and
E(v) + 1
2τ ∥v −z∥2
H ≤E(z) + 1 −µτ
2τ
∥w −z∥2
H,
∀z ∈H.
Proof. By observing that v −w = −τRE′[w], we obtain that
E(v) ≤E(w) + (RE′[w], −τRE′[w])H + τ
2 ∥RE′[w]∥2
H = E(w) −τ
2 ∥E′[w]∥2
H′ ,
thus proving the ﬁrst claim.
To obtain the second estimate, we subtract an arbitrary z ∈H from the ﬁrst
identity in (16.38) and then take the inner product of the result with v −z to
obtain
∥v −z∥2
H = (w −z, v −z)H −τ⟨E′[w], v −z⟩.
Identity (16.2) then implies that
∥v −z∥2
H + ∥w −v∥2
H = ∥w −z∥2
H + 2τE′[w](z −w) + 2τE′[w](w −v).
Recall now that, by µ-strong convexity, we have
E′[w](z −w) ≤E(z) −E(w) −µ
2 ∥z −w∥2
H,
so that
∥v −z∥2
H + ∥w −v∥2
H = (1 −µτ) ∥w −z∥2
H + 2τ [E(z) −E(w) + E′[w](w −v)]
≤(1 −µτ) ∥w −z∥2
H + 2τ [E(z) −E(v)] ,
where in the last step we used the second condition in (16.38). The claim
follows.
We are now ready to prove convergence of AGD.
Theorem 16.96 (convergence). Let H be a Hilbert space and E : H →R be µ-
strongly convex and globally L-smooth. Assume that u ∈H is the unique minimizer
of E and {uk}∞
k=0 ⊂H is the sequence obtained by the method of Deﬁnition 16.93
with L−1 = R, α ∈[ 1
2L, 1
L], and λ ≥
q
2L
µ . Then we have
λ2
L (E(uk) −E(u)) + ∥(λ + 1)yk −λuk −u∥2
H
≤

1 −1
λ
k 
2αλ2 (E(u0) −E(u)) + ∥u0 −u∥2
H

,
where the sequence {yk}∞
k=0 ⊂H is introduced in Deﬁnition 16.93.

494
Convex Optimization
Proof. Notice, ﬁrst of all, that, for every k ≥0, the vectors v = uk+1, w = yk
satisfy the assumptions of Lemma 16.95 with τ = α. Thus, setting
z =

1 −1
λ

uk + 1
λu,
we get
E(uk+1) + 1
2α
uk+1 −

1 −1
λ

uk −1
λu

2
H
≤E

1 −1
λ

uk + 1
λu

+ 1 −αµ
2α
yk −

1 −1
λ

uk −1
λu

2
H
. (16.39)
Let us consider the second term in the previous estimate in more detail. Deﬁning
ˆzk+1 = (λ + 1)yk+1 −λuk+1 −u,
and using the deﬁnition of yk+1 given in Deﬁnition 16.93, we get that
uk+1 −

1 −1
λ

uk −1
λu = 1
λ [2λuk+1 −(λ −1)uk −λuk+1 −u]
= 1
λ [(λ + 1)yk+1 −λuk+1 −u].
With this notation, (16.39) then becomes
E(uk+1) +
1
2αλ2 ∥ˆzk+1∥2
H
≤E

1 −1
λ

uk + 1
λu

+ 1 −αµ
2α
yk −

1 −1
λ

uk −1
λu

2
H
. (16.40)
We now consider the last term in (16.40). Deﬁne
zk = λyk −(λ −1)uk −u,
so that
yk −

1 −1
λ

uk −1
λu = 1
λ [λyk −(λ −1)uk −u] = 1
λzk,
and (16.40) becomes
E(uk+1) +
1
2αλ2 ∥ˆzk+1∥2
H ≤E

1 −1
λ

uk + 1
λu

+ 1 −αµ
2αλ2 ∥zk∥2
H . (16.41)
Since λ > 1, by the µ-strong convexity of E and the fact that u is its minimizer,
we have that
E

1 −1
λ

uk + 1
λu

≤

1 −1
λ

E(uk) + 1
λE(u) −µ
2

1 −1
λ

∥uk −u∥2
H.

16.9 Accelerated Gradient Descent Methods
495
This, combined with (16.41), yields
E(uk+1) −E(u) +
1
2αλ2 ∥ˆzk+1∥2
H
≤

1 −1
λ

(E(uk) −E(u)) + 1 −αµ
2αλ2 ∥zk∥2
H −µ
2

1 −1
λ

∥uk −u∥2
H.
(16.42)
We can now set x = uk, y = yk in Lemma 16.94, and use the deﬁnition of yk+1
to see that
E(uk+1) −E(u) +
1
2αλ2

∥ˆzk+1∥2
H +

λ −1
λ

∥yk −uk∥2
H

≤

1 −1
λ
 
E(uk) −E(u) +
1
2αλ2 ∥ˆzk∥2
H

+
1
2αλ2
 1
λ2 −αµ

(λ −1)

∥uk −u∥2
H + ∥zk∥2
H

.
(16.43)
Finally, we observe that the choice of parameters α and λ implies that
1
λ2 ≤µ
2L ≤αµ,
so that, ﬁnally, we conclude
E(uk+1) −E(u) +
1
2αλ2

∥ˆzk+1∥2
H +

λ −1
λ

∥yk −uk∥2
H

≤

1 −1
λ
 
E(uk) −E(u) +
1
2αλ2

∥ˆzk∥2
H + ∥uk −u∥2
H + ∥zk∥2
H

.
(16.44)
Iterating this estimate then yields the claimed convergence estimate.
Remark 16.97 (rate of convergence). Notice that setting, in Theorem 16.96,
α = L−1 and λ =
q
2L
µ , we get
E(uk) −E(u) = O


 
1 −
1
p
ˆκ(E)
!k
.
It is in this sense that this method is optimal; compare with Theorem 16.92.
Remark 16.98 (monotonicity). Notice that, in Theorem 16.96, the rate of
convergence is proved not for the objective E itself, but rather for the extended
functional
E(uk+1) −E(u) +
1
2αλ2

∥ˆzk+1∥2
H +

λ −1
λ

∥yk −uk∥2
H

.
Numerical illustrations, see Figure 16.4, conﬁrm that the value of the objective
{E(uk)}∞
k=1 does not necessarily monotonically decay for PAGD.

496
Convex Optimization
0
200
400
600
800
1000
-20
-15
-10
-5
0
5
k
log10(E(uk) −E(u))
Comparison of PSD and AGD for a quadratic objective
PSD
AGD
0
200
400
600
800
1000
-30
-25
-20
-15
-10
-5
0
5
k
log10(E(uk) −E(u))
Error of AGD vs. Optimal rate
AGD
optimal rate
Figure 16.4 Comparison of the performance of PSD (dashed) and AGD (solid) for the
quadratic objective functional deﬁned in (16.45) with µ = 1, L = 103, and N = 100.
Observe that, for AGD, the objective does not decay monotonically.
16.10
Numerical Illustrations
In this section, we illustrate and compare the performance of the methods that we
have developed. Listing 16.1 shows the implementation of each of these methods.
Starting from an initial guess, each method is run for a ﬁxed number of iterations.
The choice of parameters is in accordance with the theory. Indeed, for steepest
descent,
α = 1
2L,
whereas for accelerated gradient descent,
α = 1
2L,
λ =
s
2L
µ .
We compare the performance of these methods in a series of diﬀerent objectives.
16.10.1
Quadratic Objectives
In an attempt to replicate the construction obtained in the proof of Theorem 16.92,
we consider, for diﬀerent values of N ∈N, the objective E : RN →R deﬁned as
E(v) = L −µ
8
(v −e1)⊺A(v −e1) + µ
2 ∥v∥2
2,
(16.45)
where A is the TST matrix deﬁned in Theorem 3.38 for n = N +1. The description
of σ(A), provided in the proof of Theorem 3.38, reveals that this objective is µ-
strongly convex and globally L-smooth.
Listing 16.2 implements this objective and compares the performance of each
one of the methods for µ = 1 and L = 103. In addition, it compares the rate of
convergence of AGD with the lower bound provided in Theorem 16.92.
Figure 16.4 shows the results for N = 100. The ﬁgure clearly shows how AGD
outperforms PSD. In addition, we observe that, for AGD, the decay of the objective

16.10 Numerical Illustrations
497
0
200
400
600
800
1000
-20
-15
-10
-5
0
5
k
log10(E(uk) −E(u))
Comparison of PSD and AGD for a quadratic objective
PSD
AGD
0
200
400
600
800
1000
-30
-25
-20
-15
-10
-5
0
5
k
log10(E(uk) −E(u))
Error of AGD vs. Optimal rate
AGD
optimal rate
Figure 16.5 Comparison of the performance of PSD (dashed) and AGD (solid) for the
quadratic objective functional deﬁned in (16.45) with µ = 1, L = 103, and N = 106.
Observe that, for AGD, the objective does not decay monotonically.
0
200
400
600
800
1000
-25
-20
-15
-10
-5
0
5
10
k
log10(E(uk) −E(u))
Comparison of PSD and AGD for a strongly convex objective
PSD
AGD
Figure 16.6 Comparison of the performance of PSD (dashed) and AGD (solid) for the
nonquadratic objective functional deﬁned in (16.46) with p = 4 and N = 106. We set
µ = 1 and L = 400. Observe that, for AGD, the objective does not decay monotonically.
is not monotone. Finally, the rate of decay of AGD coincides with the optimal decay
established in Theorem 16.92.
In Figure 16.5, we present the results for N = 106. The same observations apply.
16.10.2
Nonquadratic Objectives
Let us now consider a nonquadratic, strongly convex objective that is locally, but
not globally, Lipschitz smooth. Let N ∈N and p > 2. We deﬁne the objective
E : RN →R as
E(v) = 1
p ∥v∥p
p + 1
2 ∥v∥2
2 .
(16.46)
Clearly, the minimizer is u = 0. By construction, this objective is µ-strongly convex
with µ ≥1. As we do not know the value of any local L-smoothness constant, we
set L = 400. This objective is implemented in Listing 16.3.

498
Convex Optimization
Figure 16.6 compares the performance of PSD and AGD on this problem for
p = 4 and N = 106. We again observe that AGD greatly outperforms PSD and
that the value of the objective does not monotonically decay for AGD.
Problems
16.1
Prove identity (16.2).
16.2
Prove Proposition 16.4. Give an explicit counterexample to the converse.
16.3
Complete the proof of Proposition 16.13.
16.4
Prove the Riesz Representation Theorem 16.14 in the case that H is
ﬁnite dimensional. For this problem, it may help to recall some properties of SPD
matrices.
16.5
Prove Theorem 16.16.
16.6
Prove Proposition 16.21.
16.7
Prove Proposition 16.26.
16.8
Prove Proposition 16.33.
16.9
Prove Proposition 16.37.
16.10
Prove Proposition 16.38.
16.11
Prove Proposition 16.39.
16.12
Prove Proposition 16.40.
16.13
Prove the following generalization of Taylor’s Theorem with integral
remainder. Namely, let H be a Hilbert space and E : H →R be Fr´echet
diﬀerentiable with E′[·]: H →H′ being continuous. Then
E[w] −E[v] =
Z 1
0
⟨E′[v + t(w −v)], w −v⟩dt,
∀v, w ∈H.
Hint: Consider the function ϕ: [0, 1] →R deﬁned by
ϕ(t) = E (v + t(w −v))
and apply Theorem B.39.
16.14
Complete the proof of Lemma 16.48.
16.15
Prove Lemma 16.50.
16.16
Complete the proof of Lemma 16.51.
16.17
Complete the proof of Lemma 16.53.
16.18
Prove Proposition 16.55.
16.19
Prove Theorem 16.61.
16.20
Prove the orthogonality (16.25).
16.21
Prove Corollary 16.70.
16.22
Prove Theorem 16.74. Note that this result is more subtle than the proof
for PSD, which was the content of Problem 16.19.
16.23
Prove (16.31).
Hint: Use a variant of Taylor’s Theorem.
16.24
Prove Theorem 16.77.
16.25
Prove Corollary 16.78.

Listings
499
16.26
Prove the following generalization of Theorem B.31. Let H be a Hilbert
space and E : H →R be continuously Fr´echet diﬀerentiable on H and, for v, w ∈
H, twice Fr´echet diﬀerentiable on the set S = {v + t(w −v)|t ∈(0, 1)}. Then
there is t ∈(0, 1) such that
E(w) = E(v) + ⟨E′[v], w −v⟩+ 1
2E′′[v + t(w −v)](w −v, w −v).
Hint: Consider the function ϕ: [0, 1] →R deﬁned by
ϕ(t) = E (v + t(w −v))
and apply Theorem B.31.
16.27
Prove (16.33).
16.28
Prove Lemma 16.85.
16.29
Complete the proof of Theorem 16.87.
16.30
Let N ∈N and consider the energy E : RN →R
EA(v) = 1
2v ⊺Av −f ⊺v,
where A ∈RN×N and f ∈RN are given and we assume that A is SPD with
σ(A) ⊂[λ, Λ]. Show that EA is λ-coercive and globally Lipschitz smooth with
constant Λ. Consequently,
ˆκ(EA) = Λ
λ ≤κ2(A).
16.31
Prove Lemma 16.94.
Listings
1
function [energyPSD, energyAGD, upsd, uagd] = RunMethods( ...
2
E, Ep, mu, L, MaxIts, uinit )
3
% This function runs both the Steepest Descent with approximate
4
% line search (PSD)
5
%
6
% u {k+1} = u k - alpha*Ep( u k )
7
%
8
% and the Accelerated Gradient Descent (AGD)
9
%
10
% u {k+1} = y k - alpha*Ep( y k )
11
%
12
% y {k+1} = u {k+1} + ((lambda-1)/(lambda+1))*( u {k+1} - u k );
13
%
14
% methods for a stongly convex and (locally) Lipschitz smooth
15
% objective E with derivative Ep.
16
%
17
% Input
18
%
E : the objective function
19
%
Ep : the derivative of the objective function
20
%
mu : (an estimate of) the strong convexity constant for E

500
Convex Optimization
21
%
L : (an estimate of) the (local) Lipschitz smoothness
22
%
constant for E
23
%
MaxIts : the maximal number of iterations
24
%
uinit : an initial guess for the minimizer
25
%
26
% Output
27
%
energyPSD(1:MaxIts+1) : the value of the objective at every
28
%
iteration of PSD
29
%
energyAGD(1:MaxIts+1) : the value of the objective at every
30
%
iteration of AGD
31
%
upsd : the approximate minimizer after MaxIts iterations
32
%
of PSD
33
%
uagd : the approximate minimizer after MaxIts iterations
34
%
of AGD
35
%
36
37
% We first run PSD
38
alphaPSD = 0.5/L;
39
40
energyPSD = zeros(MaxIts+1,1);
41
energyPSD(1) = E( uinit );
42
43
upsd = uinit;
44
for i=1:MaxIts
45
upsd = PSD( alphaPSD, upsd, Ep );
46
energyPSD(i+1) = E( upsd );
47
end
48
49
% Next we run AGD
50
alphaAGD = 0.5/L;
51
lambdaAGD = sqrt( 2*L/mu );
52
53
energyAGD = energyPSD;
54
uagd = uinit;
55
yagd = uinit;
56
for i=1:MaxIts
57
[uagd, yagd ] = AGD(alphaAGD, lambdaAGD, uagd, yagd, Ep );
58
energyAGD(i+1) = E( uagd );
59
end
60
end
61
62
63
function u = PSD( alpha, uold, Ep )
64
% This function does one iteration of the PSD scheme
65
%
66
% Input
67
%
alpha : the line search parameter
68
%
uold : the previous approximate minimizer
69
%
Ep : the derivative of the objective
70
%
71
% Output
72
%
u : the next approximate minimizer
73
%
74
u = uold - alpha.*Ep( uold );
75
end
76

Listings
501
77
function [u, y] = AGD(alpha, lambda, uold, yold, Ep )
78
% This function does one iteration of the AGD scheme
79
%
80
% Input
81
%
alpha : the line search parameter
82
%
lambda : the extrapolation parameter
83
%
uold : the previous approximate minimizer
84
%
yold : the previous extrapolated value
85
%
Ep : the derivative of the objective
86
%
87
% Output
88
%
u : the next approximate minimizer
89
%
y : the next approximate extrapolation
90
%
91
u = yold - alpha.*Ep( yold );
92
y = u + ((lambda-1.)/(lambda+1.)).*( u - uold );
93
end
Listing 16.1 Implementation of gradient descent and accelerated gradient descent
methods. Starting from an initial guess they are run for a ﬁxed number of iterations.
1
% This code defines a strongly convex and globally Lipschitz
2
% smooth quadratic objective and uses it to compare the
3
% performance of PSD and AGD for its minimization:
4
clear all
5
clc
6
7
% We begin by defining the space dimension and the parameters
8
% that are used to define the objective function:
9
dim = 100000; % the dimension: the number of variables
10
mu = 1.; % the strong convexity constant
11
L = 1000.; % the global Lipschitz constant
12
13
% Since the objective is quadratic, part of it is defined as
14
%
15
% v'*AA*v
16
%
17
% where AA is a tridiagonal matrix that we now define.
18
mult = 0.25*( L - mu );
19
ii = zeros(dim + 2*( dim - 1 ), 1 );
20
jj = ii;
21
vv = ii;
22
vvEuler = ii;
23
for i = 1:dim
24
ii(i) = i;
25
jj(i) = i;
26
vv(i) = 2;
27
vvEuler(i) = mult*vv(i) + mu;
28
if i<dim
29
ii(i+dim) = i;
30
jj(i+dim) = i+1;
31
vv(i+dim) = -1;
32
vvEuler(i+dim) = mult*vv(i+dim);
33
end
34

502
Convex Optimization
35
if i>1
36
ii(i+2*(dim-1)) = i;
37
jj(i+2*(dim-1)) = i-1;
38
vv(i+2*(dim-1)) = -1;
39
vvEuler(i+2*(dim-1)) = mult*vv(i+2*(dim-1));
40
end
41
end
42
43
AA = sparse(ii,jj, vv );
44
AAEuler = sparse( ii, jj, vvEuler );
45
46
e1 = zeros(dim,1);
47
e1(1,1) = 1;
48
49
% We define two anonymous function handles to describe the
50
% objective and its derivative:
51
E = @(u) Objective( u, mu, L, AA, e1 );
52
Ep = @(u) ObjectivePrime( u, mu, L, AA, e1 );
53
54
% The minimum of the objective can be found by solving the
55
% Euler equations. Since the objective is quadratic these turn
56
% out to be a linear system of equations:
57
uexact = AA*e1;
58
uexact = AAEuler\uexact;
59
uexact = 0.25*( L - mu )*uexact;
60
Eexact = E( uexact );
61
62
% We now can start with the optimization schemes. Maximum
63
% number of iterations and initial guess:
64
MaxIts = 1000; % Maximum number of iterations
65
uinit = zeros( dim, 1 ); % Initial guess
66
67
% We run the methods
68
[energyPSD, energyAGD, upsd, uagd] = RunMethods( E, Ep, mu, ...
69
L, MaxIts, uinit );
70
energyPSD = energyPSD - Eexact;
71
energyAGD = energyAGD - Eexact;
72
73
% We plot to compare
74
its = 0:MaxIts;
75
hf = figure();
76
clf;
77
plot( its, log10( energyPSD ), its, log10( energyAGD ) );
78
grid on;
79
xlabel('k');
80
ylabel('log {10}( E(u k) - E(u) )');
81
title('Comparison of PSD and AGD for a quadratic objective');
82
legend('PSD', 'AGD');
83
exportgraphics( gca, 'ComparisonQuadratic.pdf');
84
85
% We now compare the rate of convergence of AGD with the
86
% theoretically optimal one:
87
hff = figure(2);
88
clf;
89
kappa = sqrt( L/mu );
90
rate =
2.*log10( 1 - 1/kappa )*its;

Listings
503
91
92
plot( its, log10( energyAGD ), its, rate );
93
grid on;
94
xlabel('k');
95
ylabel('log {10}( E(u k) - E(u) )');
96
legend('AGD','optimal rate')
97
title('Error of AGD vs. Optimal rate');
98
exportgraphics( gca,'ComparisonAGDandOptimal.pdf');
99
100
%%%%%%%%%%%%%%%%%%%%%%%
101
102
function E = Objective( u, mu, L, AA, e1 )
103
% This function defines a quadratic objective
104
%
105
% Input
106
%
u : the argument of the Objective function
107
%
mu : the strong convexity constant of the objective
108
%
L : the Lipschitz smoothness constnat of the objective
109
%
AA : an SPD matrix used to define the objective
110
%
e1 : A fixed vector to define the objective
111
%
112
% Output
113
%
E : the value of the objective function
114
%
115
E = 0.125*( L - mu )*( ( u - e1 )'*AA*( u - e1 ) ) ...
116
+ 0.5*mu*( norm(u)ˆ2 );
117
end
118
119
function Ep = ObjectivePrime( u, mu, L, AA, e1 )
120
% This function defines the derivative of a quadratic objective
121
%
122
% Input
123
%
u : the argument of the Objective function
124
%
mu : the strong convexity constant of the objective
125
%
L : the Lipschitz smoothness constnat of the objective
126
%
AA : an SPD matrix used to define the objective
127
%
e1 : A fixed vector to define the objective
128
%
129
% Output
130
%
Ep : the value of the derivative of the objective function
131
%
132
Ep = 0.25*(L-mu)*AA*(u - e1) + mu*u;
133
end
Listing 16.2 Implementation of the quadratic objective deﬁned in (16.45) and
comparison of the performace of PSD and AGD on this objective.
1
% This code defines an objective that not quadratic, it is
2
% strongly convex, and locally but not globally Lipschitz
3
% smooth. This objective is used to compare the performance of
4
% PSD and AGD.
5
clear all
6
clc
7
8
% We begin by defining the space dimension and the parameters

504
Convex Optimization
9
% that are used to define the objective function:
10
dim = 10000; % the dimension: the number of variables
11
p = 4; % this is used to define the norm
12
mu = 1; % the strong convexity constant
13
14
% We do not know exactly the (local) Lipschitz smoothness
15
% constant. This is an estimate:
16
L = 400;
17
18
% We define two anonymous function handles to describe the
19
% objective and its derivative:
20
E = @(u) Objective( u, p );
21
Ep = @(u) ObjectivePrime( u, p, dim );
22
23
% We know the exact minimizer:
24
uexact = zeros( dim, 1 );
25
Eexact = E( uexact );
26
27
% We now can start with the optimization schemes. Maximum
28
% number of iterations and initial guess:
29
MaxIts = 500; % Maximum number of iterations
30
uinit = 10*ones(dim,1); % Initial guess
31
32
% We run the methods:
33
[errorPSD, errorAGD, upsd, uagd] = RunMethods( E, Ep, mu, ...
34
L, MaxIts, uinit );
35
errorPSD = errorPSD - Eexact;
36
errorAGD = errorAGD - Eexact;
37
38
% We plot to compare
39
its = 0:MaxIts;
40
hf = figure();
41
clf;
42
plot( its, log10( errorPSD ), its, log10( errorAGD ) );
43
grid on;
44
xlabel('k');
45
ylabel('log {10}( E(u k) - E(u) )');
46
title( ...
47
'Comparison of PSD and AGD for a strongly convex objective');
48
legend('PSD', 'AGD');
49
exportgraphics( gca, 'ComparisonStrongConvex.pdf');
50
51
%%%%%%%%%%%%%%%%%%%%%%%
52
53
function E = Objective( u, p )
54
% This defines a non quadratic, strongly convex, and locally
55
% but not globally Lipschitz smooth objective.
56
%
57
% Input
58
%
u : the argument of the Objective function
59
%
p : the order of the norm used to make this non quadratic
60
%
61
% Output
62
%
E : the value of the objective function
63
%
64
E = ( norm( u, p )ˆp )/p + 0.5*norm( u )ˆ2;

Listings
505
65
end
66
67
function Ep = ObjectivePrime( u, p, dim )
68
% This defines the derivative of the nonquadratic objective
69
%
70
% Input
71
%
u(1,dim) : the argument of the Objective function
72
%
p : the order of the norm used to make this non quadratic
73
%
dim : the dimension of u
74
%
75
% Output
76
%
Ep : the value of the derivative of the objective function
77
%
78
Ep = u + ( abs( u ).ˆ(p-2) ).*u;
79
end
Listing 16.3 Implementation of the nonquadratic, stongly convex, locally but not
globally Lipschitz smooth objective deﬁned in (16.46) and comparison of the
performace of PSD and AGD on this objective.


Part IV
Initial Value Problems for
Ordinary Diﬀerential
Equations


17
Initial Value Problems for Ordinary
Diﬀerential Equations
In this chapter, we begin the study of methods for approximating the solution of
an ordinary diﬀerential equation (ODE),
u′(t) = f (t, u(t)),
t ∈(0, T],
supplemented with a given initial condition, u(0) = u0. This is usually known as a
Cauchy problem1 or an initial value problem (IVP) for an ODE.
Ordinary diﬀerential equations, and their Cauchy problems, arise in many
problems related to the sciences and engineering. They essentially describe how the
change of one quantity — which above we denoted by t, since it is usually thought
of as time — aﬀects another quantity, u. For instance, everyone is familiar with
Newton’s second law of motion,2 which in elementary textbooks is presented as
F = ma,
where F is the net force exerted on a particle, m is its mass, and a is its acceleration.
Let us assume that the force depends only upon time. We know that acceleration
is the rate of change (with respect to time) of velocity v, which in turn is the rate
of change of position x, i.e., a(t) = v ′(t) = x′′(t). Let us introduce the variable
u(t) =
x(t)
v(t)

,
so that Newton’s second law can then be written as
u′(t) =
v(t)
a(t)

=
 v(t)
1
mF(t)

=
O3
I3
O3
O3
 x(t)
v(t)

+

0
1
mF(t)

= f (t, u(t)).
Of course, in this case, we have
f (t, u(t)) = Au(t) + b(t),
where A is a matrix and b is a time-dependent vector. If, say, the force depends
upon the position, x, and velocity, v, one would arrive at a more complicated form.
In any case, assuming that the initial position x(0) = x0 and velocity v(0) = v 0
are known, we set u(0) = u0 = [x0, v 0]⊺and arrive at a Cauchy problem.
The previous elementary discussion illustrates the meaning of such a problem: it
describes how the position of a system evolves over time, provided that we know
its initial position, initial velocity, and the forces that are acting on it. In addition,
1 Named in honor of the French mathematician Augustin Louis Cauchy (1789–1857).
2 Named in honor of the British mathematician, physicist, astronomer, theologian, and natural
philosopher Sir Isaac Newton (1642–1726/27).

510
Initial Value Problems for Ordinary Diﬀerential Equations
it illustrates a very important point: the problem we will study is a ﬁrst-order ODE,
since we only see ﬁrst derivatives of the dependent variable, u, with respect to the
independent one, t. However, there is no signiﬁcant loss in generality in doing this,
as many higher order ODEs (those that have derivatives of order larger than one)
can be reduced to a (system of) ﬁrst-order ODEs.
The IVP, as stated above, is rather vague, as we did not clearly state what it is
we mean by solving this problem, nor did we provide conditions that guarantee that
a solution exists. Because of this, before we start discussing numerical schemes for
such problems, we discuss some of the theory about the solutions to IVPs. For
further developments and complete proofs of some of the results we omit, we refer
the reader to [19, 37].
17.1
Existence of Solutions
To start, let us deﬁne precisely what we mean by a solution to an IVP. In what
follows, we will assume that the following are ﬁxed: d is a positive integer; Ω⊆Rd
is an open set; u0 is a point in Ω; I ⊆R is a closed interval; t0 is a point in I;
S = I × Ω; and f : S →Rd is a given function, which we call the slope function.
The IVP that we consider seeks a function u : I →Ωthat, in some sense, satisﬁes
the initial condition u(t0) = u0 and the equation
u′(t) = f (t, u(t)).
(17.1)
Deﬁnition 17.1 (classical solution). The function u ∈C1(I; Ω) is called a classical
solution on I to the IVP if and only if (17.1) holds point-wise for all t ∈I and
limt→t0 u(t) = u0.
Deﬁnition 17.2 (mild solution). We say that u ∈C(I; Ω) is a mild solution on I
to the IVP (17.1) if and only if, for all t ∈I, we have
u(t) = u0 +
Z t
t0
f (s, u(s))ds.
(17.2)
Theorem 17.3 (equivalence). Assume that f ∈C
 S; Rd
. A function is a mild solu-
tion on I to problem (17.1) if and only if it is a classical solution to problem (17.1).
Proof. See Problem 17.1.
Deﬁnition 17.4 (u-Lipschitz). We say that the slope function f : S →Rd is u-
Lipschitz on S if and only if there is a constant L > 0 such that
∥f (t, v 1) −f (t, v 2)∥2 ≤L ∥v 1 −v 2∥2
(17.3)
for all t ∈I and for all v 1, v 2 ∈Ω. If (17.3) holds with Ω= Rd, we say that f is
globally u-Lipschitz.3
3 The term Lipschitz is used in honor of the German mathematician Rudolf Otto Sigismund
Lipschitz (1832–1903).

17.1 Existence of Solutions
511
We have the following local existence and uniqueness result whose brief but
elegant proof is based on the Banach Fixed Point Theorem C.4; see Appendix C
for some background.
Theorem 17.5 (Picard–Lindel¨of Theorem4). Suppose that there exist constants
β, δ0 > 0, such that I0 = [t0 −δ0, t0 + δ0] ∩I ̸= ∅, and B(u0, β) ⊂Ω. Deﬁne
S0 = I0 × B(u0, β).
Assume that f ∈C
 S0; Rd
; there is a constant M > 0 such that, for all (t, v) ∈
S0, ∥f (t, v)∥2 ≤M; and f is u-Lipschitz on S0 with constant L > 0. Let
δ1 = min

δ0, δ0
2L, β
M

,
I1 = [t0 −δ1, t0 + δ1].
Then there is a unique mild solution on I1 to (17.1). Moreover, u ∈C(I1; B(u0, β)).
Proof. We will apply the the Banach Fixed Point Theorem C.4. Let J ⊆I0 be an
interval with t0 ∈J. Deﬁne the mapping A: C(J; B(u0, β)) →C(J; B(u0, β)) by
A[v](t) = u0 +
Z t
t0
f (s, v(s))ds.
Let us show ﬁrst that if J is suitably chosen and v ∈C(J; B(u0, β)), then, indeed,
A[v] ∈C(J; B(u0, β)). The continuity of A[v] is guaranteed by the Fundamental
Theorem of Calculus (Theorem B.37). We need only show that this function takes
values in B(u0, β). Note that
∥A[v](t) −u0∥2 =

Z t
t0
f (s, v(s))ds

2
≤
Z max{t0,t}
min{t0,t}
∥f (s, v(s))∥2 ds
≤M|t −t0|,
so that if J = [t0 −δ, t0 + δ] with δ = min{δ0, β/M}, then A[v](t) ∈B(u0, β) for
all t ∈J.
Now, given v 1, v 2 ∈C(J; B(u0, β)), consider
∥A[v 1] −A[v 2]∥L∞(J) = sup
t∈J

Z t
t0
(f (s, v 1(s)) −f (s, v 2(s)))ds

2
≤sup
t∈J
Z max{t0,t}
min{t0,t}
∥f (s, v 1(s)) −f (s, v 2(s))∥2 ds.
4 Named in honor of the French mathematician Charles ´Emile Picard (1856–1941) and the
Finnish mathematician Ernst Leonard Lindel¨of (1870–1946).

512
Initial Value Problems for Ordinary Diﬀerential Equations
Since f is u-Lipschitz on S, we then obtain
∥A[v 1] −A[v 2]∥L∞(J) ≤sup
t∈J
Z max{t0,t}
min{t0,t}
L ∥v 1(s) −v 2(s)∥2 ds
≤L∥v 1 −v 2∥L∞(J) sup
t∈J
|t −t0|.
Consequently, if we restrict our study to the interval
I1 =

t0 −δ0
2L, t0 + δ0
2L

∩J,
then we obtain
∥A[v 1] −A[v 2]∥L∞(I1) ≤1
2∥v 1 −v 2∥L∞(I1).
This proves that the mapping A: C(I1; B(u0, β)) →C(I1; B(u0, β)) is a contrac-
tion. Since

C(I1; B(u0, β)), ∥· ∥L∞(I1)

is a complete metric space, see Example C.6, the contraction mapping principle for
metric spaces, as stated in Theorem C.11, yields the existence and uniqueness of
a ﬁxed point for A. Call the ﬁxed point u ∈C(I1; B(u0, β)). Then
u(t0) = A[u](t0) = u0 +
Z t0
t0
f (s, u(s))ds = u0;
otherwise, for all t ∈I1,
u(t) = A[u](t) = u0 +
Z t
t0
f (s, u(s))ds.
Clearly, u is a mild solution on I1 to (17.1).
Example 17.1
Suppose that u0 > 0. Observe that u(t) =
 u−1
0
−t
−1 is a
classical solution on the interval [0, u0) to the IVP
u′(t) = u2(t),
u(0) = u0.
The autonomous slope function f (t, u(t)) = u2(t) is not globally u-Lipschitz on
S = [0, T] × R, regardless of the size of T > 0. Clearly, a global solution, i.e., a
(classical or mild) solution on R, cannot be guaranteed. In any case, Theorem 17.5
is applicable, and a unique solution, locally deﬁned around t = 0, can be guaranteed.
Observe that Theorem 17.5 only guarantees the existence of a solution on a
possibly small interval of time. For example, if M or L is very large, the interval of
existence may be quite small. To guarantee existence on a larger interval of time,
we need to strengthen the Lipschitz condition on the slope function f .

17.1 Existence of Solutions
513
To simplify notation, here and in what follows we will assume that t0 = 0 and
I = [0, T] for some T > 0. A simple translation and possible rescaling can bring us
to the general case. In addition, as we mentioned before, this justiﬁes the name:
initial value problem.
Theorem 17.6 (global existence). Assume that the slope function f ∈C
 S; Rd
is globally u-Lipschitz with constant L > 0. Then there is at least one mild solution
on [0, T] to (17.1), which we denote by u ∈C
 [0, T]; Rd
. Moreover, this solution
satisﬁes the estimate
∥u(t) −u0∥2 ≤M
L
 eLt −1

,
∀t ∈[0, T],
where M = ∥f (·, u0)∥L∞(0,T ).
Proof. We could use an argument based on the Banach Fixed Point Theorem
again. But, for variety, let us follow a slightly diﬀerent path; see, for example, [83].
Again, deﬁne, for v ∈C
 [0, T]; Rd
,
A[v](t) = u0 +
Z t
0
f
 s, v(s)

ds.
Since the slope function is globally u-Lipschitz,
∥A[v 1](t) −A[v 2](t)∥2 ≤L
Z t
0
∥v 1(s) −v 2(s)∥2 ds.
(17.4)
Now recursively deﬁne a sequence of functions as u(0)(t) = u0 for all t ∈[0, T],
and, for k ≥0, u(k+1)(t) = A

u(k)
(t) for all t ∈[0, T]. Therefore,
u(1)(t) −u(0)(t)

2 =

Z t
0
f
 s, u0

ds

2
≤
Z t
0
f
 s, u0

2 ds ≤Mt.
More generally, using (17.4), for k ≥1,
u(k)(t) −u(k−1)(t)

2 =

Z t
0

f
 s, u(k−1)(s)

−f
 s, u(k−2)(s)

ds

2
≤L
Z t
0
u(k−1)(s) −u(k−2)(s)

2 ds.
From these two estimates, we deduce, by induction, that, for all t ∈[0, T], we
have
u(k)(t) −u(k−1)(t)

2 ≤MLk−1tk
k!
.
Next, we show that the inﬁnite series
s(t) = u(t) −u0 =
∞
X
j=1

u(j)(t) −u(j−1)(t)

(17.5)
is absolutely and uniformly convergent on [0, T]. To see this, note that, for each
t ∈[0, T], the kth partial sum is
sk(t) =
k
X
j=1

u(j)(t) −u(j−1)(t)

= u(k)(t) −u0.

514
Initial Value Problems for Ordinary Diﬀerential Equations
Thus, for m > n, using the triangle inequality,
∥sm(t) −sn(t)∥2 ≤
m
X
j=n+1
u(j)(t) −u(j−1)(t)

2 ≤M
L
m
X
j=n+1
Ljtj
j! .
(17.6)
Since the series
∞
X
j=1
Ljtj
j!
= eLt −1
converges absolutely and uniformly on [0, T], the sequence of partial sums {sk}∞
k=1
is uniformly Cauchy on [0, T]. This proves that the series (17.5) converges
absolutely and uniformly on [0, T ]. Hence, s, and therefore u, is continuous on
[0, T]. Now sk →s = u −u0, uniformly on [0, T], and
∥sk(t)∥2 ≤M
L
k
X
j=1
Ljtj
j!
≤M
L
 eLt −1

.
This implies that, for all t ∈[0, T],
∥u(t) −u0∥2 ≤M
L
 eLt −1

.
To conclude the proof, we must show that the u we constructed in (17.5) is
precisely the mild solution that we are seeking. To this end, taking m →∞in
(17.6), we have, for any t ∈[0, T],
u(t) −u(n)(t)

2 = ∥u(t) −u0 −sn(t)∥2
≤M
L
∞
X
j=n+1
Ljtj
j!
≤M
L
(Lt)n+1
(n + 1)!
∞
X
j=0
Ljtj
j!
≤M
L κn+1eLt,
where κn = (LT )n
n! . For every γ > 0, there is a constant C = C(γ) > 0 such that
κn ≤C(γ)γn.
The proof of this fact is left to the reader as an exercise; see Problem 17.2. Below,
we will want to take 0 < γ < 1.

17.2 Uniqueness and Regularity of Solutions
515
Using the bounds we just established, the u-Lipschitz property of f , and the
deﬁnition of the sequence {u(k)}∞
k=0, we have, for all t ∈[0, T],
u(t) −u0 −
Z t
0
f (s, u(s))ds

2
=
u(t) −u(k+1)(t) +
Z t
0

f (s, u(k)(s)) −f (s, u(s))

ds

2
≤
u(t) −u(k+1)(t)

2 +

Z t
0

f (s, u(k)(s)) −f (s, u(s))

ds

2
≤
u(t) −u(k+1)(t)

2 +
Z t
0
f (s, u(k)(s)) −f (s, u(s))

2 ds
≤
u(t) −u(k+1)(t)

2 + L
Z t
0
u(k)(s) −u(s)

2 ds
≤C(γ)γk+2 M
L eLt + L
Z t
0
M
L C(γ)γk+1eLsds
≤C(γ)γk+2 M
L eLt + Lt M
L C(γ)γk+1eLt
≤C(γ)M
L eLT (γ + LT) γk+1
for all t ∈[0, T]. Picking γ < 1 and letting k →∞, we see that u is a mild
solution.
17.2
Uniqueness and Regularity of Solutions
We have just established the global existence of a mild solution, and therefore also
a classical solution. To prove uniqueness, we can use an a posteriori argument. We
will need the following estimates for the uniqueness proof.
Lemma 17.7 (Gr¨onwall-type inequalities5). Let T > 0, K1 ≥0, K2 ≥0, and
Φ ∈C1([0, T]). If Φ(0) = 0, Φ(t) ≥0 for all t ∈[0, T], and
Φ′(t) ≤K1Φ(t) + K2,
then
Φ(t) ≤K2
K1

eK1t −1

and
Φ′(t) ≤K2eK1t.
Proof. The solution to the IVP
Φ′(t) −K1Φ(t) = α(t),
t ∈[0, T],
Φ(0) = 0
5 Named in honor of the Swedish–American mathematician Thomas Hakon Gr¨onwall
(1877–1932).

516
Initial Value Problems for Ordinary Diﬀerential Equations
is
Φ(t) = eK1t
Z t
0
α(s)e−K1sds.
In the present case, α(t) ≤K2 for all t ∈[0, T]. Hence,
Φ(t) ≤eK1tK2
Z t
0
e−K1sds
= −eK1tK2
K1
e−K1s

s=t
s=0
= eK1t K2
K1

1 −e−K1t
= K2
K1

eK1t −1

.
Finally, using the last estimate,
Φ′(t) = α(t) + K1Φ(t) ≤K2 + K1
K2
K1

eK1t −1

= K2eK1t,
and the proof is complete.
The next ingredient that is needed is a continuous-dependence result with respect
to initial data.
Theorem 17.8 (continuous dependence). Let Ω0 ⊆Ω(and one or both possibly
equal to Rd). Assume that f
∈C
 S; Rd
is u-Lipschitz on S with Lipschitz
constant L > 0. Assume that, for each q ∈Ω0, there exists a classical solution,
u( · ; q) ∈C1([0, T]; Ω), to the parameterized IVP
u′(t; q) = f (t, u(t; q)),
u(0; q) = q.
(17.7)
Then, for all q1, q2 ∈Ω0 and t ∈[0, T], we have
∥u(t; q1) −u(t; q2)∥2 ≤exp(Lt) ∥q1 −q2∥2 .
(17.8)
Proof. Owing to Theorem 17.3, a classical solution is a mild solution. Thus, the
corresponding parameterized mild solution satisﬁes, for all t ∈[0, T],
u(t; q) = q +
Z t
0
f (s, u(s; q))ds.
Hence,
u(t; q1) −u(t; q2) = q1 −q2 +
Z t
0
[f (s, u(s; q1)) −f (s; u(s; q2))]ds,
by the triangle inequality, and the fact that f is u-Lipschitz,
∥u(t; q1) −u(t; q2)∥2 ≤∥q1 −q2∥2 + L
Z t
0
∥u(s; q1) −u(s; q2)∥2 ds.
(17.9)
Deﬁne
Φ(t) =
Z t
0
∥u(s; q1) −u(s; q2)∥2 ds;

17.2 Uniqueness and Regularity of Solutions
517
in which case,
Φ′(t) = ∥u(t; q1) −u(t; q2)∥2 .
By estimate (17.9), for all t ∈[0, T], we have that
Φ′(t) −LΦ(t) ≤∥q1 −q2∥2 .
The ﬁnal result now follows from the second Gr¨onwall inequality in Lemma 17.7
∥u(t; q1) −u(t; q2)∥2 = Φ′(t) ≤∥q1 −q2∥2 eLt,
as we intended to show.
With these two results, we are ready to prove uniqueness.
Corollary 17.9 (uniqueness). With the same hypotheses as in Theorem 17.6, the
solution u ∈C1([0, T]; Rd) is unique.
Proof. See Problem 17.4.
Following classic references, such as [86], we deﬁne specialized classes of slope
functions.
Deﬁnition 17.10 (classes of slope functions). Let f
∈C
 S; Rd
be a slope
function. We say that f ∈F 1(S) if and only if f ∈C1 S; Rd
, and there is a
real number A > 0 such that, for any i, j = 1, . . . , d, and all (t, v) ∈S,
∂ujfi(t, v)
 ≤A.
We also deﬁne, for m ∈N,
Fm(S) = F 1(S) ∩Cm(S; Rd).
Proposition 17.11 (F 1 implies Lipschitz). Let f be a slope function in F 1(S).
Then, f is u-Lipschitz on S.
Proof. See Problem 17.6.
Remark 17.12 (simpliﬁcation). The assumption that f ∈F 1(S) is not often
veriﬁed in practice. For example, consider the autonomous diﬀerential equation
u′(t) = −u3 + u,
t ∈[0, ∞)
with u(0) = u0 ∈R. In this case, f (t, u) = −u3 + u. Clearly, the ﬁrst derivative
of the slope function f with respect to u is unbounded; consequently, f ̸∈F 1(S).
Yet, this autonomous ODE has a bounded classical solution on [0, ∞). In fact, one
can show that limt→∞u(t) = 1, if u0 > 0, and limt→∞u(t) = −1, if u0 < 0.
The introduction of the class F 1(S) is merely for convenience, as the assumption
f ∈F 1(S) makes the analysis much simpler. We will not explain what is required to
remove this assumption; rather, we will content ourselves with saying that this can
often be accomplished, though usually with a little more sophisticated machinery
and a lot more eﬀort.

518
Initial Value Problems for Ordinary Diﬀerential Equations
Theorem 17.13 (higher diﬀerentiability). Let m ∈N. Suppose that the slope
function satisﬁes f ∈Fm(S). Then the unique classical solution on I to (17.1),
which we denote u ∈C1 I; Rd
, actually belongs to Cm+1 I; Rd
.
Proof. As u is a classical solution, for all t ∈I, we have
u′(t) = f (t, u(t)).
The right-hand side of this identity is diﬀerentiable on I; in particular,
d
dt [f (t, u(t))] = ∂tf (t, u(t)) + Duf (t, u(t))f (t, u(t)),
where Duf =

∂ujfi
d
i,j=1 is the d×d matrix of partial derivatives of f with respect to
u. Consequently, u′′(t) exists and is continuous on I. The higher order derivatives
exist and are continuous on I, as may be seen via an induction argument; see
Problem 17.7.
17.3
The Flow Map and the Alekseev–Gr¨obner Lemma
In Theorem 17.8, we studied the continuous dependence of the solution to an IVP
with respect to initial data. Our purpose here will be to study how the solution
depends on the changes in the slope function. We will only state the main results
in this direction and refer the reader to [37, Chapter 1] for more details.
We begin with a deﬁnition that might seem a bit odd.
Deﬁnition 17.14 (ﬂow map). Suppose that, for some m ∈N, the slope function
satisﬁes f ∈Fm(S). The ﬂow map of z′(t) = f (t, z(t)), denoted
U : I × Ω× I →Rd,
is deﬁned by
U(s, v, t) = us,v(t),
where us,v ∈C1 I; Rd
is the unique solution to the ODE problem
dus,v
dt (t) = f (t, us,v(t)),
us,v(s) = v.
(17.10)
This deﬁnition emphasizes that the solution of an IVP is a function not only of
the ﬁnal time but also of the initial time, and the value of the initial condition.
If we change any of these, the solution may change. First, we have the following
simple properties of the ﬂow map.
Proposition 17.15 (properties of the ﬂow map). Suppose that, for some m ∈N,
the slope function satisﬁes f ∈Fm(S). Denote by U the ﬂow map of z′(t) =
f (t, z(t)). Then
U(s, v, s) = v,
∀(s, v) ∈S.

17.3 The Flow Map and the Alekseev–Gr¨obner Lemma
519
For any t1, t2 ∈I and all v ∈Ω, we have
U(s, v, t2) = U (t1, U(s, v, t1), t2).
In addition, for any s, t ∈I and all v ∈Ω,
∂
∂t U(s, v, t) = f (t, U(s, v, t)).
Finally, U is continuously diﬀerentiable with respect to its second variable, i.e., the
initial condition. This derivative, denoted DvU, is a d × d matrix at every point
(s, v, t) ∈I × Ω× I, and we have
[DvU(s, v, t)]i,j = ∂Ui
∂vj
(s, v, t).
Furthermore, DvU is diﬀerentiable with respect to its third argument, t, and
satisﬁes the diﬀerential equation
∂
∂t DvU(s, v, t) = Duf (t, U(s, v, t))DvU(s, v, t)
subject to the initial data
DvU(s, v, s) = Id,
where Duf is the derivative of f with respect to its second argument.
Theorem 17.16 (Alekseev–Gr¨obner Lemma6). Let t0 ∈I and, for some m ∈N,
f , g ∈Fm(S). Denote by U the ﬂow map of z′(t) = f (t, u(t)). Suppose that
u, v ∈C1 I; Rd
are the unique classical solutions on I of
u′(t) = f (t, u(t)),
t ∈I,
u(t0) = u0,
v ′(t) = f (t, v(t)) + g(t, v(t)),
t ∈I,
v(t0) = u0.
Then
v(t) = u(t) +
Z t
t0
DvU(s, v(s), t)g (s, v(s))ds.
(17.11)
We can think of g as a perturbation function or a residual. This result then
measures the diﬀerence between two solutions: one a solution to the original system
and the second a solution to a perturbed system. Clearly, if g ≡0, there is no
diﬀerence in the solutions. Furthermore, we can imagine that g represents a small
diﬀerence resulting from some numerical approximation, and v is an approximate
solution. In this case, (17.11) measures the error in our approximation. We will use
this latter characterization to quantify the error of certain numerical methods.
6 Named in honor of the Soviet mathematician Vladimir Mikhailovich Alekseev (1932–1980)
and the Austrian mathematician Wolfgang Gr¨obner (1899–1980).

520
Initial Value Problems for Ordinary Diﬀerential Equations
17.4
Dissipative Equations
In this section, we brieﬂy present some facts about the theory of so-called dissipative
equations, which is a class of problems that often appears in practice. Let us, ﬁrst of
all, note that all the theory that we developed so far regarding existence, uniqueness,
and regularity only relies on the fact that Rd, with d ∈N, is a ﬁnite-dimensional
normed space. No other structural properties were used on the range of the slope
function or the solution. For this reason, and to allow for suﬃcient generality, in
this section we let d ∈N and consider IVPs posed on subsets of Cd. Finally, in this
section, we let (·, ·) be an inner product on Cd and ∥·∥be the induced norm.
Deﬁnition 17.17 (monotonicity). Let f : [0, T] × Cd →Cd. We say that f is
monotone with respect to ( · , · ) if and only if
ℜ[(v 1 −v 2, f (t, v 1) −f (t, v 2))] ≤0
for all t ∈[0, T] and every v 1, v 2 ∈Cd.
Theorem 17.18 (dissipativity). Assume that f : [0, T] × Cd →Cd is monotone
with respect to (·, ·). Let u0, v 0 ∈Cd. Assume that f is such that there are unique
classical solutions on [0, T] to the problems
u′(t) = f (t, u(t)),
u(0) = u0,
v ′(t) = f (t, v(t)),
v(0) = v 0.
In this setting, we have that, for all t ∈[0, T],
d
dt ∥u(t) −v(t)∥2 ≤0.
Furthermore, for all 0 ≤t1 ≤t2 ≤T,
∥u(t2) −v(t2)∥≤∥u(t1) −v(t1)∥≤∥u0 −v 0∥.
Proof. Set E(t) = 1
2 ∥u(t) −v(t)∥2. Then
d
dt E(t) = ℜ[(u(t) −v(t), u′(t) −v ′(t))]
= ℜ[(u(t) −v(t), f (t, u(t)) −f (t, v(t)))]
≤0.
This proves that the function E is nonincreasing. Thus, the second inequality
follows; see Problem 19.7.
Remark 17.19 (dissipativity). An ODE problem with these properties is said to be
dissipative with respect to ( · , · ).
Example 17.2
Let A ∈Cd×d be such that if λ ∈σ(A), then ℜλ ≤0. The slope
function
f (t, v) = Av
is monotone.

17.5 Lyapunov Stability
521
17.5
Lyapunov Stability
In this section, we present some elements of the qualitative theory of IVP for ODEs.
In other words, we will try to extract information about the behavior of the solution
to an IVP, specially for large times, without necessarily knowing the exact solution
to this problem. A particular instance of this is the theory of Lyapunov stability,
which we will present here. As always, we are barely touching the subject of a deep
and rich subject; namely, stability of dynamical systems. The interested reader is
referred to [4, 57, 62, 74, 90] for much more insight and further developments.
We will be mostly interested in, for a ﬁxed slope function f , how the — mild or
classical — solution to (17.1) depends on the initial condition u0, specially for large
times. Thus, we will operate under the implicit assumption that the slope function
is such that, for every initial condition, a — mild or classical — solution exists and
is unique for all positive times, i.e., T = ∞.
Deﬁnition 17.20 (trajectory). Let f be a slope function such that, for every
initial condition, problem (17.1) has a unique — mild or classical — solution in
C([0, ∞); Rd). The mapping
Rd →C([0, ∞); Rd),
u0 7→u( · ; u0) = U(0, u0, ·),
where U is the ﬂow map, is called the trajectory associated with u0.
The beginning of our considerations is the content of Theorem 17.8. This result
shows that, for a ﬁxed time t > 0, the trajectory is a Lipschitz mapping. The
Lipschitz constant, however, depends (exponentially) on t. While this is enough to
guarantee uniqueness of solutions, it does not say anything about how close (for
large t) two trajectories stay, provided they originate from points nearby. Having
this property is related to stability, a rigorous deﬁnition of which we provide below.
Deﬁnition 17.21 (Lyapunov stability7). Let u0 ∈Rd. The trajectory u(·; u0)
associated with u0 is called Lyapunov stable if, for every ε > 0, there is δ > 0
such that, whenever
∥u0 −q∥2 < δ,
we have
∥u(t; u0) −u(t; q)∥2 < ε,
∀t > 0.
A trajectory that is not Lyapunov stable is called unstable.
There are other notions of stability. Some are more general, some more
specialized, but we shall not concern ourselves with those here. We are now ready
to state our goal of this section: given a trajectory, we wish to determine whether
it is Lyapunov stable. For special cases, we can already provide a positive answer,
as the following example shows.
7 Named in honor of the Russian mathematician Aleksandr Mikhailovich Lyapunov
(1857–1918).

522
Initial Value Problems for Ordinary Diﬀerential Equations
Example 17.3
Let the slope function f be monotone in the sense of Deﬁnition
17.17. Theorem 17.18 shows that any trajectory is Lyapunov stable.
Given u0 ∈Rd, assume that u( · ; u0) is its associated trajectory. Suppose that
we wish to study its Lyapunov stability. To do so, we now introduce some useful
reductions. Let q ∈Rd be arbitrary and u( · , q) be its associated trajectory. Let us
introduce the change of variables
v(t) = u(t; q) −u(t; u0).
(17.12)
In this new coordinate system, we have, using the diﬀerential equation,
v ′(t) = u′(t; q) −u′(t; u0) = f (t, u(t; q)) −f (t, u(t; u0)) = A(t; q, u0)v(t),
where, in the last step, we used the Mean Value Theorem B.57 for vector-valued
functions, so that
A(t; q, u0) =
Z 1
0
Duf (t, su(t; q) + (1 −s)u(t; u0))ds.
(17.13)
In short, we have reduced the study of the Lyapunov stability of an arbitrary
trajectory u( · ; u0) to the study of the Lyapunov stability of the trivial stationary
point v ≡0 for the linear ODE
v ′(t) = A(t; q, u0)v(t),
(17.14)
where the matrix-valued function A(·; q, u0) is given by (17.13).
Remark 17.22 (stationary points). Let us provide some intuition into the value
of stationary points, and specially of the trivial one, i.e., v ≡0. To do this, let us
consider the autonomous problem
u′(t) = f (u(t)),
t > 0.
It is important to notice that most laws of physics are described by an autonomous
system. This, for instance, is a consequence of Galilean invariance.8 In layman’s
terms, this means that the laws of physics, unlike many unfortunate things in life,
are independent of the day of the week. For the same reason, if we are dealing
with a physical system, it is not unreasonable to assume that f (0) = 0 (zero input
equals zero output), so that u(t; 0) ≡0. In this case, all our derivations reduce to
studying the Lyapunov stability of the origin with respect to the system
v ′(t) = A(t)v(t),
A(t) =
Z 1
0
Duf (su(t; q))ds.
(17.15)
One ﬁnal reduction can get us to where we want to be, so that we can develop
our desired stability theory. Let us assume that not only is the slope function
8 Named in honor of the Italian astronomer, physicist, engineer, and Rennaissance man Galileo
di Vincenzo Bonaiuti d´e Galilei (1564–1642).

17.5 Lyapunov Stability
523
continuously diﬀerentiable with respect to its second argument but also that this
derivative is constant on the trivial solution, i.e.,
d
dt Duf (t, 0) ≡O.
If this is the case, we can write
v ′(t) = A(t; q, u0)v(t) = Av(t) + g(t, v),
where, for every t,
1
∥v∥2 g(t, v) →0 as v →0. In conclusion, to study the stability
of trajectories it is enough, at this level, to study the stability of the origin for the
system
v ′(t) = Av(t),
(17.16)
where A ∈Rd×d. This is called the ﬁrst or linear approximation to (17.1).
Theorem 17.23 (stability). Let A ∈Rd×d. The origin is a stable solution of
(17.16) if and only if
λ ∈σ(A)
=⇒
ℜλ ≤0;
if λ ∈σ(A) is such that ℜλ = 0, then its algebraic and geometric multiplicities
coincide.
Proof. For a given initial condition v 0 ∈Rd, we can explicitly write the solution to
the IVP for (17.16). Namely,
v(t) = exp(tA)v 0,
where exp(tA) denotes the matrix exponential. If the conditions on the spectrum
of A are satisﬁed, then
α = sup
t>0 ∥exp(tA)∥2 < ∞.
Let ε > 0. Then, for any v 0 with ∥v 0∥2 < ε
α,
∥v(t)∥2 = ∥exp(tA)v 0∥2 ≤α ∥v 0∥2 < ε,
and the origin is stable.
Assume now that there is λ ∈σ(A) with ℜλ > 0. Let q be a corresponding
eigenvalue. Given ε ∈(0, 1), deﬁne v 0 = εq to see that the solution
v(t) = exp(tA)v 0 = εeλtq
grows unboundedly large, regardless of the value of ε. This shows that, in this case,
the origin is unstable.
If λ ∈σ(A) has geometric multiplicity strictly smaller than its algebraic, then we
can ﬁnd w ∈C∞([0, ∞); Rd) such that
v(t) = exp(tA)v 0 + teλtw(t).
Now, since ℜλ = 0, |eλt| = 1 for all t and, regardless of w, the second term grows
unboundedly large. Thus, in this case, the origin is unstable as well.

524
Initial Value Problems for Ordinary Diﬀerential Equations
Problems
17.1
Prove Theorem 17.3.
17.2
Let L, T > 0, n ∈N, and deﬁne κn = (LT)n/n!. Show that, for every
γ > 0, there is C = C(γ) > 0 such that
κn ≤Cγn,
∀n ∈N.
17.3
In the setting of Theorem 17.8, assume, in addition, that d = 1 and that
the slope function satisﬁes f ∈C1(S; R)
∂uf (t, v) ≤0,
∀(t, v) ∈S.
Show that, in this case, estimate (17.8) can be improved to
|u(t, q1) −u(t, q2)| ≤|q1 −q2|.
17.4
Prove Corollary 17.9.
17.5
Show that u = −t2/4 and u = 1 −t are solutions of the IVP
2u′(t) =
p
t2 + 4u −t,
u(2) = −1.
Why does this not contradict the uniqueness of Corollary 17.9?
17.6
Prove Proposition 17.11.
17.7
Complete the proof of Theorem 17.13.
17.8
Consider the scalar equation
u′(t) = au,
where a ∈R⋆. Show that if a < 0 any trajectory is Lyapunov stable, whereas if
a > 0 all trajectories are unstable.
17.9
Show that the change of variables (17.12) is nonsingular.
17.10
Justify the derivations that lead to (17.15).

18
Single-Step Methods
In this chapter, we begin the study of approximation methods for initial value
problems (IVPs). The approach we will follow is motivatived by (17.2), which is
the deﬁning relation that a mild solution must satisfy. Indeed, if u′(t) = f (t, u(t))
and u(t0) = u0, we can approximate u(t) via
u(t) = u0 +
Z t
t0
f (s, u(s))ds ≈u0 + Q(t0,t)
1,0
[f ( · , u( · ))],
where Q(t0,t)
1,0
is a quadrature formula. Notice that, in doing so, we now only require
knowledge of f ( · , u( · )) at the quadrature nodes. Thus, for instance, if we use the
simple left-hand Riemann sum approximation:
Q(t0,t)
1,0
[f (·, u(·))] = (t −t0)f (t0, u(t0)).
Then the approximation becomes
u(t) ≈u0 + (t −t0)f (t0, u(t0))
or
1
t −t0
(u(t) −u0) ≈f (t0, u(t0)).
This is Euler’s famous approximation method. In this chapter, we will examine the
convergence of methods of this type.
To simplify the discussion, in this and upcoming chapters, we consider the IVP
over the “time” interval I = [0, T] for T > 0. A simple linear transformation can
be used to reduce the general case to this one. In addition, we will suppose that
d ∈N, Ω⊆Rd is open, and u0 ∈Ω. We set S = [0, T] × Ωand assume that the
slope function satisﬁes, at least, f ∈C
 S; Ω

. We denote by u ∈C1([0, T]; Ω) a
classical solution on [0, T] to the IVP
u′(t) = f (t, u(t)),
u(0) = u0.
(18.1)
The methods we will present here do not give us a function that approximates
u but rather a sequence of vectors that approximate this function at a particular
collection of points in time. More precisely, we let K ∈N, set τ =
T
K , which
we call the time step size, and tk = kτ. We will then produce a ﬁnite sequence
{w k}K
k=0 ⊂Rd such that w k ≈u(tk).

526
Single-Step Methods
18.1
Single-Step Approximation Methods
We begin with a deﬁnition.
Deﬁnition 18.1 (single-step method). The ﬁnite sequence

w k	K
k=0 ⊂Rd is called
a single-step approximation to u if and only if w 0 = u0 and
w k+1 = w k + τG
 tk, τ, w k, w k+1
,
k = 0, . . . , K −1,
(18.2)
where G, called the slope approximation, satisﬁes G(t, 0, v, v) = f (t, v) and
G ∈C([0, T] × [0, T] × Rd × Rd; Rd).
The single-step approximation is called explicit if and only if G is independent of
the last variable; otherwise, the approximation is called implicit. The global error
of the single-step approximation is a ﬁnite sequence

ek	K
k=0 deﬁned via
ek = u(tk) −w k.
Let us present some examples of single-step approximation methods.
Example 18.1
The forward (or explicit) Euler method:1
G(t, s, v 1, v 2) = GF E(t, s, v 1) = f (t, v 1).
(18.3)
Example 18.2
The backward (or implicit) Euler method:
G(t, s, v 1, v 2) = GBE(t, s, v 2) = f (t + s, v 2).
(18.4)
Example 18.3
The trapezoidal method:
G(t, s, v 1, v 2) = GT R(t, s, v 1, v 2) = 1
2f (t, v 1) + 1
2f (t + s, v 2).
(18.5)
Example 18.4
Taylor’s method:2
G(t, s, v 1, v 2) = GT M(t, s, v 1)
= f (t, v 1) + s
2 [∂tf (t, v 1) + Duf (t, v 1)f (t, v 1)],
(18.6)
where Duf =

∂ujfi
d
i,j=1 is the d × d Jacobian matrix of partial derivatives of f
with respect to u.
Example 18.5
The midpoint method:
G(t, s, v 1, v 2) = GMR(t, s, v 1, v 2) = f

t + s
2, 1
2v 1 + 1
2v 2

.
(18.7)
1 Named in honor of the Swiss mathematician, physicist, astronomer, geographer, logician, and
engineer Leonhard Euler (1707–1783).
2 Named in honor of the British mathematician Brook Taylor (1685–1731).

18.2 Consistency and Convergence
527
Deﬁnition 18.2 (LTE and convergence). Let

w k	K
k=0 be a single-step approxi-
mation to u generated by the slope approximation G. The local truncation error
(LTE) or consistency error of the single-step approximation is deﬁned as
EEE[u](t, s) = u(t) −u(t −s)
s
−G(t −s, s, u(t −s), u(t))
for any t ∈[s, T]. We make frequent use of the notation EEEk[u] = EEE[u](tk, τ) for
k = 1, . . . , K. We say that the approximation method is consistent to at least
order p ∈N if and only if, whenever
u ∈Cp+1 ([0, T]; Ω),
there is a constant τ0 ∈(0, T] and a constant C > 0 independent of t and τ
such that
∥EEE[u](t, τ)∥2 ≤Cτp
(18.8)
for all τ ∈(0, τ0] and t ∈[τ, T]. We say that the single-step approximation is
consistent to exactly order p if and only if p is the largest positive integer for
which (18.8) holds regardless of how smooth the exact solution u is.
We say that the single-step approximation method converges globally if
lim
K→∞
max
k=0,...,K ∥ek∥2 = 0.
In addition, we say that it converges globally, with at least order p ∈N, if and only
if, when
u ∈Cp+1 ([0, T]; Ω),
there is some τ1 ∈(0, T] and a constant C > 0 independent of k and τ such that
ek
2 ≤Cτp
for all k = 1, . . . , K and any τ ∈(0, τ1].
18.2
Consistency and Convergence
Let us now study the consistency and convergence of some single-step approxima-
tion methods. We will present a few illustrative cases that highlight the type of
techniques and ideas that are needed. The consistency and convergence of many
other methods are found in the Problems at the end of the chapter.
A useful tool in this will be a discrete analogue of the Gr¨onwall3 inequality
proved in Lemma 17.7.
Lemma 18.3 (discrete Gr¨onwall). Let K ∈N. Suppose that the ﬁnite sequence
{ak}K
k=0 ⊂R+ ∪{0} satisﬁes a0 = 0 and, for some b > 1, c ≥0,
ak+1 ≤bak + c,
k = 0, . . . , K −1.
3 Named in honor of the Swedish–American mathematician Thomas Hakon Gr¨onwall
(1877–1932).

528
Single-Step Methods
Then, for all k = 0, . . . , K,
ak ≤
c
b −1

bk −1

.
Proof. The proof is by induction. The base case, k = 0, is trivial since a0 = 0.
For the induction hypothesis, we assume that
ak ≤
c
b −1

bk −1

holds for every k = 0, . . . , n.
For the induction step, we observe that, by assumption,
an+1 ≤ban + c
≤b
c
b −1 [bn −1] + c
=
c
b −1

bn+1 −b

+ c
=
c
b −1bn+1 −
bc
b −1 + (b −1)c
b −1
=
c
b −1bn+1 −
c
b −1
=
c
b −1

bn+1 −1

,
which completes the induction argument.
18.2.1
Forward Euler Method
Proposition 18.4 (consistency). Suppose that d = 1 and f ∈F1(S). Then, for
all s ∈(0, T] and t ∈[s, T],
u(t) = u(t −s) + sf (t −s, u(t −s)) + sE[u](t, s),
where E[u](t, s) satisﬁes
|E[u](t, s)| ≤Cs
and C > 0 is a constant that is independent of t and s.
Proof. Since f ∈F1(S), u ∈C2([0, T]). Fix s ∈(0, T] and t ∈[s, T]. By Taylor’s
Theorem B.31, for some η ∈(t −s, t),
u(t) = u(t −s) + u′(t −s)s + 1
2u′′(η)s2.
Hence,
u(t) = u(t −s) + sf (t −s, u(t −s)) + sE[u](t, s),
where
E[u](t, s) = 1
2u′′(η)s.
The result is proved using that
|E[u](t, s)| ≤s
2
max
t−s≤η≤t |u′′(η)| ≤s
2 max
0≤η≤T |u′′(η)|

18.2 Consistency and Convergence
529
and taking
C = 1
2 max
0≤η≤T |u′′(η)|.
An analogous result, for d > 1, immediately implies consistency of the forward
Euler method.
Corollary 18.5 (consistency). The forward Euler method (18.3) is of exactly order
p = 1. In other words, if f ∈F1(S) and u ∈C2([0, T]; Rd) is the unique solution
to (18.1), then, for all τ ∈(0, T] and t ∈(τ, T],
∥EEE[u](t, τ)∥2 ≤CF Eτ
(18.9)
for some CF E > 0 that is independent of t and τ.
Proof. See Problem 18.3.
Theorem 18.6 (convergence). Suppose that f ∈F1(S). Let L > 0 be its
u-Lipschitz constant on S. Suppose that the forward Euler method (18.3) is used
to approximate u, the unique solution to (18.1). Then, for all k = 0, . . . , K,
ek
2 ≤CF E
L

eT L −1

τ,
where CF E > 0 is the LTE constant from (18.9). Consequently,
max
k=1,...,K
ek
2 ≤CF E
L

eT L −1

τ.
Proof. Recall that ek = u(tk) −w k and notice that we have the error equation
ek+1 = ek + τf (tk, u(tk)) −τf (tk, w k) + τEEEk+1[u]
for k = 0, . . . , K −1 with e0 = 0. Using the triangle inequality, the Lipschitz
condition, and the LTE bound (18.9), we have, for k = 0, . . . , K −1,
ek+1
2 ≤
ek
2 + τL
ek
2 + τ
EEEk+1[u]

2 ≤(1 + τL)
ek
2 + CF Eτ2.
Using Lemma 18.3, we ﬁnd, for k = 0, . . . , K,
ek
2 ≤CF E
L

(1 + τL)k −1

τ.
Now, since τL > 0, 1 + τL < eτL. Hence,
ek
2 ≤CF E
L

eτkL −1

τ ≤CF E
L

eT L −1

τ
for all k = 0, . . . , K.
18.2.2
Trapezoidal Method
Proposition 18.7 (consistency). Suppose that d = 1 and f ∈F2(S). Then, for
all s ∈(0, T] and t ∈[s, T], we have
u(t) = u(t −s) + s
2 [f (t −s, u(t −s)) + f (t, u(t))] + sE[u](t, s),

530
Single-Step Methods
where E[u](t, s) satisﬁes
|E[u](t, s)| ≤Cs2,
where C > 0 is a constant that is independent of s and t.
Proof. Since f ∈F2(S), we have that u ∈C3([0, T]). By Taylor’s Theorem B.31,
for some η ∈(t −s, t −s/2),
u(t −s/2) = u(t −s) + u′(t −s)s
2 + 1
2u′′(η)s2
4 .
Likewise, for some ζ ∈(t −s/2, t),
u(t −s/2) = u(t) + u′(t)(−s)
2
+ 1
2u′′(ζ)(−s)2
4
.
Subtracting, we have
u(t) = u(t −s) + s
2 (u′(t −s) + u′(t)) + s2
8 (u′′(η) −u′′(ζ)).
Using the Mean Value Theorem B.30, for some χ ∈(η, ζ),
u′′(η) −u′′(ζ) = u′′′(χ)(η −ζ).
Hence,
u(t) = u(t −s) + s
2 [f (t −s, u(t −s)) + f (t, u(t))] + sE[u](t, s),
where
E[u](t, s) = s η −ζ
8
u′′′(χ).
Since |η−ζ| ≤s and u ∈C3([0, T]), the result is proved via the following estimate:
|E[u](t, s)| ≤s2
8
max
t−s≤χ≤t |u′′′(χ)| ≤s2
8
max
t∈[0,T ] |u′′′(t)|.
An extension, to d > 1, of this result implies consistency of the trapezoidal
method.
Corollary 18.8 (consistency). The trapezoidal method (18.5) is of order exactly
p = 2. Precisely, if f ∈F2(S), then, for all τ ∈(0, T] and t ∈[τ, T],
∥EEE[u](t, τ)∥2 ≤CT Rτ2
(18.10)
for some CT R > 0 that is independent of t and s.
Proof. See Problem 18.6.
We can now obtain global convergence of the trapezoidal method.
Theorem 18.9 (convergence). Let f ∈F2(S) and L > 0 be its u-Lipschitz
constant on S. Suppose that the trapezoidal method (18.5) is used to approximate
the solution to (18.1). Then, for all k = 0, . . . , K, we have
ek
2 ≤CT R
L
[exp(2TL) −1] τ2,

18.2 Consistency and Convergence
531
provided that 0 < τL < 1, where CT R > 0 is the LTE constant from (18.10).
Proof. As in the case of the forward Euler method, we begin by identifying an
equation for the error ek = u(tk) −w k. In this case, we have
ek+1 = ek + τ
2

f (tk, u(tk)) −f (tk, w k)

+ τ
2

f (tk+1, u(tk+1)) −f (tk+1, w k+1)

+ τEEEk+1[u].
We take norms and apply the triangle inequality, the u-Lipschitz condition on f
and (18.10), to obtain

1 −τL
2

∥ek+1∥2 ≤

1 + τL
2

∥ek∥2 + CT Rτ3.
Since, by assumption, 1
2 < 1 −τL
2 < 1, Lemma 18.3 then implies that
∥ek+1∥2 ≤CT Rτ3
1 −τL
2
" 
1 + τL
2
1 −τL
2
!
−1
#−1 

 
1 + τL
2
1 −τL
2
!k
−1


= CT R
L


 
1 + τL
2
1 −τL
2
!k
−1

τ2.
Finally, notice that
1 + τL
2
1 −τL
2
= 1 +
τL
1 −τL
2
≤1 + 2τL ≤e2τL
to, in conclusion, obtain that
∥ek+1∥2 ≤CT R
L [e2kτL −1]τ2 ≤CT R
L [e2T L −1]τ2,
as claimed.
18.2.3
Taylor’s Method
As a last example, we consider the consistency and convergence of Taylor’s method.
Theorem 18.10 (consistency). Suppose that f ∈F2(S). For any s ∈(0, T] and
t ∈(s, T], we have that the LTE of Taylor’s method satisﬁes
∥EEE[u](t, s)∥2 ≤CT Ms2
(18.11)
for some CT M > 0 that is independent of t and s. In other words, Taylor’s method
is consistent to exactly order p = 2.
Proof. For simplicity, we will only consider the case d = 1. Since f ∈F2(S), we
know that u ∈C3([0, T]). Thus, using Taylor’s Theorem, we get that
u(t) = u(t −s) + su′(t −s) + s2
2 u′′(t −s) + s3
6 u′′′(ξ)

532
Single-Step Methods
for some ξ ∈(t −s, t). Now we note that u′(t −s) = f (t −s, u(t −s)) and also
u′′(t −s) = ∂f
∂t (t −s, u(t −s)) + ∂f
∂u (t −s, u(t −s))f (t −s, u(t −s)).
Hence,
sE[u](t, s) = u(t) −u(t −s) −sf (t −s, u(t −s)) −s2
2
∂f
∂t (t −s, u(t −s))
−s2
2
∂f
∂u (t −s, u(t −s))f (t −s, u(t −s))
= s3
6 u′′′(ξ).
The result follows.
Theorem 18.11 (convergence). Suppose that f
∈F2(S) and there is some
constant B > 0 such that
|Dαfi(t, v)| ≤B
for all multi-indices α ∈Nd+1 with |α| = 2, for all i = 1, . . . , d, and for all
(t, v) ∈S. (In other words, all second derivatives are bounded on S.) Then Taylor’s
method (18.6) is convergent and the global rate of convergence is p = 2. In
particular,
ek
2 ≤CT M
L′
h
eT L′ −1
i
s2,
where CT M > 0 is the LTE constant from (18.11) and L′ > 0 is a Lipschitz
constant given below.
Proof. The key step in this proof is to establish a global u-Lipschitz continuity for
the slope approximation, i.e., an estimate of the form
∥GT M(t, s, v 1) −GT M(t, s, v 2)∥2 ≤L′ ∥v 1 −v 2∥2 ,
for any s ∈(0, T], for any t ∈[s, T], and for all v 1, v 2 ∈Rd. This requires the
extra assumptions on the slope function that are included in the hypotheses. The
details are left to the reader as an exercise; see Problem 18.9.
18.3
Linear Slope Functions
In the case that the slope function is linear, we can provide more direct proofs of
convergence. The following example will be of interest in Chapter 28, where we
examine numerical methods for the heat equation.
Theorem 18.12 (convergence). Let A ∈Rd×d be symmetric. Suppose that
u : [0, T] →Rd is the solution to
u′(t) = Au(t),
u(0) = u0 ∈Rd.

18.3 Linear Slope Functions
533
Let K ∈N. Suppose that the sequence

w k	K
k=0 is generated using the forward
Euler method (18.3). Then, for all k = 0, . . . , K, we have
ek
2 ≤∥u0∥2 max
λ∈σ(A)
eλkτ −(1 + λτ)k .
(18.12)
Suppose that the maximum on the right-hand side of (18.12) is achieved at
λmax ∈σ(A) and, furthermore, that λmax < 0. Then there is a constant τ0 ∈(0, T]
such that, for all τ ∈(0, τ0] and all k = 1, 2, . . . , K,
ek
2 ≤T
2 λ2
max ∥u0∥2 τ.
Proof. Since A ∈Rd×d is symmetric, it is orthogonally diagonalizable. In other
words, there exists a diagonal matrix D (whose diagonal entries [D]i,i = λi are the
eigenvalues of A) and an orthogonal matrix Q such that A = QDQ⊺. The exact
solution of the equation is then given by
u(t) = QetDQ⊺u0,
where etD is a diagonal matrix whose diagonal entries are precisely

etD
i,i = etλi.
Now using the forward Euler method, it is easy to see that
w k =
 I + τA)ku0 = Q(I + τD
k Q⊺u0.
Thus,
ek = QekτDQ⊺u0 −Q(I + τD)k Q⊺u0 = Q

ekτD −(I + τD)k
Q⊺u0.
Taking norms, we get
ek
2 ≤
Q

ekτD −(I + τD)k
Q⊺
2 ∥u0∥2 ≤
ekτD −(I + τD)k
2 ∥u0∥2 .
Notice that (I + τD)k is a diagonal matrix with the entries (1+τλi)k. To conclude,
we use the fact that the 2-norm of a diagonal matrix is simply the largest diagonal
element in absolute value. Hence,
ek
2 ≤∥u0∥2 max
λ∈σ(A)
ekτλ −(1 + τλ)k .
Now, using Taylor expansions, we see that, for any x ≤0,
1 + x ≤ex ≤1 + x + 1
2x2.
Equivalently,
1 + x −1
2x2 ≤ex −1
2x2 ≤1 + x ≤ex.
Using the binomial expansion, it follows that, if n ≥2,
(1 −α)n = 1 −nα +
n
2

α2 +
n
X
j=3
n
j

(−1)jαj.

534
Single-Step Methods
There is an α0 ∈(0, 1) such that if α ∈(0, α0), then
n
2

α2 +
n
X
j=3
n
j

(−1)jαj ≥0.
Essentially, the ﬁrst term, which is positive, dominates the others. Thus, if n ≥2
and α ∈(0, α0),
(1 −α)n ≥1 −nα.
We apply the last estimate with
α = x2
2ex .
Thus,
1 −n x2
2ex ≤

1 −x2
2ex
n
.
Multiplying by enx, we get
enx −nx2
2 e(n−1)x ≤

ex −x2
2
n
≤(1 + x)n ≤enx,
provided that x ∈(−1, 0] and x is suﬃciently small in absolute value.
Now, if τλmax ∈[−1, 0) is suﬃciently small in absolute value,
−k (τλmax)2
2
e(k−1)τλmax ≤(1 + τλmax)k −ekτλmax ≤0,
or, equivalently,
0 ≤ekτλmax −(1 + τλmax)k ≤k (τλmax)2
2
e(k−1)τλmax ≤T
2 λ2
maxτ.
The result follows.
Problems
18.1
Alternate discrete Gr¨onwall inequality: Let K ∈N. Suppose that the
sequence {ak}K
k=0 ⊂R+ is such that there are b > 0 and c ≥0 for which
ak+1 ≤bak + c,
k = 0, . . . , K −1.
Prove that, for all k = 0, . . . , K,
ak ≤bka0 +


k−1
X
j=0
bj

c.
For b ̸= 1, show that
ak ≤bka0 + bk −1
b −1 c.

Problems
535
18.2
Yet another discrete Gr¨onwall inequality: Let γ, β ∈R with β > 0 and
γ > −1. Let {an}n∈N and {fn}n∈N be two sequences of nonnegative real numbers
satisfying
(1 + γ)an+1 ≤an + βfn.
Prove that
an+1 ≤
a0
(1 + γ)n+1 + β
n
X
k=0
fk
(1 + γ)n−k+1 .
18.3
Prove Corollary 18.5.
18.4
Suppose that f ∈F1(S). Prove that the backward Euler method is:
a)
Consistent to exactly order p = 1.
b)
Globally convergent with order p = 1.
18.5
Show that a sharper LTE estimate than the Proposition 18.7 estimate can
be obtained. Speciﬁcally, under the same assumptions, prove that
|E[u](t, s)| ≤s2
12 max
t∈[0,T ] |u′′′(t)|.
18.6
Prove Corollary 18.8.
18.7
Suppose that f ∈F2(S). Prove that the midpoint method is:
a)
Consistent to exactly order p = 2.
b)
Globally convergent with order p = 2.
18.8
Let θ ∈[0, 1]. The θ-method is deﬁned as
w k+1 = w k + τf (tk + (1 −θ)τ, θw k + (1 −θ)w k+1).
Assuming that f ∈F2(S), ﬁnd the consistency order of this method and show that
it is convergent.
Hint: The order of consistency depends on the value of θ.
18.9
Complete the proof of Theorem 18.11.
18.10
Consider the IVP
u′(t) = u(t),
t ∈(0, 1],
u(0) = 1.
For K ∈N, set τ = 1
K . Apply the following methods to obtain approximations of
u(1) = e.
a)
Forward Euler method.
b)
Taylor’s method.
c)
Heun’s4 method:
w k+1 = w k + τ
2

f (tk, w k) + f
 tk+1, w k + τf (tk, w k)

.
d)
Modiﬁed Euler’s method:
w k+1 = w k + τf
h
tk + τ
2 , w k + τ
2 f (tk, w k)
i
.
For these approximations, show directly (without appealing to any convergence
theorems) that w K →u(1) = e as K →∞.
4 Named in honor of the German mathematician Karl Heun (1859–1929).

19
Runge–Kutta Methods
In this chapter, we introduce Runge–Kutta (RK) approximation methods for initial
value problems (IVPs). These are single-step methods that have multiple stages,
and the form and function of these methods are rather distinct from those we have
previously deﬁned. For simplicity, we will largely neglect the convergence theory of
RK methods, though it would not present any new technical diﬃculties, only tedious
calculations.
We begin by recalling that we are trying to approximate the solution to (18.1).
To achieve this, as before, we introduce a discretization of the time interval [0, T]
via the following procedure. Let K ∈N, τ = T
K , and tk = τk for k = 0, . . . , K. As
before, we will produce a sequence {w k}K
k=0 such that u(tk) ≈w k.
The main motivation behind these methods can be explained by taking a second
look at Taylor’s method (18.6), where the slope approximation function is given by
GT M(t, s, v 1) = f (t, v 1) + s
2 [∂tf (t, v 1) + Duf (t, v 1)f (t, v 1)].
Clearly, this approximation comes from an approximation of u′(t +s) via a second-
order Taylor expansion. As we saw in Theorem 18.11, this method is convergent
with rate p = 2. While this is a perfectly acceptable method, it has one major
drawback. Namely, it requires knowledge not only of the slope function f but also
of its partial derivatives with respect to time, ∂tf , and u, Duf . In practice, these
functions may not be available, or they may be very diﬃcult to compute.
If we are to allow dealing with derivatives of the slope function, the next logical
step may be trying to devise a method, via a Taylor expansion, that is consistent to
order at least p = 3. The procedure is clear. We do a Taylor expansion of u(t + s)
about the point t and use (18.1) to arrive at
u(t + s) = u(t) + su′(t) + s2
2 u′′(t) + s3
6 u′′(t) + O(|s|4)
≈u(t) + s
n
f + s
2 [∂tf + Duf f ]
+ s2
6

∂2
t f + Du∂tf · f + Duf (∂tf + Duf f ) +
 ∂tDuf + D2
uf · f

f

,
where, for simplicity, we have suppressed the arguments of the slope function and its
derivatives. Hopefully, the reader appreciates the diﬃculties we have encountered.
Not only will such a procedure require evaluating many derivatives of the slope
function, but also the number of terms that is involved grows extremely large.
The idea behind RK methods is that, instead of diﬀerentiating the slope function,
we evaluate it at a special collection of points in [tk, tk+1] × ¯Ωcalled stages, so

19.1 Simple Two-Stage Methods
537
that, for instance, in the case of a two-stage RK method we have
κ1 = f (tk, w k),
κ2 = f (tk + cτ, w k + aτκ1);
then the approximation at tk+1 is given by a linear combination of the stages:
w k+1 = w k + τ (b1κ1 + b2κ2).
By properly choosing the coeﬃcients a, c, b1, and b2, a method that is consistent
to order at least p = 2 (like Taylor’s method) can be obtained.
19.1
Simple Two-Stage Methods
Before we give a general deﬁnition of RK methods, let us elaborate on the previous
idea for a simple, scalar, autonomous problem. We leave it to the reader to show
that the following ﬁts the general deﬁnition given later.
Theorem 19.1 (RK2). Let T > 0 be given. Consider the general two-stage explicit
RK method, deﬁned by
ξk = w k + aτf (w k),
w k+1 = w k + τ

b1f (w k) + b2f (ξk)

,
for approximating the solution to the scalar autonomous IVP
u′(t) = f (u(t)),
t ∈[0, T],
u(0) = u0.
Assume that f ∈F2(S) and, therefore, u ∈C3([0, T]). If the coeﬃcients satisfy
b1 +b2 = 1, b1, b2 ≥0, and ab2 = 1
2, then the method is consistent of order p = 2
and the method is convergent to second order.
Proof. Let us ﬁrst show consistency. By Taylor’s Theorem, for some ζ between
u(t −τ) and u(t −τ) + aτf (u(t −τ)),
f
 u(t −τ) + aτf (u(t −τ))

= f (u(t −τ)) + aτf (u(t −τ))f ′(u(t −τ))
+ 1
2
 aτf (u(t −τ))
2f ′′(ζ).
Upon setting b1 + b2 = 1 and ab2 = 1
2, the LTE satisﬁes
E[u](t, τ) = u(t) −u(t −τ)
τ
−

b1f (u(t −τ)) −b2f
 u(t −τ) + aτf (u(t −τ))

= u(t) −u(t −τ)
τ
−b1f (u(t −τ)) −b2f (u(t −τ))
−τab2f (u(t −τ))f ′(u(t −τ)) −τ2 a2b2
2
f 2(u(t −τ))f ′′(ζ)
= u(t) −u(t −τ)
τ
−f (u(t −τ))
−τ
2 f (u(t −τ)) f ′(u(t −τ)) −τ2 a2b2
2
f 2(u(t −τ))f ′′(ζ).

538
Runge–Kutta Methods
On the other hand, using Taylor’s Theorem, the exact solution must satisfy
u(t) = u(t −τ) + τu′(t −τ) + τ2
2 u′′(t −τ) + τ3
6 u′′′(σ)
= u(t −τ) + τf (u(t −τ)) + τ2
2 f ′(u(t −τ))f (u(t −τ)) + τ3
6 u′′′(σ)
for some σ ∈(t −τ, t). Comparing the expansions,
E[u](t, τ) = τ2
6 u′′′(σ) −τ2 a2b2
2
f 2(u(t −τ))f ′′(ζ)
provided that f ∈C2((−∞, ∞)), u ∈C3([0, T]), b1 + b2 = 1, and ab2 = 1
2. There
is some C > 0 such that
|E[u](t, τ)| ≤Cτ2
for any τ ∈(0, T] and t ∈[τ, T]. The proof for this is subtle and relies on the fact
that u is bounded over [0, T].
We will only show convergence in the case that a = 1/2, b1 = 0, and b2 = 1,
leaving the general case to the reader as an exercise; see Problem 19.1. With this
simpliﬁcation, the method reads
w k+1 = w k + τf

w k + τ
2 f (w k)

.
The exact solution satisﬁes
u(tk+1) = u(tk) + τf

u(tk) + τ
2 f (u(tk))

+ τEk+1[u].
Therefore,
ek+1 = ek + τf

u(tk) + τ
2 f (u(tk))

−τf

w k + τ
2 f (w k)

+ τEk+1[u].
Taking absolute values and using the triangle inequality and the Lipschitz continuity
of the slope function f , we have
|ek+1| ≤|ek| + τL
ek + τ
2 (f (u(tk)) −f (w k))
 + τ
Ek+1[u]

≤(1 + τL)|ek| + τ2L2
2
|ek| + Cτ3
=

1 + τL + τ2L2
2

|ek| + Cτ3.
Using the discrete Gr¨onwall inequality from Lemma 18.3,
|ek| ≤
Cτ2
L + τL2
2
"
1 + τL + τ2L2
2
k
−1
#
.
Now, since τL > 0,
1 + τL + τ2L2
2
< eτL;
therefore, for any m = 1, . . . , K,
(1 + τL)m < emτL ≤eKτL = eT L,

19.2 General Deﬁnition and Basic Properties
539
where we used that Kτ = T. It follows that, for all k = 0, . . . , K,
|ek| ≤C
L

eT L −1

τ2.
19.2
General Deﬁnition and Basic Properties
We now embark upon the study of general RK methods and their properties. We
begin with their deﬁnition.
Deﬁnition 19.2 (RK). Let r ∈N. A general r-stage Runge–Kutta method (RK
method)1 is a recursive algorithm for generating an approximation

w k	K
k=0 to the
solution of (18.1), via w 0 = u0 and, for k = 0, . . . , K −1,
ξi = w k + τ
r
X
j=1
ai,jf (tk + cjτ, ξj),
i = 1, . . . , r,
(19.1)
w k+1 = w k + τ
r
X
j=1
bjf (tk + cjτ, ξj).
(19.2)
Here, ai,j ∈R and bj, cj ∈[0, 1] for i, j = 1, . . . , r. An RK method is completely
determined by its weights A = [ai,j]r
i,j=1 ∈Rr×r, b = [bi]r
i=1 ∈Rr, and
c = [ci]r
i=1 ∈Rr, which are often expressed in tableau form
c
A
b⊺,
which is commonly referred to as the Butcher tableau2 of the method. The RK
method is called explicit (ERK) if and only if ai,j = 0 for all i ≤j, and is called
implicit (IRK) otherwise. The RK method is called diagonally implicit (DIRK) if
and only if ai,j = 0 for all i < j.
Remark 19.3 (equivalent deﬁnition). As we mentioned above, there is an alternate,
but equivalent, deﬁnition of the general r-stage RK method. Let us write out this
equivalent form. First, deﬁne
κj = f (tk + cjτ, ξj),
j = 1, . . . , r.
Then, from (19.2),
w k+1 = w k + τ
r
X
j=1
bjκj,
(19.3)
where, from (19.1),
κi = f

tk + ciτ, w k + τ
r
X
j=1
ai,jκj

,
i = 1, . . . , r.
(19.4)
1 Named in honor of the German mathematicians Carl David Tolm´e Runge (1856–1927) and
Martin Wilhelm Kutta (1867–1944).
2 Named in honor of the New Zealand mathematician John Charles Butcher (1933–).

540
Runge–Kutta Methods
The following theorem describes constraints on the weights of an RK method,
so that the method satisﬁes certain consistency requirements.
Theorem 19.4 (properties of weights). Assume that f ∈F1(S). Consider the
general r-stage RK method given by the weights A = [ai,j]r
i,j=1 ∈Rr×r, b =
[bi]r
i=1 ∈[0, 1]r, and c = [ci]r
i=1 ∈[0, 1]r. Let 1 = [1]r
i=1 ∈Rr.
1. For the method to be at least ﬁrst order, it is necessary that
b⊺1 = 1.
2. For the jth RK stage ξj to be at least a ﬁrst-order approximation of u(tk +cjτ),
it is necessary that
A1 = c.
(19.5)
3. Suppose that f ∈F2(S) and (19.5) holds. For the method to be at least second
order, it is necessary that
b⊺c = 1
2.
4. Suppose that f ∈F3(S) and (19.5) holds. For the method to be at least third
order, it is necessary that
b⊺Ac = 1
6.
Proof. We sketch the proof and leave the details to the reader; see Problem 19.2.
To prove the result, use the general r-step method to approximate the solution to
the linear scalar problem u′(t) = u(t), u(0) = 1, whose exact solution is u(t) = et.
At time t = τ, the solution may be expressed as
u(τ) = 1 + τ + τ2
2 + τ3
6 + τ4
24eη
for some η ∈(0, τ). For the RK stages, assume that the matrix I −τA is invertible
— this will always will be the case provided that τ is suﬃciently small — and
(I −τA)−1 = I + τA + τ2A2 + τ3A3 + · · · .
It is possible to show that the vector of stages satisﬁes
ξ = (I −τA)−11.
Solve explicitly for w 1 and compare the result with the expansion u(τ) above.
The following expressions for the ERK and IRK approximations, respectively, of
u′ = λu will be needed in subsequent chapters.
Theorem 19.5 (ampliﬁcation factor I). Applying an r-stage explicit RK method
to approximate the solution of the diﬀerential equation u′(t) = λu(t), u(0) = u0,
one obtains
w k+1 = g(λτ)w k,
g(z) =
r
X
j=0
βjzj,
w 0 = u0,

19.2 General Deﬁnition and Basic Properties
541
where βj ∈R, j = 0, . . . , r. If the method is consistent to exactly order r, then
g(z) =
r
X
j=0
zj
j! ,
i.e., βj = 1
j! for j = 0, . . . , r.
Proof. See Problem 19.3.
Theorem 19.6 (ampliﬁcation factor II). Applying an r-stage implicit RK method
to approximate the solution of the diﬀerential equation u′(t) = λu(t), u(0) = u0,
one obtains
w k+1 = g(λτ)w k,
g(z) = p1(z)
p2(z),
w 0 = u0,
where p1, p2 ∈Pr and p2 ̸≡0. In particular, g is the rational polynomial
g(z) = 1 + zb⊺(I −zA)−11 = det(I −zA + z1 b⊺)
det(I −zA)
,
where 1 = [1]r
i=1 ∈Rr.
Proof. See Problem 19.4.
Remark 19.7 (ampliﬁcation factor). The function g (a polynomial in the ERK
case and a rational function in the IRK case) that appears in Theorems 19.5 and
19.6 is called the linear ampliﬁcation factor or just the ampliﬁcation factor.
Remark 19.8 (LTE). The consistency error for any explicit RK method is deﬁned
in a straightforward way, since the RK stages can be computed explicitly in terms
of the approximations. Speciﬁcally,
τEEE[u](t, s) = u(t) −u(t −s) −s
r
X
i=1
bif (t −τ + ciτ, ξe,i),
where ξe,1 = u(t −τ) and, for i = 2, . . . , r,
ξe,i = u(t −τ) + τ
i−1
X
j=1
ai,jf (t −τ + cjτ, ξe,j).
Notice that, in the end, the ξe,i can be completely eliminated, which is a key insight.
For implicit RK methods, the situation is a bit more complicated.
We have already encountered the following, but in a slightly simpler form, in
Theorem 19.1.
Theorem 19.9 (two-stage RK method). Suppose that f ∈F2(S), so that u ∈
C3 [0, T]; Rd
is a classical solution to the IVP (18.1). Consider an explicit two-
stage RK method given by the tableau
0
0
0
c2
a2,1
0
b1
b2
.

542
Runge–Kutta Methods
The method is consistent to order p = 2 if and only if
b1 + b2 = 1,
a2,1 = c2,
b2c2 = 1
2.
Proof. For simplicity of notation, we will give a proof in the scalar case (d = 1).
We begin with a two-dimensional Taylor expansion of f
∈
C2(S). Set
q = [c2, a2,1f (t −τ, u(t −τ))]⊺. Then
f
 t −τ + c2τ, u(t −τ) + a2,1τf (t −τ, u(t −τ))

= f (t −τ, u(t −τ)) + τq1∂tf (t −τ, u(t −τ))
+ τq2∂uf (t −τ, u(t −τ)) + τ2
2 q⊺Hf (η, γ)q,
(19.6)
where Hf is the 2 × 2 Hessian matrix of second derivatives of f , η is some number
between t −τ and t −τ + c2τ, and γ is some number between u(t −τ) and
u(t −τ) + a2,1τf (t −τ, u(t −τ)). For t ∈[τ, T], it follows that, since the solution
u is twice continuously diﬀerentiable,
|q⊺Hf (η, γ)q| ≤C,
where C > 0 is independent of t.
Using the expansion (19.6) above, the local truncation error, which is deﬁned as
usual, may be expressed as
τE[u](t, τ) = u(t) −u(t −τ) −τb1f (t −τ, u(t −τ))
−τb2f
 t −τ + c2τ, u(t −τ) + τa2,1f (t −τ, u(t −τ))

= u(t) −u(t −τ) −τ(b1 + b2)f (t −τ, u(t −τ))
−τ2b2c2∂tf (t −τ, u(t −τ))
−τ2b2a2,1∂uf (t −τ, u(t −τ))f (t −τ, u(t −τ))
−τ3 b2
2 q⊺Hf (η, γ)q.
(19.7)
On the other hand, applying Taylor’s Theorem to the solution, we have, for some
β ∈(t −τ, t),
0 = u(t) −u(t −τ) −τu′(t −τ) −τ2
2 u′′(t −τ) −τ3
6 u′′′(β)
= u(t) −u(t −τ) −τf (t −τ, u(t −τ)) −τ2
2 ∂tf (t −τ, u(t −τ))
−τ2
2 ∂uf (t −τ, u(t −τ))f (t −τ, u(t −τ)) −τ3
6 u′′′(β).
(19.8)
( =⇒) Suppose that b1 + b2 = 1, a2,1 = c2, b2c2 = 1
2 in (19.7). Then, combining
(19.7) and (19.8),
τE[u](t, τ) = τ3
6 u′′′(β) −τ3 b2
2 q⊺Hf (η, γ)q
and the method is clearly consistent to order p = 2.

19.2 General Deﬁnition and Basic Properties
543
( ⇐= ) On the other hand, according to Theorem 19.4, for the method to be
consistent to exactly order p = 2, it must be that b1 +b2 = 1, a2,1 = c2, b2c2 = 1
2.
Otherwise, the method would be of order p = 1, or, perhaps, inconsistent.
The following three explicit two-stage RK methods are consistent to order p = 2
and conform to Theorem 19.9.
Example 19.1
Midpoint method:
0
0
0
1
2
1
2
0
0
1
.
Example 19.2
Heun’s method:3
0
0
0
1
1
0
1
2
1
2
.
Example 19.3
Ralston’s method:4
0
0
0
2
3
2
3
0
1
4
3
4
.
Theorem 19.10 (three-stage ERK methods). Suppose that f ∈F3(S), so that
u ∈C4  [0, T]; Rd
is a classical solution to the IVP (18.1). Consider an explicit
three-stage RK method given by the tableau
0
0
0
0
c2
a2,1
0
0
c3
a3,1
a3,2
0
b1
b2
b3
.
The method is consistent to order p = 3 if and only if
b1 + b2 + b3 = 1,
b2c2 + b3c3 = 1
2,
b2c2
2 + b3c2
3 = 1
3,
b3a3,2c2 = 1
6.
Proof. The proof can be found in [12]; see also [47].
3 Named in honor of the German mathematician Karl Heun (1859–1929).
4 Named in honor of the American mathematician Anthony Ralston (1930–).

544
Runge–Kutta Methods
Example 19.4
The following three-stage explicit RK method is consistent to
exactly order p = 3:
0
0
0
0
1
2
1
2
0
0
1
−1
2
0
1
6
2
3
1
6
.
This method is called the classical RK method.
Example 19.5
The following four-stage explicit RK method is consistent to
exactly order p = 4:
0
0
0
0
0
1
2
1
2
0
0
0
1
2
0
1
2
0
0
1
0
0
1
0
1
6
1
3
1
3
1
6
.
For a proof of the consistency, see [12]. The usual approach, which can be quite
tedious, is to use Taylor expansions to prove the result.
19.3
Collocation Methods
In this section we introduce collocation methods, which form a very general class
of numerical methods for diﬀerential equations. We show that these are related to
RK methods, in certain cases.
Deﬁnition 19.11 (RK collocation method). Let f ∈C
 S; Rd
. Suppose that the
so-called collocation points satisfy
0 ≤c1 < c2 < · · · < cr ≤1.
Let w k ∈Rd be given. Assume that pk ∈[Pr]d satisﬁes, if possible,
pk(tk) = w k,
p′
k(tk + cjτ) = f (tk + cjτ, pk(tk + cjτ))
(19.9)
for j = 1, . . . , r. Deﬁne w k+1 = pk(tk+1), for k = 0, . . . , K −1, with w 0 = u0.
This algorithm for producing the approximation sequence

w k	K
k=0 ⊂Rd is called
a Runge–Kutta collocation method.
Remark 19.12 (existence). The previous deﬁnition only makes sense if we can ﬁnd
a vector-valued polynomial
pk(t) =
r
X
j=0
ajtj,
aj ∈Rd,
j = 0, . . . , r

19.3 Collocation Methods
545
that satisﬁes (19.9). If so, we say that the implicit r-stage RK collocation method
is well deﬁned. In fact, it may be the case that such a polynomial will not exist or
will not be uniquely determined unless τ > 0 is suﬃciently small.
Theorem 19.13 (collocation). Let {cj}r
j=1 ⊂[0, 1] be a set of distinct collocation
points. Suppose that a unique polynomial pk ∈[Pr]d satisfying (19.9) exists. Deﬁne
ξi = pk(tk + ciτ),
i = 1, . . . , r,
Lj(t) =
rY
i=1
i̸=j
(t −ci)
(cj −ci),
j = 1, . . . , r,
ai,j =
Z ci
0
Lj(s)ds,
bj =
Z 1
0
Lj(s)ds,
i, j = 1, . . . , r.
Then the collocation method of Deﬁnition 19.11 is a standard implicit RK method,
as in Deﬁnition 19.2, with the weights A = [ai,j], b = [bj], and c = [cj], the last
weights being precisely the collocation points.
Proof. Suppose that pk ∈[Pr]d satisﬁes (19.9). Consider the unique Lagrange
interpolating polynomial of degree at most r −1, ρ ∈[Pr]d such that
ρ(tk + cjτ) = p′
k(tk + cjτ) = f (tk + cjτ, pk(tk + cjτ)) = νj,
j = 1, . . . , r.
Theorem 9.11 guarantees that
ρ(t) =
r
X
j=1
Lj
t −tk
τ

νj.
Observe that p′
k ∈[Pr−1]d and, in fact,
p′
k(tk + cjτ) = ρ(tk + cjτ),
j = 1, . . . , r.
Therefore, p′
k ≡ρ, since these polynomials (of degree at most r −1) agree at r
points. By (19.9),
p′
k(t) =
r
X
j=1
Lj
t −tk
τ

f (tk + cjτ, pk(tk + cjτ)).
Integrating the last expression and using the condition pk(tk) = w k, we observe
that
pk(t) = w k +
Z t
tk
r
X
j=1
Lj
s −tk
τ

f (tk + cjτ, pk(tk + cjτ))ds
= w k + τ
r
X
j=1
f (tk + cjτ, ξj)
Z
t−tk
τ
0
Lj(s)ds.
(19.10)
Setting t = tk + ciτ in (19.10), we have
ξi = w k + τ
r
X
j=1
ai,jf (tk + cjτ, ξj).

546
Runge–Kutta Methods
Setting t = tk+1 in (19.10), we ﬁnd
w k+1 = w k + τ
r
X
j=1
bjf (tk + cjτ, ξj),
and the proof is ﬁnished.
For collocation methods, everything is determined by picking the collocation
points. These are usually chosen as the roots of certain orthogonal polynomials.
Theorem 19.14 (collocation order). Suppose that {cj}r
j=1 ⊂[0, 1] is the set of r
distinct collocation points that determine the RK collocation method of Deﬁnition
19.11. Deﬁne
q(t) =
rY
n=1
(t −cn) ∈Pr.
If, for some m ∈{1, . . . , r},
Z 1
0
q(s)p(s)ds = 0,
∀p ∈Pm−1,
but there is some ˜p ∈Pm such that
Z 1
0
q(s)˜p(s)ds ̸= 0,
then the RK collocation method is consistent to exactly order p = r + m.
Proof. Consider the simple quadrature rule,
Q(0,1)
1
[f ] =
r
X
j=1
bjf (cj),
with quadrature nodes {cj}r
j=1 and quadrature weights {bj}r
j=1. Suppose that the
quadrature weights bj are chosen to satisfy the exactness condition (14.7); namely,
bj =
Z 1
0
Lj(s)ds,
Lj(s) =
rY
k=1
k̸=j
s −ck
cj −ck
,
j = 1, . . . , r.
Appealing to Corollary 14.47, the quadrature rule is consistent of order exactly
r + m −1. In other words,
Z 1
0
ψ(s)ds =
r
X
j=1
bjψ(cj),
∀ψ ∈Pr+m−1.
By the Alekseev–Gr¨obner Lemma, stated in Theorem 17.16,
pk(tk+1) −u(tk+1) =
Z tk+1
tk
DvU(s, pk(s), tk+1)g(s, pk(s))ds,
where pk is the vector-valued polynomial in the deﬁnition of the collocation method,
assuming that pk(tk) = u(tk) and that the deviation, g, in the Alekseev–Gr¨obner
Lemma satisﬁes

19.3 Collocation Methods
547
j
˜Pj(t)
0
1
1
2t −1
2
6t2 −6t + 1
3
20t3 −30t2 + 12t −1
4
70t4 −140t3 + 90t2 −20t + 1
5
252t5 −630t4 + 560t3 −210t2 + 30t −1
Table 19.1 The ﬁrst six transformed Legendre polynomials.
g(s, pk(s)) = p′
k(s) −f (s, pk(s)) = ˜g(s).
Observe that the collocation rule requires that
p′
k(tk + cjτ) = f (tk + cjτ, pk(tk + cjτ)),
j = 1, . . . , r,
which implies that ˜g vanishes at the collocation points:
˜g(tk + cjτ) = 0,
j = 1, . . . , r.
Applying the quadrature rule,
pk(tk+1) −u(tk+1) =
Z tk+1
tk
DvU(s, pk(s), tk+1)g(s, pk(s))ds
=
r
X
j=1
bjDvU(tk + τcj, pk(tk + τcj), tk+1)˜g(tk + τcj) + EQ
= EQ,
where EQ denotes the quadrature error. If we assume that
ˆg(·) = DvU(·, pk(·), tk+1)˜g(·) ∈Cr+m([tk, tk+1]; Rd),
then, by Theorem 14.18,
∥EQ∥2 ≤Cτr+m+1 ˆg(r+m)
L∞(tk,tk+1;Rd)
for some constant C > 0. Hence,
w k+1 −u(tk+1)

2 = ∥pk(tk+1) −u(tk+1)∥2 ≤˜Cτr+m+1,
where we assume that w k = u(tk). We leave it to the reader as an exercise to
prove that the local truncation error must be of order r +m; see Problem 19.6.
Deﬁnition 19.15 (transformed Legendre polynomials). By
 ˜Pj
	
j∈N0 we denote the
set of transformed Legendre polynomials,5 which have the property that
Z 1
0
˜Pi(s) ˜Pj(s)ds =
1
2j + 1δi,j.
The ﬁrst few transformed Legendre polynomials are given in Table 19.1.
5 Named in honor of the French mathematician Adrien-Marie Legendre (1752–1833).

548
Runge–Kutta Methods
Corollary 19.16 (Gauss–Legendre–RK). Let the collocation points c1, . . . , cr be
precisely the zeros of the transformed Legendre polynomial ˜Pr ∈Pr. According
to Theorem 11.10, these lie in the open interval (0, 1). Then the corresponding
collocation method of Deﬁnition 19.11 is consistent to order exactly p = 2r.
Proof. In this case, q ≡Cr ˜Pr, where 0 ̸= Cr ∈R. Since the ˜Pi form an orthogonal
basis for Pr, for any j ∈{0, 1, 2, . . . , r}, we can express
tj =
jX
m=0
βj,m ˜Pm(t)
for some constants βj,1, . . . , βj,j. Therefore,
Z 1
0
q(s)sjds = Cr
jX
m=0
βj,m
Z 1
0
˜Pr(s) ˜Pm(s)ds = 0,
provided that j ≤r −1. By Theorem 19.14, the method is exactly of order
p = r + r = 2r.
Deﬁnition 19.17 (Gauss–Legendre–RK method). The implicit r-stage RK meth-
ods constructed as collocation methods whose collocation points are the zeros
of the transformed Legendre polynomial ˜Pr are called Gauss–Legendre–Runge–
Kutta methods.6
The following Gauss–Legendre–RK methods are collocation methods con-
structed using Corollary 19.16; see Table 19.1.
Example 19.6
The midpoint rule: Suppose that r
= 1. The transformed
Legendre polynomial of order one is
˜P1(t) = 2t −1 =⇒c1 = 1
2.
The corresponding Gauss–Legendre IRK method is given by
1
2
1
2
1
and is of order 2r = 2. For a scalar autonomous system, u′ = f (u), the method
can be expressed as
w k+1 = w k + τf

w k + τ
2 κ1

,
κ1 = f

w k + τ
2 κ1

.
(19.11)
It is a simple exercise to show that this is equivalent to the midpoint rule,
w k+1 = w k + τf
w k+1 + w k
2

.
(19.12)
6 Named in honor of the German mathematician and physicist Johann Carl Friedrich Gauss
(1777–1855) and the French mathematician Adrien-Marie Legendre (1752–1833).

19.3 Collocation Methods
549
But let us write this another way. Deﬁne
˜w k+ 1
2 = w k+1 + w k
2
.
Then we can express the midpoint rule as
˜w k+ 1
2 = w k + τ
2 f

˜w k+ 1
2

,
w k+1 = 2 ˜w k+ 1
2 −w k.
(19.13)
Still another, equivalent, way of writing this method is as follows:
˜w k+ 1
2 = w k + τ
2 f

˜w k+ 1
2

,
w k+1 = ˜w k+ 1
2 + τ
2 f

˜w k+ 1
2

.
(19.14)
Observe that method (19.13) shows that the midpoint rule is essentially a backward
(implicit) Euler method with half the time step size followed by an extrapolation.
Method (19.14) expresses the midpoint rule as a half-step-size backward Euler
method followed by a half-step-size forward (explicit) Euler method.
In either case, the simple modiﬁcation of a backward Euler method, which is only
ﬁrst-order accurate, leads to a second-order accurate method; for more insight on
this, see [11].
Example 19.7
Suppose that r = 2. The transformed Legendre polynomial of
order two is
˜P2(t) = 6t2 −6t + 1 =⇒c1 = 1
2 −
√
3
6 , c2 = 1
2 +
√
3
6 .
The Gauss–Legendre IRK method is given by
1
2 −
√
3
6
1
4
1
4 −
√
3
6
1
2 +
√
3
6
1
4 +
√
3
6
1
4
1
2
1
2
and is of order 2r = 4.
Example 19.8
Suppose that r = 3. The transformed Legendre polynomial of
order three is
˜P3(t) = 20t3 −30t2 + 12t −1 =⇒c1 = 1
2 −
√
15
10 , c2 = 1
2, c3 = 1
2 +
√
15
10 .
The Gauss–Legendre IRK method is given by
1
2 −
√
15
10
5
36
2
9 −
√
15
15
5
36 −
√
15
30
1
2
5
36 +
√
15
24
2
9
5
36 −
√
15
24
1
2 +
√
15
10
5
36 +
√
15
30
2
9 +
√
15
15
5
36
5
18
4
9
5
18
and is of order 2r = 6.

550
Runge–Kutta Methods
Example 19.9
Not all IRK methods are of collocation type. Consider, for
example, the methods given by the tables
0
1
4
−1
4
2
3
1
4
5
12
1
4
3
4
1
3
5
12
−1
12
1
3
4
1
4
3
4
1
4
.
For both methods, the necessary conditions of Theorem 19.4 are satisﬁed. The
method on the left, which is consistent to exactly order p = 3, is not of collocation
type. (How do we know this?) The method on the right is of collocation type. One
can check that the collocation points c1 = 1
3 and c2 = 1 completely determine the
other weights. The method on the right is consistent to exactly order p = 3. To
see this, observe that
Z 1
0

s −1
3

(s −1)sjds = 0
only for j = 0. Thus, invoking Theorem 19.14, we have m = 1 and p = r + m = 3.
19.4
Dissipative Methods
In this section, we demonstrate how some IRK methods are particularly well suited
to approximate dissipative equations, as deﬁned in Section 17.4. We follow the
notation introduced there and consider IVPs posed on Cd, with d ∈N. The reader
will easily verify that all the numerical methods, and theory for them, we have
constructed so far extend to this case without diﬃculty.
One may wish to have a numerical method that preserves the dissipation property
presented in Theorem 17.18. To achieve this, we begin with a deﬁnition.
Deﬁnition 19.18 (algebraic stability). Assume that f : [0, T] × Cd →Cd is
monotone with respect to (·, ·). Let u0, v 0 ∈Cd. Assume that f is such that
there are unique classical solutions on [0, T] to the problems
u′(t) = f (t, u(t)),
u(0) = u0,
v ′(t) = f (t, v(t)),
v(0) = v 0.
Let

w k	K
k=0 and

zk	K
k=0 be approximations to u and v, respectively, obtained by
the same numerical method; for example, some RK method. Notice that we must
have w 0 = u0 and z0 = v 0. We say that the method is dissipative or algebraically
stable if and only if, for any K and for all starting values u0, v 0 ∈Cd,
w k −zk ≤
w k−1 −zk−1 ≤∥u0 −v 0∥
for all k = 1, . . . , K.
In the same way that an RK method can be encoded in a Butcher tableau, its
properties can be encoded in a particular matrix.

19.4 Dissipative Methods
551
Deﬁnition 19.19 (M-matrix). Suppose that the r-stage RK method is deﬁned by
the weights A = [ai,j]r
i,j=1, b = [bi]r
i=1, and c = [ci]r
i=1. The M-matrix of the
method is the matrix M = [mi,j]r
i,j=1 ∈Rr×r deﬁned via
mi,j = biai,j + bjaj,i −bibj,
i, j = 1, . . . , r.
The importance of the M-matrix of an RK method is in the dissipativity condition
given in the following result.
Theorem 19.20 (dissipativity condition). Suppose that f : [0, T] × Cd →Cd is
monotone with respect to (·, ·). Let u0, v 0 ∈Cd. Assume that f is such that there
are unique classical solutions on [0, T] to the problems
u′(t) = f (t, u(t)),
u(0) = u0,
v ′(t) = f (t, v(t)),
v(0) = v 0.
Let the r-stage RK method be deﬁned by the weights A = [ai,j]r
i,j=1, b = [bi]r
i=1,
and c = [ci]r
i=1. If its M-matrix is positive semi-deﬁnite and bj ≥0, j = 1, . . . , r,
then the RK method is dissipative.
Proof. Deﬁne, for i, j = 1, . . . , r,
ρj = f (tk + cjτ, ξj),
ξi = w k + τ
r
X
j=1
ai,jρj,
w k+1 = w k + τ
r
X
j=1
bjρj,
and
σj = f (tk + cjτ, ζj),
ζi = zk + τ
r
X
j=1
ai,jσj,
zk+1 = zk + τ
r
X
j=1
bjσj.
Then observe that
w k+1 −zk+12 =
w k −zk2 + 2τℜ



w k −zk,
r
X
j=1
bjd j



+ τ2

r
X
j=1
bjd j

2
,
where, for j = 1, . . . , r,
d j = ρj −σj.
Thus, we will have proven the result if we can show that
2τℜ



w k −zk,
r
X
j=1
bjd j



+ τ2

r
X
j=1
bjd j

2
≤0.
Since, for j = 1, . . . , r,
w k = ξj −τ
r
X
i=1
aj,iρi,
zk = ζj −τ
r
X
i=1
aj,iσi,

552
Runge–Kutta Methods
we have
ℜ



w k −zk,
r
X
j=1
bjd j



=
r
X
j=1
bjℜ
" 
ξj −ζj −τ
r
X
i=1
aj,id i, d j
!#
=
r
X
j=1
bjℜ
 ξj −ζj, d j

−τ
r
X
j=1
r
X
i=1
bjaj,iℜ[(d i, d j)].
Since f is monotone, for each j = 1, . . . , r,
ℜ
 ξj −ζj, d j

≤0.
Since the weights bj are all nonnegative,
r
X
j=1
bjℜ
 ξj −ζj, d j

≤0.
It then follows that
ℜ



w k −zk,
r
X
j=1
bjd j



≤−τ
r
X
j=1
r
X
i=1
bjaj,iℜ[(d i, d j)],
or, equivalently, after swapping summation indices,
ℜ



w k −zk,
r
X
j=1
bjd j



≤−τ
r
X
i=1
r
X
j=1
biai,jℜ[(d j, d i)].
Thus,
2τℜ



w k −zk,
r
X
j=1
bjd j



+ τ2

r
X
j=1
bjd j

2
≤−τ2
r
X
j=1
r
X
i=1
bjaj,iℜ[(d i, d j)] −τ2
r
X
i=1
r
X
j=1
biai,jℜ[(d j, d i)] + τ2

r
X
j=1
bjd j

2
= −τ2
r
X
i=1
r
X
j=1
mi,jℜ[(d i, d j)].
Since M is symmetric positive semi-deﬁnite, there is an orthogonal matrix Q and
a diagonal matrix D = diag[λ1, . . . , λr], with nonnegative diagonal entries, λj ≥0,
such that
M = QDQ⊺.
In terms of coordinates,
mi,j =
r
X
k=1
λkqi,kqj,k.

19.4 Dissipative Methods
553
Thus,
2τℜ



w k −zk,
r
X
j=1
bjd j



+ τ2

r
X
j=1
bjd j

2
≤−τ2
r
X
i=1
r
X
j=1
r
X
k=1
λkqi,kqj,kℜ[(d i, d j)]
= −τ2
r
X
k=1
λkℜ




r
X
i=1
qi,kd i,
r
X
j=1
qj,kd j




= −τ2
r
X
k=1
λk

r
X
j=1
wj,kd j

2
≤0.
The result is proved:
w k+1 −zk+12 ≤
w k −zk2 .
We will conclude this section by proving that the Gauss–Legendre IRK methods
are dissipative. To do this, we need some deﬁnitions.
Deﬁnition 19.21 (type). Let q ∈N. An r-stage RK method deﬁned by the weights
A = [ai,j]r
i,j=1, b = [bi]r
i=1, and c = [ci]r
i=1 is said to be of type B(q) if and only if
r
X
i=1
bick−1
i
= 1
k ,
k = 1, . . . , q.
The method is said to be of type C(q) if and only if
r
X
j=1
ai,jck−1
j
= ck
i
k ,
i = 1, . . . , r,
k = 1, . . . , q.
Theorem 19.22 (dissipativity criterion). Consider an r-stage RK method deﬁned
by the weights A = [ai,j]r
i,j=1, b = [bi]r
i=1, and c = [ci]r
i=1. Assume that the entries
of c = [ci]r
i=1 are distinct. If the method is of type B(2r) and of type C(r), then
the M-matrix for this method is the zero matrix.
Proof. Let M = [mi,j] ∈Rr×r denote the M-matrix for the method. Deﬁne the
matrix N = [ni,j] ∈Rr×r via
nk,m =
r
X
i=1
r
X
j=1
ck−1
i
mi,jcm−1
j
=
r
X
i=1
r
X
j=1
ck−1
i
(biai,j + bjaj,i −bibj) cm−1
j
= 0,
where k, m = 1, . . . , r and we used the fact that the method is of type B(2r) and
type C(r). The details are left to the reader as an exercise; see Problem 19.9. In
conclusion, N = O, the zero matrix. But observe that
N = V⊺MV,

554
Runge–Kutta Methods
where V = [cj−1
i
] is a variant of the Vandermonde matrix, which is nonsingular
provided that the entries of c = [ci]r
i=1 are distinct (Theorem 9.4). It follows that
M = O.
Theorem 19.23 (dissipativity). All Gauss–Legendre RK methods are dissipative.
Proof. The idea is to show that all Gauss–Legendre RK methods are of type B(2r)
and type C(r), thus implying that their M-matrices are r ×r zero matrices. Finally,
one needs to show that the weights bj are all nonnegative. The result then follows
from Theorem 19.20. The details are left to the reader as an exercise; see Problem
19.10.
Problems
19.1
Let T > 0 be given. Consider the general two-stage explicit RK method,
deﬁned by
ξk = w k + aτf (w k),
w k+1 = w k + τ(b1f (w k) + b2f (ξk)),
for approximating the solutions to the autonomous IVP
u′(t) = f (u(t)),
t ∈[0, T],
u(0) = u0.
Assume that f ∈F2(S) and the coeﬃcients satisfy b1 + b2 = 1 and ab2 = 1
2.
Prove that the method is convergent to second order.
19.2
Provide all the details for the proof of Theorem 19.4.
19.3
Prove Theorem 19.5.
19.4
Prove Theorem 19.6.
19.5
Consider the implicit RK methods given by the tableaux
0
1
4
−1
4
2
3
1
4
5
12
1
4
3
4
1
3
5
12
−1
12
1
3
4
1
4
3
4
1
4
.
a)
Show that the necessary conditions of Theorem 19.4 are all satisﬁed.
b)
Show that one of the methods is a collocation method and that the other is
not. For the one that is a collocation method, ﬁnd its order of consistency.
19.6
Complete the proof of Theorem 19.14.
19.7
Complete the proof of Theorem 17.18.
19.8
Show that no explicit RK method can be dissipative.
19.9
Complete the proof of Theorem 19.22.
19.10
Complete the proof of Theorem 19.23.

20
Linear Multi-step Methods
The fundamental formula that deﬁnes mild solutions
u(t2) = u(t1) +
Z t2
t1
f (s, u(s))ds
was used in the previous chapter with t1 = tk and t2 = tk+1. To approximate
the integral, we used information about the slope function in the time interval
[tk, tk+1] only. However, we could use more information to build an approximation
of the integral. For instance, for some q ∈N0, we set Xq = {tk−q, . . . , tk, tk+1}
and replace the slope function by its interpolant on Xq
IXq[f (·, u(·))](t) ≈f (t, u(t)).
Then
u(t2) ≈u(t1) +
Z t2
t1
IXq[f (·, u(·))](s)ds.
This is the basis of Adams-type methods and other multi-step methods, which we
examine in this chapter.
Before we begin, we recall that we are trying to approximate the solution to
(18.1) by choosing K ∈N and letting τ = T/K and tk = kτ. We will produce
{w k}K
k=0 ⊂Rd such that w k ≈u(tk).
20.1
Consistency of Linear Multi-step Methods
In Chapters 18 and 19, we discussed single-step methods. With those, all that was
needed to obtain the approximation at time level k+1 was the approximate solution
at time level k. In this chapter, we will introduce multi-step methods, which use
approximations at additional past time levels, k −1, k −2, etc.
Deﬁnition 20.1 (linear multi-step method). Let K, q ∈N with q < K. The
ﬁnite sequence

w k	K
k=0 ⊂Rd is called a linear q-step approximation (or just
a linear multi-step approximation) to u, solution of (18.1), with starting values
w 0, w 1, . . . , w q−1 if and only if, for k = 0, . . . , K −q,
q
X
j=0
ajw k+j = τ
q
X
j=0
bjf (tk+j, w k+j),
(20.1)

556
Linear Multi-step Methods
where {aj}q
j=0, {bj}q
j=0 ⊆R. The multi-step approximation is called explicit if
bq = 0; otherwise, it is called implicit. As before, the global error of the multi-step
approximation is the ﬁnite sequence

ek	K
k=0 ⊆Rd deﬁned via
ek = u(tk) −w k.
Remark 20.2 (convention). Notice that, for the multi-step method (20.1) to
make sense, we must have aq ̸= 0. In addition, observe that the coeﬃcients
{aj}q
j=0, {bj}q
j=0 are deﬁned up to multiplication by a common constant. Because
of these two considerations, here and in what follows we will assume that
aq = 1.
Deﬁnition 20.3 (LTE and consistency). Let u ∈C1([0, T]; Ω) be a classical
solution on [0, T] to (18.1). Let the sequence

w k	K
k=0 be obtained with the q-step
method (20.1) with starting values w 0, w 1, . . . , w q−1. The local truncation error
(LTE) or consistency error of the multi-step approximation is deﬁned as
EEE[u](t, τ)
= 1
τ
q
X
j=0
[aju(t + (j −q)τ) −τbjf (t + (j −q)τ, u(t + (j −q)τ))]
(20.2)
for any t ∈[tq, T]. We make frequent use of the notation EEEk[u] = EEE[u](tk, τ) for
k = q, . . . , K. We say that the linear multi-step approximation is consistent to at
least order p ∈N if and only if, when
u ∈Cp+1 [0, T]; Rd
,
there is a constant τ0 ∈(0, T] and a constant C > 0 such that, for all τ ∈(0, τ0]
and all t ∈[tq, T],
∥EEE[u](t, τ)∥2 ≤Cτp.
(20.3)
We say that the linear q-step approximation is consistent to exactly order p if
and only if p is the largest positive integer for which (20.3) holds.
We say that the multi-step approximation converges globally, with at least order
p ∈N, if and only if, when
u ∈Cp+1 [0, T]; Rd
,
there is some τ1 ∈(0, T] and a constant C > 0 such that
ek
2 ≤Cτp
for all τ ∈(0, τ1] and any k = 0, . . . , K.
The following result can be used to determine the order of a linear multi-step
method.

20.1 Consistency of Linear Multi-step Methods
557
Theorem 20.4 (method of C’s). Let f ∈Fp(S) and u ∈Cp+1([0, T]; Rd) be a
classical solution on [0, T] to (18.1). Suppose that u is approximated by the linear
q-step method (20.1). Deﬁne
Cm =













q
X
j=0
aj,
m = 0,
q
X
j=0
 jm
m!aj −
jm−1
(m −1)!bj

,
m ∈{1, 2, 3, . . .} ,
(20.4)
with the convention that 00 = 1. The method is consistent to exactly order p if
and only if C0 = 0 = C1 = · · · = Cp, but Cp+1 ̸= 0.
Proof. For simplicity of notation, let us suppose that d = 1. Consider t ∈[tq, T]
and we extend, for any k ∈Z, the deﬁnition tk = kτ . Using Taylor’s Theorem,
with the expansion point t −tq, one ﬁnds, for each j = 0, 1, . . . , q,
u(t + tj−q) =
p
X
m=0
u(m)(t −tq)(jτ)m
m!
+ u(p+1)(ζj) (jτ)p+1
(p + 1)!
and
u′(t + tj−q) =
p−1
X
m=0
u(m+1)(t −tq)(jτ)m
m!
+ u(p+1)(ξj)(jτ)p
p!
=
p
X
m=1
u(m)(t −tq) (jτ)m−1
(m −1)! + u(p+1)(ξj)(jτ)p
p! .
Observe that the j = 0 case holds if we agree that 00 = 1. Therefore,
τE[u](t, τ) =
q
X
j=0
aju(t + tj−q) −τ
q
X
j=0
bju′(t + tj−q)
=
q
X
j=0
aj
p
X
m=0
u(m)(t −tq)(jτ)m
m!
−τ
q
X
j=0
bj
p
X
m=1
u(m)(t −tq) (jτ)m−1
(m −1)!
+ τp+1
q
X
j=0

aju(p+1)(ζj)
jp+1
(p + 1)! −bju(p+1)(ξj) jp
p!

.

558
Linear Multi-step Methods
Interchanging the summations, so that we sum by powers of τ, we have
τE[u](t, τ) = u(t −tq)
q
X
j=0
aj +
p
X
m=1
τmu(m)(t −tq)
q
X
j=0

aj
jm
m! −bj
jm−1
(m −1)!

+ τp+1
q
X
j=0

aju(p+1)(ζj)
jp+1
(p + 1)! −bju(p+1)(ξj) jp
p!

= C0u(t −tq) +
p
X
m=1
Cmτmu(m)(t −tq)
+ τp+1
q
X
j=0

aju(p+1)(ζj)
jp+1
(p + 1)! −bju(p+1)(ξj) jp
p!

(20.5)
for some constants ζj, ξj ∈[t −tq, t], j = 0, . . . , q.
( =⇒) Suppose that the method is of exactly order p. Then, from (20.5), we
must have C0 = C1 = · · · = Cp = 0. If the true solution has higher regularity, say
u ∈Cp+2([0, T]), then we can extend the Taylor expansion by one term to obtain
τE[u](t, τ) = Cp+1τp+1u(p+1)(t −tq)
+ τp+2
q
X
j=0

aju(p+2)(˜ζj)
jp+2
(p + 2)! −bju(p+2)(˜ξj)
jp+1
(p + 1)!

.
Since the method does not exceed order p, it must be true that Cp+1 ̸= 0
generically, by the deﬁnition of the local truncation error.
( ⇐= ) Suppose that C0 = C1 = · · · = Cp = 0, but Cp+1 ̸= 0. Then
E[u](t, τ) = τp
q
X
j=0

aju(p+1)(ζj)
jp+1
(p + 1)! −bju(p+1)(ξj) jp
p!

and the method is consistent to at least order p. Since Cp+1 ̸= 0, the order
of accuracy cannot exceed p, even if the true solution has higher regularity, say
u ∈Cp+2([0, T]).
The consistency criterion given in Theorem 20.4 is usually known as the method
of C’s. The algorithmic description of the computation of each one of the involved
C’s is given in Listing 20.1.
Deﬁnition 20.5 (characteristic polynomials). For the linear q-step method (20.1),
we deﬁne the ﬁrst and second characteristic polynomials, respectively, as
ψ(z) =
q
X
j=0
ajzj ∈Pq,
χ(z) =
q
X
j=0
bjzj ∈Pq.
Corollary 20.6 (ﬁrst-order consistency). Let f ∈F1(S). Assume that the function
u ∈C2([0, T]; Rd) is a classical solution to the initial value problem (IVP) (18.1).

20.1 Consistency of Linear Multi-step Methods
559
Suppose that u is approximated by the linear q-step method (20.1). The method
is consistent to at least ﬁrst order if and only if
ψ(1) = 0,
ψ′(1) −χ(1) = 0.
Proof. This follows from Theorem 20.4; see Problem 20.1.
The following result provides another way to verify the consistency of a linear
multi-step method.
Theorem 20.7 (the log-method). Let f ∈Fp(S). Assume that the function
u ∈Cp+1([0, T]; Rd) is a classical solution to the IVP (18.1). Suppose that u
is approximated by the linear q-step method (20.1). The method is consistent to
exactly order p if and only if the function
φ(µ) = ψ(µ)
ln(µ) −χ(µ),
which is complex analytic in a neighborhood of µ = 1, has the property that µ = 1
is a p-fold zero or, equivalently, that
˜φ(µ) = ψ(µ) −χ(µ) ln(µ),
which is also complex analytic in a neighborhood of µ = 1, has the property that
µ = 1 is a p + 1-fold zero.
Proof. Once again, for simplicity of notation, we consider d = 1 and assume that
u is real analytic. From Theorem 20.4, we observe that the method is consistent to
order p if and only if C0 = C1 = · · · = Cp = 0, but Cp+1 ̸= 0. Using the techniques
developed in the proof of Theorem 20.4, we can expand to all orders to obtain
τE[u](t, τ) =
∞
X
m=p+1
Cmu(m)(t −tq)τm.
In particular, setting u(t) = exp(t), which is certainly real analytic, we ﬁnd
τE[exp(·)](t, τ) = exp(t −tq)
∞
X
m=p+1
Cmτm.
On the other hand, by the deﬁnition of the local truncation error, we have
τE[exp(·)](t, τ) = exp(t −tq)
q
X
j=0
{aj exp(tj) −τbj exp(tj)}
= exp(t −tq)
q
X
j=0
{aj exp(jτ) −τbj exp(jτ)}
= exp(t −tq) [ψ(exp(τ)) −τχ(exp(τ))].
Equating terms, we have
˜φ(exp(τ)) = τφ(exp(τ)) = ψ(exp(τ)) −τχ(exp(τ)) =
∞
X
m=p+1
Cmτm.

560
Linear Multi-step Methods
Thus, τ = 0 is a p-fold zero of the function φ(exp(τ)). Here, in fact, we can
assume that τ is any complex number. Setting µ = exp(τ) and using the fact that,
in a neighborhood of µ = 1,
ln(µ) =
∞
X
m=1
(−1)m+1
m
(µ −1)m
= (µ −1) −1
2(µ −1)2 + 1
3(µ −1)3 −1
4(µ −1)4 + 1
5(µ −1)5 + · · · ,
it follows that this is equivalent to the condition that φ(µ) has a p-fold zero at
µ = 1, which is equivalent to the condition that ˜φ(µ) has a p + 1-fold zero at
µ = 1.
The consistency criterion given in Theorem 20.7 is usually known as the log-
method.
Example 20.1
Consider the linear two-step method
w k+2 −w k = τ
3

f (tk+2, w k+2) + 4f (tk+1, w k+1) + f (tk, w k)

.
This method is implicit and consistent to exactly order p = 4. To prove this,
assuming that the slope function is suﬃciently regular, f ∈F4(S), we need only
to show that C0 = C1 = · · · = C4 = 0, but C5 ̸= 0, where these constants are
deﬁned in (20.4). Clearly, C0 = 0. Now
C1 =
2
X
j=0
(jaj −bj) = 2 · 1 + 1 · 0 + 0 · (−1) −
1
3 + 4
3 + 1
3

= 2 −2 = 0,
C2 =
2
X
j=0
 j2
2!aj −jbj

= 22
2 · 1 −

2 · 1
3 + 1 · 4
3

= 2 −2 = 0,
C3 =
2
X
j=0
j3
6 aj −j2
2 bj

= 23
6 · 1 −
22
2 · 1
3 + 12
2 · 4
3

= 8
6 −8
6 = 0,
C4 =
2
X
j=0
 j4
24aj −j3
6 bj

= 24
24 · 1 −
23
6 · 1
3 + 13
6 · 4
3

= 2
3 −2
3 = 0.
But
C5 =
2
X
j=0
 j5
120aj −j4
24bj

= 25
120 · 1 −
 24
24 · 1
3 + 14
24 · 4
3

= 4
15 −5
18 ̸= 0.
Example 20.2
In this example, we use Theorem 20.7 to show that the two-step
implicit method
w k+2 −w k+1 = τ
 5
12f (tk+2, w k+2) + 8
12f (tk+1, w k+1) −1
12f (tk, w k)


20.1 Consistency of Linear Multi-step Methods
561
is consistent to exactly order p = 3. To do so, it is convenient to make the change
of variables z = µ −1. Then
ψ(µ) = µ2 −µ
= (z + 1)2 −(z + 1)
= z2 + z,
χ(µ) = 5
12µ2 + 8
12µ −1
12
= 5
12(z + 1)2 + 8
12(z + 1) −1
12
= 5
12z2 + 3
2z + 1,
and
ln(µ) = (µ −1) −1
2(µ −1)2 + 1
3(µ −1)3 −1
4(µ −1)4 + 1
5(µ −1)5 + · · ·
= z −1
2z2 + 1
3z3 −1
4z4 + 1
5z5 + · · · .
We need to consider the diﬀerence
ψ(µ)
ln(µ) −χ(µ).
To do so, we ﬁrst ﬁnd an expansion, in terms of z, for ψ(µ)
ln(µ):
ψ(µ)
ln(µ) = c0 + c1z + c2z2 + c3z3 + c4z4 + · · · .
Then
 c0 + c1z + c2z2 + c3z3 + · · ·
 
z −1
2z2 + 1
3z3 −1
4z4 + · · ·

= z + z2,
which implies that
c0 = 1,
c1 −1
2c0 = 1
=⇒c1 = 3
2,
c2 −1
2c1 + 1
3c0 = 0
=⇒c2 = 5
12,
c3 −1
2c2 + 1
3c1 −1
4c0 = 0
=⇒c3 = −1
24.
Finally,
ψ(µ)
ln(µ) −χ(µ) = 1 + 3
2z + 5
12z2 −1
24z3 + c4z4 + · · · −

1 + 3
2z + 5
12z2

= −1
24z3 + c4z4 + · · · ,
which proves that the method is of exactly third order.

562
Linear Multi-step Methods
20.2
Adams–Bashforth and Adams–Moulton Methods
In this section, we derive some examples of the so-called Adams–Moulton and
Adams–Bashforth multi-step methods. We make extensive use of the Lagrange
interpolation techniques of Chapter 9. Let q < K. Assume that we have computed
k +q−1 < K approximations {w j}k+q−1
j=0
. Now, from the deﬁnition of mild solution
(17.2), we have that
u(tk+q) −u(tq+k−1) =
Z tk+q
tk+q−1
f (s, u(s))ds.
We could approximate this integral using the values w k and w k+1, and this is the
idea behind the single-step methods studied in Chapter 18. However, in doing so,
we are not making use of all the information that we had computed before, i.e.,
{w j}k+q−1
j=0
. The idea of the Adams methods is to use a subset of {f (tj, w j)}k+q−1
j=0
to construct an interpolating polynomial and use this polynomial to approximate
the integral in the previous identity. Two important classes of methods here are:
1. Adams–Bashforth methods,1 which use {f (tj, w j)}k+q−1
j=k
and thus are explicit
methods.
2. Adams–Moulton methods,2 which use {f (tj, w j)}k+q
j=k and thus are implicit.
The general strategy is clear, so we conﬁne ourselves to presenting a few examples.
Example 20.3
The Adams–Bashforth four-step method (AB4) is deﬁned as
follows: for k = 0, . . . , K −4,
w k+4 −w k+3 = τ
55
24f k+3 −59
24f k+2 + 37
24f k+1 −9
24f k

,
(20.6)
where f j = f (tj, w j). This requires the starting values w 0, w 1, w 2, w 3. The
coeﬃcients are a4 = 1, a3 = −1, a2 = a1 = a0 = 0, and b4 = 0, b3 =
55
24,
b2 = −59
24, b1 = 37
24, b0 = −9
24.
Theorem 20.8 (LTE of AB4). Suppose that f ∈F4(S) and u ∈C5([0, T]; Rd)
is the classical solution to (18.1). Then the local truncation error for the AB4
method (20.6) may be expressed as
EEEk+4[u] ≤251d1/2
720
max
η∈[tk,tk+4] ∥u(5)(η)∥2τ4
for every k = 0, 1, . . . , K −4.
1 Named in honor of the British mathematicians John Couch Adams (1819–1892) and Francis
Bashforth (1819–1912).
2 Named in honor of the British mathematician John Couch Adams (1819–1892) and
American astronomer Forest Ray Moulton (1872–1952).

20.2 Adams–Bashforth and Adams–Moulton Methods
563
Proof. Suppose that 0 ≤k ≤K −4. Let pk ∈[P3]d be the vector-valued Lagrange
interpolating polynomial with respect to the four interpolation points
{(tk+j, f (tk+j, u(tk+j)))}3
j=0 .
Then, for all t ∈[tk, tk+4],
f (t, u(t)) = pk(t) + Ek(t),
where Ek is an error function. Thus,
u(tk+4) −u(tk+3) =
Z tk+4
tk+3
f (t, u(t))dt =
Z tk+4
tk+3
pk(t)dt +
Z tk+4
tk+3
Ek(t)dt.
According to the error theory for Lagrange interpolation (Theorem 9.16) we
have, for all i = 1, . . . , d,
[Ek]i(t) = 1
4!
d4
dt4 [f (t, u(t))]i|t=ξi
3
Y
j=0
(t −tk+j) = 1
24[u(5)]i(ξi(t))
3
Y
j=0
(t −tk+j)
for all t ∈[tk, tk+4] and some ξi = ξi(t) ∈(tk, tk+4). We ﬁnd then that, after the
change of variables t = rτ + tk+3,
Z tk+4
tk+3
[Ek]i(t)dt = τ5
24
Z 1
0
r(r + 1)(r + 2)(r + 3)[u(5)]i(ξi(t))dr.
Evidently, Ek is a continuous function on [0, T] since it is the diﬀerence of
continuous functions. Now set
g(r) = r(r + 1)(r + 2)(r + 3)
and observe that g ≥0 on [0, 1]. Then, taking norms,

Z tk+4
tk+3
Ek(t)dt

2
≤τ5
24
Z 1
0
g(r)
 d
X
i=1
[u(5)]i(ξi(t))

2
!1/2
dr
≤d1/2
max
ξ∈[tk,tk+4]
u(5)(ξ)

2
τ5
24
Z 1
0
g(r)dr
=
max
ξ∈[tk,tk+4]
u(5)(ξ)

2
τ5
24
251d1/2
30
= 251d1/2
720
τ5
max
ξ∈[tk,tk+4]
u(5)(ξ)

2 .

564
Linear Multi-step Methods
Let us use the notation f k+j
e
= f (tk+j, u(tk+j)), j = 0, 1, 2, 3. Then
pk(t) = f k
e
(t −tk+1)(t −tk+2)(t −tk+3)
(tk −tk+1)(tk −tk+2)(tk −tk+3)
+ f k+1
e
(t −tk)(t −tk+2)(t −tk+3)
(tk+1 −tk)(tk+1 −tk+2)(tk+1 −tk+3)
+ f k+2
e
(t −tk)(t −tk+1)(t −tk+3)
(tk+2 −tk)(tk+2 −tk+1)(tk+2 −tk+3)
+ f k+3
e
(t −tk)(t −tk+1)(t −tk+2)
(tk+3 −tk)(tk+3 −tk+1)(tk+3 −tk+2).
Observe that
pk(tk+j) = f k+j
e
= f (tk+j, u(tk+j)),
j = 0, 1, 2, 3.
Integrating pk on the interval [tk+3, tk+4], the reader can conﬁrm that
Z tk+4
tk+3
pk(t)dt = τ
55
24f k+3
e
−59
24f k+2
e
+ 37
24f k+1
e
−9
24f k
e

.
Using the deﬁnition of the local truncation error (20.2), the result is proven.
Remark 20.9 (consistency). With the same hypotheses as in Theorem 20.8, we
can use the result of Theorem 20.4 to come to the same conclusion as above. In
particular, for the AB4 method, we ﬁnd
C0 = C1 = C2 = C3 = C4 = 0,
C5 = 251
720.
Example 20.4
The Adams–Moulton four-step method (AM4) is deﬁned as
follows: for k = 0, . . . , K −4,
w k+4 −w k+3 = τ
251
720f k+4 + 646
720f k+3 −264
720f k+2
+106
720f k+1 −19
720f k

,
(20.7)
where f j = f (tj, w j). This requires the starting values w 0, w 1, w 2, w 3. The
coeﬃcients are a4 = 1, a3 = −1, a2 = a1 = a0 = 0, and b4 = 251
720, b3 = 646
720,
b2 = −264
720, b1 = 106
720, b0 = −19
720.
As we will see in the proof of the following result, the diﬀerence between the
AB4 and AM4 methods is that the interpolating polynomial in the AM4 method
uses the additional implicit time level point f (tk+4, u(tk+4)). This results in the
AM4 method being more accurate, but comes at the cost of producing an implicit
method. Implicit methods are usually always more complicated in practice than
explicit methods.

20.3 Backward Diﬀerentiation Formula Methods
565
Theorem 20.10 (LTE for AM4). Assume that f
∈F5(S). Let the function
u ∈C6([0, T]; Rd) be the classical solution to (18.1). Then the local truncation
error for the AM4 method (20.7) may be expressed as
∥EEEk+4[u]∥2 ≤3d1/2
160
max
η∈[tk,tk+4]
u(6)(η)

2 τ5
for every k = 0, 1, . . . , K −4.
Proof. The proof is similar to that of Theorem 20.8. Suppose that 0 ≤k ≤K −4.
Let pk ∈[P4]d be the vector-valued Lagrange interpolating polynomial uniquely
determined by the ﬁve interpolation points
{(tk+j, f (tk+j, u(tk+j)))}4
j=0 .
Then, for all t ∈[tk, tk+4],
f (t, u(t)) = pk(t) + Ek(t)
and, for all i = 1, . . . , d,
[Ek]i(t) = 1
5!
d5
dt5 [f (t, u(t))]i|t=ξ
4
Y
j=0
(t −tk+j) =
1
120[u(6)]i(ξi(t))
4
Y
j=0
(t −tk+j)
for all t ∈[tk, tk+4] and some ξi = ξi(t) ∈(tk, tk+4). After the change of variables
t = rτ + tk+3,
Z tk+4
tk+3
[Ek]i(t)dt = τ6
120
Z 1
0
(r −1)r(r + 1)(r + 2)(r + 3)[u(6)]i(ξi(t))dr.
Setting
g(r) = −(r −1)r(r + 1)(r + 2)(r + 3),
we observe that g ≥0 on [0, 1] and, as before,

Z tk+4
tk+3
Ek(t)dt

2
≤3d1/2
160 τ6
max
η∈[tk,tk+4]
u(6)(η)

2 .
For the Lagrange interpolating polynomial, the reader can conﬁrm that
Z tk+4
tk+3
pk(t)dt = τ
251
720f k+4
e
+ 646
720f k+3
e
−264
720f k+2
e
+ 106
720f k+1
e
−19
720f k
e

,
where f k+j
e
= f (tk+j, u(tk+j)), j = 0, . . . , 4. The result follows from the deﬁnition
of the local truncation error (20.2).
20.3
Backward Diﬀerentiation Formula Methods
Another important class of multi-step methods is the Backward Diﬀerentiation
Formula (BDF) methods. These diﬀer from the Adams–Bashforth (AB) and
Adams–Moulton (AM) methods in a fundamental way. Whereas the AB and AM

566
Linear Multi-step Methods
methods are derived via an integration procedure, the BDF methods are derived
via diﬀerentiation. We will demonstrate this point shortly. But before that, we give
a general deﬁnition for these methods and derive some of their properties.
Deﬁnition 20.11 (BDF). A linear q-step method (20.1) is called a BDF method
(or a BDFq method) if and only if it is of order q, exactly, and
bq ̸= 0,
bq−1 = bq−2 = · · · = b1 = b0 = 0.
The reader should recall that, herein, we always assume that aq = 1.
Theorem 20.12 (construction of BDF). Let q ∈N and set β =
hPq
j=1
1
j
i−1
.
Suppose that the linear q-step method (20.1) is a BDF method. Then bq = β and
ψ(z) =
q
X
j=0
ajzj = β
q
X
j=1
1
j zq−j(z −1)j
or, equivalently,
aq = 1,
aq−m = β
q
X
j=m
(−1)m
j
 j
m

,
m = 1, . . . , q.
Proof. Appealing to Theorem 20.7, we see that, in a neighborhood of µ = 1,
ψ(µ) −bqµq ln(µ) =
∞
X
m=q+1
˜Cm(µ −1)m
with ˜Cq+1 ̸= 0. In other words, µ = 1 is a (q + 1)-fold zero. Now we make the
substitution µ = ν−1. In a neighborhood of ν = 1,
νqψ(ν−1) + bq ln(ν) =
∞
X
m=q+1
ˆCm(ν −1)m
with ˆCq+1 ̸= 0. Thus, in a neighborhood of ν = 1,
νqψ(ν−1) = bq
∞
X
m=1
(−1)m(ν −1)m
m
+
∞
X
m=q+1
ˆCm(ν −1)m ∈Pq.
This implies that the tail of the series vanishes:
νqψ(ν−1) = bq
q
X
m=1
(−1)m(ν −1)m
m
.
Therefore,
ψ(µ) = bq
q
X
m=1
(−1)mµq(µ−1 −1)m
m
= bq
q
X
m=1
µq−m(µ −1)m
m
=
q
X
j=0
ajµj.
It is clear at this point that aq
= bqβ−1. Thus, to achieve our standard
normalization, we require aq = 1, bq = β.

20.3 Backward Diﬀerentiation Formula Methods
567
Now we establish the equivalence. Using the binomial theorem,
q
X
j=0
ajµj = bq
q
X
j=1
1
j µq−j(µ −1)j
= bq
q
X
j=1
1
j µq−j
jX
m=0
 j
m

µj−m(−1)m
= bq
q
X
j=1
jX
m=0
1
j
 j
m

(−1)mµq−m
= bq
q
X
m=0
q
X
j=max{1,m}
(−1)m
j
 j
m

µq−m.
Thus, we have aq = 1 and
aq−m = β
q
X
j=m
(−1)m
j
 j
m

,
m = 1, . . . , q.
Owing to the previous result, we have the following BDF coeﬃcients.
Example 20.5
For q = 1, we ﬁnd
b1 = 1,
a1 = 1,
a0 = −1.
Of course, this corresponds to the single-step backward Euler method.
Example 20.6
For q = 2, we ﬁnd
b2 = 2
3,
a2 = 1,
a1 = −4
3,
a0 = 1
3.
Example 20.7
For q = 3, we ﬁnd
b3 = 6
11,
a3 = 1,
a2 = −18
11,
a1 = 9
11,
a0 = −2
11.
We conclude by commenting that the traditional way to develop BDF methods is
via diﬀerentiation of the Lagrange interpolating polynomials studied in Chapter 9.
The following example illustrates how to obtain the BDFq method using this
approach.

568
Linear Multi-step Methods
Example 20.8
Let d = 1 and u ∈C1([0, T]) be a classical solution to (18.1).
For 0 ≤k ≤K −2, and any t ∈[tk, tk+2],
u(t) = u(tk) (t −tk+1)(t −tk+2)
(tk −tk+1)(tk −tk+2) + u(tk+1)
(t −tk)(t −tk+2)
(tk+1 −tk)(tk+1 −tk+2)
+ u(tk+2)
(t −tk)(t −tk+1)
(tk+2 −tk)(tk+2 −tk+1) + E(t),
where E is an error term. Diﬀerentiating and evaluating at t = tk+2, we get
u′(tk+2) = u(tk)(tk+2 −tk+1) + (tk+2 −tk+2)
(tk −tk+1)(tk −tk+2)
+ u(tk+1)(tk+2 −tk) + (tk+2 −tk+2)
(tk+1 −tk)(tk+1 −tk+2)
+ u(tk+2)(tk+2 −tk) + (tk+2 −tk+1)
(tk+2 −tk)(tk+2 −tk+1)
+ E′(tk+2)
= 1
2τ u(tk) −2
τ u(tk+1) + 3
2τ u(tk+2) + E′(tk+2)
= f (tk+2, u(tk+2)).
Equivalently,
u(tk+2) −4
3u(tk+1) + 1
3u(tk) + 2τ
3 E′(tk+2) = 2τ
3 f (tk+2, u(tk+2)).
This yields the BDF2 method, as claimed.
20.4
Zero Stability
We now examine the issue of zero stability of multi-step methods. It turns out
that not all consistent multi-step approximation methods are zero stable. This was
not an issue for single-step methods; they are generally always stable. For linear
multi-step methods, we need to take great care: if a consistent method is not also
stable, it will not be a convergent method.
Deﬁnition 20.13 (zero stability). Suppose that f ∈F1(S) and u ∈C2([0, T]; Rd)
is a classical solution to (18.1). Let, for i = 1, 2,

w k
i
	K
k=0 be approximations
generated by the linear q-step method (20.1) with the starting values

w k
i
	q−1
k=0,
i = 1, 2, respectively. The method is called zero stable if and only if there is a C > 0
independent of τ > 0 and the starting values such that, for any k = q, . . . , K,
w k
1 −w k
2

2 ≤C
max
m=0,...,q−1 ∥w m
1 −w m
2 ∥2 .
Deﬁnition 20.14 (root condition). The linear q-step method (20.1) satisﬁes the
root condition if and only if:

20.4 Zero Stability
569
1. All of the roots of the ﬁrst characteristic polynomial ψ(z) = Pq
j=0 ajzj are inside
the unit disk
{z ∈C | |z| ≤1} ⊂C.
2. If ψ(ξ) = 0 and |ξ| = 1, then ξ is a simple root, i.e., its multiplicity is exactly
one, i.e., ψ′(ξ) ̸= 0.
Deﬁnition 20.15 (homogeneous zero stability). Suppose that f ≡0 and u0 = 0,
so that the unique solution to (18.1) is u(t) = 0 for all t ≥0. Let

w k	K
k=0 be
the approximation generated by the linear q-step method (20.1) with the starting
values

w k	q−1
k=0. The method is called homogeneous zero stable if and only if
there is a C > 0 independent of τ > 0 and the starting values such that, for any
k = q, . . . , K,
w k
2 ≤C
max
m=0,...,q−1 ∥w m∥2 .
Deﬁnition 20.16 (stable solutions). Suppose that {aj}q−1
j=0 ⊂C are given. An
equation of the form
ζk+q +
q−1
X
j=0
ajζk+j = 0,
k = 0, 1, 2, . . .
(20.8)
is called a homogeneous diﬀerence equation. We say that solutions to (20.8) are
stable if and only if, given any starting values {ζk}q−1
k=0 ⊂R, the sequence {ζk}∞
k=0 ⊂
R is bounded by a constant C > 0 that only depends upon the starting values.
We will see that the concepts of stability, homogeneous zero stability, and the
root condition are all actually equivalent.
Example 20.9
In this example, we exhibit a method that does not satisfy the
root condition and is not homogeneously zero stable. Consider the method q = 2,
a2 = 1, a1 = −3, a0 = 2 and b2 = 0, b1 = 0, b0 = −1. In other words,
w k+2 −3w k+1 + 2w k = −τf (tk, w k)
with the starting values w 0, w 1. The method is consistent. We ﬁnd
C0 = 0 = C1,
C2 = 1
2,
which implies that the method is consistent to order p = 1.
The ﬁrst characteristic polynomial is ψ(z) = z2 −3z + 2 = (z −1)(z −2).
Thus, the method fails to satisfy the root condition. Since we are considering
homogeneous zero stability, we take f ≡0 and u0 = 0. The solution of the
homogeneous linear constant coeﬃcient diﬀerence equation,
ζk+2 −3ζk+1 + 2ζk = 0,
k = 0, 1, . . . ,

570
Linear Multi-step Methods
is precisely
ζk = 2ζ0 −ζ1 + 2k(ζ1 −ζ0).
This can be veriﬁed by a simple induction argument. For starting values, let us
take ζ0 = 0, ζ1 = τ. Then ζk = τ(2k −1), k = 0, 1, 2, . . .. Let us examine the
approximation at time T = 1. In this case, τ = 1/K and we have, as K →∞,
w K = 2K −1
K
→∞.
Thus, the method is not homogeneously zero stable.
We need the following technical lemma in order to construct general solutions
of linear homogeneous diﬀerence equations. Here, we follow the exposition in the
book by Kress [52].
Lemma 20.17 (operator Q). Suppose that q ∈N and ν(t) = Pq
j=0 βjtj ∈Pq with
coeﬃcients βj ∈C, j = 0, . . . , q. Assume that βq ̸= 0, β0 ̸= 0. Deﬁne the operator
Q: Pq →Pq via Q[u](t) = tu′(t). The number λ ∈C is a root of the polynomial
ν of multiplicity m, m = 1, . . . , q if and only if, for every p = 0, . . . , m −1,
Qp[ν](λ) = 0,
(20.9)
where Q0 is the identity operator but Qm[ν](λ) ̸= 0.
Proof. We start with a couple of observations. First, since β0 ̸= 0, λ = 0 is not a
root of ν. Next, as the case m = 1 is trivial, we will assume that m > 1. Finally,
the repeated application of Q results in
Qp[ν](t) =
q
X
j=0
βjjptj ∈Pq,
which holds for all p = 0, 1, 2, . . ., provided we interpret 00 = 1.
( =⇒) Suppose that λ ̸= 0 is a root of ν of multiplicity m > 1. Then ν(t) =
(t −λ)mφ0(t), where φ0 ∈Pq−m and φ0(λ) ̸= 0. Then
Q[ν](t) = tφ′
0(t)(t −λ)m + tφ0(t)m(t −λ)m−1 = (t −λ)m−1φ1(t),
where φ1 ∈Pq−m+1 and φ1(λ) ̸= 0. Clearly, Q[ν](λ) = 0. Continuing recursively,
we observe that
Qp[ν](t) = (t −λ)m−pφp(t),
where φp ∈Pq−m+p and φp(λ) ̸= 0, for all p = 1, . . . , m −1, with Qp[ν](λ) = 0.
However, it is clear that Qm[ν](λ) ̸= 0.
( ⇐= ) To obtain a contradiction, suppose that λ ̸= 0 is not a root of multiplicity
m > 1, but property (20.9) holds. Then we proceed in three steps.
1. If λ is not a root of ν at all, then we arrive at a contradiction, namely Q0[ν](λ) =
ν(λ) ̸= 0.

20.4 Zero Stability
571
2. If λ is a root of ν of multiplicity n < m, then we again get a contradiction, since
Qn[ν](λ) ̸= 0.
3. Lastly, if λ is a root of ν of multiplicity n > m, we get a contradiction, since
Qm[ν](λ) = 0. It must be that property (20.9) implies that λ ̸= 0 is a root of
multiplicity exactly m.
This concludes the proof.
Theorem 20.18 (solution of diﬀerence equations). Suppose that q ∈N, aq = 1,
{aj}q−1
j=0 ⊂R, with a0 ̸= 0, and {ζj}q−1
j=0 ⊂R are given. The linear homogeneous
diﬀerence equation (20.8) has a unique solution {ζk}∞
k=0. Assume that λj ∈C,
j = 1, . . . , r, where r ≤q, are the distinct roots of the characteristic polynomial
ψ, with multiplicities mj, respectively. Hence,
ψ(z) =
rY
j=1
(z −λj)mj,
r
X
j=1
mj = q.
Then the solution to (20.8) is given by
ζk =
r
X
j=1
pj(k)λk
j ,
where pj ∈Pmj−1. In particular,
pj(z) =
mj−1
X
m=0
αj,mzm
for some coeﬃcients αj,m ∈C, #{αj,m} = q, that are uniquely determined by the
q starting values ζ0, . . . , ζq−1.
Proof. By linearity, uniqueness follows from the fact that the only possible solution
to (20.8) with zero starting values: ζj = 0 for j = 0, . . . , q −1, is a sequence of
zeros ζk = 0, k = 0, 1, . . ..
To show existence, suppose that λ is the root of ψ of multiplicity m ≥1. Consider
the sequence
ζk = knλk
(20.10)
for some n = 0, . . . , m −1. Then
ζk+q +
q−1
X
j=0
ajζk+j =
q
X
j=0
aj(k + j)nλk+j
=
q
X
j=0
aj
n
X
i=0
n
i

kijn−iλk+j
= λk
n
X
i=0
n
i

ki
q
X
j=0
ajjn−iλj
= λk
n
X
i=0
n
i

kiQn−i[ψ](λ),

572
Linear Multi-step Methods
where Q is the operator deﬁned in Lemma 20.17. Observe that, since ψ(z) =
(z −λ)mφ(z), where φ ∈Pq−m,
Qn−i[ψ](λ) = 0
for all n = 0, . . . , m −1 and i = 0, . . . , n, appealing to Lemma 20.17. This proves
that (20.10) is a solution to (20.8).
Next, we aim to prove that the general solution is just a linear combination of
the solutions from above. To establish that the general solution has the form
ζk =
r
X
j=1
mj−1
X
m=0
αj,mkmλk
j ,
k = 0, 1, . . . ,
(20.11)
we need to determine the q free parameters α1,0, α1,1, . . . , αr,mr −1 ∈C such that
(20.11) holds for the q starting values, i.e., (20.11) holds for k = 0, . . . , q −1.
This forms a square q × q system of linear equations that is uniquely solvable if
and only if the corresponding homogeneous system has only the trivial solution. In
other words, we want to show that
r
X
j=1
mj−1
X
m=0
αj,mkmλk
j = 0,
k = 0, 1, . . . , q −1
(20.12)
implies that αj,m = 0 for j = 1, . . . , r and m = 0, . . . , mj −1.
Note that we can represent the homogeneous system above as Bα = 0, where
α⊺= [α1,0, α1,2, . . . , αr,mr −1]⊺. We recall that B is nonsingular if and only if B⊺is
nonsingular if and only if BH is nonsingular. In order to prove that B⊺is nonsingular,
we consider the homogeneous adjoint equation B⊺β = 0. In particular, suppose that
βk ∈C, k = 0, . . . , q −1 satisfy the homogeneous adjoint equations
q−1
X
k=0
βkkmλk
j = 0,
j = 1, . . . , r,
m = 0, . . . , mj −1.
(20.13)
Now deﬁne
η(t) =
q−1
X
k=0
βktk.
Then, for any m ∈N,
Qm[η](t) =
q−1
X
k=0
βkkmtk,
and it is clear from (20.13) that
Qm[η](λj) = 0,
j = 1, . . . , r,
m = 0, . . . , mj −1.
Appealing to Lemma 20.17, this proves that λj is a root η of multiplicity mj for
j = 1, . . . , r. All told, η ∈Pq−1 has q roots, counting multiplicities. The only
possibility, therefore, is that η ≡0. In other words, βk = 0, k = 0, . . . , q −1.

20.4 Zero Stability
573
Because of the form of the general solution, there is a clear connection between
the root condition and the stability of solutions to the homogeneous diﬀerence
equation (20.8).
Theorem 20.19 (root condition and stability). Suppose that q ∈N. Consider a
linear q-step method (20.1) with coeﬃcients aj, bj ∈R, j = 0, . . . , q, with aq = 1
and a0 ̸= 0. The solutions to the corresponding homogeneous diﬀerence equation
(20.8) are bounded, i.e., stable, if and only if the root condition is satisﬁed.
Proof. Given the starting values ζj, j = 0, . . . , q−1, the general solution to (20.8) is
ζk =
r
X
j=1
mj−1
X
m=0
αj,mkmλk
j ,
k = 0, 1, . . . ,
where the q coeﬃcients α1,1, α1,2, . . . , αr,mr ∈C are determined uniquely by the q
starting values.
( =⇒) From the form of the general solution, it is clear that, if the root condition
is not satisﬁed, the approximations can grow unboundedly, as in Example 20.9.
( ⇐= ) Conversely, if the approximations remain bounded, the only possibility is
that the root condition is satisﬁed. The details are left to the reader as an exercise;
see Problem 20.8.
Theorem 20.20 (nonhomogeneous diﬀerence equations). Let q ∈N and, for m =
0, . . . , q −1, the sequence
n
g(m)
k
o∞
k=0 be the unique solution to the homogeneous
linear diﬀerence equation (20.8) with aj ∈R, j = 0, . . . , q, aq = 1, and a0 ̸= 0 and
starting values
g(m)
k
= δk,m,
k, m = 0, 1, 2, . . . , q −1.
(20.14)
Let {ck}∞
k=q ⊂C be a given sequence. Then there is a unique solution to the linear
diﬀerence equation
ζk+q +
q−1
X
j=0
ajζk+j = ck+q,
k = 0, 1, . . . ,
(20.15)
with starting values {ζk}q−1
k=0. This solution is given by
ζk+q =
q−1
X
j=0
ζjg(j)
k+q +
k
X
j=0
cj+qg(q−1)
k+q−j−1,
k = 0, 1, 2, . . . .
(20.16)
Proof. See Kress [52, Chapter 10] or Gautschi [32, Chapter 6].
Theorem 20.21 (zero stability). A linear q-step method (20.1) is homogeneous
zero stable if and only if solutions to the corresponding homogeneous equations
(20.8) are stable.
Proof. It suﬃces to prove the result for the scalar case, i.e., d = 1.
( =⇒) Suppose that the linear q-step method (20.1) with the ﬁrst characteristic
polynomial ψ(z) = Pq
j=0 ajzj is homogeneous zero stable, i.e., if {w k}K
k=q solves
(20.1) with starting values {w m}q−1
m=0, then

574
Linear Multi-step Methods
|w k| ≤C
max
m=0,...,q−1 |w m|.
Let now the sequence {ζk}∞
k=q solve (20.8) with starting values ζk = w k for
k = 0, . . . , q −1. Notice that we must necessarily have ζk = w k for k = q, . . . , K,
where τK = T. In other words, the product τK is always the same ﬁxed constant.
The homogeneous zero stability of (20.1) then shows that there exists a constant
C > 0 independent of τ > 0 such that, for any k = q, . . . , K,
|ζk| ≤C
max
m=0,...,q−1 |ζm| .
(20.17)
Since C is independent of τ, it must also be independent of K. In other words,
K ∈N may be arbitrarily large. It follows that (20.17) holds for any k ∈N. It must
be that {ζk}∞
k=q is bounded for any given set of starting values {ζk}q−1
k=0.
( ⇐= ) Let, for m = 0, . . . , q −1,
n
g(m)
k
o∞
k=0 be the unique solution to the
homogeneous linear diﬀerence equation (20.8) with the “impulse” starting values
(20.14). Then, for all m = 0, . . . , q −1,
g(m)
k
 ≤Cm
for all k ∈N, where Cm > 0 is independent of k. We can then deﬁne
bC =
max
m=0,...,q−1 Cm
to obtain a constant that is independent of m. Suppose that, with the starting
values

w k	q−1
k=0, the sequence

w k	∞
k=q satisﬁes (20.1) with f ≡0. Then, using
(20.16), we have
w k+q =
q−1
X
j=0
w jg(j)
k+q,
k = 0, 1, 2, . . . .
Taking absolute values and using the triangle inequality, we get
w k+q ≤
q−1
X
j=0
w j
g(j)
k+q
 ≤bC
q−1
X
j=0
w j ≤q bC
max
m=0,...,q−1 |w m| .
The proof is complete.
Remark 20.22 (a0 = 0). We have only discussed the case for which a0 ̸= 0. What
if a0 = 0?
20.5
Convergence of Linear Multi-step Methods
Having discussed the notions of consistency and stability for multi-step methods,
we are now ready to present the theory regarding their convergence. We begin with
yet another discrete incarnation of Gr¨onwall’s3 Lemma.
3 Named in honor of the Swedish–American mathematician Thomas Hakon Gr¨onwall
(1877–1932).

20.5 Convergence of Linear Multi-step Methods
575
Lemma 20.23 (discrete Gr¨onwall). Let {an}∞
n=0 ⊂R+ ∪{0} be a sequence with
the property that, for n = 1, 2, . . .,
an ≤b
n−1
X
m=0
am + c
for some constants b > 0 and c ≥0. Then, for all n = 1, 2, . . . ,
an ≤(ba0 + c) e(n−1)b.
Proof. The result follows by an induction argument, which is left to the reader as
an exercise; see Problem 20.11.
Theorem 20.24 (convergence). Let p, K, q ∈N with q < K. Suppose that f ∈
Fp(S), so that u ∈Cp+1([0, T]; Rd) satisﬁes the IVP (18.1). Let

w k	K
k=0 ⊂Rd
be an approximation generated by the linear q-step method (20.1) with the starting
values

w k	q−1
k=0 ⊂Rd. Assume that the starting values are such that, for some
constant τ0 ∈(0, T] and some C0 > 0, we have
max
k=0,...,q−1
ek
2 ≤C0τp,
∀τ ∈(0, τ0].
Assume, in addition, that the multi-step method is consistent to order p and
satisﬁes the root condition. In this setting, there are constants τ1 ∈(0, T] and
C1 > 0 such that
max
k=0,...,K
ek
2 ≤C1τp,
∀τ ∈(0, τ1].
Proof. Let us assume that the method is implicit. The explicit case is simpler. Then
we have the following error equation: for k = 0, . . . , K −q,
ek+q +
q−1
X
j=0
ajek+j = τ
q
X
j=0
bj
 f (tk+j, u(tk+j)) −f (tk+j, w k+j)

+ τEEEk+q[u]
= τck+q.
Since f satisﬁes a global u-Lipschitz condition, we can estimate ck+q, as follows:
by the triangle inequality and the Lipschitz continuity,
ck+q
2 ≤LB
q
X
j=0
ek+j
2 +
EEEk+q[u]

2 ,
(20.18)
where B = maxj=0,...,q |bj| and L > 0 is the standard Lipschitz constant.
By Theorem 20.20, the solution of the error equation can be represented as
ek+q =
q−1
X
j=0
g(j)
k+qej + τ
k
X
j=0
g(q−1)
k+q−j−1cj+q
for all k = 0, . . . , K −q. Because the q-step method satisﬁes the root condition
and is, therefore, homogeneous zero stable, the solutions g(j)
k
are bounded for every

576
Linear Multi-step Methods
j = 0, . . . , q−1 and every k = 0, 1, 2, . . .. In other words, there is a constant C > 0
such that, for all j = 0, . . . , q −1 and every k = 0, 1, 2, . . .,
g(j)
k
 ≤C.
So, we can estimate the error as
ek+q
2 ≤
q−1
X
j=0
Cτp + τ
k
X
j=0
C
cj+q
2 ≤C

τp + τ
k
X
j=0
cj+q
2

,
provided that 0 < τ ≤τ0. Here and in what follows, the constant C may change
value from line to line. The important point is that it is a constant, and it is
independent of all the involved quantities. Now we can use our estimate (20.18)
above to obtain
ek+q
2 ≤C


τp + τBL
k
X
j=0
q
X
m=0
ej+m
2 + τ
k
X
j=0
EEEj+q[u]

2



≤C


τp + BLτ
k
X
j=0
q
X
m=0
ej+m
2 + τ(k + 1)C1τp



≤C


τp + BLτ
k
X
j=0
q
X
m=0
ej+m
2 + TC1τp



(20.19)
for all k = 0, . . . , K −q and for all 0 < τ ≤τ1. Now observe that
k
X
j=0
q
X
m=0
ej+m
2 =
q
X
m=0
k
X
j=0
ej+m
2
≤(q + 1)
k+q
X
j=0
ej
2
≤(q + 1)
q−1
X
j=0
ej
2 + (q + 1)
k+q
X
j=q
ej
2
≤q(q + 1)Cτp + (q + 1)
k+q
X
j=q
ej
2 .
(20.20)
Combining estimates (20.19) and (20.20), we get
ek+q
2 ≤C


τp + ¯Cτp+1 + BL(q + 1)τ
k+q
X
j=q
ej
2 + T ¯¯Cτp



≤Cτp + τ ˜C
k+q
X
j=q
ej
2 .
Provided that τ is suﬃciently small, in particular
0 < τ ˜C < 1,

20.6 Dahlquist Theorems
577
we have, for all k = 0, 1, . . . , K −q,
ek+q
2 ≤
C
1 −τ ˜C τp + τ
˜C
1 −τ ˜C
k+q−1
X
j=q
ej
2 .
Notice that we have made the sum on the right-hand side explicit with respect to the
left-hand side. Reindexing the summation, we have, for every m = q, q + 2, . . . , K,
∥em∥2 ≤
C
1 −τ ˜C τp + τ
˜C
1 −τ ˜C
m−1
X
j=q
ej
2 .
If we further restrict the time step so that
0 < τ ˜C ≤1
2,
then it follows that
1
1 −τ ˜C ≤2,
and, for every m = q, q + 2, . . . , K,
∥em∥2 ≤2 ˜Cτp + 2τ ˜C
m−1
X
j=q
ej
2 .
Applying Lemma 20.23, we have
∥em∥2 ≤
 2τ ˜C ∥eq∥2 + 2Cτp
exp

2τ(m −q −1) ˜C

≤Cτpe2T ˜C.
The theorem is proven with C1 = Ce2T ˜C.
The reader will notice that, in the proof of Theorem 20.24, we only used the
homogeneous zero stability of the method. In fact, we now have the tools to prove
that, if f satisﬁes the usual global Lipschitz condition, the notions of zero stability
and homogeneous zero stability are equivalent.
Corollary 20.25 (equivalence). Let K, q ∈N with q < K. Suppose that f ∈F1(S),
so that u ∈C2([0, T]; Rd) satisﬁes the IVP (18.1). Let

w k	K
k=0 ⊂Rd be an
approximation generated by the linear q-step method (20.1) with the starting
values

w k	q−1
k=0 ⊂Rd. The multi-step method is zero stable if and only if it is
homogeneously zero stable.
Proof. See Problem 20.12.
20.6
Dahlquist Theorems
As a last topic in our discussion of the linear multi-step method, we present a series
of results due to Dahlquist. The ﬁrst one is known as the Dahlquist Equivalence
Theorem. This essentially gives a converse to Theorem 20.24. The proof can be
found in [32].

578
Linear Multi-step Methods
Theorem 20.26 (Dahlquist Equivalence Theorem 4). Let p, K, q ∈N with q < K.
Suppose that f ∈Fp(S), so that u ∈Cp+1([0, T]; Rd) satisﬁes the IVP (18.1).
Let

w k	K
k=0 ⊂Rd be an approximation generated by the linear q-step method
(20.1) with the starting values

w k	q−1
k=0 ⊂Rd. Suppose that the method satisﬁes
the root condition. Then the multi-step method is consistent to order p if and only
if it is globally convergent with order p.
The next theorem is known as the Dahlquist First Barrier Theorem; see, for
example, [12] or [32]. The result gives us a ﬁrm upper limit on the order of a zero
stable multi-step method.
Theorem 20.27 (Dahlquist First Barrier Theorem). The order of accuracy
(consistency) of a zero stable linear q-step method (20.1) cannot exceed q + 1 if
q is odd or q + 2 if q is even.
Problems
20.1
Complete the proof of Corollary 20.6.
20.2
Show that the two-step (implicit) Adams–Moulton method,
w k+2 −w k+1 = τ
 5
12f (tk+2, w k+2) + 8
12f (tk+1, w k+1)
−1
12f (tk, w k)

,
is at least order one using the conditions ψ(1) = 0 and ψ′(1) −χ(1) = 0. Prove
that, in fact, the method is exactly third order.
20.3
Show that the three-step (implicit) Adams–Moulton method,
w k+3 −wk+2 = τ
 9
24f (tk+3, w k+3) + 19
24f (tk+2, w k+2)
−5
24f (tk+1, w k+1) + 1
24f (tk, w k)

,
is at least order one using the conditions ψ(1) = 0 and ψ′(1) −χ(1) = 0. Prove
that, in fact, the method is exactly fourth order using both the method of C’s and
the log-method.
20.4
Find all of the values of α and β, so that the three-step method,
w k+3 + α(w k+2 −w k+1) −w k = τβ

f (tk+2, w k+2) + f (tk+1, w k+1)

,
is of order four.
20.5
Show that the BDF3 method,
w k+3 −18
11w k+2 + 9
11w k+1 −2
11w k = 6
11τf (tk+3, w k+3),
is consistent to order p = 3 using both the method of C’s and the log-method.
20.6
The Adams–Bashforth two-step method is given by
w k+2 = w k+1 + τ
3
2f (tk+1, w k+1) −1
2f (tk, w k)

.
4 Named in honor of the Swedish mathematician Germund Dahlquist (1925–2005).

Problems
579
Derive this method by an integration procedure and give an exact expression for
the local truncation error.
20.7
Derive the general form of a q-step BDF method using the method of C’s.
20.8
Complete the proof of Theorem 20.19.
20.9
Show that the BDF3 method,
w k+3 −18
11w k+2 + 9
11w k+1 −2
11w k = 6
11τf (tk+3, w k+3),
satisﬁes the root condition.
Hint: One of the roots is w = 1.
20.10
Consider the method
w k+2 −w k = τ
3

f (tk+2, w k+2) + 4f (tk+1, w k+1) + f (tk, w k)

.
Show that it is of fourth order and it obeys the root condition.
20.11
Prove Lemma 20.23.
20.12
Prove Corollary 20.25.
20.13
Show that, for all the values of α and β that make the three-step method
w k+3 + α(w k+2 −w k+1) −w k = τβ

f (tk+2, w k+2) + f (tk+1, w k+1)

of order four, the resulting method does not satisfy the root condition and is,
therefore, not convergent.
20.14
Show that a linear multi-step method is of order p ≥1 if and only if it
yields the exact solution to an ordinary diﬀerential equation problem whose solution
is a polynomial of degree no greater than p.
20.15
Recall Simpson’s quadrature rule:
Z b
a
f (x)dx = b −a
6

f (a) + 4f
a + b
2

+ f (b)

+ E[f ](a, b),
where E[f ](a, b) is an error term that satisﬁes
|E[f ](a, b)| ≤C(b −a)4
and C > 0 is a constant that depends on f . Starting from the identity
u(tk+1) = u(tk−1) +
Z tk+1
tk−1
f (s, u(s))ds,
use Simpson’s rule to derive a two-step method. Determine its order and whether
it is convergent.
20.16
Show that the method
w k+2 −3w k+1 + 2w k = τ
13
12f (tk+2, w k+2) −5
3f (tk+1, w k+1) −5
12f (tk, w k)

is of order two. However, this method does not converge. Why?
20.17
Show that the explicit multi-step method,
w k+3 + α2w k+2 + α1w k+1 + α0w k
= τ

β2f (tk+2, w k+2) + β1f (tk+1, w k+1) + β0f (tk, w k)

,

580
Linear Multi-step Methods
is fourth order only if α0 + α2 = 8 and α1 = −9. Prove that this method cannot
be both fourth order and convergent.
20.18
Consider the method
w k+2 + w k+1 −2w k = τ

f (tk+2, w k+2) + f (tk+1, w k+1) + f (tk, w k)

.
What is the order of the method? Is it a convergent method?
20.19
Study the order and convergence of the method
w k+1 −w k = τ
12

5f (tk+1, w k+1) + 8f (tk, w k) −f (tk−1, w k−1)

.
Listings
1
function res = MethodCs( a, b, m )
2
% The method of Cs to determine the order of consistency of a
3
% linear multistep method.
4
%
5
% Input
6
% a : The coefficients of the first characteristic polynomial
7
% b : The coefficients of the second characteristic polynomial
8
% m : The number of C that one wants to compute
9
%
10
% Output
11
% res: The number C m. A method is consistent to order exactly
12
%
p if C 0 = ... = C p = 0, but C {p+1} != 0
13
if m == 0
14
res = sum( a );
15
else
16
res = 0.;
17
q = length(a);
18
factmminusone = factorial(m-1);
19
factm = m*factmminusone;
20
for j=1:q
21
res = res + a(j)*(j-1)ˆm/factm - b(j)*(j-1)ˆ(m-1) ...
22
/factmminusone;
23
end
24
end
25
end
Listing 20.1 Algorithmic description of the method of C’s.

21
StiﬀSystems of Ordinary
Diﬀerential Equations and Linear
Stability
In Section 19.4, we studied the approximation of dissipative equations for which,
as we know, all solutions are Lyapunov stable; see Example 17.3. In this chapter,
we continue with the topic of stability of solutions, and try to understand which
methods are suited to approximate Lyapunov stable solutions. As the discussion of
Section 17.5 shows, it is suﬃcient to study linear equations and the stability of the
origin. With this in mind, we begin by presenting an example from [47].
Example 21.1
Suppose that u′(t) = Au(t) for t ∈[0, T] with u(0) = u0, where
A =
−100
1
0
−1
10

.
As Theorem 17.23 shows, the origin is stable for this problem. Let us apply the
forward Euler method to approximate the solutions. Then
w k = (I + τA)k u0,
k = 0, 1, . . . , K,
where τ = T/K. Now observe that we have the diagonalization A = XDX−1, where
X =
1
1
0
999
10

,
D =
−100
0
0
−1
10

.
The solution to the initial value problem (IVP) is
u(t) = XetDX−1u0,
where
etD =
e−100t
0
0
e−1
10 t

.
Decomposing u0 in the basis of eigenvectors (columns of X), we have
u0 = α1x1 + α2x2,
x1 =
1
0

,
x2 =
 1
999
10

,
for some α1, α2 ∈R. Thus,
u(t) = α1e−100tx1 + α2e−1
10 tx2.

582
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
Meanwhile,
w k = X (I + τD)k X−1Xα = α1 (1 −100τ)k x1 + α2

1 −1
10τ
k
x2.
(21.1)
Suppose that τ >
1
50. Then
100τ > 2
⇐⇒
100τ −1 > 1
=⇒
|1 −100τ| > 1,
and the ﬁrst term in the forward Euler approximation blows up rapidly.
Now, suppose that u0 is the eigenvector associated with the eigenvalue
λ2 =
−1
10:
u0 =
 1
999
10

= x2.
In this case, the solution satisﬁes
u(t) = e−1
10 tx2 →0,
as t →∞. Likewise, as k →∞,
w k =

1 −1
10τ
k
x2 →0,
provided that
1 −1
10τ
 < 1
⇐⇒
0 < τ < 20.
All seems well, but there is a problem. Suppose that
1
50 < τ < 20,
and in the general numerical approximation (21.1) α2 = 1, but α1 = ε ∈(0, 1),
with ε ≪1. This corresponds to the case that u0 ≈x2, where the ε represents a
small error.
Roundoﬀerrors will guarantee that ε(1 −100τ)k will eventually dominate,
regardless of the smallness of ε. So, while we get the correct asymptotic behavior
for the second term, the present method does not capture the correct behavior for
the ﬁrst, unless the time step τ is suﬃciently small.
Example 21.2
Not all methods behave in this way. For example, applying the
trapezoidal rule to approximate solutions to the problem in the previous example,
one obtains
w k = α1
1 −50τ
1 + 50τ
k
x1 +
 
1 −1
20τ
1 + 1
20τ
!k
x2.
For every τ > 0,
max
(
1 −50τ
1 + 50τ
 ,

1 −1
20τ
1 + 1
20τ

)
< 1,
and the correct behavior is predicted; namely, the approximation decays to 0 as
k →∞.

StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
583
Deﬁnition 21.1 (stiﬀness). Suppose that the matrix A ∈Rd×d is diagonalizable,
with spectrum σ(A) = {λ1, . . . , λd} ⊂C. Assume that
ℜλj < 0,
j = 1, . . . , d,
and, more speciﬁcally, that the eigenvalues are ordered such that
0 > ℜλ1 ≥ℜλ2 ≥· · · ≥ℜλd−1 ≥ℜλd.
Let g ∈C
 R; Rd
. The system of ordinary diﬀerential equations (ODEs)
u′(t) = Au(t) + g(t),
which is called a linearly dissipative system, is said to have stiﬀness ratio
Q = −ℜλd
−ℜλ1
≥1.
Solutions of a linear dissipative system are easily expressed. Suppose that the
linearly independent eigenpairs are denoted (λi, xi) for i = 1, . . . , d. Then the
general solution has the form
u(t) =
d
X
i=1
αieλitxi + q(t),
where the αi are constants and the function q is determined by an integral. If
g ≡0, then q ≡0; otherwise, it is not important to know the precise form of q.
Now, since ℜλj < 0 for all j = 1, . . . , d, each term eλit decays to zero. Thus, the
solution tends asymptotically to q(t) as t →∞. We call the sum Pd
i=1 αieλitxi
the dissipative part, or the transient part, of the solution.
The idea is basically the same as before. If Q is large, then there is a good chance
that the forward Euler method, say, will not accurately capture the dissipative
nature of each exponentially decaying term. In other words, our numerical method
will fail to predict that the solution should tend to q(t) as t →∞, which is a big
problem.
Remark 21.2 (stiﬀsystems). Loosely, we say that u′(t) = Au(t) + g(t) is stiﬀ
when Q ≫1. The greater the value of Q, the greater the risk that we will fail to
capture the correct asymptotic decay of the dissipative part of the true solution,
especially the rapidly decaying terms, as t →∞.
Remark 21.3 (local stiﬀness). For more general dissipative systems
u′(t) = f (t, u(t)),
we can examine stiﬀness locally, if not globally, by considering the eigenvalues of
the matrix Duf =

∂ujfi

. Assuming that the eigenvalues all have negative real
parts, etc., if the ratio Q is large at some point, then we say that the equation is
locally stiﬀin a neighborhood of that point.

584
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
DFE
ℜz
ℑz
z = −1
Figure 21.1 Linear stability domain DFE for the forward Euler method. This method is
not A-stable.
21.1
The Linear Stability Domain and A-Stability
Deﬁnition 21.4 (linear stability domain). Let λ ∈C. Consider the IVP
u′(t) = λu(t),
u(0) = 1.
(21.2)
Suppose that {w k}∞
k=0 is a numerical approximation of u generated with the time
step size τ > 0 by a single-step or Runge–Kutta (RK) method, with w 0 = 1,
or a linear q-step method, with consistent starting values, such as w j = 1 for
j = 0, . . . , q −1. The linear stability domain of this method is the set
D =

z = λτ ∈C
 w k →0, k →∞
	
.
The approximation method is called A-stable if and only if
C−= {z ∈C | ℜ(z) < 0} ⊆D.
In words, a method is A-stable if and only if its linear stability domain contains the
left-half complex plane.
Remark 21.5 (history and context). The term A-stable is due to the Swedish
mathematician Germund Dahlquist (1925–2005). Some authors use the term
linearly stable. If a method is A-stable, then it may be a good choice for
approximating the solutions of a simple linearly dissipative system with a large
stiﬀness ratio. This is due to the fact that the method will accurately predict the
asymptotic decay of each term of the dissipative part of the solution.

21.2 A-Stability of Runge–Kutta Methods
585
Example 21.3
The forward Euler method is not A-stable; therefore, it would not
be an appropriate choice for a stiﬀsystem. To see this, we compute its linear
stability domain. The forward Euler approximation applied to (21.2) yields
w k = gk(λτ),
g(z) = 1 + z.
Clearly, w k →0 as k →∞if and only if |1 + λτ| < 1. Thus,
DFE = {z ∈C | |1 + z| < 1} ,
which is a unit disk centered at z = −1. The function g is called the ampliﬁcation
factor; see Figure 21.1.
Example 21.4
The backward Euler method is A-stable. Its linear stability domain
is
DBE = {z ∈C | |1 −z| > 1} ,
as the reader can show; see Problem 21.2. Thus, DBE contains every point z ∈C
that is outside of the unit disk centered at z = 1. In particular, DBE contains C−;
therefore, the backward Euler method is A-stable.
Example 21.5
The trapezoidal rule is A-stable. Its linear stability domain is
DTR =
(
z ∈C

1 + z
2

1 −z
2
 < 1
)
.
The reader should prove that DTR = C−, and, therefore, that the trapezoidal rule
is A-stable.
21.2
A-Stability of Runge–Kutta Methods
Let us now study the A-stability of RK methods. We begin with an impossibility
result.
Theorem 21.6 (impossibility). No consistent explicit RK is A-stable.
Proof. This follows from Theorem 19.5. In particular, applying an explicit r-stage
RK method to approximate the solution to the IVP (21.2), one obtains
w k = gk(λτ),
where the ampliﬁcation factor g is a polynomial of degree at most r. Clearly, there
is a point z ∈C−such that |g(z)| > 1. At all such points, |gk(z)| = |g(z)|k →∞
as k →∞.

586
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
The situation for implicit RK methods is a bit more complicated. Many implicit
RK methods are A-stable, but some are not. The proof of the following should be
clear.
Theorem 21.7 (linear stability criterion). Suppose that an implicit r-stage RK
method is applied to approximate the solutions of the IVP (21.2). Then the method
is A-stable if and only if |g(z)| < 1 for all z ∈C−, where the ampliﬁcation factor
g is the rational polynomial from Theorem 19.6.
Proof. See Problem 21.4.
Example 21.6
The implicit RK methods given by the tables
0
1
4
−1
4
2
3
1
4
5
12
1
4
3
4
1
3
5
12
−1
12
1
3
4
1
4
3
4
1
4
are both A-stable and both order three. The reader can prove the A-stability by
considering the ampliﬁcation factors g. In this case, the ampliﬁcation factors are
the same rational polynomial:
g(z) =
1 + 1
3z
1 −2
3z + 1
6z2 .
It can be a rather diﬃcult task to prove (or disprove) that |g(z)| < 1 for all
z ∈C−. The next theorem can make the task considerably simpler.
Theorem 21.8 (A-stability criterion). Let g be a rational polynomial that is not a
constant. Then
|g(z)| < 1,
∀z ∈C−
(21.3)
if and only if all of the poles of g have positive real parts and |g(it)| ≤1 for all
t ∈R.
Proof. ( =⇒) By continuity, |g(z)| ≤1 for all z ∈C−. Thus, there are no poles
(singular points) in C−and |g(it)| ≤1 for all t ∈R.
( ⇐= ) If all of the poles of g are in the right-half complex plane,
C+ = {z ∈C | ℜz > 0} ,
then g is analytic in C−. If it is nonconstant, then, by the Maximum Modulus
Principle (see Theorem 9.33), its maximum in modulus in C−occurs on the
boundary, namely the imaginary axis. Therefore, |g(it)| ≤1 for all t ∈R implies
that |g(z)| < 1 for all z ∈C−.

21.2 A-Stability of Runge–Kutta Methods
587
Deﬁnition 21.9 (A-acceptable). A rational polynomial r is called A-acceptable if
and only if it satisﬁes (21.3). Such rational polynomials are associated with A-stable
implicit Runge–Kutta (IRK) methods. In particular, if the ampliﬁcation factor g for
an IRK method is A-acceptable, we call that method A-acceptable.
Example 21.7
Let us apply Theorem 21.8 to investigate the A-stability of the
IRK methods from Example 21.6, whose ampliﬁcation factors are
g(z) =
1 + 1
3z
1 −2
3z + 1
6z2 .
We ﬁrst ﬁnd its poles, i.e., the roots of the denominator. They are
z1,2 =
2
3 ±
q
4
9 −4 · 1 · 1
6
1
3
= 2 ±
√
2i,
and we see that they have positive real part.
On the other hand, |g(it)| ≤1 is equivalent to requiring that
|1 + 1
3it|2 ≤|1 −2
3it −1
6t2|2,
or that
1 + 1
9t2 ≤1 + 1
9t2 + 1
36t4,
which is always true and we, again, conclude that the method is A-stable.
Example 21.8
For the two-step Gauss–Legendre–RK method, which is consis-
tent to exactly order p = 4, the ampliﬁcation factor is
g(z) = 1 + 1
2z + 1
12z2
1 −1
2z + 1
12z2 .
Using Theorem 21.8, it is easy to see that the method is A-stable/A-acceptable.
The details are left to the reader as an exercise; see Problem 21.7.
To ﬁnd rational polynomials that are A-acceptable, we need the following result.
Lemma 21.10 (approximate exponential). Let g be the ampliﬁcation factor for
an IRK method of order p, then there are constants C0, C1 > 0 such that, for all
z ∈C, with |z| ≤C0,
|g(z) −ez| ≤C1|zp+1|.
As a shorthand, we write g(z) = ez + O(zp+1) as z →0.
Proof. This comes precisely from the deﬁnition of order. Since for the linear
problem (21.2), the solution is u(tk+1) = eλτu(tk) and the method gives
w k+1 = g(λτ)w k.

588
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
We introduce one ﬁnal deﬁnition.
Deﬁnition 21.11 (exponential order). Let g be a nonconstant rational polynomial.
If g(z) = ez + O(zp+1) as z →0, then we say that g is of exponential order p.
The following results essentially narrow down the possible A-acceptable, exponen-
tial order p rational polynomials. We will denote, for m, n ∈N0, by Pm/n the set of
rational polynomials q(t) = p(t)/r(t) whose numerator p ∈Pm and denominator
r ∈Pn.
Theorem 21.12 (Pad´e approximation). For every α, β ∈N0, there is a unique
gα/β ∈Pα/β that is of exponential order α + β. Moreover, gα/β = pα/β/qα/β,
where
pα/β(z) =
α
X
k=0
α
k
(α + β −k)!
(α + β)!
zk,
qα/β(z) = pβ/α(−z).
Note that gα/β(0) = 1. This gα/β is the only element in Pα/β that is of exponential
order α + β and no function in Pα/β exceeds this order.
Deﬁnition 21.13 (Pad´e approximation1). The functions gα/β ∈Pα/β of the
previous theorem are called Pad´e approximations of the exponential.
The following result elucidates when a Pad´e approximation is A-acceptable.
Theorem 21.14 (Wanner–Hairer–Nørsett Theorem2). The Pad´e approximation of
the exponential pα/β is A-acceptable if and only if α ≤β ≤α + 2.
With the help of this result, we can show that Gauss–Legendre methods are not
only of maximal order, but also A-stable.
Corollary 21.15 (A-stability). Every r-stage implicit Gauss–Legendre–RK method
is A-stable.
Proof. For a method with r stages, Theorem 19.6 shows that the associated
ampliﬁcation factor g ∈Pr/r. On the other hand, a Gauss–Legendre method has
order 2r. This implies that g is of order 2r. The uniqueness then implies that
g = gr/r, the Pad´e approximation of the exponential. Finally, since α = β = r, the
Pad´e approximation is A-acceptable.
Remark 21.16 (alternate proof). Theorem 19.23 furnishes another proof of the
fact that every r-stage implicit Gauss–Legendre–RK method is A-stable. This
follows because the linear problem (21.2) with ℜλ < 0 is a dissipative ODE system.
In fact, every dissipative numerical method is A-stable.
1 Named in honor of the French mathematician Henri Eug`ene Pad´e (1863–1953).
2 Named in honor of the Austrian mathematician Gerhard Wanner (1942–), the Austrian
mathematician Ernst Hairer (1949–), and the Norwegian mathematician Syvert Paul Nørsett
(1944–).

21.3 A-Stability of Linear Multi-step Methods
589
21.3
A-Stability of Linear Multi-step Methods
We now study the stability of linear multi-step methods. The situation here is more
delicate since, to start a q-step method, we need q initial values {w k}q−1
k=0. From the
problem, we only have the initial condition u0, so it makes sense to set w 0 = u0.
However, there is no natural way to set the remaining starting values. For this
reason, for A-stability, we need to require that the method produces the correct
asymptotic behavior for all starting values {w k}q−1
k=0.
Theorem 21.17 (linear stability criterion). Consider the linear q-step method given
by (20.1) with the associated ﬁrst and second characteristic polynomials
ψ(µ) =
q
X
j=0
ajµj ∈Pq,
χ(µ) =
q
X
j=0
bjµj ∈Pq.
Deﬁne the parameterized stability polynomial
ρ(z, µ) = ψ(µ) −zχ(µ).
Suppose that, at z ∈C, aq ̸= zbq and
ρ(z, µ) = c(z)
n(z)
Y
i=1
(µ −ωi(z))mi(z) ,
n(z)
X
i=1
mi(z) = q.
Then z ∈D, i.e., z is a point of linear stability if and only if |ωi(z)| < 1 for all
i = 1, . . . , n(z).
Proof. Applying the approximation method to (21.2) and setting z = λτ, we have
q
X
j=0
(aj −zbj)w k+j = 0,
k = 0, 1, . . . .
(21.4)
We have already constructed solutions of this equation in Section 20.4. In particular,
the solution of (21.4) is given by
w k =
n(z)
X
j=1
pj(k)ωk
j (z),
where pj ∈Pmj(z)−1. In particular,
pj(ζ) =
mj(z)−1
X
i=0
αj,iζi
for some coeﬃcients αj,i ∈C, q a number, that are uniquely determined by the
q starting values w 0, . . . , w q−1. Clearly, w k →0 if and only if |ωi(z)| < 1 for all
i = 1, . . . , n(z).
Remark 21.18 (degeneracies). We point out that it is possible for degeneracies to
arise. This happens, in particular, when aq = zbq; consequently,
n(z)
X
i=1
mi(z) < q.

590
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
Special care will be required at such points of degeneracy.
Corollary 21.19 (A-stability). The linear q-step method (20.1) is A-stable if and
only if
|ωi(z)| < 1
for all i = 1, . . . , n(z) and all z ∈C−.
Theorem 21.20 guarantees that no explicit linear multi-step method is A-stable
and it simpliﬁes the conditions on the roots of the parameterized polynomial ρ, but
only slightly.
Theorem 21.20 (A-stability criterion). A linear q-step method (20.1) is A-stable
if and only if bq > 0 and |ωj(it)| ≤1 for all j = 1, . . . , n(z) and all t ∈R.
Proof. The proof is similar to that of Theorem 21.8; see the books by Butcher [12]
or Gautschi [32] for the details.
The following general and powerful theorem is called the Dahlquist Second
Barrier Theorem.
Theorem 21.21 (Dahlquist Second Barrier Theorem3). Regarding q–step meth-
ods, we have that:
1. No explicit method is A-stable.
2. No A-stable method can have order (of consistency) greater than p = 2.
3. The second-order A-stable method with the smallest local truncation error is
the trapezoidal rule.
Proof. See the books by Butcher [12] or Gautschi [32] for a proof.
21.4
The Boundary Locus Method
The boundary locus method is a tool for determining the linear stability domain for
linear multi-step methods. We present the boundary locus method without thorough
theoretical justiﬁcation, though the details can be made rigorous. The general idea
is to approximate the boundary of the linear stability domain, ∂D, rather than D
itself.
Suppose that z ∈∂D. Then, by continuity, it is reasonable to assume that
one of the roots of the stability polynomial ρ(z, · ) satisﬁes ω(z) = eiθ, for some
θ ∈(0, 2π]. In other words,
ρ(z, eiθ) = 0.
Equivalently, provided that χ(eiθ) ̸= 0,
z = ψ(eiθ)
χ(eiθ) ,
which gives an explicit parameterization of the boundary, with respect to θ.
3 Named in honor of the Swedish mathematician Germund Dahlquist (1925–2005).

21.4 The Boundary Locus Method
591
0
5
10
−6
−4
−2
0
2
4
6
ℜ(z)
ℑ(z)
BDF2
BDF3
BDF4
Figure 21.2 The boundary of the linear stability domain, ∂DBDFq, for the BDFq method,
q = 2, 3, 4.
Example 21.9
Let us use the boundary locus method to ﬁnd the linear stability
domain of the two-step Backward Diﬀerentiation Formula (BDF2) method and, in
doing so, show that the method is A-stable. For this method, we have the following
ﬁrst and second characteristic polynomials:
ψ(µ) = µ2 −4
3µ + 1
3,
χ(µ) = 2
3µ2.
Therefore, z ∈∂D if and only if
z = ei2θ −4
3eiθ + 1
3
2
3ei2θ
= 3
2 −2e−iθ + 1
2e−i2θ.
Now, setting z(θ) = x(θ) + iy(θ), we have
x(θ) = 3
2 −2 cos(θ) + 1
2 cos(2θ),
y(θ) = 2 sin(θ) −1
2 sin(2θ).
The set ∂D is plotted in Figure 21.2. This was obtained with the help of Listing
21.1. It is straightforward to show that x(θ) ≥0 for all θ ∈(0, 2π]. Furthermore,
by checking a single point oﬀof the boundary — as long as it is not a point of
degeneracy (at which aq = zbq) — we can determine whether D is “inside” or

592
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
“outside” of the curve. Checking the outside, regular point z = −3
2, we ﬁnd
ρ

−3
2, µ

= 2µ2 −4
3µ + 1
3,
and the roots are
ω

−3
2

= 1 ± i
√
8
4
3
.
We see that
ω

−3
2

2
= 3
18;
therefore,
ω
 z = −3
2
 < 1. So −3
2 ∈D. This implies that D is outside of the
curve shown in Figure 21.2. Since x(θ) ≥0, it must be that C−⊂D. The method
is A-stable.
Example 21.10
Using the boundary locus method, it is possible to see that the
BDF3 method is not A-stable. As above, we will ﬁnd z(θ) = x(θ) + iy(θ), but
observe that x(θ) < 0 for some values of θ. This is enough to show that the BDF3
method cannot be A-stable. This fact is conﬁrmed by the Dahlquist Second Barrier
Theorem, since the BDF3 method is of order three.
Remark 21.22 (degeneracy). Note that, for the BDF2 method, z = 3
2 is a point
of degeneracy:
ρ
3
2, µ

= −4
3

µ −1
4

.
The point z = 3
2 is inside the curve in Figure 21.2. It appears that the single root
ω
 z = 3
2

=
1
4, which is clearly less than one in modulus, indicates a point of
stability. In fact, the second root at this point is at ∞, and this is not a point of
linear stability.
To see what is going on, suppose that we check the regular point z = 3
2 −3
2ε,
where 0 < ε < 1. Then
ρ(z, µ) = εµ2 −4
3µ + 1
3
and
ω(z) = 2 ± √4 −3ε
3ε
.
Then note, as ε →0,
2 + √4 −3ε
3ε
→∞,
2 −√4 −3ε
3ε
→1
4.

Problems
593
Problems
21.1
Suppose that the matrix A ∈Rd×d is diagonalizable, with spectrum σ(A) =
{λ1, . . . , λd} ⊂C. Assume that
ℜλj < 0,
j = 1, . . . , d,
and, more speciﬁcally, that the eigenvalues are ordered such that
0 > ℜλ1 ≥ℜλ2 ≥· · · ≥ℜλd−1 ≥ℜλd.
Let g ∈C(R; Rd) be a given function of time.
a)
Prove that the general solution to the linear dissipative system of ordinary
diﬀerential equations,
u′(t) = Au(t) + g(t),
is given by
u(t) =
d
X
i=1
αieλitxi + q(t),
where {x1, . . . , xd} is the linearly independent set of corresponding eigenvec-
tors, the αi are constants, and q is some function of time. Find the form of
the function q.
b)
Apply the forward Euler method to approximate the solutions of the linearly
dissipative system. For what value of the time step size, τ > 0, does the
numerical approximation accurately capture the asymptotic behavior of the
dissipative part of the solution?
c)
Apply the backward Euler method to approximate the solutions of the linearly
dissipative system. Show that, for any value of the step size, τ > 0, the
numerical approximation correctly captures the asymptotic behavior of the
solution.
21.2
Compute the linear stability domains of the backward Euler method.
21.3
For the trapezoidal rule, show that the linear stability domain is precisely
equal to the left-half complex plane
C−= {z ∈C | ℜz < 0} .
21.4
Prove Theorem 21.7.
21.5
Complete the details of Example 21.6.
21.6
Consider the implicit RK methods given by the tables
0
1
4
−1
4
2
3
1
4
5
12
1
4
3
4
1
3
5
12
−1
12
1
3
4
1
4
3
4
1
4
.
a)
Show that the necessary conditions of Theorem 19.4 are all satisﬁed.
b)
Show that these methods are both A-stable.

594
StiﬀSystems of Ordinary Diﬀerential Equations and Linear Stability
c)
Show that one of the methods is a collocation method and that the
other is not. For the one that is a collocation method, ﬁnd its order of
consistency.
21.7
Provide all the details of Example 21.8.
21.8
Use the boundary locus method to prove that BDF2 is A-stable.
21.9
If you have not already done so, prove that the BDF3 method,
w k+3 −18
11w k+2 + 9
11w k+1 −2
11w k = 6
11τf (tk+3, w k+3),
satisﬁes the root condition and is of order three. Conclude, therefore, that it must
be a convergent method. Use the boundary locus method to prove that BDF3
cannot be A-stable.
21.10
Using the boundary locus method, determine all the values of θ ∈[0, 1] for
which the θ-method,
w k+1 = w k + τ

θf (tk, w k) + (1 −θ)f (tk+1, w k+1)

,
is A-stable.
21.11
Consider the implicit method
w k+2 −w k = 2
3τ

f (tk+2, w k+2) + f (tk+1, w k+1) + f (tk, w k)

.
What is the order of the method? Is it a convergent method? (This method is
A-stable, though it is a bit diﬃcult to prove. Can you prove it?)
21.12
If you have not already done so, prove that the method
w k+2 −w k = τ
3

f (tk+2, w k+2) + 4f (tk+1, w k+1) + f (tk, w k)

is of fourth order and that it obeys the root condition, which implies that it is
convergent. Use a (high-powered) theorem to argue that the method is not A–
stable. Prove directly, by calculating roots of the stability polynomial, that it is not
A-stable.

Listings
595
Listings
1
% A procedure to plot the boundary of the linear stability
2
% domains for the BDF2, BDF3, and BDF4 methods:
3
t = 0:0.01:2*pi;
4
%
5
% BDF2
6
x2 = 1.5-2.0*cos(t)+0.5*cos(2.0*t);
7
y2 = 2.0*sin(t)-0.5*sin(2.0*t);
8
%
9
% BDF3
10
x3 = 11/6-3*cos(t)+3/2*cos(2*t)-1/3*cos(3*t);
11
y3 = 3*sin(t)-3/2*sin(2*t)+1/3*sin(3*t);
12
%
13
% BDF4
14
x4 = 25/12-4*cos(t)+3*cos(2*t)-4/3*cos(3*t)+1/4*cos(4*t);
15
y4 = 4*sin(t)-3*sin(2*t)+4/3*sin(3*t)-1/4*sin(4*t);
16
17
plot(x2,y2,x3,y3,x4,y4)
18
grid on
Listing 21.1 A simple procedure to plot the boundary of the linear stability
domains, ∂DBDFq, for the BDF2, BDF3, and BDF4 methods.

22
Galerkin Methods for Initial Value
Problems
So far, to approximate the solution to an initial value problem (IVP), we have
produced a sequence of vectors {w k}N
k=0, meant to approximate the solution at
a ﬁnite collection of instances of time {tk = kτ}N
k=0. In this chapter, we will
see how to use Galerkin techniques to approximate the solution of IVPs. Galerkin
methods are based upon a weak formulation of the problem and have a ﬂavor
diﬀerent from the methods that we have introduced up to now. In a Galerkin
technique, we approximate the spaces of possible solutions and test functions in
the weak formulation of the problem. In particular, our approximate solutions will
have values for all instances of time, not just at the grid points {tk}N
k=0. This
chapter will serve as a starting point for the discussion in subsequent chapters,
especially when we consider the ﬁnite element method in Chapter 25. The methods
we present here can be generalized to approximate solutions to time-dependent
partial diﬀerential equations (PDEs), such as those we will study in Chapter 28.
To make the transition to PDEs simple, we keep the treatment general, working
in a Hilbert H space, rather than in Rd.
22.1
Assumptions and Basic Deﬁnitions
To keep matters suﬃciently general, yet simple enough, we will assume here that
H is a real Hilbert space with inner product (·, ·)H and associated norm ∥· ∥H; see
Deﬁnition 16.2.
Deﬁnition 22.1 (coercivity and monotonicity). Let A: H →H be a (not
necessarily linear) mapping. We say that A is coercive if and only if there is a
constant C1 > 0 such that
C1∥v∥2
H ≤(Av, v)H ,
∀v ∈H.
We say that A is monotone if and only if there is a convex diﬀerentiable functional
E : H →R such that
E′[v](φ) = (Av, φ)H ,
∀v, φ ∈H,
where E′[v] is the Gateaux derivative of E at v ∈H, in the sense of Deﬁnition
16.36.
We recall that, owing to Proposition 16.39, a monotone operator satisﬁes
(Av1 −Av2, v1 −v2)H ≥0,
∀v1, v2 ∈H.
(22.1)

22.1 Assumptions and Basic Deﬁnitions
597
Remark 22.2 (monotonicity). We remark that our deﬁnition of a monotone
operator does not coincide with the one usually adopted in the literature. Usually,
(22.1) is taken as the deﬁnition of monotonicity, and then it is shown that
derivatives of convex functionals are monotone. Since we will only deal with
derivatives of convex functionals here, we will stick with this deﬁnition.
We will assume in this chapter that A is either coercive or monotone. Given this
mapping A, we are interested in approximating the solution to the following IVP:
given u0 ∈H and f ∈C([0, T]; H), ﬁnd u : [0, T] →H satisfying
u′(t) + Au(t) = f (t),
u(0) = u0.
(22.2)
We will assume that this problem always has a unique solution with the regularity
u ∈C1([0, T]; H). This can be proven easily for the case that H is ﬁnite
dimensional; see Chapter 17. The inﬁnite-dimensional case requires techniques that
are beyond the scope of this text.
Notice that we can take the inner product of the equation with an arbitrary
v ∈H to obtain the following.
Deﬁnition 22.3 (weak form). The weak form of the IVP (22.2) is given as follows:
given u0 ∈H and f ∈C([0, T]; H), ﬁnd u : [0, T] →H, satisfying u(0) = u0 ∈H,
and
(u′, v)H + (Au, v)H = (f , v)H ,
∀v ∈H,
∀t ∈[0, T].
(22.3)
The arbitrary function v ∈H in the previous identity is called a test function. A
solution u ∈C1([0, T]; H) to (22.3) is called a weak solution.
We will always assume that a weak solution exists and is unique. Now, owing to
the structure of A, the following a priori estimates can be obtained.
Theorem 22.4 (a priori estimate, coercive case). Suppose that u ∈C1([0, T]; H)
is a solution to (22.3) with u(0) = u0 ∈H. If A is coercive, then, for any t ∈[0, T],
∥u(t)∥2
H + C1
Z t
0
∥u(s)∥2
Hds ≤∥u0∥2
H + 1
C1
Z t
0
∥f (s)∥2
Hds.
(22.4)
Proof. First, observe that, since u ∈C1([0, T]; H),
d
dt ∥u∥2
H = 2 (u′, u)H .
In the weak form, start by setting v = u(t), for every t ∈[0, T]. Using the deﬁnition
of coercivity and the Cauchy–Schwarz and Young inequalities, we have, for every
t ∈[0, T],
1
2
d
dt ∥u∥2
H + C1∥u∥2
H ≤(f , u)H ≤
1
2C1
∥f ∥2
H + C1
2 ∥u∥2
H.
Integrating in time on the interval [0, t] gives the result.

598
Galerkin Methods for Initial Value Problems
t0 = 0
t1
t2
t3
t4
tK = T
Figure 22.1 A typical function in the space S p,−1(τ) for p = 0 (thin lines) and p = 1
(thick lines).
Theorem 22.5 (a priori estimate, monotone case). Suppose that u ∈C1([0, T]; H)
is a solution to (22.3) with u(0) = u0 ∈H. If A is monotone, then, for all t ∈[0, T],
1
2
Z t
0
∥u′(s)∥2
H ds + E(u(t)) ≤E(u0) + 1
2
Z t
0
∥f (s)∥2
H ds.
(22.5)
Proof. This time, set v = u′ in the weak form and use the Cauchy–Schwarz and
Young inequalities to get
∥u′∥2
H + (Au, u′)H = (f , u′)H ≤1
2∥f ∥2
H + 1
2∥u′∥2
H.
Next, notice that
d
dt E(u(t)) = (E′[u], u′)H = (Au, u′)H .
Using the monotonicity of the mapping and integrating over the interval [0, t], we
get the second result.
Remark 22.6 (convexity). Note that we have not yet used the fact that E is
convex. This property will be needed later.
We brieﬂy comment that estimates (22.4) and (22.5) can be used to prove the
existence and uniqueness of solutions in their respective cases. This, however, is
beyond the scope of our discussion here. Instead, we will focus on the development
of methods that preserve or mimic these estimates. We will do so by using Galerkin
techniques.
We begin the discretization procedure by introducing a partition of the interval
[0, T].

22.2 Coercive Operators: The Discontinuous Galerkin Method
599
Deﬁnition 22.7 (partition). A partition of [0, T] of size K ∈N is a set hboxτ =
{t0, . . . , tK} with the property
0 = t0 < t1 < · · · < tK = T.
The sets Ik = (tk−1, tk] for k ∈{1, . . . , K} are called the intervals of the partition.
On the basis of this partition, we introduce a space of discrete functions.
Deﬁnition 22.8 (space S p,−1(τ; H)). Let p ∈N0. The space of H-valued
piecewise polynomials of degree p is deﬁned via
S p,−1(τ; H) =


vτ : [0, T] →H

vτ|Ik(t) =
p
X
j=0
vk,jtj, vk,j ∈H, k = 1, . . . , K


.
(22.6)
Note that the elements of S p,−1(τ; H) are generically discontinuous at the
partition points. That is the reason for the second super-index: we use it to indicate
that functions in this space have smoothness of order minus one. Example functions
from S p,−1(τ) are shown in Figure 22.1 for the piecewise constant (p = 0) and
piecewise linear (p = 1) cases.
22.2
Coercive Operators: The Discontinuous Galerkin Method
We ﬁrst deal with the case of a coercive operator. We begin by observing that no
continuity requirements are imposed on S p,−1(τ; H), but, since the intervals Ik
are semi-open, functions on S p,−1(τ; H) are only left continuous. Owing to this,
we introduce the following notation.
Deﬁnition 22.9 (jump). Let S p,−1(τ; H) be as in (22.6). For vτ ∈S p,−1(τ; H),
and k = 0, . . . , K −1, deﬁne the quantities
v k = vτ(tk),
v k
+ = lim
t↓tk vτ(t),
Jv kK = v k
+ −v k.
The last quantity is known as the jump.
We note the following simple identity.
Proposition 22.10 (tensor product). Suppose, S p,−1(τ; H) is given by (22.6).
Then we have the decomposition
S p,−1(τ; H) =
K
O
k=1
S p,∞(Ik; H),
S p,∞(Ik; H) =


vk : Ik →H

vk(t) =
p
X
j=0
ψk,jtj, ψk,j ∈H


.
Proof. See Problem 22.1.

600
Galerkin Methods for Initial Value Problems
Now, in identity (22.3), set the test function v = v(t) ∈H. Integrating over
the interval [0, tK], we obtain
Z tK
0
(u′, v)H dt +
Z tK
0
(Au, v)H dt =
Z tK
0
(f , v)H dt.
Let us assume now that v(tK) = 0. Integrating by parts, we conclude that
−
Z tK
0
(u, v ′)H dt +
Z tK
0
(Au, v)H dt = (u0, v(0))H +
Z tK
0
(f , v)H dt.
(22.7)
We will use identity (22.7) to derive our numerical method. If we replace the exact
solution by a function uτ ∈S p,−1(τ; H) while keeping v as it is, and integrate
back by parts on each partition interval Ik, we get
−
Z tk
tk−1
(uτ, v ′)H dt =
Z
Ik
(u′
τ, v)H dt −
 uk, v k
+

H +
 uk−1
+
, v k−1
+

H ,
where we used that v is smooth, so v k = v k
+. Adding over k and recalling that
v(tK) = 0, we obtain
Z tK
0
(u′
τ, v)H dt +
K−1
X
k=1
 JukK, v k
+

H +
 u0
+, v 0
+

H +
Z tK
0
(Auτ, v)H dt
=
 u0, v 0
+

H +
Z tK
0
(f , v)H dt.
Finally, we replace in the previous expression the test function by a discrete one,
i.e., vτ ∈S p,−1(τ; H), to obtain the ﬁnal form of our method.
Deﬁnition 22.11 (DG approximation). The discontinuous Galerkin (DG) approx-
imation1 to (22.3) is deﬁned as follows: ﬁnd uτ ∈S p,−1(τ; H) such that,
u0 = u0 and
Z tK
0
(u′
τ, vτ)H dt +
K−1
X
k=1
 JukK, v k
+

H +
 u0
+, v 0
+

H +
Z tK
0
(Auτ, vτ)H dt
=
 u0, v 0
+

H +
Z tK
0
(f , vτ)H dt,
∀vτ ∈S p,−1(τ; H).
(22.8)
Notice that we can set, in (22.8), vτ = vkχIk with vk ∈S p,∞(Ik; H) and χIk
being the characteristic function of the interval Ik, k = 1, . . . , K, to obtain a local
version of our method.
Deﬁnition 22.12 (local version). The local version of the DG approximation to
(22.3) is deﬁned as follows: ﬁnd uτ ∈S p,−1(τ; H) such that u0 = u0 and, for
every k ∈{1, . . . , K} and all vk ∈S p,∞(Ik; H), it holds that
Z
Ik

(u′
τ, vk)H + (Auτ, vk)H

dt +
 Juk−1K, v k−1
k,+

H =
Z
Ik
(f , vk)H dt.
(22.9)
1 Named in honor of the Russian mathematician and engineer Boris Grigorievich Galerkin
(1871–1945).

22.2 Coercive Operators: The Discontinuous Galerkin Method
601
Remark 22.13 (upwinding). Throughout the course of the derivation of the
method, when dealing with the smooth test function v, it may have seemed arbitrary
to use v k
+, as it makes no diﬀerence at this stage. However, this is crucial when
we replace this test function by a discrete one, and it is what will allow us to
obtain stability of our method. This particular choice, against the direction of
time, corresponds to a so-called upwinding method, as the value is taken against
the direction of ﬂow of information. We will learn more about upwinding in Section
24.4.2 and Chapter 29.
22.2.1
Stability
Let us now pause a moment to see what have we gained. First of all, we must notice
that we are working with functions instead of point values, as we did in previous
chapters. Thus, we can repeat many of the arguments for the actual continuous
problem to analyze our method.
Theorem 22.14 (stability). For any partition τ, the function uτ ∈S p,−1(τ; H)
that solves (22.8) satisﬁes
∥uK∥2
H +
K−1
X
k=0
∥JukK∥2
H + C1
Z T
0
∥uτ∥2
H dt ≤∥u0∥2
H + 1
C1
Z T
0
∥f ∥2
H dt.
(22.10)
Proof. Set, in (22.9), vk = 2uτχIk to obtain
∥uk∥2
H−∥uk−1
+
∥2
H+2
Z
Ik
(Auτ, uτ)H dt + 2
 uk−1
+
−uk−1, uk−1
+

H = 2
Z
Ik
(f , uτ)H dt.
The elementary identity
2a(a −b) = a2 −b2 + (a −b)2
then allows us to conclude that
2
 uk−1
+
−uk−1, uk−1
+

H = ∥uk−1
+
∥2
H −∥uk−1∥2
H + ∥Juk−1K∥2
H.
Consequently, using the coercivity property, we have
∥uk∥2
H −∥uk−1∥2
H + C1
Z
Ik
∥uτ∥2
H dt + ∥Juk−1K∥2
H ≤1
C1
Z
Ik
∥f ∥2
H dt;
adding over k, we thus obtain (22.10).
This is the discrete version of the stability estimate (22.4). It is remarkable to
notice that this estimate was also obtained by the same technique as (22.4), i.e.,
choosing the test function to be equal to the solution.
Remark 22.15 (stability). Notice that if we examine estimate (22.10) more closely,
this only gives us control on uτ at the partition points tk. How can we say then
that this is a stability result? If p = 0, then uτ is constant on Ik and, thus, this

602
Galerkin Methods for Initial Value Problems
gives control on the whole function. On the other hand, if p = 1, notice that
we have
uτ|Ik(t) = uk−1
+
tk −t
tk −tk−1
+ uk t −tk−1
tk −tk−1
= uk−1
tk −t
tk −tk−1
+ uk t −tk−1
tk −tk−1
+ JukK
tk −t
tk −tk−1
,
so that
∥uτ|Ik(t)∥H ≤∥uk−1∥H
tk −t
tk −tk−1
+ ∥uk∥H
t −tk−1
tk −tk−1
+ ∥JukK∥H
tk −t
tk −tk−1
,
and it is not only the point value but also the jump that gives us control over the
function over the whole interval Ik.
22.2.2
Convergence
We have obtained then the stability of the method, and Problem 22.2 shows that
it is consistent. As we have seen before, and we will see time and time again, this
implies convergence.
We begin the analysis with the construction of a canonical interpolant for the
space S p,−1(τ; H).
Proposition 22.16 (interpolation). Let p ∈N0 and τ be a partition of [0, T]. For
any v ∈C([0, T]; H), there is a unique ˜vτ ∈S p,−1(τ; H) that satisﬁes, for all
k = 0, . . . , K,
˜v k = v(tk),
Z
Ik
(v −˜vτ)tmdt = 0,
∀m < p.
Moreover, if v is suﬃciently smooth, this unique function ˜vτ satisﬁes
∥v(t) −˜vτ(t)∥H ≤Cτp+1,
∀t ∈Ik.
Proof. See Problem 22.3.
A useful, and important, property of this interpolant is the following.
Lemma 22.17 (orthogonality). Let p ∈N0, τ be a partition of [0, T], and v ∈
C([0, T]; H). For any k ∈{1, . . . , K}, the interpolant ˜vτ, deﬁned in Proposition
22.16, satisﬁes
Z
Ik
(v −˜vτ)φdt = 0,
∀φ ∈S p,∞(Ik; H).
Proof. See Problem 22.4.
We are now in a position to show convergence of our method. Although it is
not necessary, to simplify the presentation, we will assume that our partition τ is
uniform. In other words, all the points are equidistant, so that
tk = kτ,
τ = T/K.
One ﬁnal, simplifying, set of assumptions we will make are that the operator A ∈
B(H), see Deﬁnition 16.18, and self-adjoint in the sense of Deﬁnition 1.17.

22.2 Coercive Operators: The Discontinuous Galerkin Method
603
Theorem 22.18 (convergence I). Assume that u, the solution of (22.2), is such
that u ∈Cp+1([0, T]; H). Let p ∈N0, K ∈N, and τ be a uniform partition of
[0, T] of size τ = T/K. Let uτ ∈S p,−1(τ; H) be the solution to (22.8). There is
a constant, C, that depends only on u such that
max
t∈τ ∥u(t) −uτ(t)∥H ≤Cτp+1.
Proof. Let ˜uτ ∈S p,−1(τ; H) be the interpolant of u, deﬁned via Proposition
22.16. We now decompose the error e = u−uτ into e = (u−˜uτ)+(˜uτ −uτ) = ρ+θ,
where θ ∈S p,−1(τ; H), and, by Lemma 22.17, we have that
ρ(tk) = 0,
∀k = 0, . . . , K,
∥ρ(t)∥H ≤Cτp+1.
It remains then to handle θ. As will happen many times in our discussion, the
strategy is to ﬁnd an equation for this function. Notice that we can integrate
(22.3) over Ik and set v ∈S p,∞(Ik; H) to obtain
Z
Ik
[(u′, v)H + (Au, v)H]dt =
Z
Ik
(f , v)H dt;
we then subtract from this the local version of our method (22.9) to obtain
Z
Ik
[(θ′, v)H + (Aθ, v)H]dt +
 Jθk−1K, v k−1
+

H
= −
Z
Ik
[(ρ′, v)H + (Aρ, v)H]dt −
 Jρk−1K, v k−1
+

H .
Notice that this previous identity is exactly the method. The only diﬀerence lies in
what the right-hand side is. For this reason, we can just apply the stability estimate
(22.10) to obtain
∥θk∥2
H −∥θk−1∥2
H + ∥Jθk−1K∥2
H + 2
Z
Ik
(Aθ, θ)H dt
= −2
Z
Ik
[(ρ′, θ)H + (Aρ, θ)H]dt −2
 Jρk−1K, θk−1
+

H .
However, for every v ∈S p,∞(Ik; H), we have
Z
Ik
(ρ′, v)H dt +
 Jρk−1K, v k−1
+

H
= −
Z
Ik
(ρ, v ′)H dt +
 ρk, v k
H −
 ρk−1
+
, v k−1
+

H
+
 ρk−1
+
, v k−1
+

H −
 ρk−1, v k−1
+

H
= 0,
where we used that ρk = ρ(tk) = 0 and the fact that v ′ is a polynomial of degree
at most p −1 with coeﬃcients in H. Notice also that the coercivity of A implies
that
0 ≤(A(ρ −θ), ρ −θ)H ;
combining this with the fact that A is self-adjoint, we get

604
Galerkin Methods for Initial Value Problems
2
Z
Ik
(Aρ, θ)H dt ≤
Z
Ik
(Aρ, ρ)H dt +
Z
Ik
(Aθ, θ)H dt ≤Cτ2(p+1)+1+
Z
Ik
(Aθ, θ)H dt,
where we used that the operator A is bounded and that ∥ρ(t)∥H ≤Cτp+1 and is
being integrated over an interval of length τ.
Gathering all the previous estimates, we get
∥θk∥2
H −∥θk−1∥2
H + ∥Jθk−1K∥2
H +
Z
Ik
(Aθ, θ)H dt ≤Cτ2(p+1)+1;
summing over k, we then get
∥θK∥2
H +
K−1
X
k=0
∥JθkK∥2
H +
Z T
0
(Aθ, θ)H dt ≤Cτ2(p+1)τ
K
X
k=1
= CTτ2(p+1),
and this implies the result.
The previous result only gives convergence at the partition points. The next
result, however, shows that this is enough to obtain convergence at every point
in [0, T]. Its proof relies on the fact that, much as in the real (or complex) case,
for every p ∈N and all Ik, we can deﬁne a least squares polynomial approximation
P1,p[u] ∈S p,∞(Ik; H). This deﬁnes the mapping
Πτ[u] ∈S p,−1(τ; H),
Πτ[u]|Ik = P1,p[u],
which satisﬁes
max
t∈[0,T ] ∥u(t) −Πτu(t)∥H + τ max
t∈[0,T ] ∥(u −Πτu)′(t)∥≤cτp+1 max
t∈[0,T ] ∥u(p+1)(t)∥H,
whenever u is suﬃciently smooth.
Theorem 22.19 (convergence II). Assume that u, the solution of (22.2), is such
that u ∈Cp+1([0, T]; H). Let p ∈N0, K ∈N, and τ be a uniform partition of
[0, T] of size τ = T/K. Let uτ ∈S p,−1(τ; H) be the solution to (22.8). There
are constants C1 and C2 such that, for every k = 1, . . . , K, we have that
max
t∈Ik ∥u(t) −uτ(t)∥H ≤∥uk −u(tk)∥H + C1∥uk−1 −u(tk−1)∥H + C2τp+1.
Example 22.1
Let us consider the simplest case, namely p = 0 and H = R. In
this case, u′
τ|Ik = 0 and
uτ|Ik = uk = uk−1
+
.
In this case, by setting v = 1, the local version (22.9) of our method reduces to
uk −uk−1 + τAuk =
Z
Ik
f (t)dt
or
uk = uk−1 + τ
 1
τ
Z
Ik
f (t)dt −Auk

.

22.3 Monotone Operators: The Continuous Petrov–Galerkin Method
605
This is nothing but a variant of the implicit Euler method that we considered in
Chapter 18, in which we replace f (tk) by its average over Ik.
Example 22.2
For p ≥1, possibly after quadrature, we arrive at particular classes
of RK methods; see Problem 22.5.
22.3
Monotone Operators: The Continuous Petrov–Galerkin Method
There is something troubling in the constructions of Section 22.2. Namely, if we
assume that u is a mild solution to (22.2), then we must at least have that
u ∈C([0, T]; H). However, the approximate solution uτ, deﬁned via (22.8), is not
continuous in time. To retain this continuity property, we must consider a Petrov–
Galerkin2 method, i.e., one where the test function does not necessarily lie in the
space where we seek the solution. Essentially, we will consider an approximation of
(22.3) but when the solution and test function lie in diﬀerent spaces. To implement
this idea, we keep the notation of the previous section and deﬁne another space of
polynomials.
Deﬁnition 22.20 (space S p,0(τ; H)). Let p ∈N. The space of continuous
H-valued piecewise polynomials of degree p is deﬁned via
S p,0(τ; H) = S p,−1(τ; H) ∩C([0, T]; H).
(22.11)
Remark 22.21 (splines). As we mentioned above, the second super-index, in the
deﬁnition of S p,0(τ; H), indicates the smoothness of functions in this space with
the zero here indicating continuity. We can similarly deﬁne, for r ∈N,
S p,r(τ; H) = S p,−1(τ; H) ∩Cr([0, T]; H).
Spaces of functions like this one are usually called splines in the literature.
In the continuous Galerkin method, we will now seek uτ ∈S p,0(τ; H). Before
even deﬁning the method, notice that continuity implies that, for all k = 0, . . . , K,
we have uτ(tk) = uk and so, once we have reached Ik, we only have p conditions
left to fully deﬁne uτ on Ik. For this reason, we will test the equation with functions
from S p−1,−1(τ; H). This means that we have the following method.
Deﬁnition 22.22 (CG approximation). The continuous Galerkin (CG) approxi-
mation to (22.3) is deﬁned as follows: ﬁnd uτ ∈S p,0(τ; H) such that u0 = u0
and, for all vτ ∈S p−1,−1(τ; H),
Z T
0

(u′
τ, vτ)H + (Auτ, vτ)H

dt =
Z T
0
(f , vτ)H dt.
(22.12)
2 Named in honor of the Soviet engineer Georgiy Ivanovich Petrov (1912–1987) and the
Russian mathematician and engineer Boris Grigorievich Galerkin (1871–1945).

606
Galerkin Methods for Initial Value Problems
22.3.1
Stability
We now study the stability of (22.12). The following result provides a discrete
analogue of (22.5).
Theorem 22.23 (stability). For any partition τ, the function uτ ∈S p,0(τ; H)
that solves (22.12) satisﬁes
1
2
Z
Ik
∥u′
τ∥2
Hdt + E(uτ(tk)) ≤1
2
Z
Ik
∥f ∥2
Hdt + E(uτ(tk−1)).
Proof. As we did in the continuous case, we will proceed by suitably choosing a
test function vτ. Namely, we observe that vτ = u′
τχIk ∈S p−1,−1(τ; H) and so it
is an admissible function for the method (22.12). This will yield
Z
Ik
∥u′
τ∥2
Hdt +
Z
Ik
(Auτ, u′
τ)H dt =
Z
Ik
(f , u′
τ)H dt ≤1
2
Z
Ik
∥f ∥2
Hdt + 1
2
Z
Ik
∥u′
τ∥2
Hdt;
if A is monotone, this implies that
1
2
Z
Ik
∥u′
τ∥2
Hdt + E(uτ(tk)) ≤1
2
Z
Ik
∥f ∥2
Hdt + E(uτ(tk−1)),
as we intended to show.
We will not carry out a convergence analysis of this method. Instead, we will try
to elucidate the form of this method in some simple cases.
Example 22.3
Let H = R. Notice that we must necessarily then have 0 < A ∈R,
so that the convex functional is
E(u) = 1
2Au2.
In addition, since we must require continuity, we must have p ≥1. Let us set
then p = 1 and τ = T/K. Observe then that elements of vτ ∈S 1,0(τ; R) are
piecewise linear functions, and those of vτ ∈S 0,−1(τ; R) piecewise constant ones.
Consequently,
u′
τ = 1
τ (uk −uk−1).
Setting vτ = 1 in (22.12), we then get
uk −uk−1 + A
Z
Ik
uτdt =
Z
Ik
f (t)dt.
In addition, since uτ is linear, we can apply the midpoint rule to get
Z
Ik
uτ dt = 1
2(uk + uk−1)τ.
In summary, we obtained the method
uk = uk−1 + τ
 1
τ
Z
Ik
f (t)dt −1
2A
 uk + uk−1
.

Problems
607
This can be thought of as a variant of either the trapezoidal or midpoint methods,
where the average of f can be approximated via
1
τ
Z
Ik
f (t)dt ≈1
2 [f (tk) + f (tk−1)]
or
1
τ
Z
Ik
f (t)dt ≈f

tk + 1
2τ

,
respectively.
Problems
22.1
Prove Proposition 22.10.
22.2
Replace, in (22.9), the Galerkin approximation uτ by the exact solution u
and obtain an identity that expresses the consistency of our method.
22.3
Prove Proposition 22.16.
22.4
Prove Lemma 22.17.
22.5
Show that, in the case that H = R, A is linear, f ≡0, and p = 1, the DG
method (22.8) satisﬁes
uk = g2/1(τA)uk−1,
where g2/1 is the Pad´e approximation of the exponential introduced in Deﬁnition
21.13.


Part V
Boundary and Initial
Boundary Value Problems


23
Boundary and Initial Boundary
Value Problems for Partial
Diﬀerential Equations
In Chapter 17, we presented some facts about the theory of initial value problems
(IVPs) for ordinary diﬀerential equations (ODEs). Many, but very far from all, ODE
problems can ﬁt into that framework. Consider the following two problems:
ay ′′(t) + by ′(t) + cy(t) = 0, t ∈(0, 1),
y(0) = 0, y ′(0) = 0,
and
ay ′′(t) + by ′(t) + cy(t) = 0, t ∈(0, 1),
y(0) = 0, y(1) = 0,
with 0 ̸= a, b, c ∈R. The ODEs in these two problems are exactly the same;
they are linear, constant-coeﬃcient, second-order equations. The ODE can be
transformed into a system of ﬁrst-order equations for the dependent variable
u = [y, y ′]⊺. The ﬁrst problem is an IVP, and it has the initial condition u(0) =
[0, 0]⊺. However, the second problem is fundamentally diﬀerent. It is not an IVP
because it has boundary conditions, i.e., conditions on the dependent variable at
the distinct points t = 0 and t = 1. This fact prevents us from recasting this
problem as an IVP.
To make matters worse, many equations derived from physics, chemistry,
economics, etc. involve rates of change with respect to more than one independent
variable. Such equations are called partial diﬀerential equations (PDEs). As one
might expect, the theory can be much more complicated for PDEs than for ODEs.
PDEs can have complicated boundary conditions, and some require initial conditions
as well.
The theory of PDEs is a very deep and rich subject, one that we cannot fully
cover here. Our purpose will be to present a few simple, yet prototypical, example
problems, and some of the most relevant theory, especially that which will be
important for numerical approximation. There are several extremely good references
for these subjects, at various levels of depth. Good starting references are [31, 49,
72, 87, 99]. We must reiterate that, since this is an overview, we will often state
our results under severely restrictive assumptions.
23.1
Heuristic Derivation of the Common Partial
Diﬀerential Equations
Before we get into the theory, it is useful to see how some of the most common
PDEs are derived.

612
Review of PDE Theory
23.1.1
Conservation Laws and the Conservation Equation
To get started, let us imagine a familiar phenomenon. Suppose a drop of dye falls
upon the surface of a ﬂowing stream. How would we model the motion of the
dye particles? Of course, the drop is advected by the motion of the stream, but it
simultaneously spreads out, until, eventually, the drop is so diﬀused that it is no
longer discernible.
Suppose that Ω⊂R3 is a bounded, open subset of three-dimensional space,
and T > 0 is a ﬁnal time. Let φ: Ω× [0, T] →R be a density of dye particles.
We will assume that the particle density φ is at least continuous, which is called
the continuum assumption. Of course, in reality, this assumption eventually breaks
down if we zoom in around any point x ∈Ωwith a suﬃciently large magniﬁcation,
because then we would see the individual particles. The units of φ are
[φ] = number
volume .
Let us suppose that there is background ﬂow of some ﬂuid medium, by which
the dye particles are advected, described by the time-independent velocity ﬁeld
u : Ω→R3. The units of the velocity vector ﬁeld are
[u] = length
time .
Oftentimes, we can and will assume that this ﬂow ﬁeld is solenoidal, or divergence
free, meaning that
∇· u(x) =
3
X
i=1
∂ui
∂xi
(x) = 0,
∀x ∈Ω.
This is a good approximation when the ﬂuid is nearly incompressible, as for the
case of water. The diﬀerential operator just deﬁned, ∇·, acts upon diﬀerentiable
vector-valued functions and is called the divergence operator. There is a companion
diﬀerential operator that we will use frequently. The gradient operator acts upon
diﬀerentiable scalar functions and is deﬁned as
∇φ(x) =
3
X
i=1
∂φ
∂xi
(x)ei,
where ei is the canonical unit vector.
Let V ⊂Ωbe a stationary control volume in Ω. The total number of dye particles
in V is given by the formula
NV (t) =
Z
V
φ(x, t)dx.
Now we can ponder by what mechanisms the number of particles in V changes
with time. When we consider all of the ways that particles are created, destroyed,
transported in, and/or transported out of V , we arrive at
dNV
dt (t) =
Z
V
∂φ
∂t (x, t)dx =
Z
∂V
−K · nV dS +
Z
V
S(φ(x, t), x, t)dx,

23.1 Heuristic Derivation of the Common Partial Diﬀerential Equations
613
where ∂V is the bounding surface of V (its boundary); nV is its outward-pointing
unit normal vector; K is a vector-valued total ﬂux; and S is a volumetric source
term of particles. The term
R
∂V −K · nV dS accounts for the time rate of change
of particles via a ﬂux of particles across the boundary of the control volume. The
term
R
V S(φ(x, t), x, t)dx accounts for the time rate of change of particles via a
volumetric source term inside of V , as from a chemical reaction, or a spontaneous
decay. In the case of our thought experiment with the dye in a stream, S would
likely be identically zero.
The Divergence Theorem, which follows from Theorem B.59, states that
Z
V
∇· F(x)dx =
Z
∂V
F(x) · nV dS
(23.1)
for all functions F ∈C1(V ; R3) ∩C(V ; R3), provided that the boundary of V is
suﬃciently regular. Using the Divergence Theorem, we ﬁnd
Z
V
∂φ
∂t (x, t) + ∇· K −S(φ(x, t), x, t)

dx = 0.
Now we argue that, since this result holds for any arbitrary control volume V , it
must be that
∂φ
∂t (x, t) + ∇· K −S(φ(x, t), x, t) = 0,
∀x ∈Ω,
∀t ∈[0, T].
Let us now consider the total ﬂux, K, in more detail. It is a vector-valued function,
as we have indicated, and its units are
[K] =
number
area · time.
Since there is a background ﬂow ﬁeld u, it is common to decompose K as follows:
K = φu(x) + J(x, φ, ∇φ),
where J is called the diﬀusive ﬂux. The ﬁrst term is called the linear advection ﬂux;
and observe that it has the correct units to be a ﬂux:
[φu] =
number
area · time.
In the case of our thought experiment with the dye in a stream, this term accounts
for the motion of the drop as it is carried by the background ﬂow. The second term
in the decomposition, J(x, φ, ∇φ), accounts for the random diﬀusive motion of the
particles, as, for example, via Brownian motion.1 Thus, we arrive at the general
conservation equation: for all x ∈Ωand all t ∈[0, T],
∂φ
∂t (x, t) + ∇· (φ(x, t)u(x)) = −∇· J(x, φ, ∇φ) + S(φ, x, t).
In our present example, this equation followed from the universal law of mass
conservation. But it applies to any conserved quantity, like energy or electric charge;
1 Named in honor of the British botanist and paleobotanist Robert Brown (1773–1858).

614
Review of PDE Theory
see Table 23.1. To construct a usable equation, we need to make some constitutive
assumptions about the form of the diﬀusion ﬂux, J, and the source term.
In general, the diﬀusion ﬂux J and the conserved quantity φ are related by a
constitutive law of the form
J(x) = −A(x, J(x))Ψ(φ, ∇φ),
which encodes information about the object in question and the physics that are
taking place in it. It is said that the process is physically linear if
A = A(x),
and geometrically linear if Ψ = ∇φ. In summary, the constitutive law that describes
the behavior of a linear process is
J = −A(x)∇φ.
In this setting, A is usually called the material modulus; see Table 23.1 for its
interpretation in several physical scenarios.
23.1.2
Advection–Reaction–Diﬀusion Equation
As mentioned above, we must pay particularly close attention to the physics of the
problem. In the mass conservation case, motivated by our thought experiment with
the dye particles, when diﬀusion is dominated by random, Brownian-like motion,
the standard model is Fick’s Law, which states that
J(x, φ, ∇φ) = −D(x, φ(x, t))∇φ(x, t),
where D: R3 × R →R3×3 is a diﬀusion coeﬃcient matrix. This law implies that
particles move, on average, from regions where their density is high to regions
where their density is low. Intuitively, this makes sense.
Now we will typically assume that D is symmetric positive deﬁnite (SPD) at
every point at which it is deﬁned. One very common and simple model is
D = a(x)I,
where I is the 3 × 3 identity matrix and a(x) > 0 for all x ∈Ω. Now, regarding the
source term, it is common to choose the linear model
S = −c(x)φ(x, t) + f (x, t),
where f is a density-independent source of particles. When c(x) > 0, the term
−c(x)φ(x, t) describes the decay of the particles, via degradation or evaporation,
say. When c(x) < 0, the term −c(x)φ(x, t) describes the growth of particles,
due, for example, to reproduction or a chemical reaction. The units of the source
term are
[S] =
number
volume · time,
which implies that c must have units
[c] =
1
time.

Problem
Conserved quantity
φ
J
a
S
Law
Deformation of an elastic rod
linear momentum
displacement
stress
Young’s modulus
body forces
Hookea
Flow of particles in a material
mass
number density
mass ﬂux
mobility
mass sources
Fickb
Heat conduction in a rod
energy
temperature
heat ﬂux
thermal conductivity
heat sources
Fourierc
Fluid ﬂow through a channel
linear momentum
velocity
shear stress
viscosity
body forces
Stokesd
Electrostatics
electric ﬂux
electric potential
electric ﬂux
dielectric permittivity
charge
Coulombe
Flow through porous medium
mass
hydraulic head
ﬂow rate
permeability
ﬂuid source
Darcyf
Table 23.1 Interpretation of the variables of (23.2) for various types of physical problems. The last column gives the name of the constitutive
law in that particular context.
a Named in honor of the English scientist, architect, and polymath Robert Hooke (1635–1703).
b Named in honor of the German physician and physiologist Adolf Eugen Fick (1829–1901).
c Named in honor of the French mathematician and physicist Jean-Baptiste Joseph Fourier (1768–1830).
d Named in honor of the Anglo-Irish physicist and mathematician Sir George Gabriel Stokes, 1st Baronet (1819–1903).
e Named in honor of the French military engineer and physicist Charles-Augustin de Coulomb (1736–1806).
f Named in honor of the French engineer Henry Philibert Gaspard Darcy (1803–1858).
https://doi.org/10.1017/9781108942607.030 Published online by Cambridge University Press

616
Review of PDE Theory
Gathering all of our assumptions, we have, for all x ∈Ωand t ∈[0, T],
∂φ
∂t (x, t) + ∇· (φ(x, t)u(x)) = ∇· (a(x)∇φ(x, t)) −c(x)φ(x, t) + f (x, t).
But observe that
∇· (φu) = u · ∇φ + φ(∇· u).
Thus, if the ﬂow ﬁeld u is solenoidal,
∂φ
∂t (x, t) + u(x) · ∇φ(x, t) = ∇· (a(x)∇φ(x, t)) −c(x)φ(x, t) + f (x, t). (23.2)
This speciﬁc conservation equation is called the linear advection–reaction–diﬀusion
equation. When u ≡0, it is called the linear reaction–diﬀusion equation; and, when
c ≡0, it is called the linear advection–diﬀusion equation. Perhaps the most well-
known variant is the diﬀusion equation, which is obtained by setting u ≡0, c ≡0,
and f ≡0:
∂φ
∂t (x, t) = ∇· (a(x)∇φ(x, t)).
(23.3)
To form a well-deﬁned problem, i.e., one that has a unique solution, we need to
specify initial and boundary conditions. An initial condition is one of the form
φ(x, 0) = q(x),
∀x ∈Ω.
Boundary conditions can come in a few diﬀerent varieties. Suppose that n is the
outward-pointing unit normal vector on the boundary of Ω, denoted by ∂Ω. We will
tacitly assume that ∂Ωis suﬃciently smooth (nice), so that n is well deﬁned and
single valued, except possibly at a set of surface area zero. We deﬁne the so-called
boundary normal derivative as
∂φ
∂n (x, t) = n(x) · ∇φ(x, t),
∀x ∈∂Ω,
∀t ∈(0, T].
There are three common types of boundary conditions for an advection–reaction–
diﬀusion problem. The ﬁrst is called a Neumann boundary condition2 (or a natural
boundary condition), which is deﬁned as
∂φ
∂n (x, t) = α(x, t),
∀x ∈∂Ω,
∀t ∈(0, T],
where α: ∂Ω× (0, T] →R is a given function. The second is called a Dirichlet
boundary condition3 (or an essential boundary condition), which is deﬁned as
φ(x, t) = β(x),
∀x ∈∂Ω,
∀t ∈(0, T],
where β : ∂Ω×(0, T] →R is a given function. The third is called a Robin boundary
condition,4 which is a kind of combination of the previous two:
∂φ
∂n (x, t) + γ(x)φ(x, t) = χ(x, t),
∀x ∈∂Ω,
∀t ∈(0, T],
2 Named in honor of the German mathematician Carl Gottfried Neumann (1832–1925).
3 Named in honor of the German mathematician Johann Peter Gustav Lejeune Dirichlet
(1805–1859).
4 Named in honor of the French mathematician Victor Gustave Robin (1855–1897).

23.1 Heuristic Derivation of the Common Partial Diﬀerential Equations
617
where γ, χ: ∂Ω× (0, T] →R are given functions.
Boundary conditions can be mixed. For example, the boundary can be split into
two or more pairwise disjoint regions, on which diﬀerent boundary conditions are
imposed. Which boundary condition is required where, and when, is a matter of
the underlying physics (or characteristics) of the problem, as the following example
suggests.
Example 23.1
Suppose that u ≡0 and S ≡0. Then the motion of the dye
particles in our thought experiment is only by Fickian diﬀusion, and the density of
particles satisﬁes the diﬀusion equation,
∂φ
∂t (x, t) = ∇· (a(x)∇φ(x, t)) ,
∀x ∈Ω,
∀t ∈(0, T].
(23.4)
Suppose that Ωdescribes the spatial location of a closed container of ﬂuid at rest
from the macroscopic point of view. The function a: Ω→R measures some known
heterogeneity of the ﬂuid that aﬀects the diﬀusion of particles. The initial condition
φ(x, 0) = q(x),
∀x ∈Ω
describes the initial concentration of the dye particles. Perhaps the particles are
highly concentrated in some location initially because we have carefully inserted
the drop via a syringe. Of course, after the initial preparation of the dye particles,
they diﬀuse throughout the background ﬂuid. But, importantly, there can be no
ﬂux of particles through the boundary ∂Ω, since the container is closed. These
no-ﬂux boundary conditions are described via
−a(x)∇φ(x, t) · n(x) = J · n(x) = 0,
∀x ∈∂Ω,
∀t ∈(0, T].
Since a(x) > 0, for all x ∈Ω, the boundary condition must be of Neumann type:
∂φ
∂n (x, t) = 0,
∀x ∈∂Ω,
∀t ∈(0, T].
We must reiterate that equation (23.2) was derived as a mass conservation equa-
tion, but it is much more general than that. It applies for any conserved quantity,
mass, momentum, charge, energy, etc. Table 23.1 describes the interpretation of
φ, J, a, and S for various types of physical problems.
23.1.3
The Linear Transport Equation
Suppose again that u is solenoidal, i.e., ∇· u ≡0. Let us assume that random
diﬀusion may be neglected, i.e., J ≡0. In this case, passive advection via the
background ﬂow is the only means by which the particles are transported from one
spatial location to another. Finally, let us assume that S ≡0, which is appropriate

618
Review of PDE Theory
when there are no reactions to produce or degrade the particles, no evaporation,
and no creation or destruction of particles via any other processes. Under these
assumptions, (23.2) becomes
∂φ
∂t (x, t) + u(x) · ∇φ(x, t) = 0,
(23.5)
which is known as the linear transport equation. Formally, we can write down a
very simple solution to this equation; namely,
φ(x, t) = q(x −u(x)t),
where φ( · , 0) = q is the initial condition. Observe that (23.5) is satisﬁed point-
wise using our proposed solution, provided that q is diﬀerentiable. One can prove
this using the Chain Rule in multiple dimensions.
Let us now consider what type of boundary conditions are meaningful in the
present case. Unlike the case of the previous section, where diﬀusion could not be
neglected, we do not need boundary conditions on all parts of the boundary. In
fact, let us divide ∂Ωinto two disjoint parts,
Γinﬂow = {x ∈∂Ω| n(x) · u(x) < 0} ,
Γoutﬂow = {x ∈∂Ω| n(x) · u(x) ≥0} ,
called the inﬂow and outﬂow boundaries, respectively. To fully deﬁne the linear
transport problem, we need an initial condition, i.e.,
φ(x, 0) = q(x),
∀x ∈Ω,
and we need to specify values for φ only on the inﬂow boundary,
φ(x, t) = r(x, t),
∀x ∈Γinﬂow,
∀t ∈(0, T].
These are called inﬂow boundary conditions.
23.1.4
Stationary Conservation Equations and Boundary Value Problems
Let us assume that, in (23.2),
lim
t→∞
∂f
∂t ( · , t) ≡0.
In other words, eventually, the source term f becomes independent of time. In this
case, it is possible — depending upon a few other factors, including what boundary
conditions are imposed — that the density of particles reaches a steady state at
very large times. This means, speciﬁcally, that
lim
t→∞
∂φ
∂t ( · , t) ≡0.
In this case, the density of particles is essentially time invariant after enough time
has elapsed. This does not mean that the density at steady state is constant;
indeed, it can, and usually will, still have spatial variation. In any case, at steady
state, we have the equation
−∇· (a(x)∇φ∞(x)) + u(x) · ∇φ∞(x) + c(x)φ∞(x) = f∞(x),
(23.6)

23.1 Heuristic Derivation of the Common Partial Diﬀerential Equations
619
where
φ∞(x) = lim
t→∞φ(x, t),
f∞(x) = lim
t→∞f (x, t),
∀x ∈Ω,
assuming that these limits make sense. Dropping the subscripts, we obtain
−∇· (a(x)∇φ(x)) + u(x) · ∇φ(x) + c(x)φ(x) = f (x),
(23.7)
which is the stationary advection–reaction–diﬀusion equation. In the case that
u ≡0, we obtain what is called the stationary reaction–diﬀusion equation
−∇· (a(x)∇φ(x, t)) + c(x)φ(x) = f (x).
(23.8)
When, additionally, c ≡0, we obtain the stationary diﬀusion equation
−∇· (a(x)∇φ(x)) = f (x).
(23.9)
The last equation, in the particular case that a ≡1, becomes the Poisson equation:5
−∆φ(x) = f (x),
(23.10)
where we used the so-called Laplacian,6 which, for a scalar-valued function w that
is twice diﬀerentiable at the point x ∈Rd, is deﬁned as:
∆w(x) = ∇· (∇w(x)) =
d
X
i=1
∂2w
∂x2
i
(x).
To solve any of the stationary problems represented by (23.7)–(23.9), we do
not need initial conditions. We do, however, need boundary conditions of one of
the three types mentioned earlier: Neumann, Dirichlet, or Robin. The combination
of (23.7) with appropriate boundary conditions is called a boundary value problem
(BVP).
23.1.5
The Heat Equation
Let us derive another famous equation, the heat equation. We will see that it is
related to an equation that we have already derived. Consider the internal energy
of a solid material, like a block of copper, that occupies the region Ω:
E =
Z
Ω
e(θ(x), ρ(x))dx,
where e is the internal energy density, θ: Ω→R is the temperature ﬁeld, and
ρ: Ω→R is the mass density. The units are as follows:
[e] = energy
volume,
[θ] = degrees,
[ρ] =
mass
volume.
5 Named in honor of the French mathematician, engineer, and physicist Baron Sim´eon Denis
Poisson (1781–1840).
6 Named in honor of the French scholar and polymath Pierre-Simon, Marquis de Laplace
(1749–1827).

620
Review of PDE Theory
Incidentally, in SI units, energy is measured in joules, length is measured in meters,
and temperature is measured in kelvin. One common, and quite simple, model for
the internal energy is given by
e(θ, ρ) = Cρθ,
where C > 0 is the speciﬁc heat capacity, which we assume is constant. It has units
[C] =
energy
mass · degrees.
Suppose that our material is insulated from the outside world. This means that
no energy can pass to or from Ω. The internal energy must be conserved. Let
V ⊂Ωbe an arbitrary control volume. Deﬁne
EV (t) =
Z
V
e(θ(x, t), ρ(x, t))dx.
Then
dEV
dt (t) =
Z
V
∂
∂t e(θ(x, t), ρ(x, t))dx
=
Z
V

C ∂ρ
∂t (x, t)θ(x, t) + Cρ(x, t)∂θ
∂t (x, t)

dx.
To simplify the situation, let us assume that the change in the mass density with
respect to time is negligible, i.e.,
∂ρ
∂t ( · , t) ≡0.
Similar to the mass conservation case, we assume that energy can move into the
control volume V through a diﬀusive ﬂux J. However, to keep the discussion simple,
we assume that there are no energy sources within V . And, of course, since the
material is solid, there is no background ﬂow. Thus,
dEV
dt (t) = −
Z
∂V
nV · J(x)dS = −
Z
V
∇· J dx,
where we have used the Divergence Theorem (23.1). Consequently,
Z
V

Cρ(x)∂θ
∂t (x, t) + ∇· J

dx = 0.
Since the control volume is arbitrary,
Cρ(x)∂θ
∂t (x, t) = −∇· J,
∀x ∈Ω,
∀t ∈(0, T],
where T > 0 is the ﬁnal time (the end of our thought experiment). To ﬁnish the
derivation of the heat equation, we invoke Fourier’s Law, which states
J = −a(x)∇θ(x, t).
Consistent with the Second Law of Thermodynamics, heat energy ﬂows, or ﬂuxes,
from hot regions into cold regions, and is proportional to ∇θ. Recall, from your
vector calculus class, that the gradient of θ points in the direction of the greatest

23.1 Heuristic Derivation of the Common Partial Diﬀerential Equations
621
increase of θ. The coeﬃcient a: Ω→R is positive and point-wise, and is called
the diﬀusion coeﬃcient. Thus, we have
Cρ(x)∂θ
∂t (x, t) = ∇· (a(x)∇θ),
(23.11)
which is called the heat equation.
Let us simplify our model a bit further. Suppose that, to a good approximation,
ρ ≡ρo, where ρo > 0 is a constant, and a ≡ao, where ao > 0 is a constant. Then
∂θ
∂t (x, t) = D∆θ(x, t),
(23.12)
where ∆is the Laplacian operator and D =
ao
Cρo > 0 is the diﬀusion constant,
which has units
[D] = area
time.
Regarding boundary conditions, recall that we assumed that the material was
thermally insulated. Mathematically, this means that there can be no ﬂux of thermal
energy across ∂Ω:
−a(x)∂θ
∂n(x, t) = n · J = 0,
x ∈∂Ω,
∀t ∈(0, T].
This implies that the appropriate boundary condition is one of Neumann type:
∂θ
∂n(x, t) = 0,
∀x ∈∂Ω,
∀t ∈(0, T].
To close the system, we only need to add an initial condition; namely, the initial
temperature proﬁle of the material in Ω.
It is not diﬃcult to show that energy is conserved in our system with insulating
boundary conditions. Indeed, using the Divergence Theorem, given in (23.1), we
obtain
dEΩ
dt
= d
dt
Z
Ω
e(θ, ρ)dx
=
Z
Ω
∂e
∂θ
∂θ
∂t (x, t) + ∂e
∂ρ
∂ρ
∂t (x, t)

dx
=
Z
Ω
Cρ(x)∂θ
∂t (x, t)dx
=
Z
Ω
Cρo
ao
Cρo
∆θ(x, t)dx
=
Z
∂Ω
aon(x) · ∇θ(x, t)da
= 0,
where we have used the homogeneous Neumann boundary condition in the last
step.
Interestingly, while the energy of the system is conserved, the entropy of the
system, which measures the amount of disorder in the system, increases until the

622
Review of PDE Theory
steady state, or equilibrium, is reached. This makes sense intuitively. Consider the
following example.
Example 23.2
Consider a long, cylindrical rod of solid material of constant cross-
sectional area A, with diﬀusion constant D = 1. (We will suppress units to make
the calculations simpler.) Suppose that it is oriented along the x1 axis. In this
thought experiment, we heat up the end of the rod at x1 = 10 to the temperature
θR = 200, and we cool the rod at the other end, x1 = −10, to the temperature
θL = 0. Assume, for simplicity, that the initial temperature proﬁle in the rod is
θ(x, 0) = 100 + 100 sin
πx1
20

.
In other words, we assume that the initial proﬁle depends only upon the long axis
position, x1. Now, after heating, let us quickly insulate the rod on all sides.
To further simplify the problem, let us assume that the variation of the
temperature with respect to the coordinates x2 and x3 can always be neglected.
Thus, the equation that we need to solve is
∂θ
∂t (x1, t) = ∂2θ
∂x2
1
(x1, t),
subject to the boundary conditions
∂θ
∂x (x1, t) = 0,
x1 = ±10.
This problem has an exact solution; namely,
θ(x1, t) = 100 + 100 exp

−π2t
400

sin
πx1
20

.
Thus, if we heat up one end of a cylindrical solid block of material, cool the other
end, and then quickly insulate the entire system, heat energy, which is conserved in
the system, ﬂows from the hotter to the colder side, until a constant temperature
is reached in the system. In fact, the reader can check that
lim
t→∞θ(x1, t) = 100,
∀x1 ∈[−10, 10].
If we wait long enough, we will, to a high degree of accuracy, observe this steady
state, which can also be termed the equilibrium state, in this case. When equilibrium
is reached, we cannot tell which end of the system was heated and which was
cooled. Information has been forever lost, and disorder (entropy) has increased to
its maximum possible value.
23.1.6
The Wave Equation
The last equation that we will consider is the wave equation. Consider an initially ﬂat
elastic sheet of material that occupies the domain Ω⊂R2. Let u : Ω× [0, T] →

23.1 Heuristic Derivation of the Common Partial Diﬀerential Equations
623
R describe the displacement of the sheet in the vertical direction (the direction
perpendicular to the initially ﬂat sheet). The units are
[u] = length.
Suppose that the sheet is pinned along the boundary ∂Ω. (Think about a drum
head, for example.) In other words,
u(x, t) = 0,
∀x ∈∂Ω,
∀t ∈(0, T].
Now, the energy of the sheet — to a good approximation, provided the displace-
ment is small, always and everywhere — can be written as
E(t) =
Z
Ω
"
ρo
2

∂u
∂t (x, t)

2
+ τo
2 ∥∇u(x, t)∥2
2
#
dx,
where ρo > 0 is the constant density per unit area of the sheet, and τo > 0 is the
surface tension, We recall that
∇u(x, t) =
 ∂u
∂x1 (x, t)
∂u
∂x2 (x, t)⊺;
consequently,
∥∇u(x, t)∥2
2 =
 ∂u
∂x1
(x, t)
2
+
 ∂u
∂x2
(x, t)
2
.
The ﬁrst term in the energy is the kinetic energy, whereas the second represents
the potential or restorative energy of the sheet. The units are
[ρo] = mass
area ,
[τo] = energy
area
= force
length.
Note that τo
ρo has units of velocity squared:
τo
ρo

= length2
time2 .
The wave equation, which models the motion of our sheet, is the following:
∂2u
∂t2 (x, t) = c2∆u(x, t),
∀x ∈Ω,
∀t ∈(0, T],
(23.13)
where
c2 = τo
ρo
is the wave speed or speed of propagation. Several references derive this equation
from Newton’s second law, or via energy methods; see, for example, [49, 99].
We will not derive the equation here, but will describe some energy properties of
the solutions. As we have indicated, the boundary conditions are of homogeneous
Dirichlet type, since the deformation is zero at the boundary. The wave equation
requires two initial conditions, which is not surprising, since the equation involves

624
Review of PDE Theory
a second partial derivative with respect to time. The initial conditions are usually
of the form
u(x, 0) = α(x),
∂u
∂t (x, 0) = β(x),
∀x ∈Ω,
where α, β : Ω→R are given functions.
Let us, at least formally, show that the wave equation is energy conservative, i.e.,
d
dt E = 0. This will be made rigorous in Theorem 23.39. We will need the following
integration-by-parts formula, which is presented in Theorem B.59:
Z
Ω
v(x) · ∇u(x)dx =
Z
∂Ω
n(x) · v(x)u(x)dS −
Z
Ω
∇· v(x)u(x)dx
for all v ∈C1(¯Ω; R2) and u ∈C1(¯Ω; R). Applying this last result with the vector
v = ∇q, we have
Z
Ω
∇q(x) · ∇u(x)dx =
Z
∂Ω
n(x) · ∇q(x)u(x)dS −
Z
Ω
∆q(x)u(x)dx.
Taking the time derivative of the energy and using the second integration-by-parts
formula, we have
dE
dt (t) = d
dt
Z
Ω
(
ρo
2
∂u
∂t
2
+ τo
2
  ∂u
∂x1
2
+
 ∂u
∂x2
2!)
dx
=
Z
Ω

ρo
∂u
∂t
∂2u
∂t2 + τo∇u · ∇
∂u
∂t

dx
=
Z
Ω

ρo
∂u
∂t
∂2u
∂t2 −τo∆u ∂u
∂t

dx + τo
Z
∂Ω
n · ∇u ∂u
∂t dx.
Since u vanishes on the boundary, its time derivative is zero there as well. Hence,
the boundary integral term is zero. With a little rewriting, we have
dE
dt (t) =
Z
Ω

ρo
∂u
∂t
∂2u
∂t2 −c2∆u

dx = 0.
Solutions to the wave equation conserve energy. They do this by transmitting
energy (without loss) at speed c to various parts of the domain; see, for example,
the discussions in [49, 99]. Solutions to the wave equation also conserve entropy.
There is no loss of information in the system as time progresses. This is related to
the fact that solutions of the wave equation are time reversible.
Let us, from the wave equation, derive another important equation. To do so, we
recall the physical origins of the wave equation; it is related to wave propagation.
In other words, periodic, in time, solutions to this equation are of importance. For
this reason, we propose the following solution ansatz:
u(x, t) = eiωt bu(x),
where ω can be regarded as the wave frequency. Substituting into (23.13), we see
that the function ˆu must be a solution to the so-called Helmholtz equation:7
7 Named in honor of the Prussian mathematician and physician Hermann Ludwig Ferdinand
von Helmholtz (1821–1894).

23.1 Heuristic Derivation of the Common Partial Diﬀerential Equations
625
∆u(x) + k2u(x) = 0,
(23.14)
where k = ω
c . Notice that this equation bears resemblance to the reaction–diﬀusion
equation (23.8), but we have a deﬁnite sign on the zero-order term, i.e., the term
that does not involve partial derivatives of our unknown function.
A close relative of the wave equation is the damped wave equation:
∂2u
∂t2 (x, t) = c2∆u(x, t) −β
ρo
∂u
∂t (x, t),
∀x ∈Ω,
∀t ∈(0, T],
(23.15)
where β ≥0 is a damping parameter with units
[β] =
mass
time · area.
The last term measures the eﬀect of the frictional force per unit area and is
proportional to the velocity of the displacement. In other words, this measures
a drag eﬀect. This type of term is responsible for the fact that a guitar string
does not vibrate forever. Instead, its vibration is damped over time. It is not hard
to show, see Problem 23.1, that energy is dissipated in the damped system at
the rate
dE
dt (t) =
Z
Ω

ρo
∂u
∂t
∂2u
∂t2 −c2∆u

dx = −β
Z
Ω
∂u
∂t
2
dx,
(23.16)
assuming, as before, pinned displacement (homogeneous Dirichlet) boundary
conditions.
23.1.7
Classiﬁcation of Linear Second-Order Equations
In all the examples above, owing to our assumptions, we obtained a linear, second-
order PDE which is supplemented by boundary, and, possibly, initial, conditions. In
the simplest case, this equation has constant coeﬃcients, so that, for some m ∈N
and x ∈Rm, it reads Du(x) = f (x), where
Dv(x) =
m
X
i,j=1
ai,j
∂2v(x)
∂xi∂xj
+
m
X
i=1
bi
∂v(x)
∂xi
+ cv(x)
= A : D2v(x) + b · ∇v(x) + cv(x),
(23.17)
where A = [ai,j] ∈Rm×m; b = [bi]⊺∈Rm; the symbol : is the so-called Frobenius8
inner product on Rm×m, which we recall is deﬁned as
A : B = tr(AB⊺),
∀A, B ∈Rm×m;
D2v(x) denotes the Hessian of v at the point x; and ∇v(x) denotes its gradient at
the same point. We comment that, since for smooth functions the Hessian matrix is
symmetric, there is no loss of generality in assuming that the matrix A is symmetric
as well.
8 Named in honor of the German mathematician Ferdinand Georg Frobenius (1849–1917).

626
Review of PDE Theory
The behavior of the solution to an equation like Du = f heavily depends on the
spectrum of A. For this reason, we give the following deﬁnition.
Deﬁnition 23.1 (classiﬁcation). Consider the second-order, constant-coeﬃcient
partial diﬀerential operator D. We say this operator is:
1. Elliptic: If σ(A) ⊂R+ or σ(A) ⊂R−.
2. Parabolic: If A contains exactly m −1 either positive or negative eigenvalues,
and zero is an eigenvalue of multiplicity one.
3. Hyperbolic: If A has m −1 either positive or negative eigenvalues, and the
remaining one is nonzero and of opposite sign.
4. Ultra-parabolic: If zero is a multiple eigenvalue, and all the remaining ones have
the same sign.
5. Ultra-hyperbolic: If zero is not an eigenvalue, and there is more than one
positive and more than one negative eigenvalue.
If the matrix A depends on the spatial variable x ∈Rd, then the classiﬁcation is
done at each point.
Example 23.3
The conservation equation, presented in (23.7) for d = 3, and its
higher-dimensional (d > 3) counterparts are elliptic, as the material modulus a(x)
is assumed to always be strictly positive. The case a ≡1 in (23.7) corresponds to
A ≡Id in (23.17). In this case, we obtain the so-called minus Laplacian operator 9
in the highest order term:
−∆v(x) = −Id : D2v(x) = −
d
X
i=1
∂2v(x)
∂x2
i
.
Example 23.4
Equation (23.2) (d = 3) and its higher-dimensional (d > 3)
counterparts are parabolic. Notice that, in this case, m = d + 1 and (x, t) ∈Rd+1
are the independent variables. The case a ≡1, u ≡0, and c ≡0 in (23.2)
corresponds to the following heat or diﬀusion operator:
Dv(x, t) = Hv(x, t) = ∂v(x, t)
∂t
+
−Id
0
0⊺
0

: D2v(x, t) = ∂v(x, t)
∂t
−∆v(x, t),
where the Laplacian, ∆, is taken over the spatial variables x only.
Example 23.5
Equation (23.13) (d = 2) and its higher-dimensional (d > 3)
counterparts are hyperbolic. Again, we have m = d + 1, and (x, t) ∈Rd+1 are the
independent variables. The appropriate operator is
Dv(x, t) = □v(x, t) =
−Id
0
0⊺
1

: D2v(x, t) = ∂2v(x, t)
∂t2
−∆v(x, t),
9 Named in honor of the French scholar and polymath Pierre-Simon, Marquis de Laplace
(1749–1827).

23.2 Elliptic Equations
627
which is known as the wave operator. Again, the Laplacian, ∆, is taken over the
spatial variables x only.
Example 23.6
Let us consider an equation that changes type. The so-called
Euler–Tricomi equation10 reads
∂2u(x, y)
∂x2
+ x ∂2u(x, y)
∂y 2
= 0.
This equation is elliptic for x > 0, parabolic for x = 0, and hyperbolic when x < 0.
23.2
Elliptic Equations
In this section, we will focus on the theory of BVPs for elliptic equations. Let d ∈N
and Ω⊆Rd be a bounded domain with boundary ∂Ω. Given A ∈C(¯Ω; Rd×d),
b ∈C(¯Ω; Rd), and c ∈C(¯Ω), we consider the operator
Lv(x) = −A(x) : D2v(x) + b(x) · ∇v(x) + c(x)v(x).
(23.18)
In addition, we assume that, for all x ∈¯Ω, the matrix A(x) is symmetric, and there
are constants λ, Λ ∈R+ such that
σ(A(x)) ⊂[λ, Λ],
∀x ∈¯Ω,
so that our operator is elliptic at every point. The prototypical example of this is
the Laplacian, which we introduced in Example 23.3.
As we have mentioned before, given f ∈C(Ω), we supplement the equation
Lu = f ,
in Ω,
with boundary conditions on ∂Ω, which can be of either Dirichlet, Neumann, or
Robin boundary conditions.
Before we embark on the study of BVPs, let us study some properties of solutions
to the equation itself, which follow from ellipticity.
23.2.1
The Maximum Principle
Recall that, if a function f : R →R is convex and smooth, then −f ′′(x) ≤0 and,
for any interval [a, b] ⊂R, we have
f (x) ≤max{f (a), f (b)},
∀x ∈(a, b).
In other words, it cannot have a maximum in the interior of a domain. Similar
reasoning can be made in several dimensions, but now the Hessian, D2v(x), is
10 Named in honor of the Swiss mathematician, physicist, astronomer, geographer, logician, and
engineer Leonhard Euler (1707–1783) and the Italian mathematician Francesco Giacomo
Tricomi (1897–1978).

628
Review of PDE Theory
assumed to be positive semi-deﬁnite at every point. Since, for an elliptic operator,
the coeﬃcient −A is negative deﬁnite, we expect that the product −A : D2v has
a sign. These statements are the intuition behind the so-called maximum principle.
Theorem 23.2 (maximum principle). Let v ∈C2(Ω) ∩C(¯Ω) be such that, for all
x ∈Ω,
Lv(x) ≤0.
1. If c ≡0, then the function v attains its maximum at the boundary, i.e.,
max
x∈¯Ωv(x) ≤max
x∈∂Ωv(x).
2. If c(x) ≥0 for all x ∈Ω, then
max
x∈¯Ωv(x) ≤max

0, max
x∈∂Ωv(x)

.
Proof. Before we proceed with the proof, it is ﬁrst instructive to sketch the idea
of what happens when c ≡0. Let x0 ∈Ωbe a point where v attains its maximum.
This means that, at this point, we must have
∇v(x0) = 0,
σ(D2v(x0)) ⊂(−∞, 0].
This means that
Lv(x0) = −A(x0) : D2v(x0) + b(x0) · ∇v(x0) ≥0,
which is almost a contradiction. The way to push this to an actual contradiction
is by a so-called comparison function.
We now proceed with the proof. Let φ(x) = eαx1, where α > 0 and suﬃciently
large, so that
Lφ(x) =
 −a1,1(x)α2 + b1(x)α

φ(x) < 0,
∀x ∈¯Ω.
This will be the desired function. Now, since A(x) is SPD, and we have bounds on
the spectrum,
a1,1(x) ≥λ,
∀x ∈¯Ω.
Also, since b ∈C(¯Ω; Rd),
|b1(x)| ≤∥b∥L∞(Ω;Rd),
∀x ∈¯Ω.
Therefore,
−a1,1(x)α2 + b1(x)α ≤−λα2 + ∥b∥L∞(Ω;Rd)α,
and the choice of α is now clear.
Assume now that c ≡0. Deﬁne the function ˜v = v + εφ, where ε > 0 is to be
chosen later. Notice now that
L˜v(x) = Lv(x) + εLφ(x) < 0.

23.2 Elliptic Equations
629
If v attains its maximum at some point x0 ∈Ω, then, for ε suﬃciently small, the
function ˜v also attains its maximum at some point x1 ∈Ω. At this point, we must
then have
L˜v(x1) ≥0,
which is the desired contradiction.
Let now c(x) ≥0 for all x ∈Ω. Notice that if v(x) ≤0 for all x ∈¯Ω, then there
is nothing to prove. Thus, assume that x0 ∈Ωis such that
v(x0) = max
x∈¯Ωv(x) > 0.
This, by continuity, implies that there is a neighborhood of the point x0 where
v > 0. Denote by Ω(x0) the largest open and connected set where this holds.
Then, for every x ∈Ω(x0), we notice that the operator
˜Lv(x) = −A(x) : D2v(x) + b(x) · ∇v(x) = Lv(x) −c(x)v(x) ≤0.
Since this operator is elliptic and it has no zero-order coeﬃcient, the previous case
implies that
0 < v(x0) =
max
x∈∂Ω(x0) v(x).
Notice then that this implies that ∂Ω(x0) cannot be contained in Ω. Otherwise,
there would be a point in ∂Ω(x0) where v is positive, and this contradicts that
Ω(x0) is maximal.
Remark 23.3 (minimum principle). If Lv ≥0, Theorem 23.2 remains valid if we
replace maxima by minima. This can be seen using linearity and the function −v.
From the maximum principle, we can easily obtain a stability result.
Corollary 23.4 (stability). There is a constant C > 0 such that, for every v ∈
C2(¯Ω), we have
∥v∥L∞(Ω) ≤∥v∥L∞(∂Ω) + C∥Lv∥L∞(Ω).
Proof. As before, we let φ(x) = Ceαx1, where C > 0 and α > 0 are chosen, so
that φ(x) ≥0 and Lφ(x) ≤−1 for all x ∈Ω.
Deﬁne
˜v± = ±v + ∥Lv∥L∞(Ω)φ.
Notice that
L˜v±(x) = ±Lv(x) + ∥Lv∥L∞(Ω)Lφ(x) ≤±Lv(x) −∥Lv∥L∞(Ω) ≤0.
Consequently, by the maximum principle of Theorem 23.2,
max
x∈¯Ω˜v±(x) = max
x∈∂Ω˜v±(x),

630
Review of PDE Theory
so that, for every x ∈Ω,
±v(x) ≤˜v±(x)
≤max
x∈∂Ω˜v±(x)
≤max
x∈∂Ω|v(x)| + ∥Lv∥L∞(Ω)∥φ∥L∞(∂Ω)
≤∥v∥L∞(∂Ω) + C∥Lv∥L∞(Ω),
as we intended to show.
As a ﬁnal application, we present a sort of monotonicity result.
Corollary 23.5 (monotonicity). Let v1, v2 ∈C2(Ω) ∩C(¯Ω) be such that v1 ≤v2
on ∂Ωand Lv1 ≤Lv2 in Ω. Then we must have v1 ≤v2 in ¯Ω.
Proof. See Problem 23.5.
23.2.2
The Dirichlet Problem: Classical Solutions
We now pose the ﬁrst BVP for an elliptic operator. Given f ∈C(Ω) and g ∈C(∂Ω),
we will seek a solution to
(
Lu(x) = f (x),
x ∈Ω,
u(x) = g(x),
x ∈∂Ω.
(23.19)
Since the boundary conditions are of Dirichlet type, this is sometimes known as
the Dirichlet problem. When L = −∆, this problem is usually referred to as the
Poisson problem.
We must specify what we mean by a solution to this problem, and there are
many ways of doing so. Our focus here is on classical solutions.
Deﬁnition 23.6 (classical solution). The function u ∈C2(Ω)∩C(¯Ω) is a classical
solution of the Dirichlet problem (23.19) if and only if the equation and boundary
condition are satisﬁed point-wise.
An immediate consequence of the maximum principle is the uniqueness of
classical solutions.
Corollary 23.7 (uniqueness). Problem (23.19) cannot have more than one classical
solution.
Proof. See Problem 23.6.
Existence, unfortunately, is a topic that is beyond our discussion here. We refer
the reader to the references given at the beginning of the chapter for details. Let
us, nevertheless, state a simpliﬁed version.

23.2 Elliptic Equations
631
Theorem 23.8 (existence). Let d = 2, Ω= (0, 1)2, and L = −∆. Assume that
f ∈C(¯Ω), g ∈C(∂Ω) are, in addition, suﬃciently smooth, so that their Fourier
series converge uniformly and absolutely. Then the Poisson problem (23.19) has a
solution.
Proof. Since we are on a square, the idea is to use separation of variables. The
smoothness assumptions on the data allow us to justify the series representations;
see [49, 99] for further details.
To conclude the discussion, we comment on the existence of results concerning
the further regularity of a classical solution. In essence, these results provide
suﬃcient conditions on the domain Ωand the data f , g to guarantee that a classical
solution satisﬁes u ∈Ck(¯Ω), for k > 2.
23.2.3
The Dirichlet Problem: Weak Solutions
To motivate this treatment of a BVP, we observe that, in Section 23.1, when we
derived PDEs, these were usually in divergence form, as this was the natural form
in which they arose from a conservation. The divergence form of an equation was
valid, even at points where the material modulus was not smooth.
For this reason, here, we will consider problems in divergence form. The operator
we will consider here is then
Lv(x) = −∇· (A(x)∇v(x)) + b(x) · ∇v(x) + c(x)v(x),
(23.20)
under the same assumptions on the coeﬃcients as before. It is not diﬃcult to see
that this operator is elliptic, in the sense of Deﬁnition 23.1. We will once again
consider the Dirichlet problem (23.19), but where the operator now is as in (23.20).
The prototypical example here, again, is L = −∆.
Before we deﬁne our notion of solution, we will ﬁrst motivate it. Let us multiply
the equation in (23.19) by a function v ∈C1(¯Ω) that vanishes in a neighborhood
of ∂Ωand integrate. An application of the Divergence Theorem reveals that
−
Z
Ω
∇· (A∇u) v dx =
Z
Ω
∇v ⊺A∇u dx −
Z
∂Ω
n⊺A∇uv dS =
Z
Ω
∇v ⊺A∇u dx,
where we used that v vanishes on ∂Ωand n denotes the outer unit normal to ∂Ω.
In short, if u is a classical solution to (23.19) with the operator L given by (23.20),
then it must also satisfy
Z
Ω
[∇v ⊺A∇u + b · ∇uv + cuv] dx =
Z
Ω
f v dx
(23.21)
for every v ∈C1(¯Ω) that vanishes on ∂Ω.

632
Review of PDE Theory
Notice that the integral identity (23.21) makes sense under much less restrictive
assumptions than those we used to derive it. In fact, repeated applications of the
Cauchy–Schwarz inequality yield the following:

Z
Ω
∇v ⊺A∇u dx
 ≤∥A∥L∞(Ω;Rd×d)∥∇u∥L2(Ω;Rd)∥∇v∥L2(Ω;Rd),

Z
Ω
b · ∇uv dx
 ≤∥b∥L∞(Ω;Rd)∥∇u∥L2(Ω;Rd)∥v∥L2(Ω),

Z
Ω
cuv dx
 ≤∥c∥L∞(Ω)∥u∥L2(Ω)∥v∥L2(Ω),

Z
Ω
f v dx
 ≤∥f ∥L2(Ω)∥v∥L2(Ω).
(23.22)
These estimates show that all that is needed from the functions u and v is that
they, together with all of their derivatives, belong to the space L2(Ω). In fact,
this can be made even weaker. All that is needed is that these functions can be
approximated in the L2(Ω) sense by smooth ones. In other words, we require that
u and v belong to the Sobolev space H1(Ω), which is discussed in Appendix D.
Having realized this, we immediately observe that (23.22) are the correct bounds
to deﬁne a weaker notion of solution, one that is based on (23.21). Notice that we
have already seen similar notions before. Namely, a weak solution to a rectangular
system of equations was introduced in Chapter 5, and a weak formulation of a
problem (for a diﬀerential equation) was given in Chapter 22.
Deﬁnition 23.9 (weak solution). We say that the function u ∈H1
0(Ω) is a weak
solution to the Dirichlet problem (23.19) with g = 0 and L given as in (23.20) if
and only if, for every test function v ∈H1
0(Ω), we have
Z
Ω
[∇v ⊺A∇u + b · ∇uv + cuv] dx =
Z
Ω
f v dx.
Let us now present the following result concerning the existence and uniqueness
of solutions. In the literature, this is commonly known as the Lax–Milgram
Theorem.
Theorem 23.10 (Lax–Milgram Theorem11). Let H be a Hilbert space and F ∈H′.
Assume that we have a function A: H × H →R that is
1. Bilinear: In other words, linear in each argument.
2. Bounded: There is a constant M > 0 such that, for every v1, v2 ∈H, we have
|A(v1, v2)| ≤M∥v1∥H∥v2∥H.
3. Coercive: There is a constant α > 0 such that, for every nonzero v ∈H,
α∥v∥2
H ≤A(v, v).
11 Named in honor of the Hungarian-born American mathematician Peter David Lax (1926–)
and the American mathematician Arthur Norton Milgram (1912–1961).

23.2 Elliptic Equations
633
In this setting, the problem: Find u ∈H such that
A(u, v) = F(v),
∀v ∈H
(23.23)
has a unique solution. This solution satisﬁes the estimate
∥u∥H ≤1
α∥F∥H′.
If, in addition, the bilinear form is symmetric, i.e., A(v1, v2) = A(v2, v1) for all
v1, v2 ∈H, then the element u ∈H minimizes the quadratic energy
E(v) = 1
2A(v, v) −F(v)
if and only if it solves problem (23.23).
Proof. Notice, ﬁrst of all, that if u ∈H is a solution to (23.23), then setting v = u
yields
α∥u∥2
H ≤A(u, u) = F(u) ≤∥F∥H′∥u∥H,
which is the claimed estimate.
Consider now the case where A is symmetric. It is not diﬃcult to see that E is a
strictly convex, continuous, and coercive energy on H, and that (23.23) is nothing
but its Euler equation. Theorem 16.31 gives then the existence and uniqueness of
a minimizer. Finally, Theorem 16.44 shows that u ∈H solves (23.23) if and only
if it minimizes the energy E.
We now address the general case. We will proceed via a homotopy technique.
Deﬁne the symmetric and skew-symmetric parts of A via
Asym(v1, v2) = 1
2 [A(v1, v2) + A(v2, v1)] ,
Ask(v, w) = 1
2 [A(v1, v2) −A(v2, v1)].
Let now t ∈[0, 1]. Deﬁne
A(v1, v2; t) = Asym(v1, v2) + tAsk(v1, v2).
Notice that this parametrized bilinear form satisﬁes
A(v1, v2; 1) = A(v1, v2),
A( · , · ; t) is bounded with constant 2M, and A(v, v; t) = A(v, v), so it is also
coercive with constant α.
Our theorem will be proved if we can prove the following claim: Let t ∈[0, 1]
and consider, for G ∈H′, the problem: ﬁnd ut ∈H such that
A(ut, v; t) = G(v),
∀v ∈H.
Assume that, for t0 ∈[0, 1] and every G ∈H′, this problem has a unique solution.
Then this problem is uniquely solvable for t ∈[t0, t0 +
α
2M ] and any G ∈H′.
If the previous statement holds, then we obtain the result. This is because
A( · , · ; 0) = Asym( · , · ) is symmetric, and the result has already been proved
for this case. By the claim, the problem is uniquely solvable for t ∈[0,
α
2M ]. Set
t0 =
α
2M and apply the claim again. In a ﬁnite number of steps, we will reach that

634
Review of PDE Theory
the problem is uniquely solvable for t = 1, but A(v1, v2; 1) = A(v1, v2), and so our
original problem has a unique solution.
It remains then to prove the claim. To do so, we deﬁne the mapping T : H →H
as uφ = Tφ, where uφ solves
A(uφ, v; t0) = G(v; t),
∀v ∈H,
G(v; t) = F(v) + (t0 −t)Ask(φ, v),
with t ∈[t0, t0 +
α
2M ]. Notice that G(·; t) ∈H′, so by the assumption of the claim
the mapping T is well deﬁned. In addition, notice that if it has a ﬁxed point, i.e.,
u = Tu, then
Asym(u, v) + t0Ask(u, v) = F(v) + (t0 −t)Ask(u, v),
so that the parameterized problem for t is well posed, and this will imply the claim.
Thus, we will show that the mapping T satisﬁes all the assumptions of the Banach
Fixed Point Theorem C.4. Let φ1, φ2 ∈H and set ui = Tφi, for i = 1, 2. We can
then subtract the corresponding equations and, by coercivity, obtain
α∥u1 −u2∥2
H ≤A(u1 −u2, u1 −u2; t0)
≤|t0 −t||Ask(φ1 −φ2, u1 −u2)|
≤
α
2M M∥φ1 −φ2∥H∥u1 −u2∥H,
so that T is a contraction.
Let us now apply the Lax–Milgram Theorem 23.10 to the existence and
uniqueness of weak solutions.
Corollary 23.11 (existence and uniqueness). Let d ∈N and Ω⊂Rd be a bounded
domain with suﬃciently nice boundary. Let A ∈C(¯Ω; Rd×d), b ∈C1(¯Ω; Rd), and
c ∈C(¯Ω). Assume that there are constants λ, Λ ∈R+ such that, for all points
x ∈¯Ω, matrix A(x) is symmetric and satisﬁes
σ(A(x)) ⊂[λ, Λ].
If f ∈L2(Ω), g = 0, and
c(x) −1
2∇· b(x) ≥0,
∀x ∈Ω,
then the Dirichlet problem (23.19) with the operator L given by (23.20) has a
unique weak solution which, moreover, satisﬁes
∥∇u∥L2(Ω;Rd) ≤1
λ(C + Λ)CP ∥f ∥L2(Ω),
where the constant C depends on the coeﬃcients b and c and CP is the constant
in the Poincar´e inequality (D.2). Finally, if b ≡0, this solution minimizes, over
H1
0(Ω), the quadratic energy
E(v) = 1
2
Z
Ω

∇v ⊺A∇v + c|v|2
dx −
Z
Ω
f vdx.
Proof. We only need to verify the assumptions of Theorem 23.10. Over the Hilbert
space H1
0(Ω), we deﬁne

23.2 Elliptic Equations
635
A(v1, v2) =
Z
Ω
[∇v ⊺
2 A∇v1 + b · ∇v1v2 + cv1v2] dx,
F(v) =
Z
Ω
f v dx.
Estimates (23.22) show that the bilinear form A is bounded and that if b ≡0,
then A is symmetric. Thus, it remains to prove its coercivity.
Using the properties of A, b, and c and integrating back by parts, we see that,
for every v ∈H1
0(Ω),
A(v, v) =
Z
Ω

∇v ⊺A∇v + b · ∇vv + c|v|2
dx
=
Z
Ω

∇v ⊺A∇v + b · ∇
1
2|v|2

+ c|v|2

dx
≥λ
Z
Ω
|∇v|2 dx +
Z
Ω
[c −1
2∇· b]|v|2d x
≥λ∥∇v∥2
L2(Ω;Rd),
which is the coercivity we were looking for.
Finally, using Cauchy–Schwarz and Poincar´e inequalities, we get

Z
Ω
f v dx
 ≤∥f ∥L2(Ω)∥v∥L2(Ω) ≤CP ∥f ∥L2(Ω)∥∇v∥L2(Ω;Rd),
so that ∥F∥H1
0(Ω)′ ≤CP ∥f ∥L2(Ω).
We conclude by discussing further regularity of weak solutions, at least in a
particular case.
Theorem 23.12 (regularity). Let d ∈N and Ω⊂Rd be a bounded domain which
is either convex or has a smooth boundary. There is a constant C > 0 such that,
whenever f ∈L2(Ω) and g = 0, the unique solution to (23.19) with L = −∆
satisﬁes
|u|H2(Ω) ≤C∥f ∥L2(Ω),
i, j = 1, . . . , d.
To conclude the discussion, since it will be needed to develop the theory of
evolution problems, we present one result regarding the eigenvalue problem for
elliptic operators. The proof of this result, however, is beyond the scope of our
discussion. For a proof, we refer the reader to [31]. We begin with a deﬁnition.
Deﬁnition 23.13 (eigenvalue). Let L be given in (23.20). We say that λ ∈C is
an eigenvalue of the diﬀerential operator L provided that the problem
(
Lϕ = λϕ,
in Ω,
ϕ = 0,
on ∂Ω
has a nontrivial weak solution ϕ ∈H1
0(Ω), which is known as an eigenfunction.
Theorem 23.14 (eigenvalue problem). Let d ∈N and Ω⊂Rd be a bounded
domain with suﬃciently nice boundary. Let A ∈C(¯Ω; Rd×d), b ≡0, and c ∈C(¯Ω).
Assume that there are constants λ, Λ ∈R+ such that, for all points x ∈¯Ω, the
matrix A(x) is symmetric and satisﬁes
σ(A(x)) ⊂[λ, Λ].

636
Review of PDE Theory
Consider the diﬀerential operator L, given in (23.20), supplemented with homoge-
neous Dirichlet boundary conditions. In this setting:
1. The set of all eigenvalues of L is at most countable.
2. All eigenvalues are real and positive.
3. Let {λk}∞
k=1 be the collection of all eigenvalues, repeated according to their
(ﬁnite) multiplicity. Then
0 < λ1 < λ2 < · · · < λk < · · · ,
λk →∞,
k →∞.
4. There exists an orthonormal basis of L2(Ω) consisting of eigenfunctions of L,
i.e., there is {ϕk}∞
k=1 ⊂H1
0(Ω) that form an orthonormal basis of L2(Ω) and
(
Lϕk = λkϕk,
in Ω,
ϕk = 0,
on ∂Ω
for all k ∈N.
Example 23.7
Let Ω= (0, 1)d. The eigenpairs of the Dirichlet Laplacian, i.e.,
nontrivial solutions of
(
−∆ϕ = λϕ,
in Ω,
ϕ = 0,
on ∂Ω,
are the pairs {(λk, ϕk)}k∈Nd ⊂R+ × H1
0(Ω) given by
λk = π2 ∥k∥2
2 ,
ϕk = 2d/2
dY
i=1
sin (π[k]i[x]i) .
23.3
Parabolic Equations
In this section, we will consider initial value problems and initial boundary value
problems (IBVPs) for parabolic equations, the prototype of which is the heat
equation introduced in Example 23.4. As always, we will just present the most
relevant notions and estimates that will be used later in the numerical treatment
of these problems.
23.3.1
The Initial Value Problem
Let us begin by considering the pure IVP for the heat equation in Rd with d ∈N.
In this case, we let T > 0. Given a suﬃciently smooth function u0 : Rd →R, we
will seek a function u : Rd × [0, T] →R that satisﬁes, in some sense,



∂u(x, t)
∂t
−∆u(x, t) = 0,
(x, t) ∈Rd × (0, T],
u(x, 0) = u0(x),
x ∈Rd.
(23.24)

23.3 Parabolic Equations
637
We begin by making precise our notion of solution.
Deﬁnition 23.15 (classical solution). A function u : Rd × [0, T] is a classical
solution of (23.24) if, for every t > 0, u( · , t) ∈C2(Rd), for every x ∈Rd
u(x, ·) ∈C1([0, T]), the equation is satisﬁed point-wise, and as t ↓0 we have
u(x, t) →u0(x) for all x ∈Rd.
We will seek a solution using the Fourier transform,12 which, for a suﬃciently
nice function v, is deﬁned via
ˆv(ξ) = F[v](ξ) =
Z
Rd e−ix·ξv(x)dx.
(23.25)
We refer the reader, for instance, to [77, 97] for a complete account of the
properties of this mapping. Here, we simply state the following.
Proposition 23.16 (properties of F). Let the functions v, v1, v2 : Rd →R be
suﬃciently smooth and decay suﬃciently fast at inﬁnity. Then we have:
1. Diﬀerentiation: For any j = 1, . . . , d,
F
 ∂v
∂xj

(ξ) = iξjF[v](ξ).
2. Convolution: The convolution of v1 and v2 is deﬁned by
(v1 ⋆v2)(x) =
Z
Rd v1(x −y)v2(y)dy.
The transform of the convolution satisﬁes
F[v1 ⋆v2](ξ) = F[v1](ξ)F[v2](ξ).
3. Gaussian:13 Let G(x) = e−∥x∥2
2. Then we have
F[G](ξ) = πd/2e−∥ξ∥2
2/4.
4. Plancherel identity:14 If v ∈L2(Rd),
∥v∥L2(Rd) = ∥ˆv∥L2(Rd).
5. Shift: For any a ∈Rd,
F[v( · −a)](ξ) = e−ia·ξF[v](ξ).
Proof. We will only sketch the proof of the shift property, as later we will need a
discrete analogue. By deﬁnition, we have
F[v( · −a)](ξ) =
Z
Rd e−ix·ξv(x −a)dx = e−ia·ξ
Z
Rd e−iy·ξv(y)dy = e−ia·ξF[v](ξ),
where we applied a simple change of variables.
12 Named in honor of the French mathematician and physicist Jean-Baptiste Joseph Fourier
(1768–1830).
13 Named in honor of the German mathematician and physicist Johann Carl Friedrich Gauss
(1777–1855).
14 Named in honor of the Swiss mathematician Michel Plancherel (1885–1967).

638
Review of PDE Theory
We incorporate the Fourier transform into our discussion because, at least
formally, this can help us construct a solution to (23.24). To see this, take the
Fourier transform of this problem to obtain the parameterized linear ODE
dˆu(ξ, t)
dt
= −∥ξ∥2
2 ˆu(ξ, t),
ˆu(ξ, 0) = ˆu0(ξ),
which can be solved exactly by
ˆu(ξ, t) = ˆu0(ξ)e−t∥ξ∥2
2.
Let us deﬁne the fundamental solution to the heat equation to be the function
Φ: Rd × (0, T] that is deﬁned via
ˆΦ(ξ, t) = e−t∥ξ∥2
2
=⇒
Φ(x, t) =
1
(4πt)d/2 e−
∥x∥2
2
4t .
(23.26)
The convolution property of the Fourier transform then shows that the solution to
(23.24) is given by
u(x, t) = (Φ( · , t) ⋆u0(·)) (x) =
1
(4πt)d/2
Z
Rd e−
∥x−y∥2
2
4t
u0(y)dy.
(23.27)
These arguments can be made rigorous to provide the existence of a solution.
Theorem 23.17 (existence). Let u0 ∈Cb(Rd) ∩L2(Rd). Then the function u,
deﬁned by (23.27), is a classical solution to (23.24). Moreover, for any x0 ∈Rd,
we have
u(x, t) →u0(x0),
(x, t) →(x0, 0).
Proof. It suﬃces to observe that, for t > 0, it is legitimate to diﬀerentiate the
integral representation (23.27) to obtain the requisite smoothness, and the fact
that this function solves the equation.
To show the sense in which the initial condition is attained, recall that
1
πd/2
Z
Rd e−∥x∥2
2 dx = 1,
use this in the expression,
u(x, t) −u0(x0) =
1
(4πt)d/2
Z
Rd e−
∥x−y∥2
2
4t
u0(y)dy −u0(x0),
and carry out the change of variables z =
1
√
4t (y −x). We leave the details to the
reader as an exercise; see Problem 23.14.
Example 23.8
Let us illustrate the solution of (23.24), described in (23.27), in
the case d = 1 and
u0(x) = e−x2/2,
x ∈R.
The derivations that led to (23.27) showed that
ˆu(ξ, t) = ˆu0(ξ)e−tξ2,
ˆu0(ξ) =
√
2πe−ξ2
2 ,

23.3 Parabolic Equations
639
x
y
1
1
t = 0
t = 0.1
t = 0.2
t = 1
t = 2
t = 5
Figure 23.1 Solution to the IVP for the heat equation (23.24) with d = 1 and Gaussian
initial data u0(x) = e−x2/2.
where we used the Fourier transform of a Gaussian. Therefore,
ˆu(ξ, t) =
√
2πe−ξ2(t+ 1
2)
=⇒
u(x, t) =
e−
x2
4t+2
√1 + 2t .
(23.28)
This solution is illustrated in Figure 23.1.
Example 23.9
The previous example helps to explain why the backward diﬀusion
equation,
∂u(x, t)
∂t
+ ∆u(x, t) = 0,
is not well posed. Formally, (23.28) is a solution to the backward diﬀusion equation.
But, as t →−1
2, the solution approaches the Dirac delta function. In other words,
solutions to the backward diﬀusion equation can blow up in ﬁnite time.
Let us now work on showing continuous dependence in the class of bounded
continuous functions Cb(Rd × [0, T]). To achieve this, we begin by deﬁning, for
t > 0, the operator
E(t): Cb(Rd) →Cb(Rd),
(E(t)v)(x) =
1
(4πt)d/2
Z
Rd e−
∥x−y∥2
2
4t
v(y)dy.
(23.29)

640
Review of PDE Theory
Proposition 23.18 (nonexpansiveness). For any t > 0, the operator E(t), deﬁned
in (23.29) is nonexpansive in Cb(Rd). As a consequence, if u1
0, u2
0 ∈Cb(Rd), then
the corresponding solutions, u1, u2, of (23.24) satisfy
∥u1( · , t) −u2( · , t)∥L∞(Rd) ≤∥u1
0 −u2
0∥L∞(Rd),
∀t > 0.
Proof. From the deﬁnition, we see that this is the convolution of a continuous func-
tion with an integrable one, so the result is continuous. To show nonexpansiveness,
we observe that, by deﬁnition, we have
|(E(t)v)(x)| =
1
(4πt)d/2

Z
Rd e−
∥x−y∥2
2
4t
v(y)dy

≤
1
(4πt)d/2 ∥v∥L∞(Rd)
Z
Rd e−
∥x−y∥2
2
4t
dy
= ∥v∥L∞(Rd).
By linearity, the stability result follows.
From the representation formula for the solution, it is also possible to obtain
smoothness of the solution.
Theorem 23.19 (regularity). Let u0 ∈Cb(Rd). Then, for all t > 0, we have that
u( · , t) ∈Cb(Rd), the solution of (23.24), is inﬁnitely diﬀerentiable in space and
time. In fact, for any j ∈N0 and any multi-index α ∈Nd
0 of length k ≥0, there is
a constant C > 0 such that we have

∂j
∂tj Dα
x u( · , t)

L∞(Rd)
≤Ct−j−k/2∥u0∥L∞(Rd),
∀t > 0.
Proof. Let t > 0. Since the integral from the representation u(x, t) = (E(t)u0)(x)
converges absolutely, we can write
∂j
∂tj Dα
x u(x, t) =
Z
Rd
∂j
∂tj Dα
x Φ(x −y, t)u0(y)dy.
It is not diﬃcult to see now that

∂j
∂tj Dα
x Φ(x, t)
 ≤t−j−k/2−d/2P
∥x∥2
√
4t

e−
∥x∥2
2
4t ,
(23.30)
where P is a polynomial. Thus, we have

∂j
∂tj Dα
x u(x, t)
 ≤t−j−k/2∥u0∥L∞(Rd)
√
td
Z
Rd P
∥x∥2
√
4t

e−
∥x∥2
2
4t dx
≤Ct−j−k/2∥u0∥L∞(Rd),
as we intended to show.
We conclude by establishing Duhamel’s formula15 for the solution of the
inhomogeneous IVP: let T > 0, given suﬃciently smooth u0 : Rd →R and
f : Rd × (0, T] →R, ﬁnd u : Rd × [0, T] →R such that
15 Named in honor of the French mathematician and physicist Jean-Marie Constant Duhamel
(1797–1872).

23.3 Parabolic Equations
641



∂u(x, t)
∂t
−∆u(x, t) = f (x, t),
(x, t) ∈Rd × R+,
u(x, 0) = u0(x),
x ∈Rd.
(23.31)
The notion of a classical solution is as before. At least formally, the solution of this
problem is given by
u(x, t) =
Z
Rd Φ(x −y, t)u0(y)dy +
Z t
0
Z
Rd Φ(x −y, s)f (y, s)dyds
= (E(t)u0)(x) +
Z t
0
(E(t −s)f ( · , s))(x)ds.
This solution representation can be made rigorous provided that u0 ∈Cb(Rd) and
f ∈C2
b(Rd × (0, T]).
23.3.2
The Initial Boundary Value Problem
We will now consider the IBVP for the heat equation. We assume that d ∈N,
Ω⊂Rd is a bounded domain with suﬃciently smooth boundary. Given T > 0,
u0 : ¯Ω→R, and f : Ω× (0, T] →R, we seek a function u : ¯Ω× [0, T] →R that,
in some sense, satisﬁes









∂u(x, t)
∂t
−∆u(x, t) = f (x, t),
(x, t) ∈Ω× (0, T],
u(x, t) = 0,
(x, t) ∈∂Ω× (0, T],
u(x, 0) = u0(x),
x ∈¯Ω.
(23.32)
The deﬁnition of the classical solution is rather standard.
Deﬁnition 23.20 (classical solution). A function u : ¯Ω× R+ ∪{0} is a classical
solution of (23.32) if, for every t > 0, u( · , t) ∈C2(Ω) ∩C(¯Ω), for every x ∈Ω
u(x, ·) ∈C1((0, T]), the equation is satisﬁed point-wise, and as t ↓0 we have
u(x, t) →u0(x) for all x ∈¯Ω.
While it is possible to develop a theory regarding existence, uniqueness,
and further regularity of such classical solutions, this imposes rather restrictive
assumptions on the domain Ωand the data u0 and f . Thus, we will seek an alternate
notion of the solution.
To motivate it, we will ﬁnd a solution representation. To achieve this, we recall
that Theorem 23.14 showed that there is a family of pairs {(λk, ϕk)}k∈N ⊂R+ ×
H1
0(Ω), called the eigenpairs of the Laplacian, where the family {ϕk}k∈N forms an
orthonormal basis of the space L2(Ω).
We consider the following ansatz for the solution of (23.32):
u(x, t) =
∞
X
k=1
uk(t)ϕk(x).

642
Review of PDE Theory
Let us assume that the data u0 and f are suﬃciently smooth that they admit the
following representations:
u0(x) =
∞
X
k=1
u0,kϕk(x),
f (x, t) =
∞
X
k=1
fk(t)ϕk(x).
Substituting these representations in (23.32), we obtain, for all t > 0,
∞
X
k=1
[u′
k(t) + λkuk(t) −fk(t)] ϕk(x) = 0.
As we mentioned above, the functions ϕk are orthonormal, and hence linearly
independent. Thus, we obtain a countable collection of linear ODEs,
u′
k(t) + λkuk(t) = fk(t),
t > 0,
uk(0) = u0,k.
This will allow us to obtain a solution representation and several estimates on it.
Theorem 23.21 (existence). Let u0 = P∞
k=1 u0,kϕk ∈L2(Ω). The function
u(x, t) =
∞
X
k=1
u0,ke−λktϕk(x)
(23.33)
satisﬁes (23.32) with f ≡0. Moreover, for all t > 0, we have
∥u( · , t)∥L2(Ω) ≤∥u0∥L2(Ω),
(23.34)
∥∇u( · , t)∥L2(Ω;Rd) ≤Ct−1/2∥u0∥L2(Ω),
(23.35)
∥∆mu( · , t)∥L2(Ω) ≤Cmt−m∥u0∥L2(Ω),
(23.36)
where the constants C and Cm are independent of t, but Cm may depend on m ∈N.
Proof. From the solution representation, with fk ≡0 for all k ∈N, we obtain
uk(t) = u0,ke−λkt.
Consequently, using that 0 < λ1 ≤λk, for all k ∈N,
∥u( · , t)∥2
L2(Ω) =
∞
X
k=1
|uk(t)|2 =
∞
X
k=1
|u0,k|2e−2λkt ≤e−2λ1t∥u0∥2
L2(Ω),
which shows that u( · , t) ∈L2(Ω) and proves (23.34). Notice now that, for t > 0,
the function deﬁned in (23.33) is smooth and the properties of ϕk show that this
function satisﬁes the heat equation and the boundary condition.
Let us now show in which sense the initial condition is attained. Let ε > 0. Since
u0 ∈L2(Ω), there is N ∈N such that
X
k>N
|u0,k|2 < ε
8.
Choose now t0 > 0, but small enough, so that, for all t ∈[0, t0], we have
max
k=1,...,N
 e−λkt −1
2 <
ε
2(∥u0∥2
L2(Ω) + 1).

23.3 Parabolic Equations
643
We now consider
∥u( · , t) −u0∥2
L2(Ω) =
∞
X
k=1
|u0,k|2|e−λkt −1|2
=
N
X
k=1
|u0,k|2|e−λkt −1|2 +
X
k>N
|u0,k|2|e−λkt −1|2
≤
max
k=1,...,N
 e−λkt −1
2
N
X
k=1
|u0,k|2 + 2(e−2λ1t + 1)
X
k>N
|u0,k|2
≤
max
k=1,...,N
 e−λkt −1
2 ∥u0∥2
L2(Ω) + 4
X
k>N
|u0,k|2
< ε.
In other words, the initial condition is attained in the L2(Ω)-sense.
Let us now show that (23.35)
∥∇u( · , t)∥2
L2(Ω;Rd) =
Z
Ω
∇u(x, t) · ∇u(x, t)dx
=
2
X
k,m=1
uk(t)um(t)
Z
Ω
∇ϕk(x) · ∇ϕm(x)dx
=
2
X
k=1
λk|uk(t)|2
=
∞
X
k=1
λke−2λkt|u0,k|2
= t−1
∞
X
k=1
(λkt)e−2λkt|u0,k|2
≤Ct−1∥u0∥2
L2(Ω).
The proof of (23.36) is left to the reader as an exercise; see Problem 23.17.
The previous result shows that although, for any t > 0, we have u( · , t) ∈H1
0(Ω),
this property, in general, does not survive as t ↓0. The following result gives
suﬃcient conditions for this to be valid for all times.
Proposition 23.22 (limit t ↓0). Assume that u0 ∈H1
0(Ω). Then we have
∥∇u( · , t)∥L2(Ω;Rd) ≤∥∇u0∥L2(Ω;Rd),
∀t ≥0.
Proof. From the proof of (23.35), we have
∥∇u( · , t)∥2
L2(Ω;Rd) =
∞
X
k=1
e−2λktλk|u0,k|2 ≤e−2λ1t
∞
X
k=1
λk|u0,k|2 ≤∥∇u0∥2
L2(Ω;Rd).

644
Review of PDE Theory
Let us ﬁnally consider the case f ̸= 0 and the analogous Duhamel’s formula. Let
us, for t > 0, deﬁne the operator
EΩ(t): L2(Ω) →L2(Ω),
(EΩ(t)v)(x) =
∞
X
k=1
vke−λktϕk(x),
where
v(x) =
∞
X
k=1
vkϕk(x).
Then the solution to (23.32) can be written as
u(x, t) = (EΩ(t)u0)(x) +
Z t
0
(EΩ(t −s)f ( · , s))(x)ds,
(23.37)
provided that f ∈L2(Ω×R+). Notice that this representation immediately implies
that
∥u( · , t)∥L2(Ω) ≤∥u0∥L2(Ω) +
Z t
0
∥f ( · , s)∥L2(Ω) ds.
All these previous computations and estimates serve to motivate our deﬁnition
of a weak solution. To properly deﬁne the notion of a weak solution, it will be
necessary to deﬁne some classes of functions. The intuition behind this deﬁnition
is that a function v : Ω× R+ →R can be thought of either as a function of two
variables or as a function of one variable (time), which at every instance t ∈R+
produces a function over the spatial domain:
t 7→(v( · , t): Ω→R).
Deﬁnition 23.23 (vector-valued Lp). Let T ∈(0, ∞], p ∈[1, ∞], and V be a
(separable) Banach space with norm ∥· ∥V. We deﬁne
Lp(0, T; V)
as the vector space of functions v : (0, T) →V such that the mapping t 7→∥v(t)∥V
is measurable and the quantity
Z T
0
∥v(t)∥p
V dt < ∞.
This is a Banach space with norm
∥v∥Lp(0,T ;V) =
Z T
0
∥v(t)∥p
V dt
1/p
,
p ∈[1, ∞),
and
∥v∥L∞(0,T ;V) = ess sup
t∈[0,T ]
∥v(t)∥V.
Let u now be a classical solution to (23.32). Multiply the equation by v ∈C1
0(¯Ω)
and integrate in Ωto obtain, after integration by parts,
Z
Ω
∂u(x, t)
∂t
v(x) + ∇u(x, t) · ∇v(x)

dx =
Z
Ω
f (x, t)v(x)dx.

23.3 Parabolic Equations
645
Notice now that, as in the elliptic case, much less regularity is needed on the
solution and the test function (in this case in space and time) for the integrals
given above to make sense. This motivates the deﬁnition of a weak solution.
Deﬁnition 23.24 (weak solution). Let T ∈(0, ∞]. The function
u ∈L2(0, T; H1
0(Ω)),
∂u
∂t ∈L2  0, T; H1
0(Ω)′
is a weak solution of (23.32) if u( · , t) →u0 in L2(Ω) as t ↓0 and, for almost
every t ∈(0, T), we have
∂u( · , t)
∂t
, v

+
Z
Ω
∇u(x, t) · ∇v(x)dx = ⟨f ( · , t), v⟩,
∀v ∈H1
0(Ω),
where ⟨· , · ⟩denotes the duality pairing of H1
0(Ω).
The following result, concerning existence, essentially shows that the solution
representation (23.33) is a weak solution.
Theorem 23.25 (well-posedness). Let T ∈(0, ∞), the initial condition satisfy
u0 ∈L2(Ω), and the right-hand side satisfy f ∈L2(0, T; H1
0(Ω)′). Then there is
a unique weak solution to the IBVP (23.32). This solution is given by Duhamel’s
formula (23.37); moreover, it satisﬁes, for all t > 0,
∥u( · , t)∥2
L2(Ω) +
Z t
0
∥∇u( · , s)∥2
L2(Ω;Rd) ds ≤∥u0∥2
L2(Ω) +
Z t
0
∥f ( · , s)∥2
H1
0(Ω)′ ds.
If, in addition, we have u0 ∈H1
0(Ω) and f ∈L2(0, T; L2(Ω)), then, for all t > 0,
∥∇u( · , t)∥2
L2(Ω;Rd) +
Z t
0

∂u( · , s)
∂t

2
L2(Ω)
ds
≤∥∇u0∥2
L2(Ω;Rd) +
Z t
0
∥f ( · , s)∥2
L2(Ω) ds.
Proof. By linearity, we see that uniqueness follows from the ﬁrst estimate.
Existence is provided by Duhamel’s formula. Let us then show, at least formally,
the energy estimates. To achieve this, we set v = u( · , t) ∈H1
0(Ω) in the deﬁnition
of a weak solution and integrate in time to obtain
1
2
Z t
0
d
dt ∥u( · , s)∥2
L2(Ω) ds +
Z t
0
∥∇u( · , s)∥2
L2(Ω;Rd) ds +
Z t
0
⟨f ( · , s), u( · , t)⟩ds.
Clearly,
Z t
0
d
dt ∥u( · , s)∥2
L2(Ω) ds = ∥u( · , t)∥2
L2(Ω) −∥u0∥2
L2(Ω).
By deﬁnition of the dual norm and Young’s inequality,
Z t
0
⟨f ( · , s), u( · , t)⟩ds ≤
Z
∥f ( · , s)∥H1
0(Ω)′∥∇u( · , s)∥L2(Ω;Rd) ds
≤1
2
Z t
0
∥f ( · , s)∥2
H1
0(Ω)′ ds + 1
2
Z t
0
∥∇u( · , s)∥2
L2(Ω;Rd) ds.

646
Review of PDE Theory
Combining these bounds, we get the ﬁrst estimate.
The second estimate is obtained, formally, by setting v = ∂u( · ,t)
∂t
. The details
are left to the reader as an exercise; see Problem 23.18.
23.3.3
The Maximum Principle
As a ﬁnal property of solutions to parabolic equations, we will show a maximum
principle. We will consider a slighly more general version of (23.32). With the same
notation as in the previous section, we seek u such that









∂u(x, t)
∂t
−∆u(x, t) = f (x, t),
(x, t) ∈Ω× (0, T],
u(x, t) = g(x, t),
(x, t) ∈∂Ω× (0, T],
u(x, 0) = u0(x),
x ∈¯Ω,
(23.38)
where the initial condition u0, the right-hand side f , and the boundary condition g
are given. We need to introduce some terminology.
Deﬁnition 23.26 (parabolic boundary). The space–time cylinder for the IBVP
(23.38) is
C = Ω× (0, T).
The parabolic boundary of C is
∂pC = (∂Ω× [0, T]) ∪
 ¯Ω× {0}

.
The following result is known as the maximum principle. It provides rigor to our
intuition behind diﬀusion: it is the tendency of a substance to evenly spread.
Theorem 23.27 (maximum principle). Let the function v : ¯C →R be suﬃciently
smooth and, for (x, t) ∈Ω× (0, T), satisfy
∂v(x, t)
∂t
−∆v(x, t) ≤0.
Then the function u attains its maximum on the parabolic boundary ∂pC.
Proof. If this is not the case, the maximum is attained at some point (¯x, ¯t) ∈
Ω× (0, T], i.e.,
M = v(¯x, ¯t) = max
¯C
v > max
∂pC v = m.
Deﬁne ˜v(x, t) = v(x, t) + ε
2∥x∥2
2, where ε > 0 is suﬃciently small, so that ˜v also
attains its maximum on Ω× (0, T]. Indeed,
max
∂pC ˜v ≤m + ε
2 max
∂pC ∥x∥2
2 < M ≤max
¯C
v.
Since ∆(∥x∥2
2) = 2d, we have
∂˜v
∂t −∆˜v = ∂v
∂t −∆v −εd < 0.

23.4 Hyperbolic Equations
647
Let (y, s) ∈Ω× (0, T] be the point where ˜v attains its maximum. We must have
−∆˜v(y, s) ≥0,
and, if s < T,
∂˜v(y, s)
∂t
= 0,
or, if s = T,
∂˜v(y, s)
∂t
≥0.
In conclusion,
∂˜v(y, s)
∂t
−∆˜v(y, s) ≥0,
which is a contradiction.
The maximum principle gives us stability of continuous solutions.
Corollary 23.28 (stability). Let u be a classical solution to (23.38). If f ≡0, then
we have
∥u∥L∞(C) ≤max

∥g∥L∞(∂Ω×(0,T )), ∥u0∥L∞(Ω)
	
.
If f ̸= 0 and there is R > 0 for which Ω⊆B(0, R), then
∥u∥L∞(C) ≤max

∥g∥L∞(∂Ω×(0,T )), ∥u0∥L∞(Ω)
	
+ R2
2d ∥f ∥L∞(C).
Proof. See Problem 23.20.
23.4
Hyperbolic Equations
In this section, we will consider hyperbolic problems, the prototype of which is the
wave equation, which we presented in Example 23.5. We will consider, mostly, IVPs.
For reasons that will become evident later, we must begin the discussion with the
transport equation that we described in Section 23.1.3.
23.4.1
The Initial Value Problem for the Transport Equation
We begin our discussion with the Cauchy, or initial value, problem for the transport
equation. In other words, given T > 0, c ∈R\{0}, f : R × (0, T] →R, and
u0 : R →R, we seek u : R × [0, T] →R such that



∂u(x, t)
∂t
+ c ∂u(x, t)
∂x
= f (x, t),
(x, t) ∈R × (0, T],
u(x, 0) = u0(x),
x ∈R.
(23.39)
Let us deﬁne what we mean by a classical solution to this problem.
Deﬁnition 23.29 (classical solution). The function u ∈C1(R × [0, T]) is called a
classical solution to (23.39) if and only if the equation and initial condition hold
point-wise.

648
Review of PDE Theory
Let us consider, ﬁrst, the homogeneous case, i.e., f ≡0. The following result
gives us the existence, uniqueness, and stability of classical solutions.
Theorem 23.30 (well-posedness). Assume that f ≡0 and u0 ∈C1(R). Then there
is a classical solution u to the Cauchy problem (23.39). This solution is given by
u(x, t) = u0(x −ct).
(23.40)
If, in addition, there is p ∈[1, ∞] such that u0 ∈Lp(R), then, for any t > 0,
∥u( · , t)∥Lp(R) = ∥u0∥Lp(R) ,
which, in particular, implies uniqueness.
Proof. Existence is clear from (23.40) and the fact that the assumed smoothness
on the initial condition allows us to diﬀerentiate u to show that the equation is
satisﬁed point-wise.
To show the norm invariance property, for p < ∞, we multiply the equation by
p|u|p−2u to obtain
p|u(x, t)|p−2u(x, t)∂u(x, t)
∂t
= −cp|u(x, t)|p−2u(x, t)∂u(x, t)
∂x
,
or, equivalently,
∂|u(x, t)|p
∂t
= −c ∂|u(x, t)|p
∂x
,
which, integrating with respect to the x-variable, yields
d
dt
Z ∞
−∞
|u(x, t)|p dx

= −c
Z ∞
−∞
∂
∂x |u(x, t)|p dx
= −c

lim
xR→∞|u(xR, t)|p −
lim
xL→−∞|u(xL, t)|p

= 0,
where the last identity follows from the fact that, since u0 ∈Lp(R), we have
u(x, t) = u0(x −ct) →0 as x →±∞. Taking the limit p →∞of the norm
invariance yields the case p = ∞.
Finally, suppose that there are two classical solutions, u1 and u2. Then, by
linearity, v = u1 −u2 solves the same problem, but with the initial data u0 ≡
0 ∈L2(R). Therefore, using the norm invariance, we have, for any t ≥0,
∥v( · , t)∥L2(R) = ∥u0∥L2(R) = 0.
Uniqueness follows.
The solution of the homogeneous transport equation, given by (23.40), helps
illustrate the so-called method of characteristics, which we will use to, formally,
give a solution to (23.39) for an inhomogeneous right-hand side. We begin by
observing that we can introduce the characteristic equation
X′(s) = c,
X(t) = x
⇐⇒
X(s) = x + c(s −t),

23.4 Hyperbolic Equations
649
x
t
x0
t0
x0 −ct0
Figure 23.2 Domain of dependence for the transport equation. The solution to (23.39)
with c > 0 at point (x0, t0) depends only on the information in the shaded region.
so that, by setting x = X(s), the transport equation (23.39) can be written as
d
ds u(X(s), s + t) = ∂u(X(s), s + t)
∂t
+ c ∂u(x, s + t)
∂x
= f (X(s), s + t)
with u(X(0), 0) = u0(X(0)). The solution to this ODE is
u(x, t) = u(X(t), t)
= u(X(0), 0) +
Z t
0
f (X(s), s)ds
= u0(x −ct) +
Z t
0
f (x + c(s −t), s)ds.
(23.41)
Remark 23.31 (domain of dependence). Formulas (23.40) and (23.41) not only
give us an explicit solution representation but also show two important things. First,
(23.40) motivates the name of this equation, it transports or advects the initial
condition with speed c. Second, these formulas show that, as opposed to elliptic
or parabolic equations, there is a ﬁnite and well-deﬁned domain of dependence. In
other words, the value of the solution to (23.39) at a point (x, t) ∈R × (0, T)
depends only on the values of the initial condition and the right-hand side of the
cone depicted in Figure 23.2.
One important thing to note is that the solution representation (23.40) makes
sense even if u0 /∈C1(R). In this case, however, we cannot speak about a classical
solution, because the equation cannot hold point-wise. For this reason, we introduce
the notion of a weak solution of the transport equation.
Deﬁnition 23.32 (weak solution). A function u : R × (0, T) →R such that, for
every t > 0 and every bounded interval, I ⊂R u( · , t) ∈L1(I) is a weak solution
to (23.39) with f ≡0 if and only if

650
Review of PDE Theory
Z
R
Z T
0
u(x, t)
∂ϕ(x, t)
∂t
+ c ∂ϕ(x, t)
∂x

dtdx +
Z
R
u0(x)ϕ(x, 0)dx = 0
for every ϕ ∈C∞(R × [0, T]) such that there is M > 0 for which |x| + t > M
implies that ϕ(x, t) = 0.
We ﬁnish the discussion of the transport equation by noticing that, as opposed
to elliptic (see Theorem 23.12) or parabolic (see Theorem 23.19) equations, there
is no regularity theory to speak of. According to (23.40) or (23.41), the solution
to the transport equation is as good, or bad, as the inital condition is.
23.4.2
The Initial Value Problem for the Wave Equation
We are now ready to consider the IVP for the wave equation of Example 23.5. To
convey the essential ideas, we will focus on the one-dimensional case. Thus, given
T > 0, a > 0, and u0, v0 : R →R, we seek a function u : R × [0, T] →R such that











∂2u(x, t)
∂t2
−a∂2u(x, t)
∂x2
= 0,
(x, t) ∈R × (0, T],
u(x, 0) = u0(x),
x ∈R,
∂u(x, 0)
∂t
= v0(x),
x ∈R.
(23.42)
A classical solution is deﬁned as usual.
Deﬁnition 23.33 (classical solution). The function u ∈C2(R × [0, T]) is called a
classical solution to (23.42) if and only if the equation and initial condition hold
point-wise.
Before we embark on the study of existence of solutions, we will present an
energy-type estimate that shows that the solution to the wave equation has a very
well-deﬁned domain of dependence; see Figure 23.3.
Theorem 23.34 (energy estimate). Let u be a classical solution to (23.42). Given
(x0, t0) ∈R × (0, T], deﬁne the cone
K(x0, t0) =

(x, t) ∈R × [0, T]
 t ≤t0, |x −x0| ≤√a(t0 −t)
	
,
and, for t ∈[0, t0], its t-section
Bt(x0, t0) = {x ∈R | (x, t) ∈K(x0, t0)} .
Then, for every t ≤t0,
Z
Bt(x0,t0)
 
∂u(x, t)
∂t

2
+ a

∂u(x, t)
∂x

2!
dx ≤
Z x0+√at0
x0−√at0

|v0(x)|2 + a |u′
0(x)|2
dx.

23.4 Hyperbolic Equations
651
x
t
x0
t0
x0 −√at0
x0 + √at0
Bt(x0, t0)
n =
1
√1+a[√a, −1]⊺
n =
1
√1+a[√a, 1]⊺
Figure 23.3 Domain of dependence for the wave equation. The solution to (23.42) at
point (x0, t0) depends only on the information in the region K(x0, t0) (shaded).
A t-section of K(x0, t0) is denoted by Bt(x0, t0). The exterior unit normal to Ms is n.
Refer to Theorem 23.34 and its proof for notation.
Proof. Multiply the diﬀerential equation by 2 ∂u(x,t)
∂t
to obtain
0 = 2
∂2u(x, t)
∂t2
−a∂2u(x, t)
∂x2
 ∂u(x, t)
∂t
= ∂
∂t
 
∂u(x, t)
∂t

2
+ a

∂u(x, t)
∂x

2!
−2a ∂
∂x
∂u(x, t)
∂t
∂u(x, t)
∂x

.
Let t ≤t0. Integrating over {(x, s) ∈K(x0, t0) | s ≤t} and using the Divergence
Theorem, we obtain
0 =
Z t
0
Z
Bs
 ∂
∂t
 
∂u(x, s)
∂t

2
+ a

∂u(x, s)
∂x

2!
−2a ∂
∂x
∂u(x, s)
∂t
∂u(x, s)
∂x
 
dxds
=
Z
Bt
 
∂u(x, t)
∂t

2
+ a

∂u(x, t)
∂x

2!
dx −
Z x0+√at0
x0−√at0

|v0(x)|2 + a |u′
0(x)|2
dx
+
Z t
0
Z
Ms
"
nt
 
∂u(x, s)
∂t

2
+ a

∂u(x, s)
∂x

2!
−2anx
∂u(x, s)
∂t
∂u(x, s)
∂x
#
dxds,
where Ms(x0, t0) = {(x, t) ∈R × [0, T]| |x −x0| = √a(t0 −s)}, n = [nx, nt]⊺
denotes the unit exterior normal to K(x0, t0), and we suppressed the dependence

652
Review of PDE Theory
on (x0, t0) to shorten notation. Upon noticing that, on Ms(x0, t0), we have nt =
1
√
2
and n2
t = an2
x. Thus,
−2anx
∂u(x, s)
∂t
∂u(x, s)
∂x
≥−2a|nx|

∂u(x, s)
∂t


∂u(x, s)
∂x

= −2√ant

∂u(x, s)
∂t


∂u(x, s)
∂x

≥nt
 
∂u(x, s)
∂t

2
+ a

∂u(x, s)
∂x

2!
,
where, in the last step, we used the inequality 2ξη ≤ξ2 +η2. Having this estimate,
it is now clear that
Z t
0
Z
Ms
"
nt
 
∂u(x, s)
∂t

2
+ a

∂u(x, s)
∂x

2!
−2anx
∂u(x, s)
∂t
∂u(x, s)
∂x
#
dxds ≥0
and the claimed energy estimate immediately follows.
The previous result can be used to not only establish uniqueness but also give
a very precise description of the domain of dependence. If u0 = v0 ≡0 on
(x0 −√at0, x0 + √at0), then the solution to (23.42) will be zero on the whole
set K(x0, t0), in particular at (x0, t0). In other words, the value of u(x0, t0) is not
aﬀected by the values of the inital data outside of the interval (x0−√at0, x0+√at0).
Let us now show the existence of solutions by means of the so-called d’Alembert
formula.16
Theorem 23.35 (existence). Assume that u0 ∈C2(R) and v0 ∈C1(R). Then
problem (23.42) has a unique classical solution, which is given by
u(x, t) = 1
2

u0(x + √at) + u0(x −√at)

+
1
2√a
Z x+√at
x−√at
v0(y)dy.
(23.43)
Proof. Uniqueness follows from Theorem 23.34. Given the assumed regularity of
the initial data, verifying that (23.43) is indeed a classical solution is merely a
calculation. Let us here, instead, provide a formal derivation of this formula, which
will help elucidate the behavior of the solution.
Let us note that, at least formally, the PDE can be factored into
 ∂
∂t + √a ∂
∂x
  ∂
∂t −√a ∂
∂x

u(x, t) = 0.
Deﬁne
w(x, t) =
 ∂
∂t −√a ∂
∂x

u(x, t)
to see that w must be a solution of
∂w(x, t)
∂t
+ √a∂w(x, t)
∂x
= 0.
16 Named in honor of the French mathematician Jean-Baptiste le Rond d’Alembert
(1717–1783).

23.4 Hyperbolic Equations
653
By (23.40),
w(x, t) = w0(x −√at),
where w0(x) = w(x, 0). With this at hand, we see that
∂u(x, t)
∂t
−√a∂u(x, t)
∂x
= w(x, t) = w0(x −√at),
which, by (23.41), gives us the solution to (23.42) in the form
u(x, t) = u0(x + √at) +
Z t
0
w0(x −√a(s −t) −√as)ds
= u0(x + √at) +
Z t
0
w0(x + √at −2√as)ds
= u0(x + √at) +
1
2√a
Z x+√at
x−√at
w0(y)dy,
where, in the last step, we applied the change of variables y = x + √at −2√as.
To conclude, we must ﬁnd the function w0. Notice that
w0(x) = w(x, 0) = ∂u(x, 0)
∂t
−√a∂u(x, 0)
∂x
= v0(x) −√au′
0(x).
Substituting,
u(x, t) = u0(x + √at) +
1
2√a
Z x+√at
x−√at

v0(y) −√au′
0(y)

dy
= u0(x + √at) +
1
2√a
Z x+√at
x−√at
v0(y)dy −1
2

u0(x + √at) −u0(x −√at)

= 1
2

u0(x + √at) + u0(x −√at)

+
1
2√a
Z x+√at
x−√at
v0(y)dy,
as we claimed.
We comment that similar representation formulas exist in more dimensions
d > 1. For d = 2, this bears the name of the Poisson formula,17 whereas, for
d = 3, this is known as the Kirchhoﬀformula.18
Similar to the IVP for the heat equation, we have Duhamel’s formula for the
solution of the inhomogeneous IVP for the wave equation: given T > 0,u0, v0 : R →
R, f : R × (0, T] →R, ﬁnd u : R × [0, T] →R such that











∂2u(x, t)
∂t2
−a∂2u(x, t)
∂x2
= f (x, t),
(x, t) ∈R × (0, T],
u(x, 0) = u0(x),
x ∈R,
∂u(x, 0)
∂t
= v0(x),
x ∈R.
17 Named in honor of the French mathematician, engineer, and physicist Baron Sim´eon Denis
Poisson (1781–1840).
18 Named in honor of the German physicist Gustav Robert Kirchhoﬀ(1824–1887).

654
Review of PDE Theory
The notion of a classical solution is as before. A formal representation formula is
given by
u(x, t) = 1
2

u0(x + √at) + u0(x −√at)

+
1
2√a
Z x+√at
x−√at
v0(y)dy
+
1
2√a
Z t
0
Z x+√as
x−√as
f (y, t −s)dyds.
(23.44)
Remark 23.36 (domain of dependence). The representation formulas (23.43) and
(23.44) illustrate an important point about the solution of the IVP for the wave
equation. The solution at a point (x0, t0) only depends on the initial data and
right-hand side within the cone K(x0, t0) deﬁned in Theorem 23.34; see Figure
23.3.
Remark 23.37 (regularity). Notice that, as in the case of the transport equation,
there is no regularity gain from the regularity of the initial data or right-hand side.
This, again, is in stark contrast to Theorems 23.12 and 23.19.
23.4.3
The Initial Boundary Value Problem for the Wave Equation
Let us now study the IBVP for the wave equation. We let d ∈N, Ω⊂Rd be
a bounded domain with suﬃciently smooth boundary, T > 0, and a > 0. Given
u0 : ¯Ω→R, v0 : ¯Ω→R, and f : Ω× (0, T] →R, we seek u : ¯Ω× [0, T] →R such
that

















∂2u(x, t)
∂t2
−a∆u(x, t) = f (x, t),
(x, t) ∈Ω× (0, T],
u(x, t) = 0,
x ∈∂Ω× (0, T],
u(x, 0) = u0(x),
x ∈Ω,
∂u(x, 0)
∂t
= v0(x),
x ∈Ω.
(23.45)
As is the case with the heat equation, while it is possible to provide a notion of
classical solutions and develop a theory for them, it is more instructive to introduce
the notion of a weak solution.
Deﬁnition 23.38 (weak solution). Let T > 0. The function
u ∈L2(0, T; H1
0(Ω)),
∂u
∂t ∈L2(0, T; L2(Ω)),
∂2u
∂t2 ∈L2(0, T; H1
0(Ω)′)
is a weak solution to (23.45) if and only if, for almost every t ∈(0, T), we have
∂2u( · , t)
∂t2
, v

+ a
Z
Ω
∇u(x, t) · ∇v(x)dx =
Z
Ω
f (x, t)v(x)dx,
∀v ∈H1
0(Ω),
u( · , t) →u0 in L2(Ω), and ∂u( · ,t)
∂t
→v0 in H1
0(Ω)′.
The motivation for this deﬁnition is as usual; see Problem 23.25. Let us show
the uniqueness of weak solutions via the so-called energy conservation property for
(23.45).

23.4 Hyperbolic Equations
655
Theorem 23.39 (conservation). Let u be a weak solution to (23.45) with f ≡0.
The quantity
E(t) =
Z
Ω
"
∂u(x, t)
∂t

2
+ a ∥∇u(x, t)∥2
2
#
dx
is constant in time, i.e., E(t) = E(0) for all t ∈(0, T]. As a consequence, weak
solutions are unique.
Proof. We leave the proof of uniqueness to the reader as an exercise; see Problem
23.27.
To prove the energy conservation property, we formally set, in the deﬁnition of
a weak solution, v = ∂u( · ,t)
∂t
to obtain
1
2
d
dt
Z
Ω
"
∂u(x, t)
∂t

2
+ a ∥∇u(x, t)∥2
2
#
dx = 0
from which energy conservation follows.
To prove existence, we follow a similar approach to that used for the heat
equation, namely an eigenvalue expansion.
Theorem 23.40 (existence). Let a > 0, T > 0, u0 ∈H1
0(Ω), v0 ∈L2(Ω), and
f ∈L2(0, T; L2(Ω)). In this setting, problem (23.45) has a weak solution, which,
in addition, for all t ∈[0, T], satisﬁes

∂u( · , t)
∂t

2
L2(Ω)
+ a ∥∇u( · , t)∥2
L2(Ω;Rd)
≤C

∥∇u0∥2
L2(Ω;Rd) + ∥v0∥2
L2(Ω) +
Z t
0
∥f ( · , s)∥2
L2(Ω) ds

for some constant that depends on T, but not the rest of the problem data.
Proof. We propose the following solution ansatz:
u(x, t) =
∞
X
k=1
uk(t)ϕk(x),
where the functions {ϕk}∞
k=1 are the eigenfunctions of the Laplacian, which we can
use owing to Theorem 23.14. Thus, by their orthonormality in L2(Ω), we obtain,
for each k ≥1, the following ODE:
u′′
k(t) + aλkuk(t) = fk(t),
uk(0) = u0,k,
u′
k(0) = v0,k,
where
u0(x) =
∞
X
k=1
u0,kϕ(x),
v0(x) =
∞
X
k=1
v0,kϕ(x),
f (x, t) =
∞
X
k=1
fk(t)ϕk(x).
Thus,
uk(t) = u0,k cos(
p
aλkt) + v0,k
√aλk
sin(
p
aλkt)
+
1
√aλk
Z t
0
fk(s) sin
p
aλk(t −s)

ds.

656
Review of PDE Theory
The assumed regularity on the problem data guarantees that the series represen-
tations converges and, moreover, gives the claimed estimate.
We conclude our discussion with a regularity result; see [31] for a proof.
Theorem 23.41 (regularity). Assume that the boundary of Ωis suﬃciently smooth,
u0 ∈H2(Ω), v0 ∈H1
0(Ω), and ∂f
∂t ∈L2(0, T; L2(Ω)). Then the weak solution of
(23.45) satisﬁes
u ∈L∞(0, T; H2(Ω)),
∂u
∂t ∈L∞(0, T; H1
0(Ω)),
∂2u
∂t2 ∈L∞(0, T; L2(Ω)),
∂3u
∂t3 ∈L2(0, T; H1
0(Ω)′).
23.4.4
Hyperbolic Systems
As a last topic in this (not so) short review of PDE theory, we consider the
Cauchy problem for so-called symmetric hyperbolic or Friedrichs systems.19 This is
a generalization of the Cauchy problem (23.39) for the transport equation. Let
d, m ∈N. Here, d will be the spatial dimension, whereas m will denote the
size of our system. We assume that we are given a ﬁnal time T > 0, an initial
condition u0 : Rd →Rm, a forcing function f : Rd × (0, T] →Rm, and a collection
{Aj}d
j=1 ⊂Rm×m
sym . We seek u : Rd × [0, T] →Rm such that







∂u(x, t)
∂t
+
d
X
j=1
Aj
∂u(x, t)
∂xj
= f (x, t),
(x, t) ∈Rd × (0, T],
u(x, 0) = u0(x),
x ∈Rd.
(23.46)
Let us give a deﬁnition.
Deﬁnition 23.42 (hyperbolicity). For y ∈Rd, deﬁne
B(y) =
d
X
j=1
[y]jAj ∈Rm×m.
We say that problem (23.46) is symmetric hyperbolic if, for every y ∈Rd, the
matrix B(y) is diagonalizable, i.e., it has m real eigenpairs {λj(y), q(y)}m
j=1 ⊂
R × Rm,
λ1(y) ≤· · · ≤λm(y),
and the eigenvectors {q(y)}m
j=1 form a basis of Rm. If, in addition, all the
eigenvalues are distinct, i.e.,
λ1(y) < · · · < λm(y),
then we say that the problem is strictly hyperbolic.
19 Named in honor of the German–American mathematician Kurt Otto Friedrichs (1901–1982).

23.4 Hyperbolic Equations
657
Before we proceed with any theory, we present some examples of systems that
ﬁt this framework.
Example 23.10
The wave equation in one dimension is actually a symmetric
strictly hyperbolic system. Indeed, let us recall that, for a > 0, the one-dimensional
wave equation reads
∂2u(x, t)
∂t2
= a∂2u(x, t)
∂x2
.
Deﬁne the variable
u =
u1
u2

: R × [0, T] →R2,
via
u1(x, t) = ∂u(x, t)
∂t
,
u2(x, t) = √a∂u(x, t)
∂x
.
Then, using the wave equation, it follows that
∂u(x, t)
∂t
=
"
∂2u(x,t)
∂t2
√a ∂2u(x,t)
∂t∂x
#
=
"
a ∂2u(x,t)
∂x2
√a ∂2u(x,t)
∂x∂t
#
= ∂
∂x
 0
√a
√a
0
 "
∂u(x,t)
∂t
√a ∂u(x,t)
∂x
#
=
 0
√a
√a
0
 ∂u(x, t)
∂x
.
This is a symmetric hyperbolic system with
A =
 0
−√a
−√a
0

.
The initial condition is given by
u0(x) =
"
v0(x)
√au′
0(x)
#
.
Example 23.11
Maxwell’s equations20 describe the evolution of an electromag-
netic ﬁeld in the vaccuum. Upon choosing suitable units, they read
20 Named in honor of the British scientist James Clerk Maxwell (1831–1879).

658
Review of PDE Theory

















∂B(x, t)
∂t
+ ∇× E(x, t) = 0,
∂E(x, t)
∂t
−∇× B(x, t) + J(x, t) = 0,
∇· B(x, t) = 0,
∇· E(x, t) = ρ(x, t),
and are supplemented with initial conditions for E and B. The unknowns in these
equations are the electric, E : R3 ×(0, ∞) →R3, and magnetic, B : R3 ×(0, ∞) →
R3, ﬁelds. The quantities ρ: R3 × (0, ∞) →R and J : R3 × (0, ∞) →R3 are the
charge and current density, respectively, and they are related by
∂ρ(x, t)
∂t
+ ∇· J(x, t) = 0.
In these equations, we introduced the curl or rotor of a vector ﬁeld. If v ∈
C1(R3; R3), then
∇× v(x) = det


e1
e2
e3,
∂
∂x1
∂
∂x2
∂
∂x3
v1(x)
v2(x)
v3(x)

=


∂v3(x)
∂x2
−∂v2(x)
∂x3
∂v1(x)
∂x3
−∂v3(x)
∂x1
∂v2(x)
∂x1
−∂v1(x)
∂x2

.
This is also a hyperbolic system. While this is true in general, we only show this in
the so-called 2 1
2-dimensional case, i.e., when E and B are independent of x3, so
that d = 2 and m = 3. Upon introducing
U =


E1
E2
B3

,
V =


B1
B2
E3

: R2 × (0, ∞) →R3,
we obtain that (see Problem 23.28)
∂U(x, t)
∂t
+ A1
∂U(x, t)
∂x1
+ A2
∂U(x, t)
∂x2
= f (x, t)
with
A1 =


0
0
0
0
0
1
0
1
0

,
A2 =


0
0
−1
0
0
0
−1
0
0

,
f (x, t) =


−J1(x, t)
−J2(x, t)
0

.
Similarly,
∂V (x, t)
∂t
+ B1
∂V (x, t)
∂x1
+ B2
∂V (x, t)
∂x2
= g(x, t)
with
B1 =


0
0
0
0
0
−1
0
−1
0

,
B2 =


0
0
1
0
0
0
1
0
0

,
g(x, t) =


0
0
−J3(x, t)

.

23.4 Hyperbolic Equations
659
Example 23.12
Let us consider a general symmetric hyperbolic system in one
dimension, d = 1, but we allow m ∈N



∂u(x, t)
∂t
+ A∂u(x, t)
∂x
= f (x, t),
(x, t) ∈R × (0, T],
u(x, 0) = u0(x),
x ∈R.
Since A is diagonalizable, there is an invertible matrix X ∈Rm×m and a diagonal
matrix Λ = diag(λ1, . . . , λm) ∈Rm×m such that
AX = XΛ.
Deﬁne
w = X−1u : R × [0, T] →Rm.
Then



∂w(x, t)
∂t
+ Λ∂w(x, t)
∂x
= g(x, t),
(x, t) ∈R × (0, T],
w(x, 0) = w 0(x),
x ∈R,
with g = X−1f and w 0 = X−1u0. Notice that, since Λ is diagonal, this is nothing
but a collection of m independent Cauchy problems for a linear transport equation.
We refer the reader to [23] and [29, Chapter 7] for several other examples of
models that can be written as Friedrichs systems.
As Example 23.12 has shown, to obtain a theory of existence and uniqueness for
(23.46) for d = 1, all that is needed is a change of coordinates. The case d > 1
can be treated with techniques that we have developed before.
For instance, we can obtain the existence of solutions via the Fourier transform.
Take, component-wise, the Fourier transform of (23.46) to obtain, using Proposi-
tion 23.16, that



dˆu(ξ, t)
dt
+ iB(ξ)ˆu(ξ, t) = ˆf (ξ, t),
t ∈(0, T],
ˆu(0; ξ) = ˆu0(ξ).
Thus, at least formally, the solution is given by the inverse Fourier transform of
ˆu(ξ, t) = exp(−itB(ξ))ˆu0(ξ) +
Z t
0
exp(i(s −t)B(ξ))ˆf (ξ, t)ds.
In addition, we can use energy methods to provide suitable a priori estimates.
Proposition 23.43 (energy estimates). Let T > 0 and d, m ∈N. Assume that
f ∈L∞(0, T; L2(Rd; Rm)) and u0 ∈L2(Rd; Rm). Then any suﬃciently smooth
solution to (23.46), which decays suﬃciently fast at inﬁnity, satisﬁes
∥u∥L∞(0,T ;L2(Rd;Rm)) ≤C

∥f ∥L∞(0,T ;L2(Rd;Rm)) + ∥u0∥L2(Rd;Rm)

,
where the constant C depends on T.

660
Review of PDE Theory
Proof. Take the L2(Rd; Rm) inner product of the equation with u(x, t) to obtain
1
2
d
dt ∥u( · , t)∥2
L2(Rd;Rm)+
d
X
j=1
Z
Rd

Aj
∂u(x, t)
∂xj

·u(x, t)dx =
Z
Rd f (x, t)·u(x, t)dx.
Notice now that, since Aj is symmetric and does not depend on x,

Aj
∂u(x, t)
∂xj

· u(x, t) = ∂u(x, t)
∂xj
· Aju(x, t).
Therefore, since we assumed that u decays suﬃciently fast at inﬁnity,
Z
Rd Aj
∂u(x, t)
∂xj
· u(x, t)dx = 1
2
Z
Rd
∂
∂xj
[(Aju(x, t)) · u(x, t)] dx = 0.
Consequently, we have obtained that
1
2
d
dt ∥u( · , t)∥2
L2(Rd;Rm) ≤1
2 ∥f ( · , t)∥2
L2(Rd;Rm) + 1
2 ∥u( · , t)∥2
L2(Rd;Rm) .
Deﬁne K1 = 1,
Φ(t) = 1
2

∥u( · , t)∥2
L2(Rd;Rm) −∥u0∥2
L2(Rd;Rm)

,
K2 = 1
2

∥f ( · , t)∥2
L∞(0,T ;L2(Rd;Rm)) + ∥u0∥2
L2(Rd;Rm)

.
The previous estimates then show that
Φ′(t) ≤K1Φ(t) + K2,
which, by Gr¨onwall’s inequality of Lemma 17.7, implies the claimed estimate. The
constant C depends (exponentially) on T.
Problems
23.1
Show (23.16).
23.2
Show that the Frobenius inner product is indeed an inner product.
23.3
Let m ∈N and consider Rm×m with the Frobenius inner product. Deﬁne
S =

A ∈Rm×mA = A⊺	
,
H =

A ∈Rm×mA = −A⊺	
.
a)
Show that S, H ≤Rm×m.
b)
Show that S and H are complementary subspaces.
c)
Show that S and H are orthogonal subspaces.
d)
Can you describe the projection Rm×m →S?
23.4
Let D be a linear, second-order partial diﬀerential operator on Rm, for some
m ∈N, that has variable, but continuous, coeﬃcients.
a)
Show that if the operator is elliptic (hyperbolic) at a point, then it is elliptic
(hyperbolic) in a neighborhood of this point.
b)
Does the previous statement hold for a parabolic operator?
c)
Let x, y ∈Rm with x ̸= y. Show that if D is elliptic at x and hyperbolic at y,
then there must be a third point where the equation is parabolic.

Problems
661
23.5
Prove Corollary 23.5.
23.6
Prove Corollary 23.7.
23.7
Consider the one-dimensional, nonlinear BVP
−u′′ + u = eu,
x ∈(0, 1),
u(0) = u(1) = 0.
Show that any solution to this problem is nonnegative, i.e., u(x) ≥0 for all x ∈
[0, 1].
23.8
Consider the divergence form operator (23.20), where A ∈C1(¯Ω; Rd×d),
b ∈C(¯Ω; Rd), and c ∈C(¯Ω). Assume that there are constants λ, Λ ∈R+ such
that, for all points x ∈Rd, the matrix A(x) is symmetric and
σ(A(x)) ⊂[λ, Λ].
Show that this operator is elliptic, in the sense of Deﬁnition 23.1.
23.9
Consider the Dirichlet problem (23.19) with g = 0 and L = −∆. Show that
if u ∈C2(Ω) ∩C(¯Ω) is a classical solution to this problem, then it is also a weak
solution. Conversely, show that if u ∈H1
0(Ω) is a weak solution, which happens to
be suﬃciently smooth, then it is also a classical solution.
23.10
Consider two bounded domains Ω1 and Ω2 with a common boundary S.
Let Γi = ∂Ωi\S for i = 1, 2. Give a variational formulation of the problem: for
i = 1, 2, ﬁnd ui : Ωi →R such that
−a1∆u1 = f1,
in Ω1,
−a2∆u2 = f2,
in Ω2,
u1 = 0,
on Γ1,
u2 = 0,
on Γ2,
and, on S,
u1 = u2,
J(a1∇u1 −a2∇u2) · nK = 0.
Here, a1, a2 are positive constants, fi ∈L2(Ωi), and n is a unit normal to S.
23.11
Give a variational formulation of the problem
−∆u = f ,
in Ω,
∇u · n + u = g,
on ∂Ω.
Here, f ∈L2(Ω) and g ∈L2(∂Ω). Use the Friedrichs inequality to show that this
problem has a unique weak solution in H1(Ω).
23.12
Let i = 1, 2 and ui ∈H1
0(Ω) be weak solutions of
−∇· (Ai∇ui) = f ,
in Ω,
u = 0,
on ∂Ω,
where the matrices Ai are SPD and σ(Ai) ⊂[λ, Λ]. Show that
∥u1 −u2∥H1
0(Ω) ≤CP
λ2 ∥A1 −A2∥L∞(Ω;Rd×d)∥f ∥L2(Ω).
23.13
Prove identity (23.26).
Hint: Use the Fourier transform of the Gaussian.
23.14
Complete the proof of Theorem 23.17.
23.15
Prove (23.30) for d = 1. Use this to deduce that there is a constant C > 0
such that
|P(y)e−y 2| ≤Ce−y 2/2,
∀y > 0.

662
Review of PDE Theory
23.16
Let
u(x, t) =
(
xt−3/2e−x2/4t,
t > 0,
0,
t = 0.
a)
Show that, for all x ∈R,
u(x, t) →0,
t ↓0.
b)
Show that this function solves (23.24) with d = 1 and f ≡0.
c)
Why is this not a counterexample to uniqueness?
Hint: Set x = t.
23.17
Prove (23.36).
23.18
Let u be a weak solution to (23.32) with u0 ∈H1
0(Ω). Assume that f ∈
L2(0, T; L2(Ω)). Show that
∥∇u( · , t)∥2
L2(Ω;Rd)+
Z t
0

∂u( · , s)
∂t

2
L2(Ω)
ds ≤∥∇u0∥2
L2(Ω;Rd)+
Z t
0
∥f ( · , s)∥2
L2(Ω) ds.
23.19
Let T ∈(0, ∞), u be a weak solution to (23.32) with u0 ∈H1
0(Ω), and
f ∈L2(0, T; L2(Ω)). Show that there is a constant C = C(T) such that, for every
t ∈(0, T],
Z t
0
s

∂u( · , s)
∂t

2
L2(Ω)
ds ≤C

∥v∥2
L2(Ω) +
Z t
0
∥f ( · , s)∥2
L2(Ω) ds

,
∥∇u( · , t)∥2
L2(Ω;Rd) ≤Ct−1

∥v∥2
L2(Ω) +
Z t
0
∥f ( · , s)∥2
L2(Ω) ds

.
23.20
Prove Corollary 23.28.
23.21
Consider the homogeneous IBVP for the heat equation with Neumann
boundary conditions, i.e.,









∂u(x, t)
∂t
−∆u(x, t) = 0,
(x, t) ∈Ω× (0, T],
∇u(x, t) · n = 0,
(x, t) ∈∂Ω× (0, T],
u(x, 0) = u0(x),
x ∈¯Ω.
Show that, for all t > 0,
1
|Ω|
Z
Ω
u(x, t)dx = 1
|Ω|
Z
Ω
u0(x)dx.
23.22
Show that (23.41) indeed solves (23.39). Give suﬃcient conditions on u0
and f , so that this is a classical solution.
23.23
Derive the notion of a weak solution, in the sense of Deﬁnition 23.32, for
the transport equation. That is, multiply the equation by ϕ, integrate, and integrate
by parts to arrive at the desired identity.
23.24
Show that if u0 ∈L1(R), then (23.40) is a weak solution, in the sense of
Deﬁnition 23.32, of (23.39) with f ≡0.

Problems
663
23.25
Derive the notion of a weak solution, in the sense of Deﬁnition 23.38, for
(23.45). That is, multiply the equation by a suﬃciently smooth function ϕ that
vanishes on the boundary, integrate, and integrate by parts to arrive at the desired
identity.
23.26
Let u be a weak solution, in the sense of Deﬁnition 23.38, for (23.45) with
f ≡0. Show that if u0 ∈H2(Ω) ∩H1
0(Ω) and v0 ∈H1
0(Ω), then, for t > 0, we
have
∥u(t)∥L2(Ω) ≤C
 ∥u0∥L2(Ω) + ∥v0∥L2(Ω)

,
∥∇u(t)∥L2(Ω;Rd) ≤C
 ∥∇u0∥L2(Ω;Rd) + ∥v0∥L2(Ω)

,
∥∆u(t)∥L2(Ω) ≤C
 ∥∆u0∥L2(Ω) + ∥∇v0∥L2(Ω;Rd)

,
∥u(t) −u0∥L2(Ω) ≤Ct
 ∥∇u0∥L2(Ω;Rd) + ∥v0∥L2(Ω)

,

∂u( · , t)
∂t
−v0

L2(Ω)
≤Ct
 ∥∆u0∥L2(Ω) + ∥∇v0∥L2(Ω;Rd)

.
23.27
Complete the proof of Theorem 23.39.
23.28
Provide all the details for Example 23.11.

24
Finite Diﬀerence Methods
for Elliptic Problems
In this chapter, we explore ﬁnite diﬀerence methods (FDMs) for solving elliptic
boundary value problems (BVPs). To present the essential ideas, without obscuring
the discussion with technical details, we will assume that our grids are uniform; the
domain is simple, say Ω= (0, 1)d, with d ∈N; and we will mostly discuss the one-
(d = 1) and two- (d = 2) dimensional cases.
As the name suggests, the main idea behind FDMs is to replace the derivatives
appearing in a BVP by diﬀerences of values at nearby points, thus reducing the BVP
to a system of algebraic equations (for the point values). This seems very natural.
After all, if a function v : Rd →R is suﬃciently smooth, for any i = 1, . . . , d, we
have
∂v(x)
∂xi
= lim
h→0
v(x + hei) −v(x)
h
,
where ei is the ith canonical basis vector in Rd, thus we can propose the following
ﬁnite diﬀerence approximations of this partial derivative:
v(x + hei) −v(x)
h
,
v(x) −v(x −hei)
h
,
h > 0.
Evidently, these are not the only two possibilities. For instance, the reader can easily
verify that, for a suﬃciently smooth function v, the quantities
v(x + hei) −v(x −hei)
2h
,
3v(x) −4v(x −hei) + v(x −2hei)
2h
converge, as h ↓0, to this partial derivative. Which ﬁnite diﬀerence approximation
must be used then? The answer to this comes from trying to satisfy two, often
contradictory, requirements, as follows.
• Consistency: We wish the ﬁnite diﬀerence approximation to be as accurate as
possible. The way this is usually veriﬁed is by assuming that the function v is
suﬃciently smooth. If this is the case, a Taylor expansion can show that, for
instance,
v(x + hei) −v(x)
h
= ∂v(x)
∂xi
+ O(h),
v(x + hei) −v(x −hei)
2h
= ∂v(x)
∂xi
+ O(h2),
as h ↓0. For this reason, we say that the ﬁrst ﬁnite diﬀerence approximation
is consistent to order one, whereas the second one is consistent to order two.

24.1 Grid Functions and Finite Diﬀerence Operators
665
From this consideration alone, it seems that we need to ﬁnd a ﬁnite diﬀerence
approximation of the derivatives that is consistent to as high order as possible.
• Stability: We are not trying to approximate the action of a derivative, but the
solution to a BVP. For this reason, the solution of our FDM must be stable,
not only with respect to perturbations of the data but, more importantly, in
(discrete versions of) the same norms in which the solution to the continuous
problem possesses its own stability properties. This will allow us to compare the
continuous and discrete solutions (via consistency) and establish convergence.
It turns out that, at least for linear problems, the satisfaction of these two
requirements is enough to obtain convergence. In the literature, this is commonly
known as Lax’s Principle:1
consistency + stability =⇒convergence.
This is one of the most fundamental mantras in the numerical approximation
of diﬀerential equations. For instance, the Dahlquist Equivalence Theorem 20.26
is stating precisely this fact. Of course, at this stage, for us this is more a
guiding principle than a rigorous statement, as we have not properly deﬁned any of
these three notions. The development, and justiﬁcation, of these ideas for elliptic
problems will be the main focus in this chapter.
As a ﬁnal comment, we mention that here we will present the easiest and most
elementary incarnation of the theory of ﬁnite diﬀerences, where we assume that the
problem data are suﬃciently smooth, and we are concerned with the approximation
of classical solutions. We refer the reader to [50], [56, Chapter VI], and [79] for
the study of FDMs that are able to handle nonsmooth data and approximate weak
solutions.
24.1
Grid Functions and Finite Diﬀerence Operators
Before we develop the approximation of BVPs via FDMs, we must introduce some
notations and notions regarding grid functions in Rd, d ∈N. Thus, we let N ∈N,
set h = 1/(N + 1), called the grid size, and deﬁne the uniform grid
Zd
h =

hz ∈Rd  z ∈Zd	
.
Deﬁnition 24.1 (grid function). A grid domain in Rd is any nonempty set Gh ⊆Zd
h.
The points x ∈Gh are the nodes or grid points. A grid function on Gh is any
function v : Gh →R. For a grid function v and hi ∈Gh, we set vi = v(hi). The set
of grid functions on Gh is denoted by
V(Gh) = {v | v : Gh →R} .
1 Named in honor of the Hungarian-born American mathematician Peter David Lax (1926–).

666
Finite Diﬀerence Methods for Elliptic Problems
i = 1
j = 1
1
5
9
13
i = 2
j = 2
2
6
10
14
i = 3
j = 3
3
7
11
15
i = 4
j = 4
4
8
12
16
i = 0
j = 0
j = 5
i = 5
Figure 24.1 A uniform grid of size 5 × 5 and spacing h = 1
5 covering Ω= (0, 1)2. The
discrete interior, Ωh, is the collection of ﬁlled circles, and the discrete boundary, ∂Ωh,
consists of the remaining ones. The space of grid functions, V(¯Ωh), is the space of
functions from all points in this grid into R. The numbers next to the ﬁlled circles denote
their lexicographical ordering, which is used to provide the isomorphism V(¯Ωh) ←→R16.
The ﬁlled circles constitute the domain ΩI
h for the two-dimensional discrete Laplacian of
Example 24.7, whereas the unﬁlled circles constitute ΩB
h . Note that, although the
unﬁlled squares on the four corner points belong to ¯Ωh, they are not used in a ﬁnite
diﬀerence approximation of the Poisson problem with Dirichlet boundary conditions.
This is a vector space. If #Gh < ∞, this space is ﬁnite dimensional; otherwise, it is
inﬁnite dimensional. In a similar manner, if V is a, ﬁnite-dimensional, vector space,
we set
V(Gh; V) = {v | v : Gh →V} .
Example 24.1
Let d = 1 and Ω= (0, 1) be the interval where we will usually
consider our elliptic BVPs. For N ∈N and h = 1/(N + 1), we set
¯Ωh = ¯Ω∩Zh = {hi | i = 0, . . . , N + 1} .

24.1 Grid Functions and Finite Diﬀerence Operators
667
Notice that, in this case, V(¯Ωh) is nothing but the collection of functions
v : {0, . . . , N + 1} →R.
Notice also that ∂Ω= {0, 1} and, similarly, we can deﬁne the discrete boundary
and discrete interior of ¯Ωh, respectively, as
∂Ωh = ∂Ω∩Zh,
Ωh = ¯Ωh\∂Ωh.
Example 24.2
The previous example can be easily generalized to any d ∈N. For
instance, if Ω= (0, 1)2, we can deﬁne
¯Ωh = ¯Ω∩Z2
h = {(hi, hj) | i, j = 0, . . . , N + 1} .
The discrete interior is then
Ωh = Ω∩Z2
h = {(hi, hj) | i, j = 1, . . . , N} .
The discrete boundary can also be deﬁned accordingly; see Figure 24.1. The space
V(¯Ωh) is deﬁned accordingly.
Since we will be mostly concerned with approximating BVPs in Ω= (0, 1)d with
d ∈N, we introduce the following notation.
Deﬁnition 24.2 (space V0(¯Ωh)). Let d ∈N, Ω= (0, 1)d, N ∈N, h = 1/(N + 1),
and ¯Ωh = ¯Ω∩Zd
h. To treat Dirichlet boundary conditions, we also introduce the
space of grid functions that vanish on the boundary
V0(¯Ωh) =

v ∈V(¯Ωh)
 vi = 0, ∀hi ∈∂Ωh
	
.
There is another natural way to understand functions in V(¯Ωh), which, depending
on the context, may also prove useful.
Proposition 24.3 (isomorphism). Let d, N ∈N, h =
1
N+1, Ω= (0, 1)d, and ¯Ωh =
¯Ω∩Zd
h. There is a one-to-one correspondence between the space of grid functions
V(Ωh) and RNd. Likewise, there is a one-to-one correspondence between the space
of grid functions V0(¯Ωh) and RNd. Therefore, dim(V(Ωh)) = dim(V0(¯Ωh)) = Nd.
Proof. Let d = 1. We make the canonical correspondence. If v ∈V(Ωh), then
deﬁne v ∈RN component-wise via
[v]i = vi,
ih ∈Ωh.
Conversely, if v ∈RN is given, deﬁne the grid function v ∈V(Ωh) via
vi = [v]i .
The grid functions with zero values on the boundary (those from V0(¯Ωh)) are
handled similarly.
To illustrate the general procedure, consider now d = 2. If v ∈V(Ωh), then
deﬁne v ∈RN2 component-wise via
[v]i+(j−1)N = vi,j,
(ih, jh) ∈Ωh.

668
Finite Diﬀerence Methods for Elliptic Problems
Conversely, if v ∈RN2 is given, deﬁne the grid function v ∈V(Ωh) component-wise
via
vi,j = [v]i+(j−1)N ,
(ih, jh) ∈Ωh.
Figure 24.1 shows an illustration of this equivalence, which is commonly called
lexicographical ordering.
Remark 24.4 (convention). Because of this canonical (and trivial) correspondence,
we need not be too careful about whether we are talking about a grid function or
its vector representation. We will often write
v ∈RNd ←→v ∈V(Ωh),
v ∈RNd ←→v ∈V0(¯Ωh)
to express the fact that our object may be viewed in either setting. We use the
convention of denoting a grid function by a lowercase Greek or Roman character
and its corresponding canonical vector representative by the boldface of the same
Greek or Roman character.
Having introduced spaces of grid functions, we can deﬁne basic operators on
them.
Deﬁnition 24.5 (shift operator). Let d ∈N and e ∈Zd. The shift operator in
the direction of e,
Se : V(Zd
h) →V(Zd
h),
is given, for x = hz ∈Zd
h, by
(Sev)(x) = v(x + he),
(Sev)z = vz+e.
Proposition 24.6 (properties of shifts). Let d ∈N. For any e ∈Zd, the shift
operator satisﬁes Se ∈L(V(Zd
h)). Moreover, this operator is invertible and its
inverse is given by
S−1
e
= S−e.
In particular, S0 is the identity operator.
Proof. See Problem 24.1.
Shifts will be the building blocks of what we will call ﬁnite diﬀerence operators,
which, simply put, will be linear combinations of shifts.
Deﬁnition 24.7 (ﬁnite diﬀerence operator). The mapping
Fh : V(Zd
h) →V(Zd
h)
is called a (linear) ﬁnite diﬀerence (FD) operator if and only if it has the form
(Fhv)(x) =
X
e∈S
ae(x, h)(Sev)(x),
x ∈Zd
h,
where the set S ⊂Zd is such that #S < ∞, 0 ∈S, and it is called the stencil of
the operator Fh. The stencil size is
max {∥j∥ℓ∞| j ∈S} .
Finally, for e ∈S, we have ae : Zd
h × R+ →R.

24.1 Grid Functions and Finite Diﬀerence Operators
669
Remark 24.8 (stencil). The previous deﬁnition is suﬃciently general for our pur-
poses. However, some sources, such as [79], allow the stencil to depend on x, i.e.,
(Fhv)(x) =
X
e∈S(x)
ae(x, h)(Sev)(x),
where, for each x ∈Zd
h, the set S(x) is a stencil according to our deﬁnition. We
will not consider such operators.
Let us now present, for d = 1, 2, several examples of ﬁnite diﬀerence operators.
Example 24.3
The forward diﬀerence operator is deﬁned as
δhv(x) = v(x + h) −v(x)
h
= vi+1 −vi
h
,
where we assumed that x = ih. Its stencil is {0, 1}.
Example 24.4
The backward diﬀerence operator is deﬁned as
¯δhv(x) = v(x) −v(x −h)
h
= vi −vi−1
h
,
where we assumed that x = ih. Its stencil is {−1, 0}.
Example 24.5
The centered diﬀerence operator is deﬁned as
˚δhv(x) = v(x + h) −v(x −h)
2h
= vi+1 −vi−1
2h
,
where we assumed that x = ih. Its stencil is {−1, 0, 1}.
Example 24.6
The one-dimensional discrete Laplace operator2 is deﬁned as
∆hv(x) = ¯δhδhv(x) = v(x + h) −2v(x) + v(x −h)
h2
= vi+1 −2vi + vi−1
h2
,
where we assumed that x = ih. Its stencil is {−1, 0, 1}.
Example 24.7
The two-dimensional discrete Laplace operator is deﬁned, for
(i, j) ∈Z2, as
∆hvi,j = vi−1,j + vi+1,j + vi,j−1 + vi,j+1 −4vi,j
h2
.
Its stencil is the ﬁve points {(0, 0), (0, ±1), (±1, 0)}.
Example 24.8
The discrete mixed derivative operator is deﬁned, for (i, j) ∈Z2,
as
δ♦
h vi,j = vi−1,j−1 + vi+1,j+1 −vi+1,j−1 + vi−1,j−1
4h2
.
Its stencil is the ﬁve points {(0, 0), (±1, ±1)}.
2 Named in honor of the French scholar and polymath Pierre-Simon, Marquis de Laplace
(1749–1827).

670
Finite Diﬀerence Methods for Elliptic Problems
Example 24.9
The two-dimensional skew Laplacian operator is deﬁned, for
(i, j) ∈Z2, as
∆□
h vi,j = vi+1,j+1 + vi+1,j−1 + vi−1,j−1 + vi−1,j+1 −4vi,j
2h2
.
Its stencil is the ﬁve points {(0, 0), (±1, ±1)}.
It turns out that the operators of the previous examples possess several
interesting properties that resemble those of the derivatives that, at least intuitively
at this stage, are meant to approximate.
Proposition 24.9 (properties of diﬀerence operators). Let d = 1 and consider the
diﬀerence operators of Examples 24.3—24.6. For any v, v1, v2 ∈V(Zh), we have
the following identities:
1. Product rule I:
δh(v1v2)(x) = δhv1(x)v2(x) + v1(x + h)¯δhv2(x + h).
2. Product rule II:
¯δh(v1v2)(x) = ¯δhv1(x)v2(x) + v1(x −h)δhv2(x −h).
3. Abel transformation:3
h
N−1
X
k=0
δhv1(kh)v2(kh) = v1(Nh)v2(Nh) −v1(0)v2(0) −h
N
X
k=1
v1(kh)¯δhv2(kh).
4. Symmetry:
∆hv(x) = δh¯δhv(x).
Proof. See Problem 24.2.
Eventually, we will use ﬁnite diﬀerence operators to approximate the derivatives
appearing in a BVP on a domain. Notice, however, that when we deal with grid
domains certain shifts will not be admissible, as they will take us outside of the
domain of a function. For this reason, we must make a distinction between two
types of points, and this is dependent on the ﬁnite diﬀerence operator in question.
Deﬁnition 24.10 (interior and boundary grid points). Let Gh ⊆Zd
h be a grid domain
and Fh be a ﬁnite diﬀerence operator with stencil S. The set of points
GI
h = {x ∈Gh | x + s ∈Gh, ∀s ∈S}
is called the set of interior grid points with respect to the operator Fh. Similarly,
GB
h = Gh\GI
h
is the set of boundary grid points with respect to the operator Fh.
3 Named in honor of the Norwegian mathematician Niels Henrik Abel (1802–1829).

24.1 Grid Functions and Finite Diﬀerence Operators
671
Example 24.10
The stencil of the forward diﬀerence operator of Example 24.3
is {0, 1}. Thus,
¯ΩI
h = [0, 1) ∩Zh = {ih | i = 0, . . . , N} .
Example 24.11
The stencil of the backward diﬀerence operator of Example 24.4
is {−1, 0}. Thus,
¯ΩI
h = (0, 1] ∩Zh = {ih | i = 1, . . . , N + 1} .
Example 24.12
The stencil of the centered diﬀerence operator of Example 24.5
is {−1, 0, 1}. Thus,
¯ΩI
h = Ωh.
Example 24.13
The stencil of the one-dimensional discrete Laplace operator of
Example 24.6 is {−1, 0, 1}. Thus,
¯ΩI
h = Ωh.
Example 24.14
The domains ΩI
h and ΩB
h
for the two-dimensional discrete
Laplace operator of Example 24.7 are illustrated in Figure 24.1.
We have then deﬁned the objects we will use to approximate diﬀerential operators
and their solutions. However, to properly deﬁne the crucial notions of consistency
and stability we alluded to at the beginning of this chapter, we must provide a
notion of convergence for functions in V(Gh), where Gh is a grid domain. If V(Gh)
is ﬁnite dimensional, which will happen if #Gh < ∞, all norms on this space
are equivalent (recall Theorem A.29). The issue here, however, is that we need
to establish notions of stability and convergence that are robust with respect to
h > 0. In addition, the norm we choose for this space must, in a sense, resemble the
natural norm for the space we are trying to approximate. The following deﬁnition
makes these considerations rigorous.
Deﬁnition 24.11 (approximation property). Let d ∈N and Ω= (0, 1)d. For each
N ∈N, deﬁne h = 1/(N + 1) and let ¯Ωh = ¯Ω∩Zd
h be the corresponding grid
domain. Let (V, ∥· ∥V) be a normed space of functions on Ω, i.e.,
V =

v : ¯Ω→R
 ∥v∥V < ∞
	
.
Assume that, for each N ∈N, we endow the space V(¯Ωh) with a norm ∥· ∥h. We
say that the family {(V(¯Ωh), ∥· ∥h)}h>0 possesses the approximation property if
there is a (linear) operator, called the grid projection operator, such that
πh : V →V(¯Ωh),
lim
h↓0 ∥πhv∥h = ∥v∥V,
∀v ∈V.
Let us give some examples of this in a somewhat simpliﬁed setting.

672
Finite Diﬀerence Methods for Elliptic Problems
Example 24.15
Let d = 1, Ω= (0, 1), and V(¯Ωh) be deﬁned as before. Let us
deﬁne, for p ∈[1, ∞), the norms
∥v∥Lp
h =
"
h
N
X
i=1
|vi|p + h
2 (|v0|p + |vN+1|p)
#1/p
,
v ∈V(¯Ωh).
These have the approximation property with respect to the norms of Lp(0, 1),
p ∈(1, ∞). Indeed, if v ∈Lp(0, 1), we deﬁne
(πhv)i =



















1
h
Z (i+1/2)h
(i−1/2)h
v(x)dx,
i ∈{1, . . . , N},
2
h
Z h/2
0
v(x)dx,
i = 0,
2
h
Z 1
1−h/2
v(x)dx,
i = N + 1,
which we call the averaging operator. Now
∥πhv∥p
Lp
h = h
N
X
i=1
|πhvi|p + h
2 (|πhv0|p + |πhvN+1|p),
and we consider, separately, the interior and boundary indices.
For the interior indices, using Young’s inequality and setting p′ = p/(p −1), we
have
h
N
X
i=1
|πhvi|p = h1−p
N
X
i=1

Z (i+1/2)h
(i−1/2)h
v(x)dx

p
≤h1−p−p/p′
N
X
i=1
Z (i+1/2)h
(i−1/2)h
|v(x)|p dx
=
N
X
i=1
Z (i+1/2)h
(i−1/2)h
|v(x)|p dx
=
Z 1−h/2
h/2
|v(x)|p dx.
Similarly,
h
2|πhv0|p ≤
Z h/2
0
|v(x)|p dx,
h
2|πhvN+1|p ≤
Z 1
1−h/2
|v(x)|p dx,
so that, in conclusion, we have, for any v ∈Lp(0, 1),
∥πhv∥Lp
h ≤∥v∥Lp(0,1)
=⇒
lim sup
h↓0
∥πhv∥Lp
h ≤∥v∥Lp(0,1).
The reverse inequality is the content of the so-called Lebesgue Diﬀerentiation
Theorem and so we have the approximation property.

24.1 Grid Functions and Finite Diﬀerence Operators
673
Example 24.16
The previous example, with few modiﬁcations, shows that V(¯Ωh)
with ∥· ∥L1
h has the approximation property with respect to L1(0, 1). We leave the
details to the reader as an exercise; see Problem 24.4.
Example 24.17
The case p = ∞requires a little modiﬁcation. As usual, we
deﬁne, for v ∈V(¯Ωh),
∥v∥L∞
h = max
x∈¯Ωh
|v(x)|
and
(πhv)i = v(ih),
i = 0, . . . , N + 1,
which we call the sampling operator. This has the approximation property on
C([0, 1]) with the norm ∥· ∥L∞(0,1). We leave the details to the reader as an exercise;
see Problem 24.5.
Remark 24.12 (scaling). One may wonder what, for p ∈[1, ∞), is the purpose of
the factor h in the deﬁnitions of the Lp
h-norms. After all, after the identiﬁcation of
Remark 24.4, we could use, as norms of V(¯Ωh), any of the norms ∥· ∥ℓp(RN), for
p ∈[1, ∞]. It is precisely to attain the approximation property. For instance, set
p = 1 and consider the constant function v ≡α > 0 on (0, 1). Then the averaging
operator yields πhvi = α for all i. Consequently,
∥πhv∥L1
h = h
N
X
i=1
vi + hα =
N
N + 1α +
1
N + 1α = ∥v∥L1(0,1).
If we were to use the ℓ1(RN)-norm, we would not attain approximation
∥πhv∥1 =
N+1
X
i=0
vi = (N + 2)α →∞,
N →∞.
Notice that, as expected, the L2
h-norm of the previous examples comes from an
inner product. Thus, we deﬁne the following inner products.
Deﬁnition 24.13 (discrete inner products). Deﬁne, on V(¯Ωh), the bilinear forms
[v (1), v (2)]L2
h = h
2

v (1)
0 v (2)
0
+ v (1)
N+1v (2)
N+1

+ h
N
X
i=1
v (1)
i
v (2)
i
,
(v (1), v (2))L2
h = h
N
X
i=1
v (1)
i
v (2)
i
,
where v (1), v (2) ∈V(¯Ωh).
These expressions satisfy the expected properties.
Proposition 24.14 (inner products). The expression [·, ·]L2
h, introduced in Deﬁni-
tion 24.13, is an inner product on V(¯Ωh) and it induces the L2
h-norm. Similarly, the

674
Finite Diﬀerence Methods for Elliptic Problems
expression (·, ·)L2
h, introduced in Deﬁnition 24.13, is an inner product on V0(¯Ωh)
and it induces the L2
h-norm. In addition, we have
(δhv1, v2)L2
h = −
 v1, ¯δhv2

L2
h ,
∀v1, v2 ∈V0(¯Ωh);
(24.1)
as a consequence,
(−∆hv1, v2)L2
h =
 ¯δhv1, ¯δhv2

L2
h = (δhv1, δhv2)L2
h ,
∀v1, v2 ∈V0(¯Ωh).
Proof. See Problem 24.6.
Remark 24.15 (summation by parts). The identity (24.1) is a discrete analogue of
the integration by parts formula of Theorem B.38. For this reason, it is commonly
referred to as the summation by parts formula.
24.2
Consistency and Stability of Finite Diﬀerence Methods
We are ﬁnally in a position to state the important notions of consistency, stability,
and convergence for a ﬁnite diﬀerence problem. Recall that our grand goal is to
approximate the solution of a BVP. Thus, we assume that we have at hand the
following problem. Let V be some normed space of functions deﬁned on ¯Ω= [0, 1]d,
d ∈N, with norm ∥· ∥V. We need to ﬁnd u ∈V, such that
(
Lu = f ,
in Ω,
ℓu = g,
in ∂Ω,
(24.2)
where L and ℓare some diﬀerential operators. The operator L encodes the
diﬀerential equation, whereas ℓrepresents the boundary conditions. Here, the
functions f : Ω→R and g : ∂Ω→R are assumed to be given and belong to
some normed spaces f ∈F and g ∈G, with norms ∥· ∥F and ∥· ∥G, respectively.
Problem (24.2) is replaced by the ﬁnite diﬀerence problem: Find w ∈V(¯Ωh)
such that
(
Lhw = fh,
in ΩI
h,
ℓhw = gh,
in ΩB
h .
(24.3)
Here, Lh and ℓh are ﬁnite diﬀerence operators; ΩI
h and ΩB
h are the interior and
boundary grid points, respectively, with respect to the operator Lh; and fh ∈V(ΩI
h)
and gh ∈V(ΩB
h ) are assumed to be given.
We can now state all our needed notions. We begin with stability.
Deﬁnition 24.16 (stability). Let {(V(¯Ωh), ∥· ∥h)}h>0 have the approximation
property with respect to (V, ∥· ∥V). We say that the FDM (24.3) is stable if and
only if there are constants h0 > 0 and C > 0 such that, for all h ∈(0, h0], problem
(24.3) has a unique solution for any pair (fh, gh) ∈V(ΩI
h) × V(ΩB
h ); additionally,
for h ∈(0, h0], we have
∥w∥h ≤C

∥fh∥V(ΩI
h) + ∥gh∥V(ΩB
h )

,

24.2 Consistency and Stability of Finite Diﬀerence Methods
675
where ∥· ∥V(ΩI
h) and ∥· ∥V(ΩB
h ) are norms that have the approximation property with
respect to (F, ∥· ∥F) and (G, ∥· ∥G), respectively.
We can now state consistency.
Deﬁnition 24.17 (operator consistency). Let {(V(Zd
h), ∥· ∥h)}h>0 have the approx-
imation property with respect to (V, ∥· ∥V). Let L: V →V be a linear, not
necessarily bounded, operator and Lh : V(Zd
h) →V(Zd
h) be a ﬁnite diﬀerence
operator. We deﬁne the consistency error at v ∈V to be
Eh[L, v](x) = (Lhπhv −πh(Lv)) (x),
x ∈Zd
h.
We say that the ﬁnite diﬀerence operator Lh is consistent to at least order p ∈N
with L if and only if there is k ∈N, usually k > p, such that there is a constant
h1 > 0 such that, whenever h ∈(0, h1] and v ∈Ck(Rd), there is C > 0, possibly
depending on v, for which
∥Eh[L, v]∥h ≤Chp.
Finally, we say that the operator is consistent with exactly order p if there is
at least one v for which the previous estimate cannot be improved upon, i.e., the
value of p cannot be increased, by assuming that v is smoother.
Deﬁnition 24.18 (consistency). Let {(V(¯Ωh), ∥· ∥h)}h>0 have the approximation
property with respect to (V, ∥· ∥V). We deﬁne the consistency error of (24.3) to
be the function Eh ∈V(¯Ωh), deﬁned as
Eh[u](x) =
(
(Lhπhu −fh)(x),
x ∈ΩI
h,
(ℓhπhu −gh)(x),
x ∈ΩB
h ,
where u ∈V solves (24.2). We say that the FDM (24.3) is consistent to at least
order p ∈N with (24.2) if and only if there is k ∈N, usually k > p, such that
if u ∈Ck(¯Ω), then there are constants h1 > 0 and C > 0 such that, whenever
h ∈(0, h1], we have
∥Eh[u]∥V(ΩI
h) + ∥Eh[u]∥V(ΩB
h ) ≤Chp.
Finally, we say that the method is consistent with exactly order p if the previous
estimate cannot be improved upon, i.e., the value of p cannot be increased, by
assuming that u is smoother.
Remark 24.19 (relation between notions). Notice that we are calling consistency
error two seemingly unrelated concepts. To show how they are related, let us
assume, for the sake of illustration, that fh = πhf . Then, for all points x ∈ΩI
h, we
have
(Lhπhu −fh) (x) = (Lhπhu −πh(Lu)) (x) + πh(Lu)(x) −fh(x)
= Eh[L, u](x) + πh(Lu −f )(x)
= Eh[L, u](x).
In other words, when fh = πhf , the consistency error of the method and the
operator coincide (at interior points). In general, we see that the consistency error

676
Finite Diﬀerence Methods for Elliptic Problems
of a method has two components: the operator consistency error, Eh[L, u](x), and
the grid projection error, πh(Lu)(x) −fh(x). A similar consideration can be made
for the boundary points and the operator ℓ.
Before we proceed, let us illustrate the notion of consistency with a few examples.
Proposition 24.20 (consistency). Let d = 1. We have:
1. The forward diﬀerence operator of Example 24.3 is consistent, on Cb(R), to
exactly order one with
dw(x)
dx
,
x ∈R.
2. The backward diﬀerence operator of Example 24.4 is consistent, on Cb(R), to
exactly order one with
dw(x)
dx
,
x ∈R.
3. The centered diﬀerence operator of Example 24.5 is consistent, on Cb(R), to
exactly order two with
dw(x)
dx
,
x ∈R.
4. The one-dimensional discrete Laplacian operator of Example 24.3 is consistent,
on Cb(R), to exactly order two with
∆w(x) = d2w(x)
dx2
,
x ∈R.
As a consequence, if P ∈P1,
∆hP(x) = 0,
∀x ∈Ωh.
5. The operator
(BDF w)i = 1
2h (3wi −4wi−1 + wi−2)
is consistent, on Cb(R), to exactly order two with
dw(x)
dx
,
x ∈R.
Proof. See Problem 24.7.
Let us return to our general notions now and deﬁne convergence.
Deﬁnition 24.21 (convergence). Let {(V(¯Ωh), ∥· ∥h)}h>0 have the approximation
property with respect to (V, ∥· ∥V). Let u ∈V and w ∈V(¯Ωh) solve (24.2) and
(24.3), respectively. The error of the method is the function e ∈V(¯Ωh), deﬁned by
e = πhu −w.
We say that the FDM (24.3) is convergent if and only if
∥e∥h →0,
h ↓0.

24.2 Consistency and Stability of Finite Diﬀerence Methods
677
We say that the method is convergent with rate p ∈N if there is C > 0 such
that, for suﬃciently small h > 0, we have
∥e∥h ≤Chp.
We can now state and prove Lax’s Principle.
Theorem 24.22 (Lax). Let problem (24.3) be stable and consistent. Then it
is convergent. In addition, if (24.3) is consistent to order p ∈N, then the
approximation is convergent with rate p.
Proof. This is nothing but an exercise in notation. By deﬁnition, we have
(
Lhπhu(x) = fh(x) + Eh[u](x),
x ∈ΩI
h,
ℓhπhu(x) = gh(x) + Eh[u](x),
x ∈ΩB
h .
By linearity, we have
(
Lh(πhu −w)(x) = Eh[u](x),
x ∈ΩI
h,
ℓh(πhu −w)(x) = Eh[u](x),
x ∈ΩB
h ,
and stability then implies that
∥πhu −w∥h ≤C

∥Eh[u]∥V(ΩI
h) + ∥Eh[u]∥V(ΩB
h )

.
Thus, convergence, even with the prescribed rate, follows from consistency.
Example 24.18
Before we proceed any further, it is instructive to observe that
we have already seen particular examples of all these constructions before. Indeed,
the methods for initial value problems for ordinary diﬀerential equations we saw in
Chapters 18 and 20 ﬁt this framework.
In these methods, we subdivided the interval [0, T ] ⊆R into K ∈N equal parts
of length τ = T/K. In doing so, we deﬁned the grid domain
[0, T]τ = [0, T] ∩Zτ = {kτ | k = 0, . . . , K} .
Our approximation methods computed sequences {w k}K
k=0 ⊂Rd which can be
identiﬁed with elements of V([0, T]τ; Rd) via w(tk) = w k. The approximation is
obtained by a nonlinear version of an FDM
Dτw(x) = G(x),
x ∈[0, T]I
τ,
w(x) = g(x),
x ∈[0, T]B
τ ,
where the interior and boundary domains [0, T ]I
τ and [0, T]B
τ , respectively, are
relative to the operator Dτ.
For single-step methods, the operator Dτ is given by
(Dτw)(tk+1) = 1
τ
 w k+1 −w k
,
k ≥0,
so that [0, T]I
τ = (0, T] ∩Zτ, [0, T]B
τ = {0}, and g = u0. The function G is given
by the slope approximation; see Deﬁnition 18.1.

678
Finite Diﬀerence Methods for Elliptic Problems
For q-step methods (q ∈N), we have
(Dτw)(tk+q) = 1
τ
q
X
j=0
ajw k+j,
k ≥0,
so that [0, T]I
τ = [tq, T] ∩Zτ, [0, T]B
τ = {t0, t1, . . . , tq−1}, and g(tj) = w j for
j = 0, . . . , q −1, i.e., the starting values of the multi-step method. The function
G is given by
G(tk+q) =
q
X
j=0
bjf (tk+j, w k+j);
see (20.1).
24.3
The Poisson Problem in One Dimension
Let us now apply all the general theory that we have developed to the approximation
of the one-dimensional Poisson problem. Let Ω= (0, 1). We wish to approximate
the solution to
−d2u(x)
dx2
= f (x), x ∈Ω,
u(0) = uL,
u(1) = uR,
(24.4)
where uL, uR ∈R and f ∈C(¯Ω).
We recall that Theorem 23.8 provided existence and uniqueness of classical
solutions to this problem. We will operate under the assumptions that guarantee
this and will approximate this solution via ﬁnite diﬀerences.
Deﬁnition 24.23 (ﬁnite diﬀerences). Suppose that u ∈C2(Ω) ∩C(¯Ω) is a
classical solution to (24.4). Let N ∈N. We call ˜w ∈V(¯Ωh) a ﬁnite diﬀerence
approximation to (24.4) if and only if
−∆h ˜w(x) = fh(x),
x ∈Ωh,
˜w(0) = uL, ˜w(1) = uR,
(24.5)
where fh ∈V(Ωh) is deﬁned as fh = πhf and πh is the sampling operator.
One of our ﬁrst concerns should be to determine whether our ﬁnite diﬀerence
approximation is well deﬁned. We can get rid of the Dirichlet boundary conditions
via a change of variables. Indeed, let P ∈V(¯Ωh) be given by
Pi = uL + (uR −uL)ih
=⇒
P0 = uL, PN+1 = uR.
Owing to Proposition 24.20, we have ∆hP = 0. Let us deﬁne w = ˜w −P. Then
w ∈V0(¯Ωh) and it solves
−∆hw(x) = fh(x),
x ∈Ωh,
w(0) = 0, w(1) = 0,
(24.6)
if and only if ˜w solves (24.5). Now, to ﬁnd w, we use the identiﬁcation of Remark
24.4 to realize that problem (24.6) can be equivalently rewritten as
Aw = h2f ,

24.3 The Poisson Problem in One Dimension
679
where w ∈RN ←→w ∈V0(¯Ωh), f ∈RN ←→fh ∈V(Ωh), and the matrix
A ∈RN×N is the so-called stiﬀness matrix, which is deﬁned as
A =


2
−1
0
. . .
0
−1
2
...
...
0
...
...
−1
0
...
−1
2
−1
0
. . .
0
−1
2


.
(24.7)
This observation is important from both practical and theoretical points of view.
First, in practice, we have reduced the approximation of a BVP to the solution of
a linear system of equations. Not only is such a problem more tractable, but also
we see that the stiﬀness matrix A is tridiagonal. Thus, to compute w, one only
needs to apply the (linear complexity) methods of Section 3.1.3. Second, and this
is what we are concerned with here, to analyze the FDM we only need to study
the properties of a linear system of equations.
Let us then analyze this method. First, assuming that problem (24.6) has a
solution, let us study its consistency. This is an easy consequence of what we have
done so far.
Proposition 24.24 (consistency). Let u ∈C4(Ω) be a classical solution of (24.4)
with uL = uR = 0. Let w ∈V0(¯Ωh) be its ﬁnite diﬀerence approximation deﬁned
by (24.6). This method is consistent, in C(¯Ω), to exactly order p = 2.
Proof. Clearly, Eh[u]0 = Eh[u]N+1 = 0, so we only need to study the consistency
of approximating the diﬀerential equation. But Proposition 24.20 has shown that
our approximation is of exactly order p = 2.
We now need to show that the discrete problem (24.6) is well posed and stable,
so that we can infer convergence. To do this, we observe that, in fact, we have
already studied the matrix A. This is the Toeplitz symmetric tridiagonal matrix,
with n = N + 1, of Theorem 3.38. The following is nothing but a restatement of
that result with a few easy additional consequences.
Theorem 24.25 (spectrum of A). Let N ∈N. Suppose that A ∈RN×N is the
stiﬀness matrix deﬁned in (24.7). Consider the set of grid functions {ϕk}N
k=1 ⊂
V0(¯Ωh), deﬁned by
[ϕk]i = sin(kπih) ,
i = 1, . . . , N.
1. The set {ϕk}N
k=1 ⊂RN with ϕk ←→ϕk is an orthogonal set of eigenvectors of
the stiﬀness matrix A.
2. The eigenvalue λk corresponding to the eigenvector ϕk is given by
λk = 2 (1 −cos(kπh)) = 4 sin2
kπh
2

.
Since 0 < λk < 4, for all k = 1, . . . , N, the stiﬀness matrix A is symmetric
positive deﬁnite (SPD) and is, therefore, invertible.

680
Finite Diﬀerence Methods for Elliptic Problems
3. There is a constant C1 > 0, independent of h, such that, if 0 < h < 1
2,
A−1
2 =
1
4 sin2   hπ
2
 ≤C1h−2.
4. The (spectral) condition number of A satisﬁes the estimate
κ2(A) = ∥A∥2
A−1
2 ≤4C1h−2.
Proof. The ﬁrst two statements are the content of, or were shown during the
course of the proof of, Theorem 3.38. Let us then focus on the norm and condition
number estimates.
The largest eigenvalue of A is λN and the smallest is λ1. Since the eigenvalues
of A−1 are the reciprocals of the eigenvalues of A, it follows that
A−1
2 = 1
λ1
=
1
4 sin2   πh
2
 =
1
2 (1 −cos(πh)).
Now, by Taylor’s Theorem B.31, for any x ∈(0, π/2), there is η ∈(0, x) such that
1 −cos(x) = x2
2! −x4
4! + x6
6! cos(η) ≥x2
2 −x4
24.
Consequently, if 0 < πh < π/2, or, equivalently, 0 < h < 1/2,
2h−2(1 −cos(πh)) ≥π2 −π4h2
12 .
Since
√
6
π > 1
2 > h > 0,
it follows that
2h−2(1 −cos(πh)) ≥π2 −π4h2
12
≥π2
2 .
Equivalently,
2
π2h2 ≥
1
2(1 −cos(πh)) = 1
λ1
and the norm estimate proof is completed upon taking C1 =
2
π2 . The condition
number estimate follows immediately from
κ2(A) = λN
λ1
≤4C1h−2.
As an immediate consequence, problem (24.6) always has a unique solution.
Corollary 24.26 (stability). For every N ∈N, the ﬁnite diﬀerence problem (24.6)
has a unique solution w ∈V0(¯Ωh) ←→w ∈RN, which, moreover, satisﬁes the
estimate
∥w∥L2
h ≤C1∥fh∥L2
h.

24.3 The Poisson Problem in One Dimension
681
x
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
y
1
2
3
4
Figure 24.2 The eigenvalues of the stiﬀness matrix A are shown (ﬁlled circles) for N = 9.
The horizontal axis is x = kh. The solid curve is the plot of 4 sin2   πx
2

. Observe that
0 < λk < 4.
Proof. For every N ∈N, the matrix A is invertible; thus, there is a unique w.
Moreover, this satisﬁes
∥w∥2 ≤h2 A−1
2 ∥f ∥2 ≤C1∥f ∥2,
where we used the estimate on the norm of the inverse. Since ∥w∥L2
h = h1/2 ∥w∥2,
the result follows.
Remark 24.27 (consistency). In one dimension, { ˜ϕk, µk}k∈N is the family of
eigenpairs of the Laplacian:
−˜ϕ′′
k(x) = µkϕk(x),
x ∈(0, 1),
˜ϕk(0) = ˜ϕk(1) = 0,
where
˜ϕk(x) = sin(kπx),
µk = k2π2.
Theorem 24.25 shows that the matrix h−2A has eigenvectors
ϕk = πh ˜ϕk,
where πh is the sampling operator, and eigenvalues (see Figure 24.2)
1
h2 λ2
k = 4(N + 1)2 sin2

kπ
2(N + 1)
2
.
For N ↑∞(h ↓0), we have
1
h2 λ2
k = 4(N + 1)2

k2π2
4(N + 1)2 + O((N + 1)−4)

= µk + O(h2).
This shows that this method is not only point-wise consistent, but also, in a sense,
spectrally consistent.

682
Finite Diﬀerence Methods for Elliptic Problems
24.3.1
Convergence in the L2
h-Norm
Let us now prove a convergence result in the L2
h-norm. To do this, we use the
previously obtained consistency error, and then we use some facts (already proven)
about the stiﬀness matrix.
Theorem 24.28 (convergence). Let N ∈N, h =
1
N+1, f ∈C(Ω), and u ∈C4(¯Ω)
be a classical solution to the one-dimensional Poisson problem (24.4) with uL =
uR = 0. Suppose that w ∈V0(¯Ωh) ←→w ∈RN is the solution to the ﬁnite
diﬀerence problem (24.6). Let e ∈V0(¯Ωh) ←→e ∈RN be its error. Then there is
a constant C2 > 0, independent of h, such that, if 0 < h < 1
2,
∥e∥L2
h ≤C1C2h2,
where C1 > 0 is the constant from Theorem 24.25.
Proof. From Proposition 24.20, we know that
∥Eh[u]∥L∞
h ≤C2h2,
∥EEEh[u]∥∞≤C2h2.
We now proceed in the usual way. By deﬁnition of the consistency error, we have,
on Ωh,
−∆hπhu = fh + Eh[u],
so that, subtracting from the method (24.6), we obtain an equation that controls
the error
−∆he = Eh[u]
←→
Ae = h2EEEh[u].
Thus, using Theorem 24.25 and proceeding as in Corollary 24.26,
∥e∥L2
h = h1/2∥e∥2 ≤h5/2 A−1
2 ∥EEEh[u]∥2 ≤C1
√
hN∥EEEh[u]∥∞≤C1C2h2.
Remark 24.29 (suboptimal estimate). Using Problem 24.8, we can immediately
get an error estimate in the L∞
h -norm using our L2
h-estimate:
∥e∥L∞
h ≤
1
√
h ∥e∥L2
h ≤C1C2h
3
2 .
As we will see, this estimate is suboptimal.
24.3.2
The Discrete Maximum Principle and Convergence in the L∞
h -Norm
We will now obtain optimal error estimates for our ﬁnite diﬀerence approximations
in the L∞
h -norm. For this purpose, we need a Discrete Maximum Principle, which
serves as the discrete analogue of the maximum principle of Theorem 23.2.
Theorem 24.30 (Discrete Maximum Principle). Suppose that v ∈V(¯Ωh) satisﬁes
−∆hv(x) ≤0,
∀x ∈Ωh.
Then, for all x ∈¯Ωh,
v(x) ≤max{v(0), v(1)}.
In other words, the maximum must occur on the boundary.

24.3 The Poisson Problem in One Dimension
683
Proof. To obtain a contradiction, suppose that a strict maximum occurs in the
interior. If this is true, there is some xk ∈Ωh such that
vk = max
xj∈Ωh vj > max{v0, vN+1}.
Let us suppose, for simplicity, that 2 ≤k ≤N −1. Then
0 ≥−h2∆hvk = 2vk −vk−1 −vk+1 ≥0.
This implies that −∆hvk = 0 and, therefore,
vk = 1
2(vk−1 + vk+1).
The only way to satisfy the last equation and the fact that vk ≥vk±1 is to have
vk = vk±1.
We can now repeat our argument at all neighboring points; we conclude that
v1 = · · · = vk = · · · = vN.
Next to the left boundary point, we have, for example,
0 = −h2∆v1 = 2v1 −v0 −v2 > 0,
since v1 > v0. This is a contradiction.
From this result, a stability result analogous to Corollary 23.4 can be easily
obtained.
Theorem 24.31 (stability). Suppose that w ∈V(¯Ωh) and fh ∈V(Ωh) and g0, g1 ∈
R are such that
−∆hw = fh,
Ωh,
w0 = g0,
wN+1 = g1.
There is some constant C > 0, independent of h and w, such that
∥w∥L∞
h ≤max
j∈{0,1} |gj| + C∥fh∥L∞
h .
Proof. Deﬁne the comparison function Φ: [0, 1] →R via
Φ(x) =

x −1
2
2
≥0.
Deﬁne the grid function φ = πhΦ, where πh is the sampling operator. Then, in Ωh,
we have
−Φ′′ ≡−2 ≡−∆hφ.
Now deﬁne the grid functions
ψ± = ±w +
∥fh∥L∞
h
2
φ.
Observe that, in Ωh, we have
−∆hψ± = ±fh −∥fh∥L∞
h ≤0.

684
Finite Diﬀerence Methods for Elliptic Problems
By the Discrete Maximum Principle, in Ωh,
±w ≤±ψ ≤max{ψ±(0), ψ±(1)} ≤max
j∈{0,1} |gj| +
∥fh∥L∞
h
8
.
Putting the two results together, we get
∥w∥L∞
h ≤max
j∈{0,1} |gj| + 1
8∥fh∥L∞
h .
This serves as a stability result in the L∞
h -norm. Thus, we can now obtain
convergence.
Corollary 24.32 (convergence). Let f ∈C(Ω) and u ∈C4(¯Ω) be a classical
solution to the one-dimensional Poisson problem (24.4) with uL = uR = 0. Let
N ∈N. Suppose that w ∈V0(¯Ωh) ←→w ∈RN is the solution to the ﬁnite
diﬀerence problem (24.6). Then there is a constant C3 > 0, independent of h,
such that
∥e∥L∞
h ≤C2C3h2,
where C2 > 0 is the consistency error constant.
Proof. In the course of the proof of Theorem 24.28, we found that the error
e ∈V0(¯Ωh) satisﬁes, in Ωh,
−∆he = Eh[u].
By the previous stability result, we have
∥e∥L∞
h ≤1
8∥Eh[u]∥L∞
h ≤C2
8 h2.
The result follows with C3 = 1
8.
24.4
Elliptic Problems in One Dimension
Let us consider here more general elliptic problems in one dimension and their ﬁnite
diﬀerence approximation. We will focus on the Dirichlet problem
(
Lu = f ,
in Ω= (0, 1),
u(0) = u(1) = 0,
(24.8)
where f ∈C([0, 1]) and the operator L is a second-order elliptic operator to be
speciﬁed below. We will be interested in constructing ﬁnite diﬀerence operators Lh
that are consistent and have stencil {−1, 0, 1}, so that the ﬁnite diﬀerence problem
will read: Find w ∈V0(¯Ωh) such that
Lhw = fh,
in Ωh,
Lhwi = −Aiwi−1 + Ciwi −Biwi+1;
(24.9)
here, fh ∈V(Ωh) is, as usual, deﬁned as fh(ih) = f (ih).

24.4 Elliptic Problems in One Dimension
685
We will call methods like (24.9) homogeneous FDMs. Although the coeﬃcients
of the equation and the method are variable, the method has the same represen-
tation at every point xi of the grid domain Ωh.
To make matters simple, we will proceed under the assumption that the
coeﬃcients of L are suﬃciently smooth. We comment, however, that this is not
necessary and we refer the reader to [50], [56, Chapter VI], and [79] for detailed
accounts of the general theory.
24.4.1
Divergence Form Operators
Here, we consider a diﬀerential operator in divergence form, i.e.,
Lu(x) = −d
dx

a(x)du(x)
dx

+ c(x)u(x),
(24.10)
where we assume that the coeﬃcients satisfy a ∈C1([0, 1]), 0 ≤c ∈C([0, 1]); in
addition, there are constants λ, Λ ∈R such that
0 < λ ≤a(x) ≤Λ,
∀x ∈[0, 1].
In this setting, the operator is elliptic in the sense of Deﬁnition 23.1, and the theory
of weak solutions presented in Section 23.2.3 provides the existence, uniqueness,
and stability of the solution to (24.8). The stability, in particular, is expressed by
Corollary 23.11.
We now wish to construct the diﬀerence method, i.e., ﬁnd the coeﬃcients
Ai, Bi, Ci. We do so arguing from consistency considerations. Namely, we consider,
for v ∈C4([0, 1]) such that v(0) = v(1) = 0, the consistency error
Eh[v] = Lhπhv −πh(Lv),
where πh is the sampling operator, and require that it satisﬁes ∥Eh[v]∥L∞
h ≤Ch2.
We begin by introducing the following change of notation. Setting αi = h2Ai,
βi = h2Bi, and γi = h2Ci, we get
Lhvi = −1
h

βi
vi+1 −vi
h
−αi
vi −vi−1
h

+ κivi = −1
h

βiδhvi −αi ¯δhvi

+ κivi,
where κi = h−2(γi −βi −αi). Notice that, at least symbolically, the ﬁnite diﬀerence
operator begins to resemble the divergence form operator L. Now, to achieve
consistency, we must have
Eh[v]i = −1
h

βiδhv(xi) −αi ¯δhv(xi)

+κiv(xi)+(a(xi)v ′(xi))′−c(xi)v(xi) = O(h2).
From Taylor expansions, we know, see Proposition 24.20,
δhv(xi) = v ′(xi) + 1
2v ′′(xi)h + 1
6v ′′′(xi)h2 + O(h3),
¯δhv(xi) = v ′(xi) −1
2v ′′(xi)h + 1
6v ′′′(xi)h2 + O(h3),

686
Finite Diﬀerence Methods for Elliptic Problems
so that, substituting in Eh[v]i, we get
Eh[v]i =

a′(xi) −βi −αi
h

v ′(xi) +

a(xi) −αi + βi
2

v ′′(xi)
−βi −αi
6
hv ′′′(xi) + (κi −c(xi))v(xi) + O(h2).
Thus, we require
βi −αi
h
= a′(xi) + O(h2),
αi + βi
2
= a(xi) + O(h2),
κi = c(xi) + O(h2).
(24.11)
There are several ways this can be achieved. For instance,
βi = a(xi + h/2),
αi = a(xi −h/2),
κi = c(xi),
(24.12)
βi = a(xi+1) + a(xi)
2
,
αi = a(xi) + a(xi−1)
2
,
κi = c(xi)
(24.13)
are possible choices; see Problem 24.9. Let us write the ﬁnal operator with the ﬁrst
choice
Lhvi = −1
h
 ai+1/2δhvi −ai−1/2¯δhvi

+ ciwi = −δh(ˆai ¯δhvi) + ciwi,
(24.14)
where ai±1/2 = a(xi ± h/2), ci = c(xi), and ˆai = ai−1/2. Notice the resemblance
to the divergence form diﬀerential operator L.
Let us now study the stability of this method. We will do so in a norm that is
natural for this problem.
Deﬁnition 24.33 (H1
h-seminorm). The H1
h-seminorm, on V(¯Ωh), is deﬁned as
∥v∥2
H1
h = h
N+1
X
i=1
|¯δhvi|2.
Notice that, indeed, this is not a norm, but only a seminorm. A grid function
that takes constant, nonzero values satisﬁes ∥v∥H1
h = 0. However, it turns out that
on V0(¯Ωh) this is a norm. Compare the following result with (D.2).
Theorem 24.34 (discrete Poincar´e). 4 There is a constant, independent of h > 0,
such that, for all v ∈V0(¯Ωh), we have
∥v∥L2
h ≤C∥v∥H1
h.
Consequently, the quantity ∥· ∥H1
h is a norm on V0(¯Ωh).
Proof. See Problem 24.10.
We can now state the stability of the method. It is important to compare the
estimate in this problem with that of Corollary 23.11.
Theorem 24.35 (stability). There is a constant C > 0 that depends only on the
coeﬃcients a and c such that any solution to (24.9) with the operator deﬁned as
in (24.14) satisﬁes
4 Named in honor of the French mathematician, theoretical physicist, engineer, and philosopher
of science Jules Henri Poincar´e (1854–1912).

24.4 Elliptic Problems in One Dimension
687
∥w∥H1
h ≤C∥fh∥L2
h.
As a consequence, the solution to this problem is unique and convergent with order
p = 2 in the H1
h-norm.
Proof. Since the FDM is a square system of linear equations, the estimate implies
uniqueness, and this in turn implies existence.
Let us now show the estimate. We can take the L2
h-inner product of the method
with w itself to obtain
−
 δh(ˆa¯δhw), w

L2
h + (cw, w)L2
h = (fh, w)L2
h ≤∥fh∥L2
h∥w∥L2
h.
Since, by assumption, c ≥0, this inequality reduces to
−
 δh(ˆa¯δhw), w

L2
h ≤∥fh∥L2
h∥w∥L2
h.
We now invoke the Abel transformation of Proposition 24.9 to obtain, since w ∈
V0(¯Ωh),
−
 δh(ˆa¯δhw), w

L2
h = h
N
X
i=1
ˆai|¯δhwi|2 ≥λ∥w∥2
H1
h,
where we used that ai = a(xi −h/2) ≥λ.
Finally, applying the discrete Poincar´e inequality of Theorem 24.34 and Young’s
inequality, we conclude that
λ∥w∥2
H1
h ≤C∥fh∥L2
h∥w∥H1
h ≤C2
2λ∥fh∥2
L2
h + λ
2 ∥w∥2
H1
h,
as we intended to show.
24.4.2
Nondivergence Form Operators. Upwinding
Here, we will consider a nondivergence form operator
Lu(x) = −a(x)d2u(x)
dx2
+ b(x)du(x)
dx
−c(x)u(x),
where the coeﬃcients satisfy a, b, c ∈C([0, 1]). In addition, we assume that c(x) ≥
0 for x ∈[0, 1] and there are constants Λ, λ > 0 such that
λ ≤a(x) ≤Λ,
∀x ∈[0, 1].
With these assumptions, this operator is elliptic and satisﬁes all the conditions of
the maximum principle of Theorem 23.2. Our main objective here is to construct
ﬁnite diﬀerence operators that are monotone, i.e., they satisfy an analogue of this
result. For simplicity, we will assume also that b(x) ≥0 for all x ∈[0, 1]. The
general case can be treated accordingly.
The main tool we will use to show monotonicity is the following auxiliary result.
Lemma 24.36 (comparison). Let {yi}N+1
i=0
⊂R. Assume that there are three
families of real numbers {Ai}N
i=1, {Bi}N
i=1, {Ci}N
i=1 ⊂R such that
−Aiyi−1 + Ciyi −Biyi+1 ≤0,
i = 1, . . . , N.

688
Finite Diﬀerence Methods for Elliptic Problems
If
Ai > 0,
Bi > 0,
Ai + Bi ≤Ci,
i = 1, . . . , N,
(24.15)
then {yi}N+1
i=0 is either constant or it satisﬁes
max
i=1,...,N yi ≤max{0, y0, yN+1}.
Proof. See Problem 24.11.
Notice that this comparison estimate immediately implies several stability
estimates for the solution of (24.9) with a ﬁnite diﬀerence operator whose
coeﬃcients satisfy (24.15).
Corollary 24.37 (existence and uniqueness). Assume that the method (24.9) is
such that the coeﬃcients of the operator satisfy (24.15). Then this method has a
unique solution.
Proof. To show existence, it is suﬃcient to show uniqueness of a solution. To do
so, consider (24.9) with fh ≡0. Appealing to Lemma 24.36, we obtain, for any
i = 1, . . . , N,
0 ≤
min
j=1,...,N wj ≤wi ≤
max
j=1,...,N wj ≤0.
Let us construct a monotone FDM, i.e., one that satisﬁes (24.15). From the form
of the operator, we see that, essentially, the only design choice that we can have is
the discretization of the ﬁrst derivative. We will choose an upwind discretization. In
other words, since it is assumed that b ≥0, we will choose a backward diﬀerence
to approximate the ﬁrst derivative, meaning that we take information from the
direction that is opposite to b. We then obtain
Lhvi = −ai∆hvi + bi ¯δhvi + civi
= −
 ai
h2 + bi
h

vi−1 +
2ai
h2 + bi
h + ci

vi −
 ai
h2

vi+1,
where ai = a(xi), bi = b(xi), and ci = c(xi). We see that this method has the
form (24.9) with
Ai = ai
h2 + bi
h ,
Bi = ai
h2 ,
Ci = 2ai
h2 + bi
h + ci.
(24.16)
Clearly, this operator is consistent, on Cb(R), with L to order exactly one. Let
us now show that it is monotone.
Lemma 24.38 (monotonicity). For every h > 0, the coeﬃcients (24.16) satisfy
(24.15).
Proof. First of all, since we assumed b ≥0, we clearly have
Ai > 0,
Bi > 0.
Finally,
Ai + Bi = 2ai
h2 + bi
h = Ci −ci ≤Ci,
where we used that c(x) ≥0.

24.4 Elliptic Problems in One Dimension
689
Remark 24.39 (downwind and centered discretizations). Let us deﬁne the so-called
mesh P´eclet number 5
Pe = h∥b∥L∞(0,1)
λ
,
which measures how strong convection (characterized by b) is with respect to
diﬀusion (characterized by λ). Notice that, if we choose a downwind or centered
discretization, we obtain monotonicity only for Pe small; see Problems 24.12
and 24.13, respectively. In practice, this means that, although the exact solution
may be monotone, the approximate solution will experience spurious oscillations for
a grid size h that is not suﬃciently small.
Having obtained monotonicity, stability follows ideas we have established before.
To make the arguments more tractable, we separate the cases c ≡0 and 0 ̸≡
c(x) ≥0 for all x ∈[0, 1].
Theorem 24.40 (stability for c ≡0). Let the ﬁnite diﬀerence operator in (24.9)
be given by (24.16) with ci ≡0. Then, for all h > 0, this method has a unique
solution. Moreover, there is a constant C > 0, independent of h > 0 and fh, such
that this solution satisﬁes the estimate
∥w∥L∞
h ≤C∥fh∥L∞
h .
Proof. Lemma 24.38 has shown that the coeﬃcients (24.16) satisfy (24.15). From
Corollary 24.37, existence and uniqueness follow.
It remains then to obtain the stability estimate. The approach is similar to that
of Theorem 24.31, in that we need to construct a comparison function φ ∈V0(¯Ωh)
that, for some positive constants C0, C1 > 0, satisﬁes
0 ≤φ ≤C0,
Lhφ ≤−C1.
If we are able to construct this function, then we deﬁne
ψ± = ±w + ∥fh∥L∞
h
C1
φ
and observe that
Lhψ± = ±fh + ∥fh∥L∞
h
C1
Lhφ ≤±fh −∥fh∥L∞
h ≤0.
The comparison of Lemma 24.36 implies then that
max
i=1,...,N ±wi ≤
max
i=1,...,N ψ±,i ≤C0
C1
∥fh∥L∞
h ,
which is precisely the claimed norm estimate.
5 Named in honor of the French physicist Jean Claude Eug`ene P´eclet (1793–1857).

690
Finite Diﬀerence Methods for Elliptic Problems
We thus need to construct the comparison function. The candidate is, as always,
a quadratic
φi = (xi −1)2 ≥0,
C0 = 1.
Let us then apply the operator Lh to this function. For this, observe that
h2∆φi = (xi+1 −1)2 −2(xi −1)2 + (xi−1 −1)2 = 2h2
and that
h¯δhφi = (xi −1)2 −(xi−1 −1)2 = 2h
xi + xi−1
2
−1

< 0,
because xi−1, xi ∈[0, 1).
Therefore,
Lhφ ≤−2λ,
and the method is monotone.
The case c ̸≡0 is more elaborate and requires several steps.
Theorem 24.41 (stability for c ≥0). Let the ﬁnite diﬀerence operator in (24.9)
be given by (24.16) such that there is at least one point xi ∈Ωh for which ci > 0.
Then, for all h > 0, this method has a unique solution. Moreover, there is a constant
C > 0, independent of h > 0 and fh, such that this solution satisﬁes the estimate
∥w∥L∞
h ≤C∥fh∥L∞
h .
Proof. In a similar way to Theorem 24.40, we obtain existence and uniqueness, so
that we only focus on the estimate.
We decompose w = ˚
w + ¯w, where ˚
w ∈V0(¯Ωh) satisﬁes
−ai∆˚
wi + bi ¯δh ˚
wi = fh,i,
so that Theorem 24.40 shows that
∥˚
w∥L∞
h ≤C∥fh∥L∞
h .
Now, by linearity, the function ¯w ∈V0(¯Ωh) must satisfy
Lh ¯wi = −ci ˚
wi.
We will estimate ¯w using the (discrete) comparison function φ ∈V0(¯Ωh), which
solves
Lhφi = ci∥˚
wi∥L∞
h ≥0.
First, we note that, by Lemma 24.36, we have φ ≥0. In addition,
Lh(± ¯w + φ)i = ci
 ∓˚
wi + ∥˚
w∥L∞
h

≥0.
Lemma 24.36 then implies that, for any i,
0 ≤min{± ¯w + φ} ≤± ¯wi + φi;

24.5 The Poisson Problem in Two Dimensions
691
therefore,
∥¯w∥L∞
h ≤∥φ∥L∞
h .
It remains then to estimate the function φ.
Recall that φ ≥0. Let xm ∈Ωh be the point where φ attains its, necessarily
positive, maximum. At this point, we must have
∆hφm ≤0,
¯δhφm ≥0.
Consequently,
0 ≤cmφm ≤Lhφm = −am∆hφm + bm¯δhφm + cmφm = cm∥˚
w∥L∞
h .
If cm ̸= 0, then
∥φ∥L∞
h = φm ≤∥˚
w∥L∞
h ,
which is the needed estimate.
If, on the other hand, cm = 0, then we can rewrite the equation for φ as
am
h2 (φm −φm+1) +
am
h2 + bm
h

(φm −φm−1) = 0,
which is only possible if φm−1 = φm = φm+1. We can now repeat the argument
at the points xm±1 to reach either the desired conclusion or that φm−2 = φm−1 =
φm = φm+1 = φm+2. Since c ̸≡0, we will eventually conclude that either φ ≡0 or
reach an index, say j, for which cj > 0, and obtain the needed estimate.
24.5
The Poisson Problem in Two Dimensions
In this section, we introduce the two-dimensional Poisson problem on, for simplicity,
a square domain Ω= (0, 1)2. Recall that, for v ∈C2(R2),
∆v(x1, x2) = ∂2v(x1, x2)
∂x2
1
+ ∂2v(x1, x2)
∂x2
2
.
Thus, we are trying to approximate the solution to
−∆u(x1, x2) = f (x1, x2),
(x1, x2) ∈Ω,
u|∂Ω= 0,
(24.17)
where f ∈C(¯Ω) is given. We again recall that Theorem 23.8 provided suﬃcient
conditions for the existence and uniqueness of a classical solution. This is the object
that we will try to approximate via ﬁnite diﬀerences.
Let us then deﬁne our ﬁnite diﬀerence approximation.
Deﬁnition 24.42 (ﬁnite diﬀerence approximation). Let d = 2, f ∈C(¯Ω), and
u ∈C2(Ω) ∩C(Ω) be a classical solution to the two-dimensional Poisson problem
(24.17). Let N ∈N and h =
1
N+1. We call w ∈V0(¯Ωh) a ﬁnite diﬀerence
approximation to u if and only if
−∆hwi,j = fi,j,
(ih, jh) ∈Ωh,
(24.18)

692
Finite Diﬀerence Methods for Elliptic Problems
where fi,j = f (ih, jh) and ∆h denotes the two-dimensional discrete Laplace operator
of Example 24.7.
We must begin by studying the consistency properties of the two-dimensional
discrete Laplace operator of Example 24.7. To do so, we must deﬁne suitable norms,
so that our setting has the approximation property. The following are analogues
of the norms we introduced in Examples 24.15—24.17. The reason for the scaling
can be inferred from similar arguments to the one-dimensional case.
Deﬁnition 24.43 (discrete Lp
h-norms). Let d = 2 and p ∈[1, ∞). The Lp
h-norm
on V0(¯Ωh) or V(Ωh) is
∥v∥Lp
h =

h2
N
X
i,j=1
|vi,j|p


1/p
.
For p = 2, this norm comes from the L2
h-inner product
(v, φ)L2
h = h2
N
X
i,j=1
vi,jφi,j.
The L∞
h -norm on these spaces is
∥v∥L∞
h =
max
i,j=1,...,N |vi,j| .
The space V0(¯Ωh) with these norms has the approximation property in Lp(Ω),
for p < ∞, and C0(¯Ω), for p = ∞, respectively. For further properties of these
norms, see Problem 24.8.
We can now study consistency.
Proposition 24.44 (consistency). The two-dimensional discrete Laplace operator
of Example 24.7 is consistent, in Cb(R2), to order exactly two with the Laplacian.
Proof. See Problem 24.14.
With consistency at hand, the program to analyze our method should by now be
clear. We need to study existence, uniqueness, and stability of the ﬁnite diﬀerence
approximation (24.18), and from these conclude convergence. We begin by showing
that this problem has a unique solution.
Theorem 24.45 (stiﬀness matrix). Let N ∈N. Deﬁne AN ∈RN×N via
AN =


4
−1
0
. . .
0
−1
4
...
...
0
...
...
−1
0
...
−1
4
−1
0
. . .
0
−1
4


.

24.5 The Poisson Problem in Two Dimensions
693
Let ON, IN ∈RN×N denote the zero and identity matrices, respectively. Deﬁne the
matrix A ∈RN2×N2 via
A =


AN
−IN
ON
. . .
ON
−IN
AN
...
...
ON
...
...
−IN
ON
...
−IN
AN
−IN
ON
. . .
ON
−IN
AN


.
(24.19)
The grid function w ∈V0(¯Ωh) ←→w ∈RN2 is a solution to the ﬁnite diﬀerence
problem (24.18) if and only if it is a solution to the problem
Aw = h2f ,
(24.20)
with f ∈V(Ωh) ←→f ∈RN2. By linearity, the error e ∈V0(¯Ωh) ←→e ∈RN2 and
the consistency error Eh[u] ∈V0(¯Ωh) ←→EEEh[u] ∈RN2 are related by
Ae = h2EEEh[u].
(24.21)
Proof. See Problem 24.15.
The matrix A constructed in the previous result is usually called the two-
dimensional stiﬀness matrix. Let us now prove that this matrix has suitable
properties.
Theorem 24.46 (spectrum of A). Let N ∈N. Suppose that A ∈RN2×N2 is the
stiﬀness matrix deﬁned in (24.19). Consider the set of vectors
S =

ϕk+(n−1)N
 (kh, nh) ∈Ωh
	
,
where the components of ϕk+(n−1)N, for (ih, jh) ∈Ωh, are

ϕk+(n−1)N

i+(j−1)N = ϕk+(n−1)N,i+(j−1)N = sin(kπih) sin(nπjh) .
Then we have:
1. S is an orthogonal set of eigenvectors of A.
2. The eigenvalue λk+(n−1)N corresponding to the eigenvector ϕk+(n−1)N is given by
λk+(n−1)N = 2 (2 −cos(kπh) −cos(nπh)) = 4 sin2
kπh
2

+ 4 sin2
nπh
2

.
Therefore, 0 < λk+(n−1)N < 8, for all (kh, nh) ∈Ωh; consequently, A is an SPD
matrix.
3. There is a constant C1 > 0, independent of h, such that, if 0 < h < 1
2,
A−1
2 =
1
8 sin2   hπ
2
 ≤C1h−2.
4. The spectral condition number of A satisﬁes the estimate
κ2(A) = ∥A∥2
A−1
2 ≤8C1h−2.
Proof. See Problem 24.16.

694
Finite Diﬀerence Methods for Elliptic Problems
From this, existence and uniqueness immediately follow.
Corollary 24.47 (well-posedness). For every N ∈N, there is a unique solution
w ∈V0(¯Ωh) ←→w ∈RN2 to the ﬁnite diﬀerence problem (24.18).
24.5.1
Convergence in the L2
h-Norm
As in the one-dimensional case, we have almost everything that is needed to present
a convergence result in the L2
h-norm.
Theorem 24.48 (convergence). Let u ∈C4(Ω) be a classical solution to the two-
dimensional Poisson problem (24.17). Let N ∈N. Suppose that w ∈V0(¯Ωh) ←→
w ∈RN2 is a solution to the ﬁnite diﬀerence problem (24.18). Let e ∈V0(¯Ωh) ←→
e ∈RN2 be its error. Then there is a constant C2 > 0, independent of h, such that
∥Eh[u]∥L∞
h ≤C2h2.
Furthermore, if 0 < h < 1
2,
∥e∥L2
h ≤C1C2h2,
where C1 > 0 is the constant from Theorem 24.46.
Proof. The consistency estimate follows from Proposition 24.44.
To obtain convergence, we recall that the consistency error Eh[u] and error e are
related by (24.21). Therefore, e = h2A−1EEEh[u] and
∥e∥2 ≤h2 A−1
2 ∥EEEh[u]∥2 ≤C1 ∥EEEh[u]∥2 ≤C1h−1 ∥EEEh[u]∥∞≤C1C2h.
Using the fact that ∥e∥L2
h = h ∥e∥2, the result follows.
Remark 24.49 (suboptimality). Once again, we get a suboptimal error estimate
in the L∞
h -norm using our L2
h estimate:
∥e∥L∞
h ≤1
h ∥e∥L2
h ≤C1C2h.
We sharpen this estimate in the next section.
24.5.2
A Discrete Maximum Principle and Convergence in the L∞
h -Norm
We now state and prove a Discrete Maximum Principle for two-dimensional grid
functions.
Theorem 24.50 (Discrete Maximum Principle). Let d = 2. Suppose that v ∈
V(¯Ωh) is such that
−∆hv(x) ≤0,
∀x ∈Ωh.
Then
max
x∈¯Ωh
v(x) ≤max
x∈∂Ωh v(x).
In other words, the maximum must occur on the boundary.

24.5 The Poisson Problem in Two Dimensions
695
Proof. To obtain a contradiction, suppose that a strict maximum occurs in the
interior. If this is true, there is some (kh, ℓh) ∈Ωh such that
v(kj, ℓh) =
max
(ih,jh)∈Ωh v(ih, jh) >
max
(ih,jh)∈∂Ωh v(ih, jh).
For simplicity, let us suppose that 2 ≤k, ℓ≤N −1. Then
0 ≥−h2∆hvk,ℓ= −vk−1,ℓ−vk+1,ℓ−vk,ℓ−1 −vk,ℓ+1 + 4vk,ℓ≥0.
This implies that ∆hwk,ℓ= 0 and
vk,ℓ= 1
4(vk−1,ℓ+ vk+1,ℓ+ vk,ℓ−1 + vk,ℓ+1).
The only way to satisfy the last equation and the fact that vk,ℓ≥vk±1,ℓ, vk,ℓ±1 is
to have vk,ℓ= vk±1,ℓ= vk,ℓ±1.
We can now repeat our argument at neighboring points, and we conclude that
vk,ℓ= vi,j,
∀(ih, jh) ∈˜Ωh = Ωh\ {(h, h), (h, Nh), (Nh, h), (N, N)} .
Next to the left boundary, we have, assuming that (h, jh) ∈˜Ωh,
0 ≥−h2∆hv1,j = −v0,j −v2,j −v1,j−1 −v1,j+1 + 4v1,j > 0,
because v1,j > v0,j. This is a contradiction. The other possible cases are treated
similarly.
With the help of the Discrete Maximum Principle, we can obtain stability.
Theorem 24.51 (stability). Let d = 2. Given f ∈V(Ωh), suppose that w ∈V(¯Ωh)
satisﬁes
−∆hw(x) = f (x),
∀x ∈Ωh.
Then there is some constant C > 0, independent of h and w, such that
∥w∥L∞
h ≤
max
(ih,jh)∈∂Ωh |wi,j| + C∥f ∥L∞
h .
Proof. The strategy, as in previous cases, is to construct a comparison function.
This time, the function Φ: [0, 1]2 →R is
Φ(x) =
x −
" 1
2
1
2
#
2
2
≥0.
Deﬁne the grid function φi,j = Φ(ih, jh). Then, for all (ih, jh) ∈Ωh,
−∆Φ(ih, jh) ≡−4 = −∆hΦi,j.
Deﬁne the grid function
Ψ± = ±w +
∥f ∥L∞
h
4
Φ.
Notice that, in Ωh, we have
−∆hΨ = ±f −∥f ∥L∞
h ≤0.

696
Finite Diﬀerence Methods for Elliptic Problems
By the Discrete Maximum Principle then, for all (ih, jh) ∈Ωh,
±wi,j ≤Ψi,j ≤max
∂Ωh Ψ ≤max
∂Ωh w +
∥f ∥L∞
h
8
,
as we needed to show.
Corollary 24.52 (stability). Let d = 2. Suppose that v ∈V(¯Ωh) satisﬁes
−∆hv(x) = 0,
∀x ∈Ωh.
Then
∥v∥L∞
h ≤max
∂Ωh |v|.
Proof. See Problem 24.17.
With these tools, we can now conclude convergence.
Corollary 24.53 (convergence). Suppose that u ∈C4(Ω) is a classical solution to
the two-dimensional Poisson problem (24.17). Let N ∈N and w ∈V0(¯Ωh) ←→
w ∈RN2 be a solution to the ﬁnite diﬀerence problem (24.18). Let e ∈V0(¯Ωh) ←→
e ∈RN2 be its error. Then there is a constant C3 > 0, independent of h, such that
∥e∥L∞
h ≤C2C3h2,
where C2 > 0 is the local truncation error constant from Theorem 24.48.
Proof. Repeat the proof of Corollary 24.32.
Problems
24.1
Prove Proposition 24.6.
24.2
Prove Proposition 24.9.
24.3
Complete the proof of Theorems 24.25 and 24.45.
24.4
Provide all the details for Example 24.16.
24.5
Provide all the details for Example 24.17.
24.6
Prove Proposition 24.14.
24.7
Prove Proposition 24.20.
24.8
This problem is about properties of the Lp
h-norms introduced in Example
24.15 for d = 1 and Deﬁnition 24.43 for d = 2. Let p, q, r ∈[1, ∞]. Show that,
for every w ∈V0(¯Ωh), we have:
a)
Discrete embedding: If p < q, then
∥w∥Lp
h ≤∥w∥Lq
h.
b)
Interpolation: If p ≤r ≤q, then
∥w∥Lr
h ≤∥w∥θ
Lp
h∥w∥1−θ
Lq
h ,
1
r = θ
p + 1 −θ
q
.
c)
Inverse inequality: If p < q, then
∥w∥Lq
h ≤hd/q−d/p∥w∥Lp
h.

Problems
697
24.9
Show that (24.12) and (24.13) satisfy (24.11).
24.10
Prove Theorem 24.34.
24.11
Prove Lemma 24.36.
24.12
Let the operator L be deﬁned in (24.10) with b(x) ≥0 and a(x) ≥λ > 0
for all x ∈[0, 1]. Consider a downwind discretization, i.e., one where the ﬁrst
derivative is discretized using forward diﬀerences,
Lhvi = −ai∆vi + biδhvi + civi.
Show that this method is monotone, in the sense that its coeﬃcients satisfy the
assumptions of Lemma 24.36, provided that
∥b∥L∞(0,1)h
λ
≤1.
24.13
Let the operator L be deﬁned in (24.10) with b(x) ≥0 and a(x) ≥λ > 0
for all x ∈[0, 1]. Consider a centered discretization, i.e., one where the second
derivative is discretized using centered diﬀerences,
Lhvi = −ai∆vi + bi˚δhvi + civi.
Show that this method is monotone, in the sense that its coeﬃcients satisfy the
assumptions of Lemma 24.36, provided that
∥b∥L∞(0,1)h
2λ
≤1.
24.14
Prove Proposition 24.44.
24.15
Prove Theorem 24.45.
Hint: Review the one-dimensional case.
24.16
Prove Theorem 24.46.
24.17
Prove Corollary 24.52.
24.18
The purpose of this problem is to introduce the method of undetermined
coeﬃcients to construct ﬁnite diﬀerence operators with a prescribed stencil that
are consistent with some diﬀerential operator. Thus, assume that we wish to
construct a ﬁnite diﬀerence operator Dh : V ¯N(Zh) →V ¯N(Zh) that is consistent, on
Cb(R), with the second derivative and has stencil {−2, −1, 0, 1, 2}. The method
of undetermined coeﬃcients consists of expressing, for v ∈V ¯N(Zh), Dhvi as
Dhvi = Avi−2 + Bvi−1 + Cvi + Dvi+1 + Evi+2,
where the coeﬃcients {A, B, C, D, E} are to be determined.
a)
By using Taylor expansions, ﬁnd the coeﬃcients.
b)
Show that there is k ≥2 such that, if u ∈Ck
b(R), the ﬁnite diﬀerence operator
Dh is consistent, on Cb(R), with the second derivative to order four.
c)
What is the minimal value of k for the previous item to hold?
24.19
Here we will construct higher order approximations to the solution to
(24.4).

698
Finite Diﬀerence Methods for Elliptic Problems
a)
Armed with the solution to Problem 24.18, propose an FDM whose consistency
error satisﬁes
∥Eh[u]∥L∞
h ≤Ch4
for some constant C > 0 independent of h. This method, however, has a major
drawback compared with (24.6). What is this drawback?
b)
Another approach to obtain a higher order discretization, while preserving the
stencil, is to use the diﬀerential equation. We will sketch this procedure, and
your job will be to ﬁll in the blanks.
i)
Observe that, during the course of the solution to Problem 24.7, we
actually obtained that
∆hv = v ′′ + h2
12v (4) + O(h4).
ii)
If u and f are suﬃciently smooth, using the diﬀerential equation, we see
that
u(4) = −f ′′;
thus, the method: Find w ∈V0(¯Ωh) such that
−∆hw = f + h2
12f ′′ in Ωh
will satisfy
∥Eh[u]∥L∞
h ≤Ch4.
24.20
The purpose of this problem is to treat other types of boundary conditions
besides Dirichlet. Consider the Neumann problem
−u′′(x) + u(x) = f (x),
x ∈Ω= (0, 1),
−u′(0) = g1,
u′(1) = 1.
A ﬁnite diﬀerence approximation will read: Find w ∈V(¯Ωh) such that
−∆hw(x) + w(x) = fh(x),
x ∈Ωh,
where fh ∈V(Ωh) satisﬁes fh(xi) = f (xi). The issue at hand is to discretize the
boundary conditions:
a)
Since the boundary conditions are ﬁrst-order derivatives, one can use left and
right ﬁnite diﬀerence approximations, respectively. What will be the eﬀect of
this on the consistency error?
b)
Another option is to use the method of undetermined coeﬃcients, introduced
in Problem 24.18, to construct, for instance,
Av0 + Bv1 + Cv2 = −v ′(0) + O(h2).
Find {A, B, C} and a corresponding discretization for the boundary condition
at x = 1, so that the consistency error has order two. This approach, however,
destroys a fundamental structural property of the FDM. Which one?
c)
As in Problem 24.19, one can use the diﬀerential equation to construct a
method whose consistency order is higher order, but the ensuing matrix is still
tridiagonal. Do this.

Problems
699
24.21
The estimates obtained in this chapter allow us to provide convergence in
other Lp
h-norms. Consider problem (24.4) and its ﬁnite diﬀerence approximation
given by (24.6).
a)
Show that, for p ∈(1, 2),
∥πhu −w∥Lp
h ≤Ch2.
Hint: Recall the embedding properties given in Problem 24.8.
b)
Show that, for p ∈(2, ∞),
∥πhu −w∥Lp
h ≤Ch2.
Hint: Recall the interpolation inequalities given in Problem 24.8.
24.22
Prove the following discrete embedding: There is a constant C > 0,
independent of h > 0, such that, for all v ∈V0(¯Ωh),
∥v∥L∞
h ≤C∥v∥H1
h.
24.23
Prove the following inverse inequality: There is a constant CI > 0,
independent of h > 0, such that, for all v ∈V0(¯Ωh),
∥v∥H1
h ≤CIh−1∥v∥L2
h.

25
Finite Element Methods
for Elliptic Problems
The purpose of this chapter is to present the most rudimentary facts of the theory
of ﬁnite element methods for the approximation of partial diﬀerential equations.
Finite element methods have established themselves as the de facto method to
approximate the solution to most problems in the sciences and engineering. This
is due to several natural advantages:
• Its formulation is based on integral — read weak — formulations of the problems
to be discretized. As we have seen, weak formulations allow for more general
problem data.
• The idea of decomposing the domain into a ﬁnite number of pieces (the
elements) and treating the properties and behavior of each piece as known
and ﬁxed not only has roots in the physical origins of many problems but also
allows us to easily treat general geometries, unstructured meshes, and local mesh
reﬁnements without great complication; see, for example, Figure 25.4.
• The approximate solution to our problem is a piecewise polynomial function, i.e.,
a polynomial on each of the elements. Having an actual function instead of,
say, a grid one is sometimes desirable, as this function can be easily evaluated,
diﬀerentiated, etc., without any special considerations.
• As opposed to a ﬁnite diﬀerence methodology, in ﬁnite elements one is
not discretizing the partial diﬀerential operators, but rather searching for an
approximation of the solution in a subspace of the solution space. For this
reason, many of the properties of the discrete problem are automatically inherited
from the continuous one: ﬁnite element methods are, in their simplest form,
automatically consistent and stable. The only remaining question is then how
well elements of the said subspace can approximate generic members of the
solution space.
Several works can be credited with originally developing the idea of ﬁnite element
methods, and we will not make any attempt here at a historical account. We refer
the interested reader to, for instance, [67, 92] for a detailed history of the origins
of the ﬁnite element method and its analysis.
The organization of this chapter will be as follows. First, we will present the
general theory of Galerkin methods, which serves as a tool not only for the analysis
of ﬁnite element methods but for other techniques as well, like the spectral methods
of Chapter 26 and classes of collocation methods discussed in Chapter 27. Then
we proceed with the construction and detailed analysis of a ﬁnite element method
for a one-dimensional problem. The two-dimensional case is illustrated in the last

25.1 The Galerkin Method
701
section, but to avoid technicalities we present several results without proofs. The
reader is referred to [9, 10, 17, 30] for a full account of the theory.
25.1
The Galerkin Method
As we mentioned above, the ﬁnite element method aims to approximate the weak
solution to an elliptic boundary value problem. Let us recall that, in Section
23.2.3, the general theory of variational problems in Hilbert spaces was used to
provide an analysis of weak solutions. With the help of the Lax–Milgram Lemma
(Theorem 23.10), we obtained existence, uniqueness, and the stability of solutions.
The Galerkin method, in its general form, aims to approximate the solution to
a variational problem in a Hilbert space via approximations that lie in (ﬁnite-
dimensional) subspaces. We have already seen incarnations of this principle when we
studied the conjugate gradient method for the iterative solution of linear systems
of algebraic equations, see Deﬁnition 7.27, and when we studied Galerkin methods
for ordinary diﬀerential equations in Chapter 22. Here, we will present the general
theory behind this idea.
Let us then operate in the setting of Section 23.2.3 and assume that H is a
Hilbert space, A: H×H →R is bounded and of coercive bilinear form, and F ∈H′.
We seek to approximate the solution to: Find u ∈H such that
A(u, v) = F(v),
∀v ∈H.
(25.1)
Let us now deﬁne Galerkin approximations in this general setting.
Deﬁnition 25.1 (Galerkin approximation1). Let n ∈N and Hn ≤H, with dim Hn =
n. We say that un ∈Hn is a Galerkin approximation to u, solution of (25.1), if
and only if
A(un, vn) = F(vn),
∀vn ∈Hn.
We recall also that, if A is symmetric, we can deﬁne the energy
E(v) = 1
2A(v, v) −F(v),
(25.2)
and Theorem 23.10 showed that u ∈H solves (25.1) if and only if it minimizes
E over H. The idea behind a Ritz approximation is to minimize this energy over a
subspace.
Deﬁnition 25.2 (Ritz approximation2). Let n ∈N and Hn ≤H, with dim Hn = n.
We say that un ∈Hn is a Ritz approximation to u, solution of (25.1), if and only if
un = argmin
vn∈Hn
E(vn).
Before we begin to provide an analysis of these methods, we must realize what
we have gained with a Galerkin, or Ritz, approach. Let us introduce a basis of Hn,
1 Named in honor of the Russian mathematician and engineer Boris Grigorievich Galerkin
(1871–1945).
2 Named in honor of the Swiss theoretical physicist Walther Heinrich Wilhelm Ritz
(1878–1909).

702
Finite Element Methods for Elliptic Problems
Bn = {φi}n
i=1 ,
Hn = span Bn.
Then every vn ∈Hn has a unique representation of the form
vn =
n
X
i=1
viφi
←→
v = [v1, . . . , vn]⊺∈Rn.
Since this representation is dependent on the basis, we will often write
vn ∈Hn
Bn
←→v ∈Rn
to emphasize this connection. The object on the right is called the coordinate
vector.
Now let us use such representation in Deﬁnition 25.1 and notice that, by linearity,
it is suﬃcient to set vn = φj ∈Bn. We obtain
A(un, φj) = A
 n
X
i=1
uiφi, φj
!
=
n
X
i=1
A(φi, φj)ui = F(φj),
j = 1, . . . , n.
We have obtained a linear system of equations for u
Bn
←→un, namely
Au = f ,
where fi = F(φi), for i = 1, . . . , n. All the properties of this system are contained
in the so-called stiﬀness matrix.
Deﬁnition 25.3 (stiﬀness matrix). Suppose that Hn is a ﬁnite-dimensional
subspace of H with basis Bn = {φ1, . . . , φn}. The square matrix A = [ai,j] ∈Rn×n
with entries
ai,j = A(φj, φi),
i, j ∈{1, . . . , n}
is called the stiﬀness matrix relative to the basis Bn. The vector f = [fi] ∈Rn
with entries
fi = F(φi),
i = 1, . . . , n
is called the load vector relative to the basis Bn.
It turns out that many of the properties of the bilinear form A imply useful
properties of the stiﬀness matrix, as the following result shows.
Lemma 25.4 (properties of A). Let Hn be a ﬁnite-dimensional subspace of H.
The stiﬀness matrix A = [ai,j] ∈Rn×n relative to the basis Bn = {φ1, . . . , φn} is
positive deﬁnite. In addition, if A is symmetric, so is A.
Proof. Let vn ∈Hn
Bn
←→v ∈Rn be arbitrary. Then, using the coercivity of A(·, ·),
we get
v ⊺Av =
n
X
i=1
n
X
j=1
A(φj, φi)vivj = A(v, v) ≥α ∥v∥2
H .
The symmetry of A is obtained by observing that, since A is symmetric,
ai,j = A(φj, φi) = A(φi, φj) = aj,i.

25.1 The Galerkin Method
703
The stiﬀness matrix is invertible; therefore, we obtain existence and unique-
ness of Galerkin approximations. The following result should be compared with
Theorem 7.28.
Theorem 25.5 (uniform well-posedness). Suppose that Hn is a ﬁnite-dimensional
subspace of H, with basis Bn = {φ1, . . . , φn}. There is a unique un ∈Hn
Bn
←→u ∈
Rn that is a Galerkin approximation of the solution to (25.1), which, moreover,
satisﬁes
Au = f .
If, in addition, A is symmetric, then un ∈Hn is the Galerkin approximation to u if
and only if it is the Ritz approximation to u. Finally, we have the estimate
∥un∥H ≤1
α ∥F∥H′ .
Proof. See Problem 25.1.
25.1.1
C`ea’s Lemma and Galerkin Orthogonality
Having obtained that, regardless of the dimension n ∈N and the choice of subspace
Hn, a Galerkin approximation is always well posed, we can proceed to obtain an error
analysis for it. The following results, which usually bear the names of C´ea’s Lemma
and Galerkin orthogonality, express that a Galerkin approximation is, in a sense,
the best possible. We have seen instances of these facts before; see Proposition
7.29.
Theorem 25.6 (C´ea’s Lemma3). Suppose that Hn is a ﬁnite-dimensional subspace
of H, with basis Bn = {φ1, . . . , φn}. Suppose that u ∈H is the unique solution to
(25.1) and un ∈Hn
Bn
←→u ∈Rn is the unique Galerkin approximation in the sense
of Deﬁnition 25.1. Then
A(u −un, vn) = 0,
∀vn ∈Hn,
(25.3)
which is called Galerkin orthogonality. Furthermore, we have the quasi-best
approximation property
∥u −uh∥H ≤M
α min
v∈Hn ∥u −v∥H ,
where M and α denote the boundedness and coercivity constants, respectively, of
the bilinear form A.
Proof. Notice that, since Hn ⊂H, we can set, in (25.1), the test function v =
vn ∈Hn. Subtracting this from the deﬁnition of Galerkin approximation, we then
obtain the Galerkin orthogonality relation (25.3).
3 Named in honor of the French mathematician Jean C´ea (1932–).

704
Finite Element Methods for Elliptic Problems
Now, for vn ∈Hn arbitrary, using coercivity and Galerkin orthogonality, we have
α∥u −un∥2
H ≤A(u −un, u −un)
= A(u −un, u −vn) + A(u −un, vn −un)
= A(u −un, u −vn)
≤M∥u −un∥H∥u −vn∥H,
where in the last step we used the boundedness of A. Since vn ∈Hn is arbitrary,
∥u −un∥H ≤M
α
inf
vn∈Hn ∥u −vn∥H .
But, of course, the inﬁmum is achieved at vn = un,
∥u −un∥H = M
α min
vn∈Hn ∥u −vn∥H ,
which is the claimed quasi-best approximation property.
Remark 25.7 (energy norm). Notice that if A is symmetric, then it deﬁnes the
so-called energy norm
∥v∥E = A(v, v)1/2,
∀v ∈H.
The proof of the previous result shows that, in this case, we actually have the best
approximation property
∥u −uh∥E = min
v∈H ∥u −v∥E .
Remark 25.8 (best approximation). Notice that the content of C´ea’s Lemma
reduced the analysis of a Galerkin approximation to a question in approximation
theory. In other words, after this result, it is no longer of relevance that we are
trying to approximate the solution to (25.1). The only thing that matters is how
well an object in H can be approximated by elements in the subspace Hn.
25.2
The Finite Element Method in One Dimension
In this section, we will present the construction of a ﬁnite element method and
provide all the details of its analysis. To keep the theoretical constructions, and
technical details, to a minimum, we will do so in a simple one-dimensional problem.
The two-dimensional case will be presented, mostly without proofs, in the following
section. For a complete analysis of the ﬁnite element method, with various degrees
of generality and abstraction, we refer the reader to [9, 10, 17, 30].
Let d = 1, Ω= (0, 1), and consider the linear diﬀerential operator Lv = −(av ′)′,
for some function a ∈C(¯Ω), such that, for some α, M > 0,
0 < α ≤a(x) ≤M,
∀x ∈[0, 1].

25.2 The Finite Element Method in One Dimension
705
We will consider the Dirichlet problem for this operator, i.e., given f ∈L2(Ω), we
seek a function u ∈H1
0(Ω) such that
A(u, v) =
Z 1
0
au′v ′ dx =
Z 1
0
f v dx,
∀v ∈H1
0(Ω).
(25.4)
Notice that, owing to Corollary 23.11, this problem is well posed. We can thus
study approximations to its solution.
The ﬁnite element method is nothing but a Galerkin approximation with a
particular choice of subspace and basis. The construction of a ﬁnite element space,
in this setting, is detailed below.
Deﬁnition 25.9 (mesh). Let Ω= (0, 1), a triangulation or mesh of Ωis a partition
of Ωinto subintervals Ii, which we call elements:
Th = {Ii}N
i=0,
Ii = (xi, xi+1),
hi = xi+1 −xi,
where the nodes are given by
0 = x0 < x1 < · · · < xN+1 = 1.
Notice that, in the previous deﬁnition, we are not assuming that the nodes are
equally spaced, i.e., hi is not constant. We set
h =
max
i=0,...,N hi,
which we call the mesh size. Subordinate to the mesh Th we deﬁne a ﬁnite element
space.
Deﬁnition 25.10 (ﬁnite element spaces). Given a mesh Th, we deﬁne the ﬁnite
element spaces of continuous piecewise linear functions
S 1,0(Th) =

vh ∈C([0, 1])
 vh|Ij ∈P1, 0 ≤j ≤N
	
,
S 1,0
0
(Th) = S 1,0(Th) ∩H1
0(Ω).
The Lagrange nodal basis4 of S 1,0(Th) is given by
{φi}N+1
i=0 ⊂S 1,0(Th),
φi(xj) = δi,j,
and the Lagrange nodal basis of S 1,0
0
(Th) is {φi}N
i=1.
Remark 25.11 (hat function). A typical depiction of a function φi is given in Figure
25.1. This motivates us to call them hat functions.
Remark 25.12 (implementation). Recall that the support of a function is deﬁned as
supp v = {x |v(x) ̸= 0}.
Notice that
supp φi ∩supp φj ̸= ∅
⇐⇒
|i −j| ≤1.
This means that the stiﬀness matrix will be very sparse; in fact, tridiagonal in the
present case. In addition, because of the way the basis functions are deﬁned, when
4 Named in honor of the Italian, later naturalized French, mathematician and astronomer
Joseph-Louis Lagrange (1736–1813).

706
Finite Element Methods for Elliptic Problems
1
x0 = 0
x1
x2
x3
x4
xN+1 = 1
Figure 25.1 A one-dimensional hat function, i.e., a member of the Lagrange nodal basis
for S 1,0(Th).
computing the entries of the stiﬀness matrix or load vector, we can subdivide the
integral into elements and operate locally, where the basis functions are linear. For
instance,
[A]i,j = ai,j =
Z 1
0
a(x)φ′
jφ′
i dx =
N
X
k=0
Z
Ik
a(x)φ′
jφ′
i dx.
Now, because of how the hat functions are deﬁned, we observe that
φ′
i(x) =







1
hi−1 ,
xi−1 < x < xi,
−1
hi ,
xi < x < xi+1,
0,
x /∈[xi−1, xi+1].
This can be used to eﬃciently implement the ﬁnite element method.
Let us now proceed to analyze the ﬁnite element method in one dimension. Recall
that, from C´ea’s Lemma (Theorem 25.6), we immediately obtain that
∥u −uh∥H1
0(0,1) ≤C
inf
vh∈S 1,0
0
(Th) ∥u −vh∥H1
0(0,1).
It is now necessary to ﬁnd the best approximation error, i.e., the right-hand side of
the previous inequality. To do so, we will construct an interpolation operator
Πh : H1
0(0, 1) →S 1,0
0
(Th),
and we will show that it has suitable approximation properties. That the interpo-
lation operator is deﬁned for H1
0(0, 1) functions is a consequence of the following
observation.
Remark 25.13 (one-dimensional embedding). In one space dimension, and in one
dimension only, we have
H1(Ω) ,→C(Ω);

25.2 The Finite Element Method in One Dimension
707
see Problem D.6. This means that H1(Ω) ⊂C(Ω), and there exists a constant
C > 0, independent of u, such that
∥u∥L∞(0,1) ≤C ∥u∥H1(0,1) .
The one-dimensional interpolation operator Πh is deﬁned as follows.
Deﬁnition 25.14 (Lagrange interpolant). The Lagrange nodal interpolation
operator
Πh : C([0, 1]) →S 1,0(Th)
is (uniquely) deﬁned by Πhv(xi) = v(xi) for all i = 0, . . . , N + 1.
Let us now prove error estimates for the Lagrange nodal interpolant. Many of
the ideas behind the proof of the following result bear a lot of resemblance to what
was done in Part II, in particular in Chapter 9. The main diﬀerence here is that
we are measuring smoothness in Sobolev spaces, instead of spaces of continuously
diﬀerentiable functions.
Theorem 25.15 (properties of Πh). Let Πh be as in Deﬁnition 25.14. There is a
constant C > 0, independent of the mesh spacings, such that, for all j = 0, . . . , N,
and all v ∈H1(0, 1) such that v ′′ ∈L2(0, 1),
∥v −Πhv∥L2(Ij) ≤Ch2
j ∥v ′′∥L2(Ij),
∥v ′ −Πhv ′∥L2(Ij) ≤Chj∥v ′′∥L2(Ij).
Proof. To alleviate the notation, let us denote by I = (xl, xr) a generic element of
the mesh and h = |I| = xr −xl.
Consider now v ∈H1(0, 1) such that v ′′ ∈L2(0, 1). An argument similar to that
of Problem D.6 shows that v ∈C1([0, 1]). Deﬁne the ﬁrst-order Taylor polynomial
of v about xl
Q1v(x) = v(xl) + v ′(xl)(x −xl) ∈P1.
Notice that:
1. Πh is polynomial space preserving in the following sense: ΠhQ1v, when restricted
to I, equals Q1v.
2. The operator Πh is max-norm stable: i.e.,
∥Πhv∥L∞(I) = max{|v(xl)|, |v(xr)|} ≤∥v∥L∞(I).
Now, to bound the interpolation error v −Πhv, we proceed as follows:
∥v −Πhv∥L∞(I) ≤∥v −Q1v∥L∞(I) + ∥Q1v −Πhv∥L∞(I) ≤2∥v −Q1v∥L∞(I).
By Taylor’s Theorem with integral remainder, see Theorem B.39,
(v −Q1v)(x) =
Z x
xl
(x −y)v ′′(y)dy,
which implies that
∥v −Q1v∥L∞(I) ≤h
Z
I
|v ′′| dy;

708
Finite Element Methods for Elliptic Problems
as a consequence,
∥v −Q1v∥2
L2(I) =
Z
I
|v −Q1v|2 dx ≤h∥v −Q1v∥2
L∞(I)
≤h3
Z
I
|v ′′| dy
2
≤h4
Z
I
|v ′′|2 dy,
where, in the last step, we applied the Cauchy–Schwarz inequality. Taking square
roots proves the ﬁrst result.
To bound the derivatives, we must note that
(Πhv)′(x) = 1
h
Z
I
v ′(y)dy,
so that
(Πhv −v)′(x) = 1
h
Z
I
(v ′(y) −v ′(x))dy,
which implies that
∥(Πhv −v)′∥2
L2(I) = 1
h2
Z
I
Z
I
(v ′(y) −v ′(x))dy
2
dx
= 1
h2
Z
I
Z
I
Z y
x
v ′′(s)dsdy
2
dx
≤1
h2
Z
I


Z
I
|x −y|1/2
 Z max{x,y}
min{x,y}
|v ′′(s)|2 ds
!1/2
dy


2
dx.
Now, since x, y ∈I, we can bound |x −y| ≤h and
Z max{x,y}
min{x,y}
|v ′′(s)|2 ds ≤
Z
I
|v ′′(s)|2 ds.
Using these upper bounds, we obtain
∥(Πhv −v)′∥2
L2(I) ≤1
h
Z
I
|v ′′(s)|2 ds
Z
I
Z
I
dy
2
dx ≤h2
Z
I
|v ′′(s)|2 ds,
as we needed to show.
Remark 25.16 (stable and space-preserving operator). Notice that, in the course
of the proof of Theorem 25.15, the particular form of the Lagrange interpolation
operator was, ultimately, inconsequential. All that was needed was that the operator
was stable and that it preserved the polynomial space. In consequence, Theorem
25.15 also holds for any other operator that satisﬁes these two properties.
This approximation result immediately implies a convergence estimate for ﬁnite
element methods.

25.2 The Finite Element Method in One Dimension
709
Corollary 25.17 (convergence of ﬁnite element method). Let u ∈H1
0(0, 1) solve
(25.4) and uh ∈S 1,0
0
(Th) be its ﬁnite element approximation. If u is such that
u′′ ∈L2(0, 1), then we have
∥u −uh∥H1
0(0,1) ≤Ch∥u′′∥L2(0,1),
where the constant C > 0 is independent of h > 0, u, and uh.
Proof. By C´ea’s Lemma, we have that
∥u −uh∥H1
0(0,1) ≤C
inf
vh∈S 1
0 (Th) ∥u −vh∥H1
0(0,1) ≤C∥u −Πhu∥H1
0(0,1),
where we used the Lagrange interpolation operator Πh. Notice that if u ∈H1
0(0, 1),
then Πhu(0) = Πhu(1) = 0, so that Πhu ∈S 1,0
0
(Th).
By the local properties of the Lagrange interpolation operator given in Theorem
25.15, we see that
∥u −Πhu∥2
H1
0(0,1) =
N
X
j=0
Z
Ij
|(u −Πhu)′|2 dx ≤C
N
X
j=0
h2
j
Z
Ij
|u′′|2 dx.
Using that h = maxj hj implies the result.
Notice that the previous result, in conjunction with the Poincar´e inequality (D.2),
implies that
∥u −uh∥L2(0,1) ≤CP ∥u −uh∥H1
0(0,1) ≤Ch∥u′′∥L2(0,1).
However, Theorem 25.15 shows that the interpolation error is O(h2). How can we
regain the missing power? For that, we need to study the dual problem, i.e., given
g ∈L2(0, 1), we need to ﬁnd zg ∈H1
0(0, 1) such that
A(v, zg) =
Z 1
0
gvdx,
∀v ∈H1
0(0, 1).
(25.5)
Notice that the order of the arguments in the bilinear form is switched. This is
irrelevant if the bilinear form is symmetric, as in our present case. If it is not
symmetric, the order is quite important.
The following result is usually known as Aubin–Nitsche duality 5 or Nitsche’s
trick.
Theorem 25.18 (L2(Ω)-estimate). Assume that, for every g ∈L2(0, 1), there is
a unique solution to the dual problem (25.5); furthermore, z′′
g ∈L2(0, 1), with the
estimate
∥z′′
g ∥L2(0,1) ≤C∥g∥L2(0,1)
for some constant C > 0. In this case, if u ∈H1
0(0, 1) solves (25.4), it is such
that u′′ ∈L2(0, 1), and uh ∈S 1,0
0
(Th) is its ﬁnite element approximation, then we
have
∥u −uh∥L2(0,1) ≤Ch2∥u′′∥L2(0,1).
5 Named in honor of the French mathematician Jean-Pierre Aubin (1939–) and the German
mathematician Joachim A. Nitsche (1926–1996).

710
Finite Element Methods for Elliptic Problems
Proof. Deﬁne the error e = u −uh ∈H1
0(0, 1) ⊂L2(0, 1). Let ze be the solution
to the dual problem (25.5) with data g = e. If that is the case, then we have
∥e∥2
L2(0,1) = A(e, ze) = A(u −uh, ze) = A(u −uh, ze −Πhze),
where the last equality follows from Galerkin orthogonality. Now, using the
boundedness of the bilinear form, we obtain
∥e∥2
L2(0,1) = A(u −uh, ze −Πhze)
≤M∥u −uh∥H1
0(0,1)∥ze −Πhze∥H1
0(0,1)
≤Ch2∥u′′∥L2(0,1)∥z′′
e ∥L2(0,1),
where we used the convergence estimate of Corollary 25.17 and, since z′′
e
∈
L2(0, 1), the interpolation estimates of Theorem 25.15. Using the estimate on
the second derivatives of ze then yields the result.
Remark 25.19 (higher order elements). In our presentation, we chose the space
S 1,0(Th) to make the discussion as transparent as possible. It is also possible to
deﬁne ﬁnite element spaces of higher order. For p ∈N, we deﬁne
S p,0(Th) =

vh ∈C([0, 1])
 vh|Ij ∈Pp, 0 ≤j ≤N
	
,
S p,0
0
(Th) = S p,0(Th) ∩H1
0(Ω).
The analysis of ﬁnite element methods with these spaces follows verbatim what we
have done here. The only diﬀerence is in the way that the Lagrange interpolation
operator is deﬁned. In short, provided that u(p+1) ∈L2(0, 1), one can prove that
∥u −uh∥H1
0(0,1) ≤Chp∥u(p+1)∥L2(0,1).
Remark 25.20 (spaces of variable degree). The spaces S p,0(Th) can be even
further generalized and allowed to have a diﬀerent polynomial degree within each
element. To deﬁne them, we let p ∈NN+1, which is called the degree vector. Then
S p,0(Th) =

vh ∈C([0, 1])
 vh|Ij ∈Ppj+1, 0 ≤j ≤N
	
,
with S p,0
0
(Th) = S p,0(Th) ∩H1
0(0, 1). These spaces form the building block of
what is known as hp ﬁnite element methods, where, to increase the accuracy of
our numerical approximation, one is allowed to either reduce the local mesh size hj
or increase the polynomial degree pj. The reader is referred, for instance, to [82]
for an account of this methodology.
25.3
The Finite Element Method in Two Dimensions
As we saw in the one-dimensional case of the previous section, the ﬁnite element
method is a particular version of the Galerkin method. More speciﬁcally, the
ﬁnite element method gives a particular subspace where we seek the approximate
solution, and a particular basis for it. In this section, we will present, mostly without

25.3 The Finite Element Method in Two Dimensions
711
Figure 25.2 A conforming polygonal partition of a polygonal domain Ω. For every pair of
elements, only one of the following possibilities holds: either Ki ∩Kj = ∅or Ki ∩Kj = e,
a complete edge of both Ki and Kj, or Ki ∩Kj = {x}, a shared vertex of Ki and Kj. The
ﬁlled circles are the interior vertices and the unﬁlled circles are the boundary vertices.
proofs, the construction and analysis of ﬁnite element methods in two dimensions.
We refer the reader to [9, 10, 17, 30] for full details, and further developments.
As in the one-dimensional case, we begin with a way to decompose our domain.
Deﬁnition 25.21 (polygonal partition). Suppose that Ω⊂R2 is an open, bounded,
polygonal domain. Let
Th = {Ki | i = 1, . . . , Ne}
be a disjoint collection of open subsets of Ωsuch that
Ω=
Ne
[
i=1
Ki.
The members Ki ∈Th are called elements. Th is called a mesh or polygonal
partition of Ωif and only if each Ki is a convex polygon. Th is called a triangulation
of Ωif and only if each element Ki is a triangle. Deﬁne the element diameters via
hi = diam(Ki),
i = 1, . . . , Ne.
The value
h = max
1≤i≤Ne hi
is called the global mesh size. By the set
Nv = {xj | j = 1, . . . , Nv} ,

712
Finite Element Methods for Elliptic Problems
T1/1
level = 0
T1/2
level = 1
T1/4
level = 2
T1/8
level = 3
T1/16
level = 4
Figure 25.3 A family of nested uniform triangulations of a square. Starting at level = 0,
the level = 1 triangulation is obtained by connecting the three midpoints of each
triangle to form four congruent sub-triangles. This process can continue indeﬁnitely. The
global mesh size decreases by two as the level increases by one.
Figure 25.4 A sophisticated triangulation of the state of Tennessee (one of the 50
constituent states of the United States of America).
we denote the set of all vertices of Th, i.e., all the vertex points of the polygons
Ki. By
N i
v = Nv ∩Ω=

xj
 j = 1, . . . , Ni
v
	
,
we denote the set of all interior vertices. The set Nv \ N i
v is the set of boundary
vertices.
Observe that, from the deﬁnition above, a triangulation is a polygonal partition.
Deﬁnition 25.22 (conforming partition). A polygonal partition Th is called
conforming if and only if for every pair of elements, only one of the following
possibilities holds:
1. Ki ∩Kj = ∅,
2. Ki ∩Kj = e, a complete edge of both Ki and Kj, or
3. Ki ∩Kj = {x}, a shared vertex of Ki and Kj.
We show a conforming polygonal partition of a two-dimensional polygonal domain
in Figure 25.2. A family of nested triangulations of a square domain is shown in
Figure 25.3. In Figure 25.4, we exhibit a sophisticated triangulation of the state
of Tennessee, which shows the ﬂexibility of ﬁnite element meshing as a means to
discretize arbitrary bounded polygonal domains.6
We now introduce ﬁnite element spaces related to triangulations.
6 Of course, the state of Tennessee, arguably, does not have a polygonal boundary, especially
along the Mississippi River. In such a case, we often approximate the shape with a polygon.

25.3 The Finite Element Method in Two Dimensions
713
Figure 25.5 Some of the hat basis functions from S 1,0
0
(Th) subordinate to a uniform
conforming triangulation of a rectangle. The supports of the basis functions are the grey
shaded triangles in the mesh. The dark grey triangles indicate the regions where the
supports intersect.
Deﬁnition 25.23 (ﬁnite element space). Suppose that p ∈N and Ω⊂R2 is an
open polygonal domain. Let Th = {Ki} be a conforming triangulation of Ω. Deﬁne
S p,0(Th) =

v ∈C(Ω)
v|K ∈Pp, ∀K ∈Th
	
.
The set S p,0(Th) is called the piecewise polynomial (of degree p) ﬁnite element
space. By the set
S p,0
0
(Th) =

v ∈S p,0(Th)
v|∂Ω= 0
	
,
we denote the subspace of functions that vanish on the boundary.
These spaces are suitable to approximate weak solutions to second-order elliptic
equations, as the following result shows.
Theorem 25.24 (embedding). Suppose that p ∈N and Ω⊂R2 is an open
polygonal domain. Let Th = {Ki} be a conforming triangulation of Ω. Then
S p,0(Th) is a subspace of H1(Ω) and S p,0
0
(Th) is a subspace of H1
0(Ω).
Proof. See [9, 10].
These subspaces are ﬁnite dimensional. In fact, we can construct bases for them.
Theorem 25.25 (basis of S 1,0(Th)). Suppose that Ω⊂R2 is an open polygonal
domain and Th = {Ki} is a conforming triangulation of Ω. Then
dim(S 1,0(Th)) = Nv,
dim(S 1,0
0
(Th)) = Ni
v.
In particular, deﬁning φk ∈S 1,0
0
(Th) via
φk(ζj) = δj,k,
∀ζj ∈N i
v,
we see that {φ1, . . . , φNiv } is a basis for S 1,0
0
(Th). The basis for S 1,0(Th) is
constructed similarly.
Proof. See Problem 25.10.
The basis functions for S 1,0
0
(Th) are called hat functions and are illustrated in
Figure 25.5. The construction of the bases for higher order spaces, S p,0(Th), with
p ≥2, is usually accomplished by adding more nodes.
But, when we make such an approximation, we incur an error, whose estimation is beyond the
scope of the text. See, for example, [10] for a discussion of this advanced topic.

714
Finite Element Methods for Elliptic Problems
Deﬁnition 25.26 (midpoints). Suppose that Ω⊂R2 is an open polygonal domain
and Th = {Ki} is a conforming triangulation of Ω. By
Nm =

ξj
 j = 1, . . . , Nm,
	
,
we denote the set of midpoints of all edges in the triangulation, i.e., the midpoints
set. By
N i
m = Nm ∩Ω=

ξj
 j = 1, . . . , Ni
m
	
,
we denote the set of all interior edge midpoints, i.e., the interior midpoints set.
The set Nm\N i
m is the boundary midpoints set.
Theorem 25.27 (basis of S 2,0(Th)). Suppose that Ω⊂R2 is an open polygonal
domain and Th = {Ki} is a conforming triangulation of Ω. Then
dim(S 2,0(Th)) = Nv + Nm,
dim(S 2,0
0
(Th)) = Ni
v + Ni
m.
In particular, deﬁning φk ∈S 2,0
0
(Th) via
φk(ζj) = δj,k,
∀ζj ∈N i
v ∪N i
m,
we see that {φ1, . . . , φNiv +Nim} is a basis for S 2,0
0
(Th). The basis for S 2,0(Th) is
constructed similarly.
Proof. See Problem 25.11. Figure 25.6 gives an illustration of this construction.
Remark 25.28 (nodal basis). The bases that we constructed in Theorems 25.25
and 25.27 are called Lagrange nodal bases. These have the property that the basis
elements satisfy φk(ζj) = δj,k, for 1 ≤j, k ≤N, where N is the number of elements
in the basis, and

ζj
	
is the Lagrange nodal set. We have only deﬁned this basis
for S p,0
0
(Th) with p = 1, 2, where Ωis a polygonal set. But we can construct this
type of basis for any p ∈N; see, for example, [10].
Having deﬁned ﬁnite element spaces, we can use them to approximate weak
solutions to elliptic problems. Let us illustrate this in the case of the Poisson
problem. Thus, let Ω⊂R2 be a bounded polygonal domain, f ∈L2(Ω), and
we seek u ∈H1
0(Ω) such that
A(u, v) =
Z
Ω
∇u · ∇v dx =
Z
Ω
f v dx = F(v),
∀v ∈H1
0(Ω).
(25.6)
The ﬁnite element method is then a Galerkin method, where we use as a subspace
S p,0
0
(Th) for some p ∈N. Thus, we will seek uh ∈S p,0
0
(Th) such that
A(uh, vh) = F(vh),
∀vh ∈S p,0
0
(Th).
(25.7)
Existence and uniqueness of discrete solutions, as well as a quasi-best approximation
result
∥u −uh∥H1
0(Ω) ≤C
inf
vh∈S p,0
0
(Th)
∥u −vh∥H1
0(Ω),
follow from the general theory described in Section 25.1. It remains then to provide
error estimates.

25.3 The Finite Element Method in Two Dimensions
715
1
10
19
28
37
46
55
64
73
2
11
20
29
38
47
56
65
74
3
12
21
30
39
48
57
66
75
4
13
22
31
40
49
58
67
76
5
14
23
32
41
50
59
68
77
6
15
24
33
42
51
60
69
78
7
16
25
34
43
52
61
70
79
8
17
26
35
44
53
62
71
80
9
18
27
36
45
54
63
72
81
Figure 25.6 A uniform triangulation of a square domain Ωshowing the nodes for the
piecewise quadratic ﬁnite element space S 2,0
0
(Th). The interior edge midpoint nodes are
the ﬁlled circles; the unﬁlled circles are the boundary edge midpoint nodes. The interior
vertex nodes are the ﬁlled squares; the unﬁlled squares are the boundary vertex nodes.
The supports of the Lagrange nodal basis elements φ21, φ62, and φ65 are shown as the
shaded regions. There are exactly 81 nodes in the mesh and dim(S 2,0
0
(Th)) = 81.
Observe that Ni
v = 16 and Ni
m = 65.
Remark 25.29 (sparsity). The supports of the Lagrange nodal basis functions
have minimal or no overlap. Consequently, the stiﬀness matrix A constructed in
the abstract Galerkin framework will be sparse, i.e., having only a few nonzero
elements.
25.3.1
Basic Error Estimates for the Finite Element Method
We now present some basic error estimates for the ﬁnite element method. The
idea is to explore the properties of the ﬁnite element spaces S p,0
0
(Th) to be able
to bound the quasi-best approximation provided by Theorem 25.6. To do so, we
introduce a so-called interpolation operator.
Deﬁnition 25.30 (Lagrange interpolant). Let Ω⊂R2 be an open, bounded, and
polygonal domain. Assume that Th = {Ki} is a conforming triangulation of Ω. Let

716
Finite Element Methods for Elliptic Problems
{φ1, . . . , φN} be the Lagrange nodal basis for the space S p,0
0
(Th), with respect to
the Lagrange nodal set

ζj
	N
j=1, so that
φj(ζk) = δj,k,
1 ≤k, j ≤N.
The Lagrange nodal interpolant
Πh :

v ∈C(Ω)
v|∂Ω= 0
	
→S p,0
0
(Th)
is deﬁned as
Πhv =
N
X
j=1
v(ζj)φj,
v ∈C(Ω).
Proposition 25.31 (projection). Suppose that Ω⊂R2 is an open, bounded, and
polygonal domain; Th = {Ki} is a conforming triangulation of Ω; and {φ1, . . . , φN}
is the Lagrange nodal basis for the space S p,0
0
(Th), with respect to the Lagrange
nodal set

ζj
	N
j=1. Then the Lagrange nodal interpolant Πh is a projection operator,
i.e., Π2
h = Πh.
Proof. See Problem 25.12.
To assess the quality of the approximation provided by the Lagrange interpolant
as the mesh size, h > 0, changes, we must enforce a geometric condition on the
elements of our triangulation. The following deﬁnition quantiﬁes this.
Deﬁnition 25.32 (shape regularity). Let Ω⊂R2 be an open, bounded, and
polygonal domain. Let {Th}h>0 be a family of (not necessarily nested) conforming
triangulations of Ω, parameterized by h > 0; see, for example, Figure 25.3 for the
nested case. The family {Th}h>0 is called shape regular if and only if there is a
constant C > 0 such that, for all h > 0 and all K ∈Th,
1 ≤ρext(K)
ρint(K) ≤C,
where ρext(K) is the radius of the smallest circle that circumscribes the triangle K
and ρint(K) is the radius of the largest circle that is inscribed in K.
Example 25.1
The nested family of triangulations shown in Figure 25.3 is shape
regular. All triangles in every triangulation are right and isosceles.
The following result is a cornerstone of piecewise polynomial approximation
theory in Sobolev spaces.
Theorem 25.33 (interpolation error estimate). Suppose that Ω⊂R2 is an open,
bounded, polygonal domain and {Th}h>0 is a family of (not necessarily nested)
conforming, shape regular triangulations of Ω, parameterized by h > 0, where

25.3 The Finite Element Method in Two Dimensions
717
h = maxK∈Th hK. Then there is a constant C1 > 0, independent of h but may
depend on p, such that, for any v ∈Hp+1(Ω) and all 0 ≤m ≤p,
∥v −Πhv∥Hm(Ω) ≤C1hp+1−m |v|Hp+1(Ω) .
Proof. See, for instance, [10].
Finally, we can combine C´ea’s Lemma with this interpolation estimate to provide
an error estimate.
Theorem 25.34 (error estimate). Suppose that Ω⊂R2 is an open, bounded,
polygonal domain; f ∈L2(Ω); and {Th}h>0 is a family of (not necessarily nested)
conforming, shape regular triangulations of Ω, parameterized by h > 0. Suppose
that u ∈H1
0(Ω) ∩Hp+1(Ω) is a weak solution to the Poisson problem (25.6).
Suppose that uh ∈S p,0
0
(Th) is the ﬁnite element approximation deﬁned by (25.7).
Then
∥u −uh∥H1(Ω) ≤M
α C1hp |u|Hp+1(Ω) ,
where C1 > 0 is the constant from Theorem 25.33.
Proof. One needs to use C´ea’s Lemma, as presented in Theorem 25.6, and the
interpolation error estimate from Theorem 25.33. The details are left to the reader
as an exercise; see Problem 25.13.
If we additionally assume that Ωis convex, we can establish a quasi-optimal error
estimate in L2(Ω) via the Aubin–Nitsche duality technique presented in Theorem
25.18. Convexity is needed because we must invoke the elliptic regularity result
presented in Theorem 23.12.
Theorem 25.35 (duality). Suppose that Ω⊂R2 is an open, bounded, polygonal,
and convex domain; f ∈L2(Ω); and {Th}h>0 is a family of (not necessarily nested)
conforming, shape regular triangulations of Ω, parameterized by h > 0. Suppose
that u ∈H1
0(Ω) is the weak solution of the Poisson problem (25.6) and uh ∈
S p,0
0
(Th) is the ﬁnite element approximation deﬁned by (25.7). Then there is a
constant C2 > 0, independent of h and u, such that
∥u −uh∥L2(Ω) ≤C2h ∥u −uh∥H1
0(Ω) .
Proof. Set e = u −uh ∈H1
0(Ω). Let ze ∈H1
0(Ω) be the unique solution of the
dual problem
A(v, ze) =
Z
Ω
ev dx,
∀v ∈H1
0(Ω).
Notice that, since A is symmetric, the dual problem is equivalent to the original
problem. Since Ωis assumed to be convex, by the elliptic regularity result of
Theorem 23.12, we have that ze ∈H2(Ω) ∩H1
0(Ω) with
|ze|H2(Ω) ≤CR ∥e∥L2(Ω) .

718
Finite Element Methods for Elliptic Problems
Now suppose that vh ∈S p,0
0
(Th) is arbitrary and set v = e in the dual problem.
Using Galerkin orthogonality, and the boundedness of A, we have that
∥e∥2
L2(Ω) =
Z
Ω
e2 dx = A(e, ze) = A(e, ze −vh) ≤M ∥e∥H1
0(Ω) ∥ze −vh∥H1
0(Ω) .
Let us choose vh = Πhz, where Πh is the Lagrange interpolation operator into
S 1,0
0
(Th), the piecewise linear ﬁnite element space. Observe that, for any p ∈N,
we have S 1,0
0
(Th) ⊆S p,0
0
(Th). Then, by Theorem 25.33,
∥e∥2
L2(Ω) ≤M ∥e∥H1
0(Ω) ∥ze −Πhze∥H1
0(Ω) ≤Ch2−1 ∥e∥H1
0(Ω) |ze|H2(Ω)
≤C2h ∥e∥H1
0(Ω) ∥e∥L2(Ω) .
Therefore,
∥e∥L2(Ω) ≤C2h ∥e∥H1
0(Ω)
and the result follows.
The following is an easy corollary.
Corollary 25.36 (L2(Ω) estimate). Suppose that Ω⊂R2 is an open, bounded,
convex, polygonal domain; f ∈L2(Ω); and {Th}h>0 is a family of (not necessarily
nested) conforming, shape regular triangulations of Ω, parameterized by h > 0.
Suppose that, for some p ∈N, u ∈H1
0(Ω) ∩Hp+1(Ω) is a weak solution of the
Poisson problem (25.6). Suppose also that uh ∈S p,0
0
(Th) is the ﬁnite element
approximation deﬁned by (25.7). Then there is a constant C > 0, independent of
h, such that
∥u −uh∥L2(Ω) ≤Chp+1 |u|Hp+1(Ω) .
Proof. See Problem 25.14.
Remark 25.37 (linear ﬁnite elements). If we set p = 1 in Corollary 25.36, i.e., if
we suppose that u ∈H1
0(Ω) ∩H2(Ω) is a weak solution of the Poisson problem,
then
∥u −uh∥L2(Ω) ≤Ch2 |u|H2(Ω) ,
provided that Ω
⊂
R2 is a convex, bounded, polygonal domain. In other
words, we get the expected second-order convergence, as for the ﬁnite diﬀerence
approximation.
Problems
25.1
Prove Theorem 25.5.
25.2
In the setting of Deﬁnition 25.10, show that {φi}N+1
i=0 is indeed a basis of
S 1,0(Th) and, therefore, the dimension of S 1,0(Th) is N + 2.
25.3
In the setting of Deﬁnition 25.10, show that {φi}N
i=1 is indeed a basis of
S 1,0
0
(Th) and, therefore, the dimension of S 1,0
0
(Th) is N.
25.4
Let L > 0 and consider the boundary value problem
−au′′ + cu = f ,
x ∈(0, L),
u(0) = u(L) = 0.
(25.8)

Problems
719
Assume that a, c, and f are constant; that a > 0; and that c ≥0. Write a
weak formulation for it. Find an approximate solution to this problem by Galerkin’s
method over the space span{ϕk}N
k=1 with ϕk(x) = sin(πkx/L).
25.5
Consider the boundary value problem (25.8) with c = 0. Find an approxi-
mate solution to this problem by Galerkin’s method over the space span{φk}N
k=1,
where the functions φk are shifted and integrated Legendre’s polynomials
φk(x) =
Z x
0
Pk(y)dy
and Pk is the shifted Legendre polynomial of degree k on [0, L].
25.6
Let L > 0. Consider the problem
−u′′ = x,
x ∈(0, L),
−u′(0) + u(0) = 1,
u′(L) + u(L) = 0.
Write down the system of equations that results from Galerkin’s method using as
a basis:
a)
Piecewise linear functions over a uniform mesh.
b)
ϕk(x) = cos(πkx/L) for k = 0, . . . , N.
c)
ψk(x) = xk for k = 0, . . . , N.
25.7
Consider the two-point boundary value problem
−u′′ + u = f ,
in (0, 1),
−u′(0) = u′(1) = 0.
Construct a Galerkin approximation over the subspace PN for some N ∈N. Since
PN = span{1, x, . . . , xN},
we can choose the monomials as a computational basis. This, however, turns out
to be a terrible idea. Why?
25.8
Show that the ﬁnite element and ﬁnite diﬀerence approximations of
−u′′ = f , in (0, 1),
u(0) = u(1) = 0,
(25.9)
over a uniform mesh of size h yield the same stiﬀness matrix.
25.9
The Green’s function G(x, y) of (25.9) is
G(x, y) =
(
(1 −x)y,
0 ≤y ≤x ≤1,
x(1 −y),
0 ≤x ≤y ≤1.
a)
Show that the solution of (25.9) is given by
u(x) =
Z 1
0
G(x, y)f (y)dy.
b)
Let S 1,0
0
(Th) be the piecewise linear ﬁnite element space over the given mesh
Th. Show that G(x, xk) ∈S 1,0
0
(Th) if xk is a node of the mesh.
c)
Show that
Z 1
0
v ′(y)∂yG(x, y)dy = v(x),
∀v ∈H1
0(0, 1).
d)
A numerical method is interpolant exact if the numerical solution uh coincides
with the interpolant of the solution, i.e., uh = Πhu. Show that a linear ﬁnite
element method for (25.9) is interpolant exact.

720
Finite Element Methods for Elliptic Problems
e)
Show that
∥u −uh∥L∞(0,1) ≤Ch2∥u′′∥L∞(0,1)
for some constant C > 0 that is independent of h and the solution u.
25.10
Prove Theorem 25.25.
25.11
Prove Theorem 25.27.
25.12
Prove Proposition 25.31.
25.13
Complete the proof of Theorem 25.34.
25.14
Prove Corollary 25.36.
25.15
Consider the problem
(
−∆u + cu = f ,
in Ω= (0, 1)2,
u = 0,
on ∂Ω,
where c > 0 and f ∈L2(Ω). Write a weak formulation for this problem, show that
it is well posed, and describe a Galerkin approximation method. Prove a C´ea-type
lemma for this problem. Assuming that the solution to this problem is such that
u ∈H2(Ω), provide an error estimate for a ﬁnite element method with S 1,0
0
(Th).
25.16
Let N be a positive integer. Deﬁne AN ∈RN×N via
AN =


4
−1
0
. . .
0
−1
4
...
...
0
...
...
−1
0
...
−1
4
−1
0
. . .
0
−1
4


.
Let ON, IN ∈RN×N denote the zero and identity matrices, respectively. Now we
construct the matrix A ∈RN2×N2 via
A =


AN
−IN
ON
. . .
ON
−IN
AN
...
...
ON
...
...
−IN
ON
...
−IN
AN
−IN
ON
. . .
ON
−IN
AN


.
Let Th be the uniform triangulation of the domain Ω= (0, 1)2 described in Figure
25.3 such that there are exactly N2 interior nodes. Number the interior nodes with
the lexicographic ordering, i.e., starting in the lower left-hand corner, proceeding
by rows to the upper right-hand corner. Show that the stiﬀness matrix, obtained
by using the piecewise linear subspace S 1,0
0
(Th), is exactly A.

26
Spectral and Pseudo-Spectral
Methods for Periodic Elliptic
Equations
In this chapter, we will investigate numerical methods for the approximate solution
of elliptic equations with periodic boundary conditions in one space dimension. We
only consider one spatial dimension for simplicity, but everything in our discussion
can be generalized to two- and three-dimensional problems on cuboids as well.
Periodic diﬀerential equations, as we will deﬁne below, arise in many applications.
For instance, they can be used when we want to understand the properties of the
solution to a model, without being obfuscated by boundary eﬀects. In addition,
many phenomena are intrinsically periodic, in the sense that they exhibit a repeating
pattern.
Our discussion of numerical methods begins by illustrating how ﬁnite diﬀerence
and Galerkin methods can be used in this setting. However, the so-called spectral
and pseudo-spectral methods are better suited for this type of problems. As we will
see, taking advantage of periodicity, these methods can converge much faster than
other approaches. The idea here is to use trigonometric functions, the quintessential
representation of periodicity, to approximate the solutions.
26.1
Periodic Diﬀerential Equations
We refer the reader to Appendix D for the deﬁnition and properties of spaces of
complex functions, as it is over these spaces that we will introduce the problem we
shall be interested in here. Given c ≥0 and f ∈Cp(0, 1; C), we seek a [0, 1]-periodic
function u : R →C that solves the [0, 1]-periodic reaction–diﬀusion problem,
−d2u(x)
dx2
+ cu(x) = f (x),
x ∈R.
(26.1)
If c = 0, we call this the [0, 1]-periodic Poisson problem.1
Deﬁnition 26.1 (classical solution). We say that u ∈C2
p(0, 1; C) is a classical
solution to the [0, 1]-periodic reaction–diﬀusion problem if and only if (26.1) holds
point-wise.
Remark 26.2 (boundary conditions). One interesting feature of the problem (26.1)
is that there are, in some sense, no boundary conditions, since we demand that
1 Named in honor of the French mathematician, engineer, and physicist Baron Sim´eon Denis
Poisson (1781–1840).

722
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
(26.1) holds on the whole real number line. Still, we sometimes say the boundary
conditions are of periodic type. In any case, for the periodic problem, one must be
careful about the value of c.
Proposition 26.3 (compatibility). Suppose that f
∈
Cp(0, 1; C) and u
∈
C2
p(0, 1; C) is a classical solution to the [0, 1]-periodic reaction–diﬀusion problem
with c ≥0. Then
c
Z 1
0
u(x)dx =
Z 1
0
f (x)dx.
Consequently, when c = 0, a necessary condition for the existence of a solution is
Z 1
0
f (x)dx = 0.
Proof. The proof follows using integration by parts, Theorem B.38, and is left to
the reader as an exercise; see Problem 26.1.
The following result provides the existence and uniqueness of solutions.
Theorem 26.4 (existence and uniqueness). Suppose that f
∈Cp(0, 1; C). If
c > 0, there exists a unique classical solution u ∈C2
p(0, 1; C) to the [0, 1]-periodic
reaction–diﬀusion problem (26.1). If c = 0 and the right-hand side f satisﬁes the
compatibility condition of Proposition 26.3, then there is a unique classical solution
u to (26.1) in the class ˚C2
p(0, 1; C). Finally, if f is real valued, so is u.
Proof. The existence and uniqueness proof can be obtained using the periodic
Green’s function, or by using Fourier series techniques; see [31, 49, 99].
To show that u must be real valued when f is, it suﬃces to take complex
conjugates of (26.1) and invoke uniqueness.
26.2
Finite Diﬀerence Approximation
We now describe how to approximate the classical solutions to problem (26.1)
using ﬁnite diﬀerence methods. Many of the techniques that we describe here are
similar to those developed in Chapter 24, and so we will follow much of the notation
introduced there.
26.2.1
Periodic Grid Functions
We begin with some preliminary deﬁnitions. Some of these notions were already
introduced in Chapter 13, but we repeat them and adapt them to our interests
here.
Deﬁnition 26.5 (periodic grid functions). Let M ∈N and h =
1
M . We deﬁne the
space of periodic grid functions to be
VM,p(C) = {v ∈V(Zh; C) | vi+mM = vi, ∀i, m ∈Z} .

26.2 Finite Diﬀerence Approximation
723
Notice that the slightly complicated notation is indeed necessary. Periodic
functions are deﬁned on the whole Zh, but they are uniquely characterized by their
values in (0, 1]∩Zh, i.e., the values in ih with i ∈{1, . . . , M}. In addition, since this
will be convenient later, we assume from the onset that periodic grid functions are
complex valued. We can now introduce norms for the space of periodic functions.
Deﬁnition 26.6 (Lp
h-norms). Let p ∈[1, ∞]. We deﬁne, on VM,p(C), the norms
∥v∥Lp
h =
 
h
M
X
i=1
|vi|p
!1/p
,
∀v ∈VM,p(C),
p ∈[1, ∞),
and
∥v∥L∞
h =
M
max
i=1 |vi|,
∀v ∈VM,p(C).
These are the usual norms on spaces of grid functions. They are just specially
adapted to our setting. From this reason, the following result is immediate.
Proposition 26.7 (inner product). The L2
h-norm on VM,p(C) is induced by the inner
product
(v, ϕ)L2
h = h
M
X
i=1
vi ¯ϕi,
∀v, ϕ ∈VM,p(C).
Proof. See Problem 26.2.
Since in the continuous setting, when c = 0, spaces of mean-zero, periodic
functions become important, we also introduce here the space of mean-zero,
periodic grid functions.
Deﬁnition 26.8 (˚VM,p(C)). Let M ∈N. We deﬁne
˚VM,p(C) =
n
v ∈VM,p(C)
 (v, 1)L2
h = 0
o
,
where by 1 we denote the grid function that is constant and has value one at every
node.
Finally, we observe that, as in the case of Chapter 24, grid functions can be
identiﬁed with CM.
Proposition 26.9 (isomorphism). For all M ∈N, there is a one-to-one correspon-
dence between the space of periodic grid functions VM,p(C) and CM. Therefore,
dim(VM,p(C)) = M as a vector space over C.
Proof. Let h =
1
M and Gh = (0, 1] ∩Zh. Observe that #Gh = M. Following
Proposition 24.3, it is not diﬃcult to establish, see Problem 26.3, the isomorphism
between V(Gh; C) and CM. Now if v ∈V(Gh; C), then we can deﬁne its unique
periodic extension v ↑∈VM,p(C) via
v ↑
i+mM = vi,
∀i ∈{1, . . . , N + 1},
∀m ∈Z.

724
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
Conversely, for any v ∈VM,p(C), there is a unique restriction v ↓∈V(Gh; C)
deﬁned via
v ↓
i = vi,
∀i ∈{1, . . . , M}.
Remark 26.10 (notation). As before, we will write
v ∈CM ←→v ∈V(Gh; C) ←→v ∈VM,p(C)
to indicate the isomorphism shown above. If we know that the components of the
vector are real, we will write v ∈RM instead. We use the same convention as before,
denoting a grid function by a Greek or Roman character and its corresponding
canonical vector representative by the boldface of the same character.
26.2.2
Finite Diﬀerence Approximation
We are now ready to introduce the ﬁnite diﬀerence approximation to (26.1).
Deﬁnition 26.11 (ﬁnite diﬀerence approximation). Assume that c ≥0, f
∈
Cp(0, 1; C), and u ∈C2
p(0, 1; C) is the classical solution to the one-dimensional
periodic reaction–diﬀusion problem (26.1). Let M ∈N and h =
1
M . We say that
w ∈VM,p(C) is a ﬁnite diﬀerence approximation to u if and only if
−∆hw + cw = fh,
(26.2)
where fh = πhf . The error is the periodic grid function e ∈VM,p(C) deﬁned via
e = πhu −w.
The consistency error is the grid function Eh[u] ∈VM,p(C),
Eh[u] = −∆hπhu + cπhu −πhf .
Above, as usual, πh denotes the sampling operator, i.e., (πhv)i = v(ih) for all
i ∈Z.
The isomorphism of Proposition 26.9 allows us to realize that problem (26.2)
can be equivalently written as
(A + ch2IM)w = h2f ,
(26.3)
where w ∈CM ←→w ∈VM,p(C), f ∈CM ←→fh ∈VM,p(C), and the matrix
A ∈RM×M is the so-called stiﬀness matrix, which is deﬁned via
A =


2
−1
0
· · ·
−1
−1
2
...
...
0
...
...
−1
0
...
−1
2
−1
−1
· · ·
0
−1
2


.
(26.4)
As before, from a practical point of view, computing a ﬁnite diﬀerence approxima-
tion reduces to the solution of a linear system of equations, which, in this case,

26.2 Finite Diﬀerence Approximation
725
x
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
y
1
2
3
4
Figure 26.1 The eigenvalues of the periodic stiﬀness matrix A are shown (open circles)
for M = 10. Observe that 0 ≤λm ≤4.
is cyclically tridiagonal. In addition, the properties of the matrix A can be used to
study the well-posedness of (26.2) and to obtain error estimates.
26.2.3
Well-Posedness and Convergence of the Finite Diﬀerence Approximation
As in Theorem 24.25, we study the spectrum of the stiﬀness matrix A, and this
will allow us to establish convergence of the method in the L2
h-norm.
Theorem 26.12 (spectrum of A). Let M ∈N. Suppose that A ∈RM×M is the
stiﬀness matrix deﬁned in (26.4). Consider the set of vectors S = {v m}M
m=1 ⊂CM,
where the components of v m are
[v m]k = vm,k = e2πikmh,
k ∈{1, . . . , M}.
Then
1. The set S is an orthonormal set of vectors in the sense that if v j ←→vj ∈
VM,p(C), then
(vi, vj)L2
h = δi,j,
i, j ∈{1, . . . , M}.
2. S is a set of eigenvectors of A and the eigenvalue λm corresponding to the
eigenvector v m is given by
λm = 4 sin2 (πmh) .
Since 0 ≤λm ≤4 (see Figure 26.1), for all m ∈{1, . . . , M}, A is a symmetric
positive semi-deﬁnite matrix.
3. A is not invertible. In particular, ker(A) = span {1}.
Proof. The details are similar to those in Theorem 24.25. We leave the details to
the reader as an exercise; see Problem 26.4.
As an immediate consequence, we obtain well-posedness in the case c > 0.

726
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
Corollary 26.13 (well-posedness). Suppose that c > 0. For every M ∈N, there is a
unique solution w ∈VM,p(C) ←→w ∈CM to the ﬁnite diﬀerence problem (26.2).
Proof. This follows because, when c > 0, the system matrix A+ch2IM for problem
(26.3) is symmetric positive denite.
The case c = 0, as in the continuous setting, requires a compatibility condition.
Theorem 26.14 (conditional well-posedness). Let c = 0. For all M ∈N, problem
(26.2) has a unique solution, which, moreover, satisﬁes w ∈˚VM,p(C) if and only if
fh ∈˚VM,p(C).
Proof. As we observed, w ∈VM,p(C) ←→w ∈CM solves the ﬁnite diﬀerence
problem (26.2) with c = 0 if and only if
Aw = h2f .
Suppose that fh ∈˚VM,p(C). Recall that
ker(A) = im(AH)⊥= im(A⊺)⊥= im(A)⊥,
where we used that A is symmetric. In addition, from Theorem 26.12, we know
that ker(A) = span {1}. Therefore, the compatibility condition implies that
fh ∈˚VM,p(C) ←→f ∈ker(A)⊥= im(A).
In other words, there exists w ∈CM such that Aw = h2f , which shows the
existence of a solution. To show uniqueness, assume that we have two solutions
w 1, w 2 ∈CM. By linearity,
A(w 1 −w 2) = 0
⇐⇒
w 1 −w 2 ∈ker(A)
⇐⇒
(w1 −w2, 1)L2
h = 0.
Since dim ker(A) = 1, this shows that there is a unique solution with the property
w ∈˚VM,p(C).
Assume now that the problem has a solution w ∈˚VM,p(C). Then
h2(f , 1)2 = (Aw, 1)2 = (w, A⊺1)2 = (w, A1)2 = 0,
where, in the last step, we used that A is real and symmetric and 1 ∈ker(A).
Now we prove a convergence result in the L2
h-norm. For simplicity, we focus
on the reaction–diﬀusion problem, i.e., c > 0, and leave the case c = 0 for
Problem 26.7.
Theorem 26.15 (error estimate). Let f ∈Cp(0, 1; C) and u ∈C4
p(0, 1; C) be a
classical solution to the one-dimensional reaction–diﬀusion problem (26.1) with
c > 0. Suppose that M ∈N and w ∈VM,p(C) ←→w ∈CM is a solution to the
ﬁnite diﬀerence problem (26.2). There is a constant C > 0, independent of h, such
that the consistency error satisﬁes
∥Eh[u]∥L∞
h ≤Ch2.

26.3 The Spectral Galerkin Method
727
In addition, there is a, possibly diﬀerent, constant C > 0, independent of h,
for which
∥e∥L2
h ≤Ch2.
Proof. The consistency error estimate is a consequence of Taylor’s Theorem. The
details are left for the reader as an exercise; see Problem 26.6.
To obtain the error estimate, we recall that, as usual,
−∆he + ce = Eh[u],
so that, with the usual identiﬁcations,
e = h2  A + ch2IM
−1 EEEh[u].
We leave it to the reader to prove that, if 0 < h < 1
2,

 A + ch2IM
−1
2 =
1
ch2 = Ch−2.
If this is the case,
∥e∥2 ≤h2 
 A + ch2IM
−1
2 ∥EEEh[u]∥2 ≤C ∥EEEh[u]∥2 ≤C
√
Mh2 = Ch
3
2 .
Using the fact that ∥e∥L2
h =
√
h ∥e∥2, the result follows.
26.3
The Spectral Galerkin Method
In this section, we describe the spectral Galerkin method for solving the periodic
Poisson problem, i.e., (26.1) with c = 0. This, like the ﬁnite element method, is
nothing but a Galerkin method where the subspace and its basis are suitably chosen
to ﬁt the needs of the problem at hand.
26.3.1
Weak Formulation
Galerkin methods aim to approximate weak solutions, and the correct functional
framework for these is Sobolev spaces. The reader is referred to Appendix D for an
overview of these spaces. The weak formulation of (26.1) with c = 0 reads (see
Problem 26.8): Given f ∈˚L2(0, 1; C), ﬁnd u ∈˚
H1
p(0, 1; C) such that
A(u, v) = F(v),
∀v ∈˚
H1
p(0, 1; C),
(26.5)
where
A(v, w) =
Z 1
0
v ′(x)w ′(x)dx,
F(v) =
Z 1
0
f (x)v(x)dx.
The analysis of this problem is a slight modiﬁcation of the theory developed in
Section 23.2.3. The ﬁrst step is the following Poincar´e-type inequality for periodic
functions.

728
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
Theorem 26.16 (Poincar´e2). There is a constant CP > 0 such that
∥u∥L2(0,1;C) ≤CP |u|H1p(0,1;C)
for all u ∈˚
H1
p(0, 1; C).
Proof. We prove this result for ˚C1
p(0, 1). The more general version can be
established using a density argument over the real and imaginary parts. Suppose
that u ∈˚C1
p(0, 1). Since u is continuous, periodic, and has zero mean, there is
a point a ∈[0, 1), where u(a) = 0. Since the function is periodic, without loss
of generality, we may assume that a = 0. Then, by the Fundamental Theorem of
Calculus, for any z ∈[0, 1], since u(0) = 0,
u(z) = u(z) −u(0) =
Z z
0
u′(x)dx.
By the Cauchy–Schwarz inequality,
|u(z)|2 =
Z z
0
1 · u′(x)dx
2
≤
Z z
0
12dx
Z z
0
|u′(x)|2 dx
= z
Z z
0
|u′(x)|2 dx
≤
Z 1
0
|u′(x)|2 dx.
Integrating, we have
Z 1
0
|u(z)|2 dz ≤
Z 1
0
Z 1
0
|u′(x)|2 dx

dz =
Z 1
0
|u′(x)|2 dx;
in this case, the result holds with CP = 1.
We can now proceed with the analysis of problem (26.5).
Theorem 26.17 (well-posedness). Let f ∈˚L2(0, 1; C). Then problem (26.5) has
a unique solution u ∈˚
H1
p(0, 1; C) ∩˚
H2
p(0, 1; C), which satisﬁes
∥u∥H1p(0,1;C) ≤C∥f ∥L2(0,1;C)
for some constant C > 0 that is independent of f and u. Furthermore, if, for some
r ∈N, we have f ∈˚
Hr
p(0, 1; C), then u ∈˚
Hr+2
p
(0, 1; C). Finally, if f ∈˚C∞
p (0, 1; C),
then u ∈˚C∞
p (0, 1; C).
Proof. We leave the proof of existence and uniqueness to the reader as an exercise;
see Problem 26.9.
2 Named in honor of the French mathematician, theoretical physicist, engineer, and philosopher
of science Jules Henri Poincar´e (1854–1912).

26.3 The Spectral Galerkin Method
729
The further regularity follows, essentially, from the following (formal) argument.
Notice, ﬁrst of all, that, since f ∈L2(0, 1; C), the expression
v 7→G(v) =
Z 1
0
f (x)v ′(x)dx
deﬁnes a continuous anti-linear functional in ˚
H1
p(0, 1; C) with
∥G∥˚
H1p(0,1;C)∗≤∥f ∥L2(0,1;C);
see again Problem 26.9 for notation and terminology. We can then invoke the well-
posedness that we just established to assert that the problem: Find ˜u ∈˚
H1
p(0, 1; C)
such that
A(˜u, v) = G(v),
∀v ∈˚
H1
p(0, 1; C),
has a unique solution which satisﬁes
∥˜u∥H1p(0,1;C) ≤C∥f ∥L2(0,1;C).
Set now, in (26.5), v = w ′ with w ∈˚
H2
p(0, 1; C) but otherwise arbitrary and
integrate by parts to obtain
A(u′, w) = G(w),
∀w ∈˚
H1
p(0, 1; C).
This shows that ˜u = u′, so that u′ ∈˚
H1
p(0, 1; C), i.e., u ∈˚
H2
p(0, 1; C).
The construction of the spectral Galerkin method requires knowledge of the
eigenvalues and eigenfunctions of the second derivative subject to periodic boundary
conditions. We show this in the following example, which should be compared with
Theorem 23.14.
Example 26.1
Let us obtain the eigenvalues and eigenfunctions of the diﬀerential
operator −d2v
dx2 on the interval (0, 1) subject to periodic boundary conditions. Deﬁne
for x ∈R and k ∈N,
ψk(x) = sin(2πkx),
φk(x) = cos(2πkx).
Deﬁne φ0(x) = 1 for all x ∈R. Then, clearly,
−d2ψk
dx2 = (2π)2k2ψk = λkψk,
−d2φk
dx2 = (2π)2k2φk = ηkφk,
and ψk, φk ∈˚C∞
p (0, 1). Observe that the eigenvalues grow unboundedly:
λk = ηk = (2π)2k2 →∞,
k →∞.
The previous computations show that {λk, φk}k<0 ∪{0, φ0} ∪{λk, ψk}k>0 are
indeed eigenvalues and eigenfunctions of this operator.
It is not diﬃcult to see that the eigenfunctions are orthogonal, i.e., for any
k, ℓ∈N, k ̸= ℓ,
(ψk, ψℓ)L2(0,1) = 0 = (φk, φℓ)L2(0,1);

730
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
for any k, ℓ∈N,
(ψk, φℓ)L2(0,1) = 0;
and, ﬁnally, for k ∈N,
(ψk, φ0)L2(0,1) = 0 = (φk, φ0)L2(0,1).
The last orthogonality is a restatement of the mean-zero property. The normaliza-
tions are
(ψk, ψk)L2(0,1) = 1
2 = (φk, φk)L2(0,1),
k ∈N,
and
(φ0, φ0)L2(0,1) = 1.
To simplify the notation and calculations, one can use complex trigonometric
functions instead of real ones. Deﬁne, for all x ∈R, k ∈Z,
χk(x) = e2πikx.
Then
−d2χk
dx2 = (2π)2k2χk = γkχk,
and we observe that
γk = γ−k = (2π)2k2.
In the complex case, the orthogonality property is expressed using the complex
L2-inner product. For the present example,
(χk, χℓ)L2(0,1;C) =
Z 1
0
e2πikxe−2πiℓxdx =
(
1,
k = ℓ,
0,
k ̸= ℓ.
26.3.2
Spectral Galerkin Methods
We are now ready to introduce the subspace that will deﬁne the spectral Galerkin
method.
Deﬁnition 26.18 ( ˚
SN(0, 1; C)). Let N ∈N. The space of complex-valued, mean-
zero, trigonometric polynomials of degree at most N is
˚
SN(0, 1; C) =
(
v =
N
X
k=−N
αkχk
αk ∈C, α0 = 0
)
.
Deﬁnition 26.19 (spectal Galerkin approximation3). Suppose that f ∈˚L2(0, 1; C)
and N ∈N is given. The spectral Galerkin approximation of the periodic Poisson
problem, i.e., (26.1) with c = 0, is a function uN ∈
˚
SN(0, 1; C) that satisﬁes
3 Named in honor of the Russian mathematician and engineer Boris Grigorievich Galerkin
(1871–1945).

26.3 The Spectral Galerkin Method
731
A(uN, vN) = F(vN),
∀vN ∈
˚
SN(0, 1; C).
Theorem 26.20 (well-posedness). Suppose that f ∈˚L2(0, 1; C). For all N ∈N,
there is a unique spectral Galerkin approximation uN ∈
˚
SN(0, 1; C) to the solution
of (26.5) in the sense of Deﬁnition 26.19. In particular, if
f =
X
k∈Z\{0}
ˆfkχk,
ˆfk = (f , χk)L2(0,1;C),
then
uN =
N
X
k=−N
k̸=0
ˆukχk,
ˆuk =
ˆfk
(2π)2k2 .
(26.6)
Furthermore, if f is real valued, then so is uN.
Proof. Existence and uniqueness follow after minor modiﬁcation to the general
theory of Galerkin approximations that take into account that functions in our
space are complex valued; see Problem 26.10.
To obtain the claimed representation, let ℓ∈{±1, ±2, . . . , ±N}. Set vN = χℓ
in Deﬁnition 26.19. We obtain
ˆfℓ= (f , χℓ)L2(0,1;C)
= A(uN, χℓ)
=
Z 1
0
d
dx



N
X
k=−N
k̸=0
ˆukχk(x)


dχℓ(x)
dx
dx
=
N
X
k=−N
k̸=0
ˆuk
Z 1
0
dχk(x)
dx
dχℓ(x)
dx
dx
=
N
X
k=−N
k̸=0
ˆuk(2π)2kℓ
Z 1
0
e2πikxe−2πiℓxdx
= ˆuℓ(2π)2ℓ2,
where in the last step we used orthonormality.
Finally, f is real valued if and only if ˆfk = ˆf−k for all k ∈Z. Clearly, uN inherits
this property.
An error estimate follows from the general theory of Galerkin approximations.
Theorem 26.21 (error estimate). Suppose that, for some r ∈N, we have f ∈
˚
Hr−1
p
(0, 1; C), so that the weak solution to the periodic Poisson problem (26.5)
satisﬁes u ∈˚
Hr+1
p
(0, 1; C). Let N ∈N and uN ∈
˚
SN(0, 1; C) be its spectral

732
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
Galerkin approximation in the sense of Deﬁnition 26.19. Then there is a constant
C > 0, which is independent of f , u, and N, for which we have
∥u −uN∥H1p(0,1;C) ≤γ
α
C
Nr |u|Hr+1
p
(0,1;C) .
(26.7)
Proof. From C´ea’s Lemma we obtain the quasi-optimal approximation property
∥u −uN∥H1p(0,1;C) ≤C
min
vN∈˚
SN(0,1;C)
∥u −vN∥H1p(0,1;C) .
The combination of the last estimate with the approximation result in Theorem
12.31 gives the desired estimate. We leave the remaining details to the reader as
an exercise; see Problem 26.11.
26.4
The Pseudo-Spectral Method
The main motivation for the so-called pseudo-spectral method comes from the
following simple example.
Example 26.2
Suppose that I ⊂Z is an index set of ﬁnite cardinality and
v(x) =
X
k∈I
αke2πikx.
Then
dv(x)
dx
=
X
k∈I
αk(2πik)e2πikx
and
d2v(x)
dx2
= −
X
k∈I
αk(2πk)2e2πikx.
In other words, derivatives modify the Fourier coeﬃcients of the series representa-
tion of a function in simple ways.
Deﬁnition 26.22 (pseudo-spectral derivative). Suppose that M ∈N, v ∈VM,p(C),
and ˆv ∈VM,p(C) is its Discrete Fourier Transform (DFT), in the sense of Deﬁnition
13.8. If M = 2K+1, K ∈N, we deﬁne the ﬁrst- and second-order pseudo-spectral
derivatives, respectively, of v via
Dhvj =
K
X
k=−K
ˆvk(2πik)e2πijkh,
D2
hvj = −
K
X
k=−K
ˆvk(2πk)2e2πijkh.
If M = 2K, K ∈N, the pseudo-spectral derivatives are deﬁned via
Dhvj =
K
X
k=−K
ˆvkωk(2πik)e2πijkh,
D2
hvj = −
K
X
k=−K
ˆvkωk(2πik)2e2πijkh,

26.4 The Pseudo-Spectral Method
733
where the weight grid function ω is deﬁned as
ωk =



1
2,
k = ±K,
1,
k ̸= ±K.
Remark 26.23 (symmetry). Let M ∈N. Suppose that v ∈VM,p(C) and ˆv ∈
VM,p(C) is its DFT. Regarding the deﬁnitions above, observe that if M is odd,
M = 2K + 1, K ∈N, then
vj =
M−1
X
k=0
ˆvke2πijkh =
K
X
k=−K
ˆvke2πijkh,
and if M is even, M = 2K, K ∈N, then
vj =
M−1
X
k=0
ˆvke2πijkh =
K
X
k=−K
ωk ˆvke2πijkh.
In other words, we do not change anything by shifting our summations. However,
note that, when M = 2K + 1, K ∈N,
M−1
X
k=0
ˆvk(2πik)e2πijkh ̸=
K
X
k=−K
ˆvk(2πik)e2πijkh,
and when M = 2K, K ∈N,
M−1
X
k=0
ˆvk(2πik)e2πijkh ̸=
K
X
k=−K
ωk ˆvk(2πik)e2πijkh.
The problem is that the grid functions α, γ, deﬁned by αk
= −2πik and
γk = (2πk)2, respectively, are not periodic grid functions. They are, however,
odd and even grid functions, respectively. The pseudo-spectral derivatives that we
introduced in Deﬁnition 26.22 are chosen to be symmetric sums about k = 0.
Proposition 26.24 (periodicity). Suppose that M ∈N, v ∈VM,p(C), and ˆv ∈
VM,p(C) is its DFT. The pseudo-spectral derivatives of v are periodic grid functions,
i.e., Dhv, D2
hv ∈VM,p(C).
Proof. See Problem 26.12.
Deﬁnition 26.25 (pseudo-spectral method). Let M ∈N. Suppose that fM ∈
˚Vm,p(C) is given, where ˚VM,p(C) was introduced in Deﬁnition 26.8. Consider the
following problem: Find w ∈˚VM,p(C) such that
−D2
hw = fM.
(26.8)
The solution to this problem, if it exists, is called the pseudo-spectral approxi-
mation of u, where u is the solution of the periodic Poisson problem, (26.1) with
c = 0.
Let us now show the well-posedness of (26.8).

734
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
Theorem 26.26 (well-posedness). Let M ∈N. Suppose that fM ∈˚VM,p(C) is given
and ˆf ∈VM,p(C) is its DFT. Problem (26.8) has a unique solution, w ∈˚VM,p(C).
In particular, if M is odd, i.e., M = 2K + 1, then
wj =
K
X
k=−K
ˆwke2πijkh,
ˆwk =
ˆfk
(2πk)2 ,
k = ±1, . . . , ±K,
with a similar result in the case that M is even.
Proof. Let us, for deﬁniteness, assume that M ∈N is odd, and that ˆw and ˆf are
the DFTs of w and fM, respectively. Since (fM, 1)L2
h = 0, ˆf0 = 0. We then have
that w solves (26.8) if and only if
K
X
k=−K
ˆwk(2πk)2e2πijkh =
K
X
k=−K
ˆfke2πijkh,
if and only if
ˆwk =
ˆfk
(2πk)2 ,
k = ±1, . . . , ±K.
The case when M ∈N is even is similar; see Problem 26.13.
As with the spectral Galerkin method, we can prove spectral convergence
properties for this approximation. We omit a discussion of this topic for the sake
of brevity. The interested reader can consult [13].
Example 26.3
Here, we describe a practical implementation of the pseudo-
spectral approximation method. Suppose that f ∈˚Cp(0, 1; C) is given. Let M ∈N.
Deﬁne ˜f ∈VM,p(C) via
˜fi = f (ih),
∀i ∈Z.
Observe now that, in general, ˜f ̸∈˚VM,p(C). For this reason, we deﬁne the so-called
mean-zero projection
Ph : VM,p(C) →˚VM,p(C),
v 7→Phv = v −(v, 1)L2
h.
Let now ˆf be the DFT of Ph ˜f . Observe that ˆf0 = 0, as desired. Assuming that M
is odd, the DFT of the solution is then given by
ˆwk =
ˆfk
(2πk)2 ,
k = ±1, . . . , ±K.
Set ˆw0 = 0. Finally, we construct the solution via the Inverse Discrete Fourier
Transform (IDFT),
w =
K
X
k=−K
ˆwke2πijkh.

Problems
735
Example 26.4
In this example, we approximate the solution of a problem whose
exact solution is known. Suppose that L > 0 and u is the [0, L]-periodic function
u(x) = exp(sin(qx)) −C,
q = 2π
L ,
where the constant C can be chosen to satisfy a mean-zero condition, as desired.
Suppose that f is such that
f = −u′′ = q2 exp(sin(qx))

cos2(qx) −sin(qx)

.
It is not diﬃcult to see that f is a mean-zero function, i.e.,
R L
0 f (x)dx = 0, and
that it is [0, L]-periodic. We can apply the ﬁnite diﬀerence and pseudo-spectral
methods to approximate the solutions of u. To do so, we ﬁrst must adjust f , so
that it is mean zero in the discrete sense, using the mean-zero projection deﬁned
in Example 26.3.
Listing 26.1 provides the code for computing approximate solutions. Figures
26.2, 26.3, and 26.4 show the output from this code for M = 8, 16, and 32,
respectively. Observe that the pseudo-spectral approximations are much more
accurate than the ﬁnite diﬀerence approximations. This is illustrated in Figure 26.5.
Remark 26.27. The error analysis of the pseudo-spectral method, which demon-
strates the spectral accuracy of the method rigorously, is both interesting and
a little bit challenging. We omit it here for the sake of brevity. However, the
concepts used in Chapter 27, where we discuss collocation methods at length,
can be modiﬁed easily to give this analysis. In fact, the pseudo-spectral method is
a type of collocation method, as one can easily show. The details are left to the
curious reader.
Problems
26.1
Prove Proposition 26.3.
26.2
Prove Proposition 26.7.
26.3
Complete the proof of Proposition 26.9.
26.4
Prove Theorem 26.12.
26.5
Let M ∈N and deﬁne T ∈RM×M via
T =


α
β
0
· · ·
0
β
α
...
...
0
...
...
β
0
...
β
α
β
0
· · ·
0
β
α


.
A matrix of this type is called TST, for Toeplitz symmetric tridiagonal. Find the
eigenvectors and eigenvalues of this matrix.
26.6
Complete the proof of Theorem 26.15.
26.7
Prove a version of Theorem 26.15 for the case c = 0.

736
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
0
0.25
0.50
0.75
1.0
1.25
1.50
1.75
2.0
−1.0
−0.50
0
0.50
1.0
1.50
exact and approximate solutions
x
max error ps = 1.479e−03
max error fd = 1.617e−01
pseudo-spectral
ﬁnite diﬀerence
exact
0
1
2
3
4
5
6
7
8
0
2
4
6
8
normalized eigenvalues
wave number, k
pseudo-spectral
ﬁnite diﬀerence
Figure 26.2 Pseudo-spectral and ﬁnite diﬀerence approximation of the solution to a
periodic Poisson problem with eight grid points (M = 8); see Example 26.4 for details.
26.8
Derive the weak formulation (26.5) of the periodic Poisson problem, i.e.,
(26.1) with c = 0.
26.9
The purpose of this problem is to complete the proof of Theorem 26.17.
To do so, we will consider the following. Let H be a complex Hilbert space with
inner product ( · , · )H. We say that a functional F : H →C is anti-linear if
F(αv + βw) = ¯αF(v) + ¯βF(w),
∀α, β ∈C,
∀v, w ∈H.
a)
Let F be an anti-linear functional. We say that it is bounded if there is a
constant C > 0 such that
|F(v)| ≤C ∥v∥H ,
∀v ∈H.
Show that an anti-linear functional is bounded if and only if it is continuous.
b)
The anti-dual space H∗is the set of all continuous, anti-linear functionals on
H. Show that this is a (complex) vector space, that
∥F∥H∗= sup
0̸=v∈H
|F(v)|
∥v∥H
deﬁnes a norm on H∗, and that H∗is complete under that norm.
Hint: Revisit Proposition 16.13.

Problems
737
0
0.25
0.50
0.75
1.0
1.25
1.50
1.75
2.0
−1.0
−0.50
0
0.50
1.0
1.50
exact and approximate solutions
x
max error ps = 8.292e−09
max error fd = 3.606e−02
pseudo-spectral
ﬁnite diﬀerence
exact
0
2
4
6
8
10
12
14
16
0
2
4
6
8
normalized eigenvalues
wave number, k
pseudo-spectral
ﬁnite diﬀerence
Figure 26.3 Pseudo-spectral and ﬁnite diﬀerence approximation of the solution to a
periodic Poisson problem with 16 grid points (M = 16); see Example 26.4 for details.
c)
Show that there is a canonical bijection between H′ and H⋆. Namely, F ∈H′
if and only if ¯F ∈H∗with equality of norms. Here,
¯F : v 7→F(v),
∀v ∈H.
d)
Prove a version of the Riesz Representation Theorem (Theorem 16.14) for
the anti-dual space: Let F ∈H∗. Then there exists a unique element R∗F ∈H
such that
F(v) = (R∗F, v)H,
∀v ∈H.
Moreover, ∥R∗F∥H = ∥F∥H∗.
e)
With this construction, show that problem (26.5) is well posed.
26.10
Complete the proof of Theorem 26.20.
26.11
Complete the proof of Theorem 26.21.
26.12
Prove Proposition 26.24.
26.13
Complete the proof of Theorem 26.26.
26.14
Let M ∈N.
a)
Deﬁne the DFT and IDFT for grid functions in VM,p(C).
b)
Prove that the DFT is bijection from VM,p(C) to VM,p(C).

738
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
0
0.25
0.50
0.75
1.0
1.25
1.50
1.75
2.0
−1.0
−0.50
0
0.50
1.0
1.50
exact and approximate solutions
x
max error ps = 6.661e−16
max error fd = 8.801e−03
pseudo-spectral
ﬁnite diﬀerence
exact
0
4
8
12
16
20
24
28
32
0
2
4
6
8
normalized eigenvalues
wave number, k
pseudo-spectral
ﬁnite diﬀerence
Figure 26.4 Pseudo-spectral and ﬁnite diﬀerence approximation of the solution to a
periodic Poisson problem with 32 grid points (M = 32); see Example 26.4 for details.
c)
If w ∈VM,p(C) and bw ∈VM,p(C) is its DFT, prove that
h
M−1
X
k=0
wkwk =
M−1
X
k=0
bwk bwk.
26.15
Consider the periodic Poisson problem on [0, 1] given by (26.1) with c = 0.
Let M ∈N.
a)
Describe the pseudo-spectral approximation of this problem quantitatively.
b)
Suppose that fM ∈VM,p(C). Show that a necessary condition for the solvability
of the pseudo-spectral approximation (26.8) is that (fM, 1)L2
h = 0, i.e., fM ∈
˚VM,p(C).
26.16
Let M ∈N and consider the ﬁnite diﬀerence approximation to the periodic
Poisson problem on [0, 1] given in (26.2) with c = 0. Recall that the stiﬀness
matrix, A ∈RM×M, is given in (26.4) and the equivalent matrix problem is
Aw = h2f .
Use the DFT to ﬁnd a solution to this matrix problem. What assumptions do you
need to make about f and w in order to get a unique solution?

Listings
739
101
102
10−16
10−15
10−14
10−13
10−12
10−11
10−10
10−09
10−08
10−07
10−06
10−05
10−04
10−03
10−02
10−01
1000
grid size, M
error
pseudo-spectral
ﬁnite diﬀerence
M−2
Figure 26.5 Errors in the pseudo-spectral and ﬁnite diﬀerence approximations of the
solution to a periodic Poisson problem; see Example 26.4 for details. The ﬁnite
diﬀerence method exhibits clear second-order convergence, while the error for the
pseudo-spectral method ﬁts the classical spectral convergence pattern. Note that the
saturation in the pseudo-spectral error, at around 10−15, is due to roundoﬀerror.
Listings
1
function [ePSpec,eFDiff] = PoissonPer(L, N)
2
%
3
% This function computes pseudo-spectral and finite difference
4
% approximations to the periodic Poisson equation:
5
%
6
%
-D {xx} u = f, where u and f are [0,L]-periodic
7
%
8
% using the Fast Fourier Transform (FFT) algorithm.
9
%
10
% Input
11
%
12
%
L : the size of the domain
13
%
N : the number of points in the periodic grid. Generally
14
%
this should be a positive, even number.
15
%
16
% Output

740
Spectral and Pseudo-Spectral Methods for Periodic Elliptic Equations
17
%
18
%
ePSpec : the max norm of the error in the pseudo-spectral
19
%
computation.
20
%
eFDiff : the max norm of the error in the finite difference
21
%
computation.
22
%
23
% h is the grid spacing; x holds the nodal values of the grid:
24
h = L/N;
25
x = (1:N)*h;
26
%
27
% Fine grid parameters for representing the exact solution:
28
Nf = 256;
29
hf = L/Nf;
30
xf = (1:Nf)*hf;
31
%
32
% q is a useful factor that allows us to easily change the
33
%
domain size:
34
q = 2*pi/L;
35
%
36
% Wave numbers, k: This definition is really convenient,
37
% essentially making k periodic. Note that our array index
38
% starts at 1, as usual in MATLAB, but the value of k starts
39
% at 0:
40
k = [0:N/2-1 N/2 -N/2+1:-1];
41
%
42
% Right-Hand-Side function, f, forced to be discrete mean-zero:
43
f = q*q*(cos(q*x).*cos(q*x)-sin(q*x)).*exp(sin(q*x));
44
f = -(f-h*sum(f)/L);
45
%
46
% Exact solution, forced to be discrete mean-zero:
47
uExact = exp(sin(q*x));
48
uExactMass = h*sum(uExact)/L;
49
uExact = uExact-uExactMass;
50
%
51
% Eigenvalues of the pseudo-spectral derivative operator -Dˆ2:
52
eigenPSpec = q*q*k.*k;
53
%
54
% Modified to avoid dividing by zero below:
55
eigenPSpec(1) = 1.0;
56
%
57
% Eigenvalues of the finite difference operator -\Delta h.
58
% Note that we have applied a shift here to be consistent with
59
% the pseudo-spectral case:
60
eigenFDiff = 4.0*sin(q*(x-h)/2).*sin(q*(x-h)/2)/(h*h);
61
%
62
% Modified to avoid dividing by zero below:
63
eigenFDiff(1) = 1.0;
64
%
65
% Approximations: Note that MATLAB fft's do not respect the fact
66
% that our data points are real numbers. So we need to get rid
67
% of any imaginary numbers in the results:
68
uPSpec = real((ifft(fft(f)./eigenPSpec)));
69
uFDiff = real((ifft(fft(f)./eigenFDiff)));
70
%
71
% Error computations:
72
ePSpec = max(abs(uPSpec-uExact));

Listings
741
73
eFDiff = max(abs(uFDiff-uExact));
74
%
75
% Redefine uExact for plotting:
76
uExact = exp(sin(q*xf));
77
uExact = uExact-uExactMass;
78
79
hf = figure(1)
80
81
subplot(2,1,1);
82
plot(x,uPSpec,'s',x,uFDiff,'o',xf,uExact,'k-')
83
grid on, xlabel x, ylabel 'exact and approximate solutions';
84
title(['Pseudo-Spectral and Finite Difference ', ...
85
'Approximations: N = ', num2str(N)]);
86
axis([0,L,-1.1,1.7])
87
set(gca,'xTick',0:L/8:L)
88
text(1.25,0.425,['max error ps = ' ...
89
num2str(ePSpec,'%8.3e')],'fontsize', 12)
90
text(1.25,0.125,['max error fd = ' ...
91
num2str(eFDiff,'%8.3e')],'fontsize', 12)
92
legend('pseudo-spectral','finite difference','exact')
93
94
subplot(2,1,2);
95
eigenPSpec(1) = 0.0;
96
eigenFDiff(1) = 0.0;
97
plot(0:N-1,h*h*eigenPSpec,'-s',0:N-1,h*h*eigenFDiff,'-o')
98
grid on, xlabel 'wave number, k', ...
99
ylabel 'normalized eigenvalues'
100
title(['Normalized Eigenvalues of the Pseudo-Spectral' ...
101
' and Finite Difference Operators']);
102
axis([0,N,0,pi*pi])
103
set(gca,'xTick',0:N/8:N)
104
legend('pseudo-spectral','finite difference')
105
106
s1 = ['000' num2str(N)];
107
s2 = s1((length(s1)-3):length(s1));
108
s3 = ['OUT/periodicPoisson', s2, '.pdf'];
109
exportgraphics(hf, s3)
110
111
end
Listing 26.1 Approximation of the periodic Poisson equation.

27
Collocation Methods for Elliptic
Equations
In this chapter, we will design and analyze collocation methods for solving elliptic
boundary value problems (BVPs). We will focus our attention on the following
problem: Let Ω= (−1, 1), c ≥0, and f : Ω→R be given. We seek u : ¯Ω→R
such that
−d2u(x)
dx2
+ cu(x) = f (x),
x ∈Ω,
u(−1) = u(1) = 0.
(27.1)
Our approximation strategy will be as follows. Let {xj}N
j=0 ⊂[−1, 1], which we will
call the set of N + 1 collocation points, with x0 = −1 and xN = 1. We look for an
approximate solution of the form
w(x) =
N
X
j=0
wjLj(x),
x ∈[−1, 1],
where Lj ∈PN is the jth Lagrange nodal basis element, deﬁned in (9.3). Thus,
it follows that w(xj) = wj. We can now form N + 1 linear equations in N + 1
unknowns. The ﬁrst and last equations are, respectively,
w(−1) = w(1) = 0,
which come from the boundary conditions. It then follows that w0 = 0 and wN = 0.
For the remaining N −1 equations, we require that the approximation w satisﬁes
the diﬀerential equation at the interior N −1 collocation points:
−w ′′(xk) + cw(xk) = f (xk),
1 ≤k ≤N −1.
In other words,
N−1
X
j=1

−L′′
j (xk) + cδj,k

wj = f (xk) = fk.
Let us deﬁne the matrix A ∈R(N−1)×(N−1) via
[A]j,k = −L′′
j (xk) + cδj,k,
1 ≤j, k ≤N −1,
(27.2)
and the vectors w, f ∈RN−1 as
[w]k = wk,
[f ]k = fk.
Then the approximation w is determined by solving the equation Aw = f .

27.1 Weighted Sobolev Spaces and Weak Formulation
743
At this stage a numerical analyst must raise several questions. Is A invertible?
How does one choose the collocation points? Is the approximation stable? How
good is the resulting approximation? How can the error analysis be conducted?
We will lean heavily on the excellent books by Shen, Tang, and Wang [85] and
Canuto et al. [13], as we navigate and answer these questions.
27.1
Weighted Sobolev Spaces and Weak Formulation
To provide a suitable analysis of a collocation method for (27.1), we must deﬁne
a special type of weak solution; to do so, we need to deﬁne a particular weighted
Sobolev space. Let
α(x) =
1
√
1 −x2 ,
x ∈(−1, 1),
be the Chebyshev weight function on (−1, 1). Observe that we have switched from
our usual notation. Since w will be reserved for our approximation, we use α to
denote our weight function. To create a weak solution, we will multiply (27.1) by
the weight function α and a test function v, and integrate by parts.
Let
Bα(−1, 1) =

g ∈C1([−1, 1])
 lim
x↓−1 α(x)g(x) = 0, lim
x↑1 α(x)g(x) = 0

.
Suppose that u ∈C2([−1, 1]) and v ∈Bα(−1, 1). Then we ﬁnd
−
Z 1
−1
α(x)u′′(x)v(x)dx = −α(x)v(x)u′(x)|x=1
x=−1 +
Z 1
−1
(α(x)v(x))′ u′(x)dx
=
Z 1
−1
(α(x)v(x))′ u′(x)dx
=
 α−1 (αv)′ , u′
L2α(−1,1) .
Recall that
(u, v)L2α(−1,1) =
Z 1
−1
u(x)v(x)α(x)dx.
In other words, for all u ∈C2([−1, 1]) and v ∈Bα(−1, 1),
 α−1 (αv)′ , u′
L2α(−1,1) = −(v, u′′)L2α(−1,1) .
Remark 27.1 (space Bα(−1, 1)). What kind of functions are in Bα(−1, 1)?
Suppose that v ∈PN, N ≥2 has zeros at x = ±1. Then
v(x) = (x −1)(x + 1)r(x),
r ∈PN−2,
and it is clear that
lim
x↓−1 α(x)v(x) = 0, lim
x↑1 α(x)v(x) = 0.

744
Collocation Methods for Elliptic Equations
Deﬁnition 27.2 (weighted Sobolev space). Suppose that α is the Chebyshev weight
function on (−1, 1) and m ∈N0. We say that u ∈Hm
α (−1, 1) if and only if
u ∈L2
α(−1, 1), u is m times weakly diﬀerentiable on (−1, 1), and
|u|2
Hℓα(−1,1) =
u(ℓ)
2
L2α(−1,1) < ∞,
∀ℓ∈{1, . . . , m}.
For u, v ∈Hm
α (−1, 1), we deﬁne
(u, v)Hm
α (−1,1) =
m
X
ℓ=0

u(ℓ), v (ℓ)
L2α(−1,1) .
The space Hm
α (−1, 1) is called a Chebyshev weighted Sobolev space of order
m.1
Theorem 27.3 (properties of Hm
α (−1, 1)). Suppose that α is the Chebyshev weight
function on (−1, 1). Then Hm
α (−1, 1) is a Hilbert space when equipped with the
inner product ( · , · )Hm
α (−1,1). Furthermore, H1
α(−1, 1) ,→C([−1, 1]), meaning that
if u ∈H1
α(−1, 1), then u coincides, up to a set of measure zero, with a function in
C([−1, 1]) and there is a constant C > 0, independent of u, such that
∥u∥L∞(−1,1) ≤C ∥u∥H1α(−1,1) .
Proof. See [13] or [85].
In light of the previous result, we can, in particular, speak of point values of a
function in a Chebyshev weighted Sobolev space. For this reason, and to later take
into account the boundary conditions, we deﬁne a subspace with zero boundary
values.
Deﬁnition 27.4 (H1
α,0(−1, 1)). Suppose that α is the Chebyshev weight function
on (−1, 1). We deﬁne
H1
α,0(−1, 1) =

u ∈H1
α(−1, 1)
 u(−1) = u(1) = 0
	
.
For all u, v ∈H1
α,0(−1, 1), set
(u, v)H1
α,0(−1,1) = (u′, v ′)L2α(−1,1) .
Proposition 27.5 (embedding). Suppose that α is the Chebyshev weight function
on (−1, 1). Then we have
Bα(−1, 1) ⊂H1
α,0(−1, 1).
Proof. Let v ∈Bα(−1, 1). By deﬁnition, the function and its ﬁrst derivative are
continuous on [−1, 1], and thus bounded there. Consequently,
Z 1
−1
α(x)
 |v(x)|2 + |v ′(x)|2
dx ≤M
Z 1
−1
α(x)dx < ∞
1 Named in honor of the Soviet mathematician Sergei Lvovich Sobolev (1908–1989).

27.1 Weighted Sobolev Spaces and Weak Formulation
745
and v ∈H1
α(−1, 1). To show that the boundary values vanish, we see that, by
continuity,
v(1) = lim
x↑1 v(x) = lim
x↑1 α(x)v(x)
1
α(x) = lim
x↑1 α(x)v(x) lim
x↑1
1
α(x) = 0 · 0 = 0,
where we used the condition α(x)v(x) →0 as x ↑1. The value at x = −1 can be
treated similarly; thus, we have the result.
Theorem 27.6 (Poincar´e). The following Poincar´e-type inequality2 is valid: there
is a constant Cα,P > 0 such that
∥u∥L2α(−1,1) ≤Cα,P ∥u′∥L2α(−1,1)
(27.3)
for any u ∈H1
α,0(−1, 1). Consequently, H1
α,0(−1, 1) is a Hilbert space with the
inner product ( · , · )H1
α,0(−1,1) and the induced norm
∥u∥H1
α,0(−1,1) =
q
(u, u)H1
α,0(−1,1) =
q
(u′, u′)L2α(−1,1),
∀u ∈H1
α,0(−1, 1).
Furthermore, the following norm equivalence is valid:
1
q
1 + C2
α,P
∥u∥H1α(−1,1) ≤∥u∥H1
α,0(−1,1) ≤∥u∥H1α(−1,1) ,
∀u ∈H1
α,0(−1, 1).
Proof. Clearly, the norm equivalence follows from (27.3).
We will prove (27.3) for functions in Bα(−1, 1). A density result, see [13] or
[85], closes the argument. Let u ∈Bα(−1, 1). As the proof of the previous
result shows, we have u(−1) = 0. By the Fundamental Theorem of Calculus, we
then have
u(x) = u(x) −u(−1)
=
Z x
−1
u′(y)dy
=
Z x
−1
α−1/2(y)α(y)1/2u′(y)dy
≤
Z x
−1
α−1(y)dy
1/2 Z x
−1
α(y)|u′(y)|2 dy
1/2
≤
Z 1
−1
α−1(y)dy
1/2 Z 1
−1
α(y)|u′(y)|2 dy
1/2
,
where, in the last two steps, we applied the Cauchy–Schwarz inequality and the
fact that the functions being integrated are nonnegative. With this estimate at
hand, we see that
2 Named in honor of the French mathematician, theoretical physicist, engineer, and philosopher
of science Jules Henri Poincar´e (1854–1912).

746
Collocation Methods for Elliptic Equations
Z 1
−1
α(x)|u(x)|2 dx ≤
Z 1
−1
α(x)
Z 1
−1
α−1(y)dy
 Z 1
−1
α(y)|u′(y)|2 dy

dx
=
Z 1
−1
α(x)dx
 Z 1
−1
α−1(y)dy

∥u′∥2
L2α(−1,1) .
The estimate of Problem 27.1, part (b) allows us to conclude. In fact, we see that
C2
α,P ≤4Cα.
Remark 27.7 (embedding). Notice that the proof of the previous result also
shows that
H1
α,0(−1, 1) ⊂L∞(−1, 1),
with the corresponding norm estimate.
We are now ready to deﬁne and analyze our weak formulation.
Deﬁnition 27.8 (weighted weak formulation). Let α be the Chebyshev weight
function on (−1, 1) and f ∈L2
α(−1, 1). The function u ∈H1
α,0(−1, 1) is called a
Chebyshev weighted weak solution of (27.1) if and only if
Aα(u, v) = Fα(v),
∀v ∈H1
α,0(−1, 1),
(27.4)
where
Aα(u, v) = aα(u, v) + c(u, v)L2α(−1,1),
aα (u, v) =
 u′, α−1 (αv)′
L2α(−1,1) =
Z 1
−1
u′(x) (α(x)v(x))′ dx,
Fα(v) = (f , v)L2α(−1,1) .
The analysis of this formulation follows the general theory presented in
Section 23.2.3.
Theorem 27.9 (coercivity). The form aα( · , · ), introduced in Deﬁnition 27.8, is
a nonsymmetric, bounded, and coercive bilinear form. In other words, there are
constants 0 < Cα,1 ≤Cα,2 such that
|aα(u, v)| ≤Cα,2 ∥u∥H1
α,0(−1,1) ∥v∥H1
α,0(−1,1) ,
∀u, v ∈H1
α,0(−1, 1),
and
Cα,1 ∥u∥2
H1
α,0(−1,1) ≤aα(u, u) ,
∀0 ̸= u ∈H1
α,0(−1, 1).
In particular, one can prove that
Cα,1 = 1
4,
Cα,2 = 1 +
r
8
3.
Proof. The fact that this is a nonsymmetric bilinear form is clear from its deﬁnition.
For the boundedness and coercivity, see, again, [13] or [85].

27.2 Weighted Spectral Galerkin Approximations
747
Corollary 27.10 (existence and uniqueness). Let α be the Chebyshev weight
function on (−1, 1), f ∈L2
α(−1, 1), and c ≥0. Then problem (27.1) has a
unique weighted weak solution in the sense of Deﬁnition 27.8. Moreover, there
is a constant C > 0, independent of u, such that
∥u∥H1
α,0(−1,1) ≤C ∥f ∥L2α(−1,1) .
Proof. Theorem 27.9 implies that the bilinear form Aα of Deﬁnition 27.8 is bilinear,
bounded, and coercive. Clearly, the form Fα is linear and bounded. Thus, an
application of the Lax–Milgram Lemma, Theorem 23.10, implies the result.
27.2
Weighted Spectral Galerkin Approximations
As a natural next step, let us deﬁne a spectral Galerkin approximation method for
problem (27.1). We will see later how this will connect to collocation approximation
methods.
Deﬁnition 27.11 (weighted spectral Galerkin approximation). For N ∈N, deﬁne
SN,0(−1, 1) = {p ∈PN | p(−1) = p(1) = 0} .
Let α be the Chebyshev weight function on (−1, 1) and f ∈L2
α(−1, 1). The
function uN ∈SN,0(−1, 1) is called the Chebyshev weighted spectral Galerkin
approximation3 of (27.1) if and only if
Aα (uN, vN) = Fα(vN),
∀vN ∈SN,0(−1, 1),
(27.5)
where the bilinear form Aα and the linear form Fα were introduced in Deﬁnition
27.8.
The analysis of this method follows the general theory detailed in Section 25.1.
Proposition 27.12 (existence and uniqueness). Assume that α is the Chebyshev
weight function on (−1, 1), f ∈L2
α(−1, 1), and c ≥0. For every N ∈N, the
spectral Galerkin approximation (27.5) is well posed. Furthermore, the following
stability condition holds:
∥uN∥H1α(−1,1) ≤C ∥f ∥L2α(−1,1)
for some C > 0 that is independent of N.
Proof. It suﬃces to, again, apply the Lax–Milgram Lemma, Theorem 23.10.
The error may be estimated using a C´ea-type4 result.
3 Named in honor of the Russian mathematician Boris Grigorievich Galerkin (1871–1945).
4 Named in honor of the French mathematician Jean C´ea (1932–).

748
Collocation Methods for Elliptic Equations
Theorem 27.13 (weighted C´ea). Suppose that α is the Chebyshev weight function
on (−1, 1), f ∈L2
α(−1, 1), and c ≥0. Let u ∈H1
α,0(−1, 1) be the unique solution
to (27.4) and uN ∈SN,0(−1, 1) be the unique solution to (27.5). Then
∥u −uN∥H1
α,0(−1,1) ≤Cα,2 + γC2
α,P
Cα,1
inf
v∈SN,0(−1,1) ∥u −v∥H1
α,0(−1,1) .
Proof. See Problem 27.4
Now the weighted C´ea’s Lemma guarantees that the spectral Galerkin approxi-
mation, uN, is a quasi-best approximation of the weak solution u. We can estimate
the norm of the error u −uN, by introducing an optimal order approximation of u
from the approximation subspace, SN,0. We will ﬁrst follow the style of [13], using
the weighted elliptic projection operator. Later, we will introduce another method
for estimating the error, using a Chebyshev interpolant.
Deﬁnition 27.14 (elliptic projection). Let α be the Chebyshev weight function
on (−1, 1). For N ∈N, we deﬁne the Chebyshev weighted elliptic projection
operator
P N,1
α,0 : H1
α,0(−1, 1) →SN,0(−1, 1),
u 7→Pα,0[u],
where P N,1
α,0 [u] is the unique solution of
 P N,1
α,0 [u], v

H1
α,0(−1,1) = (u, v)H1
α,0(−1,1) ,
∀v ∈SN,0(−1, 1).
(27.6)
The weighted elliptic projection is nothing but a projection onto a subspace.
Thus, it is well deﬁned and stable. The key point is that P N,1
α,0 [u] approximates u
very well. The following optimal-order error estimate can be derived.
Theorem 27.15 (approximation). Let m, N ∈N with N + 1 > m and α be the
Chebyshev weight function on (−1, 1). Assume that u ∈H1
α,0(−1, 1)∩Hm
α (−1, 1).
Then
u −P N,1
α,0 [u]

H1α(−1,1) ≤
C
Nm−1 |u|Hm
α (−1,1)
for some constant C > 0 that is independent of N and u.
Proof. See [13].
With the help of this projection, we can then obtain an optimal-order error
estimate for (27.5).
Theorem 27.16 (error estimate). Let m, N ∈N with N + 1 > m and α be the
Chebyshev weight function on (−1, 1). Assume that u ∈H1
α,0(−1, 1) ∩Hm
α (−1, 1)
is the unique solution to (27.4) and uN ∈PN,0(−1, 1) is the unique solution to
(27.5). Then
∥u −uN∥H1α(−1,1) ≤
C
Nm−1 |u|Hm
α (−1,1),
where the constant C > 0 is independent of N and u.

27.3 The Chebyshev Projection and the Finite Chebyshev Transform
749
Proof. Combine Theorems 27.13 and 27.15. The details are left to the reader as
an exercise; see Problem 27.5.
We have just created and analyzed a numerical method for solving the BVP
(27.1). However, there are some practical problems with it; for instance, it requires
exact integrations and the stiﬀness matrix would be dense and nonsymmetric.
Moreover, there is no obvious connection between the Galerkin method and the
collocation method that is our principal subject of study in this chapter. It turns
out that this Galerkin approximation method will be key to deﬁning and analyzing
our collocation method.
We must, for the time being, be patient and learn a bit more about Chebyshev’s
domain of ideas.
27.3
The Chebyshev Projection and the Finite Chebyshev Transform
Let us recall another projection operator; see Proposition 11.8.
Deﬁnition 27.17 (Chebyshev projection). Suppose that α is the Chebyshev weight
function on (−1, 1) and {Tj}∞
j=0 is the Chebyshev orthogonal polynomial system,
which, according to Proposition 10.15, satisﬁes the orthogonality relation
(Tk, Tℓ)L2α(−1,1) =
Z 1
−1
Tk(x)Tℓ(x)
dx
√
1 −x2 =
π
2β⋆
k
δk,ℓ,
where
β⋆
j =



1
2,
j = 0,
1,
j ∈N.
(27.7)
Suppose that {˜Tj}∞
j=0 is the normalized Chebyshev polynomial system, deﬁned via
˜Tj =
r
2β⋆
j
π Tj,
j ∈N0.
For N ∈N0, the projection operator Pα,N : L2
α(−1, 1; C) →PN(C), deﬁned via
Pα,N[f ] =
N
X
j=0
(f , ˜Tj)L2α(−1,1;C) ˜Tj,
∀f ∈L2
α(−1, 1; C),
is called the Chebyshev projection.
From the fact that this is a projection, its stability immediately follows.
Proposition 27.18 (stability). Assume that α is the Chebyshev weight function
on (−1, 1). Then
∥Pα,N[u]∥L2α(−1,1) ≤∥u∥L2α(−1,1)
(27.8)
for every u ∈L2
α(−1, 1).
Proof. See Problem 27.6.

750
Collocation Methods for Elliptic Equations
Remark 27.19 (boundary values). Note that, even if u ∈H1
α,0(−1, 1), it is not
generally true that Pα,N[f ] ∈SN,0(−1, 1). This is because there is no mechanism
to enforce the boundary conditions Pα,N[f ](±1) = 0. Consequently, this projection
is not useful if homogeneous Dirichlet boundary conditions must be maintained. On
the other hand, we still have very good approximation properties for the Chebyshev
projection.
Theorem 27.20 (error estimate). Assume that α is the Chebyshev weight function
on (−1, 1), ℓ∈{0, 1}, and m, N ∈N with N + 1 > m > ℓ. If u ∈Hm
α (−1, 1), then
∥u −Pα,N[u]∥Hℓα(−1,1) ≤CNℓ−m|u|Hm
α (−1,1)
for some constant C > 0 that is independent of N and u.
Proof. See [13].
As a consequence of Theorem 11.15, if f ∈L2
α(−1, 1; C), then one can show that
f =
∞
X
j=0
2β⋆
j
π (f , Tj)L2α(−1,1;C)

Tj
and
∥f ∥2
L2α(−1,1;C) =
∞
X
k=0
2β⋆
k
π (f , Tk)L2α(−1,1;C)
2
π
2β⋆
k
=
∞
X
k=0
2β⋆
k
π
(f , Tk)L2α(−1,1;C)

2
= 1
π
(f , T0)L2α(−1,1;C)

2
+ 2
π
∞
X
k=1
(f , Tk)L2α(−1,1;C)

2
=
∞
X
k=0

 f , ˜Tk

L2α(−1,1;C)

2
.
Like for the trigonometric case, we can deﬁne a transform via the Chebyshev
expansion.
Deﬁnition 27.21 (Chebyshev Transform5). Suppose that v ∈L2
α(−1, 1). Then its
Finite Chebyshev Transform is deﬁned as
Fα[v]j = ˆvj =
2β⋆
j
π
Z 1
−1
v(x)Tj(x)α(x)dx.
The numbers ˆvj are called the Chebyshev coeﬃcients.
Recall that
ℓ2(N0) =


a: N0 →R

∞
X
j=0
|aj|2 < ∞


.
5 Named in honor of the Russian mathematician Pafnuty Lvovich Chebyshev (1821–1894).

27.4 Chebyshev–Gauss–Lobatto Quadrature and Interpolation
751
From Theorem 11.15, we have, for all v ∈L2
α(−1, 1),
Fα[v] ∈ℓ2(N0).
Thus, Fα : L2
α(−1, 1; R) →ℓ2(N0; R). We can then deﬁne the inverse Finite
Chebyshev Transform.
Deﬁnition 27.22 (inverse Chebyshev Transform). The inverse Finite Chebyshev
Transform, F−1
α : ℓ2(N0) →L2
α(−1, 1), is deﬁned via
F−1
α [c](x) =
∞
X
j=0
cjTj(x),
∀c ∈ℓ2(N0).
As a consequence of Theorem 11.15, the Finite Chebyshev Transform is an iso-
metric isomorphism from L2
α(−1, 1) onto ℓ2(N0), and F−1
α
is the actual inverse of Fα.
27.4
Chebyshev–Gauss–Lobatto Quadrature and Interpolation
One of the drawbacks of the weighted Galerkin approximation that we introduced
earlier is that it requires exact integrations. To circumvent these, let us introduce
certain numerical quadratures that will make the numerical methods much more
practical.
For the next deﬁnition, it is a good idea to have a look back at Theorem 10.17.
Deﬁnition 27.23 (CGL nodes). Consider the Chebyshev polynomials, TN ∈PN,
N ∈N0, which are deﬁned in (10.12). For N ∈N, the zeros of TN,
zN−1,k = cos
(2k + 1)π
2N

,
k = 0, . . . , N −1,
are called the Chebyshev nodes of degree N −1. The extreme values of TN,
ζN,k = cos
kπ
N

,
k = 0, . . . , N,
which satisfy
TN(ζN,k) = (−1)N+k,
k = 0, . . . , N,
are called the Chebyshev–Gauss–Lobatto (CGL) nodes6 of degree N.
Remark 27.24 (ordering). Observe that the Chebyshev and CGL nodes are
numbered in decreasing order. There is some utility in this numbering, as we shall
see later when we introduce the Cosine Transform.
Remark 27.25 (equivalent characterization). There are other ways to characterize
the CGL nodes. Observe that, for k = 1, . . . , N −1,
T ′
N(ζN,k) = 0.
6 Named in honor of the Russian mathematician Pafnuty Lvovich Chebyshev (1821–1894), the
German mathematician and physicist Johann Carl Friedrich Gauss (1777–1855), and the
Dutch mathematician Rehuel Lobatto (1797–1866).

752
Collocation Methods for Elliptic Equations
In other words, the interior CGL nodes, {ζN,k}N−1
k=1 , are the zeros of T ′
N. Therefore,
the complete set of CGL nodes, {ζN,k}N
k=0, is the zero set for the polynomial
CN+1(x) = T ′
N(x)(1 −x2).
(27.9)
Since CN+1 ∈PN+1, it is clear that there are no other zeros besides these.
On the basis of these nodes, we can construct quadrature rules.
Proposition 27.26 (Chebyshev quadrature). Assume that α is the Chebyshev
weight function on (−1, 1). Suppose that {ζN,k}N
k=0 is the set of Chebyshev nodes
of degree N. Consider the simple quadrature rule (14.4)
Q(−1,1)
α
[f ] =
N
X
j=0
βjf (ζN,j),
where the eﬀective weights are chosen to satisfy the exactness condition (14.7).
Then the quadrature weights are precisely
βj = βC
j =
π
N + 1,
j = 0, . . . , N.
(27.10)
Furthermore, the quadrature rule is consistent to exactly order 2N + 1, i.e.,
EQ[q] =
Z 1
−1
q(x)α(x)dx −Q(−1,1)
α
[q] = 0,
∀q ∈P2N+1,
but there is q ∈P2N+2 such that
EQ[q] ̸= 0.
Proof. The simple quadrature rule is precisely a Gaussian quadrature rule of
Chebyshev type. Therefore, Theorem 14.46 applies, and the quadrature rule is
consistent to exactly order 2N + 1.
We leave to the reader as an exercise the computation of the weights; see
Problem 27.7.
Theorem 27.27 (CGL quadrature). Assume that α is the Chebyshev weight
function on (−1, 1). Suppose that {ζN,k}N
k=0 is the set of CGL nodes of degree N.
Consider the simple quadrature rule (14.4)
Q(−1,1)
α
[f ] =
N
X
j=0
βjf (ζN,j),
where the eﬀective weights are chosen to satisfy the exactness condition (14.7).
Then the quadrature weights are precisely
βj = βCGL
j
= β⋆
N,j
π
N ,
(27.11)
where
β⋆
N,j =



1
2,
j = 0, N,
1,
j = 1, . . . , N −1.
(27.12)

27.4 Chebyshev–Gauss–Lobatto Quadrature and Interpolation
753
Furthermore, the quadrature rule is consistent to exactly order 2N −1, i.e.,
EQ[q] =
Z 1
−1
q(x)α(x)dx −Q(−1,1)
α
[q] = 0,
∀q ∈P2N−1,
but there is q ∈P2N, for which
EQ[q] ̸= 0.
Proof. The computation of the weights is left to the reader as an exercise; see
Problem 27.8.
Notice that, contrary to Proposition 27.26, the quadrature nodes are not
the zeros of an orthogonal polynomial. However, the CGL nodes are the zeros
of the polynomial CN+1, deﬁned in (27.9). Now one can utilize Corollary 14.47 to
prove the result. In particular, we need to show that
(CN+1, q)L2α(−1,1) = 0,
∀q ∈PN−2,
but there is ˜q ∈PN−1 such that
(CN+1, ˜q)L2α(−1,1) ̸= 0.
To establish the ﬁrst result, observe that, for j ∈{0, . . . , N −2},
(CN+1, Tj)L2α(−1,1) =
Z 1
−1
T ′
N(x)Tj(x)
p
1 −x2 dx
= N
Z π
0
sin(Nθ) cos(jθ) sin(θ) dθ
= 0,
where we have used the change of variable x = cos(θ). On the other hand,
(CN+1, TN−1)L2α(−1,1) = π
4 .
The result is proved, appealing to Corollary 14.47.
Next, let us deﬁne a discrete inner product.
Deﬁnition 27.28 (CGL inner product). Suppose that α is the Chebyshev weight
function on (−1, 1). Assume that N ∈N and {ζN,k}N
k=0 is the set of CGL nodes
of degree N. Consider the simple quadrature rule (14.4)
Q(−1,1)
α
[f ] =
N
X
j=0
βCGL
j
f (ζN,j),
where the eﬀective weights, βCGL
j
, are deﬁned in (27.11). For p, q ∈PN, deﬁne
⟨p, q⟩α,N = Q(−1,1)
α
[pq] =
N
X
j=0
p(ζN,j)q(ζN,j)βCGL
j
.
Deﬁne, for all p ∈PN,
∥p∥α,N =
p
⟨p, p⟩α,N.

754
Collocation Methods for Elliptic Equations
The symmetric bilinear form ⟨· , · ⟩α,N is called the CGL lumped-mass inner
product. The object ∥· ∥α,N is called the CGL lumped-mass norm.
With respect to the CGL lumped-mass inner product, we have the following
orthogonality condition.
Proposition 27.29 (orthogonality). For all 0 ≤k, ℓ≤N,
⟨Tk, Tℓ⟩α,N =
π
2β⋆
N,k
δk,ℓ,
where {Tk}∞
k=0 is the orthogonal system of Chebyshev polynomials deﬁned in
(10.12), α is the Chebyshev weight function on [−1, 1], and β⋆
N,k is deﬁned in
(27.12).
Proof. Recall, from Proposition 10.15, that Chebyshev polynomials satisfy the
orthogonality relation
(Tk, Tℓ)L2α(−1,1) =
π
2β⋆
k
δk,ℓ,
k, ℓ∈N0.
If 0 ≤k < ℓ≤N, then TkTℓ∈P2N−1. Consequently,
0 = (Tk, Tℓ)L2α(−1,1) = ⟨Tk, Tℓ⟩α,N.
Suppose that k = ℓ= N. Then TkTℓ= T 2
N ∈P2N\P2N−1 and
(TN, TN)L2α(−1,1) ̸= ⟨TN, TN⟩α,N.
But, since T 2
N(ζN,j) = 1,
⟨TN, TN⟩α,N = π
2N T 2
N(−1) + π
2N T 2
N(1) + π
N
N−1
X
j=1
T 2
N

−cos
kπ
N

= 2 π
2N + (N −1) π
N
= π.
The other cases are easily established; see Problem 27.10.
Since PN is ﬁnite dimensional, all norms are equivalent on it. In particular,
the L2
α(−1, 1)-norm and the CGL lumped-mass norm. The equivalence constants,
however, may depend on N. The following result shows that this is not the case.
Theorem 27.30 (norm equivalence). Suppose that α is the Chebyshev weight
function on (−1, 1). Then, for every N ∈N and all p ∈PN,
∥p∥L2α(−1,1) ≤∥p∥α,N ≤
√
2 ∥p∥L2α(−1,1) .
Proof. See Problem 27.11.
Next, let us deﬁne a particular type of interpolation.

27.4 Chebyshev–Gauss–Lobatto Quadrature and Interpolation
755
Deﬁnition 27.31 (CGL interpolation operator). Suppose that N
∈N and
{ζN,k}N
k=0 is the set of CGL nodes of degree N. The Chebyshev–Gauss–Lobatto
(CGL) interpolation operator is
ICGL
N
: C([−1, 1]) →PN,
v 7→ICGL
N
[v](x) =
N
X
j=0
v(ζN,j)Lj(x),
where Lj is the Lagrange nodal basis element, deﬁned in (9.3), subject to the CGL
nodes.
There is an interesting alternate formulation for CGL interpolation, which we
now give.
Proposition 27.32 (CGL nodes). Suppose that N ∈N and {ζN,k}N
k=0 is the set of
CGL nodes of degree N. Then
Lj(x) = β⋆
N,j
(−1)j+1CN+1(x)
N2(x −ζN,j)
,
where Lj is the Lagrange nodal basis element, deﬁned in (9.3), subject to the CGL
nodes; CN+1 is deﬁned in (27.9); and β⋆
N,j is deﬁned in (27.12).
Proof. Recall the alternate formulation for the Lagrange nodal basis element given
in (9.9),
Lj(x) =
ωN+1(x)
(x −ζN,j)ω′
N+1(ζN,j),
where ωN+1 ∈PN+1 is the nodal polynomial relative to the CGL nodes
ωN+1(x) =
N
Y
k=0
(x −ζN,k).
It should be clear that, for some constant cN+1 ̸= 0,
ωN+1(x) = cN+1CN+1(x),
since the degrees and zeros of the two polynomials coincide. Thus,
Lj(x) =
CN+1(x)
(x −ζN,j)C′
N+1(ζN,j).
The result follows if we can show that
C′
N+1(ζN,j) =
N2
β⋆
N,j(−1)j+1 .
We leave those details to the reader as an exercise; see Problem 27.12.
To conclude this section, we state some stability and convergence results for
the interpolation operator. First, we should note that, for all m ∈N, Hm
α (−1, 1) ⊂
C([−1, 1]). Therefore, interpolation on Hm
α (−1, 1) is well deﬁned. See [13] for a
proof of the following two results.

756
Collocation Methods for Elliptic Equations
Proposition 27.33 (stability). Assume that α is the Chebyshev weight function
on (−1, 1) and N ∈N. Then there is a constant C > 0 such that
ICGL
N
[u]

H1α(−1,1) ≤C ∥u∥H1α(−1,1)
for every u ∈H1
α(−1, 1).
Theorem 27.34 (error estimate). Suppose that α is the Chebyshev weight function
on (−1, 1), ℓ∈{0, 1}, and m, N ∈N with N + 1 > m > ℓ. If u ∈Hm
α (−1, 1), then
u −ICGL
N
[u]

Hℓα(−1,1) ≤CNℓ−m|u|Hm
α (−1,1)
for some constant C > 0 that is independent of N and u.
27.5
The Discrete Cosine Transform
It turns out that, to compute the CGL interpolation operator, the most useful tool
is the Discrete Cosine Transform (DCT). Let us deﬁne that now and then show
how it can be used in the calculation of the CGL interpolant. To do so, we recall
the spaces of grid functions that were introduced in Deﬁnition 24.2.
Deﬁnition 27.35 (DCT). Suppose that d = 1, N ∈N, and h = 1
N . For u ∈V(¯Ωh),
deﬁne
ˆuk = 2β⋆
N,k
N
N
X
ℓ=0
β⋆
N,ℓuℓcos
kπℓ
N

,
k = 0, . . . , N.
The grid function ˆu ∈V(¯Ωh) is called the Discrete Cosine Transform (DCT) of u,
and we write ˆu = CN[u]. Given the grid function v ∈V(¯Ωh), the inverse Discrete
Cosine Transform (IDCT) is deﬁned as
C−1
N [v]k =
N
X
ℓ=0
vℓcos
kπℓ
N

,
k = 0, . . . , N.
To explore the properties of the DCT, we will need the following result.
Proposition 27.36 (discrete orthogonality). Suppose that N ∈N. Then, for all
j, k ∈{0, . . . , N},
N
X
i=0
β⋆
N,i cos
iπj
N

cos
iπk
N

=
N
2β⋆
N,j
δj,k.
Proof. Use the result of Proposition 27.29. The details are left to the reader as an
exercise; see Problem 27.13.
Theorem 27.37 (bijection). Suppose that N ∈N. Then CN[ · ]: V(¯Ωh) →V(¯Ωh)
is a linear bijection and C−1
N [ · ]: V(¯Ωh) →V(¯Ωh) is its inverse. Thus, for all u ∈
V(¯Ωh),
u = C−1
N [CN[u]] = CN[C−1
N [u]].

27.5 The Discrete Cosine Transform
757
Proof. The map CN[ · ] is clearly linear. Let us show that it is also one to one
and onto. Suppose that u, w ∈V(¯Ωh) have the same DCT, i.e., for every k ∈
{0, . . . , N},
2β⋆
N,k
N
N
X
ℓ=0
β⋆
N,ℓuℓcos
kπℓ
N

= 2β⋆
N,k
N
N
X
ℓ=0
β⋆
N,ℓwℓcos
kπℓ
N

.
Thus,
0 =
N
X
ℓ=0
β⋆
N,ℓ(uℓ−wℓ) cos
kπℓ
N

.
Using Proposition 27.36, if 1 ≤j ≤N −1,
0 = 2
N
N
X
k=0
β⋆
N,k
N
X
ℓ=0
β⋆
N,ℓ(uℓ−wℓ) cos
kπℓ
N

cos
kπj
N

= 2
N
N
X
ℓ=0
β⋆
N,ℓ(uℓ−wℓ)
N
X
k=0
β⋆
N,k cos
kπℓ
N

cos
kπj
N

= 2
N
N
X
ℓ=0
β⋆
N,ℓ(uℓ−wℓ) N
2 δℓ,j
= uj −wj.
A similar result will hold if j = 0 or j = N. Thus, uj = wj, for all j ∈{0, . . . , N},
and CN[ · ] is one to one.
To see that CN[ · ] is surjective on V(¯Ωh), let w ∈V(¯Ωh) be arbitrary. We will
prove that there is an element u ∈V(¯Ωh) such that w = CN[u]. In particular, deﬁne
uk =
N
X
ℓ=0
wℓcos
kπℓ
N

,
k = 0, . . . , N.
Then, if 1 ≤j ≤N −1, using Proposition 27.36 again, we ﬁnd
CN[u]j =
2β⋆
N,j
N
N
X
k=0
β⋆
N,kuk cos
jπk
N

=
2β⋆
N,j
N
N
X
k=0
β⋆
N,k
N
X
ℓ=0
wℓcos
kπℓ
N

cos
jπk
N

=
2β⋆
N,j
N
N
X
ℓ=0
wℓ
N
X
k=0
β⋆
N,k cos
kπℓ
N

cos
jπk
N

=
2β⋆
N,j
N
N
X
ℓ=0
wℓ
N
2 δℓ,j
= wj.

758
Collocation Methods for Elliptic Equations
Next, suppose that j = 0. Then
CN[u]0 = 1
N
N
X
ℓ=0
wℓ
N
X
k=0
β⋆
N,k cos
kπℓ
N

cos
0πk
N

= 1
N
N
X
ℓ=0
wℓNδℓ,0
= w0.
The case j = N is the same. Thus, for 0 ≤j ≤N,
CN[u]j = wj
and CN[ · ] is onto V(¯Ωh). Our construction also shows that C−1
N [ · ] is the inverse
of CN[ · ]. The proof is complete.
Next, we will show the link between the DCT and CGL interpolation.
Proposition 27.38 (DCT and CGL interpolation). Suppose that N ∈N, {Tk}∞
k=0
is the Chebyshev orthogonal system, {ζN,j}N
j=0 is the set of CGL nodes, and v ∈
C([−1, 1]). Writing the CGL interpolation operator of v in the Chebyshev basis,
i.e.,
ICGL
N
[v] =
N
X
j=0
cjTj,
we ﬁnd, for all 0 ≤j ≤N,
cj = CN

PCGL
N
[v]

j ,
where PCGL
N
[ · ]: C([−1, 1]) →V(¯Ωh) is the CGL grid projection operator, deﬁned
by
PCGL
N
[v]j = v(ζN,j),
j = 0, . . . , N.
Proof. To simplify notation, let us write
vj = PCGL
N
[v]j = v(ζN,j),
j = 0, . . . , N.
Then it follows that, for 0 ≤k ≤N,
vk =
N
X
j=0
cjTj(ζN,k)
=
N
X
j=0
cj cos

j · arccos

cos
kπ
N

=
N
X
j=0
cj cos
kπj
N

= C−1
N [c]k .
Thus, c = CN

PCGL
N
[v]

.

27.5 The Discrete Cosine Transform
759
Let us now introduce a Discrete Chebyshev Transform. Compare the following
with Deﬁnition 27.21.
Deﬁnition 27.39 (Chebyshev–CGL Transform). Suppose α is the Chebyshev
weight function on (−1, 1), N ∈N, and {ζN,j}N
j=0 is the set of CGL nodes. For all
u ∈V(¯Ωh), deﬁne
ˆuk = 2β⋆
N,k
N

ICGL
N
[u], Tk

α,N ,
k = 0, . . . , N,
where
ICGL
N
[u](x) =
N
X
j=0
ujLj(x)
and Lj is the jth Lagrange nodal basis element, deﬁned in (9.3). The grid function
ˆu ∈V(¯Ωh) is called the Discrete Chebyshev–CGL Transform7 of u, and we write
ˆu = Fα,N[u]. Given the grid function v ∈V(¯Ωh), the inverse Discrete Chebyshev–
CGL Transform is deﬁned as
F−1
α,N[v]k =
N
X
ℓ=0
vℓTℓ(ζN,k),
k = 0, . . . , N.
Proposition 27.40 (equivalence). The DCT and Discrete Chebyshev–CGL Trans-
forms are identical on the space V(¯Ωh).
Proof. See Problem 27.14.
We have deﬁned this Discrete Chebyshev–CGL Transform for grid functions, but
we can deﬁne it for continuous functions as well.
Deﬁnition 27.41 (Chebyshev–CGL transform). Suppose that α is the Chebyshev
weight function on (−1, 1), N ∈N, and {ζN,j}N
j=0 is the set of CGL nodes. For all
f ∈C([−1, 1]), deﬁne
ˆfN,k = 2β⋆
N,k
N

ICGL
N
[f ], Tk

α,N = CN

PCGL
N
[f ]

k ,
k = 0, . . . , N.
The grid function ˆfN ∈V(¯Ωh) is called the Discrete Chebyshev–CGL Transform
of f , and we write ˆfN = Fα,N[f ]. The numbers ˆfN,k are called the discrete
Chebyshev coeﬃcients of f .
Remark 27.42 (notation). Strictly speaking, we should denote these new objects
using the superscript CGL, i.e.,
ˆf CGL
N
= FCGL
α,N [f ],
since other discrete transforms can be chosen by using a nodal set other than
the CGL set. We caution the reader that there are several Discrete Chebyshev
Transforms and there is no universal naming convention.
7 Named in honor of the Russian mathematician Pafnuty Lvovich Chebyshev (1821–1894), the
German mathematician and physicist Johann Carl Friedrich Gauss (1777–1855), and the
Dutch mathematician Rehuel Lobatto (1797–1866).

760
Collocation Methods for Elliptic Equations
Suppose that f ∈C([−1, 1]). Assume that k, N ∈N0, with 0 ≤k < N. Then
ˆfk = 2β⋆
k
π
Z 1
−1
f (x)Tk(k)α(x)dx
= 2β⋆
N,k
π
Z 1
−1
f (x)Tk(k)α(x)dx
≈2β⋆
N,k
π

ICGL
N
[f ], Tk

α,N
= 2β⋆
N,k
π
N
X
ℓ=0
f (ζN,ℓ)Tk(ζN,ℓ)β⋆
N,ℓ
π
N
= 2β⋆
N,k
N
N
X
ℓ=0
PCGL
N
[f ]ℓTk(ζN,ℓ)β⋆
N,ℓ
= 2β⋆
N,k
N
N
X
ℓ=0
PCGL
N
[f ]ℓcos
kπℓ
N

β⋆
N,ℓ
= CN

PCGL
N
[f ]

k
= ˆfN,k.
We now want to estimate the size of the diﬀerence,
ˆfk −ˆfN,k,
which is called the aliasing error, assuming some regularity of f . We ﬁrst need the
following auxiliary result.
Lemma 27.43 (aliasing at CGL nodes). Suppose that N ∈N and {ζN,j}N
j=0 is the
set of CGL nodes. Then
T0(ζN,j) = T2ℓN(ζN,j),
∀ℓ∈N,
∀j ∈{0, . . . , N},
(27.13)
TN(ζN,j) = T2ℓN+1(ζN,j),
∀ℓ∈N,
∀j ∈{0, . . . , N}.
(27.14)
For all m ∈{1, . . . , N −1},
Tm(ζN,j) = T2ℓN±m(ζN,j),
∀ℓ∈N,
∀j ∈{0, . . . , N}.
(27.15)
Proof. See Problem 27.15.
For an illustration of Lemma 27.43, see Figure 27.1.
Theorem 27.44 (aliasing error). Let N ∈N. Suppose that f ∈C2([−1, 1]). Then
ˆfN,0 = ˆf0 +
∞
X
k=1
ˆf2kN,
(27.16)
ˆfN,N = ˆfN +
∞
X
k=1
ˆf2kN+1,
(27.17)

27.5 The Discrete Cosine Transform
761
−1.0
−0.5
0
0.5
1.0
−1.0
−0.5
0
0.5
1.0
x
T5(x), T11(x), T21(x)
Figure 27.1 Aliasing of the Chebyshev polynomials T5, T11, and T21 on the grid {ζ8,j}8
j=0,
illustrating the results in Lemma 27.43.
and, for j ∈{1, . . . , N −1},
ˆfN,j = ˆfj +
∞
X
k=1
 ˆf2kN−j + ˆf2kN+j

.
(27.18)
Proof. By Theorem 11.22, the Chebyshev projection of f converges uniformly and
absolutely to f on [−1, 1], and we may write
f =
∞
X
j=0
ˆfjTj.
(27.19)
Owing to absolute convergence, we can rearrange the terms of the series, without
aﬀecting the result. Furthermore, it follows that
∞
X
j=0
ˆfj
 < ∞
and, therefore, that the series on the right-hand sides of (27.16)–(27.18) converge
absolutely as well. Therefore, we can deﬁne the coeﬃcients {cℓ}N
ℓ=0 via

762
Collocation Methods for Elliptic Equations
c0 = ˆf0 +
∞
X
k=1
ˆf2kN,
cN = ˆfN +
∞
X
k=1
ˆf2kN+1,
and, for 1 ≤j ≤N −1,
cj = ˆfj +
∞
X
k=1
 ˆf2kN−j + ˆf2kN+j

.
Now consider the polynomial
pN =
N
X
m=0
cmTm ∈PN.
Using the aliasing of Chebyshev polynomials, shown in Lemma 27.43, and
appropriately rearranging the terms of the series in 27.19, one can show that
pN(ζN,j) =
N
X
m=0
cmTm(ζN,j) = f (ζN,j),
0 ≤j ≤N.
The details are left to the reader as an exercise; see Problem 27.16. This shows that
pN = ICGL
N
[f ], since interpolating polynomials are unique. By Proposition 27.38,
ICGL
N
[f ] =
N
X
j=0
ˆfN,jTj.
Therefore,
cj = ˆfN,j,
0 ≤j ≤N.
The result is proven.
Remark 27.45 (smoothness). Trefethen [95] shows that one can relax the
assumptions for f above. Remarkably, f is only required to be a Lipschitz continuous
function on [−1, 1].
27.6
The Chebyshev Collocation Method
Now we are ﬁnally ready to deﬁne the Chebyshev collocation method.
Deﬁnition 27.46 (Chebyshev collocation). Let N ∈N be given. Assume that
f ∈L2
α(−1, 1), where α is the Chebyshev weight function on (−1, 1). Set
fN = Pα,N[f ] ∈PN.
Suppose that {ζN,k}N
k=0 is the set of CGL nodes of degree N. The function
w(x) =
N
X
j=0
wjLj(x),
Lj(x) =
N
Y
k=0
k̸=j
x −ζN,k
ζN,j −ζN,k
,

27.6 The Chebyshev Collocation Method
763
is called the Chebyshev collocation approximation of (27.1) of degree N if and
only if
w(−1) = w(1) = 0
(27.20)
and
−w ′′(ζN,k) + cw(ζN,k) = fN(ζN,k),
1 ≤k ≤N −1.
(27.21)
Notice that (27.20) and (27.21) represent a system of N + 1 equations in N + 1
unknowns, the unknowns being the coeﬃcients w0, . . . , wN. Clearly, (27.20) implies
that
w0 = wN = 0.
Now let us use our CGL quadrature rule to deﬁne a new bilinear form to replace
the one used in the spectral Galerkin method.
Deﬁnition 27.47 (discrete bilinear forms). Suppose that N ∈N and α is the
Chebyshev weight function on (−1, 1). For uN, vN ∈SN,0(−1, 1), deﬁne
aα,N(uN, vN) = ⟨u′
N, α−1 (αv)′⟩α,N,
Aα,N(uN, vN) = aα,N(uN, vN) + c⟨uN, vN⟩α,N.
Now let us give, what turns out to be, an alternate formulation of the collocation
approximation.
Deﬁnition 27.48 (lumped-mass spectral Galerkin). Suppose that α is the Cheby-
shev weight function on (−1, 1), f ∈L2
α(−1, 1), and N ∈N. Set
fN = Pα,N[f ] ∈PN.
We say that wN ∈SN,0(−1, 1) is a Chebyshev weighted, lumped-mass, spectral
Galerkin approximation of (27.1) if and only if
Aα,N(wN, vN) = ⟨fN, v⟩α,N,
∀vN ∈SN,0(−1, 1).
(27.22)
Remark 27.49 (variational crime). The formulation of our new variational problem
in (27.22) — in contrast to the original Galerkin approximation (27.5) — involves
what is known as a variational crime. This is because the integrals are replaced by
quadratures. A standard Galerkin approximation always uses the original bilinear
forms restricted to ﬁnite-dimensional subspaces. This change means that our
method of error analysis for (27.5), facilitated by the weighted C´ea’s Lemma of
Theorem 27.13, no longer applies.
Theorem 27.50 (consistency). Suppose that α is the Chebyshev weight function
on (−1, 1) and N ∈N. For all uN, vN ∈SN,0(−1, 1),
aα,N(uN, vN) = aα (uN, vN) .
(27.23)
Furthermore, wN ∈SN,0(−1, 1) solves (27.20) and (27.21) if and only if it solves
(27.22).

764
Collocation Methods for Elliptic Equations
Proof. Let us begin by showing (27.23). Our ﬁrst task will be to show that, for
vN ∈SN,0(−1, 1),
α−1(αvN)′ ∈PN−1.
Expanding the expression, we ﬁnd
α−1(x) (α(x)vN(x))′ = v ′
N(x) +
x
1 −x2 vN(x),
∀x ∈(−1, 1).
Since vN ∈SN,0(−1, 1), we have that v ′
N ∈PN−1, and that there is r ∈PN−2 such
that
vN(x) = r(x)(x −1)(x + 1).
Thus,
x
1−x2 vN(x) = xr(x) ∈PN−1, and the ﬁrst task is complete.
Next, it follows that
α−1(αvN)′ u′
N ∈P2N−2 ⊂P2N−1,
since uN ∈PN. By Theorem 27.27,
aα,N(uN, vN) = aα(uN, vN),
∀uN, vN ∈SN,0(−1, 1),
as claimed.
To show the equivalence, notice, ﬁrst of all, that using integration by parts, the
homogeneous Dirichlet boundary conditions, and Theorem 27.27 again, we have
aα,N (uN, vN) = ⟨u′
N, α−1(αvN)′⟩α,N
=
Z 1
−1
α−1(x) (α(x)vN(x))′ u′
N(x)α(x)dx
=
Z 1
−1
(α(x)vN(x))′ u′
N(x)dx
= −
Z 1
−1
α(x)vN(x)u′′
N(x) dx
= −⟨u′′
N, vN⟩α,N.
( ⇐= ) Suppose that wN ∈SN,0(−1, 1) solves (27.22). Then the computations
above show that
⟨−w ′′
N + cwN −fN, vN⟩α,N = 0
for all vN ∈SN,0(−1, 1). We can pick vN = Lk ∈SN,0(−1, 1), the kth Lagrange
nodal basis function, with 1 ≤k ≤N −1. This implies that
−w ′′
N(ζN,k) + cwN(ζN,k) −fN(ζN,k) = 0,
1 ≤k ≤N −1,
which proves that wN is a Chebyshev collocation approximation.
( =⇒) This direction is similar and is left to the reader as an exercise; see Problem
27.17.

27.6 The Chebyshev Collocation Method
765
The last result makes a remarkable connection between Chebyshev collocation
and Chebyshev weighted, lumped-mass, spectral Galerkin methods. Therefore,
providing an analysis for one immediately implies providing an analysis for the other.
Let us then provide this analysis.
Theorem 27.51 (coercivity). Suppose that α is the Chebyshev weight function on
(−1, 1). Then aα,N ( · , · ) is a nonsymmetric bilinear form and there are constants
0 < Cα,3 ≤Cα,4, which are independent of N, such that
|aα,N (uN, vN)| ≤Cα,4 ∥uN∥H1
α,0(−1,1) ∥vN∥H1
α,0(−1,1) ,
∀uN, vN ∈SN,0(−1, 1),
and
Cα,3 ∥uN∥2
H1
α,0(−1,1) ≤aα,N (uN, uN) ,
∀uN ∈SN,0(−1, 1).
In particular, one can prove that, as for the continuous case,
Cα,3 = Cα,1 = 1
4,
Cα,4 = Cα,2 = 1 +
r
8
3.
Proof. This follows immediately from Theorem 27.9 and the fact that
aα,N (uN, vN) = aα(uN, vN),
∀uN, vN ∈SN,0(−1, 1).
Theorem 27.52 (uniform well-posedness). Suppose that α is the Chebyshev weight
function on (−1, 1) and f ∈L2
α(−1, 1). For all N ∈N, there is a unique function
wN ∈SN,0(−1, 1) that solves (27.22), or, equivalently, (27.20) and (27.21).
Furthermore, wN has the stability property
∥wN∥H1
α,0(−1,1) ≤8Cα,P ∥f ∥L2α(−1,1) .
Proof. We will apply the Lax–Milgram Lemma (Theorem 23.10) to show that
(27.22) always has a unique solution. By Theorem 27.51, the Cauchy–Schwarz
inequality, Theorem 27.30, and the Poincar´e inequality, for all uN, vN ∈SN,0(−1, 1),
we have
|Aα,N(uN, vN)| ≤|aα,N (uN, vN)| + |c⟨u, v⟩α,N|
≤Cα,2 ∥uN∥H1
α,0(−1,1) ∥vN∥H1
α,0(−1,1) + c ∥uN∥α,N ∥vN∥α,N
≤Cα,2 ∥uN∥H1
α,0(−1,1) ∥vN∥H1
α,0(−1,1) + 2c ∥uN∥L2α(−1,1) ∥vN∥L2α(−1,1)
≤Cα,2 ∥uN∥H1
α,0(−1,1) ∥vN∥H1
α,0(−1,1)
+ 2cC2
α,P ∥uN∥H1
α,0(−1,1) ∥vN∥H1
α,0(−1,1)
= ˆCα,4 ∥uN∥H1
α,0(−1,1) ∥vN∥H1
α,0(−1,1) ,
where
ˆCα,4 = Cα,2 + 2cC2
α,P .
This proves that Aα,N( · , · ) is continuous on the subspace SN,0(−1, 1).
The coercivity of Aα,N( · , · ) over the subspace SN,0(−1, 1) is simpler: by
Theorem 27.51, for all uN ∈SN,0(−1, 1),
Aα,N(uN, uN) = aα,N (uN, uN) + c⟨uN, uN⟩α,N ≥ˆCα,3 ∥uN∥2
H1
α,0(−1,1) ,

766
Collocation Methods for Elliptic Equations
where
ˆCα,3 = Cα,3 = Cα,1 = 1
4.
Next, let us deﬁne
Fα,N(v) = ⟨fN, vN⟩α,N,
∀vN ∈SN,0(−1, 1),
where fN = Pα,N[f ]. Then, using Theorem 27.30, the stability of the Chebyshev
projection (27.8), and the weighted Poincar´e inequality (27.3), we get
|Fα,N(vN)| = |⟨fN, vN⟩α,N|
≤∥fN∥α,N ∥vN∥α,N
≤2 ∥fN∥L2α(−1,1) ∥vN∥L2α(−1,1)
≤2 ∥f ∥L2α(−1,1) ∥vN∥L2α(−1,1)
≤2Cα,P ∥f ∥L2α(−1,1) ∥vN∥H1α(−1,1)
for all v ∈SN,0(−1, 1). This shows that Fα,N is a bounded linear functional on
SN,0(−1, 1) and
sup
v∈SN,0(−1,1)
v̸=0
|Fα,N(v)|
∥v∥H1
α,0(−1,1)
≤2Cα,P ∥f ∥L2α(−1,1) .
By the Lax–Milgram Lemma, Theorem 23.10, problem (27.22) has a unique
solution wN ∈SN,0(−1, 1), and the stability estimate follows as well.
The proof is complete.
27.7
Error Analysis of the Chebyshev Collocation Method
Since we have committed variational crimes, we can no longer use weighted C´ea’s
Lemma, Theorem 27.13, for our analysis. Consequently, we need to chart a new
route; ours will be based on Strang’s First Lemma.8
Theorem 27.53 (Strang). Suppose that H is real Hilbert space with the inner
product ( · , · )H and induced norm ∥· ∥H. Assume that A: H×H →R is a bounded
and coercive bilinear form with constants C2 and C1, respectively. Let F ∈H′ and
u ∈H be the (unique) solution to
A(u, v) = F(v),
∀v ∈H.
Assume that, for n ∈N, we have Hn ≤H with dim Hn = n and An : Hn ×Hn →R
is a bounded and coercive bilinear form with constants C4 and C3. Let Fn ∈H′
n.
Denote by un ∈Hn the solution to
An(un, vn) = Fn(vn),
∀vn ∈Hn.
8 Named in honor of the American mathematician William Gilbert Strang (1934–).

27.7 Error Analysis of the Chebyshev Collocation Method
767
Then the following error estimate holds:
∥u −un∥H ≤
inf
wn∈Hn

1 + C2
C3

∥u −wn∥H
+ 1
C3
sup
vn∈Hn
vn̸=0
|A(wn, vn) −An(wn, vn)|
∥vn∥H


+ 1
C3
sup
vn∈Hn
vn̸=0
|F(vn) −Fn(vn)|
∥vn∥H
.
(27.24)
Proof. The fact that u ∈H and un ∈Hn exist and are unique is guaranteed by
the Lax–Milgram Lemma, Theorem 23.10. Let us then obtain the error estimate.
Suppose that wn ∈Hn is arbitrary. Set en = un −wn ∈Hn. Then, by coercivity
of the discrete bilinear form,
C3 ∥en∥2
H ≤An(en, en)
= A(u −wn, en) + A(wn, en) −An(wn, en) + Fn(en) −F(en).
Assuming that en ̸= 0, it follows that
C3 ∥en∥H ≤C2 ∥u −wn∥H + |A(wn, en) −An(wn, en)|
∥en∥H
+ |Fn(en) −F(en)|
∥en∥H
.
Using the triangle inequality,
∥u −un∥H ≤∥u −wn∥H + ∥en∥H ,
and we have
∥u −un∥H ≤

1 + C2
C3

∥u −wn∥H + 1
C3
|A(wn, en) −An(wn, en)|
∥en∥H
+ 1
C3
|F(en) −Fn(en)|
∥en∥H
.
Taking suprema,
∥u −un∥H ≤

1 + C2
C3

∥u −wn∥H + 1
C3
sup
vn∈Hn
vn̸=0
|A(wn, vn) −An(wn, vn)|
∥vn∥H
+ sup
vn∈Hn
vn̸=0
1
C3
|F(vn) −Fn(vn)|
∥vn∥H
.
Finally, since this last result holds for an arbitrary wn ∈Hn, we can take inﬁmum
to arrive at the the error estimate (27.24).
With the aid of Strang’s First Lemma we can provide an error estimate for the
solution of (27.22).
Theorem 27.54 (error estimate). Suppose that α is the Chebyshev weight function
on (−1, 1), m ∈N, f ∈Hm
α (−1, 1), and u ∈H1
α,0(−1, 1)∩Hm
α (−1, 1) is the unique

768
Collocation Methods for Elliptic Equations
solution to (27.4). For all N ∈N with N +1 > m, we have that wN ∈SN,0(−1, 1),
the unique solution to (27.22), satisﬁes
∥u −wN∥H1α(−1,1) ≤C(N −1)1−m|u|Hm
α (−1,1) + C(N −1)−m|f |Hm
α (−1,1)
for some constant C > 0 that is independent of N.
Proof. Following the notations that deﬁne (27.4) and (27.22), we have, for any
vN ∈SN,0(−1, 1),
|FN(vN) −F(vN)| =
⟨fN, vN⟩α,N −(f , vN)L2α(−1,1)

=
⟨fN −fN−1 + fN−1, vN⟩α,N −(f , vN)L2α(−1,1)

=
⟨fN −fN−1, vN⟩α,N −(f −fN−1, vN)L2α(−1,1)

≤∥fN −fN−1∥α,N ∥vN∥α,N + ∥f −fN−1∥L2α(−1,1) ∥vN∥L2α(−1,1)
≤

2 ∥fN −fN−1∥L2α(−1,1) + ∥f −fN−1∥L2α(−1,1)

∥vN∥L2α(−1,1)
≤Cα,P

2 ∥fN −fN−1∥L2α(−1,1) + ∥f −fN−1∥L2α(−1,1)

× ∥vN∥H1
α,0(−1,1) .
Using Theorem 27.20,
∥fN −fN−1∥L2α(−1,1) ≤∥f −fN∥L2α(−1,1) + ∥f −fN−1∥L2α(−1,1)
≤CN−m|f |Hm
α (−1,1) + C(N −1)−m|f |Hm
α (−1,1)
≤C(N −1)−m|f |Hm
α (−1,1)
for some constant C > 0 that is independent of N. Therefore,
|FN(vN) −F(vN)|
∥vN∥H1
α,0(−1,1)
≤C(N −1)−m|f |Hm
α (−1,1)
for all nonzero vN ∈SN,0(−1, 1).
Next, recall that, for ℓ∈N, P ℓ,1
α,0[u] ∈Sℓ,0(−1, 1) denotes the weighted elliptic
projection deﬁned in (27.6). Using the consistency shown in (27.23), we have, for
any v ∈SN,0(−1, 1),
Aα(P N,1
α,0 [u], v) −Aα,N(P N,1
α,0 [u], v)
 = c

 P N,1
α,0 [u], v

L2α(−1,1) −⟨P N,1
α,0 [u], v⟩α,N

= c

 P N,1
α,0 [u] −P N−1,1
α,0
[u], v

L2α(−1,1)
−⟨P N,1
α,0 [u] −P N−1,1
α,0
[u], v⟩α,N

≤3c ∥v∥L2α(−1,1)
×
P N,1
α,0 [u] −P N−1,1
α,0
[u]

L2α(−1,1)
≤3cC2
α,P ∥v∥H1
α,0(−1,1)
×
P N,1
α,0 [u] −P N−1,1
α,0
[u]

H1
α,0(−1,1) .

27.8 Practical Computation of the Collocation Approximation
769
Using the triangle inequality and Theorem 27.15, we have
P N,1
α,0 [u] −P N−1,1
α,0
[u]

H1
α,0(−1,1) ≤
P N,1
α,0 [u] −u

H1
α,0(−1,1)
+
u −P N−1,1
α,0
[u]

H1
α,0(−1,1)
≤CN1−m|u|Hm
α (−1,1) + C(N −1)1−m|u|Hm
α (−1,1)
≤C(N −1)1−m|u|Hm
α (−1,1).
Then, provided that v ̸= 0,
Aα(P N,1
α,0 [u], v) −Aα,N(P N,1
α,0 [u], v)

∥v∥H1
α,0(−1,1)
≤C(N −1)1−m|u|Hm
α (−1,1).
Now, by Theorem 27.53, in particular estimate (27.24) and the estimates we
have just constructed,
∥u −uN∥H1
α,0(−1,1) ≤

1 + Cα,2 + γC2
α,P
Cα,1
 u −P N,1
α,0 [u]

H1
α,0(−1,1)
+
1
Cα,1
sup
vN∈SN,0(−1,1)
vN̸=0
Aα(P N,1
α,0 [u], vN) −Aα,N(P N,1
α,0 [u], vN)

∥vN∥H1
α,0(−1,1)
+
1
Cα,1
sup
vN∈SN,0(−1,1)
vN̸=0
|F(vN) −FN(vN)|
∥vN∥H1
α,0(−1,1)
≤C(N −1)1−m|u|Hm
α (−1,1) + C(N −1)−m|f |Hm
α (−1,1)
for some constant C > 0 that is independent of N. The proof is complete.
27.8
Practical Computation of the Collocation Approximation
We have just learned that our collocation approximation wN ∈SN,0(−1, 1) to the
solution of (27.1) is spectrally accurate, i.e.,
∥u −wN∥H1α(−1,1) ≤C(N −1)1−m|u|Hm
α (−1,1) + C(N −1)−m|f |Hm
α (−1,1).
The next question then is this: How do we compute this approximation? Can this
be done eﬃciently? We will not explore those details here, but see the books by
Shen et al. [85], Canuto et al. [13], and Trefethen [94, 95]. The methods described
in Trefethen’s books, which are used in the next example, are not “fast” methods
per se, though they are simple, and not too expensive in one space dimension,
since, often, N needs not be too large to resolve the solution to within a very
small tolerance. Essentially, one computes the dense matrix A, as in (27.2), and
inverts it by Gaussian elimination. There are fast methods that can be employed,
as described in the references above, but we will not discuss these for the sake of
brevity.

770
Collocation Methods for Elliptic Equations
Example 27.1
In this example, we approximate the solution of the following two-
point BVP:
−u′′ + u = −15 exp(4x) + sinh(4)x + cosh(4)
16
,
u(−1) = u(1) = 0.
(27.25)
The exact solution is
u(x) = exp(4x) −sinh(4)x −cosh(4)
16
.
For the sake of comparison, we compute approximations based on both the ﬁnite
diﬀerence method of Chapter 24 and the Chebyshev collocation method developed
in this chapter; see Figures 27.2 (N = 8) and 27.3 (N = 24). The errors for both
methods are reported in Figure 27.4. The method for solving the linear equation
Aw = f that arises in the approximation of the Chebyshev collocation method is
based on the simple, and not so eﬃcient, strategy in Trefethen [94]. In fact, we
have modiﬁed one of the codes from [94] for our example.
In Figure 27.4, we compare the errors for the Chebyshev collocation and ﬁnite
diﬀerence methods in the approximation of the solution of the BVP (27.25) for
the values N = 8, 12, 16, 20, 24, 32, 40, 48, 64, and 80. The Chebyshev method
is spectrally accurate, as is conﬁrmed in Figure 27.4. It requires a relatively small
value of N, N = 20, in particular, to obtain a very accurate solution. Roundoﬀ
errors dominate the collocation approximations for larger values of N. The ﬁnite
diﬀerence method, as we proved in Chapter 24, is second-order accurate. This is
also conﬁrmed in the present example. While, for N = 24, the ﬁnite diﬀerence
approximation looks very good to the human eye, its error is, astonishingly, about
1012 times larger than that of the collocation method with the same value of N.
Problems
27.1
Let α be the Chebyshev weight function on [−1, 1]. Show that:
a)
This is a doubling weight: There is a constant C > 0 such that, for every
a ∈(0, 1
2], we have
Z 2a
−2a
α(x)dx ≤C
Z a
−a
α(x)dx.
b)
There is a constant Cα > 0 such that, for all a ∈(0, 1], we have
1
4a2
Z a
−a
α(x)dx
Z a
−a
α(x)−1 dx < Cα.
Hint: Consider the change of variable x = cos t.
c)
This is a strong doubling weight: There is a constant C > 0 such that, for all
a ∈(0, 1], we have
Z 1
−1
α(x)dx ≤C
a2
Z a
−a
α(x)dx.

Problems
771
−1
−0.5
0
0.5
1.0
−2.5
−2.0
−1.5
−1.0
−0.5
0
x
max collocation error = 2.114e−04
max ﬁnite diﬀerence error = 1.299e−01
Chebyshev collocation
ﬁnite diﬀerence
Figure 27.2 Comparison of Chebyshev collocation and ﬁnite diﬀerence approximations of
the BVP (27.25) with N = 8.
Hint: Start from 2a =
R a
−a 1 dx, apply the Cauchy–Schwarz inequality, and use
the previous result.
27.2
Show that, for all N ∈N,
SN,0(−1, 1) ⊂H1
α,0(−1, 1).
27.3
Let α be the Chebyshev weight function on [−1, 1], q ≥2, and ρ be another
weight function on [−1, 1]. Assume that there is a constant C > 0 such that, for
all a ∈(0, 1],
Z a
−a
ρ(x)dx
Z a
−a
α(x)dx
−q/2
≤C.
Show that there is a constant Cq,ρ > 0 such that, for all v ∈H1
α,0(−1, 1), we have
Z 1
−1
|v(x)|qρ(x)dx
1/q
≤Cq,ρ ∥v ′∥L2α(−1,1) .
27.4
Prove Theorem 27.13.
Hint: Revisit Theorem 25.6.
27.5
Prove Theorem 27.16.
27.6
Prove Proposition 27.18.

772
Collocation Methods for Elliptic Equations
−1.0
−0.5
0
0.5
1.0
−2.5
−2.0
−1.5
−1.0
−0.5
0
x
max collocation error = 3.997e−15
max ﬁnite diﬀerence error = 1.522e−02
Chebyshev collocation
ﬁnite diﬀerence
Figure 27.3 Comparison of Chebyshev collocation and ﬁnite diﬀerence approximations of
the BVP (27.25) with N = 24. While the ﬁnite diﬀerence approximation looks very good
to the human eye, its error is, astonishingly, about 1012 times larger than that of the
collocation method.
27.7
Complete the proof of Proposition 27.26.
27.8
Complete the proof of Theorem 27.27.
27.9
Prove that the CGL lumped-mass inner product, introduced in Deﬁnition
27.28, is indeed an inner product on PN.
27.10
Complete the proof of Proposition 27.29.
27.11
Prove Theorem 27.30.
27.12
Complete the proof of Proposition 27.32.
27.13
Complete the proof of Proposition 27.36.
27.14
Prove Proposition 27.40.
27.15
Prove Lemma 27.43.
27.16
Complete the proof of Theorem 27.44.
27.17
Complete the proof of Theorem 27.50.

Problems
773
101
10−14
10−13
10−12
10−11
10−10
10−09
10−08
10−07
10−06
10−05
10−04
10−03
10−02
10−01
1000
N
max-norm error at the nodes
Chebyshev collocation
ﬁnite diﬀerence
4N−2
Figure 27.4 Comparison of Chebyshev collocation and ﬁnite diﬀerence errors in the
approximations of the solution of the BVP (27.25) for the values N = 8, 12, 16, 20, 24,
32, 40, 48, 64, and 80. The Chebyshev method is spectrally accurate and requires a
relatively small value of N, N = 20, in particular, to obtain a very accurate solution.
Roundoﬀerrors dominate the collocation solutions for larger values of N.

28
Finite Diﬀerence Methods for
Parabolic Problems
In this chapter, we study ﬁnite diﬀerence methods for the approximation of the
solution to a parabolic problem, focusing on the solution of the heat, or diﬀusion,
equation. We recall that, in one dimension, this equation has the form
∂u(x, t)
∂t
−∂2u(x, t)
∂x2
= f (x, t),
(x, t) ∈Ω× (0, T],
supplemented by suitable initial and, if necessary, boundary conditions.
Since the independent variables, time t and space x, play fundamentally diﬀerent
roles in the equation, there are two prevalent ideas used in the discretization of
time-dependent problems, based on which of the variables is discretized ﬁrst.
• The method of lines: Where one applies any of the methods discussed in
Chapters 24–27 to discretize the spatial variable to obtain, for some ﬁnite-
dimensional space S, the problem: Find uS ∈C([0, T]; S) that is a mild solu-
tion to
u′
S(t) = FS(t, uS(t)),
t ∈(0, T],
supplemented by suitable initial conditions. This is now an initial value problem
(IVP) that can be treated with any of the methods in Part IV. The origin of
the name of this approach is that solutions to the IVP in S draw “lines” on the
space–time cylinder Ω× (0, T].
• Rothe’s method1: Here, one ﬁrst applies a ﬁnite diﬀerence discretization in time
to obtain approximations at discrete instances of time which, for instance, read
w(tk+1) −∂2w(x, tk+1)
∂x2
= w(tk) + τf (x, tk+1).
This is now an elliptic equation, and we have already discussed several methods
for its discretization.
In this chapter, we will only discuss the result of applying a ﬁnite diﬀerence
approximation in space and time and the most rudimentary facts about the analysis
of the resulting methods. For more details about this approach, we refer the reader
to [56, 50, 79]. The monograph [91] discusses the application of ﬁnite element
techniques in space to the solution of parabolic problems.
1 Named in honor of the German mathematician Erich Hans Rothe (1895–1988).

28.1 Space–Time Grid Functions
775
28.1
Space–Time Grid Functions
We begin the discussion with the introduction of space–time grids, and functions
on them. We could, in principle, consider a space–time domain merely as a domain
in Rd+1, but it is sometimes useful to make the diﬀerence between space and time,
as each plays a diﬀerent role.
Deﬁnition 28.1 (space–time grid). Let d ∈N, Ω= (0, 1)d, and T > 0. For
K, N ∈N, we set τ = T/K and h = 1/(N + 1). We deﬁne the space–time grid
domain
¯Cτ
h = ¯Ωh × [0, T]τ =

(x, tk)
 x ∈¯Ωh, tk = kτ, k = 0, . . . , K
	
,
where we recall that ¯Ωh = ¯Ω∩Zd
h. We deﬁne the discrete interior of ¯Cτ
h to be
Cτ
h = Ωh × (0, T)τ.
The discrete lateral boundary is
∂LCτ
h = ∂Ωh × [0, T]τ.
Finally, the discrete parabolic boundary is
∂pCτ
h = ¯Ωh × {0} ∪∂LCτ
h .
Notice that this can be thought of as K + 1 copies of ¯Ωh, one for each k ∈
{0, . . . , K}. We can now deﬁne space–time grid functions and norms on them that
have the approximation property in suitable spaces of space–time functions.
Deﬁnition 28.2 (space–time grid functions). Let Cτ
h be a space–time grid domain.
We denote by
V( ¯Cτ
h ) =

v
 ¯Cτ
h →R
	
the space of space–time grid functions. The spaces
V(Cτ
h ),
V(∂LCτ
h )
are deﬁned accordingly. We set
V0( ¯Cτ
h ) =

v ∈V( ¯Cτ
h )
 v(x, t) = 0, ∀(x, t) ∈∂LCτ
h
	
.
If (ih, kτ) ∈¯Cτ
h and v ∈V( ¯Cτ
h ), we denote
v k
i = v(ih, kτ).
Finally, if v is a space–time grid function and k ∈{0, . . . , K}, we denote
v k ∈V(¯Ωh),
v k(x) = v(x, kτ),
∀x ∈¯Ωh.
We comment that spaces of space–time grid functions can also be thought of as
a space of spatial grid function-valued functions, much as was done with functions
in Deﬁnition 23.23. In other words,
V( ¯Cτ
h ) = V
 [0, T]τ; V(¯Ωh)

=
 V(¯Ωh)
K+1 .

776
Finite Diﬀerence Methods for Parabolic Problems
With this identiﬁcation, we see that
V0( ¯Cτ
h ) = V
 [0, T]τ; V0(¯Ωh)

.
The usefulness of this, at ﬁrst glance unnecessary and certainly overburdened,
notation is that we can introduce norms that have the approximation property with
respect to Lq(0, T; Lp(Ω)), for instance.
Deﬁnition 28.3 (space–time discrete norms). Let d ∈{1, 2}, p ∈[1, ∞], and
q ∈[1, ∞). We deﬁne the space–time norm
∥v∥Lq
τ(Lp
h) =
 
τ
K
X
k=1
∥v k∥q
Lp
h
!1/q
and
∥v∥L∞
τ (Lp
h) =
K
max
k=0 ∥v k∥Lp
h.
Proposition 28.4 (approximation property). Let d ∈{1, 2} and p, q ∈[1, ∞),
the space–time discrete norms of Deﬁnition 28.3, have the approximation property
in Lq(0, T; Lp(Ω)). In addition, the space–time discrete L∞
τ (L∞
h )-norm has the
approximation property in C( ¯C).
Proof. See Problem 28.1.
Having introduced suitable norms, we can discuss the consistency and stability of
ﬁnite diﬀerence methods as before. Since ﬁnite diﬀerence methods now depend on
two parameters, K and N (or τ and h), we usually indicate the order of consistency
with respect to each one of them separately.
Deﬁnition 28.5 (Courant number2). Let ¯Cτ
h be the space–time grid. The parabolic
Courant number is
µ = τ
h2 .
Deﬁnition 28.6 (unconditional stability). We say that a ﬁnite diﬀerence method
is unconditionally stable if it is stable, in the sense of Deﬁnition 24.16, for all
values of the Courant number µ. It is conditionally stable if, to attain stability,
some condition must be imposed on µ.
28.2
The Initial Boundary Value Problem for the Heat Equation
We are now ready to illustrate the application of the method of lines to the solution
of parabolic problems. Let Ω= (0, 1) ⊂R, T > 0, u0 ∈C(¯Ω), f ∈C(Ω× (0, T]).
We seek a classical solution to the problem: Find u : ¯Ω× [0, T] →R such that
2 Named in honor of the German–American mathematician Richard Courant (1888–1972).

28.2 The Initial Boundary Value Problem for the Heat Equation
777









∂u(x, t)
∂t
−∂2u(x, t)
∂x2
= f (x, t),
(x, t) ∈Ω× (0, T],
u(0, t) = u(1, t) = 0,
u(x, 0) = u0(x),
x ∈[0, 1].
(28.1)
We can semi-discretize this problem in space. Thus, we let N ∈N, h = 1/(N+1),
and ¯Ωh be our grid domain. We set u0,h ∈V(¯Ωh) as u0,h(x) = u0(x), and fh ∈
C([0, T]; V(Ωh)) to be fh(x, t) = f (x, t). We seek w : [0, T] →V0(¯Ωh) such that
(
w ′(t) −∆hw(t) = fh(·, t),
t ∈(0, T],
w(0) = u0,h.
(28.2)
We can now apply, for instance, any of the single-step methods in Chapter 18
to obtain a fully discrete method.
Deﬁnition 28.7 (fully discrete methods). Let K, N ∈N, τ = T/K, h = 1/(N +1),
and Cτ
h be the discrete space–time cylinder. Let u ∈C( ¯C) be the classical solution
to (28.1). Suppose that u0,h ∈V(¯Ωh) and fh ∈V(Cτ
h ) are deﬁned as usual. We
deﬁne the following methods for computing the ﬁnite diﬀerence approximation
w ∈V0( ¯Cτ
h ) to u.
1. The backward Euler method
(¯δτw k+1 −∆hw k+1 = f k+1
h
,
k = 0, . . . , K −1,
w 0 = u0,h,
(28.3)
where
¯δτv k+1 = 1
τ
 v k+1 −v k
is the backward diﬀerence in time.
2. The forward Euler method
(
δτw k −∆hw k = f k
h ,
k = 0, . . . , K −1,
w 0 = u0,h,
(28.4)
where
δτv k = 1
τ
 v k+1 −v k
is the forward diﬀerence in time.
3. The Crank–Nicolson method3



¯δτw k+1 −1
2∆h
 w k+1 + w k
= 1
2(f k+1
h
+ f k
h ),
k = 0, . . . , K −1,
w 0 = u0,h.
(28.5)
3 Named in honor of the British mathematical physicist John Crank (1916–2006) and the
British mathematician and physicist Phyllis Nicolson (1917–1968).

778
Finite Diﬀerence Methods for Parabolic Problems
Other single-step, or even multi-step, methods can be used to discretize in time.
The consistency of these methods follows by considering, separately, each one of
its components. We note that the forward Euler method is explicit, whereas the
backward Euler and Crank–Nicolson methods are implicit.
Theorem 28.8 (consistency). Let K, N ∈N, τ = T/K, h = 1/(N + 1), and ¯Cτ
h be
the discrete space–time cylinder.
1. The backward Euler method is consistent, in Cb(R2), to order
τ + h2.
2. The forward Euler method is consistent, in Cb(R2), to order
τ + h2.
3. The Crank–Nicolson method is consistent, in Cb(R2), to order
τ2 + h2.
Proof. See Problem 28.2.
28.3
Stability and Convergence in the L∞
τ (L∞
h )-Norm
In this section, we study the stability and convergence in the L∞
τ (L∞
h )-norm of
our three methods. As we will see, the main tool will be a discrete version of the
maximum principle of Theorem 23.27.
28.3.1
The Backward Euler Method
Let us now study the stability and convergence of the backward Euler method. We
must note that, as part of stability, we must show that this method is well deﬁned,
as it is an implicit one. To achieve this, we begin with a discrete maximum principle
(DMP). The following result must be compared with Theorem 23.27.
Theorem 28.9 (DMP). Let v ∈V( ¯Cτ
h ) be such that
¯δτv k+1 −∆hv k+1 ≤0,
k = 0, . . . , K −1.
Then the function v is either constant or attains its maximum on the discrete
parabolic boundary ∂pCτ
h .
Proof. We begin by writing the inequality explicitly, i.e.,
v k+1
i
−v k
i −µ
 v k+1
i−1 −2v k+1
i
+ v k+1
i+1

≤0,
where µ = τ/h2 is the Courant number. This inequality can be rewritten as
v k+1
i
≤
µ
1 + 2µv k+1
i−1 +
1
1 + 2µv k
i +
µ
1 + 2µv k+1
i+1 ,
(28.6)

28.3 Stability and Convergence in the L∞
τ (L∞
h )-Norm
779
which, since
1
1+2µ,
µ
1+2µ ∈(0, 1), shows that v k+1
i
is a convex combination of the
other values.
Let us assume that the maximum is attained at a point (ih, (k + 1)τ) ∈Ωh ×
(0, T]τ. This, in particular, implies that
v k+1
i
≥v k+1
i±1 ,
v k+1
i
≥v k
i .
The previous inequalities, together with (28.6), imply that
v k+1
i
= v k+1
i±1 = v k
i .
If any of the points ((i ±1)h, (k +1)τ) or (ih, kτ) belong to the parabolic boundary,
then we have obtained the result. If not, then we can repeat the same argument a
ﬁnite number of times until we eventually reach the parabolic boundary.
As an immediate consequence, we have a stability result.
Corollary 28.10 (stability). Let w ∈V0( ¯Cτ
h ) be the solution to (28.3). Then
∥w∥L∞
τ (L∞
h ) ≤∥u0,h∥L∞
h + C∥fh∥L∞
τ (L∞
h ).
As a consequence, the backward Euler method is unconditionally stable.
Proof. We must begin by showing that, for all k ∈{0, . . . , K −1}, the problem
(I + τ∆h) w k+1 = τf k+1
h
+ w k = ˜f k+1
h
,
where I is the identity operator, is well posed. We see that this problem can be
rewritten as a linear system of equations in RN via w k+1 ←→w k+1
Bw k+1 = ˜f
k+1,
B = IN + µA,
where the matrix A is the stiﬀness matrix introduced in (24.7). Clearly, B is
invertible for all µ > 0 and so the backward Euler method is well deﬁned.
To obtain stability, as usual, we introduce the comparison function
Φ(x) =

x −1
2
2
≥0,
x ∈[0, 1],
and φ ∈V0(¯Ωh) is φi = Φ(ih). As we observed before,
−Φ′′ = −2 = −∆hφ.
Deﬁne now
ψ± = ±w +
∥fh∥L∞
τ (L∞
h )
2
φ ∈V0( ¯Cτ
h ),
and notice that
¯δτψk+1
±
−∆hψk+1
±
= ±f k+1
h
−∥fh∥L∞
τ (L∞
h ) ≤0.
The DMP of Theorem 28.9 then implies that
±w k+1 ≤ψk+1
±
≤max
∂pCτ
h
ψ± ≤∥u0,h∥L∞
h + C∥fh∥L∞
τ (L∞
h ),
by arguments we have established before. This implies the claimed estimate and
the unconditional stability of the method.

780
Finite Diﬀerence Methods for Parabolic Problems
As usual, stability and consistency imply convergence.
Theorem 28.11 (convergence). Suppose that u is a classical solution to the one-
dimensional heat equation (28.1) with the additional regularity
u, ∂xu, . . . , ∂4
x u, ∂tu, ∂2
t u ∈C(¯Ω× [0, T]).
Let w ∈V0( ¯Cτ
h ) be the grid function obtained from the backward Euler method
(28.3). Deﬁne the error
e = πτ
hu −w ∈V0( ¯Cτ
h ),
ek
i = u(ih, kτ) −w k
i .
Then we have that there is a constant C > 0 for which
∥e∥L∞
τ (L∞
h ) ≤C
 τ + h2
.
In other words, the convergence is ﬁrst order with respect to time and second order
with respect to space.
Proof. To prove convergence, we follow the usual approach, i.e., we ﬁnd an
equation that controls the error. Take the diﬀerence between the deﬁnition of
consistency and the method to obtain
¯δτek+1 −∆hek+1 = Eτ
h [u]k+1,
k = 0, . . . , K −1,
where µ =
τ
h2 is the Courant number; ek
0 = ek
N+1 = 0 for all k ∈{0, . . . , K} and
e0
i = 0 for all i ∈{0, . . . , N + 1}. Moreover, owing to Theorem 28.8,
∥Eτ
h [u]∥L∞
τ (L∞
h ) ≤C(τ + h2)
for some C > 0 that is independent of h and τ. The stability of the method,
presented in Corollary 28.10, implies that
∥e∥L∞
τ (L∞
h ) ≤C ∥Eτ
h [u]∥L∞
τ (L∞
h ) ≤C(τ + h2),
which is what we intended to prove.
28.3.2
The Forward Euler Method
We will now focus on the forward Euler method (28.4). Notice that this is an explicit
method so it is automatically well deﬁned. We will now show its (conditional)
stability and convergence.
Theorem 28.12 (stability and convergence). Let u be a classical solution to (28.1),
which, in addition, satisﬁes
u, ∂xu, . . . , ∂4
x u, ∂tu, ∂2
t u ∈C(¯Ω× [0, T]).
Let w ∈V0( ¯Cτ
h ) be the grid function obtained from the forward Euler method
(28.4). We have:

28.3 Stability and Convergence in the L∞
τ (L∞
h )-Norm
781
1. If the Courant number satisﬁes µ ≤µ0 = 1
2, then the method is L∞
τ (L∞
h )-stable,
i.e., there is a constant C > 0, independent of h, τ, and w, such that
∥w∥L∞
τ (L∞
h ) ≤∥u0,h∥L∞
h + C∥fh∥L∞
τ (L∞
h ),
so that the forward Euler method is conditionally stable.
2. Deﬁne the error
e = πτ
hu −w ∈V0( ¯Cτ
h ),
ek
i = u(ih, kτ) −w k
i .
Then, provided that µ ≤µ0, we have that there is a constant C > 0 for which
∥e∥L∞
τ (L2
h) ≤C
 τ + h2
.
In other words, the convergence is ﬁrst order with respect to time and second
order with respect to space.
Proof. See Problem 28.4.
Remark 28.13 (CFL condition4). The restriction on the time and space step sizes
in the last result, i.e., µ ≤1
2, is called a CFL-type condition in honor of R. Courant,
K.O. Friedrichs, and H. Lewy, who pioneered the use of ﬁnite diﬀerence methods in
their breakthrough article [21]; see also the translation [22]. It indicates a restriction
between the time and space step sizes for stability and/or convergence; namely,
τ ≤h2
2 ,
which may be computationally expensive to enforce.
28.3.3
The Crank–Nicolson Method
Let us now study the Crank–Nicolson method, which is implicit. We will obtain
that this method is also conditionally stable, but the CFL-type condition is slightly
milder.
Theorem 28.14 (stability and convergence). Let u be a classical solution to (28.1),
which, in addition, satisﬁes u ∈C4(¯Ω×[0, T]). Let w ∈V0( ¯Cτ
h ) be the grid function
obtained from the Crank–Nicolson method (28.5). We have:
1. The method is well deﬁned. In particular, we have that
B2w k+1 = A1w k + τf k+1,
4 Named in honor of the German–American mathematicians Richard Courant (1888–1972) and
Kurt Otto Friedrichs (1901–1982) and the Jewish, German-born, American mathematician
Hans Lewy (1904–1988).

782
Finite Diﬀerence Methods for Parabolic Problems
where
B2 =


1 + µ
−µ
2
0
· · ·
0
−µ
2
1 + µ
...
...
0
...
...
−µ
2
0
...
−µ
2
1 + µ
−µ
2
0
· · ·
0
−µ
2
1 + µ


∈RN×N
is symmetric positive deﬁnite (SPD) and
A1 =


1 −µ
µ
2
0
· · ·
0
µ
2
1 −µ
...
...
0
...
...
µ
2
0
...
µ
2
1 −µ
µ
2
0
· · ·
0
µ
2
1 −µ


∈RN×N.
2. If the Courant number satisﬁes µ ≤µ0 = 1, then the Crank–Nicolson method
is L∞
τ (L∞
h )-stable, i.e., there is a constant C > 0, independent of h, τ, and w,
such that
∥w∥L∞
τ (L∞
h ) ≤∥u0,h∥L∞
h + C∥fh∥L∞
τ (L∞
h ),
so that the Crank–Nicolson method is conditionally stable.
3. Deﬁne the error
e = πτ
hu −w ∈V0( ¯Cτ
h ),
ek
i = u(ih, kτ) −w k
i .
Then, provided that µ ≤µ0, we have that there is a constant C > 0 for which
∥e∥L∞
τ (L∞
h ) ≤C
 τ2 + h2
.
In other words, the convergence is second order with respect to time and space.
Proof. We will prove that the method is well deﬁned and stable, leaving conver-
gence to the reader as an exercise; see Problem 28.5.
To show that the method is well deﬁned, we observe that, with the identiﬁcation
w k ←→w k ∈RN, our method can be written as

IN + µ
2 A

w k+1 =

IN −µ
2 A

w k + τf k+1,
where A is the stiﬀness matrix deﬁned in (24.7). Thus, B2 = IN + µ
2 A and A1 =
IN −µ
2 A. Observe that B2 is SPD and strictly diagonally dominant. Therefore, the
method is well deﬁned.
Let us now show stability. For simplicity, we consider the case fh ≡0. Deﬁne
zk = A1w k. Then B2w k+1 = zk. In other words,
−µ
2 w k+1
i−1 + (1 + µ)w k+1
i
−µ
2 w i+1
i+1 = zk
i ,
with slight modiﬁcations at the boundary nodes where w k
0 = w k
N+1 = 0 for all
k ∈{0, . . . , K}. Now let i0 ∈{1, . . . , N} satisfy
w k+1
∞= |w k+1
i0
|. Then

28.3 Stability and Convergence in the L∞
τ (L∞
h )-Norm
783
w k+1
∞= |w k+1
i0
| =
1
1 + µ
zk
i0 + µ
2
 w k+1
i0−1 + w i0+1
i+1

≤
1
1 + µ
h
|zk
i0| + µ
2
 |w k+1
i0−1| + |w i0+1
i+1 |
i
≤
1
1 + µ

∥zk∥∞+ µ∥w k+1∥∞

.
Thus, we have
(1 + µ)
w k+1
∞≤
zk
∞+ µ
w k+1
∞.
Now zk = A1w k can be written in component form as
zk
i = µ
2 w k
i−1 + (1 −µ)w k
i + µ
2 w k
i+1
with slight modiﬁcations at the boundary nodes. Assuming that µ ≤1, we have
that µ
2 , 1 −µ ∈(0, 1), and zk
i is a convex combination of w k
i and w k
i±1. Suppose
that j0 ∈{1, . . . , N} satisﬁes ∥zk∥∞= |zk
j0|. Then
∥zk∥∞= |zk
j0| =
µ
2 w k
j0−1 + (1 −µ)w k
j0 + µ
2 w k
j0+1

≤µ
2 |w k
j0−1| + (1 −µ)|w k
j0| + µ
2 |w k
j0+1|
≤∥w k∥∞.
Putting everything together, if µ ≤1, then we have
w k+1
L∞
h =
w k+1
∞≤
zk
∞≤
w k
∞=
w k
L∞
h .
Consequently,
∥w∥L∞
τ (L∞
h ) ≤
w 0
L∞
h = ∥u0,h∥L∞
h ,
and the stability is established.
Remark 28.15 (alternate proof). The stability of Theorem 28.14 can also be
established as follows. Write the method as
(1 + µ)w k+1
i
= (1 −µ)w k
i + µ
2 w k+1
i−1 + µ
2 w k+1
i+1 + µ
2 w k
i−1 + µ
2 w k
i+1.
Then, taking absolute values and using the fact that 1 −µ ≥0 by assumption, we
ﬁnd
(1 + µ)
w k+1
i
 = |1 −µ|
w k
i
 + µ
2
w k+1
i−1
 + µ
2
w k+1
i+1
 + µ
2
w k
i−1
 + µ
2
w k
i+1

= (1 −µ)
w k
i
 + µ
2
w k+1
i−1
 + µ
2
w k+1
i+1
 + µ
2
w k
i−1
 + µ
2
w k
i+1

≤(1 −µ)
w k
∞+ µ
w k
∞+ µ
w k+1
∞
=
w k
∞+ µ
w k+1
∞.
Since i ∈{1, 2, . . . , N} is arbitrary,
(1 + µ)
w k+1
∞≤
w k
∞+ µ
w k+1
∞.
The result follows by cancellation.

784
Finite Diﬀerence Methods for Parabolic Problems
28.4
Stability and Convergence in the L∞
τ (L2
h)-Norm
In this section, we examine the stability and convergence of our three methods in
the L∞
τ (L2
h)-norm. The technique of analysis changes in this setting to one based
on an examination of the eigenvalues of the stiﬀness matrix.
28.4.1
The Backward Euler Method
Theorem 28.16 (stability and convergence). Suppose that u is a classical solution
to the one-dimensional heat equation (28.1) with the additional regularity
u, ∂xu, . . . , ∂4
x u, ∂tu, ∂2
t u ∈C(¯Ω× [0, T]).
Let w ∈V0( ¯Cτ
h ) be the grid function obtained from the backward Euler method
(28.3). Then:
1. For every k = 0, . . . , K −1, one can write w k+1 = A(w k + τf k+1) with
∥A∥2 ≤1;
consequently, for all τ and h, we have
∥w∥L∞
τ (L2
h) ≤∥u0,h∥L2
h + ∥fh∥L2τ(L2
h).
Therefore, the backward Euler method is unconditionally stable in the L∞
τ (L2
h)-
norm.
2. Deﬁne the error
e = πτ
hu −w ∈V0( ¯Cτ
h ).
Then there is a constant C > 0 for which
∥e∥L∞
τ (L2
h) ≤C(τ + h2).
Thus, the convergence is ﬁrst order with respect to time and second order with
respect to space.
Proof. See Problem 28.6.
28.4.2
The Forward Euler Method
As before, it turns out that the forward Euler method is conditionally stable.
Theorem 28.17 (stability and convergence). Let u be a classical solution to (28.1),
which, in addition, satisﬁes
u, ∂xu, . . . , ∂4
x u, ∂tu, ∂2
t u ∈C(¯Ω× [0, T]).
Let w ∈V0( ¯Cτ
h ) be the grid function obtained from the forward Euler method
(28.4). We have:

28.4 Stability and Convergence in the L∞
τ (L2
h)-Norm
785
1. If the Courant number satisﬁes µ ≤µ0 =
1
2, then one can write w k+1 =
A(w k + τf k) for k = 0, . . . , K −1, where
∥A∥2 ≤1;
consequently, the method is L∞
τ (L2
h)-stable, i.e., we have
∥w∥L∞
τ (L2
h) ≤∥u0,h∥L2
h + ∥fh∥L2τ(L2
h),
so that the forward Euler method is conditionally stable in the L∞
τ (L2
h)-norm.
2. Deﬁne the error
e = πτ
hu −w ∈V0( ¯Cτ
h ),
ek
i = u(ih, kτ) −w k
i .
Then, provided that µ ≤µ0, we have that there is a constant C > 0 for which
∥e∥L∞
τ (L∞
h ) ≤C
 τ + h2
.
In other words, the (conditional) convergence is ﬁrst order with respect to time
and second order with respect to space.
Proof. We prove stability in the case f ≡0 and leave the rest to the reader as an
exercise; see Problem 28.7. Observe that the forward Euler method is equivalent
to w k+1 = Aw k, where
A =


1 −2µ
µ
0
· · ·
0
µ
1 −2µ
...
...
0
...
...
µ
0
...
µ
1 −2µ
µ
0
· · ·
0
µ
1 −2µ


.
Observe that the eigenvectors for A are given by
S = {v 1, v 2, . . . , v N} ,
[v n]i = sin(nπih) .
The corresponding eigenvalues of A are
λn = 1 −2µ [1 −cos(nπh)] .
Now, since A ∈RN×N
sym , we know that ∥A∥2 = ρ(A). Observe that if 0 < µ ≤
µ0 = 1
2, then
0 < 2(1 −cos(nπh)) < 4
⇐⇒
1 > λn > 1 −4µ ≥−1,
which implies that |λn| ≤1. Hence, ∥A∥2 = ρ(A) ≤1. Therefore,
w k+1
L2
h = h1/2 w k+1
2 = h1/2 Aw k
2 ≤h1/2 w k
2 =
w k
L2
h .
Consequently, for any k = 0, . . . , K,
w k
L2
h ≤∥u0,h∥L2
h ,
as we intended to show.

786
Finite Diﬀerence Methods for Parabolic Problems
28.4.3
The Crank–Nicolson Method
Let us now show that the Crank–Nicolson method is unconditionally stable in the
L∞
τ (L2
h)-norm. This is in contrast to the stability in L∞
τ (L∞
h ), which requires a CFL
condition.
Theorem 28.18 (stability and convergence). Let u be a classical solution to (28.1)
with f ≡0, which, in addition, satisﬁes u ∈C4(¯Ω× [0, T]). Let w ∈V0( ¯Cτ
h ) be
the grid function obtained from the Crank–Nicolson method (28.5). We have:
1. For k = 0, . . . , K −1, one can write w k+1 = Aw k with
∥A∥2 ≤1;
consequently, for any τ and h,
∥w∥L∞
τ (L2
h) ≤∥u0,h∥L2
h.
Thus, the Crank–Nicolson method is unconditionally stable in the L∞
τ (L2
h)-norm.
2. Deﬁne the error
e = πτ
hu −w ∈V0( ¯Cτ
h ),
ek
i = u(ih, kτ) −w k
i .
Then we have that there is a constant C > 0, independent of h and τ, for which
∥e∥L∞
τ (L∞
h ) ≤C
 τ2 + h2
.
In other words, the (unconditional) convergence is second order with respect to
time and space.
Proof. We begin by showing stability. Recall that the Crank–Nicolson method is
equivalent to B2w k+1 = A1w k, where B2 and A1 are given in Theorem 28.14.
Since the matrices are of Toeplitz symmetric tridiagonal type, the eigenvectors for
B2 and A1 are the same and are given by
S = {v 1, v 2, . . . , v N} ,
[v n]i = sin(kπih).
Calculating B2v n and A1v n, one ﬁnds
σ(B2) = {λn}N
n=1,
λn = 1 + µ [1 −cos(nπh)] ∈[1, 1 + 2µ],
and
σ(A1) = {νn}N
n=1,
νn = 1 −µ [1 −cos(nπh)].
This implies that B2 is SPD and, therefore, invertible. Deﬁne A2 = B−1
2
and A =
A2A1. Then w k+1 = Aw k.
It is an exercise to show that A ∈RN×N is symmetric; because of this, we know
that ∥A∥2 = ρ(A). The eigenvalues of A are precisely
σn = νn
λn
= 1 −µ [1 −cos(nπh)]
1 + µ [1 −cos(nπh)],
n = 1, . . . , N.

28.4 Stability and Convergence in the L∞
τ (L2
h)-Norm
787
Therefore, if we can show that |σn| ≤1 for all n = 1, . . . , N, then we are able to
conclude. Let us do this in an indirect way. The key is to start with
1 + 1
µ ≥1 ≥cos(nπh),
which is true for all µ > 0 and all 1 ≤n ≤N. This is equivalent to
−1 −1
µ ≤−1 ≤−cos(nπh),
⇐⇒
−1
µ ≤0 ≤1 −cos(nπh),
⇐⇒
−1 ≤0 ≤µ [1 −cos(nπh)],
⇐⇒
−2 ≤0 ≤2µ [1 −cos(nπh)],
⇐⇒
−2 −µ [1 −cos(nπh)] ≤−µ [1 −cos(nπh)] ≤µ [1 −cos(nπh)],
⇐⇒
−1 −µ [1 −cos(nπh)] ≤1 −µ [1 −cos(nπh)] ≤1 + µ [1 −cos(nπh)],
⇐⇒
−λn ≤νn ≤λn,
⇐⇒
|νn| ≤λn = |λn|,
⇐⇒

νn
λn
 ≤1,
⇐⇒
|σn| ≤1.
Hence, ∥A∥2 = ρ(A) ≤1.
We now show convergence. As usual, the error is deﬁned as ek
i = u(xi, tk)−w k
i .
Using the usual techniques, we ﬁnd the error equation has the form
B2ek+1 = A1ek + τEEEτ
h[u]k+1.
We have already shown that
Eτ
h [u]k
i
 ≤C
 τ2 + h2
.
Since B2 is invertible,
ek+1 = Aek + τA2EEEτ
h[u]k+1,
where A2 = B−1
2 . Therefore,
ek+1
2 ≤∥Aen∥2 + τ
A2EEEτ
h[u]k+1
2
≤
ek
2 + τ ∥A2∥2
EEEτ
h[u]k+1
2
≤
ek
2 + τ ∥A2∥2
√
N
EEEτ
h[u]k+1
∞
≤
ek
2 + τ ∥A2∥2
√
NC
 τ2 + h2
.
From stability, we know that ∥A2∥2 = ρ(A2) ≤1; therefore,
ek+1
L2
h ≤
ek
L2
h + Cτh1/2√
N
 τ2 + h2
≤
ek
L2
h + Cτ
 τ2 + h2
.
Applying this result recursively, we arrive at
∥e∥L∞(L2
h) ≤
e0
L2
h + C
 τ2 + h2
.

788
Finite Diﬀerence Methods for Parabolic Problems
h
τ
ϵh,τ =
eK
h,τ

L∞
h
log2

ϵ2h,2τ
ϵh,τ

1
32
1
32
1.0691e−02
–
1
64
1
64
2.6595e−03
2.0071
1
128
1
128
6.6404e−04
2.0018
1
256
1
256
1.6600e−04
2.0000
1
512
1
512
4.1502e−05
2.0000
Table 28.1 Convergence of the Crank–Nicolson method for the problem described in
Example 28.1. The error is computed at the ﬁnal time T = 1.0. Near-perfect
second-order accuracy is observed in the approximation.
The proof is complete.
Example 28.1
We consider, on the interval Ω= (0, 1), problem (28.1). The
initial condition u0 and forcing term f are constructed, so that the exact solution is
u(x, t) = cos(2πt) [exp(sin(2πx)) −1] .
We approximate this solution using the Crank–Nicolson method (28.5). The code
for this approximation is shown in Listing 28.1. Since, in this case, we have access
to the exact solution, we can compute the error as τ and h are made smaller; see
Table 28.1, where the error is reported at the ﬁnal time T = 1.0. Near-perfect
second-order accuracy is observed in the approximation.
28.5
Stability by Energy Techniques
In addition to maximum principles and properties of eigenvalues, there are other
techniques to analyze the stability of numerical methods. For instance, a discrete
analogue of separation of variables is illustrated in Problem 28.9. In this section,
which can be considered the parabolic analogue of Section 24.4.1, we will illustrate
the so-called energy method to obtain stability. The basic idea is to multiply
the method by an appropriate test (grid) function and utilize (a version of) the
summation by parts identity given in (24.1).
Theorem 28.19 (stability). The solution of the backward Euler method (28.3)
satisﬁes
∥w∥2
L∞
τ (L2
h) + ∥w∥2
L2τ(H1
h) ≤
w 02
L2
h + C ∥fh∥2
L2τ(L2
h) ,
where the constant C > 0 is independent of h and τ.
Proof. Take the L2
h-inner product of the method with 2τw k+1 ∈V0(¯Ωh). The
summation by parts identity (24.1) yields
2τ
 ¯δτw k+1, w k+1
L2
h + 2τ∥w k+1∥2
H1
h = 2τ(f k+1
h
, w k+1)L2
h.

28.5 Stability by Energy Techniques
789
Now, observe that the so-called polarization identity
2a(a −b) = a2 + (a −b)2 −b2,
∀a, b ∈R
implies that
2τ
 ¯δτw k+1, w k+1
L2
h = ∥w k+1∥2
L2
h −∥w k∥2
L2
h + ∥w k+1 −w k∥2
L2
h.
Now, by the Cauchy–Schwarz inequality, the discrete Poincar´e inequality of
Theorem 24.34, and Young’s inequality, we obtain that
2τ(f k+1
h
, w k+1)L2
h ≤Cτ∥f k+1
h
∥2
L2
h + τ∥w k+1∥2
H1
h.
Finally, gathering all the obtained estimates, we infer that
∥w k+1∥2
L2
h + ∥w k+1 −w k∥2
L2
h + τ∥w k+1∥2
H1
h ≤∥w k∥2
L2
h + Cτ∥f k+1
h
∥2
L2
h.
Let k0 ∈{0, . . . , K} be such that
∥w k0∥L2
h = ∥w∥L∞
τ (L2
h) .
Adding our estimate over k ∈{0, . . . , k0 −1}, the stability bound follows.
To analyze, by a similar technique, the forward Euler method (28.4), we need
the inverse inequality of Problem 24.23.
Theorem 28.20 (conditional stability). Consider the forward Euler method (28.4).
Assume that the Courant number satisﬁes µ ≤
1
2CI , where CI is the constant in
the inverse inequality of Problem 24.23. In this setting, there is a constant C > 0,
independent of τ and h, such that
∥w∥2
L∞
τ (L2
h) + ∥w∥2
L2τ(H1
h) ≤
w 02
L2
h + C ∥fh∥2
L2τ(L2
h) .
Proof. Let us again take the L2
h-inner product of the method with 2τw k+1 to
obtain, following the derivations of Theorem 28.19, that
∥w k+1∥2
L2
h + ∥w k+1 −w k∥2
L2
h + 2τ(¯δhw k+1, ¯δhw k)L2
h
≤∥w k∥2
L2
h + Cτ∥f k+1
h
∥2
L2
h + τ
2 ∥w k+1∥2
H1
h.
Observe now that
2τ(¯δhw k+1, ¯δhw k)L2
h = 2τ∥w k+1∥2
H1
h −2τ(¯δhw k+1, ¯δh(w k+1 −w k))L2
h,
so that
∥w k+1∥2
L2
h + ∥w k+1 −w k∥2
L2
h + 3
2τ∥w k+1∥2
H1
h
≤∥w k∥2
L2
h + Cτ∥f k+1
h
∥2
L2
h + 2τ(¯δhw k+1, ¯δh(w k+1 −w k))L2
h.
The Cauchy–Schwarz inequality, Young’s inequality, and the inverse inequality of
Problem 24.23 then imply that
2τ(¯δhw k+1, ¯δh(w k+1 −w k))L2
h ≤τ
2 ∥w k+1∥2
H1
h + 2τ∥w k+1 −w k∥2
H1
h
≤τ
2 ∥w k+1∥2
H1
h + 2CIτh−2∥w k+1 −w k∥2
L2
h.

790
Finite Diﬀerence Methods for Parabolic Problems
Since, by assumption, µ ≤
1
2CI , gathering all the obtained bounds and proceeding
as in Theorem 28.19 yields the desired result.
Remark 28.21 (Crank–Nicolson). The stability, using energy techniques, of the
Crank–Nicolson method (28.5) is essentially the content of Problem 28.10.
28.6
Advection–Diﬀusion and Upwinding
Let us, in this section, consider the following problem: Given u0 ∈C(¯Ω) and b > 0,
ﬁnd a classical solution to





∂tu + b∂xu −∂2
xxu = 0,
x ∈Ω, t ∈(0, T],
u(0, t) = u(1, t) = 0,
t ∈(0, T],
u(x, 0) = u0(x),
x ∈Ω,
(28.7)
with b > 0. We assume that the compatibility conditions u0(0) = u0(1) = 0 hold
and that u has regularity
u, ∂xu, . . . , ∂4
x u, ∂tu, ∂2
t u ∈C(¯Ω× [0, T]).
Our goal will be to construct explicit homogeneous methods that are monotone, in
the sense of Section 24.4.2. We refer the reader to Section 24.4.2 for some notions
that will be relevant in our present discussion. Before proceeding any further, we
comment that similar considerations can be done for implicit methods. These will
be developed in Problems 28.17 and 28.18.
Let us introduce a centered discretization of the problem. Thus, we seek w ∈
V0( ¯Cτ
h ) such that
w 0 = u0,h,
δτw k + b˚δhw k −∆hw k = 0,
k = 1, . . . , K,
(28.8)
where u0,h = πhu0 and πh is the sampling operator.
Theorem 28.22 (conditional stability). Assume that the mesh P´eclet number
introduced in Remark 24.39 satisﬁes
hb ≤2
and the Courant number satisﬁes
µ ≤1
2.
Then the method (28.8) is stable in the L∞
τ (L∞
h )-norm.
Proof. Let k be arbitrary and i0 ∈{1, . . . , N} be such that
w k+1
L∞
h = |w k+1
i0
|.

28.6 Advection–Diﬀusion and Upwinding
791
Then we have
w k+1
L∞
h = |w k+1
i0
| =
w k
io + µ
 w k
io−1 −2w k
io + w k
io+1

−bµh
2
 w k
io+1 −w k
io−1

=
µ

1 + bh
2

w k
i0−1 + (1 −2µ) w k
i0 + µ

1 −bh
2

w k
i0+1

≤µ
1 + bh
2
 |w k
i0−1| + |1 −2µ| |w k
i0| + µ
1 −bh
2
 |w k
i0+1|
≤

µ
1 + bh
2
 + |1 −2µ| + µ
1 −bh
2

 w k
L∞
h .
Now the condition on the mesh P´eclet number implies that 1 −bh
2 ≥0 and the
condition on the Courant number that 1 −2µ ≥0. Therefore,
w k+1
L∞
h ≤
w k
L∞
h ,
as we intended to show.
From stability, one can obtain convergence.
Corollary 28.23 (convergence). Let u be the classical solution to (28.7) and w ∈
V0( ¯Cτ
h ) the solution to (28.8). Deﬁne the error
e = πτ
hu −w,
ek
i = u(ih, kτ) −w k
i .
Under the assumption that the mesh P´eclet number satisﬁes
hb ≤2,
and the Courant number satisﬁes
µ ≤1
2,
we have that there is a constant C > 0 such that
∥e∥L∞
τ (L∞
h ) ≤C(τ + h2).
Proof. We have that, as usual, the error satisﬁes the equation
δτek+1 + b˚δhek −∆hek = Eτ
h [u]k,
k = 1, . . . , K,
where the consistency error satisﬁes
∥Eτ
h [u]∥L∞
τ (L∞
h ) ≤C(τ + h2).
From stability, we obtain then that
ek+1
L∞
h ≤
ek
L∞
h + τ
Eτ
h [u]k
L∞
h ≤
ek
L∞
h + Cτ(τ + h2),
which, adding over k, implies the claimed convergence result.

792
Finite Diﬀerence Methods for Parabolic Problems
As we see from the previous two results, stability for a centered discretization is
obtained under two conditions: a CFL condition, which is expected as the method
is explicit, and a condition on the mesh P´eclet number. This is due to the fact
that we are using a centered discretization. At the expense of a reduced order
of convergence, stability can be obtained for any P´eclet number using an upwind
discretization of the advection term.
We seek w ∈VK
¯N,0( ¯Cτ
h ) such that
w 0 = u0,h,
δτw k + b¯δhw k −∆hw k = 0,
k = 1, . . . , K,
(28.9)
where u0,h = πhu0.
Theorem 28.24 (conditional stability). Assume that the Courant number satisﬁes
µ ≤
1
2 + bh.
Then the method (28.9) is stable in the L∞
τ (L∞
h )-norm. Moreover, the error
e = πτ
hu −w,
ek
i = u(ih, kτ) −w k
i ,
under the same assumption, satisﬁes
∥e∥L∞
τ (L∞
h ) ≤C(τ + h).
Proof. See Problem 28.13.
28.7
The Initial Value Problem for the Heat Equation
in One Dimension
This section is the discrete analogue of Section 23.3.1, but for simplicity we will
only consider the case d = 1. We will approximate the solution of (23.24) with
a ﬁnite diﬀerence method and describe a very general, powerful, and celebrated
technique to analyze the stability of diﬀerence methods known as von Neumann
stability analysis.5
Let us introduce the ﬁnite diﬀerence methods that we will use to approximate
the solution to (23.24) with d = 1.
Deﬁnition 28.25 (simple methods). Let T > 0, K, N ∈N, τ = T/K, and h =
1/(N+1). Let u ∈Cb(R×[0, T]) be the classical solution to (23.24). Suppose that
u0,h ∈V(Zh) is deﬁned as usual. We deﬁned the following methods for computing
the ﬁnite diﬀerence approximation w ∈V([0, T]τ; V(Zh))u.
1. The backward Euler method
(¯δτw k+1 −∆hw k+1 = 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(28.10)
5 Named in honor of the Hungarian–American mathematician, physicist, computer scientist,
engineer, and polymath John von Neumann (1903–1957).

28.7 The Initial Value Problem for the Heat Equation in One Dimension
793
2. The forward Euler method
(
δτw k −∆hw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(28.11)
3. The Crank–Nicolson method



¯δτw k+1 −1
2∆h
 w k+1 + w k
= 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(28.12)
28.7.1
The Discrete Fourier Transform
The notions of consistency and stability for these, or any other method for (23.24),
are taken as usual. Notice, however, that since we are dealing with a problem on
the whole space we must deal with grid functions in V(Zh). We need to introduce
then a suitable norm on this space.
One possibility for a norm of V(Zh) is
∥v∥L∞
h = sup
j∈Z
|vj|,
which is the analogue of discrete norms we have seen before. It is important to
note that, in this case, this is truly a supremum, as the index j runs over the inﬁnite
set Z. Let us, in addition, introduce another one that will be more suited for our
purposes.
Deﬁnition 28.26 (L2
h(Zh)). For N ∈N, let h = 1/(N + 1). We denote the space
of square summable grid functions by
L2
h(Zh) =
n
v ∈V(Zh)
 ∥v∥L2
h < ∞
o
,
where
∥v∥L2
h =

h
X
j∈Z
|vj|2


1/2
.
As usual, if the range of our grid functions is a ﬁnite-dimensional vector space
V ̸= R, then we denote this by L2
h(Zh; V).
It is not diﬃcult to verify that L2
h(Zh) is a vector space and that ∥·∥L2
h is a norm
that makes this a Hilbert space.
Theorem 28.27 (properties of L2
h(Zh)). The space L2
h(Zh) is Hilbert, with the
inner product
(v1, v2)L2
h = h
X
j∈Z
v1(jh)v2(jh).
This inner product induces the L2
h-norm.
Proof. See Problem 28.19.

794
Finite Diﬀerence Methods for Parabolic Problems
On square summable grid functions, we can deﬁne a discrete analogue of the
Fourier transform (23.25).
Deﬁnition 28.28 (FZT/DFT). The so-called Fourier-Z transform (FZT) or dis-
crete Fourier transform (DFT) is the mapping Z : L2
h(Zh; C) →L2((−π, π); C)
deﬁned by
ˆv(ξ) = Z[v](ξ) = h
X
k∈Z
vke−ikξ.
The properties of the FZT are the content of the following result.
Theorem 28.29 (properties of the FZT). The FZT is well deﬁned. In particular,
∥Z[v]∥2
L2(−π,π;C) = 2πh∥v∥2
L2
h,
∀v ∈L2
h(Zh; C).
The mapping Z is invertible, and its inverse, for v ∈L2(−π, π; C), is given by
Z−1[v]j =
1
2πh
Z π
−π
v(ξ)eijξdξ.
Finally, we have the shift property: if v ∈L2
h(Zh; C) and m ∈Z, then
Z[v·−m](ξ) = e−imξZ[v](ξ).
Proof. First, observe that, for v ∈L2
h(Zh; C), we have
∥Z[v]∥2
L2(−π,π;C) = h2
Z π
−π
X
k∈Z
vke−ikξ ·
X
k∈Z
vme−imξdξ
= h2 X
k,m∈Z
vkvm
Z π
−π
ei(m−k)ξdξ
= 2πh2 X
k,m∈Z
vkvmδk,m
= 2πh2 X
k∈Z
|vk|2,
so that indeed Z[v] ∈L2(−π, π; C) with the claimed identity for norms.
To verify the formula for the inverse, consider
Z−1[Z[v]]j =
1
2πh
Z π
−π
Z[v](ξ)eijξdξ
= 1
2π
X
k∈Z
vk
Z π
−π
e−ikξeijξdξ
=
X
k∈Z
vkδk,j = vj.
The shift property is immediate
Z[v·−m](ξ) = h
X
k∈Z
vk−me−ikξ = he−imξ X
k∈Z
vk−me−i(k−m)ξ = e−imξZ[v](ξ).

28.7 The Initial Value Problem for the Heat Equation in One Dimension
795
28.7.2
The Symbol of a Finite Diﬀerence Method
To motivate the deﬁnition we will introduce next, we consider the forward Euler
method (28.11). In full analogy to what was done in Section 23.3.1 to reach the
solution representation (23.27), we take now the FZT of (28.11) to obtain, using
the shift property described in Theorem 28.29, that
ˆw k+1 −ˆw k = µ
 e−iξ + eiξ −2

ˆw k.
In other words,
ˆw k+1 = (1 −2µ + 2µ cos ξ) ˆw k.
We now see that all the properties of the method are encoded in the expression
˜Eτ
h (ξ) = 1 −2µ + 2µ cos ξ.
Deﬁnition 28.30 (two-layer method). Let Ah, Bh : V(Zh) →V(Zh) be ﬁnite
diﬀerence operators with constant coeﬃcients, i.e.,
(Ahv)j =
X
m∈SA
am(h, τ)vj−m,
(Bhv)j =
X
m∈SB
bm(h, τ)vj−m,
∀v ∈V(Zh),
where SA, SB ⊂Z have ﬁnite cardinality and am, bm : R+ × R+ →R\{0}
are functions. Assume that Ah is invertible. A two-layer time stepping ﬁnite
diﬀerence method with constant coeﬃcients is deﬁned as follows: ﬁnd w ∈
V([0, T]τ; V(Zh)) such that w 0 ∈V(Zh) is given, and
Ahw k+1 = Bhw k,
k = 0, . . . , K −1.
(28.13)
If Ah is the identity operator, then we say that the method is explicit; otherwise,
we say that it is implicit.
Deﬁnition 28.31 (symbol). The symbol or ampliﬁcation factor of the two-layer
time stepping ﬁnite diﬀerence method (28.13) is
˜Eτ
h (ξ) =
P
m∈SB bm(h, τ)e−imξ
P
m∈SA am(h, τ)e−imξ ,
ξ ∈R.
(28.14)
Example 28.2
The symbol of the backward Euler method (28.10) is
˜Eτ
h (ξ) =
1
1 + 2µ −2µ cos ξ;
see Problem 28.21.
Example 28.3
The symbol of the forward Euler method (28.11) is
˜Eτ
h (ξ) = 1 −2µ + 2µ cos ξ,
as we showed at the beginning of our discussion.

796
Finite Diﬀerence Methods for Parabolic Problems
Example 28.4
The symbol of the Crank–Nicolson method (28.12) is
˜Eτ
h (ξ) = 1 −µ + µ cos ξ
1 + µ −µ cos ξ;
see Problem 28.22.
The following is nothing but a mere observation.
Proposition 28.32 (properties of the symbol). Consider the time stepping method
(28.13). Its symbol ˜Eτ
h (ξ) is, in general, a 2π-periodic rational trigonometric
function. If the method is explicit, then the symbol is a trigonometric polynomial.
Proof. See Problem 28.23.
Since all the properties of the method (28.13) are encoded in its symbol, it
is of no surprise that this is a very useful tool to provide a stability analysis in
various norms. For instance, the following is a necessary condition for stability in
the L∞
τ (L∞
h )-norm.
Theorem 28.33 (L∞
τ (L∞
h )-stability). Let the time stepping method (28.13) with
symbol ˜Eτ
h (ξ) be explicit and stable in the L∞
τ (L∞
h )-norm. Then its symbol satisﬁes
 ˜Eτ
h (ξ)
 ≤1,
∀ξ ∈R.
Proof. We will argue by contradiction. We assume that the method is stable in the
L∞
τ (L∞
h )-norm, yet there is ξ0 ∈[−π, π] for which
 ˜Eτ
h (ξ0)
 = q > 1.
Let ε ∈(0, 1) and deﬁne v ∈V(Zh; C) via
vj = εeijξ0,
∥v∥L∞
h = ε.
Let w ∈V([0, T]τ; V(Zh)) be the solution to (28.13) with initial condition w 0 =
v. Now, since the method is explicit, for any j ∈Z, we have
w 1
j = ε
X
m∈SB
bm(h, τ)ei(j−m)ξ0 = εeijξ0 X
m∈SB
bm(h, τ)e−imξ0 = ˜Eτ
h (ξ0)vj;
this implies that, for k ≥0,
w k =
 ˜Eτ
h (ξ0)
k v.
Consequently,
w k
L∞
h = εqk →∞,
k →∞,
regardless of the value of ε. Thus, the method is not stable in the L∞
τ (L∞
h )-norm.

28.7 The Initial Value Problem for the Heat Equation in One Dimension
797
Example 28.5
As we have seen before, for the forward Euler method (28.11),
˜Eτ
h (ξ) = 1 −2µ + 2µ cos ξ.
Since cos ξ ∈[−1, 1], then
 ˜Eτ
h (ξ)
 ≤1
is possible if and only if
−1 ≤1 −2µ −2µ
and
1 −2µ + 2µ ≤1.
Thus, we require
1 −4µ ≥−1
⇐⇒
µ ≤1
2,
which is the CFL-type condition we have obtained before for stability of forward
Euler.
28.7.3
The von Neumann Stability Analysis
It turns out that the condition on the modulus of the symbol
 ˜Eτ
h (ξ)
 ≤1,
∀ξ ∈R,
(28.15)
which is called the von Neumann stability condition, is not only necessary but also
suﬃcient for stability, provided that we study stability in L∞
τ (L2
h)-norms.
Theorem 28.34 (von Neumann6). Consider the time stepping method (28.13)
with symbol ˜Eτ
h (ξ). This method is stable in the L∞
τ (L2
h)-norm if and only if the
von Neumann stability condition (28.15) holds.
Proof. Let w ∈V([0, T]τ; V(Zh)) be the solution to method (28.13) with initial
condition u0,h ∈L2
h(Zh). Problem 28.20 implies that
ˆw k+1(ξ) = ˜Eτ
h (ξ) ˆw k(ξ)
=⇒
ˆw k(ξ) =
 ˜Eτ
h (ξ)
k+1 ˆu0,h(ξ).
Assume now that (28.15) holds. Using Theorem 28.29, we have that
∥w k∥2
L2
h =
1
2πh∥ˆw k∥2
L2(−π,π;C)
=
1
2πh

 ˜Eτ
h
k ˆu0,h

2
L2(−π,π;C)
≤
1
2πh
sup
ξ∈[−π,π]
 ˜Eτ
h (ξ)
2k ∥ˆu0,h∥2
L2(−π,π;C)
=
sup
ξ∈[−π,π]
 ˜Eτ
h (ξ)
2k ∥u0,h∥2
L2
h
≤∥u0,h∥2
L2
h ,
which is the desired stability result.
6 Named in honor of the Hungarian–American mathematician, physicist, computer scientist,
engineer, and polymath John von Neumann (1903–1957).

798
Finite Diﬀerence Methods for Parabolic Problems
Assume now that (28.15) fails at a point. Owing to Proposition 28.32, the
symbol is a rational trigonometric function. Thus, without loss of generality, this is
an interior point of [−π, π], i.e., there are q > 1, δ > 0, and ξ0 ∈(−π, π) such that
[ξ0 −δ, ξ0 + δ] ⊆[−π, π]
and
 ˜Eτ
h (ξ)
 ≥q > 1,
∀ξ ∈[ξ0 −δ, ξ0 + δ].
Consider now the grid function
u0,h(kh) = eikξ0
πhk sin(kδ).
We leave it to the reader as an exercise to show, see Problem 28.24, that
u0,h ∈L2
h(Zh),
ˆu0,h(ξ) = χ[ξ0−δ,ξ0+δ].
Using this as an initial condition, we have that
∥w k∥2
L2
h =
1
2πh
Z π
−π
 ˜Eτ
h (ξ)
2k |ˆu0,h(ξ)|2dξ
≥q2k
2πh
Z ξ0+δ
ξ0−δ
|ˆu0,h(ξ)|2dξ
= q2k∥u0,h∥2
L2
h →∞,
as k →∞. In conclusion, the method is not stable.
Example 28.6
The backward Euler method (28.10) is unconditionally stable in
the L∞
τ (L2
h)-norm. Indeed, we have that
˜Eτ
h (ξ) =
1
1 + 2µ −2µ cos ξ.
Indeed, since cos ξ ∈[−1, 1],
1 = 1 + 2µ −2µ ≤1 + 2µ −2µ cos ξ,
which implies that
0 < ˜Eτ
h (ξ) ≤1,
∀ξ ∈[−π, π].
Example 28.7
The forward Euler method (28.11) is stable, in the L∞
τ (L2
h)-norm,
provided that the CFL-type condition
µ ≤1
2
holds; see Example 28.5.

28.7 The Initial Value Problem for the Heat Equation in One Dimension
799
Example 28.8
The Crank–Nicolson method (28.12) is unconditionally stable in
the L∞
τ (L2
h)-norm; see Problem 28.25.
Remark 28.35 (periodic boundary conditions). Finally, to end this section, we
remark that this powerful von Neumann stability analysis can be used under
the assumption of periodic boundary conditions, with very little change in the
computations. The details are left to the reader. The essence of the analysis is to
use the appropriate eigenfunctions of the ﬁnite diﬀerence operators in the inﬁnite
or periodic settings, where boundary eﬀects can be neglected, to compute the
symbol. With this at hand, one can easily assess the stability of the method in
the L∞
τ (L2
h)-norm. Interestingly, the same CFL-type restrictions are required for
L∞
τ (L2
h)-stability in the case of a bounded, periodic, or inﬁnite domain.
28.7.4
Convergence
It turns out that the symbol of a time stepping method can also be used to study
the convergence of this method. Here, we brieﬂy describe how this can be achieved.
To accomplish this, however, we need to introduce some notions.
Notice that we can identify grid functions with piecewise constant ones. In other
words, given v ∈V(Zh), we can deﬁne Vh : R →R by
Vh(x) = vj,
x ∈

j −1
2

h,

j + 1
2

h

.
Notice also that
∥v∥2
L2
h = h
X
j∈Z
|vj|2 =
X
j∈Z
Z (j+ 1
2 )h
(j−1
2 )h
|vj|2dx =
X
j∈Z
Z (j+ 1
2 )h
(j−1
2 )h
|Vh(x)|2dx = ∥Vh∥2
L2(R;C) ,
so that Vh ∈L2(R) if and only if v ∈L2
h(Zh). Because of this, we can take the
Fourier transform of Vh and obtain, see Problem 28.30,
F[Vh](ξ) = 2
hξ sin
hξ
2

Z[v](hξ).
(28.16)
That is, the FZT of a grid function is a dilation of the Fourier transform of its
representative times a constant factor.
We can now start to study the convergence of a time stepping method using the
symbol. We begin with a deﬁnition.
Deﬁnition 28.36 (accuracy). Let the time stepping method (28.13) have symbol
˜Eτ
h (ξ). We say that the method is accurate of order p ∈N if there are constants
C1, C2 > 0 such that whenever
|ξ| ≤C2,
then
 ˜Eτ
h (ξ) −e−µξ2 ≤C1|ξ|p+2,
where µ > 0 is the Courant number.

800
Finite Diﬀerence Methods for Parabolic Problems
The meaning of this deﬁnition is detailed in Problem 28.31.
Example 28.9
The forward Euler method (28.11) is accurate of order p = 2. As
we recall,
˜Eτ
h (ξ) = 1 −2µ + 2µ cos ξ.
Doing a Taylor expansion of this expression about ξ = 0, we ﬁnd there is η ∈R
for which
˜Eτ
h (ξ) = 1 −2µ + 2µ

1 −ξ2
2 + ξ4
24 + η|ξ|6

= 1 −µξ2 + µ
12ξ4 + 2µη|ξ|6
= 1 −µξ2 + µ2
2 ξ4 −µ
2

µ −1
6

ξ4 + 2µη|ξ|6.
If we compare this expression with the Taylor expansion of e−µξ2 around ξ = 0,
which is
e−µξ2 = 1 −µξ2 + µ2
2 ξ4 + C|ξ6|,
we conclude that, if µ ̸= 1
6, the forward Euler method is accurate of order p = 2,
and that if µ = 1
6, then it is accurate of order p = 4.
To gain some intuition into how this notion can be useful, we will look at the
error. Usually, to measure the error, we project the exact solution onto the grid.
Here, we will go the opposite route, i.e., deﬁne
ek(x) = u(x, kτ) −W k
h (x),
where, for k ∈{0, . . . , K}, the function W k
h is the piecewise constant representative
of the grid function w k. The advantage of this is that now we are dealing with usual
functions. Then we can take the Fourier transform of the error after one step to
obtain
ˆe1(ξ) = ˆu(ξ, τ) −F[W 1
h ](ξ)
= ˆu(ξ, τ) −2
hξ sin
hξ
2

Z[w 1](hξ)
= ˆu(ξ, τ) −2
hξ sin
hξ
2

˜Eτ
h (hξ)Z[w 0](hξ)
= ˆu(ξ, τ) −˜Eτ
h (hξ)F[W 0
h ](ξ)
=
 ˆu(ξ, τ) −˜Eτ
h (hξ)ˆu0(ξ)

+
  ˜Eτ
h (hξ)F

u0 −W 0
h

(hξ)

.

28.7 The Initial Value Problem for the Heat Equation in One Dimension
801
In other words, the error after one step consists of two terms. The ﬁrst one
ˆu(ξ, τ) −˜Eτ
h (hξ)ˆu0(ξ)
encodes the error in the time stepping method. The second term is
˜Eτ
h (hξ)F

u0 −W 0
h

(hξ),
which depends on how well we approximated the initial condition u0. Usually,
w 0 = πhu0, so that this expression is dependent only on the smoothness of u0.
Let us focus our attention then on the ﬁrst term. Using that ˆu(ξ, t) = ˆu0(ξ)e−tξ2,
see Section 23.3.1, implies that
ˆu(ξ, τ) −˜Eτ
h (hξ)ˆu0(ξ) =

e−µh2ξ2 −˜Eτ
h (hξ)

ˆu0(ξ),
where we also used that τ = µh2, where µ is the Courant number. Let us now
assume that ξ →0 while keeping µ constant and that the method is accurate of
order p ∈N, then
|ˆu(ξ, τ) −˜Eτ
h (hξ)ˆu0(ξ)| ≤Chp+2 = C
µ τhp.
The use of this idea to assert convergence is the content of the following result.
Theorem 28.37 (L∞
τ (L2(R))-convergence). Let the time stepping method (28.13)
be accurate of order p ∈N and stable in L∞
τ (L2
h). If u0 is suﬃciently smooth, h
is suﬃciently small, and µ = τ/h2 is kept constant, then we have that, for every
k ∈{0, . . . , K},
∥W k
h −u(·, tk)∥L2(R) ≤Ctkhp +
W 0
h −u0

L2(R) ,
where the constant C does not depend on h or τ. It may depend, however, on µ
and the smoothness of u0 via norms of its derivatives.
Proof. We begin with similar computations to those that motivated us. First, by
Plancherel’s identity of Proposition 23.16,
∥W k
h −u(·, tk)∥L2(R) = ∥c
W k
h −bu(·, tk)∥L2(R;C).
Set κ(h, ξ) = 2 sin(hξ/2)/(hξ) and observe that
c
W k
h = κ(h, ξ)Z[w k](hξ) = κ(h, ξ) ˜Eτ
h (hξ)kZ[w 0](hξ) = ˜Eτ
h (hξ)kc
W 0
h .
Therefore, by the triangle inequality,
∥W k
h −u(·, tk)∥L2(R) ≤
 ˜Eτ
h (h ·)k bu0 −bu(·, tk)

L2(R;C)
+
 ˜Eτ
h (h ·)k[c
W 0
h −bu0]

L2(R;C)
≤
 ˜Eτ
h (h ·)k bu0 −bu(·, tk)

L2(R;C) +
W 0
h −u0

L2(R) ,
where, in the last step, we used that the method is L∞
τ (L2
h)-stable; thus, by
Theorem 28.34, condition (28.15) holds, followed by, once again, an application of
Plancherel’s identity. It remains then to bound the ﬁrst term, for which we observe
that

802
Finite Diﬀerence Methods for Parabolic Problems
 ˜Eτ
h (h ·)k bu0 −bu(·, tk)
2
L2(R;C) =
Z
R
 ˜Eτ
h (hξ)k bu0(ξ) −bu(ξ, tk)
2 dξ
=
Z
R
 ˜Eτ
h (hξ)k −e−µkh2ξ2
2
|bu0(ξ)|2dξ.
(28.17)
Observe now that, since the method is accurate to order p ∈N, for |ζ| ≤C2,
we have
 ˜Eτ
h (ζ)k −e−µkζ2 =


˜Eτ
h (ζ) −e−µζ2 k−1
X
j=0
˜Eτ
h (ζ)n−1−je−µjζ2

≤
 ˜Eτ
h (ζ) −e−µζ2
k−1
X
j=0
| ˜Eτ
h (ζ)n−1−j|
≤C1k|ζ|p+2,
where we also used the von Neumann stability condition. Using this estimate in
(28.17) yields
 ˜Eτ
h (h ·)k bu0 −bu(·, tk)
2
L2(R;C) ≤C2
1k2h2(p+2)
Z
R
ξp+2bu0(ξ)
2 dξ.
Use now that k2h4 = t2
k /µ2 and the diﬀerentiation property of Proposition 23.16
to conclude then that
 ˜Eτ
h (h ·)k bu0 −bu(·, tk)
2
L2(R;C) ≤C2
1
µ2 t2
k h2p u(p+2)
0

2
L2(R) .
Gathering all the obtained bounds, the result now follows.
Problems
28.1
Prove Proposition 28.4.
28.2
Prove Theorem 28.8.
28.3
Does a maximum principle, such as that of Theorem 28.9, hold for the
forward Euler method (28.4)? Perhaps under a CFL-type condition?
28.4
Prove Theorem 28.12.
28.5
Complete the proof of Theorem 28.14.
28.6
Prove Theorem 28.16.
28.7
Complete the proof of Theorem 28.17.
28.8
To approximate the classical solution of (28.1) with f ≡0, we consider the
following method: Find a grid function w ∈V0( ¯Cτ
h ) such that w 0 = πhu0 and
¯δτw k+1 −∆h
 σw k+1 + (1 −σ)w k
= 0,
k = 0, . . . , K −1.
(28.18)
Here, σ ∈R is a given parameter. Show that if
σ = 1
2 −h2
12τ ,
then this method is consistent, in Cb(R2), with the heat equation (28.1).
Hint: Use (xi, tk + τ/2) as the base point for your Taylor expansions.

Problems
803
28.9
In this problem, we will develop the method of separation of variables to
study stability, in the L∞
τ (L2
h)-norm, of diﬀerence methods.
a)
Show that method (28.18) can be equivalently written as
δτw k −στ∆hδτw k = ∆hw k,
k = 1, . . . , K.
b)
We will seek our solution w ∈V0( ¯Cτ
h ) as the product of two functions
w = XwTw,
Xw ∈V0(¯Ωh),
Tw ∈V([0, T]τ).
Show that, for all k,
T k+1
w
−T k
w
τ
 σT k+1
w
−(1 −σ)T kw
 = ∆hXw
Xw
= −λ,
(28.19)
where λ ∈R.
c)
Show that, for any λ > 0,
T k+1
w
= qT k
w,
q = 1 −(1 −σ)τλ
1 + στλ
,
solves (28.19).
d)
Using the results of Theorem 24.25, deduce then that the pairs {Xw,n, λn}N
n=1
λn = 4
h2 sin2
nπh
2

,
Xw,n(xi) =
√
2 sin(nπih)
are solutions to (28.19). In addition, show that the system {Xw,n}N
n=1 is an
orthonormal (in the L2
h-inner product) basis of V0(¯Ωh).
e)
From the previous two points, show then that the solution to (28.18) has the
form
w k =
N
X
n=1
u0,nqk
nXw,
qn = 1 −(1 −σ)τλn
1 + στλn
,
with
w 0 =
N
X
n=1
u0,nXw,
w 0
L2
h =
 N
X
n=1
|u0,n|2
!1/2
.
f)
Show that method (28.18) is stable, in the L∞
τ (L2
h)-norm, provided that
max
n=1,...,N |qn| ≤1
⇐⇒
σ ≥1
2 −h2
4τ .
28.10
Consider the Crank–Nicolson method given in (28.5) with fh ≡0. Show
that, for every k ∈{0, . . . , K −1}, we have
∥w k+1∥2
L2
h + τ
2
¯δh(w k+1 + w k)
2
L2
h = ∥w k∥2
L2
h.

804
Finite Diﬀerence Methods for Parabolic Problems
28.11
Consider the so-called Richardson method7 to approximate the classical
solution of (28.1) with f ≡0: Find a grid function w ∈V0( ¯Cτ
h ) such that w 1 =
w 0 = πhu0 and
1
2τ
 w k+1 −w k−1
−∆hw k = 0,
k = 1, . . . , K −1.
Show that this method is unconditionally unstable in the L∞
τ (L2
h)-norm. In other
words, the method is unstable regardless of the choice of τ and h.
28.12
Consider the Du Fort–Frankel method8 to approximate the classical
solution of (28.1) with f
≡0: Find a grid function w ∈V0( ¯Cτ
h ) such that
w 1 = w 0 = πhu0 and, for k = 1, . . . , K −1 and i = 1, . . . , N,
1
2τ
 w k+1
i
−w k−1
i

−1
h2
 w k
i−1 −w k+1
i
−w k−1
i
+ w k
i+1

= 0.
Study the consistency and stability, in the L∞
τ (L2
h)-norm, of this method.
28.13
Prove Theorem 28.24.
28.14
Consider the following linear reaction–diﬀusion problem: Find u : [0, 1] ×
[0, T] →R that is a classical solution of









∂u(x, t)
∂t
−∂2u(x, t)
∂x2
+ u(x, t) = 0,
(x, t) ∈(0, 1) × (0, T],
u(0, t) = u(1, t) = 0,
t ∈(0, T],
u(x, 0) = u0(x),
x ∈[0, 1].
We propose a Crank–Nicolson-like method for this problem: Find w ∈V0( ¯Cτ
h ) that
satisﬁes
¯δτw k+1 −1
2∆h(w k+1 + w k) + 1
2(w k+1 + w k) = 0,
k = 0, . . . , K −1.
Provide the conditions that guarantee that this method is stable, in the L∞
τ (L2
h)-
norm, and convergent with the error estimate
∥e∥L∞
τ (L2
h) ≤C(τ2 + h2),
where C is independent of h and τ. You may assume, without proof, that the
consistency error has the appropriate form you need.
28.15
Consider the heat equation in a periodic domain. That is, we need to ﬁnd
u : R × [0, T] →R that is spatially one-periodic, i.e., for all t ∈[0, T] and any
x ∈R, it satisﬁes
u(x + n, t) = u(x, t),
∀n ∈Z,
and is a classical solution of



∂u(x, t)
∂t
= ∂2u(x, t)
∂x2
,
(x, t) ∈R × (0, T],
u(x, 0) = u0(x),
x ∈R,
7 Named in honor of the British mathematician, physicist, and meteorologist Lewis Fry
Richardson (1881–1953).
8 Named in honor of E.C. Du Fort and the American computer scientist Stanley Phillips
Frankel (1919–1978).

Problems
805
where u0 ∈C∞
p (0, 1). Devise a Crank–Nicolson approximation to this problem and
prove that, for all values of the Courant number µ > 0, this approximation is stable
in the L∞
τ (L2
h)-norm. In addition, show that the error satisﬁes the estimate
∥e∥L∞
τ (L2
h) ≤C
 τ2 + h2
,
where C > 0 is a constant that is independent of τ and h.
28.16
Consider the following problem: As usual, Ω= (0, 1) and we seek the
solution to the initial boundary value problem









∂u(x, t)
∂t
−∂
∂x

a(x)∂u(x, t)
∂x

= f (x, t),
(x, t) ∈Ω× (0, T],
u(0, t) = u(1, t) = 0,
t ∈[0, T],
u(x, 0) = u0(x),
x ∈¯Ω.
Here, a ∈C(¯Ω) is such that there is λ > 0 for which
a(x) ≥λ,
∀x ∈¯Ω.
In addition, f ∈C( ¯C) and u0 ∈C(¯Ω) with u(0) = u(1) = 0. To approximate the
solution to this problem, we propose the following ﬁnite diﬀerence method: Find
w ∈V0( ¯Cτ
h ) such that w 0 = πhu0 and
¯δτw k+1 + Lhw k+1 = f k+1
h
,
k = 0, . . . , K −1.
Here, πh denotes the sampling operator, fh ∈V(Cτ
h ) is given by fh(ih, kτ) =
f (ih, kτ), and Lh is the conservative ﬁnite diﬀerence method given in (24.14). Show
that the solution of this method satisﬁes, for some constant that is independent
of h and τ,
∥w∥2
L∞
τ (L2
h) + ∥w∥2
L2τ(H1
h) ≤C

∥w 0∥2
L2
h + ∥fh∥2
L2τ(L2
h)

.
Is this method convergent?
28.17
To approximate the classical solution of (28.7), we use an implicit method
with a centered discretization of the ﬁrst derivative
¯δτw k+1 + b˚δhw k+1 −∆hw k+1 = 0.
Study the stability, in the L∞
τ (L∞
h )-norm, of this method.
28.18
To approximate the classical solution of (28.7), we use an implicit method
with an upwind discretization of the ﬁrst derivative
¯δτw k+1 + b¯δhw k+1 −∆hw k+1 = 0.
Study the stability, in the L∞
τ (L∞
h )-norm, of this method.
28.19
Prove Theorem 28.27.
28.20
Show that the deﬁnition of the symbol, provided in Deﬁnition 28.31, is
obtained by taking the discrete Fourier transform of the method (28.13).
28.21
Find the symbol of the backward Euler method (28.10).
28.22
Find the symbol of the Crank–Nicolson method (28.12).
28.23
Prove Proposition 28.32.

806
Finite Diﬀerence Methods for Parabolic Problems
28.24
Consider the grid function v : Zh →C
vk = eikξ0
πhk sin(kδ),
where ξ0 ∈(−π, π) and δ < min{π −ξ0, ξ0 + π}. Show that
v ∈L2
h(Zh),
ˆv(ξ) = χ[ξ0−δ,ξ0+δ].
28.25
Provide all the details for Example 28.8.
28.26
Let T, γ > 0 and u0 ∈C(R) ∩L2(R) be given. Consider the IVP: Find
u : R × [0, T] →R such that



∂u(x, t)
∂t
−∂2u(x, t)
∂x2
+ γu(x, t) = 0,
(x, t) ∈R × (0, T],
u(x, 0) = u0(x),
x ∈R,
which models a diﬀusion with, since γ > 0, decay. Let θ ∈R and consider a family
of numerical methods of the form
¯δτw k+1 −1
2∆h
 w k+1 + w k
+ γ
 θw k+1 + (1 −θ)w k
= 0,
where τ and h are the time and space step sizes, respectively.
a)
By computing the consistency error, show that this method is O(τp + h2)
accurate, where p = 2, if θ = 1/2, and p = 1, otherwise.
b)
Using the von Neumann stability analysis, show that this method is uncondi-
tionally stable if θ ≥1/2.
c)
Show that if θ = 0, then the method is stable provided that τ ≤2/γ ,
independently of h > 0.
28.27
Consider the heat equation in a periodic domain. That is, we need to ﬁnd
u : R × [0, T] →R that is spatially one-periodic, i.e., for all t ∈[0, T] and any
x ∈R, it satisﬁes
u(x + n, t) = u(x, t),
∀n ∈Z,
and is a classical solution of



∂u(x, t)
∂t
= ∂2u(x, t)
∂x2
,
(x, t) ∈R × (0, T],
u(x, 0) = u0(x),
x ∈R,
where u0 ∈C∞
p (0, 1). Which approximation, from the ones given below, is
preferable?
a)
Leapfrog type:
1
2τ
 w k+1 −w k−1
= ∆hw k.
b)
Skew stencil type:
1
wτ

w k+1
j
−w k−1
j

= 1
h2

w k
j−1 −

w k+1
j
+ w k−1
j

+ w k
j+1

.
Justify your choice.

Listings
807
28.28
To approximate the solution to (23.24), we consider the following family
of methods: Let θ ∈[0, 1] and
¯δτw k+1 = ∆h
 θw k+1 + (1 −θ)w k
.
a)
Find the symbol of the method.
b)
Find a condition on θ that makes this method unconditionally stable.
c)
Is the method convergent?
28.29
Given T > 0, consider the following one-dimensional convection–diﬀusion
equation with periodic boundary conditions: Find u : R × [0, T], a one-periodic
function that satisﬁes



∂u(x, t)
∂t
+ b∂u(x, t)
∂x
−∂2u(x, t)
∂x2
= 0,
(x, t) ∈R × (0, T],
u(x, 0) = u0,
x ∈R.
Here, b > 0 and u0 ∈C∞
p (0, 1) is one-periodic. Consider the explicit upwind
method:
¯δτw k+1 + b¯δhw k −∆hw k = 0
with periodic boundary conditions and suitable initial conditions. Use a periodic
variant of the von Neumann stability analysis to study the stability, in the L∞
τ (L2
h)-
norm, of this method.
28.30
Prove identity (28.16).
28.31
Show that the method (28.13), with solution w, is accurate of order p ∈N
if and only if there are constants C3, C4 > 0 such that, whenever
h ≤C3,
we have
∥πhu(·, τ) −w 1∥L∞
h ≤C4τhp.
28.32
Show that, with the notation of Theorem 28.37, if u0 ∈H1(R), then there
is a constant, independent of h, for which
∥W 0
h −u0∥L2(R) ≤Ch∥u′
0∥L2(R).
Listings
1
function [error] = DiffusionCrankNic(finalT, N, K, numPlots)
2
%
3
% This function computes numerical approximations to solutions
4
% of the linear diffusion equation
5
%
6
% u t - u xx = f(x,t)
7
%
8
% using the Crank-Nicolson (CN) method on the domain [0,1]. The
9
% forcing function, f, and the intial conditions are constructed

808
Finite Diﬀerence Methods for Parabolic Problems
10
% so that the true solution is
11
%
12
% u(x,t) = cos(2*pi*t)*(exp(sin(2*pi*x))-1.0).
13
%
14
% This can be easily modified.
15
%
16
% Input
17
%
finalT : the final time
18
%
N : the number of spatial grid points in [0,1]
19
%
K : the number of time steps in the interval [0,finalT]
20
%
numPlots : the number of output frames in the time interval
21
%
[0,finalT]. numPlots must be a divisor of K.
22
%
23
% Output
24
%
error: the max norm error of the CN scheme at t = finalT
25
%
26
error = 0.0;
27
28
if N > 0 && N-floor(N) == 0
29
h = 1.0/N;
30
else
31
display('Error: N must be a positive integer.')
32
return
33
end
34
35
if (K > 0 && K-floor(K) == 0) && finalT > 0
36
tau = finalT/K;
37
else
38
display('Error: K must be a positive integer, and')
39
display('
finalT must be positive.')
40
return
41
end
42
43
mu = tau/(2.0*h*h);
44
45
x = 0:h:1.0;
46
uoCN(1:N+1) = uExact(N,0.0);
47
48
if mod(K,numPlots) == 0
49
stepsPerPlot = K/numPlots;
50
else
51
display('Error: numPlots is not a divisor of K.')
52
return
53
end
54
%
55
% Define the tridiagonal system matrix to be inverted:
56
%
57
a(
1) = 0.0;
58
a(2:N-1) = -mu;
59
b(1:N-1) = 1.0+2.0*mu;
60
c(1:N-2) = -mu;
61
c(
N-1) = 0.0;
62
%
63
% Main time loop:
64
%
65
for k = 1: numPlots

Listings
809
66
for j = 1: stepsPerPlot
67
kk = (k-1)*stepsPerPlot+j;
68
currTime = tau*(kk);
69
lastTime = tau*(kk-1.0);
70
f(1:N-1) = tau*(fForcing(N,currTime) ...
71
+fForcing(N,lastTime))/2.0;
72
for ell = 1: N-1
73
rhs(ell) = f(ell)+uoCN(ell+1)+mu*(uoCN(ell) ...
74
-2.0*uoCN(ell+1)+uoCN(ell+2));
75
end
76
[uCN(2:N),err] = TriDiagonal(a, b, c, rhs);
77
uCN(
1) = 0.0;
78
uCN(N+1) = 0.0;
79
uoCN = uCN;
80
end
81
hf
= figure(k);
82
clf
83
plot(x,uExact(N,currTime),'k-',x,uCN,'b-o')
84
grid on,
85
xlabel("x");
86
ylabel('exact and approximate solutions');
87
title(['Crank--Nicolson Approximation at T = ', ...
88
num2str(currTime), ', h = ', num2str(h), ...
89
', and tau =', num2str(tau)]);
90
legend("Exact","Crank--Nicolson")
91
set(gca,"xTick",0:0.1:1)
92
93
s1 = ['000', num2str(k)];
94
s2 = s1((length(s1)-3):length(s1));
95
s3 = ['OUT/diff', s2, '.pdf'];
96
exportgraphics(gca, s3)
97
end
98
99
error = max(abs(uExact(N,currTime)-uCN));
100
end
101
102
function frc = fForcing(N,t)
103
%
104
% Constructs the forcing term:
105
%
106
% Input
107
%
N : to compute the mesh size
108
%
t : the current time at the time interval midpoint
109
%
110
% Output
111
%
f(1:N-1) : the array of values of the forcing function
112
%
113
frc = zeros(1,N-1);
114
h = 1.0/N;
115
for i = 1: N-1
116
x = i*h;
117
tpx = 2.0*pi*x;
118
tpt = 2.0*pi*t;
119
frc(i) = 2.0*pi*sin(tpt)*(1.0-exp(sin(tpx))) ...
120
+ 4.0*pi*pi*cos(tpt)*exp(sin(tpx)) ...
121
* (sin(tpx)-cos(tpx)*cos(tpx));

810
Finite Diﬀerence Methods for Parabolic Problems
122
end
123
end
124
125
function ue = uExact(N,t)
126
%
127
% Constructs the exact solution:
128
%
129
% Input
130
%
N : to compute the mesh size
131
%
t : the current time
132
%
133
% Output
134
%
ue(1:N+1) : the array of values of the exact solution
135
%
136
ue = zeros(1,N+1);
137
h = 1.0/N;
138
for i = 0: N
139
x = i*h;
140
tpx = 2.0*pi*x;
141
tpt = 2.0*pi*t;
142
ue(i+1) = cos(tpt)*(exp(sin(tpx))-1.0);
143
end
144
end
Listing 28.1 Approximation of the diﬀusion equation by the Crank–Nicolson
method.

29
Finite Diﬀerence Methods for
Hyperbolic Problems
In this chapter, we will present fully discrete ﬁnite diﬀerence methods for hyperbolic
problems. We will focus on the Cauchy, or initial value, problem for the transport
equation; and the initial, and initial boundary, value problem for the wave equation.
Most of the techniques and ideas behind the construction and analysis of these
methods have been already presented in Chapters 24 and 28. There is, however,
one important feature that must be taken into account here. Namely, since, as
observed in Remarks 23.31 and 23.36, the solution of hyperbolic problems has
a very speciﬁc domain of dependence, this must be respected by the numerical
method. This requirement will often manifest itself in two aspects, as follows.
• The spatial grid size h and time step size τ will have to be related in a certain way.
This, often, will come in the form of a restriction on the hyperbolic1 Courant–
Friedrichs–Lewy (CFL) number
µ = τ
h ≤µ0
for some µ0 > 0 that depends on the problem parameters.
• The solution of a numerical method always reduces to the solution of a linear
system of equations
Aw k+1 = Bw k + f k+1.
In the case of parabolic equations, the matrices A and B were often symmetric, or
at least normal, so that much about the stability of the method could be inferred
from the spectrum of A and B. This will not be the case for hyperbolic problems.
For this reason, looking at the eigenvalues of matrices is often of little use, and
other arguments, like energy or von Neumann stability, must be employed.
The following example illustrates how things can go catastrophically wrong.
Example 29.1
Let us illustrate the aforementioned points with a simple example.
Let us consider the transport equation (23.39) with c = 1, f ≡0, and a downwind
discretization, i.e., we will seek w ∈V(Zh) such that w 0 = πhu0 and, for k =
0, . . . , K −1,
δτw k+1 + δhw k = 0.
1 Named in honor of the German-American mathematicians Richard Courant (1888–1972) and
Kurt Otto Friedrichs (1901–1982), and the Jewish, German-born, American mathematician
Hans Lewy (1904–1988).

812
Finite Diﬀerence Methods for Hyperbolic Problems
In other words, for i ∈Z,
w k+1
i
= w k
i −µ
 w k
i+1 −w k
i

= (1 + µ)w k
i −µw k
i+1,
where µ = τ
h . We see, ﬁrst, that the stencil of the method is not symmetric. More
importantly, this method can be analyzed with the techniques presented in Section
28.7.3. Its symbol is, see Problem 29.1,
˜Eτ
h (ξ) = (1 + µ) −µe−iξ;
therefore, if ξ ̸= 0,
| ˜Eτ
h (ξ)|2 = [1 + µ(1 −cos ξ)]2 + µ2 sin2 ξ > 1,
meaning that this method is unconditionally unstable, i.e., no matter what the
value of µ is, the von Neumann stability condition will not be satisﬁed. This is due
to the fact that, to advance in time, we are gathering information from outside the
domain of dependence of the solution; see Figure 23.2.
As a last comment before we begin, we mention that we will not cover a very
important topic in the approximation of hyperbolic problems: the approximation
of hyperbolic systems of conservation laws, as this is far beyond the scope of our
elementary discussion. For this topic, we refer the reader to [59, 43, 33, 53].
29.1
The Initial Value Problem for the Transport Equation
In this section, we will construct and analyze ﬁnite diﬀerence methods for the initial
value problem (23.39). We will operate under the notation of Section 28.7.
Let us introduce some methods.
Deﬁnition 29.1 (fully discrete methods). Let T > 0, K, N ∈N, h = 1/(N + 1),
and τ = T/K. Suppose that u ∈C1(R×[0, T]) is a classical solution to the Cauchy
problem for the transport equation (23.39), with c > 0 and f ≡0. Suppose that
u0,h ∈V(Zh) is deﬁned as usual. We deﬁne the following methods for computing
the ﬁnite diﬀerence approximation w ∈V([0, T]τ; V(Zh)) to u.
1. The upwind method
(
δτw k+1 + c¯δhw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(29.1)
2. The centered diﬀerence method
(
δτw k+1 + c˚δhw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(29.2)

29.1 The Initial Value Problem for the Transport Equation
813
3. The Lax–Friedrichs method2





1
τ

w k+1 −1
2(S1w k + S−1w k)

+ c˚δhw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h,
(29.3)
where S±1 are the shift operators given in Deﬁnition 24.5.
4. The Lax–Wendroﬀmethod3



δτw k+1 + c˚δhw k −c2τ
2 ∆hw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(29.4)
5. The Beam–Warming method4



δτw k+1 + c BDFh w k −c2τ
2 ∆hw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h,
(29.5)
where the operator BDFh was introduced in Proposition 24.20.
6. The Crank–Nicolson5 method



δτw k+1 + c
2
˚δh
 w k+1 + w k
= 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(29.6)
Remark 29.2 (explicit methods). We observe that all of the methods above, with
the exception of the last, are explicit methods. They can always be solved. The
Crank–Nicolson method is, however, implicit; its solvability property is nontrivial.
Let us now show the consistency and stability of some of these methods.
Theorem 29.3 (upwind method). The upwind method (29.1) is stable in the
L∞
τ (L2
h)-norm provided that the CFL condition
0 < cµ ≤µ0 = 1.
Moreover, if u is a suﬃciently smooth classical solution to (23.39), then the
consistency error satisﬁes the estimate
|Eτ
h [u]k
i | ≤C (τ + h) ,
i ∈Z,
k = 0, . . . , K,
for some C > 0 that is independent of τ and h.
2 Named in honor of the Hungarian-born American mathematician Peter David Lax (1926–)
and the German–American mathematician Kurt Otto Friedrichs (1901–1982).
3 Named in honor of the Hungarian-born American mathematician Peter David Lax (1926–),
the German-American mathematician Kurt Otto Friedrichs (1901–1982), and the American
mathematician Burton Wendroﬀ(1930–).
4 Named in honor of the American mathematicians Richard M. Beam (1935–) and R.F.
Warming (1931–).
5 Named in honor of the British mathematical physicist John Crank (1916–2006) and the
British mathematician and physicist Phyllis Nicolson (1917–1968).

814
Finite Diﬀerence Methods for Hyperbolic Problems
Proof. To show stability, we verify the von Neumann stability condition (28.15). It
is not diﬃcult to see that the symbol of this method is
˜Eτ
h (ξ) = 1 −cµ + cµe−iξ,
µ = τ
h .
Thus, if 0 < cµ ≤1,
| ˜Eτ
h (ξ)| =
1 −cµ + cµe−iξ ≤|1 −cµ| + cµ
e−iξh = 1 −cµ + cµ = 1.
Therefore, by Theorem 28.34, the method is L∞
τ (L2
h)-stable provided that the CFL
condition holds.
We leave the consistency to the reader as an exercise; see Problem 29.2.
The upwind method is only ﬁrst order in time and space. One might want to
increase the order of approximation in space by using, for example, the centered
diﬀerence approximation (29.2). Similarly to Example 29.1, this turns out to be a
bad idea.
Theorem 29.4 (centered method). The centered diﬀerence approximation method
(29.2) is never L∞
τ (L2
h)-stable, regardless of the value of µ.
Proof. The symbol of this method is
˜Eτ
h (ξ) = 1 −cµ
2
 eiξ −e−iξ
= 1 −icµ sin(ξ).
Consequently,
| ˜Eτ
h (ξ)|2 = 1 + c2µ2 sin2(ξ) ≥1.
On the other hand, we have the following result.
Theorem 29.5 (Crank–Nicolson method). The Crank–Nicolson method (29.6) is
unconditionally L∞
τ (L2
h)-stable. Furthermore, if u is a suﬃciently smooth classical
solution to (23.39), the consistency error satisﬁes
|Eτ
h [u]k
i | ≤C
 τ2 + h2
,
i ∈Z,
k = 0, . . . , K,
for some C > 0 that is independent of τ and h.
Proof. See Problem 29.3. Notice that, in this proof, one discovers the curious
property that
| ˜Eτ
h (ξ)| = 1.
This has an interesting implication.
Theorem 29.6 (Lax–Friedrichs method). The Lax–Friedrichs method (29.3) is
stable in the L∞
τ (L2
h)-norm provided that the CFL condition
0 < cµ ≤1
holds. Furthermore, if u is a suﬃciently smooth classical solution to (23.39), then
consistency error satisﬁes
|Eτ
h [u]k
i | ≤C(τ + h) ,
i ∈Z,
k = 0, . . . , K

29.1 The Initial Value Problem for the Transport Equation
815
for some C > 0, independent of τ and h, provided that τ = αh for some constant
α > 0.
Proof. The symbol of the method is
˜Eτ
h (ξ) = cos(ξ) −cµi sin(ξ).
We leave the rest of the details to the reader as an exercise; see Problem 29.4.
Remark 29.7 (numerical diﬀusion). Let us give some interpretations of the reason
behind the conditional stability of the upwind and Lax–Friedrichs methods. We ﬁrst
observe that the upwind method can be expressed as
δτw k+1 + c ˚δhw k −ch
2 ∆hw k = 0.
This looks like an explicit approximation of the advection–diﬀusion equation
∂u
∂t + c ∂u
∂x −ch
2
∂2u
∂x2 = 0,
(29.7)
where the diﬀusion is O(h), i.e., of order h. This small artiﬁcial diﬀusion is called
numerical diﬀusion. We can see that added numerical diﬀusion can stabilize the
centered diﬀerence method. Indeed, similar to Theorem 28.22, we can show that
this explicit method will be stable provided that the mesh P´eclet number satisﬁes
ha
ch
2
≤2,
which is always true, and the parabolic Courant number satisﬁes
ch
2
τ
h2 ≤1
2
⇐⇒
c τ
h ≤1,
which is the stability condition we obtained in Theorem 29.3.
Similarly, the Lax–Friedrichs method can be written as
δτw k+1 + c˚δhw k −h2
2τ ∆hw k = 0,
which is an explicit discretization of the advection–diﬀusion equation
∂u
∂t + c ∂u
∂x −h2
2τ
∂2u
∂x2 = 0.
(29.8)
Here, the numerical diﬀusion is O( h2
2τ ). If we recall that, for consistency, the Lax–
Friedrichs method requires that τ = αh, then we obtain
∂u
∂t + c ∂u
∂x −h
2α
∂2u
∂x2 = 0
with numerical diﬀusion of O(h). We ﬁnally comment that (29.7) and (29.8) are
known, in the literature, as the modiﬁed equations of the upwind and Lax–Friedrichs
methods, respectively.

816
Finite Diﬀerence Methods for Hyperbolic Problems
Theorem 29.8 (Lax–Wendroﬀ). The Lax–Wendroﬀmethod (29.4) is L∞
τ (L2
h)-
stable provided that the condition
0 < cµ ≤1
is satisﬁed. Furthermore, if u is a suﬃciently smooth classical solution to (23.39),
then, with certain restrictions on the time and space step size, the consistency
error satisﬁes the estimate
|Eτ
h [u]k
i | ≤C
 τ2 + h2
,
i ∈Z,
k = 0, . . . , K
for some C > 0 that is independent of τ and h.
Proof. The symbol of this method is
˜Eτ
h (ξ) = 1 + c2µ2 (cos(ξ) −1) −cµi sin(ξ).
We leave the rest of the details to the reader as an exercise; see Problem 29.5.
29.2
Positivity and Max-Norm Dissipativity
In Section 23.4.1, we showed that the solution to (23.39) is given by (23.40).
From this formula, it is evident that any pointwise bound that is valid for the initial
condition will also be valid for the solution at any positive time. For instance, if the
initial condition is positive, so will be the exact solution for all positive times.
In this section, we investigate which conditions can guarantee that a numerical
method can preserve this property. Let us begin with the upwind method.
Theorem 29.9 (positivity). Consider the upwind approximation method (29.1)
to approximate the solution to (23.39) with positive velocity c > 0. If the CFL
condition
0 < cµ ≤µ0 = 1
holds, then this method will be L∞
τ (L∞
h )-stable. Moreover, under the same
condition, if w k ≥0, for some k ∈{0, . . . , K −1}, then w k+1 ≥0.
Proof. We can write explicitly the method as
w k+1
j
= w k
j −cµ
 w k
j −w k
j−1

,
j ∈Z,
k = 0, . . . , K −1.
Therefore, if cµ ∈(0, 1],
|w k+1
j
| =
(1 −cµ)w k
j + cµw k
j−1

≤(1 −cµ)|w k
j | + cµ|w k
j−1|
≤(1 −cµ)
w k
L∞
h + cµ
w k
L∞
h
=
w k
L∞
h ,
which implies that
w k+1
L∞
h ≤
w k
L∞
h ,
and this implies the L∞
τ (L∞
h )-stability.

29.2 Positivity and Max-Norm Dissipativity
817
Suppose now that w k
j ≥0 for all j ∈Z. Since cµ and 1 −cµ are positive and
w k+1
j
= (1 −cµ)w k
j + cµw k
j−1,
it is clear that w k+1
j
≥0 for all j ∈Z.
We can generalize this result to more general methods, but ﬁrst we need to
introduce some notions.
Deﬁnition 29.10 (positivity). Consider a general two-layer explicit method in the
sense of Deﬁnition 28.30. We say that this method reproduces the constant
state if, whenever w k ≡1, we obtain that w k+1 ≡1. The method is max-norm
dissipative if and only if, for every k = 0, . . . , K −1, it satisﬁes
∥w k+1∥L∞
h ≤∥w k∥L∞
h .
It is positivity preserving if and only if whenever w k
j
≥0, for all j ∈Z, then
w k+1
j
≥0 for all j ∈Z.
The following result gives a general criterion on the coeﬃcients of an explicit
ﬁnite diﬀerence method to be positivity preserving.
Theorem 29.11 (positivity). Consider
w k+1
j
=
X
m∈SB
bm(h, τ)w k
j−m,
which is an explicit two-layer ﬁnite diﬀerence method in the sense of Deﬁnition
28.30.
1. If the method reproduces the constant state, then
X
m∈SB
bm(h, τ) = 1.
2. If the method reproduces the constant state and is max-norm dissipative, then
the method is positivity preserving.
3. If the method reproduces the constant state and is max-norm dissipative, then
bm(h, τ) ≥0 for all m ∈SB.
Proof. See Problem 29.8.
The following result, which bears the name Godunov’s Theorem, limits the con-
sistency order of positivity-preserving methods. For a more detailed presentation,
we refer the reader to [100, Chapter 9].
Theorem 29.12 (Godunov6). Consider
w k+1
j
=
X
m∈SB
bm(h, τ)w k
j−m,
6 Named in honor of the Soviet and Russian mathematician Sergei Konstantinovich Godunov
(1929–).

818
Finite Diﬀerence Methods for Hyperbolic Problems
which is an explicit two-layer ﬁnite diﬀerence method in the sense of Deﬁnition
28.30. If the method reproduces the constant state, it is max-norm dissipative, and
#SB ≥2, then the method is consistent to at most order one.
Proof. (sketch) From the form of the method, we know that its symbol is
˜Eτ
h (ξ) =
X
m∈SB
bm(h, τ)e−imξ.
Therefore, as h →0,
| ˜Eτ
h (h)|2 =
X
m∈SB
X
n∈SB
bm(h, τ)bn(h, τ)e−i(m−n)h
=
X
m,n∈SB
bm(h, τ)bn(h, τ)

1 −i(m −n)h −(m −n)2h2
2
+ O(h3)

=
X
m∈SB
bm(h, τ)
X
n∈SB
bn(h, τ) −ih
X
m,n∈SB
bm(h, τ)bn(h, τ)(m −n)
−h2
2
X
m,n∈SB
bm(h, τ)bn(h, τ)(m −n)2 + O(h3).
From Theorem 29.11, we know that bm(h, τ) ≥0, for all m ∈SB, and
X
m∈SB
bm(h, τ) = 1.
This, in turn, implies that
X
m∈SB
bm(h, τ)
X
n∈SB
bn(h, τ) = 1,
X
m,n∈SB
bm(h, τ)bn(h, τ)(m −n) = 0,
X
m,n∈SB
bm(h, τ)bn(h, τ)(m −n)2 > 0.
In conclusion, as h →0,
| ˜Eτ
h (h)|2 ≥1 −Ch2
for some C > 0.
Consider now, as an initial condition for (23.39), with f ≡0, the function
u0(x) = eix. The consistency error then, after one step, is
Eτ
h [u]1
j = ei(jh−cτ) −
X
m∈SB
bm(h, τ)ei(j−m)h
=
 e−icτ −˜Eτ
h (h)

eijh
=
 e−icµh −˜Eτ
h (h)

eijh.
Therefore,
|Eτ
h [u]1
j | =
e−icµh −˜Eτ
h (h)
 ≥C′h,
and the method cannot have second-order consistency.

29.3 The Transport Equation in a Periodic Spatial Domain
819
29.3
The Transport Equation in a Periodic Spatial Domain
We end our discussion of the transport equation with some stability and conver-
gence results for the upwind method in the spatially periodic case. Thus, we consider
(23.39) with c > 0, f ≡0, and u0 ∈C∞
p (0, 1). In this case, the representation
formula (23.40) shows then that the solution u will be one-periodic as well.
Theorem 29.13 (upwind method). Let u be the smooth one-periodic solution
to (23.39) with c
> 0, f
≡0, and u0 ∈C∞
p (0, 1). Suppose that w
∈
V([0, T]τ; VN+1,p) is the approximation to u computed by the upwind method
(29.1). Then, under the CFL condition, cµ ≤1, the method is L∞
τ (Lp
h)-stable
for p ∈{2, ∞}, i.e.,
w k+1
Lp
h ≤
w k
Lp
h ,
k = 0, . . . , K −1.
Furthermore, under the same CFL condition, the method is convergent in the
L∞
τ (Lp
h)-norm (p ∈{2, ∞}) with rate
τ + h.
Proof. We will prove the convergence for p = 2 and leave the other parts
to the reader as an exercise; see Problem 29.11. We introduce the error e ∈
V([0, T]τ; VN+1,p) and observe that it satisﬁes e0 = 0 and
δτek+1 + c¯δhek = Eτ
h [u]k+1,
k = 0, . . . , K −1,
where the consistency error is such that
∥Eτ
h [u]k+1∥L∞
h ≤C(τ + h)
for some C > 0 that is independent of τ and h. By Theorem 26.12, the set of
vectors
S =

v m ∈CNm = 1, . . . , N + 1
	
,
[v m]n = vm,n = e2πimnh,
n = 1, . . . , N+1
is orthornormal, in the sense that
h(v m, v n)2 = hv H
mv n = δm,n.
The error equation may be expressed in matrix–vector form as
ek+1 = Aek + τEEEτ
h[u]k+1,
where
A =


1 −cµ
0
0
· · ·
cµ
cµ
1 −cµ
...
...
0
...
...
0
0
...
cµ
1 −cµ
0
0
· · ·
0
cµ
1 −cµ


.
(29.9)
S is an orthonormal set of eigenvectors for A. In particular,
Av m = λmv m,
λm = 1 −cµ + cµe−2πimh.

820
Finite Diﬀerence Methods for Hyperbolic Problems
Note that the eigenvalues are complex and the matrix A is not symmetric. The
computation of the 2-norm of this matrix requires some extra work. In particular,
recall that
∥A∥2 = max
√ν
ν ∈σ(AHA)
	
.
It is a short exercise to show that, in our case, ν ∈σ(AHA) if and only if √ν = |λ|
with λ ∈σ(A). Therefore,
∥A∥2 = max {|λ| |λ ∈σ(A)} .
Using the CFL condition, observe that, for all λ ∈σ(A),
|λ| ≤|1 −cµ| + |cµ| = 1 −cµ + cµ = 1.
Therefore,
ek+1
2 ≤∥A∥2
ek
2 + τ
EEEτ
h[u]k+1
2 ≤
ek
2 + τ
√
NC(τ + h).
Equivalently,
ek+1
L2
h ≤
ek
L2
h + Cτ(τ + h).
Therefore, for any 0 ≤k ≤K,
ek
L2
h ≤
e0
L2
h + CτK(τ + h) = CT(τ + h).
Next, let us analyze the same problem, but using an energy-type argument.
Theorem 29.14 (upwind). Let u be the smooth one-periodic solution to (23.39)
with c > 0, f ≡0, and u0 ∈C∞
p (0, 1). Suppose that w ∈V([0, T]τ; VN+1,p) is the
approximation to u computed by the upwind method (29.1). Then, under the CFL
condition, cµ ≤1, the method is L∞
τ (L2
h)-stable.
Proof. As mentioned in Remark 29.7, the upwind method can be rewritten as
δτw k+1 + c ˚δhw k −ch
2 ∆hw k = 0.
Taking the L2
h-inner product of the method with 2τw k and using summation by
parts, the identity
(˚δhv1, v2)L2
h = −(v1,˚δhv2)L2
h,
∀v1, v2 ∈VN+1,p,
and the polarization identity
2(a −b)b = a2 −(a −b)2 −b2
yield
w k+12
L2
h −
w k2
L2
h −
w k+1 −w k2
L2
h + chτ∥¯δhw k∥2
L2
h = 0.
Observe now that, using the method,
chτ∥¯δhw k∥2
L2
h = chτ

δτw k+1
c

2
L2
h
= 1
cµ
w k+1 −w k2
L2
h .

29.3 The Transport Equation in a Periodic Spatial Domain
821
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
exact and approximate solutions
Finite Diﬀerence Approximations at T = 0.25, h = 0.02, and τ =0.01
exact
upwind
Lax–Wendroﬀ
Lax–Friedrichs
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
exact and approximate solutions
Finite Diﬀerence Approximations at T = 0.5, h = 0.02, and τ =0.01
exact
upwind
Lax–Wendroﬀ
Lax–Friedrichs
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
exact and approximate solutions
Finite Diﬀerence Approximations at T = 0.75, h = 0.02, and τ =0.01
exact
upwind
Lax–Wendroﬀ
Lax–Friedrichs
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
exact and approximate solutions
Finite Diﬀerence Approximations at T = 1, h = 0.02, and τ =0.01
exact
upwind
Lax–Wendroﬀ
Lax–Friedrichs
Figure 29.1 Upwind, Lax–Wendroﬀ, and Lax–Friedrichs approximations of the solution
to a periodic advection problem at times T = 0.25, 0.50, 0.75, 1.00, with N = 50,
h = 0.02, K = 100, and τ = 0.01; see Example 29.2 and Listing 29.1.
Rearranging terms, we have
cµ
w k+12
L2
h + (1 −cµ)
w k+1 −w k2
L2
h = cµ
w k2
L2
h .
Clearly, if the CFL condition µ ≤1 holds,
w k+12
L2
h ≤
w k2
L2
h ,
from which the claimed L∞
τ (L2
h)-stability immediately follows.
Example 29.2
Let u be the solution to (23.39) with c = 1, f ≡0, and u0 being
smooth and one-periodic. In fact, it is a Gaussian. Listing 29.1 presents a code that
approximates u using the upwind, Lax–Wendroﬀ, and Lax–Friedrich methods. The
output at times T = 0.25, 0.50, 0.75, and 1.00 are displayed in Figure 29.1. The
Lax–Wendroﬀmethod seems to give the best approximation. But, curiously, this
method is not positivity preserving; see Problem 29.10.

822
Finite Diﬀerence Methods for Hyperbolic Problems
29.4
Dispersion Relations
In this section, we present some properties of second-order methods. To motivate
our discussion, we recall that the solution to (23.39) is given by (23.40). Under
suitable assumptions on the initial condition, say u0 ∈L2(R), we can compute its
Fourier transform ˆu0, so that, in the Fourier domain, (23.40) is given by
ˆu(ξ, t) = e−icξt ˆu0(ξ),
where we used the shift property of Proposition 23.16. This shows that, for any
m ∈{0, . . . , K −k},
ˆu(ξ, tk+m) = e−icξmτ ˆu(ξ, tk).
Consider now a general two-layer ﬁnite diﬀerence method, in the sense of
Deﬁnition 28.30, with symbol ˜Eτ
h . By deﬁnition of the symbol, we have that
ˆw k+1(ξ) = ˜Eτ
h (ξ) ˆw k(ξ)
=⇒
ˆw k+m(ξ) =
 ˜Eτ
h (ξ)
m ˆw k(ξ).
Thus, we expect the symbol ˜Eτ
h (ξ) to approximate e−icξτ.
To further explore this relation, we write the symbol as
˜Eτ
h (ξ) = | ˜Eτ
h (ξ)|e−iω(ξ)ξτ,
where the quantity ω(ξ) is called the phase speed of the method. We recall that,
according to Theorem 28.34, we must have | ˜Eτ
h (ξ)| ≤1 for L∞
τ (L2
h)-stability. Thus,
at least for the sake of the current illustration, we will assume here that | ˜Eτ
h (ξ)| = 1.
Assume now that the phase speed ω(ξ) is equal to c. Then, for any initial
condition of the form ˆu0(ξ) = e−iaξ, with a ∈R, we would have
ˆu(ξ, tk) = e−i(a+ctk)ξ
and
ˆw k(ξ) = e−i(a+ctk)ξ,
which is a correct description of the evolution of the solution. However, in general,
ω(ξ) is only an approximation of c. We then have
ˆu(ξ, tk) −ˆw k(ξ) = e−i(a+ctk)ξ 
1 −e−i(ω(ξ)−c)tkξ
.
The error then is characterized by the quantity ω(ξ) −c, which is known as the
phase error of a method.
The eﬀect of the phase error, in practice, is manifested by a distortion of the
shape of the solution. This is illustrated in Figure 29.2, which was obtained with
the code presented in Listing 29.1 and initial condition of the form
u0(x) =





1,
x ≤1
2,
0,
x > 1
2.
We observe that oscillations appear before and after the square wave. This is due
to the phase error of the method.

29.5 The Initial Boundary Value Problem for the Wave Equation
823
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
T = 0.2, h = 0.01, and τ =0.005
exact
LW
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
T = 0.4, h = 0.01, and τ =0.005
exact
LW
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
T = 0.8, h = 0.01, and τ =0.005
exact
LW
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
0.2
0.4
0.6
0.8
1.0
x
T = 1, h = 0.01, and τ =0.005
exact
LW
Figure 29.2 Lax–Wendroﬀ(LW) approximation of the solution to a periodic advection
problem at times T = 0.2, 0.4, 0.8, 1.0, with N = 100, h = 0.01, K = 200, and
τ = 0.005. The initial condition is discontinuous. The results were produced with Listing
29.1.
We refer the reader to [88, 59] for a more detailed discussion on dispersion,
group velocities, and dispersion relations for ﬁnite diﬀerence methods.
29.5
The Initial Boundary Value Problem for the Wave Equation
Let us now study a numerical method for the approximation of the initial boundary
value problem for the wave equation (23.45) for d = 1. Thus, we let Ω= (0, 1)
and, as usual, we will seek a space–time grid function w ∈V0( ¯Cτ
h ) that is deﬁned
by a ﬁnite diﬀerence method that is consistent to order τ2 + h2. The ﬁrst issue we
encounter is the approximation of the initial conditions. While the approximation
w 0 = πhu0 is clear, the approximation of the initial velocity requires some work.
Namely, the easiest and obvious choice,
δτw 0 = πhv0,
will yield a numerical method that is only consistent to order τ + h2, due to the
fact that we are using a forward diﬀerence. Instead, as illustrated in Problem 24.19,

824
Finite Diﬀerence Methods for Hyperbolic Problems
we will use the diﬀerential equation to achieve second-order consistency (in time).
Namely, assuming that the equation holds up to t = 0, i.e.,
∂2u(x, 0)
∂t2
−a∂2u(x, 0)
∂x2
= f (x, 0),
we can obtain
1
τ (u(x, τ) −u(x, 0)) = ∂u(x, 0)
∂t
+ τ
2
∂2u(x, 0)
∂t2
+ O(τ2)
= ∂u(x, 0)
∂t
+ τ
2

a∂2u(x, 0)
∂x2
+ f (x, 0)

+ O(τ2)
= v0(x) + τ
2 (au′′
0(x) + f (x, 0)) + O(τ2).
(29.10)
Thus, if we deﬁne
v0,h = πhv0 + τ
2 πh(au′′
0 + f (·, 0)),
(29.11)
we shall obtain that
δτw 0 = v0,h
(29.12)
is second-order consistent with the initial condition; see Problem 29.13.
Having suitable approximations of the initial conditions, we are ready to deﬁne
the method. We seek w ∈V0( ¯Cτ
h ) such that





¯δτδτw k −a∆hw k = f k
h ,
k = 1, . . . , K,
w 0 = u0,h,
δτw 0 = v0,h,
(29.13)
where fh = πτ
hf , u0,h = πhu0, and v0,h is given by (29.11).
The following result is an immediate consequence of our construction.
Proposition 29.15 (consistency). The method (29.13) is consistent, in C(R2),
with (23.45) up to order τ2 + h2.
Proof. See Problem 29.14.
Let us now analyze the stability of the method. Before we embark on the technical
details, we observe that this is an explicit method. In addition, we recall that the
solution to (23.45) has a very well-deﬁned domain of dependency, see Figure 23.3,
and that any numerical method that has a chance of success must respect this
feature. For this reason, we do not expect the method to be unconditionally stable.
The following results prove the conditional stability of (29.13) in the sense that
a discrete analogue of Theorem 23.39 holds. We will do so by means of energy
techniques. For a diﬀerent method of proof, we refer the reader to Problem 29.18.
Let us begin with the case fh ≡0.
Theorem 29.16 (conditional stability, fh ≡0). Let fh ≡0. If
√aCIτ
2h
≤1,

29.5 The Initial Boundary Value Problem for the Wave Equation
825
where CI is the constant from Problem 24.23, then the method (29.13) is stable
in the sense that the sequence {Ek}K
k=1 ⊂R, deﬁned by
Ek+1 =
¯δτw k+12
L2
h + a
4
¯δhw k+1 + ¯δhw k2
L2
h −aτ2
4
¯δτ ¯δhw k+12
L2
h ,
is independent of k, i.e.,
Ek+1 = Ek.
Proof. Take the L2
h-inner product of the equation that deﬁnes the method with
˚δτw k to obtain
 ¯δτδτw k,˚δτw k
L2
h + a
 ¯δhw k, ¯δh˚δτw k
L2
h = 0.
Simple algebraic manipulations allow us to show that, for any grid function v ∈
V( ¯Cτ
h ),
 ¯δτδτv k,˚δτv k
L2
h = 1
2τ
h¯δτv k+12
L2
h −
¯δτv k2
L2
h
i
(29.14)
and
v k˚δτv k = 1
8τ

(v k+1 + v k)2 −(v k + v k−1)2
−τ
8
h ¯δτv k+12 −
 ¯δτv k2i
;
(29.15)
see Problem 29.16.
From (29.15) with v = ¯δhw, we then see that
 ¯δhw k, ¯δh˚δτw k
L2
h = 1
8τ
h¯δhw k+1 + ¯δhw k2
L2
h −
¯δhw k + ¯δhw k−12
L2
h
i
−τ
8
h¯δτ ¯δhw k+12
L2
h −
¯δτ ¯δhw k2
L2
h
i
.
The previous considerations then show that Ek+1 = Ek for all k = 0, . . . , K −1.
It thus remains to make sure that Ek ≥0. From the inverse inequality of Problem
24.23, we have that
¯δh¯δτw k+1
L2
h ≤CIh−1 ¯δτw k+1
L2
h ,
so that
Ek+1 ≥
 h2
C2
I
−aτ2
4
 ¯δτ ¯δhw k+12
L2
h + a
4
¯δhw k+1 + ¯δhw k2
L2
h ,
which will be nonnegative provided that
√aCIτ
2h
≤1,
and the claimed stability estimate follows.
The inhomogeneous case, i.e., fh ̸= 0, requires a slightly more restrictive
condition. The proof of the following result illustrates a technique known as negative
norm arguments.

826
Finite Diﬀerence Methods for Hyperbolic Problems
Corollary 29.17 (conditional stability, fh ̸= 0). Assume that there is ε ∈(0, 1) for
which
√aCIτ
2h
≤
√
1 −ε,
where CI is the constant from Problem 24.23, then the method (29.13) is stable
in the sense that the sequence {Ek}K
k=1 ⊂R, deﬁned in Theorem 29.16, satisﬁes

Ek1/2 ≤

E01/2 + τ
k
X
m=1
∥f m
h ∥B−1
h
for all k = 0, . . . , K. In the previous expression, we used the energy norm
∥z∥2
B−1
h
=
 B−1
h z, z

L2
h ,
Bh = I + aτ2
4 ∆h.
Under the given assumptions, the operator B−1
h
is self-adjoint with respect to the
L2
h-inner product, positive deﬁnite, and the norm of its inverse is uniformly bounded
with respect to h.
Proof. Let us provide the main steps of the proof and leave the minute details to
the reader as an exercise; see Problem 29.19.
We begin by considering, for v ∈V0(¯Ωh), the expression
(v, v)L2
h −aτ2
4
 ¯δhv, ¯δhv

L2
h =

I + aτ2
4 ∆h

v, v

L2
h
= (Bhv, v)L2
h .
Clearly, the operator Bh is self-adjoint in the L2
h-inner product. Using, once again,
the inverse inequality of Problem 24.23, we see that
(Bhv, v)L2
h =

1 −aτ2C2
I
4h2

∥v∥2
L2
h ,
which, under the assumed restrictions on h and τ, shows that this operator is
positive deﬁnite and, moreover, that its inverse is uniformly bounded with respect
to h and τ in the L2
h-induced norm.
With the previous observations, we see that we can rewrite the quantity Ek+1,
deﬁned in Theorem 29.16, as
Ek+1 =
¯δτw k+12
Bh + a
4
¯δhw k+1 + ¯δhw k2
L2
h ,
where we introduced the Bh-energy norm, which we now know is indeed a norm.
Now, following the proof of Theorem 29.16, we obtain that
Ek+1 −Ek = 2τ
 f k
h ,˚δτw k
L2
h
= τ
 f k
h , ¯δτw k+1 + ¯δτw k
L2
h
≤τ∥f k
h ∥B−1
h
¯δτw k+1
Bh +
¯δτw k
L2
h

≤τ∥f k
h ∥B−1
h

Ek+11/2 +

Ek1/2
,

29.6 Finite Diﬀerence Methods for Hyperbolic Systems
827
where the last step makes sense as Theorem 29.16 shows that Ek ≥0. Now we
can divide the previously obtained inequality by

Ek+11/2 +

Ek1/2 and add over
k to obtain the claimed stability estimate.
29.6
Finite Diﬀerence Methods for Hyperbolic Systems
We now present some numerical methods for the initial value problem for a
symmetric hyperbolic system (23.46). First, we observe that, as illustrated in
Example 23.12, in one dimension the system can be diagonalized and reduced
to a system of decoupled transport equations. After this, all of the methods of
Section 29.1 can be applied.
Let us here instead present another approach, which does not involve diagonal-
ization and is applicable in more general situations. Our presentation will mostly
follow [58, Section 12.2].
Deﬁnition 29.18 (ﬁnite diﬀerence methods). Let d = 1, m ∈N, and u be the
solution of (23.46) with f ≡0. Suppose that u0,h ∈V(Zh; Rm) is deﬁned as usual.
We deﬁne the following methods for computing the ﬁnite diﬀerence approximation
w ∈V([0, T]τ; V(Zh; Rm))
of u.
1. Friedrich’s method





1
τ

w k+1 −1
2

S1w k + S−1w k
+ A˚δhw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h,
(29.16)
where S±1 are the shift operators introduced in Deﬁnition 24.5.
2. The Lax–Wendroﬀmethod



1
τ
 w k+1 −w k
+ A˚δhw k −τ
2 A2∆hw k = 0,
k = 0, . . . , K −1,
w 0 = u0,h.
(29.17)
The consistency of these methods can be obtained, as before, by an application
of Taylor expansions.
Proposition 29.19 (consistency). Assume that u is a suﬃciently smooth classical
solution to (23.46). Then the consistency error for the Friedrichs method (29.16)
satisﬁes
EEEτ
h[u]k
i

2 ≤C (τ + h),
i ∈Z,
k = 0, . . . , K.
Under the same assumption, the consistency error for the Lax–Wendroﬀmethod
satisﬁes
EEEτ
h[u]k
i

2 ≤C
 τ2 + h2
,
i ∈Z,
k = 0, . . . , K.
In both estimates, the constant C > 0 is independent of τ and h.

828
Finite Diﬀerence Methods for Hyperbolic Problems
Proof. See Problem 29.20.
Let us now study the stability of the methods. As expected from setting m = 1,
these methods cannot be unconditionally stable, as they are the vector-valued
version of methods for the transport equation. The approach that we will follow
is to extend the von Neumann stability approach of Section 28.7.3 to the vector-
valued setting. For simplicity, we conﬁne the discussion to explicit methods.
Deﬁnition 29.20 (explicit method). Let m ∈N and consider a matrix-valued
ﬁnite diﬀerence method Bh : V(Zh; Rm) →V ¯N(Zh; Rm), i.e.,
(Bhv)j =
X
m∈SB
Bm(h, τ)v j−m,
∀v ∈V(Zh; Rm),
(29.18)
where SB ⊂Z has ﬁnite cardinality and Bm : R+×R+ →Rm×m\{Om}. A two-layer,
explicit, matrix-valued, time stepping, ﬁnite diﬀerence method with constant
coeﬃcients is deﬁned as follows: Find w ∈V([0, T]τ; V(Zh; Rm)) such that w 0 is
given and
w k+1 = Bhw k,
k = 0, . . . , K −1.
Clearly, all the methods of Deﬁnition 29.18 ﬁt the previous deﬁnition.
Deﬁnition 29.21 (symbol). Consider a two-layer, explicit, matrix-valued, time
stepping, ﬁnite diﬀerence method in the sense of Deﬁnition 29.20. Its matrix-
valued symbol is, for all ξ ∈R, the matrix
˜Eτ
h(ξ) =
X
m∈SB
Bm(h, τ)e−imξ ∈Cm×m.
Example 29.3
The matrix-valued symbol of the Friedrichs method (29.16) is
˜Eτ
h(ξ) = Im cos ξ −µiA sin ξ;
see Problem 29.21.
Example 29.4
The matrix-valued symbol of the Friedrichs method (29.17) is
˜Eτ
h(ξ) = Im + µ2A2(cos ξ −1) −µiA sin ξ;
see Problem 29.22.
The following result is a vector-valued analogue of Theorem 28.34.
Theorem 29.22 (von Neumann). Let m ∈N and consider the explicit time stepping
method (29.18) with symbol ˜Eτ
h(ξ) ∈Cm×m. This method is stable in the L∞
τ (L2
h)-
norm if and only if there exists a constant C > 0 such that
˜Eτ
h(ξ)k
2 ≤C,
∀ξ ∈R,
k ∈N.
We recall that ∥·∥2 denotes the induced matrix 2-norm.

29.6 Finite Diﬀerence Methods for Hyperbolic Systems
829
Proof. See Problem 29.23.
We comment that, contrary to the scalar case, the condition of Theorem 29.22
does not imply that
˜Eτ
h(ξ)

2 ≤1,
(29.19)
as the following example shows.
Example 29.5
Consider
E =
"
1
2
1
0
1
2
#
.
Then
σ(EHE) =
3 ± 2
√
2
4

=⇒
∥E∥2 > 1.
Yet, for any k ∈N,
Ek =
"  1
2
k
k
  1
2
k−1
0
  1
2
k
#
=⇒
Ek
2 ≤C.
However, if the condition of Theorem 29.22 holds, then, for every λ(ξ) ∈
σ(˜Eτ
h(ξ)), we must have
|λ(ξ)|k ≤C
=⇒
|λ(ξ)| ≤1.
The condition
σ
 ˜Eτ
h(ξ)

⊂[−1, 1],
∀ξ ∈R
(29.20)
is known as the von Neumann stability condition for the methods of Deﬁnition
29.20. Clearly, this is only a necessary condition. A suﬃcient stability condition is,
obviously, (29.19). The following result shows when this condition is satisﬁed.
Lemma 29.23 (stability). Let m ∈N and consider a two-layer, explicit, matrix-
valued, ﬁnite diﬀerence method in the sense of Deﬁnition 29.20 with symbol
˜Eτ
h(ξ) =
X
m∈SB
Bm(h, τ)e−imξ ∈Cm×m.
Assume that, for all m ∈SB, we have that Bm(h, τ) = Bm(h, τ)⊺, they are positive
semi-deﬁnite, and
X
m∈SB
Bm(h, τ) = Im.
Then this method is stable in the L∞
τ (L2
h)-norm.

830
Finite Diﬀerence Methods for Hyperbolic Problems
Proof. We will prove stability by showing that (29.19) holds. Notice that, for all
v 1, v 2 ∈Cm, we have
 ˜Eτ
h(ξ)v 1, v 2

2
 ≤
X
m∈SB
|(Bm(h, τ)v 1, v 2)2|
≤1
2
X
m∈SB
|(Bm(h, τ)v 1, v 1)2| + 1
2
X
m∈SB
|(Bm(h, τ)v 2, v 2)2|
= 1
2 ∥v 1∥2
2 + 1
2 ∥v 2∥2
2 ,
where, in the last step, we used that the matrices Bm(h, τ) are positive semi-deﬁnite
and add up to the identity.
Set now, in the previous derivation, v 2 = ˜Eτ
h(ξ)v 1 while keeping v 1 arbitrary. We
obtain
˜Eτ
h(ξ)v 1
2
2 ≤1
2 ∥v 1∥2
2 + 1
2
˜Eτ
h(ξ)v 1
2
2 ,
and this clearly implies the result.
To conclude our discussion, let us provide suﬃcient conditions for the stability
of the Friedrichs method (29.16).
Corollary 29.24 (stability). Assume that m ∈N and consider the Friedrichs
method (29.16). If the discretization parameters h and τ are such that
τ
h ∥A∥2 ≤1,
then this method is stable in the L∞
τ (L2
h)-norm.
Proof. The symbol for the Friedrichs method is given in Example 29.3:
˜Eτ
h(ξ) = 1
2 (Im −µA)eiξ + 1
2 (Im + µA)e−iξ.
By assumption, A is symmetric, and thus so is Im ± µA. In addition, clearly,
1
2 (Im −µA) + 1
2 (Im + µA) = Im.
Finally, the condition on µ = τ
h guarantees that these matrices are positive semi-
deﬁnite. Stability then follows from Lemma 29.23.
Problems
29.1
Complete the details of Example 29.1.
29.2
Complete the proof of Theorem 29.3.
29.3
Prove Theorem 29.5. How does one solve this approximation method?
29.4
Complete the proof of Theorem 29.6.
29.5
Complete the proof of Theorem 29.8.
29.6
Prove that the Beam–Warming method (29.5) is L∞
τ (L2
h)-stable provided
that the CFL condition cµ ≤1 is satisﬁed.

Problems
831
29.7
Suppose that, in (23.39), we have c > 0 and consider the following skewed
leapfrog method:
w k+1
j
= w k−1
j−2 −
cτ
h −1

(w k
j −w k
j−2).
a)
What is the order of consistency of this method?
b)
Using the von Neumann stability analysis, for what values of cτ
h is the method
stable?
29.8
In this problem, we will provide a proof of Theorem 29.11.
a)
Show that if the method reproduces the constant state, then
X
m∈SB
bm(h, τ) = 1.
b)
Show that if a method reproduces the constant state and is max-norm
dissipative, then the method is positivity preserving.
Hint: Suppose that w k
j ≥0 and w k
m = ∥w k∥L∞
h = α ≥0 for some m ∈Z.
Deﬁne ηk ∈V(Zh) by
ηk
j = w k
j −α
2 ,
j ∈Z.
Apply the method to ηk and use the fact that, for all j ∈Z,
−α
2 ≤ηk+1
j
≤α
2
to conclude the result.
c)
Show that if a method reproduces the constant state and is max-norm
dissipative, then bm(h, τ) ≥0 for all m ∈SB.
Hint: Use the previous result.
d)
Assume that this method is used to approximate the solution to (23.39) with
c = 1 and u0(x) = eix. Find an expression for the error in the L2
h-norm after
one step that depends only on the coeﬃcients {bm(h, τ)}m∈SB, the mesh size
h > 0, and the Courant number µ = τ/h, where τ > 0 is the time step size.
29.9
Provide suﬃcient conditions for the Lax–Friedrichs to be positivity preserv-
ing.
29.10
Show that the Lax–Wendroﬀis not positivity preserving.
29.11
Complete the proof of Theorem 29.13.
29.12
Consider the initial value problem (23.39) for the transport equation with
periodic boundary conditions and c < 0. Use an energy stability analysis (not the
von Newmann analysis) to establish a suﬃcient condition for stability of methods
of the form
w k+1
j
= αw k
j−1 + βw k
j ,
j ∈Z,
k = 0, . . . , K −1,
where α, β ∈R. Apply your results to the simple method
w k+1
j
= w k
j −cτ
h (w k
j −w k
j−1),
j ∈Z,
k = 0, . . . , K −1.
Extend your results to the method
w k+1
j
= w k
j −cτ
h (w k
j+1 −w k
j ),
j ∈Z,
k = 0, . . . , K −1.

832
Finite Diﬀerence Methods for Hyperbolic Problems
29.13
Justify (29.10) and use this to show that (29.12) is consistent up to second
order with
∂u(x, 0)
∂t
= v0(x), x ∈(0, 1).
29.14
Prove Proposition 29.15.
29.15
Consider the following variant of (29.13). Let σ ∈R be given. Then we
seek w ∈V0( ¯Cτ
h ) such that





¯δτδτw k −a∆h
 σw k+1 + (1 −2σ)w k + σw k−1
= f k
h ,
k = 1, . . . , K,
w 0 = u0,h,
δτw 0 = v0,h,
(29.21)
where u0,h = πhu0 and v0,h is given by (29.11).
a)
Show that if fh = πτ
hf , then, for every σ ∈R, the method is consistent to
order
τ2 + h2.
b)
Show that if
fh = πτ
h

f + h2
12
∂2f
∂x2

and
σ ≥
1
4(1 −ε) −
h2
12aτ2 ,
where ε > 0, then the method is consistent to order
τ2 + h4.
29.16
Prove identities (29.14) and (29.15).
29.17
Prove the analogue of Theorem 29.16 for (29.21). In other words, show
that if fh ≡0 and
σ ≥1
4 −
h2
aCIτ2 ,
where CI is the constant of Problem 24.23, then the method (29.21) is stable.
29.18
Prove an analogue of Theorem 29.16 but using the separation of variables
technique described in Problem 28.9.
29.19
Provide all the details in the proof of Corollary 29.17.
29.20
Prove Proposition 29.19.
29.21
Provide all the details of Example 29.3.
29.22
Provide all the details of Example 29.4.
29.23
Prove Theorem 29.22.
Hint: Follow Theorem 28.34.

Listings
833
Listings
1
function [errorUW, errorLW, errorLF] = AdvectionPer(finalT, ...
2
N, K, numPlots, typeInit)
3
% This function computes numerical approximations to solutions
4
% of the linear advection equation
5
%
6
% u t + u x = 0
7
%
8
% using the upwind (UW), Lax-Wendroff (LW), and Lax-Friedrichs
9
% (LF) methods on the periodic domain [0,1]. The initial data is
10
% constructed from either a square or a triangular wave.
11
%
12
% Input
13
%
finalT : the final time
14
%
N : the number of spatial grid points in [0,1]
15
%
K : the number of time steps in the interval [0,finalT]
16
%
numPlots : the number of output frames in the time interval
17
%
[0,finalT]. Must be a divisor of K.
18
%
typeInit :
is the type of initial condition
19
%
= 1 for the Gaussian wave
20
%
= 2 for the triangular wave
21
%
= 3 for the square wave.
22
%
23
% Output
24
%
errorUW: the max norm error of the UW scheme at t = finalT
25
%
errorLW: the max norm error of the LW scheme at t = finalT
26
%
errorLF: the max norm error of the LF scheme at t = finalT
27
%
28
errorUW = 1.0;
29
errorLW = 1.0;
30
errorLF = 1.0;
31
32
if N > 0 && N-floor(N) == 0
33
h = 1.0/N;
34
else
35
display('Error: N must be a positive integer.')
36
return
37
end
38
39
if (K > 0 && K-floor(K) == 0) && finalT > 0
40
tau = finalT/K;
41
else
42
display('Error: K must be a positive integer, and')
43
display('
finalT must be positive.')
44
return
45
end
46
47
mu = tau/h;
48
49
x = 0:h:1.0;
50
uo = zeros(1,N+1);
51
if typeInit == 1
52
uo = exp(-20*(sin(pi*(x-0.5))).ˆ2);

834
Finite Diﬀerence Methods for Hyperbolic Problems
53
elseif typeInit == 2
54
uo = triangle(N,0);
55
elseif typeInit == 3
56
uo = square(N,0);
57
else
58
display('No such initial condition.')
59
return
60
end
61
62
uoUW
= zeros(1,N+1); uUW = zeros(1,N+1);
63
uoLW
= zeros(1,N+2); uLW = zeros(1,N+2);
64
uoLF
= zeros(1,N+2); uLF = zeros(1,N+2);
65
uExact = zeros(1,N+1);
66
67
uoUW = uo;
68
uoLW = uo; uoLW(N+2) = uo(2);
69
uoLF = uo; uoLF(N+2) = uo(2);
70
71
if mod(K,numPlots) == 0
72
stepsPerPlot = K/numPlots;
73
else
74
display('Error: numPlots is not a divisor of K.')
75
return
76
end
77
78
for k = 1: numPlots
79
for j = 1: stepsPerPlot
80
kk = (k-1)*stepsPerPlot+j;
81
for ell = 2: N+1
82
uUW(ell) = (1-mu)*uoUW(ell)+mu*uoUW(ell-1);
83
uLW(ell) = uoLW(ell)-0.5*mu*(uoLW(ell+1) ...
84
-uoLW(ell-1))+0.5*mu*mu*(uoLW(ell+1) ...
85
-2.0*uoLW(ell)+uoLW(ell-1));
86
uLF(ell) = 0.5*(uoLF(ell+1)+uoLF(ell-1)) ...
87
-0.5*mu*(uoLF(ell+1)-uoLF(ell-1));
88
end
89
uUW(1) = uUW(N+1);
90
uoUW = uUW;
91
uLW(1) = uLW(N+1); uLW(N+2) = uLW(2);
92
uoLW = uLW;
93
uLF(1) = uLF(N+1); uLF(N+2) = uLF(2);
94
uoLF = uLF;
95
end
96
currTime = tau*kk;
97
if typeInit == 1
98
uExact = exp(-20*(sin(pi*(x-0.5-currTime))).ˆ2);
99
elseif typeInit == 2
100
uExact = triangle(N,currTime);
101
elseif typeInit == 3
102
uExact = square(N,currTime);
103
end
104
hf = figure(k);
105
clf
106
plot(x,uExact,'k-',x,uUW,'b-o',x,uLW(1:N+1),'r-s',x, ...
107
uLF(1:N+1),'k-d')
108
grid on;

Listings
835
109
xlabel('x');
110
ylabel('exact and approximate solutions');
111
title(['Finite Difference Approximations at T = ', ...
112
num2str(currTime), ', h = ', num2str(h), ...
113
', and tau =', num2str(tau)]);
114
legend('Exact','Upwind','Lax--Wendroff','Lax--Friedrichs')
115
axis([0,1,-0.1,1.1])
116
set(gca,'xTick',0:0.1:1)
117
118
s1 = ['000' num2str(k)];
119
s2 = s1((length(s1)-3):length(s1));
120
s3 = ['OUT/adv', s2, '.pdf'];
121
exportgraphics(gca, s3)
122
end
123
124
errorUW = max(abs(uExact-uUW));
125
errorLW = max(abs(uExact-uLW(1:N+1)));
126
errorLF = max(abs(uExact-uLF(1:N+1)));
127
end
128
129
function u = triangle(N,t)
130
%
131
% Constructs a periodic triangle wave traveling with a speed of
132
% c = 1.
133
%
134
% Input
135
%
N : to compute the mesh size
136
%
t : the current time
137
%
138
% Output
139
%
u : the array of values of the function
140
%
141
h = 1.0/N;
142
for i = 1: N+1
143
x = (i-1)*h-t;
144
x = x-floor(x);
145
if x >= 0.25 && x <= 0.50
146
u(i) = 4.0*x-1.0;
147
elseif x > 0.50 && x <= 0.75
148
u(i) = 3.0-4.0*x;
149
else
150
u(i) = 0.0;
151
end
152
end
153
end
154
155
function u = square(N,t)
156
%
157
% Constructs a periodic square wave traveling with a speed of
158
% c = 1.
159
%
160
% Input
161
%
N : to compute the mesh size
162
%
t : the current time
163
%
164
% Output

836
Finite Diﬀerence Methods for Hyperbolic Problems
165
%
u : the array of values of the function
166
h = 1.0/N;
167
for i = 1:N+1
168
x = (i-1)*h-t;
169
x = x-floor(x);
170
if x <= 0.5
171
u(i) = 1.0;
172
else
173
u(i) = 0.0;
174
end
175
end
176
end
Listing 29.1 Approximation of the periodic linear transport equation.

Appendix A Linear Algebra Review
In this appendix, we review the basic properties of vector spaces, their norms, and
inner products.
A.1
The Field of Complex Numbers
We start with some basic deﬁnitions.
Deﬁnition A.1 (ﬁeld). A set K together with two operations +: K2 →K (addi-
tion) and ·: K2 →K (multiplication) is called a ﬁeld if the following properties
hold.
1. There is an element 0 ∈K, called zero, such that
x + 0 = 0 + x = x
for all x ∈K.
2. For every x ∈K, there is an element −x ∈K, called the additive inverse of x,
such that
x + (−x) = (−x) + x = 0.
3. The associative property of addition holds, i.e., for all x, y, z ∈K,
x + (y + z) = (x + y) + z.
4. The commutative property of addition holds, i.e., for all x, y ∈K,
x + y = y + x.
5. There is an element 1 ∈K⋆= K\{0}, called the multiplicative identity, such
that
1 · x = x · 1 = x
for all x ∈K.
6. For every x ∈K⋆= K\{0}, there is an element x−1 ∈K⋆, called the
multiplicative inverse of x, such that
x · x−1 = (x−1) · x = 1.

838
Linear Algebra Review
7. The associative property of multiplication holds, i.e., for every x, y, z ∈K,
we have
x · (y · z) = (x · y) · z.
8. The commutative property of multiplication holds, i.e., for all x, y ∈K,
x · y = y · x.
9. The distributive property holds, i.e., for all x, y, z ∈K,
(x + y) · z = x · z + y · z.
Remark A.2 (notation). Usually the symbol · is dropped in the multiplication.
Thus, from now on, x · y will be replaced by xy, unless the former is needed to
avoid ambiguity.
Example A.1
The rational numbers, denoted Q, and the real numbers, denoted R,
endowed with the usual rules for addition and multiplication are ﬁelds.
Let us now deﬁne the ﬁeld of complex numbers C. This is the collection of all
ordered pairs of real numbers, i.e.,
C = {z = (x, y) | x, y ∈R} .
The ﬁrst component is called the real part, ℜz = x, while the second one is the
imaginary part, ℑz = y. This way the real numbers can be naturally embedded
into C via x 7→(x, 0) ∈C. What makes C diﬀerent from R × R is how we deﬁne
operations between elements.
Deﬁnition A.3 (complex addition and multiplication). Suppose that z1 = (x1, y1)
and z2 = (x2, y2) are arbitrary complex numbers. The standard operations of
complex addition and complex multiplication are deﬁned as
z1 + z2 = (x1, y1) + (x2, y2) = (x1 + x2, y1 + y2)
and
z1 · z2 = (x1, y1) · (x2, y2) = (x1x2 −y1y2, x1y2 + x2y1).
The veriﬁcation that C is in fact a ﬁeld is left to the reader as an exercise. The
additive and multiplicative identity elements are clearly 0 = (0, 0) and 1 = (1, 0),
respectively.
Deﬁnition A.4 (imaginary unit). The complex number i = (0, 1) is called the
imaginary unit. It follows that i2 = −1, which justiﬁes writing i = √−1.
Having introduced the imaginary unit and the multiplicative identity, we can
denote complex numbers in a more familiar way, namely
z = (x, y) ∈C
⇐⇒
z = x + iy.

A.2 Vector Spaces
839
Deﬁnition A.5 (complex conjugate). Suppose that z = x + iy ∈C. The complex
conjugate of z, denoted ¯z, is the complex number ¯z = x −iy.
Proposition A.6 (properties of the conjugate). If w, z ∈C, then z + w = ¯z + ¯w
and zw = ¯z ¯w.
Recall that, if x ∈R, its absolute value is deﬁned by
|x| =
(
x,
x ≥0,
−x,
x < 0.
Geometrically, |x| represents the length of the segment with endpoints 0 and x; in
other words, it tells us how far is x from 0 in distance. There is a similar notion for
complex numbers.
Deﬁnition A.7 (modulus). Suppose that z = x + iy ∈C. The modulus of z,
denoted |z|, is deﬁned by |z| =
p
x2 + y 2.
A.2
Vector Spaces
We are now ready to deﬁne the fundamental concept in linear algebra, the one that
will characterize all the objects we will be concerned with in this appendix.
Deﬁnition A.8 (vector space). Suppose that K is a ﬁeld, here called the ﬁeld of
scalars, and V is a set of objects called vectors. V is a vector space over K if
and only if there are operations + : V2 →V (vector addition) and · : K × V →V
(scalar multiplication), such that the triple (V, +, · ) satisﬁes the following.
1. There is an element 0 ∈V, called a zero vector, such that, for all vectors x ∈V,
x + 0 = 0 + x = x.
2. For every vector x ∈V, there is an element −x ∈V, called an additive inverse
of x, such that
x + (−x) = (−x) + x = 0.
3. The associative property of vector addition holds, i.e., for all vectors x, y, z ∈V,
we have
x + (y + z) = (x + y) + z.
4. The commutative property of vector addition holds, i.e., for all vectors x, y ∈V,
it holds that
x + y = y + x.
5. For all scalars α, β ∈K and every vector x ∈V, we have
α · (β · x) = (αβ) · x.
6. For every vector x ∈V, we have 1 · x = x, where 1 ∈K.

840
Linear Algebra Review
7. For all scalars α, β ∈K and every vector x ∈V, we have
(α + β) · x = α · x + β · x.
8. For all α ∈K and x, y ∈V, we have
α · (x + y) = α · x + α · y.
Remark A.9 (notation). Usually the symbol · in the scalar multiplication is
suppressed.
Example A.2
Any ﬁeld is a vector space over itself.
Example A.3
C is a vector space over R. The subset of C whose elements have
rational real and imaginary parts is a vector space over Q.
Example A.4
The set Pn(K) of all polynomials of degree no larger than n with
coeﬃcients from K is a vector space over K.
The following is the canonical example of a vector space.
Deﬁnition A.10 (n-vectors). Let K be a ﬁeld. We deﬁne, for any n ∈N,
Kn =





z =


z1
...
zn



zi ∈K, i = 1, . . . , n





.
We call Cn the set of complex n-vectors and Rn the set of real n-vectors.
To extract the ith component of an n-vector z ∈Kn, we use the notation
[z]i = zi ∈K. We naturally deﬁne n-vector addition and scalar multiplication
component-wise via
[x + y]i = xi + yi,
[αx]i = αxi,
i = 1, . . . , n,
where x, y ∈Kn are arbitrary n-vectors and α ∈K is an arbitrary scalar.
Proposition A.11 (Kn is a vector space). With addition and scalar multiplication
deﬁned as above, Kn is a vector space over K.
Deﬁnition A.12 (linear combination). Let V be a vector space over K and
S = {x1, . . . , xk} ⊆V.
We call x ∈V a linear combination of vectors from S if and only if there are
scalars αi ∈K, i = 1, . . . , k such that
x =
k
X
i=1
αixi ∈V.

A.2 Vector Spaces
841
Deﬁnition A.13 (span). Let S = {x1, . . . , xk} ⊆V be given, where V is a
vector space over the ﬁeld K. The span of the set S is the collection of all linear
combinations of elements of S:
span(S) = ⟨S⟩=
(
z ∈V
 ∃α1, . . . , αk ∈K with z =
k
X
i=1
αixi
)
.
Deﬁnition A.14 (linear dependence). Suppose that V is a vector space over K. Let
S = {x1, . . . , xk} ⊆V be a given ﬁnite set. We say that S is linearly independent
if and only if
k
X
i=1
αixi = 0
implies that αi = 0 ∈K for all i = 1, . . . , k. Otherwise, we say that S is linearly
dependent. If S is an inﬁnite subset of V, we say that S is linearly independent
if and only if each and every ﬁnite subset of S is linearly independent; otherwise,
we say that S is linearly dependent.
In other words, a set of vectors is linearly independent if the only linear
combination that yields the zero vector is the trivial one. This is the same as saying
that no element of S can be expressed as a linear combination of the remaining
elements of S.
Deﬁnition A.15 (subspace). Let V be a vector space over the ﬁeld K with addition
+ and multiplication · . The nonempty subset W ⊆V is called a subspace of V if
and only if (W, +, · ) is itself a vector space over K. In general, we write W ≤V,
but, if it is known that W is a proper subset of V, we write W < V.
Proposition A.16 (span(S) is a subspace). Let S = {x1, . . . , xk} ⊆V be given,
where V is a vector space over the ﬁeld K. Then span(S) is a subspace of V.
Proof. See Problem A.5.
Deﬁnition A.17 (dimension). Suppose that V is a vector space over K. Deﬁne
J = {#(S) | S ⊆V, #(S) < ∞, S is linearly independent} ⊆N,
where #(S) is the cardinality of S, i.e., the number of distinct elements in S. If J
is bounded, we say that V is ﬁnite dimensional. In this case, we write dim(V) =
max(J), and dim(V) is called the dimension of V. If J is unbounded, we say that
V is an inﬁnite-dimensional vector space, and we write dim(V) = ∞. We say that
dim({0}) = 0.
Deﬁnition A.18 (basis). Suppose that V is a ﬁnite-dimensional vector space with
dim(V) = m. A set B ⊆V is called a basis of V if and only if it is linearly
independent and #(B) = m.
Remark A.19 (ﬁnite dimensions). Observe carefully that we have deﬁned the
concept of basis only for ﬁnite-dimensional vector spaces. Bases for inﬁnite-
dimensional spaces, which are also encountered in our text, require more care.

842
Linear Algebra Review
Example A.5
Deﬁne the space of polynomials of arbitrary order,
P(C) =
[
k∈N
Pk−1(C).
The reader can easily show that this is a vector space over the ﬁeld of complex
numbers. The inﬁnite set
S =

1, x, x2, x3, x4, . . .
	
is linearly independent, as the reader can easily show. Therefore, we conclude that
P is of inﬁnite dimensions.
Example A.6
Consider the space of complex-valued continuous functions over
the interval [−1, 1], denoted C([−1, 1]; C). If α, β ∈C and f , g ∈C([−1, 1]; C),
then it is easy to see that αf + βg ∈C([−1, 1]; C). One can easily show that
C([−1, 1]; C) is a vector space over the ﬁeld of complex numbers. The vectors
in this case are functions. In fact, P ⊆C([−1, 1]; C), as long as we restrict the
domain of deﬁnition of the polynomials to [−1, 1]. This shows that C([−1, 1]; C)
is inﬁnite dimensional.
Proposition A.20 (spanning property of a basis). Suppose that V is a ﬁnite-
dimensional vector space and S is a basis. Then span(S) = V.
Proof. See Problem A.6.
Proposition A.21 (generation of subspaces). Suppose that V is a ﬁnite-dimensional
vector space over K and S = {x1, . . . , xk} ⊆V is a set of nonzero vectors. Set
W = span(S). Then some subset of S is a basis for W. Moreover,
1 ≤dim(W) ≤k
and
dim(W) ≤dim(V) < ∞.
Proof. See Problem A.8.
Theorem A.22 (basis extension). Suppose that V is a ﬁnite-dimensional vector
space over K and S = {x1, . . . , xk} ⊆V is a linearly independent set of vectors.
Then 1 ≤k ≤dim(V). Moreover, there is a basis T of V that contains S and
#(T\S) = dim(V) −k.
Proof. See Problem A.9.
Example A.7
Consider the set B = {e1, . . . , en}, where
[ei]j = δi,j =
(
1,
i = j,
0,
i ̸= j.

A.3 Normed Spaces
843
δi,j is called the Kronecker delta function.1 Then B is a basis for Cn, and it is called
the canonical basis.
Finally, using the basis concept, it can be justiﬁed that Cn is, in some sense, the
only vector space of dimension n over the ﬁeld of complex numbers. To see this,
we need the following.
Deﬁnition A.23 (isomorphism). Suppose that V and W are vector spaces over
the same ﬁeld K. A mapping F : V →W is called a vector isomorphism between
V and W if and only if F is a bijection (a one-to-one and onto mapping) from V
to W; and F is linear,
F(αx + βy) = αF(x) + βF(y),
∀α, β ∈K, ∀x, y ∈V.
We say that V and W are isomorphic if and only if there exists a vector isomorphism
between them.
Theorem A.24 (isomorphism). Let V be a vector space over C of ﬁnite dimen-
sion n. Then V is isomorphic to Cn. In other words, V can be identiﬁed with Cn.
Proof. The idea is to construct the isomorphism F using a basis for V and the
canonical basis for Cn.
From this point onward, unless it is indicated otherwise, all vector spaces are
assumed to be over the ﬁeld of complex numbers, C. Such spaces are called complex
vector spaces.
A.3
Normed Spaces
It is often said that a vector is an object that has a magnitude and a direction. We
use norms to characterize lengths (magnitude).
Deﬁnition A.25 (norm). Let V be a complex vector space. A map ∥· ∥: V →R
is called a norm if and only if it satisﬁes the following properties.
1. Positive deﬁniteness: If ∥x∥= 0, then x = 0.
2. Nonnegative homogeneity: For every λ ∈C and x ∈V, it follows that
∥λx∥= |λ|∥x∥.
3. Triangle inequality: For every x, y ∈V, it follows that
∥x + y∥≤∥x∥+ ∥y∥.
A vector space equipped with a norm is called a normed vector space.
As with the absolute value and modulus, the intuition behind a norm is that we
can measure the length of an object.
1 Named in honor of the German mathematician Leopold Kronecker (1823–1891).

844
Linear Algebra Review
Proposition A.26 (properties of the norm). Let V be a complex vector space and
suppose that ∥· ∥: V →R is a norm on V. The following are true.
1. The norm is nonnegative, i.e., for all x ∈V, ∥x∥≥0.
2. The norm satisﬁes the reverse triangle inequality: For all x, y ∈V, we have
∥x∥−∥y∥
 ≤∥x −y∥.
3. The norm is a continuous function.
Proof. See Problem A.10.
Example A.8
The real number R equipped with the absolute value | · | is a
normed vector space over R.
Example A.9
The complex number C with the modulus | · | is a normed vector
space over C.
Example A.10
Let {x0, . . . , xn} ⊆C be a set of n + 1 distinct points. The set of
polynomials Pn(C) of degree at most n ≥1, with complex coeﬃcients, equipped
with
∥p∥=
n
X
i=0
|p(xi)|
is a normed vector space.
Example A.11
Let n ∈N and z ∈Cn. For p ∈[1, ∞), we deﬁne
∥z∥ℓp(Cn) = ∥z∥p =
 n
X
i=1
|zi|p
!1/p
and, for p = ∞,
∥z∥ℓ∞(Cn) = ∥z∥∞=
n
max
i=1 |zi|.
Then Cn equipped with ∥· ∥p, 1 ≤p ≤∞, is a normed vector space. We commonly
denote this normed space ℓp(Cn).
Proposition A.27 (∥· ∥p norm). ∥· ∥ℓp(Cn) : Cn →R is a norm.
Proof. Let us show that the triangle inequality indeed holds for the p-norms with
p ∈(1, ∞). The other properties are simpler and left to the reader as an exercise;
see Problem A.11.

A.3 Normed Spaces
845
We begin by proving Young’s inequality2 for products: if a, b ≥0 and p ∈(1, ∞),
ab ≤ap
p + bq
q ,
where q = p/(p −1) is “conjugate” to p. Notice that this is trivial if a = 0 or
b = 0. For both a and b positive, we just use the fact that the logarithm function
is concave, i.e., for all t ∈(0, 1), we have
log(tap + (1 −t)bq) ≥t log(ap) + (1 −t) log(bq)
= tp log(a) + (1 −t)q log(b).
Notice that, since p > 1, we can set t = 1/p ∈(0, 1) and (1 −t) = 1/q. With
this choice, we then get
log
ap
p + bq
q

≥log(a) + log(b) = log(ab).
We conclude by exponentiating.
The next step is to prove the so-called H¨older inequality,3 which states that if
x, y ∈Cn then we have
n
X
i=1
|xiyi| ≤∥x∥p∥y∥q,
where q is as before. We begin by noticing that if, for all i = 1, . . . , n, xi = 0 or
yi = 0, then the left-hand side of this inequality is zero and there is nothing to
prove. Thus, we assume that ∥x∥p∥y∥q > 0. Deﬁne ˜xi = xi/∥x∥p and ˜yi = yi/∥y∥q.
By Young’s inequality, we then obtain
n
X
i=1
|˜xi ˜yi| ≤1
p
n
X
i=1
|˜xi|p + 1
q
n
X
i=1
|˜yi|q.
Notice now that, by construction, Pn
i=1 |˜xi|p = Pn
i=1 |˜yi|q = 1, so that
n
X
i=1
|˜xi ˜yi| ≤1
p + 1
q = 1.
Multiply by ∥x∥p∥y∥q to conclude the H¨older inequality.
We are ﬁnally ready to show the so-called Minkowski inequality for sums,4 which
amounts to showing the triangle inequality for ∥· ∥p. Let x, y ∈Cn be as before. If
x + y = 0, then there is nothing to show; otherwise,
2 Named in honor of the British mathematician William Henry Young (1863–1942).
3 Named in honor of the German mathematician Otto Ludwig H¨older (1859–1937).
4 Named in honor of the German mathematician Hermann Minkowski (1864–1909).

846
Linear Algebra Review
0 < ∥x + y∥p
p =
n
X
i=1
|xi + yi|p
=
n
X
i=1
|xi + yi||xi + yi|p−1
≤
n
X
i=1
|xi||xi + yi|p−1 +
n
X
i=1
|yi||xi + yi|p−1,
where we used the triangle inequality for the modulus. The ﬁrst term on the right-
hand side of this inequality, using H¨older, can be rewritten as:
n
X
i=1
|xi||xi + yi|p−1 ≤
 n
X
i=1
|xi|p
!1/p  n
X
i=1
|xi + yi|q(p−1)
!1/q
= ∥x∥p∥x + y∥p−1
p
,
where we used that q = p/(p −1). A similar treatment of the second term then
reveals
0 < ∥x + y∥p
p ≤(∥x∥p + ∥y∥p) ∥x + y∥p−1
p
.
Divide by ∥x + y∥p−1
p
> 0 to conclude the triangle inequality.
Norms can be used to characterize convergence and continuity. Sometimes it is
important and useful to know whether a convergent sequence (with respect to one
norm) will also be convergent with respect to another one.
Deﬁnition A.28 (norm equivalence). Let V be a complex vector space and, for
i = 1, 2, let ∥· ∥(i) be norms on V. We say that these norms are equivalent if there
are constants C1, C2 > 0 such that, for every x ∈V, we have
C1 ∥x∥(1) ≤∥x∥(2) ≤C2 ∥x∥(1) .
As the reader can easily verify, norm equivalence essentially means that a
sequence is convergent in one norm if and only if it is convergent in the other.
It turns out that, in Cn, all norms are equivalent.
Theorem A.29 (all norms are equivalent). Suppose that ∥· ∥: Cn →R is a norm.
There exist positive constants 0 < C1 ≤C2 such that
C1 ∥x∥∞≤∥x∥≤C2 ∥x∥∞,
∀x ∈Cn.
In other words, in Cn, all norms are equivalent.
Proof. Let {ei}n
i=1 be the canonical basis of Cn. Then, for every x ∈Cn, we have
x =
n
X
i=1
xiei,
xi ∈C,
∥x∥∞=
n
max
i=1 |xi|,
and
∥x∥=

n
X
i=1
xiei
 ≤
n
X
i=1
|xi|∥ei∥≤
n
max
i=1 |xi|
n
X
i=1
∥ei∥= C2∥x∥∞
with C2 = Pn
i=1 ∥ei∥. Since x is arbitrary, this shows one part of the statement.

A.4 Inner Product Spaces
847
To show the other bound, let F : Cn →R be deﬁned by F(x) = ∥x∥. Since, for
every x, y ∈Cn,
|F(x) −F(y)| = |∥x∥−∥y∥| ≤∥x −y∥≤C2 ∥x −y∥∞,
the function F is continuous with respect to ∥· ∥∞. The set
Sn−1
∞
= {x ∈Cn | ∥x∥∞= 1}
is closed and bounded and, therefore, compact; see Corollary B.15. Then, as a
consequence of Theorem B.47, there exists x0 ∈Sn−1
∞
such that
F(x0) = inf

F(x)
x ∈Sn−1
∞
	
⇐⇒
∥x0∥= F(x0) ≤F(x) = ∥x∥, ∀x ∈Sn−1
∞.
Notice also that x0 ̸= 0, so that we can set C1 = ∥x0∥> 0, and the previous
statement then reads
C1 ≤∥x∥,
∀x ∈Sn−1
∞.
Let now y ∈Cn
⋆be arbitrary. We can then deﬁne x = ∥y∥−1
∞x ∈Sn−1
∞
and, by
the previous reasoning, we ﬁnd
C1 ≤∥x∥=

1
∥y∥∞
y

⇐⇒
C1∥y∥∞≤∥y∥.
Since y was arbitrary, this provides the claimed bound.
As the reader may verify, from the result above it follows that all norms over Cn
are equivalent; see Problem A.12.
A.4
Inner Product Spaces
In the previous section, we introduced the notion of a norm, which was used to
describe the magnitude of a vector. Often, we can also characterize angles between
vectors, i.e., their relative directions. Inner products are used for this purpose.
Deﬁnition A.30 (inner product). Let V be a complex vector space. A function
(·, ·): V × V →C is called an inner product if and only if:
1. For all x, y ∈V, (x, y) = (y, x).
2. For all x, y ∈V and λ ∈C, we have (λx, y) = λ(x, y).
3. For all x, y, z ∈V, it holds that (x + y, z) = (x, z) + (y, z).
4. For all x ∈V, 0 ≤(x, x) ∈R and (x, x) = 0 implies x = 0.
A vector space equipped with an inner product is called a complex inner product
space. A real inner product space is deﬁned analogously.

848
Linear Algebra Review
Example A.12
Let x, y ∈Cn be given complex n-vectors. We deﬁne
(x, y)ℓ2(Cn) = (x, y)2 =
n
X
i=1
xi ¯yi.
This is an inner product, a fact that you should prove, and is called the Euclidean
inner product5; see Problem A.14. When x, y ∈Rn, the object is, after dropping
the unnecessary conjugation, a real inner product, which we denote by
(x, y)ℓ2(Rn) = (x, y)2 = x · y =
n
X
i=1
xiyi.
This inner product makes Rn a real (as opposed to a complex) inner product space
of n dimensions.
Proposition A.31 (Cauchy–Schwarz inequality6). Let V be a complex vector space
with inner product (·, ·). Then, for every x, y ∈V, we have
|(x, y)| ≤(x, x)1/2(y, y)1/2,
with equality if and only if x and y are linearly dependent.
Proof. If either x = 0 or y = 0, there is nothing to prove. It suﬃces, therefore, to
assume that x ̸= 0 and y ̸= 0. Deﬁne z = x −λy, with λ ∈C to be chosen. By
the properties of the inner product, we have
(z, y) = (x −λy, y) = (x, y) −λ(y, y).
Setting λ = (x, y)/(y, y), which is possible since y ̸= 0, we obtain (z, y) = 0.
Now, since x = z + λy, we ﬁnd
(x, x) = (z + λy, z + λy)
= (z, z) + λ(y, z) + ¯λ(z, y) + λ¯λ(y, y)
= (z, z) + |λ|2(y, y)
≥|λ|2(y, y).
Using the deﬁnition of λ allows us to conclude the desired result.
Corollary A.32 (Euclidean norm). Let V be a complex vector space with inner
product (·, ·). Then
∥x∥= (x, x)1/2
(A.1)
deﬁnes a norm. In other words, every complex inner product space is a complex
normed vector space.
5 Named in honor of the Greek mathematician widely regarded as the father of geometry,
Euclid of Alexandria (300 BCE).
6 Named in honor of the French mathematician Augustin Louis Cauchy (1789–1857) and the
German mathematician Karl Hermann Amandus Schwarz (1843–1921).

A.5 Gram–Schmidt Orthogonalization Process
849
Example A.13
For Cn, the natural inner product is the Euclidean inner product,
deﬁned by
(x, y)ℓ2(Cn) = (x, y)2 =
n
X
i=1
xi ¯yi.
Observe that
(x, x)2 =
n
X
i=1
xi ¯xi =
n
X
i=1
|xi|2 = ∥x∥2
2 ∈R.
The object on the right is the square of the 2-norm (p = 2). Thus, the norm
associated with our natural inner product is called the Euclidean norm.
With the concept of an inner product comes some notion of perpendicularity, or
orthogonality.
Deﬁnition A.33 (orthogonality). Let V be a complex vector space with inner
product (·, ·). We say that x, y ∈V are orthogonal, and we write x ⊥y, if and
only if (x, y) = 0. Similarly, we say that the set S ⊆V is orthogonal if and only if,
whenever x, y ∈S and x ̸= y, we have x ⊥y. Finally, if V, W ⊆V, we say that V
and W are orthogonal and write V ⊥W if and only if, for all x ∈V and y ∈W,
we have x ⊥y.
Proposition A.34 (property of orthogonal sets). Let V be a complex vector space
with inner product (·, ·). Suppose that S ⊆V is an orthogonal set. Then S is
linearly independent.
Proof. See Problem A.16.
Deﬁnition A.35 (orthogonal complement). Let W ⊆V, where V is a complex
inner product space. The orthogonal complement of W is the set
W ⊥= {y ∈V | x ⊥y ∀x ∈W} .
Proposition A.36 (property of W ⊥). Let V be a complex vector space with inner
product (·, ·). Suppose that W ⊆V is nonempty. Then W ⊥≤V.
Proof. See Problem A.17.
A.5
Gram–Schmidt Orthogonalization Process
We now discuss a process by which, from any ﬁnite collection of elements of a
vector space, we can construct an orthogonal set that spans the original one.
Deﬁnition A.37 (Gram–Schmidt process7). Let V be an n-dimensional complex
inner product space with inner product (·, ·) and norm deﬁned via (A.1). Suppose
7 Named in honor of the Danish mathematician Jørgen Pedersen Gram (1850–1916) and the
German mathematician Erhard Schmidt (1876–1959).

850
Linear Algebra Review
that {w1, . . . , wk} ⊆V⋆= V\{0}, k ≤n. The Gram–Schmidt process is an
algorithm for generating the set of vectors {q1, . . . , qk} recursively as follows: for
m = 1,
q1 =
1
∥w1∥w1.
For m = 2, . . . , k, provided that q1, . . . , qm−1 have already been successfully
computed, deﬁne
rm = wm −
m−1
X
j=1
(wm, qj)qj.
If rm = 0, the algorithm terminates without completion; otherwise, the algorithm
proceeds with
qm =
1
∥rm∥rm.
Theorem A.38 (properties of Gram–Schmidt). Let V be an n-dimensional complex
inner product space with inner product (·, ·) and norm deﬁned via (A.1). Suppose
that {w1, . . . , wk}
⊆
V⋆, with k
≤
n, is linearly independent. Then the
Gram–Schmidt process proceeds to completion to produce an orthonormal set
{q1, . . . , qk}. Furthermore,
span ({w1, . . . , wm}) = span ({q1, . . . , qm})
for each m = 1, . . . , k.
Proof. See Problem A.19.
Theorem A.39 (basis completion theorem). Suppose that V is a ﬁnite-dimensional
inner product space and W is a proper nontrivial vector subspace. Then there is
an orthonormal basis, S, for W. Moreover, any orthonormal basis for W can be
extended to an orthonormal basis, T, for the whole space V and
#(T\S) = dim(V) −dim(W).
Proof. See Problem A.20.
Example A.14
The Gram–Schmidt process can be easily extended to inﬁnite-
dimensional inner product spaces. For example, we will see later that the object
(f , g)L2(−1,1;C) =
Z 1
−1
f (x)g(x) dx
deﬁnes an inner product on the vector space C([−1, 1]; C). In fact, the reader should
be able to prove this now. Recall from Example A.5 that S = {1, x, x2, x3, . . .} is
a linearly independent set in C([−1, 1]; C). Applying the Gram–Schmidt process
to S yields the so-called Legendre polynomials, which will be used in Chapter 11
to approximate functions and in Chapter 14 to approximate the value of deﬁnite
integrals.

Problems
851
An algorithmic description of the Gram–Schmidt process, in the case that V =
Rn, is presented in Listing A.1. We must warn the reader, however, not to use this
code. This algorithm, known as the classical Gram–Schmidt process, is numerically
unstable. We will see a better algorithm for implementing this process in Section
5.6.
Problems
A.1
Properties of a ﬁeld. Show that, if K is a ﬁeld:
a)
The elements 0 ∈K and 1 ∈K are unique.
b)
The additive inverse is unique.
c)
The multiplicative inverse, if it exists, is unique.
d)
x0 = 0x = 0.
e)
xy = 0 implies that either x = 0 or y = 0.
f)
−x = (−1)x.
g)
(−1)(−x) = x.
A.2
Show that, for every z ∈C, |z| = 0 if and only if z = 0. Show that |z|2 = z ¯z
and, therefore, if z ̸= 0, then
z−1 =
¯z
|z|2 .
A.3
Let z ∈C. Show that
ℜ(z) = 1
2 (z + ¯z)
and
ℑ(z) = 1
2i (z −¯z) .
A.4
Show that any set of vectors that contains the zero vector is linearly
dependent.
A.5
Prove Proposition A.16.
A.6
Prove Proposition A.20.
A.7
Suppose that V is a ﬁnite-dimensional vector space over the ﬁeld K and B
is a basis for V. Show that basis representations are unique. In other words, if
B = {v1, . . . , vn} and x = Pn
i=1 αivi but also x = Pn
i=1 βivi, then it follows that
αi = βi ∈K, i = 1, . . . , n.
A.8
Prove Proposition A.21.
A.9
Prove Theorem A.22.
A.10
Prove Proposition A.26.
A.11
Complete the proof of Proposition A.27.
A.12
Show that all norms in Cn are equivalent.
Hint: Use Theorem A.29.
A.13
Show that, for every x ∈Cn, the following inequalities hold:
∥x∥∞≤∥x∥1 ≤n∥x∥∞,
1
√n∥x∥1 ≤∥x∥2 ≤∥x∥1,
∥x∥∞≤∥x∥2 ≤√n∥x∥∞.
A.14
Complete the details of Example A.12.
A.15
Prove Corollary A.32.

852
Linear Algebra Review
A.16
Prove Proposition A.34.
A.17
Prove Proposition A.36.
A.18
Suppose that S = {w 1, . . . , w k} ⊆Cn is an orthonormal set, i.e., w H
i w j =
δi,j, i, j = 1, . . . , k, with k < n. Let W = span(S) and v ∈Cn be arbitrary. Show
that v = w + p, where p ∈W ⊥and w ∈W.
Hint: Suppose that w = Pk
j=1
 w H
j v

w j.
A.19
Prove Theorem A.38.
Hint: Use induction.
A.20
Prove Theorem A.39.
Hint: Use the Gram–Schmidt process.
Listings
1
function [Q, err] = ClassicalGramSchmidt( W )
2
% The classical Gram-Schmidt orthogonalization process.
3
%
4
% Input
5
%
W(1:n,1:k) : a matrix representing a collection of k column
6
%
vectors of dimension n
7
%
8
% Output
9
%
Q(1:n,1:k) : a collection of k orthonormal vectors of
10
%
dimension n
11
%
err : = 0, if the columns of W are linearly independent
12
%
= 1, if an error has occurred
13
%
14
% WARNING: DO NOT USE THIS CODE!!
15
% The classical Gram-Schmidt process is numerically unstable
16
n = size(W)(1);
17
k = size(W)(2);
18
Q = zeros(n,k);
19
err = 0;
20
if k > n
21
err = 1;
22
return;
23
end
24
norm q = norm( W(:,1) );
25
if norm q > eps( norm q )
26
Q(:,1) = W(:,1)/norm q;
27
else
28
err = 1;
29
return;
30
end
31
for m=2:k
32
r = W(:,m);
33
for j = 1:m-1
34
r = r - (W(:,m)'*Q(:,j)).*Q(:,j);
35
end
36
norm q = norm( r );
37
if norm q > eps( norm q )

Listings
853
38
Q(:,m) = r/norm q;
39
else
40
err = 1;
41
return;
42
end
43
end
44
end
Listing A.1 The classical Gram–Schmidt orthogonalization process.

Appendix B Basic Analysis Review
In this appendix, we review some fundamental results from the analysis of functions
of one or more variables. Readers will ﬁnd the references [6, 76] indispensable.
B.1
Sequences and Compactness in Cd and Rd
Deﬁnition B.1 (sequences and convergence). Let d ∈N. A sequence is a function
x : N →Cd. We use the notation xn = x(n). When referring to the complete
sequence, we might use diﬀerent notations {xn}n∈N = {xn}∞
n=1 ⊂Cd. We say that
the sequence {xn}n∈N converges if and only if there is a point ξ ∈Cd such that,
for every ε > 0, there is a positive integer N ∈N such that if n is any integer
satisfying n ≥N, then
∥xn −ξ∥2 ≤ε.
For short, we write limn→∞xn = ξ, or, equivalently, xn →ξ, as n →∞We call
ξ the limit of the sequence. A sequence {xn}n∈N is called bounded if and only if
there is a positive real number M such that, for all n ∈N,
∥xn∥2 ≤M.
A sequence {xn}n∈N is called Cauchy1 if and only if, for every ε, there is a positive
integer N ∈N such that if m and n are integers satisfying m, n ≥N, then
∥xm −xn∥2 ≤ε.
A sequence {xn}n∈N ⊂R is said to be increasing (decreasing) if and only if
xn ≤xn+1 (xn ≥xn+1) for all n ∈N. It is said to be strictly increasing (strictly
decreasing) if and only if xn < xn+1 (xn > xn+1) for all n ∈N.
Deﬁnition B.2 (subsequence). Let d ∈N. Suppose that {xn}n∈N ⊂Cd is a given
sequence. The sequence {y k}k∈N ⊂Cd is called a subsequence of {xn}n∈N if and
only if there is a strictly increasing sequence (of indices) {nk}k∈N ⊆N, denoted
nk = n(k), such that y(k) = x(n(k)), or, equivalently,
y k = xnk.
For subsequences, we write {y k}k∈N = {xnk}k∈N ⊆{xn}n∈N for short.
1 Named in honor of the French mathematician Augustin Louis Cauchy (1789–1857).

B.1 Sequences and Compactness in Cd and Rd
855
Example B.1
Suppose that xn = (−1)n. This sequence is bounded, but not
convergent. Deﬁne the sequence of indices nk = 2k, which is strictly increasing.
The subsequence yk = xnk is convergent, though it is not that interesting:
yk = xnk = x2k = (−1)2k = 1
for all k ∈N.
Let us present, mostly without proof, several properties and facts about
sequences and their limits.
Theorem B.3 (properties of convergent sequences). Let d ∈N. Suppose that
{xn}n∈N ⊂Cd is a convergent sequence. Then:
1. {xn}n∈N is bounded.
2. {xn}n∈N is Cauchy.
3. If, for ξ1, ξ2 ∈Cd,
lim
n→∞xn = ξ1,
lim
n→∞xn = ξ2,
then ξ1 = ξ2. In other words, limits are unique.
When we deal with sequences of real numbers, more reﬁned tools can be invoked.
Deﬁnition B.4 (limit superior and inferior). Let {an}∞
n=1 be a bounded sequence
of real numbers. Then
lim inf
n→∞an = sup
n≥1
inf
m≥n am
and
lim sup
n→∞an = inf
n≥1 sup
m≥n
am.
The following result is well known.
Proposition B.5 (convergence criterion). Let {an}∞
n=1 be a bounded sequence of
real numbers. Then
lim inf
n→∞an ≤lim sup
n→∞an.
Furthermore, {an}∞
n=1 is convergent if and only if
lim inf
n→∞an = lim sup
n→∞an.
Theorem B.6 (Squeeze Theorem). Suppose that {an}∞
n=1 ⊂R is a convergent
sequence and limn→∞an = α. Let {bn}∞
n=1 ⊂R be another sequence with the
property that
bn ∈
(
[α, an],
an ≥α,
[an, α],
an ≤α.
Then bn →α, as n →∞.

856
Basic Analysis Review
Theorem B.7 (Monotone Convergence Theorem). Suppose that {xn}n∈N ⊂R is
a bounded monotone sequence, i.e., either nondecreasing or nonincreasing. There
is a point ξ ∈R such that limn→∞xn = ξ. If the sequence is nondecreasing,
lim
n→∞xn = ξ = sup {xn | n ∈N} ;
if nonincreasing,
lim
n→∞xn = ξ = inf {xn | n ∈N} .
It turns out that, in Cd, a sequence is Cauchy if and only if it converges.
Theorem B.8 (Cauchy criterion). Let d ∈N. Suppose that {xn}n∈N ∈Cd is a
Cauchy sequence. There is a point ξ ∈Cd such that limn→∞xn = ξ.
Theorem B.9 (Bolzano–Weierstrass Theorem2). Let d ∈N. Suppose that the
sequence {xn}n∈N ∈Cd is bounded. There is a subsequence {xnk}k∈N ⊆{xn}n∈N
and a point ξ ∈Cd such that limk→∞xnk = ξ.
In numerical analysis, it is important not only to know that a sequence converges,
but many times we also wish to describe how fast the convergence is. The following
deﬁnition quantiﬁes this.
Deﬁnition B.10 (order of convergence). Let d ∈N. Suppose that the sequence
{xk}∞
k=1 ⊂Cd converges to the point ξ ∈Cd, i.e., ξ = limk→∞xk. We say that xk
converges to ξ at least linearly if and only if there exists a sequence of positive
real numbers {εk}∞
k=1 that converges to 0 and a real number µ ∈(0, 1) such that
∥xk −ξ∥2 ≤εk, ∀k ∈N,
lim
k→∞
εk+1
εk
= µ.
(B.1)
If (B.1) holds with ∥xk −ξ∥2 = εk, for k = 1, 2, . . ., we say that xk converges to
ξ linearly, or exactly linearly.
We say that xk converges to ξ with at least order q, q > 1, if and only if there
exists a sequence of positive real numbers {εk}∞
k=1 that converges to 0 and a real
number µ > 0 such that
∥xk −ξ∥2 ≤εk, ∀k ∈N,
lim
k→∞
εk+1
εq
k
= µ.
(B.2)
If (B.2) holds with ∥xk −ξ∥2 = εk, for k = 1, 2, . . ., we say that xk converges
to ξ with order q, or with exactly order q. In particular, if q = 2, we say that xk
converges to ξ quadratically. If q = 3, we say that xk converges to ξ cubically.
At this point, it is important to introduce the so-called Big O notation, which is
one of the so-called Landau symbols.3 The notation is used to describe the relative
order of convergence of two quantities.
2 Named in honor of the Bohemian mathematician, logician, philosopher, theologian, and
Catholic priest of Italian extraction Bernardus Placidus Johann Nepomuk Bolzano
(1781–1848) and the German mathematician Karl Theodor Wilhelm Weierstrass
(1815–1897).
3 Named in honor of the German mathematician Edmund Georg Hermann Landau
(1877–1938).

B.1 Sequences and Compactness in Cd and Rd
857
Deﬁnition B.11 (Big O notation). Let {ak}k∈N, {bk}k∈N ⊂Cd. If there is a
constant M > 0 and an integer K ∈N such that, for all k ≥K, we have
∥ak∥2 ≤M∥bk∥2,
then we write ak = O(bk) and say ak is big O of bk. We use similar notation for
functions that depend on a continuous variable.
Example B.2
Suppose that xn = 1 −2−n. This sequence converges to ξ = 1.
Let us quantify how fast this convergence is. Observe that
|1 −xn| = |1 −1 + 2−n| = 2−n = εn.
Then
εn+1
εn
=
2n
2n+1 = 1
2 = µ ∈(0, 1).
Thus, xn converges to one exactly linearly. With this example, we see that geometric
convergence is just linear convergence. We might say that a sequence xn converges
geometrically if and only if
xn = ξ ± qn,
where q ∈(−1, 0) ∪(0, 1). Then xn →ξ exactly linearly and µ = |q|.
A very important notion in analysis is that of compactness, which we now
describe.
Deﬁnition B.12 (compact). A set K ⊂Cd is called compact if and only if, for
every sequence {xn}n∈N in K, there is a subsequence {y k}k∈N = {xnk}k∈N ⊆
{xn}n∈N and a point y ∈K such that y k →y as k →∞.
Deﬁnition B.13 (closed and bounded). A set K ⊂Cd is called closed if and only if
whenever {xn}n∈N ⊂K is such that xn →x as n →∞, then it follows that x ∈K.
K is called bounded if and only if there exists an M > 0 such that ∥x∥2 ≤M for
all x ∈K.
The next theorem establishes a fundamental equivalence.
Theorem B.14 (Heine–Borel Theorem4). A set K ⊂C is compact if and only if
it is closed and bounded.
An immediate corollary of this result is a compactness criterion in Cd.
Corollary B.15 (compactness in Cd). A set K ⊂Cd is compact if and only if it is
closed and bounded with respect to the ∞-norm.
Proof. Argue component-wise.
4 Named in honor of the German mathematician Heinrich Eduard Heine (1821–1881) and the
French mathematician and politician F´elix ´Edouard Justin ´Emile Borel (1871–1956).

858
Basic Analysis Review
Deﬁnition B.16 (closure). Suppose that K ⊆Cd. x ∈C is called a limit point of
K if and only if there is a convergent sequence {xn}n∈N in K such that xn →x.
The set K is called the closure of K, and is deﬁned by
K = K ∪

x ∈Cd  ∃{xn}n∈N ⊂K, xn →x
	
.
B.2
Functions of a Single Real Variable
We now list some of the most important properties of functions of a single real
variable.
Deﬁnition B.17 (interval). Let a and b be extended real numbers, i.e., a, b ∈
[−∞, ∞], with a < b. A set I ⊆R is called an interval if and only if I = (a, b), or
I = [a, b), or I = (a, b], or I = [a, b]. The interval I is called semi-inﬁnite if and
only if exactly one of its endpoints is inﬁnite.
Deﬁnition B.18 (continuity). Let I ⊆R be an interval (ﬁnite, semi-inﬁnite, or
inﬁnite) and f : I →R be a function. We say that f is continuous at x0 ∈I if and
only if, for every ε > 0, there is a δ > 0 such that if x is any point of I satisfying
|x −x0| < δ, then it follows that
|f (x) −f (x0)| < ε.
We say that f is continuous on I, and we write f ∈C(I) if and only if f is
continuous at every point of the interval I. We say that f is uniformly continuous
on I if and only if, for every ε, there exists a δ > 0, only depending upon ε and I,
such that if x1 and x2 are any two elements of I with |x1 −x2| < δ, then it follows
that
|f (x1) −f (x2)| < ε.
A sequential characterization of continuity is as follows.
Theorem B.19 (sequential continuity). Let I ⊆R be an interval (ﬁnite, semi-
inﬁnite, or inﬁnite) and f : I →R be a function. The function f is continuous
at x0 ∈I if and only if, for every sequence {xn}n∈N ⊂I with xn →x0, we have
f (xn) →f (x0).
It turns out that, on compact sets, continuity and uniform continuity are
equivalent notions.
Theorem B.20 (uniform continuity). Suppose that I = [a, b] is a compact interval
and f : I →R is a function. Then if f is continuous on I, it is also uniformly
continuous on I. For I = (a, b), a bounded or unbounded open interval, there exist
functions that are continuous on I but not uniformly continuous on I.
Intuitively, continuity means that the function gives values that are “close to
each other” provided that the arguments are suﬃciently close. This closedness
can be quantiﬁed, for instance, with the following deﬁnition.

B.2 Functions of a Single Real Variable
859
Deﬁnition B.21 (H¨older continuity5). Suppose that I ⊆R is an interval, f : I →R
is a function, and α ∈(0, 1]. We say that f is H¨older continuous of order α if
and only if there is a constant C > 0 such that
|f (x) −f (y)| ≤C|x −y|α
for all x, y ∈I. If α = 1, we say that f is Lipschitz continuous.6 For α ∈(0, 1],
the collection of all functions f : I →R that are H¨older continuous of order α
is denoted by C0,α(I). Thus, the collection of Lipschitz continuous functions is
denoted by C0,1(I).
While H¨older and Lipschitz continuity somewhat characterize the smoothness of
a function, a stronger version of this is given by diﬀerentiability.
Deﬁnition B.22 (diﬀerentiability). Suppose that I ⊆R is an interval and f : I →R
is a function. f is said to be diﬀerentiable at x0 ∈I if and only if the limit
L = lim
h→0
f (x0 + h) −f (x0)
h
exists. In this case, we write f ′(x0) = L. We say that f is diﬀerentiable on I if
and only if f is diﬀerentiable at every point in I.
Notice that, if f is diﬀerentiable on I, f ′ : I →R deﬁnes a function. We call
this function the derivative of f . The properties of the derivative (continuity, etc.)
imply several other properties for the function f itself. In addition, we can study
the diﬀerentiability of f ′, giving rise to the second derivative f ′′. In an analogous
manner, we can talk, for m ∈N, about the derivative f (m) of order m of a function,
and study its properties.
Deﬁnition B.23 (continuously diﬀerentiable). Let I ⊆R be an interval (ﬁnite,
semi-inﬁnite, or inﬁnite) and f : I →R be a function. Let m ∈N. We write
f ∈Cm(I) if and only if f ∈C(I) and all of its derivatives up to and including
the mth order derivative exist and are continuous at every point x ∈I. We write
C0(I) = C(I).
Example B.3
Let f ∈C(a, b). We say that u : [a, b] →R is a classical solution
to the problem
u′′(x) = f (x),
x ∈(a, b),
u(a) = u(b) = 0
(B.3)
if and only if u ∈C2(a, b) ∩C([a, b]) and u satisﬁes (B.3) pointwise.
5 Named in honor of the German mathematician Otto Ludwig H¨older (1859–1937).
6 Named in honor of the German mathematician Rudolf Otto Sigismund Lipschitz
(1832–1903).

860
Basic Analysis Review
Deﬁnition B.24 (boundedness). Let I ⊆R be an interval (ﬁnite, semi-inﬁnite, or
inﬁnite) and f : I →R be a function. We say that f is bounded in I if and only if
there is a real number M ≥0 such that, for every x ∈I,
|f (x)| ≤M.
We write f ∈Cm
b (I) if and only if f ∈Cm(I), and f and all of its derivatives up to
and including the mth order derivative are bounded on I.
It is of importance to determine the values that a function in C(I), where I is an
interval, can take. We now explore this.
Deﬁnition B.25 (inﬁmum and supremum of a function). Suppose that I ⊆R is
an interval and f : I →R. Set R = {f (x) | x ∈I} ⊂R. Then
inf
x∈I f (x) = inf R,
sup
x∈I
f (x) = sup R.
Deﬁnition B.26 (maximum and minimum of a function). Suppose that I ⊆R is
an interval and f : I →R. We write
f (z) = min
x∈I f (x),
z ∈argmin
x∈I
f (x)
if and only if z ∈I and z satisﬁes
f (z) = inf
x∈I f (x).
(B.4)
If z ∈I is the unique point satisfying (B.4), we write
z = argmin
x∈I
f (x).
Similarly, we write
f (w) = max
x∈I f (x),
w ∈argmax
x∈I
f (x)
if and only if w ∈I and w satisﬁes
f (w) = sup
x∈I
f (x).
(B.5)
If w ∈I is the unique point satisfying (B.5), we write
w = argmax
x∈I
f (x).
The intuition behind continuity is that the graph has “no gaps”. The following
result makes this rigorous.
Theorem B.27 (Intermediate Value Theorem). Suppose that [a, b] ⊂R is compact
and f ∈C([a, b]). Set
α =
inf
x∈[a,b] f (x),
β = sup
x∈[a,b]
f (x).
There exist γ, δ ∈R satisfying
−∞< γ ≤f (x) ≤δ < ∞,
∀x ∈[a, b],

B.2 Functions of a Single Real Variable
861
which implies that
γ ≤α ≤β ≤δ.
Furthermore, for any y ∈[α, β], there is at least one point z ∈[a, b] such that
f (z) = y. In particular, there are (not necessarily unique) points xα, xβ ∈[a, b]
such that
f (xα) = α = min
x∈[a,b] f (x),
f (xβ) = β = max
x∈[a,b] f (x).
As a consequence, in compact sets, continuous functions must be bounded.
Corollary B.28 (equality). Let [a, b] be a compact set and m ∈N0. Then
Cm([a, b]) = Cm
b ([a, b]).
Let us now investigate the possible values that the derivative of a function can
take.
Theorem B.29 (Rolle’s Theorem7). Suppose that −∞< a < b < ∞and f ∈
C([a, b]). Assume that f ′(x) exists for all x ∈(a, b) and f (a) = f (b). Then there
exists at least one point ξ ∈(a, b) such that f ′(ξ) = 0.
Theorem B.30 (Mean Value Theorem). Suppose that −∞< a < b < ∞, f ∈
C([a, b]), and f ′(x) exists for all x ∈(a, b). Then there exists at least one point
ξ ∈(a, b) such that
f ′(ξ) = f (b) −f (a)
b −a
.
Proof. Deﬁne
g(x) = f (x) −x −a
b −a (f (b) −f (a)) .
Then g : [a, b] →R satisﬁes the hypotheses of Rolle’s Theorem. In particular,
g(a) = f (a) = g(b). By Rolle’s Theorem, there is a point ξ ∈(a, b) such that
g′(ξ) = 0. But
g′(x) = f ′(x) −f (b) −f (a)
b −a
for all x ∈(a, b). Thus, f ′(ξ) −f (b)−f (a)
b−a
= 0.
A fundamental notion in numerical analysis, approximation theory, and many
applications is that smoothness of a function is equivalent to the fact that this
function can be approximated by simpler ones. One way of approximating is via
polynomials, and the use of the so-called Taylor polynomial.
Theorem B.31 (Taylor’s Theorem I8). Suppose that −∞< a < b < ∞and n
is a nonnegative integer. Assume that f ∈Cn([a, b]) and f (n+1)(x) exists for all
x ∈(a, b). For each x ∈(a, b], there exists a point ξ = ξ(x) ∈(a, x) such that
f (x) = f (a) + f ′(a)(x −a) + · · · + 1
n!f (n)(a)(x −a)n + Rn(x),
7 Named in honor of the French mathematician Michel Rolle (1652–1719).
8 Named in honor of the British mathematician Brook Taylor (1685–1731).

862
Basic Analysis Review
where the remainder term is given by
Rn(x) =
1
(n + 1)!f (n+1)(ξ)(x −a)n+1.
Theorem B.32 (Taylor’s Theorem II). Suppose that n is a nonnegative integer,
[a, b] ⊂R is compact, and c ∈[a, b]. Assume that f ∈Cn([a, b]) and f (n+1)(x)
exists for all x ∈(a, b). For each x ∈[a, c)∪(c, b], there exists a point ξ = ξ(x, c),
strictly between x and c — i.e., ξ ∈(c, x), if x ∈(c, b], and ξ ∈(x, c), if x ∈[a, c)
— such that
f (x) = f (c) + f ′(c)(x −c) + · · · + 1
n!f (n)(c)(x −c)n + Rn(x),
where the remainder term is
Rn(x) =
1
(n + 1)!f (n+1)(ξ)(x −c)n+1.
Remark B.33 (generalization). Taylor’s Theorem is an obvious generalization of
the Mean Value Theorem B.30. The latter follows from the former upon setting
n = 0.
Let us now recall some facts about the theory of the Riemann integral.
Deﬁnition B.34 (Riemann integrable9). Let I ⊆R be a ﬁnite interval and f : I →R
be a function. We write f ∈R(I) if and only if f is Riemann integrable on the
interval I, i.e.,
0 ≤

Z
I
f (x)dx
 ≤
Z
I
|f (x)| dx ≤∞.
For I = [a, b], we use the simpler notation R(a, b) in place of R([a, b]), and
similarly for [a, b), etc.
We now study when a function is Riemann integrable, and some properties of
the Riemann integral.
Theorem B.35 (integrability condition). If f ∈C([a, b]), then f ∈R(a, b).
The following is a particular case of the so-called H¨older inequality for integrals.
Theorem B.36 (H¨older). Suppose that f ∈C([a, b]) and g ∈R(a, b). Then
f g ∈R(a, b) and

Z b
a
f (x)g(x)dx
 ≤max
a≤x≤b |f (x)|
Z b
a
|g(x)| dx.
The connection between the Riemann integral and the derivative is the content
of the so-called Fundamental Theorem of Calculus (FToC).
9 Named in honor of the German mathematician Georg Friedrich Bernhard Riemann
(1826–1866).

B.2 Functions of a Single Real Variable
863
Theorem B.37 (FToC). Suppose that f ∈R(a, b) and F : [a, b] →R is a diﬀer-
entiable function satisfying F ′(x) = f (x) for all x ∈[a, b]. Then
Z b
a
f (x)dx = F(b) −F(a).
The following result, and its generalization to several dimensions, is fundamental
in the treatment of boundary value problems.
Theorem B.38 (integration by parts). Suppose that [a, b] is compact and f , g ∈
C([a, b]). Assume that f and g are diﬀerentiable at every point in (a, b) and f ′, g′ ∈
R(a, b). Then f g′, f ′g ∈R(a, b) and
Z b
a
f (x)g′(x)dx = f (x)g(x)|x=b
x=a −
Z b
a
f ′(x)g(x)dx.
The following result is a consequence of the FToC and integration by parts, and
it is known as Taylor’s Theorem with integral remainder.
Theorem B.39 (Taylor’s Theorem III). Suppose that −∞< a < b < ∞and n
is a nonnegative integer. Assume that f ∈Cn([a, b]), f (n+1) exists, and f (n+1) ∈
R(a, b). For each x ∈(a, b],
f (x) = f (a) + f ′(a)(x −a) + · · · + 1
n!f (n)(a)(x −a)n + Rn(x),
where the (integral) remainder is
Rn(x) = 1
n!
Z x
a
f (n+1)(t)(x −t)ndt.
Proof. The proof is by induction on n.
For n = 0, we have, by the FToC,
Z x
a
f ′(t)dt = f (x) −f (a),
and the n = 0 case follows.
For the inductive hypothesis, we assume that the theorem is true for all n ≤k:
for each x ∈(a, b],
f (x) = f (a) + f ′(a)(x −a) + · · · + 1
k!f (k)(a)(x −a)k + 1
k!
Z x
a
f (k+1)(t)(x −t)kdt,
provided that f ∈Ck([a, b]) and f (k+1) exists and is Riemann integrable on (a, b).
For the inductive step, we assume that f ∈Ck+1([a, b]) and f (k+2) exists and is
Riemann integrable on (a, b). Observe that f ∈Ck+1([a, b]) implies that f (k+1) ∈
R(a, b). Since the n = k case is valid,
f (x) = f (a) + (x −a)f ′(a) + · · · + 1
k!f (k)(a)(x −a)k + 1
k!
Z x
a
f (k+1)(t)(x −t)kdt.

864
Basic Analysis Review
Using integration by parts
1
k!
Z x
a
f (k+1)(t)(x −t)kdt =
1
(k + 1)!f (k+1)(a)(x −a)k+1
+
1
(k + 1)!
Z x
a
f (k+2)(t)(x −t)k+1dt.
Thus, the n = k + 1 case follows, and the result follows by induction.
The following two theorems are direct consequences of the Intermediate Value
Theorem B.27.
Theorem B.40 (Summation Mean Value Theorem). Suppose that {ci}n
i=1 ⊂R
is a ﬁnite sequence of nonnegative real numbers, [a, b] ⊂R is a compact set,
{xi}n
i=1 ⊂[a, b], and f ∈C([a, b]). Then there is a point ξ ∈[a, b] such that
n
X
i=1
cif (xi) = f (ξ)S,
S =
n
X
i=1
ci.
Thus, if ci = 1, for all i = 1, . . . , n, there is a point ξ ∈[a, b] such that
f (ξ) = 1
n
n
X
i=1
f (xi).
Proof. By the Intermediate Value Theorem B.27, there are points xα, xβ ∈[a, b]
such that
f (xα) = min
a≤x≤b f (x) = α,
f (xβ) = max
a≤x≤b f (x) = β;
furthermore, for all i ∈{1, . . . , n},
α ≤f (xi) ≤β.
Since ci ≥0, for all i ∈{1, . . . , n},
αci ≤cif (xi) ≤βci.
Summing over i, we get
αS ≤
n
X
i=1
cif (xi) ≤βS.
The case S = 0 is trivial; otherwise, we have
α ≤1
S
n
X
i=1
cif (xi) ≤β.
By the Intermediate Value Theorem B.27, there is a point ξ ∈[a, b] such that
f (ξ) = 1
S
n
X
i=1
cif (xi),
and the result follows.

B.2 Functions of a Single Real Variable
865
Theorem B.41 (Integral Mean Value Theorem). Suppose that −∞< a < b <
∞, f ∈C([a, b]), and g ∈R(a, b). Furthermore, suppose that g(x) ≥0 for all
x ∈[a, b]. Then there exists a point ξ ∈[a, b] such that
Z b
a
f (x)g(x)dx = f (ξ)
Z b
a
g(x)dx.
Thus, if g(x) = 1, for all x ∈[a, b], there exists a point ξ ∈[a, b] such that
f (ξ) =
1
b −a
Z b
a
f (x)dx.
Proof. Since f is continuous on [a, b], it is bounded, i.e.,
−∞< α = min
x∈[a,b] f (x) ≤f (x) ≤max
x∈[a,b] f (x) = β < ∞
for all x ∈[a, b]. Since g ≥0 on [a, b], it follows that, for all x ∈[a, b],
αg(x) ≤f (x)g(x) ≤βg(x).
This implies that
α
Z b
a
g(x)dx ≤
Z b
a
f (x)g(x)dx ≤β
Z b
a
g(x)dx.
(B.6)
Since g ≥0 on [a, b] and g ∈R(a, b),
0 ≤S =
Z b
a
g(x)dx < ∞.
If S = 0, then from (B.6) it follows that
Z b
a
f (x)g(x)dx = 0,
and the result follows trivially; otherwise, (B.6) implies that
α ≤y = 1
S
Z b
a
f (x)g(x)dx ≤β.
By the Intermediate Value Theorem B.27, there is a point ξ ∈[a, b] such that
f (ξ) = y, and the result follows.
We ﬁnally mention that, by arguing component-wise, we can also consider
integration of functions whose values lie in a ﬁnite-dimensional vector space. The
following deﬁnition is an example of this approach.
Deﬁnition B.42 (Riemann integrable). Let [a, b] ⊂R be a compact integral. We
say that the function f ∈R(a, b; C) if ℜf , ℑf ∈R(a, b). For such a function, we
deﬁne
Z b
a
f (x)dx =
Z b
a
ℜf (x)dx + i
Z b
a
ℑf (x)dx ∈C.

866
Basic Analysis Review
B.3
Functions of Several Variables
We now study functions of several variables.
Deﬁnition B.43 (p-ball). Suppose that d ∈N and a ∈Cd. For any r ≥0, deﬁne,
for 1 ≤p ≤∞, the p-ball of radius r to be
Bp(a, r) =

x ∈Cd  ∥x −a∥p < r
	
.
We set B(a, r) = B2(a, r). We use the notation Bp(a, r) = Bp(a, r), where the
overbar indicates closure with respect to the norm ∥· ∥p. As before, B(a, r) =
B2(a, r).
Deﬁnition B.44 (convexity). Suppose that d ∈N and Ω⊆Cd. We say that Ωis
convex if and only if, for all x, y ∈Ωand all t ∈[0, 1], we have
tx + (1 −t)y ∈Ω.
Deﬁnition B.45 (domain). Suppose that d ∈N and Ω⊆Cd. We say that Ωis a
domain if Ωis nonempty, open, and connected.
We deﬁne continuity in several variables as before.
Deﬁnition B.46 (continuity). Suppose that d ∈N, Ω⊆Cd, and f : Ω→R. We
say that f ∈C(Ω) if and only if f is continuous at every point x ∈Ω.
As in the one-dimensional case, a continuous function takes all its intermediate
values.
Theorem B.47 (Intermediate Value Theorem in Cd). Suppose that d ∈N, Ω⊂Cd
is compact (closed and bounded), and f ∈C(Ω; R). Set R = {f (x) | x ∈Ω}. Then
there are points xmin, xmax ∈Ωsuch that
f (xmin) = inf
x∈Ωf (x) = inf R,
f (xmax) = sup
x∈Ω
f (x) = sup R,
and the extrema are ﬁnite,
−∞< f (xmin) ≤f (xmax) < ∞.
If Ωis, in addition, convex and y ∈[f (xmin), f (xmax)], there is at least one point
z ∈Ωsuch that f (z) = y.
Deﬁnition B.48 (multi-index). Suppose that d ∈N. A point α ∈Nd
0 is called a
multi-index. The order of the multi-index is denoted |α| and is deﬁned as
|α| = ∥α∥1 =
d
X
i=1
αi.
Let α be a multi-index and x ∈Rd. We deﬁne the notation
xα = xα1
1 xα2
2 · · · xαd
d ,
α! = α1!α2! · · · αd!.

B.3 Functions of Several Variables
867
Multi-indices are useful in giving a shorthand notation for partial derivatives.
Deﬁnition B.49 (partial derivatives). Suppose that d, m ∈N, Ω⊆Rd, and
f : Ω→R. We say that f ∈Cm(Ω) if and only if f ∈C(Ω) and every partial
derivative
Dαf (y) = ∂|α|f (y)
∂xα
=
∂|α|f (y)
∂xα1
1 ∂xα2
2 · · · ∂xαd
d
of order |α| ≤m exists and is continuous at all points y ∈Ω.
The ﬁrst partial derivatives can be gathered into a vector.
Deﬁnition B.50 (gradient). Suppose that d ∈N, Ω⊆Rd is open, y ∈Ω, and
f ∈C(Ω) is such that every partial derivative of order one exists at y. The gradient
of f at y is the vector ∇f (y) ∈Rd with components
[∇f (y)]i = ∂f (y)
∂xi
.
The following result is a version of Taylor’s Theorem for several variables.
Theorem B.51 (Taylor’s Theorem I in Rd). Let d ∈N, a ∈Rd, and r > 0.
Suppose that f ∈C2  B(a, r); R

and x ∈B(a, r) is ﬁxed. Then there exists
ξ = ξ(x, a) ∈B(a, r) such that
f (x) = f (a) + ∇f (a)⊺(x −a) + 1
2(x −a)⊺Hf (ξ)(x −a),
where Hf : B(a, r) →Rd×d is the Hessian matrix10 of f , which has components
[Hf (y)]i,j = ∂2f (y)
∂xi∂xj
.
Writing η = x −a, the result may be expressed as
f (x) = f (a) + ∇f (a)⊺η + 1
2η⊺Hf (ξ)η.
Proof. Deﬁne, for all t ∈[0, 1],
φ(t) = f (a + tη),
and apply Taylor’s Theorem B.31. The details are left to the reader as an exercise;
see Problem B.12.
Theorem B.52 (Taylor’s Theorem II in Rd). Let d, m ∈N, a ∈Rd, and r > 0.
Suppose that f ∈Cm+1 B(a, r); R

and x ∈B(a, r) is ﬁxed. Assume that, for all
x ∈B(a, r),
|Dαf (x)| ≤A
for every multi-index |α| = m + 1. Then, for every η ∈B(0, r),
f (η + a) = f (a) +
m
X
j=1
X
|α|=j
1
α!Dαf (a)ηα + Rm,
10 Named in honor of the German mathematician Ludwig Otto Hesse (1811–1874).

868
Basic Analysis Review
where
|Rm| ≤
A
(m + 1)! ∥η∥m+1
1
.
Proof. See Problem B.14
Remark B.53 (convexity). The previous two results continue to hold when the
closed and open balls are replaced by closed and open convex sets, respectively.
The next theorem gives an alternate to Taylor’s Theorem that is suﬃcient for
many applications.
Theorem B.54 (Taylor’s Theorem III in Rd). Let d ∈N and Ω⊂Rd be a bounded,
open, convex set. Suppose that f ∈C1 (Ω; R) with the additional regularity
∥∇f (x) −∇f (y)∥2 ≤γ ∥x −y∥2
for some γ > 0 and for all x, y ∈Ω. Then, for all x, y ∈Ω,
|f (x) −f (y) −∇f (y)⊺(x −y)| ≤γ
2 ∥x −y∥2
2 .
Proof. Deﬁne, for all t ∈[0, 1],
φ(t) = f (y + t(x −y)).
By the Chain Rule,
φ′(t) = ∇f (y + t(x −y))⊺(x −y).
Then, by the Cauchy–Schwarz inequality,
|φ′(t) −φ′(0)| ≤∥x −y∥2 ∥∇f (y + t(x −y)) −∇f (y)∥2 ≤γt ∥x −y∥2
2 .
Deﬁne
δ = f (x) −f (y) −∇f (y)⊺(x −y).
Then
δ = φ(1) −φ(0) −φ′(0) =
Z 1
0
[φ′(t) −φ′(0)]dt.
Therefore,
|δ| =

Z 1
0
[φ′(t) −φ′(0)]dt
 ≤
Z 1
0
|φ′(t) −φ′(0)|dt ≤
Z 1
0
γt ∥x −y∥2
2dt
≤γ
2 ∥x −y∥2
2 ,
as we intended to show.
The last result can be extended in a natural way for vector-valued functions. To
state it, we need a few deﬁnitions.

B.3 Functions of Several Variables
869
Deﬁnition B.55 (Jacobian11). Let m, n ∈N and Ω⊆Rn be open. For k ∈N0,
we say that the function f : Ω→Rm belongs to Ck(Ω; Rm) if and only if, for each
i ∈{1, . . . , m}, we have fi ∈Ck(Ω). For f ∈C1(Ω; Rm) and y ∈Ω, the Jacobian
of f at y is the matrix Jf (y) ∈Rm×n with components
[Jf (y)]i,j = ∂fi(y)
∂xj
,
i = 1, . . . , m,
j = 1, . . . , n.
Theorem B.56 (Taylor’s Theorem IV in Rd). Let d ∈N, Ω⊂Rd be a bounded,
open, convex set. Assume that f ∈C1(Ω; Rd), with the additional regularity that
there is γ > 0 such that, for all x, y ∈Ω,
∥Jf (x) −Jf (y)∥2 ≤γ ∥x −y∥2 .
Then, for all x, y ∈Ω,
∥f (x) −f (y) −Jf (y)(x −y)∥2 ≤γ
2 ∥x −y∥2
2 .
Proof. See Problem B.16.
For vector-valued functions, a Mean Value Theorem like Theorem B.30 does not
hold.
Example B.4
Let f : [0, 2π] →R2 be given by
f (t) =

cos(t)
sin(t)

,
f ′(t) =

−sin(t)
cos(t)

.
Notice that f (2π) = f (0), yet the derivative never vanishes.
The following result is a generalization of the Mean Value Theorem B.30.
Theorem B.57 (Mean Value Theorem). Let m, n ∈N, Ω⊂Rn be a bounded,
open, convex set, and a, b ∈Ω. Let f ∈C(Ω; Rm) be diﬀerentiable on the open
segment with endpoints a and b. Then we have
∥f (b) −f (a)∥2 ≤sup
t∈[0,1]
∥Jf (tb + (1 −t)a)∥2 ∥b −a∥2 .
If, in addition, f ∈C1(Ω; Rm), then
f (b) −f (b) =
Z 1
0
Jf (tb + (1 −t)a)(b −a)dt.
We have already deﬁned the gradient. There are other diﬀerential operators, i.e.,
expressions involving partial derivatives of a function, that are useful in applications.
11 Named in honor of the German mathematician Carl Gustav Jacob Jacobi (1804–1851).

870
Basic Analysis Review
Deﬁnition B.58 (diﬀerential operators). Let d ∈N and Ω⊂Rd be a domain. For
v ∈C2(Ω), the Laplacian12 at the point x ∈Ωis
∆v(x) =
d
X
i=1
∂2v(x)
∂x2
i
= tr Hv(x) = Hv(x): Id.
For w ∈C1(Ω; Rd), the divergence at the point x ∈Ωis
∇· w(x) =
d
X
i=1
∂wi(x)
∂xi
.
If d = 2, the curl at the point x ∈Ωis
∇× w(x) = ∂w2(x)
∂x1
−∂w1(x)
∂x2
.
Finally, if d = 3, the curl is
∇× w(x) = det


e1
e2
e3
∂
∂x1
∂
∂x2
∂
∂x3
w1
w2
w3

(x)
=
∂w3(x)
∂x2
−∂w2(x)
∂x3

e1 +
∂w1(x)
∂x3
−∂w3(x)
∂x1

e2
+
∂w2(x)
∂x1
−∂w1(x)
∂x2

e3.
We need to mention a few words about integration of functions of several
variables. However, a rigorous presentation will lead us too astray. For this reason,
we will assume that the reader is familiar with the fact that, in Rd with d > 1, one
cannot integrate over arbitrary subsets, and call those over which integration, in
the Riemann sense, can be carried out admissible. Much as in the one-dimensional
case, if Ω⊂Rd is an admissible set, we denote by f ∈R(Ω; C) the fact that
f : Ω→C is Riemann integrable over Ω.
Theorem B.59 (integral identities). Let d ∈N and Ω⊂Rd be a bounded domain
with suﬃciently smooth boundary and exterior unit boundary normal n: ∂Ω→Rd.
The following identities hold.
1. Integration by parts: If u, v ∈C1(¯Ω),
Z
Ω
∂u(x)
∂xi
v(x)dx =
Z
∂Ω
u(x)v(x)n(x) · ei dS(x) −
Z
Ω
u(x)∂v(x)
∂xi
dx.
2. First Green’s identity:13 If v ∈C1(¯Ω) and w ∈C2(¯Ω),
Z
Ω
∇v(x) · ∇w(x)dx = −
Z
Ω
v(x)∆w(x)dx +
Z
∂Ω
v(x)∂w
∂n (x)dS(x),
where ∂w
∂n (x) = n(x) · ∇w(x).
12 Named in honor of the French scholar and polymath Pierre-Simon, Marquis de Laplace
(1749–1827).
13 Named in honor of the British mathematical physicist George Green (1793–1841).

B.4 Sequences of Functions
871
3. Second Green’s identity: If v, w ∈C2(¯Ω),
Z
Ω
(v(x)∆w(x) −∆v(x)w(x)) dx =
Z
∂Ω

v(x)∂w
∂n (x) −∂v
∂n (x)w(x)

dS(x).
4. If v ∈C1(¯Ω; Rd) and w ∈C1(¯Ω),
Z
Ω
∇· v(x)w(x)dx =
Z
∂Ω
v(x) · n(x)w(x)dS(x) −
Z
Ω
v(x) · ∇w(x)dx.
B.4
Sequences of Functions
Finally, we state some facts about sequences of functions, their limits, and the
passage to the limit under the derivative and integral.
Deﬁnition B.60 (uniform convergence). Suppose that d ∈N and A ⊆Rd. Let
{fn}n∈N be a sequence of functions fn : A →C. We say that {fn}n∈N converges
uniformly to a function f : A →C if and only if, for every ε > 0, there is an N ∈N
such that if n is any integer satisfying n ≥N, then
sup
x∈A
|fn(x) −f (x)| ≤ε.
We say that {fn}n∈N is uniformly Cauchy if and only if, given any ε > 0, there is
an N ∈N such that if n and m are any two integers satisfying n, m ≥N, then
sup
x∈A
|fn(x) −fm(x)| ≤ε.
As in the case of vectors in Cd, sequences of functions converge if and only if
they are Cauchy.
Theorem B.61 (uniform Cauchy criterion). Suppose that d ∈N, A ⊆Rd, and
{fn}n∈N is a sequence of functions fn : A →C. {fn}n∈N converges uniformly to a
function f : A →C if and only if {fn} is uniformly Cauchy. The limit function f is
unique.
Theorem B.62 (uniform limit of continuous functions). Suppose that d ∈N and
A ⊆Rd is compact. Let {fn}n∈N ⊂C(A; C). If {fn}n∈N converges uniformly to a
function f : A →C, then f ∈C(A; C).
Theorem B.63 (Weierstrass M-Test14). Suppose that d ∈N and A ⊆Rd is
compact. Let {fn}n∈N ⊂C(A; C) be such that
|fn(x)| ≤Mn,
∀x ∈A.
If P∞
n=1 Mn < ∞, then the series P∞
n=1 fn(x) converges uniformly and absolutely
on A. In other words, the sequence of partial sums
Sm(x) =
m
X
n=1
fn(x),
Tm(x) =
m
X
n=1
|fn(x)|
14 Named in honor of the German mathematician Karl Theodor Wilhelm Weierstrass
(1815–1897).

872
Basic Analysis Review
both converge uniformly on A to functions S ∈C(A; C) and T ∈C(A), respectively.
Theorem B.64 (uniform limits and derivatives). Suppose that [a, b] ⊆R is
compact and {fn}n∈N ⊂C1([a, b]; C). Assume that, for some x0 ∈[a, b], fn(x0)
converges and {f ′
n}n∈N converges uniformly to some function g : [a, b] →C. Then
{fn}n∈N converges uniformly to a diﬀerentiable function f : [a, b] →C and f ′ = g.
Finally, we discuss uniform limits and integrals.
Theorem B.65 (uniform limits and integrals). Let d ∈N and Ω⊂Rd be bounded
and admissible. Assume that {fn}n∈N ⊂R(Ω; C) converges uniformly with limit
f : Ω→C. Then f ∈R(Ω; C) and its integral can be computed as
lim
n→∞
Z
Ω
fn(x)dx =
Z
Ω
f (x)dx.
Problems
B.1
Exhibit a sequence that converges exactly quadratically (q = 2).
B.2
Exhibit a sequence that converges exactly cubically (q = 3).
B.3
Let f : (0, 1) →R be deﬁned by f (x) = 1/x. Show that f is continuous on
(0, 1) but not uniformly continuous on (0, 1).
B.4
Show that if A ⊆R is not compact, then Cb(A) ⊊C(A).
Hint: Consider the previous problem.
B.5
Let α ∈(0, 1). Let the function fα : [−1, 1] →R be deﬁned by
fα(x) = |x|α.
Show that, for every β ∈(0, α], we have fα ∈C0,β([−1, 1]) and that there is no
ε > 0 for which fα ∈C0,α+ε([−1, 1]).
B.6
Show that the function f : [−1, 1] →R deﬁned by f (x) = |x| is such that
f ∈C0,1([−1, 1])\C1([−1, 1]).
B.7
Let f : [−1, 1] →R be deﬁned by
f (x) = x ln |x|.
Show that, for every α ∈(0, 1), we have f ∈C0,α([−1, 1]); yet f /∈C0,1([−1, 1]).
B.8
Exhibit a function f ∈C0([−1, 1]) ∩C1((−1, 1)), but f ̸∈C1([−1, 1]).
B.9
Exhibit a function f ∈C1([a, b]) ∩C2((a, b)) such that f ̸∈C2([a, b]).
B.10
Suppose that f ∈C([a, b]) and there is a point c ∈(a, b) such that f (c) ̸=
0. Prove that there is a δ > 0 such that |f (x)| > 0 for all x ∈(c −δ, c + δ).
Hint: For this problem, you need to use the deﬁnition of continuity.
B.11
Suppose that f ∈C2([a, b]). If f ′′(x) > 0, for all x ∈[a, b], and there exists
a point c ∈(a, b) such that f ′(c) = 0, show that f has a minimum in the interval
[a, b] at the point c.
B.12
Prove Theorem B.51.
B.13
Suppose that f ∈C2(B(a, r)), ∇f (a) = 0, and Hf (ξ), the Hessian matrix
of f evaluated at ξ, is positive deﬁnite for all ξ ∈B(a, r). Prove that f has a
minimum in the closed ball B(a, r) at the point a.
B.14
Prove Theorem B.52.

Problems
873
B.15
Is it possible to use the Integral Mean Value Theorem B.41 to prove that
the remainder terms in the Taylor Theorems B.39 and B.31 are equivalent? If so,
give a proof of the equivalence and carefully state your assumptions.
B.16
Prove Theorem B.56.

Appendix C Banach Fixed Point
Theorem
In this supplement, we present the well-known Banach contraction mapping
principle, both for Banach spaces as well as for complete metric spaces.
C.1
Contractions and Fixed Points in Banach Spaces
Let us ﬁrst give a very general convergence theory based on the contraction
mapping theorem. This section requires some familiarity with Cauchy sequences.
Deﬁnition C.1 (contraction). Let X be a normed space with norm ∥·∥X. The map
T : X →X is called a contraction if and only if there is a real number q ∈(0, 1),
called the contraction parameter, such that
∥T(x) −T(y)∥X ≤q∥x −y∥X,
∀x, y ∈X.
Deﬁnition C.2 (ﬁxed point). Let T : X →X. The element x ∈X is called a ﬁxed
point of T if and only if
T(x) = x.
As is well known, all ﬁnite-dimensional normed spaces are complete, i.e., every
Cauchy sequence converges. This is not the case in inﬁnite dimensions. As, for
instance, Example 16.3 shows, there are inﬁnite-dimensional normed spaces that
are not complete. The norm in this case matters.
Deﬁnition C.3 (Banach space1). A normed space (X, ∥· ∥X) that is complete is
called Banach space.
The reader will see several examples of Banach spaces throughout the text. Their
usefulness is particularly evident in Parts II, IV, and V.
We have now arrived at the well-known Banach Contraction Mapping Theorem.
Theorem C.4 (Banach Fixed Point Theorem). Let X be a complete normed linear
space (a Banach space) and T : X →X a contraction, with contraction parameter
q ∈(0, 1). Then T has a unique ﬁxed point ¯x ∈X. Moreover, if x0 ∈X is any
point, then the sequence {xk}∞
k=0 ⊂X generated by the iteration scheme
xk+1 = T(xk),
k ≥0
(C.1)
1 Named in honor of the Polish mathematician Stefan Banach (1892–1945).

C.1 Contractions and Fixed Points in Banach Spaces
875
converges to ¯x. In addition, we have the following error estimates:
∥xn −¯x∥X ≤
qn
1 −q ∥x0 −x1∥X ,
(C.2)
∥xn −¯x∥X ≤
q
1 −q ∥xn−1 −xn∥X ,
(C.3)
∥xn −¯x∥X ≤q ∥xn−1 −¯x∥X .
(C.4)
Proof. We ﬁrst observe that, for any x0, since T : X →X, the sequence generated
by (C.1) is well deﬁned. In addition, owing to the fact that T is a contraction,
∥xn+1 −xn∥X = ∥T(xn) −T(xn−1)∥X ≤q ∥xn −xn−1∥X ≤· · · ≤qn ∥x1 −x0∥X .
Thus, for any m > n ≥1, we obtain that
∥xm −xn∥X ≤
m−n−1
X
j=0
∥xn+j+1 −xn+j∥X ≤
m−n−1
X
j=0
qn+j ∥x1 −x0∥X
≤
qn
1 −q ∥x1 −x0∥X ,
where, since q ∈(0, 1), we used the convergence of the geometric series. This
proves that {xk}∞
k=0 is a Cauchy sequence in X. Since X is a Banach space, there
is a unique limit point ¯x ∈X. Since T is a contraction, it is also continuous;
therefore,
¯x = lim
n→∞xn+1 = lim
n→∞T(xn) = T

lim
n→∞xn

= T(¯x).
This shows that ¯x is a ﬁxed point of T; we have proved existence.
To prove the uniqueness of the ﬁxed point, suppose that ¯xj, j = 1, 2, are ﬁxed
points of T. Then
∥¯x1 −¯x2∥X ≤q ∥¯x1 −¯x2∥X ,
which implies that
(1 −q) ∥¯x1 −¯x2∥X ≤0.
The only possibility is that ∥¯x1 −¯x2∥X = 0, which implies that ¯x1 = ¯x2. This shows
uniqueness.
Now we prove the estimates. To this end, ﬁx n ≥1 and suppose that m > n.
Then
∥xn −¯x∥X ≤∥xn −xm∥X + ∥xm −¯x∥X ≤
qn
1 −q ∥x1 −x0∥X + ∥xm −¯x∥X .
Since xm →¯x, for any ε > 0, there is an M ∈N such that, if m ≥M, then
∥xm −¯x∥X ≤ε.
Thus, for every ﬁxed n, there is an m ≥max(n + 1, M) such that
∥xn −¯x∥X ≤
qn
1 −q ∥x1 −x0∥X + ε.

876
Banach Fixed Point Theorem
Since ε > 0 is arbitrary, for every ﬁxed n,
∥xn −¯x∥X ≤
qn
1 −q ∥x1 −x0∥X ,
which proves (C.2). To get (C.4), observe that
∥xn −¯x∥X = ∥T(xn) −T(¯x)∥X ≤q ∥xn−1 −¯x∥X .
Finally, using (C.4), we have
∥xn−1 −¯x∥X ≤∥xn−1 −xn∥X + ∥xn −¯x∥X ≤∥xn−1 −xn∥X + q ∥xn−1 −¯x∥X ,
which implies that
(1 −q) ∥xn−1 −¯x∥X ≤∥xn−1 −xn∥X ≤q ∥xn−2 −xn−1∥X .
Thus,
∥xn−1 −¯x∥X ≤
q
1 −q ∥xn−2 −xn−1∥X .
Shifting the index, we have (C.4):
∥xn −¯x∥X ≤
q
1 −q ∥xn −xn−1∥X .
All the statements have been proved.
C.2
The Contraction Mapping Principle in Metric Spaces
Let us slightly generalize the contraction mapping principle to the case of metric
spaces, as this is relevant in one application. We begin by realizing that the notions
of convergence, continuity, and many others that we introduced for normed spaces
can be naturally generalized. The fact that the space was a vector space was not
relevant, only that distances between elements of the underlying set can be deﬁned.
For this reason, we introduce the following notion.
Deﬁnition C.5 (metric space). Let X be a set and d: X × X →R. We say that
the pair (X, d) is a metric space if:
1. For every x, y ∈X, d(x, y) = 0 if and only if x = y.
2. Symmetry: For all x, y ∈X, we have that d(x, y) = d(y, x).
3. Triangle inequality: For all x, y, z ∈X, we have d(x, z) ≤d(x, y) + d(y, z).
In this case, the function d is called a metric or distance.
The reader has already seen many examples of metric spaces.
Example C.1
Let I ⊂R be an interval. This is a metric space with
d(x, y) = |x −y|.

C.2 The Contraction Mapping Principle in Metric Spaces
877
Example C.2
Any normed space (X, ∥· ∥X) is a metric space with
d(x, y) = ∥x −y∥X .
In fact, any nonempty subset of X is also a metric space with the same distance.
Example C.3
Let S ⊂C be the unit circle, i.e.,
S = {z ∈C||z| = 1} .
While this is a metric space with the metric induced from C, there is another,
perhaps more natural, distance. This is the so-called arcwise or intrinsic distance.
Namely, since for every z ∈S we have
z = eiθ,
θ ∈[0, 2π),
we deﬁne
dS(z1, z2) = dS(eiθ1, eiθ2) = |θ1 −θ2|.
We leave it to the reader to verify that this is indeed a distance.
All the notions of convergence and continuity can be extended to metric spaces.
We only mention those that are relevant for our discussion here.
Deﬁnition C.6 (convergence). Let (X, d) be a metric space. A sequence is a
mapping N →X, which we usually denote by {xn}n∈N ⊂X. We say that a sequence
{xn}n∈N ⊂X converges to x ∈X if, for every ε > 0, there is N ∈N such that,
whenever n ≥N,
d(xn, x) < ε.
Let {xn}n∈N ⊂X be a sequence. We say that it is Cauchy if, for every ε > 0, there
is N ∈N such that, whenever m, n ≥N, we have
d(xn, xm) < ε.
Proposition C.7 (convergent =⇒Cauchy). Let (X, d) be a metric space. If the
sequence {xn}n∈N ⊂X converges, then it is Cauchy.
Proof. See Problem C.6.
The converse is not always true and so we introduce the following.
Deﬁnition C.8 (complete space). Let (X, d) be a metric space. We say that it is
complete if every Cauchy sequence converges.
Example C.4
Every Banach space is a complete metric space.

878
Banach Fixed Point Theorem
Example C.5
The set (0, 1] ⊂R with the metric induced from R is a metric
space, but it is not complete. Indeed, for every n ∈N, we have that 1
n ∈(0, 1] and
the sequence { 1
n}n∈N is Cauchy (as it converges in R). However, the limit (which
is zero) does not lie in (0, 1] and so, as a metric space, this is not complete.
Example C.6
Let I ⊂R be a compact interval and B ⊂Rd. Recall that, by
C(I; B),
we denote the subset of C(I; Rd) of functions whose values belong to B. If B is
compact, then the pair

C(I; B), ∥· ∥L∞(I;Rd)

is a complete metric space.
We can also deﬁne continuity between metric spaces.
Deﬁnition C.9 (continuity). Let (X, dX) and (Y, dY ) be metric spaces. We say
that the mapping T : X →Y is continuous at x0 ∈X if, for every ε > 0, there is
δ > 0 such that, whenever
dX(x, x0) < δ,
we necessarily have
dY (T(x), T(x0)) < ε.
Let us now extend the notions of the previous sections to the setting of a metric
space.
Deﬁnition C.10 (contraction). Let (X, d) be a metric space. We say that the
mapping T : X →X is a contraction if and only if there is q ∈(0, 1) such that
d (T(x), T(y)) ≤qd (x, y) ,
∀x, y ∈X.
The number q is usually referred to as the contraction parameter of T.
We can now state the contraction mapping principle for metric spaces.
Theorem C.11 (contraction mapping). Let (X, d) be a complete metric space and
T : X →X a contraction, with contraction parameter q ∈(0, 1). Then T has a
unique ﬁxed point ¯x ∈X. Moreover, if x0 ∈X is any point, then the sequence
{xk}∞
k=0 ⊂X generated by the iteration scheme
xk+1 = T(xk),
k ≥0
(C.5)
converges to ¯x. In addition, we have the following error estimates:
d(xn, ¯x) ≤
qn
1 −q d(x0, x1),
d(xn, ¯x) ≤
q
1 −q d(xn−1, xn),
d(xn, ¯x) ≤qd(xn−1, ¯x).

Problems
879
Proof. The proof is a mere exercise in adapting the notation used in Theorem C.4
to this setting; see Problem C.8.
Problems
C.1
Let X be a ﬁnite-dimensional vector space. Assume that the mapping f : X →
X is such that there is an n ∈N for which f n is a contraction. Show that f has a
unique ﬁxed point. Notice that f n(x) = f (f (. . . f (
|
{z
}
n times
x))).
C.2
Let X be a ﬁnite-dimensional vector space and f1, f2 : X →X two mappings
that commute, i.e., f1(f2(x)) = f2(f1(x)) for all x ∈X. Show that if f2 has a unique
ﬁxed point x0, then x0 is a ﬁxed point of f1.
C.3
Let X be a ﬁnite-dimensional vector space and f : X →X a contraction.
Show that, for every y ∈X, the equation
x = f (x) + y
has a unique solution. Denote this solution by x(y). Show that x(y) is a continuous
function of y.
C.4
The purpose of this problem is to show another proof of the Lax–Milgram
Lemma (see Theorem 23.10). Let us recall the setting: H is a Hilbert space; A is
a bounded and coercive bilinear form on H; and F ∈H′. We seek u ∈H such that
A(u, v) = F(v),
∀v ∈H.
(C.6)
a)
Show that, if x ∈H is such that
(x, y)H = 0,
∀y ∈H,
then x = 0.
b)
Let θ > 0. Show that problem (C.6) is equivalent to ﬁnding u ∈H such that
(u, v)H = (u, v)H −θ [A(u, v) −F(v)] ,
∀v ∈H.
c)
Deﬁne Pθ : H →H via the relation w = Pθ(u) if
(w, v)H = (u, v)H −θ [A(u, v) −F(v)] ,
∀v ∈H.
Use the previous two items to show that Pθ is well deﬁned and that (C.6) is
equivalent to ﬁnding a ﬁxed point of Pθ.
d)
Let α, M denote, respectively, the coercivity and boundedness constants of A.
Show that, for θ ∈(0, 2α/M2), the mapping Pθ is a contraction.
Hint: Deﬁne A: H →H by (Av, w)H = A(v, w). Show that this is a linear
mapping. How can you bound ∥Av∥H and (Av, v)H? How do you write Pθ(w)
in terms of A?
C.5
Provide all the details for Example C.3.
C.6
Prove Proposition C.7.
C.7
Let (X, d) be a metric space and T : X →X be a contraction. Show that T
is continuous at every point x ∈X.
C.8
Prove Theorem C.11.

Appendix D A (Petting) Zoo
of Function Spaces
In this appendix, we collect the (vector) spaces of functions that appear throughout
the text. We also present, mostly without proof, their most relevant properties that
will be useful to us. A careful and detailed exposition of all these matters is certainly
beyond the scope of our text, and there are many useful references for this. We
refer the reader, for instance, to [2, 55].
Before we embark on our presentation, we must begin with a remark on notation.
Remark D.1 (notation). Throughout, we will denote spaces of functions by
symbols of the form X(A; B). Here, X will possibly be decorated by sub- and
super-indices, and others, and this will indicate the property that deﬁnes the space
of functions. A is the domain of these functions, which will usually be an open or
closed subset of Rd for some d ∈N. The range of these functions is B, which is
usually a vector space or a subset of it. In an eﬀort to alleviate notation, if B = R,
we will suppress this, i.e., X(A) = X(A; R).
D.1
Spaces of Smooth Functions
We refer the reader to Appendix B for the deﬁnitions of continuity, H¨older
continuity, and diﬀerentiability of a function.
Deﬁnition D.2 (spaces of smooth functions). Let d ∈N and Ω⊂Rd be a bounded
domain. For k ∈N0 and α ∈[0, 1], we denote by
Ck,α(Ω),
Ck,α(Ω)
the vector spaces of functions that are k-times continuously diﬀerentiable on
Ωor Ωrespectively, such that the kth derivative is H¨older continuous of order α
on Ωor Ωrespectively.
In addition, to simplify notation, we set
C(Ω) = C0,0(Ω),
Ck(Ω) = Ck,0(Ω),
and
C(Ω) = C0,0(Ω),
Ck(Ω) = Ck,0(Ω).
Finally, we comment that, by arguing component-wise, spaces of vector-valued
functions can be deﬁned in a similar manner.

D.1 Spaces of Smooth Functions
881
Remark D.3 (notation). In the case d = 1, we will usually deal with functions
deﬁned on a compact interval [a, b] ⊂R or its interior: the open interval (a, b). In
this case, to alleviate notation, we set
Ck,α(a, b) = Ck,α((a, b)).
We must immediately remark that whether the set is open or closed makes a
substantial diﬀerence.
Example D.1
Let f (x) = x−1. Then f ∈C(0, 1), but f /∈C([0, 1]). Notice also
that
Z 1
0
1
x1+ε dx = +∞,
∀ε ≥0.
We now introduce Lp-norms on continuous functions.
Deﬁnition D.4 (Lp-norm). Let d ∈N and Ω⊂Rd be a bounded domain. For
each p ∈[1, ∞), deﬁne the function ∥· ∥Lp(Ω;C) : C(¯Ω; C) →[0, ∞) via
∥f ∥Lp(Ω;C) =
Z
Ω
|f (x)|pdx
1/p
,
∀f ∈C(¯Ω; C).
This is called the Lp-norm, or just p-norm. Similarly, the function ∥· ∥L∞(Ω;C) :
C(¯Ω; C) →[0, ∞), deﬁned via
∥f ∥L∞(Ω;C) = sup
x∈Ω
|f (x)|,
∀f ∈C(¯Ω; C),
is called the L∞-norm, or just ∞-norm.
The deﬁnitions above require several remarks. First, we notice that, once again,
the fact that the domain of the functions is closed is essential, regardless of the
value of p; see Example D.1. In addition, in the case p = ∞, it does not matter if
the supremum is taken over the open or closed domain. Finally, owing to Theorem
B.47, the supremum is actually a maximum.
Proposition D.5 (properties of Lp-norms). Let d ∈N and Ω⊂Rd be a bounded
domain. For every p ∈[1, ∞], the function ∥· ∥Lp(Ω;C) is a norm on C(¯Ω; C). In
particular, in the case p = 2, the L2-norm is generated by the inner product
(f , g)L2(Ω;C) =
Z
Ω
f (x)g(x) dx,
∀f , g ∈C(¯Ω; C).
Owing to this, the integral Minkowski inequality1
∥f + g∥Lp(Ω;C) ≤∥f ∥Lp(Ω;C) + ∥g∥Lp(Ω;C) ,
∀f , g ∈C(¯Ω; C)
holds. Finally, if p, q ∈[1, ∞] are conjugate pairs, i.e.,
1
p + 1
q = 1,
1 Named in honor of the German mathematician Hermann Minkowski (1864–1909).

882
A (Petting) Zoo of Function Spaces
with the understanding that 1
0 = ∞and 1
∞= 0, then the integral H¨older inequality2
Z
Ω
|f (x)||g(x)|dx ≤∥f ∥Lp(Ω;C) ∥g∥Lq(Ω;C) ,
∀f , g ∈C(¯Ω; C)
holds.
Proof. See Problem D.1
Since C(¯Ω; C) is inﬁnite dimensional, its completeness under a norm is not a
trivial matter. The following result shows this.
Theorem D.6 (completeness). Let d ∈N and Ω⊂Rd be a bounded domain. For
every 1 ≤p < ∞, the pair

C(¯Ω; C), ∥· ∥Lp(¯Ω;C)

is a normed space, though not a Banach space. On the other hand, the pair

C(¯Ω; C), ∥· ∥L∞(Ω;C)

is a Banach space.
Proof. The completeness is a consequence of Theorem B.62. The other results
are classical and the proofs can be found, for example, in the books by Rudin
[76, 77].
Now, although this can be done much more generally, we shall only need weighted
Lp-norms for functions of one variable. Thus, we introduce them only in this case.
We begin with the deﬁnition of a weight.
Deﬁnition D.7 (weight). Let [a, b] ⊂R be a compact interval. A function
w : [a, b] →R is called a weight on [a, b] if and only if it is nonnegative on
[a, b], it vanishes only at possibly a ﬁnite number of points, w ∈C(a, b), and it is
Riemann integrable on the interval [a, b], i.e.,
0 <
Z b
a
w(x)dx < ∞.
Deﬁnition D.8 (weighted Lp-norm). Let w be a weight function on the interval
[a, b] ⊂R and p ∈[1, ∞). The weighted Lp-norm of f ∈C([a, b]; C) is
∥f ∥Lp
w (a,b;C) =
Z b
a
w(x)|f (x)|pdx
1/p
.
The function
( · , · )L2w (a,b;C) : C([a, b]; C) × C([a, b]; C) →C,
(f , g)L2w (a,b;C) : C([a, b]; C) 7→
Z b
a
w(x)f (x)g(x) dx
is called the weighted L2-inner product.
2 Named in honor of the German mathematician Otto Ludwig H¨older (1859–1937).

D.1 Spaces of Smooth Functions
883
Theorem D.9 (properties of weighted Lp-norms). Let w be a weight function on
[a, b]. Then, for all p ∈[1, ∞), the weighted Lp-norms are norms on C([a, b]; C).
In particular, the weighted Minkowski inequality, i.e.,
∥f + g∥Lp
w (a,b;C) ≤∥f ∥Lp
w (a,b;C) + ∥g∥Lp
w (a,b;C) ,
∀f , g ∈C([a, b]; C),
holds. In addition, if p, q ∈(1, ∞) are conjugate exponent pairs, i.e.,
1 = 1
p + 1
q ,
then the weighted H¨older inequality is valid
Z b
a
w(x)|f (x)| |g(x)|dx ≤∥f ∥Lp
w (a,b;C) ∥g∥Lq
w (a,b;C) .
When p = ∞, we have the following variant of the weighted H¨older inequality:
Z b
a
w(x)|f (x)| |g(x)|dx ≤∥f ∥L∞(a,b;C) ∥g∥L1w (a,b;C) .
Finally, for any f ∈C([a, b]; C),
∥f ∥Lp
w (a,b;C) ≤

∥w∥L1(a,b)
1/p
∥f ∥L∞(a,b;C) .
(D.1)
Proof. The proof that these are indeed norms is left to the reader as an exercise;
see Problem D.1.
Let us prove (D.1). Let f ∈C([a, b]; C). Then there is x0 ∈[a, b] such that
∥f ∥L∞(a,b;C) = |f (x0)|
=⇒
|f (x)| ≤|f (x0)|, ∀x ∈[a, b].
Then, since w is a weight, it is nonnegative and we can estimate
∥f ∥p
Lp
w (a,b;C) =
Z b
a
w(x)|f (x)|pdx
≤|f (x0)|p
Z b
a
w(x)dx
= ∥f ∥p
L∞(a,b;C) ∥w∥L1(a,b) .
Taking pth roots, (D.1) follows.
Clearly, ∥· ∥Lp
1(a,b) = ∥· ∥Lp(a,b), so the issue presented in Theorem D.6 remains
in this case.
Theorem D.10 (not Banach). Let w be a weight function on [a, b]. For every
1 ≤p < ∞, the pair

C([a, b]; C), ∥· ∥Lp
w (a,b;C)

is a normed space, though not a Banach space.
Proof. These facts are well known and the proofs can be found, for example, in
the books by Rudin [76, 77].
The norm for the case of higher order smoothness is deﬁned similarly.

884
A (Petting) Zoo of Function Spaces
Deﬁnition D.11 (Ck,α-norm). Let d ∈N, Ω⊂Rd be a bounded domain, k ∈N0,
and α ∈[0, 1]. We deﬁne, for all ∈Ck,α(¯Ω; C),
∥f ∥Ck,α(¯Ω;C) = max





max
β∈Nd
0
|β|≤k
Dβf

L∞(Ω;C) , max
β∈Nd
0
|β|=k
sup
x,y∈¯Ω
x̸=y
|Dβf (x) −Dβf (y)|
∥x −y∥α
2





.
Clearly, ∥· ∥C0,0(¯Ω;C) = ∥· ∥L∞(Ω;C). These norms make these spaces complete.
Proposition D.12 (completeness). Let d ∈N and Ω⊂Rd be an open bounded
domain. For every k ∈N0 and α ∈[0, 1], the pair

Ck,α(¯Ω; C), ∥· ∥Ck,α(¯Ω;C)

is a Banach space.
D.1.1
Periodic Functions
In many problems, periodicity appears as a natural constraint; therefore, we must
deal with spaces of periodic functions.
Deﬁnition D.13 (periodic function). Let L > 0 be given. Suppose that f : R →C.
We say that f is L-periodic if and only if
f (x + nL) = f (x),
∀n ∈Z,
∀x ∈R.
For any m ∈N0, we deﬁne Cm
p (0, L; C) as the set of functions
Cm
p (0, L; C) = {f ∈Cm(R; C) | f (x + nL) = f (x), ∀n ∈Z, ∀x ∈R} .
The set Cm
p (0, L) is deﬁned similarly. We also deﬁne the set of complex-valued,
smooth, mean-zero L-periodic functions as
˚Cm
p (0, L; C) =

u ∈Cm
p (0, L; C)

Z L
0
u(x)dx = 0

.
Finally, deﬁne
C∞
p (0, L; C) =
∞
\
m=0
Cm
p (0, L; C).
The set C∞
p (0, L) is deﬁned similarly.
For simplicity, we will work with one-periodic functions, i.e., with L = 1. All of
the results can be recast in the case for which L > 0 is arbitrary, without much
diﬃculty.
Observe that a one-periodic function is completely characterized by its values
on [0, 1]. Thus, spaces of smooth periodic functions can be normed with any of
the norms deﬁned before, assuming that the domain is the unit interval [0, 1]. On
the other hand, it is important to realize that, for any m ∈N0, Cm
p (0, 1; C) ̸=
Cm(0, 1; C). In this situation, the periodicity is vital to its deﬁnition. For instance,
if f ∈C(0, 1; C), then it can be periodically extended to all of R, but the extension
will not be in C(R; C) unless f (0) = f (1).

D.2 Spaces of Integrable Functions
885
D.2
Spaces of Integrable Functions
The motivation for the introduction of the so-called Lebesgue spaces3 is, essentially,
the negative results of Theorem D.6 and Theorem D.10. We wish to have spaces
of functions that are complete under these norms. To properly develop them and
deﬁne them would require us to develop measure theory [77] or the theory of the
Daniell integral4 [75]. We will not do this.
Deﬁnition D.14 (Lp space). Let d ∈N and Ω⊂Rd be a domain. For p ∈[1, ∞),
we deﬁne
Lp(Ω; C) =

f : Ω→R

Z
Ω
|f (x)|pdx < ∞

.
We also deﬁne
L∞(Ω; C) =

f : Ω→R
 ess sup
x∈Ω
|f (x)| < ∞

,
where ess sup denotes the essential supremum. The Lp-norms are deﬁned as before.
In the one-dimensional case, we can do the same with weighted integrals.
Deﬁnition D.15 (w-square integrable). Let [a, b] ⊂R be a compact interval and
w be a weight on [a, b]. We say that f is w-square integrable if and only if
Z b
a
w(x)|f (x)|2dx < ∞.
The set of all such functions is labeled L2
w(a, b; C). When the functions are real
valued, we write L2
w(a, b). For two functions f , g ∈L2
w(a, b; C), we deﬁne the
weighted inner product
(f , g)L2w (a,b;C) =
Z b
a
w(x)f (x)g(x) dx,
where the overline denotes complex conjugation. The conjugation is dropped if g
is real valued.
Remark D.16 (equivalence). The reader familiar with Lebesgue integration will
recall that two measurable functions are equivalent in Lp(Ω; C), or L2
w(a, b; C), if
and only if the set on which they diﬀer has (Lebesgue) measure zero. Strictly
speaking, therefore, we think of equivalence classes of functions, rather than
functions, as the elements of Lp(Ω; C), or L2
w(a, b; C). However, for those not
familiar, it is usually ﬁne to think of the elements of the spaces as ordinary functions.
One of the foundational facts of integration theory is the completeness of Lp
spaces.
3 Named in honor of the French mathematician Henri Le´on Lebesgue (1875–1941).
4 Named in honor of the British mathematician Percy John Daniell (1889–1946).

886
A (Petting) Zoo of Function Spaces
Theorem D.17 (completeness). Let d ∈N and Ω⊂Rd be a domain. For every
p ∈[1, ∞], the pair

Lp(Ω; C), ∥· ∥Lp(Ω;C)

is a Banach space. In particular, the pair

L2(Ω; C), ∥· ∥L2(Ω;C)

is a complex Hilbert space. Similarly, if [a, b] ⊂R is a compact interval and w is a
weight function on [a, b], then the pair
 L2
w(a, b), ( · , · )L2w (a,b)

is a real Hilbert space and ( · , · )L2w (a,b) is a real inner product. The pair
 L2
w(a, b; C), ( · , · )L2w (a,b;C)

is a complex Hilbert space and ( · , · )L2w (a,b;C) is a complex inner product. As is
standard, the norm is deﬁned as
∥f ∥L2w (a,b;C) =
q
(f , f )L2w (a,b;C),
∀f ∈L2
w(a, b; C).
Proof. See [77].
D.2.1
Periodic Functions
By L2
p(0, 1; C) we denote the set of all one-periodic, Lebesgue measurable functions
f : R →C with the property that, for any compact interval [a, b] ⊂R,
Z b
a
|f (x)|2dx < ∞.
This is a vector space over the reals under the usual operations of function addition
and scalar multiplication. For two functions f , g ∈L2
p(0, 1; C), we deﬁne the inner
product
(f , g)L2(0,1;C) =
Z 1
0
f (x)g(x) dx,
where, as usual, the overline denotes complex conjugation. The set L2
p(0, 1) is
deﬁned similarly. Finally, by ˚L2(0, 1; C), we denote the space of complex-valued,
mean-zero square integrable functions on (0, 1), i.e.,
˚L2(0, 1; C) =

u ∈L2(0, 1; C)

Z 1
0
u(x)dx = 0

.
Some standard arguments in integration theory yield the following result.
Theorem D.18 (completeness). The pair
 L2
p(0, 1), ( · , · )L2(0,1)

=
 L2(0, 1), ( · , · )L2(0,1)


D.3 Sobolev Spaces
887
is a real Hilbert space, and the pair
 L2
p(0, 1; C), ( · , · )L2(0,1;C)

=
 L2(0, 1; C), ( · , · )L2(0,1;C)

is a complex Hilbert space. As usual, the norm for L2
p(0, 1; C) is deﬁned from the
inner product via
∥f ∥L2(0,1;C) =
q
(f , f )L2(0,1;C).
Proof. See [7, 76].
Remark D.19 (periodic extension). In case you missed that subtle and curious
fact above, here it is again: L2
p(0, 1; C) = L2(0, 1; C). In other words, periodicity
does not matter for merely square integrable functions. This is because, if f ∈
L2(0, 1; C), it can be easily periodically extended to all of R, and the extension will
be in L2
p(0, 1; C).
D.3
Sobolev Spaces
The study of weak solutions of partial diﬀerential equations requires one to deal
with functions that are diﬀerentiable in a sense that is weaker than the classical
one. Spaces of functions with such diﬀerentiability properties are known as Sobolev
spaces. There are two approaches to deﬁning these spaces which, in our setting,
turn out to be equivalent. We will brieﬂy present both and show the relation
between them.
We begin by introducing an auxiliary notion.
Deﬁnition D.20 (support). Let d ∈N, Ω⊂Rd be a domain, and φ: Ω→R. The
support of φ is
supp(φ) = {x ∈Ω| φ(x) ̸= 0}.
Next, we restrict our attention to a special class of domains.
Deﬁnition D.21 (Lipschitz domain5). Let d ∈N and Ω⊂Rd be a domain. We say
that Ωis a Lipschitz domain if and only if its boundary can be locally represented
by the graph of a Lipschitz function. We say that Ωis a uniformly Lipschitz
domain if it is Lipschitz, and the Lipschitz constant of the functions that represent
the boundary is uniform.
Every bounded Lipschitz domain is uniformly Lipschitz. From now on, although
this can be done much more generally, we will exclusively consider Lipschitz
domains.
Deﬁnition D.22 (Sobolev space6). Let d ∈N and Ω⊂Rd be a bounded Lipschitz
domain. We say that a function v : Ω→R belongs to the Sobolev space H1(Ω) if
there is a sequence {φk}k∈N ⊂C1(¯Ω) such that
∥v −φk∥L2(Ω) →0,
k →∞,
5 Named in honor of the German mathematician Rudolf Otto Sigismund Lipschitz
(1832–1903).
6 Named in honor of the Soviet mathematician Sergei Lvovich Sobolev (1908–1989).

888
A (Petting) Zoo of Function Spaces
and, moreover,
sup
k∈N
∥∇φk∥L2(Ω;Rd) < ∞.
We denote this fact by v ∈H1(Ω). Finally, if the sequence can be chosen so that
there is a compact K ⊂Ωsuch that
[
k∈N
supp φk ⊂K,
then we say that the function v ∈H1
0(Ω).
We immediately observe that H1
0(Ω) ⊂H1(Ω), but that they are not equal.
Notice, in addition, that in the previous deﬁnition we required that, for any
i = 1, . . . , d, the sequence of partial derivatives
∂φk
∂xi
remains bounded in L2(Ω). As we have shown before, the space L2(Ω) is Hilbert.
Owing to Theorem 16.10, we can extract a convergent subsequence. It is possible
to show, in addition, that this limit is independent of the subsequence and so we
denote it by
∂v
∂xi
,
and call it the strong partial derivative of the function v. Notice then that, by
deﬁnition, v ∈H1(Ω) if and only if v ∈L2(Ω) and ∇v ∈L2(Ω; Rd), where ∇v is
the vector of strong derivatives.
The intuition behind Sobolev spaces is correct. Functions in such spaces may not
be smooth, but they can be approximated (in the L2(Ω)-sense) by smooth ones.
Example D.2
The function v : x 7→|x| is not continuously diﬀerentiable, but
v ∈H1(−1, 1). Indeed, the sequence
φk(x) =





|x|,
|x| > 1
k ,
k
2x2 + 1
2k ,
|x| ≤1
k
belongs to C1([−1, 1]), as the reader can easily verify. In addition, for some C > 0,
∥v −φk∥2
L2(−1,1) =
Z 1/k
−1/k

|x| −k
2x2 + 1
2k
2
dx < C
k →∞,
as k →∞. Finally, for every k ∈N,
∥φ′
k∥2
L2(−1,1) = 2
Z 1
1/k
dx + k2
Z 1/k
−1/k
x2dx = 2 −4
3k < 2.

D.3 Sobolev Spaces
889
Let us now state without proof some facts about these spaces.
Proposition D.23 (properties of Sobolev spaces). Let d ∈N and Ω⊂Rd be a
bounded Lipschitz domain with suﬃciently smooth boundary.
1. Completeness: The Sobolev space H1(Ω) is Hilbert with respect to the inner
product
(v1, v2)H1(Ω) =
Z
Ω
[v1(x)v2(x) + ∇v1(x) · ∇v2(x)] dx,
∀v1, v2 ∈H1(Ω).
2. Subspace: H1
0(Ω) is a closed subspace of H1(Ω).
3. Poincar´e inequality:7 There is a constant CP > 0 that depends on Ωsuch that,
for all v ∈H1
0(Ω),
∥v∥L2(Ω) ≤CP ∥∇v∥L2(Ω;Rd).
(D.2)
As a consequence, the inner product
(v1, v2)H1
0(Ω) =
Z
Ω
∇v1(x) · ∇v2(x)dx,
∀v1, v2 ∈H1
0(Ω)
induces, on H1
0(Ω), an equivalent norm.
4. Trace inequality: There is a constant that depends on Ωsuch that
∥v∥L2(∂Ω) ≤C∥v∥H1(Ω),
∀v ∈H1(Ω).
5. Polynomials are dense in C1(¯Ω), and so they are in H1(Ω).
From the trace inequality, it follows that we can speak of boundary values
(sometimes called traces) for a function in H1(Ω), something that is not feasible
for a generic function only in L2(Ω). In addition, the meaning of the zero subscript
in H1
0(Ω) is now clear. These are functions in H1(Ω) that “vanish” on the boundary.
Remark D.24 (extension). The Poincar´e inequality (D.2) is still true if we assume
that the trace of u is zero only on some connected subset of the boundary of
positive measure. For example, if d = 2, Ωis a polygon, and the boundary trace
of u is zero on one of the edges of this polygon, then the inequality is still valid.
In all of the following results, we interpret boundary values through the lenses
of boundary trace operators. This is particularly important for boundary integrals.
The following result should be compared with Theorem B.59.
Theorem D.25 (integration by parts). Suppose that d ∈N, Ω⊂Rd is a bounded
Lipschitz domain, and u, v ∈H1(Ω). Then, for i = 1, . . . , d,
Z
Ω
∂u(x)
∂xi
v(x)dx =
Z
∂Ω
u(x)v(x)n(x) · ei dS(x) −
Z
Ω
u(x)∂v(x)
∂xi
dx,
where n: ∂Ω→Rd is the outward pointing unit normal vector on the boundary
∂Ωand {ei}d
i=1 is the canonical basis of Rd.
7 Named in honor of the French mathematician, theoretical physicist, engineer, and philosopher
of science Jules Henri Poincar´e (1854–1912).

890
A (Petting) Zoo of Function Spaces
We also need to introduce higher order Sobolev spaces, and this can be done
recursively. Essentially, we deﬁne Hm(Ω) as the space of functions whose weak
derivatives up to order m are in L2(Ω).
Deﬁnition D.26 (Sobolev space). Let 1 < m ∈N. Then we deﬁne
Hm(Ω) =

v ∈L2(Ω)

∂v
∂xj
∈Hm−1(Ω), j = 1, . . . , d

.
For v ∈Hm(Ω) and j ∈{0, . . . , m}, we deﬁne the seminorm
|v|Hj(Ω) =
v
u
u
u
t
X
α∈Nd
0
|α|=j
∥Dαv∥2
L2(Ω)
and norm
∥v∥Hm(Ω) =


m
X
j=0
|v|2
Hj(Ω)


1/2
.
It turns out that Hm(Ω) is Hilbert under this norm.
The following result should be compared with Theorem B.59.
Theorem D.27 (Green’s identities8). Suppose that d ∈N and Ω⊂Rd is a
bounded Lipschitz domain. For all v ∈H1(Ω) and w ∈H2(Ω),
Z
Ω
∇v(x) · ∇w(x)dx = −
Z
Ω
v(x)∆w(x)dx +
Z
∂Ω
v(x)∂w
∂n (x)dS(x),
where ∂w
∂n (x) = n(x) · ∇w. For any v, w ∈H2(Ω),
Z
Ω
(v(x)∆w(x) −∆v(x)w(x)) dx =
Z
∂Ω

v(x)∂w
∂n (x) −∂v
∂n (x)w(x)

dS(x).
For any v ∈H1(Ω; Rd) and w ∈H1(Ω),
Z
Ω
∇· v(x)w(x)dx =
Z
∂Ω
v(x) · n(x)w(x)dS(x) −
Z
Ω
v(x) · ∇w(x)dx.
Another approach to deﬁning Sobolev spaces is via weak derivatives. For that,
we begin by deﬁning a test function.
Deﬁnition D.28 (test function). Let d ∈N and Ω⊂Rd be a bounded domain. A
function φ: Ω→R is called a test function on Ωand we write φ ∈C∞
0 (a, b) if
and only if φ ∈C∞(Ω) and supp(φ) ⊂Ω.
Remark D.29 (vanishing derivatives). Suppose that φ ∈C∞
0 (a, b). Since supp(φ)
is closed and is a proper subset of the open bounded interval (a, b), it follows that
φ and any of its derivatives vanish on the boundary.
8 Named in honor of the British mathematical physicist George Green (1793–1841).

D.3 Sobolev Spaces
891
Example D.3
Suppose that a > 1. The bump function, φ: (−a, a) →R, deﬁned
via
φ(x) =





exp

−
1
1 −x2

,
x ∈(−1, 1),
0,
x ∈(−a, −1] ∪[1, a),
is a test function on (−a, a). It is easy to see that supp(φ) = [−1, 1].
We can now give another deﬁnition of Sobolev spaces.
Deﬁnition D.30 (Sobolev space). Let d ∈N, Ω⊂Rd be a bounded Lipschitz
domain and u ∈L2(Ω). We say that u ∈W 1(Ω) if and only if there exist functions
gi ∈L2(Ω), i = 1, . . . , d such that
Z
Ω
gi(x)φ(x)dx = −
Z
Ω
u(x)∂φ(x)
∂xi
dx,
∀φ ∈C∞
0 (Ω).
The function gi is called a weak derivative of u with respect to xi. More generally,
if m ∈N, we say that u ∈W m(Ω) if and only if there exist
{gα} α∈Nd
0
|α|≤m
⊂L2(Ω)
such that, for every multi-index α ∈Nd
0 with |α| ≤m, we have
Z
Ω
gα(x)φ(x)dx = (−1)|α|
Z
Ω
u(x)Dαφ(x)dx,
∀φ ∈C∞
0 (Ω).
The function gα is called a weak derivative of u of order α. We also deﬁne the
notation W 0(Ω) = L2(Ω). The set W m(Ω) is called the Sobolev space of order m.
Example D.4
Suppose that f (x) = |x|, for x ∈(−2, 2). Then f ∈W 1(−2, 2),
as we will momentarily show. First, recall that, in Example D.3, we showed that f
is continuously diﬀerentiable and its piecewise derivative was a Heaviside function.
In fact, we could take the following as its piecewise derivative:
g(x) =





−1,
−2 < x < 0,
0,
x = 0,
1,
0 < x < 2.

892
A (Petting) Zoo of Function Spaces
Let us show that this also works as a weak derivative of f . Let φ ∈C∞
0 (−2, 2) be
arbitrary. Then, using integration by parts,
Z 2
−2
g(x)φ(x)dx = −
Z 0
−2
φ(x)dx +
Z 2
0
φ(x)dx
= −xφ(x)|0
−2 +
Z 0
−2
xφ′(x)dx + xφ(x)|2
0 −
Z 2
0
xφ′(x)dx
=
Z 0
−2
xφ′(x)dx −
Z 2
0
xφ′(x)dx
= −
Z 2
−2
|x|φ′(x)dx.
As with the piecewise derivatives deﬁned previously, weak derivatives are, in the
pointwise sense, nonunique. They are unique in the L2-sense.
Remark D.31 (equivalence). Recall that L2 functions that diﬀer only on a set of
measure zero are equivalent.
It is no coincidence that we have called these two sets Sobolev spaces, as the
following result shows.
Theorem D.32 (Meyers–Serrin9). Let d ∈N and Ω⊂Rd be a bounded Lipschitz
domain. Then, for every m ∈N,
Hm(Ω) = W m(Ω).
In particular, the weak and strong derivatives of a function in Hm(Ω) coincide.
Remark D.33 (vector-valued functions). As always, by arguing component-wise,
we can deﬁne Sobolev spaces of vector-valued functions.
D.3.1
Periodic Functions
Let us now deﬁne Sobolev spaces of periodic functions.
Deﬁnition D.34 (periodic Sobolev space). Suppose that L > 0 and m ∈N. Deﬁne
Hm
p (0, L; C) = {u ∈Hm
loc(R; C) | u(x + nL) = u(x),
∀n ∈Z,
∀x ∈R} ,
where
Hm
loc(R; C) =
n
u : R →C
 u|(c,d) ∈Hm(c, d; C), ∀c, d ∈R, −∞< c < d < ∞
o
.
The space Hm
p (0, L; C) is called the periodic Sobolev space of order m. The
subspace of Hm
p (0, L; C), consisting of mean-zero functions, is
˚
Hm
p (0, L; C) =

f ∈Hm
p (0, L; C)

Z L
0
f (x)dx = 0

.
9 Named in honor of the American mathematicians Norman George Meyers (1930–) and James
Burton Serrin (1926–2012), who originally proved a more general version of this result in an
article concisely titled H = W ; see [61].

D.3 Sobolev Spaces
893
Remark D.35 (identiﬁcation). Since an L-periodic function can be uniquely
characterized by its values on [0, L], we usually make the identiﬁcation between
a periodic function and its restriction to this interval. With this convention, we
could have similarly deﬁned
˚
Hm
p (0, L; C) = Hm
p (0, L; C) ∩˚L2(0, L; C).
As before, we make the simplifying assumption that L = 1 from this point on.
We must point out that, for the periodic case, there is an equivalent way to
deﬁne Hm
p (0, 1; C) using Fourier series. To see how that works, let us ﬁrst do some
calculations. Suppose that J ⊆Z is an index set and
f (x) =
X
j∈J
aj exp(2πijx),
aj ∈C,
j ∈J.
If J is ﬁnite in cardinality, then f is a smooth, one-periodic function, and we can
take any number of derivatives of the sum, term by term, to obtain
f (m)(x) =
X
j∈J
aj(2πij)m exp(2πijx),
∀x ∈R
for any m ∈N. Using orthonormality, computing the L2(0, 1; C) norm of f (m), we
ﬁnd
f (m)
2
L2(0,1;C) =

X
j∈J
aj(2πij)m exp(2πij · ),
X
k∈J
ak(2πik)m exp(2πik · )


L2(0,1;C)
=
X
j∈J
X
k∈J
aj(2πij)mak(2πik)m (exp(2πij · ), exp(2πik · ))L2(0,1;C)
=
X
j∈J
X
k∈J
aj(2πij)mak(2πik)m δj,k
=
X
j∈J
|aj|2|2πij|2m
=
X
j∈J
|aj|2(4π2j2)m.
In other words,
f (m)
2
L2(0,1;C) =
X
j∈J
γm
j |aj|2,
γj = 4π2j2,
j ∈J.
Using these calculations as motivation, we can prove the following result.
Theorem D.36 (characterization of Hm
p (0, 1; C)). Suppose that f ∈L2
p(0, 1; C)
and m ∈N. Let Ψ = {ψj}j∈Z, with
ψj(x) = exp(2πijx) ,
be the standard orthonormal trigonometric polynomial system on [0, 1] and ˆfj
denote the jth Fourier coeﬃcient of f , ˆfj = (f , ψj)L2(0,1;C). Then f ∈Hm
p (0, 1; C)
if and only if the series

894
A (Petting) Zoo of Function Spaces
∞
X
j=−∞
γm
j |ˆfj|2 = lim
n→∞
n
X
j=−n
γm
j |ˆfj|2
converges, where
γj = 4π2j2,
j ∈Z.
(D.3)
If f ∈Hm
p (0, 1; C), then, in the L2-sense, we have, as n →∞,
n
X
j=−n
ˆfj(2πij)mψj −→dmf
dxm ,
and we are justiﬁed in writing
dmf
dxm =
∞
X
j=−∞
ˆfj(2πij)mψj.
Furthermore, the object
|f |2
Hm(0,1;C) =
dmf
dxm , dmf
dxm

L2(0,1)
=
∞
X
j=−∞
γm
j |ˆfj|2
(D.4)
is called the Sobolev seminorm of f or order m.
Proof. ( =⇒) This direction follows from the Riesz–Fischer Theorem 12.17 and
(D.4) follows from Parseval’s relation. The details are left to the reader as an
exercise; see Problem D.7.
( ⇐= ) This direction is beyond the scope of the text. The interested reader is
referred to [97].
Problems
D.1
Prove Proposition D.5 and complete the proof of Theorem D.9.
Hint: Revisit the proof of Proposition A.27.
D.2
Prove the following, simpliﬁed, version of Poincar´e inequality: There is a
constant C > 0 such that, for any v ∈C∞(0, 1) ∩C([0, 1]), v(0) = v(1) = 0,
Z 1
0
v 2dx ≤C
Z 1
0
dv
dx
2
dx.
D.3
Prove the Friedrichs inequality 10: Let d ∈N and Ω⊂Rd be a bounded
domain with suﬃciently smooth boundary. There is a constant C > 0 such that
∥v∥L2(Ω) ≤C

∥∇v∥2
L2(Ω;Rd) + ∥v∥2
L2(∂Ω)
1/2
,
∀v ∈C1(¯Ω).
Hint: Integrate by parts the identity
Z
Ω
|v(x)|2dx =
Z
Ω
|v(x)|2∆φ(x)dx,
where φ(x) =
1
2d ∥x∥2
2.
10 Named in honor of the German–American mathematician Kurt Otto Friedrichs (1901–1982).

Problems
895
D.4
Let f (x) = xα, for α ∈(0, 1). Show that f ∈H1(0, 1) for α > 1/2.
D.5
Show that f (x) = 1 −|x| belongs to the Sobolev space H1
0(0, 1).
D.6
A function v : [0, 1] →R is said to be H¨older continuous of order α > 0,
denoted v ∈C0,α([0, 1]), if and only if
sup
x,y∈[0,1]: x̸=y
|v(x) −v(y)|
|x −y|α
< ∞.
a)
Show that if a function is H¨older continuous of order α > 0, then it is
continuous.
b)
Show that if a function is H¨older continuous of order α > 1, then it is constant.
c)
Show that if v ∈H1
0(0, 1), then v ∈C0,1/2([0, 1]).
D.7
Complete the proof of Theorem D.36.

References
[1] N.I. Achieser. Theory of Approximation. Dover Publications, New York, NY, 1992.
Translated from the Russian and with a preface by Charles J. Hyman. Reprint of
the 1956 English translation.
[2] R.A. Adams and J.J.F. Fournier. Sobolev Spaces, 2nd ed. Academic Press, New
York, NY, 2003.
[3] L.V. Ahlfors. Complex Analysis: An Introduction to the Theory of Analytic
Functions of One Complex Variable, 3rd ed. International Series in Pure and Applied
Mathematics. McGraw-Hill, New York, NY, 1978.
[4] H. Amann. Ordinary Diﬀerential Equations: An Introduction to Nonlinear Analysis.
Vol. 13 of De Gruyter Studies in Mathematics. Walter de Gruyter, Berlin, 1990.
Translated from the German by Gerhard Metzen.
[5] K. Atkinson and W. Han. Theoretical Numerical Analysis, 3rd ed. Vol. 39 of Texts
in Applied Mathematics Springer-Verlag, New York, NY, 2009.
[6] R.G. Bartle and D.R. Sherbert. Introduction to Real Analysis, 4th ed. John Wiley
& Sons, Hoboken, NJ, 2011.
[7] R.F. Bass. Real Analysis for Graduate Students, 2nd ed. CreateSpace Independent
Publishing Platform, Scotts Valley, CA, 2013.
[8] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,
Cambridge, 2004.
[9] D. Braess. Finite Elements: Theory, Fast Solvers, and Applications in Solid
Mechanics, 3rd ed. Cambridge University Press, Cambridge, 2007.
[10] S.C. Brenner and L.R. Scott. The Mathematical Theory of Finite Element Methods,
3rd ed. Vol. 15 of Texts in Applied Mathematics. Springer-Verlag, Berlin, 2007.
[11] J. Burkardt and C. Trenchea. Refactorization of the midpoint rule. Appl. Math.
Lett., 107:106438, 2020.
[12] J.C. Butcher. Numerical Methods for Ordinary Diﬀerential Equations, 2nd ed. John
Wiley & Sons, Chichester, 2008.
[13] C. Canuto, M.Y. Hussaini, A. Quarteroni, and T.A. Zang. Spectral Methods:
Fundamentals in Single Domains. Springer-Verlag, Berlin, 2007.
[14] L. Chen, X. Hu, and S.M. Wise. Convergence analysis of the fast subspace descent
methods for convex optimization problems. Math. Comput., 89:2249–2282, 2020.
[15] E.W. Cheney. Approximation Theory. McGraw-Hill, New York, NY, 1966.
[16] P.G. Ciarlet. Introduction to Numerical Linear Algebra and Optimisation. Cambridge
University Press, Cambridge, 1989.
[17] P.G. Ciarlet. The Finite Element Method for Elliptic Problems. Vol. 40 of Classics in
Applied Mathematics. Society for Industrial and Applied Mathematics, Philadelphia,
PA, 2002.
[18] P.G. Ciarlet. Linear and Nonlinear Functional Analysis with Applications. Society
for Industrial and Applied Mathematics, Philadelphia, PA, 2013.

References
897
[19] E.A. Coddington and N. Levinson. Theory of Ordinary Diﬀerential Equations.
McGraw-Hill, New York, NY, 1955.
[20] J.W. Cooley and J.K. Tukey. An algorithm for the machine calculation of complex
Fourier series. Math. Comput., 19(2):297–301, 1965.
[21] R. Courant, K. Friedrichs, and H. Lewy. ¨Uber die partiellen Diﬀerenzengleichungen
der mathematischen Physik. Math. Ann., 100(1):32–74, 1928.
[22] R. Courant, K. Friedrichs, and H. Lewy. On the partial diﬀerence equations of
mathematical physics. IBM J. Res. Develop., 11:215–234, 1967.
[23] R. Dautray and J.-L. Lions. Evolution Problems II. Vol. 6 of Mathematical Analysis
and Numerical Methods for Science and Technology. Springer-Verlag, Berlin, 1993.
Translated from the French by Alan Craig.
[24] P.J. Davis. Interpolation and Approximation. Dover Publications, New York, NY,
1975.
[25] P.J. Davis and P. Rabinowitz. Methods of Numerical Integration. Academic Press,
Orlando, FL, 1984.
[26] J.W. Demmel. Applied Numerical Linear Algebra. Society for Industrial and Applied
Mathematics, Philadelphia, PA, 1997.
[27] J.E. Dennis, Jr. and R.B. Schnabel. Numerical Methods for Unconstrained Opti-
mization and Nonlinear Equations. Vol. 16 of Classics in Applied Mathematics.
Society for Industrial and Applied Mathematics, Philadelphia, PA, 1996. Corrected
reprint of the 1983 original.
[28] P. Deuﬂhard. Newton Methods for Nonlinear Problems: Aﬃne Invariance and
Adaptive Algorithms. Vol.
35 of Springer Series in Computational Mathematics.
Springer, Heidelberg, 2011. First softcover printing of the 2006 corrected printing.
[29] D.A. Di Pietro and A. Ern. Mathematical Aspects of Discontinuous Galerkin
Methods. Vol. 69 of Math´ematiques & Applications (Berlin) [Mathematics &
Applications]. Springer, Heidelberg, 2012.
[30] A. Ern and J.-L. Guermond. Theory and Practice of Finite Elements. Vol. 159 of
Applied Mathematical Sciences. Springer-Verlag, New York, NY, 2004.
[31] L.C. Evans. Partial Diﬀerential Equations. American Mathematical Society, Provi-
dence, RI, 2010.
[32] W. Gautschi. Numerical Analysis, 2nd ed. Birkhauser-Verlag, New York, NY, 2012.
[33] E. Godlewski and P.-A. Raviart. Numerical Approximation of Hyperbolic Systems of
Conservation Laws. Vol. 118 of Applied Mathematical Sciences. Springer-Verlag,
New York, NY, 1996.
[34] G.H. Golub and C.F. Van Loan. Matrix Computations, 4th ed. Johns Hopkins
University Press, Baltimore, MD, 2013.
[35] R.E. Greene and S.G. Krantz. Function Theory of One Complex Variable, 3rd
ed. Vol. 40 of Graduate Studies in Mathematics. American Mathematical Society,
Providence, RI, 2006.
[36] L.A. Hageman and D.M. Young. Applied Iterative Methods. Academic Press, San
Diego, CA, 1981.
[37] E. Hairer, S. Norsett, and G. Wanner. Solving Ordinary Diﬀerential Equations I:
NonstiﬀProblems, 2nd ed. Springer-Verlag, Berlin, 1993.
[38] G. Harris and C. Martin. The roots of a polynomial vary continuous as a function
of the coeﬃcients. Proc. Am. Math. Soc., 100(2):390–392, 1987.
[39] A. Hatcher. Algebraic Topology. Cambridge University Press, Cambridge, 2002.
[40] M.T. Heideman, D.H. Johnson, and C.S. Burrus. Gauss and the history of the fast
Fourier transform. IEEE ASSP Magazine, 7(7):14–21, 1984.

898
References
[41] P. Henrici. Elements of Numerical Analysis. John Wiley & Sons, New York, NY,
1964.
[42] E. Hewitt and R.E. Hewitt. The Gibbs-Wilbraham phenomenon: an episode in
Fourier analysis. Arch. Hist. Exact Sci., 21(2):129–160, 1979/80.
[43] H. Holden and N.H. Risebro. Front Tracking for Hyperbolic Conservation Laws, 2nd
ed. Vol. 152 of Applied Mathematical Sciences. Springer, Heidelberg, 2015.
[44] R.A. Horn and C.R. Johnson. Matrix Analysis. Cambridge University Press,
Cambridge, 1985.
[45] T.W. Hungerford. Algebra. Vol. 73 of Graduate Texts in Mathematics. Springer-
Verlag, Berlin, 1980. Reprint of the 1974 original.
[46] E. Isaacson and H.B. Keller. Analysis of Numerical Methods. John Wiley & Sons,
New York, NY, 1966.
[47] A. Iserles. A First Course in the Numerical Analysis of Diﬀerential Equations, 2nd
ed. Cambridge University Press, Cambridge, 2009.
[48] D. Jackson. The Theory of Approximation. Vol. 2 of Colloquium Publications. AMS,
New York, NY, 1930.
[49] F. John. Partial Diﬀerential Equations, 4th ed. Applied Mathematical Sciences.
Springer-Verlag, Berlin, 1981.
[50] B.S. Jovanovi´c and E. S¨uli. Analysis of Finite Diﬀerence Schemes: For Linear Partial
Diﬀerential Equations with Generalized Solutions. Vol.
46 of Springer Series in
Computational Mathematics. Springer, London, 2014.
[51] C.T. Kelley. Iterative Methods for Linear and Nonlinear Equations. Vol. 16 of
Frontiers in Applied Mathematics. Society for Industrial and Applied Mathematics,
Philadelphia, PA, 1995.
[52] R. Kress. Numerical Analysis. Springer-Verlag, Berlin, 1998.
[53] D. Kr¨oner. Numerical Schemes for Conservation Laws. Wiley-Teubner Series
Advances in Numerical Mathematics. John Wiley & Sons, Chichester; B.G.
Teubner, Stuttgart, 1997.
[54] V.I. Krylov. Approximate Calculation of Integrals. Macmillan, New York, NY, 1962.
[55] A. Kufner, O. John, and S. Fuˇc´ık. Function Spaces. Monographs and Textbooks
on Mechanics of Solids and Fluids; Mechanics: Analysis. NoordhoﬀInternational
Publishing, Leiden; Academia, Prague, 1977.
[56] O.A. Ladyzhenskaya. The Boundary Value Problems of Mathematical Physics.
Vol. 49 of Applied Mathematical Sciences. Springer-Verlag, New York, 1985.
Translated from the Russian by Jack Lohwater [Arthur J. Lohwater].
[57] O. Ladyzhenskaya. Attractors for Semigroups and Evolution Equations. Lezioni
Lincee. [Lincei Lectures.] Cambridge University Press, Cambridge, 1991.
[58] S. Larsson and V. Thom´ee. Partial Diﬀerential Equations with Numerical Methods.
Vol. 45 of Texts in Applied Mathematics. Springer-Verlag, Berlin, 2003.
[59] R.J. LeVeque. Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-
sity Press, Cambridge, 2002.
[60] J.E. Marsden and M.J. Hoﬀman. Basic Complex Analysis. W.H. Freeman, New
York, NY, 1989.
[61] N.G. Meyers and J. Serrin. H = W. Proc. Natl. Acad. Sci. U.S.A., 51:1055–1056,
1964.
[62] V.V. Nemytskii and V.V. Stepanov. Qualitative Theory of Diﬀerential Equations.
Princeton Mathematical Series, No. 22. Princeton University Press, Princeton, NJ,
1960.
[63] Y.E. Nesterov. A method for solving the convex programming problem with
convergence rate O(1/k2). Dokl. Akad. Nauk SSSR, 269(3):543–547, 1983.

References
899
[64] Y.E. Nesterov. Introductory Lecture Notes on Convex Optimization: A Basic
Course. Vol. 87 of Applied Optimization. Kluwer Academic Publishers, Boston,
MA, 2004.
[65] J. Nocedal and S.J. Wright. Numerical Optimization, 2nd ed. Springer Series in
Operations Research and Financial Engineering. Springer, New York, NY, 2006.
[66] J.J. O’Connor and E.F. Robertson. MacTutor History of Mathematics Archive.
Website. https://mathshistory.st-andrews.ac.uk/.
[67] J.T. Oden. Finite elements: an introduction. In Handbook of Numerical Analysis,
Vol. II, pages 3–15. North-Holland, Amsterdam, 1991.
[68] J.M. Ortega and W.C. Rheinboldt. Iterative solution of nonlinear equations in
several variables. Vol. 30 of Classics in Applied Mathematics. Society for Industrial
and Applied Mathematics, Philadelphia, PA, 2000. Reprint of the 1970 original.
[69] J.-H. Park, A.J. Salgado, and S.M. Wise. Preconditioned accelerated gradient
descent methods for locally Lipschitz smooth objectives with applications to the
solution of nonlinear PDEs. J. Sci. Comput., 89(1):17, 2021.
[70] B.N. Parlett. The Symmetric Eigenvalue Problem. Prentice-Hall, Englewood Cliﬀs,
NJ, 1980.
[71] M.J.D. Powell. Approximation Theory and Methods. Cambridge University Press,
Cambridge, 1981.
[72] M. Renardy and R.C. Rogers. An Introduction to Partial Diﬀerential Equations, 2nd
ed. Vol. 13 of Texts in Applied Mathematics. Springer-Verlag, Berlin, 2006.
[73] T.J. Rivlin. An Introduction to the Approximation of Functions. Dover Books
on Advanced Mathematics. Dover Publications, New York, NY, 1981. Corrected
reprint of the 1969 original.
[74] R.C. Robinson. An Introduction to Dynamical Systems—Continuous and Discrete,
2nd ed. Vol. 19 of Pure and Applied Undergraduate Texts. American Mathematical
Society, Providence, RI, 2012.
[75] H.L. Royden. Real Analysis, 3rd ed. Macmillan, New York, NY, 1988.
[76] W. Rudin. Principles of Mathematical Analysis, 3rd ed. McGraw-Hill, New York,
NY, 1976.
[77] W. Rudin. Real and Complex Analysis, 3rd ed. McGraw-Hill, New York, NY, 1986.
[78] Y. Saad. Iterative Methods for Sparse Linear Systems, 2nd ed. Society for Industrial
and Applied Mathematics, Philadelphia, PA, 2003.
[79] A.A. Samarskii. The Theory of Diﬀerence Schemes, Vol. 240 of Monographs and
Textbooks in Pure and Applied Mathematics. Marcel Dekker, New York, NY, 2001.
[80] A.A. Samarskii and A.V. Gulin. Numerical Methods. Nauka, Moscow, 1989. [in
Russian.]
[81] A.A. Samarskii and E.S. Nikolaev. Numerical Methods for Grid Equations. Vol. II:
Iterative Methods. Birkh¨auser Verlag, Basel, 1989. Translated from the Russian
and with a note by Stephen G. Nash.
[82] C. Schwab. p- and hp-Finite Element Methods. Theory and Applications in Solid and
Fluid Mechanics. Numerical Mathematics and Scientiﬁc Computation. Clarendon
Press, Oxford University Press, New York, NY, 1998.
[83] L.R. Scott. Numerical Analysis. Princeton University Press, Princeton, NJ, 2011.
[84] B. Sendov and A. Andreev. Approximation and interpolation theory. In Handbook
of Numerical Analysis, Vol. III, pages 223–462. North-Holland, Amsterdam, 1994.
[85] J. Shen, T. Tang, and L.L. Wang. Spectral Methods: Algorithms, Analysis and
Applications. Springer-Verlag, Berlin, 2011.
[86] J. Stoer and R. Bulirsch. Introduction to Numerical Analysis, 3rd ed. Springer-
Verlag, Berlin, 2002.

900
References
[87] W.A. Strauss. Partial Diﬀerential Equations: An Introduction, 2nd ed. John Wiley
& Sons, Chichester, 2008.
[88] J.C. Strikwerda. Finite Diﬀerence Schemes and Partial Diﬀerential Equations, 2nd
ed. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2004.
[89] E. S¨uli and D.F. Mayers. An Introduction to Numerical Analysis. Cambridge
University Press, Cambridge, 2003.
[90] R. Temam. Inﬁnite-Dimensional Dynamical Systems in Mechanics and Physics.
Vol. 68 of Applied Mathematical Sciences. Springer-Verlag, New York, NY, 1988.
[91] V. Thom´ee. Galerkin Finite Element Methods for Parabolic Problems. Vol. 25 of
Series in Computational Mathematics. Springer-Verlag, Berlin, 1997.
[92] V. Thom´ee. From ﬁnite diﬀerences to ﬁnite elements. A short history of numerical
analysis of partial diﬀerential equations. J. Comput. Appl. Math., 128(1–2):1–54,
2001.
[93] G.P. Tolstov. Fourier Series. Prentice-Hall, Englewood Cliﬀs, NJ, 1989.
[94] L.N. Trefethen. Spectral Methods in Matlab. Society for Industrial and Applied
Mathematics, Philadelphia, PA, 2001.
[95] L.N. Trefethen. Interpolation Theory and Interpolation Practice. Society for
Industrial and Applied Mathematics, Philadelphia, PA, 2013.
[96] L.N. Trefethen and D. Bau. Numerical Linear Algebra. Society for Industrial and
Applied Mathematics, Philadelphia, PA, 1997.
[97] A. Vretblad. Fourier Analysis and its Applications. Springer-Verlag, Berlin, 2003.
[98] N.J. Walkington. Nesterov’s method for convex optimization. SIAM Rev. To appear.
[99] H. Weinberger. A First Course in Partial Diﬀerential Equations. Xerox, Lexington,
MA, 1965.
[100] P. Wesseling. Principles of Computational Fluid Dynamics. Vol. 29 of Springer
Series in Computational Mathematics. Springer-Verlag, Berlin, 2001.
[101] Wikipedia, The Free Encyclopedia. Website. https://en.wikipedia.org
[102] J.H. Wilkinson. The Algebraic Eigenvalue Problem. Oxford University Press,
Oxford, 1965.
[103] D.M. Young. Iterative Solution of Large Linear Systems. Academic Press,
Cambridge, MA, 1971.
[104] A. Zygmund. Trigonometric Series, Vols. I and II, 3rd ed. Cambridge Mathematical
Library. Cambridge University Press, Cambridge, 2002.

Index
A-stable method, 584
Abel transformation, 670
Abel, N.H., 670
Adams, J.C., 562
Adams–Bashforth method, 562
Adams–Moulton method, 562
advection, 612, 613
ﬂux, 613
velocity, 612
advection–diﬀusionequation, 616
advection–reaction–diﬀusionequation,
616
stationary version, 619
Alekseev, V.M., 519
Alekseev–Gr¨obner Lemma, 519
alias error, 357
aliasingerror, 760
analytic function, 244
Arnoldi method, 188
Arnoldi, W.E., 188
Aubin, J.P., 709
backward diﬀerentiation formula (BDF)
method, 566
BDFq, 579
BDF2, 568
BDF3, 578
backward Euler method
for IVPs, 526
heat equation, 777, 792
ball of radius r, 866
Banach Fixed Point Theorem, 874
Banach Open MappingTheorem, 459
Banach space, 453
Banach, S., 453, 459, 874
Bashforth, F., 562
basis, 841
canonical basis for Cn, 843
Lagrange nodal, 235
Newton, 249
basis completion theorem, 850
basis extension theorem, 842
Bauer, F.L., 203
Bauer–Fike Theorem, 203
Beam, R.M., 813
Beam–Warming method
transport equation, 813
Bernoulli numbers, 400
Bernoulli polynomial, 401
Bernoulli, J., 400
Bernstein polynomial, 286
Bernstein Theorem, 295
Bernstein, S.N., 290, 295
Bessel’s inequality, 303, 322
Bessel, F.W., 303
best polynomial approximation
in the L∞-norm, 267
in the Lp
w-norm, 268
existence, 268
binomialcoeﬃcient, 290
bisection method, 421
Bolzano, B.P.J.N., 856
Bolzano–Weierstrass Theorem, 856
Borel, F.E.J.E., 454, 857
boundary condition
Dirichlet, 616
essential, 616
natural, 616
Neumann, 616
Robin, 616
boundary locus method, 590
boundary normal derivative, 616
boundary value problem (BVP), 619
Brown, R., 613
Brownian motion, 613, 614
Butcher, J.C., 539
C´ea’s Lemma, 703
C´ea, J., 703
Cassiniovals, 224
Cassini,G.D., 224
Cauchy’s Integral Theorem, 245
Cauchy, A.L., 245, 509, 848, 854
Cauchy–Schwarz inequality, 453, 848
centered diﬀerence method
transport equation, 812

902
Index
CFL condition, 781
CG, 169
CGNE, 187
CGNR, 187
characteristic function, 378
characteristic polynomial, 12
Chebyshev collocation method, 762
Chebyshev interpolation, 242
Chebyshev OscillationTheorem,
272
Chebyshev polynomial, 146, 179, 190, 279
Chebyshev projection, 749
Chebyshev Transform, 750
Chebyshev’s method, 145
Chebyshev, P.L., 145, 152, 272, 279, 743,
749, 751
Chebyshev–Lagrangeinterpolation, 282
nodes, 282
Cholesky factorization, 59, 64
complexity, 65
existence, 59
uniqueness, 59
Cholesky, A.L., 59
chord method, 427
closed set, 454
closure of a set, 454
coercive function, 461
collocation
method, 544, 742
Chebyshev, 762
points, 544, 742
compactness, 857
in Cd, 857
sequential, 455
comparison function, 628
complex number, 838
conjugate, 838
modulus, 839
condition number, 80
generalized, 168
spectral, 80
conjugate gradientmethod, 169
convergence of, 178
convergence rate, 178
normal equationerror, 187
normal equationresidual, 187
preconditioned, 180
standard, 179
three-layer, 183
zero start, 169
conjugate vectors
A-conjugate, 158
consistency error, 527
continuousfunction, 858
Lipschitz, 424
contraction mapping, 424, 874
convergence
spectral, 341
uniform, 871
convex
function, 460
set, 460, 866
convex optimization, 156
convolution
discrete periodic, 352
periodic, 325
Cotes, R., 385
Coulomb, C.A., 615
Courant,R., 776, 781
Crank, J., 777, 813
Crank–Nicolsonmethod
heat equation, 777, 793
transport equation, 813
curl operator, 658, 869
cyclically tridiagonalmatrix,
725
d’Alembert, J.B. le Rond, 652
DahlquistEquivalenceTheorem,
577
DahlquistFirstBarrier Theorem,
578
DahlquistSecond Barrier Theorem,
590
Dahlquist,G., 578, 584, 590
dampedwave equation, 625
Darcy, H.P.G., 615
de la Vall´ee Poussin,C.J.,
271
derivative
Fr´echet, 463
Gateaux, 464
piecewise, 334
pseudo-spectral, 732
strong, 888
weak, 891
DFT matrix, 366
diagonalizability criterion,
13
diagonalizable, 13
diﬀerence equation, 569
homogeneous, 569
stablesolutions, 569
diﬀerentiablefunction, 859
diﬀusionconstant, 621
diﬀusionequation, 616
diﬀusionﬂux, 613
dimension, 841
Dirac comb, 324
Dirac delta function, 324
Dirac, P.A.M., 324
Dirichlet problem
classicalsolution, 630
existence, 630
uniqueness, 630
weak solution, 632
existence and uniqueness, 634
Dirichlet, J.P.G.L., 616

Index
903
discontinuousGalerkin(DG) method,
600
local version, 600
discrete convolution, 352
Discrete Fourier Transform, 345, 350
inverse, 352
divergence operator, 612, 869
divergence theorem, 613
divergence-free ﬂow, 612
divideddiﬀerence, 250
extended, 259
table, 254, 255
domain
Lipschitz, 887
downwindmethod, 697
Du Fort, E.C., 804
Du Fort–Frankel method
heat equation, 804
dualproblem, 709
Duhamel’sformula, 640, 644
Duhamel,J.M.C., 640, 653
Eckart, C.H., 28
Eckart–Young Theorem, 28
eigenfunction
diﬀerentialoperator, 635, 636
eigenpair
diﬀerentialoperator, 641
linear operator, 15
matrix, 12
eigenvalue, 12
diﬀerentialoperator, 635
linear operator, 15
eigenvector, 12
linear operator, 15
elliptic projection
Chebyshev weighted, 748
energy norm, 704
equi-oscillationproperty, 271
error transfer matrix, 123
error vector,81
Euler equation, 465
Euler’s formula, 322
Euler, L., 322, 465, 526, 627
Euler–MaclaurinTheorem, 363
false positionmethod, 422
Fast Fourier Transform, 345
Fick’s law, 614
Fick, A.G., 615
ﬁeld, 837
Fike, C.T., 203
ﬁnite diﬀerence method
boundary value problems, 664
centered, 689
consistency error, 675
consistent, 675
convergent, 676
dispersion, 822
downwind, 689
error, 676
homogeneous, 685
matrixvalued, 828
matrix-valuedsymbol, 828
monotone, 687
periodic, 724
stable, 674
symbol, 795
two-layer, 795
upwind, 688
ﬁnite diﬀerence operator, 668
approximationproperty, 671
backward diﬀerence, 669
centered diﬀerence, 669
consistent, 675
discrete Laplacian
one-dimensional, 669
two-dimensional, 669
forward diﬀerence, 669
mixed derivative
two-dimensional, 669
product rule, 670
skew Laplacian, 670
stencil, 668
stencil size, 668
ﬁnite element method, 305, 704
hp, 710
element, 705
higher order, 710
in one dimension, 705
in two dimensions, 712
mesh, 705
piecewise linear, 705
triangulation, 705
Fischer, E.S., 329
ﬁxed point, 423, 874
ﬁxed point iterationmethod, 423
ﬂow
divergence-free, 612
solenoidal, 612
ﬂow map, 518
forward Euler method
for IVPs, 526
convergence, 529
heat equation, 777, 792
Fourier coeﬃcie nt,322
generalized, 309
Fourier matrix, 366
Fourier projection, 323
Fourier Transform, 330, 637
Discrete, 345, 350
Fourier’s law, 620
Fourier, J.B.J., 320, 615, 637, 793

904
Index
Fourier–Legendre expansion, 311
Fr´echet derivative, 463
second derivative, 483
Fr´echet, M.R., 463
Frankel, S.P., 804
Friedrichs inequality, 894
Friedrichs, K.O., 656, 781, 812, 827, 894
Frobenius matrix, 223
Frobenius matrix inner product,
625
Frobenius norm, 9
Frobenius, F.G., 9, 223, 625
function
analytic, 244
bounded, 859
coercive, 461
continuous, 858
H¨older, 895
continuouslydiﬀerentiable, 859
convex, 460
diﬀerentiable, 859
grid, 348
H¨older continuous, 858
holomorphic, 244
Lipschitzcontinuous, 858
locally Lipschitzsmooth, 465
lower semi-continuous, 460
meromorphic, 246
periodic, 884
piecewise continuous, 333
piecewise diﬀerentiable, 334
pole of a, 246
residue of, 246
strictlyconvex, 460
stronglyconvex, 465
support of, 887
test, 890
uniformly continuous, 858
weakly lower semi-continuous, 460
weight, 882
FundamentalTheorem of Calculus, 862
Galerkinapproximation, 170, 701
properties of, 170
Galerkinmethod, 701
lumped-massspectral,
763
spectral, 727, 730
weighted spectral, 747
Galerkinorthogonality,
170, 703
Galerkin,B.G., 170, 596, 600, 605, 701, 730
Gateauxderivative, 464
Gateaux,R.E., 464, 596
Gauss,J.C.F., 41, 128, 409, 548, 637, 751
Gauss–Seidelmethod, 128
Gaussianelimination, 31, 38, 41
complexity, 43
elementary row operation, 38
modiﬁed, 55
of a diagonallydominantmatrix,
52, 54–56
of a Hermitianpositive deﬁnite matrix, 61,
62
pivot, 38
maximalcolumn, 46
Gaussianquadrature, 409
Gelfand,I.M., 79
generalized minimization of the residual
(GMRES) method, 188
Genocchi, A., 257
Gershgorin Circle Theorem, 200
Gershgorin disk, 200
Gershgorin, S.A., 200
Gibbsphenomenon, 313
Gibbs,J.W., 313
Givens, J.W., 119
Godunov Theorem, 817
Godunov, S.K., 817
Golub, G.H., 222
Golub–Kahanmethod, 222
Gr¨obner, W., 519
Gr¨onwall, T.H., 515
Gr¨onwall inequality,
515
discrete, 527, 534, 535, 574
gradientdescent method, 161, 470
line search, 161
search direction, 161
gradientoperator, 612
Gram, J.P., 849
Gram–Schmidtprocess, 102, 849
modiﬁed, 107
grid
size, 665
uniform, 665
grid domain, 665
boundary, 670
interior, 670
grid function, 348, 665
mean-zero periodic, 723
operator
ﬁnite diﬀerence, 668
shift, 668
periodic, 348, 722
periodic delta, 349
singular periodic delta, 353
H¨older continuousfunction, 858
H¨older inequality, 845
integral, 881
weighted integral, 883
H¨older, O.L., 845, 859, 862, 881
Hadamard inequality,
104
Hadamard, J.S., 104
Hairer, E., 588
hat function, 713

Index
905
heat equation, 621
classicalsolution, 641
classicalsolutionin
Rd, 637
fundamentalsolution, 638
weak solution, 645
heat operator, 626
Heine, H.E., 454, 857
Heine–Borel Theorem, 857
Helmholtzequation, 625
Helmholtz,H.L.F., 625
Hermite interpolating polynomial, 242
Hermite interpolation, 242
error, 243
Hermite, C., 7, 242, 257, 262
Hermite–Genocchi Theorem, 257
Hermitianpositive deﬁnite (HPD)matrix,
26,
57
properties, 57, 156
square root of, 159
Hessenberg matrix, 211
Hessenberg, K.A., 211
Heun’s method, 543
Heun, K., 543
Hilbertspace, 452, 596
anti–dual, 736
dual, 455
operator norm, 455
duality pairing, 463
operator norm, 459
Hilbert,D., 452
holomorphic function, 244
homogeneous zero stability, 569
Hooke, R., 615
Horner’s method, 250
Horner, W.G., 250
Householder reﬂector, 110
Householder triangulation, 113
Householder, A.S., 110, 137
hyperbolic system
Friedrichs, 656
strictlyhyperbolic, 656
symmetric, 656
symmetric hyperbolic, 656
inﬁmumof a function, 860
inﬂow boundary conditions, 618
initialvalue problem (IVP),
509, 510
classicalsolution, 510
linearly dissipativesystem,
583
mildsolution, 510
stationary point, 522
stiﬀnessof, 583
trajectory, 521
Lyapunovstable, 521
unstable, 521
weak form, 597
inner product, 847
H1(Ω), 889
L2, 881
L2
h, 673, 793
L2
w, 882, 885
ℓ2(Cn), 848
ℓ2(Rn), 848
ℓ2(Z; C), 330
CGL lumped-mass, 753
discrete, 673
Euclidean, 848
inner product space, 847
IntegralMean Value Theorem,
864
IntermediateValue Theorem, 860
in Cd, 866
interpolatingpolynomial, 232
interpolation
Chebyshev–Lagrange, 282
Hermite, 242
Lagrange, 236, 715
Newton–Hermite, 262
Newton–Lagrange, 249
trigonometric, 347, 356
inverse Discrete Fourier Transform, 352
inverse iterationmethod, 209
isomorphism, 843
iterative method, 122
adaptive, 123
Chebyshev’s method, 145
consistent, 122, 123
error transfer matrix, 123
explicit, 123
Gauss–Seidel, 128
symmetric, 139
iterationfunction, 122
iterator matrix, 123
Jacobi, 125
linear, 122
matrixsplitting, 125
minimalcorrections method, 147
minimalresidualmethod,
146
relaxation, 135
symmetric, 139
Richardson, 133
stationary, 123
successive over-relaxation(SOR), 135
symmetrized, 138
two-layer, 122
iterator matrix, 123
Jackson’sTheorem, 295, 360
Jackson, D., 295, 360
Jacobi’s method, 125
Jacobi, C.G.J., 126, 868
Jacobianmatrix, 868
John, F., 137

906
Index
Jordan Curve Theorem, 244
Jordan, M.E.C., 244
Kahan,W.M., 222
Kantorovich inequality, 164
Kantorovich Theorem, 442
Kantorovich, L.V., 164, 442
Kirchhoﬀ, G.R., 653
Kronecker delta function, 843
Kronecker, L., 843
Krylov subspace, 169
Krylov, A.N., 169
Kutta, M.W., 539
Lagrangeinterpolating polynomial,
236
Lagrangeinterpolation, 236, 715
error, 239
complex form of, 247
in the ﬁnite element method, 707
Lagrangenodal basis, 235, 705, 714
Lagrange,J.L., 235, 236
Laplace, P.S., 626, 869
Laplacianoperator, 621, 626, 869
Laurent expansion, 245
Laurent, P.A., 245
Lax’s Principle, 665, 677
Lax, P.D., 632, 665, 677, 812, 827
Lax–Friedrichs method
transport equation, 812
Lax–MilgramTheorem, 632
Lax–Wendroﬀmethod
transport equation, 813
leapfrog method
heat equation, 806
least squares approximation
trigonometric, 368
least squares polynomial, 301
Lebesgue constant, 234, 237
Lebesgue DiﬀerentiationTheorem, 672
Lebesgue function, 237, 238
Lebesgue, H.L., 234, 672
Legendre polynomial, 310
transformed, 547
Legendre, A.M., 310, 547, 548
Lewy, H., 781
limit
inferior (liminf), 855
superior (limsup), 855
Lindel¨of, E.L., 511
line search, 161
linear dependence, 841
linear multi-stepmethod,
555
Adams–Bashforth method,
562
Adams–Moultonmethod, 562
backward diﬀerentiationformula(BDF),
566
characteristic polynomials, 558
convergence, 575
homogeneous zero stability, 569
local truncationerror, 556
logarithm method, 559
method of C’s, 556
order of, 556
root condition, 568
stability polynomial, 589
zero stability, 568
linear operator, 3
adjoint, 7
bounded, 457
eigenvalue, 15
eigenvector, 15
induced norm, 9
monotone, 291
preconditioner, 469
self-adjoint, 7
spectral decomposition, 15
spectral radius, 73
linear stability domain,
584
linear system
consistent, 89
generalized solution, 89
least squares solution, 89
minimalnorm, 106
overdetermined, 89
weak solution, 89
linearly dissipativesystem,
583
Lipschitzcontinuousfunction,
424, 858
Lipschitzdomain, 887
Lipschitzsmooth function
local, 465
Lipschitz,R.O.S., 465, 510, 859
load vector, 702
Lobatto, R., 751
local truncationerror (LTE), 527
logarithm method, 559
lower Hessenberg matrix, 211
lower semi-continuousfunction, 460
LU factorization, 35
complexity, 43, 50
existence, 36
uniqueness, 43
Lyapunov,A.M., 521
M-matrix, 550
matrix, 4
adjoint, 7
Cholesky factorization, 59
existence, 59
coeﬃcient, 31
column complete, 40
column rank, 6
column space, 6

Index
907
matrix(cont.)
condition number of, 80
conjugate transpose, 7
convergence to zero, 77
cyclically tridiagonal, 34, 725
defective, 13
diagonal, 32
diagonalizability criterion,
13
diagonalizable, 13
eigenpair, 12
eigenvalue, 12
algebraic multiplicity, 12
geometric multiplicity, 12
eigenvector, 12
elementary, 40
column complete, 41
error transfer, 123
Frobenius, 223
Givens rotation, 119
Hermitian, 7
positive deﬁnite, 57
Householder reﬂector, 110
idempotent, 93
identity, 8
image, 6
iterator, 123
Jacobian, 868
kernel, 6
lower Hessenberg, 211
LU factorization, 35
M-matrix, 550
matrix–matrixproduct, 4
matrix–vector product, 5
normal, 15
nullspace, 6
nullity, 7
orthogonal, 9
permutation, 44
preconditioner, 163
projection, 93
orthogonal, 95
rank-one, 97
pseudo-inverse, 30, 106
properties, 107
QR factorization, 101
range, 6
rank, 6
rank-one, 27
row rank, 6
row space, 6
Schur complement, 63
Schur normal form, 14
similarity, 13
singular value decomposition (SVD),
21
singular values, 21
singular vectors, 21
skew-symmetric, 7
spectral decomposition, 14
general, 160
spectral radius, 73
spectrum, 12
strictlydiagonallydominant,
51
sub-matrix, 36
leadingprinciple, 36
symmetric, 7
positive deﬁnite, 60
Toeplitz symmetric tridiagonal(TST),
60,
735
transpose, 7
triangular, 13, 32
triangularization
Householder, 113
tridiagonal, 33
unitary, 9
Vandermonde, 232
maximummodulusprinciple,
246
maximumof a function, 860
maximumprinciple
discrete, 682, 694
for elliptic equations, 628
parabolic equations, 646
Maxwell, J.C., 657
Mazur’s Lemma, 462
Mazur, S.M., 462
Mean Value Theorem, 861
method of C’s, 556
method of characteristics, 648
method of undeterminedcoeﬃcients, 697
metric space, 876
midpointmethod
for approximatingIVPs, 543
for IVPs, 526
midpointrule
quadrature, 394
Milgram,A.N., 632
minimaxproblem, 266, 267
minimumof a function, 860
Minkowski inequality,
845
integral, 881
weighted integral, 883
Minkowski, H., 845, 881
modulusof continuity, 286
modulusof smoothness, 296
Moore, E.H., 30, 106
Moulton, F.R., 562
multi-index, 866
order, 866
n-simplex
canonical, 257
Nesterov, Y.E., 489
Neumannseries, 83

908
Index
Neumann,C.G., 83, 616
Newton basis, 249
Newton construction, 249
Newton’s method, 428
aﬃne invariance, 485
damped, 487
for optimization, 485
nonlinear systems, 440
convergence, 440
quadraticconvergence, 429
quasi-Newton, 489
simpliﬁed, 427, 433
Newton, I., 249, 262, 385, 428,
433, 509
Newton–Cotes quadrature, 385
Newton–Hermite interpolation, 262
error, 262
Newton–Lagrangeinterpolation, 249
error, 255
Nicolson, P., 777, 813
Nitsche, J.A., 709
nodal polynomial, 237, 249
nodal set, 232
nonlinear equation, 419
roots, 419
zeros, 419
norm, 843
L∞, 881
Lp, 881
Lp
h, 671, 692, 723
Lp
w, 882
ℓ2(Z; C), 330
CGL lumped-mass, 753
convex, 269
energy, 704
equivalence, 846
Euclidean, 848
Frobenius, 9
matrix1-norm, 10
matrix p-norm, 10
matrixenergy norm, 140
matrixmax-norm, 9
matrixnorm
consistent, 11
sub-multiplicative, 11
p-norm, 844
strictlyconvex, 269
normal equation, 90
normal matrix, 15
Nørsett, S.P., 588
numericaldiﬀusion, 815
operator
coercive, 596
curl, 869
divergence, 869
interpolation, 234
CGL, 754
Laplacian, 869
linear, 3
bounded, 457
monotone, 291
linear preconditioner, 469
monotone, 596
optimization
convex, 156, 451
existence and uniquenessof a minimizer,
460
unconstrained, 451
order of consistency, 527
orthogonalcomplement, 849
orthogonalmatrix, 9
orthogonalpolynomial, 301
orthogonality, 849
Galerkin, 170, 703
P´eclet number, 689
P´eclet, J.C.E., 689
Pad´e approximation, 588, 607
Pad´e, H.E., 588, 607
parabolic boundary,
646
Parseval’srelation, 328
discrete, 352
Parseval, M.A., 352
partialdiﬀerentialoperator,
626
curl, 658
elliptic, 626
heat, 626
hyperbolic, 626
Laplacian, 626
parabolic, 626
ultra-hyperbolic, 626
ultra-parabolic, 626
wave, 626
partition, 598
conforming, 712
elements, 711
polygonal, 711
path, 244
closed, 244
contour, 244
Jordan, 244
simple, 244
Peano Kernel Theorem, 378
Peano, G., 378
Penrose, R., 30, 106
periodic convolution, 325
Petrov, G.I., 605
Petrov–Galerkin method, 605
Picard, C.E., 511
piecewise diﬀerentiablefunction, 334
Plancherel, M., 637

Index
909
Poincar´e inequality,
889, 894
discrete, 686
for periodic functions, 727
Poincar´e, J.H., 686, 727, 889
Poisson equation, 619
Poisson problem, 630
periodic, 721
Poisson, S.D., 619, 630, 653, 721
polarizationidentity,
789, 820
pole
of degree k, 246
simple, 246
polynomial
Bernoulli, 401
Bernstein, 286
characteristic, 12
Chebyshev, 146, 179, 190, 279
interpolating, 232
least squares, 301
Legendre, 310
nodal, 237, 249
orthogonal, 301
transformed Legendre, 547
trigonometric, 321
Wilkinson, 198
power iterationmethod, 207
preconditioned steepest descent (PSD),
163, 473
with approximate line search (PSD–ALS),
480
preconditioner
linear operator, 469
matrix, 163
projection
Chebyshev, 749
elliptic, 748
Fourier, 323
least squares
polynomial, 304
mean-zero, 734
projection matrix, 93
orthogonal, 95
rank-one, 97
pseudo-spectral
derivative, 732
method, 732, 733
QR factorization, 101
full, 104
reduced, 102
QR iteration method, 214
with shifts, 221
quadraturerule, 373
composite, 395
composite midpoint, 398
composite Simpson’s, 397
composite trapezoidal, 397
consistency, 374
corrected composite Simpson’s, 407
corrected composite trapezoidal, 407
degree r, 373
error, 373
Gaussian, 409
interpolatory type, 374
midpoint, 394
Newton–Cotes, 385
node, 373
simple, 373
Simpson’s, 373
trapezoidal, 373
weights, 373
quasi-Newtonmethods, 489
Ralston’smethod, 543
Ralston,A., 543
Rank-One Decomposition Theorem, 27
Rank-Plus-Nullity Theorem,
26
rationalpolynomial, 541
A-acceptable, 586
Rayleigh quotient, 205
Rayleigh, Lord, 205
reaction–diﬀusionequation, 616
reaction–diﬀusionproblem
periodic, 721
relaxationmethod
for nonlinear equations, 425
residualvector, 81, 89
residue, 246
residue theorem, 246
Richardson method
heat equation, 803
Richardson’s method, 133
Richardson, L.F., 133, 803
Riemann zeta function, 343
Riemann, G.F.B., 343, 862
Riesz RepresentationTheorem, 456, 737
Riesz, F., 329, 456
Riesz–Fischer Theorem, 329
Ritz method, 701
Ritz, W.H.W., 701
Robin, V.G., 616
Rodrigues Formula, 310
Rodrigues, B.O., 310
Rolle’s Theorem, 861
Rolle, M., 861
root condition, 568
roots of unity, 349
Rothe, E., 774
row echelon form, 38
Runge phenomenon, 240–242, 249,
283
Runge, C.D.T., 242, 539

910
Index
Runge–Kuttamethod, 539
A-stability of, 585, 586
algebraic stability,
550
ampliﬁcationfactor, 541
Butcher tableau, 539
classicalRK, 544
collocation method, 544
diagonallyimplicitRunge–Kutta(DIRK),
539
explicit Runge–Kutta (ERK), 539
Gauss–Legendre–Runge–Kutta, 548
Heun’s method, 543
implicitRunge–Kutta(IRK),
539
M-matrixof, 550
midpointmethod, 543
Ralston’smethod, 543
RK type, 553
Schmidt, E., 849
Schur complement, 149
Schur normal form, 14, 75
Schur, I., 14, 63
Schwarz Lemma, 484
Schwarz, K.H.A., 484, 848
search direction, 161
secant method, 439
Second Law of Thermodynamics, 620
Seidel, P.L. von, 128
seminorm
H1
h, 686
Sobolev, 893
sequence, 854
bounded, 854
Cauchy, 854
convergent, 453, 854
decreasing, 854
increasing, 854
limitof, 854
linear convergence rate of, 856
quadraticconvergence rate of, 856
subsequence, 455, 854
weakly convergent, 453
sequentialcompactness, 455
set
ball, 866
bounded, 857
bounded energy, 467
closed, 454, 857
closure, 454, 858
compact, 857
convex, 460, 866
domain, 866
limitpoint of, 858
nodal, 232
sequentiallycompact, 455
weak closure, 454
weakly closed, 454
shiftoperator, 668
simpleroot, 420
simpliﬁedNewton method, 433
Simpson’srule, 373
composite, 397
Simpson,T., 373
single-stepmethod, 526
convergent, 527
local truncation error (LTE), 527
order of, 527
slope function approximation, 526
singular value decomposition (SVD),
21
computationof, 221
existence and uniqueness, 22
full, 21
reduced, 21
singular values,
21
singular vectors, 21
singularity
isolated, 246
skew stencil method
heat equation, 806
slope function, 510
approximation, 526
monotone, 520
u-Lipschitz, 510
Sobolev embeddingtheorem, 706
Sobolev space, 887, 891
Chebyshev weighted, 743
fractional, 341
higher order, 890
periodic, 340, 892
Sobolev, S.L., 743, 887
solenoidalﬂow, 612
space–time cylinder, 646
span, 841
spectral convergence, 341
Spectral Decomposition Theorem, 14, 15
spectral radius, 73
speed
phase, 822
spline, 605
square root method, 65
stability
Lyapunov, 521
von Neumann, 792, 797
steady state, 618
steepest descent method, 163
preconditioned, 163
Steﬀensen’s method, 435
Steﬀensen, J.F., 435
stiﬀness, 583
ratio, 583
stiﬀnessmatrix, 702
ﬁnite diﬀerence, 678, 692

Index
911
Stokes, G.G., 615
Strang’sFirst Lemma, 766
Strang, W.G., 766
strictlydiagonallydominant(SDD) matrix,
51
subspace, 841
substitution
back, 32
forward, 32
SummationMean Value Theorem, 864
support of a function, 705, 887
supremum of a function, 860
symmetric positive deﬁnite (SPD) matrix,
60
Taylor’s method, 526
for IVPs
convergence, 532
Taylor’s Theorem, 861
with integralremainder, 863
Taylor, B., 526, 861
test function, 890
theorem
divergence, 613
Thomas algorithm, 33
Thomas, L.H., 33
Toeplitz symmetric tridiagonal(TST) matrix,
735
Toeplitz, O., 60
trace inequality, 889
Transform
Chebyshev, 750
CGL, 759
Discrete CGL, 759
Cosine
Discrete, 756
Fourier, 330
Z, 794
Discrete, 345, 350, 794
inverse Discrete, 352
transport equation, 618
trapezoidalmethod
for IVPs, 526
convergence, 530
trapezoidalrule, 373
composite, 397
triangulation, 711
midpointsof, 713
boundary, 713
interior, 713
shape regular, 716
vertices of, 712
boundary, 712
interior, 712
Tricomi, F.G., 627
trigonometric interpolation, 347, 356
trigonometric least squares approximation,
368
trigonometric polynomial, 321
two–step Newton method, 437
uniformly continuousfunction, 858
unitary matrix, 9
upwind method, 601
advection–diﬀusionequation, 792
transport equation, 812
Vandermonde construction, 233
Vandermonde matrix, 232
ill-conditioned, 234
Vandermonde, A.T., 232
variationalcrime, 763, 766
vector decomposition
complementary, 94
orthogonal, 95
vector space, 839
Banach, 453
basis, 841
basiscompletion, 850
complete, 451
dimension, 841
Hilbert, 452, 596
inner product, 847
orthogonality, 849
isomorphism, 843
linear combination, 840
linear dependence, 841
n-vectors, 840
norm, 843
norm equivalence, 846
orthogonalcomplement, 849
subspace, 841
complementary, 94
Krylov, 169
orthogonal, 95
sum, 94
von Neumannstability,
792
von Neumann,J., 792, 797
Wanner, G., 588
Wanner–Hairer–NørsettTheorem, 588
Warming,R.F., 813
wave equation, 623
damped, 625
energy conservation, 654
speed of propagation, 623
weak solution, 654
existence, 655
regularity, 656
wave operator, 626
weak closure of a set, 454
weak derivative, 891

912
Index
weak formulation
Dirichlet problem, 632
heat equation, 645
transport equation, 649
wave equation, 654
weighted, 746
weakly closed set, 454
weakly lower semi-continuous function, 460
Weierstrass Approximation Theorem, 231,
286
Weierstrass, K.T.W., 231, 294, 856, 871
weight function, 882
Chebyshev, 743
Legendre, 311
Wendroﬀ, B., 813, 827
Wilbraham, H., 313
Wilkinson polynomial, 198
Wilkinson, J.H., 198
Young’s inequality, 845
Young, G.Y., 28
Young, W.H., 845
zero stability, 568


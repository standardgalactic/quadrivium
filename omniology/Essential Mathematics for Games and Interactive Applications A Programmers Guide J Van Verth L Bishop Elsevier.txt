Essential Mathematics
for Games and
Interactive Applications
A Programmer’s Guide
James M. Van Verth
Red Storm Entertainment
Lars M. Bishop
Numerical Design Limited
Amsterdam Boston Heidelberg
London New York Oxford
Paris San Diego San Francisco
Singapore Sydney Tokyo
Morgan Kaufmann Publishers is an imprint of Elsevier

Senior Editor
Tim Cox
Publishing Services Manager
Simon Crump
Production Editor
Troy Lilly
Editorial Assistant
Richard Camp
Cover Design
Chen Design Associates
Text Design
Julio Esperas
Composition
Cepha Imaging Pvt. Ltd.
Technical Illustration
Dartmouth Publishing, Inc.
Copyeditor
Yonie Overton
Proofreader
John Bregoli
Indexer
Northwind Editorial Services
Interior printer
The Maple-Vail Book Manufacturing Group
Cover printer
Phoenix Color Corp.
Morgan Kaufmann Publishers is an imprint of Elsevier.
500 Sansome Street, Suite 400, San Francisco, CA 94111
This book is printed on acid-free paper.
© 2004 by Elsevier Inc. All rights reserved.
Designations used by companies to distinguish their products are often claimed as trade-
marks or registered trademarks. In all instances in which Morgan Kaufmann Publishers is
aware of a claim, the product names appear in initial capital or all capital letters. Readers,
however, should contact the appropriate companies for more complete information regard-
ing trademarks and registration.
No part of this publication may be reproduced, stored in a retrieval system, or transmit-
ted in any form or by any means—electronic, mechanical, photocopying, scanning, or
otherwise—without prior written permission of the publisher.
Permissions may be sought directly from Elsevier’s Science & Technology Rights
Department in Oxford,
UK: phone:
(+44) 1865 843830,
fax:
(+44) 1865 853333,
e-mail: permissions@elsevier.com.uk. You may also complete your request on-line via the
Elsevier homepage (http://elsevier.com) by selecting “Customer Support” and then “Obtain-
ing Permissions.”
Library of Congress Cataloging-in-Publication Data
Van Verth, James M.
Essential mathematics for games and interactive applications : a programmer’s guide /
James M. Van Verth and Lars M. Bishop.
p. cm. – (The Morgan Kaufmann series in interactive 3D technology)
Includes bibliographical references and index.
ISBN-13: 978-1-55860-863-4
ISBN-10: 1-55860-863-X (hardcover : alk. paper)
1. Computer games–Programming. 2. Three-dimensional display systems–Mathematics.
I. Bishop, Lars M. II. Title. III. Series.
QA76.76.C672V47 2004
794.8′1711–dc22
2003028267
ISBN-13: 978-1-55860-863-4
ISBN-10: 1-55860-863-X
For information on all Morgan Kaufmann publications,
visit our Web site at www.mkp.com.
Printed in the United States of America
05 06 07 08
5 4 3 2

Dedications
To Harry, Mur, and Fiona: my past, present, and future. —Jim
To Dad and Mom (Steve and Helene Bishop); your love and support have
always been by my side. Thank you. —Lars

About the Authors
James M. Van Verth is a founding member of Red Storm Entertainment, a
division of Ubisoft, where he has been a lead engineer for six years. For the
past ﬁve years he also has been a regular speaker at the Game Developers
Conference, teaching the all-day tutorial, “Essential Math for Programmers,”
on which this book is based. He began his game industry career at Virtus
Corporation, working as a sound and graphics engineer for the title Tom
Clancy: SSN. His ﬁrst position at Red Storm was as project lead and designer
of Tom Clancy’s Politika, the ﬁrst commercial Java game. This was followed
by the land warfare game Force 21 where he acted as lead engineer, focusing
on 3D graphics, vehicle physics, and pathﬁnding. His latest role at Red Storm
is as rendering technology lead for a well-known squad combat franchise.
His background includes a B.A. in mathematics and computer science from
Dartmouth College, an M.S. in computer science from the State University of
New York at Buffalo, and an M.S. in computer science from the University of
North Carolina at Chapel Hill. This is his ﬁrst book.
Lars M. Bishop is the Chief Technology Ofﬁcer for Numerical Design Limited
(NDL). Since 1996, he has specialized in real-time 3D game rendering tech-
nologies at NDL. He was a founding member of the team that created NDL’s
popular NetImmerse and Gamebryo 3D game engines, which are used in over
50 games, such as Bethesda Softworks’ Morrowind and Mythic Entertain-
ment’s Dark Age of Camelot. Lars is currently working on the development of
next-generation NDL products, speciﬁcally 3D engines and tools for handheld
devices. He holds a B.S. in mathematics and computer science from Brown
University and an M.S. in computer science from the University of North
Carolina at Chapel Hill.
vii

Contents
Preface
xxi
Introduction
1
The (Continued) Rise of 3D Games 1
How to Read this Book 2
Part I, Core Mathematics 2
Part II, Rendering 3
Part III, Animation 4
Part IV, Simulation 4
Appendices 4
Interactive Demo Applications 5
Support Libraries 5
Math Libraries 6
Engine and Rendering Libraries 6
References and Further Reading 7
Part I
Core Mathematics
Chapter 1
Vectors and Points
11
1.1
Introduction 11
1.2
Vectors 12
1.2.1
Vectors as Geometry 12
1.2.2
Real Vector Spaces 15
1.2.3
Linear Combinations and Basis Vectors 18
1.2.4
Basic Vector Class Implementation 22
ix

x
Contents
1.2.5
Vector Length 25
1.2.6
Dot Product 28
1.2.7
Gram-Schmidt Orthogonalization 33
1.2.8
Cross Product 34
1.2.9
Triple Products 37
1.3
Points 41
1.3.1
Points as Geometry 41
1.3.2
Affine Spaces 43
1.3.3
Affine Combinations 46
1.3.4
Point Implementation 47
1.3.5
Polar and Spherical Coordinates 49
1.4
Lines 52
1.4.1
Definition 52
1.4.2
Parameterized Lines 53
1.4.3
Generalized Line Equation 55
1.4.4
Collinear Points 57
1.5
Planes 57
1.5.1
Parameterized Planes 57
1.5.2
Generalized Plane Equation 58
1.5.3
Coplanar Points 60
1.6
Polygons and Triangles 60
1.7
Chapter Summary 63
Chapter 2
Linear Transformations and Matrices
65
2.1
Introduction 65
2.2
Linear Transformations 66
2.2.1
Definitions 66
2.2.2
Null Space and Range 67
2.2.3
Linear Transformations and Basis Vectors 69
2.3
Matrices 71
2.3.1
Introduction to Matrices 71
2.3.2
Simple Operations 73
2.3.3
Vector Representation 75
2.3.4
Block Matrices 75
2.3.5
Matrix Product 77
2.3.6
Transforming Vectors 79

Contents xi
2.3.7
Combining Linear Transformations 81
2.3.8
Identity Matrix 83
2.3.9
Performing Vector Operations with Matrices 84
2.3.10
Implementation 85
2.4
Systems of Linear Equations 88
2.4.1
Definition 88
2.4.2
Solving Linear Systems 89
2.4.3
Gaussian Elimination 91
2.5
Matrix Inverse 95
2.5.1
Definition 95
2.5.2
Simple Inverses 97
2.6
The Determinant 99
2.6.1
Definition 99
2.6.2
Computing the Determinant 100
2.6.3
Determinants and Elementary Row Operations 103
2.6.4
Adjoint Matrix and Inverse 105
2.7
Chapter Summary 106
Chapter 3
Affine Transformations
107
3.1
Introduction 107
3.2
Affine Transformations 108
3.2.1
Definition 108
3.2.2
Representation 109
3.3
Standard Affine Transformations 113
3.3.1
Translation 113
3.3.2
Rotation 115
3.3.3
Scaling 124
3.3.4
Reflection 126
3.3.5
Shear 130
3.3.6
Applying an Affine Transformation Around an Arbitrary
Point 132
3.3.7
Transforming Plane Normals 134
3.4
Using Affine Transformations 135
3.4.1
Manipulation of Game Objects 135
3.4.2
Matrix Decomposition 141
3.4.3
Avoiding Matrix Decomposition 143

xii
Contents
3.5
Object Hierarchies and Scene Graphs 145
3.5.1
Object Hierarchies 145
3.5.2
Scene Graphs 148
3.6
Chapter Summary 153
Chapter 4
Real-World Computer Number Representation
155
4.1
Introduction 155
4.2
Representing Integral Types on a Computer 156
4.2.1
Finiteness of Representation 156
4.2.2
Range 156
4.3
Representing Real Numbers 159
4.3.1
Approximations 160
4.3.2
Precision and Error 160
4.4
Fixed Point 162
4.4.1
Introduction 162
4.4.2
Basic Representation 162
4.4.3
Range and Precision 163
4.4.4
Addition and Subtraction 166
4.4.5
Multiplication 166
4.4.6
Division 168
4.4.7
Real-World Fixed Point 169
4.4.8
Intermediate Value Overflow and Underflow 170
4.4.9
Limits of Fixed Point 173
4.4.10
Fixed Point Summary 173
4.5
Floating-Point Numbers 173
4.5.1
Review: Scientific Notation 173
4.5.2
A Restricted Scientific Notation 174
4.6
Binary “Scientific Notation” 176
4.7
IEEE 754 Floating Point Standard 177
4.7.1
Basic Representation 177
4.7.2
Range and Precision 179
4.7.3
Arithmetic Operations 181
4.7.4
Special Values 184
4.7.5
Very Small Values 188
4.7.6
Catastrophic Cancellation 190
4.7.7
Double Precision 192

Contents xiii
4.8
Real-World Floating Point 193
4.8.1
Internal FPU Precision 193
4.8.2
Performance 194
4.8.3
IEEE Specification Compliance 197
4.9
Code 198
4.10 Chapter Summary 199
Part II
RENDERING
Chapter 5
Viewing and Projection
203
5.1
Introduction 203
5.2
The View Frame and View Transformation 204
5.2.1
Defining a Virtual Camera 204
5.2.2
Controlling the Camera 207
5.2.3
Constructing the View Transformation 210
5.3
Projective Transformation 211
5.3.1
Definition 211
5.3.2
The View Frustum 215
5.3.3
Normalized Device Coordinates 217
5.3.4
Homogeneous Coordinates 219
5.3.5
Perspective Projection 220
5.3.6
Oblique Perspective 227
5.3.7
Orthographic Parallel Projection 229
5.3.8
Oblique Parallel Projection 231
5.4
Culling and Clipping 233
5.4.1
Why Cull or Clip? 233
5.4.2
Culling 237
5.4.3
General Plane Clipping 238
5.4.4
Homogeneous Clipping 243
5.5
Screen Transformation 245
5.6
Picking 248
5.7
Management of Viewing Transformations 250
5.8
Chapter Summary 253

xiv
Contents
Chapter 6
Geometry, Shading, and Texturing
255
6.1
Introduction 255
6.2
Color Representation 256
6.2.1
The RGB Color Model 256
6.2.2
Colors as “Vectors” 257
6.2.3
Operations on Colors 258
6.2.4
Color Range Limitation 259
6.2.5
Alpha Values 261
6.2.6
Color Storage Formats 262
6.2.7
Colors in OpenGL 263
6.3
Points and Vertices 264
6.3.1
Per-Vertex Attributes 265
6.4
Surface Representation 266
6.4.1
Vertices and Surface Ambiguity 267
6.4.2
Triangles 268
6.4.3
Triangle Attributes 269
6.4.4
Vertex Indices 272
6.4.5
OpenGL Vertex Indices 274
6.5
Coloring a Surface 276
6.6
Using Constant Colors 276
6.6.1
Per-Object Colors 277
6.6.2
Per-Triangle Colors 277
6.6.3
Per-Vertex Colors 278
6.6.4
Limitations of Basic Shading
Methods 283
6.7
Texture Mapping 284
6.7.1
Introduction 284
6.7.2
Shading via Image Lookup 285
6.7.3
Texture Images 285
6.8
Texture Coordinates 289
6.8.1
Mapping Texture Coordinates 292
6.8.2
Generating Texture Coordinates 293
6.8.3
Texture Coordinate Discontinuities 294
6.8.4
Mapping Outside the Unit Square 296
6.9
Reviewing the Steps of Texturing 301
6.10 Limitations of Texturing 303

Contents xv
6.11 Procedural Colors and Shaders 304
6.12 Chapter Summary 307
Chapter 7
Lighting
309
7.1
Introduction 309
7.2
Basics of Light Approximation 310
7.2.1
Measuring Light 310
7.2.2
Light as a Ray 312
7.3
Lighting Approximation (OpenGL) 312
7.4
Types of Light Sources 313
7.4.1
Directional Lights 314
7.4.2
Point Lights 315
7.4.3
Spotlights 318
7.4.4
Other Types of Light Sources 322
7.5
Surface Materials and Light Interaction 323
7.5.1
OpenGL Materials 323
7.6
Categories of Light 324
7.6.1
Emission 324
7.6.2
Ambient 325
7.6.3
Diffuse 327
7.6.4
Specular 330
7.7
Combined Lighting Equation 335
7.8
Lighting and Shading 338
7.8.1
Flat-Shaded Lighting 338
7.8.2
Per-Vertex Lighting 340
7.8.3
Per-Pixel Lighting (Phong Shading) 344
7.9
Merging Textures and Lighting 348
7.9.1
Specular Lighting and Textures 349
7.10 Lighting and Programmable Shaders 351
7.11 Chapter Summary 351
Chapter 8
Rasterization
353
8.1
Introduction 353
8.2
Displays and Framebuffers 355
8.2.1
Framebuffer Memory Organization 356

xvi
Contents
8.2.2
Interlacing 357
8.2.3
Multiple Buffers 358
8.3
Conceptual Rasterization Pipeline 360
8.4
Determining the Pixels Contained by a Triangle 360
8.5
Determining Which Pixels are Visible 362
8.5.1
Depth Sorting 362
8.5.2
Depth Buffering 365
8.5.3
Depth Buffering in OpenGL 374
8.6
Computing Source Pixel Colors 375
8.6.1
Flat Colors 375
8.6.2
Gouraud Colors 376
8.7
Rasterizing Textures 378
8.7.1
Texture Coordinate Review 379
8.7.2
Interpolating Texture Coordinates 380
8.7.3
Mapping a Coordinate to a Texel 383
8.7.4
Mipmapping 391
8.8
Blending 401
8.8.1
Blending and Z-Buffering 403
8.8.2
Alternative Blending Modes 404
8.8.3
Blending and OpenGL 405
8.9
Antialiasing 406
8.9.1
Antialiasing in Practice 412
8.9.2
Antialiasing in OpenGL 414
8.10 Chapter Summary 416
Part III
Animation
Chapter 9
Curves
419
9.1
Introduction 419
9.2
General Definitions 421
9.3
Linear Interpolation 422
9.3.1
Definition 422
9.3.2
Piecewise Linear Interpolation 424
9.4
Lagrange Polynomials 425

Contents xvii
9.5
Hermite Curves 427
9.5.1
Definition 427
9.5.2
Automatic Generation of Hermite Curves 433
9.5.3
Natural, Cyclic, and Acyclic End Conditions 435
9.6
Catmull-Rom Splines 438
9.7
Bézier Curves 440
9.7.1
Definition 440
9.7.2
Piecewise Bézier Curves 443
9.8
B-Splines 444
9.9
Rational Curves 448
9.10 Rendering Curves 450
9.10.1
Forward Differencing 450
9.10.2
Midpoint Subdivision 453
9.10.3
Using OpenGL 456
9.11 Controlling Speed Along a Curve 459
9.11.1
Moving at Constant Speed 459
9.11.2
Computing Arc Length 462
9.11.3
Ease-In and Ease-Out 465
9.12 Camera Control 467
9.13 Chapter Summary 470
Chapter 10
Orientation Representation
471
10.1 Introduction 471
10.2 Rotation Matrices 473
10.3 Fixed and Euler Angles 474
10.3.1
Definition 474
10.3.2
Format Conversion 476
10.3.3
Concatenation 478
10.3.4
Vector Rotation 478
10.3.5
Other Issues 479
10.4 Axis-Angle Representation 481
10.4.1
Definition 481
10.4.2
Format Conversion 482
10.4.3
Concatenation 484
10.4.4
Vector Rotation 484
10.4.5
Section Summary 485

xviii
Contents
10.5 Quaternions 485
10.5.1
Definition 485
10.5.2
Rotation Quaternions 486
10.5.3
Format Conversion 489
10.5.4
Addition and Scalar Multiplication 493
10.5.5
Negation 493
10.5.6
Magnitude and Normalization 493
10.5.7
Dot Product 494
10.5.8
Concatenation 495
10.5.9
Identity and Inverse 497
10.5.10 Vector Rotation 498
10.5.11 Quaternions and Transformations 501
10.6 Interpolation 501
10.6.1
Linear Interpolation 503
10.6.2
Spherical Linear Interpolation 507
10.6.3
Performance Improvements 510
10.7 Chapter Summary 511
Part IV
Simulation
Chapter 11
Intersection Testing
515
11.1 Introduction 515
11.2 Closest Point and Distance Tests 516
11.2.1
Closest Point on Line to Point 516
11.2.2
Line-Point Distance 518
11.2.3
Closest Point on Line Segment to Point 519
11.2.4
Line Segment–Point Distance 520
11.2.5
Closest Points between Two Lines 522
11.2.6
Line-Line Distance 524
11.2.7
Closest Points between Two Line Segments 525
11.2.8
Line Segment–Line Segment Distance 527
11.2.9
General Linear Components 527
11.3 Object Intersection 528
11.3.1
Spheres 530
11.3.2
Axis-Aligned Bounding Boxes 538

Contents xix
11.3.3
Swept Spheres 546
11.3.4
Object-Oriented Boxes 550
11.3.5
Triangles 556
11.4 A Simple Collision System 562
11.4.1
Choosing a Base Primitive 562
11.4.2
Bounding Hierarchies 563
11.4.3
Dynamic Objects 568
11.4.4
Performance Improvements 569
11.4.5
Related Systems 572
11.4.6
Section Summary 575
11.5 Chapter Summary 575
Chapter 12
Rigid Body Dynamics
577
12.1 Introduction 577
12.2 Linear Dynamics 578
12.2.1
Moving with Constant Acceleration 578
12.2.2
Forces 581
12.2.3
Linear Momentum 582
12.2.4
Moving with Variable Acceleration 582
12.3 Initial Value Problems 585
12.3.1
Definition 585
12.3.2
Euler’s Method 587
12.3.3
Midpoint Method 590
12.3.4
Higher-Order Methods 593
12.3.5
Verlet Integration 594
12.3.6
Implicit Methods 596
12.4 Rotational Dynamics 599
12.4.1
Definitions 599
12.4.2
Orientation and Angular Velocity 599
12.4.3
Torque 600
12.4.4
Angular Momentum and Inertial Tensor 603
12.4.5
Integrating Rotational Quantities 605
12.5 Collision Response 607
12.5.1
Locating the Point of Collision 607
12.5.2
Linear Collision Response 611
12.5.3
Rotational Collision Response 616
12.5.4
Other Response Techniques 619

xx
Contents
12.6 Efficiency 620
12.7 Chapter Summary 621
Appendix A
Trigonometry Review
623
A.1
Basic Definitions 623
A.1.1
Ratios on the Right Triangle 623
A.1.2
Extending to General Angles 624
A.2
Properties of Triangles 626
A.3
Trigonometric Identities 629
A.3.1
Pythagorean Identities 629
A.3.2
Complementary Angle 629
A.3.3
Even-Odd 630
A.3.4
Compound Angle 631
A.3.5
Double Angle 631
A.3.6
Half Angle 632
A.4
Inverses 633
Appendix B
Calculus Review
635
B.1
Limits and Continuity 635
B.1.1
Limits 635
B.1.2
Continuity 637
B.2
Derivatives 638
B.2.1
Definition 638
B.2.2
Basic Derivatives 640
B.2.3
Derivatives of Transcendental Functions 641
B.2.4
Taylor’s Series 643
B.3
Integrals 645
B.3.1
Definition 645
B.3.2
Evaluating Integrals 647
B.3.3
Trapezoidal Rule 648
B.3.4
Gaussian Quadrature 649
B.4
Space Curves 651
Bibliography
655
Index
663
Trademarks
677
About the CD-ROM
678

Preface
Writing a book is an adventure. To begin with, it is a toy and an
amusement; then it becomes a mistress, and then it becomes a master,
and then a tyrant. The last phase is that just as you are about to be
reconciled to your servitude, you kill the monster, and ﬂing him out to
the public. — Sir Winston Churchill
The Adventure Begins
As humorous as Churchill’s statement is, there is a certain amount of truth to
it; writing this book was indeed an adventure. There is something about the
process of writing, particularly a nonﬁction work like this, that forces you to
test and expand the limits of your knowledge. We hope that you, the reader,
beneﬁt from our hard work.
How does a book like this come about? Many of Churchill’s books began
with his experience — particularly his experience as a world leader in wartime.
This book had a more mundane beginning: Two engineers at Red Storm,
separately, asked Jim to teach them about vectors. These engineers were 2D
game programmers, and 3D was not new, but was starting to replace 2D
at that point. Jim’s project was in a crunch period, so he didn’t have time
to do much about it until proposals were requested for the annual Game
Developers Conference. Remembering the engineers’ request, he thought back
to the classic “Math for SIGGRAPH” course from SIGGRAPH 1989, which he
had attended and enjoyed. Jim ﬁgured that a similar course, at that time
titled “Math for Game Programmers,” could help 2D programmers become
3D programmers.
The course was accepted, and together with a co-speaker, Marcus
Nordenstam, Jim presented it at GDC 2000. The following years (2001–2002)
Jim taught the course alone, as Marcus had moved from the game indus-
try to the ﬁlm industry. The subject matter changed slightly as well, adding
more advanced material such as curves, collision detection, and basic physical
simulation.
xxi

xxii
Preface
It was in 2002 that the seeds of what you hold in your hand were truly
planted. At GDC 2002, another GDC speaker, whose name, alas, is lost to time,
recommended that Jim turn his course into a book. This was an interesting
idea, but how to get it published? As it happened, Jim ran into Dave Eberly at
SIGGRAPH 2002, and he was looking for someone to write just that book for
Morgan Kaufmann. At the same time, Lars was presenting some of the basics
of rendering on handheld devices as part of a SIGGRAPH course. Jim and
Lars discussed the fact that handheld 3D rendering had brought back some
of the “lost arts” of 3D programming, and that this might be included in a
book on mathematics for game programming.
Thus, a co-authorship was formed. Lars joined Jim in teaching the GDC
2003 version of what was now called “Essential Math for Game Program-
mers,” and simultaneously joined Jim to help with the book, helping to expand
the topics covered to include numerical representations. As we began to ﬂesh
out the latter chapters of the outline, Lars was ﬁnding that the advent of
programmable shaders on consumer 3D hardware was bringing more and
more low-level lighting, shading, and texturing questions into his ofﬁce at
NDL. Accordingly, the planned single chapter on “texturing and antialiasing”
became three, covering a wider selection of these rendering topics.
By early 2003, we were furiously typing the ﬁrst full draft of what is
now before you. The experience was fascinating, sometimes frustrating, but
ultimately deeply rewarding. Hopefully, this fascination and respect for the
material will be conveyed to you, the reader. The topics in this book can each
take a lifetime to study to a truly great depth; we hope you will be convinced
to try just that, nonetheless!
Enjoy as you do so, as one of the few things more rewarding than pro-
gramming and seeing a correctly animated, simulated, and rendered scene on
a screen is the conﬁdence of understanding how and why everything worked.
When something in a 3D system goes wrong (and it always does), the best pro-
grammers are never satisﬁed with “I ﬁxed it, but I’m not sure how”; without
understanding, there can be no conﬁdence in the solution, and nothing new
is learned. Such programmers are driven by the desire to understand what
went wrong, how to ﬁx it, and learning from the experience. No other tool in
3D programming is quite as important to this process than the mathematical
bases1 behind it.
Those Who Helped Us Along the Road
In a traditional adventure the protagonists are assisted by various characters
that pass in and out of the pages. Similarly, while this book bears the names
1.
Vector or otherwise.

Preface xxiii
of two people on the cover, the material between its covers bears the mark of
many, many more. We would like to thank a few of them here.
The folks at our publisher, Morgan Kaufman, were extremely patient
and helpful, having undertaken the daunting task of leading two authors
through the process of ﬁnishing their ﬁrst book. In particular we wish to
thank Tim Cox, our editor, and Stacie Pierce and Richard Camp, his assis-
tants over the course of the book, who were patient beyond measure and
willing to provide excellent guidance throughout the project. We would also
like to acknowledge Troy Lilly of Elsevier and Sean Will of Darmouth Publish-
ing for their invaluable assistance throughout the production process. Special
thanks are due to Dave Eberly, our series editor, who read most of the book
several times and provided great encouragement (and the occasional scolding)
through the entire process, one he’s been through ﬁrsthand several times.
Our reviewers were top-notch.
Ian Ashdown,
Steven Woodcock,
John O’Brien, J.R. Parker, Neil Kirby, John Funge, and Michael van Lent
reviewed the initial proposal document. Peter Norvig, Tomas Akenine-Möller,
Steven Woodcock, and John Funge read an early draft of the ﬁrst few chapters,
and provided invaluable comments that signiﬁcantly improved the direc-
tion and content of the material. The entire draft of the book was read by
Ian Ashdown, Wes Hunt, Peter Lipson, Jon McAllister, and Travis Young.
Despite having a tight deadline, they provided page after page of useful feed-
back, keeping us honest and helping us generate a better arc to the material.
Several of them went well above and beyond the call of duty, providing detailed
comments and even re-reading sections of the book that required signiﬁcant
changes. Finally, Clark Gibson, Joe Sauder, and Chris Stoy also deserve nods2
for providing critiques of speciﬁc chapters.
Thanks are also due to several groups of people who received early
versions of parts of the book via Jim’s and/or Lars’s lectures, including
the attendees and reviewers of the “Essential Mathematics” course at GDC
2000–2003, the reviewers and attendees of the “Dynamic Media” course at
SIGGRAPH 2002, and the students of Andy van Dam’s CS123 course in the
fall of 2002. Marcus Nordenstam (GDC 2000) and David Holmes (SIGGRAPH
2002) were a part of the lecture teams for these course presentations that fed
this book, and provided much background that would ﬁlter into the outline of
the book itself. In addition (being Lars’s ofﬁce mate at NDL), David Holmes
provided a weekend and evening sounding board for several of the topics as
they came together. Thanks also to Victor Brueggemann and Garner Halloran,
who asked the questions that started this whole thing off ﬁve years ago.
Jim and Lars would like to acknowledge the folks at their respective jobs,
Red Storm Entertainment and Numerical Design Limited, who were very
understanding with respect to the time-consuming process of creating a book.
2.
They’ve already eaten the cookies.

xxiv
Preface
Also, thanks to the talented engineers at both companies who provided the
probing discussions and great questions that led to and continually fed this
book.
In addition, Jim would like to thank Mur, his long-suffering wife who dealt
with an occasionally absent husband during a pregnancy and the ﬁrst year of
their baby’s life; Fiona, the daughter who still has the decency to recognize
her daddy; his sister, Liz, who provided illustrations for an early draft of this
text; and his parents, Jim and Pat, who gave him the resources to make it in
the world and introduced him to the world of computers so long ago.
Lars would like to thank Jen, his wife, who provided more emotional and
technical support than anyone could ask for, and who put up with more “lost
weekends” and “after the book is done” comments than either of us can count.
And lastly, we would like to thank you, the reader, for joining us on this
adventure. May the teeth of this monster ﬁnd fertile ground in your minds,
and yield a new army of 3D programmers.

Introduction
The (Continued) Rise of 3D Games
Over the past decade or so (driven by increasingly powerful computer
hardware), 3D games have expanded from custom-hardware arcade machines
to the realm of “hardcore” PC games, on to consumer “set top” videogame
consoles, and even onto handheld devices such as personal digital assistants
(PDAs) and cellular telephones. This explosion in popularity has lead to a cor-
responding need for programmers with the ability to program these games.
As a result, programmers are entering the ﬁeld of 3D games and graphics
by teaching themselves the basics, rather than a “classic” University graphics
and mathematics education. At the same time, many University students are
looking to move directly from school into the industry. These different groups
of programmers each have their own set of skills and needs in order to make
the transition. While every programmer’s situation is different, we describe
some of the more common situations in the paragraphs below.
Many existing, self-taught 3D game programmers have strong game expe-
rience and an excellent practical approach to programming, stressing visual
results and strong optimization skills that can be lacking in university com-
puter science programs. However, these programmers are sometimes less
comfortable with the conceptual mathematics that form the underlying basis
of 3D graphics and games. This can make developing, debugging, and optimiz-
ing these systems more of a “trial and error” exercise than would be desired.
Programmers who are already established in other specializations in the
game industry, such as networking or user interfaces, are now ﬁnding that
1

2
Introduction
they want to expand their abilities into core 3D programming. While having
experience with a wide range of game concepts, these programmers often need
to learn or refresh the basic mathematics behind 3D games before continuing
on to learn the applications of these principles to rendering and animation.
On the other hand, university students entering (or hoping to enter) the
3D games industry often ask what material they need to know in order to be
prepared to work on these games. Younger students often ask what courses
they should attend in order to gain the most useful background for a program-
mer in the industry. Recent graduates, on the other hand, often ask how their
computer graphics knowledge best relates to the way games are developed for
today’s computers and game consoles.
We have designed this book to provide something for each of these groups
of readers. We attempt to provide readers with a conceptual understanding
of the mathematics needed to create 3D games, as well as an understanding
of how these mathematical bases actually apply to games and graphics. The
book provides not only theoretical mathematical background, but also many
examples of how these concepts are used to affect how a game looks (how
it is “rendered”) and plays (how objects move and react to users). Each type
of reader is likely to ﬁnd sections of the book that, for them, provide mainly
“refresher courses,” a new understanding of the applications of basic mathe-
matical concepts, or even completely new information. The speciﬁc sections
that fall into each category for a particular reader will, of course, depend on
the reader.
How to Read this Book
As with almost any technical book, how you should read this one depends
on two basic questions, What do you know? and What do you want to learn?
The twelve core chapters of the book are organized into four parts. The
four parts cover core mathematics, rendering, animation, and simulation,
respectively.
Part I, Core Mathematics
The basic mathematics (vectors, linear algebra, afﬁne algebra, and numerical
representations) are covered in Chapters 1 through 4. These chapters form the
mathematical basis for all of the following sections. Some readers will have
a passing familiarity with the topics in this section. However, most readers will
want to start with these chapters, as many of the topics are covered in more
conceptual detail than is often discussed in basic graphics texts. Readers new

How to Read this Book 3
to the material will want to read in detail, while those who already know
some linear algebra can use the chapters to ﬁll in any missing background.
All of these chapters form a basis for the rest of the book, and an under-
standing of these topics, whether existing or new, will be key to successful
3D programming.
Chapter 1 introduces vectors, points, and the operations we apply to
them. Vectors and points are the building blocks of the geometry we will use
to construct, render, and simulate our 3D objects.
Chapter 2 introduces the matrix, a powerful tool we will use to position,
view, and animate objects in our 3D worlds.
Chapter 3 discusses special forms of matrices that deﬁne common ways
of manipulating points and vectors.
Finally, Part I closes with a detailed look at computer number represen-
tations and how they can affect the way we implement 3D games.
Chapter 4 discusses the two common computer representations of the set
of real numbers, ﬁxed-point and ﬂoating-point. It also explains some issues
that can cause either number system to break down and cause incorrect or
inaccurate behavior or degraded performance in 3D applications.
Part II, Rendering
Chapters 5 through 8 explain the so-called rendering pipeline, from the way
we represent visible objects in 3D games to the methods used to draw these
objects to the display. A mixture of concepts, mathematics, and implementa-
tions, these chapters begin to show the direct applicability of the mathematics
introduced in the ﬁrst four chapters to the 3D games we see on the mar-
ket today. While not every 3D programmer will work directly on rendering,
concepts in these chapters, especially Chapters 5 and 6, which specialize
in geometric representations and transformations, are applicable to other
aspects of 3D games.
Chapter 5 applies the concepts of matrices and transformations to the
creation of virtual cameras, to be used to view our 3D worlds.
Chapter 6 examines the details of how we will represent our 3D objects
visually; how we will break them into simple pieces for rendering and how we
will apply colors and images to their surfaces.
Chapter 7 explains how we add realism to our rendering by adding
convincing, dynamic lighting.
Chapter 8 covers the basics of how 3D graphics systems actually draw
geometry to the display.
The chapters in Part II also provide many small code examples and dis-
cussions of how most of these rendering concepts can be implemented via the
OpenGL SDK.

4
Introduction
Part III, Animation
Chapters 9 and 10 build upon the ﬁrst four chapters to introduce the basics
of animation. These chapters detail the methods used to move 3D objects
smoothly over time between sets of desired positions and orientations or key
frames using different methods of interpolation. The beneﬁts and drawbacks
of each interpolation method will be compared along the way.
Chapter 9 introduces the most basic concepts of animation, focusing on
animation of the position of objects. Introducing the concepts of parametric
curves and splines, it will show how to create smooth curves that allow objects
to move in arcs that appear natural and convincing.
Chapter 10 continues with basic animation, this time focusing on animat-
ing the orientation of objects. It will introduce the quaternion, an extremely
powerful object that can represent orientation and its animation in a ﬂexible
and efﬁcient manner.
Part IV, Simulation
Chapters 11 and 12 step beyond prechoreographed animation and describe
how to make objects interact dynamically. A key feature of many games, espe-
cially action, simulation, and sports games, these methods determine when
objects collide and how they should react to one another when they do in
order to behave in a convincing manner.
Chapter 11 surveys the wide range of techniques used to determine when
a set of objects collide, emphasizing those that are fast and can trade off
accuracy and efﬁciency.
Chapter 12 serves as a basic introduction to the simulation of the laws of
physics, allowing games to include realistic motion that is computed on the
ﬂy, rather than pre-determined.
Appendices
In addition to the four major sections of the book, we have included two
appendices. Appendix A, on trigonometry, provides a very brief review of the
basic foundations of trigonometric functions, as well as an annotated listing of
frequently used trigonometric identities. Appendix B, on calculus, provides
a review of topics such as limits, derivatives, and integrals. While neither of
these appendices can teach these topics to a reader who is unfamiliar with
them, they are designed to provide a refresher to readers whose educations
in the topics are many years removed from them.

Interactive Demo Applications 5
Interactive Demo Applications
Demo
Name
Three-dimensional games and graphics are, by their nature, not only visual
but dynamic. While ﬁgures are indeed a welcome necessity in a book about
3D applications, interactive demos can be even more important. It is dif-
ﬁcult to truly understand such topics as lighting, quaternion interpolation,
or physical simulation without being able to see them work ﬁrsthand and
to interact with these complex systems. This book includes a CD-ROM of
source code and demonstrations that are designed to illustrate the concepts
in a way that is analogous to the static ﬁgures in the book itself. Throughout
the book, you will ﬁnd references to interactive demos that may be found
on the CD-ROM. Whenever a topic is illustrated with an interactive demo,
a special icon like the one seen next to this paragraph will appear in the
margin.
Support Libraries
Library
Name
In addition to the source code for each of the demos, the CD-ROM includes
the supporting libraries used to create the demos, with full source code. Often,
code from these supporting libraries is excerpted in the book itself in order
to explain how the particular concept is implemented. In such situations, an
icon will appear in the margin to note where the library code may be found on
the CD-ROM. This source code is designed to allow the reader to modify and
experiment themselves, as a way of better understanding the way the code
works.
The source code is written entirely in C++, a language that is likely to
be familiar to most game developers. C++ was chosen because it is one of
the most commonly used languages in 3D game development and because
vectors, matrices, quaternions, and graphics algorithms decompose very well
into C++ classes. In addition, C++’s support of operator overloading means
that the math library can be implemented in a way that makes the code look
very similar to the mathematical derivations in the text. However, in some
sections of the text, the class declarations as printed in the book are not
complete with respect to the code on the CD-ROM. Often, class members
that are not relevant to the particular discussion (especially member variable
accessor and “housekeeping” functions) have been omitted for clarity. These
other functions may be found in the full class declarations/deﬁnitions on the
CD-ROM.
Note that we have modiﬁed our mathematical notation slightly to allow
our equations to be as compatible as possible with the code. Mathematicians
normally start indexing with 1; for example, P1, P2, . . . , Pn. This does not
match how indexing is done in C++: P[0] is the ﬁrst element in the array P.

6
Introduction
To avoid this disconnect, in our equations we will be using the convention
that the starting element in a list is indexed as 0; thus P0, P1, . . . , Pn−1. This
should allow for a direct translation from equation to code.
Math Libraries
All of the demos use a shared core math library called IvMath, which includes
C++ classes that implement vectors and matrices of different dimensions,
along with a few other basic mathematical objects discussed in the book.
This library is designed to be useful to readers beyond the examples supplied
with the book, as the library includes a wide range of functions and operators
for each of these objects, some of which are beyond the scope of the book’s
demos.
The animation demos use a shared library called IvCurves, which includes
classes that implement spline curves, the basic objects used to animate
position, IvCurves is built upon IvMath, extending this basic functionality
to include animation. As with IvMath, the IvCurves library is likely to be useful
beyond the scope of the book, as these classes are ﬂexible enough to be used
(along with IvMath) in other applications.
Finally, the simulation demos use a shared library called IvCollision,
which implements basic object intersection (collision) data structures and
algorithms. Building on the IvMath library, this set of classes and functions
forms not only the basis for the later demos in the book but also is an excellent
starting point for experimentation with other forms of object collision and
physics modeling.
Engine and Rendering Libraries
In addition to the math libraries, the CD-ROM includes a set of classes that
implement a simple game-like application framework, basic rendering, input
handling, and timer functionality. All of these functions are grouped under
the heading of “game engine” functionality, and are located in the IvEngine
library. The engine’s rendering code takes the form of a set of renderer-
abstraction classes that simplify the interfaces between the C++ classes in
IvMath and the C-based, low-level rendering application programmer inter-
face(s), or API(s). This code is included as a part of the engine library,
IvEngine. It includes renderer setup, basic render-state management, and
rendering of simple geometric primitives, such as spheres, cubes, and boxes.
Furthermore, a set of basic classes that implement a simple scene graph
are included in the library IvScene. The classes in IvScene use and depend
upon the functionality of the IvCollision library. As a result, to avoid

References and Further Reading 7
unnecessary code dependencies, the scene graph classes were placed in their
own library, rather than in IvEngine.
Since this book focuses on the mathematics and concepts behind 3D
games, we chose not to center the discussion around a large-scale, general
3D rendering engine. Doing so would introduce an extra layer of indirection
that would not serve the conceptual requirements of the book. Valuable real
estate in the rendering chapters would be spent on background in the use of
a particular engine — the one written for the book. For an example and dis-
cussion of a full, hierarchical rendering engine, the reader is encouraged to
read Dave Eberly’s 3D Game Engine Design [27].
We have opted to implement our rendering system and examples using
the multiplatform standard SDK, OpenGL [83]. We also use the OpenGL
utility toolkit, GLUT, to implement cross-platform renderer setup and input
handling, neither of which are core topics of this book.
Microsoft’s DirectX [77] is arguably as popular as (or more popular than)
OpenGL for PC game development. However, DirectX was avoided due to its
platform dependence. Most of the mathematical content in this book, includ-
ing the concepts presented in the rendering chapters (Chapters 5 through 8),
are independent of the particular rendering API or high-level graphics engine.
In addition, DirectX is mentioned in numerous places in Part II, Rendering,
generally in places where DirectX provides an interesting contrast to OpenGL.
As mentioned, rendering methods and OpenGL are often not the core
purpose of a given demo. In these cases, we use the renderer-abstraction
code from IvEngine to avoid cluttering the mathematical examples. However,
most of the demos in the rendering section of the book are designed to
show how speciﬁc rendering features are implemented in OpenGL. In these
cases, we use some of OpenGL’s features and functions directly. These demos
include a mixture of IvEngine code and direct OpenGL calls in order to show
some of the more advanced features of OpenGL not needed in the more
mathematically-focused demos.
References and Further Reading
Hopefully, this book will leave readers with a desire to learn even more details
and the breadth of the mathematics involved in creating high-performance,
high-quality 3D games. Wherever possible, we have included references to
other books, articles, papers, and web sites that detail particular subtopics
that fall outside the scope of this book. The full set of references may be
found at the back of the book.
We have attempted to include references that the vast majority of read-
ers should be able to locate. When possible, we have referenced recent
and/or standard industry texts and well-known conference proceedings.

8
Introduction
However, in some cases we have included references to older magazine arti-
cles and technical reports when we found those references to be particularly
complete, seminal, or well-written. In some cases older references can be eas-
ier for the less experienced reader to understand, as they often tend to assume
less “common knowledge” when it comes to computer graphics and game
topics.
In the past, older magazine articles and technical reports were notoriously
difﬁcult for the average reader to locate. However, the Internet and digital
publishing have made great strides toward reversing this trend. For exam-
ple, the following sources have made several classes of resources far more
accessible:
■The magazine most commonly referenced in this book, Game Developer,
offers CD-ROMs that contain every issue of the magazine ever pub-
lished. Copies of these CD-ROMs are available from www.gdmag.com.
Several other technical magazines also offer such CD-ROMs.
■Technical societies are now placing major historical publications into
their “digital libraries,” which are often made accessible to members.
The Association for Computing Machinery (ACM) has done this via
their ACM Digital Library, which is available to ACM members. As
an example, the full text of the entire collection of papers from all
SIGGRAPH conferences (the conference proceedings most frequently
referenced in this book) is available electronically to ACM SIGGRAPH
members.
■Other papers and technical reports are often available on the Internet.
The two most common methods of ﬁnding these resources are via pub-
lication portals such as Citeseer (www.citeseer.com) and via the authors’
personal homepages (if they have them). Most of the technical reports
referenced in this book are available online from such sources. Owing
to the dynamic nature of the Internet, we suggest using a search engine
if the publication portals do not succeed in ﬁnding the desired article.
For further reading, we suggest several books that cover topics related to
this book in much greater detail. In most cases they assume that the reader
is familiar with the concepts discussed in this book. Dave Eberly’s 3D Game
Engine Design [27] discusses the design and implementation of a full game
engine, focusing mostly on graphics and animation. Books by Gino van den
Bergen [109] and Christer Ericson [34] cover topics in interactive collision
detection. Finally, Eberly’s Game Physics [30] provides a more advanced
discussion of a wide range of physical simulation topics.

Part
I
Core Mathematics

Chapter1
Vectors and
Points
1.1 Introduction
The two building blocks of most objects in our interactive digital world are
points and vectors. Points represent locations in space, which can be used
either as measurements on the surface of an object to approximate the object’s
shape (this approximation is called a model), or as simply the position of
a particular object. We can manipulate an object indirectly through its posi-
tion or by modifying its points directly. Vectors, on the other hand, represent
the difference or displacement between two points. Both have some very
simple properties that make them extremely useful throughout computer
graphics and simulation.
In this chapter we’ll discuss the properties and representation of vectors
and points, as well as the relationship between them. We’ll present; how they
can be used to build up other familiar entities from geometry classes; in partic-
ular, lines, planes, and polygons. Because many problems in computer games
boil down to examples in applied algebra, having computer representations
of standard geometric objects built on basic primitives is extremely useful.
It is likely that the reader has a basic understanding of these entities
from basic math classes but the symbolic representations used by the math-
ematician may be unfamiliar or forgotten. We will review them in detail
here. We will also cover linear algebra concepts — properties of vectors in
particular — that are essential for manipulating three-dimensional objects.
Without a thorough understanding of this fundamental material, any work in
programming 3D games and applications will be quite confusing.
11

12
Chapter 1 Vectors and Points
1.2 Vectors
One might expect that we would cover points ﬁrst since they are the building
blocks of our standard model, but in actuality the basic unit of most of the
mathematics we’ll discuss in this book is the vector. We’ll begin by discussing
the vector as a geometric entity since that’s primarily how we’ll be using it
and it’s more intuitive to think of it that way. From there we’ll present a set of
vectors known as a vector space and show how using the properties of vector
spaces allows us to represent geometric vectors in a form that allows us to
manipulate them in the computer. We’ll conclude by discussing operations
that we can perform on vectors and how we can use them to solve certain
problems in 3D programming.
1.2.1 Vectors as Geometry
A geometric vector v is an entity with magnitude (also called length) and direc-
tion and is represented graphically as a line segment with an arrowhead on
one end (Figure 1.1). The length of the segment represents the magnitude of
the vector, and the arrowhead indicates its direction. A vector whose magni-
tude is 1 is a unit or normalized vector and is shown as ˆv. The zero vector 0
has a magnitude of zero but no direction.
Note that a vector does not have a location. To make some geometric
calculations easier to understand we may draw two vectors as if they were
attached or place a vector relative to a location in space. Despite this, it
is important to remember that two vectors with the same magnitude and
direction are equal, no matter where drawn on the page. For example, in
Figure 1.1 the left-most and right-most vectors are equal.
In games we use vectors in one of two ways. The ﬁrst is as a representation
of direction. For example, a vector may indicate direction toward an enemy,
toward a light, or perpendicular to a plane. The second meaning represents
change. If we have an object moving through space, we can assign a velocity
Figure 1.1 Vectors.

1.2 Vectors 13
vector to the object, which represents change in position. We can displace
the object by adding the velocity vector to the object’s location to get a new
location. Vectors can also be used to represent change in other vectors. For
example, we can modify our velocity vector by another over a period of time;
the second vector is called acceleration.
We can perform arithmetic operations on vectors just as we can with real
numbers. One basic operation is addition. Geometrically, addition combines
two vectors together into a new vector. If we think of a vector as an agent
that changes position, then the new vector u = v + w combines the position-
changing effect of v and w into one entity.
As an example, in Figure 1.2 we have three locations P, Q, and R. There
is a vector v that represents the change in position or displacement from P to
Q and a vector w that represents the displacement from Q to R. If we want to
know the vector that represents the displacement from P to R, then we add v
and w to get the resulting vector u.
Figure 1.3 shows another approach, which is to treat the two vectors as the
sides of a parallelogram. Then the sum of the two vectors is the diagonal that
bisects them. Subtraction, or v −w, is shown by the other vector crossing the
parallelogram. Remember that the difference vector is drawn from the second
vector head to the ﬁrst vector head — the opposite of what one might expect.
The algebraic rules for vector addition are very similar to real numbers:
1. v + w = w + v (commutative property)
2. u + (v + w) = (u + v) + w (associative property)
3. v + 0 = v (additive identity)
4. For every v, there is a vector −v such that v+(−v) = 0 (additive inverse)
We can verify this informally by drawing a few test cases. For example, if
we examine Figure 1.3 again, we can see that one path along the parallelogram
Q
R
P
u
w
v
Figure 1.2 Vector addition.

14
Chapter 1 Vectors and Points
w
w
v
v
v–w
v+w
Figure 1.3 Vector addition and subtraction.
u
w
v
u+v+w
v+w
u+v
Figure 1.4 Associative property of vector addition.
represents v + w and the other represents w + v. The resulting vector is the
same in both cases. Figure 1.4 presents the associative property in a similar
fashion.
The other basic operation is scalar multiplication, which changes the
length of a vector by multiplying it by a single real value (Figure 1.5).
Multiplying a vector by 2, for example, makes it twice as long. Multiplying
by a negative value changes the length and points the vector in the opposite
direction (the length remains nonnegative). Multiplying by 0 always produces
the zero vector 0.
The algebraic rules for scalar multiplication should also look familiar:
5. (ab)v = a(bv) (associative property)
6. (a + b)v = av + bv (distributive property)

1.2 Vectors 15
Figure 1.5 Scalar multiplication.
7. a(v + w) = av + aw (distributive property)
8. 1 · v = v (multiplicative identity)
As with the additive rules, diagrams can be created that provide a certain
amount of intuitive understanding.
1.2.2 Real Vector Spaces
In symbolic mathematics and (more important for our purposes) in the com-
puter, representing vectors graphically is not convenient. The linear space or
vector space provides a formal means of encapsulating the concepts that we’ve
just covered and allows us to represent our vectors symbolically. This has
a few advantages. First of all, this symbolic representation provides a means
for storing vectors in the computer. And since it is an abstraction, we can
use it for manipulating higher-dimensional vectors than we might be able to
conceive of geometrically. It also can be used for representing entities that
we wouldn’t normally consider as vectors but that follow the same algebraic
rules, which can be quite powerful. Finally, there are certain properties of
vector spaces that will prove to be quite useful when we cover matrices and
linear transformations.
To simplify our approach, we are going to concentrate on a subset of
vector spaces known as real vector spaces, so called because their fundamental
components are drawn from R, the set of all real numbers. We usually say
that such a vector space V is over R. An element of R in this case is also known
as a scalar. As a brief review, real numbers include 0; Z, the set of all integers;
Q, the set of all rational numbers (fractions); and irrationals, numbers that
can’t be represented by fractions, like π and e.
For the most part we’ll be representing real numbers in the computer using
the ﬂoating point format. It is important to note that ﬂoating point is really
only an approximation. For one thing, it can be used only to represent a ﬁnite
set of all the real numbers. For another, improper ordering of ﬂoating point
operations can lead to serious precision problems that don’t occur with the
inﬁnite precision that real numbers provide. From time to time throughout

16
Chapter 1 Vectors and Points
this chapter and the rest of the book, we will touch on issues that will crop
up when using ﬂoating point; for more details see Chapter 4.
So what is a real vector space? One example of a real vector space is
simply R. At ﬁrst glance it may be difﬁcult to see the correspondence between
a real number and a vector, but as we’ll see next, R does meet the criteria for
a vector space.
Another real vector space is the set of all ordered pairs of real num-
bers, called R2. For now we can think of this as informally representing
two-dimensional space — for example, diagrams on an inﬁnitely extending,
ﬂat page. Symbolically, this is represented by
R2 = {(x, y) | x, y ∈R}
In this context, the symbol | means “such that” and the symbol ∈means “is a
member of.” So we read this as “The set of all possible pairs (x, y), such that
x and y are members of the set of real numbers.” As mentioned, this is a set
of ordered pairs; (1.0, −0.5) is a different member of the set from (−0.5, 1.0).
We deﬁne R3 and R4 similarly as follows:
R3 = {(x, y, z) | x, y, z ∈R}
R4 = {(w, x, y, z) | w, x, y, z ∈R}
Like R2 these are ordered lists, where two members with the same values but
differing orders are not the same. Again informally, we can think of elements
in R3 as representing positions in three-dimensional space, which is where
we will be spending most of our time. Correspondingly, R4 can be thought
of as representing fourth-dimensional space, which is difﬁcult to visualize
spatially 1 (hence our need for an abstract representation) but is extremely
useful for certain computer graphics concepts.
We can extend our deﬁnitions to Rn, a generalized n-dimensional space
over R:
Rn = {(x0, . . . , xn−1) | x0, . . . , xn−1 ∈R}
The members of Rn are referred to as an n-tuple.
Up until now we’ve been casually referring to these real number spaces as
vector spaces. For them to be proper vector spaces and not just organized lists
of numbers, we need to deﬁne two speciﬁc operations on the elements that
follow certain algebraic rules. The two operations should be familiar from our
discussion of geometric vectors: they are addition and scalar multiplication.
1.
Unless you are one of a particularly gifted pair of children [85].

1.2 Vectors 17
We’ll deﬁne these operations so that the vector space V has closure with respect
to them, that is
1. For any u and v in V , u + v is in V (additive closure)
2. For any a in R and v in V , av is in V (multiplicative closure)
So formally, we deﬁne a real vector space as a set V over R with closure
with respect to addition and scalar multiplication on its elements, where the
following properties hold:
For all u, v, w, 0 in V and all a, b in R:
1. v + w = w + v (commutative property)
2. u + (v + w) = (u + v) + w (associative property)
3. There exists an element 0 such that v + 0 = v (additive identity)
4. For every v, there is an element −v such that v + (−v) = 0 (additive
inverse)
5. (ab)v = a(bv) (associative property)
6. (a + b)v = av + bv (distributive property)
7. a(v + w) = av + aw (distributive property)
8. 1 · v = v (multiplicative identity)
These are exactly the properties we stated previously for vector addition and
scalar multiplication.
As an example, we can deﬁne addition in R2 as
(x0, y0) + (x1, y1) = (x0 + x1, y0 + y1)
and scalar multiplication as
a(x0, y0) = (ax0, ay0)
Using these deﬁnitions and the preceding algebraic axioms, it can be shown
that R2 is a vector space. Similar operations can be deﬁned for R3 and R4, as
well as for R itself. Generalized over Rn, we have
u + v = (u0, . . . , un−1) + (v0, . . . , vn−1)
= (u0 + v0, . . . , un−1 + vn−1)

18
Chapter 1 Vectors and Points
and
av = a(v0, . . . , vn−1)
= (av0, . . . , avn−1)
Suppose we have a subset W of a vector space V . We call W a subspace
if it is itself a vector space when using the same deﬁnition for addition and
multiplication operations. In order to show that a given subset W is a vector
space, we only need to show that closure under addition and scalar multipli-
cation holds; the rest of the properties are satisﬁed because W is a subset of
V . For example, the subset of all vectors in R3 with z = 0 is a subspace, since
(x0, y0, 0) + (x1, y1, 0) = (x0 + x1, y0 + y1, 0)
a(x0, y0, 0) = (ax0, ay0, 0)
The resulting vectors still lie in the subspace R3 with z = 0.
Note that any subspace must contain 0 in order to meet the conditions for
a vector space. So the subset of all vectors in R3 with z = 1 is not a subspace
since 0 cannot be represented. And while R2 is not a subspace of R3 (since the
former is a set of pairs and the latter a set of triples), it can be embedded in
a subspace of R3 by a mapping; for example, (x, y) →(x, y, 0).
It is important to understand that — despite the name — a vector space
does not necessarily have to be made up of geometric vectors. What we have
described is a series of sets of ordered lists, possibly with no relation to a geo-
metric construct. As we will see, they can be related to the geometry, but the
term vector, when used in describing members of vector spaces, is an abstract
concept. As long as a set of elements can be shown to have the preceding arith-
metic properties, we deﬁne it as a vector space and any element of a vector
space as a vector. It is perhaps more correct to say that the geometric repre-
sentations of two-dimensional and three-dimensional vectors that we use are
visualizations that help us better understand the abstract nature of R2 and R3,
rather than the other way around.
1.2.3 Linear Combinations and Basis Vectors
Our deﬁnitions of vector addition and scalar multiplication can be used to
describe some special properties of vector spaces. Suppose we have a set S
of n vectors, where S = {v0, . . . , vn−1}. We can combine these to create a new
vector v, using the function
v = a0v0 + a1v1 + · · · + an−1vn−1

1.2 Vectors 19
v1
v0
Figure 1.6 Two vectors spanning a plane.
for some arbitrary real scalars a0, . . . , an−1. This is known as a linear combi-
nation of all vectors vi in S.
If we take all the possible linear combinations of all vectors in S, then the
set T of vectors thus created is the span of S. We can also say that the set S
spans the set T . For example, vectors v0 and v1 in Figure 1.6 span the set of
vectors that lie on the surface of the page (assuming your book is held ﬂat).
We can use linear combinations to deﬁne some properties of our initial
set S. Suppose we can ﬁnd a single vector vi in S such that it’s equal to a linear
combination of other members of S. In other words,
vi = a0v0 + · · · + ai−1vi−1 + ai+1vi+1 + · · · + an−1vn−1
If such a vi exists, then we say that S is linearly dependent. If we can’t ﬁnd any
such vi, then the vectors v0, . . . , vn−1 are linearly independent. An example of a
linearly dependent set of vectors can be seen in Figure 1.7. Vector v0 is equal
to the linear combination −1 · v1 + 0 · v2, or just −v1. Two linearly dependent
vectors v and w are said to be parallel, that is, w = av.
v1
v0
v2
Figure 1.7 Linearly dependent set of vectors.

20
Chapter 1 Vectors and Points
Now suppose that for a given vector space V , we can ﬁnd a set β of n
linearly independent vectors in V that span V . We call that β a basis for V , and
each element of β is called a basis vector. There can be more than one basis for
a given vector space, but they will always have the same number of elements.
We formally deﬁne a vector space’s dimension as equal to the number of basis
vectors required to span it. So, for example, any basis for R3 will contain three
basis vectors, and so it is (as we’d expect) a three-dimensional space. Among
the many bases for a vector space, we deﬁne one as the standard basis. This
standard set of basis vectors is represented as {e0, . . . , en−1}, where
e0 = (1, 0, . . . , 0)
e1 = (0, 1, . . . , 0)
...
en−1 = (0, 0, . . . , 1)
One property of a basis β is that for every vector v in V , there is a unique
linear combination of the vectors in β that equal v. So, using a general basis
β = {b0, b1, . . . , bn−1}, there is only one list of coefﬁcients a0, . . . , an−1 such that
v = a0b0 + a1b1 + · · · + an−1bn−1
Because of this, instead of using the full equation to represent v, we can abbre-
viate it by using only the coefﬁcients a0, . . . , an−1 and store them in an ordered
n-tuple as (a0, . . . , an−1). Note that the coefﬁcient values will be dependent on
which basis we’re using and will almost certainly be different from basis to
basis. The ordering of the basis vectors is important: a different ordering will
not necessarily generate the same coefﬁcients for a given vector. For most
cases, though, we’ll be assuming the standard basis.
Let’s take as an example R3, the vector space we’ll be using most often. In
this case the standard basis is {e0, e1, e2} or, as this basis is usually represented,
{i, j, k}, where i = (1, 0, 0), j = (0, 1, 0), and k = (0, 0, 1). Their corresponding
geometric representations can be seen in Figure1.8. Note that these vectors are
of unit length and perpendicular to each other (we will deﬁne “perpendicular”
more formally when we discuss dot products).
Using this basis, we can uniquely represent any vector v in R3 by using the
formula v = a0i + a1j + a2k. As with the basis vectors, in R3 we usually replace
the general coefﬁcients a0, a1, and a2 with their more common representations
x, y, and z, so
v = xi + yj + zk
We can think of x, y, and z as the amounts we move in the i, j, and k directions,
from the tail of v to its tip (see Figure 1.8). Since the i, j, and k vectors are

1.2 Vectors 21
z
y
x
k
j
i
Figure 1.8 Standard 3D basis vectors.
known and ﬁxed, we just store the x, y, z values and use them to represent our
vector numerically. In this way a three-dimensional vector v is represented
by an ordered triple (x, y, z). We can do the same for R2 by using as our basis
{i, j}, where i = (1, 0) and j = (0, 1), and representing a two-dimensional vector
as the ordered pair (x, y).
By doing this, we have also neatly solved the problem of representing
our geometric vectors algebraically. By using a standard basis, we can use
an ordered triple to represent the same concept as a line segment with an
arrowhead. And by setting a correspondence between our algebraic basis and
our geometric representation, we can guarantee that the ordered triple we
use in one circumstance will be the same as the one we use in the other.
Because of this, when working with vectors in R2 and R3, we will use the two
representations interchangeably.
Using our new knowledge of bases, it is possible to show that our previous
deﬁnitions of addition and scalar multiplication for R3 are valid. For example,
if we add two vectors in R3 v0 and v1 together:
v0 + v1 = x0i + y0j + z0k + x1i + y1j + z1k
= x0i + x1i + y0j + y1j + z0k + z1k
= (x0 + x1)i + (y0 + y1)j + (z0 + z1)k
So, as we expect, to add two vectors we take each component in xyz order and
add them:
(x0, y0, z0) + (x1, y1, z1) = (x0 + x1, y0 + y1, z0 + z1)
(1.1)

22
Chapter 1 Vectors and Points
Scalar multiplication works similarly:
av = a(xi + yj + zk)
= a(xi) + a(yj) + a(zk)
= (ax)i + (ay)j + (az)k
Again, this follows what we deﬁned previously:
a(x, y, z) = (ax, ay, az)
(1.2)
1.2.4 Basic Vector Class Implementation
Library
IvMath
Filename
IvVector3
Now that we’ve justiﬁed our ordered triple representation, we can talk about
how we will store vectors in the computer. As we’ve mentioned many times,
if we know the basis we’re using to span our vector space, all we need to
represent a vector are the coefﬁcients of the linear combination. In our case
we’ll assume the standard basis and thus store the coefﬁcients (or components)
x, y, and z.
The following are some excerpts from the included C++ math library. For
a vector in R3, our bare bones class deﬁnition is
class IvVector3
{
inline IvVector3() {}
inline IvVector3( float _x, float _y, float _z )
{
x = _x;
y = _y;
z = _z;
}
inline ∼IvVector3() {}
IvVector3( const IvVector3& vector );
IvVector3& operator=( const IvVector3& vector );
float x,y,z;
...
}
We can observe a few things about this declaration. First, we declared
our member variables as a type float. This is the single-precision IEEE ﬂoat-
ing point representation for real numbers, which is currently standard for
computer games. It uses a minimum of space with reasonable accuracy and is

1.2 Vectors 23
also hardware-accelerated on most platforms. Double-precision ﬂoating point
uses twice as much space and may use a software implementation, which cor-
respondingly can be much slower. On the other hand, because single precision
is less precise, we have to be more careful about errors in precision. For more
information on ﬂoating point, see Chapter 4.
The second thing to notice is that, like many vector libraries, we’re making
our member variables public. This is not usually recommended practice in
C++; usually, the data is hidden and only made available through an inline
member function. One motivation for such data hiding is to avoid unexpected
side effects when changing a member variable. This is not an issue in the
case of a vector since the data is so simple. However, this breaks another
motivation for data hiding, which is that you can change your underlying
representation without modifying nonlibrary code. This is a downside of what
we are doing here, but one most vector libraries consider worthwhile for ease
of coding. Consider:
v.x = 1.0f;
rather than one of the alternatives:
v.SetX(1.0f);
v.GetX() = 1.0f;
v.X() = 1.0f;
The class has a default constructor and destructor, which do nothing. The
constructor could initialize the components to 0.0f but doing so takes time,
which adds up when we have large arrays of vectors (a common occurrence),
and in most cases we’ll be setting the values to something else anyway. For this
purpose, there is an additional constructor which takes three ﬂoating point
values and uses them to set the components. We can use the copy constructor
and assignment operator as well.
Now that we have the data set up for our class, we can add some operations
to it. The corresponding operator for vector addition is
IvVector3 operator+(const IvVector3& v0, const IvVector3& v1)
{
return IvVector3( v0.x + v1.x, v0.y + v1.y, v0.z + v1.z );
}
Scalar multiplication is also straightforward:
IvVector3
operator*( float a, const IvVector3& vector)

24
Chapter 1 Vectors and Points
{
return IvVector3( a*vector.x, a*vector.y, a*vector.z );
}
Similar operators for post-multiplication and division by a scalar are also
provided within the library; their declarations are
IvVector3 operator*( const IvVector3& vector, float scalar );
IvVector3 operator/( const IvVector3& vector, float scalar );
IvVector3& operator*=( IvVector3& vector, float scalar );
IvVector3& operator/=( IvVector3& vector, float scalar );
Some vector libraries use an alternative technique for creating vector
classes known as template metaprogramming. This approach uses templates
to trick the compiler into producing better optimized code. For example,
rather than deﬁne an operator+, this technique creates a general template class
called Sum, which can perform component-wise operations on our vectors.
When we want to add a series of vectors, it creates the appropriate class, which
performs the operation and converts it back to an IvVector3. A series of opera-
tions ends up with a nested set of templatized classes, which — assuming our
compiler is any good — ends up as an optimized series of component-wise
calculations.
We have decided not to use this for two reasons. First, and mainly, the
implementation details tend to be less clear to those unfamiliar with vectors.
What was once a simple operator+() becomes spread across classes. Second,
the purpose of the technique is to minimize the number of operations when
computing a complex equation such as
IvVector3 v1, v2, v3, v4;
v1 = 2.0f*v2 + 0.5f*v3 + v4;
In most cases we won’t see equations this complex. For those who are inter-
ested, Blinn provides more details on template meta-programming for vector
libraries in his collection Notation, Notation, Notation [13].
If you do need highly optimized code (in a tight loop, for instance), often
it can be better to expand out the terms, which may simplify the equation, or
write the assembly yourself. Many modern processors have a platform-speciﬁc
SIMD instruction set for vectors — for example, SSE on Pentium and 3DNow!
on AMD processors — which can perform several ﬂoating point operations in
parallel. For clarity of code and because ours is a cross-platform library, we
have chosen not to implement this, but for a platform-speciﬁc application this

1.2 Vectors 25
can be a signiﬁcant optimization. More information on SSE and 3DNow! can
be found in Chapter 4.
Now that we have a numeric representation for vectors and have covered
the algebraic form of addition and scaling, we can add some new vector oper-
ations as well. As before, we’ll focus primarily on the case of R3. Vectors in
R2 and R4 have similar properties; any exceptions will be discussed in the
particular parts.
1.2.5 Vector Length
We have mentioned that a vector is an entity with length or direction but so
far haven’t provided any means of measuring or comparing these quantities in
two vectors. We’ll see shortly how the dot product provides a way to compare
vector directions. First, however, we’ll consider how to measure a vector’s
magnitude.
There is a general class of size-measuring functions known as norms.
A norm ∥v∥is deﬁned as a real-valued function on a vector v with the following
properties:
1. ∥v∥≥0, and ∥v∥= 0 if and only if v = 0
2. ∥av∥= |a|∥v∥
3. ∥v + w∥≤∥v∥+ ∥w∥
We use the ∥v∥notation to distinguish a norm from the absolute value
function |a|.
An example of a norm is the Manhattan distance, also called the ℓ1 norm,
which is just the sum of the absolute values of the given vector’s components:
∥v∥ℓ1 =

i
|vi|
One that we’ll use more often is the Euclidean norm, also known as the ℓ2
norm or just length. If we give no indication of which type of norm we’re
using, this is usually what we mean.
We derive the Euclidean norm as follows. Suppose we have a two-
dimensional vector u = xi + yj. Recall the Pythagorean theorem x2 + y2 = d2.
Since x is the distance along i and y is the distance along j, then the length d
of u is
∥u∥= d =

x2 + y2

26
Chapter 1 Vectors and Points
j
i
y
x2+y2
x
Figure 1.9 Length of 2D vector.
as shown in Figure 1.9. A similar formula is used for a vector v = (x, y, z),
using the standard basis in R3:
∥v∥=

x2 + y2 + z2
(1.3)
And the general form in Rn with respect to the standard basis is
∥v∥=

v2
0 + v2
1 + · · · + v2
n−1
We’ve mentioned the use of unit length vectors as pure indicators of direc-
tion; for example, in determining viewing direction or relative location of
a light source. Often, though, the process we’ll use to generate our direction
vector will not automatically create one of unit length. To create a unit vec-
tor ˆv from a general vector v, we normalize v by multiplying it by 1 over its
length, or
ˆv =
v
∥v∥
This sets the length of the vector to ∥v∥· 1/∥v∥or, as we desire, 1.
Our implementations of length methods (for R3) are as follows:
float
IvVector3::Length() const
{
return ::IvSqrt( x*x + y*y + z*z );
}
float
IvVector3::LengthSquared() const

1.2 Vectors 27
{
return x*x + y*y + z*z;
}
IvVector3&
IvVector3::Normalize()
{
float lengthsq = x*x + y*y + z*z;
ASSERT( !::IsZero( lengthsq ) );
if ( ::IsZero( lengthsq ) )
{
x = y = z = 0.0f;
return *this;
}
float recip = ::IvInvSqrt( lengthsq );
x *= recip;
y *= recip;
z *= recip;
return *this;
}
Note that in addition to the mathematical operations we’ve just described,
we have deﬁned a LengthSquared() method. Performing the square root can be
a costly operation, even on systems that have a special machine instruction
to compute it. Often we’re only doing a comparison between lengths, so it
is better and certainly faster in those cases to compute and compare length
squared instead. Both length and length squared are increasing functions
starting at 0, so the results will be the same.
The LengthSquared() method also introduces some new functions which
will be useful to us throughout the math library. The function ::IsZero()
is a precision-safe means of testing for near-zero values. We assume that if
a ﬂoating point number is close enough to zero, it is considered essentially
zero. It is much better to use that than to do a direct comparison with 0.0f
because of the inherent precision problems with ﬂoating point.
We also use our own square root functions ::IvSqrt() and ::IvInvSqrt()
instead of sqrtf(). There are a number of reasons for this choice. As men-
tioned, the standard library implementation of square root is often slow.
Rather than use it, we can use an approximation on some platforms, which
is faster and accurate enough for our purpose. On other platforms there are
internal assembly instructions that are not used by the standard library. In
particular, there may be an instruction that performs the inverse square root,
which is faster than calculating the square root and performing the ﬂoating

28
Chapter 1 Vectors and Points
point divide. Deﬁning our own layer of indirection gives us ﬂexibility and
ensures that we can guarantee ourselves the best performance.
1.2.6 Dot Product
Now that we’ve considered vector length, we can look at vector direction.
We begin by considering a set of functions known as inner products. An inner
product is a concept, like a vector space, that is used to abstract away physical
notions of geometry while maintaining similar properties.
For all v, w in a real vector space V , we deﬁne an inner product ⟨v, w⟩as a
function returning a real scalar, with the following properties:
1. ⟨v, w⟩= ⟨w, v⟩(symmetry)
2. ⟨u + v, w⟩= ⟨u, w⟩+ ⟨v, w⟩(additivity)
3. a⟨v, w⟩= ⟨av, w⟩(homogeneity)2
4. ⟨v, v⟩≥0 (positivity)
5. ⟨v, v⟩= 0 if and only if v = 0 (deﬁniteness)
A real vector space together with such a function is called an inner product
space.
There is a particular inner product that can be tied to the physical world
in ways that will prove to be very useful to us. It is called the Euclidean inner
product, or more commonly, the dot product. It is probably the most useful
vector operation for 3D games and applications. Instead of using the ⟨·, ·⟩
form, the dot product of two vectors v and w is represented by v · w. However,
since it is an inner product, it still follows the same algebraic rules.
Given two vectors v and w with an angle θ between them, the dot product
is deﬁned as
v · w = ∥v∥∥w∥cos θ
(1.4)
Using this equation, we can ﬁnd a coordinate-dependent deﬁnition in R3 by
examining a triangle formed by v, w, and v −w (Figure 1.10). The Law of
Cosines3 gives us
∥v −w∥2 = ∥v∥2 + ∥w∥2 −2∥v∥∥w∥cos θ
2.
Note that the leading scalar does not apply to both terms on the right-hand side; assuming
so is a common mistake.
3.
See Appendix A.

1.2 Vectors 29
w
v
v–w
θ
Figure 1.10 Law of cosines.
We can rewrite this as
−2∥v∥∥w∥cos θ = ∥v −w∥2 −∥v∥2 −∥w∥2
Substituting in the deﬁnition of vector length in R3 and expanding, we get
−2∥v∥∥w∥cos θ = (vx −wx)2 + (vy −wy)2 + (vz −wz)2
−(v2
x + v2
y + v2
z) −(w2
x + w2
y + w2
z)
−2∥v∥∥w∥cos θ = −2vxwx −2vywy −2vzwz
∥v∥∥w∥cos θ = vxwx + vywy + vzwz
So, to compute the dot product in R3, multiply the vectors component-
wise, and then add:
v · w = vxwx + vywy + vzwz
Note that for this deﬁnition to hold, vectors v and w need to be represented
with respect to the standard basis {i, j, k}. The general form for vectors v and
w in Rn, again with respect to the standard basis, is
v · w = v0w0 + v1w1 + · · · + vn−1wn−1
We can relate the dot product to the length function by noting that
v · v = ∥v∥2
(1.5)
Whereas we began by deﬁning the length and then the dot product, in more
abstract inner product spaces we usually deﬁne the norm based on the inner
product. This can be done by rewriting the equation as
∥v∥=

⟨v, v⟩
Of the two, equation 1.5 will be more useful to us.

30
Chapter 1 Vectors and Points
As mentioned, the dot product has many uses. By equation 1.4, if the
angle between two vectors v and w in standard Euclidean space is 90 degrees,
then v · w = 0. So we deﬁne that two vectors v and w are perpendicular, or
orthogonal, when v · w = 0. Recall that we stated that our standard basis
vectors for R3 are orthogonal. We can now demonstrate this. For example,
taking i · j we get
i · j = (1, 0, 0) · (0, 1, 0)
= 0 + 0 + 0
= 0
It is possible, although not always recommended, to use equation 1.4
to test whether two unit vectors ˆv and ˆw are pointing generally in the same
direction. If they are, cos θ is close to 1, so 1 −ˆv · ˆw is close to 0 (we use this
formula to avoid problems with ﬂoating point precision). Similarly, if 1+ ˆv · ˆw
is close to 0, they are pointing in opposite directions. Performing this test
only takes 6 ﬂoating point addition and multiplication operations. However,
if v and w are not known to be normalized, then we need a different test:
∥v∥2∥w∥2 −(v · w)2. This takes 18 operations.
Note that for unit vectors:
1 −(ˆv · ˆw)2 = 1 −cos2 θ
= sin2 θ
and for non-unit vectors:
∥v∥2∥w∥2 −(v · w)2 = ∥v∥2∥w∥2(1 −cos2 θ)
= ∥v∥2∥w∥2 sin2 θ
So assuming we use this, the method we use to test closeness to zero will have
to be different for both cases.
In any case, using dot product for this test is not really recommended
unless your vectors are pre-normalized and speed is of the essence. As cos θ
gets close to 0, it changes very little. Due to lack of ﬂoating point precision,
the set of angles that might be considered “zero” is actually broader than one
might expect. As we will see, there is another method to test for parallel vectors
that is faster with non-unit vectors and has fewer problems with near-zero
angles.
A more common use of the dot product is to test the angle between two
vectors. If v · w > 0, then we know the angle is less than 90 degrees. If v · w < 0,
then we know that the angle is greater than 90 degrees, and if v · w = 0 then

1.2 Vectors 31
the angle is exactly 90 degrees (Figure 1.11). As opposed to testing for parallel
vectors, this will work with vectors of any length.
For example, suppose that we have an AI agent that is looking for enemy
agents in the game. The AI has a view vector v and a vector t which points
toward an object in our scene. If v · t < 0, then the object is behind us and
therefore not visible to our AI (Figure 1.12).
w2•v<0
v
w1•v=0
w0•v>0
Figure 1.11 Dot product as measurement of angle.
t
v
O
E
Figure 1.12 Measuring angle to target.

32
Chapter 1 Vectors and Points
w
v
h
vcos(h )/ w 
Figure 1.13 Dot product as projection.
Equation 1.4 allows us to use the dot product in another manner. Suppose
we have two vectors v and w, where w ̸= 0. We deﬁne the projection of
v onto w as
projwv = v · w
∥w∥2 w
This gives the part of v which is parallel to w, which is the same as dropping
a perpendicular from the end of v onto w (Figure 1.13).
We can get the part of v which is perpendicular to w by subtracting the
projection:
perpwv = v −v · w
∥w∥2 w
Both of these equations will be very useful to us. Note that if w is normalized,
then the projection simpliﬁes to
proj ˆwv = (v · ˆw) ˆw
The corresponding library implementation of dot product in R3 is as follows:
float
IvVector3::Dot( const IvVector3& other )
{
return x*other.x + y*other.y + z*other.z;
}

1.2 Vectors 33
1.2.7 Gram-Schmidt Orthogonalization
The combination of dot product and normalization allows us to deﬁne a par-
ticularly useful class of basis vectors. If a set of basis vectors β are all unit
vectors and pairwise orthogonal, we call them an orthonormal basis. Our
standard basis {i, j, k} is an example of an orthonormal basis.
In many cases we start with a general basis and want to generate the clos-
est possible orthonormal basis. One example of this is when we perform opera-
tions on the set of vectors that make up an orthonormal basis. Even if the pure
mathematical result should not change their length or relative orientation, due
to ﬂoating point precision problems the resulting vectors may no longer be
orthonormal. The process that allows us to create an orthonormal basis from
a possibly non-orthonormal basis is called Gram-Schmidt Orthogonalization.
This works as follows. Suppose we have a set of non-orthogonal basis
vectors v0, . . . , vn−1, and from them we want to create an orthonormal basis
w0, . . . , wn−1. We’ll use the ﬁrst vector from our original basis as the starting
vector for our new basis so
w0 = v0
Now we want to create a vector orthogonal to w0, which points generally in
the direction of v1. We can do this by computing the projection of v1 on w0,
which produces the component vector of v1 parallel to w0. The remainder of
v1 will be orthogonal to w0, so
w1 = v1 −projw0v1
= v1 −v1 · w0
∥w0∥w0
We perform the same process for w2: we project v2 on w0 and w1 to compute
the parallel components and then subtract those from v2 to generate a vector
orthogonal to both w0 and w1:
w2 = v2 −projw0v2 −projw1v2
= v2 −v2 · w0
∥w0∥w0 −v2 · w1
∥w1∥w1
In general we have
wi = vi −
i−1

j=0
projwj vi
= vi −
i−1

j=0
vi · wj
∥wj∥wj

34
Chapter 1 Vectors and Points
Performing this for all n basis vectors will give us an orthogonal basis. To
create an orthonormal basis, we can either normalize the resulting wj vectors
at the end or normalize as we go, the latter of which simpliﬁes the projection
calculation to (vi · wj) wj.
1.2.8 Cross Product
Suppose we have two vectors v and w and want to ﬁnd a new vector u orthogo-
nal to both. The operation that computes this is the cross product, also known
as the vector product. There are two possible choices for the direction of the
vector, each the negation of the other (Figure 1.14); the one chosen is deter-
mined by the right-hand rule. Hold your right hand so that your foreﬁnger
points forward, your middle ﬁnger points out to the left, and your thumb
points up. If you roughly align your foreﬁnger with v, and your middle ﬁn-
ger with w, then the cross product will point in the direction of your thumb
(Figure 1.15). The length of the cross product is equal to the area of a par-
allelogram bordered by the two vectors (Figure 1.16). This can be computed
using the formula
∥v × w∥= ∥v∥∥w∥sin θ
(1.6)
where θ is the angle between v and w. Note that the cross product is not
commutative, so order is important:
v × w = −(w × v)
Also, if the two vectors are parallel, sin θ = 0, so we end up with the zero
vector.
w
v
Figure 1.14 Two directions of orthogonal 3D vectors.

1.2 Vectors 35
w
v
v  w
Figure 1.15 Cross product direction.
w
v
v × w
Figure 1.16 Cross product length equals area of parallelogram.
It is a common mistake to believe that if v and w are unit vectors, the cross
product will also be a unit vector. A quick look at equation 1.6 shows this is
true only if sin θ is 1, in which case θ is 90 degrees.
The formula for the cross product is
v × w = (vywz −wyvz, vzwx −wzvx, vxwy −wxvy)
Certain processors can implement this as a two-step operation, by creating
two vectors and performing the subtraction in parallel:
v × w = (vywz, vzwx, vxwy) −(wyvz, wzvx, wxvy)
For vectors u,v, w, and scalar a the following algebraic rules apply:
1. v × w = −w × v
2. u × (v + w) = (u × v) + (u × w)

36
Chapter 1 Vectors and Points
3. (u + v) × w = (u × w) + (v × w)
4. a(v × w) = (av) × w = v × (aw)
5. v × 0 = 0 × v = 0
6. v × v = 0
There are two common uses for the cross product. The ﬁrst, and most
used, is to generate a vector orthogonal to two others. Suppose we have three
points P, Q, and R, and we want to generate a unit vector n that is orthogonal
to the plane formed by the three points (this is known as a normal vector).
Begin by computing v = (Q −P), and w = (R −P). Now we have a decision
to make. Computing v × w and normalizing will generate a normal in one
direction, whereas w × v and normalizing will generate one in the opposite
direction (Figure 1.17). Usually we’ll set things up so that the normal points
from the inside toward the outside of our object.
Like the dot product, the cross product can also be used to determine
if two vectors are parallel, by checking whether the resulting vector is close
to the zero vector. Deciding whether to use this test as opposed to the dot
product depends on what your data is. The cross product takes 9 operations.
We can test for zero by examining the dot product of the result with itself
((v × w) · (v × w)). If it is close to 0, then we know the vectors are nearly
parallel. The dot product takes an additional 5 operations, or 14 total for
our test. Recall that testing for parallel vectors using the dot product of non-
normalized vectors takes 18 operations; in this case the cross product test is
faster.
The cross product of two vectors is deﬁned only for vectors in R3. However,
in R2 we can deﬁne a similar operation on a single vector v, called the
perpendicular. This is represented as v⊥. The result of the perpendicular is
the vector rotated 90 degrees. As with the cross product, we have two choices:
in this case counterclockwise or clockwise rotation. The standard deﬁnition
is to rotate counterclockwise (Figure 1.18), so if v = (x, y), v⊥= (−y, x).
w
v
w × v
v × w
Q
R
P
Figure 1.17 Computing normal for triangle.

1.2 Vectors 37
v
v
Figure 1.18 Perpendicular vector.
The perpendicular has similar properties to the cross product. First,
it produces a vector orthogonal to the original. Also, when used in com-
bination with the dot product in R2 (also known as the perpendicular dot
product):
v⊥· w = ∥v∥∥w∥sin θ
where θ is the signed angle between v and w. That is, if the shortest rotation
to get from v to w is in a clockwise direction, then θ is negative. And similar
to the cross product, the absolute value of the perpendicular dot product is
equal to the area of a parallelogram bordered by the two vectors.
It is possible to take cross products in dimensions greater than 3 by using
n−1 vectors to take an n-dimensional cross product, but in general they won’t
be useful to us.
Our IvVector3 cross product method is
IvVector3
IvVector3::Cross( const IvVector3& other )
{
return IvVector3( y*other.z - other.y*z,
z*other.x - other.z*x,
x*other.y - other.x*y );
}
1.2.9 Triple Products
In R3 there are two extensions of the two single operation products called
triple products. The ﬁrst is the vector triple product, which returns a vector
and is computed as u × (v × w).
A special case is w×(v×w) (Figure 1.19). Examining this, v×w is perpen-
dicular both to v and w. The result of w × (v × w) is a vector perpendicular to

38
Chapter 1 Vectors and Points
v × w
v
w × (v × w)
w
Figure 1.19 Using the vector triple product.
both w and (v×w). Therefore, if we combine normalized versions of w, (v×w)
and w × (v × w), we have an orthonormal basis (all are perpendicular and of
unit length).
The second triple product is called the scalar triple product. It (naturally)
returns a scalar, and its formula is u · (v × w). To understand this geometri-
cally, suppose we treat these three vectors as the edges of a slanted box, or
parallelopiped (Figure 1.20). Then the area of the base equals ∥v × w∥, and
∥u∥cos θ gives the height of the box. So
u · (v × w) = ∥u∥∥v × w∥cos θ
or area times height equals volume of the box.
In addition to computing volume, the scalar triple product can be used
to test the direction of the angle between two vectors v and w, relative to
v × w
v
w
u
Figure 1.20 Scalar triple product equals volume of parallelopiped.

1.2 Vectors 39
a third vector u that is linearly independent to both. If u · (v × w) > 0, then
the shortest rotation from v to w is in a counterclockwise direction (assum-
ing our basis vectors are right-handed as we will discuss shortly) around u.
Similarly, if u · (v × w) < 0, the shortest rotation is in a relative clockwise
direction.
For example, suppose we have a tank with current velocity v and desired
direction d of travel. Our tank is oriented so that its current up direction points
along a vector u. We take the cross product v×d and dot it with u. If the result
is positive, then we know that d lies to the left of v (counterclockwise rotation)
and we turn left. Similarly, if the value is less than zero, then we know we
must turn right to match d (Figures 1.21 and 1.22).
If we know that the tank is always oriented so that it lies on the xy-plane,
we can simplify this considerably. Vectors v and d will always have z values
v × d
v
d
u
Figure 1.21 Scalar triple product indicates left turn.
v × d
v
d
u
Figure 1.22 Scalar triple product indicates right turn.

40
Chapter 1 Vectors and Points
i
k
j
Figure 1.23 Right-handed rotation.
of 0, and u will always point in the same direction as the standard basis vec-
tor k. In this case the result of u · (v × d) is equal to the z value of v × d. So
the problem simpliﬁes to taking the cross product of v and d and checking the
sign of the resulting z value to determine our turn direction.
Finally, we can use the scalar triple product to test whether our ordered
basis vectors in R3 are left-handed or right-handed. We can test this informally
for our standard basis by using the right-hand rule. Take your right hand
and point the thumb along k and your ﬁngers along i. Now, rotating around
your thumb, sweep your ﬁngers counterclockwise into j (Figure1.23). This
90 degree rotation of i into j shows that the basis is right-handed. We can do
the same trick with the left hand rotating clockwise to show that a basis is
left-handed.
Formally, if we have three basis vectors {v0, v1, v2}, then they are right-
handed if v0 · (v1×v2) > 0, and left-handed if v0 · (v1×v2) < 0. If v0 · (v1×v2) = 0,
we’ve got a problem — our vectors are linearly dependent and thus are not
a basis.
While the scalar triple product only applies to vectors in R3, we can use
the perpendicular dot product to test vectors in R2 for both turning direction
and right or left handedness. For example, if we have two basis vectors {v0, v1}
in R2, then they are right-handed if v⊥
0 · v1 > 0, and left-handed if v⊥
0 · v1 < 0.
For vectors u, v, and w in R3 the following algebraic rules regarding the
triple products apply:
1. u × (v × w) = (u · w)v −(u · v)w
2. (u × v) × w = (u · w)v −(v · w)u
3. u · (v × w) = w · (u × v) = v · (w × u)

1.3 Points 41
1.3 Points
Now that we have covered vectors and vector operations in some detail, we
turn our attention to a related entity, the point. While the reader probably has
some intuitive notion of what a point is, in this part we’ll provide a mathemat-
ical representation and discuss the relationship between vectors and points.
We’ll also discuss some special operations that can be performed on points
and alternatives to the standard Cartesian coordinate system.
Within this part it is also assumed that the reader has some general sense
of what lines and planes are. More information on these topics follows in
subsequent parts.
1.3.1 Points as Geometry
Everyone who has been through a ﬁrst-year geometry course should be
familiar with the notion of a point. Euclid describes the point in his work
Elements [35] as “that which has no part.” They have also been presented as the
cross-section of a line, or the intersection of two lines. A less vague but still not
satisfactory deﬁnition is to describe them as an inﬁnitely small entity which
has only the property of location. In games we use points for two primary pur-
poses: to represent the position of game objects and as the basic building block
of their geometric representation. Points are represented graphically by a dot.
Euclid did not present a means for representing position numerically,
although later Greek mathematicians used latitude, longitude, and altitude.
The primary system we use now — Cartesian coordinates — was originally
published by Rene Descartes in his 1637 work La geometrie [26] and further
revised by Newton and Leibniz.
In this system we measure a point’s location relative to a special, anchored
point, called the origin, which is represented by the letter O. In R2 we infor-
mally deﬁne two perpendicular real number lines or axes — known as the
x- and y-axes — which pass through the origin. We indicate the location of
a point P by a pair (x, y) in R2, where x is the distance from the point to
the y-axis, and y is the distance from the point to the x-axis. Another way
to think of it is that we count x units along the x-axis and then y units up
parallel to the y-axis to reach the point’s location. This combination of origin
and axes is called the Cartesian coordinate system (Figure 1.24).
For R3 three perpendicular coordinate axes — x, y, and z — intersect at
the origin. There are corresponding coordinate planes xy, yz, and xz that
also intersect at the origin. Take the room you’re sitting in as our space,
with one corner of the room as the origin, and think of the walls and ﬂoor
as the three coordinate planes (assume they extend inﬁnitely). The edges
where the walls and ﬂoor join together correspond to the axes. We can
think of a three-dimensional position as being a real number triple (x, y, z)

42
Chapter 1 Vectors and Points
y-axis
x-axis
x
y
O
P
Figure 1.24 2D Cartesian coordinate system.
corresponding to the distance of the point to the three planes, or counting
along each axis as before.
In Figure 1.25 you can see an example of a three-dimensional coordinate
system. Here the axis pointing up is called the z-axis, the one to the side is
z-axis
y-axis
x-axis
O
Figure 1.25 3D Cartesian coordinate system.

1.3 Points 43
y-axis
x-axis
z-axis
O
Figure 1.26 Alternate 3D Cartesian coordinate system.
the y-axis, and the one aimed slightly out of the page is the x-axis. Another
system that is commonly used in graphics books has the y-axis pointing up,
the x-axis to the right, and the z-axis out of the page (Figure 1.26). Some
graphics developers favor this because the x- and y-axis match the relative
axes of the two-dimensional screen, but most of the time we’ll be using the
former convention for this book.
Both of the three-dimensional coordinate systems we have described are
right-handed. As before, we can test this via the right-hand rule. This time
point your thumb along the z-axis, your ﬁngers along the x-axis, and rotate
counterclockwise into the y-axis. As with left-handed bases, we can have left-
handed coordinate systems (and will be using them later in this book), but
the majority of our work will be done in a right-handed coordinate system
because of convention.
1.3.2 Affine Spaces
We can provide a more formal deﬁnition of coordinate systems based on what
we already know of vectors and vector spaces. Before we can do so, though,
we need to deﬁne the relationship between vectors and points. Points can be

44
Chapter 1 Vectors and Points
related to vectors by means of an afﬁne space. An afﬁne space consists of a set
of points W and a vector space V . The relation between the points and vectors
is deﬁned using the following two operations:
For every pair of points P and Q in W, there is a unique vector v in V
such that
v = Q −P
Correspondingly, for every point P in W and every vector v in V , there is
a unique point Q such that
Q = P + v
(1.7)
This relationship can be seen in Figure 1.27. We can think of the vector v as
acting as a displacement between the two points P and Q. To determine the
displacement between two points, we subtract one from another. To displace
a point, we add a vector to it and that gives us a new point.
We can deﬁne a ﬁxed point O in W, known as the origin. Then using
equation 1.7, we can represent any point P in W as
P = O + v
or, expanding our vector using n basis vectors that span V :
P = O + a0v0 + a1v1 + · · · + an−1vn−1
(1.8)
Using this, we can represent our point using an n-tuple (a0, . . . , an−1) just as
we do for vectors. The combination of the origin O and our basis vectors
(v0, . . . , vn−1) is known as a coordinate frame.
Note that we can use any point in W as our origin and — for an
n-dimensional afﬁne space — any n linearly independent vectors as our
basis. Unlike the Cartesian axes, this basis does not have to be orthonormal,
but using an orthonormal basis (as with vectors) does make matching our
P
Q
v
Figure 1.27 Afﬁne relationship between points and vectors.

1.3 Points 45
O
z
k
i
j
y
x
Figure 1.28 Relationship between points and vectors in Cartesian afﬁne frame.
physical geometry with our abstract representation more straightforward.
Because of this, we will work with the standard origin (0, 0, . . . , 0), and the
standard basis {(1, 0, . . . , 0), (0, 1, . . . , 0), . . . , (0, 0, . . . , 1)}. This is known as the
Cartesian frame.
In R3 our Cartesian frame will be the origin (0, 0, 0) and the standard
ordered basis {i, j, k} as before. Our basis vectors will lie along the x-, y-, and
z-axes, respectively. By using this system, we can use the same triple (x, y, z)
to represent a point and the corresponding vector from the origin to the point
(Figure 1.28).
To compute the distance between two points we use the length of the
vector that is their difference. So if we have two points P0 = (x0, y0, z0) and
P1 = (x1, y1, z1) in R3, the difference is
v = P1 −P0 = (x1 −x0, y1 −y0, z1 −z0)
and the distance between them is
dist(P1, P0) = ∥v∥=

(x1 −x0)2 + (y1 −y0)2 + (z1 −z0)2
This is also known as the Euclidean distance. In the R3 Cartesian frame, the
distance between a point P = (x, y, z) and the origin is
dist(P, 0) =

x2 + y2 + z2

46
Chapter 1 Vectors and Points
1.3.3 Affine Combinations
So far the only operation that we’ve deﬁned on points alone is subtraction,
which results in a vector. However, there is a limited addition operation that
we can perform on points that gives us a point as a result. It is known as an
afﬁne combination, and has the form
P = a0P0 + a1P1 + · · · + akPk
(1.9)
where
a0 + a1 + · · · + ak = 1
(1.10)
So an afﬁne combination of points is like a linear combination of vectors,
with the added restriction that all the coefﬁcients need to add up to 1. We can
show why this restriction allows us to perform this operation by rewriting
equation 1.10 as
a0 = 1 −a1 −· · · −ak
and substituting into equation 1.9 to get
P = (1 −a1 −· · · −ak)P0 + a1P1 + · · · + akPk
= P0 + a1(P1 −P0) + · · · + ak(Pk −P0)
(1.11)
If we set u1 = (P1 −P0), u2 = (P2 −P0), and so on, we can rewrite this as
P = P0 + a1u1 + a2u2 + · · · + akuk
So by restricting our coefﬁcients in this manner, it allows us to rewrite the
afﬁne combination as a point plus a linear combination of vectors, a perfectly
legal operation.
Looking back at our coordinate frame equation 1.8, we can see that it too is
an afﬁne combination. Just as we use the coefﬁcients in a linear combination
of basis vectors to represent a general vector, we can use the coefﬁcients of an
afﬁne combination of origin and basis vectors to represent a general point.
An afﬁne combination spans an afﬁne space, just as a linear combination
spans a vector space. If the vectors in equation 1.11 are linearly independent,
we can represent any point in the spanned afﬁne space using the coefﬁcients
of the afﬁne combination, just as we did before with vectors. In this case
we say that the points P0, P1, . . . , Pk are afﬁnely independent, and the ordered
points are called a simplex. The coefﬁcients are called barycentric coordinates.
For example, we can create an afﬁne combination of a simplex made of three

1.3 Points 47
Figure 1.29 Convex versus non-convex set of points.
afﬁnely independent points P0, P1, and P2. The afﬁne space spanned by the
afﬁne combination a0P0 + a1P1 + a2P2 is a plane, and any point in the plane
can be speciﬁed by the coordinates (a0, a1, a2).
We can further restrict the set of points spanned by the afﬁne combination
by considering properties of convex sets. A convex set of points is deﬁned such
that a line drawn between any pair of points in the set remains within the set
(Figure 1.29). The convex hull of a set of points is the smallest convex set
that includes all the points. If we restrict our coefﬁcients (a0, . . . , an−1) such
that 0 ≤a0, . . . , an−1 ≤1, then we have a convex combination, and the span
of the convex combination is the convex hull of the points. For example, the
convex combination of three afﬁnely independent points spans a triangle. We
will discuss the usefulness of this in more detail when we cover triangles in
Part 1.6.
If the barycentric coordinates in a convex combination of n points are all
1/n, then the point produced is called the centroid, which is the mean of a set
of points.
1.3.4 Point Implementation
Library
IvMath
Filename
IvVector3
Using the Cartesian frame and standard basis in R3, the x, y, z values of a
point P in R3 match the x, y, z values of the corresponding vector P −O,
where O is the origin of the frame. This also means that we can use one class
to represent both, since one can be easily converted to the other. Because of
this, many math libraries don’t even bother implementing a point class and
just treat points as vectors.
Other libraries indicate the difference by treating them both as 4-tuples
and indicate a point as (x, y, z, 1) and a vector as (x, y, z, 0). In this system if
we subtract a point from a point, we automatically get a vector:
(x0, y0, z0, 1) −(x1, y1, z1, 1) = (x0 −x1, y0 −y1, z0 −z1, 0)

48
Chapter 1 Vectors and Points
Similarly, a point plus a vector produces a point:
(x0, y0, z0, 1) + (x1, y1, z1, 0) = (x0 + x1, y0 + y1, z0 + z1, 1)
Even afﬁne combinations give the expected results:
n

i=1
ai(xi, yi, zi, 1) =
 n

i=1
aixi,
n

i=1
aiyi,
n

i=1
aizi,
n

i=1
ai

=
 n

i=1
aixi,
n

i=1
aiyi,
n

i=1
aizi, 1

OpenGL uses this form when specifying the difference between a point
light, which casts light rays in all directions from a given position, and a direc-
tional light, which only casts light rays in one direction. Both are speciﬁed by
a single call:
GLfloat light_position[] = {1.0, 1.0, 1.0, 0.0};
glLightfv(GL_LIGHT0, GL_POSITION, light_position);
If the ﬁnal value of light_position is 0, then it is treated as a directional light;
otherwise, it is treated as a point light.
In our case we will not be using a separate class for points. There
would be a certain amount of code duplication, since the IvPoint3 class
would end up being very similar to the IvVector3 class. Also to be con-
sidered is the performance cost of converting points to vectors and back
again. Further, to maintain type correctness we may end up distorting
equations unnecessarily; this obfuscates the code and can lead to a loss in
performance as well. Finally, most production game engines don’t make the
distinction, and we wish to remain compatible with the overall state of the
industry.
Despite not making the distinction in the class structure, it is important
to remember that points and vectors are not the same. One has direction and
length and the other position, so not all operations apply to both. For example,
we can add two vectors together to get a new vector. As we’ve seen, adding
two points together is only allowed in certain circumstances. So while we will
be using a single class, we will be maintaining mathematical correctness in
the text and writing the code to reﬂect this.
As mentioned, most of what we need for points is already in the IvVector3
class. The only additional code we’ll have to implement is for distance and

1.3 Points 49
distance squared operations:
float
Distance( const IvVector3& point1,
const IvVector3& point2 )
{
float x = point1.x - point2.x;
float y = point1.y - point2.y;
float z = point1.z - point2.z;
return IvSqrt( x*x + y*y + z*z );
}
float
DistanceSquared( const IvVector3& point1,
const IvVector3& point2 )
{
float x = point1.x - point2.x;
float y = point1.y - point2.y;
float z = point1.z - point2.z;
return ( x*x + y*y + z*z );
}
1.3.5 Polar and Spherical Coordinates
Cartesian coordinates are not the only way of measuring location. We’ve
already mentioned latitude, longitude, and altitude, and there are other,
related systems. Take a point P in R2 and compute the vector v = P −0.
We can specify the location of P using the distance r from P to the origin —
which is the length of v — and the angle θ between v and the positive x-axis,
where θ > 0 corresponds to a counterclockwise rotation from the axis. The
components (r, θ) are known as polar coordinates.
It is easy to convert from polar to Cartesian coordinates. We begin by
forming a right triangle using the x-axis, a line from P to O, and the perpen-
dicular from P to the x-axis (Figure 1.30). The hypotenuse has the length r
and is θ degrees from the x-axis. Using simple trigonometry, the lengths of
the other two sides of the triangle x and y can be computed as
x = r cos θ
(1.12)
y = r sin θ
From Cartesian to polar coordinates, we reverse the process. It’s easy
enough to generate r by computing the distance between P and O. Finding θ

50
Chapter 1 Vectors and Points
y-axis
x-axis
y
x

r
O
P
Figure 1.30 Relationship between polar and Cartesian coordinates.
is not as straightforward. The naive approach is to solve equation 1.12 for
θ, which gives us θ = arccos(x/r). However, the acos() function under C++
only returns an angle in the range of [0, π), so we’ve lost the sign of the angle.
Since
y
x = r sin θ
r cos θ
= sin θ
cos θ
= tan θ
an alternate choice would be arctan(y/x), but this doesn’t handle the case when
x = 0. To manage this, C++ provides a library function called atan2(), which
takes y and x as separate arguments and computes arctan(y/x). It has no
problems with division by 0 and maintains the signed angle with a range
of [−2π, 2π]. We’ll represent the use of this function in our equations as
arctan 2(y, x). The ﬁnal result is
r =

x2 + y2
θ = arctan 2(y, x)
If r is 0, θ may be set arbitrarily.
The system that extends this to three dimensions is called spherical
coordinates. In this system we call the distance from the point to the origin ρ
instead of r. We create a sphere of radius ρ centered on the origin and deﬁne
where the point lies on the sphere by two angles, φ and θ. If we take a vector

1.3 Points 51
y-axis
x-axis
O
z-axis
t
h
r
P
Figure 1.31 Spherical coordinates.
v from the origin to the point and project it down to the xy plane, θ is the
angle between the x-axis and rotating counterclockwise around z. The other
quantity, φ, measures the angle between v and the z-axis. The three values, ρ,
φ, and θ, represent the location of our point (Figure 1.31).
Spherical coordinates can be converted to Cartesian coordinates as
follows. Begin by building a right triangle as before, except with its hypotenuse
along ρ and base along the z-axis (Figure 1.32). The length z is then ρ cos φ.
To compute x and y, we project the vector v down onto the xy plane, and then
use polar coordinates. The length r of the projected vector v′ is ρ sin φ, so we
have
x = ρ sin φ cos θ
(1.13)
y = ρ sin φ sin θ
(1.14)
z = ρ cos φ
(1.15)
To convert from Cartesian to spherical coordinates, we begin by comput-
ing ρ, which again is the distance from the point to the origin. To ﬁnd φ, we
need to ﬁnd the value of ρ sin φ. This is equal to the projected xy length r since
r =

x2 + y2
=

(ρ sin φ cos θ)2 + (ρ sin φ sin θ)2
=

(ρ sin φ)2(cos2 θ + sin2 θ)
= ρ sin φ

52
Chapter 1 Vectors and Points
y-axis
x-axis
O
z-axis
t
h
r
x
z
y
P
Figure 1.32 Relationship between spherical and Cartesian coordinates.
And since, as with polar coordinates,
r
z = ρ sin φ
ρ cos φ
= tan φ
we can compute φ = arctan 2(r, z). Similarly, θ = arctan 2(y, x). Summarizing:
ρ =

x2 + y2 + z2
φ = arctan 2

x2 + y2, z

θ = arctan 2(y, x)
1.4 Lines
1.4.1 Definition
As with the point, a line as a geometric concept should be familiar. Euclid
deﬁnes a line as “breadthless length” and a straight line as that “which lies
evenly with the points on itself.” A straight line has also been referred to as the
shortest distance between two points, although in non-Euclidean geometry
this is not necessarily true.

1.4 Lines 53
From ﬁrst-year algebra, we know that a line in R2 is represented by the
formula
y = mx + b
(1.16)
where m is the slope of the line (it describes how y changes with each step of
x), and b is the coordinate location where the line crosses the y axis (called
the y-intercept). In this case x varies over all values and y is represented in
terms of x. This general form works for all lines in R2 except for those that
are vertical, since in that case the slope is inﬁnite and the y-intercept is either
nonexistent or is all values along the y-axis.
Equation 1.16 has a few problems. First of all, as mentioned, we can’t
easily represent a vertical line — it has inﬁnite slope. And, it isn’t obvious how
to transform this equation into one useful for three dimensions. We will need
a different representation.
1.4.2 Parameterized Lines
One possible representation is known as a parametric equation. Instead of
representing the line as a single equation with a number of variables, each
coordinate value is calculated by a separate function. This allows us to use
one form for a line that is generalizable across all dimensions. As an example,
we will take equation 1.16 and parameterize it.
To compute the parametric equation for a line, we need two points on our
line. We can take the y-intercept (0, b) as one of our points, and then take one
step in the positive x direction, or (1, m+b), to get the other. Subtracting point
1 from point 2, we get a 2D vector d = (1, m), which is oriented in the same
direction as the line (Figure 1.33). If we take this vector and add all the possible
scalar multiples of it to the starting point (0, b), then the points generated will
lie along the line. We can express this in one of the following forms:
L(t) = P0 + t(P1 −P0)
(1.17)
= (1 −t)P0 + tP1
(1.18)
= P0 + td
(1.19)
The variable t in this case is called a parameter.
d
P0
P1
Figure 1.33 Line.

54
Chapter 1 Vectors and Points
d
P0
P1
Figure 1.34 Line segment.
d
P0
P1
Figure 1.35 Ray.
We started with a two-dimensional example, but the formulas we just
derived work beyond two dimensions. As long as we have two points, we
can just substitute them into the preceding equations to represent a line.
More formally, if we examine equation 1.17, we see it matches equation 1.11.
The afﬁne combination of two unequal or noncoincident points span a line.
Equation 1.19 makes this even clearer. If we think of P0 as our origin and
d as a basis vector, they span a one-dimensional afﬁne space — which is
the line.
Since our line is spanned by an afﬁne combination of our two points, the
logical next question is, What is spanned by the convex combination? The
convex combination requires that t and (1 −t) lie between 0 and 1, which
holds only if t lies in the interval [0, 1]. Clamping t to this range gives us a line
segment (Figure 1.34). The edges of polygons are line segments, and we’ll also
be using line segments when we talk about bounding objects and collision
detection.
If we clamp t to only one end of the range, usually specifying that t ≥0,
then we end up with a ray (Figure 1.35) that starts at P0 and extends inﬁnitely
along the line in the direction of d. Rays are useful for intersection and visi-
bility tests. For example, P0 may represent the position of a camera, and d is
the viewing direction.
Library
IvMath
Filename
IvLine3
IvLineSegment3
IvRay3
In code we’ll be representing our lines, rays, and line segments as a point
on the line P and a vector d; so for example, the class deﬁnition for a line
in R3 is
class IvLine3
{
public:
IvLine3( const IvVector3& direction, const IvPoint3& origin );

1.4 Lines 55
IvVector3 mDirection;
IvPoint3
mOrigin;
};
1.4.3 Generalized Line Equation
There is another formulation of our two-dimensional line which can be useful.
Let’s start by writing out the equations for both x and y in terms of t:
x = Px + tdx
y = Py + tdy
Solving for t in terms of x:
t = (x −Px)
dx
Substituting this into the y equation we get
y = dy
(x −Px)
dx
+ Py
We can rewrite this as
0 = (y −Py)
dy
−(x −Px)
dx
= (−dy)x + (dx)y + (dyPx −dxPy)
= ax + by + c
(1.20)
where
a = −dy
b = dx
c = dyPx −dxPy = −aPx −bPy
We can think of a and b as the components of a two-dimensional vector n,
which is the perpendicular to the direction vector d, and so is orthogonal to
the direction of the line (Figure 1.36). This gives us a way of testing where a
2D point lies relative to a 2D line. If we substitute the coordinates of the point
into the x, y values of the equation, then a value of 0 indicates it’s on the line,

56
Chapter 1 Vectors and Points
n = (a, b)
P0
Figure 1.36 Normal form of 2D line.
a positive value indicates that it’s on the side where the vector is pointing,
and a negative value indicates that it’s on the opposite side. If we normalize
our vector, we can use the value returned by the line equation to indicate the
distance from the point to the line.
To see why this is so, suppose we have a test point Q. We begin by con-
structing the vector between Q and our line point P, or Q −P. There are
two possibilities. If Q lies on the side of the line where n is pointing, then the
distance between Q and the line is
d = ∥Q −P∥cos θ
where θ is the angle between n and Q −P. But since n · (Q −P) = ∥n∥∥Q −
P ∥cos θ, we can rewrite this as
d = n · (Q −P)
∥n∥
If Q is lying on the opposite side of the line, then we take the dot product with
the negative of n, so
d = −n · (Q −P)
∥−n∥
= −n · (Q −P)
∥n∥
Since d is always positive, we can just take the absolute value of n · (Q −P)
to get
d = |n · (Q −P)|
∥n∥
(1.21)

1.5 Planes 57
If we know that n is normalized, we can drop the denominator. If Q = (x, y)
and (as we’ve stated) n = (a, b), we can expand our values to get
d = a(x −Px) + b(y −Py)
= ax + by −aPx −bPy
= ax + by + c
If our n is not normalized, then we need to remember to divide by ∥n∥to get
the correct distance.
1.4.4 Collinear Points
Three or more points are said to be collinear if they all lie on a line. Another
way to think of this is that despite there being more than two points, the afﬁne
space that they span is only one-dimensional.
To determine whether three points P0, P1, and P2 are collinear, we take
the cross product of P1 −P0 and P2 −P0 and test whether the result is close
to the zero vector. This is equivalent to testing whether basis vectors for the
afﬁne space are parallel.
1.5 Planes
Euclid deﬁnes a surface as “that which has length and breadth only” and
a plane surface, or just a plane, as “a surface which lies evenly with the straight
lines on itself.” Another way of thinking of this is that a plane is created by
taking a straight line and sweeping each point on it along a second straight
line. It is a ﬂat, limitless, inﬁnitely thin surface.
1.5.1 Parameterized Planes
As with lines, we can express a plane algebraically in a number of ways. The
ﬁrst follows from our parameterized line. From basic geometry we know that
two noncoincident points form a line and three noncollinear points form
a plane. So if we can parameterize a line as an afﬁne combination of two
points, then it makes sense that we can parameterize a plane as an afﬁne
combination of three points P0, P1, and P2, or
P(s, t) = (1 −s −t)P0 + sP1 + tP2

58
Chapter 1 Vectors and Points
Alternatively, we can represent this as an origin point plus the linear
combination of two vectors:
P(s, t) = P0 + s(P1 −P0) + t(P2 −P0)
= P0 + su + tv
As with the parameterized line equation, if our points are of higher dimen-
sion, we can create planes in higher dimensions from them. However, in most
cases our planes will be ﬁrmly entrenched in R3.
1.5.2 Generalized Plane Equation
We can deﬁne an alternate representation for a plane in R3, just as we did for
a line in R2. In this form a plane is deﬁned as the set of points perpendicular to
a normal vector n = (a, b, c) which also contains the point P0 = (x0, y0, z0) as
shown in Figure 1.37. If a point P lies on the plane, then the vector v = P −P0
also lies on the plane. For v and n to be orthogonal, then n · v = 0. Expanding
this gives us the normal-point form of the plane equation, or
a(x −x0) + b(y −y0) + c(z −z0) = 0
We can pull all the constants into one term to get
0 = ax + by + cz −(ax0 + by0 + cz0)
= ax + by + cz + d
So extending equation 1.20 to three dimensions gives us the equation for a
plane in R3.
This is the generalized plane equation. As with the generalized line equa-
tion, this equation can be used to test where a point lies relative to either side
of a plane. Again, comparable to the line equation, it can be proved that if n is
normalized, |ax +by +cz+d| returns the distance from the point to the plane.
n = (a, b)
P0
Figure 1.37 Normal form of plane.

1.5 Planes 59
Testing points versus planes using the general plane equation happens
quite often. For example, to detect whether a point lies inside a convex poly-
hedron, you can do a plane test for every face of the polyhedron. Assuming
the plane normals point away from the center of the polyhedron, if the point
is on the negative side of all the planes then it lies inside. We may also use
planes as culling devices that cut our world into half-spaces. If an object lies
on one half of a plane, we consider it (say, for rendering purposes); otherwise,
we ignore it. The distance property can be used to test whether a sphere is
intersecting a plane. If the distance between the sphere’s center and the plane
is less than the sphere’s radius, then the sphere is intersecting the plane.
Given three points in R3 P, Q, and R, we generate the generalized plane
equation as follows. First we compute two vectors u and v, where
u = Q −P
v = R −P
Now we take the cross product of these two vectors to get the normal to the
plane:
n = u × v
We usually normalize n at this point so that we can take advantage of the
distance measuring properties of the plane equation. This gives us our values
a, b, and c. Taking P as the point on the plane, we compute D by
d = −(aPx + bPy + cPz)
We can also use this to convert our parameterized form to the generalized
form by starting with the cross product step.
Library
IvMath
Filename
IvPlane
Since we’ll be working in R3 most of the time and because of its useful
properties, we’ll be using the generalized plane equation as the basis for our
class:
class IvPlane
{
public:
IvPlane( float a, float b, float c, float d );
IvVector3 mNormal;
float
D;
};
However, from time to time we’ll be making use of the parameterized form,
so it’s good to keep it in mind.

60
Chapter 1 Vectors and Points
1.5.3 Coplanar Points
Four or more points are said to be coplanar if they all lie on a plane. Another
way to think of this is that despite the number of points being greater than
three, the afﬁne space that they span is only two-dimensional.
To determine whether four points P0, P1, P2, and P3 are coplanar, we
create vectors P1 −P0, P2 −P0, and P3 −P0, and compute their triple scalar
product. If the result is near zero, then they may be coplanar, if they’re not
collinear. To determine if they are collinear, take the cross products (P1 −
P0) × (P2 −P0), and (P1 −P0) × (P3 −P0). If both results are near zero, then
the points are collinear instead.
1.6 Polygons and Triangles
Library
IvMath
Filename
IvTriangle
The current class of graphics processors wants their geometric data in primar-
ily one form: points. However, having just a collection of points is not enough.
We need to organize these points into smaller groups, for both rendering and
computational purposes.
A polygon is made up of a set of vertices (which are represented by points)
and edges (which are represented by line segments). The edges deﬁne how
the vertices are connected together. A convex polygon is one where the set
of points enclosed by the vertices and edges is a convex set; otherwise, it’s
a concave polygon.
The most commonly used polygons for storing geometric data are triangles
(three vertices) and quadrilaterals (four vertices). While some rendering sys-
tems accept quadrilaterals (also referred to as just quads) as data, most want
geometry grouped in triangles, so we’ll follow that convention throughout
the remainder of the book. One advantage triangles have over quadrilater-
als is that three noncollinear vertices are guaranteed to be coplanar, so they
can be used to deﬁne a single plane. If the three vertices of a triangle are
collinear, then we have a degenerate triangle. Degenerate triangles can cause
problems on some hardware and with some geometric algorithms, so it’s good
to cull them by checking for collinearity of the triangle vertices, by using the
technique described previously.
If the points are not collinear, then as we’ve stated, the three vertices P0,
P1, and P2 can be used to ﬁnd the triangle’s incident plane. If we set u = P1−P0
and v = P2 −P0, then we can deﬁne this via the parameterized plane equation
P (s, t) = P0 + su + tv. Alternately, we can compute the generalized plane
equation by computing the cross product of u and v, normalizing to get the
normal ˆn, and then computing d as described in Section 1.5.2.
It’s often necessary to test whether a 3D point lying on the triangle plane
is inside or outside of the triangle itself (Figure 1.38). We begin by computing

1.6 Polygons and Triangles 61
P1
P2
P0
P
v0
v2
v1
w0
Figure 1.38 Point in triangle test.
three vectors v0, v1, and v2, where
v0 = P1 −P0
v1 = P2 −P1
v2 = P0 −P2
We take the cross product of v0 and v1 to get a normal vector n to the triangle.
We then compute three vectors from each vertex to the test point:
w0 = P −P0
w1 = P −P1
w2 = P −P2
If the point lies inside the triangle, then the cross product of each vi with
each wi will point in the same direction as n, which we can test by using a dot
product. If the result is negative, then we know they’re pointing in opposite
directions, and the point lies outside. For example, in Figure 1.38, the normal
vector to the triangle, computed as v0 × v1, points out of the page. But the
cross product v0 × w0 points into the page, so the point lies outside.
We can speed up this operation by projecting the point and triangle to
one of the xy, xz, or yz planes and treating it as a 2D problem. To improve
our accuracy, we’ll choose the one which provides the maximum area for the
projection of the triangle. If we look at the normal n for the triangle, one of
the coordinate values (x, y, z) will have the maximum absolute value; that is,
the normal is pointing generally along that axis. If we drop that coordinate and
keep the other two, that will give us the maximum projected area. We can then
throw out a number of zero terms and end up with a considerably faster test.

62
Chapter 1 Vectors and Points
This is equivalent to using the perpendicular dot product instead of the cross
product. More detail on this technique can be found in Section 11.3.5.
Another advantage that triangles have over quads is that (again, assuming
the vertices aren’t collinear) they are convex polygons. In particular, the con-
vex combination of the three triangle vertices spans all the points that make
up the triangle. Given a point P inside the triangle and on the triangle plane,
it is possible to compute its particular barycentric coordinates (s, t), as used
in the parameterized plane equation P(s, t) = P0 + su + tv. If we compute
a vector w = P −P0, then we can rewrite the plane equation as
P = P0 + su + tv
w = su + tv
If we take the cross product of v with w, we get
v × w = v × (su + tv)
= s(v × u) + t(v × v)
= s(v × u)
Taking the length of both sides gives
∥v × w∥= |s|∥v × u∥
The quantity ∥v × u∥= ∥u × v∥. And since P is inside the triangle, we know
that to meet the requirements of a convex combination s ≥0, so
s = ∥v × w∥
∥u × v∥
A similar construction ﬁnds that
t = ∥u × w∥
∥u × v∥
Note that this is equivalent to computing the areas a and b of the two
subtriangles shown in Figure 1.39 and dividing by the total area of the
triangle c, so
s = b/c
t = a/c

1.7 Chapter Summary 63
P1
P0
P2
v
w
u
P
a
b
Figure 1.39 Computing barycentric coordinates for point in triangle.
where
a = 1
2∥u × w∥
b = 1
2∥v × w∥
c = 1
2∥u × v∥
These simple examples are only a taste of how we can use triangles in
mathematical calculations. More details on the use and implementation of
triangles can be found throughout the text, particularly in Chapters 6 and 11.
1.7 Chapter Summary
In this chapter, we have covered some basic geometric entities: vectors and
points. We have discussed linear and afﬁne spaces, the relationships between
them, and how we can use afﬁne combinations of vectors and points to deﬁne
other entities like lines and planes. We’ve also shown how we can use our
knowledge of afﬁne spaces and vector properties to compute some simple tests
on triangles. These skills will prove useful to us throughout the remainder of
the text.
For those who are interested in reading further, Anton and Rorres [3] is
a standard reference for many ﬁrst courses in linear algebra. Other texts with
slightly different approaches are Axler [7] and Friedberg [37]. Information on
points and afﬁne spaces can be found in Schneider and Eberly [96], as well
as in deRose [25].


Chapter2
Linear
Transformations
and Matrices
2.1 Introduction
In the previous chapter we discussed vectors and points and some simple
operations we can apply to them. Now we’ll begin to expand our discussion
to cover speciﬁc functions that we can apply to vectors and points; functions
known as transformations. In this chapter we’ll begin with a class of trans-
formations that we can apply to vectors called linear transformations. These
encompass nearly all of the common operations we might want to perform on
vectors and points, so understanding what they are and how to apply them is
important. We’ll deﬁne these functions and how they are distinguished from
other, more general transformations.
Properties of linear transformations allow us to use a structure called a
matrix as a compact representation for transforming vectors. A matrix is a
simple 2D array of values, but within it lies all the power of a linear trans-
formation. Through simple operations we can use the matrix to apply linear
transformations to vectors. We can also combine two transformation matri-
ces to create a new one that has the same effect of the ﬁrst two. Using matrices
effectively lies at the heart of the pipeline for manipulating virtual objects and
rendering them on the screen.
Matrices have other applications as well. Examining the structure of
a matrix can tell us something about the transformation it represents; for
example, whether it can be reversed, what that reverse transformation might
be, or whether it distorts the data that it is given. Matrices can also be used
65

66
Chapter 2 Linear Transformationsand Matrices
to solve systems of linear equations, which is useful to know for certain
algorithms in graphics and physical simulation. For all of these reasons,
matrices are primary data structures in graphics application programmer
interfaces (APIs).
2.2 Linear Transformations
Linear transformations are a very useful and important concept in linear alge-
bra. As one of a class of functions known as transformations, they map vector
spaces to vector spaces. This allows us to apply complex functions to, or
transform, vectors. Linear transformations perform this mapping while also
having the additional property of preserving linear combinations. We will see
how this permits us to describe the linear transformation in terms of how it
affects the basis vectors of a vector space. Later parts will show how this in
turn allows us to represent linear transformations using matrices.
2.2.1 Definitions
Before we can begin to discuss transformations and linear transformations
in particular, we need to deﬁne a few terms. A relation maps a set X of values
(known as the domain) to another set Y of values (known as the range). A
function is a relation where every value in the ﬁrst set maps to one and only
one value in the second set, for example f (x) = sin x. An example of a relation
that is not a function is ±√x, because there are two possible results for a
positive value of x, either positive or negative.
A function whose domain is an n-dimensional space and whose range is
an m-dimensional space is known as a transformation. A transformation that
maps from Rn to Rm is expressed as T : Rn →Rm. If the domain and the range
of a transformation are equal (i.e., T : Rn →Rn), then the transformation is
sometimes called an operator.
An example of a transformation is the function
f (x, y) = x2 + 2y
which maps from R2 to R. Another example is
f (x, y, z) = x2 + 2y + √z
which maps from R3 to R.

2.2 Linear Transformations 67
We can also map to a multidimensional space. For example, we could
deﬁne a transformation from R2 to R2 as follows:
T(a, b) = (f (a, b), g(a, b))
(2.1)
A linear transformation T is a mapping between two vector spaces V and
W, where for all v in V and for all scalars a:
1. T(v0 + v1) = T(v0) + T(v1) for all v0, v1 in V
2. T(av) = aT (v) for all v in V
To determine whether a transformation is linear, it is sufﬁcient to show
that
T(ax + y) = aT(x) + T(y)
An example of a linear transformation is T(x) = kx, where k is any ﬁxed scalar.
We can show this by
T(ax + y) = k(ax + y)
= akx + ky
= aT(x) + T(y)
On the other hand, the function g(x) = x2 is not linear because, for a = 2,
x = 1, and y = 1:
g(2(1) + 1) = (2(1) + 1)2
= 32 = 9
̸= 2(g(1)) + g(1)
= 2(12) + 12 = 3
As we might expect, the only operations possible in a linear function are
multiplication by a constant and addition.
2.2.2 Null Space and Range
We deﬁne the null space (or kernel) N(T) of a linear transformation T : V →W
as the set of all vectors in V that map to 0, or
N(T) = {x | T(x) = 0}
The dimension of N(T) is called the nullity of the transformation.

68
Chapter 2 Linear Transformationsand Matrices
We deﬁne the range R(T) of a linear transformation T : V →W as the set
of all vectors in W that are mapped to by at least one vector in V , or
R(T) = {T(x)|x ∈V }
The dimension of R(T) is called the rank of the transformation.
The null space and range have two important properties. First of all, they
are both vector spaces, and in fact the null space is a subspace of V and the
range is a subspace of W. Second,
nullity(T) + rank(T) = dim(T)
To get a better sense of this, let’s look at an example. Suppose we have the
linear transformation
T(a, b) = (a + b, 0)
The resulting range space is of the form (x, 0), so it can be spanned by the
vector (1, 0) and has dimension 1. The transformation will produce the vector
(0, 0) only when a = −b. So the null space has a basis of (1, −1) and is also
one-dimensional. As we expect, they add up to 2, the dimension of our original
vector space (Figure 2.1).
Range (y=0)
Null space (y=–x)
Figure 2.1 Range and null space for transformation T(a,b) = (a+b, 0).

2.2 Linear Transformations 69
2.2.3 Linear Transformations and Basis
Vectors
Using standard function notation to represent linear transformations (as in
equation 2.1) is not the most convenient nor compact format, particularly
for transformations between higher-dimensional vector spaces. Fortunately,
using the properties of vectors will allow us to deﬁne something more useful
to us.
Recall that we can represent any vector x in an n-dimensional vector
space V as
x = x0v0 + x1v1 + · · · + xn−1vn−1
where {v0, v1, . . . , vn−1} is a basis for V .
Now suppose we have a linear transformation T : V →W that maps from
V to an m-dimensional vector space W. If we apply our transformation to our
arbitrary vector x, then we have
T(x) = T(x0v0 + x1v1 + · · · + xn−1vn−1)
= x0T(v0) + x1T(v1) + · · · + xn−1T(vn−1)
(2.2)
So if we know how our linear transformation affects our basis for V , then
we can calculate the effect of the linear transformation for any arbitrary
vector in V .
There is still an open question: What are the components of each T(vd)
equal to? For a member vd of V ’s basis, we can represent T(vd) in terms of the
basis {w0, w1, . . . , wm−1} for W, again as a linear combination:
T(vj) = a0,jw0 + a1,jw1 + · · · + am−1,jwm−1
If {w0, . . . , wm−1} is the standard basis for W, this simpliﬁes to
T(vj) = (a0,j, a1,j, . . . , am−1,j)
(2.3)
Combining equations 2.2 and 2.3 gives us
T(x) = x0(a0,0, a1,0, . . . , am−1,0)
+ x1(a0,1, a1,1, . . . , am−1,1)
· · ·
+ xn−1(a0,n−1, a1,n−1, . . . , am−1,n−1)
(2.4)

70
Chapter 2 Linear Transformationsand Matrices
If we set b = T(x), then for a given component of b
bi = ai,0x0 + ai,1x1 + · · · + ai,n−1xn−1
(2.5)
Knowing this, we can precalculate and store the n transformed basis vec-
tors (a0,j, a1,j, . . . , am−1,j) and use this formula at any time to transform a
general vector x.
Let’s look at an example. Taking a transformation from R2 to R2, using
the standard basis for both vector spaces:
T(a, b) = (a + b, b)
If we look at how this affects our standard basis for R2, we get
T(1, 0) = (1 + 0, 0) = (1, 0)
T(0, 1) = (0 + 1, 1) = (1, 1)
Transforming an arbitrary vector in R2, say (2, 3), we get
T(2, 3) = 2T(1, 0) + 3T(0, 1)
= 2(1, 0) + 3(1, 1)
= (5, 3)
which is what we expect.
It should be made clear that applying a linear transformation to a basis
does not produce the basis for the new vector space. It only shows where
the basis vectors end up in the new vector space — in our case in terms of
the standard basis. In fact, a transformed basis may no longer be linearly
independent. Take as another example
T(a, b) = (a + b, 0)
Applying this to our standard basis for R2, we get
T(1, 0) = (1 + 0, 0) = (1, 0)
T(0, 1) = (0 + 1, 0) = (1, 0)
The two resulting vectors are clearly linearly dependent.
These two examples illustrate one useful property. If the rank of a linear
transformation T equals the number of elements in a transformed basis β,
then we can say that β is linearly independent. In fact, the rank is equal to the

2.3 Matrices 71
number of linearly independent elements in β, and those linearly independent
elements will span the range of T.
In summary, knowing that we can represent a linear transformation in
terms of how the basis vectors are transformed is a very powerful tool. As we
will see, it is precisely this property of linear transformations that allows us
to represent them concisely by using a matrix.
2.3 Matrices
2.3.1 Introduction to Matrices
A matrix is a rectangular, two-dimensional array of values. Throughout this
book, most of the values we use will be real numbers, but they could be
complex numbers or even vectors. Each individual value in a matrix is called
an element. Examples of matrices are
A =


1
0
0
0
1
0
0
0
1


B =
 0
35
−15
2
52
1

C =


2
−1
0
2
6
3


A matrix is described as having m rows by n columns, or being an m × n
matrix. A row is a horizontal group of elements from left to right, while a
column is a vertical, top-to-bottom group. Matrix A in our example has 3 rows
and 3 columns and is a 3 × 3 matrix, whereas matrix C is a 3 × 2 matrix. Rows
are numbered 0 to m−1,1 while columns are numbered 0 to n−1. An individual
element of a matrix A is referenced as either (A)i,j or just ai,j, where i is the
row number and j is the column. Looking at matrix B, element b10 contains
the value 2 and element b01 equals 35.
If an individual matrix has an equal number of rows and columns, that
is if m equals n, then it is called a square matrix. Matrix A is square, whereas
matrices B and C are not.
If all elements of a matrix are zero, then it is called a zero matrix. We will
represent a matrix of this type as 0 and assume a matrix of the appropriate
size for the operation we are performing.
If two matrices have an equal number of rows and columns, then they
are said to be the same size. If they are the same size and their corresponding
1.
As a reminder, mathematical convention starts with 1, but we’re using 0 to be compatible
with C++.

72
Chapter 2 Linear Transformationsand Matrices
elements have the same values, then they are equal. Below, the two matrices
are the same size, but they are not equal.


0
1
3
2
0
−3

̸=


0
0
2
−3
1
3


The set of elements where row and column number are the same is called
the main diagonal. In the next example the main diagonal is in bold.
U =


3
−5
0
1
0
2
6
0
0
0
1
−8
0
0
0
1


The trace of a matrix is the sum of the main diagonal elements. In this case
the trace is 3 + 2 + 1 + 1 = 7.
In matrix U, all elements below the diagonal are equal to 0. This is known
as an upper triangular matrix. Note that elements above the diagonal don’t
necessarily have to be nonzero in order for the matrix to be upper triangular,
nor does the matrix have to be square.
If elements above the diagonal are 0, then we have a lower triangular
matrix
L =


3
0
0
0
2
2
0
0
0
3
1
0
−6
1
0
1


Finally, if a square matrix has nondiagonal elements of zero, we call the
matrix a diagonal matrix:
D =


3
0
0
0
0
2
0
0
0
0
1
0
0
0
0
1


It follows that any diagonal matrix is both an upper triangular and lower
triangular matrix.

@Spy
2.3 Matrices 73
2.3.2 Simple Operations
Matrix Addition and Scalar Multiplication
We can add and scale matrices just as we can vectors. Adding two matrices
together:
S = A + B
is done componentwise like vectors, thus
si,j = ai,j + bi,j
Clearly, in order for this to work, A, B, and S must all be the same size (also
known as conformable for addition). Subtraction works similarly but as with
real numbers and vectors is not commutative.
To scale a matrix,
P = sA
each element is multiplied by the scalar, again like vectors:
pi,j = s · ai,j
Matrix addition and scalar multiplication have their algebraic rules,
which should seem quite familiar at this point:
1. A + B = B + A
2. A + (B + C) = (A + B) + C
3. A + 0 = A
4. A + (−A) = 0
5. a(A + B) = aA + aB
6. a(bA) = (ab)A
7. (a + b)A = aA + bA
8. 1A = A
As we can see, these rules match the requirements for a vector space, and so
the set of matrices of a given size is also a vector space.

@Spy
74
Chapter 2 Linear Transformationsand Matrices
Transpose
The transpose of a matrix A (represented by AT ) interchanges the rows and
columns of A. It does this by exchanging elements across the matrix’s main
diagonal, so (AT )i,j = (A)j,i. An example of this is


2
−1
0
2
6
3

=

2
0
6
−1
2
3

As we can see, the matrix does not have to be square, so an m × n matrix
becomes an n × m matrix. Also, the main diagonal doesn’t change, or is
invariant, since (AT )i,i = (A)i,i.
A matrix where (A)i,j = (A)j,i (i.e., cross-diagonal entries are equal) is
called a symmetric matrix. All diagonal matrices are symmetric. Another
example of a symmetric matrix is


3
1
2
3
1
2
−5
0
2
−5
1
−9
3
0
−9
1


The transpose of a symmetric matrix is the matrix again, since in this case
(AT )j,i = (A)i,j = (A)j,i.
A matrix where (A)i,j = −(A)j,i (i.e., cross-diagonal entries are negated
and the diagonal is 0) is called a skew symmetric matrix. An example of a skew
symmetric matrix is


0
1
2
−1
0
−5
−2
5
0


The transpose of a skew symmetric matrix is the negation of the original
matrix, since in this case (AT )j,i = (A)i,j = −(A)j,i.
Some useful algebraic rules involving the transpose are
1. (AT )T = A
2. (aAT ) = aAT
3. (A + B)T = AT + BT
where a is a scalar and A and B are conformable for addition.

@Spy
2.3 Matrices 75
2.3.3 Vector Representation
If a matrix has only one row or one column, then we have a row or column
matrix, respectively:
 .5
.25
1
−1 


5
−3
6.9


These are often used to represent vectors. While there is no particular stan-
dard as to which one to use, in this text we will assume that vectors are
represented as column matrices (also known as column vectors). First of
all, most math texts use column vectors and we wish to remain compati-
ble. In addition, we want to ensure that any matrix we may reference will be
usable by our graphics pipeline. We’ll be doing some derivations based on the
OpenGL speciﬁcation and its documentation uses column vectors. DirectX,
by comparison, uses row vectors. Finally, the classical presentation of quater-
nions (another means for performing some linear transformations) uses a
concatenation order consistent with the use of column matrices for vectors.
The choice to represent vectors as column matrices does have some effect
on how we construct and multiply our matrices, which we will discuss in
more detail in the following parts. In the cases where we do wish to indicate
that a vector is represented as a row matrix, we’ll display it with a transpose
applied, like bT .
2.3.4 Block Matrices
A matrix can also be represented by submatrices, rather than by individual
elements. This is also known as a block matrix. For example, the matrix


2
3
0
−3
2
0
0
0
1


can also be represented as
 A
0
0T
1

where
A =

2
3
−3
2


@Spy
76
Chapter 2 Linear Transformationsand Matrices
and
0 =
 0
0

We will sometimes use this to represent a matrix as a set of row or column
matrices. For example, if we have a matrix A


a0,0
a0,1
a0,2
a1,0
a1,1
a1,2
a2,0
a2,1
a2,2


We can represent its rows as three vectors
aT
0 =
 a0,0
a0,1
a0,2

aT
1 =
 a1,0
a1,1
a1,2

aT
2 =
 a2,0
a2,1
a2,2

and represent A as


aT
0
aT
1
aT
2


Similarly, we can represent a matrix B with its columns as three vectors
b0 =


b0,0
b1,0
b2,0


b1 =


b0,1
b1,1
b2,1


b2 =


b0,2
b1,2
b2,2


and subsequently B as
 b0
b1
b2

As mentioned earlier, the transpose notation tells us whether we’re using row
or column vectors.

@Spy
2.3 Matrices 77
2.3.5 Matrix Product
The primary operation we will apply to matrices is multiplication, also known
as the matrix product. The product is important to us because it allows us to
do two essential things. First, multiplying a matrix by a compatible vector will
perform a linear transformation on the vector. Second, multiplying matrices
together will create a single matrix that performs their combined linear trans-
formations. We’ll discuss how this is possible shortly, but ﬁrst we must deﬁne
how to perform matrix multiplication.
As with real numbers, the product C of two matrices A and B is
represented as
C = AB
Computing the matrix product is not as simple as multiplying real numbers
but is not that bad if you understand the process. To calculate a given element
ci,j in the product, we take the dot product of row i from A with column j
from B. We can express this symbolically as
ci,j =
n−1

k=0
ai,kbk,j
As an example, we’ll look at computing the ﬁrst element of a 3 × 3 matrix:


a0,0
a0,1
a0,2
...
...
...
...
...
...




b0,0
· · ·
· · ·
b1,0
· · ·
· · ·
b2,0
· · ·
· · ·

=


c0,0
· · ·
· · ·
...
...
...
...
· · ·
...


To compute the value of c0,0, we take the dot product of row 1 from A and
column 1 from B:
c0,0 = a0,0b0,0 + a0,1b1,0 + a0,2b2,0
Expanding this for a 2 × 2 matrix:
 a0,0
a0,1
a1,0
a1,1
  b0,0
b0,1
b1,0
b1,1

=
 a0,0b0,0 + a0,1b1,0
a0,0b0,1 + a0,1b1,1
a1,0b0,0 + a1,1b1,0
a1,0b0,1 + a1,1b1,1

If we represent A as a collection of rows and B as a collection of columns, then

aT
0
aT
1

 b0
b1

=

a0 · b0
a0 · b1
a1 · b0
a1 · b1


@Spy
78
Chapter 2 Linear Transformationsand Matrices
We can also multiply by using block matrices:
 A
B
C
D
  E
F
G
H

=
 AE + BG
AF + BH
CE + DG
CF + DH

Note that this is only allowable if the submatrices are conformable for addition
and multiplication.
There is a restriction on which matrices can be multiplied together; in
order to perform a dot product the two vectors have to have the same length.
So to multiply together two matrices, the number of columns in the ﬁrst (i.e.,
the width of each row) has to be the same as the number of rows in the second
(i.e., the height of each column). Because of this restriction, the only matrices
that can be multiplied by themselves are square.
In general, matrix multiplication is not commutative. As an example, if
we multiply a row matrix by a column matrix, we perform a dot product:
 1
2   3
4

= 1 · 3 + 2 · 4 = 11
Because of this, you may often see a dot product represented as
a · b = aT b
If we multiply them in the opposite order, we get a square matrix:
 3
4
  1
2 
=
 3
6
4
8

Even multiplication of square matrices is not necessarily commutative:
 3
6
4
8
  1
0
1
1

=
 9
6
12
8

 1
0
1
1
  3
6
4
8

=
 3
6
7
14

Aside from the size restriction and not being commutative, the algebraic
rules for matrix multiplication are very similar to those for real numbers:
1. A(BC) = (AB)C
2. a(BC) = (aB)C
3. A(B + C) = AB + AC

@Spy
2.3 Matrices 79
4. (A + B)C = AC + BC
5. (AB)T = BTAT
where A, B, and C are matrices conformable for multiplication and a is a
scalar. Note that matrix multiplication is still associative (rules 1 and 2) and
distributive (rules 3 and 4).
2.3.6 Transforming Vectors
As previously indicated, matrices can be used to represent linear transforma-
tions on vectors. We do this by multiplying the matrix by the vector we wish
to transform, or simply
b = Ax
Let’s expand our terms and examine the components of the matrix and each
vector:


b0
b1
...
bm−1

=


a0,0
a0,1
· · ·
a0,n−1
a1,0
a1,1
· · ·
a1,n−1
...
...
...
...
am−1,0
am−1,1
· · ·
am−1,n−1




x0
x1
...
xn−1


This represents a transformation from an n-dimensional space V to an
m-dimensional space W, so x has n components and the resulting vector b has
m. In order for the multiplication to proceed, matrix A must be m × n. As with
general matrix multiplication, whenever we perform matrix–vector multipli-
cation, the number of components in the multiplied vector must match the
number of columns in the matrix, and the resulting vector will have a number
of components equal to the number of rows.
To see how this operation performs a linear transformation, we’ll use
the fact that we only need to know where the basis of a vector space V is
mapped to. Suppose that we know that our standard basis {e0, e1, . . . , en−1}
is transformed to {a0, a1, . . . , an−1} in W, again using the standard basis. We
will store, in order, each of these transformed basis vectors as the columns
of A, or
A =
 a0
a1
· · ·
an−1

Using our matrix multiplication deﬁnition to compute the product of A
and a vector x in V , we see that the result for element i in b is
bi = ai,0x0 + ai,1x1 + · · · + ai,n−1xn−1

@Spy
80
Chapter 2 Linear Transformationsand Matrices
This is exactly the same as equation 2.5. So by setting up our matrix with the
transformed basis vectors in each column, we can use matrix multiplication
to perform linear transformations.
Column vectors aren’t the only possibility. We can also premultiply by a
vector by treating it as a row matrix:
 c0
c1 · · · cn−1

=
 x0
x1 · · · xm−1



a0,0
a0,1
· · ·
a0,n−1
a1,0
a1,1
· · ·
a1,n−1
...
...
...
...
am−1,0
am−1,1
· · ·
am−1,n−1


or
cT = xTA
In this case the rows of A are acting as our transformed basis vectors, and
the number of components in xT must match the number of rows in our
matrix.
At this point we can deﬁne some additional properties for matrices. The
column space of a matrix is the vector space spanned by the matrix’s column
vectors and is the range of the linear transformation performed by post-
multiplying by a column vector. Correspondingly, the row space is the vector
space spanned by the row vectors of the matrix and, as we’d expect, is the
range of the linear transformation performed by premultiplying by a row vec-
tor. As it happens, the dimensions of the row space and column space are
equal and that value is called the rank of the matrix. The matrix rank is equal
to the rank of the associated linear transformation.
The column space and row space are not necessarily the same vector
space. As an example, take the matrix


0
1
0
0
0
1
0
0
0


When postmultiplied by a column vector, it maps a vector (x, y, z) in R3 to a
vector (y, z, 0) on the xy-plane. Premultiplying by a row vector, on the other
hand, maps (x, y, z) to (0, x, y) on the yz-plane. They have the same dimension,
and hence the same rank, but they are not the same vector space.
This makes a certain amount of sense. When we multiply by a row vec-
tor, we use the row vectors of the matrix as our transformed basis instead
of the column vectors. To achieve the same result as the column vector

@Spy
2.3 Matrices 81
multiplication, we need to change our matrix’s column vectors to row vectors
by taking the transpose:
 x
y
z 


0
0
0
1
0
0
0
1
0

=
 y
z
0 
We can now see the purpose of the transpose: it exchanges a matrix’s row
space with its column space.
Like a linear transformation, a matrix also has a null space, which is all
vectors x in V such that
Ax = 0
In the preceding example, the null space N is all vectors with zero y and z
components. As with linear transformations, dim(N)+rank = dim(V ).
2.3.7 Combining Linear Transformations
Suppose we have two transformations, S : U →V and T : V →W, and we
want to perform one after the other; namely, for a vector x, we want the
result T(S(x)). If we know that we are going to transform a large collection of
vectors by S and the resulting vectors by T, it will be more efﬁcient to ﬁnd a
single transformation that generates the same result so that we only have to
transform the vectors once. This is known as the composition of S and T and
is written as
(T ◦S)(x) = T(S(x))
Composition (or alternatively, concatenation) of transformations is done via
generalized matrix multiplication.
Suppose that matrix A is the corresponding transformation matrix for S
and B is the corresponding matrix for T. Recall that in order to set up A for
vector transformation, we pretransform the standard basis vectors by S and
store them as the columns of A. Now we need to transform those vectors
again, this time by T. We could either do this explicitly or use the fact that
multiplying by B will transform vectors (in V ) by T. So we just multiply each
column of A by B and store the results, in order, as columns in a new matrix C:
C = BA
If U has dimension n, V has dimension m, and W has dimension l, then A will
be an m×n matrix and B will be an l ×m matrix. Since the number of columns

@Spy
82
Chapter 2 Linear Transformationsand Matrices
in B matches the number of rows in A, the matrix product can proceed, as
we’d expect. The result C will be an l × n matrix and will apply the trans-
formation of A followed by the transformation of B in a single matrix–vector
multiplication.
This is the power of using matrices as a representation for linear trans-
formations. By continually concatenating matrices, we can use the result to
produce the effect of an entire series of transformations, in order, through a
single matrix multiplication. Note that the order does matter. The preceding
result C will perform the result of applying A followed by B. If we swap the
terms (assuming they’re still conformable under multiplication),
D = AB
and matrix D will perform the result of applying B followed by A. This is
almost certainly not the same transformation.
For the discussion thus far, we have assumed that the resulting matrix
will be applied to a vector represented as a column matrix. It is good to be
aware that the choice of whether to represent a vector as a row matrix or
column matrix affects the order of multiplications when combining matrices.
Suppose we multiply a column vector u by three matrices, where the intended
transformation order is to apply M0, then M1, and ﬁnally M2:
v = M0u
w = M1v
x = M2w
(2.6)
If we take equation 2.6 and substitute M1v for w and then M0u for v,
we get
x = M2M1v
= M2M1M0u
= Mcu
Doing something similar for a row vector aT :
bT = aT N0
cT = bT N1
dT = cT N2

@Spy
2.3 Matrices 83
and substituting:
dT = bT N1N2
= aT N0N1N2
= aT Nr
The order difference is quite clear. When using row vectors and concate-
nating, matrix order follows the left to right progress used in English text.
Column vectors work right to left instead, which may not be as intuitive. We
will just need to be careful about our matrix order and transpose any matrices
that assume we’re using row vectors.
There are two other ways to modify transformation matrices that aren’t
used as often. Instead of concatenating two transformations, we may want to
create a new one by adding two together: Q(x) = S(x)+T(x). This is easily done
by adding the corresponding matrices together, so the matrix that performs Q
is C = A+B. Another means we might use for generating a new transformation
from an existing one is to scale it: R(x) = s · T(x). The corresponding matrix
is created by scaling the original matrix: D = sA.
2.3.8 Identity Matrix
We know that when we multiply a scalar or vector by 1, the result is the scalar
or vector again:
1 · x = x
Similarly, in matrix multiplication there is a special matrix known as the
identity matrix, represented by the letter I. Thus,
A · I = I · A = A
The identity matrix maps the basis vectors of the domain to the same vectors
in the range; it performs a linear transformation that has no effect on the
source vector: the identity transformation.
A particular identity matrix is a diagonal square matrix, where the
diagonal is all 1s:
I =


1
0
· · ·
0
0
1
0
...
...
...
0
0
· · ·
1



84
Chapter 2 Linear Transformationsand Matrices
If a particular n × n identity matrix is needed, it is sometimes referred to as
In. Take as an example I3:
I3 =


1
0
0
0
1
0
0
0
1


Rather than referring to it in this way, we’ll just use the term I to represent a
general identity matrix and assume it is the correct size in order to allow an
operation to proceed.
2.3.9 Performing Vector Operations with
Matrices
Recall that if we multiply a row vector by a column vector, it performs a dot
product:
wT v = wxvx + wyvy + wzvz = v · w
And multiplying them in the opposite order produces a square matrix:
T = v wT =


vxwx
vxwy
vxwz
vywx
vywy
vywz
vzwx
vzwy
vzwz


This square matrix T is known as the tensor product v ⊗w. We can use it to
rewrite vector expressions of the form (u · v)w as
(u · v)w = (w ⊗v)u
In particular, we can rewrite a projection by a unit vector as
(u · ˆv)ˆv = (ˆv ⊗ˆv)u
This will prove useful to us in the next chapter.
We can also perform our other vector product, the cross product, through
a matrix multiplication. If we have two vectors v and w and we want to
compute v × w, we can replace v with a particular skew symmetric matrix,
represented as ˜v:
˜v =


0
−vz
vy
vz
0
−vx
−vy
vx
0



2.3 Matrices 85
Multiplying by w gives


0
−vz
vy
vz
0
−vx
−vy
vx
0




wx
wy
wz

=


vywz −wyvz
vzwx −wzvx
vxwy −wxvy


which is the formula for the cross product. This will also prove useful to us
in subsequent chapters.
2.3.10 Implementation
Library
IvMath
Filename
IvMatrix33
IvMatrix44
One might expect that the most natural data format for, say, a 3 × 3 matrix
would be
class IvMatrix33
{
float mData[3][3];
};
However, the memory layout of such a matrix is not ideal for our purposes.
In C or C++, two-dimensional arrays are stored in what is called row major
order, meaning that the matrix is stored in memory in a row by row order. If
we use a one-dimensional array as our member variable instead:
class IvMatrix33
{
float mV[9];
};
the index order for a 3 × 3 matrix is


0
1
2
3
4
5
6
7
8


The indexing operator for a row major matrix (we have to use operator()
because operator[] only works for a single index) is
float&
IvMatrix33::operator()(unsigned int row, unsigned int col)

86
Chapter 2 Linear Transformationsand Matrices
{
return mV[col + 3*row];
}
Why won’t this work? Well, in Direct3D matrices are expected to be used
with row vectors. And even in OpenGL, despite the fact that the documenta-
tion is written using column vectors, the internal representation premultiplies
the vectors; that is, it expects row vectors as well. Accordingly, since we’re
using column vectors, we will need to transpose our matrices before we pass
them in as arguments to the graphics API. Doing this for every single matrix
takes time and is a bit of nuisance to remember. Missing that one transpose
can make debugging your algorithm a longer process than it needs to be.
The solution is to pretranspose the matrix in the storage representation.
This is a format known as column major order and stores a matrix column by
column instead of row by row. Writing out our indices in column major order
gives us


0
3
6
1
4
7
2
5
8


Notice that the indices are the transpose of row major order. The indexing
operator becomes
float&
IvMatrix33::operator()(unsigned int row, unsigned int col)
{
return mV[row + 3*col];
}
Alternatively, if we want to use two-dimensional arrays:
float&
IvMatrix33::operator()(unsigned int row, unsigned int col)
{
return mV[col][row];
}
Using column major format and column vectors, matrix–vector multipli-
cation becomes
IvVector3
IvMatrix33::operator*( const IvVector3& vector ) const

2.3 Matrices 87
{
IvVector3 result;
result.x = mV[0]*vector.x + mV[3]*vector.y + mV[6]*vector.z;
result.y = mV[1]*vector.x + mV[4]*vector.y + mV[7]*vector.z;
result.z = mV[2]*vector.x + mV[5]*vector.y + mV[8]*vector.z;
return result;
}
and matrix–matrix multiplication is
IvMatrix33
IvMatrix33::operator*( const IvMatrix33& other ) const
{
IvMatrix33 result;
result.mV[0] = mV[0]*other.mV[0] + mV[3]*other.mV[1] + mV[6]*other.mV[2];
result.mV[1] = mV[1]*other.mV[0] + mV[4]*other.mV[1] + mV[7]*other.mV[2];
result.mV[2] = mV[2]*other.mV[0] + mV[5]*other.mV[1] + mV[8]*other.mV[2];
result.mV[3] = mV[0]*other.mV[3] + mV[3]*other.mV[4] + mV[6]*other.mV[5];
result.mV[4] = mV[1]*other.mV[3] + mV[4]*other.mV[4] + mV[7]*other.mV[5];
result.mV[5] = mV[2]*other.mV[3] + mV[5]*other.mV[4] + mV[8]*other.mV[5];
result.mV[6] = mV[0]*other.mV[6] + mV[3]*other.mV[7] + mV[6]*other.mV[8];
result.mV[7] = mV[1]*other.mV[6] + mV[4]*other.mV[7] + mV[7]*other.mV[8];
result.mV[8] = mV[2]*other.mV[6] + mV[5]*other.mV[7] + mV[8]*other.mV[8];
return result;
}
Matrix addition is just
IvMatrix33
IvMatrix33::operator+( const IvMatrix33& other ) const
{
IvMatrix33 result;
for (int i = 0; i < 9; ++i)

88
Chapter 2 Linear Transformationsand Matrices
{
result.mV[i] = mV[i]+other.mV[i];
}
return result;
}
Scalar multiplication of matrices is similar.
It is common practice to refer to a matrix intended to be used with row
vectors (i.e., its transformed basis vectors are stored as rows) as row major
order and, similarly, to a matrix intended to be used with column vectors as
column major order. This is incorrect terminology. Row and column major
order refer only to the storage format; namely, where an element ai,j will lie
in the one-dimensional representation of the matrix. Whether your matrix
library intends for vectors to be pre- or postmultiplied should be independent
of the underlying storage.
2.4 Systems of Linear Equations
2.4.1 Definition
Other than performing linear transformations, another purpose of matrices
is to act as a mechanism for solving systems of linear equations. A general
system of m linear equations with n unknowns is represented as
b0 = a0,0x0 + a0,1x1 + · · · + a0,n−1xn−1
b1 = a1,0x0 + a1,1x1 + · · · + a1,n−1xn−1
...
...
bm−1 = am−1,0x0 + am−1,1x1 + · · · + am−1,n−1xn−1
(2.7)
The problem we are trying to solve is, Given a0,0, . . . , am−1,n−1 and
b0, . . . , bm−1, what are the values of x0, . . . , xn−1? For a given linear system,
the set of all possible solutions is called the solution set.
As an example, the system of equations
x0 + 2x1 = 1
3x0 −x1 = 2
has the solution set {x0 = 5/7, x1 = 1/7}.

2.4 Systems of Linear Equations 89
There may not be a single solution to the linear system. For example, the
plane equation
ax + by + cz = −d
has an inﬁnite number of solutions: the solution set for this example is all the
points on the particular plane.
Alternatively, it may not be possible to ﬁnd any solution to the linear
system. Suppose that we have the linear system
x0 + x1 = 1
x0 + x1 = 2
There are clearly no solutions for x and y. The solution set is the empty set.
Let’s reexamine equation 2.7. If we think of (x0, . . . , xn−1) as elements of
an n-dimensional vector x and (b0, . . . , bm−1) as elements of an m-dimensional
vector b, then this starts to look a lot like matrix multiplication. We can rewrite
this as


a0,0
a0,1
· · ·
a0,n−1
a1,0
a1,1
· · ·
a1,n−1
...
...
...
...
am−1,0
am−1,1
· · ·
am−1,n−1




x0
x1
...
xn−1

=


b0
b1
...
bm−1


Or our old friend
Ax = b
The coefﬁcients of the equation become the elements of matrix A, and
matrix multiplication encapsulates our entire linear system. Now the problem
becomes one of the form: Given A and b, what is x?
2.4.2 Solving Linear Systems
One case is very easy to solve. Suppose A looks like


1
a0,1
· · ·
a0,n−1
0
1
· · ·
a1,n−1
...
...
...
...
0
0
· · ·
1



90
Chapter 2 Linear Transformationsand Matrices
This is equivalent to the linear system
b0 = x0 + a0,1x1 + · · · + a0,n−1xn−1
b1 = x1 + · · · + a1,n−1xn−1
...
...
bm−1 = xn−1
We see that we immediately have the solution to one unknown via xn−1 = bm−1.
We can substitute this value into the previous m −1 equations and possibly
solve for another xi. If so, we can substitute that xi into the remaining unsolved
equations and so on up the chain. If there is a single solution for the system of
equations, we will ﬁnd it; otherwise, we will solve as many terms as possible
and derive a solution set for the remainder.
This matrix is said to be in row echelon form. The formal deﬁnition for
row echelon form is
1. If a row is entirely zeros, it will be below any nonzero rows of the
matrix; in other words, all zero rows will be at the bottom of the matrix.
2. The ﬁrst nonzero element of a row (if any) will be 1 (called a leading 1).
3. Each leading 1 will be to the right of a leading 1 in any preceding row.
If the following additional condition is met, we say that the matrix is in reduced
row echelon form.
4. Each column with a leading 1 will be zero in the other rows.
The process we’ve described gives us a clue about how to proceed in solv-
ing general systems of linear equations. Suppose we can multiply both sides
of our equation by a series of matrices so that the left-hand side becomes a
matrix in row echelon form. Then we can use this in combination with the
right-hand side to give us the solution for our system of equations.
However, we need to use matrices that preserve the properties of the linear
system; the solution set for both systems of equations must remain equal. This
restricts us to those matrices that perform one of three transformations called
elementary row operations. These are
1. Multiply a row by a nonzero scalar.
2. Add a nonzero multiple of one row to another.
3. Swap two rows.

2.4 Systems of Linear Equations 91
These three types of transformations maintain the solution set of the linear
system while allowing us to reduce it to a simpler problem. The matrices that
perform elementary row operations are called elementary matrices.
Some simple examples of elementary matrices include
one which multiplies row 2 by a scalar a:


1
0
0
0
a
0
0
0
1


one which adds k times row 2 to row 1:


1
k
0
0
1
0
0
0
1


and one that swaps rows 2 and 3:


1
0
0
0
0
1
0
1
0


2.4.3 Gaussian Elimination
Library
IvMath
Filename
IvGaussianElim
In practice we don’t solve linear systems through matrix multiplication.
Instead, it is more efﬁcient to iteratively perform the operations directly on A
and b. The most basic method for solving linear systems is known as Gaussian
elimination, after Karl Friedrich Gauss, a proliﬁc German mathematician of
the eighteenth and nineteenth centuries. It involves concatenating the matrix
A and vector b into a form called an augmented matrix and then performing
a series of elementary row operations on the augmented matrix, in a particu-
lar order. This will either give us a solution to the system of linear equations
or tell us that computing a single solution is not possible: either there is no
solution or an inﬁnite number of solutions.
To create the augmented matrix, we take the original matrix A and
combine it with our constant vector b, for example,


1
2
3
4
5
6
7
8
9

3
2
1


The vertical line within the matrix indicates the separation between A and b.
To this augmented matrix, we will directly apply one or more of our row
operations.

92
Chapter 2 Linear Transformationsand Matrices
The process begins by looking at the ﬁrst element in the ﬁrst row. The
ﬁrst step is called a pivoting step. At the very least we need to ensure that we
have a nonzero entry in the diagonal position, so if necessary we will swap
this row with one of the lower rows with a nonzero entry in the same column.
The element that we’re swapping into place is called the pivot element, and
swapping two rows to move the pivot element into place is known as partial
pivoting. For better numerical precision, we usually go one step further and
swap with the row that contains the element of largest absolute value. If no
pivot element can be found, then there is no single solution and we abort.
Now let’s say that the current pivot element value is k. We scale the entry
row by 1/k to set the diagonal entry to 1. Finally, we set the column elements
below the diagonal entry to zero by adding appropriate multiples of the cur-
rent row. Then we move on to the next row and look at its diagonal entry.
At the end of this process, our matrix will be in row echelon form.
Let’s take a look at an example. Suppose we have the following system of
linear equations:
x
−3y
+
z
= 5
2x
−y
+
2z
= 5
3x
+6y
+
9z
= 3
The equivalent augmented matrix is


1
−3
1
2
−1
2
3
6
9

5
5
3


If we look at column 0, the maximal entry is 3, in row 2. So we begin by
swapping row 2 with row 0:


3
6
9
2
−1
2
1
−3
1

3
5
5


We scale the new row 0 by 1/3 to set the pivot element to 1:


1
2
3
2
−1
2
1
−3
1

1
5
5


Now we start clearing the lower entries. The ﬁrst entry in row 1 is 2, so we
scale row 0 by −2 and add it to row 1:


1
2
3
0
−5
−4
1
−3
1

1
3
5



2.4 Systems of Linear Equations 93
We do the same for row 2, scaling by −1 and adding:


1
2
3
0
−5
−4
0
−5
−2

1
3
4


We are done with row 0 and move on to row 1. Row 1, column 1 is the
maximal entry in the column, so we don’t need to swap rows. However, it isn’t
1, so we need to scale row 1 by −1/5:


1
2
3
0
1
4/5
0
−5
−2

1
−3/5
4


We now need to clear element 1 of row 2 by scaling row 1 by 5 and adding:


1
2
3
0
1
4/5
0
0
2

1
−3/5
1


Finally we scale the bottom row by 1/2 to set the pivot element in the
row to 1:


1
2
3
0
1
4/5
0
0
1

1
−3/5
1/2


This matrix is now in row-echelon form. We have two possibilities at this
point. We could clear the upper triangle of the matrix in a fashion similar to
how we cleared the lower triangle, but by working up from the bottom and
adding multiples of rows. The solution x to the linear system would end up in
the right-hand column. This is known as Gauss-Jordan elimination.
But let’s look at the linear system we have now:
x + 2y + 3z = 1
y + 4/5z = −3/5
z = 1/2
As expected, we already have a known quantity: z. If we plug z into the second
equation, we can solve for y:
y = −3/5 −4/5z
(2.8)
= −3/5 −4/5(1/2)
(2.9)
= −1
(2.10)

94
Chapter 2 Linear Transformationsand Matrices
Once y is known, we can solve for x:
x = 1 −2y −3z
(2.11)
= 1 −2(−1) −3(1/2)
(2.12)
= 3/2
(2.13)
So our ﬁnal solution for x is (3/2, −1, 1/2).
This process of substituting known quantities into our equations is called
back substitution.
A summary of Gaussian elimination with back substitution follows:
for p = 1 to n do
// find the element with largest absolute value in col p
// if max is zero, stop!
// if max element not in row p, swap rows
// set pivot element to 1
multiply row p by 1/A[p][p]
// clear lower column entries
for r = p+1 to n do
subtract row p times A[r,p] from current row,
so that element in pivot column becomes 0
// do backwards substitution
for row = n-1 to 1
for col = row+1 to n
// subtract out known quantities
b[row] = b[row] - A[row][col]*b[col]
The pseudocode shows what may happen when we encounter a linear
system with no single solution. If we can’t swap a nonzero entry in the pivot
location, then there is a column that is all zeros. This is only possible if the
rank of the matrix (i.e., the number of linearly independent column vectors)
is less than the number of unknowns. In this case there is no solution to the
linear system and we abort.
In general, we can state that if the rank of the coefﬁcient matrix A equals
the rank of the augmented matrix A|b, then there will be at least one solution
to the linear system. If the two ranks are unequal, then there are no solutions.
There is a single solution only if the rank of A is equal to the minimum of the
number of rows or columns of A.

2.5 Matrix Inverse 95
2.5 Matrix Inverse
This may seem like a lot of trouble to go to solve a simple equation like b = Ax.
If this were scalar math, we could simply divide both sides of the equation by
A to get
x = b/A
Unfortunately, matrices don’t have a division operation. However, we can use
an equivalent concept: the inverse.
2.5.1 Definition
In scalar multiplication, the inverse is deﬁned as the reciprocal:
x · 1
x = 1
or
x · x−1 = 1
Correspondingly, for a given matrixA, we can deﬁne its inverseA−1 as a matrix
such that
A · A−1 = I
and
A−1 · A = I
There are a few things that fall out from this deﬁnition. First of all, in order
for the ﬁrst multiplication to occur, the number of rows in the inverse must
be the same as the number of columns in the original matrix. For the second
to occur, the converse is true. So the matrix and its inverse must be square
and the same size. Since not all matrices are square, it’s clear that not every
matrix has an inverse.
Second, the inverse of the inverse returns the original matrix. Given
A−1 · (A−1)−1 = I
and
A−1 · A = I

96
Chapter 2 Linear Transformationsand Matrices
then
(A−1)−1 = A
Even if a matrix is square, there isn’t always an inverse. An extreme example
is the zero matrix. Any matrix multiplied by this gives the zero matrix, so
there is no matrix multiplication that will produce the identity. Another set of
examples is matrices that have a zero row or column vector. Multiplying by
such a row or column will return a dot product of zero, so you’ll end up with
a zero row or column vector in the product as well — again, not the identity
matrix. In general, if the null space of the matrix is nonzero, then the matrix
is non-invertible; that is, the matrix is only invertible if the rank of the matrix
is equal to the number of rows and columns.
Given these identities, we can now solve for our preceding linear system.
Recall that the equation was
Ax = b
If we multiply both sides by A−1, then
A−1Ax = A−1b
Ix = A−1b
x = A−1b
Therefore, if we could ﬁnd the inverse of A, we could use it to solve for x. This
is not usually a good idea, computationally speaking. It’s usually cheaper to
solve for x directly, rather than generating the inverse and then performing the
matrix multiplication. The latter can also lead to increased numerical error.
However, sometimes ﬁnding the inverse is a necessary evil.
The left-hand side of the above derivation shows us that we can think of
the inverse A−1 as undoing the effect of A. If we start with Ax and premultiply
by A−1, we get back x, our original vector.
We can ﬁnd the inverse of a matrix using Gaussian elimination to solve
for it column by column. Suppose we call the ﬁrst column of A−1 x0. We can
represent this as
x0 = A−1e0
where, as we recall, e0 = (1, 0, . . . , 0). Multiplying both sides by A gives
Ax0 = e0
Finding the solution to this linear system gives us the ﬁrst column of A−1. We
can do the same for the other columns, but using e1, e2, and so on. Instead of

2.5 Matrix Inverse 97
solving these one at a time, though, it is more efﬁcient to create an augmented
matrix withA and e0, . . . , en−1 as columns on the right — or just I. For example,


2
0
4
0
3
−9
0
0
1

1
0
0
0
1
0
0
0
1


If we use Gauss-Jordan elimination to turn the left-hand side of the augmented
matrix into the identity matrix, then we will end up with the inverse (if any) on
the right-hand side. So from here we perform our elementary row operations
as before. The maximal entry is already in the pivot point, so we scale the ﬁrst
row by 1/2:


1
0
2
0
3
−9
0
0
1

1/2
0
0
0
1
0
0
0
1


The nonpivot entries in the ﬁrst column are zero, so we move to the second
column. Scaling the second row by 1/3 to set the pivot point to 1 gives us


1
0
2
0
1
−3
0
0
1

1/2
0
0
0
1/3
0
0
0
1


Again, our nonpivot entries in the second column are 0, so we move to the
third column. Our pivot entry is 1, so we don’t need to scale. We add −2 times
the last row to the ﬁrst row to clear that entry, then 3 times the last row to the
second row to clear that entry, and get


1
0
0
0
1
0
0
0
1

1/2
0
−2
0
1/3
3
0
0
1


The inverse of our original matrix is now on the right-hand side of the
augmented matrix.
2.5.2 Simple Inverses
Gaussian elimination, while useful, is unnecessary for computing the inverse
of many of the matrices we will be using. The majority of matrices that we will
encounter in games and 3D applications have simple inverses, and knowing
the form of the matrix can make computing the inverse trivial.

98
Chapter 2 Linear Transformationsand Matrices
One case is that of an orthogonal matrix, where the component row or
column vectors are orthonormal. Recall that this means that the vectors are
of unit length and perpendicular. If a matrix A is orthogonal, its inverse is the
transpose:
A−1 = AT
One example of an orthogonal matrix is


0
0
1
1
0
0
0
1
0


−1
=


0
1
0
0
0
1
1
0
0


Another simple case is a diagonal matrix with nonzero elements in the
diagonal. The inverse of such a matrix is also diagonal, where the new diagonal
elements are the reciprocal of the original diagonal elements, as shown by the
following:


a
0
0
0
b
0
0
0
c


−1
=


1/a
0
0
0
1/b
0
0
0
1/c


The third case is a modiﬁed identity matrix, where the diagonal is all ones
but one column or row is nonzero. One such 3×3 matrix is


1
0
x
0
1
y
0
0
1


For a matrix of this form, we simply negate the non-zero elements to invert
it. Using the previous example:


1
0
x
0
1
y
0
0
1


−1
=


1
0
−x
0
1
−y
0
0
1


Finally, we can combine this knowledge to take advantage of an algebraic
property of matrices. If we have two square matrices A and B, both of which
are invertible, then
(AB)−1 = B−1A−1
So if we know that our current matrix is the product of any of the cases we’ve
just discussed, we can easily compute its inverse using the preceding formula.
This will prove to be useful in subsequent chapters.

2.6 The Determinant 99
2.6 The Determinant
2.6.1 Definition
The determinant is a scalar quantity created by evaluating the elements of
a square matrix. In real vector spaces, it acts as a general measure of how
vectors transformed by the matrix change in size. For example, if we take the
columns of a 2 × 2 matrix (i.e., the transformed basis vectors) and use them
as the sides of a parallelogram (Figure 2.2), then the absolute value of the
determinant is equal to the area of a parallelogram. For a 3 × 3 matrix, the
absolute value of the determinant is equal to the volume of a parallelpiped
described by the three transformed basis vectors (Figure 2.3).
The sign of the determinant depends on whether or not we have switched
our ordered basis vectors from being relatively right-handed to being left-
handed. In Figure 2.2, the shortest angle from a0 to a1 is clockwise, so they
are left-handed. The determinant, therefore, is negative.
We represent the determinant in one of two ways, either det(A) or |A|. The
second is often used when showing the elements of a matrix:
det(A) =

1
−3
1
2
−1
2
3
6
9

The diagrams showing area of a parallelogram and volume of a paral-
lelpiped should look familiar from our discussion of cross product and triple
scalar product. In fact, the cross product is sometimes represented as
v × w =

i
j
k
vx
vy
vz
wx
wy
wz

j
i
a0
a1
Figure 2.2 Determinant of 2 × 2 matrix as area of parallelogram bounded by
transformed basis vectors a0 and a1.

100
Chapter 2 Linear Transformationsand Matrices
j
i
a0
a1
a2
k
Figure 2.3 Determinant of 3 × 3 matrix as volume of parallelopiped bounded by
transformed basis vectors a0, a1, and a2.
while the triple product is represented as
u · (v × w) =

ux
uy
uz
vx
vy
vz
wx
wy
wz

Since det(AT ) = det(A), this representation is equivalent.
2.6.2 Computing the Determinant
There are a few ways of representing the determinant computation for a
speciﬁc matrix A. A standard recursive deﬁnition, choosing any row i, is
det(A) =
n

j=1
ai,j(−1)(i+j) det( ˜Ai,j)
Alternatively, we can expand by column j instead:
det(A) =
n

i=1
ai,j(−1)(i+j) det( ˜Ai,j)

2.6 The Determinant 101
In both cases, ˜Ai,j is the submatrix formed by removing the ith row and jth
column from A. The base case is the determinant of a matrix with a single
element, which is the element itself.
The term det( ˜Ai,j) is also referred to as the minor of entry ai,j, and the term
(−1)(i+j) det( ˜Ai,j) is called the cofactor of entry ai,j.
The ﬁrst formula tells us: for a given row i, we multiply each row entry ai,j
by the determinant of the submatrix formed by removing row i and column
j and either add or subtract it to the total depending on its position in the
matrix. The second does the same but moves along column j instead of row i.
Let’s compute an example determinant, expanding by row 0:
det




1
1
2
2
4
−3
3
6
−5



=?
The ﬁrst element of row 0 is 1, and the submatrix with row 0 and column 0
removed is
 4
−3
6
−5

The second element is also 1. However, we negate it since we are considering
row 0 and column 1: 0 + 1 = 1, which is odd. The submatrix is A with row 0
and column 1 removed:
 2
−3
3
−5

The third element of the row is 2, with the submatrix
 2
4
3
6

We don’t negate since we are considering row 0 and column 2: 0 + 2 = 2,
which is even.
So the determinant is
det(A) = 1 ·

4
−3
6
−5
 −1 ·

2
−3
3
−5
 + 2 ·

2
4
3
6

= −1
In general, the determinant of a 2 × 2 matrix is
det
 a
b
c
d

= a · det([d]) −b · det([c]) = ad −bc

102
Chapter 2 Linear Transformationsand Matrices
And the determinant of a 3 × 3 matrix is
det




a
b
c
d
e
f
g
h
i



= a · det
 e
f
h
i

−b · det
 d
f
g
i

+ c · det
 d
e
g
h

or
a(ei −f h) −b(di −fg) + c(dh −eg)
There are some additional properties of the determinant that will be useful
to us. If we have two n × n matrices A and B, the following hold:
1. det(AB) = det(A)det(B)
2. det(A−1) =
1
det(A)
We can look at the value of the determinant to tell us some features of
our matrix. First of all, as we have mentioned, any matrix that transforms our
basis vectors from right-handed to left-handed will have a negative determi-
nant. If the matrix is also orthogonal, we call a matrix of this type a reﬂection.
We will learn more about reﬂection matrices in the next chapter.
Then there are matrices that have a determinant of 1. The matrices we
will encounter most often with this property are orthogonal matrices, where
the handedness of the resulting basis stays the same (i.e., a right-handed basis
is transformed to a right-handed basis). Figure 2.4 provides an example. Our
transformed basis vectors are (−
√
2/2,
√
2/2) and (
√
2/2,
√
2/2). They remain
orthonormal, so their area is just the product of the lengths of the two vectors,
or 1 × 1 or 1. This type of matrix is called a rotation. As with reﬂections, we’ll
see more of rotations in the next chapter.
Finally, if the determinant is 0, then we know that the matrix has no
inverse. The obvious case is if the matrix has a row or column of all 0s. Look
again at our formula for the determinant. Suppose row i is all 0s. Multiplying
all the submatrices against this row and summing together will clearly give
us 0 as a result. The same is true for a zero column. The other and related
possibility is that we have a linearly dependent row or column vector. In both
cases the rank of the matrix is less than n — the size of the matrix — and
therefore the matrix does not have an inverse. So if the determinant of a
matrix is 0, we know the matrix is not invertible.

2.6 The Determinant 103
j
i
a0
a1
Figure 2.4 Determinant of example 2 × 2 orthogonal matrix.
2.6.3 Determinants and Elementary Row
Operations
Library
IvMath
Filename
IvGaussianElim
For 2 × 2 and 3 × 3 matrices, computing the determinant in this manner
is a simple process. However, for larger and larger matrices, our recursive
deﬁnition becomes unwieldy, and for large enough n will take an unreasonable
amount of time to compute. In addition, computing the determinant in this
manner can lead to ﬂoating point precision problems. Fortunately, there is
another way.
Suppose we have an upper triangular matrix U. The ﬁrst part of the deter-
minant sum is u0,0 ˜U0,0. The other terms, however, are 0, because the ﬁrst
column with the ﬁrst row removed is all 0s. So the determinant is just
det(U) = u0,0 ˜U0,0
If we expand the recursion, we ﬁnd that the determinant is the product of all
the diagonal elements, or
det(U) = u0,0u1,1 . . . unn
As we did when solving linear systems, we can use Gaussian elimination
to change our matrix into row echelon form, which is an upper triangular
matrix. However, this assumes that elementary row operations have no effect
on the determinant, which is not the case. Let’s look at a few examples.

104
Chapter 2 Linear Transformationsand Matrices
Suppose we have the matrix

2
−4
−1
1

The determinant of this matrix is −2. If we multiply the ﬁrst row by 1/2, we get

1
−2
−1
1

which has a determinant of −1. Multiplying a row by a scalar k multiplies the
determinant by k as well.
Now suppose we add two times the ﬁrst row to the second one. We get
 1
−2
1
−3

which also has a determinant of −1. Adding a multiple of one row to another
has no effect on the determinant.
Finally we can swap row 1 with row 2:
 1
−3
1
−2

which has a determinant of 1. Swapping two rows or two columns changes
the sign of the determinant.
The effect of elementary row operations on the determinant can be
summarized as follows:
Multiply row by k:
Multiplies determinant by k
Add multiple of one row to another:
No effect
Swap rows:
Changes sign of determinant
So our approach for calculating the determinant for a general matrix is
this: as we perform Gaussian elimination, we keep a running product p of
any multiplies we do to create leading 1s and negate p for every row swap.
If we ﬁnd a zero column when we look for a pivot element, we know the
determinant is 0 and return such.
Let’s suppose our ﬁnal product is p. This represents what we’ve multiplied
the determinant of our original matrix by to get the determinant of the ﬁnal
matrix A′, or
p · det(A) = det(A′)

2.6 The Determinant 105
so
det(A) = 1
p · det(A′)
We know that the determinant of A′ is 1, since the diagonal of the row echelon
matrix is all 1s. So our ﬁnal determinant is just 1/p. However, this is just the
product of the multiplies we do to create leading 1s, and −1 for every row
swap, or
p =
1
p0,0
1
p1,1
. . .
1
pn,n
(−1)k
where k is the number of row swaps. Then
1/p = p0,0p1,1 . . . pn,n(−1)k
So all we need to do is multiply our running product by each pivot element
and negate for each row swap. At the end of our Gaussian elimination process,
our running product will be the determinant we seek.
2.6.4 Adjoint Matrix and Inverse
Library
IvMath
Filename
IvMatrix33
Recall that the cofactor of an entry ai,j is
Ci,j = (−1)(i+j) det( ˜Ai,j)
For an n×n matrix, we can construct a corresponding matrix where we replace
each element with its corresponding cofactor, or


C0,0
C0,1
· · ·
C0,n−1
C1,0
C1,1
· · ·
C1,n−1
...
...
...
...
Cn1
Cn2
· · ·
Cnn


This is called the matrix of cofactors from A, and its transpose is the adjoint
matrix Aadj.
Gabriel Cramer, a Swiss mathematician, showed that the inverse of a
matrix can be computed from the adjoint by
A−1 =
1
det(A)Aadj

106
Chapter 2 Linear Transformationsand Matrices
Many graphics engines use Cramer’s method to compute the inverse, and
for 3 × 3 and 4 × 4 matrices it’s not a bad choice; for matrices of this size
Cramer’s method is actually faster than Gaussian elimination. Because of
this, we have chosen to implement IvMatrix33::Inverse() using an efﬁcient
form of Cramer’s method.
However, whether you’re using Gaussian elimination or Cramer’s method,
you’re probably doing more work than is necessary for the matrices we will
encounter. Most will be in one of the formats described in Section 2.5.2 or a
multiple of these matrix types. Using the process described in that section, you
can compute the inverse by decomposing the matrix into a set of these types,
inverting the simple matrices, and multiplying in reverse order to compute
the matrix. This is often faster than either Gaussian elimination or Cramer’s
method and can be more tolerant of ﬂoating point errors because you can ﬁnd
near-exact solutions for the simple matrices.
2.7 Chapter Summary
In this chapter, we’ve discussed the general properties of linear transforma-
tions and how they are represented and performed by matrices. Matrices
can also be used to compute solutions to linear systems of equations by
using either Gaussian elimination or similar methods. We covered some
basic matrix properties, the concepts of matrix identity and inverse (and var-
ious methods for calculating the latter), and the meaning and calculation
of the determinant. This lays the foundation for what we’ll be discussing in
the next chapter: using matrix transformations to manipulate models in a
three-dimensional world.
For those who are interested in reading further, Anton and Rorres [3]
is a standard reference for many ﬁrst courses in linear algebra. Other texts
with slightly different approaches include Axler [7] and Friedberg [37]. More
information on Gaussian elimination and its extensions such as LU decom-
position can be found in Anton and Rorres [3], as well as in the Numerical
Recipes series [92]. Finally, Blinn has an excellent article in his collection
Notation, Notation, Notation [13] on the geometry underlying 2 × 2 matrix
operations.

Chapter3
Affine
Transformations
3.1 Introduction
Now that we’ve chosen a mathematically sound basis for representing geom-
etry in our game and discussed some aspects of matrix arithmetic, we need
to combine them into an efﬁcient method for placing and moving virtual
objects or models. There are a few reasons we seek this efﬁciency. Suppose
we wish to build a core level in our game space, say the ofﬁce of a computer
company. We could build all of our geometry in place and hard-code all of
the locations. However, if we have a number of objects that are duplicated
throughout the space — computers, desks, and chairs for example — it would
be more memory-efﬁcient to create one master copy of the geometry for each
type of object. Then, for each instance of a particular object, we can specify
just a position and orientation and let the rendering and simulation engine
handle the placement.
Another, more obvious reason is that objects in games generally move so
that setting them at a ﬁxed location is not practical. We will need to have
some means to specify, for a model as a whole, its position and orientation in
space.
There are a few characteristics we desire in our method. We want it to be
fast and work well with our existing data and math library. We want to be able
to concatenate a series of operations so we can perform them with a single
operation, just as we did with linear transformations. Since our objects consist
of collections of points, we need our method to work on points in an afﬁne
107

108
Chapter 3 Affine Transformations
space, but we’ll still need to transform vectors as well. The speciﬁc method
we will use is called an afﬁne transformation.
3.2 Affine Transformations
3.2.1 Definition
In the last chapter, we discussed linear transformations, which map from one
vector space to another. We can apply such transformations to vectors using
matrix operations. There is a nearly equivalent set of transformations that
map between afﬁne spaces, which we can apply to points and vectors in an
afﬁne space. These are known as afﬁne transformations and they too can be
applied using matrix operations, albeit in a slightly different form.
Recall that linear transformations preserve the linear operations of vec-
tor addition and scalar multiplication. In other words, linear transformations
map from one vector space to another and preserve linear combinations.
Thus, for a given linear transformation S:
S(a0v0 + a1v1 + · · · + an−1vn−1) = a1S(v0) + a1S(v1) + · · · + an−1S(vn−1)
Correspondingly, an afﬁne transformation T maps between two afﬁne spaces
A and B and preserves afﬁne combinations. For scalars a0, . . . , an−1 and points
P0, . . . , Pn−1 in A:
T(a0P0 + · · · + an−1Pn−1) = a0T(P0) + · · · + an−1T(Pn−1)
where a0 + · · · + an−1 = 1.
As with our test for linear transformations, to determine whether a given
transformation T is an afﬁne transformation, it is sufﬁcient to test a single
afﬁne combination:
T(a0P0 + a1P1) = a0T(P0) + a1T(P1)
where a0 + a1 = 1.
Afﬁne transformations are particularly useful to us because they preserve
certain properties of geometry. First, they maintain collinearity, so points on
a line will remain collinear and points on a plane will remain coplanar when
transformed.

3.2 Affine Transformations 109
If we transform a line:
L(t) = (1 −t)P0 + tP1
T(L(t)) = T((1 −t)P0 + tP1)
= (1 −t)T(P0) + tT(P1)
The result is clearly still a line (assuming T(P0) and T(P1) aren’t coincident).
Similarly, if we transform a plane:
P(t) = (1 −s −t)P0 + sP1 + tP2
T(P(t)) = T((1 −s −t)P0 + sP1 + tP2)
= (1 −s −t)T(P0) + sT(P1) + tT(P2)
The result is clearly a plane (assuming T(P0), T(P1), and T(P2) aren’t collinear).
The second property of afﬁne transformations is that they preserve relative
proportions. The point that lies at t distance between P0 and P1 on the original
line will map to the point that lies at t distance between T(P0) and T(P1) on
the transformed line.
Note that while ratios of distances remain constant, angles and exact
distances don’t necessarily stay the same. The speciﬁc subset of afﬁne trans-
formations that preserve these features are called rigid transformations; those
that don’t are called deformations. It should be no surprise that we ﬁnd rigid
transformations useful. When transforming our models, in most cases we
don’t want them distorted unrecognizably. A bottle should maintain its size
and shape — it should look like a bottle no matter where we place it in space.
However, the deformations have their use as well. On occasion we may want
to make an object larger or smaller or reﬂect it across a plane, as in a mirror.
To apply an afﬁne transformation to a vector in an afﬁne space, we can
apply it to the difference of two points that equal the vector, or
T(v) = T(P −Q) = T(P) −T(Q)
As we will see, an afﬁne transformation that is applied to a vector performs
a linear transformation.
3.2.2 Representation
Suppose we have an afﬁne transformation that maps from afﬁne spaces A and
B, where the frame for A has basis vectors (v0, . . . , vn−1) and origin OA, and

110
Chapter 3 Affine Transformations
the frame for B has basis vectors (w0, . . . , wm−1) and origin OB. If we apply an
afﬁne transformation to a point P = (x0, . . . , xn−1) in A, this gives
T(P) = T(x0v0 + · · · + xn−1vn−1 + OA)
= x0T(v0) + · · · + xn−1T(vn−1) + T(OA)
As we did with linear transformations, we can express a given T(v) in terms
of B’s frame:
T(vj) = a0,jw0 + a1,jw1 + · · · + am−1,jwm−1
Similarly, we can express T(OA) in terms of B’s frame:
T(OA) = y0w0 + y1w1 + · · · + ym−1wm−1 + OB
Again, as we did with linear transformations, we can rewrite this as a matrix
product. However, unlike linear transformations, we write a mapping from
an n-dimensional afﬁne space to an m-dimensional afﬁne space as an (m+1)×
(n + 1) matrix:


a0,0w0
a0,1w0
· · ·
a0,n−1w0
y0w0
a1,0w1
a1,1w1
· · ·
a1,n−1w1
y1w1
...
...
...
...
...
am−1,0wm−1
am−1,1wm−1
· · ·
am−1,n−1wm−1
ym−1wm−1
0
0
· · ·
0
OB




x0
x1
...
xn−1
1


The n + 1 columns represent the n transformed basis vectors plus the trans-
formed origin. We need m + 1 rows since the frame of B has m basis vectors
plus the origin OB. As we can see, in order to allow the multiplication to
proceed, we’ll represent our point with a trailing “1” component.
We can pull out the frame terms to get
w0
w1
· · ·
wm−1
OB



a0,0
a0,1
· · ·
a0,n−1
y0
a1,0
a1,1
· · ·
a1,n−1
y1
...
...
...
...
...
am−1,0
am−1,1
· · ·
am−1,n−1
ym−1
0
0
· · ·
0
1




x0
x1
...
xn−1
1


So, similar to linear transformations, if we know how the afﬁne transfor-
mation affects the frame for A, we can copy the transformed frame in terms
of the frame for B into the columns of a matrix and use matrix multiplication

3.2 Affine Transformations 111
to apply the afﬁne transformation to an arbitrary point. We can represent this
process of transformation using block matrices:
T(P) =
 A
y
0T
1
  x
1

=
 Ax + y
1

(3.1)
For the purposes of computation, the vector 0T , the 1 in the lower right-hand
corner of the matrix, and the trailing 1s in the points are unnecessary. They
take up memory and using the full matrix takes additional instructions to
multiply by constant values. Because of this, an afﬁne transformation matrix
is sometimes represented in a form where these constant terms are implied.
This form is often either an m × (n + 1) matrix or, simpler still, a matrix
multiplication plus a vector add:
Ax + y
where x consists of the point coordinates (x0, . . . , xn−1) without the trailing 1.
The matrix A is an m×n matrix, and we need at least n+1 columns in a matrix
if we’re going to multiply it by an n-dimensional point, so the multiplication
AP is not considered mathematically legal in this case.
If we subtract two points in an afﬁne space, we get a vector:
v = P0 −P1
=
 x0
1

−
 x1
1

=
 x0 −x1
0

As we can see, a vector is represented in an afﬁne space with a trailing 0.
As previously noted in Chapter 1, this provides justiﬁcation for some math
libraries to use the trailing 1 on points and trailing 0 on vectors. If we multiply
a vector using this representation by our (m + 1) × (n + 1) matrix, expanding
terms:


a0,0
· · ·
a0,n−1
y0
a1,0
· · ·
a1,n−1
y1
...
...
...
...
am−1,0
· · ·
am−1,n−1
ym−1
0
· · ·
0
1




v0
v1
...
vn−1
0


=


a0,0v0 + · · · + a0,n−1vn−1
a1,0v0 + · · · + a1,n−1vn−1
...
am−1,0v0 + · · · + am−1,n−1vn−1
0


we see that the vector is affected by the upper left m × n matrix A, but not
the vector y. This has the same effect on the ﬁrst n elements of v as multiply-
ing an n-dimensional vector by A, which is a linear transformation. So this

112
Chapter 3 Affine Transformations
representation allows us to use afﬁne transformation matrices to apply linear
transformations on vectors in an afﬁne space.
Suppose we wish to concatenate two afﬁne transformations S and T,
where the matrix representing S is
 A
y
0T
1

and the matrix representing T is
 B
z
0T
1

As with linear transformations, to ﬁnd the matrix that represents the compo-
sition of S and T, we multiply the matrices together. This gives
 A
y
0T
1
  B
z
0T
1

=
 AB
Az + y
0T
1

(3.2)
Finding the inverse for an afﬁne transformation is equally as straight-
forward; again, we can use a process similar to the one we used with linear
transformation matrices. Starting with
 A
y
0T
1
  A
y
0T
1
−1
=

I
0
0T
1

we multiply by both sides to remove the y component from the left-most
matrix:

I
−y
0T
1
  A
y
0T
1
  A
y
0T
1
−1
=

I
−y
0T
1
 
I
0
0T
1

 A
0
0T
1
  A
y
0T
1
−1
=

I
−y
0T
1

We then multiply by both sides to change the left-most matrix to the
identity:
 A−1
0
0T
1
  A
0
0T
1
  A
y
0T
1
−1
=
 A−1
0
0T
1
 
I
−y
0T
1

 A
y
0T
1
−1
=
 A−1
−A−1y
0T
1

(3.3)
thereby giving us the inverse on the right-hand side.

3.3 Standard Affine Transformations 113
When we’re working in R3, A will be a 3×3 matrix and y will be a 3-vector;
hence the full afﬁne matrix will be a 4 × 4 matrix. Most graphics libraries
expect transformations to be in the 4 × 4 matrix form, so if we do use the
more compact forms in our math library to save memory, we will still have
to expand them before rendering our objects. Because of this, we will use
the 4 × 4 form for our following discussions, with the understanding that
in our ultimate implementation we may choose one of the other forms for
efﬁciency’s sake.
3.3 Standard Affine Transformations
Now that we’ve deﬁned afﬁne transformations in general, we can discuss
some speciﬁc afﬁne transformations that will prove useful when manipulat-
ing objects in our game. We’ll cover these in terms of transformations from
R3 to R3, since they will be the most common uses. However, we can apply
similar principles to ﬁnd transformations from R2 to R2 or even R4 to R4 if we
desire.
Since afﬁne space A and B are the same in this case, to simplify things we’ll
use the same frame for each one: the standard Cartesian frame of (i, j, k, O).
3.3.1 Translation
The most basic afﬁne transformation is translation. For a single point, it’s the
same as adding a vector t to it, and when applied to an entire set of points it
has the effect of moving them rigidly through space (Figure 3.1). Since all the
points are shifted equally in space, the size and shape of the object will not
change, so this is a rigid transformation.
We can determine the matrix for a translation by computing the transfor-
mation for each of the frame elements. For the origin O, this is
T(O) = t + O
= txi + tyj + tzk + O
For a given basis vector, we can ﬁnd two points P and Q that deﬁne the vector
and compute the transformation of their difference. For example, for i:
T(i) = T(P −Q)
= T(P) −T(Q)
= (t + P) −(t + Q)
= P −Q
= i

114
Chapter 3 Affine Transformations
z
x
y
Figure 3.1 Translation.
The same holds true for j and k, so translation has no effect on the basis
vectors in our frame. We end up with a 4 × 4 matrix:


1
0
0
tx
0
1
0
ty
0
0
1
tz
0
0
0
1


Or, in block form:
Tt =

I
t
0T
1

Translation only affects points. To see why, suppose we have a vector v,
which equals the displacement between two points P and Q, that is, v = P −Q.
If we translate P −Q, we get
trans(P −Q) = (P + t) −(Q + t)
= (P −Q) + (t −t)
= v
This ﬁts with our geometric notion that points have position and hence can
be translated in space, while vectors do not and cannot.

3.3 Standard Affine Transformations 115
We
can
use
equation
3.3
to
compute
the
inverse
translation
transformation:
T−1
t
=
 I−1
−I−1t
0T
1

(3.4)
=

I
−t
0T
1

(3.5)
= T−t
(3.6)
So the inverse of a given translation negates the original translation vector
to displace the point back to its original position.
3.3.2 Rotation
The other common rigid transformation is rotation. If we consider the rota-
tion of a vector, we are rigidly changing its direction around an axis without
changing its length. In R2, this is the same as replacing a vector with the one
that’s θ degrees counterclockwise (Figure 3.2).
In R3, we usually talk about an axis of rotation. In his rotation theorem,
Euler showed that when applying a rotation in three-dimensional space, there
is a linear set of points (i.e., a line) which does not change. This is called the
axis of rotation, and the amount we rotate around this axis is the angle of
rotation. A helpful mnemonic is the right-hand rule: if you point your right
thumb in the direction of the axis vector, the curl of your ﬁngers represents
the direction of positive rotation (Figure 3.3).
x
y
h
v'
v
Figure 3.2 Rotation of vector in R2.

116
Chapter 3 Affine Transformations
Figure 3.3 Axis and plane of rotation.
x
y
h
P'
P
Figure 3.4 Rotation of point in R2.
For a given point, we rotate it by moving it along a planar arc a constant
distance from another point, known as the center of rotation (Figure 3.4). This
center of rotation is commonly deﬁned as the origin of the current frame (we’ll
refer to this as a pure rotation) but can be any arbitrary point. We can think of
this as deﬁning a vector v from the center of rotation to the point to be rotated,
rotating v, and then adding the result to the center of rotation to compute the
new position of the point. For now we’ll only cover pure rotations; applying
general afﬁne transformations about an arbitrary center will be discussed
later.
To keep things simple, we’ll begin with rotations around one of the three
frame axes, with a center of rotation equal to the origin. The following system

3.3 Standard Affine Transformations 117
of equations rotates a vector or point counterclockwise (assuming the axis is
pointing at us) around k, or the z-axis (Figure 3.5c):
x′ = x cos θ −y sin θ
y′ = x sin θ + y cos θ
(3.7)
z′ = z
x
y
z
Figure 3.5a x-axis rotation.
x
y
z
Figure 3.5b y-axis rotation.

118
Chapter 3 Affine Transformations
x
y
z
Figure 3.5c z-axis rotation.
(x', y')
(x, y)
t
h
r
Figure 3.6 Rotation in xy-plane.
Figure 3.6 shows why this works. Since we’re rotating around the z-axis, no
z values will change, so we will consider only how the rotation affects the
xy values of the points. The starting position of the point is (x, y), and we
want to rotate that θ degrees counterclockwise. Handling this in Cartesian
coordinates can be problematic, but this is one case where polar coordinates
are useful.
Recall that a point P in polar coordinates has representation (r, φ), where
r is the distance from the origin and φ1 is the counterclockwise angle from
1.
We’re using φ for polar coordinates in this case to distinguish it from the rotation angle θ.

3.3 Standard Affine Transformations 119
the x-axis. We can think of this as rotating an r length radius lying along the
x-axis by φ degrees. If we rotate this a further θ degrees, the end of the radius
will be at (r, φ+θ) (in polar coordinates). Converting to Cartesian coordinates,
the ﬁnal point will lie at
x′ = r cos(φ + θ)
y′ = r sin(φ + θ)
Using trigonometric identities, this becomes
x′ = r cos φ cos θ −r sin φ sin θ
y′ = r cos φ sin θ + r sin φ cos θ
But r cos φ = x, and r sin φ = y, so we can substitute and get
x′ = x cos θ −y sin θ
y′ = x sin θ + y cos θ
We
can
derive
similar
equations
for
rotation
around
the
x-axis
(Figure 3.5a):
x′ = x
y′ = y cos θ −z sin θ
z′ = y sin θ + z cos θ
and rotation around the y-axis (Figure 3.5b):
x′ = z sin θ + x cos θ
y′ = y
z′ = z cos θ −x sin θ
To create the corresponding transformation, we need to determine how
the frame elements are transformed. The frame’s origin will not change since
it’s our center of rotation, so y = 0. So our primary concern will be the contents
of the 3 × 3 matrix A.
For this matrix, we need to compute where i, j, and k will go. For example,
for rotations around the z-axis we can transform i to get
x′ = (1) cos θ −(0) sin θ = cos θ
y′ = (1) sin θ + (0) cos θ = sin θ
z′ = 0

120
Chapter 3 Affine Transformations
Transforming j and k similarly and copying the results into the columns of
a 3 × 3 matrix gives
Rz =


cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1


Similar matrices can be created for rotation around the x-axis:
Rx =


1
0
0
0
cos θ
−sin θ
0
sin θ
cos θ


and around the y-axis:
Ry =


cos θ
0
sin θ
0
1
0
−sin θ
0
cos θ


One thing to note about these matrices is that their determinants are
equal to 1, and they are all orthogonal. For example, look at the component
3-vectors of the z-axis rotation matrix. We have (cos θ, sin θ, 0), (−sin θ, cos θ, 0),
and (0, 0, 1). The ﬁrst two lie on the xy-plane and so are perpendicular to the
third, and they are perpendicular to each other. All three are unit length and
so form an orthonormal basis.
The product of two orthogonal matrices is also an orthogonal matrix,
thus the product of a series of pure rotation matrices is also a rotation matrix.
For example, by concatenating matrices which rotate around the z-axis, then
the y-axis, and then the x-axis, we can create one form of a generalized rotation
matrix:
RxRyRz =


CyCz
−CySz
Sy
SxSyCz + CxSz
−SxSySz + CxCz
−SxCy
−CxSyCz + SxSz
CxSySz + SxCz
CxCy


(3.8)
where
Cx = cos θx
Sx = sin θx
Cy = cos θy
Sy = sin θy
Cz = cos θz
Sz = sin θz
Recall that the inverse of an orthogonal matrix is its transpose. Because
pure rotation matrices are orthogonal, the inverse of any rotation matrix is

3.3 Standard Affine Transformations 121
also its transpose. Therefore, the inverse of the z-axis rotation, centered on
the origin, is
R−1
z
=


cos θ
sin θ
0
−sin θ
cos θ
0
0
0
1


This follows if we think of the inverse transformation as “undoing” the
original transformation. If you substitute −θ for θ in the original matrix and
replace cos(−θ) with cos θ and sin(−θ) with −sin θ, then we have:


cos(−θ)
−sin(−θ)
0
sin(−θ)
cos(−θ)
0
0
0
1

=


cos θ
sin θ
0
−sin θ
cos θ
0
0
0
1


which, as we can see, results in the immediately preceding inverse matrix.
Now that we have looked at rotations around the coordinate axes, we
will consider rotations about an arbitrary axis. The formula for a rotation of
a vector v by an angle θ around a general axis ˆr is derived as follows. We begin
by breaking v into two parts: the part parallel with ˆr and the part perpendicular
to it, which lies on the plane of rotation (Figure 3.7a). Recall from Chapter 1
that the parallel part v∥is the projection of v onto ˆr, or
v∥= (v · ˆr)ˆr
(3.9)
The perpendicular part is what remains of v after we subtract the parallel
part, or
v⊥= v −(v · ˆr)ˆr
(3.10)
To properly compute the effect of rotation, we need to create a two-
dimensional basis on the plane of rotation (Figure 3.7b). We’ll use v⊥as our
ﬁrst basis vector, and we’ll need a vector w perpendicular to it for our second
basis vector. We can take the cross product with ˆr for this:
w = ˆr × v⊥= ˆr × v
(3.11)
In the standard basis for R2, if we rotate the vector i = (1, 0) by θ, we get
the vector (cos θ, sin θ). Equivalently,
Ri = (cos θ)i + (sin θ)j

122
Chapter 3 Affine Transformations
w
T(v⊥)
T(v)
v
v⊥
θ
v||
r
>
Figure 3.7a General rotation, showing axis of rotation and rotation plane.
w
T(v⊥)
v⊥
θ
(sinθ)w
(cosθ)v⊥
Figure 3.7b General rotation, showing vectors on rotation plane.

3.3 Standard Affine Transformations 123
If we use v⊥and w as the 2D basis for the rotation plane, we can ﬁnd the
rotation of v⊥by θ in a similar manner:
Rv⊥= (cos θ)v⊥+ (sin θ)w
(3.12)
The parallel part of v doesn’t change with the rotation, so the ﬁnal result of
rotating v around ˆr by θ is
Rv = Rv∥+ Rv⊥
= Rv∥+ (cos θ)v⊥+ (sin θ)w
= (v · ˆr)ˆr + cos θ[v −(v · ˆr)ˆr] + sin θ(ˆr × v)
= cos θv + [1 −cos θ](v · ˆr)ˆr + sin θ(ˆr × v)
(3.13)
This is one form of what is known as the Rodrigues formula.
The projection (v · ˆr)ˆr can be replaced by the tensor product (ˆr ⊗ˆr)v.
Similarly, the cross product ˆr×v can be replaced by a multiplication by a skew
symmetric matrix ˜rv. This gives
Rv = cos θv + (1 −cos θ)(ˆr ⊗ˆr)v + sin θ ˜rv
= [cos θI + (1 −cos θ)(ˆr ⊗ˆr) + sin θ ˜r]v
Expanding the terms, we end up with a matrix:
Rˆrθ =


tx2 + c
txy −sz
txz + sy
txy + sz
ty2 + c
tyz −sx
txz −sy
tyz + sx
tz2 + c


where
ˆr = (x, y, z)
c = cos θ
s = sin θ
t = 1 −cos θ
As we can see, there is a wide variety of choices for the 3 × 3 matrix A,
depending on what sort of rotation we wish to perform. The full afﬁne matrix
for rotation around the origin is
 R
0
0T
1


124
Chapter 3 Affine Transformations
where R is one of the rotation matrices just given. For example, the afﬁne
matrix for rotation around the x-axis is
 Rx
0
0T
1

=


1
0
0
0
0
cos θ
−sin θ
0
0
sin θ
cos θ
0
0
0
0
1


This is also an orthogonal matrix and its inverse is the transpose, as before.
Finally, when discussing rotations one has to be careful to distinguish
rotation from orientation, which is to rotation as position is to translation.
If we consider the representation of a point in an afﬁne space:
P = v + O
then we can think of the origin as a reference position and the vector v as
a translation which relates our position to the reference. We can represent
our position as just the components of the translation. Similarly, we can
deﬁne a reference orientation 0, and any orientation  is related to it by
a rotation, or
 = R00
Just as we might use the components of the vector v to represent our posi-
tion, we can use the rotation R0 to represent our orientation. To change our
orientation, we apply an additional rotation just as we might add a translation
vector to change our position:
′ = R1
In this case our ﬁnal orientation, using the rotation component, is
R1R0
Remember that the order of concatenation matters,
because matrix
multiplication — particularly for rotation matrices — is not a commutative
operation.
3.3.3 Scaling
The remaining afﬁne transformations that we will cover are deformations,
since they don’t preserve exact lengths or angles. The ﬁrst is scaling, which
can be thought of as corresponding to our other basic vector operation, scalar

3.3 Standard Affine Transformations 125
x
y
z
Figure 3.8 Nonuniform scaling.
multiplication; however, it is not quite the same. Scalar multiplication of
a vector has only one multiplicative factor and changes a vector’s length
equally in all directions. We can also multiply a vector by a negative scalar.
In comparison, scaling as it is commonly used in computer graphics applies
a possibly different but positive factor to each basis vector in our frame.2 If
all the factors are equal, then it is called uniform scaling and is — for vectors
in the afﬁne space — equivalent to scalar multiplication by a single positive
scalar. Otherwise, it is called nonuniform scaling. Full nonuniform scaling
can be applied differently in each axis direction, so we can scale by 2 in z to
make an object twice as tall, but 1/2 in x and y to make it half as wide.
A point doesn’t have a length per se, so instead we change its relative dist-
ance from another point Cs, known as the center of scaling. We can consider
this as scaling the vector from the center of scaling to our point P. For a set of
points, this will end up scaling their distance relative to each other, but still
maintaining the same relative shape (Figure 3.8).
For now we’ll consider only scaling around the origin, so Cs = O and
y = 0. For the upper 3 × 3 matrix A, we again need to determine how the
frame basis vectors change, which is deﬁned as
T(i) = ai
T(j) = bj
T(k) = ck
2.
We’ll consider negative factors when we discuss reﬂections in the following section.

126
Chapter 3 Affine Transformations
where a, b, c > 0 and are the scale factors in the x, y, z directions, respectively.
Writing these transformed basis vectors as the columns of A, we get an afﬁne
matrix of
Sabc =


a
0
0
0
0
b
0
0
0
0
c
0
0
0
0
1


This is a diagonal matrix, with the positive scale factors lying along the
diagonal, so the inverse is
S−1
abc = S 1
a
1
b
1
c =


1/a
0
0
0
0
1/b
0
0
0
0
1/c
0
0
0
0
1


3.3.4 Reflection
The reﬂection transformation symmetrically maps an object across a plane
or through a point. One possible reﬂection is (Figure 3.9a):
x′ = −x
y′ = y
z′ = z
This reﬂects across the yz plane and gives an effect like a standard mirror
(mirrors don’t swap left to right, they swap front to back). If we want to
reﬂect across the xz-plane instead, we would use (Figure 3.9b)
x′ = x
y′ = −y
z′ = z
As one might expect, we can create a planar reﬂection that reﬂects across a
general plane, deﬁned by a normal ˆn and a point on the plane P0. For now we’ll
consider only planes that pass through the origin. If we have a vector v in our
afﬁne space, we can break it into two parts: the part coincident to the plane
v⊥, which will remain unchanged, and the part orthogonal to it v∥, which will
be reﬂected to the other side of the plane to become −v∥. The transformed
vector will be the sum of v⊥and the reﬂected −v∥(Figure 3.10).

3.3 Standard Affine Transformations 127
x
y
z
Figure 3.9a yz reﬂection.
x
y
z
Figure 3.9b xz reﬂection.
To compute v∥, we merely have to take the projection of v against the
plane normal ˆn, or
v∥= (v · ˆn)ˆn
(3.14)
Subtracting this from v, we can compute v⊥:
v⊥= v −v∥
(3.15)

128
Chapter 3 Affine Transformations
n
v
v⊥
v'
v||
–v||
Figure 3.10 General reﬂection.
We know that the transformed vector will be v⊥−v∥. Substituting
equations 3.15 and 3.14 into this gives us
T(v) = v⊥−v∥
= v −2v∥
= v −2(v · ˆn)ˆn
From Chapter 2, we know that we can perform the projection of v on ˆn by
multiplying by the tensor product matrix ˆn ⊗ˆn, so this becomes
T(v) = v −2(ˆn ⊗ˆn)v
= [I −2(ˆn ⊗ˆn)]v
Thus, the linear transformation part A of our afﬁne transformation is
[I −2(ˆn ⊗ˆn)]. Writing this as a block matrix:
Fn =
 I −2(ˆn ⊗ˆn)
0
0T
1


3.3 Standard Affine Transformations 129
x
y
z
Figure 3.11 Point reﬂection.
While in the real world we usually see planar reﬂections, in our virtual
world we can also compute a reﬂection through a point. The following
performs a reﬂection through the origin (Figure 3.11):
x′ = −x
y′ = −y
z′ = −z
The corresponding block matrix is
FO =
 −I
0
0T
1

Reﬂections are a symmetric operation — that is, the reﬂection of a reﬂec-
tion returns the original point or vector. Because of this, the inverse of
a reﬂection matrix is the matrix itself.
As an aside, we would (incorrectly) expect that if we can reﬂect through
a plane and a point, we can reﬂect through a line. The following system:
x′ = −x
y′ = −y
z′ = z

130
Chapter 3 Affine Transformations
appears to reﬂect through the z axis, giving a “funhouse mirror” effect, where
right and left are swapped (if y is left, it becomes −y in the “reﬂection” and so
ends up on the right side). However, if we examine the transformation closely,
we see that while it does perform the desired effect, this is actually a rotation of
180 degrees around the z-axis. While both pure rotations and pure reﬂections
through the origin are orthogonal matrices, we can distinguish between them
by noting that reﬂection matrices have a determinant of −1, while rotation
matrices have a determinant of 1.
3.3.5 Shear
The ﬁnal afﬁne transformation that we will cover is shear. Because it affects
the angles of objects it is not used all that often, but it comes up particularly
when discussing oblique projections. An axis-aligned shear provides a shift in
one or two axes proportional to the component in a third axis. Transforming
a square to a rhombus or a cube to a rhomboid solid is a shear transformation
(Figure 3.12).
There are a number of ways of specifying shear ([79], [96]). In our case
we will deﬁne a shear plane, with normal ˆn, that does not change due to the
transformation. We deﬁne an orthogonal shear vector s, which indicates how
planes parallel to the shear plane will be transformed. Points on the plane 1
unit of distance from the shear plane, in the direction of the plane normal,
will be displaced by s. Points on the plane 2 units from the shear plane will
x
y
z
Figure 3.12 z-shear on square.

3.3 Standard Affine Transformations 131
be displaced by 2s, and so on. In general, if we take a point P and deﬁne it
as P0 + v, where P0 is a point on the shear plane, then P will be displaced by
(ˆn · v)s.
The simplest case is when we apply shear perpendicular to one of the main
coordinate axes. For example, if we take the yz-plane as our shear plane, our
normal is i and the shear plane passes through the origin O. We know from
this that O will not change with the transformation, so our vector y is 0. As
before, to ﬁnd A we need to ﬁgure out how the transformation affects our
basis vectors. If we deﬁne j as P1 −O, then
T(j) = T(P1) −T(O)
But P1 and O lie on the shear plane, so
T(j) = P1 −O
= j
The same is true for the basis vector k. For i, we can deﬁne it as P0 −O. We
know that P0 is distance 1 from the shear plane, so it will become P0 + s, so
T(i) = T(P0) −T(O)
= P0 + s −O
= i + s
The vector s in this case is orthogonal to i, therefore it is of the form (0, a, b),
so our transformed basis vector will be (1, a, b). Our ﬁnal matrix A is
Hx =


1
0
0
a
1
0
b
0
1


We can go through a similar process to get shear by the y-axis:
Hy =


1
c
0
0
1
0
0
d
1


and shear by the z-axis:
Hz =


1
0
e
0
1
f
0
0
1



132
Chapter 3 Affine Transformations
For shearing by a general plane through the origin, we already have the
formula for the displacement: (ˆn · v)s. We can rewrite this as a tensor product
to get (ˆn ⊗s)v. Because this is merely the displacement, we need to include
the original point, and thus our origin-centered general shear matrix is simply
I + ˆn ⊗s. Our ﬁnal shear matrix is
Hˆn,s =
 I + s ⊗ˆn
0
0T
1

The inverse shear transformation is shear in the opposite direction, so the
corresponding matrix is
H−1
ˆn,s =
 I −s ⊗ˆn
0
0T
1

= Hˆn,−s
3.3.6 Applying an Affine Transformation
Around an Arbitrary Point
Up to this point, we have been assuming that our afﬁne transformations
are applied around the origin of the frame. For example, when discussing
rotation we treated the origin as our center of rotation. Similarly, our shear
planes were assumed to pass through the origin. This doesn’t necessarily have
to be the case.
Let’s look at a particular example — the rotation of a point around an
arbitrary center of rotation C — and determine how this transformation
affects the origin of our frame. If we look at Figure 3.13, we see the situation.
We have a point C and our origin O. We want to rotate the difference vector
v
v'
C
y
O
O'
Figure 3.13 Rotation of origin around arbitrary center.

3.3 Standard Affine Transformations 133
v = O−C between the two points by matrix R and determine where the result-
ing point T(O), or C + T(v), will be. From that we can compute the difference
vector y = T(O) −O. From Figure 3.13, we can see that y = T(v) −v, so we
can reduce this as follows:
y = T(v) −v
= Rv −v
= (R −I)v
It’s usually more convenient to write this in terms of the vector dual to C,
which is x = C −O = −v, so this becomes
y = −(R −I)x
= (I −R)x
We can achieve the same result by translating our center C to the frame
origin by −x, performing our origin-centered rotation, and then translating
back by x:
Mc =

I
x
0T
1
  R
0
0T
1
 
I
−x
0T
1

=
 R
x
0T
1
 
I
−x
0T
1

=
 R
(I−R)x
0T
1

Notice that the upper left-hand block R is not affected by this process.
The same construction can be used for all afﬁne transformations that
use a center of transformation: rotation, scale, reﬂection, and shear. The
exception is translation, since such an operation has no effect: P −x + t + x =
P + t. But for the others, using a point C= (x, 1) as our arbitrary center of
transformation gives
Mc =
 A
(I −A)x
0T
1

where A is the upper 3 × 3 matrix of an origin-centered transformation. The
corresponding inverse is
M−1
c
=
 A−1
(I −A−1)x
0T
1


134
Chapter 3 Affine Transformations
3.3.7 Transforming Plane Normals
As we saw in the previous section, if we want to transform a line or plane repre-
sented in parametric form, we transform the points in the afﬁne combination.
For example,
T(P(t)) = (1 −s −t)T(P0) + sT(P1) + tT(P2)
But suppose we have a plane represented using the generalized plane
equation. One way of considering this is as a plane normal (a, b, c) and a point
on the plane P0. We could transform these and try to use the resulting vector
and point to build the new plane. However, if we apply an afﬁne transform to
the plane normal (a, b, c) directly, we may end up performing a deformation.
Since angles aren’t preserved under deformations, the resulting “normal” may
no longer be orthogonal to the points in the plane.
The correct approach is as follows. We can represent the generalized plane
equation as the product of a row matrix and column matrix, or
ax + by + cz + d =
 a
b
c
d 


x
y
z
1


= nT P
Now P is clearly a point, and n is the vector of coefﬁcients for the plane.
For points that lie on the plane:
nT P = 0
If we transform all the points on the plane by some matrix M, then to
maintain the relationship between nT and P, we’ll have to transform n by
some unknown matrix Q, or
(Qn)T (MP) = 0
This can be rewritten as
nT QT MP = 0
One possible solution for this is if
I = QT M

3.4 Using Affine Transformations 135
Solving for Q gives
Q =

M−1T
So the transformed plane coefﬁcients become
n′ =

M−1T
n
The same approach will work if we’re transforming the plane normal and
point as described earlier. We transform the point P0 by M and the normal by
(M−1)T .
In many cases the inverse matrix M−1 may not exist. So if we’re just trans-
forming a normal vector (a, b, c), we can use a different method. Instead of
M−1, we use the adjoint matrix from Cramer’s rule. Normally we couldn’t
proceed at this point: if the inverse doesn’t exist, we end up dividing by
a zero determinant. However, even when the inverse exists, the division by the
determinant is a scale factor. So we can ignore it in all cases and just use the
adjoint matrix directly, because we’re going to normalize the resulting vector
anyway.
3.4 Using Affine Transformations
3.4.1 Manipulation of Game Objects
The primary use of afﬁne transformations is for the manipulation of objects
in our game world. Suppose, from our earlier hypothetical, we have an ofﬁce
environment that is acting as our game space. The artists could build the
basic level — the walls, the ﬂoor, the ceilings, and so forth — as a single set of
triangles with coordinates deﬁned to place them exactly where we might want
them in the world. However, suppose we have a single desk model that we
want to duplicate and place in various locations in the level. The artist could
build a new version of the desk for each location in the core level geometry,
but that would involve unnecessarily duplicating all the memory needed for
the model. Instead, we could have one version, or master, of the desk model
and then set a series of transformations that indicate where in the level each
copy, or instance, of the desk should be placed [106].
Before we can begin to discuss how we specify these transformations
and what they might mean, we need to deﬁne the two different coordinate
frames we are working in: the local coordinate frame and the world coordinate
frame.

136
Chapter 3 Affine Transformations
Local and World Coordinate Frames
When artists create an object or we create an object directly in a program,
the coordinates of the points that make up that object are deﬁned in that
particular object’s local frame. This is also commonly known as local space,
or alternatively as model space or object space.
The orientation of the basis vectors in the local frame is usually set so
that the engineers know which part of the object is the front, which is the top,
and which is the side. This allows us to orient the object correctly relative to
the rest of the world and to translate it in the correct direction if we want
to move it forward. The convention that we will be using in this book is one
where the x-axis points along the forward direction of the object, the y-axis
points towards the left of the object, and the z-axis points out the top of the
object (Figure 3.14). Another common convention is to use the y-axis for
up, the z-axis for forward, and the x-axis for either out to the left or to the
right, depending on whether we want to work in a right-handed or left-handed
frame.
Typically, the origin of the frame is placed in a position convenient for
the game, either at the center of the object or at the bottom of the object. The
ﬁrst is useful when we want to rotate objects around their centers, the second
for placement on the ground.
When constructing our world, we deﬁne a speciﬁc coordinate frame, or
world frame, also known as world space. The world frame acts as a common
reference among all the objects, much as the origin acts as a common ref-
erence among points. Ultimately, in order to render, simulate, or otherwise
interact with objects, we will need to transform their local coordinates into
the world frame.
When an artist builds the level geometry, the coordinates are usually set
in the world frame. Orientation of the level relative to our world frame is set
z
x
y
Figure 3.14 Local object frame.

3.4 Using Affine Transformations 137
by convention. Knowing which direction is “up” is important in a 3D game;
in our case we’ll be using the z-axis, but the y-axis is also commonly used.
Aligning the level to the other two axes (in our case, x and y) is arbitrary, but
if our level is either gridlike or box-shaped, it is usually convenient to orient
the grid lines or box sides to these remaining axes.
Positioning the level relative to the origin of the frame is also arbitrary but
is usually set so that the origin lies in the center of a box deﬁning our maximum
play area. This helps avoid precision problems, since ﬂoating point precision
is centered around 0 (see Chapter 4). For example, we might have a 300 meter
by 300 meter play area, so that in the xy directions the origin will lie directly
in the center. While we can set things so that the origin is centered in z as well,
we may want to adjust that depending on our application. If our game mainly
takes place on a ﬂat play area, such as in an arena ﬁghting game, we might set
the ﬂoor so that it lies at the origin; this will make it simple to place objects
and characters exactly at ﬂoor level. In a submarine game, we might place sea
level at the origin; negative z lies under the waterline and positive z above.
Placing Objects
If we were to use the objects’ local coordinates directly in the world frame,
they would end up interpenetrating and centered around the world origin.
To avoid that situation, we apply afﬁne transformations to each object to
place them at their own speciﬁc position and orientation in the world. For
each object, this is known as their particular local-to-world transformation.
We often display the relative position and orientation of a particular object in
the world by drawing its frame relative to the world frame (Figure 3.15). The
local-to-world transformation, or world transformation for short, describes
this relative relationship: the column vectors of the local-to-world matrix A
describe where the local frame’s basis vectors will lie relative to the world
space basis, and the vector y describes where the local frame’s origin lies
relative to the world origin.
The most commonly used afﬁne transformations for object placement are
translation, rotation, and scaling. Translation and rotation are convenient for
two reasons. First, they correspond naturally to two of the characteristics
we want to control in our objects, position and orientation. Second, they
are rigid transformations, meaning they don’t affect the size or shape of our
object, which is generally the desired effect. Scaling is a deformation but
is commonly useful to change the size of objects. For example, if two artists
build two objects but fail to agree on a relative measure of size, you might end
up with a table bigger than a room, if placed directly in the level. Rather than
have the artist redo the model, we can use scaling to make it appear smaller.
Scaling is also useful in fantastical games to either shrink a character to ﬁt in
a small space or grow a character to be more imposing. However, for most
games you can actually get away without using scaling at all.

138
Chapter 3 Affine Transformations
Figure 3.15 Local to world transformation.
Demo
Interaction
To create the ﬁnal world transformation, we’ll be concatenating a
sequence of these translation, rotation, and scaling transformations together.
However, remember that concatenation of transformations is not commu-
tative. So the order in which we apply our transformations affects the ﬁnal
result, sometimes in surprising ways. One basic example is transforming the
point (0,0,0). A pure rotation around the origin has no effect on (0,0,0), so
rotating by 90 degrees around z and then translating by (tx, ty, tz) will just act
as a translation, and we end up with (tx, ty, tz). Translating the point ﬁrst will
transform it to (tx, ty, tz), so in this case a subsequent rotation of 90 degrees
around z will have an effect, with the ﬁnal result of (−ty, tx, tz). As another
example, look at Figure 3.16a, which shows a rotation and translation.
Figure 3.16b shows the equivalent translation and rotation.
Scaling and rotation are also noncommutative. If we ﬁrst scale (1,0,0) by
(sx, sy, sz), we get the point (sx, 0, 0). Rotating this by 90 degrees around z, we
end up with (0, sx, 0). Reversing the transformation order, if we rotate (1,0,0)
by 90 degrees around z, we get the point (0, 1, 0). Scaling this by (sx, sy, sz), we
get the point (0, sy, 0). Note that in the second case we rotated our object so
that our original x-axis lies along the y-axis and then applied our scale, giving
us the unexpected result. Figures 3.17a and 3.17b show another example of
this applied to an object.
The ﬁnal combination is scaling and translation. Again, this is not
commutative. Remember that pure scaling is applied from the origin of the
frame. If we translate an object from the origin and then scale, there will be

3.4 Using Affine Transformations 139
Figure 3.16a Rotation, then translation.
Figure 3.16b Translation, then rotation.
Figure 3.17a Scale, then rotation.

140
Chapter 3 Affine Transformations
Figure 3.17b Rotation, then scale.
additional scaling done to the translation of the object. So for example, if
we scale (1, 1, 1) by (sx, sy, sz) and then translate by (tx, ty, tz), we end up with
(tx + sx, ty + sy, tz + sz). If instead we translate ﬁrst, we get (tx + 1, ty + 1, tz + 1),
and then scaling gives us (sxtx + sx, syty + sy, sztz + sz). Another example can
be seen in Figures 3.18a and 3.18b.
Generally, the desired order we wish to use for these transforms is to
scale ﬁrst, then rotate, then translate. Scaling ﬁrst gives us the scaling along
the axes we expect. We can then rotate around the origin of the frame,
and then translate it into place. This gives us the following multiplication
order:
M = TRS
Figure 3.18a Scale, then translate.

3.4 Using Affine Transformations 141
Figure 3.18b Translate, then scale.
3.4.2 Matrix Decomposition
It is sometimes useful to break an afﬁne transformation matrix into its
component basic afﬁne transformations. This is called matrix decomposition.
We performed one such decomposition when we pulled the translation infor-
mation out of the matrix, effectively representing our transformation as the
product of two matrices:
 A
y
0T
1

=

I
y
0T
1
  A
0
0T
1

Suppose we continue the process and break down A into the product of
more basic afﬁne transformations. For example, if we’re using only scaling,
rotation, and translation, it would be ideal if we could break A into the product
of a scaling and rotation matrix. If we know for a fact that A is the product
of only a scaling and rotation matrix, in the order RS, we can multiply it out
to get


r11
r12
r13
0
r21
r22
r23
0
r31
r32
r33
0
0
0
0
1




sx
0
0
0
0
sy
0
0
0
0
sz
0
0
0
0
1

=


sxr11
syr12
szr13
0
sxr21
syr22
szr23
0
sxr31
syr32
szr33
0
0
0
0
1


In this case the lengths of the ﬁrst three column vectors will give our three
scale factors sx, sy, and sz. To get the rotation matrix, all we need to do is
normalize those three vectors.

142
Chapter 3 Affine Transformations
y
x
Figure 3.19 Effect of rotation, then scale.
Unfortunately, it isn’t always that simple. As we’ll see in Part 3.5, often
we’ll be concatenating a series of TRS transformations to get something like
M = TnRnSn · · ·T1R1S1T0R0S0
In this case, even ignoring the translations, it is impossible to decompose
M into the form RS. As a quick example, suppose that all these transforma-
tions with the exception of S1 and R0 are the identity transformation. This
simpliﬁes to
M = S1R0
Now suppose S1 scales by 2 along y and by 1 along x and z, and R0 rotates
by 60 degrees around z. Figure 3.19 shows how this affects a square on the
xy plane. The sides of the transformed square are no longer perpendicular.
Somehow, we have ended up applying a shear within our transformation, and
clearly we cannot represent this by a simple concatenation RS.
One solution is to decompose the matrix using a technique known as
singular value decomposition, or simply SVD. Assuming no translation, the
matrix M can be represented by three matrices L, D, and R, where L and R
are orthogonal matrices, D is a diagonal matrix with nonnegative entries, and
M = LDR
An alternative formulation to this is polar decomposition, which breaks the
nontranslational part of the matrix into two pieces, an orthogonal matrix Q
and a stretch matrix S, where
S = UT KU

3.4 Using Affine Transformations 143
Matrix U in this case is another orthogonal matrix, and K is a diagonal matrix.
The stretch matrix combines the scale-plus-shear effect we saw in our exam-
ple: it rotates the frame to an orientation, scales along the axes, and then
rotates back. Using this, a general afﬁne matrix can be broken into four
transformations:
M = TRNS
where T is a translation matrix, Q has been separated into a rotation matrix
R and a reﬂection matrix N = ±I, and S is the preceding stretch matrix.
Performing either SVD or polar decomposition is out of the purview of
this text. As we’ll see, there are ways to avoid matrix decomposition at the cost
of some conversion before we send our models down the graphics pipeline.
However, at times we may get a matrix of unknown structure from a library
module that we don’t control. For example, we could be using a commercial
physics engine or writing a plug-in for a 3D modeling package such as
Max or Maya. Most of the time a function is provided that will decompose
such matrices for us, but this isn’t always the case. For those times and
for those who are interested in pursuing this topic, more information on
decompositions can be found in [45], [46], and [103].
3.4.3 Avoiding Matrix Decomposition
Demo
Centered
In the preceding section, we made no assumptions about the values for our
scaling factors. Now let’s assume that they are equal; that is, each scaling
matrix performs a uniform scale. Looking at just the rotation and scaling
transformations, we have
M = RnSn · · · R1S1R0S0
Since each scaling transformation is uniformly scaling, we can simplify this to
M = Rnσn · · · R1σ1R0σ0
Using matrix algebra, we can shufﬂe terms to get
M = Rn · · · R1R0σn · · · σ1σ0
= Rσ
= RS
where R is a rotation matrix and S is a uniform scaling matrix. So if we use
uniform scaling, we can in fact decompose our matrix into a rotation and
scaling matrix, as we just did.

144
Chapter 3 Affine Transformations
Demo
Separate
However, even in this case the decomposition takes three square roots
and nine scaling operations to perform. This leads to an alternate approach to
handling transformations. Instead of storing transformations for our objects
as a single 4 × 4 or even 3 × 4 matrix, we will break out the individual parts:
a scale factor s, a 3 × 3 rotation matrix R, and a translation vector t. To apply
this transformation to a point P, we use
T(P) =
 sRx + t
1

Note the similarity to equation 3.1. We’ve replaced A with sR and y with t. In
practice we ignore the trailing 1.
Concatenating transformations in matrix format is as simple as perform-
ing a multiplication. Concatenating in our alternate format is a little less
straightforward but is not difﬁcult and actually takes fewer operations on
a standard ﬂoating point processor:
s′ = s1s0
R′ = R1R0
t′ = t1 + s1R1t0
(3.16)
Computing the new scale and rotation makes a certain amount of sense,
but it may not be clear why we don’t add the two translations together to get
the new translation. If we multiply the two transforms in matrix format, we
have the following order:
M = T1R1S1T0R0S0
But since T0 is applied after R0 and S0, they have no effect on it. So if we want
to ﬁnd how the translation changes, we drop them:
M′ = T1R1S1T0
Multiplying this out in block format gives us
M′ =

I
t1
0T
1
  R1
0
0T
1
  s1I
0
0T
1
 
I
t0
0T
1

=
 R1
t1
0T
1
  s1I
s1t0
0T
1

=
 s1R1
s1R1t0 + t1
0T
1


3.5 Object Hierarchies and Scene Graphs 145
We can see that the right-hand column vector y is equal to equation 3.16. So
to get the ﬁnal translation we need to apply the second scale and rotation
before adding the second translation. Another way of thinking of this is that
we need to scale and rotate the ﬁrst translation vector into the frame of the
second translation vector before they can be combined together.
There are a few advantages to this alternate format. First of all, it’s clear
what each part does — the scale and rotation aren’t combined into a single
3 × 3 matrix. Because of this, it’s also easier to change individual elements.
We can update rotation, or scale through a simple multiplication, or even just
set them directly. Surprisingly, on a serial processor concatenation is also
cheaper. It takes 48 multiplications and 32 adds to do a traditional matrix
multiplication, but only 40 multiplications and 27 adds to perform our alter-
nate concatenation. This advantage disappears when using vector processor
operations, however. In that case, it’s much easier to parallelize the matrix
multiplication (16 operations on some systems), and the cost of scaling and
rotating the translation vector becomes more of an issue.
Even with serial processors our alternate format does have one main
disadvantage, which is that we need to create a 4 × 4 matrix to be sent to
the graphics API. Based on our previous explorations of the transformation
matrix, we can create a matrix from our alternate format quite quickly; scale
the three columns of the rotation matrix; and then copy it and the translation
vector into our 4 × 4:


sr0,0
sr0,1
sr0,2
tx
sr1,0
sr1,1
sr1,2
ty
sr2,0
sr2,1
sr2,2
tz
0
0
0
1


Which representation is better? It depends on your application. If all you
wish to do is an initial scale and then apply sequences of rotations and trans-
lations, the 4 × 4 matrix format works ﬁne and will be faster on a vector
processor. If, on the other hand, you wish to make changes to scale as well,
using the alternate format should at least be considered. And, as we’ll see, if
we wish to use a rotation representation other than a matrix, the alternate
formation is almost certainly the way to go.
3.5 Object Hierarchies and Scene Graphs
Demo
Tank
3.5.1 Object Hierarchies
In describing object transformations, we have considered them as trans-
forming from the object’s local frame (or local space) to a world frame (or
word space). However, it is possible to deﬁne an object’s transformation as

146
Chapter 3 Affine Transformations
Figure 3.20 Hierarchy of frames.
being relative to another object’s space instead. We could carry this out for
a number of steps, thereby creating a hierarchy of objects, with world space
as the root and each object’s space as a node in a tree (Figure 3.20).
For example, suppose we wish to attach an arm to a body. The body is built
with its origin relative to its center. The arm has its origin at the shoulder joint
location because that will be our center of rotation. If we were to place them
in the world using the same transformation, the arm would end up inside
the body instead of at the shoulder. We want to ﬁnd the transformation that
modiﬁes the arm’s world transformation so that it matches the movement of
the body and still remains at the shoulder. The way to do this is to deﬁne
a transformation for the arm relative to the body’s local space. If we combine
this with the transformation for the body, this should place the arm in the
correct place in world space relative to the body, no matter its position and
orientation.
So the idea is to transform the arm to body space (Figure 3.21a) and
then continue the transform into world space (Figure 3.21b). In this case, for
each stage of transformation we perform the order as scale, rotate, and then
translate. In matrix format the world transformation for the arm would be
W = TbodyRbodySbodyTarmRarmSarm
As we’ve indicated, the body and arm are treated as two separate objects, each
with its own transformations, placed in a hierarchy. The body transformation
is relative to world space, and the arm transformation is relative to the body’s
space. When rendering, for example, we begin by drawing the body with
its world transformation and then drawing the arm with the concatenation
of the body’s transformation and the arm’s transformation. By doing this,

3.5 Object Hierarchies and Scene Graphs 147
Figure 3.21a Mapping arm to body’s local space.
Figure 3.21b Mapping body and arm to world space.
we can change them independently — rotating the arm around the shoulder,
for example, without affecting the body at all. Similar techniques can be used
to create deeper hierarchies; for example, a turret that rotates on top of a tank
chassis, with a gun barrel that elevates up and down relative to the turret.
One way of coding this is to create separate objects, each of which
handles all the work of grabbing the transformation from the parent objects
and combining to get the ﬁnal display transform. The problem with this
approach is that it generates a lot of duplicated code. Using the tank exam-
ple, the code necessary for handling the hierarchy for the turret is going to
be almost identical to that for the barrel. It would be much better to design

148
Chapter 3 Affine Transformations
a data structure that handles the generalized case of a hierarchy of frames
and use that to manage our hierarchical objects. The scene graph is one such
data structure, which we will describe in the next section.
3.5.2 Scene Graphs
Demo
SceneGraph
The scene graph is meant to be used for managing hierarchical scenes, such
as a collection of rooms and the objects contained within each room. While
a generalized scene graph can be quite powerful, for now we will focus only
on the basic structures needed for controlling hierarchical models efﬁciently.
Although the scene graph is not necessarily a tree, for the purposes of this
discussion we will be using it as such. Each object in the scene graph will
have at most one parent, with one object (called the root of the scene graph)
having no parent.
The implementation that we will present is only one of many possibilities.
The more we want our scene graph to do, the more complex the implementa-
tion needs to be, but for simple purposes the following will serve. It consists
of three classes: IvSpatial, IvNode, and IvGeometry.
IvSpatial is the base class. It contains two copies of the transformations
as member variables. The ﬁrst is a transformation relative to the parent; the
transformation of a propeller relative to the submarine body, for example.
We’ll call this the local transformation. The second is the full transformation
from the object’s local space into world space, which we’ll use to render and
interact with the subobject — we’ll call this the world transformation. This is
generated by an UpdateWorldTransform() virtual method, which multiplies the
local transformation by the parent’s world transformation. Finally, we deﬁne
a method called Render(), which uses the world transformation to render each
level of the hierarchy. An abbreviated class deﬁnition looks like the following:
class IvSpatial
{
public:
IvSpatial();
virtual ∼IvSpatial();
virtual void UpdateWorldTransform();
virtual void Render() = 0;
protected:
IvSpatial* mParent;
float
mLocalScale;

3.5 Object Hierarchies and Scene Graphs 149
IvMatrix33 mLocalRotate;
IvVector3
mLocalTranslate;
float
mWorldScale;
IvMatrix33 mWorldRotate;
IvVector3
mWorldTranslate;
};
where we deﬁne UpdateWorldTransform() as
void IvSpatial::UpdateWorldTransform()
{
if (mParent)
{
mWorldScale = mParent->mWorldScale*mLocalScale;
mWorldRotate = mParent->mWorldRotate*mLocalRotate;
mWorldTranslate = mParent->mWorldTranslate
+ mParent->mWorldScale*mParent->mWorldRotate*mLocalTranslate;
}
else
{
mWorldScale = mLocalScale;
mWorldRotate = mLocalRotate;
mWorldTranslate = mLocalTranslate;
}
}
The method Render() has no data to work with in this case, so it will remain
undeﬁned.
While IvSpatial provides a framework for managing transformations, we
will never actually allocate an instance of it as an object in our scene graph.
Instead, we will use one of the following subclasses.
The subclass of IvSpatial which acts as the root and intermediary nodes
of the hierarchy is called IvNode. It contains a list or array of pointers to
IvSpatial objects which are the children of the node, as well as a method
for adding children to the node. The UpdateWorldTransform() method over-
rides the default method and calls UpdateWorldTransform() for all the child
IvSpatials in addition to the current node:
class IvNode : public IvSpatial
{
public:
IvNode();

150
Chapter 3 Affine Transformations
virtual ∼IvNode();
virtual void UpdateWorldTransform();
virtual void Render();
protected:
unsigned int mNumChildren;
IvSpatial** mChildren;
};
The methods UpdateWorldTransform() and Render() become
void IvNode::UpdateWorldTransform()
{
IvSpatial::UpdateWorldTransform();
unsigned int i;
for (i = 0; i < mNumChildren; ++i )
{
mChildren[i]->UpdateWorldTransform();
}
}
void IvNode::Render()
{
unsigned int i;
for (i = 0; i < mNumChildren; ++i )
{
mChildren[i]->Render();
}
}
The other subclass of IvSpatial is called IvGeometry. These are the leaf
nodes of the scene graph, and contain the geometric data for each subob-
ject. One way to use IvGeometry is to subclass it and hard-code our geometry
information, but most of the time it will contain a pointer to model data. In
both cases, the world transformations are updated using the base IvSpatial
method, called by the parent IvNode, so we don’t implement it. However, we
will need to implement a Render() call, which builds the 4 × 4 matrix that is
set as our world transform:
class IvGeometry : public IvSpatial
{
public:

3.5 Object Hierarchies and Scene Graphs 151
IvGeometry();
virtual ∼IvGeometry();
virtual void Render();
};
void IvGeometry::Render()
{
// build 4x4 matrix
IvMatrix44 transform( mWorldRotate );
transform(0,0) *= mWorldScale;
transform(1,0) *= mWorldScale;
transform(2,0) *= mWorldScale;
transform(0,1) *= mWorldScale;
transform(1,1) *= mWorldScale;
transform(2,1) *= mWorldScale;
transform(0,2) *= mWorldScale;
transform(1,2) *= mWorldScale;
transform(2,2) *= mWorldScale;
transform(0,3) = mWorldTranslate.x;
transform(1,3) = mWorldTranslate.y;
transform(2,3) = mWorldTranslate.z;
// set transform
::SetWorldMatrix( transform );
// render geometry
}
Using the scene graph is a two-step process. In step 1, we call Update-
WorldTransform() at the root level, which updates transforms via a recursive
traversal from the top of the tree down to the leaf nodes. At each level, we
store the updated world transforms. These transforms may now be used by
the game engine for other purposes. Step 2 occurs once we’re ready to render
the object, when we do another recursive tree traversal by calling Render()
on it. UpdateWorldTransform() does not have to be called on the root of the
scene graph in every rendered frame. Generally, it is called once on the root
object, directly following the creation of the scene graph. Thereafter, it only
needs to be called at or above any and all IvNodes whose local transforms
have changed since the last call to UpdateWorldTransform(). This is often a
small subset of the scene graph. In other words, it is often sufﬁcient and
much faster to call UpdateWorldTransform() several times on disjoint subsec-
tions (subtrees) of the scene graph that have changed than it is to make the
single call to UpdateWorldTransform() at the root of the scene.

152
Chapter 3 Affine Transformations
body
arm
Figure 3.22 Scene graph of body–arm example.
Figure 3.22 shows our body–arm example stored as a scene graph. Note
that the body is not a root node — it is a geometry leaf node that hangs directly
off of the root node. This leads to some duplication of transformation infor-
mation, but that is the price we pay for maintaining transformations in the
base class.
One might wonder why we have two recursive calls — one for generating
the new transformations and one for rendering — or, for that matter, why we
bother storing the transforms at all. We could just have one recursive call that
generates the world transformation at each level and then passes the result
down as a function argument. At the leaf level, we would create the trans-
formation data and then render the data directly. However, there is usually
a culling step where we try to avoid rendering models that are not currently
visible on the screen. As we will see, it is convenient to keep the transforma-
tion data around for this and other purposes. Scene graphs are a very ﬂexible
and modular technique, and can be mixed with other data structures and
rendering systems. For example, scene graphs are sometimes used to com-
pute hierarchical transforms without using a hierarchical Render() function
to draw the scene graph. In such cases, the scene graph is used only to manipu-
late the local transforms of objects and update the world transforms of visible
geometry. Another method (such as a ﬂat list of all of the leaf IvGeometry
objects) is used to render the scene.

3.6 Chapter Summary 153
3.6 Chapter Summary
In this chapter we’ve discussed the general properties of afﬁne transforma-
tions, how they map between afﬁne spaces, and how they can be represented
and performed by matrices at one dimension higher than the afﬁne spaces
involved. We’ve covered the basic afﬁne transformations as used in interac-
tive applications and how to combine three of them — scaling, rotation, and
translation — to manipulate our objects within our world. While it may be
desirable to separate a given afﬁne transformation back into scaling, rota-
tion, and translation components, we have seen that it is not always possible
when using nonuniform scaling. Separating components in this manner may
not be efﬁcient, so we have presented an alternative afﬁne transformation rep-
resentation with the three components separated. Finally, we have discussed
how to construct transformations relative to other objects, which allows us
to create jointed, hierarchical structures.
For those interested in reading further, information on afﬁne algebra can
be found in Schneider and Eberly [96], as well as in deRose [25]. The standard
afﬁne transformations are described in most graphics textbooks, such as
Möller and Haines [79] and Foley and van Dam [36]. Further details on hierar-
chical transformation management and scene graph construction and usage
can be found in Eberly [27].


Chapter4
Real-World
Computer Number
Representation
4.1 Introduction
In this chapter we’ll discuss what is perhaps the most fundamental basis
upon which 3D graphics pipelines are built — computer representation of
numbers. While 3D programmers often use integers, unsigned integers, and
ﬂoating-point numbers successfully without any understanding of how they
are implemented, this can lead to subtle bugs and performance problems
eventually. Most basic undergraduate computer architecture books [104]
present the basics of integral data types (e.g., int and unsigned int, short,
etc. in C/C++) but give only brief introductions to ﬂoating-point and other
nonintegral number representations. Since the mathematics of 3D graphics
are generally real-valued (wintess the predominance of R, R2, and R3 in the
preceding chapters), it is important for anyone in the ﬁeld to understand the
features, limitations, and idiosyncracies of the computer representation of
these nonintegral types.
This chapter will begin by reviewing some of the issues surrounding
the representation of whole numbers and integers on a computer. This
review mainly serves to introduce concepts that will carry over to a dis-
cussion of real numbers. The chapter will discuss two major computer
representations of the real numbers, ﬁxed point and ﬂoating point, along
with their bitwise formats, basic operations, features, and limitations. By
design, we will transition from general mathematical discussions of num-
ber representation toward implementation-related topics of speciﬁc relevance
to 3D graphics programmers. Much of the chapter will be spent on the
155

156
Chapter 4 Real-World Computer Number Representation
ubiquitous IEEE ﬂoating point numbers, especially discussions of ﬂoating
point limitations that often cause issues in 3D pipelines. It will also present
a brief case study of ﬂoating-point–related performance issues in a real
application.
4.2 Representing Integral Types on a
Computer
4.2.1 Finiteness of Representation
The sets of whole numbers ([0, 1, 2 . . .], known as W), integers ([. . . −2,
−1, 0, 1, 2 . . .], known as Z), and real numbers (e.g., 1.5, 1/3,
√
2, known as R)
share one trait in common — they each have inﬁnitely many elements. Com-
puters, on the other hand, by their very physical nature can only represent a
ﬁnite number of different values. As a result computers cannot represent any
of the aforementioned number sets exactly and completely. We will have to
settle for some ﬁnite subset of each. The sizes of these ﬁnite sets are deter-
mined by the number of distinct values that can be represented by the given
number of storage systems.
Modern computers store their numbers as binary codings: ﬁnite, ﬁxed-
length strings of bits. (For a basic discussion of binary number representation,
we refer the reader to a basic computer architecture text, such as Stallings
[104].) As such, any N-bit computer number representation can only repre-
sent 2N distinct values. For our purposes, we will generally assume 32-bit
“words,” which can represent about 4 billion different values. Each of the
distinct values in a given representation can represent at most one element
of the number set exactly. While 4 billion may seem an enormous number
of possible values, we shall see that it becomes painfully ﬁnite when used to
cover the set of real numbers.
4.2.2 Range
The range of a number representation system is described by two values: the
minimum representable value and the maximum representable value, often
written as the interval [minimum, maximum]. Values outside of this interval
are assumed to be unrepresentable (although, as we shall see, there are some
cases in ﬂoating point where values outside of the proper “range” of a repre-
sentation can still be represented indirectly). We shall review the common
computer representations of integers and whole numbers here, mainly to
discuss the properties of these numbers and to give examples of range.

4.2 Representing Integral Types on a Computer 157
The whole numbers (W) have an inherent, ﬁnite minimum: 0. In order to
represent the whole numbers with a ﬁnite computer representation, we use
the fact that for any ﬁnite whole number Wmax, there will be a ﬁnite number
of elements (possibly zero) w ∈W such that w ≤Wmax. In fact, the size
of such a set is (Wmax + 1). Based on this observation, we can represent a
useful K-element subset of the whole numbers by simply selecting a maximum
representable value of Wmax = K −1. All whole numbers less than K can be
represented exactly by such a system.
The type unsigned int is the most commonly used C/C++ representation
of W. For smaller numbers, unsigned short and unsigned char are also used.
We will discuss only unsigned int in this section; the analysis of the other
representations is analogous. On a 32-bit computer, the representation of
unsigned int is simply an unsigned 32-bit binary number, capable of rep-
resenting 232 distinct values. As a result, all whole numbers in the range
[0, 232 −1] can be represented by unsigned int. For a basic discussion of the
binary representation of unsigned numbers, see [104]. Note that the C++ spec-
iﬁcation [31] does not require unsigned int to be a 32-bit type; however, for
the course of this discussion, we will assume the common case, which is a
32-bit unsigned int.
The integers have no inherent minimum or maximum. In order to rep-
resent the integers on a computer, we must select a ﬁnite pair of values Zmin
and Zmax. There will be a ﬁnite number of elements (possibly zero) i ∈Z, such
that Zmin ≤i ≤Zmax. In fact, the size of such a set is max(0, Zmax −Zmin + 1).
Unlike the whole numbers, there are inﬁnitely many K-element sets, one for
each chosen minimum value ([Zmin, Zmin + K −1]). Historically, most com-
puter representations of integers select Zmin such that the K-element set is as
evenly distributed around 0 as possible:
Zmin ≈−Zmax, orZmin ≈−K
2
This is done to ensure that for as many elements as possible:
i ∈[Zmin, Zmax] =⇒−i ∈[Zmin, Zmax]
The type int is the most common C/C++ representation of Z (as before, we
will not discuss the similar, smaller short and char). On a 32-bit computer the
representation of int is simply a signed 32-bit binary representation using so-
called ‘2’s complement’ to represent both positive and negative numbers (see
[88] for a review of 2’s complement). Being a 32-bit representation, it is capa-
ble of representing 232 distinct values. As this is an even number of elements,
there is no way to represent 232 distinct integers and fulﬁll the requirement to
have the range of the representation exactly center about 0. The standard 2’s
complement format of int represents all integers in the range [−(231), 231 −1],

158
Chapter 4 Real-World Computer Number Representation
meaning that while −(231) can be represented, its negative −(−(231)) = 231 is
out of range, since 231 > 231 −1.
Overﬂow
Overﬂow is a term used to describe what occurs when a computation generates
a result with a value outside the range of the representation in use. As we shall
see, different representation systems have different ways of dealing with this
situation, but in all cases the result cannot be represented exactly, and in
most cases the represented result is very different from the correct result.
In general, the best method with any number system is to avoid overﬂow
entirely. Such a strategy requires that the programmer understand the exact
range of the representation(s) that they are using. The following sections will
discuss the range of the number representations, along with the likely results
of overﬂow. In many cases, standardization has set the overﬂow behavior of
a given representation across platforms.
As mentioned, the range of the common type unsigned int on a 32-bit
computer is [0, 232 −1]. Application code must take care to ensure that the
results of all operations involving unsigned int values are in range. Positive
overﬂow of 32-bit unsigned integer values is relatively uncommon in correct
code. For example, if a counter is incremented once per frame on a game that
is running at 100 frames per second as a 32-bit unsigned int, this counter will
overﬂow only after
232f rames
100f rames
sec
× 60secs
min × 60mins
hour × 24hours
day
× 365days
yr
≈1.4 years!
Negative overﬂow of unsigned int values is rather easy to generate, espe-
cially in code with simple logic errors. Commonly, such code will subtract a
larger number from a smaller one, leading to a negative result (which can-
not be represented as a whole number). The C/C++ standard requires that
unsigned int operations always return a result that is equal to the correct
value modulo 232.
For most common applications, this result is not particularly useful as it
leads to the following examples:
(232 −1) + 1→0
0 −1→(232 −1)
However, it does make sense when considered as a part of the larger
picture. The result of any unsigned integer operation is the least signiﬁcant
32 bits of the correct result. Using assembly language (where the overﬂow ﬂag,

4.3 Representing Real Numbers 159
or “carry bit,” is accessible), it is possible to chain together 32-bit addition
operations to add 64-bit (or larger) numbers. The carry bit “carries” into the
low-order bit of the next 32-bit operation.
For most applications, the best way to handle negative overﬂow of
unsigned int is to avoid the situation by ensuring that the result of the sub-
traction will be nonnegative prior to computation and reworking code that
can generate negative overﬂows.
Range and Type Conversion
Mathematically, whole numbers are a proper subset of the set of integers.
However, on a computer our representations of integers (int) and whole
numbers (unsigned int) have the same size (generally 32 bits), so the set of
ints and unsigned ints each have the same number of elements. This leads
to the (sometimes problematic) fact that on a computer, unsigned int ̸⊆int
and int ̸⊆unsigned int. Each set contains values that cannot be represented
by the other set. Programmers must be very careful when converting between
int and unsigned int to avoid problems.
Given that the range of int on a 32-bit machine is [−(231), 231 −1] and the
range of unsigned int is [0, 232 −1], the safe range for conversion is thus
[−(231), 231 −1] ∩[0, 232 −1] = [0, 231 −1]
Applications should check int values to make sure they are not nega-
tive and unsigned int values to ensure that they will not overﬂow 231 −1
prior to converting (casting) them. Most C/C++ compilers will generate a
warning (at some warning levels) unless a signed/unsigned cast is made
explicit.
4.3 Representing Real Numbers
Real numbers are, to most developers, the heart and soul of a 3D graphics
system. Most of the rest of the text is based upon real numbers and spaces
such as R2 and R3. They are the most ﬂexible of the number systems we
have described in this chapter and, not surprisingly, the most complicated
and problematic to represent on a computer. We will present two different
methods that are used to represent real numbers on computers today and will
include numerous sections describing common issues that arise from the use
of these representations in real-world applications.
All of the issues relating to storage of integers and whole numbers dis-
cussed thus far will continue to be issues with real number representation.
However, real number representations add additional complexities that will

160
Chapter 4 Real-World Computer Number Representation
result in implementation trade-offs, subtle errors, and difﬁcult to trace
performance issues that can easily confuse the programmer.
4.3.1 Approximations
While computer representations of whole numbers (unsigned int) and inte-
gers (int) are limited to a ﬁnite subset of their pure counterparts, in each case
the ﬁnite set is contiguous; that is, if i and i + 2 are both representable, then
i +1 is also representable. Inside the range deﬁned by the minimum and max-
imum representable integer values, all integers can be represented exactly.
This stems from the earlier observation that any ﬁnitely bounded range of
integers contains a ﬁnite number of elements.
When dealing with real numbers, however, this is no longer true. A sub-
set of real numbers can have inﬁnitely many elements even when bounded
by ﬁnite minimal and maximal values. As a result, no matter how tightly
we bound the range of real numbers (other than the trivial case of Rmin =
Rmax) that we choose to represent, we will be unable to represent that sub-
set of the real numbers exactly. Issues of both range and precision will
thus be constant companions over the course of our discussion of real
number representation. In order to adequately understand the representa-
tions of real numbers, we need to understand the concept of precision and
error.
4.3.2 Precision and Error
For any number representation system, we imagine a generic function Rep(A),
which returns the value in that system that is closest to the value A. In a perfect
representation system, Rep(A) = A for all values of A. When representing
real numbers, however, even limiting range to ﬁnite extremes will not allow
us to represent all numbers in the bounded range exactly. Rep(A) will be a
many-to-one mapping, with inﬁnitely many real numbers A mapping to each
distinct value returned by Rep(A). For each such distinct Rep(A), almost all
values A that map to it will not be represented exactly. In other words, for
almost all real values A, Rep(A) ̸= A. The obvious result in such cases is that
(Rep(A) −A) ̸= 0. The representation in such a case is an approximation of
the actual value.
Making use of (Rep(A) −A), we can deﬁne several derived values that
form metrics of the error induced by representing A in the representation
system. These two kinds of error metrics are called absolute error and relative
error.

4.3 Representing Real Numbers 161
The simplest way to represent error is “absolute error,” which is deﬁned as
AbsError = |Rep(A) −A|
This is simply the “number line” distance between the actual value and
its representation. While this value does correctly signify the difference
between the actual and representative values, it does not quantify another
important factor in representation error — the scale at which the error affects
computation.
To better understand this, imagine a system of measurement that is
accurate to within a kilometer. Such a system might be considered suitably
accurate for measuring the 149,597,871 km between the earth and the sun.
However, it would likely be woefully inaccurate at measuring the size of an
apple (0.00011 km), which would be rounded to 0 km! Intuitively, this is obvi-
ous, but in both cases the absolute error of representation is less than 1 km.
Clearly, absolute error is not sufﬁcient in this case.
Relative error takes the scale of the value being approximated into
account. It does so by dividing the absolute error by the actual value being
represented. Relative error is deﬁned as
RelError =

Rep(A) −A
A

As such, relative error is dimensionless; even if the values being approximated
have units (such as kilometers), the relative error has no units. Due to the
division, relative error cannot be computed for a value that approximates
zero. It is a measure of the ratio of the error to the magnitude of the value
being approximated. Revisiting our previous example, the relative errors in
each case would be (approximately)
RelErrorSun =

1 km
149, 597, 871 km
 ≈7 × 10−9
RelErrorApple =

0.00011 km
0.00011 km
 = 1.0
Clearly, relative error is a much more useful error metric in this case. The
earth–sun distance error is tiny (compared to the distance being measured),
while the size of the apple was estimated so poorly that the error had the
same magnitude as the actual value. In the former case a relatively “exact”
representation was found, while in the latter case the representation is all
but useless.

162
Chapter 4 Real-World Computer Number Representation
4.4 Fixed Point
4.4.1 Introduction
Much is made of the performance of hardware ﬂoating point units (FPUs)
in modern desktop processors and full-sized game consoles. The basic use of
ﬂoating point numbers is familiar to even the novice programmer. However,
ﬂoating point is not the only way that real numbers are approximated on com-
puters. In fact, for decades another representation, ﬁxed point, was far more
popular owing to its high performance and accuracy when used correctly,
even on low-powered computers.
Over the past decade, ﬁxed point numbers have become somewhat of a
“lost art” to all but the most hardcore, experienced 3D programmers. For
example, 3D PC games written in the late 1990s and beyond tended to use
ﬂoating point heavily (if not exclusively). However, the popularity of powerful
handheld computers and cellular telephones has brought ﬁxed-point arith-
metic back to the forefront of 3D game development. With the constant
pressure on hardware manufacturers to make smaller, lower-cost embedded
chips, it is likely that there will be a need for ﬁxed point code in handheld 3D
games for some time to come. This trend alone has caused discussion of ﬁxed
point number representation and computation among 3D game programmers
to be extremely relevant once again.
4.4.2 Basic Representation
Fixed point numbers are a method of representing a subset of the real numbers
on a computer. Fixed point numbers are based upon the computer representa-
tion of integers. In fact, as we shall see, integers can be thought of as a special
case of ﬁxed point. Like the computer representations of integers upon which
they are built, ﬁxed point numbers are ﬁnite. As such, they cannot represent
the entire set of real numbers. However, the range and precision limitations
of ﬁxed point numbers are very simple, making them easy to describe and
analyze.
Fixed point numbers are based on a very simple observation with respect
to computer representation of integers. In the standard binary representa-
tion, each bit represents twice the value of the bit to its right, with the least
signiﬁcant bit representing 1. The following diagram shows these powers of
two for a standard 8-bit unsigned value:
27
26
25
24
23
22
21
20
128
64
32
16
8
4
2
1

4.4 Fixed Point 163
Just as a decimal number can have a decimal point, which represents
the break between integral and fractional values, a binary value can have a
binary point, or more generally a radix point (a decimal number is referred
to as radix 10, a binary number as radix 2). In the previous number layout,
we can imagine the radix point being to the right of the last digit. However,
it does not have to be placed there. For example, let us revisit the previous
case, this time placing the radix point in the middle of the number (between
the fourth and ﬁfth bits). The diagram would then look like this:
23
22
21
20 . 2−1
2−2
2−3
2−4
8
4
2
1 . 1
2
1
4
1
8
1
16
Now, the least signiﬁcant bit represents 1/16. The basic idea behind ﬁxed
point is one of scaling. A ﬁxed point value is related to an integer with the
same bit pattern by an implicit scaling factor. This scaling factor is ﬁxed for
a given ﬁxed point format and is the value of the least signiﬁcant bit in the
representation. In the case of the preceding format, the scaling factor is 1/16.
The standard nomenclature for a ﬁxed point format is “M-dot-N,” where
M is the number of integral bits (to the left of the radix point) and N is the
number of fractional bits (to the right of the radix point). For example, the
8-bit format in our example would be referred to as “4-dot-4.” As a further
example, regular 32-bit integers would be referred to as “32-dot-0” because
they have no fractional bits. More generally, the scaling factor for an M-dot-N
format is simply 2−N. Note that, as expected, the scaling factor for a 32-dot-
0 format (integers) is 20 = 1. No matter what the format, the radix point
is “ﬁxed” (or locked) at N bits from the least signiﬁcant bit; thus the name
“ﬁxed point.”
4.4.3 Range and Precision
Computing the range and precision for a given ﬁxed point format is very
easy and can be computed solely by knowing the “M-dot-N” format name.
This simple analysis is made possible by the previously stated fact about the
relationship between ﬁxed point numbers and integers with the same bitwise
representation. For any ﬁxed point number viewed directly as an integer, we
compute the ﬁxed point number’s value in M-dot-N format by multiplying the
integer by a scaling factor equal to 2−N.
To compute the range of an M-dot-N format, we recall the earlier dis-
cussion regarding 2’s complement integers. We know that a 2’s complement
integer with B bits has range [−(2B−1), 2B−1 −1]. Since the total number

164
Chapter 4 Real-World Computer Number Representation
of bits in a ﬁxed-point representation is M + N, this leads to a ﬁxed-point
range of

−(2M+N−1) × 2−N, (2M+N−1 −1) × 2−N
−(2M+N−1)
2N
, (2M+N−1 −1)
2N


−(2M−1),

2M−1 −1
2N

The precision of the representation can be computed just as simply. When
dealing with integers, the spacing between each integer and its nearest neigh-
bor is simply 1.0. Multiplying by the ﬁxed point format’s scaling factor, we ﬁnd
that the difference between any M-dot-N number and its closest neighbor is
1.0 × 2−N = 1
2N
Given this ﬁxed distance between any dot-N ﬁxed point value and its closest
representable neighbor, we know that any real number A within the valid
range for the preceding M-dot-N format is, at worst, different from its repre-
sentation by half the distance between the values directly above and below A.
So, A can be represented with an absolute error of at most
AbsErrorA = |Rep(A) −A| ≤1
2N × 1
2 =
1
2N+1
This absolute error bound is constant across the range of the format. On the
other hand, the relative error bound is
RelErrorA =

Rep(A) −A
A
 ≤AbsErrorA
|A|
=
1
|A| × 2N+1
which rises sharply as A tends toward zero. In other words, with a ﬁxed-point
system the relative error falls as magnitudes increase. This leads to the basic
guideline that an application must determine how small its smallest values
can become and set the fractional precision based on this quantum.
Converting between Real and Fixed-Point
Converting a real number R to an M-dot-N ﬁxed-point number F can be
accomplished via the following method:
F = round(R × 2N)

4.4 Fixed Point 165
where round is the function used to round a real number to the nearest integer.
Basically, this method scales the real number to the correct scaling value for
the ﬁxed point format, and then rounds away any precision beyond what can
be represented in the given format. For example, to convert the value 4.5 to
our 4-dot-4 format, we do the following:
F = round(4.5 × 24)
= round(4.5 × 16)
= round(72)
= 72
72 =
0
1
0
0 . 1
0
0
0
which represents 4.5 exactly. Note that in this case, the round operation had
no effect. If the round operation had changed the value, then this would indi-
cate that the M-dot-N formula could not represent the given value exactly.
An example of such a case is 3.7 represented in 4-dot-4 format:
F = round(3.7 × 24)
= round(59.2)
= 59
59 =
0
0
1
1 . 1
0
1
1
which represents the rational value 3.6875. The absolute error of representa-
tion in this case is 3.7 −3.6875 = 0.0125. This is much less than the maximum
possible absolute error in 4-dot-4 format, which is 1/25 = 0.03125.
It is very important that the rounding step be done after the scaling, or
else the real number will be rounded to an integer and all of the fractional
precision will be lost (the N least-signiﬁcant bits of the resulting ﬁxed point
value will be zeros). Note that real numbers outside of the range of the ﬁxed
point format will overﬂow during the conversion to the integer format and
must be considered invalid.
Converting back to the desired real number is as simple as treating the
ﬁxed point number’s integer representation as a real number (in C/C++, this
is simply a typecast) and then scaling that real number by the 1/2N scaling
factor.
The conversion methods between integers and ﬁxed point values are even
simpler. This is due to the fact that the scaling factors of the ﬁxed point formats

166
Chapter 4 Real-World Computer Number Representation
we have discussed are all powers of two. Multiplying an integer by 2N is equiv-
alent to shifting that integer to the left by N bits. Shifting is an operation that
is supplied by the integer math units (arithmetic logic units, or ALUs) of all
major CPUs and is extremely fast (free on some CPUs when done at the same
time as another math operation). Dividing an integer by 2N is equivalent to
shifting the integer to the right by N bits.
We can use these fast special cases of multiplication and division in our
integer/ﬁxed point conversion. The conversion from integer to ﬁxed point can
never lose precision (although it will overﬂow if the integer is not in the range
of the ﬁxed point format) and is implemented by shifting the integer to the
left by N bits. The conversion from ﬁxed point to integer will never overﬂow
but often loses precision and is implemented by shifting the integer to the
right by N bits. Note that shifting to the right truncates the number. In 2’s
complement, this computes the ﬂoor of the number; it does not round it.
In order to round during the conversion to integer, we must ﬁrst add 2N−1
(which is equal to 1/2 in the M-dot-N format) and then shift the result to the
right by N bits. The addition of 1/2 prior to the shift turns the truncation into
a form of rounding.
4.4.4 Addition and Subtraction
Addition and subtraction of two ﬁxed point numbers of the same format is
extremely simple — they are merely the integer versions of addition and sub-
traction. This is possible because two M-dot-N ﬁxed point numbers have
radix points that line up (just like standard integers). A 4-dot-4 example
follows:
Bits
Integer
Real
0
0
0
1 . 0
0
0
0
[16]
(1.0)
+
0
0
0
0 . 1
1
0
0
[12]
(0.75)
0
0
0
1 . 1
1
0
0
[28]
(1.75)
4.4.5 Multiplication
The simplicity of addition and subtraction may lead one to hope that multi-
plication and division of ﬁxed point numbers are equally simple. However,
a quick example shows a problem with this method. For example, let us
convert 0.5 and 0.25 into 4-dot-4 ﬁxed point and multiply them together.

4.4 Fixed Point 167
We expect a result of 0.125. First, we convert the real numbers to 4-dot-4
ﬁxed point:
0.5 →8 =
0
0
0
0 . 1
0
0
0
0.25 →4 =
0
0
0
0 . 0
1
0
0
Next, we multiply them using the standard integer method:
Bits
Integer
Real
0
0
0
0 . 1
0
0
0
[8]
(0.5)
×
0
0
0
0 . 0
1
0
0
[4]
(0.25)
Incorrect!
0
0
1
0 . 0
0
0
0
[32]
(2.0)
The result is clearly incorrect and just as clearly (given the magnitude of
the error) not simple rounding error. However, there is a clear reason for
this error. Fixed point numbers are not equivalent to their bitwise represen-
tation as integers, but rather as their integer representation times the 1/2N
scaling value. Thus, the integer bits represent the real number times 2N. If we
recompute the multiplication just done adding these scale values, we ﬁnd the
following:
(0.5 × 24) × (0.25 × 24) = (0.125 × 24) × 24
The problem is that each of the two operands brings its own implicit scale
value. Multiplying these together causes the result to be too large by exactly
the scaling factor. Thus, to reestablish the correct ﬁxed point format, we must
divide the result by 24:
(0.5 × 24) × (0.25 × 24)
24
= (0.125 × 24)
This method generalizes as follows: to multiply two M-dot-N ﬁxed point
numbers, we multiply their representations using the integer multiplication
method and then divide the result by 2N. Using the same observation as we
did for integer/ﬁxed-point conversion, we replace the division by 2N with an
N-bit right shift (written as ≫N) of the result of the integer multiplication.
This gives a method for multiplying two M-dot-N numbers AM.N and BM.N of
(AM.N × BM.N) ≫N

168
Chapter 4 Real-World Computer Number Representation
Next,
we
show
this
multiplication
method
graphically,
computing
1.0 × 0.375 = 0.375:
Bits
Integer
Real
0
0
0
1 . 0
0
0
0
[16]
(1.0)
×
0
0
0
0 . 0
1
1
0
[6]
(0.375)
0
1
1
0 . 0
0
0
0
[96]
≫
4 Bits
0
0
0
0 . 0
1
1
0
[6]
(0.375)
4.4.6 Division
Fixed point division suffers from another issue of scale correction but in the
inverse manner to what happened with multiplication. Rather than ending
up with two scale values multiplying together and requiring correction, in
the case of division, the scale values cancel out, and the result is represented
as an integer with no fractional precision. We’ll demonstrate with the same
numbers used in our multiplication example:
0.5 →8 =
0
0
0
0 . 1
0
0
0
0.25 →4 =
0
0
0
0 . 0
1
0
0
We attempt to divide one by the other using the standard integer method:
Bits
Integer
Real
0
0
0
0 . 1
0
0
0
[8]
(0.5)
/
0
0
0
0 . 0
1
0
0
[4]
(0.25)
Incorrect!
0
0
0
0 . 0
0
1
0
[2]
(0.125)
In this case, we can see that the scale values have canceled:
0.5 × 24
0.25 × 24 = 0.5
0.25 = 2 = (0.125 × 24)
To reestablish the correct ﬁxed point format, we could multiply the result
by 24. However, the problem with such a method is that the result would have

4.4 Fixed Point 169
no fractional precision (a 4-dot-4 number times 24 is always an integer)! The
precision was lost in the division. To avoid this loss of precision, we must
multiply the dividend by 24:
0.5 × 24 × 24
0.25 × 24
= 0.5 × 24
0.25
= 2 × 24 = (2.0 × 24)
This division method generalizes as follows: to divide one M-dot-N ﬁxed-point
number by another, we multiply the dividend by 2N and then divide their rep-
resentations using the integer division method. Using the same observation as
we did for integer/ﬁxed-point conversion, we replace the multiplication by 2N
by shifting the dividend to the left (written ≪) by N bits. This gives a method
for dividing two M-dot-N numbers AM.N and BM.N of
(AM.N ≪N)
(BM.N)
We show this method graphically, computing 0.25/2.0 = 0.125:
Bits
Integer
Real
0
0
0
0 . 0
1
0
0
[4]
(0.25)
≪
4 Bits
0
1
0
0 . 0
0
0
0
[64]
/
0
0
1
0 . 0
0
0
0
[32]
(2.0)
0
0
0
0 . 0
0
1
0
[2]
(0.125)
4.4.7 Real-World Fixed Point
The astute reader will note that the preceding examples of ﬁxed point opera-
tions were rather contrived. In fact, it would have been very easy to generate
examples of the algorithms discussed thus far that caused overﬂow and incor-
rect results. The very nature of ﬁxed point numbers, the fact that even small
real values are represented by large integers (e.g., the real value 1.0 being
represented by 65,536 in 16-dot-16) can lead to overﬂow in surprisingly low-
magnitude situations. Additionally, while ﬁxed point has been presented as a
fast alternative to ﬂoating point on integer-only platforms, it is apparent that
there is likely to be some performance penalty for the additional shifting that
is required in many ﬁxed point operations.
In fact, several modern processors take both of these issues into account,
offering features that can assist the programmer. In one case, such a feature

170
Chapter 4 Real-World Computer Number Representation
makes the extra shifting operations computationally inexpensive (or even
free). In another, a set of extra instructions makes overﬂow far less of a
problem. Details of these situations, as well as both platform-dependent
and platform-independent methods of dealing with them, are covered in the
following section.
4.4.8 Intermediate Value Overflow and
Underflow
The basic method for ﬁxed point multiplication discussed thus far requires
that the intermediate multiplied value be shifted downward in magnitude to
reestablish the correct position of the radix point. A side effect of this method
is that the intermediate value can overﬂow, even if the ﬁnal result should be
well within the range of the ﬁxed point format. As an introductory example,
we demonstrate 1.0 × 1.0 in 4-dot-4 ﬁxed point. Clearly, the result should be
1.0, obviously within range:
Bits
Integer
Real
0
0
0
1 . 0
0
0
0
[16]
(1.0)
×
0
0
0
1 . 0
0
0
0
[16]
(1.0)
Overﬂow!
0
0
0
0 . 0
0
0
0
[256 = 0]
(0.0)
≫
4 Bits
Incorrect!
0
0
0
0 . 0
0
0
0
[0]
(0.0)
Even a simple multiplication can result in overﬂow in the intermediate
value. Seeing this situation, one might be tempted to simply move the shift
operation up in the process, shifting ﬁrst and then multiplying the preshifted
result. For example, in our 4-dot-4 case imagine a method in which each
operand was preshifted by two bits prior to the multiplication. Multiplying
1.0 × 1.0 = 1.0:
Bits
Integer
Real
0
0
0
1 . 0
0
0
0
[16]
(1.0)
≫
2 Bits
0
0
0
0 . 0
1
0
0
[4]
(0.25)
×
0
0
0
1 . 0
0
0
0
[16]
(0.25)

4.4 Fixed Point 171
≫
2 Bits
0
0
0
0 . 0
1
0
0
[4]
(0.25)
0
0
0
1 . 0
0
0
0
[16]
(1.0)
Success! However, blindly preshifting operands can result in problems as
well. This same method nets much less satisfying results with the following
case of 7.0 × 0.125 = 0.875. Note what happens to the second operand when
preshifted:
Bits
Integer
Real
0
0
0
0 . 0
0
1
0
[2]
(0.125)
≫
2 Bits
0
0
0
0 . 0
0
0
0
[0]
(0.0)
With one of the operands being truncated to zero, the result of the multi-
plication will be zero! This is quite a sizable error. Clearly, no single shifting
method can satisfactorily deal with all precision and overﬂow cases.
Extended Precision Hardware Assistance
The method that is perhaps the best at dealing with overﬂow and underﬂow
requires some outside assistance on the part of the hardware platform. This
assistance comes in the form of extended-precision mathematical opera-
tions. Such instructions are based on the fact that two N-bit numbers when
multiplied together can require up to 2N bits to avoid overﬂow. Numerous
modern processors (especially those without ﬂoating point units, such as
the ARM architecture [97]) include either 16-bit multiplication operations
with 32-bit results or 32-bit multiplication operations with 64-bit results.
Such operations are practically (if not speciﬁcally) tailor-made for ﬁxed point
implementations.
For example, imagine our 4-dot-4 (8-bit) ﬁxed point system. As we demon-
strated, if we multiply 1.0 × 1.0 using the direct method, the operation’s
intermediate value will overﬂow, leaving an incorrect result. However, with
an 8-bit × 8-bit =⇒16-bit instruction, even the larger-magnitude operation
2.0×2.0 can be completed using the direct method. Note that the 16-bit inter-
mediate result is actually the correct answer as well, but in 8-dot-8 format:
0
0
1
0 . 0
0
0
0
(2.0)
×
0
0
1
0 . 0
0
0
0
(2.0)
0
0
0
0
0
1
0
0 . 0
0
0
0
0
0
0
0

172
Chapter 4 Real-World Computer Number Representation
≫4 Bits
0
1
0
0 . 0
0
0
0
(4.0)
It is important to note that these extended-precision operations do not
actually extend the values that can be represented in a given ﬁxed point for-
mat. If the result cannot be represented in the ﬁnal format, the only option
would be to change the format used or else reformulate the problem. In the
following example, the intermediate result can be represented in the 16-dot-
16 intermediate format, but the ﬁnal result is truncated incorrectly and ends
up overﬂowing:
0
1
1
0 . 0
0
0
0
(6.0)
×
0
1
1
1 . 0
0
0
0
(7.0)
0
0
1
0
1
0
1
0 . 0
0
0
0
0
0
0
0
(42.0)
≫4 Bits
0
0
0
0
0
0
1
0 . 1
0
1
0
0
0
0
0
(42.0)
Overﬂow – Incorrect!
1
0
1
0 . 0
0
0
0
(−6.0)
In this case the extended precision was not the answer, as the ﬁnal result still
had to be converted to the shorter format. What extended precision does avoid
is overﬂow in intermediate results. This, in turn, avoids the need to preshift
precision from the operands, avoiding unnecessary underﬂow. With careful
programming, these instructions can be used for strings of operations, with
the conversion from the long format to the original format happening once
at the end. A common instruction form that is given on extended-precision
hardware is an extended-precision multiply-accumulate, as follows:
32-bit × 32-bit + 64-bit =⇒64-bit
An extended-precision multiply-accumulate instruction is useful for
quickly computing a safe, precise 16-dot-16 ﬁxed point dot product. We
assume that we begin with the pair of 3-vectors (X1, Y1, Z1) and (X2, Y2, Z2):
1. X1 × X2 =⇒Accumulator64
2. Y1 × Y2 + Accumulator64 =⇒Accumulator64
3. Z1 × Z2 + Accumulator64 =⇒Accumulator64
4. Accumulator64 ≫16 =⇒Result32

4.5 Floating-Point Numbers 173
4.4.9 Limits of Fixed Point
To better understand the signiﬁcant limitations of ﬁxed point, even with
hardware-assisted extended-precision, we shall consider one of the most pop-
ular ﬁxed point formats in general ﬁxed point libraries, the 32-bit signed
16-dot-16 format. Although applications can pick a wide range of formats,
and can even use multiple formats in the same application, 16-dot-16 is rep-
resentative. We can summarize the minimum and maximum representable
values in this format, as well as the value of epsilon (ϵ), the distance between
adjacent representable values as follows:
Maximum representable value:
Max16.16 ≈32767
Minimum representable value:
Min16.16 ≈−32768
Smallest positive value:
Eps16.16 ≈1.5 × 10−5
While these may seem like large amounts of range and precision (and
indeed they are), it should be noted that √Max16.16 ≈181, and

Eps16.16 ≈
0.004. In other words, if the application needs to store the value a · b, then
for safety ∥a∥< 181 and ∥b∥< 181 to avoid overﬂow. Similarly, to avoid
underﬂow, ∥a∥> 0.004 and ∥b∥> 0.004.
These suddenly begin to seem like signiﬁcantly tighter limitations. While
issues such as these can be overcome by careful scaling of the data throughout
the application, the simple fact is that ﬁxed point requires that programmers
keep a very close bound over the required range and precision values of their
data throughout the entire application. If the application happens to be a
complete and general 3D pipeline, this can be a rather daunting task.
4.4.10 Fixed Point Summary
In today’s applications the decision of whether or not to use ﬁxed point is more
often than not based entirely on the application’s target platform capabilities.
Few modern software 3D pipelines choose to use ﬁxed point unless their target
platform has no ﬂoating point hardware. However, on platforms with fast
integer ALUs and no FPUs, ﬁxed point can make the difference between a high-
performance, compelling 3D experience and a non-interactive, low frame rate
“slide show.”
4.5 Floating-Point Numbers
4.5.1 Review: Scientific Notation
In order to better introduce ﬂoating point numbers, it is instructive to review
the well-known standard representation for real numbers in science and

174
Chapter 4 Real-World Computer Number Representation
engineering: scientiﬁc notation. Computer ﬂoating point is very much analo-
gous to scientiﬁc notation.
Scientiﬁc notation (in its strictest, so-called normalized form) consists of
two parts:
1. A decimal number, called the mantissa, such that
1.0 ≤|mantissa| < 10.0
2. An integer, called the exponent
Together, the exponent and mantissa are combined to create the number:
mantissa × 10exponent
Any decimal can be represented in this notation (other than 0, which is
simply represented as 0.0), and the representation is unique for each number.
In other words, for two numbers written in this form of scientiﬁc notation,
the numbers are equal if and only if their mantissas and exponents are equal.
This uniqueness is a result of the requirements that the exponent be an integer
and that the mantissa be “normalized” (i.e., have magnitude between 1.0 and
10.0). Examples of numbers written in scientiﬁc notation include
102 = 1.02 × 102
243,000 = 2.43 × 105
−0.0034 = −3.4 × 10−3
Examples of numbers that constitute incorrect scientiﬁc notation include
Incorrect = Correct
11.02 × 103 = 1.102 × 104
0.92 × 10−2 = 9.2 × 10−3
4.5.2 A Restricted Scientific Notation
To further restrict the standard scientiﬁc notation, we will use a special
restricted scientiﬁc notation, purely for the purpose of introducing the
concept of ﬁniteness of representation. We extend the rules for scientiﬁc
notation:
1. The mantissa must be written with a single, nonzero integral digit.
2. The mantissa must be written with a ﬁxed number of fractional digits
(we deﬁne as M).

4.5 Floating-Point Numbers 175
3. The exponent must be written with a ﬁxed number of digits (we
deﬁne as E).
4. The mantissa and the exponent each have individual signs.
For example, the following number is in a format with M = 3, E = 2:
±1.1 2 3 × 10±1 2
Limiting the number of digits allocated to the mantissa and exponent
means that any value that can be represented by this system can be repre-
sented uniquely by six decimal digits and two signs. However, this also implies
that there are a limited number of values that could ever be represented exactly
by this system, namely:
(exponents) × (mantissas) × (exponent signs) × (mantissa signs)
= (102) × (9 × 103) × (2) × (2)
= 3,600,000
Note that the leading digit of the mantissa must be nonzero (since the
mantissa is normalized), so that there are only nine choices for its value [1, 9],
leading to 9 × 10 × 10 × 10 = 9000 possible mantissas.
This adds ﬁniteness to both the range and precision of the notation. The
minimum and maximum exponents are
±(10E −1) = ±(102 −1) = ±99
The largest mantissa value is
10.0 −(10−M) = 10.0 −(10−3) = 10.0 −0.001 = 9.999
Note that the smallest allowed nonzero mantissa value is still 1.000 due to
the requirement for normalization. This format has the following numerical
limitations:
Maximum representable value:
9.999 × 1099
Minimum representable value: −9.999 × 1099
Smallest positive value:
1.000 × 10−99
While one might never use such a restricted form of scientiﬁc notation in
practice, it demonstrates the basic building blocks of binary ﬂoating point,
the most commonly used computer representation of real numbers in modern
computers.

176
Chapter 4 Real-World Computer Number Representation
4.6 Binary “Scientific Notation”
There is no reason that scientiﬁc notation must be written in base-10. In
fact, in its most basic form, the real number representation known as ﬂoating
point is similar to a base-2 version of the restricted scientiﬁc notation given
previously. In base-2, our restricted scientiﬁc notation would become
SignM × Mantissa × 2SignE × Exponent
where Mantissa is a 1-dot-M ﬁxed-point number that is normalized, Exponent
is an E-bit integer, and SignM and SignE are independent bits representing
the signs of the mantissa and exponent, respectively. Put together, the format
involves M + E + 3 bits (M + 1 for the mantissa, E for the exponent, and two
for the signs). Creating an example that is analogous to the preceding decimal
case, we analyze the case of M = 3, E = 2:
±1.0 1 0 × 2±0 1
Any value that can be represented by this system can be represented
uniquely by 8 bits. The number of values that could ever be represented exactly
by this system is
(exponents) × (mantissas) × (exponent signs) × (mantissa signs)
= (22) × (1 × 23) × (2) × (2)
= 27 = 128
This seems odd, as an 8-bit number should have 256 different values.
However, note that the leading bit of the mantissa must be 1, since the man-
tissa is normalized (and the only choices for a bit’s value are 0 and 1). This
effectively ﬁxes one of the bits and cuts the number of possible values in
half. We shall see that the most common binary ﬂoating-point format takes
advantage of the fact that the integral bit of the mantissa is ﬁxed at 1.
In this case, the minimum and maximum exponents are
±(2E −1) = ±(22 −1) = ±3
The largest mantissa value is
2.0 −2−M = 2.0 −2−3 = 1.875
This format has the following numerical limitations:
Maximum representable value:
1.875 × 23 = 15
Minimum representable value: −1.875 × 23 = −15
Smallest positive value:
1.000 × 2−3 = 0.125

4.7 IEEE 754 Floating-Point Standard 177
From the listed limits, it is quite clear that a ﬂoating point format based
on this simple 8-bit binary notation would not be useful to most real-world
applications. However, it does introduce the basic concepts that are shared by
real ﬂoating point representations. While there are countless possible ﬂoat-
ing point formats, the universal popularity of a single set of formats (those
described in the IEEE 754 speciﬁcation [2]) makes it the obvious choice for
any discussion of the details of ﬂoating point representation. The remainder
of this chapter will explain the major concepts of ﬂoating point representation
as evidenced by the IEEE standard format.
4.7 IEEE 754 Floating-Point Standard
By the early to mid-1970s, scientists and engineers were using ﬂoating point
very frequently to represent real numbers; at the time, higher-powered
computers even included special hardware to accelerate ﬂoating point cal-
culations. However, these same scientists and engineers were ﬁnding the lack
of a ﬂoating point standard to be problematic. Their complex (and often very
important) numerical simulations were producing different results, depend-
ing only upon the make and model of computer upon which the simulation
was run. Numerical code that had to run on multiple platforms became rid-
dled with platform-speciﬁc code to deal with the differences between different
ﬂoating point processors and libraries.
In order for cross-platform numerical computing to become a reality, a
standard was needed. Over the course of the next decade, a draft standard
for ﬂoating point formats and behaviors became the de facto standard on
most ﬂoating point hardware. Once adopted, it became known as the IEEE
754 ﬂoating point standard [2], and it forms the basis of almost every hardware
and software ﬂoating point system on the market.
While the history of the standard is fascinating [69], this section will focus
on explaining part of the standard itself, as well as using the standard and
one of its speciﬁed formats to explain the concepts of modern ﬂoating-point
arithmetic.
4.7.1 Basic Representation
The IEEE standard speciﬁes a 32-bit “single-precision” format for ﬂoating-
point numbers, as well as a 64-bit “double-precision” format. It is this single-
precision format that is of greatest interest for most games and interactive
applications and is thus the format which will form the basis of most of
the ﬂoating point discussion in this text. The two formats are fundamentally

178
Chapter 4 Real-World Computer Number Representation
similar, so all of the concepts regarding single-precision are applicable to
double-precision values as well.
The following diagram shows the basic memory layout of the IEEE single-
precision format, including the location and size of the three components of
any ﬂoating point system: sign, exponent, and mantissa.
Sign
Exponent
Mantissa
1 Bit
8 Bits
23 Bits
The sign in the IEEE ﬂoating point format is represented as an explicit
bit (the high-order bit). Note that this is the sign of the number itself (the
mantissa), not the sign of the exponent. Differentiating between positive and
negative exponents is handled in the exponent itself (and is discussed next).
The only difference between X and −X in IEEE ﬂoating point is the high-order
bit. A sign bit of 0 indicates a positive number, and a sign bit of 1 indicates a
negative number.
This sign bit format allows for some efﬁciencies in creating a ﬂoating-
point math system either in hardware or software. To negate a ﬂoating-point
number, simply “ﬂip” the sign bit, leaving the rest of the bits unchanged. To
compute the absolute value of a ﬂoating point number, simply set the sign bit
to 0 and leave the other bits unchanged. In addition, the sign bits of the result
of a multiplication or division is simply the exclusive “or” of the sign bits of
the operands.
As will be seen, this explicit sign bit does lead to the existence of two
zero values, one positive and one negative. However, it also simpliﬁes the
representation of the mantissa, which is represented as unsigned (positive).
The exponent in this case is stored as a biased number. Biased numbers
represent both positive and negative integers (inside of a ﬁxed range) as whole
numbers by adding a ﬁxed, positive bias. To represent an integer I, we add
a positive bias B (that is constant for the biased format), storing the result
as the whole number (nonnegative integer) W. To decode the represented
value I from its biased representation W, the formula is simply
I = W −B
To encode an integer value, the formula is
W = I + B
Clearly, the minimum integer value that can be represented is
I = 0 −B = −B

4.7 IEEE 754 Floating-Point Standard 179
The maximal value that can be represented is related to the maximum whole
number that can be represented Wmax. For example, with an 8-bit biased
number, that value is
I = Wmax −B = (28 −1) −B
Most frequently, the bias chosen is as close as possible to Wmax/2, giving a
range that is equally distributed about zero. Over the course of this chapter,
when referring to a biased number, the term value will refer to I, while the
term bits will refer to W.
Such is the case with the IEEE ﬂoating point exponent, which uses 8 bits
of representation and a bias of 127. This would seem to lead to minimum and
maximum exponents of −127 (= 0 −127) and 128 (= 255 −127), respectively.
However, for reasons that will be explained, the minimum and maximum
values (−127 and 128) are reserved for special cases, leading to an exponent
range of [−126, 127]. As a reference, these base 2 exponents correspond to
base 10 exponents of approximately [−37, 38].
The mantissa is normalized (in almost all cases), as in our discussion of
decimal scientiﬁc notation (where the units digit was required to have magni-
tude in the range [1, 9]). However, the meaning of “normalized” in the context
of a binary system means that the leading bit of the mantissa is always 1.
Unlike a decimal digit, a binary digit has only one nonzero value. To optimize
storage in the ﬂoating point format, this leading bit is omitted, or hidden,
freeing all 23 explicit mantissa bits to represent fractional values. To decode
the mantissa M (as a 23-bit unsigned integer) into a rational number (ignoring
for the moment the exponent), the conversion is
1.0 +
M
2.023
So, for example, the mantissa bits
000000000000000000000002 = 010
become the rational number
1.0 +
0
2.023 = 1.0
4.7.2 Range and Precision
The range of single-precision ﬂoating point is by deﬁnition symmetric, as the
system uses an explicit sign bit. With an explicit sign bit, every positive value

180
Chapter 4 Real-World Computer Number Representation
has a corresponding negative value. This leaves the questions of maximal
exponent and mantissa, which when combined will represent the explicit
values of greatest magnitude. In the previous section, we found that the max-
imum base-2 exponent in single-precision ﬂoating point is 127. The largest
mantissa would be equal to setting all 23 explicit fractional mantissa bits,
resulting (along with the implicit 1.0 from the hidden bit) in a mantissa of
1.0 +
23

i=1
1
2i = 1.0 + 1.0 −1
223 = 2.0 −1
223 ≈2.0
The minimum and maximum single-precision ﬂoating-point values are then
±

2.0 −1
223

× 2127 ≈±3.402823466 × 1038
The precision of single-precision ﬂoating point can be loosely approxi-
mated as follows: for a given normalized mantissa, the difference between it
and its nearest neighbor is 2−23. To determine the actual spacing between
a ﬂoating-point number and its neighbor, the exponent must be known.
Given an exponent E, the difference between two neighboring single-precision
values is
δfp = 2E × 2−23 = 2E−23
However, we note that in order to represent a value A in single-precision,
we must ﬁnd the exponent EA such that the mantissa is normalized (i.e., the
mantissa MA is in the range 1.0 ≤MA < 2.0), or
1.0 ≤|A|
2EA < 2.0
Multiplying through, we can bound |A| in terms of 2EA:
1.0 ≤|A|
2EA < 2.0
2EA ≤|A| < 2EA × 2.0
2EA ≤|A| < 2EA+1
As a result of this bound, we can roughly approximate this entire exponent
term 2EA with |A| and substitute to ﬁnd an approximation of the distance

4.7 IEEE 754 Floating-Point Standard 181
between neighboring ﬂoating-point values around |A| (δfp) as
δfp = 2EA−23 = 2EA
223 ≈|A|
223
From our initial discussion of absolute error, we use general bound on the
absolute error equal to half the distance between neighboring representation
values:
AbsErrorA ≈δfp × 1
2 = |A|
223 × 1
2 = |A|
224
This approximation shows that the absolute error of representation in a
ﬂoating-point number is directly proportional to the magnitude of the value
being represented. Having approximated the absolute error, we can approxi-
mate the relative error as
RelErrorA = AbsErrorA
|A|
≈
|A|
224 × |A| =
1
224 ≈6 × 10−8
The relative error of representation is thus generally constant, regardless of
the magnitude of A. This is the reverse of ﬁxed point, where the absolute error
was constant, and the relative error rose in inverse proportion to the values
represented. Note that for normalized mantissas, this is not true when the
value is very close to zero. This will be discussed in detail later.
4.7.3 Arithmetic Operations
The next several sections discuss the basic methods used to perform common
arithmetic operations upon ﬂoating point numbers. While few users of ﬂoat-
ing point will ever need to implement these operations at a bitwise level
themselves, a basic understanding of the methods is a pivotal step toward
being able to understand the limitations of ﬂoating point. The methods shown
are designed for ease of understanding and do not represent the actual,
optimized algorithms that are implemented in hardware.
The IEEE standard speciﬁes that the basic ﬂoating point operations of
a compliant ﬂoating point system must return values that are equivalent to
the result computed exactly and then rounded to the available precision. The
following sections are designed as an introduction to the basics of ﬂoating-
point operations and do not discuss the exact methods used for rounding the
results. At the end of the section, there is a discussion of the programmer-
selectable rounding modes speciﬁed by the IEEE standard.

182
Chapter 4 Real-World Computer Number Representation
The intervening sections include information regarding common issues
that arise from these operations, because each operation can produce
problematic results in speciﬁc situations.
Addition and Subtraction
In order to add a pair of ﬂoating point numbers, the mantissas of the two
addends must ﬁrst be shifted such that their radix points are “lined up.” In a
ﬂoating point number, the radix points are aligned if and only if their expo-
nents are equal. If we raise the exponent of a number by one, we must shift its
mantissa to the right by one bit. For simplicity, we will ﬁrst discuss addition
of a pair of positive numbers. The standard ﬂoating point addition method
works (basically) as follows to add two positive numbers A = SA × MA × 2EA
and B = SB × MB × 2EB, where SA = SB = 1.0 due to the current assumption
that A and B are nonnegative.
1. Swap A and B if needed so that EA ≥EB.
2. Shift MB to the right by EA −EB bits. If EA ̸= EB, then this shifted
MB will not be normalized — MB will be less than 1.0. This is needed
to align the radix points.
3. Compute MA+B by adding the shifted mantissas MA and MB directly.
4. Set EA+B = EA.
5. The resulting mantissa MA+B may not be normalized (it may have an
integral value of 2 or 3). If this is the case, shift MA+B to the right one
bit and add 1 to EA+B.
Note that there are some interesting special cases implicit in this method.
For example, we are shifting the smaller number’s mantissa to the right to
align the radix points. If the two numbers differ in exponents by more than
the number of mantissa bits, then the smaller number will have all of its
mantissa shifted away, and the method will add zero to the larger value. This is
important to note, as it can lead to some very strange behavior in applications.
Speciﬁcally, if an application repeatedly adds a small value to an accumulator,
as the accumulator grows, there will come a point at which adding the small
value to the accumulator will result in no change to the accumulator’s value
(the delta value being added will be shifted to zero each iteration)!
Floating point addition must take negative numbers into account as well.
There are three distinct cases here:
■Both operands positive. Add the two mantissas as is and set the result
sign to positive.

4.7 IEEE 754 Floating-Point Standard 183
■Both operands negative. Add the two mantissas as is and set the result
sign to negative.
■One positive operand, one negative operand. Negate (2’s complement)
the mantissa of the negative number and add.
In the case of subtraction (or addition of numbers of opposite sign), the
result may have a magnitude that is signiﬁcantly smaller than either of the
operands, including a result of zero. If this is the case, there may be consider-
able shifting required to reestablish the normalization of the result, shifting
the mantissa to the left (and shifting zeros into the lowest-precision bits) until
the integral bit is 1. This shifting can lead to precision issues (see Section 4.7.6,
Catastrophic Cancellation) and can even lead to nonzero numbers that cannot
be represented by the normalized format discussed so far (see Section 4.7.5,
Very Small Values).
We have purposefully omitted discussion of rounding, as rounding the
result of an addition is rather complex to compute quickly. This complexity is
due to the fact that one of the operands (the one with the smaller exponent)
may have bits that are shifted out of the operation, but still must be considered
to meet the IEEE standard of “exact result, then rounded.” If the method
were simply to ignore the shifted bits of the smaller operand, the result could
be incorrect. You may want to refer to [63] for details on the ﬂoating point
addition algorithm.
Multiplication
Multiplication is actually rather straightforward with IEEE ﬂoating point
numbers. Once again, the three components that must be computed are the
sign, the exponent, and the mantissa. As in the previous section, we will give
the example of multiplying two ﬂoating point numbers, A and B.
Owing to the fact that an explicit sign bit is used, the sign of the result may
be computed simply by computing the exclusive-OR of the sign bits, producing
a positive result if the signs are equal and a negative result otherwise. The
result of the multiplication algorithm is sign-invariant.
To compute the initial exponent (this initial estimate may need to be
adjusted at the end of the method if the initial mantissa of the result is not
normalized), we simply sum the exponents. However, since both EA and EB
contain a bias value of 127, the sum will contain a bias of 254. We must subtract
127 from the result to reestablish the correct bias:
EA×B = EA + EB −127
To compute the result’s mantissa, we multiply the normalized source
mantissas MA and MB as 1-dot-23 format ﬁxed-point numbers, producing

184
Chapter 4 Real-World Computer Number Representation
a (possibly unnormalized) 3-dot-46 result mantissa. Note from the format
that the number of integral bits may be 3, as the resulting mantissa could
be rounded up to 4.0. Since the source mantissas are normalized, then the
resulting mantissa (if it is not 0) must be ≥1.0, leading to three possibilities
for the mantissa MA×B: it may be normalized, it may be too large by one bit,
or it may be too large by two bits. In the latter two cases, we add either 1 or 2
to EA×B and shift MA×B to the right by one or two bits until it is normalized.
Rounding Modes
The IEEE speciﬁcation deﬁnes four rounding modes that an implementation
must support. These rounding modes are
■Round toward 0
■Round toward −∞
■Round toward ∞
■Round toward nearest
The speciﬁcation deﬁnes these modes with speciﬁc references to bitwise
rounding methods that we will not discuss here, but the basic ideas are quite
simple. We break the mantissa into the part that can be represented (the
leading 1 along with the next 23 most-signiﬁcant bits), which we call M, and
the remaining lower-order bits, which we call R. Round toward 0 is also
known as “chopping” and is the simplest to understand; in this mode, M is
used and R is simply ignored, or “chopped off.” Round toward ±∞are modes
that round toward positive (∞) or negative (−∞) based on the sign of the
result and whether R = 0 or not, as shown in the following table:
Mode
Round toward −∞
Round toward ∞
M and R
R = 0
R ̸= 0
R = 0
R ̸= 0
M ≥0
M
M
M
M + 1
M < 0
M
M + 1
M
M
4.7.4 Special Values
One of the most important parts of the IEEE ﬂoating point speciﬁcation is its
deﬁnition of numerous special values. While these special values co-opt bit
patterns that would otherwise represent speciﬁc ﬂoating point numbers, this

4.7 IEEE 754 Floating-Point Standard 185
trade-off is accepted as worthwhile, owing to the nature and importance of
these special values.
Zero
The representation of 0.0 in ﬂoating point is more complex than one might
think. Since the high-order bit of the mantissa is assumed to be 1 (and
has no explicit bit in the representation), it is not enough to simply set the
23 explicit mantissa bits to zero, as that would simply represent the number
1.0 × 2Exponent−127. It is necessary to deﬁne zero explicitly, in this case as a
number whose exponent bits are all 0 and whose explicit mantissa bits are 0.
This is sensible, as this value would otherwise represent the smallest possible
normalized value. Note that the exponent bits of 0 map to an exponent value of
−127, which is reserved for special values such as zero. All other numbers with
exponent value −127 (i.e., those with nonzero mantissa bits) are reserved for a
class of very small numbers called “denormals,” which will be described later.
Another issue with respect to ﬂoating point zero arises from the fact that
IEEE ﬂoating point numbers have an explicit sign bit. The IEEE speciﬁcation
deﬁnes both positive and negative 0, differentiated by only the sign bit. To
avoid very messy code, the speciﬁcation does require that ﬂoating point com-
parisons of positive zero to negative zero return “equal.” However, the bitwise
representations are distinct, which means that applications should never use
bitwise equality tests with ﬂoating point numbers! The bitwise representations
of both zeros are
+0.0 =
0 00000000 00000000000000000000000
S Exponent
Mantissa
−0.0 =
1 00000000 00000000000000000000000
S Exponent
Mantissa
The standard does list the behavior of positive and negative zero explicitly,
including the deﬁnitions:
(+0) −(+0) = (+0)
−(+0) = (−0)
Also, the standard deﬁnes the sign of the result of a multiplication or
division operation as negative if and only if exactly one of the signs of the
operands is negative. This includes zeros. Thus,
(+0)(+0) = +0
(−0)(−0) = +0

186
Chapter 4 Real-World Computer Number Representation
(−0)(+0) = −0
(−0)P = −0
(+0)P = +0
(−0)N = +0
(+0)N = −0
where P > 0 and N < 0.
Inﬁnity
At the other end of the spectrum from zero, the standard also deﬁnes positive
inﬁnity (∞fp) and negative inﬁnity (−∞fp), along with rules for the behavior
of these values. In a sense the inﬁnities are not pure mathematical values.
Rather, they are used to represent values that fall outside of the range of valid
exponents. For example, 1.0 × 1038 is just within the range of single-precision
ﬂoating point, but in single-precision:
(1.0 × 1038)2 = 1.0 × 1076 ≈∞fp
The behavior of inﬁnity is deﬁned by the standard as follows (the standard
covers many more cases, but these are representative):
∞fp −P = ∞fp
P
∞fp
= +0
−P
∞fp
= −0
where
0 <P < ∞fp
The bitwise representations of ±∞fp use the reserved exponent value 128
and all explicit mantissa bits zeros. The only difference between the represen-
tations of the two inﬁnities is, of course, the sign bit. The representations are
diagrammed as follows:
∞fp =
0
11111111
00000000000000000000000
S
Exponent
Mantissa
−∞fp =
1
11111111
00000000000000000000000
S
Exponent
Mantissa

4.7 IEEE 754 Floating-Point Standard 187
Floating-point numbers with exponent values of 128 and nonzero man-
tissa bits do not represent inﬁnities. They represent the next class of special
values, nonnumerics.
Nonnumeric Values
All the following function call examples represent exceptional cases:
Function Call
Issue
arcsine(2.0)
Function not deﬁned for argument
sqrt(−1.0)
Result is imaginary
0.0/0.0
Result is indeterminate
∞−∞
Result is indeterminate
In each of these cases, none of the ﬂoating-point values we have discussed
will accurately represent the situation. Here we need a value that indicates
the fact that the desired computation cannot be represented as a real number.
The IEEE speciﬁcation includes a special pair of values for these cases, known
collectively as Not a Number (NaNs). There are two kinds of NaNs: quiet
(or silent) NaN (QNaN) and signaling NaN (SNaN). Compare the following
representations:
QNaN =
0
11111111
1[22 low-order bits indeterminate]
S
Exponent
Mantissa
SNaN =
0
11111111
0[22 low-order bits not all 0]
S
Exponent
Mantissa
Quiet Not a Numbers (Kahan [69] simply calls them NaNs) represent inde-
terminate values and are quietly passed through later computations (generally
as QNaNs). They are not supposed to signal an exception, but rather allow
ﬂoating point code to return the fact that the result of the desired operation
was indeterminate. Floating point implementations (hardware or software)
will generate QNaNs in cases such as those in our comparison.
SNaNs represent unrecoverable mathematical errors and signal an excep-
tion. Most FPUs are designed not to generate SNaNs — the original idea was
that authors of high-level software math packages could generate them in
terminal situations. In addition, compilers could (in debugging builds) set
all ﬂoating point values to SNaN, ensuring an exception if the programmer
left the values uninitialized. The realities of compilers and operating systems

188
Chapter 4 Real-World Computer Number Representation
make SNaNs less interesting. There have been issues in the support for SNaNs
in current compilers [69], resulting in SNaNs being encountered very rarely.
4.7.5 Very Small Values
Normalized Mantissas and the “Hole at Zero”
One side effect of the normalized mantissa is very interesting behavior near
zero. To better understand this behavior, let us look at the smallest normalized
value (we will look at the positive case; the negative case is analogous) in
single-precision ﬂoating point, which we will call Fmin. Fmin would have an
exponent of −126 and zeros in all explicit mantissa bits. The resulting mantissa
would have only the implicit units bit set, leading to a value of
Fmin = 20 × 2−126 = 2−126
The largest value smaller than this in a normalized ﬂoating-point system
would be 0.0. However, the smallest value larger than Fmin would differ by
only one bit from Fmin — the least-signiﬁcant mantissa bit would be set. This
value, which we will call Fnext would be simply:
Fnext = (20 + 2−23) × 2−126 = 2−126 + 2−149 = Fmin + 2−149
This leads to a rather interesting situation: the distance between Fmin and
its nearest smaller neighbor (0.0) is 2−126. This distance is much larger than
the distance between Fmin and its nearest larger neighbor, Fnext. The distance
between Fmin and Fnext is only
Fnext −Fmin = 2−149
In fact, Fmin has a sequence of approximately 223 larger neighbors that are
each a distance of 2−149 from the previous. This leaves a large “hole” of num-
bers between 0.0 and Fmin that cannot be represented with nearly the accuracy
as the numbers slightly larger than Fmin. This gap in the representation is often
referred to as the “hole at zero.” The operation of representing numbers in
the range (−Fmin, Fmin) with zero is often called “ﬂushing to zero.”
One problem with ﬂush-to-zero is that the subtraction of two numbers
that are not equal can result in zero. In other words, with ﬂush-to-zero
A −B = 0 ⇏A = B

4.7 IEEE 754 Floating-Point Standard 189
How can this be? See the following example:
A = 2−126 × (20 + 2−2 + 2−3)
B = 2−126 × (20)
Both of these are valid single-precision ﬂoating point numbers. In fact, they
have equal exponents: −126. Clearly, they are also not equal ﬂoating point
numbers: A’s mantissa has two additional 1 bits. However, their subtraction
produces:
A −B = (2−126 × (20 + 2−2 + 2−3) −(2−126 × (20))
= 2−126 × ((20 + 2−2 + 2−3) −(20))
= 2−126 × (2−2 + 2−3)
= 2−128 × (20 + 2−1)
which would be returned as zero on a ﬂush-to-zero ﬂoating point system.
While this is a contrived example, it can be seen that any pair of nonequal
numbers whose difference has a magnitude less than 2−126 would demon-
strate this problem. There is a solution to this and other ﬂush-to-zero issues,
however. The solution is known as “gradual underﬂow,” and it is discussed in
the next section.
Denormals and Gradual Underﬂow
The IEEE speciﬁcation speciﬁes behavior for very small numbers that avoids
this so-called hole at zero. The behavior is known as gradual underﬂow, and
this gradual underﬂow generates values called “denormals,” or “denormalized
numbers.”
The idea is quite simple. Rather than require every ﬂoating point number
to be normalized, the speciﬁcation reserves numbers with nonzero explicit
mantissa bits and an exponent of −127 for denormals. In a denormal, the
implicit high-order bit of the mantissa is 0. This allows numbers with magni-
tude smaller than 1.0 × 2−126 to be represented. In a denormal, the exponent
is assumed to be −126 (even though the actual bits would represent −127),
and the mantissa is in the range [ 1
223 , 1 −
1
223 ]. The smallest nonzero value that
can be represented with a denormal is 2−23 × 2−126 = 2−149, ﬁlling in the “hole
at zero.” Note that all nonzero ﬂoating point values are still unique, as the
speciﬁcation only allows denormalized mantissas when the exponent is −126,
the minimum valid exponent.
As an historical note, gradual underﬂow and denormalized value handling
were perhaps the most hotly contested of all sections in the IEEE ﬂoating

190
Chapter 4 Real-World Computer Number Representation
point speciﬁcation. Flush-to-zero is much simpler to implement in hardware,
which also tends to mean that it performs faster and makes the hardware
cheaper to produce. When the IEEE ﬂoating point standard was being for-
mulated in the late 1970s, several major computer manufacturers were using
the ﬂush-to-zero method for dealing with underﬂow. Changing to the use of
gradual underﬂow required these manufacturers to design FPU hardware or
software that could handle the unnormalized mantissas that are generated
by denormalization. This would lead either to more complex FPU hardware
or a system that emulated some or all of the denormalized computations in
software or microcode. The former could make the FPUs more expensive to
produce, while the latter could lead to greatly decreased performance of the
ﬂoating point system when denormals are generated. However, several man-
ufacturers showed that it could be implemented in ﬂoating point hardware,
paving the way for this more accurate method to become part of the de facto
(and later, ofﬁcial) standard. However, performance of denormalized values
is still an issue, even today. We will discuss a real-world example of denormal
performance on a modern FPU in Section 4.8.2.
4.7.6 Catastrophic Cancelation
We have used relative error as a metric of the validity of the ﬂoating point rep-
resentation of a given number. However, the relative representation errors of
the operands to a ﬂoating point addition or subtraction operation may not
accurately represent the error in the result. The addition or subtraction of
a pair of ﬂoating point numbers can lead to a result with magnitude much
smaller than either of the operands. Speciﬁcally, the subtraction of two nearly
equal (but different) values will result in such a situation. The following exam-
ple shows how the subtraction of two numbers of large magnitude can result
in a value with much lower magnitude. In this case, the source operands and
the result are represented exactly, but as we shall see in a very similar case,
the result is more problematic:
Afp = 8, 388, 609 = 223 × (20 + 2−23)
Bfp = 8, 388, 608 = 223 × (20)
Afp −Bfp = (223 × (20 + 2−23)) −(223 × (20))
= 223 × ((20 + 2−23) −(20))
= 223 × 2−23 = 20 = 1
While the result is represented exactly, note that in the last step of the oper-
ation, the value must be renormalized. Zeros are shifted into all 23 of the
low-order (explicit) mantissa bits (i.e., only the integral bit is 1). In this case

4.7 IEEE 754 Floating-Point Standard 191
the result is correct. As an example of how this process can cause catastrophic
cancellation and large relative error, let us analyze the following case. It is very
similar to the previous example, but replaces A and B with values that cannot
be represented exactly in single-precision ﬂoating point:
A = 8, 388, 609.45
B = 8, 388, 607.75
A −B = 1.7
In single-precision ﬂoating point, these values round to the same Afp and Bfp
values just given. In turn, Afp −Bfp is once again 1.0. First, we analyze the
relative representation error of Afp:
ErrorA =

A −Afp
A
 = 8, 388, 609.45 −8, 388, 609
8, 388, 609.45
≈5.4 × 10−8
which is, by itself, a very small relative error. Similarly, we compute the
representation error of Bfp:
ErrorB =

B −Bfp
B
 = 8, 388, 607.75 −8, 388, 608
8, 388, 607.75
≈3.0 × 10−8
an even smaller relative error. However, the overall error in the subtraction
Afp −Bfp versus the exact A −B is
ErrorA−B =

(A −B) −(Afp −Bfp)
A −B
 = 1.7 −1.0
1.7
≈0.41!
The relative error in the overall result is about 10,000,000 times worse than the
representational error in either Afp or Bfp! This is due to the fact that almost
all of the bits of precision in the two numbers matched. In other words, all
but one of the original mantissa bits in Afp and Bfp were canceled out in
the subtraction, leaving the least signiﬁcant bit of the operands as the most
signiﬁcant bit of the result. None of the 23 explicit (fractional) bits of the
result’s mantissa is actual data — they were simply shifted in as zeros. The
precision of such a result is very low, indeed. This is catastrophic cancelation;
the signiﬁcant bits are all canceled, causing a catastrophically large growth
in the representation error of the result.
The best way to handle catastrophic cancelation in a ﬂoating point system
is to avoid it. Numerical methods that involve computing a small value as the
subtraction or addition of two potentially large values should be reformulated

192
Chapter 4 Real-World Computer Number Representation
to remove the operation. An example of a common numerical method that
uses such a subtraction is the well-known quadratic formula:
−B ±
√
B2 −4AC
2A
Both of the subtractions in the numerator can involve large numbers whose
addition/subtraction can lead to small results. However, refactoring of the
formula can lead to better-conditioned results. The following revised version
of the quadratic formula can be used in cases where computation of one of
the two roots involves subtracting nearly equal values. The refactored formula
avoids cancelation by replacing the subtraction with an addition:
2C
−B ∓
√
B2 −4AC
A root that would be computed with a subtraction in the ﬁrst (“classic”) version
of the quadratic formula may be computed with an addition in the second
version, and vice versa.
4.7.7 Double Precision
As mentioned, the IEEE 754 speciﬁcation supports a 64-bit “double-precision”
ﬂoating point value, known in C/C++ as the intrinsic double type. The format
is completely analogous to the single-precision format, with the following
bitwise layout:
Sign
Exponent
Mantissa
1 Bit
11 Bits
52 Bits
Double-precision values have a range of approximately 10308 and can
represent values smaller than 10−308. A programmer’s common response to
the onset of precision or range issues is to switch their code to use double-
precision ﬂoating point values in the offending section of code (or sometimes
even throughout the entire system). While double precision can solve almost
all range issues and many precision issues (though catastrophic cancelation
can still persist) in interactive 3D applications, there are several drawbacks
that should be considered prior to its use:
■Memory. Since double-precision values require twice the storage of
single-precision values, memory requirements for an application can

4.8 Real-World Floating Point 193
grow quickly, especially if arrays of vectors (such as vertices) must be
stored as double-precision.
■Performance. At least some of the operations on most hardware FPUs
are signiﬁcantly slower when computing double precision results. Addi-
tional expense can be incurred for conversion between single- and
double-precision values.
■Platform issues. Not all platforms (especially game-centric platforms)
support double precision.
4.8 Real-World Floating Point
While the IEEE ﬂoating-point speciﬁcation does set the exact behavior for
a wide range of the possible cases that occur in real-world situations, in
real-world applications on real-world platforms, the speciﬁcation cannot tell
the entire story. The following sections will discuss some issues that are of
particular interest to 3D game developers.
4.8.1 Internal FPU Precision
Some readers will likely try some of the exceptional cases themselves in small
test applications. In doing so, they are likely to ﬁnd surprising behavior in
many situations. For example, examine the following code:
main()
{
float fHuge = 1.0e30f; // valid single-precision
fHuge *= 1.0e38f; // result = infinity
fHuge /= 1.0e38f; // ????
}
Stepping in a debugger, the following will happen on many major
compilers and systems:
1. After the initial assignment, fHuge = 1.0e30, as expected.
2. After the multiplication, fHuge = ∞fp, as expected.
3. After the division, fHuge = 1.0e30!

194
Chapter 4 Real-World Computer Number Representation
This seems magical. How can the system divide the single value ∞fp and
get back the original number? A look at the assembly code gives a hint. The
basic steps the compiler generates are as follows:
1. Load 1.0e30 and 1.0e38 into the FPU.
2. Multiply the two loaded values and return ∞fp, keeping the result in
the FPU as well.
3. Divide the previous result (still in the FPU) by 1.0e38 (still in the FPU),
returning the correct result.
The important item to note is that the result of each computation was
both returned and kept in the FPU for later computation. This step is where
the apparent “magic” occurs. The FPU (as per the IEEE standard) uses
high-precision (sometimes as long as long double) registers in the FPU. The
conversion to single-precision happens during the storing from the FPU into
memory. While the returned value in fBig was indeed ∞fp, the value retained
in the FPU was higher-precision and was the correct value, 1.0e68. When the
division occurs, the result is correct, not ∞fp.
However, an application cannot count on this result. If the FPU had to
ﬂush the intermediate values out of its registers, then the result of the three
lines above would have been quite different. For example, if signiﬁcant ﬂoat-
ing point work had to be computed between the above multiplication and
the ﬁnal division, the FPU might have run out of registers and had to evict
the high-precision version of fHuge. This can lead to odd behavior differ-
ences, sometimes even between optimized and debugging builds of the same
source code.
4.8.2 Performance
The IEEE ﬂoating point standard speciﬁes behavior for ﬂoating point systems;
it does not specify information regarding performance. Just because a ﬂoating
point implementation is correct does not mean that it is fast. Furthermore,
the speed of one ﬂoating point operation (e.g., addition) does not imply much
about the speed of another (e.g., square root). Finally, not all input data
are to be considered equal in terms of performance. The following sections
describe examples of some real-world performance pitfalls found in ﬂoating
point implementations.
Performance of Denormalized Numbers
During the course of creating a demo for a major commercial 3D game engine,
one of the authors found that in some conditions, the performance of the

4.8 Real-World Floating Point 195
demo dropped almost instantaneously by as much as 20 percent. The code
was proﬁled and it was found that one section of animation code was suddenly
running 10–100 times slower than in the previous frames. An examination of
the offending code determined that it consisted of nothing more than basic
ﬂoating point operations, speciﬁcally, multiplications and divisions. More-
over, there were no loops in the code, and the number of calls to the code
was not increasing. The code itself was simply taking 10–100 times longer to
execute.
Further experiments outside of the demo found that a ﬁxed set of input
data (captured from tests of the demo) could always reproduce the problem.
The developers examined the code more closely and found that very small
nonzero values were creeping into the system. In fact, these numbers
were denormalized. Adjusting the numbers by hand even slightly outside of
the range of denormals and into normalized ﬂoating-point values instantly
returned the performance to the original levels. The immediate thought
was that exceptions were causing the problem. However, all ﬂoating point
exceptions were disabled (masked) in the test application.
To verify the situation, they wrote an extremely simple test application.
Summarized, it was as follows:
float TestFunction(float fValue)
{
return fValue;
}
main()
{
int i;
float fTest;
// Start "normal" timer here
for (i = 0; i < 10000; i++)
{
// 1.0e-36f is normalized in single-precision
fTest = TestFunction(1.0e-36f);
}
// End "normal" timer here
// Start "denormal" timer here
for (i = 0; i < 10000; i++)
{
// 1.0e-40f is denormalized in single-precision
fTest = TestFunction(1.0e-40f);
}
// End "denormal" timer here
}

196
Chapter 4 Real-World Computer Number Representation
Having veriﬁed that the assembly code generated by the optimizer did
indeed call the desired function the correct number of times with the desired
arguments, they found that the denormal loop took 30 times as long as the
normal loop (even with exceptions masked). A careful reading of Intel’s perfor-
mance recommendations [64] for the Pentium series of CPUs found that any
operation (including simply loading to a ﬂoating point register) that produced
or accepted as an operand a denormal value was run using so-called assist
microcode, which is known to be much slower than standard FPU instruc-
tions. Intel’s recommendation was for high-performance code to manually
clamp small values to zero as need be.
Intel had followed the IEEE 754 speciﬁcation, but had made the design
decision to allow exceptional cases such as denormals to cause very signif-
icant performance degradation. An application that had not known of this
slowdown on the Pentium processor may have avoided manually clamping
small values to zero, out of fear of slowing the application down with extra
conditionals. However, armed with this processor-speciﬁc information, it was
much easier to justify clamping small numbers that were not already known
to be normal. Since the values in question were normalized 4-vectors, the
overall length of the vector value should be 1.0. As a result, it was more than
safe to clamp small values to zero.
Software Floating Point Emulation
Applications should take extreme care on new platforms to determine whether
or not the platform supports hardware-assisted ﬂoating point. In order to
ensure that code from other platforms ports and executes without major
rewriting, some compilers supply software ﬂoating point emulation libraries
for platforms that do not support ﬂoating point in hardware. This is espe-
cially common on popular embedded and handheld chip sets such as Intel’s
StrongARM and XScale processors [64]. These processors have no FPUs, but
C/C++ ﬂoating point code compiled for these devices will generate valid, work-
ing emulation code. The compilers will often do this silently, leaving the
uninformed developer with a working program that exhibits horrible ﬂoat-
ing point performance, in some cases hundreds of times slower than could be
expected from a hardware FPU.
It’s worth reiterating that not all FPUs support both single- and double-
precision. Some major game consoles, for example, will generate FPU code
for single-precision values and emulation code for double-precision values.
As a result, careless use of double precision can lead to much slower code.
In fact, it is important to remember that double precision can be intro-
duced into an expression in subtle ways. For example, remember that in
C/C++, ﬂoating point constants are double-precision by default, so whenever
possible, explicitly specify constants as single-precision, using the f sufﬁx.

4.8 Real-World Floating Point 197
The difference between double- and single-precision performance can be as
simple as 1.0 instead of 1.0f.
4.8.3 IEEE Specification Compliance
While major ﬂoating point errors in modern processors are relatively rare
(even Intel was caught off guard by the magnitude of public reaction to what
it considered minor and rare errors in the ﬂoating-point divider on the original
Pentium chips), this does not mean that it is safe to assume that all ﬂoating
point units in modern CPUs are always fully compliant to IEEE speciﬁca-
tions and support both single and double precision. The greatest lurking risk
to modern developers assuming full IEEE compliance are conscious design
decisions, not errors on the part of hardware engineers. However, in most
cases, for the careful and attentive programmer, these new processors offer
the possibilities of great performance increases to 3D games.
As more and more FPUs are designed and built for multimedia and 3D
applications (rather than the historically important scientiﬁc computation
applications for which earlier FPUs were designed), manufacturers are start-
ing to deviate from the IEEE speciﬁcation, optimizing for high-performance
over accuracy. This is especially true with respect to the “exceptional” cases
in the spec, such as denormals, inﬁnity, and Not a Number.
Hardware vendors make the argument that while these special values are
critically important to scientiﬁc applications, for 3D games and multimedia,
they generally occur only in error cases that are best handled by avoiding
them in the ﬁrst place.
Intel’s SSE
An important example of such design decisions involves Intel’s Streaming
SIMD Extensions (SSE) [64], a new coprocessor that was added to the Pen-
tium series with the advent of the Pentium III. The coprocessor is a special
vector processor that can execute parallel math operations on four ﬂoat-
ing point values, packed into a 128-bit register. The SSE instructions were
speciﬁcally targeted at 3D games and multimedia, and this is evident from
even a cursory view of the design. Several design decisions related to the
special-purpose FPU merit mentioning here:
■The original SSE (Pentium III) instructions can only support 32-bit
ﬂoating point values, not doubles.
■Denormal values can be (optionally) rounded to zero (“ﬂushed to zero”),
disabling gradual underﬂow.

198
Chapter 4 Real-World Computer Number Representation
■Full IEEE 754 behavior can be supported as an option but at less than
peak performance.
3D-speciﬁc FPUs
Other platforms have created graphics-centric FPUs. This 3D graphics focus
has given the hardware designers the ability to optimize the ﬂoating point
behavior of the FPUs very heavily. Unburdened by the need to support any
applications other than games, the designers of these FPUs have taken things
a step further than Intel’s SSE instructions by making the deviations from the
IEEE speciﬁcation permanent, rather than optional.
AMD’s 3DNow! [1] extensions to its x86 platforms are one such example.
While leaving the main FPU unchanged, AMD added hardware to support
up to four ﬂoating point instructions per clock cycle. As a further optimiza-
tion, 3DNow! made some decisions that broke from the IEEE speciﬁcation,
including:
■Cannot accept inﬁnity or NaN as operands
■Generates the maximal normal ﬂoating-point value on overﬂow, rather
than inﬁnity
■Flush-to-zero as the only form of underﬂow (no denormals)
■No support for ﬂoating point exceptions
The 3D-centric vector FPUs in some current game consoles have taken
similar paths. These differences from the IEEE speciﬁcation, while severe
from a scientiﬁc computing perspective, are rarely an issue in correct 3D
game code. The console processors that have these limitations are generally
designed to allow games to implement geometry pipelines. In most 3D game
code, the engine programmer takes great pains to avoid exceptional condi-
tions in their geometry pipelines. Thus, these hardware design decisions tend
to merely reﬂect the common practices of game programmers, rather than
adding new limitations upon them.
4.9 Code
Library
IvMath
While this text’s companion CD-ROM and web site do not include spe-
ciﬁc code that demonstrates the concepts in this chapter, source code that
deals with issues of ﬂoating point representation may be found through-
out the math library IvMath. For example, the source code for IvMatrix33,
IvMatrix44, IvVector3, IvVector4, and IvQuat includes sections of code that
avoid denormalized numbers and comparisons to exact ﬂoating-point zero.

4.10 Chapter Summary 199
CPU chipset manufacturers Intel and AMD have been focused on 3D
graphics and game performance and have made public many code exam-
ples, presentations, and software libraries that detail how to write high-
performance ﬂoating point code for their processors. Many of these resources
may be found on their developer web sites ([1, 64]).
As mentioned earlier, high-performance ﬁxed point code that does not
drop precision or overﬂow in common cases is best accomplished through the
use of platform-speciﬁc instructions. Major CPU vendors have realized this,
and some of them ship libraries or sample code that allow high-performance
ﬁxed-point math on their non–ﬂoating-point processors. On their developer
web site Intel [64] provides their GPP library — a set of “graphics performance
primitives” that includes basic ﬁxed point math routines — optimized for their
StrongARM and XScale processors. Also, the ARM [4] corporation includes
technical reports on their developer web site that include code and methods
for implementing high-speed ﬁxed point code on devices based on their
architecture.
4.10 Chapter Summary
In this chapter we have discussed the details of how computers represent the
sets of whole numbers, integers, and real numbers. Each of these representa-
tions has inherent limitations that any serious programmer must understand
in order to use them efﬁciently and correctly. The common representations
of real numbers, both ﬁxed-point and ﬂoating-point, present the most subtle
limitations, especially the issues of limited precision. We have also discussed
the basics of error metrics for number representations.
Hopefully, this chapter has instilled two important pieces of information
in the reader. The ﬁrst and most basic is an understanding of the inner work-
ings of the number systems that pervade 3D games. This should allow the
programmer to truly comprehend the reasons why their math-related code
behaves (or, more importantly, why it misbehaves) as it does. The second
piece of information is an appreciation of why one should pay attention to
the topic of ﬂoating point representation in the ﬁrst place — namely, to better
prepare the 3D game developer to do what they will need to do at some point in
the development of a game: optimize or ﬁx a section of slow or incorrect math
code. Better yet, it can assist the developer to avoid writing this potentially
problematic code in the ﬁrst place.
For further reading, Kahan’s papers on the history and status of the IEEE
ﬂoating point standard ([69] and related papers and lectures by Kahan, avail-
able from the same source) offer fascinating insights into the background
of modern ﬂoating point computation. In addition, back issues of Game
Developer magazine (such as [60]) provide frequent discussion of number
representations as they relate to computer games.


Part
II
RENDERING


Chapter5
Viewing and
Projection
5.1 Introduction
In previous chapters we’ve discussed how to represent objects, basic transfor-
mations we can apply to these objects, and how we can use these transforma-
tions to move and manipulate our objects within our virtual world. With that
background in place, we can begin to discuss the mathematics underlying the
techniques we use to display our game objects on a monitor or other visual
display medium.
It doesn’t take much justiﬁcation to understand why we might want to
view the game world — after all, games are primarily a visual media. Other
sensory outputs are of course possible, particularly sound and haptic (or
touch) feedback. Both have become more sophisticated and in their own way
provide another representation of the relative 3D position and orientation of
game objects. But in the current market, when we think of games, we ﬁrst
think of what we can see.
The ﬁrst part of the display process (or graphics pipeline) involves setting
up a virtual viewer or camera, which allows us to control which objects lie
in our current view. As we’ll see, this camera is just like any other object in
the game; we can set the camera’s position and orientation based on an afﬁne
transformation. Inverting this transformation allows us to transform objects
in the world frame into the point of view of the camera object.
From there we will want to transform our objects in view into 2D
coordinates so they can be represented in an image. This ﬂattening or pro-
jection takes many forms, and we’ll discuss several of the most commonly
203

204
Chapter 5 Viewing and Projection
used projections. In particular we’ll derive perspective projection, which
mimics our viewpoint of the real world most closely. Once projected, we can
take the coordinates generated and stretch and translate them to ﬁt a speciﬁc
portion of the screen, known as the viewport.
Finally, we’ll cover how to reverse this process so we can take a mouse
click on our two-dimensional screen and use it to select objects in our three-
dimensional world. This process, known as “picking,” can be useful when
building an interface with three-dimensional elements. For example, selecting
units in a 3D real-time strategy game is done via picking.
As with other chapters, we’ll be discussing how to implement these trans-
formations in production code. Because our examples are written in OpenGL,
for the most part we’ll be focusing on its pipeline and how it handles the view-
ing and projective transformations. However, we will also cover the cases
where it may differ from graphics APIs, particularly Direct3D.
5.2 The View Frame and View
Transformation
5.2.1 Defining a Virtual Camera
In order to render objects in the world, we need to represent the notion of a
viewer. This could be the main character’s viewpoint in a ﬁrst-person shooter,
or an over-the-shoulder view in a third-person adventure game, or it could be
a zoomed-out wide shot in a strategy game. We may want to control properties
of our viewer to simulate a virtual camera; for example, we may want to create
an in-game scripted sequence where we pan across a screen or follow a set
path through a space. We encapsulate these properties into a single entity,
commonly called the camera.
For now, we’ll consider only the most basic properties of the camera
needed for rendering. We are trying to answer two questions: Where am I?
and Where am I looking? [11]. The answer to the ﬁrst question is the camera’s
position, E, which is variously called the eyepoint, the view position, or the
view space origin. As we mentioned, this could be the main character’s eye
position, a location over his shoulder, or pulled back from the action. While
this can be placed relative to another object’s location, it is usually cleaner
and easier to manage if we represent it in the world frame.
A partial answer to the second question is a vector called the view direc-
tion vector, or vdir, which points along the facing direction for the camera.
This could be a vector from the camera position to an object or point of inter-
est, a vector indicating the direction the main character is facing, or a ﬁxed
direction if we’re trying to simulate an isomorphic view for a strategy game.

5.2 The View Frame and View Transformation 205
For the purposes of setting up the camera, this is also speciﬁed in the world
frame.
Since there is an inﬁnite number of orientations which align with a single
vector, the view direction vector is not enough information. To constrain our
possibilities down to one, we specify a second vector orthogonal to the ﬁrst,
called the view up vector, or vup. This indicates the direction out of the top of
the camera or the character’s head. The remaining orthogonal vector is the
view side vector, or vside, which usually points out towards the camera’s right.
All three view vectors are represented in the world frame. Since they are
orthogonal, by normalizing them we can create an orthonormal basis. Using
this basis together with the view position we can specify a new frame rela-
tive to our world coordinate system, known as the view frame, or view space
(Figure 5.1). This is how we determine our camera’s position and orientation
in the world.
We can of course deﬁne the transformation from the view frame to the
world frame (also known as the view-to-world transformation) as a 4 × 4 afﬁne
matrix. The origin E of the view frame is translated to the view position, so the
translation vector y is equal to E −O. We’ll abbreviate this as vpos. Similarly,
the view vectors represent how the standard basis vectors in view space are
transformed into world space and become columns in the upper left 3 × 3
matrix A. To build A, however, we need to deﬁne which standard basis vector
in the view frame maps to a particular view vector in the world frame.
The standard order used by most viewing systems is to map the view frame
z-axis to the view direction vector, the view frame y-axis to the view up vector,
and the view frame x-axis to the view side vector (Figure 5.2a). This aligns our
view coordinates so that in the view frame, x values vary left and right along
the plane of the screen and y values vary up and down. In addition, as objects
in front of the viewer move farther away, their z values in the view frame will
view up
view side
view direction
view point
Figure 5.1 View frame relative to the world frame.

206
Chapter 5 Viewing and Projection
x-axis
z-axis
y-axis
Figure 5.2a Standard view frame axes.
x-axis
z-axis
y-axis
Figure 5.2b OpenGL view frame axes.
increase, which is nicely intuitive. The value of z can act as a measure of the
distance between the object and the camera, which we can use for hidden
object removal.
This mapping indicates which columns the view vectors should be placed
in, and the view position takes its familiar place in the right-most column.
The corresponding transformation matrix is
Mview→world =
 ˆvside
ˆvup
ˆvdir
vpos
0
0
0
1

(5.1)
Note that in this case we are mapping from a left-handed view frame ((ˆvside ×
ˆvup) · ˆvdir < 0) to the right-handed world frame, so the upper 3 × 3 is not a
pure rotation but a rotation concatenated with a reﬂection.
OpenGL does not follow the standard model; instead, it chooses a slightly
different approach. It maintains a right-handed system where the view direc-
tion is aligned with the frame’s negative z-axis (Figure 5.2b). So in this case,
the farther away the object is, its −z coordinate gets larger in the view frame.

5.2 The View Frame and View Transformation 207
The corresponding transformation matrix for OpenGL is
Mview→world
 ˆvside
ˆvup
−ˆvdir
vpos
0
0
0
1

(5.2)
In this case, since we are mapping from a right-handed frame to a right-
handed frame, no reﬂection is necessary, and the upper 3 × 3 matrix is a pure
rotation. Not having a reﬂection can actually be a beneﬁt, particularly with
some culling methods.
5.2.2 Controlling the Camera
It’s not enough that we have a speciﬁcation for our camera position and ori-
entation. More often we’ll want to move it around the world. Positioning our
camera is a simple enough matter of translating the view position, but con-
trolling view orientation is another problem. One way is to specify the view
vectors directly and build the matrix as described. This assumes, of course,
that we already have a set of orthogonal vectors we want to use for our viewing
system.
Demo
LookAt
The more usual case is that we only know the view direction. For exam-
ple, suppose we want to continually focus on a particular object in the world
(known as the look-at object). We can construct the view direction by sub-
tracting the view position from the object’s position. But whether we have a
given view direction or we generate it from the look-at object, we still need
two other orthogonal vectors to properly construct an orthogonal basis. We
can calculate them by using one additional piece of information: the world
up vector. This is a ﬁxed vector representing the direction “up” in the world
frame. In our case we’ll use the z-axis basis vector k (Figure 5.3), although in
general any vector that we care to call “up” will do. For example, suppose we
had a mission on a boat at sea and wanted to give the impression that the boat
was rolling from side to side, without affecting the simulation. One method is
to change the world up vector over time, oscillating between two keeled-over
orientations, and use that to calculate your camera orientation.
For now, however, we’ll use k as our world up vector. Our goal is to
compute orthonormal vectors in the world frame corresponding to our view
vectors, such that one of them is our view direction vector ˆvdir, and our view
up vector ˆvup matches the world up vector as closely as possible. Recall that we
can use Gram-Schmidt orthogonalization to create orthogonal vectors from
a set of nonorthogonal vectors, and so:
vup = k −(k · ˆvdir)ˆvdir

208
Chapter 5 Viewing and Projection
view direction
eyepoint
world up
z
x
y
Figure 5.3 LookAt representation.
Normalizing gives us ˆvup. We can take the cross product to get the view side
vector:
ˆvside = ˆvdir × ˆvup
We don’t need to normalize in this case because the two vector arguments
are orthonormal. The resulting vectors can be placed as columns in the
transformation matrix as before.
One problem may arise if we are not careful: suppose that ˆvdir and k are
parallel? If they are equal we end up with
vup = k −(k · ˆvdir)ˆvdir
= k −1 · ˆvdir
= 0
If they point in opposite directions we get
vup = k −(k · ˆvdir)ˆvdir
= k −(−1) · ˆvdir
= 0
Clearly, neither case will lead to an orthonormal basis.
The recovery procedure is to pick an alternative vector that we know is
not parallel, such as i or j. This will lead to what seems like an instantaneous
rotation around the z-axis. To understand this, raise your head upward until
you are looking at the ceiling. If you keep going, you’ll end up looking at the
wall behind you, but upside down. To maintain the view looking right-side

5.2 The View Frame and View Transformation 209
up, you’d have to rotate your head 180 degrees around (don’t try this at home).
This is not a very pleasing result, so avoid aligning the view direction with the
world up vector whenever possible.
Demo
Rotation
There is a third possibility for controlling camera orientation. Suppose
we want to treat our camera just like a normal object and specify a rotation
matrix and translation vector. To do this we’ll need to specify a starting ori-
entation  for our camera and then apply our rotation matrix to ﬁnd our
camera’s ﬁnal orientation, after which we can apply our translation. Which
orientation is chosen is somewhat arbitrary, but some are more intuitive and
convenient than others. In our case we’ll say that in our default orientation
the camera has an initial view direction along the world x-axis, an initial
view up along the world z-axis, and an initial view side along the −y-axis.
This aligns the view up vector with the world up vector, and using the x-axis
as the view direction ﬁts the convention we set for objects’ local space in
Chapter 3.
Substituting these values into the view-to-world matrix for the standard
left-handed view frame (equation 5.1) gives
s =


0
0
1
0
−1
0
0
0
0
1
0
0
0
0
0
1


The equivalent matrix for the right-handed OpenGL view frame (using
equation 5.2) is
 ogl =


0
0
−1
0
−1
0
0
0
0
1
0
0
0
0
0
1


Whichever system we are using, after this we apply our rotation to orient
our frame in the direction we wish and, ﬁnally, the translation for the view
position. If the three column vectors in our rotation matrix are u, v, and w,
then for OpenGL the ﬁnal transformation matrix is
Mview→world = TR ogl
=
 i
j
k
vpos
0
0
0
1
  u
v
w
0
0
0
0
1
  −j
k
−i
0
0
0
0
1

=
 −v
w
−u
vpos
0
0
0
1


210
Chapter 5 Viewing and Projection
5.2.3 Constructing the View Transformation
Now that we have a way of representing and setting camera position and
orientation, what do we do with it? The ﬁrst step in the rendering process
is to transform all of the objects in our world so that their coordinates are
relative to the view frame, instead of the world frame. This gives us a sense of
what we can see from our camera position. In the view frame, those objects
along the line of the view direction vector (i.e., the −z-axis in the case of
OpenGL) are in front of the camera and so will most likely be visible in our
scene. Those on the other side of the plane formed by the view point, the view
side vector, and the view up vector are behind the camera, and therefore not
visible. In order to achieve this situation, we need to create a transformation
from world space to view space, known as the world-to-view transformation,
or more simply, the view transformation.
As it happens, we have a transformation that takes us from view space
to world space. To create the reverse operator, we need only to invert the
transformation. Since we know that it is an afﬁne transformation, we can
invert it as
Mworld→view =
 R−1
−(R−1vpos)
0T
1

where R is the upper 3 × 3 block of our view-to-world transformation. And
since R is the product of either a reﬂection and rotation matrix (in the standard
case) or two rotations (in the OpenGL case), it is an orthogonal matrix, so we
can compute its inverse by taking the transpose:
Mworld→view =
 RT
−(RT vpos)
0T
1

Demo
LookAt
In practice this transformation is usually calculated directly, rather than
taking the inverse of an existing transformation. For example, OpenGL has
a utility call gluLookAt() that computes the view transformation assuming
a view position, desired view position, and world up vector. One possible
implementation is
void LookAt( const IvVector3& eye,
const IvVector3& lookAt,
const IvVector3& up )
{
// compute view vectors
IvVector3 viewDir = lookAt - eye;
IvVector3 viewSide;
IvVector3 viewUp;

5.3 Projective Transformation 211
viewDir.Normalize();
viewUp = up - up.Dot(viewDir)*viewDir;
viewUp.Normalize();
viewSide = viewDir.Cross(viewUp);
// now set up matrices
// build transposed rotation matrix
IvMatrix33 rotate;
rotate.SetRows( viewSide, viewUp, -viewDir );
// transform translation
IvVector3 eyeInv = -(rotate*eye);
// build 4x4 matrix
IvMatrix44 matrix;
matrix.Rotation(rotate);
matrix(0,3) = eyeInv.x;
matrix(1,3) = eyeInv.y;
matrix(2,3) = eyeInv.z;
// set view to world transformation
::SetViewTransform( matrix.mV );
}
Note that we use the method IvMatrix33:SetRows() to set the transformed
basis vectors since we’re setting up the inverse matrix, namely, the trans-
pose. There is also no recovery code if the view direction and world up
vectors are collinear — it is assumed that any external routine will ensure this
does not happen. The call ::SetViewTransform() stores the calculated view
transformation and is discussed in more detail in Section 5.7.
5.3 Projective Transformation
5.3.1 Definition
Now that we have a method for controlling our view position and orientation,
and for transforming our objects into the view frame, we can look at taking our
three-dimensional space and transforming it into a form suitable for display
on a two-dimensional medium. This process of transforming from R3 to R2 is
called projection.
We’ve already seen one example of projection: using the dot product to
project one vector onto another. In our current case, we want to project the

212
Chapter 5 Viewing and Projection
points that make up the vertices of an object onto a plane, called the pro-
jection plane or the view plane. We do this by following a line of projection
through each point and determining where it hits the plane. These lines could
be perpendicular to the plane, but as we’ll see they don’t have to be.
To understand how this works, we’ll look at a very old form of optical
projection known as the camera obscura (Latin for “dark room”). Suppose one
enters a darkened room on a sunny day, and there is a small hole allowing
a fraction of sunlight to enter the room. This light will be projected onto the
opposite wall of the room, displaying an image of the world outside, albeit
upside down and ﬂipped left to right (Figure 5.4). This is the same principle
that allows a pinhole camera to work; the hole is acting like the focal point of
a lens. In this case all the lines of projection pass through a single center of
projection. We can determine where a point will transform to on the plane by
constructing a line through both the original point and the center of projection
and calculating where it will intersect the plane of projection.
This sort of projection is known as perspective projection. Note that this
relates to our perceived view in the real world. As an object moves farther
away, its corresponding projection will shrink on the projection plane. Sim-
ilarly, lines that are parallel in view space will appear to converge as their
extreme points move farther away from the view position. This gives us a
result consistent with our expected view in the real world. If we stand on
some railroad tracks and look down a straight section, the rails will converge
in the distance, and the ties will appear to shrink in size and become closer
together. In most cases, since we are rendering real-world scenes — or at least,
scenes that we want to be perceived as real-world — this will be the projection
we will use.
There is, of course, one minor problem: the projected image is upside
down and backwards. One possibility is just to ﬂip the image when we display
it on our medium. This is what happens with a camera: the image is captured
on ﬁlm upside down, but we can just rotate the negative or print to view it
properly. This is not usually done in graphics. Instead, the projection plane
Figure 5.4 Camera obscura.

5.3 Projective Transformation 213
Figure 5.5 Perspective projection.
is moved to the other side of the center of projection, which is now treated
as our view position (Figure 5.5). As we’ll see, the mathematics for projection
in this case are quite simple, and the objects located in the forward direction
of our view will end up being projected right-side up. The objects behind the
view will end up projecting upside-down, but (a) we don’t want to render them
anyway and (b) as we’ll see there are ways of handling this situation.
An alternate type of projection is parallel projection, which can be thought
of as a perspective projection where the center of projection is inﬁnitely dis-
tant. In this case the lines of projection do not converge; they always remain
parallel (Figure 5.6), hence the name. The placement of the view position and
view plane are irrelevant in this case, but we place them in the same relative
location to maintain continuity with perspective projection.
Parallel projection produces a very odd view if used for a scene: objects
remain the same size no matter how distant they are, and parallel lines remain
parallel. Parallel projections are usually used for CAD programs, where main-
taining parallel lines is important. They are also useful for rendering 2D
elements like interfaces; no matter how far from the eye a model is placed, it
will always be the same size, presumably the size we expect.
A parallel projection where the lines of projection are perpendicular to
the view plane is called an orthographic projection. By contrast, if they are
not perpendicular to the view plane, this is known as an oblique projection
(Figure 5.7). Two common oblique projections are the cavalier projection,
where the projection angle is 45 degrees, and the cabinet projection, where the
projection angle is cot−1(1/2). When using cavalier projections, projected lines
Figure 5.6 Orthographic parallel projection.

214
Chapter 5 Viewing and Projection
Figure 5.7 Oblique parallel projection.
have the same length as the original lines, so there is no perceived foreshort-
ening. This is useful when printing blueprints, for example; any line can be
measured to ﬁnd the exact length of material needed to build the object. With
cabinet projections, lines perpendicular to the projection plane foreshorten
to half their length (hence the cot−1(1/2)), which gives a more realistic look
without sacriﬁcing the need for parallel lines.
We can also have oblique perspective projections where the line from the
center of the view window to the center of projection is not perpendicular to
the view plane. For example, suppose we need to render a mirror. To do so,
we’ll render the space using a plane reﬂection transformation and clip it to
the boundary of the mirror. The plane of the mirror is our projection plane,
but it may be at an angle to our view direction (Figure 5.8). For now, we’ll
concentrate on constructing projective transformations perpendicular to the
projection plane and examine these special cases later.
As a side note, oblique projections can occur in the real world. The classic
pictures we see of tall buildings, shot from the ground but with parallel sides,
are done with a “view camera.” This device has an accordian-pleated hood
that allows the photographer to bend and tilt the lens up while keeping the
ﬁlm parallel to the side of the building. Ansel Adams also used such a camera
to capture some of his famous landscape photographs.
Figure 5.8 Oblique perspective projection.

5.3 Projective Transformation 215
5.3.2 The View Frustum
It is not possible to map the entire inﬁnite view plane to a display device.
Instead we set a view window, which frames the rectangular area on the view
plane that will be mapped to the device. We could, naively, project all of the
objects in the world to the view plane and then, when converting them to
pixels, ignore those pixels that lie outside of the view window. However, for
a large number of objects this would be very inefﬁcient. It would be better
to constrain our space to a convex volume, speciﬁed by a set of six planes.
Anything inside these planes will be rendered; everything outside them will
be ignored. This volume is known as the view frustum, or view volume.
To constrain what we render in the view frame xy directions, we specify
four planes aligned with the edges of the view window. For perspective pro-
jection each plane is speciﬁed by the view position and two adjacent vertices
of the view window (Figure 5.9), producing a semi-inﬁnite pyramid. The angle
between the upper plane and the lower plane is called the vertical ﬁeld of view.
There is a relationship between ﬁeld of view, view window size, and view
plane distance: given two, we can easily ﬁnd the third. For example, we can ﬁx
the view window size, adjust the ﬁeld of view, and then compute the distance
to the view plane. As the ﬁeld of view gets larger, the distance to the view
plane needs to get smaller to maintain the view window size. Similarly, a
small ﬁeld of view will lead to a longer view plane distance. Alternatively, we
can set the distance to the view plane to a ﬁxed value and use the ﬁeld of view
to determine the size of our view window. The larger the ﬁeld of view, the
larger the window and the more objects are visible in our scene. This gives us
a primitive method for creating telephoto (narrow ﬁeld of view) or wide-angle
x-axis
z-axis
y-axis
Field of view
view window
Figure 5.9 Perspective view frustum (right-handed system).

216
Chapter 5 Viewing and Projection
(wide ﬁeld of view) lenses. We will discuss the relationship among these three
quantities in more detail when we cover perspective projection.
Usually the ﬁeld of view chosen needs to match the display medium, as
the user perceives it, as much as possible. For a standard monitor placed
about three feet away, the monitor only covers about a 25–30 degree ﬁeld of
view from the perspective of the user, so we would expect that we would use
a ﬁeld of view of that size in the game. However, this constrains the amount
we can see in the game to a narrow area, which feels unnatural because we’re
used to a 180 degree ﬁeld of view in the real world. The usual compromise
is to set the ﬁeld of view to the range of 60–90 degrees. The distortion is not
that perceptible and it allows the user to see more of the game world. If the
monitor were stretched to cover more of your personal ﬁeld of view, as in
some virtual reality systems, a larger ﬁeld of view would be appropriate. And
of course, if the desired effect is of a telephoto or wide-angle lens, a narrower
or wider ﬁeld of view, respectively, is appropriate.
For parallel projection, the xy culling planes are parallel to the direction of
projection, so opposite planes are parallel and we end up with a parallelpiped
that is open at two ends (Figure 5.10). There is no concept of ﬁeld of view in
this case.
In both cases, to complete a closed view frustum we also deﬁne two planes
which constrain objects in the view frame z-direction: the near and far planes
(Figure 5.11). With perspective projection it may not be obvious why we need
x-axis
z-axis
y-axis
view window
Figure 5.10 Parallel view frustum (right-handed system).

5.3 Projective Transformation 217
far plane
near plane
view window
Figure 5.11 View frustum with near plane and far plane.
a near plane, since the xy-planes converge at the center of projection, closing
the viewing region at that end. However, as we will see when we start talking
about the perspective transformation, rendering objects at the view frame
origin (which in our case is the same as the center of projection) can lead to a
possible division by zero. This would adversely affect our rendering process.
We could also, like some viewing systems, use the view plane as the near
plane, but not doing so allows us a little more ﬂexibility.
In some sense, the far plane is optional. Since we don’t have an inﬁnite
number of objects or an inﬁnite amount of game space, we could forgo using
the far plane and just render everything within the ﬁve other planes. However,
the far plane is useful for culling objects and area from our rendering process,
so having a far plane is good for efﬁciency’s sake. It is also extremely important
in the hidden surface removal method of z-buffering; the distance between the
near and far planes is a factor in determining the precision we can expect in
our z-values.
5.3.3 Normalized Device Coordinates
Currently our objects are in view frame coordinates. However, as mentioned
we will be projecting from R3 to R2, so we will need a frame for the space of the

218
Chapter 5 Viewing and Projection
view window
j
i
Figure 5.12a NDC frame in view window.
j
i
(1, 1)
(–1, –1)
Figure 5.12b View window after NDC transformation.
view plane. We’ll use as our origin the center of the view window, and create
basis vectors that align with the sides of the view window, with magnitudes of
half the width and height of the window, respectively (Figure 5.12a). Within
this frame, our view window is transformed into a square two units wide and
centered at the origin, bounded by the x = 1, x = −1, y = 1, and y = −1 lines
(Figure 5.12b).
Using this as our frame provides a certain amount of ﬂexibility when map-
ping to devices of varying size. Rather than transform directly to our screen
area, which could be of variable width and height, we use this normalized
form as an intermediate step to simplify our calculations and then do the

5.3 Projective Transformation 219
screen conversion as our ﬁnal step. Because of this, coordinates in this frame
are known as normalized device coordinates.
To take advantage of the normalized device coordinate frame, or NDC
space, we’ll want to create our projection so that it always gives us the −1
to 1 behavior, regardless of the exact view conﬁguration. This helps us to
compartmentalize the process of viewing (just as the view matrix did).
To simplify this mapping to the NDC frame, we will begin by using a view
window in the view frame with a height of 2 units. This means that for the
case of a centered view window, xy coordinates on the view plane will be equal
to the projected coordinates in the NDC frame. In this way we can consider
the projection as related to the view plane in view coordinates and not worry
about a subsequent transformation. When adjusting our ﬁeld of view, we will
move the view plane relative to the center of projection, rather than changing
the size of the view window.
5.3.4 Homogeneous Coordinates
Previously we stated that a point in R3 can be represented by (x, y, z, 1) with-
out explaining much about what that might mean. This representation is part
of a more general representation for points known as homogeneous coordi-
nates, which prove useful to us when handling perspective projections. In
general, homogeneous coordinates work as follows: if we have a “standard”
representation in n-dimensional space, then we can represent the same point
in a (n + 1)–dimensional space by scaling the original coordinates by a single
value and then adding the scalar to the end as our ﬁnal coordinate. Since we
can choose from an inﬁnite number of scalars, a single point in Rn will be
represented by an inﬁnite number of points in the (n + 1)–dimensional space.
This (n + 1)–dimensional space is called a real projective space or RP n. In
computer graphics parlance, the real projective space RP 3 is also often called
homogeneous space.
Suppose we start with a point (x, y, z) in R3, and we want to map it to
a point (x′, y′, z′, w) in homogeneous space. We pick a scalar for our fourth
element w, and scale the other elements by it, to get (xw, yw, zw, w). As we
might expect, our standard value for w will be 1, so (x, y, z) maps to (x, y, z, 1).
To map back to three-dimensional space, divide the ﬁrst three coordinates
by w, so (x′, y′, z′, w) goes to (x′/w, y′/w, z′/w). Since our standard value
for w is just 1, we could just drop the w : (x′, y′, z′, 1) →(x′, y′, z′). However,
in the cases that we’ll be concerned with next, we need to perform the
division by w.
What happens when w = 0? In this case a point in RP 3 doesn’t represent
a point in R3, but a vector. We can think of this as a “point at inﬁnity.” While
we will try to avoid cases where w = 0, they do creep in, so checking for this
before performing the homogeneous division is often wise.

220
Chapter 5 Viewing and Projection
5.3.5 Perspective Projection
Demo
Perspective
Since this is the most common projective transform we’ll encounter, we’ll
begin by constructing the mathematics necessary for the perspective projec-
tion. To simplify things, let’s take a 2D view of the situation on the yz-plane
and ignore the near and far planes for now (Figure 5.13). We have the y-axis
pointing up, as in the view frame, and the projection direction along the neg-
ative z-axis as it would be in OpenGL. The point on the left represents our
center of projection, and the vertical line our view plane. The diagonal lines
represent our y culling planes.
Suppose we have a point Pv in view coordinates that lies on one of the
view frustum planes, and we want to ﬁnd the corresponding point Ps that lies
on the view plane. Finding the y coordinate of Ps is simple: we follow the line
of projection along the plane until we hit the top of the view window. Since
the height of the view window is 2 and is centered on 0, the y coordinate of Ps
is half the height of the view window, or 1. The z coordinate will be negative
since we’re looking along the negative z-axis and will have a magnitude equal
to the distance d from the view position to the projection plane. So the z
coordinate will be −d.
But how do we compute d? As we see, the cross section of the y view
frustum planes are represented as lines from the center of projection through
the extents of the view window (1, d) and (−1, d). The angle between these lines
is our ﬁeld of view θf ov. We’ll simplify things by considering only the area that
lies above the negative z-axis; this bisects our ﬁeld of view to an angle of θf ov/2.
If we look at the triangle bounded by the negative z-axis, the cross section of
the upper view frustum plane, and the cross section of the projection plane,
we can use trigonometry to compute d. Since we know the distance between
y-axis
–z-axis
d
Ps
Pv
θ/2
1
eyepoint
projection plane
Figure 5.13 Perspective projection construction.

5.3 Projective Transformation 221
the negative z-axis and the extreme point Ps is 1, we can say that
1
d = tan(θf ov/2)
Rewriting this in terms of d, we get
d =
1
tan
 θf ov
2

= cot
θf ov
2

So for this ﬁxed view window size, as long as we know the angle of ﬁeld
of view, we can compute the distance d, and vice versa.
This gives the coordinates for any point that lies on the upper y view
frustum plane; in this 2D cross section they all project down to a single point
(1, −d). Similarly, points that lie on the lower y frustum plane will project to
(−1, −d). But suppose we have a general point (yv, zv) in view space. We know
that its projection will lie on the view plane as well, so its zndc coordinate will
be −d. But how do we ﬁnd yndc?
We can compute this by using similar triangles (Figure 5.14). If we have
a point (yv, zv), the length of the sides of the corresponding right triangle
in our diagram are yv and −zv (since we’re looking down the −z-axis, any
visible zv is negative, so we need to negate it to get a positive value). The
length of sides of the right triangle for the projected point are yndc and d.
y-axis
projection plane
–z-axis
d
–zv
(yndc, –d)
(yv, zv)
Figure 5.14 Perspective projection similar triangles.

222
Chapter 5 Viewing and Projection
By similar triangles (both have the same angles), we get
yndc
d
= yv
−zv
Solving for yndc, we get
yndc = dyv
−zv
This gives us the coordinate in the y direction. If our view region was
square, then we could use the same formula for the x direction. Most, however,
are rectangular to match the relative dimensions of a computer monitor or
other viewing device. We must correct for this by the aspect ratio of the view
region. The aspect ratio a is deﬁned as
a = wv
hv
where wv and hv are the width and height of the view rectangle, respectively.
We’re going to assume that the NDC view window height remains at 2 and
correct the NDC view width by the aspect ratio. This gives us a formula for
similar triangles of
axndc
d
= xv
−zv
Solving for xndc:
xndc = dxv
−azv
So our ﬁnal projection transformation equations are
xndc = dxv
−azv
yndc = dyv
−zv
The ﬁrst thing to notice is that we are dividing by a z coordinate, so we
will not be able to represent the entire transformation by a matrix operation,
since it is neither linear nor afﬁne. However, it does have some afﬁne elements,
scaling by d and d/a for example, which can be performed by a transforma-
tion matrix. This is where the conversion from homogeneous space comes in.

5.3 Projective Transformation 223
Recall that to transform from RP 3 to R3 we need to divide the other coordi-
nates by the w value. If we can set up our matrix to map −zv to our w value, we
can take advantage of the homogeneous divide to handle the nonlinear part
of our transformation. We can write the situation before the homogeneous
divide as a series of linear equations:
x′ = d
a x
y′ = dy
z′ = dz
w′ = −z
and treat this as a four-dimensional linear transformation. Looking at our
basis vectors, e0 will map to (d/a, 0, 0, 0), e1 to (0, −d, 0, 0), e2 to (0, 0, d, −1),
and e3 to (0, 0, 0, 0) since w is not used in any of the equations.
Based on this, our homogeneous perspective matrix is


d/a
0
0
0
0
d
0
0
0
0
d
0
0
0
−1
0


As expected, our transformed w value will no longer be 1. Also note that the
right-most column of this matrix is all zeros, which means that this matrix
has no inverse. This is to be expected, since we are losing one dimension of
information. Individual points in view space that lie along the same line of
projection will project to a single point in NDC space. Given only the points
in NDC space, it would be impossible to reconstruct their original positions
in view space.
Let’s see how this matrix works in practice. If we multiply it by a generic
point in view space, we get


d/a
0
0
0
0
d
0
0
0
0
d
0
0
0
−1
0




xv
yv
zv
1

=


dxv/a
dyv
dzv
−zv


Dividing out the w (also called the reciprocal divide), we get
xndc = dxv
−azv

224
Chapter 5 Viewing and Projection
yndc = dyv
−zv
zndc = −d
which is what we expect.
So far, we have dealt with projecting x and y and completely ignored z. In
the preceding derivation all z values map to −d, the negative of the distance to
the projection plane. While losing a dimension makes sense conceptually —
we are projecting from a 3D space down to a 2D plane, after all — for practical
reasons it is better to keep some measure of our z values around for z-buffering
and other depth comparisons (discussed in more detail in Chapter 8). Just as
we’re mapping our x and y values within the view window to an interval of
[−1, 1], we’ll do the same for our z values within the near plane and far plane
positions. We’ll specify the near and far values n and f relative to the view
position, so points lying on the near plane have a zv value of −n, which maps
to a zndc value of −1. Those points lying on the far plane have a zv value of −f
and will map to 1 (Figure 5.15).
We’ll derive our equation for zndc in a slightly different way than our xy
coordinates. There are two parts to mapping the interval [−n, −f ] to [−1, 1].
The ﬁrst is scaling the interval to a width of 2, and the second is translating it to
[−1, 1]. Ordinarily, this would be a straightforward linear process, however we
also have to contend with the ﬁnal w divide. Instead, we’ll create a perspective
matrix with unknowns for the scaling and translation factors and use the
fact that we know the ﬁnal values for −n and −f to solve for the unknowns.
y-axis
–z-axis
–zv = –near
zndc = –1
–zv = –far
zndc = 1
Figure 5.15 Perspective projection: z values.

5.3 Projective Transformation 225
Our starting perspective matrix, then, is


d/a
0
0
0
0
d
0
0
0
0
A
B
0
0
−1
0


where A and B are our unknown scale and translation factors, respectively.
If we multiply this by a point (0, 0, −n) on our near plane:


d/a
0
0
0
0
d
0
0
0
0
A
B
0
0
−1
0




0
0
−n
1

=


0
0
−An + B
n


Dividing out the w gives
zndc = −A + B
n
We know that any point on the near plane maps to a normalized device
coordinate of −1, so we can substitute −1 for zndc and solve for B, which
gives us
B = (A −1)n
(5.3)
We’ll substitute equation 5.3 into our original matrix and multiply by a point
(0, 0, −f ) on the far plane now:


d/a
0
0
0
0
d
0
0
0
0
A
(A −1)n
0
0
−1
0




0
0
−f
1

=


0
0
−Af + (A −1)n
f


This gives us a zndc of
zndc = −A + (A −1) n
f
= −A + A
 n
f

−n
f
= A
 n
f −1

−n
f

226
Chapter 5 Viewing and Projection
Setting zndc to 1 and solving for A, we get
A
 n
f −1

−n
f = 1
A
 n
f −1

= n
f + 1
A =
n
f + 1
n
f −1
= n + f
n −f
If we substitute this into equation 5.3, we get
B =
2nf
n −f
So our ﬁnal perspective matrix is
Mpersp =


d
a
0
0
0
0
d
0
0
0
0
n+f
n−f
2nf
n−f
0
0
−1
0


The matrix that we have generated is the same one produced by an OpenGL
call: gluPerspective(). This function takes the ﬁeld of view1, aspect ratio, and
near and far plane settings, builds the perspective matrix, and multiplies it by
the current matrix.
It is important to be aware that this matrix will not work for all view-
ing systems. For one thing, for most other viewing systems (i.e., other than
OpenGL), our view frame looks down the positive z-axis, so this affects
both our xy and z transformations. For example, in this case we have
mapped [−n, −f ] to [−1, 1]. With the standard system we would want to
begin by mapping [n, f ] to the NDC z range. In addition, this range is not
always set to [−1, 1]. Direct3D, for one, maps to [0, 1] in the z direction.
1.
Recall that our value d is generated from the ﬁeld of view by d = cot(θf ov/2).

5.3 Projective Transformation 227
Using the standard view frame and this mapping gives us a perspective
transformation matrix of
MpD3D =


d
a
0
0
0
0
d
0
0
0
0
f
f −n
−nf
f −n
0
0
1
0


This matrix can be derived using the same principles described above.
When setting up a perspective matrix, it is good to be aware of the issues
involved in rasterizing z values. In particular, to maintain z precision keep the
near and far planes as close together as possible. More details on managing
perspective z precision can be found in Chapter 8.
5.3.6 Oblique Perspective
Demo
Stereo
The matrix we constructed in the previous section is an example of a standard
perspective matrix, where the direction of projection through the center of the
view window is perpendicular to the view plane. A more general example of
perspective is generated by the OpenGL glFrustum() call. This call takes six
parameters: the near and far z distances, as before, and four values that deﬁne
our view window on the near z plane: the x interval [l, r] (left, right) and the
y interval [b, t] (bottom, top). Figure 5.16a shows how this looks in R3, and
Figure 5.16b shows the cross section on the yz plane. As we can see, these
values need not be centered around the z-axis, so we can use them to generate
an oblique projection.
(top, left, –near)
(top, right, –near)
(bottom, right, –near)
(bottom, left, –near)
Figure 5.16a View window for glFrustum, 3D view.

228
Chapter 5 Viewing and Projection
y-axis
eyepoint
near plane
–z-axis
–near
(top,–near)
(bottom,–near)
Figure 5.16b View window for glFrustum, cross-section.
To derive this matrix, once again we begin by considering similar triangles
in the y-direction. Remember that given a point (yv, −zv), we project to a point
on the view plane (dyv/−zv, −d), where d is the distance to the projection.
However, since we’re using our near plane as our projection plane, this is
just (nyv/−zv, −n). The projection remains the same, we’re just moving the
window of projected points that lie within our view frustum.
With our previous derivation, we could stop at this point because our view
window on the projection plane was already in the interval [−1, 1]. However,
our new view window lies in the interval [b, t]. We’ll have to adjust our values
to properly end up in NDC space. The ﬁrst step is to translate the center of
the window, located at (t + b)/2, to the origin. Applying this translation to the
current projected y coordinate gives us
y′ = y −(t + b)
2
We now need to scale to change our interval from a magnitude of (t −b)
to a magnitude of 2 by using a scale factor 2/(t −b):
yndc =
2y
t −b −2(t + b)
2(t −b)
(5.4)
If we substitute nyv/−zv for y and simplify, we get
yndc =
2n yv
−zv
t −b −2(t + b)
2(t −b)

5.3 Projective Transformation 229
=
2n yv
−zv
t −b −
(t + b)−zv
−zv
t −b
=
1
−zv
 2n
t −byv + t + b
t −bzv

A similar process gives us the following for the x direction:
xndc =
1
−zv
 2n
r −l xv + r + l
r −l zv

We can use the same A and B from our original perspective matrix, so our
ﬁnal projection matrix is
Moblpersp =


2n
r−l
0
r+l
r−l
0
0
2n
t−b
t+b
t−b
0
0
0
n+f
n−f
2nf
n−f
0
0
−1
0


A casual inspection of this matrix gives some sense of what’s going on here.
We have a scale in the x, y, and z directions, which provides the mapping to
the interval [−1, 1]. In addition, we have a translation in the z direction to align
our interval properly. However, in the x and y directions, we are performing
a z shear to align the interval, which provides us with the oblique projection.
The equivalent Direct3D matrix is
MopD3D =


2n
r−l
0
−r+l
r−l
0
0
2n
t−b
−t+b
t−b
0
0
0
f
f −n
−nf
f −n
0
0
1
0


5.3.7 Orthographic Parallel Projection
Demo
Orthographic
After considering perspective projection in two forms, orthographic projec-
tion is much easier. Examine Figure 5.17, which shows a side view of our
projection space as before, with the lines of projection passing through the
view plane and the near and far planes shown as vertical lines. This time the
lines of projection are parallel to each other (hence this is a parallel projection)
and parallel to the z-axis (hence an orthographic projection).

230
Chapter 5 Viewing and Projection
y-axis
eyepoint
near plane
1
far plane
–z-axis
(top,–near)
(bottom,–near)
Figure 5.17 Orthographic projection construction.
We can use this to help us generate the matrix for the OpenGL glOrtho()
call. Like glFrustum(), this call takes six parameters: the near and far z dis-
tances, and four values l, r, b, and t that deﬁne our view window on the near
z plane. As before, the near plane is our projection plane, so a point (yv, zv)
projects to a point (yv, −n). Note that since this is a parallel projection, there is
no division by z or scale by d; we just use the y value directly. Like glFrustum()
we now need to consider only values between t and b and scale and translate
them to the interval [−1, 1]. Substituting yv into our range transformation
equation 5.4, we get
yndc = 2yv
t −b −t + b
t −b
A similar process gives us the equation for xndc. We can do the same for
zndc, but since our viewable z values are negative and our values for n and
f are positive, we need to negate our z value and then perform the range
transformation. The result of all three equations is
Mortho =


2
r−l
0
0
−r+l
r−l
0
2
t−b
0
−t+b
t−b
0
0
−
2
f −n
−f +n
f −n
0
0
0
1


There are a few things we can notice about this matrix. First of all, multiply-
ing by this matrix gives us a w value of 1, so we don’t need to perform the
homogeneous division. This means that our z values will remain linear; that

5.3 Projective Transformation 231
is, they will not compress as they approach the far plane. This gives us better
z resolution at far distances than the perspective matrices. It also means that
this is a linear transformation matrix and possibly invertible.
Secondly, in the x and y directions, what was previously a z-shear in the
oblique perspective matrix has become a translation. Before, we had to use
shear because for a given point the displacement was dependent on the dis-
tance from the view position. Because the lines of projection are now parallel,
all points displace equally, so only a translation is necessary.
The Direct3D equivalent matrix is
MorthoD3D =


2
r−l
0
0
−r+l
r−l
0
2
t−b
0
−t+b
t−b
0
0
1
f −n
−
n
f −n
0
0
0
1


5.3.8 Oblique Parallel Projection
Demo
Oblique
While most of the time we’ll want to use orthographic projection, we may
from time to time need an oblique parallel projection. For example, suppose
for part of our interface we wish to render our world as a set of schematics
or display particular objects with a 2D CAD/CAM feel. This set of projections
will achieve our goal.
Neither OpenGL nor Direct3D have a particular routine that handles
oblique parallel projections, so we’ll derive one ourselves. We will give our
projection a slight oblique angle (cot−1(1/2), which is about 63.4 degrees),
which gives a 3D look without perspective. More extreme angles in x and y
tend to look strangely ﬂat.
Figure 5.18 is another example of our familiar cross section, this time
showing the lines of projection for our oblique projection. As we can see, we
move one unit in the y direction for every two units we move in the z direction.
Using the formula of tan(θ) = opposite/adjacent, we get
tan(θ) = 2
1
cot(θ) = 1
2
θ = cot−1 1
2
which conﬁrms the expected value for our oblique angle.
As before, we’ll consider the yz case ﬁrst and extrapolate to x. Moving
1 unit in y and 2 units in −z gives us the vector (1, −2), so the formula for the

232
Chapter 5 Viewing and Projection
y-axis
eyepoint
θ
projection plane
1
2
–z-axis
Figure 5.18 Example of oblique parallel projection.
line of projection for a given point P is
L(t) = P + t(1, −2)
We’re only interested in where this line crosses the near plane, or where
Pz −2t = −n
Solving for t:
t = 1
2(n + Pz)
Plugging this into the formula for the y-coordinate of L(t), we get
y′ = Py + 1
2(n + Pz)
Finally, we can plug this into our range transformation equation 5.4 as
before to get
yndc = 2

yv + 1
2(n + zv)

t −b
−t + b
t −b
= 2yv
t −b −t + b
t −b + zv + n
t −b

5.4 Culling and Clipping 233
Once again, we examine our transformation equation more carefully. This
is the same as the orthographic transformation we had before, with an addi-
tional z-shear, as we’d expect for an oblique projection. In this case the shear
plane is the near plane rather than the xy plane, so we add an additional factor
of
n
t−b to take this into account.
A similar process can be used for x. Since the oblique projection has a
z-shear, z is not affected and so:
Mcab =


2
r−l
0
1
r−l
−r+l−n
r−l
0
2
t−b
1
t−b
−t+b−n
t−b
0
0
−
2
f −n
−n+f
f −n
0
0
0
1


The Direct3D equivalent matrix is
McabD3D =


2
r−l
0
1
r−l
−r+l−n
r−l
0
2
t−b
1
t−b
−t+b−n
t−b
0
0
1
f −n
−
n
f −n
0
0
0
1


5.4 Culling and Clipping
5.4.1 Why Cull or Clip?
In order to improve rendering, both for speed and appearance’s sake, it is
necessary to cull and clip objects. Culling is the process of removing objects
from consideration for some process, whether it be rendering, simulation, or
collision detection. In this case that means we want to ignore any models or
whole pieces of geometry that lie outside of the view frustum, since they will
never end up being projected to the view window. In Figure 5.19, the lighter
objects lie outside of the view frustum and so will be culled for rendering.
Clipping is the process of cutting geometry to match a boundary, whether
it be a polygon or, in our case, a plane. Vertices that lie outside the bound-
ary will be removed and new ones generated for each edge that crosses the
boundary. For example, in Figure 5.20 we see a cube being clipped by a plane,
showing the extra vertices created where each edge intersects the plane. We’ll
use this for any models that cross the view frustum, cutting the geometry
so that it ﬁts within the frustum. We can think of this as slicing a piece of
geometry off for every frustum plane.

234
Chapter 5 Viewing and Projection
Figure 5.19 View frustum culling.
Figure 5.20 View frustum clipping.
Why should we want to use either of these for rendering? For one thing,
it is more efﬁcient to remove any data that will not ultimately end up on the
screen. While copying the transformed object to the frame buffer (a process
called rasterization) is almost always done in hardware and thus is fast, it is
not free. Anywhere we can avoid unnecessary work is good.
But even if we had inﬁnite rasterization power, we would still want to
cull and clip when performing perspective projection. Figure 5.21 shows one
example why. Recall that we ﬁnessed the problem of the camera obscura
inverting images by moving the view plane. However, we still have the same
problem if an object is behind the view position; it will end up projected upside
down. The solution is to cull objects that lie behind the view position.
Figure 5.22a shows another example. Suppose we have a polygon edge
that crosses the z = 0 plane. With the correct projection, the line segment
starts at the middle of the view, moves up, and wraps around to reemerge at

5.4 Culling and Clipping 235
y-axis
projection plane
–z-axis
Figure 5.21 Projection of objects behind the eye.
projection plane
view direction
P
Q'
Q
P'
eye
Figure 5.22a Projection of line segment crossing behind view point.
the bottom of the view. In practice, however, the rendering hardware has only
the two projected vertices as input. It will end up taking the short route and
rasterizing the wrong line segment between the two vertices (Figure 5.22b). If
we clip the line segment to only the section that is viewable (Figure 5.22c), we
end with only a portion of the line segment, but at least it is from the correct
projection.

236
Chapter 5 Viewing and Projection
projection plane
view direction
P
Q'
Q
P'
eye
Figure 5.22b Incorrect line segment rendering based on projected endpoints.
projection plane
near plane
view direction
P
Pclip'
Pclip
Q'
Q
eye
Figure 5.22c Line segment rendering when clipped to near plane.
There is also the problem of vertices that lie on the z = 0 plane. When
transformed to homogeneous space by the perspective matrix, a point
(x, y, 0, 1) will become (x′, y′, z′, 0). The resulting transformation into NDC
space will be a division by 0, which is not valid.

5.4 Culling and Clipping 237
To avoid all of these issues, at the very least we need to set a near plane
that lies in front of the eye so that the view position itself does not lie within
the view frustum. We ﬁrst cull any objects that lie on the same side of the
near plane as the view position. We then clip any objects that cross the near
plane. This avoids both the potential of dividing by 0 (although it is sometimes
prudent to check for it anyway, at least in a debug build) and trying to render
any line segments passing through inﬁnity.
While clipping to a near plane is a bare minimum, clipping to the top, bot-
tom, left, and right planes is useful as well. While the windowing hardware
will usually ignore any pixels that lie outside of a window’s visible region (this
is commonly known as scissoring), it is faster if we can avoid unnecessary
rasterization. Also, if we want to set a viewport that covers a subrectangle of a
window, not clipping to the border of the viewport may lead to spurious geom-
etry being drawn (although most hardware allows for adjustable scissoring
regions; in particular, OpenGL and D3D provide interfaces to set this).
Finally, some hardware has a limited range for screen space positions, for
example, 0 to 4095. The viewable area might lie in the center of this range, say
from a minimum point of (1728,1808) to a maximum point of (2688,2288).
The area outside of the viewable area is known as the guard band — anything
rendered to this will be ignored, since it won’t be displayed. In some cases
we can avoid clipping in x and y, since we can just render objects whose
screen space projection lies within the guard band and know that they will
be handled automatically by the hardware. This can improve performance
considerably, since clipping can be quite expensive. However, it’s not entirely
free. Values that lie outside the maximum range for the guard band will wrap
around. So a vertex that would normally project to coordinates that should lie
off the screen, say (6096,6096), will wrap to (2000,2000) — right in middle of
the viewable area. Unfortunately, the only way to solve this problem is what
we were trying to avoid in the ﬁrst place: clipping in the x and y directions.
On the other hand, using the guard band carefully can reduce the amount of
clipping that we have to do overall.
5.4.2 Culling
A naive method of culling a model against the view frustum is to test each of
its vertices against each of the frustum planes in turn. We designate the plane
normal for each plane as pointing towards the “inside” half-space. If for one
plane ax + by + cz + d < 0 for every vertex P = (x, y, z), then the model lies
outside of the frustum and we can ignore it. Conversely, if for all the frustum
planes and all the vertices ax + by + cz + d > 0, then we know the model lies
entirely inside the frustum and we don’t need to worry about clipping it.
While this will work, for models with large numbers of vertices this
becomes expensive, probably outweighing any savings we might gain by not

238
Chapter 5 Viewing and Projection
rendering the objects. Instead, culling is usually done by approximating the
object with a convex bounding volume, such as a sphere, that contains all of
the vertices for the object. Rather than test each vertex against the planes, we
test only the bounding object. Since it is a convex object and all the vertices
are contained within it, we know that if the bounding object lies outside of
the view frustum, all of the model’s vertices must lie outside as well. More
information on computing bounding objects and testing them against planes
can be found in Chapter 11.
Bounding objects are usually placed in the world frame to aid with col-
lision detection, so culling is often done in the world frame as well. This
requires storing a representation of each frustum plane in world coordinates,
but the additional 24 values required is worth the speedup gained. We can
ﬁnd each x or y clipping plane in the view frame by using the view position
and two corners of the view window to generate the player. The two z planes
(in OpenGL) are z = −near and z = −f ar, respectively. Transforming them
to the world frame is a simple case of using the technique for transforming
plane normals, as described in Chapter 3.
While view frustum culling can remove a large number of objects from
consideration, it’s not the only culling method. In Chapter 6 we’ll discuss
backface culling, which allows us to determine which polygons are pointing
away from the camera and ignore them. There also are a large number of
culling methods that break up the scene in order to cull objects that aren’t
visible. This can help with interior levels, so you don’t render rooms that may
be within the view frustum but not visible because they’re blocked by a wall.
Such methods are out of the purview of this book but are described in detail
in many of the references cited in the following sections.
5.4.3 General Plane Clipping
Demo
Clipping
To clip polygons, we ﬁrst need to know how to clip a polygon edge (i.e., a line
segment) to a plane. As we’ll see, the problem of clipping a polygon to a plane
degenerates to handling this case. Suppose we have a line segment PQ, with
endpoints P and Q, that crosses a plane. We’ll say that P is inside our clip
space and Q is outside. Our clipped line segment will be PR, where R is the
intersection of the line segment and the plane (Figure 5.23).
To ﬁnd R, we take the line equation P + t(Q −P), plug it into our plane
equation ax + by + cz + d = 0, and solve for t. To simplify the equations, we’ll
deﬁne v = Q −P. Substituting the parameterized line coordinates for x, y,
and z, we get
0 = a(Px + tvx) + b(Py + tvy) + c(Pz + tvz) + d
= aPx + tavx + bPy + tbvy + cPz + tcvz + d

5.4 Culling and Clipping 239
Q
R
P
Figure 5.23 Clipping edge to plane.
= aPx + bPy + cPz + d + t(avx + bvy + cvz)
t = −aPx −bPy −cPz −d
avx + bvy + cvz
=
(aPx + bPy + cPz + d)
(aPx + bPy + cPz + d) −(aQx + bQy + cQz + d)
We can use Blinn’s notation [11], slightly modiﬁed, to simplify this to
t =
BCP
BCP −BCQ
where BCP is the result from the plane equation (the boundary coordinate)
when we test P against the plane, and BCQ is the result when we test Q against
the plane. The resulting clip point R is
R = P +
BCP
BCP −BCQ(Q −P)
To clip a polygon to a plane, we need to clip each edge in turn. A standard
method for doing this is to use the Sutherland-Hodgeman algorithm [107].
For each edge we ﬁrst test it against the plane. Depending on what the result is,
we output particular vertices for the clipped polygon. There are four possible
cases for an edge from P to Q (Figure 5.24). If both are inside, then we output
P . The vertex Q will be output when we consider it as the start of the next
edge. If both are outside, we output nothing. If P is inside and Q is outside,
then we compute R, the clip point, and output P and R. If P is outside and Q

240
Chapter 5 Viewing and Projection
Q
P
Q
P
output P
no output
Q
R P
Q
R
P
output P,R
output R
Figure 5.24 Four possible cases of clipping an edge against a plane.
is inside, then we compute R and output just R — as before, Q will be output
as the start of the next edge. The sequence of vertices generated as output will
be the vertices of our clipped polygon.
We now have enough information to build a class for clipping vertices,
which we’ll call IvClipper. We can deﬁne this as
class IvClipper
{
public:
IvClipper()
{
mFirstVertex = true;
}
∼IvClipper();
void ClipVertex( const IvVector3& end )
inline void StartClip() { mFirstVertex = true; }
inline void SetPlane( const IvPlane& plane ) { mPlane = plane; }
private:
IvPlane
mPlane;
// current clipping plane
IvVector3 mStart;
// current edge start vertex
float
mBCStart; // current edge start boundary condition

5.4 Culling and Clipping 241
bool
mStartInside; // whether current start vertex is inside
bool
mFirstVertex; // whether expected vertex is start vertex
};
Note that IvClipper::ClipVertex() takes only one argument: the end
vertex of the edge. If we send the vertex pair for each edge down to the
clipper, we’ll end up duplicating computations. For example, if we clip P0
and P1, and then P1 and P2, we have to determine whether P1 is inside or
outside twice. Rather than do that, we’ll feed each vertex in order to the clip-
per. By storing the previous vertex (mStart) and its plane test information
(mBCStart) in our IvClipper class, we need to calculate data only for the cur-
rent vertex. Of course, we’ll need to prime the pipeline by sending in the
ﬁrst vertex, not treating it as part of an edge, and just storing its boundary
information.
Using this, clipping an edge based on the current vertex might look like
void IvClipper::ClipVertex( const IvVector3& end )
{
float BCend = mPlane.Test(end);
bool endInside = ( BCend >= 0 );
if (!mFirstVertex)
{
// if one of the points is inside
if ( mStartInside || endInside )
{
// if the start is inside, just output it
if (mStartInside)
Output( mStart );
// if one of them is outside, output clip point
if ( !(mStartInside && endInside) )
{
if (endInside)
{
float t = BCend/(BCend - mBCStart);
Output( end - t*(end - mStart) );
}
else
{
float t = mBCStart/(mBCStart - BCend);
Output( mStart + t*(end - mStart) );
}
}
}
}

242
Chapter 5 Viewing and Projection
mStart = end;
mBCStart = BCend;
mStartInside = endInside;
mFirstVertex = false;
}
Note that we generate t in the same direction for both clipping cases — from
inside to outside. Polygons will often share edges. If we were to clip the same
edge for two neighboring polygons in different directions, we may end up
with two slightly different points due to ﬂoating-point error. This will lead
to visible cracks in our geometry, which is not desirable. Interpolating from
inside to outside for both cases avoids this situation.
To clip against the view frustum, or any other convex volume, we need to
clip against each frustum plane. The output from clipping against one plane
becomes the input for clipping against the next, creating a clipping pipeline.
In practice, we don’t store the entire clipped polygon, but pass each output
vertex down as we generate it. The current output vertex and the previous
one are treated as the edge to be clipped by the next plane. The Output() call
above becomes a ClipVertex() for the next stage.
Note that we have only handled generation of new positions at the clip
boundary. There are other parameters that we can associate with an edge
vertex, such as colors, normals, and texture coordinates (we’ll discuss exactly
what these are in Chapters 6–8). These will have to be clipped against the
boundary as well. We use the same t value when clipping these parameters,
so the clip part of our previous algorithm might become
// if one of them is outside, output clip vertex
if ( !(mStartInside && endInside) )
{
...
clipPosition = startPosition + t*(endPosition - startPosition);
clipColor = startColor + t*(endColor - startColor);
clipTexture = startTexture + t*(endTexture - startTexture);
// Output new clip vertex
}
This is only one example of a clipping algorithm. In most cases, it
won’t be necessary to write any code to do clipping. The hardware will han-
dle any clipping that needs to be done for rendering. However, for those
who have the need or interest, other examples of clipping algorithms are
the Liang-Barsky [73] , Cohen-Sutherland (found in [36] as well as other
graphics texts), and Cyrus-Beck [22] methods. Blinn [11] describes an algo-
rithm for lines that combines many of the features from the previously

5.4 Culling and Clipping 243
mentioned techniques; with minor modiﬁcations it can be made to work with
polygons.
5.4.4 Homogeneous Clipping
In the presentation above, we clip against a general plane. When projecting,
however, Blinn and Newell [9] noted that we can simplify our clipping by tak-
ing advantage of some properties of our projected points prior to the division
by w. Recall that after the division by w, the visible points will have normalized
device coordinates lying in the interval [−1, 1], or
−1 ≤x/w ≤1
−1 ≤y/w ≤1
−1 ≤z/w ≤1
Multiplying these equations by w provides the intervals prior to the w
division:
−w ≤x ≤w
−w ≤y ≤w
−w ≤z ≤w
In other words, the visible points are bounded by the six planes:
w = x
w = −x
w = y
w = −y
w = z
w = −z
Instead of clipping our points against general planes in the world frame or
view frame, we can clip our points against these simpliﬁed planes in RP 3
space. For example, the plane test for w = x is w −x. The full set of plane tests
for a point P are
BCP−x = w + x
BCPx = w −x

244
Chapter 5 Viewing and Projection
BCP−y = w + y
BCPy = w −y
BCP−z = w + z
BCPz = w −z
The previous clipping algorithm can be used, with these plane tests replac-
ing the IvPlane::Test() call. While these tests are cheaper to compute in
software, their great advantage is that since they don’t vary with the projec-
tion they can be built directly into hardware, making the clipping process very
fast. Because of this, OpenGL supports a two-stage clipping process. First of
all, a point is transformed into the view frame. Then it is clipped against any
user-deﬁned clipping planes set by the glClippingPlane() call. Then the point
is multiplied by the projection matrix, clipped in homogeneous space, and
ﬁnally the coordinates are divided by w to place the clipped point in the NDC
frame.
There is one wrinkle to homogeneous clipping, however. Figure 5.25
shows the visible region for the x-coordinate in homogeneous space. How-
ever, our plane tests will clip to the upper triangle region of that hourglass
shape — any points that lie in the lower region will be inadvertently removed.
With the projections that we have deﬁned, this will happen only if we use a
negative value for the w value of our points. And since we’ve chosen 1 as the
standard w value for points, this shouldn’t happen. However, if you do have
points that for some reason have negative w values, Blinn [11] recommends
the following procedure: transform, clip, and render your points normally.
x-axis
w-axis
w = x
−w = x
Figure 5.25 Homogeneous clip regions for NDC interval [−1,1].

5.5 Screen Transformation 245
Then multiply your projection matrix by −1, then transform, clip, and render
again.
5.5 Screen Transformation
Now that we’ve covered projection and clipping, our ﬁnal step in transforming
our object in preparation for rendering is to map its geometric data from the
NDC frame to the screen or device frame. This could represent a mapping to
the full display, a window within the display, or an offscreen pixel buffer.
Remember that our coordinates in the NDC frame range from a lower
left corner of (−1, −1) to an upper right corner of (1, 1). Real device space
coordinates usually range from an upper left corner (0, 0) to lower right cor-
ner (ws, hs), where ws (screen width) and hs (screen height) are usually not
the same. In addition, in screen space the y axis is commonly ﬂipped so
that y values increase as we move down the screen. Some windowing sys-
tems allow you to use the standard y direction, but we’ll assume the default
(Figure 5.26).
What we’ll need to do is map our NDC area to our screen area (Figure 5.27).
This consists of scaling it to the same size as the screen, ﬂipping our y direc-
tion, and then translating it so that the upper left hand corner becomes the
origin.
Let’s begin by considering only the y direction, because it has the special
case of the axis ﬂip. The ﬁrst step is scaling it. The NDC window is 2 units
high, whereas the screen space window is hs high, so we divide by 2 to scale
(0,0)
(ws,hs)
Figure 5.26 View window in standard screen space frame.

246
Chapter 5 Viewing and Projection
Figure 5.27 Mapping NDC space to screen space.
the NDC window to unit height, and then multiply by hs to scale to screen
height:
y′ = hs
2 yndc
Since we’re still centered around the origin, we can do the axis ﬂip by just
negating:
y′′ = −hs
2 yndc
Finally, we need to translate downwards (which is now the positive y
direction) to map the top of the screen to the origin. Since we’re already
centered on the origin, we need to translate only half the screen height, so:
ys = −hs
2 yndc + hs
2
Another way of thinking of the translation is that we want to map the extreme
point −hs/2 to 0, so we need to add hs/2.
A similar process, without the axis ﬂip, gives us our x transformation:
xs = ws
2 xndc + ws
2
This assumes that we want to cover the entire screen with our view win-
dow. In some cases, for example in a split-screen console game, we want to
cover only a portion of the screen. Again, we’ll have a width and height of our
screen space area, ws and hs, but now we’ll have a different upper left corner
position for our area: (sx, sy). The ﬁrst part of the process is the same; we
scale the NDC window to our screen space window and ﬂip the y-axis. Now,
however, we want to map (−ws/2, −hs/2) to (sx, sy), instead of (0, 0). The ﬁnal

5.5 Screen Transformation 247
translation will be (ws/2 + sx, hs/2 + sy). This gives us our generalized screen
transformation in xy as
xs = ws
2 xndc + ws
2 + sx
(5.5)
ys = −hs
2 yndc + hs
2 + sy
(5.6)
Our z coordinate is a special case. As mentioned, we’ll want to use z for
depth testing, which means that we’d really prefer it to range from 0 to ds,
where ds is usually 1. This mapping from [−1, 1] to [0, ds] is
zs = ds
2 zndc + ds
2
(5.7)
We can, of course, express this as a matrix:
Mndc→screen =


ws
2
0
0
ws
2 + sx
0
−hs
2
0
hs
2 + sy
0
0
ds
2
ds
2
0
0
0
1


Most of the time it is expected that the aspect ratio a chosen in the pro-
jection will match the aspect ratio ws/hs of the ﬁnal screen transformation.
Otherwise, the resulting image will be distorted. For example, if we use a
square aspect ratio (a = 1.0) for the projection and a standard aspect ratio of
4:3 for the screen transformation, the image will appear compressed in the y
direction. If your image does not quite look right, it is good practice to ensure
that these two values are the same.
An exception to this practice arises when your ﬁnal display has a different
aspect ratio than the offscreen buffers that you’re using for rendering. For
example, NTSC televisions have 448 scan lines, with 640 analog pixels per
scan line, so it is common practice to render to a 640 × 448 area and then
send that to the NTSC converter to be displayed. Using the offscreen buffer
size would give an aspect ratio of 10:7. But the actual television screen has a
4:3 aspect ratio, so the resulting image will be distorted, producing stretching
in the y direction. The solution is to set a = 4/3 despite the aspect ratio of
the offscreen buffer. The image in the offscreen buffer will be compressed in
the y direction, but then will be proportionally stretched in the y direction
when the image is displayed on the television, thereby producing the correct
result.

248
Chapter 5 Viewing and Projection
5.6 Picking
Demo
Picking
Now that we understand the mathematics necessary for transforming an
object from world coordinates to screen coordinates, we can consider the
opposite case. In our game we may have enemy objects that we’ll want to
target. The interface we have chosen involves tracking them with our mouse
and then clicking on the screen. The problem is, How do we take our click
location and use that to detect which object we’ve selected, if any? We need a
method that takes our 2D screen coordinates and turns them into a form that
we can use to detect object intersection in 3D game space.
For the purposes of discussion, we’ll assume that we are using the basic
OpenGL perspective matrix. Similar derivations can be created using other
projections. Figure 5.28 is yet another cross section showing our problem.
Once again, we have our view frustum, with our top and bottom clipping
planes, our projection plane, and our near and far planes. Point Ps indicates
our click location on the projection plane. If we draw a ray (known as a pick
ray) from the view position through Ps, we pass through every point that lies
underneath our click location. So to determine which object we have clicked
on, we need only generate this point on the projection plane, create the speciﬁc
ray, and then test each object for intersection with the ray. The closest object
to the eye will be the object we’re seeking.
To generate our point on the projection plane, we’ll have to ﬁnd a method
for going backwards from screen space into view space. To do this we’ll have
to ﬁnd a means to “invert” our projection. Matrix inversion seems like the
solution, but it is not the way to go. The standard projection matrix has zeros
in the right-most column, so it’s not invertible. But even using the z-depth
projection matrix doesn’t help us because (a) the reciprocal divide makes the
y
Ps
projection plane
–z
d
Figure 5.28 Pick ray.

5.6 Picking 249
process nonlinear and (b) in any case our click point doesn’t have a z value to
plug into the inversion.
Instead, we begin by transforming our screen space point (xs, ys) to an
NDC space point (xndc, yndc). Since our NDC to screen space transform is
afﬁne, this is easy enough: we need only invert our previous equations 5.5
and 5.6. That gives us
xndc = 2(xs −sx)
ws
−1
yndc = −2(ys −sy)
hs
+ 1
Now the tricky part. We need to transform our point in the NDC frame to
the view frame. We’ll begin by computing our zv value. Looking at Figure 5.28
again, this is straightforward enough. We’ll assume that our point lies on
the projection plane so the z value is just the z location of the plane or −d.
This leaves our x- and y-coordinates to be transformed. Again, since our view
region covers a rectangle deﬁned by the range [−a, a] (recall that a is our
aspect ratio) in the x direction and the range [−1, 1] in the y direction, we
only need to scale to get the ﬁnal point. The view window in the NDC frame
ranges from [−1, 1], so no scale is needed in the y direction and we scale by a
in the x direction. Our ﬁnal screen space to view space equations are
xv = 2a
ws
(xs −sx) −1
yv = −2
hs
(ys −sy) + 1
zv = −d
And since this is a system of linear equations, we can express this as a
3 × 3 matrix as follows:


xv
yv
zv

=


2a
ws
0
−2a
ws sx −1
0
−2
hs
2
hs sy + 1
0
0
−d




xs
ys
1


From here we have a choice. We can try to detect intersection with an
object in the view frame, we can detect in the world frame, or we can detect
in the object’s local frame. The ﬁrst involves transforming every object into
the view frame and then testing against our pick ray. The second involves
transforming our pick ray into the world frame and testing against the world
coordinates of each object. If we’re using a scene graph, we’re already pre-
generating our world location and bounding information. So if we’re only

250
Chapter 5 Viewing and Projection
concerned with testing for intersection against bounding information, it can
be more efﬁcient to go with testing in world space. However, usually we test
in local space so we can check for intersection within the frame of the stored
model vertices, without having to transform them into the world frame or the
view frame.
In order to do that, we’ll have to transform our view space point by the
inverse of the viewing transformation. Unlike the perspective transformation,
however, this inverse is much easier to compute. Recall that since the view
transformation is an afﬁne matrix, we can invert it to get the view-to-world
matrix Mview→world. So multiplying Mview→world by our click point in the view
frame gives us our point in world coordinates:
Pw = Mview→world · Pv
We can transform this and our view position E from world coordinates into
local coordinates by multiplying by the inverse of the local-to-world matrix:
Pl = Mworld→local · Pw
El = Mworld→local · E
Then the formula for our pick ray in local space is
R(t) = El + t(Pl −El)
We can now use this ray in combination with our models to ﬁnd the par-
ticular object the user has clicked on. Chapter 11 discusses how to determine
intersection between a ray and an object and other intersection problems.
5.7 Management of Viewing
Transformations
Library
IvEngine
Filename
IvGLHelp
Up to this point we have presented a set of transformations and correspond-
ing matrices without giving some sense of how they would ﬁt into a game
engine. While the thrust of this book is not about writing renderers, we can
still provide a general sense of how some renderers and APIs manage these
matrices, and how to set transformations for a standard API.
The view, projection, and screen transformations change only if the cam-
era is moved. As this happens rarely, these matrices are usually computed
once, stored, and then concatenated with the new world transformation every
time a new object instance is rendered. How this is handled depends on the
API used. The most direct approach is to concatenate the newly set world

5.7 Management of Viewing Transformations 251
transform matrix with the others, creating a single transformation all the
way from local space to prehomogeneous divide screen space:
Mlocal→screen = Mndc→screen · Mprojection · Mworld→view · Mlocal→world
Multiplying by this single matrix and then performing three homogeneous
divisions per vertex generates the screen coordinates for the object. This is
extremely efﬁcient, but ignores any clipping we might need to do. In this case,
we can concatenate up to homogeneous space, also known as clip space:
Mlocal→clip = Mprojection · Mworld→view · Mlocal→world
Then we transform our vertices by this matrix, clip against the view frustum,
perform the homogeneous divide, and either calculate the screen coordinates
using equations 5.5–5.7 or multiply by the NDC to screen matrix, as before.
With more complex renderers, we end up separating the transformations
further. For example, OpenGL handles lighting and some clipping prior to
projection, so it has separate GL_MODELVIEW and GL_PROJECTION matrix stacks,
to which the appropriate matrices have to be concatenated. The vertices are
transformed by the top matrix in the GL_MODELVIEW stack, lighting and user-
deﬁned clipping is computed, and then the vertices are transformed by the
top matrix in the GL_PROJECTION matrix. The resulting vertices are clipped in
homogeneous space, the reciprocal divide is performed as before, and ﬁnally
they are transformed to screen space.
In our program, we can set the view and projection matrices in OpenGL by
IvMatrix44 projection, viewTransform;
// compute projection and view transformation
...
// set in OpenGL
glMatrixMode(GL_PROJECTION);
glLoadMatrix( projection );
glMatrixMode(GL_MODELVIEW);
glLoadMatrix( viewTransform );
And when we render an object, concatenating the world matrix can be
done by
glMatrixMode(GL_MODELVIEW);
// push copy of view matrix to top of stack

252
Chapter 5 Viewing and Projection
glPushMatrix();
// multiply by world matrix
glMultMatrix( worldTransform );
// render
...
// pop to view matrix
glPopMatrix();
The push/pop calls provide a means for storing the view transformation
without reloading it into the stack. The call glPushMatrix() copies the cur-
rent matrix — in this case, the view matrix — to a new entry on the top of the
stack. The subsequent glMultMatrix() will postmultiply the world matrix by
the copy of the view matrix at the top of the stack. The resulting local-to-view
matrix will be used to transform the vertices of our object. Finally, glPop-
Matrix() removes the current matrix from the top of the stack, restoring the
view transformation as the top matrix. The effect is to save the view transfor-
mation, multiply by the world transformation and use the result to transform
the vertices, and then restore the original view transformation.
Direct3D takes this one step further, and manages storage of the view
transformation by having three separate matrices: one each for the pro-
jective, view, and world transformations. These can be set by using the
IDirect3DDevice*::SetTransform() method, and any concatenation is handled
internally to the API.
This leaves the NDC to screen space transformation. Usually the graphics
API will not require a matrix but will perform this operation directly. In the xy
directions the user is only expected to provide the dimensions and position of
the screen window area, also known as the viewport. In OpenGL this is set by
using the call glViewport(). For the z direction, OpenGL provides a function
glDepthRange(), which maps [−1, 1] to [near, f ar], where the defaults for near
and f ar are 0 and 1. Similar methods are available for other APIs.
In our case we have decided not to overly complicate things and are
providing simple convenience routines:
::IvSetWorldMatrix()
::IvSetViewMatrix()
::IvSetProjectionMatrix()
::IvSetViewport()
which act as wrappers for the OpenGL calls described.

5.8 Chapter Summary 253
5.8 Chapter Summary
Manipulating objects in the world frame is only as useful as the techniques
that we use to present that data. In this chapter we have discussed the viewing,
projection, and screen transformations necessary for rendering objects on a
screen or image. While we have focused on OpenGL as our rendering API,
the same principles apply to Direct3D or any other rendering system. We
transform the world to the perspective of a virtual viewer, project it to a view
plane, and then scale and translate the result to ﬁt our ﬁnal display. We also
covered how to reverse those transformations to allow one to select an object
in view or world space by clicking on the screen. In the following chapters,
we will discuss how to use the data generated by these transformations to
actually set pixels on the screen.
For those who are interested in reading further, most graphics text-
books — such as Möller and Haines [79] and Foley and van Dam [36] —
describe the graphics pipeline in great detail. In addition, one of Blinn’s
collections [11] is almost entirely dedicated to this subject. Various culling
techniques are discussed in Möller and Haines [79], as well as Eberly [27].
Finally,
the OpenGL Programming Guide [83] discusses the particular
implementation of the graphics pipeline used in OpenGL.


Chapter6
Geometry,
Shading, and
Texturing
6.1 Introduction
Having discussed in detail in the preceding chapters how to represent, trans-
form, view, and animate geometry, the next three chapters form a sequence
that describes the second half of the “rendering pipeline.” The second
half of the rendering pipeline is speciﬁcally focused on visual matters: the
representation, computation, and usage of color.
This chapter will discuss how we connect the points we have been trans-
forming and projecting to form solid surfaces, as well as the extra information
we use to represent the unique appearance of each surface. All visual repre-
sentations of geometry require the computation of colors; this chapter will
discuss the data structures used to store colors and perform basic color
computations. It will also discuss methods used to assign static colors to
geometry, including image-based texturing.
Chapter 7 will detail common, real-time 3D approximations to dynamic
lighting, including light sources, surface materials, lighting models, and their
applications. Chapter 7 will complete our discussion of the so-called geometry
pipeline, having taken our objects from model space to screen space and
from colorless vectors to lit, textured surfaces.
As the concluding chapter in this sequence, Chapter 8 will cover the ﬁnal
step in the overall rendering pipeline — rasterization, or the method of deter-
mining how to draw the colored surfaces to pixels on the display device. This
will complete the discussion of the rendering pipeline.
255

256
Chapter 6 Geometry, Shading, and Texturing
Each section in these chapters will relate the basic OpenGL concepts,
data structures, and functions that affect the creation, rendering, and color-
ing of geometry. As we move from geometry representation through shading,
lighting, and rasterization, OpenGL information will become increasingly fre-
quent, as the implementation of the ﬁnal stages of the rendering pipeline
are very much system-dependent. However, the basic rendering concepts
discussed will apply to most rendering systems.
As a note, we use the phrase OpenGL implementation to refer to the under-
lying software or “driver” that maps our application calls to OpenGL into
commands for a particular piece of graphics hardware. The OpenGL imple-
mentation for a particular piece of graphics hardware is generally supplied
with the device by the hardware vendor. It is not something that users of
OpenGL will have to write or even use directly. In fact, the main purpose
of OpenGL is to provide a standard interface on top of these widely varying
hardware/software 3D systems.
6.2 Color Representation
6.2.1 The RGB Color Model
To represent color, this chapter will use the additive RGB (red, green, blue)
color model that is almost universal in real-time 3D systems. Approximating
the physiology of the human visual system (which is tuned to perceive color
based on three primitives that are close to these red, green, and blue colors),
the RGB system is used in all common display devices used by real-time
3D graphics systems. Color cathode ray tubes (or CRTs, such as traditional
televisions and computer monitors), ﬂat-panel liquid crystal displays (LCDs),
plasma displays, and video projector systems are all based upon the additive
RGB system. While some colors cannot be accurately displayed using the RGB
model, it does support a very wide range of colors, as proven by the remark-
able color range and accuracy of modern television and computer displays.
For a detailed discussion of color vision and the basis of the red, green, blue
color model, see [74].
The RGB color model involves mixing different amounts of three pre-
deﬁned primary colors of light. These carefully deﬁned primary colors are
each named by the named colors that most closely match them; red, green, and
blue. By mixing independently controlled levels of these three colors of light,
a wide range of brightnesses, tones, and shades may be created. For example,
a few very general color mixes and the named color that results are
Equal parts red and green →yellow

6.2 Color Representation 257
Two parts red, one part green →orange
Equal parts of all three colors →black, gray, or white
Note that no mention of the exact levels of these colors is given. Brighter or
darker versions of these colors can be created by changing the overall amounts
of all three primary components. The next few sections will deﬁne much more
speciﬁcally how we build and represent colors using this method.
As mentioned, the levels of each of these three primary colors are
independent. In a sense, this is similar to a subset of R3, but with a “basis”
consisting of the red, green, and blue “axes,” or components. While these can
be thought of as a “basis” for our display device’s color space, they are not
a basis in any true sense for color in general.
Monochrome or grayscale displays are quite similar to color displays,
but have only a single color component instead of three. For the pur-
poses of this chapter, we will discuss only methods that are designed to
supply full-color displays with the data they require. The monochrome sit-
uation may be simulated by using only gray values between black and
white.
6.2.2 Colors as “Vectors”
The representation of colors as amounts of independent red, green, and blue
primaries is conceptually very similar to our ideas of a vector space. In this
case, our “basis vectors” represent the three color primaries. As we shall see,
while this is a useful implementation method, the behavior of colors does not
always map directly into the concept of a real vector space. However, many of
the concepts of real vector spaces are useful in describing color representation
and operations.
Our colors will be represented by 3-vectors, with the following basis
vectors:
(1, 0, 0) →red
(0, 1, 0) →green
(0, 0, 1) →blue
Often, as a form of shorthand, we will refer to the red component of a color
c as cr and to the green and blue components as cg and cb, respectively.
The following sections will describe some of the vector operations (and
vectorlike operations) we will apply to colors, as well as discussions of
how these abstract color vectors map onto their ﬁnal destinations, namely
hardware display devices.

258
Chapter 6 Geometry, Shading, and Texturing
6.2.3 Operations on Colors
Adding RGB colors is done using a method equivalent to vector addition; the
colors are added componentwise. This has the same effect as combining the
light from two light sources whose colors are equal to those of the operands;
for example, adding red (r = (1, 0, 0)) and green (g = (0, 1, 0)) gives yellow:
r + g = (1, 0, 0) + (0, 1, 0) = (1, 1, 0)
The operation of adding colors will be used through our lighting computations
to represent the addition of light from multiple light sources and to add the
multiple forms of light that each source can apply to a surface.
Scalar multiplication of RGB colors (sc) is computed in the same way as
with vectors, multiplying the scalar times each component, and is ubiqui-
tous in lighting and other color computations. It has the result of increasing
(s > 1.0) or decreasing (s < 1.0) the luminance of the color by the amount of
the scalar factor. Scalar multiplication is most frequently used to represent
light attenuation due to various physical and geometric lighting properties.
One important vector operation that is used somewhat rarely with colors
is vector length. While it might seem that vector length would be an excellent
(if expensive) way to compute the “luminance” of a color, the nature of human
color perception does not match the Euclidean norm of the linear RGB color
space. Luminance is a “norm” that is affected by the device used to display the
color, human physiology, and mathematics. The human eye is most sensitive
to green, then red, and ﬁnally to blue. As a result, the equal weighting given
to all components by the Euclidean norm means that blue contributes to the
Euclidean norm far more than it contributes to luminance.
Although there are numerous methods used to compute the luminance
of RGB colors as displayed on a screen, a common method for modern CRT
screens (assuming nonnegative color components) is
luminance(c) = 0.2125cr + 0.7154cg + 0.0721cb
The three color-space transformation coefﬁcients used to scale the color com-
ponents are basically constant for modern, standard CRT screens but do not
neccessarily apply to television screens, which use a different set of luminance
conversions. Discussion of these may be found in [90]. Note that luminance
is not equivalent to perceived brightness. The luminance as we’ve computed it
is linear with respect to the source linear RGB values. Brightness as perceived
by the human visual system is nonlinear and subject to the overall brightness
of the viewing environment, as well as the viewer’s adaptation to it. See [20]
for a related discussion of the physiology of human visual perception.
An operation that is rarely applied to vectors but is used very frequently
with colors is componentwise multiplication. Componentwise multiplication

6.2 Color Representation 259
takes two colors as operands and produces another color as its result. We will
represent the operation of componentwise multiplication of colors as “·”, or
in shorthand by placing the colors next to one another (as we would multiply
scalars), and the operation is deﬁned as follows:
a · b = ab = (arbr, agbg, abbb)
This operation is often used to represent the ﬁltering of one color of light
through an object of another color. In such a situation, one operand is
assumed to be the light color, while the other operand is assumed to be the
amount of light of each component that is passed by the ﬁlter. Another use of
componentwise color multiplication is to represent the reﬂection of light from
a surface — one color represents the incoming light and the other represents
the amount of each component that the given surface reﬂects. We will use this
frequently in the next chapter when computing lighting. For example, a color
c and a ﬁlter (or surface) f = (1, 1, 1) result in
cf = c
In this case the ﬁlter was a perfectly efﬁcient piece of clear glass — all light
passed through (or a perfect mirror, with all light reﬂecting in the surface
example). However, if the ﬁlter color were to have been f = (1, 0, 0), the result
would be
cf = (cr, 0, 0)
or the equivalent of a pure red ﬁlter; only the red component of the light
was passed, while all other light was blocked. This operation will be used
constantly in color lighting computations.
6.2.4 Color Range Limitation
The theoretical RGB color space is semi-inﬁnite in all three axes. There is an
absolute zero value for each component, bounding the negative directions, but
the positive directions are (theoretically) unbounded. The reality of physical
display devices imposes severe limitations on the color space. In fact, when
limited to the colors that can be represented by a speciﬁc display device, the
RGB color space is not inﬁnite in any direction. Real display devices for real-
time 3D, such as CRTs (standard “tube” monitors), LCD panel displays, and
video projectors all have limits of both brightness and darkness in each color
component; these are basic physical limitations of the technologies that these
displays use to emit light. For details on the functionality and limitations of

260
Chapter 6 Geometry, Shading, and Texturing
display device hardware, Hearn and Baker [56] detail many popular display
devices.
Displays generally have minimum and maximum brightnesses in each of
their three color axes, which can be represented as the color vector containing
all three minima cmin and the color vector containing all three maxima cmax.
For all of these displays, some form of “black,” k (very low, often nonzero,
roughly equal amounts of all components) and “white,” w (very high, roughly
equal amounts of all components) form the minimum and maximum points
in the RGB spaces of these devices. Generally, it is useful to have cmin = k
and cmax = w. While it might be possible to create extrema that are not pure
black and white, these are unlikely to be useful in a general display device. For
example, most applications would have no use for a cmax that was a bright,
saturated red.
Every display device is likely to have different exact values for k and w,
so it is convenient to use a standard color space for all devices as sort of
“normalized device color (or NDC)” coordinates. This color space is built
such that
(0, 0, 0) →k
(1, 1, 1) →w
The general mapping from these device-independent colors to the range of
the display is then
(r, g, b) →(kr + r(wr −kr), kg + g(wg −kg), kb + b(wb −kb))
This kind of device mapping is normally handled by the device driver or low-
level graphics API, and as a result the rest of this chapter and the following
chapter will work in these normalized color coordinates. This space deﬁnes
an RGB “color cube,” with black at the origin, white at (1, 1, 1), gray levels
down the main diagonal between them (a, a, a), and the other six corners
representing pure, maximal red (1, 0, 0), green (0, 1, 0), blue (0, 0, 1), cyan
(0, 1, 1), magenta (1, 0, 1), and yellow (1, 1, 0).
Although devices cannot generally display colors outside of the range
deﬁned by the (0, 0, 0) . . . (1, 1, 1) cube, colors outside of this cube are often
seen during intermediate color computations such as lighting. In fact, the
very nature of lighting can lead to ﬁnal colors with components outside of the
(1, 1, 1) limit. During lighting computations, these are generally allowed, but
prior to assigning ﬁnal colors to the screen, all colors must be within the nor-
malized cube. This requires either the hardware, the device driver software,
or the application to somehow limit the values of colors that do not fall within
the “safe” unit cube.

6.2 Color Representation 261
The simplest and easiest method is to clamp the color on a per-component
basis:
safe(c) = (clamp(cr), clamp(cg), clamp(cb))
where
clamp(x) = max(min(x, 1.0), 0.0)
However, it should be noted that such an operation can cause signiﬁcant
perceptual changes to the color. For example, the color (1.0, 1.0, 10.0) is
predominantly blue, but its clamped version is pure white (1.0, 1.0, 1.0). In
general, clamping a color can lead to the color becoming less saturated, or
less colorful. While this might seem unsatisfactory, it can actually be beneﬁ-
cial in lighting, as it tends to make overly bright objects appear to “wash out,”
an effect that can appear rather natural perceptually.
Another, more computationally expensive method is to rescale all three
color components of any color with a component greater than 1.0 such that
the maximal component is 1.0. This may be written as
safe(c) = (max(cr, 0), max(cg, 0), max(cb, 0))
max(cr, cg, cb, 1)
Note the appearance of 1 in the max function in the denominator to ensure
that colors already in the unit cube will not change — it will never increase
the color components. While this method does tend to avoid changing the
overall saturation of the color, it can produce some unexpected results. The
most common issue is that extremely bright colors that are scaled back into
range can actually end up appearing darker than colors that did not require
scaling. For example, comparing the two colors a = (1, 1, 0) and b = (10, 5, 0),
we ﬁnd that after scaling, b = (1, 0.5, 0), which is signiﬁcantly darker than a.
Scaling works best when it is applied equally to all colors in a scene, not
to each color individually. There are numerous methods for this, but one such
method involves ﬁnding the maximum color component of any object in the
scene, and scaling all colors equally such that this maximum maps to 1.0.
This is somewhat similar to a camera’s auto-exposure system. By scaling the
entire scene by a single scalar, color ratios between objects in the scene are
preserved.
6.2.5 Alpha Values
Frequently, RGB colors are augmented with a fourth component, called
Alpha. Such colors are often written as RGBA colors. Unlike the other three

262
Chapter 6 Geometry, Shading, and Texturing
components, the alpha component does not represent a speciﬁc color basis,
but rather deﬁnes how the combined color interacts with other colors. The
most frequent use of the alpha component is as an opacity value, which deﬁnes
how much of the surface’s color is controlled by the surface itself and how
much is controlled by the colors of objects that are behind the given surface.
When alpha is at its maximum (we will deﬁne this as 1.0), then the color of
the surface is independent of any objects behind it. The red, green, and blue
components of the surface color may be used directly; for example, in rep-
resenting a solid concrete wall. At its minimum (0.0), the RGB color of the
surface is ignored and the object is invisible, as with a pane of clear glass for
instance. At an intermediate alpha value such as 0.5, the colors of the two
objects are blended together; in the case of alpha equaling 0.5, the resulting
color will be the componentwise average of the colors of the surface and the
object behind the surface.
For the most part, alpha will be treated like any other color component
until rasterization. We will discuss the uses of the alpha value (known as
alpha blending) in Chapter 8 on rasterization. In a few cases, OpenGL handles
alpha a little differently from other color components (mention will be made
of these situations as needed).
6.2.6 Color Storage Formats
While we have discussed color values as real numbers, ﬂoating-point storage
of colors in a frame-buffer is not popular in graphics systems at this time. The
most popular format is to use unsigned 8-bit values per component, leading
to 3 bytes per RGB color, a system known as 24-bit color, or in some cases, by
the misnomer “true color.” With an alpha value, the format becomes 32 bits
per pixel, which aligns well on modern 32-bit CPU architectures. Another
common format is to use 5 bits each for red and blue and 6 bits for green,
a format that requires 16 bits per pixel. This system, which sometimes goes
by the name high color, is interesting in that it includes different amounts of
precision for green than for red or blue. As we’ve discussed, the human eye is
most sensitive to green, so the additional bit in the 16-bit format is assigned
to it. However, the number of pure gray values in this format is still 25 = 32.
Research has shown that the human visual system (depending on lighting
conditions, etc.) can perceive between 1 million and 7 million colors, which
leads to the (erroneous) theory that 24-bit color display systems, with their
224 ≈16.7 million colors are more than sufﬁcient. While it is true that the
number of different color “names” in a 24-bit system (where a color is “named”
by its 24-bit RGB triple) is a greater number than the human visual system can
discern, this does not take into account the fact that the colors being generated
on current display devices do not map directly to the 1–7 million colors that
can be discerned by the human visual system. Current display devices cannot

6.2 Color Representation 263
display the entire range of colors that the human eye can discern. In addition,
in some color ranges, different 24-bit color “names” appear the same to the
human visual system (the colors are closer to one another than the human
eye’s just noticable difference, or JND). In other words, 24-bit color wastes
precision in some ranges, while lacking sufﬁcient precision in others. Current
24-bit “true color” display systems are not sufﬁcient to cover the entire range
of human vision, either in range or in precision. Having said this, current
display devices are still quite convincing to the human eye and will continue
to improve.
The traditional reason for using these lower-precision formats is one of
storage requirements. Even 32 bits per pixel requires one-quarter the amount
of storage that is needed for ﬂoating-point RGBA values. Using full ﬂoating-
point numbers for output colors (the colors that are drawn to the output
LCD or CRT screen) is actually overkill, due to the limitations of current
display device color resolution. For example, current CRTs and LCD dis-
plays have dynamic ranges (the ratio of luminance between the brightest
and darkest levels that can be displayed by the devices) of between 200:1
and 500:1. These ratios mean that current display devices cannot deliver any-
where near the eye’s full range of perceived brightness or darkness. There are
display technologies on the horizon that will be able to represent more than
24-bit color. At that point, device-level color representations will require more
bits per component in order to avoid wasting the added precision available
from these new displays. Common “next-generation” device color formats
include 30-bit color (10 bits per component) and 48-bit color (16 bits per color
component).
Some 3D hardware devices do support higher-resolution colors inside
of the rendering pipeline, mainly due to the advent of complex pixel shading
hardware, which allow for advanced rendering techniques. The additional bits
of precision (or even a version of ﬂoating point) can be used to avoid losing pre-
cision during multi-operation color computations, but today even these hard-
ware devices generally output to an 8-to-10 bit per component display system.
6.2.7 Colors in OpenGL
OpenGL is very ﬂexible in terms of color representation. It can represent
colors as vectors of single- or double-precision ﬂoating-point values, as well
as bytes, shorts, and ints, all either signed or unsigned. When represented as
ﬂoating-point values, 0.0 and 1.0 represent the unit cube in normalized color
coordinates. However, the integral types are handled a little differently. With
both signed and unsigned bytes, shorts, and ints, a zero value (all zeros) maps
to our 0.0 normalized color value, and the maximum representable positive
value for the format (e.g., with signed bytes this would be 127) maps to the
1.0 normalized color value.

264
Chapter 6 Geometry, Shading, and Texturing
In the case of signed integral types, this leaves the negative half of the
range to map to the values −1.0 to 0.0 in normalized colors. The most negative
representable value in each number format maps to −1.0. While these negative
values have no use as ﬁnal device colors (they are clamped to 0.0), they can
be useful as source values in lighting computations.
Floating-point values are used directly in lighting computations, even if
they fall outside of the [−1.0, 1.0] range, but the ﬁnal resulting color will be
clamped to the [0.0, 1.0] range before using it to draw the geometry.
The most common color formats used by OpenGL applications are vec-
tors of single-precision ﬂoating point (for ease of application use and for the
ﬂexibility of range) and unsigned bytes (because they are compact and can
still represent the color precision of most display devices). Colors of these two
formats are set using:
glColor3f(GLfloat r, GLfloat g, GLfloat b);
glColor3ub(GLubyte r, GLubyte g, GLubyte b);
Or, for efﬁciency, the vector format allows a single argument of an in-memory
“vector” or array of the components:
GLfloat floatColor[3];
// ...
glColor3fv(floatColor);
GLubyte byteColor[3];
// ...
glColor3ubv(byteColor);
Almost all functions in OpenGL that require colors take this form. The exact
use of these functions in a larger context will be described in the next section
on vertices.
All colors in OpenGL have an alpha value, either implicit or explicit.
When a color is set using glColor3*, the alpha value is automatically set to
1.0. To set an explicit alpha value in a color, use glColor4* or glColor4*v,
where the fourth argument or array element (respectively) controls the alpha
component. The OpenGL Programming Guide [83] details all of the common
color representations.
6.3 Points and Vertices
So far, we have discussed points as our sole geometry representation. As
we begin to abstract to the higher level of a surface, points will become

6.3 Points and Vertices 265
insufﬁcient for representing the attributes of an object or for that matter the
object itself. The ﬁrst step in the move toward a way of deﬁning an object’s
surface is to associate additional data with each point. Combined together
(often into a single data structure), each point and its additional information
form what is often called a “vertex.” In a sense, a vertex is a “heavy point”: a
point with addition information that deﬁnes some properties of the surface
around it.
6.3.1 Per-Vertex Attributes
Within a vertex, the most basic value is the position of the vertex, generally
a 3D point that we will refer to as PV in later sections.
Other than vertex position, perhaps the most basic of vertex attributes are
colors. Common additions to a vertex data structure, vertex colors are used in
many different ways when drawing geometry. Much of the remainder of this
chapter will discuss the various ways that per-vertex colors can be assigned
to geometry, as well as the different ways that these vertex colors are used to
draw geometry to the screen. We will generally refer to the vertex color as CV
(and will sometimes speciﬁcally refer to the vertex alpha as AV , even though
it is technically a component of the overall color).
Another data element that can add useful information to a vertex is a
vertex normal. This is a unit-length 3-vector that deﬁnes the “orientation” of
the surface in an inﬁnitely small neighborhood of the vertex. If we assume
that the surface passing through the vertex is locally planar (at least in an
inﬁnitely small neighborhood of the vertex), the surface normal is the nor-
mal vector to this plane (recall the discussion of plane normal vectors from
Chapter 1). Normally, this vector is deﬁned in the same space as the vertices,
generally model (or object) space. As will be seen later, the normal vector is
a pivotal component in lighting computations. We will generally refer to the
normal as ˆnV.
Another vertex attribute that we will use frequently later in this chapter
is a texture coordinate. This will be discussed in detail in Sections 6.7–6.11
on texturing and in parts of the following two chapters; basically, they are
real-valued 2-vectors (most frequently, although they may also be scalars or
3-vectors) that deﬁne the position of the vertex within a smooth parameteri-
zation of the overall surface. These are used to map two-dimensional images
onto the surface in a shading process known as texturing.
Vertices in OpenGL
Demo
BasicSphere
OpenGL has the notion of a “current vertex,” at least when it is in the midst
of drawing. While we will describe how vertices are actually drawn, for the
moment we will simply introduce the concept of specifying a vertex. In order

266
Chapter 6 Geometry, Shading, and Texturing
to “push” a vertex down to OpenGL, an application uses one of the functions
in the set glVertex*. You can refer to OpenGL reference [83] for details, but
two commonly used versions are
glVertex3f(GLfloat x, GLfloat y, GLfloat z);
glVertex3fv(GLfloat* vert);
Both of these pass a 3D (ﬂoating point) vertex position down to OpenGL. The
second version assumes that X, Y, and Z are packed together in an array of
floats.
When specifying vertices, there is the notion of current values for all of
the possible vertex attributes (color, normal, etc.). These current values can
be set using the functions glColor*, glNormal*, and so forth (see OpenGL
reference text [83]). Note that these calls do not generate vertices; they only
set the current value of that attribute. When glVertex* is called, it generates
a vertex using the current values of color, normal, and the like. Multiple
calls to glColor*, glNormal*, and so on are ignored; only the last call to each
prior to a glVertex* call matters. Additional calls to these attribute functions
simply waste processor cycles. The following code generates three vertices at
different positions with different colors but with the same normal (pointing
along the Y axis);
glNormal3f(0.0, 1.0f, 0.0f);
glColor3f(1.0, 0.0f, 0.0f);
glVertex3f(1.0f, 0.0f, 1.0f);
glColor3f(0.0, 1.0f, 0.0f);
glVertex3f(1.0f, 0.0f, 0.0f);
glColor3f(0.0, 0.0f, 1.0f);
glVertex3f(0.0f, 1.0f, 0.0f);
6.4 Surface Representation
This section will discuss another important concept used to represent and
render objects in real-time 3D graphics: the concept of a surface and the most
common representation of surfaces in interactive 3D systems, sets of triangles.
These concepts will allow us to build realistic-looking objects from the sets of
vertices that we have discussed thus far.
Chapter 1 introduced the concept of a triangle, a subset of a plane deﬁned
by the convex combination of three noncollinear points. In this chapter we
will build upon this foundation and make frequent use of triangles, the normal

6.4 Surface Representation 267
vector to a triangle, and barycentric coordinates. A quick review of the sections
of Chapter 1 covering these topics is recommended.
While most of the remainder of this chapter will focus only on the assign-
ment of colors to objects for the purposes of rendering, the object and surface
representations we will discuss are useful for far more than just rendering.
Collision detection, picking, and even artiﬁcial intelligence all make use of
these representations.
6.4.1 Vertices and Surface Ambiguity
Unstructured collections of vertices (sometimes called point clouds) generally
cannot represent a surface unambiguously. For example, draw a set of 10 or
so dots representing points on a piece of paper. There are numerous ways
one could connect these two-dimensional points into a closed curve (a one-
dimensional “surface”) or even into several smaller curves. This is true even
if the vertices include normal vectors, as these normal vectors only deﬁne the
orientation of the surface in an inﬁnitely small neighborhood of the vertex.
We can see that without implicit or explicit additional structure, a ﬁnite set
of points rarely deﬁnes an unambiguous surface.
A cloud of points that is inﬁnitely dense on the desired surface can repre-
sent that surface. Obviously, such a directly stored collection of unstructured
points would be far too large to render in real time (or even store) on
a computer. We need a method of representing an inﬁnitely dense surface
of points that requires only a ﬁnite amount of representational data.
There are numerous methods of representing surfaces, including
■Parametric surfaces (see the chapters on curves and surfaces in [36]).
A parametric surface is deﬁned as a 2-dimensional subset of R3 such
that all points on the surface are generated by v = f(s, t), where s, t ∈R
are the parameters. Examples of parametric surfaces include bicubic
“patches” of all sorts, as well as surfaces of revolution.
■Implicit surfaces (see Blinn’s [10]). An implicit surface is deﬁned as
the set of all points v ∈R3 such that a given scalar-valued function
f(v) = c for a ﬁxed constant c. Examples of these include so-called
blobby objects, or “metaballs.”
While each of the methods listed above can represent some subset of all
possible surfaces perfectly, both methods can be complicated and/or expen-
sive and are not suited for all surfaces. These methods can also be difﬁcult
for artists to control at the ﬁne scale they desire — changes to one part of
such a surface can have unintended effects on other parts of the surface.
Also, such methods do not always lend themselves to an obvious or direct

268
Chapter 6 Geometry, Shading, and Texturing
method of rendering. Finally, they cannot necessarily make direct use of the
conveniently deﬁned vertices that our geometry pipeline can generate.
6.4.2 Triangles
The most common method used to represent 3D surfaces in real-time graph-
ics systems is simple, scalable, requires little additional information beyond
the existing vertices, and allows for direct rendering algorithms; it is called
approximation of surfaces with triangles, or tessellation. Tessellation refers
not only to the process that generates a set of triangles from a surface but also
to the triangles and vertices that result.
Triangles, each represented and deﬁned by only three points on the
surface, are connected point to point and edge to edge to create a locally
ﬂat (“faceted”) approximation of the surface. By varying the number and
density of triangles used to represent a surface, an application may make
any desired trade-off between compactness/rendering speed and accuracy of
representation.
One concept that we will use frequently with triangles is that of barycentric
coordinates. From the discussion in Chapter 1, we know that any point in a tri-
angle may be represented by an element of R2 (s, t) such that 0.0 ≤s, t ≤1.0.
These coordinates uniquely deﬁne each point on a nondegenerate triangle
(i.e., a triangle with nonzero area). We will often use barycentric coordi-
nates as the domain when mapping functions deﬁned across triangles, such as
color.
Triangles in OpenGL
Demo
BasicSphere
OpenGL has numerous methods for rendering triangles. The simplest (but
not the most efﬁcient computationally) is via vertex by vertex speciﬁcation.
Using this method, a primitive is opened with a function call, vertices are
passed to OpenGL one at a time (with sets of vertices deﬁning triangles),
and then the primitive is closed. As an example, the following code draws
a tetrahedron:
glBegin(GL_TRIANGLES);
glVertex3f(0.0f, 0.0f, 0.0f);
glVertex3f(1.0f, 0.0f, 0.0f);
glVertex3f(0.0f, 0.0f, 1.0f);
glVertex3f(0.0f, 0.0f, 0.0f);
glVertex3f(0.0f, 0.0f, 1.0f);
glVertex3f(0.0f, 1.0f, 0.0f);

6.4 Surface Representation 269
glVertex3f(0.0f, 0.0f, 0.0f);
glVertex3f(0.0f, 1.0f, 0.0f);
glVertex3f(1.0f, 0.0f, 0.0f);
glVertex3f(1.0f, 0.0f, 0.0f);
glVertex3f(0.0f, 1.0f, 0.0f);
glVertex3f(0.0f, 0.0f, 1.0f);
glEnd();
The function call glBegin starts the primitive (in this case, GL_TRIANGLES, which
groups each set of three vertices into a triangle), the glVertex calls pass down
the vertices, and the glEnd call closes the primitive. Because this method
requires three OpenGL function calls per triangle, it is quite expensive. A
more efﬁcient method, indexed geometry, is detailed in Section 6.4.4.
6.4.3 Triangle Attributes
In some graphics systems, triangles can have their own attributes beyond
those of the vertices that comprise the triangle. These attributes can either
override or supplement the per-vertex attributes. We will describe several
common per-triangle attributes. As triangles are often referred to by the term
faces (which is a more general term that refers to a general n-sided polygon),
these attributes are often called face attributes.
Colors are a very common per-triangle attribute. They are used in ways
analogous to vertex colors, but describe a color that is applied to the entire
triangle, rather than describing the color at or near a given vertex. These will
be used frequently during lighting computations, especially with so-called
ﬂat, or per-triangle, shading. We will generally refer to the triangle color as
CF (for “face color”).
A per-triangle normal (we will call this vector ˆnT ) is typically generated
directly from the plane of the triangle itself, using the method described in
Chapter 1:
ˆnT = (PV 2 −PV 1) × (PV 3 −PV 2)
|(PV 2 −PV 1) × (PV 3 −PV 2)|
Since this normal is a purely geometric quantity that (along with any of the
three vertices) represents the plane of the triangle, it is used in many different
algorithms, including lighting, collision detection, picking, and culling (as a
way of quickly determining which triangles are visible to the camera).
As an example of these applications, let us examine triangle culling. As
previously discussed, triangles that fall outside of the view, either to the side or
behind the camera, are generally culled out of the system (called view frustum

270
Chapter 6 Geometry, Shading, and Texturing
culling) and are not considered during the latter stages of the rendering
pipeline. In a similar way, triangles are often considered to be “sided”; that
is, a triangle is drawn differently (or not at all), depending on whether the
triangle’s front or back face is currently facing the camera. Culling based on
this is generally known as backface culling, as it culls out the triangles that
are “back-facing” with respect to the camera. This can result in the culling of
a large number of the triangles not already culled by the view frustum.
Backface culling is very inexpensive to compute; much less expensive than
rendering the triangle. The plane deﬁned by any of the triangle vertices (PV )
and the per-triangle normal ˆnT deﬁne the plane of the triangle. The plane
equation for the triangle is thus all points X such that
X : (ˆnT · X) −c = 0
X : (ˆnT · X) −(ˆnT · PV ) = 0
X : ˆnT · (X −PV ) = 0
If we consider the camera’s center of projection to be located at the point Q,
then backface culling is simply computed by evaluating the dot product of the
two vectors in the ﬁnal equation and testing the sign of the result. The two
vectors to be tested are the triangle normal ˆnT and the vector from any of the
triangle’s vertices to the camera location (Q −PV ). If
(Q −PV ) · ˆnT > 0
then the camera location Q is on the front side of the triangle, and the triangle
is front-facing. For all points Q such that
(Q −PV ) · ˆnT ≤0
then the camera location Q is on the back side of the triangle, and the triangle
is back-facing (Figure 6.1a).
Backface culling can also be computed in 2D screen space (as will be
discussed subsequently). While this is even less expensive to compute than
3D backface culling, the triangle must continue farther down the graphics
pipeline (into screen space) before this 2D backface culling can be com-
puted and can require more computation per triangle. In either case, the
most expensive stage in the pipeline, rasterization, is skipped for back-facing
triangles — generally, a signiﬁcant optimization.
OpenGL and Triangle Attributes
Demo
BasicSphere
OpenGL does not include the concept of specifying per-triangle attributes
explicitly. Each vertex has the option of specifying attributes such as the
color and normal. However, if the application sets an attribute once and

6.4 Surface Representation 271
0
2
1
3D culling
with face normals
(side view)
2D culling using 
vertex order
(view from camera)
Clockwise
(back-facing)
Counterclockwise
(front-facing)
Q
Clockwise
(back-facing)
Counterclockwise
(front-facing)
0
1
2
a
b
Figure 6.1 Culling triangles.
generates three vertices without changing that attribute, then the attribute
will be constant across that triangle. In practice, the OpenGL implementation
could detect this and treat the attribute as a per-triangle value internally. For
example, the following triangle would have three equal normals, which in this
case happens to be (and could be treated as) the per-triangle face normal.
glBegin(GL_TRIANGLES);
glNormal3f(0.0f, 1.0f, 0.0f);
glVertex3f(0.0f, 0.0f, 0.0f);
glVertex3f(1.0f, 0.0f, 0.0f);
glVertex3f(0.0f, 0.0f, 1.0f);
glEnd();

272
Chapter 6 Geometry, Shading, and Texturing
OpenGL and Triangle Culling
OpenGL allows the concept of “clockwise vs. counterclockwise” triangle
vertices to be mapped as desired onto the concept of “front vs. back facing” tri-
angles on a per-primitive basis. The linking of these concepts is handled via the
function glFrontFace. Calling this function with an argument of GL _CCW will
cause OpenGL to consider triangles whose vertices are ordered counterclock-
wise from the camera location to be front-facing. GL _CW sets the reverse: clock-
wise triangles are front-facing. The default mode is glFrontFace(GL _CCW).
Note that OpenGL does not require objects to have normals of any kind
for culling to occur; culling in OpenGL is done just prior to rasterization and
is a 2D process that requires only the screen-space vertex positions. Culling
in OpenGL is accomplished by determining whether the 2D, screen-space
triangle vertices are in clockwise or counterclockwise order. This clockwise
or counterclockwise ordering is combined with the glFrontFace setting just
described to determine whether the given triangle is front- or back-facing.
This is shown from the camera’s point of view in Figure 6.1b.
6.4.4 Vertex Indices
Most real-world surfaces are, to some degree, closed and smooth. In rep-
resenting these surfaces, we do not want to have empty space between
neighboring triangles. The best way of ensuring this is to use vertices that
have equal positions in neighboring triangles. Figure 6.2a depicts an exam-
ple of a fan of six triangles (deﬁning a hexagon) that meet in a single point.
0
1
2
3
6
4
5
Configuration
18 individual vertices
(exploded view)
7 shared vertices
Index list for shared vertices
(0,1,2),(0,2,3),(0,3,4),(0,4,5),(0,5,6),(0,6,1)
a
b
c
d
Figure 6.2 A hexagonal conﬁguration of triangles.

6.4 Surface Representation 273
With six triangles, we require 18 vertices — three for each of the six triangles,
as shown in Figure 6.2b. However, only seven of these vertices have unique
positions. In fact, on a closed surface, most triangles will share the positions
of several (or all) of their vertices with multiple other triangles in that object.
Rather than blindly generating 3T vertices for any set of T triangles, many
graphics systems (including OpenGL) allow the idea of external triangle index
information, also known as indexed geometry.
Indexed geometry deﬁnes an object with two arrays, one for the ver-
tices and one for the triangle indices. The array containing the vertices
contains only the N unique vertices. In our hexagon example, this would
be an array of seven vertices, six around the edge and one in the center.
Figure 6.2c shows these seven vertices, numbered with their indices in the
vertex array. This array does not deﬁne any information about the triangles in
the object.
The second array is an array of 3T indices. Each set of three indices rep-
resents a triangle. The indices are used to look up vertices in the vertex array;
the three vertices are joined into a triangle. Figure 6.2d shows the index list
for the hexagon example.
Note that index arrays are arrays of unsigned integers (either 16 or 32 bits)
and thus generally require far less memory than an array of vertices with the
same number of elements (since a vertex generally consists of at least three
ﬂoating-point values). There is overhead for the vertex array, but for most
surfaces (where the average vertex appears in several triangles), the memory
savings (and in some 3D hardware systems, the overall performance gains)
can be very signiﬁcant.
For example, we can compute the memory savings of indexed geometry
for our hexagon, assuming that vertices are as lightweight as possible (this will
actually skew the results in favor of nonindexed geometry). In the nonindexed
case, there are six triangles, giving a memory usage of
Nonindexed = triangles × vertices
triangle × f loats
vertex × bytes
f loat
= 6 × 3 × 3 × 4 = 216 bytes
Assuming 16-bit unsigned short indices for the index list, the indexed case
has the following combined memory usage for its two arrays:
Indexed = triangles × indices
triangle × bytes
index + vertices × f loats
vertex × bytes
f loat
= 6 × 3 × 2 + 7 × 3 × 4 = 36 + 84 = 120 bytes

274
Chapter 6 Geometry, Shading, and Texturing
A signiﬁcant savings, even in this simple case. If the vertices had included
normals, the difference between the two memory requirements would have
been even larger.
6.4.5 OpenGL Vertex Indices
Demo
IndexedGeom
In OpenGL indexed geometry can be implemented in one of several ways,
the most widely available being vertex arrays, which became a part of the
standard in OpenGL 1.1. To enable vertex arrays, an application must enable
the handling of arrays for each vertex component it would like to specify via
an array. For example, to enable vertex arrays for positions, the function is
glEnableClientState(GL_VERTEX_ARRAY)
To turn off the handling of array-based vertices (and switch back to non-
indexed mode), the call is
glDisableClientState(GL_VERTEX_ARRAY)
Having enabled the handling of vertex arrays, the entire array of vertices
can be passed to OpenGL in a single call. For example, imagine creating
the vertices for a tetrahedron:
glEnableClientState(GL_VERTEX_ARRAY)
static GLfloat verts[4 * 3] = { 0.0f, 0.0f, 0.0f,
1.0f, 0.0f, 0.0f,
0.0f, 1.0f, 0.0f,
0.0f, 0.0f, 1.0f };
glVertexPointer(3, GL_FLOAT, 0, verts);
The function glVertexPointer speciﬁes the array of vertices:
1. The ﬁrst argument speciﬁes the number of components per vertex
position (in this case, an array of 3D vertex positions).
2. The second argument speciﬁes the format of each vertex component.
3. The third argument speciﬁes any additional spacing, or “padding” (in
bytes) between each vertex.
4. The ﬁnal argument is the pointer to the vertices themselves.

6.4 Surface Representation 275
This function causes OpenGL to store the pointer and does not copy the data
(in fact, this would not even be possible, since the function does not specify
the number of vertices in the array!). As such, the storage for the array that is
passed in by the application must be valid for the entire time that it is to be
used to render.
The code for our tetrahedron contains no information about indices.
Without any triangle indices, nothing will be drawn. So, we must create the
index array. The array that follows deﬁnes 12 indices, three for each of the
four triangles in the tetrahedron. Then, it calls glDrawElements, which can
draw an entire array of primitives in a single call.
1. The ﬁrst argument deﬁnes the type of primitive (GL_TRIANGLES causes
each subsequent triple of indices to form a triangle).
2. The second argument supplies the number of indices (in the case
of GL_TRIANGLES, this number should be three times the number of
triangles).
3. The third argument deﬁnes the type of these elements in the index
array.
4. The ﬁnal parameter is the address of the base of the array:
GLushort indices[12] = { 0, 1, 3, // Y=0 plane triangle
0, 3, 2, // X=0 plane triangle
0, 2, 1, // Z=0 plane triangle
1, 2, 3
// Diagonal plane triangle
};
glDrawElements(GL_TRIANGLES, 3*4, GL_USHORT, indices);
While the preceding code does not appear to be much shorter than the origi-
nal vertex-at-a-time version, the vertex array version requires fewer OpenGL
function calls and can often be rendered at much higher speed than the indi-
vidual vertex method (as the vertices and indices for all triangles are speciﬁed
at once). Furthermore, it is possible that the indexed version will have to trans-
form only four vertices, while the individual method will have to transform
all 12 individually.
OpenGL (as well as most other rendering APIs) supports a wide range of
indexed geometry. Indexed triangle lists, such as the ones we’ve introduced,
are simple to understand but are not as optimal as other representations.
The most popular of these more optimal representations are triangle strips, or
tristrips. In a triangle strip, the ﬁrst three vertex indices represent a triangle,
just as they do in a triangle list. However, in a triangle strip, each additional

276
Chapter 6 Geometry, Shading, and Texturing
vertex (the fourth, ﬁfth, etc.) generates another triangle — each index gener-
ates a triangle out of itself and the two indices that preceded it (e.g., 0-1-2,
1-2-3, 2-3-4 . . .). This forms a ladderlike strip of triangles (note that each
triangle is assumed to have the reverse orientation of the previous triangle;
counterclockwise, then clockwise, then counterclockwise again, etc.). Then,
too, whereas triangle lists require 3T indices to generate T triangles, triangle
strips require only T + 2 indices to generate T triangles. Much research has
gone into generating optimal strips by maximizing the number of triangles
while minimizing the number of strips, since there is a two vertex “overhead”
to generate the ﬁrst triangle in a strip. The longer the strip, the lower the aver-
age number of indices required per strip. Most consumer 3D hardware that is
available today renders triangle strips at peak performance. OpenGL renders
triangle strips using an argument of GL_TRIANGLE_STRIP as the primitive type
(replacing GL_TRIANGLES).
6.5 Coloring a Surface
The following sections describe a wide range of methods to assign colors
to surface geometry. From the simplest methods (such as assigning a single,
ﬁxed color per object) all the way to the most expensive (such as effects requir-
ing the application of multiple image-based “textures”), each has its beneﬁts,
limitations, costs, and mathematical issues.
The basic goal of each method is the same; given an object O, a triangle
T ∈O, made up of vertices V 1, V 2, and V 3, along with barycentric coordinates
(s, t) deﬁning a unique point in T , return a color that is associated with
this point on the geometric object. Many of the methods we will describe
will require additional data both within the triangle and its vertices and
within the scene as a whole. However, in each case, the coloring function
Color(O, T, (s, t)) takes the information that describes the point (or “sample”)
and returns an RGB color.
The ﬁrst sections will deal with constant colors assigned prior to render-
ing, which are generally the simplest methods. Later sections will progress to
the more dynamic, per sample, per frame methods such as dynamic lighting.
6.6 Using Constant Colors
The method of coloring geometry that produces the highest runtime perfor-
mance is to assign colors to geometry prior to rendering, either by having an
artist assign colors to every surface during content creation time, or else to

6.6 Using Constant Colors 277
use an off-line process to generate static colors for all geometry. With these
static colors assigned, there is relatively little that must be done to select the
correct color for a given sample.
Put simply,
constant colors mean that given O,
T ,
and (s, t),
Color(O, T, (s, t)) will never change. No environmental information like
dynamic lighting will be factored into the ﬁnal color. The function Color is
the shading function for the geometry. In 3D rendering, a shading function
(or shading method, or shader) is simply a method that assigns colors to every
point on the geometry. It should not be confused with lighting (to be described
in great detail later), which is one way of generating source colors used in the
shading process.
6.6.1 Per-Object Colors
The simplest form of useful coloring is to assign a single color per object. The
coloring function is thus
Color(O, T, (s, t)) = CO
The color value CO is simply added to the data structure describing O. Note
that this function does not depend on T or (s, t). If we are taking multiple
samples using this function, we only need to look up CO again only if we
sample a different object. Constant coloring of an entire object is of very lim-
ited use, since the entire object will appear to be ﬂat, with no color variation.
The viewer will be able to determine only where the object “is” and “is not.”
At best, only the outline of the object will be visible against the backdrop.
As a result, except in some special cases, per-object color is rarely used as the
ﬁnal shading function for an object.
6.6.2 Per-Triangle Colors
A similar, but ﬁner-grained and more powerful method for assigning colors
to geometry is to assign a color to each triangle. This is known as faceted, or
ﬂat shading, because the resulting geometry appears planar on a per-triangle
basis. The function used to assign colors is very similar to the per-object
function:
Color(O, T, (s, t)) = CF
Normally, this requires adding a color ﬁeld (CF ) to each triangle. However,
Demo
Shading
OpenGL does not speciﬁcally support a separate per-triangle color value. In
explicit vertex-by-vertex mode, a single-color triangle may be speciﬁed by

278
Chapter 6 Geometry, Shading, and Texturing
setting the current color once and then “pushing” three vertex positions to
render a triangle with no intervening colors. All three vertices will be assigned
the same color:
// Flat-shaded blue triangle
glColor3f(0.0, 0.0f, 1.0f);
glVertex3f(1.0f, 0.0f, 1.0f);
glVertex3f(1.0f, 0.0f, 0.0f);
glVertex3f(0.0f, 1.0f, 0.0f);
However, ﬂat shading can also be enabled globally at the OpenGL level, in
which case the color of one triangle vertex (the ﬁnal vertex) will be used for
the entire triangle, even if the three vertex colors differ. Flat shading is enabled
in OpenGL with the function call
glShadeModel(GL_FLAT);
and disabled (switching to smooth shading) via the function call
glShadeModel(GL_SMOOTH);
With OpenGL vertex arrays, per-triangle colors are speciﬁed indirectly —
the color of one of the triangle’s vertices is used as the color of the entire
triangle. The OpenGL speciﬁcation details which vertex is used in each mode,
but for GL_TRIANGLES the vertex used is the last (third) vertex in the triangle.
Since OpenGL does not have a notion of a polygon color (only vertex colors),
the face color must be associated with the ﬁnal vertex that is used to generate
the triangle. This can be problematic in the case of indexed geometry, where
some vertices may have to be used as the third vertex for more than one
triangle (it is common and very easy to generate indexed geometry that has
more triangles than vertices). In such cases, it may be necessary to duplicate
vertices in order to be able to specify triangle-speciﬁc colors.
6.6.3 Per-Vertex Colors
Many of the surfaces approximated by tessellated objects are smooth, mean-
ing that the goal of coloring these surfaces is to emphasize the smoothness
of the original surface, not the artifacts of its approximation with ﬂat tri-
angles. This fact makes ﬂat shading a very poor choice for many tessellated
objects. A shading method that can generate the appearance of a smooth

6.6 Using Constant Colors 279
surface is needed. Per-vertex coloring, along with a method called Gouraud
shading (after its inventor, Henri Gouraud) does this. Gouraud shading is
based on the existence of some form of per-vertex colors, assigning a color to
any point on a triangle by linearly interpolating the three vertex colors over the
surface of the triangle. As with the other shading methods we have discussed,
Gouraud shading is independent of the source of these per-vertex colors; the
vertex colors may be assigned explicitly by the application, or generated on
the ﬂy via per-vertex lighting and so on. This linear interpolation is both sim-
ple and smooth and can be expressed as a mapping of barycentric coordinates
(s, t) as follows:
Color(O, T, (s, t)) = sCV 1 + tCV 2 + (1 −s −t)CV 3
Examining the terms of the equation, it can be seen that Gouraud shading
is simply an afﬁne transformation from barycentric coordinates (as homoge-
neous points) in the triangle to RGB color space. The mapping may be written
as the 3 × 3 matrix transform
Color(O, T, (s, t)) =


(CV 1 −CV 3)R
(CV 2 −CV 3)R
(CV 3)R
(CV 1 −CV 3)G
(CV 2 −CV 3)G
(CV 3)G
(CV 1 −CV 3)B
(CV 2 −CV 3)B
(CV 3)B




s
t
1


or simply
Color(O, T, (s, t)) =
 (CV 1 −CV 3)
(CV 2 −CV 3)
CV 3



s
t
1


An important feature of per-vertex smooth colors is that color discontinuities
can be avoided at triangle edges. This was a major drawback of per-triangle
colors, as any triangles that shared an edge would either have to be the same
color or else have a sharp color discontinuity at the shared edge. This can be
avoided with per-vertex colors.
Internal to each triangle, the colors are interpolated smoothly, as can be
seen from the fact that Gouraud shading interpolation is an afﬁne mapping
from barycentric coordinates to RGB color space. At triangle edges, color
discontinuities can be avoided by ensuring that the two vertices deﬁning a
shared edge in one triangle have the same color as the matching pair of vertices
in the other triangle. At a shared edge between two triangles, the color of
the third vertex in each triangle (the vertices that are not an endpoint of the
shared edge) does not factor into the color along that shared edge. This is
an added degree of freedom over per-triangle colors. This can be shown as
follows. Assume we have a triangle with vertex colors CV 1, CV 2, and CV 3.

280
Chapter 6 Geometry, Shading, and Texturing
By our deﬁnition of barycentric coordinates, the barycentric coordinate of
V 1 is (1,0), and the barycentric coordinate of V 3 is (0,0). Thus, in barycentric
coordinates, the edge between V 1 and V 3 is deﬁned by
(s, t) = (1 −r, 0)
where 0 ≤r ≤1. Thus, the colors across the edge are
Color = sCV 1 + tCV 2 + (1 −s −t)CV 3
= (1 −r)CV 1 + (0)CV 2 + (1 −(1 −r) −0)CV 3
= (1 −r)CV 1 + (r)CV 3
which does not involve CV 2. Similar derivations show that analogous cases
are true for any triangle edge. As a result, there will be no color discontinuities
across triangle boundaries, as long as the shared vertices between any pair
of triangles are the same in both triangles. In fact, with fully shared, indexed
geometry, this happens automatically (since co-located vertices are shared via
indexing). Figure 6.3 allows a comparison of geometry drawn with per-face
colors and with per-vertex colors.
The linear interpolation used for Gouraud shading is completely deﬁned
by the three vertices of a triangle. Gouraud shading across a general quadri-
lateral is dependent on how that quadrilateral is decomposed into triangles.
In Figure 6.4, we see a quadrilateral with its assigned vertex colors. The ﬁgure
shows that simply by changing the way the quadrilateral is broken into trian-
gles, the Gouraud shading can change signiﬁcantly. Note that the two cases
Sphere with flat shading
Sphere with Gouraud shading
Figure6.3 Flat (per-face) and Gouraud (per-vertex) shading.

6.6 Using Constant Colors 281
Triangulation
Gouraud shading
Black
Black
White
White
Figure 6.4 Gouraud shading in a quadrilateral.
use the same vertex colors and vertex positions but are simply triangulated
differently.
While the colors need not have any triangle boundary discontinuities,
there are often discontinuities in the derivative of the color at an edge. In
more visual terms, the slope of the color (how rapidly it is changing across the
face of a triangle) is deﬁned by all three triangle vertices. As a result, even if
the colors match on a triangle edge, there is often a sharp change in the way
colors are interpolated across that edge. Even though the shared vertices have
the same color, the fact that the derivative of color changes sign across the
boundary (the direction of color change reverses) makes the edge visible. If
measured as a change in derivative, this appears subtle, but the human visual
system actually enhances the discontinuity in an effect called mach banding.
Mach banding is a physiological trait of the human visual system that causes
these color gradients to appear even sharper than they are, meaning that even
Gouraud shading cannot completely hide artifacts of tessellation. For a far
more detailed discussion of the physiological perception of color, see [20].
As mentioned in passing earlier, Gouraud shading is enabled in OpenGL
via the function call
glShadeModel(GL_SMOOTH);

282
Chapter 6 Geometry, Shading, and Texturing
Demo
BasicShading
For far more details on the rendering of ﬂat versus smooth (or Gouraud)
shaded triangles, see Chapter 8. Both ﬂat and Gouraud shading are used to
interpolate colors generated by dynamic lighting. For a detailed discussion of
dynamic lighting, see Chapter 7.
Sharp Edges
Not all tessellations represent completely smooth objects. In some cases,
sharp geometric edges in the tessellation really do represent the original
surface accurately. In addition, the edge between two triangles may mark
the boundary between two different colors on the surface of the object. In
these situations, interpolating smoothly across triangle boundaries is not the
desired behavior. The vertices along an edge need to have different colors
in the two triangles. In general, when Gouraud shading is used, these situa-
tions require coincident vertices to be duplicated, so that the two coincident
copies of the vertex can have different colors. Figure 6.5 provides an example
of a cube drawn with entirely shared vertices and with duplicated vertices
to allow per-vertex, per-face colors. Note that the cube is not ﬂat-shaded in
either case — there are still color gradients across each face. The example with
duplicated vertices and sharp shading edges looks more like a cube.
In this context, a “sharp” edge is not necessarily a geometric property. It
is nothing more than an edge that is shared by two adjacent triangles where
the triangle colors on either side of the edge are different. This produces a
visible, sharp line between the two triangles where the color changes.
Sharp edges in OpenGL are a nonissue if you are using vertex-at-a-time
triangle speciﬁcation. In this case, each vertex of each triangle is already being
speciﬁed independently, making it easy to specify different vertex colors for
Shared vertices lead to
smooth-shaded edges
Duplicated vertices
allow the creation of
sharp-shaded edges
Figure 6.5 Sharp vertex discontinuities.

6.6 Using Constant Colors 283
4
5
Index list = (0,1,3),(1,2,3)
Triangles share adjacent vertices
Index list = (0,1,2),(3,4,5)
Triangles do not share adjacent vertices
0
1
2
3
0
1
2 3
Figure6.6 Duplicating indexed vertices for sharp color edges.
multiple co-located vertices. In the case of vertex arrays, however, duplicating
vertices may be required, so that co-located vertices in different triangles can
have different colors. The issue arises because the function glDrawElements
uses the same index to look up a vertex’s color as it does the vertex’s position.
As a result, the color of a vertex and its position are directly linked. When
using vertex arrays, the vertices deﬁning any sharp, shared edge must be
duplicated. The more sharp edges there are in a vertex array primitive, the
less vertex sharing is possible (i.e., more duplicated vertices), decreasing the
efﬁciency of the method. Figure 6.6 provides a visual representation of a pair
of triangles with and without a sharp color edge.
6.6.4 Limitations of Basic Shading Methods
Real-world surfaces often have detail at many scales. The shading/coloring
methods described so far require that colors be assigned only at tessellation-
level features, either per-triangle or per-vertex. While this works well for
surfaces whose colors change at geometric boundaries, many surfaces do
not ﬁt this restriction very well, making ﬂat shading and Gouraud shading
inefﬁcient at best.
For example, imagine a ﬂat sheet of paper with text written upon it. The
ﬂat, rectangular sheet of paper itself can be represented by as few as two
triangles. However, in order to use Gouraud shading to represent the text, the
piece of paper would have to be subdivided into triangles at the edges of every
character written upon it. None of these boundaries represents geometric
features, but rather are needed only to allow the color to change from white

284
Chapter 6 Geometry, Shading, and Texturing
(the paper’s color) to black (the color of the ink). Each character could easily
require hundreds of vertices to represent the ﬁne stroke details. This could
lead to a simple, ﬂat piece of paper requiring tens of thousands of vertices.
Clearly, we require a shading method that is capable of representing detail at
a ﬁner scale than the level of tessellation.
6.7 Texture Mapping
6.7.1 Introduction
One method of adding detail to a rendered image without increasing geo-
metric complexity is called texture mapping, or more speciﬁcally image-based
texture mapping. The physical analogy for texture mapping is to imagine wrap-
ping a ﬂat, paper photograph onto the surface of a geometric object. While
the overall shape of the object remains unchanged, the overall surface detail is
increased greatly by the image that has been wrapped around it. From some
distance away, it can be difﬁcult to even distinguish what pieces of visual
detail are the shape of the object and which are simply features of the image
applied to the surface.
A real-world physical analogy to this is theatrical set construction. Often,
details in the set will be painted on planar pieces of canvas, stretched over a
wooden frame (i.e.,“ﬂats”), rather than built out of actual, three-dimensional
wood, brick, or the like. With the right lighting and positioning, these
quickly painted ﬂats can appear as very convincing replicas of their real, 3D
counterparts. This is the exact idea behind texturing — using a 2D, detailed
image placed upon a simple 3D geometry to create the illusion of a complex,
detailed, fully 3D object.
An example of a good use of texturing is a rendering of a stucco wall; such
a wall appears ﬂat from any signiﬁcant distance, but a closer look shows that
it consists of many small bumps and sharp cracks. While each of these bumps
could be modeled with geometry, this is likely to be expensive and unlikely
to be necessary when the object is viewed from a distance. In a 3D computer
graphics scene, such a stucco wall will most frequently be represented by a ﬂat
plane of triangles, covered with a detailed image of the bumpy features of lit
stucco.
The fact that texture mapping can reduce the problem of generating and
rendering complex 3D objects into the problem of generating and rendering
simpler 3D objects covered with 2D paintings or photographs has made tex-
ture mapping very popular in real-time 3D. This, in turn, has led to the method
being implemented in display hardware, making the method even less expen-
sive computationally. The following sections will introduce and detail some of

6.7 Texture Mapping 285
the concepts behind texture mapping, some mathematical bases underlying
them, and basics of how texture mapping can be used in OpenGL applications.
6.7.2 Shading via Image Lookup
The real power of texturing lies in the fact that it uses a set of samples (an
image) as its means of generating color. In a sense, texturing is simply a
system of indirect coloring. Rather than directly interpolating colors that
are stored in the vertices, the vertex values serve only to describe how an
image is mapped to the triangle. By adding a level of indirection between the
per-vertex values and the ﬁnal colors, texturing can create the appearance of
a very complex shading function that is actually no more than a lookup into
a table of samples.
The process of texturing involves deﬁning three basic mappings:
1. To map all points on a surface (smoothly in most neighborhoods) into
a 2-dimensional (or in some cases, 1D or 3D) domain
2. To map points in this (possibly unbounded) domain into a unit square
(or unit interval, cube, etc.)
3. To map points in this unit square to color values
The ﬁrst stage will be done using a modiﬁcation of the method we used
for colors with Gouraud shading, an afﬁne mapping. The second stage will
involve methods such as min, max, and modulus. The ﬁnal stage is the most
unique to texturing and involves mapping points in the unit square into an
image. We will begin our discussion with a deﬁnition of texture images.
6.7.3 Texture Images
The most common form of texture images (or textures, as they are generally
known) are 2-dimensional, rectangular arrays of color values. Every texture
has a width (the number of color samples in the horizontal direction) and
a height (the number of samples in the vertical direction). Textures are
similar to almost any other digital image, including the screen, which is also
a 2D array of colors. Just as the screen has pixels (for picture elements),
textures have texels (texture elements). While some graphics systems allow
1-dimensional textures (linear arrays of texels) and even 3-dimensional tex-
tures (cubes or rectangular parallelopipeds of texels), by far the most common
and most useful are 2-dimensional, image-based textures. Our discussion of
texturing will focus entirely on 2-dimensional textures.

286
Chapter 6 Geometry, Shading, and Texturing
x=26, y=11
x=Width–1, y=0
x=0, y=0
x=Width–1, y=Height–1
x=0, y=Height–1
y=11
x=26
Figure6.7 Texel-space coordinates in an image.
We can refer to the position of a given texel via a 2D value (x, y), in
texel units — note that these coordinates are (column, row), the reverse of
how we generally refer to matrix elements. Figure 6.7 shows an example
of a common mapping of texel coordinates into a texture. Note that while
the left to right increasing mapping of x is universal in graphics systems,
the bottom to top increasing mapping of y is not (bottom to top is used in
OpenGL).
Demo
BasicTexturing
Two-dimensional texturing is enabled in OpenGL at the highest level with
the call
glEnable(GL_TEXTURE_2D);

6.7 Texture Mapping 287
and disabled with the function call
glDisable(GL_TEXTURE_2D);
A 2-dimensional texture image is speciﬁed in OpenGL via the function
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height,
0, GL_RGBA, GL_UNSIGNED_BYTE, texels);
We will avoid explaining all of the possible values for the arguments (see the
OpenGL Programming Guide [83] for details) because most are not relevant
to our discussion and can be left as is in other cases. We will conﬁne our
discussion to the following:
■The ﬁrst parameter, GL_TEXTURE_2D, speciﬁes that the 2-dimensional tex-
turing settings (the only form we will discuss in detail) are to be changed
by this call.
■Parameter three, GL_RGBA, deﬁnes the requested “internal” format. In
this case we are requesting only that the system store the given texture
as full-color image with alpha per texel. This parameter does not deﬁne
anything about the data we are passing in, only how we would like it to
be stored in the system.
■The next two parameters, width and height (integers), specify the width
and height of the texture in texels. OpenGL requires that textures have
power-of-two dimensions (i.e., width = 2m and height = 2n, where m
and n are integers).
■The seventh parameter, GL_RGBA, speciﬁes that the texel data we are
sending deﬁnes each texel as a red, green, blue, and alpha value in
sequence.
■The eighth parameter, GL_UNSIGNED_BYTE, deﬁnes that each of the com-
ponents of each texel is stored as an unsigned 8-bit byte. Together,
parameters seven and eight deﬁne that the texel data we are submitting
has 32-bit texels, stored as RGBA quads, each component of which is
between 0 and 255.
■The ﬁnal parameter is a pointer to width × height texels of the given
format,
stored in row-major,
left-to-right,
bottom-to-top format.
In
the
previous
case,
the
pointer
will
point
to
a
block
of
width × height × 4 bytes of texture data.

288
Chapter 6 Geometry, Shading, and Texturing
Efﬁcient Texture Images in OpenGL
Demo
BasicTexturing
Note that glTexImage2D speciﬁes the data for only the “current texture,” the
one active for the next set of rendered geometry. If a texture will be used
many times (perhaps once or more per frame), this interface is a slow and
cumbersome way to have to specify textures each time they are used. Instead,
OpenGL allows applications to “bind” a texture to a positive integer “name” or
“identiﬁer.” This is done in two steps. First, one or more free texture identiﬁers
are generated via a call to glGenTextures, as in the following example, which
generates identiﬁers for four subsequent textures:
GLuint textures[4];
glGenTextures(4, textures);
Upon return, the array will contain four nonzero texture names that can be
bound to textures. Textures are both bound to names and accessed from
names by the same call. A call to
glBindTexture(GL_TEXTURE_2D, textures[0]);
will bind the texture name passed as the second parameter. The exact behavior
of glBindTexture is dependent upon whether or not the given identiﬁer has
already been “bound.” On the ﬁrst call to glBindTexture with a given nonzero
identiﬁer, this function will link the given identiﬁer to the current 2D texture
that was set with glTexImage2D. Subsequent calls with the same identiﬁer will
access the texture that was linked to the identiﬁer and replace the current
texture image. Once all textures are bound to different identiﬁers, calls to
glBindTexture are all that are needed to quickly switch between all textures.
In addition, the unsigned integer values are all that the application must store
to reference their textures. When a texture is no longer needed, it should be
deleted and its identiﬁer freed for later use with glDeleteTextures, which takes
the same arguments as glGenTextures as in the following:
GLuint textures[4];
// ...
glDeleteTextures(4, textures);
While convenience of texture speciﬁcation is a useful beneﬁt, there is a
much more important reason for using texture binding in OpenGL, owing
to the design realities of 3D rendering hardware. Image data that is to be
used as a texture must be stored in special memory that is a part of the

6.8 Texture Coordinates 289
graphics subsystem. Texture images created in main system memory must
ﬁrst be formatted for consumption by the texturing hardware (sometimes
requiring the bitwise format of the pixels to be converted to something
supported by the hardware). Then, the texture must be transferred to the
texture memory of the 3D hardware. Both of these steps are time-consuming,
and if they must be redone each time the texture is used (often more than
once per frame), the result can be greatly reduced performance of the
application. Binding the texture in OpenGL allows the OpenGL implemen-
tation to take these steps once, during the ﬁrst call to glBindTexture for each
texture. Subsequent calls to bind a deﬁned texture will simply require the
OpenGL implementation to set the hardware to use the existing version of
the texture in the device’s texture memory. This is a much faster operation
than converting and reloading a texture into texture memory. Note that if the
contents of the texture must be changed (e.g., changing the color of one or
more texels) once it is bound, the texture must be processed and transferred to
texture memory again. In fact, OpenGL must be told explicitly to reload these
changes — changing the contents of the source array of pixels that was passed
into the original call to glTexImage2D will have no effect on the copy of the
texture that is in texture memory. As a result, it is best to avoid changing the
pixel colors of textures once they are bound or else decreased performance can
result.
Sometimes it may not be possible for the OpenGL hardware to ﬁt all
currently bound textures into the device’s texture memory at once. In such
cases, the OpenGL implementation must move textures in and out of texture
memory as they are needed. A texture that is currently stored in the device’s
texture memory is referred to as resident, while a texture that is not currently
in texture memory is nonresident. If an OpenGL implementation must move
textures about every frame, performance of an application will be degraded. It
is important to delete bound textures that are no longer required, because this
frees texture memory for actively used textures. OpenGL includes numerous
functions that can be used by advanced programmers to ﬁne-tune the use of
bound textures, including functions for texture prioritization. See [83] for
details on texture memory management.
6.8 Texture Coordinates
While textures can be indexed by 2D vectors of nonnegative integers on a
per-texel basis (texel coordinates), textures are normally addressed in a more
general, texel-independent manner. The texels in a texture are most often
addressed via height- and width-independent “U” and “V” values. These 2D
real-valued coordinates are mapped in the same way as texel coordinates,

290
Chapter 6 Geometry, Shading, and Texturing
u=1.0, v=1.0
u=0.0, v=1.0
u=1.0, v=0.0
u=0.0, v=0.0
Figure 6.8 Mapping UV coordinates into an image.
except for the fact that U and V are multiplied by the width and height
of the texture, respectively. Figure 6.8 depicts the common mapping of
UV coordinates into a texture. These normalized UV coordinates have the
advantage that they are completely independent of the height and width of
the texture. Almost all texturing systems use these normalized UV coordi-
nates, and as a result, they are often referred to as texture coordinates, or
texture UVs.
The real-valued texture coordinates would seem to add a continuity that
does not actually exist across the domain of an image, which is a discrete set
of color values. For example, in C or C++ one does not access an array with a
float — the index must ﬁrst be rounded to an integer value. For the purposes
of the initial discussion of texturing, we will leave the details of how real-
valued texture coordinates map to texture colors somewhat vague. This is

6.8 Texture Coordinates 291
actually a rather broad topic and will be discussed in detail in Chapter 8.
Initially, it is easiest to think of the texture coordinate as referring to the color
of the closest texel. For example, given our assumption, a texture coordinate
of (0.5, 0.5) in a texture with width and height equal to 128 texels would map
to texel (64, 64). This is referred to as nearest-neighbor texture mapping. While
this is the simplest method of mapping real-valued texture coordinates into
a texture, it is not necessarily the most commonly used in modern applica-
tions. We shall discuss more powerful and complex techniques in Chapter 8,
but nearest-neighbor mapping is sufﬁcient for the purposes of the initial
discussion of texturing.
Demo
BasicTexturing
In vertex-by-vertex mode, per-vertex texture coordinates may be assigned
to a vertex in OpenGL by setting the current texture coordinate value prior to
creating a vertex. Recall that vertices are actually created only when glVertex*
is called to specify the vertex position. The u and v coordinates (s and t in
OpenGL) can be speciﬁed with
float u,v;
// ...
glTexCoord2f(u, v);
// OR
float uv[2];
// ...
glTexCoord2fv(uv);
When using vertex arrays and shared geometry, texture coordinates are
enabled using
glEnableClientState(GL_TEXTURE_COORD_ARRAY);
and the texture coordinate array itself is passed in using
static float uvs[2 * kNumVerts];
// ...
glTexCoordPointer(2, GL_FLOAT, 0, uvs);
where the arguments to glTexCoordPointer are equivalent to those of
glVertexPointer.

292
Chapter 6 Geometry, Shading, and Texturing
6.8.1 Mapping Texture Coordinates
The texture coordinates deﬁned at the three vertices of a triangle deﬁne
an afﬁne mapping from barycentric coordinates to UV space. Given the
barycentric coordinates of a point in a triangle, the texture coordinates may be
computed as follows (Do not confuse the barycentric s and t with the OpenGL
s and t; they are unrelated.):
 u
v

=
 (uV 1 −uV 3)
(uV 2 −uV 3)
uV 3
(vV 1 −vV 3)
(vV 2 −vV 3)
vV 3
 

s
t
1


Although there is a wide range of methods used to map textures onto triangles
(i.e., to assign texture coordinates to the vertices), a common goal is to avoid
“distorting” the texture. In order to discuss texture distortion, we need to
deﬁne the U and V basis vectors in UV space. If we think of the U and V vectors
as 2-vectors rather than the “point-like” texture coordinates themselves, then
we compute the basis vectors as
eu = (1, 0) −(0, 0)
ev = (0, 1) −(0, 0)
The eu vector deﬁnes the mapping of the horizontal dimension of the texture
(and its length deﬁnes the size of the mapped texture in that dimension), while
the ev vector does the same for the vertical dimension of the texture.
If we want to avoid distorting a texture when mapping it to a surface, we
must ensure that the afﬁne mapping of a texture onto a triangle involves rigid
transforms only. In other words, we must ensure that these texture-space
basis vectors map to vectors in object-space that are perpendicular and of
equal length. We deﬁne ObjectSpace() as the mapping of a vector in texture
space to the surface of the geometry object. In order to avoid distorting the
texture on the surface, ObjectSpace() should obey the following guidelines:
ObjectSpace(eu) · ObjectSpace(ev) = 0
|ObjectSpace(eu)| = |ObjectSpace(ev)|
In terms of an afﬁne transformation, the ﬁrst constraint ensures that the tex-
ture is not sheared on the triangle (i.e., perpendicular lines in the texture
image will map to perpendicular lines in the plane of the triangle), while the
second constraint ensures that the texture is scaled in a uniform manner (i.e.,
squares in the texture will map to squares, not rectangles, in the plane of the
triangle). Figure 6.9 shows examples of texture-to-triangle mappings that do
not satisfy these constraints.

6.8 Texture Coordinates 293
Skewed mappings
Original texture
Non-perpendicular
Non-uniform scale
Non-perpendicular
Figure6.9 Examples of “skewed” texture coordinates.
Note that these constraints are by no means a requirement — many cases
of texturing will stray from them, through either artistic desire or the sim-
ple mathematical inability to satisfy them in a given situation. However, the
degree that these constraints do hold true for the texture coordinates on a tri-
angle give some measure of how closely the texturing across the triangle will
reﬂect the original planar form of the texture image.
6.8.2 Generating Texture Coordinates
Texture coordinates are often generated upon an object by some form of pro-
jection of the object-space vertex positions in R3 into the per-vertex texture
coordinates in R2. All texture coordinate generation — in fact, all 2D textur-
ing — is a type of projection. For example, imagine the cartographic problem
of drawing a ﬂat map of the earth. This problem is directly analogous to map-
ping a 2D texture onto a spherical object. The process cannot be done without
distortion of the texture image. Any 2D texturing of a sphere is an exercise in
matching a projection/“unwrapping” of the sphere onto a rectangular image
(or several images) and the creation of 2D images that take this mapping into
account. For example, a common, simple mapping of a texture onto a sphere
is to use U and V as longitude and latitude in the texture image, respectively.
This leads to discontinuities at the pole, where more and more texels are
mapped over smaller and smaller surface areas as we approach the poles.

294
Chapter 6 Geometry, Shading, and Texturing
The artist must take this into account when creating the texture image.
Except for purely planar mappings (such as the wall of a building), most
texturing work done by an artist is an artistic cycle between generating texture
coordinates upon the object and painting textures that are distorted correctly
to map in the desired way to those coordinates.
6.8.3 Texture Coordinate Discontinuities
As was the case with per-vertex colors, there are situations that require shared,
colocated vertices to be duplicated in order to allow the vertices to have dif-
ferent texture coordinates. These situations are less common than in the case
of per-vertex colors, due to the indirection that texturing allows. Pieces of
geometry with smoothly mapped texture coordinates can still allow color dis-
continuities on a per-sample level by painting the color discontinuities into
the texture. Normally, the reason for duplicating colocated vertices in order
to split the texture coordinates has to do with topology.
For example, imagine applying a texture as the label for a model of a tin
can. For simplicity, we shall ignore the top and bottom of the can and sim-
ply wrap the texture as one would a physical label. The issue occurs at the
Shared vertex UVs
Texture image
v=0
v=1
u=0.375
u=0.5
u=0.0
u=0.875
u=0.75
u=0.625
u=0.125
u=0.25
Figure6.10 Texturing a can with completely shared vertices.

6.8 Texture Coordinates 295
texture’s seam. Figure 6.10 shows a tin can modeled as an 8-sided cylinder
containing 16 shared vertices, 8 on the top and 8 on the bottom. The map-
ping in the vertical direction of the can (and the label) is simple, as shown
in the ﬁgure. The bottom 8 vertices set V = 0.0 and the top 8 vertices set
V = 1.0. So far, there is no problem. However, problems arise in the assign-
ment of U. Figure 6.10 shows an obvious mapping of U to both the top and
bottom vertices — U starts at 0.0 and increases linearly around the can until
the eighth vertex, where it is 0.875, or 1.0 −0.125.
The problem is between the eighth vertex and the ﬁrst vertex. The ﬁrst
vertex was originally assigned a U value of 0.0, but at the end of our circuit
around the can, we would also like to assign it a texture coordinate of 1.0,
which is not possible for a single vertex. If we leave the can as is, most of it
will look perfectly correct, as we see in the front view of Figure 6.11. However,
looking at the back view in Figure 6.11, we can see that the face between the
eighth and ﬁrst vertex will contain a squashed version of almost the entire
texture, in reverse! Clearly, this in not what we want (unless we can always
hide the seam). The answer is to duplicate the ﬁrst vertex, assigning the copy
associated with the ﬁrst face U = 0.0 and the copy associated with the eighth
face U = 1.0. This is shown in Figure 6.12 and looks correct from all angles.
Front side
(Appears to be correctly mapped)
Back side
(Incorrect, due to shared
vertices along the label “seam”)
Figure6.11 Shared vertices can cause texture coordinate problems.

296
Chapter 6 Geometry, Shading, and Texturing
Front side
(Correct: unchanged from
previous mapping)
Back side
(Correct, due to doubled
vertices along the label “seam”)
Figure6.12 Duplicated vertices used to solve texturing issues.
6.8.4 Mapping Outside the Unit Square
So far, our discussion has been limited to texture coordinates within the unit
square, 0.0 ≤u, v ≤1.0. However, there are interesting options available if
we allow texture coordinates to fall outside of this range. In order for this to
work, we need to deﬁne how texture coordinates map to texels in the texture
when the coordinates are less than 0.0 or greater than 1.0. These operations
are per-sample, not per-vertex, as we shall discuss.
The most common method of mapping unbounded texture coordinates
into the texture is known as texture wrapping, texture repeating, or texture
tiling. The wrapping of a component u of a texture coordinate is deﬁned as
wrap(u) = u −⌊u⌋
The result of this mapping is that multiple “copies” of the texture “tile” the
surface. Wrapping must be computed per-sample, not per-vertex. Figure 6.13
shows a square whose vertex texture coordinates are all outside of the unit
square, with a texture applied via per-sample wrapping. Clearly, this is a very

6.8 Texture Coordinates 297
Texture image
(–1,–1)
(–1,2)
(2,2)
(2,–1)
Figure6.13 An example of texture wrapping.
different result than if we had simply applied the wrapping function to each
of the vertices (which can be seen in Figure 6.14). In most cases, per-vertex
wrapping produces incorrect results.
Wrapping is often used to create the effect of a tile ﬂoor, paneled walls, and
many other effects where obvious repetition of a texture is required. However,
in other cases wrapping is used to create a more subtle effect, where the edges
of each copy of the texture are not quite as obvious. In order to make the edges
of the wrapping less apparent, texture images must be created in such a way
that the matching edges of the texture image are equal.
Wrapping creates a toroidal mapping of the texture, as tiling matches the
bottom edge of the texture with the top edge of the neighboring copy (and vice
versa), and the left edge of the texture with the right edge of the neighboring
copy (and vice versa). This is equivalent to rolling the texture into a tube
(matching the top and bottom edges), and then bringing together the ends
of the tube, matching the seams. Figure 6.15 shows this toroidal matching
of texture edges. In order to avoid the sharp discontinuities at the texture
repetition boundaries, the texture must be painted or captured in such a way
that it has “toroidal topology”; that is, the neighborhood of its top edge is equal
to the neighborhood of its bottom edge, and the neighborhood of its left edge
must match the neighborhood of its right edge. Also, the neighborhood of the
four corners must all be equal, as they come together in a point in the mapping.
This can be a tricky process for complex textures, and various algorithms have

298
Chapter 6 Geometry, Shading, and Texturing
Texture image
(–1,–1)
(–1,2)
(2,2)
(2,–1)
Per-pixel wrapping
(correct)
(0,0)
(0,1)
(1,1)
(1,0)
Per-vertex wrapping
(incorrect)
Original UVs
(–1,–1)
(–1,2)
(2,2)
(2,–1)
Figure 6.14 Computing texture wrapping.
been built to try to create toroidal textures automatically. However, the most
common method is still to have an experienced artist create the texture by
hand to be toroidal.
The other common method used to map unbounded texture coordinates
is called texture clamping, and is deﬁned as
clamp(u) = max(min(u, 1.0), 0.0)

6.8 Texture Coordinates 299
Figure6.15 Toroidal matching of texture edges when wrapping.
Clamping has the effect of simply stretching the border texels (left, right, top,
and bottom edge texels) out across the entire section of the triangle that falls
outside of the unit square. An example of the same square we’ve discussed,
but with texture clamping instead of wrapping, is shown in Figure 6.16. Note
that clamping the vertex texture coordinates is very different from texture
clamping. An example of the difference between these two operations is shown
in Figure 6.17. Texture clamping must be computed per-sample and has no
effect on any sample that would be in the unit square. Per-vertex coordinate
clamping, on the other hand, affects the entire mapping to the triangle, as
seen in Figure 6.17.
Clamping is useful when the texture image consists of a section of detail
on a solid-colored background. Rather than wasting large expanses of texels
and placing a small copy of the detailed section in the center of the texture,
the detail can be spread over the entire texture but leaving the edges of the
texture as the background color.
On many systems clamping and wrapping can be set independently for
the two dimensions of the texture. For example, say we wanted to create
the effect of a road; black asphalt with a thin set of lines down the center

300
Chapter 6 Geometry, Shading, and Texturing
Texture image
(–1,–1)
(–1,2)
(2,2)
(2,–1)
Figure6.16 An example of texture clamping.
of the road. Figure 6.18 shows how this effect can be created with a very
small texture by clamping the U dimension of the texture (to allow the
lines to stay in the middle of the road with black expanses on either side)
and wrapping in the V dimension (to allow the road to repeat off into the
distance).
Demo
TextureWrapping
OpenGL supports both clamping and wrapping independently in U (which
it calls “S”) and V (which it calls “T”). The function glTexParameteri is
used to set these values. The ﬁrst argument speciﬁes which type of textur-
ing is to be affected (1-, 2-, or 3D), the second the mode and coordinate
axis (GL_TEXTURE_WRAP_S or GL_TEXTURE_WRAP_T in 2D texturing), and the ﬁnal
argument sets the mode. The possible modes are GL_REPEAT (wrapping),
GL_CLAMP_TO_EDGE (clamping), or GL_CLAMP (a modiﬁed version of clamping
that uses a single “edge color” instead of the texture edge; see the OpenGL
Programming Guide [83] for details of the behavior of this mode). To create
our road example, we would call
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT);

6.9 Reviewing the Steps of Texturing 301
Texture image
(–1,–1)
(–1,2)
(4,2)
(4,–1)
Per-pixel clamping
(correct)
(0,0)
(0,1)
(1,1)
(1,0)
Per-vertex clamping
(incorrect)
Original UVs
(–1,–1)
(–1,2)
(4,2)
(4,–1)
Figure 6.17 Computing texture clamping.
6.9 Reviewing the Steps of Texturing
Unlike basic Gouraud shading (which interpolates the per-vertex values
directly as the ﬁnal sample colors), texturing adds several levels of indirec-
tion between the values deﬁned at the vertices (the UV values) and the ﬁnal
sample colors. This is at once the very power of the method and its most con-
fusing aspect. This indirection means that the colors applied to a triangle by
texturing can approximate an extremely complex function, far more complex
and detailed than the planar function implied by Gouraud shading. However,

302
Chapter 6 Geometry, Shading, and Texturing
Texture image
(–5,0)
(5,0)
Textured square
(–5,10)
(5,10)
U clamping
V wrapping
Figure 6.18 Mixing clamping and wrapping in a useful manner.
it also means that there are far more stages in the method whereupon things
can go awry. This section aims to pull together all of the previous textur-
ing discussion into a simple, step by step pipeline. Understanding this basic
pipeline is key to developing and debugging texturing use in any application.
Texturing is a function that maps per-vertex 2-vectors (the texture coor-
dinates), a texture image, and a group of settings into a per-sample color. The
top-level stages are as follows:
1. Map the barycentric s and t values into u and v values, using the
afﬁne mapping deﬁned by the three triangle-vertex texture coordinates:
(u1, v1), (u2, v2), and (u3, v3). These input s and t values are the barycen-
tric coordinates of the point in the triangle, and should not be confused
with OpenGl’s similar renaming of u and v:
 u
v

=
 (u1 −u3)
(u2 −u3)
u3
(v1 −v3)
(v2 −v3)
v3
 

s
t
1



6.10 Limitations of Texturing 303
2. Using the texture coordinate mapping mode (either clamping or
wrapping), map the U and V values into the unit square:
uunit, vunit = wrap(u), wrap(v)
or,
uunit, vunit = clamp(u), clamp(v)
3. Using the width and height of the texture image in texels, map the U
and V values into integral texel coordinates via simple scaling:
utexel, vtexel = ⌊uunit × width⌋, ⌊vunit × height⌋
4. Using the texture image, map the texel coordinates into colors using
image lookup:
CT = Image(utexel, vtexel)
These steps compose to create the mapping from a point on a given triangle
to a color value. The following inputs must be conﬁgured, regardless of the
speciﬁc graphics system:
■The per-vertex texture coordinates
■The texture image to be applied
■The coordinate mapping mode
6.10 Limitations of Texturing
For all of the ﬂexibility that texturing affords the real-time 3D application
developer, it still shares several limitations with its simpler cousins, ﬂat and
Gouraud shading. All of the methods described thus far assign colors that do
not change for any given sample point at runtime. In other words, no matter
what occurs in the scene, a ﬁxed point on a given surface will always return
the same color.
Real-world scenes are dynamic, with colors that change in reaction to
changes in lighting, changes in position, and even changes to the surfaces
themselves. Any shading method that relies entirely on values that are ﬁxed
over time and scene conditions will be unable to create truly convincing,

304
Chapter 6 Geometry, Shading, and Texturing
dynamic worlds. Methods that can represent real-world lighting and the
dynamic nature of moving objects are needed.
A very popular method of achieving these goals is to use a simple, fast
approximation of real-world lighting. The next chapter will discuss in detail
many aspects of how lighting is approximated in real-time 3D systems.
Another method of generating dynamic shading of geometry is so-called pro-
cedural shading. While procedural shading has long been popular in off-line
renderings for high-quality computer-generated images (such as those for fea-
ture ﬁlms), it has more recently become popular in a simpler form, even in
consumer-level 3D hardware. These simpler versions of fully general proce-
dural shading are known as pixel shaders and vertex shaders and are discussed
in the next section.
6.11 Procedural Colors and Shaders
At the highest level, the most powerful method of assigning colors to geom-
etry would be to allow a completely generic, arbitrarily complex function to
specify the color of a triangle at any given point. Such a method is often called
procedural texturing, or procedural shading, so-called because the colors are
generated by a small program or procedure, rather than directly from exist-
ing per-vertex or per-triangle colors. Such general procedural shaders are the
accepted norm in non–real-time, photorealistic rendering, because they offer
almost unlimited ﬂexibility to the programmer or artist. However, by their
very nature, these complex procedural shaders can require large amounts
of computation per sample. While such a system is well-suited for the ﬁlm
industry, where single frames can be allowed hours to render on a high-
end workstation, they are not as well-suited for real-time rendering, where
an entire frame (often over a million samples) must be rendered in under
one-thirtieth of a second on a consumer PC with a 3D graphics accelerator.
Consumer 3D hardware has advanced at an incredible rate, and most
consumer 3D hardware built today supports a limited version of this general
method via so-called vertex shaders and pixel shaders (also known as vertex
programs and fragment programs in OpenGL), very simple programs that
are run either per vertex (vertex shaders) or per sample (pixel shaders) to
determine the color of a triangle at a point. These shaders can create incredible
dynamic effects. The vertex and pixel shader standards, such as those set by
DirectX, impose some basic limits to ensure that hardware can implement
the range of possible shaders efﬁciently and consistently.
Pixel and vertex shader standards also avoid the considerable pain that
preshader PC 3D graphics programmers spent testing and coping with the
hundreds of “capability ﬂags” that each piece of 3D hardware returned to
describe their feature set. These preshader capability ﬂags led to enormous

6.11 Procedural Colors and Shaders 305
amounts of renderer code to handle all of the various cases for different
hardware cards. In fact, developers writing PC-based 3D renderers prior to
the shader standards sometimes ended up having to actually query the name
of the hardware card to enable or disable a block of code in their renderer.
This was a fragile technique that caused no end of game-compatibility issues
for end users.
The DirectX shader standard includes version numbers, which have
allowed the standard to be upgraded over time, allowing for new features
with backwards compatibility. However, this places a greater burden on a
programmer who wishes to take advantage of these new features while still
enabling their application to run on older hardware. If a shader is written
to use instructions or limitations that were expanded for pixel shader
version 2.0, for example, that shader cannot be used on a piece of 3D hardware
that only supports pixel shaders up to version 1.0. The shader author will
need to include another, more limited shader that is 1.0-compliant in order to
work on the older hardware. Limitations that existed in some of the older
shader versions include:
■A ﬁxed limit to the number of instructions in the shader (program
length)
■No looping or limited ﬂow control (branching)
■Limitations on the number and type of possible arguments to the
shading function (inputs)
■A limited number of temporary variables available during computation
(“scratch space”)
■Limited instruction set compared to general-purpose processors
Most of these limitations have been avoided in recent versions of the pixel
shader standard, but until the current version of shaders has been available in
3D hardware for one or two years, shader authors will need to include limited
(generally less interesting) versions of their more complex shaders.
One original hurdle to the acceptance of shaders was the fact that ren-
dering APIs expose shaders to the programmer at a very low level, one that
resembles the assembly language of a very simple CPU. For today’s program-
mers, most of whom are well-versed and experienced in high-level languages,
these low-level shading languages are at best cumbersome and at worst
foreign. Worse yet, the limitations of these languages made it very difﬁcult to
write reusable code, meaning that using shaders in applications could end up
involving dozens of pieces of shader code, each written for a different case.
This hurdle is been addressed agressively by hardware and API vendors,
who have been working to create and expand high-level languages such as

306
Chapter 6 Geometry, Shading, and Texturing
nVIDIA’s Cg [75] and Microsoft’s HLSL (High-Level Shading Language) for
writing shaders. Both of these systems offer high-level languages to spec-
ify and compile shaders. These languages are not a complete solution for
second-generation shader hardware however, as these shader compilers are
still limited by the underlying limitations of current and previous generation
shader hardware. Nevertheless, Cg and HLSL have gained acceptance quite
rapidly, and there is every reason to believe that low-level shader program-
ming will become less and less common as the shader hardware expands and
the shader compilers continue to improve.
Another consideration with pixel and vertex shaders is that they are cur-
rently “all or nothing” prospects. In other words, if a developer intends to use
a vertex shader for an object in an application, he or she may wish to do so to
change only one aspect of the lighting pipeline. However, a shader is respon-
sible for transformation, lighting, and projection of the vertices sent to it.
As a result, the shader author wishing to change one aspect of lighting must
write the transform and projection code into the shader as well. While this
code can often be copied between the shaders in an application (and shaders
for the common cases are available on the Internet from hardware vendors),
it is still a burden that can turn some developers away from using shaders.
The greatest remaining limitation in current vertex shaders as set by the
standards is that a vertex shader is a “one vertex in, one vertex out” pipeline
to ensure general parallelism and simplicity. For example, this means that
vertex shaders cannot subdivide geometry to add more detail, nor can they
use the connectivity information to ﬁnd or use adjacent vertices. Each vertex is
shaded as if it were the only vertex in the model. However, ﬂexibility is added
by allowing a wide, customizable set of per-vertex data. In addition to the
information normally associated with vertices, such as normals and diffuse
colors, pixel shaders can include application- or shader-speciﬁc data with each
vertex that can be used as inputs to the shader programs. These extra data
slots can be used to implement vertex position animation in hardware, special
lighting models involving complex surface properties, or almost anything the
shader author wishes.
Pixel shaders must also work on a single pixel at a time, but the ability to
sample several textures per pixel allows for pixel shaders to create incredibly
wide-ranging effects, with textures used almost as general function-lookup
tables.
Many of the most popular shaders that have been written and distributed
are simply advanced mixtures of the texturing effects described earlier in
this chapter along with some lighting tricks, such as those that will be dis-
cussed in the next chapter. As a result, rather than completely replacing the
currently known techniques, shaders often build upon them, making knowl-
edge of basic shading and lighting a prerequisite for building truly effective
shaders. As shaders and hardware advance, new techniques are surfacing that
push shaders beyond these basic methods, including even simple ray tracing

6.12 Chapter Summary 307
and other complex reﬂection, optical, and atmospheric effects. These shaders
beneﬁt not only from their authors’ understanding of computer graphics and
shaders but also from their knowledge of physically-based lighting models
and optical models. It is here where the true promise of shaders is starting to
be seen.
The topics discussed throughout this book are reﬂected in the instruction-
set architecture of shaders. The instructions tend either to implement fast
versions of notoriously expensive operations or else fold somewhat complex
but common computations into a single instruction. Examples include vertex
shader instructions such as
■dp3, Computes the 3D dot product between two vectors in a single
instruction.
■m4x3, Computes the multiplication of a 4 × 3 matrix with a 4-vector
and is useful for computing scale-rotate-translate, model-to-camera
transforms. Note that this (and some other vertex shader instruc-
tions) is a macro instruction and evaluates to a sequence of three
actual instructions. On vertex shader hardware that imposes tight lim-
its on instruction count, this can cause trouble if counted as a single
instruction.
■rsq, Computes a single reciprocal square root (i.e., 1/√x). This is
commonly used as part of a sequence of instructions to normalize
a vector.
A full discussion of the options available via shaders is outside the scope
of this text. For a more detailed introduction to pixel shaders, see [6] and [82].
For a more detailed discussion of cutting-edge shaders, see one of the growing
number of shader books, such as [33]. Shader capabilities of consumer 3D
hardware is advancing rapidly, well beyond the rate of most book publication,
making the Internet an excellent source of up-to-date shader information.
6.12 Chapter Summary
In this chapter, we have discussed a wide range of methods used to map colors
onto geometry. These techniques and concepts lay the foundation for the
next two chapters, which will discuss a popular method of generating source
colors (dynamic lighting), as well as a detailed discussion of the main con-
sumer of these colors (rasterization). While we have already discussed many
details regarding the extremely popular shading method known as texturing,
this chapter is not the last time we shall mention it. Both of the following

308
Chapter 6 Geometry, Shading, and Texturing
two chapters will discuss the ways that texturing affects other stages in the
rendering pipeline.
For further reading, popular graphics texts such as Foley, van Dam,
Feiner, and Hughes [36] detail other aspects of shading, including methods
used for high-end off-line rendering, which are exactly the kinds of meth-
ods that are now starting to be implemented as pixel and vertex shaders in
real-time hardware. Shader books such as [33] also discuss and provide exam-
ples of speciﬁc programmable shaders that implement these high-end shading
methods and can serve as springboards for further experimentation.

Chapter7
Lighting
7.1 Introduction
Much of the way we perceive the world visually, especially in terms of depth
perception, is based on the way objects in the world react to lighting. This is
especially true when the lighting in the visible scene is changing or the lights
or objects are moving. While parallax (the apparent motion of objects with
respect to one another as the viewpoint changes) is the strongest perceptual
signal of depth and relative object position, changes in lighting also make a
strong impact.
The coloring methods we have discussed so far, while powerful, use colors
that are statically assigned at content creation time (by the artist) or at load-
time (by the application). These colors do not change on a frame-to-frame
basis. At best, the colors represent a “snapshot” of the scene lighting at a
given moment for a given conﬁguration of objects. For example, imagine a
simple room scene, containing three lights at ﬁxed positions. Assume fur-
ther that we cannot move any of the objects in the room. Given these (very
limiting) assumptions, to model all possible light-switch positions, we would
still need to generate 23 = 8 completely independent sets of textures (or ver-
tex colors) for the room. Even if we did create these eight texture sets, any
shiny objects in the room still would not look realistic as the camera moved
around the room. Clearly, we need a dynamic method of rendering light-
ing in real time. The following sections will discuss the details of a popular
set of methods for approximating lighting for real-time rendering, as well as
examples of how these methods are exposed via OpenGL. Another popular
309

310
Chapter 7 Lighting
rendering API, Direct3D, uses a slightly different lighting model, but the two
are not completely divergent by any means. In order to avoid confusion, we
will discuss only the OpenGL model.
7.2 Basics of Light Approximation
The physical properties of light are incredibly complex. Even relatively simple
scenes could never be rendered realistically without “cheating.” In a sense, all
of computer graphics is little more than cheating — ﬁnding the cheapest-to-
compute approximation for a given situation that will still result in a realistic
image. Even non-real-time, photorealistic renderings are only approximations
of reality, trading off accuracy for ease and speed of computation.
Real-time renderings are even more superﬁcial approximations. Light in
the real world reﬂects, scatters, diffracts, and bounces around the environ-
ment. Real-time 3D lighting generally models only direct lighting, the light
that comes along an unobstructed path from light source to surface. Worse
yet, many basic real-time lighting systems (including OpenGL) do not support
automatic shadowing — objects located between the object being lit and the
light source are ignored in the name of efﬁciency. However, despite these lim-
itations, basic lighting can have tremendous effects on the overall impression
of a rendered 3D scene.
Lighting in real-time 3D generally involves data from at least three differ-
ent sources: the surface conﬁguration (vertex position, normal), the surface
material (how the surface reacts to light), and the light source properties (the
way the light sources emit light).
7.2.1 Measuring Light
In order to understand the mathematics of lighting, even the simpliﬁed, non-
physical approximation used by OpenGL, it is helpful to understand a little bit
about how light is actually measured. The simplest way to understand how
we measure light is in terms of an idealized light bulb and an idealized sur-
face being lit by that bulb. To understand both the brightness and luminance
(these are actually two different concepts; we will deﬁne them in the following
section) of a lit surface, we need to measure and track the following path from
end to end:
■The amount of light generated by the bulb
■The amount of light reaching the surface from the bulb
■The amount of light reaching the viewer from the surface

7.2 Basics of Light Approximation 311
Each of these is measured and quantiﬁed differently. First, we need a way
of measuring the amount of light being generated by the light bulb. Light
bulbs are generally rated according to several different criteria. The number
most people think of with respect to light bulbs is wattage; for example, we
think of a 100-watt light bulb as being much brighter than a 25-watt light bulb,
and generally, this is true. Wattage in this case is a measure of the electrical
power consumed by the bulb in order to create light. It is not a direct measure
of the amount of light actually generated by the bulb. In other words, two light
bulbs may consume the same wattage (say, 100 watts) but produce different
amounts of light — one type of bulb may simply be more efﬁcient at converting
electricity to light. So what is the measure of light output from the bulb?
Overall light output from a light source is a measure of power: light energy
per unit time. This quantity is called luminous ﬂux. The unit of luminous ﬂux
is the lumen. The luminous ﬂux from a light bulb is measured in lumens,
a quantity that is generally listed on boxes of commercially available light
bulbs, near the wattage rating. However, lumens are not how we measure the
amount of light that is incident upon a surface.
There are several different ways of measuring the light incident upon a
surface. The one that will be of greatest interest to us is illuminance. Illumi-
nance is a measure of the amount of luminous ﬂux falling on a given area of
surface. Illuminance is also called luminous ﬂux density, as it is the amount of
luminous ﬂux per unit area. It is measured in units of lux, which are deﬁned
as lumens per meter squared. Illuminance is an important quantity because
it measures not only the light power (in lumens), but also the area over which
this power is distributed (in square meters). Given a ﬁxed amount of luminous
ﬂux, increasing the surface area over which it is distributed will decrease the
illuminance proportionally. We will see this property again later, when we
discuss the illuminance from a point light source. Illuminance in this case
is only the light incident upon a surface — not the amount reﬂected from the
surface.
Light reﬂection from a surface depends on a lot of properties of the surface
and the geometric conﬁguration. We will cover approximations of reﬂection
later in this chapter. However, the ﬁnal step in our list of lighting measure-
ments is to deﬁne how we measure the reﬂected light reaching the viewer from
the surface. The quantity used to measure this is luminance, which is deﬁned
as illuminance per unit solid angle. The unit of luminance is the nit, and this
value is the closest of those we have discussed to representing “brightness.”
However, brightness is a perceived value and is not linear with respect to
luminance, due to the response curve of the human visual system. For details
of the relationship between brightness and luminance, see [20].
The preceding quantities are photometric; that is, they are weighted by the
human eye’s response to different wavelengths of light. The ﬁeld of radiometry
studies the measurement of analogous quantities that do not include this phys-
iological weighting. The radiometric equivalent of illuminance is irradiance

312
Chapter 7 Lighting
(measured in watts per meter squared), and the equivalent of luminance
is radiance. These radiometric units and quantities are relevant to anyone
working with computer graphics, as they are commonly seen in the ﬁeld of
non-real-time rendering, especially in techniques known collectively as global
illumination (see [19]).
7.2.2 Light as a Ray
Our discussion of light sources will treat light from a light source as a collec-
tion of rays, or in some cases simply as vectors. These rays represent inﬁnitely
narrow “shafts” of light. This representation of light will make it much simpler
to approximate light-surface interaction. Our light rays will often have RGB
colors or scalars associated with them that represent the “intensity” (and in
the case of RGB values, the color) of the light incident upon a surface. While
this value is often described in OpenGL literature as “brightness” or even
“luminance,” these terms are descriptive rather than physically based. In fact,
these intensity values are more closely related to and roughly approximate the
illuminance incident upon the given surface from the light source.
As we shall see, many low-level graphics systems (such as OpenGL) light
an object without considering any other objects in the scene. As a result,
no shadowing is computed. Computing even basic light occlusion can be
extremely complex, since it involves determining if any object in the scene
blocks the path between the current light and the point being lit. In fact, at its
most basic, the operation is one of picking: generating a ray between the light
position and the point being lit and checking to see if this ray intersects any
objects. A technique known as ray tracing (see [40]) uses ray-object intersec-
tion to track the way light bounces around a scene. Very convincing shadows
(and reﬂections) could be computed using ray tracing, and the technique was
very popular in the 1980s and 1990s for non-real-time rendering. Owing to
its computational complexity, this method is not generally used in real-time
lighting, but shadows are sometimes approximated using other tricks (see [6],
[81], [82], or Chapter 13 of Eberly [27]).
7.3 Lighting Approximation (OpenGL)
For the purposes of introducing a real-time lighting equation, we will discuss
an approximation that is based on OpenGL’s lighting model (or “pipeline”),
speciﬁcally mentioning when our discussion strays from the model laid out in
the OpenGL standard. OpenGL’s lighting model is both standard and similar
to those in other major graphics APIs. Initially, we will speak in terms of
lighting “a sample”: a generic point in space that may or may not represent a

7.4 Types of Light Sources 313
triangle or a vertex in a tessellation. We will attempt to avoid the concepts of
vertices and triangles in this discussion, preferring to refer to a general point
on a surface, along with a local surface normal and a surface material. (As will
be detailed later, a surface material contains all of the information needed
to determine how an object’s surface reacts to lighting.) As we’ve discussed,
OpenGL’s lighting model does not represent the “real world” — there are many
simpliﬁcations required for real-time lighting performance.
By default, OpenGL uses the supplied vertex colors directly. In order
to switch from direct use of the static vertex colors to real-time lighting
computations, use
glEnable(GL_LIGHTING);
To switch lighting off and return to static coloring, use
glDisable(GL_LIGHTING);
7.4 Types of Light Sources
The next few sections will discuss the common types of light sources that
appear in real-time 3D systems. Each section will open with a general discus-
sion of a given light source, followed by coverage in mathematical terms, and
close with the speciﬁcs of implementation in OpenGL, along with any inter-
esting results of or reasons for OpenGL’s design decisions. The discussion will
progress (roughly) from the simplest (and least computationally expensive)
light sources to the most complex.
For each type of light source, we will be computing two important values:
the unit-vector ˆL (here, we break with our notational convention of lower-
case vectors in order to make the equations more readable) and the scalar iL.
The vector ˆL is the light direction vector — it points from the current surface
sample point PV , toward the source of the light.
The scalar iL is the light intensity value, which is a rough approximation
of the illuminance from the light source at the given surface location PV .
With some types of lights, there will be per-light tuning values that adjust
the function that deﬁnes iL. In addition, in each of the ﬁnal lighting term
equations, we will also modulate in an RGB color light intensity value that
scales iL. These color terms are of the form LA, LD, and so on. They will be
deﬁned per light and per lighting component and will (in a sense) approximate
a scale factor upon the overall luminous ﬂux from the light source.
The values ˆL and iL do not take any information about the surface itself
into account, only the relative geometry between the light source and the

314
Chapter 7 Lighting
sample point in space. Discussion of the contribution of surface orientation
(i.e., the surface normal) will be taken up individually, as each type of light
and component of the lighting equation will be handled differently.
7.4.1 Directional Lights
A directional light source (also known as an “inﬁnite” light source) is similar
to the light of the Sun as seen from Earth. Relative to the size of the Earth, the
Sun seems almost inﬁnitely far away, meaning that the rays of light reaching
the Earth from the Sun are basically parallel to one another, independent of
position on the earth. Consider the source and the light it produces as a single
vector. A directional light is deﬁned by a point at inﬁnity, PL. The light source
direction is produced by turning the point into a unit vector (by subtracting
the position of the origin and normalizing the result):
ˆL = PL −0
|PL −0|
Figure 7.1 shows the basic geometry of a directional light. Note that the light
rays are the negative (reverse) of the light direction vector ˆL, since it points
from the surface to the light source.
The value iL for a directional light is constant for all sample positions:
iL = 1
Since both iL and light vector ˆL are constant for a given light (and indepen-
dent of the sample point PV ), directional lights are the least computationally
Light  rays
(infinitely distant)
PL
Figure 7.1 The basic geometry of a directional light.

7.4 Types of Light Sources 315
expensive type of light source. Neither ˆL nor iL needs to be recomputed for
each sample.
In OpenGL a directional light is signiﬁed by setting the w-coordinate of
the desired light’s position to zero, causing it to be treated as an afﬁne vector,
rather than a point. The x, y, and z components of the light position should
be set to the corresponding components of PL.
OpenGL refers to lights by integer indices. The light at a given index may
be of any type and is enabled via the function call:
int index;
// ...
glEnable(GL_LIGHT0 + index);
where index is the desired light (zero-based). The function glDisable may be
used to turn off a light in the same way. The following code sets light 0 to be a
directional light that is located inﬁnitely far away in the direction of the given
vector dir as follows:
GLfloat dir[4];
// ...
dir[3] = 0.0f; // w coord
glLightfv(GL_LIGHT0, GL_POSITION, dir);
7.4.2 Point Lights
A point or positional light source (also known as a “local” light source to
differentiate it from an inﬁnite source) is similar to a bare light bulb, hanging
in space. It illuminates equally in all directions. A point light source is deﬁned
by its location, the point PL. The light source direction produced is
ˆL = PL −PV
|PL −PV |
This is the normalized vector that is the difference from the sample position
to the light source position. It is not constant per-sample, but rather forms a
vector ﬁeld that points toward PL from all points in space. This normalization
operation is one factor that often makes point lights more computationally
expensive than directional lights. While this is not a prohibitively expensive
operation to compute once per light, we must compute the subtraction of two
points and normalize the result to compute this light vector for each lighting
sample (generally per-vertex for each light) for every frame. Figure 7.2 shows
the basic geometry of a point light.

316
Chapter 7 Lighting
Light rays
PL
Figure 7.2 The basic geometry of a point light.
In OpenGL, point lights are speciﬁed with a nonzero w-coordinate. The
following code sets light 0 to be a positional light that is located at the given
position pos.
GLfloat pos[4];
// ...
pos[3] = 1.0f; // w coord
glLightfv(GL_LIGHT0, GL_POSITION, pos);
Unlike the directional light, a positional light has a nonconstant func-
tion deﬁning iL. This nonconstant intensity function approximates a basic
physical property of light known as the inverse-square law (which we will
detail shortly). Our idealized point light source radiates a constant amount
of luminous ﬂux, which we call I, at all times. In addition, this light power
is evenly distributed in all directions from the point source’s location. Thus,
any cone-shaped subset (a solid angle) of the light coming from the point
source represents a constant fraction of this luminous ﬂux (we will call
this Icone). An example of this conical subset of the sphere is shown in
Figure 7.3.
Illuminance (the photometric value most closely related to our iL) is mea-
sured as luminous ﬂux per unit area. If we intersect the cone of light with a
plane perpendicular to the cone, the intersection forms a disc (see Figure 7.3).
This disc is the surface area illuminated by the cone of light. If we assume
that this plane is at a distance dist from the light center and the radius of

7.4 Types of Light Sources 317
dist
2dist
4or2
or2
r
2r
Figure 7.3 The inverse-square law.
the resulting disc is r, then the area of the disc is πr2. The illuminance Edist
(in the literature, illuminance is generally represented with the letter E) is
proportional to
Edist = power
area
∝Icone
πr2
However, at a distance of 2dist, then the radius of the disc is 2r (see Figure 7.3).
The resulting radius is π(2r)2, giving an illuminance E2dist proportional to
E2dist ≈
Icone
π(2r)2 = Icone
4πr2 = Edist
4
Doubling the distance divides the illuminance by a factor of four, because the
same amount of light energy is spread over four times the surface area. This
is known as the inverse-square law, and it states that for a point source, the
illuminance decreases with the square of the distance from the source. As an
example of a practical application, the inverse-square law is the reason why
a candle can illuminate a small room that is otherwise completely unlit but

318
Chapter 7 Lighting
will not illuminate an entire stadium. In both cases, the candle provides the
same amount of luminous ﬂux. However, the actual surface areas that must
be illuminated in the two cases are vastly different due to distance.
The inverse-square law results in a basic iL for a point light equal to
iL =
1
dist2
where
dist = |PL −PV |
which is the distance between the light position and the sample position.
Demo
Distance
Attenuation
While exact inverse-square law attenuation is physically correct, it does
not always work well artistically or perceptually. As a result, OpenGL and
most other modern graphics APIs support a more general distance attenuation
function for positional lights; a general quadratic. Under such a system, the
function iL for a point light is
iL =
1
kc + kldist + kqdist2
The distance attenuation constants kc, kl, and kq are deﬁned per light and
determine the shape of that light’s attenuation curve. Figure 7.4 is a visual
example of constant, linear, and quadratic attenuation curves. The spheres in
each row increase in distance linearly from left to right.
The OpenGL light values that map to kc, kl, and kq are GL_CONSTANT_ATTENU-
ATION, GL_LINEAR_ATTENUATION, and GL_QUADRATIC_ATTENUATION, respectively,
and are set using glLight*. OpenGL deﬁnes that dist be computed in “eye” or
camera coordinates; this speciﬁcation of the space used is important, as there
may be scaling differences between model space, world space, and camera
space, which would change the scale of the attenuation.
The attenuation of a point light’s intensity by this quadratic can be com-
putationally expensive, as it must be recomputed per-sample. In order to
increase performance on some systems, OpenGL applications can leave the
attenuation values at their OpenGL defaults, which are kc = 1 and kl = kq = 0.
This disables distance attenuation and can increase performance in some
cases.
7.4.3 Spotlights
A spotlight is like a point light source with the ability to limit its light to a
cone-shaped region of the world. The behavior is similar to a theatrical
spotlight with the ability to focus its light upon a speciﬁc part of the scene.

7.4 Types of Light Sources 319
Constant
Linear
Quadratic
Figure7.4 Distance attenuation.
In addition to the position PL that deﬁned a point light source, a spot-
light is deﬁned by a direction vector d, a scalar cone angle θ, and a scalar
exponent s. These additional values deﬁne the direction of the cone and the
behavior of the light source as the sample point moves away from the cen-
tral axis of the cone. The inﬁnite cone of light generated by the spotlight

320
Chapter 7 Lighting
PL
h
d
Light rays
Figure 7.5 The basic geometry of a spotlight.
has its apex at the light center PL, an axis d (pointing toward the base of the
cone), and a half angle of θ. Figure 7.5 illustrates this conﬁguration. The
exponent s is not a part of the geometric cone; as will be seen shortly, it is
used to attenuate the light within the cone itself.
The light vector is equivalent to that of a point light source:
ˆL = PL −PV
|PL −PV |
For a spotlight, iL is based on the point light function but adds an addi-
tional term to represent the focused, conical nature of the light emitted by a
spotlight:
iL =
spot
kc + kldist + kqdist2

7.4 Types of Light Sources 321
where
spot =

(−ˆL · d)s,
if (−ˆL · d) ≥cos θ
0,
otherwise
As can be seen, the spot term is 0 when the sample point is outside of the
cone. The spot term makes use of the fact that the light vector and the cone
vector are normalized, causing (−ˆL · d) to be equal to the cosine of the angle
between the vectors. We must negate ˆL because it points toward the light,
while the cone direction vector d points away from the light. Computing the
cone term ﬁrst can allow for performance improvements by skipping the rest
of the light calculations if the sample point is outside of the cone. In fact,
some graphics systems even check the bounding volume of an object against
the light cone, avoiding any spotlight computation on a per-sample basis if
the object is entirely outside of the light cone.
Inside of the cone, the light is attenuated via a function that does not
represent any physical property but is designed to allow artistic adjustment.
The light’s iL function reaches its maximum inside the cone when the vertex
is along the ray formed by the light location PL and the direction d, and
decreases as the vertex moves toward the edge of the cone. The dot product
is used again, meaning that iL falls off proportionally to
coss ω
where ω is the angle between the cone direction vector and the vector between
the sample position and the light location (PV −PL). As a result, the light need
not attenuate smoothly to the cone edge — there may be a sharp drop to iL = 0
right at the cone edge. Adjusting the s value will change the rate at which iL
falls to 0 inside the cone as the sample position moves off axis.
The multiplication of the spot term with the distance attenuation term
means that the spotlight will attenuate over distance within the cone. In this
way, it acts exactly like a point light with an added conic focus. The fact that
both of these expensive attenuation terms must be recomputed per-sample
makes the spotlight the most computationally expensive type of standard light
in most systems. When possible, applications attempt to minimize the number
of simultaneous spotlights (or even avoid their use altogether).
Spotlights with circular attenuation patterns are not universal. Another
popular type of spotlight (see Warn [111]) models the so-called barn door
spotlights that are used in theater, ﬁlm, and television. Such lights have four
metal “doors” around the edge of the light, forming a square. Each of the
doors may swing in or out to tighten the light pattern in that direction. Barn
door lights allow for much ﬁner-grained control than cone-based spotlights.
However, more information is required to model them, as the positions of
the four barn doors must be stored and used. Also, the orientation of the

322
Chapter 7 Lighting
“ring” of barn doors must be known, since the light is no longer rotationally
symmetrical around its direction vector as it was in a cone-shaped spotlight.
Because of these additional computational expenses, conical spotlights are by
far the more common form in real-time graphics systems.
Demo
Spotlight
In OpenGL, a spotlight is deﬁned as a point light source with a spotlight
cone angle (called the cutoff angle in OpenGL) that is ̸= 180 degrees. The
default spotlight cone angle for a light is 180 degrees, meaning that unless the
angle is changed, a positional light will illuminate objects in all directions (i.e.,
it will not be a spotlight). This default was chosen to ensure both performance
and ease of use. Since spotlights are so computationally expensive and can
be hard to use (it is easy to select a direction vector that causes the light to
point off in the wrong direction, leaving the scene with no light), it is best to
require an application to speciﬁcally enable them.
The spot cutoff angle is speciﬁed in degrees using glLightf, passing
the enumeration GL_SPOT_CUTOFF and the angle as a ﬂoating-point scalar.
Remember that this is actually the half-angle of the cone — the overall ﬁeld of
view of the spotlight will be twice this value. Similarly, the spotlight attenua-
tion exponent is set using glLightf with an enumeration of GL_SPOT_EXPONENT.
The spotlight direction vector is set using glLightfv, passing the enumeration
GL_SPOT_DIRECTION and a ﬂoating-point 3-vector containing the direction. An
example of setting light 0 to a spotlight at the origin, pointing along the x-axis,
with a 30-degree cutoff angle and quadratic attenuation follows:
GLfloat pos[4] = { 0.0f, 0.0f, 0.0f, 1.0f };
glLightfv(GL_LIGHT0, GL_LIGHT_POSITION, pos);
GLfloat dir[3] = { 1.0f, 0.0f, 0.0f };
glLightfv(GL_LIGHT0, GL_SPOT_DIRECTION, dir);
glLightf(GL_LIGHT0, GL_SPOT_CUTOFF, 30.0f);
glLightf(GL_LIGHT0, GL_SPOT_EXPONENT, 2.0f);
7.4.4 Other Types of Light Sources
One type of light source that is not generally supported in low-level, real-
time 3D graphics SDKs (including OpenGL) are area light sources, similar to
the ﬂuorescent light ﬁxtures seen in most ofﬁce buildings. The main interest
in area light sources are the soft-edged shadows that they produce. These
soft-edged shadows occur at shadow boundaries, where the point in partial
shadow is illuminated by part of the area light source but not all of it. The
shadow becomes progressively darker as the given point can “see” less and
less of the area light source. This soft shadow region (called the penumbra,

7.5 Surface Materials and Light Interaction 323
as opposed to the fully-shadowed region, called the umbra) is highly prized
in non–real-time, photorealistic renderings for the realistic quality it lends to
the results.
Real-time 3D lighting generally avoids testing per-sample light-object
visibility. In fact, soft shadows are even more complicated than hard shad-
ows, as the fraction of the area light that is visible from the given point must
be computed and is not just a Boolean visible/not visible switch. Since it is
very expensive to compute these soft shadows in a general way in real time,
the great beneﬁt of area light sources is lost, and most real-time systems do
not support them.
7.5 Surface Materials and Light
Interaction
Having discussed the various ways in which the light sources in our model
generate light incident upon a surface, we must complete the model by
discussing how this incoming light (our approximation of illuminance) is con-
verted (or reﬂected) into outgoing light (our approximation of luminance) as
seen by the viewer or camera. This section will discuss a common real-time
model of light/surface reﬂection.
In the presence of lighting, there is more to surface appearance than a
single color. Surfaces respond differently to light, depending upon their com-
position; for example, unﬁnished wood, versus plastic, versus metal. Gold-
colored plastic, gold-stained wood, and actual gold all respond differently to
light, even if they are all the same basic color. Most real-time 3D lighting
models take these differences into account with the concept of a material.
Demo
Components
A material describes the behavior of an object with respect to light. In our
real-time rendering model, a material describes the way a surface generates or
responds to four different categories of light: emitted light, ambient light, dif-
fuse light, and specular light. Each of these forms of light is an approximation
of real-world light and, put together, they can serve well at differentiating not
only the colors of surfaces but also the apparent compositions (shiny versus
matte, plastic versus metal, etc.). Each of the four categories of approximated
light will be individually discussed.
7.5.1 OpenGL Materials
As with the rest of the chapter, the focus will be on the lighting model
that is used by OpenGL. Most of these concepts carry over to other com-
mon low-level, real-time 3D SDKs as well, even if the methods of declaring
these values and the exact interaction semantics might differ slightly from
API to API.

324
Chapter 7 Lighting
OpenGL uses a single function set to apply all manner of material prop-
erties, glMaterial*. In order to understand the use of glMaterial*, we shall
examine an example:
GLfloat color[4] = { 1.0f, 0.0f, 0.0f, 1.0f };
glMaterialfv(GL_FRONT, GL_EMISSION, color);
This code illustrates several basic concepts of how OpenGL will handle mate-
rials. OpenGL works on the concept of a single “current” material. All calls
to glMaterial* change the values of the current material. The ﬁrst argument,
GL_FRONT, speciﬁes that the value being set is to be applied to the material
for the front “side” of the surface. OpenGL actually has two current mate-
rials, one for the front side of the triangles that form the surface and one
for the back side. This makes it easy to render a thin, double-sided surface
as a single set of triangles, without separate triangles for the front and back
surfaces.
The second and third arguments take the form of many of the other func-
tions that were introduced for light sources. The second parameter speciﬁes
the property to be set; in this case, the emissive color of the surface (which
will be covered in detail in the following section); and the third parameter
speciﬁes the value to which the property is to be set.
Note that when lighting is enabled, alpha values are handled differently
than they are in the case of static vertex colors. In the lit case, the alpha
value of the surface is the alpha component of one of the surface’s mate-
rial colors (actually, the diffuse material color as will become apparent).
The alpha components of all other material and light colors are ignored.
No other calculations are performed on alpha values during lighting. The
diffuse material color’s alpha value is passed on directly as the “lit” alpha
value, since lighting is considered to have no effect on the inherent opacity of
the surface.
7.6 Categories of Light
7.6.1 Emission
Emission, or emissive light, is the light produced by the surface itself, in
the absence of any light sources. Put simply, it is the color and intensity
with which the object “glows.” Because this is purely a surface-based prop-
erty, only surface materials (not lights) contain emissive colors. The emissive
color of a material is written as ME. One approximation that is made in

7.6 Categories of Light 325
real-time systems is the (sometimes confusing) fact that this “emitted” light
does not illuminate the surfaces of any other objects. In fact, another common
(and perhaps mo re descriptive) term used for emission is self-illumination.
The fact that emissive objects do not illuminate one another avoids the need
for the graphics systems to take other objects into account when computing
the light at a given point.
OpenGL allows the emission color of a surface material to be set using
glMaterialfv and the constant GL_EMISSION. The default value is black (i.e., no
emission), since the vast majority of objects in most scenes do not glow.
GLfloat color[4] = { 1.0f, 0.0f, 0.0f, 1.0f };
glMaterialfv(GL_FRONT, GL_EMISSION, color);
The alpha component of the emission color is ignored.
7.6.2 Ambient
Ambient light is the term used in real-time lighting as an “umbrella” under
which all forms of indirect lighting are grouped and approximated. Indirect
lighting is light that is incident upon a surface not via a direct ray from light to
surface, but rather via some other, more complex path. In the real world, light
can be scattered by particles in the air, and light can “bounce” multiple times
around a scene prior to reaching a given surface. Accounting for these multiple
bounces and random scattering effects is very difﬁcult if not impossible to do
in a real-time rendering system, so most systems use a per-light, per-material
constant for all ambient light.
A light’s ambient color represents the color and intensity of the light from
a given source that is to be scattered through the scene. The ambient material
color represents how much of the overall ambient light the particular surface
reﬂects.
Ambient light has no direction associated with it. However, most lighting
models do attenuate the ambient light from each source based on the light’s
intensity function at the given point, iL. As a result, point and spotlights do not
produce equal amounts of ambient light throughout the scene. This tends to
localize the ambient contribution of point and spotlights spatially and keeps
ambient light from overwhelming a scene. The overall ambient term for a
given light and material is thus
CA = iLLAMA

326
Chapter 7 Lighting
Figure7.6 Sphere lit by ambient light.
where LA is the light’s ambient color, and MA is the material’s ambient color.
Figure 7.6 provides a visual example of a sphere lit by purely ambient light.
Without any ambient lighting, most scenes will require the addition of many
lights to avoid dark areas, leading to decreased performance. Adding some
ambient light allows speciﬁc light sources to be used more artistically, to
highlight parts of the scene that can beneﬁt from the added dimension of
dynamic lighting. However, adding too much ambient light can lead to the
scene looking “ﬂat,” as the ambient lighting dominates the coloring.
In OpenGL both materials and lights have independent ambient colors,
each accessed using GL_AMBIENT. An example that sets each of these (for the
current material and the light at index zero) follows:
GLfloat color[4] = { 0.25f, 0.25f, 0.25f, 1.0f };
glMaterialfv(GL_FRONT, GL_AMBIENT, color);

7.6 Categories of Light 327
GLfloat light[4] = { 0.0f, 0.0f, 0.5f, 1.0f };
glLightfv(GL_LIGHT0, GL_AMBIENT, light);
In addition, OpenGL supports the concept of an overall ambient lighting level.
This ambient light is independent of any speciﬁc light source and represents
the overall ambient lighting in the scene (and is written WA, for world ambi-
ent). It is added to the contribution of the other lights and is set globally
using
GLfloat light[4] = { 0.0f, 0.0f, 0.5f, 1.0f };
glLightModelfv(GL_LIGHT_MODEL_AMBIENT, light);
The alpha component of the ambient color is ignored.
7.6.3 Diffuse
Diffuse lighting, unlike the previously discussed emissive and ambient terms,
represents direct lighting. The diffuse term is dependent on the lighting inci-
dent upon a point on a surface from each single light via the direct path. As
such, diffuse lighting is dependent on material colors, light colors, iL, and the
vectors ˆL and ˆn.
The diffuse lighting term treats the surface as a pure diffuse (or matte)
surface, sometimes called a Lambertian reﬂector. These surfaces have the
property that their luminance is independent of view direction. In other
words, like our earlier approximation terms, emissive and ambient, the dif-
fuse term is not view-dependent. The luminance is dependent on only the
incident illuminance.
The illuminance incident upon a surface is proportional to the luminous
ﬂux incident upon the surface, divided by the surface area over which it is
distributed. In our earlier discussion of illuminance, we assumed (implicitly)
that the surface in question was perpendicular to the light direction. If we
deﬁne an inﬁnitesimally narrow ray of light with direction ˆL to have luminous
ﬂux I and cross-sectional area δa (Figure 7.7), then the illuminance E incident
upon a surface whose normal ˆn = ˆL is
E ∝I
δa
However, if ˆn
̸=
ˆL (i.e., the surface is not perpendicular to the ray
of light), then the conﬁguration is as shown in Figure 7.8. The surface
area intersected by the (now oblique) ray of light is represented by δa′.

328
Chapter 7 Lighting
da
da
n^
L^
Figure 7.7 A shaft of light striking a perpendicular surface.
da
da
n^
90  h
h
L^
Figure 7.8 The same shaft of light at a glancing angle.

7.6 Categories of Light 329
From basic trigonometry and our ﬁgure, we can see that
δa′ =
δa
sin ( π
2 −θ)
=
δa
cos θ
=
δa
ˆL · ˆn
And, we can compute the illuminance E′ as follows:
E′ ∝I
δa′
∝I
 ˆL · ˆn
δa

∝
 I
δa

( ˆL · ˆn)
∝E( ˆL · ˆn)
Note that if we evaluate for the original special case ˆn = ˆL, the result is E′ = E,
as expected. Thus, the reﬂected diffuse luminance is proportional to ( ˆL · ˆn).
Figure 7.9 provides a visual example of a sphere lit by a single light source
that involves only diffuse lighting.
Generally, both the material and the light include diffuse color values (MD
and LD, respectively). The resulting diffuse color for a point on a surface and
a light is then equal to
CD = iLmax(0, ˆL · ˆn)LDMD
Note the max() function that clamps the result to 0. If the light source is behind
the surface (i.e., ˆL · ˆn < 0), then we assume that the back side of the surface
obscures the light (self-shadowing), and no diffuse lighting occurs.
In OpenGL, both materials and lights have independent diffuse colors,
each accessed using GL_DIFFUSE. An example that sets each of these (for the
current material and the light at index zero) is
GLfloat color[4] = { 1.0f, 0.0f, 0.0f, 1.0f };
glMaterialfv(GL_FRONT, GL_DIFFUSE, color);
GLfloat light[4] = { 1.0f, 1.0f, 0.0f, 1.0f };
glLightfv(GL_LIGHT0, GL_DIFFUSE, light);

330
Chapter 7 Lighting
Figure 7.9 Sphere lit by diffuse light.
The alpha component of the diffuse material color deﬁnes the alpha value for
the surface.
7.6.4 Specular
A perfectly smooth mirror reﬂects all of the light from a given direction
ˆL out along a single direction, the reﬂection direction ˆr. While few sur-
faces approach completely mirrorlike behavior, most surfaces have at least
some mirrorlike component to their lighting behavior. As a surface becomes
rougher (at a microscopic scale), it no longer reﬂects all light from ˆL out along
a single direction ˆr, but rather in a distribution of directions centered about
ˆr. This tight (but smoothly attenuating) distribution around ˆr is often called a
specular highlight, and is often seen in the real world. A classic example is the
bright, white “highlight” reﬂections seen on smooth, rounded plastic objects.
The specular component of real-time lighting is an entirely empirical approx-
imation of this reﬂection distribution, speciﬁcally designed to generate these
highlights.

7.6 Categories of Light 331
lnn^
r^
n^
lpp
lpp
L^
Figure 7.10 The relationship between the surface normal, light direction, and the
reﬂection vector.
Because specular reﬂection represents mirrorlike behavior, the intensity
of the term is dependent on the relative directions of the light ( ˆL), the surface
normal (ˆn), and the viewer (ˆv). Prior to discussing the specular term itself,
we must introduce the concept of the light reﬂection vector, ˆr. Computing
the reﬂection of a light vector ˆL about a plane normal ˆn involves negating
the component of ˆL that is perpendicular to ˆn. We do this by represent-
ing ˆL as the weighted sum of ˆn and a unit vector ˆp that is perpendicular
to ˆn (but in the plane deﬁned by ˆn and ˆL), as follows and as depicted in
Figure 7.10.
ˆL = ln ˆn + lp ˆp
The reﬂection of ˆL about ˆn is then
ˆr = ln ˆn −lp ˆp
We know that the component of ˆL in the direction of ˆn (ln) is the projection
of ˆL onto ˆn, or
ln = ˆL · ˆn
Now we can compute lp ˆp by substitution of our value for ln:
ˆL = ln ˆn + lp ˆp
ˆL = ( ˆL · ˆn)ˆn + lp ˆp
lp ˆp = ˆL −( ˆL · ˆn)ˆn

332
Chapter 7 Lighting
So, the reﬂection vector ˆr equals
ˆr = ln ˆn −lp ˆp
= ( ˆL · ˆn)ˆn −lp ˆp
= ( ˆL · ˆn)ˆn −( ˆL −( ˆL · ˆn)ˆn)
= ( ˆL · ˆn)ˆn −ˆL + ( ˆL · ˆn)ˆn
= 2( ˆL · ˆn)ˆn −ˆL
The specular term itself is designed speciﬁcally to create an intensity distri-
bution that reaches its maximum when the view vector ˆv is equal to ˆr, that
is, when the viewer is looking directly at the reﬂection of the light vector.
The intensity distribution falls off toward zero rapidly as the angle between the
two vectors increases, with a “shininess” control that adjusts how rapidly the
intensity attenuates. The term is based on the following formula:
(ˆr · ˆv)mshine = (cos θ)mshine
where θ is the angle between ˆr and ˆv. The shininess factor mshine controls the
size of the highlight; a smaller value of mshine leads to a larger, more diffuse
highlight, which makes the surface appear more dull and matte; whereas, a
larger value of mshine leads to a smaller, more intense highlight, which makes
the surface appear shiny. This shininess factor is considered a property of the
surface material and represents how smooth the surface appears. Generally,
the complete specular term includes specular colors deﬁned on both the light
and material (LS and MS), which allow the highlights to be tinted a given color.
The specular light color is often set to the diffuse color of the light, since
a colored light generally creates a colored highlight. In practice, however,
the specular color of the material is more ﬂexible. Plastic and clear-coated
surfaces (such as those covered with clear varnish), whatever their diffuse
color, tend to have white highlights, while metallic surfaces tend to have
tinted highlights. For a more detailed discussion of this and several other
(more advanced) specular reﬂection methods, see Chapter 16 of [36]. A visual
example of a sphere lit from a single light source providing only specular light
is shown in Figure 7.11. The complete specular lighting term is
CS =

iLmax(0, (ˆr · ˆv))mshineLSMS,
if ˆL · ˆn > 0
0,
otherwise
Note that, as with the diffuse term, a self-shadowing conditional is applied
( ˆL · ˆn > 0). However, unlike the diffuse case, we must make this term explicit,
as the specular term is not directly dependent upon ˆL · ˆn. Simply clamping the

7.6 Categories of Light 333
Figure 7.11 Sphere lit by specular light.
specular term to be greater than 0 could allow objects whose normals point
away from the light to generate highlights, which is not correct. In other
words, it is possible for ˆr · ˆv > 0, even if ˆL · ˆn < 0.
In OpenGL, both materials and lights have specular components but only
materials have specular exponents, as the specular exponent represents the
shininess of a particular surface:
GLfloat color[4] = { 1.0f, 0.0f, 0.0f, 1.0f };
glMaterialfv(GL_FRONT, GL_SPECULAR, color);
glMaterialf(GL_FRONT, GL_SHININESS, 10.0f);
GLfloat light[4] = { 0.0f, 1.0f, 0.0f, 1.0f };
glLightfv(GL_LIGHT0, GL_SPECULAR, light);
The alpha component of the specular color is ignored.

334
Chapter 7 Lighting
v^
h^
Surface orientation resulting
in maximum specular
reflection (defined by h)
L^
Figure 7.12 The specular halfway vector.
Inﬁnite Viewer Approximation
One of the primary reasons that the specular term is the most expensive com-
ponent of lighting is the fact that a normalized view and reﬂection vector
must be computed for each sample, requiring at least one normalization per
sample, per light. However, there is another method of approximating spec-
ular reﬂection that can avoid this expense in common cases. This method is
based on a slightly different approximation to the specular highlight geome-
try, along with an assumption that the viewer is “at inﬁnity” (at least for the
purposes of specular lighting).
Rather than computing ˆr directly, the OpenGL method uses what is known
as a “halfway” vector. The halfway vector is the vector that is the normalized
sum of ˆL and ˆv:
ˆh =
ˆL + ˆv
| ˆL + ˆv|
The resulting vector bisects the angle between ˆL and ˆv. This halfway vector
is equivalent to the surface normal ˆn that would generate ˆr such that ˆr = ˆv.
In other words, given ﬁxed light and view directions, ˆh is the surface nor-
mal that would produce the maximum specular intensity. So, the highlight is
brightest when ˆn = ˆh. Figure 7.12 is a visual representation of the conﬁgura-
tion, including the surface orientation of maximum specular reﬂection. The
resulting (modiﬁed) specular term is
CS =

iLmax(0, (ˆh · ˆn))mshineLSMS,
if ˆL · ˆn > 0
0,
otherwise

7.7 Combined Lighting Equation 335
By itself, this new method of computing the specular highlight would
not appear to be any better than the reﬂection vector system. However, if
we assume that the viewer is at inﬁnity, then we can use a constant view
vector for all vertices, generally the camera’s view direction. This is analo-
gous to the difference between a point light and a directional (inﬁnite) light.
Thanks to the fact that the halfway vector is based only on the view vector
and the light vector, the inﬁnite viewer assumption can reap great beneﬁts
when used with directional lights. Note that in this case, both ˆL and ˆv are
constant across all samples, meaning that the halfway vector ˆh is also con-
stant. Used together, these facts mean that specular lighting can be computed
very quickly if directional lights are used exclusively and the inﬁnite viewer
assumption is enabled.
By default, OpenGL uses this inﬁnite viewpoint for lighting. While this
is technically “less accurate” than using a noninﬁnite viewpoint (real-world
specular highlights move as the viewer translates), the performance beneﬁts
are signiﬁcant, making it a worthwhile default. To cause OpenGL to use the
more accurate, “local” viewpoint when computing lighting, call
glLightModeli(GL_LIGHT_MODEL_LOCAL_VIEWER, GL_TRUE);
7.7 Combined Lighting Equation
Having covered materials, lighting components, and light sources, we now
have enough information to evaluate our full lighting model at a given point.
In order to do so, we must take all of the above terms into account, including
■The material properties of the object
■The emissive, ambient, diffuse, and specular components of lighting
■The contributions of multiple, independent lights
For a visual example of all of these components combined, see the lit sphere
in Figure 7.13.
When lighting a given point, the contributions from each component of
each active light L are summed to form the ﬁnal lighting equation, which is
detailed as follows:
CV = Emissive + World Ambient
+
lights

L

Per-light Ambient + Per-light Diffuse + Per-light Specular


336
Chapter 7 Lighting
Figure 7.13 Sphere lit by a combination of ambient, diffuse, and specular lighting.
= ME + MAWA +
lights

L
(CA + CD + CS)
AV = MAlpha
(7.1)
where the results are
1. CV , the computed, lit RGB color of the sample
2. AV , the alpha component of the RGBA color of the sample
The intermediate, per-light values used to compute the results are
3. CA, the per-light ambient term, which is equal to
CA = iLMALA

7.7 Combined Lighting Equation 337
4. CD, the per-light diffuse term, which is equal to
CD = iLMDLD(max(0, ˆLL · ˆn))
5. CS, the per-light specular term, which is equal to
CS = iLMSLS

max(0, (ˆhL · ˆn))mshine,
if ˆLL · ˆn > 0
0,
otherwise
Finally, these intermediate values are computed from the following source
data items. Not all of these source values appear in the equations we’ve covered
in this section since some are used indirectly to compute iL for a given type
of light, as detailed for each category of light source:
1. MAlpha, the material’s alpha value (generally, the alpha component of
the diffuse material color)
2. ME, the emissive color of the material
3. MA, the ambient color/reﬂectance of the material
4. MD, the diffuse color/reﬂectance of the material
5. MS, the specular color/reﬂectance of the material
6. mshine, the specular shininess of the material
7. LA, the ambient color of the light L
8. LD, the diffuse color of the light L
9. LS, the specular color of the light L
10. ˆhL, the specular halfway vector for the light L and the current sample
11. ˆLL, the light direction vector for the light L and the current sample
12. WA, the overall world ambient light color
13. iL, the light intensity value of the light L, which is dependent upon the
type of light and the values that follow
14. kc, kl, and kq, the constant, linear, and quadratic distance attenuation
factors of the light L
15. θ, the spotlight cone angle of the light L
16. PL, the position of the light L
17. ˆn, the surface normal at the sample
18. PV , the position of the sample

338
Chapter 7 Lighting
The combined lighting equation 7.1 brings together all of the properties
discussed in the previous sections. Clearly, many different values and com-
ponents must come together to light even a single sample. This fact can make
lighting complicated and difﬁcult to use at ﬁrst. A completely black rendered
image can be the result of many possible errors. However, an understanding
of the lighting pipeline can make it much easier to determine which features
to disable or change in order to debug lighting issues.
7.8 Lighting and Shading
Thus far, our lighting discussion has focused on computing color at a generic
point on a surface, given a location, a surface normal, and a surface material.
Another aspect of lighting that is just as important as the basic lighting equa-
tion is the question of when and how to evaluate that equation to completely
light a surface and how to assign colors to points on the surface for which
the lighting equation is not speciﬁcally evaluated. This aspect of dynamic
lighting will involve the use of the shading methods discussed in Chapter 6.
However, when selecting between these shading methods, we must take into
account the fact that the colors we will supply to our shading functions
represent something far more speciﬁc than the generic colors discussed in
Chapter 6 — namely, dynamic lighting.
Ultimately, a triangle in view is drawn to the screen by coloring the
screen pixels covered by that triangle (as will be discussed in more detail in
Chapter 8). Any lighting system must be teamed with a shading method that
can quickly compute colors for each and every pixel covered by the triangle.
The sheer number of pixels that must be drawn per frame (e.g., a sphere that
covers 50 percent of a 1024 × 768 screen will require the shading system to
compute colors for ≈400, 000 pixels, regardless of the tessellation) requires
that many low- to mid-end graphics systems forgo computing the lighting
equation for each pixel in favor of another method. Next, we will discuss
some of the more popular methods. Some of these methods will be familiar,
as they are simply the shading methods discussed in the previous chapter,
using results of the lighting equation as source colors.
7.8.1 Flat-shaded Lighting
The simplest shading method applied to lighting is per-triangle, ﬂat shad-
ing. This method involves evaluating the lighting equation once per triangle
and using the resulting color as CF , the constant triangle color. The color is
assigned to every pixel covered by the triangle. This is the highest-performance

7.8 Lighting and Shading 339
Figure 7.14 Flat-shaded lighting.
lighting/shading combination, owing to two facts: the expensive lighting equa-
tion need only be evaluated once per triangle, and a single color can be used
for all pixels in the triangle. Figure 7.14 shows an example of a sphere lit and
shaded using per-triangle lighting and ﬂat shading.
To evaluate the lighting equation for a triangle, we need a sample loca-
tion and surface normal. The surface normal used is generally the face normal
(discussed in Chapter 1), as it accurately represents the plane of the triangle.
However, the issue of sample position is more problematic. No single point
can accurately represent the lighting across an entire triangle (except in spe-
cial cases); for example, in the presence of a point light, different points on the
triangle should be attenuated differently, according to their distance from the
light. While the centroid of the triangle is a reasonable choice, the fact that it
must be computed speciﬁcally for lighting makes it less desirable. For reasons
of efﬁciency (and often to match with the graphics system, as will be discussed
presently for OpenGL), the most common sample point for ﬂat shading is one
of the triangle vertices, as the vertices already exist in the desired space. This
can lead to artifacts, since a triangle’s vertices are (by deﬁnition) at the edge
of the area of the triangle.

340
Chapter 7 Lighting
Flat-shaded Lighting in OpenGL
As with per-triangle coloring, in OpenGL per-triangle lighting is actually done
quite simply. The ﬁnal, lit color of one of the triangle’s vertices is used directly
as the color of the entire triangle. The OpenGL speciﬁcation details which
vertex is used in each mode, but for GL_TRIANGLES the vertex used is the last
(third) vertex in the triangle. As a result, OpenGL does not have a notion
of a polygon normal for lighting. The face normal must be associated with
the ﬁnal vertex that is used to generate the triangle. This can be problem-
atic in the case of indexed geometry, where some vertices may have to be
used as the third vertex for more than one triangle (it is very easy to gener-
ate indexed geometry that has more triangles than vertices). In such cases, it
may be necessary to duplicate vertices in order to be able to specify triangle-
speciﬁc normals. Recall that ﬂat shading is enabled in OpenGL with the
function call
glShadeModel(GL_FLAT);
7.8.2 Per-Vertex Lighting
Flat-shaded lighting suffers from the basic ﬂaws and limitations of ﬂat shading
itself; the faceted appearance of the resulting geometry tends to highlight
rather than hide the piecewise triangular approximation. In the presence of
specular lighting, the tessellation is even more pronounced, causing entire
triangles to be lit with bright highlights. With moving lights or geometry, this
can cause gemstonelike “ﬂashing” of the facets. For smooth surfaces such as
the sphere in Figure 7.14 this faceting is often unacceptable.
The next logical step is to use vertex lighting with Gouraud shading. The
lighting equation is evaluated per-vertex, and the results are interpolated
across the triangles using Gouraud shading. Generating a single lit color that
is shared by all co-located vertices leads to smooth lighting across surface
boundaries. Even if co-located vertices are not shared (i.e., each triangle has
its own copy of its three vertices), simply setting the normals to be the same
in all copies of a vertex will cause all copies to be lit the same way. Figure 7.15
shows an example of a sphere lit and shaded using per-vertex lighting and
Gouraud shading.
Per-vertex lighting only requires evaluating the lighting equation once
per vertex. In the presence of well-optimized vertex sharing (where there are
more triangles than vertices), per-vertex lighting requires fewer lighting equa-
tion evaluations than does ﬂat shading. However, the shading interpolation
method used (Gouraud) is more expensive computationally, since it must
interpolate between the three vertex colors on a per-pixel basis.

7.8 Lighting and Shading 341
Figure 7.15 Gouraud-shaded lighting.
Per-vertex lighting is the standard in OpenGL, and Gouraud shading of
the results is enabled via
glShadeModel(GL_SMOOTH);
Gouraud-shaded lighting is a vertex-centric method — the surface posi-
tions and normals are used only at the vertices, with the triangles serving
only as areas for interpolation. This shift to vertices as localized surface rep-
resentations means that we will need surface normals at each vertex. The next
section will discuss several methods for generating these vertex normals.
Generating Vertex Normals
In order to generate smooth lighting that represents a surface at each vertex,
we need to generate a single normal that represents the surface at each vertex,
not at each triangle. There are several common methods used to generate these
per-vertex surface normals at content creation time or at loadtime, depending
upon the source of the geometry data.

342
Chapter 7 Lighting
When possible, the best way to generate smooth normals during the cre-
ation of a tessellation is to use analytically computed normals based on the
surface being approximated by triangles. For example, if the set of triangles
represent a sphere centered at the origin, then for any vertex at location PV ,
the surface normal is simply
ˆn = PV −0
|PV −0|
This is the vertex position, treated as a vector (thus the subtraction of
the zero point) and normalized. Analytical normals can create very realis-
tic impressions of the original surface, as the surface normals are pivotal to
the overall lighting impression. Examples of surfaces for which analytical
normals are available include most of the types of surface representations
mentioned earlier in this chapter; implicit surfaces and parametric surface
representations generally include analytically deﬁned normal vectors at every
point in their domain.
In the more common case the mesh of triangles exists by itself, with no
available method of computing exact surface normals for the surface being
approximated. In this case the normals must be generated from the trian-
gles themselves. While this is unlikely to produce optimal results in all cases,
simple methods can generate normals that tend to create the impression of a
smooth surface and remove the appearance of faceting.
One of the most popular algorithms for generating normals from triangles
takes the mean of all of the face normals for the triangles that use the given
vertex. Figure 7.16 demonstrates a two-dimensional example of averaging
triangle normal vectors. The algorithm may be pseudo-coded as follows:
for each vertex V
{
vector V.N = (0,0,0);
for each triangle T that uses V
{
vector F = TriangleNormal(T);
V.N += F;
}
V.N.Normalize();
}
Basically, the algorithm sums the normals of all of the faces that are
incident upon the current vertex and then renormalizes the resulting summed
vector. Since this algorithm is (in a sense) a mean-based algorithm, it can

7.8 Lighting and Shading 343
Triangles (side view)
True triangle normals
Averaged vertex normals
Figure7.16 Averaging triangle normal vectors.
be affected by tessellation. Triangles are not weighted by area or other such
factors, meaning that the face normal of each triangle incident upon the vertex
has an equal “vote” in the makeup of the ﬁnal vertex normal. While the method
is far from perfect, any vertex normal generated from triangles will by its
nature be an approximation. In most cases the averaging algorithm generates
convincing normals. Note that in cases where there is no fast (i.e., constant-
time) method of retrieving the set of triangles that use a given vertex (e.g., if
only the OpenGL-style index lists are available), the algorithm may be turned
“inside out” as follows:
for each vertex V
{
V.N = (0,0,0);
}
for each triangle T
{
// V1, V2, V3 are the vertices used by the triangle
vector F = TriangleNormal(T);
V1.N += F;
V2.N += F;
V3.N += F;
}

344
Chapter 7 Lighting
for each vertex V
{
V.N.Normalize();
}
Basically, this version of the algorithm uses the vertex normals as “accumu-
lators,” looping over the triangles, adding each triangle’s face normal to the
vertex normals of the three vertices in that triangle. Finally, having accumu-
lated the input from all triangles, the algorithm goes back and normalizes
each ﬁnal vertex normal. Both algorithms will result in the same vertex
normals, but each works well with different vertex/triangle data structure
organizations.
Sharp Edges
Demo
Edges
As with Gouraud shading based on ﬁxed colors, Gouraud-shaded lighting gen-
erates smooth triangle boundaries by default. In order to represent a sharp
edge, vertices along a physical crease in the geometry must be duplicated so
that the vertices can represent the surface normals on either side of the crease.
By having different surface normals in copies of co-located vertices, the trian-
gles on either side of an edge can be lit according to the correct local surface
orientation. For example, at each vertex of a cube, there will be three vertices,
each one with a normal of a different face orientation as we see in Figure 7.17.
7.8.3 Per-Pixel Lighting (Phong Shading)
There are signiﬁcant limitations to Gouraud shading. Speciﬁcally, the fact
that the lighting equation is evaluated only at the vertices can lead to arti-
facts. Even a cursory evaluation of the lighting equation shows that it is highly
nonlinear. However, Gouraud shading interpolates linearly across polygons.
Any nonlinearities in the lighting across the interior of the triangle will be
lost completely. These artifacts are not as noticeable with diffuse and ambi-
ent lighting as they are with specular lighting, because diffuse and ambient
lighting are closer to linear functions than is specular lighting (owing at least
partially to the nonlinearity of the specular exponent term and to the rapid
changes in the specular halfway vector ˆh with changes in viewer location).
For example, let us examine the specular lighting term for the surface
shown in Figure 7.18. We draw the two-dimensional case, in which the tri-
angle is represented by a line segment. In this situation the vertex normals
all point outward from the center of the triangle, meaning that the triangle
is representing a somewhat domed surface. The point light source and the

7.8 Lighting and Shading 345
V1
V2
V3
Figure 7.17 One corner of a faceted cube.
viewer are located at the same position in space, meaning that the view vector
ˆv, the light vector ˆL, and the resulting halfway vector ˆh will all be equal for all
points in space. The light and viewer are directly above the center of the tri-
angle. Because of this, the specular components computed at the two vertices
will be quite dark (note the specular halfway vectors shown in Figure 7.18 are
almost perpendicular to the normals at the vertices). Linearly interpolating
between these two dark specular vertex colors will result in a polygon that is
relatively dark.
However, if we look at the geometry that is being approximated by these
normals (a domed surface as in Figure 7.19), we can see that in this con-
ﬁguration the interpolated normal at the center of the triangle would point
straight up at the viewer and light. If we were to evaluate the lighting equa-
tion at a point near the center of the triangle in this case, we would ﬁnd an
extremely bright specular highlight there. The specular lighting across the
surface of this triangle is highly nonlinear, and the maximum is internal to
the triangle. Even more problematic is the case in which the surface is mov-
ing over time. In rendered images where the highlight happens to line up
with a vertex, there will be a bright, linearly interpolated highlight at the
vertex. However, as the surface moves so that the highlight falls between

346
Chapter 7 Lighting
Gouraud shading
of single triangle
Correct lighting of
smooth surface
Approximated
(smooth) surface
Viewer
Point light
n • h ≈ 0
^
^
n • h ≈ 0
^
^
n^
n^
L = v = h
^
^
^
L = v = h
^
^
^
Figure7.18 Gouraud shading can miss specular highlights.
vertices, the highlight will disappear completely. This is a very fundamental
problem with approximating a complex function with a piecewise-linear rep-
resentation. The accuracy of the result is dependent upon the number of linear
segments used to approximate the function. In our case this is equivalent to
the density of the tessellation.
If we want to increase the accuracy of lighting on a general Gouraud-
shaded surface, we must subdivide the surface to increase the density of
vertices (and thus lighting samples). However, this is an expensive process,
and we may not know a priori which sections of the surface will require sig-
niﬁcant tessellation. Dependent upon the particular view at runtime, almost
any tessellation may be either overly dense or too coarse. In order to create a
more general, high-quality lighting method, we must ﬁnd another way around
this problem.
So far, the methods we have discussed for lighting have all evaluated
the lighting equation once per basic geometric object, such as per-vertex or
per-triangle. Phong shading (named after its inventor, Phong Bui-Tuong [89])

7.8 Lighting and Shading 347
Phong shading of
single triangle
Correct lighting of
smooth surface
n • h ≈ 0
^
^
n • h ≈ 0
^
^
n^
n^
Interpolated
vertex normal
n • h ≈ 1
^
^
v = h = L = n
^
^
^
^
^
Viewer
Point light
Figure7.19 Phong shading of the same conﬁguration.
works by evaluating the lighting equation once for each pixel covered by the
triangle. The difference between Gouraud and Phong shading may be seen in
Figures 7.18 and 7.19. For each sample across the surface of a triangle, the
vertex normals, positions, reﬂection, and view vectors are interpolated, and
the interpolated values are used to evaluate the lighting equation. However,
since triangles tend to cover more than 1–3 pixels, such a lighting method will
result in far more lighting computations per triangle than do per-triangle or
per-vertex methods.
There are several issues that make Phong shading expensive to implement
in a high-performance, real-time system. The ﬁrst of these is the actual normal
vector interpolation, since basic barycentric interpolation of the three vertex
normals will almost never result in a normalized vector. As a result, the normal
vector will have to be interpolated and renormalized per sample, which is
much more frequent than per vertex.
Per sample, once the interpolated normal is computed and renormalized,
the full lighting equation must be evaluated. Not only is this operation expen-
sive, it is not a ﬁxed amount of computation. The complexity of the lighting
equation is dependent on the number of lights and numerous graphics engine
settings. This resulted in Phong shading being rather unpopular in game-
centric consumer 3D hardware prior to the advent of pixel and vertex shaders.
There is no standard method in OpenGL to enable Phong shading, although

348
Chapter 7 Lighting
an implementation of OpenGL could implement it and expose it via an
extension. It should be noted that with the availability of pixel shader
hardware (as discussed in Chapter 6), it is possible to implement per-pixel
lighting methods, including Phong shading and methods based upon per-pixel
interpolation of normals and lighting evaluation.
7.9 Merging Textures and Lighting
Demo
Textures
Of the methods we have discussed for coloring geometry, the two most pow-
erful are texturing and dynamic lighting. However, they each have drawbacks
when used by themselves. Texturing is normally a static method and looks
ﬂat and painted when used by itself in a dynamic scene. Lighting can gener-
ate very dynamic effects, but it is limited to face- or vertex-level detail unless
special pixel shaders are used. It is only natural that graphics systems would
want to use the results of both techniques together on a single surface. This is
possible, but the issue of how to combine the two methods must be addressed.
Each of the two methods is capable of generating a color per-sample. With
texturing, this is done directly via texture sampling; with lighting, it is done
by interpolating values computed at each vertex (generally using one of the
shading methods discussed in Chapter 6). These two colors must be combined
in a way that makes visual sense. The method of combining textures with face
or vertex colors is called the texture application mode. The most common way
of combining textures and vertex colors is via multiplication, also known as
modulate mode texturing. In modulate texture combination, the texture color
at the given sample CT and the ﬁnal (generally lit) interpolated vertex color
CV are combined by per-component multiplication:
C = CT CV
A = AT AV
The visual effect here is that the vertex colors darken the texture (or vice
versa). As a result, texture images designed to be used with modulate mode
texture combination are normally painted as if they were fully lit. The vertex
colors, representing the lighting in the scene, darken these fully lit textures
to make them look more realistic in the given environment. As Figure 7.20
demonstrates, the result of modulation can be very convincing, even though
the lighting is rather simple and the textures are static paintings. In the pres-
ence of moving or otherwise animated lights, the result can be even more
immersive, as the human perceptual system is very reliant upon lighting cues
in the real world.

7.9 Merging Textures and Lighting 349
Scene with pure vertex lighting
Scene with pure texturing
Same scene with lighting and
texturing combined
Figure7.20 Textures and lighting combined via modulation.
By default, OpenGL uses modulate mode for combining textures and
vertex colors. However, it does support other modes via
glTexEnvi(GL_TEXTURE_ENV, GL_TEXTURE_ENV_MODE, …);
including GL_REPLACE (which ignores vertex colors) and GL_DECAL (an alpha-
blended mode that applies the texture as a transparent “decal” to the surface),
among others. You may wish to refer to the OpenGL Programming Manual
[83] for details.
7.9.1 Specular Lighting and Textures
If the full lighting equation 7.1 is combined with the texture via multiplication,
then lighting can only darken the texture, since lit vertex colors CV are clamped
to the range [0, 1]. While this looks correct for diffuse or matte objects, for
shiny objects with bright specular highlights, it can look very dull. It is often

350
Chapter 7 Lighting
useful to have the specular highlights “wash out” the texture. We cannot
simply add the full set of lighting because the texture will almost always wash
out and can never get darker. To be able to see the full range of effects requires
that the diffuse colors darken the texture while the specular components of
color add highlights. This is only possible if we split the lighting components.
OpenGL includes a mode that allows this. The mode is enabled using
glLightModeli(GL_LIGHT_MODEL_COLOR_CONTROL, GL_SEPARATE_SPECULAR_COLOR);
and it works by “splitting” the results of the lighting model equation 7.1 into
two pieces. The ﬁrst piece, CD, contains the emissive, ambient, and diffuse
terms of the color summation. The second piece, CS, contains the specular
terms. The two colors are combined with the texture color as follows:
C = CT CD + CS
Because the specular term is added after the texture is multiplied, this
mode (sometimes called modulate with late add) causes the diffuse terms to
attenuate the texture color, while the specular terms wash out the result.
The differences between the separate and combined specular modes can
be very striking as Figure 7.21 makes clear. Unfortunately, the default
mode in OpenGL is to disable this feature and use combined diffuse and
Specular vertex color added to
diffuse vertex color, then modulated
with the texture
Diffuse vertex color modulated with
the texture, then specular vertex
color added
Figure7.21 Combining textures and lighting.

7.10 Lighting and Programmable Shaders 351
specular colors. Once separate specular colors is enabled, it can be disabled
with
glLightModeli(GL_LIGHT_MODEL_COLOR_CONTROL, GL_SINGLE_COLOR);
However, this “late add” effect must be supported in the rasterizer level,
as it requires two colors to be interpolated per-pixel rather than one (i.e.,
the specular and the combined ambient-diffuse-emissive). Some graphics
hardware emulates this effect using a simpler trick: allowing only a single
component (i.e., white) specular value CS that is applied as a late add. In fact,
hardware implementing this trick often uses the vertex color’s alpha channel
to hold this specular value, meaning that the feature is mutually exclusive
with respect to per-vertex alpha blending. This limitation has caused the pop-
ularity of the alpha-channel specular trick to wane in current consumer 3D
hardware.
7.10 Lighting and Programmable Shaders
Today, procedural shading using vertex and pixel shaders is rapidly gaining
popularity, requiring application developers (in many cases) to leave behind
existing lighting pipelines, such as those supplied in an OpenGL implemen-
tation, and write their own. However, while the exact methods of enabling,
disabling, and controlling lighting differ between a “ﬁxed-function” lighting
pipeline and hard-coded, shader-based lighting pipelines, all of the concepts
and formulas given in this chapter may be used in the creation of lighting-
based shaders, as well. In fact, to effectively use shaders, a developer must
have a true understanding of the concepts behind dynamic lighting, in order
to know which parts of these equations they must add as code to their shaders
and which they can ignore.
Even with the growing power of vertex and pixel shader hardware, devel-
opers must be able to actively trade off parts of the lighting pipeline if they
are to ﬁt all of their desired effects into their “performance budget.” A full
understanding of the components of the lighting pipeline, as well as the way
they ﬁt together into the overall lighting equation, is an important part of
this challenge. Interested readers should investigate any of the multitude of
shader tutorials available on the Internet at 3D hardware developers’ sites, as
well as in books such as [33].
7.11 Chapter Summary
In this chapter we have discussed the basics of dynamic lighting, both in terms
of geometric concepts and implementation in OpenGL’s standard pipeline.

352
Chapter 7 Lighting
Per-vertex (and in some cases today, per-pixel) lighting is a very powerful
addition to any 3D application. Correct use of lighting can create compelling
3D environments at limited computational expense. As we have discussed,
judicious use of lighting is important in order to maximize visual impact
while minimizing additional computation.
For further information, there are numerous paths available to the inter-
ested reader. More and more, developers are leaving behind the inﬂexible
lighting pipelines that exist in DirectX and OpenGL and are writing their own
via vertex and pixel shaders. The growing wealth of shader resources includes
web sites ([6], [82]) and even book series [33]. Many of these new shaders
are based on far more detailed and complex lighting models, such as those
presented in computer graphics conference papers and journal articles like
those of ACM SIGGRAPH or in books such as [113].

Chapter8
Rasterization
8.1 Introduction
The ﬁnal stage in the rendering pipeline is called rasterization. Rasterization
is the operation that takes screen-space geometry, a shading method such
as those described in the previous chapters, and the inputs to those shading
methods and actually draws the geometry to the low-level 2D display device.
Once again, we will focus on drawing sets of triangles, as these are the most
common primitive in 3D graphics systems. In fact, for much of this chapter,
we will focus on drawing an individual triangle. For almost all modern display
devices, this low-level “drawing” operation involves assigning color values to
each and every dot, or pixel, on the display device.
At the conceptual level, the entire topic of rasterization is simply an
“implementation detail.” Rasterization is required because the display devices
we use today are based on a dense rectangular grid of light-emitting elements,
or pixels (a short version of “picture elements”), each of whose colors and
intensities are individually adjustable in every frame.
Earlier displays (used prior to the mid-1970s) were not based on these
grids of pixels, but were instead capable of drawing only lines or curves
between points on the screen. Unlike the discrete grid of addressable points
on a raster display, the entire surface of a so-called vector display screen is
addressable continuously. The screen-space positions of each line’s endpoints
were fed to the display system, and it drew the line by directly tracing the
path between the points onto the screen. These vector displays were very
much like the screens on an engineer’s oscilloscope (in fact, many of the early
ones were oscilloscopes). An analogous, more modern example is the popular
353

354
Chapter 8 Rasterization
“laser show” seen at planetariums and live concert venues. Figure 8.1 is a
basic drawing of how such vector displays worked.
These vector displays required no rasterization, as lines and curves could
be drawn by directly tracing them onto the display. However, while vector
displays could render perfectly smooth and sharp lines between any pair of
vertices (and thus the outlines of objects), they were also limited to drawing
wireframe geometry. Furthermore, they could not generally “ﬁll” areas of the
screen with light and were (for the most part) unable to display more than
grayscale light or a few selected colors. Basically, they were not capable of
drawing scenes with any photorealism. Examples of common vector displays
include some early video games, speciﬁcally Asteroids, Tempest, and Battlezone
(all by Atari, the latter two including rudimentary color). Another (somewhat
different) example of a vectorlike display is the pen-plotter, which draws by
moving a set of colored pens across the surface of a sheet of paper.
The limitations of vector displays led to a move in the mid-1970s toward
using televisionlike raster displays, with their accompanying grids of indi-
vidually colored pixels. Decades of television images (both monochrome and
color) had proven that raster displays were very ﬂexible and could support
areas of color, complex images, and a full range of realistic color. However,
raster displays required that the images displayed on them be discretized into
a rectangular grid of color samples for each image. In order to achieve this,
a computer graphics system must convert the projected, colored geometry
representations into the required grid of colors. Moreover, in order to render
Chemical-covered screen surface
glows when hit by electron beam
Line from (X1,Y1) to (X2,Y2) is
traced directly by electron “beam”
Figure8.1 Vector display hardware.

8.2 Displays and Framebuffers 355
real-time animation, the computer graphics system must do so many times
per second. This process of generating a grid of color samples from a projected
scene is called rasterization.
By its very nature, rasterization is time-consuming when compared to
the other stages in the rendering pipeline. Whereas the other stages of the
pipeline generally require per-object, per-triangle, or per-vertex computation,
rasterization inherently requires computation of some sort for every pixel.
As of the early 2000s, displays 1600 pixels wide by 1200 pixels high — resulting
in approximately 2 million pixels on the screen — are popular. Add to this the
fact that rasterization will in practice often require each pixel to be computed
several times, and we come to the realization that the number of pixels that
must be computed generally outpaces the number of triangles in a given frame
by a factor of 10, 20, or more.
In fact, in purely software 3D pipelines, it is not uncommon to see as much
as 80 to 90 percent of rendering time spent in rasterization. This level of com-
putational demand has led to the fact that rasterization was the ﬁrst stage of
the graphics pipeline to be accelerated via purpose-built consumer hardware.
In fact, most 3D computer games began to require some form of 3D hardware
by the early 2000s. This chapter will not detail the methods and code required
to write a software 3D rasterizer, since most game developers no longer have
a need to write them. However, complete software pipelines including ras-
terization are still seen in low-power and low-cost devices, such as handheld
computers and cellular telephones. Also, so-called mass market 3D games,
which are designed to run on older computers will sometimes include a soft-
ware rasterizer, generally rendering the game at a decreased frame rate or
reduced visual quality. While we will not discuss the implementation details
of software rasterizers, many of the high-level concepts required to create
them will be covered in this chapter. For the details on how to write a set
of rasterizers, see Hecker’s excellent series of articles on perspective texture
mapping in Game Developer Magazine [60].
8.2 Displays and Framebuffers
Every piece of display device hardware, whether it be a computer monitor,
a television, or some other such device requires a source of image data. For
computer graphics systems, this source of image data is called a framebuffer
(so called because it is a buffer of data that holds the image information for
a “frame,” or a screen’s worth of image). In basic terms, a framebuffer is a
two-dimensional digital image: a block of memory that contains numerical
values that represent colors at each point on the screen. Each color value
represents the color of the screen at a given point — a picture element, or
pixel. Each pixel has red, green, and blue components. Put together, this

356
Chapter 8 Rasterization
framebuffer represents the image that is to be drawn on the screen. The dis-
play hardware reads these colors from memory every time it needs to update
the image on the screen, generally at least 30 times per second and often 60
or more times per second.
As we shall see, framebuffers often include more than just a color per
pixel. While it is the per-pixel color that is actually used to set the color and
intensity of light emitted by each point on the display, the other per-pixel
values are used internally during the rasterization process. In a sense, these
other values are analogous to per-vertex normals and per-triangle material
colors; while they are never displayed directly, they have a signiﬁcant effect
on how the ﬁnal color is computed.
8.2.1 Framebuffer Memory Organization
Cathode ray tube (CRT) displays, such as televisions and monitors, work
by redrawing the screen from left to right, top to bottom, pixel by pixel
(Figure 8.2). In order to set the color of each pixel, the display must be supplied
First pixel
Pixels drawn to
screen
Invisible
“retrace” to
next scanline
Last pixel
Figure 8.2 CRT redraw pattern.

8.2 Displays and Framebuffers 357
with the correct color as it is needed during the redrawing process. In the case
of televisions, this color information is supplied to the display device directly
from the video source (a cable TV tuner, videotape player, DVD player, etc.)
at the exact moment it is needed. As a result, most televisions do not have
framebuffers — they display the data as it is supplied.
With computer displays, the device supplying the colors as needed is the
framebuffer memory. The display system must read the color of each required
pixel from the framebuffer memory when it is needed. In order to feed this
scanning process with pixel data most efﬁciently, framebuffers are generally
arranged such that the pixels are stored in the order they are scanned out to
the screen. This is a row-major order, meaning that all pixels in each horizon-
tal line on the display are stored together, in order of increasing x coordinate.
These lines of pixels are called scanlines in the framebuffer, as they represent
a single left-to-right pass (or “scan”) across the screen. Each scanline in the
framebuffer is followed by the next lower scanline until the bottom-right cor-
ner of the screen is reached (the memory layout matches the pixel-scanning
sequence, which is shown in Figure 8.2). As mentioned in Chapter 5, the
positive y dimension of the screen is downward to match the scanning order.
This organization of framebuffer memory means that we will (as often
as possible) be drawing a given piece of geometry (normally a triangle) in
scanline-by-scanline order, thus limiting the need to jump around rather ran-
domly in the framebuffer. Such a method can reduce memory bandwidth to
the framebuffer. As we shall see, it can also be an efﬁcient way of computing
per-pixel triangle colors.
8.2.2 Interlacing
Note that most television systems actually use a slightly different scanning
method, known as interlacing. Interlacing draws the even scanlines in top-
to-bottom order, and then goes back and draws the odd scanlines in top-to-
bottom order. Each of these sets of lines is known as a ﬁeld, an even ﬁeld and
an odd ﬁeld per frame. A television redraws one ﬁeld every 60th of a second. In
redrawing this way, a television appears to be refreshing the screen every 60th
of a second when, actually, it is only refreshing half of the lines every 60th of a
second. The entire screen is redrawn only every two passes, or every 30th of a
second. However, the fact that the even and odd sets of lines “cover the screen”
means that, effectively, a low-resolution version of the entire screen is drawn
every 60th of a second. This trick reduces the amount of information that
needs to be transferred from the source per second to draw television images
(originally, this reduced the required radio bandwidth of television signals).
However, interlacing causes thin horizontal lines (which only get redrawn
every 30th of a second since they are only a part of one ﬁeld in each frame) to
ﬂicker. This makes interlacing inappropriate for computer screens (although

358
Chapter 8 Rasterization
early home computers often used owners’ existing interlaced televisions as
their monitors to reduce costs). Because they feed televisions as their display
devices, video game consoles must deal with interlacing, a fact that some
architectures will expose at the framebuffer level.
8.2.3 Multiple Buffers
It is common to have two full-sized blocks (or “buffers”) of framebuffer mem-
ory in a display system. At any given time, one of these copies is being
read by the display hardware to update the display device itself, while the
other is being written by the 3D graphics system. At the instant between the
end of reading the data of one frame out to the display device and starting
to read the next, the two buffers can be “swapped.” This swapping allows
the buffer that was just written (drawn) by the 3D system to be read out
to the display device, while making the previous frame’s buffer available to
the 3D system to prepare as the next frame. Figure 8.3 shows this process
schematically.
This system is known as double buffering because it involves two complete
screen-sized images. At any given time, one buffer (the “front buffer”) is being
read pixel by pixel onto the display device by the 2D display system, while the
other (the “back buffer”) is being written to by the 3D graphics system with
the next frame. Once the next frame is drawn to the back buffer, the next time
the front buffer is ﬁnished being read out to the display (generally during the
moment that the display is resetting itself for the next pass), the two buffers are
swapped. The back buffer becomes the new front buffer and the front buffer
that has just been read onto the screen becomes the new back buffer, ready
to be redrawn with the next frame. Note that in most cases this “swapping”
operation does not involve copying or moving the data in the buffers. It simply
involves swapping the two pointers that point to the front and back buffers.
On most display devices, this is a single instruction in the hardware. As a
result, the swap operation is extremely fast.
Double buffering is a signiﬁcant performance optimization, as it allows
parallelism between the 3D rendering and the 2D display system. While one
buffer (the current front buffer) is being read out to the screen, the 3D hard-
ware can simultaneously write the scene to the other buffer (the current
back buffer). Systems that cannot support fast buffer swapping can still
render and display in parallel, but rather than swapping the two pointers
quickly between frames, the back buffer’s contents must be copied to the
front buffer. This involves moving a lot of data from one memory block to
another, often causing memory bus performance issues. As a result, dou-
ble buffering with buffer swapping is extremely popular in modern display
hardware.

8.2 Displays and Framebuffers 359
Back buffer
Front buffer
Back buffer
Front buffer
3D display system
rasterizes frame N+1
to the back buffer
3D display system
rasterizes frame N+2
to the back buffer
2D display system
scans out the frame N
(the front buffer)
to the screen at the
same time
2D display system
scans out the frame
N+1 (the front buffer)
to the screen at the
same time
End of frame
(buffer swap)
Frame N
Frame N+1
Figure 8.3 Double buffering.

360
Chapter 8 Rasterization
8.3 Conceptual Rasterization Pipeline
Conceptually, there are several stages to even a simple rasterization pipeline.
It should be noted that while these stages tend to exist in rasterization hard-
ware implementations, hardware almost never follows the order (or even the
structure) of the conceptual stages in the list that follows. This simple pipeline
rasterizes a single triangle as follows:
1. Determine the visible pixels covered by the triangle.
2. Compute a color for the triangle at each such pixel.
3. Determine a ﬁnal color for each pixel and write to the framebuffer.
The ﬁrst stage further decomposes into two separate steps: (1) determin-
ing the pixels covered by a triangle and (2) determining which of those pixels
are visible. The rest of this chapter will discuss each of these pipeline stages
in detail.
8.4 Determining the Pixels Contained by a
Triangle
Triangles are convex, no matter how they are projected (in some cases,
triangles may appear as a line or a point, but these are still convex objects).
This is a very useful property, because it means that any triangle intersects
a scanline in at most one contiguous segment. Thus, for any scanline that
intersects a triangle, we can represent the intersection with a single “span,”
a minimum x value and a maximum x value. Thus, the representation of a
triangle during rasterization consists of a set of spans, one per scanline that
the triangle intersects. Furthermore, the convexity of triangles also implies
that the set of scanlines intersected by a triangle is contiguous in y; there is
a minimum and maximum y for a given triangle, which contains all of the
nonempty spans. An example of the set of spans for a triangle is shown in
Figure 8.4. The dark bands overlaid on the triangle represent the pixel spans
that will be used to draw the triangle.
The minimum y pixel coordinate for a triangle ymin is simply the mini-
mum y value of the three triangle vertices. Similarly, the maximum y pixel
coordinate ymax of the triangle is simply the maximum y value of the three
vertices. Thus, a simple min/max computation among the three vertices
deﬁnes the entire range of (ymax −ymin +1) spans that must be generated for a
triangle.

8.4 Determining the Pixels Contained by a 361
Min Y
Max Y
Figure 8.4 A triangle and its raster spans.
The behavior of a graphics system when a triangle vertex or edge falls
exactly on a pixel center is determined by a system-dependent ﬁll convention,
which ensures that if two triangles share a vertex or an edge, only one trian-
gle will draw to the pixel. This is very important, as without a well-deﬁned ﬁll
convention, there may be “holes” (dropouts), or double-drawn pixels on the
shared edges between triangles. Holes along a shared triangle edge allow the
background color to show through what would otherwise be a continuous,
opaque surface, making the surface appear to be “cracked.” Double-drawn
pixels along a shared edge result in more subtle artifacts, normally seen
only when transparency or other forms of blending are used (see section 8.8
on pixel blending later in this chapter). For details on implementing ﬁll
conventions, see Hecker’s Game Developer article series [60].
Generating the spans themselves simply involves intersecting the hori-
zontal scanline with the edges of the triangle. Owing to the convexity of the
triangle, unless the scanline intersects a vertex, that scanline will intersect
exactly two of the edges of the triangle (one to cross from outside the triangle
into it, and one to leave again). These two intersection points will deﬁne the
minimum and maximum x values of the span.

362
Chapter 8 Rasterization
Not all rasterizers generate a table of all spans in a triangle explicitly. In
fact, the most common method is simply to start at the top of the triangle,
computing the extents of the ﬁrst span. Having generated the ﬁrst span, all of
the pixels in that span are completely rasterized. The system then generates
the next span and rasterizes it completely and so on until all spans in the
triangle are rasterized. This has the beneﬁt of not having to store a table
of spans, which could (in theory) require as many span entries as there are
scanlines on the screen. Only the information for the current span need be
stored. In fact, in purpose-built hardware, the next span information can
even be computed by one piece of hardware while another piece of hardware
rasterizes the current span, increasing performance via parallelism.
8.5 Determining Which Pixels are Visible
The overall goal in rendering geometry is to ensure that the ﬁnal, rendered
images convincingly represent the given scene. At the highest level, this means
that objects must appear to be correctly obscured by closer objects and must
not be obscured by more distant objects. This process is known as visible
surface determination, and there are numerous, very different ways of accom-
plishing it. The methods all involve comparing the depth of surfaces at one
level of granularity or another and rendering in such a way that the object of
minimum depth (i.e. the closest object) at a given pixel is the one rendered to
the screen.
8.5.1 Depth Sorting
One of the oldest visible surface determination algorithms predates computer
graphics signiﬁcantly and is called the painter’s algorithm. It works by simulat-
ing a somewhat idealized version of the method used by artists when painting
a scene. The painter starts by painting the background, then moves to paint-
ing closer and closer objects, often painting over parts of more distant objects
that were already painted onto the canvas.
The computer graphics version of the painter’s algorithm works by sorting
all of the triangles in back-to-front (far to near) order, and drawing them in
that order. This method is actually a geometric method rather than a raster-
ization method — triangles can be sorted at any time in the pipeline, as long
as some notion of view direction is known, in order to assign camera-relative
depth to every vertex. Most frequently, depth sorting is done after the view-
space transform (either before or after the perspective division), since this
stage generates camera-relative depth as a side-effect. Each triangle drawn
will overwrite the more distant triangles that have already been drawn. At ﬁrst

8.5 Determining Which Pixels are Visible 363
glance, it would seem that when all triangles are drawn, the entire scene will
be correctly displayed.
However, triangle sorting has several major problems. First, it is poten-
tially very slow. Second, for some scenes, correct ordering of the given
triangles may not be possible. We will brieﬂy discuss some of the issues,
but a more detailed review may be found in [36]. The ﬁrst issue is one of
performance. Sorting all of the triangles in a scene against one another is an
expensive process. While a smart application can often decrease this expense
by sorting large groups of triangles as a unit (say, all of the triangles in a single
object) and then sorting the smaller groups among themselves (often called a
“divide and conquer” method), this is not a fully general optimization.
Also, real-world painters do not (generally) paint a cityscape by ﬁrst
painting all of the people in all of the ofﬁces in an ofﬁce building and then
painting over them with the building’s walls! This would be an immense
waste of time and paint. However, the most basic form of the computer
graphics painter’s algorithm does just that. It draws all of the triangles in
the scene, often drawing over the same section of the screen several times.
This is known as overdraw, and it is a waste of computation that can lead
(even on high-performance 3D hardware) to decreased performance. Avoid-
ing this overdraw can be difﬁcult and scene-dependent (see Zhang [121] for
an example of an overdraw reduction method). An excellent overview of many
depth-complexity reduction methods may be found in Chapter 12 of [27].
The larger issue with the triangle-level painter’s algorithm is that there are
many situations in which it is either difﬁcult to compute a correct ordering of
triangles or may even be impossible. The most important part of any sorting
method (in terms of correctness) is determining the metric by which we sort
the objects. While the concept of depth seems a simple metric, implementing
it for triangles can be very tricky. Most triangles do not have all three vertices
at the same depth — different parts of the triangle are at different depths.
No single depth value can adequately represent an entire triangle. Figure 8.5
is an example of such a case. Each of the triangles (seen in side view) could
(when represented by a single depth value) be considered “in front.” It is only
when they are compared pairwise to each other that we can compute which
one of the pair is in front. Even then, a general method for doing so is complex.
Worse yet, some cases simply cannot be sorted. In Figure 8.6, we see
such a case. Unless we split one of these triangles, no sorting method can
draw these four triangles correctly. In fact, the most popular triangle sorting
method requires that the scene be static and adds a preprocessing step to
split triangles that could cause such sorting issues. The method is known as
a BSP tree and is described in [38]. Basically, it involves creating a binary
“decision tree” (once for a nonmoving scene), which allows the triangles to
be sorted from back-to-front by testing the camera location versus a 3D plane
at each node. The geometry resides in the leaves of the tree, and rendering
is done by traversing the tree, following the left and right child of each node

364
Chapter 8 Rasterization
Figure8.5 Triangles that overlap in depth (side view).
in one order or the other based on the camera location versus plane test.
This method was quite popular in so-called ﬁrst-person shooter games in the
mid-to-late 1990s.
Depth sorting is an input-focused method. It ensures that the geometry
going into the rasterization process is supplied in an order that will generate
a correct image, as long as the order is preserved by the rasterization process.
Depth sorting allows the rasterization system to be “dumb” in terms of visible
surface determination. All it requires is that the rasterizer draw the geometry
in the order supplied. For software rasterizers, this is often a useful feature,
since entirely software-based 3D systems tend to do whatever possible to avoid
putting more work into the (already overburdened) rasterization code.
However, rasterizers were some of the ﬁrst parts of the raster graphics
pipeline to be accelerated with purpose-built hardware, meaning that a

8.5 Determining Which Pixels are Visible 365
Figure8.6 Triangle conﬁguration that cannot be depth-sorted without splitting.
rasterizer-based visible surface determination system could achieve high per-
formance. The depth buffer (also known as a “z-buffer,” which is actually a
special case of depth buffering) is such a rasterizer-based visibility system.
8.5.2 Depth Buffering
Depth buffering is based on the concept that visibility should be output-
focused. In other words, since pixels are the ﬁnal destination of our rendering
pipeline, visibility should be computed on a per-pixel basis. If the ﬁnal color
seen at each pixel is the color of the surface with the minimum depth (of
all surfaces drawn to that pixel), the scene will appear to be drawn correctly.

366
Chapter 8 Rasterization
In other words, of all the surfaces drawn to a pixel, the surface with minimum
depth should “win” the pixel and select that pixel’s color.
Since common rasterization methods tend to render a triangle at a time,
a given pixel may be drawn several times over the course of a frame. If we
wish to avoid sorting the triangles by depth (and we do), then the triangle that
should win a given pixel may not be the last one drawn to that pixel. We must
have some method of storing the depth of the current “nearest triangle” at
each pixel, along with the color of that triangle.
Having stored this information, we can compute a simple test each time
a pixel is drawn. If the new triangle’s depth is closer than the currently
stored depth value at that pixel, then the new triangle writes its color to the
pixel and its depth to the depth value for that pixel. If the new triangle has
greater depth than that of the current triangle coloring the pixel, then the
new triangle’s color and depth are ignored, as it represents a surface that
is behind the closest known triangle at the current pixel. Figure 8.7 repre-
sents the rendering of two triangles to a small depth buffer. Note how the
closer triangle always wins the pixel (the correct result), even if it is drawn
ﬁrst.
Because the method is per-pixel, there is no need to determine some sin-
gle metric of “overall depth” that represents an entire triangle. The depth of
each triangle is computed per-pixel, and this value is used in the comparison.
As a result, the depth buffer automatically handles conﬁgurations that cannot
be correctly displayed using triangle sorting. Geometry may be passed to the
depth buffer in any order. The situation in which this random order can be
problematic is when two surfaces have equal depth at a given pixel. In this
case order will matter, depending on the exact comparison used to order depth
(i.e., < or ≤). However, such circumstances are problematic with almost any
visible surface method.
There are several drawbacks to the depth buffer. One of the drawbacks of
the depth buffering method is implied in the name of the method; it requires a
buffer of depth values, one per pixel. This is a large block of memory, generally
requiring as much memory as (or more than) the framebuffer itself. Also,
just as the framebuffer must be cleared to the background color before each
frame, the depth buffer must be cleared to the “background depth,” which is
generally the maximum representable depth value. Finally, the depth buffer
requires the following work for each pixel covered by each triangle:
■Computation of a depth value for the triangle
■Lookup of the existing pixel depth in the depth buffer
■Comparison of these two values
■(For new “winner” pixels only) Writing the new depth to the depth
buffer

8.5 Determining Which Pixels are Visible 367
Z=8
Z=4
Framebuffer
Depth buffer
8 8 8 8 8 8 8 8 8
8 8 8 8 8 8 8
8 8 8 8 8 8 8
8 8 8 8 8 8
8 8 4 8
8
8
8
8
8
8
8
8 8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
8
4
4 4
4 4 4
4 4 4
8
8
8
8
4 4 4
4 4 4
4 4 4
4 4 4
4 4 4
4
4 4 4
4
8
8
8
8
8
8 8
8
8
8 8
8 8
8 8
8
8
8 8
8
8
8
8
8
Figure8.7 Two triangles rendered to a depth buffer.
This additional work per pixel covered by each triangle makes depth buffering
unsuitable for constant use in most software rasterizers. Fully-software 3D
systems tend to use depth sorting wherever possible, reserving depth buffering
for the few objects that truly require it.
In addition, the depth buffer does not ﬁx the problem of overdraw. We
must still compute the depth of every triangle pixel and compare it to the
buffer. However, it can make overdraw less of an issue in some cases, since it
is not necessary to compute or write the color of any pixel that fails the depth
test. In fact, some applications will try to render their depth-buffered scenes
in roughly front-to-back ordering so that the later geometry is likely to fail the
depth buffer test and not require color computations.
Depth buffering is extremely popular in 3D applications that run on
hardware-accelerated platforms, as it is easy to use and requires little

368
Chapter 8 Rasterization
application code or host CPU computation and produces quality images at
high performance.
Computing Per-Pixel Depth Values
The ﬁrst step in computing the visibility of a pixel using a depth buffer is to
compute the depth value of the current triangle at the given pixel. As we shall
see, zndc (which appeared to be a rather strange choice for z back in Chapter 5)
will work quite well. However, the reason why zndc works well and zview does
not is rather interesting.
In order to better understand the nature of how depth values change
across a triangle in screen space, we must be able to map a point on the
screen to the point in the triangle that projected to it. This is very similar to
picking, and we will use several of the concepts we ﬁrst discussed in Chapter 5.
Owing to the nonlinear nature of perspective projection, we will ﬁnd that our
mapping from screen space pixels to view space points on a given triangle is
somewhat complicated. We will follow this mapping through several smaller
stages.
A triangle in view space is simply a convex subset of a plane in view space.
As a result, we can deﬁne the plane of a triangle in view space by the values ˆn
and c, such that the points P = (xp, yp, zp) in the plane are those that satisfy
ˆn · (xp, yp, zp) + c = 0
(8.1)
Looking back at picking, a point in 2D NDC coordinates (xndc, yndc) maps to
the view space ray tr such that
tr = (xndc, yndc, −d)t,
t ≥0
where d is the projection distance (the distance from the view space origin
to the projection plane). Any point in view space that projects to the pixel at
(xndc, yndc) must intersect this ray. Normally, we cannot “invert” the projection
matrix, since a point on the screen maps to a ray in view space. However, by
knowing the plane of the triangle, we can intersect the triangle with the view
ray as follows. All points P in view space that fall in the plane of the triangle
are given by equation 8.1. In addition, we know that the point on the triangle
that projects to (xndc, yndc) must be equal to tr for some t. Substituting the
vector tr for the points (xp, yp, zp) in equation 8.1 and solving for t,
ˆn · (tr) + c = 0
t(ˆn · r) = −c
t = −c
ˆn · r

8.5 Determining Which Pixels are Visible 369
From this value of t, we can compute the point along the projection ray
(xview, yview, zview) = tr that is the view space point on the triangle that projects
to (xndc, yndc). This amounts to ﬁnding
(xview, yview, zview) = tr
= t(xndc, yndc, −d)
= −c(xndc, yndc, −d)
ˆn · r
= −c(xndc, yndc, −d)
ˆn · (xndc, yndc, −d)
=
−c(xndc, yndc, −d)
ˆnxxndc + ˆnyyndc −ˆnzd
(8.2)
However, we are only interested in zview right now, since we are trying to
compute a per-pixel value for depth buffering. The zview component of
equation 8.2 is
zview =
dc
ˆnxxndc + ˆnyyndc −ˆnzd
(8.3)
As a quick check of a known result, note that in the special case of a triangle
of constant depth zview = zconst, we can substitute
ˆn = (0, 0, 1)
and
c = −zconst
Substituted into equation 8.3 evaluates to the expected constant zview = zconst:
zview =
d(−zconst)
0xndc + 0yndc −1d
= −dzconst
−d
= zconst
As deﬁned in equation 8.3, zview is an expensive value to compute per
pixel (in the general, nonconstant depth case), because it is a fraction with a
nonconstant denominator. This would require a per-pixel division to compute

370
Chapter 8 Rasterization
zview, which is more expensive than we would like. However, depth buffering
requires only the ability to compare depth values against one another. If we
are comparing zview values, we know that they decrease with increasing depth
(as the view direction is −z), giving a depth test of
zview ≥DepthBuff er →New triangle is visible at pixel
zview < DepthBuff er →New triangle is not visible at pixel
However, if we compute and store inverse zview, then a similar comparison
still works in the same manner. If we invert all of the zview values, we get
1
zview
≤DepthBuff er →New triangle is visible at pixel
1
zview
> DepthBuff er →New triangle is not visible at pixel
If we invert equation 8.3, we can see that the per-pixel computation becomes
simpler:
1
zview
= ˆnxxndc + ˆnyyndc −ˆnzd
dc
=
 ˆnx
dc

xndc +
 ˆny
dc

yndc −
 ˆnzd
dc

where all of the parenthesized terms are constant across a triangle. In fact, this
forms an afﬁne mapping of NDC coordinates to 1/zview. Since we know that
there is an afﬁne mapping from pixel coordinates (xs, ys) to NDC coordinates
(xndc, yndc), we can compose these afﬁne mappings into a single afﬁne mapping
from screen space pixel coordinates to 1/zview. As a result, for a given projected
triangle
1
zview
= f xs + gys + h
where f , g, and h are real values and are constant per triangle. We deﬁne the
preceding mapping for a given triangle as
InvZ(xs, ys) = f xs + gys + h

8.5 Determining Which Pixels are Visible 371
An interesting property of InvZ(xs, ys) (or of any afﬁne mapping, for that
matter) can be seen from the derivation below
InvZ(xs + 1, ys) −InvZ(xs, ys) = (f (xs + 1) + gys + h) −(f xs + gys + h)
= f (xs + 1) −(f xs)
= f
meaning that
InvZ(xs + 1, ys) = InvZ(xs, ys) + f
and similarly
InvZ(xs, ys + 1) = InvZ(xs, ys) + g
In other words, once we compute our InvZ depth buffer value for any “base”
pixel, we can compute the depth buffer value of the next pixel in the scanline
by simply adding f . Once we compute a base depth buffer value for a given
span, as we step along the scanline, ﬁlling the span, all we need to do is add
f to our current depth between each pixel (Figure 8.8). This makes the per-
pixel computation of a depth value very fast indeed. In fact, once the base
InvZ of the ﬁrst span is computed, we may add or subtract f and g to or from
the previous span’s base depth to compute the base depth of the next span.
This technique is known as forward differencing, as we use the difference (or
delta) between the value at a pixel and the value at the next pixel to step along,
updating the current depth. This method will work for any value for which
there is an afﬁne mapping from screen space. We refer to such values as afﬁne
in screen space, or screen-afﬁne.
In fact, we can use the zndc value that we computed during projection as a
replacement for InvZ. In Chapter 5, on viewing and projection, we computed
a zndc value that is equal to −1 at the near plane and 1 at the far plane and was
of the form
zndc = a + bzview
zview
= a
1
zview
+ b
which is an afﬁne mapping of InvZ. As a result, we ﬁnd that our existing
value zndc is screen-afﬁne and is suitable for use as a depth buffer value. This
is the special case of depth buffering we mentioned earlier, often called “z-
buffering,” as it uses zndc directly.

372
Chapter 8 Rasterization
+12.5
+12.5
+12.5
+12.5
(2,2,150)
(6,2,200)
(4,0,100)
+25
+25
+50
+50
(8,4,300)
(0,4,200)
=(–4,4,100)
=(4,4,200)
Figure8.8 Forward differencing the depth value.
Numerical Precision and Z-Buffering
In practice, depth buffering in screen space has some numerical precision
limitations that can lead to visual artifacts. As was mentioned earlier in the
discussion of depth buffers, the order in which objects are drawn to a depth
buffering system (at least in the case of opaque objects) is only an issue if
the depth values of the two surfaces are equal at a given pixel. In theory,
this is unlikely to happen unless the geometric objects in question are truly
coplanar. However, because computer number representations do not have
inﬁnite precision (recall the discussion in Chapter 4), surfaces that are not
coplanar can map to the same depth value. This can lead to objects being
drawn in the wrong order.
If our depth values were mapped linearly into view space, then a 16-bit,
ﬁxed-point depth buffer would be able to correctly sort any objects whose
surfaces differed in depth by about one 60,000th of the difference between
the near and far plane distances. This would seem to be more than enough

8.5 Determining Which Pixels are Visible 373
for almost any application. For example, with a view distance of 1 km, this
would be equal to about 1.5 cm of resolution. Moving to a higher-resolution
depth buffer would make this value even smaller.
However, in the case of z-buffering, representable depth values are not
evenly distributed in view space. In fact, the depth values stored to the buffer
are basically 1/Zview, which is deﬁnitely not an even distribution of view
space Z. A graph of the depth buffer value over view space Z is shown in
Figure 8.9. This is a hyperbolic mapping of view space Z into depth buffer
values — notice how little the depth value changes with change in Z toward
the far plane. Using a ﬁxed-point value for this leads to very low precision
in the distance, as large intervals of Z map to the same ﬁxed-point value of
inverse Z. In fact, a common estimate is that a z-buffer focuses 90 percent of
its precision in the closest 10 percent of view space Z. This means that the
triangles of distant objects are often sorted incorrectly with respect to one
another.
The simplest way to avoid these issues is to maximize usage of the depth
buffer by moving the near plane as far out as possible so that the accuracy
close to the near plane is not wasted. Another method that is popular in 3D
hardware is known as the w-buffer. The w-buffer interpolates a screen-afﬁne
value for depth (often 1/w) at a high precision, then computes the inverse of
Max depth value
High depth
buffer
precision
Low depth buffer
precision
Depth buffer
value
Min depth value
Near plane
Far plane
View-space
Z
Figure8.9 Depth buffer value as a function of view-space Z.

374
Chapter 8 Rasterization
the interpolation at each pixel to produce a value that is linear in view space
(i.e., 1/ 1
w). It is this inverted value that is then stored in the depth buffer.
By quantizing (dropping the extra precision used during interpolation) and
storing a value that is linear in view space, the hyperbolic nature of the z-buffer
can be avoided to some degree.
8.5.3 Depth-Buffering in OpenGL
Demo
DepthBuffer
Using depth buffering in OpenGL requires additions to several points in ren-
dering code, somewhat analogous to the stages of rendering color to a pixel.
The ﬁrst step is to ensure that the rendering window or device is created
with a depth buffer. This step is platform-dependent in OpenGL. The sam-
ples abstract this step into the IvDisplay object. The samples request a 16-bit
depth buffer, but 32-bit is also common (and growing in popularity).
Having requested the creation of a depth buffer (and in most cases, it is
just that — a request for a depth buffer, dependent upon hardware support),
the buffer must be cleared at the start of each frame. The depth buffer is
cleared using the same function as the framebuffer clear, glClear, but with
a new argument, GL_DEPTH_BUFFER_BIT. While the depth buffer can be cleared
independently of the framebuffer using
glClear(GL_DEPTH_BUFFER_BIT);
if you are clearing both buffers, it can be faster on some systems to clear
them both with a single call, combining the masks together with a bitwise
“Or” operation as follows:
glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
To enable or disable depth testing, use glEnable(GL_DEPTH_TEST) and
glDisable(GL_DEPTH_TEST), respectively. By default, depth buffering is dis-
abled, so the application should enable it explicitly prior to rendering. When
enabled, depth buffering defaults to a mode in which a new pixel is writ-
ten only if its depth value is less than the current pixel. In other words,
in cases of multiple surfaces sharing the same minimum depth in a given
pixel, the ﬁrst surface drawn “wins.” To change this, use the function
glDepthFunc (the default value is equivalent to the argument GL_LESS). The
OpenGL Programming Guide [83] details all of the possible options, but the
next most common mode is GL_LEQUAL, which causes the depth “tiebreaker”
to favor the last surface drawn at a given depth.

8.6 Computing Source Pixel Colors 375
In the somewhat rare situation that the application needs to change the
depth to which the z-buffer is cleared (by default, it is the maximum repre-
sentable distance), it may do so using glClearDepth, passing in the desired
ﬂoating-point clearing depth as the only argument.
8.6 Computing Source Pixel Colors
The next stage in the rasterization pipeline is to compute the overall color (and
possibly alpha value) of a triangle at a given pixel. These source colors can
come in numerous forms, as discussed in the previous two chapters. Common
sources include:
■Per-triangle (“ﬂat” colors) diffuse colors, including those generated by
lighting
■Per-vertex (Gouraud colors) diffuse colors, including those generated
by lighting
■Per-vertex specular colors
■Textures
Note that several sources may exist for a given triangle. Each of them must
be independently computed per-pixel as a part of source color generation.
Having computed the per-pixel source colors, a ﬁnal source pixel color must
be generated. Chapter 7 discussed the various ways that per-pixel diffuse,
specular, and texture colors are combined. These methods all generate a ﬁnal
source pixel color that is passed to the last stage of the rasterization pipeline,
blending (which will be discussed later in this chapter).
The next few sections will discuss how source colors are computed per-
pixel from the sources we have listed. While there are many possible methods
that may be used, we will focus on methods that are fast to compute and
are well-suited to the scanline-centric nature of most rasterizer software and
hardware.
8.6.1 Flat Colors
As with all other stages in the pipeline, per-triangle, ﬂat-shaded colors are
the easiest to rasterize. For each visible pixel in each span, the triangle color
is the source pixel color. In fact, if the source pixel color is to be used directly as
the ﬁnal pixel color (i.e., blending and textures are not enabled, as we discuss
in Section 8.8 on blending), then the entire span may be drawn very quickly by

376
Chapter 8 Rasterization
writing the given triangle color to the consecutive pixels in an extremely tight
code loop. This is one of the reasons that ﬂat-shaded triangles were the ﬁrst
primitives to be rasterized in early raster-based 3D graphics systems, where
per-pixel computation had to be kept to an absolute minimum.
8.6.2 Gouraud Colors
Gouraud shaded colors are deﬁned by the colors at the three vertices of each
triangle, and thus their values must be interpolated and recomputed for each
pixel in the triangle. In the general case this can be an expensive operation to
compute correctly. However, we will ﬁrst look at the special case of triangles
of constant depth. The mapping in this case is not at all expensive, making it a
tempting approximation to use even when rendering triangles of nonconstant
depth.
To analyze the constant-depth case, we will determine the nature of the
mapping of our constant-depth triangle from pixel space, through NDC space,
into view space, through barycentric coordinates, and ﬁnally to color. We start
ﬁrst with a special case of the mapping from pixel space to view space.
The overall projection equations derived in Chapter 5 (mapping from view
space through NDC space to pixel coordinates) were all of the form
xs = axview
zview
+ b
ys = cyview
zview
+ d
where both a, c ̸= 0. If we assume that a triangle’s vertices are all at the
same depth (i.e., view space Z is equal to a constant zconst for all points in the
triangle), then the projection of a point in the triangle is
xs = axview
zconst
+ b =

a
zconst

xview + b = a′xview + b
ys = cyview
zconst
+ d =

c
zconst

yview + d = c′yview + d
Note that a, c ̸= 0 implies that a′, c′ ̸= 0, so we can rewrite these such
that
xview = xs −b
a′
yview = ys −d
c′

8.6 Computing Source Pixel Colors 377
Thus, for triangles of constant depth zconst
■Projection forms an afﬁne mapping from screen vertices to view-space
vertices on the zview = zconst plane.
■Barycentric coordinates are an afﬁne mapping of view-space vertices
(as we saw in Chapter 1).
■Vertex colors deﬁne an afﬁne mapping from a barycentric coordinate
to a color (Gouraud shading, as seen in Chapter 6).
If we compose these afﬁne mappings, we end up with an afﬁne mapping from
screen space pixel coordinates to color. We can write this afﬁne mapping from
pixel coordinates to colors as
Color(xs, ys) = Cxxs + Cyys + C0
where Cx, Cy, and C0 are all colors (each of which are possibly negative or
greater than 1.0). For a derivation of the formula that maps the three screen
space pixel positions and corresponding trio of vertex colors to the three colors
Cx, Cy, and C0, see page 126 of [27]. From our earlier derivation of the prop-
erties of inverse Z in screen space, we note that Color(xs, ys) is screen-afﬁne
for triangles of constant z:
Color(xs +1,ys)−Color(xs,ys)=(Cx(xs +1)+Cyys +C0)−(Cxxs +Cyys +C0)
=Cx(xs +1)−(Cxxs)
=Cx
meaning that
Color(xs + 1, ys) = Color(xs, ys) + Cx
and similarly
Color(xs, ys + 1) = Color(xs, ys) + Cy
As with inverse Z, we can compute per-pixel Gouraud colors for a constant-z
triangle simply by computing forward differences of the color of a “base pixel”
in the triangle.
When a triangle that does not have constant depth in camera space
is projected using a perspective projection, the resulting mapping is not

378
Chapter 8 Rasterization
screen-afﬁne. From our discussion of depth buffer values, we can see that
given a general (not necessarily constant depth) triangle in view space, the
mapping from NDC space to the view-space point on the triangle is of the form
xview =
dxndc
axndc + byndc + c
yview =
d′yndc
axndc + byndc + c
zview =
d′′
axndc + byndc + c
These are projective mappings, not afﬁne mappings as we had in the constant-
depth case. This means that the overall mapping from screen space to
Gouraud colors is also projective. Such a projective mapping requires two
forward differences (one for the numerator and one for the denominator)
and a division per color component, per pixel. In order to correctly interpo-
late vertex colors of a triangle in perspective, we must use this more complex
projective mapping.
Keeping in mind that Gouraud shading is an approximation method
in the ﬁrst place, there is somewhat decreased justiﬁcation for using the
projective mapping on the basis of “correctness.” Furthermore, Gouraud-
shaded colors tend to interpolate so smoothly that it can be difﬁcult to tell
whether the interpolation is perspective correct or not. In fact, Heckbert
and Moreton mention in [58] that the New York Institute of Technology’s
off-line renderer interpolated colors incorrectly in perspective for several
years before anyone noticed! As a result, hardware and (especially) soft-
ware graphics systems have often avoided the expensive, perspective-correct
projective interpolation of Gouraud colors and have simply used the afﬁne
mapping and forward differencing. However, our next interpolant, tex-
ture coordinates, will require us to be far more careful with perspective
issues.
8.7 Rasterizing Textures
Rasterizing textures requires several independent steps. First, the texture
coordinates must be correctly interpolated to determine a value at each pixel.
Then, these texture coordinates must be mapped into the texture to produce
a color. Both of these steps raise completely different issues, both mathemat-
ical and algorithmic. The following sections will detail the most important
issues arising from each step in the process.

8.7 Rasterizing Textures 379
8.7.1 Texture Coordinate Review
We will be using a number of different forms of coordinates throughout our
discussion of rasterizing textures. This section will list and review these
various texture-related coordinates and their notations.
The ﬁrst form of coordinates is most commonly known simply as texture
coordinates. These were the most common form of texture-related coordinates
in our initial discussion of texturing. These are independent of the height and
width of a texture and are normalized such that (0,0) represents the bottom-
left corner of a texture image, and (1,1) represents the upper-right corner of
a texture image. These are generally stored as real-valued numbers, namely,
ﬂoating-point or ﬁxed-point coordinates. They are the coordinates that most
graphics systems use at the application level. They are very convenient for
most applications, as they are independent of the exact resolution of the tex-
ture. However, they are not very useful at all when rasterizing textures, and
we will use them very rarely in the following rasterization discussions. We
notate texture coordinates simply as (u, v).
The next form of coordinates is often referred to as texel coordinates. Like
texture coordinates, texel coordinates are represented as real-valued num-
bers. However, unlike texture coordinates, texel coordinates are dependent
upon the width (wtexture) and height (htexture) of the texture image being used.
We will notate texel coordinates as (utexel, vtexel). The mapping from (u, v) to
(utexel, vtexel) is
(utexel, vtexel) = (u · wtexture −1
2, v · htexture −1
2)
The shift of 1/2 may seem odd, but Figure 8.10 shows why this is necessary.
Texel coordinates are relative to the texel centers. A texture coordinate of zero
is on the boundary between two repetitions of a texture. Since the texel centers
are at the middle of a texel, a texture coordinate that falls on an integer value
is really halfway between the center of the last texel of one repetition of the
texture, and the center of the ﬁrst texel in the next repetition. This is equivalent
to a texel coordinate of −1/2. See [77] (the section “Directly Mapping Texels
to Pixels”) for details of one common graphics systems texture coordinate to
texel mapping.
Another form of coordinate is the integer texel coordinate, or texel
address. Unlike the other forms of coordinates, these are (as the name implies)
integral values. As such, they can be used to index a texture directly (once the
wrapping or clamping mode is applied as we ﬁrst discussed in Chapter 6 on
texturing). We will notate integer texel coordinates as (uint, vint). Integer texel
coordinates are the values sent to the image lookup function Image(uint, vint),
discussed in the introduction to texturing. The mapping from texel coordi-
nates to integer texel coordinates is not universal and is dependent upon the

380
Chapter 8 Rasterization
Texel centers
Texel
=(wtexture–   ,htexture–   )
(0,1)
(1,1)
(0,0)
(1,0)
(  ,  )
1
2
1
2
(–  , –   )
1
2
1
2
1
2
1
2
Figure8.10 Texel coordinates and texel centers.
texture ﬁltering mode, which will be discussed in Section 8.7.4 under “Texture
Filtering and Mipmaps.”
8.7.2 Interpolating Texture Coordinates
The process of rasterizing a texture starts by interpolating the per-vertex tex-
ture coordinates to determine the correct value at each pixel. Actually, as
alluded to in the previous section, it is generally the texel coordinates that are
interpolated in a rasterizer. This is a process that is very similar to interpo-
lating colors for Gouraud shading. However, because texture coordinates are
used somewhat differently than vertex colors, we are rarely able to use the
screen-afﬁne approximation that is used for Gouraud colors.
The most basic issue has to do with the properties of afﬁne and projective
transformations. Afﬁne transformations map parallel lines to parallel lines,
while projective transformations guarantee only to map straight lines to
straight lines. Anyone who has ever looked down a long, straight road
knows that the two lines that form the edges of the road appear to meet in

8.7 Rasterizing Textures 381
Wire-frame view
Textured view
Figure8.11 Two textured triangles parallel to the view plane.
the distance, even though they are parallel. Perspective, being a projective
mapping, does not preserve parallel lines.
The classic example of the difference between afﬁne and projective inter-
polations is the checkerboard square, drawn in perspective. Figure 8.11 shows
a checkered texture as an image, along with the image applied with wrapping
to a square formed by two triangles (the two triangles are shown in outline,
or wire frame). When the top is tilted away in perspective, note that if the
texture is mapped using a projective mapping (Figure 8.12), the vertical lines
converge into the distance as expected.
If the texture coordinates are interpolated using an afﬁne mapping
(Figure 8.13), we see two distinct visual artifacts. First, within each triangle,
all of the parallel lines remain parallel, and the vertical lines do not converge
the way we expect. Furthermore, note the obvious “kink” in the lines along
the square’s diagonal (the shared triangle edge). This might at ﬁrst glance
seem to be a bug in the interpolation code, but a little analysis shows that it
is actually a basic property of an afﬁne transformation. An afﬁne transforma-
tion is deﬁned by the three points of a triangle. As a result, having deﬁned the
three points of the triangle and their texture coordinates, there are no more
degrees of freedom in the transformation. Each triangle deﬁnes its transform
independent of the other triangles, and the result is a bend in what should be
a set of lines across the square.
The projective transform, however, has additional degrees of freedom,
represented by the depth values associated with each vertex. These depth
values change the way the texture coordinate is interpolated across the

382
Chapter 8 Rasterization
Wire-frame view
Textured view
Figure8.12 Two textured triangles oblique to the view plane, drawn using a perspective
mapping.
Wire-frame view
Textured view
Figure8.13 Two textured triangles oblique to the view plane, drawn using an afﬁne
mapping.

8.7 Rasterizing Textures 383
triangle and allow the lines to remain straight, even across the triangle
boundaries. The downside of this projective mapping is that it requires the
following operations per pixel for correct evaluation:
1. An afﬁne forward difference operation to update the numerator for
utexel
2. An afﬁne forward difference operation to update the numerator for
vtexel
3. An afﬁne forward difference operation to update the shared denomina-
tor (both utexel and vtexel can use the same denominator, as it is based
on inverse depth of the triangle at the pixel)
4. A division to recover the perspective-correct utexel
5. A division to recover the perspective-correct vtexel
Hardware rasterization systems generally support this operation (or at
least a carefully-constructed approximation of it), but for software rasteriz-
ers, this is simply too expensive to compute for each pixel. There are numerous
optimizations and approximations that have been used in software rasteriz-
ers to speed up this process, but they generally fall into two basic categories:
(1) subdividing and using piecewise-afﬁne mappings for the resulting short
spans and (2) ﬁtting higher-order (e.g., quadratic) curves to approximate the
perspective curve. Each method is detailed in [60], including the arguments
in favor of and in opposition to each. However, for most modern hard-
ware rasterization systems, per-pixel perspective correct texturing is simply
assumed.
8.7.3 Mapping a Coordinate to a Texel
When rasterizing textures, we will ﬁnd that — due to the nature of perspec-
tive projection, the shape of objects, and the way texture coordinates are
generated — pixels will rarely correspond directly and exactly to texels in a
one-to-one mapping. Any rasterizer that supports texturing will need to handle
a wide range of texel-to-pixel mappings. In the initial discussions of texturing
in Chapter 6, we noted that texel coordinates generally include precision (via
either ﬂoating-point or ﬁxed-point numbers) that is much more ﬁne-grained
than the per-texel values that would seem to be required. As we shall see,
in several cases we will use this so-called sub-texel precision to improve the
quality of rendered images in a process known as texture ﬁltering.
Texture ﬁltering (in its numerous forms) performs the mapping from real-
valued texel coordinates to ﬁnal colors, through a mixture of texel coordinate

384
Chapter 8 Rasterization
mapping and combinations of the colors of the resulting texel or texels.
We will break down our discussion of texture ﬁltering into two major
cases: one in which a single texel maps to multiple pixels (magniﬁcation),
and one in which a number of texels map to a single pixel (“miniﬁcation”), as
they are handled quite differently.
Magnifying a Texture
Our initial texturing discussion stated that one common method of mapping
these sub-texel precise values to colors was simply to select the nearest texel
and use its color directly. This method, called nearest-neighbor texturing, is
very simple to compute. For any (utexel, vtexel) texel coordinate, the integer
texel coordinate (uint, vint) is the nearest integer texel center, computed via
rounding:
(uint, vint) = (⌊utexel + 0.5⌋, ⌊vtexel + 0.5⌋)
Having computed this integer texel coordinate, we simply use the Image func-
tion to look up the color of the texel. The returned value is the source texture
color for the pixel. While this method is easy and fast to compute, it has a
signiﬁcant drawback when the texture is mapped in such a way that a single
texel covers more than one pixel. In such a case the texture is said to be “mag-
niﬁed,” as a quadrilateral block of pixels on the screen is covered by one texel
in the texture, as can be seen in Figure 8.14.
With nearest neighbor texturing, all (utexel, vtexel) texel coordinates in the
square
iint −0.5 ≤utexel < iint + 0.5
jint −0.5 ≤vtexel < jint + 0.5
will map to the integer texel coordinates (iint, jint) and thus map to a con-
stant color. This is a square of height and width 1 in texel space, centered
at the texel center. This results in obvious squares of constant color, which
tends to draw attention to the fact that a low-resolution image has been
mapped onto the surface (see Figure 8.14). Often, this is not the desired visual
impression.
The problem lies with the fact that nearest neighbor texturing represents
the texture image as a piecewise constant function of (u, v). The color used
is constant across a triangle until either uint or vint changes. Since the ﬂoor
operation is discontinuous at integer values, this leads to sharp edges in the
color function over the surface of the triangle. This is not unlike the issues we
encountered with ﬂat shading.
In the case of ﬂat shading, the answer to the issue of discontinuous colors
was to interpolate between the colors at each vertex. In the case of texturing,
it involves interpolating between the colors at each texel center. Rather than

8.7 Rasterizing Textures 385
Figure 8.14 Nearest-neighbor magniﬁcation.
creating a piecewise constant function, we create a piecewise smooth color
function. The method ﬁrst computes the maximum integer texel coordinate
(uint, vint) that is less than (utexel, vtexel), the texel coordinate (i.e., the ﬂoor of
the texel coordinates):
(uint, vint) = (⌊utexel⌋, ⌊vtexel⌋)
In other words, (uint, vint) deﬁnes the minimum (lower-left in texture image
space) corner of a square of four adjacent texels that “bound” the texel coordi-
nate (Figure 8.15). Having found this square, we can also compute a fractional
texel coordinate 0.0 ≤uf rac, vf rac < 1.0 that deﬁnes the position of the texel
coordinate within the 4-texel square (see Figure 8.15).
(uf rac, vf rac) = (utexel −uint, vtexel −vint)
We use Image() to look up the texel colors at the four corners of
the square.
For ease of notation,
we deﬁne the following shorthand

386
Chapter 8 Rasterization
(uint,vint)
ufrac=0.5
vfrac=0.75
(uint,vint+1)
(utexel,vtexel)
Pixel mapped into
texel space
(uint+1,vint+1)
0.5
0.75
(uint+1,vint)
Figure8.15 Finding the four texels that “bound” a pixel center and the fractional position
of the pixel.
for the color of the texture at each of the four corners of the square
(Figure 8.16):
C00 = Image(uint, vint)
C10 = Image(uint + 1, vint)
C01 = Image(uint, vint + 1)
C11 = Image(uint + 1, vint + 1)
Then, we deﬁne a smooth interpolation (called “bilinear ﬁltering”) of the four
texels surrounding the texel coordinate. We deﬁne the smooth mapping in two
stages as shown in Figure 8.17. First, we interpolate between the colors along
the minimum-v edge of the square, based on the fractional u coordinate:
CMinV = C00(1 −uf rac) + C10uf rac
and similarly along the maximum-v edge:
CMaxV = C01(1 −uf rac) + C11uf rac

8.7 Rasterizing Textures 387
Pixel mapped into
texel space
C01
C11
C10
C00
Figure 8.16 The four corners of the texel-space bounding square around the pixel
center.
Finally, we interpolate between these two values using the fractional v
coordinate:
CFinal = CMinV (1 −vf rac) + CMaxV vf rac
See Figure 8.17 for a graphical representation of these two steps. Substituting
these into a single, direct formula, we get
CFinal =C00(1 −uf rac)(1 −vf rac) + C10uf rac(1 −vf rac)
+ C01(1 −uf rac)vf rac + C11uf racvf rac
This is known as bilinear texture ﬁltering, and is extremely popular in hard-
ware 3D graphics systems. The fact that we interpolated along u ﬁrst and then
interpolated along v does not affect the result (other than by potential preci-
sion issues). A quick substitution shows that the results are the same either
way. However, note that this is not an afﬁne mapping. Four points are not

388
Chapter 8 Rasterization
C01
C11
C10
C00
CMaxV =C01(1–ufrac)+C11ufrac
CMinV =C00(1–ufrac)+C10 ufrac
CFinal=CMinV(1–vfrac)+CMaxVvfrac
Figure 8.17 Bilinear ﬁltering.
always coplanar and as a result, in order to ﬁt the four points, the resulting
“surface” is not planar.
As with Gouraud shading, the colors along the four boundary edges are
continuous — the color at each texel edge is dependent on only the colors at
either end of the edge. An example of the visual difference between nearest-
neighbor and bilinear ﬁltering is shown in Figure 8.18. While bilinear ﬁltering
can greatly improve the image quality of magniﬁed textures by reducing the
visual “blockiness,” it will not add detail to a texture. If a texture is magniﬁed
considerably (i.e., one texel maps to many pixels), the image will look blurry
due to a lack of detail. The texture shown in Figure 8.18 is highly magniﬁed,
leading to obvious blockiness in the left image and blurriness in the right
image.
Texture Magniﬁcation in OpenGL
Demo
TextureFilter
OpenGL uses the function glTexParameteri to control numerous texturing
features. The general format of the function for 2D texturing is
glTexParameteri(GL_TEXTURE_2D, setting, value);

8.7 Rasterizing Textures 389
Extreme magnification using
nearest-neighbor filtering
Extreme magnification using
bilinear filtering
Figure 8.18 Extreme magniﬁcation of a texture.
In order to set the magniﬁcation method (or ﬁlter), setting should be
passed as GL_TEXTURE_MAG_FILTER. OpenGL supports both bilinear ﬁltering and
nearest-neighbor selection. They are each set as follows:
// Nearest-neighbor
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);
// Bilinear interpolation
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
“Minifying” a Texture
Throughout the course of our discussions of coloring and rasterization, we
have referred to pixels by their pixel centers — inﬁnite points located at the
center of a square pixel. However, pixels (whether on the screen or on the
projection plane) have nonzero area. This difference between the area of a
pixel and the point sample representing it becomes very obvious in a common
case of texturing.
As an example, imagine an object that is distant from the camera. Objects
in a scene are generally textured at high detail. This is done to avoid the blur-
riness (such as the blurriness we saw in Figure 8.18) that can occur when an
object that is close to the camera has a low-resolution texture applied to it.

390
Chapter 8 Rasterization
As that same object and texture is moved into the distance (a common situa-
tion in a dynamic scene), the detailed texture will be mapped to smaller and
smaller regions of the screen due to perspective scaling of the object. This is
known as “miniﬁcation” of a texture, as it is the inverse of magniﬁcation.
In an extreme (but actually quite frequent) case, the entire high-detail tex-
ture could be mapped in such a way that it covers only a few pixels. Figure 8.19
provides such an example; in this case, note that if the object moves even
slightly (even less than a pixel), the exact texel covering the pixel’s center
point can change drastically. In fact, such a point sample is almost random in
the texture and can lead to the color of the pixel changing wildly from frame
to frame as the object moves in tiny, sub-pixel amounts on the screen. This
can lead to ﬂickering over time, a distracting artifact in an animated, rendered
image.
The problem lies in the fact that most of the texels in the texture have
almost equal “claim” to the pixel, as all of them are projected within the
rectangular area of the pixel on the projection plane. The overall color of the
pixel should represent all of the texels that fall inside of it. One way of thinking
Mapping of texture
into screen
coordinates
Pixel centers
Figure 8.19 Extreme “miniﬁcation” of a texture.

8.7 Rasterizing Textures 391
of this is to map the square pixel on the projection plane onto the plane of
the triangle, giving a (possibly skewed) quadrilateral, as seen in Figure 8.20.
In order to color the pixel “fairly,” we need to compute a weighted average of
the colors of all of the texels in this quadrilateral, based on the relative area of
the quadrilateral covered by each texel. The more of the pixel that is covered
by a given texel, the greater the contribution of that texel’s color to the ﬁnal
color of the pixel.
While such a method would give a correct pixel color and would avoid
the issues seen with point sampling, in reality this is not an algorithm that
is best suited for real-time rasterization. Depending on how the texture is
mapped, a pixel could cover an almost unbounded number of texels. Finding
and summing these texels on a per-pixel basis would require a potentially
unbounded amount of per-pixel computation, which is well beyond the means
of even hardware rasterization systems. A faster (preferably constant-time)
method of approximating this texel averaging algorithm is required. For most
modern graphics systems, a method known as mipmapping satisﬁes these
requirements.
8.7.4 Mipmapping
Mipmapping [118] is a texture ﬁltering method that avoids the per-pixel
expense of computing the average of a large number of texels. It does so by
Texel-space back-
projection of pixel area
Screen space with pixel
of interest highlighted
Figure8.20 Mapping the square screen-space area of a pixel back into texel space.

392
Chapter 8 Rasterization
precomputing and storing additional information with each texture, requiring
some additional memory over standard texturing. Mipmapping is a constant-
time operation per pixel and requires a ﬁxed amount of extra storage per
texture (in fact, it increases the number of texels that must be stored by
approximately one-third). Mipmapping is a popular ﬁltering algorithm in both
hardware and software rasterizers and is relatively simple conceptually.
To understand the basic concept behind mipmapping, imagine a 2 × 2–
texel texture. If we look at a case where the entire texture is mapped to a
single pixel, we could replace the 2 × 2 texture with a 1 × 1 texture (a sin-
gle color). The appropriate color would be the mean of the four texels in
the 2 × 2 texture. We could use this new texture directly. If we precompute
the 1 × 1–texel texture at loadtime, we can simply choose between the two
textures as needed (Figure 8.21). When the given pixel maps to only one of
the four texels in the original texture, we simply use a magniﬁcation method
and the original texture to determine the color. When the pixel covers the
2×2 version of
texture is the closest
pixel to texel match
Screen space geometry
(same mipmapped texture applied to both squares)
1×1 version of
texture is the closest
pixel to texel match
Figure8.21 Choosing between two sizes of a texture.

8.7 Rasterizing Textures 393
entire texture, we would use the 1 × 1 texture directly, again applying the
magniﬁcation algorithm to it (although with a 1 × 1 texture, this is just the
single texel color). The 1 × 1 texture adequately represents the overall color
of the 2 × 2 texture in a single texel, but it does not include the detail of the
original 2 × 2 texel texture. Each of these two versions of the texture has a
useful feature that the other does not.
Mipmapping takes this method and generalizes it to any texture with
power-of-two dimensions. For the purposes of this discussion, we assume that
textures are square (the algorithm does not require this, as we shall see later in
our discussion of OpenGL’s mipmapping support). Mipmapping takes the ini-
tial texture image Image0 (abbreviated I0) of dimension wtexture = htexture = 2L
and generates a new version of the texture by averaging each square of four
adjacent texels into a single texel. This generates a texture image Image1 of size
1
2wtexture = 1
2htexture = 2L−1
as follows:
Image1(i, j) = I0(2i, 2j) + I0(2i + 1, 2j) + I0(2i, 2j + 1) + I0(2i + 1, 2j + 1)
4
where 0 ≤i, j < 1
2wtexture. Each of the texels in Image1 represents the overall
color of a block of the corresponding four texels in Image0 (see Figure 8.22).
Note that if we use the same original texture coordinates for both versions of
the texture, Image1 simply appears as a blurry version of Image0 (with half
the detail of Image0). If a block of about four adjacent texels in Image0 covers
a pixel, then we can simply use Image1 when texturing. But what about more
extreme cases of miniﬁcation? The algorithm can be continued recursively.
For each image Imagei whose dimensions are greater than 1, we can deﬁne
Imagei+1, whose dimensions are half of Imagei, and average texels of Imagei
into Imagei+1. This generates an entire set of L + 1 versions of the original
texture, where the dimensions of Imagei are equal to
wtexture
2i
This forms a pyramid of images, each one-half the dimensions (and contain-
ing one-quarter the texels) of the previous image in the pyramid. Figure 8.23
provides an example of such a pyramid. We compute this pyramid for each
texture in our scene once at loadtime and store each entire pyramid in mem-
ory. This simple method of computing the mipmap images is known as box
ﬁltering (as we are averaging a 2 × 2 “box” of pixels into a single pixel). Box
ﬁltering is not the sole method for generating the mipmap pyramid, nor is it

394
Chapter 8 Rasterization
I1(0,0)
I1(0,0) = I0(0,0) + I0(1,0) + I0(0,1) + I0(1,1)
4
I1(0,0) = 
(1,1,1) + (0,0,0) + (0,0,0) + (1,1,1)
4
= (   ,   ,   )
1
2
1
2
1
2
Figure8.22 Texel-block to texel mapping between mipmap levels.
128x128
64x64
32x32
16x16
8x8
4x4
2x2
1x1
Figure8.23 A full mipmap pyramid for a texture.

8.7 Rasterizing Textures 395
the highest-quality. Other, more complex methods are often used to ﬁlter each
mipmap level down to the next lower level. These methods can avoid some of
the visual issues that can crop up from the simple box ﬁlter. See Foley, van
Dam, Feiner, and Hughes [36], or Wohlberg [120] for details of other image
ﬁltering methods.
Texturing a Pixel with a Mipmap
The most simple, general algorithm for texturing a pixel with a mipmap can
be summarized as follows:
1. Determine the mapping of the pixel’s screen space rectangle into
texture space.
2. Having mapped the pixel into a quadrilateral in texture space, select
whichever mipmap level comes closest to exactly mapping the quadri-
lateral to a single texel.
3. Texture the pixel with the “best match” mipmap level selected in the
previous step, using the desired magniﬁcation algorithm.
There are numerous common ways of determining the “best match”
mipmap level, and there are numerous methods of ﬁltering this mipmap level
into a ﬁnal source pixel color. We would like to avoid having to explicitly map
the pixel corners back into texture space. As a part of rasterization, it is com-
mon to compute the difference between the texel coordinates at a given pixel
and those of the pixel to the right and below the given pixel. Such differences
are used to step the texture coordinates from one pixel to the next. These
differences are written as derivatives. The listing that follows is designed to
assign intuitive values to each of these four partial derivatives. (For those
unfamiliar with ∂, it is the symbol for a partial derivative, a basic concept of
multivariable calculus involving the change of one component of the value of
a vector-valued function over change in one of the input components.)
∂utexel
∂xs
= Change in utexel per horizontal pixel step
∂utexel
∂ys
= Change in utexel per vertical pixel step
∂vtexel
∂xs
= Change in vtexel per horizontal pixel step
∂vtexel
∂ys
= Change in vtexel per vertical pixel step

396
Chapter 8 Rasterization
If a pixel maps to about 1 texel, then
∂utexel
∂xs
2
+
∂vtexel
∂xs
2
≈1, and
∂utexel
∂ys
2
+
∂vtexel
∂ys
2
≈1
In other words, even if the texture is rotated, if the pixel is about the same
size as the texel mapped to it, then the overall change in texture coordinates
over a single pixel has a length of about 1 texel. Note that all four of these
differences are independent. These partials are dependent upon utexel and
vtexel, which are in turn dependent upon texture size. In fact, for each of these
differentials, moving from Imagei to Imagei+1 causes the differential to be
halved. As we shall see, this is a useful property when computing mipmapping
values.
A common formula that is used to turn these differentials into a metric
of pixel-texel size ratio is described in [57], which deﬁnes a formula for the
radius of a pixel as mapped back into texture space
size = max


 ∂utexel
∂xs
2
+
∂vtexel
∂xs
2
,
 ∂utexel
∂ys
2
+
∂vtexel
∂ys
2


This value is halved each time we move from Imagei to Imagei+1. So, in
order to ﬁnd a mipmap level at which we map one texel to the pixel, we
must compute the L such that
size
2L ≈1
where size is computed using the texel coordinates for Image0. Solving for L,
L = log2 size
This value of L is the mipmap level index we should use. Note that if we plug
in partials of 1, we get size = 1, which leads to L = 0, which corresponds to
the original texture image as expected.
This gives us a closed-form method that can convert existing partials (used
to interpolate the texture coordinates across a scanline) to a speciﬁc mipmap

8.7 Rasterizing Textures 397
level L. The ﬁnal formula is
L = log2

max


 ∂utexel
∂xs
2
+
∂vtexel
∂xs
2
,
 ∂utexel
∂ys
2
+
∂vtexel
∂ys
2




= log2


!
"
"
#max
∂utexel
∂xs
2
+
∂vtexel
∂xs
2
,
∂utexel
∂ys
2
+
∂vtexel
∂ys
2 

= 1
2log2

max
∂utexel
∂xs
2
+
∂vtexel
∂xs
2
,
∂utexel
∂ys
2
+
∂vtexel
∂ys
2
Note that the value of L is real, not an integer — we will discuss the methods
of mapping this value into a discrete mipmap pyramid later. The preceding
function is only one possible option for computing the mipmap level L.
Graphics systems use numerous simpliﬁcations and approximations of this
value (which is itself an approximation) or even other functions to determine
the correct mipmap level. In fact, the particular approximations of L used
by some hardware devices are so distinct that some experienced users of 3D
hardware can actually recognize a particular piece of display hardware by
looking at rendered, mipmapped images. Other pieces of 3D hardware allow
the developer (or even the end user) to adjust the mipmap level used, as some
users prefer “crisp” images (tending toward a more detailed mipmap level and
more texels per pixel) while others prefer “smooth” images (tending toward a
less detailed mipmap level and fewer texels per pixel). For a detailed derivation
of one case of mipmap level selection, see page 106 of Eberly [27].
Another method that has been used to lower the per-pixel expense of
mipmapping is to select a single mipmap level per triangle in each frame
and rasterize the entire triangle using that mipmap level. While this is a very
fast method, it can lead to serious visual artifacts, especially at the edges of
triangles, where the mipmap level may change sharply. Software rasteriz-
ers that support mipmapping often use this method, known as per-triangle
mipmapping.
Note that by its very nature, mipmapping tends to use smaller textures on
distant objects. When used with software rasterizers, this means that mipmap-
ping can actually increase performance, because the smaller mipmap levels
are more likely to ﬁt in the processor’s cache than the full-detail texture. Most
software rasterizers that support texturing are performance-bound to some
degree by the memory bandwidth of reading textures. Keeping a texture in
the cache can decrease these bandwidth requirements signiﬁcantly. Further-
more, if point sampling is used with a non-mipmapped texture, adjacent pixels
may require reading widely separated parts of the texture. These large per-
pixel strides through a texture can result in horrible cache behavior and can

398
Chapter 8 Rasterization
impede the performance of non-mipmapped rasterizers severely. These cache
miss stalls make the cost of computing mipmapping information (at least
on a per-triangle basis) worthwhile, independent of the signiﬁcant increase
in visual quality. In fact, many hardware platforms also see performance
increases when using mipmapping, owing to the small, on-chip texture cache
memories used to hold recently used textures.
Texture Filtering and Mipmaps
The methods described above work on the concept that there will be a sin-
gle, “best” mipmap level for a given pixel. However, since each mipmap level
is twice the size of the next mipmap level in each dimension, the “closest”
mipmap level may not be an exact pixel-to-texel mapping. Rather than select-
ing a given mipmap level as the best, linear mipmap ﬁltering uses a method
similar to linear texture ﬁltering. Basically, mipmap ﬁltering computes a real-
valued L, which is used to ﬁnd the pair of adjacent mipmap levels that bound
the given pixel-to-texel size. The two adjacent mipmap levels that bound the
pixel may be found using ⌊L⌋and ⌈L⌉. The remaining fractional component
is used to blend between texture colors found in the two mipmap levels.
Put together, there are now two independent ﬁltering axes, each with two
possible ﬁltering modes, leading to four possible mipmap ﬁltering modes as
shown in Table 8.1. Of these methods, the most popular is linear-bilinear,
which is also known as trilinear interpolation ﬁltering, or trilerp, as it is the
exact 3D analog to bilinear interpolation. It is the most expensive of these
mipmap ﬁltering operations, requiring the lookup of eight texels per pixel, as
well as seven linear interpolations (three per each of the two mipmap levels,
and one additional to interpolate between the levels), but it also produces
the smoothest results. Filtering between mipmap levels also increases the
Table 8.1 Mipmap Filtering Modes
Mipmap
Texture
ﬁlter
ﬁlter
Result
Nearest
Nearest
Select “best” mipmap level and then select closest
texel from it
Nearest
Bilinear
Select “best” mipmap level and then interpolate four
texels from it
Linear
Nearest
Select two “bounding” mipmap levels, select closest
texel in each, and then interpolate between the two
texels
Linear
Bilinear
Select two “bounding” mipmap levels, interpolate
four texels from each, and then interpolate between
the two results

8.7 Rasterizing Textures 399
amount of texture memory bandwidth used, as the two mipmap levels must be
accessed per sample. Thus, multilevel mipmap ﬁltering often counteracts the
aforementioned performance beneﬁts of mipmapping on hardware graphics
devices.
A ﬁnal, newer form of mipmap ﬁltering is known as anisotropic ﬁltering.
The mipmap ﬁltering methods discussed thus far implicitly assume that the
pixel, when mapped into texture space, produces a quadrilateral that is
approximated quite well by a circle. In other words, the quadrilateral in tex-
ture space is basically square. In practice, this is often not the case. With
polygons in extreme perspective, a pixel often maps to a very long, thin quadri-
lateral in texture space. The standard isotropic ﬁltering modes can tend to
look too blurry (having selected the mipmap level based on the long axis of
the quad) or too sharp (having selected the mipmap level based on the short
axis of the quad). Anisotropic texture ﬁltering takes the aspect ratio of the
texture-space quadrilateral into account when sampling the mipmap and is
capable of ﬁltering nonsquare regions in the mipmap to generate a result that
accurately represents the tilted polygon’s texturing. As of the writing of this
text, anisotropic ﬁltering is a common but not universal feature in consumer
3D hardware.
Mipmapping in OpenGL
Demo
Mipmap
The individual levels of a mipmap pyramid may be speciﬁed manually in
OpenGL through the use of the glTexImage2D function described in the intro-
duction to texturing (Chapter 6). However, in the case of mipmaps, the
(previously ignored) second argument, GLint level, speciﬁes the mipmap
level. The mipmap level of the highest-resolution image is 0. Each subsequent
level number (1, 2, 3 . . .) represents the mipmap pyramid image with half the
dimensions of the previous level. OpenGL requires that a “full” pyramid be
speciﬁed for mipmapping to work correctly. The number of mipmap levels in
a full pyramid is equal to
Levels = log2(max(wtexture, htexture)) + 1
Note that the number of mipmap levels is based on the larger dimension of
the texture. Once a dimension falls to 1 texel, it stays at 1 texel while the larger
dimension continues to decrease. So, for a 32 × 8–texel texture, the mipmap
levels are shown in Table 8.2.
Note that the texels of the mipmap level images passed to glTexImage2D
must be computed by the application. OpenGL simply accepts these images
as the mipmap levels and uses them directly. Once all of the mipmap levels for
a texture are speciﬁed, glBindTexture may be used as before (see Chapter 6)

400
Chapter 8 Rasterization
Table 8.2 Mipmap levels for a 32 × 8–texel texture
Level
Width
Height
0
32
8
1
16
4
2
8
2
3
4
1
4
2
1
5
1
1
to bind an identiﬁer to the entire mipmap pyramid for later use. An example
of specifying an entire pyramid directly follows.
char* texels0 = new unsigned char[16 * 16 * 4];
char* texels1 = new unsigned char[8 * 8 * 4];
char* texels2 = new unsigned char[4 * 4 * 4];
char* texels3 = new unsigned char[2 * 2 * 4];
char* texels4 = new unsigned char[1 * 1 * 4];
// fill texels0 with the image data
// filter the image data down into texels1– 4
// ...
// the top-level 16x16 image
glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, 16, 16,
0, GL_RGBA, GL_UNSIGNED_BYTE, texels0);
// the additional mipmap levels
glTexImage2D(GL_TEXTURE_2D, 1, GL_RGBA, 8, 8,
0, GL_RGBA, GL_UNSIGNED_BYTE, texels1);
glTexImage2D(GL_TEXTURE_2D, 2, GL_RGBA, 4, 4,
0, GL_RGBA, GL_UNSIGNED_BYTE, texels2);
glTexImage2D(GL_TEXTURE_2D, 3, GL_RGBA, 2, 2,
0, GL_RGBA, GL_UNSIGNED_BYTE, texels3);
glTexImage2D(GL_TEXTURE_2D, 4, GL_RGBA, 1, 1,
0, GL_RGBA, GL_UNSIGNED_BYTE, texels4);
As a convenience, OpenGL supports automatic ﬁltering and creation of
mipmap pyramids from a single image via the gluBuild2DMipmaps function.
The function arguments are very similar to those of glTexImage2D, with the
exception of the missing mipmap level and the loss of one other param-
eter (which we had ignored in the initial discussion of Chapter 6). After
generating the pre-ﬁltered mipmap data internally, gluBuild2DMipmaps calls
the equivalent of glTexImage2D on each of the mipmap levels. The preceding

8.8 Blending 401
code could be completely replaced with the following automatic mipmap
generation:
char* texels = new unsigned char[16 * 16 * 4];
// fill texels with the image data
// ...
// the entire mipmap pyramid
gluBuild2DMipmaps(GL_TEXTURE_2D, GL_RGBA, 16, 16,
GL_RGBA, GL_UNSIGNED_BYTE, texels);
In order to set the miniﬁcation method (or ﬁlter), glTexParameteri is called
with a setting parameter of GL_TEXTURE_MIN_FILTER. OpenGL supports both
non-mipmapped modes (bilinear ﬁltering and nearest-neighbor selection), as
well as all four mipmapped modes. The most common mipmapped mode (as
described previously) is trilinear ﬁltering, which is set using
// Trilinear filtering
glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER,
GL_LINEAR_MIPMAP_LINEAR);
8.8 Blending
Thus far, this chapter has discussed generating pixel addresses that represent
a triangle, as well as source colors that represent the color of the current trian-
gle at those pixels. The reason that we have referred to these as “source colors”
is that there is one more (optional) step in the rasterization pipeline, pixel
blending (or more simply, blending). Pixel blending is sometimes referred
to as alpha blending (which is really just a special case of general blending),
because it often involves blending (or interpolating) between the existing color
of the pixel and the new source color of the pixel based on the alpha value.
At long last, blending brings to closure the path from model-space geometry
to writing pixel colors and actually uses the alpha values we have been com-
puting, interpolating, and carrying through the pipeline! However, as we shall
see, pixel blending does not always use the alpha channel.
Once again, pixel blending is a per-pixel, nongeometric function that takes
as its inputs the source color of the current triangle at the given pixel (which
we will call Csrc), the source alpha value (which is properly a component of the
source color but which we will refer to as Asrc for convenience), the current
color of the pixel in the framebuffer (Cdst), and sometimes an existing alpha
value in the framebuffer at that pixel (Adst). These inputs, along with a pair of
blending functions Fsrc and Fdst, deﬁne the ﬁnal color (and potentially alpha

402
Chapter 8 Rasterization
value) that will be written to the pixel in the framebuffer, CP . The general
form of blending is
CP = FsrcCsrc + FdstCdst
The simplest form of pixel blending is to disable blending entirely, which
is equivalent to
Fsrc = 1
Fdst = 0
CP = FsrcCsrc + FdstCdst = (1)Csrc + (0)Cdst = Csrc
Alpha blending is a blending mode that involves using the source alpha
value Asrc as the opacity of the new triangle and linearly interpolating between
Csrc and Cdst based on Asrc:
Fsrc = Asrc
Fdst = (1 −Asrc)
CP = FsrcCsrc + FdstCdst = AsrcCsrc + (1 −Asrc)Cdst
Alpha blending requires that Cdst be referenced. Because Cdst is stored in the
framebuffer, alpha blending requires that the framebuffer be read for each
pixel blended. This increased memory bandwidth means that alpha blending
can impact performance on some systems (in a manner similar to z-buffering).
In addition, alpha blending has several other properties that make its use
somewhat challenging in practice.
Alpha blending is designed to compute a new color based on the idea
that the source pixel color represents the color of a (possibly translucent)
surface whose opacity is given by Asrc. As a result, alpha blending only uses
the alpha value of the source color, not the destination color. The destination
color is assumed to be the “background,” in front of which the translucent
source surface is placed. For the following discussion, we will write alpha
blending as
Blend(Csrc, Asrc, Cdst) = AsrcCsrc + (1 −Asrc)Cdst
The result of multiple alpha blending operations are order-dependent. Each
alpha blending operation assumes that Cdst represents the ﬁnal color of all
objects that are seen through the current surface. If we view the blending
of two possibly translucent surfaces (C1, A1) and (C2, A2) onto a background
color C0 as a sequence of two blends, we can quickly see that, in general,

8.8 Blending 403
changing the order of blending changes the result. If we compare the two
orders and expand the functions:
Blend(C2, A2, Blend(C1, A1, C0)) ?= Blend(C1, A1, Blend(C2, A2, C0))
A2C2 + (1 −A2)(A1C1 + (1 −A1)C0) ?= A1C1 + (1 −A1)(A2C2 + (1 −A2)C0)
A2C2 + (1 −A2)(A1C1 + C0 −A1C0) ?= A1C1 + (1 −A1)(A2C2 + C0 −A2C0)
−A1A2C1
?= −A1A2C2
A1A2C1
?= A1A2C2
These two sides are equal if and only if either A1 = 0, A2 = 0, or C1 = C2.
Thus, unless one of these three cases is true, the two blending orders will
produce different results. In visual terms, alpha blending of two surfaces with
a background color is order-independent if and only if
1. One or more of the two surfaces are completely translucent (in which
case they could be ignored, anyway).
2. Or, the two translucent surfaces have the same color as one another
(in which case the two blending operations could have been combined
into one).
In all other cases, we cannot switch the order of alpha blending operations.
8.8.1 Blending and Z-Buffering
In practice, order dependence when drawing alpha blended objects has sig-
niﬁcant effects on our visible-pixel algorithm, the z-buffer. Speciﬁcally, the
z-buffer is based around the theory that a surface at a given depth will com-
pletely obscure any surface at the pixel that is at a greater depth. With opaque
objects, this is true. However, in the presence of blending, it is not true,
because a surface that is to be blended relies on the color of the surfaces
behind it. In fact, the function in the preceding section is basically a recursive
function. Cdst must represent the ﬁnal combined color of all surfaces behind
the current surface. This is true for each alpha blended surface. Thus, in the
presence of alpha blending, we must compute the pixel color in a very speciﬁc
ordering. Given a set of “surfaces” (i.e., colors and depths at the current pixel),
the method of correctly coloring the pixel with alpha blending is as follows:
1. Compute
the
color
and
depth
of
the
closest
opaque
surface
(Copaque, Dopaque). Set this color as the initial Cdst and the depth as
the initial Ddst.

404
Chapter 8 Rasterization
2. For each translucent surface (Csrc, Asrc, Dsrc) that is closer than Dopaque
in order of far to near depth, set
Cdst = Blend(Csrc, Asrc, Cdst)
In terms of our standard z-buffering method, this is implemented at the
triangle level as
1. Collect the opaque triangles in the scene.
2. Collect the translucent triangles in the scene.
3. Render the opaque triangles normally, using z-buffering.
4. Sort the alpha blended triangles by depth into a far-to-near ordering.
5. Render the alpha blended triangles (in order) with blending, using
z-buffering.
Actually, since the alpha blended triangles are rendered in back-to-front order,
the z-buffer test in the ﬁnal step is only to ensure that no alpha blended pixel
whose depth is greater than the closest opaque triangle at that pixel will be
drawn. Thus, the depth of the alpha blended triangles need not be written to
the z-buffer. Many systems disable writing the z-buffer (but continue testing,
of course) when writing alpha blended pixels, in order to decrease the required
memory bandwidth. Since alpha blending already adds additional memory
bandwidth requirements, this can be a very useful optimization.
8.8.2 Alternative Blending Modes
As we have mentioned, depth-sorting of triangles is expensive and does not
always work without splitting triangles on a per-frame basis. If possible, we
would like to avoid depth-sorting the blended triangles. One popular trick to
avoid the sort is application-speciﬁc, but useful. If the blended objects can
“glow” or “ﬁlter” rather than alpha blend with the scene, then one of a pair
of other pixel-blending functions may be used. The two blending modes are
known as additive and modulate. Additive implements glowing objects and is
deﬁned as follows:
Fsrc = 1
Fdst = 1
CP = FsrcCsrc + FdstCdst = (1)Csrc + (1)Cdst = Csrc + Cdst
Note that this blending operation is clearly commutative and associative, and
thus no sorting is required. Note further that no alpha channel is required for
the effect.

8.8 Blending 405
Modulate blending implements color ﬁltering. It is similar to additive
blending in several ways and is deﬁned as
Fsrc = 0
Fdst = Csrc
CP = FsrcCsrc + FdstCdst = (0)Csrc + CsrcCdst = CsrcCdst
This blending operation is also commutative and does not involve the alpha
channel. Modulate blending is best known for creating so-called darkmap
effects, where a textured object is drawn once with its main texture, and then
the object is drawn again using modulate blending, this time with a texture
that represents the lighting applied to the scene. Put together, these two ren-
dering “passes” generate a far more complex effect, one of a detailed, textured
surface (the ﬁrst rendering pass, or base map) that is also lit by complex, sub-
tle lighting effects (the second pass, or “darkmap”). Darkmaps are so named
because the blending mode modulates the two passes, meaning that the result-
ing pixel color is always as dark or darker than either of the passes individually;
thus, the second pass darkens the ﬁrst pass — a “darkmap.” Other blending
effects are also possible.
Both additive and modulate blending modes still require the opaque
objects to be drawn ﬁrst, followed by the blended objects, but neither requires
the blended objects to be sorted into a depthwise ordering. As a result, these
blending modes (especially additive) are very popular with so-called particle
systems, which involve rendering hundreds or thousands of small, blended
triangles (generally to simulate smoke, water, or other natural phenomena)
and could be far too computationally expensive to sort on a per-frame basis.
Note that if depth buffering is used with these blending modes, the
blended objects (either additive or modulated) must be drawn with depth
buffer writing disabled, or else any out of order (front to back) rendering of
two blended objects will result in the more distant object not being drawn.
If depth buffer writing is enabled, the closer of the two blended objects will
write its depth value to the depth buffer ﬁrst, and the more distant object will
fail the depth buffer test. Again, disabling the depth buffer writes also offers
the advantage of further increasing performance on some systems by avoiding
additional memory write operations to the depth buffer.
8.8.3 Blending and OpenGL
Demo
Blending
Blending is enabled and controlled quite simply in OpenGL, although there
are many options beyond what we have discussed here. Enabling and dis-
abling blending are accomplished through the use of glEnable(GL_BLENDING)
and glDisable(GL_BLENDING), respectively. The blending modes are set via the

406
Chapter 8 Rasterization
function glBlendFunc, which sets both Fsrc and Fdst in a single function call.
To use classic alpha blending, the function call is
glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);
Additive mode is set using the call
glBlendFunc(GL_ONE, GL_ONE);
and modulate blending may be used via the call
glBlendFunc(GL_ZERO, GL_SRC_COLOR);
This interface is very ﬂexible and direct. There are far more blending func-
tions available in OpenGL (although, in practice, some hardware devices may
not be able to support all of them), and they are detailed in the OpenGL
Programming Guide [83]. The three modes described here are the most com-
mon, with alpha blending being universal. Other, more esoteric combinations
may not be supported. Note that while a hardware device may support all of
the possible source and destination functions, it may not support all possible
combinations thereof.
Recall that it is often useful to disable z-buffer writing while rendering
blended objects. This is accomplished via depth-buffer “masking.” A call to
glDepthMask(GL_TRUE) will enable writing the z-buffer — this is the default
setting in OpenGL. To disable writing the depth buffer, simply call glDepth-
Mask(GL_FALSE).
8.9 Antialiasing
In the absence of translucent objects, we have thus far discussed rasterizers
with the assumption that a single triangle “wins” a pixel and determines its
color. This is reasonable if we treat pixels as pure points, with no size. How-
ever, in our discussion of mipmapped textures, we saw that this is not the
case; each pixel represents a rectangular region on the screen with a nonzero
area. Because of this, more than one triangle may be visible inside of a pixel’s
rectangular region (just as more than one texel could fall within the region of
a pixel). Figure 8.24 provides an example of such a pixel.
Using the point-sampled methods discussed, we will select a color sam-
ple from a single triangle to represent the entire area of the triangle.

8.9 Antialiasing 407
Figure 8.24 Multiple triangles falling inside the area of a single pixel.
According to the z-buffer method, whichever triangle has the closest depth
value at the inﬁnitesimal sample point (located at the center of the pixel) will
win the pixel. However, as can be seen in Figure 8.25, this sample point may
not represent the color of the pixel as a whole. In the ﬁgure, we see that most
of the area of the pixel is dark gray, with only a very small square in the
center being bright white. As a result, selecting a pixel color of bright white
does not accurately represent the color of the pixel rectangle as a whole. Our
perception of the color of the rectangle has to do with the relative areas of
each color in the rectangle, something the point sampling method cannot
represent.
Figure 8.26 makes this even more apparent. In this situation, we see two
pixels. In both pixels, the vast majority of the surface area is dark gray. In
each of the two pixels, there is a small white rectangle. The white rectangles
are the same size in both triangles, but they are in slightly different positions
in each of the two pixels. In each of the top examples, the white rectangle
happens to contain the pixel center, while in the bottom cases, the white
rectangle does not contain the pixel centers. To the right of each pixel’s conﬁg-
uration is the color that will be assigned to that pixel. Very different colors are
assigned to these two pixels, even though their geometric conﬁgurations are

408
Chapter 8 Rasterization
Screen-space geometry
inside of pixels
Final on-screen color of pixels
Point sample falls in 
unrepresentative part of pixel
Entire pixel is assigned an 
unrepresentative color
Figure8.25 A point sample may not accurately represent the overall color of a pixel.
almost identical. This demonstrates the fact that point sampling of the color
of a pixel can lead to rather arbitrary results. In fact, if we imagine that the
white rectangle were to move across the screen over time, the pixel would ﬂash
between white and gray as the white rectangle moved through the pixel center.
It is possible to determine a more accurate color for the two pixels in the
ﬁgure. If the graphics system uses the relative areas of each color within the
pixel’s rectangle to weight the color of the pixel, the results will be much bet-
ter. In Figure 8.27, we can see that the white rectangle covers approximately
10 percent of the area of the pixel, leaving the other 90 percent as dark gray.
Weighting the color by the relative areas, we get a pixel color of
Carea = 0.1 × (1.0, 1.0, 1.0) + 0.9 × (0.25, 0.25, 0.25) = (0.325, 0.325, 0.325)
Note that this computation is independent of where the white rectangle falls
within the pixel (assuming the white rectangle is entirely within the pixel).
Such an area-based method avoids the point-sampling errors we have seen.
Such a system can be extended to any number of different-colored areas
within a given pixel. Given a pixel with area apixel and a set of n different
subsections of the pixel (each generated by a piece of visible geometry that
intersects the pixel’s rectangle), each with an area within the pixel ai and a

8.9 Antialiasing 409
Screen-space geometry
inside of pixels
Final on-screen color of pixels
Figure8.26 Sub-pixel motion causing a large change in point-sampled pixel color.
color Ci, the ﬁnal color of the pixel is then
$n
i=1 ai × Ci
apixel
=
n

i=1
ai
apixel
× Ci =
n

i=1
Fi × Ci
where Fi is the fraction of the pixel covered by the given color, or the
“coverage.” This method is known as area sampling. In fact, this is really a

410
Chapter 8 Rasterization
Pixel
Point-sampled pixel color
Point sample
location
Area-sampled pixel color
Screen space pixel coverage
10% coverage,
(1,1,1) color
90% coverage, (   ,   ,  ) color
1
4
1
4
1
4
Figure8.27 Area sampling of a pixel.
special case of a more general deﬁnite integral. If we imagine that we have a
screen-space function that represents the color of every point on the screen
(independent of pixels) C(x, y), then the color of a pixel deﬁned as the region
l ≤x ≤r, t ≤y ≤b (the left, right, top, and bottom screen coordinates of the
pixel), then using this area sampling method is equivalent to
% b
t
% r
l C(x, y)dxdy
% b
t
% r
l dxdy
=
% b
t
% r
l C(x, y)dxdy
(b −t)(r −l)
=
% b
t
% r
l C(x, y)dxdy
apixel
(8.4)
which is the integral of color over the pixel’s area, divided by the total area
of the pixel. The summation version of equation 8.4 is a simpliﬁcation of this
more general integral, using the assumption that the pixel consists entirely of
areas of piecewise constant color.
As a veriﬁcation of this method, we shall assume that the pixel is entirely
covered by a single triangle with ﬁxed color C(x, y) = CT , giving
% b
t
% r
l C(x, y)dxdy
apixel
=
% b
t
% r
l CT dxdy
apixel
= CT
% b
t
% r
l dxdy
apixel
= CT
apixel
apixel
= CT
(8.5)
which is the color we would expect in this situation.

8.9 Antialiasing 411
While area sampling does avoid completely missing or overemphasizing
any single sample, it is not the only method used, nor is it the best at represent-
ing the realities of display devices. The area sampling shown in equation 8.5
implicitly weights all regions of the pixel equally, giving the center of the pixel
weighting equal to that of the edges. As a result, it is often called unweighted
area sampling. Weighted area sampling, on the other hand, adds a weighting
function that can bias the importance of the colors in any region of the pixel
as desired. If we simplify the original pixel boundaries and the functions asso-
ciated with equation 8.4 such that boundaries of the pixel are 0 ≤x, y ≤1,
then equation 8.4 becomes
% b
t
% r
l C(x, y)dxdy
% b
t
% r
l dxdy
=
% 1
0
% 1
0 C(x, y)dxdy
1
(8.6)
Having simpliﬁed equation 8.4 into equation 8.6, we deﬁne a weighting
function W(x, y) that allows regions of the pixel to be weighted as desired:
% 1
0
% 1
0 W(x, y)C(x, y)dxdy
% 1
0
% 1
0 W(x, y)dxdy
(8.7)
In this case, the denominator is designed to normalize according to the
weighted area. A similar substitution to equation 8.5 shows that constant col-
ors across a pixel map to the given color. Note also that (unlike unweighted
area sampling) the position of a primitive within the pixel now matters. From
equation 8.7, we can see that unweighted area sampling is simply a special
case of weighted area sampling. With unweighted area sampling, W(x, y) = 1,
giving
% 1
0
% 1
0 W(x, y)C(x, y)dxdy
% 1
0
% 1
0 W(x, y)dxdy
=
% 1
0
% 1
0 (1)C(x, y)dxdy
% 1
0
% 1
0 (1)dxdy
=
% 1
0
% 1
0 C(x, y)dxdy
% 1
0
% 1
0 dxdy
=
% 1
0
% 1
0 C(x, y)dxdy
1
A full discussion of weighted area sampling, the theory behind it, and
numerous common weighting functions is given in [36]. For those desiring
more depth, [120] and [41] detail a wide range of sampling theory.

412
Chapter 8 Rasterization
8.9.1 Antialiasing in Practice
The methods so far discussed show theoretical ways for computing area-based
pixel colors. These methods require that pixel-coverage values be computed
per triangle, per pixel. Computing analytical (exact) pixel coverage values for
triangles can be complicated. In practice, the pure area-based methods do not
lead directly to simple, fast hardware antialiasing implementations.
Consumer 3D hardware is almost universally based upon the point sam-
pling methods discussed earlier in this chapter. It is only natural that
hardware developers would seek to create antialiasing-capable hardware that
did not require entirely new, area-based sampling techniques. The most pop-
ular form of antialiasing on consumer hardware is based on sampling at
multiple points inside of each pixel. This is known as multisample antialiasing.
Area-based sampling is approximated by point sampling the scene at as few
as two samples per pixel and as many as 16 or more samples per pixel.
Figure 8.28 shows some sample patterns. Each square represents a single
pixel. Filled (dark) circles represent the locations of rendered (rasterized)
sample points in the pixel. Unﬁlled (white) sample points in the ﬁgure are
4-sample
9-tap
2-sample
5-tap
2-sample
2-tap
4-sample
4-tap
Color rendered for the current pixel
Color reused from an adjacent pixel
1
2
1
2
1
4
1
4
1
4
1
4
1
8
1
8
1
2
1
8
1
8
1
16
1
16
1
8
1
16
1
16
1
8
1
4
1
8
1
8
Figure8.28 Common sample-point distributions for multisample-based
antialiasing.

8.9 Antialiasing 413
actually samples taken from adjacent pixels, reused for the current pixel.
In common multisample antialiasing nomenclature, a “sample” refers to a
color that is computed by actually rasterizing triangles at the current pixel,
while a “tap” refers to a more general notion — a color that may either be a
sample rendered for the current pixel or a sample that is simply reused from
another pixel. All conﬁgurations with more “taps” than “samples” are reusing
samples from other pixels as additional (low-cost) taps. As a result, it is the
number of samples that best represents the rasterization expense for a given
conﬁguration. Reusing samples from other pixels as taps for a given pixel
gives some of the beneﬁts of a higher number of samples per pixel without the
rasterization expense of additional per-pixel samples.
The colors of each of these samples are combined into a single pixel color
via a weighted (or in some cases unweighted) sum. Common weights used
with weighted-area versions of these sampling patterns are also shown in
Figure 8.28.
Some systems that support these subpixel samples support two forms of
multi-sample antialiasing. The ﬁrst is automatic multi-sample antialiasing; a
simple, easy-to-use system (essentially the one just described) that automat-
ically places and renders the subpixel samples and then sums them into the
ﬁnal pixel color. The other mode is a manual mode, in which the application
can choose to “mask” (i.e., disable) some of the samples and render colors
to as few as one per-pixel sample per rendering “pass.” This latter system
requires the application to deal with the setup and rendering of each pass,
but can allow for incredible ﬂexibility. In fact, this manual mode can allow
antialiasing in multiple dimensions, including
■Temporal sampling. The samples in a given pixel are each rendered
at different values of “game time,” with the scene and camera ani-
mated between each sample. This simulates motion blur, by causing
the samples to represent the time over which the camera’s “shutter”
is open.
■Optical sampling. This is done by taking the samples from multiple,
slightly different camera positions, which represent the centers of pro-
jection on the surface of a lens. The camera matrix is chosen such that
points on the focal plane of the camera (a ﬁxed distance into the scene)
are the same across all subpixel samples. The resulting image will look
perfectly sharp for objects at the focal plane and increasingly blurry
away from the focal plane.
■Area lights. For systems that can render sharp shadows based on
some lights, soft shadows can be created by rendering each subpixel
sample with the exact position of the shadow-generating light shifted
slightly from the other samples. In this way, once all of the samples are

414
Chapter 8 Rasterization
rendered, surface points in the umbras of shadows will be the darkest
(as they are in shadow for all of the jittered light positions), and surface
points in the penumbrae of shadows will be lighter (as they will be in
shadow from only some of the jittered light positions).
8.9.2 Antialiasing in OpenGL
OpenGL (in its unextended form) supports two forms of antialiasing; pixel-
coverage antialiasing and the accumulation buffer. The ﬁrst (and older) of
these two, pixel-coverage antialiasing, is based on the direct computation
(by the OpenGL implementation) of fractional pixel-coverage values for each
triangle, per pixel. These fractional coverage values are analogous to the Fi
values deﬁned in Section 8.9. During rendering, these pixel-coverage values
are used as alpha values in a pixel blending operation. Each triangle is blended
with the existing pixel color, according to its coverage value.
Pixel-coverage antialiasing uses pixel blending, meaning that alpha blend-
ing and other such effects cannot be used simultaneously with this method of
antialiasing. As with other operations involving pixel blending, depth buffer-
ing cannot resolve the visible surfaces correctly when using pixel-coverage
antialiasing. The geometry must be rendered in back-to-front order manually,
using some form of geometric sorting.
Pixel-coverage antialiasing is unsuitable for most modern, complex
scenes, owing to the fact that it requires depth sorting of geometry and is
incompatible with alpha blending. Readers interested in learning the details
of using pixel-coverage antialiasing in OpenGL should read the OpenGL
Programming Guide [83].
Owing to its heritage as an API for high-end graphics workstations,
OpenGL also includes built-in support for a rather advanced form of antialias-
ing known as an accumulation buffer, or a-buffer (see [52] and [83] for details).
While consumer 3D hardware support for a-buffers is far from universal, the
concept of an a-buffer is worth discussing, owing to its powerful, general
nature. Basically, the accumulation buffer is simply an extra, off-screen
framebuffer. Accumulation buffers generally use higher-resolution color com-
ponents (e.g., 10–16 bits per component) than the main framebuffer, to avoid
color quantization artifacts. There are four basic operations with an a-buffer:
■Clear the a-buffer pixels to a color (glClear(GL_ACCUM_BUFFER_BIT))
■Copy the framebuffer pixels (multiplied by a ﬂoating-point constant,
mult) in the a-buffer (glAccum(GL_LOAD, mult))
■Add the framebuffer pixels (multiplied by a ﬂoating-point constant,
mult) in the a-buffer (glAccum(GL_ACCUM, mult))

8.9 Antialiasing 415
■Copy the a-buffer pixels (multiplied by a ﬂoating-point constant, mult)
back into the framebuffer (glAccum(GL_RETURN, mult))
This set of operations is deceptively simple and allows an immense range of
options. For example, multisample antialiasing using N samples per pixel can
be computed with the a-buffer as follows:
// Clear the a-buffer
glClear(GL_ACCUM_BUFFER_BIT);
// For subpixel samples 1 <= i <= N
for (unsigned int i = 1; i <= N; i++)
{
// Clear the framebuffer and z-buffer
glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
// Move the camera to subpixel sample position i
// Render the scene to the framebuffer
// ...
// Accumulate the framebuffer into the a-buffer,
// scaled by 1/N
glAccum(GL_ACCUM, 1.0f/N);
}
// Read back the a-buffer into the framebuffer
glAccum(GL_RETURN, 1.0f);
// Display the framebuffer
// ...
Basically, this pseudocode renders the entire scene N times, once from each
of N (slightly shifted) camera positions. These camera positions represent the
positions of the subpixel samples at each pixel. So, if we wish to render a 3×3
grid of subpixel samples, we would render the scene nine times, computing a
single subpixel sample for all pixels in each rendering.
The semantics of a-buffering make a-buffers expensive for 3D hardware
to implement. The additional high-precision buffer requires considerable
additional framebuffer memory. In addition, the required copy operations
from framebuffer to a-buffer and back are computationally expensive. Most
current hardware devices simply cannot support the accumulation buffer
interfaces at high performance, supporting only multisample antialiasing.
Hardware vendors have made other methods of antialiasing (especially so-
called “single pass” methods that render all subpixel samples for the entire
screen in a single rendering pass) available via OpenGL extensions such as

416
Chapter 8 Rasterization
GL_ARB_multisample and via Microsoft’s Direct3D multisample pixel formats.
These multi-sample implementations can often support both automatic sub-
pixel spatial antialiasing and the more complex multipass effects using sample
masking. The developer Web sites of the popular 3D hardware vendors ([6],
[82]) include detailed discussions of their devices’ support for these features
in both rendering APIs.
8.10 Chapter Summary
This chapter concludes the discussion of the rendering pipeline. Rasterization
provides us with some of the lowest-level yet most mathematically interesting
concepts in the entire pipeline. We have discussed the connections between
mathematical concepts such as projective transforms and rendering meth-
ods such as perspective-correct texturing. In addition, we addressed issues
of mathematical precision in our discussion of the depth buffer. Finally, the
concept of point sampling versus area sampling appeared twice, relating to
both mipmapping and antialiasing. Whether it is implemented in hardware,
software, or a mixture of the two, the entire graphics pipeline is ultimately
designed only to feed a rasterizer, making the rasterizer one of the most
important yet least understood pieces of rendering technology.
Thanks to the availability of high-quality, low-cost 3D hardware on a wide
range of platforms, the percentage of readers who will ever have to implement
their own rasterizer is quite small. However, an understanding of how raster-
izers function is important even to those who will never need to write one. For
example, even a basic practical understanding of the z-buffering system can
help a programmer build a scene that avoids visual artifacts during visible
surface determination. Understanding the inner workings of rasterizers can
help a 3D programmer quickly debug problems in the geometry pipeline.
Finally, this knowledge can guide the programmer to better optimize their
geometry pipeline, “feeding” their rasterizer with high-performance datasets.
For further reading, we recommend the OpenGL Programming Guide [83],
which details many more features of OpenGL, especially as they relate to
rasterizing, texturing, and antialiasing. As referenced numerous times in this
and other chapters, Chris Hecker’s series on perspective texture mapping [60]
is an excellent introduction to the many details that must be considered when
designing a high-performance software rasterization system.

Part
III
Animation


Chapter9
Curves
9.1 Introduction
Up to this point, we have considered only motion (more speciﬁcally, trans-
formations) that has been created programmatically. In order to create
a particular motion (e.g., a submarine moving through the world), we have to
write a speciﬁc program to generate the appropriate sequence of transforma-
tions for our model. However, this takes time and it can be quite tedious to
move objects in this fashion. It would be much more convenient to predeﬁne
our transformation set in a tool and then somehow regenerate it within our
game. An artist could create the sequence using a modeling package, and then
a programmer would just write the code to play it back, much as a projector
plays back a strip of ﬁlm. This process of pregenerating a set of data and then
playing it back is known as animation.
The best way to understand animation is to look at the art form in which it
has primarily been used: motion pictures. In this case, the illusion of motion
is created by drawing or otherwise recording a series of images on ﬁlm and
then projecting them at 24 or 30 frames per second (for ﬁlm and video, respec-
tively). The illusion is maintained by a property of the eye-brain combination
known as persistence of motion: the eye-brain system sees two frames and
invisibly (to our perception) ﬁlls in the gaps between them, thus giving us the
notion of smooth motion.
We could do something similar in our game. Suppose we had a character
that we want to move around the world. The artist could generate various
animation sets at 60 frames per second (f.p.s.), and then when we want the
character to run, we play the appropriate running animation. When we want
419

420
Chapter 9 Curves
the character to walk, we switch to the walking animation. The same process
can be used for all the possible motions in the game.
However, there are a number of problems with this. First, by setting the
animation set to a rate of 60 frames per second and then playing it back
directly, we have effectively locked the frame rate for the game at 60 f.p.s.
as well. Many monitors can run at 85 f.p.s., and when running in windowed
mode, the graphics can be updated much faster than that. It would be much
better if we could ﬁnd some way to generate 85 f.p.s. or more from a 60 f.p.s.
dataset. In other words, we need to take our initial dataset and generate a new
one at a different rate. This is known as resampling.
This brings us to our second problem. Storing 60 f.p.s per animation adds
up to a lot of data. As an example, if we have 10 data points per model that
we’re storing, with 16 ﬂoats per point (i.e., a 4 × 4 matrix), that adds up to
about 38 KB per second of animation. A minute of animation adds up to
over 2 MB of data, which can be a serious hit, particularly if we’re running
on a low-memory platform such as a console. It would be better if we could
generate our data at a lower rate, say 10 or 15 f.p.s., and then resample up to
the speed we need. This is essentially the same problem as our ﬁrst one — it’s
just that our initial data set has fewer samples.
Alternately, we could take another cue from movie animation. The pri-
mary animators on a ﬁlm draw only the important, infrequent “key” frames
that capture the essential ﬂow of an animation. The work of generating the
remaining “in between” frames is left to secondary animators, who generate
these intermediate frames from the supplied key frames. These artists are
known as ’tweeners. In our case, we could store key frames that store the
essential positions of our motion. These key frames would not have to be
separated by a constant time interval, but at smaller intervals when the posi-
tions are changing quickly, and at larger intervals when the positions change
very slowly. The resampling function would act as our ’tweener for this key
frame data.
Fortunately, we have already been introduced to one technique for doing
all of this, albeit in another form. This method is known as interpolation, and
we ﬁrst saw it when generating a line from two points. Interpolation takes a set
of discrete sample points at given time intervals and generates a continuous
function that passes through the points. Using this, we can pick any time along
the domain of the function and generate a new point so that we might ﬁll in
the gaps. We’re using the interpolation function to sample at a different rate.
An alternative is approximation, which uses the points to guide the result-
ing function. In this case the function does not pass through the points. This
may seem odd, but it can help us better control the shape of the function.
However, the same principle applies: we generate a function based on the
initial sample data and resample later at a different frame rate. The general
class of functions we’ll be using for both interpolating and approximating are
called parametric curves.

9.2 General Definitions 421
9.2 General Definitions
A parametric curve is a function Q(u) that maps a set of real values (represented
by the parameter u) to a set of points. The derivative Q′(u) for parameter u
is a tangent vector to the curve at location Q(u). When mapping to R3, we
commonly use a parametric curve broken into three separate functions, one
for each coordinate: Q(u) = (x(u), y(u), z(u)). This is also known as a space
curve. The derivative of a space curve is Q′(u) = (x′(u), y′(u), z′(u)).
When curves are used for animation, the parameter u or t usually repre-
sents time, although the units used don’t necessarily have any relationship to
seconds. In our discussion we will often use u as the parameter to a normal-
ized curve such that Q(0) is the start of the curve and Q(1) is the end. When
we want to use a general parameterization, we will refer to the parameter t.
In this case we usually set a time value ti for each point Pi; we expect to end
up at position Pi in space at time ti. The sequence t0, t1, . . . , tn is sorted (as are
the corresponding points) so that it is monotonically increasing.
The average speed r we travel along a curve is related to the distance d
traveled along the curve and the time it takes to travel that distance, namely,
r = d/u
The instantaneous speed at a particular parameter u is the length of the
derivative vector Q′(u).
For a given point P on a smooth curve Q(u), we deﬁne a circle with ﬁrst
and second derivative vectors equal to those at P as the osculating1 circle.
If the radius of the osculating circle is ρ, the curvature κ at P is 1/ρ. The
curvature at any point is always nonnegative. The higher the curvature, the
more the curve bends at that point; the curvature of a straight line is 0.
In general, it is not practical to construct a single, closed form polynomial
that uses all of the sample points — most of the curves we will discuss use
at most four points as their geometric foundation. Instead, we will create
curve segments that each apply over a sequential subset of the points and join
these segments together to create a function across the entire domain. How we
create this joint determines the type of continuity we will have in our function.
Formally, we say that a function f is continuous at a value x0 if
lim
x→x0 f (x) = f (x0)
In addition, we say that a function f (x) is continuous over an interval (a, b) if it
is continuous for every value x in the interval. We can also say that the function
has positional, or C0, continuity over the interval (a, b). Informally, we can
1.
So called because it “kisses” up to the point.

422
Chapter 9 Curves
think of a continuous function as one that we can draw without ever lifting the
pen from the page. When using curve segments, we can achieve C0 continuity
by ensuring that the end point of one curve segment is equal to the start point
of the next segment.
This can be taken further: a function f (x) has tangential, or C1, continuity
across an interval (a, b) if the ﬁrst derivative f ′(x) of the function is continuous
across the interval. We can achieve C1 continuity when using curve segments
by guaranteeing that tangent vectors are equal at the end of one segment
and the start of the next segment. A related form of continuity is G1 continu-
ity, where the tangents at each segment are not necessarily equal but point
in the same direction. In many cases G1 continuity is good enough for our
purposes.
Occasionally, we may be concerned with C2 continuity, also known as
curvature continuity. A function f (x) has C2 continuity across an interval
(a, b) if the second derivative f ′′(x) of the function is continuous across the
interval. Higher orders of continuity are possible, but they are not relevant to
the discussion that follows.
9.3 Linear Interpolation
9.3.1 Definition
The most basic parametric curve is a line passing through two points. By using
the parameterized line equation based on the two points, we can generate
any point along the line. This is known as linear interpolation and is the most
commonly used form of interpolation in game programming, mainly because
it is the fastest. From our familiar line equation
Q(u) = P0 + u(P1 −P0)
we can rearrange to get
Q(u) = (1 −u)P0 + uP1
The value u is the factor we use to control our interpolation, or parameter.
Recall that if u is 0, Q(u) returns our starting point P0, and if u is 1, then Q(u)
returns P1, our end point. Values of u between 0 and 1 will return a point
along the line segment P0P1. When interpolating, we usually care only about
values of u within the interval [0, 1] and, in fact, state that the interpolation is
undeﬁned outside of this interval.
It is common when creating parametric curves to represent them as matrix
equations. As we’ll see next, it makes it simple to set certain conditions

9.3 Linear Interpolation 423
for a curve and then solve for the equation we want. The standard matrix
form is
Q(u) = U · M · G
where U is a row matrix containing the polynomial interpolants we’re using:
1, u,u2,u3,and so on; M is a matrix containing the coefﬁcients necessary for
the parametric curve; and G is a matrix containing the coordinates of the
geometry that deﬁnes the curve. In the case of linear interpolation
U =
 u
1 
M =
 −1
1
1
0

G =
 x0
y0
z0
x1
y1
z1

With this formulation, the result UMG will be a 1 × 3 matrix:
UMG =
 x(u)
y(u)
z(u) 
=
 (1 −u)x0 + ux1
(1 −u)y0 + uy1
(1 −u)z0 + uz1

This is counter to our standard convention of using column vectors. However,
rather than write out G as individual coordinates, we can write G as a column
matrix of n points, where for linear interpolation this is
G =
 P0
P1

Then, using block matrix multiplication, the result UMG becomes
UMG = (1 −u)P0 + uP1
This form allows us to use a convenient shorthand to represent a general
parameterized curve without having to expand into three essentially similar
functions.
Recall that in most cases we are given time values t0 and t1 that are asso-
ciated with points P0 and P1, respectively. In other words, we want to start
at point P0 at time t0 and end up at point P1 at time t1. These times are not
necessarily 0 and 1, so we’ll need to remap our time value t in the interval
[t0, t1] to a parameter u in the interval [0, 1], which we’ll use in our original

424
Chapter 9 Curves
interpolation equation. If we want the percentage u that a time value t lies
between t0 and t1, we can use the formula
u = t −t0
t1 −t0
(9.1)
Using this parameter u with the linear interpolation will give us the effect we
desire. We can use this approach to change any curve valid over the interval
[0, 1] using u as a parameter to be valid over [t0, t1] using t as a parameter.
9.3.2 Piecewise Linear Interpolation
Demo
Linear
Pure linear interpolation works ﬁne if we have only two values, but in most
cases we will have many more than two. How do we interpolate among
multiple points?
The simplest method is to linearly interpolate from the ﬁrst point to the
second, then from the second point to the third, and so on, until we get to
the end. For each pair of points Pi and Pi+1, we use equation 9.1 to adjust the
time range [ti, ti+1] to [0, 1] so we can interpolate properly.
For a given time value t, we need to ﬁnd the stored time values ti and
ti+1 such that ti ≤t ≤ti+1. From there we look up their corresponding Pi and
Pi+1 values and interpolate. If we start with n + 1 points, we will end up with
a series of n segments labeled Q0, Q1, . . . , Qn−1. Each Qi is deﬁned by points
Pi and Pi+1 where
Qi(u) = (1 −u)Pi + uPi+1
and Qi(1) = Qi+1(0). This last condition guarantees C0 continuity.
Expressed as code:
IvVector3 EvaluatePiecewiseLinear( float t, unsigned int count,
const IvVector3* positions,
const float* times)
{
// handle boundary conditions
if ( t <= times[0] )
return positions[0];
else if ( t >= times[count-1] )
return positions[count-1];
// find segment and parameter
unsigned int i;
for ( i = 0; i < count-1; ++i )
{

9.4 Lagrange Polynomials 425
P0
P1
P2
P3
Q0
Q1
Q2
Figure 9.1 Piecewise linear interpolation.
if ( t < times[i+1] )
break;
}
float t0 = times[i];
float t1 = times[i+1];
float u = (t - t0)/(t1 - t0);
//evaluate
return (1-u)*positions[i] + u*positions[i+1];
}
In the pseudocode we found the subcurve by using a straight linear search.
For large sets of points, using a binary search will be more efﬁcient since we’ll
be storing the values in sorted order. We can also use temporal coherence:
since our time values won’t be varying wildly and will be increasing in value,
we can ﬁrst check whether we lie in the interval [ti, ti+1] from the last frame
and then check subsequent intervals.
This works reasonably well and is quite fast, but as Figure 9.1 demon-
strates, will lead to sharp changes in direction. If we treat the piecewise
interpolation of n + 1 points as a single function f (t) over [t0, tn], we ﬁnd that
the derivative f ′(t) is discontinuous at the sample points, so f (t) is not C1
continuous. In animation this expresses itself as sudden changes in the speed
and direction of motion, which may not be desirable. Despite this, because of
its speed, piecewise linear interpolation is a reasonable choice if the slopes of
the piecewise line segments are relatively close. If not, or if smoother motion
is desired, other methods using higher order polynomials are necessary.
9.4 Lagrange Polynomials
Demo
Lagrange
One way to create smoother motion is to generate a polynomial function
that will pass through every point. So if we have three sample points, we will
generate a quadratic function; if we have four sample points, a cubic function;
and so on. The most common method to generate such a curve is to use a set of

426
Chapter 9 Curves
generalized functions known as Lagrange polynomials. They allow us to take
a set of any n+1 points P0, . . . , Pn, along with their corresponding time values
t0, . . . , tn, and construct an n-degree polynomial. For example, if we have two
points, the corresponding Lagrange polynomial is a ﬁrst-degree polynomial
or a line, as we expect. If we have three noncollinear points, we can create
a quadratic equation that passes through all three points.
The general form of the Lagrange polynomial is
P(t) =
n

k=0
PkLn,k(t)
where
Ln,k(t) =
(t −t0)(t −t1) · · · (t −tk−1)(t −tk+1) · · · (t −tn)
(tk −t0)(tk −t1) · · · (tk −tk−1)(tk −tk+1) · · · (tk −tn)
=
n
&
i=0,i̸=k
(t −ti)
(tk −ti)
(9.2)
Equation 9.2 is known as the Lagrange product. Let’s take a closer look. For
the kth equation, if we substitute tk for t, we get
Ln,k(tk) = (tk −t0)(tk −t1) · · · (tk −tk−1)(tk −tk+1) · · · (tk −tn)
(tk −t0)(tk −t1) · · · (tk −tk−1)(tk −tk+1) · · · (tk −tn)
= 1
Otherwise, if we substitute tk for t in any of the other Lagrange products,
we get
Ln,j(tk) =
(tk −t0) · · · (tk −tk) · · · (tk −tj−1)(t −tj+1) · · · (t −tn)
(tj −t0) · · · (tj −tk) · · · (tj −tj−1)(tj −tj+1) · · · (tj −tn)
= 0
So for a given tk, P(tk) returns Pk, which is what we expect.
If we have two points, the corresponding Lagrange polynomial is
P(t) = (t −t1)
(t0 −t1)P0 + (t −t0)
(t1 −t0)P1
If our two points are at time values t0 = 0 and t1 = 1, then
P(t) = (t −1)
−1
P0 + (t −0)
1
P1
= (1 −t)P0 + tP1

9.5 Hermite Curves 427
So the Lagrange polynomial with two points is our standard linear interpola-
tion formula.
Three points gives us the following equation:
P (t) = (t −t1)(t −t2)
(t0 −t1)(t0 −t2)P0 + (t −t0)(t −t2)
(t1 −t0)(t1 −t2)P1 + (t −t0)(t −t1)
(t2 −t0)(t2 −t1)P2
(9.3)
Substituting our time values for each point and simplifying the equation
generates a quadratic equation that will interpolate from P0 to P1 to P2.
This works ﬁne for small numbers of points. But suppose we have
a larger dataset of, say, 23 points and time values. Or a number of differ-
ent datasets, each with different numbers of sample points and times. One
possibility would be to generate the Lagrange equation for each data set and
then simplify to a less complicated equation. If our data is ﬁxed, this works
ﬁne, but usually animators will tweak their values throughout the develop-
ment of a game. An animation may change in time value or in the number
of sample points. If this happens, the entire Lagrange equation is invalid and
would have to be recalculated.
Based on the assumption that our data is going to be changing frequently,
we could use the generalized Lagrange equation, but that would involve
at least 22 multiplications and subtractions per sample point, for 23 sam-
ples, or 506 multiplications and subtractions total. This is not very efﬁcient
and would grow worse with more points.
Lagrange polynomials have other issues that make them impractical for
our purposes. An animator can’t adjust the curve other than by moving points
or adjusting t values, which is both unwieldy and inﬂexible. And when interpo-
lating large numbers of points, the curve tends to oscillate in order to maintain
continuity and pass through every point. Lagrange polynomials also run into
numerical problems with larger and larger numbers of points. Because of
this, they are ﬁne for interpolating small datasets, but other methods are
more useful for real animation data.
9.5 Hermite Curves
9.5.1 Definition
Demo
Hermite
Clearly, trying to build a single parametric curve by using all of the points is
not going to be a productive method. Instead, let’s return to the idea of piece-
wise equations. But this time, instead of using piecewise linear equations,
which give us discontinuities in the derivative at the sample points, we will
use higher-order equations, in particular cubic curves. If we control the curve
properly at each point, then we can smoothly transition from one point to the

428
Chapter 9 Curves
P1
P0
Q0
P′1
P′0
Figure 9.2 Hermite curve.
next, avoiding the obvious discontinuities. In particular, what we want to do
is to set up our piecewise curves so that the tangent at the end of one curve
matches the tangent at the start of the next curve. This will remove the ﬁrst
order discontinuity at each point — the derivative will be continuous over the
entire time interval that we are concerned with.
Why a cubic curve and not a quadratic curve? Take a look at Figure 9.2.
We have set two positions P0 and P1, and two tangents P′
0 and P′
1. Clearly,
a line won’t pass through the two points and also have a derivative at each
point that matches its corresponding tangent vectors. The same is true for
a parabola. The next order curve is cubic, which will satisfy these conditions.
Intuitively, this makes sense. A line is constrained by two points, or one point
and a vector. A parabola can be deﬁned by three points, or by two points and
a tangent. And a cubic curve can be deﬁned by four points, or two points and
two tangents.
Using our given constraints, or boundary conditions, let’s derive our cubic
equation. A generalized cubic function and corresponding derivative are
Q(u) = au3 + bu2 + cu + D
(9.4)
Q′(u) = 3au2 + 2bu + c
(9.5)
We’ll solve for our four unknowns a, b, c, and D by using our four
boundary conditions. We’ll assume that when u = 0, Q(0) = P0 and Q′(0) = P′
0.
Similarly, at u = 1, Q(1) = P1 and Q′(1) = P′
1. Substituting these values into
equations 9.4 and 9.5, we get
Q(0) = D = P0
(9.6)
Q(1) = a + b + c + D = P1
(9.7)

9.5 Hermite Curves 429
Q′(0) = c = P′
0
(9.8)
Q′(1) = 3a + 2b + c = P′
1
(9.9)
We can see that equations 9.6 and 9.8 already determine that c and D
are P′
0 and P0, respectively. Substituting these into equations 9.7 and 9.9 and
solving for a and b gives
a = 2(P0 −P1) + P′
0 + P1
b = 3(P1 −P0) −2P′
0 −P′
1
Substituting our now known values for a, b, c, and D into equation 9.4
gives:
Q(u) =

2(P0 −P1) + P′
0 + P′
1

u3 +

3(P1 −P0) −2P′
0 −P′
1

u2 + P′
0u + P0
This can be rearranged in terms of the boundary conditions to produce our
ﬁnal equation:
Q(u) = (2u3 −3u2 + 1)P0 + (−2u3 + 3u2)P1 + (u3 −2u2 + u)P′
0 + (u3 −u2)P′
1
This is known as a Hermite curve. We can also represent this as the product of
a matrix multiplication, just as we did with linear interpolation. In this case,
the matrices are
U =

u3
u2
u
1

M =


2
−2
1
1
−3
3
−2
−1
0
0
1
0
1
0
0
0


G =


P0
P1
P′
0
P′
1


We can use either formulation to build piecewise curves just as we did for
linear interpolation. As before, we can think of each segment as a separate
function, valid over the interval [0, 1]. Then to create a C1 continuous curve,
two adjoining segments Qi and Qi+1 would have to have matching positions
such that
Qi(1) = Qi+1(0)

430
Chapter 9 Curves
and matching tangent vectors such that
Q′
i(1) = Q′
i+1(0)
What we end up with is a set of sample positions {P0, . . . , Pn}, tangent vec-
tors {P′
0, . . . , P′
n}, and times {t0, . . . , tn}. At a given point adjoining two curve
segments Qi and Qi+1
Qi(1) = Qi+1(0) = Pi+1
Q′
i(1) = Q′
i+1(0) = P′
i+1
Figure 9.3 shows this situation in the piecewise Hermite curve.
The tangent vectors are used for more than just maintaining ﬁrst-
derivative continuity across each sample point. Changing their magnitude
also controls the speed at which we move through the point and consequently
through the curve. They also affect the shape of the curve. Take a look at
Figures 9.4a and 9.4b. The longer the vector, the faster we will move and the
sharper the curvature. We can create a completely different curve through
our sample points, simply by adjusting the tangent vectors.
There is, of course, no reason that the tangents Q′
i(1) and Q′
i+1(0) have
to match. One possibility is to match the tangent directions but not the tan-
gent magnitudes — this gives us G1 continuity. The resulting function has
a discontinuity in its derivative but usually still appears smooth. It also has
the advantage that it allows us to control how our curve looks across each
segment a little better. For example, it might be that we want to have the
appearance of a continuous curve but also be able to have more freedom in
how each individual segment is shaped. By maintaining the same direction
but allowing for different magnitudes, this function provides for the kind of
ﬂexibility we need in this instance (Figure 9.5).
P1
P0
Q0
P′1
Q′0(1)
Q′1(0)
P′0
Q1
P′2
P2
Figure 9.3 Piecewise Hermite curve.
Tangents at P1 match direction and
magnitude.

9.5 Hermite Curves 431
P1
P0
P′1
P′0
(a)
P1
P0
P′1
P′0
(b)
Figure 9.4 Hermite curve with (a) small tangent and low curvature (b) large
tangent and higher curvature.
P1
P0
P2
Q0
Q′1(0)
Q′0(1)
Q1
Figure 9.5 Piecewise Hermite curve. Tangents at P1 have same direction but
differing magnitudes.
Another possibility is that the tangent directions don’t match at all. In this
case we’ll end up with a kink, or cusp, in the whole curve (Figure 9.6). While
not physically realistic, it does allow for sudden changes in direction. The
combination of all the possibilities at each sample point — equal tangents,
equal tangent directions with non-equal magnitudes, and non-equal tangent
directions — gives us a great deal of ﬂexibility in creating our interpolating

432
Chapter 9 Curves
P1
P0
P2
Q0
Q0(1)
Q1(0)
Q1
Figure 9.6 Piecewise Hermite curve. Tangents at P1 have differing directions and
magnitudes.
function across all the sample points. To allow for this level of control, we
need to set two tangents at each internal sample point, which we’ll express as
P′
i,1 and P′
i+1,0. Alternatively, we can think of a curve segment as being deﬁned
by two points Pi and Pi+1, and two tangents P′
i,0 (the “incoming” tangent) and
P′
i+1,1 (the “outgoing” tangent).
One question remains: how do we generate these tangents? One simple
answer is that most existing tools that artists will use, such as Alias’s Maya and
Discreet’s 3D Studio Max, provide ways to set up Hermite curves and their
corresponding tangents. When exporting the sample points for subsequent
animation, we export the tangents as well. Some tweaking may need to be
done to guarantee that the curves generated in internal code match that in the
artist program; information on a particular representation is usually available
from the manufacturer.
Another common way of generating Hermite data is using in-house tools
built for a speciﬁc purpose — for example, a tool for managing paths for
cameras and other animated objects. In this case, an interface will have to be
created to manage construction of the path. One possibility is to click to set the
next sample position, and then drag the mouse away from the sample position
to set tangent magnitude and direction. A line segment with an arrowhead can
be drawn showing the outgoing tangent, and a corresponding line segment
with a tail drawn showing the incoming tangent (Figure 9.7).
P1
P0
P2
Figure 9.7 Possible interface for Hermite curves, showing in–out tangent vectors.

9.5 Hermite Curves 433
We will need to modify the tangents so that they can either have differ-
ent magnitudes or different directions. Many drawing programs control this
by allowing three different tangent types. For example, Jasc’s Paint Shop Pro
refers to them as symmetric, asymmetric, and cusp. With the symmetric node,
clicking and dragging on one of the segment ends rotates both segments and
changes their lengths equally, to maintain equal tangents. With an asymmet-
ric node, clicking and dragging will rotate both segments to maintain equal
direction but change only the length of the particular tangent clicked on. And
with a cusp, clicking and dragging a segment end changes only the length
and direction of that tangent. This allows for the full range of possibilities in
continuity previously described.
9.5.2 Automatic Generation of Hermite Curves
But suppose we don’t need the full control of generating tangents for each
sample position. Instead, we just want to automatically generate a smooth
curve that passes through all the sample points. To do this we’ll need to have
a method of creating reasonable tangents for each sample. One solution is
to use Lagrange interpolation to generate a quadratic function using a given
sample point and its two neighbors, and then take the derivative of the func-
tion to get a tangent value at the sample point. A similar possibility is to take,
for a given point Pi, the weighted average of (Pi+1 −Pi) and (Pi −Pi−1). How-
ever, for both of these it will still be necessary to set a tangent for the two
endpoints, since they have only one neighboring point.
Demo
Auto Hermite
Another method creates tangents that maintain C2 continuity at the inte-
rior sample points. To do this, we’ll need to solve a system of linear equations,
using our sample points as the known quantities and the tangents as our
unknowns. We’ll begin by computing the ﬁrst derivative of the Hermite
curve Q:
Q′
i(u) = (6u2 −6u)Pi + (−6u2 + 6u)Pi+1 + (3u2 −4u + 1)P′
i + (3u2 −2u)P′
i+1
and from that the second derivative Q′′:
Q′′
i (u) = (12u −6)Pi + (−12u + 6)Pi+1 + (6u −4)P′
i + (6u −2)P′
i+1
At a given interior point Pi+1, we want the outgoing second derivative of curve
Qi to equal the incoming second derivative of curve Qi+1. We’ll assume that
each curve segment has a valid parameterization from 0 to 1, so we want
Q′′
i (1) = Q′′
i+1(0)
6Pi −6Pi+1 + 2P′
i + 4P′
i+1 = −6Pi+1 + 6Pi+2 −4P′
i+1 −2P′
i+2

434
Chapter 9 Curves
This can be rewritten to place our knowns on one side of the equation and
unknowns on the other:
2P′
i + 8P′
i+1 + 2P′
i+2 = 6[(Pi+2 −Pi+1) + (Pi+1 −Pi)]
This simpliﬁes to
P′
i + 4P′
i+1 + P′
i+2 = 3(Pi+2 −Pi)
Applying this to all of our sample points {P0, . . . , Pn} creates n −1 linear
equations. This can be written as a matrix product as follows:


1
4
1
· · ·
0
0
0
1
4
1
· · ·
0
0
...
0
0
· · ·
1
4
1
0
0
0
· · ·
0
1
4
1




P′
0
P′
1
...
P′
n−1
P′
n


=


3(P2 −P0)
3(P3 −P1)
...
3(Pn−1 −Pn−3)
3(Pn −Pn−2)


This means we have n−1 equations with n+1 unknowns. To solve this, we will
need two more equations. We have already constrained our interior tangents
by ensuring C2 continuity; what remains is to set our two tangents at each
extreme point. One possibility is to set them to given values v0 and v1, or
Q′
0(0) = P′
0 = v0
(9.10)
Q′
n−1(1) = P′
n = v1
(9.11)
This is known as a clamped end condition, and the resulting curve is a clamped
cubic spline. Our ﬁnal system of equations is


1
0
0
0
· · ·
0
0
1
4
1
0
· · ·
0
0
0
1
4
1
· · ·
0
0
...
0
0
· · ·
1
4
1
0
0
0
· · ·
0
1
4
1
0
0
· · ·
0
0
0
1




P′
0
P′
1
...
P′
n−1
P′
n


=


v0
3(P2 −P0)
3(P3 −P1)
...
3(Pn−1 −Pn−3)
3(Pn −Pn−2)
v1


Solving this system of equations gives us the appropriate tangent vectors. This
is not as bad as it might seem. Because this matrix (known as a tridiagonal
matrix) is sparse and extremely structured, the system is very easy and efﬁcient
to solve.

9.5 Hermite Curves 435
For this discussion, we have assumed uniform time values (this is also
known as a normalized cubic spline). However, as mentioned under linear
interpolation, our time values may vary from ti to ti+1 across each spline
segment. One solution is to do the same thing we did for linear interpolation: if
we know that a given value t lies between ti and ti+1, we can use equation 9.1 to
normalize our time value to the range 0 ≤u ≤1, and use that as our parameter
to curve segment Qi. While not strictly correct, this provides a reasonable
approximation. For those who require it, a full derivation for non-normalized
splines can be found in [95].
9.5.3 Natural, Cyclic, and Acyclic End
Conditions
Demo
Auto Hermite
In the preceding examples, we generated splines assuming that the begin-
ning and end tangents were clamped to values set by the programmer or
the user. This may not be convenient; we may want to avoid specifying tan-
gents at all. An alternative approach is to set conditions on the end tangents,
just as we did with the internal tangents, to reduce the amount of input
needed.
The ﬁrst such possibility is to assume that the second derivative is 0 at
the two extremes; that is, Q′′
0(0) = Q′′
n−1(1) = 0. This is known as a relaxed or
natural end condition, and the spline created is known as a natural spline. As
the name indicates, this produces a very smooth and natural looking curve
at the endpoints, and in most cases this is the end condition we would want
to use.
With a natural spline, we don’t need to specify tangent information at
all — we can compute the two unconstrained tangents from the clamped
spline using the second derivative condition.
At point P0, we know that
0 = Q′′
0(0)
= −6P0 + 6P1 −4P′
0 −2P′
1
As before, we can rewrite this so that the unknowns are on the left side and
the knowns on the right:
4P′
0 + 2P′
1 = 6P1 −6P0
or
2P′
0 + P′
1 = 3(P1 −P0)
(9.12)

436
Chapter 9 Curves
Similarly, at point Pn, we know that
0 = Q′′
n−1(1)
= 6Pn−1 −6Pn + 2P′
n−1 + 4P′
n
This can be rewritten as
P′
n−1 + 2P′
n = 3(Pn −Pn−1)
(9.13)
We can substitute equations 9.12 and 9.13 for our ﬁrst and last equations in
the clamped case, to get the matrix product:


2
1
0
0
· · ·
0
0
1
4
1
0
· · ·
0
0
0
1
4
1
· · ·
0
0
...
0
0
· · ·
1
4
1
0
0
0
· · ·
0
1
4
1
0
0
· · ·
0
0
1
2




P′
0
P′
1
...
P′
n−1
P′
n


=


3(P1 −P0)
3(P2 −P0)
3(P3 −P1)
...
3(Pn−1 −Pn−3)
3(Pn −Pn−2)
3(Pn −Pn−1)


Once again, by solving this system of linear equations or inverting the main
matrix, we can ﬁnd the values for our tangents.
Another possibility, known as the cyclic end condition, is to assume that
the ﬁrst and second derivatives at the endpoints are equal. Note that this
doesn’t necessarily mean that the positions of the two endpoints have to be
equal. Neither does it mean that the resulting curve will be symmetric if they
are equal (i.e., you can’t guarantee an oval). You might use a curve of this type
if you want to ensure that the animated object ends up moving in the same
direction at the end of the curve as it does at the beginning.
We can represent the cyclic end condition as
Q′
0(0) = Q′
n−1(1)
Q′′
0(0) = Q′′
n−1(1)
Expanding the ﬁrst equation gives
P′
0 = P′
n
which is not all that surprising: the initial tangent is equal to the ﬁnal tangent.
Expanding the second gives
−6P0 + 6P1 −4P′
0 −2P′
1 = 6Pn−1 −6Pn + 2P′
n−1 + 4P′
n

9.5 Hermite Curves 437
or
2P′
0 + P′
1 + P′
n−1 + 2P′
n = 3(P1 −P0) + 3(Pn −Pn−1)
We can substitute P′
0 for P′
n, since they are equal, to get the ﬁnal constraint
equation:
4P′
0 + P′
1 + P′
n−1 = 3(P1 −P0) + 3(Pn −Pn−1)
As before, we can set this up as a series of linear equations. However, since
P′
0 = P′
n, we have only n −1 unknowns, and so we need only n −1 equations.
Our matrix ends up being


4
1
0
0
· · ·
1
1
4
1
0
· · ·
0
0
1
4
1
· · ·
0
...
...
...
0
0
· · ·
1
4
1
1
0
· · ·
0
1
4




P′
0
P′
1
...
P′
n−1


=


3(P1 −P0) + 3(Pn −Pn−1)
3(P2 −P0)
3(P3 −P1)
...
3(Pn−1 −Pn−3)
3(Pn −Pn−2)


The acyclic end condition is similar to the cyclic end condition, except that
the ﬁrst and second derivatives are negatives of each other. If the positions
of the two endpoints are equal, this can produce a shape like the head of
a tennis racket. You might use a curve of this type if you want to ensure that
the animated object ends up moving in the opposite direction at the end of
the curve as it does at the beginning.
We can represent the acyclic end condition as
Q′
0(0) = −Q′
n(1)
Q′′
0(0) = −Q′′
n(1)
Using a similar process to the cyclic end condition, we end up with the
matrix equation for the acyclic end condition:


4
1
0
0
· · ·
−1
1
4
1
0
· · ·
0
0
1
4
1
· · ·
0
...
...
...
0
0
· · ·
1
4
1
−1
0
· · ·
0
1
4




P′
0
P′
1
...
P′
n−1


=


3(P1 −P0) −3(Pn −Pn−1)
3(P2 −P0)
3(P3 −P1)
...
3(Pn−1 −Pn−3)
3(Pn −Pn−2)



438
Chapter 9 Curves
9.6 Catmull-Rom Splines
Demo
Catmull
An alternative for automatic generation of a parametric curve is the Catmull-
Rom spline. This takes a similar approach to some of the initial methods
we described for Hermite curves (tangent of parabola, weighted average),
where tangents are generated based on the positions of the sample points.
The standard Catmull-Rom splines create the tangent for a given sample point
by taking the neighboring sample points, subtracting to create a vector, and
halving the length. So, for sample Pi, the tangent P′
i is
P′
i = 1
2(Pi+1 −Pi−1)
If we substitute this into our matrix deﬁnition of a Hermite curve between Pi
and Pi+1, this gives us
Qi(u) =

u3
u2
u
1



2
−2
1
1
−3
3
−2
−1
0
0
1
0
1
0
0
0




Pi
Pi+1
1
2(Pi+1 −Pi−1)
1
2(Pi+2 −Pi)


We can rewrite this in terms of Pi−1, Pi, Pi+1, Pi+2 to get
Qi(u) =

u3
u2
u
1
 1
2


−1
3
−3
1
2
−5
4
−1
−1
0
1
0
0
2
0
0




Pi−1
Pi
Pi+1
Pi+2


This provides a deﬁnition for curve segments Q1 to Qn−2, so it can be
used to generate a C1 curve from P1 to Pn−1. However, since there is no P−1
or Pn+1, we once again have the problem that curves Q0 and Qn−1 are not
valid due to undeﬁned tangents at the end points. And as before, these can
either be provided by the artist or programmer, or automatically generated.
Parent [87] presents one technique. For P0, we can take the next two points,
P1 and P2, and use them to generate a new phantom point, P1 + (P1 −P2).
If we subtract P0 from the phantom point and halve the length, this gives
a reasonable tangent for the start of the curve (Figure 9.8). The tangent at Pn
can be generated similarly.
Since our knowns for the outer curve segments are two points and a tan-
gent, another possibility is to use a quadratic equation to generate these seg-
ments. We can derive this in a similar manner as the Hermite spline equation.

9.6 Catmull-Rom Splines 439
P1
P0
P2
P′0
P′1
–(P2–P1)
(P2–P1)
Figure 9.8 Automatic generation of tangent vector at P0, based on positions of P1
and P2.
The general quadratic equation will have the form:
Q(u) = au2 + bu + C
(9.14)
For the case of Q0, we know that
Q0(0) = C = P0
Q0(1) = a + b + C = P1
Q′
0(1) = 2a + b = P′
1
= 1
2(P2 −P0)
Solving for a, b, and C and substituting into equation 9.14, we get
Q0(u) =
1
2P0 −P1 + 1
2P2

u2 +

−3
2P0 + 2P1 −1
2P2

u + P0
Rewriting in terms of P0, P1, and P2 gives
Q0(u) =
1
2u2 −3
2u + 1

P0 +

−u2 + 2u

P1 +
1
2u2 −1
2u

P2
As before, we can write this in matrix form:
Q0(u) =

u2
u
1
 1
2


1
−2
1
−3
4
−1
2
0
0




P0
P1
P2



440
Chapter 9 Curves
A similar process can be used to derive Qn−1:
Qn−1(u) =

u2
u
1
 1
2


1
−2
1
−1
0
1
0
2
0




Pn−2
Pn−1
Pn


9.7 Bézier Curves
9.7.1 Definition
Demo
Bézier
The previous techniques for generating curves from a set of points meet the
functional requirements of controlling curvature and maintaining continuity.
However, other than Hermite curves where the tangents are user-speciﬁed,
they are not so good at providing a means of controlling the shape that is
produced. It is not always clear how adjusting the position of a point will
change the curve produced, and if we’re using a particular type of curve and
want to pass through a set of ﬁxed points, there is usually only one possibility.
Bézier curves were created to meet this need. They were devised by Pierre
Bézier for modeling car bodies for Renault and further reﬁned by Forrest,
Gordon, and Riesenfeld. A cubic Bézier curve uses four control points: two
endpoints P0 and P3 that the curve interpolates, and two points P1 and P2
that the curve approximates. Their positions act, as their name suggests, to
control the curve. The convex hull, or control polygon, formed by the control
points bounds the curve (Figures 9.9a and 9.9b). Another way to think of it
is that the curve mimics the shape of the control polygon. Note that the four
points in this case do not have to be coplanar, which means that the curve
generated will not necessarily lie on a plane either.
The tangent vector at point P0 points in the same direction as the vector
P1 −P0. Similarly, the tangent at P3 has the same direction as P3 −P2. As we
will see, there is a deﬁnite relationship between these vectors and the tangent
vectors used in Hermite curves. For now we can think of the polygon edge
between the interpolated end point and neighboring control point as giving
us an intuitive sense of what the tangent is like at that point.
So far we’ve only shown cubic Bézier curves, but there is no reason
why we couldn’t use only three control points to produce a quadratic Bézier
curve (Figure 9.10) or more control points to produce higher-order curves.
A general Bézier curve is deﬁned by the function
Q(u) =
n

i=0
PiJn,i(u)

9.7 Bézier Curves 441
P1
P0
P2
P3
(a)
P1
P2
P3
P0
(b)
Figure 9.9 Example of cubic Bézier curve showing convex hull.
P0
P1
P2
Figure 9.10 Example of quadratic Bézier curve showing convex hull.
where the set of Pi are the control points, and
Jn,i(u) =
 n
i

ui (1 −u)n−i
where
 n
i

=
n!
i!(n −i)!
The polynomials generated by Jn,i are also known as the Bernstein poly-
nomials, or Bernstein basis.

442
Chapter 9 Curves
In most cases, however, we will use only cubic Bézier curves. Higher
order curves are more expensive and can lead to odd oscillations in the shape
of the curve. Quadratic curves are useful when processing power is limited
(the game Quake 3 used them, for example) but don’t have quite the ﬂexibility
of cubic curves. For example, they don’t allow for the familiar S shape in
Figure 9.9b. To generate something similar with quadratic curves requires
two piecewise curves, and hence more data.
The standard representation of an order n Bézier curve is to use an ordered
list of points P0, . . . , Pn as the control points. Using this representation, we can
expand the general deﬁnition to get the formula for the cubic Bézier curve:
Q(u) = (1 −u)3P0 + 3u(1 −u)2P1 + 3u2(1 −u)P2 + u3P3
(9.15)
The matrix form looks like
Q(u) =

u3
u2
u
1



−1
3
−3
1
3
−6
3
0
−3
3
0
0
1
0
0
0




P0
P1
P2
P3


We can think of the curve as a set of afﬁne combinations of the four points,
where the weights are deﬁned by the four basis functions J3,i. We can see these
basis functions graphed in Figure 9.11. At a given parameter value u, we grab
the four basis values and use them to compute the afﬁne combination.
1
1
0
0
t
J3,0
J3,1
J3,2
J3,3
Figure 9.11 Cubic Bézier curve basis functions.

9.7 Bézier Curves 443
As hinted at, there is a relationship between cubic Bézier curves and
Hermite curves. If we set our Hermite tangents to 3(P1 −P0) and 3(P3 −P2),
substitute those values into our cubic Hermite equation, and simplify, we end
up with the cubic Bézier equation.
9.7.2 Piecewise Bézier Curves
As with linear interpolation and Hermite curves, we can interpolate a curve
through more than two points by creating curve segments between each neigh-
boring pair of interpolation points. Many of the same principles apply with
Bézier curves as did with Hermite curves. In order to maintain matching direc-
tion for our tangents, giving us G1 continuity, each interpolating point and
its neighboring control points need to be collinear. To obtain equal tangents,
and therefore C1 continuity, the control points need to be collinear with and
equidistant to the shared interpolating point. Drawing a line segment through
the three points gives a three-lobed barbell shape, seen in Figure 9.12.
The barbell makes another very good interface for managing our curves. If
we set up our interpolating point as a pivot, then we can grab one neighboring
control point and rotate it around to change the direction of the tangent.
The other neighboring control point will rotate correspondingly to maintain
collinearity and equal distance, and thereby C1 continuity. If we drag the
control point away from our interpolating point, that will increase the length
of our tangent. We can leave the other control point at the original distance, if
we like, to create different arrival/departure speeds while still maintaining G1
continuity. Or, we can match its distance from the sample as well, to maintain
C1 continuity. And of course, we can move each neighboring control point
independently to create a cusp at that interpolating point.
This seems very similar to our Hermite interface, so the question may
be, why use Bézier curves? The main advantage of the Bézier interface over
the Hermite interface is that, as mentioned, the control points act to bound
the curve, and so give a much better idea of how the shape of the curve will
Figure 9.12 Example interface for Bézier curves.

444
Chapter 9 Curves
Pi
Pi+1
Pi+2
Pi –1
1/3(Pi+1 – Pi–1)
1/3(Pi – Pi+2)
Figure 9.13 Automatic construction of approximating control points with Bézier
curve.
change as we move the control points around. Because of this, many drawing
packages use Bézier curves instead of Hermite curves.
While in most cases we will want to make use of user-created data with
Bézier curves, it is sometimes convenient to automatically generate them, just
as we did with Hermite curves. Parent [87] provides a method for automati-
cally generating Bézier control points from a set of sample positions, as shown
in Figure 9.13. Given four points Pi−1, Pi, Pi+1, and Pi+2, we want to compute
the two control points between Pi and Pi+1. We compute the tangent vector
at Pi by computing the difference between Pi+1 and Pi−1. From that we can
compute the ﬁrst control point as Pi + 1
3(Pi+1 −Pi−1). The same can be done
to create the second control point as Pi+1 −1
3(Pi+2 −Pi). This is very similar
to how we created the Catmull-Rom spline, but with tangents twice as large
in magnitude.
9.8 B-Splines
Demo
B-Spline
The ﬁrst set of curves we looked at were interpolating curves, which pass
through all the given points. With Bézier curves, the resulting curve interpo-
lates two of the control points, while approximating the others. B-splines are
a generalization of this — depending on the form of the B-spline, all or none of
the points can be interpolated. Because of this, in a B-spline all of the control
points can be used as approximating points (Figure 9.14). In fact, B-splines
are so ﬂexible they can be used to represent all of the curves we have described
so far. However, with ﬂexibility comes a great deal of complexity. Because of
this, B-splines are not yet in common usage in games, either for animation or
surface construction. Hence, this section is designed only to give an overview,
with implementation details of a single commonly used B-spline.
The motivation for using B-splines is twofold. First of all, most of our pre-
vious solutions have only C1 continuity. In some cases we may want to ensure

9.8 B-Splines 445
P0
P1
P2
P3
P4
P5
Figure 9.14 B-spline approximating curve.
that we have at least C2 continuity (although, admittedly, such cases are rare
in animation). While natural cubic splines provide that level of continuity,
they also are subject to global control. That is, changing a single point affects
the entire curve, which requires us to recalculate the entire thing.
B-splines, in comparison, provide what’s called local control. Each point
has inﬂuence only over a limited region of the curve. This is controlled by a set
of basis functions (hence the B in B-spline) that are computed for each sample
location and added up to give our ﬁnal curve position. Piecewise Hermite and
Bézier curves do allow us local control, but at the cost of having to adjust other
control points or tangents to maintain continuity. B-splines can maintain
continuity without such adjustments.
B-splines are computed similarly to Bézier curves. We set up a basis func-
tion for each control point in our curve, and then for each parameter value u
we multiply the appropriate basis function by its point and add the results. In
general, this can be represented by
Q(u) =
n

i=0
PiBi(u)
where each Pi is a point and Bi is a basis function for that point. The basis
functions in this case are far more general than those described for Bézier
curves, which gives B-splines their ﬂexibility and their power.
Like our previous piecewise curves, B-splines are broken into smaller
segments. The difference is that the number of segments is not necessarily
dependent on the number of points, and the intermediary point between
each segment is not necessarily one of our control points. These intermediary
points are called knots. If the knots are spaced equally in time, the curve is
known as a uniform B-spline. Otherwise it is a nonuniform B-spline.
The standard example of a uniform cubic B-spline has knots lying 1 unit
apart in u — the knots are at Q(0), Q(1), Q(2), and so on. So a given segment
Qi describes the curve between Q(i) and Q(i + 1). Assuming that we’re using

446
Chapter 9 Curves
the same convention we have before, where each segment is parameterized
from 0 to 1, the partial basis functions
Bi−3(u) = 1
6(−u3 + 3u2 −3u + 1)
Bi−2(u) = 1
6(3u3 −6u2 + 4)
Bi−1(u) = 1
6(−3u3 + 3u2 + 3u + 1)
Bi(u) = 1
6(u3)
give us C2 continuity at each knot. Figure 9.15 shows these bases graphed
within the interval of one segment.
The matrix representation for a particular segment Qi is
Qi(u) =

u3
u2
u
1
 1
6


−1
3
−3
1
3
−6
3
0
−3
0
3
0
1
4
1
0




Pi−3
Pi−2
Pi−1
Pi


We stated that the Bis we set above were partial basis functions. For a given
point Pi, its corresponding full basis function Bi forms a bell-shaped curve
(Figure 9.16). Each corresponding Bi−1, Bi+1 is a translation of this curve,
with the peak centered over each knot parameter k. If we look at the complete
Bi−2
Bi−3
Bi−1
Bi
1
1
0
0
t
Figure 9.15 Uniform cubic B-spline basis functions.

9.8 B-Splines 447
ui+1
ui+2
ui+3
ui+4
ui
Figure 9.16 Single uniform B-spline basis function.
0
1
2
3
4
5
6
7
8
9
Figure 9.17 Overlapping basis functions for three B-spline segments.
basis functions for a series of segments joined together (Figure 9.17), we see
that each basis affects up to four segments. Each segment is only controlled
by four points, and each point controls no more than four segments. This
demonstrates the local control properties of the B-spline.
Figure 9.14 shows an example of a uniform cubic B-spline generated using
the preceding basis. Clearly, this is a pure approximating curve where the
curve doesn’t pass through any of the control points. As we’ve seen before
with Catmull-Rom splines, the end segments are undeﬁned since we don’t
have enough points to describe them. Usually, this isn’t an issue since it is an
approximating curve, but Bartels et al. [8] describe methods for creating end
conditions for B-splines, much as we did with Hermite curves.
Finally, suppose we don’t want to approximate all the points, but want
to ensure that the curve passes through speciﬁc positions. With uniform
B-splines, we copy the points we want to interpolate. Duplication will draw
the curve closer to the point and triplicating it will cause the curve to pass
through it. The one drawback is that we will end up with a kink in the
curve at that point. Another possibility is to use nonuniform B-splines.

448
Chapter 9 Curves
By triplicating knot values, we can cause the curve to pass through a point,
again with a loss of continuity. The curve produced will not quite be the same
curve as with the control point triplication method.
This is merely a taste of what is possible. As mentioned, B-splines are
not often used for animation; they are more commonly used when building
surface representations. A full description of the power and complexity of
B-splines is out of the purview of this text, so for those who are interested,
more information on B-splines and other curves can be found in [36], [94],
or [8].
9.9 Rational Curves
The curves we have discussed so far have the property that any afﬁne trans-
formation on the set of points (or tangents, in the case of Hermite curves)
generating the curve will transform the curve accordingly. So for example, if
we want to transform a Bézier curve from the local frame to the view frame,
all we need to do is transform the control points and then generate the curve
in the view frame.
However, this will not work for a perspective transformation, due to the
need for a reciprocal division at each point on the curve. The answer is to apply
a process similar to the one we used when transforming points, by adding an
additional parameterized function w(u) that we divide by when generating
the points along the curve.
We create a rational curve by ﬁrst considering a curve Q(u) in RP 3, similar
to our space curve in R3 but with the w(u) function added:
Q(u) = (x(u), y(u), z(u), w(u))
The corresponding rational curve R(u) projects this homogeneous curve Q(u)
into R3 as
R(u) =
 x(u)
w(u), y(u)
w(u), z(u)
w(u)

So for example, we can deﬁne Q(u) as a Bézier curve:
Q(u) =
3

i=0
PiJ3,i(u)
but now each Pi is a point in RP 3, or
Pi = (wixi, wiyi, wizi, wi)

9.9 Rational Curves 449
Note that w(u) in this case is just another Bézier function:
w(u) =
3

i=0
wiJ3,i(u)
The corresponding rational curve is
R(u) =
$ wixiJ3,i(u)
$ wiJ3,i(u) ,
$ wiyiJ3,i(u)
$ wiJ3,i(u) ,
$ wiziJ3,i(u)
$ wiJ3,i(u)

or
R(u) =
$ wiPiJ3,i(u)
$ wiJ3,i(u)
where each Pi is one of our standard control points in R3.
We can create a rational curve from a nonrational one by implicitly setting
the wis to 1, so rational curves encapsulate nonrational curves. In the previous
case, R(u) collapses to the standard cubic Bézier deﬁnition.
To apply a perspective projection to a curve, we transform the control
points as we normally would with our projection matrix, but defer the divi-
sion by w until we actually generate the points along the curve. This is much
more efﬁcient than the alternative, where we generate the curve points in
world space and apply the full perspective transformation to every single point
generated.
There are a number of uses for rational curves. The ﬁrst has already been
stated: we can use it as a more efﬁcient method for projecting curves. But it
also allows us to set weights wi for the control points so that we can direct
the curve to pass closer to one point or another. Figure 9.18 shows the effect
P0
P1
P2
P3
Figure 9.18 Dotted line shows effect of giving vertex P1 greater weight in a rational
Bézier curve.

450
Chapter 9 Curves
of such weighting being applied to control point P1 on a Bézier curve. The
higher the relative weight, the more the curve tends towards that point.
Another use of rational curves is to create conic section curves, such
as circles and ellipses. Nonrational curves, since they are polynomials, can
only approximate conic sections. As an example, we can construct a quarter-
circle in R2 with a rational quadratic Bézier curve in RP 2 (i.e., coordinates
are (x, y, w)), where the Bézier function is
Q(u) = (1 −t)2P0 + 2(1 −t)tP1 + t2P2
and the control points are
P0 = (0, 1, 1)
P1 =
√
2/2,
√
2/2,
√
2/2

P2 = (1, 0, 1)
The entire circle can be exactly duplicated by a piecewise curve made up of
four such curve segments, with control points in the appropriate quadrants.
In the examples thus far, we have used rational Bézier curves, but the most
commonly used of the rational curves are nonuniform rational B-splines, or
NURBS. Since they can produce conic as well as general curves and sur-
faces, they are extremely useful in CAD systems and modeling for computer
animation. Like B-splines, rational curves and particularly NURBS are not yet
used much in games because of their relative performance cost and because
of concern by artists about lack of control.
9.10 Rendering Curves
9.10.1 Forward Differencing
Library
IvCurves
Filename
IvHermite
Given a parametric curve, it is only natural that we might want to render it at
some point. The main purpose would be to allow artists to see, and thus more
accurately control, the animation paths that they are creating. We may also
want to render a curve for debugging, to allow engineers testing the animation
code to ensure that the path taken is the one intended. We may want rendered
curves for other reasons as well: game interface components, for example.
In most cases we will be using a cubic curve. The simplest render-
ing method is to take the general function for our curve or curve segment
Q(u) = au3 + bu2 + cu + D, evaluate it at n + 1 values of u, and then use those
n+1 points to create n line segments, which we render with our standard line
drawing algorithm. Assuming that we’re generating points in R3, this will take

9.10 Rendering Curves 451
11 multiplies and 9 adds per point (we save three multiplies by computing u3
as u · u2).
An alternative which is slightly faster is to use Horner’s rule, which
expresses the same cubic curve as
Q(u) = ((au + b)u + c)u + D
This will take only 9 multiplies and 9 adds per point. In addition, it can
actually improve our ﬂoating point accuracy under certain circumstances.
This assumes that there is no pattern to how we evaluate our curve.
Suppose we can instead sample our curve at even intervals of u, say at a time
step of every h. This gives us a list of n + 1 parameter values: 0, h, 2h, . . . , nh.
In such a situation, we can use a technique called forward differencing.
For the time being, let’s consider computing only the x values for our
points. For a given value xi, located at parameter u, we can compute the next
value xi+1 at parameter u + h. Subtracting xi from xi+1:
xi+1 −xi = x(u + h) −x(u)
We’ll label this difference between xi+1 and xi as x1(u). For a cubic curve
this equals
x1(u) = a(u + h)3 + b(u + h)2 + c(u + h) + d −(au3 + bu2 + cu + d)
= a(u3 + 3hu2 + 3h2u + h3) + b(u2 + 2hu + h2) + c(u + h) + d
−au3 −bu2 −cu −d
= au3 + 3ahu2 + 3ah2u + ah3 + bu2 + 2bhu + bh2 + cu + ch + d
−au3 −bu2 −cu −d
= 3ahu2 + 3ah2u + ah3 + 2bhu + bh2 + ch
= (3ah)u2 + (3ah2 + 2bh)u + (ah3 + bh2 + ch)
Pseudocode to compute the set of values might look like
u = 0;
x = d;
output(x);
dx1 = ahˆ3 + bhˆ2 + ch;
for ( i = 1; i <= n; i++ )
{
u += h;
x += dx1;

452
Chapter 9 Curves
output(x);
dx1 = (3ah)uˆ2 + (3ahˆ2 + 2bh)u + (ahˆ3 + bhˆ2 + ch);
}
While we have removed the cubic equation, we have introduced evaluation of
a quadratic equation x1(u). Fortunately, we can perform the same process
to simplify this equation. Computing the difference between x1(u + h) and
x1(u) as x2(u):
x2(u) = x1(u + h) −x1(u)
= (3ah)(u + h)2 + (3ah2 + 2bh)(u + h) + (ah3 + bh2 + ch)
−[(3ah)u2 + (3ah2 + 2bh)u + (ah3 + bh2 + ch)]
= 3ahu2 + 6ah2u + 3ah3 + (3ah2 + 2bh)u + 3ah3 + 2bh2 + (ah3 + bh2 + ch)
−[(3ah)u2 + (3ah2 + 2bh)u + (ah3 + bh2 + ch)]
= 6ah2u + (6ah3 + 2bh2)
This changes our pseudocode to
u = 0;
x = d;
output(x);
dx1 = ahˆ3 + bhˆ2 + ch;
dx2 = 6ahˆ3 + 2bhˆ2;
for ( i = 1; i <= n; i++)
{
u += h;
x += dx1;
output(x);
dx1 += dx2;
dx2 = 6ahˆ2u + (6ahˆ3 + 2bhˆ2);
}
We can carry this one ﬁnal step further to remove the linear equation
for x2. Computing the difference between x2(u + h) and x2(u) as x3(u):
x3(u) = x2(u + h) −x2(u)
= 6ah2(u + h) + (6ah3 + 2bh2)
−6ah2u + (6ah3 + 2bh2)

9.10 Rendering Curves 453
= 6ah2u + 6ah3 + (6ah3 + 2bh2)
−6ah2u + (6ah3 + 2bh2)
= 6ah3
Our ﬁnal code for forward differencing becomes
x = d;
output(x);
dx1 = ahˆ3 + bhˆ2 + ch;
dx2 = 6ahˆ3 + 2bhˆ2;
dx3 = 6ahˆ3;
for ( i = 1; i <=n; i++ )
{
x += dx1;
output(x);
dx1 += dx2;
dx2 += dx3;
}
We have simpliﬁed our evaluation from 3 multiplies and 3 adds, down to
3 adds. We’ll have to perform similar calculations for y and z, with differing
deltas and a, b, c, and d values for each coordinate, giving a total of 9 adds for
each point.
Note that forward differencing is only possible if the time steps between
each point are equal. Because of this, we can’t use it for animating along
a curve, as time between frames may vary from frame to frame. In this case
Horner’s rule for our degree polynomial is the most efﬁcient solution.
9.10.2 Midpoint Subdivision
Library
IvCurves
Filename
IvBezier
An alternative method for generating points along a curve is to recursively
subdivide the curve until we have a set of subcurves, each of which can
be approximated by a line segment. This subdivision usually stops at pixel
resolution if necessary. This may end up with a more accurate and more
efﬁcient representation of the curve than forward differencing since more
curve segments will be generated in areas with high curvature (areas that we
might cut across with forward differencing), and fewer in areas with lower
curvature.
We can perform this subdivision by taking a curve Q(u) and breaking it
into two new curves L(s) and R(t), usually at the midpoint Q(1/2). In this

454
Chapter 9 Curves
case, L(s) is the subcurve of Q(u) where 0 ≤u ≤1/2, and R(t) is the subcurve
where 1/2 ≤u ≤1. The parameters s and t are related to u by
s = 2u
t = 2u −1
Each subcurve is then tested for relative “straightness” — if it can be
approximated by a line segment, we stop subdividing, otherwise we keep
going. The general algorithm looks like
void
RenderCurve( Q )
{
if ( Straight( Q ) )
DrawLine( Q(0), Q(1) );
else
{
MidpointSubdivide( Q, &L, &R );
RenderCurve( L );
RenderCurve( R );
}
}
There are a few ways of testing how straight a curve is. The most accurate
is to measure the length of the curve and compare it to the length of the
line segment between the curve’s two extreme points. If the two lengths are
within a certain tolerance ϵ, then we can say the curve is relatively straight.
This assumes that we have an efﬁcient method for computing the arc length
of a curve. We discuss some ways of calculating this next.
Another method is to use the two endpoints and the midpoint
(Figure 9.19a). If the distance between the midpoint and the line segment
formed by the two endpoints is close to 0, then we can usually say that the
curve is relatively close to a line segment. The one exception is when the curve
crosses the line segment between the two endpoints (Figure 9.19b), which will
Figure 9.19a Midpoint test for curve straightness. Total distance from endpoints
to midpoint (block dot) is compared to distance between endpoints.

9.10 Rendering Curves 455
Figure 9.19b Midpoint test for curve straightness. Example of midpoint test
failure.
result in a false positive when clearly the curve is not straight. To avoid the
worst examples of this case, Parent [87] recommends performing forward
differencing down to a certain level and only then adaptively subdividing.
The convex hull properties of the Bézier curve lead to a particularly efﬁ-
cient method for testing straightness, with no need of calculating a midpoint.
If the interior control points are incident with the line segment formed by the
two exterior control points, the area of the convex hull is 0, and the curve
generated is itself a line segment. So for a cubic Bézier curve, we can test
distance squared between the line segment formed by P0 and P3 and the two
control points P1 and P2 (Figure 9.20). If both squared distances are less than
some tolerance value, then we can say that the curve is relatively straight.
How we subdivide the curve if it fails the test depends on the type of
curve. The simplest curves to subdivide are Bézier curves. To achieve this, we
will generate new control points for each subcurve from our existing control
points. So for a cubic curve, we will compute new control points L1, L2,
L3, and L4 for curve L, and new control points R1, R2, R3, and R4 for curve
R. These can be built by using a technique devised by de Casteljau. This
method — known as de Casteljau’s method — geometrically evaluates a Bézier
curve at a given parameter u, and as a side effect creates the new control
points needed to subdivide the curve at that point.
Figure 9.21 shows the construction for a cubic Bézier curve. L0 and R3 are
already known: they are the original control points P0 and P3, respectively.
P0
P3
P1
P2
Figure 9.20 Test of straightness for Bézier curve. Measure distance of P1 and P2
to line segment P0P3.

456
Chapter 9 Curves
L0 = P0
R2
P1
P2
P3 = R3
R1
H
L1
L2
L3 = R0
Figure 9.21 de Casteljau’s method for subdividing Bézier curves.
Point L1 lies on segment P0P1 at position (1 −u)P0 + uP1. Similarly, point H
lies on segment P1P2 at (1 −u)P1 + uP2, and point R2 at (1 −u)P2 + uP3. We
then linearly interpolate along the newly formed line segments L1H and HR2
to form L2 = (1−u)L1 +uH and R1 = (1−u)H +uR2. Finally, we split segment
L2R1 to ﬁnd Q(u) = L3 = R1 = (1 −u)L2 + uR1.
Using the midpoint to subdivide is particularly efﬁcient in this case.
It takes only 6 adds and 6 multiplies (to perform the division by 2):
L0 = P0;
R3 = P3;
L1 = (P0 + P1) * 0.5f;
H = (P1 + P2) * 0.5f;
R2 = (P2 + P3) * 0.5f;
L2 = (L1 + H) * 0.5f;
R1 = (H + R2) * 0.5f;
L3 = R0 = (L2 + R1) * 0.5f;
Subdividing other types of curves, in particular B-splines, can be handled
by using an extension of this method devised by Böhm [15]. More information
on Böhm subdivision and knot insertion can be found in Bartels et al. [8].
9.10.3 Using OpenGL
Library
IvCurves
Filename
IvUniformBSpline
If we’re using OpenGL as our graphics API, we can take advantage of an
interface that assists in the rendering of parametric curves, in particular those
that can be emulated by the Bernstein basis. If a curve can be converted to
a Bézier curve, then we can render it using this interface. Fortunately, this
applies to any of the curves that we have discussed. The interface consists of

9.10 Rendering Curves 457
two parts: setting up a Bézier evaluator for the curve, and then evaluating it
at increasing parameter values to create the appropriate OpenGL rendering
calls.
The ﬁrst part is done by using one of the routines glMap1f() or glMap1d().
This sets up the data for an evaluator function of one parameter, which we
can use to generate the curve to be rendered. There can be only one such
evaluator at a time: calling glMap1f() or glMap1d() a second time will overwrite
the previously deﬁned values. The arguments are as follows:
glMap1{fd}(GLenum target, TYPE u1, TYPE u2, GLint stride,
GLint order, const TYPE* points );
The TYPE in this case is either float or double, depending on whether we use
glMap1f() or glMap1d(). The target argument indicates what kind of rendering
data we want to create: positions, colors, normals, or texture coordinates. The
standard for rendering a curve is to use GL_MAP1_VERTEX_3. Arguments u1 and
u2 represent the minimum and maximum u values on the curve, respectively.
The value of stride is the offset (in number of ﬂoating-point values) between
each control point in the array points. The order of the curve is the degree of
the curve plus 1, so a cubic curve has order 4. Finally, the array points are
the control points for the curve.
An example of using glMap1f() to set up a simple Bézier curve is
IvVector3 controlPoints[] =
{
IvVector3(0.0f, 0.0f, 0.0f),
IvVector3(1.0f, 1.0f, 0.0f),
IvVector3(2.0f, -1.0f, 0.0f),
IvVector3(4.0f, 0.0f, 0.0f)
};
glMap1f( GL_MAP1_VERTEX_3, 0.0f, 1.0f, 3, 4,
(float*) &controlPoints[0].x );
glEnable( GL_MAP1_VERTEX_3 );
Note that we call glEnable() to activate the evaluator so we can use it.
To render the curve we need to evaluate it at increasing parameter values.
We can do this in one of two ways: manually, which allows us more con-
trol over where the curve is actually evaluated, or automatically through
OpenGL.
The manual method uses the routine glEvalCoord1f(). It takes a single
argument u, and evaluates the curve at that parameter. Then, depending on

458
Chapter 9 Curves
the target value set in glMap1f(), it will make an OpenGL call for that par-
ticular data value. So if target equals GL_MAP1_VERTEX_3, it will internally call
glVertex3(); for colors it will call glColor(); and so forth. How this is used
depends on the graphics primitive set by glBegin(). So for example, to render
a curve using line segments we might do
glBegin(GL_LINE_STRIP);
for (unsigned int i = 0; i < 32; ++i)
{
glEvalCoord1f( (float)i/32.0f );
}
glEnd();
An alternative to this is to pass in an array of pregenerated parameter values, or
float params[32];
for (unsigned int i = 0; i < 32; ++i)
{
params[i] = (float)i/32.0f;
}
...
glBegin(GL_LINE_STRIP);
glEvalCoord1fv( params );
glEnd();
Rather than generate the parameter values ourselves, we could let
OpenGL do it for us. This requires a two part interface: one part that gen-
erates a set of equally spaced values, and one that uses the stored data to
actually render the curve. The ﬁrst has the format
void glMapGrid1{fd}(GLint n, TYPE u1, TYPE u2)
This will generate n + 1 equally spaced parameter values starting at u1 and
at subsequent values of i · (u2 −u1)/n. Like glMap1f() and glMap1d(), this
can be set up once and reused over subsequent rendering passes but will be
overwritten by the next call of glMapGrid1f() or glMapGrid1d().
To render using this set of parameters, we use the routine glEvalMesh1(),
which has arguments
void glEvalMesh1(GLenum mode, GLint p1, GLint p2)

9.11 Controlling Speed Along a Curve 459
This will render parameters in the array from index p1 to p2 (0 ≤p1 ≤p2 ≤
n), using primitive mode. The mode can be either GL_POINT or GL_LINE. The
equivalent of applying both of these routines in sequence is
glBegin(mode);
for (unsigned int i = 0; i < n; ++i)
{
glEvalCoord1f( u1 + (float)i*(u2 - u1)/(float)n );
}
glEnd();
All of these interfaces evaluate at parameters speciﬁed by the user, so they
save only in cost of evaluation (potentially) and ease of interface. Designed
particularly for uniformly spaced subdivision, they are not nearly as useful if
we want to employ an adaptive subdivision method.
9.11 Controlling Speed Along a Curve
9.11.1 Moving at Constant Speed
Demo
Speed Control
One common requirement for animation is that the object animated move
at a constant speed along a curve. However, in most interesting cases, using
a given curve directly will not achieve this. The problem is that in order to
achieve variety in curvature, the ﬁrst derivative must vary as well, and hence
the distance we travel in a constant time will vary depending on where we
start on the curve. For example, Figure 9.22 shows a curve subdivided at
equal intervals of the parameter u. The lengths of the subcurves generated
vary greatly from one to another.
Ideally, given a constant rate of travel r and time of travel t, we’ll want to
cover a distance of s = rt. So given a starting parameter u1 on the curve, we
Figure 9.22 Parameter-based subdivision of curve, showing non-equal segment
lengths.

460
Chapter 9 Curves
want to ﬁnd the parameter u2 such that the distance along the curve, or arc
length, between Q(u1) and Q(u2) equals s.
We’ll discuss how to compute the arc length of a curve in the next section,
but for now suppose we somehow have a function G(u) that returns the length
s from Q(0) to Q(u). So for the case where u1 = 0, we can use the inverse
function G−1(s) to determine the parameter u2, given an input length s. This
is known as a reparameterization by arc length. Unfortunately, in general the
arc length function for a parameterized curve is impossible to invert in terms
of a ﬁnite number of elementary functions, so numerical methods are used
instead.
One way is to note that ﬁnding u2 is equivalent to the problem of ﬁnding
the solution u of the equation
s −length(u1, u) = 0
(9.16)
A method that allows us to solve this is Newton-Raphson root ﬁnding. Burden
and Faires [17] present a derivation for this using the Taylor series expansion.
Suppose we have a function f (x) where we want to ﬁnd p such that
f (p) = 0. We begin with a guess for p, which we’ll call ¯x, such that
f ′(¯x) ̸= 0 and |p −¯x| is relatively small. In other words, ¯x may not quite
be p but it’s a pretty good guess. If we use ¯x as a basis for the Taylor series
polynomial:
f (x) = f (¯x) + (x −¯x)f ′(¯x) + 1
2(x −¯x)2f ′′(ξ(x))
We assume that ξ(x) is bounded by x and ¯x, so we can ignore the remainder
of the terms. If we substitute p for x, then f (p) = 0 and
0 = f (¯x) + (p −¯x)f ′(¯x) + 1
2(p −¯x)2f ′′(ξ(x))
Since |p −¯x| is relatively small, we assume that (p −¯x)2 is small enough that
we can ignore it, and so
0 ≈f (¯x) + (p −¯x)f ′(x)
Solving for p gives
p ≈¯x −f (¯x)
f ′(¯x)
(9.17)
This gives us our method. We make an initial guess ¯x at the solution and use
the result of equation 9.17 to get a more accurate result p. If p still isn’t close

9.11 Controlling Speed Along a Curve 461
enough, then we feed it back into the equation as ¯x to get a still more accurate
result, and so on until we reach a solution of sufﬁcient accuracy or after
a given number of iterations is performed.
For our initial guess in solving equation 9.16, Eberly [27] recommends
taking the ratio of our traveled length to the total arc length of the curve and
map it to our parameter space. Assuming our curve is normalized so that u is
in [0, 1], then pseudocode for our root-ﬁnding method will look like
float FindParameterByDistance( float u1, float s )
{
// ensure that we remain within valid parameter space
if (s > ArcLength(u1,1.0f))
return 1.0f;
// get total length of curve
float len = ArcLength(0.0f,1.0f);
// make first guess
float p = u1 + s/len;
for (int i = 0; i < MAX_ITER; ++i)
{
// compute function value and test against zero
float func = ArcLength(u1,p) - s;
if ( fabsf(func) < EPSILON )
{
return p;
}
// perform Newton-Raphson iteration step
p -= func/Length(Derivative(p));
}
// done iterating, return last guess
return p;
}
The ﬁrst test ensures that the distance we wish to travel is not greater than
the remaining length of the curve. In this case we assume that this is the
last segment of a piecewise curve and just jump to the end. A more robust
implementation should subtract the remaining length from the distance and
restart at the beginning of the next segment.
A few other implementation notes are in order at this point. As we’ll see,
computing ArcLength() can be a nontrivial operation. Because of this, if we’re

462
Chapter 9 Curves
going to be calling FindParameterByDistance() many times for a ﬁxed curve,
it is more efﬁcient to precompute ArcLength(0.0f,1.0f) and use this stored
value instead of recomputing it each time. Also, the constants MAX_ITER and
EPSILON will need to be tuned depending on the type of curve and the num-
ber of iterations we can feasibly calculate due to performance constraints.
Reasonable starting values for this tuning process are 32 for MAX_ITER and
1.0e-06f for EPSILON.
There are two pieces missing in order to solve this completely: a deriv-
ative for the curve, and a function that computes arc length between two
parameters. The ﬁrst is easily derived from the deﬁnition of the curve, as
we did for clamped and natural splines. The second is discussed in the next
section.
9.11.2 Computing Arc Length
The most accurate method of computing the length of a smooth curve (see
Appendix B) Q(u) from Q(a) to Q(b) is to directly compute the line integral
s =
' b
a
((Q′(u)
(( du
Unfortunately, for most cubic polynomial curves, it is not possible to ﬁnd an
analytic solution to this integration. For quadratic curves, there is a closed
form solution, but evaluating the resulting functions is more expensive than
using a numerical method that gives similar accuracy. In any case, if we wish
to vary our curve types, we would have to redo the calculation and so it is not
always practical.
The usual approach is to use a numerical method to solve the integral.
There are many methods, which Burden and Faires [17] cover in some detail.
In this case the most efﬁcient for its accuracy is Gaussian quadrature, since
it attempts to minimize the number of function evaluations, which can be
expensive. It approximates a deﬁnite integral from −1 to 1 by a weighted sum
of unevenly spaced function evaluations, or
' 1
−1
f (x)dx ≈
n

i=1
cif (xi)
The actual ci and xi values depend on n and are carefully selected to give
the best approximation to the integral. Appendix B tabulates values up to
n = 5, and Burden and Faires [17] describe in detail how these are derived for
arbitrary values of n.

9.11 Controlling Speed Along a Curve 463
The restriction that we have to integrate over [−1, 1] is not a seri-
ous obstacle. For a general deﬁnite integral over [a, b], we can remap to
[−1, 1] by
' b
a
f (x)dx =
' 1
−1
f
(b −a)t + b + a
2
 b −a
2
dt
Guenter and Parent [51] describe a method that uses Gaussian quadrature
in combination with adaptive subdivision to get very efﬁcient results when
computing arc length. Similar to using adaptive subdivision for rendering,
we cut the current curve segment in half. We use Gaussian quadrature to
measure the length of each half, and compare their sum to the length of
the entire curve, again computed using Gaussian quadrature. If the results
are close enough, we stop and return the sum of lengths of the two halves.
Otherwise, we recursively compute their lengths via subdivision.
There are other arc length methods that don’t involve computing the inte-
gral in this manner. One is to subdivide the curve as we would for rendering,
and use the sums of the lengths of the line segments created to approximate
arc lengths at each of the subdivision points. We can create a sorted table of
pairs (ui, si), where ui is the parameter for each subdivision, and si is the cor-
responding length at the point Q(ui). Since both u and len are monotonically
increasing, we can sort by either parameter. An example of such a table can
be seen in Table 9.1.
To ﬁnd the length from the start of the curve for a given u, we search
through the table to ﬁnd the two neighboring entries with parameters uk and
uk+1 such that uk ≤u ≤uk+1. Since the entries are sorted, this can be handled
efﬁciently by a binary search. The length can then be approximated by linearly
Table 9.1 Mapping Parameter Value to Arc Length
u
s
0.0
0.0
0.1
0.2
0.15
0.3
0.29
0.7
0.35
0.9
0.56
1.1
0.72
1.6
0.89
1.8
1.00
1.9

464
Chapter 9 Curves
interpolating between the two entries:
s ≈uk+1 −u
uk+1 −uk
sk +
u −uk
uk+1 −uk
sk+1
A higher-order curve can be used to get a better approximation.
To ﬁnd the length between two parameters a and b where a ≤b, we
compute the length for each and subtract one from the other, or
length(Q, a, b) = length(Q, b) −length(Q, a)
We can also use Table 9.1 to solve our original reparameterization prob-
lem, which is to ﬁnd u given a length s. In this case we invert the process
and search for the two neighboring entries with lengths sj and sj+1 such
that sj ≤s ≤sj+1. Again, we can use linear interpolation to approximate the
parameter u which gives us length s as
u ≈sj+1 −s
sj+1 −sj
uj +
s −sj
sj+1 −sj
uj+1
To ﬁnd the parameter b given a starting parameter a and a length s, we com-
pute the length at a and add that to s. We then use the preceding process with
the total length to ﬁnd parameter b.
The obvious disadvantage of this scheme is that it takes additional mem-
ory for each curve. However, it is simple to implement, somewhat fast, and
does avoid the Newton-Raphson iteration needed with other methods.
If we are using cubic Bézier curves, we can use a method described by
Gravesen [49]. First of all, given a parameter u we can subdivide the curve
(using de Casteljau’s method) to be the subcurve from [0, u]. The new control
points for this new subcurve can be used to calculate bounds on the length.
The length of the curve is bounded by the length of the chord P0P3 as the
minimum, and the sum of the lengths of the line segments P0P1, P1P2 and
P2P3 as the maximum. We can approximate the arc length by the average of
the two, or
Lmin = ∥P3 −P0∥
Lmax = ∥P1 −P0∥+ ∥P2 −P1∥+ ∥P3 −P2∥
L ≈1
2(Lmin + Lmax)
The error can be estimated by the square of the difference between the
minimum and maximum:
ξ = (Lmax −Lmin)2

9.11 Controlling Speed Along a Curve 465
If the error is judged to be too large, then the curve can be subdivided and
the length becomes the sum of the lengths of the two halves. Gravesen [49]
states that for m subdivisions the error drops to 0 as 2−4m.
9.11.3 Ease-In and Ease-Out
In our original equation for computing the desired distance to travel, s = rt,
we assumed that we were traveling at a constant rate of speed. However, it
is often convenient to have an adjustable rate of speed over the length of the
curve. We can represent this by a general distance-time function s(t), which
maps a time value t to the total distance traveled from t0. As an example,
Figure 9.23 shows s(t) = rt as a distance-time graph.
Other than traveling at a constant rate, the most common distance-time
function is known as ease-in/ease-out. Here, we start at a zero rate of speed,
accelerate up to a constant nonzero rate of speed in the middle, and then
decelerate down again to a stop. This feels natural, as it approximates the
need to accelerate a physical camera, move it, and slow it down to a stop.
Figure 9.24 shows the distance-time graph for one such function.
Parent [87] describes two methods for constructing ease-in/ease-out
distance-time functions. One is to use sinusoidal pieces for the acceleration/
deceleration areas of the function and a constant velocity in the middle. The
pieces are carefully chosen to ensure C1 continuity over the entire function.
The user speciﬁes percentages of the interval that are used for acceleration
and deceleration, represented by k1 and k2. If the curve is normalized over the
interval [0, 1], then an object that moves along the curve will accelerate from
s(t) = rt
Time
Distance
Figure 9.23 Example of distance-time graph: moving at constant speed.

466
Chapter 9 Curves
Time
Distance
Figure 9.24 Example of distance-time graph. Ease-in/ease-out function.
Q(0) to Q(k1), move at constant velocity until Q(k2), and then decelerate until
the end at Q(1). The piecewise function constructed is
ease(t) =




k1 2
π

sin

t
k1
π
2 −π
2

+ 1
 -
f
0 ≤t ≤k1

k1 2
π + t −k1
 -
f
k1 ≤t ≤k2

k1 2
π + k2 −k1 +

(1 −k2) 2
π

sin

t−k2
1−k2
π
2
 -
f
k2 ≤t ≤1
where f = k1 2
π + k2 −k1 + (1 −k2) 2
π .
The second method involves setting a maximum velocity that we wish
to attain in the center part of the function, and assumes that we move with
constant acceleration in the opening and closing ease-in/ease-out areas. This
gives a velocity-time curve as in Figure 9.25. By integrating this, we get
a distance-time curve. By assuming that we start at the beginning of the curve,
v0
Time
Velocity
Figure 9.25 Example of velocity-time function. Ease-in/ease-out with constant
acceleration/deceleration.

9.12 Camera Control 467
this gives us a piecewise curve with parabolic acceleration and deceleration:
ease(t) =



v0 t2
2k1
0 ≤t ≤k1
v0 k1
2 + v0(t −k1)
k1 ≤t ≤k2
v0 k1
2 + v0(k2 −k1) +

v0 −1
2

v0 t−k2
1−k2

(t −k2)
k2 ≤t ≤1
Which one we use depends on the needs of the application. The sinu-
soidal implementation has fewer parameters for the user to manage, but
provides no control over the velocity reached during the constant velocity
section.
9.12 Camera Control
Demo
Camera Control
One common use for a parametric curve is as a path for controlling the motion
of a virtual camera. In games this comes into play most often when setting
up in-game cinematics, where we want to play a series of scripted events in
engine while giving it a cinematic feel via the clever use of camera control. For
example, we might want to have a camera track around a pair of characters as
they dance about a room. Or, we might want to simulate a crane shot zooming
from a far point of view right down into a close-up. While either of these could
be done programmatically, it would be better to provide external control to
the artist, who will most likely be setting up the shot. The artist sets the path
for the camera — all the programmer needs to do is provide code to move the
camera along the given path.
Determining the position of the camera isn’t a problem. Given the start
time ts for the camera and the current time tc, we compute the parameter
t = tc −ts and then use our time controls together with our curve description
to determine the current position at Q(t).
Computing orientation is another matter. The most basic option is to set a
ﬁxed orientation for the entire path. This might be appropriate if we are trying
to create the effect of a panning shot but is rather limiting and somewhat
static. Another way would be to set orientations at each sample time as well as
positions, and interpolate orientations. Techniques for handling this situation
are discussed in Chapter 10, but for now we’ll assume that we don’t have such
sample orientations available.
A further possibility is to use the Frenet frame for the curve. This is an
orthonormal frame with an origin of the current position on the curve, and
a basis {ˆu, ˆv, ˆw} where ˆu points in the direction of the ﬁrst derivative, ˆv points
roughly in the direction of the second derivative, and ˆw is the cross product
of the ﬁrst two. The vector ˆu acts as our view direction vector, ˆv acts as our
view side vector, and ˆw acts as our view up vector.

468
Chapter 9 Curves
For any curve speciﬁed by the matrix form Q(u) = UMG, we can eas-
ily compute the ﬁrst derivative by using the form Q′(u) = U′MG, where for
a cubic curve
U′ =

3u2
2u
1
0

Similarly, we can compute the second derivative as Q′′(u) = U′′MG where
U′′ =
 6u
2
0
0 
Setting u = Q′(u), we can compute v using Gram-Schmidt orthogonalization:
v = Q′′(u) −u · Q′′(u)
u · u
u
Finally, w is the cross product of these two:
w = u × v
Normalizing u,v, and w gives us our orthonormal basis.
Parent [87] describes a few ﬂaws with using the Frenet frame directly.
First of all, the second derivative may be 0. We can handle this situation
by interpolating between two frames on either side of our current location.
Since the second derivative is zero, or near zero, the ﬁrst derivative won’t
be changing much, so we’re really interpolating between two frames in R2.
This consists of ﬁnding the angle between them and interpolating along that
angle (Figure 9.26). The one ﬂaw with this is that when ﬁnding these frames
we’re still using Q′′, which may be near zero and hence lead to ﬂoating-point
issues. In particular, if we are moving with linear motion, there will be no
valid neighboring values for estimating Q′′.
Then, too, it assumes that the second derivative exists for all values of
t, namely, that Q(t) is C2 continuous. Many of the curves we’ve discussed,
w0
w1
v0
v1
u
u
Figure 9.26 Interpolating between two path frames.

9.12 Camera Control 469
Figure 9.27 Frame interpolation issues. Discontinuity of second derivative at
point.
in particular the piecewise curves, do not meet this criterion. In such cases
the camera will rather jarringly change orientation. For example, suppose we
have two curve segments as seen in Figure 9.27, where the second derivative
instantly changes to the opposite direction at the join between the segments.
In the Frenet frame for the ﬁrst segment, the w vector points out of the page. In
the second segment, it points into the page. As the camera crosses the join, it
will instantaneously ﬂip upside down. This is probably not what the animator
had in mind.
Finally, we may not want to use the second derivative at all. For example,
if we have a path that heads up and then down, like a hill on a roller coaster,
the direction of the second derivative points generally down along that section
of path. This means that our view up vector will end up parallel to the ground
for that section of curve — again, probably not the intention of the animator.
One solution is to adopt the technique from Chapter 5 and use the ﬁrst
derivative as our view direction vector, computing the view up vector from
this and the world up vector. The view side vector is the cross product of these
two. This solves the problem, but does mean that if we have a ﬁxed up-vector
we can’t roll our camera through a banking turn — its up vector will remain
relatively aligned with the given up-vector.
A reﬁnement of this is to allow user-speciﬁed up vectors at each sample
position, which default to the world up-vector. The program would interpo-
late between these up vectors just as it interpolates between the positions.
Alternatively, the user could set a path U(t) that is used to calculate the up
vector: vup = U(t) −Q(t). The danger here is that the user may specify two up
vectors of opposing directions that end up interpolating to 0, or an up vector
that aligns with the view direction vector, which would lead to a cross product
of 0. If the user is allowed this kind of ﬂexibility, recovery cases and some sort
of error message will be needed.
We can take this one step further; separate our view direction from the
Frenet frame and use our familar look-at point method, again from Chapter 5.

470
Chapter 9 Curves
The choice of what we use as our look-at point can depend on the camera
effect desired. For example, we might pick a ﬁxed point on the ground and
then perform a ﬂy-by. We could use the position of an object, or the centroid
of positions for a set of objects. We could set an additional path, and use the
position along that path at our current time, to give the effect of a moving
point of view without tying it to a particular object.
Another possibility is to look ahead along our current path a few steps
in time, as if we were following an object a few seconds ahead of us. So
if we’re at position Q(t), we use as our look-at point the position Q(t + δt).
In this situation, we have to be sure to reparameterize the curve based on
arc length, because otherwise the distance ∥Q(t) −Q(t + δt)∥may change
depending on where we are on the curve, which may lead to odd changes in
the view direction.
An issue with this technique is that it may make the camera seem clair-
voyant, which can ruin the drama in some situations. Also, if our curve is
particularly twisty, looking ahead may lead to sudden changes in direction.
We can smooth this by averaging a set of points ahead of our position on the
curve. How separated the points are makes a difference: too separated and
our view direction may not change much. Too close together and the smooth-
ing effect will be nulliﬁed. It’s usually best to make the amount of separation
another setting available to the animator so that he or she can control the
effect desired.
9.13 Chapter Summary
In this chapter we have touched on some of the issues involved with using
parametric curves to aid in animation. We have discussed the most com-
monly used of the many possible curve types and how to render and subdivide
these curves. Possible interfaces have been presented that allow animators
and designers to create curves that can be used in the games they create. We
have also covered some of the most common animation tasks: controlling
travel speed along curves and maintaining a logical camera orientation.
For further reading, Rogers and Adams [95] and Bartels, Beatty, and
Barsky [8] present much of this material in greater detail, in particular focus-
ing on B-splines. Parent [87] covers the use of splines in animation, as well as
additional animation techniques. Burden and Faires [17] have a chapter on
interpolation and explain some of the numerical methods used with curves,
in particular integration techniques and the Newton-Raphson method.
We have not discussed parametric surfaces, but many of the same princi-
ples apply: surfaces are approximated or interpolated by a grid of points and
are usually rendered using a subdivision method. Rogers [94] is an excellent
resource for understanding how NURBS surfaces, the most commonly used
parametric surfaces, are created and used.

Chapter10
Orientation
Representation
10.1 Introduction
So far in our exploration of animation we’ve considered only interpolation
of position. For a coordinate frame, this means only translating the frame in
space, without considering rotation. This is ﬁne for moving an object along
a path, assuming we wanted it to remain oriented in the same manner as
its base frame — generally, we don’t. One possibility that we mentioned in the
previous chapter is to align the forward vector of the object to the tangent vec-
tor of the curve, and use either the second derivative vector or an up vector to
build a frame. This will work in general for airplanes and missiles, which tend
to orient along their direction of travel. But suppose we want to interpolate a
camera so that it travels sideways along a section of curve, or we’re trying
to model a helicopter, which can face in one direction while moving in
another?
Another reason we want to interpolate orientation is for the purpose of
animating a character. Usually characters are broken into a scene-graph–like
data structure, called the skeleton, where each level, or bone, is stored at a
constant translation from its parent, and only relative rotation is changed to
move a particular node (Figure 10.1). So to move a forearm, for example, we
rotate it relative to an upper arm (Figure 10.2). Accordingly, we can generate a
set of keyframes for an animated character by storing a set of poses generated
by setting rotations at each bone. To animate the character, we interpolate
from one keyframe rotation to another.
471

472
Chapter 10 Orientation Representation
Figure 10.1 Example of skeleton showing relationship between bones.
As we shall see, when interpolating orientation we can’t quite use the
same techniques as we did with position. Rotational space doesn’t behave in
the same way as positional space; we’ll be more concerned with interpolating
along the surface of a sphere instead of along a line.
Before covering interpolation of orientation, we’ll look at four different
orientation formats and compare them on the basis of the following criteria:
■Represents orientation/rotation with a small number of values
■Can be concatenated efﬁciently to form new orientations/rotations
■Rotates points and vectors efﬁciently

10.2 Rotation Matrices 473
B2
B0
B1
Figure 10.2 Relative bone poses for bending arm.
The ﬁrst item is important if memory usage is an issue, either because we are
working with a memory-limited machine such as a console, or because we
want to store a large number of animations. In either case, any reduction in
representation size means that we have freed-up memory that can be used for
more animations, for more animation frames (leading to a smoother result),
or for some other aspect of the game. Rotating points and vectors efﬁciently
may seem like an obvious requirement, but one that merits mentioning;
not all representations are good at this. Similarly, for some representations
concatenation is not possible.
Once we’ve presented these different representations, we’ll discuss inter-
polation, as well as the pros and cons of each representation for handling
that task. As we’ll see, there is no one choice that meets all of our require-
ments; each has its strengths and weaknesses in each area, depending on our
implementation needs.
10.2 Rotation Matrices
Since we have been using matrices as our primary orientation/rotation
representation, it is natural to begin our discussion with them.

474
Chapter 10 Orientation Representation
For our ﬁrst desired property, memory usage, matrices do not fare well.
Euler’s law of rotations states that the minimum number of values needed to
represent a rotation in three dimensions is 3. The smallest possible rotation
matrix requires 9 values, or 3 orthonormal basis vectors. It is possible to
compress a rotation matrix, but in most cases this is not done unless we’re
sending data across a network. Even then it is better to convert to one of the
more compact representations that we will present in the following sections,
rather than compress the matrix.
However, for the second two properties, matrices do quite well. Concate-
nation is done through a matrix-matrix multiplication, which for two 3 × 3
matrices takes 27 multiplies and 18 additions, or 45 total operations. Rotat-
ing a vector is done through a matrix-vector multiply, which for a matrix and
3-vector takes 9 multiplies and 6 additions, or 15 total operations. On a SIMD
processor, which can perform matrix and vector operations in parallel, both
of these operations can be performed even faster. One such parallel proces-
sor can do matrix-vector multiplication in 3 instructions, and matrix-matrix
multiplication in 9 instructions. Most graphics hardware has built-in circuitry
that performs similarly. And as we’ve seen, 4 × 4 matrices can be useful for
more than just rotation. Because of all these reasons, matrices continue to be
useful despite their memory footprint.
10.3 Fixed and Euler Angles
10.3.1 Definition
We’ve just stated that the minimum number of values needed to represent a
rotation in three-dimensional space is 3. As it happens, these 3 values can be
the angles of three sequential rotations around a set of orthogonal axes. In
Chapter 3, we used this as one means of building a generalized rotation matrix.
Our chosen sequence of axes in this case was z-y-x, so the values (0, π/4, π/2)
represent a rotation of 0 radians around the z-axis, followed by a rotation of
π/4 radians (or 45 degrees) around the y-axis, and concluding with a rotation
of π/2 radians (90 degrees) around the x-axis. Angles can be less than 0 or
greater than 2π, to represent reversed rotations and multiple rotations around
a given axis. Note that we are using radians rather than degrees to represent
our angles; either convention is acceptable, but the trigonometric functions
used in C or C++ expect radians.
The order we’ve given is somewhat arbitrary, as there is no standard order
that is used for the three axes. We could have used the sequence x-y-z, or z-x-y
just as well. We can even duplicate one axis, so long as it is not the same axis
in a row, so y-z-y is a valid sequence, while an axis rotation sequence such as
z-y-y is not permitted. This is because duplicating an axis is redundant and
doesn’t add an additional degree of freedom.

10.3 Fixed and Euler Angles 475
y
z
x
3
2
1
Figure 10.3 Order and direction of rotation for z-y-x ﬁxed angles.
These rotations are performed around either the world axes or the object’s
local axes. When the angles represent world axis rotations, they are usually
called ﬁxed angles (Figure 10.3). The most convenient way to use ﬁxed angles
is to create an x-, y-, or z-rotation matrix for each angle and apply it in turn to
our set of vertices. So an x-y-x ﬁxed angle representation can be concatenated
into a single matrix R = RxRyRx in matrix form.
A sequence of local axis rotations, in turn, is said to consist of Euler
angles1. The three Euler angles are commonly known as roll, pitch, and
heading, after the three axes in a ship or an airplane. Heading is also some-
times referred to as yaw. Roll represents rotation around the forward axis,
pitch rotation around a side axis, and heading rotation around the up axis
(Figure 10.4). Whether a given roll, pitch, or heading rotation is around
x, y, or z depends on how we’ve deﬁned our coordinate frame. Suppose we
are using a coordinate system where the z-axis represents up, the x-axis repre-
sents forward, and the y-axis represents left. Then heading is rotation around
the z-axis, pitch is rotation around the y-axis, and roll is rotation around the
x-axis. They are commonly applied in the order roll-pitch-heading, so the
corresponding Euler angles for our case are x-y-z.
1.
Just to be confusing, sometimes (a sequence of ) rotations around world space axes are also
referred to as Euler angles. Context should tell you which one the author means.

476
Chapter 10 Orientation Representation
Roll
Pitch
Heading
Figure 10.4 Roll, pitch, and yaw rotations relative to the local coordinate axes.
To create a rotation matrix which applies Euler angles, we concatenate
in the reverse order of ﬁxed angles. To see why, let’s take our set of
x-y-z Euler angles. We begin by applying the Rx matrix, to give us a rotation
around x. We then want to apply a rotation around the object’s local y-axis.
However, because of the x rotation, the y-axis has been transformed to a new
orientation. So if we concatenate as we normally would, our rotation will
be about the transformed y-axis, which is not what we want. To avoid this,
we transform by Ry ﬁrst, then by Rx, giving RxRy. The same is true for the
z rotation: we need to rotate around z ﬁrst to ensure we rotate around the
local z-axis, not the transformed one. The resulting matrix is
REuler = RxRyRz
So x-y-z Euler angles are the same as z-y-x ﬁxed angles.
10.3.2 Format Conversion
By concatenating three general axis rotation matrices and expanding out the
terms, we can create a generalized rotation matrix. The particular matrix will
depend on which axis rotations we’re using and whether they are ﬁxed or

10.3 Fixed and Euler Angles 477
Euler angles. For z-y-x ﬁxed angles, or x-y-z Euler angles, the matrix looks
like
R = RxRyRz =


CyCz
−CySz
Sy
SxSyCz + CxSz
−SxSySz + CxCz
−SxCy
−CxSyCz + SxSz
CxSySz + SxCz
CxCy


where
Cx = cos θx
Sx = sin θx
Cy = cos θy
Sy = sin θy
Cz = cos θz
Sz = sin θz
This should look familiar from Chapter 3. By combining terms appropriately,
this takes 6 transcendentals, 12 multiplies, and 4 adds to compute.
When possible, we can save some instructions by computing each sine
and cosine using a single sincos() call. This function is not supported on
all processors, or even in all math libraries, so we have provided a wrapper
function IvSinCosf() (accessible by including IvMath.h) that will calculate it
depending on the platform. In any case, because we can’t be guaranteed of its
availability, we will assume that the function doesn’t exist when computing
our instruction count.
We can convert from a matrix back to a possible set of ﬁxed angles by
inverting this process. Note that since we’ll be using inverse trigonometric
functions there are multiple resulting angles. We’ll also be taking a square
root, the result of which could be positive or negative. Hence, there are
multiple possibilities of Euler or ﬁxed angles for a given matrix — the best
we can do is ﬁnd one. Assuming we’re using z-y-x ﬁxed angles, we can see
that sin θy is equal to R02. Finding cos θy can be done by using the identity
cos θy =

1 −sin2 θy. The rest falls out from dividing quantities out of the ﬁrst
row and last column of the matrix, so
sin θy = R02
cos θy =

1 −sin2 θy
sin θx = −R12/ cos θy
cos θx = R22/ cos θy
sin θz = −R01/ cos θy
cos θz = R00/ cos θy

478
Chapter 10 Orientation Representation
Note that we have no idea whether cos θy should be positive or negative, so
we assume that it’s positive. Also, if cos θy = 0, then the x and z axes have
become aligned (see Section 10.3.5) and we can’t distinguish between rota-
tions around x and rotations around z. One possibility is to assume that
rotation around z is 0, so
sin θz = 0
cos θz = 1
sin θx = R21
cos θx = R11
Calling arctan2() for each sin/cos pair will return a possible angle in radians,
generally in the range [0, 2π). Note that we have lost one of the few beneﬁts
of ﬁxed/Euler angles, which is that it can represent multiple rotations around
an axis by using angles greater than 2π radians, or 360 degrees. We have also
lost any notion of “negative” rotation.
Assuming that cos θy is not 0, this will take 2 additions, 5 multiplies, 1
divide, and 4 transcendental functions. If it is 0, this takes 1 addition, 1
multiply, and 4 transcendentals.
10.3.3 Concatenation
Clearly, ﬁxed and Euler angles meet our ﬁrst criteria for a good orientation
representation: they use the minimum number of values. However, they don’t
really meet the remainder of our requirements. First of all, they don’t concate-
nate well. Adding angles doesn’t work: applying (π/2, π/2, π/2) twice doesn’t
end up at the same orientation as (π, π, π). The most straightforward method
for concatenating two Euler or ﬁxed angle triples is to convert each sequence
of angles to a matrix, concatenate the matrix, and then convert the matrix
back to Euler or ﬁxed angles. In the worst case, this will take 24 additions,
34 multiplies, and 10 transcendentals, and will only give an approximate
result, due to the ill-formed nature of the matrix to ﬁxed/Euler conversion.
10.3.4 Vector Rotation
Euler and ﬁxed angles also aren’t the most efﬁcient method for rotating
vectors. Recall that to rotate a vector around z uses the formula
Rz(x, y, θ) = (x cos θ −y sin θ, x sin θ + y cos θ)

10.3 Fixed and Euler Angles 479
So using the angles directly means for each axis, we compute a sine and cosine
(2 transcendental calls) and then apply the preceding formula (4 multiplies
and 2 adds). This is a total of 6 transcendental operations, 12 multiplies, and
6 adds. Even if we cache the sine and cosine values for a set of vectors, this
is still more expensive than the 9 multiplies and 6 adds of a matrix multiply.
So when rotating multiple vectors (the break-even point is 5 vectors), it’s more
efﬁcient to convert to matrix format.
10.3.5 Other Issues
As if all of these disadvantages are not enough, the fatal blow is that in certain
cases ﬁxed or Euler angles can lose one degree of freedom. We can think of this
as a mathematical form of gimbal lock. In aeronautic navigational systems,
there is often a set of gyroscopes, or gimbals, which control the orientation of
an airplane or rocket. Gimbal lock is a mechanical failure where one gimbal
is rotated to the end of its physical range and it can’t be rotated any further,
thereby losing one degree of freedom. While in the virtual world, we don’t
have mechanical gyroscopes to worry about, a similar situation can arise.
Suppose we are using x-y-z ﬁxed angles and we consider the case where,
no matter what we use for the x and z angles, we will always rotate around the
y-axis by 90 degrees. This rotates the original world x-axis — the axis we ﬁrst
rotate around — to be aligned with the world negative z-axis (Figure 10.5).
Now any rotation we do with θz will subtract from any rotation to which we
x
z
y
x
World z
y
z
Figure 10.5 Demonstration of mathematical gimbal lock. A rotation of 90 degrees
around y will lead to the local x-axis aligning with the -z world axis, and a loss of a
degree of freedom.

480
Chapter 10 Orientation Representation
x
z
y
Figure 10.6 Effect of gimbal lock. Rotating the box around the world x axis, then
world y axis, then the world z axis ends up having the same effect as rotating the box
around just the y axis.
have applied θx. The combination of x- and z-rotations can be represented
by one value θx −θz, applied as the initial x-axis rotation. For example in
Figure 10.6, applying the ﬁxed angles (π/2, π/2, π/2) gets us back to our
original (0, π/2, 0). Instead of using (θx, π/2, θz), we could just as well use
(θx −θz, π/2, 0) or (0, π/2, θz −θx). We have effectively lost one degree of
freedom.
To try this for yourself, take an object whose orientation can be clearly
distinguished, like a book or CD case. From your point of view, rotate the
object clockwise 90 degrees around an axis pointing forward (roll). Now
rotate the new top of the object away from you by 90 degrees (pitch). Now
rotate the object counterclockwise 90 degrees around an axis pointing up
(heading). The result is the same as pitching the object downward 90 degrees
(see Figure 10.6).
Still, in some cases ﬁxed or Euler angles do provide an intuitive represen-
tation for orientation. For example, in a hierarchical system it is very intuitive
to deﬁne rotations at each joint as a set of Euler angles and to constrain certain
axes to remain ﬁxed. An elbow or knee joint, for instance, could be consid-
ered a set of Euler angles with two constraints and only one axis available
for applying rotation. It’s also easy to set a range of angles so that the joint
doesn’t bend too far one way or the other. However, these limited advantages

10.4 Axis-Angle Representation 481
are not enough to outweigh the problems with ﬁxed/Euler angles. So in most
cases, ﬁxed/Euler angles are used as a means to semi-intuitively set other rep-
resentations (being aware of the dangers of gimbal lock, of course), and our
library will be no exception.
10.4 Axis-Angle Representation
10.4.1 Definition
Recall from Chapter 3 that we can represent a general rotation in R3 by an axis
of rotation, and the amount we rotate around this axis by an angle of rotation.
Therefore, we can represent rotations in two parts: a 3-vector r that lies along
the axis of rotation, and a scalar θ which corresponds to a counterclockwise
rotation around the axis, if the axis is pointing towards us. Usually, a normal-
ized vector ˆr is used instead, which constrains the four values to three degrees
of freedom, corresponding to the three degrees of freedom necessary for 3D
rotations.
Generating the axis-angle rotation that takes us from one normalized vec-
tor ˆv to another vector ˆw is straightforward (Figure 10.7). The angle of rotation
is the angle between the two vectors:
θ = arccos(ˆv · ˆw)
(10.1)
The two vectors lie in the plane of rotation, and so the axis of rotation is
perpendicular to both of them:
r = ˆv × ˆw
(10.2)
w
v
 θ
ˆr
Figure 10.7 Axis-angle representation. Rotation around r by angle θ rotates v
into w.

482
Chapter 10 Orientation Representation
Normalizing r gives us ˆr. Near-parallel vectors may cause us some problems
either because the dot product is near 0, or normalizing the cross product
ends up dividing by a near-zero value. In those cases, we set θ to 0, and ˆr to
any arbitrary, normalized vector.
10.4.2 Format Conversion
To convert an axis-angle represention to a matrix, we can use the derivation
from Chapter 3:
Rˆrθ =


tx2 + c
txy −sz
txz + sy
txy + sz
ty2 + c
tyz −sx
txz −sy
tyz + sx
tz2 + c


(10.3)
where
ˆr = (x, y, z)
c = cos θ
s = sin θ
t = 1 −cos θ
This will take 12 multiplies, 10 adds, and 2 transcendental evaluations.
Converting from a matrix to the axis-angle format has similar issues as the
ﬁxed angle format, since opposing vectors ˆr and −ˆr can be used to generate the
same rotation by rotating in opposite directions, and multiple angles (0 and
2π, for example) applied to the same axis can rotate to the same orientation.
The following method is from Eberly [29].
We begin by computing the angle. The sum of the diagonal elements, or
trace of a rotation matrix R, is equal to 2 cos θ + 1, where θ is our angle of
rotation. This gives us an easy method for computing θ:
θ = arccos
1
2(trace(R) −1)

There are three possibilities for θ. If θ is 0, then we can use any arbitrary unit
vector as our axis. If θ lies in the range (0, π), then we can compute the axis
by using the formula
R −RT = 2 sin θS
(10.4)

10.4 Axis-Angle Representation 483
where S is a skew symmetric matrix of the form
S =


0
−z
y
z
0
−x
−y
x
0


The values x, y, and z in this case are the components of our axis vector ˆr.
So we can compute r as (R21−R12, R02−R20, R10−R01), and normalize to get ˆr.
If θ equals π, then R −RT = 0, which doesn’t help us at all. In this case,
we can use another formulation for the rotation matrix, which only holds if
θ = π:
R = I + 2S2 =


1 −2y2 −2z2
2xy
2xz
2xy
1 −2x2 −2z2
2yz
2xz
2yz
1 −2x2 −2y2


The idea is that we can use the diagonal elements to compute the three axis
values. By subtracting appropriately, we can solve for one term, and then use
that value to solve for the other two. For example, R00−R11−R22+1 expands to
R00 −R11 −R22 + 1 = 1 −2y2 −2z2 −1 + 2x2 + 2z2 −1 + 2x2 + 2y2 + 1
= 4x2
So
x = 1
2

R00 −R11 −R22 + 1
(10.5)
and consequently,
y = R01
2x
z = R02
2x
To avoid problems with numeric precision and square roots of negative
numbers, we’ll choose the largest diagonal element as the term that we’ll solve
for. So if R00 is the largest diagonal element, we’ll use the preceding equations.
If R11 is the largest, then
y = 1
2

R11 −R00 −R22 + 1
x = R01
2y

484
Chapter 10 Orientation Representation
z = R12
2y
Finally, if R22 is the largest element we use
z = 1
2

R22 −R00 −R11 + 1
x = R02
2z
y = R12
2z
Computing the angle takes 1 multiply, 3 additions, and 1 arccos(). If
θ is 0, then we’re done. If 0 < θ < 2π, then computing the axis takes an
additional 6 multiplies, 5 adds, 1 divide, and 1 transcendental (we can save
the divide if we have an InvSquareRoot() function available), for a total of
7 multiplies, 8 additions, 1 divide, and 2 transcendentals. For θ = 2π, the
total is 3 multiplies, 6 additions, 1 divide, and 2 transcendentals.
10.4.3 Concatenation
Concatenating two axis-angle representations is not straightforward. One
method is to convert them to matrices, multiply, and then convert back to
the axis-angle format. Converting the pair of axis-angle rotations to matrices
takes 24 multiplies, 20 adds, and 4 transcendental functions. Added to that
operation count is the matrix multiplication, which takes 27 multiplies and
18 adds. Finally, in the worst case converting back takes 7 multiplies, 8 addi-
tions, 1 divide, and 2 transcendentals, for a total of 58 multiplies, 46 additions,
1 division, and 6 transcendentals.
10.4.4 Vector Rotation
For the rotation of a vector v by the axis-angle representation (ˆr, θ), we can
use the Rodrigues formula that we derived in Chapter 3:
Rv = cos θv + [1 −cos θ](v · ˆr)ˆr + sin θ(ˆr × v)
If we precompute cos θ and sin θ and reuse intermediary values, we can com-
pute this in 19 multiplies and 12 additions, or 31 operations. We can improve

10.5 Quaternions 485
this slightly by using the identity
ˆr × (ˆr × v) = (v · ˆr)ˆr −(ˆr · ˆr)v
= (v · ˆr)ˆr −v
and substituting to get an alternate Rodrigues formula:
Rv = v + (1 −cos θ)[ˆr × (ˆr × v)] + sin θ(ˆr × v)
This will require only 18 multiplies and 12 additions,
assuming that
(1 −cos θ) and sin θ are precomputed. In both these cases, the trade-off is
whether to store the results of the transcendental functions and thereby use
more memory, or compute them every time and lose speed. The answer will
depend on the needs of the implementation.
When rotating two or more vectors, it is more efﬁcient to convert the
axis-angle format to a matrix and then multiply. Assuming that we haven’t
pregenerated the sine and cosine values, this takes 12 + 9x multiplies,
10 + 6x adds, and 2 transcendental evaluations, where x is the number of vec-
tors we’re transforming. The break-even point is two vectors, so if you’re only
transforming one vector, don’t bother converting; otherwise, use a matrix.
10.4.5 Section Summary
While being a useful way of thinking about rotation, the axis-angle format
still has some problems. Concatenating two axis-angle representations is
extremely expensive. And unless we store two additional values, rotating vec-
tors requires computing transcendental functions, which is not very efﬁcient
either. Our next representation encapsulates some of the useful properties of
the axis-angle format, while providing a more efﬁcient method for concate-
nation. It precomputes the transcendental functions and uses them to rotate
vectors in nearly equivalent time to the axis-angle method. Because of this,
we have not explicitly provided an implementation for the axis-angle format.
10.5 Quaternions
10.5.1 Definition
Library
IvMath
Filename
IvQuat
The ﬁnal orientation representation we’ll consider could be considered a vari-
ant of the axis-angle representation, and in fact it’s often simplest to think
of it that way. It is called the quaternion and was created by the Irish math-
ematician Sir William Hamilton [54] in the 19th century and introduced to

486
Chapter 10 Orientation Representation
computer graphics by Ken Shoemake [98] in the 1980s. Quaternions require
only four values, they don’t have problems of gimbal lock, the mathematics
for concatenation is relatively simple, and if properly constructed they can be
used to rotate vectors in a reasonably efﬁcient manner.
Hamilton’s general formula for a quaternion q is as follows:
q = w + xi + yj + zk
The quantities i, j, and k can be thought of as the standard basis for all
quaternions, so it is common to write a quaternion as just
q = (w, x, y, z)
The xi+yj+zk part of the quaternion is akin to a vector in R3, so a quaternion
can also be written as
q = (w, v)
where w is called the scalar part and v is called the vector part.
Frequently, we’ll want to use vectors in combination with quaternions.
To do so, we’ll zero out the scalar part and set the vector part equal to our
original vector. So the quaternion corresponding to a vector u is
qu = (0, u)
Other than terminology, we aren’t that concerned about Hamilton’s
intentions for generalized quaternions, because we are only going to con-
sider a specialized case discovered by Arthur Cayley [18]. He determined
that if you took a quaternion with four values (as just described), treated
it like a fourth-dimensional vector and normalized it, it can be used to
describe pure rotations. Later on, Courant and Hilbert [21] determined
the relationship between normalized quaternions and the axis and angle
representation.
10.5.2 Rotation Quaternions
Since we want to represent rotations, we will be normalizing all of our quater-
nions. In a normalized quaternion, w can be thought of as representing the
angle of rotation θ. More speciﬁcally, w = cos(θ/2). The vector v represents
the axis of rotation, but normalized and scaled by sin(θ/2). So v = sin(θ/2)ˆr.
For example, suppose we wanted to rotate by 90 degrees around the z-axis.

10.5 Quaternions 487
Our axis is (0, 0, 1) and half our angle is π/4 (in radians). The corresponding
quaternion components are
w = cos
π
4

=
√
2
2
x = 0 · sin
π
4

= 0
y = 0 · sin
π
4

= 0
z = 1 · sin
π
4

=
√
2
2
giving us a ﬁnal quaternion of
q =
√
2
2 , 0, 0,
√
2
2

So why reformat our previously simple axis and angle to this somewhat
strange representation? As we’ll see shortly, pre-cooking the data in this way
allows us to concatenate, rotate vectors, and interpolate with ease.
As with the axis-angle format, it is often useful to create a quaternion
that rotates a vector v1 into another vector v2, although in this case we’ll use
a different approach. Melax [76] provides a method that uses trigonometric
identities for efﬁciency’s sake, and also avoids some issues with numerical
error when v1 and v2 are nearly collinear.
We begin by normalizing v1 and v2. We’ll deﬁne r as ˆv1 × ˆv2, and d as
ˆv1 · ˆv2. We know that ∥r∥= sin θ and d = cos θ, but what we want is sin(θ/2)
and cos(θ/2). From half-angle trigonometric identities, we know that
cos
θ
2

=
.
1 + cos θ
2
sin
θ
2

=
.
1 −cos θ
2
We could use these to compute w, and then normalize r and multiply by
sin(θ/2). However, by normalizing and then re-scaling by sin(θ/2), we are
actually scaling by
sin
θ
2

sin θ
=
.
1 −cos θ
2
√
1 −cos2 θ
=
 
1 −cos θ
2(1 −cos2 θ)

488
Chapter 10 Orientation Representation
=
 
1 −cos θ
2(1 + cos θ)(1 −cos θ)
=
 
1
2(1 + cos θ)
=
1
√2(1 + cos θ)
So we can precompute s, where s = √2(1 + cos θ), and scale r by 1/s to
compute v directly. And as it happens w = s/2, since
s/2 =
√2(1 + cos θ)
2
=
.
2(1 + cos θ)
4
=
.
(1 + cos θ)
2
= cos
θ
2

= w
The ﬁnal formulas for computing the quaternion are
r = ˆv1 × ˆv2
s =

2(1 + ˆv1 · ˆv2)
q = (s/2, r/s)
Our class implementation for quaternions looks like
class IvQuat
{
public:
// constructor/destructor
inline IvQuat() {}
inline IvQuat( float_w, float _x, float _y, float _z )
:
w(_w), x(_x), y(_y), z(_z)
{
}
IvQuat(const IvVector3& axis, float angle);
IvQuat(const IvVector3& v1, const IvVector3& v2);

10.5 Quaternions 489
explicit IvQuat(const IvVector3& vector);
inline ∼IvQuat() {}
// member variables
float x, y, z, w;
};
Much of this follows from what we’ve already discussed. We can set our
quaternion values directly, use an axis-angle format, compute rotation from
two vectors, or explicitly use a vector. Recall that in this last case, we use the
vector to set our x, y, and z terms, and set w to 0.
10.5.3 Format Conversion
Converting from axis-angle format to a quaternion takes 1 multiply for the
half-angle, 2 function calls for the sine and cosine, and 3 multiplies to scale
the axis vector. To convert back, we take the arccos of w to get half the angle,
and then use
√
1 −w2 to get the length of v so we can normalize it. The full
conversion is
θ = 2 arccos(w)
∥v∥=

1 −w2
ˆr = v/∥v∥
This takes 1 addition, 5 multiplies, 1 divide, and 2 transcendental functions.
Converting a normalized quaternion to a 3 × 3 rotation matrix takes the
following form:
Mq =


1 −2y2 −2z2
2xy −2wz
2xz + 2wy
2xy + 2wz
1 −2x2 −2z2
2yz −2wx
2xz −2wy
2yz + 2wx
1 −2x2 −2y2


(10.6)
If the quaternion is not normalized, we need to scale the matrix by
1
w2 + x2 + y2 + z2
There is a lot of duplication of terms here, so on a serial processor this can be
done with 12 multiplies and 12 adds if normalized, plus an additional 3 adds,

490
Chapter 10 Orientation Representation
4 multiplies, and a ﬂoating-point divide if not normalized. The following is
derived from Shoemake [99]:
IvMatrix33&
IvMatrix33::Rotation( const IvQuat& q )
{
float s, xs, ys, zs, wx, wy, wz, xx, xy, xz, yy, yz, zz;
// if q is normalized, s = 2.0f
s = 2.0f/( q.x*q.x + q.y*q.y + q.z*q.z + q.w*q.w );
xs = s*q.x;
ys = s*q.y;
zs = s*q.z;
wx = q.w*xs;
wy = q.w*ys;
wz = q.w*zs;
xx = q.x*xs;
xy = q.x*ys;
xz = q.x*zs;
yy = q.y*ys;
yz = q.y*zs;
zz = q.z*zs;
mV[0] = 1.0f - (yy + zz);
mV[3] = xy - wz;
mV[6] = xz + wy;
mV[1] = xy + wz;
mV[4] = 1.0f - (xx + zz);
mV[7] = yz - wx;
mV[2] = xz - wy;
mV[5] = yz + wx;
mV[8] = 1.0f - (xx + yy);
return *this;
}
// End of Rotation()
If we have a parallel vector processor that can perform fast matrix mul-
tiplication, another way of doing this is to generate two 4 × 4 matrices and
multiply them together:
Mq =


w
−z
y
x
z
w
−x
y
−y
x
w
z
−x
−y
−z
w




w
−z
y
−x
z
w
−x
−y
−y
x
w
−z
x
y
z
w


If the quaternion is normalized, the product will be the homogeneous rotation
matrix corresponding to the quaternion.

10.5 Quaternions 491
To convert a matrix to a quaternion, we can use an approach that com-
bines our matrix to axis-angle conversion with our method of creating a
quaternion from two vectors. Recall that the trace of a rotation matrix is
2 cos θ + 1, where θ is our angle of rotation. Assuming that the trace is greater
than 0, if we add 1 to this and take the square root, we get the same s as when
we rotated one vector into another:
s =

2(cos θ + 1)
so
w = s/2
as before. From equation 10.4, we know that the vector r = (R21 −R12,
R02−R20, R10−R01) will have length 2 sin θ. The value s is equal to sin θ/ sin(θ/2),
so we need to scale r by 1/(2s) to give it length sin(θ/2), or
x = (R21 −R12)/(2s)
y = (R02 −R20)/(2s)
z = (R10 −R01)/(2s)
If the trace of the matrix is less than zero, then this will not work. We’ll
need to use an approach similar to when we extracted the axis from a rotation
matrix. By taking the largest diagonal element and subtracting the elements
from it, we can derive an equation to solve for a single axis component
(e.g., equation 10.5). Using that value as before, we can then compute the
other quaternion components from the elements of the matrix.
So if the largest diagonal element is R00:
x = 1
2

R00 −R11 −R22 + 1
y = R01 + R10
4x
z = R02 + R20
4x
w = R21 −R12
4x

492
Chapter 10 Orientation Representation
If the largest diagonal element is R11:
y = 1
2

R11 −R00 −R22 + 1
x = R01 + R10
4y
z = R12 + R21
4y
w = R02 −R20
4y
And if the largest diagonal element is R22:
z = 1
2

R22 −R00 −R11 + 1
x = R02 + R20
4z
y = R21 + R12
4z
w = R10 −R01
4z
Converting from a ﬁxed angle format to a quaternion requires creating a
quaternion for each rotation around a coordinate axis, and then concatenating
them together. For the z-y-x ﬁxed angle format, the result is
w = cos θx
2 cos θy
2 cos θz
2 −sin θx
2 sin θy
2 sin θz
2
x = sin θx
2 cos θy
2 cos θz
2 + cos θx
2 sin θy
2 sin θz
2
y = cos θx
2 sin θy
2 cos θz
2 −sin θx
2 cos θy
2 sin θz
2
z = cos θx
2 cos θy
2 sin θz
2 + sin θx
2 sin θy
2 cos θz
2
Converting a quaternion to ﬁxed or Euler angles is, quite frankly, an awful
thing to do. If it’s truly necessary (e.g., for an interface) the simplest method
is to convert the quaternion to a matrix, and extract the Euler angles from the
matrix.

10.5 Quaternions 493
10.5.4 Addition and Scalar Multiplication
Like vectors, quaternions can be scaled and added componentwise. For both
operations a quaternion acts just like a 4-vector, so
(w1, x1, y1, z1) + (w2, x2, y2, z2) = (w1 + w2, x1 + x2, y1 + y2, z1 + z2)
a(w, x, y, z) = (aw, ax, ay, az)
The algebraic rules for addition and scalar multiplication that apply to vectors
and matrices apply here, so like them, the set of all quaternions is also a
vector space. However, the set of normalized quaternions is not, since neither
operation maintains unit length. Therefore, if we use one of these operations,
we’ll need to normalize afterwards to ensure that we’re using a proper rotation
quaternion.
We’ll use scale primarily for normalization purposes, and addition will be
used together with scale for linear interpolation. We’ll also see another use
for addition when we discuss using quaternions in physical simulation. The
implementation of these operations is similar to that for vectors.
10.5.5 Negation
Negation is a subset of scale, but it’s worth discussing separately. One would
expect that negating a normalized quaternion would produce a quaternion
that applies a rotation in the opposite direction — it would be the inverse.
However, while it does rotate in the opposite direction, it also rotates around
the negative axis. The end result is that a vector rotated by either quaternion
ends up in the same place, but if one quaternion rotates by θ radians around
ˆr, its negation rotates 2π −θ radians around −ˆr. Figure 10.8 shows what this
looks like on the rotation plane. The negated quaternion can be thought of as
“taking the other way around,” but both quaternions rotate the vector to the
same orientation. This will cause some issues when we get to interpolation
but can be handled by adjusting our values appropriately, which we’ll discuss
next. Otherwise, we can use q and −q interchangeably.
10.5.6 Magnitude and Normalization
As mentioned, we will normalize quaternions as if we were using 4-vectors.
The magnitude of a quaternion is therefore as follows:
∥q∥=

(w2 + x2 + y2 + z2)

494
Chapter 10 Orientation Representation
rˆ
 θ
 2π–θ
–rˆ
w
w
v
v
Figure 10.8 Comparing rotation performed by a normalized quaternion (left) with
its negation (right).
A normalized quaternion ˆq is
ˆq =
q
∥q∥
Since we’re assuming that our quaternions are normalized, we’ll forgo the use
of the notation ˆq to keep our equations from being too cluttered.
10.5.7 Dot Product
The dot product of two quaternions should also look familiar:
q1 · q2 = w1w2 + x1x2 + y1y2 + z1z2
As with vectors, this is still equal to the cosine of the angle between the
quaternions, except that our “angle” is in four dimensions instead of the
usual three. What this gives us is a way of measuring how “different” two
quaternions are. If q1 · q2 is close to 1 (remember that they’re normalized),
then they apply very similar rotations. Also, since we know that the nega-
tion of a quaternion performs the same rotation as the original, if the dot
product is close to −1 the two still apply very similar rotations. So parallel nor-
malized quaternions (|q1 · q2| ≈1) are similar. Correspondingly, orthogonal
normalized quaternions (q1 · q2 = 0) produce extremely different rotations.

10.5 Quaternions 495
10.5.8 Concatenation
As with matrices, if we wish to concatenate the transformations performed
by two quaternions, we multiply them together to get a new quaternion.
Expanding out the terms of the multiplication produces the following result:
(w2 + x2i + y2 j + z2k)(w1 + x1i + y1 j + z1k)
(10.7)
= w2w1 + w2x1i + w2y1 j + w2z1k
+ x2w1i + x2x1i2 + x2y1ij + x2z1ik
+ y2w1 j + y2x1 ji + y2y1 j2 + y2z1 jk
+ z2w1k + z2x1ki + z2y1kj + z2z1k2
We deﬁne the products of the i, j, k quantities as follows:
ij = k
jk = i
ki = j
ji = −k
kj = −i
ik = −j
and
i2 = j2 = k2 = ijk = −1
Note that order does matter.
We can use these properties and well-known vector operations to simplify
the product to
q2 · q1 = (w1w2 −v1 · v2, w1v2 + w2v1 + v2 × v1)
Note that we’ve expressed this in a right-to-left order, like our matrices. This is
because the rotation deﬁned by q1 will be applied ﬁrst, followed by the rotation
deﬁned by q2. We’ll see this more clearly when we look at how we use quater-
nions to transform vectors. Also note the cross product; due to this, quaternion
multiplication is also not commutative. This is what we expect with rotations;
applying two rotations in one order does not necessarily provide the same
result as applying them in the reverse order.
Multiplying two normalized quaternions does produce a normalized
quaternion. However, due to ﬂoating-point error, it is wise to renormalize the
result — if not after every multiplication, at least often and deﬁnitely before
using the quaternion to rotate vectors.
A straightforward implementation of quaternion multiplication might
look like

496
Chapter 10 Orientation Representation
IvQuat operator*(IvQuat q2, IvQuat q1)
{
IvVector3 v1(q1.x, q1.y, q1.z);
IvVector3 v2(q2.x, q2.y, q2.z);
float w = q1.w*q2.w - v1.Dot(v2);
IvVector3 v = q1.w*v2 + q2.w*v1 + v2.Cross(v1);
IvQuat q(w, v);
return q;
}
Alternatively, we can unroll the operations to get
IvQuat operator*(IvQuat q2, IvQuat q1)
{
w = q2.w*q1.w - q2.x*q1.x
- q2.y*q1.y - q2.z*q1.z;
x = q2.y* q1.z - q2.z*q1.y
+ q2.w*q1.x + q1.w*q2.x;
y = q2.z*q1.x - q2.x*q1.z
+ q2.w*q1.y + q1.w*q2.y;
z = q2.x*q1.y - q2.y*q1.x
+ q2.w*q1.z + q1.w*q2.z;
return IvQuat(w,x,y,z);
}
This takes 16 multiplies and 12 additions, so concatenating two quater-
nions is actually faster than multiplying two matrices together.
An example of concatenating quaternions is the conversion from z-y-x
ﬁxed-angle format to a quaternion. The corresponding quaternions for each
axis are
qz =

cos θz
2 , 0, 0, sin θz
2

qy =

cos θy
2 , 0, sin θy
2 , 0

qx =

cos θx
2 , sin θx
2 , 0, 0

Multiplying
these
together
in
the
order
qxqyqz
gives
the
result
in
Section 10.5.3.

10.5 Quaternions 497
10.5.9 Identity and Inverse
As with matrix products, there is an identity quaternion and, subsequently,
there are multiplicative inverses. The identity quaternion is (1, 0, 0, 0), or (1, 0).
Multiplying this by any quaternion q = (w, v) gives
q · (1, 0) = (1 · w −0 · v, 1v + w0 + v × 0)
= (w, v)
In this case multiplication is commutative, so q · (1, 0) = (1, 0) · q = q.
As with matrices, the inverse q−1 of a quaternion q is one such that q−1q =
qq−1 = (1, 0). If we consider a quaternion as rotating θ degrees counterclock-
wise around an axis ˆr, then to undo the rotation we should rotate θ degrees
clockwise around the same axis. This is the same as rotating −θ degrees coun-
terclockwise: to create the inverse we negate the angle (Figure 10.9a). So if
(w, v) =

cos
θ
2

, ˆr sin
θ
2

then
(w, v)−1 =

cos

−θ
2

, ˆr sin

−θ
2

=

cos
θ
2

, −ˆr sin
θ
2

(w, v)−1 = (w, −v)
(10.8)
At ﬁrst glance,
negating the vector part of the quaternion to reverse
the rotation is counterintuitive. But after some thought this still makes
rˆ
θ
– θ
Figure 10.9a Relationship between quaternion and its inverse. Inverse rotates
around same axis but negative angle.

498
Chapter 10 Orientation Representation
rˆ
–rˆ
θ
– θ
v
v
w
w
Figure 10.9b Rotation direction around axis by negative angle is same as rotation
direction around negative axis by positive angle.
sense geometrically. A clockwise rotation around an axis turns in the same
direction as a counterclockwise rotation around the negative of the axis
(Figure 10.9b).
Equation 10.8 only holds if our quaternion is normalized. While it should
be since we’re working with rotation quaternions, if it is not then we need to
scale by one over the length squared, or
q−1 =
1
∥q∥2 (w, −v)
(10.9)
Avoiding the ﬂoating-point divide in this case is another good reason to keep
our quaternions normalized.
It bears repeating that the negative of a quaternion, where both w and
v are negated, is not the same as the inverse. When applied to vectors, the
negative actually rotates the vector to the same orientation but taking the
other way around the axis.
10.5.10 Vector Rotation
If qr is used to concatenate two quaternions q and r, then for a vector p we
might expect qp to rotate the vector by the quaternion, just as it does for
a matrix. Unfortunately for intuition, this is not the case. For one thing, the
result of this multiplication is not a vector (w will not be 0). The actual formula

10.5 Quaternions 499
for rotating a vector by a quaternion is
Rqp = qpq−1
(10.10)
It may look like the effect of the operation is to perform the rotation and then
undo it, but this is not the case. Remember that quaternion multiplication is
not commutative, so if q is not the identity:
qpq−1 ̸= qq−1p = p
We can use our rotation formula for axis and angle to show that equa-
tion 10.10 does rotate a vector. We begin by breaking it out into its component
vector operations. Assuming that our quaternion is normalized, if we expand
the full multiplication and combine terms, we get
Rqp = (2w2 −1)p + 2(v · p)v + 2w(v × p)
(10.11)
Substituting cos(θ/2) for w, and ˆr sin(θ/2) for v:
Rq (p) =

2 cos2
θ
2

−1

p + 2

ˆr sin
θ
2

· p

ˆr sin
θ
2

+ 2 cos
θ
2
 
ˆr sin
θ
2

× p

Reducing terms and using the appropriate trigonometric identities, we end
up with
Rq(p) =

cos2
θ
2

−sin2
θ
2

p + 2 sin2
θ
2

(ˆr · p)ˆr + 2 cos
θ
2

sin
θ
2

(ˆr × p)
= cos θp + [1 −cos θ](ˆr · p)ˆr + sin θ(ˆr × p)
(10.12)
We see that equation 3.13 is equal to equation 10.12, so our quaternion
multiplication — odd as it may look — does rotate a vector around an axis by
a given angle.
In our code, we won’t want to use the qpq−1 form, since perform-
ing both quaternion multiplications isn’t very efﬁcient. Instead, we’ll use
equation 10.11:
IvVector3
IvQuat::Rotate( const IvVector3& vector ) const
{
ASSERT( IsUnit() );

500
Chapter 10 Orientation Representation
float vMult = 2.0f*(x*vector.x + y*vector.y + z*vector.z);
float crossMult = 2.0f*w;
float pMult = crossMult*w - 1.0f;
return IvVector3( pMult*vector.x + vMult*x + crossMult*(y*vector.z - z*vector.y),
pMult*vector.y + vMult*y + crossMult*(z*vector.x - x*vector.z),
pMult*vector.z + vMult*z + crossMult*(x*vector.y - y*vector.x) );
}
// End of IvQuat::Rotate()
The operation count is 21 multiplications and 12 additions, which is still
more than the 9 multiplications and 6 additions of matrix multiplication, but
comparable to the 18 multiplications and 12 additions of Rodrigues’ formula
for axis-angle.
An alternate version:
Rqp = (v · p)v + w2p + 2w(v × p) + v × (v × p)
is useful for processors that have fast cross-product operations.
Neither of these formulas is as efﬁcient as matrix multiplication, but for
a single vector it is more efﬁcient to perform these operations rather than
convert the quaternion to a matrix and then multiply. However, if we need to
rotate multiple vectors by the same quaternion, matrix conversion becomes
worthwhile.
To see how concatenation of rotations works, suppose we apply a rotation
from one quaternion followed by a second rotation from another quaternion.
We can rearrange parentheses to get
q(rpr−1)q−1 = (qr)p(qr)−1
As we see, concatenated quaternions will apply their rotation, one after the
other. The order is right-to-left, as we have stated.
If we substitute −q in place of q in equation 10.10, we can see in another
way how negating the quaternion doesn’t affect rotation. By equation 10.8,
(−q)−1 = −q−1, so
R−q(p) = −qp(−q)−1
= qpq−1
The two negatives cancel, and we’re back with our familar result.

10.6 Interpolation 501
10.5.11 Quaternions and Transformations
Demo
Transform
While quaternions are good for rotations, they don’t help us much when per-
forming translation and scale. Fortunately, we already have a transformation
format that quaternions ﬁt right into. Recall that in Chapter 3, instead of using
a generalized 4 × 4 matrix for afﬁne transformations, we used a single scale
factor s, a 3 × 3 rotation matrix R, and a translation vector t. Our formula for
transformation was
p′ = R(sp) + t
We can easily replace our matrix R with an equivalent quaternion r, which
gives us
p′ = r(sp)r−1 + t
Concatenation using the quaternion is similar to concatenation with our orig-
inal separated format, except that we replace multiplication by the rotation
matrix with quaternion operations:
s′ = s1s0
r′ = r1r0
t′ = t1 + r1(s1t0)r−1
1
Again, to add the translations, we ﬁrst need to scale t0 by s1 and then rotate
by the quaternion r1.
As with lone quaternions, concatenation on a serial processor can be much
cheaper in this format than using a 4 × 4 matrix. However, transformation of
points is more expensive. As was the case with simple rotation, for multiple
points it will be better to convert the quaternion to a matrix and transform
them that way.
10.6 Interpolation
Our interpolation problem for position was to ﬁnd a space curve — a func-
tion given a time parameter that returns a position — that passes through our
sample points and maintains our desired curvature at each sample point. The
same is true of interpolating orientation, except that our curve doesn’t pass
through a series of positions, but a series of orientations.
We can think of this as wanting to interpolate from one coordinate frame
to another. If we were simply interpolating two vectors v1 and v2, we could

502
Chapter 10 Orientation Representation
ﬁnd the rotation between them via the axis-angle representation (θ, ˆr), and
then interpolate by rotating v1 as
v(t) = R(tθ, ˆr)v1
In other words, we linearly interpolate the angle from 0 to θ and continually
apply the newly generated rotation to v1 to get our interpolated orientations.
But for a coordinate frame, we need to interpolate three vectors simultane-
ously. We could use the same process for all three basis vectors, but it’s not
guaranteed that they will remain orthogonal. What we would need to do is ﬁnd
the overall rotation in axis-angle form from one coordinate frame to another,
and then apply the process described. This is not a simple thing to do, and as
it turns out there are better ways.
However, for ﬁxed angles and axis-angle formats, we can use this to inter-
polate simple cases of rotation around a single axis. For instance, if we’re
interpolating from (90, 0, 0) to (180, 0, 0), we can linearly interpolate the ﬁrst
angle from 90 degrees to 180 degrees. Or, with an axis-angle format, if the rota-
tion is from the reference orientation to another orientation, again we only
need to interpolate the angle. Using this method also allows for interpolations
over angles greater than 360 degrees. Suppose we want to rotate twice around
the z-axis and represent this as only two values; we could interpolate between
the two x-y-z ﬁxed angles (0,0,0) and (0,0,4π). As we interpolate from 0 to 1,
our object will rotate twice. More sample orientations are needed to do this
with matrices and quaternions.
Demo
Euler
But extending this to more complex cases does not work. Suppose we take
as our starting orientation (0,90,0) and our ending orientation (90, 45, 90); if
we linearly interpolate the angles to ﬁnd a value halfway between them, we
get (45, 67.5, 45). But this is wrong. One possible value which is correct is
(90, 22.5, 90). The consequence of interpolating linearly from one sequence
of Euler angles to another is that the object tends to sidle along, rotating
around mostly one axis and then switching to rotations around mostly another
axis, instead of rotating around a single axis, directly from one orientation to
another.
We can mitigate this problem by deﬁning Hermite or higher-order splines
to better control the interpolation, and some 3D modeling packages provide
output to do just that. However, you may not want to dedicate the space for
the intermediary keyframes or the processing power to perform the spline
interpolation, and it’s still an approximation. For more complex cases, the
only two formats that are practical are matrices and quaternions, and as we’ll
see this is where quaternions truly shine.
There are generally two approaches used when interpolating matrices and
quaternions in games: linear interpolation and spherical linear interpolation.
Both methods are usually applied piecewise between each orientation sample
pair, and even though this will generate discontinuities at the sample points,

10.6 Interpolation 503
the artifacts are rarely noticeable. While we will mention some ways of com-
puting cubic curves, they generally are just too expensive for the small gain
in visual quality.
10.6.1 Linear Interpolation
Demo
LerpSlerp
By using the scalar multiplication and addition operations, we can linearly
interpolate rotation matrices and quaternions just as we did vectors. Let’s
look at a matrix example ﬁrst. Consider two orientations: one represented as
the identity matrix and the other by a rotation of 90 degrees around the z-axis.
Using linear interpolation to ﬁnd the orientation halfway between the start
and end orientations:
1
2


1
0
0
0
1
0
0
0
1

+ 1
2


0
1
0
−1
0
0
0
0
1

=


1
2
1
2
0
−1
2
1
2
0
0
0
1


The result is not a well-formed rotation matrix. The basis vectors are indeed
perpendicular, but they are not unit length. In order to restore this, we need
to perform Gram-Schmidt orthogonalization, which is a rather expensive
operation to perform every time we want to perform an interpolation.
With quaternions we run into some problems similar to those encountered
with matrices. Suppose we perform the same interpolation, from the identity
quaternion to a rotation of 90 degrees around z. This second quaternion is
(
√
2/2, 0, 0,
√
2/2). The resulting interpolated quaternion when t = 1/2 is
r = 1
2(1, 0, 0, 0) + 1
2
√
2
2 , 0, 0,
√
2
2

=

2 +
√
2
4
, 0, 0,
√
2
4

The length of r is 0.9239 — clearly, not 1. Just as with matrices, we had
to reorthogonalize after performing linear interpolation, with quaternions
we will have to renormalize. Fortunately, this is a cheaper operation than
orthogonalization, so quaternions have the advantage here.
In both cases, this happens because linear interpolation has the effect of
cutting across the arc of rotation. If we compare a vector in one orientation
with its equivalent in the other, we can get some sense of this. In the ideal
case, as we rotate from one vector to another, the tips of the interpolated
vectors trace an arc across the surface of a sphere (Figure 10.10). But as we
can see in Figure 10.11, the linear interpolation is following a line segment

504
Chapter 10 Orientation Representation
Figure 10.10 Ideal orientation interpolation,
showing intermediate vectors
tracing path along arc.
Figure 10.11 Linear orientation interpolation, showing intermediate vectors
tracing path along line.
between the two tips of the vectors, which causes the interpolated vectors to
shrink to a length of
√
2/2 at the halfway point, and then back up to 1.
Another problem with linear interpolation is that it doesn’t move at a
constant rate of rotation. Let’s divide our interpolation at the t values 0, 1/4,
1/2, 3/4, and 1. In the ideal case, we’ll travel one quarter of the arc length to
get from orientation to orientation.
However, when we use linear interpolation, the t value doesn’t inter-
polate along the arc, but along that chord which passes between the start
and end orientations. When we divide the chord into four equal parts, the
corresponding arcs on the surface of the sphere are no longer equal in
length (Figure 10.12). Those closest to the center of interpolation are longer.
The effect is that instead of moving at a constant rate of rotation throughout
the interpolation, we will move at a slower rate at the endpoints and faster
in the middle. This is particularly noticeable for large angles, as the ﬁgure
shows. What we really want is a constant change in rotation angle as we
apply a constant change in t.

10.6 Interpolation 505
Figure 10.12 Effect of linear orientation interpolation on arc length when
interpolating over 1/4 intervals.
One way to solve both of these issues is to insert one or two additional
sample orientations and use quadratic or cubic interpolation. However, these
are still only approximations to the spherical curve, and they involve storing
additional orientation keyframes.
And even if you are willing to deal with nonconstant rotation speed, and
eat the cost of orthogonalization, linear interpolation does create other prob-
lems. Suppose we use linear interpolation to ﬁnd the orientation midway
between these two matrices:
1
2


0
0
1
0
1
0
−1
0
0

+ 1
2


0
0
−1
0
1
0
1
0
0

=


0
0
0
0
1
0
0
0
0


(10.13)
This is clearly not a rotation matrix, and no amount of orthogonalization will
help us. The problem is that our two rotations (a rotation of π/2 around y and
a rotation of −π/2 around y, respectively) produce opposing orientations —
they’re 180 degrees apart. As we interpolate between the pairs of transformed
i and k basis vectors, we end up passing through the origin.
Quaternions are no less susceptible to this. Suppose we have a rotation
of π radians counterclockwise around the y-axis, and a rotation of π radians
clockwise around y. Interpolating the equivalent quaternions gives us
r = 1
2(0, 0, 1, 0) + 1
2(0, 0, −1, 0)
= (0, 0, 0, 0)
And again, no amount of normalization will turn this into a unit quaternion.
The problem here is that we are trying to interpolate between two quaternions
that are negatives of each other. They represent two rotations in the opposite

506
Chapter 10 Orientation Representation
direction that rotate to the same orientation. Rotating a vector 180 degrees
counterclockwise around y will end up in the same place as rotating the same
vector 180 degrees clockwise (or −180 degrees counterclockwise) around y.
Even if we considered this an interpolation that runs entirely around the
sphere, it is not clear which path to take — there are inﬁnitely many.
This problem with negated quaternions shows up in other ways. Let’s
look at our ﬁrst example again, interpolating from the identity quater-
nion to a rotation of π/2 around z. Recall that our result with t = 1/2 was
(2 +
√
2/4, 0, 0,
√
2/4). This time we’ll negate the second quaternion, giving us
a rotation of −3π/2 around z. We get the result
r = 1
2(1, 0, 0, 0) + 1
2

−
√
2
2 , 0, 0, −
√
2
2

=

2 −
√
2
4
, 0, 0, −
√
2
4

This new result is not the negation of the original result, nor is it the inverse.
What is happening is that instead of interpolating along the shortest arc
along the sphere, we’re interpolating all the way around the other way, via
the longest arc. This will happen when the dot product between the two
quaternions is negative, so the angle between them is greater than 90 degrees.
This may be the desired result, but usually it’s not. What we can do to coun-
teract it is to negate the ﬁrst quaternion and reinterpolate. In our example,
we end up with
r = 1
2(−1, 0, 0, 0) + 1
2

−
√
2
2 , 0, 0, −
√
2
2

=

−2 +
√
2
4
, 0, 0, −
√
2
4

This gives us the negation of our original result, but this isn’t a problem as it
will rotate to the same orientation.
This also takes care of the case of interpolating from a quaternion to its
negative, so for example, interpolating from (0, 0, 1, 0) to (0, 0, −1, 0):
r = −1
2(0, 0, 1, 0) + 1
2(0, 0, −1, 0)
= (0, 0, −1, 0)
Negating the ﬁrst one ends up interpolating to and from the same quaternion,
which is a waste of processing power, but won’t give us invalid results. Note
that we will have to do this even if we are using spherical linear interpolation,

10.6 Interpolation 507
p
q
Figure 10.13 Effect of spherical linear interpolation when interpolating at quarter
intervals. Interpolates equally along arc and angle.
which we will address next. All in all, it is better to avoid such cases by culling
them out of our data beforehand.
10.6.2 Spherical Linear Interpolation
Demo
LerpSlerp
To better solve the nonconstant rotation speed and normalization issues, we
need an interpolation method known as spherical linear interpolation (usually
abbreviated as slerp2). Slerp is similar to linear interpolation except that
instead of interpolating along a line, we’re interpolating along an arc on the
surface of a sphere. Figure 10.13 shows the desired result. When using spher-
ical interpolation at quarter intervals of t, we travel one quarter of the arc
length to get from orientation to orientation. We can also think of slerp as
interpolating along the angle, or in this case dividing the angle between the
orientations into quarter intervals.
It can be shown that for two rotations P and Q, the slerp function is
computed as follows:
slerp (P, Q, t) = P(P −1Q)t
For matrices, the question is how to take a matrix R to a power t. We can use a
method provided by Eberly [29] as follows. Since we know that R is a rotation
matrix, we can pull out the axis v and angle θ of rotation for the matrix as
we’ve described, multiply θ by t to get a percentage of the rotation, and convert
back to a matrix to get Rt. This is an extraordinarily expensive operation,
taking 77 multiplies, 58 additions, 1 division, and 6 transcendental functions.
2.
As Shoemake [98] says, because it’s fun.

508
Chapter 10 Orientation Representation
p
q
tθ
θ
(1–t)θ
r
Figure 10.14 Construction for quaternion slerp. Angle θ is divided by interpolant
t into subangles tθ and (1 −t)θ.
However, if we want to use matrices, it does give us the result we want of
interpolating smoothly along arc length from one orientation to another.
For quaternions, we can derive slerp in another way, as demonstrated by
Eberly [30]. Figure 10.14 shows the situation. We have two quaternions p and
q, and an interpolated quaternion r. The angle between p and q is θ, calculated
as θ = arccos(p · q). Since slerp interpolates the angle, the angle between p and
r will be a fraction of θ as determined by t, or tθ. Similarly, the angle between
r and q will be (1 −t)θ.
The general interpolation of p and q can be represented as
r = a(t)p + b(t)q
(10.14)
The goal is to ﬁnd two interpolating functions a(t) and b(t) so that they meet
the criteria for slerp.
We determine these as follows. If we take the dot product of p with
equation 10.14 we get
p · r = a(t)p · p + b(t)p · q
cos(tθ) = a(t) + b(t) cos θ
Similarly, if we take the dot product of q with equation 10.14 we get
cos((1 −t)θ) = a(t) cos θ + b(t)
We have two equations and two unknowns. Solving for a(t) and b(t) gives us
a(t) = cos(tθ) −cos((1 −t)θ) cos θ
(1 + cos2 θ)

10.6 Interpolation 509
b(t) = cos((1 −t)θ) −cos(tθ) cos θ
(1 + cos2 θ)
Using trigonometric identities, these simplify to
a(t) = sin((1 −t)θ)
sin θ
b(t) = sin(tθ)
sin θ
Our ﬁnal slerp equation is
slerp(p, q, t) = sin((1 −t)θ)p + sin(tθ)q
sin θ
(10.15)
As we can see, this still is an expensive operation, consisting of three
sines and a ﬂoating-point divide, not to mention the precalculation of the
arccosine. But at 16 multiplications, 8 additions, 1 divide, and 4 transcenden-
tals, it is much cheaper than the matrix method. It is clearly preferable to use
quaternions versus matrices (or any other form) if you want to interpolate
orientation.
One thing to notice is that as θ approaches 0 — as p and q become close
to equal — sin θ and thus the denominator of the slerp function approaches 0.
Testing for equality is not enough to catch this case, because of ﬁnite ﬂoating-
point precision. Instead, we should test cos θ before proceeding. If it’s close
to 1 (> (1 −ϵ), say), then we use linear interpolation or lerp instead, since
it’s reasonably accurate for small angles and avoids the undesirable case of
dividing by a very small number. It also has the nice beneﬁt of helping our
performance; lerp is much cheaper. In fact, it’s generally best only to use slerp
in the cases where it is obvious that rotation speed is changing.
Just as we do with linear interpolation, if we want to make sure that our
path is taking the shortest route on the sphere and to avoid problems with
opposing quaternions, we also need to test cos θ to ensure that it is greater
than 0 and negate the start quaternion if necessary. While slerp does maintain
unit length for quaternions, it’s still useful to normalize afterwards to handle
any variation due to ﬂoating-point error.
Cubic Methods
Just as with lerp, if we do piecewise slerp we will have discontinuities at
the sample orientations, which may lead to visible changes in orientation
rather than the smooth curve we want. And just as we had available when
interpolating points, there are cubic methods for interpolating quaternions.

510
Chapter 10 Orientation Representation
One such method is squad, which uses the formula
squad (p, a, b, q, t) = slerp (slerp(p, q, t), slerp(a, b, t), 2(1 −t)t)
(10.16)
This is a modiﬁcation of a technique of using linear interpolation to do Bezier
curves, described by Böhm [16]. It performs a Bezier interpolation from p to
q, using a and b as additional control points (or control orientations, to be
more precise).
We can use similar techniques for other curve types, such as B-splines
and Catmull-Rom curves. However, these methods usually are not used in
games. They are more expensive than slerp (which is expensive enough), and
most of the time the data being interpolated has been generated by an ani-
mation package or exists as samples from motion capture. Both of these tend
to smooth the data out and insert additional samples at places where orien-
tation is changing sharply, so smoothing the curve isn’t that necessary. For
those who are interested, Shoemake ([98],[99]) covers some of these spline
methods in more detail.
10.6.3 Performance Improvements
Demo
SlerpApprox
As we’ve seen, using slerp for interpolation, even when using quaternions, can
take quite a bit of time — something we don’t usually have. A typical character
can have 20+ bones, all of which are being interpolated once a frame. If we
have a team of characters in a room, there can be up to 20 characters being
rendered at one time. The less time we spend interpolating, the better.
The simplest speedup is to use lerp all the time. It’s very fast: ignoring the
setup time (checking angles and adjusting quaternions) and normalization,
only 12 basic ﬂoating-point operations are necessary on a serial processor,
and on a vector processor this drops to 3. We do have the problems with
inconsistent rotational speeds, but if our angles are small enough, or we’re
willing to live with it, lerp is a ﬁne solution.
However, if we want better quality, then we need to try something else.
One solution is to improve the speed of slerp. If we assume that we’re dealing
with a set of stored quaternions for keyframed animation, there are some
things we can do here. First of all, we can precompute θ and 1/sin θ for each
quaternion pair and store them with the rest of our animation data. In fact,
if we’re willing to give up the space, we could pre-scale p and q by 1/sin θ and
store those values instead. This would mean storing up to two copies for each
quaternion: one as the starting orientation of an interpolation and one as the
ending orientation. Finally, if t is changing at a constant rate, we can use
forward differencing to reduce our operations further. Shoemake [99] states
that this can be done in 8 multiplies, 6 adds, and 2 table lookups for the two
remaining sines.

10.7 Chapter Summary 511
If memory is plentiful and our frame rate is constant, then this approach
can work well. However, neither of these is typically the case. Animation
data usually takes up enough of our memory budget without nearly doubling
its size, and frame rates can be variable, depending on what is being ren-
dered or simulated. One possibility that doesn’t have these restrictions is to
approximate the most expensive operations – 1/ sin θ, sin(tθ), and sin((1−t)θ) —
by splines. This can provide reasonable accuracy for less cost than the
standard evaluation.
An alternate method is proposed by Jonathan Blow [14]. His idea is
that instead of trying to change our interpolation method to ﬁx our vari-
able rotation speeds, we adjust our t values to counteract the variations.
So in the section where an object would normally rotate faster with a con-
stantly increasing t, we slow t down. Similarly, in the section where an object
would rotate slower, we speed t up. Blow uses a cubic spline to perform this
adjustment:
t′ = 2kt3 −3kt2 + (1 + k)t
where
k = 0.5069269(1 −0.7878088 cos θ)2
and cos θ is the dot product between the two quaternions. This technique
tends to diverge from the slerp result when t > 0.5, so Blow recommends
detecting this case and swapping the two quaternions (i.e., interpolate from
q to p instead of from p to q). In this way our interpolant always lies between
0 and 0.5.
The nice thing about this method is that it requires very few ﬂoating-
point operations, doesn’t involve any transcendental functions or ﬂoating-
point divides, and ﬁts in nicely with our existing lerp functions. It gives us
slerp interpolation quality with close to lerp speed, which can considerably
speed up our animation system.
10.7 Chapter Summary
In this chapter we’ve discussed four different representations for orientation
and rotation: matrices, ﬁxed/Euler angles, axis and angle, and quaternions.
In the introduction we gave three criteria for our format: it may be informative
to compare them along with their usefulness in interpolation.
As far as size, matrices are the worst at 9 values, and ﬁxed/Euler angles
the best at 3 values. However, quaternions and axis-angle representation are

512
Chapter 10 Orientation Representation
close to ﬁxed/Euler angles at 4 values, and they avoid the problems engendered
by gimbal lock.
For concatenation, quaternions take the fewest number of operations,
followed closely by matrices, and then by axis-angle and ﬁxed/Euler represen-
tations. The last two are hampered by not having low-cost methods for direct
concatenation and so the majority of their expense is tied up in converting to
a more favorable format.
When transforming vectors, matrices are the clear winner. Assuming
pre-cached sine and cosine data, ﬁxed/Euler angles are close behind, while
axis-angle and quaternions take a bit longer. However, if we don’t pre-cache
our data, the sine and cosine computations will probably take longer, and
quaternions come in second.
Finally, ﬁxed/Euler and axis-angle formats interpolate well only under
simple circumstances. Matrices can be interpolated, but at signiﬁcantly
greater cost than quaternions. If you need to interpolate orientation, the clear
choice is to use quaternions.
For further reading about quaternions, the best place to start is the writ-
ings of Shoemake, in particular [98]. Hamilton’s original series of articles
on quaternions [54] is in the public domain, and can be found by search-
ing online. Courant and Hilbert [21] cover applications of quaternions, in
particular to represent rotations. Finally, Eberly has an article [29] com-
paring orientation formats, and an entire chapter in his latest book [30] on
quaternions, with additional material by Shoemake.

Part
IV
Simulation


Chapter11
Intersection
Testing
11.1 Introduction
In the previous chapters, we have been primarily focused on manipulating and
displaying our game objects in isolation. Whether we are rendering an object
or animating it, we haven’t been concerned with how it might be interacting
with other objects in our scene. This is neither realistic nor interesting. For
example, you are manipulating an object right now: this book. You can hold
it in your hand, turn its pages, or drop it on the ﬂoor. In the latter case it stops
reacting to you and starts reacting to the ﬂoor. If good gameplay derives from
interesting interactions, then we need some way to detect when two game
objects should be affecting one another and respond accordingly.
In this chapter we’ll be concerned with a very straightforward question:
how do we tell when two geometric entities are intersecting? This knowledge
proves useful in many cases throughout a game engine. The most obvious is
collision detection and response. Rather than have game objects pass through
each other, we want them to push against each other and respond realistically.
In the real world, this is a simple problem. Solid objects are solid; due to their
physical properties, they just don’t interpenetrate. But in the virtual world,
we have to create these constraints ourselves. Despite the fact that we have
completely deﬁned the geometry of our game objects, we still need to provide
methods to detect when they interpenetrate. Only when we have a way to
handle this can we write the code to perform the proper response.
Another time when we want to detect when two geometric entities inter-
penetrate is when we want to cast a ray and see what objects it intersects.
515

516
Chapter 11 Intersection Testing
One example of this we have seen already: detecting the object we’ve clicked
on by generating a pick ray from a screen space mouse click, and determin-
ing the ﬁrst object we hit with that ray. Another way this is used is in artiﬁcal
intelligence. In order to simulate whether one AI agent can see another, we
cast a ray from the ﬁrst to the second and see if it intersects any objects. If
not, then we can say that the ﬁrst agent’s target is in sight.
We have also mentioned a third use of object intersection before: deter-
mining which objects are visible in a view frustum so that we can do quick
visibility culling. If they interpenetrate or are inside the frustum, then we go
ahead to the rendering step; otherwise they get skipped. This can considerably
speed up our rendering.
Due to the variety of shapes and primitives used in a standard game
engine, ﬁnding intersections between all of the cases can get quite complex;
a single chapter is not enough to cover everything. Instead, we’ll cover ﬁve
basic objects, some methods for improving performance and accuracy, and
directions for improvement. We will also brieﬂy discuss how to use these
methods in a simple collision detection system, and how we can apply similar
techniques to our ray casting and frustum culling problems. Details on more
complex systems can be found in the recommended reading in the “Chapter
Summary.”
11.2 Closest Point and Distance Tests
As we’ll ﬁnd, object intersection tests can often be described more easily in
terms of a distance computation between two primitives, such as a point
and a line. In particular, we’ll often want to know if the distance between
two primitives is less than some value, such as a radius. So before we begin
our discussion of determining intersections between bounding objects, we
will cover a selection of useful methods for testing distances between certain
geometric primitives.
Related to that topic is determining the closest points of approach between
those same primitives; if we can ﬁnd the closest points, the distance between
the two primitives is the distance between those points. Because of this, we’ll
ﬁrst consider closest point problems followed by how to calculate the distance
between the same two primitives.
11.2.1 Closest Point on Line to Point
Library
IvMath
Filename
IvLine3
Our ﬁrst problem is illustrated in Figure 11.1: given a point Q, and a line
deﬁned by a point P and a vector v, how do we ﬁnd the point on the line Q′
that is closest to Q? We approach this by examining the geometric relation-
ships between the point and line. In particular, we notice that the dotted

11.2 Closest Point and Distance Tests 517
Q
Q
P
w
v
projvw
Figure 11.1 Closest point line.
line segment between Q and Q′ is orthogonal to the line. This line segment
corresponds to a line of projection: to ﬁnd Q′, we need to project Q onto
the line.
To do this, we begin by computing the difference vector w between Q and
P , or w = Q −P. Then we project this onto v, to get the component of w that
points along v. Recall that this is
projvw = w · v
∥v∥2 v
We add this to the line point P to get our projected point Q′, or
Q′ = P + w · v
∥v∥2 v
The equivalent code is
IvVector3 IvLine3::ClosestPoint(const IvVector3& point)
{
IvVector3 w = point - mOrigin;
float vsq = mDirection.Dot(mDirection);
float proj = w.Dot(mDirection);
return mOrigin + (proj/vsq)*mDirection;
}

518
Chapter 11 Intersection Testing
11.2.2 Line-Point Distance
Library
IvMath
Filename
IvLine3
As before, we’re given a point Q and a line deﬁned by a point P and a vector v.
In this case, we want to ﬁnd the distance between the point and the line.
One way is to compute the closest point on the line and compute the distance
between that and Q. A more direct approach is to use the Pythagorean theorem
(Figure 11.2).
We note that w = Q−P can be represented as the sum of two vectors, one
parallel to v (w∥) and one perpendicular (w⊥). These form a right triangle, so
from Pythagoras, ∥w∥2 = ∥w∥∥2 + ∥w⊥∥2. We want to know the length of w⊥,
so we can rewrite this as
∥w⊥∥2 = ∥w∥2 −∥w∥∥2
= w · w −
(((w · v
v · v v
(((
2
= w · w −
w · v
v · v
2
v · v
= w · w −(w · v)2
v · v
Taking the square root of both sides will give us the distance between the
point and the line.
The equivalent code is
float IvLine3::DistanceSquared(const IvVector3& point)
{
IvVector3 w = point - mOrigin;
Q
Q
P
w
v
w
w⊥
Figure 11.2 Computing distance from point to line, using right triangle.

11.2 Closest Point and Distance Tests 519
float vsq = mDirection.Dot(mDirection);
float wsq = w.Dot(w);
float proj = w.Dot(mDirection);
return wsq - proj*proj/vsq;
}
Note that in this case we’re computing the squared distance. In most cases
we’ll be using this to avoid computing a square root. Another optimization is
possible if we can guarantee that v is normalized; in that case we can avoid
calculating and dividing by v · v, since its value is 1.
11.2.3 Closest Point on Line Segment to Point
Library
IvMath
Filename
IvLineSegment3
Recall that a line segment can be deﬁned as the convex combination of two
points P0 and P1, or
S(t) = (1 −t)P0 + tP1
where 0 ≤t ≤1. We can rewrite this as
S(t) = P0 + t(P1 −P0)
or
S(t) = P + tv
where t is similarly constrained. In this case v should not be normalized, as
its length is the length of our line segment, and the endpoints are P and P +v.
In the problem of ﬁnding the closest point on a line, we computed the
projection of the point onto the line. Doing the same for a line segment gives
us three cases (Figure 11.3). In the ﬁrst case, the result of projecting Q0 lies
outside the segment but closest to P0. In the second case, the result of project-
ing Q1 lies outside the segment but closest to P1. In the third case, the projected
Q2 lies on the segment, and we can use the same projection calculations that
we used with a line.
To determine which case we’re in, we begin by noting that
t = w · v
v · v
is acting as our parameter t for the projected point, where again w = Q −P.
If t < 0, then the projected point lies beyond P0, and the closest point is P0.
Similarly, if t > 1, then the closest point is P1.

520
Chapter 11 Intersection Testing
Q0
Q2
Q1
P0
P1
Figure 11.3 Three cases when projecting point onto line segment.
Testing t directly requires a ﬂoating-point division. By modifying our test
we can defer the division to be performed only when we truly need it, that is,
when the point lies on the segment. Since v · v > 0, then w · v < 0 in order for
t < 0. And in order for t > 1, then w · v > v · v.
The equivalent code is
IvVector3 IvLineSegment3::ClosestPoint(const IvVector3& point)
{
IvVector3 w = point - mOrigin;
float proj = w.Dot(mDirection);
if ( proj <= 0 )
return mOrigin;
else
{
float vsq = mDirection.Dot(mDirection);
if ( proj >= vsq )
return mOrigin + mDirection;
else
return mOrigin + (proj/vsq)*mDirection;
}
}
Library
IvMath
Filename
IvLineSegment3
11.2.4 Line Segment-Point Distance
As with lines, we can compute the distance to the line segment by computing
the distance to the closest point on the line segment. If we recall, there are

11.2 Closest Point and Distance Tests 521
three cases: the closest point is P0, P1, or a point somewhere else on the
segment, which we’ll calculate.
If the closest point is P0, then we can compute the distance as ∥Q −P0∥.
Since w = Q −P0, then the squared distance is equal to w · w.
If the closest point is P1, then the squared distance is (Q −P1) · (Q −P1).
However, we’re representing our endpoint as P1 = P0 + v, so this becomes
(Q −P0 −v) · (Q −P0 −v). We can rewrite this as
distsq(Q, P1) = ((Q −P0) −v) · ((Q −P0) −v)
= (w −v) · (w −v)
= w · w −2w · v + v · v
We’ve already calculated most of these dot products when determining
whether we’re closest to P1, so all we need to compute is w · w and add.
If the closest point lies elsewhere on the segment, then we use the line
distance calculation just given. The ﬁnal code is
float IvLineSegment3::DistanceSquared(const IvVector3& point)
{
IvVector3 w = point - mOrigin;
float proj = w.Dot(mDirection);
if ( proj <= 0 )
{
return w.Dot(w);
}
else
{
float vsq = mDirection.Dot(mDirection);
if ( proj >= vsq )
{
return w.Dot(w) - 2.0f*proj + vsq;
}
else
{
return w.Dot(w) - proj*proj/vsq;
}
}
}

522
Chapter 11 Intersection Testing
11.2.5 Closest Points Between Two Lines
Library
IvMath
Filename
IvLine3
Sunday [105] provides the following construction for ﬁnding the closest
points between two lines. Note that in this case there are two closest points,
one on each line, since there are two degrees of freedom. The situation
is shown in Figure 11.4. Line L1 is described by the point P0 and the
vector u. Correspondingly, line L2 is described by the point Q0 and the
vector v, or
L1(s) = P0 + su
L2(t) = Q0 + tv
Vectors u and v are not necessarily normalized.
We’ll deﬁne the two closest points that we’re looking for as lying at param-
eters sc and tc on the lines, and call them L1(sc) and L2(tc), respectively. We’ll
refer to the vector from L2(tc) to L1(sc) as wc.
Expanding wc, we have
wc = L1(sc) −L2(tc)
= P0 + scu −Q0 −tcv
= (P0 −Q0) + scu −tcv
P0
Q0
w0
wc
P(sc)
Q(tc)
u
v
Figure 11.4 Finding closest points between two lines.

11.2 Closest Point and Distance Tests 523
We’ll use w0 to represent the difference vector P0 −Q0, so
wc = w0 + scu −tcv
(11.1)
In order for wc to represent the vector of closest distance, it needs to be
perpendicular to both L1 and L2. This means that
wc · u = 0
wc · v = 0
Substituting in equation 11.1 and expanding, we get
0 = w0 · u + scu · u −tcu · v
(11.2)
0 = w0 · v + scu · v −tcv · v
(11.3)
We have two equations and two unknowns sc and tc, so we can solve for this
system of equations. Doing so, we get the result that
sc = be −cd
ac −b2
(11.4)
tc = ae −bd
ac −b2
(11.5)
where
a = u · u
b = u · v
c = v · v
d = u · w0
e = v · w0
There is one case where we need to be careful. If the two lines are parallel,
then u and v are parallel, so |u · v| = ∥u∥∥v∥. Then the denominator ac −b2
equals
ac −b2 = (u · u)(v · v) −(u · v)2
= ∥u∥2∥v∥2 −(∥u∥∥v∥)2
= 0
This leads to a division by 0. The problem is that there are an inﬁnite number
of pairs of closest points, spaced along each line. In this case we’ll just ﬁnd
the closest point Q′ on L2 to the origin P0 of line L1, and return P0 and Q′.

524
Chapter 11 Intersection Testing
void ClosestPoints( IvVector3& point1,
IvVector3& point2,
const IvLine3& line1,
const IvLine3& line2 )
{
IvVector3 w0 = line1.mOrigin - line2.mOrigin;
float a = line1.mDirection.Dot( line1.mDirection );
float b = line1.mDirection.Dot( line2.mDirection );
float c = line2.mDirection.Dot( line2.mDirection );
float d = line1.mDirection.Dot( w0 );
float e = line2.mDirection.Dot( w0 );
float denom = a*c - b*b;
if ( ::IsZero(denom) )
{
point1 = mOrigin;
point2 = other.mOrigin + (e/c)*other.mDirection;
}
else
{
point1 = mOrigin + ((b*e - c*d)/denom)*mDirection;
point2 = other.mOrigin + ((a*e - b*d)/denom)*other.mDirection;
}
}
11.2.6 Line-Line Distance
Library
IvMath
Filename
IvLine3
From the calculation of closest points between two lines, we know that wc is
the vector of closest distance. Therefore, its length equals the distance between
the two lines. Rather than compute the closest points directly, we can substi-
tute the values of sc and tc into equation 11.1 and compute the length of wc.
As before, to avoid the square root, we can use ∥wc∥2 = wc · wc instead.
The code is as follows:
float DistanceSquared( const IvLine3& line1, const IvLine3& line2
)
{
// compute parameters
IvVector3 w0 = line1.mOrigin - line2.mOrigin;
float a = line1.mDirection.Dot( line1.mDirection );
float b = line1.mDirection.Dot( line2.mDirection );
float c = line2.mDirection.Dot( line2.mDirection );
float d = line1.mDirection.Dot( w0 );
float e = line2.mDirection.Dot( w0 );

11.2 Closest Point and Distance Tests 525
float denom = a*c - b*b;
// if lines parallel
if ( ::IsZero(denom) )
{
IvVector3 wc = w0 - (e/c)*line2.mDirection;
return wc.Dot(wc);
}
// otherwise
else
{
IvVector3 wc = w0 + ((b*e - c*d)/denom)*line1.mDirection
- ((a*e - b*d)/denom)*line2.mDirection;
return wc.Dot(wc);
}
}
11.2.7 Closest Points Between Two Line
Segments
Library
IvMath
Filename
IvLineSegment3
Finding the closest points between two line segments follows from ﬁnding the
closest points between two lines. We compute sc and tc, as we’ve done, but
then need to clamp the results to the ranges of s and t deﬁned by the endpoints
of the two line segments. As before, we’ll deﬁne our line segments as starting
at the source point of the line, and ending at that source point plus the line
vector. So for line L1, the two points are P0 and P0 + u and for line L2, the two
points are Q0 and Q0 +v. This gives us parameters 0 and 1 for the locations of
the two endpoints. If our results sc and tc lie between the values 0 and 1, then
our closest points lie on the two segments, and we’re done.
Otherwise, we need to clamp our test to each of the endpoints and try
again. To see how to do that, let’s take a look at the s = 0 endpoint. Remember
that what we want to do is ﬁnd the smallest possible distance between the two
points while not sliding off the end of the segment; namely, we want to mini-
mize the length of wc while maintaining s = 0. Since length is always increas-
ing, we’ll use ∥wc∥2, which will be much easier to minimize. Remember that
wc = w0 + scu −tcv
Since we’re clamping sc to 0, this becomes
wc = w0 −tcv
And so for this endpoint we try to ﬁnd the minimum value for
wc · wc = (w0 −tcv) · (w0 −tcv)
(11.6)

526
Chapter 11 Intersection Testing
To do this, we return to calculus. To ﬁnd a minimum value (in this case there
is only one) for a function, we ﬁnd a place where the derivative is 0. Taking
the derivative of equation 11.6 in terms of tc, we get the result
0 = −2v · (w0 −tcv)
Solving for tc:
tc = v · w0
v · v
(11.7)
So for the ﬁxed point on line L1 at s = 0, this gives us the parameter of
the closest point on line L2. As we can see, this is equivalent to computing the
closest point between a line and a point, where the line is L2 and the point
is P0.
For the s = 1 endpoint, we follow a similar process. Our minimization
function is
wc · wc = (w0 + u −tcv) · (w0 −tcv)
(11.8)
The corresponding zero derivative function is
0 = −2v · (w0 + u −tcv)
And solving for tc gives us
tc = v · w0 + u · v
v · v
Again, this is equivalent to computing the closest point between a line and a
point, where the line is L2 and the point is P0 + v. The solutions for sc when
clamping to t = 0 or t = 1 are similar.
One nice thing about these functions is that they use the a through e
values that we’ve already calculated for the basic line-line distance calculation.
So equation 11.7 becomes
tc = e
c
So which endpoints do we check? Well, if the parameter sc is less than 0, then
the closest segment point to line L2 will be the s = 0 endpoint. And if sc is
greater than 1, then the closest segment point will be at s = 1. Choosing one
or the other, we re-solve for tc, and check that it lies between 0 and 1. If not,
we perform the same process to clamp tc to either the t = 0 or t = 1 endpoint,
and recalculate sc accordingly (with some minor adjustments to ensure that
we keep sc within 0 and 1).

11.2 Closest Point and Distance Tests 527
Once again, there is a trick we can do to avoid multiple ﬂoating-point
divisions. Instead of computing, say, sc directly and testing against 0 and 1,
we can compute the numerator sN and denominator sD. The initial sD is always
greater than zero, so we know that if sN is less than zero, sc is less than zero
and we clamp to s = 0 accordingly. Similarly, if sN is greater than sD, we know
that sc > 1, and we clamp to s = 1. The same can be done for the t values. Using
this, we can recalculate the numerator and denominator when necessary, and
do the ﬂoating-point divides only after all the clamping has been done.
For example, the following code snippet calculates the s values:
// clamp s_c to 0
if (sN < 0.0f)
{
sN = 0.0f;
tN = e;
tD = c;
}
// clamp s_c to 1
else if (sN > sD)
{
sN = sD;
tN = e + b;
tD = c;
}
The full code is too long to contain here, but can be found on the demo CD.
11.2.8 Line Segment–Line Segment Distance
Library
IvMath
Filename
IvLineSegment3
Finding the segment to segment squared distance is similar to line to line
distance: we follow the procedure for closest points between line segments,
calculate wc directly from the ﬁnal sc and tc, and then compute its length. The
full code can be found on the CD in the IvLineSegment3 friend function
DistanceSquared().
11.2.9 General Linear Components
Library
IvMath
Filename
IvLine3
IvRay3
IvLineSegment3
Testing ray versus ray or line versus line segment is actually a simpliﬁcation
of the segment-segment closest point and distance determination. Instead
of clamping against both components, we need only clamp against those
endpoints that are necessary. So for example, if we treat P0 +su as the param-
eterization of a line segment, and Q0+tv as a line, then we need only to ensure

528
Chapter 11 Intersection Testing
that sc is between 0 and 1, clamp to the appropriate endpoint, and adjust tc
accordingly. Similarly, if we’re working with rays, we need only to clamp sc
or tc to 0.
Implementations of these algorithms can be found in the appropriate
classes.
11.3 Object Intersection
Now that we’ve covered some methods for measuring distance between prim-
itives, we can talk about object intersection. The most direct, and naive,
approach to determine whether two objects are intersecting is to work directly
from raw object data. We could start with a triangle in object A and a triangle
in object B and see if they are intersecting. Then we move to the next triangle
in object A and test again. While ultimately this may work (the exception is
if one object is inside the other), it will take a while to do and most of the
time performing all those tests isn’t even necessary. Take the two objects in
Figure 11.5. They are clearly not intersecting — we can tell that in an instant.
But our minds are not considering each object as a collection of lines and
doing individual tests. Rather we are comparing them as a whole, as two
rough blobs, and determining that the blobs aren’t intersecting. By using a
similar process in our intersection routines, we can save ourselves a lot of time.
For
instance,
suppose
we
surround
each
object
with
a
sphere
(Figure 11.6). We can begin by testing for intersection between the spheres.
Figure 11.5 Non-intersecting objects.

11.3 Object Intersection 529
Figure 11.6 Non-intersecting objects with bounding sphere.
If the two spheres aren’t intersecting, we know the objects aren’t either. If the
spheres are intersecting, we can try comparing another simpliﬁed version of
our object — say, two boxes. The boxes ﬁt the shape of our objects better but
are still a simpler test than our full triangle-triangle comparison. If the boxes
intersect, only then do we perform our complex collision detection routine.
This technique of using simpliﬁed objects to test intersections before per-
forming more expensive operations is commonly used in game engines, and
is necessary to get collision detection and other intersection-based systems
running in real time. The simpliﬁed objects are known as bounding objects,
and are named speciﬁcally after the basic primitive we used to approximate
the object: bounding spheres and bounding boxes. In games, we can often
get away with ignoring the underlying geometry completely and only using
bounding objects to determine intersections. For example, when handling
collisions in this way, either the action happens so fast that we don’t notice
any overlapping objects or objects reacting to collision when they appear sep-
arated, or the error is so slight that it doesn’t matter. In any case, choosing
the side of making the simulation run faster for a better play experience is
usually a good decision.
To keep things concise, we will be focused primarily on detecting intersec-
tions between a few simple shapes. Other books are more detailed, covering
many different polytopes (the 3D equivalent of polygons) and interactions

530
Chapter 11 Intersection Testing
between all sorts of bounding objects. In our case we’ll focus on a few simple
shapes, beginning with the simplest objects, and moving on to the most
complex, or most expensive, to compute.
Within each section we’ll only consider three cases of intersection. We’ll
ﬁrst look at intersections between objects of the same bounding type, which is
useful in collision detection. Second, we’ll cover intersections between a ray
and the particular bounding object, which we’ll need for picking and visibility
testing for AI. Finally, we’ll discuss how to determine intersection between a
plane and the bounding object, which can be used for both culling against
frustum planes and collisions with essential planar objects like walls. In all
cases we aren’t concerned with the exact point of intersection, just whether
we intersect.
11.3.1 Spheres
Deﬁnition
Library
IvCollision
Filename
IvBounding
Sphere
The simplest possible bounding object is a sphere. It also has the most com-
pact representation: a center point C and a radius r (Figure 11.7). When
bounding a rigid object, a sphere is also independent of the object’s orienta-
tion. This allows us to update a sphere quickly — when an object moves, we
need only to update the sphere’s position. If the object is scaled, we can scale
the radius accordingly. The combination of low memory usage, fast update
C
r
Figure 11.7 Bounding sphere.

11.3 Object Intersection 531
time, and fast intersection tests makes bounding spheres a ﬁrst choice in any
real-time system.
The surface of the sphere is deﬁned as all points P such that the length of
the vector from C to P is equal to the radius

(Px −Cx)2 + (Py −Cy)2 + (Pz −Cz)2 = r
or

(P −C) · (P −C) = r
Ideally, we’ll want to choose the smallest possible sphere that encom-
passes the entire object. Too small a sphere, and we may skip two objects that
are actually intersecting. Too large and we’ll be unnecessarily performing
our more expensive tests for objects that are clearly separate. Unfortunately,
the most obvious methods for choosing a bounding sphere will not always
generate as tight a ﬁt as we might like.
One such method is to take the local origin of the object as our center C,
and compute r by taking the maximum distance from that to all the vertices in
the object. There are many problems with this. The most common is that the
local origin could be considerably offset from the most desirable center point
for the object (Figure 11.8a). This could happen if you have a character whose
origin is at its feet, so it can be placed on the ground properly. An alternate
but equivalent situation is where the origin is at a reasonable center point for
Figure 11.8a Bounding sphere, offset origin.

532
Chapter 11 Intersection Testing
Figure 11.8b Bounding sphere, outlying point.
Figure 11.8c Bounding sphere, using centroid, object vertices.
the majority of the object’s vertices, but there are one or two outlying vertices
that cause problems (Figure 11.8b).
Eberly [27] provides a number of methods for ﬁnding a better ﬁt. One
is to average all the vertex locations to get the centroid and use that as our
center. This works well for the case of the noncentered origin (Figure 11.8c),
but still is a problem for the object with the outlying points. The reason is that
the majority of the points lie within a small area and thus weight the centroid
in that direction, pulling it away from the extrema.
We could also take an axis-aligned bounding box in the object’s local
space, and use its endpoints to compute our sphere position and radius

11.3 Object Intersection 533
Figure 11.8d Bounding sphere, using box center, box vertices.
Figure 11.8e Bounding sphere, using box center, object vertices.
(Figure 11.8d). This tends to center the sphere better but leads to a looser
ﬁt. A compromise method uses the center of the bounding box as our sphere
position, and computes the radius as the maximum distance from the center
to our points. This gives a slightly better result (Figure 11.8e). The code for
this last method is
void
IvBoundingSphere::Set( const IvPoint3* points, unsigned int numPoints )
{
ASSERT( points );
// compute minimal and maximal bounds
IvVector3 min(points[0]), max(points[0]);

534
Chapter 11 Intersection Testing
for ( unsigned int i = 1; i < numPoints; ++i )
{
if (points[i].x < min.x)
min.x = points[i].x;
else if (points[i].x > max.x )
max.x = points[i].x;
if (points[i].y < min.y)
min.y = points[i].y;
else if (points[i].y > max.y )
max.y = points[i].y;
if (points[i].z < min.z)
min.z = points[i].z;
else if (points[i].z > max.z )
max.z = points[i].z;
}
// compute center and radius
mCenter = 0.5f*(min + max);
float maxDistance = ::DistanceSquared( mCenter, points[0] );
for ( unsigned int i = 1; i < numPoints; ++i )
{
float dist = ::DistanceSquared( mCenter, points[i] );
if (dist > maxDistance)
maxDistance = dist;
}
mRadius = ::IvSqrt( maxDistance );
}
It should be noted that none of these methods is guaranteed to ﬁnd
the smallest bounding sphere.
The standard algorithm for this is by
Welzl [117], who showed that linear programming can be used to ﬁnd the
optimally smallest sphere surrounding a set of points. Two implementa-
tions are readily available online: one by Bernd Gaertner is provided under
the GNU General Public License; another by Dave Eberly is at www.magic-
software.com.
While we don’t want to be cavalier about using ridiculously large bounding
spheres, in some cases having the tightest possible ﬁt isn’t that much of an
issue. Our objects will not be generally spherical, and so we’ll be using some-
thing more complex for our ﬁnal intersection test. As long as our spheres are

11.3 Object Intersection 535
reasonably close to a good ﬁt, they will act to cull a great number of obvious
cases, which is all we can ask for.
Sphere-Sphere Intersection
Determining whether two spheres are intersecting is as simple as their
representation. We need only to determine whether the distance between
their centers is less than the sum of their two radii (Figure 11.9), or

(C1 −C2) · (C1 −C2) <= r1 + r2
(11.9)
The square root operation is expensive, and in any case it is unneces-
sary. Since we’re not looking for the absolute difference, just a relation, we
can use
(C1 −C2) · (C1 −C2) <= (r1 + r2)2
(11.10)
As promised, this gives us an extremely cheap test for culling large num-
bers of intersections. This is why bounding spheres are used everywhere
in computer graphics and simulation; we perform an initial fast check
with a bounding sphere ﬁrst before even considering the more complex
cases.
d
r2
C2
C1
r1
Figure 11.9 Sphere-sphere intersection.

536
Chapter 11 Intersection Testing
The code is as follows:
bool
IvBoundingSphere::Intersect( const IvBoundingSphere& other )
{
IvVector3 centerDiff = mCenter - other.mCenter;
float radiusSum = mRadius + other.mRadius;
return ( centerDiff.Dot(centerDiff) <= radiusSum*radiusSum );
}
Sphere-Ray Intersection
Intersection between a sphere and a ray is nearly as simple. Instead of testing
two centers and comparing the distance with the sum of two radii, we test the
distance between a single sphere center and a ray. If the distance is less than or
equal to the sphere’s radius, then the ray intersects the sphere (Figure 11.10).
We can use the line-point distance measurement described as the basis
for this test. The code is as follows (it assumes an initial nonzero, nonnormal-
ized v):
bool
IvBoundingSphere::Intersect( const IvRay3& ray )
{
// compute intermediate values
IvVector3 w = mCenter - ray.mOrigin;
float wsq = w.Dot(w);
float proj = w.Dot(ray.mDirection);
float rsq = mRadius*mRadius;
// if sphere behind ray, no intersection
if ( proj < 0.0f && wsq > mRadius*mRadius )
return false;
float vsq = ray.mDirection.Dot(ray.mDirection);
// test length of difference vs. radius
return ( vsq*wsq - proj*proj <= vsq*mRadius*mRadius );
}
An additional check has been added since we’re using a ray. If the sphere
lies behind the origin of the ray, then there is no intersection. This is true
if the angle between the difference vector w and the line direction is greater
than 90 degrees (proj < 0.0f) and the line origin lies outside of the sphere
(wsq > mRadius*mRadius).

11.3 Object Intersection 537
r
C
d
Figure 11.10 Line-sphere intersection.
We also remove the need for a ﬂoating-point divide by multiplying through
by vsq. This adds 2 multiplications, but this should still be faster on most
ﬂoating-point processors. As before, if we can guarantee that the ray direction
vector is normalized, then we can remove the need for vsq altogether.
Sphere-Plane Intersection
Testing whether a sphere lies entirely on one side of a plane can be done quite
efﬁciently. Recall that we can determine the distance between a point and
such a plane by taking the absolute value of the result of the plane equation.
If the result is positive and the distance is greater than the radius, then the
sphere lies on the inside of the plane. If the result is negative, and the distance
is greater than sphere’s radius, then the sphere lies outside of the plane.
Otherwise, the sphere intersects the plane.
The code for this test is
float
IvBoundingSphere::Classify( const IvPlane& plane )
{
float distance = plane.test(mCenter):
if ( distance > radius)
{

538
Chapter 11 Intersection Testing
return distance-radius;
}
else if ( distance < -radius )
{
return distance+radius;
}
else
{
return 0.0f;
}
}
Here we’re returning a signed distance, like the standard plane test. If the
sphere intersects, we return zero. Otherwise, we return the signed distance
minus the signed distance of the radius.
11.3.2 Axis-Aligned Bounding Boxes
Library
IvCollision
Filename
IvAABB
Deﬁnition
Spheres work well as either cheap culling objects or as bounding objects for
a small class of models (i.e., if you’re tossing grenades or writing a billards
game). For more angular objects, we need a better ﬁtting bounding surface.
One possibility is the bounding box. Just like the bounding sphere, the ideal
bounding box is the smallest possible box that encloses a model.
The ﬁrst type we’ll consider is the AABB, or axis-aligned bounding box,
so-called because the box edges are aligned to the world axes. This makes rep-
resentation of the box simple: we use two points, one each for the minimum
and maximum xyz positions (Figure 11.11). When the object is translated, to
update the box we translate the min and max points. Similarly, if the model
is scaled, we scale the two points relative to the box center. However, because
the box is aligned to the world axes, any rotation of the object means that we
have to recalculate the min-max points from the model vertices’ new positions
in world space.
The other disadvantage AABBs have is that in many cases, like spheres,
they still aren’t a very close ﬁt to the model they are trying to approximate
(Figure 11.12). And for rounded objects like submarines or organic objects like
humans, the fact that they have corners is a disadvantage as well. However,
they are relatively cheap to compute and cheap to test as well, so they continue
to prove useful.
One advantage that world axis-aligned boxes have over a box oriented
to the model’s local space is that we need only recompute them once per

11.3 Object Intersection 539
(xmin, ymin, zmin)
(xmax, ymax, zmax)
Figure 11.11 Axis-aligned bounding box.
Figure 11.12 Fitting-axis-aligned bounding box.
frame, and then we can compare them directly without further transforma-
tion, since they are all in the same coordinate frame. So while AABBs have
a high per-frame overhead (since we have to recalculate them each time an
object reorients), they are extremely cheap to test against one another. As
we’ll see, there is a lot more overhead for determining intersection between
oriented boxes. Oriented boxes are generally cheap per-frame (they move
with the transforms of the object) but are more expensive to test against one
another.
To compute an AABB, we ﬁrst transform the model into world space. Then
we set the minimum and maximum points to be equal to the ﬁrst point (in
world space, remember) in the model. Starting with the second point, we com-
pare the xyz values of each point with those in the minimum and maximum.

540
Chapter 11 Intersection Testing
If any coordinate is less than that in the minimum, set the minimum coordi-
nate to that value. And the same for the maximum, except use greater than.
When done, this will give you the axis-aligned extrema for your box.
void
IvAABB::Set( const IvPoint3* points, unsigned int numPoints )
{
ASSERT( points );
// compute minimal and maximal bounds
mMinima.Set(points[0]);
mMaxima.Set(points[0]);
for ( unsigned int i = 1; i < numPoints; ++i )
{
if (points[i].x < mMinima.x)
mMinima.x = points[i].x;
else if (points[i].x > mMaxima.x )
mMaxima.x = points[i].x;
if (points[i].y < mMinima.y)
mMinima.y = points[i].y;
else if (points[i].y > mMaxima.y )
mMaxima.y = points[i].y;
if (points[i].z < mMinima.z)
mMinima.z = points[i].z;
else if (points[i].z > mMaxima.z )
mMaxima.z = points[i].z;
}
}
AABB-AABB Intersection
In order to understand how we ﬁnd intersections between two axis-aligned
boxes, we introduce the notion of a separating plane. The general idea is this:
we check the boxes in each of the coordinate directions in world space. If we
can ﬁnd a plane that separates the two boxes in any of the coordinate direc-
tions, then the two boxes are not intersecting. If we fail all three separating
plane tests, then they are intersecting and we handle it appropriately.
Let’s look at the process of ﬁnding a separating plane between two boxes
in the x-direction. Since the boxes are axis-aligned, this becomes a one-
dimensional problem on a number line. The min and max values of the two
boxes become the extrema of two intervals on the line. If the two intervals are
separate, then there is a separating plane and the two boxes are separate along
the x-direction. This is the case only if the maximum value of one interval is

11.3 Object Intersection 541
less than the minimum value of the other interval (Figure 11.13). Expressing
this for all three axes:
bool
IvAABB::Intersect( const IvAABB& other )
{
// if separated in x direction
if (mMinima.x > other.mMaxima.x || other.mMinima.x > mMaxima.x )
return false;
// if separated in y direction
if (mMinima.y > other.mMaxima.y || other.mMinima.y > mMaxima.y )
return false;
// if separated in z direction
if (mMinima.z > other.mMaxima.z || other.mMinima.z > mMaxima.z )
return false;
// no separation, must be intersecting
return true;
}
Examining this code makes another advantage of AABBs clear. If we’re using
3D objects in an essentially 2D game, we can ignore the z-axis and so save a
step in our computations. This is not always possible with boxes aligned to
the local axes of an object.
min1
max1
min2
max2
Figure 11.13 Axis-aligned box-box separation test.

542
Chapter 11 Intersection Testing
AABB-Ray Intersection
Determining intersection between a ray and an axis-aligned box is similar
to determining intersection between two boxes. We check one axis direction
at a time as before, except that in this case there is a little more interaction
between steps.
Figure 11.14 shows a 2D cross section of the situation. The ray R shown
intersects the minimum and maximum x planes of the box at R(sx) and R(tx),
respectively, and the minimum and maximum y planes at R(sy) and R(ty).
Instead of testing for extrema overlaps in the box axes directions, we’ll test
whether there is overlap between the line segment from R(sx) to R(tx), and
the line segment from R(sy) to R(ty). This is the same as testing whether the
intervals of the line parameters [sx, tx] and [sy, ty] overlap.
If the ray misses the box, as in the ﬁgure, then the [sx, tx] interval doesn’t
overlap the [sy, ty] interval, just like the preceding box-box intersection. So if
there’s no overlap (if tx < sy, or vice versa), then there’s no intersection, and
we stop. If they do overlap, then we test that overlap interval against the
z intersections. If there’s overlap there as well, then we know that the ray
intersects the box.
For each axis, we begin by computing the parameters where the ray
(represented by the point P and vector v) crosses the min and max planes.
So for example, in the x direction we’ll calculate intersections with the
xmin
xmax
ymin
ymax
R(sy)
R(ty)
R(sx)
R(tx)
Figure 11.14 Axis-aligned box-ray separation test.

11.3 Object Intersection 543
x = xmin and x = xmax planes. To do this, we need to solve the following
equations:
Px + sxvx = xmin
Px + txvx = xmax
Solving for sx and tx, we get
sx = xmin −Px
vx
tx = xmax −Px
vx
There’s one special case we need to handle: clearly if vx is zero, then there are
no solutions for sx and tx; the ray is parallel to the minimum and maximum
planes. In this case we need to test whether Px lies between xmin and xmax.
If not, the ray misses the box and there is no intersection.
We’ll track our parameter overlap interval by using two values smax and
tmin, initialized to the maximum interval [−∞, ∞]. These represent the max-
imum s and minimum t values seen so far. After we calculate intersection
parameters for each axis, we’ll sort them so that s < t, and then update smax
and tmin if s > smax or t < tmin. We know that the ray misses the box if we ever
ﬁnd that smax > tmin. For example, looking at Figure 11.14, after doing the
x-axis calculations we see that smax = sx and tmin = tx. After the y-axis param-
eters are computed, tmin is updated to ty, and smax remains sx. But sx > ty, so
there is no intersection.
Since we’re using a ray, there is one further check: if any t value is ever
less than zero, we know that both parameters are less than zero, and that the
box is behind the ray and there is no intersection. The code, abbreviated for
space, is as follows:
bool
IvAABB::Intersect( const IvRay3& ray )
{
float maxS = -FLT_MAX;
float minT = FLT_MAX;
// do x coordinate test (yz planes)
// ray is parallel to plane
if ( ::IsZero( ray.mDirection.x ) )
{
// ray passes by box

544
Chapter 11 Intersection Testing
if ( ray.mOrigin.x < mMin.x
|| ray.mOrigin.x > mMax.x )
return false;
}
else
{
// compute intersection parameters and sort
float s = (mMin.x - ray.mOrigin.x)/ray.mDirection.x;
float t = (mMax.x - ray.mOrigin.x)/ray.mDirection.x;
if ( s > t )
{
float temp = s;
s = t;
t = temp;
}
// adjust min and max values
if ( s > maxS )
maxS = s;
if ( t < minT )
minT = t;
// check for intersection failure
if ( minT < 0.0f || maxS > minT )
return false;
}
// do y and z coordinate tests (xz & xy planes)
...
// done, have intersection
return true;
}
AABB-Plane Intersection
The most naive test to determine whether a box intersects a plane is to see
whether a single box edge crosses the plane. That is, if two neighboring ver-
tices lie on either side of the plane, there is an intersection. There are 12 edges,
so this requires 24 plane tests. There are two improvements we can make to
this. The ﬁrst is to note that we need test only opposing corners of the box,
that is, two vertices that lie at either end of a diagonal that passes through
the box center. This cuts the number of “edges” to be checked down to 4. The
second improvement is provided by Möller and Haines [79], who note that we
really need to test only one: the diagonal most closely aligned with the plane
normal. Figure 11.15 shows a cross section of the situation.

11.3 Object Intersection 545
n^
Figure 11.15 Axis-aligned box-plane separation test.
Code to manage this is as follows. As before, we return zero if there is an
intersection, the signed distance otherwise.
float
IvAABB::Classify( const IvPlane& plane )
{
IvVector3 diagMin, diagMax;
// set min/max values for x direction
if ( plane.mNormal.x >= 0)
{
diagMin.x = mMin.x;
diagMax.x = mMax.x;
}
else
{
diagMin.x = mMin.x;
diagMax.x = mMax.x;
}
// ditto for y and z directions
...
// minimum on positive side of plane, box on positive side
float test = plane.mNormal.Dot( diagMin ) + plane.mD;
if ( test > 0.0f )
return test;
test = plane.mNormal.Dot ( diagMax ) + plane.mD;
// min on non-positive side, max on non-negative side, intersection
if ( test >= 0.0f )
return 0.0f;
// max on negative side, box on negative side

546
Chapter 11 Intersection Testing
else
return test;
}
11.3.3 Swept Spheres
Deﬁnition
Library
IvCollision
Filename
IvCapsule
The bounding sphere and the axis-aligned bounding box have one problem:
there is no real sense of orientation. The sphere is symmetric across all axes
and the AABB is always aligned to the world axes. For objects that have deﬁnite
long and short axes (a human, for example), this doesn’t provide for an ideal
approximation. The next two bounding objects we’ll consider are not tied
to the world axes at all, which makes them much more suitable for general
models.
The simplest of such bounding regions are the swept spheres. If we
consider the sphere as a region enclosed by a radius around a point, or a
zero-dimensional center, the swept spheres use higher dimensional centers.
One example is the capsule, which is a line segment surrounded by a radius
(Figure 11.16a). Another possibility is the lozenge, which has a quadrilat-
eral center (Figure 11.16b). For our purposes, we’ll concentrate on capsules
(Eberly [27] provides more information on lozenges and other swept spheres).
Computing the capsule in local space for a set of points is fairly straight-
forward, but not as simple as spheres or bounding boxes. We are ﬁrst going
to assume that our model is generally axis-aligned in local space. This is not
unreasonable considering that the artists usually build models in this way.
For models that are not axis-aligned, see Eberly [27] or Van Verth [110].
Our ﬁrst step is to ﬁnd the long axis for the model. We do this by computing
the bounding box, and ﬁnding the longest side. The line that we will use for
our base line segment runs through the middle of the box. We’ll use the center
of one end of the box as our line point A, and the box axis w as our line vector.
We could use the local origin and a coordinate axis for our line, but while
Figure 11.16a Capsule.

11.3 Object Intersection 547
Figure 11.16b Lozenge.
we’re willing to assume axis alignment, we’re not so optimistic as to assume
that the model is centered on a coordinate axis.
Now we need to compute the radius r of the capsule. For each point in
the model, we compute the distance from the point to the line. The maximum
distance becomes our radius. The line combined with the radius gives us a
tube with radius r and ends extending to inﬁnity. All the points in the model
just ﬁt inside the tube.
The ﬁnal part to building the capsule is capping the tube with two hemi-
spheres that just contain any points near the end of the model. Eberly [27]
describes a method for doing this. The center of each hemisphere is one of
the two endpoints of the line segment, so ﬁnding the hemisphere allows us to
deﬁne the line segment. Let’s consider the endpoint with the smaller t value —
call it L(ξ0) — shown in Figure 11.17. We want to slide the endcap in from the
right until we ﬁnd the smallest ξ0 such that all points in the model either lie on
the hemisphere (such as point P0) or to the right of it (point P1). Another way
to think of this is that for each point we’ll compute the hemisphere centered
P0
P1
t = –
L(n0)
Figure 11.17 Capsule endcap ﬁtting.

548
Chapter 11 Intersection Testing
P
u
X0
^
v^
w^
A
d
Figure 11.18 Determining hemisphere center X0 for given point P ′.
on the line that exactly contains it, and choose the one with the smallest ξ0
value. If we do the same at the other end, with hemispheres oriented the other
way and choosing the one with largest parameter value ξ1, then all points will
be tightly enclosed by the capsule.
To set this up, we ﬁrst need to transform our points from the local space
of the model to the local space of the line. We’ll build a coordinate frame
consisting of the line point A, normalized line vector ˆw, and two vectors per-
pendicular to ˆw: ˆu and ˆv. Subtracting the line point from the model point, and
multiplying by a 3 × 3 matrix formed from ˆu, ˆv, and ˆw, transforms the model
space point P to a local line space P ′ with line space coordinates (u, v, w).
Since ˆw is normalized, a point L(ξ0) on the line equals (0, 0, ξ0) in line space.
If P ′ lies on a hemisphere with radius r and center X0 on the line, the length
of a vector d from X0 to P ′
i should be equal to the radius r (Figure 11.18). Given
this and the other parameters, we should be able to solve for X0, and hence ξ0.
The vector d = P ′ −X0. In line space d = (u, v, w) −(0, 0, ξ) = (u, v, w −ξ).
Ensuring that ∥d∥= r means that
u2 + v2 + (w −ξ0)2 = r2
Solving for ξ0, we get
ξ0 = w −

±

r2 −(u2 + v2)

Since this is a hemisphere, we want X0 to be to the right of P, so w ≥ξ0 and
this becomes
ξ0 = w +

r2 −(u2 + v2)
Computing this for every point P in our model and ﬁnding the minimum ξ0
gives us our ﬁrst endpoint. Similarly, the second endpoint is found by ﬁnding
the maximum value of
ξ1 = w −

r2 −(u2 + v2)

11.3 Object Intersection 549
Capsule-Capsule Intersection
Handling capsule-capsule intersection is very similar to sphere-sphere inter-
section. Instead of calculating the distance between two points, and deter-
mining whether that is less than the sum of the two radii, we calculate the
distance between two line segments and check against the radii. As before,
if the distance is less than the sum of the two radii, we have intersecting
capsules.
bool
IvCapsule::Intersect( const IvCapsule& other )
{
float radiusSum = mRadius + other.mRadius;
return ( mSegment.DistanceSquared( other.mSegment )
<= radiusSum*radiusSum );
}
Capsule-Ray Intersection
Capsule-ray intersection follows from capsule-capsule collision. Instead of
ﬁnding the distance between two line segments, we need to ﬁnd the distance
between a ray and a line segment, and compare to the radius of the capsule:
bool
IvCapsule::Intersect( const IvRay3& ray )
{
// test distance between line and segment vs. radius
return ( ray.DistanceSquared( mSegment ) <= mRadius*mRadius );
}
Capsule-Plane Intersection
There are two tests necessary to determine whether a capsule intersects a
plane. First of all, if the two endpoints of the line segment deﬁning the capsule
lie on either side of the plane, then clearly the capsule intersects the plane.
However, even if the line segment lies on one side of the plane, the distance
between one of the endpoints and the plane may be less than the radius. In this
case the capsule and plane would also intersect. Both cases are easy to test;
we already have the pieces in place.
The code is
float
IvCapsule::Classify( const IvPlane& plane )

550
Chapter 11 Intersection Testing
{
float s0 = plane.Test( mSegment.GetEndpoint0() );
float s1 = plane.Test( mSegment.GetEndpoint1() );
// points on opposite sides or intersecting plane
if (s0*s1 <= 0.0f)
return 0.0f;
// intersect if either endpoint is within radius distance of plane
if( ::IvAbs(s0) <= mRadius || ::IvAbs(s1) <= mRadius )
return 0.0f;
// return signed distance
return ( ::IvAbs(s0) < ::IvAbs(s1) ? s0 : s1 );
}
11.3.4 Object-Oriented Boxes
Library
IvSimulation
Filename
Iv0BB
World axis-aligned boxes are easy to create and fast to use for detecting inter-
sections, but are not a very tight ﬁt around models that are not themselves
generally aligned to the world axes (Figure 11.19). A more accurate approach
is to create an initial bounding box that is a tight ﬁt around the model in local
space, and then rotate and translate the box as well as the model. These are
known as object-oriented bounding boxes, or OBBs. This has another advan-
tage in that we don’t have to recalculate the box every time the model moves,
just transform the initial one. Also, for rigid models with a large number of
vertices, recomputing the AABB every frame may be too expensive. The dis-
advantage is that testing intersections between two object-oriented boxes is
more complicated. In the axis-aligned case, we could simplify our cases down
Figure 11.19 Oriented bounding boxes.

11.3 Object Intersection 551
r2
r1
a
C
r3
Figure 11.20 Properties of OBBs.
to three tests because of the alignment. In the OBB case, the two can be at any
relative orientation to each other, which complicates the issue considerably.
The representation for an OBB A consists of the center point Ca, an ori-
entation matrix Ra, and an extent vector a (Figure 11.20). The extent vector
represents the difference from the center point to the point of maximum x, y,
and z on the box. Note that the center of the box is not necessarily the same
as the local origin of the model, nor does the orientation of the box have to
match the orientation of the model. If either is the case, some adjusting of the
model’s local-to-world transformation will have to be done to generate the box
axes and center location in world space. If the box to model space orientation
transformation is Rbox→model and the model’s orientation is Rmodel→world, then
the box’s local to world rotation is
Rbox→world = Rmodel→world · Rbox→model
To simplify our life, however, we can use boxes aligned to the model’s local
coordinates, with a vector d in model space indicating the box center relative
to the model center (as mentioned in Chapter 3, it’s not usually practical to
build models with their bounding box center as their local origin). In either
case, any time we need the box center c in world space we can use
c = Rmodel→worldd + t
OBB-OBB Intersection
There have been many methods for testing intersections between two
arbitrarily oriented boxes, including linear programming techniques and

552
Chapter 11 Intersection Testing
closest-feature tracking. The most efﬁcient technique known to date, how-
ever, uses the concept of separating axes and is due to Gottschalk, Lin,
and Manocha [48]. The following discussion is heavily drawn from this paper,
with some additional concepts due to Eberly [27] and van den Bergen [109].
Recall that to test whether two axis-aligned boxes were intersecting, we
did three tests, one for each axis x, y, and z. For each test, we checked the
extents of each box along each of the axis directions. This is equivalent to
projecting the box along the basis vectors i, j, and k. If the intervals of a given
projection don’t overlap, then there is a separating plane normal to the test
vector and therefore no intersection. The corresponding axis is known as a
separating axis.
This works well for axis-aligned boxes, but we need a slightly different test
for oriented boxes. To simplify our equations and improve performance, we’ll
use transformations relative to box A. We end up with a single translation
vector c from A to B, where c = RT
a · (Cb −Ca), and a relative rotation matrix
R = RT
b Ra. A’s extent vector remains the same, since it’s relative to its local
space. B’s extent vector becomes RT b.
Now suppose we have a potential separating axis direction v. We want to
perform the same test we did with the AABBs: project each box onto the vector
and check to see whether the projections are separate or not. Another way of
representing this is to project the box centers onto the vector as endpoints,
and then project the extent vectors closest to the center onto the vector as well
(Figure 11.21). If the distance between the projected box centers is less than
the sum of the lengths of the projected extents, then there is no intersection.
Expressed mathematically, there is no intersection if
|c · v| > |a · v + (RT b) · v|
This works if the extent vectors are aligned appropriately to give us the max-
imum projected length, but we can’t make that assumption. Instead we’ll use
a pseudo-dot product that forces maximum length, so the equivalent to a · v is
|axvx| + |ayvy| + |azvz|
This is legal because the extents can be taken from any of the 8 octants, so we
can get any sign we want for any term.
An equivalent equation can be found for (RT b) · v. The ﬁnal separating
axis equation is
|c · v| >

i
|aivi| +

i
|(RT b)ivi|
(11.11)
While this gives us our test, there is an inﬁnite number of choices for
v, which is not practical. Gottschalk [48] demonstrates that any separating

11.3 Object Intersection 553
Ca
Cb
a
c
RTb
v^
^
a•v
^
(RTb)•v
^
c•v
Figure 11.21 Example of OBB separation test.
plane will either be parallel to one of the box faces or parallel to an
edge from each box. This means that a maximum of 15 separating axis
tests are necessary: 3 against the axes of box A, 3 against the axes of box B,
and 9 cross products using one axis from A and one from B.
The nice thing about this result is that it allows us to simplify our equations
considerably. For example, let’s use the cross product of the local x axis from
A and local y axis from B. In A’s local space, the x-axis of A is i = (1, 0, 0). If we
represent the matrix R as the three column vectors (r0, r1, r2), then the y-axis
of B in A’s space is (r01, r11, r21). Performing the cross product i × r1, we get
v = (0, −r21, r11)
(11.12)
Converting this to terms relative to B’s basis via the transpose of R:
RT (i × r1) =


r0 · (i × r1)
r1 · (i × r1)
r2 · (i × r1)

=


i · (r1 × r0)
i · (r1 × r1)
i · (r1 × r2)

=


i · (−r2)
i · 0
i · r0


So v in B space is
RT v = (−r01, 0, r00)
(11.13)
Substituting equations 11.12 and 11.13 into equation 11.11 and multiplying
out the terms, the ﬁnal axis test is
|c2r11 −c1r21| > a1|r32| + a2|r11| + b0|r02| + b2|r00|

554
Chapter 11 Intersection Testing
The test for other axes can be derived similarly. All use the absolute value
of elements from the matrix R so it is far more efﬁcient to precompute them
and then perform the axis tests. If this is done, the algorithm takes about
200 operations. It can be found in IvOBB::Intersect().
One caveat: any implementation of this algorithm needs to take steps to
avoid numerical problems with ﬂoating-point precision. In particular, if two
edges, one from each box, are nearly parallel, the resulting cross product
will be near-zero. This will lead to invalid results for the separation test. The
solution is to detect the condition, and only test against the six main axes of
the boxes.
OBB-Ray Intersection
Detecting intersection between a linear component and an oriented box is
much simpler than detecting intersection between two boxes. One method is
to transform the ray into the box’s local space and perform a standard AABB
intersection test. To transform the linear component, the origin point is trans-
formed by the inverse of the box’s world transform matrix, and the direction
vector by the inverse rotation of the box’s transformation matrix. The newly
transformed line, ray, or line segment can be passed into the appropriate
AABB routine.
An alternative is to use a modiﬁed version of the AABB algorithm, as
described by Möller and Haines [79]. In this case, instead of using planes
normal to the three world axes, we’ll use planes normal to the three box axes.
Recall that these axes are speciﬁed as the three column vectors in our rotation
matrix.
Each axis has two parallel planes associated with it. If we treat the box’s
center as the origin of our frame, the extent vector a contains the magnitude
of our d values for these planes. For example, two of the parallel box planes
are r00x + r10y + r20z + ax = 0 and r00x + r10y + r20z −ax = 0.
If we translate our ray so that its origin is relative to the box origin, we
can determine s and t parameters for the intersections with these planes, just
as we did with the axis-aligned box. In this case, the formulas for s and t for
each axis (including the translation) are
s = ri · (C −P) −ai
Ri · v
t = ri · (C −P) + ai
Ri · v
We also need to modify our test to determine whether the ray is parallel
to the current pair of planes we’re testing — this is easily done by taking the
dot product of the direction vector v and the plane normal, and seeing if it
is close to zero. If so, the ray is parallel to the plane, and we need to project
the vector C −P onto the current axis, and see if the result lies outside the
extents.

11.3 Object Intersection 555
The modiﬁed code is
bool
IvOBB::Intersect( const IvRay3& ray )
{
float maxS = -FLT_MAX;
float minT = FLT_MAX;
// compute difference vector
IvVector3 diff = mCenter - ray.mOrigin;
// for each axis do
for (int i = 0; i < 3; ++i)
{
// get axis i
IvVector3 axis = mRotation.GetColumn( i );
// project relative vector onto axis
float e = axis.Dot( diff );
float f = ray.mDirection.Dot( axis );
// ray is parallel to plane
if ( ::IsZero( f ) )
{
// ray passes by box
if ( -e - mA[i] > 0 || -e + mA[i] > 0 )
return false;
continue;
}
float s = (e - mA[i])/f;
float t = (e + mA[i])/f;
// fix order
...
// adjust min and max values
...
// check for intersection failure
...
}
// done, have intersection
return true;
}
Performance can be improved here by storing the rotation matrix as an array
of three vectors instead of an IvMatrix33.

556
Chapter 11 Intersection Testing
OBB-Plane Intersection
As we did with with OBB-ray intersection, we can classify the intersection
between an OBB and a plane by transforming the plane to the OBB’s frame
and using the AABB-plane classiﬁcation algorithm. Since the transformation
is just a pure rotation and a translation, we can ﬁnd the transformed normal by
ˆn′ = RT ˆn
We apply the transpose since we’re going from world space into box space.
The minimal and maximal points for the AABB in this case are the extent
vector and its negative, a and −a, respectively.
An alternative, presented by Möller and Haines [79], is to use the principle
of separating planes again. This time, our test vector will be the plane normal,
and we’ll project the box diagonal on to it. To ensure we get maximum extent,
we’ll add the absolute values of the elements together, similar to what we did
before:
r = |(a0r0) · n| + |(a1r1) · n| + |(a2r2) · n|
Here each ri represents a column of the rotation matrix. The box intersects
the plane if the distance between the box center and the plane is less than r.
The resulting code is
float IvOBB::Classify( const IvPlane& plane )
{
IvVector3 xNormal = ::Transpose(mRotation)*plane.mNormal;
float r = mExtents.x*::IvAbs(xNormal.x) + mExtents.y*::IvAbs(xNormal.y)
+ mEextents.z*::IvAbs(xNormal.z);
float d = plane. Test(mCenter);
if (::IvAbs(d) < r)
return 0.0f;
else if (d < 0.0f)
return d + r;
else
return d - r;
}
11.3.5 Triangles
Library
IvMath
Filename
IvTriangle
All of the bounding objects we’ve discussed up until now have been approxi-
mations to our model (assuming our model is more complex than, say, a box
or a sphere). To test actual intersections between models, we need to get right

11.3 Object Intersection 557
down to the basic building block of our geometry: the triangle. As before, we
will be representing our triangle as the convex combination of three points.
Triangle-Triangle Intersection
A naive approach to determining triangle-triangle intersection uses the
triangle-ray intersection test that follows. If one of the line segments compos-
ing an edge of one triangle intersects the other triangle, then the two triangles
are intersecting. While this works, there are faster methods. One such is pre-
sented by Martin Held in his ERIT system [62]. The general algorithm has
four major steps.
Figure 11.22 shows the situation. Taking the ﬁrst triangle T , composed
of points P0, P1, and P2, we compute its plane equation. Recall that the plane
equation for a normal n = (a, b, c) and a point on the plane P0 = (x0, y0, z0) is
0 = ax + by + cz −(ax0 + by0 + cz0)
or
0 = ax + by + cz + d
In this case the plane normal is computed from (P1 −P0) × (P2 −P0) and
normalized, and the plane point is P0.
Now we take our second triangle, composed of points Q0, Q1, and Q2.
We plug each point into T ’s plane equation and test whether all three lie on
the same side of the plane. This is true if all three results have the same sign.
If they do, there is no intersection and we quit. Otherwise we store the results
d0, d1, and d2 generated from the plane equation for each point and continue.
P1
P2
P0
R0
Q2
Q1
Q0
R1
Figure 11.22 Triangle intersection.

558
Chapter 11 Intersection Testing
Using the dis, we determine which edges of the second triangle cross the
plane. If a given di,dj pair have opposite signs, then the corresponding points
are on opposite sides of the plane. In Figure 11.22, those pairs are Q0, Q2 and
Q1, Q2. We can compute the intersections of the corresponding triangle edge
with the plane, using the formula
R = Qi +
di
di −dj
(Qj −Qi)
Doing this for each pair will produce two endpoints (R0, R1) of a line segment
L lying on T ’s plane.
The ﬁnal step is determining whether the line segment is outside the
boundary of T . We’ll simplify our 3D problem to a 2D one by projecting the
triangle T and line segment L to one of the xy, xz, or yz planes to create T ′
and L′. To improve our accuracy, we’ll choose the one which provides the
maximum area for the projection of T . If we look at the normal n for T , one
of the coordinate values (x, y, z) will have the maximum absolute value, that
is, the normal is pointing generally along that axis. If we drop that coordinate
and keep the other two, this will give us the maximum projected area.
To test whether the projected line segment is inside T ′, we compute the
line equation Ax + By + C for each pair of projected edge points. We can use
this like the plane equation to test to which side of a line a point lies. If both
endpoints R′
0 and R′
1 lie on the inner side of each line, then the line segment
lies inside the triangle, and we have an intersection.
There is one other case: the line segment may be crossing an edge of the
triangle. We can test for this by computing the intersection of the line segment
with the line formed from each edge. If the intersection lies on the edge, then
the line segment crosses the triangle, and we have an intersection.
Triangle-Ray Intersection
There are two possible approaches to determining triangle-ray intersection.
The ﬁrst is to use the plane equation for the triangle (computed from the
three vertices) and determine the intersection point of the ray with the plane
(if any). We can then use a point-in-triangle test to determine whether the
intersection lies within the triangle.
While a relatively simple approach, it has some disadvantages. First of
all, we need to either store the plane equation or, if we’re short on space,
compute it every time we wish to do the intersection test. Second, it’s a two-
pass algorithm: compute the plane intersection, and then test whether it’s
in the triangle. Fortunately, we have an alternative. The following approach,
presented by Möller and Haines [79], uses afﬁne combinations to compute
the ray-triangle intersection.

11.3 Object Intersection 559
We deﬁne our triangle as having vertices V0, V1, and V2. We can deﬁne two
edge vectors u and v (Figure 11.23), where
u = V1 −V0
v = V2 −V0
Recall that the point V0 with the vectors u and v can be used to create an afﬁne
combination that spans the plane of the triangle, with barycentric coordinates
(u, v). So the formula for a point T (u, v) on the plane is
T (u, v) = V0 + uu + vv
= V0 + u(V1 −V0) + v(V2 −V0)
Rearranging terms, we get
T (u, v) = (1 −u −v)V0 + uV1 + vV2
We want the contribution of each point to be nonnegative, so for a point inside
the triangle
u ≥0
v ≥0
u + v ≤1
V1
u
V2
V0
v
Figure 11.23 Afﬁne space of triangle.

560
Chapter 11 Intersection Testing
If u or v < 0, then the point is on the outside of one of the two axis edges. If
u+v > 1, the point is outside the third edge. So if we can compute the barycen-
tric coordinates for the intersection point T (u, v), we can easily determine
whether the point is outside the triangle.
To compute the u, v coordinates of the intersection point, the result of the
line equation L = P +td will equal a solution to the afﬁne combination T (u, v)
(Figure 11.24). So
P + td = (1 −u −v)V0 + uV1 + vV2
We can express this as a matrix product
 −d V1 −V0 V2 −V0



t
u
v

= P −V0
Using Cramer’s rule, or row-reduction, we can solve this matrix equation
for (t, u, v). The ﬁnal result is
t = q · e2
p · e1
u = p · s
p · e1
v = q · d
p · e1
V1
uu
V2
V0
vv
T(u,v)
Figure 11.24 Barycentric coordinates of line intersection.

11.3 Object Intersection 561
where
e1 = V1 −V0
e2 = V2 −V0
s = P −V0
p = d × e2
q = s × e1
The ﬁnal algorithm includes checks for division by zero and intersections
that lie outside the triangle:
bool
TriangleIntersect( const IvVector3& v0, const IvVector3& v1,
const IvVector3& v2, const IvRay& ray )
{
// test ray direction against triangle
IvVector3 e1 = v1 - v0;
IvVector3 e2 = v2 - v0;
IvVector3 p =
ray.mDirection.Cross(e2);
float a = e1.Dot(p)
// if result zero, no intersection or infinite intersections
// (ray parallel to triangle plane)
if ( ::IsZero(a) )
return false;
// compute denominator
float f = 1.0f/a;
// compute barycentric coordinates
IvVector3 s = ray.mOrigin - v0;
u = f*s.Dot(p)
if (u < 0.0f || u > 1.0f) return false;
IvVector3 q = s.Cross(e1);
v = f*ray.mDirection.Dot(q);
if (v < 0.0f || u+v > 1.0f) return false;
// compute line parameter
t = f*e2.Dot(q);

562
Chapter 11 Intersection Testing
return (t >= 0);
}
Parameters u, v, and t can be returned if the barycentric coordinates on the
triangle or the parameter for the exact point of intersection are needed.
Triangle-Plane Intersection
We covered triangle-plane intersection when we discussed triangle-triangle
intersection. We take our triangle, composed of points P0, P1, and P2, and
plug each point into the plane equation. If all three lie on the same side of
the plane, then there is no intersection. Otherwise, there is, and if we desire
we can ﬁnd the particular line segment of intersection, as described earlier.
If there is no intersection, the signed distance is the plane equation result of
minimum magnitude.
11.4 A Simple Collision System
Now that we have some methods for testing intersection between various
primitive types, we can make use of them in a practical system. The example
we’ll consider is collision detection. Rather than building a fully general col-
lision system, we’ll do only as much as we need to for a basic game — in our
case, we’ll use a submarine game as our example. This is to keep things as
simple as possible and to illustrate various points to consider when building
your own system. It’s also good to keep in mind that a particular subsystem
of a game, whether it is collision or rendering, needs only to be as accurate as
the game calls for. Building a truly ﬂexible collision system that handles all
possible situations may be overkill and eat up processing time that could be
used to do work elsewhere.
11.4.1 Choosing a Base Primitive
The ﬁrst step in building the system is to choose the base bounding shape for
our models. We’ll see in the following sections how we can use a hierarchy
of bounding primitives to get a better ﬁt to the model’s surface, but for now
we’ll consider only one per model. Which primitive we choose depends highly
on the expected topology we’re trying to approximate with it. For example,
if we’re writing a pool game, using bounding spheres for our balls makes
perfect sense. However, for a human character bounding spheres are not a

11.4 A Simple Collision System 563
good choice because one axis of the model is far longer than the other two —
not a good ﬁt. In particular, getting characters through an interior space
might be a tricky proposition unless all your doorways and hallways are at
least six feet wide.
Considering that our model is made of triangles, using them should give us
the most accurate results. However, while they are cheap as a one-on-one test,
it would be costly to test every possible triangle-triangle combination between
two objects. This becomes more feasible when we have some sort of culling
hierarchy to whittle down the possible triangle pairs to a few contenders —
we’ll discuss that in more detail shortly. However, if we can get a good ﬁt with
a simpler bounding volume, we can get a reasonably accurate measure of
collision by doing a volume-volume test without having to do the full triangle-
triangle test.
Since AABBs change size depending on the model’s orientation, they are
not usually a good choice for a base bounding primitive. They are more often
used as a culling test, such as in the sweep-and-prune system described in
Section 11.4.4.
Among the primitives we’ve discussed, this leaves us with capsules and
OBBs. Which we choose depends on our performance requirements and how
angular our models are. If we have mostly boxy models — like tanks —
capsules or even lozenges won’t provide very compelling collisions. An OBB is
a better shape to choose for this situation. For our case, however, submarines
and torpedoes are both generally sausage-shaped. If we had to go with a single
bounding object which approximates a submarine, capsules are an excellent
choice.
11.4.2 Bounding Hierarchies
Demo
Hierarchy
Unless our objects are almost exactly the shape of the bounding primitive
(such as our pool ball example), then there are still going to be places where
our test indicates intersection where there is visibly no collision. For example,
the conning tower of our submarine makes the bounding capsule encompass
a large area of empty space at the top of the hull. Suppose a torpedo is heading
towards our submarine and through that area. Instead of harmlessly passing
over the hull as we would expect from the visual evidence, it will explode
because we have detected a collision with the inaccurately large bounding
region.
The solution is to use a set of bounding primitives to get a better approx-
imation to the surface of the model. In our submarine example, we could use
one capsule for the main hull and one for the conning tower. If we are willing
to allow a slightly forgiving system, we could ignore the conning tower for
the purposes of collision and get a very nice ﬁt with the hull capsule. Or we
could go the more detailed route and add one for the conning tower, as well

564
Chapter 11 Intersection Testing
Figure 11.25 Using multiple bounding objects.
as a third for the periscope (Figure 11.25). To check for intersection, we test
each bounding primitive for the ﬁrst model against all the primitives in the
second, much as we would have done for the triangles.
To speed this up we can keep our original bounding capsule and use it as
a rough test before checking further. Better still, we can generate bounding
spheres for each model and test against those instead. It’s a very cheap test and
can do a great job of culling large numbers of cases. We could also generate
bounding spheres for each of our smaller capsules, and use these spheres in
preliminary culling steps before checking individual capsule pairs.
This gives us a bounding hierarchy for our model (Figure 11.26). We
compare the top level bounding spheres ﬁrst. Only if they are intersect-
ing do we then move on to the lower level of sphere check and capsule
check. This can cull out a large number of cases and make it much more
likely that we’ll be testing only the two lower-level capsules that are actually
intersecting.
Bounding hierarchies work very well with scene graphs, and it’s fairly
simple to add this functionality to our existing classes. We begin by adding
an IvBoundingSphere member to each IvSpatial object. In addition, our
IvGeometry leaf nodes will have two IvCapsule members: one for local space
and one that we’ll transform into world space. This gives us our culling sphere
hierarchy, with capsules as the lowest-level test. Now we need to pregener-
ate the bounding parameters before initiating the collision test process. This
is done as a part of the recursive propagation of transform information. We
propagate the transform information down from the root. When we reach a
leaf node, we generate a new world space sphere and capsule from the updated
transform data. Then as we undo the recursion, we propagate the changes in
the bounding spheres back up. At each IvNode level, we merge its children’s
bounding spheres to obtain the sphere for the node. Note that if an update is
called on a node other than the root, undoing the recursion is not sufﬁcient.
We must contain propagating the new bound upward, all the way to the root.
The procedure to merge two spheres is as follows. If one sphere completely
surrounds the other, then the larger sphere is clearly the minimum enclos-
ing sphere. However, in most cases the two spheres are interpenetrating or
separate. The situation can be seen in Figure 11.27. We have two spheres, one

11.4 A Simple Collision System 565
Figure 11.26 Using bounding hierarchy.
C0
C1
r0
r1
C
C1 – C0
Figure 11.27 Merging two spheres.

566
Chapter 11 Intersection Testing
with center C0 and radius r0, and the other with center C1 and radius r1. The
diameter of the new sphere will be r0 + ∥C1 −C0∥+ r1, so the radius r will be
1/2(r0+r1)+1/2∥C1−C0∥. The new center will lie along the line C0+t(C1−C0).
We determine t by moving r0 in distance back along the line to the edge of the
sphere, and then r units forward to the new center. The resulting code is
Library
IvCollision
Filename
IvBoundingSphere
IvBoundingSphere Merge( const IvBoundingSphere& s0,
const IvBoundingSphere& s1 )
{
IvVector3 diff = s1.mCenter - s0.mCenter;
float distsq = diff.Dot(diff);
float radiusdiff = s1.mRadius - s0.mRadius;
// one sphere inside other
if ( distsq <= radiusdiff*radiusdiff )
if ( s0.mRadius > s1.mRadius )
return s0;
else
return s1;
// build new sphere
float dist = ::IvSqrt( distsq );
float newRadius = 0.5f*( s0.mRadius + s1.mRadius + dist );
IvVector3 newCenter = s0.mCenter;
if (!::IsZero( dist ))
newCenter += ((newRadius-s0.mRadius)/dist)*diff;
return IvBoundingSphere( newCenter, newRadius );
}
Library
IvScene
Filename
IvGeometry
IvNode
Finding collisions between the two hierarchies is another recursive pro-
cess. We’ll deﬁne a virtual method in IvSpatial called Colliding, which
checks for collision between the current object and another IvSpatial object.
Represented in pseudocode, this is
Boolean IvGeometry::Colliding(IvSpatial* other)
{
if other is not IvGeometry node
return other->Colliding( this )
else
if both spheres and capsules intersecting return TRUE
}

11.4 A Simple Collision System 567
For IvNodes, we use the following:
Boolean IvNode::Colliding(IvSpatial* other)
{
if bounding spheres are not colliding, return FALSE
else
if this node has children
for each child do
if child->Colliding(other)
return TRUE
return FALSE
else if other node has children
for each child in other node do
if other_child->Colliding(this)
return TRUE
return FALSE
else
return FALSE
// shouldn’t happen
}
This will ﬁnd the ﬁrst collision between the hierarchies. You may wish to ﬁnd
them all (there may be more than one if our models are not convex). If so,
instead of returning TRUE immediately when a collision is found, store the
collision information and proceed to the next child.
We can take this technique of using bounding hierarchies further. For
example, if we want to do triangle-triangle intersection testing, we can build
a hierarchy to perform coarser but cheaper intersection tests. If two objects
are intersecting, we can traverse the two hierarchies until we get to the two
intersecting triangles (there may be more than two if the objects are concave).
Obviously, we’ll want to create much larger hierarchies in this case. Gener-
ating them so that they are as efﬁcient as possible — they both cull well and
have a reasonably small tree size — is not a simple task. Gottschalk et al. [48]
provide some information for building OBB-trees, while Ericson [34] covers
the general cases.
Spheres, capsules, AABBs, and OBBs have all been used as primitives
for culling bounding hierarchies. Most tests have been done for hierarchies
with triangles as leaf nodes. Gottschalk [48] demonstrates that OBBs work
better than both AABBs and spheres if our models have static geometry.
However, if we’re constantly deforming our vertices — for example, with
skinned character models — recomputing the OBBs in the hierarchy is an
expensive step. Using spheres or AABBs can be a better choice in this
circumstance.

568
Chapter 11 Intersection Testing
11.4.3 Dynamic Objects
So far we have been using intersection tests assuming that our objects don’t
move between frames. This is clearly not so. In games, objects are constantly
moving, and we need to be careful when we use static tests to catch collisions
between moving objects.
For example, in one frame we have two objects moving towards each
other, clearly heading for a collision somewhere in the center of the screen
(Figure 11.28a). Ideally, in the next frame we want to catch a snapshot of
them just as they collide, or are slightly intersecting. However, if we take too
large a simulation step, they may have passed partially through each other
(Figure 11.28b). Using a frame-by-frame static test we will miss the initial
collision. Worse yet, if we take a larger step, the two objects will have passed
right through each other, and we’ll miss the collision entirely.
One way to catch this is to sweep our bounding primitives along a path
and then test intersection between the swept primitives that we’ve generated.
A simple example of this is testing intersection between two moving spheres.
If we sweep a sphere along a line segment, we get — no surprise — a capsule.
Based on the two objects’ velocities, we can generate capsules for each object
and test for intersection. If one is found, then we know the two objects may
collide somewhere between frames and we can investigate further.
We generally have to worry about this problem only when the relative
velocities of objects are large enough or the frame times are long enough that
Figure 11.28a Potential collision.
Figure 11.28b Partially missed collision.

11.4 A Simple Collision System 569
one object can move, relative to another, farther than half its thickness in the
direction of travel. For example, a tank with a speed of 30 km/hr moves about
0.12 m/frame, assuming 60 frames/sec. If the tank is 10 m long, its movement
is miniscule compared to its total length and we can probably get away with
static testing. Suppose, however, that we ﬁre a 1 m long missile at that tank,
traveling at 120 km/hr. We also have a bug in our rendering code which causes
us to drop to 10 frames/sec, giving us a travel distance of 3-1/3 m. The missile’s
path crosses through the tank at an angle and is already through it by the next
frame. This may seem like an extreme example, but in collision systems it’s
often best to plan for the extreme case.
Walls, since they are inﬁnitely thin, also insist for a dynamic test of some
kind. In a ﬁrst-person shooter you don’t want your players using a cheat to
teleport through a wall by moving too fast. One way to handle this is to do
a simple test of the player’s path versus the nearest wall plane. Another is to
create a plane for each wall with the normals pointing into the room; if a
plane test shows that the object is on the negative side of the plane, then it’s
no longer in the room.
Submarines are large and move relatively slowly for their size, so for
this collision system we don’t need to worry about this issue. However, it is
good to be aware of it. For more information on managing dynamic tests,
see Eberly [27].
11.4.4 Performance Improvements
Demo
SweepPrune
Now that we’ve handled questions of which bounding shapes to use on our
objects and how to achieve a tighter ﬁt even with simple primitives, we’ll
consider ways of improving our performance. The main way we’ll approach
this is to cut down on intersection tests. We’ve already handled this to some
extent at the model level, by using a bounding hierarchy to cut down on
intersection tests between primitives. Now we want to look at the world level,
by cutting down on tests between models. For example, if two objects are
relatively small and at opposite ends of the map from each other, it’s a pretty
good bet that they’re not colliding.
The most basic way to check collisions among all objects is the following
loop:
for each object i
for each object j, where j <> i
test for collision between i and j
There are a number of problems with this. First of all, we’re doing n(n −1)
tests, which is an O(n2) algorithm. Half of those tests are duplicates: if we

570
Chapter 11 Intersection Testing
test for collision between objects 1 and 5, we’ll also test for collision between
5 and 1. Also, there may be a number of objects that we wish to collide with
that simply aren’t moving. We don’t want to test collision between two such
static objects. A better loop which handles these cases is
for each object i
for each object j, where j > i
if (i is moving or j is moving)
test for collision between i and j
There are other possibilities. We can have two lists: one of moving objects
called Colliders and one of moving or static objects called Collidables. In the
ﬁrst loop we iterate through the Colliders and in the second the Collidables.
Each Collider should be tagged after its turn through the loop, to ensure
collision pairs aren’t checked twice. Still, even with this change, we’re still
doing O(nm) tests, where n is the number of Colliders and m is the number
of Collidables. We need to ﬁnd a way to further cut down the number of
checks.
Most approaches involve some sort of spatial subdivision to do this. The
simplest is to slice the world, along the x-axis say, by a series of evenly spaced
planes (Figure 11.29). This creates a set of slabs, bounded by the planes along
the x-direction, and by whatever bounds we’ve set for our world in the y- and
z-directions. For each slab, we store the set of objects that intersect it. To test
for collisions for a particular object, we determine which slabs it intersects
and then test against only the objects in those slabs. This approach can be
extended to other spatial subdivisions, such as a grid or voxel-based system.
One of the disadvantages of the regular spatial subdivisions is that they
don’t handle clumping very well. Let’s consider slabs again. If our world is
fairly sparse, there may be large numbers of slabs with no objects in them,
and a very few with most of the objects in them. We still may end up doing
a large number of checks within each slab — which is the problem we were
trying to avoid.
There is another possibility used by a number of collision-detection sys-
tems, known as the sweep-and-prune method. It is similar to the separating
axis test that we used for OBBs (it’s also related to some scanline rasteriza-
tion algorithms). Instead of using a regular grid for our world, we’ll use the
extents of our objects as our grid. For each object, we project its extents onto
the x-axis. To keep things efﬁcient, we can use our root-level bounding sphere
to compute our extents, which for a sphere with center C and radius r, gives
us an interval of [cx −r, cx + r].
Given the extent endpoint pairs for each object, we’ll mark them with a
pointer to the object, and indicate for each value whether it is the low (start)
or high (ﬁnish) endpoint. Finally, we sort all endpoints from low to high.

11.4 A Simple Collision System 571
Figure 11.29 Cutting collision space into slabs.
Once the sorted list of endpoints is created, the collision detection process
runs as follows:
for each endpoint do
if a start point
if object is moving
check collisions against all objects in list
else
check collisions against moving objects in list
add corresponding object to list
else if a finish point
remove corresponding object from list
Figure 11.30 shows how this works. We sweep from left to right along the
x-axis and use the sorted endpoints to test intersections of intervals before
the more complex intersection tests.

572
Chapter 11 Intersection Testing
x-axis
Figure 11.30 Dividing collision space by sweep and prune.
Normally this would be an (n log n) algorithm due to the sorting oper-
ation. However, if the timestep is small enough, the relative position of the
objects won’t have changed that much from frame to frame — this is referred
to as temporal coherence. Any changes that do happen will be rare but local-
ized. Therefore, if we use a sorting algorithm that works best on mostly sorted
lists, such as bubble or insertion sort, we can get linear time for our sort and
hence an O(n) algorithm.
This algorithm still has problems, of course. If our objects are highly
localized (or clumped) in the x direction, but separated in the y direction,
then we may still be doing a high number of unnecessary intersection tests.
But it is still much better than the naive O(n2) algorithm we were using before.
11.4.5 Related Systems
The other two systems we mentioned earlier were ray casting, for picking and
AI tests, and frustum culling. Both systems can beneﬁt from the techniques
described in our collision system, in particular the use of bounding hierarchies
and spatial partitioning.
Consider the case of ray casting. Instead of testing the ray directly against
the object, we can take the ray and pass it through the hierarchy until (if
we desire) we get the exact triangle of intersection. Further culling of testing

11.4 A Simple Collision System 573
Figure 11.31 False positive for frustum intersection.
can be done by using a spatial partitioning system such as voxels to consider
only those objects that lie in the areas of the spatial partitioning that intersect
the ray.
When handling frustum culling, the most basic approach involves testing
an object against the six frustum planes. If, after this test, we determine that
the object lies outside one of the planes, then we consider it outside the frus-
tum and do not render it. As with ray casting, we can improve performance
by using a bounding hierarchy at progressive levels to remove obvious cases.
We can also use a spatial partition again, and consider only objects that lie in
the areas of the partition within the view frustum.
However, there is one aspect of frustum culling of which we need to be
careful. This also applies to any intersection test that requires determin-
ing whether we are inside a convex object. Consider the situation shown
in Figure 11.31. The bounding sphere is near the corner of the view frus-
tum and clearly intersecting two planes. By using the scheme described, this
sphere would be considered as intersecting the frustum, but it is clearly not.
An alternative is shown in Figure 11.32a. Instead of using the frustum, we
trace around the frustum with the bounding sphere to get a rounded, larger
frustum1. This represents the maximum extent that a bounding sphere can
1.
This process is also known as convolution.

574
Chapter 11 Intersection Testing
Figure 11.32a Expanding view frustum for simpler inclusion test.
Figure 11.32b Expanding view frustum for simpler inclusion test.

11.5 Chapter Summary 575
have and still be inside the frustum. Instead of testing the sphere, we can test
its center against this shape. In practice we can just push out the frustum
planes by the sphere radius (Figure 11.32b), which is close enough. Similar
techniques can be used for other bounding objects; see Möller and Haines [79]
or Watt and Policarpo [113] for more details.
11.4.6 Section Summary
The proceeding should give some sense of the decisions that have to be made
when handling collision detection or other systems that involve object inter-
section: pick base primitives, choose when you’ll use them, consider whether
to manage dynamic intersections, and cull unnecessary tests. However, this
shouldn’t be taken as the only approach. There are many other possible algo-
rithms that handle much more complex cases than these. For example, there
are systems, such as the University of North Carolina’s I-COLLIDE, that
track closest pairs of objects. This allows for considerable culling of intersec-
tion tests. There are also more sophisticated methods for managing spatial
partitions, such as portals, octrees, BSP trees, and kd-trees. Whether the
algorithmic complexity is necessary will depend on the application.
11.5 Chapter Summary
Testing intersection between geometric primitives is a standard part of any
interactive application. This chapter has presented a few examples to provide
a taste of how such algorithms are created. Most derive from a careful use
of the basic properties of vectors and points as presented in Chapter 1. Using
our intersection methods wisely allows us to build an efﬁcient system for
detecting collision between objects, casting rays for AI visibility checks and
picking, and frustum culling.
For those who are interested in reading further, a more thorough pre-
sentation of geometric distance and intersection methods can be found in
Schneider and Eberly [96]. These techniques fall under a general class of algo-
rithms known as computational geometry; good references are Preparata and
Shamos [91], and O’Rourke [84]. Two different approaches to building colli-
sion detection systems can be found in van den Bergen [109] and Ericson [34].
Finally, use of intersection techniques in rendering, plus information on more
complex spatial partitioning techniques, can be found in both Möller and
Haines [79] and Watt and Policarpo [113].


Chapter12
Rigid Body
Dynamics
12.1 Introduction
In many games, we move our objects around using a very simple movement
model. In such a game, if we hold down the up arrow key, for example, we
apply a constant forward translation, once a frame, to the object until the key
is released, at which point the object immediately stops moving. Similarly,
we can apply a constant rotation to the object if the left arrow key is held,
and again, it stops upon release. This is ﬁne for something with fast action
like a platform game or a ﬁrst-person shooter, where we want quick response
to our input. As soon as we hit a key, our character starts moving and stops
immediately upon release. This motion model is known as kinetics and can
be thought of as an application of the theories of Aristotle.
But suppose we want to do a more realistically styled game, for example,
a submarine game. Submarines don’t start and stop on a dime. When the
propeller starts turning, it takes some time for the submarine to start forward.
And they don’t really have instantaneous brakes — when the engine is shut off
they will drift for quite a while before stopping. Turning is much the same —
they will respond slowly to application of the rudder and then straighten out
over time.
Even in a fast action game, we may want to model how objects in the
world react to our main character. When we push an object, we don’t expect
it to stop instantly when we stop pushing, nor do we expect it to keep moving
forever. If we knock a chair over, we don’t expect it to fall straight back and
then stick to the ﬂoor; we expect it to turn depending on where we hit it, and
577

578
Chapter 12 Rigid Body Dynamics
then bounce and possibly roll once. We want the game world to react to our
character as the real world reacts to us, in a physically correct manner.
For both of these cases, we will want a better model of movement, known
as a physically based simulation. One chapter is hardly enough space to
encompass this broad topic, which covers the preceding effects, as well as
objects deforming due to contact, ﬂuid simulation, and soft body simulations
such as cloth and rope. Instead, we’ll concentrate on a simpliﬁed problem
which is useful in many circumstances: objects that don’t deform (known
as rigid bodies) and move based on Newton’s laws of motion (known as
dynamics). We’ll discuss techniques for translating rigid bodies through space
in a physically based manner (linear dynamics) and then how to encompass
rotational effects (rotational dynamics). Finally, we’ll discuss some meth-
ods for handling collisions within our simulation, again covering linear and
rotational effects in turn.
The convention in physics is to represent some vector quantities by capital
letters. To maintain compatibility with physics texts we will use the same
notation and assume that the reader can distinguish between such quantities
and the occasional matrix by context.
12.2 Linear Dynamics
12.2.1 Moving with Constant Acceleration
Let’s consider our object’s movement through our game world as a function
X(t), which represents the position of the object for every time t. If we plot
just the x values against t for our simple motion model, we would end up with
a graph similar to that in Figure 12.1. Notice that we travel in a straight line
for a while and then turn sharply in another direction, or we hold position.
This is like our piecewise linear interpolation, except that in this case, the
future x values are unknown; they are determined by the input of the player.
For a given frame i, this can be represented by a line equation
Xi(hi) = Xi + hivi
where Xi represents the position at the start of frame i, vi is a vector generated
from the player input which points along each line segment, and hi is our
frame time. We’ll simplify things further by considering just the function on
the ﬁrst line segment, from time t ≥0:
X(t) = X0 + tv0
where X0 = X(0).

12.2 Linear Dynamics 579
x
t
Figure 12.1 Graph of current motion model, showing x-coordinate of particle as a
function of time.
If we take the derivative of this function with respect to t, we end up with
dX
dt = X′(t) = v0
(12.1)
This derivative of the position function is known as velocity, which is usually
measured in meters per second, or m/s. For our original motion model, we
have a constant velocity across each segment. If we continue taking deriva-
tives, we ﬁnd that the second derivative of our position function is zero, which
is what we’d expect when our velocity is constant.
Now let’s assume that our second derivative, instead of being zero, is
a constant nonzero function. To achieve this, we’ll change our velocity
function to
v(t) = v0 + ta
(12.2)
Now v(t) is also an afﬁne function, this time with a constant derivative vector a,
called acceleration, or
dv
dt = v′(t) = a
(12.3)
The units for acceleration are usually measured in meters per second squared,
or m/s2.
Our original function X(t) used a constant v0, so now we’ll need to rewrite
it in terms of v(t). Since v is changing at a constant rate across our time

580
Chapter 12 Rigid Body Dynamics
interval, we can instead use the average velocity across the interval, which is
just one-half the starting velocity plus the ending velocity, or
¯v = 1
2(v0 + v(t))
Substituting this into our original X(t) gives us
X(t) = X0 + t
1
2(v0 + v(t))

Substituting in for v(t) gives the ﬁnal result of
X(t) = X0 + tv0 + 1
2t2a
(12.4)
Our equation for position becomes a quadratic equation, and our velocity is
represented as a linear equation:
Pi(t) = Pi + tvi + 1
2t2ai
vi(t) = vi + tai
So given a starting position and velocity, and an acceleration which is
constant over the entire interval [0, t], we can compute any position within
the interval. As an example, let’s suppose we have a projectile, with an initial
velocity v0 and initial position P0. We represent acceleration due to gravity by
the constant g, which is 9.8 m/s2. This acceleration is applied only downward,
or in the −z direction, so a is the vector (0, 0, −g). If we plot the z component
as a function of t, then we get a parabolic arc, as seen in Figure 12.2. This
function will work for any projectile (assuming we ignore air friction), from
x0
v0
Figure 12.2 Parabolic path of object with initial velocity and affected only by
gravity.

12.2 Linear Dynamics 581
a thrown rock (low initial velocity) to a cannonball (medium initial velocity)
to a bullet (high initial velocity)1.
Within our game, we can use these equations on a frame-by-frame basis
to compute the position and velocity at each frame, where the time between
frames is hi. So for a given frame i + 1:
Xi+1 = Xi + hivi + 1
2h2
i ai
vi+1 = vi + hiai
12.2.2 Forces
One question that has been left open is how to compute our acceleration
value. We do so based on a vector quantity known as a force. Forces cause
change in an object’s motion, pushing or pulling it around, either to speed it
up or slow it down. So for example, to throw a ball your hand and arm exert
a certain force on it, to begin its motion through the air. That force, when
applied, produces an acceleration directly proportional to the object’s mass,
measured in kilograms. The proportional relationship is shown in Newton’s
second law of motion:
F = ma
The units for force end up being kg-m/s2, or newtons, in homage to its
creator.
In the previous section, we represented gravity as an acceleration, but in
truth it is a force whose value is always proportional to the mass of the object.
For an object with mass m on Earth, its magnitude is mg and its direction
points to the center of the Earth, although we usually assume the world is
locally ﬂat. Other possible forces include the friction caused by air or water
molecules pushing against an object to slow it down, or the thrust generated
by a rocket engine or propeller, or simply the normal force of the ground
pushing up to counteract gravity (there has to be such a force; otherwise we’d
sink into the earth). In general, if something is pushing or pulling on an object,
there is a force there.
Usually we have more than one force applied to an object at a time. Taking
our ball example, we have the initial force when the ball is thrown, force due
to gravity, and forces due to air resistance and wind. After the ball leaves your
hand, that pushing force will be removed, leaving only gravity and air effects.
Forces are vectors, so in both cases we can add all forces on an object together
1.
In most cases, this last is approximated by a line equation for efﬁciency reasons.

582
Chapter 12 Rigid Body Dynamics
to create a single force which encapsulates their total effect on the object. We
then scale the total force by 1/m to get the acceleration for equation 12.4.
For simplicity’s sake, we will assume for now that our forces are applied
in such a way that we have no rotational effects. In Section 12.4 we’ll discuss
how to handle such cases.
12.2.3 Linear Momentum
As we’ve seen, the relationship between acceleration and velocity is
a = dv
dt
There is a corresponding related entity P for a force F, which is
F = ma = mdv
dt = dP
dt
The quantity P = mv is known as the linear momentum of the object, and
it represents the tendency for an object to remain in its current linear motion.
The heavier the object or faster it is moving, the greater the force needed to
change its velocity. So while a pebble at rest is easier to kick aside than a
boulder, this is not necessarily true if the pebble is shot out of a gun.
An important property of Newtonian physics is the conservation of
momentum. Suppose we take a collection of objects and treat them as a sin-
gle system of objects. Now consider only the forces within the system, that
is, only those forces acting between objects. Newton’s third law of motion
states that for every action, there is an equal and opposite reaction. So for
example, if you push on the ground due to gravity, the ground pushes back
just as much, and the forces cancel. Due to this, within the system, pairwise
forces between objects will cancel and the total force is zero. If the external
force is 0 as well, then
F = dP
dt = 0
so P is constant. No matter how objects may move within the system, the total
momentum must be conserved. This property will be useful to us when we
consider collisions.
12.2.4 Moving with Variable Acceleration
There is a problem with the approach that we’ve been taking so far: we
are assuming that total force, and hence acceleration, is constant across

12.2 Linear Dynamics 583
the entire interval. For more complex simulations this is not the case. For
example, it is common to compute a drag force proportional to but opposite
in direction to velocity:
Fdrag = −mρv
(12.5)
This can provide a simple approximation to air friction; the faster we go, the
greater the friction force. The quantity ρ in this case controls the magnitude
of drag. An alternative example is if we wish to model a spring in our system.
The force applied depends on the current length of the spring, so the force is
dependent on position:
Fspring = −kX
The spring constant k fulﬁlls a similar role to ρ: it controls the proportion of
force dependent on the position. In both of these cases, since acceleration is
directly dependent on the force, it will vary over the time interval as velocity
or position vary. It is no longer constant. So for these cases, equations 12.2
and 12.4 are incorrect.
In order to handle this, we’ll have to use an alternative approach. We begin
by deriving a function for velocity in terms of any acceleration. Rewriting
equation 12.3 gives us
dv = a dt
To ﬁnd v we take the indeﬁnite integral or antiderivative of both sides
'
dv =
'
a dt
For example, if we assume as before that a is constant, we can move it
outside the integral sign
'
dv = a
'
dt
And integrating gives us
v = ta + c
We can solve for c by using our velocity v0 at time t = 0:
c = v0 −0 · a
= v0

584
Chapter 12 Rigid Body Dynamics
So our ﬁnal equation is as before:
v(t) = v0 + ta
We can perform a similar integration for position. Rewriting equation 12.1
gives
dX = v(t)dt
We can substitute equation 12.2 into this to get
dX = v0 + ta dt
Integrating this, as we did with velocity, produces equation 12.4 again.
For general equations we perform the same process, re-integrating dv to
solve for v(t) in terms of a(t). So, using our drag example, we can divide
equation 12.5 by the mass m to give acceleration:
a = dv
dt = −ρv(t)
Rearranging this and integrating gives
'
dv =
'
−ρv(t) dt
We can consult a standard table of integrals to ﬁnd that the answer in this
case is
v(t) = v0e−ρt
where, as before, v0 = v(0).
While this particular equation was relatively straightforward, in general
calculating an exact solution is not as simple as the case of constant
acceleration. First of all, differential equations in which the quantity we’re
solving for is part of the equation are not always easily — if at all — solvable
by analytic means. In many cases we will not necessarily be able to ﬁnd an
exact equation for v(t), and thus not for X(t). And even if we can ﬁnd a solution,
every time we change our simulation equations, we’ll have to integrate them
again, and modify our simulation code accordingly. Since we’ll most likely
have many different possible situations with many different applications of
force, this could grow to be quite a nuisance. Because of both these reasons,
we’ll have to use a numerical method that can approximate the result of the
integration.

12.3 Initial Value Problems 585
12.3 Initial Value Problems
12.3.1 Definition
The solutions for v and X that we’re trying to integrate fall under a class of
differential equation problems called initial value problems. In an initial value
problem, we know the following about a function y(t):
1. An initial value of the function y0 = y(t0)
2. A derivative function f(t, y) = y′(t)
3. A time interval h
The problem we’re trying to solve is, given these parameters, what is the
value at y(t0 + h)? For our purposes, this actually becomes a series of ini-
tial value problems: at each frame our previous solution becomes our new
initial value yi, and our interval hi will be based on the current frame time.
Once computed, our new solution will become the next initial value yi+1.
More speciﬁcally, the initial value yi is our current position Xi and current
velocity vi, stored in a single 6-vector as
yi =
 Xi
vi

So how do we evaluate the derivative function f(t, y)? This will be another
vector quantity:
f(t, y) =
 X′
i
v′
i

The value of our derivative for Xi is our current velocity vi. Our derivative for
vi is the acceleration, which is based on the current total force. To compute
this total force, it is convenient to create a function called CurrentForce(),
which takes X and v as arguments and combines any forces derived from
position and velocity with any constant forces, such as those created from
player input. We’ll represent this as Ftot(t, X, v) in our equations. So given our
current state, the result of our function f(t, y) will be
y′ = f(t, y) =

vi
Ftot(ti, Xi, vi)/m


586
Chapter 12 Rigid Body Dynamics
x
t
Figure 12.3 Various solution lines for initial value problem x′ = −ρx, integrated
against time. Which line is correct depends on initial value chosen.
The function f(t, y) is important in understanding how we can solve this
problem. If we graph such a function for a ﬁxed t and y we can see that
it is a vector ﬁeld. Figure 12.3 shows a two-dimensional plot of one such vec-
tor ﬁeld, accentuating certain lines of ﬂow. If we start at a particular point
and follow the vector ﬂow, this will trace out one possible solution to the dif-
ferential equation, starting at that initial value. This gives us a sense of what
our general approach will be. We’ll start at yi and then, using our derivative
function, take steps in time to generate new samples that approximate the
function, until we generate an approximation for yi+1. In a way, we are doing
the opposite of what we were doing when we were interpolating. Instead of
generating an approximation to an unknown function based on known sample
points, we’re generating approximate sample points based on the derivative
of an unknown function.

12.3 Initial Value Problems 587
12.3.2 Euler’s Method
Demo
Force
Assuming our current time is t and we want to move ahead h in time, we could
use Taylor’s series to compute y(t + h):
y(t + h) = y(t) + hy′(t) + h2
2 y′′(t) + · · · + hn
n! y(n)(t) + · · ·
We can rewrite this to compute the value for timestep i + 1, where the
time from ti to ti+1 is hi:
yi+1 = yi + hiy′
i + h2
i
2 y′′
i + · · · + hn
i
n! y(n)
i
+ · · ·
This assumes, of course, that we know all the values for the entire inﬁnite
series at timestep i, which we don’t — we have only yi and y′
i. However, if hi is
small enough and all values of y′′
i are bounded, we can use an approximation
instead:
yi+1 ≈yi + hiy′
i
≈yi + hif(ti, yi)
Another way to think of this is that we have a function f(ti, yi) that, given a
time ti and initial value yi, can compute tangents to the unknown function’s
curve. We can start at our known initial value, and step hi distance along
the tangent vector to get to the next approximation point in the vector ﬁeld
(Figure 12.4).
Separating out position and velocity gives us
Xi+1 ≈Xi + hiX′
i
≈Xi + hivi
vi+1 ≈vi + hiv′
i
≈vi + hiFtot(ti, Xi, vi)/m
This is known as Euler’s method.
To use this in our game, we start with our initial position and velocity. At
each new frame, we grab the difference in time between the previous frame
and current frame and use that as hi. To compute f(ti, yi) for the velocity,
we use our ComputeForces() method to add up all of the forces on our object
and divide the result by the mass to get our acceleration. Plugging in our
current values, we use the preceding formulas to generate our new position
and velocity. In code, this looks like

588
Chapter 12 Rigid Body Dynamics
x
t
x0
x1
x2
Figure 12.4 Using Euler’s method to move from one position to another, using
derivative function. Note that we end up stepping from one solution curve to another.
void
SimObject::Integrate( float h )
{
IvVector3 accel;
// compute acceleration
accel = ComputeForces( mTime, mPosition, mVelocity ) / mMass;
// clear small values
accel.Clean();
// compute new position, velocity
mPosition += h*mVelocity;
mVelocity += h*accel;
// clear small values
mVelocity.Clean();
}
It’s important to compute the new velocity after the new position in this case,
so that we don’t overwrite the velocity prematurely.
Note that we clear near-zero values in the new velocity. This prevents little
shifts in position due to tiny changes in velocity, such as those generated after
an object has slowed down due to drag. While technically accurate, they can
be visually distracting, so after a certain point we clamp our velocity to zero.
The same is done with acceleration.

12.3 Initial Value Problems 589
For many cases, this works quite well. If our time steps are small enough,
then the resulting approximation points will lie close to the actual function
and we will get good results. However, the ultimate success of this method is
based on the assumption that the slope at the current point is a good estimate
of the slope over the entire time interval h. If not, then the approximation
can drift off the function, and the farther it drifts, the worse the tangent
approximation can get. An example of this can be seen in Figure 12.5. The
ﬁrst step in our approximation takes us to a point in the vector ﬁeld where the
derivatives are ﬂowing in the other direction, and we thus oscillate around
the actual solution. Once the error grows, in many cases further steps don’t
get us back, and we continue to drift off of the actual solution.
For Euler’s method, we say that the error is directly dependent on the
time step, or O(h). So one solution to this problem is to decrease the time
step — for example take a step of h/2, followed by another step of h/2. While
this may solve some cases, we may need to take a smaller time step, say h/4.
x
t
x0
x1
x2
Figure 12.5 Taking too large a simulation step and oscillating around the solution.

590
Chapter 12 Rigid Body Dynamics
And this may still lead to signiﬁcant error. In the meantime, we are grinding
our simulation to a halt while we recalculate quantities 4 or 8 or however
many times for a single frame.
Situations that can lead to problems with Euler’s method are often char-
acterized by large forces. If we examine the remaining terms of the Taylor
expansion,
h2
i
2 y′′
i + · · · + hn
i
n! y(n)
i
+ · · ·
we can see why this could cause a problem. When we set up our approxi-
mation, we assumed that hi was small and y′′
i bounded. If we’re considering
position, a large force leads to a large acceleration, which leads to a larger
difference between our approximation and the actual value. Larger values of
hi will magnify this error. Also, if the force changes quickly, this means that
the magnitude of the velocity’s second derivative is high, and so we can run
into similar problems with velocity.
There are other issues with our particular example. It falls into a class
of differential equations known as stiff systems. Situations that can lead to
stiffness problems are often characterized by large spring and damping forces,
such as in a stiff spring (hence the name). Examples of such systems have
terms with rapidly decaying values, such as e−ρt — exactly the case when we
apply drag. These terms tend to zero as t approaches inﬁnity but, as we’ve
seen, won’t always converge with a numerical method unless we control the
step size appropriately. The larger ρ is, the smaller h must be. This can also
affect systems where we wouldn’t expect the term to contribute that much. For
example, suppose the solution to our system is y(t) = 1+e−200t. As t increases
from zero, y(t) quickly approaches 1. However, approximating this with a
numerical method without taking care to control the error can lead the e−200t
term to dominate the calculations, which leads to invalid results.
So while we can try to reduce stepsize, the number of calculations required
may make it unworkable. Fortunately, there are other methods that we
can try.
12.3.3 Midpoint Method
Demo
Force
So far we’ve been using the derivative at the beginning of the interval as our
estimate of the average tangent. A better possibility may be to take the deriva-
tive in the middle of the interval. To do this, we ﬁrst use Euler’s method to take
a step halfway into the interval; that is, we integrate using a step size of h/2.
Given our estimated position and velocity at the halfway point, we calculate
f(t, y) at this location. We then go back to our original starting location, and

12.3 Initial Value Problems 591
x
t
x0
x'1/2
x1/2
Figure 12.6a First step of midpoint method. Step one-half time increment using
Euler’s method and compute derivative there.
x
t
x0
x1
Figure 12.6b Using the midpoint derivative to step forward to our next position.
use the derivatives we calculated at the midpoint to move across the entire
interval. This method is known as the midpoint method.
Figures 12.6a and 12.6b show how this works with our original function.
In Figure 12.6a, the arrow shows our initial half-step, and the line our esti-
mated tangent. Figure 12.6b uses the tangent we’ve calculated with our full
time step, and our ﬁnal location. As we can see, with this method we are

592
Chapter 12 Rigid Body Dynamics
following much closer to the actual solution and so our error is much less
than before. The order of the error for the midpoint method is dependent on
the square of the time step, or O(h2), which for values of h less than 1 is better
than Euler’s method. Instead of approximating the function with a line, we
are approximating it with a quadratic.
Code to compute the midpoint method is as follows:
void
SimObject::Integrate( float h )
{
IvVector3 totalForce = ComputeForces( mPosition, mVelocity );
IvVector3 accel;
// compute acceleration
accel = 1.0f/mMass * totalForce;
// clear small values
accel.Clean();
// compute midpoint position, velocity
float h2 = 0.5f*h;
IvVector3 midPosition = mPosition + h2*mVelocity;
IvVector3 midVelocity = mVelocity + h2*accel;
// clear small values
midVelocity.Clean();
// compute force there
totalForce = ComputeForces( midPosition, midVelocity );
accel = 1.0f/mMass * totalForce;
// clear small values
accel.Clean();
// compute final position, velocity
mPosition += h*midVelocity;
mVelocity += h*accel;
// clear small values
mVelocity.Clean();
}
While the midpoint method does have better error tolerance than Euler’s
method, it still has problems when h gets large enough. To handle this, we’ll
have to consider some methods with better error tolerances still.

12.3 Initial Value Problems 593
12.3.4 Higher-order Methods
Both the midpoint method and Euler’s method fall under a larger class of
algorithms known as Runga-Kutta methods. Whereas both of our previous
techniques used a single estimate to compute a tangent for the entire interval,
others within the Runga-Kutta family compute multiple tangents at ﬁxed time
steps across the interval and take their weighted average.
One possibility is to take the derivative at the end of the interval, and aver-
age with the derivative at the beginning. Like the midpoint method, we can’t
actually compute the derivative at the end of the interval, so we’ll approxi-
mate it by performing normal Euler integration and computing the derivative
at that point. This is known as the modiﬁed Euler’s method. Interestingly, the
error for this approach is still O(h2), due to the fact that we’re taking an inac-
curate measure of the ﬁnal derivative. Another approach is Heun’s method,
which takes 1/4 of the starting derivative, and 3/4 of an approximated deriva-
tive 2/3 along the step size. Again, its error is O(h2), or no better than the
midpoint method.
The standard O(h4) method is known as Runga-Kutta order four, or
simply RK4. RK4 can be thought of as a combination of the midpoint
method and modiﬁed Euler, where we weight the midpoint tangent esti-
mates higher than the endpoint estimates. Representing this with our function
notation:
u1 = hif(ti, yi)
u2 = hif(ti + hi
2 , yi + 1
2u1)
u3 = hif(ti + hi
2 , yi + 1
2u2)
u4 = hif(ti + hi, yi + u3)
yi+1 = yi + 1
6[u1 + 2u2 + 2u3 + u4]
Clearly, improved accuracy doesn’t come without cost. To perform stan-
dard Euler requires calculating a result for f (t, y) only once. Midpoint,
modiﬁed Euler, and Heun’s need two calculations, and RK4 takes four. While
to achieve the level of error tolerance of RK4 would require many more evalu-
ations of Euler’s method, using RK4 still adds both complexity and increased
simulation time that may not be necessary. It does depend on your appli-
cation, but for simple rigid-body simulations with fast frame rates and low
accelerations, Euler’s method or one of the other two methods will probably
be suitable.

594
Chapter 12 Rigid Body Dynamics
12.3.5 Verlet Integration
Demo
Force
There is another class of integration methods, known as Verlet methods, that
is commonly used in molecular dynamics. Verlet methods have come to the
attention of the games community because they can be useful in simulating
collections of small, unoriented masses known as particles — in particular,
when constrained distances between particles are required [65]. Such systems
of constrained particles can simulate soft objects such as cloth, rope, and dead
bodies (this last is also known as rag doll physics).
The most basic Verlet method can be derived by adding the Taylor
expansion for the current timestep with the expansion for the previous
timestep:
y(t + h) + y(t −h) = y(t) + hy′(t) + h2
2 y′′(t) + · · ·
+ y(t) −hy′(t) + h2
2 y′′(t) −· · ·
Solving for y(t + h) gives us
y(t + h) = 2y(t) −y(t −h) + h2y′′(t) + O(h4)
Rewriting in our stepwise format:
yi+1 = 2yi −yi−1 + h2
i y′′
i
This gives us an O(h2) solution for integrating position from acceleration,
without involving velocity at all. This can be a problem if we want to use
velocity in our calculations, but we can estimate it as
vi = (Xi+1 −Xi)
2hi
One question may be, How do we ﬁnd the ﬁrst yi−1? The standard method is
to start the process off with one pass of standard Euler or other Runga-Kutta
method and store the initial position and integrated position. From there we’ll
have two positions to apply to our Verlet integration.
Standard Verlet has a few advantages: it is time invariant, which means
that we can run it forwards and then backwards and end up in the same place.
Also, the lack of velocity means that we have one less quantity to calculate.
Because of this, it is often used for particle systems, which generally are
not dependent on velocity. However, if we want to apply friction based on
velocity or when we want to handle spinning rigid objects, the lack of velocity
and angular velocity makes it more difﬁcult. There are ways around this, as

12.3 Initial Value Problems 595
described in [65], but in most cases it will be easier to use a method that
allows us to track both velocity terms. One other disadvantage is that our
velocity estimation is (a) not very accurate and (b) one time step behind our
position.
If you wish to use Verlet methods and require velocity, you have two
choices. Leapfrog Verlet tracks velocity, but at half a time step off from the
position calculation:
v(t + h/2) = v(t −h/2) + ha(t)
X(t + h) = X(t) + hv(t + h/2)
Like with standard Verlet, we can start this off with a Runga-Kutta method
by computing velocity at a half-step and proceed from there. If velocity on a
whole step is required, it can be computed from the velocities, but as with
standard Verlet, one time step behind position:
vi = (vi+1/2 −vi−1/2)
2
As with standard Verlet, leapfrog Verlet is an O(h2) method.
The third, and most accurate, Verlet method is velocity Verlet:
X(t + h) = X(t) + hv(t) + h2
2 a(t)
v(t + h) = v(t) + h/2[a(t) + a(t + h)]
Unlike with the previous Verlet methods, we now have to compute the accel-
eration twice: once at the start of the interval and once at the end. This can
be done in a stepwise manner by:
vi+1/2 = vi + hi/2ai
Xi+1 = Xi + hivi+1/2
vi+1 = vi+1/2 + hi/2ai+1
In between the position calculation and the velocity calculation, we recompute
our forces and then the acceleration ai+1. Note that in this case the forces
can be dependant only on position, since we have added only half of the
acceleration contribution to velocity. In the case of molecular dynamics or
particles, this isn’t a problem since most of the forces between them will be
positional, but again, for rigid body problems this is not the case.

596
Chapter 12 Rigid Body Dynamics
12.3.6 Implicit Methods
All the methods we’ve described so far integrate based on the current posi-
tion and velocity. They are called explicit methods and make use of known
quantities at each time step, for example Euler’s method:
yi+1 = yi + hy′
i
But even higher-order explicit methods don’t handle extreme cases of stiff
equations very well. Consider the following situation, as presented by Witkin
and Baraff [119]. Suppose we have a planet revolving in a perfectly circular
orbit. Our initial position is at the top of the orbit, and our initial velocity
pointing out to the right. Our only force is gravity, pointing towards the cen-
ter of the orbit. After one time step, our new position is off of the original
orbit and into a new one (Figure 12.7). In addition, we’ve added a tiny bit
of the gravitational acceleration to the velocity, making it slightly larger in
magnitude. After two steps, our position is further off our desired position,
and our velocity is still larger. As we continue, our approximation grows worse
and worse, spiraling away from our actual function.
What has happened is that our simulation error has accumulated and we
have added more and more energy to our system. One solution is to take tinier
steps, but as we saw with other stiff systems, we’re just trading simulation time
for more accuracy.
Figure 12.7 Using Euler’s method to simulate an orbit. The result spirals off of
the actual solution.

12.3 Initial Value Problems 597
Implicit methods make use of quantities from the next time step:
yi+1 = yi + hiy′
i+1
This particular implicit method is known as backwards Euler. The idea is
that we are going to grab the derivative at our destination rather than at our
current position. That is, we are going to ﬁnd a yi+1 with the derivative that,
if we were to run the simulation backwards, would end up at yi.
Implicit methods don’t add energy to the system, but instead lose it. This
doesn’t guarantee us more accuracy, but it does avoid simulations that spin
out of control — instead, they’ll dampen down to an equilibrium state. Since,
in most cases, we’re going to add a damping factor anyway, this is a small
price to pay for a more stable simulation.
This sounds good in theory, but in practice, how do we calculate y′
i+1?
One way is to solve for it directly. For example, let’s consider our air friction
example again. Recall that our force is directly dependent on velocity, but in
the opposing direction. Considering only velocity:
vi+1 = vi −hρvi+1
Solving for vi+1 gives us
vi+1 =
vi
1 + hρ
Figure 12.8 graphs this against the actual solution v0e−ρt. Note that we don’t
converge as fast when using the implicit method. However, we do converge,
and so this is better than the explicit method, which as we’ve seen oscillates
wildly for large h values.
We can’t always use this approach. Either we will have a function too
complex to solve in this manner, or we’ll be experimenting with a number
of functions and won’t want to take the time to solve each one individually.
Another way is to use a predictor-corrector method. We move ahead one step
using an explicit method to get an approximation. Then we use that approx-
imation to calculate our y′
i+1. This will be more accurate than the explicit
method alone, but it does involve twice the number of calculations, and
we’re depending on the accuracy of the ﬁrst approximation to make our ﬁnal
calculation.
Another more accurate approach is to rewrite the equation so that it can
be solved as a linear system. If we represent yi+1 as yi + yi, and ignore the
factor t, we can rewrite backwards Euler as
yi + yi = yi + hif(yi + yi)

598
Chapter 12 Rigid Body Dynamics
x
t
x0
x1
x3
x2
Figure 12.8 Comparing the exact solution with implicit Euler. The arrows for
implicit Euler point backwards to indicate that we are getting the derivative from the
next time step.
or
yi = hif(yi + yi)
We can approximate f(yi + yi) as f(yi) + f′(yi)yi. Note that f′(yi) is a matrix
since f(yi) is a vector. Substituting this approximation, we get
yi ≈hi(f(yi) + f′(yi)yi)
Solving for yi gives
yi ≈
 1
hi
I −f ′(yi)
−1
f(yi)
In most cases, this linear system will be sparse, so it can be solved in near-
linear time. More information can be found in [119].
As mentioned, implicit methods are really only necessary when our equa-
tions are so stiff that explicit methods are not practical. Examples of situa-
tions where implicit methods are useful are when simulating cloth, rope, or
rag doll physics. In general it is better to begin with an explicit method because
it is more efﬁcient, and only if you see wild oscillations or other signs of stiff
systems do you look into implicit methods.

12.4 Rotational Dynamics 599
12.4 Rotational Dynamics
12.4.1 Definitions
The equations and methods that we’ve discussed so far allow us to create
physical simulations which modify an object’s position. However, one aspect
of dynamics we’ve passed over is simulating changes in an object’s orientation
due to the application of forces, or rotational dynamics. When discussing
rotational dynamics, we use quantities that are very similar to those used in
linear dynamics. Comparing the two:
Linear
Rotational
position X
orientation  or q
velocity v
angular velocity ω
force F
torque τ
linear momentum P
angular momentum L
mass m
inertial tensor J
We’ll discuss each of these quantities in turn.
12.4.2 Orientation and Angular Velocity
Orientation we have seen before;
we’ll represent it by a matrix  or
a quaternion q. The angular velocity ω represents the change in orientation, or
ω = d
dt
It is a vector quantity, where the vector direction is the axis we rotate around
to effect the change in orientation, and the length of the vector represents the
rate of rotation around that axis, in radians per second.
The orientation and angular velocity are applied to an object around a
point known as the center of mass. The center of mass can be deﬁned as the
point associated with an object where, if you apply a force at that point, it will
move without rotating. One can think of it as the point where the object would
perfectly balance. Figure 12.9 shows the center of mass for some common
objects. The center of mass for a seesaw is directly in the center, as we’d
expect. The center of mass for a hammer, however, is closer to one end than
the other, since the head of the hammer is more massive than the handle.
For our objects, we’ll assume that we have some sense of where the cen-
ter of mass is — either it’s set by the artist or by some other means. One
possibility discussed shortly is to compute the center of mass directly from

600
Chapter 12 Rigid Body Dynamics
Figure 12.9 Comparing centers of mass. The seesaw balances close to the center,
while the hammer has center of mass closer to one end.
our model data. Other choices are to use the local model origin, or the bound-
ing box center (or centroid) as an approximation. Once the center of mass
is determined, it is usually convenient to translate our object so that we can
treat the local model origin as the center of mass, and therefore use the same
orientation and position representation for both simulation and rendering.
It is possible to convert from angular velocity to linear velocity. Given an
angular velocity ω, and a point at displacement r from the center of mass, we
can compute the linear velocity at the point by using the equation
v = ω × r
(12.6)
This makes sense if we look at a rotating sphere. If we look at various points
on the sphere (Figure 12.10a), their linear velocity is orthogonal to both the
axis of rotation and their displacement vector, and this corresponds to the
direction of the cross product. The length of v will be
∥v∥= ∥ω∥∥r∥sin θ
where θ is the angle between ω and r. This also makes sense. As the rate of
rotation ∥ω∥increases, we’d expect the linear velocity of each point on the
object to increase. As we move out from the equator, a rotating point has to
move a longer linear distance in order to maintain the same angular velocity
relative to the center (Figure 12.10b), so as ∥r∥increases, ∥v∥will increase.
Finally, the linear velocity of a point as we move from the equator to the poles
will decrease to zero (Figure 12.10c) and the quantity sin θ provides this.
12.4.3 Torque
Up until now we’ve been simplifying our equations by applying forces only
at the center of mass, and therefore generating only linear motion. On the
other hand, if we apply an off-center force to an object, we expect it to spin.

12.4 Rotational Dynamics 601
r
r
r
v
v
v
ω
Figure 12.10a Linear velocity of points on surface of rotating sphere. Velocity
is orthogonal to both angular velocity vector and displacement vector from center of
rotation.
ω
Figure 12.10b Comparison of speed of points on surface of rotating disk. Points
farther from center of rotation have larger linear velocity.
The rotational force created, known as torque, is directly dependent on the
location where the force is applied. The farther away from the center of mass
we apply a given force, the larger the torque. To compute torque, we take the
cross product of the vector from the center of mass to the force application
point, with the corresponding force (Figure 12.11) or
τ = r × F
(12.7)

602
Chapter 12 Rigid Body Dynamics
ω
Figure 12.10c Comparison of speed of points on surface of rotating sphere.
Points closer to equator of sphere have larger linear velocity.
r
F
τ
Figure 12.11 Computing torque. Torque is the cross product of displacement
vector and force vector.
The direction of τ combined with the right-hand rule tells us the direction
of rotation the torque will attempt to induce. If you align your right thumb
along the direction of torque, your curled ﬁngers will indicate the direction
of rotation — if the vector is pointing towards you this is counterclockwise
around the axis of torque. The magnitude of τ provides the magnitude of the
corresponding torque.
To compute the total torque, we need to compute the corresponding
torque for each application of force, and then add them up. Adding the offsets
and taking the cross product of the resulting vector with the total force will
not compute the correct result, as shown by Figure 12.12. The sum of the
offsets is 0, producing a torque of 0, which is clearly not the case — the true
total torque as shown will start the circle rotating counterclockwise.

12.4 Rotational Dynamics 603
r1
r2
F1
F2
Figure 12.12 Adding two torques. If forces and displacements are added sepa-
rately and then the cross product is taken total torque will be 0. Each torque must be
computed and then added together.
12.4.4 Angular Momentum and Inertial Tensor
Recall that a force F is the derivative of the linear momentum P. There is a
related quantity L for torque, such that
τ = dL
dt
Like linear momentum, the angular momentum L describes how much an
object tends to stay in motion, but in rotational motion rather than linear
motion. The higher the angular momentum, the larger the torque needed to
change the object’s angular velocity. Recall that linear momentum is equal
to the mass of the object times its velocity. Angular momentum is similar,
except that we use angular velocity, and the rotational equivalent of mass, the
inertial tensor matrix:
L = J ω
(12.8)
Why use a matrix J instead of a scalar, as we did with mass? The problem
is that while shape has no effect (other than, say, for friction) on the general
equations for linear dynamics, it does have an effect on how objects rotate.
Take the classic example of a ﬁgure skater in a spin. As she starts the spin, her
arms are out from her sides, and she has a low angular velocity. As she brings

604
Chapter 12 Rigid Body Dynamics
her arms in, her angular velocity increases until she opens her arms again to
gracefully pull out of the spin. Torque is near-zero in this case (ignoring some
minimal friction from the ice and air), so we can consider angular momentum
to be constant. Since angular velocity is clearly changing and mass is constant,
the shape of the skater is the only factor that has a direct effect to cause this
change.
So to represent this effect of shape on rotation, we use a 3 × 3 symmetric
matrix, where
J =


Ixx
−Ixy
−Ixz
−Ixy
Iyy
−Iyz
−Ixz
−Iyz
Izz


We need these many factors because, as we’ve said, rotation depends heavily
on shape and each factor describes how the rotation changes around a par-
ticular axis. The diagonal elements are called the moments of inertia. If we’re
in the correct coordinate frame, then the nondiagonal elements, or products
of inertia, are zero. For such a frame, the axes are called the principle axes.
For example, if the object is symmetric, the principle axes lie along the axes
of symmetry and through the center of mass. We’ll see next how to handle the
case if our object is not in the principle axes frame.
The following are some examples of simple inertial tensors for objects
with constant density and mass m:
Sphere (radius of r):


2
5mr2
0
0
0
2
5mr2
0
0
0
2
5mr2


Solid cylinder (main axis aligned along x, radius r, length d):


1
2mr2
0
0
0
1
4mr2 + 1
12md2
0
0
0
1
4mr2 + 1
12md2


Box (xdim × ydim × zdim):


1
12m(y2
dim + z2
dim)
0
0
0
1
12m(x2
dim + z2
dim)
0
0
0
1
12m(x2
dim + y2
dim)


For many purposes, these can be reasonable approximations. If necessary,
it is possible to compute an inertia tensor and center of mass for a generalized

12.4 Rotational Dynamics 605
model, assuming a constant density. An initial description of how do to do
this is provided in [78], while more detail and code optimized for triangular
data can be found in [30].
12.4.5 Integrating Rotational Quantities
Demo
Torque
As with linear dynamics, we use our angular velocity to update to our new
orientation. Ideally, we could use Euler’s method directly and compute our
new orientation as
i+1 = i + hωi
However, this won’t work, mainly because we are trying to combine vector
and matrix quantities. What we need to do is compute a matrix that represents
the derivative and use that with Euler’s method.
Recall that the column vectors of a rotation matrix are three orthonormal
vectors. We need to know how each vector will change with time; that is, we
need the linear velocity at each vector tip. What we want to do is convert
the angular velocity into a linear velocity for each of our basis vectors. We
can apply equation 12.6 to each of our basis vectors to compute this, and
then use the matrix generated to integrate orientation. One way would be
to take the cross product of ω with each column vector, but instead we can
take our three angular velocity values, and create a skew symmetric matrix ˜ω,
where
˜ω =


0
−ω3
ω2
ω3
0
−ω1
−ω2
ω1
0


(12.9)
If we multiply this by our current orientation matrix, this will take the cross
product of ω with each column vector, and we end up with the derivative of
orientation in matrix form. Using this with Euler’s method, we end up with
n+1 = n + h( ˜ωnn)
(12.10)
If we’re using a quaternion representation for orientation, we use a similar
approach. We take our angular velocity vector and convert it to a quaternion
w, where
w = (0, ω)

606
Chapter 12 Rigid Body Dynamics
We can multiply this by one-half of our original quaternion to get the
derivative in quaternion form, giving us, again with Euler’s method,
qn+1 = qn + h
1
2wnqn

(12.11)
A derivation of this equation is provided by Witken and Baraff [119] and
Eberly [30], for those who are interested.
Using either of these methods allows us to integrate orientation. As far as
updating angular velocity, computing acceleration for rotational dynamics is
rather complicated, so we won’t be using angular acceleration at all. Instead,
since torque is the derivative of angular momentum, we’ll integrate the torque
to update angular momentum, and then compute the angular velocity from
that. As when we integrated force, we’ll need a function to compute total
torque across the entire interval, called CurrentTorque(). For both methods,
we’ll have to modify our input variables to take into account orientation and
angular velocity as well as position and velocity.
To ﬁnd the angular velocity, we rewrite equation 12.8 to solve for ω:
ω = J−1L
(12.12)
When computing the angular velocity in this way, there is one detail that
needs to be managed carefully. The inertial tensor is in the local space of
the object. However, angular momentum is integrated from torque, which
is computed in world space, and we want our resulting angular velocity to
also be in world space. To keep things consistent, we need a way to convert
our local J−1 to world space. If we’re using a rotation matrix to represent
orientation, we can use it to transform L from world to local space, apply the
inverse inertial tensor, and then transform back into world space. So, for a
given time step:
ωi+1 = i+1J−1T
i+1Li+1
(12.13)
If we’re using quaternions, the most efﬁcient way to handle this is to convert
our quaternion to a matrix, and then compute equation 12.13.
Using Euler’s method and quaternions, the full code for handling rota-
tional quantities looks like:
// compute new orientation, angular momentum
IvQuat w = IvQuat( 0.0f, mAngVelocity.x,
mAngVelocity.y, mAngVelocity.z );
mRotate += h*0.5f*w*mRotate;
mRotate.Normalize();
mRotate.Clean();

12.5 Collision Response 607
mAngularMomentum += h*CurrentTorque( mTranslate, mVelocity,
mRotate, mAngVelocity);
mAngularMomentum.Clean();
// update angular velocity
IvMatrix33 rotateMat(mRotate);
IvMatrix33 worldMomentsInverse =
rotateMat*mMomentsInverse*::Transpose(rotateMat);
mAngularVelocity = worldMomentsInverse*mAngularMomentum;
mAngularVelocity.Clean();
12.5 Collision Response
Up to this point, we haven’t considered collisions. Our objects are moving
gracefully through the world, speeding up or slowing down as we adjust our
forces. All of which is accurately modeled, except that the objects go right
through each other. Not a very realistic or fun game. Instead, we’ll need a
way to simulate the two objects bouncing away from each other due to the
collision. We can do so by using the methods we’ve discussed in the previous
chapter in combination with some new techniques.
12.5.1 Locating the Point of Collision
For the purposes of this discussion, we’ll assume a simple collision model,
where the objects are mostly convex and there aren’t multiple collision points.
To perform our collision response properly, we have to know two things about
the collision. The ﬁrst is the exact point of collision between the two objects
A and B — in other words, the point on the objects where they just touch
(Figure 12.13). Since the two objects are just touching, there is a tangent
plane which passes between the two, which also intersects both at that point.
This is represented in the ﬁgure as a line. The second thing we need to know
is the normal ˆn to that plane. We’ll choose our normal to point from A, the
ﬁrst object; to B, the second.
Our main problem in ﬁguring out collision location is that we’re trying to
detect collisions within an interval of time. In one time step, two objects may
be completely separate; in the next, they are colliding. In fact, in most cases
when collision is detected, we have missed the initial point of collision and
the objects are already interpenetrating (Figure 12.14). Because of this, there
is no single point of collision.
One possibility for ﬁnding the exact point when initial collision occurs
is to do a binary search within the time interval. We begin by running our

608
Chapter 12 Rigid Body Dynamics
A
B
Figure 12.13 Point of collision. At the moment of impact between two convex
objects, there is a single point of collision. Also shown is the collision plane and its
normal.
Figure 12.14 Interpenetrating objects. There is no single point of collision.
simulation, and then testing for collisions. If we ﬁnd one, and the two objects
involved are interpenetrating, we start our binary search:
dt = h/2
diff = h/4
while (dt > VerySmallNumber)
{
Integrate from current time to current time+dt
if just touching
break

12.5 Collision Response 609
else if intersecting
dt -= diff
else
dt += diff
diff /= 2
}
At the end of the search, we’ll either have found the exact collision point or
will be reasonably close.
This technique has a few ﬂaws. First of all, it’s slow. Chances are that
every time you get a collision, you’ll need to run the simulation at least two
or three additional times to get a point where the objects are just touching. In
addition, in order for detection to be perfectly accurate, you need to rerun the
simulation for all the objects, because their position at the time of the colli-
sion will be slightly different than their position at the end of the time interval.
This may affect which objects are colliding. So you need to run the simula-
tion back, determine the collision point, apply the collision response, and
then run the simulation forward until you hit another collision, do another
binary search, and so on. In the worst case, with many colliding objects, your
simulation will get bogged down, and you’ll end up with long frame times.
The accuracy of this method may be suitable for ofﬂine simulation, but it’s
not good for interactivity.
Another possibility is to ignore it, approximate the collision location and
normal, and let the collision response push the two objects apart. This can
work, but if the response is too slow, the two objects may remain interpen-
etrated for a while. This looks quite odd and ruins the illusion of reality
(Figure 12.15).
The third alternative begins by looking at the overlap between the two
objects. The longest distance along that overlap is known as the penetration
distance. We can push the two objects apart by the penetration distance until
they just touch, and then use the point and normal from that intersection for
collision calculations.
For example, take two spheres (Figure 12.16), with centers Ca and Cb, and
radii ra and rb. If we subtract one center Ca from the other center Cb, we get
the direction for our collision normal. The penetration distance p is then the
sum of the two radii minus the length of this vector, or
p = (ra + rb) −∥Cb −Ca∥
(12.14)
We can move each sphere in opposite directions along this normal by the
distance p/2, which will move them to a position where they just touch. This
assumes that both objects can move — if one is not expected to move, like a

610
Chapter 12 Rigid Body Dynamics
Figure 12.15 Allowing collision response to separate objects over time.
ra
rb
Ca
Cb
Figure 12.16 Determining penetration distance and collision normal.

12.5 Collision Response 611
boulder or a church, we translate the other object by the entire normal length.
So for two moving objects A and B, the formula is
mTranslate -= 0.5f*penetration*centerDiff;
other->mTranslate += 0.5f*penetration*centerDiff;
Once we’ve pushed them apart, the collision point is where our center
difference vector crosses the boundary of the two spheres. We can compute
this point by halving the difference vector and adding it to the old Ca. We ﬁnish
up by normalizing the difference vector to get our collision normal.
Handling penetration distance for capsules is just as simple. Instead of
using the center points to compute the collision normal, we use the closest
points on the line segments that deﬁne each capsule. The penetration distance
becomes the sum of the radii minus the distance between these points. For
bounding boxes, Eberly [27] provides a method that computes the penetration
distance between two oriented boxes.
This technique does have some ﬂaws. First, pushing the two objects apart
by the entire penetration distance may look too abrupt. Instead, we can push
them apart by a fraction of the penetration distance and assume that the
collision response will separate them the rest of the way. The slight interpen-
etration will only be noticeable for one or two frames. Second, if objects are
moving fast enough and the collision is detected too late, the two objects may
pass through each other. If this case is not handled in the collision detection,
we will get some very odd results when the objects are pushed apart. Finally,
because we’re pushing objects away from each other instantaneously, we may
end up with situations where two objects collide, and one of them is moved
into a third, causing a new interpenetration. Because we may have already
tested for collision between the second pair of objects, we’ll miss this colli-
sion. If we’re expecting a large number of collisions between close objects,
this system may not be practical.
12.5.2 Linear Collision Response
Demo
LinCollision
Whatever method we use, we now have two of the properties of the collision we
need to compute the linear part of our collision response: a collision normal
ˆn and a collision point P. The other two elements are the incoming velocities
of the two objects, va and vb. Using this information, we are ﬁnally ready to
compute our collision response.
The technique we’ll use is known as an impulse-based system. The idea is
that near the time of collision, the forces and position remain nearly constant,
but there is a discontinuity in the velocity. At one point in time, the velocities
of the objects are heading towards one another — in the next inﬁnitesimal

612
Chapter 12 Rigid Body Dynamics
Figure 12.17 Instantaneous change in velocity at time of collision.
moment later, they are heading away (Figure 12.17). How much and in what
relation the velocities change depends on the magnitude and direction of the
incoming velocities, the direction of the collision normal, and the masses of
the two objects.
Let’s look again at the simple case of our two spheres A and B
(Figure 12.18a). For now, let’s assume their masses are equal. We again see
our two incoming velocities va and vb, and our collision normal ˆn. The idea
is that we want to modify our velocity by an impulse velocity normal to the
point of collision. The impulse will act to push the two objects apart — if the
masses are equal, it will be equal in magnitude, but opposite in direction for
each object. So we need to generate a scale factor j for our collision normal,
and then add the scaled collision normal j ˆn and −j ˆn to va and vb to get our
outgoing velocities. So in order to compute the impulse vector, we need to
compute this factor j.
To begin our computation, we need the relative velocity vab, which is
just va −vb (Figure 12.18a). From that, we’ll compute the amount of relative
velocity that is applied along the collision normal (Figure 12.18b). Recall that
the dot product of any vector with a normalized vector gives the projection

12.5 Collision Response 613
vab
va
vb
n
A
B
Figure 12.18a Computing collision response. Calculating relative velocity.
vab
va
vb
n
A
B
vn
Figure 12.18b Collision response. Computing relative velocity along normal.
along the normal vector, which is just what we want. So
vn = ( vab · ˆn)ˆn
At this point, we do one more test to see if we actually need to calculate an
impulse vector. If the relative velocity along the collision normal is negative,
then the two objects are heading away from each other and we don’t need

614
Chapter 12 Rigid Body Dynamics
to compute an impulse. We can break out of the collision response code and
proceed to the next collision. Otherwise, we continue with computing j.
In order to compute a proper impulse, two conditions need to be met. First
of all, we need to set the ratio of the outgoing velocity along the collision nor-
mal to the incoming velocity. We do this by using a coefﬁcient of restitution ϵ:
v′
n = −ϵ vn
or
( v′
a −v′
b) · ˆn = −ϵ( va −vb) · ˆn
(12.15)
This simulates two different physical properties. First of all, when two
objects collide some energy is lost, usually in the form of heat. Second, if
two objects are somewhat soft and/or sticky, or nonelastic, the bonding forces
between the objects will decrease the outgoing velocities. Elastic in this case
doesn’t refer to the stretchiness of the object, but how resilient it is. A superball
is relatively hard, but has very elastic collisions. So the quantity ϵ represents
how much energy is lost and how elastic the collision between the two objects
is. If ϵ is 1, then the two objects will bounce away from each other with the
same relative velocities they had coming in. If ϵ is 0, they will stick together like
two clay balls and move as one. Values in between will give a linear range of
elastic responsiveness. Values greater than 1, or less than 0, are not permitted.
An ϵ greater than 1 would add energy into the system, so a ball bouncing on
a ﬂat surface would bounce progressively higher and higher. An ϵ less than 0
means that the objects would be highly attracted to each other upon collision
and would lead to undesirable interpenetrations.
Even if energy is not quite conserved (technically it is, but we’re not track-
ing the heat loss), then momentum is. Because of this, the total momentum
of the system of objects before and after the collision needs to be equal. So
ma va + j ˆn = mav′
a
or
v′
a = va + j
ma
ˆn
(12.16)
Similarly,
mb vb −j ˆn = mb v′
b
or
v′
b = vb −j
mb
ˆn
(12.17)

12.5 Collision Response 615
v'a
va
v'b
n
A
B
j/man
vb
–j/mbn
Figure 12.18c Collision response. Adding impulses to create outgoing velocities.
With this, we ﬁnally have all the pieces that we need. If we substitute
equations 12.16 and 12.17 into equation 12.15 and solve for j, we get the ﬁnal
impulse factor equation
j = −(1 + ϵ) vab · ˆn
ˆn · ˆn

1
ma +
1
mb

(12.18)
Now that we have our impulse value, we substitute this back into equa-
tions 12.6 and 12.17 to get our outgoing velocities (Figure 12.18c). Note the
effect of mass on the outgoing velocities. As we expect, as the mass of an
object grows larger, it grows more resistant to changing its velocity due to an
incoming object. This is counteracted by j, which grows as relative velocity
increases, or as the combined masses increase.
Our ﬁnal algorithm for collision response between two spheres is as
follows:
float radiusSum = mRadius + other->mRadius;
collisionNormal = other->mTranslate - mTranslate;
float distancesq = collisionNormal.LengthSquared();
// if distance squared < sum of radii squared, collision!
if ( distancesq <= radiusSum*radiusSum )
{
// handle collision
// penetration is distance - radii
float distance = ::IvSqrt(distancesq);
penetration = radiusSum - distance;

616
Chapter 12 Rigid Body Dynamics
collisionNormal.Normalize();
// collision point is average of penetration
collisionPoint = 0.5f*(mTranslate + mRadius*collisionNormal)
+ 0.5f*(other->mTranslate - other->mRadius*collisionNormal);
// push out by penetration
mTranslate -= 0.5f*penetration*collisionNormal;
other->mTranslate += 0.5f*penetration*collisionNormal;
// compute relative velocity
IvVector3 relativeVelocity = mVelocity - other->mVelocity;
float vDotN = relativeVelocity*collisionNormal;
if (vDotN < 0)
return;
// compute impulse factor
float numerator = -(1.0f+mElasticity)*vDotN;
float denominator = (collisionNormal*collisionNormal);
denominator *= (1.0f/mMass + 1.0f/other->mMass);
float j = numerator/denominator;
// update velocities
mVelocity += j/mMass*collisionNormal;
other->mVelocity -= j/other->mMass*collisionNormal;
}
In this simple example, we have interleaved the sphere collision detection
with the computation of the collision point and normal. This is for efﬁciency’s
sake, since both use the sum of the two radii and the difference vector between
the two centers for their computations. In a more complex collision system
it is usually better to separate intersection detection from calculation of col-
lision parameters. This is particularly true with hierarchical systems, where
we may encounter many intersections between the bounding hierarchies of
two objects. Only when we determine actual collision between leaf nodes do
we calculate the collision normal and penetration distance.
12.5.3 Rotational Collision Response
Demo
RotCollision
This is all well and good, but most objects are not spheres, which means that
they have a visible orientation. When one collides with another at an offset to
the center of mass, we would expect some change in angular velocity as well
as linear velocity. In addition, any incoming angular velocity should affect

12.5 Collision Response 617
the collision as well. A cue ball with spin (or English) applied causes a much
different effect on a target pool ball than a cue ball with no spin.
As with linear and rotational dynamics, the way we handle rotational
collision response is very similar to how we handle linear collision response.
We need to modify only a few equations and recalculate our impulse factor j.
One modiﬁcation we have to make is the effect of angular velocity on the
incoming velocity. Up to this point, we’ve assumed that when the two objects
strike each other, their surfaces are not moving, so the velocity at the collision
point is simply the linear velocity. However, if one or both of the objects are
rotating, then there is an additional velocity factor applied at the point of
collision, as one surface passes by the other. Recall that equation 12.6 allows
us to take an angular velocity ω and a displacement from the center of rotation
r and compute the linear velocity contributed by the angular velocity at the
point of displacement. Adding this to the original incoming velocities, we get
¯va = va + ωa × ra
¯vb = vb + ωb × rb
So now the relative velocity vab at the collision point becomes
vab = ¯va −¯vb
and equation 12.15 becomes
(¯v′
a −¯v′
b) = −ϵ(¯va −¯vb)
(12.19)
The other change needed is that in addition to handling linear momentum,
we also need to conserve angular momentum. This is a bit more complex
compared to the equations for linear motion, but the general concept is the
same. The outgoing angular momentum should equal the sum of the incoming
angular momentum and any momentum imparted by the collision. For object
A, this is represented by
Iaωa + ra × j ˆn = Iaω′
a
(12.20)
or
ω′
a = ωa + I−1
a ( ra × j ˆn)
(12.21)
For object B, this is
Ibωb −rb × j ˆn = Ibω′
b
(12.22)

618
Chapter 12 Rigid Body Dynamics
or
ω′
b = ωb −I−1
a ( rb × j ˆn)
(12.23)
Just as with linear collision response, we can substitute equations 12.21
and 12.23 into 12.19, and solve for j to get
j =
−(1 + ϵ) vab · ˆn
ˆn · ˆn

1
ma +
1
mb

+

( I−1
a ( ra × ˆn)) × ra + ( I−1
b ( rb × ˆn)) × rb

(12.24)
Using this j we calculate new angular momenta using equations 12.20
and 12.22 and from that calculate angular velocity as we did with angular
dynamics, using equation 12.8. We use this same j for our linear collision
response as well.
We change our linear collision handling code in three places to achieve
this. First of all the relative velocity collision incorporates incoming angular
velocity:
// compute relative velocity
IvVector3 r1 = collisionPoint - mTranslate;
IvVector3 r2 = collisionPoint - other->mTranslate;
IvVector3 vel1 = mVelocity + Cross( mAngularVelocity, r1 );
IvVector3 vel2 = other->mVelocity + Cross( other->mAngularVelocity, r2 );
IvVector3 relativeVelocity = vel1 - vel2;
Then we add angular factors to our calculation for j:
// compute impulse factor
float numerator = -(1.0f+mElasticity)*vDotN;
float denominator = (1.0f/mMass
+ 1.0f/other->mMass)*(collisionNormal.Dot(collisionNormal));
// compute angular factors
IvVector3 cross1 = Cross(r1, collisionNormal);
IvVector3 cross2 = Cross(r2, collisionNormal);
cross1 = mWorldMomentsInverse*cross1;
cross2 = other->mWorldMomentsInverse*cross2;
IvVector3 sum = Cross(cross1, r1) + Cross(cross2, r2);
denominator += (sum.Dot(collisionNormal));

12.5 Collision Response 619
Finally, in addition to linear velocity, we recalculate angular velocity:
// update angular velocities
mAngularMomentum += Cross(r1, collisionNormal);
mAngularVelocity = mWorldMomentsInverse*mAngularMomentum;
other->mAngularMomentum -= Cross(r2, collisionNormal);
other->mAngularVelocity = other->mWorldMomentsInverse*other->mAngularMomentum;
12.5.4 Other Response Techniques
There are some other techniques that have been used for collision response,
with mixed results. The ﬁrst is called the penalty method. Instead of gener-
ating an instantaneous change in velocity at a collision, the penalty method
uses spring forces to push the objects away from each other. The more the
two objects are interpenetrated, the larger the force. The problem with this
method is that if you use small forces to avoid problems with stiff systems,
your collisions look rather soft, and objects stay interpenetrated too long.
And if you increase the forces to avoid the soft collisions, you end up with
stiff systems and have to use implicit methods to solve your equations.
A constraint system is another technique which uses forces. Suppose we
have a collection of particles, and want to keep each of them a ﬁxed distance
away from their neighbors, say in a grid (Figure 12.19). After any other force
calculations are done, the constraint system analyzes the forces and velocities
applied to each particle and computes exact forces to maintain the distance
between the particles. Similar calculations can be done to keep particles on a
wire or three particles at a relative angle. Constraint systems are very good for
modeling chains, rope, cloth, or dead bodies. The downside is that in order
to compute the exact forces, you have to solve large but sparse systems of
Figure 12.19 Mesh of particles constrained by distance.

620
Chapter 12 Rigid Body Dynamics
linear equations. Also, constraint forces have trade-offs that are similar to
those for penalty methods, in that you either end up with a stiff system or
rather spongy simulations. Details for building a constraint system can be
found in [119], [65], and [30].
12.6 Efficiency
Now that we have a simple simulation system, some notes on using it efﬁ-
ciently may be appropriate. The ﬁrst rule is that this is a game. Don’t waste
time with any more processing power than you need to get the effect you want.
While a fully realistic simulation may be desirable, it can’t take too much pro-
cessing power away from the other subsystems, for instance, graphics or AI.
How resources are allocated among subsystems in a game depends on the
game’s focus. If a simpler solution will come close enough to the appearance
of realism, then it is sometimes better to use that instead.
One way to reduce the amount of resources used is to simplify the prob-
lem. So far we’ve been assuming that we’re building a truly 3D game, where
the objects need to move in three degrees of freedom. If, however, you were
building a tank game, it’s highly unlikely that the tank would leave the ground.
In most cases, land warfare games take place on a 2D map, with some height
variation, so with the exception of projectiles the entire situation is really a 2D
problem. You don’t have to consider gravity, angular dynamics is constrained
to just rotation around z, and thus you really need only one factor for your
moments of inertia. This considerably simpliﬁes the angular dynamics equa-
tions. The same is true for a ﬁrst-person shooter; in general, characters will
interact as cylinders sliding on a ﬂat ﬂoor, with vertical walls as boundaries.
In this case, we can simplify the collision problem to circles on a 2D plane.
Another way to improve efﬁciency is to run simulation code only on some
of the objects in the world. For example, we could restrict full simulation to
those objects that are visible or near the player. We could use a simpliﬁed
simulation model for the other objects or not move them at all. We could also
not simulate objects that aren’t currently moving, and begin simulation only
when forces are applied or another object collides with them. When using
this technique, we need to be careful about discontinuities in the simulation.
We don’t want a falling object that passes out of view to stop in midair, only
to start falling again when it’s visible again. Nor do we want objects to jerk,
move strangely, or jump position as one simulation model ceases and another
takes over. While managing these discontinuities can be tricky, using such
restrictions can also gain quite a performance boost.
Simplifying the forces computed during simulation is another place to
ﬁnd speed improvements. We’ve alluded to this before. In a truly complete
simulation we would compute a gravitational force, a normal force to keep

12.7 Chapter Summary 621
the object from sinking through the ground, and a static frictional force to
keep the object from sliding down any inclines. In most cases we can assume
that the sum of all these forces is zero and ignore them completely. Friction
is a similar case. We could compute a complex equation for an object that
handles all contact points, current surface area, and whether we are moving
or at rest — or we could just use a drag coefﬁcient multiplied by velocity. If
your game calls for the full friction model, then by all means do it, but in
many cases it is overkill.
12.7 Chapter Summary
The use of physical simulation is becoming an important part of providing
realistic motion in games and other interactive applications. In this chap-
ter, we have described a simple physical simulation system, using basic
Newtonian physics. We covered some techniques of numeric integration,
starting with Euler’s method, and discussed their pros and cons. Using these
integration techniques, we have created a simple system for linear and rota-
tional rigid body dynamics. Finally, we have shown how we can use the
results of our collision system to generate impulses for collision response.
The system we’ve presented is a very simple one — we’ve barely scratched
the surface of what is possible in terms of physical simulation. For those
who are interested in proceeding further, Eberly [30] presents a more com-
plete look at game physics, including the use of physics in graphics shaders.
Burden and Faires [17] and Golub and Ortega [47] have more description
of numerical integration techniques and managing error bounds. Finally,
Witken and Baraff [119] and Jakobson [65] describe different methods for
building constraint systems, useful for soft-body simulations such as cloth
and rag doll.

AppendixA
Trigonometry
Review
A.1 Basic Definitions
A.1.1 Ratios on the Right Triangle
The trigonometric functions sine, cosine, and tangent are based on ratios of
the sides of a right triangle, relative to one acute angle θ (Figure A.1):
sin θ = opp/hyp
cos θ = adj/hyp
tan θ = opp/adj = sin θ/ cos θ
We also deﬁne the reciprocal functions secant, cosecant, and cotangent
as follows:
sec θ = hyp/adj = 1/ cos θ
csc θ = hyp/opp = 1/ sin θ
cot θ = adj/opp = 1/ tan θ
= cos θ/ sin θ = sec θ/csc θ
623

624
Appendix A Trigonometry Review
adj
hyp
opp
θ
Figure A.1 Computing trigonometric functions on the right triangle.
A.1.2 Extending to General Angles
Consider a standard Cartesian frame for R2. We place a line segment, or
radius, with length r and one endpoint ﬁxed at the origin. The other endpoint
is located at a point (x, y). We deﬁne θ as the angle between the radius and
the positive x-axis. The angle is positive if the direction of rotation from the
x-axis to the radius is counterclockwise, negative if clockwise. A full rotation
is broken into 2π radians, or 360 degrees. The coordinate axes divide the plane
into four quadrants: they are numbered in the order of rotation. Within this
we can inscribe a right triangle, with the radius as hypotenuse and one side
incident with the x-axis (Figure A.2).
y
x
θ
r
Quadrant 2
(x,y)
Quadrant 1
Quadrant 3
Quadrant 4
Figure A.2 Computing trigonometric functions on the standard Cartesian frame,
showing the four ordered quadrants.

A.1 Basic Definitions 625
We can represent the sine and cosine based on the length r of the radius
and the location (x, y) of the free endpoint:
sin θ = y/r
cos θ = x/r
In this case the tangent becomes the slope of the radius:
tan θ = y/x
For angles greater than π/2, the magnitude of the result is the same, but
the sign may be negative depending on which quadrant the angle is in:
Functions
Quadrant
Sign
sin, csc
1,2
+
3,4
−
cos, sec
1,4
+
2,3
−
tan, cot
1,3
+
2,4
−
The tangent, cotangent, secant, and cosecant all involve divisions by x or
y, which may be 0. This leads to singularities at those locations, which can
be seen in the function graphs in Figures A.3 through A.8. This sequence of
ﬁgures shows the six trigonometric functions graphed against θ (in radians).
Also note that these functions are periodic. For example, sin(0) = sin(2π) =
sin(−4π). In general, sin(x) = sin(n·2π +x), for any integer n. The same is true
- 1
- 0.5
0.5
1.
–3π/2
–π/2
π/2
3π/2
Sin (h)
h
Figure A.3 Graph of sin θ.

626
Appendix A Trigonometry Review
-1
-0.5
0.5
1
Cos (h)
h
-π/2
π/2
-3π/2
3π/2
Figure A.4 Graph of cos θ.
-10
-5
5
10
-3π/2
3π/2
-π/2
π/2
Tan(h)
h
Figure A.5 Graph of tan θ.
for cosine, secant, and cosecant. Tangent and cotangent are periodic with
period π: tan(x) = tan(n · π + x).
A.2 Properties of Triangles
There are three laws that relate angles in a triangle to sides of a triangle,
using trigonometric functions. Figure A.9 shows a general triangle with sides
of length a, b, and c, and corresponding opposite angles α, β, and γ .

A.2 Properties of Triangles 627
-10
-5
5
10
-3π/2
3π/2
-π/2
π/2
Cot(h)
h
Figure A.6 Graph of cot θ.
-10
-5
5
10
-3π/2
3π/2
-π/2
π/2
Sec (h)
h
Figure A.7 Graph of sec θ.
The law of sines relates angles to their opposing sides as a constant ratio
for each pair:
sin α
a
= sin β
b
= sin γ
c
(A.1)
Recall the Pythagorean theorem:
c2 = a2 + b2

628
Appendix A Trigonometry Review
-10
-5
5
10
-3π/2
3π/2
-π/2
π/2
Csc (h)
h
Figure A.8 Graph of csc θ.
b
a
α
c
γ
β
Figure A.9 General triangle, with sides and angles labeled.
which relates two sides of a right triangle to the hypotenuse. The law of cosines
is an extension to this, which can be used to compute the length of a side from
the length of two other sides and the angle between them:
c2 = a2 + b2 −2ab cos γ
(A.2)
Substituting π/2 for γ produces the speciﬁc case of the Pythagorean theorem.
The law of tangents relates two angles and their corresponding opposite
sides:
a −b
a + b = tan( 1
2(α −β))
tan( 1
2(α + β))
(A.3)

A.3 Trigonometric Identities 629
All of these can be used to construct information about a triangle from
partial data.
While not speciﬁcally one of the laws, a related set of formulas computes
the area of a triangle:
ab sin γ
2
= bc sin α
2
= ac sin β
2
(A.4)
A.3 Trigonometric Identities
A.3.1 Pythagorean Identities
Again, from the Pythagorean theorem we know that
a2 + b2 = c2
where c is the length of the hypotenuse and a and b are the lengths of the other
two sides. In the case where the length of the hypotenuse is 1, the length of
the other two sides are cos θ and sin θ, so
sin2 θ + cos2 θ = 1
(A.5)
where sin2 θ = (sin θ)(sin θ), and similarly for cos2 θ.
Dividing equation A.5 through by cos2 θ:
sin2 θ
cos2 θ + cos2 θ
cos2 θ =
1
cos2 θ
tan2 θ + 1 = sec2θ
(A.6)
If we instead divide equation A.5 by sin2 θ:
sin2 θ
sin2 θ
+ cos2 θ
sin2 θ
=
1
sin2 θ
cot2θ + 1 = csc2θ
A.3.2 Complementary Angle
If we consider one acute angle θ in a right triangle, the other acute angle
is its complement π
2 −θ. We can compute trigonometric functions for the

630
Appendix A Trigonometry Review
complementary angle by changing the sides we use when computing the
ratios, for example,
sin
π
2 −θ

= adj/hyp = cos(θ)
The complementary angle identities are
cos θ = sin
π
2 −θ

(A.7)
sin θ = cos
π
2 −θ

(A.8)
cot θ = tan
π
2 −θ

(A.9)
tan θ = cot
π
2 −θ

(A.10)
csc θ = sec
π
2 −θ

(A.11)
sec θ = csc
π
2 −θ

(A.12)
A.3.3 Even-Odd
Two of the trigonometric functions, cosine and secant, are symmetric across
θ = 0 and are called even functions:
cos(−θ) = cos θ
(A.13)
sec (−θ) = sec θ
(A.14)
The remainder are antisymmetric across θ = 0 and are called odd func-
tions:
sin(−θ) = −sin θ
(A.15)
csc(−θ) = −csc θ
(A.16)
tan(−θ) = −tan θ
(A.17)
cot(−θ) = −cot θ
(A.18)

A.3 Trigonometric Identities 631
A.3.4 Compound Angle
For two angles α and β, the sines of the sum and difference of the angles are,
respectively,
sin(α + β) = sin α cos β + cos α sin β
(A.19)
sin(α −β) = sin α cos β −cos α sin β
(A.20)
Similarly, the cosines of the sum and difference of the angles are
cos(α + β) = cos α cos β −sin α sin β
(A.21)
cos(α −β) = cos α cos β + sin α sin β
(A.22)
These can be combined to create the compound angle formulas for the
tangent:
tan(α + β) = tan α + tan β
1 −tan α tan β
(A.23)
tan(α −β) = tan α −tan β
1 + tan α tan β
(A.24)
A.3.5 Double Angle
If we substitute the same angle θ for both α and β into the compound angle
identities, we get the double angle identities:
sin 2θ = 2 sin θ cos θ
(A.25)
cos 2θ = cos2 θ −sin2 θ
(A.26)
The latter can be rewritten using the Pythagorean identity as
cos 2θ = 1 −2 sin2 θ
(A.27)
= 2 cos2 θ −1
(A.28)
The double angle identity for tangent is
tan 2θ =
2 tan θ
1 −tan2 θ
(A.29)

632
Appendix A Trigonometry Review
A.3.6 Half Angle
Equations A.27 and A.28 can be rewritten as
sin2 α = 1 −cos 2α
2
(A.30)
cos2 α = 1 + cos 2α
2
(A.31)
Substituting θ/2 for α and taking the square roots gives
sin
θ
2

= ±
.
1 −cos θ
2
(A.32)
cos
θ
2

= ±
.
1 + cos θ
2
(A.33)
-1
1
-3π/2
-π/2
π/2
3π/2
h
Sin-1(h)
Figure A.10 Graph of arcsin θ.

A.4 Inverses 633
Note that due to the square root, there are two choices for each identity,
positive and negative — the one chosen depends on what quadrant θ/2 is in.
A.4 Inverses
The trigonometric functions invert to multivalued functions because they are
periodic. For example, the graph of the inverse sin−1 θ, or arcsine can be seen
in Figure A.10. Its domain is the interval [−1, 1] and its range is R.
Because of this, it is common to restrict the range of an inverse trigno-
metric function so that it maps only to one value, given a value in the domain.
Standard choices for these restrictions are as follows:
Function
Domain
Range
sin−1
[−1, 1]
[−π/2, π/2]
cos−1
[−1, 1]
[0, π]
tan−1
R
[−π/2, π/2]

AppendixB
Calculus Review
B.1 Limits and Continuity
B.1.1 Limits
The expression
L = lim
x→a f (x)
is read as, L is the limit of a function f as x approaches a given value a.
Informally, this represents that as x gets closer to a, f (x) will get closer to L.
We can more formally represent the notion of “closeness” to L and a by using
the following deﬁnition:
A function f (x) has a limit L at a if given any ϵ > 0 there exists
δ > 0 such that |f (x) −L| < ϵ when 0 < |x −a| < δ.
In other words, for each value of ϵ larger than zero, f (x) is less than ϵ away
from L for all x sufﬁciently close to a. The value of δ provides a measure of
what “sufﬁciently close” means.
In many cases, the limit is just the value of the function at a. For example,
if we have the function
f (x) = x2
(B.1)
635

636
Appendix B Calculus Review
then
lim
x→a f (x) = lim
x→a x2 = a2 = f (a)
for all values of a. However, consider:
g(x) = x2 −1
x −1
(B.2)
At x = 1, the value of g(x) is undeﬁned since the resulting denominator is 0.
But if we graph g, as in Figure B.1, it appears that as we get close to 1
the function value gets close to 2. As it happens, 2 is the limit of g(x) as x
approaches 1. In this case we can say that while at x = 1, the function value
is undeﬁned, however:
lim
x→1
x2 −1
x −1 = 2
Note that there may not necessarily be a limit at a given a. For example,
as graphed in Figure B.2, the step function:
h(x) =
/ 1
x ≥0
−1
x < 0
(B.3)
has no limit at 0. In this case we can talk about a right-hand limit (approach-
ing only from the positive direction) or left-hand limit (approaching from
(x2 − 1)/(x − 1)
x
−1
−0.5
0.5
1
1.5
2
0.5
1
1.5
2
2.5
3
Figure B.1 Part of function with discontinuity but valid limit at x = 1.

B.1 Limits and Continuity 637
−6
−5
−4
−3
−2
−1
1
2
3
4
5
6
−6
−4
−2
2
4
6
Sign(x)
x
Figure B.2 Function with discontinuity and no two-sided limit at x = 0.
the negative direction) or, respectively,
lim
x→0+ h(x) = 1
lim
x→0−h(x) = −1
B.1.2 Continuity
There are three possibilities with regard to the limit of a function f (x) as x
approaches a:
1. limx→a f (x) exists and equals f (a) (e.g., equation B.1)
2. limx→a f (x) exists and does not equal f (a) (e.g., equation B.2)
3. limx→a f (x) does not exist (e.g., equation B.3)
In the ﬁrst case, we say that f is continuous at a. Otherwise, it is disconti-
nuous at a.
We also say that a function f (x) is continuous over an interval (a, b) (or
[a, b]) if it is continuous for every value x in the interval. Informally, we can
think of a continuous function as one that we can draw without ever lifting
the pen from the page.

638
Appendix B Calculus Review
B.2 Derivatives
B.2.1 Definition
Suppose we have a function f (x). If we take two points on the curve at time
x and time x + h, then we can compute the slope of the secant that passes
through the points by the function
f (x + h) −f (x)
h
(B.4)
As the value of h approaches 0, the limit (if it exists) approaches the slope of
a line tangent to the function at the point x. We can use this to create a new
function of x, which computes slopes of f (x) for every value of x where the
limit exists:
f ′(x) = lim
h→0
f (x + h) −f (x)
h
(B.5)
This function is called the ﬁrst derivative, or simply the derivative, which we
have represented as f ′(x). Other common representations are df/dx (also
known as Leibnitz notation), or when taken with respect to time we place
a dot over the function, as ˙f (t).
The derivative f ′(x) describes the instantaneous rate of change of f(x)
at the value x. If f ′(x) is positive, f(x) is said to be increasing at that
point. Correspondingly, if f ′(x) is negative, f(x) is said to be decreasing. The
magnitude of f ′(x) describes how great the rate of change is.
A derivative may not necessarily exist for every value in the domain of
a function. If this is the case for a particular value x, we say the function is
not differentiable at x. If a function is discontinuous, it is not differentiable
at the discontinuity. However, even if it is continuous, it may not be possible.
For example, Figure B.3, the absolute value function:
|x| =

x;
x ≥0
−x;
x < 0
has no derivative at x = 0. This discontinuity represents a sudden change
in slope, or if our function represents a path in space, a sudden change in
direction.
A function f is differentiable on an open interval (a, b) if it is differentiable
at each point in (a, b). It is differentiable on a closed interval [a, b] if it is

B.2 Derivatives 639
−6
−5
−4
−3
−2
−1
1
2
3
4
5
6
−6
−4
−2
2
4
6
x
x
Figure B.3 Function that is continuous but has discontinuity in its ﬁrst derivative
at x = 0.
differentiable on (a, b) and the limits
lim
h→0+
f (a + h) −f (a)
h
and
lim
h→0−
f (b + h) −f (b)
h
exist. If either limit exists at a point x, then we say that f has a one-sided
derivative at x. For example, the absolute value function is differentiable
on the intervals [c, 0) and (0, d], where c < 0 and d > 0, despite not being
differentiable at 0.
Since the derivative is itself a function, assuming it is differentiable we
can take its derivative to get the second derivative, represented by f ′′(x). If
the second derivative is positive, it represents a part of the function which
is concave-up (the cross section of a bowl). If it is negative, that part of the
function is concave-down (an arch). If the ﬁrst derivative is continuous but
there is a discontinuity in the second derivative, then this represents a sudden
change in concavity.
So long as a function and its subsequent derivatives are differentiable, we
can continue this process of taking the derivative of derivatives. In general,
the nth derivative of a function f at x is represented as f (n)(x), and if such
a derivative exists, we say that f is differentiable to order n. If we can keep
differentiating in perpetuity, we say that f is inﬁnitely differentiable.

640
Appendix B Calculus Review
B.2.2 Basic Derivatives
Power of a Variable
The derivative for the power of a variable x, or f (x) = xk is
f ′(x) = kxk−1
(B.6)
By this, the derivative for a linear function g(x) = x is just
g′(x) = 1 · x0 = 1
The derivative of a constant term f (x) = a is
f ′(x) = 0
Arithmetic Operations on Functions
The derivative of the sum of two functions is the sum of the derivatives:
d
dx (f (x) + g(x)) = f ′(x) + g′(x)
(B.7)
The derivative of the difference of two functions is the difference of the
derivatives:
d
dx (f (x) −g(x)) = f ′(x) −g′(x)
(B.8)
The derivative of the product of two functions is
d
dx (f (x)g(x)) = f ′(x)g(x) + g′(x)f (x)
(B.9)
The derivative of the quotient of two functions is
d
dx
f (x)
g(x)

= f ′(x)g(x) −g′(x)f (x)
g(x)2
(B.10)
Composite Functions
If we have the composite of two functions
h(x) = f (g(x)) = (f ◦g)(x)

B.2 Derivatives 641
then the derivative is found by using the chain rule. We take the derivative of
f with respect to the function g, and multiply that by the derivative of g with
respect to the variable x, or
h′(x) = f ′(g(x))g′(x)
(B.11)
For example, suppose we have
h(x) = (2x2 + 1)5
We change variables to set f (u)
=
u5 and g(x)
=
2x2 + 1, so that
h(x) = f (g(x)). Then
h′(x) = f ′(g(x))g′(x) = 5(2x2 + 1)4 · 4x
= 20x(2x2 + 1)4
General Polynomials
If we have a general polynomial
f (x) =
n

i=0
aixi
we can combine equations B.6, B.7, and B.9 to ﬁnd its resulting derivative:
f ′(x) =
n

i=0
aiixi−1
B.2.3 Derivatives of Transcendental
Functions
Trigonometric Functions
The derivatives of the standard trigonometric functions are
d
dx sin x = cos x
d
dx cos x = −sin x

642
Appendix B Calculus Review
d
dx tan x = sec2x = 1 + tan2 x
d
dx cot x = −csc2x = −(1 + cot2 x)
d
dx secx = secx tan x
d
dx cscx = −cscx cot x
Trigonometric Inverses
The derivatives of the trigonometric inverses are
d
dx sin−1 x =
1
√
1 −x2 ;
|x| < 1
d
dx cos−1 x = −
1
√
1 −x2 ;
|x| < 1
d
dx tan−1 x =
1
1 + x2
d
dx cot−1 x = −
1
1 + x2
d
dx sec−1x =
1
x
√
x2 −1
;
|x| > 1
d
dx csc−1x = −
1
x
√
x2 −1
; |x| > 1
Exponentials and Logarithms
The derivative of the natural exponential function f (x) = ex is
d
dx ex = ex
That is, the exponential is its own derivative.
The inverse of an exponential function is a logarithmic function. For
example, the inverse of the natural exponential function ex is loge x, usually

B.2 Derivatives 643
written as ln x and called the natural logarithm. The derivative of the natural
logarithm is
d
dx ln x = 1
x
A general exponential function ax can be represented in terms of the
natural exponential as ax = ex ln a. So by the Chain rule:
d
dx ax = ln a · ax
A logarithm with an arbitrary base a can be represented in terms of the
natural logarithm as
loga x = ln x
ln a
Using this, the derivative is
d
dx loga x =
1
x ln a
B.2.4 Taylor’s Series
A power series centered on h is an inﬁnite summation of the form
∞

k=0
ak(x −h)k
Suppose it is possible to represent a function f as a power series centered
on h. Expanding terms, we can then write f (x) as
f (x) = a0 + a1(x −h) + a2(x −h)2 + · · ·
To solve for a0, a1, . . ., we begin by ﬁnding the value at f (h):
f (h) = a0 + a1(h −h) + a2(h −h)2 + · · ·
All terms but the ﬁrst cancel, and so a0 = f (h). Assuming that f is dif-
ferentiable at h, we can differentiate both sides and again evaluate at h
to get
f ′(h) = a1 + 2a2(h −h) + 3a3(h −h)2 · · ·

644
Appendix B Calculus Review
So a1 = f ′(h). Differentiating one more time (again, assuming that it is
possible) gives us
f ′′(h) = 2a2 + 6a3(h −h) + 12a4(h −h)2 · · ·
giving a2 = f ′′(h)/2. Continuing this process gives us a general formula
for ak of
ak = f (k)(h)
k!
Assuming that f is inﬁnitely differentiable, the Taylor series expansion
for f is
f (x) =
∞

k=0
f (k)(h)
k!
(x −h)k
(B.12)
The ﬁrst few terms of this look like
f (x) = f (h) + f ′(h)(x −h) + f ′′(h)
2
(x −h)2 + f ′′′(h)
6
(x −h)3 + · · ·
In general, a function f may not be inﬁnitely differentiable, so another
form is used. Suppose f is differentiable to degree n + 1 within an interval
I, and h lies within I. Then we can approximate f with pn, the nth Taylor
polynomial
f (x) ≈pn(x) =
n

k=0
f (k)(h)
k!
(x −h)k
The error of the approximation is given by rn, the nth Taylor remainder,
where
rn(x) = f (x) −pn(x)
It can be proved that for every x in I, there is a value ξ(x) between x and h
which allows us to represent rn(x) as
rn(x) = f (n+1)(ξ(x))
(n + 1)!
(x −h)n+1
This is also known as the Lagrange remainder formula.

B.3 Integrals 645
B.3 Integrals
B.3.1 Definition
Given a function f (x), the indeﬁnite integral (also known as the antiderivative)
of f (x) is represented as
'
f (x) dx
The term dx, or differential, represents the fact that we are integrating with
respect to the variable x; any other variables will be considered constant.
The result of the indeﬁnite integral for f (x) is a function F(x) + C, where
F ′(x) = f (x).
The arbitrary constant C is appended to indicate a possible constant term,
the value of which will differentiate to 0. For example, differentiating the
functions f (x) = x2+x+1 and g(x) = x2+x−12 produces f ′(x) = g′(x) = 2x+1.
Integrating 2x + 1 with respect to x gives the result x2 + x + C.
The deﬁnite integral of a function f (x) across an interval [a, b] is repre-
sented as
' b
a
f (x) dx
We say in this case that we are integrating from a to b. The result of the deﬁnite
integral is a quantity. In particular, when f (x) ≥0 it equals the area between
the curve and the axis represented by the differential — in this case, the x-axis.
For example, the following deﬁnite integral
' 1
0
x2 dx
computes the area (also known as the area under the curve) shown in
Figure B.4. The result is 1/3.
If any of the curve being evaluated is negative along the interval, the area
computed by the deﬁnite integral between that section of curve and the axis
in question is also negative. For example, the following deﬁnite integral
' 0
−1
x dx
computes the area shown in Figure B.5. The result of the deﬁnite integral
is −1/2.

646
Appendix B Calculus Review
−1
−0.5
0.5
1
1.5
2
−1
−0.5
0.5
1
1.5
2
x2
x
Figure B.4 Deﬁnite integral returns area between curves and x-axis. The result in
this case is 1
3.
0.5
1
1.5
2
−2
−1.5
−1
0.5
1
1.5
2
−0.5
−2
−1.5
−1
−0.5
x
x
Figure B.5 Deﬁnite integral of areas of curve below axis produces negative results.
The result in this case is – 1
2.
The fundamental theorem of calculus states that a deﬁnite integral can be
computed from two evaluations of the indeﬁnite integral. More speciﬁcally, if
f (x) is a continuous function on a closed interval [a, b], and an antiderivative
F(x) can be found such that F ′(x) = f (x) for all x in [a, b], then
' b
a
f (x)dx = F(x)|b
a = F(b) −F(a)

B.3 Integrals 647
B.3.2 Evaluating Integrals
Computing an integral for a general function is often not easy, if it can be done
at all. Most of the time in games numerical methods are used for evaluation
of deﬁnite integrals. However, knowing some simple integrals can be useful.
For more complex forms the reader is directed to a more detailed calculus
reference such as [32].
The integral of the sum of two functions is the sum of the integrals of the
functions:
'
f (x) + g(x) dx =
'
f (x) dx +
'
g(x) dx
If a function is multiplied by a constant, we can pull the constant out of
the integral:
'
a · f (x) dx = a
'
f (x) dx
If the limits of integration are reversed, then the result is negated:
' a
b
f (x) dx = −
' b
a
f (x) dx
The integral of a polynomial term xk, where k ̸= −1, is
'
xk dx = xk+1
k + 1 + C
If k = −1, then we note that
d
dx ln x = 1
x
so
' 1
x dx = ln x + C
Tables of integrals can be found in many places, in particular [122]. A few
selected examples are
'
cos x dx = sin x + C

648
Appendix B Calculus Review
'
sin x dx = −cos x + C
'
tan x dx = −ln | cos x| + C
'
cot x dx = ln | sin x| + C
'
secx dx = ln | secx + tan x| + C
'
cscx dx = −ln | cscx + cot x| + C
'
ex dx = ex + C
'
ax dx = ax
ln a + C
'
ln x dx = x ln x −x + C
'
1
√
a2 −x2 dx = sin−1 x
a + C
'
1
a2 + x2 dx = 1
a tan−1 x
a + C
'
1
x
√
x2 −a2 dx = 1
a sec−1 x
a
 + C
B.3.3 Trapezoidal Rule
In many cases it is either inconvenient or impossible to compute the integral
directly. For example, the sinc function f (x) = sin x/x cannot be integrated
analytically. In these cases numerical methods are used to approximate the
value of a deﬁnite integral. One of the simplest such methods is the trape-
zoidal rule.
Figure B.6 shows a function which we want to integrate. We can approx-
imate the curve between a and b by using a line segment, and the area under
the curve is approximated by the area of a trapezoid:
' b
a
f (x) dx ≈1
2(b −a)[f (b) + f (a)]

B.3 Integrals 649
−1
1
2
3
−1
1
2
3
4
5
f (a)
f (b)
a
b
f(x)
x
Figure B.6 Approximating the deﬁnite integral using a single trapezoid.
We can get a better approximation by slicing the interval into n equally
spaced subintervals, computing the areas of the resulting trapezoids and
adding them together (Figure B.7). This is equal to
' b
a
f (x) dx ≈b −a
2n
n−1

i=0
[f (xi+1) + f (xi)]
= b −a
2n [f (b) + f (a)] + b −a
n
n−1

i=1
f (xi)
where each xi = a + (b −a)i/n.
B.3.4 Gaussian Quadrature
While the trapezoid rule provides reasonable approximation of a deﬁnite
integral for little cost, we can get a better approximation using a method
called Gaussian quadrature.
The trapezoid rule can be rewritten as a summation of the form
' b
a
f (x) dx ≈
n

i=0
cif (xi)

650
Appendix B Calculus Review
f (x1)
f (b)
f (a)
−1
1
2
3
−1
1
2
3
4
5
x1
a
b
f (x2)
f (x3)
f (x4)
f (x5)
x2
x3
x4
x5
f(x)
x
Figure B.7 Approximating the deﬁnite integral using multiple trapezoids.
where our ci and xi are
ci =

(b −a)/2n;
i = 0, i = n
(b −a)/n;
0 < i < n
xi = a + (b −a)i/n
Gaussian quadrature uses a similar form, except that it uses nonuniform
samples and calculates weights to minimize error and get a better approx-
imation. The error is measured relative to a polynomial; using Gaussian
quadrature with n samples, we want the exact result when integrating
a polynomial P of degree 2n −1 or less.
It can be shown that for a given value of n and limits of integration
of [−1, 1], the values of xi needed to meet this criteria are the roots of the
nth member of a set of polynomials called the Legendre polynomials. The
corresponding values of ci are given by
ci =
' 1
−1
n
&
j=1,j̸=i
x −xj
xi −xj
dx

B.4 Space Curves 651
The roots xi and the associated constants ci are easily precomputed for
a given n. The ﬁrst few are
n
xi
ci
2
±
√
1/3
1
3
0
8/9
±
√
3/5
5/9
4
± 0.3399810436
0.6521451549
± 0.8611363116
0.3478548451
5
0.0000000000
0.5688888889
± 0.5384693101
0.4786286705
± 0.9061798459
0.2369268850
Note that using these values is valid only when integrating from −1 to 1.
If our integral has a general interval of [a, b], we can use the following to
transform it so it can be used with Gaussian quadrature:
' b
a
f (x) dx =
' 1
−1
f
(b −a)t + b + a
2
 b −a
2
dt
B.4 Space Curves
A parametric curve is a function Q(t) that maps a set of real values (repre-
sented by the parameter t) to a set of points. When mapping to R3, we com-
monly use a parametric curve broken into three separate functions, one for
each coordinate: Q(t) = (x(t), y(t), z(t)). This is also known as a space curve.
The ﬁrst derivative of a space curve is found by computing the derivatives
of the functions x(t), y(t), and z(t), so Q′(t) = (x′(t), y′(t), z′(t)). The result of
Q′ at parameter t is a vector tangent to the curve at location Q(t), instead
of a single slope value. The magnitude of the vector represents the speed
at which Q(t) changes relative to time; the larger the vector, the faster the
position changes. Q′ is also known as the velocity v(t).
Computing the second derivative of Q(t) is done similarly, by comput-
ing the second derivatives of the individual functions x, y, and z: Q′′(t) =
(x′(t), y′(t), z′(t)). This represents the change in velocity and is also known as
acceleration, or a(t).

652
Appendix B Calculus Review
If we normalize Q′(t) at each parameter t, we get the tangent T(t):
T(t) =
Q′(t)
∥Q′(t)∥
We can also compute the derivative of T(t) and normalize it to get the
normal N(t):
N(t) =
T′(t)
∥T′(t)∥
Note that this is not the same as the acceleration. While the acceleration’s
direction may vary relative to the velocity, the result of N(t) is always per-
pendicular to T(t). By taking the cross product of T and N, we get the
binormal B(t):
B(t) = T(t) × N(t)
Using T(t), N(t), and B(t) as an orthonormal basis and Q(t) as the origin, this
gives us a coordinate frame for every parameter t, known as the Frenet frame.
As mentioned, N(t) is not the same as acceleration. The acceleration vector
lies in the subspace formed by using T and N as basis vectors, or
a = aT T + aNN
where
aT = d∥v∥
dt
aN = ∥v∥
((((
dT
dt
((((
A parametric curve Q(t) is smooth on an interval [a, b] if it has a continuous
derivative on [a, b] and Q′(t) ̸= 0 for all t in (a, b). A parametric curve Q(t) is
piecewise smooth on an interval [a, b] if it can be broken into a ﬁnite number
of subintervals, where it is smooth on each subinterval and Q has one-sided
derivatives on (a, b).
For a given point P on a smooth curve Q(t), we deﬁne a circle with radius
ρ and ﬁrst and second derivative vectors equal to those at P as the osculating
circle. The curvature κ at P is 1/ρ. We can also deﬁne the curvature of Q as
κ(t) = ∥T′(t)∥
∥Q′(t)∥
(B.13)
The curvature at any point is always nonnegative. The higher the curvature,
the more the curve bends at that point; the curvature of a straight line is 0.

B.4 Space Curves 653
We can compute the length L of a piecewise smooth space curve Q on an
interval [a, b] by
L =
' b
a
∥Q′(t)∥dt
(B.14)
If Q(t) is smooth, we can also deﬁne the arc length function s(t) as
s(t) =
' t
a
∥Q′(u)∥du
If t ≥a, this measures the length of the curve from a given point Q(a) to
a variable point Q(t). If we differentiate both sides with respect to t, we get
s′(t) = ∥Q′(t)∥= ∥v(t)∥
Since vector length is nonnegative, and we also know that Q′(t) ̸= 0 (since
Q is smooth), we know that s(t) is strictly increasing and thus invertible to
a function t(s). Based on this, we can reparameterize a curve represented by
Q(t) by s, by using Q(t(s)). This is known as reparameterization by arc length.
Rather than mapping a time t to a position on the curve, we can map a length
L to a position on the curve.
It is usually impossible to evaluate the integral in equation B.14, and
hence the arc length, directly. Instead the length is approximated by using
numerical methods, such as the trapezoid rule or Gaussian quadrature.

Bibliography
[1]
AMD. AMD developer support website. www.amd.com.
[2] American National Standards Institute and Institute of Electrical and
Electronic Engineers.
IEEE standard for binary ﬂoating-point arithmetic.
ANSI/IEEE Standard, Std 754-1985, New York, 1985.
[3] Howard Anton and Chris Rorres. Elementary Linear Algebra: Applications
Version. Wiley, New York, 7th edition, 1994.
[4] ARM. Arm Ltd. developer support website. www.arm.com.
[5] James Arvo, editor. Graphics Gems II. Academic Press, Inc., San Diego,
1991.
[6] ATI. ATI developer support website. www.ati.com.
[7] Sheldon Axler. Linear Algebra Done Right. Springer-Verlag, New York,
2nd edition, 1997.
[8] Richard H. Bartels, John C. Beatty, and Brian A. Barsky. An Introduction
to Splines for Use in Computer Graphics and Geometric Modeling.
Morgan
Kaufman, San Francisco, 1987.
[9] J. F. Blinn and M. E. Newell. Clipping using homogeneous coordinates. In
Computer Graphics (SIGGRAPH ’78 Proceedings), pages 245–251, New York,
1978.
[10] James F. Blinn. A generalization of algebraic surface drawing. ACM
Transactions on Graphics, 1(3):235–256, 1982.
[11] Jim Blinn. A Trip Down the Graphics Pipeline. Morgan Kaufmann, San
Francisco, 1996.
[12] Jim Blinn. Dirty Pixels. Morgan Kaufmann, San Francisco, 1998.
[13] Jim Blinn.
Notation, Notation, Notation.
Morgan Kaufmann, San
Francisco, 2002.
[14] Jonathan Blow. Hacking quaternions. Game Developer, March 2002.
[15] W. Boehm. Inserting new knots into B-spline curves. Computer Aided
Design, 12(4):199–201, 1980.
655

656
Bibliography
[16] W. Boehm.
On cubics: A survey.
Computer Graphics and Image
Processing, 19:201–226, 1982.
[17] Richard L. Burden and J. Douglas Faires. Numerical Analysis. PWS,
Boston, MA, 5th edition, 1993.
[18] Arthur Cayley.
The Collected Mathematical Papers of Arthur Cayley.
Cambridge University Press, Cambridge, 1889–1897.
[19] Michael F. Cohen and John R. Wallace. Radiosity and Realistic Image
Synthesis. Morgan Kaufman, San Francisco, 1993.
[20] T. N. Cornsweet. Visual Perception. Academic Press, New York, 1970.
[21] R. Courant and D. Hilbert. Methods of Mathematical Physics, Volume
One. Wiley, New York, 1989 (reprint).
[22] M. Cyrus and J. Beck. Generalized two- and three-dimensional clipping.
Computers and Graphics, 3:23–28, 1978.
[23] Mark DeLoura, editor.
Game Programming Gems.
Charles River,
Hingham, MA, 2000.
[24] Mark DeLoura, editor.
Game Programming Gems 2.
Charles River,
Hingham, MA, 2001.
[25] Tony deRose. Three-dimensional computer graphics: A coordinate-free
approach. Technical Report, University of Washington, 1993.
[26] Rene Descartes. La Geometrie (The Geometry of Rene Descartes). Dover
Publications, New York, 1954.
[27] David H. Eberly.
3D Game Engine Design.
Morgan Kaufmann, San
Francisco, 2001.
[28] David H. Eberly. Personal communication with authors, 2002.
[29] David H. Eberly.
Rotation representations and performance issues.
Technical Report, Magic Software, 2002.
[30] David H. Eberly. Game Physics. Morgan Kaufmann, San Francisco,
2003.
[31] Margaret A. Ellis and Bjarne Stroustrup. The Annotated C++ Reference
Manual. Addison-Wesley, Reading, MA, 1990.
[32] Robert Ellis and Danny Gulik.
Calculus With Analytic Geometry.
Harcourt, San Diego, CA, 2nd edition, 1982.
[33] Wolfgang Engel, editor. Direct3D ShaderX. Wordware, Plano, Texas,
2002.
[34] Christer Ericson. Real-Time Collision Detection. Morgan Kaufmann,
San Francisco, 2004 (forthcoming).

Bibliography 657
[35] Euclid. The Elements. Dover Publications, New York, 1956.
[36] James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes.
Computer Graphics: Principles and Practice. Addison-Wesley, Reading, MA,
2nd edition, 1992.
[37] Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence. Linear
Algebra. Prentice-Hall, Englewood Cliffs, NJ, 1979.
[38] H. Fuchs, Z. M. Kedem, and B. Naylor. On visible surface generation by
a priori tree structures. In Computer Graphics (SIGGRAPH ’80 Proceedings),
1980.
[39] Andrew S. Glassner, editor. Graphics Gems. Academic Press, San Diego,
CA, 1990.
[40] Andrew S. Glassner, editor. An Introduction to Ray Tracing. Academic
Press, Boston, MA, 1989.
[41] Andrew S. Glassner.
Principles of Digital Image Synthesis.
Morgan
Kaufmann, San Francisco, 1994.
[42] Ronald N. Goldman.
Matrices and transformations.
In Andrew S.
Glassner, editor, Graphics Gems, pages 472–475. Academic Press, San Diego,
CA, 1990.
[43] Ronald N. Goldman. Some properties of Bézier curves. In Andrew S.
Glassner, editor, Graphics Gems, pages 472–475. Academic Press, San Diego,
CA, 1990.
[44] Ronald N. Goldman.
Recovering the data from the transformation
matrix. In James Arvo, editor, Graphics Gems II, pages 324–331. Academic
Press, San Diego, CA, 1991.
[45] Ronald N. Goldman. Decomposing linear and afﬁne transformations.
In David Kirk, editor, Graphics Gems III, pages 108–116. Academic Press, San
Diego, CA, 1992.
[46] Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns
Hopkins University Press, Baltimore, MD, 1993.
[47] Gene H. Golub and James M. Ortega.
Scientiﬁc Computing and Dif-
ferential Equations: An Introduction to Numerical Methods. Academic Press,
Boston, MA, 1992.
[48] S. Gottschalk, M. C. Lin, and D. Manocha.
OBBtree: A hierarchical
structure for rapid interference detection. In Computer Graphics (SIGGRAPH
’96 Proceedings), pages 171–180, 1996.
[49] Jens Gravesen. The length of Bézier curves. In Graphics Gems V, pages
199–205. Academic Press, San Diego, CA, 1998.

658
Bibliography
[50] Gil
Gribb
and
Klaus
Hartmann.
Fast
extraction
of
view-
ing
frustum
planes
from
the
world-view
projection
matrix,
2001.
www2.ravensoft.com/users/ggribb/plane%20extraction.pdf.
[51] Brian Guenter and Richard Parent.
Computing the arc length of
parametric curves. IEEE Computer Graphics and Applications, 10(3):72–78,
1990.
[52] P. Haeberli and K. Akeley. The accumulation buffer: Hardware sup-
port for high-quality rendering.
In Computer Graphics (SIGGRAPH ’90
Proceedings), 1990.
[53] Halliday, David and Robert Resnik. Fundamentals of Physics. Wiley and
Sons, New York, 1981, 2nd edition.
[54] William Hamilton.
On quaternions, or on a new system of imagi-
naries in algebra.
Philosophical Magazine, 1844–1850.
(Available online).
www.maths.soton.ac.uk/EMIS/classics/Hamilton.
[55] John C. Hart, George K. Francis, and Louis H. Kauffman.
Visual-
izing quaternion rotation.
ACM Transactions on Graphics, 13(3):256–276,
1994.
[56] Donald Hearn and M. Pauline Baker. Computer Graphics. Prentice-Hall,
Upper Saddle River, NJ, 2nd edition, 1996.
[57] Paul Heckbert. Texture mapping polygons in perspective. Technical
Report, New Institute of Technology, 1983.
[58] Paul Heckbert and Henry Moreton. Interpolation for polygon texture
mapping and shading.
In David Rogers and Rae Earnshaw, editors, State
of the Art in Computer Graphics: Visualization and Modeling, pages 101–111.
Springer-Verlag, New York, 1991.
[59] Paul S. Heckbert, editor. Graphics Gems IV. Academic Press, San Diego,
CA, 1994.
[60] Chris Hecker. Under the hood/behind the screen: Perspective texture
mapping (series). Game Developer, 1995–1996.
[61] Chris Hecker.
Behind the screen: Physics (series).
Game Developer
Magazine, 1996–1997.
[62] Martin Held. ERIT — a collection of efﬁcient and reliable intersection
tests. Journal of Graphics Tools, 2(4):25–44, 1997.
[63] John L. Hennessy and David A. Patterson.
Computer Architecture: A
Quantitative Approach.
Morgan Kaufmann, San Francisco, 2nd edition,
1996.
[64] Intel. Intel developer support website. http://developer.intel.com.

Bibliography 659
[65] Thomas Jakobson. Advanced character physics. In Proceedings of Game
Developers Conference, 2001.
[66] Kenneth Joy.
On-line geometric modeling notes: Afﬁne combina-
tions, barycentric coordinates and convex combinations. Technical report,
University of California, Davis, 2000.
[67] Kenneth Joy. On-line geometric modeling notes: Points and vectors.
Technical report, University of California, Davis, 2000.
[68] Kenneth Joy.
On-line geometric modeling notes: Vector spaces.
Technical report, University of California, Davis, 2000.
[69] William
Kahan.
Lecture
notes
on
the
status
of
IEEE-754.
Postscript ﬁle accessible electronically through the Internet at the address
http://cs.berkeley.edu/∼wkahan/ieee754status/ieee754.ps, 1996.
[70] David Kirk, editor. Graphics Gems III. Academic Press, San Diego, CA,
1992.
[71] Leslie Lamport.
LATEX: A Document Preparation System.
Addison-
Wesley, Reading, MA, 1986.
[72] Eric Lengyel. Mathematics for 3D Game Programming and Computer
Graphics. Charles River, Hingham, MA, 2002.
[73] Yu-Dong Liang and Brian Barsky. A new concept and method for line
clipping. ACM Transactions on Graphics, 3(1):1–22, 1984.
[74] D. Malacara.
Color Vision and Colorimetry: Theory and Applications.
SPIE Press, Bellingham, WA, 2002.
[75] William R. Mark, R. Steven Glanville, Kurt Akeley, and Mark J. Kilgard.
Cg: A system for programming graphics hardware in a C-like language. In
Computer Graphics (SIGGRAPH ’03 Proceedings), 2003.
[76] Stan Melax. Finding the shortest path quaternion. In Mark DeLoura,
editor, Game Programming Gems. Charles River, Hingham, MA, 1998.
[77] Microsoft.
Direct X SDK.
Available for free download from
http://msdn.microsoft.com.
[78] Brian Mirtich.
Fast and accurate computation of polyhedral mass
properties. Journal of Graphics Tools, 1(2):31–50, 1996.
[79] Tomas Möller and Eric Haines.
Real-Time Rendering.
A. K. Peters,
Natick, MA, 1999.
[80] William M. Newman and Robert F. Sproull.
Principles of Interactive
Computer Graphics. McGraw-Hill, New York, 1981.
[81] Hubert Nguyen. Casting shadows. Game Developer, March 1999.

660
Bibliography
[82] nVIDIA. nVIDIA developer support website. http://developer.nvidia.com.
[83] OpenGL Architecture Review Board, Mason Woo, Jackie Neider, Tom
Davis, and Dave Shreiner. OpenGL Programming Guide: The Ofﬁcial Guide to
Learning OpenGL. Addison-Wesley, Boston, MA, 3rd edition, 1999.
[84] Joseph O’Rourke. Computational Geometry in C. Cambridge University
Press, Cambridge, UK, 2000.
[85] Lewis Padgett. Mimsy were the borogroves. Reprinted in Science Fiction
Hall of Fame, Volume One, Doubleday, NY, 1943.
[86] Alan W. Paeth, editor. Graphics Gems V. Academic Press, San Diego,
CA, 1995.
[87] Rick Parent. Computer Animation: Algorithms and Techniques. Morgan
Kaufmann, San Francisco, 2002.
[88] David A. Patterson and John L. Hennessy. Computer Organization and
Design: The Hardware/Software Interface. Morgan Kaufmann, San Francisco,
1994.
[89] Bui Tuong Phong.
Illumination for computer generated pictures.
Communications of the ACM, 18(6):311–317, 1975.
[90] Charles Poynton. Charles Poynton’s color FAQ. www.poynton.com/.
[91] Franco P. Preparata and Michael Ian Shamos. Computational Geometry:
An Introduction. Springer-Verlag, New York, 1991.
[92] William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T.
Vetterling.
Numerical Recipes in C : The Art of Scientiﬁc Computing, 2nd
edition. Cambridge University Press, New York, 1993.
[93] Anthony Ralston. A First Course in Numerical Analysis. McGraw-Hill,
New York, 1965.
[94] David F. Rogers. An Introduction to NURBS: With Historical Perspective.
Morgan Kaufmann, San Francisco, 2000.
[95] David F. Rogers and J. Alan Adams.
Mathematical Elements for
Computer Graphics. McGraw-Hill, New York, 1990.
[96] Philip J. Schneider and David H. Eberly. Geometric Tools for Computer
Graphics. Morgan Kaufmann, San Francisco, 2002.
[97] David Seal, editor.
ARM Architecture Reference Manual.
Addison-
Wesley, Reading, MA, 2nd edition, 2000.
[98] Ken Shoemake. Animating rotation with quaternion curves. In Com-
puter Graphics (SIGGRAPH ’85 Proceedings), volume 19, pages 245–254,
1985.

Bibliography 661
[99] Ken Shoemake.
Quaternion calculus for animation.
In Math for
SIGGRAPH (ACM SIGGRAPH ’89 Course Notes 23), pages 187–205. 1989.
[100] Ken Shoemake. Quaternions and 4×4 matrices. In James Arvo, editor,
Graphics Gems II, pages 351–354. Academic Press, San Diego, CA, 1991.
[101] Ken Shoemake. Euler angle conversion. In Paul S. Heckbert, editor,
Graphics Gems IV, pages 222–229. Academic Press, San Diego, CA, 1994.
[102] Ken Shoemake.
Polar matrix decomposition.
In Paul S. Heckbert,
editor, Graphics Gems IV, pages 207–221. Academic Press, San Diego, CA,
1994.
[103] Ken Shoemake and Tom Duff. Matrix animation and polar decompo-
sition. In Proceedings of Graphics Interface ’92, pages 258–264, 1992.
[104] William Stallings. Computer Organization and Architecture. Prentice-
Hall, Englewood Cliffs, NJ, 5th edition, 2000.
[105] Dan Sunday. Distance between Lines and Segments with their Closest
Point of Approach. Technical Report, http://geometryalgorithms.com, 2001.
[106] I. E. Sutherland. Sketchpad: A man-machine graphical communica-
tions system. In IFIPS Proceedings of the Spring Joint Computer Conference,
1963.
[107] I. E. Sutherland and G. W. Hodgeman. Reentrant polygon clipping.
Communications of the ACM, 17(1):32–42, 1974.
[108] Dante Treglia, editor.
Game Programming Gems 3.
Charles River,
Hingham, MA, 2002.
[109] Gino van den Bergen. Collision Detection in Interactive 3D Environ-
ments. Morgan Kaufmann, San Francisco, 2003.
[110] James M. Van Verth. Using the covariance matrix for better ﬁtting
bounding objects. In Andrew Kirmse, editor, Game Programming Gems 4.
Charles River, Hingham, MA, 2004.
[111] David R. Warn. Lighting controls for synthetic images. In Computer
Graphics (SIGGRAPH ’83 Proceedings), 1983.
[112] Alan Watt.
3D Computer Graphics, 2nd edition.
Addison-Wesley,
Wokingham, UK, 1993.
[113] Alan Watt and Fabio Policarpo. 3D Games: Real-Time Rendering and
Software Technology, Volume One. Addison-Wesley, Harlow, UK, 2001.
[114] Alan Watt and Mark Watt.
Advanced Animation and Rendering
Techniques: Theory and Practice. Addison-Wesley, Wokingham, UK, 1992.
[115] Eric W. Weisstein.
Eric W. Weisstein’s World of Mathematics.
http://mathworld.wolfram.com.

662
Bibliography
[116] Eric
W.
Weisstein.
Eric
W.
Weisstein’s
World
of
Physics.
http://scienceworld.wolfram.com/physics.
[117] E. Welzl. Smallest enclosing disks (balls and ellipsoids). In H. Maurer,
editor, Lecture Notes in Computer Science, New Results and New Trends in
Computer Science, volume 555, pages 359–370. Springer-Verlag, New York,
1991.
[118] Lance Williams.
Pyramidal parametrics.
In Computer Graphics
(SIGGRAPH ’83 Proceedings), 1983.
[119] Andrew Witkin and David Baraff. Physically based modelling: Princi-
ples and practice. In ACM SIGGRAPH 2001 Course Notes, 2001.
[120] George Wohlberg. Digital Image Warping. IEEE Computer Society
Press, Los Alamitos, 1990.
[121] Hansong Zhang. Effective occlusion culling for the interactive display
of arbitrary models. Technical Report TR99-027, UNC/Chapel Hill, January,
1998.
[122] Daniel Zwillinger. CRC Standard Mathematical Tables and Formulae.
CRC Press, Boca Raton, FL, 1995.

Index
A
absolute error
of ﬁxed point numbers, 160–161, 164
of ﬂoating point numbers, 181
acceleration, 579
accumulation buffer (A-buffer),
414–416
acyclic end condition, 437
adding
colors, 258
ﬁxed point numbers, 166
ﬂoating point numbers, 182–183
matrices, 73
quaternions, 493
vectors, 13
additive pixel blending, 404
adjoint matrix, 105–106
afﬁne combinations, 46–47
afﬁne in screen space, deﬁnition, 371
afﬁnely independent, 46
afﬁne spaces, 43–45
afﬁne transformations
arbitrary points and, 132–133
deﬁned, 108–109
manipulation of game objects using,
135–141
matrix decomposition, 141–145
reﬂection, 126–130
representation, 109–113
rigid, 109
rotation, 115–124
scaling, 124–126
shear, 130–132
transforming plane normals,
134–135
translation, 113–115
alpha blending, 262, 401–406
alpha values, 261–262
ambient light, 325–327
AMD, 199
3DNow!, 24–25, 198
angle(s)
axis-angle, 481–485
complementary, 629–630
compound, 631
double, 631
ﬁxed and Euler, 474–481
half, 632
of rotation, 115
angular momentum, 603–605
angular velocity, 599–600
animation
See also curves
deﬁned, 419
motion picture, 419–420
anisotropic texture ﬁltering, 399
antialiasing, 406–416
approximation, 420
arc length, 460, 462–465
area sampling, 409–411
ARM processor architecture, 171
atan2() vs. acos(), 50
augmented matrix, 91
axis-aligned bounding boxes (AABB)
AABB-AABB intersection, 540–541
AABB-plane intersection, 544–546
663

664
Index
axis-aligned bounding boxes (AABB)
AABB-ray intersection, 542–544
deﬁned, 538–540
axis-angle representation
concatenation, 484
deﬁned, 481–482
format conversion, 482–484
vector rotation, 484–485
axis of rotation, 115
B
backface culling, 270
back substitution, 94
backwards Euler, 597
barycentric coordinates, 46, 268,
279–280, 292
basis vectors, linear combinations
and, 18–22
Bernstein basis (polynomials), 441, 456
Bézier curves, 440–444
bilinear texture ﬁltering, 386–388
blending, pixel 401–406
Blinn’s notation (clipping), 239, 242
block matrices, 75–76
borogroves, mimsy, 16
boundary conditions, 428
bounding boxes
axis-aligned, 538–546
object-oriented, 550–556
bounding hierarchies, 563–567
bounding objects, 529
box ﬁltering, 393, 395
brightness, 310, 311
B-splines, 444–448
BSP tree, 363
buffering
depth, 365–375
z-, 372–374
C
cabinet projection, 214
camera
controlling, 207–209, 467–470
deﬁning, 204–207
obscura, 212
capsule
capsule-capsule intersection, 549
capsule-plane intersection, 549–550
capsule-ray intersection, 549
deﬁned, 546–548
Cartesian coordinates, 41
converting polar and spherical
coordinates to and from, 49–52
Cartesian frame, 45
catastrophic cancellation, 190–192
Catmull-Rom splines, 438–440
cavalier projection, 213–214
center of mass, 599
centroid, 47
Cg (shader language), 306
clamped cubic spline, 434
clamping
color, 261
texture coordinates, 298–301
clipping
algorithms, 242–243
deﬁned, 233
general plane, 238–243
homogeneous, 243–245
reasons for, 233–237
closest point and distance tests
between two lines, 522–524
between two line segments, 525–527
general linear components, 527–528
line-line distance, 524–525
line-point distance, 518–519
line segment-line segment distance,
527
line segment-point distance,
520–521
on line segment to point, 519–520
on line to point, 516–517
closure, 17
coefﬁcient of restitution, 614
collinear points, 57
collision response
linear, 611–616
locating the point of, 607–611
other methods, 619–620
rotational, 616–619

Index 665
collision system, 562–575
color
alpha values, 261–262
clamping, 261
computing source pixel, 375–378
face, 269
ﬂat, 375–376
Gouraud, 376–378
in OpenGL, 263–264
operations upon, 258–259
precision, 262–263
procedural, 304–307
range limitations, 259–261
rescaling, 261
RGB, 256–257
storage formats, 262–263
as vectors, 257
vertices, 265–266
coloring surfaces
constant colors, using, 276–284
ﬂat shading, 375–376
Gouraud shading, 376–378
objects, assigning colors, 277
sharp edges, 282–283
triangles, assigning colors, 277–278
vertices, assigning colors, 278–283
column major order, 86
column space, 80
computer number representation
See also ﬁxed point numbers;
ﬂoating point numbers
error, absolute and relative, 160–161
ﬁniteness of, 156
overﬂow, 158–159
range, 156–159
representing real numbers, 159–161
concatenation
of axis-angle, 484
of ﬁxed/Euler angles, 478
of quaternions, 495–496
of transformations, 81–83
concave polygon, 60
constant colors, using, 276–284
constraint systems, 619–620
continuity, 637
convex combination, 47
convex hull, 47
convex polygon, 60
convex sets, 47
coordinate frame, 44
cosines, law of, 28–29, 628
Cramer’s method, 106
cross product, 34–37
culling
backface, 270
deﬁned, 233
frustum, 573–574
process, 237–238
reasons for, 233–237
triangle, 269–272
curves
Bézier, 440–444
B-splines, 444–448
Catmull-Rom splines, 438–440
controlling speed, 459–467
Hermite. See curves, Hermite
Lagrange polynomials, 425–427
linear interpolation, 422–425
NURBS, 450
rational, 448–450
curves, Hermite
deﬁned, 427–433
end conditions, 435–437
generation of, 433–435
curves, parametric
deﬁned, 421–422
space, 421
curves, rendering
forward differencing, 450–453
midpoint subdivision, 453–456
OpenGL for, 456–459
cyclic end condition, 436–437
D
de Casteljau’s method, 455
decomposition
matrix, 141–145
polar, 142–143
singular value, 142, 143
deformations, 109, 124
degenerate triangles, 60
denormals (ﬂoating point), 185,
189–190, 194–196

666
Index
depth buffering, 365–375
depth sorting, 362–365
derivatives, 638–644
determinants
adjoint matrix and inverse, 105–106
computing, 100–102
deﬁned, 99–100
elementary row operations and,
103–105
diagonal matrix, 72
diffuse light, 327–330
directional light source, 314–315
Direct3D®, 86, 229, 231, 233, 252, 310
DirectX®, 75, 304–305
distance tests. See closest point and
distance tests
dividing,
ﬁxed point numbers, 168–169
domain, 66
dot product, 28–32
perpendicular, 37
quaternions, 494
double buffering, 358–359
double precision, 192–193
dynamics. See rigid body dynamics
E
ease-in/ease-out, 465–467
edges, 60
element, matrix, 71
elementary row operations, 90–91
determinants and, 103–105
emissive light, 324–325
error, absolute and relative
ﬁxed point numbers, 160–161, 164
ﬂoating point numbers, 181
Euclidean distance, 45
Euclidean inner product.
See dot product
Euclidean norm, 25–26
Euler angles
concatenation, 478
deﬁned, 474–476
format conversion, 476–478
other issues, 479–481
vector rotation, 478–479
Euler’s method, 587–590
backwards, 597
explicit methods, 596
F
face (polygon) attributes, 269
face (polygon) color, 269
faceted shading, 277
ﬁll convention, polygon, 361
ﬁltering, texture, 259
ﬁxed angles
concatenation, 478
deﬁned, 474–476
format conversion, 476–478
other issues, 479–481
vector rotation, 478–479
ﬁxed point numbers
adding, 166
basic representation, 162–163
converting real numbers to and
from, 164–166
dividing, 168–169
error, absolute and relative, 164
limitations of, 173
multiplying, 166–168
overﬂow and underﬂow, 170–172
range and precision, 163–166
real-world issues, 169–170
subtracting, 166
ﬂat shading, 277–278, 375–376
lighting and, 338–340
ﬂoating point numbers, 15–16, 22–23
code, 198–199
internal hardware precision,
193–194
performance of denormalized
numbers, 194–196
real-world issues, 193–198
scientiﬁc notation, 173–176
software emulation, 196–197
ﬂoating point numbers, IEEE 754
standard
adding, 182–183
basic representation, 177–179
catastrophic cancellation, 190–192
denormals, 185, 189–190, 194–196

Index 667
double precision, 192–193
error, absolute and relative, 181
inﬁnity, 186–187
multiplying, 183–184
normalized mantissas and hole at
zero, 188–189
not a number (NaN), 187–188
range and precision, 179–181
rounding modes, 184
special values, 184–188
subtracting, 183
underﬂow, 189–190
zero, 185–186
forces, 581–582
forward differencing, 371, 450–453
framebuffers
deﬁned, 355–356
double buffering, 358–359
interlacing, 357–358
memory organization, 356–357
scanlines, 357
Frenet frame, 467–468
frustum culling, 573–574
function, 66
G
game objects, afﬁne transformations
used to manipulate, 135–141
Gaussian elimination, 91–94
Gaussian quadrature, 649–651
Gauss-Jordan elimination, 93, 97
generalized line equation, 55–57
generalized plane equation, 58–59
gimbal lock, 479
GL_AMBIENT, 326–327
GL_CCW, 272
GL_CONSTANT_ATTENUATION, 318
GL_DEPTH_BUFFER_BIT, 374
GL_DIFFUSE, 329
GL_EMISSION, 325
GL_LEQUAL, 374
GL_LINEAR_ATTENUATION, 318
GL_MODELVIEW, 251–252
GL_PROJECTION, 251–252
GL_QUADRATIC_ATTENUATION, 318
GL_REPLACE, 349
GL_SPOT_CUTOFF, 322
GL_SPOT_DIRECTION, 322
GL_SPOT_EXPONENT, 322
GL_TEXTURE_MAG_FILTER, 389
GL_TEXTURE_MIN_FILTER, 401
GL_TRIANGLES, 275, 340
GL_TRIANGLE_STRIP, 276
glBegin(), 268
glBindTexture(), 288, 289, 399
glBlendFunc(), 406
glClear(), 374
glClippingPlane(), 244
glColor3f(), 264
glColor3ub(), 264
glDeleteTextures(), 288
glDisable(), 287
glDisableClientState(), 274
glDepthMask(), 406
glEnable(), 286–287
glEnableClientState(), 274
glEvalCoordlf(), 457–458
glFrontFace(), 272
glFrustum(), 227
glGenTextures(), 288
glLightf(), 322
glLightModeli(), 335, 350, 351
glMapld(), 457
glMaplf(), 457
glMaterialfv(), 324, 325, 326,
329, 333
glMultMatrix(), 252
Global illumination, 312
glOrtho(), 230
glPopMatrix(), 252
glPushMatrix(), 252
glShadeModel(), 278, 340, 341
glTexEnvi(), 349
glTexImage2D(), 287, 399–400
glTexParameteri(), 300, 388, 401
gluBuild2DMipmaps(), 400
gluLookAt(), 210–211
gluPerspective(), 226
glVertexPointer(), 274–275
glVertex3f(), 266, 268–269
glVertex3fv(), 266
Gouraud-shaded colors, 376–378
Gouraud shading, 279–282, 340–341

668
Index
Gram-Schmidt orthogonalization,
33–34, 207, 503
guard band (clipping), 237
H
heading angle, 475
Hermite curves. See curves, Hermite
hierarchies, bounding, 563–567
high color, 262
high-level shading language
(HLSL), 306
homogeneous clipping, 243–245
homogeneous coordinates, 219
homogeneous space, 219
I
identity
matrix, 83–84
quaternions, 497–498
IEEE 754. See ﬂoating point numbers,
IEEE 754 standard
illuminance, 311
Image(), 384, 385–386
implicit methods, 596–598
implicit surfaces, 267
impulse-based response, 611
indexed geometry, 273
inertial tensor, 603–605
inﬁnite viewer approximation
(lighting), 334–335
inﬁnity, ﬂoating point, 186–187
initial value problems
deﬁned, 585–586
Euler’s method, 587–590
higher-order methods, 593
implicit methods, 596–596
midpoint method, 590–592
Verlet integration, 594–595
inner products, 28
inner product space, 28
int (C/C++ data type), 157–158
range and type conversion, 159
integer texel coordinate, 379
integrals, 645–651
Intel Corp., 199
SSE (Streaming SIMD Extensions),
24–25, 197–198
interlacing, 357–358
interpolation, 420
linear, 422–425, 503–507
orientation, 471–472, 501–511
performance improvements,
510–511
spherical linear, 507–510
intersection testing
closest point and distance tests,
516–528
collision system, 562–575
object, 528–562
inverse
of a matrix, 105–106
of quaternions, 497–498
of trigonometric functions, 633
inverse-square law, 316–318
irradiance, 311–312
isotropic texture ﬁltering, 399
IsZero(), 27
IvInvSqrt(), 27
IvSinCosf(), 477
IvSqrt(), 27
K
kernel, 67
kinetics, 577
L
Lagrange polynomials, 425–427
Lagrange product, 426
Lagrange remainder formula, 644
Lambertian reﬂector, 327
law of cosines, 28–29
leapfrog Verlet, 595
left-hand rule, 40, 43
length (magnitude), vector, 12, 25–28,
258
LengthSquared(), 27
libraries
engine and rendering, 6–7

Index 669
math, 6
support, 5–6
light (lighting)
ambient, 325–327
diffuse, 327–330
direction vector, 313
emission, 324–325
equation, 335–338
intensity value, 313
measuring, 310–312
merging textures and, 348–351
programmable shaders and, 351
as a ray, 312
specular, 330–335, 349–351
surface reﬂection and surface
materials, 323–324
tuning values, 313
light, shading and
ﬂat-shaded light, 338–340
per-pixel light (Phong shading),
344–348
per-vertex light, 340–344
light approximation
basics of, 310–312
OpenGL, 312–313
light sources
directional, 314–315
other types of, 322–323
point, 315–318
spotlights, 318–322
limits, 635–637
linear-bilinear (trilinear) texture
ﬁltering, 398
linear collision response, 611–616
linear combinations, 18–22
linear dynamics
forces, 581–582
linear momentum, 582
moving with constant acceleration,
578–581
moving with variable acceleration,
582–584
linear equations
deﬁned, 88–89
Gaussian elimination, 91–94
solving, 89–91
linear interpolation, 422–425, 503–507
spherical, 507–510
linearly dependent, 19
linearly independent, 19
linear momentum, 582
linear space. See vector space
linear transformations
deﬁned, 66–67
null space, 67–68
range, 66, 68
vectors and, 69–71
lines
collinear points, 57
deﬁned, 52–53
generalized equation, 55–57
parameterized, 53–55
segments, 54
straight, 52
local frame, 136
local space, 136
local-to-world transformation, 137
lower triangular matrix, 72
lumen, 311
luminance, 258, 310, 311
luminous ﬂux, 311
density, 311
M
mach banding, 281
magnitude (length)
quaternions, 493–494
vector, 12, 25–28, 258
main diagonal, 72
Manhattan distance, 25
mantissas and hole at zero, 188–189
matrices
adding, 73
adjoint, 105–106
augmented, 91
block, 75–76
combining linear transformations,
81–83
decomposition, 141–145
deﬁned, 65–66, 71–72
diagonal, 72
examples of, 71

670
Index
matrices (continued)
identity, 83–84
implementation, 85–88
inverse of, 105–106
orthogonal, 98
product, 77–79
reﬂection, 102
rotation, 102
scalar multiplication, 73
skew symmetric, 74
square, 71
symmetric, 74
transforming vectors, 79–81
transpose, 74
vector operations with, 84–85
vector representation, 75
zero, 71, 96
matrix inverse
deﬁned, 95–97
simple, 97–98
midpoint method, 590–592
midpoint subdivision, 453–456
mipmapping, 391–401
model, 11
model space, 136
modulate texture blending, 405
modulate mode texturing, 348
modulate with late add texture
blending, 350
moments of inertia, 604
multiplying
colors, 258
componentwise, 258–259
ﬁxed point numbers, 166–168
ﬂoating point numbers, 183–184
matrices, 77–79
quaternion
by quaternion, 495–496
by scalar, 493
vector, by scalar, 14–15
multisample antialiasing, 412–413
N
natural (relaxed) end condition,
435–436
natural spline, 435
nearest neighbor texture ﬁltering, 384
negating quaternions, 493
Newton-Raphson root ﬁnding, 460
normalized cubic spline, 435
normalized device coordinates,
217–219
normalized quaternions, 493–494
normalized vector, 12, 36
normals, generating vertex, 341–344
norms, 25–26
not a number (NaN), 187–188
null space, 67–68
numeric integration. See initial value
problems
NURBS, 450
O
object(s)
assigning colors to, 277
bounding, 529
dynamic, 568–569
hierarchies, 145–148
space, 136, 292
object intersections, 528–529
axis-aligned bounding boxes,
538–546
object-oriented bounding boxes,
550–556
spheres, 530–538
swept spheres, 546–550
triangles, 556–562
object-oriented bounding boxes (OBBs)
deﬁned, 550–551
OBB-OBB intersection, 551–554
OBB-plane intersection, 556
OBB-ray intersection, 554–555
oblique parallel projection, 231–233
oblique perspective, 227–229
oblique projection, 213–214
OpenGL, 48, 75, 86, 206–207, 209, 210
See also gl
ambient light, 326–327
antialiasing, 414–416
blending, 405–406
colors, 263–264
depth buffering, 374–375

Index 671
diffuse light, 329–330
directional light source, 315
emissive light, 325
ﬂat-shaded lighting, 340
lighting approximation, 312–313
look at, 210–211
matrix stack, 252
mipmapping, 399–401
per-vertex lighting, 341
point light source, 316–318
projection, 226–230
rendering curves, 456–459
specular highlight, 333
spotlights, 322
surface materials, 323–324
texture magniﬁcation, 388–389
texture miniﬁcation, 401
textures, 286–289
triangle attributes, 270–271
triangle culling and, 272
triangles, 268–269
vertex indices, 274–276
vertices, 265–266
operator, 66
orientation
interpolation, 471–472, 501–511
rotational dynamics, 599–600
orientation formats
axis-angle, 481–485
ﬁxed and Euler angles, 474–481
quaternions, 485–501
rotation matrices, 115–124, 473–474
origin, 41, 44
orthogonal matrix, 98
orthogonalization, Gram-Schmidt,
33–34, 207, 503
orthographic parallel projection,
229–231
orthographic projection, 213
orthonormal basis vectors, 33
overdraw, 363
overﬂow, 158–159
ﬁxed point numbers, 170–172
P
painter’s algorithm, 362
parallel projection, 213
parallel vectors, 19
parameterized lines, 53–55
parameterized planes, 57–58
parametric curves. See curves,
parametric
parametric surfaces, 267
partial pivoting, 92
penalty method, 619
pen-plotter, 354
perpendicular, 36–37
perpendicular dot product, 37
per-pixel lighting, 344–348
perspective correct, 378, 383
perspective projection, 212, 220–227
per-triangle mipmapping, 397
per-vertex lighting, 340–344
Phong shading, 344–348
pick ray, 248–250
piecewise Bézier curves, 443–444
piecewise linear interpolation,
424–425
pitch angle, 475
pivoting step, 92
pixel(s)
antialiasing, 406–416
blending, 401–406
computing depth values, 368–371
computing source colors,
375–378
-coverage antialiasing, 414
determining, contained by
a triangle, 360–362
determining visibility of object at,
362–375
shading, 263
texturing, with mipmaps, 395–398
z-buffering, 372–374
planes
clipping, 238–243
coplanar points, 60
deﬁned, 57
generalized equation, 58–59
parameterized, 57–58
transforming normals, 134–135
point at inﬁnity, 314
point clouds, 267
point light source, 315–318

672
Index
points
afﬁne combinations, 46–47
afﬁne spaces, 43–45
collinear, 57
coplanar, 60
deﬁned, 41
as geometry, 41–43
implementation, 47–49
polar and spherical coordinates,
49–52
role of, 11
polar coordinates, 49–50
polar decomposition, 142–143
polygons, 60–63
polynomials
Bernstein, 441
Lagrange, 425–427
positional light source, 315–318
predictor-corrector method, 597
procedural texturing or shading,
304–307
projection
dot product as, 32
plane, 212
projective transformations
cabinet, 214
cavalier, 213–214
deﬁned, 211–214
homogeneous coordinates, 219
normalized device coordinates,
217–219
oblique parallel projection, 213,
231–233
oblique perspective projection,
213–214, 227–229
orthographic parallel projection,
213, 229–231
perspective projection, 212, 220–227
view frustum, 215–217
Pythagorean identities, 629
Q
quadrilaterals, 60
quaternions
adding and multiplying by scalar,
493
concatenation, 495–496
deﬁned, 485–486
dot product, 494
format conversion, 489–492
identity and inverse, 497–498
magnitude and normalization,
493–494
negation, 493
product, 495–496
rotation, 486–489
transformation and, 501
vector rotation by, 498–500
quiet not a number (QNaN), 187
R
radiance, 312
radix point, 163
range, 66, 68
ﬁxed point numbers and, 163–166
ﬂoating point numbers and, 179–181
number representation and,
156–159
rank
linear transformation, 68
matrix, 80
raster display, 354
rasterization
antialiasing, 406–416
blending, 401–406
computing source pixel colors,
375–378
deﬁned, 353, 355
determining pixels contained by
a triangle, 360–362
determining visibility of pixels,
362–375
displays and framebuffers, 355–359
stages of, 360
textures and, 378–401
vector display hardware, 353–355
rational curves, 448–450
ray casting, 573
ray tracing, 312
real numbers, 15–16
See also computer number
representation

Index 673
converting ﬁxed point numbers
to and from, 164–166
representing, 159–161
real projective space, 219
real vector spaces, 15–18
reduced row echelon, 90
reﬂection
afﬁne transformations, 126–130
matrices, 102
relative error
ﬁxed point numbers, 160–161, 164
ﬂoating point numbers, 181
Render(), 148, 149
resampling, 420
RGB color model, 256–257
right-hand rule, 34–35, 40, 43
rigid bodies, deﬁned, 578
rigid body dynamics
collision response, 607–620
initial value problems, 585–598
linear, 578–584
rotational dynamics, 599–607
rigid transformations, 109
Rodrigues rotation formula, 123
roll angle, 475
rotation
afﬁne transformation, 115–124
angle of, 115
axis of, 115
matrices, 102, 473–474
pure, 116
quaternions, 486–489
rotational collision response,
616–619
rotational dynamics
angular momentum and inertial
tensor, 603–605
deﬁned, 599
integrating rotational quantities,
605–607
orientation and angular velocity,
599–600
torque, 600–603
rounding and conversion from real
numbers to ﬁxed point, 165
rounding modes, for ﬂoating point
numbers, 184
row echelon form, 90
row major order, 85
row space, 80
rsq, 307
Runga-Kutta methods, 593
Runga-Kutta order four (RK4), 593
S
scalar, 15
triple product, 38–40
scaling, 124–126
scanlines, 357
scene graphs, 148–152
scientiﬁc notation, 173–177
scissoring, 237
Screen afﬁne, 371
screen transformation, 245–247
shading
See also coloring surfaces; light,
shading and ﬂat, 277–278
Gouraud, 279–282
procedural, 304–307
sharp edges, 282–283, 344
shear, 130–132
signaling not a number (SNaN),
187–188
signed angle, 37
SIMD instruction, 24
singular value decomposition (SVD),
142, 143
skew symmetric matrix, 74
solution set, 88
sorting, depth, 362–365
space curves, 421, 501, 651–653
spans, 19
specular highlight, 330–335, 349–351
spheres
deﬁned, 530–535
swept, 546–550
spheres, intersections
sphere-plane, 537–538
sphere-ray, 536–537
sphere-sphere, 535–536
spherical coordinates, 50–52
spherical linear interpolation (slerp),
507–510

674
Index
splines
B-, 444–448
Catmull-Rom, 438–440
clamped cubic, 434
end conditions, 435–437
normalized cubic, 435
spotlights, 318–322
square matrix, 71
sqrtf(), 27
SSE (Streaming SIMD Extensions),
24–25, 197–198
stiff systems, 590
storage formats, color, 262–263
straight lines, 52
subspace, 18
subtracting
ﬁxed points, 166
ﬂoating points, 183
vectors, 13
surface representation
See also coloring surfaces
triangle attributes, 269–272
triangles in OpenGL, 268–269
vertex indices, 272–276
vertices and ambiguity, 267–268
Sutherland-Hodgeman algorithm, 239
sweep-and-prune method, 570
swept spheres, 546–550
symmetric matrix, 74
T
Taylor’s series, 643–644
template metaprogramming, 24
tensor product, 84
tessellation, 268
texels, 285
address, 379
centers, 379
coordinates, 379
fractional coordinates, 385
mapping texture coordinates to,
383–391
texture application mode, 348
texture coordinates
clamping, 298–300
discontinuities, 294–295
generating, 293–294
how to use, 289–291
interpolating, 380–383
mapping, 292–293
mapping outside unit square,
296–300
mapping to a texel, 383–391
review of, 379–380
wrapping, 296–298
texture ﬁltering, 383–391
mipmaps and, 398–399
texture mapping
how to use, 284–285
image lookup, 285
images, 285–289
nearest-neighbor, 291
textures (texturing)
clamping, 298–301
limitations of, 303–304
magnifying, 384–389
merging lighting and, 348–351
minifying, 389–391
mipmapping, 391–401
nearest neighbor, 384
procedural, 304–307
rasterizing, 378–401
repeating, 296
resident versus nonresident
(OpenGL), 289
steps, review of, 301–303
tiling, 296
wrapping, 296–298
3DNow! architecture, 24–25, 198
torque, 600–603
trace, of a matrix, 72
transformations
See also afﬁne transformations;
linear transformations; projective
transformations
concatenation (composition), 81–83
deﬁned, 65
quaternions and, 501
screen, 245–247
view, 210–211, 250–252
translation, 113–115
transpose, matrix, 74
trapezoidal rule, 648–649

Index 675
triangles, 60–63
See also surface representation
assigning colors to, 277–278
attributes, 269–272
culling, 269–272
determining pixels contained by,
360–362
in OpenGL, 268–269
properties of, 626–629
ratios on the right, 623–624
strips, 275–276
triangle-plane intersection, 562
triangle-ray intersection, 558–562
triangle-triangle intersection,
557–558
tridiagonal matrix, 434
trigonometry, 623–633
trilinear interpolation (trilerp), 398
triple products, scalar and vector,
37–40
tristrips, 275–276
24-bit color, 262–263
true color, 262–263
U
underﬂow
ﬁxed point numbers, 170–172
ﬂoating point numbers, 189–190
unit vector, 12
unsigned char (C/C++ data type), 157
unsigned int (C/C++ data type), 157
overﬂow, 158–159
range and type conversion, 159
unsigned short (C/C++ data type), 157
unweighted area sampling, 411
upper triangular matrix, 72
V
vector(s)
adding, 13
class implementation, 22–25
color as, 257
cross product, 34–37
deﬁned, 11
dot product, 28–32
as geometry, 12–15
Gram-Schmidt orthogonalization,
33–34
length (magnitude), 12, 25–28, 258
linear combinations and basis,
18–22
linear transformations and, 69–71
matrices and representation of, 75
normal, 12, 36
orthonormal basis, 33
parallel, 19
perpendicular, 36–37
scalar multiplication, 14–15
spaces, real, 15–18, 257
subtracting, 13
transforming, 79–81
triple product, 37–40
unit (normalized), 12
vector display hardware, 353–355
vector product. See cross product
vector rotation
axis-angle, 484–485
ﬁxed/Euler angles, 478–479
quaternions, 498–500
velocity, 579
angular, 599–600
Verlet, 595
Verlet methods, 594–595
vertex (vertices), 60
assigning colors to, 283
deﬁned, 265
indices, 272–276
lighting, 340–341
normals, generating, 341–344
in OpenGL, 265–266, 274–276, 341
surface ambiguity and, 267–268
view
direction vector, 204
ﬁeld of, 215–216
frame, 205–207
frustum (volume), 215–217
frustum culling, 269–270
pick ray, 248–250
plane, 212
position, 204
side vector, 205
space origin, 204

676
Index
view (continued)
transformation, 210–211, 250–252
up vector, 205
window, 215
viewing
camera, controlling, 207–209
camera, deﬁning, 204–207
visible surface determination
deﬁned, 362
depth buffering, 365–375
depth sorting, 362–365
W
weighted area sampling, 411
whole numbers, 156, 157
world frame, 136
world space, 136
Y
yaw angle, 475
Z
z-buffering, 372–374
blending, 403–404
zero
ﬂoating point, 185–186
hole at, 188–189
matrix, 71, 96

Trademarks
The following trademarks, mentioned in this book and the accompanying
CD-ROM, are the property of the following organizations:
3D Studio Max is a trademark of Autodesk, Inc.
AMD, K6, 3DNow! and combinations thereof are trademarks of Advanced
Micro Devices, Inc.
ARM is a trademark of ARM Limited.
Asteroids, Battlezone, and Tempest are trademarks and © of Atari Interactive,
Inc.
CodeWarrior is a trademark of Metrowerks Corp.
DirectX, Direct3D, Visual C++, and Windows are trademarks of Microsoft
Corporation.
Intel, StrongARM, XScale, Pentium, SSE, and Streaming SIMD Extensions
are trademarks of Intel Corporation.
Macintosh, Mac, Mac OS, and Xcode are trademarks of Apple Computer, Inc.
Maya is a trademark of Alias Systems.
NVIDIA and Cg are trademarks of NVIDIA Corporation.
OpenGL is a trademark of Silicon Graphics, Inc.
Playstation2 is a trademark of Sony Computer Entertainment, Inc.
Quake is a trademark of Id Software, Inc.

About the CD-ROM
Introduction
Many of the concepts in this book are visual, dynamic, or both. While static illustrations are used
throughout the book to illuminate some of these concepts, the truly dynamic concepts can be best
understood only via experiencing them in an interactive illustration. Computer-based examples
serve this purpose quite well.
This book includes a CD-ROM that contains numerous interactive demonstration programs
for concepts discussed in the book. The demos are supported on Windows (2000 and XP), MacOS
(OS X), and Linux. The main contents of the CD-ROM are:
■Pre-compiled versions of the demos for Windows, ready to run. These are likely to be useful
to the widest range of readers of the book, as they are ready to use as supplied, and can be
experienced quickly, with book in hand.
■Source for all of the demos, ready to edit and recompile on all platforms. For many students,
this is an excellent way to start tinkering with actual graphics, animation and simulation
code. The demos can form excellent launching pads for further experimentation.
■Source for the graphics and math libraries used to create the demos. These libraries can
form the basis of even more complex graphics applications, especially the low-level math-
ematics libraries. In addition, the source to these libraries is used as a set of design and
implementation examples throughout the book.
Updates
To distribute updates and corrections to this code as well as new demos, a webpage has been
established for this book at www.essentialmath.com. Please visit this site before using the included
CD-ROM to read any important news or updates regarding the CD-ROM that were added
following the production of the book’s CD-ROM.
Installing the CD-ROM
In order to use the CD-ROM, simply insert the disc into a CD-ROM drive that is mounted on
the computer and use the ﬁle explorer or command prompt to open the top-level directory of
the disc.
Getting Started
There are two ﬁles that anyone planning to use the CD-ROM should read prior to copying and
using the demos or any of the code. The ﬁrst of these ﬁles is the license information, LICENSE.PDF.

This ﬁle details the license agreement that all users are bound by when using the demo code. The
“grant” clause of this software license agreement (“SLA”) are as follows:
1. Grant. We grant you a nonexclusive, nontransferable, and perpetual license to use The Software subject to
the terms and conditions of the Agreement:
a)
You must own a copy of The Book (“Own The Book”) to use The Software. Ownership of one book by
two or more people does not satisfy the intent of this constraint.
b)
The Software may be used by you for noncommercial products. A noncommercial product is one that
you create for yourself as well as for others to use at no charge. If you redistribute any portion of the
source code of The Software to another person, that person must Own The Book. Redistribution of
any portion of the source code of The Software to a group of people requires each person in that group
to Own The Book. Redistribution of The Software in binary format, either as part of an executable
program or as part of a dynamic link library, is allowed with no obligation to Own The Book by the
receiving person(s), subject to the constraint in item (d).
c)
The Software may be used by you for commercial products. The source code of The Software may not
be redistributed with a commercial product. Redistribution of The Software in binary format, either
as part of an executable program or as part of a dynamic link library, is allowed with no obligation to
Own The Book by the receiving person(s), subject to the constraint in item (d). Each member of a
development team for a commercial product must Own The Book.
d)
Redistribution of The Software in binary format, either as part of an executable program or as part of
a dynamic link library, is allowed. The intent of this Agreement is that any product, whether
noncommercial or commercial, is not built solely to wrap The Software for the purposes of
redistributing it or selling it as if it were your own product. The intent of this clause is that you use
The Software, in part or in whole, to assist you in building your own original products. An example of
acceptable use is to incorporate the rendering portion of The Software in a game to be sold to an end
user. An example that violates this clause is to compile a library from only The Software, bundle it
with the headers ﬁles as a Software Development Kit (SDK), then sell that SDK to others. If there is
any doubt about whether you can use The Software for a commercial product, contact us and explain
what portions you intend to use. We will consider creating a separate legal document that grants you
permission to use those portions of The Software in your commercial product.
2. Limitation of Liability. The Publisher warrants the media on which the software is furnished to be free
from defects in materials and workmanship under normal use for 30 days from the date that you obtain the
Product. The warranty set forth above is the exclusive warranty pertaining to the Product, and the Publisher
disclaims all other warranties, express or implied, including, but not limited to, implied warranties of
merchantability and ﬁtness for a particular purpose, even if the Publisher has been advised of the possibility
of such purpose. Some jurisdictions do not allow limitations on an implied warranty’s duration, therefore the
above limitations may not apply to you.
3. Limited Warranty. Your exclusive remedy for breach of this warranty will be the repair or replacement of the
Product at no charge to you or the refund of the applicable purchase price paid upon the return of the Product,
as determined by the Publisher in its discretion. In no event will the Publisher, and its directors, ofﬁcers,
employees, and agents, or anyone else who has been involved in the creation, production, or delivery of this
software be liable for indirect, special, consequential, or exemplary damages, including, without limitation,
for lost proﬁts, business interruption, lost or damaged data, or loss of goodwill, even if the Publisher or
an authorized dealer or distributor or supplier has been advised of the possibility of such damages. Some
jurisdictions do not allow the exclusion or limitation of indirect, special, consequential, or exemplary damages
or the limitation of liability to speciﬁed amounts, therefore the above limitations or exclusions may not apply
to you.
The full details may be found in the license ﬁle on the CD-ROM.
The second set of ﬁles that any user should read are the “read me” ﬁles. The general “read me”
ﬁle, README_FIRST.TXT relates information that is pertinent to all users of the code. In addition,

there are README ﬁles for each of the supported platforms. Put together, these ﬁles contain a wide
range of information, including:
■Descriptions of supported platforms, hardware, and development tools
■Instructions on how to prepare your computer to run the demos on each of the supported
platforms
■Instructions on how to build the engine libraries and demos themselves (on each of the
supported platforms)
■Known issues with any of the demos or libraries
The book makes many references in its text to these demos, where appropriate, using the icons
described in the introduction to the book. However, there are additional, unreferenced demos
that were written after the book text was ﬁnalized. These newer demos are available on the CD-
ROM, but are not referenced in the text. Please refer to the README_FIRST.TXT ﬁle in the root
directory of the CD-ROM, as well as the demo directories for each chapter for additional demos
not referenced in the book text.

Contents of the CD-ROM
Shared Libraries Directory
/common
Contains the source and build conﬁguration ﬁles for the support libraries (collectively known as Iv)
described in the book and used to create the book’s demos
/Includes
Contains copies of all of the headers from the Iv directories listed below
/IvCollision
Contains bounding volume classes and intersection methods
/IvCurves
Contains position-interpolating curve classes
/IvEngine
Contains classes and functions that support interactive application development
/IvMath
Contains foundation mathematical classes such as vectors
/IvScene
Contains classes implementing a basic hierarchical scene graph
/IvUtility
Contains low-level system support code such as ﬁle I/O
/Libs
Contains the libraries built by each of the Iv library directories
Examples Directory
/Examples
Contains all of the demo applications referenced by the book text
/Ch03-Xforms
Contains the demos for Chapter 3: Afﬁne Transformations
/Transforms-01-Interaction
/Transforms-02-Centered
/Transforms-03-Separate
/Transforms-04-Tank
/Transforms-05-SceneGraph
/Ch05-Viewing
Contains the demos for Chapter 5: Viewing and Projection
/Viewing-01-LookAt
/Viewing-02-Rotation
/Viewing-03-Perspective
/Viewing-04-Stereo
/Viewing-05-Orthographic
/Viewing-06-Oblique
/Viewing-07-Clipping
/Viewing-08-Picking

Contents of the CD-ROM (continued)
/Ch06-GeometryColoring
Contains the demos for Chapter 6: Geometry, Shading, and Texturing
/Geometry-01-BasicSphere
/Geometry-02-IndexedGeom
/Geometry-03-BasicShading
/Geometry-04-BasicTexturing
/Geometry-05-TextureWrapping
/Ch07-Lighting
Contains the demos for Chapter 7: Lighting
/Lighting-01-DistanceAttenuation
/Lighting-02-Spotlight
/Lighting-03-Components
/Lighting-04-Edges
/Lighting-05-Textures
/Ch08-Raster
Contains the demos for Chapter 8: Rasterization
/Raster-01-DepthBuffering
/Raster-02-TextureFilter
/Raster-03-Mipmapping
/Raster-04-Blending
/Ch09-Curves
Contains the demos for Chapter 9: Curves
/Curves-01-Linear
/Curves-02-Lagrange
/Curves-03-Hermite
/Curves-04-AutoHermite
/Curves-05-Catmull
/Curves-06-Bezier
/Curves-07-B-Spline
/Curves-08-SpeedControl
/Curves-09-CameraControl
/Ch10-Orientation
Contains the demos for Chapter 10: Orientation Representation
/Orientation-01-Euler
/Orientation-02-Transform
/Orientation-03-LerpSlerp
/Ch11-Collision
Contains the demos for Chapter 11: Intersection Testing
/Collision-01-Hierarchy
/Collision-02-SweepPrune
/Ch12-Simulation
Contains the demos for Chapter 12: Rigid Body Dynamics
/Simulation-01-Force
/Simulation-02-Torque
/Simulation-03-Tank
/Simulation-04-LinCollision
/Simulation-05-AngCollision
/gl
Contains the GLUT headers and library ﬁles that are needed to build the Iv libraries and demo
applications
/Win32System
Contains Windows runtime DLL(s) required by the GLUT system

Glossary of Notation
Scalars
W, Z, R
whole numbers, integers, real numbers
[a, b],(a, b)
closed interval, open interval
|a|, ⌊a⌋, ⌈a⌉
absolute value, ﬂoor, ceiling
min(a, ..., b),
max(a, ..., b)
minimum of a set of scalars, maximum of a set of
scalars
Vectors, Points, and Lines
R2,R3,Rn
pairs of real numbers, triples of real numbers, n-tuples
of real numbers
v, vT, 0, vi
vector, vector transpose, zero vector, vector element
{i, j}, {i, j, k}
standard basis in R2, standard basis in R3
u · v, u × v, u ⊗v
dot product, cross product, tensor product
∥v∥, ˆv, v⊥
vector length or norm, unit length vector, vector
perpendicular to v
projwv
projection of vector v on vector w
P, P Q, dist(P,Q)
point, line segment, distance between two points
L(t), P(u, v), Q(u)
parameterized line, parameterized plane,
parameterized curve
Matrices and Transformations
T, T−1, T ◦S
transformation, inverse transformation,
transformation composition
M, M−1, MT , I
matrix, matrix inverse, matrix transpose, identity
matrix
mi,j or (M)i,j
matrix element at row i and column j
˜w
skew-symmetric matrix representing cross product
by w

det(M) or |M|, Madj
matrix determinant, adjoint matrix
Rv, 
rotation of vector v, orientation
v||, v⊥
part of v parallel to rotation, part of vector v orthogonal
to rotation
(x, y, z, w), RP 3
homogeneous point, homogeneous space
Functions and Calculus
f (x), f ′(x), f ′′(x)
function, ﬁrst derivative, second derivative
dy
dx , ˙y
ﬁrst derivative with respect to x, ﬁrst derivative with
respect to time
∂y
∂x
partial derivative of y with respect to x
$b
i=a, 0b
i=a,
%
,
% b
a
summation, product, indeﬁnite integral, deﬁnite
integral
C0, C1, C2, G1
positional, tangential, curvature, and geometric
continuity
Orientation
q, q−1, qvq−1
quaternion, inverse quaternion, rotation of vector v by
quaternion
Simulation
X, v, a, F, P, m
position, velocity, acceleration, force, linear
momentum, mass
ω, τ, L, J
angular velocity, torque, angular momentum, inertial
tensor matrix


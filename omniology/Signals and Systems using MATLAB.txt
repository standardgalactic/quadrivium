
Signals and Systems
Using MATLAB
Luis F. Chaparro
Department of Electrical and Computer Engineering
University of Pittsburgh
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Academic Press is an imprint of Elsevier

Academic Press is an imprint of Elsevier
30 Corporate Drive, Suite 400, Burlington, MA 01803, USA
Elsevier, The Boulevard, Langford Lane, Kidlington, Oxford, OX5 1GB, UK
Copyright c⃝2011 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical,
including photocopying, recording, or any information storage and retrieval system, without permission in writing from
the publisher. Details on how to seek permission, further information about the Publisher’s permissions policies and our
arrangements with organizations such as the Copyright Clearance Center and the Copyright Licensing Agency, can be
found at our website: www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by the Publisher (other than
as may be noted herein). MATLAB R⃝is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks
does not warrant the accuracy of the text or exercises in this book. This books use or discussion of MATLAB R⃝software
or related products does not constitute endorsement or sponsorship by The MathWorks of a particular pedagogical
approach or particular use of the MATLAB R⃝software.
Notices
Knowledge and best practice in this ﬁeld are constantly changing. As new research and experience broaden our
understanding, changes in research methods, professional practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using any
information, methods, compounds, or experiments described herein. In using such information or methods they
should be mindful of their own safety and the safety of others, including parties for whom they have a
professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability
for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise,
or from any use or operation of any methods, products, instructions, or ideas contained in the material herein.
Library of Congress Cataloging-in-Publication Data
Chaparro, Luis F.
Signals and systems using MATLAB
/ Luis F. Chaparro.
p. cm.
ISBN 978-0-12-374716-7
1. Signal processing–Digital techniques. 2. System analysis. 3. MATLAB. I. Title.
TK5102.9.C472 2010
621.382’2–dc22
2010023436
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
For information on all Academic Press publications
visit our Web site at www.elsevierdirect.com
Printed in the United States of America
10 11 12 13
9 8 7 6 5 4 3 2 1
R⃝

To my family, with much love.

Contents
PREFACE.....................................................................................................................
xi
ACKNOWLEDGMENTS ................................................................................................ xvi
Part 1
Introduction
1
CHAPTER 0
From the Ground Up!.............................................................................
3
0.1
Signals and Systems and Digital Technologies........................................
3
0.2
Examples of Signal Processing Applications ...........................................
5
0.2.1
Compact-Disc Player ................................................................
5
0.2.2
Software-Deﬁned Radio and Cognitive Radio...............................
6
0.2.3
Computer-Controlled Systems ...................................................
8
0.3
Analog or Discrete? .............................................................................
9
0.3.1
Continuous-Time and Discrete-Time Representations ..................
10
0.3.2
Derivatives and Finite Differences .............................................
12
0.3.3
Integrals and Summations.........................................................
13
0.3.4
Differential and Difference Equations .........................................
16
0.4
Complex or Real? ................................................................................
20
0.4.1
Complex Numbers and Vectors..................................................
20
0.4.2
Functions of a Complex Variable................................................
23
0.4.3
Phasors and Sinusoidal Steady State ..........................................
24
0.4.4
Phasor Connection ...................................................................
26
0.5
Soft Introduction to MATLAB ...............................................................
29
0.5.1
Numerical Computations ..........................................................
30
0.5.2
Symbolic Computations ............................................................
43
Problems............................................................................................
53
Part 2
Theory and Application of Continuous-Time
Signals and Systems
63
CHAPTER 1
Continuous-Time Signals .........................................................................
65
1.1
Introduction .......................................................................................
65
1.2
Classiﬁcation of Time-Dependent Signals...............................................
66
iv

Contents
v
1.3
Continuous-Time Signals .....................................................................
67
1.3.1
Basic Signal Operations—Time Shifting and Reversal...................
71
1.3.2
Even and Odd Signals ..............................................................
75
1.3.3
Periodic and Aperiodic Signals ..................................................
77
1.3.4
Finite-Energy and Finite Power Signals ......................................
79
1.4
Representation Using Basic Signals.......................................................
85
1.4.1
Complex Exponentials..............................................................
85
1.4.2
Unit-Step, Unit-Impulse, and Ramp Signals .................................
88
1.4.3
Special Signals—the Sampling Signal and the Sinc ....................... 100
1.4.4
Basic Signal Operations—Time Scaling, Frequency Shifting,
and Windowing ....................................................................... 102
1.4.5
Generic Representation of Signals.............................................. 105
1.5
What Have We Accomplished? Where Do We Go from Here?.................... 106
Problems............................................................................................ 108
CHAPTER 2
Continuous-Time Systems ....................................................................... 117
2.1
Introduction ....................................................................................... 117
2.2
System Concept.................................................................................. 118
2.2.1
System Classiﬁcation................................................................ 118
2.3
LTI Continuous-Time Systems .............................................................. 119
2.3.1
Linearity ................................................................................. 120
2.3.2
Time Invariance....................................................................... 125
2.3.3
Representation of Systems by Differential Equations.................... 130
2.3.4
Application of Superposition and Time Invariance ....................... 135
2.3.5
Convolution Integral................................................................. 136
2.3.6
Causality ................................................................................ 143
2.3.7
Graphical Computation of Convolution Integral ........................... 145
2.3.8
Interconnection of Systems—Block Diagrams .............................. 147
2.3.9
Bounded-Input Bounded-Output Stability ................................... 153
2.4
What Have We Accomplished? Where Do We Go from Here?.................... 156
Problems............................................................................................ 157
CHAPTER 3
The Laplace Transform ............................................................................ 165
3.1
Introduction ....................................................................................... 165
3.2
The Two-Sided Laplace Transform ........................................................ 166
3.2.1
Eigenfunctions of LTI Systems................................................... 167
3.2.2
Poles and Zeros and Region of Convergence ............................... 172
3.3
The One-Sided Laplace Transform ........................................................ 176
3.3.1
Linearity ................................................................................. 185
3.3.2
Differentiation ......................................................................... 188
3.3.3
Integration.............................................................................. 193
3.3.4
Time Shifting........................................................................... 194
3.3.5
Convolution Integral................................................................. 196

vi
Contents
3.4
Inverse Laplace Transform ................................................................... 197
3.4.1
Inverse of One-Sided Laplace Transforms ................................... 197
3.4.2
Inverse of Functions Containing e−ρs Terms ................................ 209
3.4.3
Inverse of Two-Sided Laplace Transforms................................... 212
3.5
Analysis of LTI Systems ....................................................................... 214
3.5.1
LTI Systems Represented by Ordinary Differential Equations ........ 214
3.5.2
Computation of the Convolution Integral .................................... 221
3.6
What Have We Accomplished? Where Do We Go from Here?.................... 226
Problems............................................................................................ 226
CHAPTER 4
Frequency Analysis: The Fourier Series .................................................. 237
4.1
Introduction ....................................................................................... 237
4.2
Eigenfunctions Revisited ..................................................................... 238
4.3
Complex Exponential Fourier Series ...................................................... 245
4.4
Line Spectra ....................................................................................... 248
4.4.1
Parseval’s Theorem—Power Distribution over Frequency ............. 248
4.4.2
Symmetry of Line Spectra ......................................................... 250
4.5
Trigonometric Fourier Series ................................................................ 251
4.6
Fourier Coefﬁcients from Laplace.......................................................... 255
4.7
Convergence of the Fourier Series......................................................... 265
4.8
Time and Frequency Shifting................................................................ 270
4.9
Response of LTI Systems to Periodic Signals........................................... 273
4.9.1
Sinusoidal Steady State............................................................. 274
4.9.2
Filtering of Periodic Signals....................................................... 276
4.10
Other Properties of the Fourier Series .................................................... 279
4.10.1
Reﬂection and Even and Odd Periodic Signals ............................. 279
4.10.2
Linearity of Fourier Series—Addition of Periodic Signals............... 282
4.10.3
Multiplication of Periodic Signals ............................................... 284
4.10.4
Derivatives and Integrals of Periodic Signals ............................... 285
4.11
What Have We Accomplished? Where Do We Go from Here?.................... 289
Problems............................................................................................ 290
CHAPTER 5
Frequency Analysis: The Fourier Transform ........................................... 299
5.1
Introduction ....................................................................................... 299
5.2
From the Fourier Series to the Fourier Transform .................................... 300
5.3
Existence of the Fourier Transform ....................................................... 302
5.4
Fourier Transforms from the Laplace Transform ..................................... 302
5.5
Linearity, Inverse Proportionality, and Duality ........................................ 304
5.5.1
Linearity ................................................................................. 304
5.5.2
Inverse Proportionality of Time and Frequency............................ 305
5.5.3
Duality ................................................................................... 310

Contents
vii
5.6
Spectral Representation....................................................................... 313
5.6.1
Signal Modulation .................................................................... 313
5.6.2
Fourier Transform of Periodic Signals ......................................... 317
5.6.3
Parseval’s Energy Conservation................................................. 320
5.6.4
Symmetry of Spectral Representations........................................ 322
5.7
Convolution and Filtering..................................................................... 327
5.7.1
Basics of Filtering .................................................................... 329
5.7.2
Ideal Filters............................................................................. 332
5.7.3
Frequency Response from Poles and Zeros.................................. 337
5.7.4
Spectrum Analyzer................................................................... 341
5.8
Additional Properties .......................................................................... 344
5.8.1
Time Shifting .......................................................................... 344
5.8.2
Differentiation and Integration .................................................. 346
5.9
What Have We Accomplished? What Is Next? ....................................... 350
Problems............................................................................................ 350
CHAPTER 6
Application to Control and Communications ........................................... 359
6.1
Introduction ....................................................................................... 359
6.2
System Connections and Block Diagrams ............................................... 360
6.3
Application to Classic Control............................................................... 363
6.3.1
Stability and Stabilization ......................................................... 369
6.3.2
Transient Analysis of First- and Second-Order Control Systems ..... 371
6.4
Application to Communications ............................................................ 377
6.4.1
AM with Suppressed Carrier ..................................................... 379
6.4.2
Commercial AM....................................................................... 380
6.4.3
AM Single Sideband ................................................................. 382
6.4.4
Quadrature AM and Frequency-Division Multiplexing .................. 383
6.4.5
Angle Modulation .................................................................... 385
6.5
Analog Filtering.................................................................................. 390
6.5.1
Filtering Basics........................................................................ 390
6.5.2
Butterworth Low-Pass Filter Design........................................... 393
6.5.3
Chebyshev Low-Pass Filter Design ............................................ 396
6.5.4
Frequency Transformations ...................................................... 402
6.5.5
Filter Design with MATLAB ...................................................... 405
6.6
What Have We Accomplished? What Is Next? ........................................ 409
Problems............................................................................................ 409
Part 3
Theory and Application of Discrete-Time
Signals and Systems
417
CHAPTER 7
Sampling Theory...................................................................................... 419
7.1
Introduction ....................................................................................... 419

viii
Contents
7.2
Uniform Sampling ............................................................................... 420
7.2.1
Pulse Amplitude Modulation ..................................................... 420
7.2.2
Ideal Impulse Sampling ............................................................ 421
7.2.3
Reconstruction of the Original Continuous-Time Signal ................ 428
7.2.4
Signal Reconstruction from Sinc Interpolation.............................. 432
7.2.5
Sampling Simulation with MATLAB ........................................... 433
7.3
The Nyquist-Shannon Sampling Theorem .............................................. 437
7.3.1
Sampling of Modulated Signals.................................................. 438
7.4
Practical Aspects of Sampling............................................................... 439
7.4.1
Sample-and-Hold Sampling ....................................................... 439
7.4.2
Quantization and Coding .......................................................... 441
7.4.3
Sampling, Quantizing, and Coding with MATLAB........................ 444
7.5
What Have We Accomplished? Where Do We Go from Here?.................... 446
Problems............................................................................................ 447
CHAPTER 8
Discrete-Time Signals and Systems ......................................................... 451
8.1
Introduction ..................................................................................... 451
8.2
Discrete-Time Signals.......................................................................... 452
8.2.1
Periodic and Aperiodic Signals .................................................. 454
8.2.2
Finite-Energy and Finite-Power Discrete-Time Signals ................. 458
8.2.3
Even and Odd Signals .............................................................. 461
8.2.4
Basic Discrete-Time Signals ...................................................... 465
8.3
Discrete-Time Systems ........................................................................ 478
8.3.1
Recursive and Nonrecursive Discrete-Time Systems..................... 481
8.3.2
Discrete-Time Systems Represented by Difference
Equations ............................................................................... 486
8.3.3
The Convolution Sum ............................................................... 487
8.3.4
Linear and Nonlinear Filtering with MATLAB.............................. 494
8.3.5
Causality and Stability of Discrete-Time Systems......................... 497
8.4
What Have We Accomplished? Where Do We Go from Here?.................... 502
Problems............................................................................................ 502
CHAPTER 9
The Z-Transform ...................................................................................... 511
9.1
Introduction ....................................................................................... 511
9.2
Laplace Transform of Sampled Signals................................................... 512
9.3
Two-Sided Z-Transform ....................................................................... 515
9.3.1
Region of Convergence............................................................. 516
9.4
One-Sided Z-Transform........................................................................ 521
9.4.1
Computing the Z-Transform with Symbolic MATLAB ................... 522
9.4.2
Signal Behavior and Poles ......................................................... 522
9.4.3
Convolution Sum and Transfer Function ..................................... 526

Contents
ix
9.4.4
Interconnection of Discrete-Time Systems................................... 537
9.4.5
Initial and Final Value Properties ............................................... 539
9.5
One-Sided Z-Transform Inverse ............................................................ 542
9.5.1
Long-Division Method .............................................................. 542
9.5.2
Partial Fraction Expansion ........................................................ 544
9.5.3
Inverse Z-Transform with MATLAB............................................ 547
9.5.4
Solution of Difference Equations ................................................ 550
9.5.5
Inverse of Two-Sided Z-Transforms............................................ 561
9.6
What Have We Accomplished? Where Do We Go from Here?.................... 564
Problems............................................................................................ 564
CHAPTER 10 Fourier Analysis of Discrete-Time Signals and Systems........................... 571
10.1
Introduction ....................................................................................... 571
10.2
Discrete-Time Fourier Transform .......................................................... 572
10.2.1
Sampling, Z-Transform, Eigenfunctions, and the DTFT ................. 573
10.2.2
Duality in Time and Frequency .................................................. 575
10.2.3
Computation of the DTFT Using MATLAB .................................. 577
10.2.4
Time and Frequency Supports ................................................... 580
10.2.5
Parseval’s Energy Result........................................................... 585
10.2.6
Time and Frequency Shifts........................................................ 587
10.2.7
Symmetry ............................................................................... 589
10.2.8
Convolution Sum ..................................................................... 595
10.3
Fourier Series of Discrete-Time Periodic Signals...................................... 596
10.3.1
Complex Exponential Discrete Fourier Series .............................. 599
10.3.2
Connection with the Z-Transform .............................................. 601
10.3.3
DTFT of Periodic Signals........................................................... 602
10.3.4
Response of LTI Systems to Periodic Signals ............................... 604
10.3.5
Circular Shifting and Periodic Convolution .................................. 607
10.4
Discrete Fourier Transform .................................................................. 614
10.4.1
DFT of Periodic Discrete-Time Signals ........................................ 614
10.4.2
DFT of Aperiodic Discrete-Time Signals...................................... 616
10.4.3
Computation of the DFT via the FFT .......................................... 617
10.4.4
Linear and Circular Convolution Sums ........................................ 622
10.5
What Have We Accomplished? Where Do We Go from Here?.................... 628
Problems............................................................................................ 629
CHAPTER 11 Introduction to the Design of Discrete Filters .......................................... 639
11.1
Introduction ....................................................................................... 639
11.2
Frequency-Selective Discrete Filters...................................................... 641
11.2.1
Linear Phase ........................................................................... 641
11.2.2
IIR and FIR Discrete Filters ....................................................... 643

x
Contents
11.3
Filter Speciﬁcations ............................................................................. 648
11.3.1
Frequency-Domain Speciﬁcations .............................................. 648
11.3.2
Time-Domain Speciﬁcations ...................................................... 652
11.4
IIR Filter Design.................................................................................. 653
11.4.1
Transformation Design of IIR Discrete Filters .............................. 654
11.4.2
Design of Butterworth Low-Pass Discrete Filters ......................... 658
11.4.3
Design of Chebyshev Low-Pass Discrete Filters........................... 666
11.4.4
Rational Frequency Transformations .......................................... 672
11.4.5
General IIR Filter Design with MATLAB ..................................... 677
11.5
FIR Filter Design................................................................................. 679
11.5.1
Window Design Method ........................................................... 681
11.5.2
Window Functions ................................................................... 683
11.6
Realization of Discrete Filters ............................................................... 689
11.6.1
Realization of IIR Filters............................................................ 690
11.6.2
Realization of FIR Filters ........................................................... 699
11.7
What Have We Accomplished? Where Do We Go from Here?.................... 701
Problems............................................................................................ 701
CHAPTER 12 Applications of Discrete-Time Signals and Systems................................. 709
12.1
Introduction ....................................................................................... 709
12.2
Application to Digital Signal Processing................................................. 710
12.2.1
Fast Fourier Transform ............................................................. 711
12.2.2
Computation of the Inverse DFT ................................................ 715
12.2.3
General Approach of FFT Algorithms ......................................... 716
12.3
Application to Sampled-Data and Digital Control Systems ........................ 722
12.3.1
Open-Loop Sampled-Data System .............................................. 724
12.3.2
Closed-Loop Sampled-Data System ............................................ 726
12.4
Application to Digital Communications.................................................. 729
12.4.1
Pulse Code Modulation............................................................. 730
12.4.2
Time-Division Multiplexing ....................................................... 733
12.4.3
Spread Spectrum and Orthogonal Frequency-Division
Multiplexing............................................................................ 735
12.5
What Have We Accomplished? Where Do We Go from Here?.................... 742
APPENDIX
Useful Formulas....................................................................................... 743
BIBLIOGRAPHY ........................................................................................................... 746
INDEX ......................................................................................................................... 749

Preface
In this book I have only made up a bunch
of other men’s ﬂowers, providing of my own
only the string that ties them together.
M. de Montaigne (1533–1592)
French essayist
Although it is hardly possible to keep up with advances in technology, it is reassuring to know that in science
and engineering, development and innovation are possible through a solid understanding of basic principles.
The theory of signals and systems is one of those fundamentals, and it will be the foundation of much research
and development in engineering for years to come. Not only engineers will need to know about signals and
systems—to some degree everybody will. The pervasiveness of computers, cell phones, digital recording, and
digital communications will require it.
Learning as well as teaching signals and systems is complicated by the combination of mathematical abstraction
and concrete engineering applications. Mathematical sophistication and maturity in engineering are needed.
Thus, a course in signals and systems needs to be designed to nurture the students’ interest in applications,
but also to make them appreciate the signiﬁcance of the mathematical tools. In writing this textbook, as in
teaching this material for many years, the author has found it practical to follow Einstein’s recommendation
that “Everything should be made as simple as possible, but not simpler,” and Melzak’s [47] dictum that “It is
downright sinful to teach the abstract before the concrete.” The aim of this textbook is to serve the students’
needs in learning signals and systems theory as well as to facilitate the teaching of the material for faculty by
proposing an approach that the author has found effective in his own teaching.
We consider the use of MATLAB, an essential tool in the practice of engineering, of great signiﬁcance in the learn-
ing process. It not only helps to illustrate the theoretical results but makes students aware of the computational
issues that engineers face in implementing them. Some familiarity with MATLAB is beneﬁcial but not required.
LEVEL
The material in this textbook is intended for courses in signals and systems at the junior level in electrical and
computer engineering, but it could also be used in teaching this material to mechanical engineering and bioengi-
neering students and it might be of interest to students in applied mathematics. The “student-friendly” nature
of the text also makes it useful to practicing engineers interested in learning or reviewing the basic principles of
signals and systems on their own. The material is organized so that students not only get a solid understand-
ing of the theory—through analytic examples as well as software examples using MATLAB—and learn about
applications, but also develop conﬁdence and proﬁciency in the material by working on problems.
xi

xii
Preface
The organization of the material in the book follows the assumption that the student has been exposed to the
theory of linear circuits, differential equations, and linear algebra, and that this material will be followed by
courses in control, communications, or digital signal processing. The content is guided by the goal of nurturing
the interest of students in applications, and of assisting them in becoming more sophisticated mathematically.
In teaching signals and systems, the author has found that students typically lack basic skills in manipulating
complex variables, in understanding differential equations, and are not yet comfortable with basic concepts in
calculus. Introducing discrete-time signals and systems makes students face new concepts that were not explored
in their calculus courses, such as summations, ﬁnite differences, and difference equations. This text attempts to
ﬁll the gap and nurture interest in the mathematical tools.
APPROACH
In writing this text, we have taken the following approach:
1.
The material is divided into three parts: introduction, theory and applications of continuous-time signals
and systems, and theory and applications of discrete-time signals and systems. To help students under-
stand the connection between continuous- and discrete-time signals and systems, the connection between
inﬁnitesimal and ﬁnite calculus is made in the introduction part, together with a motivation as to why com-
plex numbers and functions are used in the study of signals and systems. The treatment of continuous- and
discrete-time signals and systems is then done separately in the next two parts; combining them is found to
be confusing to students. Likewise, the author believes it is important for students to understand the connec-
tions and relevance of each of the transformations used in the analysis of signals and systems so that these
transformations are seen as a progression rather than as disconnected methods. Thus, the author advocates
the presentation of the Laplace analysis followed by the Fourier analysis, and the Z-transform followed by the
discrete Fourier, and capping each of these topics with applications to communications, control, and ﬁlter-
ing. The mathematical abstraction and the applications become more sophisticated as the material unfolds,
taking advantage as needed of the background on circuits that students have.
2.
An overview of the topics to be discussed in the book and how each connects with some basic mathematical
concepts—needed in the rest of the book—is given in Chapter 0 (analogous to the ground ﬂoor of a build-
ing). The emphasis is in relating summations, differences, difference equations, and sequence of numbers
with the calculus concepts that the students are familiar with, and in doing so providing a new interpreta-
tion to integrals, derivatives, differential equations, and functions of time. This chapter also links the theory
of complex numbers and functions to vectors and to phasors learned in circuit theory. Because we strongly
believe that the material in this chapter should be covered before beginning the discussion of signals and
systems, it is not relegated to an appendix but placed at the front of the book where it cannot be ignored. A
soft introduction to MATLAB is also provided in this chapter.
3.
A great deal of effort has been put into making the text “student friendly.” To make sure that the student does
not miss some of the important issues presented in a section, we have inserted well-thought-out remarks—
we want to minimize the common misunderstandings we have observed from our students in the past.
Plenty of analytic examples with different levels of complexity are given to illustrate issues. Each chapter
has a set of examples in MATLAB, illustrating topics presented in the text or special issues that the student
should know. The MATLAB code is given so that students can learn by example from it. To help students
follow the mathematical derivations, we provide extra steps whenever necessary and do not skip steps that
are necessary in the understanding of a derivation. Summaries of important issues are boxed and concepts
and terms are emphasized to help students grasp the main points and terminology.
4.
Without any doubt, learning the material in signals and systems requires working analytical as well as com-
putational problems. It is important to provide problems of different levels of complexity to exercise not
only basic problem-solving skills, but to achieve a level of proﬁciency and mathematical sophistication.
The problems at the end of the chapter are of different types, some to be done analytically, others using

Preface
xiii
MATLAB, and some both. The repetitive type of problem was avoided. Some of the problems explore issues
not covered in the text but related to it. The MATLAB problems were designed so that a better understanding
of the theoretical concepts is attained by the student working them out.
5.
We feel two additional features would be beneﬁcial to students. One is the inclusion of quotations and
footnotes to present interesting ideas or historical comments, and the other is the inclusion of sidebars that
attempt to teach historical or technical information that students should be aware of. The theory of signals
and systems clearly connects with mathematics and a great number of mathematicians have contributed to
it. Likewise, there is a large number of engineers who have contributed signiﬁcantly to the development and
application of signals and systems. All of them need to be recognized for their contributions, and we should
learn from their experiences.
6.
Finally, other features are: (1) the design of the index of the book so that it can be used by students to ﬁnd
deﬁnitions, symbols, and MATLAB functions used in the text; and (2) a list of references to the material.
CONTENT
The core of the material is presented in the second and third part of the book. The second part of the book
covers the basics of continuous-time signals and systems and illustrates their application. Because the concepts
of signals and systems are relatively new to students, we provide an extensive and complete presentation of these
topics in Chapters 1 and 2. The presentation in Chapter 1 goes from a very general characterization of signals
to very speciﬁc classes that will be used in the rest of the book. One of the aims is to familiarize students with
continuous-time as well as discrete-time signals so as to avoid confusion in their processing later on—a common
difﬁculty encountered by students. Chapter 1 initiates the representation of signals in terms of basic signals that
will be easily processed later with the transform methods. Chapter 2 introduces the general concept of systems,
in particular continuous-time systems. The concepts of linearity, time invariance, causality, and stability are
introduced in this chapter, trying as much as possible to use the students’ background in circuit theory. Using
linearity and time invariance, the computation of the output of a continuous-time system using the convolution
integral is introduced and illustrated with relatively simple examples. More complex examples are treated with
the Laplace transform in the following chapter.
Chapter 3 covers the basics of the Laplace transform and its application in the analysis of continuous-time
signals and systems. It introduces the student to the concept of poles and zeros, damping and frequency, and
their connection with the signal as a function of time. This chapter emphasizes the solution of differential
equations representing linear time-invariant (LTI) systems, paying special attention to transient solutions due
to their importance in control, as well as to steady-state solutions due to their importance in ﬁltering and in
communications. The convolution integral is dealt with in time and using the Laplace transform to emphasize
the operational power of the transform. The important concept of transfer function for LTI systems and the
signiﬁcance of its poles and zeros are studied in detail. Different approaches are considered in computing the
inverse Laplace transform, including MATLAB methods.
Fourier analysis of continuous-time signals and systems is covered in detail in Chapters 4 and 5. The Fourier
series analysis of periodic signals, covered in Chapter 4, is extended to the analysis of aperiodic signals resulting
in the Fourier transform of Chapter 5. The Fourier transform is useful in representing both periodic and aperi-
odic signals. Special attention is given to the connection of these methods with the Laplace transform so that,
whenever possible, known Laplace transforms can be used to compute the Fourier series coefﬁcients and the
Fourier transform—thus avoiding integration but using the concept of the region of convergence. The concept
of frequency, the response of the system (connected to the location of poles and zeros of the transfer function),
and the steady-state response are emphasized in these chapters.
The ordering of the presentation of the Laplace and the Fourier transformations (similar to the Z-transform
and the Fourier representation of discrete-time signals) is signiﬁcant for learning and teaching of the material.

xiv
Preface
Our approach of presenting ﬁrst the Laplace transform and then the Fourier series and Fourier transform is
justiﬁed by several reasons. For one, students coming into a signals and systems course have been familiarized
with the Laplace transform in their previous circuits or differential equations courses, and will continue using
it in control courses. So expertise in this topic is important and the learned material will stay with them longer.
Another is that a common difﬁculty students have in applying the Fourier series and the Fourier transform is
connected with the required integration. The Laplace transform can be used not only to sidestep the integration
but to provide a more comprehensive understanding of the frequency representation. By asking students to
consider the two-sided Laplace transform and the signiﬁcance of its region of convergence, they will appreciate
better the Fourier representation as a special case of Laplace’s in many cases. More importantly, these transforms
can be seen as a continuum rather than as different transforms. It also makes theoretical sense to deal with
the Laplace representation of systems ﬁrst to justify the existence of the steady-state solution considered in the
Fourier representations, which would not exist unless stability of the system is guaranteed, and stability can only
be tested using the Laplace transform. The paradigm of interest is the connection of transient and steady-state
responses that must be understood by students before they can understand the connections between Fourier and
Laplace analyses.
Chapter 6 presents applications of the Laplace and the Fourier transforms to control, communications, and ﬁl-
tering. The intent of the chapter is to motivate interest in these areas. The chapter illustrates the signiﬁcance of
the concepts of transfer function, response of systems, and stability in control, and of modulation in communi-
cations. An introduction to analog ﬁltering is provided. Analytic as well as MATLAB examples illustrate different
applications to control, communications, and ﬁlter design.
Using the sampling theory as a bridge, the third part of the book covers the theory and illustrates the application
of discrete-time signals and systems. Chapter 7 presents the theory of sampling: the conditions under which the
signal does not lose information in the sampling process and the recovery of the analog signal from the sampled
signal. Once the basic concepts are given, the analog-to-digital and digital-to-analog converters are considered
to provide a practical understanding of the conversion of analog-to-digital and digital-to-analog signals.
Discrete-time signals and systems are discussed in Chapter 8, while Chapter 9 introduces the Z-transform.
Although the treatment of discrete-time signals and systems in Chapter 8 mirrors that of continuous-time sig-
nals and systems, special emphasis is given in this chapter to issues that are different in the two domains. Issues
such as the discrete nature of the time, the periodicity of the discrete frequency, the possible lack of periodicity
of discrete sinusoids, etc. are considered. Chapter 9 provides the basic theory of the Z-transform and how it
relates to the Laplace transform. The material in this chapter bears similarity to the one on the Laplace trans-
form in terms of operational solution of difference equations, transfer function, and the signiﬁcance of poles and
zeros.
Chapter 10 presents the Fourier analysis of discrete signals and systems. Given the accumulated experience of
the students with continuous-time signals and systems, we build the discrete-time Fourier transform (DTFT) on
the Z-transform and consider special cases where the Z-transform cannot be used. The discrete Fourier transform
(DFT) is obtained from the Fourier series of discrete-time signals and sampling in frequency. The DFT will be
of great signiﬁcance in digital signal processing. The computation of the DFT of periodic and aperiodic discrete-
time signals using the fast Fourier transform (FFT) is illustrated. The FFT is an efﬁcient algorithm for computing
the DFT, and some of the basics of this algorithm are discussed in Chapter 12.
Chapter 11 introduces students to discrete ﬁltering, thus extending the analog ﬁltering in Chapter 6. In this
chapter we show how to use the theory of analog ﬁlters to design recursive discrete low-pass ﬁlters. Frequency
transformations are then presented to show how to obtain different types of ﬁlters from low-pass prototype
ﬁlters. The design of ﬁnite-impulse ﬁlters using the window method is considered next. Finally, the implementa-
tion of recursive and nonrecursive ﬁlters is shown using some basic techniques. By using MATLAB for the design
of recursive and nonrecursive discrete ﬁlters, it is expected that students will be motivated to pursue on their
own the use of more sophisticated ﬁlter designs.

Preface
xv
Finally, Chapter 12 explores topics of interest in digital communications, computer control, and digital signal
processing. The aim of this chapter is to provide a brief presentation of topics that students could pursue after
the basic courses in signals and systems.
TEACHING USING THIS TEXT
The material in this text is intended for a two-term sequence in signals and systems: one on continuous-time
signals and systems, followed by a term in discrete-time signals and systems with a lab component using MAT-
LAB. These two courses would cover most of the chapters in the text with various degrees of depth, depending
on the emphasis the faculty would like to give to the course. As indicated, Chapter 0 was written as a necessary
introduction to the rest of the material, but does not need to be covered in great detail—students can refer to it as
needed. Chapters 6 and 11 need to be considered together if the emphasis on applications is in ﬁlter design. The
control, communications, and digital signal processing material in Chapters 6 and 12 can be used to motivate
students toward those areas.
TO THE STUDENT
It is important for you to understand the features of this book, so you can take advantage of them to learn the
material:
1.
Refer as often as necessary to the material in Chapter 0 to review or to learn the mathematical background;
to understand the overall structure of the material; or to review or learn MATLAB as it applies to signal
processing.
2.
As you will see, the complexity of the material grows as it develops. The material in part three has been
written assuming good understanding of the material in the ﬁrst two. See also the connection of the material
with applications in your own areas of interest.
3.
To help you learn the material, clear and concise results are emphasized by putting them in boxes. Justi-
ﬁcation of these results is then given, complemented with remarks regarding issues that need a bit more
clariﬁcation, and illustrated with plenty of analytic and computational examples. Important terms are
emphasized throughout the text. Tables provide a good summary of properties and formulas.
4.
A heading is used in each of the problems at the end of the chapters, indicating how it relates to speciﬁc
topics and if it requires to use MATLAB to solve it.
5.
One of the objectives of this text is to help you learn MATLAB, as it applies to signal and systems, on your
own. This is done by providing the soft introduction to MATLAB in Chapter 0, and then by showing examples
using simple code in each of the chapters. You will notice that in the ﬁrst two parts basic components of
MATLAB (scripts, functions, plotting, etc.) are given in more detail than in part three. It is assumed you are
very proﬁcient by then to supply that on your own.
6.
Finally, notice the footnotes, the vignettes, and the historical sidebars that have been included to provide a
glance at the background in which the theory and practice of signals and systems have developed.

Acknowledgments
I would like to acknowledge with gratitude the support and efforts of many people who made the writing of this text
possible. First, to my family—my wife Cathy, my children William, Camila, and Juan, and their own families—many
thanks for their support and encouragement despite being deprived of my attention. To my academic mentor, Professor
Eliahu I. Jury, a deep sense of gratitude for his teachings and for having inculcated in me the love for a scholarly career
and for the theory and practice of signals and systems. Thanks to Professor William Stanchina, chair of the Department
of Electrical and Computer Engineering at the University of Pittsburgh, for his encouragement and support that made
it possible to dedicate time to the project. Sincere thanks to Seda Senay and Mircea Lupus, graduate students in my
department. Their contribution to the painful editing and proofreading of the manuscript, and the generation of the
solution manual (especially from Ms. Senay) are much appreciated. Equally, thanks to the publisher and its editors, in
particular to Joe Hayton and Steve Merken, for their patience, advising, and help with the publishing issues. Thanks
also to Sarah Binns for her help with the ﬁnal editing of the manuscript. Equally, I would like to thank Professor James
Rowland from the University of Kansas and the following reviewers for providing signiﬁcant input and changes to
the manuscript: Dimitrie Popescu, Old Dominion University; Hossein Hakim, Worcester Polytechnic Institute; Mark
Budnik, Valparaiso University; Periasamy Rajan, Tennessee Tech University; and Mohamed Zohdy, Oakland University.
Thanks to my colleagues Amro El-Jaroudi and Juan Manfredi for their early comments and suggestions.
Lastly, I feel indebted to the many students I have had in my courses in signals and systems over the years I have
been teaching this material in the Department of Electrical and Computer Engineering at the University of Pittsburgh.
Unknown to them, they contributed to my impetus to write a book that I felt would make the teaching of signals and
systems more accessible and fun to future students in and outside the university.
RESOURCES THAT ACCOMPANY THIS BOOK
A companion website containing downloadable MATLAB code for the worked examples in the book is available at:
http://booksite.academicpress.com/chaparro
For instructors, a solutions manual and image bank containing electronic versions of ﬁgures from the book are
available by registering at:
www.textbooks.elsevier.com
Also Available for Use with This Book – Elsevier Online Testing
Web-based testing and assessment feature that allows instructors to create online tests and assignments which
automatically assess student responses and performance, providing them with immediate feedback. Elsevier’s
online testing includes a selection of algorithmic questions, giving instructors the ability to create virtually unlim-
ited variations of the same problem. Contact your local sales representative for additional information, or visit
http://booksite.academicpress.com/chaparro/ to view a demo chapter.
xvi

1
PART
Introduction

This page intentionally left blank

CHAPTER 0
From the Ground Up!
In theory there is no difference
between theory and practice.
In practice there is.
Lawrence “Yogi” Berra, 1925
New York Yankees baseball player
This chapter provides an overview of the material in the book and highlights the mathematical back-
ground needed to understand the analysis of signals and systems. We consider a signal a function of
time (or space if it is an image, or of time and space if it is a video signal), just like the voltages or
currents encountered in circuits. A system is any device described by a mathematical model, just like
the differential equations obtained for a circuit composed of resistors, capacitors, and inductors.
By means of practical applications, we illustrate in this chapter the importance of the theory of signals
and systems and then proceed to connect some of the concepts of integro-differential Calculus with
more concrete mathematics (from the computational point of view, i.e., using computers). A brief
review of complex variables and their connection with the dynamics of systems follows. We end this
chapter with a soft introduction to MATLAB, a widely used high-level computational tool for analysis
and design.
Signiﬁcantly, we have called this Chapter 0, because it is the ground ﬂoor for the rest of the material
in the book. Not everything in this chapter has to be understood in a ﬁrst reading, but we hope that
as you go through the rest of the chapters in the book you will get to appreciate that the material in
this chapter is the foundation of the book, and as such you should revisit it as often as needed.
0.1 SIGNALS AND SYSTEMS AND DIGITAL TECHNOLOGIES
In our modern world, signals of all kinds emanate from different types of devices—radios and TVs,
cell phones, global positioning systems (GPSs), radars, and sonars. These systems allow us to com-
municate messages, to control processes, and to sense or measure signals. In the last 60 years, with
the advent of the transistor, the digital computer, and the theoretical fundamentals of digital signal
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00002-8
c⃝2011, Elsevier Inc. All rights reserved.
3

4
CHAPTER 0:
From the Ground Up!
processing, the trend has been toward digital representation and processing of data, most of which
are in analog form. Such a trend highlights the importance of learning how to represent signals in
analog as well as in digital forms and how to model and design systems capable of dealing with
different types of signals.
1948
The year 1948 is considered the birth year of technologies and theories responsible for the spectacular advances in com-
munications, control, and biomedical engineering since then. In June 1948, Bell Telephone Laboratories announced the
invention of the transistor. Later that month, a prototype computer built at Manchester University in the United Kingdom
became the ﬁrst operational stored-program computer. Also in that year, many fundamental theoretical results were pub-
lished: Claude Shannon’s mathematical theory of communications, Richard W. Hamming’s theory on error-correcting codes,
and Norbert Wiener’s Cybernetics comparing biological systems with communication and control systems [51].
Digital signal processing advances have gone hand-in-hand with progress in electronics and comput-
ers. In 1965, Gordon Moore, one of the founders of Intel, envisioned that the number of transistors
on a chip would double about every two years [35]. Intel, the largest chip manufacturer in the world,
has kept that pace for 40 years. But at the same time, the speed of the central processing unit (CPU)
chips in desktop personal computers has dramatically increased. Consider the well-known Pentium
group of chips (the Pentium Pro and the Pentium I to IV) introduced in the 1990s [34]. Figure 0.1
shows the range of speeds of these chips at the time of their introduction into the market, as well as
the number of transistors on each of these chips. In ﬁve years, 1995 to 2000, the speed increased by
a factor of 10 while the number of transistors went from 5.5 million to 42 million.
FIGURE 0.1
The Intel Pentium CPU chips. (a) Range of
CPU speeds in MHz for the Pentium Pro
(1995), Pentium II (1997), Pentium III (1999),
and Pentium IV (2000). (b) Number of
transistors (in millions) on each of the above
chips. (Pentium data taken from [34].)
1995
1997
1999
2000
0
1000
2000
MHz
Year
(a)
(b)
1995
1996
1997
1998
1999
2000
10
20
30
40
Million transistors
Year

0.2 Examples of Signal Processing Applications
5
Advances in digital electronics and in computer engineering in the past 60 years have permitted the
proliferation of digital technologies. Digital hardware and software process signals from cell phones,
high-deﬁnition television (HDTV) receivers, radars, and sonars. The use of digital signal processors
(DSPs) and more recently of ﬁeld-programmable gate arrays (FPGAs) have been replacing the use of
application-speciﬁc integrated circuits (ASICs) in industrial, medical, and military applications.
It is clear that digital technologies are here to stay. Today, digital transmission of voice, data, and video is
common, and so is computer control. The abundance of algorithms for processing signals, and the pervasive
presence of DSPs and FPGAs in thousands of applications make digital signal processing theory a necessary
tool not only for engineers but for anybody who would be dealing with digital data; soon, that will be every-
body! This book serves as an introduction to the theory of signals and systems—a necessary ﬁrst step in the
road toward understanding digital signal processing.
DSPs and FPGAs
A digital signal processor (DSP) is an optimized microprocessor used in real-time signal processing applications [67]. DSPs
are typically embedded in larger systems (e.g., a desktop computer) handling general-purpose tasks. A DSP system typically
consists of a processor, memory, analog-to-digital converters (ADCs), and digital-to-analog converters (DACs). The main
difference with typical microprocessors is they are faster. A ﬁeld-programmable gate array (FPGA) [77] is a semiconductor
device containing programmable logic blocks that can be programmed to perform certain functions, and programmable
interconnects. Although FPGAs are slower than their application-speciﬁc integrated circuits (ASICs) counterparts and use
more power, their advantages include a shorter time to design and the ability to be reprogrammed.
0.2 EXAMPLES OF SIGNAL PROCESSING APPLICATIONS
The theory of signals and systems connects directly, among others, with communications, control,
and biomedical engineering, and indirectly with mathematics and computer engineering. With the
availability of digital technologies for processing signals, it is tempting to believe there is no need
to understand their connection with analog technologies. It is precisely the opposite is illustrated
by considering the following three interesting applications: the compact-disc (CD) player, software-
deﬁned radio and cognitive radio, and computer-controlled systems.
0.2.1 Compact-Disc Player
Compact discs [9] were ﬁrst produced in Germany in 1982. Recorded voltage variations over time due
to an acoustic sound is called an analog signal given its similarity with the differences in air pressure
generated by the sound waves over time. Audio CDs and CD players illustrate best the conversion
of a binary signal—unintelligible—into an intelligible analog signal. Moreover, the player is a very
interesting control system.
To store an analog audio signal (e.g., voice or music) on a CD the signal must be ﬁrst sampled and
converted into a sequence of binary digits—a digital signal—by an ADC and then especially encoded
to compress the information and to avoid errors when playing the CD. In the manufacturing of a CD,

6
CHAPTER 0:
From the Ground Up!
DAC
Laser
Sensor
Audio
amplifier
Speaker
FIGURE 0.2
When playing a CD, the CD player follows the tracks in the disc, focusing a laser on them, as the CD is spun.
The laser shines a light that is reﬂected by the pits and bumps put on the surface of the disc and corresponding
to the coded digital signal from an acoustic signal. A sensor detects the reﬂected light and converts it into a
digital signal, which is then converted into an analog signal by the DAC. When ampliﬁed and fed to the speakers
such a signal sounds like the originally recorded acoustic signal.
pits and bumps corresponding to the ones and zeros from the quantization and encoding processes
are impressed on the surface of the disc. Such pits and bumps will be detected by the CD player and
converted back into an analog signal that approximates the original signal when the CD is played.
The transformation into an analog signal uses a DAC.
As we will see in Chapter 7, an audio signal is sampled at a rate of about 44,000 samples/second
(sec) (corresponding to a maximum frequency around 22 KHz for a typical audio signal) and each of
these samples is represented by a certain number of bits (typically 8 bits/sample). The need for stereo
sound requires that two channels be recorded. Overall, the number of bits representing the signal is
very large and needs to be compressed and especially encoded. The resulting data, in the form of pits
and bumps impressed on the CD surface, are put into a spiral track that goes from the inside to the
outside of the disc.
Besides the binary-to-analog conversion, the CD player exempliﬁes a very interesting control system
(see Figure 0.2). Indeed, the player must: (1) rotate the disc at different speeds depending on the
location of the track within the CD being read, (2) focus a laser and a lens system to read the pits
and bumps on the disc, and (3) move the laser to follow the track being read. To understand the
exactness required, consider that the width of the track and the high of the bumps is typically less
than a micrometer (10−6 meters or 3.937 × 10−5 inches) and a nanometer (10−9 meters or 3.937 ×
10−8 inches), respectively.
0.2.2 Software-Deﬁned Radio and Cognitive Radio
Software-deﬁned radio and cognitive radio are important emerging technologies in wireless commu-
nications [43]. In software-deﬁned radio (SDR), some of the radio functions typically implemented
in hardware are converted into software [64]. By providing smart processing to SDRs, cognitive radio
(CR) will provide the ﬂexibility needed to more efﬁciently use the radio frequency spectrum and to
make available new services to users. In the United States the Federal Communication Commission
(FCC), and likewise in other parts of the world the corresponding agencies, allocates the bands for

0.2 Examples of Signal Processing Applications
7
different users of the radio spectrum (commercial radio and TV, amateur radio, police, etc.). Although
most bands have been allocated, implying a scarcity of spectrum for new users, it has been found that
locally at certain times of the day the allocated spectrum is not being fully utilized. Cognitive radio
takes advantage of this.
Conventional radio systems are composed mostly of hardware, and as such cannot be easily recon-
ﬁgured. The basic premise in SDR as a wireless communication system is its ability to reconﬁgure
by changing the software used to implement functions typically done by hardware in a conventional
radio. In an SDR transmitter, software is used to implement different types of modulation procedures,
while ADCs and DACs are used to change from one type of signal to another. Antennas, audio ampli-
ﬁers, and conventional radio hardware are used to process analog signals. Typically, an SDR receiver
uses an ADC to change the analog signals from the antenna into digital signals that are processed
using software on a general-purpose processor. See Figure 0.3.
Given the need for more efﬁcient use of the radio spectrum, cognitive radio (CR) uses SDR technology
while attempting to dynamically manage the radio spectrum. A cognitive radio monitors locally the
radio spectrum to determine regions that are not occupied by their assigned users and transmits
in those bands. If the primary user of a frequency band recommences transmission, the CR either
moves to another frequency band, or stays in the same band but decreases its transmission power
level or modulation scheme to avoid interference with the assigned user. Moreover, a CR will search
Super-
heterodyne
ADC
Demodulator
DAC
Antenna
Speaker
TRANSMITTER
RECEIVER
ADC
Modulator
DAC
Microphone
Antenna
FIGURE 0.3
Schematics of a voice SDR mobile two-way radio. Transmitter: The voice signal is inputted by means of
a microphone, ampliﬁed by an audio ampliﬁer, converted into a digital signal by an ADC, and then modulated
using software, before being converted into analog by an DAC, ampliﬁed, and sent as a radio frequency signal
via an antenna. Receiver: The signal received by the antenna is processed by a superheterodyne front-end,
converted into a digital signal by an ADC before being demodulated and converted into an analog signal by a
DAC, ampliﬁed, and fed to a speaker. The modulator and demodulator blocks indicate software processing.

8
CHAPTER 0:
From the Ground Up!
for network services that it can offer to its users. Thus, SDR and CR are bound to change the way we
communicate and use network services.
0.2.3 Computer-Controlled Systems
The application of computer control ranges from controlling simple systems such as a heater (e.g.,
keeping a room temperature comfortable while reducing energy consumption) or cars (e.g., con-
trolling their speed), to that of controlling rather sophisticated machines such as airplanes (e.g.,
providing automatic ﬂight control) or chemical processes in very large systems such as oil reﬁneries.
A signiﬁcant advantage of computer control is the ﬂexibility computers provide—rather sophisticated
control schemes can be implemented in software and adapted for different control modes.
Typically, control systems are feedback systems where the dynamic response of a system is changed to
make it follow a desirable behavior. As indicated in Figure 0.4, the plant is a system, such as a heater,
car, or airplane, or a chemical process in need of some control action so that its output (it is also
possible for a system to have several outputs) follows a reference signal (or signals). For instance, one
could think of a cruise-control system in a car that attempts to keep the speed of the car at a certain
value by controlling the gas pedal mechanism. The control action will attempt to have the output of
the system follow the desired response, despite the presence of disturbances either in the plant (e.g.,
errors in the model used for the plant) or in the sensor (e.g., measurement error). By comparing the
reference signal with the output of the sensor, and using a control law implemented in the computer,
a control action is generated to change the state of the plant and attain the desired output.
To use a computer in a control application it is necessary to transform analog signals into digital
signals so that they can be inputted into the computer, while it is also necessary that the output of
the computer be converted into an analog signal to drive an actuator (e.g., an electrical motor) to
provide an action capable of changing the state of the plant. This can be done by means of ADCs
and DACs. The sensor should also be able to act as a transducer whenever the output of the plant is
Digital
computer
DAC
Plant
Sensor
ADC
Clock
+
−
r(t)
w(t)
y (t)
v (t)
FIGURE 0.4
Computer-controlled system for an analog plant (e.g., cruise control for a car). The reference signal is r(t) (e.g.,
desired speed) and the output is y(t) (e.g., car speed). The analog signals are converted to digital signals by an
ADC, while the digital signal from the computer is converted into an analog signal (an actuator is probably
needed to control the car) by a DAC. The signals w(t) and v(t) are disturbances or noise in the plant and the
sensor (e.g., electronic noise in the sensor and undesirable vibration in the car).

0.3 Analog or Discrete?
9
of a different type than the reference. Such would be the case, for instance, if the plant output is a
temperature while the reference signal is a voltage.
0.3 ANALOG OR DISCRETE?
Inﬁnitesimal calculus, or just plain calculus, deals with functions of one or more continuously changing
variables. Based on the representation of these functions, the concepts of derivative and integral are
developed to measure the rate of change of functions and the areas under the graphs of these
functions, or their volumes. Differential equations are then introduced to characterize dynamic
systems.
Finite calculus, on the other hand, deals with sequences. Thus, derivatives and integrals are replaced
by differences and summations, while differential equations are replaced by difference equations.
Finite calculus makes possible the computations of calculus by means of a combination of digital
computers and numerical methods—thus, ﬁnite calculus becomes the more concrete mathematics.1
Numerical methods applied to sequences permit us to approximate derivatives, integrals, and the
solution of differential equations.
In engineering, as in many areas of science, the inputs and outputs of electrical, mechanical, chemical,
and biological processes are measured as functions of time with amplitudes expressed in terms of
voltage, current, torque, pressure, etc. These functions are called analog or continuous-time signals, and
to process them with a computer they must be converted into binary sequences—or a string of ones
and zeros that is understood by the computer. Such a conversion is done in a way as to preserve as
much as possible the information contained in the original signal. Once in binary form, signals can
be processed using algorithms (coded procedures understood by computers and designed to obtain
certain desired information from the signals or to change them) in a computer or in a dedicated piece
of hardware.
In a digital computer, differentiation and integration can be done only approximately, and the solu-
tion of differential equations requires a discretization process as we will illustrate later in this chapter.
Not all signals are functions of a continuous parameter—there exist inherently discrete-time signals
that can be represented as sequences, converted into binary form, and processed by computers. For
these signals the ﬁnite calculus is the natural way of representing and processing them.
Analog or continuous-time signals are converted into binary sequences by means of an ADC, which, as we will
see, compresses the data by converting the continuous-time signal into a discrete-time signal or a sequence
of samples, each sample being represented by a string of ones and zeros giving a binary signal. Both time and
signal amplitude are made discrete in this process. Likewise, digital signals can be transformed into analog
signals by means of a DAC that uses the reverse process of the ADC. These converters are commercially
available, and it is important to learn how they work so that digital representation of analog signals is obtained
1The use of concrete, rather than abstract, mathematics was coined by Graham, Knuth, and Patashnik in Concrete Mathematics: A
Foundation for Computer Science [26]. Professor Donald Knuth from Stanford University is the the inventor of the Tex and Metafont
typesetting systems that are the precursors of Latex, the document layout system in which the original manuscript of this book was
done.

10
CHAPTER 0:
From the Ground Up!
with minimal information loss. Chapters 1, 7, and 8 will provide the necessary information about continuous-
time and discrete-time signals, and show how to convert one into the other and back. The sampling theory
presented in Chapter 7 is the backbone of digital signal processing.
0.3.1 Continuous-Time and Discrete-Time Representations
There are signiﬁcant differences between continuous-time and discrete-time signals as well as in their
processing. A discrete-time signal is a sequence of measurements typically made at uniform times,
while the analog signal depends continuously on time. Thus, a discrete-time signal x[n] and the
corresponding analog signal x(t) are related by a sampling process:
x[n] = x(nTs) = x(t)|t=nTs
(0.1)
That is, the signal x[n] is obtained by sampling x(t) at times t = nTs, where n is an integer and Ts is
the sampling period or the time between samples. This results in a sequence,
{· · · x(−Ts) x(0) x(Ts) x(2Ts) · · · }
according to the sampling times, or equivalently
{· · · x[−1] x[0] x[1] x[2] · · · }
according to the ordering of the samples (as referenced to time 0). This process is called sampling or
discretization of an analog signal.
Clearly, by choosing a small value for Ts we could make the analog and the discrete-time signals look
very similar—almost indistinguishable—which is good, but this is at the expense of memory space
required to keep the numerous samples. If we make the value of Ts large, we improve the memory
requirements, but at the risk of losing information contained in the original signal. For instance,
consider a sinusoid obtained from a signal generator:
x(t) = 2 cos(2πt)
for 0 ≤t ≤10 sec. If we sample it every Ts1 = 0.1 sec, the analog signal becomes the following
sequence:
x1[n] = x(t)|t=0.1n= 2 cos(2πn/10)
0 ≤n ≤100
providing a very good approximation to the original signal. If, on the other hand, we let Ts2 = 1 sec,
then the discrete-time signal becomes
x2[n] = x(t)|t=n= 2 cos(2πn) = 2
0 ≤n ≤10
See Figure 0.5. Although for Ts2 the number of samples is considerably reduced, the representation
of the original signal is very poor—it appears as if we had sampled a constant signal, and we have
thus lost information! This indicates that it is necessary to come up with a way to choose Ts so that
sampling provides not only a reasonable number of samples, but, more importantly, guarantees that
the information in the analog and the discrete-time signals remains the same.

0.3 Analog or Discrete?
11
FIGURE 0.5
Sampling an analog sinusoid
x(t) = 2 cos(2πt), 0 ≤t ≤10, with two
different sampling periods,
(a) Ts1 = 0.1 sec and (b) Ts2 = 1 sec, giving
x1(0.1n) and x2(n). The sinusoid is shown
by dashed lines. Notice the similarity
between the discrete-time signal and the
analog signal when Ts1 = 0.1 sec, while
they are very different when Ts2 = 1 sec,
indicating loss of information.
0
2
4
(a)
(b)
6
8
10
−2
−1
0
1
2
0
2
4
6
8
10
−2
−1
0
1
2
t(sec)
x1(0.1n)
x2(n)
FIGURE 0.6
Weekly closings of ACM stock for 160
weeks in 2006 to 2009. ACM is the trading
name of the stock of the imaginary
company, ACME Inc., makers of everything
you can imagine.
100
120
140
160
180
200
220
240
260
Dollars
ACM Closings, Jan. 2006−Dec. 2009
20
40
60
80
100
120
140
Week
As indicated before, not all signals are analog; there are some that are naturally discrete. Figure 0.6
displays the weekly average of the stock price of a ﬁctitious company, ACME. Thinking of it as a signal,
it is naturally discrete-time as it does not come from the discretization of an analog signal.
We have shown in this section the signiﬁcance of the sampling period Ts in the transformation of an analog
signal into a discrete-time signal without losing information. Choosing the sampling period requires knowl-
edge of the frequency content of the signal—this is an example of the relation between time and frequency to
be presented in great detail in Chapters 4 and 5, where the Fourier representation of periodic and nonperiodic

12
CHAPTER 0:
From the Ground Up!
signals is given. In Chapter 7, where we consider the problem of sampling, we will use this relation to
determine appropriate values for the sampling period.
0.3.2 Derivatives and Finite Differences
Differentiation is an operation that is approximated in ﬁnite calculus. The derivative operator
D[x(t)] = dx(t)
dt
= lim
h→0
x(t + h) −x(t)
h
(0.2)
measures the rate of change of an analog signal x(t). In ﬁnite calculus the forward ﬁnite-difference
operator
1[x(nTs)] = x((n + 1)Ts) −x(nTs)
(0.3)
measures the change in the signal from one sample to the next. If we let x[n] = x(nTs), for a known
Ts, the forward ﬁnite-difference operator becomes a function of n:
1[x[n]] = x[n + 1] −x[n]
(0.4)
The forward ﬁnite-difference operator measures the difference between two consecutive samples: one
in the future x((n + 1)Ts) and the other in the present x(nTs). (See Problem 0.4 for a deﬁnition of
the backward ﬁnite-difference operator.) The symbols D and 1 are called operators as they operate on
functions to give other functions. The derivative and the ﬁnite-difference operators are clearly not the
same. In the limit, we have that
dx(t)
dt
|t=nTs= lim
Ts→0
1[x(nTs)]
Ts
(0.5)
Depending on the signal and the chosen value of Ts, the ﬁnite-difference operation can be a crude or
an accurate approximation to the derivative multiplied by Ts.
Intuitively, if a signal does not change very fast with respect to time, the ﬁnite-difference approximates
well the derivative for relatively large values of Ts, but if the signal changes very fast one needs very
small values of Ts. The concept of frequency of a signal can help us understand this. We will learn that
the frequency content of a signal depends on how fast the signal varies with time; thus a constant
signal has zero frequency while a noisy signal that changes rapidly has high frequencies. Consider a
constant signal x0(t) = 2 having a derivative of zero (i.e., such a signal does not change at all with
respect to time or it is a zero-frequency signal). If we convert this signal into a discrete-time signal
using a sampling period Ts = 1 (or any other positive value), then x0[n] = 2 and so
1[x0[n]] = 2 −2 = 0
coincides with the derivative. Consider then a signal x1(t) = t with derivative 1 (this signal changes
faster than x(t) so it has frequencies larger than zero). If we sample it using Ts = 1, then x1[n] = n
and the ﬁnite difference is
1[x1[n]] = 1[n] = (n + 1) −n = 1

0.3 Analog or Discrete?
13
which again coincides with the derivative. Finally, we consider a signal that changes faster than x(t)
and x1(t) such as x2(t) = t2. Sampling x2(t) with Ts = 1, we have x2[n] = n2 and its forward ﬁnite
difference is given by
1[x2[n]] = 1[n2] = (n + 1)2 −n2 = 2n + 1
which gives as an approximation to the derivative 1[x2[n]]/Ts = 2n + 1. The derivative of x2(t)
is 2t, which at 0 equals 0, and at 1 equals 2. On the other hand, 1[n2]/Ts equals 1 and 3 at
n = 0 and n = 1, respectively, which are different values from those of the derivative. Suppose
Ts = 0.01, so that x2[n] = x2(nTs) = (0.01n)2 = 0.0001n2. If we compute the difference for this signal
we get
1[x2(0.01n)] = 1[(0.01n)2] = (0.01n + 0.01)2 −0.0001n2 = 10−4(2n + 1)
which gives as an approximation to the derivative 1[x2(0.01n)]/Ts = 10−2(2n + 1), or 0.01 when
n = 0 and 0.03 when n = 1 which are a lot closer to the actual values of
dx2(t)
dt
|t=0.01n = 2t |t=0.01n = 0.02n
The error now is 0.01 for each case instead of 1 as in the case when Ts = 1. Thus, whenever the
rate of change of the signal is faster, the difference gets closer to the derivative by making Ts
smaller.
It becomes clear that the faster the signal changes, the smaller the sampling period Ts should be in order to
get a better approximation of the signal and its derivative. As we will learn in Chapters 4 and 5 the frequency
content of a signal depends on the signal variation over time. A constant signal has frequency zero, while a
signal that changes very fast over time would have high frequencies. The higher the frequencies in a signal,
the more samples would be needed to represent it with no loss of information, thus requiring that Ts be
smaller.
0.3.3 Integrals and Summations
Integration is the opposite of differentiation. To see this, suppose I(t) is the integration of a
continuous signal x(t) from some time t0 to t (t0 < t),
I(t) =
t
Z
t0
x(τ)dτ
(0.6)

14
CHAPTER 0:
From the Ground Up!
or the sum of the area under x(t) from t0 to t. Notice that the upper bound of the integral is t so the
integrand depends on a dummy variable.2 The derivative of I(t) is
dI(t)
dt
= lim
h→0
I(t) −I(t −h)
h
= lim
h→0
1
h
t
Z
t−h
x(τ)dτ
≈lim
h→0
x(t) + x(t −h)
2
= x(t)
where the integral is approximated as the area of a trapezoid with sides x(t) and x(t −h) and height
h. Thus, for a continuous signal x(t),
d
dt
t
Z
t0
x(τ)dτ = x(t)
(0.7)
or if using the derivative operator D[.], then its inverse D−1[.] should be the integration operator.
That is, the above equation can be written
D[D−1[x(t)]] = x(t).
(0.8)
We will see in Chapter 3 a similar relation between the derivative and the integral. The Laplace trans-
form operators s and 1/s (just like D and 1/D) imply differentiation and integration in the time
domain.
Computationally, integration is implemented by sums. Consider, for instance, the integral of x(t) = t
from 0 to 10, which we know is equal to
10
Z
0
t dt = t2
2
10
t=0 = 50.
That is, the area of a triangle with a base of 10 and a height of 10. For Ts = 1, suppose we approximate
the signal x(t) by pulses p[n] of width Ts = 1 and height nTs = n, or pulses of area n for n = 0, . . . , 9.
This can be seen as a lower-bound approximation to the integral, as the total area of these pulses
gives a result smaller than the integral. In fact, the sum of the areas of the pulses is given by
9
X
n=0
p[n] =
9
X
n=0
n = 0 + 1 + 2 + · · · 9 = 0.5
" 9
X
n=0
n +
0
X
k=9
k
#
= 0.5
" 9
X
n=0
n +
9
X
n=0
(9 −n)
#
= 9
2
9
X
n=0
1 = 10 × 9
2
= 45
2The integral I(t) is a function of t and as such the integrand needs to be expressed in terms of a so-called dummy variable τ that takes
values from t0 to t in the integration. It would be confusing to let the integration variable be t. The variable τ is called a dummy variable
because it is not crucial to the integration; any other variable could be used with no effect on the integration.

0.3 Analog or Discrete?
15
FIGURE 0.7
Approximation of area under
x(t) = t, t ≥0, 0 otherwise, by pulses of
width 1 and height nTs, where Ts = 1 and
n = 0, 1, . . .
0
2
4
6
8
10
0
2
4
6
8
10
t
t, n
t
The approximation of the area using Ts = 1 is very poor (see Figure 0.7). In the above, we used the
fact that the sum is not changed whether we add the numbers from 0 to 9 or backwards from 9 to 0,
and that doubling the sum and dividing by 2 would not change the ﬁnal answer. The above sum can
thus be generalized to
N−1
X
n=0
n = 1
2
"N−1
X
n=0
n +
N−1
X
n=0
(N −1 −n)
#
= 1
2
N−1
X
n=0
(N −1)
= N × (N −1)
2
(0.9)
a result that Gauss found out when he was a preschooler!3
To improve the approximation of the integral we use Ts = 10−3, which gives a discretized signal nTs
for 0 ≤nTs < 10 or 0 ≤n ≤(10/Ts) −1. The area of the pulses is nT2
s and the approximation to the
integral is then
104−1
X
n=0
p[n] =
104−1
X
n=0
n10−6
= 104 × (104 −1)
106 × 2
= 49.995
3Carl Friedrich Gauss (1777–1855) was a German mathematician. He was seven years old when he amazed his teachers with his trick
for adding the numbers from 1 to 100 [7]. Gauss is one of the most accomplished mathematicians of all times [2]. He is in a group of
selected mathematicians and scientists whose pictures appear in the currency of a country. His picture was on the Mark, the previous
currency of Germany [6].

16
CHAPTER 0:
From the Ground Up!
which is a lot better result. In general, we have that the integral can be computed quite accurately
using a very small value of Ts, indeed
(10/Ts)−1
X
n=0
p[n] =
(10/Ts)−1
X
n=0
nT2
s
= T2
s
(10/Ts) × ((10/Ts) −1)
2
= 10 × (10 −Ts)
2
which for very small values of Ts (so that 10 −Ts ≈10) gives 100/2 = 50, as desired.
Derivatives and integrals take us into the processing of signals by systems. Once a mathematical model for a
dynamic system is obtained, typically differential equations characterize the relation between the input and
output variable or variables of the system. A signiﬁcant subclass of systems (used as a valid approximation in
some way to actual systems) is given by linear differential equations with constant coefﬁcients. The solution
of these equations can be efﬁciently found by means of the Laplace transform, which converts them into
algebraic equations that are much easier to solve. The Laplace transform is covered in Chapter 3, in part to
facilitate the analysis of analog signals and systems early in the learning process, but also so that it can be
related to the Fourier theory of Chapters 4 and 5. Likewise for the analysis of discrete-time signals and systems
we present in Chapter 9 the Z-transform, having analogous properties to those from the Laplace transform,
before the Fourier analysis of those signals and systems.
0.3.4 Differential and Difference Equations
A differential equation characterizes the dynamics of a continuous-time system, or the way the system
responds to inputs over time. There are different types of differential equations, corresponding to
different systems. Most systems are characterized by nonlinear, time-dependent coefﬁcient differential
equations. The analytic solution of these equations is rather complicated. To simplify the analysis,
these equations are locally approximated as linear constant-coefﬁcient differential equations.
Solution of differential equations can be obtained by means of analog and digital computers. An
electronic analog computer consists of operational ampliﬁers (op-amps), resistors, capacitors, voltage
sources, and relays. Using the linearized model of the op-amps, resistors, and capacitors it is possible
to realize integrators to solve a differential equation. Relays are used to set the initial conditions on
the capacitors, and the voltage source gives the input signal. Although this arrangement permits the
solution of differential equations, its drawback is the storage of the solution, which can be seen with
an oscilloscope but is difﬁcult to record. Hybrid computers were suggested as a solution—the analog
computer is assisted by a digital component that stores the data. Both analog and hybrid computers
have gone the way of the dinosaurs, and it is digital computers aided by numerical methods that are
used now to solve differential equations.
Before going into the numerical solution provided by digital computers, let us consider why inte-
grators are needed in the solution of differential equations. A ﬁrst-order (the highest derivative
present in the equation); linear (no nonlinear functions of the input or the output are present) with

0.3 Analog or Discrete?
17
FIGURE 0.8
RC circuit.
+
−
vi(t)
1Ω
1F
i(t)
FIGURE 0.9
Realization of ﬁrst-order differential equation using
(a) a differentiator and (b) an integrator.
(a)
(b)
vi(t )
+ −
dvc(t)
dt
vc(t)
d(·)
dt
vi(t )
+
−
dvc(t)
dt
vc(t)
∫(·)dt
constant-coefﬁcient differential equations obtained from a simple RC circuit (Figure 0.8) with a con-
stant voltage source vi(t) as input and with resistor R = 1; and capacitor C = 1 F (with huge plates!)
connected in series is given by
vi(t) = vc(t) + dvc(t)
dt
(0.10)
with an initial voltage vc(0) across the capacitor.
Intuitively, in this circuit the capacitor starts with an initial charge of vc(0), and will continue charging
until it reaches saturation, at which point no more charge will ﬂow (the current across the resistor and
the capacitor is zero). Therefore, the voltage across the capacitor is equal to the voltage source–that
is, the capacitor is acting as an open circuit given that the source is constant.
Suppose, ideally, that we have available devices that can perform differentiation. There is then the
tendency to propose that the differential equation (Eq. 0.10) be solved following the block diagram
shown in Figure (0.9). Although nothing is wrong analytically, the problem with this approach is that
in practice most signals are noisy (each device produces electronic noise) and the noise present in the
signal may cause large derivative values given its rapidly changing amplitudes. Thus, the realization
of the differential equation using differentiators is prone to being very noisy (i.e., not good). Instead
of, as proposed years ago by Lord Kelvin,4 using differentiators we need to smooth out the process by
using integrators, so that the voltage across the capacitor vc(t) is obtained by integrating both sides of
Equation (0.10). Assuming that the source is switched on at time t = 0 and that the capacitor has an
initial voltage vc(0), using the inverse relation between derivatives and integrals gives
vc(t) =
t
Z
0
[vi(τ) −vc(τ)]dτ + vc(0)
t ≥0
(0.11)
4William Thomson, Lord Kelvin, proposed in 1876 the differential analyzer, a type of analog computer capable of solving differential
equations of order 2 and higher. His brother James designed one of the ﬁrst differential analyzers [78].

18
CHAPTER 0:
From the Ground Up!
which is represented by the block diagram in Figure 0.9(b). Notice that the integrator also provides a
way to include the initial condition, which in this case is the initial voltage across the capacitor, vc(0).
Different from the accentuating the effect of differentiators on noise, integrators average the noise,
thus reducing its effects.
Block diagrams like the ones shown in Figure 0.9 allow us to visualize the system much better, and are
commonly used. Integrators can be efﬁciently implemented using operational ampliﬁers with resistors and
capacitors.
How to Obtain Difference Equations
Let us then show how Equation (0.10) can be solved using integration and its approximation, result-
ing in a difference equation. Using Equation (0.11) at t = t1 and t = t0 for t1 > t0, we have that
vc(t1) −vc(t0) =
t1
Z
t0
vi(τ)dτ −
t1
Z
t0
vc(τ)dτ
If we let t1 −t0 = 1t where 1t →0 (i.e., a very small time interval), the integrals can be seen as
the area of small trapezoids of height 1t and bases vi(t1) and vi(t0) for the input source and vc(t1)
and vc(t0) for the voltage across the capacitor (see Figure 0.10). Using the formula for the area of a
trapezoid we get an approximation for the above integrals so that
vc(t1) −vc(t0) = [vi(t1) + vi(t0)]1t
2 −[vc(t1) + vc(t0)]1t
2
from which we obtain
vc(t1)

1 + 1t
2

= [vi(t1) + vi(t0)]1t
2 + vc(t0)

1 −1t
2

Assuming 1t = T, we then let t1 = nT and t0 = (n −1)T. The above equation can be written as
vc(nT) =
T
2 + T [vi(nT) + vi((n −1)T)] + 2 −T
2 + T vc((n −1)T)
n ≥1
(0.12)
and initial condition vc(0) = 0. This is a ﬁrst-order linear difference equation with constant
coefﬁcients approximating the differential equation characterizing the RC circuit. Letting the input
FIGURE 0.10
Approximation of area under the
curve by a trapezoid.
vc(t0)
vc(t1)
t0
Δt
t1
t

0.3 Analog or Discrete?
19
be vi(t) = 1 for t ≥0, we have
vc(nT) =
0
n = 0
2T
2+T + 2−T
2+T vc((n −1)T)
n ≥1
(0.13)
The advantage of the difference equation is that it can be solved for increasing values of n using
previously computed values of vc(nT), which is called a recursive solution. For instance, letting T =
10−3, vi(t) = 1, and deﬁning M = 2T/(2 + T), K = (2 −T)/(2 + T), we obtain
n = 0
vc(0) = 0
n = 1
vc(T) = M
n = 2
vc(2T) = M + KM = M(1 + K)
n = 3
vc(3T) = M + K(M + KM) = M(1 + K + K2)
n = 4
vc(4T) = M + KM(1 + K + K2) = M(1 + K + K2 + K3)
· · ·
The values are M = 2T/(2 + T) ≈T = 10−3, K = (2 −T)/(2 + T) < 1, and 1 −K = M. The response
increases from the zero initial condition to a constant value, which is the effect of the dc source—the
capacitor eventually acts as an open circuit, so that the voltage across the capacitor equals that of
the input. Extrapolating from the above results it seems that in the steady-state (i.e., when nT →∞)
we have5
vc(nT) = M
∞
X
m=0
Km =
M
1 −K = 1
Even though this is a very simple example, it clearly illustrates that very good approximations to the
solution of differential equations can be obtained using numerical methods that are appropriate for
implementation in digital computers.
The above example shows how to solve a differential equation using integration and approximation of the
integrals to obtain a difference equation that a computer can easily solve. The integral approximation used
above is the trapezoidal rule method, which is one among many numerical methods used to solve differential
equations. Also we will see later that the above results in the bilinear transformation, which connects the
Laplace s variable with the z variable of the Z-transform, and that will be used in Chapter 11 in the design of
discrete ﬁlters.
5The inﬁnite sum converges if |K| < 1, which is satisﬁed in this case. If we multiply the sum by (1 −K) we get
(1 −K)
∞
X
m=0
Km =
∞
X
m=0
Km −
∞
X
m=0
Km+1
= 1 +
∞
X
m=1
Km −
∞
X
ℓ=1
Kℓ= 1
where we changed the variable in the second equation to ℓ= m + 1. This explains why the sum is equal to 1/(1 −K).

20
CHAPTER 0:
From the Ground Up!
0.4 COMPLEX OR REAL?
Most of the theory of signals and systems is based on functions of a complex variable. Clearly, sig-
nals are functions of a real variable corresponding to time or space (if the signal is two-dimensional,
like an image) so why would one need complex numbers in processing signals? As we will see later,
time-dependent signals can be characterized by means of frequency and damping. These two charac-
teristics are given by complex variables such as s = σ + j (where σ is the damping factor and  is
the frequency) in the representation of analog signals in the Laplace transform, or z = rejω (where r
is the damping factor and ω is the discrete frequency) in the representation of discrete-time signals in
the Z-transform. Both of these transformations will be considered in detail in Chapters 3 and 9. The
other reason for using complex variables is due to the response of systems to pure tones or sinusoids.
We will see that such response is fundamental in the analysis and synthesis of signals and systems.
We thus need a solid grasp of what is meant by complex variables and what a function of these is
all about. In this section, complex variables will be connected to vectors and phasors (which are
commonly used in the sinusoidal steady-state analysis of linear circuits).
0.4.1 Complex Numbers and Vectors
A complex number z represents any point (x, y) in a two-dimensional plane by z = x + jy, where
x = Re[z] (real part of z) is the coordinate in the x axis and y = Im[z] (imaginary part of z) is the
coordinate in the y axis. The symbol j = √−1 just indicates that z needs to have two components
to represent a point in the two-dimensional plane. Interestingly, a vector ⃗z that emanates from the
origin of the complex plane (0, 0) to the point (x, y) with a length
|⃗z| =
q
x2 + y2 = |z|
(0.14)
and an angle
θ = ∠⃗z = ∠z
(0.15)
also represents the point (x, y) in the plane and has the same attributes as the complex number z. The
couple (x, y) is therefore equally representable by the vector ⃗z or by a complex number z that can be
written in a rectangular or in a polar form,
z = x + jy = |z|ejθ
(0.16)
where the magnitude |z| and the phase θ are deﬁned in Equations (0.14) and (0.15).
It is important to understand that a rectangular plane or a polar complex plane are identical despite
the different representation of each point in the plane. Furthermore, when adding or subtracting
complex numbers the rectangular form is the appropriate one, while when multiplying or dividing
complex numbers the polar form is more advantageous. Thus, if complex numbers z = x + jy = |z|ej∠z
and v = p + jq = |v|ej∠v are added analytically, we obtain
z + v = (x + p) + j(y + q)

0.4 Complex or Real?
21
FIGURE 0.11
(a) Representation of a complex number z by a
vector (b) addition of complex numbers z and v;
(c) integer powers of j; and (d) complex conjugate.
(x, y)
θ
|z|
x
(a)
(c)
(d)
y
z + v
(b)
z
v
−1 = j 2, j 6, …
1 = j 0, j 4, …
j = j 1, j 5, …
−j = j 3, j 7, …
(x, y)
(x, −y)
θ
−θ
|z|
|z|
Using their polar representations requires a geometric interpretation: the addition of vectors (see
Figure 0.11). On the other hand, the multiplication of z and v is easily done using their polar
forms as
zv = |z|ej∠z|v|ej∠v = |z||v|ej(∠z+∠v)
but it requires more operations if done in the rectangular form—that is,
zv = (x + jy)(p + jq) = (xp −yq) + j(xq + yp)
It is even more difﬁcult to obtain a geometric interpretation. Such an interpretation will be seen
later on. Addition and subtraction as well as multiplication and division can thus be done more
efﬁciently by choosing the rectangular and the polar representations, respectively. Moreover, the polar
representation is also useful when ﬁnding powers of complex numbers. For the complex variable
z = |z|e∠z, we have that
zn = |z|nejn∠z
for n integer or rational. For instance, if n = 10, then z10 = |z|10ej10∠z, and if n = 3/2, then z1.5 =
(√|z|)3ej1.5∠z. The powers of j are of special interest. Given that j = √−1 then, we have
jn = (−1)n/2 =
(−1)m
n = 2m,
n even
(−1)mj
n = 2m + 1,
n odd

22
CHAPTER 0:
From the Ground Up!
so that j0 = 1, j1 = j, j2 = −1, j3 = −j, and so on. Letting j = 1ejπ/2, we can see that the increasing
powers of jn = 1ejnπ/2 are vectors with angles of 0 when n = 0, π/2 when n = 1, π when n = 2,
and 3π/2 when n = 3. The angles repeat for the next four values, the four after that, and so on. See
Figure 0.11.
One operation possible with complex numbers that is not possible with real numbers is complex
conjugation. Given a complex number z = x + jy = |z|ej∠z its complex conjugate is z∗= x −jy =
|z|e−j∠z—that is, we negate the imaginary part of z or reﬂect its angle. This operation gives that
(i)
z + z∗= 2x
or
Re[z] = 0.5[z + z∗]
(ii)
z −z∗= 2jy
or
Im[z] = 0.5[z −z∗]
(iii)
zz∗= |z|2
or
|z| =
√
zz∗
(iv)
z
z∗= ej2∠z
or
∠z = −j0.5[log(z) −log(z∗)]
(0.17)
The complex conjugation provides a different approach to the division of complex numbers in rect-
angular form. This is done by making the denominator a positive real number by multiplying both
numerator and denominator by the complex conjugate of the denominator. For instance,
z = 1 + j1
3 + j4 = (1 + j1)(3 −j4)
(3 + j4)(3 −j4) =
7 −j
9 + 16 = 7 −j
25
Finally, the conversion of complex numbers from rectangular to polar needs to be done with care,
especially when computing the angles. For instance, z = 1 + j has a vector representing in the ﬁrst
quadrant of the complex plane, and its magnitude is |z| =
√
2 while the tangent of its angle θ is
tan(θ) = 1 or θ = π/4 radians. If z = −1 + j, the vector representing it is now in the second quadrant
with the same magnitude as before, but its angle is now
θ = π −tan−1(1) = 3π/4
That is, we ﬁnd the angle with respect to the negative real axis and subtract it from π. Likewise, if
z = −1 −j, the magnitude does not change but the phase is now
θ = π + tan−1(1) = 5π/4
which can also be expressed as −3π/4. Finally, when z = 1 −j, the angle is −π/4 and the magnitude
remains the same. The conversion from polar to rectangular form is much easier. Indeed, given a
complex number in polar form z = |z|ejθ its real part is x = |z| cos(θ) (i.e., the projection of the vector
corresponding to z onto the real axis) and the imaginary part is y = |z| sin(θ), so that z = x + jy. For
instance, z =
√
2e3π/4 can be written as
z =
√
2 cos(3π/4) + j
√
2 sin(3π/4) = −1 + j

0.4 Complex or Real?
23
0.4.2 Functions of a Complex Variable
Just like real-valued functions, functions of a complex variable can be deﬁned. For instance, the
logarithm of a complex number can be written as
v = log(z) = log(|z|ejθ) = log(|z|) + jθ
by using the inverse connection between the exponential and the logarithmic functions. Of particular
interest in the theory of signals and systems is the exponential of complex variable z deﬁned as
v = e z = ex+jy = exejy
It is important to mention that complex variables as well as functions of complex variables are more
general than real variables and real-valued functions. The above deﬁnition of the logarithmic function
is valid when z = x, with x a real value, and also when z = jy, a purely imaginary value. Likewise, the
exponential function for z = x is a real-valued function.
Euler’s Identity
One of the most famous equations of all times6 is
1 + ejπ = 1 + e−jπ = 0
due to one of the most proliﬁc mathematicians of all times, Leonard Euler.7 The above equation can
be easily understood by establishing Euler’s identity, which connects the complex exponential and
sinusoids:
ejθ = cos(θ) + j sin(θ)
(0.18)
One way to verify this identity is to consider the polar representation of the complex number cos(θ) +
j sin(θ), which has a unit magnitude since
p
cos2(θ) + sin2(θ) = 1 given the trigonometric identity
cos2(θ) + sin2(θ) = 1. The angle of this complex number is
ψ = tan−1
 sin(θ)
cos(θ)

= θ
Thus, the complex number
cos(θ) + j sin(θ) = 1ejθ
which is Euler’s identity. Now in the case where θ = ±π the identity implies that e±jπ = −1,
explaining the famous Euler’s equation.
6A reader’s poll done by Mathematical Intelligencer named Euler’s identity the most beautiful equation in mathematics. Another poll by
Physics World in 2004 named Euler’s identity the greatest equation ever, together with Maxwell’s equations. Paul Nahin’s book Dr. Euler’s
Fabulous Formula (2006) is devoted to Euler’s identity. It states that the identity sets “the gold standard for mathematical beauty” [73].
7Leonard Euler (1707–1783) was a Swiss mathematician and physicist, student of John Bernoulli, and advisor of Joseph Lagrange. We
owe Euler the notation f(x) for functions, e for the base of natural logs, i = √−1, π for pi, 6 for sum, the ﬁnite difference notation 1,
and many more!

24
CHAPTER 0:
From the Ground Up!
The relation between the complex exponentials and the sinusoidal functions is of great importance
in signals and systems analysis. Using Euler’s identity the cosine can be expressed as
cos(θ) = Re[ejθ] = ejθ + e−jθ
2
(0.19)
while the sine is given by
sin(θ) = Im[ejθ] = ejθ −e−jθ
2j
(0.20)
Indeed, we have
ejθ = cos(θ) + j sin(θ)
e−jθ = cos(θ) −j sin(θ)
Adding them we get the above expression for the cosine, and subtracting the second from the ﬁrst we
get the given expression for the sine. The variable θ is in radians, or in the corresponding angle in
degrees (recall that 2π radians equals 360 degrees).
These relations can be used to deﬁne the hyperbolic sinusoids as
cos( jα) = e−α + eα
2
= cosh(α)
(0.21)
j sin( jα) = e−α −eα
2
= −sinh(α)
(0.22)
from which the other hyperbolic functions are deﬁned. Also, we obtain the following expression for
the real-valued exponential:
e−α = cosh(α) −sinh(α)
(0.23)
Euler’s identity can also be used to ﬁnd different trigonometric identities. For instance,
cos2(θ) =
"
ejθ + e−jθ
2
#2
= 1
4[2 + ej2θ + e−j2θ] = 1
2 + 1
2 cos(2θ)
sin2(θ) = 1 −cos2(θ) = 1
2 −1
2 cos(2θ)
sin(θ) cos(θ) = ejθ −e−jθ
2j
ejθ + e−jθ
2
= ej2θ −e−j2θ
4j
= 1
2 sin(2θ)
0.4.3 Phasors and Sinusoidal Steady State
A sinusoid x(t) is a periodic signal represented by
x(t) = A cos(0t + ψ)
−∞< t < ∞
(0.24)
where A is the amplitude, 0 = 2πf0 is the frequency in rad/sec, and ψ is the phase in radians. The
signal x(t) is deﬁned for all values of t, and it repeats periodically with a period T0 = 1/f0 (sec), so

0.4 Complex or Real?
25
that f0 is the frequency in cycles/sec or in Hertz (Hz) (in honor of H. R. Hertz8). Given that the units
of 0 is rad/sec, then 0t has as units (rad/sec) × (sec) = (rad), which coincides with the units of the
phase ψ, and permits the computation of the cosine. If ψ = 0, then x(t) is a cosine, and if ψ = −π/2,
then x(t) is a sine.
If one knows the frequency 0 (rad/sec) in Equation (0.24), the cosine is characterized by its
amplitude and phase. This permits us to deﬁne phasors9 as complex numbers characterized by the
amplitude and the phase of a cosine signal of a certain frequency 0. That is, for a voltage signal
v(t) = A cos(0t + ψ) the corresponding phasor is
V = Aejψ = A cos(ψ) + jA sin(ψ) = A∠ψ
(0.25)
and such that
v(t) = Re[Vej0t] = Re[Aej(0t+ψ)] = A cos(0t + ψ)
(0.26)
One can thus think of the voltage signal v(t) as the projection of the phasor V onto the real axis and
turning counterclockwise at a rate of 0 rad/sec. At time t = 0 the angle of the phasor is ψ. Clearly
the phasor deﬁnition is true for only one frequency, in this case 0, and it is always connected to a
cosine function.
Interestingly enough, the angle ψ can be used to differentiate cosines and sines. For instance, when
ψ = 0, the phasor V moving around at a rate of 0 generates as a projection on the real axis the
voltage signal A cos(0t), while when ψ = −π/2, the phasor V moving around again at a rate of
0 generates a sinusoid A sin(0t) = A cos(0t −π/2) as it is projected onto the real axis. This estab-
lishes the well-known fact that the sine lags the cosine by π/2 radians or 90 degrees, or that the cosine
leads the sine by π/2 radians or 90 degrees. Thus, the generation and relation of sines and cosines
can be easily obtained using the plot in Figure 0.12.
Phasors can be related to vectors. A current source, for instance,
i(t) = A cos(0t) + B sin(0t)
FIGURE 0.12
Generation of sinusoids
from phasors of a
frequency 0.
−cos
cos
−sin
sin
Ω0
8Heinrich Rudolf Hertz was a German physicist known for being the ﬁrst to demonstrate the existence of electromagnetic radiation in
1888.
9In 1883, Charles Proteus Steinmetz (1885–1923), German-American mathematician and engineer, introduced the concept of phasors
for alternating current analysis. In 1902, Steinmetz became a professor of electrophysics at Union College in Schenectady, New York.

26
CHAPTER 0:
From the Ground Up!
can be expressed as
i(t) = C cos(0t + γ )
where C and γ are to be determined (the sinusoidal components of i(t) must depend on a unique
frequency 0; if that was not the case the concept of phasors would not apply). To obtain the equiv-
alent representation, we ﬁrst obtain the phasor corresponding to A cos(0t), which is I1 = Aej0 = A,
and for B sin(0t) the corresponding phasor is I2 = Be−jπ/2, so that
i(t) = Re[(I1 + I2)ej0t]
Thus, the problem has been transformed into the addition of two vectors I1 and I2, which gives a
vector
I =
p
A2 + B2e−j tan−1(B/A)
so that
i(t) = Re[Iej0t]
= Re[
p
A2 + B2 e−j tan−1(B/A)ej0t]
=
p
A2 + B2 cos(0t −tan−1(B/A))
Or, an equivalent source with amplitude C =
√
A2 + B2, phase γ = −tan−1(B/A), and frequency 0–
that is, an equivalent phasor that generates i(t) and has the magnitude C, the angle γ , and rotates at
frequency 0.
In Figure 0.13 we display the result of adding two phasors (frequency f0 = 20 Hz) and the sinusoid
that is generated by the phasor I = I1 + I2 = 27.98ej30.4o.
0.4.4 Phasor Connection
The fundamental property of a circuit made up of constant resistors, capacitors, and inductors is that
its response to a sinusoid is also a sinusoid of the same frequency in steady state. The effect of the
circuit on the input sinusoid is on its magnitude and phase and depends on the frequency of the input
sinusoid. This is due to the linear and time-invariant nature of the circuit, and can be generalized to
more complex continuous-time as well as discrete-time systems as we will see in Chapters 3, 4, 5, 9
and 10.
To illustrate the connection of phasors with dynamic systems consider a simple RC circuit (R = 1 
and C = 1F). If the input to the circuit is a sinusoidal voltage source vi(t) = A cos(0t) and the voltage
across the capacitor vc(t) is the output of interest, the circuit can be easily represented by the ﬁrst-order
differential equation
dvc(t)
dt
+ vc(t) = vi(t)

0.4 Complex or Real?
27
10
20
  30
30
210
60
240
90
270
120
300
150
330
180
0
Phasor 1+ Phasor 2
(a)
10
20
30
30
210
60
240
90
270
(b)
120
300
150
330
180
0
0
0.02
0.04
0.06
0.08
0.1
(c)
0.12
0.14
0.16
0.18
0.2
−40
−20
0
20
40
FIGURE 0.13
(a) Sum of phasors I1 = 10ej0 (solid arrow) and I2 = 20e jπ/4 (dashed arrow) with the result in blue; (c) sinusoid
generated by phasor I = I1 + I2 (b).

28
CHAPTER 0:
From the Ground Up!
Assume that the steady-state response of this circuit (i.e., vc(t) as t →∞) is also a sinusoid
vc(t) = C cos(0t + ψ)
of the same frequency as the input, with amplitude C and phase ψ to be determined. This response
must satisfy the differential equation, or
vi(t) = dvc(t)
dt
+ vc(t)
A cos(0t) = −C0 sin(0t + ψ) + C cos(0t + ψ)
= C0 cos(0t + ψ + π/2) + C cos(0t + ψ)
= C
q
1 + 2
0 cos(0t + ψ + tan−1(C0/C))
Comparing the two sides of the above equation gives
C =
A
q
1 + 2
0
ψ = −tan−1(0)
for a steady-state response
vc(t) =
A
q
1 + 2
0
cos(0t −tan−1(0)).
Comparing the steady-state response vc(t) with the input sinusoid vi(t), we see that they both have the
same frequency 0, but the amplitude and phase of the input are changed by the circuit depending
on the frequency 0. Since at each frequency the circuit responds differently, obtaining the frequency
response of the circuit will be useful not only in analysis but in the design of circuits.
The sinusoidal steady-state is obtained using phasors. Expressing the steady-state response of the
circuit as
vc(t) = Re
h
Vcej0ti
where Vc = Cejψ is the corresponding phasor for vc(t), we ﬁnd that
dvc(t)
dt
= dRe[Vcej0t]
dt
= Re
"
Vc
dej0t
dt
#
= Re
h
j0Vcej0ti
By replacing vc(t), dvc(t)/dt, obtained above, and
vi(t) = Re
h
Viej0ti
where Vi = Aej0

0.5 Soft Introduction to MATLAB
29
in the differential equation, we obtain
Re
h
Vc(1 + j0)ej0ti
= Re
h
Aej0ti
so that
Vc =
A
1 + j0
=
A
q
1 + 2
0
e−j tan−1(0)
= Cejψ
and the sinusoidal steady-state response is
vc(t) = Re
h
Vcej0ti
=
A
q
1 + 2
0
cos(0t −tan−1(0))
which coincides with the response obtained above. The ratio of the output phasor Vc to the input
phasor Vi,
Vc
Vi
=
1
1 + j0
gives the response of the circuit at frequency 0. If the frequency of the input is a generic , changing
0 above for  gives the frequency response for all possible frequencies.
The concepts of linearity and time invariance will be used in both continuous-time as well as discrete-time
systems, along with the Fourier representation of signals in terms of sinusoids or complex exponentials, to
simplify the analysis and to allow the design of systems. Thus, transform methods such as Laplace and the
Z-transform will be used to solve differential and difference equations in an algebraic setup. Fourier repre-
sentations will provide the frequency perspective. This is a general approach for both continuous-time and
discrete-time signals and systems. The introduction of the concept of the transfer function will provide tools
for the analysis as well as the design of linear time-invariant systems. The design of analog and discrete ﬁlters
is the most important application of these concepts. We will look into this topic in Chapters 5, 6, and 11.
0.5 SOFT INTRODUCTION TO MATLAB
MATLAB is a computing language based on vectorial computations.10 In this section, we will
introduce you to the use of MATLAB for numerical and symbolic computations.
10MATLAB stands for matrix laboratory. MatWorks, the developer of MATLAB, was founded in 1984 by Jack Little, Steve Bangert, and
Cleve Moler. Moler, a math professor at the University of New Mexico, developed the ﬁrst version of MATLAB in Fortran in the late
1970s. It only had 80 functions and no M-ﬁles or toolboxes. Little and Bangert reprogrammed it in C and added M-ﬁles, toolboxes,
and more powerful graphics [49].

30
CHAPTER 0:
From the Ground Up!
0.5.1 Numerical Computations
The following instructions are intended for users who have no background in MATLAB but are inter-
ested in using it in signal processing. Once you get the basic information on how to use the language
you will be able to progress on your own.
1.
Create a directory where you will put your work, and from where you will start MATLAB. This is
important because when executing a program, MATLAB will look at the current directory, and if
the ﬁle is not present in the current directory, and if it is not a MATLAB function, MATLAB gives
an error indicating that it cannot ﬁnd the desired program.
2.
There are two types of programs in MATLAB: the script, which consists in a list of commands
using MATLAB functions or your own functions, and the functions, which are programs that can
be called with different inputs and provide the corresponding outputs. We will show examples of
both.
3.
Once you start MATLAB, you will see three windows: the command window, where you will type
commands; the command history, which keeps a list of commands that have been used; and the
workspace, where the variables used are kept.
4.
Your ﬁrst command on the command window should be to change to your data directory where
you will keep your work. You can do this in the command window by using the command CD
(change directory) followed by the desired directory. It is also important to use the command
clear all and clf to clear all previous variables in memory and all ﬁgures.
5.
Help is available in several forms in MATLAB. Just type helpwin, helpdesk, or demo to get started. If
you know the name of the function, help will give you the necessary information on the particular
function, and it will also give you information on help itself. Use help to ﬁnd more about the
functions used in this introduction to MATLAB.
6.
To type your scripts or functions you can use the editor provided by MATLAB; simply type edit.
You can also use any text editor to create scripts or functions, which need to be saved with the .m
extension.
Creating Vectors and Matrices
Comments are preceded by percent, and to begin a script, as the following, it is always a good idea
to clear all previous variables and all previous ﬁgures.
% matlab primer
clear all
% clear all variables
clf
% clear all ﬁgures
% row and column vectors
x = [ 1 2 3 4]
% row vector
y = x’
% column vector
The corresponding output is as follows (notice that there is no semicolon (;) at the end of the lines
to stop MATLAB from providing an output when the above script is executed).
x =
1
2
3
4

0.5 Soft Introduction to MATLAB
31
y =
1
2
3
4
To see the dimension of x and y variables, type
whos
% provides information on existing variables
to which MATLAB responds
Name
Size
Bytes Class
x
1x4
32 double array
y
4x1
32 double array
Grand total is 8 elements using 64 bytes
Notice that a vector is thought of as a matrix; for instance, vector x is a matrix of one row and four
columns. Another way to express the column vector y is the following, where each of the row terms
is separated by a semicolon (;)
y = [1;2;3;4]
% another way to write a column
To give as before:
y =
1
2
3
4
MATLAB does not allow arguments of vectors or matrices to be zero or negative. For instance, if we
want the ﬁrst entry of the vector y we need to type
y(1)
% ﬁrst entry of vector y
giving as output
ans =
1
If we type
y(0)
it will give us an error, to which we get the following warning:
??? Subscript indices must either be real positive integers or logicals.
MATLAB also has a peculiar way to provide information in a vector, for instance:
y(1:3)
% ﬁrst to third entry of column vector y

32
CHAPTER 0:
From the Ground Up!
giving as expected the ﬁrst to the third entries of the column vector y:
ans =
1
2
3
The following will give the third to the ﬁrst entry in the row vector x (notice the difference in the two
outputs; as expected the values of y are given in a column, while the requested entries of x are given
in a row).
x(3:-1:1)
% displays entries x(3) x(2) x(1)
Thus,
ans =
3
2
1
Matrices are constructed as an concatenation of rows (or columns):
A = [ 1 2; 3 4; 5 6]
% matrix A with rows [1 2], [3 4] and [5 6]
A =
1
2
3
4
5
6
To create a vector corresponding to a sequence of numbers (in this case integers) there are different
approaches, as follows:
n = 0:10
% vector with entries 0 to 10 increased by 1
This approach gives the following as output:
n =
Columns 1 through 10
0
1
2
3
4
5
6
7
8
9
Column 11
10
which is the same as the command
n = [0:10]
If we wish the increment different from 1 (default value), then we indicate it as in the following:
n1 = 0:2:10 % vector with entries from 0 to 10 increased by 2
which gives
n1 =
0
2
4
6
8
10
We can combine the above vectors into one as follows:
nn1 = [n n1]
% combination of vectors

0.5 Soft Introduction to MATLAB
33
to get
nn1 =
Columns 1 through 10
0
1
2
3
4
5
6
7
8
9
Columns 11 through 17
10
0
2
4
6
8
10
Vectorial Operations
MATLAB allows the conventional vectorial operations as well as facilitates others. For instance, if we
wish to multiply by 3 every entry of the row vector x given above, the command
z = 3∗x
% multiplication by a constant
would give
z =
3
6
9
12
Besides the conventional multiplication of vectors with the correct dimensions, MATLAB allows two
types of multiplications of one vector by another. The ﬁrst one is where the entries of one vector are
multiplied by the corresponding entries of the other. To effect this the two vectors should have the
same dimension (i.e., both should be columns or rows with the same number of entries) and it is
necessary to put a dot before the multiplication operator—that is, as shown here:
v = x.∗x
% multiplication of entries of two vectors
v =
1
4
9
16
The other type of multiplication is the conventional multiplication allowed in linear algebra. For
instance, with that of a row vector by a column vector,
w = x∗x’
% multiplication of x (row vector) by x’(column vector)
w = 30
the result is a constant—in this case, the length of the row vector should coincide with that of the
column vector. If you multiply a column (say x’) of dimension 4 × 1 by a row (say x) of dimension
1 × 4 (notice that the 1s coincide at the end of the ﬁrst dimension and at the beginning of the
second), the multiplication z = x′ ∗x results in a 4 × 4 matrix.
The solution of a set of linear equations is very simple in MATLAB. To guarantee that a unique solu-
tion exists, the determinant of the matrix should be computed before inverting the matrix. If the
determinant is zero MATLAB will indicate the solution is not possible.
% Solution of linear set of equations Ax = b
A = [1 0 0; 2 2 0; 3 3 3];
% 3x3 matrix
t = det(A);
% MATLAB function that calculates determinant
b = [2 2 2]’;
% column vector
x = inv(A)∗b;
% MATLAB function that inverts a matrix

34
CHAPTER 0:
From the Ground Up!
The results of these operations are not given because of the semicolons at the end of the commands.
The following script could be used to display them:
disp(’ Ax = b’)
% MATLAB function that displays the text in ’ ’
A
b
x
t
which gives
Ax = b
A =
1
0
0
2
2
0
3
3
3
b =
2
2
2
x =
2.0000
−1.0000
−0.3333
t =
6
Another way to solve this set of equations is
x = b’/A’
Try it!
MATLAB provides a fast way to obtain certain vectors/matrices; for instance,
% special vectors and matrices
x = ones(1, 10)
% row of ten 1s
x =
1
1
1
1
1
1
1
1
1
1
A = ones(5, 5)
% matrix of 5 x 5 1s
A =
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
x1 = [x zeros(1, 5)]
% vector with previous x and 5 0s

0.5 Soft Introduction to MATLAB
35
x1 =
Columns 1 through 10
1
1
1
1
1
1
1
1
1
1
Columns 11 through 15
0
0
0
0
0
A(2:5, 2:5) = zeros(4, 4)
% zeros in rows 2−5, columns 2−5
A =
1
1
1
1
1
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
y = rand(1,10)
% row vector with 10 random values (uniformly
% distributed in [0,1]
y =
Columns 1 through 6
0.9501
0.2311
0.6068
0.4860
0.8913
0.7621
Columns 7 through 10
0.4565
0.0185
0.8214
0.4447
Notice that these values are between 0 and 1. When using the normal or Gaussian-distributed noise
the values can be positive or negative reals.
y1 = randn(1,10)
% row vector with 10 random values
% (Gaussian distribution)
y1 =
Columns 1 through 6
−0.4326
−1.6656
0.1253
0.2877
−1.1465
1.1909
Columns 7 through 10
1.1892
−0.0376
0.3273
0.1746
Using Built-In Functions and Creating Your Own
MATLAB provides a large number of built-in functions. The following script uses some of them.
% using built-in functions
t = 0:0.01:1;
% time vector from 0 to 1 with interval of 0.01
x = cos(2∗pi∗t/0.1);
% cos processes each of the entries in
% vector t to get the corresponding value in vector x
% plotting the function x
ﬁgure(1)
% numbers the ﬁgure
plot(t, x)
% interpolated continuous plot
xlabel(’t (sec)’)
% label of x-axis
ylabel(’x(t)’)
% label of y-axis

36
CHAPTER 0:
From the Ground Up!
% let’s hear it
sound(1000∗x, 10000)
The results are given in Figure 0.14.
To learn about any of these functions use help. In particular, use help to learn about MATLAB routines
for plotting plot and stem. Use help sound and help waveplay to learn about the sound routines available
in MATLAB. Additional related functions are put at the end of these help ﬁles. Explore all of these
and become aware of the capabilities of MATLAB. To illustrate the plotting and the sound routines,
let us create a chirp that is a sinusoid for which the frequency is varying with time.
y = sin(2∗pi∗t.ˆ2/.1);
% notice the dot in the squaring
% t was deﬁned before
sound(1000∗y, 10000) % to listen to the sinusoid
ﬁgure(2) % numbering of the ﬁgure
plot(t(1:100), y(1:100)) % plotting of 100 values of y
ﬁgure(3)
plot(t(1:100), x(1:100), ’k’, t(1:100), y(1:100), ’r’) % plotting x and y on same plot
Let us hope you were able to hear the chirp, unless you thought it was your neighbor grunting. In
this case, we plotted the ﬁrst 100 values of t and y and let MATLAB choose the color for them. In the
second plot we chose the colors: black (dashed lines) for x and blue (continuous line) for the second
signal y(t) (see Figure 0.15).
Other built-in functions are sin, tan, acos, asin, atan, atan2, log, log10, exp, etc. Find out what each does
using help and obtain a listing of all the functions in the signal processing toolbox.
0
0.2
0.4
0.6
0.8
1
−1
−0.5
0
0.5
1
t(sec)
x (t)
0
0.2
0.4
0.6
0.8
1
−1
−0.5
0
0.5
1
t (sec)
(a)
(b)
x (0.01n)
FIGURE 0.14
(a) Plotting of a sinusoid using plot, which gives a continuous plot, and (b) stem, which gives a discrete plot.

0.5 Soft Introduction to MATLAB
37
0
0.2
0.4
0.6
0.8
−1
−0.5
0
0.5
1
t(sec)
y (t)
0
0.2
0.4
0.6
0.8
−1
−0.5
0
0.5
1
t(sec)
y (t ), x (t )
x(t)
y(t)
(a)
(b)
FIGURE 0.15
(a) Plotting chirp (MATLAB chooses color), (b) sinusoid and chirp (the sinusoid is plotted with dashed lines and
the chirp with solid lines).
You do not need to deﬁne π, as it is already done in MATLAB. For complex numbers also you do not
need to deﬁne the square root of −1, which for engineers is ‘j’ and for mathematicians ‘i’ (they have
no current to worry about).
% pi and j
pi
j
i
ans =
3.1416
ans =
0 + 1.0000i
ans =
0 + 1.0000i
Creating Your Own Functions
MATLAB has created a lot of functions to make our lives easier, and it allows us also to create—in the
same way—our own. The following ﬁle is for a function f with an input of a scalar x and output of a
scalar y related by a mathematical function:
function y = f(x)
y = x∗exp(−sin(x))/(1 + xˆ2);
Functions cannot be executed on their own—they need to be part of a script. If you try to execute the
above function MATLAB will give the following:
??? format compact;function y = f(x)
|
Error: A function declaration cannot appear within a script M-ﬁle.

38
CHAPTER 0:
From the Ground Up!
A function is created using the word “function” and then deﬁning the output (y), the name of the
function ( f), and the input of the function (x), followed by lines of code deﬁning the function, which
in this case is given by the second line. In our function the input and the output are scalars. If you
want vectors as input/output you need to do the computation in vectorial form—more later.
Once the function is created and saved (the name of the function followed by the extension .m), MAT-
LAB will include it as a possible function that can be executed within a script. If we wish to compute
the value of the function for x = 2 ( f.m should be in the working directory) we proceed as follows:
y = f(2)
gives
y = 0.1611
To compute the value of the function for a vector as input, we compute for each of the values in the
vector the corresponding output using a for loop as shown in the following.
x = 0:0.1:100;
% create an input vector x
N = length(x);
% ﬁnd the length of x
y = zeros(1,N);
% initialize the output y to zeros
for n = 1:N,
% for the variable n from 1 to N, compute
y(n) = f(x(n));
% the function
end
ﬁgure(3)
plot(x, y)
grid
% put a grid on the ﬁgure
title(’Function f(x)’)
xlabel(’x’)
ylabel(’y’)
This is not very efﬁcient. A general rule in MATLAB is: Loops are to be avoided, and vectorial
computations are encouraged. The results are shown in Figure 0.16.
FIGURE 0.16
Result of using the function f(.)
0
20
40
60
80
100
0
0.1
0.2
0.3
0.4
0.5
0.6
x
y= f(x)

0.5 Soft Introduction to MATLAB
39
The function working on a vector x, rather than one value, takes the following form (to make it
different from the above function we let the denominator be 1 + x instead of 1 + x2):
function yy = ff(x)
% vectorial function
yy = x.∗exp(−sin(x))./(1 + x);
Again, this function must be in the working directory. Notice that the computation of yy is done
considering x a vector; the .* and ./ are indicative of this. Thus, this function will accept a vector x and
will give as output a vector yy, computed as indicated in the last line. When we use a function, the
names of the variables used in the script that calls the function do not need to coincide with the ones
in the deﬁnition of the function. Consider the following script:
z = ff(x);
% x deﬁned before,
% z instead of yy is the output of the function ff
ﬁgure(4)
plot(x, z); grid
title(’Function ff(x)’) % MATLAB function that puts title in plot
xlabel(’x’) % MATLAB function to label x-axis
ylabel(’z’)
% MATLAB function to label y-axis
The difference between plot and stem is important. The function plot interpolates the vector to be plot-
ted and so the plot appears continuous, while stem simply plots the entries of the vector, separating
them uniformly. The input x and the output of the function are discrete time and we wish to plot
them as such, so we use stem.
stem(x(1:30), z(1:30))
grid
title(’Function ff(x)’)
xlabel(’x’)
ylabel(’z’)
The results are shown in Figure 0.17.
More on Plotting
There are situations where we want to plot several plots together. One can superpose two or more
plots by using hold on and hold off. To put several ﬁgures in the same plot, we can use the function
subplot. Suppose we wish to plot four ﬁgures in one plot and they could be arranged as two rows of
two ﬁgures each. We do the following:
subplot(221)
plot(x, y)
subplot(222)
plot(x, z)
subplot(223)
stem(x, y)
subplot(224)
stem(x, z)

40
CHAPTER 0:
From the Ground Up!
FIGURE 0.17
Results of using the function ff(.)
(notice the difference in scale in the
x axis).
0
20
40
60
80
100
0
0.5
1
1.5
2
2.5
x
y = ff (x)
0
0.5
1
1.5
2
2.5
0
0.1
0.2
0.3
0.4
0.5
0.6
x
y = ff (x)
In the subplot function the ﬁrst two numbers indicate the number of rows and the number of columns,
and the last digit refers to the order of the graph that is, 1, 2, 3, and 4 (see Figure 0.18).
There is also a way to control the values in the axis, by using the function (you guessed!) axis. This
function is especially useful after we have a graph and want to improve its looks. For instance, suppose
that the professor would like the above graphs to have the same scales in the y-axis (picky professor).
You notice that there are two scales in the y-axis, one 0-0.8 and another 0-3. To have both with the
same scale, we choose the one 0-3, and modify the above code to the following
subplot(221)
plot(x, y)
axis([0 100 0 3])
subplot(222)
plot(x, z)
axis([0 100 0 3])
subplot(223)
stem(x, y)

0.5 Soft Introduction to MATLAB
41
FIGURE 0.18
Plotting four ﬁgures in one.
0
50
100
0
0.5
1
0
50
100
0
1
2
3
0
50
100
0
0.5
1
0
50
100
0
1
2
3
axis([0 100 0 3])
subplot(224)
stem(x, z)
axis([0 100 0 3])
Saving and Loading Data
In many situations you would like to either save some data or load some data. The following is one
way to do it. Suppose you want to build and save a table of sine values for angles between 0 and
360 degrees in intervals of 3 degrees. This can be done as follows:
x = 0:3:360;
y = sin(x∗pi/180); % sine computes the argument in radians
xy = [x’ y’]; % vector with 2 columns one for x’
% and another for y’
Let’s now save these values in a ﬁle “sine.mat” by using the function save (use help save to learn more):
save sine.mat xy
To load the table, we use the function load with the name given to the saved table “sine” (the extension
*.mat is not needed). The following script illustrates this:
clear all
load sine
whos

42
CHAPTER 0:
From the Ground Up!
where we use whos to check its size:
Name
Size
Bytes
Class
xy
121x2
1936
double array
Grand total is 242 elements using 1936 bytes
This indicates that the array xy has 121 rows and 2 columns, the ﬁrst colum corresponding to x, the
degree values, and the second column corresponding to the sine values, y. Verify this and plot the
values by using
x = xy(:, 1);
y = xy(:, 2);
stem(x, y)
Finally, MATLAB provides some data ﬁles for experimentation and you only need to load them. The
following “train.mat” is the recording of a train whistle, sampled at the rate of Fs samples/sec, which
accompanies the sampled signal y(n) (see Figure 0.19).
clear all
load train
whos
Name
Size
Bytes
Class
Fs
1x1
8
double array
y
12880x1
103040
double array
Grand total is 12881 elements using 103048 bytes
sound(y, Fs)
plot(y)
FIGURE 0.19
Train signal.
0
2000
4000
6000
8000
10000
12000
−1
−0.5
0
0.5
1
n (samples)
y [n]

0.5 Soft Introduction to MATLAB
43
FIGURE 0.20
Clown in gray scale.
n
m
50
100
150
200
250
300
20
40
60
80
100
120
140
160
180
200
MATLAB also provides two-dimensional signals, or images, such as “clown.mat,” a 200 × 320 pixels
image.
clear all
load clown
whos
Name
Size
Bytes
Class
X
200x320
512000
double array
caption
2x1
4
char array
map
81x3
1944
double array
Grand total is 64245 elements using 513948 bytes
We can display this image in gray levels by using the following script (see Figure 0.20):
colormap(’gray’)
imagesc(X)
Or in color using
colormap(’hot’)
imagesc(X)
0.5.2 Symbolic Computations
We have considered the numerical capabilities of MATLAB, by which numerical data are transformed
into numerical data. There will be many situations when we would like to do algebraic or calculus
operations resulting in terms of variables rather than numerical data. For instance, we might want
to ﬁnd a formula to solve quadratic algebraic equations, to ﬁnd a difﬁcult integral, or to obtain the
Laplace or the Fourier transform of a signal. For those cases MATLAB provides the Symbolic Math
Toolbox, which uses the interface between MATLAB and MAPLE, a symbolic computing system. In
this section, we provide you with an introduction to symbolic computations by means of examples,
and hope to get you interested in learning more on your own.

44
CHAPTER 0:
From the Ground Up!
Derivatives and Differences
The following script compares symbolic with numeric computations of the derivative of a chirp signal
(a sinusoid with changing frequency) y(t) = cos(t2), which is
z(t) = dy(t)
dt
= −2t sin(t2)
clf; clear all
% symbolic
syms t y z % deﬁne the symbolic variables
y = cos(tˆ2) % chirp signal -- notice no . before ˆ since t is no vector
z = diff(y) % derivative
ﬁgure(1)
subplot(211)
ezplot(y, [0, 2∗pi]);grid % plotting for symbolic y between 0 and 2∗pi
hold on
subplot(212)
ezplot(z, [0, 2∗pi]);grid
hold on
%numeric
Ts = 0.1; % sampling period
t1 = 0:Ts:2∗pi; % sampled time
y1 = cos(t1.ˆ2); % sampled signal --notice difference with y above
z1 = diff(y1)./diff(t1); % difference -- approximation to derivative
ﬁgure(1)
subplot(211)
stem(t1, y1, ’r’);axis([0 2∗pi 1.1∗min(y1) 1.1∗max(y1)])
subplot(212)
stem(t1(1:length(y1) - 1), z1, ’r’);axis([0 2∗pi 1.1∗min(z1) 1.1∗max(z1)])
legend(’Derivative (black)’,’Difference (blue)’)
hold off
The symbolic function syms deﬁnes the symbolic variables (use help syms to learn more). The signal
y(t) is written differently than y1(t) in the numeric computation. Since t1 is a vector, squaring it
requires a dot before the symbol. That is not the case for t, which is not a vector but a variable. The
results of using diff to compute the derivative of y(t) is given in the same form as you would have
obtained doing the derivative by hand—that is,
y = cos(tˆ2)
z = −2∗t∗sin(tˆ2)
The symbolic toolbox provides its own graphic routines (use help to learn about the different ez-
routines). For plotting y(t) and z(t), we use the function ezplot, which plots the above two functions
for t ∈[0, 2π] and titles the plots with these functions.
The numeric computations differ from the symbolic in that vectors are being processed, and we are
obtaining an approximation to the derivative z(t). We sample the signal with Ts = 0.1 and use again

0.5 Soft Introduction to MATLAB
45
0
1
2
3
4
5
6
−1
−0.5
0
0.5
1
t
cos(t 2)
(a)
(b)
0
1
2
3
4
5
6
−10
−5
0
5
10
t
−2 sin(t 2) t
Derivative (black)
Difference (blue)
FIGURE 0.21
Symbolic and numeric computation of the derivative of the chirp y(t) = cos(t2). (a) y(t) and the sampled signal
y(nTs), Ts = 0.1 sec. (b) Displays the exact derivative (continuous line) and the approximation of the derivative at
samples nTs. Better approximation to the derivative can be obtained by using a smaller value of Ts.
the function diff to approximate the derivative (the denominator diff(t1) is the same as Ts). Plot-
ting the exact derivative (continuous line) with the approximated one (samples) using stem clariﬁes
that the numeric computation is an approximation at nTs values of time. See Figure 0.21.
The Sinc Function and Integration
The sinc function is very signiﬁcant in the theory of signals and systems. It is deﬁned as
y(t) = sin πt
πt
−∞< t < ∞
It is symmetric with respect to the origin, and deﬁned from −∞to ∞. The value of y(0) can be found
using L’Hˆopital’s rule. We will see later (Parseval’s result in Chapter 5) that the integral of y2(t) is

46
CHAPTER 0:
From the Ground Up!
equal to 1. In the following script we are combining numeric and symbolic computations to show
this. First, after deﬁning the variables, we use the symbolic function int to compute the integral of the
squared sinc function, with respect to t, from 0 to integer values 1 ≤k ≤10. We then use the function
subs to convert the symbolic results into a numerical array zz. The numeric part of the script deﬁnes
a vector y to have the values of the sinc function for 100 time values equally spaced between [−4, 4],
obtained using the function linspace. We then use plot and stem to plot the sinc and the values of the
integrals, which as seen in Figure 0.22 reach a value close to unity in less than 10 steps. Please use
help to learn more about each of these functions.
clf; clear all
% symbolic
syms t z
for k = 1:10,
−4
−3
−2
−1
0
(a)
(b)
1
2
3
4
0
0.5
1
t
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
n
FIGURE 0.22
(a) Computation of the integral of the squared sinc function (b) Illustrates that the area under the curve of this
function, or its integral, is unity. Using the symmetry of the function only the integral for t ≥0 needs to be
computed.

0.5 Soft Introduction to MATLAB
47
z = int(sinc(t)ˆ2, t, 0, k);
% integral of sincˆ2 from 0 to k
zz(k) = subs(2∗z);
% substitution to numeric value zz
end
% numeric
t1 = linspace(−4, 4);
% 100 equally spaced points in [-4,4]
y = sinc(t1).ˆ2;
% numeric deﬁnition of the squared sinc function
n = 1:10;
ﬁgure(1)
subplot(211)
plot(t1, y);grid;axis([−4 4 −0.2 1.1∗max(y)]);title(’y(t)=sincˆ2(t)’);
xlabel(’t’)
subplot(212)
stem(n(1:10), zz(1:10)); hold on
plot(n(1:10), zz(1:10), ’r’);grid;title(’
R
y(τ) dτ’); hold off
axis([1 10 0 1.1*max(zz)]); xlabel(’n’)
Figure 0.22 shows the squared sinc function and the values of the integral
2
k
Z
0
sinc2(t)dt = 2
k
Z
0
sin(πt)
πt
2
dt
k = 1, . . . , 10
which quickly reaches the ﬁnal value of unity. In computing the integral from (−∞, ∞) we are using
the symmetry of the function and thus the multiplication by 2.
Chebyshev Polynomials and Lissajous Figures
The Chebyshev polynomials are used in the design of ﬁlters. They can be obtained by plotting two
cosine functions as they change with time t, one of ﬁx frequency and the other with increasing
frequency:
x(t) = cos(2πt)
y(t) = cos(2πkt)
k = 1, . . . , N
The x(t) gives the x axis coordinate and y(t) the y axis coordinate at each value of t. If we solve for t in
the top equation, we get
t = 1
2π cos−1(x(t))
which then replaced in the bottom equation gives
y(t) = cos

k cos−1(x(t))

k = 1, . . . , N
as an expression for the Chebyshev polynomials (we will see in Chapter 6 that these equations can
be expressed as regular polynomials). Figure 0.23 shows the Chebyshev polynomials for N = 4. The
following script is used to compute and plot these polynomials.

48
CHAPTER 0:
From the Ground Up!
−1
0
1
−0.5
0
0.5
1
x
y
x=cos(2πt), y= cos(2πt)
x =cos(2πt), y=cos(4πt)
x =cos(2πt), y =cos(8πt)
x= cos(2πt), y =cos(6πt)
−1
0
1
−0.5
0
0.5
1
x
y
−1
0
1
−0.5
0
0.5
1
x
y
−1
0
1
−0.5
0
0.5
1
x
(a)
(b)
(c)
(d)
y
FIGURE 0.23
The Chebyshev polynomials for n = 1, 2, 3, 4. First (a) to fourth (d) polynomials. Notice that these polynomials
are deﬁned between [−1, 1] in the x axis.
clear all;clf
syms x y t
x = cos(2∗pi∗t); theta=0;
ﬁgure(1)
for k = 1:4,
y = cos(2∗pi∗k∗t + theta);
if k == 1, subplot(221)
elseif k == 2, subplot(222)
elseif k == 3, subplot(223)
else subplot(224)
end
ezplot(x, y);grid;hold on
end
hold off

0.5 Soft Introduction to MATLAB
49
−1 −0.5
0
0.5
1
−0.5
0
0.5
1
x
y
−0.5
0
0.5
1
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
x
y
x = cos(2πt), y = 1/2 cos(2πt +1/4π) 
−0.5
0
0.5
1
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
x
y
x = cos(2πt), y = −1/4 sin(2πt) 
−0.5
0
0.5
1
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
x
y
x = cos(2πt), y = −1/8 sin(2πt +1/4π) 
(a)
(b)
−1
−0.5
0
0.5
1
−0.5
0
0.5
1
x
y
x=cos(2πt), y= cos(2πt) 
x = cos(2πt), y = cos(2πt) 
x=cos(2πt), y=−sin(2πt) 
−1 −0.5
0
0.5
1
−0.5
0
0.5
x
y
x = cos(2πt), y = cos(2πt +1/4π) 
−1 −0.5
0
0.5
1
−0.5
0
0.5
x
y
−1 −0.5
0
0.5
1
−0.5
0
0.5
x
y
x= cos(2πt), y=−sin(2πt +1/4π) 
FIGURE 0.24
Lissajous ﬁgures: (a) (four left plots) case 1 input and output of same amplitude (A = 1) but phase differences
of 0, π/4, π/2, and 3π/4; (b) (four right plots) case 2 input has unit amplitude but output has decreasing
amplitudes and same phase differences as in case 1.
The Lissajous ﬁgures we consider next are a very useful extension of the above plotting of sinusoids in
the x and y axes. These ﬁgures are used to determine the difference between a sinusoidal input and its
corresponding sinusoidal steady state. In the case of linear systems, which we will formally deﬁne in
Chapter 2, for a sinusoidal input the outputs of the system are also sinusoids of the same frequency,
but they differ with the input in the amplitude and phase.
The differences in amplitude and phase can be measured using an oscilloscope for which we put
the input in the horizontal sweep and the output in the vertical sweep, giving ﬁgures from which
we can ﬁnd the differences in amplitude and phase. Two situations are simulated in the following
script, one where there is no change in amplitude but the phase changes from zero to 3π/4, while
in the other case the amplitude decreases as indicated and the phase changes in the same way as
before. The plots, or Lissajous ﬁgures, indicate such changes. The difference between the maximum
and the minimum of each of the ﬁgures in the x axis gives the amplitude of the input, while the
difference between the maximum and the minimum in the y axis gives the amplitude of the output.
The orientation of the ellipse provides the difference in phase with respect to that of the input.
The following script is used to obtain the Lissajous ﬁgures in these cases. Figure 0.24 displays the
results.
clear all;clf
syms x y t
x = cos(2∗pi∗t); % input of unit amplitude and frequency 2*pi
A = 1;ﬁgure(1)
% amplitude of output in case 1
for i = 1:2,
for k = 0:3,

50
CHAPTER 0:
From the Ground Up!
theta = k∗pi/4;
% phase of output
y = Aˆk∗cos(2∗pi∗t + theta);
if k == 0,subplot(221)
elseif k == 1,subplot(222)
elseif k == 2,subplot(223)
else subplot(224)
end
ezplot(x, y);grid;hold on
end
A = 0.5; ﬁgure(2) % amplitude of output in case 2
end
Ramp, Unit-Step, and Impulse Responses
To close this introduction to symbolic computations we illustrate the response of a linear system
represented by a differential equation,
d2y(t)
dt2
+ 5dy(t)
dt
+ 6y(t) = x(t)
where y(t) is the output and x(t) the input. The input is a constant x(t) = 1 for t ≥0 and zero other-
wise (MATLAB calls this function heaviside, but we will call it the unit-step signal). We then let the
input be the derivative of x(t), which is a signal that we will call impulse, and ﬁnally we let the input
be the integral of x(t), which is what we will call the ramp signal. The following script is used to ﬁnd
the responses, which are displayed in Figure 0.25.
clear all; clf
syms y t x z
% input a unit-step (heaviside) response
y = dsolve(’D2y + 5*Dy + 6∗y = heaviside(t)’,’y(0) = 0’,’Dy(0) = 0’,’t’);
x = diff(y); % impulse response
z = int(y); % ramp response
ﬁgure(1)
subplot(311)
ezplot(y, [0,5]);title(’Unit-step response’)
subplot(312)
ezplot(x, [0,5]);title(’Impulse response’)
subplot(313)
ezplot(z, [0,5]);title(’Ramp response’)
This example illustrates the intuitive appeal of linear systems. When the input is a constant value (or
a unit-step signal or a heaviside signal) the output tries to follow the input after some initial inertia
and it ends up being constant. The impulse signal (obtained as the derivative of the unit-step sig-
nal) is a signal of very short duration equivalent to shocking the system with a signal that disappears
very fast, different from the unit-step signal that is like a dc source. Again the output tries to follow
the input, eventually disappearing as t increases (no energy from the input!), and the ramp that is

0.5 Soft Introduction to MATLAB
51
0
1
2
3
4
5
0
0.05
0.1
0.15
Unit-step response
0
1
2
3
4
5
0
0.05
0.1
0.15
Impulse response
0
1
2
3
4
5
0
0.2
0.4
0.6
t
Ramp response
FIGURE 0.25
Response of a second order system represented by a differential equation for input of the unit-step signal, its
derivative, or the impulse signal and the ramp signal that is the integral of the unit-step input.
the integral of the unit-step signal grows with time, providing more and more energy to the system
as time increases, thus the response we obtained. The function dsolve solves differential equations
explicitly given (D stands for the derivative operator, so D is the ﬁrst derivative and D2 is the sec-
ond derivative). A second-order system requires two initial conditions, the output and its derivative
at t = 0.
We hope this introduction to MATLAB has provided you with the necessary background to understand the
basic way MATLAB operates, and shown you how to continue increasing your knowledge of it. Your best
source of information is the help command. Explore the different modules that MATLAB has and you will
become quickly convinced that these modules provide a great number of computational tools for many areas
of engineering and mathematics. Try it—you will like it! Tables 0.1 and 0.2 provide a listing of the numeric
and symbolic variables and operations.

52
CHAPTER 0:
From the Ground Up!
Table 0.1 Basic Numeric Matlab
Special variables
ans
Default name for result
pi
π value
inf, NaN
inﬁnity, not-a-number error (e.g., 0/0)
i, j
i = j = √−1
Function(s)
Operation
Mathematical
abs, angle
magnitude, angle of complex number
acos, asine, atan
inverse cosine, sine, tangent
acosh, asinh, atanh
inverse cosh, sinh, tanh
cos, sin, tan
cosine, sine, tangent
cosh, sinh, tanh
hyperbolic cosine, sine, tangent
conj, imag, real
complex conjugate, imaginary, real parts
exp, log, log10
exponential, natural and base 10 logarithms
Special operations
ceil, ﬂoor
round up, round down to integer
ﬁx, round
round toward zero, to nearest integer
.∗, ./
entry-by-entry multiplication, division
.ˆ
entry-by-entry power
x’, A’
transpose of vector x, matrix A
Array operations
x=ﬁrst:increment:last
row vector x from ﬁrst to last by increment
x=linspace(ﬁrst,last,n)
row vector x with n elements from ﬁrst to last
A=[x1;x2]
matrix A with rows x1, x2
ones(N,M), zeros(N,M)
N × M ones and zeros arrays
A(i,j)
(i, j) entry of matrix A
A(i,:), A(:,j)
i row ( j-column) and all columns (rows) of matrix A
whos
display variables in workspace
size(A)
(number rows, number of colums) of matrix A
length(x)
number rows (colums) of vector x
Control ﬂow
for, if, elseif
for loop, if, else-if loop
while
while loop
pause, pause(n)
pause and pause n seconds
Plotting
plot, stem
continuous, discrete plots
ﬁgure
ﬁgure for plotting
subplot
subplots
hold on, hold off
hold plot on or off
axis, grid
axis, grid of plots
xlabel, ylabel, title, legend
labeling of axes, plots, and subplots
Saving and loading
save, load
saving and loading data
Information and managing
help
help
clear, clf
clear variables from memory, clear ﬁgures
Operating system
cd, pwd
change directory, current working directory

Problems
53
Table 0.2 Basic Symbolic Matlab Functions
Function
Operation
Calculus
diff
differentiate
int
integrate
limit
limit
taylor
Taylor series
symsum
summation
Simpliﬁcation
simplify
simplify
expand
expand
factor
factor
simple
ﬁnd shortest form
subs
symbolic substitution
Solving equations
solve
solve algebraic equations
dsolve
solve differential equations
Transforms
fourier
Fourier transform
ifourier
inverse Fourier transform
laplace
Laplace transform
ilaplace
inverse Laplace transform
ztrans
Z-transform
iztrans
inverse Z-transform
Symbolic operations
sym
create symbolic objects
syms
create symbolic objects
pretty
make pretty expression
Special functions
dirac
Dirac or delta function
heaviside
unit-step function
Plotting
ezplot
function plotter
ezpolar
polar coordinate plotter
ezcontour
contour plotter
ezsurf
surface plotter
ezmesh
mesh (surface) plotter
PROBLEMS
For the problems requiring implementation in MATLAB, write scripts or functions to solve them
numerically or symbolically. Label the axes of the plots, give a title, and use legend to identify dif-
ferent signals in a plot. To save space use subplot to put several plots into one. To do the problem
numerically, sample analog signals with a small Ts.

54
CHAPTER 0:
From the Ground Up!
0.1. Bits or bytes
Just to get an idea of the number of bits or bytes generated and processed by a digital system consider the
following applications:
(a) A compact disc is capable of storing 75 minutes of “CD-quality” stereo (left and right channels are
recorded) music. Calculate the number of bytes and the number of bits that are stored in the CD.
Hint: Find out what “CD quality” means in the binary representation of each sample, and what is the
sampling rate your CD player uses.
(b) Find out what the vocoder in your cell phone is used for. Assume then that in attaining “telephone
quality” you use a sampling rate of 10,000 samples/sec to achieve that type of voice quality. Each
sample is represented by 8 bits. With this information, calculate the number of bits that your cell
phone has to process every second that you talk. Why would you then need a vocoder?
(c) Find out whether text messaging is cheaper or more expensive than voice. Explain how text mes-
saging works.
(d) Find out how an audio CD and an audio DVD compare. Find out why it is said that a vinyl long play
record reproduces sounds much better. Are we going backwards with digital technology in music
recording? Explain.
(e) To understand why video streaming in the Internet is many times of low quality, consider the amount
of data that need to be processed by a video compressor every second. Assume the size of a video
frame, in picture elements or pixels, is 352 × 240, and that an acceptable quality for the image is
obtained by allocating 8 bits/pixel, and to avoid jerking effects we use 60 frames/sec.
I
How many pixels would have to be processed every second?
I
How many bits would be available for transmission every second?
I
The above are raw data. Compression changes the whole picture (literally); ﬁnd out what some of
the compression methods are.
0.2. Sampling—MATLAB
Consider an analog signal x(t) = 4 cos(2πt) deﬁned for −∞< t < ∞. For the following values of the
sampling period Ts, generate a discrete-time signal x[n] = x(nTs) = x(t)|t=nTs.
I
Ts = 0.1 sec
I
Ts = 0.5 sec
I
Ts = 1 sec
Determine for which values of Ts the discrete-time signal has lost the information in the analog signal. Use
MATLAB to plot the analog signal (use the plot function) and the resulting discrete-time signals (use the
stem function). Superimpose the analog and the discrete-time signals for 0 ≤t ≤3; use subplot to plot the
four ﬁgures as one ﬁgure. For plotting the analog signal use Ts = 10−4. You also need to ﬁgure out how to
label the different axes and have the same scales and units. In Chapter 7 on sampling we will show how to
reconstruct sampled signals.
0.3. Derivative and ﬁnite difference—MATLAB
Let y(t) = dx(t)/dt, where x(t) is the signal in Problem 0.2. Find y(t) analytically and determine a value of Ts
for which 1[x(nTs)]/Ts = y(nTs) (consider Ts = 0.01 and Ts = 0.1). Use the MATLAB function diff or create
your own to compute the ﬁnite difference. Plot the ﬁnite difference in the range [0,1] and compare it with
the actual derivative y(t) in that range. Explain your results for the given values of Ts.

Problems
55
0.4. Backward difference—MATLAB
Another deﬁnition for the ﬁnite difference is the backward difference:
1[x(nTs)] = x(nTs) −x((n −1)Ts)
(1[x(nTs)]/Ts approximates the derivative of x(t).)
(a) Indicate how this new deﬁnition connects with the ﬁnite difference deﬁned earlier in this chapter.
(b) Solve Problem 0.3 with MATLAB using this new ﬁnite difference and compare your results with the
ones obtained there.
(c) For the value of Ts = 0.1, use the average of the two ﬁnite differences to approximate the derivative of
the analog signal x(t). Compare this result with the previous ones. Provide an expression for calculating
this new ﬁnite difference directly.
0.5. Differential and difference equations—MATLAB
Find the differential equation relating a current source is(t) = cos(0t) with the current iL(t) in an inductor,
with inductance L = 1 H, connected in parallel with a resistor of R = 1 (see Figure 0.26). Assume a zero
initial current in the inductor.
(a) Obtain a discrete equation from the differential equation using the trapezoidal approximation of an
integral.
(b) Create a MATLAB script to solve the difference equation for Ts = 0.01 and three frequencies for
is(t), 0 = 0.005π, 0.05π, and 0.5π. Plot the input current source is(t) and the approximate solution
iL(nTs) in the same ﬁgure. Use the MATLAB function plot. Use the MATLAB function ﬁlter to solve the
difference equation (use help to learn about ﬁlter).
(c) Solve the differential equation using symbolic MATLAB when the input frequency is 0 = 0.5π.
(d) Use phasors to ﬁnd the amplitude of iL(t) when the input is is(t) with the given three frequencies.
FIGURE 0.26
Problem 0.5. RL circuit: input is(t) and output
iL(t).
is(t )
iL(t )
1H
1Ω
0.6. Sums and Gauss—MATLAB
Three rules in the computation of sums are
I
Distributive law:
X
k
cak = c
X
k
ak
I
Associative law:
X
k
(ak + bk) =
X
k
ak +
X
k
bk

56
CHAPTER 0:
From the Ground Up!
I
Commutative law:
X
k
ak =
X
p(k)
ap(k)
for any permutation p(k) of the set of integers k in the summation.
(a) Explain why the above rules make sense when computing sums. To do that consider
X
k
ak =
2
X
k=0
ak
and similarly for P
k bk. Let c be a constant, and choose any permutation of the values [0,1,2] for
instance [2,1,0] or [1,0,2].
(b) The trick that Gauss played when he was a preschooler can be explained by using the above rules.
Suppose you want to ﬁnd the sum of the integers from 0 to 10000 (Gauss did it for integers between 0
and 100 but he was then just a little boy, and we can do better!). That is, we want to ﬁnd S where
S =
10000
X
k=0
k = 0 + 1 + 2 + · · · + 10000
To do so, consider
2S =
10000
X
k=0
k +
0
X
k=10000
k
and apply the above rules to ﬁnd S.
(c) Find the sum of an arithmetic progression
S =
N
X
k=0
(α + βk)
for constants α and β, using the given three rules.
(d) Find out if MATLAB can do these sums symbolically (i.e., without having numerical values).
0.7. Integrals and sums—MATLAB
Suppose you wish to ﬁnd the area under a signal using sums. You will need the following result found
above:
N
X
n=0
n = N(N + 1)
2
(a) Consider ﬁrst x(t) = t, 0 ≤t ≤1, and zero otherwise. The area under this signal is 0.5. The integral can
be approximated from above and below as
N−1
X
n=1
(nTs)Ts <
1
Z
0
tdt <
N
X
n=1
(nTs)Ts

Problems
57
where NTs = 1 (i.e., we segment the interval [0,1] into N intervals of width Ts). Graphically show that
the above equation makes sense by showing the right and left bounds as approximations for the area
under x(t).
(b) Let Ts = 0.001. Use the symbolic function symsum to compute the left and right bounds for the above
integral. Find the average of these results and compare it with the actual value of the integral.
(c) Verify the symbolic results by ﬁnding the sums on the left and the right of the above inequality using
the summation given at the beginning of the problem. You need to change the dummy variables.
(d) Write a similar MATLAB script to compute the area under the signal y(t) = t2 from 0 ≤t ≤1. Let
Ts = 0.001. Compare the average of the lower and upper bounds to the value of the integral.
0.8. Integrals and sums—MATLAB
Although sums behave like integrals, because of the discrete nature of sums one needs to be careful with
the upper and lower limits more than in the integral case. To illustrate this, consider the separation of an
integral into two integrals and compare them with the separation of a sum into two sums. For the integral
we have that
1
Z
0
tdt =
0.5
Z
0
tdt +
1
Z
0.5
tdt
Show that this is true by computing the three integrals. Then consider the sum
S =
100
X
n=0
n
Find this sum and determine which of the following is equal to this sum:
S1 =
50
X
n=0
n +
100
X
n=50
n
S2 =
50
X
n=0
n +
100
X
n=51
n
Use symbolic MATLAB function symsum to verify your answers.
0.9. Sum of geometric series
The geometric series
S =
N−1
X
n=0
αn
will be used quite frequently in the next chapters, so let us look at some of its properties:
(a) Suppose α = 1; what is S equal to?
(b) Suppose α ̸= 1; show that
S = 1 −αN
1 −α
This can be done by showing that (1 −α)S = (1 −αN). Why do you need the constraint that α ̸= 1?
Would this sum exist if α > 1? Explain.

58
CHAPTER 0:
From the Ground Up!
(c) Give an expression of the above sum for all possible values of α.
(d) Suppose now that N = ∞; under what conditions will S exist? If it does, what would S be equal to?
Explain.
(e) Suppose the derivative of S with respect to α is
S1 = dS
dα =
∞
X
n=0
nαn
Obtain an expression to ﬁnd S1.
0.10. Exponentials—MATLAB
The exponential x(t) = eat for t ≥0 and zero otherwise is a very common analog signal. Likewise, y[n] = αn
for integers n ≥0 and zero otherwise is a very common discrete-time signal. Let us see how they are
related. Do the following using MATLAB:
(a) Let a = −0.5; plot x(t).
(b) Let a = −1; plot the corresponding signal x(t). Does this signal go to zero faster than the exponential
for a = −0.5?
(c) Suppose we sample the signal x(t) using Ts = 1; what would be x(nTs) and how can it be related to
y(n) (i.e., what is the value of α that would make the two equal)?
(d) Suppose that a current x(t) = e−0.5t for t ≥0 and zero otherwise is applied to a discharged capacitor
of capacitance C = 1 F at t = 0. What would be the voltage in the capacitor at t = 1 sec?
(e) How would you obtain an approximate result to the above problem using a computer? Explain.
0.11. Algebra of complex numbers
Consider complex numbers z = 1 + j1, w = −1 + j1, v = −1 −j1, and u = 1 −j1.
(a) In the complex plane, indicate the point (x, y) that corresponds to z and then show a vector ⃗z that joins
the point (x, y) to the origin. What is the magnitude and the angle corresponding to z or ⃗z?
(b) Do the same for the complex numbers w, v, and u. Plot the four complex numbers and ﬁnd their sum
z + w + v + u analytically and graphically.
(c) Find the ratios z/w, w/v, and u/z. Determine the real and imaginary parts of each, as well as their
magnitudes and phases. Using the ratios ﬁnd u/w.
(d) The phase of a complex number is only signiﬁcant when the magnitude of the complex number is
signiﬁcant. Consider z and y = 10−16z; compare their magnitudes and phases. What would you say
about the phase of y?
0.12. Algebra of complex numbers
Consider a function of z = 1 + j1,
w = ez
(a) Find log(w).
(b) Find the real and the imaginary parts of w.
(c) What is w + w∗, where w∗is the complex conjugate of w?
(d) Determine |w|, ∠w.
(e) What is | log(w)|2?
(f) Express cos(1) in terms of w using Euler’s equation.
0.13. Euler’s identity and trigonometric identities
Use Euler’s identity to obtain an expression for e j(α+β) = e jαe jβ; obtain its real and imaginary components
and show the following identities:
I
cos(α + β) = cos(α) cos(β) −sin(α) sin(β)
I
sin(α + β) = sin(α) cos(β) + sin(β) cos(β)
Hint: Find real and imaginary parts of e jαe jβ and of e j(α+β).

Problems
59
0.14. Euler’s identity and trigonometric identities
Use Euler’s identity to ﬁnd an expression for cos(α) cos(β), and from the relation between cosines and sines
obtain an expression for sin(α) sin(β).
0.15. Algebra of complex numbers
(a) The complex conjugate of z = x + jy is z∗= x −jy. Using these rectangular representations, show that
zz∗= x2 + y2
1
z = z∗
zz∗
(b) Show that it is easier to ﬁnd the above results by using the polar representation z = |z|e jθ of z where
|z| =
q
x2 + y2
is the magnitude of z and
θ = tan−1 y
x

is the angle or phase of z. Thus, whenever we are multiplying or dividing complex numbers the polar
form is more appropriate.
(c) Whenever we are adding or subtracting complex numbers the rectangular representation is more
appropriate. Show that for two complex numbers z = x + jy and w = v + jq; then,
(z + w)∗= z∗+ w∗
On the other hand, when showing that (zw)∗= z∗w∗the polar form is more appropriate.
(d) If the above conclusions still do not convince you, consider then the case of multiplying two complex
numbers:
z = r cos(θ) + jr sin(θ)
w = ρ cos(φ) + jρ sin(φ)
Find the polar forms of z and w and then ﬁnd zw by using the rectangular and then the polar forms
and decide which is easier. As a bonus you should get the trigonometric identities for cos(θ + φ) and
sin(θ + φ). What are they?
0.16. Vectors and complex numbers
Using the vectorial representation of complex numbers it is possible to get some interesting inequalities:
(a) Is it true that for a complex number z = x + jy:
|x| ≤|z|?
Show it geometrically by representing z as a vector.
(b) The so-called triangle inequality says that for any complex (or real) numbers z and v we have that
|z + v| ≤|z| + |v|
Show a geometric example that veriﬁes this.
0.17. Complex functions of time—MATLAB
Consider the complex function x(t) = (1 + jt)2 for −∞< t < ∞.
(a) Find the real and the imaginary parts of x(t) and carefully plot them with MATLAB. Try to make
MATLAB plot x(t) directly. What do you get? Does MATLAB warn you? Does it make sense?

60
CHAPTER 0:
From the Ground Up!
(b) Compute the derivative y(t) = dx(t)/dt and plot its real and imaginary parts. How do these relate to the
real and the imaginary parts of x(t)?
(c) Compute the integral
1
Z
0
x(t)dt
(d) Would the following statement be true (remember ∗indicates complex conjugate)?


1
Z
0
x(t)dt


∗
=
1
Z
0
x∗(t)dt
0.18. Euler’s equation and orthogonality of sinusoids
Euler’s equation,
e jθ = cos(θ) + j sin(θ)
is very useful not only in obtaining the rectangular and polar forms of complex numbers, but in many other
respects as we will explore in this problem.
(a) Carefully plot x[n] = e jπn for −∞< n < ∞. Is this a real or a complex signal?
(b) Suppose you want to ﬁnd the trigonometric identity corresponding to
sin(α) sin(β)
Use Euler’s equation to express the sines in terms of exponentials, multiply the resulting exponentials,
and use Euler’s equation to regroup the expression in terms of sinusoids.
(c) As we will see later on, two periodic signals x(t) and y(t) of period T0 are said to be orthogonal if the
integral over a period T0 is
Z
T0
x(t)y(t)dt = 0
For instance, consider x(t) = cos(πt) and y(t) = sin(πt). Check ﬁrst that these functions repeat every
T0 = 2 (i.e., show that x(t + 2) = x(t) and that y(t + 2) = y(t)). Thus, T0 = 2 can be seen as their period.
Then use the representation of a cosine in terms of complex exponentials,
cos(θt) = e jθ + e−jθ
2
to express the integrand in terms of exponentials and calculate the integral.
0.19. Euler’s equation and trigonometric expressions
Obtain using Euler’s equation an expression for sin(θ) in terms of exponentials and then
(a) Use it to obtain the trigonometric identity for sin2(θ).
(b) Compute the integral
1
Z
0
sin2(2πt)dt

Problems
61
0.20. De Moivre’s theorem for roots
Consider the calculation of roots of an equation,
zN = α
where N ≥1 is an integer and α = |α|e jφ a nonzero complex number.
(a) First verify that there are exactly N roots of this equation and that they are given by
zk = re jθk
where r = |α|1/N and θk = (φ + 2πk)/N for k = 0, 1, . . . , N −1.
(b) Use the above result to ﬁnd the roots of the following equations:
z2 = 1
z2 = −1
z3 = 1
z3 = −1
and plot them in a polar plane (i.e., indicating their magnitude and phase).
(c) Explain how the roots are distributed around a circle of radius r in the complex polar plane.
0.21. Natural log of complex numbers
Suppose you want to ﬁnd the log of a complex number z = |z|e jθ. Its logarithm can be found to be
log(z) = log(|z|e jθ) = log(|z|) + log(e jθ) = log(|z|) + jθ
If z is negative it can be written as z = |z|e jπ and we can ﬁnd log(z) by using the above derivation. The log
of any complex number can be obtained this way also.
(a) Justify each one of the steps in the above equation.
(b) Find
log(−2)
log(1 + j1)
log(2e jπ/4)
0.22. Hyperbolic sinusoids—MATLAB
In ﬁlter design you will be asked to use hyperbolic functions. In this problem we relate these functions to
sinusoids and obtain a deﬁnition of these functions so that we can actually plot them.
(a) Consider computing the cosine of an imaginary number—that is, use
cos(x) = e jx + e−jx
2
Let x = jθ and ﬁnd cos(x). The resulting function is called the hyperbolic cosine or
cos( jθ) = cosh(θ)
(b) Consider then the computation of the hyperbolic sine sinh(θ); how would you do it? Carefully plot it
as a function of θ.

62
CHAPTER 0:
From the Ground Up!
(c) Show that the hyperbolic cosine is always positive and bigger than 1 for all values of θ.
(d) Show that sinh(θ) = −sinh(−θ).
(e) Write a MATLAB script to compute and plot these functions between −10 and 10.
0.23. Phasors!
A phasor can be thought of as a vector, representing a complex number, rotating around the polar plane
at a certain frequency expressed in radians/sec. The projection of such a vector onto the real axis gives a
cosine. This problem will show the algebra of phasors, which would help you with some of the trigonometric
identities that are hard to remember.
(a) When you plot a sine signal y(t) = A sin(0t), you notice that it is a cosine x(t) = A cos(0t) shifted in
time—that is,
y(t) = A sin(0t) = A cos(0(t −1t)) = x(t −1t)
How much is this shift 1t? Better yet, what is 1θ = 01t or the shift in phase? One thus only need to
consider cosine functions with different phase shifts instead of sines and cosines.
(b) You should have found the answer above is 1θ = π/2 (if not, go back and try it and see if it works).
Thus, the phasor that generates x(t) = A cos(0t) is Ae j0 so that x(t) = Re[Ae j0e j0t]. The phasor
corresponding to the sine y(t) should then be Ae−jπ/2. Obtain an expression for y(t) similar to the one
for x(t) in terms of this phasor.
(c) According to the above results, give the phasors corresponding to −x(t) = −A cos(0t) and −y(t) =
−sin(0t). Plot the phasors that generate cos, sin, −cos, and −sin for a given frequency. Do you see
now how these functions are connected? How many radians do you need to shift in a positive or
negative direction to get a sine from a cosine, etc.
(d) Suppose then you have the sum of two sinusoids, for instance z(t) = x(t) + y(t), adding the corre-
sponding phasors for x(t) and y(t) at some time (e.g., t = 0), which is just a sum of two vectors, you
should get a vector and the corresponding phasor. Get the phasor for z(t) and the expression for it in
terms of a cosine.

2
PART
Theory and Application of
Continuous-Time Signals and
Systems

This page intentionally left blank

CHAPTER 1
Continuous-Time Signals
A journey of a thousand miles
begins with a single step.
Lao Tzu (604–531 BCE)
Chinese philosopher
1.1 INTRODUCTION
In this second part of the book, we will concentrate on the representation and processing of
continuous-time signals. Such signals are familiar to us. Voice, music, as well as images and video
coming from radios, cell phones, IPods, and MP3 players exemplify these signals. Clearly each of
these signals has some type of information, but what is not clear is how we could capture, represent,
and perhaps modify these signals and their information content.
To process signals we need to understand their nature—to classify them—so as to clarify the limita-
tions of our analysis and our expectations. Several realizations could then come to mind. One could
be that almost all signals vary randomly and continuously with time. Consider a voice signal. If you
are able to capture such a signal, by connecting a microphone to your computer and using the hard-
ware and software necessary to display it, you realize that when you speak into the microphone a
rather complicated signal that changes in unpredictable ways is displayed. You would ask yourself
how is it that your spoken words are converted into this signal, and how could it be represented
mathematically to allow you to develop algorithms to change it. In this book we consider the rep-
resentation of deterministic—rather than random—signals, clearly a ﬁrst step in the long process of
answering these signiﬁcant questions.
A second realization could be that to input signals into a computer the signals must be in binary
form. How do we convert the voltage signal generated by the microphone into a binary form? This
requires that we compress the information in a way that permits us to get it back, as when we wish
to listen to the voice signal stored in the computer.
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00004-1
c⃝2011, Elsevier Inc. All rights reserved.
65

66
CHAPTER 1:
Continuous-Time Signals
One more realization could be that the processing of signals requires us to consider systems. In our
example, one could think of the human vocal system and of a microphone as a system that converts
differences in air pressure into a voltage signal. Signals and systems go together. We will consider the
interaction of signals and systems in the next chapter.
Speciﬁcally in this chapter we will discuss the following issues:
I
The mathematical representation of signals—Generally, how to think of a signal as a function of
either time (e.g., music and voice signals), space (e.g., images), or of time and space (e.g., videos).
In this book we will concentrate on time-dependent signals.
I
Classiﬁcation of signals—Using practical characteristics of signals we offer a classiﬁcation of signals
indicating the way a signal is stored, processed, or both. As indicated, this second part of the book
will concentrate on the representation and analysis of continuous-time signals and systems, while
the next part will discuss the representation and analysis of discrete-time signals and systems.
I
Signal manipulation—What it means to delay or advance a signal, to reﬂect it, or to ﬁnd its odd
or even components. These are signal operations that will help us in their representation and
processing.
I
Basic signal representation—We show that any signal can be represented using basic signals. This
will permit us to highlight certain characteristics of the signal and to simplify ﬁnding the cor-
responding outputs of systems. In particular, the representation in terms of sinusoids is of great
interest as it allows the development of the so-called Fourier representation, which is essential in
the development of the theory of linear systems.
1.2 CLASSIFICATION OF TIME-DEPENDENT SIGNALS
Considering signals as functions of time-carrying information, there are many ways in which they
can be classiﬁed:
(a)
According to the predictability of their behavior, signals can be random or deterministic. While
a deterministic signal can be represented by a formula or a table of values, random signals can
only be approached probabilistically. In this book we will only consider deterministic signals.
(b)
According to the variation of their time variable and their amplitude, signals can be either
continuous-time or discrete-time, analog or discrete amplitude, or digital. This classiﬁcation relates
to the way signals are either processed, stored, or both.
(c)
According to their energy content, signals can be characterized as ﬁnite- or inﬁnite-energy signals.
(d)
According to whether the signals exhibit repetitive behavior or not as periodic or aperiodic signals.
(e)
According to the symmetry with respect to the time origin, signals can be even or odd.
(f)
According to the dimension of their support, signals can be of ﬁnite or of inﬁnite support. Sup-
port can be understood as the time interval of the signal outside of which the signal is always
zero.

1.3 Continuous-Time Signals
67
1.3 CONTINUOUS-TIME SIGNALS
That signals are functions of time-carrying information is easily illustrated with a recorded voice
signal. Such a signal can be thought of as a continuously varying voltage, generated by a microphone,
that can be transformed into an audible acoustic signal—providing the voice information—by means
of an ampliﬁer and speakers. Thus, the speech signal is represented by a function of time
v(t),
tb ≤t ≤tf
(1.1)
where tb is the time at which this signal starts, and tf the time at which it ends. The function v(t)
varies continuously with time, and its amplitude can take any possible value (as long as the speakers
are not too loud!). This signal obviously carries the information provided by the voice message.
Not all signals are functions of time alone. A digital image stored in a computer provides visual
information. The intensity of the illumination of the image depends on its location within the image.
Thus, a digital image can be represented as a function of two space variables (m, n) that vary discretely,
creating an array of values called picture elements or pixels. The visual information in the image is thus
provided by the signal p(m, n) where 0 ≤m ≤M −1 and 0 ≤n ≤N −1 for an image of size M × N
pixels. Each of the pixel values can be represented, for instance, by 256 gray scale values or 8 bits/pixel.
Thus, the signal p(m, n) varies discretely in space and in amplitude. A video, as a sequence of images
in time, is accordingly a function of time and of two space variables. How their time or space variables
and their amplitudes vary characterizes signals.
For a time-dependent signal, time and amplitude vary continuously or discretely. Thus, according
to the independent variable, signals are continuous-time or discrete-time signals—that is, t takes an
innumerable or a ﬁnite set of values. Likewise, the amplitude of either a continuous-time or a discrete-
time signal can vary continuously or discretely. Thus, continuous-time signals can be continuous-
amplitude as well as discrete-amplitude signals. Continuous-amplitude, continuous-time signals are
called analog signals given that they resemble the pressure variations caused by an acoustic signal. A
continuous-amplitude, discrete-time signal is called a discrete-time signal. A digital signal has discrete
time and discrete amplitude. If the samples of a digital signal are given as binary codes the signal is
called a binary signal.
A good way to illustrate the signal classiﬁcation is to consider the steps needed to process the voice
signal v(t) in Equation (1.1) with a computer. As indicated above, in v(t) time varies continuously
between tb and tf, and the amplitude also varies continuously, and we assume it could take any
possible real value (i.e., v(t) is an analog signal). As such, v(t) cannot be processed with a computer.
It would require to store an innumerable number of signal values (even when tb is very close to tf)
and for an accurate representation of the amplitude values v(t), we might need a large number of
bits. Thus, it is necessary to reduce the amount of data without losing the information provided by
the signal. To accomplish that, we sample the signal by taking signal values at equally spaced times
nTs, where n is an integer and Ts is the sampling period, which is appropriately chosen for this signal
(in Chapter 7 we will learn how to chose Ts).

68
CHAPTER 1:
Continuous-Time Signals
As a result of the sampling, we obtain the discrete-time signal
v(nTs) = v(t)|t=nTs
0 ≤n ≤N −1
(1.2)
where Ts = (tf −tb)/N and we have taken samples at times tb + Tsn. Clearly, this discretization of the
time variable reduces the number of values to enter into the computer, but the amplitudes of these
samples still can take possibly innumerable values. Now, to represent each of the v(nTs) values with a
certain number of bits, we also discretize the amplitude of the samples. To do so, the dynamic range
(the difference between the maximum and the minimum amplitude) of the analog signal is equally
divided into a certain number of levels. A sample value falling within one of these levels is allocated
a unique binary code. For instance, if we want each sample to be represented by 8 bits we have 28 or
256 possible levels. These operations are called quantization and coding. The resulting signal is digital,
where each sample is represented as a binary number.
Given that many of the signals we encounter in practical applications are analog, if it is desirable
to process such signals with a computer, the above procedure is commonly done. The device that
converts an analog signal into a digital signal is called an analog-to-digital converter (ADC) and
it is characterized by the number of samples it takes per second (sampling rate 1/Ts) and by the
number of bits that it allocates to each sample. To convert a digital signal into an analog signal a
digital-to-analog converter (DAC) is used. Such a device inverts the ADC process: binary values are
converted into pulses with amplitudes approximating those of the original samples, which are then
smoothed out resulting in an analog signal. We will discuss in Chapter 7 how the sampling, binary
representation, and reconstruction of an analog signal is done.
Figure 1.1 shows how the discretization of an analog signal in time and amplitude can be understood,
while Figure 1.2 illustrates the sampling and quantization of a segment of speech.
A continuous-time signal can be thought of as a real-(or complex-) valued function of time:
x(.) : R →R (C)
t
x(t)
(1.3)
FIGURE 1.1
Discretization in time and amplitude of an analog
signal. The parameters are the sampling period
Ts and the quantization level 1. In time, samples
are taken at uniform times {nTs}, and in amplitude
the range of amplitudes is divided into a ﬁnite
number of levels so that each sample value is
approximated by them.
x(t )
Level
2Ts
Ts
3Ts
x(nTs)
4Ts
t
− Δ
− Δ/2
Δ/2
Δ

1.3 Continuous-Time Signals
69
0
2
4
6
8
10
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
(a)
t(sec)
v(t)
1.45
1.5
1.55
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
(b)
t, nTs
v(t), v(nTs)
1.45
1.5
1.55
−0.5
0
0.5
(d)
(c)
v(nTs), vq(nTs)
1.45
1.5
1.55
−0.5
0
0.5
nTs
e(nTs)
FIGURE 1.2
(a) A segment of this speech signal is sampled and quantized. (b) The speech segment (continuous line) and
the sampled signal (vertical samples) using a sampling period Ts = 10−3 sec. (c) The sampled and the
quantized signal. (d) The quantization error (that is, the difference between the sampled and the quantized
signals) is shown.
Thus, the independent variable is time t, and the value of the function at some time t0, x(t0), is a real (or a
complex) value. (Although in practice signals are real, it is useful in theory to have the option of complex-
valued signals.) It is assumed that both time t and signal amplitude x(t) can vary continuously, if needed, from
−∞to ∞.
The term analog used for continuous-time signals derives from the similarity of acoustic signals to the
pressure variations generated by voice, music, or any other acoustic signal. The terms continuous-time
and analog are used interchangeably for these signals.

70
CHAPTER 1:
Continuous-Time Signals
I Example 1.1
Characterize the sinusoidal signal
x(t) =
√
2 cos(πt/2 + π/4)
−∞< t < ∞
Solution
The signal x(t) is
I
Deterministic, as the value of the signal can be obtained for any possible value of t.
I
Analog, as there is a continuous variation of the time variable t from −∞to ∞, and of the
amplitude of the signal between −
√
2 to
√
2.
I
Of inﬁnite support, as the signal does not become zero outside any ﬁnite interval.
The amplitude of the sinusoid is
√
2, its frequency is  = π/2 (rad/sec), and its phase is π/4 rad
(notice that t has radians as units so that it can be added to the phase). Because of the inﬁnite
support, this signal cannot exist in practice, but we will see that sinusoids are extremely important
in the representation and processing of signals.
I
I Example 1.2
A complex signal y(t) is deﬁned as
y(t) = (1 + j)ejπt/2
0 ≤t ≤10
and zero otherwise. Express y(t) in terms of the signal x(t) from Example 1.1. Characterize y(t).
Solution
Since 1 + j =
√
2ejπ/4, then using Euler’s identity:
y(t) =
√
2ej(πt/2+π/4) =
√
2 [cos(πt/2 + π/4) + j sin(πt/2 + π/4)]
0 ≤t ≤10
Thus, the real and imaginary parts of this signal are
Re[y(t)] =
√
2 cos(πt/2 + π/4)
Im[y(t)] =
√
2 sin(πt/2 + π/4)
for 0 ≤t ≤10 and zero otherwise. The signal y(t) can be written as
y(t) = x(t) + jx(t −1)
0 ≤t ≤10
and zero otherwise. Notice that
x(t −1) =
√
2 cos(π(t −1)/2 + π/4) =
√
2 cos(πt/2 −π/2 + π/4) =
√
2 sin(πt/2 + π/4)
The signal y(t) is
I
Analog of ﬁnite support—that is, the signal is zero outside the interval 0 ≤t ≤10.

1.3 Continuous-Time Signals
71
I
Complex, composed of two sinusoids of frequency  = π/2 rad/sec, phase π/4 in rad, and
amplitude
√
2 in 0 ≤t ≤10, and it is zero outside that time interval.
I
I Example 1.3
Consider the pulse signal
p(t) = 1
0 ≤t ≤10
and zero elsewhere. Characterize this signal, and use it along with x(t) in Example 1.1, to represent
y(t) in the above example.
Solution
The analog signal p(t) is of ﬁnite support and real-valued. We have that
Re[y(t)] = x(t)p(t)
Im[y(t)] = x(t −1)p(t)
so that
y(t) = [x(t) + jx(t −1)]p(t)
The multiplication by p(t) makes x(t)p(t) and x(t −1)p(t) ﬁnite-support signals. This operation is
called time windowing as the signal p(t) only allows us to see the values of x(t) wherever p(t) = 1,
while ignoring the values of x(t) wherever p(t) = 0. It acts like a window.
I
Examples 1.1–1.3 not only illustrate how different types of signal can be related to each other, but
also how signals can be be deﬁned in shorter or more precise forms. Although the representations for
y(t) in Example 1.2 and in this example are equivalent, the one here is shorter and easier to visualize
by the use of the pulse p(t).
1.3.1 Basic Signal Operations—Time Shifting and Reversal
The following are basic signal operations used in the representation and processing of signals (for
some of these operations we indicate the system that is used to realize the operation):
I
Signal addition—Two signals x(t) and y(t) are added to obtain their sum z(t). An adder is used.
I
Constant multiplication—A signal x(t) is multiplied by a constant α. A constant multiplier is used.
I
Time and frequency shifting—The signal x(t) is delayed τ seconds to get x(t −τ), and advanced by
τ to get x(t + τ). A signal can be shifted in frequency or frequency modulated by multiplying it
by a complex exponential or a sinusoid. A delay shifts right a time signal, while a modulator shifts
the signal in frequency.
I
Time scaling—The time variable of a signal x(t) is scaled by a constant α to give x(αt). If α = −1,
the signal is reversed in time (i.e., x(−t)), or reﬂected. Only the delay can be implemented in
practice.
I
Time windowing—A signal x(t) is multiplied by a window signal w(t) so that x(t) is available in the
support of w(t).

72
CHAPTER 1:
Continuous-Time Signals
FIGURE 1.3
Diagrams of basic signal operations: (a) adder,
(b) constant multiplier, (c) delay, and (d) time
windowing or modulation.
(a) 
x(t)
z (t) = x(t) + y(t)
y(t)
+
+
(d) 
×
x(t)
w(t)
x(t)   w(t)
(b)
α
x(t)
α   x(t)
(c) 
x(t)
Delay τ
x(t−τ)
Given the simplicity of the ﬁrst two operations we will only discuss the others. In this section we
consider time shifting and reﬂection (a special case of the time scaling) and leave the rest for a later
section.
In Figure 1.3 we show the diagrams used for the implementation of the addition of two signals, the
multiplication of a signal by a constant, the delay of a signal, and the time windowing or modulation
of a signal. These will be used in the block diagrams for systems in the next chapters.
It is important to understand that advancing or reﬂecting cannot be implemented in real time—that
is as the signal is being processed. Delays can be implemented in real time. Advancing and reﬂection
require that the signal be saved or recorded. Thus, an acoustic signal recorded on magnetic tape can
be delayed or advanced with respect to an initial time, or played back, faster or slower, but it can only
be delayed if we have the signal coming from a live microphone.
We will see later in this chapter that shifting in frequency results in the process of signal modulation,
which is of great signiﬁcance in communications. Scaling of the time variable results in a contracted
and expanded version of the original signal and causes changes in the frequency content of the signal.
I
For a positive value τ, a signal x(t −τ) is the original signal x(t) shifted right or delayed τ seconds,
as illustrated in Figure 1.4(b). That the original signal has been shifted to the right can be veriﬁed
by ﬁnding that the x(0) value of the original signal appears in the delayed signal at t = τ (which
results from making t −τ = 0).
I
Likewise, a signal x(t + τ) is the original signal x(t) shifted left or advanced by τ seconds as illus-
trated in Figure 1.4(c). The original signal is now shifted to the left—that is, the value x(0) of the
original signal occurs now earlier (i.e., it has been advanced) at time t = −τ.
I
Reﬂection consists in negating the time variable. Thus, the reﬂection of x(t) is x(−t). This operation
can be visualized as ﬂipping the signal about the origin. See Figure 1.4(d).
Given an analog signal x(t) and τ > 0 we have that with respect to x(t):
(a)
x(t −τ) is delayed or shifted right τ seconds.
(b)
x(t + τ) is advanced or shifted left τ seconds.

1.3 Continuous-Time Signals
73
FIGURE 1.4
(a) Continuous-time signal, and its
(b) delayed, (c) advanced, and
(d) reﬂected versions.
(a)
(c) 
t
x(t)
−τ
t
x (t + τ)
(b)
(d)
τ
t
x (t−τ )
t
x (−t )
(c)
x(−t) is reﬂected.
(d)
x(−t −τ) is reﬂected and shifted left τ seconds, while x(−t + τ) is reﬂected and shifted right τ seconds.
Remarks Whenever we combine the delaying or advancing with reﬂection, delaying and advancing are
swapped. Thus, x(−t + 1) is x(t) reﬂected and delayed, or shifted to the right, by 1. Likewise, x(−t −1) is
x(t) reﬂected and advanced, or shifted to the left by 1. Again, the value x(0) of the original signal is found in
x(−t + 1) at t = 1, and in x(−t −1) at t = −1.
I Example 1.4
Consider an analog pulse
x(t) =
1
0 ≤t ≤1
0
otherwise
Find mathematical expressions for x(t) delayed by 2, advanced by 2, and the reﬂected signal x(−t).
Solution
The delayed signal x(t −2) can be found mathematically by replacing the variable t by t −2 so that
x(t −2) =
1
0 ≤t −2 ≤1 or 2 ≤t ≤3
0
otherwise
The value x(0) (which in x(t) occurs at t = 0) in x(t −2) now occurs when t = 2, so that the signal
x(t) has been shifted to the right two units of time, and since the values are occurring later, the
signal x(t −2) is said to be “delayed” by 2 with respect to x(t).

74
CHAPTER 1:
Continuous-Time Signals
Likewise, we have that
x(t + 2) =
1
0 ≤t + 2 ≤1 or −2 ≤t ≤−1
0
otherwise
The signal x(t + 2) can be seen to be the advanced version of x(t), as it is this signal shifted to the
left by two units of time. The value x(0) for x(t + 2) now occurs at t = −2, which is ahead of t = 0.
Finally, the signal x(−t) is given by
x(−t) =
1
0 ≤−t ≤1 or −1 ≤t ≤0
0
otherwise
This signal is a mirror image of the original: the value x(0) still occurs at the same time, but x(1)
occurs when t = −1.
I
I Example 1.5
When the shifting and reﬂecting operations are considered together the best approach to visual-
ize the operation is to make a table computing several values of the new signal and comparing
these with those from the original signal. Consider the pulse in Example 1.4, and plot the signal
x(−t + 2).
Solution
Although one can see that this signal is reﬂected, it is not clear whether it is advanced or delayed
by 2. By computing a few values:
t
x(−t + 2)
2
x(0) = 1
1.5
x(0.5) = 1
1
x(1) = 1
0
x(2) = 0
−1
x(3) = 0
it becomes clear that x(−t + 2) is reﬂected and “delayed” by 2. In fact, as indicated above, whenever
the signal is a function of −t (i.e., reﬂected), the −t + τ operation becomes reﬂection and “delay,”
and −t −τ becomes reﬂection and “advancing.”
I
Remarks When computing the convolution integral later on, we will consider the signal x(t −τ) as a
function of τ for different values of t. As indicated from Example 1.5, this signal is a reﬂected version of
x(τ) being shifted to the right t seconds. To see this, consider t = 0 then x(t −τ)|t=0 = x(−τ), the reﬂected
version, and x(0) occurs at τ = 0. When t = 1, then x(t −τ)|t=1 = x(1 −τ) and x(0) occurs at τ = 1, so
that x(1 −τ) is x(−τ) shifted to the right by 1, and so on.

1.3 Continuous-Time Signals
75
1.3.2 Even and Odd Signals
Symmetry with respect to the origin differentiates signals and will be useful in their Fourier analysis.
We have that an analog signal x(t) is called
I
Even whenever x(t) coincides with its reﬂection x(−t). Such a signal is symmetric with respect to
the time origin.
I
Odd whenever x(t) coincides with −x(−t)—that is, the negative of its reﬂection. Such a signal is
asymmetric with respect to the time origin.
Even and odd signals are deﬁned as follows:
x(t)
even :
x(t) = x(−t)
(1.4)
x(t)
odd :
x(t) = −x(−t)
(1.5)
Even and odd decomposition: Any signal y(t) is representable as a sum of an even component ye(t) and an
odd component yo(t):
y(t) = ye(t) + yo(t)
(1.6)
where
ye(t) = 0.5 [y(t) + y(−t)]
(1.7)
yo(t) = 0.5 [y(t) −y(−t)]
(1.8)
Using the deﬁnitions of even and odd signals, any signal y(t) can be decomposed into the sum of an
even and an odd function. Indeed, the following is an identity:
y(t) = 1
2 [y(t) + y(−t)] + 1
2 [y(t) −y(−t)]
where the ﬁrst term is the even and the second is the odd components of y(t). It can be easily veriﬁed
that ye(t) is even and that yo(t) is odd.
I Example 1.6
Consider the analog signal
x(t) = cos(2πt + θ)
−∞< t < ∞
Determine the value of θ for which x(t) is even and odd. If θ = π/4, is x(t) = cos(2πt + π/4),
−∞< t < ∞, even or odd?

76
CHAPTER 1:
Continuous-Time Signals
Solution
The reﬂection of x(t) is x(−t) = cos(−2πt + θ). Then:
1.
x(t) is even if x(t) = x(−t) or
cos(2πt + θ) = cos(−2πt + θ)
= cos(2πt −θ)
or θ = −θ or θ = 0, π. Thus, x1(t) = cos(2πt) as well as x2(t) = cos(2πt + π) = −cos(2πt) are
even.
2.
for x(t) to be odd, we need that x(t) = −x(−t) or
cos(2πt + θ) = −cos(−2πt + θ) = cos(−2πt + θ ± π) = cos(2πt −θ ∓π)
which can be obtained with θ = −θ ∓π or θ = ∓π/2. Indeed, cos(2πt −π/2) = sin(2πt) and
cos(2πt + π/2) = −sin(2πt) are both odd. Thus, x3(t) = ± sin(2πt) is odd.
When θ = π/4, x(t) = cos(2πt + π/4) is neither even nor odd according to the above.
I
I Example 1.7
Consider the signal
x(t) =
2 cos(4t)
t > 0
0
otherwise
Find its even and odd decomposition. What would happen if x(0) = 2 instead of 0—that is, when
we deﬁne the sinusoid at t = 0? Explain.
Solution
The signal x(t) is neither even nor odd given that its values for t ≤0 are zero. For its even–odd
decomposition, the even component is given by
xe(t) = 0.5[x(t) + x(−t)]
=



cos(4t)
t > 0
cos(4t)
t < 0
0
t = 0
and the odd component is given by
xo(t) = 0.5[x(t) −x(−t)]
=



cos(4t)
t > 0
−cos(4t)
t < 0
0
t = 0
which when added together become the given signal.

1.3 Continuous-Time Signals
77
If x(0) = 2, we have
xe(t) = 0.5[x(t) + x(−t)]
=



cos(4t)
t > 0
cos(4t)
t < 0
2
t = 0
while the odd component is the same. The even component has a discontinuity at t = 0.
I
1.3.3 Periodic and Aperiodic Signals
A useful characterization of signals is whether they are periodic or aperiodic (nonperiodic).
An analog signal x(t) is periodic if
I
it is deﬁned for all possible values of t, −∞< t < ∞, and
I
there is a positive real value T0, the period of x(t), such that
x(t + kT0) = x(t)
(1.9)
for any integer k.
The period of x(t) is the smallest possible value of T0 > 0 that makes the periodicity possible. Thus, although
NT0 for an integer N > 1 is also a period of x(t) it should not be considered the period.
Remarks
I
The inﬁnite support and the unique characteristic of the period make periodic signals nonexistent in
practical applications. Despite this, periodic signals are of great signiﬁcance in the Fourier representation
of signals and in their processing, as we will see later. The representation of aperiodic signals is obtained
from that of periodic signals, and the response of systems to periodic sinusoids is fundamental in the theory
of linear systems.
I
Although seemingly redundant, the ﬁrst part of the deﬁnition of a periodic signal indicates that it is not
possible to have a nonzero periodic signal with a ﬁnite support (i.e., the analog signal is zero outside an
interval t ∈[t1, t2]). This ﬁrst part of the deﬁnition is needed for the second part to make sense.
I
It is exasperating to ﬁnd the period of a constant signal x(t) = A; visually x(t) is periodic but its period
is not clear. Any positive value could be considered the period, but none will be taken. The reason is that
x(t) = A = A cos(0t) or of zero frequency, and as such its period is not determined since we would have
to divide by zero—not permitted. Thus, a constant signal is a periodic signal of nondeﬁnable period!
I Example 1.8
Consider the analog sinusoid
x(t) = A cos(0t + θ)
−∞< t < ∞
Determine the period of this signal, and indicate for what frequency 0 the period of x(t) is not
clearly deﬁned.

78
CHAPTER 1:
Continuous-Time Signals
Solution
The analog frequency is 0 = 2π/T0 so T0 = 2π/0 is the period. Whenever T0 > 0 (or 0 > 0)
these sinusoidals are periodic. For instance, consider
x(t) = 2 cos(2t −π/2)
−∞< t < ∞
Its period is found by noticing that this signal has an analog frequency 0 = 2 = 2πf0 (rad/sec),
or a hertz frequency of f0 = 1/π = 1/T0, so that T0 = π is the period in seconds. That this is the
period can be seen for an integer N,
x(t + NT0) = 2 cos(2(t + NT0) −π/2) = 2 cos(2t + 2πN −π/2)
= 2 cos(2t −π/2) = x(t)
since adding 2πN (a multiple of 2π) to the angle of the cosine gives the original angle. If 0 = 0—
that is, dc frequency—the period cannot be deﬁned because of the division by zero when ﬁnding
T0 = 2π/0.
I
I Example 1.9
Consider a periodic signal x(t) of period T0. Determine whether the following signals are periodic,
and if so, ﬁnd their corresponding periods:
(a)
y(t) = A + x(t).
(b)
z(t) = x(t) + v(t) where v(t) is periodic of period T1 = NT0, where N is a positive integer.
(c)
w(t) = x(t) + u(t) where u(t) is periodic of period T1, not necessarily a multiple of T0.
Determine under what conditions w(t) could be periodic.
Solution
(a)
Adding a constant to a periodic signal does not change the periodicity, so y(t) is periodic of
period T0—that is, for an integer k, y(t + kT0) = A + x(t + kT0) = A + x(t) since x(t) is periodic
of period T0.
(b)
The period T1 = NT0 of v(t) is also a period of x(t), and so z(t) is periodic of period T1 since
for any integer k,
z(t + kT1) = x(t + kT1) + v(t + kT1) = x(t + kNT0) + v(t) = x(t) + v(t)
given that v(t + kT1) = v(t), and that kN is an integer so that x(t + kNT0) = x(t). The peri-
odicity can be visualized by considering that in one period of v(t) we can place N periods
of x(t).
(c)
The condition for w(t) to be periodic is that the ratio of the periods of x(t) and of u(t) be
T1
T0
= N
M

1.3 Continuous-Time Signals
79
where N and M are positive integers not divisible by each other so that MT1 = NT0 becomes
the period of w(t). That is,
w(t + MT1) = x(t + MT1) + u(t + MT1) = x(t + NT0) + u(t + MT1) = x(t) + u(t)
I
I Example 1.10
Let x(t) = ej2t and y(t) = ejπt, and consider their sum z(t) = x(t) + y(t), and their product w(t) =
x(t)y(t). Determine if z(t) and w(t) are periodic, and if so, ﬁnd their periods. Is p(t) = (1 + x(t))(1 +
y(t)) periodic?
Solution
According to Euler’s identity,
x(t) = cos(2t) + j sin(2t)
y(t) = cos(πt) + j sin(πt)
indicating x(t) is periodic of period T0 = π (the frequency of x(t) is 0 = 2 = 2π/T0) and y(t) is
periodic of period T1 = 2 (the frequency of y(t) is 1 = π = 2π/T1).
For z(t) to be periodic requires that T1/T0 be a rational number, which is not the case as T1/T0 =
2/π. So z(t) is not periodic.
The product is w(t) = x(t)y(t) = ej(2+π)t = cos(2t) + j sin(2t) where 2 = 2 + π = 2π/T2 so
that T2 = 2π/(2 + π), so w(t) is periodic of period T2.
The terms 1 + x(t) and 1 + y(t) are periodic of period T0 = π and T1 = 2, and from the case of the
product above, one would hope this product be periodic. But since p(t) = 1 + x(t) + y(t) + x(t)y(t)
and x(t) + y(t) is not periodic, then p(t) is not periodic.
I
I
Analog sinusoids of frequency 0 > 0 are periodic of period T0 = 2π/0. If 0 = 0, the period is not
well deﬁned.
I
The sum of two periodic signals x(t) and y(t), of periods T1 and T2, is periodic if the ratio of the periods
T1/T2 is a rational number N/M, with N and M being nondivisible. The period of the sum is MT1 = NT2.
I
The product of two sinusoids is periodic. The product of two periodic signals is not necessarily periodic.
1.3.4 Finite-Energy and Finite Power Signals
Another possible classiﬁcation of signals is based on their energy and power. The concepts of energy
and power introduced in circuit theory can be extended to any signal. Recall that for a resistor of unit
resistance its instantaneous power is given by
p(t) = v(t)i(t) = i2(t) = v2(t)

80
CHAPTER 1:
Continuous-Time Signals
where i(t) and v(t) are the current and voltage in the resistor. The energy in the resistor for an interval
[t0, t1], of duration T = t1 −t0, is the accumulation of instantaneous power over that time interval,
ET =
t1
Z
t0
p(t)dt =
t1
Z
t0
i2(t)dt =
t1
Z
t0
v2(t)dt
The power in the interval T = t1 −t0 is the average energy
PT = ET
T = 1
T
t1
Z
t0
i2(t)dt = 1
T
t1
Z
t0
v2(t)dt
corresponding to the heat dissipated by the resistor (and for which you pay the electric company).
The energy and power concepts can thus be easily generalized.
The energy and the power of an analog signal x(t) are deﬁned for either ﬁnite or inﬁnite-support signals as:
Ex =
∞
Z
−∞
|x(t)|2dt
(1.10)
Px = lim
T→∞
1
2T
T
Z
−T
|x(t)|2dt
(1.11)
The signal x(t) is then said to be ﬁnite energy, or square integrable, whenever
Ex < ∞
(1.12)
The signal is said to have ﬁnite power if
Px < ∞
(1.13)
Remarks
I
The above deﬁnitions of energy and power are valid for any signal of ﬁnite or inﬁnite support, since a
ﬁnite-support signal is zero outside its support.
I
In the formulas for energy and power we are considering the possibility that the signals might be complex
and so we are squaring its magnitude: If the signal being considered is real, this simply is equivalent to
squaring the signal.
I
According to the above deﬁnitions, a ﬁnite-energy signal has zero power. Indeed, if the energy of the signal
is some constant Ex < ∞, then
Px = lim
T→∞
Ex
2T = 0

1.3 Continuous-Time Signals
81
I
An analog signal x(t) is said to be absolutely integrable if x(t) satisﬁes the condition
∞
Z
−∞
|x(t)|dt < ∞
(1.14)
I Example 1.11
Find the energy and the power of the following:
(a)
The periodic signal x(t) = cos(πt/2 + π/4).
(b)
The complex signal y(t) = (1 + j)ejπt/2, for 0 ≤t ≤10 and zero otherwise.
(c)
The pulse z(t) = 1, for 0 ≤t ≤10 and zero otherwise.
Determine whether these signals are ﬁnite energy, ﬁnite power, or both.
Solution
The energy in these signals is computed as follows:
Ex =
∞
Z
−∞
cos2(πt/2 + π/4)dt →∞
Ey =
10
Z
0
|(1 + j)ejπt/2|2dt = 2
10
Z
0
dt = 20
Ez =
10
Z
0
dt = 10
where we used |(1 + j)ejπt/2|2 = |1 + j|2|ejπt/2|2 = |1 + j|2 = 2. Thus, x(t) is an inﬁnite-energy sig-
nal while y(t) and z(t) are ﬁnite-energy signals. The power of y(t) and z(t) are zero because they
have ﬁnite energy. The power of x(t) can be calculated by using the symmetry of the signal squared
and letting T = NT0:
Px = lim
T→∞
2
2T
T
Z
0
cos2(πt/2 + π/4)dt = lim
N→∞
1
NT0
NT0
Z
0
cos2(πt/2 + π/4)dt
= lim
N→∞
1
NT0

N
T0
Z
0
cos2(πt/2 + π/4)dt

= 1
T0
T0
Z
0
cos2(πt/2 + π/4)dt
Using the trigonometric identity
cos2(πt/2 + π/4) = 1
2 [cos(πt + π/2) + 1]

82
CHAPTER 1:
Continuous-Time Signals
we have that
Px = 1
8
4
Z
0
cos(πt + π/2)dt + 1
8
4
Z
0
dt = 0 + 0.5 = 0.5
The ﬁrst integral is the area of the sinusoid over two of its periods, thus zero. So we have that x(t)
is a ﬁnite-power but inﬁnite-energy signal, while y(t) and z(t) are ﬁnite-power and ﬁnite-energy
signals.
I
I Example 1.12
Consider an aperiodic signal x(t) = e−at, a > 0, for t ≥0 and zero otherwise. Find the energy and
the power of this signal and determine whether the signal is ﬁnite energy, ﬁnite power, or both.
Solution
The energy of x(t) is given by
Ex =
∞
Z
0
e−2atdt = 1
2a < ∞
for any value of a > 0. The power of x(t) is then zero. Thus, x(t) is a ﬁnite-energy and ﬁnite-power
signal.
I
I Example 1.13
Consider the following analog signal, which we call a causal sinusoid because it is zero for t < 0:
x(t) =
2 cos(4t −π/4)
t ≥0
0
otherwise
This is the kind of signal that you would get from a signal generator that is started at a certain
initial time (in this case 0) and that continues until the signal generator is switched off (in this
case possibly inﬁnity). Determine if this signal is ﬁnite energy, ﬁnite power or both.
Solution
Clearly, the analog signal x(t) has inﬁnite energy:
Ex =
∞
Z
−∞
x2(t)dt
=
∞
Z
0
4 cos2(4t −π/4)dt →∞

1.3 Continuous-Time Signals
83
Although this signal has inﬁnite energy, it has ﬁnite power. Letting T = NT0 where T0 is the period
of 2 cos(4t −π/4) (or T0 = 2π/4), then its power is
Px = lim
T→∞
1
2T
T
Z
−T
x2(t)dt = lim
T→∞
1
2T
T
Z
0
x2(t)dt
= lim
N→∞
N
2NT0
T0
Z
0
x2(t)dt =
1
2T0
T0
Z
0
4 cos2(4t −π/4)dt
which is a ﬁnite value and therefore the signal has ﬁnite power but inﬁnite energy.
I
As we will see later in the Fourier series representation, any periodic signal is representable as a pos-
sibly inﬁnite sum of sinusoids of frequencies multiples of the fundamental frequency of the periodic
signal being represented. These frequencies are said to be harmonically related, and for this case the
power of the signal is shown to be the sum of the power of each of the sinusoidal components—that
is, there is superposition of the power. This superposition is still possible when a sum of sinusoids
creates a nonperiodic signal. This is illustrated in Example 1.14.
I Example 1.14
Consider the signals x(t) = cos(2πt) + cos(4πt) and y(t) = cos(2πt) + cos(2t), −∞< t < ∞.
Determine if these signals are periodic, and if so, ﬁnd their periods. Compute the power of these
signals.
Solution
The sinusoids cos(2πt) and cos(4πt) periods T1 = 1 and T2 = 1/2, so x(t) is periodic since T1/T2 =
2 with period T1 = 2T2 = 1. The two frequencies are harmonically related. The sinusoid cos(2t)
has as period T3 = π. Therefore, the ratio of the periods of the sinusoidal components of y(t) is
T1/T3 = 1/π, which is not rational, and so y(t) is not periodic and the frequencies 2π and 2 are
not harmonically related.
Using the trigonometric identities
cos2(θ) = 1
2(1 + cos(2θ))
cos(α) cos(β) = 1
2 (cos(α + β) + cos(α −β))
we have that
x2(t) = cos2(2πt) + cos2(4πt) + 2 cos(2πt) cos(4πt)
= 1 + 1
2 cos(4πt) + 1
2 cos(8πt) + cos(6πt) + cos(2πt)

84
CHAPTER 1:
Continuous-Time Signals
which is again a sum of harmonically related frequency sinusoids, so that x2(t) is periodic of period
T0 = 1. As in the previous examples, we have
Px = 1
T0
T0
Z
0
x2(t)dt = 1
which is the integral of the constant since the other integrals are zero. In this case, we used the
periodicity of x(t) and x2(t) to calculate the power directly. That is not possible when computing
the power of y(t) because it is not periodic, so we have to consider each of its components. We
have that
y2(t) = cos2(2πt) + cos2(2t) + 2 cos(2πt) cos(2t)
= 1 + 1
2 cos(4πt) + 1
2 cos(4t) + cos(2(π + 1)t) + cos(2(π −1)t)
and the power of y(t) is
Py = lim
T→∞
1
2T
T
Z
−T
y2(t)dt
= 1 +
1
2T4
T4
Z
0
cos(4πt)dt +
1
2T5
T5
Z
0
cos(4t)dt
+ 1
T6
T6
Z
0
cos(2(π + 1)t)dt + 1
T7
T7
Z
0
cos(2(π −1)t)dt = 1
where T4, T5, T6, and T7 are the periods of the sinusoidal components of y2(t). Fortunately, only
the ﬁrst integral is not zero and the others are zero (the average over a period of the sinusoidal
components of y2(t)). Fortunately, too, we have that the power of x(t) and the power of y(t) are the
sum of the powers of its components. That is if
x(t) = cos(2πt) + cos(4πt) = x1(t) + x2(t)
y(t) = cos(2πt) + cos(2t) = y1(t) + y2(t)
then as in previous examples Px1 = Px2 = Py1 = Py2 = 0.5, so that
Px = Px1 + Px2 = 1
Py = Py1 + Py2 = 1
I

1.4 Representation Using Basic Signals
85
The power of a sum of sinusoids,
x(t) =
X
k
Ak cos(kt) =
X
k
xk(t)
(1.15)
with harmonically or nonharmonically related frequencies {k}, is the sum of the power of each of the
sinusoidal components,
Px =
X
k
Pxk
(1.16)
1.4 REPRESENTATION USING BASIC SIGNALS
A fundamental idea in signal processing is to attempt to represent signals in terms of basic signals,
which we know how to process. In this section we consider some of these basic signals (complex
exponentials, sinusoids, impulse, unit-step, and ramp) that will be used to represent signals and for
which we will obtain their responses in a simple way in the next chapter.
1.4.1 Complex Exponentials
A complex exponential is a signal of the form
x(t) = Aeat
= |A|ert [cos(0t + θ) + j sin(0t + θ)] −∞< t < ∞
(1.17)
where A = |A|ejθ, and a = r + j0 are complex numbers.
Using Euler’s identity, ejφ = cos(φ) + j sin(φ), and from the deﬁnitions of A and a as complex
numbers, we have that
x(t) = |A|ejθe(r+j0)t = |A|erte(j0t+θ)
= |A|ert [cos(0t + θ) + j sin(0t + θ)]
We will see later that complex exponentials are fundamental in the Fourier representation of signals.
Remarks
I
Suppose that A and a are real, then
x(t) = Aeat
−∞< t < ∞
is a decaying exponential if a < 0, and a growing exponential if a > 0. See Figure 1.5.

86
CHAPTER 1:
Continuous-Time Signals
I
If A is real, but a = j0, then we have
x(t) = Aej0t
= A cos(0t) + jA sin(0t)
where the real part of x(t) is Re[x(t)] = A cos(0t) and the imaginary part of x(t) is Im[x(t)] =
A sin(0t), and j = √−1.
I
If both A and a are complex, x(t) is a complex signal and we need to consider separately its real and
imaginary parts. For instance, the real part function is
g(t) = Re[x(t)]
= |A|ert cos(0t + θ)
The envelope of g(t) can be found by considering that
−1 ≤cos(0t + θ) ≤1
and that when multiplied by |A|ert > 0, we have
−|A|ert ≤|A|ert cos(0t + θ) ≤|A|ert
so that
−|A|ert ≤g(t) ≤|A|ert
Whenever r < 0 the g(t) signal is a damped sinusoid, and when r > 0 then g(t) grows, as illustrated in
Figure 1.5.
I
According to the above, several signals can be obtained from the complex exponential.
FIGURE 1.5
Analog exponentials:
(a) decaying exponential,
(b) growing exponential, and
(c–d) modulated exponential
(c) decaying and (d) growing.
−2
0
2
1
2
3
4
t
−2
0
2
1
2
3
4
t
e−0.5t
e0.5 t
(a)
(b)
−2
0
2
−4
−2
0
2
4
t
e−0.5t cos(2πt)
−2
0
2
−4
−2
0
2
4
t
e0.5 t cos(2πt )
(c)
(d)

1.4 Representation Using Basic Signals
87
Sinusoids
Sinusoids are of the general form
A cos(0t + θ) = A sin(0t + θ + π/2)
−∞< t < ∞
(1.18)
where A is the amplitude of the sinusoid, 0 = 2πf0(rad/sec) is the frequency, and θ is a phase shift. The
frequency and time variables are inversely related, as follows:
0 = 2πf0 = 2π
T0
The cosine and the sine signals, as indicated above, are out of phase by π/2 radians. The frequency
can also be expressed in hertz or 1/sec units, and in that case 0 = 2πf0, and the period is found by
the relation f0 = 1/T0 (it is important to point out the inverse relation between time and frequency
shown here, which will be important in the representation of signals later on).
Recall from Chapter 0, that the Euler’s identity provides the relation of the sinusoids with the complex
exponential
ej0t = cos(0t) + j sin(0t)
(1.19)
that will allow us to represent in terms of sines and cosines any signal that is represented in terms of
complex exponentials. Likewise, the Euler’s identity also permits us to represent sines and cosines in
terms of complex exponentials, since
cos(0t) = 1
2

ej0t + e−j0t
(1.20)
sin(0t) = 1
2j

ej0t −e−j0t
(1.21)
Remarks A sinusoid is characterized by its amplitude, frequency, and phase. When we allow these three
parameters to be functions of time, or
A(t) cos((t)t + θ(t))
the following different types of modulation systems in communications are obtained:
I
Amplitude modulation (AM)—The amplitude A(t) changes according to the message, while the
frequency and the phase are constant,
I
Frequency modulation (FM)—The frequency (t) changes according to the message, while the
amplitude and phase are constant,
I
Phase modulation (PM)—The phase θ(t) varies according to the message and the other parameters are
kept constant.

88
CHAPTER 1:
Continuous-Time Signals
1.4.2 Unit-Step, Unit-Impulse, and Ram Signals
Unit-Step and Unit-Impulse Signals
Consider a rectangular pulse of duration 1 and unit area
p1(t) =



1
1
−1/2 ≤t ≤1/2
0
t < −1/2 and t > 1/2
(1.22)
Its integral is
u1(t) =
t
Z
−∞
p1(τ)dτ =



1
t > 1/2
1
1

t + 1
2

−1/2 ≤t ≤1/2
0
t < −1/2
(1.23)
The pulse p1(t) and its integral u1(t) are shown in Figure 1.6.
Suppose that 1 →0, then
I
The pulse p1(t) still has a unit area but is an extremely narrow pulse. We will call the limit the
unit-impulse signal,
δ(t) = lim
1→0 p1(t)
(1.24)
which is zero for all values of t except at t = 0 when its value is not deﬁned.
I
The integral u1(t), as 1 →0 has a left-side limit of u1(−ϵ) →0 and a right-side limit of u1(ϵ) →
1, for some inﬁnitesimal ϵ > 0, and at t = 0 it is 1/2. Thus, the limit is
lim
1→0 u1(t) =



1
t > 0
1/2
t = 0
0
t < 0
(1.25)
FIGURE 1.6
Generation of δ(t) and u(t) from limit as 1 →0 of a
pulse p1(t) and its integral u1(t).
pΔ(t )
Δ/2
−Δ/2
1/Δ
t
t
t
t
1
0.5
δ (t )
u(t )
1
(1)
uΔ(t)
−Δ/2
Δ/2

1.4 Representation Using Basic Signals
89
Ignoring the value at t = 0 we deﬁne the unit-step signal as
u(t) =
1
t > 0
0
t < 0
You can think of the u(t) as the switching of a dc signal generator from off to on, while δ(t) is a very
strong pulse of very short duration.
The impulse signal δ(t) is:
I
Zero everywhere except at the origin where its value is not well deﬁned (i.e., δ(t) = 0, t ̸= 0, and undeﬁned
at t = 0).
I
its area is unity, i.e.,
t
Z
−∞
δ(τ)dτ =
(
1
t > 0
0
t < 0.
(1.26)
The unit-step signal is
u(t) =
(
1
t > 0
0
t < 0
The δ(t) and u(t) are related as follows:
u(t) =
t
Z
−∞
δ(τ)dτ
(1.27)
δ(t) = du(t)
dt
(1.28)
According to calculus we have
u1(t) =
t
Z
−∞
p1(τ)dτ
p1(t) = du1(t)
dt
and so letting 1 →0, we obtain the relation between u(t) and δ(t).
Remarks
I
Since u(t) is not a continuous function, it jumps from 0 to 1 instantaneously around t = 0, from the
calculus point of view it should not have a derivative. That δ(t) is its derivative must be taken with
suspicion, which makes the δ(t) signal also suspicious. Such signals can, however, be formally deﬁned
using the theory of distributions.

90
CHAPTER 1:
Continuous-Time Signals
I
The impulse δ(t) is impossible to generate physically, but characterizes very brief pulses of any shape. It
can be derived using pulses or functions different from the rectangular pulse (see Eq. 1.22). In Problem
1.7 at the end of the chapter it is indicated how it can be derived from either a triangular pulse or a sinc
function of unit area.
I
Signals with jump discontinuities can be represented as the sum of a continuous signal and unit-step
signals at the discontinuities. This is useful in computing the derivative of these signals.
Ramp Signal
The ramp signal is deﬁned as
r(t) = t u(t)
(1.29)
Its relation to the unit-step and the unit-impulse signals is
dr(t)
dt
= u(t)
(1.30)
d2r(t)
dt2
= δ(t)
(1.31)
The ramp is a continuous function and its derivative is given by
dr(t)
dt
= dtu(t)
dt
= u(t) + tdu(t)
dt
= u(t) + t δ(t)
= u(t) + 0 δ(t) = u(t)
I Example 1.15
Consider the discontinuous signals
x1(t) = cos(2πt)[u(t) −u(t −1)]
x2(t) = u(t) −2u(t −1) + u(t −2)
Represent each of these signals as the sum of a continuous signal and unit-step signals, and ﬁnd
their derivatives.
Solution
The signal x1(t) is a period of a cosine of period T0 = 1, 0 ≤t ≤1, with a discontinuity of 1 at t = 0
and t = 1. Subtracting u(t) −u(t −1) from x1(t) we obtain a continuous signal, but to compensate
we must add a unit pulse between t = 0 and t = 1, giving
x1(t) = (cos(2πt) −1)[u(t) −u(t −1)] + [u(t) −u(t −1)] = x1a(t) + x1b(t)

1.4 Representation Using Basic Signals
91
where the ﬁrst term x1a(t) is continuous and the second x1b(t) is discontinuous. The derivative is
dx1(t)
dt
= −2π sin(2πt)[u(t) −u(t −1)] + (cos(2πt) −1)[δ(t) −δ(t −1)] + δ(t) −δ(t −1)
= −2π sin(2πt)[u(t) −u(t −1)] + δ(t) −δ(t −1)
since
(cos(2πt) −1)[δ(t) −δ(t −1)] = (cos(2πt) −1)δ(t) −(cos(2πt) −1)δ(t −1)
= (cos(0) −1)δ(t) −(cos(2π) −1)δ(t −1)
= 0δ(t) + 0δ(t −1) = 0
The term δ(t) in the derivative indicates that there is a jump from 0 to 1 in x1(t) at t = 0 and that
in −δ(t −1) there is a jump of −1 (from 1 to 0) at t = 1. See Figure 1.7.
0
0.2
0.4
0.6
0.8
1
−2
0
2
x1(t)
0
0.2
0.4
0.6
0.8
1
−2
0
2
x1a(t)
0
0.2
0.4
0.6
0.8
1
−2
0
2
x1b(t )
t
(c)
t
(b)
t
(a)
FIGURE 1.7
(a) Decomposition of x1(t) = cos(2πt)[u(t) −u(t −1)] into (b) a continuous and (c) a discontinuous signal
(a pulse).

92
CHAPTER 1:
Continuous-Time Signals
The signal x2(t) has jump discontinuities at t = 0, t = 1, and t = 2, and we can think of it as
completely discontinuous so that its continuous component is 0. The derivative is
dx2(t)
dt
= δ(t) −2δ(t −1) + δ(t −2)
The area of each of the deltas coincides with the jump in the discontinuities.
I
Signal Generation with MATLAB
In the following examples we illustrate how to generate analog signals using MATLAB. This is done by
either approximating continuous-time signals by discrete-time signals or by using the symbolic tool-
box. The function plot uses an interpolation algorithm that makes the plots of discrete-time signals
look like analog signals.
I Example 1.16
Write a script and the necessary functions to generate a signal,
y(t) = 3r(t + 3) −6r(t + 1) + 3r(t) −3u(t −3)
Then plot it and verify analytically that the obtained ﬁgure is correct.
Solution
We wrote functions ramp and ustep to generate ramp and unit-step signals for obtaining a numeric
approximation of the signal y(t). The following script shows how these functions are used to gen-
erate y(t). The arguments of ramp determine the support of the signal, the slope, and the shift (for
advance, a positive number, and for delay, a negative number). For ustep we need to provide the
support and the shift.
%%%%%%%%%%%%%%%%%%%
% Example 1.16
%%%%%%%%%%%%%%%%%%%
clear all; clf
Ts = 0.01; t = -5:Ts:5; % support of signal
% ramp with support [-5, 5], slope of 3 and advanced
% (shifted left) with respect to the origin by 3
y1 = ramp(t,3,3);
y2 = ramp(t,-6,1);
y3 = ramp(t,3,0);
% unit-step function with support [-5,5], delayed by 3
y4 = -3 * ustep(t,-3);
y = y1 + y2 + y3 + y4;
plot(t,y,’k’); axis([-5 5 -1 7]); grid
Our functions ramp and ustep are as follows.
function y = ramp(t,m,ad)
% ramp generation

1.4 Representation Using Basic Signals
93
% t: time support
% m: slope of ramp
% ad : advance (positive), delay (negative) factor
% USE: y = ramp(t,m,ad)
N = length(t);
y = zeros(1,N);
for i = 1:N,
if t(i) >= -ad,
y(i) = m * (t(i)+ad);
end
end
function y = ustep(t,ad)
% generation of unit step
% t: time
% ad : advance (positive), delay (negative)
% USE y = ustep(t,ad)
N = length(t);
y = zeros(1,N);
for i = 1:N,
if t(i) >= -ad,
y(i) = 1;
end
end
Analytically,
I
y(t) = 0 for t < −3 and for t > 3, so the chosen support −5 ≤t ≤5 displays the signal in a
region where the signal occurs.
I
For −3 ≤t ≤−1, y(t) is 3r(t + 3) = 3(t + 3), which is 0 at t = −3 and 6 at t = −1.
I
For −1 ≤t ≤0, y(t) is 3r(t + 3) −6r(t + 1) = 3(t + 3) −6(t + 1) = −3t + 3, which is 6 at t =
−1 and 3 at t = 0.
I
For 0 ≤t ≤3, y(t) is 3r(t + 3) −6r(t + 1) + 3r(t) = −3t + 3 + 3t = 3.
I
For t ≥3 the signal is 3r(t + 3) −6r(t + 1) + 3r(t) −3u(t −3) = 3 −3 = 0.
These coincide with the signal shown in Figure 1.8.
I
I Example 1.17
Consider the following script that uses the functions ramp and ustep to generate a signal y(t). Obtain
analytically the formula for the signal y(t). Write a function to compute and plot the even and odd
components of y(t).
clear all; clf
t = -5:0.01:5;
y1 = ramp(t,2,2.5);
y2 = ramp(t,-5,0);

94
CHAPTER 1:
Continuous-Time Signals
FIGURE 1.8
Generation of
y(t) = 3r(t + 3) −6r(t + 1) + 3r(t) −
3u(t −3), −5 ≤t ≤5, and zero otherwise.
−5
0
5
−1
0
1
2
3
4
5
6
7
t(sec)
y(t)
y3 = ramp(t,3,-2);
y4 = ustep(t,-4);
y = y1 + y2 + y3 + y4;
plot(t,y,’k’); axis([-5 5 -3 5]); grid
The signal y(t) = 0 for t < −5 and t > 5.
Solution
The signal y(t) displayed on Figure 1.9(a) is given analytically by
y(t) = 2r(t + 2.5) −5r(t) + 3r(t −2) + u(t −4)
Clearly, y(t) is neither even nor odd. To ﬁnd its even and odd components we use the function
evenodd, shown in the following code with inputs as the signal and its support and outputs as the
even and odd components. The results are shown on the bottom plots of Figure 1.9. Adding these
two signals gives back the original signal y(t). The script used is as follows.
%%%%%%%%%%%%%%%%%%%
% Example 1.17
%%%%%%%%%%%%%%%%%%%
[ye, yo] = evenodd(t,y);
subplot(211)
plot(t,ye,’r’)
grid
axis([min(t) max(t) -2 5])
subplot(212)
plot(t,yo,’r’)

1.4 Representation Using Basic Signals
95
−5
0
5
−3
−2
−1
0
1
2
3
4
5
(a)
(c)
t (sec)
y (t)
−5
0
5
−2
0
2
4
ye(t )
−5
0
5
0
2
4
t
(b)
t
yo(t )
FIGURE 1.9
(a) Signal y(t) = 2r(t + 2.5) −5r(t) + 3r(t −2) + u(t −4), (b) even component ye(t), and (c) odd component
yo(t).

96
CHAPTER 1:
Continuous-Time Signals
grid
axis([min(t) max(t) -1 5])
function [ye,yo] = evenodd(t,y)
% even/odd decomposition
% t: time
% y: analog signal
% ye, yo: even and odd components
% USE [ye,yo] = evenodd(t,y)
%
yr = ﬂiplr(t,y);
ye = 0.5 * (y + yr);
yo = 0.5 * (y - yr);
The MATLAB function ﬂiplr reverses the values of the vector y giving the reﬂected signal.
I
I Example 1.18
Use symbolic MATLAB to generate the following analog signals.
(a)
For the damped sinusoid signal
x(t) = e−t cos(2πt)
obtain a script to generate x(t) and its envelope.
(b)
For a rough approximation of a periodic pulse generated by adding three cosines of
frequencies multiples of 0 = π/10—that is
x1(t) = 1 + 1.5 cos(20t) −0.6 cos(40t)
write a script to generate x1(t).
Solution
The following script generates the damped sinusoid signal, and its envelope y(t) = ±e−t.
%%%%%%%%%%%%%%%%%%%
% Example 1.18 --- damped sinusoid
%%%%%%%%%%%%%%%%%%%
t = sym(’t’);
x = exp(-t) * cos(2 * pi * t);
y = exp(-t);
ezplot(x,[-2,4])
grid
hold on
ezplot(y,[-2,4])

1.4 Representation Using Basic Signals
97
−10
−5
0
5
10
−1
−0.5
0
0.5
1
1.5
2
t
x (t )
−2
−1
0
1
2
3
4
−8
−6
−4
−2
0
2
4
6
8
t
(a)
(b)
y (t )
1 +3/2 cos(1/5πt)−3/5 cos(2/5πt )
FIGURE 1.10
(a) Damped sinusoid, and (b) sum of weigthed cosines approximating a pulse.
hold on
ezplot(-y,[-2,4])
axis([-2 4 -8 8])
hold off
The approximate pulse signal is generated by the following script.
clear; clf
t = sym(’t’);
% sum of constant and cosines
x = 1 + 1.5 * cos(2 * pi * t/10)-.6 * cos(4 * pi * t/10);
ezplot(x,[-10,10]); grid
The plots of the damped sinusoid and the approximate pulse are given in Figure 1.10.
I
I Example 1.19
Consider the generation of a triangular signal,
3(t) =



t
0 ≤t ≤1
−t + 2
1 < t ≤2
0
otherwise
using ramp signals r(t). Use the unit-step signal to represent the derivative of d3(t)/dt.
Solution
The triangular pulse can be represented as
3(t) = r(t) −2r(t −1) + r(t −2)
(1.32)

98
CHAPTER 1:
Continuous-Time Signals
FIGURE 1.11
(a) The triangular signal 3(t) and (b) its derivative.
dt
0
0
1
1
−1
1
1
2
2
t
t
Λ(t )
dΛ(t )
(a)
(b)
In fact, since r(t −1) and r(t −2) have values different from 0 for t ≥1 and t ≥2, respectively,
then
3(t) = r(t) = t
for 0 ≤t ≤1
and that for 1 ≤t ≤2,
3(t) = r(t) −2r(t −1) = t −2(t −1) = −t + 2
Finally, for t > 2 the three ramp signals are different from zero, so
3(t) = r(t) −2r(t −1) + r(t −2)
= t −2(t −1) + (t −2)
= 0
t > 2
and by deﬁnition 3(t) is zero for t < 0. So the given expression for 3(t) in terms of the ramp
functions is identical to its given mathematical deﬁnition.
Using the mathematical deﬁnition of the triangular function, its derivative is given by
d3(t)
dt
=



1
0 ≤t ≤1
−1
1 < t ≤2
0
otherwise
Using the representation in Equation (1.32) this derivative is also given by
d3(t)
dt
= u(t) −2u(t −1) + u(t −2)
which are two unit pulses, as shown in Figure 1.11.
I
I Example 1.20
Consider a full-wave rectiﬁed signal,
x(t) = |cos(2πt)|
−∞< t < ∞

1.4 Representation Using Basic Signals
99
FIGURE 1.12
Eight periods of full-wave rectiﬁed signal
x(t) = | cos(2πt)|, −∞< t < ∞.
−2
−1
0
1
2
0
0.2
0.4
0.6
0.8
1
t(sec)
x(t )
of period T0 = 0.5. Obtain a representation for a period between 0 and 0.5, and represent x(t) in
terms of shifted versions of it. A full-wave rectiﬁed signal is used in designing dc sources. It is a ﬁrst
step in converting an alternating voltage into a dc voltage. See Figure 1.12.
Solution
The period between 0 and 0.5 can be expressed as
p(t) = x(t)[u(t) −u(t −0.5)] = |cos(2πt)|[u(t) −u(t −0.5)]
Since x(t) is a periodic signal of period T0 = 0.5, we have then that
x(t) =
∞
X
k=−∞
p(t −kT0)
I
I Example 1.21
Generate a causal train of pulses that repeats every two units of time using as ﬁrst period
s(t) = u(t) −2u(t −1) + u(t −2)
Find the derivative of the train of pulses.
Solution
Considering that s(t) is the ﬁrst period of the train of pulses of period two, then
ρ(t) =
∞
X
k=0
s(t −2k)

100
CHAPTER 1:
Continuous-Time Signals
ρ(t)
dρ(t )
dt
1
1
1
−1
2
2
3
3
0
0
t
t
(1)
(2)
(−2)
(−2)
· · ·
· · ·
FIGURE 1.13
Causal train of pulses ρ(t) and its derivative. The number enclosed in () is the area of the corresponding delta
function and it indicates the jump at the particular discontinuity—positive when increasing and negative when
decreasing.
is the desired signal. Notice that ρ(t) equals zero for t < 0, thus it is causal. Given that the derivative
of a sum of signals is the sum of the derivative of each of the signals, the derivative of ρ(t) is
dρ(t)
dt
=
∞
X
k=0
ds(t −2k)
dt
=
∞
X
k=0
[δ(t −2k) −2δ(t −1 −2k) + δ(t −2 −2k)]
which can be simpliﬁed to
dρ(t)
dt
= [δ(t) −2δ(t −1) + δ(t −2)] + [δ(t −2) −2δ(t −3) + δ(t −4)] + [δ(t −4) · · ·
= δ(t) + 2
∞
X
k=1
δ(t −2k) −2
∞
X
k=1
δ(t −2k + 1)
where δ(t), 2δ(t −2k), and −2δ(t −2k + 1) for k ≥1 occur at t = 0, t = 2k, and t = 2k −1 for
k ≥1, or the times at which the discontinuities of ρ(t) occur. The value associated with the δ(t)
corresponds to the jump of the signal from the left to the right. Thus, δ(t) indicates there is a
discontinuity in ρ(t) at zero as it jumps from 0 to 1, while the discontinuities at 2, 4, . . . have a
jump of 2 from −1 to 1, increasing. The discontinuities indicated by δ(t −2k −1) occurring at 1,
3, 5, . . . are from 1 to −1 (i.e., decreasing, so the value of −2). See Figure 1.13.
I
1.4.3 Special Signals—the Sampling Signal and the Sinc
Two signals of great signiﬁcance in the sampling of continuous-time signals and their reconstruction
are the sampling signal and the sinc. Sampling a continuous-time signal consists in taking samples of
the signal at uniform times. One can think of this process as the multiplication of a continuous-time

1.4 Representation Using Basic Signals
101
signal x(t) by a train of very narrow pulses of the sampling period Ts. For simplicity, considering that
the width of the pulses is much smaller than Ts, the train of pulses can be approximated by a train of
impulses that is periodic of period Ts—that is, the sampling signal δTs(t) is
δTs(t) =
∞
X
n=−∞
δ(t −nTs)
(1.33)
The sampled signal xs(t) is then
xs(t) = x(t)δTs(t)
=
∞
X
n=−∞
x(nTs)δ(t −nTs)
(1.34)
or a sequence of uniformly shifted impulses with amplitude the value of the signal x(t) at the time
when the impulse occurs.
A fundamental result in sampling theory is the recovery of the original signal, under certain con-
strains, by means of an interpolation using sinc signals. Moreover, we will see that the sinc is connected
with ideal low-pass ﬁlters. The sinc function is deﬁned as
S(t) = sin πt
πt
−∞< t < ∞
(1.35)
This signal has the following characteristics:
I
The time support of this signal is inﬁnite.
I
It is an even function of t, as
S(−t) = sin(−πt)
−πt
= −sin(πt)
−πt
= S(t)
(1.36)
I
At t = 0 the numerator and the denominator of the sinc are zero; thus the limit as t →0 is found
using L’Hˆopital’s rule—that is,
lim
t→0 S(t) = lim
t→0
d sin(πt)/dt
dπt/dt
= lim
t→0
π cos(πt)
π
= 1
(1.37)
I
S(t) is bounded—that is, since −1 ≤sin(πt) ≤1, then for t ≥0,
−1
πt ≤sin(πt)
πt
= S(t) ≤1
πt
(1.38)
and given that S(t) is even, it is equally bounded for t < 0. As t →±∞, S(t) →0.
I
The zero-crossing time of S(t) are found by letting the numerator equal zero—that is, when
sin(πt) = 0, the zero-crossing times are such that πt = kπ, or t = k for a nonzero integer k or
k = ±1, ±2, . . . .

102
CHAPTER 1:
Continuous-Time Signals
I
A property that is not obvious and that requires the frequency representation of S(t) is that the
integral
∞
Z
−∞
|S(t)|2dt = 1
(1.39)
Recall that we showed this in Chapter 0 using numeric and symbolic MATLAB.
The sinc signal will appear in several places in the rest of the book.
1.4.4 Basic Signal Operations—Time Scaling, Frequency Shifting, and
Windowing
Given a signal x(t), and real values α ̸= 0 or 1, and φ > 0:
I
x(αt) is said to be contracted if |α| > 1, and if α < 0 it is also reﬂected.
I
x(αt) is said to be expanded if |α| < 1, and if α < 0 it is also reﬂected.
I
x(t)ejφt is said to be shifted in frequency by φ radians.
I
For a window signal w(t), x(t)w(t) displays x(t) within the support of w(t).
To illustrate the time scaling, consider a signal x(t) with a ﬁnite support t0 ≤t ≤t1. Assume that
α > 1, then x(αt) is deﬁned in t0 ≤αt ≤t1 or t0/α ≤t ≤t1/α, a smaller support than the original
one. For instance, for α = 2, t0 = 2, and t1 = 4, then the support of x(2t) is 1 ≤t ≤2, while the
support of x(t) is 2 ≤t ≤4. If α = −2, then x(−2t) is not only contracted but also reﬂected. Similarly,
x(0.5t) would have a support of 2t0 ≤t ≤2t1, which is larger than the original support.
Multiplication by an exponential shifts the frequency of the original signal. To illustrate this consider
the case of an exponential x(t) = ej0t of frequency 0. If we multiply x(t) by an exponential ejφt, then
x(t)ejφt = ej(0+φ)t = cos((0 + φ)t) + j sin((0 + φ)t)
so that the frequency of the new exponential is greater than 0 if φ > 0 or smaller if φ < 0. So we have
shifted the frequency of x(t). If we have a sum of exponentials (they do not need to be harmonically
related as in the Fourier series we will consider later),
x(t) =
X
k
Akejkt
then
x(t)ejφt =
X
k
Akej(k+φ)t
so that each of the frequencies of the signal x(t) has been shifted. This shifting of the frequency is
signiﬁcant in the development of amplitude modulation, and as such this frequency shift process
is called modulation—that is, the signal x(t) modulates the exponential and x(t)ejφt is the modulated
signal.

1.4 Representation Using Basic Signals
103
Notice that time scaling also changes the frequency content of the signal. For instance, a signal x(t) =
ej0t is periodic of period T0 = 2π/0, while x(αt) = ejα0t has a period T0/α or a frequency α0,
which is larger than the original frequency of 0 when α > 1 and smaller than 0 when 0 < α < 1.
Remarks We can thus summarize the above as follows:
I
If x(t) is periodic of period T0 then the time-scaled signal x(αt), α ̸= 0, is also periodic of period T0/|α|.
I
The support in time of a periodic or nonperiodic signal is inversely proportional to the support in frequency
for that signal.
I
The frequencies present in a signal can be changed by modulation—that is, multiplying the signal by
a complex exponential or, equivalently, by sines and cosines. The frequency change is also possible by
expansion and compression of the signal.
I
Reﬂection is a special case of time scaling with α = −1.
I Example 1.22
Let x1(t), for 0 ≤t ≤T0, be one period of a periodic signal x(t) of period T0. Represent x(t) in terms
of advanced and delayed versions of x1(t). What would be x(2t)?
Solution
The periodic signal x(t) can be written as
x(t) = · · · + x1(t + 2T0) + x1(t + T0) + x1(t) + x1(t −T0) + x1(t −2T0) + · · ·
=
∞
X
k=−∞
x1(t −kT0)
and the contracted signal x(2t) is then
x(2t) =
∞
X
k=−∞
x1(2t −kT0)
and periodic of period T0/2.
I
I Example 1.23
An acoustic signal x(t) has a duration of 3.3 minutes and a radio station would like to use the
signal for a three-minute segment. Indicate how to make it possible.
Solution
We need to contract the signal by a factor of α = 3.3/3 = 1.1, so that x(1.1t) can be used in the
three-minute piece. If the signal is recorded on tape, the tape player can be run 1.1 times faster
than the recording speed. This would change the voice or music on the tape, as the frequencies
x(1.1t) are increased with respect to the original frequencies in x(t).
I

104
CHAPTER 1:
Continuous-Time Signals
I Example 1.24
One way of transmitting a message over the airwaves is to multiply it by a sinusoid of frequency
higher than those in the message, thus changing the frequency content of the signal. The resulting
signal is called an amplitude-modulated (AM) signal: The message changes the amplitude of the
sinusoid. To recover the message from the transmitted signal, one can make the envelope of the
modulated signal be related to the message. Use again the ramp and ustep functions to generate a
signal y(t) = 2r(t + 2) −4r(t) + r(t −2) + r(t −3) + u(t −3) to modulate a so-called carrier signal
x(t) = sin(5πt) to give the AM signal z(t). Obtain a script to generate and plot the AM signal.
Indicate whether the envelope of the AM signal is connected with the message signal y(t).
Solution
The signal y(t) analytically equals
y(t) =



0
t < −2
2r(t + 2) = 2(t + 2)
−2 ≤t < 0
2r(t + 2) −4r(t) = −2t + 4
0 ≤t < 2
2r(t + 2) −4r(t) + r(t −2) = −t + 2
2 ≤t < 3
2r(t + 2) −4r(t) + r(t −2) + r(t −3) + u(t −3) = 0
t ≥3
The following script is used to generate the message signal y(t), the AM signal z(t), and the cor-
responding plots. The MATLAB function sound is used to produce the sound corresponding to
100z(t). In Figure 1.14 we show z(t) and emphasize the envelope (dashed line) that corresponds
to ±y(t).
%%%%%%%%%%%%%%%
% Example 1.24 --- AM signal
%%%%%%%%%%%%%%%
t = -5:0.01:5;
x = sin(5 * pi * t);
y1 = ramp(t,2,2);
y2 = ramp(t,-4,0);
y3 = ramp(t,1,-2);
y4 = ramp(t,1,-3);
y5 = ustep(t,-3);
y = y1 + y2 + y3 + y4 + y5;
z = y. * x;
sound(100 * z,1000)
plot(t,z,’k’); hold on
plot(t,y,’r’,t,-y,’r’); axis([-5 5 -5 5]); grid
hold off
xlabel(’t’); ylabel(’z(t)’)

1.4 Representation Using Basic Signals
105
FIGURE 1.14
AM signal.
−5
0
5
−5
0
5
t
z (t)
I
1.4.5 Generic Representation of Signals
Consider the following integral:
∞
Z
−∞
f(t)δ(t)dt
The product of f(t) and δ(t) gives zero everywhere except at the origin where we get an impulse of
area f(0)—that is, f(t)δ(t) = f(0)δ(t) (let t0 = 0 in Figure 1.15). Therefore,
∞
Z
−∞
f(t)δ(t)dt =
∞
Z
−∞
f(0)δ(t)dt = f(0)
∞
Z
−∞
δ(t)dt = f(0)
(1.40)
since the area under the curve of the impulse is unity. This property of the impulse function is appro-
priately called the sifting property. The result of this integration is to sift out f(t) for all t except for t = 0,
where δ(t) occurs. If we delay or advance the δ(t) function in the integrand, the result is that all values
of f(t) are sifted out except for the value corresponding to the location of the delta function—that is,
∞
Z
−∞
f(t)δ(t −τ)dt =
∞
Z
−∞
f(τ)δ(t −τ)dt = f(τ)
∞
Z
−∞
δ(t −τ)dt
= f(τ)
for any τ
since the last integral is still unity. Figure 1.15 illustrates the multiplication of a signal f(t) by an
impulse signal δ(t −t0), located at t = t0.

106
CHAPTER 1:
Continuous-Time Signals
FIGURE 1.15
Multiplication of a signal f(t) by an impulse signal
δ(t −t0).
×
=
t0
t0
t
t
t
δ (t −t0)
f(t )
f(t0)δ (t −t0)
(f (t0))
t
t
t
=
+
2Δ
x(0)
· · ·
+
xΔ(t )
x(t )
Δ
Δ
Δ
↔
x(Δ)
FIGURE 1.16
Generic representation of x(t) as an inﬁnite sum of pulses of height x(k1) and width 1 when 1 →0, so that the
sum becomes an integral of weighted impulse signals.
By the sifting property of the impulse function δ(t), any signal x(t) can be represented by the following generic
representation:
x(t) =
∞
Z
−∞
x(τ)δ(t −τ)dτ
(1.41)
Figure 1.16 shows a generic representation. Equation (1.41) basically indicates that any signal can
be viewed as a stacking of pulses x(k1)p1(t −k1), which in the limit as 1 →0 become impulses
x(τ)δ(t −τ).
Equation (1.41) provides a generic representation of a signal in terms of basic signals, in this case
impulse signals. As we will see in the next chapter, once we determine the response of a system to an
impulse we will use the generic representation to ﬁnd the response of the system to any signal.
1.5 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO FROM
HERE?
We have taken another step in our long journey. In this chapter we discussed the main classiﬁcation of
signals and have started the study of deterministic, continuous-time signals. We have also discussed
important characteristics of signals such as periodicity, energy, power, evenness, and oddness, and
learned basic signal operations that will be useful as we will see in the next chapters. Interestingly,

1.5 What Have We Accomplished? Where Do We Go from Here?
107
Table 1.1 Basic Signals
Signal
Deﬁnition
Complex exponential
|A|ert [cos(0t + θ) + j sin(0t + θ)]
−∞< t < ∞
Sinusoid
A cos(0t + θ) = A sin(0t + θ + π/2)
−∞< t < ∞
Unit impulse
δ(t) = 0 t ̸= 0, undeﬁned at t = 0
tR
−∞
δ(τ)dτ = 1,
t > 0
∞
R
−∞
f(τ)δ(t −τ)dτ = f(t)
Unit step
u(t) =
(
1
t > 0
0
t < 0
Ramp
r(t) = tu(t) =
(
t
t > 0
0
t < 0
δ(t) = du(t)/dt
u(t) =
tR
−∞
δ(τ)dτ
r(t) =
tR
−∞
u(τ)dτ
Rectangular pulse
p(t) = A [u(t) −u(t −1)] =
(
A
0 ≤t ≤1
0
otherwise
Triangular pulse
3(t) = A[r(t) −2r(t −1) + r(t −2)] =



At
0 ≤t ≤1
A(2 −t)
1 < t ≤2
0
otherwise
Sampling
δTs(t) = P
k δ(t −kTs)
Sinc
S(t) = sin(πt)/(πt)
S(0) = 1
S(k) = 0 k ̸= 0 integer
∞
R
−∞
S2(t)dt = 1
we began to see how some of these operations lead to practical applications, such as amplitude, fre-
quency, and phase modulations, which are basic in the theory of communications. Very importantly,
we have also begun to represent signals in terms of basic signals, which in later chapters will allow us
to simplify the analysis and will give us ﬂexibility in the synthesis of systems. These basic signals are
used as test signals in control systems. Table 1.1 displays basic signals.
Our next step is to connect signals with systems. We are particularly interested in developing a
theory that can be used to approximate, to some degree, the behavior of most systems of inter-
est in engineering. After that we consider the analysis of signals and systems time and frequency
domains.

108
CHAPTER 1:
Continuous-Time Signals
PROBLEMS
1.1. Signal energy and RC circuit—MATLAB
The signal x(t) = e−|t| is deﬁned for all values of t.
(a) Plot the signal x(t) and determine if this signal is ﬁnite energy. That is, compute the integral
∞
Z
−∞
|x(t)|2dt
and determine if it is ﬁnite.
(b) If you determine that x(t) is absolutely integrable, or that the integral
∞
Z
−∞
|x(t)|dt
is ﬁnite, could you say that x(t) has ﬁnite energy? Explain why or why not. Hint: Plot |x(t)| and |x(t)|2
as functions of time.
(c) From your results above, is it true the energy of the signal
y(t) = e−t cos(2πt)u(t)
is less than half the energy of x(t)? Explain. To verify your result, use symbolic MATLAB to plot y(t)
and to compute its energy.
(d) To discharge a capacitor of 1 mF charged with a voltage of 1 volt we connect it, at time t = 0, with a
resistor of R . When we measure the voltage in the resistor we ﬁnd it to be vR(t) = e−tu(t). Determine
the resistance R. If the capacitor has a capacitance of 1 µF, what would be R? In general, how are R
and C related?
1.2. Power in RL circuits
Consider a circuit consisting of a sinusoidal source vs(t) = cos(t)u(t) volts connected in series to a resistor
R and an inductor L and assume they have been connected for a very long time.
(a) Let R = 0 and L = 1 H. Compute the instantaneous and the average powers delivered to the inductor.
(b) Let R = 1  and L = 1 H. Compute the instantaneous and the average powers delivered to the resistor
and the inductor.
(c) Let R = 1  and L = 0 H. Compute the instantaneous and the average powers delivered to the resistor.
Hint: In the above parts of the problem use phasors or the trigonometric formula
cos(α) cos(β) = 0.5[cos(α −β) + cos(α + β)]
(d) The average power used by the resistor is what you pay to the electric company, but there is also a
reactive power for which you do not. The complex power supplied to the circuit is deﬁned as
P = 1
2VsI∗
where Vs and I are the phasors corresponding to the source and the current in the circuit, and I∗is the
complex conjugate of I. Consider the values of the resistor and the inductor given above, and compute
the complex power and relate it to the average power computed in each case.
1.3. Power in periodic and nonperiodic sum of sinusoids
Consider the periodic signal x(t) = cos(20t) + 2 cos(0t), −∞< t < ∞, and 0 = π. The frequencies of
the two sinusoids are said to be harmonically related (one is a multiple of the other).

Problems
109
(a) Determine the period T0 of x(t).
(b) Compute the power Px of x(t).
(c) Verify that the power Px is the sum of the power P1 of x1(t) = cos(2πt) and the power P2 of x2(t) =
2 cos(πt).
(d) In the above case you are able to show that there is superposition of the powers because the fre-
quencies are harmonically related. Suppose that y(t) = cos(t) + cos(πt) where the frequencies are not
harmonically related. Find out whether y(t) is periodic or not. Indicate how you would ﬁnd the power
Py of y(t). Would Py = P1 + P2 where P1 is the power of cos(t) and P2 is the power of cos(πt)? Explain
what is the difference with respect to the case of harmonic frequencies.
1.4. Periodicity of sum of sinusoids—MATLAB
Consider the periodic signals x1(t) = 4 cos(πt) and x2(t) = −sin(3πt + π/2).
(a) Find the periods of x1(t) and x2(t).
(b) Is the sum x(t) = x1(t) + x2(t) periodic? If so, what is its period?
(c) In general, two periodic signals x1(t) and x2(t) having periods T1 and T2 such that their ratio T1/T2 =
M/K is a rational number (i.e., M and K are positive integers), then the sum x(t) = x1(t) + x2(t) is
periodic. Suppose the rationality condition is satisﬁed and M = 3 and K = 12. Determine the period of
x(t).
(d) Determine whether x(t) = x1(t) + x2(t) is periodic when
I
x1(t) = 4 cos(2πt) and x2(t) = −sin(3πt + π/2)
I
x1(t) = 4 cos(2t) and x2(t) = −sin(3πt + π/2)
Use symbolic MATLAB to plot x(t) in the above two cases and conﬁrm your analytic results about the
periodicity or lack of periodicity of x(t).
1.5. Time shifting
Consider a ﬁnite-support signal
x(t) = t
0 ≤t ≤1
and zero elsewhere.
(a) Carefully plot x(t + 1).
(b) Carefully plot x(−t + 1).
(c) Add the above two signals to get a new signal y(t). To verify your results, represent each of the above
signals analytically and show that the resulting signal is correct.
(d) How does y(t) compare to the signal 3(t) = (1 −|t|)(u(t + 1) −u(t −1)? Plot them. Compute the
integrals of y(t) and 3(t) for all values of t and compare them. Explain.
1.6. Even and odd hyperbolic functions—MATLAB
According to Euler’s identity the sine and the cosine are deﬁned in terms of complex exponentials. You
would then ask what if instead of complex exponentials you were to use real exponentials. Well, using
Euler’s identity we obtain the hyperbolic functions deﬁned in −∞< t < ∞:
cosh(0t) = e0t + e−0t
2
sinh(0t) = e0t −e−0t
2
(a) Let 0 = 1 rad/sec. Use the deﬁnition of the real exponentials to plot cosh(t) and sinh(t).
(b) Is cosh(t) even or odd?

110
CHAPTER 1:
Continuous-Time Signals
(c) Is sinh(t) even or odd?
(d) Obtain an expression for x(t) = e−tu(t) in terms of the hyperbolic functions. Use symbolic MATLAB to
plot x(t) = e−tu(t) and to plot your expression in terms of the hyperbolic functions. Compare them.
1.7. Impulse signal generation—MATLAB
When deﬁning the impulse or δ(t) signal, the shape of the signal used to do so is not important. Whether
we use the rectangular pulse we considered in this chapter or another pulse, or even a signal that is not a
pulse, in the limit we obtain the same impulse signal. Consider the following cases:
(a) The triangular pulse,
31(t) = 1
1

1 −

t
1


(u(t + 1) −u(t −1))
Carefully plot it, compute its area, and ﬁnd its limit as 1 →0. What do you obtain in the limit? Explain.
(b) Consider the signal
S1(t) = sin(πt/1)
πt
Use the properties of the sinc signal S(t) = sin(πt)/(πt) to express S1(t) in terms of S(t). Then ﬁnd its
area, and the limit as 1 →0. Use symbolic MATLAB to show that for decreasing values of 1 the S1(t)
becomes like the impulse signal.
1.8. Impulse and unit-step signals
By introducing the impulse δ(t) and the unit-step u(t) signals, we expand the conventional calculus. One
of the advantages of having the δ(t) function is that we are now able to ﬁnd the derivative of discontinuous
signals. Let us illustrate this advantage. Consider a periodic sinusoid deﬁned for all times,
x(t) = cos(0t)
−∞< t < ∞
and a causal sinusoid deﬁned as
x1(t) = cos(0t)u(t)
where the unit-step function indicates that the function has a discontinuity at zero, since for t = 0+ the
function is close to 1, and for t = 0−the function is zero.
(a) Find the derivative y(t) = dx(t)/dt and plot it.
(b) Find the derivative z(t) = dx1(t)/dt (treat x1(t) as the product of two functions cos(0t) and u(t)) and
plot it. Express z(t) in terms of y(t).
(c) Verify that the integral of z(t) gives you back x1(t).
1.9. Series RC circuit response to a unit-step signal
A unit-step function u(t) can be considered a causal constant source (e.g., a battery in a circuit if the units
of u(t) is volts).
(a) From basic principles consider the response of an RC circuit to u(t)—that is, a battery connected in
series with the resistor and the capacitor. Remember that the voltage across the capacitor results
from an accumulation of charge, and that the presence of the resistor simply means that the charge is
slowly accumulated. Therefore, plot what would be the voltage across the capacitor for t > 0 (assume
the capacitor has no initial voltage at t = 0).
(b) What would be the voltage across the capacitor in the steady state? Explain.
(c) Finally, suppose that the capacitor is disconnected from the circuit at some time t0 >> 0. Ideally, what
would be the voltage across the capacitor from then on?
(d) If you disconnect the capacitor, again at t0 >> 0, but somehow it is left connected to the resistor, so
they are in parallel, what would happen to the voltage across the capacitor? Plot approximately the
voltage across the capacitor for all times and explain the reason for your plot.

Problems
111
1.10. Ramp in terms of unit-step signals
A ramp, r(t) = tu(t), can be expressed as
r(t) =
∞
Z
0
u(τ)u(t −τ)dτ
(a) Show that the above expression for r(t) is equivalent to
r(t) =
t
Z
0
dτ = tu(t)
(b) Compute the derivative of
r(t) =
∞
Z
0
u(τ)u(t −τ)dτ
to show that
u(t) =
∞
Z
0
u(τ)δ(t −τ)dτ
1.11. Sampling signal and impulse signal—MATLAB
Consider the sampling signal
δT(t) =
∞
X
k=0
δ(t −kT)
which we will use in the sampling of analog signals later on.
(a) Plot δT(t). Find
ssT(t) =
t
Z
−∞
δT(τ)dτ
and carefully plot it for all t. What does the resulting signal ss(t) look like? In reference 17, Craig calls it
the “stairway to the stars.” Explain.
(b) Use MATLAB function stairs to plot ssT(t) for T = 0.1. Determine what signal would be the limit as
T →0.
(c) A sampled signal is
xs(t) = x(t)δT(t) =
∞
X
k=0
x(kTs)δ(t −kTs)
Let x(t) = cos(2πt)u(t) and Ts = 0.1. Find the integral
t
Z
−∞
xs(t)dt
and use MATLAB to plot it for 0 ≤t ≤10. In a simple way this problem illustrates the operation of a
discrete-to-analog converter, which converts a discrete-time into a continuous-time signal (its cousin
is the digital-to-analog converter or DAC).

112
CHAPTER 1:
Continuous-Time Signals
1.12. Reﬂection and time shifting
Do the reﬂection and the time-shifting operations commute? That is, do the two block diagrams in
Figure 1.17 provide identical signals (i.e., is y(t) equal to z(t))? To provide an answer to this consider the
signal x(t) shown in Figure 1.17. Reﬂect x(t) to get v(t) = x(−t) and then shift it to get y(t) = v(t −2). Then
consider delaying x(t) to get w(t) = x(t −2), and reﬂecting it to get z(t) = w(−t). Perform each of these
operations on x(t) to get y(t) and z(t); plot them and compare these plots. What is your conclusion? Explain
FIGURE 1.17
Problem 1.12.
t
1
1
Reﬂection
Reﬂection
Delay by 2
Delay by 2
x(t )
x(t )
x(t )
v(t )
w(t )
z(t )
y(t )
1.13. Contraction and expansion of signals
Let x(t) be the analog signal considered in Problem 1.12 (see Figure 1.17). In this problem we would like to
consider expanded and compressed versions of that signal.
(a) Plot x(2t) and determine if it is a compressed or expanded version of x(t).
(b) Plot x(t/2) and determine if it is a compressed or expanded version of x(t).
(c) Suppose x(t) is an acoustic signal—let’s say it is a music signal recorded in a magnetic tape. What
would be a possible application of the expanding and compression operations? Explain.
1.14. Even and odd decomposition and power
Consider the analog signal x(t) in Figure 1.18.
FIGURE 1.18
Problem 1.14.
1
1
0
t
x(t )
(a) Plot the even–odd decomposition of x(t) (i.e., ﬁnd and plot the even xe(t) and the odd xo(t) components
of x(t)).

Problems
113
(b) Show that the energy of the signal x(t) can be expressed as the sum of the energies of its even and
odd components—that is, that
∞
Z
−∞
x2(t)dt =
∞
Z
−∞
x2
e (t)dt +
∞
Z
−∞
x2
o (t)dt
(c) Verify that the energy of x(t) is equal to the sum of the energies of xe(t) and xo(t).
1.15. Generation of periodic signals
A periodic signal can be generated by repeating a period.
(a) Find the function g(t), deﬁned in 0 ≤t ≤2 only, in terms of basic signals and such, that when repeated
using a period of 2, generates the periodic signal x(t), as shown in Figure 1.19.
(b) Obtain an expression for x(t) in terms of g(t) and shifted versions of it.
(c) Suppose we shift and multiply by a constant the periodic signal x(t) to get new signals y(t) = 2x(t −2),
z(t) = x(t + 2), and v(t) = 3x(t). Are these signals periodic?
(d) Let then w(t) = dx(t)/dt, and plot it. Is w(t) periodic? If so, determine its period.
FIGURE 1.19
Problem 1.15.
1
0
t
1
2
x(t)
−1
−1
· · ·
· · ·
1.16. Contraction and expansion and periodicity—MATLAB
Consider the periodic signal x(t) = cos(πt) of period T0 = 2 sec.
(a) Is the expanded signal x(t/2) periodic? If so, indicate its period.
(b) Is the compressed signal x(2t) periodic? If so, indicate its period.
(c) Use MATLAB to plot the above two signals and verify your analytic results.
1.17. Derivatives and integrals of periodic signals
Consider the triangular train of pulses x(t) in Figure 1.20.
FIGURE 1.20
Problem 1.17.
1
0
t
1
2
· · ·
· · ·
x(t )
−1

114
CHAPTER 1:
Continuous-Time Signals
(a) Carefully plot the signal y(t) = dx(t)/dt, the derivative of x(t).
(b) Can you compute
z(t) =
∞
Z
−∞
[x(t) −0.5]dt
If so, what is it equal to? If not, explain why.
(c) Is x(t) a ﬁnite-energy signal? How about y(t)?
1.18. Complex exponentials
For a complex exponential signal x(t) = 2ej2πt:
(a) Determine its analog frequency 0 in rad/sec and its analog frequency f in hertz. Then ﬁnd the signal’s
period.
(b) Suppose y(t) = ejπt. Would the sum of these signals z(t) = x(t) + y(t) also be periodic? If so, what is
the period of z(t)?
(c) Suppose we then generate a signal v(t) = x(t)y(t), with the x(t) and y(t) signals given before. Is v(t)
periodic? If so, what is its period?
1.19. Full-wave rectiﬁed signal—MATLAB
Consider the full-wave rectiﬁed signal
y(t) = | sin(πt)|
−∞< t < ∞
part of which is shown in Figure 1.21.
FIGURE 1.21
Problem 1.19.
−2
−1
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
t
y(t)
(a) As a periodic signal, y(t) does not have ﬁnite energy, but it has a ﬁnite power Py. Find it.
(b) It is always useful to get a quick estimate of the power of a periodic signal by ﬁnding a bound for the
signal squared. Find a bound for |y(t)|2 and show that Py < 1.
(c) Use symbolic MATLAB to check if the full-wave rectiﬁed signal has ﬁnite power and if that value
coincides with the Py you found above. Plot the signal and provide the script for the computation of
the power. How does it coincide with the analytical result?

Problems
115
1.20. Multipath effects, ﬁrst part—MATLAB
In wireless communications, the effects of multipath signiﬁcantly affect the quality of the received signal.
Due to the presence of buildings, cars, etc. between the transmitter and the receiver, the sent signal does
not typically go from the transmitter to the receiver in a straight path (called line of sight). Several copies
of the signal, shifted in time and frequency as well as attenuated, are received—that is, the transmission
is done over multiple paths each attenuating and shifting the sent signal. The sum of these versions of
the signal appears quite different from the original signal given that constructive as well as destructive
effects may occur. In this problem we consider the time-shift of an actual signal to illustrate the effects of
attenuation and time shift. In the next problem we consider the effects of time and frequency shifting and
attenuation.
Assume that the MATLAB “handel.mat” signal is an analog signal x(t) that it is transmitted over three
paths, so that the received signal is
y(t) = x(t) + 0.8x(t −τ) + 0.5x(t −2τ)
and let τ = 0.5 seconds. Determine the number of samples corresponding to a delay of τ seconds by using
the sampling rate Fs (samples per second) given when the ﬁle “handel.mat” is loaded.
To simplify matters, just work with a signal of duration 1 second—that is, generate a signal from “han-
del.mat” with the appropriate number of samples. Plot the segment of the original “handel.mat” signal x(t)
and the signal y(t) to see the effect of multipath. Use the MATLAB function sound to listen to the original
and the received signals.
1.21. Multipath effects, second part—MATLAB
Consider now the Doppler effect in wireless communications. The difference in velocity between the trans-
mitter and the receiver causes a shift in frequency in the signal, which is called the Doppler effect (e.g., this
is just like the acoustic effect of a train whistle as a train goes by).
To illustrate the frequency-shift effect, consider a complex exponential x(t) = ej0t. Assume two paths:
One that does not change the signal, while the other causes the frequency shift and attenuation, resulting
in the signal
y(t) = ej0t + αej0tejφt
= ej0t h
1 + αejφti
where α is the attenuation and φ is the Doppler frequency shift, which is typically much smaller than the
signal frequency. Let 0 = π, φ = π/100, and α = 0.7. This is analogous to the case where the received
signal is the sum of the line-of-sight signal and an attenuated signal affected by Doppler.
(a) Consider the term αejφt, a phasor with frequency φ = π/100 to which we add 1. Use the MATLAB
plotting function compass to plot the addition 1 + 0.7ejφt for times from 0 to 256 sec, changing in
increments of T = 0.5 sec.
(b) If we write y(t) = A(t)ej(0t+θ(t)), give analytical expressions for A(t) and θ(t), and compute and plot
them using MATLAB for the times indicated above.
(c) Compute the real part of the signal
y1(t) = x(t) + 0.7x(t −100)ejφ(t−100)
That is, the effects of time and frequency delays, put together with attenuation, for the times indicated
in part (a). Use the function sound (let Fs = 2000 in this function) to listen to the different signals.
1.22. Beating or pulsation—MATLAB
An interesting phenomenon in the generation of musical sounds is beating or pulsation. Suppose NP dif-
ferent players try to play a pure tone, a sinusoid of frequency 160 Hz, and that the signal recorded is the

116
CHAPTER 1:
Continuous-Time Signals
sum of these sinusoids. Assume the NP players while trying to play the pure tone end up playing tones
separated by 0.02 Hz, so that the recorded signal is
y(t) =
NP
X
i=1
10 cos(2πfit)
where the fi are frequencies from 159 to 161 separated by 1 Hz.
(a) Generate the signal y(t) 0 ≤t ≤200 sec in MATLAB. Let each musician play a unique frequency.
Consider an increasing number of players, letting NP be ﬁrst 51 players with 1 = 0.04 Hz, and then
101 players with 1 = 0.02 Hz. Plot y(t) for each of the different number of players.
(b) Explain how this is related with multipath and Doppler effects discussed in the previous problems.
1.23. Chirps—MATLAB
Pure tones or sinusoids are not very interesting to listen to. Modulation and other techniques are used to
generate more interesting sounds. Chirps, which are sinusoids with time-varying frequency, are some of
those more interesting sounds. For instance, the following is a chirp signal:
y(t) = A cos(ct + s(t))
(a) Let A = 1, c = 2, and s(t) = t2/4. Use MATLAB to plot this signal for 0 ≤t ≤40 sec in steps of
0.05 sec. Use the sound function to listen to the signal.
(b) Let A = 1, c = 2, and s(t) = −2 sin(t). Use MATLAB to plot this signal for 0 ≤t ≤40 sec in steps of
0.05 sec. Use the sound function to listen to the signal.
(c) The frequency of these chirps is not clear. The instantaneous frequency IF(t) is the derivative with
respect to t of the argument of the cosine. For instance, for a cosine cos(0t), the IF(t) = d0t/dt =
0, so that the instantaneous frequency coincides with the conventional frequency. Determine the
instantaneous frequencies of the two chirps and plot them. Do they make sense as frequencies?
Explain.

CHAPTER 2
Continuous-Time Systems
Things should be made as simple as possible,
but not any simpler.
Albert Einstein (1879–1955)
physicist
2.1 INTRODUCTION
In this chapter we will consider the following topics:
I
Systems and their classiﬁcation—The concept of system is useful in dealing with actual devices or
processes for purposes of analysis and synthesis. A transmission line, for instance, carrying infor-
mation from one point to another is a system, even though physically it is just wires connecting
two terminals. Voltages and currents in this system are not just functions of time but also of
space. It takes time for a voltage signal to “travel” from one point to another separated by miles—
Kirchhoff’s laws do not apply. Resistance, capacitance, and inductance of the line are distributed
over the length of the line—that is, the line is modeled as a concatenation of circuits charac-
terized by values of resistance, capacitance, and inductance per unit length. A less complicated
system could be one consisting of resistors, capacitors, and inductors where ideal models are
used to represent these elements and to perform analysis and synthesis. The word “ideal” indi-
cates that the models only approximate the real behavior of resistors, capacitors, and inductors. A
more realistic model for a resistor would need to consider possible changes in the resistance due
to temperature, and perhaps other marginal effects present in the resistor. Although this would
result in a better model, for most practical applications it would be unnecessarily complicated.
I
Linear time-invariant systems—We initiate the characterization of systems, and propose the linear
time-invariant (LTI) model as a mathematical idealization of the behavior of systems—a good
starting point. It will be seen that most practical systems deviate from it, but despite that, the
behavior of many devices is approximated as linear and time invariant. A transistor, which is a
nonlinear device, is analyzed using linear models around an operating point. Although the vocal
system is hardly time invariant or linear, or even represented by a differential equation, in speech
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00005-3
c⃝2011, Elsevier Inc. All rights reserved.
117

118
CHAPTER 2:
Continuous-Time Systems
synthesis short intervals of speech are modeled as the output of linear time-invariant models.
Finally, it will be seen that the LTI model is not appropriate to represent communication systems;
rather, nonlinear or time-varying systems are more appropriate.
I
Convolution integral, causality, and stability— The output of a LTI system due to any signal is
obtained by means of the generic signal representation obtained in Chapter 1. The response due
to an impulse, together with the linearity and time-invariance of the system, gives the output as
an integral. This convolution integral, although difﬁcult to compute, even in simple cases, has
signiﬁcant theoretical value. It allows us not only to determine the response of the system for very
general cases, but also provides a way to characterize causal and stable systems. Causality relates
to the cause and effect of the input and the output of the system, giving us the conditions for
real-time processing while stability characterizes useful systems. These two conditions are of great
practical signiﬁcance.
2.2 SYSTEM CONCEPT
Although we view a system as a mathematical transformation of an input signal (or signals) into an
output signal (or signals), it is important to understand that such transformation results from an
idealized model of the physical device or process we are interested in.
For instance, in the interconnection of physical resistors, capacitors, and inductors, the model
idealizes how to deal with the resistors, capacitors, and inductors. In this simple RLC circuit, we
would ignore, for instance, stray inductive and capacitive effects and the effect of temperature on
the resistors. The resistance, capacitance, and impedance would be assumed localized in the physical
devices and the wires would not have resistance, inductance, or capacitance. We would then use the
circuits laws to obtain a differential equation to characterize the interconnection. A wire that in the
RLC circuit model connects two elements, in a transmission line a similar wire is modeled as having
capacitance, inductance, and resistance distributed over the line to realize the way the voltages travel
over it. In practice, the model and the mathematical representation are not unique.
A system can be considered a connection of subsystems. Thinking of the RLC circuit as a system, for
instance, the resistor, the capacitor, the inductor, and the source are the subsystems.
In engineering, the models are typically developed in the different areas. There will be, however,
analogs as it is the case between mechanical and electrical systems. In such cases, the mathematical
equations are similar, or even identical, but their signiﬁcance is very different.
2.2.1 System Classiﬁcation
According to general characteristics attributed to systems, they can be classiﬁed as follows:
I
Static or dynamic systems—A dynamic system has the capability of storing energy, or remembering
its state, while a static system does not. A battery connected to resistors is a static system, while
the same battery connected to resistors, capacitors, and inductors constitutes a dynamic system.
The main difference is the capability of capacitors and inductors to store energy, to remember the
state of the device, that resistors do not have.

2.3 LTI Continuous-Time Systems
119
I
Lumped- or distributed-parameter systems—This classiﬁcation relates as to how the elements of the
system are viewed. In the case of the RLC circuit, the resistance, capacitance, and inductance
are localized so that these physical elements are modeled as lumped elements. In the case of
a transmission line resistance, capacitance and inductance are modeled as distributed over the
length of the line.
I
Passive or active systems—A system is passive if it is not able to deliver energy to the outside world.
Constant resistors, capacitors, and inductors are passive elements. An operational ampliﬁer is an
active system.
Dynamic systems with lumped parameters, such as the RLC circuit, are typically represented by ordi-
nary differential equations, while distributed-parameter dynamic systems like the transmission line
are represented by partial differential equations. In the case of lumped systems only the time varia-
tion is of interest, while in the case of distributed systems we are interested in both time and space
variations of the signals. In this book we consider only dynamic systems with lumped parameters,
possibly changing with time, with a single input and a single output.
A further classiﬁcation of systems is obtained by considering the types of signals present at the input
and the output of the system.
Whenever the input(s) and output(s) are both continuous time, discrete time, or digital, the corresponding
systems are continuous time, discrete time, or digital, respectively. It is also possible to have hybrid systems
when the input(s) and output(s) are not of the same type.
Of the systems presented in Chapter 0, the CD player is a hybrid system as it has a digital input (the
bits stored on the disc) and an analog output (the acoustic signal put out by the player). The SDR
system, on the other hand, can be considered to have an analog input (in the transmitter) and an
analog output (at the receiver), making it an analog system, but having hybrid subsystems.
2.3 LTI CONTINUOUS-TIME SYSTEMS
A continuous-time system is a system in which the signals at its input and output are continuous-time signals.
Mathematically we represent it as a transformation S that converts an input signal x(t) into an output signal
y(t) = S[x(t)] (see Figure 2.1):
x(t)
⇒
y(t) = S[x(t)]
(2.1)
Input
Output
FIGURE 2.1
System S with input x(t) and
output y(t).
S
Input
x(t)
Output
y(t)=S[x(t)]

120
CHAPTER 2:
Continuous-Time Systems
When developing a mathematical model for a continuous-time system it is important to contrast the
accuracy of the model with its simplicity and practicality. The following are some of the characteristics
of the model being considered:
I
Linearity
I
Time invariance
I
Causality
I
Stability
The linearity between the input and the output, as well as the constancy of the system parameters,
simplify the mathematical model. Causality, or nonanticipatory behavior of the system, relates to
the cause–effect relationship between the input and the output. It is essential when the system is
working under real-time situations—that is, when there is limited time for the system to process
signals coming into the system. Stability is needed in practical systems. A stable system behaves well
under reasonable inputs. Unstable systems are useless.
2.3.1 Linearity
A system represented by S is said to be linear if for inputs x(t) and v(t), and any constants α and β,
superposition holds—that is,
S[αx(t) + βv(t)] = S[αx(t)] + S[βv(t)]
= αS[x(t)] + βS[v(t)]
(2.2)
When checking the linearity of a system we ﬁrst need to check the scaling—that is, if the output
y(t) = S[x(t)] for some input x(t) is known, then for a scaled input αx(t) the output should be αy(t) =
αS[x(t)]. If this condition is not satisﬁed, the system is nonlinear. If the condition is satisﬁed, you
would then test the additivity or that the response to the sum of weighted inputs, S[αx(t) + βv(t)], is
the sum of the corresponding responses αS[x(t)] + βS[v(t)].
The scaling property of linear systems indicates that whenever the input of a linear system is zero
the output is zero. Thus, if the output corresponding to an input x(t) is y(t), then the output
corresponding to αx(t) is αy(t); and if, in particular, α = 0, then both input and output are zero.
I Example 2.1
Consider a biased averager—that is, the output y(t) of such a system is given by
y(t) = 1
T
t
Z
t−T
x(τ)dτ + B
for an input x(t). The system ﬁnds the average over an interval T and adds a constant value B. Is
this system linear? If not, is there a way to make it linear? Explain.

2.3 LTI Continuous-Time Systems
121
Solution
Let y(t) be the system response corresponding to x(t). Assume then that we scale the input by a
factor α so that the input is αx(t). The corresponding output is then
1
T
t
Z
t−T
αx(τ)dτ + B = α
T
t
Z
t−T
x(τ)dτ + B
which is not equal to
αy(t) = α
T
t
Z
t−T
x(τ)dτ + αB
so the system is not linear. Notice that the difference is due to the term associated with B, which is
not affected at all by the scaling of the input. So to make the system linear we let B = 0.
The constant B is the response due to zero input, and as such, the response can be seen as the sum
of a linear system and a zero-input response. This type of system is called incrementally linear given
that if
S[x1(t)] = y1(t) −B
S[x2(t)] = y2(t) −B
then
S[x1(t) −x2(t)] = S[x1(t)] −S[x2(t)] = y1(t) −y2(t)
= 1
T
t
Z
t−T
[x1(τ) −x2(τ)]dτ
That is, the difference of the responses to two inputs is linear.
I
I Example 2.2
Whenever the explicit relation between the input and the output of a system is represented by a
nonlinear expression the system is nonlinear. Consider the following input–output relations that
show the corresponding systems are nonlinear:
(i) y(t) = |x(t)|
(ii) z(t) = cos(x(t)) assuming |x(t)| ≤1
(iii) v(t) = x2(t)
where x(t) is the input and y(t), z(t), and v(t) are the outputs.

122
CHAPTER 2:
Continuous-Time Systems
Solution
Superposition is not satisﬁed for the ﬁrst system. If the outputs for x1(t) and x2(t) are y1(t) = |x1(t)|
and y2(t) = |x2(t)|, respectively, the output for x1(t) + x2(t) is
y12(t) = |x1(t) + x2(t)| ≤|x1(t)| + |x2(t)| = y1(t) + y2(t)
For the second system, if the response for x(t) is z(t) = cos(x(t)), the response for −x(t) is not −z(t)
because the cosine is an even function of its argument. Thus,
−x(t) →cos(−x(t)) = cos(x(t)) = z(t)
For the third system, if x1(t) →v1(t) = (x1(t))2 and x2(t) →v2(t) = (x2(t))2 are corresponding
input–output pairs, then
x1(t) + x2(t) →(x1(t) + x2(t))2 = (x1(t))2 + (x2(t))2 + 2x1(t)x2(t) ̸= v1(t) + v2(t)
Thus, it is nonlinear.
I
I Example 2.3
Consider each of the components of an RLC circuit and determine under what conditions they are
linear.
Solution
Because a resistor, a capacitor, and an inductor are one-port or two-terminal elements, input and
output variables are not obvious. However, from physics the cause and effect are well understood.
A resistor R has a voltage–current relation
v(t) = Ri(t)
(2.3)
If this relation is a straight line through the origin the resistor is linear; otherwise it is non-linear.
A diode is an example of a nonlinear resistor: and its voltage–current relation is nonlinear.
If the voltage-current relation is a straight line of constant slope R, considering the current is the
input, superposition is satisﬁed. Indeed, if we apply to the resistor a current i1(t) to get Ri1(t) =
v1(t) and get Ri2(t) = v2(t) when we apply a current i2(t), then when a current ai1(t) + bi2(t),
for any constants a and b, is applied, the voltage across the resistor is v(t) = R(ai1(t) + bi2(t)) =
av1(t) + bv2(t)—that is, the resistor R is a linear system.
A capacitor is characterized by the charge–voltage relation
q(t) = Cvc(t)
(2.4)
If this relation is not a straight line, the capacitor is nonlinear. A varactor is a diode for which its
capacitance depends nonlinearly on the voltage applied to its terminals, and thus it is a nonlinear
capacitor.

2.3 LTI Continuous-Time Systems
123
When the relation is a straight line through the origin with a constant slope C, using the current–
charge relation i(t) = dq(t)/dt, we get the differential equation
i(t) = Cdvc(t)/dt
characterizing the capacitor. Letting i(t) be the input, solving this differential equation gives as
output the voltage
vc(t) = 1
C
t
Z
0
i(τ)dτ + vc(0)
(2.5)
which explains the way the capacitor works. For time t > 0, the capacitor accumulates charge on
its plates beyond the original charge due to an initial voltage vc(0). The capacitor is seen to be a
linear system if vc(0) = 0; otherwise it is not. In fact, when vc(0) = 0, the outputs corresponding
to i1(t) and i2(t) are
vc1(t) = 1
C
t
Z
0
i1(τ)dτ
vc2(t) = 1
C
t
Z
0
i2(τ)dτ
respectively, and the output due to a combination ai1(t) + bi2(t) is
1
C
t
Z
0
[ai1(τ) + bi2(τ)]dτ = avc1(t) + bvc2(t)
Thus, a linear capacitor is a linear system if it is not initially charged. When the initial condition is
not zero, the capacitor is affected by the current input i(t) as well as by the initial condition vc(0),
and as such it is not possible to satisfy linearity, as only the current input can be changed. The
capacitor is thus an incrementally linear system.
The inductor L is the dual of the capacitor (replacing currents by voltages and C by L in the above
equations, we obtain the equations for the inductor). A linear inductor is characterized by the
magnetic ﬂux–current relation
φ(t) = LiL(t)
(2.6)
being a straight line of slope L > 0. If the plot of the magnetic ﬂux φ(t) and the current iL(t) is not
a line, the inductor is nonlinear. The voltage across the inductor is
v(t) = dφ(t)
dt
= LdiL(t)
dt

124
CHAPTER 2:
Continuous-Time Systems
according to Faraday’s induction law. Solving this differential equation for the current we obtain
iL(t) = 1
L
t
Z
0
v(τ)dτ + iL(0)
(2.7)
Like the capacitor, the inductor is not a linear system unless the initial current in the inductor is
zero. The inductor can be considered an incrementally linear system.
Notice that an explicit relation between the input and the output was necessary to determine
linearity.
I
Op-Amps and Feedback
Operational ampliﬁers, or op-amps, are high-gain ampliﬁers typically used with feedback. In the 1930s, Harold S. Black
developed the principles of feedback ampliﬁers—that is the application of a portion of the output back to the input to
reduce the overall gain. By doing so, the characteristics of the ampliﬁer are greatly enhanced. In the late 1930s, George A.
Philbrick developed a vacuum-tube circuit that performed some of the op-amp functions. Professor John Ragazzini, from
Columbia University, coined the name of “operational ampliﬁer” in 1947. Early op-amps were vacuum-tube based, and thus
bulky and expensive. The trend to cheaper and smaller op-amps began in the 1960s [50, 72].
The Op-Amp
An excellent example of a device that can be used as either a nonlinear or a linear system is the
operational ampliﬁer or op-amp. It is a two-port device (see Figure 2.2) with two voltage inputs: v−(t),
in the inverting terminal, and v+(t), in the noninverting terminal. The output voltage v0(t) is a nonlinear
function of the difference between the two inputs—that is,
vo(t) = f[v+(t) −v−(t)] = f(vd(t))
The function f(vd(t)) is approximately linear for small values ±1V of vd(t), in the order of millivolts,
and it becomes constant beyond ±1V. The output voltage v0(t) is, however, in the order of volts, so
that letting
vo(t) = Avd(t)
−1V ≤vd(t) ≤1V
FIGURE 2.2
Operational ampliﬁer: (a) circuit
diagram, and (b) input–output
voltage relation.
−ΔV
ΔV
vo(t)
vd(t)
−Vsat
Vsat
(a)
(b)
−
+
v−(t)
v+(t)
vo(t)
+
+
+
−
−
−
A
i−
i+

2.3 LTI Continuous-Time Systems
125
be a line through the origin, its slope A is very large. If |vd(t)| > 1V the output voltage is a constant
Vsat. That is, the gain of the ampliﬁer saturates. Furthermore, the input resistance of the op-amp is
large so that the currents into the negative and the positive terminals are very small. The op-amp
output resistance is relatively small.
Thus, depending on the dynamic range of the input signals, the op-amp operates in either a linear
region or a nonlinear region. Restricting the operational ampliﬁer to operate in the linear region sim-
pliﬁes the model. Assuming that A →∞, and that Rin →∞, then we obtain the following equations
deﬁning an ideal operational ampliﬁer:
i−= i+ = 0
vd(t) = v+(t) −v−(t) = 0
(2.8)
These equations are called the virtual short and are valid only if the output voltage of the operational
ampliﬁer is limited by the saturation voltage Vsat—that is, when
−Vsat ≤vo(t) ≤Vsat
Later in the chapter we will consider ways to use the op-amp to get inverters, integrators, adders, and
buffers.
2.3.2 Time Invariance
A continuous-time system S is time invariant if whenever for an input x(t) with a corresponding output
S[x(t)], the output corresponding to a shifted input x(t ∓τ) (delayed or advanced) is the original output shifted
in time S[x(t ∓τ)] (delayed or advanced). Thus,
x(t)
⇒
y(t) = S[x(t)]
x(t ∓τ)
⇒
y(t ∓τ) = S[x(t ± τ)]
(2.9)
That is, the system does not age—its parameters are constant.
A system that satisﬁes both the linearity and the time invariance is called a linear time-invariant or LTI
system.
Remarks
I
It should be clear that linearity and time invariance are independent of each other. Thus, one can have
linear time-varying or nonlinear time-invariant systems.
I
Although most actual systems are, according to the above deﬁnitions, nonlinear and time varying, linear
models are used to approximate around an operating point the nonlinear behavior, and time-invariant
models are used to approximate in short segments the system’s time-varying behavior. For instance, in
speech synthesis the vocal system is typically modeled as a linear time-invariant system for intervals of
about 20 msec, attempting to approximate the continuous variation in shape in the different parts of the
vocal system (mouth, cheeks, nose, etc.). A better model for such a system is clearly a linear time-varying
model.

126
CHAPTER 2:
Continuous-Time Systems
I
In many cases time invariance can be determined by identifying—if possible—the input and the output,
and letting the rest represent the parameters of the system. If these parameters change with time, the system
is time varying. For instance, if the input x(t) and the output y(t) of a system are related by the equation
y(t) = f(t)x(t)
the parameter of the system is the function f(t), and if it is not constant, the system is time varying. Thus,
the system y(t) = tx(t) is time varying as can be easily veriﬁed. Likewise, the AM modulation system given
by y(t) = cos(0t)x(t) is time varying as the function f(t) = cos(ot).
AM Communication Systems
Amplitude modulation (AM) communication systems arose from the need to send an acoustic sig-
nal, the “message,” over the airwaves using a reasonably sized antenna to radiate it. The size of the
antenna depends inversely on the frequencies present in the message, and voice and music have rel-
atively low frequencies. A voice signal typically has frequencies in the range of 100 Hz to about 5
KHz (the frequencies needed to make a telephone conversation intelligible), while music typically
displays frequencies up to about 22 KHz. The transmission of such signals with a practical antenna
is impossible. To make the transmission possible, modulation was introduced—that is, multiplying
the message m(t) by a periodic signal such as a cosine cos(0t), the carrier, with a frequency 0
much larger than those in the acoustic signal. Amplitude modulation provided the larger frequencies
needed to reduce the size of the antenna. Thus, y(t) = m(t) cos(0t) is the signal to be transmitted,
and we will see later that the effect of this multiplication is to change the frequency content of the
input. Such a system is clearly linear, but time-varying. Indeed, if the input is m(t −τ) the output
would be m(t −τ) cos(0t), which is not y(t −τ) = cos(0(t −τ))m(t −τ), as a time-invariant sys-
tem would give. Figure 2.3 illustrates the AM transmitter and receiver. In Chapter 6, we will discuss
AM and other modulation systems and will illustrate them with MATLAB simulations.
In comparison with the AM system, a frequency modulation (FM) system is represented by the
following input–output equation, where m(t) is the input message and z(t) the output:
z(t) = cos(ct +
t
Z
−∞
m(τ)dτ)
FIGURE 2.3
AM modulation: transmitter and receiver.
m(t)
cos(Ω0t)
y(t)
×
Transmitter
Receiver
m˜ (t)

2.3 LTI Continuous-Time Systems
127
The FM system is nonlinear. Suppose that we scale the message to γ m(t), for some constant γ , the
corresponding output is given by
cos(ct + γ
t
Z
−∞
m(τ)dτ)
which is not the previous output scaled (i.e., γ z(t)); thus FM is a nonlinear system.
The Beginnings of Radio
The names of Nikola Tesla (1856–1943) and Reginald Fessenden (1866–1932) are linked to the invention of radio and ampli-
tude modulation [3, 58, 75]. Radio was initially called “wireless telegraphy” and then “wireless.” Tesla was a mechanical
as well as an electrical engineer, but mostly an inventor. He has been credited with signiﬁcant contributions to electricity
and magnetism in the late 19th and early 20th centuries. His work is the basis of the alternating-current (AC) power system
and the induction motor. His work on wireless communications using the “Tesla coils” was capable of transmitting and
receiving radio signals. Although Tesla submitted a patent application for his basic radio before Guglielmo Marconi, it was
Marconi who was initially given the patent for the invention of the radio (1904). The Supreme Court in 1943 reversed the
decision in favor of Tesla [45].
Fessenden has been called the “father of radio broadcasting.” His early work on radio led to demonstrations in December
1906 of the capability of point-to-point wireless telephony, and what appears to be the ﬁrst radio broadcasts of entertainment
and music ever made to an audience (in this case, shipboard radio operators in the Atlantic). Fessenden was a professor of
electrical engineering at Purdue University and the ﬁrst chairman of the electrical engineering department of the University
of Pittsburgh in 1893.
Vocal System
A remarkable system that we all have is the vocal system (see Figure 2.4). The air pushed out from
the lungs in this system is directed by the trachea through the vocal cords, making them vibrate and
create resonances similar to those from a wind musical instrument. The generated sounds are then
mufﬂed by the mouth and the nasal cavities, resulting in an acoustic signal carrying a message. Given
the length of the typical vocal system, it is modeled as a distributed system and represented by partial
differential equations. Due to the complexity of this model, it is the speech signal along with the
understanding of the speech production that is used to obtain models of the vocal system. Speech
processing is one of the most fascinating areas of electrical engineering.
A typical linear time-invariant model for speech production considers segments of speech of about
20 msec, and for each develops a low-order LTI system. The input is either a periodic pulse for the
generation of voiced sounds (e.g., vowels) or a noiselike signal for unvoiced sounds (e.g., the /sh/
sound). Processing these inputs gives speechlike signals. A linear time-varying model would take into
consideration the variations of the vocal system with time and it would thus be more appropriate.
I Example 2.4
Characterize time-varying resistors, capacitors, and inductors. Assume zero initial conditions in the
capacitors and inductors.

128
CHAPTER 2:
Continuous-Time Systems
Nose
Lips
Nasal cavity
Tongue
Vellum
Epiglottis
Vocal
chords
Lungs
(a)
LTI system
ev(t)
Voiced
s(t)
Unvoiced
eu(t)
Speech
(b)
FIGURE 2.4
(a) Vocal system: principal organs of articulation. (b) Model for speech production.
Solution
If we generalize the characteristic equations for the resistor, capacitor, and inductor to be
v(t) = R(t)i(t)
q(t) = C(t)vc(t)
φ(t) = L(t)iL(t)
as straight lines with time-varying slope, we have linear but time-varying elements. Using i(t) =
dq(t)/dt and v(t) = dφ(t)/dt, we obtain the following voltage–current relations:
v(t) = R(t)i(t)
i(t) = C(t)dvc(t)
dt
+ dC(t)
dt
vc(t)
v(t) = L(t)diL(t)
dt
+ dL(t)
dt iL(t)
As R(t) is a function of time, the resistor is a time-varying system. The second and third equation
are linear differential equations with time-varying coefﬁcients representing time-varying capacitors
and inductors.
I

2.3 LTI Continuous-Time Systems
129
I Example 2.5
Consider constant linear capacitors and inductors, represented by differential equations
dvc(t)
dt
= 1
Ci(t)
diL(t)
dt
= 1
Lv(t)
with initial conditions vc(0) = 0 and iL(0) = 0. Under what conditions are these time-invariant
systems?
Solution
Given the duality of the capacitor and the inductor, we only need to consider one of these. Solving
the differential equation for the capacitor, we get
vc(t) = 1
C
t
Z
0
i(τ)dτ
Let us then ﬁnd out what happens when we delay (or advance) the input current i(t) by λ sec. The
corresponding output for t > λ is given by
1
C
t
Z
0
i(τ −λ)dτ = 1
C
0
Z
−λ
i(ρ)dρ + 1
C
t−λ
Z
0
i(ρ)dρ
(2.10)
by changing the integration variable to ρ = τ −λ. For Equation (2.10) to equal the voltage at the
capacitor delayed λ sec, given by
vc(t −λ) = 1
C
t−λ
Z
0
i(ρ)dρ
we need that i(t) = 0 for t < 0, so that the ﬁrst integral in the right expression in Equation (2.10) is
zero. Thus, the system is time invariant if the input current i(t) = 0 for t < 0. If the initial condition
v(0) is not zero, or if the input i(t) is not zero for t < 0, then linearity or time invariance, or both,
are not satisﬁed. A similar situation occurs with the inductor.
Thus, an RLC circuit is an LTI system provided that it is not energized for t < 0—that is, that the
initial conditions as well as the input are zero for t < 0.
I
RLC Circuits
An RLC circuit is represented by an ordinary differential equation of order equal to the number of
independent inductors and capacitors (i.e., if two or more capacitors are connected in parallel, or
if two or more inductors are connected in series they share the same initial conditions and can be
simpliﬁed to one capacitor and one inductor), and with constant coefﬁcients (due to the assumption

130
CHAPTER 2:
Continuous-Time Systems
FIGURE 2.5
RLC circuit.
v(t)
i(t)
+
−
t=0
R
C
L
that the R, L, and C values are constant). If the initial conditions of the RLC circuit are zero, and the
input is zero for t < 0, then the system represented by the linear differential equation with constant
coefﬁcients is LTI.
Consider, for instance, the circuit in Figure 2.5 consisting of a series connection of a resistor R, an
inductor L, and a capacitor C. The switch has been open for a very long time and it is closed at t = 0,
so that there is no initial energy stored in either the inductor or the capacitor (the initial current in
the inductor is iL(0) = 0 and the initial voltage in the capacitor is vC(0) = 0) and the voltage applied
to the elements is zero for t < 0. This circuit is represented by a second-order differential equation
with constant coefﬁcients. According to Kirchhoff’s voltage law,
v(t) = Ri(t) + Ldi(t)
dt
+ 1
C
t
Z
0
i(τ)dτ
and taking a derivative of v(t) with respect to t we obtain
dv(t)
dt
= Rdi(t)
dt
+ Ld2i(t)
dt2
+ 1
Ci(t)
a second-order differential equation, with input the voltage source v(t) and output the current i(t).
2.3.3 Representation of Systems by Differential Equations
Given a dynamic system represented by a linear differential equation with constant coefﬁcients,
a0y(t) + a1
dy(t)
dt
+ · · · + dNy(t)
dtN
= b0x(t) + b1
dx(t)
dt
+ · · · + bM
dMx(t)
dtM
t ≥0
with N initial conditions y(0), dky(t)/dtk|t=0 for k = 1, . . . , N −1 and input x(t) = 0 for t < 0, its complete
response y(t) for t ≥0 has two components:
I
The zero-state response, yzs(t), due exclusively to the input as the initial conditions are zero.
I
The zero-input response, yzi(t), due exclusively to the initial conditions as the input is zero. So that
y(t) = yzs(t) + yzi(t)
(2.11)
Thus, when the initial conditions are zero, then y(t) depends exclusively on the input (i.e., y(t) = yzs(t)), and
the system is linear and time invariant or LTI.

2.3 LTI Continuous-Time Systems
131
On the other hand, if the initial conditions are different from zero, when checking linearity and time invariance
we only change the input and do not change the initial conditions so that yzi(t) remains the same, and thus
the system is nonlinear. The Laplace transform will provide the solution of these systems.
Most continuous-time dynamic systems with lumped parameters are represented by linear ordinary
differential equations with constant coefﬁcients. By linear it is meant that there are no nonlinear
terms such as products of the input and the output, quadratic terms of the input and the output, etc.
If the coefﬁcients change with time the system is time varying. The order of the differential equation
equals the number of independent elements capable of storing energy.
Consider a dynamic system represented by an Nth-order linear differential equation with constant
coefﬁcients, and with x(t) as the input and y(t) as the output:
a0y(t) + a1
dy(t)
dt
+ · · · + dNy(t)
dtN
= b0x(t) + b1
dx(t)
dt
+ · · · + bM
dMx(t)
dtM
t ≥0
(2.12)
The corresponding N initial conditions are y(0), dky(t)/dtk|t=0 for k = 1, . . . , N −1. Deﬁning the
derivative operator as
Dn[y(t)] = dny(t)
dtn
n > 0, integer
D0[y(t)] = y(t)
we write the differential Equation (2.12) as
(a0 + a1D + · · · + DN)[y(t)] = (b0 + b1D + · · · + bMDM)[x(t)]
t ≥0
Dk[y(t)]t=0,
k = 0, . . . , N −1
As indicated before, the system represented by this differential equation is LTI if the initial conditions
as well as the input are zero for t < 0—that is, the system is not energized for t < 0. However, many
LTI systems represented by differential equations have nonzero initial conditions. Considering that
the input signal x(t) is independent of the initial conditions, we can think of these as two different
inputs. As such, using superposition we have that the complete solution of the differential equation is
composed of a zero-input solution, due to the initial conditions when the input x(t) is zero, and the
zero-state response due to the input x(t) with zero initial conditions.
Thus, to ﬁnd the complete solution we need to solve the following two related differential equations:
(a0 + a1D + · · · + DN)[y(t)] = 0
(2.13)
with initial conditions Dk[y(t)]t=0, k = 0, . . . , N −1, and the differential equation
(a0 + a1D + · · · + DN)[y(t)] = (b0 + b1D + · · · + bMDM)[x(t)]
(2.14)
with zero initial conditions. If yzi(t) is the response of the zero-input differential Equation (2.13),
and yzs(t) the zero-state (or zero initial conditions) differential Equation (2.14), we have that the

132
CHAPTER 2:
Continuous-Time Systems
complete response is their sum,
y(t) = yzi(t) + yzs(t)
Indeed, yzi(t) and yzs(t) satisfy
(a0 + a1D + · · · + DN)[yzi(t)] = 0
Dk[yzi(t)]t=0,
k = 0, . . . , N −1
(a0 + a1D + · · · + DN)[yzs(t)] = (b0 + b1D + · · · + bMDM)[x(t)]
Adding these equations gives
(a0 + a1D + · · · + DN)[yzi(t) + yzs(t)] = (b0 + b1D + · · · + bMDM)[x(t)]
Dk[y(t)]t=0,
k = 0, . . . , N −1
indicating that yzi(t) + yzs(t) is the complete solution.
To ﬁnd the solution of the zero-input and the zero-state equations we need to factor out the derivative
operator a0 + a1D + · · · + DN. We can do so by replacing D by a complex variable s, as the roots will
be either real or in complex-conjugate pairs, simple or multiple. The characteristic polynomial
a0 + a1s + · · · + sN =
Y
k
(s −pk)
is then obtained. The roots of this polynomial are called the natural frequencies or eigenvalues and
characterize the dynamics of the system as it is being represented by the differential equation. The
solution of the zero-state can be obtained from a modiﬁed characteristic polynomial.
The solution of differential equations will be efﬁciently done using the Laplace transform in the next
chapter.
I Example 2.6
Consider a circuit that is a series connection of a resistor R = 1  and an inductor L = 1 H, with
a voltage source v(t) = Bu(t), and I0 amps is the initial current in the inductor. Find and solve
the differential equation for B = 1 and B = 2 for initial conditions I0 = 1 and I0 = 0, respectively.
Determine the zero-input and the zero-output responses. Under what conditions is the system
linear and time invariant?
Solution
The ﬁrst-order differential equation representing this circuit is given by
v(t) = i(t) + di(t)
dt
i(0) = I0

2.3 LTI Continuous-Time Systems
133
The solution of this differential equation is given by
i(t) = [I0e−t + B(1 −e−t)]u(t)
(2.15)
which satisﬁes the initial condition i(0) = I0 and the differential equation. In fact, if t = 0+
(slightly larger than 0) we have that the solution gives i(0+) = I0, and that for t > 0 when we
replace in the differential equation the input voltage by B, i(t), and di(t)/dt (using the above
solution), we get
B
|{z}
v(t)
= [I0e−t + B(1 −e−t)]
|
{z
}
i(t)
+ [Be−t −I0e−t]
|
{z
}
di(t)/dt
= B
t > 0
or an identity indicating i(t) in Equation (2.15) is the solution of the differential equation.
Initial Condition Different from Zero
When I0 = 1 and B = 1, the complete solution given by Equation (2.15) becomes
i1(t) = [e−t + (1 −e−t)]u(t)
= u(t)
(2.16)
The zero-state response (i.e., the response due to v(t) = u(t) and zero initial condition) is
i1zs(t) = (1 −e−t)u(t)
which is obtained by letting B = 1 and I = 0 in Equation (2.15). The zero-input response, when
v(t) = 0 and the initial condition is I0 = 1, is
i1zi(t) = e−tu(t)
obtained by subtracting the zero-state response from the complete response in Equation (2.16).
If we then consider B = 2 (i.e., we double the original input) and keep I0 = 1, the complete
solution is given by
i2(t) = [e−t + 2(1 −e−t)]u(t)
= (2 −e−t)u(t)
which is completely different from the expected 2i1(t) = 2u(t) for a linear system. Thus, the system
is not linear (see Figure 2.6). In this case we have that the zero-state response due to v(t) = 2u(t)
and zero-initial conditions is doubled so that
i2zs(t) = 2(1 −e−t)u(t)
while the zero-input response remains the same, as the initial condition did not change. So,
i2zi(t) = e−tu(t)
and we get the complete solution shown above. The output in this case depends on the input v(t)
and on the initial condition, and when testing linearity we are only changing v(t).

134
CHAPTER 2:
Continuous-Time Systems
FIGURE 2.6
Nonlinear behavior of RL
circuit: (top) I0 = 1,
B = 1,
v(t) = u(t), i1(t) = u(t),
and (bottom) I0 = 1,
B = 2, v(t) = 2u(t),
i2(t) = (2 −e−t)u(t),
and i2(t) ̸= 2ii(t).
−1
0
1
2
3
4
5
0
0.5
1
v (t), i1(t)
2v (t), i2(t)
−1
0
1
2
3
4
5
0
0.5
1
1.5
2
t (sec)
v (t)
i1(t)
2v(t)
i2(t)
Zero initial conditions
Suppose then we perform the above experiments with I0 = 0 when B = 1 and when B = 2. We get
i1(t) = (1 −e−t)u(t)
for B = 1, and for B = 2 we get
i2(t) = 2(1 −e−t)u(t)
= 2i1(t)
which indicates the system is linear. In this case the response only depends on the input v(t).
Time invariance
Suppose now that B = 1, v(t) = u(t −1), and the initial condition is I0. The complete response is
i3(t) = I0e−tu(t) + (1 −e−(t−1))u(t −1)
If I0 = 0, then the above response is i3(t) = (1 −e−(t−1))u(t −1), which equals i(t −1) (Equa-
tion (2.15) with B = 1 and I0 = 0 delayed by 1) indicating the system is time invariant. On the
other hand, when I0 = 1 the complete response is not equal to i(t −1) because the term with the
initial condition is not shifted like the second term. The system in that case is time varying. Thus,
if I0 = 0 the system is LTI.
I

2.3 LTI Continuous-Time Systems
135
Analog mechanical systems
Making the analogy shown in Table 2.1 between the different variables and elements in a circuit and
in a mechanical system the differential equations representing mechanical systems are found to be
like those for RLC circuits.
Consider the translational mechanical system shown in Figure 2.7, composed of a mass M to which
an external force f(t) is being applied, and is moving at a velocity w(t). It is assumed that between the
mass and the ﬂoor there is a damping with a damping coefﬁcient D. Just as with Kirchhoff’s voltage
law, the applied force equals the sum of the forces generated by the mass and the damper. Thus,
f(t) = Mdw(t)
dt
+ Dw(t)
which is analogous to the differential equation of an RL series circuit with a voltage source v(t):
v(t) = Ldi(t)
dt
+ Ri(t)
Exactly the same as with the RL circuit, if the initial velocity and the external force are zero for t < 0,
the above differential equation represents a LTI mechanical system.
2.3.4 Application of Superposition and Time Invariance
The computation of the output of an LTI system is simpliﬁed when the input can be represented as
the combination of signals for which we know their response. This is done by applying superposition
and time invariance. This property of LTI systems will be of great importance in their analysis as you
will soon learn.
Table 2.1 Equivalences in
Mechanical and Electrical
Systems
Mechanical System
Electrical System
force f(t)
voltage v(t)
velocity w(t)
current i(t)
mass M
inductance L
damping D
resistance R
compliance K
capacitance C
FIGURE 2.7
Analog mechanical and electrical systems. Using
the equivalences R = D, L = M, v(t) = f(t), and
i(t) = w(t), the two systems are represented by
identical differential equations.
f(t)
M
D
w(t)
i(t)
R
L
v(t)
+
−

136
CHAPTER 2:
Continuous-Time Systems
If S is the transformation corresponding to an LTI system, so that the response of the system is
y(t) = S[x(t)] for an input x(t)
then we have that
S

X
k
Akx(t −τk)

=
X
k
AkS[x(t −τk)] =
X
k
Aky(t −τk)
S
Z
g(τ)x(t −τ)dτ

=
Z
g(τ)S[x(t −τ)]dτ =
Z
g(τ)y(t −τ)dτ
In the next section we will see that this property allows us to ﬁnd the response of a linear time-invariant
system due to any signal, if we know the response of the system to an impulse signal.
I Example 2.7
The response of an RL circuit to a unit-step source v(t) = u(t) is
i(t) = (1 −e−t)u(t)
Find the response to a source v(t) = u(t) −u(t −2).
Solution
Using superposition and time invariance, the output current due to the pulse v(t) = u(t) −u(t −2)
volts is
i(t) −i(t −2) = 2(1 −e−t)u(t) −2(1 −e(t−2))u(t −2)
Figure 2.8 shows the responses to u(t) and u(t −2) and the overall response to v(t) = u(t)
−u(t −2).
I
I Example 2.8
Suppose we know that the response to a rectangular pulse v1(t) is the current i1(t) shown in
Figure 2.9. If the input voltage is a train of two pulses, v(t), ﬁnd the corresponding current i(t).
Solution
Graphically the response to v(t) of the LTI system is given by i(t) as shown in Figure 2.9.
I
2.3.5 Convolution Integral
In this section we consider the computation of the output of a continuous-time linear time-invariant
(LTI) system due to any continuous-time input signal.

2.3 LTI Continuous-Time Systems
137
FIGURE 2.8
Response of an RL
circuit to a pulse
v(t) = u(t) −u(t −2)
using superposition and
time invariance.
i(t) , −i(t −2)
i(t) −i(t −2)
−1
0
1
4
5
6
7
−2
−1
0
1
2
3
2
i(t )
−i(t−2)
−1
0
1
2
4
3
5
6
7
0
0.5
1
1.5
2
t (sec)
FIGURE 2.9
Application of
superposition and
time invariance to
ﬁnd the response of
an LTI system.
0
1
0
1
1
1
2
2
3
2
1
1
0
−1
0.5
0.5
−0.5
0
t
t
t
t
v1(t)
i1(t)
−i1(t −1)
v(t)
i(t)
i1(t)
Recall that the generic representation of a signal x(t) in terms of shifted δ(t) signals found in Chapter 1
is given by
x(t) =
∞
Z
−∞
x(τ)δ(t −τ)dτ
(2.17)

138
CHAPTER 2:
Continuous-Time Systems
Next we deﬁne the impulse response of an LTI and ﬁnd the response due to x(t). The impulse response of
an analog LTI system, h(t), is the output of the system corresponding to an impulse δ(t) as input, and initial
conditions equal to zero.
If the input x(t) in Equation (2.17) is seen as an inﬁnite sum of weighted and shifted impulses
x(τ)δ(t −τ) then the output of an LTI system is the superposition of the responses to each of these
terms.
The response of an LTI system S represented by its impulse response h(t) = S[δ(t)] (i.e., the output of the
system to an impulse signal δ(t) and zero initial conditions) to any signal x(t) is the convolution integral
y(t) =
∞
Z
−∞
x(τ)h(t −τ)dτ =
∞
Z
−∞
x(t −τ)h(τ)dτ
= [x ∗h](t) = [h ∗x](t)
(2.18)
where the symbol ∗stands for the convolution integral of the input signal and the impulse response of the
system.
The above can be seen as follows:
I
Assuming no energy is initially stored in the system (i.e., initial conditions are zero) the response
to δ(t) is the impulse response h(t).
I
Given that the system is time invariant, the response to δ(t −τ) is h(t −τ) and by linearity the
response to x(τ)δ(t −τ) is x(τ)h(t −τ) since x(τ) is not a function of time t.
I
Thus, the response of the system to the generic representation Equation (2.17)
x(t) =
∞
Z
−∞
x(τ)δ(t −τ)dτ
is by superposition
y(t) =
∞
Z
−∞
x(τ)h(t −τ)dτ
or equivalently
y(t) =
∞
Z
−∞
x(t −σ)h(σ)dσ
after letting σ = t −τ. The two integrals are identical—each gives the response of the LTI system.
The impulse response h(t) represents the system. Notice that in the convolution integral the input
and the impulse response commute (i.e., are interchangeable).

2.3 LTI Continuous-Time Systems
139
Remarks
I
We will see that the impulse response is fundamental in the characterization of linear time-invariant
systems.
I
Any system characterized by the convolution integral is linear and time invariant by the above construction.
The convolution integral is a general representation of LTI systems, given that it was obtained from a
generic representation of the input signal.
I
We showed before that a system represented by a linear differential equation with constant coefﬁcients and
no initial conditions, or input, before t = 0 is LTI. Thus, one should be able to represent that system by a
convolution integral after ﬁnding its impulse response h(t).
I Example 2.9
Obtain the impulse response of a capacitor and use it to ﬁnd its unit-step response by means of
the convolution integral. Let C = 1 F.
Solution
For a capacitor with a initial voltage vc(0) = 0, we have that
vc(t) = 1
C
t
Z
0
i(τ)dτ
The impulse response of a capacitor is found by letting the input i(t) = δ(t) and the output vc(t) =
h(t), which according to the above equation becomes
h(t) = 1
C
t
Z
0
δ(τ)dτ = 1
C
t > 0
and zero if t < 0, or h(t) = (1/C)u(t). For C = 1F, to compute the unit-step response of the
capacitor we let the input i(t) = u(t), and vc(0) = 0. The voltage across the capacitor is
vc(t) =
∞
Z
−∞
h(t −τ)i(τ)dτ =
∞
Z
−∞
1
Cu(t −τ)u(τ)dτ
and since, as a function of τ, u(t −τ)u(τ) = 1 for 0 ≤τ ≤t and zero otherwise, we have that
vc(t) =
t
Z
0
dτ = t
for t ≥0 and zero otherwise (as the input is zero for t < 0 and there are no initial conditions),
or vc(t) = r(t). The above result makes physical sense since the capacitor is accumulating charge
and the input is providing a constant charge, so that the result is a ramp function. Notice that the
impulse response is the derivative of the unit-step response.

140
CHAPTER 2:
Continuous-Time Systems
The relation between the impulse response and the unit-step and the ramp responses can be gen-
eralized for any system as the impulse response h(t), the unit-step response s(t), and the ramp
response ρ(t) are related by
h(t) =
ds(t)/dt
d2ρ(t)/dt2
(2.19)
This can be shown by computing ﬁrst s(t) (the output due to a unit-step input):
s(t) =
∞
Z
−∞
u(t −τ)h(τ)dτ =
t
Z
−∞
h(τ)dτ
since
u(t −τ) =
1
τ ≤t
0
τ > t
The derivative of s(t) is h(t).
Similarly, the ramp response ρ(t) of a LTI system, represented by the impulse response h(t), is
given by
ρ(t) =
∞
Z
−∞
h(τ)(t −τ)u(t −τ)dτ =
t
Z
−∞
h(τ)(t −τ)dτ = t
t
Z
−∞
h(τ)dτ −
t
Z
−∞
h(τ)τdτ
and its derivative is
dρ(t)
dt
=
t
Z
−∞
h(τ)dτ + th(t)
|
{z
}
d(t
tR
−∞
h(τ)dτ)/dt
−
th(t)
|{z}
d(
tR
−∞
h(τ)τdτ)/dt
=
t
Z
−∞
h(τ)dτ
so that the second derivative of ρ(t) is h(t)—that is,
d2ρ(t)
dt2
= d
dt


t
Z
−∞
h(τ)dτ

= h(t)
Using the Laplace transform, one is able to obtain the above relations in a much simpler way.
I
I Example 2.10
The output y(t) of an analog averager is given by
y(t) = 1
T
t
Z
t−T
x(τ)dτ

2.3 LTI Continuous-Time Systems
141
which corresponds to the accumulation of values of x(t) in a segment [t −T, t] divided by its
length T, or the average of x(t) in [t −T, t]. Use the convolution integral to ﬁnd the response of the
averager to a ramp.
Solution
To ﬁnd the ramp response using the convolution integral we ﬁrst need h(t). The impulse response
of an averager can be found by letting x(t) = δ(t) and y(t) = h(t) or
h(t) = 1
T
t
Z
t−T
δ(τ)dτ
If t < 0 or if t −T > 0 this integral is zero as in these two situations t = 0, where the delta function
occurs, is not included in the integral limits. However, when t −T < 0 and t > 0, or 0 < t < T, the
integral is 1 as the origin t = 0, where δ(t) occurs, is included in this interval. Thus, the impulse
response of the analog averager is
h(t) =
 1
T
0 < t < T
0
otherwise
We then have that the output y(t), for a given input x(t), is given by the convolution integral
y(t) =
∞
Z
−∞
h(τ)x(t −τ)dτ =
T
Z
0
1
T x(t −τ)dτ
which can be shown to equal the deﬁnition of the averager by a change of variable. Indeed, let
σ = t −τ, so when τ = 0 then σ = t, and when τ = T then σ = t −T. Moreover, we have that
dσ = −dτ. The above integral becomes
y(t) = −1
T
t−T
Z
t
x(σ)dσ = 1
T
t
Z
t−T
x(σ)dσ
Thus, we have that
y(t) = 1
T
t
Z
0
x(t −τ)dτ = 1
T
t
Z
t−T
x(σ)dσ
(2.20)
If the input is a ramp, x(t) = tu(t), the ramp response ρ(t) is
ρ(t) = 1
T
t
Z
t−T
x(σ)dσ = 1
T
t
Z
t−T
σu(σ)dσ

142
CHAPTER 2:
Continuous-Time Systems
If t −T < 0 and t ≥0, the above integral becomes
ρ(t) = 1
T
t
Z
0
σdσ = t2
2T
0 ≤t < T
but if t −T ≥0, we would then get
ρ(t) = 1
T
t
Z
t−T
σdσ = t2 −(t −T)2
2T
= t −T
2
t ≥T
So that the ramp response is
ρ(t) =



0
t < 0
t2/(2T)
0 ≤t < T
t −T/2
t ≥T
Notice that the second derivative of ρ(t) is
d2ρ(t)
dt2
=
1/T
0 ≤t < T
0
otherwise
which is the impulse response of the averager as found before.
I
I Example 2.11
Find the convolution integral yT(t) of a pulse x(t) = u(t) −u(t −T0) with a sampling signal
δT(t) =
∞
X
k=−∞
δ(t −kT)
Consider T = T0 and T = 2T0. Find and plot the corresponding yT(t).
Solution
For any value of T the convolution integral is given by
yT(t) =
∞
Z
−∞
δT(τ)x(t −τ)dτ =
∞
Z
−∞
∞
X
k=−∞
δ(τ −kT)x(t −τ)dτ
=
∞
X
k=−∞
∞
Z
−∞
δ(τ −kT)x(t −τ)dτ =
∞
X
k=−∞
x(t −kT)
∞
Z
−∞
δ(τ −kT)dτ
=
∞
X
k=−∞
x(t −kT)

2.3 LTI Continuous-Time Systems
143
where we used the sifting property of the impulse and that its area is unity. If we let T = T0 and
let the unit step be u(0) = 0.5, the signal yT0(t) = 1 for −∞< t < ∞. When T = 2T0, the signal
y2T0(t) is a periodic train of rectangular pulses of period 2T0. See Figure 2.10.
y2T0(t)
−T0
T0
2T0
3T0
t
−T0
T0
2T0
3T0
t
yT0(t)
1
1
FIGURE 2.10
Convolution with a sequence of unit impulses as input. Notice the result is the superposition of the input signal
shifted by the time-shift kT of the impulses. For T = T0, yT(t) = 1 and for T = 2T0 is a sequence of pulses.
I
2.3.6 Causality
Causality relates to the conditions under which processing of a signal can be performed in real time—
when it is necessary to process the signal as it comes into the system. For real-time processing the
system needs to be causal. In many situations the data can be stored and processed without the
requirements of real-time processing; under such circumstances causality is not necessary.
A continuous-time system S is called causal if:
I
Whenever the input x(t) = 0 and there are no initial conditions, the output is y(t) = 0.
I
The output y(t) does not depend on future inputs.
For a value τ > 0, when considering causality it is helpful to think of
I
The time t (the time at which the output y(t) is being computed) as the present.
I
Times t −τ as the past.
I
Times t + τ as the future.
Remarks
Causality is independent of the linearity and the time-invariance properties of a system. For instance, the
system represented by the input–output equation
y(t) = x2(t)
where x(t) is the input and y(t) the output, is nonlinear but time invariant, and according to the above
deﬁnition is a causal system. Likewise, an LTI system can be noncausal. Consider the following averager:
y(t) = 1
2T
t+T
Z
t−T
x(τ)dτ

144
CHAPTER 2:
Continuous-Time Systems
which can be written as
y(t) = 1
2T
t
Z
t−T
x(τ)dτ + 1
2T
t+T
Z
t
x(τ)dτ
At the present time t, y(t) consists of the average of a past and present values in [t −T, t] of the input, and
of the average of future values of the signal (i.e., the average of values x(t) for [t, t + T]). Thus, this system is
not causal.
An LTI system represented by its impulse response h(t) is causal if
h(t) = 0
for t < 0
(2.21)
The output of a causal LTI system with a causal input x(t) (i.e., x(t) = 0 for t < 0) is
y(t) =
t
Z
0
x(τ)h(t −τ)dτ
(2.22)
One can understand the above results by considering the following:
I
The choice of the starting time as t = 0 is for convenience. It is purely arbitrary as the system being
considered is time invariant, so that similar results are obtained for any other starting time.
I
When computing the impulse response h(t), the input δ(t) only occurs at t = 0 and there are no
initial conditions. Thus, h(t) should be zero for t < 0 since for t < 0 there is no input and there
are no initial conditions.
I
A causal LTI system is represented by the convolution integral
y(t) =
∞
Z
−∞
x(τ)h(t −τ)dτ
=
t
Z
−∞
x(τ)h(t −τ)dτ +
∞
Z
t
x(τ)h(t −τ)dτ
where the second integral is zero according to the causality of the system (h(t −τ) = 0 when τ > t
since the argument of h(.) becomes negative). Thus, we obtain
y(t) =
t
Z
−∞
x(τ)h(t −τ)dτ

2.3 LTI Continuous-Time Systems
145
I
If the input signal x(t) is causal (i.e., x(t) = 0 for t < 0), we can simplify further the above
equation. Indeed
y(t) =
t
Z
0
x(τ)h(t −τ)dτ
where the lower limit of the integral is set by the causality of the input signal, and the upper
limit is set by the causality of the system. This equation clearly indicates that the system is causal,
as the output y(t) depends on present and past values of the input (considering the integral an
inﬁnite sum, the integrand depends continuously on x(τ), from τ = 0 to τ = t, which are past
and present input values). Also if x(t) = 0 the output is also zero.
2.3.7 Graphical Computation of Convolution Integral
Graphically, the computation of the convolution integral, Equation (2.18), consists in multiplying
x(τ) (as a function of τ) by a reﬂected (again as function of τ) and shifted to the right t sec impulse
response h(t −τ). Once this product is obtained we integrate it from 0 to t (the time at which we
are computing the convolution). The computational cost of this operation is rather high considering
that these operations need to be done for each value of t for which we are interested in ﬁnding the
output y(t). A more efﬁcient way will be by using the Laplace transform as we will see in the next
chapter.
I Example 2.12
Graphically ﬁnd the unit-step y(t) response of an averager, with T = 1 sec, which has an impulse
response
h(t) = u(t) −u(t −1)
Solution
Plotting the input signal x(τ) = u(τ) and the reﬂected and delayed impulse response h(t −τ), both
as functions of τ, for some value of t (notice that when t = 0, h(−τ) is the reﬂected version of
the impulse response, and for t > 0, h(t −τ) is h(−τ) shifted by t to the right) are as shown in
Figure 2.11. Notice the position of h(t −τ) with respect to x(τ) as it moves from left to right as t
goes from −∞to ∞.
We then have the following results for different values of t:
I
If t < 0, then h(t −τ) and x(τ) do not overlap and so the convolution integral is zero, or
y(t) = 0 for t < 0. That is, the system for t < 0 has not yet been affected by the input.
I
For t ≥0 and t −1 < 0, or equivalently 0 ≤t < 1, h(t −τ) and x(τ) increasingly overlap, and
as such the integral increases linearly from 0 at t = 0 to 1 when t = 1. So that y(t) = t for
0 ≤t < 1. That is, for this period of time the system starts reacting slowly to the input.

146
CHAPTER 2:
Continuous-Time Systems
FIGURE 2.11
Graphical convolution for a
unit-step input into an
averager with T = 1.
h(t −τ )
τ
0
1
t
t −1
y(t )
t
0
1
1
x(τ )
τ
0
1
I
For t ≥1, the overlap of h(t −τ) and x(τ) remains constant, and as such the integral is unity
from then on, or y(t) = 1 for t ≥1. The response for t ≥1 has attained steady state. Thus, the
complete response is given as
y(t) = r(t) −r(t −1)
where r(t) = tu(t), the ramp function.
I
I Example 2.13
Consider the graphical computation of the convolution integral of two pulses of the same duration
(see Figure 2.12).
FIGURE 2.12
Graphical convolution of two equal
pulses—that is, a system with input
x(t) = u(t) −u(t −1) and impulse
response h(t) = x(t).
y(t)
t
0
1
2
1
h(t−τ )
τ
0
1
t
t −1
x(τ )
τ
0
1
1

2.3 LTI Continuous-Time Systems
147
Solution
In this case, x(t) = h(t) = u(t) −u(t −1). Again we plot x(τ) and h(t −τ) both as functions of τ,
for −∞< t < ∞.
I
It should be noticed that while computing the convolution integral for t increasing from nega-
tive to positive values, h(t −τ) moves from left to right while x(τ) remains stationary, and that
they only overlap on a ﬁnite support.
I
For t < 0, h(t −τ) and x(τ) do not overlap, so y(t) = 0 for t < 0.
I
h(t −τ) and x(τ) increasingly overlap for 0 ≤t < 1 and decreasingly overlap for 1 ≤t < 2. So
that y(t) = t for 0 ≤t < 1, and y(t) = 2 −t for 1 ≤t < 2.
I
For t > 2, there is no overlap and so y(t) = 0 for t > 2.
Thus, the complete response is
y(t) = r(t) −2r(t −1) + r(t −2)
where r(t) = tu(t) is the ramp signal.
Notice in this example that:
I
The result of the convolution of these two pulses, y(t), is smoother than x(t) and h(t). This is
because y(t) is the continuous average of x(t), as h(t) is the impulse response of the averager in
example 2.12.
I
The length of the support of y(t) equals the sum of the lengths of the supports of x(t) and h(t).
This is a general result that applies to any two signals x(t) and h(t).
I
The length of the support of y(t) = [x ∗h](t) is equal to the sum of the lengths of the supports of x(t) and h(t).
2.3.8 Interconnection of Systems—Block Diagrams
Systems can be considered a connection of subsystems. In the case of LTI systems, to visualize the
interaction of the different subsystems each of the subsystems is represented by a block with the
corresponding impulse response, or equivalently by its Laplace transform as we will see in the next
chapter. The ﬂow of the signals is indicated by arrows, and the addition of signals or multiplication
of a signal by a constant is indicated by means of circles.
Two possible connections, the cascade and the parallel connections, result from the properties of
the convolution integral, while the feedback connection is found in many natural systems and has
been replicated in engineering, especially in control. The concept of feedback is one of the greatest
achievements of the 20th century. See Figure 2.13.
Cascade Connection
When connecting LTI systems in cascade the impulse response of the overall system can be found
using the convolution integral.

148
CHAPTER 2:
Continuous-Time Systems
FIGURE 2.13
Block diagrams for connecting two LTI systems
with impulse responses h1(t) and h2(t) in (a)
cascade, (b) parallel, and (c) negative
feedback.
h1(t)
h2(t)
y(t)
x(t)
+
(b)
h1(t)
h2(t)
y(t)
x(t)
e(t)
−
+
(c)
h1(t)
h2(t)
y(t)
x(t)
(a)
Two LTI systems with impulse responses h1(t) and h2(t) connected in cascade have as an overall impulse
response
h(t) = [h1 ∗h2](t) = [h2 ∗h1](t)
where h1(t) and h2(t) commute (i.e., they can be interchanged).
In fact, if the input to the cascade connection is x(t), the output y(t) is found as
y(t) = [[x ∗h1] ∗h2](t)
= [x ∗[h1 ∗h2]](t)
= [x ∗[h2 ∗h1]](t)
where the last two equations show the commutative property of convolution. The impulse response of
the cascade connection indicates that the order in which we connect LTI systems is not important—
that we can put the system with impulse response h1(t) ﬁrst, or the system with impulse response h2(t)
ﬁrst with no effect in the overall response of the system (we will see later that this is true provided
that the two systems do not load each other). When dealing with linear but time-varying systems,
however, the order in which we connect the systems in cascade is important.

2.3 LTI Continuous-Time Systems
149
Parallel Connection
If we connect in parallel two LTI systems with impulse responses h1(t) and h2(t), the impulse response of the
overall system is
h(t) = h1(t) + h2(t)
In fact, the output of the parallel combination is
y(t) = [x ∗h1](t) + [x ∗h2](t)
= [x ∗(h1 + h2)](t)
which is the distributive property of convolution.
Feedback Connection
In these connections the output of the system is fed back and compared with the input of the system.
The fedback output is either added to the input giving a positive feedback system or subtracted from the
input giving a negative feedback system. In most cases, especially in control systems, negative feedback
is used. Figure 2.13(c) illustrates the negative feedback connection.
Given two LTI systems with impulse responses h1(t) and h2(t), a negative feedback connection (Figure 2.13(c))
is such that the output is
y(t) = [h1 ∗e](t)
where the error signal is
e(t) = x(t) −[y ∗h2](t)
The overall impulse response h(t), or the impulse response of the closed-loop system, is given by the implicit
expression
h(t) = [h1 −h ∗h1 ∗h2](t)
If h2(t) = 0 (i.e., there is no feedback) the system is called an open-loop system and h(t) = h1(t).
Using the Laplace transform we will obtain later an explicit expression for the Laplace transform of
h(t). To obtain the above result we consider the output of the system as the overall impulse response
y(t) = h(t) due to an input x(t) = δ(t). Then e(t) = δ(t) −[h ∗h2](t), and so when replaced in the
expression for the output
h(t) = [e ∗h1](t) = [(δ −h ∗h2) ∗h1](t) = [h1 −h ∗h1 ∗h2](t)
the implicit expression is as given above. When there is no feedback, h2(t) = 0, then h(t) = h1(t).

150
CHAPTER 2:
Continuous-Time Systems
I Example 2.14
Consider the block diagram in Figure 2.14 with input a unit-step signal, u(t). The averager is such
that for an input x(t) its output is
y(t) = 1
T
t
Z
t−T
x(τ)dτ
Determine what the system is doing as we let the delay 1 →0. Consider that the averager and the
system with input u(t) and output x(t) are LTI.
FIGURE 2.14
Block diagram of the cascading of two LTI
systems, one of them being an averager.
×
+
−
u(t)
Delay Δ
Averager
y(t)
x(t)
1
Δ
Solution
Since it is not clear from the given block diagram what the system is doing, using the LTI of the two
systems connected in cascade lets us reverse their order so that the averager is ﬁrst (see Figure 2.15),
obtaining an equivalent block diagram.
FIGURE 2.15
Equivalent block diagram of the cascading of two
LTI systems, one of them being an averager.
×
+
−
u(t)
s(t)
Delay Δ
Averager
y(t)
1
Δ
The output of the averager is
s(t) = 1
T
t
Z
t−T
u(τ)dτ =



0
t < 0
t/T
0 ≤t < T
1
t ≥T
as we obtained before in example 2.12. The output y(t) of the other system is given by
y(t) = 1
1 [s(t) −s(t −1)]

2.3 LTI Continuous-Time Systems
151
If we then let 1 →0 we have that (recall that ds(t)/dt = h(t) is the relation between the unit-step
response s(t) and the impulse response h(t))
lim
1→0 y(t) = ds(t)
dt
= h(t) = 1
T [u(t) −u(t −T)]
That is, this system approximates the impulse response of the averager.
I
I Example 2.15
Consider the circuits obtained with an operational ampliﬁer when we feed back its output with a
wire, a resistor, and a capacitor (Figure 2.16). Assume the linear model for the op-amp. The circuits
in Figure 2.16 are called a voltage follower, an integrator, and an adder.
Solution
Virtual follower circuit. Although the operational ampliﬁer can be made linear, its large gain
A makes it not useful. Feedback is needed to make the op-amp useful. The voltage follower circuit
(Figure 2.16(a)), which is used to isolate cascaded circuits, is a good example of a feedback system.
Given that the voltage differential is assumed to be zero, then v−(t) = vi(t), and therefore the
output voltage is
vo(t) = vi(t)
FIGURE 2.16
Operational ampliﬁer circuits:
(a) virtual follower, (b) inverting
integrator, and (c) adder with
inversion.
+
+
−
−
−
−
+
−
vi(t)
vo(t)=vi(t)
(a)
R
C
+
+
−
vi(t)
+
−
vo(t)
(b)
(c)
R
+
+
−
+
−
+
−
vo(t)
v1(t)
v2(t)
R1
R2

152
CHAPTER 2:
Continuous-Time Systems
The input resistance of this circuit is Rin = ∞and the output resistance is Rout = 0 so that the
output behaves as an ideal voltage source. The voltage follower is used to isolate two circuits con-
nected in cascade, as the connected circuit at either the input or the output port does not draw any
current from the ﬁrst—that is, it does not load the other circuit. This is because of the inﬁnite input
resistance, or the behavior of the circuit as a voltage source (Rout = 0). This circuit is very useful in
the implementation of analog ﬁlters.
Inverting integrator circuit. If we let the feedback element be a capacitor, we obtain the follow-
ing equation from the virtual short equations. The current through the resistor R is vi(t)/R given
that v−(t) = 0 and it is the current through the capacitor as no current enters the negative terminal.
Therefore, the output voltage is
vo(t) = −vc(t) = −1
C
t
Z
0
vi(τ)
R
dτ −vc(0)
where vc(0) is the voltage across the capacitor at t = 0, when the voltage source is turned on. If we
let vc(0) = 0 and RC = 1 the above equation is the negative of the integral of the voltage source.
Thus, we have a circuit that realizes an integrator with a sign inversion. Again this circuit will be
very useful in the implementation of analog ﬁlters.
Adder circuit. Since the circuit components are linear, the circuit is linear and we can use super-
position. Letting v2(t) = 0 the output voltage due to it is zero, and the output voltage due to v1(t) is
vo1(t) = −v1(t)R/R1. Similarly, if we let v1(t) = 0, its corresponding output is zero, and the output
due to v2(t) is vo2(t) = −v2(t)R/R2, so that when both v1(t) and v2(t) are considered the output is
vo(t) = vo1(t) + vo2(t) = −v1(t) R
R1
−v2(t) R
R2
Using this circuit:
1.
When R1 = R2 = R, we have an adder with a sign inversion:
vo(t) = −[v1(t) + v2(t)]
2.
When R2 →∞and R1 = R, we get an inverter of the input
vo(t) = −v1(t),
3.
When R2 →∞and R1 = αR, we get a constant multiplier with sign inversion:
vo(t) = −1
α v1(t)
i.e., the inverted input with a gain 1/α.
The above three circuits illustrate the realization of a buffer, an integrator, and an adder that
can be used to realize analog ﬁlters.
I

2.3 LTI Continuous-Time Systems
153
2.3.9 Bounded-Input Bounded-Output Stability
Stability characterizes useful systems. A stable system is such that well-behaved outputs are obtained
for well-behaved inputs. Of the many possible deﬁnitions of stability, we consider here bounded-
input bounded-output (BIBO) stability.
Bounded-input bounded-output (BIBO) stability establishes that for a bounded (i.e., well-behaved) input x(t)
the output of a BIBO stable system y(t) is also bounded. This means that if there is a ﬁnite bound M < ∞such
that |x(t)| < M (you can think of it as an envelope [−M, M] inside which the input is in) the output is also
bounded.
An LTI system with an absolutely integrable impulse response—that is,
∞
Z
−∞
|h(t)|dt < ∞
(2.23)
is BIBO stable. A simpler way, using the Laplace transform, to test the BIBO stability of a system is given later.
For a bounded input, the output y(t) of an LTI system is represented by a convolution integral that is
bounded as follows:
|y(t)| =

∞
Z
−∞
x(t −τ)h(τ)dτ

≤
∞
Z
−∞
|x(t −τ)||h(τ)|dτ
≤M
∞
Z
−∞
|h(τ)|dτ
≤ML < ∞
where L is the bound for
R ∞
−∞|h(τ)|dτ, or equivalently the impulse response is absolutely integrable.
I Example 2.16
Consider the BIBO stability and causality of RLC circuits. Consider, for instance, a series RL circuit
where R = 1 and L = 1 H, and a voltage source vs(t), which is bounded. Discuss why such a
system would be causal and stable.
Solution
RLC circuits are naturally stable. As you know, inductors and capacitors simply store energy and
so LC circuits simply exchange energy between these elements. Resistors consume energy, which

154
CHAPTER 2:
Continuous-Time Systems
is transformed into heat, and so RLC circuits spend the energy given to them. This characteristic
is called passivity, indicating that RLC circuits can only use energy, not generate it. Clearly, RLC
circuits are also causal systems as one would not expect them to provide any output before they
are activated.
According to Kirchhoff’s voltage law, the RL circuit is represented by a ﬁrst-order differential
equation
vs(t) = i(t)R + Ldi(t)
dt
= i(t) + di(t)
dt
To ﬁnd its impulse response we would need to solve this equation with input vs(t) = δ(t) and zero
initial condition, i(0) = 0. In the next chapter, the Laplace domain will provide us an algebraic way
to solve the differential equation and will conﬁrm our intuitive solution given here. Intuitively, in
response to a large and sudden impulse vs(t) = δ(t), the inductor tries to follow it by instanta-
neously increasing its current. But as time goes by and the input is not providing any additional
energy, the current in the inductor goes to zero. Thus, we conjecture that the current in the inductor
is i(t) = h(t) = e−tu(t) when vs(t) = δ(t) and initial conditions are zero, i(0) = 0. It is possible to
conﬁrm that is the case. Replacing vs(t) = δ(t) and i(t) = e−tu(t) in the differential equation, we get
δ(t)
|{z}
vs(t)
= e−tu(t)
| {z }
i(t)
+ [e−tδ(t) −e−tu(t)]
|
{z
}
di(t)/dt
= e0δ(t) = δ(t)
which is an identity, conﬁrming that indeed our conjectured solution is the solution of the dif-
ferential equation. The initial condition is also satisﬁed by remembering that there is no initial
current at the source—that is, δ(t) is zero right before we close the switch—and that physically the
inductor remains at that point for an extremely short time before reacting to the strong input.
Thus, the RL circuit where R = 1  and L = 1 H has an impulse response of
h(t) = e−tu(t)
indicating that it is causal since h(t) = 0 for t < 0; that is, the circuit output is zero given that the
initial conditions are zero, and that the input δ(t) is also zero before 0. We can also show that the
RL circuit is stable. In fact,
∞
Z
−∞
|h(t)|dt =
∞
Z
0
e−tdt = 1
I
I Example 2.17
Consider the causality and BIBO stability of an echo system (or a multipath system). See
Figure 2.17. Let the output y(t) be given by
y(t) = α1x(t −τ1) + α2x(t −τ2)

2.3 LTI Continuous-Time Systems
155
FIGURE 2.17
Echo system with two paths.
×
×
+
x(t)
Delay τ 1
Delay τ 2
α 2
α 1
y(t)
where x(t) is the input, and αi, τi > 0, for i = 1 and 2, are attenuation factors and delays. Thus,
the output is the superposition of attenuated and delayed versions of the input. Typically, the
attenuation factors are less than unity. Is this system causal and BIBO stable?
Solution
Since the output depends only on past values of the input, the echo system is causal. To determine
if the system is BIBO stable we consider a bounded input signal x(t), and determine if the output
is bounded. Suppose x(t) is bounded by a ﬁnite value M, or |x(t)| < M < ∞, for all times, which
means that the value of x(t) cannot exceed an envelope [−M, M] at all times. This would also hold
when we shift x(t) in time, so that
|y(t)| ≤|α1||x(t −τ1)| + |α2||x(t −τ2)| < [|α1| + |α2|]M
so the corresponding output is bounded. The system is BIBO stable.
We can also ﬁnd the impulse response h(t) of the echo system, and show that it satisﬁes the abso-
lutely integrable condition of BIBO stability. Indeed, if we let the input of the echo system be
x(t) = δ(t) the output is
y(t) = h(t) = α1δ(t −τ1) + α2δ(t −τ2)
and the integral is
∞
Z
−∞
|h(t)|dt = |α1|
∞
Z
−∞
δ(t −τ1)dt + |α2|
∞
Z
−∞
δ(t −τ2)dt = |α1| + |α2| < ∞
I
I Example 2.18
Consider a positive feedback system created by a microphone close to a set of speakers that are
putting out an ampliﬁed acoustic signal (see Figure 2.18). The microphone picks up the input
signal x(t) as well as the ampliﬁed and delayed signal βy(t −τ), |β| ≥1. Find the equation that
connects the input x(t) and the output y(t) and recursively from it obtain an expression for y(t) in
terms of past values of the input. Determine if the system is BIBO stable or not—use x(t) = u(t),
β = 2, and τ = 1 in doing so.

156
CHAPTER 2:
Continuous-Time Systems
FIGURE 2.18
Positive feedback system: the microphone picks
up input signal x(t) and the ampliﬁed and
delayed signal βy(t −τ), making the system
unstable.
+
×
x(t)
βy (t−τ )
Delay τ
y (t)
β
Solution
The input–output equation is
y(t) = x(t) + βy(t −τ)
If we use this expression to obtain y(t −τ), we get that
y(t −τ) = x(t −τ) + βy(t −2τ)
and replacing it in the input–output equation, we get
y(t) = x(t) + β[x(t −τ) + βy(t −2τ)] = x(t) + βx(t −τ) + β2y(t −2τ)
Repeating the above scheme, we will obtain the following expression for y(t) in terms of the input
y(t) = x(t) + βx(t −τ) + β2x(t −2τ) + β3x(t −3τ) + · · ·
If we let x(t) = u(t) and β = 2, the corresponding output is
y(t) = u(t) + 2u(t −1) + 4u(t −2) + 8u(t −3) + · · ·
which continuously grows as time increases. The output is clearly not a bounded signal, although
the input is bounded. Thus, the system is unstable, and the screeching sound from the speakers
will prove it—you need to separate the speakers and the microphone to avoid it.
I
2.4 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO FROM
HERE?
By now you should have begun to see the forest for the trees. In this chapter we connected signals
with systems. Especially, we initiated the study of linear time-invariant dynamic systems. As you will
learn throughout your studies, this model is of great use in representing systems in many engineering
applications. The appeal is its simplicity and mathematical structure. We also indicated some practi-
cal properties of systems such as causality and stability. Simple yet signiﬁcant examples of systems,
ranging from the vocal system to simple RLC circuits, illustrate the use of the LTI model and point
to its practical application. At the same time, modulators also show that more complicated systems
need to be explored to be able to communicate wirelessly. Finally, you were given a system’s approach

Problems
157
to the theory of differential equations and shown some features that will come back when we apply
transforms.
Our next step is to do the analysis of systems with continuous-time signals by means of transforms. In
Chapter 3 we discuss the Laplace transform that allows transient as well as steady-state analysis and
that will convert the solution of differential equations into an algebraic problem. More important,
it will provide the concept of transfer function that connects with the impulse response and the
convolution integral covered in this chapter. The Laplace transform is very signiﬁcant in the area of
classic control.
PROBLEMS
2.1. Temperature measuring system—MATLAB
The op-amp circuit shown in Figure 2.19 is used to measure the changes of temperature in a system. The
output voltage is given by
vo(t) = −R(t)vi(t)
Suppose that the temperature in the system changes cyclically after t = 0, so that
R(t) = [1 + 0.5 cos(20πt)] u(t)
Let the input be vi(t) = 1 volt.
FIGURE 2.19
Problem 2.1.
−
+
−
+
+
−
1 Ω
Thermistor R (t)
vi(t)
t 0
vo(t)
(a) Assuming that the switch closes at t0 = 0 sec, use MATLAB to plot the output voltage vo(t) for 0 ≤
t ≤0.2 sec in time intervals of 0.01 sec.
(b) If the switch closes at t0 = 50 msec, plot the output voltage v0(t) for 0 ≤t ≤0.2 sec in time intervals
of 0.01 sec.
(c) Use the above results to determine if this system is time invariant. Explain.
2.2. Zener diode—MATLAB
A zener diode circuit is such that the output corresponding to an input vs(t) = cos(πt) is a “clipped”
sinusoid
x(t) =
 0.5
|vs(t)| > 0.5
vs(t)
otherwise
as shown in Figure 2.20 for a few periods. Use MATLAB to generate the input and the output signals and
plot them in the same plot for 0 ≤t ≤4 at time intervals of 0.001.
(a) Is this system linear? Compare the output obtained from vs(t) with that obtained from 0.3vs(t).
(b) Is the system time invariant? Explain.

158
CHAPTER 2:
Continuous-Time Systems
FIGURE 2.20
Problem 2.2.
−4
−3
−2
−1
0
1
2
3
−1
−0.5
0
0.5
1
t (sec)
x (t )
2.3. Analog averaging system
Consider the analog averager
y(t) = 1
T
t+T/2
Z
t−T/2
x(τ)dτ
where x(t) is the input and y(t) is the output.
(a) Find the impulse response h(t) of the averager. Is this system causal?
(b) Let x(t) = u(t). Find the output of the averager.
2.4. LTI determination from input–output relation
An analog system has the input–output relation
y(t) =
t
Z
0
e−(t−τ)x(τ)dτ
t ≥0
and zero otherwise. The input is x(t) and y(t) is the output.
(a) Is this a linear time-invariant system? If so, can you determine without any computation the impulse
response of the system? Explain.
(b) Is this system causal? Explain.
(c) Find the unit-step response s(t) and from it ﬁnd the impulse response h(t). Is this a BIBO-stable
system? Explain.
(d) Find the response due to a pulse x(t) = u(t) −u(t −1).
2.5. p-n diode—MATLAB
The voltage–current characterization of a p-n diode is given by (see Figure 2.21)
i(t) = Is(eqv(t)/kT −1)
where i(t) and v(t) are the current and the voltage in the diode (in the direction indicated in the diode), Is
is the reversed saturation current, and kT/q is a constant.

Problems
159
FIGURE 2.21
Problem 2.5: p-n diode and i-v characteristic.
−
+
0
i (t)
i(t)
Is
v(t)
v (t)
(a) Consider the voltage v(t) as the input and the current i(t) as the output of the diode. Is the p-n diode a
linear system? Explain.
(b) An ideal diode is such that when the voltage is negative, v(t) < 0, the current is zero (i.e., open circuit),
and when the current is positive, i(t) > 0, the voltage is zero or short circuit. Under what conditions
does the p-n diode voltage–current characterization approximate the characterization of the ideal
diode? Use MATLAB to plot the current–voltage plot for a diode with Is = 0.0001 and kT/1 = 0.026,
and compare it to the ideal diode current–voltage plot. Determine if the ideal diode is linear.
(c) Consider the circuit using an ideal diode in Figure 2.22, where the source is a sinusoid signal v(t) =
sin(2πt)u(t) and the output is the voltage in the resistor R = 1  or vR(t). Plot vR(t). Is this system
linear? Where would you use this circuit?
FIGURE 2.22
Problem 2.5: ideal diode circuit.
−
+
R
Ideal diode
v(t)
i(t)
2.6. Capacitor/inductor circuit
Consider the circuit in Figure 2.23, where the value of the capacitor is C = 1 F, and the initial condition
is vc(0) = −1 volts. Assume the input is the current source is(t) and the voltage in the capacitor vc(t) the
output.
(a) Let the current in the circuit be is(t) = u(t) −u(t −1), and the initial voltage in the capacitor be vc(0) =
−1 volts. Plot the voltage in the capacitor vc(t) for all times. Suppose then we double the current,
is(t) = 2(u(t) −u(t −1)), but keep the same initial condition. Plot the voltage in the capacitor vc(t) for
all times, and compare it with the one obtained before. Is the capacitor with non-zero initial conditions
a linear system? Explain.
(b) Consider the dual circuit where the value of the inductor is L = 1 H, the initial current in the inductor
is iL(0) = −1 amps, and the input is the source vs(t). Let vs(t) = u(t) −u(t −1). Plot the corresponding
current iL(t) for all times. Double the voltage source vs(t) = 2[u(t) −u(t −1)], and plot the correspond-
ing current in the inductor iL(t) for all times. Compare the two currents and determine if the inductor
with the initial current is linear. What if iL(0) = 0?

160
CHAPTER 2:
Continuous-Time Systems
FIGURE 2.23
Problem 2.6.
1F
+
−
+
−
is(t)
vs(t)
1H
iL(t)
2.7. Time-varying capacitor
A time-varying capacitor is characterized by the charge–voltage equation
q(t) = C(t)v(t)
That is, the capacitance is not a constant but a function of time.
(a) Given that i(t) = dq(t)/dt, ﬁnd the voltage–current relation for this time-varying capacitor.
(b) Let C(t) = 1 + cos(2πt) and v(t) = cos(2πt). Determine the current i1(t) in the capacitor for all t.
(c) Let C(t) be as above, but delay v(t) by 0.25 sec. Determine i2(t) for all time. Is the system TI?
2.8. Sinusoidal Test for LTI
A fundamental property of linear time-invariant systems is that whenever the input of the system is a sinu-
soid of a certain frequency, the output will also be a sinusoid of the same frequency but with an amplitude
and phase determined by the system. For the following systems let the input be x(t) = cos(t), −∞< t <
∞, and ﬁnd the output y(t) and determine if the system is LTI.
(a) y(t) = |x(t)|2
(b) y(t) = 0.5[x(t) + x(t −1)]
(c) y(t) = x(t)u(t)
(d) y(t) = 1
2
t
Z
t−2
x(τ)dτ
2.9. Testing the time invariance of systems
Consider the following systems and ﬁnd the response to x1(t) = u(t) and x2(t) = u(t −1). Determine from
the corresponding outputs whether the system is time-varying or not.
(a) y(t) = x(t) cos(πt)
(b) y(t) = x(t)[u(t) −u(t −2)]
(c) y(t) = 0.5[x(t) + x(t −1)]
Plot y1(t) and y2(t) for each case.
2.10. Window/modulator
Consider the system where for an input x(t) the output is y(t) = x(t)f(t) for some function f(t).
(a) Let f(t) = u(t) −u(t −10). Determine whether the system with input x(t) and output y(t) is linear, time
invariant, causal, and BIBO stable.
(b) Suppose x(t) = 4 cos(πt/2), and f(t) = cos(6πt/7) and both are periodic. Is the output y(t) also
periodic? What frequencies are present in the output? Is this system linear? Is it time invariant?
Explain.

Problems
161
(c) Let f(t) = u(t) −u(t −2) and the input x(t) = u(t). Find the corresponding output y(t). Suppose you
shift the input so that it is x1(t) = x(t −3). What is the corresponding output y1(t). Is the system time
invariant? Explain.
2.11. Initial conditions, LTI, steady state, and stability
The input–output characterization of a system is
y(t) = e−2ty(0) + 2
t
Z
0
e−2(t−τ)x(τ)dτ
t ≥0
and zero otherwise. In the above equation x(t) is the input and y(t) is the output.
(a) Is this system LTI? Is it possible to determine a value for y(0) that would make this an LTI system?
Explain.
(b) Find the differential equation that also characterizes this system.
(c) Suppose for x(t) = u(t) and any value of y(0), we wish to determine the steady-state response of the
system. Is the value of y(0) of any signiﬁcance—that is, do we get the same steady-state response if
y(0) = 0 or y(0) = 1? Explain.
(d) Compute the steady-state response when y(0) = 0 and x(t) = u(t) using the convolution integral. To
do so, ﬁrst ﬁnd the impulse response of the system h(t). Then relate the integral in the equation given
above with the convolution integral and graphically compute it.
(e) Suppose the input is zero. Is the system depending on the initial condition BIBO stable? Find the
zero-input response y(t) when y(0) = 1. Is it bounded?
2.12. Ampliﬁer with saturation
The input–output equation characterizing an ampliﬁer that saturates once the input reaches certain values
is
y(t) =



100x(t)
−10 ≤x(t) ≤10
1000
x(t) > 10
−1000
x(t) < 10
where x(t) is the input and y(t) is the output.
(a) Plot the relation between the input x(t) and the output y(t). Is this a linear system? Explain.
(b) For what range of input values is the system linear, if any?
(c) Suppose the input is a sinusoid x(t) = 20 cos(2πt)u(t). Carefully plot x(t) and y(t) for t = −2 to 4.
(d) Let the input be delayed by two units of time (i.e., the input is x1(t) = x(t −2)). Find the corresponding
output y1(t) and indicate how it relates to the output y(t) due to x(t) found above. Is the system time
invariant?
2.13. QAM system
A quadrature amplitude modulation (QAM) system is a communication system capable of transmitting two
messages m1(t), m2(t) at the same time. The transmitted signal s(t) is
s(t) = m1(t) cos(ct) + m2(t) sin(ct)
Carefully draw a block diagram for the QAM system.
(a) Determine if the system is time invariant or not.
(b) Assume m1(t) = m2(t) = m(t)—that is, we are sending the same message using two different modu-
lators. Express the modulated signal in terms of a cosine with carrier frequency c, amplitude A, and
phase θ. Obtain A and θ. Is the system linear? Explain.

162
CHAPTER 2:
Continuous-Time Systems
2.14. Steady-state response of averager—MATLAB
An analog averager is given by
y(t) = 1
T
t
Z
t−T
x(τ)dτ
(a) Let x(t) = u(t) −u(t −1). Find the average signal y(t) using the above integral. Let T = 1. Carefully
plot y(t). Verify your result by graphically computing the convolution of x(t) and the impulse response
h(t) of the averager.
(b) To see the effect of T on the averager, consider the signal to be averaged to be x(t) = cos(2πt/T0)u(t).
Select the smallest possible value of T in the averager so that the steady-state response of the system,
y(t) as t →∞, will be 0.
(c) Use MATLAB to compute the output in part (b). Compute the output y(t) for 0 ≤t ≤2 at intervals
Ts = 0.001. Approximate the convolution integral using the function conv (use help to ﬁnd about conv)
multiplied by Ts.
2.15. Echo system modeling
An echo system could be modeled as follows:
(a) Using feedback systems is of great interest in control and in the modeling of many systems. An echo
is created as the sum of one or more delayed and attenuated output signals that are fed back into the
present signal. A possible model for an echo system is
y(t) = x(t) + α1y(t −τ) + · · · + αNy(t −Nτ)
where x(t) is the present input signal, y(t) is the present output, y(t −kτ) is the previous delayed
outputs, and the |αk| < 1 values are attenuation factors. Carefully draw a block diagram for this system.
(b) Consider the echo model for N = 1 and parameters τ = 1 and α1 = 0.1. Is the resulting echo system
LTI? Explain.
(c) Another possible model is given by a nonrecursive, or without feedback, system,
z(t) = x(t) + β1x(t −τ) + · · · + βMx(t −Mτ)
where several present and past inputs are delayed and attenuated and added up to form the output.
The parameters |βk| < 1 are attenuation factors and τ is a delay. Carefully draw a block diagram for the
echo system characterized by the above equation. Does the above equation represent an LTI system?
Explain.
2.16. An ideal low-pass ﬁlter—MATLAB
The impulse response of an ideal low-pass ﬁlter is
h(t) = sin(t)
t
or a sinc signal.
(a) Given that the impulse response is the response of the system to an input x(t) = δ(t) with zero initial
conditions, can an ideal low-pass ﬁlter be used for real-time processing? Explain.
(b) Is the ideal low-pass ﬁltering bounded-input bounded-output stable? Use MATLAB to check if the
impulse response satisﬁes the condition for BIBO stability.

Problems
163
2.17. Response to unbounded inputs versus BIBO stability
The BIBO stability assumes that the input is always bounded, limited in amplitude. If that is not the case,
even a stable system would provide an unbounded output. Consider the analog averager, with an input–
output relationship of
y(t) = 1
T
t
Z
t−T
x(τ)dτ
(a) Suppose that the input to the averager is a bounded signal x(t) (i.e., there is a ﬁnite value M such that
|x(t)| < M). Find the value for the bound of the output y(t) and determine whether the averager is BIBO
stable or not.
(b) Let the input to the averager be x(t) = tu(t) (i.e., a ramp signal). Compute the output y(t) and determine
if it is bounded or not. If y(t) is not bounded, does that mean that the averager is an unstable system?
Explain.
2.18. Sampler and hold circuit
In an analog-to-digital converter (ADC), the analog signal is ﬁrst sampled and then each of its samples is
converted into a digital value. Since each of the samples is obtained momentarily, there is the need for a
circuit that holds the value long enough for the ADC to convert it into a binary number. The circuit having
the sampler and the hold circuit is called the sampler and hold circuit, an example of which is shown in
Figure 2.24. The input is the sampled signal xs(t), which we are considering a train of rectangular pulses of
duration 1 and periodicity Ts and different magnitudes corresponding to x(nTs).
The value rC << 1, where 1 is the duration of the pulse and RC >> Ts where Ts >> 1, is the sampling
period. The ﬁrst condition allows the capacitor to be charged fast in 1 seconds, and the second condition
allows slow discharge in Ts seconds.
(a) Consider the ﬁrst sample conversion. Let the input to the hold circuit be a pulse of duration 1 and
amplitude x(0). The switch has been opened before t = 0 so that the capacitor is discharged. The
switch closes at t = 0 and remains closed until t = 1 and then it opens. Carefully draw the voltage in
the capacitor from t = 0 to Ts.
(b) Since the RC circuit is a linear time-invariant system, the output corresponding to the other samples
can be found from the result of the ﬁrst sample. Suppose the analog signal is a ramp x(t) = tu(t),
sampled with Ts = 1 and 1 = 0.1. Plot the voltage in the capacitor from t = 0 to t = 4 sec.
2.19. AM envelope detector—MATLAB
Consider an envelope detector that is used to detect the message sent in the AM system shown in the
examples. The envelope detector as a system is composed of two cascaded systems: one that computes
the absolute value of the input (implemented with ideal diodes), and a second that low-pass ﬁlters its input
(implemented with an RC circuit). The following is an implementation of these operations in the discrete
time so we can use numeric MATLAB.
FIGURE 2.24
Problem 2.18: sample and hold circuit.
R
C
+
−
+
−
xs(t)
r
x∗(t)

164
CHAPTER 2:
Continuous-Time Systems
Let the input to the envelope detector be
x(t) = [p(t) + P] cos(0t)
where P is the minimum of p(t) scaled. Use MATLAB to solve numerically this problem.
(a) Consider ﬁrst
p(t) = 20[u(t) −u(t −40)] −10 [u(t −40) −u(t −60)
Let 0 = 2π, P = 1.1| min
 p(t) |. Generate the signals p(t) and x(t) for 0 ≤t ≤100 with an interval of
Ts = 0.01.
(b) Consider then the subsystem that computes the absolute value of the input x(t).
(c) Compute the low-pass ﬁltered signal by using an RC circuit with impulse response h(t) = e−0.8tu(t).
To implement the convolution use the conv function multiplied by Ts. Plot together the message signal
p(t), the modulated signal x(t), the absolute value y(t), and the envelope of x(t). Does this envelope look
like p(t)?
(d) Consider the message signal p(t) = 2 cos(0.2πt), 0 = 10π, and P = | min
 p(t) |, and repeat the
process. Scale the signal to get the original p(t).
2.20. Frequency modulation (FM)—MATLAB
Frequency modulation, or FM, uses a wider bandwidth than amplitude modulation, or AM, but it is not
affected as much by noise as AM is. The output of an FM transmitter is of the form
y(t) = cos(ct + 2πν
t
Z
0
m(τ)dτ)
where m(t) is the message and ν is a factor in Hz/volt if the units of the message are in volts.
(a) Create as the message a signal
m(t) = cos(t)
Find the FM signal y(t) for ν = 10 and then for ν = 1. Let the carrier frequency c = 2π. Use MATLAB
to generate the different signals for times 0 ≤t ≤10 at intervals of Ts = 0.01. Plot m(t) and the two FM
signals (one for ν = 10 and the other for ν = 1) in the same plot. Is the FM transmitter a linear system?
Explain.
(b) Create a message signal
m1(t) =

1
when m(t) ≥0
−1
when m(t) < 0
Find the corresponding FM signal for ν = 1.

CHAPTER 3
The Laplace Transform
What we know is not much.
What we do not know is immense.
Pierre-Simon marquis de Laplace (1749–1827)
French mathematician and astronomer
3.1 INTRODUCTION
The material in this chapter is very signiﬁcant for the analysis of continuous-time signals and systems.
The main issues discussed are:
I
Frequency domain analysis of continuous-time signals and systems—We begin the frequency domain
analysis of continuous-time signals and systems using transforms. The Laplace transform, the
most general of these transforms, will be followed by the Fourier transform. Both provide com-
plementary representations of a signal to its own in the time domain, and an algebraic character-
ization of systems. The Laplace transform depends on a complex variable s = σ + j, composed
of damping σ and frequency , while the Fourier transform considers only frequency .
I
Damping and frequency characterization of continuous-time signals—The growth or decay of a signal—
damping—as well as its repetitive nature—frequency—in the time domain are characterized in
the Laplace domain by the location of the roots of the numerator and denominator, or zeros and
poles, of the Laplace transform of the signal.
I
Transfer function characterization of continuous-time LTI systems—The Laplace transform provides a
signiﬁcant algebraic characterization of continuous-time systems: The ratio of the Laplace trans-
form of the output to that of the input—or the transfer function of the system. It uniﬁes the
convolution integral and the differential equations system representations. The concept of trans-
fer function is not only useful in analysis but also in design, as we will see later. The location
of the poles and the zeros of the transfer function relates to the dynamic characteristics of the
system.
I
Stability, and transient and steady-state responses—Certain characteristics of continuous-time sys-
tems can only be veriﬁed or understood via the Laplace transform. Such is the case of stability,
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00006-5
c⃝2011, Elsevier Inc. All rights reserved.
165

166
CHAPTER 3:
The Laplace Transform
and of transient and steady-state responses. This is a signiﬁcant reason to study the Laplace
analysis before the Fourier analysis, which deals exclusively with the frequency characterization
of continuous-time signals and systems. Stability and transients are important issues in classic
control theory, thus the importance of the Laplace transform in this area. The frequency character-
ization of signals and the frequency response of systems—provided by the Fourier transform—are
signiﬁcant in communications.
I
One- and two-sided Laplace transforms—Given the prevalence of causal signals (those that are zero
for negative time) and of causal systems (having zero impulse responses for negative time) the
Laplace transform is typically known as “one-sided,” but the“two-sided” transform also exists.
The impression is that these are two different transforms, but in reality it is the Laplace transform
applied to two different types of signals and systems. We will show that by separating the signal
into its causal and its anti-causal components, we only need to apply the one-sided transform.
Care should be exercised, however, when dealing with the inverse transform so as to get the
correct signal.
I
Region of convergence and the Fourier transform—Since the Laplace transform requires integration
over an inﬁnite domain, it is necessary to consider if and where this integration converges—or
the “region of convergence” in the s-plane. Now, if such a region includes the j axis of the s-
plane, then the Laplace transform exists for s = j, and when computed there it coincides with
the Fourier transform of the signal. Thus, the Fourier transform for a large class of functions
can be obtained directly from their Laplace transforms—a good reason to study ﬁrst the Laplace
transform. In a subtle way, the Laplace transform is also connected with the Fourier series rep-
resentation of periodic continuous-time signals. Such a connection reduces the computational
complexity of the Fourier series by eliminating integration in cases when we can compute the
Laplace transform of a period.
I
Eigenfunctions of LTI systems—LTI systems respond to complex exponentials in a very special way:
The output is the exponential with its magnitude and phase changed by the response of the
system at the exponent. This provides the characterization of the system by the Laplace transform,
in the case of exponents of the complex frequency s, and by the Fourier representation when the
exponent is j. The eigenfunction concept is linked to phasors used to compute the steady-state
response in circuits (see Figure 3.1).
3.2 THE TWO-SIDED LAPLACE TRANSFORM
Rather than giving the deﬁnitions of the Laplace transform and its inverse, let us see how they could
be obtained intuitively. As indicated before, a basic idea in characterizing signals—and their response
when applied to LTI systems—is to consider them a combination of basic signals for which we can
easily obtain a response. In Chapter 2, when considering the time-domain solutions, we represented
the input as an inﬁnite combination of impulses occurring at all possible times and weighted by
the value of the input signal at those times. The reason we did so is because the response due to
an impulse is the impulse response of the LTI system, which is fundamental in our studies. A similar
approach will be followed when attempting to obtain the frequency-domain representation of signals
and their responses when applied to an LTI system. In this case, the basic functions used are com-
plex exponentials or sinusoids that depend on frequency. The concept of eigenfunction is somewhat

3.2 The Two-Sided Laplace Transform
167
LTI System
H(s)
y(t)= x(t) H(s0)
x(t)= es0t
FIGURE 3.1
Eigenfunction property of LTI systems. The input of the system is x(t) = es0t = eσ0tej0t and the output of the
system is the same input multiplied by the complex value H(s0) where H(s) = L[h(t)]—that is, the Laplace
transform of the impulse response h(t) of the LTI system.
abstract at the beginning, but after you see it applied here and in the Fourier representation later
you will think of it as a way to obtain a representation analogous to the impulse representation. You
will soon discover the importance of using complex exponentials, and it will then become clear that
eigenfunctions are connected with phasors that greatly simplify the sinusoidal steady-state solution
of circuits.
3.2.1 Eigenfunctions of LTI Systems
Consider as the input of an LTI system the complex signal
x(t) = es0t
s0 = σ0 + j0
for −∞< t < ∞, and let h(t) be the impulse response of the system. According to the convolution
integral, the output of the system is
y(t) =
∞
Z
−∞
h(τ)x(t −τ)dτ =
∞
Z
−∞
h(τ)es0(t−τ)dτ
= es0t
∞
Z
−∞
h(τ)e−τs0dτ = x(t)H(s0)
(3.1)
Since the same exponential at the input appears at the output, x(t) = es0t is called an eigenfunction1
of the LTI system. The input x(t) is changed at the output by the complex function H(s0), which is
related to the system through the impulse response h(t). In general, for any s, the eigenfunction at the
output is modiﬁed by a complex function
H(s) =
∞
Z
−∞
h(τ)e−τsdτ
which corresponds to the Laplace transform of h(t)!
1German mathematician David Hilbert (1862–1943) seems to be the ﬁrst to use the German word eigen to denote eigenvalues and
eigenvectors in 1904. The word eigen means own or proper.

168
CHAPTER 3:
The Laplace Transform
An input x(t) = es0t, s0 = σ0 + j0, is called an eigenfunction of an LTI system with impulse response h(t) if
the corresponding output of the system is
y(t) = x(t)
∞
Z
−∞
h(t)e−s0t = x(t)H(s0)
where H(s0) is the Laplace transform of h(t) computed at s = s0. This property is only valid for LTI systems—it
is not satisﬁed by time-varying or nonlinear systems.
Remarks
I
You could think of H(s) as an inﬁnite combination of complex exponentials, weighted by the impulse
response h(τ). One can use a similar representation for signals.
I
Consider now the signiﬁcance of applying the eigenfunction result. Suppose a signal x(t) is expressed as a
sum of complex exponentials in s = σ + j,
x(t) =
1
2πj
σ+j∞
Z
σ−j∞
X(s)estds
That is, an inﬁnite sum of exponentials in s each weighted by the function X(s)/(2πj) (this equation is
connected with the inverse Laplace transform as we will see soon). Using the superposition property of LTI
systems, and considering that for an LTI system with impulse response h(t) the output due to est is H(s)est,
then the output due to x(t) is
y(t) =
1
2πj
σ+j∞
Z
σ−j∞
X(s)

H(s)est
ds =
1
2πj
σ+j∞
Z
σ−j∞
Y(s)estds
where we let Y(s) = X(s)H(s). But from Chapter 2 we have that y(t) is the convolution y(t) = [x ∗h](t).
Thus, these two expressions are connected:
y(t) = [x ∗h](t)
⇔
Y(s) = X(s)H(s)
The expression on the left indicates how to compute the output in the time domain, and the one on the
right shows how to compute the Laplace transform of the output in the frequency domain. This is the most
important property of the Laplace transform: It reduces the complexity of the convolution integral in time
to the multiplication of the Laplace transforms of the input X(s) and of the impulse response H(s).
Now we are ready for the proper deﬁnition of the direct and inverse Laplace transforms of a signal or
of the impulse response of a system.

3.2 The Two-Sided Laplace Transform
169
The two-sided Laplace transform of a continuous-time function f(t) is
F(s) = L[ f(t)] =
∞
Z
−∞
f(t)e−stdt
s ∈ROC
(3.2)
where the variable s = σ + j, with  as the frequency in rad/sec and σ as a damping factor. ROC stands for
the region of convergence—that is, where the integral exists.
The inverse Laplace transform is given by
f(t) = L−1[F(s)] =
1
2πj
σ+j∞
Z
σ−j∞
F(s)estds
σ ∈ROC
(3.3)
Remarks
I
The Laplace transform F(s) provides a representation of f(t) in the s-domain, which in turn can be con-
verted back into the original time-domain functon in a one-to-one manner using the region of convergence.
Thus,
F(s)
ROC
⇔
f(t)
I
If f(t) = h(t), the impulse response of an LTI system, then H(s) is called the system or transfer function
of the system and it characterizes the system in the s-domain just like h(t) does in the time-domain. If f(t)
is a signal, then F(s) is its Laplace transform.
I
The inverse Laplace transform in Equation (3.3) can be understood as the representation of f(t) (whether
it is a signal or an impulse response) by an inﬁnite summation of complex exponentials with weights
F(s) at each. The computation of the inverse Laplace transform using Equation (3.3) requires complex
integration. Algebraic methods will be used later to ﬁnd the inverse Laplace transform, thus avoiding the
complex integration.
Laplace and Heaviside
The Marquis Pierre-Simon de Laplace (1749–1827) [2, 7] was a French mathematician and astronomer. Although from hum-
ble beginnings he became royalty by his political abilities. As an astronomer, he dedicated his life to the work of applying
the Newtonian law of gravitation to the entire solar system. He was considered an applied mathematician and, as a member
of the Academy of Sciences, knew other great mathematicians of the time such as Legendre, Lagrange, and Fourier. Besides
his work on celestial mechanics, Laplace did signiﬁcant work in the theory of probability from which the Laplace transform
probably comes. He felt that “the theory of probabilities is only common sense expressed in number.” Early transformations
similar to Laplace’s had been used by Euler and Lagrange. It was, however, Oliver Heaviside (1850–1925) who used the
Laplace transform in the solution of differential equations. Heaviside, an Englishman, was a self-taught electrical engineer,
mathematician, and physicist [76].

170
CHAPTER 3:
The Laplace Transform
I Example 3.1
A problem in wireless communications is the so-called multipath effect on the transmitted message.
Consider the channel between the transmitter and the receiver as a system like the one depicted
in Figure 3.2. The sent message x(t) does not necessarily go from the transmitter to the receiver
directly (line of sight) but it may take different paths, each with different length so that the signal
in each path is attenuated and delayed differently.2 At the receiver, these delayed and attenuated
signals are added, causing a fading effect—given the different phases of the incoming signals their
addition at the receiver results in a weak or a strong signal, thus giving the sensation of the message
fading back and forth. If x(t) is the message sent from the transmitter, and the channel has N
different paths with attenuation factors {αi} and corresponding delays {ti}, i = 0, . . . , N, use the
eigenfunction property to ﬁnd the system function of the channel causing the multipath effect.
FIGURE 3.2
Block diagram of a wireless
communication channel causing a
multipath effect on the sent message
x(t). The message x(t) is delayed
and attenuated when sent over
N + 1 paths. The effect is similar to
that of an echo in acoustic signals.
+
α0
α1
αN
Delay t0
Delay t1
Delay tN
x(t)
y(t)
•
•
•
Solution
The output of the channel or multipath system in Figure 3.2 can be written as
y(t) = α0x(t −t0) + α1x(t −t1) + · · · + αNx(t −tN)
(3.4)
Considering s = σ + j as the variable, the response of the multipath system to x(t) = est is y(t) =
x(t)H(s), so that when replacing them in Equation (3.4), we get
x(t)H(s) = x(t)

α0e−st0 + · · · + αNe−stN
2Typically, there are three effects each path can have on the sent signal: The distance the signal needs to travel (in each path this is due
to reﬂection or refraction on buildings, structures, cars, etc.) determines how much it is attenuated and delayed (the longer the path,
the more attenuated and delayed with respect to the time it was sent) and the third effect is a frequency shift—or Doppler effect—that
is caused by the relative velocity between the transmitter and the receiver.

3.2 The Two-Sided Laplace Transform
171
giving as the system function for the channel,
H(s) = α0e−st0 + · · · + αNe−stN
Notice that the time shifts in the input–output equation became exponentials in the Laplace
domain, a property we will see later.
I
Let us consider the different types of functions (either continuous-time signals or the impulse
responses of continuous-time systems) we might be interested in calculating Laplace transforms of.
I
Finite support functions: the function f(t) in this case is
f(t) = 0
for t ̸∈ﬁnite segment t1 ≤t ≤t2
for any ﬁnite, positive or negative t1 and t2, and so that t1 < t2. We will see that the Laplace trans-
form of these ﬁnite support signals is of particular interest in the computation of the coefﬁcients
of the Fourier series of periodic signals.
I
Inﬁnite support functions: In this case, f(t) is deﬁned over an inﬁnite support (e.g., t1 < t < t2 where
either t1 or t2 are inﬁnite, or both are inﬁnite as long as t1 < t2).
A ﬁnite, or inﬁnite, support function f(t) is called (see examples in Figure 3.3):
I Casual if f(t) = 0
t < 0,
I Anti-causal if f(t) = 0
t ≥0,
I Non causal if a combination of the above.
In each of these cases we need to consider the region in the s-plane where the transform exists or its
region of convergence (ROC). This is obtained by looking at the convergence of the transform.
FIGURE 3.3
Examples of different types of signals:
(a) noncausal ﬁnite support signal x1(t), (b) causal
ﬁnite support signal x2(t), (c) noncausal inﬁnite
support signal x3(t), and (d) causal inﬁnite
support signal x4(t).
(a)
x1(t )
t
(b)
x2(t)
t
(d)
x4(t)
t
(c)
x3(t )
t

172
CHAPTER 3:
The Laplace Transform
For the Laplace transform of f(t) to exist we need that

∞
Z
−∞
f(t)e−stdt

=

∞
Z
−∞
f(t)e−σte−jtdt

≤
∞
Z
−∞
| f(t)e−σt | dt < ∞
or that f(t)e−σt be absolutely integrable. This may be possible by choosing an appropriate σ even in the case
when f(t) is not absolutely integrable. The value chosen for σ determines the ROC of F(s); the frequency 
does not affect the ROC.
3.2.2 Poles and Zeros and Region of Convergence
The region of convergence (ROC) can be obtained from the conditions for the integral in the Laplace
transform to exist. The ROC is related to the poles of the transform, which is in general a complex
rational function.
For a rational function F(s) = L[ f(t)] = N(s)/D(s), its zeros are the values of s that make the function F(s) = 0,
and its poles are the values of s that make the function F(s) →∞. Although only ﬁnite zeros and poles are
considered, inﬁnite zeros and poles are also possible.
Typically, F(s) is rational, a ratio of two polynomials N(s) and D(s), or F(s) = N(s)/D(s), and as such
its zeros are the values of s that make the numerator polynomial N(s) = 0, while the poles are the
values of s that make the denominator polynomial D(s) = 0. For instance, for
F(s) =
2(s2 + 1)
s2 + 2s + 5 = 2(s + j)(s −j)
(s + 1)2 + 4 =
2(s + j)(s −j)
(s + 1 + 2j)(s + 1 −2j)
we have the zeros are at s = ±j, roots of N(s) = 0, since F(±j) = 0, and a pair of complex conjugate
poles −1 ± 2j, the roots of the equation D(s) = 0 and such that F(−1 ± 2j) →∞. Geometrically,
zeros can be visualized as those values that make the function go to zero, and poles as those val-
ues that make the function approach inﬁnity (looking like the main “pole” of a circus tent). See
Figure 3.4.
Not all rational functions have poles or a ﬁnite number of zeros. Consider the Laplace transform
P(s) = 1
s
 es −e−s
P(s) seems to have a pole at s = 0. Its zeros are obtained by letting es −e−s = 0, which when
multiplied by es gives
e2s = 1 = ej2πk

3.2 The Two-Sided Laplace Transform
173
100
50
0
0
−2
−2
2
1
−1
−3
0
Damping
Frequency
−50
−100
FIGURE 3.4
Three-dimensional plot of the logarithm of the magnitude of F(s) = 2(s2 + 1)/(s2 + 2s + 5) as a function of
damping σ and frequency . The poles shoot up, while the zeros shoot down. In the logarithmic scale both
poles and zeros will have inﬁnite value: When F(s) = 0 (zero) its logarithm is −∞, while when F(s) →∞(pole)
the logarithm is ∞.
for an integer k = 0, ±1, ±2, . . . . Thus, the zeros are sk = jπk, k = 0, ±1, ±2, . . . . Now, when k = 0,
the zero at 0 cancels the pole at zero; therefore, P(s) has only zeros, an inﬁnite number of them,
{jπk, k = ±1, ±2, . . .}.
Poles and ROC
The ROC consists of the values of σ such that

∞
Z
−∞
x(t)e−stdt

≤
∞
Z
−∞
|x(t)||e−(σ+j)t|dt =
∞
Z
−∞
|x(t)|e −σtdt < ∞
(3.5)
This is equivalent to choosing values of σ for which x(t)e−σt is absolutely integrable.
Two general comments that apply to all types of signals when ﬁnding ROCs are:
I
No poles are included in the ROC, which means that for the ROC to be that region where the
Laplace transform is deﬁned, the transform cannot become inﬁnite at any point in it. So poles
should not be present in the ROC.
I
The ROC is a plane parallel to the j axis, which means that it is the damping σ that deﬁnes the
ROC, not frequency . This is because when we compute the absolute value of the integrand in

174
CHAPTER 3:
The Laplace Transform
the Laplace transform to test for convergence, we let s = σ + j and the term |ej| = 1. Thus, all
regions of convergence will contain −∞<  < ∞.
If {σi} are the real parts of the poles of F(s) = L[f(t)], the region of convergence corresponding to
different types of signals or impulse responses is determined from its poles as follows:
I
For a causal f(t), f(t) = 0 for t < 0, the region of convergence of its Laplace transform F(s) is a
plane to the right of the poles,
Rc = {(σ, ) : σ > max{σi}, −∞<  < ∞}
I
For an anti-causal f(t), f(t) = 0 for t > 0, the region of convergence of its Laplace transform F(s)
is a plane to the left of the poles,
Rac = {(σ, ) : σ < min{σi}, −∞<  < ∞}
I
For a noncausal f(t) (i.e., f(t) deﬁned for −∞< t < ∞), the region of convergence of its Laplace
transform F(s) is the intersection of the regions of convergence corresponding to the causal
component, Rc, and Rac corresponding to the anti-causal component:
Rc
\
Rac
See Figure 3.5 for an example illustrating how the ROCs connect with the poles and the type of signal.
Special case: The Laplace transform of a function f(t) of ﬁnite support t1 ≤t ≤t2, has the whole s-plane as
ROC.
FIGURE 3.5
ROC for (a) causal signal with poles with
σmax = 0; (b) causal signal with poles with
σmax < 0; (c) anti-causal signal with poles with
σmin > 0; (d) noncausal signal where ROC is
bounded by poles (poles on the left-hand
s-plane give causal component and poles on
the right-hand s-plane give the anti-causal
component of the signal). The ROCs do not
contain poles, but they can contain zeros.
(a)
j Ω
o
o
×
×
×
σ
(b)
jΩ
o
×
×
σ
(c) 
jΩ
o
×
o
o
σ
(d)
j Ω
o
o
×
×
×
×
σ

3.2 The Two-Sided Laplace Transform
175
Indeed, the integral deﬁning the Laplace transform is bounded for any value of σ ̸= 0. If A =
max(|f(t)|), then
|F(s)| ≤
t2
Z
t1
|f(t)||e−st|dt ≤A
t2
Z
t1
e−σtdt = Ae−σt1 −e−σt2
σ
< ∞
σ ̸= 0
The Laplace transform of a
I
Finite support function (i.e., f(t) = 0 for t < t1 and t > t2, for t1 < t2) is
L[f(t)] = L [f(t)[u(t −t1) −u(t −t2)]]
whole s-plane
I
Causal function (i.e., f(t) = 0 for t < 0) is
L[f(t)u(t)]
Rc = {(σ,) : σ > max{σi}, −∞<  < ∞}
I
Anti-causal function (i.e., f(t) = 0 for t > 0) is
L[f(t)u(−t)]
Rac = {(σ, ) : σ < min{σi}, −∞<  < ∞}
I
Noncausal function (i.e., f(t) = fac(t) + fc(t) = f(t)u(−t) + f(t)u(t)) is
L[f(t)] = L[fac(−t)u(t)](−s) + L[fc(t)u(t)]
Rc
\
Rac
Although redundant, a causal function f(t) (i.e., f(t) = 0 for t < 0) is denoted as f(t)u(t). Its Laplace
transform is thus
L[f(t)u(t)] =
∞
Z
−∞
f(t)u(t)e−stdt =
∞
Z
0
f(t)e−stdt
which is called the one-sided Laplace transform. Likewise, if f(t) is anti-causal (i.e., f(t) = 0 for t > 0),
we will denote it as f(t)u(−t) and its Laplace transform is given by
L[ f(t)u(−t)] =
0
Z
−∞
f(t)u(−t)e−stdt =
∞
Z
0
f(−t′)u(t′)est′dt′
or the one-sided Laplace transform of the causal signal f(−t)u(t), with s changed into −s.
A noncausal signal f(t) is deﬁned for all values of t (i.e., for −∞< t < ∞). Such a signal has a causal
component fc(t), which is obtained by multiplying f(t) by the unit-step function, fc(t) = f(t)u(t), and
an anti-causal component fac(t), which is obtained by multiplying f(t) by u(−t), so that
f(t) = fac(t) + fc(t)
= f(t)u(−t) + f(t)u(t)
(3.6)

176
CHAPTER 3:
The Laplace Transform
At t = 0 we assume that u(0) = 0.5 to get f(0) from the sum fc(0) + fac(0). The Laplace transform of
the two-sided signal f(t) can then be computed as
F(s) =
∞
Z
0
f(−t)u(t)estdt +
∞
Z
0
f(t)u(t)e−stdt
= L[ fac(−t)u(t)](−s) + L[ fc(t)u(t)]
(3.7)
with an ROC the intersection of the ROCs of the causal and the anti-causal Laplace transforms.
3.3 THE ONE-SIDED LAPLACE TRANSFORM
The one-sided Laplace transform is deﬁned as
F(s) = L[ f(t)u(t)] =
∞
Z
0−
f(t)u(t)e−stdt
(3.8)
where f(t) is either a causal function or made into a causal function by the multiplication by u(t). The one-
sided Laplace transform is of signiﬁcance given that most of the applications deal with causal systems and
signals, and that any signal or system can be decomposed into causal and anti-causal components requiring
only the computation of one-sided Laplace transforms.
Remarks
I
If f(t) is causal the multiplication by u(t) is redundant but harmless, but if f(t) is not causal the multi-
plication by u(t) makes f(t)u(t) causal. Notice that when f(t) is causal, the two-sided and the one-sided
Laplace transforms of f(t) coincide.
I
The lower limit of the integral in the one-sided Laplace transform is set to 0−= 0 −ε, which corresponds
to a value on the left side of 0 for an inﬁnitesimal value ε. The reason for this is to make sure that an
impulse function δ(t), only deﬁned at t = 0, is included when we are computing its Laplace transform.
For any other signal this limit can be taken as 0 with no effect on the transform.
I
As we will see, the advantage of the one-sided Laplace transform is that it can be used in the solution
of differential equations with initial conditions. In fact, the two-sided Laplace transform by starting at
t = −∞(lower bound of the integral) ignores initial conditions at t = 0, and thus it is not useful in
solving differential equations unless the initial conditions are zero.
I Example 3.2
Find the Laplace transforms of δ(t), u(t), and a pulse p(t) = u(t) −u(t −1). Use MATLAB to verify
the transforms.

3.3 The One-Sided Laplace Transform
177
Solution
Even though δ(t) is not a regular signal, its Laplace transform can be easily obtained:
L[δ(t)] =
∞
Z
−∞
δ(t)e−stdt =
∞
Z
−∞
δ(t)e−s0dt =
∞
Z
−∞
δ(t)dt = 1
Since there are no conditions for the integral to exist, we say that L[δ(t)] = 1 exists for all values
of s, or that its ROC is the whole s-plane. This is also indicated by the fact that L[δ(t)] = 1 has no
poles.
The Laplace transform of u(t) can be found as
U(s) = L[u(t)] =
∞
Z
−∞
u(t)e−stdt =
∞
Z
0
e−stdt =
∞
Z
0
e−σte−jtdt
where we replaced the variable s = σ + j. Using Euler’s equation, the above equation becomes
U(s) =
∞
Z
0
e−σt[cos(t) −j sin(t)]dt
and since the sine and the cosine are bound, then we need to ﬁnd a value for σ so that the expo-
nential e−σt does not grow as t increases. If σ < 0, the exponential e−σt for t ≥0 will grow and
the integral will not converge. On the other hand, if σ > 0, the integral will converge as e−σt for
t ≥0 decays, and it is not clear what happens when σ = 0. Thus, the integral exists in the region
deﬁned by σ > 0 and all frequencies −∞<  < ∞(the frequency values do not interfere in the
convergence). Such a region is the open right-hand s-plane, and is called the ROC of U(s).
In the region of convergence, the integral is found to be
U(s) = e−st
−s |∞
t=0 = 1
s
where the limit for t = ∞is zero since σ > 0. So the Laplace transform U(s) = 1/s converges in
the region deﬁned by {(σ, ) : σ > 0, −∞<  < ∞}, or the open (i.e., the j axis is not included)
right-hand s-plane. This ROC can also be obtained by considering that the pole of U(s) is at s = 0
and that u(t) is casual.
We can ﬁnd the Laplace transform of signals using symbolic computations in MATLAB. For the
unit-step and the delta functions, once the symbolic parameters are deﬁned, the MATLAB function
laplace computes their Laplace transforms as indicated by the following script.
%%%%%%%%%%%%%%%%%
% Example 3.2
%%%%%%%%%%%%%%%%%
syms t s
% Unit-step function

178
CHAPTER 3:
The Laplace Transform
u = sym(’Heaviside(t)’)
U=laplace(u)
% Delta function
d = sym(’Dirac(t)’)
D = laplace(d)
giving
u = Heaviside(t)
U = 1/s
d = Dirac(t)si
D = 1
where U and D stand for the Laplace transforms of u and d. The naming of u(t) and δ(t) as Heaviside
and Dirac functions is used in MATLAB.3
The pulse p(t) = u(t) −u(t −1) is a ﬁnite support signal and so its ROC is the whole s-plane. Its
Laplace transform is
P(s) = L[u(t + 1) −u(t −1)] =
1
Z
−1
e−stdt = −e−st
s
|1
t=−1 = 1
s [es −e−s] = es
s [1 −e−2s]
which as shown before has an inﬁnite number of zeros, and the one at the origin cancels the pole,
so that
P(s) =
∞
Y
k=−∞,k̸=0
(s −jπk)
I
I Example 3.3
Let us ﬁnd and use the Laplace transform of ej(0t+θ)u(t) to obtain the Laplace transform of x(t) =
cos(0t + θ)u(t). Consider the special cases for θ = 0 and θ = −π/2. Determine the ROCs. Use
MATLAB to plot the signals and the corresponding poles/zeros when 0 = 2 and θ = 0 and π/4.
Solution
The Laplace transform of the complex causal signal ej(0t+θ)u(t) is found to be
L[ej(0t+θ)u(t)] =
∞
Z
0
ej(0t+θ)e−stdt = ejθ
∞
Z
0
e−(s−j0)tdt
3Oliver Heaviside (1850–1925) was an English electrical engineer who adapted the Laplace transform to the solution of differential
equations (the so-called operational calculus), while Paul Dirac (1902–1984) was also an English electrical engineer, better known for
his work in physics.

3.3 The One-Sided Laplace Transform
179
=
−ejθ
s −j0
e−σt−j(−0)t |∞
t=0=
ejθ
s −j0
ROC: σ > 0
According to Euler’s identity
cos(0t + θ) = ej(0t+θ) + e−j(0t+θ)
2
by the linearity of the integral and using the above result, we get that
L[cos(0t + θ)u(t)] = 0.5L[ej(0t+θ)u(t)] + 0.5L[e−j(0t+θ)u(t)]
= 0.5ejθ(s + j0) + e−jθ(s −j0)
s2 + 2
0
= s cos(θ) −0 sin(θ)
s2 + 2
0
and a region of convergence {(σ, ) : σ > 0, −∞<  < ∞}.
Now if we let θ = 0, −π/2 in the above equation we have the following Laplace transforms:
L[cos(0t)u(t)] =
s
s2 + 2
0
L[sin(0t)u(t)] =
0
s2 + 2
0
as cos(0t −π/2) = sin(0t). The ROC of the above Laplace transforms is {(σ, ) : σ > 0, −∞<
 < ∞}, or the open right-hand s-plane (i.e., not including the j axis). See Figure 3.6 for the
pole-zero plots and the corresponding signals for θ = 0, θ = π/4, and 0 = 2.
I
I Example 3.4
Use MATLAB symbolic computation to ﬁnd the Laplace transform of a real exponential, x(t) =
e−tu(t), and of x(t) modulated by a cosine or y(t) = e−t cos(10t)u(t). Plot the signals and the poles
and zeros of their Laplace transforms.
Solution
The following script is used. The MATLAB function laplace is used for the computation of the
Laplace transform and the function ezplot allows us to do the plotting. For the plotting of the poles
and zeros we use our function splane. When you run the script you obtain the Laplace transforms
X(s) =
1
s + 1
Y(s) =
s + 1
s2 + 2s + 101

180
CHAPTER 3:
The Laplace Transform
FIGURE 3.6
Location of the poles and zeros of
cos(2t + θ)u(t) for (a) θ = 0 and for (b)
θ = π/4. Note that the zero is moved
to the right to 2 because the zero of
the Laplace transform is
s = 0 tan(θ) = 2 tan(π/4) = 2.
−1
0
1
2
3
−2
0
2
j Ω
σ
0
5
10
−1
−0.5
0
0.5
1
t
x1(t)
jΩ
−2
0
2
−1
0
1
2
3
σ
−1
−0.5
0
0.5
1
0
5
10
t
x2(t)
(a)
(b)
%%%%%%%%%%%%%%%%%
% Example 3.4
%%%%%%%%%%%%%%%%%
syms t
x = exp (-t);
y = x * cos(10 * t);
X = laplace(x)
Y = laplace(y)
% plotting of signals and poles/zeros
ﬁgure(1)
subplot(221)
ezplot(x,[0,5]);grid
axis([0 5 0 1.1]);title(’x(t) = exp(-t)u(t)’)
numx = [0 1];denx = [1 1];
subplot(222)
splane(numx,denx)
subplot(223)
ezplot(y,[-1,5]);grid
axis([0 5 -1.1 1.1]);title(’y(t) = cos(10t)exp(-t)u(t)’)
numy = [0 1 1];deny = [1 2 101];
subplot(224)
splane(numy,deny)
The results are shown in Figure 3.7.
I

3.3 The One-Sided Laplace Transform
181
0
2
4
−1
−0.5
0
0.5
1
t
y(t)=cos(10t)exp(−t)u(t)
−2
0
2
−10
−5
0
5
10
σ
jΩ
(a)
(b) 
x(t)=exp(−t )u(t)
0
2
4
0
0.5
1
−2
0
2
−1
−0.5
0
0.5
1
σ
jΩ
FIGURE 3.7
Poles and zeros of the Laplace transform of (a) causal signal x(t) = e−tu(t) and of (b) causal decaying signal
y(t) = e−t cos(10t)u(t).
I Example 3.5
In statistical signal processing, the autocorrelation function c(τ) of a random signal describes the
correlation that exists between the random signal x(t) and shifted versions of it, x(t + τ) and x(t −
τ) for shifts −∞< τ < ∞. Clearly, c(τ) is two-sided (i.e., nonzero for both positive and negative
values of τ) and symmetric. Its two-sided Laplace transform is related to the power spectrum of
the signal x(t). Let c(t) = e−a|t|, where a > 0 (we replaced the τ variable for t for convenience). Find
its Laplace transform. Determine if it would be possible to compute |C()|2, which is called the
power spectrum of the random signal x(t).
Solution
The autocorrelation can be expressed as
c(t) = c(t)u(t) + c(t)u(−t)
= cc(t) + cac(t)
where cc(t) is the causal component and cac(t) the anti-causal component of c(t). The Laplace
transform of c(t) is then given by
C(s) = L[cc(t)u(t)] + L[cac(−t)u(t)](−s)

182
CHAPTER 3:
The Laplace Transform
The Laplace transform for cc(t) = e−atu(t), as seen before, is
Cc(s) =
1
s + a
with a region of convergence {(σ, ) : σ > −a, −∞<  < ∞}. The Laplace transform of the anti-
causal part is
L[cac(−t)u(t)](−s) =
1
−s + a
and since it is anti-causal and has a pole at s = a, its region of convergence is {(σ, ) : σ < a, −∞<
 < ∞}.
We thus have that
C(s) =
1
s + a +
1
−s + a
=
2a
a2 −s2
with a region of convergence the intersection of σ > −a with σ < a or {(σ, ) : −a < σ < a, −∞<
 < ∞}. This region contains the j axis, which permits us to compute the distribution of the
power over frequencies or the power spectrum of the random signals |C()|2 (see in Figure 3.8).
I
I Example 3.6
Consider a noncausal LTI system with impulse response
h(t) = e−tu(t) + e2tu(−t)
= hc(t) + hac(t)
Let us compute the system function H(s) for this system, and ﬁnd out whether we could compute
H( j) from its Laplace transform.
Solution
As from before, the Laplace transform of the causal component, hc(t), is
Hc(s) =
1
s + 1
provided that σ > −1. For the anti-causal component
L[hac(t)] = L[hac(−t)u(t)](−s) =
1
−s + 2
which converges when σ −2 < 0 or σ < 2, or its region of convergence is {(σ, ) : σ < 2, −∞<
 < ∞}.

3.3 The One-Sided Laplace Transform
183
FIGURE 3.8
Poles (top right) of the Laplace
transform of the autocorrelation
c(t) = e−2|t| (top left), which is
noncausal. The ROC of C(s) is the
region in between the poles, which
includes the j axis. The spectrum
|C()|2 corresponding to c(t) is
shown in the bottom plot—this is the
magnitude square of the Fourier
transform of c(t).
−20
−10
0
10
20
0
0.2
0.4
0.6
0.8
1
Ω(rad/sec)
|C(Ω)|2
−5
0
5
0
0.2
0.4
0.6
0.8
1
t
c(t)
−2
0
2
−1
−0.5
0
0.5
1
σ
jΩ
Thus, the system function is
H(s) =
1
s + 1 +
1
−s + 2 =
−3
(s + 1)(s −2)
with a region of convergence the intersection of the regions of convergence of its components, or
the intersection of {(σ, ) : σ > −1, −∞<  < ∞} and {(σ, ) : σ < 2, −∞<  < ∞}, or
{(σ, ) : −1 < σ < 2, −∞<  < ∞}
which is a sector of the s-plane that includes the j axis. Thus, H( j) can be computed from its
Laplace transform.
I

184
CHAPTER 3:
The Laplace Transform
I Example 3.7
Compute the Laplace transform of the ramp function r(t) = tu(t) and use it to ﬁnd the Laplace of
a triangular pulse 3(t) = r(t + 1) −2r(t) + r(t −1).
Solution
Notice that although the ramp is an ever-increasing function of t, we still can obtain its Laplace
transform
R(s) =
∞
Z
0
te−stdt = e−st
s2 (−st −1)
∞
t=0 = 1
s2
where we let σ > 0 for the integral to exist. Thus, R(s) = 1/s2 with region of convergence {(σ, ) :
σ > 0, −∞<  < ∞}. The above integration can be avoided by noticing that if we ﬁnd the
derivative with respect to s of the Laplace transform of u(t), or
d U(s)
ds
=
∞
Z
0
de−st
ds dt
=
∞
Z
0
(−t)e−stdt
= −R(s)
where we assumed the derivative and the integral can be interchanged. We then have
R(s) = −d U(s)
ds
= 1
s2
The Laplace transform of 3(t) can then be shown to be (try it!)
3(s) = 1
s2 [es −2 + e−s]
The zeros of 3(s) are the values of s that make es −2 + e−s = 0, or multiplying by e−s,
1 −2e−s + e−2s = (1 −e−s)2 = 0
which is equivalent to e−s = 1 = ej2πk, for integer k, or double zeros at
sk = j2πk
k = 0, ±1, ±2, . . .
In particular, when k = 0 there are two zeros at 0, which cancel the two poles at 0 resulting from
the denominator s2. Thus, 3(s) has an inﬁnite number of zeros but no poles given this pole-zero
cancellation (see Figure 3.9). Therefore, 3(s) has the whole s-plane as its region of convergence,
and can be calculated at s = j.
I

3.3 The One-Sided Laplace Transform
185
o
o
o
o
...
...
t
1
−1
1
s-plane
w(t )
j4π  
j 2π
−j2π
−j4π
jΩ
σ
FIGURE 3.9
The Laplace transform of triangular signal 3(t) has as ROC the whole s-plane, since it has no poles but an
inﬁnite number of double zeros at ±j2πk, for k = ±1, ±2, . . ..
We will consider next the basic properties of the one-sided Laplace transform—many of these prop-
erties will be encountered in the Fourier analysis, presented in a slightly different form, given the
connection between the Laplace and the Fourier transforms. Something to observe is the duality that
exists between the time and the frequency domains. The time and the frequency domain represen-
tations of continuous-time signals and systems are complementary—that is, certain characteristics of
the signal or the system can be seen better in one domain than in the other. In the following, we
consider the properties of the Laplace transform of signals but they equally apply to the impulse
response of a system.
3.3.1 Linearity
For signals f(t) and g(t), with Laplace transforms F(s) and G(s), and constants a and b, we have the Laplace
transform is linear:
L[af(t)u(t) + bg(t)u(t)] = aF(s) + bG(s)
The linearity of the Laplace transform is easily veriﬁed using integration properties:
L[af(t)u(t) + bg(t)u(t)] =
∞
Z
0
[af(t) + bg(t)]u(t)e−stdt
= a
∞
Z
0
f(t)u(t)e−stdt + b
∞
Z
0
g(t)u(t)e−stdt
= aL[f(t)u(t)] + bL[g(t)(t)]
We will use the linearity property to illustrate the signiﬁcance of the location of the poles of the
Laplace transform of causal signals. As seen before, the Laplace transform of an exponential signal

186
CHAPTER 3:
The Laplace Transform
f(t) = Ae−atu(t) where a in general can be a complex number is
F(s) =
A
s + a
ROC: σ > −a
The location of the pole s = −a closely relates to the signal. For instance, if a = 5, f(t) = Ae−5tu(t) is
a decaying exponential and the pole of F(s) is at s = −5 (in left-hand s-plane); if a = −5, we have an
increasing exponential and the pole is at s = 5 (in right-hand s-plane). The larger the value of |a| the
faster the exponential decays (for a > 0) or increases (for a < 0); thus, Ae−10tu(t) decays a lot faster
that Ae−5tu(t), and Ae10tu(t) grows a lot faster than Ae5tu(t).
The Laplace transform F(s) = 1/(s + a) of f(t) = e−atu(t), for any real value of a, has a pole on the real
axis σ of the s-plane, and we have the following three cases:
I
For a = 0, the pole at the origin s = 0 corresponds to the signal f(t) = u(t), which is constant for
t ≥0 (i.e., it does not decay).
I
For a > 0, the signal is f(t) = e−atu(t), a decaying exponential, and the pole s = −a of F(s) is in
the real axis σ of the left-hand s-plane. As the pole is moved away from the origin toward the left,
the faster the exponential decays, and as it moves toward the origin, the slower the exponential
decays.
I
For a < 0, the pole s = −a is on the real axis σ of the right-hand s-plane, and corresponds to
a growing exponential. As the pole moves to the right the exponential grows faster, and as it is
moved toward the origin it grows at a slower rate—clearly this signal is not useful, as it grows
continuously.
The conclusion is that the σ axis of the Laplace plane corresponds to damping. A single pole on this axis
and in the left-hand s-plane corresponds to a decaying exponential, and a single pole on this axis and in the
right-hand s-plane corresponds to a growing exponential.
Suppose then we consider
g(t) = A cos(0t)u(t) = Aej0t
2 u(t) + Ae−j0t
2
u(t)
and let a = j0 to express g(t) as
g(t) = 0.5[Aeatu(t) + Ae−atu(t)]
Then, by the linearity of the Laplace transform and the previous result we obtain
G(s) = A
2
1
s −j0
+ A
2
1
s + j0
=
As
s2 + 2
0
(3.9)
with a zero at s = 0, and the poles are values for which
s2 + 2
0 = 0 ⇒s2 = −2
0
or s1,2 = ±j0

3.3 The One-Sided Laplace Transform
187
which are located on the j axis. The farther away from the origin of the j axis the poles are, the
higher the frequency 0, and the closer the poles are to the origin, the lower the frequency. Thus,
the j axis corresponds to the frequency axis. Furthermore, notice that to generate the real-valued
signal g(t) we need two complex conjugate poles, one at +j0 and the other at −j0. Although
frequency, as measured by frequency meters, is a positive value, “negative” frequencies are needed to
represent “real” signals (if the poles are not complex conjugate pairs, the inverse Laplace transform is
complex—rather than real valued).
The conclusion is that the Laplace transform of a sinusoid has a pair of poles on the j axis. For these poles
to correspond to a real-valued signal they should be complex conjugate pairs, requiring negative as well as
positive values of the frequency. Furthermore, when these poles are moved away from the origin of the j
axis, the frequency increases, and the frequency decreases whenever the poles are moved toward the origin.
Finally, consider the case of a signal d(t) = Ae−αt cos(0t)u(t) or a causal sinusoid multiplied (or
modulated) by e−αt. According to Euler’s identity,
d(t) = A
"
e(−α+j0)t
2
u(t) + e(−α−j0)t
2
u(t)
#
and as such we can again use linearity to get
D(s) =
A(s + α)
(s + α)2 + 2
0
(3.10)
Notice the connection between Equations (3.9) and (3.10). Given G(s), then D(s) = G(s + α), with
G(s) corresponding to g(t) = A cos(0t) and D(s) to d(t) = g(t)e−αt. Multiplying a function g(t) by an
exponential e−αt, with α real or imaginary, shifts the transform to G(s + α)—that is, it is a complex
frequency-shift property. The poles of D(s) have as the real part the damping factor −α and as the
imaginary part the frequencies ±0. The real part of the pole indicates decay (if α > 0) or growth
(if α < 0) in the signal, while the imaginary part indicates the frequency of the cosine in the signal.
Again, the poles will be complex conjugate pairs since the signal d(t) is real valued.
The conclusion is that the location of the poles (and to some degree the zeros), as indicated in the previous two
cases, determines the characteristics of the signal. Signals are characterized by their damping and frequency
and as such can be described by the poles of its Laplace transform.
If we were to add the different signals considered above, then the Laplace transform of the resulting
signal would be the sum of the Laplace transform of each of the signals and the poles would be
the aggregation of the poles from each. This observation will be important when ﬁnding the inverse
Laplace transform, then we would like to do the opposite: To isolate poles or pairs of poles (when
they are complex conjugate) and associate with each a general form of the signal with parameters that
are found by using the zeros and the other poles of the transform. Figure 3.10 provides an example
illustrating the importance of the location of the poles, and the signiﬁcance of the σ and j axes.

188
CHAPTER 3:
The Laplace Transform
0
5
10
0
0.5
1
t
u(t)
0
5
10
−1
0
1
t
cos(5t)u(t)
0
5
10
0
0.5
1
t
exp(−0.5t)u(t)
0
5
10
−1
0
1
t
cos(5t)exp(−0.5t)u(t)
−2
−1
0
1
−10
0
10
σ
jΩ
FIGURE 3.10
For poles shown in the middle, possible signals are displayed around them anti–clockwise from bottom right.
The pole s = 0 corresponds to a unit-step signal; the complex conjugate poles on the j axis correspond to a
sinusoid; the pair of complex conjugate poles with a negative real part provides a sinusoid multiplied by an
exponential; and the pole in the negative real axis gives a decaying exponential. The actual amplitudes and
phases are determined by the other poles and by the zeros.
3.3.2 Differentiation
For a signal f(t) with Laplace transform F(s) its one-sided Laplace transform of its ﬁrst-and second-order
derivatives are
L
df(t)
dt u(t)

= sF(s) −f(0−)
(3.11)

3.3 The One-Sided Laplace Transform
189
L
"
d2f(t)
dt2 u(t)
#
= s2F(s) −sf(0−) −df(t)
dt
t=0−
(3.12)
In general, if f (N)(t) denotes an Nth-order derivative of a function f(t) that has a Laplace transform F(s), we
have
L[f (N)(t)u(t)] = sNF(s) −
N−1
X
k=0
f (k)(0−)sN−1−k
(3.13)
where f (m)(t) = dmf(t)/dtm is the mth-order derivative, m > 0, and f (0)(t) ≜f(t).
The Laplace transform of the derivative of a causal signal is
L
df(t)
dt u(t)

=
∞
Z
0−
df(t)
dt e−stdt
This integral is evaluated by parts. Let w = e−st, then dw = −se−stdt, and let v = f(t) so that dv =
[df(t)/dt]dt, and
Z
wdv = wv −
Z
vdw
We would then have
∞
Z
0−
df(t)
dt e−stdt = e−stf(t)
∞
0−−
∞
Z
0−
f(t)(−se−st)dt
= s
∞
Z
0−
f(t)e−stdt −f(0−)
= sF(s) −f(0−)
where e−stf(t)|t=0−= f(0−) and e−stf(t)|t→∞= 0 since the region of convergence guarantees that
lim
t→∞f(t)e−σt = 0
For a second-order derivative we have that
L
d2f(t)
dt2 u(t)

= L
"
df (1)(t)
dt
u(t)
#
= sL[f (1)(t)] −f (1)(0−)
= s2F(s) −sf(0−) −df(t)
dt
|t=0−
where we used the notation f (1)(t) = df(t)/dt. This approach can be extended to any higher order to
obtain the general result shown above.

190
CHAPTER 3:
The Laplace Transform
Remarks
I
The derivative property for a signal x(t) deﬁned for all t is
∞
Z
−∞
dx(t)
dt e−stdt = sX(s)
This can be seen by computing the derivative of the inverse Laplace transform with respect to t, assuming
that the integral and the derivative can be interchanged. Using Equation (3.3):
dx(t)
dt
=
1
2πj
σ+j∞
Z
σ−j∞
X(s)dest
dt ds
=
1
2πj
σ+j∞
Z
σ−j∞
(sX(s))estds
or that sX(s) is the Laplace transform of the derivative of x(t). Thus, the two-sided transform does not
include initial conditions. The above result can be generalized to any order of the derivative as
L[dNx(t)/dtN] = sNX(s)
I
Application of the linearity and the derivative properties of the Laplace transform makes solving differential
equations an algebraic problem.
I Example 3.8
Find the impulse response of an RL circuit in series with a voltage source vs(t) (see Figure 3.11).
The current i(t) is the output and the input is the voltage source vs(t).
Solution
To ﬁnd the impulse response of the RL circuit we let vs(t) = δ(t) and set the initial current in the
inductor to zero. According to Kirchhoff’s voltage law,
vs(t) = Ldi(t)
dt
+ Ri(t)
i(0−) = 0
FIGURE 3.11
Impulse response i(t) of an RL circuit with input
vs(t).
+
−
vs(t)
i(t)
i(t)
R
L
t

3.3 The One-Sided Laplace Transform
191
which is a ﬁrst-order linear differential equation with constant coefﬁcients, zero initial condition,
and a causal input so that it is a linear time-invariant system, as discussed before.
Letting vs(t) = δ(t) and computing the Laplace transform of the above equation (using the linearity
and the derivative properties of the transform and remembering the initial condition is zero), we
obtain the following equation in the s-domain:
L[ δ(t)] = L

Ldi(t)
dt
+ Ri(t)

1 = sLI(s) + RI(s)
where I(s) is the Laplace transform of i(t). Solving for I(s) we have that
I(s) =
1/L
s + R/L
which as we have seen is the Laplace transform of
i(t) = 1
Le−(R/L)tu(t)
Notice that i(0−) = 0 and that the response has the form of a decaying exponential trying to follow
the input signal, a delta function.
I
I Example 3.9
In this example we consider the duality between the time and the Laplace domains. The differentia-
tion property indicates that computing the derivative of a function in the time domain corresponds
to multiplying by s the Laplace transform of the function (assuming initial conditions are zero).
We will illustrate in this example the dual of this—that is, when we differentiate a function in the
s-domain its effect in the time domain is to multiply by −t. Consider the connection between δ(t),
u(t), and r(t) (i.e., the unit impulse, the unit step, and the ramp, respectively), and relate it to the
indicated duality. Explain how this property connects with the existence of multiple poles, real and
complex, in general.
Solution
The relation between the signals δ(t), u(t), and r(t) is seen from
L[r(t)] = 1
s2
L

u(t) = dr(t)
dt

= s 1
s2 = 1
s
L

δ(t) = du(t)
dt

= s1
s = 1
which also shows that a double pole at the origin, 1/s2, corresponds to a ramp function r(t) = tu(t).

192
CHAPTER 3:
The Laplace Transform
The above results can be explained by looking for a dual of the derivative property. Multiplying by
−t the signal x(t) corresponds to differentiating X(s) with respect to s. Indeed for an integer N > 1,
dNX(s)
dsN
=
∞
Z
0
x(t)dNe−st
dsN dt
=
∞
Z
0
x(t)(−t)Ne−stdt
Thus, if x(t) = u(t), X(s) = 1/s, then −tx(t) has Laplace transform dX(s)/ds = −1/s2, or tu(t) and
1/s2 are Laplace transform pairs. In general, the Laplace transform of tN−1u(t), for N ≥1, has N
poles at the origin.
What about multiple real (different from zero) and multiple complex poles? What are the
corresponding inverse Laplace transforms? The inverse Laplace transform of
20s/(s2 + 2
0)2
having double complex poles at ±j0, is
t sin(0t)u(t)
Likewise,
te−atu(t)
has as Laplace transform 1/(s + a)2. So multiple poles correspond to multiplication by t in the
time domain.
I
I Example 3.10
Obtain from the Laplace transform of x(t) = cos(0t)u(t) the Laplace transform of sin(t)u(t) using
the derivative property.
Solution
The causal sinusoid
x(t) = cos(0t)u(t)
has a Laplace transform
X(s) =
s
s2 + 2
0
Then,
dx(t)
dt
= u(t)d cos(0t)
dt
+ cos(0t)du(t)
dt

3.3 The One-Sided Laplace Transform
193
= −0 sin(0t)u(t) + cos(0t)δ(t)
= −0 sin(0t)u(t) + δ(t)
so that the Laplace transform of dx(t)/dt is given by
sX(s) −x(0−) = −0L[sin(0t)u(t)] + L[δ(t)]
Thus, the Laplace transform of the sine is
L[sin(0t)u(t)] = −sX(s) −x(0−) −1
0
= 1 −sX(s)
0
=
0
s2 + 2
0
since x(0−) = 0 and X(s) = L[cos(oT)] given above.
I
Notice that whenever the signal is discontinuous at t = 0, as in the case of x(t) = cos(0t)u(t), its
derivative will include a δ(t) signal due to the discontinuity. On the other hand, whenever the signal
is continuous at t = 0, for instance y(t) = sin(0t)u(t), its derivative does not contain δ(t) signals. In
fact,
dy(t)
dt
= 0 cos(0t)u(t) + sin(0t)δ(t)
= 0 cos(0t)u(t)
since the sine is zero at t = 0.
3.3.3 Integration
The Laplace transform of the integral of a causal signal y(t) is given by
L


t
Z
0
y(τ)dτ u(t)

= Y(s)
s
(3.14)
This property can be shown by using the derivative property. Call the integral
f(t) =
t
Z
0
y(τ)dτu(t)
Using the fundamental theorem of calculus, we then have that
df(t)
dt
= y(t)u(t)

194
CHAPTER 3:
The Laplace Transform
and so
L
df(t)
dt

= sF(s) −f(0)
= Y(s)
since f(0) = 0 (the integral over a point), then
F(s) = L


t
Z
0
y(τ)dτ

= Y(s)
s
I Example 3.11
Suppose that
t
Z
0
y(τ)dτ = 3u(t) −2y(t)
Find the Laplace transform of y(t), a causal signal.
Solution
Applying the integration property gives
Y(s)
s
= 3
s −2Y(s)
so that solving for Y(s) we obtain
Y(s) =
3
2(s + 0.5)
corresponding to y(t) = 1.5e−0.5tu(t).
I
3.3.4 Time Shifting
If the Laplace transform of f(t)u(t) is F(s), the Laplace transform of the time-shifted signal f(t −τ)u(t −τ) is
L[f(t −τ)u(t −τ)] = e−τsF(s)
(3.15)
This indicates that when we delay (advance) the signal to get f(t −τ)u(t −τ) ( f(t + τ)u(t + τ)) its
corresponding Laplace transform is F(s) multiplied by e−τs (eτs). This property is easily shown by a
change of variable when computing the Laplace transform of the shifted signals.

3.3 The One-Sided Laplace Transform
195
I Example 3.12
Suppose we wish to ﬁnd the Laplace transform of the causal sequence of pulses x(t) shown in
Figure 3.12. Let x1(t) denote the ﬁrst pulse (i.e., for 0 ≤t < 1).
x(t)
x1(t)
t
0
1
t
· · ·
1
0
2
3
FIGURE 3.12
Generic causal pulse signal.
Solution
We have for t ≥0,
x(t) = x1(t) + x1(t −1) + x1(t −2) + · · ·
and 0 for t < 0. According to the shifting and linearity properties, we have
X(s) = X1(s)

1 + e−s + e−2s + · · ·

= X1(s)

1
1 −e−s

Notice that 1 + e−s + e−2s + · · · = 1/(1 −e−s), which is veriﬁed by cross-mutiplying:
[1 + e−s + e−2s + · · · ](1 −e−s) = (1 + e−s + e−2s + · · · ) −(e−s + e−2s + · · · ) = 1
The poles of X(s) are the poles of X1(s) and the roots of 1 −e−s = 0 (the s values such that e−s = 1,
or sk = ±j2πk for any integer k ≥0). Thus, there is an inﬁnite number of poles for X(s), and the
partial fraction expansion method that uses poles to invert Laplace transforms, presented later,
will not be useful. The reason this example is presented here, ahead of the inverse Laplace, is to
illustrate that when we are ﬁnding the inverse of this type of Laplace function we need to con-
sider the time-shift property, otherwise we would need to consider an inﬁnite partial fraction
expansion.
I
I Example 3.13
Consider the causal full-wave rectiﬁed signal shown in Figure 3.13. Find its Laplace transform.

196
CHAPTER 3:
The Laplace Transform
FIGURE 3.13
Full-wave rectiﬁed causal signal.
−1
0
1
2
3
4
5
6
7
−0.2
0
0.2
0.4
0.6
0.8
1
t
x(t)
Solution
The ﬁrst period of the full-wave rectiﬁed signal can be expressed as
x1(t) = sin(2πt)u(t) + sin(2π(t −0.5))u(t −0.5)
and its Laplace transform is
X1(s) = 2π(1 + e−0.5s)
s2 + (2π)2
And the train of these sinusoidal pulses
x(t) =
∞
X
k=0
x1(t −0.5k)
will then have the following Laplace transform:
X(s) = X1(s)[1 + e−s/2 + e−s + · · · ] = X1(s)
1
1 −e−s/2 =
2π(1 + e−s/2)
(1 −e−s/2)(s2 + 4π2)
I
3.3.5 Convolution Integral
Because this is the most important property of the Laplace transform we provide a more extensive
coverage later, after considering the inverse Laplace transform.
The Laplace transform of the convolution integral of a causal signal x(t), with Laplace transforms X(s), and a
causal impulse response h(t), with Laplace transform H(s), is given by
L[(x ∗h)(t)] = X(s)H(s)
(3.16)

3.4 Inverse Laplace Transform
197
If the input of an LTI system is the causal signal x(t) and the impulse response of the system is h(t),
then the output y(t) can be written as
y(t) =
∞
Z
0
x(τ)h(t −τ)dτ
t ≥0
and zero otherwise. Its Laplace transform is
Y(s) = L


∞
Z
0
x(τ)h(t −τ)dτ

=
∞
Z
0


∞
Z
0
x(τ)h(t −τ)dτ

e−stdt
=
∞
Z
0
x(τ)


∞
Z
0
h(t −τ) e−s(t−τ) dt

e−sτdτ = X(s)H(s)
where the internal integral is shown to be H(s) = L[h(t)] (change variable to ν = t −τ) using the
causality of h(t). The remaining integral is the Laplace transform of x(t).
The system function or transfer function H(s) = L[h(t)], the Laplace transform of the impulse response h(t) of
an LTI system, can be expressed as the ratio
H(s) = L[y(t)]
L[ x(t)] = L[ output ]
L[ input ]
(3.17)
This function is called transfer function because it transfers the Laplace transform of the input to the output.
Just as with the Laplace transform of signals, H(s) characterizes an LTI system by means of its poles and
zeros. Thus, it becomes a very important tool in the analysis and synthesis of systems.
3.4 INVERSE LAPLACE TRANSFORM
Inverting the Laplace transform consists in ﬁnding a function (either a signal or an impulse response
of a system) that has the given transform with the given region of convergence. We will consider three
cases:
I
Inverse of one-sided Laplace transforms giving causal functions.
I
Inverse of Laplace transforms with exponentials.
I
Inverse of two-sided Laplace transforms giving anti-causal or noncausal functions.
The given function X(s) we wish to invert can be the Laplace transform of a signal or a transfer
function—that is, the Laplace transform of an impulse response.
3.4.1 Inverse of One-Sided Laplace Transforms
When we consider a causal function x(t), the region of convergence of X(s) is of the form
{(σ, ) : σ > σmax, −∞<  < ∞}

198
CHAPTER 3:
The Laplace Transform
where σmax is the maximum of the real parts of the poles of X(s). Since in this section we only
consider causal signals, the region of convergence will be assumed and will not be shown with the
Laplace transform.
The most common inverse Laplace method is the so-called partial fraction expansion, which consists in
expanding the given function in s into a sum of components of which the inverse Laplace transforms
can be found in a table of Laplace transform pairs. Assume the signal we wish to ﬁnd has a rational
Laplace transform—that is,
X(s) = N(s)
D(s)
(3.18)
where N(s) and D(s) are polynomials in s with real-valued coefﬁcients. In order for the partial fraction
expansion to be possible, it is required that X(s) be proper rational, which means that the degree of
the numerator polynomial N(s) is less than that of the denominator polynomial D(s). If X(s) is not
proper, then we need to do long division until we obtain a proper rational function—that is,
X(s) = g0 + g1s + · · · + gmsm + B(s)
D(s)
(3.19)
where the degree of B(s) is now less than that of D(s)—so that we can perform partial expansion for
B(s)/D(s). The inverse of X(s) is then given by
x(t) = g0δ(t) + g1
dδ(t)
dt
+ · · · + gm
dmδ(t)
dtm
+ L−1
 B(s)
D(s)

(3.20)
The presence of δ(t) and its derivatives (called doublets, triplets, etc.) are very rare in actual signals,
and as such the typical rational function has a numerator polynomial that is of lower degree than the
denominator polynomial.
Remarks
I
Things to remember before performing the inversion are:
I
The poles of X(s) provide the basic characteristics of the signal x(t).
I
If N(s) and D(s) are polynomials in s with real coefﬁcients, then the zeros and poles of X(s) are real
and/or complex conjugate pairs, and can be simple or multiple.
I
In the inverse, u(t) should be included since the result of the inverse is causal—the function u(t) is an
integral part of the inverse.
I
The basic idea of the partial expansion is to decompose proper rational functions into a sum of rational
components of which the inverse transform can be found directly in tables. Table 3.1 displays common
one-sided Laplace transform pairs, while Table 3.2 provides properties of the one-sided Laplace transform.
We will consider now how to obtain a partial fraction expansion when the poles are real, simple and
multiple, and in complex conjugate pairs, simple and multiple.

3.4 Inverse Laplace Transform
199
Table 3.1 One-Sided Laplace Transforms
Function of Time
Function of s, ROC
1.
δ(t)
1,
whole s-plane
2.
u(t)
1
s , Re[s] > 0
3.
r(t)
1
s2 , Re[s] > 0
4.
e−atu(t), a > 0
1
s+a, Re[s] > −a
5.
cos(0t)u(t)
s
s2+2
0 , Re[s] > 0
6.
sin(0t)u(t)
0
s2+2
0 , Re[s] > 0
7.
e−at cos(0t)u(t), a > 0
s+a
(s+a)2+2
0 , Re[s] > −a
8.
e−at sin(0t)u(t), a > 0
0
(s+a)2+2
0 , Re[s] > −a
9.
2A e−at cos(0t + θ)u(t), a > 0
A∠θ
s+a−j0 +
A∠−θ
s+a+j0 , Re[s] > −a
10.
1
(N−1)! tN−1u(t)
1
sN N an integer, Re[s] > 0
11.
1
(N−1)! tN−1e−atu(t)
1
(s+a)N N an integer, Re[s] > −a
12.
2A
(N−1)! tN−1e−at cos(0t + θ)u(t)
A∠θ
(s+a−j0)N +
A∠−θ
(s+a+j0)N , Re[s] > −a
Table 3.2 Basic Properties of One-Sided Laplace Transforms
Causal functions and constants
αf(t), βg(t)
αF(s), βG(s)
Linearity
αf(t) + βg(t)
αF(s) + βG(s)
Time shifting
f(t −α)
e−αsF(s)
Frequency shifting
eαtf(t)
F(s −α)
Multiplication by t
t f(t)
−dF(s)
ds
Derivative
df(t)
dt
sF(s) −f(0−)
Second derivative
d2f(t)
dt2
s2F(s) −sf(0−) −f (1)(0)
Integral
R t
0−f(t′)dt
F(s)
s
Expansion/contraction
f(αt) α ̸= 0
1
|α|F
  s
α

Initial value
f(0+) = lims→∞sF(s)
Final value
limt→∞f(t) = lims→0 sF(s)
Simple Real Poles
If X(s) is a proper rational function
X(s) = N(s)
D(s) =
N(s)
Q
k(s −pk)
(3.21)

200
CHAPTER 3:
The Laplace Transform
where the {pk} are simple real poles of X(s), its partial fraction expansion and its inverse are given by
X(s) =
X
k
Ak
s −pk
⇔x(t) =
X
k
Akepktu(t)
(3.22)
where the expansion coefﬁcients are computed as
Ak = X(s)(s −pk)
s=pk
According to Laplace transform tables the time function corresponding to Ak/(s −pk) is Akepktu(t),
thus the form of the inverse x(t). To ﬁnd the coefﬁcients of the expansion, say Aj, we multiply both
sides of the Equation (3.22) by the corresponding denominator (s −pj) so that
X(s)(s −pj) = Aj +
X
k̸=j
Ak(s −pj)
s −pk
If we let s = pj, or s −pj = 0, in the above expression, all the terms in the sum will be zero and we
ﬁnd that
Aj = X(s)(s −pj)
s=pj
I Example 3.14
Consider the proper rational function
X(s) =
3s + 5
s2 + 3s + 2 =
3s + 5
(s + 1)(s + 2)
Find its causal inverse.
Solution
The partial fraction expansion is
X(s) =
A1
s + 1 +
A2
s + 2
Given that the two poles are real, the expected signal x(t) will be a superposition of two decaying
exponentials, with damping factors −1 and −2, or
x(t) = [A1e−t + A2e−t]u(t)
where as indicated above,
A1 = X(s)(s + 1)|s=−1 = 3s + 5
s + 2 |s=−1 = 2

3.4 Inverse Laplace Transform
201
and
A2 = X(s)(s + 2)|s=−2 = 3s + 5
s + 1 |s=−2 = 1
Therefore,
X(s) =
2
s + 1 +
1
s + 2
and as such
x(t) = [2e−t + e−2t]u(t)
To check that the solution is correct one could use the initial or the ﬁnal value theorems shown in
Table 3.2. According to the initial value theorem, x(0) = 3 should coincide with
lim
s→∞

sX(s) =
3s2 + 5s
s2 + 3s + 2

= lim
s→∞
3 + 5/s
1 + 3/s + 2/s2 = 3
as it does. The ﬁnal value theorem indicates that limt→∞x(t) = 0 should coincide with
lim
s→0

sX(s) =
3s2 + 5s
s2 + 3s + 2

= 0
as it does. Both of these validations seem to indicate that the result is correct.
I
Remarks The coefﬁcients A1 and A2 can be found using other methods. For instance,
I
We can compute
X(s) =
A1
s + 1 +
A2
s + 2
(3.23)
for two different values of s (as long as we do not divide by zero), such as s = 0 and s = 1,
s = 0
X(0) = 5
2 = A1 + 1
2A2
s = 1
X(1) = 8
6 = 1
2A1 + 1
3A2
which gives a set of two linear equations with two unknows, and applying Cramer’s rule we ﬁnd that
A1 = 2 and A2 = 1.
I
We cross-multiply the partial expansion given by Equation (3.23) to get
X(s) =
3s + 5
s2 + 3s + 2 = s(A1 + A2) + (2A1 + A2)
s2 + 3s + 2
Comparing the numerators, we have that A1 + A2 = 3 and 2A1 + A2 = 5, two equations with two
unknowns, which can be shown to have as unique solutions A1 = 2 and A2 = 1, as before.

202
CHAPTER 3:
The Laplace Transform
Simple Complex Conjugate Poles
The partial fraction expansion of a proper rational function
X(s) =
N(s)
(s + α)2 + 2
0
=
N(s)
(s + α −j0)(s + α + j0)
(3.24)
with complex conjugate poles {s1,2 = −α ± j0} is given by
X(s) =
A
s + α −j0
+
A∗
s + α + j0
where
A = X(s)(s + α −j0)|s=−α+j0 = |A|ejθ
so that the inverse is the function
x(t) = 2|A|e−αt cos(0t + θ)u(t)
(3.25)
Because the numerator and the denominator polynomials of X(s) have real coefﬁcients, the zeros
and poles whenever complex appear as complex conjugate pairs. One could thus think of the case
of a pair of complex conjugate poles as similar to the case of two simple real poles presented above.
Notice that the numerator N(s) must be a ﬁrst-order polynomial for X(s) to be proper rational. The
poles of X(s), s1,2 = −α ± j0, indicate that the signal x(t) will have an exponential e−αt, given that
the real part of the poles is −α, multiplied by a sinusoid of frequency 0, given that the imaginary
parts of the poles are ±0. We have the expansion
X(s) =
A
s + α −j0
+
A∗
s + α + j0
where the expansion coefﬁcients are complex conjugate of each other. From the pole information,
the general form of the inverse is
x(t) = Ke−αt cos(0t + 8)u(t)
for some constants K and 8. As before, we can ﬁnd A as
A = X(s)(s + α −j0)|s=−α+j0 = |A|ejθ
and that X(s)(s + α + j0)|s=−α−j0 = A∗can be easily veriﬁed. Then the inverse transform is given by
x(t) =
h
Ae−(α−j0)t + A∗e−(α+j0)ti
u(t)
= |A|e−αt(ej(0t+θ) + e−j(0t+θ))u(t)
= 2|A|e−αt cos(0t + θ)u(t).

3.4 Inverse Laplace Transform
203
Remarks
I
An equivalent partial fraction expansion consists in expressing the numerator N(s) of X(s), for some
constants a and b, as N(s) = a + b(s + α), a ﬁrst-order polynomial, so that
X(s) = a + b(s + α)
(s + α)2 + 2
0
= a
0
0
(s + α)2 + 2
0
+ b
s + α
(s + α)2 + 2
0
so that the inverse is a sum of a sine and a cosine multiplied by a decaying exponential. The inverse
Laplace transform is
x(t) =
 a
0
e−αt sin(0t) + be−αt cos(0t)

u(t)
which can be simpliﬁed, using the sum of phasors corresponding to sine and cosine, to
x(t) =
s
a2
2
0
+ b2 e−αt cos

0t −tan−1
 a
0b

u(t)
I
When α = 0 the above indicates that the inverse Laplace transform of
X(s) = a + bs
s2 + 2
0
is
x(t) =
s
a2
2
0
+ b2 cos

0t −tan−1
 a
0b

u(t)
which is transform of a cosine with a phase shift not commonly found in tables.
I
When the frequency 0 = 0, we get that the inverse Laplace transform of
X(s) = a + b(s + α)
(s + α)2
=
a
(s + α)2 +
b
s + α
(corresponds to a double pole at −α) is
x(t) = lim
0→0
 a
0
e−αt sin(0t) + be−αt cos(0t)

u(t)
= [ate−αt + be−αt]u(t)
where the ﬁrst limit is found by L’Hˆopital’s rule. Notice that when computing the partial fraction expansion
of the double pole s = −α the expansion is composed of two terms, one with denominator (s + α)2 and
the other with denominator s + α of which the sum gives a ﬁrst-order numerator and a second-order
denominator to satisfy the proper rational condition.

204
CHAPTER 3:
The Laplace Transform
I Example 3.15
Consider the Laplace function
X(s) =
2s + 3
s2 + 2s + 4 =
2s + 3
(s + 1)2 + 3
Find the corresponding causal signal x(t), then use MATLAB to validate your answer.
Solution
The poles are at −1 ± j
√
3, so that we expect that x(t) is a decaying exponential with a damping
factor of −1 (the real part of the poles) multiplied by a causal cosine of frequency
√
3. The partial
fraction expansion is of the form
X(s) =
2s + 3
s2 + 2s + 4 = a + b(s + 1)
(s + 1)2 + 3
so that 3 + 2s = (a + b) + bs, or b = 2 and a + b = 3 or a = 1. Thus,
X(s) =
1
√
3
√
3
(s + 1)2 + 3 + 2
s + 1
(s + 1)2 + 3
which corresponds to
x(t) =
 1
√
3
sin(
√
3t) + 2 cos(
√
3t)

e−tu(t)
The value x(0) = 2 and according to the initial value theorem the following limit should equal it:
lim
s→∞

sX(s) =
2s2 + 3s
s2 + 2s + 4

= lim
s→∞
2 + 3/s
1 + 2/s + 4/s2 = 2
which is the case, indicating the result is probably correct (satisfying the initial value theorem is
not enough to indicate the result is correct, but if it does not the result is wrong).
We use the MATLAB function ilaplace to compute symbolically the inverse Laplace transform and
plot the response using ezplot, as shown in the following script.
%%%%%%%%%%%%%%%%%
% Example 3.15
%%%%%%%%%%%%%%%%%
clear all; clf
syms s t w
num = [0 2 3]; den = [1 2 4]; % coefﬁcients of numerator and denominator
subplot(121)
splane(num,den) % plotting poles and zeros
disp(’>>>>> Inverse Laplace <<<<<’)
x = ilaplace((2 * s + 3)/(s ˆ 2 + 2 * s + 4)); % inverse Laplace transform
subplot(122)

3.4 Inverse Laplace Transform
205
−2
0
2
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
σ
jΩ
0
5
10
−0.5
0
0.5
1
1.5
2
2.5
t
x(t)
(a)
(b)
FIGURE 3.14
Inverse Laplace transform of X(s) = (2s + 3)/(s2 + 2s + 4): (a) poles and zeros and (b) inverse x(t).
ezplot(x,[0,12]); title(’x(t)’)
axis([0 12 -0.5 2.5]); grid
The results are shown in Figure 3.14.
I
Double Real Poles
If a proper rational function has double real poles
X(s) =
N(s)
(s + α)2 = a + b(s + α)
(s + α)2
=
a
(s + α)2 +
b
s + α
(3.26)
then its inverse is
x(t) = [ate−αt + be−αt]u(t)
(3.27)
where a can be computed as
a = X(s)(s + α)2 |s=−α
After replacing it, b is found by computing X(s0) for a value s0 ̸= α.

206
CHAPTER 3:
The Laplace Transform
When we have double real poles we need to express the numerator N(s) as a ﬁrst-order polynomial,
just as in the case of a pair of complex conjugate poles. The values of a and b can be computed in
different ways, as we illustrate in the following examples.
I Example 3.16
Typically, the Laplace transforms appear as combinations of the different terms we have consid-
ered, for instance a combination of ﬁrst- and second-order poles gives
X(s) =
4
s(s + 2)2
which has a pole at s = 0 and a double pole at s = −2. Find the causal signal x(t). Use MATLAB to
plot the poles and zeros of X(s) and to ﬁnd the inverse Laplace transform x(t).
Solution
The partial fraction expansion is
X(s) = A
s + a + b(s + 2)
(s + 2)2
The value of A = X(s)s|s=0 = 1, and so
X(s) −1
s = 4 −(s + 2)2
s(s + 2)2
= −(s + 4)
(s + 2)2
= a + b(s + 2)
(s + 2)2
Comparing the numerators of X(s) −1/s and the one in the partial fraction expansion gives b = −1
and a + 2b = −4 or a = −2. We then have
X(s) = 1
s + −2 −(s + 2)
(s + 2)2
so that
x(t) = [1 −2te−2t −e−2t]u(t)
Another way to do this type of problem is to express X(s) as
X(s) = A
s +
B
(s + 2)2 +
C
s + 2
We ﬁnd the A as before, and then ﬁnd B by multiplying both sides by (s + 2)2 and letting s = −2,
which gives
X(s)(s + 2)2|s=−2 =
A(s + 2)2
s
+ B + C(s + 2)

s=−2
so that
B = X(s)(s + 2)2|s=−2

3.4 Inverse Laplace Transform
207
FIGURE 3.15
Inverse Laplace
transform of
X(s) = 4/(s(s + 2)2):
(a) poles and zeros and
(b) x(t).
−2
0
2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
σ
jΩ
0
5
10
0
0.2
0.4
0.6
0.8
1
t
x(t)
(a)
(b)
To ﬁnd C we compute the partial fraction expansion for a value of s for which no division by zero
is possible. For instance, if we let s = 1 we can ﬁnd the value of C, after which we can ﬁnd the
inverse.
The initial value x(0) = 0 coincides with
lim
s→∞

sX(s) =
4s
s(s + 2)2

= lim
s→∞
4/s2
(1 + 2/s)2 = 0
To ﬁnd the inverse Laplace transform with MATLAB we use a similar script to the one used before;
only the numerator and denominator description needs to be changed. The plots are shown in
Figure 3.15.
I
I Example 3.17
Find the inverse Laplace transform of the function
X(s) =
4
s((s + 1)2 + 3)
which has a simple real pole s = 0, and complex conjugate poles s = −1 ± j
√
3.

208
CHAPTER 3:
The Laplace Transform
Solution
The partial fraction expansion is
X(s) =
A
s + 1 −j
√
3
+
A∗
s + 1 + j
√
3
+ B
s
We then have
B = sX(s)|s=0 = 1
A = X(s)(s + 1 −j
√
3)|s=−1+j
√
3 = 0.5

−1 +
j
√
3

=
1
√
3
∠150◦
so that
x(t) =
2
√
3
e−t cos(
√
3t + 150◦)u(t) + u(t)
= −[cos(
√
3t) + 0.577 sin(
√
3t)]e−tu(t) + u(t)
I
Remarks
I
Following the above development, when the poles are complex conjugate and double the procedure for the
double poles is repeated. Thus, the partial expansion is given as
X(s) =
N(s)
(s + α −j0)2(s + α + j0)2
= a + b(s + α −j0)
(s + α −j0)2
+ a∗+ b∗(s + α + j0)
(s + α + j0)2
(3.28)
so that ﬁnding a and b we obtain the inverse.
I
The partial fraction expansion for second- and higher-order poles should be done with MATLAB.
I Example 3.18
In this example we use MATLAB to ﬁnd the inverse Laplace transform of more complicated func-
tions than the ones considered before. In particular, we want to illustrate some of the additional
information that our function pfeLaplace gives. Consider the Laplace transform
X(s) =
3s2 + 2s −5
s3 + 6s2 + 11s + 6
Find poles and zeros of X(s), and obtain the coefﬁcients of its partial fraction expansion (also
called the residues). Use ilaplace to ﬁnd its inverse and plot it using ezplot.
Solution
The following is the function pfeLaplace.

3.4 Inverse Laplace Transform
209
function pfeLaplace(num,den)
%
disp(’>>>>> Zeros <<<<<’)
z = roots(num)
[r,p,k]=residue(num,den);
disp(’>>>>> Poles <<<<<’)
p
disp(’>>>>> Residues <<<<<’)
r
splane(num,den)
The function pfeLaplace uses the MATLAB function roots to ﬁnd the zeros of X(s) deﬁned by the coef-
ﬁcients of its numerator and denominator given in descending order of s. For the partial fraction
expansion, pfeLaplace uses the MATLAB function residue, which ﬁnds coefﬁcients of the expansion
as well as the poles of X(s). (The residue r(i) in the vector r corresponds to the expansion term for
the pole p(i); for instance, the residue r(1) = 8 corresponds to the expansion term corresponding
to the pole p(1) = −3.) The symbolic function ilaplace is then used to ﬁnd the inverse x(t); as input
to ilaplace the function X(s) is described in a symbolic way. The MATLAB function ezplot is used for
the plotting of the symbolic computations.
The analytic results are shown in the following, and the plot of x(t) is given in Figure 3.16.
>>>>> Zeros <<<<<
z = -1.6667
1.0000
>>>>> Poles <<<<<
p = -3.0000
-2.0000
-1.0000
>>>>> Residues <<<<<
r = 8.0000
-3.0000
-2.0000
>>>>> Inverse Laplace <<<<<
x = 8 * exp(-3 * t)-3 * exp(-2 * t)-2 * exp(-t)
I
3.4.2 Inverse of Functions Containing e−ρs Terms
When X(s) has exponentials e−ρs in the numerator or denominator, ignore these terms and perform partial
fraction expansion on the rest, and at the end consider the exponentials to get the correct time shifting.
In particular, when
X(s) =
N(s)
D(s)(1 −e−αs) = N(s)
D(s) + N(s)e−αs
D(s)
+ N(s)e−2αs
D(s)
+ · · ·

210
CHAPTER 3:
The Laplace Transform
FIGURE 3.16
Inverse Laplace
transform of
X(s) = (3s2 + 2s −5)/
(s3 + 6s2 + 11s + 6).
(a) Poles and zeros of
X(s) are given with (b)
the corresponding
inverse x(t).
−4
−2
0
2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
σ
jΩ
(a)
0
5
10
−1
−0.5
0
0.5
1
1.5
2
2.5
3
t
(b)
x (t)
if f(t) is the inverse of N(s)/D(s), then
x(t) = f(t) + f(t −α) + f(t −2α) + · · ·
Another possibility is when the function is given as
X(s) =
N(s)
D(s)(1 + e−αs) = N(s)
D(s) −N(s)e−αs
D(s)
+ N(s)e−2αs
D(s)
−· · ·
If f(t) is the inverse of N(s)/D(s), we then have
x(t) = f(t) −f(t −α) + f(t −2α) −· · ·
The time-shifting property of Laplace makes it possible for the numerator N(s) or the denominator
D(s) to have e−σs terms. The procedure for inverting such functions is to initially ignore these terms
and do the partial fraction expansion on the rest and at the end consider them to do the necessary
time shifting. For instance, the inverse of
X(s) = e s −e−s
s
= e s
s −e−s
s

3.4 Inverse Laplace Transform
211
is obtained by ﬁrst considering the term 1/s, which has u(t) as inverse, and then using the information
in the numerator to get the ﬁnal response,
x(t) = u(t + 1) −u(t −1)
The two sums
∞
X
k=0
e−αsk =
1
1 −e−αs
∞
X
k=0
(−e−αs)k =
1
1 + e−αs
can be easily veriﬁed by cross-multiplying. So when the function is
X1(s) =
N(s)
D(s)(1 −e−αs) = N(s)
D(s)
∞
X
k=0
e−αsk
= N(s)
D(s) + N(s)e−αs
D(s)
+ N(s)e−2αs
D(s)
+ · · ·
and if f(t) is the inverse of N(s)/D(s), we then have
x1(t) = f(t) + f(t −α) + f(t −2α) + · · ·
Likewise, when
X2(s) =
N(s)
D(s)(1 + e−αs) = N(s)
D(s)
∞
X
k=0
(−1)ke−αsk
= N(s)
D(s) −N(s)e−αs
D(s)
+ N(s)e−2αs
D(s)
−· · ·
if f(t) is the inverse of N(s)/D(s), we then have
x2(t) = f(t) −f(t −α) + f(t −2α) −· · ·
I Example 3.19
We wish to ﬁnd the causal inverse of
X(s) =
1 −e−s
(s + 1)(1 −e−2s)
Solution
We let
X(s) = F(s)
∞
X
k=0
(e−2s)k

212
CHAPTER 3:
The Laplace Transform
where
F(s) = 1 −e−s
s + 1
The inverse of F(s) is
f(t) = e−tu(t) −e−(t−1)u(t −1)
and the inverse of X(s) is thus given by
x(t) = f(t) + f(t −2) + f(t −4) + · · ·
I
3.4.3 Inverse of Two-Sided Laplace Transforms
When ﬁnding the inverse of a two-sided Laplace transform we need to pay close attention to the
region of convergence and to the location of the poles with respect to the j axis. Three regions of
convergence are possible:
I
A plane to the right of all the poles, which corresponds to a causal signal.
I
A plane to the left of all poles, which corresponds to an anti-causal signal.
I
A region that is in between poles on the right and poles on the left (no poles included in it),
which corresponds to a two-sided signal.
If the j axis is included in the region of convergence, bounded-input bounded-output (BIBO) sta-
bility of the system, or absolute integrability of the impulse response of the system, is guaranteed.
Furthermore, the system with that region of convergence would have a frequency response, and the
signal a Fourier transform. The inverses of the causal and the anti-causal components are obtained
using the one-sided Laplace transform.
I Example 3.20
Find the inverse Laplace transform of
X(s) =
1
(s + 2)(s −2)
ROC: −2 < Re(s) < 2
Solution
The ROC −2 < Re(s) < 2 is equivalent to {(σ, ) : −2 < σ < 2, −∞<  < ∞}. The partial
fraction expansion is
X(s) =
1
(s + 2)(s −2) = −0.25
s + 2 + 0.25
s −2
−2 < Re(s) < 2
where the ﬁrst term with the pole at s = −2 corresponds to a causal signal with a region of con-
vergence Re(s) > −2, and the second term corresponds to an anti-causal signal with a region of

3.4 Inverse Laplace Transform
213
convergence Re(s) < 2. That this is so is conﬁrmed by the intersection of these two regions of
convergence that gives
[Re(s) > −2] ∩[Re(s) < 2] = −2 < Re(s) < 2
As such, we have
x(t) = −0.25e−2tu(t) −0.25e2tu(−t)
I
I Example 3.21
Consider the transfer function
H(s) =
s
(s + 2)(s −1) = 2/3
s + 2 + 1/3
s −1
with a zero at s = 0, and poles at s = −2 and s = 1. Find out how many impulse responses can be
connected with H(s) by considering different possible regions of convergence and by determining
in which cases the system with H(s) as its transfer function is BIBO stable.
Solution
The following are the different possible impulse responses:
I
If ROC: Re(s) > 1, the impulse response
h1(t) = (2/3)e−2tu(t) + (1/3)etu(t)
corresponding to H(s) with this region of convergence is causal. The corresponding system is
unstable—due to the pole in the right-hand s-plane, which will make the impulse response
grow as t increases.
I
If ROC: −2 < Re(s) < 1, the impulse response corresponding to H(s) with this region of
convergence is noncausal, but the system is stable. The impulse response would be
h2(t) = (2/3)e−2tu(t) −(1/3)etu(−t)
Notice that the region of convergence includes the j axis, and this guarantees the stability
(verify that h2(t) is absolutely integrable), and as we will see later, also the existence of the
Fourier transform of h2(t).
I
If ROC: Re(s) < −2, the impulse response in this case would be anti-causal, and the system is
unstable (please verify it), as the impulse response is
h3(t) = −(2/3)e−2tu(−t) −(1/3)etu(−t)
I
Two very important generalizations of the results in this example are:
I
An LTI with a transfer function H(s) and region of convergence R is BIBO stable if the j axis is contained
in the region of convergence.

214
CHAPTER 3:
The Laplace Transform
I
If the system is BIBO stable and causal, then the region of convergence includes the j axis so that the
frequency response H(j) exists, and all the poles of H(s) are in the open left-hand s-plane (the j axis is
not included).
3.5 ANALYSIS OF LTI SYSTEMS
Dynamic linear time-invariant systems are typically represented by differential equations. Using the
derivative property of the one-sided Laplace transform (allowing the inclusion of initial conditions)
and the inverse transformation, differential equations are changed into easier-to-solve algebraic equa-
tions. The convolution integral is not only a valid alternate representation for systems represented by
differential equations, but for other systems. The Laplace transform provides a very efﬁcient com-
putational method for the convolution integral. More important, the convolution property of the
Laplace transform introduces the concept of transfer function, a very efﬁcient representation of LTI
systems whether they are represented by differential equations or not. In Chapter 6, we will present
applications of the material in this section to classic control theory.
3.5.1 LTI Systems Represented by Ordinary Differential Equations
Two ways to characterize the response of a causal and stable LTI system are:
I
Zero-state and zero-input responses, which have to do with the effect of the input and the initial
conditions of the system.
I
Transient and steady-state responses, which have to do with close and faraway behavior of the
response.
The complete response y(t) of a system represented by an Nth-order linear differential equation with constant
coefﬁcients,
y(N)(t) +
N−1
X
k=0
aky(k)(t) =
M
X
ℓ=0
bℓx(ℓ)(t)
N > M
(3.29)
where x(t) is the input and y(t) is the output of the system, and initial conditions
{y(k)(t), 0 ≤k ≤N −1}
(3.30)
is obtained by inverting the Laplace transform
Y(s) = B(s)
A(s)X(s) +
1
A(s)I(s)
(3.31)
where Y(s) = L[y(t)], X(s) = L[x(t)], and
A(s) =
N
X
k=0
aksk
aN = 1
B(s) =
M
X
ℓ=0
bℓsℓ

3.5 Analysis of LTI Systems
215
I(s) =
N
X
k=1
ak


k−1
X
m=0
sk−m−1y(m)(0)


That is, I(s) depends on the initial conditions.
The notation y(k)(t) and x(ℓ)(t) indicates the kth and the ℓth derivatives of y(t) and of x(t), respectively
(it is to be understood that y(0)(t) = y(t) and likewise x(0)(t) = x(t) in this notation). The assump-
tion N > M avoids the presence of δ(t) and its derivatives in the solution, which are realistically not
possible. To obtain the complete response y(t) we compute the Laplace transform of Equation (3.29):
" N
X
k=0
aksk
#
|
{z
}
A(s)
Y(s) =
" M
X
ℓ=0
bℓsℓ
#
|
{z
}
B(s)
X(s) +
N
X
k=1
ak


k−1
X
m=0
s(k−1)−my(m)(0)


|
{z
}
I(s)
which can be written as
A(s)Y(s) = B(s)X(s) + I(s)
(3.32)
by deﬁning A(s), B(s), and I(s) as indicated above. Solving for Y(s) in Equation (3.32), we have
Y(s) = B(s)
A(s)X(s) +
1
A(s)I(s)
and ﬁnding its inverse we obtain the complete response y(t).
Letting
H(s) = B(s)
A(s)
and H1(s) =
1
A(s)
the complete response y(t) = L−1[Y(s)] of the system is obtained by the inverse Laplace transform of
Y(s) = H(s)X(s) + H1(s)I(s)
(3.33)
which gives
y(t) = yzs(t) + yzi(t)
(3.34)
where
zero-state response:
yzs(t) = L−1[H(s)X(s)]
zero-input response:
yzi(t) = L−1[H1(s)I(s)]
In terms of convolution integrals,
y(t) =
t
Z
0
x(τ)h(t −τ)dτ +
t
Z
0
i(τ)h1(t −τ)dτ
(3.35)

216
CHAPTER 3:
The Laplace Transform
where h(t) = L−1[H(s)] and h1(t) = L−1[H1(s)], and
i(t) = L−1[I(s)] =
N
X
k=1
ak


k−1
X
m=0
y(m)(0)δ(k−m−1)(t)


where {δ(m)(t)} are mth derivatives of the impulse signal δ(t) (as indicated before, δ(0)(t) = δ(t)).
Zero-State and Zero-Input Responses
Despite the fact that linear differential equations, with constant coefﬁcients, do not represent linear
systems unless the initial conditions are zero and the input is causal, linear system theory is based on
these representations with initial conditions. Typically, the input is causal so it is the initial conditions
not always being zero that causes problems. This can be remedied by a different way of thinking
about the initial conditions. In fact, one can think of the input x(t) and the initial conditions as two
different inputs to the system, and apply superposition to ﬁnd the responses to these two different
inputs. This deﬁnes two responses. One is due completely to the input, with zero initial conditions,
called the zero-state solution. The other component of the complete response is due exclusively to the
initial conditions, assuming that the input is zero, and is called the zero-input solution.
Remarks
I
It is important to recognize that to compute the transfer function of the system
H(s) = Y(s)
X(s)
according to Equation (3.33) requires that the initial conditions be zero, or I(s) = 0.
I
If there is no pole-zero cancellation, both H(s) and H1(s) have the same poles, as both have A(s) as
denominator, and as such h(t) and h1(t) might be similar.
Transient and Steady-State Responses
Whenever the input of a causal and stable system has poles in the closed left-hand s-plane, poles in
the j-axis being simple, the complete response will be bounded. Moreover, whether the response
exists as t →∞can then be determined without using the inverse Laplace transform.
The complete response y(t) of an LTI system is made up of transient and steady-state components.
The transient response can be thought of as the system’s reaction to the initial inertia after applying
the input, while the steady-state response is how the system reacts to the input away from the initial
time when the input starts.
If the poles (simple or multiple, real or complex) of the Laplace transform of the output, Y(s), of an LTI system
are in the open left-hand s-plane (i.e., no poles on the j axis), the steady-state response is
yss(t) = lim
t→∞y(t) = 0

3.5 Analysis of LTI Systems
217
In fact, for any real pole s = −α, α > 0, of multiplicity m ≥1, we have that
L−1

N(s)
(s + α)m

=
m
X
k=1
Aktk−1e−αtu(t)
where N(s) is a polynomial of degree less or equal to m −1. Clearly, for any value of α > 0 and any
order m ≥1, the above inverse will tend to zero as t increases. The rate at which these terms go to zero
depends on how close the pole(s) is (are) to the j axis: The farther away, the faster the term goes to
zero. Likewise, complex conjugate pairs of poles with a negative real part also give terms that go to
zero as t →∞, independent of their order. For complex conjugate pairs of poles s1,2 = −α ± j0 of
order m ≥1, we have
L−1
"
N(s)
((s + α)2 + 2
0)m
#
=
m
X
k=1
2|Ak|tk−1e−αt cos(0t + ∠(Ak))u(t)
where again N(s) is a polynomial of degree less or equal to 2m −1. Due to the decaying exponentials
this type of term will go to zero as t goes to inﬁnity.
Simple complex conjugate poles and a simple real pole at the origin of the s-plane cause a steady-state
response. Indeed, if the pole of Y(s) is s = 0 we know that its inverse transform is of the form Au(t),
and if the poles are complex conjugates ±j0 the corresponding inverse transform is a sinusoid—
neither of which is transient. However, multiple poles on the j-axis, or any poles in the right-hand s-plane
will give inverses that grow as t →∞. This statement is clear for the poles in the right-hand s-plane. For
double- or higher-order poles in the j axis their inverse transform is of the form
L−1
"
N(s)
(s2 + 2
0)m
#
=
m
X
k=1
2|Ak|tm−1 cos(0 + ∠(Ak))u(t)
which will continuously grow as t increases.
In summary, when solving differential equations—with or without initial conditions—we have
I
The steady-state component of the complete solution is given by the inverse Laplace transforms of the
partial fraction expansion terms of Y(s) that have simple poles (real or complex conjugate pairs) in the
j-axis.
I
The transient response is given by the inverse transform of the partial fraction expansion terms with poles
in the left-hand s-plane, independent of whether the poles are simple or multiple, real or complex.
I
Multiple poles in the j axis and poles in the right-hand s-plane give terms that will increase as t increases.
I Example 3.22
Consider a second-order (N = 2) differential equation,
d2y(t)
dt2
+ 3dy(t)
dt
+ 2y(t) = x(t)

218
CHAPTER 3:
The Laplace Transform
Assume the above equation represents a system with input x(t) and output y(t). Find the impulse
response h(t) and the unit-step response s(t) of the system.
Solution
If the initial conditions are zero, computing the two- or one-sided Laplace transform of the two
sides of this equation, after letting Y(s) = L[y(t)] and X(s) = L[x(t)], and using the derivative
property of Laplace, we get
Y(s)[s2 + 3s + 2] = X(s)
To ﬁnd the impulse response of this system (i.e., the system response y(t) = h(t)), we let x(t) = δ(t)
and the initial condition be zero. Since X(s) = 1, then Y(s) = H(s) = L[h(t)] is
H(s) =
1
s2 + 3s + 2 =
1
(s + 1)(s + 2) =
A
s + 1 +
B
s + 2
We obtain values A = 1 and B = −1, and the inverse Laplace transform is then
h(t) =

e−t −e−2t
u(t)
which is completely transient.
In a similar form we obtain the unit-step response s(t), by letting x(t) = u(t) and the initial
conditions be zero. Calling Y(s) = S(s) = L[s(t)], since X(s) = 1/s, we obtain
S(s) = H(s)
s
=
1
s(s2 + 3s + 2) = A
s +
B
s + 1 +
C
s + 2
It is found that A = 1/2, B = −1, and C = 1/2, so that
s(t) = 0.5u(t) −e−tu(t) + 0.5e−2tu(t)
The steady state of s(t) is 0.5 as the two exponentials go to zero. Interestingly, the relation sS(s) =
H(s) indicates that by computing the derivative of s(t) we obtain h(t). Indeed,
ds(t)
dt
= 0.5δ(t) + e−tu(t) −e−tδ(t) −e−2tu(t) + 0.5e−2tδ(t)
= [0.5 −1 + 0.5]δ(t) + [e−t −e−2t]u(t)
= [e−t −e−2t]u(t) = h(t)
I
Remarks
I
Because the existence of the steady-state response depends on the poles of Y(s) it is possible for an unstable
causal system (recall that for such a system BIBO stability requires all the poles of the system transfer
function be in the open, left-hand s-plane) to have a steady-state response. It all depends on the input.
Consider, for instance, an unstable system with H(s) = 1/(s(s + 1)), being unstable due to the pole at

3.5 Analysis of LTI Systems
219
s = 0; if the system input is x1(t) = u(t) so that X1(s) = 1/s, then Y1(s) = 1/(s2(s + 1)). There will be
no steady state because of the double pole s = 0. On the other hand, X2(s) = s/(s + 2)2 will give
Y2(s) = H(s)X2(s) =
1
s(s + 1)
s
(s + 2)2 =
1
(s + 1)(s + 2)2
which will give a zero steady state, even though the system is unstable. This is possible because of the
pole-zero cancellation.
I
The steady-state response is the response of the system away from t = 0, and it can be found by letting
t →∞(even though the steady state can be reached at ﬁnite times, depending on how fast the transient
goes to zero). In Example 3.22, the steady-state response of h(t) = (e−t −e−2t)u(t) is zero, while for
s(t) = 0.5u(t) −e−tu(t) + 0.5e−2tu(t) it is 0.5. The transient responses are then h(t) −0 = h(t) and
s(t) −0.5u(t) = −e−tu(t) + 0.5e−2tu(t). These transients eventually disappear.
I
The relation found between the impulse response h(t) and the unit-step response s(t) can be extended to
more cases by the deﬁnition of the transfer function—that is, H(s) = Y(s)/X(s) so that the response Y(s)
is connected with H(s) by Y(s) = H(s)X(s), giving the relation between y(t) and h(t). For instance, if
x(t) = δ(t), then Y(s) = H(s) × 1, with inverse the impulse response. If x(t) = u(t), then Y(s) = H(s)/s
is S(s), the Laplace transform of the unit-step response, and so s(t) = dh(t)/dt. And if x(t) = r(t), then
Y(s) = H(s)/s2 is ρ(s), the Laplace transform of the ramp response, and so ρ(t) = d2h(t)/dt2 = ds(t)/dt.
I Example 3.23
Consider again the second-order differential equation in the previous example,
d2y(t)
dt2
+ 3dy(t)
dt
+ 2y(t) = x(t)
but now with initial conditions y(0) = 1 and dy(t)/dt|t=0 = 0, and x(t) = u(t). Find the complete
response y(t). Could we ﬁnd the impulse response h(t) from this response? How could we do it?
Solution
The Laplace transform of the differential equation gives
[s2Y(s) −sy(0) −dy(t)
dt
t=0] + 3[sY(s) −y(0)] + 2Y(s) = X(s)
Y(s)(s2 + 3s + 2) −(s + 3) = X(s)
so we have that
Y(s) =
X(s)
(s + 1)(s + 2) +
s + 3
(s + 1)(s + 2)
=
1 + 3s + s2
s(s + 1)(s + 2) = B1
s +
B2
s + 1 +
B3
s + 2

220
CHAPTER 3:
The Laplace Transform
after replacing X(s) = 1/s. We ﬁnd that B1 = 1/2, B2 = 1, and B3 = −1/2, so that the complete
response is
y(t) = [0.5 + e−t −0.5e−2t]u(t)
(3.36)
Again, we can check that this solution satisﬁes the initial condition y(0) and dy(0)/dt (this
is particularly interesting to see, try it!). The steady-state response is 0.5 and the transient
[e−t −0.5e−2t]u(t).
According to Equation (3.36), the complete solution y(t) is composed of the zero-state response,
due to the input only, and the response due to the initial conditions only or the zero-input
response. Thus, the system considers two different inputs: One that is x(t) = u(t) and the other
the initial conditions.
If we are able to ﬁnd the transfer function H(s) = Y(s)/X(s) its inverse Laplace transform would be
h(t). However that is not possible when the initial conditions are nonzero. As shown above, in the
case of nonzero initial conditions, we get that the Laplace transform is
Y(s) = X(s)
A(s) + I(s)
A(s)
where in this case A(s) = (s + 1)(s + 2) and I(s) = s + 3, and thus we cannot ﬁnd the ratio
Y(s)/X(s). If we make the second term zero (i.e., I(s) = 0), we then have that Y(s)/X(s) = H(s) =
1/A(s) and h(t) = e−tu(t) −e−2tu(t).
I
I Example 3.24
Consider an analog averager represented by
y(t) = 1
T
t
Z
t−T
x(τ)dτ
(3.37)
where x(t) is the input and y(t) is the output. The derivative of y(t) gives the ﬁrst-order differential
equation
dy(t)
dt
= 1
T [x(t) −x(t −T)]
with a ﬁnite difference for the input. Let us ﬁnd the impulse response of this analog averager.
Solution
The impulse response of the averager is found by letting x(t) = δ(t) and the initial condition be
zero. Computing the Laplace transform of the two sides of the differential equation, we obtain
sY(s) = 1
T [1 −e−sT]X(s)

3.5 Analysis of LTI Systems
221
and substituting X(s) = 1, then
H(s) = Y(s) = 1
sT [1 −e−sT]
The impulse response is then
h(t) = 1
T [u(t) −u(t −T)].
I
3.5.2 Computation of the Convolution Integral
From the point of view of signal processing, the convolution property is the most important
application of the Laplace transform to systems. The computation of the convolution integral is
difﬁcult even for simple signals. In Chapter 2 we showed how to obtain the convolution integral
analytically as well as graphically. As we will see in this section, it is not only that the convolution
property of the Laplace transform gives an efﬁcient solution to the computation of the convolution
integral, but that it introduces an important representation of LTI systems, namely the transfer func-
tion of the system. A system, like signals, is thus represented by the poles and zeros of the transfer
function. But it is not only the pole-zero characterization of the system that can be obtained from the
transfer function. The system’s impulse response is uniquely obtained from the poles and zeros of
the transfer function and the corresponding region of convergence. The way the system responds to
different frequencies will be also given by the transfer function. Stability and causality of the system
can be equally related to the transfer function. Design of ﬁlters depends on the transfer function.
The Laplace transform of the convolution y(t) = [x ∗h](t) is given by the product
Y(s) = X(s)H(s)
(3.38)
where X(s) = L[x(t)] and H(s) = L[h(t)]. The transfer function of the system H(s) is deﬁned as
H(s) = L[h(t)] = Y(s)
X(s)
(3.39)
H(s) transfers the Laplace transform X(s) of the input into the Laplace transform of the output Y(s). Once Y(s)
is found, y(t) is computed by means of the inverse Laplace transform.
I Example 3.25
Use the Laplace transform to ﬁnd the convolution y(t) = [x ∗h](t) when
(1)
the input is x(t) = u(t) and the impulse response is a pulse h(t) = u(t) −u(t −1), and
(2)
the input and the impulse response of the system are x(t) = h(t) = u(t) −u(t −1).
Solution
I
The Laplace transforms are X(s) = L[u(t)] = 1/s and H(s) = L[h(t)] = (1 −e−s)/s, so that
Y(s) = H(s)X(s) = 1 −e−s
s2

222
CHAPTER 3:
The Laplace Transform
Its inverse is
y(t) = r(t) −r(t −1)
where r(t) is the ramp signal. This result coincides with the one obtained graphically in
Example 2.12 in Chapter 2.
I
In the second case, X(s) = H(s) = L[u(t) −u(t −1)] = (1 −e−s)/s, so that
Y(s) = H(s)X(s) = (1 −e−s)2
s2
= 1 −2e−s + e−2s
s2
which corresponds to
y(t) = r(t) −2r(t −1) + r(t −2)
or a triangular pulse as we obtained graphically in Example 2.13 in Chapter 2.
I
I Example 3.26
To illustrate the signiﬁcance of the Laplace approach in computing the output of an LTI system by
means of the convolution integral, consider an RLC circuit in series with input a voltage source
x(t) and as output the voltage y(t) across the capacitor (see Figure 3.17). Find its impulse response
h(t) and its unit-step response s(t). Let LC = 1 and R/L = 2.
Solution
The RLC circuit is represented by a second-order differential equation given that the inductor and
the capacitor are capable of storing energy and their initial conditions are not dependent on each
other. To obtain the differential equation we apply Kirchhoff’s voltage law (KVL)
x(t) = Ri(t) + Ldi(t)
dt
+ y(t)
where i(t) is the current through the resistor, inductor and capacitor and where the voltage across
the capacitor is given by
y(t) = 1
C
t
Z
0
i(σ)dσ + y(0)
FIGURE 3.17
RLC circuit with input a voltage source
x(t) and output the voltage across the
capacitor y(t).
−
+
x(t)
R
L
C
y(t)
+
−

3.5 Analysis of LTI Systems
223
with y(0) the initial voltage in the capacitor and i(t) the current through the resistor, inductor, and
capacitor. The above two equations are called integro-differential given that they are composed of
an integral equation and a differential equation. To obtain a differential equation in terms of the
input x(t) and the output y(t), we ﬁnd the ﬁrst and second derivative of y(t), which gives
dy(t)
dt
= 1
Ci(t)
⇒
i(t) = Cdy(t)
dt
d2y(t)
dt2
= 1
C
di(t)
dt
⇒Ldi(t)
dt
= LCd2y(t)
dt2
which when replaced in the KVL equation gives
x(t) = RCdy(t)
dt
+ LCd2y(t)
dt2
+ y(t)
(3.40)
which, as expected, is a second-order differential equation with two initial conditions: y(0), the
initial voltage in the capacitor, and i(0) = Cdy(t)/dt|t=0, the initial current in the inductor. To ﬁnd
the impulse response of this circuit, we let x(t) = δ(t) and the initial conditions be zero. The Laplace
transform of Equation (3.40) gives
X(s) = [LCs2 + RCs + 1]Y(s)
The impulse response of the system is the inverse Laplace transform of the transfer function
H(s) = Y(s)
X(s) =
1/LC
s2 + (R/L)s + 1/LC
If LC = 1 and R/L = 2, then the transfer function is
H(s) =
1
(s + 1)2
which corresponds to the impulse response
h(t) = te−tu(t)
Now that we have the impulse response of the system, suppose then the input is a unit-step signal,
x(t) = u(t). To ﬁnd its response we consider the convolution integral
y(t) =
∞
Z
−∞
x(τ)h(t −τ)dτ
=
∞
Z
−∞
u(τ)(t −τ)e−(t−τ)u(t −τ)dτ
=
t
Z
0
(t −τ)e−(t−τ)dτ
= [1 −e−t(1 + t)]u(t)

224
CHAPTER 3:
The Laplace Transform
which satisﬁes the initial conditions and attempts to follow the input signal. This is the unit-step
response.
In the Laplace domain, the above can be easily computed as follows. From the transfer function,
we have that
Y(s) = H(s)X(s)
=
1
(s + 1)2
1
s
where we replaced the transfer function and the Laplace transform of x(t) = u(t). The partial
fraction expansion of Y(s) is then
Y(s) = A
s +
B
s + 1 +
C
(s + 1)2
and after obtaining that A = 1, C = −1, and B = −1, we get
y(t) = s(t) = u(t) −e−tu(t) −te−tu(t)
which coincides with the solution of the convolution integral. It has been obtained, however, in a
much easier way.
I
I Example 3.27
Consider the positive feedback system created by a microphone close to a set of speakers that are
putting out an ampliﬁed acoustic signal (see Figure 3.18), which we considered in Example 2.18
in Chapter 2. Find the impulse response of the system using the Laplace transform, and use it to
express the output in terms of a convolution. Determine the transfer function and show that the
system is not BIBO stable. For simplicity, let β = 1, τ = 1, and x(t) = u(t). Connect the location of
the poles of the transfer function with the unstable behavior of the system.
Solution
As we indicated in Example 2.18 in Chapter 2, the impulse response of a feedback system cannot
be explicitly obtained in the time domain, but it can be done using the Laplace transform. The
input–output equation for the positive feedback is
y(t) = x(t) + βy(t −τ)
FIGURE 3.18
Positive feedback created by closeness
of a microphone to a set of speakers.
+
×
x(t)
y(t)
Delay τ
βy (t −τ)
β

3.5 Analysis of LTI Systems
225
If we let x(t) = δ(t), the output is y(t) = h(t) or
h(t) = δ(t) + βh(t −τ)
and if H(s) = L[h(t)], then the Laplace transform of the above equation is H(s) = 1 + βH(s)e−sτ or
solving for H(s):
H(s) =
1
1 −βe−sτ =
1
1 −e−s
=
∞
X
k=0
e−sk = 1 + e−s + e−2s + e−3s + · · ·
after replacing the given values for β and τ. The impulse response h(t) is the inverse Laplace
transform of H(s) or
h(t) = δ(t) + δ(t −1) + δ(t −2) + · · · =
∞
X
k=0
δ(t −k)
If x(t) is the input, the output is given by the convolution integral
y(t) =
∞
Z
−∞
x(t −τ)h(τ)dτ =
∞
Z
−∞
∞
X
k=0
δ(τ −k)x(t −τ)dτ
=
∞
X
k=0
∞
Z
−∞
δ(τ −k)x(t −τ)dτ =
∞
X
k=0
x(t −k)
and replacing x(t) = u(t), we get
y(t) =
∞
X
k=0
u(t −k)
which tends to inﬁnity as t increases.
For this system to be BIBO stable, the impulse response h(t) must be absolutely integrable, which
is not the case for this system. Indeed,
∞
Z
−∞
|h(t)|dt =
∞
Z
−∞
∞
X
k=0
δ(t −k)dt
=
∞
X
k=0
∞
Z
−∞
δ(t −k)dt =
∞
X
k=0
1 →∞
The poles of H(s) are the roots of 1 −e−s = 0, which are the values of s such that e−sk = 1 = ej2πk or
sk = ±j2πk. That is, there is an inﬁnite number of poles on the j axis, indicating that the system
is not BIBO stable.
I

226
CHAPTER 3:
The Laplace Transform
3.6 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO FROM
HERE?
In this chapter you have learned the signiﬁcance of the Laplace transform in the representation of
signals as well as of systems. The Laplace transform provides a complementary representation to the
time representation of a signal, so that damping and frequency, poles and zeros, together with regions
of convergence, conform a new domain for signals. But it is more than that—you will see that these
concepts will apply for the rest of this part of the book. When discussing the Fourier analysis of signals
and systems we will come back to the Laplace domain for computational tools and for interpretation.
The solution of differential equations and the different types of responses are obtained algebraically
with the Laplace transform. Likewise, the Laplace transform provides a simple and yet very signiﬁcant
solution to the convolution integral. It also provides the concept of transfer function, which will be
fundamental in analysis and synthesis of linear time-invariant systems.
The common thread of the Laplace and the Fourier transforms is the eigenfunction property of LTI
systems. You will see that understanding this property will provide you with the needed insight into
the Fourier analysis, which we will cover in the next two chapters.
PROBLEMS
3.1. Generic signal representation and the Laplace transform
The generic representation of a signal x(t) in terms of impulses is
x(t) =
∞
Z
−∞
x(τ)δ(t −τ)dτ
Considering the integral an inﬁnite sum of terms x(τ)δ(t −τ) (think of x(τ) as a constant, as it is not a
function of time t), ﬁnd the Laplace transform of each of these terms and use the linearity property to ﬁnd
X(s) = L[x(t)]. Are you surprised at this result?
3.2. Impulses and the Laplace transform
Given
x(t) = 2[δ(t + 1) + δ(t −1)]
(a) Find the Laplace transform X(s) of x(t) and determine its region of convergence.
(b) Plot x(t).
(c) The function X(s) is complex. Let s = σ + j and carefully obtain the magnitude |X(σ + j)| and the
phase ∠X(σ + j).
3.3. Sinusoids and the Laplace transform
Consider the following cases involving sinusoids:
(a) Find the Laplace transform of y(t) = sin(2πt)u(t) −sin(2π(t −1))u(t −1)) and its region of conver-
gence. Carefully plot y(t). Determine the region of convergence of Y(s).
(b) A very smooth pulse, called the raised cosine, x(t) is obtained as
x(t) = 1 −cos(2πt)
0 ≤t ≤1

Problems
227
and zero elsewhere. The raised cosine is used in communications to transmit signals with minimal
interference. Find its Laplace transform and its corresponding region of convergence.
(c) Indicate three possible approaches to ﬁnding the Laplace transform of cos2(t)u(t). Use two of these
approaches to ﬁnd the Laplace transform.
3.4. Unit-step signals and the Laplace transform
Find the Laplace transform of the reﬂection of the unit-step signal (i.e., u(−t)) and its region of conver-
gence. Then use the result together with the Laplace transform of u(t) to see if you can obtain the Laplace
transform of a constant or u(t) + u(−t) (assume u(0) = 0.5 so there is no discontinuity at t = 0).
3.5. Laplace transform of noncausal signal
Consider the noncausal signal
x(t) = e−|t|u(t + 1)
Carefully plot it, and ﬁnd its Laplace transform X(s) by separating x(t) into a causal signal and an anti-
causal signal, xc(t) and xac(t), respectively, and plot them separately. Find the ROC of X(s), Xc(s), and
Xac(s).
3.6. Transfer function and differential equation
The transfer function of a causal LTI system is
H(s) =
1
s2 + 4
(a) Find the differential equation that relates the input x(t) and the output y(t) of the system.
(b) Suppose we would like the output y(t) to be identically zero for t greater or equal to zero. If we let
x(t) = δ(t), what would the initial conditions be equal to?
3.7. Transfer function
The input to an LTI system is
x(t) = u(t) −2u(t −1) + u(t −2)
If the Laplace transform of the output is given by
Y(s) = (s + 2)(1 −e−s)2)
s2(s + 1)2
determine the transfer function of the system.
3.8. Inverse Laplace transform—MATLAB
Consider the following inverse Laplace transform problems for a causal signal x(t):
(a) Given the Laplace transform
X(s) =
s4 + 2s + 1
s3 + 4s2 + 5s + 2
which is not proper, determine the amplitude of the δ(t) and dδ(t)/dt terms in the inverse signal x(t).
(b) Find the inverse Laplace transform of
X(s) =
s2 −3
(s + 1)(s + 2)
Can you use the initial-value theorem to check your result? Explain.

228
CHAPTER 3:
The Laplace Transform
(c) The inverse Laplace transform of
X(s) =
3s −4
s(s + 1)(s + 2)
should give a response of the form
x(t) = [Ae−t + B + Ce−2t]u(t)
Find the values of A, B, and C. Use the MATLAB function ilaplace to get the inverse.
3.9. Steady state and transient
Consider the following cases where we want to determine either the steady state, transient, or both.
(a) Without computing the inverse of the Laplace transform
X(s) =
1
s(s2 + 2s + 10)
corresponding to a causal signal x(t), determine its steady-state solution. What is the value of x(0)?
Show how to obtain this value without computing the inverse Laplace transform.
(b) The Laplace transform of the output of an LTI system is
Y(s) =
1
s((s + 2)2 + 1)
What would be the steady-state response yss(t)?
(c) The Laplace transform of the output of an LTI system is
Y(s) =
e−s
s((s −2)2 + 1)
How would you determine if there is a steady state or not? Explain.
(d) The Laplace transform of the output of an LTI system is
Y(s) =
s + 1
s((s + 1)2 + 1)
Determine the steady-state and the transient responses corresponding to Y(s).
3.10. Inverse Laplace transformation—MATLAB
Consider the following inverse Laplace problems using MATLAB for causal signal x(t):
(a) Use MATLAB to compute the inverse Laplace transform of
X(s) =
s2 + 2s + 1
s(s + 1)(s2 + 10s + 50)
and determine the value of x(t) in the steady state. How would you be able to obtain this value without
computing the inverse? Explain
(b) Find the poles and zeros of
X(s) = (1 −se−s)
s(s + 2)
Find the inverse Laplace transform x(t) (use MATLAB to verify your result).

Problems
229
3.11. Convolution integral
Consider the following problems related to the convolution integral:
(a) The impulse response of an LTI system is h(t) = e−2tu(t) and the system input is a pulse x(t) = u(t) −
u(t −3). Find the output of the system y(t) by means of the convolution integral graphically and by
means of the Laplace transform.
(b) It is known that the impulse response of an analog averager is h(t) = u(t) −u(t −1). Consider the
input to the averager x(t) = u(t) −u(t −1), and determine graphically as well as by means of the
Laplace transform the corresponding output of the averager y(t) = [h ∗x](t). Is y(t) smoother than
the input signal x(t)? Provide an argument for your answer.
(c) Suppose we cascade three analog averagers each with the same impulse response h(t) = u(t) −u(t −
1). Determine the transfer function of this system. If the duration of the support of the input to the ﬁrst
averager is M sec, what would be the duration of the support of the output of the third averager?
3.12. Deconvolution
In convolution problems the impulse response h(t) of the system and the input x(t) are given and one
is interested in ﬁnding the output of the system y(t). The so-called ”deconvolution” problem consists in
giving two of x(t), h(t), and y(t) to ﬁnd the other. For instance, given the output y(t) and the impulse
response h(t) of the system, one wants to ﬁnd the input. Consider the following cases:
(a) Suppose the impulse response of the system is h(t) = e−t cos(t)u(t) and the output has a Laplace
transform
Y(s) =
4
s((s + 1)2 + 1)
What is the input x(t)?
(b) The output of an LTI system is y(t) = r(t) −2r(t −1) + r(t −2), where r(t) is the ramp signal.
Determine the impulse response of the system if it is known that the input is x(t) = u(t) −u(t −1).
3.13. Application of superposition
One of the advantages of LTI systems is the superposition property. Suppose that the transfer function of
a LTI system is
H(s) =
s
s2 + s + 1
Find the unit-step response s(t) of the system, and then use it to ﬁnd the response due to the following
inputs:
x1(t) = u(t) −u(t −1)
x2(t) = δ(t) −δ(t −1)
x3(t) = r(t)
x4(t) = r(t) −2r(t −1) + r(t −2)
Express the responses yi(t) due to xi(t) for i = 1, . . . , 4 in terms of the unit-step response s(t).
3.14. Properties of the Laplace transform
Consider computing the Laplace transform of a pulse
p(t) =
(
1
0 ≤t ≤1
0
otherwise

230
CHAPTER 3:
The Laplace Transform
(a) Use the integral formula to ﬁnd P(s), the Laplace transform of p(t). Determine the region of
convergence of P(s).
(b) Represent p(t) in terms of the unit-step function and use its Laplace transform and the time-shift
property to ﬁnd P(s). Find the poles and zeros of P(s) to verify the region of convergence obtained
above.
3.15. Frequency-shift property
Duality occurs between time and frequency shifts. As shown, if L[x(t)] = X(s), then L[x(t −t0)] =
X(s)e−t0s. The dual of this would be L[x(t)e−αt] = X(s + α), which we call the frequency-shift property.
(a) Use the integral formula for the Laplace transform to show the frequency-shift property.
(b) Use the above frequency-shift property to ﬁnd X(s) = L[x(t) = cos(0t)u(t)] (represent the cosine
using Euler’s identity). Find and plot the poles and zeros of X(s).
(c) Recall the deﬁnition of the hyperbolic cosine, cosh(0t) = 0.5(e0t + e−0t), and ﬁnd the Laplace
transform Y(s) of y(t) = cosh(0t)u(t). Find and plot the poles and zeros of Y(s). Explain the relation
of the poles of X(s) and Y(s) by connecting x(t) with y(t).
3.16. Poles and zeros
Consider the pulse x(t) = u(t) −u(t −1).
(a) Find the zeros and poles of X(s) and plot them.
(b) Suppose x(t) is the input of an LTI system with a transfer function H(s) = 1/(s2 + 4π2). Find and plot
the poles and zeros of Y(s) = L[y(t)] = H(s)X(s) where y(t) is the output of the system.
(c) If the transfer function of the LTI system is
G(s) = Z(s)
X(s) =
∞
Y
k=1
1
s2 + (2kπ)2
and the input is the above signal x(t), compute the output z(t).
3.17. Poles and zeros—MATLAB
The poles corresponding to the Laplace transform X(s) of a signal x(t) are
p1,2 = −3 ± jπ/2
p3 = 0
(a) Within some constants, give a general form of the signal x(t).
(b) Let
X(s) =
1
(s + 3 −jπ/2)(s + 3 −jπ/2)s
From the location of the poles, obtain a general form for x(t). Use MATLAB to ﬁnd x(t) and plot it. How
well did you guess the answer?
3.18. Solving differential equations—MATLAB
One of the uses of the Laplace transform is the solution of differential equations.
(a) Suppose you are given the differential equation that represents an LTI system,
y(2)(t) + 0.5y(1)(t) + 0.15y(t) = x(t)
t ≥0
where y(t) is the output and x(t) is the input of the system, and y(1)(t) and y(2)(t) are ﬁrst- and second-
order derivatives with respect to t. The input is causal, (i.e., x(t) = 0 t < 0). What should the initial
conditions be for the system to be LTI? Find Y(s) for those initial conditions.

Problems
231
(b) If y(1)(0) = 1 and y(0) = 1 are the initial conditions for the above differential equation, ﬁnd Y(s). If
the input to the system is doubled—that is, the input is 2x(t) with Laplace transform 2X(s)—is Y(s)
doubled so that its inverse Laplace transform y(t) is doubled? Is the system linear?
(c) Use MATLAB to ﬁnd the poles and zeros and the solutions of the differential equation when the
input is u(t) and 2u(t) with the initial conditions given above. Compare the solutions and verify your
response in (b).
3.19. Differential equation, initial conditions, and impulse response—MATLAB
The following function Y(s) = L[y(t)] is obtained applying the Laplace transform to a differential equation
representing a system with nonzero initial conditions and input x(t), with Laplace transform X(s):
Y(s) =
X(s)
s2 + 2s + 3 +
s + 1
s2 + 2s + 3
(a) Find the differential equation in y(t) and x(t) representing the system.
(b) Find the initial conditions y′(0) and y(0).
(c) Use MATLAB to determine the impulse response h(t) of this system. Find the poles of the transfer
function H(s) and determine if the system is BIBO stable.
3.20. Different responses—MATLAB
Let Y(s) = L[y(t)] be the Laplace transform of the solution of a second-order differential equation
representing a system with input x(t) and some initial conditions,
Y(s) =
X(s)
s2 + 2s + 1 +
s + 1
s2 + 2s + 1
(a) Find the zero-state response (response due to the input only with zero initial conditions) for x(t) = u(t).
(b) Find the zero-input response (response due to the initial conditions and zero input).
(c) Find the complete response when x(t) = u(t).
(d) Find the transient and the steady-state response when x(t) = u(t).
(e) Use MATLAB to verify the above responses.
3.21. Poles and stability
The transfer function of a BIBO-stable system has poles only on the open left-hand s-plane (excluding the
j axis).
(a) Let the transfer function of a system be
H1(s) = Y(s)
X(s) =
1
(s + 1)(s −2)
and let X(s) be the Laplace transform of signals that are bounded (i.e., the poles of X(s) are on the
left-hand s-plane). Find limt→∞y(t). Determine if the system is BIBO stable. If not, determine what
makes the system unstable.
(b) Let the transfer function be
H2(s) = Y(s)
X(s) =
1
(s + 1)(s + 2)
and X(s) be as indicated above. Find
lim
t→∞y(t)
Can you use this limit to determine if the system is BIBO stable? If not, what would you do to check
its stability?

232
CHAPTER 3:
The Laplace Transform
3.22. Poles, stability, and steady-state response
The steady-state solution of stable systems is due to simple poles in the j axis of the s-plane coming from
the input. Suppose the transfer function of the system is
H(s) = Y(s)
X(s) =
1
(s + 1)2 + 4
(a) Find the poles and zeros of H(s) and plot them in the s-plane. Find then the corresponding impulse
response h(t). Determine if the impulse response of this system is absolutely integrable so that the
system is BIBO stable.
(b) Let the input x(t) = u(t). Find y(t) and from it determine the steady-state solution.
(c) Let the input x(t) = tu(t). Find y(t) and from it determine the steady-state response. What is the
difference between this case and the previous one?
(d) To explain the behavior in the case above consider the following: Is the input x(t) = tu(t) bounded?
That is, is there some ﬁnite value M such that |x(t)| < M for all times? So what would you expect the
output to be knowing that the system is stable?
3.23. Responses from an analog averager
The input–output equation for an analog averager is given by
y(t) = 1
T
t
Z
t−T
x(τ)dτ
where x(t) is the input and y(t) is the output. This equation corresponds to the convolution integral.
(a) Change the above equation so that you can determine from it the impulse response h(t).
(b) Graphically determine the output y(t) corresponding to a pulse input x(t) = u(t) −u(t −2) using the
convolution integral (let T = 1) relating the input and the output. Carefully plot the input and the
output. (The output can also be obtained intuitively from a good understanding of the averager.)
(c) Using the impulse response h(t) found above, use now the Laplace transform to ﬁnd the output
corresponding to x(t) = u(t) −u(t −2). Let again T = 1 in the averager.
3.24. Transients for second-order systems—MATLAB
The type of transient you get in a second-order system depends on the location of the poles of the system.
The transfer function of the second-order system is
H(s) = Y(s)
X(s) =
1
s2 + b1s + b0
and let the input be x(t) = u(t).
(a) Let the coefﬁcients of the denominator of H(s) be b1 = 5 and b0 = 6. Find the response y(t). Use
MATLAB to verify the response and to plot it.
(b) Suppose then that the denominator coefﬁcients of H(s) are changed to b1 = 2 and b0 = 6. Find the
response y(t). Use MATLAB to verify the response and to plot it.
(c) Explain your results above by relating your responses to the location of the poles of H(s).
3.25. Effect of zeros on the sinusoidal steady state
To see the effect of the zeros on the complete response of a system, suppose you have a system with a
transfer function
H(s) = Y(s)
X(s) =
s2 + 4
s((s + 1)2 + 1)

Problems
233
(a) Find and plot the poles and zeros of H(s). Is this BIBO stable?
(b) Find the frequency 0 of the input x(t) = 2 cos(0t)u(t) such that the output of the given system is
zero in the steady state. Why do you think this happens?
(c) If the input is a sine instead of a cosine, would you get the same result as above? Explain why or why
not.
3.26. Zero steady-state response of analog averager—MATLAB
The analog averager can be represented by the differential equation
dy(t)
dt
= 1
T [x(t) −x(t −T)]
where y(t) is the output and x(t) is the input.
(a) If the input–output equation of the averager is
y(t) = 1
T
t
Z
t−T
x(τ)dτ
show how to obtain the above differential equation and that y(t) is the solution of the differential
equation.
(b) If x(t) = cos(πt)u(t), choose the value of T in the averager so that the output is y(t) = 0 in the steady
state. Graphically show how this is possible for your choice of T. Is there a unique value for T that
makes this possible? How does it relate to the frequency 0 = π of the sinusoid?
(c) Use the impulse response h(t) of the averager found before, to show using Laplace that the steady
state is zero when x(t) = cos(πt)u(t) and T is the above chosen value. Use MATLAB to solve the
differential equation and to plot the response for the value of T you chose. (Hint: Consider x(t)/T the
input and use superposition and time invariance to ﬁnd y(t) due to (x(t) −x(t −T))/T.)
3.27. Partial fraction expansion—MATLAB
Consider the following functions Yi(s) = L[yi(t)], i = 1, 2 and 3:
Y1(s) =
s + 1
s(s2 + 2s + 4)
Y2(s) =
1
(s + 2)2
Y3(s) =
s −1
s2((s + 1)2 + 9)
where {yi(t), i = 1, 2, 3} are the complete responses of differential equations with zero initial conditions.
(a) For each of these functions, determine the corresponding differential equation, if all of them have as
input x(t) = u(t).
(b) Find the general form of the complete response {yi(t), i = 1, 2, 3} for each of the {Yi(s) i = 1, 2, 3}. Use
MATLAB to plot the poles and zeros for each of the {Yi(s)}, to ﬁnd their partial fraction expansions,
and the complete responses.
3.28. Iterative convolution integral—MATLAB
Consider the convolution of a pulse x(t) = u(t + 0.5) −u(t −0.5) with itself many times. Use MATLAB for
the calculations and the plotting.
(a) Consider the result for N = 2 of these convolutions—that is,
y2(t) = (x ∗x)(t)

234
CHAPTER 3:
The Laplace Transform
Find Y2(s) = L[y2(t)] using the convolution property of the Laplace transform and ﬁnd y2(t).
(b) Consider then the result for N = 3 of these convolutions—that is,
y3(t) = (x ∗x ∗x)(t)
Find Y3(s) = L[y3(t)] using the convolution property of the Laplace transform and ﬁnd y3(t).
(c) The signal x(t) can be considered the impulse response of an averager that ”smooths” out a signal.
Letting y1(t) = x(t), plot the three functions yi(t) for i = 1, 2, and 3. Compare these signals on their
smoothness and indicate their supports in time. (For y2(t) and y3(t), how do their supports relate to
the supports of the signals convolved?)
3.29. Positive and negative feedback
There are two types of feedback, negative and positive. In this problem we explore their difference.
(a) Consider negative feedback. Suppose you have a system with transfer function H(s) = Y(s)/E(s)
where E(s) = C(s) −Y(s), and C(s) and Y(s) are the transforms of the feedback system’s reference
c(t) and output y(t). Find the transfer function of the overall system G(s) = Y(s)/C(s).
(b) In positive feedback, the only equation that changes is E(s) = C(s) + Y(s); the other equations remain
the same. Find the overall feedback system transfer function G(s) = Y(s)/C(s).
(c) Suppose that C(s) = 1/s, H(s) = 1/(s + 1). Determine G(s) for both negative and positive feedback.
Find y(t) = L−1[Y(s)] for both types of feedback and comment on the difference in these signals.
3.30. Feedback stabilization
An unstable system can be stabilized by using negative feedback with a gain K in the feedback loop. For
instance, consider an unstable system with transfer function
H(s) =
2
s −1
which has a pole in the right-hand s-plane, making the impulse response of the system h(t) grow as t
increases. Use negative feedback with a gain K > 0 in the feedback loop, and put H(s) in the forward
loop. Draw a block diagram of the system. Obtain the transfer function G(s) of the feedback system and
determine the value of K that makes the overall system BIBO stable (i.e., its poles in the open left-hand
s-plane).
3.31. All-pass stabilization
Another stabilization method consists in cascading an all-pass system with the unstable system to cancel
the poles in the right-hand s-plane. Consider a system with a transfer function
H(s) =
s + 1
(s −1)(s2 + 2s + 1)
which has a pole in the right-hand s-plane, s = 1, so it is unstable.
(a) The poles and zeros of an all-pass ﬁlter are such that if p12 = −σ ± j0 are complex conjugate poles
of the ﬁlter, then z12 = σ ± j0 are the corresponding zeros, and for real poles p = −σ there is a
corresponding z = σ. The orders of the numerator and the denominator of the all-pass ﬁlter are equal.
Write the general transfer function of an all-pass ﬁlter Hap(s) = KN(s)/D(s).
(b) Find an all-pass ﬁlter Hap(s) so that when cascaded with H(s) the overall transfer function G(s) =
H(s)Hap(s) has all its poles in the left-hand s-plane.
(c) Find K of the all-pass ﬁlter so that when s = 0 the all-pass ﬁlter has a gain of unity. What is the relation
between the magnitude of the overall system |G(s)| and that of the unstable ﬁlter |H(s)|.
3.32. Half-wave rectiﬁer—MATLAB
In the generation of DC from AC voltage, the ”half-wave” rectiﬁed signal is an important part. Suppose the
AC voltage is x(t) = sin(2πt)u(t).

Problems
235
(a) Carefully plot the half-wave rectiﬁed signal y(t) from x(t).
(b) Let y1(t) be the period of y(t) between 0 ≤t ≤1. Show that y1(t) can be written as
y1(t) = sin(2πt)u(t) + sin(2π(t −0.5))u(t −0.5)
or
y1(t) = sin(2πt)[u(t) −u(t −0.5)]
Use MATLAB to verify this. Find the Laplace transform X1(s) of x1(t).
(c) Express y(t) in terms of y1(t) and ﬁnd the Laplace transform Y(s) of y(t).
3.33. Polynomial multiplication—MATLAB
When the numerator or denominator is given in a factorized form, we need to multiply polynomials.
Although this can be done by hand, MATLAB provides the function conv that computes the coefﬁcients
of the polynomial resulting from the product of two polynomials.
(a) Use help in MATLAB to ﬁnd how conv can be used, and then consider two polynomials
P(s) = s2 + s + 1 and Q(s) = 2s3 + 3s2 + s + 1
Do the multiplication of these polynomials by hand to ﬁnd Z(s) = P(s)Q(s) and use conv to verify your
results.
(b) The output of a system has a Laplace transform
Y(s) = N(s)
D(s) =
(s + 2)
s2(s + 1)((s + 4)2) + 9)
Use conv to ﬁnd the denominator polynomial and then ﬁnd the inverse Laplace transform using
ilaplace.
3.34. Feedback error—MATLAB
Consider a negative feedback system used to control a plant G(s) = 1/(s(s + 1)(s + 2)). The output y(t) of
the feedback system is connected via a sensor with transfer function H(s) = 1 to a differentiator where
the reference signal x(t) is also connected. The output of the differentiator is the feedback error e(t) =
x(t) −v(t) where v(t) is the output of the feedback sensor.
(a) Carefully draw the feedback system, and ﬁnd an expression for E(s), the Laplace transform of the
feedback error e(t).
(b) Two possible reference test signals for the given plant are x(t) = u(t) and x(t) = r(t). Choose the one
that would give a zero steady-state feedback error.
(c) Use MATLAB to do the partial fraction expansions for the two error functions E1(s), corresponding
to when x(t) = u(t) and E2(s) when x(t) = r(t). Use these partial fraction expansions to ﬁnd e1(t) and
e2(t), and thus verify your results obtained before.

This page intentionally left blank

CHAPTER 4
Frequency Analysis: The Fourier
Series
A Mathematician is a device for
turning coffee into theorems.
Paul Erdos (1913–1996)
mathematician
4.1 INTRODUCTION
In this chapter and the next we consider the frequency analysis of continuous-time signals and
systems—the Fourier series for periodic signals in this chapter, and the Fourier transform for both
periodic and aperiodic signals as well as for systems in Chapter 5. In these chapters we consider:
I
Spectral representation—The frequency representation of periodic and aperiodic signals indicates
how their power or energy is allocated to different frequencies. Such a distribution over frequency
is called the spectrum of the signal. For a periodic signal the spectrum is discrete, as its power
is concentrated at frequencies multiples of a so-called fundamental frequency, directly related to
the period of the signal. On the other hand, the spectrum of an aperiodic signal is a contin-
uous function of frequency. The concept of spectrum is similar to the one used in optics for
light, or in material science for metals, each indicating the distribution of power or energy over
frequency. The Fourier representation is also useful in ﬁnding the frequency response of linear
time-invariant systems, which is related to the transfer function obtained with the Laplace trans-
form. The frequency response of a system indicates how an LTI system responds to sinusoids of
different frequencies. Such a response characterizes the system and permits easy computation of
its steady-state response, and will be equally important in the synthesis of systems.
I
Eigenfunctions and Fourier analysis—It is important to understand the driving force behind the
representation of signals in terms of basic signals when applied to LTI systems. For instance, the
convolution integral that gives the output of an LTI system resulted from the representation of its
input signal in terms of shifted impulses. Along with this result came the concept of the impulse
response of an LTI system. Likewise, the Laplace transform can be seen as the representation of
signals in terms of general eigenfunctions. In this chapter and the next we will see that complex
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00007-7
c⃝2011, Elsevier Inc. All rights reserved.
237

238
CHAPTER 4:
Frequency Analysis: The Fourier Series
exponentials or sinusoids are used in the Fourier representation of periodic as well as aperiodic
signals by taking advantage of the eigenfunction property of LTI systems. The results of the Fourier
series in this chapter will be extended to the Fourier transform in Chapter 5.
I
Steady-state analysis—Fourier analysis is in the steady state, while Laplace analysis considers both
transient and steady state. Thus, if one is interested in transients, as in control theory, Laplace is
a meaningful transformation. On the other hand, if one is interested in the frequency analysis,
or steady state, as in communications theory, the Fourier transform is the one to use. There will
be cases, however, where in control and communications both Laplace and Fourier analysis are
considered.
I
Application of Fourier analysis—The frequency representation of signals and systems is extremely
important in signal processing and in communications. It explains ﬁltering, modulation of mes-
sages in a communication system, the meaning of bandwidth, and how to design ﬁlters. Likewise,
the frequency representation turns out to be essential in the sampling of analog signals—the
bridge between analog and digital signal processing.
4.2 EIGENFUNCTIONS REVISITED
As indicated in Chapter 3, the most important property of stable LTI systems is that when the input
is a complex exponential (or a combination of a cosine and a sine) of a certain frequency, the output
of the system is the input times a complex constant connected with how the system responds to the
frequency at the input. The complex exponential is called an eigenfunction of stable LTI systems.
If x(t) = ej0t, −∞< t < ∞, is the input to a causal and a stable system with impulse response h(t), the
output in the steady state is given by
y(t) = ej0tH( j0)
(4.1)
where
H( j0) =
∞
Z
0
h(τ)e−j0τ dτ
(4.2)
is the frequency response of the system at 0. The signal x(t) = ej0t is said to be an eigenfunction of the LTI
system as it appears at both input and output.
This can be seen by ﬁnding the output corresponding to x(t) = ej0t by means of the convolution
integral,
y(t) =
∞
Z
0
h(τ)x(t −τ)dτ = ej0t
∞
Z
0
h(τ)e−j0τdτ
= ej0tH( j0)

4.2 Eigenfunctions Revisited
239
where we let H( j0) equal the integral in the second equation. The input signal appears in the output
modiﬁed by the frequency response of the system H( j0) at the frequency 0 of the input. Notice
that the convolution integral limits indicate that the input started at −∞and that we are considering
the output at ﬁnite time t—this means that we are in steady state. The steady-state response of a
stable LTI system is attained by either considering that the initial time when the input is applied to
the system is −∞and we reach a ﬁnite time t, or by starting at time 0 and going to ∞.
The above result for one frequency can be easily extended to the case of several frequencies present
at the input. If the input signal x(t) is a linear combination of complex exponentials, with different
amplitudes, frequencies, and phases, or
x(t) =
X
k
Xkejkt
where Xk are complex values, since the output corresponding to Xkejkt is XkejktH( jk) by
superposition the response to x(t) is
y(t) =
X
k
XkejktH( jk)
=
X
k
Xk|H( jk)|ej(kt+∠H( jk))
(4.3)
The above is valid for any signal that is a combination of exponentials of arbitrary frequencies. As we
will see in this chapter, when x(t) is periodic it can be represented by the Fourier series, which is a
combination of complex exponentials harmonically related (i.e., the frequencies of the exponentials
are multiples of the fundamental frequency of the periodic signal). Thus, when a periodic signal is
applied to a causal and stable LTI system its output is computed as in Equation (4.3).
The signiﬁcance of the eigenfunction property is also seen when the input signal is an integral
(a sum, after all) of complex exponentials, with continuously varying frequency, as the integrand.
That is, if
x(t) = 1
2π
∞
Z
−∞
X()ejtd
then using superposition and the eigenfunction property of a stable LTI system, with frequency
response H( j), the output is
y(t) = 1
2π
∞
Z
−∞
X()ejtH( j)d
= 1
2π
∞
Z
−∞
X()|H( j)|e( jt+j∠H( j))d
(4.4)

240
CHAPTER 4:
Frequency Analysis: The Fourier Series
The above representation of x(t) corresponds to the Fourier representation of aperiodic signals, which
will be covered in Chapter 5. Again here, the eigenfunction property of LTI systems provides an efﬁ-
cient way to compute the output. Furthermore, we also ﬁnd that by letting Y() = X()H( j) the
above equation gives an expression to compute y(t) from Y(). The product Y() = X()H( j) cor-
responds to the Fourier transform of the convolution integral y(t) = x(t) ∗h(t), and is connected with
the convolution property of the Laplace transform. It is important to start noticing these connections,
to understand the link between Laplace and Fourier analysis.
Remarks
I
Notice the difference of notation for the frequency representation of signals and systems used above. If x(t)
is a periodic signal its frequency representation is given by {Xk}, and if aperiodic by X(), while for a
system with impulse response h(t) its frequency response is given by H( j).
I
When considering the eigenfunction property, the stability of the LTI system is necessary to ensure that
H( j) exists for all frequencies.
I
The eigenfunction property applied to a linear circuit gives the same result as the one obtained from phasors
in the sinusoidal steady state. That is, if
x(t) = A cos(0t + θ) = Aejθ
2 ej0t + Ae−jθ
2
e−j0t
(4.5)
is the input of a circuit represented by the transfer function
H(s) = Y(s)
X(s) = L[y(t)]
L[x(t)]
then the corresponding steady-state output is given by
yss(t) = Aejθ
2 ej0tH( j0) + Ae−jθ
2
e−j0tH(−j0)
= A|H( j0)| cos(0t + θ + ∠H( j0))
(4.6)
where, very importantly, the frequency of the output coincides with that of the input, and the amplitude
and phase of the input are changed by the magnitude and phase of the frequency response of the system
for the frequency 0. The frequency response is H( j0) = H(s)|s=j0, and as we will see its magnitude
is an even function of frequency, or |H( j)| = |H(−j)|, and its phase is an odd function of frequency,
or ∠H( j0) = −∠H(−j0). Using these two conditions we obtain Equation (4.6).
The phasor corresponding to the input
x(t) = A cos(0t + θ)
is deﬁned as a vector,
X = A∠θ
rotating in the polar plane at the frequency of 0. The phasor has a magnitude A and an angle θ with
respect to the positive real axis. The projection of the phasor onto the real axis, as it rotates at the given

4.2 Eigenfunctions Revisited
241
frequency, with time generates a cosine of the indicated frequency, amplitude, and phase. The transfer
function is computed at s = j0 or
H(s)|s=j0 = H( j0) = Y
X
(ratio of the phasors corresponding to the output Y and the input X). The phasor for the output
is thus
Y = H( j0)X = |Y|ej∠Y
Such a phasor is then converted into the sinusoid (which equals Eq. 4.6):
yss(t) = Re[Yej0t] = |Y| cos(0t + ∠Y)
I
A very important application of LTI systems is ﬁltering, where one is interested in preserving desired
frequency components of a signal and getting rid of less-desirable components. That an LTI system can be
used for ﬁltering is seen in Equations (4.3) and (4.4). In the case of a periodic signal, the magnitude
|H( jk)| can be set ideally to one for those components we wish to keep and to zero for those we wish
to get rid of. Likewise, for an aperiodic signal, the magnitude |H( j)| could be set ideally to one for
those components we wish to keep and zero for those components we wish to get rid of. Depending on
the ﬁltering application, an LTI system with the appropriate characteristics can be designed, obtaining the
desired transfer function H(s).
For a stable LTI with transfer function H(s) if the input is
x(t) = Re[Aej(0t+θ)] = A cos(0t + θ)
the steady-state output is given by
y(t) = Re[AH( j0)ej(0t+θ)]
= A|H( j0)| cos(0t + θ + ∠H( j0))
(4.7)
where
H( j0) = H(s)|s=j0
I Example 4.1
Consider the RC circuit shown in Figure 4.1. Let the voltage source be vs(t) = 4 cos(t + π/4) volts.
the resistor be R = 1, and the capacitor C = 1 F. Find the steady-state voltage across the capacitor.
Solution
This problem can be approached in two ways.
I
Phasor approach. From the phasor circuit in Figure 4.1, by voltage division we have the follow-
ing phasor ratio, where Vs is the phasor corresponding to the source vs(t) and Vc the phasor

242
CHAPTER 4:
Frequency Analysis: The Fourier Series
FIGURE 4.1
RC circuit and corresponding phasor
circuit.
vs(t)
1Ω
1F
vc(t)
+
−
+
−
1
−j
+
−
Vc
+
−
Vs
corresponding to vc(t):
Vc
Vs
=
−j
1 −j = −j(1 + j)
2
=
√
2
2 ∠−π/4
Since Vs = 4∠π/4, then
Vc = 2
√
2∠0
so that in the steady state,
vc(t) = 2
√
2 cos(t)
I
Eigenfunction approach. Considering the output is the voltage across the capacitor and the input
is the voltage source, the transfer function is obtained using voltage division as
H(s) = Vc(s)
Vs(s) =
1/s
1 + 1/s =
1
s + 1
so that the system frequency response at the input frequency 0 = 1 is
H( j1) =
√
2
2 ∠−π/4
According to the eigenfunction property the steady-state response of the capacitor is
vc(t) = 4|H( j1)| cos(t + π/4 + ∠H( j1))
= 2
√
2 cos(t)
which coincides with the solution found using phasors.
I
I Example 4.2
An ideal communication system provides as output the input signal with only a possible delay in
the transmission. Such an ideal system does not cause any distortion to the input signal beyond

4.2 Eigenfunctions Revisited
243
the delay. Find the frequency response of the ideal communication system, and use it to determine
the steady-state response when the delay caused by the system is τ = 3 sec, and the input is x(t) =
2 cos(4t −π/4).
Solution
The impulse response of the ideal system is h(t) = δ(t −τ) where τ is the delay of the transmission.
In fact, the output according to the convolution integral gives
y(t) =
∞
Z
0
δ(ρ −τ)
|
{z
}
h(ρ)
x(t −ρ)dρ = x(t −τ)
as expected. Let us then ﬁnd the frequency response of the ideal communication system. According
to the eigenvalue property, if the input is x(t) = ej0t, then the output is
y(t) = ej0tH( j0)
but also
y(t) = x(t −τ) = ej0(t−τ)
so that comparing these equations we have that
H( j0) = e−jτ0
For a generic frequency 0 ≤ < ∞, we would get
H( j) = e−jτ
which is a complex function of , with a unity magnitude |H( j)| = 1, and a linear phase
∠H( j) = −τ. This system is called an all-pass system, since it allows all frequency components
of the input to go through with a phase change only.
Consider the case when τ = 3, and that we input into this system x(t) = 2 cos(4t −π/4), then
H( j) = 1e−j3, so that the output in the steady state is
y(t) = 2|H( j4)| cos(4t −π/4 + ∠H( j4))
= 2 cos(4(t −3) −π/4)
= x(t −3)
where we used H( j4) = 1e−j12 (i.e., |H( j4)| = 1 and ∠H( j4) = 12).
I
I Example 4.3
Although there are better methods to compute the frequency response of a system represented by
a differential equation, the eigenfunction property can be easily used for that. Consider the RC

244
CHAPTER 4:
Frequency Analysis: The Fourier Series
circuit shown in Figure 4.1 where the input is
vs(t) = 1 + cos(10,000t)
with components of low frequency,  = 0, and of large frequency,  = 10,000 rad/sec. The output
vc(t) is the voltage across the capacitor in steady state. We wish to ﬁnd the frequency response of
this circuit to verify that it is a low-pass ﬁlter (it allows low-frequency components to go through,
but ﬁlters out high-frequency components).
Solution
Using Kirchhoff’s voltage law, this circuit is represented by a ﬁrst-order differential equation,
vs(t) = vc(t) + dvc(t)
dt
Now, if the input is vs(t) = ejt, for a generic frequency , then the output is vc(t) = ejtH( j).
Replacing these in the differential equation, we have
ejt = ejtH( j) + dejtH( j)
dt
= ejtH( j) + jejtH( j)
so that
H( j) =
1
1 + j
or the frequency response of the ﬁlter for any frequency . The magnitude of H( j) is
|H( j)| =
1
√
1 + 2
which is close to one for small values of the frequency, and tends to zero when the frequency
values are large—the characteristics of a low-pass ﬁlter.
For the input
vs(t) = 1 + cos(10,000t) = cos(0t) + cos(10,000t)
(i.e., it has a zero frequency component and a 10,000-rad/sec frequency component) using Euler’s
identity, we have that
vs(t) = 1 + 0.5

ej10,000t + e−j10,000t
and the steady-state output of the circuit is
vc(t) = 1H( j0) + 0.5H( j10,000)ej10,000t + 0.5H(−j10,000)e−j10,000t
≈1 +
1
10,000 cos(10,000t −π/2) ≈1

4.3 Complex Exponential Fourier Series
245
since
H( j0) = 1
H( j10,000) ≈
1
j 104 =
−j
10,000
H(−j10,000) ≈
1
−j 104 =
j
10,000
Thus, this circuit acts like a low-pass ﬁlter by keeping the DC component (with the low frequency
 = 0) and essentially getting rid of the high-frequency ( = 10,000) component of the signal.
Notice that the frequency response can also be obtained by considering the phasor ratio for a
generic frequency , which by voltage division is
Vc
Vs
=
1/j
1 + 1/j =
1
1 + j
which for  = 0 is 1 and for  = 10,000 is approximately −j/10,000 (i.e., corresponding to H( j0)
and H( j10,000) = H∗( j10,000)).
I
Fourier and Laplace
French mathematician Jean-Baptiste-Joseph Fourier (1768–1830) was a contemporary of Laplace with whom he shared
many scientiﬁc and political experiences [2, 7]. Like Laplace, Fourier was from very humble origins but he was not as
politically astute. Laplace and Fourier were affected by the political turmoil of the French Revolution and both came in close
contact with Napoleon Bonaparte, French general and emperor. Named chair of the mathematics department of the Ecole
Normale, Fourier led the most brilliant period of mathematics and science education in France. His main work was “The
Mathematical Theory of Heat Conduction” where he proposed the harmonic analysis of periodic signals. In 1807 he received
the grand prize from the French Academy of Sciences for this work. This was despite the objections of Laplace, Lagrange,
and Legendre, who were the referees and who indicated that the mathematical treatment lacked rigor. Following Galton’s
advice of “Never resent criticism, and never answer it,” Fourier disregarded these criticisms and made no change to his
1822 treatise in heat conduction. Although Fourier was an enthusiast for the Revolution and followed Napoleon on some
of his campaigns, in the Second Restoration he had to pawn his belongings to survive. Thanks to his friends, he became
secretary of the French Academy, the ﬁnal position he held.
4.3 COMPLEX EXPONENTIAL FOURIER SERIES
The Fourier series is a representation of a periodic signal x(t) in terms of complex exponentials or
sinusoids of frequency multiples of the fundamental frequency of x(t). The advantage of using the
Fourier series to represent periodic signals is not only the spectral characterization obtained, but in
ﬁnding the response for these signals when applied to LTI systems by means of the eigenfunction
property.
Mathematically, the Fourier series is an expansion of periodic signals in terms of normalized orthog-
onal complex exponentials. The concept of orthogonality of functions is similar to the concept of

246
CHAPTER 4:
Frequency Analysis: The Fourier Series
perpendicularity of vectors: Perpendicular vectors cannot be represented in terms of each other, as
orthogonal functions provide mutually exclusive information. The perpendicularity of two vectors
can be established using the dot or scalar product of the vectors, and the orthogonality of functions is
established by the inner product, or the integration of the product of the function and its conjugate.
Consider a set of complex functions {ψk(t)} deﬁned in an interval [a, b], and such that for any pair of
these functions, let’s say ψℓ(t) and ψm(t), ℓ̸= m, their inner product is
b
Z
a
ψℓ(t)ψ∗
m(t)dt =
 0
ℓ̸= m
1
ℓ= m
(4.8)
Such a set of functions is called orthonormal (i.e., orthogonal and normalized).
A ﬁnite-energy signal x(t) deﬁned in [a, b] can be approximated by a series
ˆx(t) =
X
k
akψk(t)
(4.9)
according to some error criterion. For instance, we could minimize the energy of the error function
ε(t) = x(t) −ˆx(t) or
b
Z
a
|ε(t)|2dt =
b
Z
a
x(t) −
X
k
akψk(t)

2
dt
(4.10)
The expansion can be ﬁnite or inﬁnite, and may not approximate the signal point by point.
Fourier proposed sinusoids as the functions {ψk(t)} to represent periodic signals, and solved the
quadratic minimization posed in Equation (4.10) to obtain the coefﬁcients of the representation.
For most signals, the resulting Fourier series has an inﬁnite number of terms and coincides with the
signal pointwise. We will start with a more general expansion that uses complex exponentials and
from it obtain the sinusoidal form. In Chapter 5 we extend the Fourier series to represent aperiodic
signals—leading to the Fourier transform that is in turn connected with the Laplace transform.
Recall that a periodic signal x(t) is such that
I
It is deﬁned for −∞< t < ∞(i.e., it has an inﬁnite support).
I
For any integer k, x(t + kT0) = x(t), where T0 is the fundamental period of the signal or the
smallest positive real number that makes this possible.
The Fourier series representation of a periodic signal x(t), of period T0, is given by an inﬁnite sum of weighted
complex exponentials (cosines and sines) with frequencies multiples of the signal’s fundamental frequency
0 = 2π/T0 rad/sec, or
x(t) =
∞
X
k=−∞
Xkejk0t
0 = 2π
T0
(4.11)

4.3 Complex Exponential Fourier Series
247
where the Fourier coefﬁcients Xk are found according to
Xk = 1
T0
t0+T0
Z
t0
x(t)e−jk0tdt
(4.12)
for k = 0, ±1, ±2, . . . , and any t0. The form of Equation (4.12) indicates that the information needed for the
Fourier series can be obtained from any period of x(t).
Remarks
I
The Fourier series uses the Fourier basis {ejk0t, k integer} to represent the periodic signal x(t) of period
T0. The Fourier basis functions are also periodic of period T0 (i.e., for an integer m,
ejk0(t+mT0) = ejk0tejkm2π = ejk0t
as ejkm2π = 1).
I
The Fourier basis functions are orthonormal over a period—that is,
1
T0
t0+T0
Z
t0
ejk0t[ejℓ0t]∗dt =
(
1
k = ℓ
0
k ̸= ℓ
(4.13)
That is, ejk0t and ejℓ0t are said to be orthogonal when for k ̸= ℓthe above integral is zero, and they
are normal (or normalized) when for k = ℓthe above integral is unity. The functions ejk0t and ejℓ0t
are orthogonal since
1
T0
t0+T0
Z
t0
ejk0t[ejℓ0t]∗dt = 1
T0
t0+T0
Z
t0
ej(k−ℓ)0tdt
= 1
T0
t0+T0
Z
t0
[cos((k −ℓ)0t) + j sin((k −ℓ)0t)] dt
= 0
k ̸= ℓ
The above integrals are zero given that the integrands are sinusoids and the limits of the integrals cover
one or more periods of the integrands. The normality of the Fourier functions is easily shown when for
k = ℓthe above integral is
1
T0
t0+T0
Z
t0
ej0tdt = 1

248
CHAPTER 4:
Frequency Analysis: The Fourier Series
I
The Fourier coefﬁcients {Xk} are easily obtained using the orthonormality of the Fourier functions: First,
we multiply the expression for x(t) in Equation (4.11) by e−jℓ0t and then integrate over a period to get
Z
T0
x(t)e−jℓ0tdt =
X
k
Xk
Z
T0
ej(k−ℓ)0tdt
=
X
k
XkT0δ(k −ℓ)
= XℓT0
given that when k = ℓ, then
R
T0 ej(k−ℓ)0tdt = T0; otherwise it is zero according to the orthogonality of the
Fourier exponentials. This then gives us the expression for the Fourier coefﬁcients {Xℓ} in Equation (4.12).
You need to recognize that the k and ℓare dummy variables in the Fourier series, and as such the expression
for the coefﬁcients is the same regardless of whether we use ℓor k.
I
It is important to realize from the given Fourier series equations that for a periodic signal x(t), of period
T0, any period
x(t), t0 ≤t ≤t0 + T0
provides all the necessary information in the time-domain characterizing x(t). In an equivalent way the
coefﬁcients and their corresponding frequencies {Xk, k0} provide all the necessary information about x(t)
in the frequency domain.
4.4 LINE SPECTRA
The Fourier series provides a way to determine the frequency components of a periodic signal and the
signiﬁcance of these frequency components. Such information is provided by the power spectrum of
the signal. For periodic signals, the power spectrum provides information as to how the power of the
signal is distributed over the different frequencies present in the signal. We thus learn not only what
frequency components are present in the signal but also the strength of these frequency components.
In practice, the power spectrum can be computed and displayed using a spectrum analyzer, which
will be described in Chapter 5.
4.4.1 Parseval’s Theorem—Power Distribution over Frequency
Although periodic signals are inﬁnite-energy signals, they have ﬁnite power. The Fourier series
provides a way to ﬁnd how much of the signal power is in a certain band of frequencies.
The power Px of a periodic signal x(t), of period T0, can be equivalently calculated in either the time or the
frequency domain:
Px = 1
T0
Z
T0
|x(t)|2dt =
X
k
|Xk|2
(4.14)

4.4 Line Spectra
249
The power of a periodic signal x(t) of period T0 is given by
Px = 1
T0
Z
T0
|x(t)|2dt
Replacing the Fourier series of x(t) in the power equation we have that
1
T0
Z
T0
|x(t)|2dt = 1
T0
Z
T0
X
k
X
m
Xk X∗
mej0kte−j0mtdt
=
X
k
X
m
Xk X∗
m
1
T0
Z
T0
ej0kte−j0mtdt
=
X
k
|Xk|2
after we apply the orthonormality of the Fourier exponentials. Even though x(t) is real, we let |x(t)|2 =
x(t)x∗(t) in the above equations, permitting us to express them in terms of Xk and its conjugate. The
above indicates that the power of x(t) can be computed in either the time or the frequency domain
giving exactly the same result.
Moreover, considering the signal to be a sum of harmonically related components or
x(t) =
X
k
Xkejk0t =
X
k
xk(t)
the power of each of these components is given by
1
T0
Z
T0
|xk(t)|2dt = 1
T0
Z
T0
|Xkejk0t|2dt = 1
T0
Z
T0
|Xk|2dt = |Xk|2
and the power of x(t) is the sum of the powers of the Fourier series components. This indicates that
the power of the signal is distributed over the harmonic frequencies {k0}. A plot of |Xk|2 versus
the harmonic frequencies k0, k = 0, ±1, ±2, . . . , displays how the power of the signal is distributed
over the harmonic frequencies. Given the discrete nature of the harmonic frequencies {k0} this plot
consists of a line at each frequency and as such it is called the power line spectrum (that is, a periodic
signal has no power in nonharmonic frequencies). Since {Xk} are complex, we deﬁne two additional
spectra, one that displays the magnitude |Xk| versus k0, called the magnitude line spectrum, and the
phase line spectrum or ∠Xk versus k0 showing the phase of the coefﬁcients {Xk} for k0. The power
line spectrum is simply the magnitude spectrum squared.
A periodic signal x(t), of period T0, is represented in the frequency by its
Magnitude line spectrum :
|Xk| vs k0
(4.15)
Phase line spectrum :
∠Xk vs k0
(4.16)

250
CHAPTER 4:
Frequency Analysis: The Fourier Series
The power line spectrum |Xk|2 versus k0 of x(t) displays the distribution of the power of the signal over
frequency.
4.4.2 Symmetry of Line Spectra
For a real-valued periodic signal x(t), of period T0, represented in the frequency domain by the Fourier
coefﬁcients {Xk = |Xk|ej∠Xk} at harmonic frequencies {k0 = 2πk/T0}, we have that
Xk = X∗
−k
(4.17)
or equivalently that
1.
|Xk| = |X−k| (i.e., magnitude |Xk| is even function of k0)
2.
∠Xk = −∠X−k (i.e., phase ∠Xk is odd function of k0)
(4.18)
Thus, for real-valued signals we only need to display for k ≥0 the
Magnitude line spectrum: Plot of |Xk| versus k0
Phase line spectrum: Plot of ∠Xk versus k0
For a real signal x(t), the Fourier series of its complex conjugate x∗(t) is
x∗(t) =
"X
ℓ
Xℓejℓ0t
#∗
=
X
ℓ
X∗
ℓe−jℓ0t =
X
k
X∗
−kejk0t
Since x(t) = x∗(t), the above equation is equal to
x(t) =
X
k
Xkejk0t
Comparing the Fourier series coefﬁcients in the expressions, we have that X∗
−k = Xk, which means
that if Xk = |Xk|ej∠Xk, then
|Xk| = |X−k|
∠Xk = −∠X−k
or that the magnitude is an even function of k, while the phase is an odd function of k. Thus, the line
spectra corresponding to real-valued signals is given for only positive harmonic frequencies, with the
understanding that for negative values of the harmonic frequencies the magnitude line spectrum is
even and the phase line spectrum is odd.

4.5 Trigonometric Fourier Series
251
4.5 TRIGONOMETRIC FOURIER SERIES
The trigonometric Fourier series of a real-valued, periodic signal x(t), of period T0, is an equivalent
representation that uses sinusoids rather than complex exponentials as the basis functions. It is given by
x(t) = X0 + 2
∞
X
k=1
|Xk| cos(k0t + θk)
= c0 + 2
∞
X
k=1
[ck cos(k0t) + dk sin(k0t)]
0 = 2π
T0
(4.19)
where X0 = c0 is called the DC component, and {2|Xk| cos(k0t + θk)} are the kth harmonics for k = 1, 2 . . ..
The frequencies {k0} are said to be harmonically related. The coefﬁcients {ck, dk} are obtained from x(t) as
follows:
ck = 1
T0
t0+T0
Z
t0
x(t) cos(k0t) dt
k = 0, 1, . . .
dk = 1
T0
t0+T0
Z
t0
x(t) sin(k0t) dt
k = 1, 2, . . .
(4.20)
The coefﬁcients Xk = |Xk|ejθk are connected with the coefﬁcients ck and dk by
|Xk| =
q
c2
k + d2
k
θk = −tan−1
dk
ck

The functions {cos(k0t), sin(k0t)} are orthonormal.
Using the relation Xk = X∗
−k, obtained in the previous section, we express the exponential Fourier
series of a real-valued periodic signal x(t) as
x(t) = X0 +
∞
X
k=1
[Xkejk0t + X−ke−jk0t]
= X0 +
∞
X
k=1
h
|Xk|ej(k0t+θk) + |Xk|e−j(k0t+θk)i
= X0 + 2
∞
X
k=1
|Xk| cos(k0t + θk)
which is the top equation in Equation (4.19).

252
CHAPTER 4:
Frequency Analysis: The Fourier Series
Let us then show how the coefﬁcients ck and dk can be obtained directly from the signal. Using the
relation Xk = X∗
−k and the fact that for a complex number z = a + jb, then z + z∗= (a + jb) + (a −
jb) = 2a = 2Re(z), we have that
x(t) = X0 +
∞
X
k=1
[Xkejk0t + X−ke−jk0t]
= X0 +
∞
X
k=1
[Xkejk0t + X∗
ke−jk0t]
= X0 +
∞
X
k=1
2Re[Xkejk0t]
Since Xk is complex (verify this!),
2Re[Xkejk0t] = 2Re[Xk] cos(k0t) −2Im[Xk] sin(k0t)
Now, if we let
ck = Re[Xk] = 1
T0
t0+T0
Z
t0
x(t) cos(k0t) dt
k = 1, 2, . . .
dk = −Im[Xk] = 1
T0
t0+T0
Z
t0
x(t) sin(k0t) dt
k = 1, 2, . . .
we then have
x(t) = X0 +
∞
X
k=1
 2Re[Xk] cos(k0t) −2Im[Xk] sin(k0t)

= X0 + 2
∞
X
k=1
(ck cos(k0t) + dk sin(k0t))
and since the average X0 = c0 we obtain the second form of the trigonometric Fourier series shown
in Equation (4.19). Notice that d0 = 0 and so it is not necessary to deﬁne it.
The coefﬁcients Xk = |Xk|ejθk are connected with the coefﬁcients ck and dk by
|Xk| =
q
c2
k + d2
k
θk = −tan−1
dk
ck

This can be shown by adding the phasors corresponding to ck cos(k0t) and dk sin(k0t) and ﬁnding
the magnitude and phase of the resulting phasor.

4.5 Trigonometric Fourier Series
253
Finally, since the exponential basis {ejk0t} = {cos(k0t) + j sin(k0t)}, the sinusoidal bases cos(k0t)
and sin(k0t) just like the exponential basis are periodic, of period T0, and orthonormal.
I Example 4.4
Find the Fourier series of a raised-cosine signal (B ≥A),
x(t) = B + A cos(0t + θ)
which is periodic of period T0 and fundamental frequency 0 = 2π/T0. Call y(t) = B + cos(0t −
π/2). Find its Fourier series coefﬁcients and compare them to those for x(t). Use symbolic MATLAB
to compute the Fourier series of y(t) = 1 + sin(100t). Find and plot its magnitude and phase line
spectra.
Solution
In this case we do not need to compute the Fourier coefﬁcients since x(t) is already in the trigono-
metric form. From Equation (4.19) its dc value is B, and A is the coefﬁcient of the ﬁrst harmonic in
the trigonometric Fourier series, so that X0 = B, |X1| = A/2, and ∠X1 = θ. Likewise, using Euler’s
identity we obtain that
x(t) = B + A
2
h
ej(0t+θ) + e−j(0t+θ)i
= B + Aejθ
2 ej0t + Ae−jθ
2
e−j0t
which gives
X0 = B
X1 = Aejθ
2
X−1 = X∗
1
If we let θ = −π/2 in x(t), we get
y(t) = B + A sin(0t)
Its Fourier series coefﬁcients are Y0 = B and Y1 = Ae−jπ/2/2 so that |Y1| = |Y−1| = A/2 and
∠Y1 = −∠Y−1 = −π/2. The magnitude and phase line spectra of the raised cosine (θ = 0) and
of the raised sine (θ = −π/2) are shown in Figure 4.2. For both x(t) and y(t) there are only two
frequencies—the dc frequency and 0—and as such the power of the signal is concentrated at
those two frequencies as shown in Figure 4.2. The difference between the line spectra of x(t) and
y(t) is in the phase.

254
CHAPTER 4:
Frequency Analysis: The Fourier Series
FIGURE 4.2
(a) Magnitude (top left) and
phase (bottom left) line spectra
of raised cosine and (b)
magnitude (top right) and
phase (bottom right) line
spectra of raised sine.
B
−Ω0
Ω0
k Ω0
|Xk|
−Ω0
Ω0
k Ω0
∠Xk
A
2
A
2
(a)
(b)
k Ω0
−Ω0
Ω0
|Yk|
B
A
2
A
2
2
−Ω0
kΩ0
Ω0
∠Yk
π
−
π
2
Using symbolic MATLAB integration we can easily ﬁnd the Fourier series coefﬁcients, and the
corresponding magnitude and phase are then plotted using stem to obtain the line spectra. Using
our MATLAB function fourierseries the magnitude and phase of the line spectrum corresponding to
the periodic raised sine y(t) = 1 + sin(100t) is shown in Figure 4.3.
function [X, w] = fourierseries(x, T0, N)
%%%%%
% symbolic Fourier Series computation
% x: periodic signal
% T0: period
% N: number of harmonics
% X,w: Fourier series coefﬁcients at harmonic frequencies
%%%%%
syms t
% computation of N Fourier series coefﬁcients
for k = 1:N,
X1(k) = int(x ∗exp(−j ∗2 ∗pi ∗(k - 1) ∗t/T0), t, 0, T0)/T0;
X(k) = subs(X1(k));
w(k) = (k−1) ∗2 ∗pi/T0; % harmonic frequencies
end

4.6 Fourier Coefﬁcients from Laplace
255
0
0.2
0.4
0.6
0.8
1
Magnitude line spectrum
|Yk|
−200
0
200
−1
0
1
Phase line spectrum
Ω(rad/sec)
−200
0
200
Ω(rad/sec)
|Yk|
0
0.1
0.2
0
0.5
1
1.5
2
y (t)
t (sec)
FIGURE 4.3
Line spectra of Fourier series of y(t) = 1 + sin(100t) (top ﬁgure). Notice the even and the odd symmetries of the
magnitude and the phase spectra. The phase is −π/2 at  = 100 rad/sec.
I
Remarks Just because a signal is a sum of sinusoids, which are always periodic, is not enough for it to have a
Fourier series. The signal should be periodic. The signal x(t) = cos(t) −sin(πt) has components with periods
T1 = 2π and T2 = 2 so that the ratio T1/T2 = π is not a rational number. Thus, x(t) is not periodic and no
Fourier series for it is possible.
4.6 FOURIER COEFFICIENTS FROM LAPLACE
The computation of the Xk coefﬁcients (see Eq. 4.12) requires integration that for some signals
can be rather complicated. The integration can be avoided whenever we know the Laplace trans-
form of a period of the signal as we will show. In general, the Laplace transform of a period of
the signal exists over the whole s-plane, given that it is a ﬁnite-support signal. In some cases, the
dc coefﬁcient cannot be computed with the Laplace transform, but the dc term is easy to compute
directly.

256
CHAPTER 4:
Frequency Analysis: The Fourier Series
For a periodic signal x(t), of period T0, if we know or can easily compute the Laplace transform of a period
of x(t),
x1(t) = x(t)[u(t0) −u(t −t0 −T0)]
for any t0
Then the Fourier coefﬁcients of x(t) are given by
Xk = 1
T0
L [x1(t)]s=jk0
0 = 2π
T0
fundamental frequency
(4.21)
This can be seen by comparing the equation for the Xk coefﬁcients with the Laplace transform of a
period x1(t) = x(t)[u(t0) −u(t −t0 −T0)] of x(t). Indeed, we have that
Xk = 1
T0
t0+T0
Z
t0
x(t)e−jk0tdt
= 1
T0
t0+T0
Z
t0
x(t)e−stdt
s=jk0
= 1
T0
L [x1(t)]s=jk0
I Example 4.5
Consider the periodic pulse train x(t), of period T0 = 1, shown in Figure 4.4. Find its Fourier series.
Solution
Before ﬁnding the Fourier coefﬁcients, we see that this signal has a dc component of 1, and that
x(t) −1 (zero-average signal) is well represented by cosines, given its even symmetry, and as such
FIGURE 4.4
Train of rectangular pulses.
· · ·
· · ·
2
x(t)
−0.25
−0.75
0.75
0.25
1.25
−1.25
t
T0 = 1

4.6 Fourier Coefﬁcients from Laplace
257
the Fourier coefﬁcients will be real. Doing this analysis before the computations is important so
we know what to expect.
The Fourier coefﬁcients are obtained directly using their integral formulas or from the Laplace
transform of a period. Since T0 = 1, the fundamental frequency of x(t) is 0 = 2π rad/sec. Using
the integral expression for the Fourier coefﬁcients we have
Xk = 1
T0
3/4
Z
−1/4
x(t)e−j0ktdt =
1/4
Z
−1/4
2e−j2πktdt
= 2
πk
"
ejπk/2 −e−jπk/2
2j
#
= sin(πk/2)
(πk/2)
which are real as we predicted. The Fourier series is then
x(t) =
∞
X
k=−∞
sin(πk/2)
(πk/2) ejk2πt
To ﬁnd the Fourier coefﬁcients with the Laplace transform, let the period be x1(t) = x(t) for −0.5 ≤
t ≤0.5. Delaying it by 0.25 we get x1(t −0.25) = 2[u(t) −u(t −0.5)] with a Laplace transform
e−0.25sX1(s) = 2
s (1 −e−0.5s)
so that X1(s) = (2/s)[e0.25s −e−0.25s], and therefore
Xk = 1
T0
L [x1(t)]
s=jk0
=
2
jk0T0
2j sin(k0/4)
and for 0 = 2π, T0 = 1, we get
Xk = sin(πk/2)
πk/2
k ̸= 0
Since the above equation gives zero over zero when k = 0 (i.e., it is undeﬁned), the dc value is
found from the integral formula as
X0 =
1/4
Z
−1/4
2dt = 1
These Fourier coefﬁcients coincide with the ones found before.
The following script is used to ﬁnd the Fourier coefﬁcients with our function fourierseries and to
plot the magnitude and phase line spectra.

258
CHAPTER 4:
Frequency Analysis: The Fourier Series
%%%%%%%%%%%%%%%%%
% Example 4.5---Fourier series of train of pulses
%%%%%%%%%%%%%%%%%
clear all;clf
syms t
T0 = 1; m = heaviside(t) −heaviside(t −T0/4) + heaviside(t −3 ∗T0/4);x = 2 ∗m
[X,w] = fourierseries(x,T0,20);
subplot(221); ezplot(x,[0 T0]); grid
subplot(223); stem(w,abs(X))
subplot(224); stem(w,angle(X))
Notice that in this case:
1.
The Xk Fourier coefﬁcients of the train of pulses are given in terms of the sin(x)/x or the sinc
function. This function was presented in Chapter 1. Recall that the sinc is
I
Even—that is, sin(x)/x = sin(−x)/(−x).
I
The value at x = 0 is found by means of L’Hˆopital’s rule because the numerator and the
denominator of sinc are zero for x = 0, so
lim
x→0
sin(x)
x
= lim
x→0
d sin(x)/dx
dx/dx
= 1
I
It is bounded, indeed
−1
x
≤sin(x)
x
≤1
x
2.
Since the dc component of x(t) is 1, once it is subtracted it is clear that the rest of the series can
be represented as a sum of cosines:
x(t) = 1 +
∞
X
k=−∞,k̸=0
sin(πk/2)
(πk/2) ejk2πt
= 1 + 2
∞
X
k=1
sin(πk/2)
(πk/2)
cos(2πkt)
This can also be seen by considering the trigonometric Fourier series of x(t). Since x(t) sin(k0t)
is odd, as x(t) is even and sin(k0t) is odd, then the coefﬁcients corresponding to the sines in
the expansion will be zero. On the other hand, x(t) cos(k0t) is even and gives nonzero Fourier
coefﬁcients. See Equations (4.20).
3.
In general, the Fourier coefﬁcients are complex and as such need to be represented by their
magnitudes and phases. In this case, the Xk coefﬁcients are real-valued, and in particular zero
when kπ/2 = ±mπ, m an integer, or when k = ±2, ±4, . . .. Since the Xk values are real, the
corresponding phase would be zero when Xk ≥0, and ±π when Xk < 0. In Figure 4.5 we show
a period of the signal, and the magnitude and phase line spectra displayed only for positive
values of frequency (with the understanding that the magnitude spectrum is even and the
phase is odd functions of the frequency).

4.6 Fourier Coefﬁcients from Laplace
259
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
Period
x(t)
t
0
50
100
0
0.2
0.4
0.6
0.8
1
Magnitude line spectrum
|Xk|
|Xk|
Ω(rad/sec)
0
50
100
0
1
2
3
4
Phase line spectrum
Ω(rad/sec)
FIGURE 4.5
Period of train of rectangular pulses (top) and its magnitude and phase line spectra (bottom).
4.
The Xk coefﬁcients and its squares, related to the power line spectrum, are obtained using the
fourierseries function (see Figure 4.5):
k
Xk = X−k
X2
k
0
1
1
1
0.64
0.41
2
0
0
3
−0.21
0.041
4
0
0
5
0.13
0.016
6
0
0
7
−0.09
0.008

260
CHAPTER 4:
Frequency Analysis: The Fourier Series
Notice that about 11 of them (including the zero values), or the dc value and 5 harmon-
ics, provide a very good approximation of the pulse train, and would occupy a bandwidth
of approximately 10π rad/sec. The power contribution, as indicated by X2
k after k = ±6, is
relatively small.
I
I Example 4.6
Find the Fourier series of the full-wave rectiﬁed signal x(t) = | cos(πt)| shown in Figure 4.6. This
signal is used in the design of dc sources. The rectiﬁcation of an ac signal is the ﬁrst step in this
design.
Solution
The integral to ﬁnd the Fourier coefﬁcients is
Xk =
0.5
Z
−0.5
cos(πt)e−j2πktdt
which can be computed by using Euler’s identity or any other method. We want to show that this
can be avoided by using the Laplace transform.
A period x1(t) of x(t) can be expressed as
x1(t −0.5) = sin(πt)u(t) + sin(π(t −1))u(t −1)
−2
−1
0
1
2
−0.2
0
0.2
0.4
0.6
0.8
1
t
x (t)
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0
0.2
0.4
0.6
0.8
1
t
x1(t)
(a)
(b)
FIGURE 4.6
(a) Full-wave rectiﬁed signal x(t) and (b) one of its periods x1(t).

4.6 Fourier Coefﬁcients from Laplace
261
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
t
Period
0
50
100
0
0.1
0.2
0.3
0.4
0.5
0.6
Magnitude line spectrum
|Xk|
Ω (rad/sec)
0
50
100
0
1
2
3
4
Phase line spectrum
<Xk
Ω (rad/sec)
FIGURE 4.7
Period of full-wave rectiﬁed signal x(t) and its magnitude and phase line spectra.
and using the Laplace transform we have
X1(s)e−0.5s =
π
s2 + π2 [1 + e−s]
so that
X1(s) =
π
s2 + π2 [e0.5s + e−0.5s]
The Fourier coefﬁcients are then
Xk = 1
T0
X1(s)|s=j0k
where T0 = 1 and 0 = 2π, giving
Xk =
π
( j2πk)2 + π2 2 cos(2πk/2)
=
2(−1)k
π(1 −4k2)

262
CHAPTER 4:
Frequency Analysis: The Fourier Series
since cos(πk) = (−1)k. The DC value of the full-wave rectiﬁed signal is X0 = 2/π. Notice that the
Fourier coefﬁcients are real given that the signal is even.
The MATLAB script used in the previous example can be used again with the following
modiﬁcation for the generation of a period of x(t). The results are shown in Figure 4.7.
%%%%%%%%%%%%%%%%%
% Example 4.6---Fourier series of full-wave rectiﬁed signal
%%%%%%%%%%%%%%%%
% period generation
T0 = 1;
m = heaviside(t) −heaviside(t −T0);x = abs(cos(pi ∗t)) ∗m
I
I Example 4.7
Computing the derivative of a signal enhances higher harmonics. To illustrate this consider the
train of triangular pulses y(t) (Figure 4.8) with fundamental period T0 = 2. Let x(t) = dy(t)/dt. Find
its Fourier series and compare |Xk| with |Yk| to determine which of these signals is smoother—that
is, which one has lower frequency components.
Solution
A period of y(t), −1 ≤t ≤1, is given by
y1(t) = r(t + 1) −2r(t) + r(t −1)
with a Laplace transform
Y1(s) = 1
s2

es −2 + e−s
FIGURE 4.8
(a) Train of triangular
pulses y(t) and (b) its
derivative x(t). Notice
that y(t) is a continuous
function while x(t) is
discontinuous.
(a)
(b)
···
y(t)
t
−1
0
1
−2
2
1
···
−2
−1
0
1
2
t
1
−1
···
···
x(t)= dy(t)
dt

4.6 Fourier Coefﬁcients from Laplace
263
so that the Fourier coefﬁcients are given by (T0 = 2, 0 = π):
Yk = 1
T0
Y1(s)|s=jok =
1
2( jπk)2 [2 cos(πk) −2]
= 1 −cos(πk)
π2k2
= 1 −(−1)k
π2k2
k ̸= 0
This is also equal to
Yk = 0.5
sin(πk/2)
(πk/2)
2
(4.22)
using the identity 1 −cos(πk) = 2 sin2(πk/2). By observing y(t) we deduce that its DC value is
Y0 = 0.5.
Let us then consider the periodic signal x(t) = dy(t)/dt (shown in Fig. 4.8(b)) with a dc value
X0 = 0. For −1 ≤t ≤1, its period is x1(t) = u(t + 1) −2u(t) + u(t −1) and
X1(s) = 1
s

es −2 + e−s
which gives the Fourier series coefﬁcients (T0 = 2,  (the period and the fundamental frequency
are equal to the ones for y(t))
Xk = sin2(kπ/2)
kπ/2
j
(4.23)
since Xk = 1
2X1(s)|s=jπk.
(a)
(b)
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
t
Period
y(t)
0
20
40
60
0
0.2
0.4
0.6
0.8
Ω (rad/sec)
0
20
40
60
−1
−0.5
0
0.5
1
Ω (rad/sec)
|Yk|
|Yk|
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
t
Period
x(t)
|Yk|
0
20
40
60
0
0.2
0.4
0.6
0.8
0
20
40
60
0
0.5
1
1.5
2
Ω (rad/sec)
Ω (rad/sec)
|Xk|
FIGURE 4.9
Magnitude and phase line spectra of (a) triangular signal y(t) (top left) and (b) its derivative x(t) (top right).
Ignoring the dc values, the {|Yk|} decay faster to zero than the {|Xk|}, thus y(t) is smoother than x(t).

264
CHAPTER 4:
Frequency Analysis: The Fourier Series
For k ̸= 0 we have |Yk| = |Xk|/(πk), so that as k increases the frequency components of y(t) decrease
in magnitude faster than the corresponding ones of x(t). Thus, y(t) is smoother than x(t). The
magnitude line spectrum |Yk|, ignoring its average, goes faster to zero than the magnitude line
spectrum |Xk|, as seen in Figure 4.9.
Notice that in this case y(t) is even and its Fourier coefﬁcients Yk are real, while x(t) is odd and
its Fourier coefﬁcients Xk are purely imaginary. If we subtract the average of y(t), the signal y(t)
can be clearly approximated as a series of cosines, thus the need for real coefﬁcients in its complex
exponential Fourier series. The signal x(t) is zero-average and as such it can be clearly approximated
by a series of sines requiring its Fourier coefﬁcients Xk to be imaginary.
I
I Example 4.8
Integration of a periodic signal, provided it has zero mean, gives a smoother signal. To see this,
ﬁnd and compare the magnitude line spectra of a sawtooth signal x(t), of period T0 = 2, and its
integral
y(t) =
Z
x(t)dt
shown Figure 4.10.
Solution
Before doing any calculations it is important to realize that the integral would not exist if the dc is
not zero. Using the following script we can compute the Fourier series coefﬁcients of x(t) and y(t).
A period of x(t) is
x1(t) = tw(t) + (t −2)w(t −1)
0 ≤t ≤2
where w(t) = u(t) −u(t −1) is a rectangular window.
FIGURE 4.10
(a) Sawtooth signal x(t)
and (b) its integral y(t).
Notice that x(t) is a
discontinuous function
while y(t) is continuous.
(a)
(b)
x(t)
1
−1
−2
0
1
−1
t
···
···
y(t) = ∫ x(t)dt
−1
−2
t
0.5
0
1
2
···
···

4.7 Convergence of the Fourier Series
265
(a)
(b)
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
t
Period
x(t)
0
20
40
60
0
0.1
0.2
0.3
0.4
Ω
|Xk|
|Xk|
0
20
40
60
−2
−1
0
1
2
Ω
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
t
Period
y(t)
|Xk|
0
20
40
60
0
1
2
3
4
Ω
0
20
40
60
0
0.1
0.2
0.3
0.4
Ω
|Yk|
FIGURE 4.11
(a) Periods of the sawtooth signal x(t) and (b) its integral y(t) and their magnitude and phase line spectra.
%%%%%%%%%%%%%%%%%
% Example 4.8---Saw-tooth signal and its integral
%%%%%%%%%%%%%%%%
syms t
T0 = 2;
m = heaviside(t) −heaviside(t −T0/2);
m1 = heaviside(t −T0/2) - heaviside(t −T0);
x = t ∗m + (t −2) ∗m1;
y = int(x);
[X, w] = fourierseries(x, T0, 20);
[Y, w] = fourierseries(y, T0, 20);
The signal y(t) is smoother than x(t); y(t) is a continuous function of time, while x(t) is
discontinuous. This is indicated as well by the magnitude line spectra of the two signals. Ignor-
ing the dc components, the {|Yk|} of y(t) decay a lot faster to zero than the {|Xk|} of x(t) (See
Figure 4.11). As we will see in Section 4.10, computing the derivative of a periodic signal is equiva-
lent to multiplying its Fourier series coefﬁcients by j0k, which emphasizes the higher harmonics.
If the periodic signal is zero-mean so that its integral exists, the Fourier coefﬁcients of the integral
can be found by dividing them by j0k so that now the low harmonics are emphasized.
I
4.7 CONVERGENCE OF THE FOURIER SERIES
It can be said, without overstating it, that any periodic signal of practical interest has a Fourier series.
Only very strange signals would not have a converging Fourier series. Establishing convergence is
necessary because the Fourier series has an inﬁnite number of terms. To establish some general

266
CHAPTER 4:
Frequency Analysis: The Fourier Series
conditions under which the series converges, we need to classify signals with respect to their
smoothness.
A signal x(t) is said to be piecewise smooth if it has a ﬁnite number of discontinuities, while a smooth
signal has a derivative that changes continuously. Thus, smooth signals can be considered special
cases of piecewise smooth signals.
The Fourier series of a piecewise smooth (continuous or discontinuous) periodic signal x(t) converges for all
values of t. The mathematician Dirichlet showed that for the Fourier series to converge to the periodic signal
x(t), the signal should satisfy the following sufﬁcient (not necessary) conditions over a period:
I
Be absolutely integrable.
I
Have a ﬁnite number of maxima, minima, and discontinuities.
The inﬁnite series equals x(t) at every continuity point and equals the average
0.5[x(t + 0+) + x(t + 0−)]
of the right limit x(t + 0+) and the left limit x(t + 0−) at every discontinuity point. If x(t) is continuous
everywhere, then the series converges absolutely and uniformly.
Although the Fourier series converges to the arithmetic average at discontinuities, it can be observed
that there is some ringing before and after the discontinuity points. This is called the Gibb’s phe-
nomenon. To understand this phenomenon it is necessary to explain how the Fourier series can be
seen as an approximation to the actual signal, and how when a signal has discontinuities the conver-
gence is not uniform around them. It will become clear that the smoother the signal x(t) is, the easier
it is to approximate it with a Fourier series with a ﬁnite number of terms.
When the signal is continuous everywhere, the convergence is such that at each point t the series
approximates the actual value x(t) as we increase the number of terms in the approximation. How-
ever, that is not the case when discontinuities occur in the signal. This is despite the fact that a
minimum mean-square approximation seems to indicate that the approximation could give a zero
error. Let
xN(t) =
N
X
k=−N
Xkejk0t
(4.24)
be the Nth-order approximation of a periodic signal x(t), of fundamental frequency 0, that
minimizes the average quadratic error over a period
EN = 1
T0
Z
T0
|x(t) −xN(t)|2dt
(4.25)

4.7 Convergence of the Fourier Series
267
with respect to the Fourier coefﬁcients Xk. To minimize EN with respect to the coefﬁcients Xk we set
its derivative with respect to Xk to zero. Let ε(t) = x(t) −xN(t), so that
dEN
dXk
= 1
T0
Z
T0
2ε(t)dε∗(t)
dXk
dt
= −1
T0
Z
T0
2[x(t) −xN(t)]e−jk0tdt
= 0
which after replacing xN(t) and using the orthogonality of the Fourier exponentials gives
Xk = 1
T0
Z
T0
x(t)e−j0ktdt
(4.26)
corresponding to the Fourier coefﬁcients of x(t) for −N ≤k ≤N. As N →∞the average error
EN →0.
The only issue left is how xN(t) converges to x(t). As indicated before, if x(t) is smooth xN(t) approxi-
mates x(t) at every point, but if there are discontinuities the approximation is in an average fashion.
The Gibb’s phenomenon indicates that around discontinuities there will be ringing, regardless of the
order N of the approximation, even though the average quadratic error EN goes to zero as N increases.
This phenomenon will be explained in Chapter 5 as the effect of using a rectangular window to obtain
a ﬁnite-frequency representation of a periodic signal.
I Example 4.9
To illustrate the Gibb’s phenomenon consider the approximation of a train of pulses x(t) with
zero mean and period T0 = 1 (see the dashed signal in Figure 4.12) with a Fourier series xN(t)
with N = 1, . . . , 20.
Solution
We compute analytically the Fourier coefﬁcients of x(t) and use them to obtain an approxima-
tion xN(t) of x(t) having a zero DC component and up to 20 harmonics. The dashed-line plot in
Figure 4.12 is x(t) and the solid–line plot is xN(t) when N = 20. The discontinuities of the pulse
train cause the Gibb’s phenomenon. Even if we increase the number of harmonics there is an
overshoot in the approximation around the discontinuities.

268
CHAPTER 4:
Frequency Analysis: The Fourier Series
FIGURE 4.12
Approximate Fourier series xN(t) of the pulse
train x(t) (discontinuous) using the DC component
and 20 harmonics. The approximate xN(t)
displays the Gibb’s phenomenon around the
discontinuities.
0
0.2
0.4
0.6
0.8
−1
−0.5
0
0.5
1
x(t), xN(t)
t(sec)
%%%%%%%%%%%%%%%%%
% Example 4.9---Simulation of Gibb’s phenomenon
%%%%%%%%%%%%%%%%
clf; clear all
w0 = 2 ∗pi; DC = 0; N = 20; % parameters of periodic signal
% computation of Fourier series coefﬁcients
for k = 1:N,
X(k) = sin(k ∗pi/2)/(k ∗pi/2);
end
X = [DC X]; % Fourier series coefﬁcients
% computation of periodic signal
Ts = 0.001; t = 0:Ts:1 −Ts;
L = length(t); x = [ones(1, L/4) zeros(1, L/2) ones(1, L/4)]; x = x −0.5;
% computation of approximate
xN = X(1)∗ones(1,length(t));
for k = 2:N,
xN = xN + 2 ∗X(k) ∗cos(2 ∗pi ∗(k −1). ∗t); % approximate signal
plot(t, xN); axis([0 max(t) 1.1 ∗min(xN) 1.1 ∗max(xN)])
hold on; plot(t, x, ’r’)
ylabel(’x(t), x N(t)’); xlabel(’t (sec)’);grid
hold off
pause(0.1)
end
When you execute the above script, it pauses to display the approximation for an increasing num-
ber of terms in the approximation. At each of these values ringing around the discontinuities the
Gibb’s phenomenon is displayed.
I

4.7 Convergence of the Fourier Series
269
I Example 4.10
Consider the mean-square error optimization to obtain an approximation of the periodic sig-
nal x(t) shown in Figure 4.4 from Example 4.5. We wish to obtain an approximate x2(t) =
α + 2β cos(0t), given that it is clear that x(t) has an average, and that once we subtract it from the
signal the resulting signal is approximated by a cosine function. Minimize the mean-square error
E2 = 1
T0
Z
T0
|x(t) −x2(t)|2dt
with respect to α and β to ﬁnd these values.
Solution
To minimize E2 we set to zero its derivatives with respect to α and β to get
dE2
dα = −1
T0
Z
T0
2[x(t) −α −2β cos(0t)]dt = −1
T0
Z
T0
2[x(t) −α]dt = 0
dE2
dβ = −1
T0
Z
T0
2[x(t) −α −2β cos(0t)] cos(0t)dt = 0
which, after getting rid of 2
T0 of both sides of the above equations and applying the orthogonality
of the Fourier basis, gives
α = 1
T0
Z
T0
x(t)dt
β = 1
T0
Z
T0
x(t) cos(0t)dt
For the signal in Figure 4.4 we obtain
α = 1
β = 2
π
giving as approximation the signal
x2(t) = 1 + 4
π cos(2πt)
which at t = 0 gives x2(0) = 2.27 instead of the expected 2; x2(0.25) = 1 (because of the
discontinuity at this point, this value is the average of 2 and 0, the values, respectively, before
and after the discontinuity) instead of 2 and x2(0.5) = −0.27 instead of the expected 0.
I

270
CHAPTER 4:
Frequency Analysis: The Fourier Series
I Example 4.11
Consider the train of pulses in Example 4.5. Determine how many Fourier coefﬁcients are necessary
to get a representation containing 97% of the power of the periodic signal.
Solution
The desired 97% of the power of x(t) is
0.97 1
T0
Z
T0
x2(t)dt = 0.97
0.25
Z
−0.25
4dt = 1.94
and so we need to ﬁnd an integer N such that
N
X
k=−N
|Xk|2 =
N
X
k=−N

sin(πk/2)
(πk/2)

2
= 1.94
The value of N is found by trial and error, adding consecutive values of the magnitude squared
of Fourier coefﬁcients. Using MATLAB, it is found that for N = 5 (dc and 5 harmonics) the
Fourier series approximation has a power of 1.93. Thus, 11 Fourier coefﬁcients give a very good
approximation to the periodic train of pulses, with about 97% of the signal power.
I
4.8 TIME AND FREQUENCY SHIFTING
Time shifting and frequency shifting are duals of each other.
I
Time-shifting: A periodic signal x(t), of period T0, remains periodic of the same period when shifted in
time. If Xk are the Fourier coefﬁcients of x(t), the Fourier coefﬁcients for x(t −t0) are
n
Xke−jk0t0 = |Xk|ej(∠Xk−k0t0)o
(4.27)
That is, only a change in phase is caused by the time shift. The magnitude spectrum remains the same.
I
Frequency-shifting: When a periodic signal x(t), of period T0, modulates a complex exponential ej1t:
I
The modulated signal x(t)ej1t is periodic of period T0 if 1 = M0 for an integer M ≥1.
I
The Fourier coefﬁcients Xk are shifted to frequencies k0 + 1.
I
The modulated signal is real-valued by multiplying x(t) by cos(1t).
If we delay or advance in time a periodic signal, the resulting signal is periodic of the same period.
Only a change in the phase of the coefﬁcients occurs to accommodate for the shift. Indeed, if
x(t) =
X
k
Xkejk0t

4.8 Time and Frequency Shifting
271
we then have that
x(t −t0) =
X
k
Xkejk0(t−t0) =
X
k
h
Xke−jk0t0
i
ejk0t
x(t + t0) =
X
k
Xkejk0(t+t0) =
X
k
h
Xkejk0t0
i
ejk0t
so that the Fourier coefﬁcients {Xk} corresponding to x(t) are changed to {Xke∓jk0t0} for
x(t ∓t0). In both cases, they have the same magnitude |Xk| but different phases.
In a dual way, if we multiply the above periodic signal x(t) by a complex exponential of frequency
1, ej1t, we obtain a so-called modulated signal y(t) and its spectrum is shifted in frequency by 1
with respect to the spectrum of the periodic signal x(t). In fact,
y(t) = x(t)ej1t
=
X
k
Xkej(0k+1)t
indicating that the harmonic frequencies are shifted by 1. The signal y(t) is not necessarily periodic.
Since T0 is the period of x(t), then
y(t + T0) = x(t + T0)ej1(t+T0)
and for it to be equal to y(t), then 1T0 = 2πM, for an integer M ̸= 0 or
1 = M0
M >> 1
which goes along with the condition that the modulating frequency 1 is chosen much larger than
0. The modulated signal is then given by
y(t) =
X
k
Xkej(0k+1)t =
X
k
Xkej0(k+M)t =
X
ℓ
Xℓ−Mej0ℓt
so that the Fourier coefﬁcients are shifted to new frequencies 0(k + M).
To keep the modulated signal real-valued, one multiplies the periodic signal x(t) by a cosine of
frequency 1 = M0 for M >> 1 to obtain a modulated signal
y1(t) = x(t) cos(1t)
=
X
k
0.5Xk[ej(k0+1)t + ej(k0−1)t]
so that the harmonic components are now centered around ±1.

272
CHAPTER 4:
Frequency Analysis: The Fourier Series
I Example 4.12
To illustrate the modulation property using MATLAB consider modulating a sinusoid cos(20πt)
with a train of square pulses
x1(t) = 0.5[1 + sign(sin(πt)]
and with a sinusoid
x2(t) = cos(πt)
Use our function fourierseries to ﬁnd the Fourier series of the modulated signals and plot their
magnitude line spectra.
Solution
The function sign is deﬁned as
sign(x(t)) =
−1
x(t) < 0
1
x(t) ≥0
(4.28)
That is, it determines the sign of the signal. Thus, 0.5[1 + sign
 sin(πt)] = u(t) −u(t −1) equals
1 for 0 ≤t ≤1, and 0 for 1 < t ≤2, which corresponds to a period of a train of square
pulses.
The following script allows us to compute the Fourier coefﬁcients of the two modulated signals.
%%%%%%%%%%%%%%%%%
% Example 4.12---Modulation
%%%%%%%%%%%%%%%%
syms t
T0 = 2;
m = heaviside(t) −heaviside(t −T0/2);
m1 = heaviside(t) −heaviside(t −T0);
x = m ∗cos(20 ∗pi ∗t);
x1 = m1 ∗cos(pi ∗t) ∗cos(20 ∗pi ∗t);
[X, w] = fourierseries(x, T0, 60);
[X1, w1] = fourierseries(x1, T0, 60);
The modulated signals and their corresponding magnitude line spectra are shown in Figure 4.13.
The Fourier coefﬁcients of the modulated signals are now clustered around the frequency 20π.

4.9 Response of LTI Systems to Periodic Signals
273
FIGURE 4.13
(a) Modulated square-wave
x1(t) cos(20πt) and (b) cosine
x2(t) cos(20πt) and their
respective magnitude line
spectra.
(a)
(b)
0
2
4
−1
−0.5
0
0.5
1
t
x1(t)
0
50
100
150
0
0.1
0.2
Ω
|X1k|
0
2
4
−1
−0.5
0
0.5
1
t
x2(t)
0
0
50
100
150
0.1
0.2
Ω
|X2k|
I
4.9 RESPONSE OF LTI SYSTEMS TO PERIODIC SIGNALS
The most important property of LTI systems is the eigenfunction property.
Eigenfunction property: In steady state, the response to a complex exponential (or a sinusoid) of a certain
frequency is the same complex exponential (or sinusoid), but its amplitude and phase are affected by the
frequency response of the system at that frequency.
Suppose that the impulse response of an LTI system is h(t) and that H(s) = L[h(t)] is the
corresponding transfer function. If the input to this system is a periodic signal x(t), of period T0,
with Fourier series
x(t) =
∞
X
k=−∞
Xkejk0t
0 = 2π
T0
(4.29)
then according to the eigenfunction property the output in the steady state is
yss(t) =
∞
X
k=−∞
[XkH( jk0)] ejk0t
(4.30)

274
CHAPTER 4:
Frequency Analysis: The Fourier Series
If we call Yk = XkH( jk0) we have a Fourier series representation of yss(t) with Yk as its Fourier
coefﬁcients.
4.9.1 Sinusoidal Steady State
If the input of a stable and causal LTI system, with impulse response h(t), is x(t) = Aej0t, the
output is
y(t) =
∞
Z
0
h(τ)x(t −τ)dτ = Aej0t
∞
Z
0
h(τ)e−j0τdτ
= Aej0tH( j0) = A|H( j0)|ej0t+∠H( j0)
(4.31)
The limits of the ﬁrst integral indicate that the system is causal (the h(τ) = 0 for τ < 0) and that
the input x(t −τ) is applied from −∞(when τ = ∞) to t (when τ = 0); thus y(t) is the steady-state
response of the system. If the input is a sinusoid—for example,
x1(t) = Re[x(t) = Aej0t] = A cos(0t)
(4.32)
then the corresponding steady-state response is
y1(t) = Re[A|H( j0)|ej0t+∠H( j0)]
= A|H( j0)| cos(0t + ∠H( j0)).
(4.33)
As in the eigenfunction property, the frequency of the output coincides with the frequency of the
input, however, the magnitude and the phase of the input signal is changed by the response of the
system at the input frequency.
The following script simulates the convolution of a sinusoid x(t) of frequency  = 20π, amplitude
10, and random phase with the impulse response h(t) (a modulated decaying exponential) of an LTI
system. The convolution integral is approximated using the MATLAB function conv.
%%%%%%%%%%%%%%%%%
% Simulation of Convolution
%%%%%%%%%%%%%%%%
clear all; clf
Ts = 0.01; Tend = 2; t = 0:Ts:Tend;
x = 10 ∗cos(20 ∗pi ∗t + pi ∗(rand(1, 1) −0.5));
% input signal
h = 20 ∗exp(−10.ˆt). ∗cos(40 ∗pi ∗t);
% impulse response
% approximate convolution integral
y = Ts ∗conv(x, h);

4.9 Response of LTI Systems to Periodic Signals
275
M = length(x);
ﬁgure(1)
x1 = [zeros(1, 5) x(1:M)];
z = y(1); y1 = [zeros(1, 5) z zeros(1, M −1)];
t0 = −5 ∗Ts:Ts:Tend;
for k = 0:M −6,
pause(0.05)
h0 = ﬂiplr(h);
h1 = [h0(M - k - 5:M) zeros(1, M - k - 1)];
subplot(211)
plot(t0, h1, ’r’)
hold on
plot(t0, x1, ’k’)
title(’Convolution of x(t) and h(t)’)
ylabel(’x(τ), h(t-τ)’); grid; axis([min(t0) max(t0) 1.1*min(x) 1.1*max(x)])
hold off
subplot(212)
plot(t0, y1, ’b’)
ylabel(’y(t) = (x ∗h)(t)’); grid; axis([min(t0) max(t0) 0.1 ∗min(x) 0.1 ∗max(x)])
z = [z y(k + 2)];
y1 = [zeros(1, 5) z zeros(1, M - length(z))];
end
Figure 4.14 displays the last step of the convolution integral simulation. Notice that the steady state
is attained in a very short time (around t = 0.5 sec). The transient changes every time that the script
is executed due to the random phase.
FIGURE 4.14
Convolution simulation: (a) input x(t) (solid line) and
h(t −τ) (dashed line), and (b) output y(t): transient
and steady-state response.
0
0.5
1
(a)
(b)
1.5
2
−10
−5
0
5
10
x(τ), h(t−τ)
τ
0
0.5
1
1.5
2
−0.5
0
0.5
y(t) =(x*h)(t)
t

276
CHAPTER 4:
Frequency Analysis: The Fourier Series
If the input x(t) of a causal and stable LTI system, with impulse response h(t), is periodic of period T0 and has
the Fourier series
x(t) = X0 + 2
∞
X
k=1
|Xk| cos(k0t + ∠Xk)
0 = 2π
T0
(4.34)
the steady-state response of the system is
y(t) = X0|H( j0)| cos(∠H( j0)) + 2
∞
X
k=1
|Xk||H( jk0)| cos(k0t + ∠Xk + ∠H( jk0))
(4.35)
where
H( jk0) =
∞
Z
0
h(τ)e−jk0τ dτ
(4.36)
is the frequency response of the system at k0.
Remarks
I
If the input signal x(t) is a combination of sinusoids of frequencies that are not harmonically related, the
signal is not periodic, but the eigenfunction property still holds. For instance, if
x(t) =
X
k
Ak cos(kt + θk)
and the frequency response of the LTI system is H( j), the steady-state response is
y(t) =
X
k
Ak|H( jk)| cos(kt + θk + ∠H( jk))
I
It is important to realize that if the LTI system is represented by a differential equation and the input
is a sinusoid, or combination of sinusoids, it is not necessary to use the Laplace transform to obtain the
complete response and then let t →∞to ﬁnd the sinusoidal steady-state response. The Laplace transform
is only needed to ﬁnd the transfer function of the system, which can then be used in Equation (4.35) to
ﬁnd the sinusoidal steady state.
4.9.2 Filtering of Periodic Signals
According to Equation (4.35) if we know the frequency response of the system (Eq. 4.36), at the
harmonic frequencies of the periodic input, H( jk0), we have that in the steady state the output of
the system y(t) is as follows:
I
Periodic of the same period as the input.
I
Its Fourier coefﬁcients are those of the input Xk multiplied by the frequency response at the
harmonic frequencies, H( jk0).

4.9 Response of LTI Systems to Periodic Signals
277
I Example 4.13
To illustrate the ﬁltering of a periodic signal, consider a zero-mean pulse train
x(t) =
∞
X
k=−∞,̸=0
sin(kπ/2)
kπ/2
ej2kπt
as the driving source of an RC circuit that realizes a low-pass ﬁlter (i.e., a system that tries to
keep the low-frequency harmonics and get rid of the high-frequency harmonics of the input). The
transfer function of the RC low-pass ﬁlter is
H(s) =
1
1 + s/100
Solution
The following script computes the frequency response of the ﬁlter at the harmonic frequencies
H( jk0) (see Figure 4.15).
%%%%%%%%%%%%%%%%%
% Example 4.13
%%%%%%%%%%%%%%%%
% Freq response of H(s)=1/(s/scale+1) -- low-pass ﬁlter
w0 = 2 * pi;
% fundamental frequency of input
M = 20; k = 0:M - 1; w1 = k. * w0;
% harmonic frequencies
H = 1./(1 + j * w1/100); Hm = abs(H); Ha = angle(H); % frequency response
subplot(211)
stem(w1, Hm, ’ﬁlled’); grid; ylabel(’—H(jω)—’)
axis([0 max(w1) 0 1.3])
subplot(212)
stem(w1, Ha, ’ﬁlled’); grid
axis([0 max(w1) -1 0])
ylabel(’¡H(j ω)’); xlabel(’w (rad/sec)’)
The response due to the pulse train can be found by ﬁnding the response to each of its Fourier
series components and adding them. Approximating x(t) using N = 20 harmonics by
xN(t) =
20
X
k=−20,̸=0
sin(kπ/2)
kπ/2
ej2kπt
Then the output voltage across the capacitor is given in the steady state,
yss(t) =
20
X
k=−20,̸=0
H( j2kπ)sin(kπ/2)
kπ/2
ej2kπt

278
CHAPTER 4:
Frequency Analysis: The Fourier Series
Because the magnitude response of the low-pass ﬁlter changes very little in the range of frequencies
of the input, the output signal is very much like the input (see Figure 4.15). The following script is
used to ﬁnd the response.
% low-pass ﬁltering
% FS coefﬁcients of input
X(1) = 0; % mean value
for k = 2:M - 1,
X(k) = sin((k −1) ∗pi/2)/((k −1) ∗pi/2);
end
% periodic signal
Ts = 0.001; t1 = 0:Ts:1 - Ts;L = length(t1);
x1 = [ones(1, L /4) zeros(1, L /2) ones(1, L /4)]; x1 = x1 −0.5; x = [x1 x1];
% output of ﬁlter
t = 0:Ts:2 −Ts;
y = X(1) ∗ones(1, length(t)) ∗Ha(1);
plot(t, y); axis([0 max(t) −.6 .6])
for k = 2:M - 1,
y = y + X(k) ∗Hm(k) ∗cos(w0 ∗(k −1). ∗t + Ha(k));
plot(t, y); axis([0 max(t) −.6 .6]); hold on
plot(t, x, ’r’); axis([0 max(t) −0.6 0.6]); grid
ylabel(’x(t), y(t)’); xlabel(’t (sec)’) ; hold off
pause(0.1)
end
(a)
(b)
0
20
40
60
80
100
0
0.5
1
|H(j Ω)|
0
20
40
60
80
100
−1
−0.5
0
<H(j Ω)
Ω (rad/sec)
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
x(t), y(t)
t(sec)
FIGURE 4.15
(a) Magnitude and phase response of the low-pass RC ﬁlter H(s) at harmonic frequencies, and (b) response due
to a train of pulses.
I

4.10 Other Properties of the Fourier Series
279
4.10 OTHER PROPERTIES OF THE FOURIER SERIES
In this section we present additional properties of the Fourier series that will help us with its compu-
tation and with our understanding of the relation between time and frequency. We are in particular
interested in showing that even and odd signals have special representations, and that it is possible
to ﬁnd the Fourier series of the sum, product, derivative, and integral of periodic signals without the
integration required by the deﬁnition of the series.
4.10.1 Reﬂection and Even and Odd Periodic Signals
If the Fourier series of x(t), periodic with fundamental frequency 0, is
x(t) =
X
k
Xkejk0t
then the one for its reﬂected version x(−t) is
x(−t) =
X
m
Xme−jm0t =
X
k
X−kejk0t
(4.37)
so that the Fourier coefﬁcients of x(−t) are X−k (remember that m and k are just dummy variables).
This can be used to simplify the computation of Fourier series of even and odd signals.
For an even signal x(t), we have that x(t) = x(−t), and as such Xk = X−k and therefore x(t) is naturally
represented in terms of cosines and a dc term. Indeed, its Fourier series is
x(t) = X0 +
−1
X
k=−∞
Xkejkot +
∞
X
k=1
Xkejkot
= X0 +
∞
X
k=1
Xk[ejkot + e−jkot]
= X0 + 2
∞
X
k=1
Xk cos(k0t)
(4.38)
indicating that Xk are real-valued. This is also seen from
Xk = 1
T0
Z
T0
x(t)e−jk0tdt = 1
T0
Z
T0
x(t)[cos(k0t) −j sin(k0)]dt
= 1
T0
Z
T0
x(t) cos(k0t)dt
because x(t) sin(k0t) is odd and their integral is zero. It will be similar for an odd function for which
x(t) = −x(−t), or Xk = −X−k, in which case the Fourier series has a zero dc value and sine harmonics.

280
CHAPTER 4:
Frequency Analysis: The Fourier Series
The Xk are purely imaginary. Indeed, for an odd x(t),
Xk = 1
T0
Z
T0
x(t)e−jk0tdt = 1
T0
Z
T0
x(t)[cos(k0t) −j sin(k0)]dt
= −j
T0
Z
T0
x(t) sin(k0t)dt
since x(t) cos(k0t) is odd. The Fourier series of an odd function can thus be written as
x(t) = 2
∞
X
k=1
( jXk) sin(k0t)
(4.39)
According to the even and odd decomposition, any periodic signal x(t) can be expressed as
x(t) = xe(t) + xo(t)
where xe(t) is the even and xo(t) is the odd component of x(t). Finding the Fourier coefﬁcients of
xe(t), which will be real, and those of xo(t), which will be purely imaginary, we would then have
Xk = Xek + Xok since
xe(t) = 0.5[x(t) + x(−t)]
⇒
Xek = 0.5[Xk + X−k]
xo(t) = 0.5[x(t) −x(−t)]
⇒
Xok = 0.5[Xk −X−k]
(4.40)
I
Reﬂection: If the Fourier coefﬁcients of a periodic signal x(t) are {Xk} then those of x(−t), the time-reversed
signal with the same period as x(t), are {X−k}.
I
Even periodic signal x(t): Its Fourier coefﬁcients Xk are real, and its trigonometric Fourier series is
x(t) = X0 + 2
∞
X
k=1
Xk cos(k0t)
(4.41)
I
Odd periodic signal x(t): Its Fourier coefﬁcients Xk are imaginary, and its trigonometric Fourier series is
x(t) = 2
∞
X
k=1
jXk sin(k0t)
(4.42)
For any periodic signal x(t) = xe(t) + xo(t) where xe(t) and xo(t) are the even and odd component of x(t), then
Xk = Xek + Xok
(4.43)
where {Xek} are the Fourier coefﬁcients of xe(t) and {Xok} are the Fourier coefﬁcients of xo(t).
I Example 4.14
Consider the periodic signals x(t) and y(t) shown in Figure 4.16. Determine their Fourier
coefﬁcients by using the symmetry conditions and the even–odd decomposition.

4.10 Other Properties of the Fourier Series
281
FIGURE 4.16
Nonsymmetric periodic signals.
2
1
−1
−2
0
0
1
2
3
−2
−1
2
2
3
t
t
···
···
···
···
x(t)
y(t)
Solution
The given signal x(t) is neither even nor odd, but the advance signal x(t + 0.5) is even with a period
of T0 = 2, 0 = π. Then between −1 and 1 the shifted period is
x1(t + 0.5) = 2[u(t + 0.5) −u(t −0.5)]
so that its Laplace transform is
X1(s)e0.5s = 2
s
h
e0.5s −e−0.5si
which gives the Fourier coefﬁcients
Xk = 1
2
2
jkπ
h
ejkπ/2 −e−jkπ/2i
e−jkπ/2
=
1
0.5πk sin(0.5πk)e−jkπ/2
after replacing s by jko = jkπ and dividing by the period T0 = 2. These coefﬁcients are complex
as corresponding to a signal that is neither even nor odd. The dc coefﬁcient is X0 = 1.
The given signal y(t) is neither even nor odd, and cannot be made even or odd by shifting. The even
and odd components of a period of y(t) are shown in Figure 4.17. The even and odd components
of a period y1(t) between −1 and 1 are
y1e(t) = [u(t + 1) −u(t −1)]
|
{z
}
rectangular pulse
+ [r(t + 1) −2r(t) + r(t −1)]
|
{z
}
triangle
y1o(t) = t[u(t + 1) −u(t −1)] = [(t + 1)u(t + 1) −u(t + 1)] −[(t −1)u(t −1) + u(t −1)]
= r(t + 1) −r(t −1) −u(t + 1) −u(t −1)

282
CHAPTER 4:
Frequency Analysis: The Fourier Series
FIGURE 4.17
Even and odd components of the period of y(t),
−1 ≤t ≤1.
t
y1e(t)
y1o(t)
1
−1
−1
−1
1
0
2
t
1
1
Thus, the mean value of ye(t) is the area under y1e(t) divided by 2 or 1.5, and for k ̸= 0,
Yek = 1
T0
Y1e(s)
s=jk0 = 1
2
1
s (es −e−s) + 1
s2 (es −2 + e−s)

s=jkπ
= sin(kπ)
πk
+ 1 −cos(kπ)
(kπ)2
= 0 + 1 −cos(kπ)
(kπ)2
= 1 −(−1)k
(kπ)2
The mean value of yo(t) is zero, and for k ̸= 0,
Yok = 1
T0
Y1o(s)
s=jk0 = 1
2
es −e−s
s2
−es + e−s
s

s=jkπ
= −jsin(kπ)
(kπ)2 + jcos(kπ)
kπ
= 0 + jcos(kπ)
kπ
= j(−1)k
kπ
Finally, the Fourier series coefﬁcients of y(t) are
Yk =
 Ye0 + Yo0 = 1.5 + 0 = 1.5
k = 0
Yek + Yok = (1 −(−1)k)/(kπ)2 + j(−1)k/(kπ)
k ̸= 0
I
4.10.2 Linearity of Fourier Series—Addition of Periodic Signals
I
Same fundamental frequency: If x(t) and y(t) are periodic signals with the same fundamental frequency
0, then the Fourier series coefﬁcients of z(t) = αx(t) + βy(t) for constants α and β are
Zk = αXk + βYk
(4.44)
where Xk and Yk are the Fourier coefﬁcients of x(t) and y(t).
I
Different fundamental frequencies: If x(t) is periodic of period T1, and y(t) is periodic of period T2 such
that T2/T1 = N/M, for nondivisible integers N and M, then z(t) = αx(t) + βy(t) is periodic of period

4.10 Other Properties of the Fourier Series
283
T0 = MT2 = NT1, and its Fourier coefﬁcients are
Zk = αXk/N + βYk/M
for integers k such that k/N, and k/M are integers
(4.45)
where Xk and Yk are the Fourier coefﬁcients of x(t) and y(t).
If x(t) and y(t) are periodic signals of the same period T0, the Fourier coefﬁcients of z(t) = αx(t) +
βy(t) (also periodic of period T0) are then Zk = αXk + βYk where Xk and Yk are the Fourier coefﬁcients
of x(t) and y(t), respectively.
In general, if x(t) is periodic of period T1, and y(t) is periodic of period T2, their sum z(t) = αx(t) +
βy(t) is periodic if the ratio T2/T1 is a rational number (i.e., T2/T1 = N/M for some nondivisible
integers N and M). If so, the period of z(t) is T0 = MT2 = NT1. The fundamental frequency of z(t)
would be 0 = 1/N = 2/M for 1 the fundamental frequency of x(t) and 2 the fundamental
frequency of y(t). The Fourier series of z(t) is then
z(t) = αx(t) + βy(t) = α
X
k
Xkej1kt + β
X
m
Ymej2mt
= α
X
k
XkejN0kt + β
X
m
YmejM0mt
= α
X
n=0,±N,±2N,...
Xn/Nej0nt + β
X
ℓ=0,±M,±2M,...
Yℓ/Mej0ℓt
Thus, the coefﬁcients are
Zk = αXk/N + βYk/M
for integers k such that k/N and k/M are integers.
I Example 4.15
Consider the sum z(t) of a periodic signal x(t) of period T1 = 2, with a periodic signal y(t) with
period T2 = 0.2. Find the Fourier coefﬁcients Zk of z(t) in terms of the Fourier coefﬁcients Xk and
Yk of x(t) and y(t).
Solution
The ratio T2/T1 = 1/10 = N/M is rational, so z(t) is periodic of period T0 = T1 = 10T2 = 2.
The fundamental frequency of z(t) is 0 = 1 = π, and 2 = 100 = 10π is the fundamental
frequency of y(t). Thus, the Fourier coefﬁcients of z(t) are
Zk =
Xk + Yk/10
when k = 0, ±10, ±20, . . .
Xk
otherwise
I

284
CHAPTER 4:
Frequency Analysis: The Fourier Series
4.10.3 Multiplication of Periodic Signals
If x(t) and y(t) are periodic signals of same period T0, then their product
z(t) = x(t)y(t)
(4.46)
is also periodic of period T0, and with Fourier coefﬁcients that are the convolution sum of the Fourier
coefﬁcients of x(t) and y(t):
Zk =
X
ℓ
XℓYk−ℓ
(4.47)
If x(t) and y(t) are periodic with the same period T0, then z(t) = x(t)y(t) is also periodic of period T0,
since z(t + kT0) = x(t + kT0)y(t + kT0) = x(t)y(t) = z(t). Furthermore,
x(t)y(t) =
X
k
Xkejk0t X
ℓ
Yℓejℓ0t =
X
k
X
ℓ
XkYℓej(k+ℓ)0t
=
X
m
"X
k
XkYm−k
#
ejm0t = z(t)
where we let m = k + ℓ. The coefﬁcients of the Fourier series of z(t) are then
Zm =
X
k
XkYm−k
or the convolution sum of the sequences Xk and Yk, to be formally deﬁned in Chapter 8.
I Example 4.16
Consider the train of rectangular pulses x(t) shown in Figure 4.4. Let z(t) = 0.25x2(t). Use the
Fourier series of z(t) to show that
Xk = α
X
m
XmXk−m
for some constant α. Determine α.
Solution
The signal 0.5x(t) is a train of pulses of unit amplitude, so that z(t) = (0.5x(t))2 = 0.5x(t). Thus,
Zk = 0.5Xk, but also as a product of 0.5x(t) with itself we have that
Zk =
X
m
[0.5Xm][0.5Xk−m]

4.10 Other Properties of the Fourier Series
285
and thus
0.5Xk
| {z }
Zk
= 0.25
X
m
XmXk−m
⇒Xk = 1
2
X
m
XmXk−m
(4.48)
so that α = 0.5.
The Fourier series of z(t) = 0.5x(t) according to the results in Example 4.5 is
z(t) = 0.5x(t) =
∞
X
k=−∞
sin(πk/2)
πk
ejk2πt
If we deﬁne
S(k) = 0.5Xk = sin(kπ/2)
kπ
⇒Xk = 2S(k)
we have from Equation (4.48) the interesting result
S(k) =
∞
X
m=−∞
S(m)S(k −m)
or the convolution sum of the discrete sinc function S(k) with itself is S(k).
I
4.10.4 Derivatives and Integrals of Periodic Signals
I
Derivative: The derivative dx(t)/dt of a periodic signal x(t), of period T0, is periodic of the same period T0.
If {Xk} are the coefﬁcients of the Fourier series of x(t), the Fourier coefﬁcients of dx(t)/dt are
jk0Xk
(4.49)
where 0 is the fundamental frequency of x(t).
I
Integral: For a zero-mean, periodic signal y(t), of period T0, the integral
z(t) =
t
Z
−∞
y(τ)dτ
is periodic of the same period as y(t), with Fourier coefﬁcients
Zk =
Yk
jk0
k ̸= 0
Z0 = −
X
m̸=0
Ym
1
jm0
0 = 2π
T0
(4.50)
These properties come naturally from the Fourier series representation of the periodic signal. Once
we ﬁnd the Fourier series of a periodic signal, we can differentiate it or integrate it (only when the dc

286
CHAPTER 4:
Frequency Analysis: The Fourier Series
value is zero). The derivative of a periodic signal is obtained by computing the derivative of each of
the terms of its Fourier series—that is, if
x(t) =
X
k
Xkejk0t
then
dx(t)
dt
=
X
k
Xk
dejk0t
dt
=
X
k
[ jk0Xk] ejk0t
indicating that if the Fourier coefﬁcients of x(t) are Xk, the Fourier coefﬁcients of dx(t)/dt are
jk0Xk.
To obtain the integral property we assume y(t) is a zero-mean signal so that its integral z(t) is ﬁnite.
If for some integer M, MT0 ≤t < (M + 1)T0, then
z(t) =
t
Z
−∞
y(τ)dτ =
MT0
Z
−∞
y(τ)dτ +
t
Z
MT0
y(τ)dτ
= 0 +
t
Z
MT0
y(τ)dτ
Replacing y(t) by its Fourier series gives
z(t) =
t
Z
MT0
y(τ)dτ =
t
Z
MT0
X
k̸=0
Ykejk0τdτ
=
X
k̸=0
Yk
t
Z
MT0
ejk0τdτ =
X
k̸=0
Yk
1
jk0
h
ejk0t −1
i
= −
X
k̸=0
Yk
1
jk0
+
X
k̸=0
Yk
1
jk0
ejk0t
where the ﬁrst term corresponds to the average Z0 and Zk = Yk/( jk0), k ̸= 0, are the rest of the
Fourier coefﬁcients of z(t).
Remarks It should be now clear why the derivative of a periodic signal x(t) enhances its higher harmonics.
Indeed, the Fourier coefﬁcients of the derivative dx(t)/dt are those of x(t), Xk, multiplied by j0k, which
increases with k. Likewise, the integration of a zero-mean periodic signal x(t) does the opposite—that is, it
makes the signal smoother, as we multiply Xk by decreasing terms 1/( jk0) as k increases.

4.10 Other Properties of the Fourier Series
287
I Example 4.17
Let g(t) be the derivative of a triangular train of pulses f(t), of period T0 = 1. The period of f(t),
0 ≤t ≤1, is
f1(t) = 2r(t) −4r(t −0.5) + 2r(t −1)
Use the Fourier series of g(t) to ﬁnd the Fourier series of f(t).
Solution
According to the derivative property we have that
Fk =
Gk
jk0
k ̸= 0
are the Fourier coefﬁcients of f(t). The signal g(t) = df(t)/dt has a corresponding period g1(t) =
df1(t)/dt = 2u(t) −4u(t −0.5) + 2u(t −1). The Fourier series coefﬁcients of g(t) are
Gk = 2e−0.5s
s

e0.5s −2 + e−0.5s
|s=j2πk = 2(−1)k cos(πk) −1
jπk
k ̸= 0
which are used to obtain the coefﬁcients Fk for k ̸= 0. The dc component of f(t) is found to be 0.5
from its plot as g(t) does not provide it.
I
I Example 4.18
Consider the reverse of Example 4.17. That is, given the periodic signal g(t) of period T0 = 1 and
Fourier coefﬁcients
Gk = 2(−1)k cos(πk) −1
jπk
k ̸= 0
and G0 = 0. Find the integral
z(t) =
t
Z
−∞
g(τ)dτ
Solution
As shown above, z(t) is also periodic of the same period as g(t) (i.e., T0 = 1). The Fourier
coefﬁcients of z(t) are
Zk =
Gk
j0k = (−1)k 4(cos(πk) −1)
( j2πk)2
= (−1)(k+1) cos(πk) −1
π2k2
k ̸= 0

288
CHAPTER 4:
Frequency Analysis: The Fourier Series
FIGURE 4.18
Two periods of the approximate triangular signal
xN(t) using 100 harmonics.
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
xN(t)
t(sec)
and the average term is
Z0 = −
X
m̸=0
Gm
1
j2mπ =
X
m̸=0
(−1)m cos(πm) −1
(πm)2
= 0.5
∞
X
m=−∞,m̸=0
(−1)m+1
sin(πm/2)
(πm/2)
2
where we used 1 −cos(πm) = 2 sin2(πm/2). We used the following script to obtain the average,
and to approximate the triangular signal using 100 harmonics (see Figure 4.18). The mean is
obtained as 0.498.
%%%%%%%%%%%%%%%%%
% Example 4.18
%%%%%%%%%%%%%%%%
clf; clear all
w0 = 2 * pi; N = 100; % parameters of periodic signal
% computation of mean value
DC = 0;
for m = 1:N,
DC = DC + 2 * (-1) ˆ (m) * (cos(pi * m) -1)/(pi * m) ˆ 2;
end
% computation of Fourier series coefﬁcients
Ts = 0.001; t = 0:Ts:2 - Ts;
for k = 1:N,
X(k) = (-1)ˆ(k + 1)*(cos(pi * k) - 1)/((pi * k)ˆ2);
end
X = [DC X]; % Fourier series coefﬁcients
xa = X(1)*ones(1,length(t));
ﬁgure(1)

4.11 What Have We Accomplished? Where Do We Go from Here?
289
for k = 2:N,
xa = xa + 2 * abs(X(k)) * cos(w0 * (k - 1). * t + angle(X(k))); % approximate signal
end
I
4.11 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO FROM
HERE?
Periodic signals are not to be found in practice, so where did Fourier get the intuition to come up with
a representation for them? As you will see, the fact that periodic signals are not found in practice does
not mean that they are not useful. The Fourier representation of periodic signals will be fundamental
in ﬁnding a representation for nonperiodic signals.
A very important concept you have learned in this chapter is that the inverse relation between time
and frequency provides complementary information for the signal. The frequency domain consti-
tutes the other side of the coin in representing signals. As mentioned before, it is the eigenfunction
property of linear time-invariant systems that holds the theory together. It will provide the funda-
mental principle for ﬁltering. You should have started to experience d´ej`a vu in terms of the properties
of the Fourier series; some look like a version of the ones in the Laplace transform. This is due to
the connection existing between these transforms. You should have also noticed the usefulness of the
Laplace transform in ﬁnding the Fourier coefﬁcients, avoiding integration whenever possible. Table
4.1 provides the basic properties of the Fourier series for continuous–time periodic signals.
Chapter 5 will extend some of the results obtained in this chapter, thus unifying the treatment of
periodic and nonperiodic signals and the concept of spectrum. Also the frequency representation of
Table 4.1 Basic Properties of Fourier Series
Time Domain
Frequency Domain
Signals and constants
x(t), y(t) periodic
Xk, Yk
with period T0, α, β
Linearity
αx(t) + βy(t)
αXk + βYk
Parseval’s power relation
Px =
1
T0
R
T0 |x(t)|2dt
Px = P
k |Xk|2
Differentiation
dx(t)
dt
jk0Xk
Integration
R t
−∞x(t′)dt′ only if X0 = 0
Xk
jk0
k ̸= 0, −
X
m̸=0
Xm
jm0
, dc
Time shifting
x(t −α)
e−jα0Xk
Frequency shifting
ejM0tx(t)
Xk−M
Symmetry
x(t) real
|Xk| = |X−k| even
function of k
∠Xk = −∠X−k odd
function of k

290
CHAPTER 4:
Frequency Analysis: The Fourier Series
systems will be introduced and exempliﬁed by its application in ﬁltering. Modulation is the basic
tool in communications and can be easily explained in the frequency domain.
PROBLEMS
4.1. Eigenfunctions and LTI systems
The eigenfunction property is only valid for LTI systems. Consider the cases of nonlinear and of time-
varying systems.
(a) A system represented by the following input–output equation is nonlinear:
y(t) = x2(t)
Let x(t) = ejπt/4. Find the corresponding system output y(t). Does the eigenfunction property hold?
Explain.
(b) Consider a time-varying system
y(t) = x(t)[u(t) −u(t −1)]
Let x(t) = ejπt/4. Find the corresponding system output y(t). Does the eigenfunction property hold?
Explain.
4.2. Eigenfunctions and LTI systems
The output of an LTI system is
y(t) =
t
Z
0
h(τ)x(t −τ)dτ
where the input x(t) and the impulse response h(t) of the system are assumed to be causal. Let x(t) =
2 cos(2πt)u(t). Compute the output y(t) in the steady state and determine if the eigenfunction property
holds.
4.3. Eigenfunctions and frequency response of LTI systems
The input–output equation for an analog averager is
y(t) = 1
T
t
Z
t−T
x(τ)dτ
Let x(t) = ej0t. Since the system is LTI, then the output should be
y(t) = ej0tH( j0)
(a) Find y(t) for the given input and then compare it with the above equation to ﬁnd H( j0), the response
of the averager at frequency 0.
(b) Find H(s) and verify the frequency response value H( j0) obtained above.
4.4. Generality of eigenfunctions
The eigenfunction property holds for any input signal, periodic or not, that can be expressed in sinusoidal
form.
(a) Consider the input x(t) = cos(t) + cos(2πt), −∞< t < ∞, into an LTI system. Is x(t) periodic? If so,
indicate its period.

Problems
291
(b) Suppose that the system is represented by a ﬁrst-order differential equation,
y′(t) + 5y(t) = x(t)
where y(t) is the output of the system and the given x(t) is the input of the system. Find the steady-
state response y(t) due to x(t) using the eigenfunction property.
4.5. Steady state of LTI systems
The transfer function of an LTI system is
H(s) = Y(s)
X(s) =
s + 1
s2 + 3s + 2
If the input to this system is x(t) = 1 + cos(t + π/4), −∞< t < ∞, what is the output y(t) in the steady
state?
4.6. Eigenfunction property of LTI systems and Laplace
The transfer function of an LTI system is given by
H(s) = Y(s)
X(s) =
1
s2 + 3s + 2
and its input is
x(t) = 4u(t)
(a) Use the eigenfunction property of LTI systems to ﬁnd the steady-state response y(t) of this system.
(b) Verify your result in (a) by means of the Laplace transform.
4.7. Different ways to compute the Fourier coefﬁcients—MATLAB
We would like to ﬁnd the Fourier series of a sawtooth periodic signal x(t) of period T0 = 1. The period of
x(t) is
x1(t) = r(t)[u(t) −u(t −1)]
(a) Carefully plot x(t) and compute the Fourier coefﬁcients Xk using the integral deﬁnition.
(b) An easier way to do this is to use the Laplace transform of x1(t). Find Xk this way.
(c) Use MATLAB to plot the signal x(t) and its magnitude and phase line spectra.
(d) Obtain a trigonometric Fourier series ˆx(t) consisting of the DC term and 40 harmonics to approximate
x(t). Use MATLAB to ﬁnd the values of ˆx(t) for t = 0 to 10 in steps of 0.001. How does it compare with
x(t)?
4.8. Addition of periodic signals—MATLAB
Consider a sawtooth signal x(t) with period T0 = 2 and period
x1(t) =
 t
0 ≤t < 1
0
otherwise
(a) Find the Fourier coefﬁcients Xk using the Laplace transform. Consider the cases when k is odd and
even (k ̸= 0). You need to compute X0 directly from the signal.
(b) Let y(t) = x(−t). Find the Fourier coefﬁcients Yk.
(c) The sum z(t) = x(t) + y(t) is a triangular function. Find the Fourier coefﬁcients Zk and compare them
to Xk + Yk.
(d) Use MATLAB to plot x(t), y(t), and z(t) and their corresponding magnitude line spectra. Find an
approximate of z(t) using the dc value and10 harmonics and plot it.

292
CHAPTER 4:
Frequency Analysis: The Fourier Series
4.9. Fourier series coefﬁcients via Laplace—MATLAB
The computation of the Fourier series coefﬁcients is simpliﬁed by the relation between the formula for
these coefﬁcients and the Laplace transform of a period of the periodic signal.
(a) A periodic signal x(t), of period T0 = 2 sec, has as period with the signal x1(t) = u(t) −u(t −1), so
that x(t) can be represented as
x(t) =
∞
X
m=−∞
x1(t −mT0)
Expand this sum, and use the information for x1(t) and T0 to carefully plot the periodic signal x(t).
(b) Find the Laplace transform of x1(t), and let s = jk0, where 0 = 2π/T0 is the fundamental frequency,
to obtain the Fourier coefﬁcients of x(t).
(c) Use MATLAB to plot the magnitude line spectrum of x(t). Find an approximate of x(t) using the dc
and 40 harmonics. Plot it.
4.10. Half- and full-wave rectifying and Fourier—MATLAB
Rectifying a sinusoid provides a way to create a dc source. In this problem we consider the Fourier series
of the full- and half-wave rectiﬁed signals. The full-wave rectiﬁed signal xf (t) has a period T0 = 1 and its
period from 0 to 1 is
x1(t) = sin(πt)
0 ≤t ≤1
while the period for the half-wave rectiﬁer signal xh(t) is
x2(t) =
sin(πt)
0 ≤t ≤1
0
1 < t ≤2
with period T1 = 2.
(a) Obtain the Fourier coefﬁcients for both of these periodic signals.
(b) Use the even and odd decomposition of xh(t) to obtain its Fourier coefﬁcients. This computation of
the Fourier coefﬁcients of xh(t) avoids some difﬁculties when you attempt to plot its magnitude line
spectrum. Use MATLAB and your analytic results here to plot the magnitude line spectrum of the
half-wave signal and use the dc and 40 harmonics to obtain an approximation of the half-wave signal.
4.11. Smoothness and Fourier series—MATLAB
The smoothness of a period determines the way the magnitude line spectrum decays. Consider the
following periodic signals x(t) and y(t), both of period T0 = 2 sec, and with a period from 0 ≤t ≤T0
equal to
x1(t) = u(t) −u(t −1)
y1(t) = r(t) −2r(t −1) + r(t −2)
Find the Fourier series coefﬁcients of x(t) and y(t) and use MATLAB to plot their magnitude line spectrum
for k = 0, ±1, ±2, . . . , ±20. Determine which of these spectra decays faster and how it relates to the
smoothness of the period. (To see this relate |Xk| to the corresponding |Yk|.)
4.12. Time support and frequency content—MATLAB
The support of a period of a periodic signal relates inversely to the support of the line spectrum. Consider
two periodic signals: x(t) of period T0 = 2 and y(t) of period T1 = 1, and with periods
x1(t) = u(t) −u(t −1)
0 ≤t ≤2
y1(t) = u(t) −u(t −0.5)
0 ≤t ≤1

Problems
293
(a) Find the Fourier series coefﬁcients for x(t) and y(t).
(b) Use MATLAB to plot the magnitude line spectra of the two signals from 0 to 40π rad/sec. Plot them on
the same ﬁgure so you can determine which has a broader support. Indicate which signal is smoother
and explain how it relates to its line spectrum.
4.13. Derivatives and Fourier Series
Given the Fourier series representation for a periodic signal,
x(t) =
∞
X
k=−∞
Xkejk0t
we can compute derivatives of it, just like for any other signal.
(a) Consider the periodic train of pulses shown in Figure 4.19. Compute its derivative
y(t) = dx(t)
dt
and carefully plot it. Find the Fourier series of y(t).
(b) Use the Fourier series representation of x(t) and ﬁnd its derivative to obtain the Fourier series of y(t).
How does it compare to the Fourier series obtained above?
4.14. Fourier series of sampling delta
The periodic signal
δTs(t) =
∞
X
m=−∞
δ(t −mTs)
will be very useful in the sampling of continuous-time signals.
(a) Find the Fourier series of this signal—that is,
δTs(t) =
∞
X
k=−∞
1kejkst
ﬁnd the Fourier coefﬁcients 1k.
(b) Plot the magnitude line spectrum of this signal.
(c) Plot δTs(t) and its corresponding line spectrum 1k as functions of time and frequency. Are they both
periodic? How are their periods related? Explain.
FIGURE 4.19
Problem 4.13: train of
rectangular pulses.
· · ·
· · ·
2
x(t)
−0.25
−0.75
0.75
0.25
1.25
−1.25
t
T0 = 1

294
CHAPTER 4:
Frequency Analysis: The Fourier Series
4.15. Figuring out Fourier’s idea
Fourier proposed to represent a periodic signal as a sum of sinusoids, perhaps an inﬁnite number of
them. For instance, consider the representation of a periodic signal x(t) as a sum of cosines of different
frequencies
x(t) =
∞
X
k=0
Ak cos(kt + θk)
(a) If x(t) is periodic of period T0, what should the frequencies k be?
(b) Consider x(t) = 2 + cos(2πt) −3 cos(6πt + π/4). Is this signal periodic? If so, what is its period T0?
Determine its trigonometric Fourier series as given above by specifying the values of Ak and θk for all
values of k = 0, 1, . . ..
(c) Let the signal x1(t) = 2 + cos(2πt) −3 cos(20t + π/4) (this signal is almost like x(t) given above,
except that the frequency 6π rad/sec of the second cosine has been approximated by 20 rad/sec).
Is this signal periodic? Can you determine its Fourier series as given above by specifying the values
of Ak and θk for all values of k = 0, 1, . . .? Explain.
4.16. DC output from a full-wave rectiﬁed signal—MATLAB
Consider a full-wave rectiﬁer that has as output a periodic signal x(t) of period T0 = 1 and a period of it is
given as
x1(t) =
cos(πt)
−0.5 ≤t ≤0.5
0
otherwise
(a) Obtain the Fourier coefﬁcients Xk.
(b) Suppose we pass x(t) through an ideal ﬁlter of transfer function H(s). Determine the values of this
ﬁlter at harmonic frequencies 2πk, = 0, ±1, ±2, . . ., so that its output is a constant (i.e., we have a dc
source).
(c) Use MATLAB to plot the signal x(t) and its magnitude line spectrum.
4.17. Fourier series of sum of periodic signals
Suppose you have the Fourier series of two periodic signals x(t) and y(t) of periods T1 and T2, respectively.
Let Xk and Yk be the Fourier series coefﬁcients corresponding to x(t) and y(t).
(a) If T1 = T2, what would be the Fourier series coefﬁcients of z(t) = x(t) + y(t) in terms of Xk
and Yk?
(b) If T1 = 2T2, determine the Fourier series coefﬁcients of w(t) = x(t) + y(t) in terms of Xk and Yk?
4.18. Manipulation of periodic signals
Let the following be the Fourier series of a periodic signal x(t) of period T0 (fundamental frequency 0 =
2π/T0):
x(t) =
∞
X
k=−∞
Xkej0kt
Consider the following functions of x(t), and determine if they are periodic and what are their periods
if so:
I
y(t) = 2x(t) −3
I
z(t) = x(t −2) + x(t)
I
w(t) = x(2t)
Express the Fourier series coefﬁcients Yk, Zk, and Wk in terms of Xk.

Problems
295
4.19. Using properties to ﬁnd the Fourier series
Use the Fourier series of a square train of pulses (done in this chapter) to compute the Fourier series of the
triangular signal x(t) with a period,
x1(t) = r(t) −2r(t −1) + r(t −2)
(a) Find the derivative of x(t) or y(t) = dx(t)/dt and carefully plot it. Plot also z(t) = y(t) + 1. Use the
Fourier series of the square train of pulses to compute the Fourier series coefﬁcients of y(t) and z(t).
(a) Obtain the trigonometric Fourier series of y(t) and z(t) and explain why they are represented by sines
and why z(t) has a nonzero mean.
(c) Obtain the Fourier series coefﬁcients of x(t) from those of y(t).
(d) Obtain the sinusoidal form of x(t) and explain why the cosine representation is more appropriate for
this signal than a sine representation.
4.20. Applying Parseval’s result—MATLAB
We wish to approximate the triangular signal x(t) in Problem 4.19 by its Fourier series with a ﬁnite number
of terms, let’s say 2N. This approximation should have 95% of the average power of the triangular signal.
Use MATLAB to ﬁnd the value of N.
4.21. Fourier series of multiplication of periodic signals
Consider the Fourier series of two periodic signals,
x(t) =
∞
X
k=−∞
Xkejokt
y(t) =
∞
X
k=−∞
Ykej1kt
(a) Let 1 = 0. Is z(t) = x(t)y(t) periodic? If so, what is its period and its Fourier series coefﬁcients?
(b) If 1 = 20. Is w(t) = x(t)y(t) periodic? If so, what is its period and its Fourier series coefﬁcients?
4.22. Integration of periodic signals
Consider now the integral of the Fourier series of the pulse signal p(t) = x(t) −1 of period T0 = 1, where
x(t) is given in Figure 4.20.
FIGURE 4.20
Problem 4.23: train of
rectangular pulses.
· · ·
· · ·
2
x(t)
−0.25
−0.75
0.75
0.25
1.25
−1.25
t
T0 = 1

296
CHAPTER 4:
Frequency Analysis: The Fourier Series
(a) Given that an integral of p(t) is the area under the curve, ﬁnd and plot the function
s(t) =
t
Z
−∞
p(t)dt
t ≤1
Indicate the values of s(t) for t = 0, 0.25, 0.5, 0.75, and 1.
(b) Find the Fourier series of p(t) and s(t) and relate their Fourier series coefﬁcients.
(c) Suppose you want to compute the integral
T0/2
Z
−T0/2
p(t)dt
using the Fourier series of p(t). What is the integral equal to?
(d) You can also compute the integral from the plot of p(t):
T0/2
Z
−T0/2
p(t)dt
What is it? Does it coincide with the result obtained using the Fourier series? Explain.
4.23. Full-wave rectifying and DC sources
Let x(t) = sin2(2πt), a periodic signal of period T0 = 1, and y(t) = | sin(2πt)|, which is also periodic of
period T1 = 0.5.
(a) A trigonometric identity gives that
x(t) = 1
2 [1 −cos(4πt)]
Use this result to ﬁnd its complex exponential Fourier series.
(b) Use the Laplace transform to ﬁnd the Fourier series of y(t).
(c) Are x(t) and y(t) identical? Explain.
(d) Indicate how you would use an ideal low-pass ﬁlter to get a DC source of unit value from x(t) and
y(t). Indicate the bandwidth and the magnitude of the ﬁlters. Compare these two signals in terms of
advantages or disadvantages in generating the desired DC source.
4.24. Windowing and music sounds—MATLAB
In the computer generation of musical sounds, pure tones need to be windowed to make them more
interesting. Windowing mimics the way a musician would approach the generation of a certain sound.
Increasing the richness of the harmonic frequencies is the result of the windowing, as we will see in this
problem. Consider the generation of a musical note with frequencies around fA = 880 Hz. Assume our
“musician” while playing this note uses three strokes corresponding to a window w1(t) = r(t) −r(t −
T1) −r(t −T2) + r(t −T0), so that the resulting sound would be the multiplication, or windowing, of a
pure sinusoid cos(2πfAt) by a periodic signal w(t), with w1(t) a period that repeats every T0 = 5T where
T is the period of the sinusoid. Let T1 = T0/4 and T2 = 3T0/4.
(a) Analytically determine the Fourier series of the window w(t) and plot its line spectrum using MATLAB.
Indicate how you would choose the number of harmonics needed to obtain a good approximation to
w(t).
(b) Use the modulation or the convolution properties of the Fourier series to obtain the coefﬁcients of the
product s(t) = cos(2πfAt)w(t). Use MATLAB to plot the line spectrum of this periodic signal and again
determine how many harmonic frequencies you would need to obtain a good approximation to s(t).

Problems
297
(c) The line spectrum of the pure tone p(t) = cos(2πfAt) only displays one harmonic, the one
corresponding to the fA = 880 Hz frequency. How many more harmonics does s(t) have? To listen
to the richness in harmonics use the MATLAB function sound to play the sinusoid p(t) and s(t) (use
Fs = 2 × 880 Hz to play both).
(d) Consider a combination of notes in a certain scale; for instance, let
p(t) = sin(2π × 440t) + sin(2π × 550t) + sin(2π × 660t)
Use the same windowing w(t), and let s(t) = p(t)w(t). Use MATLAB to plot p(t) and s(t) and to
compute and plot their corresponding line spectra. Use sound to play p(nTs) and s(nTs) using
Fs = 1000.
4.25. Computation of π—MATLAB
As you know, π is an irrational number that can only be approximated by a number with a ﬁnite number
of decimals. How to compute this value recursively is a problem of theoretical interest. In this problem we
show that the Fourier series can provide that formulation.
(a) Consider a train of rectangular pulses x(t), with a period
x1(t) = 2[u(t + 0.25) −u(t −0.25)] −1
−0.5 ≤t ≤0.5
and period T0 = 1. Plot the periodic signal and ﬁnd its trigonometric Fourier series.
(b) Use the above Fourier series to ﬁnd an inﬁnite sum for π.
(c) If πN is an approximation of the inﬁnite sum with N coefﬁcients, and π is the value given by MATLAB,
ﬁnd the value of N so that πN is 95% of the value of π given by MATLAB.
4.26. Square error approximation of periodic signals—MATLAB
To understand the Fourier series consider a more general problem, where a periodic signal x(t), of period
T0, is approximated as a ﬁnite sum of terms,
ˆx(t) =
N
X
k=−N
ˆXkφk(t)
where {φk(t)} are orthonormal functions. To pose the problem as an optimization problem, consider the
square error
ε =
Z
T0
|x(t) −ˆx(t)|2dt
and look for the coefﬁcients { ˆX(k)} that minimize ε.
(a) Assume that x(t) as well as ˆx(t) are real valued, and that x(t) is even so that the Fourier series
coefﬁcients Xk are real. Show that the error can be expressed as
ε =
Z
T0
x2(t)dt −2
N
X
k=−N
ˆXk
Z
T0
x(t)φk(t)dt +
N
X
ℓ=−N
| ˆXℓ|2T0
(b) Compute the derivative of ε with respect to ˆXn and set it to zero to minimize the error. Find ˆXn.
(c) In the Fourier series the {φk(t)} are the complex exponentials and the { ˆXn} coincide with the Fourier
series coefﬁcients. To illustrate the above procedure consider the case of the pulse signal x(t), of period
T0 = 1, and a period
x1(t) = 2[u(t + 0.25) −u(t −0.25)]

298
CHAPTER 4:
Frequency Analysis: The Fourier Series
Use MATLAB to compute and plot the approximation ˆx(t) and the errror ε for increasing values of N
from 1 to 100.
(d) Concentrate your plot of ˆx(t) around the one of the discontinuities, and observe the Gibb’s phe-
nomenon. Does it disappear when N is very large. Plot ˆx(t) around the discontinuity for N =
1000.
4.27. Walsh functions—MATLAB
As seen in Problem 4.26, the Fourier series is one of a possible class of representations in terms of orthonor-
mal functions. Consider the case of the Walsh functions, which are a set of rectangular pulse signals that
are orthonormal in a ﬁnite time interval [0, 1]. These functions are such that: (1) they take only 1 and −1
values, (2) φk(0) = 1 for all k, and (3) they are ordered according to the number of sign changes.
(a) Consider obtaining the functions {φk}k = 0, . . . ,5. The Walsh functions are clearly normal since when
squared they are unity for t ∈[0, 1]. Let φ0(t) = 1 for t ∈[0, 1] and zero elsewhere. Obtain φ1(t) with
one change of sign and that is orthogonal to φ0(t). Find then φ2(t), which has two changes of sign
and is orthogonal to both φ0(t) and φ1(t). Continue this process. Carefully plot the {φi(t)}, i = 0, . . . , 5.
Use the MATLAB function stairs to plot these Walsh functions.
(b) Consider the Walsh functions obtained above as sequences of 1s and −1s of length 8, and care-
fully write these six sequences. Observe the symmetry of the sequences corresponding to {φi(t), i =
0, 1, 3, 5}, and determine the circular shift needed to ﬁnd the sequence corresponding to φ2(t) from
the sequence from φ1(t), and φ4(t) from φ3(t). Write a MATLAB script that generates a matrix 8 with
entries as the sequences. Find the product (1/8)88T, and explain how this result connects with the
orthonormality of the Walsh functions.
(c) We wish to approximate a ramp function x(t) = r(t), 0 ≤t ≤1, using {φk}k=0, . . . ,5. This could be
written as
r = 8a
where r is a vector of x(nT) = r(nT) where T = 1/8, a are the coefﬁcients of the expansion, and 8 is
the Walsh matrix found above. Determine the vector a and use it to obtain an approximation of x(t).
Plot x(t) and the approximation ˆx(t) (use stairs for this signal).

CHAPTER 5
Frequency Analysis: The Fourier Transform
Imagination is the beginning of creation.
You imagine what you desire, you will what you imagine,
and at last you create what you will.
George Bernard Shaw (1856–1950)
Irish dramatist
5.1 INTRODUCTION
In this chapter we continue the frequency analysis of signals. In particular, we will concentrate in the
following issues:
I
Generalization of the Fourier series—The frequency representation of signals as well as the frequency
response of systems are tools of great signiﬁcance in signal processing, communications, and
control theory. In this chapter we will complete the Fourier representation of signals by extend-
ing it to aperiodic signals. By a limiting process the harmonic representation of periodic signals
is extended to the Fourier transform, a frequency-dense representation for nonperiodic signals.
The concept of spectrum introduced for periodic signals is generalized for both finite-power and
finite-energy signals. Thus, the Fourier transform measures the frequency content of a signal, and
uniﬁes the representation of periodic and aperiodic signals.
I
Laplace and Fourier transform—In this chapter the connection between the Laplace and the Fourier
transforms will be highlighted for computational and analytical reasons. The Fourier transform
turns out to be a very important case of the Laplace transform for signals of which the region of
convergence includes the j axis. There are, however, signals where the Fourier transform cannot
be obtained from the Laplace transform; for those cases, properties of the Fourier transform will
be used. The duality of the direct and inverse transforms is of special interest in computing the
Fourier transform.
I
Basics of filtering—Filtering is an important application of the Fourier transform. The Fourier rep-
resentation of signals and the eigenfunction property of LTI systems provide the tools to change
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00008-9
c⃝2011, Elsevier Inc. All rights reserved.
299

300
CHAPTER 5:
Frequency Analysis: The Fourier Transform
the frequency content of a signal by processing it with an LTI system with a desired frequency
response.
I
Modulation and communications—The idea of changing the frequency content of a signal via modu-
lation is basic in analog communications. Modulation allows us to send signals over the airwaves
using antennas of reasonable sizes. Voice and music are relative low-frequency signals that can-
not be easily radiated without the help of modulation. Continuous-wave modulation changes
the amplitude, the frequency, or the phase of a sinusoidal carrier of frequency much greater than
the frequencies present in the message we wish to transmit.
5.2 FROM THE FOURIER SERIES TO THE FOURIER TRANSFORM
In practice there are no periodic signals—such signals would have infinite supports and exact periods,
which are not possible. Since only finite-support signals can be processed numerically, signals in
practice are treated as aperiodic. To obtain the Fourier representation of aperiodic signals, we use the
Fourier series representation in a limiting process.
An aperiodic, or nonperiodic, signal x(t) can be thought of as a periodic signal ˜x(t) with an infinite period.
Using the Fourier series representation of this signal and a limiting process we obtain a pair
x(t)
⇔
X()
where the signal x(t) is transformed into a function X() in the frequency domain by the
Fourier transform:
X() =
∞
Z
−∞
x(t)e−jtdt
(5.1)
while X() is transformed into a signal x(t) in the time domain by the
Inverse Fourier transform:
x(t) = 1
2π
∞
Z
−∞
X()e jtd
(5.2)
Any aperiodic signal can be assumed to be periodic with an infinite period. That is, an aperiodic
signal x(t) can be expressed as
x(t) = lim
T0→∞˜x(t)
where ˜x(t) is a periodic signal of period T0. The Fourier series representation of ˜x(t) is
˜x(t) =
∞
X
n=−∞
Xne jn0t
0 = 2π
T0
Xn = 1
T0
T0/2
Z
−T0/2
˜x(t)e−jn0tdt

5.2 From the Fourier Series to the Fourier Transform
301
As T0 →∞, Xn will tend to zero. To avoid this we define X(n) = T0Xn where {n = n0} are the
harmonic frequencies.
Letting 1 = 2π/T0 = 0 be the frequency interval between harmonics, we can then write the above
equations as
˜x(t) =
∞
X
n=−∞
X(n)
T0
e jnt =
X
n
X(n)e jnt 1
2π
X(n) =
T0/2
Z
−T0/2
˜x(t)e−jntdt
As T0 →∞, then 1 →d, the line spectrum becomes denser—that is, the lines in the line spectrum
get closer, the sum becomes an integral, and n = n0 = n1 →, so that in the limit we obtain
x(t) = 1
2π
∞
Z
−∞
X()ejtd
X() =
∞
Z
−∞
x(t)e−jtdt
which are the inverse and the direct Fourier transforms, respectively. The first equation transforms
a function in the frequency domain X() into a signal in the time domain x(t), while the other
equation does the opposite.
The Fourier transform measures the frequency content of a signal. As we will see, time and frequency
are complementary, thus the characterization in one domain provides information that is not clearly
available in the other.
Remarks
I
Although we have obtained the Fourier transform from the Fourier series, the Fourier transform of a periodic
signal cannot be obtained from the above integral. Consider x(t) = cos(0t), −∞< t < ∞, which is peri-
odic of period 2π/0. If you attempt to compute its Fourier transform using the integral you do not have a
well-defined problem (try to obtain the integral to convince yourself ). But it is known from the line spectrum
that the power of this signal is concentrated at the frequencies ±0, so somehow we should be able to find
its Fourier transform. Sinusoids are basic functions.
I
On the other hand, if you consider a decaying exponential x(t) = e−|a|t signal, which has finite energy
and is absolutely integrable and has a Laplace transform that is valid on the j axis (i.e., the region
of convergence X(s) includes this axis), then its Fourier transform is simply the Laplace transform X(s)
computed at s = j, as we will see. There is no need for the integral formula in this case, although if you
apply it your result coincides with the one from the Laplace transform.
I
Finally, consider finding the Fourier transform of a sinc function (which is the impulse response of a
low-pass filter as we see later). Neither the integral nor the Laplace transform can be used to find it. For
this signal, we need to exploit the duality that exists between the direct and the inverse Fourier transforms
(Notice the duality in Equations (5.1) and (5.2)).

302
CHAPTER 5:
Frequency Analysis: The Fourier Transform
5.3 EXISTENCE OF THE FOURIER TRANSFORM
For the Fourier transform to exist, x(t) must be absolutely integrable—that is,
|X()| ≤
∞
Z
−∞
|x(t)e−jt|dt =
∞
Z
−∞
|x(t)|dt < ∞
Moreover, x(t) must have only a finite number of discontinuities and a finite number of minima and
maxima in any finite interval. (Given the limiting connection between the Fourier transform and the
Fourier series, it is not surprising that the above conditions coincide with the existence conditions for
the Fourier series.)
The Fourier transform
X() =
∞
Z
−∞
x(t)e−jtdt
of a signal x(t) exists (i.e., we can calculate its Fourier transform via this integral) provided
I
x(t) is absolutely integrable or the area under |x(t)| is finite.
I
x(t) has only a finite number of discontinuites as well as maxima and minima.
From the deﬁnitions of the direct and the inverse Fourier transforms—both being inﬁnite integrals—
one wonders whether they exist in general, and if so how to most efﬁciently compute them.
Commenting on the existence conditions, Professor E. Craig [17] wrote:
It appears that almost nothing has a Fourier transform—nothing except practical communi-
cation signals. No signal amplitude goes to inﬁnity and no signal lasts forever; therefore, no
practical signal can have inﬁnite area under it, and hence all have Fourier transforms.
Indeed, signals of practical interest have Fourier transforms and their spectra can be displayed using
a spectrum analyzer (or better yet, any signal for which we can display its spectrum will have a
Fourier transform). A spectrum analyzer is a device that displays the energy or the power of a signal
distributed over frequencies.
5.4 FOURIER TRANSFORMS FROM THE LAPLACE TRANSFORM
The region of convergence of the Laplace transform X(s) indicates the region in the s-plane where X(s)
is defined. The following applies to signals whether they are causal, anti-causal, or noncausal.
If the region of convergence (ROC) of X(s) = L[x(t)] contains the j axis, so that X(s) can be defined for s = j,
then
F[x(t)] = L[x(t)]|s=j =
∞
Z
−∞
x(t)e−jtdt
= X(s)
s=j
(5.3)

5.4 Fourier Transforms from the Laplace Transform
303
The following rules of thumb will help you get a better understanding of the time-frequency relation-
ship of a signal and its Fourier transform, and the best way to compute it. On a first reading the use
of these rules might not be obvious, but they will be helpful in understanding the discussions that
follow and you might want to come back to these rules.
Rules of Thumb for Computing the Fourier Transform of a Signal x(t)
I
If x(t) has a finite time support and in that support x(t) is finite, its Fourier transform exists. To find it use the integral
definition or the Laplace transform of x(t).
I
If x(t) has a Laplace transform X(s) with a region of convergence including the j axis, its Fourier transform is X(s)|s=j.
I
If x(t) is periodic of infinite energy but finite power, its Fourier transform is obtained from its Fourier series using delta
functions.
I
If x(t) is none of the above, if it has discontinuities (e.g., x(t) = u(t)) or it has discontinuities and it is not finite energy
(e.g., x(t) = cos(0t)u(t)), or it has possible discontinuities in the frequency domain even though it has finite energy
(e.g., x(t) = sinc(t)), use properties of the Fourier transform.
Keep in mind to
I
Consider the Laplace transform if the interest is in transients and steady state, and the Fourier
transform if steady-state behavior is of interest.
I
Represent periodic signals by their Fourier series before considering their Fourier transforms.
I
Attempt other methods before performing integration to find the Fourier transform.
I Example 5.1
Discuss whether it is possible to obtain the Fourier transform of the following signals using their
Laplace transforms:
(a) x1(t) = u(t)
(b) x2(t) = e−2tu(t)
(c) x3(t) = e−|t|
Solution
(a)
The Laplace transform of x1(t) is X1(s) = 1/s with a region of convergence corresponding
to the open right s-plane, or ROC = {s = σ + j : σ > 0, −∞<  < ∞}, which does not
include the j axis, so the Laplace transform cannot be used to find the Fourier transform
of x1(t).
(b)
The signal x2(t) has as Laplace transform X2(s) = 1/(s + 2) with a region of convergence ROC
= {s = σ + j : σ > −2, −∞<  < ∞} containing the j axis. Then the Fourier transform of
x2(t) is
X2() =
1
s + 2
s=j =
1
j + 2

304
CHAPTER 5:
Frequency Analysis: The Fourier Transform
(c)
The Laplace transform of x3(t) is
X3(s) =
1
s + 1 +
1
−s + 1 =
2
1 −s2
with a region of convergence ROC = {s = σ + j : −1 < σ < 1, −∞<  < ∞} that contains
the j axis. Then the Fourier transform of x3(t) is
X3() = X3(s)|s=j =
2
1 −( j)2 =
2
1 + 2
I
5.5 LINEARITY, INVERSE PROPORTIONALITY, AND DUALITY
Many of the properties of the Fourier transform are very similar to those of the Fourier series or of
the Laplace transform, which is to be expected given the strong connection among these transforma-
tions. The linearity and the duality between time and frequency of the Fourier transform will help us
determine the transform of signals that do not satisfy the existence conditions given before.
5.5.1 Linearity
Just like the Laplace transform, the Fourier transform is linear.
If F[x(t)] = X() and F[y(t)] = Y(), for constants α and β, we have that
F[αx(t) + βy(t)] = αF[x(t)] + βF[y(t)]
= αX() + βY()
(5.4)
I Example 5.2
Suppose you create a periodic sine
x(t) = sin(0t)
−∞< t < ∞
by adding a causal sine v(t) = sin(0t)u(t) and an anti-causal sine y(t) = sin(0t)u(−t), for each
of which you can find Laplace transforms V(s) and Y(s). Discuss what would be wrong with this
approach to find the Fourier transform of x(t) by letting s = j.
Solution
The Laplace transforms of v(t) and y(t) are
V(s) =
0
s2 + 2
0
ROC: Re [s] > 0
Y(s) =
−0
(−s)2 + 2
0
ROC: Re [s] < 0
giving X(s) = V(s) + Y(s) = 0. Moreover, the region of convergence of X(s) is the intersection of
the two given ROCs, which is null, so it is not possible to obtain the Fourier transform of x(t) this

5.5 Linearity, Inverse Proportionality, and Duality
305
way. This is so even though the time signals add correctly to x(t). The Fourier transform of the sine
signal will be found using the periodicity of x(t) or the duality property.
I
5.5.2 Inverse Proportionality of Time and Frequency
It is very important to realize that frequency is inversely proportional to time, and that as such,
time and frequency signal characterizations are complementary. Consider the following examples to
illustrate this.
I
The impulse signal x1(t) = δ(t), although not a regular signal, has finite support (its support is
only at t = 0 as the signal is zero everywhere else). It is also absolutely integrable, so it has a
Fourier transform
X1() = F[δ(t)] =
∞
Z
−∞
δ(t)e−jtdt = e−j0
∞
Z
−∞
δ(t)dt = 1
−∞<  < ∞
displaying inﬁnite support. (The Fourier transform could have also been obtained from the
Laplace transform L[δ(t)] = 1 for all values of s. For s = j, we have that F[δ(t)] = 1.) This result
means that since δ(t) changes so fast in such a short time, its Fourier transform has all possible
frequency components.
I
Consider then the opposite case: A signal that is constant for all times, that does not change, or
a dc signal x2(t) = A, −∞< t < ∞. We know that the frequency of  = 0 is assigned to it since
the signal does not vary at all. The Fourier transform cannot be found by means of the integral
because x2(t) is not absolutely integrable, but we can verify that it is given by X2() = 2πAδ()
(we will formally show this using the duality property). In fact, the inverse Fourier transform is
1
2π
∞
Z
−∞
X2()e jtd = 1
2π
∞
Z
−∞
2πAδ()e jtd = A
Notice the complementary nature of x1(t) and x2(t): x1(t) = δ(t) has a one-point support, while
x2(t) = A has infinite support. Their corresponding Fourier transforms X1() = 1 and X2() =
2πAδ() have infinite and one-point support in the frequency domain, respectively.
I
To appreciate the transition from the dc signal to the impulse signal, consider a pulse signal
x3(t) = A[u(t + τ/2) −u(t −τ/2)]. This signal has finite energy, and its Fourier transform can be
found using its Laplace transform. We have
X3(s) = A
s

e sτ/2 −e−sτ/2
with the whole s-plane as its region of convergence, so that
X3() = X(s)|s=j
= A(ejτ/2 −e−jτ/2)
j
= Aτ sin(τ/2)
τ/2
(5.5)

306
CHAPTER 5:
Frequency Analysis: The Fourier Transform
or a sinc function where Aτ
corresponds to the area under x3(t). The Fourier trans-
form X3() is an even function of . At  = 0 using L’Hˆopital’s rule we find that
X3(0) = Aτ. Finally, the Fourier transform of the pulse becomes zero when  = 2kπ/τ,
k = ±1, ±2, . . ..
If we let A = 1/τ (so that the area of the pulse is unity), and let τ →0, the pulse x3(t) becomes
a delta function δ(t) in the limit and the sinc function expands (for τ →0, X3() is not zero
for any finite value) to become unity. On the other hand, if we let τ →∞, the pulse becomes
a constant signal A extending from −∞to ∞, and the Fourier transform gets closer and closer
to δ() (the sinc function becomes zero at values very close to zero and the amplitude at  = 0
becomes larger and larger, although the area under the curve remains constant). As shown above,
X3() = 2πAδ() is the transform of x3(t) = A, −∞< t < ∞.
I
To illustrate the transition in the Fourier transform as the time support increases, we used the
following MATLAB script to compute the Fourier transform of pulses of the same amplitude A = 1
but different time supports 1 and 4. The script below shows the case when the support is 1, but
it can be easily changed to get the support of 4. The symbolic MATLAB function fourier computes
the Fourier transform. The results are shown in Figure 5.1.
%%%%%%%%%%%%%%%%%%%%%
% Time-frequency relation
%%%%%%%%%%%%%%%%%%%%%
syms t w
x = heaviside(t + 0.5) −heaviside(t −0.5);
subplot(211)
ezplot(x, [−3]);axis([−3 3 −0.1 1.1]);grid
X = fourier(x) % Fourier transform
subplot(212)
ezplot(X, [−50 50]); axis([−50 50 −1 5]);grid
In summary, the support of X() is inversely proportional to the support of x(t). If x(t) has a Fourier transform
X() and α ̸= 0 is a real number, then x(αt) is an
I
Contracted (α > 1),
I
Contracted and reﬂected (α < −1),
I
Expanded (0 < α < 1),
I
Expanded and reﬂected (−1 < α < 0), or
I
Simply reﬂected (α = −1)
signal, and we have the pair
x(αt)
⇔
1
|α|X

α

(5.6)
First let us mention that the symbol ⇔means that to a signal x(t) in the time domain (on the left)
there corresponds a Fourier transform X() in the frequency domain (on the right). This is not an
equality—far from it!

5.5 Linearity, Inverse Proportionality, and Duality
307
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
t
−3
−2
−1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
t
−50
0
50
−1
0
1
2
3
4
5
Ω
X2(Ω)
x1(t)
x2(t )
−50
−40
−30
−20
−10
0
10
20
30
40
50
−1
0
1
2
3
4
5
Ω
X1(Ω)
(a)
(b)
FIGURE 5.1
Fourier transform of pulses (a) x1(t) with A = 1 and τ = 1, and (b) x2(t) with A = 1 and τ = 4. Notice the wider
the pulse the more concentrated in frequency its Fourier transform, and that Xi(0) = Aτ, i = 1, 2, is the area
under the pulses.

308
CHAPTER 5:
Frequency Analysis: The Fourier Transform
This property is shown by a change of variable in the integration,
F[x(αt)] =
∞
Z
−∞
x(αt)e−jtdt =



1
α
∞
R
−∞
x(ρ)e−jρ/αdρ
α > 0
−1
α
∞
R
−∞
x(ρ)ejρ/αdρ
α < 0
= 1
|α|X

α

by change of variable ρ = αt. If |α| > 1, when compared with x(t) the signal x(αt) contracts while its
corresponding Fourier transform expands. Likewise, when 0 < |α| < 1, the signal x(αt) expands, as
compared with x(t), and its Fourier transform contracts. If α < 0, the corresponding contraction or
expansion is accompanied by a reﬂection in time. In particular, if α = −1, the reﬂected signal x(−t)
has X(−) as its Fourier transform.
I Example 5.3
Consider a pulse x(t) = u(t) −u(t −1). Find the Fourier transform of x1(t) = x(2t).
Solution
The Laplace transform of x(t) is
X(s) = 1 −e−s
s
with the whole s-plane as its region of convergence. Thus, its Fourier transform is
X() = 1 −e−j
j
= e−j/2(ej/2 −e−j/2)
2j/2
= sin(/2)
/2
e−j/2
To the finite-support signal x(t) corresponds X() of infinite support. Then,
x1(t) = x(2t) = u(2t) −u(2t −1) = u(t) −u(t −0.5)
and its Fourier transform is found, again using its Laplace transform, to be
X1() = 1 −e−j/2
j
= e−j/4(ej/4 −e−j/4)
j
= 1
2
sin(/4)
/4
e−j/4 = 1
2X(/2)
which is an expanded version of X() in the frequency domain and coincides with the result from
the property. See Figure 5.2.

5.5 Linearity, Inverse Proportionality, and Duality
309
FIGURE 5.2
(a) Pulse x(t) and its compressed
version x1(t) = x(2t), and (b) the
magnitude of their Fourier
transforms. Notice that when the
signal contracts in time it expands
in frequency.
0
0.2
0.4
0.6
0.8
1
0
0.5
1
x(t ), x1(t)
x(t )
x1(t)
(a)
t
−50
0
50
0
0.5
1
|X(Ω)|, |X1(Ω)|
|X(Ω)|
|X1(Ω)|
(b)
Ω
The Fourier transforms can be found from the integral definitions. Thus, for x(t),
X() =
1
Z
0
1ejtdt = e−jt
−j |1
0 = sin(/2)
/2
e−j/2
Likewise, for x1(t),
X1() =
0.5
Z
0
1ejtdt = 0.5sin(/4)
/4
e−j/4
I
I Example 5.4
Apply the reﬂection property to find the Fourier transform of x(t) = e−a|t|, a > 0. For a = 1, plot
using MATLAB the signal and its magnitude and phase spectra.
Solution
The signal x(t) can be expressed as x(t) = e−atu(t) + eatu(−t) = x1(t) + x1(−t). The Fourier trans-
form of x1(t) is
X1() =
1
s + a
s=j =
1
j + a

310
CHAPTER 5:
Frequency Analysis: The Fourier Transform
FIGURE 5.3
Magnitude and phase
spectrum of two-sided
signal x(t) = e−|t|. The
magnitude spectrum
indicates x(t) is low
pass. Notice the phase
is zero.
−10
−5
0
5
10
0
0.2
0.4
0.6
0.8
1
t (sec)
x(t )
−10
−5
0
5
10
0
0.5
1
1.5
2
Ω
|X(Ω)|
−20
0
20
−1
0
1
Ω
<X(Ω)
and according to the given result x1(−t) (α = −1), we have that
F[x1(−t)] =
1
−j + a
so that
X() =
1
j + a +
1
−j + a =
2a
a2 + 2
If a = 1, using MATLAB the signal x(t) = e−|t| and its magnitude and phase spectra are com-
puted and plotted as shown in Figure 5.3. Since X() is real and positive, the corresponding
phase spectrum is zero. This signal is called low-pass since its energy is concentrated in the low
frequencies.
I
5.5.3 Duality
Besides the inverse relationship of frequency and time, by interchanging the frequency and the time
variables in the definitions of the direct and the inverse Fourier transform (see Eqs. 5.1 and 5.2)
similar equations are obtained. Thus, the direct and the inverse Fourier transforms are dual.
To the Fourier transform pair
x(t)
⇔
X()
(5.7)
corresponds the following dual–Fourier transform pair
X(t)
⇔
2πx(−)
(5.8)

5.5 Linearity, Inverse Proportionality, and Duality
311
This can be shown by considering the inverse Fourier transform
x(t) = 1
2π
∞
Z
−∞
X(ρ)ejρtdρ
and replacing t by − and multiplying by 2π to get
2πx(−) =
∞
Z
−∞
X(ρ)e−jρdρ
=
∞
Z
−∞
X(t)e−jtdt
= F[X(t)]
To understand the above equations you need to realize that ρ and t are dummy variables inside the
integral, and as such they are not reﬂected outside the integral.
Remarks
I
This duality property allows us to obtain the Fourier transform of signals for which we already have a
Fourier pair and that would be difficult to obtain directly. It is thus one more method to obtain the Fourier
transform, besides the Laplace transform and the integral definition of the Fourier transform.
I
When computing the Fourier transform of a constant signal, x(t) = A, we indicated that it would be
X() = 2πAδ(). Indeed, we have the dual pairs
Aδ(t)
⇔
A
A
⇔
2πAδ(−) = 2πAδ()
(5.9)
where in the second equation we use the fact that δ() is even.
I Example 5.5
Use the duality property to find the Fourier transform of the sinc signal
x(t) = A sin(0.5t)
0.5t
= A sinc(0.5t)
−∞< t < ∞
Solution
The Fourier transform of the sinc signal cannot be found using the Laplace transform or the integral
definition of the Fourier transform. The duality property provides a way to obtain it. We found
before, for τ = 0.5, the following pair of Fourier transforms:
A[u(t + 0.5) −u(t −0.5)]
⇔
A sin(0.5)
0.5
= A sinc(0.5)

312
CHAPTER 5:
Frequency Analysis: The Fourier Transform
−1
−0.5
0
0.5
1
0
2
4
6
8
10
t
−100
−50
0
50
100
0
5
10
Ω
−1
−0.5
0
0.5
1
0
20
40
60
Ω
x1(t)
−100
−50
0
50
100
0
5
10
t
x (t )= X1(t)
X1(Ω)
X(Ω)= 2 πx 1(−Ω)
FIGURE 5.4
Application of duality to find the Fourier transform of x(t) = 10 sinc(0.5t). Notice that
X() = 2πx1() ≈6.28x1() = 62.8[u( + 0.5) −u( −0.5)].
Then according to the duality property, the Fourier transform of x(t) is
X() = 2πA[u(− + 0.5) −u(− −0.5)] = 2πA[u( + 0.5) −u( −0.5)]
given that the function is even with respect to . So the Fourier transform of the sinc is a rectangular
pulse in frequency, in the same way that the Fourier transform of a pulse in time is a sinc function
in frequency. Figure 5.4 shows the dual pairs for A = 10.
I
I Example 5.6
Find the Fourier transform of x(t) = A cos(0t) using duality.
Solution
The Fourier transform of x(t) cannot be computed using the integral definition since this signal
is not absolutely integrable, or using the Laplace transform since x(t) does not have a Laplace
transform. As a periodic signal, x(t) has a Fourier series representation and we will use it later to
find its Fourier transform. For now, let us consider the Fourier pair
δ(t −ρ0) + δ(t + ρ0) ⇔e−jρ0 + ejρ0 = 2 cos(ρ0)

5.6 Spectral Representation
313
where we used the Laplace transform of δ(t −ρ0) + δ(t + ρ0), which is e−sρ0 + e sρ0 defined over
the whole s-plane. At s = j, we get 2 cos(ρ0). According to the duality property, we thus have
the following Fourier pair:
2 cos(ρ0t) ⇔2π[δ(− −ρ0) + δ(− + ρ0)] = 2π[δ( + ρ0) + δ( −ρ0)]
Replacing ρ0 by 0 and canceling the 2 in both sides, and multiplying by A both sides, we have
x(t) = A cos(0t)
⇔
X() = πA[δ( + 0) + δ( −0)]
(5.10)
indicating that it only exists at ±0.
I
5.6 SPECTRAL REPRESENTATION
In this section, we consider first how to find the Fourier transform of periodic signals using the
modulation property, and then consider Parseval’s result for finite-energy signals. With these results,
we will unify the spectral representation of both periodic and aperiodic signals.
5.6.1 Signal Modulation
One of the most significant properties of the Fourier transform is modulation. Its application to
signal transmission is fundamental in communications.
I
Frequency shift: If X() is the Fourier transform of x(t), then we have the pair
x(t)ej0t
⇔
X( −0)
(5.11)
I
Modulation: The Fourier transform of the modulated signal
x(t) cos(0t)
(5.12)
is given by
0.5 [X( −0) + X( + 0)]
(5.13)
That is, X() is shifted to frequencies 0 and −0, and multiplied by 0.5.
The frequency shifting property is easily shown:
F[x(t)ej0t] =
∞
Z
−∞
[x(t)ej0t]e−jtdt
=
∞
Z
−∞
x(t)e−j(−0)tdt
= X( −0)

314
CHAPTER 5:
Frequency Analysis: The Fourier Transform
Applying the frequency shifting to
x(t) cos(0t) = 0.5x(t)ej0t + 0.5x(t)e−j0t
we obtain the Fourier transform of the modulated signal (Eq. 5.13). In communications, the message
x(t) (typically of lower frequency content than the frequency of the cosine) modulates the carrier
cos(0t) to obtain the modulated signal x(t) cos(0t). Modulation is an important application of the
Fourier transform, as it allows us to change the original frequencies of a message to much higher
frequencies, making it possible to transmit the signal over the airwaves.
Remarks
I
As indicated before, amplitude modulation consists in multiplying an incoming signal x(t), or
message, by a sinusoid of frequency higher than the maximum frequency of the incoming signal. The
modulated signal is
x(t) cos(0t) = 0.5[x(t)ej0t + x(t)e−j0t]
with a Fourier transform, according to the frequency shifting property, of
F[x(t) cos(0t)] = 0.5[X( −0) + X( + 0)]
Thus, modulation shifts the frequencies of x(t) to frequencies around ±0.
I
Modulation using a sine, instead of a cosine, changes the phase of the Fourier transform of the incoming
signal besides performing the frequency shift. Indeed,
F[x(t) sin(0t)] = F
"
x(t)ej0t −x(t)e−j0t
2j
#
= 1
2jX( −0) −1
2jX( + 0)
= −j
2 X( −0) + j
2X( + 0)
where the −j and j terms add −π/2 and π/2, respectively, radians to the signal phase.
I
According to the eigenfunction property of LTI systems, modulation systems are not LTI. Modulation shifts
the frequencies at the input to new frequencies at the output. Nonlinear or time-varying systems are
typically used as amplitude modulation transmitters.
I Example 5.7
Consider modulating a carrier cos(10t) with the following signals:
1.
x1(t) = e−|t|, −∞< t < ∞. Use MATLAB to find the Fourier transform of y1(t) = x1(t) cos(10t)
and plot y1(t) and its magnitude and phase spectra.
2.
x2(t) = 0.2[r(t + 5) −2r(t) + r(t + 5)], where r(t) is the ramp signal. Use MATLAB to plot x2(t)
and y2(t) = x2(t) cos(10t) and compute and plot the magnitude of their Fourier transforms.

5.6 Spectral Representation
315
Solution
The modulated signals are
• y1(t) = x1(t) cos(10t) = e−|t| cos(10t)
−∞< t < ∞
• y2(t) = x2(t) cos(10t) = 0.2[r(t + 5) −2r(t) + r(t + 5)] cos(10t)
The signal x1(t) is very smooth, although of inﬁnite support, and thus most of its frequency com-
ponents are of low frequency. The signal x2(t) is not as smooth and has a finite support, so that its
frequency components are mostly low pass but its spectrum also displays higher frequencies.
The MATLAB scripts used to compute the Fourier transform of the modulated signals and to plot
the signals, their modulated versions, and the magnitude and phase of the Fourier transforms are
very similar. The following script indicates how to generate y1(t) and how to find the magnitude
and phase of its Fourier transform Y1(). Notice the way the phase is computed.
%%%%%%%%%%%%%%%%%%%%%
% Example 5.7---Modulation
%%%%%%%%%%%%%%%%%%%%%
y1 = exp(−abs(t)). ∗cos(10 ∗t);
% magnitude and phase of Y1(Omega)
Y1 = fourier(y1); Ym = abs(Y1); Ya = atan(imag(Y1)/real(Y1));
The signal x2(t) is a triangular signal. The following script shows how to generate the signal x2(t).
Instead of multiplying x2(t) by the cosine, we multiply it by the cosine-equivalent representa-
tion in complex exponentials, which will give better plots of the Fourier transforms when using
ezplot.
m = heaviside(t + 5) −heaviside(t);
m1 = heaviside(t) −heaviside(t−5);
x2 = (t + 5) ∗m + m1 ∗(−t + 5); x2 = x2/5;
x = x2 ∗exp(−j ∗10 ∗t)/2; y = x2 ∗exp(+j ∗10 ∗t)/2;
X = fourier(x); Y = fourier(y);
Y2m = abs(X) + abs(Y); % magnitude of Y 2(Omega)
X2 = fourier(x2); X2m = abs(X2); % magnitude of X 2(Omega)
The results are shown in Figure 5.5.
I
Why Modulation?
The use of modulation to change the frequency content of a message from its baseband frequencies
to higher frequencies makes its transmission over the airwaves possible. Let us explore why it is nec-
essary to use modulation to transmit a music or a speech signal. Typically, acoustic signals such as
music are audible up to frequencies of about 22 KHz, while speech signals typically display frequen-
cies from about 100 Hz to about 5 KHz. Thus, music and speech signals are relatively low-frequency

316
CHAPTER 5:
Frequency Analysis: The Fourier Transform
(a)
(b)
−5
0
5
−0.5
0
0.5
1
t (sec)
y1(t )
−20
0
20
0
0.2
0.4
0.6
0.8
1
Ω
−20
0
20
−1
0
1
Ω
|Y1(Ω)|
<Y1(Ω)
−5
0
5
0
0.5
1
t
−5
0
5
−1
−0.5
0
0.5
1
t
y2(t ) =x2(t ) cos(10t )
−5
0
5
0
1
2
3
4
5
Ω
−20
−10
0
10
20
0
1
2
3
4
5
Ω
|Y2(Ω)|
|X2(Ω)|
x2(t )
FIGURE 5.5
(a) The modulated signal y1(t) = e−|t| cos(10t) and its magnitude and phase spectra. Notice that the phase is
zero. (b) The triangular signal, its modulated version, and their magnitude spectra.

5.6 Spectral Representation
317
signals. When radiating a signal with an antenna, the length of the antenna is about a quarter of the
wavelength,
λ = 3 × 108
f
meters
where f is the frequency in hertz of the signal being radiated. Thus, if we assume that frequencies
up to f = 30 KHz are present in the signal (this would allow us to include music and speech in the
signal) the wavelength is 10 kilometers and the size of the antenna is 2.5 kilometers—a 1.5-mile
long antenna! Thus, for a music or a speech signal to be transmitted with a reasonable-size antenna
requires increasing the frequencies present in the signal. Modulation provides an efficient way to shift
an acoustic or speech signal to a desirable frequency.
5.6.2 Fourier Transform of Periodic Signals
By applying the frequency-shifting property to compute the Fourier transform of periodic signals, we
are able to unify the Fourier representation of aperiodic as well as periodic signals.
For a periodic signal x(t) of period T0, we have the Fourier pair
x(t) =
X
k
Xkejk0t
⇔
X() =
X
k
2πXkδ( −k0)
(5.14)
obtained by representing x(t) by its Fourier series.
Since a periodic signal x(t) is not absolutely integrable, its Fourier transform cannot be computed
using the integral formula. But we can use its Fourier series
x(t) =
X
k
Xkejk0t
where the {Xk} are the Fourier coefﬁcients, and 0 = 2π/T0 is the fundamental frequency of the peri-
odic signal x(t) of period T0. As such, according to the linearity and the frequency-shifting properties
of the Fourier transform, we obtain
X() =
X
k
F[Xkejk0t]
=
X
k
2πXkδ( −k0)
where we used that Xk as a constant has a Fourier transform 2πXkδ(). Notice that for a periodic sig-
nal the Fourier coefﬁcients {Xk} still characterize its frequency representation: The Fourier transform
of a periodic signal is a sequence of impulses in frequency at the harmonic frequencies, {δ( −k0)},
with amplitudes {2πXk}.

318
CHAPTER 5:
Frequency Analysis: The Fourier Transform
Remarks
I
When plotting |X()| versus , which we call the Fourier magnitude spectrum, for a periodic signal
x(t), we notice it is analogous to its line spectrum discussed before. Both indicate that the signal power is
concentrated in multiples of the fundamental frequency, the only difference being in how the information
is provided at each of the frequencies. The line spectrum displays the Fourier series coefficients at their
corresponding frequencies, while the spectrum from the Fourier transform displays the concentration of the
power at the harmonic frequencies by means of delta functions with amplitudes of 2π times the Fourier
series coefficients. Thus, there is a clear relation between these two spectra, showing exactly the same
information in slightly different form.
I
The Fourier transform of a cosine signal can now be computed directly as
F[cos(0t)] = F[0.5ej0t + 0.5e−j0t]
= πδ( −0) + πδ( + 0)
and for a sine (compare this result with the one obtained before),
F[sin(0t)] = F
0.5
j ej0t −0.5
j e−j0t

= π
j δ( −0) −π
j δ( + 0)
= πe−jπ/2δ( −0) + πejπ/2δ( + 0)
The magnitude spectra of the two signals coincide, but the cosine has a zero-phase spectrum, while the
phase spectrum for the sine displays a phase of ±π/2 at frequencies ±0.
I Example 5.8
Consider a periodic signal x(t) with a period
x1(t) = r(t) −2r(t −0.5) + r(t −1)
If the fundamental frequency is 0 = 2π, determine the Fourier transform X() analytically and
using MATLAB. Plot several periods of the signal and its Fourier transform.
Solution
The given period x1(t) corresponds to a triangular signal. Its Laplace transform is
X1(s) = 1
s2

1 −2e−0.5s + e−s
= e−0.5s
s2

e0.5s −2 + e−0.5s

5.6 Spectral Representation
319
so that the Fourier coefficients of x(t) are (T0 = 1):
Xk = 1
T0
X1(s)|s=j2πk =
1
( j2πk)2 2(cos(πk) −1)e−jπk
= (−1)(k+1) cos(πk) −1
2π2k2
= (−1)k sin2(πk/2)
π2k2
after using the identity cos(2θ) −1 = −2 sin2(θ). The DC term is X0 = 0.5. The Fourier transform
of x(t) is then
X() = 2πX0δ() +
∞
X
k=−∞,̸=0
2πXkδ( −2kπ)
To compute the Fourier transform using symbolic MATLAB, we approximate x(t) by its Fourier
series by means of its average and N = 10 harmonics (the Fourier coefﬁcients are computed using
the fourierseries function from Chapter 4). We then create a sequence {2πXk} and the correspond-
ing harmonic frequencies {k = k0} and plot them as the spectrum X() (see Figure 5.6). The
following script gives some of the necessary steps to generate the periodic signal and to find its
Fourier transform. The MATLAB function ﬂiplr is used to reﬂect the Fourier coefﬁcients.
FIGURE 5.6
(a) Triangular periodic signal x(t),
and (b) its Fourier transform X(),
which is zero except at harmonic
frequencies where it is an impulse
of magnitude 2πXk where Xk is a
Fourier coefficient of x(t).
0
1
2
3
4
5
0
0.5
1
t
x(t )
(a)
−50
0
50
−1
0
1
2
3
4
Ω(rad/sec)
X(Ω)
(b)

320
CHAPTER 5:
Frequency Analysis: The Fourier Transform
%%%%%%%%%%%%%%%%%%%%%
% Example 5.8---Fourier series
%%%%%%%%%%%%%%%%%%%%%
T0 = 1; N = 10; w0 = 2 ∗pi/T0;
m = heaviside(t) −heaviside(t −T0/2);
m1 = heaviside(t −T0/2) −heaviside(t −T0);
x = t ∗m + m1 ∗(−t + T0); x = 2 ∗x; % periodic signal
[Xk, w] = fourierseries(x, T0, N); % Fourier coefﬁcients, harmonic frequencies
% Fourier series approximation
for k = 1:N,
if k == 1;
x1 = abs(Xk(k));
else
x1 = x1 + 2 ∗abs(Xk(k)) ∗cos(w0 ∗(k−1) ∗t + angle(Xk(k)));
end
end
% sequence of Fourier coefﬁcients and harmonic frequencies
k = 0:N−1; Xk1 = 2 ∗pi ∗Xk; wk = [−ﬂiplr(k(2:N−1)) k] ∗w0; Xk = [ﬂiplr(Xk1(2:N −1)) Xk1];
In this case, the Laplace transform simplifies the computation of the Xk values. Indeed, the Fourier
series coefficients are given by
Xk =
0.5
Z
0
te−j2πktdt +
1
Z
0.5
(1 −t)e−j2πktdt
which need to be found using integration by parts.
I
5.6.3 Parseval’s Energy Conservation
We saw in Chapter 4 that for periodic signals having finite power but inﬁnite energy, Parseval’s theo-
rem indicates how the power of the signal is distributed among the harmonic components. Likewise,
for aperiodic signals of finite energy, an energy version of Parseval’s result indicates how the signal
energy is distributed over frequencies.
For a finite-energy signal x(t) with Fourier transform X(), its energy is conserved when going from the time
to the frequency domain, or
Ex =
∞
Z
−∞
|x(t)|2dt = 1
2π
∞
Z
−∞
|X()|2d
(5.15)
Thus, |X()|2 is an energy density indicating the amount of energy at each of the frequencies .
The plot |X()|2 versus  is called the energy spectrum of x(t), and it displays how the energy of the signal is
distributed over frequency.

5.6 Spectral Representation
321
This energy conservation property is shown using the inverse Fourier transform. The finite-energy
signal of x(t) can be computed in the frequency domain by
∞
Z
−∞
x(t)x∗(t)dt =
∞
Z
−∞
x(t)

1
2π
∞
Z
−∞
X∗()e−jtd

dt
= 1
2π
∞
Z
−∞
X∗()


∞
Z
−∞
x(t)e−jtdt

d
= 1
2π
∞
Z
−∞
|X()|2d
I Example 5.9
Parseval’s result helps us to understand better the nature of an impulse δ(t). It is clear from its
definition that the area under an impulse is unity, which means δ(t) is absolutely integrable, but
does it have finite energy? Show how Parseval’s result can help resolve this issue.
Solution
Let’s consider this from the frequency point of view, using Parseval’s result. The Fourier transform
of δ(t) is unity for all values of frequency and as such its energy is infinite. Such a result seems
puzzling, because δ(t) was defined as the limit of a pulse of finite duration and unity area. This is
what happens if
p1(t) = 1
1[u(t + 1/2) −u(t −1/2)]
is a pulse of the unity area from which we obtain the impulse by letting 1 →0. The signal
p2
1(t) = 1
12 [u(t + 1/2) −u(t −1/2)]
is a pulse of area 1/1. If we then let 1 →0, the squared pulse p2
1(t) will tend to infinity with an
infinite area under it. Thus, δ(t) is not finite energy.
I
I Example 5.10
Consider a pulse p(t) = u(t + 1) −u(t −1). Use its Fourier transform P() and Parseval’s result to
show that
∞
Z
−∞
sin()

2
d = π

322
CHAPTER 5:
Frequency Analysis: The Fourier Transform
Solution
The energy of the pulse is E = 2 (the area under the pulse). But according to Parseval’s result the
energy computed in the frequency domain is given by
1
2π
∞
Z
−∞
2 sin()

2
d = Ex
since 2 sin()/ = P() = F(p(t)). Replacing Ex, we obtain the interesting and not obvious result
∞
Z
−∞
sin()

2
d = π
This is one more way to compute π!
I
5.6.4 Symmetry of Spectral Representations
Now that the Fourier representation of aperiodic and periodic signals is unified, we can think of
just one spectrum that accommodates both finite-energy as well as infinite-energy signals. The word
spectrum is loosely used to mean different aspects of the frequency representation. In the following
we provide definitions and the symmetry characteristic of the spectrum of real-valued signals.
If X() is the Fourier transform of a real-valued signal x(t), periodic or aperiodic, the magnitude |X()| is an
even function of :
|X()| = |X(−)|
(5.16)
and the phase ∠X() is an odd function of :
∠X() = −∠X(−)
(5.17)
We then have:
Magnitude spectrum:
|X()| versus 
Phase spectrum:
∠X() versus 
Energy/power spectrum:
|X()|2 versus 
To show this, consider the inverse Fourier transform of a real-valued signal x(t),
x(t) =
∞
Z
−∞
X()ejtd
which, because of being real, is identical to
x∗(t) =
∞
Z
−∞
X∗()e−jtd =
∞
Z
−∞
X∗(−)ejtd

5.6 Spectral Representation
323
since the integral can be thought of as an inﬁnite sum of complex values. Comparing the two
integrals, we have that
X() = X∗(−)
or
|X()|ejθ() = |X(−)|e−jθ(−)
where θ() = ∠(X()) is the phase of X(). We can then see that
|X()| = |X(−)|
θ() = −θ(−)
or that the magnitude is an even function of  and the phase is an odd function of . It can also be
seen that
Re[X()] = Re[X(−)]
Im[X()] = −Im[X(−)]
or that the real part of the Fourier transform is an even function and that the imaginary part of the
Fourier transform is an odd function of .
Remarks
I
Clearly, if the signal is complex, the above symmetry will not hold. For instance, if x(t) = ej0t =
cos(0t) + j sin(0t), using the frequency-shift property its Fourier transform is
X() = 2πδ( −o)
which occurs at  = 0 only, so the symmetry in the magnitude and phase does not exist.
I
It is important to recognize the meaning of “negative” frequencies. In reality, only positive frequencies
exist and can be measured, but as shown the spectrum, magnitude or phase, of a real-valued signal
requires negative frequencies. It is only under this context that negative frequencies should be understood
as necessary to generate “real-valued” signals.
I Example 5.11
Use MATLAB to compute the Fourier transform of the following signals:
(a) x1(t) = u(t) −u(t −1)
(b) x2(t) = e−tu(t)
Plot their magnitude and phase spectra.
Solution
Three possible ways to compute the Fourier transforms of these signals using MATLAB are: (1)
find their Laplace transforms as in Chapter 3 using laplace and compute the magnitude and phase

324
CHAPTER 5:
Frequency Analysis: The Fourier Transform
function by letting s = j, (2) by using the symbolic function fourier, and (3) sample x(t) and find
its Fourier transform (this requires sampling theory—see Chapter 7).
The following script is used to compute and plot the signal x2(t) = e−tu(t) and the magnitude and
phase of its Fourier transform using symbolic MATLAB. A similar script is used for x1(t).
%%%%%%%%%%%%%%%%%%%%%
% Example 5.11
%%%%%%%%%%%%%%%%%%%%%
symm t
x2 = exp(−t) ∗heaviside(t);
X2 = fourier(x2)
X2m = sqrt((real(X2))ˆ2 + (imag(X2))ˆ2; % magnitude
X2p = imag(log(X2); % phase
Notice the way that the magnitude and the phase are computed. To compute the magnitude we
use
|X2()| =
q
Re[X2()]2 + Im[X2()]2.
The computation of the phase is complicated by the lack of the function atan2 in symbolic MAT-
LAB, which extends the principal values of the inverse tangent to (−π, π] by considering the sign
of the real part. The phase computation can be done by using the log function; indeed
log(X2()) = log
 |X2()|ej∠X2()
= log(|X2()|) + j∠X2()
so that
∠X2() = Im[log(X2())]
Analytically, the phase of the Fourier transform of x1(t) = u(t) −u(t −1) can be found by
considering the advanced signal z(t) = x1(t + 0.5) = u(t + 0.5) −u(t −0.5) with Fourier transform
Z() = sin(/2)
/2
Given that Z() is real, its phase is either zero when Z() ≥0 and ±π when Z() < 0 (using these
values so that the phase is an odd function of ). Since z(t) = x1(t + 0.5), then Z() = X1()ej0.5,
so that
X1() = e−j0.5Z()
and as such
∠X1() = ∠Z() −0.5 =
−0.5
Z() ≥0
±π −0.5
Z() < 0

5.6 Spectral Representation
325
The Fourier transform of x2(t) = e−tu(t) is
X2() =
1
1 + j
The magnitude and phase are given by
|X2()| =
1
√
1 + 2
θ() = −tan−1 
When we compute these in terms of , we have

|X2()|
θ()
0
1
0
1
1
√
2
−π/4
∞
0
−π/2
That is, the magnitude spectrum decays as  increases. The signal x2(t) is called low-pass given that
the magnitude of its Fourier transform is concentrated in the low frequencies. This also implies
that the signal x2(t) is rather smooth. See Figure 5.7 for results.
I
I Example 5.12
It is not always the case that the Fourier transform is a complex-valued function. Consider the
signals
(a) x(t) = 0.5e−|t|
(b) y(t) = e−|t| cos(0t)
Find their Fourier transforms. Discuss the smoothness of these signals.
Solution
(a)
The Fourier transform of x(t) is
X() =
1
2 + 1
which is a real-valued function of . Indeed,

|X()| = X()
θ()
0
1
0
1
0.5
0
∞
0
0

326
CHAPTER 5:
Frequency Analysis: The Fourier Transform
0
2
4
−0.2
0
0.2
0.4
0.6
0.8
1
t (sec)
x1(t )
−20
−2
0
20
0
0.2
0.4
0.6
0.8
1
Ω
−20
0
20
Ω
|X1(Ω)|
∠X1(Ω)
−3
−2
−1
0
1
2
3
(a)
x2(t)
0
2
4
0
0.5
1
t(sec)
−20
0
20
Ω
−20
0
20
Ω
|X2(Ω)|
<X2(Ω)
0
0.5
1
−1
0
1
(b)
FIGURE 5.7
Fourier transforms of (a) pulse x1(t) and of (b) decaying exponential x2(t).

5.7 Convolution and Filtering
327
This is also a low-pass signal like x2(t) in Example 5.11, but this is “smoother” than that one
because the magnitude response is more concentrated in the low frequencies. Compare the
values of the magnitude responses to verify this. Also this signal has zero phase, because its
Fourier transform is real and positive for all values of .
(b)
The signal y(t) = 2x(t) cos(0t) is a band-pass signal. It is not as smooth as the signals in the
above example given that the concentration of
Y() = X( −0) + X( + 0)
is around the frequency 0, a frequency typically higher than the frequencies in x(t). The
higher this frequency, the more variation is displayed by the signal. In communications, low-
pass signals are called base-band signals.
I
The bandwidth of a signal x(t) is the support—on the positive frequencies—of its Fourier transform X().
There are different deﬁnitions of the bandwidth of a signal depending on how the support of its Fourier trans-
form is measured. We will discuss some of the bandwidth measures used in ﬁltering and in communications
in Chapter 6.
The bandwidth together with the information about the signal being low-pass or band-pass provides
a good characterization of the signal. The concept of the bandwidth of a filter that was discussed
in circuit theory is one of its possible definitions; other possible definitions will be introduced in
Chapter 6. The spectrum analyzer, a device used to measure the spectral characteristics of a signal,
will be presented in section 5.7.4 after considering ﬁltering.
5.7 CONVOLUTION AND FILTERING
The modulation and the convolution integral properties are the most important properties of the
Fourier transform. Modulation is essential in communications, and the convolution property is basic
in the analysis and design of filters.
If the input x(t) (periodic or aperiodic) to a stable LTI system has a Fourier transform X(), and the system has
a frequency response H(j) = F[h(t)] where h(t) is the impulse response of the system, the output of the LTI
system is the convolution integral y(t) = (x ∗h)(t), with Fourier transform
Y() = X() H( j)
(5.18)
In particular, if the input signal x(t) is periodic, the output is also periodic with Fourier transform
Y() =
∞
X
k=−∞
2π Xk H( jk 0)δ( −k0)
(5.19)
where Xk are the Fourier series coefficients of x(t) and 0 are its fundamental frequency.

328
CHAPTER 5:
Frequency Analysis: The Fourier Transform
This can be shown by considering the eigenfunction property of LTI systems. The Fourier
representation of x(t), if aperiodic, is an inﬁnite summation of complex exponentials ejt multiplied
by complex constants X(), or
x(t) = 1
2π
∞
Z
−∞
X()ejtd
According to the eigenfunction property, the response of an LTI system to each term X()ejt is
X()ejtH( j) where H(j) is the frequency response of the system, and thus by superposition the
response y(t) is
y(t) = 1
2π
∞
Z
−∞
[X()H( j)] ejtd
= 1
2π
∞
Z
−∞
Y()ejtd
so that Y() = X()H( j).
If x(t) is periodic of period T0 (or fundamental frequency 0 = 2π/T0), then
X() =
∞
X
k=−∞
2πXkδ( −k0)
so that the output y(t) has as its Fourier transform
Y() = X()H( j)
=
∞
X
k=−∞
2πXkH( j)δ( −k0)
=
∞
X
k=−∞
2πXkH( jk0)δ( −k0)
Therefore, the output is periodic—that is,
y(t) =
∞
X
k=−∞
Ykejk0t
where Yk = XkH( jk0).
An important consequence of the convolution property, just like in the Laplace transform, is that the
ratio of the Fourier transforms of the input and the output gives the frequency response of the system,

5.7 Convolution and Filtering
329
or
H( j) = Y()
X()
(5.20)
The magnitude and the phase of H( j) are the magnitude and phase frequency responses of the
system, or how the system responds to each particular frequency.
Remarks
I
It is important to keep in mind the following connection between the impulse response h(t), the transfer
function H(s), and the frequency response H( j) that characterize an LTI system:
H( j) = L[h(t)]|s=j
= H(s)|s=j
= Y(s)
X(s |s=j
I
As the Fourier transform of a real-valued function, the impulse response h(t), the function H( j) has a
magnitude |H( j)| and a phase ∠H( j), which are even and odd functions of the frequency .
I
The convolution property relates to the processing of an input signal by an LTI system. But it is possible, in
general, to consider the case of convolving two signals x(t) and y(t) to get z(t) = [x ∗y](t), in which case
we have that Z() = X()Y() where X() and Y() are the Fourier transforms of x(t) and y(t).
5.7.1 Basics of Filtering
The most important application of LTI systems is ﬁltering. Filtering consists in getting rid of
undesirable components of a signal. A typical example is when noise η(t) is added to a desired
signal x(t) (i.e., y(t) = x(t) + η(t)), and the spectral characteristics of x(t) and the noise η(t) are
known. The problem then is to design a ﬁlter, or an LTI system, that will get rid of the noise
as much as possible. The ﬁlter design consists in ﬁnding a transfer function H(s) = B(s)/A(s) that
satisﬁes certain speciﬁcations that will allow getting rid of the noise. Such speciﬁcations are typ-
ically given in the frequency domain. This is a rational approximation problem, as we look for
the coefﬁcients of the numerator and denominator of H(s) that make H( j) in magnitude and
phase approximate the ﬁlter speciﬁcations. The designed ﬁlter should be implementable and sta-
ble. In this section we discuss the basics of ﬁltering and in Chapter 6 we introduce the ﬁlter
design.
Frequency-discriminating filters keep the frequency components of a signal in a certain frequency
band and attenuate the rest. Filtering an aperiodic signal x(t) represented by its Fourier transform
X() with magnitude |X()| and phase ∠X(), using a filter with frequency response H( j), gives
an output y(t) with a Fourier transform of
Y() = H( j)X()
Thus, the output y(t) is composed of only those frequency components of the input that are not
filtered out by the filter. When designing the filter, we assign appropriate values to the magnitude in

330
CHAPTER 5:
Frequency Analysis: The Fourier Transform
the desirable frequency band or bands, and let it be close to zero in those frequencies we would not
want in the output signal.
If the input signal x(t) is periodic of period T0, or fundamental frequency 0 = 2π/T0, the Fourier
transform of the output is
Y() = X()H( j)
= 2π
X
k
XkH( jk0)δ( −k0)
(5.21)
where the magnitude and the phase of each of the Fourier series coefﬁcients are changed by the fre-
quency response of the ﬁlter at the harmonic frequencies. Indeed, Xk corresponding to the frequency
k0 is changed into
XkH( jk0) = |Xk||H( jk0)|ej(∠Xk+∠H( jk0))
The filter output y(t) is also periodic of period T0 but is missing the harmonics of the input that have
been filtered out.
The above shows that independent of whether the input signal x(t) is periodic or aperiodic, the output
signal y(t) has the frequency components allowed through by the filter.
I Example 5.13
Consider how to obtain a dc source using a full-wave rectifier and a low-pass filter (it keeps only
the low-frequency components). Let the full-wave rectified signal x(t) be the input of the filter
and let the output of the filter be y(t). We want y(t) = 1 volt. The rectifier and the low-pass filter
constitute a system that converts alternating into direct voltage.
Solution
We found in Chapter 4 that the Fourier series coefficients of x(t) = | cos(πt)|, −∞< t < ∞, are
given by
Xk =
2(−1)k
π(1 −4k2)
so that the average of x(t) is X0 = 2/π. To filter out all the harmonics and leave only the average
component, we need an ideal low-pass filter with a magnitude A and a cut-off frequency 0 <
c < 0 where 0 = 2π/T0 = 2π is the fundamental frequency of x(t). Thus, the filter is given by
H( j) = A for −0 < c < 0 and zero otherwise. According to the convolution property, then
Y() = H( j)X() = H( j)

2πX0δ() +
X
k̸=0
2πXkδ( −k0)


= 2πAX0δ()

5.7 Convolution and Filtering
331
so that AX0 = 1, or A = 1/X0 = π/2, to get the output to have a unit amplitude. Although the
proposed filter is not realizable, the above provides what needs to be done to obtain a dc source
from a full-wave rectified signal.
I
I Example 5.14
Windowing is a time-domain process by which we select part of a signal. This is done by multiplying
the signal by a “window” signal w(t). Consider the rectangular window
w(t) = u(t + 1) −u(t −1)
1 > 0
For a given signal x(t), the windowed signal is given by
y(t) = x(t)w(t)
Discuss how windowing relates to the convolution property.
Solution
Windowing is the dual of filtering. In this case, the signal y(t) has the support determined by the
window, or −1 ≤t ≤1, and as such it is zero outside this interval. The window gets rid of parts
of the signal outside its support. The signal y(t) can be written as
y(t) = w(t)x(t) = w(t) 1
2π
∞
Z
−∞
X(ρ)ejρtdρ
= 1
2π
∞
Z
−∞
X(ρ)w(t)ejρtdρ
Considering the integral an infinite summation, the Fourier transform of y(t) is
Y() = 1
2π
∞
Z
−∞
X(ρ)F[w(t)ejρt]dρ
= 1
2π
∞
Z
−∞
X(ρ)W( −ρ)dρ
using the frequency-shifting property. Thus, we have that the windowing, or multiplication in the
time domain, y(t) = w(t)x(t) gives Y() as the convolution of X() = F[x(t)] and
W() = F[w(t)] = 1
s

e1s −e−1s
s=j = 2 sin(1)


332
CHAPTER 5:
Frequency Analysis: The Fourier Transform
multiplied by a constant. This is one more example of the inverse relationship between time and
frequency. In this case, the support of the result of the windowing is finite, while the convolu-
tion in the frequency domain gives an inﬁnite support for Y() given that W() has an inﬁnite
support.
I
5.7.2 Ideal Filters
Frequency-discriminating ﬁlters that keep low-, middle-, and high-frequency components, or a
combination of these, are called low-pass, band-pass, high-pass, and multiband ﬁlters, respectively. A
band-eliminating or notch ﬁlter gets rid of middle-frequency components. It is also possible to have an
all-pass ﬁlter that although it does not ﬁlter out any of the input frequency components, it changes
the phase of the input signal.
The magnitude frequency response of an ideal low-pass filter is given by
|Hlp( j)| =
1
−1 ≤ ≤1
0
otherwise
and the phase frequency response of this filter is
∠Hlp( j) = −α
which as a function of  is a straight line with slope −α, thus its term linear phase. The frequency 1
is called the cut-off frequency of the low-pass filter. The above magnitude and phase responses only
need to be given for positive frequencies, given that the magnitude and the phase responses are the
even and odd function of . The rest of the frequency response is obtained by symmetry.
An ideal band-pass filter has a magnitude response
|Hbp( j)| =
1
1 ≤ ≤2
and −2 ≤ ≤−1
0
otherwise
with cut-off frequencies 1 and 2. The magnitude response of an ideal high-pass filter is given by
|Hhp( j)| =
1
 ≥2
and  ≤−2
0
otherwise
with a cut-off frequency of 2. For both of these ﬁlters, i is assumed the phase is linear in the pass-
band (band of frequencies where the magnitude is unity).
From these definitions, we have that the ideal band-stop filter has as magnitude response of
|Hbs( j)| = 1 −|Hbp( j)|
The sum of the magnitude responses of the given low-, band-, and high-pass ﬁlters gives the
magnitude response of an ideal all-pass ﬁlter
|Hap( j)| = |Hlp( j)| + |Hbp( j)| + |Hhp( j)| = 1

5.7 Convolution and Filtering
333
FIGURE 5.8
Ideal filters: (a) low pass, (b) band pass, (c) band
eliminating, and (d) high pass.
|Hlp( jΩ)|
Ω1
Ω
(a)
|Hbp( jΩ)|
Ω1
Ω2
Ω
(b)
|Hbe( jΩ)|
Ω1
Ω2
Ω
(c)
|Hhp( jΩ)|
Ω3
Ω
(d)
for all frequencies, since in this case we chose the frequencies 1 and 2 so that the response of these
filters add to unity. An ideal multi-band filter can be obtained as a combination of the low-, band-,
and high-pass filters. Figure 5.8 displays the frequency responses of the ideal ﬁlters discussed here.
Remarks
I
If hlp(t) is the impulse response of a low-pass ﬁlter, applying the modulation property we get that
2hlp(t) cos(0t) (where 0 >> 1 and 1 is the cut-off frequency of the low-pass ﬁlter) corresponds to
the impulse response of a band-pass ﬁlter centered around 0. Indeed, its Fourier transform is given by
F[2hlp(t) cos(0t)] = Hlp( j( −0)) + Hlp( j( + 0))
which is the frequency response of the low-pass filter shifted to new center frequencies 0 and −0,
making it a band-pass filter.
I
A zero-phase ideal low-pass filter Hlp( j) = u( + 1) −u( −1) has as impulse response a sinc
function with a support from −∞to ∞. This ideal low-pass filter is clearly noncausal as its impulse
response is not zero for negative values of time t. To make it causal we could approximate its impulse
response by a function h1(t) = hlp(t)w(t) where w(t) = u(t + τ) −u(t −τ) is a rectangular window
where the value of τ is chosen so that outside the window the values of the impulse response hlp(t) are very
close to zero. Although the Fourier transform of h1(t) is a very good approximation of the desired frequency
response, the frequency response of h1(t) displays ringing around the cut-off frequency 1 because of
the rectangular window. Finally, we delay h1(t) by τ to get a causal filter with linear phase. That is,
h1(t −τ) has as its magnitude response |H1( j)| ≈|Hlp( j)| and its phase response is ∠H1(j) =
−τ. Although the above procedure is a valid way to obtain approximate low-pass filters with linear
phase, they are not guaranteed to be rational and would be difficult to implement. Thus, other methods
are used to design filters.
I
Since ideal filters are not causal they cannot be used in real-time applications—that is when the input
signal needs to be processed as it comes to the filter. Imposing causality on the filter restricts the frequency
response of the filter in significant ways. According to the Paley-Wiener integral condition, a causal

334
CHAPTER 5:
Frequency Analysis: The Fourier Transform
and stable filter with frequency response H( j) should satisfy the condition
∞
Z
−∞
| log(H( j))|
1 + 2
d < ∞
(5.22)
To satisfy this condition, H( j) cannot be zero in any band of frequencies, because in such cases the
numerator of the integrand would be infinite. The Paley-Wiener integral condition is clearly not
satisfied by ideal filters. So they cannot be implemented or used in actual situations, but they can be
used as models for designing filters.
I
That ideal filters are not realizable can be understood also by considering what it means to make the
magnitude response of a filter zero in some frequency bands. A measure of attenuation is given by the loss
function in decibels, defined as
α() = −10 log10 |H( j)|2
= −20 log10 |H( j)| dB
Thus, when |H( j)| = 1 and there is no attenuation the loss is 0 dB, and when |H( j)| = 10−5 for a
large attenuation the loss is 100 dB. You quickly convince yourself that if a filter achieves a magnitude
response of 0 at any frequency this would mean a loss or attenuation at that frequency of ∞dBs! Values
of 60 to 100 dB attenuation are considered extremely good, and to obtain that the signal needs to be
attenuated by a factor of 10−3 to 10−5. A curious term JND or “just noticeable difference” is used by
experts in human hearing to characterize the smallest sound intensity that can be judged by a human
as different. Such a value varies from 0.25 to 1 dB. To illustrate what is loud in the dB scale, consider
the following cases: A sound pressure level higher than 130 dB causes pain; 110 dB is generated by an
amplified rock band performance [65].
I Example 5.15
The Gibb’s phenomenon, which we mentioned when discussing the Fourier series of periodic
signals with discontinuities, consists in ringing around these discontinuities. To see this, consider
a periodic train of square pulses x(t) of period T0 displaying discontinuities at kT0/2, for k =
±1, ±2, . . .. Show how the Gibb’s phenomenon is due to ideal low-pass filtering.
Solution
Choosing 2N + 1 of the Fourier series coefficients to approximate the signal x(t) is equivalent to
passing x(t) through an ideal low-pass filter,
H( j) =
1
−c ≤ ≤c
0
otherwise
having as impulse response a sinc function h(t). If the Fourier transform of the periodic signal x(t)
of fundamental frequency 0 = 2π/T0 is
X() =
∞
X
k=−∞
2πXkδ( −k0)

5.7 Convolution and Filtering
335
the output of the filter is the signal
xN(t) = F−1[X()H( j)]
= F−1


N
X
k=−N
2πXkδ( −k0)


or the inverse Fourier transform of X() multiplied by a low-pass filter with an ideal magnitude
response of 1 for −c <  < c where the cut-off frequency c is chosen so that N0 < c <
(N + 1)0. As such, xN(t) is the convolution
xN(t) = [x ∗h](t)
where h(t) is the inverse Fourier transform of H( j), or a sinc signal of inﬁnite support. The con-
volution around the discontinuities of x(t) causes ringing before and after them, and this ringing
appears independent of the value of N.
I
I Example 5.16
Obtain different filters from an RLC circuit (Figure 5.9) by choosing different outputs. Let the input
be a voltage source with Laplace transform Vi(s). For simplicity, let R = 1 , L = 1 H, and C = 1 F,
and assume the initial conditions to be zero.
Solution
I
Low-pass filter: Let the output be the voltage across the capacitor; by voltage division we have
that
VC(s) =
Vi(s)/s
1 + s + 1/s =
Vi(s)
s2 + s + 1
so that the transfer function is
Hlp(s) = VC(s)
Vi(s) =
1
s2 + s + 1
This is the transfer function of a second-order low-pass filter. If the input is a dc source, so
that its frequency is  = 0, the inductor is a short circuit (its impedance would be 0) and
FIGURE 5.9
RLC circuit for implementing different
filters.
−
−
−
+
+
+
R
L
C
vi(t)
vC(t)
vR(t)
vL(t)
+
−

336
CHAPTER 5:
Frequency Analysis: The Fourier Transform
the capacitor is an open circuit (its impedance would be infinite), so that the voltage in the
capacitor is equal to the voltage in the source. On the other hand, if the frequency of the input
source is very high, then the inductor is an open circuit and the capacitor a short circuit (its
impedance is zero) so that the capacitor voltage is zero. This is a low-pass filter. Notice that this
filter has no finite zeros, and complex conjugate poles.
I
High-pass filter: Suppose then that we let the output be the voltage across the inductor. Then
again by voltage division the transfer function
Hhp(s) = VL(s)
Vi(s) =
s2
s2 + s + 1
is that of a high-pass ﬁlter. Indeed, for a dc input (frequency zero) the impedance in the induc-
tor is zero, so that the inductor voltage is zero, and for very high frequency the impedance
of the inductor is very large so that it can be considered open circuit and the voltage in the
inductor equals that of the source. This ﬁlter has the same poles of the low-pass ﬁlter (this is
determined by the overall impedance of the circuit, which has not changed) and double zeros
at zero. It is these zeros that make the frequency response for low frequencies be close to zero.
I
Band-pass filter: Letting the output be the voltage across the resistor, its transfer function is
Hbp(s) = VR(s)
Vi(s) =
s
s2 + s + 1
or the transfer function of a band-pass ﬁlter. For zero frequency, the capacitor is an open cir-
cuit so the current is zero and the voltage across the resistor is zero. Similarly, for very high
frequency the impedance of the inductor is very large, or an open circuit, making the voltage
across the resistor zero because again the current is zero. For some middle frequency the serial
combination of the inductor and the capacitor resonates and will have zero impedance. At the
resonance frequency, the current achieves its largest value and the voltage across the resistor
does too. This behavior is that of a band-pass ﬁlter. This ﬁlter again has the same poles as the
other two, but only one zero at zero.
I
Band-stop filter: Finally, suppose we consider as output the voltage across the connection of the
inductor and the capacitor. At low and high frequencies, the impedance of the LC connection
is very high, or open circuit, and so the output voltage is the input voltage. At the resonance
frequency r = 1 the impedance of the LC connection is zero, so the output voltage is zero.
The resulting filter is a band-stop filter with the transfer function
Hbs(s) =
s2 + 1
s2 + s + 1
Second-order filters can then be easily identified by the numerator of their transfer functions.
Second-order low-pass filters have no zeros, and the numerator is N(s) = 1; band-pass filters
have a zero at s = 0 so N(s) = s, and so on. We will see next that such a behavior can be easily
seen from a geometric approach.
I

5.7 Convolution and Filtering
337
5.7.3 Frequency Response from Poles and Zeros
Given a rational transfer function H(s) = B(s)/A(s), to calculate its frequency response we let s = j
and find the magnitude and phase for a discrete set of frequencies. This can be done using MATLAB.
A geometric way to obtain an approximate magnitude and phase frequency responses is using the
effects of zeros and poles on the frequency response of a system.
Consider a function
G(s) = s −z
s −p
with a zero z and a pole p, as shown in Figure 5.10. The frequency response corresponding to G(s) at
some frequency 0 is found by letting s = j0, or
G(s)|s=j0 = j0 −z
j0 −p
Representing j0, z, and p, which are complex numbers, as vectors coming from the origin, then the
vector ⃗Z(0) = j0 −z (adding to ⃗Z(0) the vector corresponding to z gives a vector corresponding
to j0) goes from the zero z to j0, and likewise the vector ⃗P(0) = j0 −p goes from the pole p to
j0. The argument 0 in the vectors indicates that the magnitude and phase of these vectors depend
on the frequency at which we are finding the frequency response. As we change the frequency at
which we are finding the frequency response, the lengths and the phases of these vectors change.
Therefore,
G( j0) =
⃗Z(0)
⃗P(0)
= |⃗Z(0)|
|⃗P(0)|
ej(∠⃗Z(0)−∠⃗P(0))
and the magnitude response is
|G( j0)| = |⃗Z(0)|
|⃗P(0)|
(5.23)
and the phase response is
∠G( j0) = ∠⃗Z(0) −∠⃗P(0)
(5.24)
FIGURE 5.10
Geometric interpretation of poles and
zeros.
j Ω0
Z (Ω0)
P(Ω0)
→
→
p
z
s-plane

338
CHAPTER 5:
Frequency Analysis: The Fourier Transform
So that for 0 ≤0 < ∞, if we compute the length and the angle of ⃗Z(0) and ⃗P(0), the ratio of these
lengths gives the magnitude response and the difference of their angles gives the phase response.
For a filter with a transfer function
H(s) =
Q
i(s −zi)
Q
k(s −pk)
where zi, pk are zeros and poles of H(s) with vectors ⃗Zi() = j −zi and ⃗Pk() = j −pk, going from each of
the zeros and poles to the frequency at which we are computing the magnitude and phase response in the j
axis, gives
H( j) = H(s)|s=j =
Q
i ⃗Zi()
Q
k ⃗Pk()
=
Q
i |⃗Zi()|
Q
k |⃗Pk()|
|
{z
}
|H( j)|
ej
hP
i ∠(⃗Zi())−P
k ∠(⃗Pk()
i
|
{z
}
ej∠H( j)
(5.25)
I Example 5.17
Consider series RC circuit with a voltage source vi(t). Choose the output to obtain low-pass and
high-pass filters and use the poles and zeros of the transfer functions to determine their frequency
responses. Let R = 1 , C = 1 F, and the initial conditions be zero.
Solution
I
Low-pass filter: Let the output be the voltage across the capacitor. By voltage division, we obtain
that the transfer function of the filter is
H(s) = VC(s)
Vi(s) =
1/Cs
R + 1/Cs
For dc frequency, the capacitor behaves as an open circuit so that the output voltage equals the
input voltage, and for very high frequencies the impedance of the capacitor tends to zero so
that the voltage across the capacitor also goes to zero. This is a low-pass filter.
Let R = 1  and C = 1 F, so
H( j) =
1
1 + j =
1
⃗P()
Drawing a vector from the pole s = −1 to any point on the j axis gives ⃗P(), and for different
frequencies we get
 = 0
⃗P(0) = 1ej0
 = 1
⃗P(1) =
√
2ejπ/4
 = ∞
⃗P(∞) = ∞ejπ/2

5.7 Convolution and Filtering
339
Since there are no zeros, the frequency response of this filter depends inversely on the behavior
of the pole vector ⃗P(). The frequency responses for these three frequencies are:
H( j0) = 1ej0
H( j1) = 0.707e−jπ/4
H( j∞) = 0e−jπ/2
Thus, the magnitude response is unity at  = 0 and it decays as the frequency increases. The
phase is zero at  = 0, −π/4 at  = 1, and −π/2 at  →∞. The magnitude response is even
and the phase response is odd.
I
High-pass filter: Consider then the output being the voltage across the resistor. Again by voltage
division we obtain the transfer function of this circuit as
H(s) = Vr(s)
Vs(s) =
CRs
CRs + 1
Again, let C = R = 1, so the frequency response is
H( j) =
j
1 + j =
⃗Z()
⃗P()
The vector ⃗Z() goes from zero at the origin s = 0 to j in the j axis, and the vector ⃗P()
goes from the pole s = −1 to j in the j axis. The vectors and the frequency response, at three
different frequencies, are given by
 = 0
⃗P(0) = 1ej0
⃗Z(0) = 0ejπ/2
H( j0) =
⃗Z(0)
⃗P(0)
= 0ejπ/2
 = 1
⃗P(1) =
√
2ejπ/4
⃗Z(1) = 1ejπ/2
H( j1) =
⃗Z(1)
⃗P(1)
= 0.707ejπ/4
 = ∞
⃗P(∞) = ∞ejπ/2
⃗Z(∞) = ∞ejπ/2
H( j∞) =
⃗Z(∞)
⃗P(∞)
= 1ej0
Thus, the magnitude response is zero at  = 0 (this is due to the zero at s = 0, making ⃗Z(0) = 0
as it is right on top of the zero), and it grows to unity as the frequency increases (at very high
frequency, the lengths of the pole and the zero vectors are alike and so the magnitude response
is unity and the phase response is zero).
I
Remarks
I
Poles create “hills” at frequencies in the j axis in front of the poles imaginary parts. The closer the pole
is to the j axis, the narrower and higher the hill. If, for instance, the poles are on the j axis (this would
correspond to an unstable and useless filter) the frequency response at the frequency of the poles will be
infinity.

340
CHAPTER 5:
Frequency Analysis: The Fourier Transform
I
Zeros create “valleys” at the frequencies in the j axis in front of the zeros imaginary parts. The closer the
zero is to the j axis (from its left or its right, as the zeros are not restricted by stability to be in the open
left-hand s-plane) the closer the frequency response is to zero. If the zeros are on the j axis, the frequency
response at the frequency of the zeros is zero. Thus, poles produce frequency responses that look like hills (or
like the main pole in a circus) around the frequencies of the poles, and zeros make the frequency response
go to zero in the form of valleys around the frequencies of the zeros.
I Example 5.18
Use MATLAB to find and plot the poles and zeros and the corresponding magnitude and phase
frequency responses of:
(a)
A second-order band-pass filter and a high-pass filter realized using a series connection of a
resistor, an inductor, and a capacitor, each with unit resistance, inductance, and capacitance.
Let the input be a voltage source vs(t) and initial conditions be zero.
(b)
An all-pass filter with a transfer function
H(s) = s2 −2.5s + 1
s2 + 2.5s + 1
Solution
Our function freq resp s computes and plots the poles and the zeros of the filter transfer function
and the corresponding frequency response (the function requests the coefficients of its numerator
and denominator in decreasing order of powers of s).
(a)
As from a Example 5.16, the transfer functions of the band-pass and high-pass second-order
filters are
Hbp(s) =
s
s2 + s + 1
Hhp(s) =
s2
s2 + s + 1
The denominator in the two cases is exactly the same since the values of R, L, and C remain
the same for the two ﬁlters—the only difference is in the numerator.
To compute the frequency response of these ﬁlters and to plot their poles and zeros, we
used the following script, which uses two functions: freqresp s, which we give below, and
splane, which plots the poles and zeros. The coefﬁcients of the numerator and the denomi-
nator correspond to the coefﬁcients, from the highest to the lowest order of s, of the transfer
function.
%%%%%%%%%%%%%%%%%%%%%
% Example 5.18---Frequency response
%%%%%%%%%%%%%%%%%%%%%
n = [0 1 0]; % numerator coefﬁcients -- bandpass
% n = [1 0 0]; % numerator coefﬁcients -- highpass
d = [1 1 1]; % denominator coefﬁcients

5.7 Convolution and Filtering
341
wmax = 10; % maximum frequency
[w, Hm, Ha] = freqresp s(n, d, wmax); % frequency response
splane(n, d) % plotting of poles and zeros
The following is the function freqresp s used to compute the magnitude and phase response
of the filter with the given numerator and denominator coefficients.
function [w, Hm, Ha] = freqresp s(b, a, wmax)
w = 0:0.01:wmax;
H = freqs(b, a, w);
Hm = abs(H); % magnitude
Ha = angle(H) ∗180/pi; % phase in degrees
I
Band-pass filter: Letting the output of the filter be the voltage across resistor, we find that
the transfer function has a zero at zero, so that the frequency response is zero at  = 0.
When  goes to infinity, one of the two poles cancels the zero effect so that the other pole
makes the frequency response tend to zero.
I
High-pass filter: When the output of the filter is the voltage across the inductor the filter is
high pass. In this case there is a double zero at s = 0, and the poles are located as before.
Thus, when  = 0 the magnitude response is zero due to the double zeros at zero, and
when  goes to infinity the effect of two poles and the two zeros cancel out giving a
constant magnitude response, which corresponds to a high-pass filter.
The results for the band-pass and the high-pass filters are shown in Figure 5.11. Notice that the
frequency response of the band-pass and the high-pass ﬁlter is determined by the ’number’
of zeros at the origin. The ’location’ of zeros, like in the all-pass ﬁlter we consider next, also
determines the frequency response.
(b)
All-pass filter: The poles and the zeros of an all-pass filter have the same imaginary parts, but
the negative of its real part. At any frequency in the j-axis the lengths of the vectors from the
poles equal the length of the vectors from the zeros to the frequency in the j axis. Thus the
magnitude response of the ﬁlter is unity. The following changes to the above script are needed
for the all-pass ﬁlter:
clear all
clf
n = [1 −2.5 1];
d = [1 2.5 1];
wmax = 10;
freq resp s(n, d, wmax)
The results are shown in Figure 5.12.
I
5.7.4 Spectrum Analyzer
A spectrum analyzer is a device that measures the spectral characteristics of a signal. It can be imple-
mented as a bank of narrow band band-pass ﬁlters with fixed bandwidths covering the desired
frequencies (see Figure 5.13). The power at the output of each ﬁlter is computed and displayed at the
corresponding center frequency. Another possible implementation is using a band-pass ﬁlter with an
adjustable center frequency, with the power in its bandwidth being computed and displayed [16].

342
CHAPTER 5:
Frequency Analysis: The Fourier Transform
0
5
10
0
0.2
0.4
0.6
0.8
1
|H(jΩ)|
Ω
Magnitude Response
0
5
10
−50
0
50
∠H(j Ω)
Ω
σ
Phase Response
−1
−0.5
0
0.5
−1
0
1
j Ω
Poles/Zeros
(a)
0
5
10
0
0.5
1
|H(j Ω)|
Ω
Magnitude Response
0
5
10
0
50
100
150
∠H(jΩ)
Ω
Phase Response
−1
−0.5
0
0.5
−1
0
1
σ
jΩ
Poles/Zeros
(b)
FIGURE 5.11
Frequency response and poles/zeros location of (a) band-pass and (b) high-pass RLC filters.

5.7 Convolution and Filtering
343
0
0.2
0.4
0.6
0.8
1
|H(jω)|
Ω
0
5
10
Magnitude Response
0
5
10
Ω
−100
0
100
∠H(jω)
Phase Response
−2
0
2
−1
−0.5
0
0.5
1
σ
jΩ
Poles/Zeros
FIGURE 5.12
Frequency response and poles/zeros location of the all-pass filter.
FIGURE 5.13
Bank-of-filter spectrum analyzer. LPF stands for
low-pass filter, and BPFi corresponds to band-pass
filters, i = 1, . . . , N.
x(t )
Power
measurement
Power
measurement
Power
measurement
LPF
BPF1
BPFN
Px(0)
Px(Ω1)
Px(ΩN)
.
.
.
.
.
.

344
CHAPTER 5:
Frequency Analysis: The Fourier Transform
If the input of the spectrum analyzer is x(t), the output of either the fixed- or the adjustable-bandpass
filters in the implementations—assumed to have a very narrow bandwidth 1—would be
y(t) = 1
2π
0+0.51
Z
0−0.51
X()ejtd
≈1
2π 1 X(0)ej0t
Computing the mean square of this signal we get
1
T
Z
T
|y(t)|2dt =
1
2π
2
|X(0)|2
which is proportional to the power or the energy of the signal in 0 ± 1. A similar computation
can be done at each of the frequencies of the input signal.
Remarks
I
The bank-of-filter spectrum analyzer is used for the audio range of the spectrum.
I
Radio frequency spectrum analyzers resemble an AM demodulator. It usually consists of a single narrow-
band intermediate frequency (IF) bandpass ﬁlter fed by a mixer. The local oscillator sweeps across the
desired band, and the power at the output of the ﬁlter is computed and displayed on a monitor.
5.8 ADDITIONAL PROPERTIES
We consider now some additional properties of the Fourier transform, some of which look like those
of the Laplace transform when s = j and some are different.
5.8.1 Time Shifting
If x(t) has a Fourier transform X(), then
x(t −t0) ⇔X()e−jt0
x(t + t0) ⇔X()ejt0
(5.26)
The Fourier transform of x(t −t0) is
F[x(t −t0)] =
∞
Z
−∞
x(t −t0)e−jtdt
=
∞
Z
−∞
x(τ)e−j(τ+t0)dτ = e−jt0X()
where we changed the variable to τ = t −t0. Likewise for x(t + t0).

5.8 Additional Properties
345
It is important to realize that shifting in time does not change the frequency content of the signal—
that is, the signal does not change when delayed or advanced. This is clear when we see that the
magnitude of the two transforms, corresponding to the original and the shifted signals, is the same,
|X()| = |X()e±jt0|
and the effect of the time shift is only in the phase spectrum.
I Example 5.19
Consider the signal
x(t) = A[δ(t −τ) + δ(t + τ)]
Find its Fourier transform X(). Use this Fourier pair and the duality property to verify the Fourier
transform of a cos(0t) obtained before.
Solution
Applying the time-shift property, we have
X() = A[1e−jτ + 1ejτ]
= 2A cos(τ)
giving the Fourier transform pair
x(t) = A[δ(t −τ) + δ(t + τ)]
⇔
X() = 2A cos(τ)
Using the duality property, we then have
X(t) = 2A cos(tτ)
⇔
2πx(−) = 2πA[δ(− −τ) + δ(− + τ)]
Let τ = 0, Use the evenness of δ(t) to get
A cos(0t)
⇔
πA[δ( + 0) + δ( −0)]
I
I Example 5.20
Consider computing the Fourier transform of y(t) = sin(0t) using the Fourier transform of the
cosine signal x(t) = cos(0t) we just found.
Solution
Since y(t) = cos(0t −π/2) = x(t −π/(20)), applying the time-shifting property, we then get
F[sin(0t)] = F[x(t −π/20)]
= π[δ( −0) + δ( + 0)]e−jπ/(20)
= πδ( −0)e−jπ/2 + πδ( + 0)ejπ/2
= −jπδ( −0) + jπδ( + 0)

346
CHAPTER 5:
Frequency Analysis: The Fourier Transform
after applying the sifting property of δ(). The above shows that this Fourier transform is different
from the one for the cosine in the phase only.
I
5.8.2 Differentiation and Integration
If x(t), −∞< t < ∞, has a Fourier tranform X(), then
dNx(t)
dtN
⇔
( j)NX()
(5.27)
t
Z
−∞
x(σ)dσ
⇔
X()
j
+ πX(0)δ()
(5.28)
where
X(0) =
∞
Z
−∞
x(t)dt
From the inverse Fourier transform given by
x(t) = 1
2π
∞
Z
−∞
X()ejtd
we then have that
dx(t)
dt
= 1
2π
∞
Z
−∞
X()d ejt
dt
d
= 1
2π
∞
Z
−∞
[X()j] ejtd
indicating that
dx(t)
dt
⇔jX()
and similarly for higher derivatives.
The proof of the integration property can be done in two parts:
1.
The convolution of u(t) and x(t) gives the integral—that is
t
Z
−∞
x(τ)dτ =
∞
Z
−∞
x(τ)u(t −τ)dτ = [x ∗u](t)

5.8 Additional Properties
347
since u(t −τ) as a function of τ equals
u(t −τ) =
1
τ < t
0
τ > t
We thus have that
F


t
Z
−∞
x(τ)dτ

= X()F[u(t)]
(5.29)
2.
Since the unit-step signal is not absolutely integrable its Fourier transform cannot be found from
the integral definition, and we cannot use its Laplace transform either because its ROC does not
include the j axis. Let’s transform it into an absolutely integrable signal by subtracting 1/2 and
multiplying the result by 2. This gives the sign signal:
sgn(t) = 2[u(t) −0.5] =
1
t > 0
−1
t < 0
The derivative of sgn(t) is
dsgn(t)
dt
= 2δ(t)
and thus if S() = F[sgn(t)],
S() = 2
j
using the derivative property. The linearity of the Fourier transform applied to the deﬁnition of
sgn(t) gives
F[sgn(t)] = 2F[u(t)] −2πδ()
⇒F[u(t)] = 1
j + πδ()
(5.30)
Replacing the Fourier transform of u(t) in Equation (5.29), we get
F


t
Z
−∞
x(τ)dτ

= X()
 1
j + πδ()

= X()
j
+ πX(0)δ()
(5.31)
Remarks
I
Just like in the Laplace transform where the operator s corresponds to the derivative operation in time of
the signal, in the Fourier transform j becomes the corresponding operator for the derivative operation in
time of the signal.

348
CHAPTER 5:
Frequency Analysis: The Fourier Transform
I
If X(0) (i.e., the dc value of X()) is zero, then the operator 1/( j) corresponds to integration in time
of x(t), just like 1/s in the Laplace domain. For X(0) to be zero the integral of the signal from −∞to ∞
must be zero.
I Example 5.21
Suppose a system is represented by a second-order differential equation with constant coefficients:
2y(t) + 3dy(t)
dt
+ d2y(t)
dt2
= x(t)
and that the initial conditions are zero. Let x(t) = δ(t). Find y(t).
Solution
Computing the Fourier transform of this equation, we get
[2 + 3j + ( j)2]Y() = X()
Replacing X() = 1 and solving for Y(), we have
Y() =
1
2 + 3j + ( j)2
=
1
( j + 1)( j + 2)
=
1
( j + 1) +
−1
( j + 2)
and the inverse Fourier transform of these terms gives
y(t) = [e−t −e−2t]u(t)
I
I Example 5.22
Find the Fourier transform of the triangular pulse
x(t) = r(t) −2r(t −1) + r(t −2)
which is piecewise linear, using the derivative property.
Solution
A first derivative gives
dx(t)
dt
= u(t) −2u(t −1) + u(t −2)

5.8 Additional Properties
349
and a second derivative gives
d2x(t)
dt2
= δ(t) −2δ(t −1) + δ(t −2)
Using the time-shift and the derivative properties, we get from the expression for the second
derivative and letting X() be the Fourier transform of x(t):
( j)2X() = 1 −2e−j + e−j2
= e−j[ej −2 + e−j]
so that
X() = 2e−j
2 [1 −cos()]
I
I Example 5.23
Consider the integral
y(t) =
t
Z
−∞
x(τ)dτ −∞< t < ∞
where x(t) = u(t + 1) −u(t −1). Find the Fourier transform Y() directly and from the integration
property.
Solution
The integral is
y(t) =



0
t < −1
t + 1
−1 ≤t < 1
2
t ≥1
or
y(t) = [r(t + 1) −r(t −1) −2u(t −1)]
|
{z
}
y1(t)
+2u(t −1)
The Fourier transform of y1(t) is
Y1() =
e s −e−s
s2
−2e−s
s

s=j
= −2j sin()
2
+ j2e−j


350
CHAPTER 5:
Frequency Analysis: The Fourier Transform
The Fourier transform of 2u(t −1) is −2je−j/ + 2πδ() so that
Y() = −2j sin()
2
+ j2e−j

−j2e−j

+ 2πδ()
= −2j sin()
2
+ 2πδ()
To use the integration property we first need X(), which is
X() = 2 sin()

and according to the property,
Y() = X()
j
+ πX(0)δ()
= −2j sin()
2
+ 2πδ()
since X(0) = 2 (using L’Hˆopital’s rule). As expected, the two results coincide.
I
5.9 WHAT HAVE WE ACCOMPLISHED? WHAT IS NEXT?
You should by now have a very good understanding of the frequency representation of signals and
systems. In this chapter, we have unified the treatment of periodic and nonperiodic signals and their
spectra, and consolidated the concept of frequency response of a linear time-invariant system.
Basic properties of the Fourier transform and important Fourier pairs are given in Tables 5.1 and
5.2. Two signiﬁcant applications are in ﬁltering and communications. We introduced the basics of
ﬁltering in this chapter and will expand on them in Chapter 6. The fundamentals of modulation
provided in this chapter will be illustrated in Chapter 6 where we will consider their application in
communications.
Certainly the next step is to find out where the Laplace and the Fourier analyses apply, which will be
done in Chapter 6. After that, we will go into discrete-time signals and systems. We will show that
sampling, quantization, and coding bridge the continuous-time and the digital signal processing, and
that transformations similar to the Laplace and the Fourier transforms will permit us to do processing
of discrete–time signals and systems.
PROBLEMS
5.1. Fourier series versus Fourier transform—MATLAB
The connection between the Fourier series and the Fourier transform can be seen by considering what
happens when the period of a periodic signal increases to a point at which the periodicity is not clear
as only one period is seen. Consider a train of pulses x(t) with a period T0 = 2, and a period of x(t) is
x1(t) = u(t + 0.5) −u(t −0.5). Let T0 be increased to 4, 8, and 16.

Problems
351
Table 5.1 Basic Properties of the Fourier Transform
Time Domain
Frequency Domain
Signals and constants
x(t), y(t), z(t), α, β
X(), Y(), Z()
Linearity
αx(t) + βy(t)
αX() + βY()
Expansion/contraction in time
x(αt), α ̸= 0
1
|α|X
  
α

Reﬂection
x(−t)
X(−)
Parseval’s energy relation
Ex =
R ∞
−∞|x(t)|2dt
Ex =
1
2π
R ∞
−∞|X()|2d
Duality
X(t)
2πx(−)
Time differentiation
dnx(t)
dtn , n ≥1, integer
( j)nX()
Frequency differentiation
−jtx(t)
dX()
d
Integration
R t
−∞x(t′)dt′
X()
j + πX(0)δ()
Time shifting
x(t −α)
e−jαX()
Frequency shifting
ej0tx(t)
X( −0)
Modulation
x(t) cos(ct)
0.5[X( −c) + X( + c)]
Periodic signals
x(t) = P
k Xkejk0t
X() = P
k 2πXkδ( −k0)
Symmetry
x(t) real
|X()| = |X(−)|
∠X() = −∠X(−)
Convolution in time
z(t) = [x ∗y](t)
Z() = X()Y()
Windowing/multiplication
x(t)y(t)
1
2π [X ∗Y]()
Cosine transform
x(t) even
X() =
R ∞
−∞x(t) cos(t)dt, real
Sine transform
x(t) odd
X() = −j
R ∞
−∞x(t) sin(t)dt, imaginary
Table 5.2 Fourier Transform Pairs
Function of Time
Function of 
1
δ(t)
1
2
δ(t −τ)
e−jτ
3
u(t)
1
j + πδ()
4
u(−t)
−1
j + πδ()
5
sgn(t) = 2[u(t) −0.5]
2
j
6
A, −∞< t < ∞
2πAδ()
7
Ae−atu(t), a > 0
A
j+a
8
Ate−atu(t), a > 0
A
( j+a)2
9
e−a|t|, a > 0
2a
a2+2
10
cos(0t), −∞< t < ∞
π[δ( −0) + δ( + 0)]
11
sin(0t), −∞< t < ∞
−jπ[δ( −0) −δ( + 0)]
12
A[u(t + τ) −u(t −τ)], τ > 0
2Aτ sin(τ)
τ
13
sin(0t)
πt
u( + 0) −u( −0)
14
x(t) cos(0t)
0.5[X( −0) + X( + 0)]

352
CHAPTER 5:
Frequency Analysis: The Fourier Transform
(a) Find the Fourier series coefficient X0 for each of the values of T0 and indicate how it changes for the
different values of T0.
(b) Find the Fourier series coefficients for x(t) and carefully plot the magnitude line spectrum for each of
the values of T0. Explain what is happening in these spectra.
(c) If you were to let T0 be very large, what would you expect to happen to the Fourier coefficients?
Explain.
(d) Write a MATLAB script that simulates the conversion from the Fourier series to the Fourier transform
of a sequence of rectangular pulses as the period is increased. The Fourier coefﬁcients need to be
multiplied by the period so that they do not become insignificant. Plot using stem the magnitude line
spectrum for pulse sequences with periods T0 from 4 to 62.
5.2. Fourier transform from Laplace transform—MATLAB
The Fourier transform of finite-support signals, which are absolutely integrable or finite energy, can be
obtained from their Laplace transform rather than doing the integral. Consider the following signals:
x1(t) = u(t + 0.5) −u(t −0.5)
x2(t) = sin(2πt)[u(t) −u(t −0.5)]
x3(t) = r(t + 1) −2r(t) + r(t −1)
(a) Plot each of the signals.
(b) Find the Fourier transforms {Xi()} for i = 1, 2, and 3 using the Laplace transform.
(c) Use MATLAB’s symbolic function fourier to compute the Fourier transform of the given signals. Plot
the magnitude spectrum corresponding to each of the signals.
5.3. Fourier transform from Laplace transform of infinite-support signals—MATLAB
For signals with infinite support, their Fourier transforms cannot be derived from the Laplace transform
unless they are absolutely integrable or the region of convergence of the Laplace transform contains the j
axis. Consider the signal x(t) = 2e−2|t|.
(a) Plot the signal x(t) for −∞< t < ∞.
(b) Use the evenness of the signal to find the integral
∞
Z
−∞
|x(t)|dt
and determine whether this signal is absolutely integrable or not.
(c) Use the integral definition of the Fourier transform to find X().
(d) Use the Laplace transform of x(t) to verify the above found Fourier transform.
(e) Use MATLAB’s symbolic function fourier to compute the Fourier transform of x(t). Plot the magnitude
spectrum corresponding to x(t).
5.4. Fourier and Laplace transforms—MATLAB
Consider the signal x(t) = 2e−2t cos(2πt)u(t).
(a) Use the fact this signal is bounded by the exponential ±2e−2tu(t) to show that the integral
∞
Z
−∞
|x(t)|dt

Problems
353
is finite, indicating the signal is absolutely integrable and also finite energy.
(b) Use the Laplace transform to find the Fourier transform X() of x(t).
(c) Use the MATLAB function fourier to compute the magnitude and phase spectrum of X().
5.5. Fourier transform of causal signals
Any causal signal x(t) having a Laplace transform with poles in the open-left s-plane (i.e., not including the
j axis) has, as we saw before, a region of convergence that includes the j axis, and as such its Fourier
transform can be found from its Laplace transform. Consider the following signals:
x1(t) = e−2tu(t)
x2(t) = r(t)
x3(t) = x1(t)x2(t)
(a) Determine the Laplace transform of the above signals (use properties of the Laplace transform)
indicating the corresponding region of convergence.
(b) Determine for which of these signals you can find its Fourier transform from its Laplace transform.
Explain.
(c) Give the Fourier transform of the signals that can be obtained from their Laplace transform.
5.6. Duality of Fourier transform
There are some signals for which the Fourier transforms cannot be found directly by either the integral
definition or the Laplace transform, and for those we need to use the properties of the Fourier transform, in
particular the duality property. Consider, for instance,
x(t) = sin(t)
t
or the sinc signal. Its importance is that it is the impulse response of an ideal low-pass filter.
(a) Let X() = A[u( + 0) −u( −0] be a possible Fourier transform of x(t). Find the inverse Fourier
transform of X() using the integral equation to determine the values of A and 0.
(b) How could you use the duality property of the Fourier transform to obtain X()? Explain.
5.7. Cosine and sine transforms
The Fourier transforms of even and odd functions are very important. The reason is that they are
computationally simpler than the Fourier transform. Let x(t) = e−|t| and y(t) = e−tu(t) −etu(−t).
(a) Plot x(t) and y(t), and determine whether they are even or odd.
(b) Show that the Fourier transform of x(t) is found from
X() =
∞
Z
−∞
x(t) cos(t)dt
which is a real function of , thus its computational importance. Show that X() is also even as a
function of .
(c) Find X() from the above equation (called the cosine transform).
(d) Show that the Fourier transform of y(t) is found from
Y() = −j
∞
Z
−∞
y(t) sin(t)dt

354
CHAPTER 5:
Frequency Analysis: The Fourier Transform
which is imaginary function of , thus its computational importance. Show that Y() is also odd as a
function of .
(e) Find Y() from the above equation (called the sine transform). Verify that your results are correct by
finding the Fourier transform of z(t) = x(t) + y(t) directly and using the above results.
(f) What advantages do you see to the cosine and sine transforms? How would you use the cosine and the
sine transforms to compute the Fourier transform of any signal, not necessarily even or odd? Explain.
5.8. Time versus frequency—MATLAB
The supports in time and in frequency of a signal x(t) and its Fourier transform X() are inversely
proportional. Consider a pulse
x(t) = 1
T0
[u(t) −u(t −T0)]
(a) Let T0 = 1 and T0 = 10 and find and compare the corresponding |X()|.
(b) Use
MATLAB
to
simulate
the
changes
in
the
magnitude
spectrum
when
T0 = 10k
for
k = 0, . . . , 4 for x(t). Compute X() and plot its magnitude spectra for the increasing values of
T0 on the same plot. Explain the results.
5.9. Smoothness and frequency content—MATLAB
The smoothness of the signal determines the frequency content of its spectrum. Consider the signals
x(t) = u(t + 0.5) −u(t −0.5)
y(t) = (1 + cos(πt))[u(t + 0.5) −u(t −0.5)]
(a) Plot these signals. Can you tell which one is smoother?
(b) Find X() and carefully plot its magnitude |X()| versus frequency .
(c) Find Y() (use the Fourier transform properties) and carefully plot its magnitude |Y()| versus
frequency .
(d) Which one of these two signals has higher frequencies? Can you now tell which of the signals is
smoother? Use MATLAB to decide. Make x(t) and y(t) have unit energy. Plot 20 log10 |Y()| and
20 log10 |X()| using MATLAB and see which of the spectra shows lower frequencies.
5.10. Smoothness and frequency—MATLAB
Let the signals x(t) = r(t + 1) −2r(t) + r(t −1) and y(t) = dx(t)/dt.
(a) Plot x(t) and y(t).
(b) Find X() and carefully plot its magnitude spectrum. Is X() real? Explain.
(c) Find Y() (use properties of Fourier transform) and carefully plot its magnitude spectrum. Is Y() real?
Explain.
(d) Determine from the above spectra which of these two signals is smoother. Use MATLAB to plot
20 log10 |Y()| and 20 log10 |X()| and decide. Would you say in general that computing the derivative
of a signal generates high frequencies or possible discontinuities?
5.11. Integration and smoothing—MATLAB
Consider the signal
x(t) = u(t + 1) −2u(t) + u(t −1)
and let
y(t) =
t
Z
−∞
x(τ)dτ

Problems
355
(a) Plot x(t) and y(t).
(b) Find X() and carefully plot its magnitude spectrum. Is X() real? Explain. (Use MATLAB to do the
plotting.)
(c) Find Y() and carefully plot its magnitude spectrum. Is Y() real? Explain. (Use MATLAB to do the
plotting.)
(d) Determine from the above spectra which of these two signals is smoother. Use MATLAB to decide.
Would you say that in general by integrating a signal you get rid of higher frequencies, or smooth out
a signal?
5.12. Duality of Fourier transforms
As indicated by the derivative property, if we multiply a Fourier transform by ( j)N, it corresponds to
computing an Nth derivative of its time signal. Consider the dual of this property—that is, if we compute
the derivative of X(), what would happen to the signal in the time?
(a) Let x(t) = δ(t −1) + δ(t + 1). Find its Fourier transform (using properties) X().
(b) Compute dX()/d and determine its inverse Fourier transform.
5.13. Periodic functions in frequency
The duality property provides interesting results. Consider the signal
x(t) = δ(t + T1) + δ(t −T1)
(a) Find X() = F[x(t)] and plot both x(t) and X().
(b) Suppose you then generate a signal
y(t) = δ(t) +
∞
X
k=1
[δ(t + kT0) + δ(t −kT0)]
Find its Fourier transform Y() and plot both y(t) and Y().
(c) Are y(t) and the corresponding Fourier transform Y() periodic in time and in frequency? If so,
determine their periods.
5.14. Sampling signal
The sampling signal
δTs(t) =
∞
X
n=−∞
δ(t −nTs)
will be important in the sampling theory later on.
(a) As a periodic signal of period Ts, express δTs(t) by its Fourier series.
(b) Determine then the Fourier transform 1() = F[δTs(t)].
(c) Plot δTs(t) and 1() and comment on the periodicity of these two functions.
5.15. Piecewise linear signals
The derivative property can be used to simplify the computation of some Fourier transforms. Let
x(t) = r(t) −2r(t −1) + r(t −2)
(a) Find and plot the second derivative with respect to t of x(t), or y(t) = d2x(t)/dt2.
(b) Find X() from Y() using the derivative property.
(c) Verify the above result by computing the Fourier transform X() directly from x(t) using the Laplace
transform.

356
CHAPTER 5:
Frequency Analysis: The Fourier Transform
5.16. Periodic signal-equivalent representations
Applying the time and frequency shifts it is possible to get different but equivalent Fourier transforms of
periodic signals. Assume a period of a periodic signal x(t) of period T0 is x1(t), so that
x(t) =
X
k
x1(t −kT0)
and as seen in Chapter 4 the Fourier series coefficients of x(t) are found as Xk = X1( jk0)/T0, so that x(t)
can also be represented as
x(t) = 1
T0
X
k
X1( jk0)ejk0t
(a) Find the Fourier transform of the first expression given above for x(t) using the time-shift property.
(b) Find the Fourier transform of the second expression for x(t) using the frequency-shift property.
(c) Compare the two expressions and comment on your results.
5.17. Modulation property
Consider the raised-cosine pulse
x(t) = [1 + cos(πt)](u(t + 1) −u(t −1))
(a) Carefully plot x(t).
(b) Find the Fourier transform of the pulse p(t) = u(t + 1) −u(t −1).
(c) Use the definition of the pulse p(t) and the modulation property to find the Fourier transform of x(t) in
terms of P() = F[p(t)].
5.18. Solution of differential equations
An analog averager is characterized by the relationship
dy(t)
dt
= 0.5[x(t) −x(t −2)]
where x(t) is the input and y(t) the output. If x(t) = u(t) −2u(t −1) + u(t −2):
(a) Find the Fourier transform of the output Y().
(b) Find y(t) from Y().
5.19. Generalized AM
Consider the following generalization of amplitude modulation where instead of multiplying by a cosine we
multiply by a periodic signal with harmonic frequencies much higher than those of the message. Suppose
the carrier c(t) is a periodic signal with fundamental frequency 0, let’s say
c(t) =
6
X
k=4
2 cos(k0t)
and that the message is a sinusoid of frequency 0 = 2π, or x(t) = cos(0t).
(a) Find the AM signal s(t) = x(t)c(t).
(b) Determine the Fourier transform S().
(c) What would be a possible advantage of this generalized AM system? Explain.

Problems
357
5.20. Filter for half-wave rectifier
Suppose you want to design a dc source using a half-wave rectified signal x(t) and an ideal filter. Let x(t)
be periodic, T0 = 2, and with a period
x1(t) =
sin(πt)
0 ≤t ≤1
0
1 < t ≤2,
(a) Find the Fourier transform X() of x(t), and plot the magnitude spectrum including the dc and the first
three harmonics.
(b) Determine the magnitude and cut-off frequency of an ideal low-pass filter H( j) such that when we
have x(t) as its input, the output is y(t) = 1. Plot the magnitude response of the ideal low-pass filter.
(For simplicity assume the phase is zero.)
5.21. Passive RLC filters—MATLAB
Consider an RLC series circuit with a voltage source vs(t). Let the values of the resistor, capacitor, and
inductor be unity. Plot the poles and zeros and the corresponding frequency responses of the filters with
the output the voltage across the
(a) Capacitor
(b) Inductor
(c) Resistor
Indicate the type of filter obtained in each case. Use MATLAB to plot the poles and zeros, the magnitude,
and the phase response of each of the filters obtained above.
5.22. AM modulation and demodulation
A pure tone x(t) = 4 cos(1000t) is transmitted using an AM communication system with a carrier
cos(10,000t). The output of the AM system is
y(t) = x(t) cos(10,000t)
At the receiver, to recover x(t) the sent signal y(t) needs first to be separated from the thousands of other
signals. This is done with a band-pass filter with a center frequency equal to the carrier frequency, and the
output of this filter then needs to be demodulated.
(a) Consider an ideal band-pass ﬁlter H( j). Let its phase be zero. Determine its bandwidth, center
frequency, and amplitude so we get as its output 10y(t). Plot the spectrum of x(t), 10y(t), and the
magnitude frequency response of H( j).
(b) To demodulate 10y(t), we multiply it by cos(10,000t). You need then to pass the resulting signal
through an ideal low-pass ﬁlter to recover the original signal x(t). Plot the spectrum of
z(t) = 10y(t) cos(10,000t)
and from it determine the frequency response of the low-pass filter G( j) needed to recover x(t). Plot
the magnitude response of G( j).
5.23. Ideal low-pass filter—MATLAB
Consider an ideal low-pass filter H(s) with zero phase and magnitude response
|H( j)| =
1
−π ≤ ≤π
0
otherwise
(a) Find the impulse response h(t) of the low-pass filter. Plot it and indicate whether this filter is a causal
system or not.

358
CHAPTER 5:
Frequency Analysis: The Fourier Transform
(b) Suppose you wish to obtain a band-pass filter G( j) from H( j). If the desired center frequency of
|G( j)| is 5π, and its desired magnitude is 1 at the center frequency, how would you process h(t) to
get the desired filter? Explain your procedure.
(c) Use symbolic MATLAB to find h(t), g(t), and G( j). Plot |H( j)|, h(t), g(t), and |G( j)|.
5.24. Magnitude response from poles and zeros—MATLAB
Consider the following filters with the given poles and zeros and dc constant:
H1(s):
K = 1 poles
p1 = −1, p2,3 = −1 ± jπ; zeros
z1 = 1, z2,3 = 1 ± jπ
H2(s):
K = 1 poles
p1 = −1, p2,3 = −1 ± jπ; zeros
z1,3 = ±jπ
H3(s):
K = 1 poles
p1 = −1, p2,3 = −1 ± jπ;
zero
z1 = 1
Use MATLAB to plot the magnitude responses of these ﬁlters and indicate the type of ﬁlters
they are.
5.25. Different types of AM modulations—MATLAB
Let the signal
m(t) = sin(2πt)[u(t) −u(t −1)]
be the message or input to different types of AM systems with the output the following signals. Carefully
plot m(t) and the following outputs in 0 ≤t ≤1 and their corresponding spectra using MATLAB. Let the
sampling period be Ts = 0.001.
(a) y1(t) = m(t) cos(20πt)
(b) y2(t) = [1 + m(t)] cos(20πt)
5.26. Windows—MATLAB
The signal x(t) in Problem 5.17 is called a raised-cosine window. Notice that it is a very smooth signal and
that it decreases at both ends. The rectangular window is the signal y(t) = u(t + 1) −u(t −1).
(a) Use MATLAB to compute the magnitude spectrum of x(t) and y(t) and indicate which is the smoother
of the two by considering the presence of high frequencies as an indication of roughness.
(b) When computing the Fourier transform of a very long signal it makes sense to break it up into smaller
sections and compute the Fourier transform of each. In such a case, windows are used to smooth out
the transition from one section to the other. Consider a sinusoid z(t) = cos(2πt) for 0 ≤t ≤1000 sec.
Divide the signal into two sections of duration 500 sec. Multiply the corresponding signal in each of
the sections by a raised-cosine x(t) and rectangular y(t) windows of length 500 and compute using
MATLAB the corresponding Fourier transforms. Compare them to the Fourier transform of the whole
signal and comment on your results. Sample all the signals using Ts = 1/(4π) as the sampling period.
(c) Consider the computation of the Fourier transform of the acoustic signal corresponding to a train
whistle, which MATLAB provides as a sampled signal in “train.mat” using the discrete approximation
of the Fourier transform. The frequency content of the whole signal (hard to find) would not be as
meaningful as the frequency content of a smaller section of it as they change with time. Compute
the Fourier transform of sections of 1000 samples by windowing the signal with the raised-cosine
window (sampled with the same sampling period as the “train.mat” signal or Ts = 1/Fs where Fs is the
sampling frequency given for “train.mat”). Plot the spectra of a few of these segments and comment
on the change in the frequency content as time changes.

CHAPTER 6
Application to Control and
Communications
Who are you going to believe? Me or
your own eyes.
Julius “Groucho” Marx (1890–1977)
comedian and actor
6.1 INTRODUCTION
Control and communications are areas in electrical engineering where the Laplace and the Fourier
analyses apply. In this chapter, we illustrate how these transform methods and the concepts of trans-
fer function, frequency response, and spectrum connect with the classical theories of control and
communications.
In classical control, the objective is to change the dynamics of a given system to be able to achieve
a desired response by frequency-domain methods. This is typically done by means of a feedback
connection of a controller to a plant. The plant is a system such as a motor, a chemical plant, or an
automobile we would like to control so that it responds in a certain way. The controller is a system we
design to make the plant follow a prescribed input or reference signal. By feeding back the response of
the system to the input, it can be determined how the plant responds to the controller. The commonly
used negative feedback generates an error signal that permits us to judge the performance of the
controller. The concepts of transfer function, stability of systems, and different types of responses
obtained through the Laplace transform are very useful in the analysis and design of classical control
systems.
A communication system consists of three components: a transmitter, a channel, and a receiver. The
objective of communications is to transmit a message over a channel to a receiver. The message is a
signal, for instance, a voice or a music signal, typically containing low frequencies. Transmission of
the message can be done over the airwaves or through a line connecting the transmitter to the receiver,
or a combination of the two—constituting channels with different characteristics. Telephone commu-
nication can be done with or without wires, and radio and television are wireless. The concepts of
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00009-0
c⃝2011, Elsevier Inc. All rights reserved.
359

360
CHAPTER 6:
Application to Control and Communications
frequency, bandwidth, spectrum, and modulation developed by means of the Fourier transform are
fundamental in the analysis and design of communication systems.
The aim of this chapter is to serve as an introduction to problems in classical control and commu-
nications and to link them with the Laplace and Fourier analyses. More in-depth discussion of these
topics can be found in many excellent texts in control and communications.
The other topic covered in this chapter is an introduction to analog ﬁlter design. Filtering is a very
important application of LTI systems in communications, control, and digital signal processing.
The material in this chapter will be complemented by the design of discrete ﬁlters in Chapter 11.
Important issues related to signals and system are illustrated in the design and implementation of
ﬁlters.
6.2 SYSTEM CONNECTIONS AND BLOCK DIAGRAMS
Control and communication systems consist of interconnection of several subsystems. As we
indicated in Chapter 2, there are three important connections of LTI systems:
I
Cascade
I
Parallel
I
Feedback
Cascade and parallel result from properties of the convolution integral, while the feedback con-
nection relates the output of the overall system to its input. With the background of the Laplace
transform we present now a transform characterization of these connections that can be related to
the time-domain characterizations given in Chapter 2.
The connection of two LTI continuous-time systems with transfer functions H1(s) and H2(s) (and correspond-
ing impulse responses h1(t) and h2(t)) can be done in:
I
Cascade (Figure 6.1(a)): Provided that the two systems are isolated, the transfer function of the overall
system is
H(s) = H1(s)H2(s)
(6.1)
I
Parallel (Figure 6.1(b)): The transfer function of the overall system is
H(s) = H1(s) + H2(s)
(6.2)
I
Negative feedback (Figure 6.4): The transfer function of the overall system is
H(s) =
H1(s)
1 + H2(s)H1(s)
(6.3)
I
Open-loop transfer function: Hoℓ(s) = H1(s).
I
Closed-loop transfer function: Hcℓ(s) = H(s).

6.2 System Connections and Block Diagrams
361
Cascading of LTI Systems
Given two LTI systems with transfer functions H1(s) = L[h1(t)] and H2(s) = L[h2(t)] where h1(t) and
h2(t) are the corresponding impulse responses of the systems, the cascading of these systems gives a
new system with transfer function
H(s) = H1(s)H2(s) = H2(s)H1(s)
provided that these systems are isolated from each other (i.e., they do not load each other). A graph-
ical representation of the cascading of two systems is obtained by representing each of the systems
with blocks with their corresponding transfer function (see Figure 6.1(a)). Although cascading of
systems is a simple procedure, it has some disadvantages:
I
It requires isolation of the systems.
I
It causes delay as it processes the input signal, possibly compounding any errors in the processing.
Remarks
I
Loading, or lack of system isolation, needs to be considered when cascading two systems. Loading does
not allow the overall transfer function to be the product of the transfer functions of the connected systems.
Consider the cascade connection of two resistive voltage dividers (Figure 6.2), each with a simple transfer
function Hi(s) = 1/2, i = 1, 2. The cascade in Figure 6.2(b) clearly will not have as transfer function
H(s) = H1(s)H2(s) = (1/2)(1/2) unless we include a buffer (such as an operational ampliﬁer voltage
(a)
(b)
X(s)
Y(s)
y(t)
H1(s)
H2(s)
x(t)
Y(s)
y(t)
H2(s)
H1(s)
X(s)
x(t)
+
FIGURE 6.1
(a) Cascade and (b) parallel connections of systems with transfer function H1(s) and H2(s). The input and output
are given in the time or in the frequency domains.
(a)
(b)
V2(s)
+
−
V0(s)
+
−
1Ω
1Ω
1Ω
1Ω
V0(s)
V1(s)
+
+
−
+
−
−
1Ω
1Ω
1Ω
1Ω
FIGURE 6.2
Cascading of two voltage dividers: (a) using a voltage follower gives V1(s)/V0(s) = (1/2)(1/2) with no loading
effect, and (b) using no voltage follower V2(s)/V0(s) = 1/5 ̸= V1(s)/V0(s) due to loading.

362
CHAPTER 6:
Application to Control and Communications
FIGURE 6.3
Cascading of (a) an LTV and (b) an LTI system.
The outputs are different, y1(t) ̸= y2(t).
Modulator
x(t)
y2(t)
d
dt
Modulator
x(t)
y1(t)
f(t)
f(t)
d
dt
×
×
(a)
(b)
follower) in between (see Figure 6.2(a)). The cascading of the two voltage dividers without the voltage
follower gives a transfer function H1(s) = 1/5, as can be easily shown by doing mesh analysis on the
circuit.
I
The block diagrams of the cascade of two or more LTI systems can be interchanged with no effect on the
overall transfer function, provided the connection is done with no loading. That is not true if the systems
are not LTI. For instance, consider cascading a modulator (LTV system) and a differentiator (LTI) as
shown in Figure 6.3. If the modulator is ﬁrst, Figure 6.3(a), the output of the overall system is
y2(t) = dx(t)f(t)
dt
= f(t)dx(t)
dt
+ x(t)df(t)
dt
while if we put the differentiator ﬁrst, Figure 6.3(b), the output is
y1(t) = f(t)dx(t)
dt
It is obvious that if f(t) is not a constant, the two responses are very different.
Parallel Connection of LTI Systems
According to the distributive property of the convolution integral, the parallel connection of two
or more LTI systems has the same input and its output is the sum of the outputs of the systems
being connected (see Figure 6.1(b)). The parallel connection is better than the cascade, as it does
not require isolation between the systems, and reduces the delay in processing an input signal. The
transfer function of the parallel system is
H(s) = H1(s) + H2(s)
Remarks
I
Although a communication system can be visualized as the cascading of three subsystems—the transmitter,
the channel, and the receiver—typically none of these subsystems is LTI. As we discussed in Chapter 5, the
low-frequency nature of the message signals requires us to use as the transmitter a system that can generate
a signal with much higher frequencies, and that is not possible with LTI systems (recall the eigenfunction
property). Transmitters are thus typically nonlinear or linear time varying. The receiver is also not LTI. A
wireless channel is typically time varying.
I
Some communication systems use parallel connections (see quadrature amplitude modulation (QAM)
later in this chapter). To make it possible for several users to communicate over the same channel, a
combination of parallel and cascade connections are used (see frequency division multiplexing (FDM)
systems later in this chapter). But again, it should be emphasized that these subsystems are not LTI.

6.3 Application to Classic Control
363
FIGURE 6.4
Negative-feedback connection of
systems with transfer function H1(s)
and H2(s). The input and the output
are x(t) and y(t), respectively, and e(t)
is the error signal.
H2(s)
H1(s)
e(t)
y(t)
x(t)
+
−
Feedback Connection of LTI Systems
In control, feedback connections are more appropriate than cascade or parallel connections. In the
feedback connection, the output of the ﬁrst system is fed back through the second system into
the input (see Figure 6.4). In this case, like in the parallel connection, beside the blocks representing
the systems we use adders to add/subtract two signals.
It is possible to have positive- or negative-feedback systems depending on whether we add or subtract the
signal being fed back to the input. Typically, negative feedback is used, as positive feedback can greatly
increase the gain of the system. (Think of the screeching sound created by an open microphone near
a loud-speaker: the microphone continuously picks up the ampliﬁed sound from the loud-speaker,
increasing the volume of the produced signal. This is caused by positive feedback.) For negative
feedback, the connection of two systems is done by putting one in the feedforward loop, H1(s),
and the other in the feedback loop, H2(s) (there are other possible connections). To ﬁnd the overall
transfer function we consider the Laplace transforms of the error signal e(t), E(s), and of the output
y(t), Y(s), in terms of the Laplace transform of the input x(t), X(s), and the transfer functions H1(s)
and H2(s) of the systems:
E(s) = X(s) −H2(s)Y(s)
Y(s) = H1(s)E(s)
Replacing E(s) in the second equation gives
Y(s)[1 + H1(s)H2(s)] = H1(s)X(s)
and the transfer function of the feedback system is then
H(s) = Y(s)
X(s) =
H1(s)
1 + H1(s)H2(s)
(6.4)
As you recall, in Chapter 2 we were not able to ﬁnd an explicit expression for the impulse response
of the overall system and now you can understand why.
6.3 APPLICATION TO CLASSIC CONTROL
Because of different approaches, the theory of control systems can be divided into classic and modern
control. Classic control uses frequency-domain methods, while modern control uses time-domain
methods. In classic linear control, the transfer function of the plant we wish to control is available;

364
CHAPTER 6:
Application to Control and Communications
Hc(s)
e(t)
x(t)
y(t)
η(t)
G(s)
+
+
−
Hc(s)
x(t)
y(t)
η(t)
G(s)
+
(a)
(b)
FIGURE 6.5
(a) Closed- and (b) open-loop control systems. The transfer function of the plant is G(s) and the transfer function
of the controller is Hc(s).
let us call it G(s). The controller, with a transfer function Hc(s), is designed to make the output of
the overall system perform in a speciﬁed way. For instance, in a cruise control the plant is the car,
and the desired performance is to automatically set the speed of the car to a desired value. There are
two possible ways the controller and the plant are connected: in open-loop or in closed-loop (see
Figure 6.5).
Open-Loop Control
In the open-loop approach the controller is cascaded with the plant (Figure 6.5(b)). To make the
output y(t) follow the reference signal at the input x(t), we minimize an error signal
e(t) = y(t) −x(t)
Typically, the output is affected by a disturbance η(t), due to modeling or measurement errors. If we
assume initially no disturbance, η(t) = 0, we ﬁnd that the Laplace transform of the output of the
overall system is
Y(s) = L[y(t)] = Hc(s)G(s)X(s)
and that of the error is
E(s) = Y(s) −X(s) = [Hc(s)G(s) −1]X(s)
To make the error zero, so that y(t) = x(t), it would require that Hc(s) = 1/G(s) or the inverse of the
plant, making the overall transfer function of the system Hc(s)G(s) unity.
Remarks
Although open-loop systems are simple to implement, they have several disadvantages:
I
The controller Hc(s) must cancel the poles and the zeros of G(s) exactly, which is not very practical. In
actual systems, the exact location of poles and zeros is not known due to measurement errors.
I
If the plant G(s) has zeros on the right-hand s-plane, then the controller Hc(s) will be unstable, as its poles
are the zeros of the plant.
I
Due to ambiguity in the modeling of the plant, measurement errors, or simply the presence of noise, the
output y(t) is typically affected by a disturbance signal η(t) mentioned above (η(t) is typically random—
we are going to assume for simplicity that it is deterministic so we can compute its Laplace transform).

6.3 Application to Classic Control
365
The Laplace transform of the overall system output is
Y(s) = Hc(s)G(s)X(s) + η(s)
where η(s) = L[η(t)]. In this case, E(s) is given by
E(s) = [Hc(s)G(s) −1]X(s) + η(s)
Although we can minimize this error by choosing Hc(s) = 1/G(s) as above, in this case e(t) cannot be
made zero—it remains equal to the disturbance η(t) and we have no control over this.
Closed-Loop Control
Assuming y(t) and x(t) in the open-loop control are the same type of signals, (e.g., both are voltages,
or temperatures), if we feed back y(t) and compare it with the input x(t) we obtain a closed-loop
control. Considering the case of negative-feedback system (see Figure 6.5(a)), and assuming no
disturbance (η(t) = 0), we have that
E(s) = X(s) −Y(s)
Y(s) = Hc(s)G(s)E(s)
and replacing Y(s) gives
E(s) =
X(s)
1 + G(s)Hc(s)
If we wish the error to go to zero in the steady state, so that y(t) tracks the input, the poles of E(s)
should be in the open left-hand s-plane.
If a disturbance signal η(t) (consider it for simplicity deterministic and with Laplace transform η(s))
is present (See Figure 6.5(a)), the above analysis becomes
E(s) = X(s) −Y(s)
Y(s) = Hc(s)G(s)E(s) + η(s)
so that
E(s) = X(s) −Hc(s)G(s)E(s) −η(s)
or solving for E(s),
E(s) =
X(s)
1 + G(s)Hc(s) −
η(s)
1 + G(s)Hc(s)
= E1(s) + E2(s)
If we wish e(t) to go to zero in the steady state, then poles of E1(s) and E2(s) should be in the open left-
hand s-plane. Different from the open-loop control, the closed-loop control offers more ﬂexibility in
achieving this by minimizing the effects of the disturbance.

366
CHAPTER 6:
Application to Control and Communications
Remarks
A control system includes two very important components:
I
Transducer: Since it is possible that the output signal y(t) and the reference signal x(t) might not be of
the same type, a transducer is used to change the output so as to be compatible with the reference input.
Simple examples of a transducer are: lightbulbs, which convert voltage into light; a thermocouple, which
converts temperature into voltage.
I
Actuator: A device that makes possible the execution of the control action on the plant, so that the output
of the plant follows the reference input.
I Example 6.1: Controlling an unstable plant
Consider a dc motor modeled as an LTI system with a transfer function
G(s) =
1
s(s + 1)
The motor is not BIBO stable given that its impulse response g(t) = (1 −e−t)u(t) is not absolutely
integrable. We wish the output of the motor y(t) to track a given reference input x(t), and pro-
pose using a so-called proportional controller with transfer Hc(s) = K > 0 to control the motor (see
Figure 6.6). The transfer function of the overall negative-feedback system is
H(s) = Y(s)
X(s) =
KG(s)
1 + KG(s)
Suppose that X(s) = 1/s, or the reference signal is x(t) = u(t). The question is: What should be
the value of K so that in the steady state the output of the system y(t) coincides with x(t)? Or,
equivalently, is the error signal in the steady state zero? We have that the Laplace transform of the
error signal e(t) = x(t) −y(t) is
E(s) = X(s) [1 −H(s)] =
1
s(1 + G(s)K) =
s + 1
s(s + 1) + K
The poles of E(s) are the roots of the polynomial s(s + 1) + K = s2 + s + K, or
s1,2 = −0.5 ± 0.5
√
1 −4K
For 0 < K ≤0.25 the roots are real, and complex for K > 0.25, and in either case in the left-hand
s-plane. The partial fraction expansion corresponding to E(s) would be
E(s) =
B1
s −s1
+
B2
s −s2
FIGURE 6.6
Proportional control of a motor.
K
e(t)
x(t)
y(t)
+
−
Proportional
controller
Motor
G(s)=
1
s(s +1)

6.3 Application to Classic Control
367
for some values B1 and B2. Given that the real parts of s1 and s2 are negative, their corresponding
inverse Laplace terms will have a zero steady-state response. Thus,
lim
t→∞e(t) →0
This can be found also with the ﬁnal value theorem, e(0) is
sE(s)|s=0 = 0
So for any K > 0, y(t) →x(t) in steady state.
Suppose then that X(s) = 1/s2 or that x(t) = tu(t), a ramp signal. Intuitively, this is a much harder
situation to control, as the output needs to be continuously growing to try to follow the input. In
this case, the Laplace transform of the error signal is
E(s) =
1
s2(1 + G(s)K) =
s + 1
s(s(s + 1) + K)
In this case, even if we choose K to make the roots of s(s + 1) + K be in the left-hand s-plane, we
have a pole at s = 0. Thus, in the steady state, the partial fraction expansion terms corresponding
to poles s1 and s2 will give a zero steady-state response, but the pole s = 0 will give a constant
steady-state response A where
A = E(s)s|s=0 = 1/K
In the case of a ramp as input, it is not possible to make the output follow exactly the input
command, although by choosing a very large gain K we can get them to be very close.
I
Choosing the values of the gain K of the open-loop transfer function
G(s)Hc(s) = KN(s)
D(s)
to be such that the roots of
1 + G(s)Hc(s) = 0
are in the open left-hand s-plane, is the root-locus method, which is of great interest in control theory.
I Example 6.2: A cruise control
Suppose we are interested in controlling the speed of a car or in obtaining a cruise control. How to
choose the appropriate controller is not clear. We consider initially a proportional plus integral (PI)
controller Hc(s) = 1 + 1/s and will ask you to consider the proportional controller as an exercise.
See Figure 6.7.
Suppose we want to keep the speed of the car at V0 miles/hour for t ≥0 (i.e., x(t) = V0u(t)), and
that the model for a car in motion is a system with transfer function
Hp(s) = β/(s + α)

368
CHAPTER 6:
Application to Control and Communications
FIGURE 6.7
Cruise control system: reference speed
x(t) = V0u(t) and output speed of car v(t).
Hp(s)
e(t)
c(t)
x(t)
v(t)
+
−
Pl Controller
Plant
Hc(s)= 1 + 1
s
with both β and α positive values related to the mass of the car and the friction coefﬁcient. For
simplicity, let α = β = 1. The question is: Can this be achieved with the PI controller? The Laplace
transform of the output speed v(t) of the car is
V(s) =
Hc(s)Hp(s)
1 + Hc(s)Hp(s)X(s)
=
V0(s + 1)
s(s2 + 2s + 1) =
V0
s(s + 1)
The poles of V(s) are s = 0 and s = −1 on the left-hand s-plane. We can then write V(s) as
V(s) =
B
s + 1 + A
s
where A = V0. The steady-state response is
lim
t→∞v(t) = V0
since the inverse Laplace transform of the ﬁrst term goes to zero due to its poles being in the left-
hand s-plane. The error signal e(t) = x(t) −v(t) in the steady state is zero. The controlling signal
c(t) (see Figure 6.7) that changes the speed of the car is
c(t) = e(t) +
t
Z
0
e(τ)dτ
so that even if the error signal becomes zero at some point—indicating the desired speed had been
reached—the value of c(t) is not necessarily zero. The values of e(t) at t = 0 and at steady–state can
be obtained using the initial- and the ﬁnal-value theorems of the Laplace transform applied to
E(s) = X(s) −V(s) = V0
s

1 −
1
s + 1

The ﬁnal-value theorem gives that the steady-state error is
lim
t→∞e(t) = lim
s→0 sE(s) = 0
coinciding with our previous result. The initial value is found as
e(0) = lim
s→∞sE(s)
= lim
s→∞V0

1 −
1/s
1 + 1/s

= V0

6.3 Application to Classic Control
369
The PI controller used here is one of various possible controllers. Consider a simpler and cheaper
controller such as a proportional controller with Hc(s) = K. Would you be able to obtain the same
results? Try it.
I
6.3.1 Stability and Stabilization
A very important question related to the performance of systems is: How do we know that a given
causal system has ﬁnite zero-input, zero-state, or steady-state responses? This is the stability prob-
lem of great interest in control. Thus, if the system is represented by a linear differential equation
with constant coefﬁcients the stability of the system determines that the zero-input, the zero-state,
as well as the steady-state responses may exist. The stability of the system is also required when con-
sidering the frequency response in the Fourier analysis. It is important to understand that only the
Laplace transform allows us to characterize stable as well as unstable systems; the Fourier transform
does not.
Two possible ways to look at the stability of a causal LTI system are:
I
When there is no input so that the response of the system depends on initial energy in the system.
This is related to the zero-input response of the system.
I
When there is a bounded input and no initial condition. This is related to the zero-state response
of the system.
Relating the zero-input response of a causal LTI system to stability leads to asymptotic stability. An LTI
system is said to be asymptotically stable if the zero-input response (due only to initial conditions in
the system) goes to zero as t increases—that is,
yzi(t) →0
t →∞
(6.5)
for all possible initial conditions.
The second interpretation leads to the bounded-input bounded-output (BIBO) stability, which we
deﬁned in Chapter 2. A causal LTI system is BIBO stable if its response to a bounded input is also
bounded. The condition we found in Chapter 2 for a causal LTI system to be BIBO stable was that
the impulse response of the system be absolutely integrable—that is
∞
Z
0
|h(t)|dt < ∞
(6.6)
Such a condition is difﬁcult to test, and we will see in this section that it is equivalent to the poles
of the transfer function being in the open left-hand s-plane, a condition that can be more easily
visualized and for which algebraic tests exist.
Consider a system being represented by the differential equation
y(t) +
N
X
k=1
ak
dky(t)
dtk
= b0x(t) +
M
X
ℓ=1
bℓ
dℓx(t)
dtℓ
M < N

370
CHAPTER 6:
Application to Control and Communications
For some initial conditions and input x(t), with Laplace transform X(s), we have that the Laplace
transform of the output is
Y(s) = Yzi(s) + Yzs(s) = L[y(t)] = I(s)
A(s) + X(s)B(s)
A(s)
A(s) = 1 +
N
X
k=1
aksk, B(s) = b0 +
M
X
m=1
bmsm
where I(s) is due to the initial conditions. To ﬁnd the poles of H1(s) = 1/A(s), we set A(s) = 0,
which corresponds to the characteristic equation of the system and its roots (real, complex conjugate,
simple, and multiple) are the natural modes or eigenvalues of the system.
A causal LTI system with transfer function H(s) = B(s)/A(s) exhibiting no pole-zero cancellation is said
to be:
I
Asymptotically stable if the all-pole transfer function H1(s) = 1/A(s), used to determine the zero-input
response, has all its poles in the open left-hand s-plane (the j axis excluded), or equivalently
A(s) ̸= 0
for
Re[s] ≥0
(6.7)
I
BIBO stable if all the poles of H(s) are in the open left-hand s-plane (the j axis excluded), or equivalently
A(s) ̸= 0
for
Re[s] ≥0
(6.8)
I
If H(s) exhibits pole-zero cancellations, the system can be BIBO stable but not necessarily asymptotically
stable.
Testing the stability of a causal LTI system thus requires ﬁnding the location of the roots of A(s), or the
poles of the system. This can be done for low-order polynomials A(s) for which there are formulas to
ﬁnd the roots of a polynomial exactly. But as shown by Abel,1 there are no equations to ﬁnd the roots
of higher than fourth-order polynomials. Numerical methods to ﬁnd roots of these polynomials only
provide approximate results that might not be good enough for cases where the poles are close to
the j axis. The Routh stability criterion [53] is an algebraic test capable of determining whether the
roots of A(s) are on the left-hand s-plane or not, thus determining the stability of the system.
I Example 6.3: Stabilization of a plant
Consider a plant with a transfer function G(s) = 1/(s −2), which has a pole in the right-hand
s-plane and therefore is unstable. Let us consider stabilizing it by cascading it with an all-pass ﬁlter
(Figure 6.8(a)) so that the overall system is not only stable but also keeps its magnitude response.
1Niels H. Abel (1802–1829) was a Norwegian mathematician who accomplished brilliant work in his short lifetime. At age 19, he
showed there is no general algebraic solution for the roots of equations of degree greater than four, in terms of explicit algebraic
operations.

6.3 Application to Classic Control
371
Ha(s)
x(t)
y(t)
G(s)
K
x(t)
y(t)
G(s)
+
−
(a)
(b)
FIGURE 6.8
Stabilization of an unstable plant G(s) using (a) an all-pass ﬁlter and (b) a proportional controller of gain K.
To get rid of the pole at s = 2 and to replace it with a new pole at s = −2, we let the all-pass ﬁlter be
Ha(s) = s −2
s + 2
To see that this ﬁlter has a constant magnitude response consider
Ha(s)Ha(−s) = (s −2)(−s −2)
(s + 2)(−s + 2)
= (s −2)(s + 2)
(s + 2)(s −2) = 1
If we let s = j, the above gives the magnitude-squared function
Ha( j)H(−j) = Ha( j)H∗( j)
= |H( j)|2
which is unity for all values of frequency. The cascade of the unstable system with the all-pass
system gives a stable system
H(s) = G(s)Ha(s) =
1
s + 2
with the same magnitude response as G(s). This is an open-loop stabilization and it depends on
the all-pass system having a zero exactly at 2 so that it cancels the pole causing the instability. Any
small change on the zero and the overall system would not be stabilized. Another problem with
the cascading of an all-pass ﬁlter to stabilize a ﬁlter is that it does not work when the pole causing
the unstability is at the origin, as we cannot obtain an all-pass ﬁlter able to cancel that pole.
Consider then a negative-feedback system (Figure 6.8(b)). Suppose we use a proportional
controller with a gain K, then the overall system transfer function is
H(s) =
KG(s)
1 + KG(s) =
K
s + (K −2)
and if the gain K is chosen so that K −2 > 0 or K > 2, the feedback system will be stable.
I
6.3.2 Transient Analysis of First- and Second-Order Control Systems
Although the input to a control system is not known a-priori, there are many applications where the
system is frequently subjected to a certain type of input and thus one can select a test signal. For
instance, if a system is subjected to intense and sudden inputs, then an impulse signal might be the

372
CHAPTER 6:
Application to Control and Communications
appropriate test input for the system; if the input applied to a system is constant or continuously
increasing, then a unit step or a ramp signal would be appropriate. Using test signals such as an
impulse, a unit-step, a ramp, or a sinusoid, mathematical and experimental analyses of systems can
be done.
When designing a control system its stability becomes its most important attribute, but there are other
system characteristics that need to be considered. The transient behavior of the system, for instance,
needs to be stressed in the design. Typically, as we drive the system to reach a desired response, the
system’s response goes through a transient before reaching the desired response. Thus, how fast the
system responds and what steady-state error it reaches need to be part of the design considerations.
First-Order Systems
As an example of a ﬁrst-order system consider an RC serial circuit with a voltage source vi(t) = u(t) as
input (Figure 6.9), and as the output the voltage across the capacitor, vc(t). By voltage division, the
transfer function of the circuit is
H(s) = Vc(s)
Vi(s) =
1
1 + RCs
Considering the RC circuit, a feedback system with input vi(t) and output vc(t), the feedforward
transfer function G(s) in Figure 6.9 is 1/RCs. Indeed, from the feedback system we have
E(s) = Vi(s) −Vc(s)
Vc(s) = E(s)G(s)
Replacing E(s) in the second of the above equations, we have that
Vc(s)
Vi(s) =
G(s)
1 + G(s) =
1
1 + 1/G(s)
so that the open-loop transfer function, when we compare the above equation to H(s), is
G(s) =
1
RCs
The RC circuit can be seen as a feedback system: the voltage across the capacitor is constantly com-
pared with the input voltage, and if found smaller, the capacitor continues charging until its voltage
coincides with it. How fast depends on the RC value.
FIGURE 6.9
Feedback modeling of an RC circuit in series.
e(t)
vi(t)
vi(t)
vc(t)
vc(t)
+ −
G(s) =
1
RCs
+
+
−
−
R
C

6.3 Application to Classic Control
373
−1
−0.8
−0.6
−0.4
−0.2
0
(a)
(b)
−0.5
0
0.5
σ
jΩ
RC= 1
RC= 10
0
10
20
30
40
50
0
0.2
0.4
0.6
0.8
1
t
vc(t)
RC =10
RC= 1
FIGURE 6.10
(a) Clustering of poles and (b) time responses of a ﬁrst-order feedback system for 1 ≤RC ≤10.
For vi(t) = u(t), so that Vi(s) = 1/s, then the Laplace transform of the output is
Vc(s) =
1
s(sRC + 1) =
1/RC
s(s + 1/RC) = 1
s −
1
s + 1/RC
so that
vc(t) = (1 −e−t/RC)u(t)
The following MATLAB script plots the poles Vc(s)/Vi(s) and simulates the transients of vc(t) for
1 ≤RC ≤10, shown in Figure 6.10. Thus, if we wish the system to respond fast to the unit-step input
we locate the system pole far from the origin.
%%%%%%%%%%%%%%%%%%%%%
% Transient analysis
%%%%%%%%%%%%%%%%%%%%%
clf; clear all
syms s t
num = [0 1];
for RC = 1:2:10,
den = [ RC 1];
ﬁgure(1)
splane(num, den) % plotting of poles and zeros
hold on
vc = ilaplace(1/(RC ∗s ˆ 2 + s)) % inverse Laplace
ﬁgure(2)
ezplot(vc, [0, 50]); axis([0 50 0 1.2]); grid
hold on
end
hold off

374
CHAPTER 6:
Application to Control and Communications
Second-Order System
A series RLC circuit with the input a voltage source, vs(t), and the output the voltage across the
capacitor, vc(t), has a transfer function
Vc(s)
Vs(s) =
1/Cs
R + Ls + 1/Cs =
1/LC
s2 + (R/L)s + 1/LC
If we deﬁne
Natural frequency: n =
1
√
CL
(6.9)
Damping ratio: ψ = 0.5R
r
C
L
(6.10)
we can write
Vc(s)
Vs(s) =
2
n
s2 + 2ψns + 2n
(6.11)
A feedback system with this transfer function is given in Figure 6.11 where the feedforward transfer
function is
G(s) =
2
n
s(s + 2ψn)
Indeed, the transfer function of the feedback system is given by
H(s) = Vc(s)
Vs(s) =
G(s)
1 + G(s)
=
2
n
s2 + 2ψns + 2n
The dynamics of a second-order system can be described in terms of the parameters n and ψ, as these
two parameters determine the location of the poles of the system and thus its response. We adapted
the previously given script to plot the cluster of poles and the time response of the second-order
system.
Assume n = 1 rad/sec and let 0 ≤ψ ≤1 (so that the poles of H(s) are complex conjugate for 0 ≤
ψ < 1 and double real for ψ = 1). Let the input be a unit-step signal so that Vs(s) = 1/s. We then
have:
FIGURE 6.11
Second-order feedback system.
vs(t)
e(t)
G(s)
vc(t)
−
+

6.3 Application to Classic Control
375
−1
−0.5
0
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
σ
jΩ
(a)
0
10
20
30
40
50
0
0.5
1
1.5
2
0
10
20
30
40
50
0
0.2
0.4
0.6
0.8
1
t
t
vc(t)
vc(t)
(b)
(c)
FIGURE 6.12
(a) Clustering of poles and time responses vc(t) of second-order feedback system for (b)
√
2/2 ≤ψ ≤1 and
(c) 0 ≤ψ ≤
√
2/2.
(a)
If we plot the poles of H(s) as ψ changes from 0 (poles in j axis) to 1 (double real poles) the
response y(t) in the steady state changes from a sinusoid shifted up by 1 to a damped signal. The
locus of the poles is a semicircle of radius n = 1. Figure 6.12 shows this behavior of the poles
and the responses.
(b)
As in the ﬁrst-order system, the location of the poles determines the response of the system. The
system is useless if the poles are on the j axis, as the response is completely oscillatory and
the input will never be followed. On the other extreme, the response of the system is slow when
the poles become real. The designer would have to choose a value in between these two for ψ.
(c)
For values of ψ between
√
2/2 to 1 the oscillation is minimum and the response is relatively
fast (see Figure 6.12(b)). For values of ψ from 0 to
√
2/2 the response oscillates more and more,
giving a large steady-state error (see Figure 6.12(c)).
I Example 6.4
In this example we ﬁnd the response of an LTI system to different inputs by using functions in the
control toolbox of MATLAB. You can learn more about the capabilities of this toolbox, or set of
specialized functions for control, by running the demo respdemo and then using help to learn more
about the functions tf, impulse, step, and pzmap, which we will use here.

376
CHAPTER 6:
Application to Control and Communications
We want to create a MATLAB function that has as inputs the coefﬁcients of the numerator N(s)
and of the denominator D(s) of the system’s transfer function H(s) = N(s)/D(s) (the coefﬁcients
are ordered from the highest order to the lowest order or constant term). The other input of the
function is the type of response t where t = 1 corresponds to the impulse response, t = 2 to the
unit-step response, and t = 3 to the response to a ramp. The output of the function is the desired
response. The function should show the transfer function, the poles, and zeros, and plot the cor-
responding response. We need to ﬁgure out how to compute the ramp response using the step
function.
Consider the following transfer functions:
(a) H1(s) =
s + 1
s2 + s + 1
(b) H2(s) =
s
s3 + s2 + s + 1
Determine the stability of these systems.
Solution
The following script is used to look at the desired responses of the two systems and the location
of their poles and zeros. We consider the second system; you can run the script for the ﬁrst system
by putting % at the numerator and the denominator after H2(s) and getting rid of % after H1(s)
in the script. The function response computes the desired responses (in this case the impulse, step,
and ramp responses).
%%%%%%%%%%%%%%%%%%%
% Example 6.4 -- Control toolbox
%%%%%%%%%%%%%%%%%%%
clear all; clf
% % H 1(s)
% nu = [1 1]; de = [1 1 1];
%% H 2(s)
nu = [1 0]; de = [1 1 1 1]; % unstable
h = response(nu, de, 1);
s = response(nu, de, 2);
r = response(nu, de, 3);
function y = response(N, D, t)
sys = tf(N, D)
poles = roots(D)
zeros = roots(N)
ﬁgure(1)
pzmap(sys);grid
if t == 3,
D1 = [D 0]; % for ramp response
end

6.4 Application to Communications
377
ﬁgure(2)
if t == 1,
subplot(311)
y = impulse(sys,20);
plot(y);title(’ Impulse response’);ylabel(’h(t)’);xlabel(’t’); grid
elseif t == 2,
subplot(312)
y = step(sys, 20);
plot(y);title(’ Unit-step response’);ylabel(’s(t)’);xlabe(’t’);grid
else
subplot(313)
sys = tf(N, D1); % ramp response
y = step(sys, 40);
plot(y); title(’ Ramp response’); ylabel(’q(t)’); xlabel(’t’);grid
end
The results for H2(s) are as follows.
Transfer function:
s
-----------------------
sˆ3 + sˆ2 + s + 1
poles =
−1.0000
−0.0000 + 1.0000i
−0.0000 −1.0000i
zeros =
0
As you can see, two of the poles are on the j axis, and so the system corresponding to H2(s) is
unstable. The other system is stable. Results for both systems are shown in Figure 6.13.
I
6.4 APPLICATION TO COMMUNICATIONS
The application of the Fourier transform in communications is clear. The representation of signals in
the frequency domain and the concept of modulation are basic in communications. In this section we
show examples of linear (amplitude modulation or AM) as well as nonlinear (frequency modulation
or FM, and phase modulation or PM) modulation methods. We also consider important extensions
such as frequency-division multiplexing (FDM) and quadrature amplitude modulation (QAM).
Given the low-pass nature of most message signals, it is necessary to shift in frequency the spectrum
of the message to avoid using a very large antenna. This can be attained by means of modulation,
which is done by changing either the magnitude or the phase of a carrier:
A(t) cos(2πfc + θ(t))
(6.12)

378
CHAPTER 6:
Application to Control and Communications
(b)
(a)
0
0.5
−0.5
1
Impulse Response
h(t)
0
10
20
30
40
50
60
70
80
90
100
t
0
10
20
30
40
50
60
70
80
90
100
t
0
10
20
30
40
50
60
70
80
90
100
t
0
1
2
Unit−Step Response
s(t)
0
20
40
Ramp Response
q(t)
Pole−Zero Map
Real Axis
Imaginary Axis
−1
−0.8
−0.6
−0.4
−0.2
0
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0.72
0 86
0.96
0.1
0.22
0 32
0.44
0.58
0.72
0.86
0.96
0.2
0.4
0.6
0.8
1
0.1
0.22
0.32
0.44
0.58
0
10
20
30
40
50
60
70
80
90
100
t
0
10
20
30
40
50
60
70
80
90
100
t
0
10
20
30
40
50
60
70
80
90
100
t
−1
0
1
Impulse Response
h(t)
−1
0
1
Unit−Step Response
s(t)
0
1
2
Ramp Response
q(t)
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0.95
0 1
0 2
0.32
0.44
0.56
0.7
0.84
0.95
0.2
0.4
0.6
0.8
1
1.2
0.1
0.2
0.32
0.44
0.56
0.7
0 84
Pole−Zero Map
Real Axis
Imaginary Axis
System: sys
Pole : −0.5 − 0.866i
Damping: 0.5
Overshoot (%): 16.3
Frequency (rad/sec): 1
FIGURE 6.13
Impulse, unit-step, and ramp responses and poles and zeros for system with transfer function (a) H1(s) and
(b) H2(s).
When A(t) is proportional to the message, for constant phase, we have amplitude modulation (AM).
On the other hand, if we let θ(t) change with the message, keeping the amplitude constant, we then
have frequency modulation (FM) or phase modulation (PM), which are called angle modulations.

6.4 Application to Communications
379
6.4.1 AM with Suppressed Carrier
Consider a message signal m(t) (e.g., voice or music, or a combination of the two) modulating a
cosine carrier cos(ct) to give an amplitude modulated signal
s(t) = m(t) cos(ct)
(6.13)
The carrier frequency c >> 2πf0 where f0 (Hz) is the maximum frequency in the message (for music
f0 is about 22 KHz). The signal s(t) is called an amplitude modulated with suppressed carrier (AM-SC)
signal (the last part of this denomination will become clear later). According to the modulation
property of the Fourier transform, the transform of s(t) is
S() = 1
2 [M( −c) + M( + c)]
(6.14)
where M() is the spectrum of the message. The frequency content of the message is now shifted to
a much larger frequency c (rad/sec) than that of the baseband signal m(t). Accordingly, the antenna
needed to transmit the amplitude modulated signal is of reasonable length. An AM-SC system is
shown in Figure 6.14.
At the receiver, we need to ﬁrst detect the desired signal among the many signals transmitted by
several sources. This is possible with a tunable band-pass ﬁlter that selects the desired signal and
rejects the others. Suppose that the signal obtained by the receiver, after the band-pass ﬁltering, is
exactly s(t)—we then need to demodulate this signal to get the original message signal m(t). This is
done by multiplying s(t) by a cosine of exactly the same frequency of the carrier in the transmitter
(i.e., c), which will give r(t) = 2s(t) cos(ct), which again according to the modulation property has
a Fourier transform
R() = S( −c) + S( + c) = M() + 1
2 [M( −2c) + M( + 2c)]
(6.15)
The spectrum of the message, M(), is obtained by passing the received signal r(t) through a low-pass
ﬁlter that rejects the other terms M( ± 2c). The obtained signal is the desired message m(t).
The above is a simpliﬁcation of the actual processing of the received signal. Besides the many other
transmitted signals that the receiver encounters, there is channel noise caused by interferences from
×
×
m(t)
m(t)
cos(Ωct)
2cos(Ωct)
Channel
Band-pass
filter
Low-pass
filter
ˆ
Transmitter
Receiver
FIGURE 6.14
AM-SC transmitter, channel, and receiver.

380
CHAPTER 6:
Application to Control and Communications
equipment in the transmission path and interference from other signals being transmitted around
the carrier frequency. This noise will also be picked up by the band-pass ﬁlter and a perfect recovery
of m(t) will not be possible. Furthermore, the sent signal has no indication of the carrier frequency
c, which is suppressed in the sent signal, and so the receiver needs to guess it and any deviation
would give errors.
Remarks
I
The transmitter is linear but time varying. AM-SC is thus called a linear modulation. The fact that the
modulated signal displays frequencies much higher than those in the message indicates the transmitter is
not LTI—otherwise it would satisfy the eigenfunction property.
I
A more general characterization than c >> 2πf0 where f0 is the largest frequency in the message is given
by c >> BW where BW (rad/sec) is the bandwidth of the message. You probably recall the deﬁnition
of bandwidth of ﬁlters used in circuit theory. In communications there are several possible deﬁnitions for
bandwidth. The bandwidth of a signal is the width of the range of positive frequencies for which some
measure of the spectral content is satisﬁed. For instance, two possible deﬁnitions are:
I The half-power or 3-dB bandwidth is the width of the range of positive frequencies where a peak
value at zero or inﬁnite frequency (low-pass and high-pass signals) or at a center frequency (band-pass
signals) is attenuated to 0.707, the value at the peak. This corresponds to the frequencies for which the
power at dc, inﬁnity, or center frequency reduces to half.
I The null-to-null bandwidth determines the width of the range of positive frequencies of the spectrum
of a signal that has a main lobe containing a signiﬁcant part of the energy of the signal. If a low-pass
signal has a clearly deﬁned maximum frequency, then the bandwidth are frequencies from zero to the
maximum frequency, and if the signal is a band-pass signal and has a minimum and a maximum
frequency, its bandwidth is the maximum minus the minimum frequency.
I
In AM-SC demodulation it is important to know exactly the carrier frequency. Any small deviation would
cause errors when recovering the message. Suppose, for instance, that there is a small error in the carrier
frequency—that is, instead of c the demodulator uses c + 1—so that the received signal in that case
has the Fourier transform
˜R() = S( −c −1) + S( + c + 1)
= 1
2 [M( + 1) + M( −1)]
+ 1
2 [M( −2(c + 1/2)) + M( + 2(c + 1/2)]
The low-pass ﬁltered signal will not be the message.
6.4.2 Commercial AM
In commercial broadcasting, the carrier is added to the AM signal so that information of the carrier
is available at the receiver helping in the identiﬁcation of the radio station. For demodulation, such
information is not important, as commercial AM uses envelope detectors to obtain the message. By
making the envelope of the modulated signal look like the message, detecting this envelope is all

6.4 Application to Communications
381
that is needed. Thus, the commercial AM signal is of the form
s(t) = [K + m(t)] cos(ct)
where the AM modulation index K is chosen so that K + m(t) > 0 for all values of t so that the
envelope of s(t) is proportional to the message m(t). The Fourier transform is given by
S() = Kπ [δ( −c) + δ( + c)] + 1
2 [M( −c) + M( + c)]
The receiver for this type of AM is an envelope receiver, which basically detects the message by ﬁnding
the envelope of the received signal.
Remarks
I
The advantage of adding the carrier to the message, which allows the use of a simple envelope detector,
comes at the expense of increasing the power in the transmitted signal.
I
The demodulation in commercial AM is called noncoherent. Coherent demodulation consists in
multiplying—or mixing—the received signal with a sinusoid of the same frequency and phase of the
carrier. A local oscillator generates this sinusoid.
I
A disadvantage of commercial as well as suppressed-carrier AM is the doubling of the bandwidth of the
transmitted signal compared to the bandwidth of the message. Given the symmetry of the spectrum, in
magnitude as well as in phase, it becomes clear that it is not necessary to send the upper and the lower
sidebands of the spectrum to get back the signal in the demodulation. It is thus possible to have upper- and
lower-sideband AM modulations, which are more efﬁcient in spectrum utilization.
I
Most AM receivers use the superheterodyne receiver technique developed by Armstrong and Fessenden.2
I Example 6.5: Simulation of AM modulation with MATLAB
For simulations, MATLAB provides different data ﬁles, such as “train.mat” (the extension mat indi-
cates it is a data ﬁle) used here. Suppose the analog signal y(t) is a recording of a “choo-choo” train,
and we wish to use it to modulate a cosine cos(ct) to create an amplitude modulated signal z(t).
Because the train y(t) signal is given in a sampled form, the simulation requires discrete-time pro-
cessing, and so we will comment on the results here and leave the discussion of the issues related
to the code for the next chapters.
The carrier frequency is chosen to be fc = 20.48 KHz. For the envelope detector to work at the
transmitter we add a constant K to the message to ensure this sum is positive. The envelope of the
AM-modulated signal should resemble the message. Thus, the AM signal is
z(t) = [K + y(t)] cos(ct)
c = 2πfc
In Figure 6.15 we show the train signal, a segment of the signal, and the corresponding modulated
signal displaying the envelope, as well as the Fourier transform of the segment and of its modulated
2Reginald Fessenden was the ﬁrst to suggest the heterodyne principle: mixing the radio-frequency signal using a local oscillator of
different frequency, resulting in a signal that could drive the diaphragm of an earpiece at an audio frequency. Fessenden could not
make a practical success of the heterodyne receiver, which was accomplished by Edwin H. Armstrong in the 1920s using electron tubes.

382
CHAPTER 6:
Application to Control and Communications
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
(a)
t (sec)
y(t)
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
×104
0
50
100
150
(b)
(c)
f (Hz)
−2.5
−2
−1.5 −1
−0.5
0
0.5
1
1.5
2
2.5
×104
f (Hz)
|Z(Ω)|
|Y(Ω)|
0
100
200
300
400
500
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
−1
−0.5
0
0.5
1
y(t)
t (sec)
−2
−1
0
1
2
z(t)
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
t (sec)
FIGURE 6.15
Commercial AM modulation: (a) original signal, (b) part of original signal and corresponding AM-modulated
signal, and (c) spectrum of the original signal, and of the modulated signal.
version. Notice that the envelope resembles the original signal. Also from the spectrum of the
segment of the train signal its bandwidth is about 5 Khz, while the spectrum of the modulated
segment displays the frequency-shifted spectrum plus the large spectrum at fc corresponding to the
carrier.
I
6.4.3 AM Single Sideband
The message m(t) is typically a real-valued signal that, as indicated before, has a symmetric
spectrum—that is, the magnitude and the phase of the Fourier transform M() are even and odd

6.4 Application to Communications
383
FIGURE 6.16
Upper sideband AM transmitter. c is
the carrier frequency and B is the
bandwidth in rad/sec of the message.
×
BPF
m(t)
cos(Ωct)
Ωc
Ωc+B
Ω
1
s(t)
H( jΩ)
functions of frequency. When using AM modulation the resulting spectrum has redundant informa-
tion by providing the upper and the lower sidebands. To reduce the bandwidth of the transmitted
signal, we could get rid of either the upper or the lower sideband of the AM signal. The resulting
modulation is called AM single sideband (AM-SSB) (upper or lower sideband depending on which
of the two sidebands is kept). This type of modulation is used whenever the quality of the received
signal is not as important as the advantages of a narrowband and having less noise in the frequency
band of the received signal. AM-SSB is used by amateur radio operators.
As shown in Figure 6.16, the upper sideband modulated signal is obtained by band-pass ﬁltering the
upper sideband in the modulated signal. At the receiver, band-pass ﬁltering the received signal the
output is then demodulated like in an AM-SC system, and the result is then low-pass ﬁltered using
the bandwidth of the message.
6.4.4 Quadrature AM and Frequency-Division Multiplexing
Quadrature amplitude modulation (QAM) and frequency division multiplexing (FDM) are the pre-
cursors of many of the new communication systems. QAM and FDM are of great interest for their
efﬁcient use of the radio spectrum.
Quadrature Amplitude Modulation
QAM enables two AM-SC signals to be transmitted over the same frequency band, conserving band-
width. The messages can be separated at the receiver. This is accomplished by using two orthogonal
carriers, such as a cosine and a sine (see Figure 6.17). The QAM-modulated signal is given by
s(t) = m1(t) cos(ct) + m2(t) sin(ct)
(6.16)
where m1(t) and m2(t) are the messages. You can think of s(t) as having a phasor representation that
is the sum of two phasors perpendicular to each other (the cosine leading the sine by π/2); indeed,
s(t) = Re[(m1(t)ej0 + m2(t)e−jπ/2)ejct].
Since
m1(t)ej0 + m2(t)e−jπ/2 = m1(t) −jm2(t)
we could interpret the QAM signal as the result of amplitude modulating the real and the imaginary
parts of a complex message m(t) = m1(t) −jm2(t).

384
CHAPTER 6:
Application to Control and Communications
m1(t)
m2(t)
m2(t)
s(t) r(t)
cos(Ωct)
×
+
LPF
LPF
π
2 shift
π
2 shift
˜
m1(t)
˜
Transmitter
Receiver
cos(Ωct)
×
×
×
FIGURE 6.17
QAM transmitter and receiver: s(t) is the transmitted signal and r(t) is the received signal.
To simplify the computation of the spectrum of s(t), let us consider the message m(t) = m1(t) −jm2(t)
(i.e., a complex message) with spectrum M() = M1() −jM2() so that
s(t) = Re[m(t)ejct]
= 0.5[m(t)ejct + m∗(t)e−jct]
where ∗stands for complex conjugate. The spectrum of s(t) is then given by
S() = 0.5[M( −c) + M∗( + c)]
= 0.5[M1( −c) −jM2( −c) + M∗
1( + c) + jM∗
2( + c)]
where the superposition of the spectra of the two messages is clearly seen. At the receiver, if we
multiply the received signal (for simplicity assume it to be s(t)) by cos(ct), we get
r1(t) = s(t) cos(ct)
= 0.25[m(t) + m∗(t)] + 0.25[m(t)ej2ct + m∗(t)e−j2ct]
which when passed through a low-pass ﬁlter, with the appropriate bandwidth, gives
0.25[m(t) + m∗(t)] = 0.25[m1(t) −jm2(t) + m1(t) + jm2(t)]
= 0.5m1(t)
Likewise, to get the second message we multiply s(t) by sin(ct) and pass the resulting signal through
a low-pass ﬁlter.
Frequency-Division Multiplexing
Frequency-division multiplexing (FDM) implements sharing of the spectrum by several users by allo-
cating a speciﬁc frequency band to each. One could, for instance, think of the commercial AM or FM

6.4 Application to Communications
385
Channel
+
m1(t)
m2(t)
×
×
×
cos(Ω1t)
cos(Ω2t)
cos(Ω3t)
cos(Ω1t)
cos(Ω2t)
cos(Ω3t)
m3(t)
BPF
×
LPF
m1(t)
∧
m2(t)
∧
m3(t)
∧
BPF
×
LPF
BPF
×
LPF
FIGURE 6.18
FDM system: transmitter (left), channel, and receiver (right).
locally as an FDM sytem. In the United States, the Federal Communication Commission (FCC) is in
charge of the spectral allocation. In telephony, using a bank of ﬁlters it is possible to also get several
users in the same system—it is, however, necessary to have a similar system at the receiver to have a
two-way communication.
To illustrate an FDM system (Figure 6.18), consider we have a set of messages of known ﬁnite band-
width (we could low-pass ﬁlter the messages to satisfy this condition) that we wish to transmit. Each
of the messages modulate different carriers so that the modulated signals are in different frequency
bands without interfering with each other (if needed a frequency guard could be used to make sure
of this). These frequency-multiplexed messages can now be transmitted. At the receiver, using a bank
of band-pass ﬁlters centered at the carrier frequencies in the transmitter and followed by appropri-
ate demodulators recover the different messages (see FDM receiver in Figure 6.18). Any of the AM
modulation techniques could be used in the FDM system.
6.4.5 Angle Modulation
Amplitude modulation is said to be linear modulation, because as a system it behaves like a linear
system. Frequency and phase, or angle, modulation systems on the other hand are nonlinear. The
interest in angle modulation is due to the decreasing effect of noise or interferences on it, when
compared with AM, although at the cost of a much wider bandwidth and greater complexity in
implementation. The nonlinear behavior of angle modulation systems makes their analysis more
difﬁcult than that for AM. The spectrum of an FM or PM signal is much harder to obtain than that
of an AM signal. In the following we consider the case of the so-called narrowband FM where we are
able to ﬁnd its spectrum directly.
Professor Edwin H. Armstrong developed the ﬁrst successful frequency modulation system—
narrowband FM.3 If m(t) is the message signal, and we modulate a carrier signal of frequency
3Edwind H. Armstrong (1890–1954), professor of electrical engineering at Columbia University, and inventor of some of the basic
electronic circuits underlying all modern radio, radar, and television, was born in New York. His inventions and developments form
the backbone of radio communications as we know it.

386
CHAPTER 6:
Application to Control and Communications
c (rad/sec) with m(t), the transmitted signal s(t) in angle modulation is of the form
s(t) = A cos(ct + θ(t))
(6.17)
where the angle θ(t) depends on the message m(t). In the case of phase modulation, the angle function
is proportional to the message m(t)—that is,
θ(t) = Kf m(t)
(6.18)
where Kf > 0 is called the modulation index. If the angle is such that
dθ(t)
dt
= 1 m(t)
(6.19)
this relation deﬁnes frequency modulation. The instantaneous frequency, as a function of time, is the
derivative of the argument of the cosine or
IF(t) = d[ct + θ(t)]
dt
(6.20)
= c + dθ(t)
dt
(6.21)
= c + 1 m(t)
(6.22)
indicating how the frequency is changing with time. For instance, if θ(t) is a constant—so that the
carrier is just a sinusoid of frequency c and constant phase θ—the instantaneous frequency is simply
c. The term 1 m(t) relates to the spreading of the frequency about c. Thus, the modulation paradox
Professor E. Craig proposed in his book [17]:
In amplitude modulation the bandwidth depends on the frequency of the message, while in
frequency modulation the bandwidth depends on the amplitude of the message.
Thus, the modulated signals are
PM:
sPM(t) = cos(ct + Kfm(t))
(6.23)
FM:
sFM(t) = cos(ct + 1
t
Z
−∞
m(τ)dτ)
(6.24)
Narrowband FM
In this case the angle θ(t) is small, so that cos(θ(t)) ≈1 and sin(θ(t)) ≈θ(t), simplifying the spectrum
of the transmitted signal:
S() = F [cos(ct + θ(t))]
= F [cos(ct) cos(θ(t)) −sin(ct) sin(θ(t))]
≈F [cos(ct) −sin(ct)θ(t)]
(6.25)

6.4 Application to Communications
387
Using the spectrum of a cosine and the modulation theorem, we get
S() ≈π [δ( −c) + δ( + c)] −1
2j [2( −c) −2( + c)]
(6.26)
where 2() is the spectrum of the angle, which is found to be (using the derivative property of the
Fourier transform)
2() = 1
j M()
(6.27)
If the angle θ(t) is not small, we have wideband FM and its spectrum is more difﬁcult to obtain.
I Example 6.6: Simulation of FM modulation with MATLAB
In these simulations we will concern ourselves with the results and leave the discussion of issues
related to the code for the next chapter since the signals are approximated by discrete-time signals.
For the narrowband FM we consider a sinusoidal message
m(t) = 80 sin(20πt)u(t),
and a sinusoidal carrier of frequency fc = 100 Hz, so that for 1 = 0.1π the FM signal is
x(t) = cos(2πfct + 0.1π
t
Z
−∞
m(τ)dτ)
Figure 6.19 shows on the top left the message and the narrowband FM signal x(t) right below it, and
on the top right their corresponding magnitude spectra |M()| and below |X()|. The narrowband
FM has only shifted the frequency of the message. The instantaneous frequency (the derivative of
the argument of the cosine) is
IF(t) = 2πfc + 0.1πm(t) = 200π + 8π sin(20πt) ≈200π
That is, it remains almost constant for all times. For the narrowband FM, the spectrum of the
modulated signal remains the same for all times. To illustrate this we computed the spectrogram
of x(t). Simply, the spectrogram can be thought of as the computation of the Fourier transform as
the signal evolves with time (see Figure 6.19(c)).
To illustrate the wideband FM, we consider two messages,
m1(t) = 80 sin(20πt)u(t)
m2(t) = 2000tu(t)
giving FM signals,
xi(t) = cos(2πfcit + 50π
t
Z
−∞
mi(τ)dτ)
i = 1, 2
where fc1 = 2500 Hz and fc2 = 25 Hz. In this case, the instantaneous frequency is
IFi(t) = 2πfci + 50πmi(t)
i = 1, 2

388
CHAPTER 6:
Application to Control and Communications
0
0.05
0.1
0.15
0.2
−50
0
50
m(t)
0
0.05
0.1
0.15
0.2
−0.5
0
0.5
1
x(t)
t (sec)
−500
0
500
0
5
10
|M(Ω)|
−500
0
500
0
0.05
0.1
f
|x(Ω)|
(a)
(b)
500
450
400
350
300
250
200
150
100
50
0
Frequency
0.6/7 0.7/7 0.8/7 0.9/7
1/7
1.1/7 1.2/7 1.3/7
0.2
Time
(c)
FIGURE 6.19
Narrowband frequency modulation: (a) message m(t) and narrowband FM signal x(t); (b) magnitude spectra of
m(t) and x(t); and (c) spectrogram of x(t) displaying evolution of its Fourier transform with respect to time.
These instantaneous frequencies are not almost constant as before. The frequency of the carrier
is now continuously changing with time. For instance, for the ramp message the instantaneous
frequency is
IF2(t) = 50π + 105tπ
so that for a small time interval [0, 0.1] we get a chirp (sinusoid with time-varying frequency), as
shown in Figure 6.20(b). Figure 6.20 display the messages, the FM signals, and their corresponding
magnitude spectra and their spectrograms. These FM signals are broadband, occupying a band of
frequencies much larger than the messages, and their spectrograms show that their spectra change
with time.
I

6.4 Application to Communications
389
0
0.02
0.04
0.06
0.08
0.1
−50
0
50
m(t)
−200 −150 −100 −50
0
50
100
150
200
0
5
10
|M(Ω)|
−5000
0
5000
0
0.005
0.01
0.015
f
|X(Ω)|
Time
Frequency
0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
0
0.02
0.04
0.06
0.08
0.1
−1
−0.5
0
0.5
1
x(t)
m(t)
x(t)
t (sec)
0
0.02
0.04
0.06
0.08
0.1
0
50
100
150
200
−1000
−500
0
500
1000
0
5
10
|M(Ω)|
−5000
0
5000
1
1.5
2
2.5
3
f
|X(Ω)|
×10−3
Time
Frequency
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
t (sec)
0
0.02
0.04
0.06
0.08
0.1
−1
−0.5
0
0.5
1
(a)
(b)
FIGURE 6.20
Wideband frequency modulation, from top to bottom, for (a) the sinusoidal message and for (b) the ramp
message: messages, FM-modulated signals, spectra of messages, spectrum of FM signals, and spectrogram of
FM signals.

390
CHAPTER 6:
Application to Control and Communications
6.5 ANALOG FILTERING
The basic idea of ﬁltering is to get rid of frequency components of a signal that are not desirable.
Application of ﬁltering can be found in control, in communications, and in signal processing. In this
section we provide a short introduction to the design of analog ﬁlters. Chapter 11 is dedicated to the
design of discrete ﬁlters and to some degree that chapter will be based on the material in this section.
According to the eigenfunction property of LTI systems (Figure 6.21) the steady-state response of an
LTI system to a sinusoidal input—with a certain magnitude, frequency, and phase—is a sinusoid of
the same frequency as the input, but with magnitude and phase affected by the response of the system
at the frequency of the input. Since periodic as well as aperiodic signals have Fourier representations
consisting of sinusoids of different frequencies, the frequency components of any signal can be mod-
iﬁed by appropriately choosing the frequency response of the LTI system, or ﬁlter. Filtering can thus
be seen as a way of changing the frequency content of an input signal.
The appropriate ﬁlter for a certain application is speciﬁed using the spectral characterization of the
input and the desired spectral characteristics of the output. Once the speciﬁcations of the ﬁlter are set,
the problem becomes one of approximation as a ratio of polynomials in s. The classical approach in
ﬁlter design is to consider low-pass prototypes, with normalized frequency and magnitude responses,
which may be transformed into other ﬁlters with the desired frequency response. Thus, a great deal
of effort is put into designing low-pass ﬁlters and into developing frequency transformations to map
low-pass ﬁlters into other types of ﬁlters. Using cascade and parallel connections of ﬁlters also provide
a way to obtain different types of ﬁlters.
The resulting ﬁlter should be causal, stable, and have real-valued coefﬁcients so that it can be used in
real-time applications and realized as a passive or an active ﬁlter. Resistors, capacitors, and inductors
are used in the realization of passive ﬁlters, while resistors, capacitors, and operational ampliﬁers are
used in active ﬁlter realizations.
6.5.1 Filtering Basics
A ﬁlter H(s) = B(s)/A(s) is an LTI system having a speciﬁc frequency response. The convolution
property of the Fourier transform gives that
Y() = X()H( j)
(6.28)
where
H( j) = H(s)|s=j
Thus, the frequency content of the input, represented by the Fourier transform X(), is changed by
the frequency response H( j) of the ﬁlter so that the output signal with spectrum Y() only has
desirable frequency components.
FIGURE 6.21
Eigenfunction property of continuous LTI systems.
LTI system
H(s)
Aej(Ω0t+θ)
A|H(jΩ0)|ej(Ω0t+θ+∠H(jΩ0)

6.5 Analog Filtering
391
Magnitude Squared Function
The magnitude-squared function of an analog low-pass ﬁlter has the general form
|H( j)|2 =
1
1 + f(2)
(6.29)
where for low frequencies f(2) ≈0 so that |H( j)|2 ≈1, and for high frequencies f(2) →∞so
that |H( j)|2 →0. Accordingly, there are two important issues to consider:
I
Selection of the appropriate function f(.).
I
The factorization needed to get H(s) from the magnitude-squared function.
As an example of the above steps, consider the Butterworth low-pass analog ﬁlter. The Butterworth
magnitude-squared response of order N is
|HN( j)|2 =
1
1 +
h

hp
i2N
(6.30)
where hp is the half-power frequency of the ﬁlter. We then have that for  << hp, |HN( j)| ≈1,
and for  >> hp, then |HN( j)| →0. To ﬁnd H(s) we need to factorize Equation (6.30). Letting S
be a normalized variable S = s/hp, the magnitude-squared function (Eq. 6.30) can be expressed in
terms of the S variable by letting S/j = /hp to obtain
H(S)H(−S) =
1
1 + (−S2)N
since |H( j′)|2 = H( j′)H∗( j′) = H( j′)H(−j′). As we will see, the poles of H(S)H(−S) are sym-
metrically clustered in the s-plane with none on the j axis. The factorization then consists of
assigning poles in the open left-hand s-plane to H(S), and the rest to H(−S). We thus obtain
H(S)H(−S) =
1
D(S)
1
D(−S)
so that the ﬁnal form of the ﬁlter is
H(S) =
1
D(S)
where D(S) has roots on the left-hand s-plane. A ﬁnal step is the replacement of S by the
unnormalized variable s, to obtain the ﬁnal form of the ﬁlter transfer function:
Butterworth low-pass ﬁlter: H(s) = H(S)|S=s/hp
(6.31)
Filter Speciﬁcations
Although an ideal low-pass ﬁlter is not realizable (recall the Paley-Wiener condition in Chapter 5)
its magnitude response can be used as prototype for specifying low-pass ﬁlters. Thus, the desired
magnitude is speciﬁed as
1 −δ2 ≤|H( j)| ≤1
0 ≤ ≤p
(passband)
0 ≤|H( j)| ≤δ1
 ≥s
(stopband)
(6.32)

392
CHAPTER 6:
Application to Control and Communications
FIGURE 6.22
Magnitude speciﬁcations for
a low-pass ﬁlter.
Ω
Ω
1
0
|H(jΩ)|
1−δ 2
δ 1
Ωp
Ωp
α (Ω)
α max
α min
Ωs
Ωs
for some small values δ1 and δ2. There is no speciﬁcation in the transition region p <  < s. Also
the phase is not speciﬁed, although we wish it to be linear at least in the passband. See Figure 6.22.
To simplify the computation of the ﬁlter parameters, and to provide a scale that has more resolution
and physiological signiﬁcance than the speciﬁcations given above, the magnitude speciﬁcations are
typically expressed in a logarithmic scale. Deﬁning the loss function (in decibels, or dBs) as
α() = −10 log10 |H( j)|2
= −20 log10 |H( j)|
dBs
(6.33)
an equivalent set of speciﬁcations to those in Equation (6.32) is
0 ≤α() ≤αmax
0 ≤ ≤p
(passband)
α() ≥αmin
 ≥s
(stopband)
(6.34)
where αmax = −20 log10(1 −δ2) and αmin = −20 log10(δ1).
In the above speciﬁcations, the dc loss was 0 dB corresponding to a normalized dc gain of 1. In
more general cases, α(0) ̸= 0 and the loss speciﬁcations are given as α(0) = α1, α2 in the passband
and α3 in the stopband. To normalize these speciﬁcations we need to subtract α1, so that the loss
speciﬁcations are
α(0) = α1 (dc loss)
αmax = α2 −α1 (maximum attenuation in passband)
αmin = α3 −α1 (minimum attenuation in stopband)
Using {αmax, p, αmin, s} we proceed to design a magnitude-normalized ﬁlter, and then use α1 to
achieve the desired dc gain.
The design problem is then: Given the magnitude speciﬁcations in the passband (α(0), αmax, and p)
and in the stopband (αmin and s) we then
1.
Choose the rational approximation method (e.g., Butterworth).
2.
Solve for the parameters of the ﬁlter to obtain a magnitude-squared function that satisﬁes the
given speciﬁcations.

6.5 Analog Filtering
393
3.
Factorize the magnitude-squared function and choose the poles on the left-hand s-plane,
guaranteeing the ﬁlter stability, to obtain the transfer function HN(s) of the ﬁlter.
6.5.2 Butterworth Low-Pass Filter Design
The magnitude-squared approximation of a low-pass Nth-order Butterworth ﬁlter is given by
|HN( j′)|2 =
1
1 +
 /hp
2N
′ = 
hp
(6.35)
where hp is the half-power or −3-dB frequency. This frequency response is normalized with respect
to the half-power frequency (i.e., the normalized frequency is ′ = /hp) and normalized in mag-
nitude as the dc gain is |H( j0)| = 1. The frequency ′ = /hp = 1 is the normalized half-power
frequency since |HN( j1)|2 = 1/2. The given magnitude-squared function is thus normalized with
respect to frequency (giving a unity half-power frequency) and in magnitude (giving a unity DC gain
for the low-pass ﬁlter). The approximation improves (i.e., gets closer to the ideal ﬁlter) as the order
N increases.
Remarks
I
The half-power frequency is called the −3-dB frequency because in the case of the low-pass ﬁlter with a
dc gain of 1, at the half-power frequency hp the magnitude-squared function is
|H( jhp)|2 = |H( j0)|2
2
= 1
2.
(6.36)
In the logarithmic scale we have
10 log10(|H( jhp)|2) = −10 log10(2) ≈−3 (dB)
(6.37)
This corresponds to a loss of 3 dB.
I
It is important to understand the signiﬁcance of the frequency and magnitude normalizations typical in
ﬁlter design. Having a low-pass ﬁlter with normalized magnitude, its dc gain is 1, if one desires a ﬁlter
with a DC gain K ̸= 1 it can be obtained by multiplying the magnitude-normalized ﬁlter by the constant
K. Likewise, a ﬁlter H(S) designed with a normalized frequency, say ′ = /hp so that the normalized
half-power frequency is 1, is converted into a denormalized ﬁlter H(s) with a desired hp by replacing
S = s/hp in H(S).
Factorization
To obtain a ﬁlter that satisﬁes the speciﬁcations and that is stable we need to factorize the
magnitude-squared function. By letting S = s/hp be a normalized Laplace variable, then S/j = ′ =
/hp and
H(S)H(−S) =
1
1 + (−S2)N
If the denominator can be factorized as
D(S)D(−S) = 1 + (−S2)N
(6.38)

394
CHAPTER 6:
Application to Control and Communications
we let H(S) = 1/D(S)—that is, we assign to H(S) the poles in the left-hand s-plane so that the resulting
ﬁlter is stable. The roots of D(S) in Equation (6.38) are
S2N
k
= ej(2k−1)π
e−jπN
= ej(2k−1+N)π for integers k = 1, . . . , 2N
after replacing −1 = ej(2k−1)π and (−1)N = e−jπN. The 2N roots are then
Sk = ej(2k−1+N)π/(2N)
k = 1, . . . , 2N
(6.39)
Remarks
I
Since |Sk| = 1, the poles of the Butterworth ﬁlter are on a circle of unit radius. De Moivre’s theorem guar-
antees that the poles are also symmetrically distributed around the circle, and because of the condition that
complex poles should be complex conjugate pairs, the poles are symmetrically distributed with respect to the
σ axis. Letting S = s/hp be the normalized Laplace variable, then s = Shp, so that the denormalized
ﬁlter H(s) has its poles in a circle of radius hp.
I
No poles are on the j′ axis, as can be seen by showing that the angle of the poles are not equal to π/2 or
3π/2. In fact, for 1 ≤k ≤N, the angle of the poles are bounded below and above by letting 1 ≤k and
then k ≤N to get
π
2

1 + 1
N

≤(2k −1 + N)π
2N
≤π
2

3 −1
N

and for integers N ≥1 the above indicates that the angle will not be equal to either π/2 or 3π/2, or on
the j′ axis.
I
Consecutive poles are separated by π/N radians from each other. In fact, subtracting the angles of two
consecutive poles can be shown to give ±π/N.
Using the above remarks and the fact that the poles must be in conjugate pairs, since the coefﬁcients
of the ﬁlter are real-valued, it is easy to determine the location of the poles geometrically.
I Example 6.7
A second-order low-pass Butterworth ﬁlter, normalized in magnitude and in frequency, has a
transfer function of
H(S) =
1
S2 +
√
2S + 1
We would like to obtain a new ﬁlter H(s) with a dc gain of 10 and a half-power frequency hp =
100 rad/sec.
The DC gain of H(S) is unity—in fact, when  = 0, S = j0 gives H( j0) = 1. The half-power
frequency of H(S) is unity, indeed letting ′ = 1, then S = j1 and
H( j1) =

1
j2 + j
√
2 + 1

=
1
j
√
2
so that |H( j1)|2 = |H( j0)|2/2 = 1/2, or ′ = 1 is the half-power frequency.

6.5 Analog Filtering
395
Thus, the desired ﬁlter with a dc gain of 10 is obtained by multiplying H(S) by 10. Furthermore,
if we let S = s/100 be the normalized Laplace variable when S = j′
hp = j1, we get that s = jhp =
j100, or hp = 100, the desired half-power frequency. Thus, the denormalized ﬁlter in frequency
H(s) is obtained by replacing S = s/100. The denormalized ﬁlter in magnitude and frequency
is then
H(s) =
10
(s/100)2 +
√
2(s/100) + 1
=
105
s2 + 100
√
2s + 104
I
Design
For the Butterworth low-pass ﬁlter, the design consists in ﬁnding the parameters N, the minimum
order, and hp, the half-power frequency, of the ﬁlter from the constrains in the passband and in the
stopband.
The loss function for the low-pass Butterworth is
α() = −10 log10 |HN(/hp)|2 = 10 log10(1 +
 /hp
2N)
The loss speciﬁcations are
0 ≤α() ≤αmax
0 ≤ ≤p
αmin ≤α() < ∞
 ≥s
At  = p, we have that
10 log10(1 +
 p/hp
2N) ≤αmax
so that
 p
hp
2N
≤100.1αmax −1
(6.40)
and similarly for  = s, we have that
100.1αmin −1 ≤
 s
hp
2N
(6.41)
We then have that from Equation (6.40) and (6.41), the half-power frequency is in the range
p
(100.1αmax −1)1/2N ≤hp ≤
s
(100.1αmin −1)1/2N
(6.42)
and from the log of the two extremes of Equation (6.42), we have that
N ≥log10[(100.1αmin −1)/(100.1αmax −1)]
2 log10(s/p)
(6.43)

396
CHAPTER 6:
Application to Control and Communications
Remarks
I
According to Equation (6.43) when either
I
The transition band is narrowed (i.e., p →s), or
I
The loss αmin is increased, or
I
The loss αmax is decreased
the quality of the ﬁlter is improved at the cost of having to implement a ﬁlter with a high order N.
I
The minimum order N is an integer larger or equal to the right side of Equation (6.43). Any integer
larger than the minimum N also satisﬁes the speciﬁcations but increases the complexity of the ﬁlter.
I
Although there is a range of possible values for the half-power frequency, it is typical to make the frequency
response coincide with either the passband or the stopband speciﬁcations giving a value for the half-power
frequency in the range. Thus, we can have either
hp =
p
(100.1αmax −1)1/2N
(6.44)
or
hp =
s
(100.1αmin −1)1/2N
(6.45)
as possible values for the half-power frequency.
I
The design aspect is clearly seen in the ﬂexibility given by the equations. We can select out of an inﬁnite
possible set of values of N and of half-power frequencies. The optimal order is the smallest value of N and
the half-power frequency can be taken as one of the extreme values.
I
After the factorization, or the formation of D(S) from the poles, we need to denormalize the obtained
transfer function HN(S) = 1/D(S) by letting S = s/hp to get HN(s) = 1/D(s/hp), the ﬁlter that
satisﬁes the speciﬁcations. If the desired DC gain is not unit, the ﬁlter needs to be denormalized in
magnitude by multiplying it by an appropriate gain K.
6.5.3 Chebyshev Low-Pass Filter Design
The normalized magnitude-squared function for the Chebyshev low-pass ﬁlter is given by
|HN(′)|2 =
1
1 + ε2C2
N(/p)
′ = 
p
(6.46)
where the frequency is normalized with respect to the passband frequency p so that ′ = /p,
N stands for the order of the ﬁlter, ε is a ripple factor, and CN(.) are the Chebyshev orthogonal4
polynomials of the ﬁrst kind deﬁned as
CN(′) =
 cos(N cos−1(′))
|′| ≤1
cosh(N cosh−1(′))
|′| > 1
(6.47)
The deﬁnition of the Chebyshev polynomials depends on the value of ′. Indeed, whenever |′| > 1,
the deﬁnition based in the cosine is not possible since the inverse would not exist; thus the cosh(.)
4Pafnuty Chebyshev (1821–1894), a brilliant Russian mathematician, was probably the ﬁrst one to recognize the general concept of
orthogonal polynomials.

6.5 Analog Filtering
397
deﬁnition is used. Likewise, whenever |′| ≤1, the deﬁnition based in the hyperbolic cosine would
not be possible since the inverse of this function only exists for values of ′ bigger or equal to 1 and
so the cos(.) deﬁnition is used. From the deﬁnition it is not clear that CN(′) is an Nth-order poly-
nomial in ′. However, if we let θ = cos−1(′) or ′ = cos(θ) when |′| ≤1, we have that CN(′) =
cos(Nθ) and
CN+1(′) = cos((N + 1)θ) = cos(Nθ) cos(θ) −sin(Nθ) sin(θ)
CN−1(′) = cos((N −1)θ) = cos(Nθ) cos(θ) + sin(Nθ) sin(θ)
so that adding them we get
CN+1(′) + CN−1(′) = 2 cos(θ) cos(Nθ) = 2′CN(′)
This gives a three-term expression for computing CN(′), or a difference equation
CN+1(′) + CN−1(′) = 2′CN(′)
N ≥0
(6.48)
with initial conditions
C0(′) = cos(0) = 1
C1(′) = cos(cos−1(′)) = ′
We can then see that
C0(′) = 1
C1(′) = ′
C2(′) = −1 + 2′2
C3(′) = −3′ + 4′3
...
which are polynomials in ′ of order N = 0, 1, 2, 3, . . .. In Chapter 0 we gave a script to compute
and plot these polynomials using symbolic MATLAB.
Remarks
I
Two fundamental characteristics of the CN(′) polynomials are: (1) they vary between 0 and 1 in the
range ′ ∈[−1, 1], and (2) they grow outside this range (according to their deﬁnition, the Chebyshev
polynomials outside this range become cosh(.) functions, which are functions always bigger than 1). The
ﬁrst characteristic generates ripples in the passband, while the second makes these ﬁlters have a magnitude
response that goes to zero faster than Butterworth’s.
I
There are other characteristics of interest for the Chebyshev polynomials. The Chebyshev polynomials are
unity at ′ = 1 (i.e., CN(1) = 1 for all N). In fact, C0(1) = 1, C1(1) = 1, and if we assume that
CN−1(1) = CN(1) = 1, we then have that CN+1(1) = 1 according to the three-term recursion. This
indicates that the magnitude-square function is |HN( j1)|2 = 1/(1 + ε2) for any N.

398
CHAPTER 6:
Application to Control and Communications
I
Different from the Butterworth ﬁlter that has a unit dc gain, the dc gain of the Chebyshev ﬁlter depends
on the order of the ﬁlter. This is due to the property of the Chebyshev polynomial of being |CN(0)| = 0 if
N is odd and 1 if N is even. Thus, the dc gain is 1 when N is odd, but 1/
√
1 + ε2 when N is even. This
is due to the fact that the Chebyshev polynomials of odd order do not have a constant term, and those of
even order have 1 or −1 as the constant term.
I
Finally, the polynomials CN(′) have N real roots between −1 and 1. Thus, the Chebyshev ﬁlter displays
N/2 ripples between 1 and
√
1 + ε2 for normalized frequencies between 0 and 1.
Design
The loss function for the Chebyshev ﬁlter is
α(′) = 10 log10

1 + ε2C2
N(′)

′ = 
p
(6.49)
The design equations for the Chebyshev ﬁlter are obtained as follows:
I
Ripple factor ε and ripple width (RW): From CN(1) = 1, and letting the loss equal αmax at that
normalized frequency, we have that
ε =
p
100.1αmax −1
RW = 1 −
1
√
1 + ε2
(6.50)
I
Minimum order: The loss function at 
′
s is bigger or equal to αmin, so that solving for the Chebyshev
polynomial we get after replacing ε,
CN(
′
s) = cosh(N cosh−1(
′
s))
≥
 10.1αmin −1
10.1αmax −1
0.5
where we used the cosh(.) deﬁnition of the Chebyshev polynomials since 
′
s > 1. Solving for N
we get
N ≥
cosh−1
h
100.1αmin−1
100.1αmax−1
i0.5
cosh−1 
s
p

(6.51)
I
Half-power frequency: Letting the loss at the half-power frequency equal 3 dB and using that 100.3 ≈
2, we obtain from Equation 6.49 the Chebyshev polynomial at that normalized frequency to be
CN(
′
hp) = 1
ε
= cosh

N cosh−1(
′
hp)


6.5 Analog Filtering
399
where the last term is the deﬁnition of the Chebyshev polynomial for 
′
hp > 1. Thus, we get
hp = p cosh
 1
N cosh−1
1
ε

(6.52)
Factorization
The factorization of the magnitude-squared function is a lot more complicated for the Cheby-
shev ﬁlter than for the Butterworth ﬁlter. If we let the normalized variable S = s/p equal j′, the
magnitude-squared function can be written as
H(S)H(−S) =
1
1 + ε2C2
N(S/j) =
1
D(S)D(−S)
As before in the Butterworth case, the poles in the left-hand s-plane gives H(S) = 1/D(S), a stable
ﬁlter.
The poles of the H(S) can be found to be in an ellipse. They can be connected with the poles of the
corresponding order Butterworth ﬁlter by an algorithm due to Professor Ernst Guillemin. The poles
of H(S) are given by the following equations for k = 1, . . . , N, with N the minimal order of the ﬁlter:
a = 1
N sinh−1
1
ε

σk = −sinh(a) cos(ψk)
(real part)
′
k = ± cosh(a) sin(ψk)
(imaginary part)
(6.53)
where 0 ≤ψk < π/2 (refer to Equation 6.39) are the angles corresponding to the Butterworth ﬁlters
(measured with respect to the negative real axis of the s-plane).
Remarks
I
The dc gain of the Chebyshev ﬁlter is not easy to determine as in the Butterworth ﬁlter, as it depends on
the order N. We can, however, set the desired dc value by choosing the appropriate value of a gain K so
that ˆH(S) = K/D(S) satisﬁes the dc gain speciﬁcation.
I
The poles of the Chebyshev ﬁlter depend now on the ripple factor ε and so there is no simple way to ﬁnd
them as it was in the case of the Butterworth.
I
The ﬁnal step is to replace the normalized variable S = s/p in H(S) to get the desired ﬁlter H(s).
I Example 6.8
Consider the low-pass ﬁltering of an analog signal x(t) = [−2 cos(5t) + cos(10t) + 4 sin(20t)]u(t)
with MATLAB. The ﬁlter is a third-order low-pass Butterworth ﬁlter with a half-power frequency
hp = 5 rad/sec—that is, we wish to attenuate the frequency components of the frequencies
10 and 20 rad/sec. Design the desired ﬁlter and show how to do the ﬁltering.
The design of the ﬁlter is done using the MATLAB function butter where besides the speciﬁcation of
the desired order, N = 3, and half-power frequency, hp = 5 rad/sec, we also need to indicate that

400
CHAPTER 6:
Application to Control and Communications
the ﬁlter is analog by including an ’s’ as one of the arguments. Once the coefﬁcients of the ﬁlter
are obtained, we could then either solve the differential equation from these coefﬁcients or use the
Fourier transform, which we choose to do. Symbolic MATLAB is thus used to compute the Fourier
transform of the input X(), and after generating the frequency response function H( j) from the
ﬁlter coefﬁcients, we multiply these two to get Y(), which is inversely transformed to obtain y(t).
To obtain H( j) symbolically we multiply the coefﬁcients of the numerator and denominator
obtained from butter by variables ( j)n where n corresponds to the order of the coefﬁcient in
the numerator or the denominator, and then add them. The poles of the designed ﬁlter and its
magnitude response are shown in Figure 6.23, as well as the input x(t) and the output y(t). The
following script was used for the ﬁlter design and the ﬁltering of the given signal.
−6
−4
−2
0
2
−5
−4
−3
−2
−1
0
1
2
3
4
5
jΩ
0
10
20
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
|H(jΩ)|
σ
Ω
0
5
10
15
20
−5
0
5
t
x(t)
0
5
10
15
20
−5
0
5
t
y(t)
cos(10t)−2cos(5t) +4sin(20t)
FIGURE 6.23
Filtering of an analog signal x(t) using a low-pass Butterworth ﬁlter. Notice that the output of the ﬁlter is
approximately the sinusoid of 5 rad/sec in x(t), as the other two components have been attenuated.
%%%%%%%%%%%%%%%%%%%
% Example 6.8 -- Filtering with Butterworth ﬁlter
%%%%%%%%%%%%%%%%%%%
clear all; clf
syms t w
x = cos(10 ∗t) −2 ∗cos(5 ∗t) + 4 ∗sin(20 ∗t); % input signal
X = fourier(x);
N = 3; Whp = 5;
% ﬁlter parameters
[b, a] = butter(N, Whp, ’s’); % ﬁlter design
W = 0:0.01:30; Hm = abs(freqs(b, a, W)); % magnitude response in W
% ﬁlter output
n = N:−1:0; U = ( j ∗w).ˆn
num = b −conj(U’); den = a −conj(U’);

6.5 Analog Filtering
401
H = num/den; % frequency response
Y = X ∗H; % convolution property
y = ifourier(Y, t); % inverse Fourier
I
I Example 6.9
In this example we will compare the performance of Butterworth and Chebyshev low-pass ﬁlters
in the ﬁltering of an analog signal x(t) = [−2 cos(5t) + cos(10t) + 4 sin(20t)]u(t) using MATLAB.
We would like the two ﬁlters to have the same half-power frequency.
The magnitude speciﬁcations for the low-pass Butterworth ﬁlter are
αmax = 0.1dB, p = 5rad/sec
(6.54)
αmin = 15dB, s = 10 rad/sec
(6.55)
and a dc loss of 0 dB. Once this ﬁlter is designed, we would like the Chebyshev ﬁlter to have the
same half-power frequency. In order to obtain this, we need to change the p speciﬁcation for the
Chebyshev ﬁlter. To do that we use the formulas for the half-power frequency of this type of ﬁlter
to ﬁnd the new value for p.
The Butterworth ﬁlter is designed by ﬁrst determining the minimum order N and the half-power
frequency hp using the function buttord, and then ﬁnding the ﬁlter coefﬁcients by means of the
function butter. Likewise, for the design of the Chebyshev ﬁlter we use the function cheb1ord
to ﬁnd the minimum order and the cut-off frequency (the new p is obtained from the half-
power frequency). The ﬁltering is implemented using the Fourier transform as before.
There are two signiﬁcant differences between the designed Butterworth and Chebyshev ﬁlters.
Although both of them have the same half-power frequency, the transition band of the Cheby-
shev ﬁlter is narrower, [6.88 10], than that of the Butterworth ﬁlter, [5 10], indicating that the
Chebyshev is a better ﬁlter. The narrower transition band is compensated by a lower minimum
order of ﬁve for the Chebyshev compared to the six-order Butterworth. Figure 6.24 displays the
poles of the Butterworth and the Chebyshev ﬁlters, their magnitude responses, as well as the input
signal x(t) and the output y(t) for the two ﬁlters (the two perform very similarly).
%%%%%%%%%%%%%%%%%%%
% Example 6.9 -- Filtering with Butterworth and Chebyshev ﬁlters
%%%%%%%%%%%%%%%%%%%
clear all;clf
syms t w
x = cos(10 ∗t) −2 ∗cos(5 ∗t) + 4 ∗sin(20 ∗t); X = fourier(x);
wp = 5;ws = 10;alphamax = 0.1;alphamin = 15; % ﬁlter parameters
% butterworth ﬁlter
[N, whp] = buttord(wp, ws, alphamax, alphamin, ’s’)
[b, a] = butter(N, whp, ’s’)
% cheby1 ﬁlter
epsi = sqrt(10ˆ(alphamax/10) −1)

402
CHAPTER 6:
Application to Control and Communications
−8 −6 −4 −2
0
2
−5
0
5
Butterworth
σ
σ
−4
−2
0
2
−5
0
5
Chebyshev
jΩ
jΩ
0
10
20
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Ω
|H(jΩ)|
Butterworth
Chebyshev
0
5
10
15
20
−5
0
5
t
y(t)
0
5
10
15
20
−5
0
5
t
x(t)
cos(10t) −2cos(5t) +4sin(20t)
FIGURE 6.24
Comparison of ﬁltering of an analog signal x(t) using a low-pass Butterworth and Chebyshev ﬁlter with the same
half-power frequency.
wp = whp/cosh(acosh(1/epsi)/N) % recomputing wp to get same whp
[N1, wn] = cheb1ord(wp, ws, alphamax, alphamin, ’s’);
[b1, a1] = cheby1(N1, alphamax, wn, ’s’);
% frequency responses
W = 0:0.01:30;
Hm = abs(freqs(b, a, W));
Hm1 = abs(freqs(b1, a1, W));
% generation of frequency response from coefﬁcients
n = N:−1:0; n1 = N1:−1:0;
U = ( j ∗w).ˆn; U1 = ( j ∗w).ˆn1
num = b ∗conj(U’); den = a ∗conj(U’);
num1 = b1 ∗conj(U1’); den1 = a1 ∗conj(U1’)
H = num/den; % Butterworth LPF
H1 = num1/den1; % Chebyshev LPF
% output of ﬁlter
Y = X ∗H;
Y1 = X ∗H1;
y = ifourier(Y, t)
y1 = ifourier(Y1, t)
I
6.5.4 Frequency Transformations
As indicated before, the design of an analog ﬁlter is typically done by transforming the frequency of
a normalized prototype low-pass ﬁlter. The frequency transformations were developed by Professor
Ronald Foster [72] using the properties of reactance functions. The frequency transformations for the

6.5 Analog Filtering
403
basic ﬁlters are given by:
Low pass-low pass :
S =
s
0
Low pass-high pass :
S = 0
s
Low pass-band pass :
S = s2 + 2
0
s BW
Low pass-band eliminating :
S =
s BW
s2 + 2
0
(6.56)
where S is the normalized and s the ﬁnal variables, while 0 is a desired cut-off frequency and BW is
a desired bandwidth.
Remarks
I
The low-pass to low-pass (LP-LP) and low-pass to high-pass (LP-HP) transformations are linear in the
numerator and denominator; thus the number of poles and zeros of the prototype low-pass ﬁlter is preserved.
On the other hand, the low-pass to band-pass (LP-BP) and low-pass to band-eliminating (LP-BE) trans-
formations are quadratic in either the numerator or the denominator, so that the number of poles/zeros
is doubled. Thus, to obtain a 2Nth-order band-pass or band-eliminating ﬁlter the prototype low-pass ﬁlter
should be of order N. This is an important observation useful in the design of these ﬁlters with MATLAB.
I
It is important to realize that only frequencies are transformed, and the magnitude of the prototype ﬁlter
is preserved. Frequency transformations will be useful also in the design of discrete ﬁlters, where these
transformations are obtained in a completely different way, as no reactance functions would be available
in that domain.
I Example 6.10
To illustrate how the above transformations can be used to convert a prototype low-pass ﬁlter
we use the following script. First a low-pass prototype ﬁlter is designed using butter, and then to
this ﬁlter we apply the lowpass to highpass transformation with 0 = 40 (rad/sec) to obtain a
high-pass ﬁlter. Let then 0 = 6.32 (rad/sec) and BW = 10 (rad/sec) to obtain a band-pass and
a band-eliminating ﬁlters using the appropriate transformations. The following is the script used.
The magnitude responses are plotted with ezplot. Figure 6.25 shows the results.
clear all; clf
syms w
N = 5; [b, a] = butter(N, 1, ’s’) % low-pass prototype
omega0 = 40;BW = 10; omega1=sqrt(omega0); % transformation parameters
% low-pass prototype
n = N:−1:0;
U = ( j ∗w).ˆn; num = b ∗conj(U’); den = a ∗conj(U’);
H = num/den;
% low-pass to high-pass

404
CHAPTER 6:
Application to Control and Communications
0
(a)
(b)
(c)
(d)
5
10
0
0.2
0.4
0.6
0.8
1
Ω
LP Prototype
|H(jΩ)|
0
20
40
60
80
0
0.2
0.4
0.6
0.8
1
Ω
LP−HP
|H1( jΩ)|
0
10
20
30
40
0
0.2
0.4
0.6
0.8
1
Ω
LP−BP
|H2(jΩ)|
0
10
20
30
40
0
0.2
0.4
0.6
0.8
1
Ω
LP−BE
|H3(jΩ)|
FIGURE 6.25
Frequency transformations: (a) prototype low-pass ﬁlter, (b) low-pass to high-pass transformation, (c) low-pass
to band-pass transformation, and (d) low-pass to band-eliminating transformation.
U1 = (omega0/( j ∗w)).ˆn;
num1 = b ∗conj(U1’); den1 = a ∗conj(U1’);
H1 = num1/den1;
% low-pass to band-pass
U2 = ((−wˆ2 + omega1ˆ2)/(BW ∗j ∗w)).ˆn
num2 = b ∗conj(U2’); den2 = a ∗conj(U2’);
H2 = num2/den2;
% low-pass to band-eliminating
U3 = ((BW ∗j ∗w)/(−wˆ2 + omega1ˆ2)).ˆn
num3 = b ∗conj(U3’); den3 = a ∗conj(U3’);
H3 = num3/den3
I

6.5 Analog Filtering
405
6.5.5 Filter Design with MATLAB
The design of ﬁlters, analog and discrete, is simpliﬁed by the functions that MATLAB provides. Func-
tions to ﬁnd the ﬁlter parameters from magnitude speciﬁcations, as well as functions to ﬁnd the ﬁlter
poles/zeros and to plot the designed ﬁlter magnitude and phase responses, are available.
Low-Pass Filter Design
The design procedure is similar for all of the approximation methods (Butterworth, Chebyshev,
elliptic) and consists of both
I
Finding the ﬁlter parameters from loss speciﬁcations.
I
Obtaining the ﬁlter coefﬁcients from these parameters.
Thus, to design an analog low-pass ﬁlter using the Butterworth approximation, the loss speciﬁcations
αmax and αmin, and the frequency speciﬁcations, p and s are ﬁrst used by the function buttord to
determine the minimum order N and the half-power frequency hp of the ﬁlter that satisﬁes the
speciﬁcations. Then the function butter uses these two values to determine the coefﬁcients of the
numerator and the denominator of the designed ﬁlter. We can then use the function freqs to plot
the designed ﬁlter magnitude and phase. Similarly, this applies for the design of low-pass ﬁlters using
the Chebyshev or the elliptic design methods. To include the design of low-pass ﬁlters using the
Butterworth, Chebyshev (two versions), and the elliptic methods we wrote the function analogﬁl.
function [b, a] = analogﬁl(Wp, Ws, alphamax, alphamin, Wmax, ind)
%%
%
Analog ﬁlter design
%
Parameters
%
Input: loss speciﬁcations (alphamax, alphamin), corresponding
%
frequencies (Wp,Ws), frequency range [0,Wmax] and indicator ind (1 for
%
Butterworth, 2 for Chebyshev1, 3 for Chebyshev2 and 4 for elliptic).
%
Output: coefﬁcients of designed ﬁlter.
%
Function plots magnitude, phase responses, poles and zeros of ﬁlter, and
%
loss speciﬁcations
%%%
if ind == 1,% Butterworth low-pass
[N, Wn] = buttord(Wp, Ws, alphamax, alphamin, ’s’)
[b, a] = butter(N, Wn, ’s’)
elseif ind == 2, % Chebyshev low-pass
[N, Wn] = cheb1ord(Wp, Ws, alphamax, alphamin, ’s’)
[b, a] = cheby1(N, alphamax, Wn, ’s’)
elseif ind == 3, % Chebyshev2 low-pass
[N, Wn] = cheb2ord(Wp, Ws, alphamax, alphamin, ’s’)
[b, a] = cheby2(N, alphamin, Wn, ’s’)
else % Elliptic low-pass
[N, Wn] = ellipord(Wp, Ws, alphamax, alphamin, ’s’)
[b, a] = ellip(N, alphamax, alphamin, Wn, ’s’)
end

406
CHAPTER 6:
Application to Control and Communications
W = 0:0.001:Wmax; % frequency range for plotting
H = freqs(b, a, W); Hm = abs(H); Ha = unwrap(angle(H)) % magnitude (Hm) and phase (Ha)
N = length(W); alpha1 = alphamax ∗ones(1, N); alpha2 = alphamin ∗ones(1, N); % loss specs
subplot(221)
plot(W, Hm); grid; axis([0 Wmax 0 1.1 ∗max(Hm)])
subplot(222)
plot(W, Ha); grid; axis([0 Wmax 1.1 ∗min(Ha) 1.1 ∗max(Ha)])
subplot(223)
splane(b, a)
subplot(224)
plot(W, −20 ∗log10(abs(H))); hold on
plot(W, alpha1, ’r’, W, alpha2, ’r’); grid; axis([0 max(W) −0.1 100])
hold off
I Example 6.11
To illustrate the use of analogﬁl consider the design of low-pass ﬁlters using the Chebyshev2 and
the Elliptic design methods. The speciﬁcations for the designs are
α(0) = 0,
αmax = 0.1,
αmin = 60 dB
p = 10,
s = 15 rad/sec
We wish to ﬁnd the coefﬁcients of the designed ﬁlters, plot their magnitude and phase, and plot
the loss function for each of the ﬁlters and verify that the speciﬁcations have been met. The results
are shown in Figure 6.26.
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
Ω
|H(Ω)|
−4
−2
0
2
−20
−10
0
10
20
σ
jΩ
0
5
10
15
20
25
−10
−8
−6
−4
−2
0
Ω
<H(Ω)
0
5
10
15
20
25
0
20
40
60
80
100
Ω
α(Ω)
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
Ω
0
5
10
15
20
25
−10
−8
−6
−4
−2
0
Ω
|H(Ω)|
<H(Ω)
−15
−10
−5
0
−50
0
50
σ
0
5
10
15
20
25
0
20
40
60
80
100
Ω
jΩ
α(Ω)
(a)
(b)
FIGURE 6.26
(a) Elliptic and (b) Chebyshev2 low-pass ﬁlter designs using analogﬁl function. Clockwise: magnitude, phase,
loss function, and poles and zeros are shown for each design.

6.5 Analog Filtering
407
%%%%%%%%%%%%%%%%%%%
% Example 6.11 -- Filter design using analogﬁl
%%%%%%%%%%%%%%%%%%%
clear all; clf
alphamax = 0.1;
alphamin = 60;
Wp =10; Ws = 15;
Wmax = 25;
ind = 4 % elliptic design
% ind = 3 % chebyshev2 design
[b, a] = analogﬁl(Wp, Ws, alphamax, alphamin, Wmax, ind)
The elliptic design is illustrated above. To obtain the Chebyshev2 design get rid of the comment
symbol % in front of the corresponding indicator and put it in front of the one for the elliptic
design.
I
General comments on the design of low-pass ﬁlters using Butterworth, Chebyshev (1 and 2), and
Elliptic methods are:
I
The Butterworth and the Chebyshev2 designs are ﬂat in the passband, while the others display
ripples in that band.
I
For identical speciﬁcations, the obtained order of the Butterworth ﬁlter is much greater than the
order of the other ﬁlters.
I
The phase of all of these ﬁlters is approximately linear in the passband, but not outside it. Because
of the rational transfer functions for these ﬁlters, it is not possible to have linear phase over all
frequencies. However, the phase response is less signiﬁcant in the stopband where the magnitude
response is very small.
I
The ﬁlter design functions provided by MATLAB can be used for analog or discrete ﬁlters. When
designing an analog ﬁlter there is no constrain in the values of the frequency speciﬁcations and
an ’s’ indicates that the ﬁlter being designed is analog.
General Filter Design
The ﬁlter design programs butter, cheby1, cheby2, and ellip allow the design of other ﬁlters besides
low-pass ﬁlters. Conceptually, a prototype low-pass ﬁlter is designed and then transformed into the
desired ﬁlter by means of the frequency transformations given before. The ﬁlter is speciﬁed by the
order and cut-off frequencies. In the case of low-pass and high-pass ﬁlters the speciﬁed cut-off fre-
quencies are scalar, while for band-pass and stopband ﬁlters the speciﬁed cut-off frequencies are given
as a vector. Also recall that the frequency transformations double the order of the low-pass prototype
for the band-pass and band-eliminating ﬁlters, so when designing these ﬁlters half of the desired
order should be given.
I Example 6.12
To illustrate the general design consider:
(a)
Using the cheby2 method, design a band-pass ﬁlter with the following speciﬁcations:
I
order N = 10
I
α() = 60 dB in the stopband

408
CHAPTER 6:
Application to Control and Communications
I
passband frequencies [10, 20] rad/sec
I
unit gain in the passband
(b)
Using the ellip method, design a band-stop ﬁlter with unit gain in the passbands and the
following speciﬁcations:
I
order N = 20
I
α() = 0.1 dB in the passband
I
α() = 40 dB in the stopband
I
passband frequencies [10, 11] rad/sec
The following script is used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 6.12 --- general ﬁlter design
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
clear all;clf
N = 10;
[b, a] = ellip(N/2, 0.1, 40, [10 11], ’stop’, ’s’) % elliptic band-stop
%[b, a] = cheby2(N, 60, [10 20], ’s’) % cheby2 bandpass
W = 0:0.01:30;
H = freqs(b, a, W);
Notice that the order given to ellip is 5 and 10 to cheby2 since a quadratic transformation will be
used to obtain the notch and the band-pass ﬁlters from a prototype low-pass ﬁlter. The magnitude
and phase responses of the two designed ﬁlters are shown in Figure 6.27.
I
0
5
10
15
20
25
30
0
0.2
0.4
0.6
0.8
1
Ω
|H(Ω)|
0
5
10
15
20
25
30
−10
−5
0
5
10
Ω
<H(Ω)
0
5
10
15
20
25
30
0
0.2
0.4
0.6
0.8
1
Ω
|H(Ω)|
0
5
10
15
20
25
30
−10
−5
0
5
10
Ω
<H(Ω)
(a)
(b)
FIGURE 6.27
Design of (a) a notch ﬁlter using ellip and of (b) a band-pass ﬁlter using cheby2.

Problems
409
6.6 WHAT HAVE WE ACCOMPLISHED? WHAT IS NEXT?
In this chapter we have illustrated the application of the Laplace and the Fourier analysis to the
theories of control, communications, and ﬁltering. As you can see, the Laplace transform is very
appropriate for control problems where transients as well as steady-state responses are of interest.
On the other hand, in communications and ﬁltering there is more interest in steady-state responses
and frequency characterizations, which are more appropriately treated using the Fourier transform.
It is important to realize that stability can only be characterized in the Laplace domain, and that it
is necessary when considering steady-state responses. The control examples show the importance of
the transfer function and transient and steady-state computations. Block diagrams help to visualize
the interconnection of the different systems. Different types of modulation systems are illustrated
in the communication examples. Finally, this chapter provides an introduction to the design of
analog ﬁlters. In all the examples, the application of MATLAB was illustrated.
Although the material in this chapter does not have sufﬁcient depth, reserved for texts in control,
communications, and ﬁltering, it serves to connect the theory of continuous-time signals and systems
with applications. In the next part of the book, we will consider how to process signals using comput-
ers and how to apply the resulting theory again in control, communications, and signal processing
problems.
PROBLEMS
6.1. Cascade implementation and loading
The transfer function of a ﬁlter H(s) = 1/(s + 1)2 is to be implemented by cascading two ﬁrst-order ﬁlters
Hi(s) = 1/(s + 1), i = 1, 2.
(a) Implement Hi(s) as a series RC circuit with input vi(t) and output vi+1(t), i = 1, 2. Cascade two of
these circuits and ﬁnd the overall transfer function V3(s)/V1(s). Carefully draw the circuit.
(b) Use a voltage follower to connect the two circuits when cascaded and ﬁnd the overall transfer function
V3(s)/V1(s). Carefully draw the circuit.
(c) Use the voltage follower circuit to implement a new transfer function
G(s) =
1
(s + 1000)(s + 1)
Carefully draw your circuit.
6.2. Cascading LTI and LTV systems
The receiver of an AM system consists of a band-pass ﬁlter, a demodulator, and a low-pass ﬁlter. The
received signal is
r(t) = m(t) cos(40000πt) + q(t)
where m(t) is a desired voice signal with bandwidth BW = 5 KHz that modulates the carrier
cos(40, 000πt) and q(t) is the rest of the signals available at the receiver. The low-pass ﬁlter is ideal with
magnitude 1 and bandwidth BW. Assume the band-pass ﬁlter is also ideal and that the demodulator is
cos(ct).
(a) What is the value of c in the demodulator?
(b) Suppose we input the received signal into the band-pass ﬁlter cascaded with the demodulator and
the low-pass ﬁlter. Determine the magnitude response of the band-pass ﬁlter that allows us to recover
m(t). Draw the overall system and indicate which of the components are LTI and which are LTV.

410
CHAPTER 6:
Application to Control and Communications
(c) By mistake we input the received signal into the demodulator, and the resulting signal into the
cascade of the band-pass and the low-pass ﬁlters. If you use the band-pass ﬁlter obtained above,
determine the recovered signal (i.e., the output of the low-pass ﬁlter). Would you get the same result
regardless of what m(t) is? Explain.
6.3. Op-amps as feedback systems
An ideal operational ampliﬁer circuit can be shown to be equivalent to a negative-feedback system. Con-
sider the ampliﬁer circuit in Figure 6.28 and its two-port network equivalent circuit to obtain a feedback
system with input Vi(s) and output V0(s). What is the effect of A →∞on the above circuit?
−Av−
Ro
0
+
+
+
+
+
+
R1
R2
R1
R2
v−
v−
v+
v+
Ri
vi(t)
vi(t)
vo(t)
vo(t)
→
→
−
−
−
−
−
−
∞
FIGURE 6.28
6.4. RC circuit as feedback system
Consider a series RC circuit with input a voltage source vi(t) and output the voltage across the capacitor
vo(t).
(a) Draw a negative-feedback system for the circuit using an integrator, a constant multiplier, and an
adder.
(b) Let the input be a battery (i.e., vi(t) = Au(t)). Find the steady-state error e(t) = vi(t) −vo(t).
6.5. RLC circuit as feedback system
A resistor R, a capacitor C, and an inductor L are connected in series with a source vi(t). Consider the
output of the voltage across the capacitor vo(t). Let R = 1, C = 1 F and L = 1 H.
(a) Use integrators and adders to implement the differential equation that relates the input vi(t) and the
output vo(t) of the circuit.
(b) Obtain a negative-feedback system block diagram with input Vi(s) and output V0(s). Determine the
feedforward transfer function G(s) and the feedback transfer function H(s) of the feedback system.
(c) Find an equation for the error E(s) = Vi(s) −V0(s)H(s) and determine its steady-state response when
the input is a unit-step signal (i.e., Vi(s) = 1/s).
6.6. Ideal and lossy integrators
An ideal integrator has a transfer function 1/s, while a lossy integrator has a transfer function 1/(s + K).
(a) Determine the feedforward transfer function G(s) and the feedback transfer function H(s) of a
negative-feedback system that implements the overall transfer function
Y(s)
X(s) =
K
K + s
where X(s) and Y(s) are the Laplace transforms of the input x(t) and the output y(t) of the feedback
system. Sketch the magnitude response of this system and determine the type of ﬁlter it is.

Problems
411
(b) If we let G(s) = s in the previous feedback system, determine the overall transfer function Y(s)/X(s)
where X(s) and Y(s) are the Laplace transforms of the input x(t) and the output y(t) of this new feed-
back system. Sketch the magnitude response of the overall system and determine the type of ﬁlter
it is.
6.7. Feedback implementation of an all-pass system
Suppose you would like to obtain a feedback implementation of an all-pass ﬁlter
T(s) = s2√
2s + 1
s2√
2s + 1
(a) Determine if the T(s) is the transfer function corresponding to an all-pass ﬁlter by means of the poles
and zeros of T(s).
(b) Determine the feedforward transfer function G(s) and the feedback transfer function H(s) of a
negative-feedback system that has T(s) as its overall transfer function.
(c) Would it be possible to implement T(s) using a positive-feedback system? If so, indicate its
feedforward transfer function G(s) and the feedback transfer function H(s).
6.8. Filter stabilization
The transfer function of a designed ﬁlter is
G(s) =
1
(s + 1)(s −1)
which is unstable given that one of its poles is in the right-hand s-plane.
(a) Consider stabilizing G(s) by means of negative feedback with a gain K > 0 in the feedback. Determine
the range of values of K that would make the stabilization possible.
(b) Use the cascading of an all-pass ﬁlter Ha(s) with the given G(s) to stabilize it. Give Ha(s). Would it be
possible for the resulting ﬁlter to have the same magnitude response as G(s)?
6.9. Error and feedforward transfer function
Suppose the feedforward transfer function of a negative-feedback system is G(s) = N(s)/D(s), and the
feedback transfer function is unity.
(a) Given that the Laplace transform of the error is
E(s) = X(s)[1 −H(s)]
where H(s) = G(s)/(1 + G(s)) is the overall transfer function of the feedback system, ﬁnd an expres-
sion for the error in terms of X(s), N(s), and D(s). Use this equation to determine the conditions under
which the steady-state error is zero for x(t) = u(t).
(b) If the input is x(t) = u(t), the denominator D(s) = (s + 1)(s + 2), and the numerator N(s) = 1, ﬁnd an
expression for E(s) and from it determine the initial value e(0) and the ﬁnal value limt→∞e(t) of the
error.
6.10. Product of polynomials in s—MATLAB
Given a transfer function
Y(s)
X(s) = N(s)
D(s)
where Y(s) and X(s) are the Laplace transforms of the output y(t) and of the input x(t) of an LTI system,
and N(s) and D(s) are polynomials in s, to ﬁnd the output
Y(s) = X(s)N(s)
D(s)

412
CHAPTER 6:
Application to Control and Communications
we need to multiply polynomials to get Y(s) before we perform partial fraction expansion to get y(t).
(a) Find out about the MATLAB function conv and how it relates to the multiplication of polynomials.
Let P(s) = 1 + s + s2 and Q(s) = 2 + 3s + s2 + s3. Obtain analytically the product Z(s) = P(s)Q(s) and
then use conv to compute the coefﬁcients of Z(s).
(b) Suppose that X(s) = 1/s2, and we have N(s) = s + 1, D(s) = (s + 1)((s + 4)2 + 9). Use conv to ﬁnd
the numerator and the denominator polynomials of Y(s) = N1(s)/D1(s). Use MATLAB to ﬁnd y(t), and
to plot it.
(c) Create a function that takes as input the values of the coefﬁcients of the numerators and denominators
of X(s) and of the transfer function H(s) of the system and provides the response of the system. Show
your function, and demonstrate its use with the X(s) and H(s) given above.
6.11. Feedback error—MATLAB
Control systems attempt to follow the reference signal at the input, but in many cases they cannot follow
particular types of inputs. Let the system we are trying to control have a transfer function G(s), and the
feedback transfer function be H(s). If X(s) is the Laplace transform of the reference input signal, and Y(s)
the Laplace transform of the output, then the close-loop transfer function is
Y(s)
X(s) =
G(s)
1 + G(s)H(s)
The Laplace transform of the error signal is E(s) = X(s) −Y(s)H(s),
G(s) =
1
s(s + 1)(s + 2)
and
H(s) = 1
(a) Find an expression for E(s) in terms of X(s), G(s), and H(s).
(b) Let x(t) = u(t) and the Laplace transform of the corresponding error be E1(s). Use the ﬁnal value
property of the Laplace transform to obtain the steady-state error e1ss.
(c) Let x(t) = tu(t) (i.e., a ramp signal) and E2(s) be the Laplace transform of the corresponding error
signal. Use the ﬁnal value property of the Laplace transform to obtain the steady-state error e2ss. Is
this error value larger than the one above? Which of the two inputs u(t) and r(t) is easier to follow?
(d) Use MATLAB to ﬁnd the partial fraction expansions of E1(s) and E2(s) and use them to ﬁnd e1(t) and
e2(t) and then plot them.
6.12. Wireless transmission—MATLAB
Consider the transmission of a sinusoid x(t) = cos(2πf0t) through a channel affected by multipath and
Doppler. Let there be two paths, and assume the sinusoid is being sent from a moving transmitter so that
a Doppler frequency shift occurs. Let the received signal be
r(t) = α0 cos(2π(f0 −ν)(t −L0/c)) + α1 cos(2π(f0 −ν)(t −L1/c))
where 0 ≤αi ≤1 are attenuations, Li are the distances from the transmitter to the receiver that the signal
travels in the ith path i = 1, 2, c = 3 × 108 m/sec, and the frequency shift ν is caused by the Doppler
effect.
(a) Let f0 = 2 KHz, ν = 50 Hz, α0 = 1, α1 = 0.9, and L0 = 10,000 meters. What would be L1 if the two
sinusoids have a phase difference of π/2?
(b) Is the received signal r(t), with the parameters given above but L1 = 10,000, periodic? If so, what
would be its period and how much does it differ from the period of the original sinusoid? If x(t) is
the input and r(t) the output of the transmission channel, considered a system, is it linear and time
invariant? Explain.
(c) Sample the signals x(t) and r(t) using a sampling frequency Fs = 10 KHz. Plot the sampled sent x(nTs)
and received r(nTs) signals for n = 0 to 2000.

Problems
413
(d) Consider the situation where f0 = 2 KHz, but the parameters of the paths are random, trying to
simulate real situations where these parameters are unpredictable, although somewhat related. Let
r(t) = α0 cos(2π( f0 −ν)(t −L0/c)) + α1 cos(2π( f0 −ν)(t −L1/c))
where ν = 50η Hz, L0 = 1,000η, L1 = 10,000η, α0 = 1 −η, α1 = α0/10, and η is a random number
between 0 and 1 with equal probability of being any of these values (this can be realized by using the
rand MATLAB function). Generate the received signal for 10 different events, use Fs = 10,000 Hz as
the sampling rate, and plot them together to observe the effects of the multipath and Doppler.
6.13. RLC implementation of low-pass Butterworth ﬁlters
Consider the RLC circuit shown in Figure 6.29 where R = 1 .
(a) Determine the values of the inductor and the capacitor so that the transfer function of the circuit when
the output is the voltage across the capacitor is
Vo(s)
Vi(s) =
1
s2 +
√
2s + 1
That is, it is a second-order Butterworth ﬁlter.
(b) Find the transfer function of the circuit, with the values obtained in (a) for the capacitor and the induc-
tor, when the output is the voltage across the resistor. Carefully sketch the corresponding frequency
response and determine the type of ﬁlter it is.
FIGURE 6.29
L
C
R =1Ω
vi(t)
+
−
6.14. Design of low-pass Butterworth/Chebyshev ﬁlters
The speciﬁcations for a low-pass ﬁlter are:
I
p = 1500 rad/sec, αmax = 0.5 dBs
I
s = 3500 rad/sec, αmin = 30 dBs
(a) Determine the minimum order of the low-pass Butteworth ﬁlter and compare it to the minimum
order of the Chebyshev ﬁlter that satisfy the speciﬁcations. Which is the smaller of the two?
(b) Determine the half-power frequencies of the designed Butterworth and Chebyshev low-pass
ﬁlters by letting α(p) = αmax. Use the minimum orders obtained above.
(c) For the Butterworth and the Chebyshev designed ﬁlters, ﬁnd the loss function values at p and
s. How are these values related to the αmax and αmin speciﬁcations? Explain.
(d) If new speciﬁcations for the passband and stopband frequencies are p = 750 rad/sec and s =
1750 rad/sec, respectively, are the minimum orders of the Butterworth and the Chebyshev ﬁlters
changed? Explain.
6.15. Low-pass Butterworth ﬁlters
The loss at a frequency  = 2000 rad/sec is α(2000) = 19.4 dBs for a ﬁfth-order low-pass Butterworth
ﬁlter. If we let α(p) = αmax = 0.35 dBs, determine

414
CHAPTER 6:
Application to Control and Communications
I
The half-power frequency hp of the ﬁlter.
I
The passband frequency p of the ﬁlter.
6.16. Design of low-pass Butterworth/Chebyshev ﬁlters
The speciﬁcations for a low-pass ﬁlter are:
I
α(0) = 20 dBs
I
p = 1500 rad/sec, α1 = 20.5 dBs
I
s = 3500 rad/sec, α2 = 50 dBs
(a) Determine the minimum order of the low-pass Butterworth and Chebyshev ﬁlters, and determine
which is smaller.
(b) Give the transfer function of the designed low-pass Butterworth and Chebyshev ﬁlters (make sure the
dc loss is as speciﬁed).
(c) Determine the half-power frequency of the designed ﬁlters by letting α(p) = αmax.
(d) Find the loss function values provided by the designed ﬁlters at p and s. How are these val-
ues related to the αmax and αmin speciﬁcations? Explain. Which of the two ﬁlters provides more
attenuation in the stopband?
(e) If new speciﬁcations for the passband and stopband frequencies are p = 750 rad/sec and s =
1750 rad/sec, respectively, are the minimum orders of the ﬁlter changed? Explain.
6.17. Butterworth, Chebyshev, and Elliptic ﬁlters—MATLAB
Design an analog low-pass ﬁlter satisfying the following magnitude speciﬁcations:
I
αmax = 0.5 dB; αmin = 20 dB
I
p = 1000 rad/sec; s = 2000 rad/sec
(a) Use the Butterworth method. Plot the poles and zeros and the magnitude and phase of the
designed ﬁlter. Verify that the speciﬁcations are satisﬁed by plotting the loss function.
(b) Use the Chebyshev method cheby1. Plot the poles and zeros and the magnitude and phase of the
designed ﬁlter. Verify that the speciﬁcations are satisﬁed by plotting the loss function.
(c) Use the elliptic method. Plot the poles and zeros and the magnitude and phase of the designed
ﬁlter. Verify that the speciﬁcations are satisﬁed by plotting the loss function.
(d) Compare the three ﬁlters and comment on their differences.
6.18. Chebyshev ﬁlter design—MATLAB
Consider the following low-pass ﬁlter speciﬁcations:
I
αmax = 0.1 dB; αmin = 60 dB
I
p = 1000 rad/sec; s = 2000 rad/sec
(a) Use MATLAB to design a Chebyshev low-pass ﬁlter that satisﬁes the above speciﬁcations. Plot the
poles and zeros and the magnitude and phase of the designed ﬁlter. Verify that the speciﬁcations
are satisﬁed by plotting the loss function.
(b) Compute the half-power frequency of the designed ﬁlter.
6.19. Getting rid of 60-Hz hum with different ﬁlters—MATLAB
A desirable signal
x(t) = cos(100πt) −2 cos(50πt)
is recorded as y(t) = x(t) + cos(120πt)—that is, as the desired signal but with a 60-Hz hum. We would
like to get rid of the hum and recover the desired signal. Use symbolic MATLAB to plot x(t) and y(t).
Consider the following three different alternatives (use symbolic MATLAB to implement the ﬁltering and
use any method to design the ﬁlters):
(a) Design a band-eliminating ﬁlter to get rid of the 60-Hz hum in the signal. Plot the output of the
band-eliminating ﬁlter.
(b) Design a high-pass ﬁlter to get the hum signal and then subtract it from y(t). Plot the output of the
high-pass ﬁlter.

Problems
415
(c) Design a band-pass ﬁlter to get rid of the hum. Plot the output of the band-pass ﬁlter.
(d) Is any of these alternatives better than the others? Explain.
6.20. Demodulation of AM—MATLAB
The signal at the input of an AM receiver is
u(t) = m1(t) cos(20t) + m2(t) cos(100t)
where the messages mi(t), i = 1, 2 are the outputs of a low-pass Butterworth ﬁlter with inputs
x1(t) = r(t) −2r(t −1) + r(t −2)
x2(t) = u(t) −u(t −2)
respectively. Suppose we are interested in recovering the message m1(t).
(a) Design a 10th-order low-pass Butterworth ﬁlter with half-power 10 rad/sec. Implement this ﬁlter using
MATLAB and ﬁnd the two messages mi(t), i = 1, 2 using the indicated inputs xi(t), i = 1, 2, and
plot them.
(b) To recover the desired message m1(t), ﬁrst use a band-pass ﬁlter to obtain the desired signal con-
taining m1(t) and to suppress the other. Design a band-pass Butterworth ﬁlter with a bandwidth of
10 rad/sec, centered at 20 rad/sec and order 10 that will pass the signal m1(t) cos(20t) and reject the
other signal.
(c) Multiply the output of the band-pass ﬁlter by a sinusoid cos(20t) (exactly the carrier in the transmitter),
and low-pass ﬁlter the output of the mixer (the system that multiplies by the carrier frequency cosine).
Design a low-pass Butterworth ﬁlter of bandwidth 10 rad/sec, and order 10 to ﬁlter the output of the
mixer.
(d) Use MATLAB to display the different spectra. Compute and plot the spectrum of m1(t), u(t), the output
of the band-pass ﬁlter, the output of the mixer, and the output of the low-pass ﬁlter. Write numeric
functions to compute the analog Fourier transform and its inverse.
6.21. Quadrature AM—MATLAB
Suppose we would like to send the two messages mi(t), i = 1, 2, created in Problem 6.20 using the same
bandwidth and to recover them separately. To implement this, consider the QAM approach where the
transmitted signal is
s(t) = m1(t) cos(50t) + m2(t) sin(50t)
Suppose that at the receiver we receive s(t) and that we only need to demodulate it to obtain mi(t), i = 1, 2.
Design a low-pass Butterworth ﬁlter of order 10 and a half-power frequency 10 rad/sec (the bandwidth of
the messages).
(a) Use MATLAB to plot s(t) and its magnitude spectrum |S()|. Write numeric functions to compute the
analog Fourier transform and its inverse.
(b) Multiply s(t) by cos(50t), and ﬁlter the result using the low-pass ﬁlter designed before. Use MATLAB
to plot the result and to ﬁnd and plot its magnitude spectrum.
(c) Multiply s(t) by sin(50t), and ﬁlter the result using the low-pass ﬁlter designed before. Use MATLAB
to plot the result and to ﬁnd and plot its magnitude spectrum.
(d) Comment on your results.

This page intentionally left blank

3
PART
Theory and Application of
Discrete-Time Signals and Systems

This page intentionally left blank

CHAPTER 7
Sampling Theory
The pure and simple truth
is rarely pure and never simple.
Oscar Wilde (1854–1900)
Irish writer and poet
7.1 INTRODUCTION
Since many of the signals found in applications such as communications and control are analog, if
we wish to process these signals with a computer it is necessary to sample, quantize, and code them
to obtain digital signals. Once the analog signal is sampled in time, the amplitude of the obtained
discrete-time signal is quantized and coded to give a binary sequence that can be either stored or
processed with a computer.
The main issues considered in this chapter are:
I
How to sample—As we will see, it is the inverse relation between time and frequency that provides
the solution to the problem of preserving the information of an analog signal when it is sampled.
When sampling an analog signal one could choose an extremely small value for the sampling
period so that there is no signiﬁcant difference between the analog and the discrete signals—
visually as well as from the information content point of view. Such a representation would,
however, give redundant values that could be spared without losing the information provided
by the analog signal. If, on the other hand, we choose a large value for the sampling period,
we achieve data compression but at the risk of losing some of the information provided by the
analog signal. So how do we choose an appropriate value for the sampling period? The answer is
not clear in the time domain. It does become clear when considering the effects of sampling in
the frequency domain: The sampling period depends on the maximum frequency present in the
analog signal. Furthermore, when using the correct sampling period the information in the analog
signal will remain in the discrete signal after sampling, thus allowing the reconstruction of the
original signal from the samples. These results, introduced by Nyquist and Shannon, constitute
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00011-9
c⃝2011, Elsevier Inc. All rights reserved.
419

420
CHAPTER 7:
Sampling Theory
the bridge between analog and discrete signals and systems and were the starting point for digital
signal processing as a technical area.
I
Practical aspects of sampling—The device that samples, quantizes, and codes an analog signal is
called an analog-to-digital converter (ADC), while the device that converts digital signals into ana-
log signals is called a digital-to-analog converter (DAC). These devices are far from ideal and thus
some practical aspects of sampling and reconstruction need to be considered. Besides the pos-
sibility of losing information by choosing too large of a sampling period, the ADC also loses
information in the quantization process. The quantization error is, however, made less signiﬁ-
cant by increasing the number of bits used to represent each sample. The DAC interpolates and
smooths out the digital signal, converting it back into an analog signal. These two devices are
essential in the processing of continuous-time signals with computers.
7.2 UNIFORM SAMPLING
The ﬁrst step in converting a continuous-time signal x(t) into a digital signal is to discretize the time
variable—that is, to consider samples of x(t) at uniform times t = nTs, or
x(nTs) = x(t)|t=nTs
n integer
(7.1)
where Ts is the sampling period. The sampling process can be thought of as a modulation process, in
particular connected with pulse amplitude modulation (PAM), a basic approach in digital communi-
cations. A pulse amplitude modulated signal consists of a sequence of narrow pulses with amplitudes
the values of the continuous-time signal within the pulse. Assuming that the width of the pulses is
much narrower than the sampling period Ts permits a simpler analysis based on impulse sampling.
7.2.1 Pulse Amplitude Modulation
A PAM system can be visualized as a switch that closes every Ts seconds for 1 seconds, and remains
open otherwise. The PAM signal is thus the multiplication of the continuous-time signal x(t) by a
periodic signal p(t) consisting of pulses of width 1, amplitude 1/1, and period Ts. Thus, xPAM(t)
consists of narrow pulses with the amplitudes of the signal within the pulse width. For a small pulse
width 1, the PAM signal is approximately a train of pulses with amplitudes x(mTs)—that is,
xPAM(t) = x(t)p(t) ≈1
1
X
m
x(mTs)[u(t −mTs) −u(t −mTs −1)]
(7.2)
Now, as a periodic signal we represent p(t) by its Fourier series
p(t) =
X
k
Pkejk0t
0 = 2π
Ts
where Pk are the Fourier series coefﬁcients. Thus, the PAM signal can be expressed as
xPAM(t) =
X
k
Pk x(t) ejk0t

7.2 Uniform Sampling
421
and its Fourier transform is
XPAM() =
X
k
PkX( −k0)
showing that PAM is a modulation of the train of pulses p(t) by the signal x(t). The spectrum of
xPAM(t) is the spectrum of x(t) shifted in frequency by {k0}, weighted by Pk, and superposed.
7.2.2 Ideal Impulse Sampling
Given that the pulse width 1 is much smaller than Ts, p(t) can be replaced by a periodic sequence of
impulses of period Ts (see Figure 7.1) or δTs(t). This simpliﬁes considerably the analysis and makes
the results easier to grasp. Later in the chapter we consider the effects of having pulses instead of
impulses, a more realistic assumption.
The sampling function δTs(t), or a periodic sequence of impulses of period Ts, is
δTs(t) =
X
n
δ(t −nTs)
(7.3)
where δ(t −nTs) is an approximation of the normalized pulse [u(t −nTs) −u(t −nTs −1)]/1 when
1 << Ts. The sampled signal is then given by
xs(t) = x(t)δTs(t)
=
X
n
x(nTs)δ(t −nTs)
(7.4)
as illustrated in Figure 7.1.
There are two equivalent ways to view the sampled signal xs(t) in the frequency domain:
I
Modulation: Since δTs(t) is periodic, of fundamental frequency s = 2π/Ts, its Fourier series is
δTs(t) =
∞
X
k=−∞
Dkejkst
FIGURE 7.1
Ideal impulse sampling.
t
t
t
×
0
0
0
x(t)
xs(t)
2Ts
Ts
2Ts
Ts
−Ts
···
···
δTs(t)

422
CHAPTER 7:
Sampling Theory
where the Fourier coefﬁcients {Dk} are
Dk = 1
Ts
Ts/2
Z
−Ts/2
δTs(t)e−jkstdt = 1
Ts
Ts/2
Z
−Ts/2
δ(t)e−jkstdt
= 1
Ts
Ts/2
Z
−Ts/2
δ(t)e−j0dt = 1
Ts
The last equation is obtained using the sifting property of the δ(t) and that the area of the impulse
is unity. Thus, the Fourier series of the sampling signal is
δTs(t) =
∞
X
k=−∞
1
Ts
ejkst
(7.5)
and the sampled signal xs(t) = x(t)δTs(t) is then expressed as
xs(t) = 1
Ts
∞
X
k=−∞
x(t)ejkst
with Fourier transform
Xs() = 1
Ts
∞
X
k=−∞
X( −ks)
(7.6)
where we used the frequency-shift property of the Fourier transform, and let X() and Xs() be
the Fourier transforms of x(t) and xs(t), respectively.
I
Discrete-time Fourier transform: The Fourier transform of the sum representation of xs(t) in the
second equation in Equation (7.4) is
Xs() =
X
n
x(nTs)e−jTsn
(7.7)
where we used the Fourier transform of a shifted impulse. This equation is equivalent to Equa-
tion (7.6) and will be used later in deriving the Fourier transform of discrete-time signals.
Remarks
I
The spectrum Xs() of the sampled signal, according to Equation (7.6), is a superposition of shifted
analog spectra {X( −ks)} multiplied by 1/Ts (i.e., the modulation process involved in the sampling).
I
Considering that the output of the sampler displays frequencies that are not present in the input, according
to the eigenfunction property the sampler is not LTI. It is a time-varying system. Indeed, if sampling
x(t) gives xs(t), sampling x(t −τ) where τ ̸= kTs for an integer k will not be xs(t −τ). The sampler is,
however, a linear system.

7.2 Uniform Sampling
423
I
Equation (7.7) provides the relation between the continuous frequency  (rad/sec) of x(t) and the discrete
frequency ω (rad) of the discrete-time signal x(nTs) or x[n]1:
ω = Ts
[rad/sec] × [sec] = [rad]
Sampling a continuous-time signal x(t) at uniform times {nTs} gives a sampled signal
xs(t) =
X
n
x(nTs)δ(t −nTs)
(7.8)
or a sequence of samples {x(nTs)}. Sampling is equivalent to modulating the sampling signal
δTs(t) =
X
n
δ(t −nTs)
(7.9)
periodic of period Ts (the sampling period) with x(t).
If X() is the Fourier transform of x(t), the Fourier transform of the sampled signal xs(t) is given by the
equivalent expressions
Xs() = 1
Ts
X
k
X( −ks)
=
X
n
x(nTs)e−jTsn
s = 2π
Ts
(7.10)
Depending on the maximum frequency present in the spectrum of x(t) and on the chosen sampling frequency
s (or the sampling period Ts) it is possible to have overlaps when the spectrum of x(t) is shifted and added
to obtain the spectrum of the sampled signal. We have three possible situations:
I
If the signal has a low-pass spectrum of ﬁnite support—that is, X() = 0 for || > max (see
Figure 7.2(a)) where max is the maximum frequency present in the signal—such a signal is called
band limited. As shown in Figure 7.2(b), for band-limited signals it is possible to choose s so that the
spectrum of the sampled signal consists of shifted nonoverlapping versions of (1/Ts)X(). Graphically (see
Figure 7.2(b)), this can be accomplished by letting s −max ≥max, or
s ≥2max
which is called the Nyquist sampling rate condition. As we will see later, in this case we are able
to recover X(), or x(t), from Xs() or from the sampled signal xs(t). Thus, the information in x(t) is
preserved in the sampled signal xs(t).
I
On the other hand, if the signal x(t) is band limited but we let s < 2max, then when creating Xs()
the shifted spectra of x(t) overlap (see Figure 7.2(c)). In this case, due to the overlap it will not be
1To help the reader visualize the difference between a continuous-time signal, which depends on a continuous variable t, or a real
number, and a discrete-time signal, which depends on the integer variable n, we will use square brackets for these. Thus, η(t) is a
continuous-time signal, while ρ[n] is a discrete-time signal.

424
CHAPTER 7:
Sampling Theory
(a)
(b)
(c)
1
−Ωmax
Ωmax
X(Ω)
Ω
No aliasing
Ωmax
Ωs≥2 Ωmax
Xs(Ω)
1/Ts
Ωs
Ω
· · ·
· · ·
Aliasing
Ωs<2 Ωmax
Xs(Ω)
1/Ts
Ωs − Ωmax
Ωs= Ωmax
Ω
· · ·
· · ·
FIGURE 7.2
(a) Spectrum of band-limited signal, (b) spectrum of sampled signal when satisfying the Nyquist sampling rate
condition, and (c) spectrum of sampled signal with aliasing (superposition of spectra, shown in dashed lines,
gives a constant shown by continuous line).
possible to recover the original continuous-time signal from the sampled signal, and thus the sampled
signal does not share the same information with the original continuous-time signal. This phenomenon is
called frequency aliasing since due to the overlapping of the spectra some frequency components of the
original continuous-time signal acquire a different frequency value or an “alias.”
I
When the spectrum of x(t) does not have a ﬁnite support (i.e., the signal is not band limited) sampling
using any sampling period Ts generates a spectrum of the sampled signal consisting of overlapped shifted
spectra of x(t). Thus, when sampling non-band-limited signals frequency aliasing is always present. The
only way to sample a non-band-limited signal x(t) without aliasing—at the cost of losing information
provided by the high-frequency components of x(t) — is by obtaining an approximate signal xa(t) that
lacks the high-frequency components of x(t), thus permitting us to determine a maximum frequency for it.
This is accomplished by antialiasing ﬁltering commonly used in samplers.
A band-limited signal x(t)—that is, its low-pass spectrum X() is such that
|X()| = 0 for || > max
(7.11)

7.2 Uniform Sampling
425
where max is the maximum frequency in x(t)—can be sampled uniformly and without frequency aliasing
using a sampling frequency
s = 2π
Ts
≥2max
(7.12)
This is called the Nyquist sampling rate condition.
I Example 7.1
Consider the signal x(t) = 2 cos(2πt + π/4), −∞< t < ∞. Determine if it is band limited or not.
Use Ts = 0.4, 0.5, and 1 sec/sample as sampling periods, and for each of these ﬁnd out whether
the Nyquist sampling rate condition is satisﬁed and if the sampled signal looks like the original
signal or not.
Solution
Since x(t) only has the frequency 2π, it is band limited with max = 2π rad/sec. For any Ts the
sampled signal is given as
xs(t) =
∞
X
n=−∞
x(nTs)δ(t −nTs)
Ts sec/sample
(7.13)
with x(nTs) = x(t)|t=nTs.
Using Ts = 0.4 sec/sample the sampling frequency in rad/sec is s = 2π/Ts = 5π > 2max = 4π,
satisfying the Nyquist sampling rate condition. The samples in Equation (7.13) are then
x(nTs) = 2 cos(2π 0.4n + π/4) = 2 cos
4π
5 n + π
4

−∞< n < ∞
The sampled signal xs(t) repeats periodically every ﬁve samples. Indeed, for Ts = 0.4,
xs(t + 5Ts) =
∞
X
n=−∞
x(nTs)δ(t −(n −5)Ts)
letting m = n −5
=
∞
X
m=−∞
x((m + 5)Ts)δ(t −mTs) = xs(t)
since x((m + 5)Ts) = x(mTs). Looking at Figure 7.3(b), we see that there are three samples in each
period of the analog sinusoid, and it is not obvious that the information of the continuous-time
signal is preserved. We will show in the next section that it is actually possible to recover x(t) from
this sampled signal xs(t), which allows us to say that xs(t) has the same information as x(t).
When Ts = 0.5 the sampling frequency is s = 2π/Ts = 4π = 2max, barely satisfying the Nyquist
sampling rate condition. The samples in Equation (7.13) are now
x(nTs) = 2 cos(2πn0.5 + π/4) = 2 cos
2π
2 n + π
4

−∞< n < ∞

426
CHAPTER 7:
Sampling Theory
0
1
2
3
−2
−1
0
1
2
x(t), x(0.2n)
(a)
0
1
2
3
−2
−1
0
1
2
x(t), x(0.4 n)
(b)
0
1
2
3
−2
−1
0
1
2
t
x(t), x(0.5n)
(c)
0
1
2
3
−2
−1
0
1
2
t
x(t), x(n)
(d)
FIGURE 7.3
Sampling of x(t) = 2 cos(2πt + π/4) with sampling periods (a) Ts = 0.2, (b) Ts = 0.4, (c) Ts = 0.5, and (d) Ts = 1
sec/sample.
In this case it can be shown that the sampled signal repeats periodically every two samples, since
x((n + 2)Ts) = x(nTs), which can be easily checked. According to the Nyquist sampling rate condi-
tion, this is the minimum number of samples per period allowed before we start having aliasing.
In fact, if we let s = max = 2π corresponding to the sampling period Ts = 1, the samples in
Equation (7.13) are
x(nTs) = 2 cos(2πn + π/4) = 2 cos(π/4) =
√
2
and the sampled signal is
√
2δTs(t). With Ts = 1, the sampled signal cannot be possibly con-
verted back into an analog sinusoid. Thus, we have lost the information provided by the sinusoid.
Undersampling (getting too few samples per unit time) has changed the nature of the original
signal.
We use MATLAB to plot the continuous signal and four sampled signals (see Figure 7.3) for differ-
ent values of Ts. Clearly, when Ts = 1 sec/sample there is no similarity between the analog and the
discrete signals due to frequency aliasing.
I

7.2 Uniform Sampling
427
I Example 7.2
Consider the following signals:
(a)
x1(t) = u(t + 0.5) −u(t −0.5)
(b)
x2(t) = e−tu(t)
Determine if they are band limited or not. If not, determine the frequency for which the energy
of the non-band-limited signal corresponds to 99% of its total energy and use this result to
approximate its maximum frequency.
Solution
(a)
The signal x1(t) = u(t + 0.5) −u(t −0.5) is a unit pulse signal. Clearly, this signal can be easily
sampled by choosing any value of Ts << 1. For instance, Ts = 0.01 sec would be a good value,
giving a discrete-time signal x1(nTs) = 1, for 0 ≤nTs = 0.01n ≤1 or 0 ≤n ≤100. There seems
to be no problem in sampling this signal; however, we have that the Fourier transform of x1(t),
X1() = ej0.5 −e−j0.5
j
= sin(0.5)
0.5
does not have a maximum frequency and so x1(t) is not band limited. Thus, any chosen value
of Ts will cause aliasing. Fortunately, the values of the sinc function go fast to zero, so that
one could compute an approximate maximum frequency that covers 99% of the energy of
the signal.
Using Parseval’s energy relation we have that the energy of x1(t) (the area under x2
1(t)) is 1
and if we wish to ﬁnd a value M, such that 99% of this energy is in the frequency band
[−M, M], we need to look for the limits of the following integral so it equals 0.99:
0.99 = 1
2π
M
Z
−M
sin(0.5)
0.5
2
d
Since this integral is difﬁcult to ﬁnd analytically, we use the following script in MATLAB to
approximate it.
%%%%%%%%%%%%%%%%%%%%%
% Example 7.2 --- Parseval’s relation and sampling
%%%%%%%%%%%%%%%%%%%%%
syms W
for k = 1:23;
E(k) = int((sin(0.5*W)/(0.5*W))ˆ2,0,k*pi)/pi
if E(k)> = 0.9900,
k
return
end
end

428
CHAPTER 7:
Sampling Theory
We found that for M = 20π rad/sec 98.9% of the energy of the signal is included, and thus
it could be used to determine that Ts < π/M = 0.05 sec/sample.
(b)
For the causal exponential
x(t) = e−tu(t)
its Fourier transform is
X() =
1
1 + j
so that
|X()| =
1
√
1 + 2
which does not go to zero for any ﬁnite , then x(t) is not band limited. To ﬁnd a frequency
M so that 99% of the energy is in −M ≤ ≤M, we let
1
2π
M
Z
−M
|X()|2d = 0.99
2π
∞
Z
−∞
|X()|2d
which gives
2 tan−1()|M
0
= 2 × 0.99 tan−1()|∞
0
or
M = tan
0.99π
2

= 63.66
If we choose s = 2π/Ts = 5M or Ts = 2π/(5 × 63.66) ≈0.02, there will be hardly any
aliasing or loss of information.
I
7.2.3 Reconstruction of the Original Continuous-Time Signal
If the signal x(t) to be sampled is band limited with Fourier transform X() and maximum frequency
max, by choosing the sampling frequency s to satisfy the Nyquist sampling rate condition, or
s > 2max, the spectrum of the sampled signal xs(t) displays a superposition of shifted versions
of the spectrum of x(t), multiplied by 1/Ts, but with no overlaps. In such a case, it is possible to
recover the original analog signal from the sampled signal by ﬁltering. Indeed, if we consider an
ideal low-pass analog ﬁlter Hlp(j) with magnitude Ts in the pass-band −s/2 <  < s/2, and zero
elsewhere—that is,
Hlp(j) =
(
Ts
−s/2 <  < s/2
0
elsewhere
(7.14)
the Fourier transform of the output of the ﬁlter is Xr() = Hlp(j)Xs() or
Xr() =
(
X()
−s/2 <  < s/2
0
elsewhere
which coincides with the Fourier transform of the original signal x(t). So that when sampling a band-
limited signal, using a sampling period Ts that satisﬁes the Nyquist sampling rate, the signal can be
recovered exactly from the sampled signal by means of an ideal low-pass ﬁlter.

7.2 Uniform Sampling
429
Bandlimited or Not?
The following, taken from David Slepian’s paper “On Bandwidth” [66], clearly describes the uncertainty about bandlimited
signals:
The Dilemma—Are signals really bandlimited? They seem to be, and yet they seem not to be.
On the one hand, a pair of solid copper wires will not propagate electromagnetic waves at optical frequencies and
so the signals I receive over such a pair must be bandlimited. In fact, it makes little physical sense to talk of energy
received over wires at frequencies higher than some ﬁnite cutoff W, say 1020 Hz. It would seem, then, that signals
must be bandlimited.
On the other hand, however, signals of limited bandwith W are ﬁnite Fourier transforms,
s(t) =
W
Z
−W
e2πiftS(f )df
and irrefutable mathematical arguments show them to be extremely smooth. They possess derivatives of all orders.
Indeed, such integrals are entire functions of t, completely predictable from any little piece, and they cannot vanish
on any t interval unless they vanish everywhere. Such signals cannot start or stop, but must go on forever. Surely
real signals start and stop, and they cannot be bandlimited!
Thus we have a dilemma: to assume that real signals must go on forever in time (a consequence of bandlimit-
edness) seems just as unreasonable as to assume that real signals have energy at arbitrary high frequencies (no
bandlimitation). Yet one of these alternatives must hold if we are to avoid mathematical contradiction, for either
signals are bandlimited or they are not: there is no other choice. Which do you think they are?
Remarks
I
In practice, the exact recovery of the original signal may not be possible for several reasons. One could be
that the continuous-time signal is not exactly band limited, so that it is not possible to obtain a maximum
frequency causing frequency aliasing in the sampling. Second, the sampling is not done exactly at uniform
times—random variation of the sampling times may occur. Third, the ﬁlter required for the exact recovery
is an ideal low-pass ﬁlter, which in practice cannot be realized; only an approximation is possible. Although
this indicates the limitations of sampling, in most cases where: (1) the signal is band limited or approx-
imately band limited, (2) the Nyquist sampling rate condition is satisﬁed in the sampling, and (3) the
reconstruction ﬁlter approximates well the ideal low-pass ﬁlter, the recovered signal closely approximates
the original signal.
I
For signals that do not satisfy the band-limitedness condition, one can obtain an approximate signal that
satisﬁes that condition. This is done by passing the non-band-limited signal through an ideal low-pass
ﬁlter. The ﬁlter output is guaranteed to have as maximum frequency the cut-off frequency of the ﬁlter
(see Figure 7.4). Because of the low-pass ﬁltering, the ﬁltered signal is a smoothed version of the original
signal—high frequencies of the signal have been removed. The low-pass ﬁlter is called an antialiasing
ﬁlter, since it makes the approximate signal band limited, thus avoiding aliasing in the frequency domain.
I
In applications, the cut-off frequency of the antialiasing ﬁlter is set according to prior knowledge. For
instance, when sampling speech, it is known that speech has frequencies ranging from about 100 Hz to

430
CHAPTER 7:
Sampling Theory
FIGURE 7.4
Anti-aliasing ﬁltering
of non-band-limited
signal.
−Ωc
−Ωc
Ωc
Ωc
Ω
1
X(Ω)
Xa(Ω)
H(s)
H(jΩ)
Ω
Ω
about 5 KHz (this range of frequencies provides understandable speech in phone conversations). Thus,
when sampling speech an anti-aliasing ﬁlter with a cut-off frequency of 5 KHz is chosen and the sampling
rate is then set to 10,000 samples/sec. Likewise, it is also known that an acceptable range of frequencies
from 0 to 22 KHz provides music with good ﬁdelity, so that when sampling music signals the anti-aliasing
ﬁlter cut-off frequency is set to 22 KHz and the sampling rate to 44 K samples/sec or higher to provide
good-quality music.
Origins of the Sampling Theory—Part 1
The sampling theory has been attributed to many engineers and mathematicians. It seems as if mathematicians and
researchers in communications engineering came across these results from different perspectives. In the engineering com-
munity, the sampling theory has been attributed traditionally to Harry Nyquist and Claude Shannon, although other famous
researchers such as V. A. Kotelnikov, E. T. Whittaker, and D. Gabor came out with similar results. Nyquist’s work did not deal
directly with sampling and reconstruction of sampled signals but it contributed to advances by Shannon in those areas.
Harry Nyquist was born in Sweden in 1889 and died in 1976 in the United States. He attended the University of North
Dakota at Grand Forks and received his Ph.D. from Yale University in 1917. He worked for the American Telephone and
Telegraph (AT&T) Company and the Bell Telephone Laboratories, Inc. He received 138 patents and published 12 technical
articles. Nyquist’s contributions range from the ﬁelds of thermal noise, stability of feedback ampliﬁers, telegraphy, and
television, to other important communications problems. His theoretical work on determining the bandwidth requirements
for transmitting information provided the foundations for Claude Shannon’s work on sampling theory [33].
As Hans D. Luke [44] concludes in his paper “The Origins of the Sampling Theorem,” regarding the attribution of the
sampling theorem to many authors:
This history also reveals a process which is often apparent in theoretical problem in technology or physics: ﬁrst
the practicians put forward a rule of thumb, then theoreticians develop the general solution, and ﬁnally someone
discovers that the mathematicians have long since solved the mathematical problem which it contains, but in
“splendid isolation.”
I Example 7.3
Consider the two sinusoids
x1(t) = cos(0t)
−∞≤t ≤∞
x2(t) = cos((0 + s)t)
−∞≤t ≤∞

7.2 Uniform Sampling
431
Show that if we sample these signals using Ts = 2π/s, we cannot differentiate the sampled signals
(i.e., x1(nTs) = x2(nTs)). Use MATLAB to show the above graphically when 0 = 1 and s = 7.
Explain the signiﬁcance of this.
Solution
Sampling the two signals using Ts = 2π/s, we have
x1(nTs) = cos(0nTs)
−∞≤n ≤∞
x2(nTs) = cos((0 + s)nTs)
−∞≤n ≤∞
but since sTs = 2π, the sinusoid x2(nTs) can be written as
x2(nTs) = cos((0Ts + 2π)n)
= cos(0Tsn) = x1(nTs)
The following script shows the aliasing effect when 0 = 1 and s = 7 rad/sec. Notice that x1(t) is
sampled satisfying the Nyquist sampling rate condition (s = 7 > 20 = 2 rad/sec), while x2(t) is
not (s = 7 < 2(0 + s) = 16 rad/sec).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 7.3 ---Two sinusoids of different frequencies being sampled
% with same sampling period -- aliasing for signal with higher frequency
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
clear all; clf
% sinusoids
omega 0 = 1;omega s = 7;
T = 2 ∗pi/omega 0; t = 0:0.001:T; % a period of x1
x1 = cos(omega 0 ∗t); x2 = cos((omega 0 + omega s) ∗t);
N = length(t); Ts = 2 ∗pi/omega s; % sampling period
M = ﬁx(Ts/0.001); imp = zeros(1,N);
for k = 1:M:N −1.
imp(k) = 1; % sequence of impulses
end
xs = imp. ∗x1; % sampled signal
plot(t,x1,’b’,t,x2,’k’); hold on
stem(t,imp. ∗x1,’r’,’ﬁlled’);axis([0 max(t) −1.1 1.1]); xlabel(’t’); grid
Figure 7.5 shows the two sinusoids and the sampled signal that coincides for the two signals. The
result in the frequency domain is shown in Figure 7.6: The spectra of the two sinusoids are different
but the spectra of the sampled signals are identical.
I

432
CHAPTER 7:
Sampling Theory
FIGURE 7.5
Sampling of two sinusoids of
frequencies 0 = 1 and
0 + s = 8 with Ts = 2π/s.
The higher-frequency signal is
undersampled, causing aliasing,
which makes the two sampled
signals coincide.
0
1
2
3
4
5
6
−1
−0.5
0
0.5
1
t
x1(t), x2(t), x1(nTs)
x1(t)
x2(t)
x1(nTs)
FIGURE 7.6
(a) Spectra of sinusoids x1(t) and x2(t).
(b) The spectra of the sampled signals x1s(t)
and x2s(t) look exactly the same due to the
undersampling of x2(t).
(a)
(b)
1
8
6
6
1
1
−1
8
8
X1(Ω)
X1s(Ω)
X2(Ω)
X2s(Ω)
Ω
Ω
Ω
Ω
−8
−8 −6
−1
−8 −6
· · ·
· · ·
· · ·
· · ·
−1
7.2.4 Signal Reconstruction from Sinc Interpolation
The analog signal reconstruction from the samples can be shown to be an interpolation using sinc
signals. First, the ideal low-pass ﬁlter Hlp(s) in Equation (7.14) has as impulse response
hlp(t) = Ts
2π
s/2
Z
−s/2
ejtd = sin(πt/Ts)
πt/Ts
(7.15)
which is a sinc function that has an inﬁnite time support and decays symmetrically with respect to the
origin t = 0. The reconstructed signal xr(t) is the convolution of the sampled signal xs(t) and hlp(t),
which is found to be
xr(t) = [xs ∗hlp](t) =
∞
Z
−∞
xs(τ)hlp(t −τ)dτ

7.2 Uniform Sampling
433
=
∞
Z
−∞
"X
n
x(nTs)δ(τ −nTs)
#
hlp(t −τ)dτ
=
X
n
x(nTs)sin(π(t −nTs)/Ts)
π(t −nTs)/Ts
(7.16)
after replacing xs(τ) and applying the sifting property of the delta function. The recovered signal is
thus an interpolation in terms of time-shifted sinc signals with amplitudes the samples {x(nTs)}. In
fact, if we let t = kTs, we can see that
xr(kTs) =
X
n
x(nTs)sin(π(k −n))
π(k −n)
= x(kTs)
since
sin(π(k −n))
π(k −n)
=
1
k −n = 0 or k = n
0
k ̸= n
This is because the above sinc function by L’Hˆopital’s rule is shown to be unity when k = n, and it is
0 when k ̸= n since the sine is zero at multiples of π. Thus, the values at t = kTs are recovered exactly,
and the rest are interpolated by a sum of sinc signals.
7.2.5 Sampling Simulation with MATLAB
The simulation of sampling with MATLAB is complicated by the representation of analog signals
and the numerical computation of the analog Fourier transform. Two sampling rates are needed:
one being the sampling rate under study, fs, and the other being the one used to simulate the analog
signal, fsim >> fs. The computation of the analog Fourier transform of x(t) can be done approximately
using the fast Fourier transform (FFT) multiplied by the sampling period. For now, think of the FFT
as an algorithm to compute the Fourier transform of a discretized signal.
To illustrate the sampling procedure consider sampling a sinusoid x(t) = cos(2πf0t) where f0 = 1
KHz. To simulate this as an analog signal we choose a sampling period Tsim = 0.5 × 10−4 sec/sample
or a sampling frequency fsim = 20, 000 samples/sec.
No aliasing sampling—If we sample x(t) with a sampling frequency fs = 6000 > 2 f0 = 2000 Hz, the
sampled signal y(t) will not display aliasing in its frequency representation, as we are satisfying the
Nyquist sampling rate condition. Figure 7.7(a) displays the signal x(t) and its sampled version y(t),
as well as their approximate Fourier transforms. The magnitude spectrum |X()| corresponds to the
sinusoid x(t), while |Y()| is the ﬁrst period of the spectrum of the sampled signal (recall the spec-
trum of the sampled signal is periodic of period s = 2πfs). In this case, when no aliasing occurs, the
ﬁrst period of the spectrum of y(t) coincides with the spectrum of x(t) (notice that as a sinusoid, the
magnitude spectrum |X()| is zero except at the frequency of the sinusoid or ±1 KHz; likewise |Y()|
is zero except at ±1 KHz and the range of frequencies is [−fs/2, fs/2] = [−3, 3] KHz). In Figure 7.7(b)
we show the sinc interpolation of three samples of y(t); the solid line is the interpolated values or the
sum of sincs centered at the three samples. At the bottom of that ﬁgure we show the sinc interpola-
tion, for all the samples, obtained using our function sincinterp. The sampling is implemented using
our function sampling.

434
CHAPTER 7:
Sampling Theory
0
1
2
3
4
5
6
7
−1
0
1
t (sec)
x(t), y(t)
x(t) xr(t)
(a)
(b)
−10 −8
−6
−4
−2
0
2
4
6
8
0
0.2
0.4
−3
−2
−1
0
1
2
0
0.2
0.4
f (KHz)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
−0.5
0
0.5
1
1.5
0
1
2
3
4
5
6
−1
−0.5
0
0.5
1
t (sec)
×10−3
×10−4
×10−3
|X(Ω)|
|Y(Ω)|
Interpolated
sinc
Analog signal
Sampled signal
FIGURE 7.7
No aliasing: sampling simulation of x(t) = cos(2000πt) using fs = 6000 samples/sec. (a) Plots are of the signal
x(t) and the sampled signal y(t), and their spectra (|Y()| is periodic and so a period is shown). (b) The top plot
illustrates the sinc interpolation of three samples, and the bottom plot is the sinc-interpolated signal xr(t) and the
sampled signal. In this case xr(t) is very close to the original signal.
Sampling with aliasing—In Figure 7.8 we show the case when the sampling frequency is fs = 800 <
2fs = 2000, so that in this case we have aliasing. This can be seen in the sampled signal y(t) in the
top plot of Figure 7.8(a), which appears as if we were sampling a sinusoid of lower frequency. It can
also be seen in the spectra of x(t) and y(t): |X()| is the same as in the previous case, but now |Y()|,
which is a period of the spectrum of the sampled signal y(t), displays a frequency of 200 Hz, lower
than that of x(t), within the frequency range [−400, 400] Hz or [−fs/2, fs/2]. Aliasing has occurred.
Finally, the sinc interpolation gives a sinusoid of frequency 0.2 KHz, different from x(t).
Similar situations occur when a more complex signal is sampled. If the signal to be sampled is
x(t) = 2 −cos(πf0t) −sin(2πf0t) where f0 = 500 Hz, if we use a sampling frequency of fs = 6000 > 2
fmax = 2 f0 = 1000 Hz, there will be no aliasing. On the other hand, if the sampling frequency is
fs = 800 < 2fmax = 2f0 = 1000 Hz, frequency aliasing will occur. In the no aliasing sampling, the
spectrum |Y()| (in a frequency range [−3000, 3000] = [−fs/2, fs/2]) corresponding to a period of
the Fourier transform of the sampled signal y(t) shows the same frequencies as |X()|. The recon-
structed signal equals the original signal. See Figure 7.9(a). When we use fs = 800 Hz, the given
signal x(t) is undersampled and aliasing occurs. The spectrum |Y()| corresponding to a period of
the Fourier transform of the undersampled signal y(t) does not show the same frequencies as |X()|.
The reconstructed signal shown in the bottom right plot of Figure 7.9(b) does not resemble the
original signal.

7.2 Uniform Sampling
435
0
1
2
3
4
5
6
7
8
−1
0
1
t (sec)
0
0.5
1
1.5
2
2.5
3
3.5
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
−1
−0.5
0
0.5
1
t (sec)
x(t), y(t)
−10 −8
−6
−4
−2
0
2
4
6
8
0
0.2
0.4
|X(Ω)|
−0.4 −0.3 −0.2 −0.1
0
0.1
0.2
0.3
0
0.2
0.4
f (KHz)
|Y(Ω)|
×10−3
×10−3
×10−3
x(t), xr(t)
(a)
(b)
Analog signal
Sampled signal
FIGURE 7.8
Aliasing: sampling simulation of x(t) = cos(2000πt) using fs = 800 samples/sec. (a) Plots display the original
signal x(t) and the sampled signal y(t) (it looks like a lower-frequency signal being sampled). The sprectra of x(t)
and y(t) are shown below (|Y()| is periodic and displays a lower frequency than |X()|). (b) Sinc interpolation
for three samples and the whole signal. The reconstructed signal xr(t) is a sinusoid of period 0.5 × 10−2 or
200-Hz frequency due to aliasing.
The following function implements the sampling and computes the Fourier transform of the analog
signal and of the sampled signal using the fast Fourier transform. It gives the range of frequencies for
each of the spectra.
function [y,y1,X,fx,Y,fy] = sampling(x,L,fs)
%
% Sampling
%
x analog signal
%
L length of simulated x
%
fs sampling rate
%
y sampled signal
%
X,Y magnitude spectra of x,y
%
fx,fy frequency ranges for X,Y
%
fsim = 20000; % analog signal sampling frequency
% sampling with rate fsim/fs
delta = fsim/fs;
y1 = zeros(1,L);

436
CHAPTER 7:
Sampling Theory
0
2
4
6
0
1
2
3
4
x(t), y(t)
−10 −5
0
5
0
0.5
1
1.5
2
f (KHz)
−0.4 −0.2
0
0.2
0
0.5
1
1.5
2
f (KHz)
0
0.01
0.02
0.03
0.04
0.05
0
1
2
3
4
t (sec)
t (sec)
×10−3
|Y(Ω)|
|X(Ω)|
xr(t)
(a)
(b)
0
2
4
6
0
1
2
3
4
t (sec)
x(t), y(t)
Analog signal
Sampled signal
Analog signal
Sampled signal
−10 −5
0
5
0
0.5
1
1.5
2
f (KHz)
−3 −2 −1
0
1
2
0
0.5
1
1.5
2
f (KHz)
0
2
4
6
0
1
2
3
4
t (sec)
|Y(Ω)|
×10−3
×10−3
|X(Ω)|
xr(t)
FIGURE 7.9
Sampling of x(t) = 2 −cos(500πt) −sin(1000πt) with (a) no aliasing (fs = 6000 samples/sec) and (b) with
aliasing (fs = 800 samples/sec).
y1(1:delta:L) = x(1:delta:L);
y = x(1:delta:L);
% analog FT and DTFT of signals
dtx = 1/fsim;
X = fftshift(abs(fft(x))) ∗dtx;
N = length(X); k = 0:(N −1); fx = 1/N.*k; fx = fx ∗fsim/1000 −fsim/2000;
dty = 1/fs;
Y = fftshift(abs(fft(y))) ∗dty;
N = length(Y); k = 0:(N −1); fy = 1/N.*k; fy = fy ∗fs/1000 −fs/2000;
The following function computes the sinc interpolation of the samples.
function [t,xx,xr] = sincinterp(x,Ts)
%
% Sinc interpolation
% x sampled signal
% Ts sampling period of x
% xx,xr original samples and reconstructed in range t
%
N = length(x)
t = 0:dT:N;
xr = zeros(1,N ∗100 + 1);
for k = 1:N,

7.3 The Nyquist-Shannon Sampling Theorem
437
xr = xr + x(k) ∗sinc(t −(k −1));
end
xx(1:100:N ∗100) = x(1:N);
xx = [xx zeros(1,99)];
NN = length(xx)
t = 0:NN −1;t = t ∗Ts/100;
7.3 THE NYQUIST-SHANNON SAMPLING THEOREM
If a low-pass continuous-time signal x(t) is band limited (i.e., it has a spectrum X() such that X() = 0 for
|| > max, where max is the maximum frequency in x(t)), we then have:
I
x(t) is uniquely determined by its samples x(nTs) = x(t)|t=nTs, n = 0, ±1, ±2, · · · , provided that the
sampling frequency s (rad/sec) is such that
s ≥2max
Nyquist sampling rate condition
(7.17)
or equivalently if the sampling rate fs (samples/sec) or the sampling period Ts (sec/sample) are given by
fs = 1
Ts
≥max
π
(7.18)
I
When the Nyquist sampling rate condition is satisﬁed, the original signal x(t) can be reconstructed by
passing the sampled signal xs(t) through an ideal low-pass ﬁlter with the following frequency response:
H() =
(
Ts
−s
2
<  < s
2
0
elsewhere
The reconstructed signal is given by the following sinc interpolation from the samples:
xr(t) =
X
n
x(nTs)sin(π(t −nTs)/Ts)
π(t −nTs)/Ts
(7.19)
Remarks
I
The value 2max is called the Nyquist sampling rate. The value s/2 is called the folding rate.
I
The units of the sampling frequency fs are samples/sec
and as such the units of Ts are sec/sample.
Considering the number of samples available, every second or the time at which each sample is available
we can get a better understanding of the data storage requirements, the speed limitations imposed by
real-time processing, and the need for data compression algorithms. For instance, music being sampled at
44, 000 samples/sec, with each sample represented by 8 bits/sample, for every second of music we would
need to store 44 × 8 = 352 Kbits/sec, and in an hour of sampling we would have 3600 × 44 × 8 Kbits.
If you want better quality, let’s say 16 bits/sample, then double that quantity, and if you want more ﬁdelity
increase the sampling rate but be ready to provide more storage or to come up with some data compression
algorithm. Likewise, if you were to process the signal you would have a new sample every Ts = 0.0227
msec, so that any real-time processing would have to be done very fast.

438
CHAPTER 7:
Sampling Theory
Origins of the Sampling Theory — Part 2
As mentioned in Chapter 0, the theoretical foundations of digital communications theory were given in the paper “A Math-
ematical Theory of Communication” by Claude E. Shannon in 1948 [51]. His results on sampling theory made possible the
new areas of digital communications and digital signal processing.
Shannon was born in 1916 in Petoskey, Michigan. He studied electrical engineering and mathematics at the University
of Michigan, pursued graduate studies in electrical engineering and mathematics at MIT, and then joined Bell Telephone
Laboratories. In 1956, he returned to MIT to teach.
Besides being a celebrated researcher, Shannon was an avid chess player. He developed a juggling machine, rocket-powered
frisbees, motorized Pogo sticks, a mind-reading machine, a mechanical mouse that could navigate a maze, and a device that
could solve the Rubik’s CubeTM puzzle. At Bell Labs, he was remembered for riding the halls on a unicycle while juggling
three balls [23, 52].
7.3.1 Sampling of Modulated Signals
The given Nyquist sampling rate condition applies to low-pass or baseband signals. Sampling of
band-pass signals is used for simulation of communication systems and in the implementation of
modulation systems in software radio. For modulated signals it can be shown that the sampling rate
depends on the bandwidth of the message or modulating signal, not on the absolute frequencies
involved. This result provides a signiﬁcant savings in the sampling, as it is independent of the car-
rier. A voice message transmitted via a satellite communication system with a carrier of 6 GHz, for
instance, would only need to be sampled at about a 10-KHz rate, rather than at 12 GHz as determined
by the Nyquist sampling rate condition when we consider the frequencies involved.
Consider a modulated signal x(t) = m(t) cos(ct) where m(t) is the message and cos(ct) is the carrier
with carrier frequency
c >> max
where max is the maximum frequency present in the message. The sampling of x(t) with a sampling
period Ts generates in the frequency domain a superposition of the spectrum of x(t) shifted in fre-
quency by s and multiplied by 1/Ts. Intuitively, to avoid aliasing the shifting in frequency should
be such that there is no overlapping of the shifted spectra, which would require that
c + max −s < c −max
⇒s > 2max
or
Ts <
π
max
Thus, the sampling period depends on the bandwidth max of the message m(t) rather than on the
maximum frequency present in the modulated signal x(t). A formal proof of this result requires the
quadrature representation of band-pass signals typically considered in communication theory [16].
If the message m(t) of a modulated signal x(t) = m(t) cos(c) has a bandwidth B Hz, x(t) can be reconstructed
from samples taken at a sampling rate
fs ≥2B
independent of the frequency c of the carrier cos(c).

7.4 Practical Aspects of Sampling
439
I Example 7.4
Consider the development of an AM transmitter that uses a computer to generate the modulated
signal and is capable of transmitting music and speech signals. Indicate how to implement the
transmitter.
Solution
Let the message be m(t) = x(t) + y(t) where x(t) is a speech signal and y(t) is a music signal. Since
music signals display larger frequencies than speech signals, the maximum frequency of m(t) is
that of the music signals, or fmax = 22 KHz. To transmit m(t) using AM, we modulate it with a
sinusoid of frequency fc > fmax, say fc = 3fmax = 66 KHz.
To satisfy the Nyquist sampling rate condition, the maximum frequency of the modulated sig-
nal would be fc + fmax = (66 + 22) KHz = 88 KHz, and so we would choose Ts = 10−3/176
sec/sample as the sampling period. However, according to the above results we can also choose
Ts = 1/(2B) where B is the bandwidth of m(t) in hertz or B = fmax = 22 KHz, which gives
Ts = 10−3/44 — four times larger than the previous sampling period, so we choose this as the
sampling period.
The analog signal m(t) to be transmitted is inputted into an ADC in the computer, capable of
sampling at 44, 000 samples/sec. The output of the converter is then multiplied by a computer-
generated sinusoid
cos(2πfcnTs) = cos(2π × 66 × 103 × (10−3/44)n) = cos(3πn) = (−1)n
to obtain the AM signal. The AM digital signal can then be inputted into a DAC and its output sent
to an antenna for broadcasting.
I
7.4 PRACTICAL ASPECTS OF SAMPLING
To process analog signals with computers it is necessary to convert analog into digital signals and
digital into analog signals. The analog-to-digital and digital-to-analog conversions are done by ADCs
and DACs. In practice, these converters differ from the ideal versions we have discussed so far where
the sampling is done with impulses, the discrete-time samples are assumed representable with inﬁ-
nite precision, and the reconstruction is performed by an ideal low-pass ﬁlter. Pulses rather than
impulses are needed, and the discrete-time signals need to be discretized also in amplitude and the
reconstruction ﬁlter needs to be reconsidered.
7.4.1 Sample-and-Hold Sampling
In an actual ADC the time required to do the sampling, quantization, and coding needs to be con-
sidered. Therefore, the width 1 of the sampling pulses cannot be zero as assumed. A sample-and-hold
sampling system takes the sample and holds it long enough for quantization and coding to be done
before the next sample is acquired. The question is then how does this affect the sampling process
and how does it differ from the ideal results obtained before? We hinted at the effects when we
considered the PAM before, except that now the resulting pulses are ﬂat.

440
CHAPTER 7:
Sampling Theory
FIGURE 7.10
Sampling using a sample-and-hold system (δ = Ts).
t
Δ
1
×
t
t
0
x(t)
δTs(t)
xs(t)
h(t)
ys(t)
h(t)
x(t)
0
Ts
xs(t)
Ts
ys(t)
The system shown in Figure 7.10 generates the desired signal. Basically, we are modulating the ideal
sampling signal δTs(t) with the analog input x(t), giving an ideally sampled signal xs(t). This signal
is then passed through a zero-order hold ﬁlter, an LTI system having as impulse response h(t) a pulse
of the desired width 1 ≤Ts. The output of the sample-and-hold system is a weighted sequence of
shifted versions of the impulse response. In fact, the output of the ideal sampler is xs(t) = x(t)δTs(t),
and using the linearity and time invariance of the zero-order hold system its output is
ys(t) = (xs ∗h)(t)
(7.20)
with a Fourier transform of
Ys() = Xs()H( j)
=
"
1
Ts
X
k
X( −ks)
#
H( j)
(7.21)
where the term in the brackets is the spectrum of the ideally sampled signal and
H( j) = e−1s/2
s
(e1s/2 −e−1s/2)|s=j
= sin(1/2)
/2
e−j1/2
(7.22)
is the frequency response of the LTI system.
Remarks
I
Equation (7.20) can be written as
ys(t) =
X
n
x(nTs)h(t −nTs)
That is, ys(t) is a train of pulses h(t) = u(t) −u(t −1) shifted and weighted by the sample values x(nTs),
a more realistic representation of the sampled signal.

7.4 Practical Aspects of Sampling
441
FIGURE 7.11
Sample-and-hold circuit.
r
R
C
+
+
−
−
Ts
xsh(t)
x(t)
I
Two signiﬁcant changes due to considering the pulses of width 1 > 0 in the sampling are:
I The spectrum of the ideal sampled signal xs(t) is now weighted by the sinc function of the frequency
response H( j) of the zero-order hold ﬁlter. Thus, the spectrum of the sampled signal using the sample-
and-hold system will not be periodic and will decay as  increases.
I The reconstruction of the original signal x(t) requires a more complex ﬁlter than the one used in the
ideal sampling. Indeed, the concatenation of the zero-order hold ﬁlter with the reconstruction ﬁlter
should be such that H(s)Hr(s) = 1, or that Hr(s) = 1/H(s).
I
A circuit used for implementing the sample-and-hold system is shown in Figure 7.11. In this circuit the
switch closes every Ts seconds and remains closed for a short time 1. If the time constant rC << 1, the
capacitor charges very fast to the value of the sample attained when the switch closes at some nTs, and by
setting the time constant RC >> Ts when the switch opens 1 seconds later, the capacitor slowly discharges.
The cycle repeats providing a signal that approximates the output of the sample-and-hold system explained
before.
I
The DAC also uses a holder to generate an analog signal from the discrete signal coming out of the decoder
into the DAC. There are different possible types of holders, providing an interpolation that will make the
ﬁnal smoothing of the signal a lot easier. The so-called zero-order hold basically expands the sample
value in between samples, providing a rough approximation of the discrete signal, which is then smoothed
out by a low-pass ﬁlter to provide the analog signal.
7.4.2 Quantization and Coding
Amplitude discretization of the sampled signal xs(t) is accomplished by a quantizer consisting of a
number of ﬁxed amplitude levels against which the sample amplitudes {x(nTs)} are compared. The
output of the quantizer is one of the ﬁxed amplitude levels that best represents x(nTs) according to
some approximation scheme. The quantizer is a nonlinear system.
Independent of how many levels, or equivalently of how many bits are allocated to represent each
level of the quantizer, there is a possible error in the representation of each sample. This is called
the quantization error. To illustrate this, consider a 2-bit or 22-level quantizer shown in Figure 7.12.
The input of the quantizer are the samples x(nTs), which are compared with the values in the bins
[−21, −1], [−1, 0], [0, 1], and [1, 21], and depending on which of these bins the sample falls in
it is replaced by the corresponding levels −21, −1, 0, or 1. The value of the quantization step 1 for
the four-level quantizer is
1 = 2 max|x(t)|
22
(7.23)

442
CHAPTER 7:
Sampling Theory
FIGURE 7.12
Four-level quantizer and coder.
Δ
Δ
10
11
01
00
x (nTs)
−Δ
−2Δ
−Δ
−2Δ
2Δ
x(nTs)
∧
That is, 1 is assigned so as to cover the possible peak-to-peak range of values of the signal, or its
dynamic range. To each of the levels a binary code is assigned. The code assigned to each of the
levels uniquely represents the different levels [−21, −1, 0, 1]. As to the way to approximate the
given sample to one of these levels, it can be done by rounding or by truncating. The quantizer shown
in Figure 7.12 approximates by truncation—that is, if the sample k1 ≤x(nTs) < (k + 1)1, for k =
−2, −1, 0, 1, then it is approximated by the level k1.
To see the quantization, coding, and quantization error, let the sampled signal be
x(nTs) = x(t)|t=nTS
The given four-level quantizer is such that
k1 ≤x(nTs) < (k + 1)1
⇒
ˆx(nTs) = k1
k = −2, −1, 0, 1
(7.24)
where the sampled signal x(nTs) is the input and the quantized signal ˆx(nTs) is the output. Therefore,
−21 ≤x(nTs) < −1
⇒
ˆx(nTs) = −21
−1 ≤x(nTs) < 0
⇒
ˆx(nTs) = −1
0 ≤x(nTs) < 1
⇒
ˆx(nTs) = 0
1 ≤x(nTs) < 21
⇒
ˆx(nTs) = 1
To transform the quantized values into unique binary 2-bit values, one could use a code such as
ˆx(nTs)
=
−21
⇒
10
ˆx(nTs)
=
−1
⇒
11
ˆx(nTs)
=
01
⇒
00
ˆx(nTs)
=
1
⇒
01
which assigns a unique 2-bit binary number to each of the four quantization levels.
If we deﬁne the quantization error as
ε(nTs) = x(nTs) −ˆx(nTs)

7.4 Practical Aspects of Sampling
443
and use the characterization of the quantizer given in Equation (7.24), we have then that the error
ε(nTs) is obtained from
ˆx(nTs) ≤x(nTs) ≤ˆx(nTs) + 1 by subtracting ˆx(nTs) ⇒0 ≤ε(nTs) ≤1
(7.25)
indicating that one way to decrease the quantization error is to make the quantization step 1 very
small. That clearly depends on the quality of the ADC. Increasing the number of bits of the ADC
makes 1 smaller (see Equation (7.23) where the denominator is 2 raised to the number of bits),
which will make the quantization error smaller.
In practice, the quantization error is considered random, and so it needs to be characterized proba-
bilistically. This characterization becomes meaningful only when the number of bits is large, and the
input signal is not a deterministic signal. Otherwise, the error is predictable and thus not random.
Comparing the energy of the input signal to the energy of the error, by means of the so-called signal-
to-noise ratio (SNR), it is possible to determine the number of bits that are needed in a quantizer to
get a reasonable quantization error.
I Example 7.5
Suppose we are trying to decide between an 8- and a 9-bit ADC for a certain application. The
signals in this application are known to have frequencies that do not exceed 5 KHz. The amplitude
of the signals is never more than 5 volts (i.e., the dynamic range of the signals is 10 volts, so that
the signal is bounded as −5 ≤x(t) ≤5). Determine an appropriate sampling period and compare
the percentage of error for the two ADCs of interest.
Solution
The ﬁrst consideration in choosing the ADC is the sampling period, so we need to get an ADC
capable of sampling at fs = 1/Ts > 2fmax samples/sec. Choosing fs = 4fmax = 20 K samples/sec,
then Ts = 1/20 msec/sample. Suppose then we look at an 8-bit ADC, which means that the quan-
tizer would have 28 = 256 levels so that the quantization step is 1 = 10/256 volts. If we use the
truncation quantizer given above the quantization error would be
0 ≤ε(nTs) ≤10/256
If we ﬁnd that objectionable we can then consider a 9-bit ADC, with a quantizer of 29 = 512 levels
and the quantization step is 1 = 10/512 or half that of the 8-bit ADC
0 ≤ε(nTs) ≤10/512
So that by increasing 1 bit we cut the quantization error in half from the previous quantizer (in
practice, one of the 8 or 9 bits is used to determine the sign of the sampled value). Inputting a signal
of constant amplitude 5 into the 9-bit ADC gives a quantization error of [(10/512)/5] × 100% =
(100/256)% ≈0.4% in representing the input signal. For the 8-bit ADC it would correspond to a
0.8% error.
I

444
CHAPTER 7:
Sampling Theory
7.4.3 Sampling, Quantizing, and Coding with MATLAB
The conversion of an analog signal into a digital signal consists of three steps: sampling, quantization,
and coding. These are the three operations an ADC does. To illustrate them consider a sinusoid
x(t) = 4 cos(2πt). Its sampling period, according to the Nyquist sampling rate condition, is
Ts ≤π/max = 0.5 sec/sample
as the maximum frequency of x(t) is max = 2π. We let Ts = 0.01 (sec/sample) to obtain a sam-
pled signal xs(nTs) = 4 cos(2πnTs) = 4 cos(2πn/100), a discrete sinusoid of period 100. The following
script is used to get the sampled x[n] and the quantized xq[n] signals and the quantization error ε[n]
(see Figure 7.13).
0
0.5
1
−4
−2
0
2
4
t (sec)
x(t)
0
50
100
−4
−2
0
2
4
n
x[n]
Sampled signal
(b)
(a)
(d)
(c)
0
50
100
−4
−2
0
2
4
Quantized signal
n
0
50
100
0
1
2
3
4
e[n]
n
xq[n]
FIGURE 7.13
(a) Sinusoid, (b) sampled sinusoid using Ts = 0.01, (c) quantized sinusoid using four levels, and (d) quantization
error.

7.4 Practical Aspects of Sampling
445
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sampling, quantization and coding
%%%%%%%%%%%%%%%%%%%%%%%%%%%
clear all; clf
% analog signal
t = 0:0.01:1; x = 4 ∗sin(2 ∗pi ∗t);
% sampled signal
Ts = 0.01; N = length(t); n = 0:N −1;
xs = 4 ∗sin(2 ∗pi ∗n ∗Ts);
% quantized signal
Q = 2;
% quantization levels is 2Q
[d,y,e] = quantizer(x,Q);
% binary signal
z = coder(y,d)
The quantization of the sampled signal is implemented with the function quantizer which compares
each of the samples xs(nTs) with four levels and assigns to each the corresponding level. Notice the
appproximation of the values given by the quantized signal samples to the actual values of the signal.
The difference between the original and the quantized signal, or the quantization error, ε(nTs), is also
computed and shown in Figure 7.13.
function [d,y,e] = quantizer(x,Q)
% Input: x, signal to be quantized at 2Q levels
% Outputs: y quantized signal
%
e, quantization error
%
d quantum
% USE [d,y,e] = quantizer(x,Q)
%
N = length(x);
d = max(abs(x))/Q;
for k = 1:N,
if x(k)> = 0,
y(k) = ﬂoor(x(k)/d)*d;
else
if x(k) == min(x),
y(k) = (x(k)/abs(x(k))) ∗(ﬂoor(abs(x(k))/d) ∗d);
else
y(k) = (x(k)/abs(x(k))) ∗(ﬂoor(abs(x(k))/d) ∗d + d);
end
end
if y(k) == 2 ∗d,
y(k) = d;
end
end

446
CHAPTER 7:
Sampling Theory
The binary signal corresponding to the quantized signal is computed using the function coder which
assigns the binary codes ’10’,’11’,’00’, and ’01’ to the four possible levels of the quantizer. The result is
a sequence of 0s and 1s, each pair of digits sequentially corresponding to each of the samples of the
quantized signal. The following is the function used to effect this coding.
function z1 = coder(y,delta)
% Coder for 4-level quantizer
% input: y quantized signal
% output: z1 binary sequence
% USE z1 = coder(y)
%
z1 = ’00’; % starting code
N = length(y);
for n = 1:N,
y(n)
if y(n) == delta
z = ’01’;
elseif y(n) == 0
z = ’00’;
elseif y(n) == -delta
z = ’11’;
else
z = ’10’;
end
z1 = [z1 z];
end
M = length(z1);
z1 = z1(3:M) % get rid of starting code
7.5 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO
FROM HERE?
The material in this chapter is the bridge between analog and digital signal processing. The sampling
theory provides the necessary information to convert a continuous-time signal into a discrete-time
signal and then into a digital signal with minimum error. It is the frequency representation of an
analog signal that determines the way in which it can be sampled and reconstructed. Analog-to-
digital and digital-to-analog converters are the devices that in practice convert an analog signal into
a digital signal and back. Two parameters characterizing these devices are the sampling rate and the
number of bits each sample is coded into. The rate of change of a signal determines the sampling
rate, while the precision in representing the samples determines the number of levels of the quantizer
and the number of bits assigned to each sample.
In the following chapters we will consider the analysis of discrete-time signals, as well as the analysis
and synthesis of discrete systems. The effect of quantization in the processing and design of systems

Problems
447
is an important problem that is left for texts in digital signal processing. We will, however, develop
the theory of discrete-time signals.
PROBLEMS
7.1. Sampling actual signals
Consider the sampling of real signals.
(a)
Typically, a speech signal that can be understood over a telephone shows frequencies from about
100 Hz to about 5 KHz. What would be the sampling frequency fs (samples/sec) that would be used
to sample speech without aliasing? How many samples would you need to save when storing an hour
of speech? If each sample is represented by 8 bits, how many bits would you have to save for the hour
of speech?
(b)
A music signal typically displays frequencies from 0 up to 22 KHz. What would be the sampling
frequency fs that would be used in a CD player?
(c)
If you have a signal that combines voice and musical instruments, what sampling frequency would
you use to sample this signal? How would the signal sound if played at a frequency lower than the
Nyquist sampling frequency?
7.2. Sampling of band-limited signals
Consider the sampling of a sinc signal and related signals.
(a)
For the signal x(t) = sin(t)/t, ﬁnd its magnitude spectrum |X()| and determine if this signal is band
limited or not.
(b)
Suppose you want to sample x(t)). What would be the sampling period Ts you would use for the
sampling without aliasing?
(c)
For a signal y(t) = x2(t), what sampling frequency fs would you use to sample it without aliasing?
How does this frequency relate to the sampling frequency used to sample x(t)?
(d)
Find the sampling period Ts to sample x(t) so that the sampled signal xs(0) = 1, otherwise xs(nTs) = 0
for n ̸= 0.
7.3. Sampling of time-limited signals—MATLAB
Consider the signals x(t) = u(t) −u(t −1) and y(t) = r(t) −2r(t −1) + r(t −2).
(a)
Are either of these signals band limited? Explain.
(b)
Use Parseval’s theorem to determine a reasonable value for a maximum frequency for these signals
(choose a frequency that would give 90% of the energy of the signals). Use MATLAB.
(c)
If we use the sampling period corresponding to y(t) to sample x(t), would aliasing occur? Explain.
(d)
Determine a sampling period that can be used to sample both x(t) and y(t) without causing aliasing
in either signal.
7.4. Uncertainty in time and frequency—MATLAB
Signals of ﬁnite time support have inﬁnite support in the frequency domain, and a band-limited signal has
inﬁnite time support. A signal cannot have ﬁnite support in both domains.
(a)
Consider x(t) = (u(t + 0.5) −u(t −0.5))(1 + cos(2πt)). Find its Fourier transform X(). Compute the
energy of the signal, and determine the maximum frequency of a band-limited approximation signal
ˆx(t) that would give 95% of the energy of the original signal.
(b)
The fact that a signal cannot be of ﬁnite support in both domains is expressed well by the uncertainty
principle, which says that
1(t)1() ≥1
4π

448
CHAPTER 7:
Sampling Theory
where
1(t) =


∞
R
−∞
t2 |x(t)|2 dt
Ex


0.5
measures the duration of the signal for which the signal is signiﬁcant in time, and
1() =


∞
R
−∞
2 |X()|2 d
Ex


0.5
measures the frequency support for which the Fourier representation is signiﬁcant. The energy of
the signal is represented by Ex. Compute 1(t) and 1() for the given signal x(t) and verify that the
uncertainty principle is satisﬁed.
7.5. Nyquist sampling rate condition and aliasing
Consider the signal
x(t) = sin(0.5t)
0.5t
(a)
Find the Fourier transform X() of x(t).
(b)
Is x(t) band limited? If so, ﬁnd its maximum frequency max.
(c)
Suppose that Ts = 2π. How does s relate to the Nyquist frequency 2max? Explain.
(d)
What is the sampled signal x(nTs) equal to? Carefully plot it and explain if x(t) can be reconstructed.
7.6. Anti-aliasing
Suppose you want to ﬁnd a reasonable sampling period Ts for the noncausal exponential
x(t) = e−|t|
(a)
Find the Fourier transform of x(t), and plot |X()|. Is x(t) band limited?
(b)
Find a frequency 0 so that 99% of the energy of the signal is in −o ≤ ≤o.
(c)
If we let s = 2π/Ts = 50, what would be Ts?
(d)
Determine the magnitude and bandwidth of an anti-aliasing ﬁlter that would change the original
signal into the band-limited signal with 99% of the signal energy.
7.7. Sampling of modulated signals
Assume you wish to sample an amplitude modulated signal
x(t) = m(t) cos(ct)
where m(t) is the message signal and c = 2π104 rad/sec is the carrier frequency.
(a)
If the message is an acoustic signal with frequencies in a band of [0, 22] KHz, what would be the
maximum frequency present in x(t)?
(b)
Determine the range of possible values of the sampling period Ts that would allow us to sample x(t)
satisfying the Nyquist sampling rate condition.
(c)
Given that x(t) is a band-pass signal, compare the above sampling period with the one that can be
used to sample band-pass signals.

Problems
449
7.8. Sampling output of nonlinear system
The input–output relation of a nonlinear system is
y(t) = x2(t)
where x(t) is the input and y(t) is the output.
(a)
The signal x(t) is band limited with a maximum frequency M = 2000π rad/sec. Determine if y(t) is
also band limited, and if so, what is its maximum frequency max?
(b)
Suppose that the signal y(t) is low-pass ﬁltered. The magnitude of the low-pass ﬁlter is unity and the
cut-off frequency is c = 5000π rad/sec. Determine the value of the sampling period Ts according to
the given information.
(c)
Is there a different value for Ts that would satisfy the Nyquist sampling rate condition for both x(t)
and y(t) and that is larger than the one obtained above? Explain.
7.9. Signal reconstruction
You wish to recover the original analog signal x(t) from its sampled form x(nTs).
(a)
If the sampling period is chosen to be Ts = 1 so that the Nyquist sampling rate condition is satis-
ﬁed, determine the magnitude and cut-off frequency of an ideal low-pass ﬁlter H( j) to recover the
original signal and plot them.
(b)
What would be a possible maximum frequency of the signal? Consider an ideal and a nonideal low-
pass ﬁlter to reconstruct x(t). Explain.
7.10. CD player versus record player
Explain why a CD player cannot produce the same ﬁdelity of music signals as a conventional record player.
(If you do not know what these are, ignore this problem, or get one to ﬁnd out what they do or ask your
grandparents about LPs and record players!)
7.11. Two-bit analog-to-digital converter—MATLAB
Let x(t) = 0.8 cos(2πt) + 0.15, 0 ≤t ≤1, and zero otherwise, be the input to a 2-bit analog-to-digital
converter.
(a)
For a sampling period Ts = 0.025 sec determine and plot using MATLAB the sampled signal,
x(nTs) = x(t)|t=nTS
(b)
The four-level quantizer (see Figure 1.2) corresponding to the 2-bit ADC is deﬁned as
k1 ≤x(nTs) < (k + 1)1
→
ˆx(nTs) = k1
k = −2, −1, 0, 1
(7.26)
where x(nTs), found above, is the input and ˆx(nTs) is the output of the quantizer. Let the quantization
step be 1 = 0.5. Plot the input–output characterization of the quantizer, and ﬁnd the quantized output
for each of the sample values of the sampled signal x(nTs).
(c)
To transform the quantized values into unique binary 2-bit values, consider the following code:
ˆx(nTs) = −21
→
10
ˆx(nTs) = −1
→
11
ˆx(nTs) = 01
→
00
ˆx(nTs) = 1
→
01
Obtain the digital signal corresponding to x(t).

This page intentionally left blank

CHAPTER 8
Discrete-Time Signals and Systems
It’s like d´ej`a-vu,
all over again.
Lawrence “Yogi” Berra (1925)
Yankees baseball player
8.1 INTRODUCTION
As you will see in this chapter, the basic theory of discrete-time signals and systems is very much like
that for continuous-time signals and systems. However, there are signiﬁcant differences that need to
be understood. Speciﬁcally in this chapter we will consider the following contrasting issues:
I
Discrete-time signals resulting from sampling of continuous-time signals are only available at
uniform times determined by the sampling period; they are not deﬁned in-between sampling
periods. It is important to emphasize the signiﬁcance of sampling according to the Nyquist sam-
pling rate condition since the characteristics of discrete-time signals will depend on it. Given the
knowledge of the sampling period, discrete-time signals depend on an integer variable n, which
uniﬁes the treatment of discrete-time signals obtained from analog signals by sampling and those
that are naturally discrete. It will also be seen that the frequency in the discrete domain differs
from the analog frequency. The radian discrete frequency cannot be measured, and depends on
the sampling period used whenever the discrete-time signals result from sampling.
I
Although the concept of periodicity of discrete-time signals coincides with that for continuous-
time signals, there are signiﬁcant differences. As functions of an integer variable, discrete-time
periodic signals must have integer periods. This imposes some restrictions that do not exist in
continuous-time periodic signals. For instance, continuous-time sinusoids are always periodic as
their period can be a positive real number; however, that will not be the case for discrete-time
sinusoids. It is possible to have discrete-time sinusoids that are not periodic, even if they resulted
from the uniform sampling of continuous-time sinusoids.
I
Characteristics such as energy, power, and symmetry of continuous-time signals are conceptually
the same for discrete-time signals. Integrals are replaced by sums, derivatives by ﬁnite differences,
and differential equations by difference equations. Likewise, one can deﬁne a set of basic signals
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00012-0
c⃝2011, Elsevier Inc. All rights reserved.
451

452
CHAPTER 8:
Discrete-Time Signals and Systems
just like those for continuous-time signals. However, some of these basic signals do not display
the mathematical complications of their continuous-time counterparts. For instance, the discrete-
impulse signal is deﬁned at every integer value in contrast with the continuous-impulse response,
which is not deﬁned at zero.
I
The discrete approximation of derivatives and integrals provides an approximation of diffe-
rential equations, representing dynamic continuous-time systems by difference equations.
Extending the concept of linear time invariance to discrete-time systems, we obtain a convo-
lution sum to represent LTI systems. Thus, dynamic discrete-time systems can be represented
by difference equations and convolution sums. A computationally signiﬁcant difference with
continuous-time systems is that the solution of difference equations can be recursively obtained,
and that the convolution sum provides a class of systems that do not have a counterpart in the
analog domain.
8.2 DISCRETE-TIME SIGNALS
A discrete-time signal x[n] can be thought of as a real- or complex-valued function of the integer sample
index n:
x[.] : I →R (C)
n
x[n]
(8.1)
The above means that for discrete-time signals the independent variable is an integer n, the sample
index, and that the value of the signal at n, x[n], is either a real- or a complex-value function. Thus,
the signal is only deﬁned at integer values n—no deﬁnition exists for values between the integers.
Remarks
I
It should be understood that a sampled signal x(nTs) = x(t)|t=nTs is a discrete-time signal x[n] that is a
function of n only. Once the value of Ts is known, the sampled signal only depends on n, the sample index.
However, this should not prevent us in some situations from considering a discrete-time signal obtained
through sampling as a function of time t where the signal values only exist at discrete times {nTs}.
I
Although in many situations discrete-time signals are obtained from continuous-time signals by sampling,
that is not always the case. There are many signals that are inherently discrete—think, for instance, of a
signal consisting of the ﬁnal values attained daily by the shares of a company in the stock market. Such
a signal would consist of the values reached by the share in the days when the stock market opens. This
signal is naturally discrete. A signal generated by a random number generator in a computer would be
a sequence of real values and can be considered a discrete-time signal. Telemetry signals, consisting of
measurements—for example, voltages, temperatures, pressures—from a certain process, taken at certain
times, are also naturally discrete.
I Example 8.1
Consider a sinusoidal signal
x(t) = 3 cos(2πt + π/4)
−∞< t < ∞

8.2 Discrete-Time Signals
453
Determine an appropriate sampling period Ts according to the Nyquist sampling rate condition,
and obtain the discrete-time signal x[n] corresponding to the largest allowed sampling period.
Solution
To sample x(t) so that no information is lost, the Nyquist sampling rate condition indicates that
the sampling period should be
Ts ≤
π
max
= π
2π = 0.5
For the largest allowed sampling period Ts = 0.5, we obtain
x[n] = 3 cos(2πt + π/4)|t=0.5n = 3 cos(πn + π/4)
−∞< n < ∞
which is a function of the integer n.
I
I Example 8.2
To generate the celebrated Fibonacci sequence of numbers, {x[n]}, we use the recursive equation
x[n] = x[n −1] + x[n −2]
n ≥2
x[0] = 0
x[1] = 1
which is a difference equation with zero input and two initial conditions. The Fibonacci sequence
has been used to model different biological systems.1 Find the Fibonacci sequence.
Solution
The given equation allows us to compute the Fibonacci sequence recursively. For n ≥2, we ﬁnd
x[2] = 1 + 0 = 1
x[3] = 1 + 1 = 2
x[4] = 2 + 1 = 3
x[5] = 3 + 2 = 5
...
where we are simply adding the previous two numbers in the sequence. The sequence is purely
discrete as it is not related to a continuous-time signal.
I
1Leonardo of Pisa (also known as Fibonacci) in his book Liber Abaci described how his sequence could be used to model the
reproduction of rabbits over a number of months assuming bunnies begin breeding when they are a few months old.

454
CHAPTER 8:
Discrete-Time Signals and Systems
8.2.1 Periodic and Aperiodic Signals
A discrete-time signal x[n] is periodic if
I
It is deﬁned for all possible values of n, −∞< n < ∞.
I
There is a positive integer N, the period of x[n], such that
x[n + kN] = x[n]
(8.2)
for any integer k.
Periodic discrete-time sinusoids, of period N, are of the form
x[n] = A cos
2πm
N n + θ

−∞< n < ∞
(8.3)
where the discrete frequency is ω0 = 2πm/N rad, for positive integers m and N, which are not divisible by
each other, and θ is the phase angle.
The deﬁnition of a discrete-time periodic signal is similar to that of continuous-time periodic signals,
except for the period being an integer. That discrete-time sinusoids are of the given form can be easily
shown: Shifting the sinusoid in Equation (8.3) by a multiple k of the period N, we have
x[n + kN] = A cos
2πm
N (n + kN) + θ

= A cos
2πm
N n + 2πmk + θ

= x[n]
since we add to the original angle a multiple mk (an integer) of 2π, which does not change the angle.
Remarks
I
The units of the discrete frequency ω is radians. Moreover, discrete frequencies repeat every 2π (i.e.,
ω = ω + 2πk for any integer k), and as such we only need to consider the range −π ≤ω < π. This is
in contrast with the analog frequency , which has rad/sec as units, and its range is from −∞to ∞.
I
If the frequency of a periodic sinusoid is
ω = 2π
N m
for nondivisible integers m and N > 0, the period is N. If the frequency of the sinusoid cannot be written
like this, the discrete sinusoid is not periodic.
I Example 8.3
Consider the sinusoids
x1[n] = 2 cos(πn −π/3)
x2[n] = 3 sin(3πn + π/2)
−∞< n < ∞

8.2 Discrete-Time Signals
455
From their frequencies determine if these signals are periodic, and if so, determine their
corresponding periods.
Solution
The frequency of x1[n] can be written as
ω1 = π = 2π
2
where m = 1 and N = 2, so that x1[n] is periodic of period N1 = 2. Likewise, the frequency of x2[n]
can be written as
ω2 = 3π = 2π
2 3
where m = 3 and N = 2, so that x2[n] is also periodic of period N2 = 2, which can be veriﬁed as
follows:
x2[n + 2] = 3 sin(3π(n + 2) + π/2) = 3 sin(3πn + 6π + π/2) = x[n]
I
I Example 8.4
What is true for continuous-time sinusoids—that they are always periodic—is not true for discrete-
time sinusoids. These sinusoids can be nonperiodic even if they result from uniformly sampling a
continuous-time sinusoid. Consider the discrete signal x[n] = cos(n + π/4), which is obtained by
sampling the analog sinusoid x(t) = cos(t + π/4) with a sampling period Ts = 1 sec/sample. Is x[n]
periodic? If so, indicate its period. Otherwise, determine values of the sampling period, satisfying
the Nyquist sampling rate condition, that when used in sampling x(t) result in periodic signals.
Solution
The sampled signal x[n] = x(t)|t=nTs = cos(n + π/4) has a discrete frequency ω = 1 rad that cannot
be expressed as 2πm/N for any integers m and N because π is an irrational number. So x[n] is not
periodic.
Since the frequency of the continuous-time signal x(t) is  = 1 (rad/sec), then the sampling period,
according to the Nyquist sampling rate condition, should be
Ts ≤π
 = π
and for the sampled signal x(t)|t=nTs = cos(nTs + π/4) to be periodic of period N or
cos((n + N)Ts + π/4) = cos(nTs + π/4)
is necessary that
NTs = 2kπ
for an integer k (i.e., a multiple of 2π). Thus, Ts = 2kπ/N ≤π satisﬁes the Nyquist sampling con-
dition at the same time that it ensures the periodicity of the sampled signal. For instance, if we

456
CHAPTER 8:
Discrete-Time Signals and Systems
wish to have a sinusoid with period N = 10, then Ts = 0.2kπ for k chosen so the Nyquist sampling
rate condition is satisﬁed—that is,
0 < Ts = kπ/5 ≤π
so that 0 < k ≤5.
From these possible values for k we choose k = 1 and 3 so that N and k are not divisible by each
other and we get the desired period N = 10 (the values k = 2 and 4 would give 5 as the period,
and k = 5 would give a period of 2 instead of 10). Indeed, if we let k = 1, then Ts = 0.2π satisﬁes
the Nyquist sampling rate condition, and we obtain the sampled signal
x[n] = cos(0.2nπ + π/4) = cos
2π
10 n + π
4

which according to its frequency is periodic of period 10. This is the same for k = 3.
I
When sampling an analog sinusoid
x(t) = A cos(0t + θ)
−∞< t < ∞
(8.4)
of period T0 = 2π/0, 0 > 0, we obtain a periodic discrete sinusoid,
x[n] = A cos(0Tsn + θ) = A cos
2πTs
T0
n + θ

(8.5)
provided that
Ts
T0
= m
N
(8.6)
for positive integers N and m, which are not divisible by each other. To avoid frequency aliasing the sampling
period should also satisfy
Ts ≤π
0
= T0
2
(8.7)
Indeed, sampling a continuous-time signal x(t) using as sampling period Ts, we obtain
x[n] = A cos(0Tsn + θ)
= A cos
2πTs
T0
n + θ

where the discrete frequency is ω0 = 2πTs/T0. For this signal to be periodic we should be able to
express this frequency as 2πm/N for nondivisible positive integers m and N. This requires that
T0
Ts
= N
m
be a rational number, or that
mT0 = NTs
(8.8)

8.2 Discrete-Time Signals
457
which says that a period (m = 1) or several periods (m > 1) should be divided into N > 0 segments of
duration Ts seconds. If the condition in Equation (8.6) is not satisﬁed, then the discretized sinusoid
is not periodic. To avoid frequency aliasing the sampling period should be chosen so that
Ts ≤π
0
= T0
2
The sum z[n] = x[n] + y[n] of periodic signals x[n] with period N1, and y[n] with period N2 is periodic if the
ratio of periods of the summands is rational—that is,
N2
N1
= p
q
where p and q are integers not divisible by each other. If so, the period of z[n] is qN2 = pN1.
If qN2 = pN1, we then have that
z[n + pN1] = x[n + pN1] + y[n + pN1]
= x[n] + y[n + qN2]
= x[n] + y[n] = z[n]
since pN1 and qN2 are multiples of the periods of x[n] and y[n].
I Example 8.5
The signal
z[n] = v[n] + w[n] + y[n]
is the sum of three periodic signals v[n], w[n], and y[n] of periods N1 = 2, N2 = 3, and N3 = 4,
respectively. Determine if z[n] is periodic, and if so, determine its period.
Solution
Let x[n] = v[n] + w[n], so that z[n] = x[n] + y[n]. The signal x[n] is periodic since N2/N1 = 3/2 is a
rational number and 3 and 2 are non-divisible by each other, and its period is N4 = 3N1 = 2N2 = 6.
The signal z[n] is also periodic since
N4
N3
= 6
4 = 3
2
Its period is N = 2N4 = 3N3 = 12. Thus, z[n] is periodic of period 12, indeed
z[n + 12] = v[n + 6N1] + w[n + 4N2] + y[n + 3N3] = v[n] + w[n] + y[n] = z[n]
I

458
CHAPTER 8:
Discrete-Time Signals and Systems
I Example 8.6
Determine if the signal
x[n] =
∞
X
m=0
Xm cos(mω0n)
ω0 = 2π
N0
is periodic, and if so, determine its period.
Solution
The signal x[n] consists of the sum of a constant X0 and cosines of frequency
mω0 = 2πm
N0
m = 1, 2, . . .
The periodicity of x[n] depends on the periodicity of the cosines. According to the frequency of the
cosines, they are periodic of period N0. Thus, x[n] is periodic of period N0. Indeed
x[n + N0] =
∞
X
m=0
Xm cos(mω0(n + N0))
=
∞
X
m=0
Xm cos(mω0n + 2πm) = x[n]
I
8.2.2 Finite-Energy and Finite-Power Discrete-Time signals
For discrete-time signals, we obtain deﬁnitions for energy and power similar to those for continuous-
time signals by replacing integrals by summations.
For a discrete-time signal x[n], we have the following deﬁnitions:
Energy:
εx =
∞
X
n=−∞
|x[n]|2
(8.9)
Power:
Px = lim
N→∞
1
2N + 1
N
X
n=−N
|x[n]|2
(8.10)
I
x[n] is said to have ﬁnite energy or to be square summable if εx < ∞.
I
x[n] is called absolutely summable if
∞
X
n=−∞
|x[n]| < ∞
(8.11)
I
x[n] is said to have ﬁnite power if Px < ∞.

8.2 Discrete-Time Signals
459
I Example 8.7
A “causal” sinusoid, obtained from a signal generator after it is switched on, is
x(t) =
(
2 cos(0t −π/4)
t ≥0
0
otherwise
The signal x(t) is sampled using a sampling period of Ts = 0.1 sec to obtain a discrete-time signal
x[n] = x(t)|t=0.1n = 2 cos(0.10n −π/4)
n ≥0
and zero otherwise. Determine if this discrete-time signal has ﬁnite energy and ﬁnite power and
compare these characteristics with those of the continuous-time signal x(t) when 0 = π and when
0 = 3.2 rad/sec (an upper approximation of π).
Solution
The continuous-time signal x(t) has inﬁnite energy, and so does the discrete-time signal x[n], for
both values of 0. Indeed, its energy is
εx =
∞
X
n=−∞
x[n]2 =
∞
X
n=0
4 cos2(0.10n −π/4) →∞
Although the continuous-time and the discrete-time signals have inﬁnite energy, they have ﬁnite
power. That the continuous-time signal has ﬁnite power can be shown as indicated in Chapter 1.
For the discrete-time signal x[n], we have for the two frequencies:
1.
For 0 = π, x1[n] = 2 cos(πn/10 −π/4) = 2 cos(2πn/20 −π/4) for n ≥0 and zero other-
wise. Thus, x[n] repeats every N0 = 20 samples for n ≥0, and its power is
Px = lim
N→∞
1
2N + 1
N
X
n=−N
|x1[n]|2 = lim
N→∞
1
2N + 1
N
X
n=0
|x1[n]|2
= lim
N→∞
1
2N + 1 N

1
N0
N0−1
X
n=0
|x1[n]|2


|
{z
}
power of period, n≥0
=
1
2N0
N0−1
X
n=0
|x1[n]|2 < ∞
where we used the causality of the signal (x1[n] = 0 for n < 0), and considered N periods of
x1[n] for n ≥0, and for each computed its power to get the ﬁnal result. Thus, for 0 = π
the discrete-time signal x1[n] has ﬁnite power and can be computed using a period for n ≥0.

460
CHAPTER 8:
Discrete-Time Signals and Systems
To ﬁnd the power we use the trigonometric identity (or Euler’s equation) cos2(θ) = 0.5(1 +
cos(2θ)), and so replacing x1[n], we have
Px =
4
2N0
0.5


N0−1
X
n=0
1 +
N0−1
X
n=0
cos(0.2πn −π/2)


N0 = 20
= 2
40[20 + 0] = 1
where the sum of the cosine is zero, as we are adding the values of the periodic cosine over a
period.
2.
For 0 = 3.2, x2[n] = 2 cos(3.2n/10 −π/4) for n ≥0 and zero otherwise. The signal now does
not repeat periodically after n = 0, as the frequency 3.2/10 (which equals the rational 32/100)
cannot be expressed as 2πm/N (which due to π is an irrational value) for integers m and N.
Thus, in this case we do not have a close form for the power, so we can simply say that the
power is
Px = lim
N→∞
1
2N + 1
N
X
n=−N
x2[n]
2
and conjecture that because the corresponding analog signal has ﬁnite power, so would x2[n].
Thus, we use MATLAB to compute the power for both cases.
%%%%%%%%%%%%%%%%%%%%%%%
% Example 8.7 --- Power
%%%%%%%%%%%%%%%%%%%%%%%
clear all;clf
n = 0:100000;
x2 = 2∗cos(0.1∗n∗3.2 −pi/4); % non-periodic for positive n
x1 = 2∗cos(0.1∗n∗pi −pi/4);
% periodic for positive n
N = length(x1)
Px1 = sum(x1.ˆ2)/(2∗N+1) % power of x1
Px2 = sum(x2.ˆ2)/(2∗N+1) % power of x2
P1 = sum(x1(1:20).ˆ2)/(20); %power in period of x1
The signal x1[n] in the script has unit power and so does the signal x2[n] when we consider
100,001 samples. The two signals and their squared magnitudes are shown in Figure 8.1.
I
I Example 8.8
Determine if a discrete-time exponential
x[n] = 2(0.5)n
n ≥0
and zero otherwise, has ﬁnite energy, ﬁnite power, or both.

8.2 Discrete-Time Signals
461
FIGURE 8.1
(a) Signal x2[n]
(nonperiodic for n ≥0)
and (b) signal x1[n]
(periodic for n ≥0).
The arrows show that
the values are not
equal for x2[n] and
equal for x1[n]. (c) The
square of the signals
differ slightly,
suggesting that if
x1[n] has ﬁnite power
so does x2[n].
−10
0
10
20
30
40
−2
−1
0
1
2
n
x2[n]
−10
0
10
20
30
40
−2
−1
0
1
2
n
x1[n]
(a)
(b)
−10
0
10
20
30
40
0
1
2
3
4
n
x 2
1[n], x 2
2[n]
x 2
2[n]
x 2
1[n]
(c)
Solution
The energy is given by
εx =
∞
X
n=0
4(0.5)2n = 4
∞
X
n=0
(0.25)n =
4
1 −0.25 = 16
3
thus, x[n] is a ﬁnite-energy signal. Just as with continuous-time signals, a ﬁnite-energy signal is a
ﬁnite-power (actually zero power) signal.
I
8.2.3 Even and Odd Signals
Time shifting and scaling of discrete-time signals are very similar to the continuous-time cases, the
only difference being that the operations are now done using integers.
A discrete-time signal x[n] is said to be
I
Delayed by N (an integer) samples if x[n −N] is x[n] shifted to the right N samples.
I
Advanced by M (an integer) samples if x[n + M] is x[n] shifted to the left M samples.
I
Reﬂected if the variable n in x[n] is negated (i.e., x[−n]).
The shifting to the right or the left can be readily seen by considering where x[0] is attained. For x[n −
N], this is when n = N (i.e., N samples to the right of the origin), or x[n] is delayed by N samples.

462
CHAPTER 8:
Discrete-Time Signals and Systems
Likewise, for x[n + M] the x[0] appears advanced by M samples, or shifted to the left (i.e., when
n = −M). Negating the variable n ﬂips over the signal with respect to the origin.
I Example 8.9
A triangular discrete pulse is deﬁned as
x[n] =
n
0 ≤n ≤10
0
otherwise
Find an expression for y[n] = x[n + 3] + x[n −3] and z[n] = x[−n] + x[n] in terms of n and
carefully plot them.
Solution
Replacing n by n + 3 and n −3 in the deﬁnition of x[n], we get the advanced and delayed signals
x[n + 3] =
n + 3
−3 ≤n ≤7
0
otherwise
and
x[n −3] =
n −3
3 ≤n ≤13
0
otherwise
so that when added, we get
y[n] = x[n + 3] + x[n −3] =



n + 3
−3 ≤n ≤2
2n
3 ≤n ≤7
n −3
8 ≤n ≤13
0
otherwise
Likewise, we have that
z[n] = x[n] + x[−n] =



n
1 ≤n ≤10
0
n = 0
−n
−10 ≤n ≤−1
0
otherwise
The results are shown in Figure 8.2.
I
I Example 8.10
We will see that in the convolution sum we need to ﬁgure out how a signal x[n −k] behaves as a
function of k for different values of n. Consider the signal
x[k] =
k
0 ≤k ≤3
0
otherwise

8.2 Discrete-Time Signals
463
−5
0
5
10
15
0
5
10
15
n
y[n] = x[n +3] + x[n−3]
−5
0
5
10
15
n
x[n+ 3]
0
2
4
6
8
10
12
x[n−3]
−5
0
5
10
15
n
0
2
4
6
8
10
12
0
2
4
6
8
10
12
x[−n]
−10
0
10
−10
0
10
0
2
4
6
8
10
12
n
n
x[n]
−15
−10
−5
0
5
10
15
0
2
4
6
8
10
12
n
z [n] =x[n] + x[−n]
(a)
(b)
−5
0
5
10
15
0
2
4
6
8
10
12
n
x[n]
FIGURE 8.2
Generation of (a) y[n] = x[n + 3] + x[n −3] and (b) z[n] = x[n] + x[−n].

464
CHAPTER 8:
Discrete-Time Signals and Systems
Obtain an expression for x[n −k] for −2 ≤n ≤2 and determine in which direction it shifts as n
increases from −2 to 2.
Solution
Although x[n −k], as a function of k, is reﬂected it is not clear if it is advanced or delayed as n
increases from −2 to 2. If n = 0,
x[−k] =
−k
−3 ≤k ≤0
0
otherwise
For n ̸= 0, we have that
x[n −k] =
n −k
n −3 ≤k ≤n
0
otherwise
As n increases from −2 to 2 the supports of x[n −k] move to the right. For n = −2 the support of
x[−2 −k] is −5 ≤k ≤−2, while for n = 0 the support of x[−k] is −3 ≤k ≤0, and for n = 2 the
support of x[2 −k] is −1 ≤k ≤2, each shifted to the right.
I
We can thus use the above to deﬁne even and odd signals and obtain a general decomposition of any
signal in terms of even and odd signals.
Even and odd discrete-time signals are deﬁned as
x[n] is even:
⇔
x[n] = x[−n]
(8.12)
x[n] is odd:
⇔
x[n] = −x[−n]
(8.13)
Any discrete-time signal x[n] can be represented as the sum of an even and an odd component,
x[n] = 1
2
 x[n] + x[−n]

|
{z
}
xe[n]
+ 1
2
 x[n] −x[−n]

|
{z
}
xo[n]
= xe[n] + xo[n]
(8.14)
The even and odd decomposition can be easily seen. The even component xe[n] = 0.5(x[n] + x[−n])
is even since xe[−n] = 0.5(x[−n] + x[n]) equals xe[n], and the odd component xo[n] = 0.5(x[n] −
x[−n]) is odd since xo[−n] = 0.5(x[−n] −x[n]) = −xo[n].
I Example 8.11
Find the even and the odd components of the discrete-time signal
x[n] =
4 −n
0 ≤n ≤4
0
otherwise

8.2 Discrete-Time Signals
465
Solution
The even component of x[n] is given by
xe[n] = 0.5
 x[n] + x[−n]

When n = 0 then xe[0] = 0.5 × 2x[0] = 4, when n > 0 then xe[n] = 0.5x[n], and when n < 0 then
xe[n] = 0.5x[−n], giving
xe[n] =



2 + 0.5n
−4 ≤n ≤−1
4
n = 0
2 −0.5n
1 ≤n ≤4
0
otherwise
The odd component
xo[n] = 0.5
 x[n] −x[−n]

gives 0 when n = 0, 0.5x[n] for n > 0, and −0.5x[−n] when n < 0, or
xo[n] =



−2 −0.5n
−4 ≤n ≤−1
0
n = 0
2 −0.5n
1 ≤n ≤4
0
otherwise
The sum of these two components gives x[n].
I
Remarks Expansion and compression of discrete-time signals is more complicated than in the continuous-
time signals. In the discrete domain, expansion and compression can be related to the change of the sampling
period in the sampling. Thus, if a continuous-time signal x(t) is sampled using a sampling period Ts, by chang-
ing the sampling period to MTs for an integer M > 1, we obtain fewer samples, and by changing the sampling
period to Ts/L for an integer L > 1, we increase the number of samples. For the corresponding discrete-time
signal x[n], increasing the sampling period would give x[Mn], which is called the downsampling of x[n] by
M. Unfortunately, because the argument of discrete-time signals must be integers, it is not clear what x[n/L]
is unless the values for n are multiples of L (i.e., n = ±0, ±L, ±2L, . . .) without a clear deﬁnition when n
takes other values. This leads to the deﬁnition of the upsampled signal
xu[n] =
x[n/L]
n = ±0, ±L, ±2L, . . .
0
otherwise
(8.15)
To replace the zero entries with the values obtained by decreasing the sampling period we need to low-pass ﬁlter
the upsampled signal. MATLAB provides the functions decimate and interp to implement the downsampling
and upsampling without losing information due to possible frequency aliasing. In Chapter 10, we will continue
the discussion of these operations including their frequency characterization.
8.2.4 Basic Discrete-Time Signals
The representation of discrete-time signals via basic signals is simpler than in the continuous-time
domain. This is due to the lack of ambiguity in the deﬁnition of the impulse and the unit-step
discrete-time signals. The deﬁnitions of impulses and unit-step signals in the continuous-time
domain are more abstract.

466
CHAPTER 8:
Discrete-Time Signals and Systems
Discrete-Time Complex Exponential
Given complex numbers A = |A|ejθ and α = |α|ejω0, a discrete-time complex exponential is a signal of
the form
x[n] = Aαn
= |A||α|nej(ω0n+θ)
= |A||α|n [cos(ω0n + θ) + j sin(ω0n + θ)]
(8.16)
where ω0 is a discrete frequency in radians.
Remarks
I
The discrete-time complex exponential looks different from the continuous-time complex exponential. This
can be explained by sampling the continuous-time complex exponential
x(t) = Ae(−a+j0)t
(for simplicity we let A be real) using as sampling period Ts. The sampled signal is
x[n] = x(nTs) = Ae(−anTs+j0nTs) = A(e−aTs)nej(0Ts)n
= Aαnejω0n
where we let α = e−aTs and ω0 = 0Ts.
I
Just as with the continuous-time complex exponential, we obtain different signals depending on the chosen
parameters A and α. For instance, the real part of x[n] in Equation (8.16) is a real signal
g[n] = Re[x[n]] = |A||α|n cos(ω0n + θ)
where when |α| < 1 it is a damped sinusoid, and when |α| > 1 it is a growing sinusoid (see Figure 8.3).
If α = 1 then the above signal is a sinusoid.
I
It is important to realize that for α > 0 the real exponential
x[n] = (−α)n = (−1)nαn = αn cos(πn)
I Example 8.12
Given the analog signal
x(t) = e−at cos(0t)u(t)
determine the values of a > 0, 0, and Ts that permit us to obtain a discrete-time signal
y[n] = αn cos(ω0n)
n ≥0

8.2 Discrete-Time Signals
467
−6
−4
−2
0
2
4
6
0
1
2
3
4
n
x1[n]= (0.8)n
x2[n] = 1.25n
−6
−4
−2
0
2
4
6
n
0
1
2
3
4
−5
0
5
−4
−2
0
2
4
n
y1[n]
−5
0
5
−4
−2
0
2
4
n
y2[n]
(a)
(b)
FIGURE 8.3
(a) Real exponential x1[n] = 0.8n, x2[n] = 1.25n, and (b) modulated exponential y1[n] = x1[n] cos(πn) and
y2[n] = x2[n] cos(πn).
and zero otherwise. Consider the case when α = 0.9 and ω0 = π/2. Find a, 0, and Ts that will
permit us to obtain y[n] from x(t) by sampling. Plot x[n] and y[n] using MATLAB.
Solution
Comparing the sampled continuous-time signal x(nTs) = (e−aTs)n cos((0Ts)n)u[n] with y[n] we
obtain the following two equations:
α = e−aTs
ω0 = 0Ts
with three unknowns (a, 0, and Ts), so there is no unique solution. According to the Nyquist
sampling rate condition,
Ts ≤
π
max

468
CHAPTER 8:
Discrete-Time Signals and Systems
Assuming the maximum frequency is max = N0 for N ≥2 (since the signal is not band limited
the maximum frequency is not known; to estimate it we could use Parseval’s result as indicated in
Chapter 7, instead we are assuming that it is a multiple of 0), if we let Ts = π/N0 after replacing
it in the above equations, we get
α = e−aπ/N0
ω0 = 0π/N0 = π/N
If we want α = 0.9 and ω0 = π/2, we have that N = 2 and
a = −20
π
log 0.9
for any frequency 0 > 0. For instance, if 0 = 2π, then a = −4 log 0.9 and Ts = 0.25. Figure 8.4
displays the continuous- and the discrete-time signals generated using the above parameters. The
following script is used. The continuous-time and the discrete-time signals coincide at the sample
times.
%%%%%%%%%%%%%%%%%%%%%
% Example 8.12
%%%%%%%%%%%%%%%%%%%%%
a = −4∗log(0.9);Ts = 0.25; % parameters
alpha = exp(−a∗Ts);
n = 0:30; y = alpha.ˆn.∗cos(pi∗n/2); % discrete-time signal
t = 0:0.001:max(n)∗Ts; x = exp(−a∗t).∗cos(2∗pi∗t); % analog signal
stem(n, y, ’r’); hold on
plot(t/Ts, x); grid; legend(’y[n]’, ’x(t)’); hold off
FIGURE 8.4
Determination of parameters for a
continuous-time signal x(t) that
when sampled gives a desired
discrete-time signal y[n].
0
5
10
15
20
25
30
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
t /Ts
x (t ), y [n]
y[n]
x(t)
I

8.2 Discrete-Time Signals
469
I Example 8.13
Show how to obtain the discrete-time exponential x[n] = (−1)n for n ≥0 and zero otherwise, by
sampling a continuous-time signal x(t).
Solution
Because the values of x[n] are 1 and −1, x[n] cannot be generated by sampling a real exponential
signal e−atu(t); indeed, e−at > 0 for any values of a and t. The discrete signal can be written as
x[n] = (−1)n = cos(πn)
for n ≥0. If we sample an analog signal x(t) = cos(0t)u(t) with a sampling period Ts, we get
x[n] = x(nTs) = cos(0nTs) = cos(πn)
n ≥0
and zero otherwise. Thus, 0Ts = π, giving Ts = π/0. For instance, for 0 = 2π, then Ts = 0.5. I
Discrete-Time Sinusoids
Discrete-time sinusoids are a special case of the complex exponential. Letting α = ejω0 and A = |A|ejθ,
we have according to Equation (8.16),
x[n] = Aαn = |A|ej(ω0n+θ) = |A| cos(ω0n + θ) + j|A| sin(ω0n + θ)
(8.17)
so the real part of x[n] is a cosine, while the imaginary part is a sine. As indicated before, discrete
sinusoids of amplitude A and phase shift θ are periodic if they can be expressed as
A cos(ω0n + θ) = A sin(ω0n + θ + π/2)
−∞< n < ∞
(8.18)
where w0 = 2πm/N rad is the discrete frequency for integers m and N > 0, which are not divisible
by each other. Otherwise, discrete-time sinusoids are not periodic.
Because ω is given in radians, it repeats periodically with 2π as the period—that is,
ω = ω + 2πk
k integer
(8.19)
To avoid this ambiguity, we will let −π < ω ≤π as the possible range of discrete frequencies. This is
possible since
ω =
ω −2πk
when ω > 2π, for some k > 0 integer
ω −2π
0 ≤ω ≤2π
(8.20)
See Figure 8.5. Thus, sin(3πn) equals sin(πn), and sin(1.5πn) equals sin(−0.5πn) = −sin(0.5πn).

470
CHAPTER 8:
Discrete-Time Signals and Systems
FIGURE 8.5
Discrete frequencies ω.
ω = π/2, 5π/2, 9π/2 · · · = π/2
ω =3π/2, 7π/2, 11π/2, ··· = −π/2
ω = 0, 2π, 4π, ··· =0
ω = π, 3π, 5π ··· = π = −π
I Example 8.14
Consider the following four sinusoids:
(a) x1[n] = sin(0.1πn)
(b) x2[n] = sin(0.2πn)
(c) x3[n] = sin(0.6πn)
(d) x4[n] = sin(0.7πn)
Find if they are periodic, and if so, determine their periods. Are these signals harmonically
related? Use MATLAB to plot these signals from n = 0, . . . , 40. Comment on which of these signals
resemble sampled analog sinusoids.
Solution
To ﬁnd if they are periodic, rewrite the given signals as follows indicating that the signals are
periodic of periods 20, 10, 10, and 20:
(a) x1[n] = sin(0.1πn) = sin
2π
20 n

(b) x2[n] = sin(0.2πn) = sin
2π
10 n

(c) x3[n] = sin(0.6πn) = sin
2π
10 3n

(d) x4[n] = sin(0.7πn) = sin
2π
20 7n

If we let ω1 = 2π/20, the frequencies of x2[n], x3[n], and x4[n] are 2ω1, 6ω1, and 7ω1, respectively;
thus they are harmonically related. Also, one could consider the frequencies of x1[n] and x4[n]
harmonically related (i.e., the frequency of x4[n] is seven times that of x1[n]), and likewise the
frequencies of x2[n] and x3[n] are also harmonically related, with the frequency of x3[n] being

8.2 Discrete-Time Signals
471
0
10
20
(a)
(b)
(c)
(d)
30
40
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0.5
0
1
−1
−0.5
0
0.5
1
n
0
10
20
30
40
n
x1[n]
x3[n]
x4[n]
x2[n]
0
10
20
30
40
n
0
10
20
30
40
n
FIGURE 8.6
Periodic signals xi[n], (a) i = 1, (b) i = 2, (c) i = 3, and (d) i = 4, given in Example 8.14.
three times that of x2[n]. When plotting these signals using MATLAB, the ﬁrst two resemble analog
sinusoids but not the other two. See Figure 8.6.
I
Remarks
I
The discrete-time sine and cosine signals, as in the continuous-time case, are out of phase π/2 radians.
I
The discrete frequency ω is given in radians since n, the sample index, does not have units. This can also
be seen when we sample a sinusoid using a sampling period Ts so that
cos(0t)|t=nTs = cos(0Tsn) = cos(ω0n)
where we deﬁned ω0 = 0Ts, and since 0 has rad/sec as units and Ts has seconds as units, then ω0 has
radians as units.
I
The frequency  of analog sinusoids can vary from 0 (dc frequency) to ∞. Discrete frequencies ω as
radian frequencies can only vary from 0 to π. Negative frequencies are needed in the analysis of real-
valued signals; thus −∞<  < ∞and −π < ω ≤π. A discrete-time cosine of frequency 0 is constant
for all n, and a discrete-time cosine of frequency π varies from −1 to 1 from sample to sample, giving the
largest variation possible for the discrete-time signal.

472
CHAPTER 8:
Discrete-Time Signals and Systems
Discrete-Time Unit-Step and Unit-Sample Signals
The unit-step u[n] and the unit-sample δ[n] discrete-time signals are deﬁned as
u[n] =
(
1
n ≥0
0
n < 0
(8.21)
δ[n] =
(
1
n = 0
0
otherwise
(8.22)
These two signals are related as follows:
δ[n] = u[n] −u[n −1]
(8.23)
u[n] =
∞
X
k=0
δ[n −k] =
n
X
m=−∞
δ[m]
(8.24)
It is easy to see the relation between the two signals u[n] and δ[n]:
δ[n] = u[n] −u[n −1]
u[n] = δ[n] + δ[n −1] + · · ·
=
∞
X
k=0
δ[n −k] =
n
X
m=−∞
δ[m]
where the last expression is obtained by a change of variable, m = n −k. These two equations should
be contrasted with the ones for u(t) and δ(t). Instead of the derivative relation δ(t) = du(t)/dt, we
have a difference relation, and instead of the integral connection
u(t) =
t
Z
−∞
δ(τ)dτ
we now have a summation relation between u[n] and δ[n].
Remarks Notice that there is no ambiguity in the deﬁnition of u[n] or δ[n] as there is for their continuous-
time counterparts u(t) and δ(t). Moreover, the deﬁnitions of these functions do not depend on u(t) or δ(t),
and u[n] and δ[n] are not sampled versions of u(t) and δ(t).
Generic Representation of Discrete-Time Signals
Any discrete-time signal x[n] is represented using unit-sample signals as
x[n] =
∞
X
k=−∞
x[k]δ[n −k]
(8.25)

8.2 Discrete-Time Signals
473
The representation of any signal x[n] in terms of δ[n] results from the sifting property of the unit-sample
signal:
x[n]δ[n −n0] = x[n0]δ[n −n0]
which is due to
δ[n −n0] =
(1
n = n0
0
otherwise
Thus, considering x[n] a sequence of samples
. . . x[−1] x[0] x[1] . . .
at times . . . −1, 0, 1, . . . , we can write x[n] as
x[n] = · · · + x[−1]δ[n + 1] + x[0]δ[n] + x[1]δ[n −1] + · · ·
=
∞
X
k=−∞
x[k]δ[n −k]
The generic representation (Eq. 8.25) of any signal x[n] will be useful in ﬁnding the output of a
discrete-time linear time-invariant system.
I Example 8.15
Use the generic representations in terms of unit-sample signals to represent the ramp signal r[n]
deﬁned as
r[n] = nu[n]
and from it show that
r[n] =
n
X
m=0
(n −m) −
n
X
m=1
(n −m)

474
CHAPTER 8:
Discrete-Time Signals and Systems
Solution
Using the unit-sample signal generic representation, we have
r[n] =
∞
X
k=−∞
(ku[k])δ[n −k] =
∞
X
k=0
kδ[n −k] = 0δ[n] + 1δ[n −1] + 2δ[n −2] + · · ·
Letting m = n −k, we write the above equation as
r[n] =
n
X
m=−∞
(n −m)δ[m] =
n
X
m=−∞
(n −m)(u[m] −u[m −1])
=
n
X
m=0
(n −m) −
n
X
m=1
(n −m) = n +
n
X
m=1
(n −m) −
n
X
m=1
(n −m) = n
n ≥0
For n < 0, r[n] = 0.
I
I Example 8.16
Consider a discrete pulse
x[n] =
1
0 ≤n ≤N −1
0
otherwise
Obtain representations of x[n] using unit-sample and unit-step signals.
Solution
The signal x[n] can be represented as
x[n] =
N−1
X
k=0
δ[n −k]
and using δ[n] = u[n] −u[n −1], we obtain a representation of the discrete pulse in terms of unit-
step signals,
x[n] =
N−1
X
k=0
(u[n −k] −u[n −k −1]) = (u[n] −u[n −1]) + (u[n −1] −u[n −2])
+ · · · −u[n −N]
= u[n] −u[n −N]
because of the cancellation of consecutive terms.
I

8.2 Discrete-Time Signals
475
I Example 8.17
Consider how to generate a train of triangular, discrete-time pulses t[n], which is periodic of period
N = 11. A period of t[n] is
τ[n] =



n
0 ≤n ≤5
−n + 10
6 ≤n ≤10
0
otherwise
Find then an expression for its ﬁnite difference d[n] = t[n] −t[n −1].
Solution
The periodic signal can be generated by adding shifted versions of τ[n], or
t[n] = · · · + τ[n + 11] + τ[n] + τ[n −11] + · · · =
∞
X
k=−∞
τ[n −11k]
The ﬁnite difference d[n] is then
d[n] = t[n] −t[n −1]
=
∞
X
k=−∞
(τ[n −11k] −τ[n −1 −11k])
The signal d[n] is also periodic of the same period N = 11 as t[n]. If we let
s[n] = τ[n] −τ[n −1] =



1
0 ≤n ≤5
−1
6 ≤n ≤10
0
otherwise
then
d[n] =
∞
X
k=−∞
s[n −11k]
When sampled, these two signals look very much like the continuous-time train of triangular
pulses, and its derivative.
I
I Example 8.18
Consider the discrete-time signal
y[n] = 3r(t + 3) −6r(t + 1) + 3r(t) −3u(t −3)|t=0.15n

476
CHAPTER 8:
Discrete-Time Signals and Systems
obtained by sampling a continuous-time signal formed by ramp and unit-step signals with a
sampling period Ts = 0.15. Write MATLAB functions to generate the ramp and the unit-step signals
and obtain y[n]. Then write a MATLAB function that provides the even and the odd decomposition
of y[n].
Solution
The real-valued signal is obtained by sequentially adding the different signals as we go from −∞
to ∞:
y(t) =



0
t < −3
3r(t + 3) = 3t + 9
−3 ≤t < −1
3t + 9 −6r(t + 1) = −3t + 3
−1 ≤t < 0
−3t + 3 + 3r(t) = 3
0 ≤t < 3
3 −3 = 0
t ≥3
The three functions ramp, ustep, and evenodd for this example are shown below. The following
script shows how they can be used to generate the ramp signals, with the appropriate slopes and
time shifts, as well as the unit-step signals with the desired delay, and then how to compute the
even and the odd decomposition of y[n].
%%%%%%%%%%%%%%%%%%%%
% Example 8.18
%%%%%%%%%%%%%%%%%%%%
Ts = 0.15; % sampling period
t = −5:Ts:5; % time support
y1 = ramp(t, 3, 3); y2 = ramp(t, −6, 1); y3 = ramp(t, 3, 0); % ramp signals
y4 = −3∗ustep(t, −3); % unit-step signal
y = y1 + y2 + y3 + y4;
[ze, zo] = evenodd(t, y);
We choose as support −5 ≤t ≤5 for the continuous-time signal y(t), which translates into a sup-
port −5 ≤0.15n ≤5 or −5/0.15 ≤n ≤5/0.15 for the discrete-time signal. Since the limits are not
integers, to make them integers (as required because n is an integer) we use the MATLAB function
ceil to ﬁnd integers larger than −5/0.15 and 5/0.15 giving a range [−33, 33]. This is used when
plotting y[n].
The following function generates a ramp signal for a range of time values, for different slopes and
time shifts.
function y = ramp(t, m, ad)
% ramp generation
% t: time support
% m: slope of ramp
% ad : advance (positive), delay (negative) factor

8.2 Discrete-Time Signals
477
N = length(t);
y = zeros(1, N);
for i = 1:N,
if t(i)> = −ad,
y(i) = m∗(t(i) + ad);
end
end
Likewise, the following function generates unit-step signals with different time shifts (notice the
similarities with the ramp function).
function y = ustep(t, ad)
% generation of unit step
% t: time support
% ad : advance (positive), delay (negative)
N = length(t);
y = zeros(1, N);
for i = 1:N,
if t(i) >= −ad,
y(i) = 1;
end
end
Finally, the following function can be used to compute the even and the odd decomposition of a
discrete-time signal. The MATLAB function ﬂliplr reﬂects the signal as needed in the generation of
the even and the odd components.
function [ye, yo] = evenodd(y)
% even/odd decomposition
% NOTE: the support of the signal should be
%
symmetric about the origin
% y: analog signal
% ye, yo: even and odd components
yr = ﬂiplr(y);
ye = 0.5∗(y + yr);
yo = 0.5∗(y −yr);
The results are shown in Figure 8.7. The discrete-time signal is given as
y[n] =



0
n ≤−21
0.45n + 9
−20 ≤n ≤−6
−0.45n + 3
−7 ≤n ≤0
3
1 ≤n ≤19
0
n ≥20
I

478
CHAPTER 8:
Discrete-Time Signals and Systems
−30
−20
−10
0
)
c
(
)
a
(
(b)
10
20
30
−1
0
1
2
3
4
5
6
7
y[n]
n
−30
−20
−10
0
10
20
30
−1
0
1
2
3
4
5
ze[n]
−30
−20
−10
0
10
20
30
−2
−1
0
1
2
n
n
zo[n]
FIGURE 8.7
(a) Discrete-time signal, and (b) even and (c) odd components.
8.3 DISCRETE-TIME SYSTEMS
Just as with continuous-time systems, a discrete-time system is a transformation of a discrete-time
input signal x[n] into a discrete-time output signal y[n]—that is,
y[n] = S{x[n]}
(8.26)
Just as we were when we studied the continuous-time systems, we are interested in dynamic systems
S{.} having the following properties:
I
Linearity
I
Time invariance
I
Stability
I
Causality
A discrete-time system S is said to be
I
Linear: If for inputs x[n] and v[n] and constants a and b, it satisﬁes the following
I
Scaling: S{ax[n]} = aS{x[n]}
I
Additivity: S{x[n] + v[n]} = S{x[n]} + S{v[n]}
or equivalently if superposition applies—that is,
S{ax[n] + bv[n]} = aS{x[n]} + bS{v[n]}
(8.27)
I
Time-invariant: If for an input x[n] with a corresponding output y[n] = S{x[n]}, the output corresponding
to a delayed or advanced version of x[n], x[n ± M], is y[n ± M] = S{x[n ± M]} for an integer M.

8.3 Discrete-Time Systems
479
I Example 8.19
A square-root computation system. The input–output relation characterizing a discrete-time sys-
tem is nonlinear if there are nonlinear terms that include the input x[n], the output y[n], or both
(e.g., a square root of x[n], products of x[n] and y[n], etc.). Consider the development of an iterative
algorithm to compute the square root of a positive real number α. If the result of the algorithm
is y[n] as n →∞, then y2[n] = α and likewise y2[n −1] = α, thus y[n] = 0.5(y[n −1] + y[n −1]).
Replacing y[n −1] = α/y[n −1] in this equation, the following difference equation, with some
initial condition y[0], can be used to ﬁnd the square root of α:
y[n] = 0.5

y[n −1] +
α
y[n −1]

n > 0
Find recursively the solution of this difference equation. Use the results of ﬁnding the square roots
of 4 and 2 to show the system is nonlinear. Solve the difference equation and plot the results for
α = 4, 2 with MATLAB.
Solution
The given difference equation is ﬁrst order, nonlinear (expanding it you get the product of y[n]
with y[n −1] and y2[n −1], which are nonlinear terms) with constant coefﬁcients. This equation
can be solved recursively for n > 0 by replacing y[0] to get y[1], and use this to get y[2] and so
on—that is,
y[1] = 0.5

y[0] +
α
y[0]

y[2] = 0.5

y[1] +
α
y[1]

y[3] = 0.5

y[2] +
α
y[2]

...
For instance, let y[0] = 1 and α = 4 (i.e., we wish to ﬁnd the square root of 4),
y[0] = 1
y[1] = 0.5

1 + 4
1

= 2.5
y[2] = 0.5

2.5 + 4
2.5

= 2.05
...

480
CHAPTER 8:
Discrete-Time Signals and Systems
FIGURE 8.8
Nonlinear system:
(a) square root of 2,
(b) square root of 4
compared with twice the
square root of 2, and
(c) sum of previous
responses with response
when computing square
root of 2 + 4. Figure (b)
shows scaling does not
hold and (c) shows that
additivity does not hold
either. The system is
nonlinear.
0
1
2
3
4
(a)
(b)
(c)
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
0
2
4
0
1
2
3
4
5
6
7
8
9
0
2
4
n
n
n
square root of 2
square root of 4
square root of 6
y1[n]
y2[n]
y3[n]
which is converging to 2, the square root of 4 (see Figure 8.8). Thus, as indicated before, when
n →∞, then y[n] = y[n −1] = Y, for some value Y, which according to the difference equation
satisﬁes the relation Y = 0.5Y + 0.5(4/Y) or Y =
√
4 = 2.
Suppose then that the input is α = 2, half of what it was before. If the system is linear, we should
get half the previous output according to the scaling property. That is not the case, however. For
the same initial condition y[0] = 1, we obtain recursively for α[n] = 2u[n −1]:
y[0] = 1
y[1] = 0.5[1 + 2] = 1.5
y[2] = 0.5

1.5 + 2
1.5

= 1.4167
...
This solution is clearly not half of the previous one. Moreover, as n →∞, we expect y[n] =
y[n −1] = Y, for Y that satisﬁes the relation Y = 0.5Y + 0.5(2/Y) or Y =
√
2 = 1.4142, so that
the solution is tending to
√
2 and not to 2 as it would if the system were linear. Finally, if we add

8.3 Discrete-Time Systems
481
the signals in the above two cases and compare the resulting signal with the one we obtain when
ﬁnding the square root of the previous two values of α, 2 and 4, they do not coincide. The additive
condition is not satisﬁed either, so the system is not linear.
I
8.3.1 Recursive and Nonrecursive Discrete-Time Systems
Depending on the relation between the input x[n] and the output y[n] two types of discrete-time systems of
interest are:
I
Recursive system:
y[n] = −
N−1
X
k=1
aky[n −k] +
M−1
X
m=0
bmx[n −m]
n ≥0
initial conditions y[−k], k = 1, . . . , N −1
(8.28)
This system is also called inﬁnite-impulse response (IIR).
I
Nonrecursive system:
y[n] =
M−1
X
m=0
bmx[n −m]
(8.29)
This system is also called ﬁnite-impulse response (FIR).
The recursive system is analogous to a continuous-time system represented by a differential equation.
For this type of system the discrete-time input x[n] and the discrete-time output y[n] are related by an
(N −1)th-order difference equation. If such a difference equation is linear, with constant coefﬁcients,
zero initial conditions, and the input is zero for n < 0, then it represents a linear and time-invariant
system. For these systems the output at a present time n, y[n], depends or recurs on previous values
of the output {y[n −k], k = 1, . . . , N −1}, and thus they are called recursive. We will see that these
systems are also called inﬁnite-impulse response or IIR because their impulse responses are typically of
inﬁnite length.
On the other hand, if the output y[n] does not depend on previous values of the output, but only
on weighted and shifted inputs {bmx[n −m], m = 0, . . . , M −1}, the system is called nonrecursive. We
will see that the impulse response of nonrecursive systems is of ﬁnite length; as such, these systems
are also called ﬁnite impulse response or FIR.
I Example 8.20
Moving-average discrete ﬁlter: A third-order moving-average ﬁlter (also called a smoother since
it smooths out the input signal) is an FIR ﬁlter for which the input x[n] and the output y[n] are
related by
y[n] = 1
3(x[n] + x[n −1] + x[n −2])
Show that this system is linear time invariant.

482
CHAPTER 8:
Discrete-Time Signals and Systems
Solution
This is a nonrecursive system that uses a present sample, x[n], and two past values, x[n −1] and
x[n −2], of the input to get an average, y[n], at every n. Thus, its name, moving-average ﬁlter.
Linearity: If we let the input be ax1[n] + bx2[n], and assume that {yi[n], i = 1, 2} are the correspond-
ing outputs to {xi[n], i = 1, 2}, the ﬁlter output is
1
3[(ax1[n] + bx2[n])+(ax1[n −1] + bx2[n −1])+(ax1[n −2] + bx2[n −2])] = ay1[n] + by2[n]
That is, the system is linear.
Time invariance: If the input is x1[n] = x[n −N], the corresponding output to it is
1
3(x1[n] + x1[n −1] + x1[n −2]) = 1
3(x[n −N] + x[n −N −1] + x[n −N −2])
= y[n −N]
That is, the system is time invariant.
I
I Example 8.21
Autoregressive discrete ﬁlter: The recursive discrete-time system represented by the ﬁrst-order
difference equation (with initial condition y[−1])
y[n] = ay[n −1] + bx[n]
n ≥0, y[−1]
is also called an autoregressive (AR) ﬁlter. “Autoregressive” refers to the feedback in the output—that
is, the present value of the output y[n] depends on its previous value y[n −1].
Find recursively the solution of the difference equation and determine under what conditions the
system represented by this difference equation is linear and time invariant.
Solution
Let’s ﬁrst discuss why the initial condition is y[−1]. The initial condition is the value needed to
compute y[0], which according to the difference equation
y[0] = ay[−1] + bx[0]
is y[−1] since x[0] is known.
Assume that the initial condition is y[−1] = 0, and that the input is x[n] = 0 for n < 0 (i.e.,
the system is not energized for n < 0). The solution of the difference equation when the input
x[n] is not deﬁned can be found by a repetitive substitution of the input–output relationship.
Thus, replacing y[n −1] = ay[n −2] + bx[n −1] in the difference equation, and then replacing

8.3 Discrete-Time Systems
483
y[n −2] = ay[n −3] + bx[n −2], and so on, we obtain
y[n] = a(ay[n −2] + bx[n −1]) + bx[n]
= a(a(ay[n −3] + bx[n −2])) + abx[n −1] + bx[n]
= · · ·
= · · · a3bx[n −3] + a2bx[n −2] + abx[n −1] + bx[n]
until we reach x[0]. The solution can be written as
y[n] =
n
X
k=0
bakx[n −k]
(8.30)
which we will see in the next section is the convolution sum of the impulse response of the system
and the input.
To see that Equation (8.30) is actually the solution of the given difference equation, we need to
show that when replacing the above expression for y[n] in the right term of the difference equation
we obtain the left term y[n]. Indeed, we have that
ay[n −1] + bx[n] = a
"n−1
X
k=0
bakx[n −1 −k]
#
+ bx[n]
=
n
X
m=1
bamx[n −m] + bx[n] =
n
X
m=0
bamx[n −m] = y[n]
where the dummy variable in the sum was changed to m = k + 1, so that the limits of the sum-
mation became m = 1 when k = 0, and m = n when k = n −1. The ﬁnal equation is identical
to y[n].
To establish if the system represented by the difference equation is linear, we use the solution
Eq. (8.30) with input x[n] = αx1[n] + βx2[n], where the outputs {yi[n], i = 1, 2} correspond to
inputs {xi[n], i = 1, 2}, and α and β are constants. The output for x[n] is
n
X
k=0
bakx[n −k] =
n
X
k=0
bak  αx1[n −k] + βx2[n −k]

= α
n
X
k=0
bakx1[n −k] + β
n
X
k=0
bakx2[n −k] = αy1[n] + βy2[n]
So the system is linear.

484
CHAPTER 8:
Discrete-Time Signals and Systems
The time invariance is shown by letting the input be v[n] = x[n −N], n ≥N, and zero otherwise.
The corresponding output according to Equation (8.30) is
n
X
k=0
bakv[n −k] =
n
X
k=0
bakx[n −N −k]
=
n−N
X
k=0
bakx[n −N −k] +
n
X
k=n−N+1
bakx[n −N −k] = y[n −N]
since the summation
n
X
k=n−N+1
bakx[n −N −k] = 0
given that x[−N] = · · · = x[−1] = 0 is assumed. Thus, the system represented by the above differ-
ence equation is linear and time invariant. As in the continuous-time case, however, if the initial
condition y[−1] is not zero, or if x[n] ̸= 0 for n < 0, the system characterized by the difference
equation is not LTI.
I
I Example 8.22
Autoregressive moving average ﬁlter: The recursive system represented by the ﬁrst-order difference
equation
y[n] = 0.5y[n −1] + x[n] + x[n −1]
n ≥0, y[−1]
is called the autoregressive moving average given that it is the combination of the two systems
discussed before. Consider two cases:
I
Let the initial condition be y[−1] = −2, and the input be x[n] = u[n] ﬁrst and then x[n] =
2u[n].
I
Let the initial condition be y[−1] = 0, and the input be x[n] = u[n] ﬁrst and then x[n] = 2u[n].
Determine in each of these cases if the system is linear.
Find the steady-state response—that is,
lim
n→∞y[n]

8.3 Discrete-Time Systems
485
Solution
For an initial condition y[−1] = −2 and x[n] = u[n], we get recursively
y[0] = 0.5y[−1] + x[0] + x[−1] = 0
y[1] = 0.5y[0] + x[1] + x[0] = 2
y[2] = 0.5y[1] + x[2] + x[1] = 3
. . .
Let us then double the input (i.e., x[n] = 2u[n]) and call the response y1[n]. As the initial condition
remains the same (i.e., y1[−1] = −2), we get
y1[0] = 0.5y1[−1] + x[0] + x[−1] = 1
y1[1] = 0.5y1[0] + x[1] + x[0] = 4.5
y1[2] = 0.5y1[1] + x[2] + x[1] = 6.25
. . .
It is clear that the y1[n] is not 2y[n]. Due to the initial condition not being zero, the system is
nonlinear.
If the initial condition is set to zero, and the input x[n] = u[n], the response is
y[0] = 0.5y[−1] + x[0] + x[−1] = 1
y[1] = 0.5y[0] + x[1] + x[0] = 2.5
y[2] = 0.5y[1] + x[2] + x[1] = 3.25
. . .
If we double the input (i.e., x[n] = 2u[n]) and call the response y1[n], y1[−1] = 0, we obtain
y1[0] = 0.5y1[−1] + x[0] + x[−1] = 2
y1[1] = 0.5y1[0] + x[1] + x[0] = 5
y1[2] = 0.5y1[1] + x[2] + x[1] = 6.5
. . .

486
CHAPTER 8:
Discrete-Time Signals and Systems
For the zero initial condition, it is clear that y1[n] = 2y[n] when we double the input. One can
also show that superposition holds for this system. For instance, if we let the input be the sum of
the previous inputs, x[n] = u[n] + 2u[n] = 3u[n], and let y12[n] be the response when the initial
condition is zero, y12[0] = 0, we get
y12[0] = 0.5y12[−1] + x[0] + x[−1] = 3
y12[1] = 0.5y12[0] + x[1] + x[0] = 7.5
y12[2] = 0.5y12[1] + x[2] + x[1] = 9.75
. . .
showing that y12[n] is the sum of the responses when the inputs are u[n] and 2u[n]. Thus, the
system represented by the given difference equation with a zero initial condition is linear.
Although when the initial condition is −2 or 0, and x[n] = u[n] we cannot ﬁnd a closed form
for the response, we can see that the response is going toward a ﬁnal value or a steady-state
response. Assuming that as n →∞we have that Y = y[n] = y[n −1], and since x[n] = x[n −1] =
1, according to the difference equation the steady-state value Y is found from
Y = 0.5Y + 2
or
Y = 4
independent of the initial condition. Likewise, when x[n] = 2u[n], the steady-state solution Y is
obtained from Y = 0.5Y + 4 or Y = 8, independent of the initial condition.
I
Remarks
I
Like in the continuous-time system, to show that a discrete-time system is linear and time invariant an
explicit expression relating the input and the output is needed.
I
Although the solution of linear difference equations can be obtained in the time domain, just like with
differential equations, we will see in the next chapter that their solution can also be obtained using the
Z-transform, just like the Laplace transform being used to solve linear differential equations.
8.3.2 Discrete-Time Systems Represented by Difference Equations
As we saw before, a recursive discrete-time system is represented by a difference equation
y[n] = −
N−1
X
k=1
aky[n −k] +
M−1
X
m=0
bmx[n −m]
n ≥0
initial conditions y[−k], k = 1, . . . , N −1
(8.31)
If the system is discrete-time, the difference equation naturally characterizes the dynamics of the
system. On the other hand, the difference equation could be the approximation of a differential
equation representing a continuous-time system that is being processed discretely. For instance, to
approximate a second-order differential equation by a difference equation, we could approximate

8.3 Discrete-Time Systems
487
the ﬁrst derivative of a continuous-time function vc(t) as
dvc(t)
dt
≈vc(t) −vc(t −Ts)
Ts
and its second derivative as
d2vc(t)
dt2
= d dvc(t)
dt
dt
≈d(vc(t) −vc(t −Ts))/Ts)
dt
≈vc(t) −2vc(t −Ts) + vc(t −2Ts)
T2s
to obtain a second-order difference equation when t = nTs. Choosing a small value for Ts provides an
accurate approximation to the differential equation. Other transformations can be used. In Chapter 0
we indicated that approximating integrals by the trapezoidal rule gives the bilinear transformation,
which can also be used to change differential into difference equations.
Just as in the continuous-time case, the system being represented by the difference equation is not LTI
unless the initial conditions are zero and the input is causal. The Z-transform will, however, allow us
to ﬁnd the complete response of the system even when the initial conditions are not zero. When the
initial conditions are not zero, just like in the continuous case, these systems are incrementally linear.
The complete response of a system represented by the difference equation can be shown to be
composed of a zero-input and a zero-state responses—that is, if y[n] is the solution of the difference
Equation (8.31) with initial conditions not necessarily equal to zero, then
y[n] = yzi[n] + yzs[n]
(8.32)
The component yzi[n] is the response when the input x[n] is set to zero, thus it is completely due to the
initial conditions. The response yzs[n] is due to the input, as we set the initial conditions equal to zero.
The complete response y[n] is thus seen as the superposition of these two responses. The Z-transform
provides an algebraic way to obtain the complete response, whether the initial conditions are zero or
not.
It is important, as in the continuous-time system, to differentiate the zero-input and the zero-state
responses from the transient and the steady-state responses.
8.3.3 The Convolution Sum
Let h[n] be the impulse response of an LTI discrete-time system, or the output of the system corresponding
to an impulse δ[n] as input and initial conditions (if needed) equal to zero. Using the generic representation of
the input x[n] of the LTI system,
x[n] =
∞
X
k=−∞
x[k]δ[n −k]
(8.33)

488
CHAPTER 8:
Discrete-Time Signals and Systems
the output of the system is given by either of the following two equivalent forms of the convolution sum:
y[n] =
∞
X
k=−∞
x[k]h[n −k]
=
∞
X
m=−∞
x[n −m]h[m]
(8.34)
The impulse response h[n] of a discrete-time system is due exclusively to an input δ[n]; as such,
the initial conditions are set to zero. In some cases there are no initial conditions, as in the case of
nonrecursive systems.
Now, if h[n] is the response due to δ[n], by time invariance the response to δ[n −k] is h[n −k]. By
superposition, the response due to x[n] with the generic representation
x[n] =
X
k
x[k]δ[n −k]
is the sum of responses due to x[k]δ[n −k], which is x[k]h[n −k] (x[k] is not a function of n), or
y[n] =
X
k
x[k]h[n −k]
i.e., the convolution sum of the input x[n] with the impulse response h[n] of the system. The second
expression of the convolution sum in Equation (8.34) is obtained by a change of variable m = n −k.
Remarks
I
The output of nonrecursive or FIR systems is the convolution sum of the input and the impulse response of
the system. The input–output expression of an FIR system is
y[n] =
N−1
X
k=0
bkx[n −k]
(8.35)
and its impulse response is found by letting x[n] = δ[n], which gives
h[n] =
N−1
X
k=0
bkδ[n −k] = b0δ[n] + b1δ[n −1] + · · · + bN−1δ[n −(N −1)]
so that h[n] = bn for n = 0, . . . , N −1, and zero otherwise. Replacing the bk coefﬁcients in
Equaiton (8.35) by h[k] we ﬁnd that the output can be written as
y[n] =
N−1
X
k=0
h[k]x[n −k]

8.3 Discrete-Time Systems
489
or the convolution sum of the input and the impulse response. This is a very important result, indicating
that the output of FIR systems is obtained by means of the convolution sum rather than difference
equations, which gives great signiﬁcance to the efﬁcient computation of the convolution sum.
I
Considering the convolution sum as an operator—that is,
y[n] = [h ∗x][n] =
∞
X
k=−∞
x[k]h[n −k]
it is easily shown to be linear. Indeed, whenever the input is ax1[n] + bx2[n], and {yi[n]} are the outputs
corresponding to {xi[n]} for i = 1, 2, then we have that
[h ∗(ax1 + bx2)][n] =
X
k
(ax1[k] + bx2[k])h[n −k] = a
X
k
x1[k]h[n −k] + b
X
k
x2[k]h[n −k]
= a[h ∗x1][n] + b[h ∗x2][n] = ay1[n] + by2[n]
as expected, since the system was assumed to be linear when the expression for the convolution sum was
obtained. We will then have that if the output corresponding to x[n] is y[n], given by the convolution
sum, then the output corresponding to a shifted version of the input, x[n −N], should be y[n −N]. In
fact, if we let x1[n] = x[n −N], the corresponding output is
[h ∗x1][n] =
X
k
x1[n −k]h[k] =
X
k
x[n −N −k]h[k]
= [h ∗x][n −N] = y[n −N]
Again, this result is expected given that the system was considered time invariant when the convolution
sum was obtained.
I
From the equivalent representations for the convolution sum we have that
[h ∗x][n] =
X
k
x[k]h[n −k] =
X
k
x[n −k]h[k]
= [x ∗h][n]
indicating that the convolution commutes with respect to the input x[n] and the impulse response h[n].
I
Just as with continuous-time systems, when conecting two LTI discrete-time systems (with impulse
responses h1[n] and h2[n]) in cascade or in parallel, their respective impulse responses are given by
[h1 ∗h2][n] and h1[n] + h2[n]. See Figure 8.9 for block diagrams.
I
There are situations when instead of giving the input and the impulse response to compute the output, the
information that it is available is, for instance, the input and the output and we wish to ﬁnd the impulse
response of the system, or we have the output and the impulse response and wish to ﬁnd the input. This
type of problem is called deconvolution. We consider this problem later in this chapter after considering
causality, and in Chapter 9 where we show that it can be easily solved using the Z-transform.
I
The computation of the convolution sum is typically difﬁcult. It is made easier when the Z-transform is
used, as we will see. MATLAB provides the function conv which greatly simpliﬁes the computation.

490
CHAPTER 8:
Discrete-Time Signals and Systems
FIGURE 8.9
(a) Cascade and
(b) parallel
connections of LTI
systems with impulse
responses h1[n] and
h2[n]. Equivalent
systems on the right.
Notice the
interchange of
systems in the
cascade connection.
(a)
(b)
h2[n]
h1[n]
h1[n] + h2[n]
x[n]
+
y[n]
y[n]
x[n]
x[n]
h1[n]
h2[n]
h2[n]
h1[n]
(h1∗h2) [n]
x[n]
y[n]
y[n]
y[n]
x[n]
I Example 8.23
Consider a moving-averaging ﬁlter where the input is x[n] and the output is y[n]:
y[n] = 1
3(x[n] + x[n −1] + x[n −2])
Find the impulse response h[n] of this ﬁlter. Then,
(a)
Let x[n] = u[n]. Find the output of the ﬁlter y[n] using the input–output relation and the
convolution sum.
(b)
If the input of the ﬁlter is x[n] = A cos(2πn/N)u[n], determine the values of A and N, so that
the steady-state response of the ﬁlter is zero. Explain. Use MATLAB to verify your results.
Solution
(a) If the input is x[n] = δ[n], the output of the ﬁlter is y[n] = h[n], or the impulse response of the
system. No initial conditions are needed. We thus have that
h[n] = 1
3(δ[n] + δ[n −1] + δ[n −2])
so that h[0] = 1/3 as δ[0] = 1 but δ[−1] = δ[−2] = 0; likewise, h[1] = h[2] = 1/3 so that the
coefﬁcients of the ﬁlter equal the impulse response of the ﬁlter at n = 0, 1, and 2.

8.3 Discrete-Time Systems
491
Now if x[n] is the input to the ﬁlter according to the convolution sum, its output is
y[n] =
n
X
k=0
x[n −k]h[k] = h[0]x[n] + h[1]x[n −1] + h[2]x[n −2]
= 1
3
 x[n] + x[n −1] + x[n −2]

Notice that the lower bound of the sum is set by the impulse response being zero for n < 0,
while the upper bound is set by the input being zero for n < 0 (i.e., if k > n, then n −k < 0 and
x[n −k] = 0). The convolution sum coincides with the input–output equation. This holds for any
FIR ﬁlter.
For any input x[n], let us then ﬁnd a few values of the convolution sum to see what happens as n
grows. If n < 0, the arguments of x[n], x[n −1], and x[n −2] are negative giving zero values, and
so the output is also zero (i.e., y[n] = 0, n < 0). For n ≥0, we have
y[0] = 1
3
 x[0] + x[−1] + x[−2]

= 1
3x[0]
y[1] = 1
3
 x[1] + x[0] + x[−1]

= 1
3(x[0] + x[1])
y[2] = 1
3
 x[2] + x[1] + x[0]

= 1
3(x[0] + x[1] + x[2])
y[3] = 1
3
 x[3] + x[2] + x[1]

= 1
3(x[1] + x[2] + x[3])
· · ·
Thus, if x[n] = u[n], then we have that y[0] = 1/3, y[1] = 2/3, and y[n] = 1 for n ≥2.
(b) Notice that for n ≥2, the output is the average of the present and past two values of the input.
Thus, when the input is x[n] = A cos(2πn/N), if we let N = 3 and A be any real value, the input
repeats every three samples and the local average of three of its values is zero, giving y[n] = 0 for
n ≥2; thus the steady-state response will be zero.
The following MATLAB script uses the function conv to compute the convolution sum when the
input is either x[n] = u[n] or x[n] = cos(2πn/3)u[n].
%%%%%%%%%%%%%%%%%%
% Example 8.23 -- Convolution sum
%%%%%%%%%%%%%%%%%%
x1 = [0 0 ones(1, 20)] % unit-step input
n = −2:19; n1 = 0:19;
x2 = [0 0 cos(2∗pi∗n1/3)]; % cosine input
h = (1/3)∗ones(1, 3); % impulse response
y = conv(x1, h); y1 = y(1:length(n)); % convolution sums
y = conv(x2, h); y2 = y(1:length(n));

492
CHAPTER 8:
Discrete-Time Signals and Systems
FIGURE 8.10
Convolution sums for
a moving-averaging
ﬁlter with input
x1[n] = u[n] and
x2[n] = cos(2πn/3)u[n].
0
5
10
15
0
5
10
15
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
n
0
5
10
15
n
n
−1
−0.5
0
0.5
1
0
5
10
15
−0.1
0
0.1
0.2
0.3
0.4
n
x1[n]
x2[n]
y2[n]
y1[n]
Notice that each of the input sequences has two zeros at the beginning so that the response can
be found at n ≥−2. Also, when the input is of inﬁnite support, like when x[n] = u[n], we can
only approximate it as a ﬁnite sequence in MATLAB, and as such the ﬁnal values of the convolu-
tion obtained from conv are not correct and should not be considered. In this case, the ﬁnal two
values of the convolution results are not correct and are not considered. The results are shown in
Figure 8.10.
I
I Example 8.24
Consider an autoregressive system represented by a ﬁrst-order difference equation
y[n] = 0.5y[n −1] + x[n]
n ≥0
Find the impulse response h[n] of the system and then compute the response of the system to
x[n] = u[n] −u[n −3] using the convolution sum. Verify results with MATLAB.
Solution
The impulse response h[n] can be found recursively. Letting x[n] = δ[n], y[n] = h[n], and initial
condition y[−1] = h[−1] = 0, we have
h[0] = 0.5h[−1] + δ[0] = 1
h[1] = 0.5h[0] + δ[1] = 0.5
h[2] = 0.5h[1] + δ[2] = 0.52
h[3] = 0.5h[2] + δ[3] = 0.53
· · ·

8.3 Discrete-Time Systems
493
from which a general expression for the impulse response is obtained as h[n] = 0.5nu[n].
The response to x[n] = u[n] −u[n −3] using the convolution sum is then given by
y[n] =
∞
X
k=−∞
x[k]h[n −k] =
∞
X
k=−∞
(u[k] −u[k −3])0.5n−ku[n −k]
Since as functions of k, u[k]u[n −k] = 1 for 0 ≤k ≤n, zero otherwise, and u[k −3]u[n −k] = 1
for 3 ≤k ≤n, zero otherwise (in the two cases, draw the two signals as functions of k and verify
this is true), y[n] can be expressed as
y[n] = 0.5n
" n
X
k=0
0.5−k −
n
X
k=3
0.5−k
#
u[n]
=



0
n < 0
0.5n Pn
k=0 0.5−k
n = 0, 1, 2
0.5n P2
k=0 0.5−k
n ≥3.
Another way to solve this problem is to notice that the input can be rewritten as
x[n] = δ[n] + δ[n −1] + δ[n −2]
and since the system is LTI, the output can be written as
y[n] = h[n] + h[n −1] + h[n −2] = 0.5nu[n] + 0.5n−1u[n −1] + 0.5n−2u[n −2]
which gives
y[0] = 0.50 = 1
y[1] = 0.51 + 0.50 = 3
2
y[2] = 0.52 + 0.51 + 0.50 = 7
4
y[3] = 0..53 + 0.52 + 0.5 = 7
8
· · ·
which coincides with the above more general solution. It should be noticed that even in a simple
example like this the computation required by the convolution sum is quite high. We will see
that the Z-transform simpliﬁes these types of problems, just like the Laplace transform does in the
computation of the convolution integral.
The following MATLAB script is used to verify the above results. The MATLAB function ﬁlter is
used to compute the impulse response and the response of the ﬁlter to the pulse. The output
obtained then with ﬁlter coincided with the output computed using conv, as it should. Figure 8.11
displays the results.

494
CHAPTER 8:
Discrete-Time Signals and Systems
FIGURE 8.11
First-order
autoregressive ﬁlter
impulse response h[n]
and response y[n] due
to x[n] = u[n] −
u[n −3].
0
5
10
15
20
0
0.5
1
h [n]
n
n
0
2
4
6
8
10
12
0
0.5
1
1.5
2
y[n]
%%%%%%%%%%%%%%%%%%%%%
% Example 8.24
%%%%%%%%%%%%%%%%%%%%%
a = [1 −0.5]; b = 1; % coefﬁcients of the difference equation
d = [1 zeros(1, 99)]; % approximate delta function
h = ﬁlter(b, a, d); % impulse response
x = [ones(1, 3) zeros(1, 10)]; % input
y = ﬁlter(b, a, x); % output from ﬁlter function
y1 = conv(h, x); y1 = y1(1:length(y)) % output from conv
I
8.3.4 Linear and Nonlinear Filtering with MATLAB
One is not always able to get rid of undesirable components of a signal by means of linear ﬁltering.
In this section we will illustrate the possible advantages of using nonlinear ﬁlters.
Linear Filtering
To illustrate the way a linear ﬁlter works, consider getting rid of a random disturbance η[n], which we
model as Gaussian noise (this is one of the possible noise signals MATLAB provides) that has been
added to a sinusoid x[n] = cos(πn/16). Let y[n] = x[n] + η[n]. We will use an averaging ﬁlter having
an input–output equation
z[n] = 1
M
M−1
X
k=0
y[n −k]

8.3 Discrete-Time Systems
495
This M-order ﬁlter averages M past input values {y[n −k], k = 0, . . . , M −1} and assigns this average
to the output z[n]. The effect is to smooth out the input signal by attenuating the high-frequency
components of the signal due to the noise. The larger the value of M the better the results, but at the
expense of more complexity and a larger delay in the output signal (this is due to the linear-phase
frequency response of the ﬁlter, as we will see later).
We use a third-order and a ﬁfteenth-order ﬁlter, implemented by our function averager given below.
The denoising is done by means of the following script.
%%%%%%%%%%%%%%%%%
% Linear ﬁltering
%%%%%%%%%%%%%%%%%
N = 200; n = 0:N −1;
x = cos(pi∗n/16); % input signal
noise = 0.2∗randn(1, N); % noise
y = x + noise; % noisy signal
z = averager(3, y); % averaging linear ﬁlter with M = 3
z1 = averager(15, y); % averaging linear ﬁlter with M = 15
Our function averager deﬁnes the coefﬁcients of the averaging ﬁlter and then uses the MATLAB func-
tion ﬁlter to compute the ﬁlter response. The inputs of ﬁlter are the vector b = (1/M)[1 · · · 1], the
coefﬁcients connected with the input, the unit coefﬁcient connected with the output, and x is a vector
with the entries the signal samples we wish to ﬁlter. The results of ﬁltering using these two ﬁlters are
shown in Figure 8.12. As expected, the performance of the ﬁlter with M = 15 is a lot better, but a
delay of 8 samples (or the integer larger than M/2) is shown in the ﬁlter output.
function y = averager(M,x)
% Moving average of signal x
%
M: order of averager
%
x: input signal
%
b = (1/M)∗ones(1, M);
y = ﬁlter(b, 1, x);
Nonlinear Filtering
Is linear ﬁltering always capable of getting rid of noise? The answer is: It depends on the type of noise.
In the previous example we showed that a high-order averaging ﬁlter, which is linear, performs well
for Gaussian noise. Let us now consider an impulsive noise that is either zero or a certain value at
random. This is the type of noise occurring in communications whenever cracking sounds are heard
in the transmission, or the “salt-and-pepper” noise that appears in images.
It will be shown that even the 15th-order averager—that did well before—is not capable of denoising
the signal with impulsive noise. A median ﬁlter considers a certain number of samples (the example
shows the case of a 5th-order median ﬁlter), orders them according to their amplitudes, and chooses
the one in the middle value (i.e., the median) as the output of the ﬁlter. Such a ﬁlter is nonlinear as it
does not satisfy superposition. The following script is used to ﬁlter the noisy signal using a linear and

496
CHAPTER 8:
Discrete-Time Signals and Systems
0
20
40
60
80
100
(a)
(b)
120
140
160
180
0
20
40
60
80
100
120
140
160
180
−1
−0.5
0
0.5
1
y[n], z[n]
y[n], z1[n]
−1
−0.5
0
0.5
1
n
FIGURE 8.12
Averaging ﬁltering with ﬁlters of order (a) M = 3 and of order (b) M = 15 result used to get rid of Gaussian noise
added to a sinusoid x[n] = cos(πn/16). Solid line corresponds to the noisy signal, while the dashed line is for the
ﬁltered signal. The ﬁltered signal is very much like the noisy signal (a) when M = 3 is the order of the ﬁlter, while
the ﬁltered signal looks like the sinusoid, but shifted, (b) when M = 15. The plotting in this ﬁgure is done using
plot instead of stem to allow a better visualization of the ﬁltering results.
a nonlinear ﬁlter, and a comparison of the results is shown in Figure 8.13. In this case the nonlinear
ﬁlter is able to denoise the signal much better than the linear ﬁlter.
%%%%%%%%%%%%%%%%%%%
% Nonlinear ﬁltering
%%%%%%%%%%%%%%%%%%%
clear all; clf
N = 200;n = 0:N −1;
% impulsive noise
for m = 1:N,
d = rand(1, 1);
if d >= 0.95,
noise(m) = −1.5;
else
noise(m) = 0;
end
end

8.3 Discrete-Time Systems
497
0
20
40
60
80
100
(a)
120
140 160
180
200
−2
−1
0
1
2
n
y1[n]
(b)
(c)
0
20
40
60
80
100 120
140 160
180
200
n
0
20
40
60
80
100 120
140 160
180
200
n
−2
−1
0
1
2
z1[n]
z2[n]
−2
−1
0
1
2
FIGURE 8.13
Top ﬁgure (a): noisy signal (dashed blue line) and clean signal (solid line). The clean signal (dashed line) is
superposed on the denoised signal (solid blue line) in the bottom plots. The solid line in plot (b) is the result of
median ﬁltering, and the solid line in plot (c) is the result of the averager.
x = [2∗cos(pi∗n(1:100)/256) zeros(1, 100)];
y1 = x + noise;
% linear ﬁltering
z2 = averager(15, y1);
% nonlinear ﬁltering -- median ﬁltering
z1(1) = median([0 0 y1(1) y1(2) y1(3)]);
z1(2) = median([0 y1(1) y1(2) y1(3) y1(4)]);
z1(N −1) = median([y1(N −3) y1(N −2) y1(N −1) y1(N) 0]);
z1(N) = median([y1(N −2) y1(N −1) y1(N) 0 0]);
for k = 3:N −2,
z1(k) = median([y1(k −2) y1(k −1) y1(k) y1(k + 1) y1(k + 2)]);
end
Although the theory of nonlinear ﬁltering is beyond the scope of this book, it is good to remember
that in cases like this when linear ﬁlters do not seem to do well, there are other methods to use.
8.3.5 Causality and Stability of Discrete-Time Systems
As with continuous-time systems, two additional independent properties of discrete-time systems are
causality and stability. Causality relates to the conditions under which computation can be performed
in real time, while stability relates to the usefulness of the system.
Causality
In many situations signals need to be processed in real time—that is, the processing must be done
as the signal comes into the system. In those situations, the system must be causal. In many other

498
CHAPTER 8:
Discrete-Time Signals and Systems
situations, real-time processing is not required as the data can be stored and processed without the
requirements of real time. Under such circumstances causality is not necessary.
A discrete-time system S is causal if:
I
Whenever the input x[n] = 0, and there are no initial conditions, the output is y[n] = 0.
I
The output y[n] does not depend on future inputs.
Causality is independent of the linearity and time-invariance properties of a system. For instance, the
system represented by the input–output equation
y[n] = x2[n]
where x[n] is the input and y[n] is the output is nonlinear but time invariant. According to the above
deﬁnition it is a causal system: The output is zero whenever the input is zero, and the output depends
on the present value of the input. Likewise, an LTI system can be noncausal, as can be seen in the
following discrete-time system that computes the moving average of the input:
y[n] = 1
3(x[n + 1] + x[n] + x[n −1]).
The input–output equation indicates that at the present time n to compute y[n] we need a present
value x[n], a past value x[n −1], and a future value x[n + 1]. Thus, the system is LTI but noncausal
since it requires future values of the input.
I
An LTI discrete-time system is causal if the impulse response of the system is such that
h[n] = 0
n < 0
(8.36)
I
A signal x[n] is said to be causal if
x[n] = 0
n < 0
(8.37)
I
For a causal LTI discrete-time system with a causal input x[n] its output y[n] is given by
y[n] =
n
X
k=0
x[k]h[n −k]
n ≥0
(8.38)
where the lower limit of the sum depends on the input causality, x[k] = 0 for k < 0, and the upper limit
depends on the causality of the system, h[n −k] = 0 for n −k < 0 or k > n .
That h[n] = 0 for n < 0 is the condition for an LTI discrete-time system to be causal is understood by
considering that when computing the impulse response, the input δ[n] only occurs at n = 0 and there
are no initial conditions, so the response for n < 0 should be zero. Extending the notion of causality

8.3 Discrete-Time Systems
499
to signals we can then see that the output of a causal LTI discrete-time system can be written in terms
of the convolution sum as
y[n] =
∞
X
k=−∞
x[k]h[n −k] =
∞
X
k=0
x[k]h[n −k] =
n
X
k=0
x[k]h[n −k]
where we ﬁrst used the causality of the input (x[k] = 0 for k < 0) and then that of the system (i.e.,
h[n −k] = 0 whenever n −k < 0 or k > n). According to this equation the output depends on inputs
{x[0], . . . , x[n]}, which are past and present values of the input.
I Example 8.25
So far we have considered the convolution sum as a way of computing the output y[n] of an LTI
system with impulse response h[n] for a given input x[n]. But it actually can be used to ﬁnd either
of these three variables given the other two. The problem is then called deconvolution. Assume
the input x[n] and the output y[n] of a causal LTI system are given. Find equations to compute
recursively the impulse response h[n] of the system. Consider ﬁnding the impulse response h[n]
of a causal LTI system with input x[n] = u[n] and output y[n] = δ[n]. Use the MATLAB function
deconv to ﬁnd h[n].
Solution
If the system is causal and LTI, the input x[n] and the output y[n] are connected by the convolu-
tion sum
y[n] =
n
X
m=0
h[n −m]x[m] = h[n]x[0] +
n
X
m=1
h[n −m]x[m]
To ﬁnd h[n] from given x[n] and y[n], under the condition that x[0] ̸= 0, the above equation can
be rewritten as
h[n] =
1
x[0]
"
y[n] −
n
X
m=1
h[n −m]x[m]
#
so that the impulse response of the causal LTI can be found recursively as follows:
h[0] =
1
x[0]y[0]
h[1] =
1
x[0]
 y[1] −h[0]x[1]

h[2] =
1
x[0]
 y[2] −h[0]x[2] −h[1]x[1]

...

500
CHAPTER 8:
Discrete-Time Signals and Systems
For the given case where y[n] = δ[n] and x[n] = u[n], we get, according to the above,
h[0] =
1
x[0]y[0] = 1
h[1] =
1
x[0]
 y[1] −h[0]x[1]

= 0 −1 = −1
h[2] =
1
x[0]
 y[2] −h[0]x[2] −h[1]x[1]

= 0 −1 + 1 = 0
h[3] =
1
x[0]
 y[3] −h[0]x[3] −h[1]x[2] −h[2]x[3]

= 0 −1 + 1 −0 = 0
...
and, in general, h[n] = δ[n] −δ[n −1].
The length of the convolution y[n] is the sum of the lengths of the input x[n] and of the impulse
response h[n] minus one. Thus,
length of h[n] = length of y[n] −length of x[n] + 1
When using deconv we need to make sure that the length of y[n] is always larger than that of x[n]. If
x[n] is of inﬁnite length, like when x[n] = u[n], this would require an even longer y[n], which is not
possible. However, MATLAB can only provide a ﬁnite-support input, so we make the support of
y[n] larger. In this example we have found analytically that the impulse response h[n] is of length
2, so if the length of y[n] is chosen so that length y[n] is larger than the length of x[n] by one, we
get the correct answer (case (a) in the script below); otherwise we do not (case (b)). Run the two
cases to verify this (get rid of % symbol to run case (b)).
%%%%%%%%%%%%%%%%%%%%%%
% Example 8.25 --- Deconvolution
%%%%%%%%%%%%%%%%%%%%%%
clear all
x = ones(1, 100);
y = [1 zeros(1, 100)]; % (a) correct h
% y = [1 zeros(1, 99)]; % (b) incorrect h
[h, r] = deconv(y, x)
I
Bounded-Input Bounded-Output Stability
Stability characterizes useful systems. A stable system provides well-behaved outputs for well-behaved
inputs. Bounded-input bounded-output (BIBO) stability establishes that for a bounded (that is what
is meant by well-behaved) input x[n] the output of a BIBO-stable system y[n] is also bounded. This
means that if there is a ﬁnite bound M < ∞such that |x[n]| < M for all n (you can think of it as
an envelope [−M, M] inside which the input is in for all time), the output is also bounded (i.e.,
|y[n]| < L for L < ∞and all n).

8.3 Discrete-Time Systems
501
An LTI discrete-time system is said to be BIBO stable if its impulse response h[n] is absolutely summable,
X
k
|h[k]| < ∞
(8.39)
Assuming that the input x[n] of the system is bounded, or that there is a value M < ∞such
that |x[n]| < M for all n, the output y[n] of the system represented by a convolution sum is also
bounded, or
|y[n]| ≤

∞
X
k=−∞
x[n −k]h[k]

≤
∞
X
k=−∞
|x[n −k]||h[k]|
≤M
∞
X
k=−∞
|h[k]| ≤MN < ∞
provided that P∞
k=−∞|h[k]| < N < ∞, or that the impulse response be absolutely summable.
Remarks
I
Nonrecursive or FIR systems are BIBO stable. Indeed, the impulse response of such a system is of ﬁnite
length and thus absolutely summable.
I
For a recursive or IIR system represented by a difference equation, to establish stability we need to ﬁnd the
system impulse response h[n] and determine whether it is absolutely summable or not.
I
A much simpler way to test the stability of an IIR system will be based on the location of the poles of the
Z-transform of h[n], as we will see in Chapter 9.
I Example 8.26
Consider an autoregressive system
y[n] = 0.5y[n −1] + x[n]
Determine if the system is BIBO stable.
Solution
As shown in Example 8.24, the impulse response of the system is h[n] = 0.5nu[n]. Checking the
BIBO stability condition, we have
∞
X
n=−∞
|h[n]| =
∞
X
n=0
0.5n =
1
1 −0.5 = 2
Thus, the system is BIBO stable.
I

502
CHAPTER 8:
Discrete-Time Signals and Systems
8.4 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO
FROM HERE?
As you saw in this chapter the theory of discrete-time signals and systems is very similar to the the-
ory of continuous-time signals and systems. Many of the results in the continuous-time theory are
changed by swapping integrals for sums and differential equations for difference equations. However,
there are signiﬁcant differences imposed by the way the discrete-time signals and systems are gener-
ated. For instance, the discrete frequency can be considered ﬁnite but circular, and it depends on the
sampling time. Discrete sinusoids, as another example, are not necessarily periodic. Thus, despite the
similarities there are also signiﬁcant differences between the continuous-time and the discrete-time
signals and systems.
Now that we have a basic structure for discrete-time signals and systems, we will continue developing
the theory of linear time-invariant discrete-time systems using transforms. Again, you will ﬁnd a great
deal of similarity but also some very signiﬁcant differences. In the next chapters, carefully notice the
relation that exists between the Z-transform and the Fourier representations of discrete-time signals
and systems, not only with each other but with the Laplace and Fourier transforms. There is a great
deal of connection among all of these transforms, and a clear understanding of this would help you
with the analysis and synthesis of discrete-time signals and systems.
PROBLEMS
8.1. Discrete sequence—MATLAB
Consider the following formula
x[n] = x[n −1] + x[n −3]
n ≥3
x[0] = 0
x[1] = 1
x[2] = 2
Find the rest of the sequence for 0 ≤n ≤50 and plot it using the MATLAB function stem.
8.2. Finite-energy signals—MATLAB
Given the discrete signal x[n] = 0.5nu[n]:
(a) Use MATLAB to plot the signal x[n] for n = −5 to 200.
(b) Is this a ﬁnite-energy discrete-time signal? That is compute the inﬁnite sum
∞
X
n=−∞
|x[n]|2
Hint: Show that
∞
X
n=0
αn =
1
1 −α

Problems
503
or equivalently that
(1 −α)
∞
X
n=0
αn = 1
provided |α| < 1.
(c) Verify your results by using symbolic MATLAB to ﬁnd an expression for the above sum.
8.3. Periodicity of sampled signals—MATLAB
Consider an analog periodic sinusoid x(t) = cos(3πt + π/4) being sampled using a sampling period Ts to
obtain the discrete-time signal x[n] = x(t)|t=nTs = cos(3πTsn + π/4).
(a) Determine the discrete frequency of x[n].
(b) Choose a value of Ts for which the discrete-time signal x[n] is periodic. Use MATLAB to plot a few
periods of x[n], and verify its periodicity.
(c) Choose a value of Ts for which the discrete-time signal x[n] is not periodic. Use MATLAB to plot x[n]
and choose an appropriate length to show the signal is not periodic.
(d) Determine under what condition the value of Ts makes x[n] periodic.
8.4. Even and odd decomposition and energy—MATLAB
Suppose you sample the analog signal
x(t) =
1 −t
0 ≤t ≤1
0
otherwise
with a sampling period Ts = 0.25 to generate x[n] = x(t)|t=nTs.
(a) Use MATLAB to plot x[−n] for an appropriate interval.
(b) Find xe[n] = 0.5[x[n] + x[−n]] and plot it carefully using MATLAB.
(c) Find xo[n] = 0.5[x[n] −x[−n]] and plot it carefully using MATLAB.
(d) Verify that xe[n] + xo[n] = x[n] graphically.
(e) Compute the energy of x[n] and compare it to the sum of the energies of xe[n] and xo[n].
8.5. Signal representation in terms of u[n]—MATLAB
We have shown how any discrete-time signal can be represented as a sum of weighted and shifted versions
of δ[n]. Given that
δ[n] = u[n] −u[n −1]
it should be possible to represent any signal as a combination of unit-step functions. Consider a discrete-
time ramp r[n] = nu[n], which in terms of δ[n] is written as
r[n] =
∞
X
k=−∞
r[k]δ[n −k]
Replace r[k] = ku[k] and use δ[n] = u[n] −u[n −1] to show that r[n] can be expressed in terms of u[n] as
r[n] =
∞
X
k=1
u[n −k]
Does this equation make sense? Use MATLAB to plot the obtained r[n] to help you answer this.

504
CHAPTER 8:
Discrete-Time Signals and Systems
8.6. Generation of periodic discrete-time signals—MATLAB
Periodic signals can be generated by obtaining a period and adding shifted versions of this period. Suppose
we wish to generate a train of triangular pulses. A period of the signal is
x[n] = r[n] −2r[n −1] + r[n −2]
where r[n] = nu[n] is the discrete-time ramp signal.
(a) Carefully plot x[n].
(b) Let
y[n] =
∞
X
k=−∞
x[n −2k]
and carefully plot it. Indicate the period N of y[n].
(c) Write a MATLAB script to generate and plot the periodic signal y[n].
8.7. Expansion and compression of discrete-time signals—MATLAB
Consider the discrete-time signal x[n] = cos(2πn/7).
(a) The discrete-time signal can be compressed by getting rid of some of its samples (downsampling).
Consider the downsampling by 2. Write a MATLAB script to obtain and plot z[n] = x[2n]. Plot also
x[n] and compare it with z[n]. What happended? Explain.
(b) The expansion for discrete-time signals requires interpolation, and we will see it later. However, a ﬁrst
step of this process is the so-called upsampling. Upsampling by 2 consists in deﬁning a new signal
y[n] such that y[n] = x[n/2] for n even, and y[n] = 0 otherwise. Write a MATLAB script to perform
upsampling on x[n]. Plot the resulting signal y[n] and explain its relation with x[n].
(c) If x[n] resulted from sampling a continuous-time signal x(t) = cos(2πt) using a sampling period Ts
and with no frequency aliasing, determine Ts. How would you sample the analog signal x(t) to get the
downsampled signals z[n]? That is, choose a value for the sampling period Ts to get z[n] directly from
x(t). Can you choose Ts to get y[n] from x(t) directly? Explain.
8.8. Absolutely summable and ﬁnite-energy discrete-time signals—MATLAB
Suppose we sample the analog signal x(t) = e−2tu(t) using a sample period Ts = 1.
(a) Expressing the sampled signal as x(nTs) = x[n] = αnu[n], what is the corresponding value of α? Use
MATLAB to plot x[n].
(b) Show that x[n] is absolutely summable—that is, show the following sum is ﬁnite:
∞
X
n=−∞
|x[n]|
(c) If you know that x[n] is absolutely summable, could you say that x[n] is a ﬁnite-energy signal? Use
MATLAB to plot |x[n]| and x2[n] in the same plot to help you decide.
(d) In general, for what values of α are signals y[n] = αnu[n] ﬁnite energy? Explain.
8.9. Discrete-time periodic signals
Determine whether the following discrete-time sinusoids are periodic or not. If periodic, determine its
period N0.
x[n] = 2 cos(πn −π/2)
y[n] = sin(n −π/2)
z[n] = x[n] + y[n]
v[n] = sin(3πn/2)

Problems
505
8.10. Periodicity of discrete-time signals
Consider periodic signals x[n], of period N1 = 4, and y[n], of period N2 = 6. What would be the period of
z[n] = x[n] + y[n]
v[n] = x[n]y[n]
w[n] = x[2n]
8.11. Periodicity of sum and product of periodic signals—MATLAB
If x[n] is periodic of period N1 > 0 and y[n] is periodic of period N2 > 0:
(a) What should be the condition for the sum of x[n] and y[n] to be periodic?
(b) What would be the period of the product x[n]y[n]?
(c) Would the formula
N1N2
gcd(N1, N2)
(gcd(N1, N2) stands for the greatest common divisor of N1 and N2) give the period of the sum and the
product of the two signals x[n] and y[n]?
(d) Use MATLAB to plot the signals x[n] = cos(2πn/3)u[n], and y[n] = (1 + sin(6πn/7))u[n], their sum
and product, and to ﬁnd their periods and to verify your analytic results.
8.12. Echoing of music—MATLAB
An effect similar to multipath in acoustics is echoing or reverberation. To see the effects of an echo in an
acoustic signal consider the simulation of echoes on the “handel.mat” signal y[n]. Pretend that this piece is
being played in a round theater where the orchestra is in the middle of two concentric circles and the walls
on one half side are at a radial distances of 17.15 meters (corresponding to the inner circle) and 34.3 meters
(corresponding to the outer circle) on the other side (yes, an usual theater!) from the orchestra. The speed
of sound is 343 meters/sec. Assume that the recorded signal is the sum of the original signal y[n] and
attenuated echoes from the two walls so that the recorded signal is given by
r[n] = y[n] + 0.8y[n −N1] + 0.6y[n −N2]
where N1 is the delay caused by the closest wall and N2 is the delay caused by the farther wall. The
recorder is at the center of the auditorium where the orchestra is and we record for 1.5 seconds.
(a) Find the values of the two delays N1 and N2. Give the expression for the discrete-time recorded signal
r[n]. The sampling frequency Fs of “handel.mat” is given when you load it in MATLAB.
(b) Simulate the echo signal. Plot r[n]. Use sound to listen to the original and the echoed signals.
8.13. Envelope modulation—MATLAB
In the generation of music by computer, the process of modulation is extremely important. When playing
an instrument, the player typically does it in three stages: (1) rise time or attack, (2) sustained time, and
(3) decay time. Suppose we model these three stages as an envelope continuous-time signal given by
e(t) = 1
3 [r(t) −r(t −3)] −1
0.1[r(t −20) + r(t −30)]
where r(t) is the ramp signal.
(a) For a simple tone x(t) = cos(2π/T0t), the modulated signal is y(t) = x(t)e(t). Find the period T0 so that
100 cycles of the sinusoid occur for the duration of the envelope signal.
(b) Simulate in MATLAB the modulated signal using the value of T0 = 1 and a simulation sampling time
of 0.1T0. Plot y(t) and e(t) (discretized with the sampling period 0.1T0) and listen to the modulated
signal using sound.

506
CHAPTER 8:
Discrete-Time Signals and Systems
8.14. LTI of ADCs
An ADC can be thought of as composed of three subsystems: a sampler, a quantizer, and a coder.
(a) The sampler, as a system, has as input an analog signal x(t) and as output a discrete-time signal
x(nTs) = x(t)|t=nTs where Ts is the sampling period. Determine whether the sampler is a linear system
or not.
(b) Sample x(t) = cos(0.5πt)u(t) and x(t −0.5) using Ts = 1 to get y(nTs) and z(nTs), respectively. Plot
x(t), x(t −0.5), and y(nTs) and z(nTs). Is z(nTs) a shifted version of y(nTs) so that you can say the
sampler is time invariant? Explain.
8.15. LTI of ADCs (part 2)
A two-bit quantizer of an ADC has as input x(nTs) and as output ˆx(nTs), such that if
k1 ≤x(nTs) < (k + 1)1
→
ˆx(nTs) = k1
k = −2, −1, 0, 1
(a) Is this system time invariant? Explain.
(b) Suppose that the value of 1 in the quantizer is 0.25, and the sampled signal is x(nTs) = nTs, Ts = 0.1
and −5 ≤n ≤5. Use the sampled signal to determine whether the quantizer is a linear system or not.
Explain.
(c) From the results in this and the previous problem, would you say that the ADC is an LTI system?
Explain.
8.16. Rectangular windowing system—MATLAB
A window is a signal w[n] that is used to highlight part of another signal. The windowing process consists
in multiplying an input signal x[n] by the window signal w[n], so that the output is
y[n] = x[n]w[n]
There are different types of windows used in signal processing. One of them is the so-called rectangular
window, which is given by
w[n] = u[n] −u[n −N]
(a) Determine whether the rectangular windowing system is linear. Explain.
(b) Suppose x[n] = nu[n]. Plot the output y[n] of the windowing system (with N = 6).
(c) Let the input be x[n −6]. Plot the corresponding output of the rectangular windowing system, and
indicate whether the rectangular windowing system is time invariant.
8.17. Impulse response of an IIR system—MATLAB
A discrete-time IIR system is represented by the difference equation
y[n] = 0.15y[n −2] + x[n]
n ≥0
where x[n] is the input and y[n] is the output.
(a) To ﬁnd the impulse response h[n] of the system, let x[n] = δ[n], y[n] = h[n], and the initial conditions
be zero, y[n] = h[n] = 0, n < 0. Find recursively the values of h[n] for values of n ≥0.
(b) As a second way to do it, replace the relation between the input and the output given by the difference
equation to obtain a convolution sum representation that will give the impulse response h[n]. What
is h[n]?
(c) Use the MATLAB function ﬁlter to get the impulse response h[n] (use help to learn about the function
ﬁlter).

Problems
507
8.18. FIR ﬁlter—MATLAB
An FIR ﬁlter has a nonrecursive input–output relation
y[n] =
5
X
k=0
kx[n −k]
(a) Find and plot using MATLAB the impulse response h[n] of this ﬁlter.
(b) Is this a causal and stable ﬁlter? Explain.
(c) Find and plot the unit-step response s[n] for this ﬁlter.
(d) If the input x[n] for this ﬁlter is bounded, i.e., |x[n]| < 3, what would be a minimum bound M for the
output (i.e., |y[n]| ≤M)?
(e) Use the MATLAB function ﬁlter to compute the impulse response h[n] and the unit-step response s[n]
for the given ﬁlter and plot them.
8.19. LTI and convolution sum—MATLAB
The impulse response of a discrete-time system is h[n] = (−0.5)nu[n].
(a) If the input of the system is x[n] = δ[n] + δ[n −1] + δ[n −2], use the linearity and time invariance of
the system to ﬁnd the corresponding output y[n].
(b) Find the convolution sum corresponding to the above input, and show that your solution coincides
with the output y[n] obtained above.
(c) Use the MATLAB function conv to ﬁnd the output y[n] due to the given input x[n]. Plot x[n], h[n], and
y[n] using MATLAB.
8.20. Steady state of IIR systems—MATLAB
Suppose an IIR system is represented by a difference equation
y[n] = ay[n −1] + x[n]
where x[n] is the input and y[n] is the output.
(a) If the input x[n] = u[n] and it is known that the steady-state response is y[n] = 2, what would be a for
that to be possible (in steady state x[n] = 1 and y[n] = y[n −1] = 2 since n →∞).
(b) Writing the system input as x[n] = u[n] = δ[n] + δ[n −1] + δ[n −2] + · · · then according to the
linearity and time invariance, the output should be
y[n] = h[n] + h[n −1] + h[n −2] + · · ·
Use the value for a found above, that the initial condition is zero (i.e., y[−1] = 0) and that the input is
x[n] = u[n], to ﬁnd the values of the impulse response h[n] for n ≥0 using the above equation. The
system is causal.
(c) Use the MATLAB function ﬁlter to compute the impulse response h[n] and compare it with the one
obtained above.
8.21. Causal systems and real-time processing
Systems that operate under real-time conditions need to be causal—that is, they can only process present
and past inputs. When no real-time processing is needed the system can be noncausal.

508
CHAPTER 8:
Discrete-Time Signals and Systems
(a) Consider the case of averaging an input signal x[n] under real-time conditions. Suppose you are given
two different ﬁlters,
• y[n] = 1
N
N−1
X
k=0
x[n −k]
• y[n] = 1
N
N−1
X
k=−N+1
x[n −k]
Which one of these would you use and why?
(b) If you are given a tape with the data, which of the two ﬁlters would you use? Why? Would you use
either? Explain.
8.22. IIR versus FIR systems
A signiﬁcant difference between IIR and FIR discrete-time systems is stability. Consider an IIR ﬁlter with
the difference equation
y1[n] = x[n] −0.5y1[n −1]
where x[n] is the input and y1[n] is the output. Then consider an FIR ﬁlter
y2[n] = x[n] + 0.5x[n −1] + 3x[n −2] + x[n −5]
where x[n] is the input and y2[n] is the output.
(a) Since to check the stability of these ﬁlters we need their impulse responses, ﬁnd the impulse responses
h1[n] corresponding to the IIR ﬁlter by recursion, and h2[n] corresponding to the FIR ﬁlter.
(b) Use the impulse response h1[n] to check the stability of the IIR ﬁlter.
(c) Use the impulse response h2[n] to check the stability of the FIR ﬁlter.
(d) Since the impulse response of a FIR ﬁlter has a ﬁnite number of nonzero terms, would it be correct to
say that FIR ﬁlters are always stable? Explain.
8.23. Unit-step versus impulse response—MATLAB
The unit-step response of a discrete-time LTI system is
s[n] = 2[(−0.5)n −1]u[n]
Use this information to ﬁnd
(a) The impulse response h[n] of the discrete-time LTI system.
(b) The response of the LTI system to a ramp signal x[n] = nu[n]. Use the MATLAB function ﬁlter and
superposition to ﬁnd it.
8.24. Convolution sum—MATLAB
A discrete-time system has a unit-impulse response h[n].
(a) Let the input to the discrete-time system be a pulse x[n] = u[n] −u[n −4]. Compute the output of the
system in terms of the impulse response.
(b) Let h[n] = 0.5nu[n]. What would be the response of the system y[n] to x[n] = u[n] −u[n −4]? Plot the
output y[n].
(c) Use the convolution sum to verify your response y[n].
(d) Use the MATLAB function conv to compute the response y[n] to x[n] = u[n] −u[n −4]. Plot both the
input and output.
8.25. Discrete envelope detector—MATLAB
Consider an envelope detector that would be used to detect the message sent in an AM system. Consider
the envelope detector as a system composed of the cascading of two systems: one which computes the

Problems
509
absolute value of the input, and a second one that low-pass ﬁlters its input. A circuit that is used as an
envelope detector consists of a diode circuit that does the absolute value operation, and an RC circuit that
does the low-pass ﬁltering. The following is an implementation of these operations in the discrete-time
system.
Let the input to the envelope detector be a sampled signal,
x(nTs) = p(nTs) cos(2000πnTs)
where
p(nTs) = u(nTs) −u(nTs −20Ts) + u(nTs −40Ts) −u(nTs −60Ts)
where two pulses of duration 20Ts and amplitude equal to one.
(a) Choose Ts = 0.01, and generate 100 samples of the input signal x(nTs) and plot it.
(b) Consider then the subsystem that computes the absolute value of the input x(nTs) and compute and
plot 100 samples of y(nTs) = |x(nTs)|.
(c) Let the low-pass ﬁltering be done by a moving averager of order 15—that is, if y(nTs) is the input, then
the output of the ﬁlter is
z(nTs) = 1
15
14
X
k=0
y(nTs −kTs)
Implement this ﬁlter using the MATLAB function ﬁlter, and plot the result. Explain your results.
(d) Is this a linear system? Come up with an example using the script developed above to show that the
system is linear or not.

This page intentionally left blank

CHAPTER 9
The Z-Transform
I was born not knowing and have had
only a little time to change that here and there.
Richard P. Feynman, (1918–1988)
Professor and Nobel Prize physicist
9.1 INTRODUCTION
Just as with the Laplace transform for continuous-time signals and systems, the Z-transform provides
a way to represent discrete-time signals and systems, and to process discrete-time signals.
Although the Z-transform can be related to the Laplace transform, the relation is operationally not
very useful. However, it can be used to show that the complex z-plane is in a polar form where the
radius is a damping factor and the angle corresponds to the discrete frequency ω in radians. Thus,
the unit circle in the z-plane is analogous to the j axis in the Laplace plane, and the inside of the
unit circle is analogous to the left-hand s-plane. We will see that once the connection between the
Laplace plane and the z-plane is established, the signiﬁcance of poles and zeros in the z-plane can be
obtained like in the Laplace plane.
The representation of discrete-time signals by the Z-transform is very intuitive—it converts a sequence
of samples into a polynomial. The inverse Z-transform can be achieved by many more methods than
the inverse Laplace transform, but the partial fraction expansion is still the most commonly used
method. Using the one-sided Z-transform, for solving difference equations that could result from
the discretization of differential equations, but not exclusively, is an important application of the
Z-transform.
As it was the case with the Laplace transform and the convolution integral, the most important
property of the Z-transform is the implementation of the convolution sum as a multiplication of
polynomials. This is not only important as a computational tool but also as a way to represent a
discrete system by its transfer function. Filtering is again an important application, and as before, the
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00013-2
c⃝2011, Elsevier Inc. All rights reserved.
511

512
CHAPTER 9:
The Z-Transform
localization of poles and zeros determines the type of ﬁlter. However, in the discrete domain there is
a greater variety of ﬁlters than in the analog domain.
9.2 LAPLACE TRANSFORM OF SAMPLED SIGNALS
The Laplace transform of a sampled signal
x(t) =
X
n
x(nTs)δ(t −nTs)
(9.1)
is given by
X(s) =
X
n
x(nTs)L[δ(t −nTs)]
=
X
n
x(nTs)e−nsTs
(9.2)
By letting z = esTs, we can rewrite Equation (9.2) as
Z[x(nTs)] = L[xs(t)]
z=esTs
=
X
n
x(nTs)z−n
(9.3)
which is called the Z-transform of the sampled signal.
Remarks The function X(s) in Equation (9.2) is different from the Laplace transforms we considered
before:
I
Letting s = j, X() is periodic of period 2π/Ts (i.e., X( + 2π/Ts) = X() for an integer k). Indeed,
X( + 2π/Ts) =
X
n
x(nTs)e−jn(+2π/Ts)Ts =
X
n
x(nTs)e−jn(Ts+2π) = X()
I
X(s) may have an inﬁnite number of poles or zeros—complicating the partial fraction expansion when
ﬁnding its inverse. Fortunately, the presence of the {e−nsTs} terms suggests that the inverse should be done
using the time-shift property of the Laplace transform instead, giving Equation (9.1).
I Example 9.1
To see the possibility of an inﬁnite number of poles and zeros in the Laplace transform of a sam-
pled signal, consider a pulse x(t) = u(t) −u(t −T0) sampled with a sampling period Ts = T0/N.
Find the Laplace transform of the sampled signal and determine its poles and zeros.
Solution
The sampled signal is
x(nTs) =
1
0 ≤nTs ≤T0 or 0 ≤n ≤N
0
otherwise

9.2 Laplace Transform of Sampled Signals
513
with Laplace transform
X(s) =
N
X
n=0
e−nsTs = 1 −e−(N+1)sTs
1 −e−sTs
The poles are the sk values that make the denominator zero—that is,
e−skTs = 1
= e j2πk
k integer, −∞< k < ∞
or sk = −j2πk/Ts for any integer k, an inﬁnite number of poles. Similarly, one can show that X(s)
has an inﬁnite number of zeros by ﬁnding the values sm that make the numerator zero, or
e−(N+1)smTs = 1
= e j2πm
m integer, −∞< m < ∞
or sm = −j2πm/((N + 1)Ts) for any integer m. Such a behavior can be better understood when we
consider the connection between the s-plane and the z-plane.
I
The History of the Z-Transform
The history of the Z-transform goes back to the work of the French mathematician De Moivre, who in 1730 introduced the
characteristic function to represent the probability mass function of a discrete random variable. The characteristic function
is identical to the Z-transform. Also, the Z-transform is a special case of the Laurent’s series, used to represent complex
functions.
In the 1950s the Russian engineer and mathematician Yakov Tsypkin (1919–1997) proposed the discrete Laplace transform,
which he applied to the study of pulsed systems. Then Professor John Ragazzini and his students Eliahu Jury and Lofti
Zadeh at Columbia University developed the Z-transform. Ragazzini (1912–1988) was chairman of the Department of
Electrical Engineering at Columbia University. Three of his students are well recognized in electrical engineering for their
accomplishments: Jury for the Z-transform, nonlinear systems, and the inners stability theory; Zadeh for the Z-transform
and fuzzy set theory; and Rudolf Kalman for the Kalman ﬁltering.
Jury was born in Iraq, and received his doctor of engineering science degree from Columbia University in 1953. He was
professor of electrical engineering at the University of California, Berkeley, and at the University of Miami. Among his
publications, Professor Jury’s “Theory and Application of the Z-transform,” is a seminal work on the theory and application
of the Z-transform.
Remarks
I
The relation z = esTs provides the connection between the s-plane and the z-plane:
z = esTs = e(σ+j)Ts = eσTse jTs

514
CHAPTER 9:
The Z-Transform
Letting r = eσTs and ω = Ts, we have that
z = re jω
which is a complex variable in polar form, with radius 0 ≤r < ∞and angle ω in radians. The variable
r is a damping factor and ω is the discrete frequency in radians, so the z-plane corresponds to circles of
radius r and angles −π ≤ω < π.
I
Let us see how the relation z = e sTs maps the s-plane into the z-plane. Consider the strip of width
2π/Ts across the s-plane shown in Figure 9.1. The width of this strip is related to the Nyquist
condition establishing that the maximum frequency of the analog signals we are considering is
M = s/2 = π/Ts where s is the sampling frequency and Ts is the sampling period. If Ts →0,
we would be considering the class of signals with maximum frequency approaching ∞—that is, all signals.
The relation z = e sTs maps the real part of s = σ + j, Re(s) = σ, into the radius r = eσTs ≥0, and
the analog frequencies −π/Ts ≤ ≤π/Ts into −π ≤ω < π, according to the frequency connection
ω = Ts. Thus, the mapping of the j axis in the s-plane, corresponding to σ = 0, gives a circle of radius
r = 1 or the unit circle.
The right-hand s-plane, σ > 0, maps into circles with radius r > 1, and the left-hand s-plane, σ < 0,
maps into circles of radius r < 1. Points A, B, and C in the strip are mapped into corresponding points
in the z-plane as shown in Figure 9.1. So the given strip in the s-plane maps into the whole z-plane—
similarly for other strips of the same width. Thus, the s-plane, as a union of these strips, is mapped onto
the same z-plane.
I
The mapping z = e sTs can be used to illustrate the sampling process. Consider a band-limited signal x(t)
with maximum frequency π/Ts with a spectrum in the band [−π/Ts π/Ts]. According to the relation
z = e sTs the spectrum of x(t) in [−π/Ts π/Ts] is mapped into the unit circle of the z-plane from [−π, π)
on the unit circle. Going around the unit circle in the z-plane, the mapped frequency response repeats
periodically just like the spectrum of the sampled signal.
z = e sTs
jΩ
σ
A
B
ω
r
A
B
C
C
s-plane
z-plane
−j π
Ts
j π
Ts
FIGURE 9.1
Mapping of the Laplace plane into the z-plane. Slabs of width 2π/Ts in the left-hand s-plane are mapped into the
inside of a unit circle in the z-plane. The right-hand side of the slab is mapped outside the unit circle. The j-axis
in the s-plane is mapped into the unit-circle in the z-plane. The whole s-plane as a union of these slabs is
mapped onto the same z-plane.

9.3 Two-Sided Z-Transform
515
9.3 TWO-SIDED Z-TRANSFORM
Given a discrete-time signal x[n], −∞< n < ∞, its two-sided Z-transform is
X(z) =
∞
X
n=−∞
x[n]z−n
(9.4)
deﬁned in a region of convergence (ROC) in the z-plane.
Considering the sampled signal x(nTs) a function of n in Equation (9.3), we obtain the two-sided
Z-transform.
Remarks
I
The Z-transform can be thought of as the transformation of the sequence {x[n]} into a polynomial X(z)
(possibly of inﬁnite degree in positive and negative powers of z) where to each x[n0] we attach a monomial
z−n0. Thus, given a sequence of samples {x[n]} its Z-transform simply consists in creating a polynomial
with coefﬁcients x[n] corresponding to z−n. Given a Z-transform as in Equation (9.4), its inverse is easily
obtained by looking at the coefﬁcients attached to the z−n monomials for positive as well as negative values
of the sample value n. Clearly, this inverse is not in a closed form. We will see ways to compute these later
in this chapter.
I
The two-sided Z-transform is not useful in solving difference equations with initial conditions, just as the
two-sided Laplace transform was not useful either in solving differential equations with initial conditions.
To include initial conditions in the transformation it is necessary to deﬁne the one-sided Z-transform.
The one-sided Z-transform is deﬁned for a causal signal, x[n] = 0 for n < 0, or for signals that are made causal
by multiplying them with the unit-step signal u[n]:
X1(z) = Z(x[n]u[n]) =
∞
X
n=0
x[n]u[n]z−n
(9.5)
in a region of convergence R1.
The two-sided Z-transform can be expressed in terms of the one-sided Z-transform as follows:
X(z) = Z
 x[n]u[n]

+ Z
 x[−n]u[n]

|z −x[0]
(9.6)
The region of convergence of X(z) is
R = R1 ∩R2
where R1 is the region of convergence of Z
 x[n]u[n]

and R2 is the region of convergence of Z
 x[−n]u[n]
z.
The one-sided Z-transform coincides with the two-sided Z-transform whenever the discrete-time sig-
nal x[n] is causal (i.e., x[n] = 0 for n < 0). If the signal is noncausal, multiplying it by u[n] makes it

516
CHAPTER 9:
The Z-Transform
causal. To express the two-sided Z-transform in terms of the one-sided Z-transform we separate the
sum into two and make each into a causal sum:
X(z) =
∞
X
n=−∞
x[n]z−n =
∞
X
n=0
x[n]u[n]z−n +
0
X
n=−∞
x[n]u[−n]z−n −x[0]
= Z
 x[n]u[n]

+
∞
X
m=0
x[−m]u[m]zm −x[0]
= Z
 x[n]u[n]

+ Z
 x[−n]u[n]

|z −x[0]
where the inclusion of the additional term x[0] in the sum from −∞to 0 is compensated by subtract-
ing it, and in the same sum a change of variable (m = −n) gives a one-sided Z-transform in terms of
positive powers of z, as indicated by the notation Z
 x[−n]u[n]

|z.
9.3.1 Region of Convergence
The inﬁnite summation of the two-sided Z-transform must converge for some values of z. For X(z) to
converge it is necessary that
|X(z)| =

X
n
x[n]z−n
 ≤
X
n
|x[n]||r−ne jωn| =
X
n
|x[n]||r−n| < ∞
Thus, the convergence of X(z) depends on r. The region in the z-plane where X(z) converges, its ROC,
connects the signal and its Z-transform uniquely. As with the Laplace transform, the poles of X(z) are
connected with its region of convergence.
The poles of a Z-transform X(z) are complex values {pk} such that
X(pk) →∞
while the zeros of X(z) are the complex values {zk} that make
X(zk) = 0
I Example 9.2
Find the poles and the zeros of the following Z-transforms:
(a) X1(z) = 1 + 2z−1 + 3z−2 + 4z−3
(b) X2(z) =
(z−1 −1)(z−1 + 2)2
z−1(z−2 +
√
2z−1 + 1)

9.3 Two-Sided Z-Transform
517
Solution
To see the poles and the zeros more clearly let us express X1(z) as a function of positive powers
of z:
X1(z) = z3(1 + 2z−1 + 3z−2 + 4z−3)
z3
= z3 + 2z2 + 3z + 4
z3
= N1(z)
D1(z)
There are three poles at z = 0, the roots of D1(z) = 0, and the zeros are the roots of N1(z) = z3 +
2z2 + 3z + 4 = 0.
Likewise, expressing X2(z) as a function of positive powers of z,
X2(z) =
z3(z−1 −1)(z−1 + 2)2
z3(z−1(z−2 +
√
2z−1 + 1))
= (1 −z)(1 + 2z)2
1 +
√
2z + z2
= N2(z)
D2(z)
The poles of X2(z) are the roots of D2(z) = 1 +
√
2z + z2 = 0, while the zeros of X2(z) are the roots
of N2(z) = (1 −z)(1 + 2z)2 = 0.
I
The region of convergence depends on the support of the signal. If it is ﬁnite, the ROC is very much
the whole z-plane; if it is inﬁnite, the ROC depends on whether the signal is causal, anti-causal,
or noncausal. Something to remember is that in no case does the ROC include any poles of the
Z-transform.
ROC of Finite-Support Signals
The ROC of the Z-transform of a signal x[n] of ﬁnite support [N0, N1] where −∞< N0 ≤n ≤N1 < ∞,
X(z) =
N1
X
n=N0
x[n]z−n
(9.7)
is the whole z-plane, excluding the origin z = 0 and/or z = ±∞depending on N0 and N1.
Given the ﬁnite support of x[n] its Z-transform has no convergence problem. Indeed, for any z ̸= 0
(or z ̸= ±∞if positive powers of z occur in Equation (9.40)), we have
|X(z)| ≤
N1
X
n=N0
|x[n]||z−n| ≤(N1 −N0 + 1) max |x[n]| max |z−n| < ∞
The poles of X(z) are either at the origin of the z-plane (e.g., when N0 ≥0) or there are no poles (e.g.,
when N1 ≤0). Thus, only when z = 0 or z = ±∞would X(z) go to inﬁnity. The ROC is the whole
z-plane excluding these values.

518
CHAPTER 9:
The Z-Transform
I Example 9.3
Find the Z-transform of a discrete-time pulse
x[n] =
1
0 ≤n ≤9
0
otherwise
Determine the region of convergence of X(z).
Solution
The Z-transform of x[n] is
X(z) =
9
X
n=0
1 z−n = 1 −z−10
1 −z−1 = z10 −1
z9(z −1)
(9.8)
That this sum equals the term on the right can be shown by multiplying the left term by the
denominator 1 −z−1 and verifying the result is the same as the numerator in negative powers of
z. In fact,
(1 −z−1)
9
X
n=0
1 z−n =
9
X
n=0
1 z−n −
9
X
n=0
1 z−n−1
= (1 + z−1 + · · · + z−9) −(z−1 + · · · + z−9 + z−10) = 1 −z−10
Since x[n] is a ﬁnite sequence there is no problem with the convergence of the sum, although X(z)
in Equation (9.8) seems to indicate the need for z ̸= 1 (z = 1 makes the numerator and denom-
inator zero). From the sum, if we let z = 1, then X(1) = 10, so there is no need to restrict z to be
different from 1. This is caused by the pole at z = 1 being canceled by a zero. Indeed, the zeros zk
of X(z) (see Eq. 9.8) are the roots of z10 −1 = 0, which are zk = e j2πk/10 for k = 0, . . . , 9. Therefore,
the zero when k = 0, or z0 = 1, cancels the pole at 1 so that
X(z) =
Q9
k=1(z −e jπk/5)
z9
That is, X(z) has nine poles at the origin and nine zeros around the unit circle except at z = 1. Thus,
the whole z-plane excluding the origin is the region of convergence of X(z) .
I
ROC of Inﬁnite-Support Signals
Signals of inﬁnite support are either causal, anti-causal, or a combination of these or noncausal. Now
for the Z-transform of a causal signal xc[n] (i.e., xc[n] = 0, n < 0)
Xc(z) =
∞
X
n=0
xc[n]z−n =
∞
X
n=0
xc[n]r−ne−jnω
to converge we need to determine appropriate values of r, the damping factor. The frequency ω
has no effect on the convergence. If R1 is the radius of the farthest-out pole of Xc(z), then there is

9.3 Two-Sided Z-Transform
519
an exponential Rn
1u[n] such that |xc[n]| < MRn
1 for n ≥0 for some value M > 0. Then, for X(z) to
converge we need that
|Xc(z)| ≤
∞
X
n=0
|xc[n]||r−n| < M
∞
X
n=0

R1
r

n
< ∞
or that R1/r < 1, which is equivalent to |z| = r > R1. As indicated, this ROC does not include any
poles of Xc(z)—it is the outside of a circle containing all the poles of Xc(z).
Likewise, for an anti-causal signal xa[n], if we choose a radius R2 that is smaller than the radius of all
the poles of Xa(z), the region of convergence is |z| = r < R2. This ROC does not include any poles of
Xa(z)—it is the inside of a circle that does not contain any of the poles of Xa(z).
If the signal x[n] is noncausal, it can be expressed as
x[n] = xc[n] + xa[n]
where the supports of xa[n] and xc[n] can be ﬁnite or inﬁnite or any possible combination of these
two. The corresponding ROC of X(z) = Z{x[n]} would then be
0 ≤R1 < |z| < R2 < ∞
This ROC is a torus surrounded on the inside by the poles of the causal component, and in the
outside by the poles of the anti-causal component. If the signal has ﬁnite support, then R1 = 0 and
R2 = ∞, coinciding with the result for ﬁnite-support signals.
For the Z-transform X(z) of an inﬁnite-support signal:
I
A causal signal x[n] has a region of convergence |z| > R1 where R1 is the largest radius of the poles of
X(z)—that is, the region of convergence is the outside of a circle of radius R1.
I
An anti-causal signal x[n] has as region of convergence the inside of the circle deﬁned by the smallest
radius R2 of the poles of X(z), or |z| < R2.
I
A noncausal signal x[n] has as region of convergence R1 < |z| < R2, or the inside of a torus of inside
radius R1 and outside radius R2 corresponding to the maximum and minimum radii of the poles of Xc(z)
and Xa(z), which are the Z-transforms of the causal and anti-causal components of x[n].
I Example 9.4
The poles of X(z) are z = 0.5 and z = 2. Find all the possible signals that can be associated with it
according to different regions of convergence.
Solution
Possible regions of convergence are:
I
{R1 : |z| > 2}—the outside of a circle of radius 2, we associate X(z) with a causal signal x1[n].

520
CHAPTER 9:
The Z-Transform
I
{R2 : |z| < 0.5}—the inside of a circle of radius 0.5, an anti-causal signal x2[n] can be associated
with X(z).
I
{R3 : 0.5 < |z| < 2}—a torus of radii 0.5 and 2, a noncausal signal x3[n] can be associated with
X(z).
Three different signals can be connected with X(z) by considering three different regions of
convergence.
I
I Example 9.5
Find the regions of convergence of the Z-transforms of the following signals:
(a) x1[n] =
1
2
n
u[n]
(b) x2[n] = −
1
2
n
u[−n −1]
Determine then the Z-transform of x1[n] + x2[n].
Solution
The signal x1[n] is causal, while x2[n] is anti-causal. The Z-transform of x1[n] is
X1(z) =
∞
X
n=0
1
2
n
z−n =
1
1 −0.5z−1 =
z
z −0.5
provided that |0.5z−1| < 1 or that its region of convergence is R1 : |z| > 0.5. The region R1 is the
outside of a circle of radius 0.5.
The signal x2[n] grows as n decreases from −1 to −∞, and the rest of its values are zero. Its Z-
transform is found as
X2(z) = −
−1
X
n=−∞
1
2
n
z−n = −
∞
X
m=0
1
2
−m
zm + 1
= −
∞
X
m=0
2mzm + 1 =
−1
1 −2z + 1 =
z
z −0.5
with a region of convergence of R2 : |z| < 0.5.
Although the signals are clearly different, their Z-transforms are identical. It is the corresponding
regions of convergence that differentiate them. The Z-transform of x1[n] + x2[n] does not exist
given that the intersection of R1 and R2 is empty.
I

9.4 One-Sided Z-Transform
521
Remarks The uniqueness of the Z-transform requires that the Z-transform of a signal be accompanied by
a region of convergence. It is possible to have identical Z-transforms with different regions of convergence,
corresponding to different signals.
I Example 9.6
Let c[n] = α|n|, 0 < α < 1, be a discrete-time signal (it is actually an autocorrelation function
related to the power spectrum of a random signal). Determine its Z-transform.
Solution
To ﬁnd its two-sided Z-transform C(z) we consider its causal and anti-causal components. First,
Z(c[n]u[n]) =
∞
X
n=0
αnz−n =
1
1 −αz−1
with the region of convergence of |αz−1| < 1 or |z| > α. For the anti-causal component,
Z(c[−n]u[n])z =
∞
X
n=0
αnzn =
1
1 −αz
with a region of convergence of |αz| < 1 or |z| < |1/α|.
Thus, the two-sided Z-transform of c[n] is (notice that the term for n = 0 was used twice in the
above calculations, so we need to subtract it)
C(z) =
1
1 −αz−1 +
1
1 −αz −1 =
z
z −α −
z
(z −1/α)
=
(α −1/α)z
(z −α)(z −1/α)
with a region of convergence of
|α| < |z| <

1
α

For instance, for α = 0.5, we get
C(z) =
−1.5z
(z −0.5)(z −2)
0.5 < |z| < 2
I
9.4 ONE-SIDED Z-TRANSFORM
In most situations where the Z-transform is used the system is causal (its impulse response is h[n] = 0
for n < 0) and the input signal is also causal (x[n] = 0 for n < 0). In such cases the one-sided Z-
transform is very appropriate. Moreover, as we saw before, the two-sided Z-transform can be expressed

522
CHAPTER 9:
The Z-Transform
in terms of one-sided Z-transforms. Another valid reason to study the one-sided Z-transform in more
detail is its use in solving difference equations with initial conditions.
Recall that the one-sided Z-transform is deﬁned as
X1(z) = Z(x[n]u[n]) =
∞
X
n=0
x[n]u[n]z−n
(9.9)
in a region of convergence R1. Also recall that the computation of the two-sided Z-transform using
the one-sided Z-transform is given in Equation (9.6).
9.4.1 Computing the Z-Transform with Symbolic MATLAB
Similar to the computation of the Laplace transform, the computation of the Z-transform can be
done using the symbolic toolbox of MATLAB. The following is the necessary code for computing the
Z-transform of
h1[n] = 0.9u[n]
h2[n] = u[n] −u[n −10]
h3[n] = cos(ω0n)u[n]
h4[n] = hsigna1[n]h2[n]
The results are shown at the bottom. (As in the continuous case, in MATLAB the heaviside function is
the same as the unit-step function.)
%%%%%%%%%%%%%%%%%%%%%
% Z-transform computation
%%%%%%%%%%%%%%%%%%%%%
syms n w0
h1 = 0.9. ˆ n; H1 = ztrans(h1)
h2 = heaviside(n) - heaviside(n-10); H2 = ztrans(h2)
h3 = cos(w0 ∗n) ∗heaviside(n); H3 = ztrans(h3)
H4 = ztrans(h1 ∗h3)
H1 = 10/9/(10/9 ∗z - 1) ∗z
H2 = 1 + 1/z + 1/z ˆ 2 + 1/z ˆ 3 + 1/z ˆ 4 + 1/z ˆ 5 + 1/z ˆ 6 + 1/z ˆ 7 + 1/z ˆ 8 + 1/z ˆ 9
H3 = (z - cos(w0)) ∗z/(z ˆ 2 - 2 ∗z ∗cos(w0) + 1)
H4 = 10/9 ∗(10/9 ∗z - cos(w0))*z/(100/81 ∗z ˆ 2 - 20/9 ∗z ∗cos(w0) + 1)
The function iztrans computes the inverse Z-transform. We will illustrate its use later on.
9.4.2 Signal Behavior and Poles
In this section we will use the linearity property of the Z-transform to connect the behavior of the
signal with the poles of its Z-transform.

9.4 One-Sided Z-Transform
523
The Z-transform is a linear transformation, meaning that
Z(ax[n] + by[n]) = aZ(x[n]) + bZ(y[n])
(9.10)
for signals x[n] and y[n] and constants a and b.
To illustrate the linearity property as well as the connection between the signal and the poles of its
Z-transform, consider the signal x[n] = αnu[n] for real or complex values α. Its Z-transform will be
used to compute the Z-transform of the following signals:
I
x[n] = cos(ω0n + θ)u[n] for frequency 0 ≤ω0 ≤π and phase θ.
I
x[n] = αn cos(ω0n + θ)u[n] for frequency 0 ≤ω0 ≤π and phase θ.
Show how the poles of the corresponding Z-transform connect with the signals.
The Z-transform of the causal signal x[n] = αnu[n] is
X(z) =
∞
X
n=0
αnz−n =
∞
X
n=0
(αz−1)n =
1
1 −αz−1 =
z
z −α
ROC: |z| > |α|
(9.11)
Using the last expression in Equation (9.11) the zero of X(z) is z = 0 and its pole is z = α, since the
ﬁrst value makes X(0) = 0 and the second makes X(α) →∞. For α real, be it positive or negative,
the region of convergence is the same, but the poles are located in different places. See Figure 9.2 for
α < 0.
If α = 1 the signal x[n] = u[n] is constant for n ≥0 and the pole of X(z) is at z = 1e j0 (the radius
is r = 1 and the lowest discrete frequency ω = 0 rad). On the other hand, when α = −1 the signal
is x[n] = (−1)nu[n], which varies from sample to sample for n ≥0; its Z-transform has a pole at
z = −1 = 1e jπ (a radius r = 1 and the highest discrete frequency ω = π rad). As we move the pole
toward the center of the z-plane (i.e., |α| →0), the corresponding signal decays exponentially for 0 <
α < 1, and is a modulated exponential of |α|n(−1)nu[n] = |α|n cos(πn)u[n] for −1 < α < 0. When
|α| > 1 the signal becomes either a growing exponential (α > 1) or a growing modulated exponential
(α < −1).
FIGURE 9.2
Region of convergence (shaded area) of X(z) with a pole at
z = α, α < 0 (same ROC if pole is at z = −α).
α ×
−1
1
z-plane

524
CHAPTER 9:
The Z-Transform
For a real value α = |α|e jω0 for ω0 = 0 or π,
x[n] = αnu[n]
⇔
X(z) =
1
1 −αz−1 =
z
z −α
ROC: |z| > |α|
and the location of the pole of X(z) determines the behavior of the signal:
I
When α > 0, then ω0 = 0 and the signal is less and less damped as α →∞.
I
When α < 0, then ω0 = π and the signal is a modulated exponential that grows as α →−∞.
To compute the Z-transform of x[n] = cos(ω0n + θ)u[n], we use Euler’s identity to write x[n] as
x[n] =
"
e j(ω0n+θ)
2
+ e−j(ω0n+θ)
2
#
u[n]
Applying the linearity property and using the above Z-transform when α = e jω0 and its conjugate
α∗= e−jω0, we get
X(z) = 1
2
"
e jθ
1 −e jω0z−1 +
e−jθ
1 −e−jω0z−1
#
= 1
2
2 cos(θ) −2 cos(ω0 −θ)z−1
1 −2 cos(ω0)z−1 + z−2

= cos(θ) −cos(ω0 −θ)z−1
1 −2 cos(ω0)z−1 + z−2
(9.12)
Expressing X(z) in terms of positive powers of z, we get
X(z) = z(z cos(θ) −cos(ω0 −θ))
z2 −2 cos(ω0)z + 1
= z(z cos(θ) −cos(ω0 −θ))
(z −e jω0)(z −e−jω0)
(9.13)
which is valid for any value of θ. If x[n] = cos(ω0n)u[n], then θ = 0 and the poles of X(z) are a
complex conjugate pair on the unit circle at frequency ω0 radians. The zeros are at z = 0 and z =
cos(ω0). When x[n] = sin(ω0n)u[n] = cos(ω0n −π/2)u[n], then θ = −π/2 and the poles are at the
same location as those for the cosine, but the zeros are at z = 0 and z = cos(ω0 + π/2)/ cos(π/2) →
∞, so there is only one ﬁnite zero at zero. For any other value of θ, the poles are located in the same
place but there is a zero at z = 0 and another at z = cos(ω0 −θ)/ cos(θ).
For simplicity, we let θ = 0. If ω0 = 0, one of the double poles at z = 1 is canceled by one of the zeros
at z = 1, resulting in the poles and the zeros of Z([u[n]). Indeed, the signal when ω0 = 0 and θ = 0
is x[n] = cos(0n)u[n] = u[n]. When the frequency ω0 > 0 the poles move along the unit circle from
the lowest (ω0 = 0 rad) to the highest (ω0 = π rad) frequency.

9.4 One-Sided Z-Transform
525
The Z-transform pairs of a cosine and a sine are, respectively,
cos(ω0n)u[n]
⇔
z(z −cos(ω0))
(z −e jω0)(z −e−jω0)
ROC : |z| > 1
(9.14)
sin(ω0n)u[n]
⇔
z sin(ω0)
(z −e jω0)(z −e−jω0)
ROC : |z| > 1
(9.15)
The Z-transforms for these sinusoids have identical poles 1e±jω0, but different zeros. The frequency of the
sinusoid increases from the lowest (ω0 = 0 rad) to the highest (ω0 = π rad)) as the poles move along the unit
circle from 1 to −1 in its lower and upper parts.
Consider then the signal x[n] = rn cos(ω0n + θ)u[n], which is a combination of the above cases. As
before, the signal is expressed as a linear combination
x[n] =
"
e jθ(re jω0)n
2
+ e−jθ(re−jω0)n
2
#
u[n]
and it can be shown that its Z-transform is
X(z) = z(z cos(θ) −r cos(ω0 −θ))
(z −re jω0)(z −re−jω0)
(9.16)
The Z-transform of a sinusoid is a special case of the above (i.e., when r = 1). It also becomes clear
that as the value of r decreases toward zero, the exponential in the signal decays faster, and that
whenever r > 1, the exponential in the signal grows making the signal unbound.
The Z-transform pair
rn cos(ω0n + θ)u[n]
⇔
z(z cos(θ) −r cos(ω0 −θ))
(z −re jω0)(z −re−jω0)
(9.17)
shows how complex conjugate pairs of poles inside the unit circle represent the damping indicated by the
radius r and the frequency given by ω0 in radians.
Double poles are related to the derivative of X(z) or to the multiplication of the signal by n. If
X(z) =
∞
X
n=0
x[n]z−n
its derivative with respect to z is
dX(z)
dz
=
∞
X
n=0
x[n]dz−n
dz
= −z−1
∞
X
n=0
nx[n]z−n

526
CHAPTER 9:
The Z-Transform
Or the pair
nx[n]u[n]
⇔
−zdX(z)
dz
(9.18)
For instance, if X(z) = 1/(1 −αz−1) = z/(z −α), we ﬁnd that
dX(z)
dz
= −
α
(z −α)2
That is, the pair
nαnu[n]
⇔
αz
(z −α)2
indicates that double poles correspond to multiplication of x[n] by n.
The above shows that the location of the poles of X(z) provides basic information about the signal
x[n]. This is illustrated in Figure 9.3, where we display the signal and its corresponding poles.
9.4.3 Convolution Sum and Transfer Function
The most important property of the Z-transform, as it was for the Laplace transform, is the
convolution property.
The output y[n] of a causal LTI system is computed using the convolution sum
y[n] = [x ∗h][n] =
n
X
k=0
x[k]h[n −k] =
n
X
k=0
h[k]x[n −k]
(9.19)
where x[n] is a causal input and h[n] is the impulse response of the system. The Z-transform of y[n] is the
product
Y(z) = Z{[x ∗h][n]} = Z{x[n]}Z{h[n]} = X(z)H(z)
(9.20)
and the transfer function of the system is thus deﬁned as
H(z) = Y(z)
X(z) = Z[output y[n]]
Z[input x[n]]
(9.21)
That is, H(z) transfers the input X(z) into the output Y(z).
Remarks
I
The convolution sum property can be seen as a way to obtain the coefﬁcients of the product of two polyno-
mials. Whenever we multiply two polynomials X1(z) and X2(z), of ﬁnite or inﬁnite order, the coefﬁcients
of the resulting polynomial can be obtained by means of the convolution sum. For instance, consider
X1(z) = 1 + a1z−1 + a2z−2

9.4 One-Sided Z-Transform
527
−2
0
2
−1
0
1
Real part
Imaginary part
0
5
10
15
20
h2(n)
−1
0
1
n
−2
0
2
−1
0
1
Real part
Imaginary part
0
5
10
15
20
0
0.5
1
h1(n)
n
(a)
(b)
(c)
(d)
Imaginary part
0
5
10
15
20
h3(n)
−1
0
1
n
−2
0
2
2
−1
0
1
Real part
−3
−2
−1
0
1
2
3
−1
0
1
2
Imaginary part
0
5
10
15
20
h4(n)
−50
0
50
n
Real part
FIGURE 9.3
Effect of pole location on the inverse Z-transform: (a) if the pole is at z = 1 the signal is u[n], constant for n ≥0;
(b) if the pole is at z = −1 the signal is a cosine of frequency π continuously changing, constant amplitude; (c, d)
when poles are complex, if inside the unit circle the signal is a decaying modulated exponential, and if outside
the unit circle the signal is a growing modulated exponential.

528
CHAPTER 9:
The Z-Transform
and
X2(z) = 1 + b1z−1
Their product is
X1(z)X2(z) = 1 + b1z−1 + a1z−1 + a1b1z−2 + a2z−2 + a2b1z−3
= 1 + (b1 + a1)z−1 + (a1b1 + a2)z−2 + a2b1z−3
The convolution sum of the two sequences [1 a1 a2] and [1 b1], formed by the coefﬁcients of X1(z) and
X2(z), is [1 (a1 + b1) (a2 + b1a1) a2], which corresponds to the coefﬁcients of the product of the polyno-
mials X1(z)X2(z). Also notice that the sequence of length 3 (corresponding to the ﬁrst-order polynomial
X1(z)) and the sequence of length 2 (corresponding to the second-order polynomial X2(z)) when convolved
give a sequence of length 3 + 2 −1 = 4 (corresponding to the third-order polynomial X1(z)X2(z)).
I
A ﬁnite-impulse response or FIR ﬁlter is implemented by means of the convolution sum. Consider an FIR
with an input–output equation
y[n] =
N−1
X
k=0
bkx[n −k]
(9.22)
where x[n] is the input and y[n] is the output. The impulse response of this ﬁlter is (let x[n] = δ[n] and
set initial conditions to zero, so that y[n] = h[n])
h[n] =
N−1
X
k=0
bkδ[n −k]
giving h[n] = bn for n = 0, . . . , N −1, and accordingly, we can write Equation (9.22) as
y[n] =
N−1
X
k=0
h[k]x[n −k]
which is the convolution of the input x[n] and the impulse response h[n] of the FIR ﬁlter. Thus, if X(z) =
Z(x[n]) and H(z) = Z(h[n]), then
Y(z) = H(z)X(z)
and
y[n] = Z−1[Y(z)]
I
The length of the convolution of two sequences of lengths M and N is M + N −1.
I
If one of the sequences is of inﬁnite length, the length of the convolution is inﬁnite. Thus, for an inﬁnite-
impulse response (IIR) or recursive ﬁlters the output is always of inﬁnite length for any input signal, given
that the impulse response of these ﬁlters is of inﬁnite length.

9.4 One-Sided Z-Transform
529
I Example 9.7
Consider computing the output of an FIR ﬁlter,
y[n] = 1
2
 x[n] + x[n −1] + x[n −2]

for an input x[n] = u[n] −u[n −4] using the convolution sum, analytically and graphically, and
the Z-transform.
Solution
The impulse response is h[n] = 0.5(δ[n] + δ[n −1] + δ[n −2]), so that h[0], h[1], h[2] are,
respectively, 0.5, 0.5, and 0.5, and h[n] is zero otherwise.
Convolution sum formula: The equation
y[n] =
n
X
k=0
h[k]x[n −k]
= x[0]h[n] + x[1]h[n −1] + · · · + x[n]h[0]
n ≥0
with the condition that in each entry the arguments of x[.] and h[.] add to n ≥0, gives
y[0] = x[0]h[0] = 0.5
y[1] = x[0]h[1] + x[1]h[0] = 1
y[2] = x[0]h[2] + x[1]h[1] + x[2]h[0] = 1.5
y[3] = x[0]h[3] + x[1]h[2] + x[2]h[1] + x[3]h[0] = x[1]h[2] + x[2]h[1] + x[3]h[0] = 1.5
y[4] = x[0]h[4] + x[1]h[3] + x[2]h[2] + x[3]h[1] + x[4]h[0] = x[2]h[2] + x[3]h[1] = 1
y[5] = x[0]h[5] + x[1]h[4] + x[2]h[3] + x[3]h[2] + x[4]h[1] + x[5]h[0] = x[3]h[2] = 0.5
and the rest are zero. In the above computations, we notice that the length of y[n] is 4 + 3 −1 =
6 since the length of x[n] is 4 and that of h[n] is 3.
Graphical approach: The convolution sum is given by either
y[n] =
n
X
k=0
x[k]h[n −k]
=
n
X
k=0
h[k]x[n −k]
Choosing one of these equations, let’s say the ﬁrst one, we need x[k] and h[n −k], as functions
of k, for different values of n. Multiply them and then add the nonzero values. For instance, for
n = 0 the sequence h[−k] is the reﬂection of h[k]; multiplying x[k] by h[−k] gives only one value
different from zero at k = 0, or y[0] = 1/2. For n = 1, the sequence h[1 −k], as a function of k,

530
CHAPTER 9:
The Z-Transform
x[k]
h[−1 −k]
h[2−k]
h[5 −k]
x[k]
x[k]
1
2
3
0
4
5
k
k
k
−3
−2
−1
−3
−2
−1
−3
−2
−1
0
0
1
1
2
2
3
3
4
4
5
5
n =−1
n = 2
n = 5
y[−1]= 0
y[2] = x[0]h[2] +x[1]h[1]+x[2]h[0]
y[5] = x[3]h[2]
(a)
(b)
(c)
FIGURE 9.4
Graphical approach: convolution sum for (a) n = −1, (b) n = 2, and (c) n = 5 with corresponding outputs y[−1],
y[2], and y[5]. Both x[k] and h[n −k] are plotted as functions of k for a given value of n. The signal x[k] remains
stationary, while h[n −k] moves linearly from left to right. Thus, the convolution sum is also called a linear
convolution.
is h[−k] shifted to the right one sample. Multiplying x[k] by h[1 −k] gives two values different
from zero, which when added gives y[1] = 1, and so on. For increasing values of n we shift to
the right one sample to get h[n −k], multiply it by x[k], and then add the nonzero values to
obtain the output y[n]. Figure 9.4 displays the graphical computation of the convolution sum
for n = −1, n = 2 and n = 5.
Convolution sum property: We have
X(z) = 1 + z−1 + z−2 + z−3
H(z) = 1
2

1 + z−1 + z−2

9.4 One-Sided Z-Transform
531
−2
0
2
n
x[n]
h[n]
y[n]
n
n
4
6
(a)
(b)
(c)
8
0
0.5
1
−2
0
2
4
6
8
0
0.5
1
−2
0
2
4
6
8
0
0.5
1
1.5
FIGURE 9.5
Convolution sum for an averager FIR: (a) x[n], (b) h[n], and (c) y[n]. The output y[n] is of length 6 given that x[n]
is of length 4 and h[n] is the impulse response of a second-order FIR ﬁlter of length 3.
and according to the convolution sum property,
Y(z) = X(z)H(z) = 1
2(1 + 2z−1 + 3z−2 + 3z−3 + 2z−4 + z−5)
Thus, y[0] = 0.5, y[1] = 1, y[2] = 1.5, y[3] = 1.5, y[4] = 1, and y[5] = 0.5, just as before.
In MATLAB the function conv is used to compute the convolution sum giving the results shown
in Figure 9.5, which coincide with the ones obtained in the other approaches.
I
I Example 9.8
Consider an FIR ﬁlter with impulse response
h[n] = δ[n] + δ[n −1] + δ[n −2]
Find the ﬁlter output for an input x[n] = cos(2πn/3)(u[n] −u[n −14]). Use the convolution sum
to ﬁnd the output, and verify your results with MATLAB.

532
CHAPTER 9:
The Z-Transform
Solution
Graphical approach: Let us use the formula
y[n] =
n
X
k=0
x[k]h[n −k]
which keeps the more complicated signal x[k] as the unchanged signal. The term h[n −k] is
h[k] reversed for n = 0, and then shifted to the right for n ≥1. The output is zero for negative
values of n, and for n ≥0 we have
y[0] = 1
y[1] = 0.5
y[n] = 0
2 ≤n ≤13
y[14] = 0.5
y[15] = −0.5
The ﬁrst value is obtained by reﬂecting the impulse response to get h[−k], and when multiplied
by x[k] we only have the value at k = 0 different from zero, therefore y[0] = x[0]h[0] = 1. As
we shift the impulse response to the right to get h[1 −k] for n = 1 and multiply it by x[k], we
get two values different from zero; when added they equal 0.5. The result for 2 ≤n ≤13 is zero
because we add three values of −0.5, 1 and −0.5 from the cosine. These results are veriﬁed by
MATLAB as shown in Figure 9.6. (the cosine does not look like a sampled cosine given that
only three values are used per period).
Convolution property approach: By the convolution property, the Z-transform of the output y[n] is
Y(z) = X(z)H(z) = X(z)(1 + z−1 + z−2) = X(z) + X(z)z−1 + X(z)z−2
The coefﬁcients of Y(z) can be obtained by adding the coefﬁcients of X(z), X(z)z−1, and X(z)z−2:
z0
z−1
z−2
z−3
z−4
z−5
z−6
z−7
z−8
z−9
z−10
z−11 z−12 z−13 z−14 z−15
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5 −0.5
1 −0.5
Adding these coefﬁcients vertically, we obtain
Y(z) = 1 + 0.5z−1 + 0z−2 + · · · + 0z−13 + 0.5z−14 −0.5z−15
= 1 + 0.5z−1 + 0.5z−14 −0.5z−15
Notice from this example that
I
The convolution sum is simply calculating the coefﬁcients of the polynomial product
X(z)H(z).

9.4 One-Sided Z-Transform
533
0
5
10
15
−1
−0.5
0
0.5
1
1.5
n
x[n]
0
5
10
15
0
0.5
1
n
h[n]
−2
0
2
4
6
8
10
12
14
16
18
−1
−0.5
0
0.5
1
n
y[n]
(a)
(b)
(c)
FIGURE 9.6
Convolution sum for FIR ﬁlter: (a) x[n], (b) h[n], and (c) y[n].
I
The length of the convolution sum = length of x[n] + length of h[n] −1 = 14 + 3 −1 =
16—that is, Y(z) is a polynomial of order 15.
I
I Example 9.9
The convolution sum of noncausal signals is more complicated graphically than that of the causal
signals we showed in the previous examples. Let
h1[n] = 1
3
 δ[n + 1] + δ[n] + δ[n −1]

be the impulse response of a noncausal averager FIR ﬁlter, and x[n] = u[n] −u[n −4] be the input.
Compute the ﬁlter output using the convolution sum.
Solution
Graphically, it is a bit confusing to plot h1[n −k], as a function of k, to do the convolution
sum. Using the convolution and the time-shifting properties of the Z-transform we can view the
computation more clearly.

534
CHAPTER 9:
The Z-Transform
According to the convolution property the Z-transform of the output of the noncausal
ﬁlter is
Y1(z) = X(z)H1(z)
= X(z)[zH(z)]
(9.23)
where we let
H1(z) = Z[h1[n]] = 1
3(z + 1 + z−1)
= z
1
3(1 + z−1 + z−2)

= zH(z)
where H(z) = (1/3)Z[δ[n] + δ[n −1] + δ[n −2]] is the transfer function of a causal ﬁlter. Let
Y(z) = X(z)H(z) be the Z-transform of the convolution sum y[n] = [x ∗h][n] of x[n] and h[n], both
of which are causal and can be computed as before.
According to Equation (9.23), we then have Y1(z) = zY(z) or y1[n] = [x ∗h1][n] = y[n + 1].
I
Let x1[n] be the input to a noncausal LTI system, with an impulse response h1[n] such that h1[n] = 0 for
n < N1 < 0. Assume x1[n] is also noncausal (i.e., x1[n] = 0 for n < N0 < 0). The output y1[n] = [x1 ∗h1][n]
has a Z-transform of
Y1(z) = X1(z)H1(z) = [zN0X(z)][zN1H(z)]
where X(z) and H(z) are the Z-transforms of a causal signal x[n] and of a causal impulse response h[n]. If we
let
y[n] = [x ∗h][n] = Z−1[X(z)H(z)]
then
y1[n] = [x1 ∗h1][n] = y[n + N0 + N1]
Remarks
I
The impulse response of an IIR system, represented by a difference equation, is found by setting the initial
conditions to zero, therefore, the transfer function H(z) also requires a similar condition. If the initial
conditions are not zero, the Z-transform of the total response Y(z) is the sum of the Z-transforms of the
zero-state and the zero-input responses—that is, its Z-transform is of the form
Y(z) = X(z)B(z)
A(z)
+ I0(z)
A(z)
(9.24)
and it does not permit us to compute the ratio Y(z)/X(z) unless the component due to the initial conditions
is I0(z) = 0.

9.4 One-Sided Z-Transform
535
I
It is important to remember the relations
H(z) = Z[h[n]] = Y(z)
X(z) = Z[y[n]]
Z[x[n]]
where H(z) is the transfer function and h[n] is the impulse response of the system, with x[n] as the input
and y[n] as the output.
I Example 9.10
Consider a discrete-time IIR system represented by the difference equation
y[n] = 0.5y[n −1] + x[n]
(9.25)
with x[n] as the input and y[n] as the output. Determine the transfer function of the system and
from it ﬁnd the impulse and the unit-step responses. Determine under what conditions the system
is BIBO stable. If stable, determine the transient and steady-state responses of the system.
Solution
The system transfer function is given by
H(z) = Y(z)
X(z) =
1
1 −0.5z−1
and its impulse response is
h[n] = Z−1[H(z)] = 0.5nu[n]
The response of the system to any input can be easily obtained by the transfer function. If the input
is x[n] = u[n], we have
Y(z) = H(z)X(z) =
1
(1 −0.5z−1)(1 −z−1)
=
−1
1 −0.5z−1 +
2
1 −z−1
so that the total solution is
y[n] = −0.5nu[n] + 2u[n]
From the transfer function H(z) of the LTI system, we can test the stability of the system by ﬁnding
the location of its poles—very much like in the analog case. An LTI system is BIBO stable if and
only if the impulse response of the system is absolutely summable—that is,
X
n
|h[n]| ≤∞

536
CHAPTER 9:
The Z-Transform
An equivalent condition is that the poles of H(z) are inside the unit circle. In this case, h[n] is
absolutely summable, indeed
∞
X
n=0
0.5n =
1
1 −0.5 = 2
On the other hand,
H(z) =
1
1 −0.5z−1 =
z
z −0.5
has a pole at z = 0.5, inside the unit circle. Thus, the system is BIBO stable. As such, its transient
and steady-state responses exist. As n →∞, y[n] = 2 is the steady-state response, and −0.5nu[n] is
the transient solution.
I
I Example 9.11
An FIR system has the input–output equation
y[n] = 1
3[x[n] + x[n −1] + x[n −2]]
where x[n] is the input and y[n] is the output. Determine the transfer function and the impulse
response of the system, and from them indicate whether the system is BIBO stable or not.
Solution
The transfer function is
H(z) = 1
3[1 + z−1 + z−2]
= z2 + z + 1
3z2
and the corresponding impulse response is
h[n] = 1
3[δ[n] + δ[n −1] + δ[n −2]]
The impulse response of this system only has three nonzero values, h[0] = h[1] = h[2] = 1/3, and
the rest of the values are zero. As such, h[n] is absolutely summable and the ﬁlter is BIBO stable.
FIR ﬁlters are always BIBO stable given their impulse responses will be absolutely summable, due
to their ﬁnal support, and equivalently because the poles of the transfer function of these system
are at the origin of the z-plane, very much inside the unit circle.
I
Nonrecursive or FIR systems: The impulse response h[n] of an FIR or nonrecursive system
y[n] = b0x[n] + b1x[n −1] + · · · + bMx[n −M]

9.4 One-Sided Z-Transform
537
has ﬁnite length and is given by
h[n] = b0δ[n] + b1δ[n −1] + · · · + bMδ[n −M]
Its transfer function is
H(z) = Y(z)
X(z)
= b0 + b1z−1 + · · · + bMz−M
= b0zM + b1zM−1 + · · · + bM
zM
with all its poles at the origin z = 0 (multiplicity M), and as such the system is BIBO stable.
Recursive or IIR systems: The impulse response h[n] of an IIR or recursive system
y[n] = −
N
X
k=1
aky[n −k] +
M
X
m=0
bmx[n −m]
has (possible) inﬁnite length and is given by
h[n] = Z−1[H(z)]
= Z−1
" PM
m=0 bmz−m
1 + PN
k=1 akz−k
#
= Z−1
B(z)
A(z)

=
∞
X
ℓ=0
h[ℓ]δ[n −ℓ]
where H(z) is the transfer function of the system. If the poles of H(z) are inside the unit circle, or
A(z) ̸= 0 for |z| ≥1, the system is BIBO stable.
9.4.4 Interconnection of Discrete-Time Systems
Just like with analog systems, two discrete-time LTI systems with transfer functions H1(z) and H2(z)
(or with impulse responses h1[n] and h2[n]) can be connected in cascade, parallel, or feedback. The
ﬁrst two forms result from properties of the convolution sum.
The transfer function of the cascading of the two LTI systems is
H(z) = H1(z)H2(z) = H2(z)H1(z)
(9.26)

538
CHAPTER 9:
The Z-Transform
=
=
H1(z)
H2(z)
x[n]
y[n]
H1(z)H2(z)
x[n]
y[n]
H1(z)
x[n]
y[n]
H2(z)
x[n]
y[n]
H1(z) +H2(z)
H1(z)
H2(z)
+
y[n]
x[n]
=
(a)
(b)
(c)
=
x[n]
y[n]
H1(z)
H2(z)
x[n]
y[n]
+
−
e[n]
w[n]
H1(z)
1+ H1(z)H2(z)
FIGURE 9.7
Connections of LTI systems: (a) cascade, (b) parallel, and (c) negative feedback.
showing that there is no effect on the overall system if we interchange the two systems (see
Figure 9.7(a)). Recall that such a property is only valid for LTI systems. In the parallel system, as
in Figure 9.7(b), both systems have the same input and the output is the sum of the output of the
subsystems. The overall transfer function is
H(z) = H1(z) + H2(z)
(9.27)
Finally, the negative feedback connection of the two systems shown in Figure 9.7(c) gives in the
feedforward path
Y(z) = H1(z)E(z)
(9.28)
where Y(z) = Z[y[n]] is the Z-transform of the output y[n] and E(z) = X(z) −W(z) is the
Z-transform of the error function e[n] = x[n] −w[n]. The feedback path gives that
W(z) = Z[w[n]] = H2(z)Y(z)
Replacing W(z) in E(z), and then replacing E(z) in Equation (9.28), we obtain the overall transfer
function
H(z) = Y(z)
X(z) =
H1(z)
1 + H1(z)H2(z)
(9.29)

9.4 One-Sided Z-Transform
539
9.4.5 Initial and Final Value Properties
In some control applications and to check a partial fraction expansion, it is useful to ﬁnd the initial
or the ﬁnal value of a discrete-time signal x[n] from its Z-transform. These values can be found as
shown in the following box.
If X(z) is the Z-transform of a causal signal x[n], then
Initial value:
x[0] = lim
z→∞X(z)
Final value:
lim
n→∞x[n] = lim
z→1(z −1)X(z)
(9.30)
The initial value results from the deﬁnition of the one-sided Z-transform—that is,
lim
z→∞X(z) = lim
z→∞

x[0] +
X
n≥1
x[n]
zn

= x[0]
To show the ﬁnal value, we have that
(z −1)X(z) =
∞
X
n=0
x[n]z−n+1 −
∞
X
n=0
x[n]z−n
= x[0]z +
∞
X
n=0
[x[n + 1] −x[n]]z−n
and thus the limit
lim
z→1(z −1)X(z) = x[0] +
∞
X
n=0
(x[n + 1] −x[n])
= x[0] + (x[1] −x[0]) + (x[2] −x[1]) + (x[3] −x[2]) · · ·
= lim
n→∞x[n]
given that the entries in the sum cancel out as n increases, leaving x[∞].
I Example 9.12
Consider a negative-feedback connection of a plant with a transfer function G(z) = 1/(1 −0.5z−1)
and a constant feedback gain K (see Figure 9.8). If the reference signal is a unit step, x[n] = u[n],
determine the behavior of the error signal e[n]. What is the effect of the feedback, from the error
point of view, on an unstable plant G(z) = 1/(1 −z−1)?

540
CHAPTER 9:
The Z-Transform
FIGURE 9.8
Negative-feedback system with plant G(z).
+
−
x[n]
y[n]
e[n]
K
G(z)
w[n]
Solution
For G(z) = 1/(1 −0.5z−1), the Z-transform of the error signal is
E(z) = X(z) −W(z) = X(z) −KG(z)E(z)
and for X(z) = 1/(1 −z−1),
E(z) =
X(z)
1 + KG(z) =
1
(1 −z−1)(1 + KG(z))
The initial value of the error signal is then
e[0] = lim
z→∞E(z) =
1
1 + K
since G(∞) = 1.
The steady-state or ﬁnal value of the error is
lim
n→∞e[n] = lim
z→1(z −1)E(z) = lim
z→1
(z −1)X(z)
1 + KG(z)
= lim
z→1
z(z −1)
(z −1)(1 + KG(z)) =
1
1 + 2K
since G(1) = 2. If we want the steady-state error to go to zero, then K must be large. In that case,
the initial error is also zero.
If G(z) = 1/(1 −z−1) (i.e., the plant is unstable), the initial value of the error function remains the
same, e[0] = 1/(1 + K), but the steady state error goes to zero since G(1) →∞.
I
Tables 9.1 and 9.2 provide a list of one-side Z-transforms and the basic properties of the one-sided
Z-transform.

9.4 One-Sided Z-Transform
541
Table 9.1 One-Sided Z-Transforms
Function of Time
Function of z, ROC
1.
δ[n]
1, whole z-plane
2.
u[n]
1
1 −z−1 , |z| > 1
3.
nu[n]
z−1
(1 −z−1)2 , |z| > 1
4.
n2u[n]
z−1(1 + z−1)
(1 −z−1)3 , |z| > 1
5.
αnu[n], |α| < 1
1
1 −αz−1 , |z| > |α|
6.
nαnu[n], |α| < 1
αz−1
(1 −αz−1)2 , |z| > |α|
7.
cos(ω0n)u[n]
1 −cos(ω0)z−1
1 −2 cos(ω0)z−1 + z−2 , |z| > 1
8.
sin(ω0n)u[n]
sin(ω0)z−1
1 −2 cos(ω0)z−1 + z−2 , |z| > 1
9.
αn cos(ω0n)u[n], |α| < 1
1 −α cos(ω0)z−1
1 −2α cos(ω0)z−1 + z−2 , |z| > 1
10.
αn sin(ω0n)u[n], |α| < 1
α sin(ω0)z−1
1 −2α cos(ω0)z−1 + z−2 , |z| > |α|
Table 9.2 Basic Properties of One-Sided Z-Transform
Causal signals and constants
αx[n], βy[n]
αX(z), βY(z)
Linearity
αx[n] + βy[n]
αX(z) + βY(z)
Convolution sum
(x ∗y)[n] = P
k x[n]y[n −k]
X(z)Y(z)
Time shifting—causal
x[n −N] N integer
z−NX(z)
Time shifting—noncausal
x[n −N]
z−NX(z) + x[−1]z−N+1
x[n] noncausal, N integer
+ x[−2]z−N+2 + · · · + x[−N]
Time reversal
x[−n]
X(z−1)
Multiplication by n
n x[n]
−zdX(z)
dz
Multiplication by n2
n2 x[n]
z2 d2X(z)
dz2
+ zdX(z)
dz
Finite difference
x[n] −x[n −1]
(1 −z−1)X(z) −x[−1]
Accumulation
Pn
k=0 x[k]
X(z)
1 −z−1
Initial value
x[0]
lim
z→∞X(z)
Final value
lim
n→∞x[n]
lim
z→1(z −1)X(z)

542
CHAPTER 9:
The Z-Transform
9.5 ONE-SIDED Z-TRANSFORM INVERSE
Different from the inverse Laplace transform, which was done mostly by the partial fraction expan-
sion, the inverse Z-transform can be done in different ways. For instance, if the Z-transform is given
as a ﬁnite-order polynomial, the inverse can be found by inspection. Indeed, if the given Z-transform
is
X(z) =
N
X
n=0
x[n]z−n
= x[0] + x[1]z−1 + x[2]z−2 + · · · + x[N]z−N
(9.31)
by the deﬁnition of the Z-transform, x[k] is the coefﬁcient of the monomial z−k for k = 0, 1, . . . , N;
thus the inverse Z-transform is given by the sequence {x[0], x[1], . . . , x[n]}. For instance, if we have a
Z-transform
X(z) = 1 + 2z−10 + 3z−20
the inverse is a sequence
x[n] = δ[n] + 2δ[n −10] + 3δ[n −20]
so that x[0] = 1, x[10] = 2, x[20] = 3, and x[n] = 0 for n ̸= 0, 10, 20, respectively. In this case it
makes sense to do this because N is ﬁnite, but if N →∞, this way of ﬁnding the inverse Z-transform
might not be very practical. In that case, the long-division method and the partial fraction expan-
sion method, which we consider next, are more appropriate. In this section we will consider the
inverse of one-sided Z-transforms, and in the next section we consider the inverse of two-sided
transforms.
9.5.1 Long-Division Method
When a rational function X(z) = B(z)/A(z), having as ROC the outside of a circle of radius R (i.e., x[n] is causal),
is expressed as
X(z) = x[0] + x[1]z−1 + x[2]z−2 + · · ·
then the inverse is the sequence {x[0], x[1], x[2], . . .}, or
x[n] = x[0]δ[n] + x[1]δ[n −1] + x[2]δ[n −2] + · · ·
To ﬁnd the inverse we simply divide the polynomial B(z) by A(z) to obtain a possible inﬁnite-order
polynomial in negative powers of z−1. The coefﬁcients of this polynomial are the inverse values. The
disadvantage of this method is that it does not provide a closed-form solution, unless there is a clear
connection between the terms of the sequence. But this method is useful when we are interested in
ﬁnding some of the initial values of the sequence x[n].

9.5 One-Sided Z-Transform Inverse
543
I Example 9.13
Find the inverse Z-transform of
X(z) =
1
1 + 2z−2
|z| >
√
2
Solution
We can perform the long division to ﬁnd the x[n] values, or equivalently let
X(z) = x[0] + x[1]z−1 + x[2]z−2 + · · ·
and ﬁnd the {x[n]} samples so that the product X(z)(1 + 2z−2) = 1. Thus,
1 = (1 + 2z−2)(x[0] + x[1]z−1 + x[2]z−2 + · · · )
= x[0] + x[1]z−1 + x[2]z−2 + x[3]z−3 + · · ·
+ 2x[0]z−2 + 2x[1]z−3 + · · ·
and comparing the terms on the two sides of the equality gives
x[0] = 1
x[1] = 0
x[2] + 2x[0] = 0 ⇒x[2] = −2
x[3] + 2x[1] = 0 ⇒x[3] = 0
x[4] + 2x[2] = 0 ⇒x[4] = (−2)2
...
So the inverse Z-transform is x[0] = 1 and x[n] = (−2)log2(n) for n > 0 and even, and zero
otherwise. Notice that this sequence grows as n →∞.
Another possible way to ﬁnd the inverse is to use the geometric series equation
∞
X
k=0
αn =
1
1 −α |α| < 1
with −α = 2z−2 (notice that |α| = 2/|z|2 < 1 or |z| >
√
2, the given ROC). Therefore,
X(z) =
1
1 + 2z−2 = 1 + (−2z−2)1 + (−2z−2)2 + (−2z−2)3 + · · ·
but this method is not as general as the long division.
I

544
CHAPTER 9:
The Z-Transform
9.5.2 Partial Fraction Expansion
The basics of partial fraction expansion remain the same for the Z-transform as for the Laplace
transform. A rational function is a ratio of polynomials N(z) and D(z) in z or z−1:
X(z) = N(z)
D(z)
The poles of X(z) are the roots of D(z) = 0 and the zeros of X(z) are the roots of the equation N(z) = 0.
Remarks
I
The basic characteristic of the partial fraction expansion is that X(z) must be a proper rational function,
or that the degree of the numerator polynomial N(z) must be smaller than the degree of the denominator
polynomial D(z) (assuming both N(z) and D(z) are polynomials in either z−1 or z). If this condition is
not satisﬁed, we perform long division until the residue polynomial is of a degree less than that of the
denominator.
I
It is more common in the Z-transform than in the Laplace transform to ﬁnd that the numerator and the
denominator are of the same degree—this is because δ[n] is not as unusual as the analog impulse function
δ(t).
I
The partial fraction expansion is generated, from the poles of the proper rational function, as a sum of
terms of which the inverse Z-transforms are easily found in a Z-transform table. By plotting the poles and
the zeros of a proper X(z), the location of the poles provides a general form of the inverse within some
constants that are found from the poles and the zeros.
I
Given that the numerator and the denominator polynomials of a proper rational function X(z) can be
expressed in terms of positive or negative powers of z, it is possible to do partial fraction expansions in
either z or z−1. We will see that the partial fraction expansion in negative powers is more like the partial
fraction expansion in the Laplace transform, and as such we will prefer it. Partial fraction expansion in
positive powers of z requires more care.
I Example 9.14
Consider the nonproper rational function
X(z) =
2 + z−2
1 + 2z−1 + z−2
(the numerator and the denominator are of the same degree in powers of z−1). Determine how to
obtain an expansion of X(z) containing a proper rational term to ﬁnd x[n].
Solution
By division we obtain
X(z) = 1 +
1 −2z−1
1 + 2z−1 + z−2

9.5 One-Sided Z-Transform Inverse
545
where the second term is proper rational as the denominator is of a higher degree in powers of z−1
than the numerator. The inverse Z-transform of X(z) will then be
x[n] = δ[n] + Z−1

1 −2z−1
1 + 2z−1 + z−2

The inverse of the proper rational term is done as indicated in this section.
I
I Example 9.15
Find the inverse Z-transform of
X(z) =
1 + z−1
(1 + 0.5z−1)(1 −0.5z−1)
=
z(z + 1)
(z + 0.5)(z −0.5)
|z| > 0.5
by using the negative and the positive powers of z expressions.
Solution
Clearly, X(z) is proper if it is considered a function of negative powers z−1 (in z−1, the numerator
is of degree 1 and the denominator of degree 2), but it is not proper if it is considered a function
of positive powers z (the numerator and the denominator are both of degree 2). It is, however,
unnecessary to perform long division to make X(z) proper when it is considered as a function of
z. One simple approach is to consider X(z)/z as the function. We wish to ﬁnd its partial fraction
expansion—that is,
X(z)
z
=
z + 1
(z + 0.5)(z −0.5)
(9.32)
which is proper. Thus, whenever X(z), as a function of z terms, is not proper it is always possible to
divide it by some power in z to make it proper. After obtaining the partial fraction expansion then
the z term is put back.
Consider then the partial fraction expansion in z−1 terms,
X(z) =
1 + z−1
(1 + 0.5z−1)(1 −0.5z−1)
=
A
1 + 0.5z−1 +
B
1 −0.5z−1
Given that the poles are real—one at z = −0.5 and the other at z = 0.5—from the
Z-transform table, we get that a general form of the inverse is
x[n] = [A (−0.5)n + B 0.5n]u[n]

546
CHAPTER 9:
The Z-Transform
The A and B coefﬁcients can be found (by analogy with the Laplace transform partial fraction
expansion) as
A = X(z)(1 + 0.5z−1)|z−1=−2 = −0.5
B = X(z)(1 −0.5z−1)|z−1=2 = 1.5
so that
x[n] = [−0.5(−0.5)n + 1.5(0.5)n]u[n]
Consider then the partial fraction expansion in positive powers of z. From Equation (9.32) the
proper rational function X(z)/z can be expanded as
X(z)
z
=
z + 1
(z + 0.5)(z −0.5)
=
C
z + 0.5 +
D
z −0.5
The values of C and D are obtained as follows:
C = X(z)
z
(z + 0.5)|z=−0.5 = −0.5
D = X(z)
z
(z −0.5)|z=0.5 = 1.5
We then have that
X(z) = −0.5z
z + 0.5 +
1.5z
z −0.5
which according to the table (if entries are in negative powers of z, convert them into positive
powers of z) we get
x[n] = [−0.5(−0.5)n + 1.5(0.5)n]u[n]
which coincides with the above result.
Two simple checks on our result are given by the initial and the ﬁnal value results. For the initial
value,
x[0] = 1 = lim
z→∞X(z)
and
lim
n→∞x[n] = lim
z→1(z −1)X(z) = 0
Both of these check. It is important to recognize that these two checks do not guarantee that we
did not make mistakes in computing the inverse, but if the initial or the ﬁnal values were not to
coincide with our results, our inverse would be wrong.
I

9.5 One-Sided Z-Transform Inverse
547
9.5.3 Inverse Z-Transform with MATLAB
Symbolic MATLAB can be used to compute the inverse one-sided Z-transform. The function iztrans
provides the sequence that corresponds to its argument. The following script illustrates the use of this
function.
%%%%%%%%%%%%%%%%%%
% Inverse Z-transform
%%%%%%%%%%%%%%%%%%
syms n z
x1 = iztrans((z ∗(z + 1))/((z + 0.5) ∗(z - 0.5)))
x2 = iztrans((2 - z)/(2 ∗(z - 0.5)))
x3 = iztrans((8 - 4 ∗z ˆ (-1))/(z ˆ (-2) + 6 ∗z ˆ (-1) + 8))
The above gives the following results:
x1 = 3/2 ∗(1/2) ˆ n - 1/2 ∗(-1/2) ˆ n
x2 = -2 ∗charfcn[0](n) + 3/2 ∗(1/2) ˆ n
x3 = -3 ∗(-1/4) ˆ n + 4 ∗(-1/2) ˆ n
Notice that the Z-transform can be given in positive or negative powers of z, and that when it is
nonproper the function charfcn[0] corresponds to δ[n].
Partial Fraction Expansion with MATLAB
Several numerical functions are available in MATLAB to perform partial fraction expansion of a
Z-transform and to obtain the corresponding inverse. In the following we consider the cases of single
and multiple poles.
(1) Simple Poles
Consider ﬁnding the inverse Z-transform of
X(z) =
z(z + 1)
(z −0.5)(z + 0.5) =
(1 + z−1)
(1 −0.5z−1)(1 + 0.5z−1)
|z| > 0.5
The MATLAB function residuez provides the partial fraction expansion coefﬁcients or residues r[k],
the poles p[k], and the gain k corresponding to X(z) when the coefﬁcients of its denominator and
of its numerator are inputted. If the numerator or the denominator is given in a factored form (as
is the case of the denominator above) we need to multiply the terms to obtain the denominator
polynomial. Recall that multiplication of polynomials corresponds to convolution of the polynomial
coefﬁcients. Thus, to perform the multiplication of the terms in the denominator, we use the MATLAB
function conv to obtain the coefﬁcients of the product. The convolution of the coefﬁcients [1 −0.5] of
p1(z) = 1 −0.5z−1 and [1 0.5] of p2(z) = 1 + 0.5z−1 gives the denominator coefﬁcients. By means
of the MATLAB function poly we can obtain the polynomials in the numerator and denominator
from the zeros and poles. These polynomials are then multiplied as indicated before to obtain the
numerator with coefﬁcients {b[k]}, and the denominator with coefﬁcients {a[k]}.

548
CHAPTER 9:
The Z-Transform
To ﬁnd the poles and the zeros of X(z), given the coefﬁcients {b[k]} and {a[k]} of the numerator and
the denominator, we use the MATLAB function roots. To get a plot of the poles and the zeros of X(z),
the MATLAB function zplane, with inputs the coefﬁcients of the numerator and the denominator of
X(z), is used (conventionally, an ’x’ is used to denote poles and an ’o’ for zeros).
Two possible approaches can now be used to compute the inverse Z-transform x[n]. We can com-
pute the inverse (below we call it x1[n] to differentiate it from the other possible solution, which
we call x[n]) by using the information on the partial fraction expansion (the residues r[k]) and
the corresponding poles. An alternative is to use the MATLAB function ﬁlter, which considers X(z)
as a transfer function, with the numerator and the denominator deﬁned by the b and a vectors
of coefﬁcients. If we assume the input is a delta function of Z-transform unity, the function ﬁlter
computes as output the inverse Z-transform x[n] (i.e., we have tricked ﬁlter to give us the desired
result).
The following script is used to implement the generation of the terms in the numerator and the
denominator to obtain the corresponding coefﬁcients, plot them, and ﬁnd the inverse in the two
different ways indicated above. For additional help on the functions used here use help.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Two methods for inverse Z-transform
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
p1 = poly(0.5); p2 = poly(-0.5); % generation of terms in denominator
a = conv(p1,p2) % denominator coefﬁcients
z1 = poly(0); z2 = poly(-1); % generation of terms in numerator
b = conv(z1,z2) % numerator coefﬁcients
z = roots(b) % zeros of X(z)
[r,p,k] = residuez(b,a) % partial fraction expansion, poles and gain
zplane(b,a) % plot of poles and zeros
d = [1 zeros(1,99)]; % impulse delta[n]
x = ﬁlter(b,a,d); % x[n] computation from ﬁlter
n = 0:99;
x1 = r(1) ∗p(1). ˆ n + r(2) ∗p(2). ˆ n; % x[n] computation from residues
a = 1.0000
0
-0.2500
b = 1
1
0
z = 0
-1
r = 1.5000
-0.5000
p = 0.5000
-0.5000
Figure 9.9 displays the plot of the zeros and the poles and the comparison between the inverses x1[n]
and x[n] for 0 ≤n ≤99, which coincide sample by sample.

9.5 One-Sided Z-Transform Inverse
549
−1
−0.5
0
0.5
1
−1.5
−1
−0.5
0
0.5
1
1.5
Real part
Imaginary part
0
5
10
(a)
(b)
15
20
0
0.2
0.4
0.6
0.8
1
n
x[n], x1[n]
FIGURE 9.9
(a) Poles and zeros of X(z) and (b) inverse Z-transforms x[n] and x1[n] found using filter and the residues.
(2) Multiple Poles
Whenever multiple poles are present one has to be careful in interpreting the MATLAB results. First,
use help to get more information on residuez and how the partial fraction expansion for multiple
poles is done. Notice from the help ﬁle that the residues are ordered the same way the poles are.
Furthermore, the residues corresponding to the multiple poles are ordered from the lowest to the
highest order. Also notice the difference between the partial fraction expansion of MATLAB and ours.
For instance, consider the Z-transform
X(z) =
az−1
(1 −az−1)2
|z| > a
with inverse x[n] = nanu[n]. Writing the partial fraction expansion as MATLAB does gives
X(z) =
r1
1 −az−1 +
r2
(1 −az−1)2
r1 = −1, r2 = 1
(9.33)

550
CHAPTER 9:
The Z-Transform
where the second term is not found in the Z-transforms table. To write it so that each of the terms in
the expansion are in the Z-transforms table, we need to obtain values for A and B in the expansion
X(z) =
A
1 −az−1 +
Bz−1
(1 −az−1)2
(9.34)
so that Equations (9.33) and (9.34) are equal. We ﬁnd that A = r1 + r2, while B −Aa = −r1a or
B = ar2. With these values we ﬁnd the inverse to be
x[n] = [(r1 + r2)an + nr2an]u[n] = nanu[n]
as expected.
To illustrate the computation of the inverse Z-transform from the residues in the case of multiple
poles, consider the transfer function
X(z) =
0.5z−1
1 −0.5z−1 −0.25z−2 + 0.125z−3 .
The following script is used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Inverse Z-transform --- multiple poles
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
b = [0 0.5 0 0 ];
a = [1 -0.5 -0.25 0.125]
[r,p,k] = residuez(b,a)
% partial fraction expansion, poles and gain
zplane(b,a) % plot of poles and zeros
n = 0:99; xx = p(1). ˆ n; yy = xx. ∗n;
x1 = (r(1) + r(2)). ∗xx + r(2). ∗yy + r(3) ∗p(3). ˆ n; % inverse computation
The poles and the zeros and the inverse Z-transform are shown in Figure 9.10—there is a double pole
at 0.5. The residues and the corresponding poles are
r = -0.2500
0.5000
-0.2500
p = 0.5000
0.5000
-0.5000
Computationally, our method and MATLAB’s are comparable but the inverse transform in our
method is found directly from the table, while in the case of MATLAB’s you need to change the
expansion to get it into the forms found in the tables.
9.5.4 Solution of Difference Equations
In this section we will use the shifting in time property of the Z-transform in the solution of difference
equations with initial conditions. You will see that the partial fraction expansion used to ﬁnd the
inverse Z-transform is like the one used in the inverse Laplace transform.

9.5 One-Sided Z-Transform Inverse
551
−1
−0.5
0
(a)
(b)
0.5
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2
2
Real part
Imaginary part
0
2
4
6
8
10
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
n
x[n]
FIGURE 9.10
(a) Poles and zeros of X(z) and (b) inverse Z-transform x[n].
If x[n] has a one-sided Z-transform X(z), then x[n −N] has the following one-sided Z-transform:
Z[x[n −N]] = z−NX(z) + x[−1]z−N+1 + x[−2]z−N+2 + · · · + x[−N]
(9.35)
Indeed, we have that
Z(x[n −N]) =
∞
X
n=0
x[n −N]z−n =
∞
X
m=−N
x[m]z−(m+N)
= z−N
∞
X
m=0
x[m]z−m +
−1
X
m=−N
x[m]z−(m+N)
= z−NX(z) + x[−1]z−N+1 + x[−2]z−N+2 + · · · + x[−N]
where we ﬁrst let m = n −N and then separated the sum into two, one corresponding to the
Z-transform of x[n] multiplied by z−N (the delay on the signal) and a second sum that corresponds
to initial values {x[i], −N ≤i ≤−1}.

552
CHAPTER 9:
The Z-Transform
Remarks
I
If the signal is causal, so that {x[i], −N ≤i ≤−1} are all zero, we then have that Z(x[n −N]) =
z−NX(z), indicating that the operator z−1 is a delay operator. Thus, x[n −N] has been delayed N samples
and its Z-transform is then simply X(z) multiplied by z−N.
I
The shifting in time property is useful in the solution of difference equations, especially when it has
nonzero initial conditions as we will see next. On the other hand, if the initial conditions are zero, either
the one-sided or the two-sided Z-transforms could be used.
The analog of differential equations are difference equations, which result directly from the modeling
of a discrete system or from discretizing differential equations. The numerical solution of differen-
tial equations requires that these equations be converted into difference equations since computers
cannot perform integration. Many methods are used to solve differential equations with different
degrees of accuracy and sophistication. This is a topic of numerical analysis, outside the scope of this
text, and thus only simple methods are illustrated here.
I Example 9.16
A discrete-time IIR system is represented by a ﬁrst-order difference equation
y[n] = ay[n −1] + x[n]
n ≥0
(9.36)
where x[n] is the input of the system and y[n] is the output. Discuss how to solve it using recursive
methods and the Z-transform. Obtain a general form for the complete solution y[n] in terms of
the impulse response h[n] of the system.
For input x[n] = u[n] −u[n −11], zero initial conditions, and a = 0.8, use the MATLAB function
ﬁlter to ﬁnd y[n]. Plot the input and the output.
Solution
In the time domain a unique solution is obtained by using the recursion given by the difference
equation. We would need an initial condition to compute y[0], indeed
y[0] = ay[−1] + x[0]
and as x[0] is given, we need y[−1] as the initial condition. Given y[0], recursively we ﬁnd the rest
of the solution:
y[1] = ay[0] + x[1]
y[2] = ay[1] + x[2]
y[3] = ay[2] + x[3]
· · ·
where at each step the needed output values are given by the previous step of the recursion.
However, this solution is not in closed form.

9.5 One-Sided Z-Transform Inverse
553
To obtain a closed-form solution, we use the Z-transform. Taking the one-sided Z-transform of the
two sides of the equation, we get
Z(y[n]) = Z(ay[n −1]) + Z[x[n])
Y(z) = a(z−1Y(z) + y[−1]) + X(z)
Solving for Y(z) in the above equation, we obtain
Y(z) =
X(z)
1 −az−1 + ay[−1]
1 −az−1
(9.37)
where the ﬁrst term depends exclusively on the input and the second depends exclusively on the
initial condition. If the input x[n] and the initial condition y[−1] are given, we can then ﬁnd the
inverse Z-transform to obtain the complete solution y[n] of the form
y[n] = yzs[n] + yzi[n]
where the zero-state response yzs[n] is due exclusively to the input x[n] and zero initial conditions,
and the zero-input response yzi[n] is the response due to the initial condition y[−1] with zero
input.
In this simple case we can obtain the complete solution for any input x[n] and any initial condition
y[−1]. Indeed, by expressing 1/(1 −az−1) as its Z-transform sum—that is,
1
1 −az−1 =
∞
X
k=0
akz−k
Equation (9.37) becomes
Y(z) =
∞
X
k=0
X(z)akz−k + ay[−1]
∞
X
k=0
akz−k
= X(z) + aX(z)z−1 + a2X(z)z−2 + · · · + ay[−1](1 + az−1 + a2z−2 + · · · )
Using the time-shift property we then get the complete solution,
y[n] = x[n] + ax[n −1] + a2x[n −2] + · · · + y[−1]a(1 + aδ[n −1] + a2δ[n −2] + · · · )
=
∞
X
k=0
akx[n −k] + ay[−1]
∞
X
k=0
akδ[n −k]
(9.38)
for any input x[n], initial condition y[−1], and a.

554
CHAPTER 9:
The Z-Transform
To solve the difference equation with a = 0.8, x[n] = u[n] −u[n −11], and zero initial condition
y[−1] = 0, using MATLAB, we use the following script.
%%%%%%%%%%%%%%%%
% Example 9.16
%%%%%%%%%%%%%%%%
N = 100; n = 0:N - 1; x = [ones(1,10) zeros(1,N - 10)];
den=[1 -0.8]; num = [1 0];
y = ﬁlter(num, den,x)
The function ﬁlter requires that the initial conditions are zero. The results are shown in Figure 9.11.
Let us now ﬁnd the impulse response h[n] of the system. For that, let x[n] = δ[n] and y[−1] = 0,
then we have y[n] = h[n] or Y(z) = H(z). Thus, we have
H(z) =
1
1 −az−1
so that h[n] = anu[n]
0
5
10
15
20
25
30
0
1
2
3
4
5
x[n]
0
5
10
15
20
25
30
0
2
4
6
(a)
n
(b)
n
y[n]
FIGURE 9.11
(a) Solution of the ﬁrst-order difference equation with (b) input.

9.5 One-Sided Z-Transform Inverse
555
You can then see that the ﬁrst term in Equation (9.38) is a convolution sum, and the second is the
impulse response multiplied by ay[−1]—that is,
y[n] = yzs[n] + yzi[n]
=
∞
X
k=0
h[k]x[n −k] + ay[−1]h[n]
I
I Example 9.17
Consider a discrete-time system represented by a second-order difference equation with constant
coefﬁcients
y[n] −a1y[n −1] −a2y[n −2] = x[n] + b1x[n −1] + b2x[n −2]
n ≥0
where x[n] is the input, y[n] is the output, and the initial conditions are y[−1] and y[−2]. Use the
Z-transform to obtain the complete solution.
Solution
Applying the one-sided Z-transform to the two sides of the difference equation, we have
Z(y[n] −a1y[n −1] −a2y[n −2]) = Z(x[n] + b1x[n −1] + b2x[n −2])
Y(z) −a1(z−1Y(z) + y[−1]) −a2(z−2Y(z) + y[−1]z−1 + y[−2]) = X(z)(1 + b1z−1 + b2z−2)
where we used the linearity and the time-shift properties of the Z-transform. It was also assumed
that the input is causal, x[n] = 0 for n < 0, so that the Z-transforms of x[n −1] and x[n −2] are
simply z−1X(z) and z−2X(z). Rearranging the above equation, we have
Y(z)(1 −a1z−1 −a2z−2) = (y[−1](a1 + a2z−1) + a2y[−2]) + X(z)(1 + b1z−1 + b2z−2)
and solving for Y(z), we have
Y(z) = X(z)(1 + b1z−1 + b2z−2)
1 −a1z−1 −a2z−2
+ y[−1](a1 + a2z−1) + a2y[−2])
1 −a1z−1 −a2z−2
where again the ﬁrst term is the Z-transform of the zero-state response, due to the input only, and
the second term is the Z-transform of the zero-input response, which is due to the initial conditions
alone. The inverse Z-transform of Y(z) will give us the complete response.
I
Remarks As we saw in Chapter 8, if either the initial conditions are not zero or the input is not causal,
the system is not linear time invariant (LTI). However, the time-shift property allows us to ﬁnd the complete
response in that case. We can think of two inputs applied to the system: one due to the initial conditions
and the other due to the regular input. By using superposition, we obtain the zero-state and the zero-input
responses, which add to the total response.

556
CHAPTER 9:
The Z-Transform
Just as with the Laplace transform, the steady-state response of a difference equation
y[n] +
N
X
k=1
aky[n −k] =
M
X
m=0
bmx[n −m]
is due to simple poles of Y(z) on the unit circle. Simple or multiple poles inside the unit circle give a transient,
while multiple poles on the unit circle or poles outside the unit circle create an increasing response.
I Example 9.18
Solve the difference equation
y[n] = y[n −1] −0.25y[n −2] + x[n]
n ≥0
with zero initial conditions and x[n] = u[n].
Solution
The Z-transform of the terms of the difference equation gives
Y(z) =
X(z)
1 −z−1 + 0.25z−2
=
1
(1 −z−1)(1 −z−1 + 0.25z−2) =
z3
(z −1)(z2 −z + 0.25)
|z| > 1
Y(z) has three zeros at z = 0, a pole at z = 1, and a double pole at z = 0.5. The partial fraction
expansion of Y(z) is of the form
Y(z) =
A
1 −z−1 + B(1 −0.5z−1) + Cz−1
(1 −0.5z−1)2
(9.39)
where the terms of the expansion can be found in the Z-transforms table. Within some constants,
the complete response is
y[n] = Au[n] + [B(0.5)n + Cn(0.5)n] u[n]
The steady state is then yss[n] = A (corresponding to the pole on the unit circle z = 1) since the
other two terms, corresponding to the double pole z = 0.5 inside the unit circle, make up the
transient response. The value of A is obtained as
A = Y(z)(1 −z−1)
z−1=1 = 4
To ﬁnd the complete response y[n] we ﬁnd the constants in Equation (9.39). Notice in Equa-
tion (9.39) that the expansion term corresponding to the double pole z = 0.5 has as numerator a
ﬁrst-order polynomial, with constants B and C to be determined, to ensure that the term is proper

9.5 One-Sided Z-Transform Inverse
557
rational. That term equals
B(1 −0.5z−1) + Cz−1
(1 −0.5z−1)2
=
B
1 −0.5z−1 +
Cz−1
(1 −0.5z−1)2
which is very similar to the expansion for multiple poles in the inverse Laplace transform. Once
we ﬁnd the values of B and C, the inverse Z-transforms are obtained from the Z-transforms table. A
simple method to obtain the coefﬁcients B and C is to ﬁrst obtain C by multiplying the two sides
of Equation (9.39) by (1 −0.5z−1)2 to get
Y(z)(1 −0.5z−1)2 = B(1 −0.5z−1) + Cz−1
and then letting z−1 = 2 on both sides to ﬁnd that
C = Y(z)(1 −0.5z−1)2
z−1
z−1=2 = −0.5
The B value is then obtained by choosing a value for z−1 that is different from 1 or 0.5 to compute
Y(z). For instance, assume you choose z−1 = 0 and that you have found A and C, then
Y(z)|z−1=0 = A + B = 1
from which B = −3. The complete response is then
y[n] = 4u[n] −3(0.5)n −0.5n(0.5)nu[n]
I
I Example 9.19
Find the complete response of the difference equation
y[n] + y[n −1] −4y[n −2] −4y[n −3] = 3x[n]
n ≥0
y[−1] = 1
y[−2] = y[−3] = 0
x[n] = u[n]
Determine if the discrete-time system corresponding to this difference equation is BIBO stable or
not, and the effect this has in the steady-state response.
Solution
Using the time-shifting and linearity properties of the Z-transform, and replacing the initial
conditions, we get
Y(z)[1 + z−1 −4z−2 −4z−3] = 3X(z) + [−1 + 4z−1 + 4z−2]

558
CHAPTER 9:
The Z-Transform
Letting
A(z) = 1 + z−1 −4z−2 −4z−3 = (1 + z−1)(1 + 2z−1)(1 −2z−1)
we can write
Y(z) = 3X(z)
A(z) + −1 + 4z−1 + 4z−2
A(z)
|z| > 2
(9.40)
To determine whether the steady-state response exists or not let us ﬁrst consider the stability of
the system associated with the given difference equation. The transfer function H(z) of the system
is computed by letting the initial conditions be zero (this makes the second term on the right
of the above equation zero) so that we can get the ratio of the Z-transform of the output to the
Z-transform of the input. If we do that then
H(z) = Y(z)
X(z) =
3
A(z)
Since the poles of H(z) are the zeros of A(z), which are z = −1, z = −2, and z = 2, then the impulse
response h[n] = Z−1[H(z)] will not be absolutely summable, as required by the BIBO stability,
because the poles of H(z) are on and outside the unit circle. Indeed, a general form of the impulse
response is
h[n] = [C + D (2)n + E (−2)n]u[n]
where C, D, and E are constants that can be found by doing a partial fraction expansion of H(z).
Thus, h[n] will grow as n increases and it would not be absolutely summable—that is, the system
is not BIBO stable.
Since the system is unstable, we expect the total response to grow as n increases. Let us see how we
can justify this. The partial fraction expansion of Y(z), after replacing X(z) in Equation (9.40), is
given by
Y(z) =
2 + 5z−1 −4z−3
(1 −z−1)(1 + z−1)(1 + 2z−1)(1 −2z−1)
=
B1
1 −z−1 +
B2
1 + z−1 +
B3
1 + 2z−1 +
B4
1 −2z−1
B1 = Y(z)(1 −z−1)|z−1=1 = −1
2
B2 = Y(z)(1 + z−1)|z−1=−1 = −1
6
B3 = Y(z)(1 + 2z−1)|z−1=−1/2 = 0
B4 = Y(z)(1 −2z−1)|z−1=1/2 = 8
3

9.5 One-Sided Z-Transform Inverse
559
so that
y[n] =

−0.5 −1
6(−1)n + 8
32n

u[n]
which as expected will grow as n increases — there is no steady-state response.
In a problem like this the chance of making computational errors is large, so it is important to
ﬁgure out a way to partially check your answer. In this case we can check the value of y[0] using the
difference equation, which is y[0] = −y[−1] + 4y(−2) + 4y(−3) + 3 = −1 + 3 = 2, and compare
it with the one obtained using our solution, which gives y[0] = −3/6 −1/6 + 16/6 = 2. They
coincide. Another way to partially check your answer is to use the initial and the ﬁnal values
theorems.
I
Solution of Differential Equations
The solution of differential equations requires converting them into difference equations, which can
then be solved in a closed form by means of the Z-transform.
I Example 9.20
Consider an RLC circuit represented by the second-order differential equation
d2vc(t)
dt2
+ dvc(t)
dt
+ vc(t) = vs(t)
where the voltage across the capacitor vc(t) is the output and the source vs(t) = u(t) is the input.
Let the initial conditions be zero. Find the voltage across the capacitor vc(t).
Solution
The Laplace transform of the output is found from the differential equation as
Vc(s) =
Vs(s)
1 + s + s2
=
1
s(s2 + s + 1) =
1
s((s + 0.5)2 + 3/4)
where the ﬁnal equation is obtained after replacing Vs(s) = 1/s. The solution of the differential
equation is of the general form
vc(t) = [A + Be−0.5t cos(
√
3/2t + θ)]u(t)
for constants A, B, and θ. To convert the differential equation into a difference equation we
approximate the ﬁrst derivative as
dvc(t)
dt
≈vc(t) −vc(t −Ts)
Ts

560
CHAPTER 9:
The Z-Transform
and the second derivative as
d2vc(t)
dt2
= d dvc(t)
dt
dt
≈d(vc(t) −vc(t −Ts))/Ts)
dt
≈vc(t) −2vc(t −Ts) + vc(t −2Ts)
T2s
which when replaced in the differential equation, and computing the resulting equation for t =
nTs, gives
 1
T2s
+ 1
Ts
+ 1

vc(nTs) −
 2
T2s
+ 1
Ts

vc((n −1)Ts) +
 1
T2s

vc((n −2)Ts) = vs(nTs)
Although we know that we need to choose a very small value for Ts to get a good approx-
imation to the exact result, for simplicity let us ﬁrst set Ts = 1, so that the difference
equation is
3vc[n] −3vc[n −1] + vc[n −2] = vs[n]
n ≥0
For zero initial conditions and unit-step input, we can recursively compute this equation to get
vc[0] = 1/3
vc[n] = 1
n →∞
A closed-form solution can be obtained using the Z-transform, giving (assuming zero initial
conditions)
[3 −3z−1 + z−2]Vc(z) =
1
1 −z−1
so that
Vc(z) =
z3
(z −1)(3z2 −3z + 1)
from which we obtain that there is a triple zero at z = 0, and poles at 1 and −0.5 ± j
√
3/6. The
partial fraction expansion will be of the form
Vc(z) =
A
1 −z−1 +
B
1 + (0.5 + j
√
3/6)z−1 +
B∗
1 + (0.5 −j
√
3/6)z−1
Since the complex poles are inside the unit circle, the steady-state response is due to the input that
has a single pole at 1 (i.e., the steady state is limn→∞vc[n] = A = 1).
We ﬁrst use the symbolic MATLAB functions ilaplace and ezplot to ﬁnd the exact solution of the dif-
ferential equation. We then sample the input signal using a sampling period Ts = 0.1 sec and
use the approximations of the ﬁrst and second derivatives to obtain the difference equation,
which is computed using ﬁlter. The results are shown in Figure 9.12. The exact solution of the

9.5 One-Sided Z-Transform Inverse
561
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
1.2
t, nTs
y(t), y[n]
FIGURE 9.12
Solution of the differential equation d2vc(t)/dt2 + dvc(t)/dt + vc(t) = vs(t) (solid line). Solution of the difference
equation approximating the differential equation for Ts = 0.1 (dotted line). Exact and approximate solutions are
very close.
differential equation is well approximated by the solution of the difference equation obtained by
approximating the ﬁrst and second derivatives.
%%%%%%%%%%%%%%%%
% Example 9.20
%%%%%%%%%%%%%%%%
syms s
vc = ilaplace(1/(s ˆ 3 + s ˆ 2 + s)); % exact solution
ezplot(vc,[0,10]);grid; hold on % plotting of exact solution
Ts = 0.1; % sampling period
a1 = 1/Tsˆ2 + 1/Ts + 1;a2 = -2/Ts ˆ 2 - 1/Ts;a3 = 1/Ts ˆ 2; % coefﬁcients
a = [1 a2/a1 a3/a1];b = 1;
t = 0:Ts:10; N = length(t);
vs=ones(1,N); % input
vca = ﬁlter(b,a,vs);vca = vca/vca(N); % solution
I
9.5.5 Inverse of Two-Sided Z-Transforms
When ﬁnding the inverse of a two-sided Z-transform, or a noncausal discrete-time signal, it is impor-
tant to relate the poles to the causal and the anti-causal components. The region of convergence plays

562
CHAPTER 9:
The Z-Transform
a very important role in making this determination. Once this is done, the inverse is found by looking
for the causal and the anti-causal partial fraction expansion components in a Z-transforms table. The
coefﬁcients of the partial fraction expansion are calculated like those in the case of causal signals.
I Example 9.21
Consider ﬁnding the inverse Z-transform of
X(z) =
2z−1
(1 −z−1)(1 −2z−1)2
1 < |z| < 2
which corresponds to a noncausal signal.
Solution
The function X(z) has two zeros at z = 0, a pole at z = 1, and a double pole at z = 2. For the region
of convergence to be a torus of internal radius 1 and outer radius 2, we need to associate with the
pole at z = 1 the region of convergence
R1 : |z| > 1
corresponding to a causal signal, and with the pole at z = 2, we associate a region of convergence
R2 : |z| < 2
associated with an anti-causal signal. Thus, we have
1 < |z| < 2 = R1 ∩R2
The partial fraction expansion is then done so that
X(z) =
A
1 −z−1
|
{z
}
R1:|z|>1
+

B
1 −2z−1 +
Cz−1
(1 −2z−1)2

|
{z
}
R2:|z|<2
That is, the ﬁrst term has R1 as the region of convergence and the terms in the square brackets
have R2 as their region of convergence. The inverse of the ﬁrst term will be a causal signal, and the
inverse of the other two terms will be an anti-causal signal.
The coefﬁcients are found like in the case of causal signals. In this case, we have that
A = X(z)(1 −z−1)|z−1=1 = 2
C = X(z)(1 −2z−1)2
z−1
|z−1=0.5 = 4
To calculate B we compute X(z) and its expansion for a value of z−1 ̸= 1 or 0.5. For instance,
z−1 = 0 gives
X(0) = A + B = 0

9.5 One-Sided Z-Transform Inverse
563
so that B = −A = −2. The inverse is then found to be
x[n] = 2u[n]
| {z }
causal
+
h
−2(n+1)u[−n −1] + 2(n+2)nu[−n −1]
i
|
{z
}
anti-causal
I
I Example 9.22
Find all the possible impulse responses connected with the following transfer function of a discrete
ﬁlter having poles at z = 1 and z = 0.5:
H(z) =
1 + 2z−1 + z−2
(1 −0.5z−1)(1 −z−1)
determine the cases when the ﬁlter is BIBO stable.
Solution
As a function of z−1 this function is not proper since both its numerator and denominator are of
degree 2. After division we have the following partial fraction expansion:
H(z) = B0 +
B1
1 −0.5z−1 +
B2
1 −z−1
There are three possible regions of convergence that can be attached to H(z):
I
R1 : |z| > 1 so that the corresponding impulse response h1[n] = Z−1[H(z)] is causal with a
general form
h1[n] = B0δ[n] + [B1(0.5)n + B2]u[n]
The pole at z = 1 makes this ﬁlter unstable, as its impulse response is not absolutely summable.
I
R2 : |z| < 0.5 for which the corresponding impulse response h2[n] = Z−1[H(z)] is anti-causal
with a general form
h2[n] = B0δ[n] −(B1(0.5)n + B2)u[−n −1]
The region of convergence R2 does not include the unit circle and so the impulse response is
not absolutely summable (H(z) cannot be deﬁned at z = 1 because it is not in the region of
convergence R2). The impulse response h2[n] grows as n becomes smaller and negative.
I
R3 : 0.5 < |z| < 1, which gives a two-sided impulse response h2[n] = Z−1[H(z)] of general
form
h3[n] = B0δ[n] + B1((0.5)nu[n]
|
{z
}
causal
−B2u[−n −1]
|
{z
}
anti-causal
Again, this ﬁlter is not stable.
I

564
CHAPTER 9:
The Z-Transform
9.6 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO FROM
HERE?
Although the history of the Z-transform is originally connected with probability theory, for discrete-
time signals and systems it can be connected with the Laplace transform. The periodicity in the
frequency domain and the possibility of an inﬁnite number of poles and zeros makes this connec-
tion not very useful. Deﬁning a new complex variable in polar form provides the deﬁnition of the
Z-transform and the z-plane. As with the Laplace transform, poles of the Z-transform characterize
discrete-time signals by means of frequency and attenuation. One- and two-sided Z-transforms are
possible, although the one-sided version can be used to obtain the two-sided one. The region of con-
vergence makes the Z-transform have a unique relationship with the signal, and it will be useful in
obtaining the discrete Fourier representations in Chapter 10.
Dynamic systems represented by difference equations use the Z-transform for representation by
means of the transfer function. The one-sided Z-transform is useful in the solution of difference
equations with nonzero initial conditions. As in the continuous-time case, ﬁlters can be represented
by difference equations. However, discrete ﬁlters represented by polynomials are also possible. These
nonrecursive ﬁlters give signiﬁcance to the convolution sum, and will motivate us to develop methods
that efﬁciently compute it.
You will see that the reason to present the Z-transform before the Fourier representation of discrete-
time signals and systems is to use their connection, thereby simplifying calculations.
PROBLEMS
9.1. Mapping of s-plane into the z-plane
The poles of the Laplace transform X(s) of an analog signal x(t) are
p1,2 = −1 ± j1
p3 = 0
p4,5 = ±j1
There are no zeros. If we use the transformation z = e sTs with Ts = 1:
(a) Determine where the given poles are mapped into the z-plane.
(b) How would you determine if these poles are mapped inside, on, or outside the unit circle in the z-
plane? Explain.
(c) Carefully plot the poles and the zeros of the analog and the discrete-time signals in the Laplace and
the z-planes.
9.2. Mapping of z-plane into the s-plane
Consider the inverse relation given by z = e sTs—that is, how to map the z-plane into the s-plane.
(a) Find an expression for s in terms of z from the relation z = e sTs.
(b) Consider the mapping of the unit circle (i.e., z = 1e jω, −π ≤ω < π). Obtain the segment in the s-plane
resulting from the mapping.
(c) Consider the mapping of the inside and the outside of the unit circle. Determine the regions in the
s-plane resulting from the mappings.

Problems
565
(d) From the above results, indicate the region in the s-plane to which the whole z-plane is mapped into.
Since ω = ω + 2π, is this mapping unique? Explain.
9.3. Z-transform and ROCs
Consider the noncausal sequence
s[n] = s1[n] + s2[n]
where s1[n] = u[n] is causal and s2[n] = −u[−n] is anti-causal. This signal is the signum, or sign function,
that extracts the sign of a real-valued signal—that is,
s[n] = sgn(x[n]) =



−1
x[n] < 0
0
x[n] = 0
1
x[n] > 0
(a) Find the Z-transforms of s1[n] and s2[n], indicating the corresponding ROC.
(b) Determine the Z-transform S(z).
9.4. Z-transform and ROC
Given the anti-causal signal
x[n] = −αnu[−n]
(a) Determine the Z-transform X(z), and carefully plot the ROC when α = 0.5 and α = 2. For which of the
two values of α does X(e jω) exist?
(b) Find the signal that corresponds to the derivative dX(z)/dz. Express it in terms of α.
9.5. Signiﬁcance of ROC
Consider a causal signal x1[n] = u[n] and an anti-causal signal x2[n] = −u[−n −1].
(a) Find the Z-transforms X1(z) and X2(z) and carefully plot their ROCs. If the ROCs are not included with
the Z-transforms, would you be able to tell which is the correct inverse? Explain.
(b) Determine if it is possible to ﬁnd the Z-transform of x1[n] + x2[n].
9.6. Fibonacci sequence generation—MATLAB
Consider the Fibonacci sequence generated by the difference equation
f[n] = f[n −1] + f[n −2]
n ≥0
with initial conditions f[−1] = 1, f[−2] = −1.
(a) Find the Z-transform of f[n], or F(z).
(b) Find the poles φ1 and φ2 and the zeros of F(z) and plot them. How are the poles connected? How are
they related to the “golden ratio”?
(c) The Fibonacci difference equation has zero input, but its response is a sequence of ever-increasing
integers. Obtain a partial fraction expansion of F(z) and ﬁnd f[n] in terms of the poles φ1 and φ2, and
show that the result is always integer. Use MATLAB to implement the inverse in term of the poles.
9.7. Laplace and Z-transforms of sampled signals
An analog pulse x(t) = u(t) −u(t −1) is sampled using a sampling period Ts = 0.1.
(a) Obtain the discrete-time signal x(nTs) = x(t)|t=nTs and plot it as a function of nTs.
(b) If the sampled signal is represented as an analog signal as
xs(t) =
N−1
X
n=0
x(nTs)δ(t −nTs)
determine the value of N in the above equation.

566
CHAPTER 9:
The Z-Transform
(c) Compute the Laplace transform of the sampled signal (i.e., Xs(s) = L[xs(t]).
(d) Determine the Z-transform of x(nTs), or X(z).
(e) Indicate how to transform Xs(s) into X(z)
9.8. Computation of Z-transform—MATLAB
Consider a discrete-time pulse x[n] = u[n] −u[n −10].
(a) Plot x[n] as a function of n and use the deﬁnition of the Z-transform to ﬁnd X(z).
(b) Use the Z-transform of u[n] and properties of the Z-transform to ﬁnd X(z). Verify that the expressions
obtained above for X(z) are identical.
(c) Find the poles and the zeros of X(z) and plot them in the z-plane. Use MATLAB to plot the poles and
zeros.
9.9. Computation of Z-transform
A causal exponential x(t) = 2e−2tu(t) is sampled using a sampling period Ts = 1. The corresponding
discrete-time signal is x[n] = 2e−2nu[n].
(a) Express the discrete-time signal as x[n] = 2αnu[n] and give the value of α.
(b) Find the Z-transform X(z) of x[n] and plot its poles and zeros in the z-plane.
9.10. Computation of Z-transform
Consider the signal x[n] = 0.5(1 + [−1]n)u[n].
(a) Plot x[n] and use the deﬁnition of the Z-transform to obtain its Z-transform, X(z).
(b) Use the linearity property and the Z-transforms of u[n] and [−1]nu[n] to ﬁnd the Z-transform X(z) =
Z[x[n]].
(c) Determine and plot the poles and the zeros of X(z).
9.11. Solution of difference equations with Z-transform
Consider a system represented by the ﬁrst-order difference equation
y[n] = x[n] −0.5y[n −1]
where y[n] is the output and x[n] is the input.
(a) Find the Z-transform Y(z) in terms of X(z) and the initial condition y[−1].
(b) Find an input x[n] ̸= 0 and an initial condition y[−1] ̸= 0 so that the output is y[n] = 0 for n ≥0. Verify
you get this result by solving the difference equation recursively.
(c) For zero initial conditions, ﬁnd the input x[n] so that y[n] = δ[n] + 0.5δ[n −1].
9.12. Transfer function, stability, and impulse response—MATLAB
Consider a second-order discrete-time system represented by the difference equation
y[n] −2r cos(ω0)y[n −1] + r2y[n −2] = x[n]
n ≥0
where r > 0 and 0 ≤ω0 ≤2π, y[n] is the output, and x[n] is the input.
(a) Find the transfer function H(z) of this system.
(b) Find the value of ω0 and determine the values of r that would make the system stable. Use the
MATLAB function zplane to plot the poles and the zeros for r = 0.5 and ω0 = π/2 radians.
(c) Let ω0 = π/2. Find the corresponding impulse response h[n] of the system. What other value of ω0
would get the same impulse response?
9.13. Generation of discrete-time sinusoid—MATLAB
Given that the Z-transform of a discrete-time cosine A cos(ω0n)u[n] is
A(1 −cos(ω0)z−1)
1 −2 cos(ω0)z−1 + z−2

Problems
567
(a) Use the given Z-transform to ﬁnd a difference equation for which the output y[n] is a discrete-time
cosine A cos(ω0n) and the input is x[n] = δ[n]. What should you use as initial conditions?
(b) Verify your algorithm by generating a signal y[n] = 2 cos(πn/2)u[n] by implementing your algorithm
in MATLAB. Plot the input and the output signals x[n] and y[n].
(c) Indicate how to change your previous algorithm to generate a sine function y[n] = 2 sin(πn/2)u[n].
Use MATLAB to ﬁnd y[n], and to plot it.
9.14. Inverse Z-transform and poles and zeros
When ﬁnding the inverse Z-transform of functions with z−1 terms in the numerator, the fact that z−1 can
be thought of as a delay operator can be used to simplify the computation. Consider
X(z) = 1 −z−10
1 −z−1
(a) Use the Z-transform of u[n] and the properties of the Z-transform to ﬁnd x[n].
(b) If we consider X(z) a polynomial in negative powers of z, what would be its degree and the values of
its coefﬁcients?
(c) Find the poles and the zeros of X(z) and plot them on the z-plane. Is there a pole or zero at z = 1?
Explain.
9.15. Initial conditions and steady state
Consider a second-order system represented by the difference equation
y[n] = 0.25y[n −2] + x[n]
where x[n] is the input and y[n] is the output.
(a) For the zero-input case (i.e., when x[n] = 0), ﬁnd the initial conditions y[−1] and y[−2] so that y[n] =
0.5nu[n].
(b) Suppose the input is x[n] = u[n]. Without solving the difference equation can you ﬁnd the correspond-
ing steady state yss[n]? Explain how and give the steady-state output. Verify by inverse Z-transform
that the steady-state response yss[n] is the one obtained.
9.16. Initial conditions and impulse response
A second-order system has the difference equation
y[n] = 0.25y[n −2] + x[n]
where x[n] is the input and y[n] is the output.
(a) Find the input x[n] so that for zero initial conditions, the output is given as y[n] = 0.5nu[n].
(b) If x[n] = δ[n] + 0.5δ[n −1] is the input to the above difference equation, ﬁnd the impulse response of
the system.
9.17. Convolution sum and product of polynomials
The convolution sum is a fast way to ﬁnd the coefﬁcients of the polynomial resulting from the multiplication
of two polynomials.
(a) Suppose x[n] = u[n] −u[n −3]. Find its Z-transform X(z), a second-order polynomial in z−1.
(b) Multiply X(z) by itself to get a new polynomial Y(z) = X(z)X(z) = X2(z). Find Y(z).
(c) Graphically show the convolution of x[n] with itself and verify that the result coincides with the
coefﬁcients of Y(z).

568
CHAPTER 9:
The Z-Transform
9.18. Inverse Z-transform
Find the inverse Z-transform of
X(z) =
8 −4z−1
z−2 + 6z−1 + 8
and determine x[n] as n →∞. Assume x[n] is causal.
9.19. Z-transform properties and inverse transform
Sometimes the partial fraction expansion is not needed in ﬁnding the inverse Z-transform—instead the
properties of the transform can be used. Consider the function
F(z) =
z + 1
z2(z −1)
(a) Determine whether F(z) is a proper rational function as a function of z and of z−1.
(b) Verify that F(z) can be written as
F(z) =
z−2
1 −z−1 +
z−3
1 −z−1
Find the inverse Z-transform f[n] using the above expression.
9.20. Inverse Z-transform—MATLAB
We are interested in the unit-step solution of a system represented by the difference equation
y[n] = y[n −1] −0.5y[n −2] + x[n] + x[n −1]
(a) Find an expression for Y(z).
(b) Do a partial expansion of Y(z).
(c) Find the inverse Z-transform y[n] and verify your results using MATLAB.
9.21. Pad´e approximation
Suppose we are given a ﬁnite-length sequence h[n] (it could be part of an inﬁnite-length impulse response
from a discrete system that has been windowed) and would like to obtain a rational approximation for it.
This means that if H(z) = Z[h[n]], a rational approximation of it would be H(z) = B(z)/A(z), from which
we get
H(z)A(z) = B(z)
Letting
B(z) =
M−1
X
k=0
bkz−k
A(z) = 1 +
N−1
X
k=1
akz−k
for some choice of M and N, equations from H(z)A(z) = B(z) should allow us to ﬁnd the M + N −1
coefﬁcients {ak bk}.

Problems
569
(a) Find a matrix equation that would allow us to ﬁnd the coefﬁcients of B(z) and A(z).
(b) Let h[n] = 0.5n(u[n] −u[n −101]) be the sequence we wish to obtain a rational approximation and
let B(z) = b0 while A(z) = a0 + a1z−1. Find the equations to solve for the coefﬁcients {b0, a0, a1}.
9.22. Prony’s rational approximation—MATLAB
The Pad´e approximants provide an exact matching of M + N −1 values of h[n] where M and N are,
respectively, the orders of the numerator and the denominator of the rational approximation. But there is
no method for choosing the numerator and the denominator orders, M and N. Also, there is no guarantee
on how well the rest of the signal is matched. Prony’s rational approximation considers how well the rest
of the signal is approximated when ﬁnding the approximation. Let h[n] = 0.9nu[n] be the exact impulse
response for which we wish to ﬁnd a rational approximation. Take the ﬁrst 100 values of this signal as the
impulse response.1
(a) Assume the order of the numerator and the denominator are equal, M = N = 1. Use the
MATLAB function prony to obtain the rational approximation, and then use ﬁlter to verify that the
impulse response of the rational approximation is close to the given 100 values. Plot the error between
h[n] and the impulse response of the rational approximation for the ﬁrst 200 samples.
(b) Plot the poles and the zeros of the rational approximation and compare them to the poles and the
zeros of H(z) = Z(h[n].
(c) Suppose that h[n] = (h1 ∗h2)[n]—that is, the convolution of h1[n] = 0.9nu[n] and h2[n] = 0.8nu[n].
Use again prony to ﬁnd the rational approximation when the ﬁrst 100 values of h[n] are available. Use
conv from MATLAB to compute h[n]. Compare the impulse response of the rational approximation to
h[n]. Plot the poles and the zeros of H(z) = Z(h[n]) and of the rational approximation.
(d) Consider the h[n] given above, and perform the Prony approximation using orders M = N = 3. Explain
your results. Plot the poles and the zeros.
9.23. MATLAB partial fraction expansion
Consider the partial fraction expansion that MATLAB uses.
(a) Find the inverse Z-transform of a/(1 −az−1)2.
(b) Suppose that the partial fraction expansion given by MATLAB is
X(z) =
−1
1 −0.5z−1 +
1
(1 −0.5z−1)2
Determine the inverse x[n].
9.24. MATLAB partial fraction expansion
Consider ﬁnding the inverse Z-transform of
X(z) =
2z−1
(1 −z−1)(1 −2z−1)2
|z| > 2
MATLAB does the partial fraction expansion as
X(z) =
A
1 −z−1 +
B
1 −2z−1 +
C
(1 −2z−1)2
while we do it in the following form:
X(z) =
D
1 −z−1 +
E
1 −2z−1 +
Fz−1
(1 −2z−1)2
Show that the two partial fraction expansions give the same result.
1Gaspar de Prony (1765–1839) was a French mathematician and engineer, while Henri Pad´e (1863–1953) was a French mathemati-
cian interested in rational approximations.

570
CHAPTER 9:
The Z-Transform
9.25. Prony method and Z-Transform—MATLAB
Consider ﬁnding the Z-transform of a noncausal signal h[n] = 0.5nu[n + 1] using the Prony approxima-
tion.
(a) Use the prony function to ﬁnd a rational approximation for h[n] (i.e., the Z-transform H(z) = B(z)/A(z)).
Use a ﬁrst order for the numerator and the denominator.
(b) Separate the signal into its causal and anti-causal components, and use prony to ﬁnd the rational
approximation of the causal and then add the anti-causal component to correct the above result.

CHAPTER 10
Fourier Analysis of Discrete-Time
Signals and Systems
Diligence is the mother of good luck.
Benjamin Franklin (1706–1790)
Printer, inventor, scientist, and diplomat
I am a great believer in luck,
and I ﬁnd the harder I work,
the more I have of it.
President Thomas Jefferson (1743–1826)
Main author of the U.S. Declaration of Independence
10.1 INTRODUCTION
In this chapter we will consider the Fourier representation of discrete-time signals and systems. Simi-
lar to the connection between the Laplace and the Fourier transforms of continuous-time signals
and systems, if the region of convergence of the Z-transform of a signal or of the transfer function
of a discrete system includes the unit circle, then the discrete-time Fourier transform (DTFT) of the
signal or the frequency response of the system is easily found. Duality in time and frequency is
used whenever signals and systems do not satisfy this condition. We can thus obtain the Fourier
representation of most discrete-time signals and systems.
Two computational disadvantages of the DTFT are that the direct DTFT is a function of a contin-
uously varying frequency, and the inverse DTFT requires integration. These disadvantages can be
removed by sampling in frequency the DTFT, resulting in the so-called discrete Fourier transform
(DFT) (notice the difference in the naming of these two related frequency representations). An
interesting connection determines their computational feasibility: a discrete–time signal has a peri-
odic continuous-frequency transform—the DTFT—while a periodic discrete-time signal has a periodic and
discrete-frequency transform—the DFT. As we will discuss in this chapter, any periodic or aperiodic sig-
nal can be represented by the DFT, a computationally feasible transformation where both time and
frequency are discrete and no integration is required, and that can be implemented very efﬁciently by
the Fast Fourier Transform (FFT) algorithm.
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00014-4
c⃝2011, Elsevier Inc. All rights reserved.
571

572
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
In this chapter, we will see that a great deal of the Fourier representation of discrete-time signals
and characterization of discrete systems can be obtained from our knowledge of the Z-transform.
To obtain the DFT, which is of great signiﬁcance in digital signal processing, we will proceed in an
opposite direction as in the continuous-time analysis. First, we consider the Fourier representation of
aperiodic signals and then that of periodic discrete-time signals, and ﬁnally use this representation
to obtain the DFT.
10.2 DISCRETE-TIME FOURIER TRANSFORM
The discrete-time Fourier transform (DTFT) of a discrete-time signal x[n],
X(e jω) =
X
n
x[n]e−jωn
−π ≤ω < π
(10.1)
converts x[n] into a function X(e jω) of the discrete frequency ω (rad), while the inverse transform gives back
x[n] from X(e jω) according to
x[n] = 1
2π
π
Z
−π
X(e jω)e jωndω
(10.2)
Remarks
I
The DTFT measures the frequency content of a discrete-time signal. When using the DTFT, it is important
to remember some of the differences between the continuous and the discrete domains. Discrete-time
signals are only deﬁned for uniform sample times nTs or integers n, and the discrete frequency is such that
it repeats every 2π radians (i.e., ω = ω + 2πk for any integer k), so that X(e jω) is periodic and only the
frequencies [−π, π) need to be considered.
I
The DTFT X(e jω) is periodic of period 2π. Indeed, for an integer k,
X(e j(ω+2πk)) =
X
n
x[n]e−j(ω+2πk)n = X(e jω)
since e−j(ω+2πk)n = e−jωne−j2πkn = e−jωn. Thus, one can think of Equation (10.1) as the Fourier series
of X(e jω): If ϕ = 2π is the period, the Fourier series coefﬁcients are given by
x[n] = 1
ϕ
Z
ϕ
X(e jω)e j2πnω/ϕdω = 1
2π
π
Z
−π
X(e jω)e jnωdω
I
For the DTFT to converge, as an inﬁnite sum, it is necessary that
|X(e jω)| ≤
X
n
|x[n]||e jωn| =
X
n
|x[n]| < ∞

10.2 Discrete-Time Fourier Transform
573
or that x[n] be absolutely summable. This means that only for those signals we can use the above direct
(Eq. 10.1) and inverse (Eq. 10.2) DTFT deﬁnitions. We will see in the next section how to obtain the
DTFT of signals that do not satisfy the absolutely summable condition.
10.2.1 Sampling, Z-Transform, Eigenfunctions, and the DTFT
The connection of the DTFT with sampling, eigenfunctions, and the Z-transform can be shown as
follows:
I
Sampling and the DTFT. When sampling an analog signal x(t), the sampled signal xs(t) can be
written as
xs(t) =
X
n
x(nTs)δ(t −nTs)
Its Fourier transform is then
F[xs(t)] =
X
n
x(nTs)F[δ(t −nTs)]
=
X
n
x(nTs)e−jnTs
Letting ω = Ts, the discrete frequency in radians, the above equation can be written as
Xs(e jω) = F[xs(t)] =
X
n
x(nTs)e−jnω
(10.3)
coinciding with the DTFT of the discrete-time signal x(nTs) = x(t)|t=nTs or x[n].
At the same time, the spectrum of the sampled signal can be equally represented as
Xs(e jTs) = Xs(e jω) =
X
k
1
Ts
X
 ω
Ts
−2πk
Ts

(10.4)
which is a periodic repetition, with period 2π/Ts, of the spectrum of the analog signal being
sampled. Thus, sampling converts a continuous-time signal into a discrete-time signal with a
periodic spectrum varying continuously in frequency.
I
Z-transform and the DTFT. If in the above we ignore Ts and consider x(nTs) a function of n, we can
see that
Xs(e jω) = X(z)|z=e jω
(10.5)
That is, it is the Z-transform computed on the unit circle. For the above to happen, X(z) must
have a region of convergence (ROC) that includes the unit circle. There are discrete-time sig-
nals for which we cannot ﬁnd their DTFTs from the Z-transform because they are not absolutely
summable—that is, their ROCs do not include the unit circle. However, any discrete-time signal
x[n], of ﬁnite support in time, has a Z-transform X(z) with a region of convergence the whole
z-plane, excluding either the origin or inﬁnity, and as such its DTFT X(e jω) is computed from X(z)
by letting z = e jω.

574
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
I
Eigenfunctions and the DTFT. The frequency representation of a discrete-time linear time-invariant
(LTI) system is shown to be the DTFT of the impulse response of the system. Indeed, according to
the eigenfunction property of LTI systems, if the input of such a system is a complex exponential,
x[n] = e jω0n, the steady-state output, calculated with the convolution sum, is given by
y[n] =
X
k
h[k]x[n −k] =
X
k
h[k]e jω0(n−k) = e jω0nH(e jω0)
(10.6)
where
H(e jω0) =
X
k
h[k]e−jω0k
(10.7)
or the DTFT of the impulse response h[n] of the system computed at ω = ω0. As with continuous-
time systems, the system needs to be bounded-input bounded-output (BIBO) stable. Without the
stability of the system, there is no guarantee that there will be a steady-state response.
I Example 10.1
Consider the noncausal signal x[n] = α|n| with |α| < 1. Determine its DTFT. Use the obtained DTFT
to ﬁnd
∞
X
n=−∞
α|n|
Solution
The Z-transform of x[n] is
X(z) =
∞
X
n=0
αnz−n +
∞
X
m=0
αmzm −1
=
1
1 −αz−1 +
1
1 −αz −1 =
1 −α2
1 −α(z + z−1) + α2
where the ﬁrst term has an ROC of |z| > |α|, and the ROC of the second term is |z| < 1/|α|. Thus,
the region of convergence of X(z) is
ROC: |α| < |z| < 1
|α|
and that includes the unit circle. Thus, the DTFT is
X(e jω) =
1 −α2
(1 + α2) −2α cos(ω)
(10.8)

10.2 Discrete-Time Fourier Transform
575
According to the formula for the DTFT at ω = 0, we have that
X(e j0) =
∞
X
n=−∞
x[n]e j0n =
∞
X
n=−∞
α|n| =
2
1 −α −1 = 1 + α
1 −α
and according to Equation (10.8), equivalently we have
X(e j0) =
1 −α2
1 −2α + α2 = 1 −α2
(1 −α)2 = 1 + α
1 −α
I
10.2.2 Duality in Time and Frequency
In practice, there are many signals of interest that do not satisfy the absolute summability condition,
and so we cannot ﬁnd their DTFTs with the deﬁnition given in the previous section. Duality in the
time and frequency representation of signals permits us to obtain the DTFT of those signals.
Consider the DTFT of the signal δ[n −k] for some integer k. Since Z[δ[n −k]] = z−k with ROC the
whole z-plane except for the origin, the DTFT of δ[n −k] is e−jωk. By duality, as in the continuous-
time domain, we would expect that the signal e−jω0n, −π ≤ω0 < π, would have 2πδ(ω + ω0) (where
δ(ω) is the analog delta function) as its DTFT. Indeed, the inverse DTFT of 2πδ(ω + ω0) gives
1
2π
π
Z
−π
2πδ(ω + ω0)e jωndω = e−jω0n
π
Z
−π
δ(ω + ω0)dω = e−jω0n
Using these results, we have the following dual pairs:
∞
X
k=−∞
x[k]δ[n −k] ⇔
∞
X
k=−∞
x[k]e−jωk
∞
X
k=−∞
X[k]e−jωkn ⇔
∞
X
k=−∞
2πX[k]δ(ω + ωk)
(10.9)
The top left equation is the generic representation of a discrete-time signal x[n] and the corresponding
term on the right is its DTFT X(e jω), one more veriﬁcation of Equation (10.1). The bottom pair is a
dual of the above.1 Using Equation (10.9), we then have the following dual pairs as special cases:
x[n] = Aδ[n] ⇔X(e jω) = A
y[n] = A, −∞< n < ∞⇔Y(e jω) = 2πAδ(ω)
−π ≤ω < π
The signal y[n] is not absolutely summable, and as a constant it does not change from −∞to ∞,
so that its frequency is ω = 0, thus its DTFT Y(e jω) is concentrated in that frequency. Consider then
1Calling this a “dual” is not completely correct given that the ωk are discrete values of frequency instead of continuous as expressed by
ω, and that the delta functions are not the same in the continuous and the discrete domains, but a duality of some sort exists in these
two pairs, which we would like to take advantage of.

576
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
a sinusoid x[n] = cos(ω0n + θ), which is not absolutely summable. According to the above pairs,
we get
x[n] = 1
2
h
e j(ω0n+θ) + e−j(ω0n+θ)i
⇔X(e jω) = π
h
e jθδ(ω −ω0) + e−jθδ(ω + ω0)
i
The DTFT of the cosine indicates that its power is concentrated at the frequency ω0.
The “dual” pairs
δ[n −k], integer k ⇔e−jωk
(10.10)
e−jω0n,
−π ≤ω0 < π ⇔2πδ(ω + ω0)
(10.11)
allow us to obtain the DTFT of signals that do not satisfy the absolutely summable condition. Thus, in general,
we have
X
k
X[k]e−jωkn ⇔
X
k
2πX[k]δ(ω + ωk)
(10.12)
The linearity of the DTFT and the above result give that for a signal that is not absolutely summable
x[n] =
X
ℓ
Aℓcos(ωℓn + θℓ)
its DTFT is
X(e jω) =
X
ℓ
πAℓ
h
e jθℓδ(ω −ωℓ) + e−jθℓδ(ω + ωℓ)
i
−π ≤ω < π
If x[n] is periodic, the discrete frequencies are harmonically related, i.e., ωℓ= ℓω0 where ω0 is the fundamental
frequency of x[n].
I Example 10.2
The DTFT of a signal x[n] is
X(e jω) = 1 + δ(ω −4) + δ(ω + 4) + 0.5δ(ω −2) + 0.5δ(ω + 2)
The signal x[n] = A + B cos(ω0n) cos(ω1n) is given as a possible signal that gives X(e jω). Determine
whether you can ﬁnd A, B, and ω0 and ω1 to obtain the desired DTFT. If not, provide a better x[n].
Solution
Using trigonometric identities or Euler’s identity, we have that
cos(ω0n) cos(ω1n) = 0.5 cos((ω0 + ω1)n) + 0.5 cos((ω1 −ω0)n)

10.2 Discrete-Time Fourier Transform
577
so that x[n] = A + 0.5B cos((ω0 + ω1)n) + 0.5B cos((ω1 −ω0)n). Letting ω2 = ω0 + ω1 and ω3 =
ω1 −ω0, the DTFT of x[n] is
X(e jω) = 2πA + πB(δ(ω −ω2) + δ(ω + ω2)) + πB(δ(ω −ω3) + δ(ω + ω3))
Comparing this DTFT with the given one, we ﬁnd that
2πA = 1 ⇒A = 1/(2π)
ω2 = ω0 + ω1 = 4
ω3 = ω1 −ω0 = 2
πB = 1, 0.5 ⇒
no unique value for B
Although we ﬁnd that A = 1/(2π) and ω0 = 1 and ω1 = 3, there is no unique value for B, so the
given x[n] is not the correct answer. The correct signal should be x[n] = (1/π)(0.5 + cos(4n)) +
(1/2π) cos(2n), which has the desired DTFT (verify it!).
I
10.2.3 Computation of the DTFT Using MATLAB
According to the deﬁnitions of the direct and the inverse DTFT, their computation needs to be done
for a continuous frequency ω ∈[−π, π) and requires integration. In MATLAB the DTFT is approxi-
mated in a discrete set of frequency values, and summation instead of integration is used. As we will
see later, this can be done by sampling in frequency the DTFT to obtain the discrete Fourier transform
(DFT), which in turn is efﬁciently implemented by an algorithm called the Fast Fourier Transform
(FFT). We will introduce the FFT in Chapter 12, and so for now consider the FFT as a black box
capable of giving a discrete approximation of the DTFT.
To understand the use of the MATLAB function fft in the script below consider the following issues:
I
The command X = fft(x), where x is a vector with the entries the sample values x[n], n = 0, . . . , L −1,
computes the FFT values X[k], k = 0, . . . , L −1, or the DTFT X(e jω) at discrete frequencies {2πk/L}.
I
The {k} values correspond to the discretized frequencies {ωk = 2πk/L}, which go from 0 to 2π(L −
1)/L (close to 2π for large L). This is a discretization of the frequency ω ∈[0, 2π).
I
To ﬁnd an equivalent representation of the frequency ω ∈[−π, π), we simply subtract π from
ωk = 2πk/L to get a band of frequencies,
˜ωk = ωk −π = π 2k −L
L
, k = 0, . . . , L −1 or −π ≤˜ωk < π
The frequency ˜ω can be normalized to [−1, 1) with no units by dividing by π. This change in
the frequency scale requires a corresponding shift of the magnitude and the phase spectra. This is
done by means of the MATLAB function fftshift.
I
When plotting the signal, which is discrete in time, the function stem is more appropriate than
plot. However, the plot function is more appropriate for plotting the magnitude and the phase
frequency response functions, which are supposed to be continuously varying with respect to
frequency.

578
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
I
The function abs computes the magnitude and the function angle computes the phase of the
frequency response. The magnitude and the phase are even and odd symmetric when plotted in
ω ∈[−π, π) or in the normalized frequency ω/π ∈[−1, 1).
The three signals in the following script are a rectangular pulse, a windowed sinusoid, and a chirp. In
the script, to process one of the signals you delete the corresponding comment % and keep it for the
other two. The length of the FFT is set to L = 256, which is larger or equal to the length of either of
the three signals.
%%%%%%%%%%%%%%%%%%%%%%
% DTFT of aperiodic signals
%%%%%%%%%%%%%%%%%%%%%%
% signals
L = 256; % length of FFT with added zeros
% N = 21; x = [ones(1,N) zeros(1,L - N)];
% pulse
% N = 200; n = 0:N - 1; x = [cos(4 ∗pi ∗n/N) zeros(1,L - N)];
% windowed sinusoid
n = 0:L - 1; x = cos(pi ∗n. ˆ 2/(4 ∗L));
% chirp
X = fft(x);
w = 0:2 ∗pi/L:2 ∗pi - 2 ∗pi/L;w1 = (w - pi)/pi; % normalized frequency
n = 0:length(x) - 1;
subplot(311)
stem(n,x); axis([0 length(n) - 1 1.1 ∗min(x) 1.1 ∗max(x)]); grid;
xlabel(‘n’); ylabel(‘x(n)’)
subplot(312)
plot(w1,fftshift(abs(X))); axis([min(w1) max(w1) 0 1.1 ∗max(abs(X))]);
ylabel(‘|X|’); grid
subplot(313)
plot(w1,fftshift(angle(X))); ylabel(‘<X’); xlabel(‘ω/π’); grid
axis([min(w1) max(w1) 1.1 ∗min(angle(X)) 1.1 ∗max(angle(X))])
As expected, the magnitude spectrum for the rectangular pulse is like a sinc. The windowed sinusoid
has a spectrum that resembles that of the sinusoid but the rectangular window makes it broader.
Finally, a chirp is a sinusoid with time-varying frequency; thus its magnitude spectrum displays com-
ponents over a range of frequencies. We will comment on the phase spectra later. The results are
shown in Figure 10.1.
Sampled Signals
When computing the DTFT of a sampled signal, it is important to display the frequency in radi-
ans/second or in hertz rather than the discrete frequency in radians. The discrete frequency ω (rad)
is converted into the analog signal  (rad/sec) according to the relation ω = Ts where Ts is the
sampling period used. Thus,
 = ω/Ts rad/sec
(10.13)
If the signal is sampled according to the Nyquist sampling rate condition, the discrete-frequency
range ω ∈[−π, π) (rad) corresponds to  ∈[−π/Ts, π/Ts) or [−s/2, s/2), where s/2 ≥max

10.2 Discrete-Time Fourier Transform
579
0
10
20
30
40
50
60
70
80
90
100
0
0.5
1
n
x[n]
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0
10
20
|X(e jω)|
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−2
0
2
<X(e jω)
(a)
(b)
0
20
40
60
80
100
120
140
160
180
200
−1
0
1
n
x(n)
x(n)
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0
20
40
60
80
|X (e jω)|
|X(e jω)|
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−2
0
2
<X(e jω)
<X(e jω)
(c)
0
10
20
30
40
50
60
70
80
90
100
−1
0
1
n
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0
10
20
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−2
0
2
ω/π
ω/π
ω/π
FIGURE 10.1
MATLAB computation of the DTFT of (a) a pulse, (b) a windowed sinusoid, and (c) a chirp: magnitude and phase
spectra are shown for each.
for s the sampling frequency in radians/second and max the maximum frequency in the signal
being sampled.
To illustrate this, we sampled a signal x(t) = 5−2tu(t) with Ts = 0.01 sec/sample, created a vector of
256 values from the signal, and computed its FFT as before. The above script is modiﬁed to consider
the change of scale. The changes are as follows:
%%%%%%%%%%%%%%%%%%%%%
% DTFT of sampled signal
%%%%%%%%%%%%%%%%%%%%%
L = 256;Ts = 0.01; t = 0:Ts:(L - 1) ∗Ts; x = 5.ˆ( - 2 ∗t); % sampling of signal
X = fft(x);
w = 0:2 ∗pi/L:2 ∗pi - 2 ∗pi/L;W = (w - pi)/Ts;
% W is analog frequency

580
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
FIGURE 10.2
DTFT of (a) a sampled
signal, and (b) the
magnitude of DTFT and
(c) the phase of DTFT as
functions of  (rad/sec).
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.2
0.4
0.6
0.8
1
nTs (sec)
x(nTs)
−300
−200
−100
0
100
200
300
0
20
|X(e j Ω Ts)|
−300
−200
−100
0
100
200
300
−1
0
1
<X (e jΩ Ts)
Ω (rad/sec)
(a)
(b)
(c)
The results are shown in Figure 10.2. Given that the signal is very smooth, most of the frequency
components have low frequencies.
10.2.4 Time and Frequency Supports
The Fourier representation of a discrete-time signal gives a complementary characterization to its
time representation, just like in the continuous-time case. The following examples illustrate the
complementary nature of the DTFT of discrete-time signals.
Just like with analog signals, the frequency support of the DTFT of a discrete-time signal is inversely
proportional to the time support of the signal.
I Example 10.3
Consider a discrete pulse
p[n] = u[n] −u[n −N]
Find its DTFT P(e jω) and discuss the relation between its frequency support and the time support
of p[n] when N = 1 and when N →∞.

10.2 Discrete-Time Fourier Transform
581
Solution
Since p[n] has a ﬁnite support, its Z-transform has as region of convergence the whole z-plane,
except for z = 0, and we can ﬁnd its DTFT from it. We have
P(z) =
N−1
X
n=0
z−n = 1 + z−1 + · · · + z−(N−1) = 1 −z−N
1 −z−1
The DTFT of p[n] is then given by
P(e jω) = 1 + e−jω + · · · + e−jω(N−1)
or equivalently
P(e jω) = 1 −e−jωN
1 −e−jω = e−jωN/2[e jωN/2 −e−jωN/2]
e−jω/2[e jω/2 −e−jω/2]
= e−jω((N−1)/2) sin(ωN/2)
sin(ω/2)
The function sin(ωN/2)/ sin(ω/2) is the discrete counterpart of the sinc function in frequency. It
can be shown that like the sinc function, this function is:
I
Even function of ω, as both the numerator and the denominator are odd functions of ω.
I
0/0 at ω = 0, so that using L’Hˆopital’s rule its value at ω = 0 is
lim
ω→0
sin(ωN/2)
sin(ω/2) = N
I
Zero at ω = 2πk/N for integer values k ̸= 0, as sin(ωN/2)|ω=2πk/N = sin(πk) = 0.
I
Periodic of period 2π when N is odd, which can be seen from its equivalent representation,
sin(ωN/2)
sin(ω/2) = e jω((N−1)/2)P(e jω)
since P(e jω) is periodic of period 2π and
e j(ω+2π)((N−1)/2) = e jω((N−1)/2)e jπ(N−1) = e jω((N−1)/2)
when N is odd (e.g., N = 2M + 1 for some integer M). When N is even, e jπ(N−1) = −1, so
sin(ωN/2)/ sin(ω/2) is not periodic of period 2π.
Consider the discrete pulse when N = 1, then p[n] = u[n] −u[n −1] = δ[n], or the discrete
impulse. The DTFT is then P(e jω) = 1. In this case, the support of p[n] is one point, while the
support of P(e jω) is all discrete frequencies, or [−π, π).
As we let N →∞, the pulse tends to a constant, 1, making p[n] not absolutely summable. In the
limit, we ﬁnd that P(e jω) = 2πδ(ω), −π ≤ω < π. Indeed, the inverse DTFT is found to be
1
2π
π
Z
−π
2πδ(ω)e jωndω =
π
Z
−π
δ(ω)dω = 1
The time support of p[n] = 1 is inﬁnite, while P(e jω) = 2πδ(ω) exists at only one frequency.
I

582
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
Downsampling and Upsampling
Although the expanding and contracting of discrete-time signals is not as obvious as in the
continuous time, the dual effects of contracting and expanding in time and frequency also occur
in the discrete case. Contracting and expanding of discrete-time signals relates to downsampling and
upsampling.
Downsampling a signal x[n] means getting rid of samples (i.e., contracting the signal). The signal
downsampled by an integer factor M > 1 is given by
xd[n] = x[Mn]
(10.14)
If x[n] has a DTFT X(e jω), −π/M ≤ω ≤π/M, and zero otherwise in [−π, π) (analogous to
bandlimited signals in continuous time), by replacing n by Mn in the inverse DTFT, x[n] gives
x[Mn] = 1
2π
π/M
Z
−π/M
X(e jω)e jMnωdω = 1
2π
π
Z
−π
1
MX(e jρ/M)e jnρdρ
where we let ρ = Mω. Thus, the DTFT of xd[n] is 1
MX(e jω/M)—that is, an expansion by a factor of M
of the DTFT of x[n].
Upsampling a signal x[n], on the other hand, consists in adding L −1 zeros for some integer L > 1 in
between its samples—that is, the upsampled signal is
xu[n] =
x[n/L]
n = 0, ±L, ±2L, . . .
0
otherwise
(10.15)
thus expanding the original signal. The DTFT of the upsampled signal, xu[n], is found to be
Xu(e jω) = X(e jLω)
−π ≤ω < π
Indeed, the DTFT of xu[n] is
Xu(e jω) =
X
n=0,±L,...
x[n/L]e−jωn =
∞
X
m=−∞
x[m]e−jωLm = X(e jLω)
(10.16)
indicating that it is a contraction of the DTFT of x[n].
I
A signal x[n], bandlimited to π/M in [−π, π) or |X(e jω)| = 0, |ω| > π/M for an integer M > 1, can be
downsampled to generate a discrete-time signal
xd[n] = x[Mn] with Xd(e jω) = 1
MX(e jω/M)
(10.17)
which is an expanded version of X(e jω).
I
A signal x[n] is upsampled to generate a signal xu[n] = x[n/L] for n = ±kL, k = 0, 1, 2, . . . , and zero
otherwise. The DTFT of xu[n] is X(e jLω), or a compressed version of X(e jω).

10.2 Discrete-Time Fourier Transform
583
I Example 10.4
Consider the frequency response of an ideal low-pass ﬁlter,
H(e jω) =
1
−π/2 ≤ω ≤π/2
0
−π ≤ω < −π/2 and π/2 < ω ≤π
which is the DTFT of an impulse response h[n]. Determine h[n]. Suppose that we downsam-
ple h[n] with a factor of M = 2. Find the downsampled impulse response hd[n] = h[2n] and its
corresponding frequency response Hd(e jω).
Solution
The impulse response h[n] corresponding to the ideal low-pass ﬁlter is found to be
h[n] = 1
2π
π/2
Z
−π/2
e jωndω =
0.5
n = 0
sin(πn/2)/(πn)
n ̸= 0
The downsampled impulse response is given by
hd[n] = h[2n] =
0.5
n = 0
sin(πn)/(2πn) = 0
n ̸= 0
or hd[n] = 0.5δ[n], with a DTFT of Hd(e jω) = 0.5 for −π < ω ≤π (i.e., an all-pass ﬁlter). This
agrees with the downsampling theory, which gives that
Hd(e jω) = 1
2H(e jω/2) = 1
2,
−π ≤ω < π
That is, H(e jω) multiplied by 1/M = 1/2 and expanded by M = 2.
I
I Example 10.5
A discrete pulse is given by x[n] = u[n] −u[n −4]. Suppose we downsample x[n] by a factor of
M = 2, so that the length 4 of the original signal is reduced to 2, giving
xd[n] = x[2n] = u[2n] −u[2n −4] = u[n] −u[n −2]
Find the corresponding DTFTs for x[n] and xd[n], and determine how they are related.
Solution
The Z-transform of x[n] is
X(z) = 1 + z−1 + z−2 + z−3

584
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
with the whole z-plane (except for the origin) as its region of convergence. Thus, the DTFT of
x[n] is
X(e jω) = e−j( 3
2 ω) h
e j( 3
2 ω) + e j( 1
2 ω) + e−j( 1
2 ω) + e−j( 3
2 ω)i
= 2e−j( 3
2 ω)

cos
ω
2

+ cos
3ω
2

The Z-transform of the downsampled signal (M = 2) is
Xd(z) = 1 + z−1
and the DTFT of xd[n] is
Xd(e jω) = e−j( 1
2 ω) h
e j( 1
2 ω) + e−j( 1
2 ω)i
= 2e−j( 1
2 ω) cos
ω
2

Clearly, this is not equal to 0.5X(e jω/2). This is caused by aliasing: The maximum frequency of x[n]
is not π/M = π/2 and so Xd(e jω) is the sum of superposed and shifted X(e jω).
Suppose we pass x[n] through an ideal low-pass ﬁlter H(e jω) with cut-off frequency π/2. Its output
would be a signal x1[n] with a maximum frequency of π/2, and downsampling it with M = 2
would give a signal with a DTFT of 0.5X1(e jω/2).
I
I Example 10.6
Discuss the effects of downsampling a discrete signal that is not band limited versus the case of
one that is. Consider a unit rectangular pulse of length N = 10. Downsample it by a factor of
M = 2, and compute and compare the DTFTs of the pulse and its downsampled version. Do a
similar procedure to a sinusoid of discrete frequency π/4 and comment on the results. Explain the
difference between the above two cases. Use the MATLAB function decimate (low-pass ﬁltering is
used to avoid aliasing followed by downsampling) to perform similar operations and comment on
the differences with downsampling. Use the MATLAB function interp to interpolate (upsampling
with smoothing by a low-pass ﬁlter) the downsampled signals. See Figure 10.3 for illustrations of
downsampler and upsampler and decimator and interpolator.
Solution
As indicated, when we downsample a discrete-time signal x[n] by a factor of M, in order not to
have aliasing in frequency the signal must be band limited to π/M. If the signal satisﬁes this
condition, the spectrum of the downsampled signal is an expanded version of the spectrum of
x[n]. To illustrate this in the following script we downsample by a factor of M = 2 ﬁrst a signal that
is not band limited to π/2, and then another that is.

10.2 Discrete-Time Fourier Transform
585
FIGURE 10.3
Top: Downsampler and decimator. Bottom:
upsampler and interpolator.
H (z)
H (z)
LPF
Downsampler
Decimator
Upsampler
˜
LPF
Interpolator
x[n]
x [n]
x[n]
x1[n]
x1[Mn]
x[n]
x [Mn]
x [n/L]
x[n/L]
x [n]
M ↓
M ↓
M ↓
L ↑
L ↑
%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.6---down--sampling and decimation
%%%%%%%%%%%%%%%%%%%%%%%%%
x = [ones(1,10) zeros(1,100)];Nx = length(x); n1 = 0:19; % ﬁrst signal
% Nx = 200;n = 0:Nx - 1; x = cos(pi ∗n/4);
% second signal
y = x(1:2:Nx - 1);
% downsampling with M = 2
X = fft(x);Y = fft(y);
% ffts of original and downsampled signals
L = length(X);w = 0:2 ∗pi/L:2 ∗pi - 2 ∗pi/L;w1 = (w - pi)/pi; % frequency range
z = decimate(x,2,‘ﬁr’);
% decimation with M = 2
Z = fft(z);
% fft of decimated signal
%%%%%%%%%%%%%%%%%%%%%%%%%
% interpolation
%%%%%%%%%%%%%%%%%%%%%%%%%
s = interp(y,2);
As shown in Figure 10.4, the rectangular pulse is not band limited to π/2 since it has frequency
components beyond π/2, while the sinusoid is band limited. The DTFT of the downsampled rect-
angular pulse (a narrower pulse) is not an expanded version of the DTFT of the pulse, while the
DTFT of the downsampled sinusoid is an expanded version. The MATLAB function decimate uses
an FIR low-pass ﬁlter to smooth out x[n] to a frequency of π/2 before downsampling. In the case of
the sinusoid, which satisﬁes the downsampling condition, the downsampling and the decimation
provide the same results, but not for the rectangular pulse.
The original discrete-time signal can be recovered by interpolation. This procedure is composed
of upsampling followed by low-pass ﬁltering. The MATLAB function interp is used to that effect.
If we use the downsampled signal as input to this function, we obtain slightly better results for
the sinusoid than for the pulse when comparing the interpolated signal to the original signal.
The results are shown in Figure 10.5. The error s[n] −x[n] is shown also. The signal s[n] is the
interpolation of the downsampled signal y[n].
I
10.2.5 Parseval’s Energy Result
Just like in the case of continuous-time signals, the energy or power of a discrete-time signal x[n] can
be equally computed in either the time or the frequency domain.

586
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
(a)
(b)
0
5
10
15
0
5
10
15
0
5
10
15
0
0.5
1
x [n]
−1
−0.5
0
0.5
−1
−0.5
0
0.5
−1
−0.5
0
0.5
0
5
10
|X (ejω)|
|Y(ejω)|
|Z (ejω)|
0
0.5
1
y [n]
0
5
10
0
0.5
1
z [n]
n
0
5
10
ω/π
|X(e jω)|
|Y (e jω)|
|Z(e jω)|
ω/π
0
5
10
15
0
5
10
15
0
5
10
15
−1
0
1
−1
−0.5
0
0.5
−1
−0.5
0
0.5
−1
−0.5
0
0.5
0
50
100
−1
0
1
0
50
100
−1
0
1
n
0
50
100
x [n]
y [n]
z[n]
FIGURE 10.4
Downsampling of (a) non-band-limited and (b) band-limited discrete-time signals. The signals x[n] correspond
to the original signals, while y[n] and z[n] are their downsampled and decimated signals, respectively. The
corresponding magnitude spectra are shown. Notice the difference between the downsampled and the
decimated signals, they are identical when the signals are band-limited, slightly different otherwise.

10.2 Discrete-Time Fourier Transform
587
(a)
(b)
0
2
4
6
8
10
12
14
16
18
−0.4
−0.2
0
s[n]−x [n]
0
2
4
6
8
10
12
14
16
18
0
0.2
0.4
0.6
0.8
1
n
s[n], x [n]
Interpolated
Original
0
2
4
6
8
10
12
14
16
0
2
4
6
8
10
12
14
16
−1
−0.5
0
0.5
1
n
s[n], x [n]
−0.15
−0.1
−0.05
0
0.05
0.1
n
n
s [n]−x [n]
Interpolated
Original
FIGURE 10.5
Interpolation of (a) non-band-limited and (b) bandlimited discrete-time signals. The interpolated signal is
compared to the original signal, and the interpolation error is shown. The errors signals show that the original
signal can be recovered almost exactly when the signal satisﬁes the bandlimiting condition, not otherwise.
If the DTFT of a ﬁnite-energy signal x[n] is X(e jω), we have that the energy of the signal is given by
Ex =
∞
X
n=−∞
|x[n]|2 = 1
2π
π
Z
−π
|X(e jω)|2dω
(10.18)
The magnitude square |X(e jω)|2 has the units of energy per radian, and so it is called an energy density.
When |X(e jω)|2 is plotted against frequency ω, the plot is called the energy spectrum of the signal, or
how the energy of the signal is distributed over frequencies.
10.2.6 Time and Frequency Shifts
Shifting in time does not change the frequency content of a signal. Thus, the magnitude of the signal
DTFT is not affected, only the phase is. Indeed, if x[n] has a DTFT X(e jω), then the DTFT of x[n −N]
for some integer N is
F(x[n −N]) =
X
n
x[n −N]e−jωn
=
X
m
x[m]e−jω(m+N) = e−jωNX(e jω)
If x[n] has a DTFT
X(e jω) = |X(e jω)|e jθ(ω)
where θ(ω) is the phase, the shifted signal x1[n] = x[n −N] has a DTFT of
X1(e jω) = X(e jω)e−jωN
= |X(e jω)|e−j(ωN−θ(ω))

588
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
In a dual way, when we multiply a signal by a complex exponential e jω0n for some frequency ω0,
the spectrum of the signal is shifted in frequency. So if x[n] has a DTFT X(e jω), the modulated signal
x[n]e jω0n has as DTFT X(e j(ω−ω0)). Indeed, the DTFT of x1[n] = x[n]e jω0n is
X1(e jω) =
X
n
x1[n]e−jωn =
X
n
x[n]e−j(ω−ω0)n = X(e j(ω−ω0))
The following pairs illustrate the duality in time and frequency shifts: if the DTFT of x[n] is X(e jω) then
x[n −N] ⇔X(e jω)e−jωN
x[n]e jω0n ⇔X(e j(ω−ω0))
(10.19)
Remark The signal x[n]e jω0n was called modulated because x[n] modulates the complex exponential or
discrete-time sinusoids. It can be written as
x[n] cos(ω0n) + jx[n] sin(ω0n)
I Example 10.7
The DTFT of x[n] = cos(ω0n), −∞< n < ∞, cannot be found from the Z-transform or from the
sum deﬁning the DTFT as x[n] is not a ﬁnite-energy signal. Use the frequency-shift and the time-
shift properties to ﬁnd the DTFTs of x[n] = cos(ω0n) and y[n] = sin(ω0n).
Solution
Using Euler’s identity we have that
x[n] = cos(ω0n) = e jω0n + e−jω0n
2
and so the DTFT of x[n] is given by
X(e jω) = F[0.5e jω0n] + F[0.5e−jω0n]
= F[0.5]ω−ω0 + F[0.5]ω+ω0
= π[δ(ω −ω0) + δ(ω + ω0)]
Since
y[n] = sin(ω0n) = cos(ω0n −π/2) = cos(ω0(n −π/(2ω0)) = x[n −π/(2ω0)]
we have that according to the time-shift property its DTFT is given by
Y(e jω) = X(e jω)e−jωπ/(2ω0)
= π
h
δ(ω −ω0)e−jωπ/(2ω0) + δ(ω + ω0)e−jωπ/(2ω0)i

10.2 Discrete-Time Fourier Transform
589
= π
h
δ(ω −ω0)e−jπ/2 + δ(ω + ω0)e jπ/2i
= −jπ[δ(ω −ω0) −δ(ω + ω0)]
Thus, the frequency content of the cosine and the sine is concentrated at the frequency ω0.
Although the sinusoids are inﬁnite-energy signals they have ﬁnite power and their spectra can
be measured with a spectrum analyzer, which displays how the power is distributed over the
frequencies.
I
10.2.7 Symmetry
When plotting or displaying the spectrum of a real-valued discrete-time signal it is important to know
that it is only necessary to show the magnitude and the phase spectra for frequencies [0 π], since the
magnitude and the phase of X(e jω) are even and odd functions of ω, respectively. This can be shown
by considering a real-valued discrete-time signal x[n], with inverse DTFT given by
x[n] = 1
2π
π
Z
−π
X(e jω)e jωndω
and its complex conjugate is
x∗[n] = 1
2π
π
Z
−π
X∗(e jω)e−jωndω = 1
2π
π
Z
−π
X∗(e−jω′)e jω′ndω′
Since x[n] = x∗[n], as x[n] is real, comparing the above integrals we have that
X(e jω) = X∗(e−jω)
|X(e jω)|e jθ(ω) = |X(e−jω)|e−jθ(−ω)
Re[X(e jω)] + jIm[X(e jω)] = Re[X(e−jω)] −jIm[X(e−jω)]
or that the magnitude is an even function of ω—that is,
|X(e jω)| = |X(e−jω)|
(10.20)
and that the phase is an odd function of ω, or
θ(ω) = −θ(−ω)
(10.21)
Likewise, the real and the imaginary parts of X(e jω) are also even and odd functions of ω:
Re[X(e jω)] = Re[X(e−jω)]
Im[X(e jω)] = −Im[X(e−jω)]
(10.22)

590
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
I Example 10.8
For the signal x[n] = αnu[n], 0 < α < 1, ﬁnd the magnitude and the phase of its DTFT X(e jω).
Solution
The DTFT of x[n] is
X(e jω) =
1
1 −αz−1
z=e jω =
1
1 −αe−jω
since the Z-transform has a region of convergence |z| > α that includes the unit circle. Its
magnitude is
|X(e jω)| =
1
p
(1 −α cos(ω))2 + α2 sin2(ω)
which is an even function of ω given that cos(ω) = cos(−ω) and sin2(−ω) = (−sin(ω))2 =
sin2(ω). The phase is given by
θ(ω) = −tan−1

α sin(ω)
1 −α cos(ω)

which is an odd function of ω. As functions of ω, the numerator is odd and the denominator is
even, so that the argument of the inverse tangent is odd, which is in turn odd.
I
I Example 10.9
For a discrete-time signal
x[n] = cos(ω0n + φ)
−π ≤φ < π
determine how the magnitude and the phase responses of the DTFT X(e jω) change with φ.
Solution
The signal x[n] has a DTFT
X(e jω) = π
h
e−jφδ(ω −ω0) + e jφδ(ω + ω0)
i
Its magnitude is
|X(e jω)| = |X(e−jω)| = π[δ(ω −ω0) + δ(ω + ω0)]
for all values of φ. The phase of X(e jω) is
θ(ω) = φδ(ω + ω0) −φδ(ω −ω0)
=



φ
ω = −ω0
−φ
ω = ω0
0
otherwise

10.2 Discrete-Time Fourier Transform
591
In particular, if φ = 0, the signal x[n] is a cosine and its phase is zero. If φ = −π/2, x[n] is a sine
and its phase is π/2 at ω = −ω0 and −π/2 at ω = −ω0. The DTFT of a sine is
X(e jω) = π
h
δ(ω −ω0)e−jπ/2 + δ(ω + ω0)e jπ/2i
The DTFTs of the cosine and the sine are only different in the phase.
I
The symmetry property, like other properties, also applies to systems. If h[n] is the impulse response of an LTI
discrete-time system, and it is real valued, its DTFT is
H(e jω) = Z
 h[n]
 z=e jω = H(z)
z=e jω
if the region of convergence of H(z) includes the unit circle. As with the DTFT of a signal, the frequency
response of the system, H(e jω), has a magnitude that is an even function of ω, and a phase that is an odd
function of ω. Thus, the magnitude response of the system is such that
|H(e jω)| = |H(e−jω)|
(10.23)
and the phase response is such that
∠H(e jω) = −∠H(e−jω)
(10.24)
According to these symmetries and that the frequency response is periodic, it is only necessary to give these
responses in [0, π] rather than in (−π, π].
Computation of the Phase Spectrum
Computation of the phase using MATLAB is complicated by the following three issues:
I
Deﬁnition of the phase of a complex number: Given a complex number z = x + jy = |z|e jθ, its phase
θ is computed using the inverse tangent function
θ = tan−1 y
x

This computation is not well deﬁned because the principal values of tan−1 are [−π/2, π/2], while
the phase can extend beyond those values. By adding the information of which quadrants x and y
are in, the principal values can be extended to [−π, π). When the phase is linear (i.e., θ = −Nω
for some integer N), using the extended principal values is not good enough.
I
Signiﬁcance of magnitude when computing phase: Given two complex numbers, z1 = 1 + j =
√
2e jπ/4
and z2 = z1 × 10−16 =
√
2 × 10−16e jπ/4, they both have the same phase of π/4 but the magni-
tudes are very different, |z2| = 10−16|z1|. For practical purposes, the magnitude of z1 is more
signiﬁcant than that of z2, which is very close to zero, so one could disregard the phase of z2 with
no effect on computations.
I
Noisy measurements: Given that noise is ever present in actual measurements, even very small noise
present in the signal can change the computation of phase.

592
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
Phase unwrapping
Another problem with phase computation has to do with the way the phase is displayed as a function
of frequency with values within [−π, π)—the wrapped phase. If the DTFT of a signal is zero or inﬁ-
nite at some frequencies, the phase at those frequencies is not determined, as any value would be
as good as any other because the magnitude is zero or inﬁnity. On the other hand, if there are
no zeros or poles on the unit circle to make the magnitude zero or inﬁnite at some frequencies,
the phase is continuous. However, because of the way the phase is computed and displayed with
values between −π to π, it seems discontinuous. These phase discontinuities are 2π wide, mak-
ing the phase values right before and right after the discontinuity identical. Finding the frequencies
where these discontinuities occur and patching the phase, it is possible to obtain the continuous
phase. The process is called the unwrapping of the phase. The MATLAB function unwrap is used for this
purpose.
I Example 10.10
Consider a sinusoid x[n] = sin(πn/2) to which we add a Gaussian noise η[n] generated by the
MATLAB function randn, which theoretically can take any real value. Use the signiﬁcance of the
magnitude computed by MATLAB to estimate the phase.
Solution
The DTFT of x[n] consists of two impulses, one at π/2 and the other at −π/2. Thus, the phase
should be zero everywhere except at these frequencies. However, any amount of added noise (even
in noiseless cases, given the computational precision of MATLAB) would give nonzero phases in
places where it should be zero.
In this case, since we know that the magnitude spectrum is signiﬁcant at the negative and the
positive values of the sinusoid frequency, we use the magnitude as a mask to indicate where
the phase computation can be considered important given the signiﬁcance of the magnitude.
The following script is used to illustrate the masking using the signiﬁcance of the magnitude.
%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.10---phase of sinusoid in noise
%%%%%%%%%%%%%%%%%%%%%%%%%
n = 0:99; x = sin(pi ∗n/2) + 0.1 ∗randn(1,100);
% sine plus noise
X = fftshift(fft(x)); % fft of signal
X1 = abs(X); theta = angle(X); % magnitude and phase
theta1 = theta. ∗X1/max(X1);
% masked phase
L = length(X); w = 0:2 ∗pi/L:2 ∗pi - 2 ∗pi/L;w1 = (w - pi)/pi; % frequency range
Using the mask, the noisy phase (Figure 10.6(b)) is converted into the phase of the sine, which
occurs when the magnitude is signiﬁcant (Figures 10.5(a) and 10.5(c)).
I

10.2 Discrete-Time Fourier Transform
593
FIGURE 10.6
Phase spectrum of
sinusoid in Gaussian
noise using magnitude
masking: (a) magnitude
response, (b) wrapped
phase and
(c) unwrapped phase.
−1
−0.8
−0.6
−0.4
−0.2
0
(a)
(b)
(c)
0.2
0.4
0.6
0.8
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0
20
40
−2
0
2
θ(rad)
−2
0
2
θ1(rad)
ω /π
|X(e j ω)|
I Example 10.11
Consider two FIR ﬁlters with impulse responses
h1[n] =
9
X
k=0
1
10δ[n −k]
h2[n] = 0.5δ[n −3] + 1.1δ[n −4] + 0.5δ[n −5]
Determine which of these ﬁlters has linear phase, and use the MATLAB function unwrap to ﬁnd
their unwrapped phase functions. Explain the results.
Solution
The transfer function of the ﬁlter with h1[n] is
H1(z) = 1
10
9
X
n=0
z−n = 0.11 −z−10
1 −z−1 = 0.1 z10 −1
z9(z −1) = 0.1
Q9
k=1(z −e j2πk/10)
z9
Because this ﬁlter has nine zeros on the unit circle, its phase is not continuous and it cannot be
unwrapped. The impulse response of the second ﬁlter is symmetric about n = 4; thus its phase is

594
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
linear and continuous. Indeed, the transfer function of this ﬁlter is
H2(z) = 0.5z−3 + 1.1z−4 + 0.5z−5 = z−4(0.5z + 1.1 + 0.5z−1)
which gives the frequency response
H2(e jω) = e−j4ω(1.1 + cos(ω))
Since 1.1 + cos(ω) > 0 for −π ≤ω < π, the phase is ∠H2(e jω) = −4ω, which is a line through the
origin with slope −4 (i.e., a linear phase)
The following script is used to compute the frequency responses of the two ﬁlters using fft, their
wrapped phases using angle, and then unwrapping them using unwrap. Figure 10.7 displays the
magnitude responses of the two ﬁlters, as well as their wrapped and unwrapped phases.
%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.11---Phase unwrapping
%%%%%%%%%%%%%%%%%%%%%%%%%
h1 = (1/10) ∗ones(1,10); % ﬁr ﬁlter 1
h2 = [ zeros(1,3) 0.5 1.1 0.5 zeros(1,3)]; % ﬁr ﬁlter 2
H1 = fft(h1,256); % fft of h1
H2 = fft(h2,256); % fft of h2
H1m = abs(H1(1:128)); H1p = angle(H1(1:128)); % magnitude/phase of H1(z)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
−3
−2
−1
0
1
−3
−2
−1
0
1
0
0.2
0.4
0.6
0.8
|H1(e j ω)|
<H1(eω) (rad)
<H1u(ejω) (rad)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.5
1
1.5
2
ω/π
ω/π
ω/π
0
0.2
0.4
0.6
0.8
−2
0
2
0
0.2
0.4
0.6
0.8
−12
−10
−8
−6
−4
−2
0
|H2(e jω)|
<H2(ejω) (rad)
<H2u(ejω) (rad)
(a)
(b)
FIGURE 10.7
Unwrapping of the phase: (a) ﬁrst ﬁlter magnitude response, and wrapped and unwrapped phase responses;
(b) second ﬁlter magnitude response, and wrapped and unwrapped phase (linear phase) responses. Notice that
the phase of the ﬁrst ﬁlter cannot be unwrapped because the zeros of this ﬁlter are on the unit circle (see
corresponding magnitude response).

10.2 Discrete-Time Fourier Transform
595
H1up = unwrap(H1p); % unwrapped phase of H1(z)
H2m = abs(H2(1:128)); H2p = angle(H2(1:128)); % magnitude/phase of H2(z)
H2up = unwrap(H2p); % unwrapped phase of H1(z)
I
10.2.8 Convolution Sum
The computation of the convolution sum, just like the convolution integral in the continuous-time
domain, is simpliﬁed in the Fourier domain.
If h[n] is the impulse response of a stable LTI system, its output y[n] can be computed by means of the
convolution sum
y[n] =
X
k
x[k] h[n −k]
where x[n] is the input. The Z-transform of y[n] is the product
Y(z) = H(z)X(z)
ROC: RY = RH ∩RX
If the unit circle is included in RY, then
Y(e jω) = H(e jω)X(e jω)
or
|Y(e jω)| = |H(e jω)||X(e jω)|
∠Y(e jω) = ∠H(e jω) + ∠X(e jω)
(10.25)
Remarks
I
Since the system is stable, the ROC of H(z) includes the unit circle, and so if the ROC of X(z) includes
the unit circle, the intersection of these regions will also include the unit circle.
I
We will see later that it is still possible for y[n] to have a DTFT when the input x[n] does not have a
Z-transform with a region of convergence including the unit circle, as when the input is periodic. In this
case the output is also periodic. These signals are not ﬁnite energy, but ﬁnite power, and can be represented
by DTFT containing analog delta functions.
I Example 10.12
Let H(z) be the cascade of ﬁrst-order systems with transfer functions
Hi(z) = Ki
z −1/αi
z −α∗
i
|z| > |αi|, i = 1, . . . , N −1
where |αi| < 1 and Ki > 0. Such a system is called an all-pass system because its magnitude response
is a constant for all frequencies. If the DTFT of the ﬁlter input x[n] is X(e jω), determine the gains {Ki}
so that the magnitude of the DTFT of the output y[n] of the system coincides with the magnitude
of X(e jω).

596
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
Solution
Notice that if 1/αi is a zero of Hi(z), a pole at the conjugate reciprocal α∗
i exists. To show that the
magnitude of Hi(e jω) is a constant for all frequencies, consider the magnitude-squared function
|Hi(e jω)|2 = Hi(e jω)H∗(e jω) = K2
i
(e jω −1/αi)(e−jω −1/α∗
i )
(e jω −α∗
i )(e−jω −αi)
= K2
i
e jω(e−jω −αi)e−jω(e jω −α∗
i )
αiα∗
i (e jω −α∗
i )(e−jω −αi)
= K2
i
|αi|2
Thus, by letting Ki = |αi|, the above gives a unit magnitude. The cascade of the Hi(z) gives a transfer
function of
H(z) =
Y
i
Hi(z) =
Y
i
|αi|z −1/αi
z −αi
so that
H(e jω) =
Y
i
Hi(e jω) =
Y
i
|αi|e jω −1/αi
e jω −αi
|H(e jω)| =
Y
i
|Hi(e jω)| = 1
∠H(e jω) =
X
i
∠Hi(e jω)
which in turn gives
Y(e jω) = |X(e jω)|ej(∠X(e jω)+∠H(e jω))
so that the magnitude of the output coincides with that of the input; however, the phase of Y(e jω)
is the sum of the phases of X(e jω) and H(e jω). Thus, the all-pass system allows all frequency com-
ponents in the input to appear at the output with no change in the magnitude spectrum but with
a phase shift.
I
10.3 FOURIER SERIES OF DISCRETE-TIME PERIODIC SIGNALS
Like in the continuous-time domain, we are interested in ﬁnding the response of an LTI system to
a periodic signal. As in that case, we represent the periodic signal as a combination of complex
exponentials and use the eigenfunction property of LTI systems to ﬁnd the response.
Notice that we are proceeding in the reverse order we followed in the continuous-time case: We are
considering the Fourier representation of periodic signals after that of aperiodic signals. Theoretically,
there is no reason why this cannot be done, but practically it has the advantage of ending with a

10.3 Fourier Series of Discrete-Time Periodic Signals
597
representation for discrete-time periodic signals that is discrete and periodic in frequency so it can be
implemented in a computer. This is the basis of the so-called discrete Fourier transform (DFT), which
is fundamental in digital signal processing (see Section 10.4). An algorithm called the Fast Fourier
Transform (FFT) implements it very efﬁciently (see Chapter 12).
Before ﬁnding the representation of periodic discrete-time signals recall that:
I
A discrete-time signal x[n] is periodic if there is a positive integer N such that x[n + kN] = x[n]
for any integer k. This value N is the smallest positive integer that satisﬁes this condition and it is
called the period of x[n]. For the periodicity to hold we need that x[n] be of inﬁnite support (i.e.,
x[n] must be deﬁned in −∞< n < ∞).
I
According to the eigenfunction property of discrete-time LTI systems, whenever the input to such
systems is a complex exponential Ae j(ω0n+θ) the corresponding output in the steady state is
y[n] = Ae j(ω0n+θ)H(e jω0)
where H(e jω0) is the frequency response of the system at the input frequency ω0. The advantage
of this, as demonstrated in the continuous-time domain, is that if we are able to express the input
signal as a linear combination of complex exponentials, then superposition gives us a linear
combination of the responses to each exponential. Thus, if the input signal is of the form
x[n] =
X
k
A[k]e jωkn
then the output will be
y[n] =
X
k
A[k]e jωknH(e jωk)
This property is valid whether the frequency components of the input signal are harmonically
related (when x[n] is periodic) or not.
I
We showed before that a signal x(t) that is periodic of period T0 can be represented by its Fourier
series
x(t) =
∞
X
k=−∞
ˆX[k]e j 2πkt
T0
(10.26)
If we sample x(t) using a sampling period Ts = T0/N where N is a positive integer, we get
x(nTs) =
∞
X
k=−∞
ˆX[k]ej 2πknTs
T0
=
∞
X
k=−∞
ˆX[k]ej 2πkn
N
The last summation repeats the frequencies between 0 to 2π. To avoid these repetitions, we let
k = m + rN where 0 ≤m ≤N −1 and r = 0, ±1, ±2, . . .—that is, we divide the inﬁnite support

598
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
of k into an inﬁnite number of ﬁnite segments of length N. We then have
x(nTs) =
N−1
X
m=0
∞
X
r=−∞
ˆX[m + rN]e j 2π(m+rN)n
N
=
N−1
X
m=0
"
∞
X
r=−∞
ˆX[m + rN]
#
e j 2πmn
N
=
N−1
X
m=0
X[m]e j 2πmn
N
This representation is in terms of complex exponentials with frequencies 2πm/N, m = 0, . . . ,
N −1, from 0 to 2π(N −1)/N. It is this Fourier series representation that we will develop
next.
Circular Representation of Discrete-Time Periodic Signals
Considering that the period N of a periodic signal x[n] and the samples in a ﬁrst period x1[n]
completely characterize a periodic signal x[n], a circular rather than a linear representation would
more efﬁciently represent the signal. The circular representation is obtained by locating uniformly
around a circle the values of the ﬁrst period starting with x[0] and putting in a clockwise direc-
tion the remaining terms x[1], . . . , x[N −1]. Continuing in the clockwise direction are the values
x[N] = x[0], x[N + 1] = x[1], . . . , x[2N −1] = x[N −1], and so on. In general, any value x[m] where
m is represented as
m = kN + r
for integers k, the exact divisor of m by N, and the residue 0 ≤r < N, equals one of the samples in
the ﬁrst period—that is,
x[m] = x[kN + r] = x[r]
This representation is called circular in contrast to the equivalent linear representation introduced
before:
x[n] =
∞
X
k=−∞
x1[n + kN]
which superposes shifted versions of the ﬁrst period. The circular representation becomes very useful
in the computation of the DFT, as we will see later in this chapter.
Figure 10.8 shows the circular and the linear representations of a periodic signal x[n] of
period N = 4.

10.3 Fourier Series of Discrete-Time Periodic Signals
599
x[0], x [4],...
x[1], x[5],...
x [2], x[6],...
x[3], x[7],...
· · ·
· · ·
n
0 1
2
3
4
5
6
N  = 4
x[n]
x [5]
x [6]
x[2]
x[4]
x [3]
x [1]
x[0]
(a)
(b)
FIGURE 10.8
(a) Circular and (b) linear representation of a periodic discrete-time signal x[n] of period N = 4. Notice how the
circular representation shows the periodicity x[0] = x[4], . . . , x[3] = x[7], . . . for positive as well as negative
integers.
10.3.1 Complex Exponential Discrete Fourier Series
Consider the representation of a discrete-time signal x[n] periodic of period N, using the orthog-
onal functions {φ[k, n] = e j2πkn/N} for n, k = 0, . . . , N −1. Two important characteristics of these
functions are:
I
The functions {φ[k, n]} are periodic with respect to k and n with period N. In fact,
φ[k + ℓN, n] = e j 2π(k+ℓN)n
N
= e j 2πkn
N e j2πℓn
= e j 2πkn
N
where we used that e j2πℓn = 1. It can be equally shown that the functions {φ(k, n)} are periodic
with respect to n with a period N.
I
The functions {φ(k, n)} are orthogonal with respect to n—that is,
N−1
X
n=0
e j 2π
N kn(e j 2π
N ℓn)∗=
N
if k −ℓ= 0
0
if k −ℓ̸= 0
and can be normalized by dividing them by
√
N. So {φ[k, n]/
√
N} are orthonormal functions.
These two properties will be used in obtaining the Fourier series representation of periodic discrete-
time signals.

600
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
The Fourier series representation of a periodic signal x[n] of period N is
x[n] =
k0+N−1
X
k=k0
X[k]e j 2π
N kn
(10.27)
where the Fourier series coefﬁcients {X[k]} are obtained from
X[k] = 1
N
n0+N−1
X
n=n0
x[n]e−j 2π
N kn
(10.28)
The frequency ω0 = 2π/N rad is the fundamental frequency, and k0 and n0 in Equations (10.27) and (10.28)
are arbitrary integer values. The Fourier series coefﬁcients X[k], as functions of frequency 2πk/N, are periodic
of period N.
Remarks
I
The connection of the above two equations can be veriﬁed by using the orthonormality of the {φ[k, n]/
√
N}
functions. In fact, if we multiply x[n] by e−j(2π/N)ℓn and sum these values for n changing over a period,
using Equation (10.27) we get:
X
n
x[n]e−j2πnℓ/N =
X
n
X
k
X[k]e j2π(k−ℓ)n/N
=
X
k
X[k]
X
n
e j2π(k−ℓ)n/N = NX[ℓ]
since P
n e j2π(k−ℓ)n/N is zero, except when k −ℓ= 0, in which case the sum is equal to N.
I
Both x[n] and X[k] are periodic with respect to n and k of the same period N, as can be easily shown
using the periodicity of the functions {φ(k, n)}. Consequently, the sum over k in the Fourier series and the
sum over n in the Fourier coefﬁcients are computed over any period of x[n] and X[k]. Thus, the sum in the
Fourier series can be computed in any period, or from k = k0 to k0 + N −1 for any value of k0. Likewise,
the summation in the computation of the Fourier coefﬁcients goes from n = n0 to no + N −1, which is
an arbitrary period for any integer value n0.
I
Notice that both x[n] and X[k] can be computed with a computer since the frequency is discrete and only
sums are needed to implement them. We will use these characteristics in the practical computation of the
Fourier transform of discrete-time signals, or DFT.
I Example 10.13
Find the Fourier series of a periodic signal
x[n] = 1 + cos(2πn/4) + sin(2πn/2)
−∞< n < ∞

10.3 Fourier Series of Discrete-Time Periodic Signals
601
Solution
The period of x[n] is N = 4. Indeed,
x[n + 4] = 1 + cos(2π(n + 4)/4) + sin(2π(n + 4)/2)
= 1 + cos(2πn/4 + 2π) + sin(2πn/2 + 4π) = x[n]
The frequencies in x[n] are: a DC frequency, corresponding to the constant, and frequencies
ω0 = 2π/4 = π/2 and ω1 = 2π/2 = 2ω0, corresponding to the cosine and the sine. No other
frequencies are present in the signal. The fundamental frequency is ω0 = 2π/N = π/2, and the
complex exponential Fourier series can be obtained directly from x[n] using Euler’s equation:
x[n] = 1 + 0.5(e jπn/2 + e−jπn/2) −0.5j(e jπn −e−jπn)
= X[0] + X[1]e jω0n + X[−1]e−jω0n + X[2]e j2ω0n + X[−2]e−j2ω0n
ω0 = π
2
so
that
the
Fourier
series
coefﬁcients
are
X[0] = 1,
X[1] = X∗[−1] = 0.5,
and
X[2] =
X∗[−2] = −0.5j.
I
10.3.2 Connection with the Z-Transform
Recall that the Laplace transform was used to ﬁnd the Fourier series coefﬁcients. Likewise, for periodic
discrete-time signals we will show that the Z-transform of a period of the signal, which always exists,
can be connected with the Fourier series coefﬁcients.
If x1[n] = x[n](u[n] −u[n −N]) is a period of a periodic signal x[n] of period N, its Z-transform
Z(x1[n]) =
N−1
X
n=0
x[n]z−n
has the whole plane, except for the origin, as its region of convergence. The Fourier series coefﬁcients of x[n]
are thus determined as
X[k] = 1
N
N−1
X
n=0
x[n]e−j 2π
N kn
= 1
N Z(x1[n])
z=e j 2π
N k
(10.29)
I Example 10.14
Consider a discrete pulse x[n] with a period N = 20, and x1[n] = u[n] −u[n −10] is the period
between 0 and 19. Find the Fourier series of x[n].

602
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
Solution
The Fourier series coefﬁcients are found as (ω0 = 2π/20 rad)
X[k] = 1
20Z(x1[n])
z=e j 2π
20 k = 1
20
9
X
n=0
z−n z=e j 2π
20 k = 1
20
1 −z−10
1 −z−1
z=e j 2π
20 k
A close expression for X[k] is obtained as follows:
X[k] =
z−5(z5 −z−5)
20z−0.5(z0.5 −z−0.5)
z=e j 2π
20 k
=
e−jπk/2 sin(πk/2)
20e−jπk/20 sin(πk/20)
= e−j9πk/20
20
sin(πk/2)
sin(πk/20)
I
10.3.3 DTFT of Periodic Signals
A discrete-time periodic signal x[n] of period N with a Fourier series representation of
x[n] =
X
k
X[k]e j2πnk/N
(10.30)
has a DTFT
X(e jω) =
X
k
2πX[k]δ(ω −2πk/N)
−π ≤ω < π
(10.31)
If we let F(.) indicate the DTFT, the DTFT of a periodic signal x[n] is
X(e jω) = F
 x[n]

= F
 X
k
X[k]e j2πnk/N
!
=
X
k
F

X[k]e j2πnk/N
=
X
k
2πX[k]δ(ω −2πk/N)
−π ≤ω < π
where δ(ω) is the analog delta function since ω varies continuously.
I Example 10.15
The periodic signal
δM[n] =
∞
X
m=−∞
δ[n −mM]
has a period M. Find its DTFT.

10.3 Fourier Series of Discrete-Time Periodic Signals
603
Solution
The DTFT of δM[n] is given by
1M(e jω) =
∞
X
m=−∞
F(δ[n −mM]) =
∞
X
m=−∞
e−jωmM
(10.32)
An equivalent result can be obtained if before we ﬁnd the DTFT we ﬁnd the Fourier series of δM[n].
The coefﬁcients of the Fourier series of δM[n] are
1
M
M−1
X
n=0
δM[n]e−j2πnk/M = 1
M
M−1
X
n=0
δ[n]e−j2πnk/M = 1
M
so that the Fourier series of δM[n] is
δM[n] =
M−1
X
k=0
1
Me j2πnk/M
and its DTFT is then given by
1M(e jω) =
M−1
X
k=0
F
 1
Me j2πnk/M

= 2π
M
M−1
X
k=0
δ

ω −2πk
M

−π ≤ω < π
(10.33)
Notice the equivalence of Equations (10.32) and (10.33).
Putting the pair δM[n] and 1M(e jω) together gives an interesting relation:
∞
X
m=−∞
δ[n −mM] ⇔2π
M
M−1
X
k=0
δ

ω −2πk
M

π ≤ω < π
Both terms are discrete in time and in frequency, and both are periodic. The period of δM[n] is
M and the one of 1(e jω) is 2π/M. Furthermore, the DTFT of an impulse train in time is also an
impulse train in frequency. However, it should be observed that the delta functions δ[n −mM] on
the left term are discrete, while the ones on the right term, δ(ω −2πk/M), are continuous.
I
Computation of the Fourier Series using MATLAB
Given that periodic signals only have discrete frequencies, and that the Fourier series coefﬁcients
are obtained using summations, the computation of the Fourier series can be implemented with a
frequency discretized version of the DTFT, or the DFT, which can be efﬁciently computed using the
FFT algorithm. To illustrate this using MATLAB, consider three different signals as indicated in the
following script and displayed in Figure 10.9.
Each of the signals is periodic and we consider 10 periods to compute their FFTs, which provides
the Fourier series coefﬁcients (only 3 periods are displayed in Figure 10.9). A very important issue to
remember is that one needs to input exactly one or more periods, and that the FFT length must be

604
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
FIGURE 10.9
Computation of
Fourier series
coefﬁcients of different
periodic signals: the
corresponding
magnitude line
spectrum for each
signal is shown on
the right.
0
10
20
30
0
10
20
30
0
10
20
30
−1
0
1
x[n]
x1[n]
x2[n]
−1
−0.5
0
0.5
−1
−0.5
0
0.5
−1
−0.5
0
0.5
0
0.2
0.4
−1
0
1
0
0.2
0.4
0.6
−1
0
1
n
0
0.1
0.2
ω/π
|X (e jω)|
|X1(e jω)|
|X2(e jω)|
that of a period or of multiples of a period. Notice that the MATLAB function sign is used to generate
a periodic train of pulses from the cosine function. The need to divide by the number of periods used
will be discussed later in section 10.4.3.
%%%%%%%%%%%%%%%%%%%%%%%%%
% Fourier series using FFT
%%%%%%%%%%%%%%%%%%%%%%%%%
N = 10; M = 10; N1 = M ∗N;n = 0:N1 - 1;
x = cos(2 ∗pi ∗n/N); % sinusoid
x1 = sign(x); % train of pulses
x2 = x - sign(x); % sinusoid minus train of pulses
X = fft(x)/M;X1 = fft(x1)/M;X2 = fft(x2)/M; % ffts of signals
X = X/N;X1 = X1/N;X2 = X2/N; % FS coefﬁcients
10.3.4 Response of LTI Systems to Periodic Signals
Let x[n], a periodic signal of period N, be the input of an LTI system with transfer function H(z). If the Fourier
series of x[n] is
x[n] =
N−1
X
k=0
X[k]ej(kω0)n
ω0 = 2π
N
fundamental frequency

10.3 Fourier Series of Discrete-Time Periodic Signals
605
then according to the eigenfunction property of LTI systems, the output is also periodic of period N with
Fourier series
y[n] =
N−1
X
k=0
X[k]H(ejkω0)ejkω0n
ω0 = 2π
N
fundamental frequency
and coefﬁcients Y[k] = X[k]H(ejkω0). That is, it is affected by the frequency response of the system
H(ejkω0) = H(z)
z=e jkω0
at the harmonic frequencies {kω0, k = 0, . . . , N −1}.
Remarks
I
Although the input x[n] and the output y[n] of the LTI system are both periodic of the same period, the
Fourier series coefﬁcients of the output are affected by the frequency response H(ejkω0) of the system at
each of the harmonic frequencies.
I
A similar result is obtained by using the convolution property of the DTFT, so that if X(e jω) is the DTFT
of the periodic signal x[n], then the DTFT of the output y[n] is given by
Y(e jω) = X(e jω)H(e jω)
=
"N−1
X
k=0
2πX[k]δ(ω −2πk/N)
#
H(e jω)
=
N−1
X
k=0
2πX[k]H(e j2πk/N)δ(ω −2πk/N)
and letting X[k]H(e j2πk/N) = Y[k], we get the DTFT of the periodic output y[n].
I Example 10.16
Consider how to implement a crude spectral analyzer for discrete-time signals using MATLAB.
Divide the discrete frequencies [0 π] in three bands: [0 0.1π], (0.1π 0.6π], and (0.6π π], to obtain
low-pass, band-pass, and high-pass components of the signal x[n] = sign(cos(0.2πn)). Use the
MATLAB function ﬁr1 to design the three ﬁlters. Plot the original signal and its components in
the three bands. Verify that the overall ﬁlter is an all-pass ﬁlter. Obtain the sum of the outputs of
the ﬁlters, and explain how it relates to the original signal.
Solution
The script for this example uses several MATLAB functions that facilitate the ﬁltering of signals, and
for which you can get more information by using help.
The DTFT of x[n] is found using the FFT algorithm as indicated before. The low-pass, band-
pass, and high-pass ﬁlters are obtained using ﬁr1, and the function ﬁlter allows us to obtain the

606
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
corresponding outputs y1[n], y2[n], and y3[n]. The frequency responses {Hi(e jω)}, i = 1, 2, 3 of the
ﬁlters are found using the function freqz.
The three ﬁlters separate x[n] into its low-, middle-, and high-band components from which we
are able to obtain approximately the power of the signal in these three bands—that is, we have a
crude spectral analyzer. Ideally, we would like the sum of the ﬁlters outputs to be equal to x[n],
with some delay, and so the sum of the frequency responses
H(e jω) = H1(e jω) + H2(e jω) + H3(e jω)
should be the frequency response of an all-pass ﬁlter. Indeed, that is the result shown in
Figure 10.10, where we obtain the input signal as the output of the ﬁlter with transfer function
H(z), delayed 15 samples (which corresponds to half the order of the ﬁlters used).
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
ω /π
ω /π
0
5
10
15
20
25
30
35
40
45
−1
0
1
n
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
x[n]
|Hi (ω)|
|X(e jω)|
(a)
0
5
10
15
20
25
30
35
40
45
50
−0.2
−0.1
0
0.1
0.2
0
5
10
15
20
25
30
35
40
45
50
−0.5
0
0.5
0
5
10
15
20
25
30
35
40
45
50
−2
0
2
n
y1[n]
y2[n]
y3[n]
(b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.2
0.4
0.6
0.8
1
ω/π
0
5
10
15
20
25
30
35
40
45
50
−1
−0.5
0
0.5
1
n
|H (ω)|
y [n]
(c)
FIGURE 10.10
A crude spectral analyzer: (a) magnitude response of low-pass, band-pass, and high-pass ﬁlters; input signal
and its magnitude spectrum; (b) outputs of ﬁlters; (c) overall magnitude response of the bank of ﬁlters, an
all-pass ﬁlter, and overall response. Delay is due to linear phase of the bank of ﬁlters.

10.3 Fourier Series of Discrete-Time Periodic Signals
607
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.16---Filtering of a periodic signal
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
N = 500;n = 0: N - 1; x = cos(0.2 ∗pi ∗n); x = sign(x); % pulse signal
X = fft(x)/50; X = X(1:250); % approximate DTFT of signal using fft
L = 500; w1 = 0:2 ∗pi/L:pi - 2 ∗pi/L;w1 = w1/pi; % range of frequencies
h1 = ﬁr1(30,0.1); % low-pass ﬁlter
h2 = ﬁr1(30,0.6,‘high’); % high-pass ﬁlter
h3 = ﬁr1(30,[0.1 0.6]); % band-pass ﬁlter
y1 = ﬁlter(h1,1,x); y2 = ﬁlter(h2,1,x); y3 = ﬁlter(h3,1,x);
y = y1 + y2 + y3; % outputs of ﬁlters
[H1,w] = freqz(h1,1); [H2,w] = freqz(h2,1); [H3,w] = freqz(h3,1);
H = H1 + H2 + H3; % frequency responses
I
10.3.5 Circular Shifting and Periodic Convolution
Circular Shifting
When a periodic signal x[n] of period N is shifted by M samples the signal is still periodic. The
circular representation provides the appropriate visualization of this shift, as it concentrates on the
period displayed by the representation. Values are rotated circularly.
The Fourier series of the shifted signal x1[n] = x[n −M] is obtained from the Fourier series of x[n] by
replacing n by n −M to get
x1[n] = x[n −M] =
X
k
X[k]e j2π(n−M)k/N
=
X
k

X[k]e−j2πMk/N
e j2πnk/N
so that the shifted signal and its Fourier series coefﬁcients are related as
x[n −M] ⇔X[k]e−j2πMk/N
(10.34)
It is important to consider what happens for different values of M. This shift can be represented as
M = mN + r,
m = 0, ±1, ±2, . . . , 0 ≤r ≤N −1
and as such
e−j2πMk/N = e−j2π(mN+r)k/N = e−j2πrk/N
(10.35)
for any value of M, so that shifting by more than a period is equivalent to shifting by the residue r of
dividing the shift M by N.

608
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
I Example 10.17
To visualize the difference between a linear shift and a circular shift consider the periodic signal
x[n] of period N = 4 with a ﬁrst period
x1[n] = n
n = 0, . . . , 3
Plot x[−n] and x[n −1] as functions of n using the linear and the circular representations.
Solution
In the circular representation of x[n], the samples x[0], x[1], x[2], and x[3] of the ﬁrst period are
located in a clockwise direction in the E(ast), S(outh), W(est), and N(orth) directions in the circle.
Considering the E direction the origin of the representation, circular shifting is done analogously
to linear shifting with respect to the origin. Delaying by M in the circular representation corre-
sponds to shifting circularly, or rotating, M positions in the clockwise direction. Advancing by M
corresponds to shifting circularly M positions in the counterclockwise direction. Reﬂection corre-
sponds to reversing the order of the samples of x1[n] to E, N, W, and S (i.e., placing the entries of
x1[n] counterclockwise starting with x[0] in E).
The circular representation of x[−n] starts with x[0] in the ﬁrst quadrant and then follows with x[1],
x[2], and x[3] in a counterclockwise direction (Figure 10.11). Looking at the circular representation
in a clockwise direction we obtain the linear representation that as expected coincides with the
reﬂection of x[n].
FIGURE 10.11
Circular representation of x[−n]
and x[n −1].
0
0
1
1
2
2
3
3
x[−n]
n
…
…
0
0
1
1
2
2
3
3
x [n −1]
n
…
…
x[1]
x [2]
x[0]
x [3]
x [1]
x[2]
x [3]
x[0]

10.3 Fourier Series of Discrete-Time Periodic Signals
609
For the circular representation of x[n −1] we shift the term x[0] from the E to the S in the clockwise
direction and shift the others the same angle to get the circular representation of x[n −1]—
a circular shift of one. Shifting linearly by one, we obtain the equivalent representation of
x[n −1].
I
Periodic Convolution
Consider then the multiplication of two periodic signals x[n] and y[n] of the same period N. The
product v[n] = x[n]y[n] is also periodic of period N, and its Fourier series coefﬁcients are
V[m] =
N−1
X
k=0
X[k]Y[m −k]
0 ≤m ≤N −1
as we will show next. That v[n] is periodic of period N is clear. Its Fourier series is found by letting
the fundamental frequency be ω0 = 2π
N and replacing the given Fourier coefﬁcients,
v[n] =
N−1
X
m=0
V[m]e jω0nm =
N−1
X
m=0
N−1
X
k=0
X[k]Y[m −k]e jω0nm
=
N−1
X
k=0
X[k]
 N−1
X
m=0
Y[m −k]e jω0n(m−k)
!
e jω0kn =
N−1
X
k=0
X[k]y[n]e jω0kn = y[n]x[n]
Thus, we have that the Fourier series coefﬁcients of the product of two periodic signals of the same
period give
x[n]y[n] ⇔
N−1
X
k=0
X[k]Y[m −k]
(10.36)
and by duality
N−1
X
k=0
x[k]y[n −k] ⇔NX[k]Y[k]
(10.37)
Although
N−1
X
k=0
x[k]y[n −k] and
N−1
X
k=0
X[k]Y[m −k]
look like the convolution sums we had before, the periodicity of the sequences makes them different.
These are called periodic convolution sums. Given the inﬁnite support of periodic signals, the convolu-
tion sum of periodic signals does not exist—it would not be ﬁnite. The periodic convolution is done
only for a period of the signals.

610
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
Remarks
I
As before, multiplication in one domain causes convolution in the other domain.
I
In computing a periodic convolution we need to remember that: (1) the convolving sequences must have
the same period, and (2) the Fourier series coefﬁcients of a periodic signal share the same period with the
signal.
I Example 10.18
To understand how the periodic convolution sum results, consider the product of two periodic
signals x[n] and y[n] of period N = 2. Find the Fourier series of their product v[n] = x[n]y[n].
Solution
The multiplication of the Fourier series
x[n] = X[0] + X[1]e jω0n
y[n] = Y[0] + Y[1]e jω0n
ω0 = 2π/N = π
can be seen as a product of two polynomials in complex exponentials ζ[n] = e jω0n so that
x[n]y[n] = (X[0] + X[1]ζ[n])(Y[0] + Y[1]ζ[n])
= X[0]Y[0] +
 X[0]Y[1] + X[1]Y[0]

ζ[n] + X[1]Y[1]ζ 2[n]
Now ζ 2[n] = e j2ω0n = e j2πn = 1 so that
x[n]y[n] =
 X[0]Y[0] + X[1]Y[1]

|
{z
}
V[0]
+
 X[0]Y[1] + X[1]Y[0]

|
{z
}
V[1]
e jω0n = v[n]
after replacing ζ[n]. When using the periodic convolution formula, we have
V[0] =
1
X
k=0
X[k]Y[−k] = X[0]Y[0] + X[1]Y[−1] = X[0]Y[0] + X[1]Y[2 −1]
V[1] =
1
X
k=0
X[k]Y[1 −k] = X[0]Y[1] + X[1]Y[0]
where in the upper equation we used the periodicity of Y[k] so that Y[−1] = Y[−1 + N] = Y[−1 +
2] = Y[1]. Thus, the multiplication of periodic signals can be seen as the product of polynomials
in ζ[n] = e jω0n = e j2πn/N such that
ζ pN+m[n] = e j 2π
N (pN+m) = e j 2π
N m = ζ m[n]
p = ±1, ±2, . . . , 0 ≤m ≤N −1
which ensures that the resulting polynomial is always of order N −1.
Graphically, we proceed in a manner analogous to the convolution sum (see Figure 10.12) by
representing X[k] and Y[m −k] circularly, and shifting Y[m −k] clockwise by m = 0 and m = 1,

10.3 Fourier Series of Discrete-Time Periodic Signals
611
FIGURE 10.12
Periodic convolution of the Fourier
series coefﬁcients X[k] and Y[k].
m = 1
m = 0
X[1]
X[0]
Y[1]
Y[0]
X[1]
X [0]
Y [1]
Y[0]
V[0] = X[0]Y[0] + X [1]Y[1]
V[1] = X [0]Y[1] + X [1]Y[0]
while keeping X[k] stationary. The circular representation of X[k] is given by the internal circle
with the values of X[0] and X[1] in the clockwise direction, while Y[m −k] is represented in the
outer circle with the two values of a period in the counterclockwise direction (corresponding to
the reﬂection of the signal or Y[−k] for m = 0). Multiplying the values opposite to each other and
adding them we get V[0] = X[0]Y[0] + X[1]Y[1]. If we shift the outer circle 180 degrees clock-
wise for m = 1 and multiply the values opposite to each other and add their product, we get
V[1] = X[0]Y[1] + X[1]Y[0]. There is no need for further shifting, as the results would coincide
with the ones obtained before. The process is similar to the linear convolution but implemented
circularly.
I
For periodic signals x[n] and y[n] of period N, we have
(a)
Duality in time and frequency circular shifts: The Fourier series coefﬁcients of the signals on the left are
the terms on the right:
x[n −M] ⇔X[k]e−j2πMk/N
x[n]e j2πMn/N ⇔X[k −M]
(10.38)
(b)
Duality in multiplication and periodic convolution sum: The Fourier series coefﬁcients of the signals on
the left are the terms on the right:
z[n] = x[n]y[n] ⇔Z[k] =
N−1
X
m=0
X[m]Y[k −m]
v[n] =
N−1
X
m=0
x[m]y[n −m] ⇔V[k] = NX[k]Y[k]
(10.39)

612
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
I Example 10.19
A periodic signal x1[n] of period N = 4 has a period
x1[n] =
1
n = 0, 1
0
n = 2, 3
Suppose that we want to ﬁnd the periodic convolution sum of x[n] with itself—call it v[n]. Let then
y[n] = x[n −2], and ﬁnd the periodic convolution sum of x[n] and y[n]—call it z[n]. How does v[n]
compare with z[n]?
Solution
The circular representation of x[n] is shown in Figure 10.13. To ﬁnd the periodic convolution
sum we consider a period x1[n] and represent the stationary signal by the internal circle, and the
circularly shifted signal by the outside circle. Multiplying the values in each of the spokes and
adding them we ﬁnd the values of a period of v[n], which is given by
v1[n] =



1
n = 0
2
n = 1
1
n = 2
0
n = 3
Analytically, the Fourier series coefﬁcients of v[n] are V[k] = N(X[k])2 = 4(X[k])2. Using the
Z-transform, X1(z) = 1 + z−1 so that X2
1(z) = 1 + 2z−1 + z−2 and
V[k] = 4 X2
1(z)
4 × 4|z=e j2πk/4
= 1
4(1 + 2e−j2πk/4 + e−j2π2k/4) = 1
4(1 + 2e−jπk/2 + e−jπk)
This can be veriﬁed by using the period obtained from the periodic convolution sum so that
V[k] = 1
N
N−1
X
n=0
v[n]e−j2πnk/N = 1
4(1 + 2e−j2πk/4 + e−j2π2k/4)
which equals the above expression.
Graphically, the periodic convolution of x[n] and y[n] is shown in Figure 10.13(c) where the sta-
tionary signal is chosen as x[m], represented by the inner circle, and the circularly shifted signal
is chosen as y[n −m], represented by the outer circle. The result of the convolution is a periodic
signal z[n] of period
z1[n] =



1
n = 0
0
n = 1
1
n = 2
2
n = 3

10.3 Fourier Series of Discrete-Time Periodic Signals
613
(a)
0
1
1
2
3
4
5
n
x[n]
…
…
(b)
1
0
1
2
2
3
n
v[n]
…
…
1
1
0
0, 0, 1, 1
1, 0, 0, 1
0, 1, 1, 0
1, 1, 0, 0
0
1
1
0
0
x [n]
(c)
n
1
2
0
1
2
3
z [n]
…
…
0, 0, 1, 1
1, 0, 0, 1
0, 1, 1, 0
1, 1, 0, 0
1
1
0
0
FIGURE 10.13
Periodic convolution sum of x[n] with itself to get v[n]: (a) linear and circular representations of x[n]; (b) periodic
convolution sum giving v[n]. (c) Circular representation of periodic convolution sum of x[n] and y[n] = x[n −2],
the result is z[n] = v[n −2].
As before, the Fourier series coefﬁcients of z[n] are given by
Z[k] = 4X1(z)Y1(z)
4 × 4
|z=e j2πk/4 = z−2 + 2z−3 + z−4
4
|z=e j2πk/4
= 1
4(e−j2π 2k/4 + 2e−j2π 3k/4 + e−j2π 4k/4) = 1
4(1 + e−j2π 2k/4 + 2e−j2π 3k/4)

614
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
which coincides with the Z[k] obtained by the periodic convolution, which is given by
Z[k] = 1
N
N−1
X
n=0
z[n]e−j2πnk/N = 1
4(1 + z−2 + 2z−3)|z=e j2πk/4 = 1
4(1 + e−j2π 2k/4 + 2e−j2π 3k/4)
I
10.4 DISCRETE FOURIER TRANSFORM
Recall that the direct and the inverse DTFTs corresponding to a discrete-time signal x[n] are
X(e jω) =
X
n
x[n]e−jωn
−π ≤ω < π
x[n] = 1
2π
π
Z
−π
X(e jω)e jωndω
These equations have the following computational disadvantages:
I
The frequency ω varies continuously from −π to π, and as such computing X(e jω) needs to be
done for an uncountable number of frequencies.
I
The inverse DTFT requires integration that cannot be implemented exactly in a computer.
To resolve these issues we consider the discrete Fourier transform or DFT (notice the name difference
with respect to the DTFT), which is computed at discrete frequencies and its inverse does not require
integration. Moreover, the DFT is efﬁciently implemented using an algorithm called the Fast Fourier
Transform (FFT).
The development of the DFT is based on the representation of periodic discrete-time signals. Both
the signal and the Fourier coefﬁcients are periodic of the same period. Thus, the representation of
discrete-time periodic signals is discrete in both time and frequency. We need then to consider how
to extend aperiodic signals into periodic signals, with an appropriate period, to obtain their DFTs.
10.4.1 DFT of Periodic Discrete-Time Signals
A periodic signal ˜x[n] of period N is represented by N values in a period. Its discrete Fourier series is
˜x[n] =
N−1
X
k=0
˜X[k]e jω0nk
0 ≤n ≤N −1
(10.40)
where ω0 = 2π/N is the fundamental frequency. The coefﬁcients { ˜X[k]} correspond to harmonic fre-
quencies {kω0} for 0 ≤k ≤N −1, so that ˜x[n] has no frequency components at any other frequencies.
Thus, ˜x[n] and ˜X[k] are both discrete and periodic of the same period N. Moreover, the Fourier series

10.4 Discrete Fourier Transform
615
coefﬁcients can be calculated using the Z-transform as
˜X[k] = 1
NZ[˜x1[n]]
z=e jkω0
= 1
N
N−1
X
n=0
˜x[n]e−jω0nk
0 ≤k ≤N −1, ω0 = 2π/N
(10.41)
where ˜x1[n] = ˜x[n]W[n] is a period of ˜x[n] and W[n] is a rectangular window—that is,
W[n] = u[n] −u[n −N] =
(
1
0 ≤n ≤N −1
0
otherwise
Thus, the periodic signal ˜x[n] can be expressed as
˜x[n] =
∞
X
r=−∞
˜x1[n + rN]
(10.42)
Although one could call Equation (10.41) the DFT of the periodic signal ˜x[n] and Equation (10.40)
the corresponding inverse DFT, traditionally the DFT of ˜x[n] is N ˜X[k], or
X[k] = N ˜X[k] =
N−1
X
n=0
˜x[n]e−jω0nk
0 ≤k ≤N −1, ω0 = 2π/N
(10.43)
and the inverse DFT is
˜x[n] = 1
N
N−1
X
k=0
X[k]e jω0nk
0 ≤n ≤N −1
(10.44)
Equations (10.43) and (10.44) show that the representation of periodic signals is completely discrete:
summations instead of integrals and discrete rather than continuous frequencies. Thus, the DFT and
its inverse can be evaluated by computer.
Given a periodic signal x[n] of period N, its DFT is given by
X[k] =
N−1
X
n=0
x[n]e−j2πnk/N
0 ≤k ≤N −1
(10.45)
Its inverse DFT is
x[n] = 1
N
N−1
X
k=0
X[k]e j2πnk/N
0 ≤n ≤N −1
(10.46)
Both X[k] and x[n] are periodic of the same period N.

616
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
10.4.2 DFT of Aperiodic Discrete-Time Signals
We obtain the DFT of an aperiodic signal y[n] by sampling its DTFT, Y(e jω), in frequency. Suppose
we choose {ωk = 2πk/L, k = 0, . . . , L −1} as the sampling frequencies, where an appropriate value
for the integer L > 0 needs to be determined. Analogous to the sampling-in-time we did before,
sampling-in-frequency generates a periodic signal in time:
˜y[n] =
∞
X
r=−∞
y[n + rL]
(10.47)
Now, if y[n] is of ﬁnite length N, then when L ≥N the periodic expansion ˜y[n] clearly displays a ﬁrst
period equal to the given signal y[n] (with some zeros attached at the end when L > N). On the other
hand, if the length L < N the ﬁrst period of ˜y[n] does not coincide with y[n] because of superposition
of shifted versions of it (this corresponds to time aliasing, the dual of frequency aliasing, which occurs
in time sampling).
Assuming y[n] is of ﬁnite length N and that L ≥N, as the dual of sampling in time we then have that
˜y[n] =
∞
X
r=−∞
y[n + rL] ⇔Y[k] = Y(e j2πk/L) =
N−1
X
n=0
y[n]e−j2πnk/L
k = 0, . . . , L −1
(10.48)
The equation on the right is the DFT of y[n]. The inverse DFT is the Fourier series representation of
˜y[n] (normalized with respect to L) or its ﬁrst period
y[n] = 1
L
L−1
X
k=0
Y[k]e j2πnk/L
0 ≤n ≤L −1
(10.49)
where Y[k] = Y(e j2πk/L).
Thus, instead of the frequency aliasing that sampling-in-time causes, we have time-aliasing whenever
the length N of y[n] is greater than the chosen L in the sampling-in-frequency. In practice, the gen-
eration of the periodic extension ˜y[n] is not needed—we just need to generate a period that either
coincides with y[n] when L = N, or when L > N that coincides with y[n] with a sequence of L −N
zeros attached to it (i.e., y[n] is padded with zeros). To avoid time aliasing we do not consider choosing
L < N.
If the signal y[n] is a very long signal, in particular if N →∞, it does not make sense to compute
its DFT, even if we could. Such a DFT would give the frequency content of the whole signal and
since an inﬁnite-length signal could have all types of frequencies its DFT would just give no valuable
information. A possible approach to obtain, over time, the frequency content of a signal with a large
time support is to window it and compute the DFT of each of these segments. Thus, when y[n] is of
inﬁnite length, or its length is much larger than the desired or feasible length L, we use a window
WL[n] of length L, and represent y[n] as the superposition
y[n] =
X
m
ym[n] where ym[n] = y[n]WL[n −mL]
(10.50)

10.4 Discrete Fourier Transform
617
Therefore, by the linearity of the DFT, we have the DFT of y[n] is
Y[k] =
X
m
DFT(ym[n]) =
X
m
Ym[k]
(10.51)
where each Ym[k] provides a frequency characterization of the windowed signal or the local frequency
content of the signal. Practically, this would be more meaningful than ﬁnding the DFT of the whole
signal. Now we have frequency information corresponding to segments of the signal and possibly
evolving over time.
The DFT of an aperiodic signal x[n] of ﬁnite length N is found as follows:
I
Choose an integer L ≥N that is the length of the DFT to be the period of a periodic extension ˜x[n] having
x[n] as a period with padded zeros if necessary.
I
Find the normalized Fourier series representation of ˜x[n],
˜x[n] = 1
L
L−1
X
k=0
˜X[k]ej2πnk/L
0 ≤n ≤L −1
(10.52)
where
˜X[k] =
L−1
X
n=0
˜x[n]e−j2πnk/L
0 ≤k ≤L −1
(10.53)
I
Then,
I
X[k] = ˜X[k] for 0 ≤k ≤L −1 is the DFT of x[n].
I
x[n] = ˜x[n]W[n] where W[n] = u[n] −u[n −L] is a rectangular window of length N, is the IDFT of
X[k]. The IDFT x[n] is deﬁned for 0 ≤n ≤L −1.
10.4.3 Computation of the DFT via the FFT
Although we now have discrete frequencies and use only summations to compute the direct and the
inverse DFT, there are still several issues that should be understood when computing these trans-
forms. Assuming that the given signal is ﬁnite length, or it is made ﬁnite length by windowing, we
have:
I
Efﬁcient computation with the FFT algorithm: A very efﬁcient computation of the DFT is done by
means of the FFT algorithm, which takes advantage of some special characteristics of the DFT, as
we will discuss in Chapter 12. It should be understood that the FFT is not another transformation
but an algorithm to efﬁciently compute DFTs. For now, we will consider the FFT as a black box
that for an input x[n] (or X[k]) gives as output the DFT X[k] (or IDFT x[n]).
I
Causal aperiodic signals: If the given signal x[n] is causal of length N—that is, the samples
{x[n], n = 0, 1, . . . , N −1}
are given—we can proceed to obtain {X[k], k = 0, 1, . . . , N −1} or the DFT of x[n] by means of
an FFT of length L = N. To compute an L > N DFT we simply attach L −N zeros at the end of the

618
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
above sequence and obtain L values corresponding to the DFT of x[n] of length L (why this could
be seen as a better version of the DFT of x[n] is discussed below in frequency resolution).
I
Noncausal aperiodic signals: When the given signal x[n] is noncausal of length N—that is, the
samples
{x[n], n = −n0, . . . , 0, 1, . . . , N −n0 −1}
are given—we need to recall that a periodic extension of x[n] or ˜x[n] was used to obtain its DFT.
This means that we need to create a sequence of N values corresponding to the ﬁrst period of
˜x[n]—that is,
x[0] x[1] · · · x[N −n0 −1]
|
{z
}
causal samples
x[−n0] x[−n0 + 1] · · · x[−1]
|
{z
}
noncausal samples
where as indicated the samples x[−n0] x[−n0 + 1] · · · x[−1] are the values that make x[n] non-
causal. If we wish to consider zeros after x[N −n0 −1] to be part of the signal, so as to obtain a
better DFT transform as we discuss later in frequency resolution, we simply attach zeros between
the causal and noncausal components—that is,
x[0] x[1] · · · x[N −n0 −1]
|
{z
}
causal samples
0 0 · · · 0 0 x[−n0] x[−n0 + 1] · · · x[−1]
|
{z
}
noncausal samples
to compute an L > N DFT of the noncausal signal. The periodic extension ˜x[n] represented
circularly instead of linearly would clearly show the above sequence.
I
Periodic signals: If the signal x[n] is periodic of period N we will then choose L = N (or a multiple
of N) and calculate the DFT X[k] by means of the FFT algorithm. If we use a multiple of the period
(e.g., L = MN for some integer M > 0), we need to divide the obtained DFT by the value M. For
periodic signals we cannot choose L to be anything but a multiple of N as we are really computing
the Fourier series of the signal. Likewise, no zeros can be attached to a period (or periods when
M > 1) to improve the frequency resolution of its DFT—by attaching zeros to a period we distort
the signal.
I
Frequency resolution: When the signal x[n] is periodic of period N, the DFT values are normalized
Fourier series coefﬁcients of x[n] that only exist for the harmonic frequencies {2πk/N}, as no fre-
quency components exist for any other frequencies. On the other hand, when x[n] is aperiodic,
the number of possible frequencies depend on the length L chosen to compute its DFT. In either
case, the frequencies at which we compute the DFT can be seen as frequencies around the unit cir-
cle in the z-plane. In both cases one would like to have a signiﬁcant number of frequencies in the
unit circle so as to visualize the frequency content of the signal well. The number of frequencies
considered is related to the frequency resolution of the DFT of the signal.
I
If the signal is aperiodic we can improve the frequency resolution of its DFT by increasing the
number of samples in the signal without distorting the signal. This can be done by padding
the signal with zeros (i.e., attaching zeros to the end of the signal). These zeros do not change
the frequency content of the signal (they can be considered part of the aperiodic signal) but
permit us to increase the available frequency components of the signal.

10.4 Discrete Fourier Transform
619
I
On the other hand, the harmonic frequencies of a periodic signal of period N are ﬁxed to
2πk/N for 0 ≤k < N. In such a case we cannot pad the given period of the signal with an
arbitrary number of zeros, because such zeros are not part of the periodic signal. As an alter-
native, to increase the frequency resolution of a periodic signal we consider more periods
which give the same harmonic frequencies as for one period, but add zeros in between the
harmonic frequencies when considering more than one period.
I
Frequency scales: When computing the DFT we obtain a sequence of complex values X[k] for
k = 0, 1, . . . , N −1 corresponding to an input signal x[n] of length N. Since each of the k val-
ues corresponds to a discrete frequency 2πk/N the k = 0, 1, . . . , N −1 scale is converted into a
discrete frequency scale [0 2π(N −1)/N] (rad) (the last value is always smaller than 2π to keep
the periodicity in frequency of X[k]) by multiplying the integer scale {0 ≤k ≤N −1} by 2π/N.
Subtracting π to this frequency scale we obtain discrete frequencies [−π π −2π/N] (rad) where
again the last frequency does not coincide with π in order to keep the periodicity of 2π of the
X[k]. We obtain a normalized discrete-frequency scale by dividing the above scale by π so as to
obtain a nonunits normalized scale of [0 1 −2(N −1)/N] or [−1 1 −2/N]. Finally, if the signal
is the result of sampling and we wish to display the analog frequency, we then use the relation
where Ts is the sampling period and fs is the sampling frequency:
 = ω
Ts
= ωfs (rad/sec) or f =
ω
2πTs
= ωfs
2π (Hz)
(10.54)
giving analog scales [0 πfs] (rad/sec) and [0 fs/2] (Hz).
I Example 10.20
Consider the DFT computation via the FFT of a causal signal x[n] = (sin(πn/32))(u[n] −u[n −34])
and of its advanced version x[n + 16]. To improve its frequency resolution compute FFTs of length
N = 512. Explain the difference between computing the FFTs of the causal and the noncausal
signals.
Solution
As indicated above, when computing the FFT of a causal signal the signal is simply inputted into
the function. However, to improve the frequency resolution of the FFT we attach zeros to the signal.
These zeros make it possible to have additional values of the frequency response of the signal, with
no effect on the frequency content of the signal.
For the noncausal signal x[n + 16], we need to recall that the DFTs of an aperiodic signal were
computed by extending the signal into a periodic signal with an arbitrary period N, which exceeds
the length of the signal. Thus, the periodic extension of x[n + 16] can be obtained by creating an
input vector consisting of x[n], n = 0, . . . , 16; N −33 zeros (N being the length of the FFT and 33
the length of the signal) to improve the frequency resolution, and x[n], n = −16, . . . , −1.
In either case, the output of the FFT is available as an array of length N = 512 values. This array
X[k], k = 0, . . . , N −1 can be understood as values of the spectrum at frequencies 2πk/N—that is,

620
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
from 0 to 2π(N −1)/N rad (notice that this value is close to 2π but always smaller than 2π as
needed to display a period of X[k]). We can change this scale into other frequency scales—for
instance, if we wish a scale that considers positive as well as negative frequencies, to the above
scale we subtract π, and if we wish a normalized scale [−1 1), we simply divide the previous scale
by π. When shifting to a [−π π) or [−1 1) frequency scale, the spectrum also needs to be shifted
accordingly—this is done using the fftshift function. To understand this change recall that X[k] is
also periodic of period N.
The following script is used to compute the DFT of x[n] and x[n + 16] given above. The results are
shown in Figure 10.14.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.20---FFFT computation of causal and
%
noncausal signals
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
clear all; clf
N = 512; % order of the FFT
n = 0:N - 1;
% causal signal
x = [ones(1,33) zeros(1,N - 33)]; x = x. ∗sin(pi ∗n/32); % zero-padding
X = fft(x); X = fftshift(X); % fft and its shifting to [-1 1] frequency scale
w = 2 ∗[0:N - 1]./N - 1; % normalized frequencies
n1 = [-9:40]; % time scale
% noncausal signal
xnc = [zeros(1,3) x(1:33) zeros(1,3)]; % noncausal signal
x = [x(17:33) zeros(1,N-33) x(1:16)]; % periodic extension and zero-padding
X = fft(x); X = fftshift(X);
n1 = [-19:19]; % time scale
I
I Example 10.21
Consider improving the frequency resolution of a periodic sampled signal
y(nTs) = 4 cos(2πf0nTs) −cos(2πf1nTs)
f0 = 100 Hz, f1 = 4f0
where the sampling period is Ts = 1/(3f1) sec/sample.
Solution
In the case of a periodic signal, the frequency resolution of its FFT cannot be improved by attaching
zeros. The length of the FFT must be the period or a multiple of the period of the signal. The
following script illustrates how the FFT of the given periodic signal can be obtained by using 4 or
12 periods. As the number of periods increases the harmonic components appear in each case at
exactly the same frequencies, and only zeros in between these ﬁxed harmonic frequencies result
from increasing the number of periods. However, the magnitude frequency response is increasing

10.4 Discrete Fourier Transform
621
FIGURE 10.14
Computation of the FFT
of (a) a causal signal
and (b) a noncausal
signal. Notice that as
expected the magnitude
responses are
equal—only the phase
responses change.
(a)
(b)
x1[n]
x[n]
−20
−15
−10
−5
0
5
10
15
20
0
0.5
1
0
0.5
1
n
|X1(ejω)|
|X (ejω)|
0
5
10
15
5
10
15
20
25
−1
0
−0.5
0
0.5
1
ω/π
−1
−0.5
0
0.5
1
ω /π
−4
−2
0
2
4
<X1(ejω)
−1
−0.5
0
0.5
1
1
ω/π
−1
−0.5
0
0.5
−4
−2
0
2
4
ω /π
<X(ejω)
−5
−10
0
5
10
15
20
25
30
35
40
n
as the number of periods increases. Thus, we need to divide by the number of periods used in
computing the FFT.
Since the signal is sampled, it is of interest to have the frequency scale of the FFTs in hertz, so we
convert the discrete frequency ω (rad) into f (Hz) according to
f =
ω
2πTs
= ωfs
2π

622
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
where fs = 1/Ts is the sampling rate given in samples/second. The results are shown in
Figure 10.15.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.21---Improving frequency resolution of FFFT of periodic signals
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
f0 = 100; f1 = 4 ∗f0; % frequencies in Hz of signal
Ts = 1/(3 ∗f1); % sampling period
t = 0:Ts:4/f0; % time for 4 periods
y = 4 ∗cos(2 ∗pi ∗f0 ∗t) - cos(2 ∗pi ∗f1 ∗t); % sampled signal (4 periods)
M = length(y);
Y = fft(y,M); Y = fftshift(Y)/4; % fft using 4 periods, shifting and normalizing
t1 = 0:Ts:12/f0; % time for 12 periods
y1 = 4 ∗cos(2 ∗pi ∗f0 ∗t1) - cos(2 ∗pi ∗f1 ∗t1); % sampled signal (12 periods)
Y1 = fft(y1);Y1 = fftshift(Y1)/12; % fft using 12 periods, shifting and normalizing
w = 2 ∗[0:M - 1]./M - 1;f = w/(2 ∗Ts); % frequency scale (4 periods)
N = length(y1);
w1 = 2 ∗[0:N - 1]./N - 1;f = w/(2 ∗Ts); % frequency scale (12 periods)
I
10.4.4 Linear and Circular Convolution Sums
The most important property of the DFT is the convolution property, which permits the computation
of the linear convolution sum very efﬁciently by means of the FFT.
Consider the convolution sum that gives the output y[n] of a discrete-time LTI system with impulse
response h[n] and input x[n]:
y[n] =
X
m
x[m]h[n −m]
In frequency, y[n] is the inverse DTFT of the product
Y(e jω) = X(e jω)H(e jω)
Assuming that x[n] has a ﬁnite length M and that h[n] has a ﬁnite length K, then y[n] has a ﬁnite
length N = M + K −1. If we choose a period L ≥N for the periodic extension ˜y[n] of y[n], we would
obtain the frequency-sampled periodic sequence
Y(e jω)|ω=2πk/L = X(e jω)H(e jω)|ω=2πk/L
or the DFT of y[n] as the product of the DFTs of x[n] and h[n]:
Y[k] = X[k]H[k] for k = 0, 1, . . . , L −1
We then obtain y[n] as the inverse DFT of Y[k]. It should be noticed that the L-length DFT of x[n] and
of h[n] requires that we pad x[n] with L −M zeros and h[n] with L −K zeros, so that both X[k] and
H[k] have the same length L and can be multiplied at each k.

10.4 Discrete Fourier Transform
623
FIGURE 10.15
Computation of the
FFT of a periodic
signal using (a) 4 and
(b) 12 periods to
improve the
frequency resolution
of the FFT. Notice
that both magnitude
and phase
responses look alike,
but when we use 12
periods these
spectra look sharper
due to the increase
in the number of
frequency
components added.
−600 −400 −200
0
200
400
600
0
5
10
15
20
25
f(Hz)
|X(f )|
−600 −400 −200
0
200
400
600
−4
−2
0
2
4
f (Hz)
<X (f )
0
5
10
15
20
25
|X (f )|
−600 −400 −200
0
200
400
600
f(Hz)
−600 −400 −200
0
200
400
600
f (Hz)
−4
−2
0
2
4
<X(f )
0
0.02
0.04
0.06
0.08
0.1
0.12
−6
−4
−2
0
2
4
x (nTs)
nTs
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
−6
−4
−2
0
2
4
x (nTs)
nTs
(a)
(b)

624
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
Given x[n] and h[n] of lengths M and K, the linear convolution sum y[n] of length N = M + K −1 can be found
by following these three steps:
I
Compute DFTs X[k] and H[k] of length L ≥N for x[n] and h[n].
I
Multiply them to get Y[k] = X[k]H[k].
I
Find the inverse DFT of Y[k] of length L to obtain y[n].
Although it seems computationally more expensive than performing the direct computation of the convolution
sum, the above approach implemented with the FFT can be shown to be much more efﬁcient.
The above procedure could be implemented by a circular convolution sum in the time domain,
although in practice it is not done due to the efﬁciency of the implementation with FFTs. A circu-
lar convolution uses circular rather than linear representation of the signals being convolved. The
periodic convolution sum introduced before is a circular convolution of ﬁxed length—the period of the
signals being convolved. When we use the DFT to compute the response of an LTI system the length
of the circular convolution is given by the possible length of the linear convolution sum. Thus, if the
system input is a ﬁnite sequence x[n] of length M and the impulse response of the system h[n] has
a length K, then the output y[n] is given by a linear convolution of length M + K −1. The length
L ≥M + K −1 of the DFT Y[k] = X[k]H[k] corresponds to a circular convolution of length L of the
x[n] and h[n] padded with zeros so that both have length L. In such a case the circular and the linear
convolutions coincide.
If x[n] of length M is the input of an LTI system with impulse response h[n] of length K, then
Y[k] = X[k]H[k] ⇔y[n] = (x ⊗L h)[n]
(10.55)
where X[k], H[k], and Y[k] are, respectively, DFTs of length L of the input, the impulse response, and the
output of the LTI system, and ⊗L stands for the circular convolution of length L.
If L is chosen so that L ≥M + K −1, the circular and the linear convolution sums coincide—that is,
y[n] = (x ⊗L h)[n] = (x ∗h)[n]
(10.56)
Remark If we consider the periodic expansions of x[n] and h[n] with period L = M + K −1, we can use
their circular representations and implement the circular convolution as shown in Figure 10.16. Since the
length of the linear convolution or convolution sum, M + K −1, coincides with the length of the circular
convolution, the two convolutions coincide. Given the efﬁciency of the FFT algorithm in computing the DFT,
the convolution is typically done using the DFT as indicated above.
I Example 10.22
To illustrate the connection between the circular and the linear convolution, compute using MAT-
LAB the circular convolution of a pulse signal x[n] = u[n] −u[n −21] of length N = 20 with itself
for different values of its length. Determine the length for which the circular convolution coincides
with the linear convolution of x[n] with itself.

10.4 Discrete Fourier Transform
625
FIGURE 10.16
Circular convolution of length L = 8 of x[n]
and y[n]. The signal x[k] is stationary with a
circular representation given by the inside
circle, while y[n −k] is represented by the
outside circle and rotated in the clockwise
direction. The shown circular convolution
sum corresponds to n = 0.
y [3]
y [2]
y[1]
y [0]
y[7]
y[6]
y [5]
y [4]
x [3] x [2]
x [1]
x [0]
x [7]
x[6]
x [5]
x [4]
n = 0
FIGURE 10.17
Circular versus linear
convolutions: (a) Plot
corresponds to linear
convolution. (b) and (c)
Plots are circular
convolutions wih
L < 2N −1. (d) Plot is
circular convolution with
L > 2N −1 coinciding
with the linear
convolution.
0
10
20
30
40
0
5
10
15
20
z(n)
Linear convolution
0
10
20
30
40
0
5
10
15
20
y (n)
Circular convolution (L = 20)
0
10
20
30
40
0
5
10
15
20
n
Circular convolution (L = 49)
y2(n)
0
10
20
30
40
0
5
10
15
20
n
Circular convolution (L = 30)
y1(n)
(a)
(b)
(c)
(d)
Solution
We know that the length of the linear convolution z[n] = (x ∗x)[n] is N + N −1 = 39. If we use
the function circonv2 shown below to compute the circular convolution of x[n] with itself with
length N < 2N −1, for instance L = 20 as shown in Figure 10.17(b), the result will not equal the
linear convolution. Likewise, if the circular convolution is of length N + 10 = 30 < 2N −1, only
part of the result resembles the linear convolution (see Figure 10.17(c)). If we let the length of the
circular convolution be 2N + 9 = 49 > 2N −1, the result is identical to the linear convolution
(see Figure 10.17(d)). The script is given as follows.

626
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.22---Linear and circular convolution
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
clear all; clf
N = 20; x = ones(1,N);
% linear convolution
z = conv(x,x);z = [z zeros(1,10)];
% circular convolution
y = circonv2(x,x,N);
y1 = circonv2(x,x,N + 10);
y2 = circonv2(x,x,2 ∗N + 9);
Mz = length(z); My = length(y); My1 = length(y1);My2 = length(y2);
y = [y zeros(1,Mz - My)]; y1 = [y1 zeros(1,Mz - My1)]; y2 = [y2 zeros(1,Mz-My2)];
The function circonv2 has as inputs the signals to be convolved and the desired length of the circular
convolution. It computes and multiplies the FFTs of the signals and then ﬁnds the inverse FFT to
obtain the circular convolution. If the desired length of the circular convolution is larger than the
length of each of the signals, the signals are padded with zeros to make them the length of the
circular convolution. The following is the code for this function.
function xy = circonv2(x,y,N)
M = max(length(x),length(y))
if M>N
disp(‘Increase N’)
end
x = [x zeros(1,N - M)];
y = [y zeros(1,N - M)];
% circular convolution
X = fft(x,N); Y = fft(y,N); XY = X. ∗Y;
xy = real(ifft(XY,N));
I
I Example 10.23
A signiﬁcant advantage of using the FFT for computing the DFT is in ﬁltering. Assume that
the signal to ﬁlter consists of the MATLAB ﬁle “laughter.mat,” multiplied by 5, to which a sig-
nal that continuously changes between −0.3 and 0.3 is added. We wish to recover the original
“laughter.mat” signal using a ﬁlter. Use the MATLAB function ﬁr1 to design the required ﬁlter.
Solution
Noticing that since the disturbance 0.3(−1)n is a signal of frequency π, we need a low-pass ﬁlter
with a wide bandwidth so as to get rid of the disturbance while trying to keep the frequency
components of the desired signal. The following script is used to design the desired low-pass ﬁlter,
and to implement the ﬁltering. To compare the results obtained with the FFT we use the function
conv to ﬁnd the output of the ﬁlter in the time domain. The results are shown in Figure 10.18.

10.4 Discrete Fourier Transform
627
FIGURE 10.18
FIR ﬁltering of disturbed
signal. Comparison of results
using conv and fft
functions. (a) Actual and
noisy signal (plotted as a
continuous signal using black
lines); (b) denoised signal
(notice the delay caused by
the FIR ﬁlter); (c) impulse
response and magnitude
response of low-pass FIR
ﬁlter; and (d) error signal
ϵ[n] = y[n] −y1[n] between
output from conv- and
fft-based ﬁltering.
0
10
20
30
40
−0.2
0
0.2
0.4
0.6
0.8
1
n
h[n]
0
20
40
60
80
100
120
140
−1
−0.5
0
0.5
1
n
y[n]
0
20
40
60
80
100
120
140
−2
0
2
4
n
×10−16
ε [n]
0
10
20
30
40
50
60
70
80
90
100
−1
−0.5
0
0.5
1
n
x [n], x1[n]
0
1
2
3
0
0.2
0.4
0.6
0.8
1
ω
|H (e jω)|
(a)
(c)
(b)
(d)

628
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 10.23---Filtering using convolution and FFT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
clear all; clf
N = 100; n = 0:N - 1;
load laughter
x = 5 ∗y(1:N)’; x1 = x + 0.3 ∗(-1).ˆn; % desired signal plus disturbance
h = ﬁr1(40,0.9); [H,w] = freqz(h,1); % low-pass FIR ﬁlter design
% ﬁltering using convolution
y = conv(x,h); % convolution of signal and impulse response of FIR
% computing using FFT
M = length(x) + length(h) - 1; % circular and linear convolutions equal
X = fft(x,M);
H = fft(h,M);
Y = X. ∗H;
y1 = ifft(Y); % output of ﬁltering
I
10.5 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO
FROM HERE?
In this chapter we have considered the Fourier representation of discrete-time signals and systems.
Just as with the Laplace and the Fourier transforms, in the continuous case there is a large class of
discrete-time signals and impulse responses of systems for which we are able to ﬁnd their discrete-
time Fourier transform from their Z-transforms. For signals that are not absolutely summable, the
time-frequency duality and other properties of the transform are used to ﬁnd their DTFTs. Properties
of the DTFT are very similar to those of the Z-transform. Although theoretically useful, the DTFT is
computationally not feasible, due to the continuity of the frequency variable and to the integration
required in the inverse transformation. It is the Fourier series of discrete-time signals that makes
the Fourier representation computationally feasible. In Table 10.1, the DTFT of common signals and
some DTFT properties are given.
The Fourier series coefﬁcients constitute a periodic sequence of the same period as the signal; thus
both are periodic. Moreover, the Fourier series and its coefﬁcients are obtained as sums, and the
frequency used is discretized. Thus, they can be obtained by computer. To take advantage of this,
the spectrum of an aperiodic signal resulting from the DTFT is sampled so that in the time domain
there is a periodic repetition of the original signal. For ﬁnite-support signals we can then obtain a
periodic extension that gives the discrete Fourier transform or DFT. The signiﬁcance of this result is
that we have frequency representations of discrete-time signals that are computed algorithmically.
Table 10.2 displays properties of the discrete Fourier series and of the Discrete Fourier Transform
(DFT). What remains then is to take a look at the algorithm used for those computations or the
fast Fourier transform (FFT). We will do that in Chapter 12, where we will show that this algorithm
efﬁciently computes the DFT and makes the convolution sum a more feasible procedure.

Problems
629
Table 10.1 DTFT of Common Signals and DTFT Properties
Discrete-Time Fourier Transforms
Discrete-Time Signal
DTFT X(e jω), Periodic of Period 2π
1.
δ[n]
1,
−π ≤ω < π
2.
A
2πAδ(ω),
−π ≤ω < π
3.
e jω0n
2πδ(w −ω0),
−π ≤ω < π
4.
αnu[n], |α| < 1
1
1−αe−jω ,
−π ≤ω < π
5.
nαnu[n], |α| < 1
αe−jω
(1−αe−jω)2 ,
−π ≤ω < π
6.
cos(ω0n)u[n]
π [δ(ω −ω0) + δ(ω + ω0)] ,
−π ≤ω < π
7.
sin(ω0n)u[n]
−jπ [δ(ω −ω0) + δ(ω + ω0)] ,
−π ≤ω < π
8.
α|n|, |α| < 1
1−α2
1−2α cos(ω)+α2 ,
−π ≤ω < π
9.
u[n + N/2] −u[n −N/2]
sin(ω(N+1)/2)
sin(ω/2)
,
−π ≤ω < π
10.
αn cos(ω0n)u[n]
1−α cos(ω0)e−jω
1−2α cos(ω0)e−jω+α2e−2jω ,
−π ≤ω < π
11.
αn sin(ω0n)u[n]
α sin(ω0)e−jω
1−2α cos(ω0)e−jω+α2e−2jω ,
−π ≤ω < π
Properties of the DTFT
Z-transform:
x[n], X(z), |z| = 1 ∈ROC
X(e jω) = X(z)|z=e jω
Periodicity:
x[n]
X(e jω) = X(e j(ω+2πk)), k integer
Linearity:
αx[n] + βy[n]
αX(e jω) + βY(e jω)
Time-shift:
x[n −N]
e−jωNX(e jω)
Frequency-shift:
x[n]e jωon
X(e j(ω−ω0))
Convolution:
(x ∗y)[n]
X(e jω)Y(e jω)
Multiplication:
x[n]y[n]
1
2π
R π
−π X(e jθ)Y(e j(ω−θ))dθ
Symmetry:
x[n], real valued
|X(e jω)|, even function of ω
∠X(e jω), odd function of ω
Parseval’s relation:
P∞
n=∞|x[n]|2 =
1
2π
R π
−π |X(e jω|2dω
PROBLEMS
10.1. Eigenfunction property and frequency response—MATLAB
An IIR ﬁlter is characterized by the difference equation
y[n] = 0.5y[n −1] + x[n] −2x[n −1]
n ≥0
where x[n] is the input and y[n] is the output of the ﬁlter. Let H(z) be the transfer function of the ﬁlter.

630
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
Table 10.2 Properties of Discrete Fourier Series and Discrete Fourier Transform.
Fourier Series of Discrete-Time Periodic Signals
x[n], periodic signal of period N
X[k], periodic FS coefﬁcients of period N
Z-transform
x1[n] = x[n](u[n] −u[n −N])
X[k] = 1
N Z(x1[n])
z=e j2πk/N
DTFT
x[n] = P
k X[k]e j2πnk/N
X(e jω) = P
k 2πX[k]δ(ω −2πk/N)
LTI response
Input: x[n] = P
k X[k]e j2πnk/N
Output: y[n] = P
k X[k]H(e jkω0)e j2πnk/N
H(e jω), frequency response of system
Time shift (circular shift)
x[n −M]
X[k]e−j2πkM/N
Modulation
x[n]e j2πMn/N
X[k −M]
Multiplication
x[n]y[n]
PN−1
m=0 X[m]Y[k −m], periodic convolution
Periodic convolution
PN−1
m=0 x[m]y[n −m]
NX[k]Y[n]
Discrete Fourier Transform
x[n], ﬁnite-length N aperiodic signal
˜x[n], periodic extension of period L ≥N
˜x[n] = 1
N
PL−1
k=0 ˜X[k]e j2πnk/L
˜X[k] = PL−1
n=0 ˜x[n]e−j2πnk/L
IDFT/DFT
x[n] = ˜x[n]W[k], W[n] = u[n] −u[n −N]
X[k] = ˜X[k]W[k], W[k] = u[k] −u[k −N]
Circular convolution
(x ⊗L y)[n]
X[k]Y[k]
Circular and linear convolution
(x ⊗L y)[n] = (x ∗y)[n], L ≥M + K −1
M = length of x[n], K = length of y[n]
(a) The given ﬁlter is LTI, and as such the eigenfunction property applies. Obtain the magnitude
response |H(e jω)| of the ﬁlter using the eigenfunction property.
(b) Compute the magnitude response |H(e jω)| at discrete frequencies ω = 0, π/2, and π radians. Show
that the magnitude response is constant for 0 ≤ω ≤π, and as such this is an all-pass ﬁlter.
(c) Use the MATLAB function freqz to compute the frequency response (magnitude and phase) of this
ﬁlter and to plot them.
(d) Determine the transfer function H(z) = Y(z)/X(z). Find its pole and zero and indicate how they are
related.
10.2. Frequency transformation of low-pass to high-pass ﬁlters—MATLAB
You have designed an IIR low-pass ﬁlter with an input–output relation given by the difference equation
y[n] = 0.5y[n −1] + x[n] + x[n −1]
n ≥0
where x[n] is the input and y[n] is the output. You are told that by changing the difference equation to
y[n] = −0.5y[n −1] + x[n] −x[n −1]
n ≥0
you obtain a high-pass ﬁlter.

Problems
631
(a) From the eigenfunction property ﬁnd the frequency response of the two ﬁlters at ω = 0, π/2, and π
radians. Use the MATLAB functions freqz and abs to compute the magnitude responses of the two
ﬁlters. Plot them to verify that the ﬁlters are low pass and high pass.
(b) Call H1(e jω) the frequency response of the ﬁrst ﬁlter and H2(e jω) the frequency response of the
second ﬁlter. Show that
H2(e jω) = H1(e j(π−ω))
and relate the impulse response h2[n] to h1[n].
(c) Use the MATLAB function zplane to ﬁnd and plot the poles and the zeros of the ﬁlters and determine
the relation between the poles and the zeros of the two ﬁlters.
10.3. Computations from deﬁnition of DTFT and IDTFT
Consider the discrete-time signal x[n] = 0.5|n|, and ﬁnd its DTFT X(e jω). From the direct and the inverse
DTFT of x[n]:
(a) Determine the inﬁnite sum
∞
X
k=−∞
0.5|n|
(b) Find the integral
π
Z
−π
X(e jω)dω
(c) Find the phase of X(e jω).
(d) Determine the sum
∞
X
k=−∞
(−1)n0.5|n|
10.4. Frequency shift of FIR ﬁlters—MATLAB
Consider a moving-average FIR ﬁlter with an impulse response
h[n] = 1
3(δ[n] + δ[n −1] + δ[n −2])
Let H(z) be the Z-transform of h[n].
(a) Find the frequency response H(e jω) of the FIR ﬁlter.
(b) Let the impulse response of a new ﬁlter be given by
h1[n] = h[n]e jπn
Use the eigenfunction property to ﬁnd the frequency response H1(e jω) of the new FIR ﬁlter.
(c) Use the MATLAB functions freqz and abs to compute the magnitude response of the two ﬁlters. Plot
them and determine the location of the poles and the zeros of the two ﬁlters. What type of ﬁlters are
these?
10.5. Duality of DTFT
The DTFT of a discrete-time signal x[n] is given as
X(e jω) = e jπ/4δ(ω −0.5π) + e−jπ/4δ(ω + 0.5π) −2πe−jπ/8δ(ω −0.71) −2πe jπ/8δ(ω + 0.71)
(a) Is the signal x[n] periodic? If so, indicate its period.
(b) Determine the signal x[n], and verify your answer above.

632
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
10.6. Chirps for jamming—MATLAB
A chirp signal is a sinusoid of continuously changing frequency. Chirps are frequently used to jam
communication transmissions. Consider the chirp
x[n] = cos(θn2)u[n]
θ = π
2L 0 ≤n ≤L −1
(a) A measure of the frequency of the chirp is the so-called instantaneous frequency, which is deﬁned
as the derivative of the phase in the cosine—that is,
IF(n) = dθn2
dn
Find the instantaneous frequency of the given chirp. Use MATLAB to plot x[n] for L = 256.
(b) Let L = 256 and use MATLAB to compute the DTFT of x[n] and to plot its magnitude. Indicate the
range of discrete frequencies that would be jammed by the given chirp.
10.7. Time speciﬁcations for FIR ﬁlters—MATLAB
When designing discrete ﬁlters the speciﬁcations can be given in the time domain. One can think of
converting the frequency-domain speciﬁcations into the time domain. Assume you wish to obtain a ﬁlter
that approximates an ideal low-pass ﬁlter with a cut-off frequency ωc = π/2 and that has a linear phase
−Nω. Thus, the frequency response is
H(e jω) =

1e−jNω
−π/2 ≤ω ≤π/2
0
−π ≤ω < π/2 and π/2 < ω ≤π
(a) Find the corresponding impulse response using the inverse DTFT of H(e jω).
(b) If N = 50, plot h[n] using the MATLAB function stem for 0 ≤n ≤100. Comment on the shape of the
plot.
(c) Suppose we want a band-pass ﬁlter of center frequency ω0 = π/2. Use the above impulse response
h[n] to obtain the impulse response of the desired band-pass ﬁlter.
10.8. Z-transform and DTFT—MATLAB
Consider a discrete pulse p[n] = u[n] −u[n −N].
(a) Use the deﬁnition of the DTFT to determine P(e jω) and then use the Z-transform P(z) of p[n] to verify
your result.
(b) For N = 5, 10, and 20, use the MATLAB function zplane to ﬁnd the zeros of P(z) and indicate at what
frequencies P(e jω) is zero. Verify your response using freqz.
(c) Suppose the impulse response of a ﬁlter is h[n] = u[n] −u[n −4], and its input is
v[n] =
2
X
k=1
cos(kω0n)
For what value of ω0 is the steady-state response of the ﬁlter zero?
10.9. Downsampling and DTFT—MATLAB
Consider pulses x1[n] = u[n] −u[n −20] and x2[n] = u[n] −u[n −10], and their product x[n] =
x1[n]x2[n].
(a) Plot the three pulses. Could you say that x[n] is a downsampled version of x1[n]? What would be the
downsampling rate? Find X1(e jω).
(b) Find directly the DTFT of x[n] and compare it to X1(e jω/M) where M is the downsampling rate found
above. If we downsample x1[n] to get x[n], would the result be affected by aliasing? Use MATLAB
to plot the magnitude DTFT of x1[n] and x[n] to provide an answer.

Problems
633
10.10. Cascading of interpolators and decimators—MATLAB
Suppose you cascade an interpolator (an upsampler and a low-pass ﬁlter) and a decimator (a low-pass
ﬁlter and a downsampler).
(a) If both the interpolator and the decimator have the same rate M, carefully draw a block diagram of
the interpolator–decimator system.
(b) Suppose that the interpolator is of rate 3 and the decimator of rate 2. Carefully draw a block diagram
of the interpolator–decimator system. What would be the equivalent of sampling the input of this
system to obtain the same output?
(c) Use the MATLAB functions interp and decimate to process the ﬁrst 100 samples of the test signal
handel where the interpolator’s rate is 3 and the decimator’s is 2. How many samples does the output
have?
10.11. MATLAB and phase computation
The computation of the phase of a complex number or function using MATLAB has some issues that you
need to understand:
(a) The range of possible values of the inverse tangent needs to be extended to [−π, π) depending on
the quadrant the complex number is in. Consider the complex numbers 1 + j, −1 + j, −1 −j, and
1 −j, and represent each by a vector from the origin and consider what changes are needed when
we use the formula to ﬁnd the phase.
(b) The phase of a complex number is only signiﬁcant if its magnitude is signiﬁcant. Use MATLAB to
compute the magnitudes and the phases of the complex numbers x = 1 + j and y = 10−6 + j10−6.
How do the phases of these numbers compare? What about their magnitudes? Explain.
(c) If the function X(e jω) is zero or inﬁnite at a frequency ω0, the phase is undetermined at that frequency
and of no signiﬁcance since the corresponding magnitude is zero or inﬁnity. Let X(z) = z −1, so that
X(e jω) = e jω −1. Can you determine the phase of X(e j0)? Explain. Likewise, if X(z) = 1/(z −1),
what is the phase of X(e jω) at ω = 0?
(d) If the phase is linear (i.e., θ = −Nω), MATLAB will plot the values only between [−π, π] and so the
phase will not appear linear. Let X(z) = z−4. Find the phase of X(e jω)—is it linear? Then use the
MATLAB functions freqz and angle to compute the phase of X(e jω)—does it appear linear? Explain.
10.12. Linear phase and phase unwrapping—MATLAB
A DTFT X(e jω) is said to have linear phase if its phase is a line through the origin of the frequency
plane. Let
X(e jω) = 2e−j4ω
−π ≤ω < π
(a) Carefully plot the magnitude and the phase of X(e jω). Is the phase linear?
(b) Use the MATLAB functions freqz and angle to compute the phase of X(e jω) and then plot it. (Hint:
Let z = e jω to be able to use freqz.) Does the phase computed by MATLAB appear linear? What are
the maximum and minimum values of the phase, and how many radians separate the minimum from
the maximum?
(c) Now, recalculate the phase, but after using angle use the function unwrapping in the resulting phase
and plot it. Does the phase appear linear?
10.13. Linear phase and symmetry—MATLAB
Consider the signal x[n] = Aδ[n] + u[n + 9] −u[n −10].
(a) Carefully plot x[n]. Find the Z-transform X(z) of x[n] and from it X(e jω), the DTFT of x[n]. Find the
value of A so that the phase of X(e jω) is zero. Use MATLAB to verify your results.
(b) Consider now x1[n] = x[n −9], and use the value of A found before. Carefully plot x1[n] and ﬁnd its
DTFT using the Z-transform X1(z). Is its phase linear? Use MATLAB to verify your results. Use freqz,
angle, and unwrap to compute the phase.

634
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
10.14. Sinusoidal form of DTFT
A triangular pulse is given by
t[n] =



3 + n
−2 ≤n ≤−1
3 −n
0 ≤n ≤2
0
otherwise
(a) The pulse can be written as
t[n] =
∞
X
k=−∞
Akδ[n −k]
Find the {Ak} coefﬁcients.
(b) Find a sinusoidal expression for the DTFT of t[n]—that is,
T(e jω) = B0 +
∞
X
k=1
Bk cos(kω)
Express the coefﬁcients B0 and Bk in terms of the Ak coefﬁcients.
10.15. DTFT and Z-transform—MATLAB
Let x[n] = r[n] −r[n −3] −u[n −3] where r[n] is the ramp signal.
(a) Carefully plot x[n] and ﬁnd its Z-transform X(z).
(b) If y[n] = x[−n], give Y(z) in terms of X(z).
(c) Use the above results to ﬁnd the DTFT of x[n], x[−n], and x[n] + x[−n]. Find the magnitude of each
of these DTFTs and then use MATLAB to compute them and plot them.
10.16. Computations from DTFT deﬁnition
For simple signals it is possible to obtain some information on their DTFTs without computing them. Let
x[n] = δ[n] + 2δ[n −1] + 3δ[n −2] + 2δ[n −3] + δ[n −4]
(a) Find X(e j0) and X(e jπ) without computing the DTFT X(e jω).
(b) Find
π
Z
−π
|X(e jω)|2dω
(c) Find the phase of X(e jω). Is it linear?
10.17. DTFT of even and odd functions
A signal
x[n] = 0.5nu[n]
is neither even nor odd.
(a) Find the even xe[n] and the odd xo[n] components of x[n], and carefully plot them.
(b) Find the Z-transforms of xe[n] and xo[n], and from them ﬁnd the DFTs Xe(e jω) and Xo(e jω). Are they
real or imaginary?
(c) Since x[n] = xe[n] + xo[n] so that X(e jω) = Xe(e jω) + Xo(e jω), how dos the real and the imaginary
parts of X(e jω) relate to Xe(e jω) and Xo(e jω)? Explain.
(d) Use Parseval’s result to obtain that Ex = Exe + Exo i.e., the energy of the signal is the sum of the
energies of its even and odd components.

Problems
635
10.18. Power spectral density
Consider an autocorrelation function
c[n] = 0.5|n|
−∞< n < ∞
(a) Find the magnitude square of the DTFT C(e jω) of c[n], which is called the power spectral density.
(b) Find the Z-transform of c[n] and determine where its poles and zeros are. Are there any zeros or poles
on the unit circle?
(c) Find C(e j0)—that is, the dc value of the power spectral density. Determine the phase of C(e jω)—is
it linear?
10.19. Convolution sum and product of polynomials
The convolution sum can be seen as a way to compute the coefﬁcients of the product of polynomials.
This is because
[x ∗y][n] ⇔X(z)Y(z) ⇔X(e jω)Y(e jω)
(a) Let X(z) = 1 + 2z−1 + 3z−2 and Y(z) = z−2 + 4z−3 if x[n] = 1δ[n] + 2δ[n −1] + 3δ[n −2] and
y[n] = 1δ[n −2] + 4δ[n −3] are sequences formed by the coefﬁcients of the polynomials. Compute
the convolution sum [x ∗y][n] and compare it to the coefﬁcients of the polynomial Z(z) = X(z)Y(z),
or Z(e jω) = X(e jω)Y(e jω).
(b) Suppose that the transfer function of a discrete-time system is
H(z) = W(z)
V(z) = 3z2 + 2z + 2z−1 + 3z−2
and that it is known that the input is v[n] = u[n] −u[n −3]. Use the connection between the product
of the polynomials and the convolution sum to ﬁnd the output w[n] of the system.
10.20. Windowing and DTFT—MATLAB
A window w[n] is used to consider the part of a signal we are interested in.
(a) Let w[n] = u[n] −u[n −20] be a rectangular window of length 20. Let x[n] = sin(0.1πn). We are
interested in a period of the inﬁnite length x[n], or y[n] = x[n]w[n]. Compute the DTFT of y[n] and
compare it with the DTFT of x[n]. Write a MATLAB script to compute Y(e jω).
(b) Let w1[n] = (1 + cos(2πn/11))(u[n + 5] −un[n −5]) be a raised-cosine window that is symmetric
with respect to n = 0 (noncausal). Adapt the script in the previous part to ﬁnd the DTFT of
z[n] = x[n]w1[n]
where x[n] is the sinusoid given above.
10.21. Z-transform and Fourier series—MATLAB
Let
x1[n] = 0.5n
0 ≤n ≤9
be a period of a periodic signal x[n].
(a) Use the Z-transform to compute the Fourier series coefﬁcients.
(b) Use MATLAB to plot the magnitude and the phase line spectrum (i.e., |Xk| and ∠Xk versus frequency
−π ≤ω ≤π).

636
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
10.22. Linear equations and Fourier series—MATLAB
The Fourier series of a signal x[n] and its coefﬁcients Xk are both periodic of some value N, and as such
can be written as
I
x[n] =
N−1
X
k=0
Xke j2πnk/N
0 ≤n ≤N −1
I
Xk = 1
N
N−1
X
n=0
x[n]e−j2πnk/N
0 ≤k ≤N −1
(a) To ﬁnd the x[n], 0 ≤n ≤N −1 given Xk, 0 ≤k ≤N −1, write a set of N linear equations. Indicate
how you would ﬁnd the x[n] from the matrix equation.
(b) As you can see, there is a lot of duality in the Fourier series and its coefﬁcients. If you consider the
reverse problem in the previous part, how would you solve for Xk given the x[n]?
(c) Let x[n] = n for n = 0, 1, 2, and 0 for n = 3 be a period of a periodic signal x[n] of period N = 4. Use
the above method to solve for the Fourier series coefﬁcients Xk, 0 ≤k ≤3. Use MATLAB to ﬁnd the
inverse of the complex exponential matrix.
(d) Suppose that when computing the Xk for the x[n] signal given above, you separate the sum into two
sums, one for the even values of n (i.e., n = 0, 2) and the other for the odd values of n (i.e., n = 1, 3).
Try to simplify the complex exponentials and write an equivalent matrix expression for the Xk.
10.23. Operations on Fourier series—MATLAB
A periodic signal x[n] of period N can be represented by its Fourier series
x[n] =
N−1
X
k=0
Xke j2πnk/N
0 ≤n ≤N −1
If you consider this a representation of x[n]:
(a) Is x1[n] = x[n −3] periodic? If so, use the Fourier series of x[n] to obtain the Fourier series
coefﬁcients of x1[n].
(b) Let x2[n] = x[n] −x[n −1] (i.e., the ﬁnite difference). Determine if x2[n] is periodic, and if so, ﬁnd its
Fourier series coefﬁcients.
(c) If x3[n] = x[n](−1)n, is x3[n] periodic? If so, determine its Fourier series coefﬁcients.
(d) Let x4[n] = sign[cos(0.5πn)] where sign(ξ) is a function that gives 1 when ξ ≥0 and −1 when ξ < 0.
Determine the Fourier coefﬁcients of x4[n] if periodic.
(e) Use MATLAB to ﬁnd the Fourier series coefﬁcients for xi[n], i = 1, 2, 3, and 4 and to plot them as
functions of k.
10.24. Fourier series of even and odd signals—MATLAB
Let x[n] be an even signal and y[n] be an odd signal.
(a) Determine whether the Fourier coefﬁcients Xk and Yk corresponding to x[n] and y[n] are complex,
real, or imaginary.
(b) Consider x[n] = cos(2πn/N) and y[n] = sin(2πn/N) for N = 3 and N = 4. Use the above results to
ﬁnd the Fourier series coefﬁcients for the two signals with the different periods.
(c) Use MATLAB to ﬁnd the Fourier series coefﬁcients of the above two signals with the different periods,
and plot their magnitude and phase spectra.
10.25. Response of LTI systems to periodic signals—MATLAB
Suppose you get noisy periodic measurements
y[n] = (−1)nx[n] + Aη[n]

Problems
637
where x[n] is the desired signal and η[n] is a noise that varies from 0 to 1 at random.
(a) Let A = 0 and x[n] = sign[cos(0.1πn)]. Determine how to process y[n], indicating the type of ﬁlter
to obtain an approximate version of x[n]. Consider the ﬁrst 100 samples of x[n] and use MATLAB to
ﬁnd the spectrum of x[n] and y[n] to show that the ﬁlter you recommend will do the job.
(b) Use the MATLAB function ﬁr1 to generate the kind of ﬁlter you decided to use above and show that
when ﬁltering y[n] for A = 0, you obtain the desired result.
(c) Consider the ﬁrst 100 samples of the MATLAB ﬁle “handel.mat” a period of a signal that continuously
replays these values over and over. Let x[n] be the desired signal that results from this. Now let
A = 0.01, and use the function rand to generate the noise, and come up with suggestions as to how
to get rid of the effects of the multiplication by (−1)n and of the noise η[n]. Recover the desired
signal x[n].
10.26. DFT of an aperiodic and periodic signal—MATLAB
Consider a signal x[n] = (−0.95)n(u[n] −u[n −70]).
(a) To compute the DFT of x[n] we pad it with zeros so as to obtain a signal with length 2γ , the larger but
closest to the length of x[n]. Determine the value of γ and use the MATLAB function fft to compute
the DFT X[k] of the padded-with-zeros signal. Plot its magnitude and phase.
(b) Let now x1[n] = x[n −10]. Compute its DFT X1[k] using the fft function. Pad x1[n] with zeros to
compute the γ -length FFT where γ is the value obtained above.
(c) Consider x[n] a period of a periodic signal of period N = 70. Compute its DFT using the fft algorithm
and then plot its magnitude and phase. What is the length of the FFT? Can we pad with zeros the
period to ﬁnd its DFT?
10.27. Frequency resolution of DFT—MATLAB
When we pad an aperiodic signal with zeros, we are improving its frequency resolution—that is, the more
zeros we attach to the original signal the better the frequency resolution, as we obtain the frequency
representation at a larger number of frequencies around the unit circle.
(a) Consider an aperiodic signal x[n] = u[n] −u[n −10], and compute its DFT by means of the fft func-
tion padding it with 10 and then 100 zeros. Plot the magnitude response using stem. Comment on
the frequency resolution of the two DFTs.
(b) When the signal is periodic, one cannot pad a period with zeros. When computing the FFT in theory
we generate a periodic signal of period L equal or larger than the length of the signal when the signal
is aperiodic, but if the signal is periodic we must let L be the signal period or a multiple of it. Adding
zeros to the period makes the signal different from the periodic signal. Consider x[n] = cos(πn/5),
−∞< n < ∞as a periodic signal, and do the following:
I
Consider exactly one period of x[n] and compute the FFT of this sequence.
I
Consider 10 periods of x[n] and compute the FFT of this sequence.
I
Consider attaching 10 zeros to one period and compute the FFT of the resulting sequence.
If we consider the ﬁrst of these cases giving the correct DFT of x[n], how many harmonic frequencies
does it show. What happens when we consider the 10 periods? Are the harmonic frequencies the
same as before? What are the values of the DFT in frequencies in between the harmonic frequencies?
What happened to the magnitude at the original frequencies. Finally, does the last FFT relate at all
to the ﬁrst FFTs?
10.28. DFT and IIR ﬁlters—MATLAB
A deﬁnite advantage of the FFT is that it reduces considerably the computation in the convolution sum.
Thus, if x[n], 0 ≤n ≤N −1 is the input of an FIR ﬁlter with impulse response h[n], 0 ≤n ≤M −1,
their convolution sum y[n] = [x ∗h][n] will be of length M + N −1. Now if X[k] and H[k] are the DFTs
(computed by the FFT) of x[n] and h[n], and if Y[k] = X[k]H[k] is the DFT of the convolution sum of
length bigger or equal to M + N −1, then to be able to multiply the FFTs X[k] and H[k] they both should

638
CHAPTER 10:
Fourier Analysis of Discrete-Time Signals and Systems
be of the same length as Y[k] (i.e., bigger or equal to M + N −1). Consider what happens when the ﬁlter
is an IIR that possibly has an impulse response of very large length. Let
y[n] −1.755y[n −1] + 0.81y[n −2] = x[n] + 0.5x[n −1]
be the difference equation representing an IIR ﬁlter with input x[n] and output y[n]. Assume the initial
conditions are zero, and the input is x[n] = u[n] −u[n −50]. Use MATLAB to obtain your results.
(a) Compute using ﬁlter the ﬁrst 40 values of the impulse response h[n] and use them to approximate
it (call it ˆh[n]). Compute the ﬁlter output ˆy[n] using the FFT as indicated above. In this case, we are
approximating the IIR ﬁlter by an FIR ﬁlter of length 40. Plot the input and the output. Use FFTs of
length 128.
(b) Suppose now that we do not want to approximate h[n], so consider the following procedure. Find
the transfer function of the IIR ﬁlter, say H(z) = B(z)/A(z), and if X(z) is the Z-transform of the input,
then
Y(z) = B(z)X(z)
A(z)
Compute as before the FFT for x[n], of length 128 (call it X[k]) and compute the 128 length of the
coefﬁcients of B(z) and A(z) to obtain DFTs B[k] and A[k]. Multiplying X[k] by B[k] and dividing by
A[k], all of length 128, results in a sequence of length 128 that should correspond to Y[k], the DFT of
y[n]. Compute the inverse FFT to get y[n].
(c) Use ﬁlter to solve the difference equation and obtain y[n] for x[n] = u[n] −u[n −50]. If this is the
exact solution, calculate the error with respect to the other responses in (a) and (b)?
10.29. Circular and linear convolutions—MATLAB
Consider the circular convolution of two signals x[n] = n, 0 ≤n ≤3, and y[n] = 1, n = 0, 2 and zero for
n = 1, 3.
(a) Compute the convolution sum or linear convolution of x[n] and y[n]. Do it graphically and verify your
results by multiplying the DTFTs of x[n] and y[n].
(b) Use MATLAB to ﬁnd the linear convolution. Plot x[n], y[n], and the linear convolution z[n] = (x ∗
y)[n].
(c) We wish to compute the circular convolution of x[n] and y[n] for different lengths N = 4, N = 7, and
N = 10. Determine for which of these values does the circular and the linear convolutions coincide.
Show the circular convolution for the three cases. Use MATLAB to verify your results.
(d) If we use the convolution property of the DFT verify your result in the above parts of this problem.

CHAPTER 11
Introduction to the Design
of Discrete Filters
When in doubt, don’t.
Benjamin Franklin (1706–1790)
Printer, inventor, scientist, and diplomat
11.1 INTRODUCTION
In this chapter we introduce the design of discrete ﬁlters. This material complements the theory of
analog and discrete ﬁltering presented in previous chapters, and in particular provides continuity to
the introduction to analog ﬁlter design from Chapter 6.
Filtering is an important application of linear time-invariant (LTI) systems. According to the eigen-
function property of LTI systems (Figure 11.1) the steady-state response of a discrete-time LTI system
to a sinusoidal input—with a certain frequency, magnitude, and phase—is also a sinusoid of the
same frequency as the input, but with the magnitude and the phase affected by the response of the
system at the input frequency. Since periodic as well as aperiodic signals have Fourier representations
consisting of sinusoids of different frequencies, the frequency components of any signal can be mod-
iﬁed by appropriately choosing the frequency response of an LTI system or ﬁlter. Filtering can thus be
seen as a way to change the frequency content of an input signal.
The appropriate ﬁlter is speciﬁed using the spectral characterization of the input and the desired spec-
tral characteristics of the output of the ﬁlter. Once the speciﬁcations of the ﬁlter are set, the problem
becomes one of approximation, either by a ratio of polynomials or by a polynomial (if possible).
After establishing that the ﬁlter resulting from the approximation satisﬁes the given speciﬁcations, it
is then necessary to check its stability (if not guaranteed by the design method) in the case of the ﬁlter
being a rational approximation, and if stable we need to ﬁgure out what would be the best possible
way to implement the ﬁlter in either hardware or software. If not stable, we need either to repeat the
approximation or to stabilize the ﬁlter before its implementation.
In the continuous-time domain, ﬁlters are obtained by means of rational approximation. In the
discrete-time domain, there are two possible types of ﬁlters. The ﬁrst is the result of rational
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00015-6
c⃝2011, Elsevier Inc. All rights reserved.
639

640
CHAPTER 11:
Introduction to the Design of Discrete Filters
FIGURE 11.1
Eigenfunction property of LTI systems.
H(e jω)
x [n]=e jω0n
y [n] = e jω0n H (e jω0)
LTI
approximation—these ﬁlters are called recursive or inﬁnite-impulse response (IIR) ﬁlters. The other
is the nonrecursive or ﬁnite-impulse response (FIR) ﬁlters that result from a polynomial approxima-
tion. In the continuous-time domain, depending on their implementation, ﬁlters are either passive
or active. Passive ﬁlters are implemented using resistors, capacitors, and inductors, while active ﬁl-
ters are implemented with resistors, capacitors, and operational ampliﬁers. The implementation of
discrete or digital ﬁlters is done by means of software or dedicated hardware.
As we will see, the discrete ﬁlter speciﬁcations can be in the frequency or in the time domain. For
recursive or IIR ﬁlters, the speciﬁcations are typically given in the form of magnitude and phase
speciﬁcations, while the speciﬁcations for nonrecursive or FIR ﬁlters can be in the time domain as a
desired impulse response. The discrete ﬁlter design problem then consists in: Given the speciﬁcations
of a ﬁlter we look for a polynomial or rational (ratio of polynomials) approximation to the speciﬁca-
tions. The resulting ﬁlter should be realizable, which besides causality and stability requires that the
ﬁlter coefﬁcients be real valued.
The typical approach in ﬁlter design is to consider low-pass prototypes with normalized frequency
and magnitude responses that may be transformed into other ﬁlters with the desired frequency
response. Thus, a great deal of effort is put into the design of low-pass ﬁlters and into developing fre-
quency transformations to map low-pass ﬁlters into other types of ﬁlters. Using cascade and parallel
connections of ﬁlters also provides a way to obtain different types of ﬁlters.
There are different ways to obtain the rational approximation for discrete IIR ﬁlters—by transforma-
tion of analog ﬁlters, or by optimization methods that include stability as a constraint. We will see
that the classical analog design methods (Butterworth, Chebyshev, Elliptic, etc.) can be used to design
discrete ﬁlters by means of the bilinear transformation that maps the analog s-plane into the z-plane.
Given that the FIR ﬁlters are unique to the discrete domain, the approximation procedures for FIR
ﬁlters are unique to that domain.
The difference between discrete and digital ﬁlters is in the quantization and coding. For a discrete ﬁlter
we assume that the input and the coefﬁcients of the ﬁlter are represented with inﬁnite precision—that
is, using an inﬁnite number of quantization levels—and thus no coding is performed. The coefﬁcients
of a digital ﬁlter are binary and the input and output are quantized and coded. Thus, quantization
affects the performance of a digital ﬁlter, while it has no effect in discrete ﬁlters.
Considering continuous-to-discrete converters (CDCs) and discrete-to-continuous converters
(DCCs) as simply samplers and reconstruction ﬁlters, it is possible to implement the ﬁltering of
band-limited analog signals using discrete ﬁlters (Figure 11.2). In such an application, an additional
speciﬁcation for the ﬁlter design is the sampling period. In this process it is crucial that the sampling
period in the CDCs and DCCs be synchronized. In practice, ﬁltering of analog signals is done using
analog-to-digital (ADC) and digital-to-analog (DAC) together with digital ﬁlters.

11.2 Frequency-Selective Discrete Filters
641
FIGURE 11.2
Discrete ﬁltering of analog signals using CDCs
and DCCs. The CDC is simply a sampler while
the DCC is a reconstruction ﬁlter.
CDC
H(z )
DCD
x (t )
y (t )
x (nTs)
y(nTs)
Ts
11.2 FREQUENCY-SELECTIVE DISCRETE FILTERS
The principle behind discrete ﬁltering is easily understood by considering the response of an LTI
system to sinusoids. If H(z) is the transfer function of a discrete-time LTI system, and
x[n] =
X
k
Ak cos(ωkn + φk)
(11.1)
is the input of the system, according to the eigenfunction property of LTI systems the steady-state
response of the system is
yss[n] =
X
k
Ak|H(ejωk)| cos(ωkn + φk + θ(ωk))
(11.2)
where |H(ejωk)| and θ(ωk) are the magnitude and the phase of H(ejω), which is the frequency response
of the system, at the frequency ωk. The frequency response is the transfer function computed on
the unit circle (i.e., H(ejω) = H(z)|z=ejω). It becomes clear from Equation (11.2) that by judiciously
choosing the frequency response of the LTI system we can select the frequency components of the
input we wish to have at the output, and attenuate or amplify their amplitudes or change their
phases.
In general, for an input x[n] with Z-transform X(z), the Z-transform of the output of the ﬁlter is
Y(z) = H(z)X(z)
(11.3)
or on the unit circle (when z = ejω),
Y(ejω) = H(ejω)X(ejω).
(11.4)
By selecting the frequency response H(ejω) we allow some frequency components of x[n] to appear
in the output, and others to be ﬁltered out.
Ideal frequency-selective ﬁlters, such as low-pass, high-pass, band-pass, and stopband ﬁlters, cannot
be realized. They serve as prototypes for the actual ﬁlters.
11.2.1 Linear Phase
A ﬁlter changes the spectrum of its input in magnitude as well as in phase. Distortion in magnitude
can be avoided by using an all-pass ﬁlter with unit magnitude for all frequencies. Phase distortion can
be avoided by requiring the phase response of the ﬁlter to be linear. For instance, when transmitting
a voice signal in a communication system it is important that the signals at the transmitter and at

642
CHAPTER 11:
Introduction to the Design of Discrete Filters
the receiver be ideally equal within a time delay and a constant attenuation factor. To achieve this,
the transfer function of an ideal communication channel should equal that of an all-pass ﬁlter with
a linear phase.
Indeed, if the output of the transmitter is a discretized baseband signal x[n] and the recovered signal
at the receiver is αx[n −N0], for an attenuation factor α and a time delay N0, ideally the channel is
represented by a transfer function
H(z) = Z(αx[n −N0])
Z(x[n])
= αz−N0
(11.5)
The constant gain of the all-pass ﬁlter permits all frequency components of the input to appear in the
output. The linear phase simply delays the signal, which is a very tolerable distortion.
To appreciate the effect of linear phase, consider the ﬁltering of a signal
x[n] = 1 + cos(ω0n) + cos(ω1n)
ω1 = 2ω0
n ≥0
using an all-pass ﬁlter with transfer function H(z) = αz−N0. The magnitude response of this ﬁlter is
α, and its phase is linear, as shown in Figure 11.3(a). The steady-state output of the all-pass ﬁlter is
yss[n] = 1H(ej0) + |H(ejω0)| cos(ω0n + ∠H(ejω0)) + |H(ejω1)| cos(ω1n + ∠H(ejω1))
= α [1 + cos(ωo(n −N0)) + cos(ω1(n −N0))] = αx[n −N0]
which is the input signal attenuated by α and delayed N0 samples.
Suppose then that the all-pass ﬁlter has a phase function that is nonlinear, for instance, the one in
Figure 11.3(b). The steady-state output would then be
yss[n] = 1H(ej0) + |H(ejω0)| cos(ω0n + ∠H(ejω0) + |H(ejω1)| cos(ω1n + ∠H(ejω1))
= α[1 + cos(ω0(n −N0)) + cos(ω1(n −0.5N0))] ̸= αx[n −N0]
In the case of the linear phase each of the frequency components is delayed N0 samples, and thus
the output is just a delayed version of the input. On the other hand, in the case of a nonlinear phase
the frequency component of frequency ω1 is delayed less than the other two frequency components,
creating distortion in the signal so that the output is not a delayed version of the input.
FIGURE 11.3
(a) Linear and (b) nonlinear phase.
θ (ω )
ω 0 ω 1
ω
π
−N0 ω 0
−π
(b)
ω 0
θ (ω)
−N0 ω 0
−N0 ω 1
ω 1
ω
π
−π
(a)

11.2 Frequency-Selective Discrete Filters
643
Group Delay
A measure of linearity of the phase is obtained from the group delay function, which is deﬁned as
τ(ω) = −dθ(ω)
dω
(11.6)
The group delay is constant when the phase is linear. Deviation of the group delay from a constant
indicates the degree of nonlinearity of the phase. In the above cases, when the phase is linear (i.e.,
for 0 ≤ω ≤π),
θ(ω) = −N0ω
⇒τ(ω) = N0
and when the phase is nonlinear or
θ(ω) =
−N0ω
0 < ω ≤ω0
−N0ω0
ω0 < ω ≤π
for 0 ≤ω ≤π, then we have that the group delay is
τ(ω) =
N0
0 < ω ≤ω0
0
ω0 < ω ≤π
which is not constant.
11.2.2 IIR and FIR Discrete Filters
I
A discrete ﬁlter with transfer function
H(z) = B(z)
A(z) =
PM−1
m=0 bmz−m
1 + PN−1
k=1 akz−k =
∞
X
n=0
h[n]z−n
(11.7)
is called inﬁnite-impulse response or IIR since its impulse response h[n] typically has inﬁnite length. It is
also called recursive because if the input of the ﬁlter H(z) is x[n] and y[n] is its output, the input–output
relationship is given by the difference equation
y[n] = −
N−1
X
k=1
aky[n −k] +
M−1
X
m=0
bmx[n −m]
(11.8)
where the output recurs on previous outputs (i.e., the output is fed back).
I
The transfer function of a ﬁnite-impulse response or FIR ﬁlter is
H(z) = B(z) =
M−1
X
m=0
bmz−m
(11.9)
Its impulse response is h[n] = bn, n = 0, . . . , M −1, and zero elsewhere, thus of ﬁnite length. This ﬁlter is
called nonrecursive given that the input–output relationship is given by
y[n] =
M
X
m=0
bmx[n −m] = (b ∗x)[n]
(11.10)
or the convolution sum of the ﬁlter coefﬁcients (or impulse response) and the input.

644
CHAPTER 11:
Introduction to the Design of Discrete Filters
For practical reasons, these ﬁlters are causal (i.e., h[n] = 0 for n < 0) and bounded-input
bounded-output (BIBO) stable (i.e., all the poles of H(z) must be inside the unit circle). This guaran-
tees that the ﬁlter can be implemented and used in real-time processing, and that the output remains
bounded when the input is bounded.
Remarks
I
Calling the IIR ﬁlters recursive is more appropriate. It is possible to have a ﬁlter with a rational transfer
function that does not have an inﬁnite-length impulse response. However, it is traditional to refer to these
ﬁlters as IIR.
I
When comparing the IIR and the FIR ﬁlters, neither has a deﬁnite advantage:
I IIR ﬁlters are implemented more efﬁciently than FIR ﬁlters in terms of number of operations and
required storage (having similar frequency responses, an IIR ﬁlter has fewer coefﬁcients than an FIR
ﬁlter).
I The implementation of an IIR ﬁlter using the difference equation resulting from its transfer function
is simple and computationally efﬁcient, but FIR ﬁlters can be implemented using the Fast Fourier
Transform (FFT) algorithm, which is computationally very efﬁcient.
I Since the transfer function of any FIR ﬁlter only has poles at the origin of the z-plane, FIR ﬁlters are
always BIBO stable, but for an IIR ﬁlter we need to check that the poles of its transfer function (i.e.,
zeros of A(z)) are inside the unit circle if the design procedure does not guarantee stability.
I FIR ﬁlters can be designed to have linear phase, while IIR ﬁlters usually have nonlinear phase, but
approximately linear phase in the passband region.
I Example 11.1
The phase of IIR ﬁlters is always nonlinear. Although it is possible to design FIR ﬁlters with linear
phase, not all FIR ﬁlters have linear phase. As we will see, symmetry conditions on the impulse
response of FIR ﬁlters are needed to have linear phase. Consider the following two ﬁlters with
input–output equations
(a) y[n] = 0.5y[n −1] + x[n]
(b) y[n] = 1
3
 x[n −1] + x[n] + x[n + 1]

where x[n] is the input and y[n] is the output. Use MATLAB to compute and plot the magnitude
and the phase response of each of these ﬁlters.
Solution
The transfer functions of the given ﬁlters are:
(a) H1(z) =
1
1 −0.5z−1
(b) H2(z) = 1
3[z−1 + 1 + z] = 1 + z + z2
3z
= (z −1ej2.09)(z −1e−j2.09)
3z

11.2 Frequency-Selective Discrete Filters
645
|H2(e jω)|
<H2(e jω)
0
1
2
3
0.5
1
1.5
2
ω
|H1(e jω)|
<H1(e jω)
0
1
2
3
−0.6
−0.4
−0.2
0
0.2
ω
0
1
2
3
0
0.2
0.4
0.6
0.8
1
ω
0
1
2
3
−2
0
2
4
ω
(a)
(b)
FIGURE 11.4
Magnitude and phase responses of an IIR ﬁlter with a transfer function of (a) H1(z) = 1/(1 −0.5z−1), and of an
FIR ﬁlter with a transfer function of (b) H2(z) = (z −1ej2.09)(z −1e−j2.09)/3z. Notice the phase responses are
nonlinear.
Thus, the ﬁrst is an IIR ﬁlter and the second is an FIR ﬁlter (notice that this ﬁlter is noncausal as it
requires future values of the input to compute the present output). The phase responses of these
ﬁlters are clearly nonlinear. The transfer function H2(z) has zeros on the unit circle z = 1e±j2.09,
making the phase of this ﬁlter not continuous and so it cannot be unwrapped. Figure 11.4 shows
the magnitude and the phase responses of the ﬁlters H1(z) and H2(z).
I
I Example 11.2
A simple model for the multipath effect in the channel of a wireless system is
y[n] = x[n] −αx[n −N0]
α = 0.8, N0 = 11

646
CHAPTER 11:
Introduction to the Design of Discrete Filters
That is, the output y[n] is a combination of the input x[n] and of a delayed and attenuated version
of the input. Determine the transfer function of the ﬁlter that gives the above input–output equa-
tion. Use MATLAB to plot its magnitude and phase. If the phase is nonlinear, how would you
recover the input x[n] (which is the message)? Let the input be x[n] = 2 + cos(πn/4) + cos(πn). In
practice, the delay N0 and the attenuation α are not known at the receiver and need to be estimated.
What would happen if the delay is estimated to be 12 and the attenuation 0.79?
Solution
The transfer function of the ﬁlter with input x[n] and output y[n] is
H(z) = Y(z)
X(z) = 1 −0.8z−11 = z11 −0.8
z11
with a pole of z = 0 of multiplicity 11, and zeros the roots of z11 −0.8 = 0, or
zk = (0.8)1/11ej2πk/11
k = 0, . . . , 10
Using the freqz function to plot its magnitude and phase responses (see Figure 11.5), we ﬁnd that
the phase is nonlinear, and as such the output of H(z), y[n], will not be a delayed version of the
input. To recover the input, we use an inverse ﬁlter G(z) such that cascaded with H(z) the overall
ﬁlter is an all-pass ﬁlter (i.e., H(z)G(z) = 1). Thus,
G(z) =
z11
z11 −0.8
The poles and zeros and the magnitude and the phase responses of H(z) are shown in
Figure 11.5(a). The ﬁlters with transfer functions H(z) and G(z) are called comb ﬁlters given the
shape of their magnitude responses.
If the delay is estimated to be 11 and the attenuation 0.8, the input signal x[n] (the message) is
recovered exactly; however, if we have slight variations on these values the message might not be
recovered. When the delay is estimated to be 12 and the attenuation 0.79, the inverse ﬁlter is
G(z) =
z12
z12 −0.79
having the poles, zeros, and magnitude and phase responses shown in Figure 11.5(b). In
Figure 11.5(c), the effect of these changes are illustrated. The output of the inverse ﬁlter z[n] does
not resemble the sent signal x[n]. The signal y[n] is the output of the channel with a transfer
function H(z).
I

11.2 Frequency-Selective Discrete Filters
647
0
10
20
30
40
50
60
70
80
90
100
−10
0
10
x [n]
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
−10
0
10
y [n]
−10
0
10
n
n
n
z [n]
(c)
0
0.5
1
0
0.5
1
1.5
ω/π
|H (e jω)|
−1
0
1
−1
−0.5
0
0.5
1
11
Real part
Imaginary part
12
−1
−0.5
0
0.5
1
Imaginary part
−1
0
1
Real part
0
2
4
|G(ejω)|
0
0.5
1
ω/π
0
0.5
1
−1
−0.5
0
0.5
1
<G(ejω)
ω/π
(b)
0
0.5
1
−1
−0.5
0
0.5
1
ω/π
<H(e jω)
(a)
FIGURE 11.5
(a) Poles and zeros and frequency response of the FIR comb ﬁlter H(z) = (z11 −0.8)/z11, and (b) the estimated
inverse IIR comb ﬁlter G(z) = z12/(z12 −0.79). (c) The message x[n] = 2 + cos(πn/4)+ cos(πn), the output
y[n] of channel H(z), and the output z[n] of the estimated inverse ﬁlter G(z).

648
CHAPTER 11:
Introduction to the Design of Discrete Filters
11.3 FILTER SPECIFICATIONS
There are two ways to specify a discrete ﬁlter: in the frequency domain and in the time domain. The
frequency-domain speciﬁcation of the desired ﬁlter’s magnitude and phase is more common in IIR
ﬁlter design, while the time-domain speciﬁcation in terms of the desired impulse response of the
ﬁlter is more common in FIR ﬁlter design.
11.3.1 Frequency-Domain Speciﬁcations
Conventionally, when designing IIR ﬁlters a prototype low-pass ﬁlter is obtained ﬁrst and then con-
verted into the desired ﬁlter. The magnitude speciﬁcations of a discrete low-pass ﬁlter are given for
frequencies [0, π] due to the periodicity and the even characteristics of the magnitude. Typically, the
phase is not speciﬁed but it is expected to be approximately linear.
For a low-pass ﬁlter, the desired magnitude |Hd(ejw)| is to be close to unity in a passband frequency
region, and close to zero in a stopband frequency region. A transition frequency region where the
ﬁlter is not speciﬁed is needed. Thus, the magnitude speciﬁcations are displayed in Figure 11.6(a).
The passband [0, ωp] is the band of frequencies for which the attenuation speciﬁcation is the smallest;
the stopband [ωst, π] is the band of frequencies where the attenuation speciﬁcation is the greatest; and
the transition band (ωp, ωst) is the frequency band where the ﬁlter is not speciﬁed. The frequencies
ωp and ωst are called the passband and the stopband frequencies, respectively.
Loss Function
As for analog ﬁlters, the linear, or normal, scale speciﬁcations shown in Figure 11.6(a) do not give
the sense of the attenuation, and thus the loss or log speciﬁcation in decibels (dB) is preferred. The
logarithmic scale also provides a greater resolution of the magnitude. Figure 11.7 shows the relation
between a magnitude value |G| and its corresponding loss value in decibels. Notice that the loss
increases by 20 dB whenever the ﬁlter attenuates the input signal by a factor of 10−1.
The magnitude speciﬁcations of a discrete low-pass ﬁlter in a linear scale are (Figure 11.6(a)):
Passband:
δ1 ≤|H(ejω)| ≤1
0 ≤ω ≤ωp
Stopband:
0 < |H(ejω)| ≤δ2
ωst ≤ω ≤π
(11.11)
FIGURE 11.6
Low-pass magnitude
speciﬁcations: (a)
normal scale and (b)
logarithmic scale or loss.
|H(e jω)|
1
π
ω
(a)
δ 1
δ 2
ωp
ωst
ω
α(e jω)(dB)
αmin
αmax
0
(b)
π
ωp
ωst

11.3 Filter Speciﬁcations
649
FIGURE 11.7
Loss α (dB) for
normalized gain |G|.
10−5
10−4
10−3
10−2
10−1
100
Loss α (dB)
Magnitude |G|
0
10
20
30
40
50
60
70
80
90
100
Deﬁning the loss function for a discrete ﬁlter as
α(ejω) = −10 log10 |H(ejω)|2 = −20 log10 |H(ejω)|
dB
(11.12)
equivalent magnitude speciﬁcations for a discrete low-pass ﬁlter are (Figure 11.6(b)):
Passband:
0 ≤α(ejω) ≤αmax
0 ≤ω ≤ωp
Stopband:
αmin ≤α(ejω) < ∞
ωst ≤ω ≤π
(11.13)
where αmax = −20 log10 δ1 and αmin = −20 log10 δ2 (which are positive since both δ1 and δ2 are positive
and smaller than 1).
I Example 11.3
Consider the following speciﬁcations for a low-pass ﬁlter:
0.9 ≤|H(ejω)| ≤1.0
0 ≤ω ≤π/2
0 < |H(ejω)| ≤0.1
3π/4 ≤ω ≤π
Determine the equivalent loss speciﬁcations.

650
CHAPTER 11:
Introduction to the Design of Discrete Filters
Solution
The loss speciﬁcations are then
0 ≤α(ejω) ≤0.92
0 ≤ω ≤π/2
α(ejω) ≥20
3π/4 ≤ω ≤π
where αmax = 0.92 dB and αmin = 20 dB. These speciﬁcations indicate that in the passband the loss
is small, or that the magnitude would change from 1 to
10−αmax/20 = 10−0.92/20 = 0.9
while in the stopband we would like a large attenuation, at least αmin, or that the magnitude would
have values smaller than
10−αmin/20 = 0.1
I
Remarks
I
The dB scale is an indicator of attenuation: If we have a unit magnitude the corresponding loss is 0 dB,
and for every 20 dB in loss this magnitude is attenuated by 10−1, so that when the loss is 100 dB the unit
magnitude would be attenuated to 10−5. The dB scale also has the physiological signiﬁcance of being a
measure of how humans detect levels of sound.
I
Besides the physiological signiﬁcance, the loss speciﬁcations have intuitive appeal. They indicate that
in the passband, where minimal attenuation of the input signal is desired, the “loss” is minimal as it is
constrained to be below a maximum loss of αmax dB. Likewise, in the stopband where maximal attenuation
of the input signal is needed, the “loss” is set to be larger than αmin dB.
I
When specifying a high-quality ﬁlter the αmax value should be small, the αmin value should be large,
and the transition band should be as narrow as possible—that is, approximating as much as possible the
frequency response of an ideal low-pass ﬁlter. The cost of this is a large order for the resulting ﬁlter, making
the implementation expensive computationally and requiring large memory space.
Magnitude Normalization
The speciﬁcations of the low-pass ﬁlter in Figure 11.6 are normalized in magnitude: The dc gain
is assumed to be unity (or the dc loss is 0 dB), but there are many cases where that is not so. See
Figure 11.8.
Not-normalized magnitude speciﬁcations: In general, the dc loss is different from 0 dB, so that the loss
speciﬁcations are
α1 ≤ˆα(ejω) ≤α2
0 ≤ω ≤ωp
α3 ≤ˆα(ejω)
ωst ≤ω ≤π
Writing the above loss as
ˆα(ejω) = α1 + α(ejω)
(11.14)

11.3 Filter Speciﬁcations
651
FIGURE 11.8
Not-normalized loss speciﬁcations for a
low-pass ﬁlter.
ω
α(e jω)(dB)
α 3
α 2
α 1
π
ωp
ωst
the dc loss of α1 is achieved by multiplying a magnitude-normalized ﬁlter by a constant K, such that
ˆα(ej0) = α1 = −20 log10 K
or
K = 10−α1/20
(11.15)
The normalized speciﬁcations are then
0 ≤α(ejω) ≤αmax
0 ≤ω ≤ωp
αmin ≤α(ejω)
ωst ≤ω ≤π
where αmax = α2 −α1 and αmin = α3 −α1.
I Example 11.4
Suppose the loss speciﬁcations of a low-pass ﬁlter are
10 ≤ˆα(ejω) ≤11
0 ≤ω ≤π
2
ˆα(ejω) ≥50
3π
4 ≤ω ≤π
Determine the loss speciﬁcations that can be used to design a magnitude-normalized ﬁlter, and
the gain K to make it satisfy the above speciﬁcations.
Solution
If we let
ˆα(ejω) = 10 + α(ejω)
the loss speciﬁcations for a normalized ﬁlter would be
0 ≤α(ejω) ≤1
0 ≤ω ≤π
2
α(ejω) ≥40
3π
4 ≤ω ≤π

652
CHAPTER 11:
Introduction to the Design of Discrete Filters
Then the dc loss is 10 dB and αmax = 1 and αmin = 40 dB. Suppose that we design a ﬁlter H(z) that
satisﬁes the normalized ﬁlter speciﬁcations. If we let ˆH(z) = KH(z) be the ﬁlter that satisﬁes the
given loss speciﬁcations, at the dc frequency we must have that
−20 log10 | ˆH(ej0)| = −20 log10 K −20 log10 |H(ej0)|
10 = −20 log10 K + 0
so that K = 10−0.5 = 1/
√
10.
I
Frequency Scales
Given that discrete ﬁlters can be used to process continuous-time as well as discrete-time signals,
there are different equivalent ways in which the frequency of a discrete ﬁlter can be expressed (see
Figure 11.9).
In the discrete processing of continuous-time signals the sampling frequency ( fs in hertz or s in rad/sec) is
known, and so we have the following possible scales:
I
The f(Hz) scale from 0 to fs/2, the foldover or Nyquist frequency, that comes from the sampling theory.
I
The scale  = 2πf (rad/sec) where f is the previous scale, the frequency range is then from 0 to s/2.
I
The discrete frequency scale ω = Ts (rad) ranging from 0 to π.
I
A normalized discrete-frequency scale ω/π (no units) ranging from 0 to 1.
If the speciﬁcations are in the discrete domain, the scale is the ω (rad) or the normalized ω/π.
Remarks Other scales are possible, but less used. One of these consists in dividing by the sampling frequency
either in hertz or in rad/sec: The f/fs (no units) scale goes from 0 to 1/2, and so does the /s (no units)
scale. It is clear that when the speciﬁcations are given in any scale, it can be easily transformed into any
other desired scale. If the ﬁlter is designed for use in the discrete domain only the scales in radians and the
normalized ω/π are meaningful.
11.3.2 Time-domain Speciﬁcations
Time-domain speciﬁcations consist in giving a desired impulse response hd[n]. For instance, when
designing a low-pass ﬁlter with cut-off frequency ωc and linear phase φ(ω) = −Nω, the desired
FIGURE 11.9
Frequency scales used in discrete ﬁlter design.
ω (rad)
0
Ω (rad/sec)
0
f (Hz)
0
π
0.5 Ωs
0.5 fs
1
0
ωp
Ωp
fp
ωp
π
ωst
Ωst
fst
π
ωst
ω
π

11.4 IIR Filter Design
653
frequency response in 0 ≤ω ≤π is
Hd(ejω) =
1e−jωN
0 ≤ω ≤ωc
0
ωc < ω ≤π
The desired impulse response for this ﬁlter is then found from
hd[n] = 1
2π
ωc
Z
−ωc
1e−jωNejωndω
The resulting hd[n] will be used as the desired impulse response to approximate.
I Example 11.5
Consider an FIR ﬁlter with the following desired magnitude response in 0 ≤ω ≤π:
|Hd(ejω)| =
1
0 ≤ω ≤π
4
0
elsewhere in 0 ≤ω ≤π
and zero phase. Find the desired impulse response hd[n] that we wish to approximate.
Solution
The desired impulse response is computed as follows:
hd[n] = 1
2π
π
Z
−π
Hd(ejω)ejωndω = 1
2π
π/4
Z
−π/4
ejωndω
=
sin(πn/4)/πn
n ̸= 0
0.25
n = 0
which corresponds to the impulse response of a noncausal system. As we will see later, windowing
and shifting of hd[n] are needed to make it into a causal, ﬁnite-length ﬁlter.
I
11.4 IIR FILTER DESIGN
Two possible approaches in the design of IIR ﬁlters are:
I
Using analog ﬁlter design methods and transformations between the s-plane and the z-plane.
I
Using optimization techniques.
The ﬁrst is a frequency transformation approach. Using a mapping between the analog and the
discrete frequencies, we obtain the speciﬁcations for an analog ﬁlter from the discrete ﬁlter speci-
ﬁcations. Applying well-known analog ﬁlter design methods, we then design the analog ﬁlter from
the transformed speciﬁcations. The discrete ﬁlter is ﬁnally obtained by transforming the designed
analog ﬁlter.

654
CHAPTER 11:
Introduction to the Design of Discrete Filters
The optimal approach designs the ﬁlter directly, setting the rational approximation as a nonlinear
optimization. The added ﬂexibility of this approach is diminished by the need to ensure stability of
the designed ﬁlter. Stability is guaranteed, on the other hand, in the transformation approach.
11.4.1 Transformation Design of IIR Discrete Filters
To take advantage of well-understood analog ﬁlter design, a common practice is to design dis-
crete ﬁlters by means of analog ﬁlters and mappings of the s-plane into the z-plane. Two mappings
used are:
I
The sampling transformation z = esTs.
I
The bilinear transformation,
s = K 1 −z−1
1 + z−1
Recall the transformation z = esTs was found when relating the Laplace transform of a sampled signal
with its Z-transform. Using this transformation, we convert the analog impulse response ha(t) of an
analog ﬁlter into the impulse response h[n] of a discrete ﬁlter and obtain the corresponding transfer
function. The resulting design procedure is called the impulse-invariant method. Advantages of this
method are:
I
It preserves the stability of the analog ﬁlter.
I
Given the linear relation between the analog and the discrete frequencies the speciﬁcations for
the discrete ﬁlter can be easily transformed into the speciﬁcations for the analog ﬁlter.
Its drawback is possible frequency aliasing. Sampling of the analog impulse response requires that
the analog ﬁlter be band limited, which might not be possible to satisfy in all cases. Due to this we
will concentrate on the approach based on the bilinear transformation.
Bilinear Transformation
The bilinear transformation results from the trapezoidal rule approximation of an integral. Suppose
that x(t) is the input and y(t) is the output of an integrator with transfer function
H(s) = Y(s)
X(s) = 1
s
(11.16)
Sampling the input and the output of this ﬁlter using a sampling period Ts, we have that the integral
at time nTs is
y(nTs) =
nTs
Z
(n−1)Ts
x(τ)dτ + y((n −1)Ts)
(11.17)
where y((n −1)Ts) is the integral at time (n −1)Ts. Consider then the approximation of the integral. If
Ts is very small, the integral between (n −1)Ts and nTs can be approximated by the area of a trapezoid

11.4 IIR Filter Design
655
with bases x((n −1)Ts) and x(nTs) and height Ts (this is called the trapezoidal rule approximation of
an integral):
y(nTs) ≈[x(nTs) + x((n −1)Ts)] Ts
2
+ y((n −1)Ts)
(11.18)
with a Z-transform given by
Y(z) = Ts(1 + z−1)
2(1 −z−1) X(z)
The discrete transfer function is thus
H(z) = Y(z)
X(z) = Ts
2
1 + z−1
1 −z−1
(11.19)
which can be obtained directly from H(s) by letting
s = 2
Ts
1 −z−1
1 + z−1
(11.20)
The resulting transformation is linear in both numerator and denominator, and thus it is called
the bilinear transformation. Thinking of the above transformation as a transformation from the z to
the s variable, solving for the variable z in that equation, we obtain a transformation from the s to the
z variable:
z = 1 + (Ts/2)s
1 −(Ts/2)s
(11.21)
The bilinear transformation:
z- to s-plane:
s = K 1 −z−1
1 + z−1
K = 2
Ts
s- to z-plane:
z = 1 + s/K
1 −s/K
(11.22)
maps
I
The j axis in the s-plane into the unit circle in the z-plane.
I
The open left-hand s-plane Re[s] < 0 into the inside of the unit circle in the z-plane, or |z| < 1.
I
The open right-hand s-plane Re[s] > 0 into the outside of the unit circle in the z-plane, or |z| > 1.
Thus, as shown in Figure 11.10, for point A, s = 0 or the origin of the s-plane is mapped into z = 1 on
the unit circle; for points B and B′, s = ±j∞are mapped into z = −1 on the unit circle; for point C,
s = −1 is mapped into z = (1 −1/K)/(1 + 1/K) < 1, which is inside the unit circle; and ﬁnally for
point D, s = 1 is mapped into z = (1 + 1/K)/(1 −1/K) > 1, which is located outside the unit circle.

656
CHAPTER 11:
Introduction to the Design of Discrete Filters
FIGURE 11.10
Bilinear transformation mapping of s-plane into
z-plane.
A
B
B'
σ
jΩ
s-plane
C
D
A
B
B'
C
z-plane
D
In general, by letting K = 2
Ts , z = rejω and s = σ + j in Equation (11.21), we obtain
r =
s
(1 + σ/K)2 + (/K)2
(1 −σ/K)2 + (/K)2
ω = tan−1

/K
1 + σ/K

+ tan−1

/K
1 −σ/K

(11.23)
From this we have that:
I
In the j axis of the s-plane (i.e., when σ = 0 and −∞<  < ∞), we obtain r = 1 and −π ≤
ω < π, which correspond to the unit circle of the z-plane.
I
On the open left-hand s-plane, or equivalently when σ < 0 and −∞<  < ∞, we obtain r < 1
and −π ≤ω < π, or the inside of the unit circle in the z-plane.
I
Finally, on the open right-hand s-plane, or equivalently when σ > 0 and −∞<  < ∞, we
obtain r > 1 and −π ≤ω < π, or the outside of the unit circle in the z-plane.
The above transformation can be visualized by thinking of a giant who puts a nail in the origin of the
s-plane and then grabs the plus and minus inﬁnity extremes of the j axis and pulls them together
to make them agree into one point, getting a magniﬁcent circle, keeping everything in the left plane
inside, and keeping out the rest. If our giant lets go, we get back the original s-plane!
Remarks The bilinear transformation maps the whole s-plane into the whole z-plane, differently from the
transformation z = esTs that only maps a slab of the s-plane into the z-plane (see Chapter 9 on the Z-
transform). Thus, a stable analog ﬁlter with poles in the open left-hand s-plane will generate a discrete ﬁlter
that is also stable as it has poles inside the unit circle.
Frequency Warping
A minor drawback of the bilinear transformation is the nonlinear relation between the analog and the
discrete frequencies. Such a relation creates a warping that needs to be taken care of when specifying
the analog ﬁlter using the discrete ﬁlter speciﬁcations.
The analog frequency  and the discrete frequency ω according to the bilinear transformation are related by
 = K tan(ω/2)
(11.24)

11.4 IIR Filter Design
657
FIGURE 11.11
Relation between  and ω
for K = 1.
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
−10
−8
−6
−4
−2
0
2
4
6
8
10
Ω(rad/ sec)
ω/π
which when plotted displays a linear relation around the low frequencies but it warps as we get into large
frequencies (see Figure 11.11).
The relation between the frequencies is obtained by letting σ = 0 in the second equation in Equa-
tion (11.23). The linear relationship at low frequencies can be seen using the expansion of the tan(.)
function
 = K
ω
2 + ω3
24 + · · ·

≈ω
Ts
for small values of ω or ω ≈Ts. As frequency increases the effect of the terms beyond the ﬁrst one
makes the relation nonlinear. See Figure 11.11.
To compensate for the nonlinear relation between the frequencies, or the warping effect, the following
steps to design a discrete ﬁlter are followed:
1.
Using the frequency warping relation (Eq. 11.24) the speciﬁed discrete frequencies ωp and ωst are
transformed into speciﬁed analog frequencies p and st. The magnitude speciﬁcations remain
the same in the different bands—only the frequency is being transformed.
2.
Using the speciﬁed analog frequencies and the discrete magnitude speciﬁcations, an analog ﬁlter
HN(s) that satisﬁes these speciﬁcations is designed.
3.
Applying the bilinear transformation to the designed ﬁlter HN(s), the discrete ﬁlter HN(z) that
satisﬁes the discrete speciﬁcations is obtained.

658
CHAPTER 11:
Introduction to the Design of Discrete Filters
11.4.2 Design of Butterworth Low-Pass Discrete Filters
Our aim in this section is to show how to design discrete low-pass ﬁlters based on the analog
Butterworth low-pass ﬁlter design using the bilinear transformation as a frequency transformation.
Applying the warping relation between the continuous and the discrete frequencies
 = K tan(ω/2)
(11.25)
to the magnitude-squared function of the Butterworth low-pass analog ﬁlter
HN(′)
2 =
1
1 + (′)2N
′ =

hp
gives the magnitude-squared function for the Butterworth low-pass discrete ﬁlter:
|HN(ejω)|2 =
1
1 +
h tan(0.5ω)
tan(0.5ωhp)
i2N
(11.26)
As a frequency transformation (no change to the loss speciﬁcations) we directly obtain the minimal order N
and the half-power frequency bounds by replacing
s
p
= tan(ωs/2)
tan(ωp/2)
(11.27)
in the corresponding formulas for N and hp of the analog ﬁlter, giving
N ≥log10[(100.1αmin −1)/(100.1αmax −1)]
2 log10
h tan(ωs/2)
tan(ωp/2)
i
2 tan−1

tan(ωp/2)
(100.1αmax −1)1/2N

≤ωhp ≤2 tan−1

tan(ωs/2)
(100.1αmin −1)1/2N

(11.28)
The normalized half-power frequency ′
hp = 1 in the continuous domain is mapped into the discrete half-
power frequency ωhp, giving the constant in the bilinear transformation
Kb =
′
tan(0.5ω)
′=1,ω=ωhp =
1
tan(0.5ωhp)
(11.29)
The bilinear transformation s = Kb(1 −z−1)/(1 + z−1) is then used to convert the analog ﬁlter HN(s),
satisfying the transformed speciﬁcations, into the desired discrete ﬁlter,
HN(z) = HN(s)
s=Kb(1−z−1)/(1+z−1)
The basic idea of this design is to convert an analog frequency-normalized Butterworth magnitude-
squared function into a discrete function using the relationship in Equation (11.25). To understand

11.4 IIR Filter Design
659
why this is an efﬁcient approach consider the following issues that derive from the application of the
bilinear transformation to the Butterworth design:
I
Since the discrete magnitude speciﬁcations are not changed by the bilinear transformation,
we only need to change the analog frequency term in the formulas obtained before for the
Butterworth low-pass analog ﬁlter.
I
It is important to recognize that when ﬁnding the minimal order N and the half-power relation
the value of K is not used. This constant is only important in the ﬁnal step where the analog ﬁlter
is transformed into the discrete ﬁlter using the bilinear transformation.
I
When considering that K = 2/Ts depends on Ts, one might think that a small value for Ts
improves the design, but that is not the case. Given that the analog frequency is related to the
discrete frequency as
 = 2
Ts
tan
ω
2

(11.30)
for a given value of ω if we choose a small value of Ts the speciﬁed analog frequency would
increase, and if we choose a large value of Ts the analog frequency would decrease. In fact, in
the above equation we can only choose either  or Ts. To avoid this ambiguity, we ignore the
connection of K with Ts and concentrate on K.
I
An appropriate value for K for the Butterworth design is obtained by connecting the normalized
half-power frequency 
′
hp = 1 in the analog domain with the corresponding frequency ωhp in
the discrete-domain. This allows us to go from the discrete-domain speciﬁcations directly to the
analog normalized frequency speciﬁcations. Thus, we map the normalized half-power frequency
′
hp = 1 into the discrete half-power frequency ωhp, by means of Kb.
I
Once the analog ﬁlter HN(s) is obtained, using the bilinear transformation with the Kb we
transform HN(s) into a discrete ﬁlter
HN(z) = HN(s)
s=Kb z−1
z+1
I
The ﬁlter parameters (N, ωhp) can also be obtained directly from the discrete loss function
α(ejω) = 10 log
h
1 + (tan(0.5ω)/ tan(0.5ωhp))2Ni
(11.31)
and the loss speciﬁcations
0 ≤α(ejω) ≤αmax
0 ≤ω ≤ωp
α(ejω) ≥αmin
ω ≥ωst
just as we did in the continuous case. The results coincide with those where we replace the warping
frequency relation.
I Example 11.6
The analog signal
x(t) = cos(40πt) + cos(500πt)

660
CHAPTER 11:
Introduction to the Design of Discrete Filters
is sampled using the Nyquist frequency and processed with a discrete ﬁlter H(z) that is obtained
from a second-order, high-pass analog ﬁlter
H(s) =
s2
s2 +
√
2s + 1
The discrete-time output y[n] is then converted into analog. Apply MATLAB’s bilinear function to
obtain the discrete ﬁlter with half-power frequencies ωhp = π/2. Use MATLAB to plot the poles
and the zeros of the discrete ﬁlter in the z-plane and the corresponding magnitude and phase
responses. Use the function plot to plot the sampled input and the ﬁlter output and consider these
approximations to the analog signals. Change the frequency scale of the discrete ﬁlter into f in
hertz and indicate what is the corresponding half-power frequency in hertz.
Solution
The coefﬁcients of the numerator and denominator of the discrete ﬁlter are found from H(s) using
the MATLAB function bilinear. The input Fs in this function equals Kb/2 where Kb corresponds to
the transformation of the discrete half-power frequency ωhp into the normalized analog half-power
frequency hp = 1. The following script is used.
%%%%%%%%%%%%%%%
% Example 11.6
%%%%%%%%%%%%%%%
b = [1 0 0]; a = [1 sqrt(2) 1]; % coefﬁcients of analog ﬁlter
whp = 0.5 ∗pi; % desired half-power frequency of discrete ﬁlter
Kb = 1/tan(whp/2); Fs = Kb/2; [num, den]=bilinear(b,a,Fs); % bilinear transformation
Ts = 1/500; % sampling period
n = 0:499; x1 = cos(2 ∗pi ∗20 ∗n ∗Ts)+cos(2 ∗pi ∗250 ∗n ∗Ts); % sampled signal
zplane(num, den) % poles/zeros of discrete ﬁlter
[H,w] = freqz(num,den); % frequency response of discrete ﬁlter
phi = unwrap(angle(H)); % unwrapped phase of discrete ﬁlter
y = ﬁlter(num,den,x1); % output of discrete ﬁlter with input x1
We ﬁnd the transfer function of the discrete ﬁlter to be
H(z) = 0.2929(1 −z−1)2
1 + 0.1715z−2
The poles and the zeros of H(z) can be found with the MATLAB function roots and plotted with
zplane. The frequency response is obtained using freqz. To have the frequency scale in hertz we
consider that ω = Ts, letting  = 2πf, then
f =
ω
2πTs
=
ω
π
 fs
2


11.4 IIR Filter Design
661
−1
0
1
2
Real part
Imaginary
part
−5
0
5
−2
−1
0
1
2
t
x(t )
−1
−0.5
0
0.5
1
t
y(t)
0
0.02
0.04
0.06
0.08
0.1
0
0.02
0.04
0.06
0.08
0.1
(b)
0
0.5
1
|H (f )|
0
50
100
150
200
250
0
50
100
150
200
250
0
2
4
<H (f)
f [Hz]
(a)
FIGURE 11.12
Bilinear transformation of a high-pass analog ﬁlter into a discrete ﬁlter with half-power frequencies ωhp = π/2 or
fhp = 125 Hz. (a) Poles and zeros and magnitude and phase responses of the discrete ﬁlter. (b) The analog input
and output obtained using the MATLAB function plot to interpolate the sampled signal x(nTs) and the output of
the discrete ﬁlter y(nTs) into x(t) and y(t).
so we multiply the normalized discrete frequency ω/π by fs/2 = 250, resulting in a maximum
frequency of 250 Hz. The half-power frequency in hertz is thus 125 Hz. The magnitude and the
phase responses of H(z) are shown in Figure 11.12. Notice that phase is approximately linear in
the passband, despite the fact that no phase speciﬁcations are considered.
Since the maximum frequency of x(t) is 250 Hz we choose Ts = 1/500. As a high-pass ﬁlter, when
we input x(nTs) into H(z) its low-frequency component cos(40πnTs) is attenuated. The input and
corresponding output of the ﬁlter are shown in Figure 11.12.
I
Remarks
I
The design is simpliﬁed by giving a desired half-power frequency ωhp, as we then only need to calculate
the order of the ﬁlter by using the stopband constraint. In this case, by setting αmax = 3 dB and ωp = ωhp
one could also use the lower equation in Equation (11.28) to ﬁnd N.
I
A very important consequence of using the bilinear transformation is that the resulting transfer function
HN(z) is guaranteed to be BIBO stable. This transformation maps the poles of a stable ﬁlter HN(s) in
the left-hand s-plane into poles inside the unit circle corresponding to HN(z), making the discrete ﬁlter
stable.
I
The bilinear transformation creates a pole and a zero in the z-plane for each pole in the s-plane. Analytic
calculation of the poles of HN(z) is not as important as in the analog case. The MATLAB function zplane
can be used to plot its poles and zeros, and the function roots can be used to ﬁnd the values of the poles
and zeros.

662
CHAPTER 11:
Introduction to the Design of Discrete Filters
I
Applying the bilinear transformation by hand to ﬁlters of order higher than 2 is cumbersome. When doing
so, HN(s) should be expressed as a product or sum of ﬁrst- and second-order transfer functions before
applying the bilinear transformation to each. That is, we express HN(s) as
HN(s) =
Y
i
HNi(s)
or
=
X
ℓ
HNℓ(s)
where HNi(s) or HNℓ(s) are ﬁrst- or second-order functions with real coefﬁcients. Applying the bilinear
transformation to each of the HNi(s) or HNℓ(s) components to obtain HNi(z) and HNℓ(z), the discrete
ﬁlter becomes
HN(z) =
Y
i
HNi(z)
or
=
X
ℓ
HNℓ(z)
I
Since the resulting ﬁlter has normalized magnitude, a speciﬁed dc gain can be attained by multiplying
HN(z) by a constant value G so that |GH(ej0)| equals the desired dc gain.
I Example 11.7
The speciﬁcations of a low-pass discrete ﬁlter are:
ωp = 0.47π (rad)
αmax = 2 dB
ωst = 0.6π (rad)
αmin = 9 dB
α(ej0) = 0 dB
Use MATLAB to design a discrete low-pass Butterwoth ﬁlter by means of the bilinear
transformation.
Solution
Since the frequency speciﬁcations are in radians, we use these values directly in the MATLAB func-
tion buttord, which for inputs αmax, αmin, ωp/π, and ωst/π, provides the minimal order N and the
half-power frequency ωhp/π of the ﬁlter. Notice that the input and the output frequencies are nor-
malized (i.e., divided by π). With the outputs of buttord as inputs of the function butter we obtain
the coefﬁcients of the numerator and the denominator of the designed ﬁlter H(z) = B(z)/A(z).
The function roots is used to ﬁnd the poles and the zeros of H(z), while zplane plots them. The
magnitude and the phase responses are found using freqz aided by the functions abs, angle, and
unwrap. Notice that butter obtains the normalized analog ﬁlter and transforms it using the bilinear
transformation. The script used is as follows.

11.4 IIR Filter Design
663
%%%%%%%%%%%%%%%%
% Example 11.7
%%%%%%%%%%%%%%%%
% LP Butterworth
alphamax = 2; alphamin = 9; % loss speciﬁcations
wp = 0.47;ws = 0.6; % passband and stopband frequencies
[N,wh] = buttord(wp,ws,alphamax,alphamin) % minimal order, half-power frequency
[b,a] = butter(N,wh); % coefﬁcients of designed ﬁlter
[H,w] = freqz(b,a);w = w/pi;N = length(H); % frequency response
spec1 = alphamax ∗ones(1,N); spec2 = alphamin ∗ones(1,N); % speciﬁcation lines
hpf = 3.01 ∗ones(1,N); % half-power frequency line
disp(‘poles’) % display poles
roots(a)
disp(‘zeros’) % display zeros
roots(b)
alpha = -20 ∗log10(abs(H));
% loss in dB
The results of the design are shown in Figure 11.13. The order of the designed ﬁlter is N = 3 and
the half-power frequency is ωhp = 0.499π. The poles are along the imaginary axis of the z-plane
(Kb = 1) and there are three zeros at z = −1. The transfer function of the designed ﬁlter is
H(z) = 0.166 + 0.497z−1 + 0.497z−2 + 0.166z−3
1 −0.006z−1 + 0.333z−2 −0.001z−3
Finally, to verify that the speciﬁcations are satisﬁed we plot the loss function α(ejω) along with three
horizontal lines corresponding to αmax = 2 dB, 3 dB for the half-power frequency, and αmin = 9 dB.
−5
0
5
−1
0
1
3
Real part
Imaginary
part
0.1
0
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
2
4
6
8
10
12
α(ω)dB
ω/π
(b)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.5
1
|H(e jω)|
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−6
−4
−2
0
<H(e j ω)
ω/π
(a)
FIGURE 11.13
Design of low-pass Butterworth ﬁlter using MATLAB: (a) poles and zeros and magnitude and phase responses,
and (b) veriﬁcation of speciﬁcations using loss function α(ω).

664
CHAPTER 11:
Introduction to the Design of Discrete Filters
The crossings of these lines with the ﬁlter loss function indicate that at the normalized frequency
of 0.6 the loss is 9 as desired, and that at the normalized frequency 0.47 the loss is less than 2 dB,
so that the normalized half-power frequency is about 0.5.
I
I Example 11.8
In this example we consider designing a Butterworth low-pass discrete ﬁlter for processing an
analog signal. The ﬁlter speciﬁcations are:
fp = 2250 Hz passband frequency
fst = 2700 Hz stopband frequency
fs = 9000 Hz sampling frequency
α1 = −18 dB dc loss
α2 = −15 dB loss in passband
α3 = −9 dB loss in stopband
Solution
The speciﬁcations are not normalized (see Figure 11.14). Normalizing them, we have that:
α(ej0) = −18 dB
αmax = α2 −α1 = 3 dB
αmin = α3 −α1 = 9 dB
and
ωp = 2πfhp
fs
= 0.5π
ωst = 2πfst
fs
= 0.6π
Note that ωp = ωhp since the difference in the losses at dc and at ωp is 3 dB.
FIGURE 11.14
Loss speciﬁcations for a discrete low-pass
ﬁlter for processing an analog signal.
−18
−15
−9
f(KHz)
2.25 2.7
4.5
α(f )dB

11.4 IIR Filter Design
665
Since the sample period is
Ts = 1/fs = (1/9) × 10−3 sec/sample
⇒
Kb = cot(πfhpTs) = 1
Given that the half-power frequency is known, only the minimal order of the ﬁlter is needed. The
loss function for the Butterworth ﬁlter is then
α(ejω) = 10 log10
 
1 +
 tan(0.5ω)
tan(0.5ωhp)
2N!
= 10 log10(1 + (tan(0.5ω))2N)
since 0.5ωhp = π/4. For ω = ωst, letting α(ejωst) = αmin, solving for N we get
N = log10(100.1αmin −1)
2 log10(tan(0.5ωst))
Replacing αmin and ωst we obtain that the minimal order is N = 4. The MATLAB script used in the
design is as follows.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 11.8---ﬁltering of analog signal
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
wh = 0.5 ∗pi; ws = 0.6 ∗pi; alphamin = 9; Fs = 9000; % ﬁlter speciﬁcations
N = log10((10ˆ(0.1 ∗alphamin)-1))/(2 ∗log10(tan(ws/2)/tan(wh/2)));N = ceil(N)
[b,a] = butter(N,wh/pi);
[H,w] = freqz(b,a);w = w/pi;N = length(H);f = w ∗Fs/2;
alpha0 = -18;
G = 10ˆ(-alpha0/20);H = H ∗G;
spec2 = alpha0 + alphamin ∗ones(1,N);
hpf = alpha0 + 3.01 ∗ones(1,N);
disp(‘poles’); p = roots(a)
disp(‘zeros’); z = roots(b)
alpha = -20 ∗log10(abs(H));
The transfer function of the discrete ﬁlter is found as
H(z) = H1(z)H2(z)
where each H4i(z), i = 1, 2 is the result of applying the bilinear transformation to Hi(s), i = 1, 2
formed by pairs of complex-conjugate poles of the analog low-pass Butterworth. To ensure that the
loss is −18 dB at ω = 0, we included a gain G in the numerator so that
H′(z) = GH(z) =
G(z + 1)4
10.61(z2 + 0.45)(z2 + 0.04)
satisﬁes the loss speciﬁcations. Notice that when Kb = 1, the poles are imaginary. The dc gain G of
the ﬁlter is found from the dc loss α(ej0) = −18 = −20 log10 G as G = 7.94.

666
CHAPTER 11:
Introduction to the Design of Discrete Filters
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
4
Real part
Imaginary part
0
1000
2000
3000
4000
0
2
4
6
8
|H(f)|
f (Hz)
0
1000
2000
3000
4000
−6
−4
−2
0
<H (f)
f(Hz)
0
1000
2000
3000
4000
−20
−15
−10
−5
0
α (f ) dB
f(Hz)
FIGURE 11.15
Low-pass ﬁlter for ﬁltering of an analog signal.
Since this ﬁlter is used to ﬁlter an analog signal the frequency scale of the magnitude and the
phase responses of the ﬁlter is given in hertz (see Figure 11.15). To verify that the speciﬁcations
are satisﬁed the loss function is plotted and compared with the losses corresponding to fhp and fst.
The loss at fhp = 2250 Hz coincides with the dc loss plus 3 dB, and the loss at fst = 2700 Hz is
above the speciﬁed value.
I
11.4.3 Design of Chebyshev Low-Pass Discrete Filters
The design of Chebyshev low-pass ﬁlters is very similar to the design of Butterworth low-pass ﬁlters.
The constant Kc of the bilinear transform for the Chebyshev ﬁlter is calculated by transforming the normalized
pass frequency ′p = 1 into the discrete frequency ωp:
Kc =
1
tan(0.5ωp)
(11.32)

11.4 IIR Filter Design
667
Replacing
′ = 
p
= Kc tan(0.5ω) = tan(0.5ω)
tan(0.5ωp)
(11.33)
into the magnitude-squared function for the Chebyshev analog ﬁlter it yields the magnitude-squared function
of the discrete Chebyshev low-pass ﬁlter,
|HN(ejω)|2 =
1
1 + ε2C2
N(tan(0.5ω)/ tan(0.5ωp))
(11.34)
where C(.) are the Chebyshev polynomials of the ﬁrst kind encountered before in the analog design. The ripple
parameter remains the same as in the analog design (since it does not depend on frequency):
ε = (100.1α max −1)1/2
(11.35)
while using the warping relation between the continuous and discrete frequencies gives that the minimal
order of the ﬁlter is
N ≥cosh−1[(100.1α min −1)/(100.1α max −1)]1/2
cosh−1[tan(0.5ωst)/ tan(0.5ωp)]
(11.36)
and that the half-power frequency can be found to be
ωhp = 2 tan−1

tan(0.5ωp) cosh
 1
N cosh−1
1
ε

(11.37)
After calculating these parameters, the transfer function of the Chebyshev discrete ﬁlter is found by
transforming the Chebyshev analog ﬁlter of order N into a discrete ﬁlter using the bilinear transformation
HN(z) = HN(s)|s=Kc(1−z−1)/(1+z−1)
(11.38)
Remarks
I
Just as with the Butterworth ﬁlter, the equations for the ﬁlter parameters (N, ωhp) can be obtained from
the analog formulas by substituting
st
p
= Kc tan(0.5ωst) = tan(0.5ωst)
tan(0.5ωp)
I
The ﬁlter parameters (N, ωhp, ε) can also be found from the loss function, obtained from the discrete
Chebyshev squared magnitude,
α(ejω) = 10 log10

1 + ε2C2
N
 tan(0.5ω)
tan(0.5ωp)

(11.39)
This is done by following a similar approach to the one in the analog case.

668
CHAPTER 11:
Introduction to the Design of Discrete Filters
I
Like in the discrete Butterworth, for Chebyshev ﬁlters the dc gain (i.e., gain at ω = 0) can be set to any
desired value by allowing a constant gain G in the numerator such that
|HN(ej0)| = |HN(1)| = G|N(1)|
|D(1)| = desired gain
(11.40)
I
MATLAB provides two functions to design Chebyshev ﬁlters. The function cheby1 is for designing the
ﬁlters covered in this section, while cheby2 is to design ﬁlters with a ﬂat response in the passband and
with ripples in the stopband. The order of the ﬁlter is found using cheb1ord and cheb2ord. The functions
cheby1 and cheby2 will give the ﬁlter coefﬁcients.
I Example 11.9
Consider the design of two low-pass Chebyshev ﬁlters. The speciﬁcations for the ﬁrst ﬁlter are:
α(ej0) = 0 dB
ωp = 0.47π rad
αmax = 2 dB
ωst = 0.6π rad
αmin = 6 dB
For the second ﬁlter, let ωp = 0.48π rad and keep the other speciﬁcations. Determine the half-
power frequency of the two ﬁlters. Use MATLAB for the design.
Solution
We obtained in Example 11.7 a third-order Butterworth low-pass ﬁlter that satisﬁes the speciﬁca-
tions of the ﬁrst ﬁlter. According to the results in this example a second-order Chebyshev ﬁlter
satisﬁes the same speciﬁcations. It is always so that a Chebyshev ﬁlter satisﬁes the same speciﬁca-
tions as a Butterworth with a lower order. For the second ﬁlter we narrow the transition band by
0.01π radians, and so the order increases by one. The following is the script for the design of the
two ﬁlters.
%%%%%%%%%%%%%%%%%%%%%%%%%
% Example 11.9---LP Chebyshev
%%%%%%%%%%%%%%%%%%%%%%%%%
alphamax = 2; alphamin = 9; % loss specs
ﬁgure(1)
for i = 1:2,
wp = 0.47 + (i-1) ∗0.01; ws = 0.6; % normalized frequency specs
[N,wn] = cheb1ord(wp,ws,alphamax,alphamin)
[b,a] = cheby1(N,alphamax,wn);
wp = wp ∗pi;
% magnitude and phase
[H,w] = freqz(b,a); w = w/pi; M = length(H);H = H/H(1);
% to verify specs
spec0 = zeros(1,M); spec1 = alphamax ∗ones(1,M) ∗(-1)ˆ(N+1);
spec2 = alphamin ∗ones(1,M);
alpha = -20 ∗log10(abs(H));

11.4 IIR Filter Design
669
hpf = (3.01 + alpha(1)) ∗ones(1,M);
% epsilon and half-power frequency
epsi = sqrt(10ˆ(0.1 ∗alphamax)-1);
whp = 2 ∗atan(tan(0.5 ∗wp) ∗cosh(acosh(sqrt(10ˆ(0.1 ∗3.01)-1)/epsi)/N));
whp = whp/pi
% plotting
subplot(221); zplane(b,a)
subplot(222)
plot(w,abs(H)); grid; axis([0 max(w) 0 1.1 ∗max(abs(H))])
subplot(223)
plot(w,unwrap(angle(H)));grid;
subplot(224)
plot(w,alpha);
hold on; plot(w,spec0,‘r’); hold on; plot(w,spec1,‘r’)
hold on; plot(w,hpf,‘k’); hold on
plot(w,spec2,‘r’); grid; axis([0 max(w) 1.1 ∗min(alpha) 1.1 ∗(alpha(1) + 3)]);
hold off
ﬁgure(2)
end
The transfer function of the ﬁrst ﬁlter is
H1(z) = 0.224 + 0.449z−1 + 0.224z−2
1 −0.264z−1 + 0.394z−2
and its half-power frequency is ωhp = 0.493π rad. The second-order ﬁlter has a transfer function of
H2(z) = 0.094 + 0.283z−1 + 0.283z−2 + 0.094z−3
1 −0.691z−1 + 0.774z−2 −0.327z−3
and a half-power frequency of ωhp = 0.4902π. The poles and the zeros as well as the magnitude
and the phase responses of the two ﬁlters are shown in Figure 11.16. Notice the difference in the
gain (or losses) in the passband of the two ﬁlters. In order for the dc gain to be unity, the gain in
the even-order ﬁlter reaches values above 1, while the odd-order ﬁlter does the opposite.
The cut-off frequency given as output by cheb1ord and that cheby1 uses is ωp. Since the half power is
not given by cheb1ord, the half-power frequency of the ﬁlter is calculated using the minimal order
N, the ripple factor ε, and the passband frequency ωp in Equation (11.37). See the script.
I
I Example 11.10
Consider the following speciﬁcations of a ﬁlter that will be used to ﬁlter an acoustic signal:
dc gain = 10
Half-power frequency: fhp = 4 KHz
Band-stop frequency: fst = 5 Khz, αmin = 60 dB
Sampling frequency: fs = 20 KHz

670
CHAPTER 11:
Introduction to the Design of Discrete Filters
−1
0
1
−1
−0.5
0
0.5
1
2
Imaginary part
0
0.2
0.4
0.6
0.8
0
0.5
1
|H(e jω)|
|H(e jω)|
−1 −0.5
0
0.5
1
−1
−0.5
0
0.5
1
3
Real part
Imaginary part
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
0
0.5
1
−5
−4
−3
−2
−1
0
0
0.2
0.4
0.6
0.8
0
1
2
3
Real part
ω/pi
<H(e jω)
ω /pi
ω/π
ω /π
α(ω)dB
0
0.5
1
−4
−3
−2
−1
0
0
0.2
0.4
0.6
0.8
−2
−1
0
1
2
3
<H(e jω)
ω/π
ω /π
α (ω) dB
(a)
(b)
FIGURE 11.16
Two Chebyshev ﬁlters with different transition bands: (a) even-order ﬁlter for ωp = 0.47π, and
(b) odd-order ﬁlter for ωp = 0.48π (narrower transition band).

11.4 IIR Filter Design
671
FIGURE 11.17
Equal-order (N = 15) (a) Butterworth
and (b) Chebyshev ﬁlters for ﬁltering of
acoustic signal.
(b)
(a)
−1
0
1
−1
−0.5
0
0.5
1
Real part
Imaginary part
0
2000 4000 6000 8000
0
2
4
6
8
10
|H(f )|
f (Hz)
0
2000 4000 6000 8000
−40
−30
−20
−10
0
<H(f )
f (Hz)
0
−20
−10
0
10
20
α (ω) dB
f (Hz)
2000 4000 6000 8000
−1
0
1
−1
−0.5
0
0.5
1
Real part
Imaginary part
0
2000 4000 6000 8000
0
2
4
6
8
10
|H(f )|
f(Hz)
0
2000 4000 6000 8000
−40
−30
−20
−10
0
<H(f )
f(Hz)
0
2000 4000 6000 8000
−20
−10
0
10
20
α (ω) dB
f (Hz)
Design a Butterworth and a Chebyshev low-pass ﬁlters of the same order and compare their
frequency responses.
Solution
The speciﬁcations of the discrete ﬁlter are:
dc gain = 10 ⇒α(ej0) = −20 dB
Half-power frequency: ωhp = 2πfhp(1/fs) = 0.4π rad
Band-stop frequency: ωst = 2πfst(1/fs) = 0.5π rad

672
CHAPTER 11:
Introduction to the Design of Discrete Filters
When designing the Butterworth ﬁlter we only need to ﬁnd the minimal order N given that the
half-power frequency is speciﬁed. We ﬁnd that N = 15 satisﬁes the speciﬁcations. Using this value
with the given discrete half-power frequency the function butter gives the coefﬁcients of the ﬁlter.
See Figure 11.17 for the results.
The design of the Chebyshev ﬁlter with order N = 15 and half-power frequency ωhp = 0.4π cannot
be done directly with the function cheby1, as we do not have the passband frequency ωp. Using
the equation to compute the half-power frequency, we solve for ωp after we give a value to αmax
(arbitrarily chosen to be 0.001 dB), which allows us to compute the ripple factor ε. See the script
part corresponding to the Chebyshev design. The function cheby1 with inputs N, αmax, and ωp
gives the coefﬁcients of the designed ﬁlter. Using the coefﬁcients we plot the poles and the zeros,
the magnitude and the phase responses, and the loss function, as shown in Figure 11.17. According
to the loss function plots the Chebyshev ﬁlter displays a sharper response in the transition band
than the Butterworth ﬁlter, as expected.
% Butterworth/Chebyshev ﬁlters for analog signal
wh = 0.4 ∗pi;ws = 0.5 ∗pi; alphamin = 40;Fs = 20000;
% Butterworth
N = log10((10ˆ(0.1 ∗alphamin)-1))/(2 ∗log10(tan(ws/2)/tan(wh/2))); N = ceil(N)
% [b,a] = butter(N,wh/pi); % to get Butterworth ﬁlter get rid of ‘ % ’
% Chebyshev
alphamax = 0.001;
epsi = sqrt(10ˆ(0.1 ∗alphamax)-1);
% computation of wp for Chebyshev design
wp = 2 ∗atan(tan(0.5 ∗wh)/(cosh(acosh(sqrt(10ˆ(0.1 ∗3.01)-1)/epsi)/N))); wp = wp/pi;
[b,a] = cheby1(N,alphamax,wp);
% magnitude and phase
[H,w] = freqz(b,a);w = w/pi;M = length(H);f = w ∗Fs/2;
alpha0 = -20;H = H ∗10;
% to verify specs
spec2 = alpha0 + alphamin ∗ones(1,M);
hpf = alpha0 + 3.01 ∗ones(1,M);
alpha = -20 ∗log10(abs(H));
Ha = unwrap(angle(H));
I
11.4.4 Rational Frequency Transformations
As indicated before, the conventional approach to ﬁlter design is to obtain ﬁrst a prototype low-pass
ﬁlter and then to transform it into different types of ﬁlters by means of frequency transformations.
The magnitude speciﬁcations remain unchanged.
When using analog ﬁlters to design IIR discrete ﬁlters the frequency transformation could be done in
two ways:
I
Transform a prototype low-pass analog ﬁlter into a desired analog ﬁlter, which in turn is converted
into the desired discrete ﬁlter using the bilinear or other transformation.

11.4 IIR Filter Design
673
I
Design a prototype low-pass discrete ﬁlter and then transform it into the desired discrete ﬁlter
[1,55].
The ﬁrst approach has the advantage that the analog frequency transformations (See Chapter 6) are
available and well understood. Its drawback appears when applying the bilinear transformation as it
may cause undesirable warping in the higher frequencies. So the second approach will be used.
Given a prototype low-pass ﬁlter Hℓp(Z), we wish to transform it into a desired ﬁlter H(z), which is
typically another low-pass, band-pass, high-pass, or stopband ﬁlter. The transformation
G(z−1) = Z−1
(11.41)
should preserve the rationality and the stability of the low-pass prototype. Accordingly, G(z−1)
should
I
Be rational to preserve the rationality.
I
Map the inside of the unit circle in the Z-plane into the inside of the unit circle in the z-plane to
preserve stability.
I
Map the unit circle |Z| = 1 into the unit circe |z| = 1.
If Z = Rejθ and z = rejω, the third condition on G(z−1) corresponds to
G(e−jω) = |G(e−jω)|ej∠(G(e−jω)) =
1 e−jθ
| {z }
unit circle in Z-plane
(11.42)
indicating that the frequency transformation G(z−1) has the characteristics of an all-pass ﬁlter, with
magnitude |G(e−jω)| = 1 and phase ∠G(e−jω) = −θ.
Using the general form of the transfer function of an all-pass ﬁlter (ratio of two equal-order polyno-
mials with poles and zeros being the conjugate inverse of each other), we obtain the general form of
the rational transformation as
Z−1 = G(z−1) = K
Y
k
z−1 −αk
1 −α∗
kz−1
(11.43)
where |αk| < 1 and K is ±1. The values of K and {αk} are obtained from the prototype and the desired
ﬁlters.
Low-Pass to Low-Pass Transformation
We wish to obtain the transformation Z−1 = G(z−1) to convert a prototype low-pass ﬁlter into a
different low-pass ﬁlter. The all-pass transformation should be able to expand or contract the fre-
quency support of the prototype low-pass ﬁlter but keep its order. Thus, it should be a ratio of linear
transformations,
Z−1 = K z−1 −α
1 −αz−1
(11.44)
for some parameters K and α. Since the zero frequency in the Z-plane is to be mapped into the zero
frequency in the z-plane, we let Z = z = 1 in the transformation to get K = 1. To obtain α, we let

674
CHAPTER 11:
Introduction to the Design of Discrete Filters
Z = ejθ and z = ejω in Equation (11.44) to obtain
e−jθ = e−jω −α
1 −αe−jω
(11.45)
The value of α, in Equation (11.44) that maps the cut-off frequency θp of the prototype into the
desired cut-off frequency ωd (see Figure 11.18(a)), is found from Equation (11.45) as follows. First,
we have that
α = e−jω −e−jθ
1 −e−j(θ+ω) =
e−jω −e−jθ
2je−j0.5(θ+ω) sin((θ + ω)/2) = sin((θ −ω)/2)
sin((θ + ω)/2)
and then replacing θ and ω by θp and ωd gives
α = sin((θp −ωd)/2)
sin((θp + ωd)/2)
(11.46)
Notice that if the prototype ﬁlter coincides with the desired ﬁlter (i.e., θp = ωd), then α = 0, and the
transformation is Z−1 = z−1. For different values of α between 0 and 1 the transformation shrinks the
FIGURE 11.18
(a) Frequency transformation from a prototype
low-pass ﬁlter with cut-off frequency θp into a
low-pass ﬁlter with desired cut-off frequency ωd.
(b) Mapping of θ into ω frequencies in low-pass
to low-pass frequency transformation: the
continuous lines correspond to 0 < α ≤1, while
the dashed lines correspond to values
−1 ≤α ≤0. The arrow shows the
transformation of θp = 0.4π into ωd ≈0.95π
when α = −0.9.
∗
∗
∗
∗
∗
∗
∗
∗
∗∗
θp
−θp
Z-plane
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗∗
∗
∗
z-plane
ωd
−ωd
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
θ/π
ω /π
(b)
(a)

11.4 IIR Filter Design
675
support of the prototype low-pass ﬁlter, and conversely for −1 ≤α < 0 the transformation expands
the support of the prototype. (In Figure 11.18 the frequencies θ and ω are normalized to values
between 0 and 1—that is, both are divided by π.)
Remarks
I
The low-pass to low-pass (LP-LP) transformation then consists in:
I Given θp and ωd, ﬁnd the corresponding α value using Equation (11.46).
I Use the found α in the transformation (Eq. 11.44) with K = 1.
I
Even in the simple case of a low-pass to low-pass transformation, the relation between the frequencies θ
and ω is highly nonlinear. Indeed, solving for ejω in the transformation (Eq. 11.45), we get
e−jω =
 
e−jθ + α
1 + αe−jθ
!  
1 + αejθ
1 + αejθ
!
= e−jθ + 2α + α2ejθ
1 + 2α cos(θ) + α2
= 2α + (1 + α2) cos(θ)
1 + 2α cos(θ) + α2
|
{z
}
A
−j
(1 −α2) sin(θ)
1 + 2α cos(θ) + α2
|
{z
}
B
and since e−jω = cos(ω) −j sin(ω), comparing the two sides we ﬁnd that cos(ω) = A and sin(ω) = B
so that tan(ω) = B/A, and thus
ω = tan−1

(1 −α2) sin(θ)
2α + (1 + α2) cos(θ)

which when plotted for different values of α gives Figure 11.18(b). These curves clearly show the mapping
of a frequency θp into ωd and the value of α needed to perform the correct transformation.
Low-Pass to High-Pass Transformation
The duality between low-pass and high-pass ﬁlters indicates that this transformation, like the LP-LP,
should be linear in both numerator and denominator. Also notice that the prototype low-pass ﬁlter
can be transformed into a high-pass ﬁlter with the same bandwidth, by changing Z−1 into −Z−1.
Indeed, complex poles or zeros R1e±jθ1 of the low-pass ﬁlter are mapped into −R1e±jθ1 = R1ej(π±θ1)
corresponding to a high-pass ﬁlter. For instance, a low-pass ﬁlter
H(Z) =
Z + 1
Z −0.5
with a zero at −1 and a pole at 0.5 becomes
H1(Z) =
−Z + 1
−Z −0.5 =
Z −1
Z + 0.5
with a zero at 1 and a pole at −0.5, which makes it a high-pass ﬁlter.
The low-pass to high-pass (LP-HP) transformation is then
Z−1 = −z−1 −α
1 −αz−1

676
CHAPTER 11:
Introduction to the Design of Discrete Filters
and to obtain α we replace θp by π −θp in Equation (11.46) to get:
α = sin(−(θp + ωd)/2 + π/2)
sin((θp −ωd)/2 + π/2)
= −sin((θp + ωd)/2 −π/2)
sin((θp −ωd)/2 + π/2)
= cos((θp + ωd)/2)
cos((θp −ωd)/2)
(11.47)
As before, θp is the cut-off frequency of the prototype low-pass ﬁlter and ωd is the desired cut-off
frequency of the high-pass ﬁlter.
When the low-pass and the high-pass ﬁlters have the same bandwidth, ωd = π −θp, we have that
θp + ωd = π and so α = 0 giving as a transformation Z−1 = −z−1, which transforms the low-pass
prototype into a high-pass ﬁlter, both of the same bandwidth.
Low-Pass to Band-Pass and Band-Stop Transformations
By being linear in both the numerator and the denominator, the LP-LP and LP-HP transformations
preserve the number of poles and zeros of the prototype ﬁlter. To transform a low-pass ﬁlter into a
band-pass or into a band-stop ﬁlter, the number of poles and zeros must be doubled. For instance, if
the prototype is a ﬁrst-order low-pass ﬁlter (with real-valued poles and zeros) we need a quadratic,
rather than a linear, transformation in both numerator and denominator to obtain band-pass or
band-stop ﬁlters from the low-pass ﬁlter since band-pass or band-stop ﬁlters cannot be ﬁrst-order
ﬁlters.
The low-pass to band-pass (LP-BP) transformation is
Z−1 = −z−2 −bz−1 + c
cz−2 −bz−1 + 1
(11.48)
while the low-pass to band-stop (LP-BS) transformation is
Z−1 =
z−2 −(b/k)z−1 −c
−cz−2 −(b/k)z−1 + 1
(11.49)
where
b = 2αk/(k + 1)
c = (k −1)/(k + 1)
and
α = cos((ωd2 + ωd1)/2)
cos((ωd2 −ωd1)/2)
k = cot((ωd2 −ωd1)/2) tan(θp/2)
The frequencies ωd1 and ωd2 are the desired lower and higher cut-off frequencies in the band-pass
and band-stop ﬁlters.

11.4 IIR Filter Design
677
11.4.5 General IIR Filter Design with MATLAB
The following function buttercheby1 can be used to design low-pass, high-pass, band-pass, and stop-
band Butterworth as well as Chebyshev ﬁlters. One important thing to remember when designing
band-pass and band-stop ﬁlters is that the order of the low-pass prototype is half that of the desired
ﬁlter.
function [b,a,H,w] = buttercheby1(lp order,wn,BC,type)
%
% Design of frequency discriminating ﬁlters
% using Butterworth and Chebyshev methods, the bilinear transformation and
% frequency transformations
%
% lp order : order of low-pass ﬁlter prototype
% wn : vector containing the cut-off normalized frequency(ies)
% (entries must be normalized)
% BC: Butterworth (0) or Chebyshev1 (1)
% type : type of ﬁlter desired
%
1 = low-pass
%
2 = high-pass
%
3 = band-pass
%
4 = stopband
%
[b,a] : numerator, denominator coefﬁcients of designed ﬁlter
%
[H,w] : frequency response, frequency range
% USE:
% [b,a,H,w] = buttercheby1(lp order,wn,BC,type)
if BC == 0; % Butterworth ﬁlter
if type == 1
[b,a] = butter(lp order,wn);
% lowpas
elseif type == 2
[b,a] = butter(lp order,wn,‘high’); % high-pass
elseif type == 3
[b,a] = butter(lp order,wn);
% band-pass
else
[b,a] = butter(lp order,wn,‘stop’); % stopband
end
[H,w] = freqz(b,a,256);
else % Chebyshev1 ﬁlter
R = 0.01;
if type == 1,
[b,a] = cheby1(lp order,R,wn);
% lowpas
elseif type == 2,
[b,a] = cheby1(lp order,R,wn,‘high’); % high-pass
elseif type == 3,
[b,a] = cheby1(lp order,R,wn);
% band-pass
else

678
CHAPTER 11:
Introduction to the Design of Discrete Filters
[b,a] = cheby1(lp order,R,wn,‘stop’); % stopband
end
[H,w] = freqz(b,a,256);
end
To illustrate the design of ﬁlters other than low-pass ﬁlters, consider the design of a Butterworth and
Chebyshev band-pass and band-stop ﬁlter of order N = 30 and half-power frequencies [0.4π, 0.6π].
The following script is the design, which is shown in Figure 11.19.
FIGURE 11.19
(a) Band-Stop Butterworth and (b)
Chebyshev ﬁlters: (clockwise for each
side from top left) poles and zeros,
magnitude, phase frequency responses,
and loss.
(b)
(a)
−1
0
1
−1
−0.5
0
0.5
1
Real part
Imaginary part
0
0.2
0.4
0.6
0.8
0
50
100
150
200
Loss (dB)
ω /π
Real part
Imaginary part
Loss (dB)
ω /π
−1
−0.5
0
0.5
1
600
400
200
0
−1
0
1
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
1
Magnitude
ω/π
0
0.2
0.4
0.6
0.8
−30
−20
−10
0
Phase (rad)
ω/π
Magnitude
ω/π
Phase(rad)
ω/π
0
0.5
1
−30
−40
−50
−20
−10
0
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8

11.5 FIR Filter Design
679
%%%%%%%%%%%%%%%
% band-stop Butterworth
%%%%%%%%%%%%%%%
ﬁgure(1)
[b1,a1] = buttercheby1(15,[0.4 0.6],0,4)
%%%%%%%%%%%%%%%
% band-stop Chebyshev
%%%%%%%%%%%%%%%
ﬁgure(2)
[b2,a2] = buttercheby1(15,[0.4 0.6],1,4)
There are other ﬁlters that can be designed with MATLAB, following a procedure similar to the previ-
ous cases. For instance, to design a band-pass elliptic ﬁlter with cut-off frequencies [0.45π, 0.55π] of
order 20 and with loss speciﬁcations of 0.1 and 40 dB in the passband and the stopband, we use the
command shown below. Likewise, to design a high-pass ﬁlter using the cheby2 function we specify
the order 10, the loss in the stopband, and the cut-off frequency 0.55π and indicate it is a high-pass
ﬁlter. The results are shown in Figure 11.20.
%%%%%%%%%%%%%%%
% Elliptic and Cheby2
%%%%%%%%%%%%%%%
[b1,a1] = ellip(10,0.1,40,[0.45 0.55]);
[b2,a2] = cheby2(10,40, 0.55,‘high’);
11.5 FIR FILTER DESIGN
The design of FIR ﬁlters is typically discrete. The speciﬁcation of FIR ﬁlters is usually given in the time
domain rather than in the frequency domain. FIR ﬁlters have three deﬁnite advantages: (1) stability,
(2) possible linear phase, and (3) efﬁcient implementation. Indeed, the poles of an FIR ﬁlter are
at the origin of the z-plane; thus FIR ﬁlters are stable. An FIR ﬁlter can be designed to have linear
phase, and since the input–output equation of an FIR ﬁlter is equivalent to a convolution sum, FIR
ﬁlters are implemented using the Fast Fourier Transform (FFT). A minor disadvantage is the storage
required—typically FIR ﬁlters have a large number of coefﬁcients.
I Example 11.11
A moving-average ﬁlter has an impulse response
h[n] = 1
M
0 ≤n ≤M −1
and zero otherwise. The transfer function of this ﬁlter is
H(z) =
M−1
X
n=0
1
Mz−n = 1
M
zM −1
zM−1(z −1)

680
CHAPTER 11:
Introduction to the Design of Discrete Filters
FIGURE 11.20
(a) Elliptic band-pass ﬁlter and (b)
high-pass ﬁlter using cheby2: (clockwise
for each side from top left) poles and
zeros, magnitude, phase frequency
responses, and loss.
−1
−0.5
0
0.5
1
Imaginary part
−1
0
1
Real part
Loss (dB)
ω/π
0
0.5
1
0
40
20
60
80
100
−1
−0.5
0
0.5
1
−1
0
1
0
0.5
1
0
40
20
60
80
100
Real part
Imaginary part
Loss (dB)
ω/π
(b)
(a)
Magnitude
0
0.2
0.4
0.6
0.8
ω/π
0
0.2
0.4
0.6
0.8
1
Phase (rad)
ω/π
0
0.2
0.4
0.6
0.8
0
5
−5
0
0.2
0.4
0.6
0.8
0
0.2
0.4
0.6
0.8
0
4
2
6
Magnitude
ω/π
Phase (rad)
ω/π
0
0.2
0.4
0.6
0.8
1
Consider the stability of this ﬁlter, and determine if the phase of this ﬁlter is linear and what type
of ﬁlter it is.
Solution
The impulse response h[n] is absolutely summable given its ﬁnite length M; thus the ﬁlter is BIBO
stable. Indeed, the apparent pole at z = 1, which would make the ﬁlter unstable, is canceled by a
zero also at z = 1 (notice that H(1) is 0/0, according to the ﬁnal expression above, indicating that
a pole and a zero at z = 1 exist, but also from the sum H(1) = 1, so there are no poles at z = 1).

11.5 FIR Filter Design
681
The remaining M −1 poles of this ﬁlter are at the origin of the z-plane, making the ﬁlter stable. The
zeros of H(z) are complex numbers that make the numerator zero (i.e., zM −1 = 0, or zk = ej2π/M,
for k = 0, . . . , M −1). When k = 0 the zero is 1, which cancels the pole at 1, so that
H(z) = (z −1) QM−1
k=1 (z −ej2π/M)
MzM−1(z −1)
=
QM−1
k=1 (z −ej2π/M)
MzM−1
To convince yourself of the pole–zero cancellation let M = 3, for which
H(z) = 1
3
z3 −1
z2(z −1) = 1
3
(z2 + z + 1)(z −1)
z2(z −1)
= z2 + z + 1
3z2
showing the pole–zero cancellation.
Since the zeros of the ﬁlter are on the unit circle, the phase of this ﬁlter is not linear. Although the
ﬁlter is considered a low-pass ﬁlter, it is of very poor quality in terms of its magnitude response.
I
11.5.1 Window Design Method
The usual ﬁlter speciﬁcations of magnitude and linear phase can be translated into a time-domain
speciﬁcation (i.e., a desired impulse response) by means of the discrete-time Fourier transform. In
this section, we will show how to design FIR ﬁlters using this speciﬁcation with the window method.
You will see that this is a trial-and-error method, as there is no measure of how close the designed
ﬁlter is to the desired response, and that using different windows we obtain different results.
Let Hd(ejω) be the desired frequency response of an ideal discrete low-pass ﬁlter. Assume that the
phase of Hd(ejω) is zero. The desired impulse response is given by the inverse discrete-time Fourier
transform
hd[n] = 1
2π
π
Z
−π
Hd(ejω)ejωn dω
(11.50)
which is noncausal and of inﬁnite length. The ﬁlter
Hd(z) =
∞
X
n=−∞
hd[n]z−n
(11.51)
is thus not an FIR ﬁlter. To obtain an FIR ﬁlter that approximates Hd(ejω) we need to window the
impulse response hd[n] to get a ﬁnite length, and then delay the resulting windowed impulse response
to achieve causality.
For an odd integer N, deﬁne
hw[n] = hd[n]w[n] =
hd[n]
−(N −1)/2 ≤n ≤(N −1)/2
0
elsewhere
where w[n] is a rectangular window,
w[n] =
1
−(N −1)/2 ≤n ≤(N −1)/2
0
otherwise
(11.52)

682
CHAPTER 11:
Introduction to the Design of Discrete Filters
that causes the truncation of hd[n]. The windowed impulse response hw[n] has a discrete-time Fourier
transform of
Hw(ejω) =
(N−1)/2
X
n=−(N−1)/2
hw[n]e−jωn
For a large value of N we have that Hw(ejω) must be a good approximation of Hd(ejω)—that is,
|Hw(ejω)| ≈|Hd(ejω)|
∠Hw(ejω) = ∠Hd(ejω) = 0
It is not clear how the value of N should be chosen—this is what we meant by this design is a
trial-and-error method.
To make Hw(z) a causal ﬁlter, we shift to the right the impulse response hw[n] by (N −1)/2 (assume
N is chosen to be an odd number so that this division is an integer) samples to obtain
ˆH(z) = Hw(z)z−(N−1)/2 =
(N−1)/2
X
m=−(N−1)/2
hw[m]z−(m+(N−1)/2)
=
N−1
X
n=0
hd[n −(N −1)/2)]w[n −(N −1)/2)]z−n
after letting n = m + (N −1)/2. For a large value of N, we have
| ˆH(ejω)| = |Hw(ejω)e−jω(N−1)/2| = |Hw(ejω)| ≈|Hd(ejω)|
∠ˆH(ejω) = ∠Hw(ejω) −N −1
2
ω = −N −1
2
ω
(11.53)
since ∠Hw(ejω) = ∠Hd(ejω) = 0. That is, the magnitude response of the FIR ﬁlter ˆH(z) is approxi-
mately (depending on the value of N) the desired response and its phase response is linear. These
results can be generalized as follows.
I
If the desired low-pass frequency response has a magnitude
|Hd(ejω)| =
(
1
−ωc ≤ω ≤ωc
0
otherwise
(11.54)
and a linear phase
θ(ω) = −ω(N −1)/2
the corresponding impulse response is given by
hd[n] =
(
sin(ωc(n −α))/(π(n −α))
n ̸= α
ωc/π
n = α
(11.55)

11.5 FIR Filter Design
683
where α = (N −1)/2. Using a window w[n] of length N and centered at (N −1)/2, the windowed
impulse response is h[n] = hd[n]w[n], and the designed FIR ﬁlter is
H(z) =
N−1
X
n=0
h[n]z−n
I
The design using windows is a trial-and-error procedure. Different trade-offs can be obtained by using
various windows and various lengths of the windows.
I
The symmetry of the impulse response h[n] with respect to (N −1)/2, independent of whether this is an
integer or not, guarantees the linear phase of the ﬁlter.
11.5.2 Window Functions
In the previous section, the windowed impulse response hw[n] was written as
hw[n] = hd[n]w[n]
where
w[n] =
1
−(N −1)/2 ≤n ≤(N −1)/2
0
otherwise
(11.56)
is a rectangular window of length N. If we wish Hw(ejω) = Hd(ejω), we would need a rectangular win-
dow of inﬁnite length so that the impulse responses hw[n] = hd[n] (i.e., no windowing). This ideal
rectangular window has a discrete-time Fourier transform
W(ejω) = 2πδ(ω)
−π ≤ω < π
(11.57)
Since hw[n] = w[n]hd[n], then Hw(ejω) is the convolution of Hd(ejω) and W(ejω) in the frequency
domain—that is,
Hw(ejω) = 1
2π
π
Z
−π
Hd(ejθ)W(ej(ω−θ))dθ
=
π
Z
−π
Hd(ejθ)δ(ω −θ)dθ = Hd(ejω)
Thus, for N →∞, the result of this convolution is Hd(ejω), but if N is ﬁnite the convolution in the
frequency domain would give a distorted version of Hd(ejω). Thus, to obtain a good approximation
of Hd(ejω) using a ﬁnite window w[n] the window must have a spectrum approximating that of the
ideal rectangular window. That is, an impulse in frequency in −π ≤ω < π as in Equation (11.57)
with most of its energy concentrated in the low frequencies. The smoothness of the window makes
this possible.

684
CHAPTER 11:
Introduction to the Design of Discrete Filters
Examples of windows that are smoother than the rectangular window are:
Triangular or Barlett window: w[n] =
(
1 −2|n|
N−1
−(N −1)/2 ≤n ≤(N −1)/2
0
otherwise
(11.58)
Hamming window: w[n] =
(
0.54 + 0.46 cos(2πn/(N −1))
−(N −1)/2 ≤n ≤(N −1)/2
0
otherwise
(11.59)
Kaiser window: This window has a parameter β that can be adjusted. It is given by
w[n] =



I0

β√
1−(n/(N−1))2

I0(β)
−(N −1)/2 ≤n ≤(N −1)/2
0
otherwise
(11.60)
where I0(x) is the zero-order Bessel function of the ﬁrst kind, which can be computed by the series
I0(x) = 1 +
∞
X
k=1
 
(0.5x)k
k!
!2
(11.61)
When β = 0 the Kaiser window coincides with a rectangular window, since I0(0) = 1. As β increases
the window becomes smoother.
The above deﬁnitions are for windows symmetric with respect to the origin. Figures 11.21 and 11.22
show the causal rectangular, Barlett, Hamming, and Kaiser windows, and their magnitude spectra.
FIGURE 11.21
(a) Rectangular and (b) Barlett
causal windows and (c) their
spectra.
0
10
20
30
0
0.5
1
n
w1[n]
0
10
20
30
0
0.5
1
n
w2[n]
(b)
(a)
0.2
0.4
0.6
0.8
1
−100
−80
−60
−40
−20
0
ω /π
Gain(dB)
Rectangular
Barlett
0
(c)

11.5 FIR Filter Design
685
FIGURE 11.22
(a) Hamming and (b) Kaiser
causal windows and (c) their
spectra.
0
10
20
30
0
0.5
1
n
w3[n]
0
10
20
30
0
0.5
1
n
w4[n]
(b)
(a)
0
0.2
0.4
0.6
0.8
1
−150
−100
−50
0
ω/π
Gain(dB)
(c)
Hamming
Kaiser
Given that the sidelobes for the Kaiser window have the largest loss, the Kaiser window is considered
the best of these four, followed by the Hamming, the Bartlett, and the rectangular windows. Notice
that the width of the ﬁrst lobe is the widest for the Kaiser and the narrowest for the rectangular, as
this width depends on the smoothness of the window.
Remarks The linear phase is a result of the symmetry of the impulse response of the designed ﬁlter. It can be
shown that if the impulse response h[n] of the FIR ﬁlter is even or odd symmetric with respect to the sample
(N −1)/2 (whether this is a integer or not) the ﬁlter has a linear phase.
I Example 11.12
Design a low-pass FIR ﬁlter with N = 21 to be used in ﬁltering analog signals and that approxi-
mates the following ideal frequency response:
Hd(ejω) =
1
0 ≤f ≤125 Hz
0
elsewhere in 0 ≤f ≤fs/2
where ω = 2πf/fs and fs = 1000 Hz is the sampling rate. Use ﬁrst a rectangular window, and then
a Hamming window. Compare the designed ﬁlters.
Solution
The discrete frequency response is given by
Hd(ejω) =
1
0 ≤ω ≤π/4 rad
0
elsewhere in 0 ≤ω ≤π

686
CHAPTER 11:
Introduction to the Design of Discrete Filters
The desired impulse response is thus
hd[n] = 1
2π
π
Z
−π
Hd(ejω)ejωndω = 1
2π
π/4
Z
−π/4
ejωndω
=
sin(πn/4)/(πn)
n ̸= 0
0.25
n = 0
Using a rectangular window, the FIR ﬁlter is then of the form (the delay is (N −1)/2 = 10)
ˆH(z) = Hw(z)z−10 =
20
X
n=0
hd[n −10]z−n
= 0.25z−10 +
20
X
n=0,n̸=10
sin(π(n −10)/4)
π(n −10)
z−n
The magnitude and the phase of this ﬁlter are shown in Figure 11.23 when we use a rectangular
and a Hamming window.
The magnitude and the phase responses of the ﬁlter designed using the Hamming window is much
improved over the one obtained using the rectangular window. Notice that the second lobe in the
stopband for the Hamming window design is at about −50 dB while for the rectangular window
design it is at about −20 dB, a signiﬁcant difference. In both cases, the phase response is linear
in the passband of the ﬁlter, corresponding to the impulse response h[n] being symmetric with
respect to the (N −1)/2 = 10 sample.
I
I Example 11.13
Design a high-pass ﬁlter of order 14 and a cut-off frequency of 0.2π using the Kaiser window. Use
MATLAB.
Solution
Let hlp[n] be the impulse response of an ideal low-pass ﬁlter:
Hlp(ejω) =
1
−ωc ≤ω ≤ωc
0
otherwise in [−π, π)
According to the modulation property of the DTFT, we have that
2hlp[n] cos(ω0n)
⇔
Hlp(ej(ω+ω0)) + Hlp(ej(ω−ω0))
If we let ω0 = π, then the right term gives a high-pass ﬁlter, and so hhp[n] = 2hlp[n] cos(πn) =
2(−1)nhlp[n] is the desired impulse response of the high-pass ﬁlter. The following script shows
how to use the function ﬁr to design the high-pass ﬁlter.

11.5 FIR Filter Design
687
FIGURE 11.23
Low-pass FIR ﬁlters using (a)
rectangular and (b) Hamming
windows.
0
5
10
15
20
−0.1
0
0.1
0.2
0.3
h[n]
n
ω/π
0
0.5
1
−100
−80
−60
−40
−20
0
20log10 |H(e jω)|(dB)
−100
−80
−60
−40
−20
0
20log10|H(e jω)| (dB)
0
0.5
1
ω/π
0
0.5
1
0
0.2
0.4
0.6
0.8
1
ω/π
|H(ejω)|
ω/π
0
0.5
1
−8
−6
−4
−2
0
<H(ejω)
−10
−5
0
<H(ejω)
0
0.5
1
0
0.2
0.4
0.6
0.8
1
ω/π
0
0.5
1
ω/π
|H(ejω)|
0
5
10
15
20
0
0.1
0.2
0.3
h [n]
n
(b)
(a)
%%%%%%%%%%%%%%%%
% Example 11.13---FIR ﬁlter from ‘ﬁr’
%%%%%%%%%%%%%%%%
M = 14;wc = 0.2;wo = 1;wind = 4;
[b] = ﬁr(M,wc,wo,wind);
[H,w] = freqz(b,1,256);
The results are shown in Figure 11.24. Notice the symmetry of the impulse response with respect
to M/2 = 7 gives a linear phase in the passband of the high-pass ﬁlter. The second lobe of the gain
in dB is about −50 dB.

688
CHAPTER 11:
Introduction to the Design of Discrete Filters
FIGURE 11.24
High-pass FIR ﬁlter design
using Kaiser window.
0
5
10
−0.1
0
0.1
0.2
h[n]
n
−80
−100
−60
−40
−20
0
20log10|H(e jω)|(dB)
0
0.5
1
ω/π
0
0.5
1
0
0.2
0.4
0.6
0.8
1
ω/π
|H(e jω)|
−2
0
2
4
6
<H(e jω)
0
0.5
1
ω/π
The function ﬁr can be used to design low-pass, high-pass, and band-pass FIR ﬁlters using differ-
ent types of windows. When designing high-pass and band-pass FIRs, ﬁr ﬁrst designs a prototype
low-pass ﬁlter and then uses the modulation property to shift it in frequency to a desired center
frequency.
function [b] = ﬁr(N,wc,wo,wind)
%
% FIR ﬁlter design using window method and
% frequency modulation
%
% N : order of the FIR ﬁlter
% wc : normalized cutoff frequency (between 0 and 1)
% of low-pass prototype
% wo : normalized center frequency (between 0 and 1)
% of high-pass, bandpass ﬁlters
% wind : type of window function
%
1 : rectangular
%
2 : hanning
%
3 : hamming
%
4 : kaiser
% [b] : coefﬁcients of designed ﬁlter
%
% USE:
% [b] = ﬁr(N,wc,wo,wind)
%

11.6 Realization of Discrete Filters
689
n = 0:N;
if wind == 1
window = boxcar(N + 1);
disp(‘ ***** RECTANGULAR WINDOW *****’)
elseif wind == 2
window = hanning(N + 1);
disp(‘ *****HANNING WINDOW *****’)
elseif wind == 3
window = hamming(N + 1);
disp(‘ *****
HAMMING WINDOW *****’)
else
window = kaiser(N + 1,4.55);
disp(‘ *****
KAISER WINDOW *****’)
end
% calculation of ideal impulse response
den = pi ∗(n - N/2);
num = sin(wc*den);
% if N even, this prevents 0/0
if ﬁx(N/2) == N/2,
num(N/2 + 1) = wc;
den(N/2 + 1) = 1;
end
b = (num./den). ∗window’;
% frequency shifting
[H,w] = freqz(b,1,256); %% low-pass
if wo > 0 & wo < 1,
b = 2 ∗b.*cos(wo ∗pi ∗(n - N/2))/H(1);
elseif wo == 0,
b = b/abs(H(1));
elseif wo == 1;
b = b. ∗cos(wo ∗pi ∗(n - N/2));
end
MATLAB provides the function ﬁr1 to design FIR ﬁlters with the window method. As expected, the
results are identical for either ﬁr1 and ﬁr. The reason for writing ﬁr is to simplify the code and to
show how the modulation property can be used in the design of ﬁlters different from low-pass
ﬁlters.
I
11.6 REALIZATION OF DISCRETE FILTERS
The realization of a discrete ﬁlter can be done in hardware or in software. In either case, the imple-
mentation of the transfer function H(z) of a discrete ﬁlter requires delays, adders, and constant
multipliers as actual hardware or as symbolic components. Figure 11.25 depicts the operation of
each of these components as block diagrams.

690
CHAPTER 11:
Introduction to the Design of Discrete Filters
FIGURE 11.25
Block diagrams of different components
used to realize discrete ﬁlters: (a) delay,
(b) constant multiplier, and (c) adder.
z −1
x [n]
x[n −1]
(a)
y [n]
x [n]+ y[n]
x[n]
+
(c)
α x [n]
x[n]
(b)
α
In choosing a structure over another to realize a ﬁlter, two factors to consider are:
I
Computational complexity, which relates to the number of operations (mainly multiplications
and additions), but more importantly to the number of delays used. The aim is to reduce to a
minimum the number of delays in the structure.
I
Quantization effects or the representation of ﬁlter parameters using ﬁnite-length registers. The aim
is to minimize quantization effects on parameters and on operations.
We will consider here the computational complexity of the structures seeking to obtain mini-
mal realizations—that is, to optimize the number of delays used. The quantization effects are not
considered.
11.6.1 Realization of IIR Filters
The structures commonly used to realize IIR ﬁlters are:
I
Direct forms I and II
I
Cascade
I
Parallel
The direct forms represent the difference equation resulting from the transfer function of the IIR ﬁlter
while attempting to minimize the number of delays. The cascade and parallel structures are based on
the product or sum of ﬁrst- and second-order ﬁlters to express the ﬁlter transfer function, which are
in turn implemented using a direct form.
Direct Form Realizations
Given the transfer function of an IIR ﬁlter
H(z) = Y(z)
X(z) =
PM−1
k=0 bkz−k
1 + PN−1
k=1 akz−k
(11.62)
where Y(z) and X(z) are the Z-transforms of the output y[n] and the input x[n], the input–output
relationship is given by the difference equation
y[n] = −
N−1
X
k=1
aky[n −k] +
M−1
X
k=0
bkx[n −k]
(11.63)
The direct forms attempt to realize this equation with no more than N −1 delays.

11.6 Realization of Discrete Filters
691
Direct Form I
The direct form I is the implementation of the above difference Equation (11.63) as is, by means of
delays, constant multipliers, and adders. Assuming the input x[n] is available, then M −1 delays are
needed to generate the delayed inputs {x[n −k]} for k = 1, . . . , M −1. Likewise, the output compo-
nents require additional N −1 delays. Thus, a direct form I realization requires M + N −2 delays for
an (N −1)th-order difference equation. In terms of number of delays, the direct form I is the least
efﬁcient realization.
I Example 11.14
Use the direct form I to realize the transfer function
H(z) = 1 + 1.5z−1
1 + 0.1z−1
of a discrete ﬁlter.
Solution
The transfer function corresponds to a system with a ﬁrst-order difference equation
y[n] = x[n] + 1.5x[n −1] −0.1y[n −1]
so M = N = 2 and this equation can be realized as shown in Figure 11.26 with M + N −2 = 2
delays.
The difference equation, and thus the transfer function, for this ﬁlter can be easily obtained from
the realization. The above realization is nonminimal since it uses two delays to represent a ﬁrst-
order system.
The output of the above realization is seen to be
y[n] = −0.1y[n −1] + x[n] + 1.5x[n −1]
so that the transfer function is
H(z) = Y(z)
X(z) = 1 + 1.5z−1
1 + 0.1z−1
by letting a delay be represented by z−1 in the z-domain.
I
FIGURE 11.26
Direct form I realization of
H(z) = (1 + 1.5z−1)/(1 + 0.1z−1).
+
+
−
x [n]
y[n]
1.5
0.1
z−1
z−1

692
CHAPTER 11:
Introduction to the Design of Discrete Filters
Remarks
I
In general, given a direct form I realization one can easily obtain the difference equation and consequently
the transfer function of the ﬁlter from it.
I
The minimal realization of qth-order discrete ﬁlter must use q delays. The direct form I is only capable
of providing these minimal realizations for all-pole ﬁlters (i.e., when the numerator in Equation (11.62)
is a constant), otherwise we need to use the direct form II to to obtain minimal realizations. If the transfer
function has only poles, then it is possible to obtain a minimal realization with direct form I. Indeed, if
H(z) = Y(z)
X(z) =
b0
1 + PN−1
k=1 akz−k
(11.64)
where Y(z) and X(z) are the Z-transforms of the output y[n] and the input x[n], the input–output
relationship is given by the difference equation
y[n] = −
N−1
X
k=1
aky[n −k] + b0x[n]
(11.65)
which only requires N −1 delays for the output, and none for the input. This is a minimal realization of
H(z) as only N −1 delays are needed.
Direct Form II
If the polynomials
B(z) =
M−1
X
k=0
bkz−k and A(z) =
N−1
X
k=0
akz−k
M ≤N
represent the numerator and denominator of the transfer function H(z) of the ﬁlter we wish to realize,
we have
H(z) = Y(z)
X(z) = B(z)
A(z)
where X(z) and Y(z) correspond to the Z-transforms of the input and of the output of the ﬁlter. We
then have that
Y(z) = H(z)X(z) = B(z)
X(z)
A(z)

(11.66)
Deﬁning an output w[n] with W(z) = X(z)/A(z), corresponding to the second term in the last
equation, we obtain an all-pole ﬁlter with transfer function
W(z)
X(z) =
1
A(z)
(11.67)
The output y[n] is then obtained as the inverse of
Y(z) = B(z)W(z)
(11.68)

11.6 Realization of Discrete Filters
693
By realizing the all-pole ﬁlter given in Equation (11.67), and using its output w[n] in the realization
of Equation (11.68), we minimize the number of delays used. The realization of Equation (11.68)
does not require new delays, as the delayed w[n]’s are already available from the realization of the
all-pole ﬁlter. Thus, the number of delays used corresponds to the order of the denominator A(z),
which is the order of the ﬁlter.
I Example 11.15
Consider the same transfer function as in Example 11.14 to obtain a direct form II realization of it.
Solution
We have that
W(z) =
X(z)
1 + 0.1z−1
gives
w[n] = x[n] −0.1w[n −1]
Now, according to Equation (11.68),
Y(z) = (1 + 1.5z−1)W(z)
which corresponds to the difference equation
y[n] = w[n] + 1.5w[n −1]
(11.69)
Notice that in Equation (11.69) w[n] and w[n −1] are already available, and thus there is no
need for new delays in this step. The direct form II realization using only one delay is shown
in Figure 11.27.
Obtaining the transfer function from the direct form II realization is not as obvious as from the
direct form I. In this case, we need to obtain the transfer function corresponding to the all-pole
ﬁlter ﬁrst and use it to obtain the overall transfer function. From the above realization, we have
w[n] = x[n] −0.1w[n −1]
y[n] = w[n] + 1.5w[n −1]
FIGURE 11.27
Direct form II realization of
H(z) = (1 + 1.5z−1)/(1 + 0.1z−1). This is a
minimal realization of H(z), which is a ﬁrst-order
system, as only one delay is used.
0.1
1.5
x[n]
y[n]
w [n]
z −1
+
−
+
+

694
CHAPTER 11:
Introduction to the Design of Discrete Filters
If we replace the ﬁrst equation into the second we obtain an expression containing w[n]
and w[n −2] and x[n] so that we cannot express y[n] directly in terms of the input. Instead, consider
the Z-transforms of the above equation,
(1 + 0.1z−1)W(z) = X(z)
Y(z) = (1 + 1.5z−1)W(z)
(11.70)
Thus, we obtain from the top equation in Equation (11.70):
W(z) =
X(z)
1 + 0.1z−1
which when replaced in the bottom equation in Equation (11.70) gives
Y(z) = X(z)(1 + 1.5z−1)
1 + 0.1z−1
giving the transfer function H(z).
I
Remarks
I
Direct form II is more advantageous than direct form I because of the consequence of using fewer delays.
We will use direct form II to realize ﬁrst- and second-order modules in the cascade and parallel realizations.
I
The cascade and the parallel realizations will connect ﬁrst- and second-order systems to realize a given
transfer function. General direct form II realizations for a ﬁrst and second-order ﬁlter with respective
transfer functions
H1(z) = b0 + b1z−1
1 + a1z−1
(11.71)
H2(z) = b0 + b1z−1 + b2z−2
1 + a1z−1 + a2z−2
(11.72)
are given in Figure 11.28. The coefﬁcients of the above transfer functions are real. The realization of H1(z)
is obtained by getting rid of the lower part of the realization (i.e., getting rid of the constant multipliers
FIGURE 11.28
Direct form II realization of ﬁrst- and
second-order ﬁlters (for the ﬁrst-order ﬁlters
let a2 = b2 = 0 thus eliminating the constant
multipliers and the lower delay).
x [n]
y [n]
w [n]
z−1
−
+
+
z−1
−
b0
b1
b2
a2
a1

11.6 Realization of Discrete Filters
695
because a2 = b2 −0 and the lower delay because it is not needed once these constant multiplier are
deleted).
I
Obtaining the transfer function from a direct form II realization is not as obvious as it is from a direct
form I realization. For a given realization direct form II, we need to use the auxiliary variable w[n] to
obtain the transfer function from the realization. Instead of using the time-domain equations one should
use the Z-transform equations to obtain the overall transfer function. For instance, consider the direct form
II realization in Figure 11.28 for the second-order system. From the realization we obtain
w[n] = x[n] −a1w[n −1] −a2w[n −2] ⇒(1 + a1z−1 + a2z−2)W(z) = X(z)
y[n] = b0w[n] + b1w[n −1] + b2w[n −2] ⇒Y(z) = (b0 + b1z−1 + b2z−2)W(z)
Solving for W(z) in the top equation and replacing it in the bottom equation will give H2(z).
Cascade Realization
The cascade realization is obtained by representing the given transfer function H(z) = B(z)/A(z) as a
product of ﬁrst- and second-order ﬁlters Hi(z) with real coefﬁcients:
H(z) =
Y
i
Hi(z)
(11.73)
Each transfer function Hi(z) is realized by direct form II and cascaded. Different from the analog case,
this cascade realization is not constrained by loading.
I Example 11.16
Obtain a cascade realization of the ﬁlter with transfer function
H(z) = 3 + 3.6z−1 + 0.6z−2
1 + 0.1z−1 −0.2z−2
Solution
The poles of H(z) are z = −0.5 and z = 0.4, and the zeros are z = −1 and z = −0.2, all of which
are real. One way of obtaining the cascade realization is to express H(z) as
H(z) =
 3(1 + z−1)
1 + 0.5z−1
 1 + 0.2z−1
1 −0.4z−1

If we let
H1(z) = 3(1 + z−1)
1 + 0.5z−1
H2(z) = 1 + 0.2z−1
1 −0.4z−1
Realizing H1(z) and H2(z) separately and then cascading them we obtain the realization for H(z)
shown in Figure 11.29.

696
CHAPTER 11:
Introduction to the Design of Discrete Filters
z−1
−
+
+
1
−0.4
0.2
v [n]
x[n]
y[n]
z−1
−
+
+
+
+
3
0.5
3
w[n]
FIGURE 11.29
Cascade realization of H(z) = (3 + 3.6z−1 + 0.6z−2)/(1 + 0.1z−1 −0.2z−2).
It is also possible to express H(z) as
H(z) =
1 + 0.2z−1
1 + 0.5z−1

|
{z
}
ˆH1(z)
 3(1 + z−1)
1 −0.4z−1

|
{z
}
ˆH2(z)
which would give a different but equivalent realization of H(z).
Since loading is not applicable, the product of the transfer functions always gives the overall trans-
fer function. As LTI systems these realizations can be cascaded in different orders with the same
result.
I
I Example 11.17
Obtain a cascade realization of
H(z) =
1 + 1.2z−1 + 0.2z−2
1 −0.4z−1 + z−2 −0.4z−3
Solution
The zeros of H(z) are z = −1 and z = −0.2, while its poles are z = ±j and z = 0.4. We can thus
rewrite H(z) as the following two equivalent expressions:
H(z) =
1 + z−1
1 + z−2
 1 + 0.2z−1
1 −0.4z−1

=
1 + 0.2z−1
1 + z−2
 
1 + z−1
1 −0.4z−1

where the complex-conjugate poles give the denominator of the ﬁrst ﬁlter. Realizing each of these
components and cascading in any order would give different but equivalent representation of H(z).
Figure 11.30 shows the realization of the top form of H(z).
I

11.6 Realization of Discrete Filters
697
x[n]
w[n]
z−1
−
+
+
z−1
y [n]
z−1
+
−
+
1
1
1
−0.4
1
0.2
v [n]
y1[n]
FIGURE 11.30
Cascade realization of H(z) = [(1 + z−1)/(1 + z−2)] [(1 + 0.2z−1)/(1 −0.4z−1)].
Parallel Realization
In this case the given transfer function H(z) is represented as a partial fraction expansion,
H(z) = B(z)
A(z) = C +
r
X
i=1
Hi(z)
(11.74)
where C is a constant and the r ﬁlters Hi(z) are ﬁrst- or second-order systems with real coefﬁcients
that are implemented with the direct form II.
The constant C in the expansion is needed when the numerator (in positive powers of z) is of larger
or equal order than the denominator. If the numerator is of larger order than the denominator, the
ﬁlter is noncausal. To illustrate this, consider a ﬁrst-order ﬁlter with a transfer function where the
numerator is of second order (in terms of positive powers of z)
H(z) = Y(z)
X(z) = b0z2 + b1z + b2
z + a1
= b0z + b1 + b2z−1
1 + a1z−1
The difference equation representing this system is
y[n] = −a1y[n −1] + b0x[n + 1] + b1x[n] + b2x[n −1]
requiring a future input x[n + 1] to compute the present y[n] (i.e., corresponding to a noncausal
ﬁlter).
The cascade and parallel realizations are shown in Figure 11.31.
I Example 11.18
Let
H(z) = 3 + 3.6z−1 + 0.6z−2
1 + 0.1z−1 −0.2z−2 = 3z2 + 3.6z + 0.6
z2 + 0.1z −0.2
Obtain a parallel realization.

698
CHAPTER 11:
Introduction to the Design of Discrete Filters
FIGURE 11.31
(a) Cascade and (b) parallel realizations of IIR
ﬁlters.
Hi(z) first- or second-order direct form II realization
···
x[n]
y [n]
H1(z)
H2(z)
HN(z)
···
x[n]
+
y [n]
G1(z)
G2(z)
GM(z)
Gi(z) first- or second-order direct form II realization
(b)
(a)
Solution
The transfer function H(z) is not proper rational, in either positive or negative powers of z, and the
poles are z = −0.5 and z = 0.4. Thus, the transfer function can be expanded as
H(z) = A1 +
A2
1 + 0.5z−1 +
A3
1 −0.4z−1
In this case we need A1 because the numerator, in positive as well as in negative powers of z, is of
the same order as the denominator. We then have
A1 = H(z)|z=0 = −3
A2 = H(z)(1 + 0.5z−1)|z−1=−2 = −1
A3 = H(z)(1 −0.4z−1)|z−1=2.5 = 7
Letting
H1(z) =
−1
1 + 0.5z−1
H2(z) =
7
1 −0.4z−1
we obtain the parallel realization for H(z) shown in Figure 11.32.
I

11.6 Realization of Discrete Filters
699
FIGURE 11.32
Parallel realization for H(z) = (3 + 3.6z−1 +
0.6z−2)/(1 + 0.1z−1 −0.2z−2).
y[n]
x[n]
3
7
0.5
0.4
+
−
+
+
+
−
−
z−1
z−1
FIGURE 11.33
Direct form realization of FIR ﬁlter.
+
+
+
+
z−1
z−1
z−1
b0
b1
b2
b3
x[n]
y[n]
11.6.2 Realization of FIR Filters
The realization of FIR ﬁlters can be done using direct and cascade forms. Since these ﬁlters are
nonrecursive, there are no different direct forms and there is no way to implement FIR ﬁlters in
parallel.
The direct realization of an FIR ﬁlter consists in realizing the input–output equation using delays,
constant multipliers, and summers. For instance, if the transfer function of an FIR ﬁlter is given by
H(z) =
M
X
k=0
bkz−k
(11.75)
the Z-transform of the ﬁlter output can be written as
Y(z) = H(z)X(z)
where X(z) is the Z-transform of the ﬁlter input. In the time domain we have
y[n] =
M
X
k=0
bkx(n −k)
which can be realized as shown in Figure 11.33 in the case of M = 3.
Notice that M is the number of delays needed and that there are M + 1 taps, which has given the
name of tapped ﬁlters to FIR ﬁlters realized this way.

700
CHAPTER 11:
Introduction to the Design of Discrete Filters
The cascade realization of an FIR ﬁlter is based on the representation of H(z) in Equation (11.75) as
a cascade of ﬁrst- and second-order ﬁlters—that is, we let
H(z) =
rY
i=1
Hi(z)
where
Hi(z) = boi + b1iz−1
or
Hi(z) = boi + b1iz−1 + b2iz−2
I Example 11.19
Provide the cascade realization of an FIR ﬁlter with transfer function
H(z) = 1 + 3z−1 + 3z−2 + z−3
Solution
The transfer function is factored as
H(z) = (1 + 2z−1 + z−2)(1 + z−1)
which can be realized as the cascade of two FIR ﬁlters,
y1[n] = x[n] + x[n −1]
y[n] = y1[n] + 2y1[n −1] + y1[n −2]
which are realized as shown in Figure 11.34.
FIGURE 11.34
Cascade realization of FIR ﬁlter.
+
+
z −1
z −1
z −1
x[n]
x [n−1]
y1[n−1]
y1[n]
y1[n−2]
y [n]
2
I

Problems
701
11.7 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE GO
FROM HERE?
In Chapter 6 and in this chapter you have been introduced to the most important application of lin-
ear time-invariant systems: ﬁltering. The design and realization of analog and discrete ﬁlters gathers
many practical issues in signals and systems. If you pursue this topic, you will see the signiﬁcance,
for instance, of passive and active elements, feedback and operational ampliﬁers, reactance functions,
and frequency transformation in analog ﬁltering. The design and realization of discrete ﬁlters brings
together interesting topics such as quantization error and its effect on the ﬁlters, optimization meth-
ods for ﬁlter design, stabilization of unstable ﬁlters, etc. If you pursue ﬁltering deeper, you will ﬁnd
that there is a lot more on ﬁlter design than what we have provided you in this chapter. A lot more.
Also remember that MATLAB has a large number of tools to design and implement ﬁlters.
PROBLEMS
11.1. FIR ﬁlters: causality and phase—MATLAB
A three-point moving-average ﬁlter is of the form:
y[n] = β
 αx[n −1] + x[n] + αx[n + 1]

where α and β are constants, and x[n] is the input and y[n] is the output of the ﬁlter.
(a) Determine the transfer function H(z) = Y(z)/X(z) of the ﬁlter, and from it determine the frequency
response H(ejω) of the ﬁlter in terms of α and β.
(b) Find the values of α and β so that the dc gain of the ﬁlter is unity, and the ﬁlter has a zero phase. For
α = 0.5 and the corresponding value of β, sketch H(ejω) and ﬁnd the poles and zeros of H(z) and plot
them in the z-plane. Verify your results using MATLAB.
(c) Suppose we let v[n] = y[n −1] be the output of a second ﬁlter. Is this ﬁlter causal? Find its transfer
function G(z) = V(z)/X(z). Use MATLAB to compute the unwrapped phase of G(z) and to plot the
poles and zeros of G(z) and explain the relation between G(z) and H(z).
11.2. FIR and IIR ﬁlters: causality and zero phase—MATLAB
Let the ﬁlter H(z) be the cascade of a causal ﬁlter with transfer function G(z) and an anti-causal ﬁlter with
transfer function G(z−1), so that
H(z) = G(z)G(z−1)
(a) Suppose that G(z) is an FIR ﬁlter with transfer function
G(z) = 1
3(1 + 2z−1 + z−2)
Find the frequency response H(ejω) and determine its phase.
(b) Determine the impulse response of the ﬁlter H(z). Is H(z) a causal ﬁlter? If not, would delaying its
impulse response make it causal? Explain. What would be the transfer function of the causal ﬁlter.
(c) Use MATLAB to verify the unwrapped phase of H(z) you obtained analytically, and to plot the poles
and zeros of H(z).
(d) How would you use the MATLAB function conv to ﬁnd the impulse response of H(z).
(e) Suppose then that G(z) = 1/(1 −0.5z−1). Find the ﬁlter H(z) = G(z)G(z−1). Is this ﬁlter zero phase?
If so, where are its poles and zeros? If you think of ﬁlter H(z) as causal, is it BIBO stable?

702
CHAPTER 11:
Introduction to the Design of Discrete Filters
11.3. FIR and IIR ﬁlters: symmetry of impulse response and linear phase—MATLAB
Consider two FIR ﬁlters with transfer functions
H1(z) = 0.5 + 0.5z−1 + 2.2z−2 + 0.5z−3 + 0.5z−4
H2(z) = −0.5 −0.5z−1 + 0.5z−3 + 0.5z−4
(a) Find the impulse responses h1[n] and h2[n] corresponding to H1(z) and H2(z). Plot them carefully
and determine the sample value for which these impulse responses are even or odd.
(b) Show that G(z) = z2H1(z) is zero phase, and from it determine the phase of the ﬁlter H1(z). Use
MATLAB to ﬁnd the unwrapped phase of H1(z) and conﬁrm your analytic results.
(c) Find the phase of H2(z) by ﬁnding the phase of F(z) = z2H2(z). Use MATLAB to ﬁnd the unwrapped
phase of H2(z). Is it linear?
(d) If H(z) is an IIR ﬁlter, according to the above arguments could it be possible for it to have linear phase?
Explain.
11.4. Effect of phase on ﬁltering—MATLAB
Consider two ﬁlters with transfer functions
H1(z) = z−100
H2(z) =
 
0.5 1 −2z−1
1 −0.5z−1
!10
(a) Verify that the magnitude of these two ﬁlters is unity, but that they have different phases. Find
analytically the phase of H1(z) and use MATLAB to ﬁnd the unwrapped phase of H2(z) and to plot it.
(b) Consider the MATLAB signal “handel.mat” (a short piece of the “Messiah” by composer George
Handel). Use the MATLAB function ﬁlter to ﬁlter it with the two given ﬁlters. Listen to the output and
plot them and compare them. What is the difference (look at ﬁrst 200 samples of the outputs from the
two ﬁlters)?
(c) Can you recover the original signal by advancing either of the outputs? Explain.
11.5. Butterworth versus Chebyshev speciﬁcations
A Butterworth low-pass discrete ﬁlter of order N has been designed to satisfy the following speciﬁcations:
Sampling period Ts = 100 µsec
αmax = 0.7 dB for 0 ≤f ≤fp = 1000 Hz
αmin = 10 dB for fst = 1200 ≤f ≤fs/2 Hz
What should be the new value of the stopband frequency fst so that an Nth-order Chebyshev low-pass
ﬁlter satisﬁes the design speciﬁcations for Ts, αmax, αmin, and fp.
11.6. Bilinear transformation and pole location—MATLAB
Find the poles of the discrete ﬁlter obtained by applying the bilinear transformation with K = 1 to an
analog second-order Butterworth low-pass ﬁlter. Determine the half-power frequency ωhp of the resulting
discrete ﬁlter. Use the MATLAB function bilinear to verify your results.
11.7. Warping effect of the bilinear transformation—MATLAB
The nonlinear relation between the discrete frequency ω (rad) and the continuous frequency  (rad/sec)
in the bilinear transformation causes warping in the high frequencies. To see this consider the following:
(a) Use MATLAB to design a Butterworth analog band-pass ﬁlter of order N = 12 and with half-
power frequencies 1 = 10 and 2 = 20 (rad/sec). Use the MATLAB function bilinear with K = 1
to transform the resulting ﬁlter into a discrete ﬁlter. Plot the magnitude and the phase of the discrete
ﬁlter.

Problems
703
(b) Increase the order of the ﬁlter to N = 14 and keep the other speciﬁcations the same. Design an analog
band-pass ﬁlter and use again bilinear with K = 1 to transform the analog ﬁlter into a discrete ﬁlter.
Plot the magnitude and the phase of the discrete ﬁlter. Explain your results.
11.8. Warping effect of the bilinear transformation—MATLAB
The warping effect of the bilinear transformation also affects the phase of the transformed ﬁlter. Consider
a ﬁlter with transfer function G(s) = e−5s.
(a) Find the transformed discrete frequencies ω (rad) corresponding to 0 ≤ ≤20 (rad/sec) using a
bilinear transformation with K = 1. Plot  versus ω.
(b) Discretize the continuous frequencies 0 ≤ ≤20 (rad/sec) to compute values of G(j) and use
MATLAB functions to plot the phase of G(j).
(c) Find the function
H(ejω) = G(j)|=tan(ω/2)
and plot its unwrapped phase using MATLAB for the discrete frequencies corresponding to the
analog frequencies to 0 ≤ ≤20 (rad/sec).
11.9. Discrete Butterworth ﬁlter for analog processing—MATLAB
Design a Butterworth low-pass discrete ﬁlter that satisﬁes the following speciﬁcations:
0 ≤α(ejω) ≤3 dB
for 0 ≤f ≤25 Hz
α(ejω) ≥38 db
for 50 ≤f ≤Fs/2 Hz
The sampling frequency is Fs = 2000 Hz. Express the transfer function H(z) of the designed ﬁlter as a
cascade of ﬁlters. Use ﬁrst the design formulas and then use MATLAB to conﬁrm your results. Show that
the designed ﬁlter satisﬁes the speciﬁcations, plotting the loss function of the designed ﬁlter.
11.10. All-pass IIR ﬁlter—MATLAB
Consider an all-pass analog ﬁlter
G(s) = s4 −4s3 + 8s2 −8s + 4
s4 + 4s3 + 8s2 + 8s + 4
(a) Use MATLAB functions to plot the magnitude and the phase responses of G(s). Indicate whether the
phase is linear.
(b) A discrete ﬁlter H(z) is obtained from G(s) by the bilinear transformation. By trial and error, ﬁnd the
value of K in the bilinear transformation so that the poles and zeros of H(z) are on the imaginary axis
of the z-plane. Use MATLAB functions to do the bilinear transformation and to plot the magnitude
and unwrapped phase of H(z) and its poles. Is it an all-pass ﬁlter? If so, why?
(c) Let the input to the ﬁlter H(z) be x[n] = sin(0.2πn), 0 ≤n < 100, and the corresponding output be
y[n]. Use MATLAB functions to compute and plot y[n]. From these results would you say that the
phase of H(z) is approximately linear? Why or why not?
11.11. Butterworth ﬁltering of analog signal—MATLAB
We wish to design a discrete Butterworth ﬁlter that can be used in ﬁltering a continuous-time signal.
The frequency components of interest in this signal are between 0 and 1 KHz, so we would like the ﬁlter
to have a maximum passband attenuation of 3 dB within that band. The undesirable components of the
input signal occur beyond 2 KHz, and we would like to attenuate them by at least 10 dB. The maximum
frequency present in the input signal is 5 KHz. Finally, we would like the dc gain of the ﬁlter to be 10.
Choose the Nyquist sampling frequency to process the input signal. Use MATLAB to design the ﬁlter.
Give the transfer function of the ﬁlter, plot its poles and zeros and its magnitude and unwrapped phase
response using an analog frequency scale in KHz.

704
CHAPTER 11:
Introduction to the Design of Discrete Filters
11.12. Butterworth versus Chebyshev ﬁltering—MATLAB
If we wish to preserve low-frequency components of the input, a low-pass Butterworth ﬁlter could perform
better than a Chebyshev ﬁlter. MATLAB provides a second Chebyshev ﬁlter function cheby2 that has a
ﬂat response in the passband and a rippled one in the stopband. Let the signal to be ﬁltered be the ﬁrst
100 samples from MATLAB’s “train” signal. To this signal add some Gaussian noise to be generated by
randn, multiply it by 0.1, and add it to the 100 samples of the train signal. Design three discrete ﬁlters,
each of order 20, and a half frequency (for Butterworth butter) and passband frequency (for the Chebyshev
ﬁlters) of ωn = 0.5. For the design with cheby1 let the maximum passband attenuation be 0.01 dB, and
for the design with cheby2 let the minimum stopband attenuation be 60 dB. Obtain the three ﬁlters and
use them to ﬁlter the noisy “train” signal.
Using MATLAB plot the following for each of the three ﬁlters:
(a) Using the fft function compute the DFT of the original signal, the noisy signal, and the noise, and plot
their magnitudes. Is the cut-off frequency of the ﬁlters adequate to get rid of the noise? Explain.
(b) Compute and plot the magnitude and the unwrapped phase and the poles and the zeros for each of
the three ﬁlters. Comment on the differences in the magnitude responses.
(c) Use the ﬁlter function to obtain the output of each of the ﬁlters, and plot the original noiseless signal
and the ﬁltered signals. Compare them.
11.13. Butterworth, Chebyshev, and elliptic ﬁlters—MATLAB
The gain speciﬁcations of a ﬁlter are:
−0.1 ≤20 log10 |H(ejω)| ≤0 (dB)
0 ≤ω ≤0.2π
20 log10 |H(ejω)| ≤−60 (dB)
0.3π ≤ω ≤π
(a) Find the loss speciﬁcations for this ﬁlter.
(b) Design using MATLAB a Butterworth, a Chebyshev (using cheby1), and an elliptic ﬁlter. Plot in one
plot the magnitude response of the three ﬁlters, and compare them and indicate which gives the
lowest order.
11.14. Notch and all-pass ﬁlters—MATLAB
Notch ﬁlters are a family of ﬁlters that include the all-pass ﬁlter. For the ﬁlter
H(z) = K (1 −α1z−1)(1 + α2z−1)
(1 −0.5z−1)(1 + 0.5z−1)
(a) Determine the values of α1, α2, and K that would make H(z) an all-pass ﬁlter of unit magnitude. Use
MATLAB to compute and plot the magnitude response of H(z) using the obtained values for α and
K. Plot the poles and the zeros of this ﬁlter.
(b) If we would like the ﬁlter H(z) to be a notch ﬁlter of unit gain at ω = π/2 rad, determine the values of
α and K to achieve this and then determine where the notch(es) are. Use MATLAB functions to verify
that the ﬁlter is a notch ﬁlter, and to plot the poles and the zeros.
(c) Place the zeros of H(z) at positions between the zeros for the all-pass and the notch ﬁlters, and use
MATLAB to plot the magnitude responses. Each of these ﬁlters must have unit gain at ω = π/2 rad.
Explain the connection between the all-pass and the notch ﬁlters.
(d) Suppose we use the transformation z−1 = jZ−1 to obtain a ﬁlter H(Z). Repeat the above part of the
problem for H(Z). Where are the notches of this new ﬁlter. What would be the difference between the
all-pass ﬁlters H(z) and H(Z)?

Problems
705
11.15. IIR comb ﬁlters—MATLAB
Consider a ﬁlter with transfer function
H(z) = K
1 + z−4
1 + (1/16)z−4
(a) Find the gain K so that this ﬁlter has a unit dc gain. Use then MATLAB to ﬁnd and plot the magnitude
response of H(z) and its poles and zeros. Indicate why it is called a comb ﬁlter.
(b) Use MATLAB to ﬁnd the phase response of the ﬁlter H(z). Why is it that the phase seems to be
wrapped and it cannot be unwrapped by MATLAB?
(c) Suppose you wish to obtain an IIR comb ﬁlter that is sharper around the notches of H(z) and ﬂatter in
between notches. Implement such a ﬁlter using the function butter to obtain two notch ﬁlters of order
10 and appropriate cut-off frequencies. Decide how to connect the two ﬁlters. Plot the magnitude and
the phase of the resulting ﬁlter and its poles and zeros.
11.16. Three-band discrete spectrum analyzer—MATLAB
To design a three-band discrete spectrum analyzer for speech signals, we need to design a low-pass, a
band-pass, and a high-pass ﬁlter. Let the sampling frequency be Fs = 10 KHz. Consider the three bands,
in KHz, to be [0 Fs/4], (Fs/4 3Fs/8], and (3Fs/8 Fs/2]. Let all the ﬁlters be of order N = 4, and choose
the cut-off frequencies so that the sum of the three ﬁlters is approximately an all-pass ﬁlter of unit gain.
11.17. FIR ﬁlter design with different windows
Design a low-pass FIR digital ﬁlter with N = 21. The desired response of the ﬁlter is
|Hd(ejwT)| =
1
0 ≤f ≤250 Hz
0
elsewhere in 0 ≤f ≤(fs/2)
where ω = 2πf/fs and the phase is zero for all frequencies. The sampling frequency is fs = 2000 Hz.
(a) Use a rectangular window in your design. Plot the magnitude and the phase of the designed ﬁlter.
(b) Use a triangular window in the design and compare the magnitude and the phase plots of this ﬁlter
with those obtained in (a).
11.18. FIR ﬁlter design—MATLAB
Design an FIR low-pass ﬁlter with a cut-off frequency of π/3 and lengths N = 21 and then N = 81:
(a) Using a rectangular window.
(b) Use MATLAB to design the ﬁlter using the rectangular, Hamming, and Kaiser windows, and compare
the magnitude of the resulting ﬁlters.
11.19. Modulation property transformation for IIR ﬁlters—MATLAB
The modulation-based frequency transformation is applicable to IIR ﬁlters. It is obvious in the case of FIR
ﬁlters, but requires a few more steps in the case of IIR ﬁlters. In fact, if we have that the transfer function
of the prototype IIR low-pass ﬁlter is H(z) = B(z)/A(z), with impulse response h[n], let the transformed
ﬁlter be ˆH(z) = Z(2h[n] cos(ωon) for some frequency ω0.
(a) Find the transfer function ˆH(z) in terms of H(z).
(b) Consider an IIR low-pass ﬁlter
H(z) =
1
1 −0.5z−1
If ω0 = π/2, determine ˆH(z).
(c) How would you obtain a high-pass ﬁlter from H(z) given in the previous item? Use MATLAB to plot
the resulting ﬁlters here and in the past item.

706
CHAPTER 11:
Introduction to the Design of Discrete Filters
11.20. Down-sampling transformations—MATLAB
Consider down sampling the impulse response h[n] of a ﬁlter with transfer function
H(z) =
1
1 −0.5z−1
(a) Use MATLAB to plot h[n] and the down sampled impulse response g[n] = h[2n].
(b) Plot the magnitude responses corresponding to h[n] and g[n] and comment on the effect of the down
sampling.
11.21. Modulation property transformation—MATLAB
Consider a moving-average, low-pass, FIR ﬁlter,
H(z) = 1 + z−1 + z−2
3
(a) Use the modulation property to convert the given ﬁlter into a high-pass ﬁlter.
(b) Use MATLAB to plot the magnitude responses of the low-pass and the high-pass ﬁlters.
11.22. Implementation of IIR rational transformation—MATLAB
Use MATLAB to design a Butterworth second-order low-pass discrete ﬁlter H(Z) with half-power fre-
quency θhp = π/2 and a dc gain of 1. Consider this low-pass ﬁlter a prototype that can be used to
obtain other ﬁlters. Implement using MATLAB the frequency transformations Z−1 = N(z)/D(z) using the
convolution property to multiply polynomials to obtain:
(a) A high-pass ﬁlter with a half-power frequency ωhp = π/3 from the low-pass ﬁlter.
(b) A band-pass ﬁlter with ω1 = π/2 and ω2 = 3π/4 from the low-pass ﬁlter.
(c) Plot the magnitude of the low-pass, high-pass, and band-pass ﬁlters.
Give the corresponding transfer functions for the low-pass as well as the high-pass and the band-pass
ﬁlters.
11.23. Parallel connection of IIR ﬁlters—MATLAB
Use MATLAB to design a Butterworth second-order low-pass discrete ﬁlter with half-power frequency
θhp = π/2 and a dc gain of 1; call it H(z). Use this ﬁlter as a prototype to obtain a ﬁlter composed of a
parallel combination of the following ﬁlters:
(a) Assume that we upsample by L = 2 the impulse response h(n) of H(z) to get a new ﬁlter H1(z) =
H(z2). Determine H1(z) and plot its magnitude using MATLAB.
(b) Assume then that we shift H(z) by π/2 to get a band-pass ﬁlter H2(z). Find the transfer function of
H2(z) from H(z) and then plot its magnitude.
(c) If the ﬁlters H1(z) and H2(z) are connected in parallel, what is the overall transfer function G(z) of the
parallel connection? Plot the magnitude response corresponding to G(z).
11.24. Realization of IIR ﬁlters
Consider the following transfer function:
H(z) =
2(z −1)(z2 +
√
2 z + 1)
(z + 0.5)(z2 −0.9z + 0.81)
(a) Develop a cascade realization of H(z) using ﬁrst- and second-order sections. Use direct form II to
realize each of the sections.
(b) Develop a parallel realization of H(z) by considering ﬁrst- and second-order sections, each realized
using direct form II.

Problems
707
11.25. Realization of IIR ﬁlters
Given the realization in Figure 11.35:
FIGURE 11.35
Problem – 11.25: IIR realization.
+
+
+
+
z−1
z−1
z−1
x[n]
v[n]
g [n]
w[n]
y [n]
2
3
0.3
0.4
−0.8
4.5
1.3
1.8
(a) Obtain the difference equations relating g[n] to x[n] and g[n] to y[n].
(b) Obtain the transfer function H(z) = Y(z)/X(z) for this ﬁlter.

This page intentionally left blank

CHAPTER 12
Applications of Discrete-Time
Signals and Systems
Nullius in verba
(Take nobody’s word for it).
Motto of the Royal Society,
Britain’s 350-year-old science fraternity
12.1 INTRODUCTION
In this chapter we will present applications of the theory of discrete-time signals and systems to three
important areas: digital signal processing, digital control, and digital communications. The material
in this chapter is meant to be a more motivational than detailed presentation. We encourage our
readers to look for the details in excellent textbooks in these three areas [54, 22, 16].
Given the advances in digital technologies and computers, processing of signals is being done mostly
digitally. Early results in sampling, analog-to-digital conversion, and the fast computation of the
output of linear systems using the Fast Fourier Transform (FFT) made it possible for digital signal
processing to become a technical area on its own. (The ﬁrst books in this area [63, 54] come from the
mid-1970s.) Although the origins of the FFT have been traced back to the German mathematician
Gauss in the early 1800s, the modern theory of the algorithm comes from the 1960s. It should be
understood that the FFT is not yet another transform, but an efﬁcient algorithm to compute the
discrete Fourier transform (DFT), which we covered in Chapter 10.
Analog classic control systems can be implemented digitally using analog-to-digital and digital-to-
analog converters and computers to implement the control laws. The theory of sampled data shows
the connection between the Laplace and the Z-transform. The difﬁculty in the analysis of these
systems is the mixing of continuous- and discrete-time signals.
Digital communication systems provide a more efﬁcient way to communicate information than
analog communication systems, but they are more demanding in terms of bandwidth. As
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00016-8
c⃝2011, Elsevier Inc. All rights reserved.
709

710
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
indicated before, digital communications began with the introduction of pulse code modulation
(PCM). Telephony and radio using baseband and band-pass signals have converged into wireless
communications. Many of the principles of analog communications have remained, but its
implementation has changed from analog to digital with slightly different objectives. Efﬁcient use
of the radio spectrum and efﬁcient processing have become the objectives of modern wireless com-
munication systems such as spread spectrum and orthogonal frequency-division multiplexing, which
we introduce here.
12.2 APPLICATION TO DIGITAL SIGNAL PROCESSING
In many applications, such as speech processing or acoustics, one would like to digitally process
analog signals. In practice, this is possible by converting the analog signals into binary signals using
an analog-to-digital converter (ADC), and if the output is desired in analog form a digital-to-analog
converter (DAC) is used to convert the binary signal into a continuous-time signal. Ideally, if no
quantization is considered and if the discrete-time signal is converted into an analog signal by sinc
interpolation the system can be visualized as in Figure 12.1.
Viewing the whole system as a black box with an analog signal x(t) as input, and giving as output
also an analog signal y(t), the processing can be seen as a continuous-time system with a transfer
function G(s). Under the assumption of no quantization, the discrete-time signal x[n] is obtained by
sampling x(t) using a sampling period determined by the Nyquist sampling condition. Likewise, con-
sidering the transformation of a discrete-time (or sampled signal) y[n] into a continuous-time signal
y(t) by means of the sinc interpolation, the ideal DAC is an analog low-pass ﬁlter that interpolates
the discrete-time samples to obtain an analog signal. Finally, the discrete-time signal x[n] is pro-
cessed by a discrete-time system with transfer function H(z), which depends on the desired transfer
function G(s).
Thus, one can process discrete- or continuous-time signals using discrete systems. A great deal of the
computational cost of this processing is due to the convolution sum used to obtain the output of
the discrete system. That is where the signiﬁcance of the FFT algorithm lies. Although the DFT allows
us to simplify the convolution to a multiplication, it is the FFT that as an algorithm provides a very
efﬁcient implementation of this process. In the next section, we will introduce you to the FFT and
provide some of the basics of this algorithm for you to understand its efﬁciency.
FIGURE 12.1
Discrete processing of analog signals
using an ideal ADC and DAC. G(s) is
the transfer function of the overall
system, while H(z) is the transfer
function of the discrete-time system.
Discrete system
Equivalent analog system
G(s)
x(t)
x[n]
y[n]
y(t)
Hr(jΩ)
H(z)
Ideal ADC
Ideal DAC

12.2 Application to Digital Signal Processing
711
12.2.1 Fast Fourier Transform
Comparing the equations for the DFT and the inverse DFT, or IDFT
X[k] =
N−1
X
n=0
x[n]Wkn
N
k = 0, . . . , N −1
(12.1)
x[n] = 1
N
N−1
X
k=0
X[k]W−kn
N
n = 0, . . . , N −1
(12.2)
where WN = e−j2π/N, one sees a great deal of duality (more so if both the DFT and the IDFT had the
term 1/
√
N instead of only the 1/N in the IDFT). Since X[k] is complex, one can also see that one
algorithm could be used for both the direct and the inverse DFTs if we assume x[n] to be complex.
Two issues used to assess the complexity of an algorithm are:
I
Total number of additions and multiplications: Typically, the complexity of a computational algo-
rithm is assessed by determining the number of additions and multiplications it requires. The
direct calculation of X[k] using Equation (12.1) for k = 0, . . . , N −1 requires N × N complex
multiplications, and N × (N −1) complex additions. Computing the number of real multiplica-
tions and real divisions needed, it is found that the total number of these operations is of the
order of N2.
I
Storage: Besides the number of computations, the required storage is an issue of interest.
Radix 2 FFT Algorithm
In the following, we consider the basic algorithm for the FFT. We assume that the FFT length is N = 2γ
for an integer γ > 1. Excellent references on the DFT and the FFT are Briggs and Henson [10] and
Brigham [11]
The FFT algorithm:
I
Uses the fundamental principle of “divide and conquer”: Dividing a problem into smaller prob-
lems with similar structure, the original problem can be successfully solved by solving each of the
smaller problems.
I
Takes advantage of periodicity and symmetry properties of Wnk
N :
(a)
Periodicity: Wnk
N is periodic of period N with respect to n, and with respect to k—that is,
Wnk
N =
(
W(n+N)k
N
Wn(k+N)
N
(b)
Symmetry: The conjugate of Wnk
N is such that
h
Wnk
N
i∗
= W(N−n)k
N
= Wn(N−k)
N

712
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
Decimation-in-Time Algorithm
Applying the divide-and-conquer principle, we express X[k] as
X[k] =
N−1
X
n=0
x[n]Wkn
N
k = 0, . . . , N −1
=
N/2−1
X
n=0
h
x[2n]Wk(2n)
N
+ x[2n + 1]Wk(2n+1)
N
i
That is, we gather the samples with even arguments separately from those with odd arguments.
From the deﬁnition of Wnk
N we have that
Wk(2n)
N
= e−j2π(2kn)/N = e−j2πkn/(N/2) = Wkn
N/2
Wk(2n+1)
N
= Wk
NWkn
N/2
which allows us to write
X[k] =
N/2−1
X
n=0
x[2n]Wkn
N/2 + Wk
N
N/2−1
X
n=0
x[2n + 1]Wkn
N/2
= Y[k] + Wk
NZ[k]
k = 0, . . . , N −1
(12.3)
where Y[k] and Z[k] are DFTs of length N/2 of the even-numbered sequence {x[2n]} and of the
odd-numbered sequence {x[2n + 1]}, respectively.
Although it is clear how to compute the values of X[k] for k = 0, . . . , (N/2) −1 as
X[k] = Y[k] + Wk
NZ[k]
k = 0, . . . , (N/2) −1
(12.4)
it is not clear how to proceed for k ≥N/2. The N/2 periodicity of Y[k] and Z[k] allows us to ﬁnd
those values:
X[k + N/2] = Y[k + N/2] + Wk+N/2
N
Z[k + N/2]
= Y[k] −Wk
NZ[k]
k = 0, . . . , N/2 −1
(12.5)
where besides the periodicity of Y[k] and Z[k], we used
Wk+N/2
N
= e−j2π[k+N/2]/N = e−j2πk/Ne−jπ = −Wk
N
Writing Equations (12.4) and (12.5) in a matrix form we have
XN =
IN/2
N/2
IN/2
−N/2
 YN/2
ZN/2

= A1
YN/2
ZN/2

(12.6)

12.2 Application to Digital Signal Processing
713
where IN/2 is a unit matrix and N/2 is a diagonal matrix with entries {Wk
N, k = 0, . . . , N/2 −1}; both
matrices have dimension N/2 × N/2. The vectors XN, YN/2, and ZN/2 contain the coefﬁcients of x[n],
y[n], and z[n].
Repeating the above computation for Y[k] and Z[k] we can express it in a similar matrix form until
we reduce the process to 2 × 2 matrices. While performing these computations, the ordering of x[n]
is changed. This scrambling of x[n] is obtained by a permutation matrix PN (with 1 and 0 entries
indicating the resulting ordering of the x[n] samples). If N = 2γ , the XN vector, containing the DFT
terms X[k], is obtained as the product of γ matrices Ai and the permutation matrix. That is,
XN =
" γY
i=1
Ai
#
PN x
x = [x[0], . . . , x[N −1]]T
(12.7)
where T stands for transpose. Given the large number of 1s and 0s in the {Ai} and the PN matrices,
the number of additions and multiplications is much lower than those in the original formulas.
The number of operations is found to be of the order of Nlog2N = γ N, which is much smaller
than the original number of order N2. For instance, if N = 210 = 1024, the number of additions and
multiplications for the computation of the DFT from its original formula is N2 = 220 = 1,048,576,
while the FFT computation requires Nlog2N = 1024 × 10 = 10,240—that is, the FFT requires about
1% of the number of operations required by the original formula for the DFT.
I Example 12.1
Consider the decimation-in-time FFT algorithm for N = 4. Give the equations to compute the four
DFT values X[k], k = 0, . . . , 3 in matrix form.
Solution
If we compute the DFT of x[n] directly we have that
X[k] =
3
X
n=0
x[n]Wnk
4
k = 0, . . . , 3
which can be rewritten in the matrix form as


X[0]
X[1]
X[2]
X[3]

=


1
1
1
1
1
W1
4
W2
4
W3
4
1
W2
4
1
W2
4
1
W3
4
W2
4
W1
4




x[0]
x[1]
x[2]
x[3]


where we used
W4
4 = W4+0
4
= e−j2π0/4 = W0
4 = 1
W6
4 = W4+2
4
= e−j2π2/4 = W2
4
W9
4 = W4+4+1
4
= e−j2π1/4 = W1
4

714
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
which requires 16 multiplications (8 if multiplications by 1 are not counted) and 12 additions. Thus,
either 28 or 20, if multiplications by 1 are not counted, multiplications and additions are required.
Since the entries are complex, these are complex additions and multiplications. A complex addition
requires 2 real additions, and a complex multiplication requires 4 real multiplications and 3 real
additions. Indeed, for two complex numbers z = a + jb and v = c + jd, z + v = (a + c) + j(b + c)
and zv = (ac −bd + j(bc + ad)). Thus, the total number of real multiplications is 16 × 4 and real
additions is 12 × 2 + 16 × 3 for a total of 136 operations.
Separating the even- and the odd-numbered entries of x[n], we have
X[k] =
1
X
n=0
x[2n]Wkn
2 + Wk
4
1
X
n=0
x[2n + 1]Wkn
2
= Y[k] + Wk
4Z[k]
k = 0, . . . , 3
which can be written as
X[k] = Y[k] + Wk
4Z[k]
X[k + 2] = Y[k] −Wk
4Z[k]
k = 0, 1
In matrix form the above equations can be written as


X[0]
X[1]
· · ·
X[2]
X[3]


=


1
0
...
1
0
0
1
...
0
W1
4
· · ·
· · ·
· · ·
· · ·
· · ·
1
0
...
−1
0
0
1
...
0
−W1
4




Y[0]
Y[1]
· · ·
Z[0]
Z[1]


= A1


Y[0]
Y[1]
Z[0]
Z[1]


which is in the form indicated by Equation (12.6).
Now we have that
Y[k] =
1
X
n=0
x[2n]Wkn
2 = x[0]W0
2 + x[2]Wk
2
Z[k] =
1
X
n=0
x[2n + 1]Wkn
2 = x[1]W0
2 + x[3]Wk
2
k = 0, 1

12.2 Application to Digital Signal Processing
715
which in matrix form is


Y[0]
Y[1]
· · ·
Z[0]
Z[1]


=


1
1
...
0
0
1
−1
...
0
0
· · ·
· · ·
· · ·
· · ·
· · ·
0
0
...
1
1
0
0
...
1
−1




x[0]
x[2]
· · ·
x[1]
x[3]


= A2


x[0]
x[2]
x[1]
x[3]


where we replaced W1
2 = e−j2π/2 = −1. Notice the change in the ordering of the {x[n]}.
Reordering the x[n] entries we have


x[0]
x[2]
x[1]
x[3]

=


1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1




x[0]
x[1]
x[2]
x[3]

= P4


x[0]
x[2]
x[1]
x[3]


which ﬁnally gives


X[0]
X[1]
X[2]
X[3]

= A1 A2 P4


x[0]
x[1]
x[2]
x[3]


The count of multiplications is now much lower given the number of 1s and 0s. The complex
additions and multiplications are now 10 (2 complex multiplications and 8 complex additions) if
we do not count multiplications by 1 or −1. Half of what it was before!
I
12.2.2 Computation of the Inverse DFT
The FFT algorithm can be used to compute the inverse DFT without any changes in the algorithm.
Assuming the input x[n] is complex (x[n] being real is a special case), the complex conjugate of the
inverse DFT equation, multiplied by N, is
Nx∗[n] =
N−1
X
k=0
X∗[k]Wnk
(12.8)
Ignoring that the right term is in the frequency domain, we recognize it as the DFT of a sequence
{X∗[k]} and it can be computed using the FFT algorithm discussed before. The desired x[n] is thus
obtained by computing the complex conjugate of Equation (12.8) and dividing it by N. As a result,
the same algorithm, with the above modiﬁcation, can be used to compute both the direct and the
inverse DFTs.
Remark In the FFT algorithm the 2N memory allocations for the complex input (one allocation for the real
part and another for the imaginary part of the input) are the same ones used for the output. Each step uses

716
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
the same locations. Since X[k] is typically complex, to have identical allocation with the output, the input
sequence x[n] is assumed to be complex. If x[n] is real, it is possible to transform it into a complex sequence
and use properties of the DFT to obtain X[k].
12.2.3 General Approach of FFT Algorithms
Although there are many algorithmic approaches to the FFT, the initial idea was to represent the
ﬁnite one-dimensional signal x[n] as a two-dimensional array. This can be done by representing the
length N of x[n] as the product of smaller integers, provided N is not prime. If N is prime, the DFT
computation is done with the conventional formula, as the FFT does not provide any simpliﬁcation.
However, in that case we could attach zeros to the signal (if the signal is not periodic) to increase
its length to a nonprime number. This factorization approach has historic signiﬁcance as it was the
technique used by Cooley and Tukey, the authors of the FFT.
Suppose N can be factored as N = pq; then the frequency and time indices k and n in the direct DFT
can be written as
k = k1p + k0
for k0 = 0, . . . , p −1, k1 = 0, . . . , q −1
n = n1q + n0
for n0 = 0, . . . , q −1, n1 = 0, . . . , p −1
The values of k range from 0 (when k0 = k1 = 0) to N −1 (when k1 = q −1 and k0 = p −1 then
k = k1p + k0 = (q −1)p + (p −1) = qp −1 = N −1). Likewise for n. The direct DFT
X[k] =
N−1
X
n=0
x[n]Wnk
can be written to reﬂect the dependence on the new indices as
X[k0, k1] =
q−1
X
n0=0
p−1
X
n1=0
x[n0, n1]W(n1q+n0)(k1p+k0)
N
(12.9)
giving a two-dimensional array.
The decimation-in-time FFT presented before may be viewed in this framework by letting q = 2 and
p = N/2, where as before N = 2γ is even. We then have using W2n1(k1p+k0)
N
= Wn1(k1p+k0)
N/2
,
X[k0, k1] =
1
X
n0=0
Wn0(k1p+k0)
N
N/2−1
X
n1=0
x[n0, n1]Wn1(k1p+k0)
N/2
=
N/2−1
X
n1=0
x[0, n1]Wn1(k1p+k0)
N/2
+ W(k1p+k0)
N
N/2−1
X
n1=0
x[1, n1]Wn1(k1p+k0)
N/2
= Y[k] + W(k1p+k0)
N
Z[k]
k0 = 0, . . . , N/2 −1, k1 = 0, 1
(12.10)
where when n0 = 0 the even terms (x[0, n1] = x[qn1] = x[2n1]) in the input are being transformed,
while when n0 = 1 the odd terms (x[1, n1] = x[qn1 + 1] = x[2n1 + 1]) of the input are being

12.2 Application to Digital Signal Processing
717
transformed. Since k = k1p + k0 the ﬁnal equation is Y[k] + Wk
NZ[k], which we obtained in the
decimation-in-time approach (see Eq. 12.3).
Factoring N = 2 × N/2 corresponds to one step of the decimation-in-time method. If we factor N/2
as N/2 = (2)(N/4), we would obtain the second step in the decimation-in-time algorithm. If N = 2γ ,
this process is repeated γ times or until the length is 2.
Remark A dual of the decimation-in-time FFT algorithm is the decimation-in-frequency method.
The Modern FFT
A paper by James Cooley, an IBM researcher, and Professor John Tukey from Princeton University [15] describing an algo-
rithm for the machine calculation of complex Fourier series appeared in Mathematics of Computation in 1965. Cooley,
a mathematician, and Tukey, a statistician, had in fact developed an efﬁcient algorithm to compute the discrete Fourier
transform (DFT), which will be called the FFT. Their result was a turning point in digital signal processing: The proposed
algorithm was able to compute the DFT of a sequence of length N using N log N arithmetic operations, much smaller than
the N 2 operations that had blocked the practical use of the DFT. As Cooley indicated in his paper “How the FFT Gained
Acceptance” [14], his interest in the problem came from a suggestion from Tukey on letting N be a composite number,
which would allow a reduction in the number of operations of the DFT computation.
The FFT algorithm was a great achievement for which the authors received deserved recognition, but also beneﬁted the
new digital signal processing area, and motivated further research on the FFT. But as in many areas of research, Cooley
and Tukey were not the only ones who had developed an algorithm of this class. Many other researchers before them had
developed similar procedures. In particular, Danielson and Lanczos, in a paper published in the Journal of the Franklin
Institute in 1942 [19], proposed an algorithm that came very close to Cooley and Tukey’s results. Danielson and Lanczos
showed that a DFT of length N could be represented as a sum of two N/2 DFTs proceeding recursively with the condition
that N = 2γ . Interestingly, they mention that (remember this was in 1942!):
Adopting these improvements the approximation time for Fourier analysis are: 10 minutes for 8 coefﬁcients, 25
minutes for 16 coefﬁcients, 60 minutes for 32 coefﬁcients, and 140 minutes for 64 coefﬁcients.
I Example 12.2
Consider computing the FFT of a signal of length N = 23 = 8 using the decimation-in-time
algorithm.
Solution
Letting N = qp = 2 × 4, we then have that
X[k0, k1] =
1
X
n0=0
Wn0(k1p+k0)
8
N/2−1
X
n1=0
x[n0, n1]Wn1(k1p+k0)
4
=
3
X
n1=0
x[0, n1]Wn1(k1p+k0)
4
+ W(k1p+k0)
8
3
X
n1=0
x[1, n1]Wn1(k1p+k0)
4
where
k = 4k1 + k0
for k0 = 0, . . . , 3, k1 = 0, 1
n = 2n1 + n0
for n0 = 0, 1, n1 = 0, . . . , 3

718
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
FIGURE 12.2
Lattice for n0 = 0, 1 and n1 = 0, . . . , 3 (the values in
parentheses are the indices of the samples). Notice the
ordering in the two columns.
n1
n0
1
0
2
3
1
(0)
(2)
(4)
(6)
(1)
(3)
(5)
(7)
Figure 12.2 displays a lattice for n0 and n1 and the indices of the samples are in parentheses. By
replacing k = 4k1 + k0, we get the ﬁrst step of the decimation-in-time.
If we let y[n] = x[2n] and z[n] = x[2n + 1], n = 0, . . . , 3, we can then repeat the above procedure
by factoring 4 = pq = 2 × 2 and expressing Y[k] and Z[k] as we did for X[k]. Thus, we have
Y[k0, k1] =
1
X
n0=0
Wn0(k1p+k0)
4
1
X
n1=0
y[n0, n1]Wn1(k1p+k0)
2
=
1
X
n1=0
y[0, n1]Wn1(k1p+k0)
2
+ W(k1p+k0)
4
1
X
n1=0
y[1, n1]Wn1(k1p+k0)
2
and
Z[k0, k1] =
1
X
n1=0
z[0, n1]Wn1(k1p+k0)
2
+ W(k1p+k0)
4
1
X
n1=0
z[1, n1]Wn1(k1p+k0)
2
where now
k = 2k1 + k0
for k0 = 0, 1, k1 = 0, 1
n = 2n1 + n0
for n0 = 0, 1, n1 = 0, 1
If we replace k = 2k1 + k0, we obtain
Y[k] = I[k] + Wk
4G[k]
Z[k] = H[k] + Wk
4F[k]
where
I[k] =
1
X
n1=0
y[0, n1]Wn1k
2
=
1
X
n1=0
y[2n1]Wn1k
2
=
1
X
n1=0
x[4n1]Wn1k
2

12.2 Application to Digital Signal Processing
719
G[k] =
1
X
n1=0
y[1, n1]Wn1k
2
=
1
X
n1=0
y[2n1 + 1]Wn1k
2
=
1
X
n1=0
x[4n1 + 2]Wn1k
2
Likewise,
H[k] =
1
X
n1=0
z[0, n1]Wn1k
2
=
1
X
n1=0
z[2n1]Wn1k
2
=
1
X
n1=0
x[4n1 + 1]Wn1k
2
F[k] =
1
X
n1=0
z[1, n1]Wn1k
2
=
1
X
n1=0
z[2n1 + 1]Wn1k
2
=
1
X
n1=0
x[4n1 + 3]Wn1k
2
I
I Example 12.3
In this example we wish to compare the efﬁciency of the FFT algorithm with that of our algorithm
dft.m that computes the DFT using its deﬁnition. Consider the computation of the FFT and the DFT
of a signal consisting of ones of increasing lengths N = 2r, r = 8, . . . , 11, or 256 to 2048.
Solution
To compare the algorithms we use the following script. The MATLAB function cputime measures
the time it takes for each of the algorithms to compute the DFT of the sequence of ones.
%%%%%%%%%%%%%%
% example 12.3
% fft vs dft
%%%%%%%%%%%%%%
clf; clear all
time = zeros(1,4); time1 = time;
for r = 8:11,$$
N(r) = 2 ˆ r;
i = r - 7;
t = cputime;
fft(ones(1,N(r)),N(r));
time(i) = cputime - t;
t = cputime;
dft(ones(N(r),1),N(r));
time1(i) = cputime - t;
end
%%%%%%%%%%%
% function dft
%%%%%%%%%%%
function X = dft(x,N)
n = 0:N - 1;

720
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
FIGURE 12.3
CPU times for the fft and the
dft functions used in
computing the DFT of
sequences of ones of lengths
N = 256 to 2048
(corresponding to n = 8, . . . 11).
The CPU time for the FFT is
multiplied by 104.
8
8.5
9
9.5
10
10.5
11
0
100
200
300
400
500
600
700
800
n = log2(N)
CPU Time (sec)
104 fft time 
dft time
W = ones(1,N);
for k = 1:N - 1,
W = [W; exp(-j ∗2 ∗pi ∗n ∗k/N)];
end
X = W ∗x;
The results of the comparison are shown in Figure 12.3. Notice that to make the Computer Pro-
cessing Unit (CPU) time for the FFT comparable with that of the dft algorithm, it is multiplied
by 104, illustrating how much faster the FFT is compared to the computation of the DFT from its
deﬁnition.
I
I Example 12.4
The convolution sum is computationally very expensive. Compare the CPU time used by the MAT-
LAB function conv, which is used to compute the convolution sum in the time domain, with the
CPU time used by an implementation of the convolution sum in frequency using the FFT. Recall
that the frequency implementation requires computing the DFT of the signals being convolved,
their multiplication, and ﬁnally the computation of the IDFT to the ﬁnal convolution result.
Solution
To illustrate the efﬁciency in computation provided by the FFT in computing the convolu-
tion sum we compare the CPU times used by the conv function and the implementation of
the convolution sum using the FFT. As indicated before, the convolution of two signals x[n],

12.2 Application to Digital Signal Processing
721
and y[n] of lengths N and M is obtained in the frequency domain by following these three
steps:
I
Compute the DFTs X[k], Y[k] of x[n], and y[n] of length M + N −1.
I
Multiply these complex DFTs to get X[k]Y[k] = U[k].
I
Compute the IDFT of U[k] corresponding to the convolution x[n] ∗y[n].
Implementing the DFT and the IDFT with the FFT algorithm it can be shown that the computa-
tional complexity of the above three steps is much smaller than that of computing the convolution
sum directly using the conv function.
To demonstrate the efﬁciency of the FFT implementation we consider the convolution of a signal,
for increasing lengths, with itself. The signal is a sequence of ones of increasing length of 1000 to
10,000 samples. The CPU times used by the functions conv and the FFT three-step procedure are
measured and compared for each of the lengths. The CPU time used by conv is divided by 10 to be
able to plot it with the CPU of the FFT-based procedure shown in the following script. The results
are shown in Figure 12.4.
%%%%%%%%%%%%%
% example 12.4
% conv vs fft
%%%%%%%%%%%%%
time1 = zeros(1,10);time2 = time1;
for i = 1:10,
NN = 1000 ∗i;
x = ones(1,NN);
FIGURE 12.4
CPU times for the fft and
the conv functions when
computing the convolution of
sequences of ones of lengths
N = 1000 to 10,000. The CPU
time used by conv is divided
by 10.
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
×104
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Length of Convolution Sum
CPU Time (sec)
fft time
conv time /10

722
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
M = 2 ∗NN-1;
t0 = cputime;
y = conv(x,x); % convolution using conv
time1(i) = cputime-t0;
t1 = cputime;
X = fft(x,M); X = fft(x,M); Y = X. ∗X; y1 = ifft(Y); % convolution using fft
time2(i) = cputime-t1
sum(y-y1) % check conv and fft results coincide
pause % check for small difference
end
I
Gauss and the FFT
Going back to the sources used by the FFT researchers it was discovered that many well-known mathematicians had
developed similar algorithms for different values of N. But that an algorithm similar to the modern FFT had been developed
and used by Carl Gauss, the German mathematician, probably in 1805, predating even Fourier’s work on harmonic analysis
in 1807, was an interesting discovery—although not surprising [31]. Gauss has been called the “prince of mathematicians”
for his prodigious work in so many areas of mathematics, and for the dedication to his work. His motto was Pauca sed matura
(few, but ripe); he would not disclose any of his work until he was very satisﬁed with it. Moreover, as it was customary in
his time, his treatises were written in Latin using a difﬁcult mathematical notation, which made his results not known or
understood by modern researchers. Gauss’s treatise describing the algorithm was not published in his lifetime, but appeared
later in his collected works. He, however, deserves the paternity of the FFT algorithm.
The developments leading to the FFT, as indicated by Cooley [14], point out two important concepts in numerical analysis
(the ﬁrst of which applies to research in other areas): (1) the divide-and-conquer approach—that is, it pays to break a
problem into smaller pieces of the same structure; and (2) the asymptotic behavior of the number of operations. Cooley’s
ﬁnal recommendations in his paper are worth serious consideration by researchers in technical areas:
I
Prompt publication of signiﬁcant achievements is essential.
I
Review of old literature can be rewarding.
I
Communication among mathematicians, numerical analysts, and workers in a wide range of applications can be
fruitful.
I
Do not publish papers in neoclassic Latin.
12.3 APPLICATION TO SAMPLED-DATA AND DIGITAL CONTROL
SYSTEMS
Most control systems being used today use computers and ADCs and DACs. Control systems where
continuous- and discrete-time signals appear are called sample-data systems. The analysis of these
systems is more complicated than that of either continuous- or discrete-time systems, given the mixed
signals in the system. In the following analysis we will ignore the effect of the quantizer and the coder,
so that we are not considering digital control systems, but rather sampled-data or discrete control
systems. Understanding the effects of sampling and the conversion of signals from continuous to

12.3 Application to Sampled-Data and Digital Control Systems
723
FIGURE 12.5
Digital implementation of a
continuous-feedback system. ZOH
stands for zero-order hold.
ADC
Computer
DAC
ZOH
Plant
Clock
c(t)
e(t)
y(t)
+
−
Controller
discrete and back from discrete to continuous is very important in obtaining a discrete-time system
from a sampled-data control system.
Consider the relation between a continuous control system and its implementation using a computer
as shown in Figure 12.5. In the continuous-feedback system, the controller responds to an error signal
e(t), which is the difference between a reference input signal c(t) and the system output y(t), attempt-
ing to change the dynamics of an analog plant. A digital realization of this continuous-feedback
system typically requires that the error signal be converted into a digital signal by means of an ADC
before being fed to a computer implementing the controller (e.g., a PID controller). A DAC with a
zero-order hold that is synchronously connected and has the same sampling period as the previous
ADC is used to generate a signal that will act on the plant. The output of the plant y(t) is fed back and
compared with the command signal c(t) to obtain the error e(t). The digital controller is composed
of the ADC, the computer, and the DAC with the zero-order hold, all of which are synchronized by a
common clock.
Why are sampled-data and digital control systems needed? In part, because many systems are inher-
ently discrete—for example, a radar tracking system scans to convert azimuth and elevation into
sampled data. But in general we have that:
I
A continuous control system operates in real time, and the amplitude of its signals are allowed to
take any possible value, but its elements are susceptible to degradation with time, and the system
is sensitive to noise and difﬁcult to change since it is hardwired.
I
Digital components are less susceptible to aging, environmental variations, and noise. A digital
controller can be modiﬁed easily by changing software without changing the hardware. However,
computational speed and resolution (word length) are limitations of digital controllers that can
cause instabilities.
Remarks
I
In the following development you need to remember:
1.
The Laplace transform of an ideally sampled signal,
xs(t) =
X
n
x(nTs)δ(t −nTs)

724
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
where x(nTs) = x(t)|t=nTs are the sample values of the continuous-time signal x(t),
Xs(s) = L[xs(t)] =
X
n
x(nTs)e−snTs
2.
The Laplace transform Xs(s) is related to the Z-transform of the discrete-time signal x(nTs) by letting
z = esTs.
I
Recall that the ideal sampler is a time-varying system and that the quantizer is a nonlinear system;
thus sampled-data and digital control systems are time varying and time-varying nonlinear, respectively.
Therefore, the complexity of their analyses.
12.3.1 Open-Loop Sampled-Data System
Consider the system shown in Figure 12.6. Assume the discrete-time signal x(nTs) coming from a
computer is used to drive an analog plant with a transfer function G(s). To change the state of the
plant, x(nTs) is converted into a continuous signal that holds the value of the sample for the duration
of the sample period Ts. This can be implemented using a DAC with a zero-order hold (ZOH), which
holds the value of x(nTs) until the next sample arrives at (n + 1)Ts. Furthermore, to allow the output
signal to be possibly processed by a computer, assume the output of the plant y(t) is also sampled
to get y(nTs). We are interested in the transfer function that relates the discrete input x(nTs) to the
discrete output y(nTs) where Ts is the sampling period chosen according to the maximum frequency
present in the analog input x(t).
As we saw in Chapter 7, the transfer function of a zero-order hold is
Hzoh(s) = 1 −e−sTs
s
(12.11)
which corresponds to an impulse response
hzoh(t) = u(t) −u(t −Ts)
(12.12)
or a pulse of duration Ts and unit amplitude. If the sampled signal is written as
xs(t) =
X
n
x(nTs)δ(t −nTs)
(12.13)
FIGURE 12.6
Open-loop sampled-data system for an
analog plant G(s). The output of the DAC
with a ZOH is illustrated.
DAC
ZOH
Plant
G(s)
ADC
y(nTs)
x(nTs)
y(t)
x(t)
t
ZOH signal
Ts
0
2Ts
3Ts
4Ts 5Ts 6Ts

12.3 Application to Sampled-Data and Digital Control Systems
725
FIGURE 12.7
Equivalent discrete-time system of the open-loop sampled-data
system.
y(nTs)
x(nTs)
F(z)= Y(z)
X(z)
(i.e., a sequence of impulses at times {nTs} with amplitude the sampled values x(nTs)), then the output
of the DAC with ZOH is
v(t) = [xs ∗hzoh](t) =
X
n
x(nTs)hzoh(t −nTs)
(12.14)
or a piecewise constant signal (see Figure 12.6). Putting together the transfer function of the ZOH
with that of the plant so that F(s) = Hzoh(s)G(s), we have that Y(s) = F(s)Xs(s).
If we let f(t) = L−1[F(s)], then the output of the plant is given by the convolution integral as
y(t) = [xs ∗f](t) =
X
n
x(nTs)[δ ∗f](t −nTs) =
X
n
x(nTs)f(t −nTs)
which is the convolution sum of the discrete input and the sampled-impulse response of the plant
combined with that of the ZOH. For Y(z) = Z[y(nTs)] and X(z) = Z[x(nTs)], we have that when we
sample y(t), then
y(kTs) = y(t)|t=kTs =
X
n
x(nTs)f(kTs −nTs)
The transfer function of the discrete system is
F(z) = Z[ f(nTs)] = Y(z)
X(z)
(12.15)
which can be obtained by sampling the inverse Laplace transform f(t) = L−1[F(s)] and then comput-
ing its Z-transform. We have thus obtained the equivalent discrete-time system to the sampled-data
system shown in Figure 12.7.
I Example 12.5
Consider the open-loop sampled-data system shown in Figure 12.6, where the DAC with ZOH is
synchronized with an ADC, which is just an ideal sampler. Let Ts = 1 sec/sample be the sampling
period. If the transfer function of the plant is
G(s) =
1
(s + 1)(s + 2)
ﬁnd the transfer function F(z) = Y(z)/X(z).

726
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
Solution
The combined transfer function of the ZOH and the plant is
F(s) = G(s)(1 −e−s)
s
= G(s)
s
−G(s)e−s
s
so that if we ﬁnd the inverse Laplace transform of ˆG(s) = G(s)/s, call it ˆg(t), then
f(t) = ˆg(t) −ˆg(t −1)
The inverse of ˆG(s) = G(s)/s is obtained by partial fraction expansion
ˆG(s) = G(s)
s
=
1
s(s + 1)(s + 2) = A
s +
B
s + 1 +
C
s + 2
= 0.5
s
−
1
s + 1 + 0.5
s + 2
so that
ˆg(t) = [0.5 −e−t + 0.5e−2t]u(t)
Sampling f(t) = ˆg(t) −ˆg(t −1) with a sampling period Ts = 1 gives
f(n) = ˆg(n) −ˆg(n −1)
where ˆg(n) = [0.5 −e−n + 0.5e−2n]u(n). The Z-transform of f(n) is then the transfer function
F(z) = Y(z)
X(z) = ˆG(z)(1 −z−1)
= (1 −z−1)

0.5
1 −z−1 −
1
1 −e−1z−1 +
0.5
1 −e−2z−1

= 0.5 −(1 −z−1)
1 −e−1z−1 + 0.5(1 −z−1)
1 −e−2z−1
I
12.3.2 Closed-Loop Sampled-Data System
Consider the feedback system shown in Figure 12.8 where for simplicity we assume H(s) = 1 (i.e., no
feedback sensor). An equivalent block diagram is obtained by moving back the sampler at the input.
Consider ﬁnding the transfer function of the sampled input command cs(t) and the sampled output
of the system ys(t). The above open-loop development can be used to ﬁnd the transfer function of the
feedback system.
The sampled error signal is
es(t) = cs(t) −ys(t)

12.3 Application to Sampled-Data and Digital Control Systems
727
FIGURE 12.8
Closed-loop sampled-data control system.
Plant
ADC
Sampler
Computer
DAC
ZOH
c(t)
y(t)
e(t)
es(t)
Ps(s)
H(s)
G(s)
+
−
Sensor
1−e −sTs
s
with Laplace transform
Es(s) = Cs(s) −Ys(s)
(12.16)
The function Ps(s) corresponds to the discretization of an analog controller, such as a PID controller.
The Laplace transform of the output of the computer is then
Ms(s) = Ps(s)Es(s) =
X
n
m(nTs)e−snTs
(12.17)
or the Laplace transform of a sampled signal. On the other hand, the DAC with ZOH and the plant
have together a transfer function
ˆG(s) = (1 −e−sTs)G(s)
s
Thus, the Laplace transform of the output of the plant is
Ms(s) ˆG(s) =
X
n
m(nTs)
h
ˆG(s)e−snTs
i
(12.18)
Using the time-shifting property, the inverse Laplace transform of the above equation is
X
n
m(nTs)ˆg(t −nTs)
which when sampled at t = kTs gives the convolution sum
X
n
m(nTs)ˆg(kTs −nTs) = ys(nTs)
(12.19)
so that M(z) ˆG(z) = Y(z).
Letting
z = esTs
in
Equation
(12.16),
we
obtain
E(z) = C(z) −Y(z),
and
replacing
it
in
Equation (12.17) gives
M(z) = P(z)E(z) = P(z)[C(z) −Y(z)]

728
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
We thus have
P(z)[C(z) −Y(z)]
|
{z
}
M(z)
ˆG(z) = Y(z)
from which we obtain
Y(z) = P(z)C(z) ˆG(z)
1 + P(z) ˆG(z)
Calling F(z) = P(z) ˆG(z) (the feed-forward transfer function consisting of the discretized analog
controller and the ZOH and the plant), we get
Y(z)
C(z) =
F(z)
1 + F(z)
(12.20)
or the transfer function of the data-sampled system. Notice that this equation looks like the equation
of a continuous-feedback system.
Remarks
I
In the equivalent discrete-time system obtained above, the information of the output of the open-loop or
the closed-loop systems in between the sampling instants is not available; only the samples y(nTs) are. This
is also indicated by the use of the Z-transform.
I
Depending on the location of the sampler, there are some sampled-data control systems for which we
cannot ﬁnd a transfer function. This is due to the time-variant nature of the system.
I Example 12.6
Suppose we wish to have a data-sampled system like the one shown in Figure 12.8 that simulates
the effects of an integral analog controller. Let the plant be a ﬁrst-order system,
G(s) =
1
s + 1
Let the sampling period be Ts = 1. Determine P(z) and ﬁnd the discrete transfer function of the
sampled-data system when H(s) = 1.
Solution
If e(t) is the input of an integrator and v(t) its output, letting t = nTs and approximating the integral
by a sum we have that
v(nTs) =
n
X
k=0
e(kTs)Ts =
n−1
X
k=0
e(kTs)Ts + e(nTs)Ts
= v(nTs −Ts) + Tse(nTs)

12.4 Application to Digital Communications
729
After replacing Ts = 1 it becomes v[n] = v[n −1] + e[n], so the transfer function of the
integrator is
P(z) = V(z)
E(z) =
1
1 −z−1
The transfer function of the DAC with ZOH and the plant is
ˆG(s) = (1 −e−sTs)G(s)
s
= (1 −e−s)
s(s + 1)
If we let D(s) = 1/s(s + 1), then ˆg(t) = d(t) −d(t −1). Expanding D(s) as
D(s) =
1
s(s + 1) = A
s +
B
s + 1 = 1
s −
1
s + 1
so that d(t) = u(t) −e−tu(t) sampled, gives
d(nTs) = u(nTs) −e−nTsu(nTs)
which has a Z-transform
D(z) =
1
1 −z−1 −
1
1 −e−1z−1
⇒
ˆG(z) = (1 −z−1)D(z)
The transfer function is then
Y(z)
C(z) =
P(z)(1 −z−1)D(z)
1 + P(z)(1 −z−1)D(z)
=
D(z)
1 + D(z)
since P(z)(1 −z−1) = 1.
I
12.4 APPLICATION TO DIGITAL COMMUNICATIONS
Although over the years the principles of communications have remained the same, their implemen-
tation has changed considerably. Analog communications transitioned into digital communications,
while telephony and radio have coalesced into wireless communications. The scarcity of radio spec-
trum changed the original focus on bandwidth and energy efﬁciency into more efﬁcient utilization
of the available spectrum by sharing it, and by transmitting different types of data together. Wireless
communications has allowed the growth of cellular telephony, personal communication systems,
and wireless local area networks.
Modern digital communications was initiated with the concept of pulse code modulation, which
allowed the transmission of binary signals. PCM is a practical implementation of sampling, quantiza-
tion, and coding, or analog-to-digital conversion, of an analog message into a digital message. Using
the sample representation of a message, the idea of mixing several messages—possibly of different

730
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
types—developed into time-division multiplexing (TDM), which is the dual of frequency-division
multiplexing (FDM). In TDM, samples from different messages are interspersed into one message
that can be quantized, coded, and transmitted and then separated into the different messages at the
receiver.
As multiplexing techniques, FDM and TDM became the basis for similar techniques used in wireless
communications. Typically, three forms of sharing the available radio spectrum are: frequency-
division multiple access (FDMA) where each user is assigned a band of frequencies all of the time;
time-division multiple access (TDMA) where a user is given access to the available bandwidth for
a limited time; and code-division multiple axis (CDMA) where users share the available spectrum
without interfering with each other thanks to a unique code given to each user.
In this section we will introduce some of these techniques avoiding technical details, which we leave
to excellent books in communications, telecommunications, and wireless communications. As you
will learn, digital communications has a number of advantages over analog communications:
I
The cost of digital circuits decreases as digital technology becomes more prevalent.
I
Data from voice and video can be merged with computer data and transmitted over a common
transmission system.
I
Digital signals can be denoised easier than analog signals, and errors in digital communications
can be corrected using special coding.
However, digital communication systems require a much larger bandwidth than analog communica-
tion systems, and quantization noise is introduced when transforming an analog signal into a digital
signal.
12.4.1 Pulse Code Modulation
PCM can be seen as an implementation of analog-to-digital conversion of analog signals providing
a serial bit stream. This means that sampling applied to a continuous-time message gives a pulse
amplitude–modulated (PAM) signal that is then quantized and assigned a binary code to differentiate
the different quantization levels. If each of the digital words has b binary digits, there are 2b levels,
and to each a different code word is assigned. An example of a code commonly used is the Gray code
where only one bit changes from one quantization level to another. The most signiﬁcant bit can be
thought to correspond to the sign of the signal (1 for positive values, and 0 for negative values) and
the others differentiate each level.
PCM is widely used in digital communications given that:
I
It is realized with inexpensive digital circuitry.
I
It allows merging and transmission of data from different sources (audio, video, computers, etc.)
by means of time-division multiplexing, which we will see next.
I
PCM signals can be easily regenerated by repeaters in long-distance digital telephony.
Despite all of these advantages, you need to remember that because of the sampling and the quan-
tization the process used to obtain PCM signals is neither linear nor time invariant, and as such

12.4 Application to Digital Communications
731
FIGURE 12.9
PCM system: transmitter, channel,
and receiver.
LPF
Sample
and hold
Quantizer
and coder 
Repeater
Decoder
LPF
PCM transmitter
Repeater
Repeater
PCM receiver
Channel 
m(t)
···1011100···
···1011100···
···1011100···
···1011100···
m(t)
^
its analysis is complicated. Figure 12.9 shows a transmitter, a channel, and a receiver of a PCM
system.
The main disadvantage of PCM is that its bandwidth is wider than that of the analog message
it represents. This is due to the rectangular pulses in the signal. If we represent the PCM signal
s(t) as
s(t) =
N−1
X
n=0
anϕ(t −nτs)
where ϕ(t) is a function and τs is the duration of a symbol, the spectrum of s(t) will be
S() =
N
X
n=0
anF[ϕ(t −nτs)]
=
N
X
n=0
anφ()e−jnτs
Suppose that the function is a sinc function having an inﬁnite time support so that s(t) also has an
inﬁnite time support. If
ϕ(t) = sin(πt/τs)
πt
it has a band-limited spectrum,
φ() = u( + π/τs) −u( −π/τs)
so that the spectrum of the signal is also band limited. On the other hand, by duality if we use
a rectangular pulse as the function ϕ(t), its spectrum will spread over all frequencies, making the
spectrum of the signal of very large bandwidth.

732
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
I Example 12.7
Suppose you have a binary signal 01001001, with a duration of 8 units of time, and wish to
represent it using rectangular pulses and sinc functions. Consider the bandwidth of each of the
representations.
Solution
Using pulses ϕ(t), the digital signal can be expressed as
s(t) =
7
X
n=0
bnϕ(t −nτs)
where bn are the binary digits of the digital signal (i.e., b0 = 0, b1 = 1, b2 = 0, b3 = 0, b4 = 1,
b5 = 0, b6 = 0, b7 = 1, and τs = 1). Thus,
s(t) = ϕ(t −1) + ϕ(t −4) + ϕ(t −7)
and the spectrum of s(t) is
S() = φ()(e−j + e−j4 + e−j7)
= φ()e−j4(ej3 + 1 + e−j3)
= φ()e−j4(1 + 2 cos(3))
so that
|S()| = |φ()||1 + 2 cos(3)|
If the pulses are rectangular,
ϕ(t) = u(t) −u(t −1)
the PCM signal will have an inﬁnite-support spectrum because the pulse is of ﬁnite support. On
the other hand, if we use sinc functions,
ϕ(t) = sin(πt/τs)
πt
its time support is inﬁnite but its frequency support is ﬁnite (i.e., the sinc function is band limited).
In which case, the spectrum of the PCM signal is also of ﬁnite support.
If this digital signal is transmitted and received without any distortion, at the receiver we can use
the orthogonality of the ϕ(t) signals or sample the received signal at nTs to obtain the bn. Clearly,
each of these pulses has disadvantages—the advantage of having a ﬁnite support in the time or in
the frequency becomes a disadvantage in the other domain.
I

12.4 Application to Digital Communications
733
Baseband and Band-Pass Communication Systems
A baseband signal can be transmitted over a pair of wires (like in a telephone), coaxial cables, or
optical ﬁbers. But a baseband signal cannot be transmitted over a radio link or a satellite because this
would require a large antenna to radiate the low-frequency spectrum of the signal. Thus, the signal
spectrum must be shifted to a higher frequency by modulating a carrier by the baseband signal.
This can be done by amplitude and by angle modulation (frequency and phase). Several forms are
possible.
I Example 12.8
Suppose the binary signal 01001101 is to be transmitted over a radio link using AM and FM
modulation. Discuss the different band-pass signals obtained.
Solution
The binary message can be represented as a sequence of pulses with different amplitudes. For
instance, we could represent the binary digit 1 by a pulse of constant amplitude, and the binary
0 is represented by switching off the pulse (see the corresponding modulating signal m1(t) in
Figures 12.10(a) and 12.10(b)). Another possible representation would be to represent the binary
digit 1 with a positive pulse of constant amplitude, and 0 with the negative of the pulse used for 1
(see the corresponding modulating signal m2(t) in Figures 12.10(c) and 12.10(d)).
In AM modulation, if we use m1(t) to modulate a sinusoidal carrier cos(0t) we obtain the
amplitude-shift keying (ASK) signal shown in Figure 12.10(a). On the other hand, when using
m2(t) to modulate the same carrier we obtain a phase-shift keying (PSK) signal shown in
Figure 12.10(c). In this case, the phase of the carrier is shifted 180o as the pulse changes from
positive to negative.
Using FM modulation, the symbol 0 is transmitted using a windowed cosine of some frequency
c0 and the symbol 1 is transmitted with a windowed cosine of frequency c1 resulting in
frequency-shift keying (FSK). The data are transmitted by varying the frequency. In this case it
is possible to get the same modulated signals for both m1(t) and m2(t). The modulated signals are
shown in Figure 12.10(b and d).
The ASK, PSK, and FSK are also known as BASK, BPSK, and BFSK, respectively, by adding the word
“binary” (B) to the corresponding amplitude-, phase-, and frequency-shift keying.
I
12.4.2 Time-Division Multiplexing
In a telephone system, multiplexing enables multiple conversations to be carried across a single
shared circuit. The ﬁrst multiplexing system used was frequency-division multiplexing (FDM), which
we covered in Chapter 6. In FDM an available bandwidth is divided among different users. In the
case of voice communications, each user is allocated a bandwidth of 4 KHz, which provides good
ﬁdelity. In FDM, a user could use the allocated frequencies all of the time, but the user could not go
outside the allocated band of frequencies.

734
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
0
1
2
3
4
5
6
7
−1
−0.5
0
0.5
1
(a)
(c)
(d)
(b)
t (sec)
m1(t), x1(t)
0
1
2
3
4
5
6
7
−1
−0.5
0
0.5
1
t (sec)
m2(t), x2(t)
m1(t), y1(t)
m2(t), y2(t)
0
1
2
3
4
5
6
7
−1
−0.5
0
0.5
1
t (sec)
0
1
2
3
4
5
6
7
−1
−0.5
0
0.5
1
t (sec)
FIGURE 12.10
Pulse signals (continuous line) m1(t) and m2(t) corresponding to binary sequence 01001101. (a, c), ASK signals
x1(t) = m1(t) cos(4πt) and x2(t) = m2(t) cos(4πt) in dashed lines. (b, d) In dashed lines the FSK signals y1(t)
and y2(t) equal cos(4πt) when m1(t), m2(t) are 1 and cos(8πt) when m2(t) is 0 or −1.

12.4 Application to Digital Communications
735
FIGURE 12.11
TDM system: transmitter,
channel, and receiver.
Quantizer
& encoder
Channel
Decoder
Commutator
Decommutator
m1(t)
m2(t)
m3(t)
m1(t)
∧
m2(t)
∧
m3(t)
∧
Pulse-modulated signals have large bandwidths, and as such, when transmitted together they overlap
in frequency, interfering with each other. However, these signals only provide information at each
of the sampling times, so that one could insert in between these times other samples that will be
separated at the receiver. This is the principle of time-division multiplexing (TDM), where pulses from
different signals are interspersed into one signal and converted into a PCM signal and transmitted. See
Figure 12.11. At the receiver, the signal is changed back into the pulse-modulated signal and separated
into the number of signals interspersed at the input. Repeaters placed between the transmitter and
the receiver regenerate a clean binary signal from a noisy binary signal along the way. The noisy signal
coming into the repeater is thresholded to known binary levels and resent. A large part of the cost of
a transmission facility is due to these repeaters that are placed about every mile along the line.
TDM allows the transmission of different types of data, and mixture of analog and digital using
different multiplexing techniques. Not to lose information, the switch at the receiver needs to be
synchronized with the switch at the transmitter. Frame synchronization consists in sending a synchro-
nizing signal for each frame. An additional channel is allocated for this purpose. To accommodate
more users, the width of the pulses used for each user needs to become narrower, which increases the
overall bandwidth of the multiplexed signal.
12.4.3 Spread Spectrum and Orthogonal Frequency-Division
Multiplexing
The objective of TDM is to put several users or different types of data together sharing the same
bandwidth at different times. Likewise, FDM users share part of the available bandwidth all the time.
TDM and FDM are examples of how to use bandwidth in an efﬁcient way. In other situations, like
in quadrature-amplitude modulation (QAM), the objective is to send two messages over the same
bandwidth using the orthogonality of the carriers to recover them. In spread spectrum, the objective is
to use the orthogonality of the carriers associated with different users to share the available spectrum,
while spreading the message in frequency so that it occupies a bandwidth much larger than that of the
message. On the other hand, orthogonal frequency-division multiplexing (OFDM) is a multicarrier
system where the carriers are orthogonal.
Sharing the radio spectrum among users, or multiple access, is a basic strategy of wireless commu-
nication systems. Basic modalities are derived from FDM, TDM, and spread spectrum. In FDMA the
spectrum is shared by assigning speciﬁc channels to users, permanently or temporarily. TDMA allows
access to all of the available spectrum, but each user is assigned a time interval in which to access
it. CDMA uses spread spectrum, where a user’s message is spread or encrypted over the available

736
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
spectrum using a code to differentiate the different users. The objective of these three techniques is
to maximize the radio spectrum utilization.
Spread Spectrum—A Famous Actress Idea
Not surprisingly, the ﬁrst mention of the use of frequency hopping, a form of spread spectrum, for secure communications
came from a patent by Nikolas Tesla in 1903. As you recall, Tesla is the world-renowned Serbian-American inventor, and
physicist, and mechanical and electrical engineer who pioneered amplitude modulation.
The most celebrated invention of frequency-hopping was, however, that of Hedy Lamarr and George Antheil, who in 1942
received a U.S. patent for their “secret communications system,” in which they used a piano-roll for frequency-hopping.
This was during World War II, and their idea was to stop the enemy from detecting or jamming radio-guided torpedoes.
To avoid the jammer, in frequency-hopping spread spectrum the transmitter changes in a quasi-random way the center
frequency of the transmitted signal. Hedy Lamarr (1913–2000) was an Austrian-American actress and communications
technology innovator, while George Antheil (1900–1959) was an American composer and pianist. Their patent was never
applied, and it would be many years before the technology was actually deployed. Ms. Lamarr conceived the idea of hopping
from frequency to frequency just as a piano player plays the same notes, but in different octaves. Their concept eventually
provided the basis for the CDMA airlink, which Qualcomm commercialized in 1995. Today, CDMA and its core principles
provide the backbone for wireless communications, thanks to the creative vision of an extraordinary woman [70, 74, 62].
Spread Spectrum
A spread-spectrum system is one in which the transmitted signal is spread over a wide frequency
band, much wider than the bandwidth required to transmit the message. Such a system would take a
baseband voice signal with a bandwidth of a few kilohertz and spread it to a band of many megahertz.
Two types of spread-spectrum systems are:
I
Direct-sequence system: A digital code sequence with a bit rate higher than the message is used to
obtain the modulated signal.
I
Frequency-hopping system: The carrier frequency is shifted in discrete increments in a pattern
dictated by a code sequence. We will not consider this here.
Direct-Sequence Spread-Spectrum
Suppose the message m(t) we wish to transmit is a polar binary signal, and that a spreading code c(t),
also in polar binary form, is modulated by the message to obtain the modulated baseband signal
x(t) = m(t)c(t)
(12.21)
The sequence c(t) is pseudo-random, unpredictable to an outsider, but that can be generated deter-
ministically. Each user is assigned uniquely one of these sequences—that is, the spreading codes
assigned to two users are not related at all. Moreover, the bit rate of c(t) is much higher than that of
the message. As in many other modulation systems, the modulated baseband signal x(t) has a much
higher rate than the message, and as such its spectrum is much wider than that of the message that is
already wide given that it is a sequence of pulses. This can also be seen by considering that x(t) as the
product of m(t), and c(t), its spectrum is the convolution of the spectrum of m(t) with the spectrum
of c(t) with a bandwidth equal to the sum of the bandwidths of these spectra.

12.4 Application to Digital Communications
737
When transmitting over a radio link the baseband signal x(t) modulates an analog carrier to obtain
the transmitting signal s(t). At the receiver, if no interference occurred in the transmission, the received
signal r(t) = s(t), and after demodulation using the analog carrier frequency, the spread signal x(t) is
obtained. If we multiply it by c(t) we get
x(t)c(t) = c2(t)m(t) = m(t)
(12.22)
since c2(t) = 1 for all t. See Figure 12.12.
Two signiﬁcant advantages of direct-sequence spread spectrum are:
I
Robustness to noise and jammers: The above detection or despreading is idealized. The received
signal will have interferences due to channel noise, interference from other users, and even, in
military applications, intentional jamming. Jamming attempts to corrupt the sent message by
adding to it either a narrowband or a wideband signal. If at the receiver, the spread signal con-
tains additive noise η(t) and a jammer j(t), it is demodulated by the BPSK system. The received
baseband signal is
ˆr(t) = x(t) + ˆη(t) + ˆj(t)
(12.23)
where the noise and the jammer have been affected by the demodulator.
Multiplying it by c(t) gives
ˆr(t)c(t) = m(t) + ˆη(t)c(t) + ˆj(t)c(t)
(12.24)
or the desired message with a spread noise and jammer. Thus, the transmitted signal is resistant
to interferences by spreading them over all frequencies.
FIGURE 12.12
Direct-sequence
spread-spectrum system.
Spreader
BPSK Modulator
×
LPF
×
×
×
BPSK Demodulator
Despreader
m(t)
A cos(Ωct)
A cos(Ωct)
c(t)
r(t)=s(t)+η(t) + j(t)
c(t)
s(t)
m(t)
∧

738
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
I
Robustness to interference from other users: Assuming no noise or jammer, if the received baseband
signal comes from two users—that is,
ˆr(t) = m1(t)c1(t) + m2(t)c2(t)
(12.25)
where the codes c1(t) and c2(t) are the corresponding codes for the two users, and m1(t) and m2(t)
their messages. At the receiver of user 1, despreading using code c1(t) we get
ˆr(t)c1(t) = m1(t)c2
1(t) + m2(t)c2(t)c1(t) ≈m1(t)
(12.26)
since the codes are generated so that c2
1(t) = 1 and c1(t) and c2(t) are not correlated. Thus, we
detect the message corresponding to user 1. The same happens when there is interference from
more than one user.
Simulation of direct sequence spread spectrum. In this simulation we consider that the mes-
sage is randomly generated and that the spreading code is also randomly generated (our code does
not have the same characteristics as the one used to generate the code for spread-spectrum systems).
To generate the train of pulses for the message and the code we use ﬁlters of different length (recall the
spreading code changes more frequently than the message). The spreading makes the transmitting
signal have a wider spectrum than that of the message (see Figure 12.13).
The binary transmitting signal modulates a sinusoidal carrier of frequency 100 Hz. Assuming the
communication channel does not change the transmitted signal and perfect synchronization at the
analog receiver is possible, the despread signal coincides with the sent message. In practice, the effects
of multipath in the channel, noise, and possible jamming would not make this possible.
%%%%%%%%%%%%%%%%
% Simulation of
% spread spectrum
%%%%%%%%%%%%%%%%
clear all; clf
% message
m1 = rand(1,12)>0.9;m1 = (m1-0.5) ∗2;
m = zeros(1,00);
m(1:9:100) = m1
h = ones(1,9);
m = ﬁlter(h,1,m);
% spreading code
c1 = rand(1,25)>0.5;c1 = (c1-0.5) ∗2;
c = zeros(1,100);
c(1:4:100) = c1;
h1 = ones(1,4);
c = ﬁlter(h1,1,c);
Ts = 0.0001; t = [0:99] ∗Ts;
s = m. ∗c;
ﬁgure(1)

12.4 Application to Digital Communications
739
0
1
2
3
4
5
6
7
8
9
−1
0
1
m(t)
Message
0
1
2
3
4
5
6
7
8
9
−1
0
1
c(t)
Code
0
1
2
3
4
5
6
7
8
9
−1
0
1
s(t)
t (sec)
Spread Message
−5000 −4000 −3000 −2000 −1000
0
1000
2000
3000
4000
0
20
40
60
|M (f )|
Message Spectrum
−5000 −4000 −3000 −2000 −1000
0
1000
2000
3000
4000
0
10
20
30
|S(f )|
f (Hz)
Spread Signal Spectrum
0
1
2
3
4
5
6
7
8
9
−1
0
1
0
1
2
3
4
5
6
7
8
9
−1
0
1
r(t)
0
1
2
3
4
5
6
7
8
9
−1
0
1
0
1
2
3
4
5
6
7
8
9
−1
0
1
t (sec)
×10 3
×10 3
×10 3
×10 3
×10 3
×10 3
×10 3
sa(t)
ma(t)
m1(t)
(a)
(b)
(c)
FIGURE 12.13
Simulation of direct-sequence spread-spectrum communication. (a) Displays from top to bottom the message,
the code, and the spread signal. (b) Displays the spectrum of the message and of the spread signal (notice it is
wider than that of the message). (c) Displays the band-pass signals sent and received (assumed equal), the
despread analog, and the binary message.
subplot(311)
bar(t,m); axis([0 max(t) -1.2 1.2]);grid; ylabel(‘m(t)’)
subplot(312)
bar(t,c); axis([0 max(t) -1.2 1.2]);grid; ylabel(‘c(t)’)
subplot(313)
bar(t,s); axis([0 max(t) -1.2 1.2]);grid; ylabel(‘s(t)’); xlabel(‘t (sec)’)
% spectrum of message and spread signal
M = fftshift(abs(fft(m)));
S = fftshift(abs(fft(s)));
N = length(M);K = [0:N-1];w = 2 ∗K ∗pi/N-pi; f = w/(2 ∗pi ∗Ts);
ﬁgure(2)
subplot(211)

740
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
plot(f,M);grid; axis([min(f) max(f) 0 1.1 ∗max(M)]); ylabel(‘—M(f)—’)
subplot(212)
plot(f,S); grid; axis([min(f) max(f) 0 1.1 ∗max(S)]);ylabel(‘—S(f)—’); xlabel(‘f (Hz)’)
% analog modulation and demodulation
s = s. ∗cos(200 ∗pi ∗t);
r = s. ∗cos(200 ∗pi ∗t);
% despreading
mm = r. ∗c;
for k = 1:length(mm);
if mm(k) > 0
m2(k) = 1;
else
m2(k) = -1;
end
end
ﬁgure(3)
subplot(411)
plot(t,s); axis([0 max(t) 1.1 ∗min(s) 1.1 ∗max(s)]);grid; ylabel(‘s a(t)’)
subplot(412)
plot(t,r); axis([0 max(t) 1.1 ∗min(r) 1.1 ∗max(r)]);grid; ylabel(‘r(t)’)
subplot(413)
plot(t,mm); axis([0 max(t) 1.1 ∗min(mm) 1.1 ∗max(mm)]);grid; ylabel(‘m a(t)’)
subplot(414)
bar(t,m2); axis([0 max(t) -1.2 1.2]);axis([0 max(t) 1.1 ∗min(mm) 1.1 ∗max(mm)])
grid;ylabel(‘\m(t)’); xlabel(‘t (sec)’)
Orthogonal Frequency-Division Multiplexing
OFDM is a multicarrier modulation technique where the available bandwidth is divided into nar-
rowband subchannels. It is used for high data-rate transmission over mobile wireless channels
[27, 60, 4].
If {dk, k = 0, . . . , N −1} are symbols to be transmitted, the OFDM-modulated signal is
s(t) =
∞
X
m=−∞
N−1
X
k=0
dkej2πfktp(t −mT)
(12.27)
where T is the symbol duration, fk = f0 + k1f for a subchannel bandwidth 1f = 1/T with initial
frequency f0, and p(t) = u(t) −u(t −T). Thus, the carriers are conventional complex exponentials.
Considering a baseband transmission, at the receiver the orthogonality of these exponentials in
[0, T] allows us to recover the symbols. Indeed, assuming that no interference is introduced by the
transmission channel (i.e., the received signal r(t) = s(t)), multiplying r(t) by the conjugate of the
exponential carrier and smoothing the result we obtain for k = 0, . . . , N −1, and m ≤t ≤(m + 1)T

12.4 Application to Digital Communications
741
(where p(t −mT) = 1),
1
T
(m+1)T
Z
mT
r(t)e−j2πfktdt = 1
T
(m+1)T
Z
mT
N−1
X
ℓ=0
dℓej2πfℓte−j2πfktdt
=
N−1
X
ℓ=0
dℓ
1
T
(m+1)T
Z
mT
e−j2π(fk−fℓ)tdt
=
N−1
X
ℓ=0
dℓδ[k −ℓ] = dk
for any −∞< m < ∞, and where we let fk −fℓ= (k −ℓ)1f = (k −ℓ)/T.
OFDM Implementation with FFT
If the modulated signal s(t), 0 ≤t ≤T, in Equation (12.27) is sampled at t = nT/N, we obtain for a
frame the inverse DFT
s[n] =
N−1
X
k=0
dkej2πfknT/N =
N−1
X
k=0
dkej2πkn/N
0 ≤n ≤N −1
(12.28)
where 2πfkT/N = 2πk/N are the discrete frequencies in radians. At the receiver, with no interferences
present, the symbols {dk} are obtained by computing the DFT of the baseband received signal. Given
that the inverse and the direct DFT can be efﬁciently implemented by the FFT, the OFDM is a very efﬁ-
cient technique that is used in wireless local area networks (WLANs) and digital audio broadcasting
(DAB).
Figure 12.14 gives a general description of the transmitter and receiver in an OFDM system: The high-
speed data in binary form coming into the system are transformed from serial to parallel and fed into
an IFFT block giving as output the transmitting signal that is sent to the channel. The received signal
is then fed into an FFT block providing estimates of the sent symbols that are ﬁnally put in serial
form.
FIGURE 12.14
Discrete model of baseband OFDM. The
blocks S/P and P/S convert a serial into a
parallel stream and a parallel to serial,
respectively.
S/P
IFFT
FFT
P/S
d0
d1
Channe
{dk}
dN−1
s(n)
r(n)
d0
∧
{dk}
∧
dN−1
∧
d1
∧
. . .
. . .

742
CHAPTER 12:
Applications of Discrete-Time Signals and Systems
12.5 WHAT HAVE WE ACCOMPLISHED? WHERE DO WE
GO FROM HERE?
In this chapter we have seen how the theoretical results presented in the third part of the book relate
to digital signal processing, digital control, and digital communications. The Fast Fourier Transform
made possible the establishment and signiﬁcant growth of digital signal processing as a technical
area. The next step for you could be to get into more depth in the theory and applications of digital
signal processing, preferably including some theory of random variables and processes, toward statis-
tical signal processing, speech, and image processing. We have shown you also the connection of the
discrete-time signals and systems with digital control and communications. Deeper understanding of
these areas would be an interesting next step. You have come a long way, but there is more to learn.

APPENDIX
Useful Formulas
Trigonometric Relations
Reciprocal
csc (θ) =
1
sin(θ)
sec (θ) =
1
cos(θ)
cot (θ) =
1
tan(θ)
Pythagorean Identity
sin2(θ) + cos2(θ) = 1
Sum and Difference of Angles
sin(θ ± φ) = sin(θ) cos(φ) ± cos(θ) sin(φ)
sin(2θ) = 2 sin(θ) cos(θ)
cos(θ ± φ) = cos(θ) cos(φ) ∓sin(θ) sin(φ)
cos(2θ) = cos2(θ) −sin2(θ)
Multiple Angle
sin(nθ) = 2 sin((n −1)θ) cos(θ) −sin((n −2)θ)
cos(nθ) = 2 cos((n −1)θ) cos(θ) −cos((n −2)θ)
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00017-x
c⃝2011, Elsevier Inc. All rights reserved.
743

744
APPENDIX:
Useful Formulas
Products
sin(θ) sin(φ) = 1
2[cos(θ −φ) −cos(θ + φ)]
cos(θ) cos(φ) = 1
2[cos(θ −φ) + cos(θ + φ)]
sin(θ) cos(φ) = 1
2[sin(θ + φ) + sin(θ −φ)]
cos(θ) sin(φ) = 1
2[sin(θ + φ) −sin(θ −φ)]
Euler’s Identity
e jθ = cos(θ) + j sin(θ)
j =
√
−1
cos(θ) = e jθ + e−jθ
2
sin(θ) = e jθ −e−jθ
2j
tan(θ) = −j
"
e jθ −e−jθ
e jθ + e−jθ
#
Hyperbolic Trigonometry Relations
Hyperbolic cosine: cosh(α) = 1
2(eα + e−α)
Hyperbolic sine: sinh(α) = 1
2(eα −e−α)
cosh2(α) −sinh2(α) = 1
Calculus
Derivatives (u, v functions of x; α, β constants)
duv
dx = udv
dx + vdu
dx
dun
dx = nun−1 du
dx
Integrals
Z
φ(y)dx =
Z φ(y)
y′ dy, where y′ = dy
dx
Z
udv = uv −
Z
vdu
Z
xndx = xn+1
n + 1
n ̸= −1, integer

APPENDIX:
Useful Formulas
745
Z
x−1dx = log(x)
Z
eaxdx = eax
a
a ̸= 0
Z
xeaxdx = eax
a2 (ax −1)
Z
sin(ax)dx = −1
a cos(ax)
Z
cos(ax)dx = 1
a sin(ax)
Z sin(x)
x
dx =
∞
X
n=0
(−1)n
x2n+1
(2n + 1)(2n + 1)!
integral of sinc function
Z ∞
0
sin(x)
x
dx =
Z ∞
0
sin(x)
x
2
dx = π
2

Bibliography
[1] A. Antoniou. Digital Filters. New York: McGraw-Hill, 1979.
[2] E. T. Bell. Men of Mathematics. New York: Simon and Schuster, 1965.
[3] J. Belrose. Fessenden and the early history of radio science. http://www.ieee.ca/millennium/radio/radio radioscientist.
html, accessed 2010.
[4] J. Bingham. Multicarrier modulation for data transmission: An idea whose time has come. IEEE Communications
Magazine, May 1990: 5–14.
[5] N. K. Bose. Digital Filters. Salem, MA: Elsevier, 1985.
[6] J. L. Bourjaily. http://www-personal.umich.edu/∼jbourj/money.htm, accessed 2010.
[7] C. Boyer. A History of Mathematics. New York: Wiley, 1991.
[8] R. Bracewell. The Fourier Transform and Its Application. Boston: McGraw-Hill, 2000.
[9] M. Brain. How CDs work. http://electronics.howstuffworks.com/cd7.htm, accessed 2010.
[10] W. Briggs and V. Henson. The DFT. Philadelphia: Society for Industrial and Applied Mathematics, 1995.
[11] O. Brigham. The Fast Fourier Transform and Its Applications. Englewood Cliffs, NJ: Prentice-Hall, 1988.
[12] D. Cheng. Analysis of Linear Systems. Reading, MA: Addison-Wesley, 1959.
[13] L. Chua, C. Desoer, and E. Kuh. Linear and Nonlinear Circuits. New York: McGraw-Hill, 1987.
[14] J. Cooley. How the FFT gained acceptance. In A History of Scientiﬁc Computing, edited by S. Nash. New York:
Association for Computing Machinery, Inc. Press, 1990, pp. 133–140.
[15] J. W. Cooley and J. W. Tukey. An algorithm for the machine calculation of complex Fourier series. Mathematics of
Computation, 19, April 1965: 297.
[16] L. W. Couch. Digital and Analog Communication Systems. Upper Saddle River, NJ: Pearson/Prentice-Hall, 2007.
[17] E. Craig. Laplace and Fourier Transforms for Electrical Engineers. New York: Holt, Rinehart, and Winston, 1966.
[18] E. Cunningham. Digital Filtering. Boston: Houghton Mifﬂin, 1992.
[19] G. Danielson and C. Lanczos. Some improvements in practical Fourier analysis and their applications to X-ray
scattering from liquids. J. Franklin Institute, 1942: 365–380.
[20] C. Desoer and E. Kuh. Basic Circuit Theory. New York: McGraw-Hill, 1969.
[21] R. Dorf and R. Bishop. Modern Control Systems. Upper Saddle River, NJ: Prentice-Hall, 2005.
[22] G. Franklin, J. Powell, and M. Workman. Digital Control of Dynamic Systems. Reading, MA: Addison-Wesley, 1998.
746
Signals and Systems Using MATLAB®. DOI: 10.1016/B978-0-12-374716-7.00025-9
c⃝2011, Elsevier Inc. All rights reserved.

Bibliography
747
[23] G. Johnson. Claude Shannon, Mathematician, Dies at 84. New York Times, February 27, 2001.
[24] R. Gabel and R. Roberts. Signals and Linear Systems. New York: Wiley, 1987.
[25] S. Goldberg. Introduction to Difference Equations. New York: Dover, 1958.
[26] R. Graham, D. Knuth, and O. Patashnik. Concrete Mathematics: A Foundation for Computer Science. Reading, MA:
Addison-Wesley, 1994.
[27] S. Haykin and M. Moher. Communication Systems. Hoboken, NJ: Wiley, 2009.
[28] S. Haykin and M. Moher. Modern Wireless Communications. Upper Saddle River, NJ: Pearson/Prentice-Hall, 2005.
[29] S. Haykin and M. Moher. Introduction to Analog and Digital Communications. Hoboken, NJ: Wiley, 2007.
[30] S. Haykin and B. Van Veen. Signals and Systems. New York: Wiley, 2003.
[31] M. Heideman, D. Johnson, and S. Burrus. Gauss and the history of the FFT. IEEE Acoustics, Speech and Signal
Processing (ASSP) Magazine, vol. 1, pp. 14–21, Oct. 1984.
[32] K. Howell. Principles of Fourier Analysis. Boca Raton, FL: Chapman & Hall, CRC Press 2001.
[33] IEEE. Nyquist biography. http://www.ieee.org/web/aboutus/history center/biography/nyquist.html, accessed 2010.
[34] Intel. Microprocessor quick reference guide. http://www.intel.com/pressroom/kits/quickreffam.htm, accessed 2010.
[35] Intel. Moore’s law. http://www.intel.com/technology/mooreslaw/index.htm, accessed 2010.
[36] L. Jackson. Signals, Systems, and Transforms. Reading, MA: Addison-Wesley, 1991.
[37] E. I. Jury. Theory and Application of the Z-Transform Method. New York: Wiley, 1964.
[38] E. Kamen and B. Heck. Fundamentals of Signals and Systems. Upper Saddle River, NJ: Pearson/Prentice-Hall, 2007.
[39] R. Keyes. Moore’s law today. IEEE Circuits and Systems Magazine, 8, 2008: 53–54.
[40] B. P. Lathi. Modern Digital and Analog Communication Systems. New York: Oxford University Press, 1998.
[41] B. P. Lathi. Linear Systems and Signals. New York: Oxford University Press, 2002.
[42] E. Lee and P. Varaiya. Structure and Interpretation of Signals and Systems. Boston: Addison-Wesley, 2003.
[43] W. Lehr, F. Merino, and S. Gillet. Software radio: Implications for wireless services, industry structure, and public
policy. http://itc.mit.edu, accessed 2002.
[44] D. Luke. The origins of the sampling theorem. IEEE Communications Magazine, April 1999:106–108.
[45] M. Bellis. The invention of radio. http://inventors.about.com/od/rstartinventions/a/radio.htm, accessed 2010.
[46] D. McGillem and G. Cooper. Continuous and Discrete Signals and System Analysis. New York: Holt, Rinehart, and
Wiston, 1984.
[47] Z. A. Melzak. Companion to Concrete Mathematics. New York: Wiley, 1973.
[48] S. Mitra. Digital Signal Processing. New York: McGraw-Hill, 2006.
[49] C. Moler. Cleve’s Corner—The Origins of MATLAB. http://www.mathworks.com/company/newsletters/news notes/
clevescorner/dec04.html, accessed 2010.
[50] National. Op-amp history. http://www.analog.com/library/analogdialogue/.../39.../web chh ﬁnal.pdf, accessed 2008.
[51] F. Nebeker. Signal Processing—The Emergence of a Discipline, 1948 to 1998. New Brunswick, NJ: IEEE History Center,
1998. (This book was especially published for the 50th anniversary of the creation of the IEEE Signal Processing
Society in 1998).
[52] MIT news. MIT Professor Claude Shannon dies; was founder of digital communications. http://web.mit.edu/
newsofﬁce/2001/shannon.html, accessed 2009.
[53] K. Ogata. Modern Control Engineering. Upper Saddle River, NJ: Prentice-Hall, 1997.
[54] A. Oppenheim and R. Schafer. Digital Signal Processing. Englewood Cliffs, NJ: Prentice-Hall, 1975.

748
Bibliography
[55] A. Oppenheim and R. Schafer. Discrete-Time Signal Processing. Upper Saddle River, NJ: Prentice-Hall, 2010.
[56] A. Oppenheim and A. Willsky. Signals and Systems. Upper Saddle River, NJ: Prentice-Hall, 1997.
[57] A. Papoulis. Signal Analysis. New York: McGraw-Hill, 1977.
[58] PBS.org. Who invented radio? http://www.pbs.org/tesla/ll/ll whoradio.html, accessed 2010.
[59] C. Phillips, J. Parr, and E. Riskin. Signals, Systems and Transforms. Upper Saddle River, NJ: Pearson/Prentice-Hall,
2003.
[60] R. Prasad and S. Hara. Overview of multi-carrier CDMA. IEEE Communications Magazine, vol. 35, pp. 126–133,
Dec. 1997.
[61] J. Proakis and M. Salehi. Communication Systems Engineering. Upper Saddle River, NJ: Prentice-Hall, 2002.
[62] Qualcomm. Who we are: History. http://www.qualcomm.com/who we are/history.html, accessed 2010.
[63] L. Rabiner and B. Gold. Theory and Application of Digital Signal Processing. Englewood Cliffs, NJ: Prentice-Hall,
1975.
[64] GNU Radio. The GNU software radio. http://www.gnu.org/software/gnuradio/, accessed 2008.
[65] Jaycar
Electronics
Reference
Data
Sheet.
Understanding
decibels.
http://www.jaycar.com.au/images
uploaded/decibels.pdf, accessed 2009.
[66] D. Slepian. On bandwidth. Proceeding of the IEEE, 64, March 1976. 292–300.
[67] S. Smith. The Scientist and Engineer’s Guide to Digital Signal Processing (http://www.dspguide.com). California
Technical Publishing, San Diego, CA, 1997.
[68] S. Soliman and M. Srinath. Continuous and Discrete Signals and Systems. Upper Saddle River, NJ: Prentice-Hall,
1998.
[69] A. Stanoyevitch. Introduction to Numerical Ordinary and Partial Differential Equations Using MATLAB . New York:
Wiley, 2005.
[70] Statemaster.com. Spread spectrum. http://www.statemaster.com/encyclopedia/Spread-spectrum, accessed 2009.
[71] H. Stern and S. Mahmoud. Communication Systems—Analysis and Design. Upper Saddle River, NJ: Pearson/Prentice-
Hall, 2004.
[72] M. Van Valkenburg. Analog Filter Design. New York: Oxford University Press, 1982.
[73] Wikipedia. Euler’s identity. http://en.wikipedia.org/wiki/Euler’s identity, accessed 2009.
[74] Wikipedia. Hedy Lamarr. http://en.wikipedia.org/wiki/Hedy Lamarr, accessed 2009.
[75] Wikipedia. Nikola Tesla. http://en.wikipedia.org/wiki/Nikola Tesla, accessed 2009.
[76] Wikipedia. Oliver Heaviside. http://en.wikipedia.org/wiki/Oliver Heaviside, accessed 2009.
[77] Wikipedia. Field-programable gate array. http://en.wikipedia.org/wiki/Field-programmable gate array, accessed 2008.
[78] M. R. Williams. A History of Computing Technologies. New York: Wiley/IEEE Computer Society Press, 1997.

Index
Ex, 80
F(s) = L[f(t)], 169
Fs, 437
F() = F[f(t)], 305, 344–346
F() = F[f[n]], 587
F(z) + Z[f[n]], 512, 523
H(s) = L[y(t)]/L[x(t)], 197
N, 77
Px, 85
Ts, 456
Xk, 256
1, 441
0 = 2π/T0, 256
, 656
s, 423
δ(t), 89
δTs(t), 423
ω, 423
τ, 73
ϵx, 458
ϵ(nTs), 442
ejℓ0t, 247
h(t), 149
n, 452
r(t), 90
u(t), 89
x[n], 452, 454
xe[n], 464, 465
xo[n], 465
xe(t), 76
xo(t), 76
yzi(t), 130, 215
yzs(t), 130, 215
A
absolutely summable impulse
response, 501, 535–536, 680
absolutely summable signals, 575,
576, 628
advanced signal, 324
amplitude modulation (AM), 87
demodulation, 380
envelope receiver, 381
single sideband, 382–383
suppressed carrier, 379–380
tunable band-pass ﬁlter, 379
analog
signal, 9, 67–71
signal, deﬁnition, 67
analog communication systems, 730
analog control systems, 363
actuator, 366
cruise control system, 367–369
feedback, 363
open-loop and closed-loop,
364–365
positive and negative
feedback, 363
proportional controller, 366
proportional plus integral (PI)
controller, 367
stability and stabilization,
369–371
transducer, 366
analog ﬁltering, 390
basics, 390–393
Butterworth low-pass design,
391, 393–396
Chebyshev low-pass design,
396–402
Chebyshev polynomials, 396
eigenfunction property, 390
factorization, 391, 393–394, 399
frequency transformations,
402–404
loss function, 392
low-pass speciﬁcations, 392
magnitude and frequency
normalization, 393
magnitude-squared
function, 391
speciﬁcations, 391–393
analog Fourier series
absolutely uniform convergence,
265–270
coefﬁcients, 247
coefﬁcients from Laplace,
255–265
complex exponential, 245–248
convergence, 265–270
DC component, 251
even and odd signals, 279
fundamental frequency, 246,
253, 256
fundamental period, 246
harmonics, 251
linearity, 282–283
line spectrum, 250, 255
mean-square approximation,
266
Parseval’s theorem, 248–250
product of signals, 284
time and frequency shifting,
270–273
time reversal, 280
trigonometric, 251–255
analog Fourier transform
amplitude modulation, 314
convolution, 327–329
differentiation and integration,
346–350
direct and inverse, 299, 301
duality, 310–313
frequency shifting, 313–314
Laplace ROC, 302, 304
linearity, 304–305
periodic signals, 317–320
shifting in time, 345
spectrum and line spectrum, 318
symmetry, 322–327
analog frequency, 619
analog LTI systems
BIBO stability, 153–156
749

750
Index
analog LTI systems (continued)
causality, 143–145
complete response, 216
continuous-time, 119
convolution integral, 136–143
eigenfunction property, 167,
240, 273
frequency response, 240, 327
impulse response, 138
impulse response, transfer
function, and frequency
response, 329
represented by differential
equations, 214–221
steady-state response, 214
transfer function, 213
transient response, 214
unit-step response, 218, 219
zero-input response, 133, 214
zero-state response, 133, 214
analog systems
causality, 143–145
DC source, 329
passivity, 154
stability, 153
windowing, 331
analog-to-digital converter (ADC),
68, 420
anti-aliasing ﬁlter, 430
application-speciﬁc integrated
circuit (ASIC), 5
approximate solution of differential
equations, 559
B
band-limited signal, 423
basic analog signals
ramp, 90–92
triangular pulse, 90
unit-impulse, 88
unit-step, 89
basic discrete-time signals, 465–478
complex exponentials, 596
damped sinusoid, 466
discrete sinusoids, 469–471
basic signal operations
adder, 72
advancing and delaying, 73
constant multiplier, 71
modulation, 72
reﬂection, 72
time scaling, 71
windowing, 71
BIBO stability of discrete
systems, 501
bilinear transformation, 654–656
warping, 656
block diagrams, 148, 150
bounded-input bounded-output
(BIBO) stability, 153–156,
499–501
C
causal
sinusoid, 82, 110
causality
discrete LTI systems, 498
discrete signal, 497–498
discrete systems, 497–500
causal systems and signals, 507–508
channel noise, 379
circular shifting, 607–609
cognitive radio, 6–8
compact-disc (CD) player, 5–6
complex variable function, 23–24
complex variables, 20, 23–24
computer-control systems, 8–9
connection of s-plane and
z-plane, 513
continuous-time
signal, 67–85
convolution integral, 136–133
commutative property, 148
distributive property, 149
Fourier, 327
graphical computation, 145–147
Laplace, 221
convolution sum, 487–494,
526–537
commutative property, 148
deconvolution, 229
noncausal signals, 533
D
delayed signal, 73
difference equations, 18–19,
550–561
digital communications, 709
orthogonal frequency-division
multiplexing (OFDM), 710
PCM, 710
spread spectrum, 710
time-division multiplexing, 730
digital signal processing, 710–722
FFT, 711–715
FFT algorithm, 711
digital signal processor (DSP), 5
digital-to-analog converter, 5,
68, 420
discrete complex exponentials,
466–469
discrete ﬁltering
analog signals, 640
bilinear transformation, 640
Butterworth LPF, 658–664
Chebyshev LPF, 666–672
direct, cascade, and parallel IIR
realizations, 698
eigenfunction, 639
FIR design, 681
FIR realizations, 699–700
FIR window design, 681
frequency scales, 652–653
frequency-selective ﬁlters, 641
frequency speciﬁcations, 659
group delay, 643
IIR and FIR, 643–647
IIR design, 672
linear phase, 641–643
loss function, 648–650
rational frequency
transformations, 672–676
realization, 689–700
time speciﬁcations, 652–653
windows for FIR design,
681–683
discrete ﬁlters
FIR, 643–647
IIR, 643–647
discrete Fourier series, 599–601
circular representation, 598–599
circular shifting, 607–609
complex exponential, 599–601
periodic convolution, 609–614
Z-transform, 601–602
discrete Fourier transform (DFT),
614–627
fast Fourier transform (FFT), 614
linear and circular
convolution, 624
discrete frequency, 454, 471
discrete LTI systems
causality, 498
response to periodic signals,
273–278
discrete sinusoid, 444
discrete systems
autoregressive (AR), 482
autoregressive moving average
(ARMA), 484
BIBO stability, 500–501
causality and stability, 497–501
convolution sum, 487–494
difference equation
representation, 486–487

Index
751
moving average (MA), 481–482
nonlinear system, 498
time-invariance, 498
discrete-time Fourier transform
(DTFT), 572–596
convergence, 591
convolution sum, 595–596
downsampling and
upsampling, 582
eigenfunctions, 573–575
Parseval’s theorem, 585–587
sampled signal, 578–580
symmetry, 589–595
time and frequency shifts, 628
time-frequency duality, 628
time-frequency supports,
580–585
Z-transform, 573–575
discrete-time signals
absolutely summable, 575,
576, 628
basic, 465–478
deﬁnition, 452
Fibonacci sequence, 453
ﬁnite energy, 458–461
ﬁnite power, 458–461
inherently discrete-time, 452
sample index, 452
sinusoid, 469–472
square summable, 458
discrete transfer function, 655
E
energy, 80
discrete-time signals, 458–461
Euler’s identity, 23–24, 87
even signal, 279, 461–465
F
Fibonacci sequence
difference equation, 453
ﬁeld-programmable gate array
(FPGA), 5
ﬁltering, 276–278, 327–344
analog, 390–408
median ﬁlter, 495
ﬁlters
anti-aliasing, 430
passband, 332
RC high-pass ﬁlter, 336
RC low-pass ﬁlter, 277
ﬁnite calculus, 9
ﬁnite difference, 12–13
summations, 13–16
FIR ﬁlters and convolution sum,
528, 529, 531, 533
Fourier basis, 247
four-level quantizer, 441, 442
frequency, harmonically related, 83
frequency aliasing, 424
frequency modulation (FM), 87
frequency response, poles and zeros,
342, 343
G
Gibb’s phenomenon, 266, 267
ﬁltering, 334
graphical convolution sum, 530
H
hybrid system, 119
I
ideal ﬁlters
band-pass, 332
high-pass, 332
linear phase, 332
low-pass, 332
zero-phase, 333
ideal impulse sampling, 421–428
inverse Laplace
with exponentials, 209
partial fraction expansion, 198
two-sided, 212–214
inverse Z-transform, 542–563
inspection, 542
long-division method, 542–543
partial fraction expansion,
544–546
positive powers of z, 545, 546
L
Laplace transform
convolution integral, 196–197
derivative, 189
integration, 193–194
inverse, 169, 197–214
linearity, 185–188
one-sided, 176–197
proper rational, 198
region of convergence (ROC),
166, 172–176
transfer function, 214, 223
two-sided, 166–176
length of convolution sum, 721
L’Hopital’s rule, 101, 306, 433
LTI systems, superposition, 135–136
M
magnitude line spectrum, 249
Matlab
analog Butterworth and
Chebyshev ﬁlter design, 414
analog Butterworth ﬁltering, 414
control toolbox, 375
decimation and interpolation,
585
DFT and FFT, 577
discrete ﬁlter design, 644
DTFT computation, 577
FFT computation, 717
ﬁlter design, 405–408
Fourier series computation,
603–604
functions, 36
general discrete ﬁlter design, 646
numerical computations, 30
phase computation, 591
phase unwrapping, 592
plotting, 39–41
saving and loading, 41–43
symbolic computations, 43–53
vectorial operations, 33–35
vectors and matrices, 30–33
N
negative frequencies, 323
nonlinear ﬁltering, median
ﬁlter, 495
nonzero initial conditions, 552
normality, 247
Nyquist sampling rate, 431
Nyquist sampling theorem, 431
O
odd signal, 75–77
one-sided Z-transform, 511
orthogonality, 248
P
Parseval’s relation and sampling,
427
periodic convolution, 609–614, 624
periodic discrete sinusoids, 454, 456
phase line spectrum, 249, 250, 253,
257, 259, 261, 263, 265
phase modulation (PM), 87, 378,
386
phasors, sinusoidal steady state,
24–26, 28
poles and zeros, 172–176
poles and zeros of Z-transforms, 511,
549, 551, 564

752
Index
power, 79–85, 248–250
average, 80
discrete-time signals, 458–461
instantaneous, 79, 80
proper rational functions, 198–200,
202, 205, 544, 546
pulse amplitude modulation (PAM),
420–421
pulse code modulation (PCM), 729,
730–733
Q
quantization error, 441
quantization step, 441
R
rational functions, 542
real-time processing, 118
S
sampled analog signals, 451
sampled data and digital control,
722–729
closed-loop control, 726–729
feedback, 726
open-loop control, 724–726
sampler time-varying system, 422
sampling
anti-aliasing ﬁlter, 430
frequency aliasing, 424
holder for DAC, 439
Nyquist rate, 431
Nyquist-Shannon theorem,
437–439
Parseval’s application, 427
period, 431
practical aspects, 420, 439–446
quantization, 439
quantization and coding, 68
quantization error, 441, 442
quantizer, 441
rate, 430
sample-and-hold system, 440
sampling period, 69
signal recovery, 429
sinc interpolation, 432–433
sampling frequency, 423
sampling function, 421
sampling period, 431
sampling rate, 430
Shannon, 430
sifting property of δ(t), 106
signal radiation with antenna, 317
signal recovery in sampling, 429
signals
absolutely integrable, 81
absolutely summable, 575, 576,
628
advanced, 74
analog, 68
aperiodic, 66, 77–79
band-limited, 423
basic analog signals, 85–106
causal, anti-causal, noncausal,
174
causal discrete sinusoid, 454
causality, 145
causal sinusoid, 82
complex exponentials, 87
continuous-time, 67–85
convolution integral, 141
delayed, 73
deterministic, 66
digital, 67
discrete ﬁnite energy, 459
discrete periodic sinusoids, 454
discrete sinusoid, 444
discrete time, 67
even, 66, 75–77
even and odd decomposition,
75, 76
ﬁnite energy, 66, 80
ﬁnite-energy discrete signal,
458–461
ﬁnite power, 79–85
ﬁnite support, 71
full-wave rectiﬁed, 99, 262
inherently discrete, 452
modulation, 126
odd, 66, 75–77
periodic, 66, 77–79
piecewise smooth, 266
random, 65
real and imaginary parts, 86
sampled analog, 470
shifting and reﬂecting, 74
sinc, 311
sinusoids, 70, 87
smooth, 266
speech, 69
square integrable, 80
square summable, 458
train of rectangular pulses, 143
windowing, 72
sinc interpolation of recovered
signals, 433
sinusoidal steady-state phasor,
24–26
software-deﬁned radio, 6–8
spectrum analyzer, 248
square summable signals, 458. See
also Finite-energy
discrete-time signals
stability, discrete systems, 478
system
all-pass, 243
amplitude modulation (AM),
126
analog, 119
analog averager, 158
averager, 158
communications, 383
continuous-time, 120
deﬁnition, 117
differential equation
representation, 131
digital, 119
discrete-time, 119
hybrid, 119
ideal communication system,
243
linearity, 120
multipath channel, 170
nonlinear, 127
RLC circuit, 129–130
time-invariance, 118
time-varying, 128
vocal system, 128
T
trapezoidal rule approximation, 19,
654, 655
transient analysis, 371
U
uniform sampling, 420–437
Z
Z-transform
connection with sampling,
601–602
damping radius, 511
discrete frequency, 511, 523
inverse, 511, 527, 542,
543–550, 562
linearity, 522–524, 541, 555, 557
one-sided transform, 515,
521–537, 542–550
ROC and uniqueness, 516–521
for sampled signals, 512
signiﬁcance of poles, 511
solution of difference equations,
550–561
time-shifting, 533, 555, 557
two-sided transform, 515–521,
561–564


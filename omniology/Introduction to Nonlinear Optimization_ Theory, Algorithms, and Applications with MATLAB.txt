INTRODUCTION TO 
NONLINEAR OPTIMIZATION 
Theory, Algorithms, and 
Applications with MATLAB 
Amir Beck 

INTRODUCTION TO 
NONLINEAR OPTIMIZATION 

MOS-SIAM Series on Optimization 
This series is published jointly by the Mathematical Optimization Society and the Society for Industrial 
and Applied Mathematics. It includes research monographs, books on applications, textbooks at all 
levels, and tutorials. Besides being of high scientific quality, books in the series must advance the 
understanding and practice of optimization. They must also be written clearly and at an appropriate 
level for the intended audience. 
Editor-in-Chief 
Katya Scheinberg 
Lehigh University 
Editorial Board 
Santanu S. Dey, Georgia Institute of Technology 
Maryam Fazel, University of Washington 
Andrea Lodi, University of Bologna 
Arkadi Nemirovski, Georgia Institute of Technology 
Stefan Ulbrich, Technische Universitat Darmstadt 
Luis Nunes Vicente, University of Coimbra 
David Williamson, Cornell University 
Stephen J. Wright, University of Wisconsin 
Series Volumes 
Beck, Amir, Introduction to Nonlinear Optimization: Theory, Algorithms, and Applications with MATLAB 
Attouch, Hedy, Buttazzo, Giuseppe, and Michaille, Gerard, Variational Analysis in Sobolev and BV Spaces: 
Applications to PDEs and Optimization, Second Edition 
Shapiro, Alexander, Dentcheva, Darinka, and Ruszczyt'lski, Andrzej, Lectures on Stochastic Programming: 
Modeling and Theory, Second Edition 
Locatelli, Marco and Schoen, Fabio, Global Optimization: Theory, Algorithms, and Applications 
De Loera, Jesus A., Hemmecke, Raymond, and Kappe, Matthias, Algebraic and Geometric Ideas in the 
Theory of Discrete Optimization 
Blekherman, Grigoriy, Parrilo, Pablo A., and Thomas, Rekha R., editors, Semidefinite Optimization and 
Convex Algebraic Geometry 
Delfour, M. C., Introduction to Optimization and Semidifferential Calculus 
Ulbrich, Michael, Semismooth Newton Methods for Variational Inequalities and Constrained Optimization 
Problems in Function Spaces 
Biegler, Lorenz T., Nonlinear Programming: Concepts, Algorithms, and Applications to Chemical Processes 
Shapiro, Alexander, Dentcheva, Darinka, and Ruszczy('lski, Andrzej, Lectures on Stochastic Programming: 
Modeling and Theory 
Conn, Andrew R., Scheinberg, Katya, and Vicente, Luis N., Introduction to Derivative-Free Optimization 
Ferris, Michael C., Mangasarian, Olvi L., and Wright, Stephen J., Linear Programming with MATLAB 
Attouch, Hedy, Buttazzo, Giuseppe, and Michaille, Gerard, Variational Analysis in Sobolev and BV Spaces: 
Applications to PDEs and Optimization 
Wallace, Stein W. and Ziemba, William T., editors, Applications of Stochastic Programming 
Grotschel, Martin, editor, The Sharpest Cut: The Impact of Manfred Padberg and His Work 
Renegar, James, A Mathematical View of Interior-Point Methods in Convex Optimization 
Ben-Tai, Aharon and Nemirovski, Arkadi, Lectures on Modern Convex Optimization: Analysis, Algorithms, 
and Engineering Applications 
Conn, Andrew R., Gould, Nicholas I. M., and Toint, Phillippe L., Trust-Region Methods 

INTRODUCTION TO 
NONLINEAR OPTIMIZATION 
Theory, Algorithms, and 
Applications with MATLAB 
• 
® 
SJ.aJTL.. 
Society for Industrial and Applied Mathematics 
Philadelphia 
Amir Beck 
Technion-Israel Institute of Technology 
Kfar Saba, Israel 
Mathematical 
Optimization Society 
Mathematical Optimization Society 
Philadelphia 

Copyright© 2014 by the Society for Industrial and Applied Mathematics and the Mathematical 
Optimization Society 
10987654321 
All rights reserved. Printed in the United States of America. No part of this book may be 
reproduced, stored, or transmitted in any manner without the written permission of the 
publisher. For information, write to the Society for Industrial and Applied Mathematics, 
3600 Market Street, 6th Floor, Philadelphia, PA 19104-2688 USA. 
Trademarked names may be used in this book without the inclusion of a trademark symbol. 
These names are used in an editorial context only; no infringement of trademark is intended. 
MATLAB is a registered trademark of The MathWorks, Inc. For MATLAB product information, 
please contact The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA 01760-2098 USA, 
508-647-7000, Fax: 508-647-7001, info@mathworks.com, www.mathworks.com. 
Library of Congress Cataloging-in-Publication Data 
Beck, Amir, author. 
Introduction to nonlinear optimization : theory, algorithms, and applications with MATLAB/ 
Amir Beck, Technion-Israel Institute of Technology, Kfar Saba, Israel. 
pages cm. -
(MOS-SIAM series on optimization) 
Includes bibliographical references and index. 
ISBN 978-1-611973-64-8 
1. Mathematical optimization. 2. Nonlinear theories. 3. MATLAB. I. Title. 
QA402.5.B4224 2014 
519.6-dc23 
• 
5.1aJTL is a registered trademark. 
2014029493 
Mathematical 
Optimization Society is a registered trademark 

For 
My wife Nili 
My daughters Noy and Vered 
My parents Nili and Itzhak 


Contents 
Preface 
. 
Xl 
1 
Mathematical Preliminaries 
1 
1.1 
The Space Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
1 
1.2 
The Space Rmxn . • . • • • • • . • • • • . • • . • • • • • • • • • • • • • • • • • 
2 
1.3 
Inner Products and Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . 
2 
1.4 
Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . 
5 
1.5 
Basic Topological Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . 
6 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
10 
2 
Optimality Conditions for Unconstrained Optimization 
13 
2.1 
Global and Local Optima . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
13 
2.2 
Classification of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
17 
2.3 
Second Order Optimality Conditions . . . . . . . . . . . . . . . . . . . . 
23 
2.4 
Global Optimality Conditions . . . . . . . . . . . . . . . . . . . . . . . . . 
30 
2.5 
Quadratic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
32 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
34 
3 
Least Squares 
37 
3.1 
"Solution" of Overdetermined Systems . . . . . . . . . . . . . . . . . . . 
37 
3 .2 
Data Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
39 
3 .3 
Regularized Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
41 
3.4 
Deno1smg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
42 
3.5 
Nonlinear Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
45 
3.6 
Circle Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
45 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
4 7 
4 
The Gradient Method 
49 
4.1 
Descent Directions Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 
49 
4.2 
The Gradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
52 
4.3 
The Condition Number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
58 
4.4 
Diagonal Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
63 
4.5 
The Gauss-Newton Method . . . . . . . . . . . . . . . . . . . . . . . . . . 
67 
4.6 
The Fermat-Weber Problem . . . . . . . . . . . . . . . . . . . . . . . . . . 
68 
4.7 
Convergence Analysis of the Gradient Method . . . . . . . . . . . . . . 
73 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
79 
5 
Newton's Method 
83 
5.1 
Pure Newton's Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
83 
vii 

viii 
Contents 
5.2 
Damped Newton's Method .......................... . 
5.3 
The Cholesky Factorization ......................... . 
Exercises ............................................ . 
88 
90 
94 
6 
Convex Sets 
97 
6.1 
Definition and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
97 
6.2 
Algebraic Operations with Convex Sets . . . . . . . . . . . . . . . . . . . 100 
6.3 
The Convex Hull . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 
6.4 
Convex Cones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 
6.5 
Topological Properties of Convex Sets . . . . . . . . . . . . . . . . . . . . 108 
6.6 
Extreme Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 
7 
Convex Functions 
117 
7 .1 
Definition and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 
7 .2 
First Order Characterizations of Convex Functions . . . . . . . . . . . 119 
7 .3 
Second Order Characterization of Convex Functions . . . . . . . . . . 123 
7.4 
Operations Preserving Convexity . . . . . . . . . . . . . . . . . . . . . . . 125 
7 .5 
Level Sets of Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . 130 
7.6 
Continuity and Differentiability of Convex Functions . . . . . . . . . 132 
7.7 
Extended Real-Valued Functions . . . . . . . . . . . . . . . . . . . . . . . . 135 
7 .8 
Maxima of Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . 137 
7. 9 
Convexity and Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 
8 
Convex Optimization 
147 
8.1 
Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 
8.2 
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 
8.3 
The Orthogonal Projection Operator . . . . . . . . . . . . . . . . . . . . 156 
8.4 
cvx. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 
9 
Optimization over a Convex Set 
169 
9.1 
Stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 
9.2 
Stationarity in Convex Problems . . . . . . . . . . . . . . . . . . . . . . . 173 
9.3 
The Orthogonal Projection Revisited . . . . . . . . . . . . . . . . . . . . 173 
9.4 
The Gradient Projection Method . . . . . . . . . . . . . . . . . . . . . . . 175 
9.5 
Sparsity Constrained Problems . . . . . . . . . . . . . . . . . . . . . . . . 183 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 
10 
Optimality Conditions for Linearly Constrained Problems 
191 
10.1 
Separation and Alternative Theorems . . . . . . . . . . . . . . . . . . . . 191 
10.2 
The KKT conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 
10.3 
Orthogonal Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 
11 
The KK.T Conditions 
207 
11.1 
Inequality Constrained Problems . . . . . . . . . . . . . . . . . . . . . . . 207 
11.2 
Inequality and Equality Constrained Problems . . . . . . . . . . . . . . 210 
11.3 
The Convex Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 
11.4 
Constrained Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 

Contents 
ix 
11.5 
Second Order Optimality Conditions . . . . . . . . . . . . . . . . . . . . 222 
11.6 
Optimality Conditions for the Trust Region Subproblem . . . . . . . 227 
11.7 
Total Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 
12 
Duality 
237 
12.1 
Motivation and Definition. . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 
12.2 
Strong Duality in the Convex Case . . . . . . . . . . . . . . . . . . . . . . 241 
12.3 
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270 
Bibliographic Notes 
Bibliography 
Index 
275 
277 
281 


Preface 
This book emerged from the idea that an optimization training should include three 
basic components: a strong theoretical and algorithmic foundation, familiarity with var-
ious applications, and the ability to apply the theory and algorithms on actual "real-life" 
problems. The book is intended to be the basis of such an extensive training. The mathe-
matical development of the main concepts in nonlinear optimization is done rigorously, 
where a special effort was made to keep the proofs as simple as possible. The results 
are presented gradually and accompanied with many illustrative examples. Since the aim 
is not to give an encyclopedic overview, the focus is on the most useful and important 
concepts. The theory is complemented by numerous discussions on applications from 
various scientific fields such as signal processing, economics and localization. Some basic 
algorithms are also presented and studied to provide some B.avor of this important aspect 
of optimization. Many topics are demonstrated by MATLAB programs, and ideally, the 
interested reader will find satisfaction in the ability of actually solving problems on his or 
her own. The book contains several topics that, compared to other classical textbooks, 
are treated differently. The following are some examples of the less common issues. 
• The treatment of stationarity is comprehensive and discusses this important notion 
in the presence of sparsity constraints. 
• The concept of "hidden convexity" is discussed and illustrated in the context of the 
trust region subproblem. 
• The MATLAB toolbox CVX is explored and used. 
• The gradient mapping and its properties are studied and used in the analysis of the 
gradient projection method. 
• Second order necessary optimality conditions are treated using a descent direction 
approach. 
• Applications such as circle fitting, Chebyshev center, the Fermat-Weber problem, 
denoising, clustering, total least squares, and orthogonal regression are studied both 
theoretically and algorithmically, illustrating concepts such as duality. MATLAB 
programs are used to show how the theory can be implemented. 
The book is intended for students and researchers with a basic background in advanced cal-
culus and linear algebra, but no prior knowledge of optimization theory is assumed. The 
book contains more than 170 exercises, which can be used to deepen the understanding 
of the material. The MATLAB functions described throughout the book can be found at 
www.siam.org/books/mo19. 
xi 

Preface 
The outline of the book is as follows. Chapter 1 recalls some of the important concepts 
in linear algebra and calculus that are essential for the understanding of the mathematical 
developments in the book. Chapter 2 focuses on local and global optimality conditions 
for smooth unconstrained problems. Quadratic functions are also introduced along with 
their basic properties. Linear and nonlinear least squares problems are introduced and 
studied in Chapter 3. Several applications such as data fitting, denoising, and circle fitting 
are discussed. The gradient method is introduced and studied in Chapter 4. The chapter 
also contains a discussion on descent direction methods and various stepsize strategies. 
Extensions such as the scaled gradient method and damped Gauss-Newton are consid-
ered. The connection between the gradient method and Weiszfeld's method for solving 
the Fermat-Weber problem is established. Newton's method is discussed in Chapter 5. 
Convex sets and functions along with their basic properties are the subjects of Chapters 6 
and 7. Convex optimization problems are introduced in Chapter 8, which also includes 
a variety of applications as well as CVX demonstrations. Chapter 9 focuses on several im-
portant topics related to optimization problems over convex sets: stationarity, gradient 
mappings, and the gradient projection method. The chapter ends with results on spar-
sity constrained problems, illuminating the different type of results obtained when the 
underlying set is not convex. The derivation of the KK.T optimality conditions from the 
separation and alternative theorems is the subject of Chapter 10, where only linearly con-
strained problems are considered. The extension of the KK.T conditions to problems with 
nonlinear constraints is discussed in Chapter 11, which also considers the second order 
necessary conditions. Applications of the conditions to the trust region and total least 
squares problems are studied. The book ends with a discussion on duality in Chapter 12. 
Strong duality under convexity assumptions is established. This chapter also includes a 
large amount of examples, applications, and MATLAB illustrations. 
I would like to thank Dror Pan and Luba Tetruashvili for reading the book and for 
their helpful remarks. It has been a pleasure working with the SIAM staff, namely with 
Bruce Bailey, Elizabeth Greenspan, Sara Murphy, Gina Rinelli, and Kelly Thomas. 

Chapter 1 
Mathematical 
Pre Ii mi naries 
In this short chapter we will review some important notions and results from calculus, 
linear algebra, and topology that will be frequently used throughout the book. This chap-
ter is not intended to be, by any means, a comprehensive treatment of these subjects, and 
the interested reader can find more material in advanced calculus and linear algebra books. 
1.1 • The Space an 
The vector space Rn is the set of n-dimensional column vectors with real components 
endowed with the component-wise addition operator 
+ 
and the scalar-vector product 
Yi 
Y2 
Yn 
Xi +Yi 
X2+Y2 
xn 
A.xn 
where in the above Xi, x2, • •• , xn, A. are real numbers. Throughout the book we will be 
mainly interested in problems over Rn, although other vector spaces will be considered 
in a few cases. We will denote the standard basis of Rn by ei,e2, ••• ,en, where ei is the 
n-length column vector whose ith component is one while all the others are zeros. The 
column vectors of all ones and all zeros will be denoted by e and 0, respectively, where 
the length of the vectors will be clear from the context. 
Important Subsets of Rn 
The nonnegative orthant is the subset of Rn consisting of all vectors in Rn with nonnega-
tive components and is denoted by JR~: 

2 
Chapter 1. Mathematical Preliminaries 
Similarly, the positive orthant consists of all the vectors in Rn with positive components 
and is denoted by R~+: 
R~+ = { ( x1, x2, ••• , xn) T : x1, x2, ... , Xn > 0} . 
For given x, y E Rn, the closed line segment between x and y is a subset of Rn denoted by 
[ x, y] and defined as 
[x,y] = {x + a(y-x): a E [O, 1]}. 
The open line segment ( x, y) is similarly defined as 
(x,y) = {x+a(y-x): a E (0, 1)} 
when x ~ y and is the empty set 0 when x = y. The unit-simplex, denoted by ll.n, is the 
subset of Rn comprising all nonnegative vectors whose sum is 1: 
ll.n = { x E Rn : x > 0,eT x = 1}. 
1.2 • The Space lRmxn 
The set of all real-valued matrices of order m x n is denoted by Rmxn. Some special matri-
ces that will be frequently used are the n x n identity matrix denoted by In and the m x n 
zeros matrix denoted by Omxn· We will frequently omit the subscripts of these matrices 
when the dimensions will be clear from the context. 
1.3 • Inner Products and Norms 
Inner Products 
We begin with the formal definition of an inner product. 
Definition 1.1 (inner product). An inner product on Rn is a map(·,·): Rn x Rn--+ R 
with the following properties: 
1. (symmetry) (x, y) = (y, x) for any x, y E Rn. 
2. (additivity) (x, y + z) = (x, y) + (x, z) for any x, y, z E Rn. 
3. (homogeneity) (.A.x,y) = J.(x,y) for any A ER and x,y E Rn. 
4. (positive definiteness) (x,x) > 0 for any x E Rn and (x,x) = 0 if and only if x = 0. 
Example 1.2. Perhaps the most widely used inner product is the so-called dot product 
defined by 
n 
(x,y) = XT y = L: Xi Yi for any x, y E Rn. 
i=l 
Since this is in a sense the "standard" inner product, we will by default assume-unless 
explicitly stated otherwise-that the underlying inner product is the dot product. 
I 
Example 1.3. The dot product is not the only possible inner product on Rn. For exam-
ple, let w E R~+. Then it is easy to show that the following weighted dot product is also 
an inner product: 
n 
(x,y)w = L: W;X;Y;· 
I 
i=l 

1.3. Inner Products and Norms 
3 
Vector Norms 
Definition 1.4 (norm). A norm II · II on Rn is a function II · II : Rn -+ JR satisfying the 
following: 
1. (nonnegativity) I lxl I > 0 for any x E Rn and I lxl I = 0 if and only if x = 0. 
2. (positive homogeneity) llhll = 1.-llllxllfor any x E Rn and A ER 
3. (triangle inequality) llx+yll < llxll + llYllforany x,y E Rn. 
One natural way to generate a norm on Rn is to take any inner product (-, ·) on Rn 
and define the associated norm 
llxll = J (x, x) for all x E Rn, 
which can be easily seen to be a norm. If the inner product is the dot product, then the 
associated norm is the so-called Euclidean norm or l2-norm: 
llxll, = ~ t xf for all x E JR". 
By default, the underlying norm on Rn is II· lb, and the subscript 2 will be frequently 
omitted. The Euclidean norm belongs to the class of LP norms (for p > 1) defined by 
n 
llxllp = 
P L lxdP · 
i=1 
The restriction p > 1 is necessary since for 0 < p < 1, the function 11 • llp is actually not a 
norm (see Exercise 1.1). Another important norm is the l00 norm given by 
llxlloo =. max lxJ 
i=1,2, ... ,n 
Unsurprisingly, it can be shown (see Exercise 1.2) that 
llxll 00 = lim llxllp· 
p-+oo 
An important inequality connecting the dot product of two vectors and their norms 
is the Cauchy-Schwarz inequality, which will be used frequently throughout the book. 
Lemma 1.5 (Cauchy-Schwarz inequality). For any x, y E Rn, 
lxT YI < llxl '2 · llYI '2 · 
Equality is satisfied if and only if x and y are linearly dependent. 
Matrix Norms 
Similarly to vector norms, we can define the concept of a matrix norm. 

4 
Chapter 1. Mathematical Preliminaries 
Definition 1.6. A norm 11·11 onRmxn is a function 11·11: Rmxn-+ Rsatisfying the following: 
1. (nonnegativity) I IAI I > 0 for any A E Rmxn and I IAI I = 0 if and only if A = 0. 
2. (positive homogeneity) llAAll =I.A.I· llAllfor any A E Rmxn and A ER. 
3. (triangle inequality) llA+Bll < llAll+llBllforanyA,BERmxn. 
Many examples of matrix norms are generated by using the concept of induced norms, 
which we now describe. Given a matrix A E Rmxn and two norms II· Ila and 11 · llb on Rn 
and Rm, respectively, the induced matrix norm llAlla,b is defined by 
I !Alla b = max {llAxllb : llxl la < 1} · 
' 
x 
It can be shown that the above definition implies that for any x E Rn the inequality 
holds. An induced matrix norm is indeed a norm in the sense that it satisfies the three 
properties required from a vector norm (see Definition 1.4): nonnegativity, positive ho-
mogeneity, and the triangle inequality. We refer to the matrix norm 11 · 1 la,b as the (a, b )-
norm. When a = b (for example, when the two vector norms are la norms), we will 
simply refer to it as an a-norm and omit one of the subscripts in its notation; that is, the 
notation is 11 · I la instead of 11 · 1 la,a. 
Example 1.7 (spectral norm). If II· Ila = 11 · llb =II· lb, then the induced norm of a matrix 
A E Rmxn is the maximum singular value of A: 
Since the Euclidean norm is the "standard" vector norm, the induced norm, namely the 
spectral norm, will be the standard matrix norm, and thus the subscripts of this norm 
will usually be omitted. 
I 
Example 1.8 (1-norm). When II· Ila= 11·llb=11·111' the induced matrix norm of a matrix 
A E Rmxn is given by 
m 
llAll1 =.max L:IA;,jl· 
7=1,2,. .. ,n i=1 
This norm is also called the maximum absolute column sum norm. 
I 
Example 1.9 (<x:>·norm). When II· Ila= 11·llb=11·1100 , then the induced matrix norm of 
a matrix A E Rmxn is given by 
n 
llAlloo = . max L: IAi,j I· 
i=1,2, ... ,m . 1 
1= 
This norm is also called the maximum absolute row sum norm. 
I 
An example of a matrix norm that is not an induced norm is the Frobenius norm de-
fined by 
m 
n 
~~A2 A E l!])mxn. 
~~ ij' 
.Ii'\. 
i=l j=l 

1.4. Eigenvalues and Eigenvectors 
5 
1.4 • Eigenvalues and Eigenvectors 
Let A E IR.nxn. Then a nonzero vector v E Rn is called an eigenvector of A if there exists 
a .A. E C (the complex field) for which 
Av=..A.v. 
The scalar .A. is the eigenvalue corresponding to the eigenvector v. In general, real-valued 
matrices can have complex eigenvalues, but it is well known that all the eigenvalues of sym-
metric matrices are real. The eigenvalues of a symmetric n x n matrix A are denoted by 
The maximum eigenvalue is also denoted by ..A.max(A)(= Ai(A)) and the minimum eigen-
value is also denoted by .A.min {A)(= An (A)). One of the most useful results related to eigen-
values is the spectral decomposition theorem, which states that any symmetric matrix A 
has an orthonormal basis of eigenvectors. 
Theorem 1.10 (spectral decomposition theorem). Let A E Rnxn be an n x n symmet· 
ric matrix. Then there exists an orthogonal matrix U E Rnxn (UTU = uur =I) and 
a diagonal matrix D = diag( di, di, ... , dn) for which 
UTAU=D. 
(1.1) 
The columns of the matrix U in the factorization (1.1) constitute an orthnormal basis 
comprised of eigenvectors of A and the diagonal elements of D are the corresponding 
eigenvalues. A direct result of the spectral decomposition theorem is that the trace and 
the determinant of a matrix can be expressed via its eigenvalues: 
n 
Tr{A) = 2: ..A.i(A), 
(1.2) 
i=l 
n 
det{A) = n ..A.i(A). 
(1.3) 
i=i 
Another important consequence of the spectral decomposition theorem is the bounding 
of the so-called Rayleigh quotient. For a symmetric matrix A E Rnxn, the Rayleigh quo-
tient is defined by 
xTAx 
RA(x) = llxll2 for any x ;/; 0. 
We can now use the spectral decomposition theorem to prove the following lemma pro-
viding lower and upper bounds on the Rayleigh quotient. 
Lemma 1.11. Let A E Rnxn be symmetric. Then 
Proof. By the spectral decomposition theorem there exists an orthogonal matrix U E 
Rnxn and a diagonal matrix D = diag(di,di, ... ,dn) such that UT AU= D. We can as-
sume without the loss of generality that the diagonal elements of D, which are the eigen-
values of A, are ordered nonincreasingly: di > d2 > · · · > dn, where di = A.naxCA) and 

6 
Chapter 1. Mathematical Preliminaries 
dn = A.min(A). Making the change of variables x =Uy and noting that U is a nonsingular 
matrix, we obtain that 
Since d; <di for all i = 1, 2, ... , n, it follows that L?=i d;yf <di (L?=i yf ), and hence 
The inequality RA(x) > ~in(A) follows by a similar argument. 
0 
The lower and upper bounds on the Rayleigh quotient given in the latter lemma are 
attained at eigenvectors corresponding to the minimal and maximal eigenvalues respec-
tively. Indeed, if v and w are eigenvectors corresponding to the minimal and maximal 
eigenvalues respectively, then 
The above facts are summarized in the following lemma. 
Lemma 1.12. Let A E Rnxn be symmetric. Then 
(1.4) 
and the eigenvectors of A corresponding to the minimal eigenvalue are minimizers of problem 
(1.4 ). In addition, 
maxRA(x) = ~ax(A), 
xi:O 
(1.5) 
and the eigenvectors of A corresponding to the maximal eigenvalue are maximizers of problem 
(1.5). 
1.5 • Basic Topological Concepts 
We begin with the definition of a ball. 
Definition 1.13 (open ball, closed ball). The open ball with center c e Rn and radius r 
is denoted by B(c, r) and defined by 
B(c, r) = {x E !Rn: llx-cll < r}. 
The closed ball with center c and radius r is denoted by B[ c, r] and defined by 
B[c, r] = {xeRn: llx-cll < r}. 

1.5. Basic Topological Concepts 
7 
Note that the norm used in the definition of the ball is not necessarily the Euclidean 
norm. As usual, if the norm is not specified, we will assume by default that it is the 
Euclidean norm. The ball B( c, r) for some arbitrary r > 0 is also referred to as a neighbor-
hood of c. The first topological notion we define is that of an interior point of a set. This 
is a point which has a neighborhood contained in the set. 
Definition 1.14 (interior points). Given a set U C JR.n, a point c E U is an interior point 
of U if there exists r > 0 for which B( c, r) C U. 
The set of all interior points of a given set U is called the interior of the set and is 
denoted by int( U): 
int(U) = {x EU: B(x, r) CU for some r > O}. 
Example 1.15. Following are some examples of interiors of sets which were previously 
discussed: 
int(lR.~) =JR.~+' 
int(B[ c, r ]) = B( c, r) ( c E JR.n, r ER++), 
int([x,y ]) = (x,y) (x,y E JR.n ,x ;if y). 
I 
Definition 1.16 (open sets). An open set is a set that contains only interior points. In other 
words, UC JR.n is an open set if 
for every x E U there exists r > 0 such that B( x, r) C U. 
Examples of open sets are JR.n, open balls (hence the name), open line segments, and 
the positive orthant JR.~+. A known result is that a union of any number of open sets is 
an open set and the intersection of a finite number of open sets is open. 
Definition 1.17 (closed sets). A set UC JR.n is said to be closed if it contains all the limits 
of convergent sequences of points in U; that is, U is closed if for every sequence of points 
{xi };~ 1 C U satisfying X; ..,. x* as i -. oo, it holds that x* E U. 
A known property is that a set U is closed if and only if its complement U' is open (see 
Exercise 1.15). Examples of closed sets are the closed ball B[ c, r ], closed lines segments, 
the nonnegative orthant JR.~, and the unit simplex ~n. The space JR.n is both closed and 
open. An important and useful result states that level sets, as well as contour sets, of 
continuous functions are closed. This is stated in the following proposition. 
Proposition 1.18 (closedness of level and contour sets of continuous functions). Let 
f be a continuous function defined over a closed set S C JR.n. Then for any a E JR. the sets 
are closed. 
Lev(f ,a)= {x ES: f(x) <a}, 
Con(f ,a)= {x ES: f(x) =a} 
Definition 1.19 (boundary points). Given a set U C JR.n, a boundary point of U is a 
point x E JR.n satisfying the following: any neighborhood of x contains at least one point in U 
and at least one point in its complement U'. 

8 
Chapter 1. Mathematical Preliminaries 
The set of all boundary points of a set U is denoted by bd( U) and is called the 
boundary of U. 
Example 1.20. Some examples of boundary sets are 
bd(B(c, r)) = bd(B[c, r ]) = {x E lRn: llx-cll = r} (cE Rn, r E lR++), 
bd(JR~+) = bd(JR~) = {XE JR~ : 3i : X; = 0} , 
bd(JRn) = 0. 
I 
The closure of a set U C Rn is denoted by cl( U) and is defined to be the smallest closed 
set containing U: 
cl(U) = n{T: Uc T, Tis closed}. 
The closure set is indeed a closed set as an intersection of closed sets. Another equivalent 
definition of cl( U) is given by 
Example 1.21. 
cl(U)= Uubd(U). 
cl(JR~+) =JR~, 
cl( B( c, r)) = B [ c, r] ( c E JR n, r E lR++), 
cl((x,y)) = [x,y] (x,y E lRn ,x 1 y). 
I 
Definition 1.22 (boundedness and compactness). 
1. A set U C Rn is called bounded if there exists M > 0 for which U C B(O,M). 
2. A set U C ]Rn is called compact if it is closed and bounded. 
Examples of compact sets are closed balls and line segments. The positive orthant is 
not compact since it is unbounded, and open balls are not compact since they are not 
closed. 
1.5.1 • Differentiability 
Let f be a function defined on a set SC ]Rn. Let x E int(S) and let 01 d E lRn. If the limit 
1. 
f(x+ td)-f(x) 
1m------
t-+o+ 
t 
exists, then it is called the directional derivative of f at x along the direction d and is de-
noted by f'(x;d). For any i = 1,2, ... , n, the directional derivative at x along the direction 
e; (the i th vector in the standard basis) is called the i th partial derivative and is denoted 
by Bf (x): 
Bx, 
a f (x) = lim f(x+ te;)-f(x)_ 
0 X; 
t-+0+ 
t 

1.5. Basic Topological Concepts 
9 
If all the partial derivatives of a function f exist at a point x E Rn, then the gradient off 
at x is defined to be the column vector consisting of all the partial derivatives: 
Vf(x)= 
li(x) 
OX1 
Bf 
ax;(x) 
Bf (x) 
a;;; 
A function f defined on an open set U C Rn is called continuously differentiable over U 
if all the partial derivatives exist and are continuous on U. The definition of continuous 
differentiability can also be extended to nonopen sets by using the convention that a func-
tion f is said to be continuously differentiable over a set C if there exists an open set U 
containing C on which the function is also defined and continuously differentiable. In 
the setting of continuous differentiability, we have the following important formula for 
the directional derivative: 
/'(x;d) = V/(x)T d 
for all x E U and d E Rn. It can also be shown in this setting of continuous differentiabil-
ity that the following approximation result holds. 
Proposition 1.23. Let f: U--+ JR. be defined on an open set U C Rn. Suppose that f is 
continuously differentiable over U. 1ben 
1. /(x+d)-/(x)-V/(xfd_ fi 
ll 
d~ 
lldll 
-0 ora xE U. 
Another way to write the above result is as follows: 
f (y) = f (x) + V /(x)T (y-x) + o(lly-xll), 
where o( ·) : JR.~ --+ JR. is a one-dimensional function satisfying o~t) --+ 0 as t --+ o+. 
The partial derivatives a f are themselves real-valued functions that can be partially 
ax; 
differentiated. The (i,j)th partial derivatives off at x E U (if it exists) is defined by 
a2f 
a(U;) 
ax.ax.(x)= 
ax. (x). 
' 
J 
' 
A function f defined on an open set U C Rn is called twice continuously differentiable 
over U if all the second order partial derivatives exist and are continuous over U. Under 
the assumption of twice continuous differentiability, the second order partial derivatives 
are symmetric, meaning that for any i -:/- j and any x E U 
a21 
a2f 
a (x) = 
(x). 
ax. X· 
ax.ax. 
i 
J 
J 
' 

10 
Chapter 1. Mathematical Preliminaries 
The Hessian off at a point x E U is the n x n matrix 
a21 
A 
a?(x) 
1 
2 (x) 
1 
a21 
<Pf 
v2f(x) = ax2Jx1 (x) 
J?(x) 
2 
A 
a21 
n 
1 (x) 
axnax2 (x) 
%<x) 
n 
where all the second order partial derivatives are evaluated at x. Since f is twice con-
tinuously differentiable over U, the Hessian matrix is symmetric. There are two main 
approximation results Qinear and quadratic) which are direct consequences of Taylor's 
approximation theorem that will be used frequently in the book and are thus recalled 
here. 
Theorem 1.24 (linear approximation theorem). Let f : U--+ IR be a twice continuously 
differentiable function over an open set U C Rn, and let x E U, r > 0 sa,tisfy B(x, r) C U. 
Then for any y EB( x, r) there exists ~ E [ x, y] such that 
1 
f(y) = f(x)+ Vf(x)7(y-x)+-(y-x)TV2f(~)(y-x). 
2 
Theorem 1.25 (quadratic approximation theorem). 
Let f: U--+ IR be a twice con· 
tinuously differentiable function over an open set U C Rn, and let x E U, r > 0 sa,tisfy 
B(x, r) C U. Then for any y E B(x, r) 
1 
f (y) = f(x) + \1 f (xf (y-x) + 2(y-x)TV2f (x)(y-x) + o(lly-xl!2). 
Exercises 
1.1. Show that II· 11112 is not a norm. 
1.2. Prove that for any x E IRn one has 
llxlloo = lim llxllp· 
p-+oo 
1.3. Show that for any x,y,z E IRn 
llx-zll < llx-yll + lly-zll· 
1.4. Prove the Cauchy-Schwarz inequality (Lemma 1.5). Show that equality holds if 
and only if the vectors x and y are linearly dependent. 
1.5. Suppose that IRm and IRn are equipped with norms 11 · llb and II· Ila, respectively. 
Show that the induced matrix norm 11 · 1 la,b satisfies the triangle inequality. That is, 
for any A,B E IRmxn the inequality 
I IA+ Bl la,b < I IAI la,b +I IBI la,b 
holds. 

Exercises 
11 
1.6. Let 11·11 be a norm onlRn. Show that the norm functionf(x) = llxll is a continuous 
function over Rn. 
1.7. (attainment of the maximum in the induced norm definition). Suppose that 
Rm and Rn are equipped with norms ll·llb and ll·lla, respectively, and let A E 1Rmxn. 
Show that there exists x E 1Rn such that llxlla < 1 and llAxllb = llAlla,b· 
1.8. Suppose that Rm and Rn are equipped with norms 11 · llb and II· Ila, respectively. 
Show that the induced matrix norm 11 · lla,b can be computed by the formula 
llAlla b = max{llAxllb: llxlla = 1 }. 
' 
x 
1.9. Suppose that Rm and Rn are equipped with norms 11 · llb and II· Ila, respectively. 
Show that the induced matrix norm 11 · 1 la,b can be computed by the formula 
1.10. Let A E 1Rmxn ,BE 1Rnxk and assume that Rm ,Rn, and JRk are equipped with the 
norms II· II,, 11 · llb, and II· Ila, respectively. Prove that 
llABlla,c < llAllb,cllBlla,b• 
1.11. Prove the formula of the co-matrix norm given in Example 1.9. 
1.12. Let A E 1Rmxn. Prove that 
(i) },;llAlloo < llAlb < JmllAlloo, 
(ii) }mllAll1 < llAlb < JnllAll1· 
1.13. Let A E 1Rmxn. Show that 
(i) llAll = llATll (here II· II is the spectral norm), 
(ii) llAll; = :E7=t A;(AT A). 
1.14. Let A E 1Rnxn be a symmetric matrix. Show that 
max{xT Ax: llxll2 = 1} =Amax( A). 
x 
1.15. Prove that a set UC Rn is closed if and only if its complement U' is open. 
1.16. 
(i) Let {Ai }ieI be a collection of open sets where I is a given index set. Show 
that uieIAi is an open Set. Show that if I is finite, then nieJAi is open. 
(ii) Let {Ai} ie/ be a collection of closed sets where I is a given index set. Show 
that nie/Ai is a closed set. Show that if I is finite, then Uie/Ai is closed. 
1.17. Give an example of open sets Ai, i EI for which nie1 Ai is not open. 
1.18. Let A,B C Rn. Prove that cl(A nB) C cl(A)ncl(B). Give an example in which the 
inclusion is proper. 
1.19. Let A,B c Rn. Prove that int(A nB) = int(A) n int(B) and that int(A) U int(B) C 
int(A U B). Show an example in which the latter inclusion is proper. 

Chapter 2 
Optimality Conditions for 
Unconstrained 
Optimization 
2.1 • Global and Local Optima 
Although our main interest in this section is to discuss minimum and maximum points 
of a function over the entire space, we will nonetheless present the more general definition 
of global minimum and maximum points of a function over a given set. 
Definition 2.1 (global minimum and maximum). Let f : S -+ R be defined on a set 
S CRn. Then 
1. x* E S is called a global minimum point off over S if f ( x) > f ( x*) for any x E S, 
2. x* ES is called a strict global minimum point off over S if f(x) > f(x*)for any 
x* -f:. x ES, 
3. x* ES is called a global maximum point off over S if f (x) < f (x*) for any x ES, 
4. x* ES is called a strict global maximum point off over S if f (x) < f (x*) for any 
x* -f:. x ES. 
The set S on which the optimization off is performed is also called the feasible set, 
and any point x E S is called a feasible solution. We will frequently omit the adjective 
"global" and just use the terminology "minimum point" and "maximum point." It is also 
customary to refer to a global minimum point as a minimizer or a global minimizer and 
to a global maximum point as a maximizer or a global maximizer. 
A vector x* E S is called a global optimum off over S if it is either a global minimum 
or a global maximum. The maximal value off over S is defined as the supremum off 
over S: 
max{f(x): x ES}= sup{/ (x): x ES}. 
If x* ES is a global maximum off over S, then the maximum value off over Sis f (x*). 
Similarly the minimal value off over S is the infimum off over S, 
min{f(x): x ES}= inf{f(x): x ES}, 
and is equal to f (x*) when x* is a global minimum off over S. Note that in this book we 
will not use the sup/inf notation but rather use only the min/max notation, where the 
13 

14 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
usage of this notation does not imply that the maximum or minimum is actually attained. 
As opposed to global maximum and minimum points, minimal and maximal values are 
always unique. There could be several global minimum points, but there could be only 
one minimal value. The set of all global minimizers off over S is denoted by 
argmin{/ (x): x ES}, 
and the set of all global maximizers off over S is denoted by 
argmax{/(x): x ES}. 
Note that the notation"/: S--+ JR" means in particular that Sis the domain off, that 
is, the subset of Rn on which f is defined. In Definition 2.1 the minimization and max-
imization is over the domain of the function. However, later on we will also deal with 
functions f : S -+ JR and discuss problems of finding global optima points with respect to 
a subset of the domain. 
Example 2.2. Consider the two-dimensional linear function f(x,y) = x+y defined over 
the unit ball S =B[O, 1] = {(x,y)7 : x2 + y2 < 1}. Then by the Cauchy-Schwarz inequal-
ity we have for any (x,yf ES 
x+y = (x y)G) < J x2+y2v'12+ 12 < J2. 
Therefore, the maximal value off over S is upper bounded by /2. On the other hand, 
the upper bound J2 is attained at (x,y) = ( }i, )i). It is not difficult to see that this is the 
only point that attains this value, and thus ( }i, Ji) is the strict global maximum point of 
f over S, and the maximal value is /2. A similar argument shows that (-Ji, - }i) is the 
strict global minimum point off over S, and the minimal value is -/2. 
I 
Example 2.3. Consider the two-dimensional function 
x+y 
f(x,y)= x2+y2+1 
defined over the entire space JR2• The contour and surface plots of the function are given 
in Figure 2.1. This function has two optima points: a global maximizer ( x, y) = (Ji, Ji) 
and a global minimizer (x,y) = (-Ji,-}i). The proof of these facts will be given in 
Example 2.36. The maximal value of the function is 
1 
1 
72+72 
1 
(Ji )2 + ( }i )2 + 1 - J2' 
and the minimal value is - Ji. 
I 
Our main task will usually be to find and study global minimum or maximum points; 
however, most of the theoretical results only characterize local minima and maxima which 
are optimal points with respect to a neighborhood of the point of interest. The exact 
definitions follow. 

2.1. Global and Local Optima 
5..--~~~~--.-~~~~----, 
4 
3 
~ ~ ~)) 
-~ ( ©.1 
l~ ··:,,, 
-2 
<i I 
\ 
~ 
- 3 
~ 
0 
:.. 
0 
-4 
-5...._~~~~-'-~~~~~~ 
- 5 
0 
5 
.. ·: ... ······ ····· .... . ·········· ····· ····· 
0.8 
...... +·· .... 1·············· 
0.6 
................... 
... ·····1 ........ :········ 
0.4 
.......................... : 
0.2 
0 
- 0.2 
- 0.4 
- 0.6 
Figure 2.1. Contour and surf ace plots off ( x, y) = /+? 1• 
x +y + 
15 
5 
Definition 2.4 (local minima and maxima). Let f : S -. IR be defined on a set S C !Rn. 
Then 
1. x* E S is called a local minimum point off over S if there exists r > 0 for which 
f(x*) < f(x)forany x ES nB(x*, r), 
2. x* E S is called a strict local minimum point off over S if there exists r > 0 for 
which f (x*) < f (x) for any x* "f; x ES n B(x*, r ), 
3. x* E S is called a local maximum point off over S if there exists r > 0 for which 
f(x*) > f(x)for any x ES nB(x*, r), 
4. x* E S is called a strict local maximum point off over S if there exists r > 0 for 
which f(x*) > f(x)forany x* "f; x ES nB(x*, r). 
Of course, a global minimum (maximum) point is also a local minimum (maximum) 
point. As with global minimum and maximum points, we will also use the terminology 
local minimizer and local maximizer for local minimum and maximum points, respec-
tively. 
Example 2.5. Consider the one-dimensional function 
f(x)= 
(x-1)2 +2, 
2, 
-(x-2)2 +2, 
(x-3)2 +1.5, 
-(x-5)2 + 3.5, 
-2x + 14.5, 
2x-11.5, 
-1<x<1, 
1<x<2, 
2 < x < 2.5, 
2.5 < x < 4, 
4< x < 6, 
6 < x < 6.5, 
6.5 < x < 8, 
described in Figure 2.2 and defined over the interval [-1, 8]. The point x = -1 is a strict 
global maximum point. The point x = 1 is a nonstrict local minimum point. All the 
points in the interval (1,2) are nonstrict local minimum points as well as nonstrict local 
maximum points. The point x = 2 is a local maximum point. The point x = 3 is a strict 

16 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
local minimum, and a non-strict global minimum point. The point x = 5 is a strict local 
maximum and x = 6.5 is a strict local minimum, which is a nonstrict global minimum 
point. Finally, x = 8 is a strict local maximum point. Note that, as already mentioned, 
x = 3 and x = 6.5 are both global minimum points of the function, and despite the fact 
that they are strict local minima, they are nonstrict global minimum points. 
I 
x -.... 
5 
4 
3.5 
3 
2 
1.5 
1 
0 
- 1'--~---L-~~L--~-'-~---'~~-'-~--'-~~....._---L-_,_~~ 
- 1 
0 
2 
3 
4 
5 
6 6.5 7 
8 
x 
Figure 2.2. Local and global optimum points of a one-dimensional function. 
First Order Optimality Condition 
A well-known result is that for a one-dimensional function f defined and differentiable 
over an interval (a, b ), if a point x* E (a, b) is a local maximum or minimum, then 
f'(x*) = 0. This is also known as Fermat's theorem. The multidimensional extension 
of this result states that the gradient is zero at local optimum points. We refer to such 
an optimality condition as a first order optimality condition, as it is expressed in terms of 
the first order derivatives. In what follows, we will also discuss second order optimality 
conditions that use in addition information on the second order (partial) derivatives. 
Theorem 2.6 (first order optimality condition for local optima points). Let f: U ~ 
ffi. 
be a function defined on a set U C ffi.n. Suppose that x* E int( U) is a local optimum point 
and that all the partial derivatives off exist at x*. Then "V f ( x*) = 0. 
Proof. Let i E { 1, 2, ... , n} and consider the one-dimensional function g ( t) = f ( x* + t ei ). 
Note that g is differentiable at t = 0 and that g'(O) = ::. (x*). Since x* is a local optimum 
I 
point off, it follows that t = 0 is a local optimum of g, which immediately implies that 
g'(O) = 0. The latter equality is exact~y the same as ; { (x*) = 0. Since this is true for any 
i E { 1,2, ... , n }, the result "V f (x*) = 0 follows. 
D 
Note that the proof of the first order optimality conditions for multivariate functions 
strongly relies on the first order optimality conditions for one-dimensional functions. 

2.2. Classification of Matrices 
17 
Theorem 2.6 presents a necessary optimality condition: the gradient vanishes at all local 
optimum points, which are interior points of the domain of the function; however, the re-
verse claim is not true-there could be points which are not local optimum points, whose 
gradient is zero. For example, the derivative of the one-dimensional function f ( x) = x3 
is zero at x = 0, but this point is neither a local minimum nor a local maximum. Since 
points in which the gradient vanishes are the only candidates for local optima among the 
points in the interior of the domain of the function, they deserve an explicit definition. 
Definition 2.7 (stationary points). Let f : U -. JR be a function defined on a set U C Rn. 
Suppose that x* E int( U) and that f is differentiable over some neighborhood of x*. Tben x* 
is called a stationary point off ifV f (x*) = 0. 
Theorem 2.6 essentially states that local optimum points are necessarily stationary 
points. 
Example 2.8. Considertheone-dimensionalquarticfunctionf(x) = 3x4-20x3+42x2-
36x. To find its local and global optima points over JR, we first find all its stationary points. 
Since f'(x) = 12x3 -60x2 + 84x -36 = 12(x-1)2(x-3), it follows that f'(x) = 0 for 
x = 1,3. The derivative f'(x) does not change its sign when passing through x = 1-it 
is negative before and after-and thus x = 1 is not a local or global optimum point. On 
the other hand, the derivative does change its sign from negative to positive while passing 
through x = 3, and thus it is a local minimum point. Since the function must have a 
global minimum by the property that f(x) ---. oo as lxl ---. oo, it follows that x = 3 is 
the global minimum point. 
I 
2.2 •Classification of Matrices 
In order to be able to characterize the second order optimality conditions, which are 
expressed via the Hessian matrix, the notion of "positive definiteness" must be defined. 
Definition 2.9 (positive definiteness). 
1. A symmetric matrix A E lRnxn is called positive semidefinite, denoted by A>- 0, if 
xT Ax> Oforevery x E lRn. 
2. A symmetric matrix A E lRnxn is called positive definite, denoted by A >- 0, if 
x7 Ax> Oforevery 0 f. x E lRn. 
In this book a positive definite or semidefinite matrix is always assumed to be sym-
metric. 
Positive definiteness of a matrix does not mean that its components are positive, as the 
following examples illustrate. 
Example 2.10. Let 
( 2 -1) 
A= -1 
1 
. 

18 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
Thus, A is positive semidefinite. In fact, since xi +(x1 -x2)2 = 0 if and only if x1 = x2 = 0, 
it follows that A is positive definite. This example illustrates the fact that a positive definite 
matrix might have negative components. 
I 
Example 2.11. Let 
This matrix, whose components are all positive, is not positive definite since for x = 
(1,-tf 
x7 Ax=(l,-l)G i)(-~1)=-2. 
I 
Although, as illustrated in latter examples, not all the components of a positive defi-
nite matrix need to be positive, the following result shows that the signs of the diagonal 
components of a positive definite matrix are positive. 
Lemma 2.12. Let A E Rnxn be a positive definite matrix. Then the diagonal elements of A 
are positive. 
Proof. Since A is positive definite, it follows that ef Aei > 0 for any i E { 1, 2, ... , n}, 
which by the fact that ef Aei = Aii implies the result. 
0 
A similar argument shows that the diagonal elements of a positive semidefinite matrix 
are nonnegative. 
Lemma 2.13. Let A be a positive semidefinite matrix. Then the diagonal elements of A are 
nonnegative. 
Closely related to the notion of positive (semi)definiteness are the notions of negative 
(semi)definiteness and indefiniteness. 
Definition 2.14 (negative (semi)definiteness, indefiniteness). 
1. A symmetric matrix A E Rnxn is called negative semidefinite, denoted by A-< 0, if 
x7 Ax < 0 for every x E Rn. 
2. A symmetric matrix A E Rn xn is called negative definite, denoted by A -< 0, if 
x7 Ax < 0 for every 0 f:. x E Rn. 
3. A symmetric matrix A E Rn xn is called indefinite if there exist x and y E Rn such 
thatx7 Ax>Oandy7 Ay<O. 
It follows immediately from the above definition that a matrix A is negative (semi)-
definite if and only if -A is positive (semi)definite. Therefore, we can prove and state all 
the results for positive (semi)definite matrices and the corresponding results for negative 
(semi)definite matrices will follow immediately. For example, the following result on 
negative definite and negative semidefinite matrices is a direct consequence of Lemmata 
2.12 and 2.13. 

2.2. Classification of Matrices 
19 
Lemma 2.15. 
{a) Let A be a negative definite matrix. Then the di.agonal elements of A are negative. 
{b) Let A be a negative semidefinite matrix. Then the di.agonal elements of A are nonposi-
tive. 
When the diagonal of a matrix contains both positive and negative elements, then the 
matrix is indefinite. The reverse claim is not correct. 
Lemma 2.16. Let A be a symmetric n x n matrix. If there exist positive and negative elements 
in the di.agonal of A, then A is indefinite. 
Proof. Let i,j E {1,2, ... , n} be indices such that Ai,i > 0 and Aj,j < 0. Then 
and hence A is indefinite. 
D 
In addition, a matrix is indefinite if and only if it is neither positive semidefinite nor 
negative semidefinite. It is not an easy task to check the definiteness of a matrix by using 
the definition given above. Therefore, our main task will be to find a useful characteri-
zation of positive {semi)definite matrices. It turns out that a complete charachterization 
can be given in terms of the eigenvalues of the matrix. 
Theorem 2.17 (eigenvalue characterization theorem). Let A be a symmetric n x n ma· 
trix. Then 
{a) A is positive definite if and only if all its eigenvalues are positive, 
{b) A is positive semidefinite if and only if all its eigenvalues are nonnegative, 
{c) A is negative definite if and only if all its eigenvalues are negative, 
{d) A is negative semidefinite if and only if all its eigenvalues are nonpositive, 
{ e) A is indefinite if and only if it has at least one positive eigenvalue and at least one 
negative eigenvalue. 
Proof. We will prove part {a). The other parts follow immediately or by similar argu-
ments. Since A is symmetric, it follows by the spectral decomposition theorem (Theorem 
1.10) that there exist an orthogonal matrix U and a diagonal matrix D = diag{ d1, d2, • •• , dn) 
whose diagonal elements are the eigenvalues of A, for which ur AU= D. Making the lin-
ear transformation of variables x = Uy, we obtain that 
n 
xr Ax=yrur AUy=yrDy= L:diy;. 
i=1 
We can therefore conclude by the nonsingularity of U that xT Ax > 0 for any x f= 0 if and 
only if 
n L: diy; > 0 for any y f= 0. 
{2.1) 
i=1 

20 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
Therefore, we need to prove that (2.1) holds if and only if di > 0 for all i. Indeed, if (2.1) 
holds then for any i E { 1, 2, ... , n }, plugging y = ei in the inequality implies that di > 0. 
On the other hand, if di > 0 for any i, then surely for any nonzero vector y one has 
I:7=t diyf > 0, meaning that (2.1) holds. 
D 
Since the trace and determinant of a symmetric matrix are the sum and product of its 
eigenvalues respectively, a simple consequence of the eigenvalue characterization theorem 
is the following. 
Corollary 2.18. Let A be a positive semidefinite (definite) matrix. Then Tr(A) and det(A) 
are nonnegative (positive). 
Since the eigenvalues of a diagonal matrix are its diagonal elements, it follows that the 
sign of a diagonal matrix is determined by the signs of the elements in its diagonal. 
Lemma 2.19 (sign of diagonal matrices). Let D = diag( d1, d2, • •• , dn ). Then 
(a) Dis positive definite if and only if di > 0 for all i, 
(b) D is positive semidefinite if and only if di > 0 for all i, 
(c) Dis negative definite if and only if di < 0 for all i, 
(d) D is negative semidefinite if and only if di < 0 for all i, 
(e) D is indefinite if and only if there exist i, j such that di > 0 and dj < 0. 
The eigenvalues of a matrix give full information on the sign of the matrix. However, 
we would like to find other simpler methods for detecting positive (semi)definiteness. We 
begin with an extremely simple rule for 2 x 2 matrices stating that for 2 x 2 matrices the 
condition in Corollary 2.18 is necessary and sufficient. 
Proposition 2.20. Let A be a symmetric 2 x 2 matrix. Then A is positive semidefinite 
(definite) if and only ifTr(A),det(A) > 0 (Tr(A),det(A) > 0). 
Proof. We will prove the result for positive semidefiniteness. The result for positive 
definiteness follows from similar arguments. By the eigenvalue characterization theo-
rem (Theorem 2.17), it follows that A is positive semidefinite if and only if .A1(A) > 
0 and .Ai(A) > 0. The result now follows from the simple fact that for any two real 
number a, b E JR one has a, b > 0 if and only if a + b > 0 and a · b > 0. Therefore, 
A is positive semidefinite if and only if .A1(A) + .Ai(A) = Tr(A) > 0 and A1(A).A2(A) = 
det(A) > 0. 
D 
Example 2.21. Consider the matrices 
B=G ! ! ) . 
1 0.1 
The matrix A is positive definite since 
Tr(A) = 4+3 =7> O, 
det(A) =4·3-1·1=11>0. 

2.2. Classification of Matrices 
21 
As for the matrix B, we have 
Tr(B) = 1+1+0.1=2.1>0, 
det(B)=O. 
However, despite the fact that the trace and the determinant of B are nonnegative, we 
cannot conclude that the matrix is positive semidefinite since Proposition 2.20 is valid 
only for 2 x 2 matrices. In this specific example we can show (even without computing 
the eigenvalues) that Bis indefinite. Indeed, 
e'[Be1 >0, 
(ei-e3)TB(ei-e3) = -0.9 < 0. 
I 
For any positive semidefinite matrix A, we can define the square root matrix A! in the 
following way. Let A = UDUT be the spectral decomposition of A; that is, U is an orthog-
onal matrix, and D = diag( d1, di, ... , dn) is a diagonal matrix whose diagonal elements are 
the eigenvalues of A. Since A is positive semidefinite, we have that d1, di, ... , dn > 0, and 
we can define 
! 
T 
Ai =UEU , 
where E = diag( ,./d;_, J(l;_, ... , ,.,./d;,). Obviously, 
The matrix Ai is also called the positive semidefinite square root. 
A well-known test 
for positive definiteness is the principal minors criterion. Given an n x n matrix, the 
determinant of the upper left k x k submatrix is called the kth principal minor and is 
denoted by Dk( A). For example, the principal minors of the 3 x 3 matrix 
are 
The principal minors criterion states that a symmetric matrix is positive definite if and 
only if all its principal minors are positive. 
Theorem 2.22 (principal minors criterion). Let A be an n x n symmetric matrix. Tben 
A is positive definite if and only if D1(A) > O,Di(A) > 0, ... ,Dn(A) > 0. 
Note that the principal minors criterion is a tool for detecting positive definiteness of 
a matrix. It cannot be used in order detect positive semidefiniteness. 
Example 2.23. Let 
A=G ~ 
; ; ) 
, 
2 -1 C=(T 
1 
-4 
1 ! ) . 
-4 

22 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
The matrix A is positive definite since 
(4 2 3) 
D 3(A) = det 2 3 2 = 13 > 0. 
3 2 4 
The principal minors of Bare nonnegative: D 1(B) = 2,D2(B) = D3(B) = O; however, 
since they are not positive, the principal minors criterion does not provide any informa-
tion on the sign of the matrix other than the fact that it is not positive definite. Since the 
matrix has both positive and negative diagonal elements, it is in fact indefinite (see Lemma 
2.16). As for the matrix C, we will show that it is negative definite. For that, we will use 
the principal minors criterion to prove that -C is positive definite: 
( 4 -1) 
D 1(-C) = 4 > O, 
D2(-C) = det _ 1 
4 = 15 > 0, 
D3(-C)=det(~1 ~
1 =~)=50>0. 
I 
-1 -1 
4 
An important class of matrices that are known to be positive semidefinite is the class 
of diagonally dominant matrices. 
Definition 2.24 (diagonally dominant matrices). Let A be a symmetric n x n matrix. 
Then 
1. A is called diagonally dominant if 
for all i = 1,2, ... ,n, 
2. A is called strictly diagonally dominant if 
for all i = 1, 2, ... , n. 
We will now show that diagonally dominant matrices with nonnegative diagonal ele-
ments are positive semidefinite and that strictly diagonally dominant matrices with posi-
tive diagonal elements are positive definite. 
Theorem 2.25 (positive semidefi.niteness of diagonally dominant matrices). 
(a) Let A be a symmetric n x n diagonally dominant matrix whose diagonal elements are 
nonnegative. Then A is positive semidefinite. 
(b) Let A be a symmetric n x n strictly diagonally dominant matrix whose diagonal ele-
ments are positive. Then A is positive definite. 
Proof. (a) Suppose in contradiction that there exists a negative eigenvalue A of A, and let u 
be a corresponding eigenvector. Let i e { 1, 2, ... , n} be an index for which I ui I is maximal 

2.3. Second Order Optimality Conditions 
23 
among ju1j, luil, ... , lunl· Then by the equality Au= AU we have 
implying that IAii -
..1.1 < IAii I, which is a contradiction to the negativity of). and the 
nonnegativity of Aii. 
(b) Since by part (a) we know that A is positive semidefinite, all we need to show is 
that A has no zero eigenvalues. Suppose in contradiction that there is a zero eigenvalue, 
meaning that there is a vector u f; 0 such that Au = 0. Then, similarly to the proof of 
part (a), let i E {1,2, ... ,n} be an index for which luil is maximal among lu11, luil, ... , lun I, 
and we obtain 
which is obviously impossible, establishing the fact that A is positive definite. 
0 
2.3 • Second Order Optimality Conditions 
We begin by stating the necessary second order optimality condition. 
Theorem 2.26 (necessary second order optimality conditions). Let f : U -+ IR be a 
function defined on an open set U C Rn. Suppose that f is twice continuously differentiable 
over U and that x* is a stationary point. Then the following hold: 
(a) If x* is a local minimum point off over U, then vi f (x*) >- 0. 
(b) If x* is a local maximum point off over U, then vi f ( x*) --< 0. 
Proof. (a) Since x* is a local minimum point, there exists a ball B(x*, r) C U for which 
f(x) > f(x*) for all x E B(x*, r). Let d E Rn be a nonzero vector. For any 0 <a< rrarr' 
we have x; = 
x* +ad E B( x*, r ), and hence for any such a 
f(x~) > f(x*). 
(2.2) 
On the other hand, by the linear approximation theorem (Theorem 1.24), it follows that 
there exists a vector za E [ x*, x; J such that 
Since x* is a stationary point of f, and by the definition of x:, the latter equation re-
duces to 
(2.3) 
Combining (2.2) and (2.3), it follows that for any a E (0, lldll) the inequality drvi f ( z0)d > 
0 holds. Finally, using the fact that za -+ x* as a -+ o+, and the continuity of the Hessian, 

24 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
we obtain that dTV' f (x*)d > 0. Since the latter inequality holds for any d E Rn, the 
desired result is established. 
(b) The proof of (b) follows immediately by employing the result of part (a) on the 
function -f. 
D 
The latter result is a necessary condition for local optimality. The next theorem states 
a sufficient condition for strict local optimality. 
Theorem 2.27 (sufficient second order optimality condition). Let f : U --+ lR be a 
function defined on an open set U C Rn. Suppose that f is twice continuously differentiable 
over U and that x* is a stationary point. 1ben the following hold: 
(a) If V'2 f (x*) >- 0, then x* is a strict local minimum point off over U. 
(b) If V'2 f (x*) --( 0, then x* is a strict local maximum point off over U. 
Proof. We will prove part (a). Part (b) follows by considering the function -f. Suppose 
then that x* is a stationary point satisfying V'2f(x*) >- 0. Since the Hessian is continuous, 
it follows that there exists a ball B(x*, r) C U for which V'2f (x) >- 0 for any x E B(x*, r). 
By the linear approximation theorem (Theorem 1.24), it follows that for any x E B( x*, r ), 
there exists a vector zx E [ x*, x] (and hence zx E B(x*, r )) for which 
(2.4) 
Since V'2f(zx) >- 0, it follows by (2.4) that for any x E B(x*, r) such that x f:. x*, the 
inequality f(x) > f(x*) holds, implying that x* is a strict local minimum point off 
over U. 
D 
Note that the sufficient condition implies the stronger property of strict local opti-
mality. However, positive definiteness of the Hessian matrix is not a necessary condition 
for strict local optimality. For example, the one-dimensional function f(x) = x4 over lR 
has a strict local minimum at x = 0, but f"(O) is not positive. Another important concept 
is that of a saddle point. 
Definition 2.28 (saddle point). Let f : U --+ lR be a function defined on an open set 
U C Rn. Suppose that f is continuously differentiable over U. A stationary point x* is called 
a saddle point off over U if it is neither a local minimum point nor a local maximum point 
off over U. 
A sufficient condition for a stationary point to be a saddle point in terms of the prop-
erties of the Hessian is given in the next result. 
Theorem 2.29 (sufficient condition for a saddle point). Let f : U --+ lR be a function 
defined on an open set U C Rn. Suppose that f is twice continuously differentiable over U 
and that x* is a stationary point. If V'2 f ( x*) is an indefinite matrix, then x* is a saddle point 
off over U. 
Proof. Since V'2 f (x*) is indefinite, it has at least one positive eigenvalue .A > 0, corre-
sponding to a normalized eigenvector which we will denote by v. Since U is an open set, 
it follows that there exists a positive real r > 0 such that x* +av E U for any a E (0, r ). 

2.3. Second Order Optimality Conditions 
25 
By the quadratic approximation theorem (Theorem 1.25) and recalling that V f(x*) = 0, 
we have that there exists a function g : IR++ -+ IR satisfying 
g(t) 
-
-+0 as t -+0, 
(2.5) 
t 
such that for any a E (0, r) 
a2 
f(x* +av)= f(x*)+-vTV2f(x*)v+ g(a2llv!12) 
2 
Aa2 
= f(x*) + -llvll2 + g(llvll2a2). 
2 
Since llvll = 1, the latter can be rewritten as 
Aa2 
f(x* +av)= f(x*)+- + g(a2). 
2 
By (2.5) it follows that there exists an e 1 E (0, r) such that g( a 2) >-~a2 for all a E (0, e 1), 
and hence f(x* +av) > f(x*) for all a E (0,e1). This shows that x* cannot be a local 
maximum point off over U. A similar argument-exploiting an eigenvector of V 2f (x*) 
corresponding to a negative eigenvalue-shows that x* cannot be a local minimum point 
off over U, establishing the desired result that x* is a saddle point. 
D 
Another important issue is the one of deciding on whether a function actually has a 
global minimizer or maximizer. This is the issue of attainment or existence. A very well 
known result is due to Weierstrass, stating that a continuous function attains its minimum 
and maximum over a compact set. 
Theorem 2.30 (Weierstrass theorem). 
Let f be a continuous function defined over a 
nonempty and compact set C C Rn. Then there exists a global minimum point off over C 
and a global maximum point off over C. 
When the underlying set is not compact, the Weierstrass theorem does not guarantee 
the attainment of the solution, but certain properties of the function f can imply attain-
ment of the solution even in the noncompact setting. One example of such a property is 
coerciveness. 
Definition 2.31 (coerciveness). Let f : Rn -+ IR be a continuous function defined over Rn. 
The function f is called coercive if 
lim f(x) = oo. 
llxll-+oo 
The important property of coercive functions that will be frequently used in this book 
is that a coercive function always attains a global minimum point on any closed set. 
Theorem 2.32 (attainment under coerciveness). Let f: Rn -+IR be a continuous and 
coercive function and let S C Rn be a nonempty closed set. Then f has a global minimum 
point over S. 
Proof. Let Xo E S be an arbitrary point in S. Since the function is coercive, it follows that 
there exists an M > 0 such that 
f(x) > f(Xo) for any x such that llxll >M. 
(2.6) 

26 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
Since any global minimizer x* off over S satisfies f(x*) </(Ko), it follows from (2.6) 
that the set of global minimizers off over S is the same as the set of global minimizers 
off over Sn B[O,M]. The set Sn B[O,M] is compact and nonempty, and thus by the 
Weierstrass theorem, there exists a global minimizer off over S nB[O,M] and hence also 
over S. 
0 
Example 2.33. Consider the function f (x1, x2) = x: +xi over the set 
C = {(x1,x2): x1 +x2 <-1}. 
The set C is not bounded, and thus the Weierstrass theorem does not guarantee the exis-
tence of a global minimizer off over C, but since f is coercive and C is closed, Theorem 
2.32 does guarantee the existence of such a global minimizer. It is also not a difficult task 
to find the global minimum point in this example. There are two options: In one op-
tion the global minimum point is in the interior of C, and in that case by Theorem 2.6 
"\! f ( x) = 0, meaning that x = 0, which is impossible since the zeros vector is not in C. The 
other option is that the global minimum point is attained at the boundary of C given by 
bd( C) = { ( x1, Xi) : x1 + x2 = -1}. We can then substitute x1 = -x2 -1 into the objective 
function and recast the problem as the one-dimensional optimization problem of mini-
mizing g(x2) = (-1-x2)2 +xi over JR. Since g'(x2) = 2(1+x2)+2x2, it follows that g' 
has a single root, which is x2 = -0.5, and hence x1 = -0.5. Since (x1,x2) = (-0.5,-0.5) 
is the only candidate for a global minimum point, and since there must be at least one 
global minimizer, it follows that (x1,x2) = (-0.5,-0.5) is the global minimum point off 
over C. 
I 
Example 2.34. Consider the function 
f(x 1,x2) = 2x: +Jxi +3x;x2-24x2 
over 1R2• Let us find all the stationary points off over 1R2 and classify them. First, 
"\! f(x) = ( 6x: + 6;1 x2 ) • 
6x2 +3x1 -24 
Therefore, the stationary points are those satisfying 
6x: +6x1x2 = 0, 
6x2 + Jx;-24 = 0. 
The first equation is the same as 6x1 ( x1 + x2) = 0, meaning that either x1 = 0 or x1 + x2 = 0. 
If x1 = 0, then by the second equation x2 = 4. If x1 +x2 = 0, then substituting x1 = -x2 in 
the second equation yields the equation 3x:-6x1 -24 = 0 whose solutions are x1 = 4,-2. 
Overall, the stationary points of the function f are ( 0, 4 ), ( 4, -4 ), (-2, 2). The Hessian of 
f is given by 
'\72/(x,,x,) = 6(2\~X2 ~'). 
For the stationary point (0,4) we have 

2.3. Second Order Optimality Conditions 
27 
which is a positive definite matrix as a diagonal matrix with positive components in the 
diagonal (see Lemma 2.19). Therefore, (0,4) is a strict local minimum point. It is not a 
global minimum point since such a point does not exist as the function f is not bounded 
below: 
f(xpO) = 2xi-+ -oo as x1 -+-oo. 
The Hessian off at (4,-4) is 
v 2/(4,-4) = 6(: 1). 
Since det(V2/(4,-4)) = -62 • 12 < 0, it follows that the Hessian at (4,-4) is indefinite. 
Indeed, the determinant is the product of its two eigenvalues - one must be positive and 
the other negative. Therefore, by Theorem 2.29, ( 4, -4) is a saddle point. The Hessian at 
(-2,2) is 
2 
(-2 -2) 
v f(-2,2)=6 -2 
1 ' 
which is indefinite by the fact that it has both positive and negative elements on its diag-
onal. Therefore, (-2,2) is a saddle point. To summarize, (0,4) is a strict local minimum 
point off, and (4,-4),(-2,2) are saddle points. 
I 
Example 2.35. Let 
f(x 1, x2) = (x; +xi-1)2 +(xi-1)2• 
The gradient off is given by 
Vf ( ) _ 4 ( 
( x: + x;- 1 )x1 
) 
x -
(x: +xi-1)x2 +(xi-1)x2 · 
The stationary points are those satisfying 
( x; + xi - 1 )x1 = 0, 
(x; + xi-1)x2 +(xi-1)x2 = 0. 
(2.7) 
(2.8) 
By equation (2.7), there are two cases: either x1 = 0, and then by equation (2.8) x2 is 
equal to one of the values 0, 1,-1; the second option is that x: +xi = 1, and then by 
equation (2.8), we have that x2 = 0,±1 and hence x1 is ±1,0 respectively. Overall, there 
are 5 stationary points: (0,0),(1,0),(-1,0),(0, 1),(0,-1). The Hessian of the function is 
V2f(x) = 4 (3x: +x;-1 
2X1X2 
) 
2X1X2 
x: +6x;-2 ' 
Since 
V2/(0,0)=4(~1 _'.'2)-<o, 
it follows that (0,0) is a strict local maximum point. By the fact that f(x1,0) = (x:-
1 )2 + 1 -+ oo as x1 -+ oo, the function is not bounded above and thus (0, 0) is not a global 
maximum point. Also, 

28 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
which is an indefinite matrix and hence (1,0) and (-1,0) are saddle points. Finally, 
V'2f(O, I)= V'2 f(0,-1) = 4(~ ~) >- 0. 
The fact that the Hessian matrices off at (0, 1) and (0,-1) are positive semidefinite is not 
enough in order to conclude that these are local minimum points; they might be saddle 
points. However, in this case it is not difficult to see that (0, 1) and (0,-1) are in fact 
global minimum points since f (0, 1) = f (0,-1) = 0, and the function is lower bounded 
by zero. Note that since there are two global minimum points, they are nonstrict global 
minima, but they actually are strict local minimum points since each has a neighborhood 
in which it is the unique minimizer. The contour and surface plots of the function are 
plotted in Figure 2.3. 
I 
.. ······1··· ···:··· 
.. ···: 
: 
.. ···< L ·r ····(···"(····· 
:: .. :·_·_::•.=.·:·.· .. ·.·· .. ·.·1.=::.··: .... ;,_:, 
-~ · · · ·· · ·t·· · ····~······· 
. . . . : 
: 
...... ~ ..... . · l. " ... . ~: 
. 'f ....... · ~:· ..... . 
10 
. . 
. .· 
·· ····~··· .... ; 
···= ....... : ... . . 
.· 
: ..... ·( 
····: 
8 
.... 
6 
4 
2 
2 
Figure 2.3. Contour and surface plots off (xl' Xi) = (x: + xi-1 )2 +(xi - 1 )2. The five 
stationary points (0,0),(0, 1),(0,-1),(1,0),(-1,0) are denoted by asterisks. The points (0,-1),(0, 1) 
are strict local minimum points as well as global minimum points, (0, 0) is a local maximum point, and 
(-1, 0), ( 1, 0) are saddle points. 
Example 2.36. Returning to Example 2.3, we will now investigate the stationary points 
of the function 
x+y 
f(x,y) = 2 
2 
• 
x +y +1 
The gradient of the function is 
V 
_ 
1 
((x2+y2+1)-2(x +y)x) 
f(x,y)- (x2+y2+1)2 (x2+y2+1)-2(x +y)y · 
Therefore, the stationary points of the function are those satisfying 
-x2 -2xy+y2 =-1, 
2 
2 
2 -
1 
x -
xy-y -- . 

2.3. Second Order Optimality Conditions 
29 
Adding the two equations yields the equation xy = ! . Subtracting the two equations 
yields x2 = y2, which, along with the fact that x and y have the same sign, implies that 
x = y. Therefore, we obtain that 
1 
x2=-
2 
whose solutions are x = ±Ji. Thus, the function has two stationary points which are 
()i, }z) and (-)i,-}z). We will now prove that the statement given in Example 2.3 is 
correct: ( Ji, Ji) is the global maximum point off over R2 and (-Ji, - Ji) is the global 
minimum point off over R2. Indeed, note that f()i, }z) = Ji· In addition, for any 
(x,yf E R2, 
x+y 
· Jx 2 +y2 
t 
f(x,y) = 
2 
2 
< J2 2 
2 
< v'2.max 2 
' 
x + y + 1 
x + y + 1 
t~ t + 1 
where the first inequality follows from the Cauchy-Schwarz inequality. Since for any 
t > 0, the inequality t2+1>2t holds, we have that f(x,y) < Ji for any (x,y)T E R2. 
Therefore, (Ji, }z) attains the maximal value of the function and is therefore the global 
maximum point off over R2. A similar argument shows that (-Ji, - Ji) is the global 
minimum point off over R2. 
I 
Example 2.37. Let 
f(xi,x2) =-2x; +xi xi +4xi. 
The gradient of the function is 
V f (x) = (-4xi + x~ + 16xi), 
2XiX2 
and the stationary points are those satisfying the system of equations 
-4xi +xi+ 16x: = 0, 
2XiX2 = 0. 
By the second equation, we obtain that either Xi = 0 or x2 = 0 (or both). If xi = 0, then 
by the first equation, we have x2 = 0. If x2 = 0, then by the first equation-4x1+16xi = 0 
and thus x1 (-1 + 4x:) = 0, so that x1 is one of the three valu~s 0, 0.5, -0.5. The function 
therefore has three stationary points: (0,0),(0.5,0),(-0.5,0). The Hessian off is 
't"72/ ( 
) _ (-4 + 48x; 
2x2) 
v 
XpX2 -
2 
2 
• 
X2 
Xi 
Since 
V2f(0.5,0} = (~ ~) >-0, 
it follows that (0.5, 0) is a strict local minimum point off. It is not a global minimum 
point since the function is not bounded below: /(-1, x2) = 2-x~ -+-oo as x2 -+ oo. As 
for the stationary point (-0.5, 0), 
v2f(--0.s,o)=(~ ~1). 

30 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
and hence since the Hessian is indefinite, (-0.5, 0) is a saddle point of f. Finally, the 
Hessian off at the stationary point (0, 0) is 
2 
(-4 0) 
V' /(0,0)= 
0 
0 . 
The fact that the Hessian is negative semidefinite implies that (0, 0) is either a local maxi-
mum point or a saddle point off. Note that 
f(a4,a) =-2a8 +a6 +4a16 =a6(-2a2 +1 +4a10 ). 
It is easy to see that for a small enough a > 0, the above expression is positive. Similarly, 
f(-a4, a) =-2a8 -a6 +4a16 = a6(-2a2 -1+4a10), 
and the above expression is negative for small enough a> 0. This means that (0,0) is a 
saddle point since at any one of its neighborhoods, we can find points with smaller values 
than f (0, 0) = 0 and points with larger values than f (0, 0) = 0. The surface and contour 
plots of the function are given in Figure 2.4. 
I 
1 
II 
0.8 
0.6 
0.4 
0.2 
0 
• 
- 0.2 
- 0.4 
- 0.6 
- 0.8 
- 1 
- 1 
- 0.5 
0 
I 
I 
Ii) 
0.5 
1 
3 
1.5 
1 
0.5 
0 
- 0.5 
-1 
1 
0.5 
1 
Figure 2.4. Contour and surf ace plots off ( x1, :xz) = -2x:+ x1 x; +4x:. The three stationary 
point (0, 0), (0.5, 0), (-0.5, 0) are denoted by asterisks. The point (0.5, 0) is a strict local minimum, while 
(0,0) and (-0.5,0) are saddle points. 
2.4 • Global Optimality Conditions 
The conditions described in the last section can only guarantee-at best-local optimality 
of stationary points since they exploit only local information: the values of the gradient 
and the Hessian at a given point. Conditions that ensure global optimality of points must 
use global information. For example, when the Hessian of the function is always positive 
semidefinite, all the stationary points are also global minimum points. Later on, we will 
refer to this property as convexity. 

2.4. Global Optimality Conditions 
31 
Theorem 2.38. Let f be a twice continuously differentiable function defined over Rn. Sup-
pose that V2 f (x) >- 0 for any x E Rn. Let x* E Rn be a stationary point off. Then x* is a 
global minimum point off. 
Proof. By the linear approximation theorem (Theorem 1.24), it follows that for any x E 
Rn, there exists a vector zx E [ x*, x] for which 
Since V2f(zx) >- 0, we have that f(x) > f(x*), establishing the fact that x* is a global 
minimum point off. 
0 
Example 2.39. Let 
Then 
(
2x1 +x2 +x3 +4x1(x; +xi +xi)) 
Vf(x)= 2x2+x1 +x3+4xi(x;+xi+xi) . 
2x3 + x1+x2+4x3(x; +xi+ xi) 
Obviously, x = 0 is a stationary point. We will show that it is a global minimum point. 
The Hessian off is 
The Hessian is positive semidefinite since it can be be written as the sum 
V 2f(x) = A+B(x)+C(x), 
where 
1 1) 
2 1 , 
1 2 
The above three matrices are positive semidefinite for any x E R3; indeed, A >- 0 since it is 
diagnoally dominant with positive diagonal elements. The matrix B(x), as a nonnegative 
multiplier of the identity matrix, is positive semidefinite, and finally, 
C(x) = 8xx7 , 
and hence C(x) is positive semidefinite as a positive multiply of the matrix xx7 , which 
is positive semidefinite (see Exercise 2.6 ). To summarize, V2 f ( x) is positive semidefinite 
as the sum of three positive semidefnite matrices (simple extension of Exercise 2.4), and 
hence, by Theorem 2.38, x = 0 is a global minimum point off over R3• 
I 

32 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
2.5 • Quadratic Functions 
Quadratic functions are an important class of functions that are useful in the modeling 
of many optimization problems. We will now define and derive some of the basic results 
related to this important class of functions. 
Definition 2.40. A quadratic function over Rn is a function of the form 
f(x) = x7 Ax+2b7 x+c, 
(2.9) 
where A E Rnxn is symmetric, b E Rn, and c E JR. 
We will frequently refer to the matrix A in (2. 9) as the matrix associated with the 
quadratic function f. The gradient and Hessian of a quadratic function have simple ana-
lytic formulas: 
Vf(x) = 2Ax+2b, 
V2f(x)=2A. 
(2.10) 
(2.11) 
By the above formulas we can deduce several important properties of quadratic functions, 
which are associated with their stationary points. 
Lemma 2.41. Let f(x) = x7 Ax+ 2b7 x + c, where A E Rnxn is symmetric, b E Rn, and 
c ER. Then 
(a) x is a stationary point off if and only if Ax = -b, 
(b) if A >- 0, then x is a global minimum point off if and only if Ax = -b, 
(c) if A >- 0, then x = -A-1b is a strict global minimum point off. 
Proof. (a) The proof of (a) follows immediately from the formula of the gradient of f 
(equation (2.10)). 
(b) Since V2f(x) = 2A >- 0, it follows by Theorem 2.38 that the global minimum 
points are exactly the stationary points, which combined with part (a) implies the result. 
(c) When A>- 0, the vector x =-A-1b is the unique solution to Ax =-b, and hence 
by parts (a) and (b), it is the unique global minimum point off. 
0 
We note that when A >- 0, the global minimizer off is x* = -A-1 b, and consequently 
the minimal value of the function is 
f (x*) = (x*)7 Ax*+ 2b7 x* + c 
=(-A-1b)7 A(-A-1b)-2b7 A-1b+c 
=c-b7A-1b. 
Another useful property of quadratic functions is that they are coercive if and only if the 
associated matrix A is positive definite. 
Lemma 2.42 (coerciveness of quadratic functions). Let f(x) = x7 Ax+2b7 x+c, where 
A E Rnxn is symmetric, b E Rn, and c E JR. Then f is coercive if and only if A>- 0. 

2.5. Quadratic Functions 
33 
Proof. If A>- 0, then by Lemma 1.11, xT Ax> allx112 for a= .Amin(A) > 0. We can thus 
wnte 
( 
llhll) 
/(x) > allxll2 +2bT x+c > allxll2-2llhll · llxll + c = allxll llxll-2-;-
+ c, 
where we also used the Cauchy-Schwarz inequality. Since obviously allxll(llxll-2~)+ 
c--+ oo as llxll--+ oo, it follows that /(x)--+ oo as llxll--+ oo, establishing the coerciveness 
off. Now assume that f is coercive. We need to prove that A is positive definite, or 
in other words that all its eigenvalues are positive. We begin by showing that there does 
not exist a negative eigenvalue. Suppose in contradiction that there exists such a negative 
eigenvalue; that is, there exists a nonzero vector v E Rn and A< 0 such that Av= .Av. 
Then 
/(av)= .Allvll2a2 +2(bT v)a +c --+-oo 
as a tends to oo, thus contradicting the assumption that f is coercive. We thus conclude 
that all the eigenvalues of A are nonnegative. We will show that 0 cannot be an eigen-
value of A. By contradiction assume that there exists v f:. 0 such that Av = 0. Then for 
any a ER 
/(av)= 2(bT v)a +c. 
Then if bT v = 0, we have /(av) --+ c as a --+ oo. If bT v > 0, then /(av) --+ -oo as 
a--+ -oo, and if bT v < 0, then /(av)--+ -oo as a--+ oo, contradicting the coerciveness 
of the function. We have thus proven that A is positive definite. 
D 
The last result describes an important characterization of the property that a quad-
ratic function is nonnegative over the entire space. It is a generalization of the property 
that A >- 0 if and only if xT Ax > 0 for any x E Rn. 
Theorem 2.43 (characterization of the nonnegativity of quadratic functions). Let 
f (x) = xT Ax+ 2bT x + c, where A E Rnxn is symmetric, b E Rn, and c E JR.. Then the 
following two claims are equivalent: 
(a) /(x)= xT Ax+2hT x+c > Oforall xERn. 
(b) ( ~ ~) >- 0. 
Proof. Suppose that (b) holds. Then in particular for any x E Rn the inequality 
holds, which is the same as the inequality xT Ax+2hT x+c > 0, proving the validity of (a). 
Now, assume that (a) holds. We begin by showing that A >- 0. Suppose in contradiction 
that A is not positive semidefinite. Then there exists an eigenvector v corresponding to a 
negative eigenvalue A < 0 of A: 
Av=.Av. 
Thus, for any a E JR. 
/(av)= .Allvll2a2 +2(bT v)a+c --+-oo 

34 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
as a~ -oo, contradicting the nonnegativity off. Our objective is to prove (b); that is, 
we want to show that for any y E Rn and t E JR 
which is equivalent to 
(2.12) 
To show the validity of (2.12) for any y E Rn and t e R, we consider two cases. If t = 0, 
then (2.12) reads as y7 Ay > 0, which is a valid inequality since we have shown that A >- 0. 
The second case is when t f:. 0. To show that (2.12) holds in this case, note that (2.12) 
is the same as the inequality 
which holds true by the nonnegativity off. 
0 
Exercises 
2.1. Find the global minimum and maximum points of the function f ( x, y) = x2 +y2 + 
2x-3y over the unit ball S = B[O, 1] = {(x,y): x2 +y2 < 1}. 
2.2. Let a E Rn be a nonzero vector. Show that the maximum of a7 x over B[O, 1] = 
{x E Rn : llxll < 1} is attained at x* = 11: 11 and that the maximal value is llall· 
2.3. Find the global minimum and maximum points of the function f ( x, y) = 2x - 3y 
over the set S = {(x,y): 2x2 + 5y2 < 1}. 
2.4. Show that if A,B are n x n positive semidefinite matrices, then their sum A+B is 
also positive semidefinite. 
2.5. Let A E Rnxn and BE Rmxm be two symmetric matrices. Prove that the following 
two claims are equivalent: 
{i) A and B are positive semidefinite. 
{") ( A 0 
) . 
• • 
'd fi . 
u 
0 
'Bm is pos1t1ve semi e mte. 
mxn 
2.6. Let BE Rnxk and let A= BB7 . 
(i) Prove A is positive semidefinite. 
{ii) Prove that A is positive definite if and only if B has a full row rank. 
2.7. 
(i) Let A be an n x n symmetric matrix. Show that A is positive semidefinite if 
and only if there exists a matrix BE Rnxn such that A= BBT. 
{ii) Let x E Rn and let A be defined as 
Aij = xixj, i,j = 1,2, ... ,n. 
Show that A is positive semidefinite and that it is not a positive definite matrix 
when n > 1. 

Exercises 
35 
2.8. Let Q E IR.nxn be a positive definite matrix. Show that the "Q-norm" defined by 
is indeed a norm. 
2. 9. Let A be an n x n positive semidefinite matrix. 
(i) Show that for any i f:. j 
Ai;A;; > A;i' 
(ti) Show that if for some i E { 1, 2, ... , n} Aii = 0, then the ith row of A consists 
of zeros. 
2.10. Let Aa be then x n matrix (n > 1) defined by 
Aa -{ a, i = j, 
ij -
1, 
i f:. j. 
Show that A a is positive semidefinite if and only if a > 1. 
2.11. Let d E ~n (~n being the unit-simplex). Show that then x n matrix A defined by 
A··={ 
,, 
is positive semidefinite. 
di-df, 
-d-d. 
' I' 
2.12. Prove that a 2 x 2 matrix A is negative semidefinite if and only if Tr(A) < 0 and 
det(A) > 0. 
2.13. For each of the following matrices determine whether they are positive/negative 
semidefinite/ definite or indefinite: 
2 2 0 0 
(i) A= ; 
; 
~ ~ . 
0 0 1 3 
(ti) B=G ~ D· 
(iii) c = (i ~ ~) . 
3 1 2 
(iv) D = (~
5 ~7 
~ ) . 
1 
1 
-5 
2.14. (Schur complement lemma) Let 
where A E IR.nxn, b E Rn, c E IR. Suppose that A>- 0. Prove that D >- 0 if and only 
if c-bT A-1b > 0. 

36 
Chapter 2. Optimality Conditions for Unconstrained Optimization 
2.15. For each of the following functions, determine whether it is coercive or not: 
(i) f(x 1, xi)= xi+ xj. 
(ii) f ( Xp Xi) = ex: + ex~ - x:oo - xioo. 
(iii) f(xp xi)= 2x:-8x1 Xi+ xi. 
(iv) f(x 1,xi) = 4x: +2x1xi +2x~. 
(v) f(x 1, Xi, x3) =xi+ x~ +xi. 
(vi) f (x1, xi)= x:-2x1 xi+ x;. 
{vii) f (x) = 1j;~~, where A E JRnxn is positive definite. 
2.16. Find a function f : JRi -+JR which is not coercive and satisfies that for any a E JR 
2.17. For each of the following functions, find all the stationary points and classify 
them according to whether they are saddle points, strict/nonstrict local/global 
minimum/ maximum points: 
(i) f(x1,xi)=(4x:-xi)i. 
{ii) f (x1, Xi, x3) = xi-2xi +xi+ 2xix3 + 2xi. 
{iii) f (x1, xi)= 2x~ -6x~ + 3x:xi. 
(iv) f(xpxi)=xi+2x:xi+x~-4x:-8x1 -8xi. 
{v) f(x 1, xi)= (x1-2xi)4 +64x1 Xi· 
(vi) f(xpxi) = 2x: +3xi-2x1xi +2x1 -3xi. 
(vii) f(x 1,xi)=x:+4x1xi+xi+x1-xi. 
2.18. Let/ be twice continuously differentiable function over Rn. Suppose that vif(x) 
>- 0 for any x E Rn. Prove that a stationary point off is necessarily a strict global 
minimum point. 
2.19. Let f (x) = xT Ax+ 2bT x + c, where A E JRnxn is symmetric, b E ]Rn, and c E JR. 
Suppose that A>- 0. Show that f is bounded below1 over Rn if and only if b E 
Range(A) = {Ay: ye JRn}. 
1 A function f is bounded below over a set C if there exists a constant a such that f (x) ~a for any x EC. 

Chapter 3 
Least Squares 
3.1 • "Solution" of Overdetermined Systems 
Suppose that we are given a linear system of the form 
Ax=h, 
where A E Rmxn and b E Rm. Assume that the system is overdetermined, meaning that 
m > n. In addition, we assume that A has a full column rank; that is, rank(A) = n. In 
this setting, the system is usually inconsistent (has no solution) and a common approach 
for finding an approximate solution is to pick the solution resulting with the minimal 
squared norm of the residual r = Ax - b: 
(LS) 
min I IAx - hi 12• 
xe!Rn 
Problem (LS) is a problem of minimizing a quadratic function over the entire space. The 
quadratic objective function is given by 
Since A is of full column rank, it follows that for any x E Rn it holds that V2/(x) = 
2AT A >- 0. (Otherwise, if A if not of full column, only positive semidefiniteness can be 
guaranteed.) Hence, by Lemma 2.41 the unique stationary point 
(3.1) 
is the optimal solution of problem (LS). The vector xLs is called the least squares solution 
or the least squares estimate of the system Ax = b. It is quite common not to write the 
explicit expression for xLs but instead to write the associated system of equations that 
defines it: 
(AT A)xLs =Arb. 
The above system of equations is called the normal system. We can actually omit the as-
sumption that the system is overdetermined and just keep the assumption that A is of full 
column rank. Under this assumption m > n, and in the case when m = n, the matrix A 
is nonsingular and the least squares solution is actually the solution of the linear system, 
that is, A-1b. 
37 

38 
Example 3.1. Consider the inconsistent linear system 
Xt +2x2 = 0, 
2x1+x2 =1, 
3x1 +2x2 =1. 
Chapter 3. Least Squares 
We will denote by A and b the coefficients matrix and right-hand-side vector of the sys-
tem. The least squares problem can be explicitly written as 
min(x1 +2x2)2+(2x1 +x2-1)2+(3x1 +2x2-1)2• 
XpX2 
Essentially, the solution to the above problem is the vector that yields the minimal sum 
of squares of the errors corresponding to the three equations. To find the least squares 
solution, we will solve the normal equations: 
G ~r G D(~)=G ~r G). 
which are the same as 
G6 ~0)(~)=G). 
The solution of the above system is the least squares estimate: 
( 15/26) 
XLS= -8/26 . 
Note that 
AxLs = (~s~~
8) , 
1.115 
so that the residual vector containing the errors in each the equations is 
( -0.038) 
AxLs -b = -0.154 . 
0.115 
The total sum of squares of the errors, which is the optimal value of the least squares 
problem is (-0.038)2 + (-0.154)2 +0.1152 = 0.038. 
I 
In MATLAB, finding the least squares solution is a very easy task. 
MATLAB Implementation 
To find the least squares solution of an overdetermined linear system Ax =bin MATLAB, 
the backslash operator \ should be used. Therefore, Example 3.1 can be solved by the 
following commands 
>>A =[l,2;2,1;3,2]; 
>> b= [ 0; 1i1] ; 
>> format rational; 
>> A\b 
ans = 
15/26 
-4/13 

3.2. Data Fitting 
39 
3.2 • Data Fitting 
One area in which least squares is being frequently used is data fitting. We begin by 
describing the problem of linear fitting. Suppose that we are given a set of data points 
(s;, t;), i = 1,2, ... , m, where si E Rn and ti ER, and assume that a linear relation of the 
form 
ti =sf x, 
i = 1, 2, ... , m, 
approximately holds. In the least squares approach the objective is to find the parame-
ters vector x E Rn that solves the problem 
m 
min "'"'\s! x-ti)2. 
xeRn~ ' 
i=l 
We can alternatively write the problem as 
min I ISx - ti 1
2, 
xeRn 
where 
T 
-s-
1 
T 
-s-
S= 
2 
t= 
T 
-s -
m 
ti 
t2 
tm 
Example 3.2. Consider the 30 points in R2 described in the left image of Figure 3 .1. The 
30 x-coordinates are xi = (i-1)/29, i = 1,2, ... , 30, and the corresponding y-coordinates 
are defined by Yi = 2xi + 1 + e i, where for every i, e i is randomly generated from a stan-
dard normal distribution with zero mean and standard deviation of 0.1. The MATLAB 
commands that generated the points and plotted them are 
randn('seed',319); 
d=linspace(0,1,30)'; 
e=2*d+l+0.1*randn(30,1); 
plot(d,e,'*') 
Note that we have used the command randn ('seed', sd) in order to control the ran-
dom number generator. In future versions of MATLAB, it is possible that this command 
will not be supported anymore, and will be replaced by the command rng ( sd, 'v4 ' ) . 
Given the 30 points, the objective is to find a line of the form y = ax + b that best fits 
them. The corresponding linear system that needs to be "solved" is 
X1 
1 
Yt 
X2 
1 (b)= 
Y2 
X30 
1 
Y3o 
~ 
'-v-" 
x 
y 
The least squares solution of the above system is (Xrx)-1X7 y. In MATLAB, the param-
eters a and b can be extracted via the commands 

40 
Chapter 3. Least Squares 
3.5 .-----.----..---r---,.-----, 
3.5 .------.----..---r---,.-----, 
3 
• 
3 
.., 
.. .. 
• I' 
• 
" / 
• 
/ 
2.5 
.... 
2.5 
"· 
.... 
.. , 
/ 
.. 
.. 
.. .. .A .. 
.. .... 
/ 
2 
2 
/ 
+ .. 
•/ . 
.. ,. 
*/ .. 
/ 
/ .. 
1.5 
.. .. .. 
1.5 
/ • .. 
.. * .. 
.. r 
/ 
.. 
.. 
.. / .. 
1 .. 
1 ,, .. 
0 
0.2 
0.4 
0.6 
0.8 
1 
0 
0.2 
0.4 
0.6 
0.8 
1 
Figure 3.1. Left image: 30 points in the plane. Right image: the points and the corresponding 
least squares line. 
>> u=[d,ones(30,l)]\e; 
>> a=u(l),b=u(2) 
a = 
2.0616 
b = 
0.9725 
Note that the obtained estimates of a and b are very close to the "true" a and b (2 and 1, 
respectively) that were used to generate the data. The least squares line as well as the 30 
points is described in the right image of Figure 3.1. 
I 
The least squares approach can be used also in nonlinear fitting. Suppose, for example, 
that we are given a set of points in lR.2: ( ui, Yi), i = 1, 2, ... , m, and that we know a priori 
that these points are approximately related via a polynomial of degree at most d; i.e., there 
exists a0, ... ,ad such that 
d 
"Laju{ ~Yi ' 
i = l, ... ,m. 
j=O 
The least squares approach to this problem seeks a0,a1, ... ,ad that are the least squares 
solution to the linear system 
1 
U1 
u2 
1 
ud 
1 
ao 
Yo 
1 
U2 
u2 
2 
ud 
2 
al 
Y1 
1 um 
u2 
ud 
ad 
Ym 
m 
m 
u 
The least squares solution is of course well-defined if the m x ( d + 1) matrix is of a full 
column rank. This of course suggests in particular that m > d + 1. The matrix U consists 

3.3. Regularized Least Squares 
41 
of the first d + 1 column of the so-called Vandermonde matrix, 
1 
U1 
u2 
m-1 
1 
u1 
1 
U2 
u2 
m-1 
2 
u2 
1 um 
u2 
um-1 
m 
m 
which is known to be invertible when all the U; s are different from each other. Thus, 
when m > d + 1, and all the U; s are different from each other, the matrix U is of a full 
column rank. 
3.3 • Regularized Least Squares 
There are several situations in which the least squares solution does not give rise to a 
good estimate of the "true" vector x. For example, when A is underdetermined, that 
is, when there are fewer equations than variables, there are several optimal solutions to 
the least squares problem, and it is unclear which of these optimal solutions is the one 
that should be considered. In these cases, some type of prior information on x should be 
incorporated into the optimization model. One way to do this is to consider a penalized 
problem in which a regularization function R(·) is added to the objective function. The 
regularized least squares (RLS) problem has the form 
(RLS) 
min llAx-bll2 + AR(x). 
(3.2) 
x 
The positive constant A is the regularization parameter. As A gets larger, more weight is 
given to the regularization function. 
In many cases, the regularization is taken to be quadratic. In particular, R(x) = llDxl 12 
where DE Iipxn is a given matrix. The quadratic regularization function aims to control 
the norm of Dx and is formulated as follows: 
min I IAx-hi 12 +Al IDxl 12• 
x 
To find the optimal solution of this problem, note that it can be equivalently written as 
min{fRLs(x)=xT(AT A+ADTD)x-2bT Ax+llhll2}. 
x 
Since the Hessian of the objective function is V2 /RI..s(x) = 2(A TA+ ..{DTD) >- 0, it follows 
by Lemma 2.41 that any stationary point is a global minimum point. The stationary 
points are those satisfying V fRLs(x) = 0, that is, 
(AT A+..{DTD)x=ATb. 
Therefore, if AT A+ ADTD >- 0, then the RLS solution is given by 
xRLs =(AT A+..{0Tor1ATh. 
Example 3.3. Let A E Ji3x3 be given by 
A= 
3 
s+10-3 
(
2+ 10-3 
3 
4 
7 
The matrix was constructed via the MATLAB code 
B=[l,l,l;l,2,3]; 
A=B'*B+0.001*eye(3); 
4 
) 
7 
. 
10+ 10-3 
(3.3} 

42 
Chapter 3. Least Squares 
The "true" vector was chosen to be Xi:rue = (1,2,3)T, and bis a noisy measurement of 
A:Ki:rue: 
>> x_true=[1;2;3]; 
>> randn('seed',315); 
>> b=A*x_true+O.Ol*randn(3,1) 
b = 
20.0019 
34.0004 
48.0202 
The matrix A is in fact of a full column rank since its eigenvalues are all positive (which 
can be checked, for example, by the MATLAB command eig (A)), and the least squares 
solution is given by xLs' whose value can be computed by 
A\b 
ans = 
4.5446 
-5.1295 
6.5742 
xLs is rather far from the true vector Xi:rue· One difference between the solutions is that the 
squared norm llxLsll2 = 90.1855 is much larger then the correct squared norm 11Xi:ruell2 = 
14. In order to control the norm of the solution we will add the quadratic regularization 
function llxll2. The regularized solution will thus have the form (see (3.3)) 
xRLs =(AT A+ .A.1)-1 A Tb. 
Picking the regularization parameter as .A.= 1, the RLS solution becomes 
>> x_rls=(A'*A+eye(3))\(A'*b) 
x_rls = 
1.1763 
2.0318 
2.8872 
which is a much better estimate for Xi:rue than xLs· 
I 
3.4 • Denoising 
One application area in which regularization is commonly used is denoising. Suppose that 
a noisy measurement of a signal x e Rn is given: 
h=x+w. 
Here x is an unknown signal, w is an unknown noise vector, and b is the known measure-
ments vector. The denoising problem is the following: Given b, find a "good" estimate 
of x. The least squares problem associated with the approximate equations x ~ b is 
min llx - bll2. 
However, the optimal solution of this problem is obviously x = b, which is meaningless. 
This is a case in which the least squares solution is not informative even though the as-
sociated matrix-the identity matrix-is of a full column rank. To find a more relevant 

3.4. Denoising 
43 
problem, we will add a regularization term. For that, we need to exploit some a priori 
information on the signal. For example, we might know in advance that the signal is 
smooth in some sense. In that case, it is very natural to add a quadratic penalty, which is 
the sum of the squares of the differences of consecutive components of the vector; that is, 
the regularization function is 
n-1 
R(x) = 2:<xi -xi+1)2. 
i=l 
This quadratic function can also be written as R(x) = 11Lxll2, where L E JR(n-t)xn is 
given by 
1 -1 
0 
0 
0 
0 
0 
1 
-1 
0 
0 
0 
L= 0 
0 
1 
-1 
0 
0 
0 
0 
0 
0 
1 -1 
The resulting regularized least squares problem is (with ). a given regularization para-
meter) 
min I Ix - hi 12 + Al ILxl 12, 
x 
and its optimal solution is given by 
(3.4) 
Example 3.4. Consider the signal x E R300 constructed by the following MATLAB com-
mands: 
t=linspace(0,4,300} '; 
x=sin ( t} +t. * (cos ( t} . "'2) ; 
Essentially, this is the signal given by xi = sin( 4 ~;J) + ( 4 ~;;) cos2( 4 ~;; ), i = 1, 2, ... , 300. 
A normally distributed noise with zero mean and standard deviation of 0.05 was added 
to each of the components: 
randn('seed',314}; 
b=x+0.05*randn(300,l}; 
The true and noisy signals are given in Figure 3.2, which was constructed by the MATLAB 
commands. 
subplot(l,2,1); 
plot(l:300,x,'LineWidth',2}; 
subplot(l,2,2); 
plot(1:300,b, 'LineWidth',2); 
In order to denoise the signal b, we look at the optimal solution of the RLS problem given 
by (3.4) for four different values of the regularization parameter: ). = 1, 10, 100, 1000. The 
original true signal is denoted by a dotted line. As can be seen in Figure 3.3, as ). gets 
larger, the RLS solution becomes smoother. For).= 1 the RLS solution xRLs(l) is not 

44 
Chapter 3. Least Squares 
3.5 
3.5 
3 
3 
2.5 
2.5 
2 
2 
1.5 
1.5 
1 
0.5 
0.5 
0 
0 
- 0.5 
0 
50 
100 
150 
200 
250 
300 
0 
50 
100 
150 
200 
250 
300 
Figure 3.2. A signal (/.eft image) and its noisy version (right image). 
A.:= 1 
/...=10 
3.5 
3.5 
3 
3 
2.5 
2.5 
2 
2 
1.5 
1 
1.5 
0.5 
0 
0.5 
- 0.5 
0 
50 
100 
150 
200 
250 
300 
50 
100 
150 
200 
250 
300 
A.=100 
A.:=1 000 
3.5 
3.5 
3 
3 
..... 
2.5 
2.5 
2 
2 
1.5 
1.5 
0.5 
0.5 
0 
0 . 
0 
50 
100 
150 
200 
250 
300 
0 
50 
100 
150 
200 
250 
300 
Figure 3.3. Four reconstructions of a noisy signal by RLS solutions. 
smooth enough and is very close to the noisy signal b. For).= 10 the RLS solution is a 
rather good estimate of the original vector x. For).= 100 we get a smoother RLS signal, 
but evidently it is less accurate than xRLs(lO), especially near the boundaries. The RLS 
solution for).= 1000 is very smooth, but it is a rather poor estimate of the original signal. 
In any case, it is evident that the parameter ). is chosen via a trade off between data fidelity 
(closeness of x to b) and smoothness (size of Lx). The four plots where produced by the 
MATLAB commands 
L=zeros(299,300); 
for i=1:299 
L(i,i)=l; 
L(i,i+l)=-1; 
end 
x_rls=(eye(300)+1*L'*L}\b; 
x_rls=[x_rls, (eye(300)+10*L'*L}\b]; 
x_rls=[x_rls, (eye(300)+100*L'*L)\b]; 
x_rls=[x_rls, (eye(300)+1000*L'*L)\b]; 
figure(2) 

3.5. Nonlinear Least Squares 
for j=l:4 
end 
I 
subplot(2,2,j); 
plot(1:300,x_rls(:,j), 'LineWidth',2); 
hold on 
plot(l:300,x,':r', 'LineWidth',2); 
hold off 
title(['\lambda=',num2str(10A(j-1))]); 
3.5 • Nonlinear Least Squares 
45 
The least squares problem considered so far is also referred to as "linear least squares" 
since it is a method for finding a solution to a set of approximate linear equalities. There 
are of course situations in which we are given a system of nonlinear equations 
fi(x) ~ ci, 
i = 1,2, ... , m. 
In this case, the appropriate problem is the nonlinear least squares (NLS) problem, which 
is formulated as 
m 
min ~{fi(x)-ci)2 • 
(3.5) 
i=l 
As opposed to linear least squares, there is no easy way to solve NLS problems. In Section 
4.5 we will describe the Gauss-Newton method which is specifically devised to solve NLS 
problems of the form (3.5), but the method is not guaranteed to converge to the global 
optimal solution of (3.5) but rather to a stationary point. 
3.6 • Circle Fitting 
Suppose that we are given m points a1, a2, . •. , am E lRn. The circle fitting problem seeks to 
find a circle 
C( x, r) = {y E lRn : I ly-xi I = r} 
that best fits the m points. Note that we use the term "circle," although this terminol-
ogy is usually used in the plane ( n = 2), and here we consider the general n-dimensional 
space ]Rn. Additionally, note that C(x, r) is the boundary set of the corresponding ball 
B(x, r ). An illustration of such a fit is given in Figure 3.4. The circle fitting problem has 
applications in many areas, such as archaeology, computer graphics, coordinate metrol-
ogy, petroleum engineering, statistics, and more. The nonlinear (approximate) equations 
associated with the problem are 
llx-adl ~ r, 
i = 1,2, ... ,m. 
Since we wish to deal with differentiable functions, and the norm function is not differ-
entiable, we will consider the squared version of the latter: 
llx-a;ll2 ~ r2, 
i = 1,2, ... ,m. 

46 
Chapter 3. Least Squares 
15 
------ .... _ 
/ 
.... , 
,,. 
' 
/ 
' 
, 
' 
10 
I 
\ 
I 
I 
I 
I 
I 
I 
5 
I 
"' 
I 
0 
~ 
I 
' 
· ~ 
' 
,, 
' 
,; " 
..... 
- 5 
-- - - --
35 
40 
45 
50 
55 
60 
65 
70 
75 
Figure 3.4. The best circle fit (the optimal solution of problem (3.6)) of 10 points denoted by asterisks. 
The NLS problem associated with these equations is 
(3.6) 
From a first glance, problem (3.6) seems to be a standard NLS problem, but in this case 
we can show that it is in fact equivalent to a linear least squares problem, and therefore 
the global optimal solution can be easily obtained. We begin by noting that problem (3.6) 
is the same as 
~.~n { ~(-2a[ x + llxii2- r 2 +Ila; 112)2 : x E JR", r E JR} . 
(3.7) 
Making the change of variables R = I lxl 12 -
r 2, the above problem reduces to 
Note that the change of variables imposed an additional relation between the variables 
that is given by the constraint llx112 > R. We will show that in fact this constraint can be 
dropped; that is, problem (3.8) is equivalent to the linear least squares problem 
min {t(-2aj x + R + llai 112)2 : x E Rn ,RE ffi.}. 
x,R 
i=1 
(3.9) 
/\ 
/\ 
Indeed, any optimal solution (x,R) of (3.9) automatically satisfies llxll2 > R since other-
" 
wise, if I lxl 12 < R, we would have 
i = 1, ... ,m. 

Exercises 
47 
Squaring both sides of the first inequality in the above equation and summing over i yield 
m 
m 
f(x,R) = L:(-2af x+R. +llaill2)2 > L:(-2af i+llill2 + llai112)2 = f(i,llill2), 
i=l 
i=1 
I\ 
showing that (x,llxll2) gives a lower function value than (x,R), in contradiction to the 
optimality of (x,R). To conclude, problem (3.6) is equivalent to the least squares problem 
(3.9), which can also be written as 
min 11Ay-bll2, 
yEJR.n+I 
(3.10) 
where y = {l) and 
2aT -1 
lla1112 
,. 
2at -1 
lla2112 
A= 
2 
h= 
' 
(3.11) 
2aT 
m -1 
llam 112 
If A is of full column rank, then the unique solution of the linear least squares problem 
(3.10) is 
y= (AT A)-lATh. 
The optimal x is given by the first n components of y and the radius r is given by 
r = Jllxll2-R, 
where R is the last (i.e., (n + l)th) component of y. We summarize the above discussion 
in the following lemma. 
Lemma 3.5. Let y = (AT A)-1 ATb, where A and, b are given in (3.11). Then the optimal 
solution of problem (3.6) is given by {i, r ), where x consists of the first n components of y and, 
r = Jllxll2-Yn+1 · 
Exercises 
3.1. Let A E Rmxn ,b E Rn ,LE JRPXn, and A E IR++· Consider the regularized least 
squares problem 
(R.LS) 
min llAx-hll2 + .AllLxll2. 
xElR.n 
Show that (R.LS) has a unique solution if and only if Null(A) n Null(L) = {0}, 
where here for a matrix B, Null(B) is the null space of B given by {x: Bx= O}. 
3.2. Generate thirty points (xi,Yi), i = 1,2, ... , 30, by the MATLAB code 
randn('seed',314); 
x=linspace(0,1,30)'; 
y=2*X.A2-3*x+l+0.05*randn(size(x)); 
Find the quadratic function y = ax2 + bx + c that best fits the points in the least 
squares sense. Indicate what are the parameters a, b, c found by the least squares 

48 
Chapter 3. Least Squares 
0.8 
0.6 
0.4 
0.2 
0 
- 0.2 
- 0.4 ~-~--~--~--~--~ 
0 
0.2 
0.4 
0.6 
0.8 
Figure 3.5. 30 points and their best quadratic least squares fit. 
solution, and plot the points along with the derived quadratic function. The re-
sulting plot should look like the one in Figure 3.5. 
3.3. Write a MATLAB function circle_fi t whose input is an n x m matrix A; the 
columns of A are them vectors in Rn to which a circle should be fitted. The call 
to the function will be of the form 
[x,r]=circle_fit(A) 
The output (x, r) is the optimal solution of (3.6). Use the code in order to find the 
best circle fit in the sense of (3.6) of the 5 points 

Chapter 4 
The Gradient Method 
4.1 • Descent Directions Methods 
In this chapter we consider the unconstrained minimization problem 
min{/(x): x E Rn}. 
We assume that the objective function is continuously differentiable over Rn. We have 
already seen in Chapter 2 that a first order necessary optimality condition is that the gra-
dient vanishes at optimal points, so in principle the optimal solution of the problem can 
be obtained by finding among all the stationary points of f the one with the minimal 
function value. In Chapter 2 several examples were presented in which such an approach 
can lead to the detection of the unconstrained global minimum off, but unfortunately 
these were exceptional examples. In the majority of problems such an approach is not im-
plementable for the following reasons: (i) it might be a very difficult task to solve the set 
of (usually nonlinear) equations V /(x) = O; (ii) even if it is possible to find all the station-
ary points, it might be that there are infinite number of stationary points and the task of 
finding the one corresponding to the minimal function value is an optimization problem 
which by itself might be as difficult as the original problem. For these reasons, instead 
of trying to find an analytic solution to the stationarity condition, we will consider an 
iterative algorithm for finding stationary points. 
The iterative algorithms that we will consider in this chapter take the form 
where dk is the so-called direction and tk is the stepsize. We will limit ourselves to "descent 
directions," whose definition is now given. 
Definition 4.1 (descent direction). Let f : Rn -+ R be a continuously differentiable func-
tion over Rn. A vector 0 ~ d E Rn is called a descent direction off at x if the directional 
derivative /'(x;d) is negative, meaning that 
f'(x;d) = V f(x)T d < 0. 
The most important property of descent directions is that taking small enough steps 
along these directions lead to a decrease of the objective function. 
49 

50 
Chapter 4. The Gradient Method 
Lemma 4.2 (descent property of descent directions). Let f be a continuously differen-
tiable function over Rn, and let x E Rn. Suppose that d is a descent direction off at x. Then 
there exists c > 0 such that 
f(x+ td) <f(x) 
for any t E(O,c]. 
Proof. Since f' ( x; d) < 0, it follows from the definition of the directional derivative that 
lim f(x+ td)-f(x) = f'(x;d) < 0. 
t-+O+ 
t 
Therefore, there exists an c > 0 such that 
f(x+td)-f(x) 
------<0 
t 
for any t E (0, c ], which readily implies the desired result. 
D 
We are now ready to write in a schematic way a general descent directions method. 
Schematic Descent Directions Method 
Initialization: Pick Xo E Rn arbitrarily. 
General step: For any k = 0, 1,2, ... set 
(a) Pick a descent direction dk. 
(b) Find a stepsize tk satisfying f ( xk + tk dk) < f ( xk ). 
(c) Set xk+t = xk + tkdk. 
(d) Ha stopping criterion is satisfied, then STOP and xk+t is the output. 
Of course, many details are missing in the above description of the schematic algorithm: 
• What is the starting point? 
• How to choose the descent direction? 
• What stepsize should be taken? 
• What is the stopping criteria? 
Without specification of these missing details, the descent direction method remains "con-
ceptual" and cannot be implemented. The initial starting point can be chosen arbitrarily 
(in the absence of an educated guess for the optimal solution). The main difference be-
tween different methods is the choice of the descent direction, and in this chapter we 
will elaborate on one of these choices. An example of a popular stopping criteria is 
llV f(xk+i)ll < c. We will assume that the stepsize is chosen in such a way such that 
f(xk+t) < f(xk)· This means that the method is assumed to be a descent method, that 
is, a method in which the function values decrease from iteration to iteration. The pro-
cess of finding the stepsize tk is called line search, since it is essentially a minimization 

4.1. Descent Directions Methods 
51 
procedure on the one-dimensional function g(t) = f (xk + tdk)· There are many choices 
for stepsize selection rules. We describe here three popular choices: 
• constant stepsize tk = i for any k. 
• exact line search tk is a minimizer off along the ray xk + tdk: 
tk E argmintd(xk + tdk). 
• backtracking The method requires three parameters: s > O,a E (0, 1),(3E(0,1). 
The choice of tk is done by the following procedure. First, tk is set to be equal to 
the initial guess s. Then, while 
f(xk)- f(xk + tkdk) < -atk V f(xk)T dk, 
we set tk +- f3 tk. In other words, the stepsize is chosen as tk = s f3i1e, where ik is the 
smallest nonnegative integer for which the condition 
. 
. 
T 
f(xk)-f(xk +sf3'1edk) >-asf3'1eVf(xk) dk 
is satisfied. 
The main advantage of the constant stepsize strategy is of course its simplicity, but at this 
point it is unclear how to choose the constant. A large constant might cause the algorithm 
to be nondecreasing, and a small constant can cause slow convergence of the method. The 
option of exact line search seems more attractive from a first glance, but it is not always 
possible to actually find the exact minimizer. The third option is in a sense a compromise 
between the latter two approaches. It does not perform an exact line search procedure, 
but it does find a good enough stepsize, where the meaning of "good enough" is that it 
satisfies the following sufficient decrease condition: 
(4.1) 
The next result shows that the sufficient decrease condition (4.1) is always satisfied for 
small enough tk. 
Lemma 4.3 (validity of the sufficient decrease condition). Let f be a continuously 
differentiable function over Rn, and let x E Rn. Suppose that 0 f:. d E Rn is a descent direction 
off at x and let a E (0, 1). Then there exists c > 0 such that the inequality 
f(x)-f(x+ td) >-atVf(x)7 d 
holds for all t E [O, c ]. 
Proof. Since f is continuously differentiable it follows that (see Proposition 1.23) 
f(x+ td) = f(x)+ tVf(xf d+o(tlldll), 
and hence 
f(x)- f (x+ td) = -atV f(x)7 d-(1-a)tV f (x)7 d-o(tlldjl). 
(4.2) 
Since d is a descent direction off at x we have 
1. (1-a)tVf(xf d+o(tlldll) 
( 
) f( )Td 
1m 
= 1-a V 
x 
< 0. 
t-+0+ 
t 

52 
Chapter 4. The Gradient Method 
Hence, there exists £ > 0 such that for all t E (0, £] the inequality 
(1-a)t\7 f (x)T d + o(tlldll) < 0 
holds, which combined with (4.2) implies the desired result. 
0 
We will now show that for a quadratic function, the exact line stepsize can be easily 
computed. 
Example 4.4 (exact line search for quadratic functions). Letf(x) = xT Ax+2hT x+c, 
where A is an n x n positive definite matrix, h E Rn, and c E R. Let x E Rn and let d E Rn 
be a descent direction off at x. We will find an explicit formula for the stepsize generated 
by exact line search, that is, an expression for the solution of 
We have 
minf(x+ td). 
t~ 
g(t) = f(x+ td) = (x+ tdf A(x+ td)+2hT(x+ td)+c 
=(dTAd)t2 +2(dT Ax+dTb)t+xT Ax+2hT x+c 
= (dT Ad)t2 +2(dT Ax+dTb)t + f(x). 
Since g'(t) = 2(dT Ad)t + 2dT (Ax+ h) and \7 f (x) = 2(Ax + b), it follows that g'(t) = 0 
if and only if 
-
dT\lf(x) 
t = t = 2dT Ad . 
Since d is a descent direction off at x, it follows that dT\7 f (x) < 0 and hence i > 0, 
which implies that the stepsize dictated by the exact line search rule is 
_ 
dT\lf(x) 
t= 
. 
2dTAd 
{4.3) 
Note that we have implicitly used in the above analysis the fact that the second order 
derivative of g is always positive. 
I 
4.2 • The Gradient Method 
In the gradient method the descent direction is chosen to be the minus of the gradient 
at the current point: dk = -\7 f(xk)· The fact that this is a descent direction whenever 
\lf(xk) f:. 0 can be easily shown-just note that 
f'(xk;-\7 f(xk)) =-V f(xk)T\7 f(xk) = -llV f(xk)ll2 < 0. 
In addition for being a descent direction, minus the gradient is also the steepest direction 
method. This means that the normalized direction-Vf(xk)/llVf(xk)ll corresponds to 
the minimal directional derivative among all normalized directions. A formal statement 
and proof is given in the following result. 
Lemma 4.5. Let f be a continuously differentiable function, and let x E Rn be a non-
stationary point (\7 f ( x) f:. 0 ). 1hen an optimal solution of 
min{f'{x;d): lldll = 1} 
(4.4) 
de!Rn 
· d 
V/(x) 
ZS = 
llV/(x)ll' 

4.2. The Gradient Method 
Proof. Since f'(x;d) = '\1 f (x)7 d, problem (4.4) is the same as 
min{'\! f (xf d: lldll = 1 }. 
dEIRn 
By the Cauchy-Schwarz inequality we have 
V f (xf d > -llV f (x)ll · lldll = -llV f(x)ll. 
53 
Thus, -llVf(x)ll is a lower bound on the optimal value of (4.4). On the other hand, 
plugging d = 
11~j~:~11 in the objective function of (4.4) we obtain that 
, ( 
'\1 f (x) ) 
T ( V f (x) ) 
f 
x, 11'1 f(x)ll =-'\! f(x) 
11'1 f(x)ll = -llV f(x)ll, 
and we thus come to the conclusion that the lower bound -11'1 f(x)ll is attained at d = 
11~f~:~11 , which readily implies that this is an optimal solution of (4.4). 
D 
We will now present the gradient method with the standard stopping criteria, which 
is the condition 11'1 f (xk+i)ll ~ c · 
The Gradient Method 
Input: c > 0 - tolerance parameter. 
Initialization: Pick Xo E :!Rn arbitrarily. 
General step: For any k = 0, 1, 2, ... execute the following steps: 
(a) Pick a stepsize tk by a line search procedure on the function 
(b) Set xk+l = xk -tk V f(xk). 
(c) If 11'1 f(xk+i)ll < c, then STOP and xk+t is the output. 
Example 4.6 (exact line search). Consider the two-dimensional minimization problem 
min x2 +2y2 
(4.5) 
x,y 
whose optimal solution is (x,y) = (0,0) with corresponding optimal value 0. To invoke 
the gradient method with stepsize chosen by exact line search, we constructed a MATLAB 
function, called gradient_method_quadratic, that finds (up to a tolerance) the op-
timal solution of a quadratic problem of the form 
min {xT Ax+ 2bT x}, 
xEIRn 
where A E :!Rn xn positive definite and b E :!Rn. The function invokes the gradient method 
with exact line search, which by Example 4.4 is equal at the kth iteration to tk 
llV/(xk)il2 
Th M A'TLAB fu 
. 
. d 
.b db 1 
iv/(xk)7AV/(xk). 
e 
.n. 
ncuon 1s escn e 
e ow. 

54 
Chapter 4. The Gradient Method 
function [x,fun_val]=gradient_method_quadratic(A,b,xO,epsilon) 
% INPUT 
% ======================· 
% A ....... the positive definite matrix associated with the 
% 
objective function 
% b ....... a column vector associated with the linear part of the 
% 
objective function 
% xO ...... starting point of the method 
% epsilon . tolerance parameter 
% OUTPUT 
% ======================= 
% x ....... an optimal solution (up to a tolerance) of 
% 
min(xAT A x+2 bAT x) 
% fun_val . the optimal function value up to a tolerance 
x=xO; 
iter=O; 
grad=2* (A*x+b}; 
while (norm(grad}>epsilon} 
iter=iter+l; 
t=norm(grad)A2/(2*grad'*A*grad}; 
x=X-t*grad; 
grad=2* (A*x+b); 
fun_val=X'*A*X+2*b'*Xi 
fprintf('iter_number = %3d norm_grad = %2.6f fun_val = %2.6f\n', ... 
iter,norm(grad),fun_val); 
end 
In order to solve problem (4.5) using the gradient method with exact line search, tol-
erance parameter e = 10-5, and initial vector Xo = (2, 1)T, the following MATLAB com-
mand was executed: 
[x,fun_val]=gradient_method_quadratic([l,0;0,2], [0;0], [2;1] ,le-5) 
The output is 
iter_number = 
1 norm_grad = 1.885618 fun_ val = 0.666667 
iter_number = 
2 norm_grad = 0.628539 fun_ val = 0.074074 
iter_number = 
3 norm_grad = 0.209513 fun_ val = 0.008230 
iter_number = 
4 norm_grad = 0.069838 fun_ val = 0.000914 
iter_number = 
5 norm_grad = 0.023279 fun_ val = 0.000102 
iter_number = 
6 norm_grad = 0.007760 fun_ val = 0.000011 
iter_number = 
7 norm_grad = 0.002587 fun_ val = 0.000001 
iter_number = 
8 norm_grad = 0.000862 fun_ val = 0.000000 
iter_number = 
9 norm_grad = 0.000287 fun_ val = 0.000000 
iter_number = 10 norm_grad = 0.000096 fun_ val = 0.000000 
iter_number = 
11 norm_grad = 0.000032 fun_ val = 0.000000 
iter_number = 12 norm_grad = 0.000011 fun_ val = 0.000000 
iter_number = 13 norm_grad = 0.000004 fun_ val = 0.000000 
The method therefore stopped after 13 iterations with a solution which is pretty close to 
the optimal value: 
x = 
1.0e-005 * 
0.1254 
-0.0627 

4.2. The Gradient Method 
55 
0 
0.5 
1.5 
2 
Figure 4.1. The iterates of the gradient method along with the contour lines of the objective function. 
It is also very informative to visually look at the progress of the iterates. The iterates and 
the contour plots of the objective function are given in Figure 4.1. 
I 
An evident behavior of the gradient method as illustrated in Figure 4.1 is "zig-zag" 
effect, meaning that the direction found at the kth iteration xk+t - xk is orthogonal to 
the direction found at the ( k + 1 )th iteration xk+i-Xk+ 1• This is a general property whose 
proof will be given now. 
Lemma 4.7. Let {xkh>o be the sequence generated by the gradU!nt method with exact line 
search for solving a problem of minimizing a continuously differentiable function f. 1hen 
for any k = 0, 1, 2, ... 
Proof. By the definition of the gradient method we have that xk+t -xk = -tk V f (xk) and 
xk+i-Xk+t =-tk+t V f (xk+t)· Therefore, we wish to prove that V f(xk)TV f(xk+t) = 0. 
Since 
tk E argmint~{g(t) =f(xk- t"V f(xk))}, 
and the optimal solution is not tk = 0, it follows that g'(tk) = 0. Hence, 
meaning that the desired result V f ( xk) TV f ( xk+ 1) = 0 holds. 
0 
Let us now consider an example with a constant stepsize. 
Example 4.8 (constant stepsize). Consider the same optimization problem given in Ex-
ample 4.6 
min x2+2y2• 
x,y 

56 
Chapter 4. The Gradient Method 
To solve this problem using the gradient method with a constant stepsize, we use the fol-
lowing MATLAB function that employs the gradient method with a constant stepsize for 
an arbitrary objective function. 
function [x,fun_val]=gradient_method_constant(f,g,xO,t,epsilon) 
% Gradient method with constant stepsize 
% 
% INPUT 
%======================================= 
% f ......... objective function 
% g ......... gradient of the objective function 
% xO ......... initial point 
% t ......... constant stepsize 
% epsilon ... tolerance parameter 
% OUTPUT 
%======================================= 
% x ......... optimal solution (up to a tolerance) 
% 
of min f(x) 
% fun_val ... optimal function value 
x=xO; 
grad=g(x); 
iter=O; 
while (norm(grad)>epsilon) 
iter=iter+l; 
x=x-t*grad; 
fun_val=f (x); 
grad=g(x); 
fprintf('iter_number = %3d noril\_grad = %2.6f fun_val = %2.6f \n', ... 
iter,norm(grad),fun_val); 
end 
We can employ the gradient method with constant stepsize tk = 0.1 and initial vector 
Xo = (2, 1)T by executing the MATLAB commands 
A=[l,0;0,2]; 
[x,fun_val]=gradient_method_constant(@(x)X'*A*X,@(x)2*A*X, [2;1],0.l,le-5); 
and the long output is 
iter_number = 
1 norm_grad = 4.000000 fun_ val = 3.280000 
iter_number = 
2 norm_grad = 2.937210 fun_ val = 1.897600 
iter_nwnber = 
3 norm_grad = 2.222791 fun_ val = 1.141888 
iter_number = 56 norm_grad = 0.000015 fun_ val = 0.000000 
iter_nwnber = 57 norm_grad = 0.000012 fun_ val = 0.000000 
iter_nwnber = 58 norm_grad = 0.000010 fun_ val = 0.000000 
The excessive number of iterations is due to the fact that the stepsize was chosen to be too 
small. However, taking a stepsize which is large might lead to divergence of the iterates. 
For example, taking the constant stepsize to be 100 results in a divergent sequence. 
>> A=[l,0;0,2}; 
>> [x,fun_val]=gradient_method_constant(@(x)x'*A*x,@(x)2*A*X, [2;1],100,le-5); 
iter_number = 
1 norm_grad 
1783.488716 fun_val = 476806.000000 
iter_number = 
2 norrn_grad = 656209.693339 fun_val = 56962873606.000000 
iter_number = 
3 norm_grad = 256032703.004797 fun_val = 8318300807190406.0 
iter_number = 119 norrn_grad = NaN fun_val = NaN 

4.2. The Gradient Method 
57 
The important question is therefore how to choose the constant stepsize so that it will 
not be too large (to ensure convergence) and not too small (to ensure that the convergence 
will not be too slow). We will consider again the theoretical issue of choosing the constant 
stepsize in Section 4.7. 
I 
Let us now consider an example with a backtracking stepsize selection rule. 
Example 4. 9 (stepsize selection by backtracking). The following MATLAB function 
implements the gradient method with a backtracking stepsize selection rule. 
function [x,fun_val]=gradient_method_backtracking(f,g,xO,s,alpha, ... 
beta, epsilon) 
% Gradient method with backtracking stepsize rule 
% 
% INPUT 
%======================================= 
% f ......... objective function 
% g ......... gradient of the objective function 
% xO ......... initial point 
% s ......... initial choice of stepsize 
% alpha ..... tolerance parameter for the stepsize selection 
% beta ...... the constant in which the stepsize is multiplied 
% 
at each backtracking step (O<beta<l) 
% epsilon ... tolerance parameter for stopping rule 
% OUTPUT 
%======================================= 
% x ......... optimal solution (up to a tolerance) 
% 
of min f(x) 
% fun_val ... optimal function value 
x=xO; 
grad=g(x); 
fun_val=f (x); 
iter=O; 
while (norm(grad)>epsilon) 
iter=iter+l; 
end 
t=s; 
while 
(fun_val-f(x-t*grad)<alpha*t*norm(grad)~2) 
end 
x=x-t*grad; 
fun_val=f (x); 
grad=g(x); 
fprintf('iter_nurnber = %3d norn\_grad = %2.6f fun_val = %2.6f \n', ... 
iter,norm(grad),fun_val); 
As in the previous examples, we will consider the problem of minimizing the func-
tion x2 + 2y2• Employing the gradient method with backtracking stepsize selection rule, 
a starting vector x0 = ( 2, 1 )7 and parameters e = 10-5, s = 2, a = ~, f3 = ! results in the 
following output: 
>> A=[l,0;0,2); 
>> [x,fun_val]=gradient_method_backtracking(@(x)x'*A*X,@(x)2*A*X, ... 
[2;1] ,2,0.25,0.5,le-5); 
iter_number = 
1 norm_grad = 2.000000 fun_val = 1.000000 
iter_number = 
2 norm_grad = 0.000000 fun_val = 0.000000 
That is, the gradient method with backtracking terminated after only 2 iterations. 
In fact, in this case (and this is probably a result of pure luck), it converged to the exact 

58 
Chapter 4. The Gradient Method 
optimal solution. In this example there was no advantage in performing an exact line 
search procedure, and in fact better results were obtained by the nonexact/backtracking 
line search. Computational experience teaches us that in practice backtracking does not 
have real disadvantages in comparison to exact line search. 
The gradient method can behave quite badly. As an example, consider the minimiza-
tion problem 
. 
1 
mmx2+-y2 
x,y 
100 
' 
and suppose that we employ the backtracking gradient method with initial vector ( 1~, 1 f: 
>> A=[l,0;0,0.01]; 
>> [x,fun_val]=gradient_rnethod_backtracking(@(x)x'*A*x,@(x)2*A*X, ... 
[O.Ol;l],2,0.25,0.5,le-5); 
iter_nurnber = 
1 norrn_grad = 0.028003 fun_val = 0.009704 
iter_nurnber = 
2 norrn_grad = 0.027730 fun_val = 0.009324 
iter_nurnber = 
3 norrn_grad = 0.027465 fun_val = 0.008958 
iter_nurnber = 201 norrn_grad = 0.000010 fun_val = 0.000000 
I 
Clearly, 201 iterations is a large number of steps in order to obtain convergence for a 
two-dimensional problem. The main question that arises is whether we can find a quan-
tity that can predict in some sense the number of iterations required by the gradient 
method for convergence; this measure should quantify in some sense the "hardness" of 
the problem. This is an important issue that actually does not have a full answer, but a 
partial answer can be found in the notion of condition number. 
4.3 • The Condition Number 
Consider the quadratic minimization problem 
min{f (x) = 
xT Ax}, 
xER.n 
(4.6) 
where A>- 0. The optimal solution is obviously x* = 0. The gradient method with exact 
line search takes the form 
Xk+t = Xk - tkdk, 
where dk = 2Axk is the gradient off at xk and the stepsize tk chosen by the exact mini-
mization rule is (see formula (4.3)) 
Therefore, 
f(xk+d = xk+1Axk+l 
= (xk-tkdk)T A(xk-tkdk) 
= xr Axk -2tkdk Axk + tidk Adk 
= xr Axk - tkdk dk + tidk Adk. 
(4.7) 

4.3. The Condition Number 
59 
Plugging in the expression for tk given in (4.7) into the last equation we obtain that 
T 
1 <dr dk)2 
T 
( 
1 
<dr dk)2 
) 
f(xk )- x Ax.k - -
- x Axk 1- - ___ 
....;.;._ ___ _ 
+l -
k 
4 drAdk -
k 
4(drAdk)<xrAA-1Axk) 
( 
<drdk)2 
) 
= 1- <dr Adk)<dr A-ldk) f<xk)· 
(4.8) 
We will now use the following well-known result, also known as the Kantorovich 
inequality. 
Lemma 4.10 (K.antorovich inequality). Let A be a positive definite n x n matrix. Then 
for any 0 (;. x E Rn the inequality 
(x7 x)2 
4Amax(A)Amin(A) 
----- > 
(4.9) 
(x7 Ax)(x7 A-1x) - (Amax(A)+Amin(A))2 
holds. 
Proof. Denotem = Amin(A)andM = Amax(A). TheeigenvaluesofthematrixA+MmA-1 
are A;(A)+ f:c~>, i = 1, ... , n. It is easy to show that the maximum of the one-dimensional 
function rp(t) = t + Mtm over [m,M] is attained at the endpoints m and M with a cor-
responding value of M + m, and therefore, since m < A;(A) < M, it follows that the 
eigenvalues of A+ M mA-1 are smaller than (M + m ). Thus, 
A+MmA-1 -<(M +m)I. 
Multiplying by xT from the left and by x from the right we obtain 
x7 Ax+ M m(xT A-1x) < (M + m )(xT x), 
which combined with the simple inequality a {3 < i< a + {3)2 (for any two real numbers 
a, {3) yields 
1 [ 
]2 
(M +m)2 
(x7 Ax.)[Mm(x7 A-1x)] < 4 (x7 Ax)+Mm(xT A-1x) < 
4 
(xT x)2, 
which after some simple rearrangement of terms establishes the desired result. 
0 
Coming back to the convergence rate analysis of the gradient method on problem 
(4.6), it follows by using the Kantorovich inequality that (4.8) yields 
f (xk+l) < (1-
4M m 2) f (xk) = (M - m )2 f (xk), 
(M+m) 
M+m 
where M = Amax(A),m = Amin(A). We summarize the above discussion in the following 
lemma. 
Lemma 4.11. Let {xk}k>o be the sequence generated by the gradient descent method with 
exact line search for solving problem (4.6). Then for any k = 0, 1, ... 
( M m)2 
f(xk+1) < M + m 
f(xk), 
(4.10) 
where M = Amax(A), m = Amin(A). 

60 
Chapter 4. The Gradient Method 
Inequality (4.10) implies 
where c = ( ~~: )2• That is, the sequence of function values is bounded above by a decreas-
ing geometric sequence. In this case we say that the sequence of function values converges 
at a linear rate to the optimal value. The speed of the convergence depends on c; as c gets 
larger, the convergence speed becomes slower. The quantity c can also be written as 
-(x-1)2 
C-
' 
x+1 
where x = ! = ~:~~~j. Since c is an increasing function of x, it follows that the behavior 
of the gradient method depends on the ratio between the maximal and minimal eigenval-
ues of A; this number is called the condition number. Although the condition number 
can be defined for general matrices, we will restrict ourselves to positive definite matrices. 
Definition 4.12 (condition number). Let A bean n x n positive definite matrix. Then the 
condition number of A is defined by 
We have already found one illustration in Example 4.9 that the gradient method ap-
plied to problems with large condition number might require a large number of iterations 
and vice versa, the gradient method employed on problems with small condition number 
is likely to converge within a small number of steps. Indeed, the condition number of 
the matrix associated with the function x2 +0.01y2 is o.~t = 100, which is relatively large, 
is the cause for the 201 iterations that were required for convergence, and the small con-
dition number of the matrix associated with the function x2 + 2y2 (x = 2) is the reason 
for the small number of required steps. Matrices with large condition number are called 
ill-conditioned, and matrices with small condition number are called well·conditioned. Of 
course, the entire discussion until now was on the restrictive class of quadratic objective 
functions, where the Hessian matrix is constant, but the notion of condition number also 
appears in the context of nonquadratic objective functions. In that case, it is well known 
that the rate of convergence of xk to a given stationary point x* depends on the condition 
number of x(V2f(x*)). We will not focus on these theoretical results, but will illustrate 
it on a well-known ill-conditioned problem. 
Example 4.13 (Rosenbrock function). The Rosenbrock function is the following func-
uon: 
f (x1, x2 ) = 100(x2 -x;)2 + (1-x1 )2. 
The optimal solution is obviously (x1,x2) = (1, 1) with corresponding optimal value 0. 
The Rosenbrock function is extremely ill-conditioned at the optimal solution. Indeed, 
nf( )-(-400x1(x2-x:)-2(1-x1)) 
v 
x -
200(x2-x;) 
' 
n2f( ) - (-400x2 + 12oox; + 2 -400x1) 
v 
x -
-400x1 
200 
· 

4.3. The Condition Number 
61 
It is not difficult to show that (x1,x2) = (1, 1) is the unique stationary point. In addition, 
21 
( 802 
-400) 
v (l, l) = -400 
200 
and hence the condition number is 
>> A=[802,-400;-400,200]; 
>> cond(A) 
ans = 
2.5080e+003 
A condition number of more than 2500 should have severe effects on the convergence 
speed of the gradient method. Let us then employ the gradient method with backtracking 
on the Rosenbrock function with starting vector Xo = ( 2, 5) T: 
>> f=@(x)100*(X(2)-x(l)A2)A2+(1-x(l))A2; 
» 
g=@ (x) [ -400 * (x ( 2) - x ( 1) A2) *X ( 1) - 2 * ( 1-x ( 1) ) ; 200 * (x ( 2) -x ( 1) A2) ] ; 
>> [x,fun_val]=gradient_method_backtracking(f,g, [2;5],2,0.25,0.5,le-5); 
iter_number = 
1 norm_grad = 118.254478 fun_val = 3.221022 
iter_number = 
2 norm_grad = 0.723051 fun_val = 1.496586 
iter_number = 6889 norm_grad = 0.000019 fun_val = 0 . 000000 
iter_number = 6890 norm_grad = 0.000009 fun_val = 0.000000 
This run required the huge amount of 6890 iterations, so the ill-conditioning effect 
has a significant impact. To better understand the nature of this pessimistic run, consider 
the contour plots of the Rosenbrock function along with the iterates as illustrated in Fig-
ure 4.2. Note that the function has banana-shaped contour lines surrounding the unique 
stationary point (1, 1). 
I 
- 4 
- 3 
- 2 
- 1 
0 
1 
2 
3 
4 
Figure 4.2. Contour lines of the Rosenbrock function along with thousands of iterations of 
the gradient method. 

62 
Chapter 4. The Gradient Method 
Sensitivity of Solutions to Linear Systems 
The condition number has an important role in the study of the sensitivity of linear sys-
tems of equations to perturbation in the right-hand-side vector. Specifically, suppose that 
we are given a linear system Ax = b, and for the sake of simplicity, let us assume that A is 
positive definite. The solution of the linear system is of course x = A-1b. Now, suppose 
that instead of b in the right-hand side, we consider a perturbation b + .6.b. Let us denote 
the solution of the new system by x + .6.x, that is, A(x + .6.x) = b + .6.b. We have 
x+.6.x = A-1(b + .6.b) = x+A-1.6.b, 
so that the change in the solution is .6.x = A-1.6.b. Our purpose is to find a bound on the 
relative error 11~11 in terms of the relative error of the right-hand-side perturbation 11
1~tll: 
ll.6.xll _ llA-1.6.hll < llA-111 • ll.6.hll __ 
Am_ax_(A_-1_)11_.6.h_ll 
llxll -
llxll 
-
llxll 
-
llxll 
' 
where the last equality follows from the fact that the spectral norm of a positive definite 
matrix Dis llDll = Amax(D). By the positive definiteness of A, it follows that Amax(A-1) = 
;:mi!(A), and we can therefore continue the chain of equalities and inequalities: 
ll.6.xll < 
1 
ll.6.hll 
1 
11.6.hll 
llxll - Amin(A) llxll 
Amin(A) llA-1hll 
1 
ll.6.hll 
< 
·----
- Amin(A) Amin(A-1)llhll 
Amax(A) ll.6.hll 
- Amin(A) llhll 
ll.6.hll 
= x(A) llhlJ ' 
(4.11) 
where inequality (4.11) follows from the fact that for a positive definite matrix A, the 
inequality llA-1hll > Amin(A-1)llhll holds. Indeed, 
llA-1hll = VbT A-2b > V Amin(A-2)llbll2 = Amin(A-1)llhll· 
We can therefore deduce that the sensitivity of the solution of the linear system to right-
hand-side perturbations depends on the condition number of the coefficients matrix. 
Example 4.14. As an example, consider the matrix 
( 1+10-5 
1 
) 
A= 
1 
1+10-5 
' 
whose condition number is larger than 20000: 
>> format long 
>> A=[l+le-5,l;l,l+le-5]; 
>> cond(A) 
ans = 
2.000009999998795e+005 

4.4. Diagonal Scaling 
The solution of the system 
lS 
>> A\[1;1] 
ans = 
0.499997500018278 
0.499997500006722 
63 
Ax=G) 
That is, approximately (0.5, 0.5)7 . However, if we make a small change to the right-hand-
side vector: (1.1, 1f instead of (1, 1)7 (relative error of 111~~~~~~~~
111 = 0.0707), then the per-
turbed solution is very much different from (0.5, 0.5f: 
>> A\[1.1;1] 
ans = 
1.0e+003 * 
5.000524997400047 
-4.999475002650021 
I 
4.4 • Diagonal Scaling 
The problem of ill-conditioned problems is a major one, and many methods have been 
developed in order to circumvent it. One of the most popular approaches is to "condition" 
the problem by making an appropriate linear transformation of the decision variables. 
More precisely, consider the unconstrained minimization problem 
min{f(x): x E Rn}. 
For a given nonsingular matrix S E Rn xn, we make the linear transformation x = Sy and 
obtain the equivalent problem 
min{g(y) =f(Sy): yE Rn}. 
Since V g(y) = S7 V f (Sy)= S7 V f (x), it follows that the gradient method applied to the 
transformed problem takes the form 
Multiplying the latter equality by S from the left, and using the notation xk = Syk, we 
obtain the recursive formula 
Defining D = SS7 , we obtain the following version of the gradient method, which we 
call the scaled gradient method with scaling matrix D: 

64 
Chapter 4. The Gradient Method 
By its definition, the matrix Dis positive definite. The direction -DV f(xk) is a descent 
direction off at xk when V f(xk) f; 0 since 
where the latter strict inequality follows from the positive definiteness of the matrix D. 
To summarize the above discussion, we have shown that the scaled gradient method with 
scaling matrix Dis equivalent to the gradient method employed on the function g(y) = 
f(D112y). We note that the gradient and Hessian of g are given by 
v g(y) = D112v f(D1f2y) = D112v f (x), 
v2g(y) = D112v2 f(D112y)D112 = D112v2 f(x)D112, 
where x = D112y. 
The stepsize in the scaled gradient method can be chosen by each of the three options 
described in Section 4 .1. It is often beneficial to choose the scaling matrix D differently 
at each iteration, and we describe this version explicitly below. 
Scaled Gradient Method 
Input: E - tolerance parameter. 
Initialization: Pick Xo e Rn arbitrarily. 
General step: For any k = 0, 1, 2, ... execute the following steps: 
(a) Pick a scaling matrix Dk >- 0. 
(b) Pick a stepsize tk by a line search procedure on the function 
(c) Set xk+t = xk - tkDk V f (xk). 
(d) If llV f (xk+t)ll < E, then STOP, and xk+t is the output. 
Note that the stopping criteria was chosen to be llV f (xk+i)ll < E. A different and 
perhaps more appropriate stopping criteria might involve bounding the norm of 
D~12v f (xk+1)· 
The main question that arises is of course how to choose the scaling matrix Dk. To 
accelerate the rate of convergence of the generated sequence, which depends on the con-
dition number of the scaled Hessian D!/2V2f(xk)D!/2, the scaling matrix is often cho-
sen to make this scaled Hessian to be as close as possible to the identity matrix. When 
V2f(xk) >- 0, we can actually choose Dk= (V2f(xk))-1 and the scaled Hessian becomes 
the identity matrix. The resulting method 
is the celebrated Newton's method, which will be the topic of Chapter 5. One difficulty 
associated with Newton's method is that it requires full knowledge of the Hessian 
and in addition, the term V2 f ( xk )-1 V f ( xk) suggests that a linear system of the form 

4.4. Diagonal Scaling 
65 
V2f(xk)d = Vf(xk) needs to be solved at each iteration, which might be costly from a 
computational point of view. This is why simpler scaling matrices are suggested in the 
literature. The simplest of all scaling matrices are diagonal matrices. Diagonal scaling is 
in fact a natural idea since the ill-conditioning of optimization problems often arises as a 
result of a large variety of magnitudes of the decision variables. For example, if one vari-
able is given in kilometers and the other in millimeters, then the first variables is 6 orders 
of magnitude larger than the second. This is a problem that could have been solved at the 
initial stage of the formulation of the problem, but when it is not, it can be resolved via 
diagonal rescaling. A natural choice for diagonal elements is 
Dii = (V2f(xk))~ 1 • 
With the above choice, the diagonal elements of D112V2f(xk)D112 are all one. Of course, 
this choice can be made only when the diagonal of the Hessian is positive. 
Example 4.15. Consider the problem 
min{lOOox; +40x1x2 + x;}. 
We begin by employing the gradient method with exact stepsize, initial point 
(1, 1000f, and tolerance parameter e 
10-5 using the MATLAB function 
gradient_method_quadratic that uses an exact line search. 
>> A=[l000,20;20,l]; 
>> gradient_method_quadratic(A, [0;0], [1;1000],le-5) 
iter_number = 
iter_number = 
iter_number = 
iter_number = 
iter_number = 
ans = 
1.0e-005 * 
-0.0136 
0.6812 
1 norm_grad = 1199.023961 fun_val = 598776.964973 
2 norm_grad = 24186.628410 fun_val = 344412.923902 
3 norm_grad = 689.671401 fun_val = 198104.25098 
68 norm_grad = 0.000287 fun_val = 0.000000 
69 norm_grad = 0.000008 fun_val = 0.000000 
The excessive number of iterations is not surprising since the condition number of 
the associated matrix is large: 
>> cond(A) 
ans = 
1.6680e+003 
The scaled gradient method with diagonal scaling matrix 
should converge faster since the condition number of the scaled matrix 0 112 AD112 is sub-
stantially smaller: 
>> D=diag(l./diag(A)); 
>> sqrtm(D)*A*sqrtm(D) 

66 
Chapter 4. The Gradient Method 
ans = 
1.0000 
0.6325 
>> cond(ans) 
ans = 
4.4415 
0.6325 
1.0000 
To check the performance of the scaled gradient method, we will use a slight 
modification of the MATLAB function gradient_method_quadratic, which we 
call gradient_scaled_quadratic. 
function [x,fun_val]=gradient_scaled_quadratic(A,b,D,xO,epsilon) 
% INPUT 
% ====================== 
% A 
% 
% b 
% 
% D ••••••• 
% xO ..... . 
% epsilon . 
% OUTPUT 
the positive definite matrix associated 
with the objective function 
a colWllil vector associated with the linear 
of the objective function 
scaling matrix 
starting point of the method 
tolerance parameter 
% ======================= 
% x ....... an optimal solution (up to a tolerance) .. . 
of min(xAT A x+2 bAT x) 
part 
% fun_val . the optimal function value up to a tolerance 
x=xO; 
iter=O; 
grad=2* (A*X+b); 
while (norm(grad)>epsilon) 
end 
iter=iter+l; 
t=grad'*D*grad/(2*(grad'*D')*A*(D*grad)); 
x=x-t*D*grad; 
grad=2* (A*x+b); 
fun_val=X'*A*X+2*b'*Xi 
fprintf('iter_number = %3d norm_grad = %2.6f fun_val = %2.6f \n', 
iter,norm(grad),fun_val); 
The running of this code requires only 19 iterations: 
>> gradient_scaled_quadratic(A, [0;0],D, [1;1000],le-5) 
iter_number = 
1 norrn_grad = 10461.338850 fun_val = 102437.875289 
iter_number = 
2 norrn_grad = 4137.812524 fun_val = 10080.228908 
iter_number = 18 norrn_grad = 0.000036 fun_val = 0.000000 
iter_number = 19 norrn_grad = 0.000009 fun_val = 0.000000 
ans = 
1.0e-006 * 
-0.0106 
0.3061 
I 

4.5. The Gauss-Newton Method 
67 
4.5 • The Gauss-Newton Method 
In Section 3.5 we considered the nonlinear least squares problem 
(4.12) 
We will assume here that ft, ... ,fm are continuously differentiable over Rn for all i = 
1, 2, ... , m and that c 1, ••• , cm E R. The problem is sometimes also written in the terms of 
the vector-valued function 
F(x)= 
ft(x)-c1 
fi(x)-c2 
and then it takes the form 
min llF(x)112. 
The general step of the Gauss-Newton method goes as follows: given the kth iterate xk, 
the next iterate is chosen to minimize the sum of squares of the linear approximations of 
fi at xk, that is, 
The minimization problem above is essentially a linear least squares problem 
min llAkx-bkll2, 
xe!Rn 
where 
Vft(xk)T 
Vfi(xkf 
Ak = 
=J(xk) 
Vfm(xkf 
is the so-called Jacobian matrix and 
Vft(xk)T xk-ft(xk) + c1 
'1 fi(xk)T xk - fi(xk) + C2 
The underlying assumption is of course thatJ(xk) is of a full column rank; otherwise the 
minimization in (4.13) will not produce a unique minimizer. In that case, we can also 
write an explicit expression for the Gauss-Newton iterates (see formula (3.1) in Chap-
ter 3): 
Note that the method can also be written as 
xk+t = (J(xk)7 J(xk))-1J(xk)7 (J(xk)xk -F(xk)) 
= xk-(J(xk)TJ(xk)r1J(xk)7 F(xk)· 

68 
Chapter 4. The Gradient Method 
The Gauss-Newton direction is therefore dk = (](xk)7 J(xk))-1J(xk)7 F(xk)· Noting that 
V g(x) = 2J(x)7 F(x), we can conclude that 
meaning that the Gauss-Newton method is essentially a scaled gradient method with the 
following positive definite scaling matrix 
1 
T 
-1 
Dk= 2(](xk) J(xk)) · 
This fact also explains why the Gauss-Newton method is a descent direction method. The 
method described so far is also called the pure Gauss-Newton method since no stepsize is 
involved. To transform this method into a practical algorithm, a stepsize is introduced, 
leading to the damped Gauss-Newton method. 
Damped Gauss-Newton Method 
Input: e > 0 - tolerance parameter. 
Initialization: Pick "o E Rn arbitrarily. 
General step: For any k = 0, 1, 2, ... execute the following steps: 
(a) Set dk = (](xk)7J(xk))-1J(xk)7 F(xk)· 
(b) Set tk by a line search procedure on the function 
(c) Set xk+t = xk-tkdk. 
(c) If llV g(xk+i)ll < e, then STOP, and xk+t is the output. 
4.6 • The Fermat-Weber Problem 
The gradient method is the basis for many other methods that might seem at first glance 
to be unrelated to it. One interesting example is the Fermat-Weber problem. In the 17th 
century Pierre de Fermat posed the following problem: "Given three distinct points in 
the plane, find the point having the minimal sum of distances to these three points." The 
Italian physicist Torricelli solved this problem and defined a construction by ruler and 
compass for finding it (the point is thus called "the Torricelli point" or "the Torricelli-
Fermat point"). Later on, it was generalized by the German economist Weber to a prob-
lem in the space Rn and with an arbitrary number of points. The problem known today 
as "the Fermat-Weber problem" is the following: given m points in Rn : a1, .•• ,am-also 
called the "anchor point"-and m weights w 1,w2, ••• ,wm > 0, find a point x E Rn that 
minimizes the weighted distance of x to each of the points a1, ••• , am. Mathematically, 
this problem can be cast as the minimization problem 

4.6. The Fermat-Weber Problem 
69 
Note that the objective function is not differentiable at the anchor points a1, ••• ,am. This 
problem is one of the fundamental localization problems, and it is an instance of a facility 
locatwn problem. For example, a1, a2, ••• , am can represent locations of cities, and x will 
be the location of a new airport or hospital (or any other facility that serves the cities); the 
weights might be proportional to the size of population at each of the cities. One popular 
approach for solving the problem was introduced by Weiszfeld in 193 7. The starting point 
is the first order optimality condition: 
Vf(x) = 0. 
Note that we implicitly assume here that x is not an anchor point. The latter equality can 
be explicitly written as 
m 
x-a-
tt cvi llx-a: II = O. 
After some algebraic manipulation, the latter relation can be written as 
which is the same as 
1 
I:m 
C<.>·a-
' ' 
x= 
w 
• 
1:?:1 11x-!li11 i=l llx-adl 
We can thus reformulate the optimality condition as x = T(x), where Tis the operator 
1 
m 
CV ·a · 
T(x) = m 
w · I: ' ' . 
~ -• llx-a-11 
~i=l llx-a;ll i=l 
i 
Thus, the problem of finding a stationary point of f can be recast as the problem of 
finding a fixed point of T. Therefore, a natural approach for solving the problem is via a 
fixed point method, that is, by the iterations 
We can now write explicitly Weiszfeld's theorem for solving the Fermat-Weber problem. 
Weiszfeld's Method 
Initialization: Pick Xo E Rn such that x ;if al' az, ... , am. 
General step: For any k = 0, 1,2, ... compute 
(4.14) 
Note that the algorithm is defined only when the iterates xk are all different from 
a1, ••• , am. Although the algorithm was initially presented as a fixed point method, the 

70 
Chapter 4. The Gradient Method 
surprising fact is that it is basically a gradient method. Indeed, 
' ' 
1 
L:m 
GiJ·a· 
Xk+t = ~m 
""• 
llx -a·ll 
""-'i=l llxk-a,ll i=l 
k 
' 
1 
~ xk-ai 
= xk- ~m 
""• 
£...JGiJi llx -a·ll 
""-'i=t llxk-a, 11 i=t 
k 
, 
1 
=xk-
m 
""• 
Vf(xk)· 
Li=l llxk-a,11 
Therefore, Weiszfeld's method is essentially the gradient method with a special choice of 
. 
steps1ze: 
1 
tk = 
m 
""' 
• 
Li=t 11xk-a,11 
We are left of course with several questions. Is the method well-defined? That is, can 
we guarantee that none of the iterates xk is equal to any of the points a1, ••• , am? Is the 
sequence of objective function values decreases? Does the sequence {xk}k>o converge to 
a global optimal solution? We will answer at least part of these questions in this section. 
We would like to show that the generated sequence of function values is nonincreasing. 
For that, we define the auxiliary function 
where .Jl/ = 
{a1,ai, ... ,am}· The function h(·,·) has several important properties. First 
of all, the operator T can be computed on a vector x ~ .Pf by minimizing the function 
h(y,x) over ally E Rn. 
Lemma 4.16. For any x E JR.n\Jlf, one has 
T(x) = argmil\{h(y,x): y E Rn}. 
(4.15) 
Proof. The function h(·,x) is a quadratic function whose associated matrix is positive 
definite. In fact, the associated matrix is (l:~ 1 llx~, 11 )1. Therefore, by Lemma 2.41, the 
unique global minimum of (4.15), which we denote by y*, is the unique stationary point 
of h(·,x), that is, the point for which the gradient vanishes: 
Vyh(y*,x) = 0. 
Thus, 
Extracting y* from the last equation yields y* = T(x), and the result is established. 
D 
Lemma 4.16 basically shows that the update formula of Weiszfeld's method can be 
written as 

4.6. The Fermat-Weber Problem 
71 
We are now ready to prove other important properties of h, which will be crucial for 
showing that the sequence {/ ( xk)} k~o is nonincreasing. 
Lemma 4.17. /f x E Rn\.Jl/, then 
(a) h(x, x) = f (x), 
(b) h(y,x) > 2/ (y)-f (x)for any y E Rn, 
(c) /(T(x)) < /(x) and /(T(x)) = /(x) if and only if x = T(x), 
(d) x = T(x) if and only ifV f (x) = 0. 
Proof. (a) h(x,x) = L~t w; 
1/~-:;}/; = Lr=t w;llx-adl = /(x). 
(b) For any nonnegative number a and positive number b, the inequality 
a2 
->2a-b 
b -
holds. Substituting a = I ly-a; 11 and b = I Ix - a; 11, it follows that for any i = 1, 2, ... , m 
lly-a;ll2 
llx-a;ll > 2lly-a;ll-llx-a;ll· 
Multiplying the above inequality by w i and summing over i = 1, 2, ... , m, we obtain that 
m 
lly-a;ll2 
m 
m 
ttwi llx-a;ll > 2ttw;lly-a;1l-ttw;llx-a;ll. 
Therefore, 
h(y,x) > 2/(y)-/(x). 
(c) Since T(x) = argmin,eRnh(y,x), it follows that 
h(T(x),x) < h(x,x) = /(x), 
where the last equality follows from part (a). Part (b) of the lemma yields 
h(T(x),x) > 2/(T(x))-/(x), 
which combined with (4.17) implies 
f (x) > h(T(x),x) > 2/(T(x))-/(x), 
(4.16) 
(4.17) 
(4.18) 
establishing the fact that /(T(x)) < /(x). To complete the proof we need to show that 
/(T(x)) = /(x) if and only if T(x) = x. Of course, if T(x) = x, then /(T(x)) = /(x). 
To show the reverse implication, let us assume that /(T(x)) = /(x). By the chain of 
inequalities (4.18) it follows that h(x,x) = /(x) = h(T(x),x). Since the unique minimizer 
of h(·,x) is T(x), it follows that x = T(x). 
(d) The proof of (d) follows by simple algebraic manipulation. 
D 
We are now ready to prove the descent property of the sequence {/ (xk)} k>o under the 
assumption that all the iterates are not anchor points. 
-

72 
Chapter 4. The Gradient Method 
Lemma 4.18. Let {xkh>o 'be the sequen.ce generated by Weiszfeld's method (4.14), where we 
assume that xk ¢. d for all k > 0. Then we have the following: 
(a) The sequence {/(xk)} is nonincreasing: for any k > 0 the inequality /(xk+i) < /(xk) 
holds. 
(b) For any k, f(xk) = /(xk+i) if and only if\! f(xk) = 0. 
Proof. (a) Since xk ¢. d for all k, this result follows by substituting x = xk in part (c) of 
Lemma 4.17. 
(b) By part (c) of Lemma 4.17 it follows that f (xk) = /(xk+i) = f(T(xk)) if and 
only if xk = xk+t = T(xk)· By part (d) of Lemma 4.17 the latter is equivalent to 
\lf(xk) = 0. 
D 
We have thus shown that sequence {/(xk)}k>o is strictly decreasing as long as we are 
not stuck at a stationary point. The underlying -assumption that xk ¢. d is problematic 
in the sense that it cannot be verified easily. One approach to make sure that the sequence 
generated by the method does not contain anchor points is to choose the starting point 
Xo so that its value is strictly smaller than the values of the anchor points: 
f (x0) < min{/(a1),/(az), ... ,/ (ap)}. 
This assumption, combined with the monotonicity of the function values of the sequ-
ence generated by the method, implies that the iterates do not include anchor points. We 
will prove that under this assumption any convergent subsequence of {xk}k>o converges 
. 
. 
-
to a stationary point. 
Theorem 4.19. Let { xk} k>o 'be the sequen.ce generated by Weiszfeld's method and assume that 
f (Ko) < min{f (a1),/ (a2): ••• ,/(am)}. Then all the limit points of {xk }k>o are stationary 
points off. 
-
Proof. Let { xk } n>o be a subsequence of { xk} k>o that converges to a point x*. By the 
n 
-
-
monotonicity of the method and the continuity of the objective function we have 
f (x*) < /(Xo) <min{/ (a1),/ (az), ... ,/(ap) }. 
Therefore, x* ¢. d, and hence \7 f (x*) is defined. We will show that \7 f (x*) = 0. By the 
continuity of the operator Tat x*, it follows that the sequence xk +t = T(xk ) --.. T(x*) as 
n 
n 
n --.. oo. The sequence of function values {/ ( xk)} k>o is nonincreasing and bounded below 
by 0 and thus converges to a value which we denote by/*. Obviously, both {f(xk Hn>o 
n 
-
and {f(xk +l)}n>O converge to/*. By the continuity off, we thus obtain that/(T(x*)) = 
n 
-
f (x*) = f*, which by parts (c) and (d) of Lemma 4.17 implies that \7 f (x*) = 0. 
D 
It can be shown that for the Fermat-Weber problem, stationary points are global 
optimal solutions, and hence the latter theorem shows that all limit points of the sequence 
generated by the method are global optimal solutions. In fact, it is also possible to show 
that the entire sequence converges to a global optimal solution, but this analysis is beyond 
the scope of the book. 

4.7. Convergence Analysis of the Gradient Method 
4.7 •Convergence Analysis of the Gradient Method 
4. 7 .1 • Lipschitz Property of the Gradient 
73 
In this section we will present a convergence analysis of the gradient method employed 
on the unconstrained minimization problem 
min{/ (x): x E Rn}. 
We will assume that the objective function f is continuously differentiable and that its 
gradient V f is Lipschitz continuous over Rn, meaning that 
llV /(x)-V /(y)ll < Lllx-yll for any x,y E Rn. 
Note that if V f is Lipschitz with constant L, then it is also Lipschitz with constant l 
for all l > L. Therefore, there are essentially infinite number of Lipschitz constants for 
a function with Lipschitz gradient. Frequently, we are interested in the smallest possible 
Lipschitz constant. The class of functions with Lipschitz gradient with constant L is 
denoted by c£·1(Rn) or just c£·1• Occasionally, when the exact value of the Lipschitz 
constant is unimportant, we will omit it and denote the class by C1•1• The following are 
some simple examples of C 1•1 functions: 
• Linear functions Given a E Rn' the function /(x) = a7 xis in c~· 1 • 
• Quadratic functions Let A be an n x n symmetric matrix, b E Rn, and c E R. 
Then the function I (x) = x7 Ax+ 2h7 x + c is a C 1•1 function. 
To compute a Lipschitz constant of the quadratic function f (x) = x7 Ax+ 2h7 x + c, we 
can use the definition of Lipschitz continuity: 
llV/(x)-V/(y)ll = 2ll(Ax+h)-(Ay+h)ll = 211Ax-Ayll = 2llA(x-y)ll < 2llAll·llx-yll· 
We thus conclude that a Lipschitz constant of V f is 2llAll· 
A useful fact, stated in the following result, is that for twice continuously differen-
tiable functions, Lipschitz continuity of the gradient is equivalent to boundedness of the 
Hessian. 
Theorem 4.20. Let f be a twice continuously differentiable function over Rn. Then the 
following two claims are equivalent: 
(a) f E C£·1(Rn). 
(b) llV2/(x)ll <LforanyxeRn. 
Proof. (b)::::} (a). Suppose that llV2/(x)ll < L for any x E Rn. Then by the fundamental 
theorem of calculus we have for all x, y E Rn 
V f(y) = Vf(x)+ f V2f(x+ t(y-x))(y-x)dt 
=Vf(x)+(f V2f(x+t(y-x))di) ·(y-x), 

74 
Thus, 
Chapter 4. The Gradient Method 
llV f (y)-V f (x)ll = (f V2 f (x + t(y-x))d t) · (y-x) 
< f V2f(x+t(y-x))dt lly-xll 
< (f llV2f(x+ t(y-x))lldt) lly-xll 
< Liiy-xii, 
establishing the desired result IE c1·1• 
(a)=> (b). Suppose now that IE c1·1• Then by the fundamental theorem of calculus 
for any d E ]Rn and a > 0 we have 
Vf(x+ad)-Vf(x)= r 
V2f(x+td)ddt. 
Thus, 
(f V2f(x+td)dt )d = llVf(x+ad)-Vf(x)ll < allldll· 
Dividing by a and taking the limit a -+ o+, we obtain 
implying that llV2/(x)ll < L. 
D 
Example 4.21. Let f : JR -+ JR be given by f ( x) = J 1 + x2• Then 
1 
0 <f"(x) = 
< 1 
-
(l + x2)3/2 -
for any x E JR, and thus I E c11•1• 
I 
4. 7.2 • The Descent Lemma 
An important result for C 1• 1 functions is that they can be bounded above by a quadratic 
function over the entire space. This result, known as "the descent lemma," is fundamental 
in convergence proofs of gradient-based methods. 
Lemma 4.22 (descent lemma). Let f E Ci•1(1Rn). Then for any x,y E JRn 
L 
/(y) < /(x)+ V'/(x)T(y-x)+ 2 llx-y!l2. 
Proof. By the fundamental theorem of calculus, 
f(y)-f(x)= f (Vf(x+ t(y-x)),y-x)dt. 

4.7. Convergence Analysis of the Gradient Method 
Therefore, 
Thus, 
f(y)-f(x) = (Vf(x),y-x} + f \ilf(x+ t(y-x))-Vf(x),y-x}dt. 
lf(y)-f(x)-(V f(x),y-x}I = f (V f(x+ t(y-x))-Vf(x),y-x}dt 
< f l(V f(x+ t(y-x))-Vf(x),y-x)ldt 
< f llVf(x+ t(y-x))-Vf(x)ll · llr-xlldt 
< f tllly-xi!2dt 
L 
= -lly-x!l2. 
D 
2 
75 
Note that the proof of the descent lemma actually shows both upper and lower bounds 
on the function: 
L 
L 
/(x)+ V/(x)7 (y-x)-211x-yll2 </(y) </(x)+ V/(x)7(y-x)+ 2 llx-yll2• 
4. 7 .3 • Convergence of the Gradient Method 
Equipped with the descent lemma, we are now ready to prove the convergence of the 
gradient method for C 1• 1 functions. Of course, we cannot guarantee convergence to a 
global optimal solution, but we can show the convergence to stationary points in the 
sense that the gradient converges to zero. We begin by the following result showing a 
"sufficient decrease" of the gradient method at each iteration. More precisely, we show 
that at each iteration the decrease in the function value is at least a constant times the 
squared norm of the gradient. 
Lemma 4.23 (sufficient decrease lemma). 
Suppose that IE Ci•1(R.n). Then for any 
xERn and t >0 
f (x)-f (x- t V f (x)) > t ( 1-L;) llV f (x)ll2• 
(4.19) 
Proof. By the descent lemma we have 
Lt2 
f (x- tV f (x)) < /(x)- ti IV /(x)ll2 + z-llV /(x)ll2 
= f(x)- t ( 1-~) llV f (x)i!2. 
The result then follows by simple rearrangement of terms. 
D 

76 
Chapter 4. The Gradient Method 
Our goal now is to show that a sufficient decrease property occurs in each of the 
stepsize selection strategies: constant, exact line search, and backtracking. In the constant 
stepsize setting, we assume that tk = i E (0, f ). Substituting x = xk, t = i in (4.19) yields 
the inequality 
f (xk )-f (xk+t )> i ( 1-L:) llV f ("k )Ii'. 
(4.20) 
Note that the guaranteed descent in t~e gradient method per iteration is 
If we wish to obtain the largest guaranteed bound on the decrease, then we seek the max-
imum of i(l- if) over (0, f ). This maximum is attained at i = f, and thus a popular 
choice for a stepsize is f. In this case we have 
In the exact line search setting, the update formula of the algorithm is 
xk+t = xk - tk "V f(xk), 
where tk E argmint~of(xk- t"V /(xk)). By the definition of tk we know that 
f(xk-tk V f(xk)) < f ( "k- ~ V f(xk)), 
and thus we have 
where the last inequality was shown in (4.21). 
In the backtracking setting we seek a small enough stepsize tk for which 
(4.21) 
(4.22) 
(4.23) 
where a E (0, 1). We would like to find a lower bound on tk. There are two options. 
Either tk = s (the initial value of the stepsize) or tk is determined by the backtracking 
procedure, meaning that the stepsize tk = tk/ (3 is not acceptable and does not satisfy 
(4.23): 
f(xk)-f (xk- ~ Vf<"k)) <a~ 11Vf(xk)112. 
(4.24) 
Substituting x = xk, t =~in (4.19) we obtain that 
f(xk)-f ( xk- ~ Vf(xk)) > ~ ( 1- ~~) llVf(xk)ll', 
which combined with (4.24) implies that 
tk ( 
Ltk) 
tk 
(3 1- 2(3 < a (3, 

4.7. Convergence Analysis of the Gradient Method 
77 
which is the same as tk > i(t~a)/3. Overall, we obtained that in the backtracking setting 
we have 
. { 2(1-a)(3} 
tk >mm s, 
L 
, 
which combined with (4.23) implies that 
(4.25) 
We summarize the above discussion, or, better said, the three inequalities (4.20), (4.22), 
and (4.25), in the following result. 
Lemma 4.24 (sufficient decrease of the gradient method). Let f E Ci·1(:1R.n). Let 
{ xk} k?:O be the sequence generated by the gradient method for solving 
minf(x) 
xeRn 
with one of the following stepsize strategies: 
• constant stepsize i E (0, f ), 
• exact line search, 
• backtracking procedure with parameters s E R++'a E (0, 1), and f3E(0,1). 
Then 
where 
{ i(1- ii) 
M = 
2~ . { 
2(1-a)/3} 
amm s, 
L 
constant stepsize, 
exact line search, 
backtracking. 
(4.26) 
We will now show the convergence of the norms of the gradients llV f(xk)ll to zero. 
Theorem 4.25 (convergence of the gradient method). Let f E Ci•1(JR.n), and let {xk h?:o 
be the sequence generated by the gradient method for solving 
minf(x) 
xeRn 
with one of the following stepsize strategies: 
• constant stepsize i E (0, f ), 
• exact line search, 
• backtracking procedure with parameters s ER++' a E (0, 1 ), and f3 E (0, 1 ). 

78 
Chapter 4. The Gradient Method 
Assume that f is bounded below over Rn, that is, there exists m ER such that f(x) > m for 
allxeRn. 
Then we have the following: 
{a) The sequence {/ ( xk)} k>O is nonincreasing. In addition, for any k > 0, f ( xk+ 1) < 
f(xk) unless V f(xk) -0. 
{b) V f (xk) -+ 0 as k -+ oo. 
Proof. (a) By {4.26) we have that 
{4.27) 
for some constant M > 0, and hence the equality f(xk) = f(xk+t) can hold only when 
Vf(xk)=O. 
{b) Since the sequence {f(xk)}k>o is nonincreasing and bounded below, it converges. 
Thus, in particular f(xk)- f(xk+tf-+ 0 ask-+ oo, which combined with {4.27) implies 
that llV f(xk)ll-+ 0 ask-+ oo. 
D 
We can also get an estimate for the rate of convergence of the gradient method, or 
more precisely of the norm of the gradients. This is done in the following result. 
Theorem 4.26 (rate of convergence of gradient norms). Under the setting of Theorem 
4.25, let f* be the limit of the convergent sequence {f(xk)}k;:::o· Then for any n = 0, 1,2, ... 
where 
min llV f (xk)ll < 
k=0,1, ... ,n 
M={ 
i ( 1- i;) 
1 
2L 
. { 
2/3(1-a)} 
amm s, 
L 
f(Xo)-f* 
M(n+l)' 
constant stepsize, 
exact line search, 
backtracking. 
Proof. Summing the inequality {4.26) over k = 0, 1, ... , n, we obtain 
n 
f (Xo)-f (xn+1) >ML llV f (xk)ll2 • 
k=O 
Since f(xn+t) > f*, we can thus conclude that 
n 
f(Xo)-f* > MLllVf(xk)l!2. 
k=O 

Exercises 
79 
Finally, using the latter inequality along with the fact that for every k = 0, 1, ... , n the 
obvious inequality llV f(xk)ll2 > mink=O,t, ... ,n llV f(xk)ll2 holds, it follows that 
f(Xo)-f*>M(n+l) min llV'f(xk)ll2, 
k=0,1, ... ,n 
implying the desired result. 
D 
Exercises 
4.1. Let f E C~' 1 (1Rn) and let {xk }k~o be the sequence generated by the gradient method 
with a constant stepsize tk = ±. Assume that xk -+ x*. Show that if V' f ( xk) f; 0 
for all k > 0, then x* is not a local maximum point. 
4.2. [9, Exercise 1.3.3] Consider the minimization problem 
min{x7 Qx: x E IR2}, 
where Q is a positive definite 2 x 2 matrix. Suppose we use the diagonal scaling 
matnx 
D =(Qo~· o ) 
Q;l . 
Show that the above scaling matrix improves the condition number of Q in the 
sense that 
x(ot/2Qot/2) < x(Q). 
4.3. Consider the quadratic minimization problem 
min{x7 Ax: x E IR5}, 
where A is the 5 x 5 Hilbert matrix defined by 
1 
Ai,i = i + j-l' i,j = 1,2,3,4,5. 
The matrix can he constructed via the MATLAB command A = hi lb ( 5 ) . Run 
the following methods and compare the number of iterations required by each 
of the methods when the initial vector is Xo = (1,2,3,4,5)7 to obtain a solution x 
with llV f(x)ll < 10-4: 
• gradient method with backtracking stepsize rule and parameters a = 0.5, f3 = 
0.5,s = 1; 
• gradient method with backtracking stepsize rule and parameters a = 0.1, f3 = 
0.5,s = 1; 
• gradient method with exact line search; 
• diagonally scaled gradient method with diagonal elements Di i -
} 
, i 
II 
1, 2, 3, 4, 5 and exact line search; 
• diagonally scaled gradient method with diagonal elements Dii -
At , i 
II 
1,2,3,4,5 and backtracking line search with parameters a = 0.1,{3 = 0.5, 
s = 1. 

80 
Chapter 4. The Gradient Method 
4.4. Consider the Fermat-Weber problem 
mi~ {f(x)= i:w;llx-a;ll}, 
xe!R 
i=l 
where w 1, ••• , w m > 0 and a1, ••• , am E Rn are m different points. Let 
p E argmini=t,l, ... ,mf(a;). 
Suppose that 
a -a. 
2:w; II P -
~II > wp. 
i=fp 
ap 
a, 
(i) Show that there exists a direction d E Rn such that f 1 ( ap; d) < 0. 
(ii) Showthatthereexists:xa E Rn satisfyingf(Xo) < min{f (a1),f (ai), ... ,f (ap)}. 
Explain how to compute such a vector. 
4.5. In the "source localization problem" we are given m locations of sensors a1, a2, ••• , 
am E Rn and approximate dist~ces between the sensors and an unknown "source" 
located at x E Rn: 
d; ~ llx-a;ll· 
The problem is to find and estimate x given the locations a1,a2,. .. ,am and the 
approximate distances d1, di, ... , dm. A natural formulation as an optimization 
problem is to consider the nonlinear least squares problem 
We will denote the set of sensors by .121 = { a1, a2, ••• , am}. 
(i) Show that the optimality condition V f(x) = 0 (x ~ d) is the same as 
1 { m 
m 
x-a. } 
x= - 2:a; + 2:di II -
~II . 
m 
i=t 
i=t 
x 
a, 
(ii) Show that the corresponding fixed point method 
is a gradient method, assuming that xk ~ .121 for all k > 0. What is the step-
size? 
4.6. Another formulation of the source localization problem consists of minimizing 
the following objective function: 
(SL2) 
min {f(x) = ~(llx-a;112 -d~)2 }. 
xe!Rn 
~ 
z 
t=1 
This is of course a nonlinear least squares problem, and thus the Gauss-Newton 
method can be employed in order to solve it. We will assume that n = 2. 

Exercises 
81 
(i) Show that as long as all the points a1, a2, ... , am do not reside on the same line 
in the plane, the method is well-defined, meaning that the linear least squares 
problem solved at each iteration has a unique solution. 
(ii) Write a MATLAB function that implements the damped Gauss-Newton 
method employed on problem (SL2) with a backtracking line search strat-
egy with parameters s = 1,a = f3 = 0.5,e = 10-4. Run the function on the 
two-dimensional problem (n = 2) with 5 anchors (m = 5) and data generated 
by the MATLAB commands 
randn ( ' seed' , 31 7 ) ; 
A=randn(2,5); 
x=randn(2,l); 
d=sqrt(sum((A-x*ones(l,5)) .A2))+0.05*randn(l,5); 
d=d'; 
The columns of the 2 x 5 matrix A are the locations of the five sensors, 
x is the "true" location of the source, and d is the vector of noisy measure-
ments between the source and the sensors. Compare your results (e.g., num-
ber of iterations) to the gradient method with backtracking and parameters 
s = 1, a = f! = 0.5, e = 10-4. Start both methods with the initial vector 
( 1000, -500)7. 
4.7. Let f (x) = xT Ax+ 2bT x + c, where A is a symmetric n x n matrix, b E Rn, and 
c E JR. Show that the smallest Lipschitz constant of V f is 2llAll· 
4.8. Let I : Rn -+JR be given by I (x) = .../ 1 + llxll2. Show that IE cf·1• 
4.9. Let f E Ci•1(1Rm), and let A E Rmxn, b E Rm. Show that the function g: Rn-+ JR 
defined by g(x) =/(Ax+ b) satisfies g E Cl•1(1Rn), where i = llAll2L. 
4.10. Give an example of a function f E Ci•1(1R) and a starting point Xo E JR such that 
the problem min f ( x) has an optimal solution and the gradient method with con-
stant stepsize t = f diverges. 
4.11. Suppose that IE Ci•1(1Rn) and assume that V2/(x) >- 0 for any x E Rn. Suppose 
that the optimal value of the problem mi°xeRnf(x) is/*. Let {xkh>o be these-
quence generated by the gradient method with constant stepsize t. Show that if 
{ xk} k~o is bounded, then f ( xk) -+ f* as k -+ oo. 

Chapter 5 
Newton's Method 
5.1 • Pure Newton's Method 
In the previous chapter we considered the unconstrained minimization problem 
min {f (x) : x E Rn}, 
where we assumed that f is continuously differentiable. We studied the gradient method 
which only uses first order information, namely information on the function values and 
gradients. In this chapter we assume that f is twice continuously differentiable, and we 
will present a second order method, namely a method that uses, in addition to the infor-
mation on function values and gradients, evaluations of the Hessian matrices. We will 
concentrate on Newton's method. The main idea of Newton's method is the following. 
Given an iterate xk, the next iterate xk+l is chosen to minimize the quadratic approxima-
tion of the function around xk: 
The above update formula is not well-defined unless we further assume that \72/(xk) is 
positive definite. In that case, the unique minimizer of the minimization problem (5.1) is 
the unique stationary point: 
which is the same as 
(5.2) 
The vector -(\72 f ( xk) )-1 \7 f ( xk) is called the Newton direction, and the algorithm in-
duced by the update formula (5.2) is called the pure Newton's method. Note that when 
\72 f ( xk) is positive definite for any k, pure Newton's method is essentially a scaled gradi-
ent method, and Newton's directions are descent directions. 
83 

84 
Chapter 5. Newton's Method 
Pure Newton's Method 
Input: c > 0 - tolerance parameter. 
Initialization: Pick "o E Rn arbitrarily. 
General step: For any k = 0, 1,2, ... execute the following steps: 
(a) Compute the Newton direction dk, which is the solution to the linear system 
V 2f{xk)dk =-Vf(xk). 
(b) Set xk+t = xk +dk. 
(c) If llV f (xk+i)ll < c, then STOP, and xk+t is the output. 
At the very least, Newton's method requires that V 2f(x) is positive definite for 
every x E Rn, which in particular implies that there exists a unique optimal solution x*. 
However, this is not enough to guarantee convergence, as the following example illustrates. 
Example 5.1. Consider the function f ( x) = J 1 + x2 defined over the real line. The 
minimizer off over IR is of course x = 0. The first and second derivatives off are 
I 
X 
f (x) = 
, 
J1+x2 
II 
1 
f (x)= {1+x2)3/2· 
Therefore, (pure) Newton's method has the form 
f'(xk) 
( 
2) 
3 
xk+t = xk-
II 
= xk-xk 1 +xk =-xk. 
f (xk) 
We therefore see that for IXol > 1 the method diverges and that for lx01 < 1 the method 
converges very rapidly to the correct solution x* = 0. 
I 
Despite the fact that a lot of assumptions are required to be made in order to guarantee 
the convergence of the method, Newton's method does have one very attractive feature: 
under certain assumptions one can prove local quadratic rate of convergence, which means 
that near the optimal solution the errors ek = llxk -x*ll (where x* is the unique optimal 
solution) satisfy the inequality ek+t < M e2 for some positive M > 0. This property essen-
tially means that the number of accuracy digits is doubled at each iteration. 
Theorem 5.2 (quadratic local convergence of Newton's method). Let f be a twice 
continuously differentiable function defined over Rn. Assume that 
• there exists m > 0 for which V2f(x) >- mifor any x E Rn, 
• there exists L > Oforwhich llV2f(x)-V2f(y)ll < Lllx-yllforany x,yEIRn. 
Let {xkh>o be the sequence generated by Newton's method, and let x* be the unique mini-
mizer off over Rn. Then for any k = 0, 1, ... the inequality 
L 
. 
2 
llxk+t -x*ll < 2m llxk -x*ll 
(5.3) 
holds. In addition, if I l"o - x* 11 < 7, then 
2 ( 1)2" 
llxk -x*ll < ~ 2 , k = 0, 1,2, .... 
(5.4) 

5.1. Pure Newton's Method 
85 
Proof. Let k be a nonnegative integer. Then 
xk+1-x* 
xk-(\72/(xk))-1\l/(xk)-x* 
Vf(x*)=O xk -x* + (\72/ (xk))-1(\l f (x*)-\! f(xk)) 
-
xk-x' +(V2/(x.i.)r1 f [V2/(xk + t(x' -xk))](x'-xk)dt 
(V2/(xk))-1 f (V2/(x.i, + t(x' -xk))-V2 /(xk) ](x' -xk)dt. 
Combining the latter equality with the fact that \72 I ( xk) >- ml implies that 11(\72 I ( xk) )-111 
<!·Hence, 
llxk+i -x'll < ll(V2/(xk))-111 f [V2/(xk + t(x'-xk))-V2/(xk) ](x' -xk)dt 
< ll(V2/(xk))-111 f ii[V2/(x.i, + t(x'-xk))-V2/(xk)](x'-xk)ll dt 
< ll(V2/ (x.i.)r'll f 11v2 I (x.i, + t(x' -xk))-v2 I (xk)ll · llx' -xklld t 
L 11 
* 2 
L 
* 2 
< -
tllxk-x II dt = -llxk-x II · 
m 
0 
2m 
We will prove inequality (5.4) by induction on k. Note that fork= 0, we assumed that 
m 
llXo-x*ll < L' 
so in particular 
establishing the basis of the induction. Assume that (5.4) holds for an integer k, that is, 
llxk-x*ll < 2Z(!)2k; we will show it holds fork+ 1. Indeed, by (5.3) we have 
L 
L (2m ( 1)2k)2 2m ( 1 
)2k+i 
llxk+i-x*ll< 2mllxk-x*W< 2m L 2 
=y 2 
' 
proving the desired result. 
D 
A very naive implementation of Newton's method in MATLAB is given below. 
function x=pure_newton(f,g,h,xO,epsilon) 
% Pure Newton's method 
% 
% INPUT 
% ============== 
% f 
% g 
% h 
objective function 
gradient of the objective function 
Hessian of the objective function 

86 
Chapter 5. Newton's Method 
% xO ........... initial point 
% epsilon ..... tolerance parameter 
% OUTPUT 
% ============== 
% x - solution obtained by Newton's method (up to some tolerance) 
if (nargin<5) 
epsilon=le-5; 
end 
x=xO; 
gval=g (x); 
hval=h(x); 
iter=O; 
while ((norm(gval)>epsilon)&&(iter<lOOOO)) 
iter=iter+l; 
x=x-hval\gval; 
fprintf('iter= %2d f(x)=%10.10f\n',iter,f{x)) 
gval=g (x); 
hval=h(x); 
end 
if (iter==lOOOO) 
fprintf('did not converge') 
end 
Note that the above implementation does not check the positive definiteness of the 
Hessian, and it essentially assumes implicitly that this property holds. As already men-
tioned, Newton's method requires quite a lot of assumptions in order to guarantee con-
vergence, and hence the described implementation includes a divergence criteria (10000 
iterations). 
Example 5.3. Consider the minimization problem 
min 100x4 + O.Oly4, 
x,y 
whose optimal solution is obviously (x,y) = (0,0). This is a rather poorly scaled prob-
lem, and indeed the gradient method with initial vector Xo = (1, lf and parameters 
(s,a,(3,t) = (1,0.5,0.5, 10-6) converges after the huge amount of 14612 iterations: 
>> f:@(X)100*X(l)A4+0,01*X(2)A4; 
>> Q=@(X) [4Q0*X(l)A3;0.04*X(2)A3); 
>> [x,fun_val]=gradient_method_backtracking(f,g, [l;l],l,0.5,0.5,le-6) 
iter_number = 
1 norm_grad = 90.513620 fun_val = 13.799181 
iter_number = 
2 norm_grad = 32.381098 fun_ val = 3.511932 
iter_number = 
3 norm_grad = 11. 472585 fun_ val = 0.887929 
iter_number = 14611 norm_grad = 0.000001 fun_ val = 0.000000 
iter_number = 14612 noritLgrad = 0.000001 fun_ val = 0.000000 
Invoking pure Newton's method, we obtain convergence after only 17 iterations: 
>> h=@(x) [12Q0*X(l}"2,0;0,Q.12*X{2)"2]; 
>> pure_newton(f,g,h, [l;l],le-6) 
iter= 
1 f{x}=19.7550617284 
iter= 
2 f(x)=3.9022344155 
iter= 
3 f(x}=0.7708117364 

5.1. Pure Newton's Method 
iter= 15 f(x)=0.0000000027 
iter= 16 f (x)=0.0000000005 
iter= 17 f (x)=0.0000000001 
87 
Note that the basic assumptions required for the convergence of Newton's method as 
described in Theorem 5.2 are not satisfied. The Hessian is always positive semidefinite, 
but it is not always positive definite and does not satisfy a Lipschitz property. 
I 
The previous example exhibited convergence even when the basic underlying assump-
tions of Theorem 5.2 are not satisfied. However, in general, convergence is unfortu-
nately not guaranteed in the absence of these very restrictive assumptions. 
Example 5.4. Consider the minimization problem 
min Jx~+ 1+Jx;+1, 
X1,X2 
whose optimal solution is x = 0. The Hessian of the function is 
0 
) 
1 
>- 0. 
(x~+t)312 
Note that despite the fact that the Hessian is positive definite, there does not exist an 
m > 0 for which V2/(x) >- ml. This violation of the basic assumptions can be seen 
practically. Indeed, if we employ Newton's method with initial vector Xo = (1, 1? and 
tolerance parameter t = 10-s we obtain convergence after 37 iterations: 
>> f=@(x)sqrt(l+x(l)A2)+sqrt(l+x(2)A2); 
>> g=@(x) [x(l)/sqrt(x(l)A2+l);x(2)/sqrt(x(2)A2+1)]; 
>> h=@(x)diag([l/(x(l)A2+l)Al.5,1/(x(2)A2+1)Al.5]); 
>> pure_newton(f,g,h, [1;1],le-8) 
iter= 
1 f (x)=2.8284271247 
iter= 
2 f(x)=2.8284271247 
iter= 30 f(x)=2.8105247315 
iter= 31 f(x)=2.7757389625 
iter= 32 f(x)=2.6791717153 
iter= 33 f (x)=2.4507092918 
iter= 34 f (x)=2.1223796622 
iter= 35 f(x)=2.0020052756 
iter= 36 f(x)=2.0000000081 
iter= 37 f (x)=2.0000000000 
Note that in the first 30 iterations the method is almost stuck. On the other hand, the 
gradient method with backtracking and parameters ( s, a, /3) = ( 1, 0.5, 0.5) converges after 
only 7 iterations: 
>> [x,fun_val]=gradient_method_backtracking(f,g, [l;l],l,0.5,0.5,le-8); 
iter_number = 
1 norm_grad = 0.397514 fun_val = 2.084022 
iter_number = 
2 norm_grad = 0.016699 fun_val = 2.000139 
iter_number = 
3 norm_grad = 0.000001 fun_val = 2.000000 
iter_number = 
4 nornt...grad = 0.000001 fun_val = 2.000000 

88 
Chapter 5. Newton's Method 
iter_number = 
iter_number = 
iter_number = 
5 norm_grad = 0.000000 fun_val = 2.000000 
6 norm_grad = 0.000000 fun_val = 2.000000 
7 norm_grad = 0 . 000000 fun_val = 2.000000 
If we start from the more distant point (10, 10)7. The situation is much more severe. 
The gradient method with backtracking converges after 13 iterations: 
>> [x,fun_val]=gradient_method_backtracking(f,g, [10 ; 10] , l,0.5,0.5,le- 8); 
iter_nurnber = 
1 norm_grad = 1 . 405573 fun_val = 18 . 120635 
iter_nurnber = 
2 norm_grad = 1.403323 fun_val = 16 . 146490 
iter_nurnber = 
12 norm_grad = 0.000049 fun_val = 2.000000 
iter_nurnber = 
13 norm_grad = 0.000000 fun_val = 2.000000 
Newton's method, on the other hand, diverges: 
>> pure_newton(f,g,h, [10;10],le-8); 
iter= 
1 f (x)=2000.0009999997 
iter= 
2 f (x)=1999999999.9999990000 
iter= 
3 f (x)=1999999999999997300000000000.0000000 
iter= 
4 f(x)=l99999999999999230000000000000000000 .... 
iter= 
5 f (x)= 
Inf 
I 
As can be seen in the last example, pure Newton's method does not guarantee descent 
of the generated sequence of function values even when the Hessian is positive definite. 
This drawback can be rectified by introducing a stepsize chosen by a certain line search 
procedure, leading to the so-called t:Limped Newton's method. 
5.2 •Damped Newton's Method 
Below we describe the damped Newton's method with a backtracking stepsize strategy. 
Of course, other stepsize strategies may be used. 
Damped Newton's Method 
Input: a,{3E(0,1)- parameters for the backtracking procedure. 
t > 0 - tolerance parameter. 
Initialization: Pick Xo E Rn arbitrarily. 
General step: For any k = 0, 1,2, ... execute the following steps: 
(a) Compute the Newton direction dk, which is the solution to the linear system 
V2/(xk)dk =-V/(xk)· 
(b) Set tk = 1. While 
' 
set tk := {3tk. 
(c) xk+t = xk + tkdk. 
(c) If llV /(xk+i)ll < t, then STOP, and xk+l is the output. 

5.2. Damped Newton's Method 
A MATLAB implementation of the method is given below. 
function x=newton_backtracking(f,g,h,xO,alpha,beta,epsilon) 
% Newton's method.with backtracking 
% 
% INPUT 
%======================================= 
% f ......... objective function 
% g ......... gradient of the objective function 
% h ......... hessian of the objective function 
% xO ......... initial point 
% alpha ..... tolerance parameter for the stepsize selection strategy 
% beta ...... the proportion in which the stepsize is multiplied 
% 
at each backtracking step (O<beta<l) 
% epsilon ... tolerance parameter for stopping rule 
% OUTPUT 
%======================================= 
% x ......... optimal solution (up to a tolerance) 
% 
of min f(x) 
% fun_val 
optimal function value 
x=xO; 
gval=g (x); 
hval::::h(x); 
d=hval\gval; 
iter=O; 
while ((norm(gval)>epsilon)&&(iter<lOOOO)) 
iter=iter+l; 
end 
t=l; 
while(f (x-t*d)>f (x)-alpha*t*gval'*d) 
t=beta*t; 
end 
X=X-t*d; 
fprintf('iter= %2d f(x)=%10.10f\n',iter,f(x)) 
gval=g(x); 
hval=h(x); 
d=hval\gval; 
if (iter==lOOOO) 
fprintf('did not converge\n') 
end 
89 
Example 5.5. Continuing Example 5.4, invoking Newton's method with the same initial 
vector x0 = (10, 10)7 that caused the divergence of pure Newton's method, results in 
convergence after 17 iterations. 
>> newton_backtracking(f,g,h, [10;10),0.5,0.5,le-8); 
iter= 
1 f (x)=4.6688169339 
iter= 
2 f (x)=2.4101973721 
iter= 
3 f (x)=2.0336386321 
iter= 16 f (x)=2.0000000005 
iter= 17 f(x)=2.0000000000 
I 

90 
Chapter 5. Newton's Method 
5.3 • The Cholesky Factorization 
An important issue that naturally arises when employing Newton's method is the one of 
validating whether the Hessian matrix is positive definite, and if it is, then another issue 
is how to solve the linear system V 2 f ( xk )d = -V f ( xk ). These two issues are resolved by 
using the Cholesky factorization, which we briefly recall in this section. 
Given an n x n positive definite matrix A, a Cholesky factorization is a factorization 
of the form 
A=LLr, 
where L is a lower triangular n x n matrix whose diagonal is positive. Given a Cholesky 
factorization, the task of solving a linear system of equations of the form Ax = b can be 
easily done by the following two steps: 
A. Find the solution u of Lu = b. 
B. Find the solution x of LT x = u. 
Since L is a triangular matrix with a positive diagonal, steps A and B can be carried out by 
backward or forward substitution, which requires only an order of n2 arithmetic opera-
tions. The computation of the Cholesky factorization requires an order of n3 operations. 
The computation of the Cholesky factor L is done via a simple recursive formula. 
Consider the following block matrix partition of the matrices A and L: 
A= (A¥ 
A12), 
L = (L11 
0 ) , 
At2 
A22 
L21 
Lz2 
whereA11 E lR,A12 E JR1x{n-1),A22 E JR(n-t)x(n-1) ,Lu E lR,Lz1E1Rn-1,Lz2 E JR(n-t)x(n-1). 
Since A= LLT we have 
( A11 
A12)-( Li1 
LuLit 
) 
Ai'z 
A22 -
Lu Lz1 
L11 Lit + L22Lii . 
Therefore, in particular, 
and we can thus also write 
1 
Lz2Lii = A22 -Lz1 LI1 = A22 - A AfzA12· 
11 
We are left with the task of finding a Cholesky factorization of the (n -1) x (n -1) 
matrix A22--j-Ai'zA12• Continuing in this way, we can compute the complete Cholesky 
11 
factorization of the matrix. The process is illustrated in the following simple example. 
Example 5.6. Let 
A=G 
3 i1} 
17 
21 
107 
We will denote the Cholesky factor by 
('" 
0 0) 
L= 
121 
122 
0 
. 
131 
l32 
/33 

5.3. The Cholesky Factorization 
91 
Then 
and 
(121)- 1 (3)-(1) 
/31 - ../9 3 -
1 . 
We now need to find the Cholesky factorization of 
To do so, let us write 
T 
(/22 0) 
~2 -
l 
l 
. 
32 
33 
Consequently, 122 = .JI6 = 4 and /32 = }a· 20 = 5. We are thus left with the task of 
finding the Cholesky factorization of 
1 
106- 16. (20. 20) = 81, 
which is of course /33 = J8I = 9. To conclude, the Cholesky factor is given by 
(3 0 0) 
L= 1 4 
0 . 
1 5 9 
I 
The process of computing the Cholesky factorization is well-defined as long as all the 
diagonal elements l;; that are computed during the process are positive, so that comput-
ing their square root is possible. The positiveness of these elements is equivalent to the 
property that the matrix to be factored is positive definite. Therefore, the Cholesky fac-
torization process can be viewed as a criteria for positive definiteness, and it is actually the 
test that is used in many algorithms. 
Example 5.7. Let us check whether the matrix 
A=G ~ ~) 
is positive definite. We will invoke the Cholesky factorization process. We have 
(121) 1 (4) 
/31 = ../2 7 . 
Now we need to find the Cholesky factorization of 
At this point the process fails since we are required to find the square root of -2, which 
is of course not a real number. The conclusion is that A is not positive definite. 
I 

92 
Chapter 5. Newton's Method 
In MATLAB, the Cholesky factorization if performed via the function chol. Thus, 
for example, the factorization of the matrix from Example 5.6 can be done by the follow-
ing MATLAB commands: 
>> A=[9,3,3;3,17,21;3,21,107]; 
>> L=chol(A, 'lower') 
L = 
3 
1 
1 
0 
4 
5 
0 
0 
9 
The function chol can also output a second argument which is zero if the matrix is 
positive definite, or positive when the matrix is not positive definite, and in the latter 
case a Cholesky factorization cannot be computed. For the nonpositive definite matrix 
of Example 5.7 we obtain 
>> A=[2,4,7;4,6,7;7,7,4]; 
>> [L,p]=chol(A,'lower'); 
>> p 
p = 
2 
As was already mentioned several times, Newton's method (pure or not) assumes that the 
Hessian matrix is positive definite and we are thus left with the question of how to employ 
Newton's method when the Hessian is not always positive definite. There are several ways 
to deal with this situation, but perhaps the simplest one is to construct a hybrid method 
that employs either a Newton step at iterations in which the Hessian is positive definite 
or a gradient step when the Hessian is not positive definite. The algorithm is written in 
detail below and also incorporates a backtracking procedure. 
Hybrid Gradient-Newton Method 
Input: a, f3 E ( 0, 1) - parameters for the backtracking procedure. 
£ > 0 - tolerance parameter. 
Initialization: Pick "o E }Rn arbitrarily. 
General step: For any k = 0, 1, 2, ... execute the following steps: 
(a) If V2/(xk) >- 0, then take dk as the Newton direction dk, which is the solution 
to the linear system V 2f(xk)dk =-Vf(xk)· Otherwise, set dk =-Vf(xk)· 
{b) Set tk = 1. While 
set tk := f3 tk 
(c) xk+1 = xk + tkdk. 
(c) If llV f(xk+i)ll <£,then STOP, and xk+l is the output. 

5.3. The Cholesky Factorization 
93 
Following is a MATLAB implementation of the method that also incorporates the 
Cholesky factorization. 
function x=newton_hybrid(f,g,h,xO,alpha,beta,epsilon) 
% Hybrid Newton's method 
% 
% INPUT 
%======================================= 
% f ......... objective function 
% g ......... gradient of the objective function 
% h ......... hessian of the objective function 
% xO ......... initial point 
% alpha ..... tolerance parameter for the stepsize selection strategy 
% beta ...... the proportion in which the stepsize is multiplied 
% 
at each backtracking step (O<beta<l) 
% epsilon ... tolerance parameter for stopping rule 
% OUTPUT 
%======================================= 
% x ......... optimal solution (up to a tolerance) 
% 
of min f(x) 
% fun_val 
optimal function value 
x=xO; 
gval=g (x); 
hval=h(x); 
[L,p]=chol(hval, 'lower'); 
if (p==O) 
d=L' \ (L\gval); 
else 
d=gval; 
end 
iter=O; 
while ((norm(gval)>epsilon)&&(iter<10000)) 
iter=iter+l; 
end 
t=l; 
while(f(x-t*d)>f(x)-alpha*t*gval'*d) 
end 
X=X-t*d; 
fprintf('iter= %2d f(x)=%10.10f\n' ,iter,f(x)) 
gval=g (x); 
hval=h(x); 
[L,p]=chol(hval, 'lower'); 
if (p==O) 
d=L'\(L\gval); 
else 
d=gval; 
end 
if (iter==lOOOO) 
fprintf('did not converge\n') 
end 
Example 5.8 (Rosenbrock function). Recall that the Rosenbrock function introduced 
in Example 4.13 is given by f(x 1,x2) = 100(x2 -
xD2 + (1- x1)2 and is severely ill-
conditioned near the minimizer ( 1, 1) (which is the unique stationary point). In Example 
4.13 we employed the gradient method with a backtracking stepsize selection strategy, 
and it took approximately 6900 iterations to converge from the starting point (2,5)7 to 

94 
Chapter 5. Newton's Method 
a point satisfying llV/(x)ll < 10-5• Employing hybrid Newton's method with the same 
starting point and stopping criteria results in a fast convergence after only 18 iterations: 
>> f=@{x}100*{X{2}-x{l)A2)A2+{1-x{l}}A2; 
>> g=@{x} [-400* {x{2}-x{1} A2) *X{l}-2* {1-x{l}} ;200* {x(2}-x(1} A2}] 
>> h=@(x} (-400*X(2}+1200*X{l}A2+2,-400*X{1} ;-400*X{1} ,200] i 
>> x=newton_hybrid{f,g,h, [2;5],0.5,0.5,le-5}; 
iter= 
1 f{x}=3.2210220151 
iter= 
2 f {x}=l.4965858368 
iter= 16 f {x}=0.0000000000 
iter= 17 f{x}=0.0000000000 
The contour plots of the Rosenbrock. function along with the 18 iterates are illustrated in 
Figure 5.1. 
I 
- 4 
- 3 
- 2 
- 1 
0 
1 
2 
3 
4 
Figure 5.1. Contour lines of the Rosenbrock function along with the 18 iterates of the hybrid 
Newton's method. 
Exercises 
5.1. Find without MATLAB the Cholesky factorization of the matrix 
1 
A= 2 
4 
2 
13 
23 
7 38 
4 
7 
23 
38 
77 
122 . 
122 294 

Exercises 
5.2. Consider the Freudenstein and Roth test function 
where 
/(x) = fi(x)2 + fi(x)2, 
fi(x) = -13 + x1 + ((5-x2)x2 -2)x2, 
fi(x) = -29 + x1+((x2+1)x2 -14)x2. 
95 
(i) Show that the function f has three stationary points. Find them and prove 
that one is a global minimizer, one is a strict local minimum and the third is 
a saddle point. 
(ii) Use MATLAB to employ the following three methods on the problem of 
minimizing f: 
1. the gradient method with backtracking and parameters ( s, a, {3) 
(1,0.5,0.5). 
2. the hybrid Newton's method with parameters (s,a,[3) = (0.5,0.5). 
3. damped Gauss-Newton's method with a backtracking line search strat-
egy with parameters ( s, a, {3) = ( 1, 0.5, 0.5). 
All the algorithms should use the stopping criteria llV f (x)ll < 10-5• Each 
algorithm should be em/loyed four times on the following four starting 
points: (-50,7)7,(20,7) ,(20,-18)r,(5,-10)T. For each of the four start-
ing points, compare the number of iterations and the point to which each 
method converged. If a method did not converge, explain why. 
5.3. Let f be a twice continuously differentiable function satisfying LI>- \72 f (x) >- ml 
for some L > m > 0 and let x* be the unique minimizer off over Rn. 
(i) Show that 
m 
/(x)-/(x*) > 2llx-x*ll2 
for any xERn. 
(ii) Let { xk h>o be the sequence generated by damped Newton's method with 
constant stepsize tk = 7 . Show that 
m 
T 
2 
1 
/(xk)-/(xk+i) > 2L V/(xk) (V /(xk))- V/(xk)· 
(iii) Show that xk -+ x* as k -+ oo. 

Chapter 6 
Convex Sets 
In this chapter we begin our exploration of convex analysis, which is the mathematical 
theory essential for analyzing and understanding the theoretical and practical aspects of 
. . . 
opum1zauon. 
6.1 • Definition and Examples 
We begin with the definition of a convex set. 
Definition 6.1 (convex sets). A set C C Rn is called convex if for any x,y E C and 
). E [O, 1 ], the point Ax+ ( 1-A)y belongs to C. 
The above definition is equivalent to saying that for any x, y E C, the line segment 
[ x, y] is also in C. Examples of convex and nonconvex sets in R2 are illustrated in Figure 
6.1. We will now show some basic examples of convex sets. 
convex sets 
nonconvex set 
Figure 6.1. The three left sets are convex, while the three right sets are nonconvex. 
Example 6.2 (convexity of lines). A line in Rn is a set of the form 
L={z+td:tER}, 
where z,d E Rn and d f; 0. To show that Lis indeed a convex set, let us take x,y EL. 
Then there exist ti, t2 E R such that x = z + ti d and y = z + t2d. Therefore, for any 
97 

98 
Chapter 6. Convex Sets 
A E (0, 1] we have 
...lx+(l-A)y = A(z+ t1d)+(1-A)(z+ t2d) = z+(At1 +(1-A)t2)deL. 
I 
Similarly we can show that for any x, y E Rn, the closed and open line segments 
[x,y],(x,y) are also convex sets. Simpler examples of convex sets are the empty set 0 
and the entire space Rn. A hyperplane is a set of the form H = {x E Rn: aT x = b }, where 
a E Rn\ { 0}, b E R, and the associated half-space is the set H- = { x E Rn : a T x < b}. Both 
hyperplanes and half-spaces are convex sets. 
Lemma 6.3 (convexity of hyperplanes and half-spaces). Let a E Rn\{O} and b ER. 
Then the following sets are convex: 
(a) the hyperplane H = {x E Rn: a7 x = b }, 
(b) the half-space H- = {x E Rn: a7 x < b }, 
(c) the open halfspace {x E Rn: aT x < b }. 
Proof. We will prove only the convexity of the half-space since the proof of convexity of 
the other two sets is almost identical. Let x,y EH- and let A E (0, 1]. We will show that 
z = Ax+ ( 1-A)y EH-. Indeed, 
aT z=aT[...lx+(l-A)y]= A(aT x)+(l-A)(aTy)< Ab+(l-A)b = b, 
where the inequality in the above chain of equalities and inequalities follows from the fact 
that a T x < b, a Ty < b, and A E [ 0, 1 ]. 
D 
Other important examples of convex sets are the closed and open balls. 
Lemma 6.4 (convexity of balls). Let c E Rn and r > 0. Let II· II be an arbitrary norm 
defined on Rn. Then the open ball 
B(c, r) = {x E Rn: llx-cll < r} 
and closed ball 
B[c,r]={xERn :llx-cll< r} 
are convex. 
Proof. We will show the convexity of the closed ball. The proof of the convexity of the 
open ball is almost identical. Let x, y EB[ c, r] and let A E (0, 1 ]. Then 
llx-cll < r,lly-cll < r. 
Let z =Ax+ ( 1- A)y. We will show that z EB[ c, r ]. Indeed, 
llz-cll = 11...lx+(l-A)y-cll 
= llA(x-c)+(l-A)(y-c)ll 
< llA(x-c)ll + 11(1-A)(y-c)ll 
= Allx-cll+(l-A)lly-cll 
<Ar +(1-A)r = r 
Hence, z EB[ c, r ], establishing the result. 
D 
(triangle inequality) 
(O<A<l) 
(equation (6.1)). 
(6.1) 

6.1. Definition and Examples 
99 
2 
1.5 
.. 
0.5 
0 
- 0.5 
- 1 
- 1.5 
- 2 
- 2 
Note that the above result is true for any norm defined on Rn. The unit-ball is the 
ball B[O, 1]. There are different unit-balls, depending on the norm that is being used. 
The /1, 12, and /00 balls are illustrated in Figure 6.2. As always, unless otherwise specified, 
we assume in this book that the underlying norm is the /2-norm and that the balls are 
with respect to the /2-norm. 
loo 
2
,----,.~--.---,.----.-~..--. 
2 
1.5 
1.5 
0.5 D 
0 . 
- 0.5 
- 1 
- 1.5 
_:~ ; 0 
- 1 
..... ;. 
.. ;. .. . 
. ... '··· 
... ; ... . 
:(>
' .. ·:··· 
·r.-:· 
.. 
.. 
··>· 
. 
. 
·? 
. 
"+· 
. 
. 
f 
·· · ·· ···r···· 
. ..... 
- 1.5 
........ 
..... 
. . . . 
- 2 
- 2 
- 1 
0 
2 
- 2 
- 1 
0 
2 
- 2 
- 1 
0 
2 
Figure 6.2. 11, 12, and 100 balls in R.2• 
Another important example of convex sets are ellipsoids. 
Example 6.5 (convexity of ellipsoids). An ellipsoid is a set of the form 
E={xERn :xTQx+2bTx+c<O}, 
where Q E Rn xn is positive semidefinite, b E Rn, and c E R. Denoting 
f(x)=xTQx+2bT x+c, 
the set E can be rewritten as 
E = {xE Rn :f(x) < 0}. 
To prove the convexity of E, we take x,y EE and). E (0, 1]. Then f(x) < O,f(y) < 0, 
and thus the vector z = Ax+ ( 1-).)y satisfies 
zT Qz = ().x+(l-A)y)T Q().x+(l-A)y) 
= ).2xT Qx + ( 1-).)2yT Qy + 2).( 1-).)xT Qy. 
(6.2) 
Now, note that xT Qy = (Q112xf (Q112y), and hence by the Cauchy-Schwarz inequality, 
it follows that 
where the last inequality follows from the fact that JtiC < !<a+ c) for any two nonnega-
tive scalars a, c. Plugging inequality (6.3) into (6.2), we obtain that 

100 
and hence 
Chapter 6. Convex Sets 
/(z) = zrQz+2br z+c 
< AxrQx+(1-A)yrQy+2).bT x+2(1-A)br y+c 
= ).(xTQx+2br x+c)+(1-A)(yrQy+2br y+c) 
= )./(x) + (1-).)/ (y) < o, 
establishing the desired result that z E E. 
I 
6.2 • Algebraic Operations with Convex Sets 
An important property of convexity is that it is preserved under the intersection of sets. 
Lemma 6.6. Let Ci C 1Rn be a convex set for any i E I, where I is an index set (possibly 
infinite). Then the set ni el Ci is convex. 
Proof. Suppose that x,y E nieI Ci and let ). E [O, 1]. Then x,y E Ci for any i E /, 
and since C; is convex, it follows that Ax + ( 1 - ).)y E Ci for any i E I. Therefore, 
).x+(l-A)y e nie1 Ci. 
0 
Example 6.7 (convex polytopes). A direct consequence of the above result is that a set 
defined by a set of linear inequalities, specifically, 
P={xelR.n :Ax<b}, 
where A E 1R m xn and b E 1R. m is convex. The convexity of P follows from the fact that it 
is an intersection of half-spaces: 
m 
P = n{x E Rn: Aix < b;}, 
i=l 
where A; is the ith row of A. Since half-spaces are convex (Lemma 6.3), the convexity of 
P follows. Sets of the form P are called convex polytopes. 
I 
Convexity is also preserved under addition, Cartesian product, linear mappings, and 
inverse linear mappings. This result is now stated, and its simple proof is left as an exercise 
(see Exercise 6.1). 
Theorem 6.8 (preservation of convexity under addition, intersection and linear map-
pings). 
zs convex. 

6.3. The Convex Hull 
(b) Let Ci C Rk, be a convex set for any i = 1, 2, ... , m. Then the Cartesi.an product 
Ci x C2 x ... x cm= {(xi,X2,···,xm): X; E ci,i = 1,2, ... ,m} 
is convex. 
(c) Let MC Rn be a convex set and let A E Rmxn. Then the set 
A(M) ={Ax: x EM} 
is convex. 
(d) Let DC Rm be a convex set, and let A E Rmxn. Then the set 
A-i(D) = {x E JRn: Ax ED} 
is convex. 
101 
A direct result of part (a) of Theorem 6.8 is that if C C Rn is a convex set and b E Rn, 
then the set 
C+b={x+b:xeC} 
is also convex. 
6.3 •The Convex Hull 
Definition 6. 9 (convex combinations). Given k vectors Xp x2, ... , xk E Rn, a convex 
combination of these k vectors is a vector of the form Ai Xi + A.2x2 + · · · + · · · + A.kxk, where 
Ai, A.2, ... , A.k are nonnegative numbers satisfying Ai+ A.2 + · · · + A.k = 1. 
A convex set is defined by the property that any convex combination of two points 
from the set is also in the set. We will now show that a convex combination of any number 
of points from a convex set is in the set. 
Theorem6.10. LetC CRn beaconvexsetandletxi,X2, ... ,xm E c. ThenforanyA.et::.m, 
the relation I:7::i Ai xi E C holds. 
Proof. We will prove the result by induction on m. For m = 1 the result is obvious (it 
essentially says that x1 E C implies that Xi E C ... ). The induction hypothesis is that for 
any m vectors Xpx2, ... ,xm EC and any AE Am, the vector I:7::i A.ixi belongs to C. We 
will now prove the theorem for m + 1 vectors. Suppose that x1, x2, ••• , xm+ 1 E C and that 
A E Am+i· We will show that z = I:7::"ii A.ixi EC. If Am+l = 1, then z = Xm+l EC and 
the result obviously follows. If Am+t < 1, then 
m 
= 2:A.ixi +Am+iXm+i 
i=1 
m 
X 
= (1-Am+i) 2: 
A xi +A.m+1Xm+1· 
i=1 1- m+t 
v 

102 
Chapter 6. Convex Sets 
S · 
~m 
..l.; 
-
l-..l.m+t -
1, it follows that v (as defined in the above equation) 
mce ""-'i=i° 1-Am+t -
1-Am+t -
is a convex combination of m points from C, and hence by the induction hypotheses 
we have that v E C. Thus, by the definition of a convex set, z = (1- Am+t)v + 
Am+tXm+t EC. 
0 
Definition 6.11 (convex hulls). Let S C Rn. Then the convex hull of S, denoted by 
conv( S), is the set comprising all the convex combinations of vectors from S: 
Note that in the definition of the conv~x hull, the number of vectors kin the convex 
combination representation can be any positive integer. The convex hull conv(S) is the 
"smallest" convex set containing S meaning that if another convex set T contains S, then 
conv( S) C T. This property is stated and proved in the following lemma. 
Lemma 6.12. Let S C Rn. If S C T for some convex set T, then conv( S) C T. 
Proof. Suppose that indeed SC T for some convex set T. To prove that conv(S) CT, 
take z E conv( S). Then by the definition of the convex hull, there exist x1, x2, ••• , xk E 
S C T (where k is a positive integer) and A. E Ak such that z = 2::=1 A.i xi. By Theorem 
6.10 and the convexity of T, it follows that any convex combination of elements from T 
is in T, and therefore, since x1, x2, ••• , xk E T, it follows that z E T, showing the desired 
result. 
0 
An example of a convex hull of a nonconvex polytope is given in Figure 6.3. 
c 
conv(C) 
-
Figure 6.3. A rwnconvex set and its convex hull. 
The following well-known result, called the Caratheodory theorem, states that any 
element in the convex hull of a subset of a given set S C Rn can be represented as a convex 
combination of no more than n + 1 vectors from S. 
Theorem 6.13 (Caratheodory theorem). Let SC IRn and let x E conv(S). Then there 
exist x1,x2, ••• ,xn+t ES such that x E conv( {x1,x2, ••• ,xn+t }); that is, there exist A. E .6.n+t 
such that 

6.3. The Convex Hull 
103 
Proof. Let x E conv( S). By the definition of the convex hull, there exist x1, x2, ••• , xk E S 
and ). E l::.k such that 
k 
x = 2: A.ixi. 
i=l 
We can assume that A; > 0 for all i = 1, 2, ... , k, since otherwise the vectors corresponding 
to the zero coefficients can be omitted. If k < n + 1, the result is proven. Otherwise, if 
k > n + 2, then the vectors x2 - x1, x3 - x1, ••• , xk - x1, being more than n vectors in Rn, 
are necessarily linearly dependent, which means that there exist µ2, µ3, ... , µk which are 
not all zeros such that 
k 
2=µ;(X; -x1) = 0. 
i=2 
Defining µ 1 =-L7=2 µ;,we obtain that 
k 
~µ·X·=O 
~ ' ' 
' 
i=l 
where not all of the coefficients µ1' µ 2, ••• , µk are zeros and in addition they satisfy 
L~=l µi = 0. In particular, there exists an index i for which µi < 0. Let a ER+. Then 
k 
k 
k 
k 
x= 2:.A.;X; = 2:.A.;X; +a 2=µ;X; = 2:<.A.i +aµ;)X;. 
(6.4) 
i=l 
i=l 
i=l 
i=l 
We have L7=1(A.; +aµ;)= 1, so the representation (6.4) is a convex combination repre-
sentation if and only if 
A;+ aµi > 0 for all i = 1, ... ,k. 
(6.5) 
Since A;> 0 for all i, it follows that the set of inequalities (6.5) is satisfied for all a E (O,e-] 
where e- = mini:µ,< 0{-1, }. The scalar e- is well-defined since, as was already mentioned, 
there exists an index for which µ; < 0. If we substitute a = e-, then (6.5) still holds, but 
A.i +e-µi = 0 for j E argmini:µ;< 0{-'t }. This means that we have found a representation 
of x as a convex combination of k - 1 vectors. This process can be carried on until a 
representation of x as a convex combination of no more than n + 1 vectors is derived. 
0 
Example 6.14. For n = 2, consider the following four vectors: 
1 
1 
1 
1 
(13) 
X = -X1 + - X2 + - X3 + - X4 = 181 
• 
8 
4 
2 
8 
T 
By the Caratheodory theorem, x can be expressed as a convex combination of three of the 
four vectors x1,x2,x3,x4• To find such a convex combination, let us employ the process 
described in the proof of the theorem. The vectors 
x, -x, = ( ~)' 
X3 - x, = G)' 
X4 - x, = G) 

104 
Chapter 6. Convex Sets 
are linearly dependent, and the linear dependence is given by the equation 
We thus have the linear dependence relation 
Therefore, we can write the following for any a > 0: 
x= (~-a )x1 +(~+a )x2+G+a )x,+(~-a )x.· 
The weights in the above representation add up to one, so we need only guarantee that 
they are nonnegative, meaning that 
1 --a>O 
8 
-
' 
1 -+a>O, 
4 
-
1 -+a>O, 
2 
-
1 --a>O 
8 
-
' 
which combined with a> 0 yields that 0 <a< l· Substituting a= l, we obtain the 
convex combination 
3 
5 
x= -x2+-x3• 
8 
8 
Note that in this example two coefficients were turned into zero, so we obtained a repre-
sentation with only two vectors, while the Caratheodory theorem can guarantee only a 
presentation by at most three vectors. 
I 
6.4 • Convex Cones 
A set S is called a cone if it satisfies the following property: for any x E S and A > 0, the 
inclusion AX E S is satisfied. The following lemma shows that there is a very simple and 
elegant characterization of convex cones. 
Lemma 6.15. A set S is a convex cone if and only if the following properties hold: 
A. x,ye S =>x+yeS. 
B. x es, A> o =>Axes. 
Proof. (convex cone => A,B). Suppose that S is a convex cone. Then property B follows 
from the definition of a cone. To prove property A, assume that x, y e S. Then by the 
convexity of S we have that !<x + y) e S, and hence, since Sis a cone, it follows that 
x+y = 2 · j(x+y) e S. 
(A,B => convex cone). Now assume that S satisfies properties A and B. Then S is a 
cone by property B. To prove the convexity, let x,y e Sand A e [O, 1]. Then Ax,(1-
A)y e S by property B, and thus by property A, Ax+ (1- A)y e S, establishing the 
convexity of S. 
D 
The following are some well-known examples of convex cones. 

6.4. Convex Cones 
105 
Example 6.16. Consider the convex polytope 
C={xERn :Ax<O}, 
where A E Rmxn. The set C is clearly a convex cone since it is a convex polytope (see 
Example 6.7). It is also a cone since 
x E C, A> 0 =>Ax< 0, A> 0 =>A( A.x) < 0 => Ax E C. 
Taking, for example, m = n and A= -I, the set C reduces to the nonnegative orthant 
R~. 
I 
Example 6.17 (Lorenz cone). The Lorenz cone, or ice cream cone whose boundary is de-
scribed in Figure 6.4, is given by 
The Lorenz cone is in fact a convex cone. To show this, let us take(~),(~) E Ln. Then 
llxll < t, llYll < s, which combined with the triangle inequality implies that 
llx+yll < llxll+llYll < t +s, 
showing that ( ~) + ( ~) E L n, and hence that property A holds. To show property B, 
take(~) E Ln and A> 0, then since llxll < t, it readily follows that ll..1xll < A.t, so that 
..1(neLn. 
I 
2 
1.5 
1 
0.5 
0 
2 
. . ·: 
.·· . 
.. :· ·· 
.... ·:··· 
.. 
• 
_ ..... 
.·· . 
. ·· . 
.. ·:· 
.·· . 
. · 
·. 
. ·: . 
. · 
: · .. 
... .. : 
. . ·· 
. . 
:·· . 
·· .. : . . ·. 
. . 
. . . . . ~ ...... 
~. 
. . 
: ·.. 
: ·· .. 
: 
\~~..,v- · · 
: ... <. .... ~. . 
··: 
~ ·:·:..:......... 
. . . 
: . . . : 
..... . 
·. . 
· . . 
-.... . 
·.· 
.. 
·:-· . . 
. .......... ,,'·· ..... :· . 
. .: 
.. ·""·......... 
: 
·.·· 
· .. ·· 
- 2 - 2 
Figure 6.4. The boundary of the ice cream cone L 2• 
2 
Example 6.18 (nonnegative polynomials). Consider the set of all coefficients of poly-
nomials of degree of at most n - 1 which are nonnegative over R: 
Kn = { x E Rn : x1 t n-l + x2 t n-i + · · · + Xn-l t + xn > 0 for all t E R}. 
It is easy to verify that this is a convex cone. Let us consider two special cases. When 
n = 2, then clearly 
K 2 = {(x1,x2)7 : x1t +x2 > O} = {(xpx2): x1 = O,x2 > O}, 

106 
Chapter 6. Convex Sets 
so K2 is the nonnegative part of the x2-axis. For n = 3 we have 
A quadratic polynomial rp( t) =a t2 + b t + c is nonnegative over JR if and only if a, c > 0 
and the discriminant A = b2 -4ac is nonpositive. We thus conclude that 
K3 = {(xpx2,x3)T: x1'x3 > O,xi < 4x1x3}. I 
Similarly to the notion of a convex combination, we will now define the concept of a 
conic combination. 
Definition 6.19 (conic combination). Given k points xpx2, ••• ,xk E Rn, a conic combi-
nation of these k points is a vector of the form ...l1x1 + ...l2x2 +· ··+· · ·+...lkxk, where AE JR!. 
It is easy to show that any conic combination of points in a convex cone C belong 
to C. The proof of this elementary result is left as an exercise (Exercise 6.14). 
Lemma 6.20. Let C bea convex cone, and let x1,x2, ... ,xk EC and ...l1'...l2, ••• ,...lk > 0. 
Then I:7=t ...lixi EC. 
The definition of the conic hull is now quite natural. 
Definition 6.21 (conic hulls). Let SC Rn. Then the conic hull of S, denoted by cone(S), 
is the set comprising all the conic combinations of vectors from S: 
Similar to the convex hull, the conic hull of a set S is the smallest convex cone con-
taining S. The proof of this result is left as an exercise (Exercise 6.15). 
Lemma 6.22. Let S C Rn. If S C T for some convex cone T, then cone( S) C T. 
A natural question that arises is whether we can establish a result similar to the 
Caratheodory theorem on the representation of vectors in the conic hull of a set. Interest-
ingly, we can establish an even stronger result for conic hulls: each vector in the conic hull 
of a set SC Rn can be represented as a convex combination of at most n vectors from S. 
(Recall that in Caratheodory n + 1 vectors are required.) 
Theorem 6.23 (conic representation theorem). Let SC Rn and let x E cone(S). Then 
there exist k linearly independent vectors x1, x2, ••• , xk E S such that x E cone ( { x1, x2, ••• , xk } ); 
that is, there exists A E JR! such that 
In addition, k < n. 
k 
x=~...lixi. 
i=l 

6.4. Convex Cones 
107 
Proof. The proof is similar to the proof of the Caratheodory theorem. Let x E cone( S). 
By the definition of the convex hull, there exist x1, x2, ••• , xk E S and ). E JR~ such that 
k 
x = ~::>li xi. 
i=l 
We can assume that A; > 0 for all i = 1, 2, ... , k, since otherwise the corresponding 
vectors can just be omitted. If the vectors x1, x2, ••• , xk are linearly independent, then 
the result is proven. Otherwise, if the vectors are linearly dependent, then there exist 
µ1' µ 2, ••• , µk E JR which are not all zeros such that 
Then for any a E JR 
k 
2:µixi = 0. 
i=1 
k 
k 
k 
k 
x= ~Xx-=~ Xx. +a ~µ-x· = ~(,.l. +aµ·)X· 
~ 
' 
i 
~ 
' 
i 
~ 
i 
' 
~ 
' 
' 
,. 
i=l 
i=l 
i=1 
i=l 
The representation (6.6) is a conic representation if and only if 
A; + aµ i > 0 for all i = 1, ... , k. 
(6.6) 
(6.7) 
Since A; > 0 for all i, it follows that the set of inequalities (6.5) is satisfied for all a E I 
where I is a closed interval with a nonempty interior. Note that one (but not both) of 
the endpoints of I might be infinite. If we substitute one of the finite endpoints of I, 
call it a, into a, then we still get that (6.7) holds, but in addition Ai+ aµi = 0 for some 
index j. Thus we obtain a representation of x as a conic combination of at most k - 1 
vectors. This process can be carried on until we obtain a representation of x as a conic 
combination of linearly independent vectors. Since the vectors x1, x2, ••• , xk are linearly 
independent vectors in ]Rn it follows that k < n. 
D 
The latter representation theorem has an important application to convex polytopes 
of the form 
P = {xElin :Ax= b,x> O}, 
where A E lRmxn and b E ]Rm. We will assume without loss of generality that the rows of 
A are linearly independent. Linear systems consisting of linear equalities and nonnega-
tivity constraints often appear as constraints in standard formulations of linear program-
ming problems. An important property of nonempty convex polytopes of the form P 
is that they contain at least one vector with at most m nonzero elements (m being the 
number of constraints). An important notion in this respect is the one of a basic feasible 
solution. 
Definition 6.24 (basic feasible solutions). Let P = {x E lRn : Ax= b,x > 0}, where 
A E lRmxn and b E Rm. Suppose that the rows of A are linearly independent. 1ben i is a 
basic feasible solution (abbreviated bfs) of P if the columns of A corresponding to the indices 
of the positive values of i are linearly independent. 
Obviously, since the columns of A reside in JR m, it follows that a bfs has at most m 
nonzero elements. 

108 
Example 6.25. Consider the linear system 
X1 +xz+X3 =6, 
x2 +x3 = 3, 
x1, x2, x3 > 0. 
Chapter 6. Convex Sets 
An example of a bfs of the system is (x1,x2,x3) = (3,3,0). This vector is indeed a bfs since 
it satisfies all the constraints and the columns corresponding to the positive elements, 
meaning columns 1 and 2 
are linearly independent. 
I 
The existence of a bfs in P, provided that it is nonempty, follows directly from Theo-
rem 6.23. 
Theorem 6.26. Let P = {x E Rn : Ax= b,x > O}, where A E Rmxn andb E Rm. If Pf:. 0, 
then it contains at least one bfs. 
Proof. Since P f:. 0, it follows that b E cone( { a1, a2, ... , an}), where ai denotes the i th 
column of A. By the conic representation theorem (Theorem 6.23), we have that b can be 
represented as a conic combination of k linearly independent vectors from {a1,az, ... ,an}; 
that is, there exist indices i1 < i2 < ... < ik and k numbers yi1,yi2 , ... ,yik > 0 such that 
b = L:~ _1 Yi ai and ai , ai , ... , ai are linearly independent. Denote i = L:~ _1 Yi ei . Then 
J-
J 
J 
1 
2 
k 
J-
J 
J 
obviously i > 0 and in addition 
k 
k 
Ai= ~Yi Aei =~Yi ai =h. 
£....i, 
J 
£....i,, 
j=1 
j=l 
Therefore, i is contained in P and satisfies that the columns of A corresponding to the 
indices of the positive components of i are linearly independent, meaning that P contains 
a bfs. 
0 
6.5 • Topological Properties of Convex Sets 
We begin by proving that the closure of a convex set is a convex set. 
Theorem 6.27 (convexity preservation under closure). Let C C Rn be a convex set. 
Then cl( C) is a convex set. 
Proof. Let x, y E cl( C) and let ). E [O, 1 ]. Then by the definition of the closure set, it 
follows that there exist sequences {xk}k>o C C and {yk}k>O CC for which xk -+ x and 
Yk -+ y ask-+ oo. By the convexity of C, it follows that ..lxk + (1-A)Yk EC for any 
k > 0. Since ..lxk + (1- ..l)yk-+ ..lx + (1- ..l)y, it follows that there exists a sequence in C 
that converges to ..lx+(l-..l)y, implying that ..lx+ (1-..l)y E cl(C). 
0 
Proving that the interior of a convex set is also a convex set is more tricky, and we will 
require the following technical and quite useful result called the line segment principle. 

6.5. Topological Properties of Convex Sets 
109 
Lemma 6.28 (line segment principle). Let C bea convex set, and assume that int(C) f:. 0. 
Suppose that x E int( C) and ye cl( C). 1hen ( 1-A.)x +A.ye int( C) for any A. e (0, 1 ). 
Proof. Since x E int(C), there exists c > 0 such that B(x,c) CC. Let z = (1-A.)x+ A.y. 
To prove that z E int(C), we will show that in fact B(z,(1-A.)c) CC. Let then w be a 
vector satisfying I lw - zl I < ( 1- A)c. Since y E cl( C), it follows that there exists w 1 E C 
such that 
(1-A.)c -llw-zll 
llw1 -yll < 
A. 
• 
(6.8) 
Set w2 = t~A(w-A.w1 ). Then 
W-AW1 
llw2-xll = 
-x 
1-A. 
1 
l-A ll(w-z)+ A.(y-w1)ll 
1 
< 1_;,<llw-zll+A.llwi-YID 
(6.8) 
< c, 
and hence, since B( x, c) C C, it follows that w 2 E C. Finally, since w = A.w 1 + ( 1- A.)w 2 
with w 1, w 2 E C, we have that w E C, and the line segment principle is thus proved. 
0 
The immediate consequence of the line segment principle is the convexity of interiors 
of convex sets. 
Theorem 6.29 (convexity of interiors of convex sets). Let C C Rn be a convex set. 1hen 
int(C) is convex. 
Proof. If int( C) = 0, then the theorem is obviously true. Otherwise, let x1, x2 E int( C), 
and let A E (0, 1). Then by the line segment principle we have that A.x1 +(1-A.)x2 E int(C), 
establishing the convexity of int(C). 
0 
Other topological properties that are implied by the line segment property are given 
in the next result. 
Lemma 6.30. Let C be a convex set with a nonempty interior. 1hen 
(a) cl(int(C)) = cl(C), 
(b) int(cl(C)) = int(C). 
Proof. (a) Obviously, since int(C) CC, the inclusion cl(int(C)) C cl(C) holds. To prove 
the opposite, let x E cl( C) and let y E int( C). Then by the line segment principle, xk = 
iY+( 1-i )x E int( C) for any k > 1. Since xis the limit of the sequence { xk} k~t C int( C), 
it follows that x E cl( int( C) ). 
(b) The inclusion int(C) C int(cl(C)) follows immediately from the inclusion C C 
cl(C). To show the reverse inclusion, we take x e int(cl(C)) and show that x E int(C). 
Since x E int( cl( C) ), there exists c > 0 such that B( x, c) C cl( C). Let y E int( C). If y = x, 

110 
Chapter 6. Convex Sets 
then the result is proved. Otherwise, define 
z = x+a(x-y), 
where a = 211x~yll" Since I lz-xi I = j, it follows that z E cl( C). Therefore, ( 1-.A)y + .Az E 
int(C) for any .A E [O, 1) and specifically for A= .A.a= l~a. Noting that (1-.Aa)y+.Aaz = x, 
we conclude that x E int(C). 
D 
In general, the convex hull of a closed set is not necessarily a closed set. A classical 
example for this fact is the the closed set 
S = {(O,O)T} U {(x,y)T: xy > 1,x > O,y > O}, 
whose convex hull is given by the set 
conv(S) = {(O,O)T} UlR~+' 
which is neither closed nor open. However, closedness is preserved under the convex 
hull operation if the set is compact. As will be shown in the proof of the following result, 
this nontrivial fact follows from the Caratheodory theorem. 
Proposition 6.31 (closedness of convex hulls of compact set). Let SC lRn be a compact 
set. Then conv( S) is compact. 
Proof. To prove the boundedness of conv(S), note that since S is compact, there ex-
ists M > 0 such that llxll < M for any x E S. Now, let y E conv(S). Then by the 
Caratheodory theorem it follows that there exist x1,x2, ••• ,xn+l ES and .A E An+t for 
which y = '2:?~1
1 .Ai xi, and therefore 
n+l 
n+t 
n+1 
llYI I = L .Ai xi < L Ai I lxi II < ML: Ai = M, 
i=l 
i=l 
i=1 
establishing the boundedness of conv( S). To prove the closedness of conv( S), let {Yk} k>t C 
conv( S) be a sequence of vectors from conv( S) converging to y e lRn. Our objective 1s to 
show that ye conv(S). By the Caratheodory theorem we have that for any k > 1 there 
exist vectors~,~, ... , x!+i ES and .A.k E An+t such that 
(6.9) 
By the compactness of Sand An+t' it follows that {(.Ak ,~ ,~, ... ,X:+1)}k?:l has a conver-
gent subsequence {(.Ak, ,x~' ,~', ... ,x~+t)}j?:t whose limit will be denoted by 
(.A,x1,X2, ... ,xn+1) 
with .A E An+1,x1,x2, ••• ,Xn+l ES. Taking the limit j-+ oo in 
n+l k. k· 
Yk. = ~ 
.A/x/, 
I~ 
1=1 

6.6. Extreme Points 
we obtain that 
n+l 
Y = """"" ).. x . 
~ 
i 
,, 
i=l 
meaning that y E conv( S) as required. 
D 
111 
Another topological result which might seem simple but actually requires the conic 
representation theorem is the closedness of the conic hull of a finite set of points. 
Lemma 6.32 (closedness of the conic hull of a finite set). Let a1,a2, ... ,ak E Rn. Then 
cone( { a1, a2, ... , ak}) is closed. 
Proof. By the conic representation theorem, each element of cone( { a1, a2, ... , ak}) can 
be represented as a conic combination of a linearly independent subset of { a1, a2, ... , ak}. 
Therefore, if Sp S2, ... , SN are all the subsets of { a1, a2, ... , ak} comprising linearly inde-
pendent vectors, then 
N 
cone( {a1,a2, ... ,ak}) = LJcone(Si)· 
i=l 
It is enough to show that cone(S;) is closed for any i E {1,2, ... ,N}. Indeed, let i E 
{1,2, ... ,N}. Then 
S; ={hp h2, ... , bm} 
for some linearly independent vectors h1, h2, ••• , bm. We can write cone(S;) as 
cone( Si)= {By: y ER~}, 
where Bis the matrix whose columns are h 1,b2, .. • ,bm. Suppose that xk E cone(S;) for 
all k > 1 and that xk --+ i. We need to show that i E cone(SJ. Since xk E cone(S;), it 
follows that there exists y k E R ~ such that 
(6.10) 
Therefore, using the fact that the columns of B are linearly independent, we can deduce 
that 
Yk = (BTB)-1BT xk. 
Taking the limit as k --+ oo in the last equation, we obtain that Yk --+ y where y = 
(BTB)-1BT i, and since Yk ER~ for all k, we also have y ER~. Thus, taking the limit in 
(6.10), we conclude that i =By with y ER~, and hence i E cone( Si). 
D 
6.6 • Extreme Points 
Definition 6.33 (extreme points). Let S C R_n. A point x ES is called an extreme point 
of S if there do not exist x1,x2 E S(x1 -::j; x2) and A E (0, 1), such that x = ..lx1 + (1-..l)x2• 
That is, an extreme point is a point in the set that cannot be represented as a nontriv-
ial convex combination of two different points in S. The set of extreme points is denoted 
by ext( S). The set of extreme points of a convex polytope consists of all its vertices; see, 
for example, Figure 6.5. 

112 
Chapter 6. Convex Sets 
-2'--~~_.._~~-'-~~~~~~~~~~~~~ 
- 2 
-1 
0 
1 
2 
3 
4 
Figure 6.5. 
The filled area is the convex set S = conv{x1,x2,x3,x4}, where x1 
(2,-1)7,x2 = (1,3}7,x3 = (-1,2)7,x4 =(1,1)7. The extreme points set is ext(S) = {x1,x2,x3}. 
We can fully characterize the extreme points of convex polytopes of the form P = 
{x E Rn : Ax= b,x > O}, where A E Rmxn has linearly independent rows and b E Rm. 
Recall (see Section 6.4) that i is called a basic feasible solution of P if the columns of 
A corresponding to the indices of the positive values of i are linearly independent. In 
Section 6.4 it was shown that if P is not empty, then it has at least one basic feasible 
solution. Interestingly, the extreme points of P are exactly the basic feasible solutions of 
P, which means that the linear independence of the columns of A corresponding to the 
positive variables is an algebraic characterization of extreme points. 
Theorem 6.34 (equivalence between extreme points and basic feasible solutions). Let 
P = {x E Rn : Ax= b,x > 0}, where A E Rmxn has linearly independent rows and b E Rm. 
Then i is a basic feasible solution of P if and only if it is an extreme point of P. 
Proof. Suppose that i is a basic feasible solution and assume without loss of generality 
that its first k components are positive while the others are zero: x1 > 0, Xi > 0, ... , xk > 
0, xk+t = xk+2 = · · · = xn = 0. Since i is a basic feasible solution, the first k columns 
of A denoted by a1,a2, ••• ,ak are linearly independent. Suppose in contradiction that 
i <t ext(P). Then there exist two different vectors y,z E P and ). E (0, 1) such that i = 
Ay + ( 1- A)z. Combining this with the fact that y, z > 0, we can conclude that the last 
n -k variables in y and z are zeros. We can therefore write 
k 
L:Y;a; = b, 
i=l 
k 
L:z;a; = b. 
i=l 
Subtracting the second inequality from the first, we obtain 
k 
L:<Yi - z; )a; = 0, 
i=l 
and since y =I z, we obtain that the vectors a1, a2, ••• , ak are linearly dependent, which is a 
contradiction to the assumption that they are linearly independent. To prove the reverse 

Exercises 
113 
direction, let us suppose that i E P is an extreme point and assume in contradiction that i 
is not a basic feasible solution. This means that the columns corresponding to the positive 
components are linearly dependent. Without loss of generality, let us assume that the 
positive variables are exactly the first k components. The linear dependance of the first k 
columns means that there exists a nonzero vector y E Rk for which 
k 
LYiai =0. 
i=l 
The latter identity can also be written as Ay = 0, where y = ( o?'-1e ). Since the first k 
components of i are positive, it follows that there exist an.: > 0 for which x1 = i+.:y > 0 
and x2 = i-.:y > 0. In addition Ax1 =Ai+.: Ay = b +.: · 0 = b, and similarly Ax2 = b. 
We thus conclude that x1,x2 E P; these two vectors are different since y is not the zeros 
vector, and finally, i = ~ x1 + ~ x2, contradicting the assumption that i is an extreme 
point. 
0 
We finish this section with a very important and well-known theorem called the Krein-
Milman theorem, stating that a compact convex set is the convex hull of its extreme points. 
We will state this theorem without a proof. 
Theorem 6.35 (Krein-Milman). Let S C Rn be a compact convex set. Then 
S = conv( ext( S) ). 
Exercises 
6.1. Prove Theorem 6.8. 
6.2. Give an example of two convex sets C1, C2 whose union C1 U C2 is not convex. 
6.3. Show that the following set is not convex: 
S = { x E R2 : x: - xi + x1 + x2 < 4}. 
6.4. Prove that 
conv{e1,e2,-e1,-e2} = {x E R2 : lx11+lx21<1}, 
where e1 = (1,0f ,e2 =(0,1)7. 
6.5. Let K be a convex, bounded, and symmetric2 set such that 0 E intK. Define the 
Minkowski functional as 
p(x) =inf {A> 0 : ~ EK}, x E Rn. 
Show that p(·) is a norm. 
6.6. Show the following properties of the convex hull: 
(i) If S C T, then conv( S) C conv( T). 
(ii) For any SC Rn, the identity conv(conv(S)) = conv(S) holds. 
2 A set S is called symmetric if x E S implies -x E S. 

114 
Chapter 6. Convex Sets 
(iii) For any Sp S2 C lRn the following holds: 
conv(S1 + S2) = conv(S1) + conv(S2). 
6.7. Let C be a convex set. Prove that cone( C) is a convex set. 
6.8. Show that the conic hull of the set 
is the set 
{(x1,x2): x1 > O} U {(0,0)}. 
Remark: This is an example illustrating the fact that the conic hull of a closed set 
is not necessarily a closed set. 
6.9. Let a, b E lRn(a ;zf b ). For what values ofµ is the set 
sµ = {xelRn: llx-all < µllx-bll} 
convex? 
6.10. Let C C lRn be a nonempty convex set. For each x E C define the normal cone 
of Cat x by 
Nc(x) = {w E lRn: (w,y-x) < 0 for all yE C}, 
and define Nc(x) = 0 when x ~ C. Show that Nc(x) is a closed convex cone. 
6.11. Let CC lRn be cone. The dual cone is defined by 
C* = {yE lRn: (y,x) > 0 for all xE C}. 
(i) Prove that C* is a closed convex cone (even if C is nonconvex). 
(ii) Prove that if C1, C2 are cones satisfying C1 c C2, then c; cc;. 
(iii) Show that (Ln)* = Ln. 
(iv) Show that the dual cone of K = {(~): llxll1 < t} is 
6.12. A cone K is called pointed if it contains no lines, meaning that x, -x E K => x = 0. 
Show that if K has a nonempty interior, then K* is pointed. 
6.13. Consider the optimization problem 
(Pa) 
min{aTx:xES}, 
where S C lRn. Let x* E S and let K C lRn be the set of all vectors a for which x* is 
an optimal solution of (Pa)· Show that K is a convex cone. 
6.14. Prove Lemma 6.20. 
6.15. Prove Lemma 6.22. 
6.16. Find all the basic feasible solutions of the system 
-4x2 +x3 = 6, 
2x1-2x2-x4 =1, 
x1, X2, x3, x4 > 0. 

Exercises 
115 
6.17. Let S be a convex set. Prove that x E S is an extreme point of S if and only if S\ { x} 
is convex. 
6.18. Let S = {x E Rn : llxll00 < 1 }. Show that 
ext(S) = {x E Rn : xf = 1, i = 1,2, ... , n }. 
6.19. Let S = {x E Rn : llxlb < 1 }. Show that 
ext(S) = {x E Rn : llxlb = 1 }. 
6.20. Let Xi cRn1,i = 1,2, ... ,k. Prove that 

Chapter 7 
Convex Functions 
7 .1 • Definition and Examples 
In the last chapter we introduced the notion of a convex set. This chapter is devoted to 
the concept of convex functions, which is fundamental in the theory of optimization. 
Definition 7 .1 (convex functions). A function f : C --+ IR defined on a convex set C C Rn 
is called convex {or convex over C) if 
f(Ax+(l-A)y) < Af(x)+ (1-A)f(y)forany x,y EC,..{ E [O, 1]. 
(7.1) 
The fundamental inequality (7.1) is illustrated in Figure 7.1. 
'A f(x)+(1-'A)f(y) 
f('A x+(1 - 'A)y) 
I 
------ ~- ---
___ ·:.:·_:: '-1··. 
I 
I 
. 
I 
I 
- -----
-, - ---- -- ---I-----~ 
I 
I 
x 
A. x+(1 - A.)y 
y 
Figure 7.1. Illustration of the inequality f (.A.x+ (1-.J.)y) < )./(x) + (1 - .J.)f (y). 
In case when no domain is specified, then we naturally assume that f is defined over 
the entire space Rn. If we do not allow equality in (7.1) when x f:. y and..{ E (0, 1), the 
function is called strictly convex. 
Definition 7 .2 (strictly convex functions). A function f : C --+ IR defined on a convex set 
CC Rn is called strictly convex if 
f (Ax+ ( 1-A)y) < Af (x) + (1- A)f (y) for any x f:. y EC,). E (0, 1). 
117 

118 
Chapter 7. Convex Functions 
Another important concept is concavity. A function is called concave if -f is convex. 
Similarly, f is called strictly concave if -f is strictly convex. We can of course write a 
more direct definition of concavity based on the definition of convexity. A function f is 
concave if and only if for any x, y E C and ...l E (0, 1] we have 
f(h+(l-...l)y) > ...lf(x)+(t-...l)f(y). 
Equipped only with the definition of convexity, we can give some elementary examples 
of convex functions. We begin by showing the convexity of affine functions, which are 
functions of the form f (x) = aT x + b, where a E ]Rn and b E JR. (If b = 0, then f is also 
called linear.) 
Example 7.3 (convexity of affine functions). Let f(x) = aT x + b, where a E ]Rn and 
b E JR. To show that f is convex, take x, y E ]Rn and ...l E (0, 1 ]. Then 
f(...lx+(t-...l)y) = aT(...lx+(l-...l)y)+ b 
= ...l(aT x)+(l-...l)(aT y)+...lb +(1-...l)b 
= ...l(aT x+ b)+(l-...l)(a7 y+b) 
= ...lf(x)+ (1-...l)/(y), 
and thus in particular f(...lx+(l-...l)y) < ...lf(x)+(l-...l)f(y), and convexity follows. Of 
course, if f is an affine function, then so is -f, which implies that affine functions (and 
in fact, as shown in Exercise 7.3, only affine functions) are both convex and concave. 
I 
Example 7.4 (convexity of norms). Let II· II be a norm on Rn. We will show that the 
norm function f(x) = llxll is convex. Indeed, let x,y E lRn and ...l E (0, 1]. Then by the 
triangle inequality we have 
f(...lx+(l-...l)y) = 11...lx+(l-...l)yll 
establishing the convexity off. 
I 
< 11...lxll + 11(1-...l)yll 
= ...lllxll + ( 1- ...l)l IYll 
= ...lf(x)+(l-...l)f(y), 
The basic property characterizing a convex function is that the function value of a 
convex combination of two points x and y is smaller than or equal to the correspond-
ing convex combination of the function values f(x) and f(y). An interesting result is 
that convexity implies that this property can be generalized to convex combinations of 
any number of vectors. This is the so-called Jensen's inequality. 
Theorem 7.5 Oensen's inequality). Let f : C -+ JR be a convex function where C C Rn 
is a convex set. Then for any x1, x2, ••• , xk E C and A E ~k' the following inequality holds: 
(7.2) 
Proof. We will prove the inequality (7 .2) by induction on k. Fork = 1 the result is obvious 
(it amounts to f(x1) < f(x1) for any x1 EC). The induction hypothesis is that for any k 

7.2. First Order Characterizations of Convex Functions 
119 
vectors x1,x2, ••• ,xk EC and any A. E l:l.k, the inequality (7.2) holds. We will now prove 
the theorem fork+ 1 vectors. Suppose that x1,x2, ... ,xk+l EC and that A. E l:l.k+l · We 
will show that f(z) < L7~i A.if(xi), where z = L7~i A.ixi. If A.k+l = 1, then z = xk+l 
and (1.2) is obvious. If A.k+l < 1, then 
f(z)=f(~A,x,) 
= f (ii:A;x; +Ak+txk+t) 
(1.3) 
(7.4) 
v 
(1.5) 
Since L~-11_1 = 1
1=:~.1i+i = 1, it follows that vis a convex combination of k points 
I-
k+t 
Ak+t 
from C, and hence by the induction hypothesis we have that f(v) < L~=l 1_t+1f(xi), 
which combined with (1.5) yields 
k+l 
f(z) < L: A.if(xi). 
D 
i=l 
7 .2 • First Order Characterizations of Convex Functions 
Convex functions are not necessarily differentiable, but in case they are, we can replace 
the Jensen's inequality definition with other characterizations which utilize the gradient 
of the function. An important characterizing inequality is the gradient inequality, which 
essentially states that the tangent hyperplanes of convex functions are always underesti-
mates of the function. 
Theorem 7.6 (the gradient inequality). Let f: C-+ IR be a continuously differenti4ble 
function defined on a convex set C C Rn. Then f is convex over C if and only if 
f (x) + \7 f (x)7 (y-x) < f(y)for any x,y EC. 
(1.6) 
Proof. Suppose first that f is convex. Let x,y E C and A. E (0, 1]. If x = y, then (1.6) 
trivially holds. We will therefore assume that x 'f:. y. Then 
f(A.y+(l-A.)x) < A.f(y)+(l-A.)f(x), 
and hence 
f(x+A(y~x))-f(x) <f(y)-f(x). 
Taking A.-+ o+, the left-hand side converges to the directional derivative off at x in the 
direction y-x, so that 
f'(x;y-x) <f(y)-f(x). 

120 
Chapter 7. Convex Functions 
Since f is continuously differentiable, it follows that f'(x;y-x) = Vf(xf (y-x), and 
hence (1.6) follows. 
To prove the reverse direction, assume that that the gradient inequality holds. Let 
z, w EC, and let A E (0, 1). We will show that f(Az+(1-A)w) < Af(z)+ (1-A)f(w). 
Let u= Az+(1-A)wE C. Then 
u-(1-A)w 
1-A 
z-u= 
A 
-u=--A-(w-u). 
Invoking the gradient inequality on the pairs z, u and w, u, we obtain 
f(u) + V f(u)T (z-u) < f(z), 
A 
f(u)- l-A Vf(u)T(z-u) < f(w). 
Multiplying the first inequality by 1~Jt and adding it to the second one, we obtain 
1 
A 
1-Af(u)< 1-Af(z)+f(w), 
which after multiplication by 1-A amounts to the desired inequality: 
f (u) < Af (z) + (1-A)f (w). 
D 
A slight modification of the above proof will show that a function is strictly convex if 
and only if the gradient inequality is satisfied with strict inequality for any x f= y. 
Theorem 7.7 (the gradient inequality for strictly convex function). Let f: C-+ JR 
be a continuously differentiable function defined on a convex set C C Rn. Then f is strictly 
convex over C if and only if 
f(x)+ Vf(x)7 (y-x) < f(y)forany x,yE C satisfying x f=y. 
Geometrically, the gradient inequality essentially states that for convex functions, the 
tangent hyperplane is below the surface of the function. A two-dimensional illustration 
is given in Figure 7 .2. 
A direct result of the gradient inequality is that the first order optimality condition 
V f ( x*) = 0 is sufficient for global optimality. 
Proposition 7.8 (sufficiency of stationarity under convexity). Let f be a continuously 
differentiable function which is convex over a convex set C C Rn. Suppose that V f (x*) = 0 
for some x* E C. Then x* is a global minimizer off over C. 
Proof. Let z EC. Plugging x = x* and y = z in the gradient inequality (1.6), we obtain 
that 
f(z) > f(x*) + V f(x*f (z-x*), 
which by the fact that V f(x*) = 0 implies that f(z) > f(x*), thus establishing that x* is 
the global minimizer off over C. 
D 

7 .2. First Order Characterizations of Convex Functions 
4 
0 
- 2 
- 4 
- 6 
- 8 
- 10 
- 2 
- 1.5 
- 1 
- 0.5 
0 
0.5 
121 
....... ::::::1 
..... •! ............ 
-~ 
............ .i ............. l 
1.5 
2 
Figure 7.2. The function f(x,y) = x2 + y2 and its tangent hyperplane at (1, 1), which is a 
lower bound of the function's surf ace. 
We note that Proposition 7.8 establishes only the sufficiency of the stationarity condi-
tion V f(x*) = 0 for guaranteeing that x* is a global optimal solution. When C is not the 
entire space, this condition is not necessary. However, when C = Rn, then by Theorem 
2.6, this is also a necessary condition, and we can thus write the following statement. 
Theorem 7.9 (necessity and sufficiency of stationarity). Let f: Rn~ IR. be a continu-
ously differentiable convex function. Then V f ( x*) = 0 if and only if x* is a global minimum 
point off over Rn. 
Using the gradient inequality we can now establish the conditions under which a 
quadratic function is convex/ strictly convex. 
Theorem 7.10 (convexity and strict convexity of quadratic functions with positive 
semidefinite matrices). Let f : Rn ~ 
IR. be the quadratic function given by f ( x) = xT Ax+ 
2bT x + c, where A E Rnxn is symmetric, b E Rn, and c ER. Then f is {strictly) convex if 
and only if A >- 0 (A >- 0). 
Proof. By Theorem 7 .6 the convexity of f is equivalent to the validity of the gradient 
inequality: 
f (y) > f (x) + V f (x)T (y-x) for any x, y E Rn, 
which can be written explicitly as 
yT Ay+2bT y+c >xT Ax+2bT x+c+2(Ax+b)T(y-x) for anyx,yERn. 
After some rearrangement of terms, we can rewrite the latter inequality as 
(y-xf A(y-x) > 0 for any x,y E Rn. 
(7.7) 
Making the transformation d = y- x, we conclude that inequality (7.7) is equivalent to 
the inequality dT Ad> 0 for any d E Rn, which is the same as saying that A>- 0. To prove 
the strict convexity variant, note that strict convexity off is the same as 
f(y) > f(x)+ Vf(x)T (y-x) for any x,y ERn such that x tf y. 

122 
Chapter 7. Convex Functions 
The same arguments as above imply that this is equivalent to 
dT Ad> 0 for any 0 -:f. d E IRn, 
which is the same as A >- 0. 
0 
Examples of convex and nonconvex quadratic functions are illustrated in Figure 7.3. 
80 
70 
60 
50 
40 
30 .· 
20 
' 
- 2 
4 6 
0 2 
y 
- 5 _54 
x 
0 v ·· 
- 10 
- 20 
- 30 
- 50 
- 60 
- 70 
- 80 
- x2- y2 
.• 
: 
~ 2 4 
- 2 
- 4 
- 6 
x 
x2- y2 
40 
30 
20 
10 
0 
- 10 
- 20 
- 30 
- 40 
6 
·. 
2 4 6 
> 
0 
- 2 
y 
- 5 
- 4 
x 
- 6 
Figure 7.3. The left quadratic function is convex (f (x,y) = x2 + y2), while the middle 
(-x2 -y2} and right (x 2 -y2} functions are nonconvex. 
Another type of a first order characterization of convexity is the monotonicity prop-
erty of the gradient. In the one-dimensional case, this means that the derivative is nonde-
creasing, but another definition of monotonicity is required in the n-dimensional case. 
Theorem 7.11 (monotonicity of the gradient). Suppose that f is a continuously differen-
tiable function over a convex set C C IRn. Then f is convex over C if and only if 
(V/(x)-\1/(y))T(x-y) > Oforany x,y EC. 
(7.8) 
Proof. Assume first that f is convex over C. Then by the gradient inequality we have for 
anyx,y E C 
/(x) > /(y)+ V/(y)T(x-y), 
f (y) > f (x) + \1 f(x)T (y- x). 
By summing the two inequalities, the inequality (7.8) follows. To prove the opposite 
direction, suppose that (7 .8) holds and let x, y E C. Let g be the one-dimensional function 
defined by 
g(t) = /(x + t(y- x)), 
t E (0, 1]. 

7 .3. Second Order Characterization of Convex Functions 
By the fundamental theorem of calculus we have 
f(y) = g(t) = g(O)+ f g'(t)dt 
= f(x)+ f (y-xf Vf(x+ t{y-x))dt 
= f(x)+ Vf(xf (y-x)+ f (y-xf (Vf(x+ t(y-x))-V f(x))dt 
> f(x) + V f(xf (y-x), 
123 
where the last inequality follows from the fact that for any t > 0 we have by the mono-
tonicity of V f that 
1 
(y-x)T(Vf(x+ t(y-x))-Vf(x)) = -(Vf(x+ t(y-x))-Vf(x))T(x+ t(y-x)-x) > 0 
t 
0 
7.3 •Second Order Characterization of Convex Functions 
When the function is twice continuously differentiable, convexity can be characterized 
by the positive semidefiniteness of the Hessian matrix. 
Theorem 7.12 (second order characterization of convexity). Let f "be a twice continu· 
ously differenti.able function over an open convex set C C Rn. Then f is convex if and only 
if V2f(x) >-Oforanyxe C. 
Proof. Suppose that V2f(x) >- 0 for all x e C. We will prove the gradient inequality, 
which by Theorem 7.6 is enough in order to establish convexity. Let x,y EC. Then by 
the linear approximation theorem (theorem 1.24) we have that there exists z E [ x, y] (and 
hence z E C) for which 
1 
f(y) = f(x)+ Vf(xf (y-x)+ 2(y-x)TV2f(z)(y-x). 
(7.9) 
Since V2f(z) >- 0, it follows that (y-xfV2f(z)(y-x) > 0, and hence by (7.9), the 
inequality f(y) > f(x)+ Vf(xf (y-x) holds. 
To prove the opposite direction, assume that f is convex over C. Let x E C and let 
y E Rn. Since C is open, it follows that x + .Ay E C for 0 < A < e, where t is a small 
enough positive number. Invoking the gradient inequality we have 
f(x+.Ay)> f(x)+.AVf(x)Ty. 
In addition, by the quadratic approximation theorem (Theorem 1.25) we have that 
_A2 
f(x+ .Ay) = f(x)+ .AVf(x)T y+ 2yTV2f(x)y+o(.A2llyl!2), 
which combined with (7.10) yields the inequality 
_A2 
2yTV2f(x)y+o(.A2llYll2) > 0 
(7.10) 

124 
Chapter 7. Convex Functions 
for any A e (0, t ). Dividing the latter inequality by A2 we have 
Finally, taking A -+ o+, we conclude that 
yT'\12 f (x)y > 0 
for any y E Rn, implying that V2f(x) >- 0 for any x EC. 
D 
We also present the corresponding result for strictly convex functions stating that if 
the Hessian is positive definite, then the function is strictly convex. The proof of this 
result is similar to the one given in Theorem 7.12 and is hence left as an exercise (see 
Exercise 7.6). 
Theorem 7.13 (sufficient second order condition for strict convexity). Let f bea twice 
continuously differentiable function over a convex set C C Rn, and suppose that '\12 f ( x) >- 0 
for any x E C. Then f is strictly convex over C. 
Note that the positive definiteness of the Hessian is only a sufficient condition for 
strict convexity and is not necessary. Indeed, the function f ( x) = x4 is strictly convex, 
but its second order derivative f"(x) = 12x2 is equal to zero for x = 0. The Hessian test 
immediately establishes the strict convexity of the one-dimensional functions x2, ex, e-x 
and also of-ln(x),x ln(x) over IR++· A much more complicated example is that of the 
so-called log-sum-exp function, whose convexity can be shown by the Hessian test. 
Example 7 .14 (convexity of the log-sum-exp function). Consider the function 
f(x) = ln(ex1 +ex2 + ... +exn), 
called the log-sum-exp function and defined over the entire space Rn. We will prove its 
convexity using the Hessian test. The partial derivatives off are given by 
and therefore 
af 
ex, 
-a (x)=""n 
xk' 
i=1,2, ... ,n, 
xi 
"'-'k=t e 
a2f (x)= { 
ax.ax. 
I 
J 
( e"i e"i )2, 
i ..J. 1., 
Lk=te"k 
I 
e"• e"i 
e"• 
(~n 
"k )2 + ~n - e"k ' 
i = J • 
"""k=le 
"""k-1 
We can thus write the Hessian matrix as 
'\12 f (x) = diag(w)-ww7 , 
where wi = :E:"' £' . In particular, w E ~n. To prove the positive semidefiniteness of 
J=le 
V2f(x), take 0 # v E Rn and consider the expression 
n 
v7 V 2f(x)v= .L:w;vf-(v7 w)2• 
i=l 

7 .4. Operations Preserving Convexity 
125 
The latter expression is nonnegative since employing the Cauchy-Schwarz inequality on 
the vectors s, t defined by 
S· = f'iii):v. 
i 
'V""'i ,, 
ti = .ftiii, i = 1, 2, ... , n, 
yields 
(VT w)2 = ( .r t)2 < ll•ll'lltll2 = ( t, W;'V~) ( t, W;) WE~ "tt·"~' 
establishing the inequality v7 V2f(x)v > 0. Since the latter inequality is valid for any 
v E Rn, it follows that V2 f (x) is indeed positive semidefinite. 
I 
Example 7.15 (quadratic-over-linear). Let 
defined over Rx R++ = {(x1, x2): x2 > O}. The Hessian off is given by 
By Proposition 2.20, since the Hessian is a 2 x 2 matrix, it is enough to show that the trace 
and determinant are nonnegative, and indeed, 
establishing the positive semidefiniteness of V 2f(x1'x2) and hence the convexity 
off. I 
7.4 •Operations Preserving Convexity 
There are several important operations that preserve the convexity property. First, the 
sum of convex functions is a convex function and a multiplication of a convex function 
by a nonnegative number results with a convex function. 
Theorem 7.16 (preservation of convexity under summation and multiplication by 
nonnegative scalars). 
(a) Let f be a convex function defined over a convex set C C Rn and let a > 0. Then a f 
is a convex function over C. 
(b) Let ft ,Ji, ... ,[p be convex functions over a convex set C C Rn. Then the sum function 
ft+ h + · · · + fp is convex over C. 

126 
Chapter 7. Convex Functions 
Proof. (a) Denote g(x) = 
a/(x). We will prove the convexity of g by definition. Let 
x,y EC and A E [O, 1]. Then 
g(...lx+{1-...l)y) = a/(...lx+(1-...l)y) 
(definition of g) 
< a...l/(x) + a(1-...l)/(y) (convexity of/) 
= ...lg(x)+(1-...l)g(y) 
(definition of g). 
(b) Let x,y EC and ...le [O, 1]. For each i = 1,2, ... ,p, since f; is convex, we have 
/;(...lx+(1-...l)y) < ...lf;(x)+{l-...l)f;(y). 
Summing the latter inequality over i = 1, 2, ... , k yields the inequality 
g(...lx + (1- ...l)y) < ...lg(x) + (1- ...l)g(y) 
for all x,y EC and ...l E [O, 1], where g =ft+ h. +···+fr We have thus established that 
the sum function is convex. 
0 
Another important operation preserving convexity is linear change of variables. 
Theorem 7.17 (preservation of convexity under linear change of variables). Let f: 
C -+JR be a convex function defined on a convex set C C ]Rn. Let A E 1Rnxm and b E 1Rn. 
Then the function g defined by 
g(y) = /(Ay+ b) 
is convex over the convex set D = {y E JR m : Ay + b E C}. 
Proof. First of all, note that D is indeed a convex set since it can be represented as an 
inverse linear mapping of a translation of C (see Theorem 6.8): 
X1 =Ay1 +h, 
x2 =Ay2 +b, 
(7.11) 
(7.12) 
which by the definition of D satisfy x1,x2 E C. Let A E [O, 1]. By the convexity off 
we have 
/(...lx1 +(1-...l)x2) < ...l/(x1)+(1-...l)/(x2). 
Plugging the expressions (7.11) and (7.12) of x1 and x2 into the latter inequality, we ob-
tain that 
which is the same as 
thus establishing the convexity of g. 
0 

7 .4. Operations Preserving Convexity 
127 
Example 7.18 (generalized quadratic-over-linear). Let A E IRmxn, b E IRm, c E IRn, and 
d E IR. We assume that cf= 0. We will show that the quadratic-over-linear function 
is convex over D = {x E IRn : cT x + d > O}. We begin by proving the convexity of the 
function 
h(y, t) = llYll2 
t 
over the convex set C = {(DE IRm+t : y E IRm, t > O}. For that, note that h = :E~ 1 hi 
where 
y~ 
hi (y' t) = -' . 
t 
By the convexity of the quadratic-over-linear function rp(x,z) = : 2 over {(x,z) : x E 
IR,z > O} (see Example 7.15), it follows that hi is convex for any i (specifically, hi is 
generated from rp by the linear transformation x =Yi, z = t ). Hence, h is convex over C. 
The function f can be represented as 
f(x) = h(Ax+b,cT x+d). 
Consequently, since f is the function h which has gone through a linear change of vari-
ables, it is convex over the domain { x E IRn : cT x + d > 0}. 
I 
Example 7 .19. Consider the function 
f (x1, Xz) = x: + 2x1Xz+3xi + 2x1 -3x2 +ext. 
To prove the convexity off, note that f =ft + fi, where 
ft(xpx2) =xi +2x1x2 +3xi +2x1 -3x2, 
fi(x1, Xz) = ex1. 
The function ft is convex since it is a quadratic function with an associated matrix A = 
( l l) which is positive semidefinite since Tr(A) = 4 > 0, det(A) = 2 > 0. The function h. 
is convex since it is generated from the one-dimensional convex function rp( t) = et by the 
linear transformation t = x1. 
I 
Example 7.20. The function f ( x1, x2, x3) = ex1-x2+x3 +e2x2 +x1 is convex over IR3 as a sum 
of three convex functions: the function ex1-x2+x3, which is convex since it is constructed 
by making the linear change of variables t = x1 -x2 +x3 in the one-dimensional rp( t) =et. 
For the same reason, e2x2 is convex. Finally, the function x1, being linear, is convex. 
I 
Example 7.21. The function f(x1,x2) = -ln(x1x2) is convex over IR~+ since it can be 
written as 
f(x 1,x2) =-ln(x1)-ln(x2), 
and the convexity of-ln(x1) and-ln(x2) follows from the convexity of rp(t) = -ln(t) 
overIR++· 
I 

128 
Chapter 7. Convex Functions 
In general, convexity is not preserved under composition of convex functions. For 
example, let g(t) = t 2 and h(t) = t 2 -4. Then g and h are convex. However, their 
composition 
s(t) = g(h(t)) = (t2-4)2 
is not convex, as illustrated in Figure 7.4. (This can also be seen by the fact that s"(t) = 
12t2 -16 and hence s" ( t) < 0 for all It I < JI.) The next result shows that convexity is 
preserved in the case of a composition of a nondecreasing convex function with a convex 
function. 
(x2-4)2 
1200 
1000 
800 
600 
400 
200 
0 
- 6 
- 4 
- 2 
0 
2 
4 
6 
x 
Figure 7.4. The rumconvexfunctum (t2-4)2• 
Theorem 7.22 (preservation of convexity under composition with a nondecreasing 
convex function). Let f : C -+ JR be a convex function over the convex set C C JRn. Let 
g : I -+ JR be a one-dimensional nondecreasing convex function over the interval I C JR. 
Assume that the image ofC under f is contained in I: f(C) CI. Then the composition of g 
with f defined by 
h(x) = 
g(/(x)), 
x EC, 
is a convex function over C. 
Proof. Let x,y EC and let A E (0, 1]. Then 
h(...lx+(t-A)y) = g(/(Ax+(1-A)y)) 
< g(Af (x) + (1-A)f (y)) 
< Ag(/(x)) + (1-A)g(/(y)) 
= Ah(x) + (1 - A)h(y) 
thus establishing the convexity of h. 
D 
(definition of h) 
(convexity off and monotonicity of g) 
(convexity of g) 
(definition of h), 
Example 7.23. The function h(x) = ellxll2 is convex since it can be represented as h(x) = 
g(/(x)), where g( t) = et is a nondecreasing convex function and f (x) = llx112 is a convex 
function. 
I 

7 .4. Operations Preserving Convexity 
129 
Example 7.24. The function h(x) = (llx112+1)2 is a convex function over Rn since it can 
be represented as h(x) = g{f(x)), where g(t) = t 2 and f(x) = llxll2 + 1. Both f and g 
are convex, but note that g is not a nondecreasing function. However, the image of Rn 
under f is the interval [1,oo) on which the function g is nondecreasing. Consequently, 
the composition h(x) = g{f(x)) is convex. 
I 
Another important operation that preserves convexity is the pointwise maximum of 
convex functions. 
Theorem 7 .25 (pointwise maximum of convex functions). Let ft, ... , fp : C __. IR be p 
convex functions over the convex set C C Rn. Then the maximum function 
f(x) =. max J;(x) 
i=l,2, ... ,p 
is a convex function over C. 
Proof. Let x,y EC and let A. E [O, 1]. Then 
f (.A.x+ (1-.A.)y) = maxi=t,2, ... ,p/i(.A.x+ (1-.A.)y) 
< maxi=t,2, ... ,p { .A.J;(x) + (1-.A.)J;(y)} 
< .A.max._12 
"-(x)+ (1-.A.)max._12 
l'.(y) 
-
,_, , ... ,pJi 
,_, , ... ,pJi 
= .A.f(x) + (1- .A.)f (y) 
(definition off) 
(convexity of/;) 
(*) 
(definition off). 
The inequality(*) follows from the fact that for any two sequences {ai }f =l' {bi }f =l one has 
. max (ai + h;) < . max a;+. max b;. 
D 
i=l,2, ... ,p 
i=l,2, ... ,p 
z=l,2, ... ,p 
Example 7 .26 (convexity of the maximum function). Let 
f (x) = max{x1' x2, ... ,xn}· 
Then since f is the maximum of n linear functions, which are in particular convex, it 
follows by Theorem 7 .25 that it is convex. 
I 
Example 7.27 (convexity of the sum of the k largest values). Given a vector x = 
(x1,x2, ... ,xnf · 
Let x[i] denote the ith largest value in x. 
In particular, x(t] = 
max { x1, x2, ••• , xn } and x[ n J = min { x1, x2, ••• , xn}. As stated in the previous example, the 
function h(x) = x[t] is convex. However, in general the function h(x) = x[i] is not convex. 
On the other hand, the function 
that is, the function producing the sum of the k largest components, is in fact convex. To 
see this, note that bk can be rewritten as 
so that bk, as a maximum of linear (and hence convex) functions, is a convex function. 
I 
Another operation preserving convexity is partial minimization. 

130 
Chapter 7. Convex Functions 
Theorem 7.28. Let f : C x D -+ JR be a convex function defined over the set C x D where 
C C Rm and D C Rn are convex sets. Let 
g(x) = minf(x,y), 
x e C, 
yeD 
where we assume that the minimum in the above definition is finite. Then g is convex over C. 
Proof. Let x1,x2 EC and). E (0, 1]. Take e > 0. Then there exist y1,y2 ED such that 
By the convexity off we have 
f(x1,Y1) < g(x1)+e, 
f (x2, Y2) < g(x2) + t · 
f(...lx1 +(1-...l)x2,AY1 +(1-...l)y2) 
< 
...lf(x1,Y1)+(1-...l)f(x2,Y2) 
(7.13),(7.14) 
< 
...l(g(x1)+t)+(1-...l)(g(x2)+t) 
...lg(x1) + (1-...l)g(x2) + t. 
By the definition of g we can conclude that 
g(...lx1 +(1-...l)x2) < ...lg(x1)+(1-...l)g(x2)+e. 
(7.13) 
(7.14) 
Since the above inequality holds for any e > 0, it follows that g(,lx1+(1-...l)x2) < ,lg(x1)+ 
(1-...l)g(x2), and the convexity of g is established. 
D 
Note that in the latter theorem, we only assumed that the minimum is finite, but we 
did not assume that it is attained. 
Example 7.29 (convexity of the distance function). Let CC Rn be a convex set. The 
distance function defined by 
d(x, C) = min{llx-yll: y EC} 
is convex since the function f (x, y) = I lx-yl I is convex over Rn x C, and thus by Theorem 
7.28 it follows that d(·,C) is convex. 
I 
7 .5 • Level Sets of Convex Functions 
We begin with the definition of a level set. 
Definition 7.30 (level sets). Let f : S -+JR be a function defined over a set S C Rn. Then 
the level set off with level a is given by 
Lev(/,a) = {x ES: f(x) <a}. 
A fundamental property of convex functions is that their level sets are necessarily 
convex. 
Theorem 7.31 (convexity of level sets of convex functions). Let f: C-+ JR bea convex 
function defined over a convex set C C Rn. Then for any a E JR the level set Lev(/, a) is 
convex. 

7 .5. Level Sets of Convex Functions 
131 
Proof. Let x,yE Lev(f,a) and A E [O, 1]. Thenf(x),f(y) <a. By the convexity of C we 
have that Ax+ ( 1-A)y E C, which combined with the convexity off yields 
f(Ax+(l-A)y) < Af(x)+(1-A)f(y) < Aa+(1-A)a =a, 
establishing the fact that Ax + ( 1 - A)y E Lev(/, a) and subsequently the convexity of 
Lev(f,a). 
0 
Example 7.32. Consider the following subset of R_n: 
where Q >- 0 is an n x n matrix. The set D is convex as a level set of a convex function. 
Specifically, D = Lev(/, 3 ), where 
/(x) = (xTQx+ 1)2 +1n(t.•"). 
The function f is indeed convex as the sum of two convex functions: the log-sum-exp 
function, which was shown to be convex in Example 7.14, and the function g(x) = 
(xT Qx + 1)2, which is convex as a composition of the nondecreasing convex function 
<p( t) = ( t + 1 )2 defined on R+ with the convex quadratic function xT Qx. 
I 
All convex functions have convex level sets, but the reverse claim is not true. That is, 
there do exist nonconvex functions whose level sets are all convex. Functions satisfying 
the property that all their level sets are convex are called quasi-convex functions. 
Definition 7 .33 (quasi-convex functions). A function f : C -+ JR defined over the convex 
set C C Rn is called quasi-convex if for any a E JR the set Lev(/, a) is convex. 
The following example demonstrates the fact that quasi-convex functions may be non-
convex. 
Example 7.34. The one-dimensional function f(x) = JiXj is obviously not convex (see 
Figure 7.5), but its level sets are convex: for any a < 0 we have that Lev(/, a) = 0, and for 
any a > 0 the corresponding level set is convex: 
Lev(/, a)= {x: JiXi <a}= {x: lxl < a 2} = [-a2,a2]. 
We deduce that the nonconvex function f is quasi-convex. 
I 
Example 7.35 (linear-over-linear). Consider the function 
where a, c e Rn and b, d E JR. To avoid trivial cases, we assume that c -:j:. 0 and that the 
function is defined over the open half-space 
C={xeR.n :cTx+d>O}. 

132 
Chapter 7. Convex Functions 
2 .................... . 
1.5 ................... . 
1 ................... . 
0.5 ................... . 
o~--~--~--~~--~--~--~ 
- 6 
- 4 
- 2 
0 
2 
4 
6 
Figure 7 .5. The quasi-convex function JiXj. 
In general, f is not a convex function, but it is not difficult to show that it is quasi-convex. 
Indeed, let a E JR. Then the corresponding level set is given by 
Lev(f ,a)= {x EC :f(x) <a}= {x E Rn: cT x+d > O,(a-ac)T x+(b-ad) < O}, 
which is convex due to the fact that it is an intersection of two half-spaces (which are in 
particular convex sets) when a -:/: ac, and when a= ac it is either a half-space (if b-ad < 0) 
or the empty set (if b-ad > 0). 
I 
7 .6 • Continuity and Differentiability of Convex Functions 
Convex functions are not necessarily continuous when defined on nonopen sets. Let us 
consider, for example, the function 
/(x)= { 1, 
x2 , 
x=O, 
0<x<1, 
defined over the interval (0, 1 ]. It is easy to see that this is a convex function, and obviously 
it is not a continuous function (as also illustrated in Figure 7.6). The main result is that 
convex functions are always continuous at interior points of their domain. Thus, for 
: 
: 
1 ··························!····························i····························i·····························f····························i··························· 
~: r ! r i ! 
: 
: 
. 
o.2 
··························t····························1····························i ···· 
·· · · ·· · ··· · ·· · ·· · · · r···· · ·· ·· ·· ·· ··· · ··· · ······~ · ······· · ··· ·· ·· · ··· · ······ 
o 
· ···· · ······ ··· ·· · · · ··· ···'---=
···=·
···=
· ······ ~·- ·· · ······ ·· ··· · ··· · ··· · ··· r · · · · · · ··· · ··· · ··· · ··· · ··· · ·:····· · ··· · ··· · ·· · ···· · ····· + ··· · · · ········· · ··· · ··· · ·· 
- 0.2 .__ _
_ _.._ ___ 
..___ __ 
~---~--~--~ 
- 0.2 
0 
0.2 
0.4 
0.6 
0.8 
1 
Figure 7 .6. A noncontinuous convex function over the interval [ 0, 1 ]. 

7 .6. Continuity and Differentiability of Convex Functions 
133 
example, functions which are convex over the entire space Rn are always continuous. We 
will prove an even stronger result: convex functions are always local Lipschitz continuous 
at interior points of their domain. 
Theorem 7.36 (local Lipschitz continuity of convex functions). Let f : C -... R be a 
convex function defined over a convex set C C Rn. Let Xo E int( C). 1ben there exist E > 0 
and L > 0 such that B[Xo,E] CC and 
l/(x)-/(Xo)I < Lllx-:xoll 
(7.15) 
for all x E B[Xo,E ]. 
Proof. Since Xo E int( C), it follows that there exists E > 0 such that 
Next we show that/ is upper bounded over BocJXo,E ]. Letv1, v2, ••• , v2n bethe2n extreme 
points of B00 [ :xo, E ]; these are the vectors vi = x0 + EW i, where w 1, ••• , w 2n are the vectors 
in {-1, l}n. Then by the Krein-Milman theorem (Theorem 6.35), for any x E B00[x0,E] 
there exists). E 1:12n such that x = LT:i ).ivi, and hence, by Jensen's inequality, 
where M = maxi=t,i, .. .,2n f (vi). Since llxlloo < llxlb for any Rn it holds that 
B2[x0,E] = B[Xo,E] = {x E Rn: llx-x0lb < E} C B00[Xo,E]. 
We therefore conclude that /(x) < M for any x E B[Xo,E ]. Let x E B[x0,E] be such that 
x '/:- Xo· (The result (7.15) is obvious when x = Xo·) Define 
1 
z = Xo +-(x-:xo), 
a 
where a= ~llx-:xoll· Then obviously a< 1 and z E B[Xo,E ], and in particular /(z) <M. 
In addition, 
x = az+(l-a):xo. 
Consequently, by Jensen's inequality we have 
/(x) < a/(z)+{l-a)/(Xo) 
<f(:xo)+a(M -f(Xo)) 
M-f(Xo) 
=/(Xo)+ 
llx-:xoll· 
E 
We can therefore deduce that /(x)-/(Xo) < Lllx-:xoll, where L = M-~(Xo). To prove the 
result, weneedtoshowthat/{x)-/(Xo) >-Lllx-:xoll· For that, defineu = x0 +~(:xo-x). 
Clearly we have llu -:xoll = E and hence u E B[Xo,E] and in particular /(u) < M. In 
addition, x = Xo + a( Xo - u ). Therefore, 
/(x) = f(Xo +a(:xo-u)) > /(Xo)+a{f(:xo)-/(u)). 
(7.16) 

134 
Chapter 7. Convex Functions 
The latter inequality is valid since 
1 
a 
Xo = -(Xo+a(Xo-u))+-u, 
1+a 
1+a 
and hence, by Jensen's inequality 
1 
a 
f(Xo)<-f(Xo+a(Xo-u))+-f(u), 
l+a 
l+a 
which is the same as the inequality in (7.16) (after some rearrangment of terms). Now, 
continuing (7.16), 
f(x) > f(Xo)+ a<f(Xo)-f(u)) 
> f(Xo)- a(M - f (xo)) 
=f(Xo)- M-f(Xo)llx-Xoll 
e 
= f(Xo)-Lllx-Xoll, 
and the desired result is established. 
0 
Convex functions are not necessarily differentiable, but on the other hand, as will be 
shown in the next result, all the directional derivatives at interior points exist. 
Theorem 7.37 (existence of directional derivatives for convex functions). Let f: C--.. 
JR be a convex function defined over the convex set C C Rn. Let x E int{ C). 1hen for any 
d f:. 0, the directional derivative f'(x; d) exists. 
Proof. Let x E int{ C) and d f:. 0. Then the directional derivative (if exists) is the limit 
l. 
g(t)-g(O) 
tm 
' 
t-o+ 
t 
(7.17) 
wh_ere g(t) = f(x + td). Defining h(t) = 
g(t~g(O), the limit (7.17) can be equivalently 
written as 
lim h(t). 
t-o+ 
Note that g, as well as h, is defined for small enough values of t by the fact that x E int( C). 
In fact, we will take an e > 0 for which x + td, x - td E C for all t E [O, e]. Now, let 
0 < t1 < ti < e. Then 
and thus, by the convexity off we have 
f(x+ t1d) < (1-~)f(x)+ ~f(x+ tid). 
ti 
ti 
The latter inequality can be rewritten (after some rearrangement of terms) as 
f(x+ t1d)-f(x) 
f(x+ tid)-f(x) 
------< 
' 
ti 
ti 

7.7. Extended Real-Valued Functions 
135 
which is the same as h(t1) < h(t2). We thus conclude that the function h is monotone 
nondecreasing over R++· All that is left is to prove that his bounded below over (0,E ]. 
Indeed, taking 0 < t < E, note that 
E 
t 
x= -(x+td)+-(x-Ed). 
E+t 
E+t 
Hence, by the convexity off we have 
E 
t 
f(x) < -f(x+ td)+ -f(x-Ed), 
E+t 
E+t 
which after some rearrangement of terms can be seen to be equivalent to the inequality 
h 
f(x+ td)-f(x) f(x)-f(x-Ed) 
(t)= 
> 
, 
t 
E 
showing that h is bounded below over (0, E ]. Since h is nondecreasing and bounded be-
low over ( 0, E] it follows that the limit limt-+0+ h( t) exists, meaning that the directional 
derivative f' ( x; d) exists. 
0 
7.7 •Extended Real-Valued Functions 
Until now we have discussed functions that are finite-val.ued, meaning that they take their 
values in R = (-oo, oo ). It is also quite natural to consider functions that are defined over 
the entire space Rn that take values in RU { oo} = (-oo, oo ]. Such a function is called an 
extended real-valued function. One very important example of an extended real-valued 
function is the indicator function, which is defined as follows: given a set S C Rn, the 
indicator function 8 s : Rn -. RU { oo} is given by 
85(x)= { ~ if x ES, 
if x'if S. 
The effective domain of an extended real-valued function is the set of vectors for which 
the function takes a finite value: 
dom(f) = { 
x E Rn : f (x) < oo }. 
An extended real-valued function f : Rn -. RU { oo} is called proper if it is not always equal 
to oo, meaning that there exists Xo E Rn such that f(Xo) < oo. Similarly to the definition 
for finite-valued functions, an extended real-valued function is convex if for any x,y E Rn 
and A E [ 0, 1] the following inequality holds: 
f(..l.x+(t-..l.)y) < ..l.f(x)+(l-..l.)f(y), 
where we use the usual arithmetic with oo: 
a+ oo = oo for any a ER, 
a · oo = oo for any a ER++· 
In addition, we have the much less obvious rule that 0 · oo = 0. The above definition 
of convexity of extended real-valued functions is equivalent to saying that dom(f) is a 

136 
Chapter 7. Convex Functions 
convex set and that the restriction off to its effective domain dom(f ); that is, the function 
g : dom(f)--+ IR defined by g(x) = /(x) for any x E dom(f) is a convex finite-valued 
function over dom(f). As an example, the indicator function Sc(-) of a set C C Rn is 
convex if and only if C is a convex set. 
An important set associated with extended real-valued functions is its epigraph. Sup-
pose that f: Rn--+ RU {oo}. Then the epigraph set epi(f) C JRn+t is defined by 
epi(f)= { (;) :f(x) < t}. 
An example of an epigraph can be seen in Figure 7.7. It is not difficult to show that an 
extended real-valued (or a real-valued) function f is convex if and only if its epigraph 
set epi(f) is convex (see Exercise 7 .29). An important property of convex extended real-
valued functions that convexity is preserved under the maximum operation. As was al-
ready mentioned, we do not use the "sup" notation in this book and we always refer to 
the maximum of a function or the maximum over a given index set. 
epi(f) 
f 
Figure 7.7. The epigraph of a one-dimensional function. 
Theorem 7.38 (preservation of convexity under maximum). Let fi : Rn --+ IR U { oo} be 
an extended real-valued convex function for any i E I {I being an arbitrary index set}. Then 
the function f (x) = maxiEI f;(x) is an extended real-valued convex function. 
Proof. The result follows from the fact that epi(f) = niEI epi(f;). The convexity of fi 
for any i E I implies the convexity of epi(f;) for any i E I . Consequently, epi(f), as an 
intersection of convex sets, is convex, and hence the convexity off is established. 
0 
The differences between Theorems· 7.38 and 7.25 are that the functions in Theorem 
7.38 are not necessarily finite-valued and that the index set I can be infinite. 
Example 7.39 (support functions). Let SC Rn. ThesupportfunctionofS is the function 
T 
us(x) = maxx y. 
yes 
Since for each y ES, the function /y(x) = yT xis a convex function over Rn {being linear), 
it follows by Theorem 7.38 that us is an extended real-valued convex function. 
I 

7 .8. Maxima of Convex Functions 
137 
As an example of a support function, let us consider the unit (Euclidean) ball S = 
B[O, 1] = {y E Rn : llYll < 1 }. Let x E Rn. We will show that 
us(x) = llxll· 
(7.18) 
Obviously, if x = 0, then u5(x) = 0 and hence (7.18) holds for x = 0. H x-/= 0, then for 
any y E S and x E JR_n, we have by the Cauchy-Schwarz inequality that 
XT y < llxll · llYll < llxll· 
On the other hand, taking y = 11!11 x E S, we have 
and the desired formula (7.18) follows. 
Example 7 .40. Consider the function 
where d E Rn and A( t) = I:~ 1 t;A; with A1, ••• , Am being n x n positive definite matrices. 
We will show that this function is convex over R~+. Indeed, for any x E Rn, the function 
is convex over Rm since it is an affine function over its convex domain. The corresponding 
max function is 
for t ER~+ and oo elsewhere. Therefore, the extended real-valued function 
j(t) = { dT A(t)-1d, t ER~+' 
oo 
else 
is convex over Rm, which is the same as saying that f is convex over JR.~+· 
I 
7.8 •Maxima of Convex Functions 
In the next chapter we will learn that problems consisting of minimizing a convex func-
tion over a convex set are in some sense "easy," but in this section we explore some impor-
tant properties of the much more difficult problem of maximizing a convex function over 
a convex feasible set. First, we show that the maximum of a nonconstant convex function 
defined on a convex set C cannot be attained at an interior point of the set C. 
Theorem 7.41. Let f: C-+ R be a convex function which is not constant over the convex 
set C. Then f does not attain a maximum at a point in int( C). 
Proof. Assume in contradiction that x* E int( C) is a global maximizer off over C. Since 
the function is not constant, there exists y EC such that f (y) < f (x*). Since x* E int( C), 

138 
Chapter 7. Convex Functions 
there exists e > 0 such that z = x* + e( x* -y) E C. Since x* = e~ 1 y + e! 1 z, it follows by 
the convexity off that 
e 
1 
f (x*) < -f 
(y) + -f(z), 
e+l 
e+l 
and hence f(z) > e(f(x*)-f(y)) + f(x*) > f(x*), which is a contradiction to the opti-
mality of x*. 
D 
When the underlying set is also compact, then the next result shows that there exists 
at least one maximizer that is an extreme point of the set. 
Theorem 7.42. Let f : C -+ R. be a convex and continuous function over the convex and 
compact set C C R_n. 1hen there exists at least one maximizer off over C that is an extreme 
point ofC. 
Proof. Let x* be a maximizer off over C (whose existence is guaranteed by the Weier-
strass theorem, Theorem 2.30). If x* is an extreme point of C, then the result is estab-
lished. Otherwise, if x* is not an extreme point, then by the Krein-Milman theorem 
(Theorem 6.35), C = conv(ext(C)), which means that there exist Xpx2,. .. ,xk E ext(C) 
and A E Ak such that 
k 
x*= ~Xx. 
~ 
t ,, 
i=t 
and Ai > 0 for all i = 1, 2, ... , k. Hence, by the convexity off we have 
k 
f(x*) < L:--lif(xi), 
i=t 
or equivalently 
k L: ..li(f(xi)-f(x*)) > 0. 
i=l 
(7.19) 
Since x* is a maximizer off over C, we have [(xi)< f(x*) for all i = 1,2, ... ,k. This 
means that inequality (7 .19) states that a sum of nonpositive numbers is nonnegative, 
implying that each of the terms is zero, that is, f (xi)= f (x*). Consequently, the extreme 
points x1, x2, ••• , xk are all maximizers of f over C. 
D 
Example 7 .43. Consider the problem 
max{x7 Qx: llxll00 < 1}, 
where Q >- 0. Since the objective function is convex, and the feasible set is convex and 
compact, it follows that there exists a maximizer at an extreme point of the feasible set. 
The set of extreme points of the feasible set is {-1, 1 }n, and hence we conclude that there 
exists a maximizer that satisfies that each of its components is equal to 1 or -1. 
I 
Example 7.44 (computation of llAll1,1). Let A E R.mxn. Recall that (see Example 1.8) 
llAll1,1 = max{llAxll1: llxll1<1 }. 

7.9. Convexity and Inequalities 
139 
Since the optimization problem consists of maximizing a convex function (composition 
of a norm function with a linear function) over a compact convex set, there exists a max-
imizer which is an extreme point of the 11 ball. Note that there are exactly 2n extreme 
points to the 11 ball: e1,-e1,e2,-e2, ••• ,en,-en. In addition, 
m 
llAeill1 = llA(-ei)ll1 = ~IAi,jl, 
i=l 
and thus 
m 
llAll1,1 =.max llAejll1 =.max ~IAi,jl· 
1=1,2, ... ,n 
1=1,2, ... ,n . 1 
t= 
This is exactly the maximum absolute column sum norm introduced in Example 1.8. 
I 
7 .9 • Convexity and Inequalities 
Convexity is a powerful tool for proving inequalities. For example, the arithmetic geo-
metric mean (AGM) inequality follows directly from the convexity of the scalar function 
-ln(x) over R++· 
Proposition 7 .45 (AGM inequality). For any x1, x2, ••• , xn > 0 the following inequality 
holds: 
(7.20) 
More generally, for any A E l::..n one has 
(7.21) 
Proof. Employing Jensen's inequality on the convex function f(x) = -ln(x), we have 
that for any X1' Xi' ..• ' xn > 0 and A E t::..n 
and hence 
or 
Taking the exponent of both sides we have 
n 
~Xx.> eL7=1 Ajln(x,) 
~ 
t 
t -
' 
i=l 

140 
Chapter 7. Convex Functions 
which is the same as the generalized AGM inequality (7.21). Plugging in Ai = ~ for all 
i yields the special case (7.20). We have proven the AGM inequalities only for the case 
when x1, x2, ••• , xn are all positive. However, they are trivially satisfied if there exists an i 
for which xi = 0, and hence the inequalities are valid for any x1, x2, ••• , xn > 0. 
0 
A direct result of the generalized AGM inequality is Young's inequality. 
Lemma 7.46 (Young's inequality). For any s,t > Oand p,q > 1satisfying~+~=1 it 
holds that 
sP 
tq 
st<-+-. 
p 
q 
Proof. By the generalized AGM inequality we have for any x,y > 0 
Setting x = sP ,y = tq in the latter inequality, the result follows. 
D 
(7.22) 
We can now prove several important inequalities. The first one is Holder's inequality, 
which is a generalization of the Cauchy-Schwarz inequality. 
Lemma 7.47 (Holder's inequality). For any x,y E Rn and p,q > 1satisfying~+~=1 
it holds that 
Proof. First, if x = 0 or y = 0, then the inequality is trivial. Suppose then that x -:J 0 and 
..J. 0 F 
. { 1 2 
} 
. 
- _hl 
d -
IY; I . (7 22) . ld h ' 
al" 
y / 
. or any z E 
, , ... , n , settmg s -
llxllp an t -
llxllq m . 
y1e st e mequ 1ty 
Summing the above inequality over i = 1,2, ... , n we obtain 
:l:7=1 lxiYi I < 2_ :l:7=1 lxi IP + ~ :l:?=1 IY; lq = 2_ + ~ = l. 
llxllpllYllp - P 
llxll~ 
q 
llYll: 
P 
q 
Hence, by the triangle inequality we have 
n 
lxT YI< 2: I xi Yi I< llxllpllYllq· 
D 
i=l 
Of course, for p = q = 2 Holder's inequality is just the Cauchy-Schwarz inequality. 
Another inequality that can be deduced as a result of convexity is Minkowski's inequality, 
stating that the p-norm (for p > 1) satisfies the triangle inequality. 
Lemma 7.48 (Minkowski's inequality). Let p > 1. Then for any x, y E Rn the inequality 
holds. 

Exercises 
141 
Proof. For p = 1, the inequality follows by summing up the inequalities Ix; +Yi I < 
Ix;! + IYd- Suppose then that p > 1. We can assume that x # 0, y # 0, and x + y # 0. 
Otherwise, the inequality is trivial. The function <p( t) = tP is convex over lR+ since 
<p11(t) = p(p- l)tP-2 > 0 fort> 0. Therefore, by the definition of convexity we have 
that for any A.1, A.2 > 0 with A.1 + A.2 = 1 one has 
L 
· { 1 2 
} Pl 
· 
, _ 
llxll, 
, _ 
llYll, 
_ Ix, I 
d _ IY, I · h 
et i E 
' '· · · 'n · 
uggmg 11.1 -
llxll,+llYll, '11.2 -
llxll,+llYll,' t -
llxll,' an s -
llYll, 10 t e 
above inequality yields 
Summing the above inequality over i = 1,2, ... , n, we obtain that 
and hence 
n 
L(lx; I+ IY; l)P < (llxllp + llYllp)P · 
i=l 
Finally, 
n 
n 
llx + Yllp = ' L Ix; + y;IP < ' L(lx; I+ IYd)P < llxllp + llYllP' 
D 
i=l 
i=l 
Exercises 
7.1. For each of the following sets determine whether they are convex or not (explain-
ing your choice). 
(i) C1 = {xelRn: llxll2=1}. 
(ii) C2 = { x E lRn : maxi=t,2, ... ,n xi < 1}. 
(iii) C3 = {XE lRn : mini=l,2, ... ,n X; < 1}. 
(iv) C4 = { x E JR~+ : IT?=t xi > 1} . 
7 .2. Show that the set 
M = { x E ]Rn : XT Qx < (a T x )2' a 
T x > 0}' 
where Q is an n x n positive definite matrix and a E lRn is a convex cone. 

142 
Chapter 7. Convex Functions 
7 .3. Let f : JR.n -+ JR. be a convex as well as concave function. Show that f is an affine 
function; that is, there exist a E JR.n and b E JR. such that f(x) = aT x + b for any 
x e JR_n. 
7.4. Let f : JR.n -+JR. be a continuously differentiable convex function. Show that for 
any e > 0, the function 
g.(x) = f (x) + ellxll2 
. 
. 
1s coercive. 
7.5. Let f : JR.n -+JR.. Prove that f is convex if and only if for any x E JR.n and d -:J 0, 
the one-dimensional function &x,d( t) = f (x + td) is convex. 
7.6. Prove Theorem 7.13. 
7.7. Let C C JR.n be a convex set. Let f be a convex function over C, and let g be 
a strictly convex function over C. Show that the sum function f + g is strictly 
convex over C. 
7 .8. 
(i) Let f be a convex function defined on a convex set C. Suppose that f is not 
strictly convex on C. Prove that there exist x,y E JR.n(x -:J y) such that f is 
affine over the segment [ x, y]. 
(ii) Prove that the function f ( x) = x4 is strictly convex on JR. and that g ( x) = xP 
for p > 1 is strictly convex over JR.+. 
7.9. Show that the log-sum-exp function f (x) = ln(L:?=l ex,) is not strictly convex 
over JRn. 
7.10. Show that the following functions are convex over the specified domain C: 
(i) f(x 1,x2,x3) = -Jx1x2 +ix;+ 2xi +3x:-2x1x2-2x2x3 over JR.~+· 
(ii) f (x) = llxll4 over JRn. 
(iii) f(x) = I:?=t X; ln(x;)-(I:?=t x;)ln(L:?=t x;) over JR~+· 
(iv) f(x) = JxT Qx+ 1 over Rn, where Q !:::: 0 is an n x n matrix. 
(v) f(x 1,x2,x3) =max{ J x; +x; +20xi-x1x2-4x2x3+1,(x;+xi+x1 +x2+ 
2)2} over JR3• 
(vi) f(xp x2) = (2x; + 3xi)(!x; + jx;). 
7.11. Let A E JRmxn, and let f: JRn-+ JR. be defined by 
where A; is the i th row of A. Prove that f is convex over ]Rn. 
7.12. Prove that the following set is a convex subset of JRn+2: 
C = { (0: llxll' < yz,x E IR" ,y,z E lR+}. 
7.13. Show that the function f (x1, x2, x3) = -e<-x1+x2-lx3)2 is not convex over JR_n. 
7.14. Prove that the geometric mean function /(x) = v'Il?=t X; is concave over JR~+· 
Is it strictly concave over JR~+? 

Exercises 
143 
7.15. 
(i) Let f : Rn -+JR be a convex function which is nondecreasing with respect 
to each of its variables separately; that is, for any i E { 1, 2, ... , n} and fixed 
x1, Xi, •.. , X;_ 1, X; + 1, ••• , xn, the one-dimensional function 
g;(Y) = f (x1' Xz, · .. 'xi-1,y, Xi+l" .. 'xn) 
is nondecreasing with respect to y. Let hp h2, ... , hn : JRP -+ JR be convex 
functions. Prove that the composite function 
r(z1, z2, ••• , zp) = f (h1 (z1, z2, ... ,zp), ... ,hn(z1, z2, ... , zp)) 
. 
1S convex. 
(ii) Prove that the function f(x1,x2) = ln(exf+x~ + e.vfxf+i) is convex over R2• 
7.16. Let f be a convex function over Rn and let x,y E Rn and a > 0. Define z = 
x+ ~(x-y). Prove that 
/(y) > /(x) + a(f(x)-/(z)). 
7.17. Let f be a convex function over a convex set C C Rn. Let x1,x3 E C and let 
x2 E [x1'x3]. Prove that if x1,x2,x3 are different from each other, then 
7.18. Let</>: R++-+ JR be a convex function. Then the function f: JR~+-+ JR defined by 
f(x,y) = y<f G)• x,y > 0, 
is convex over JR~+. 
7.19. Prove that the function f(x,y) =-xPy1-P(O < p < 1) is convex over JR~+· 
7 .20. Let f : C -+ JR be a function defined over the convex set C C Rn. Prove that f is 
quasi-convex if and only if 
/(.J.x+ (1-.J.}y) < max{/(x),/(y}}, 
for any x,y EC,). E [O, 1]. 
7.21. Let f (x) = £~=~,where g is a convex function defined over a convex set C C Rn 
and h(x) = aT x + b for some a E Rn and b E JR. Assume that h(x) > 0 for all 
x E C. Show that f is quasi-convex over C. 
7 .22. Show an example of two quasi-convex functions whose sum is not a quasi-convex 
function. 
7.23. Let /(x) = xT Ax+2bT x+ c, where A is an n x n symmetric matrix, b E Rn, and 
c E JR. Show that f is quasi-convex if and only if A >- 0. 
7.24. A function f: C-+ JR is called log-concave over the convex set CC Rn if /(x) > 0 
for any x E C and ln(f} is a concave function over C. 
(i) Show that the function f ( x) = I:~ 1 l. is log-concave over JR~+. 
•=1 x, 
(ii) Let f be a twice continuously differentiable function over JR with /(x) > 0 
for all x E JR. Show that f is log-concave if and only if f"(x)f(x) < (f'(x))2 
for all x ER. 

144 
Chapter 7. Convex Functions 
7.25. Prove that if f and g are convex, twice differentiable, nondecreasing, and posi-
tive on JR, then the product f g is convex over JR. Show by an example that the 
positivity assumption is necessary to establish the convexity. 
7.26. Let C be a convex subset of Rn. A function f is called strongly convex over C 
if there exists u > 0 such that the function f ( x) - I I Ix! 1
2 is convex over C. The 
parameter u is called the strong convexity parameter. In the following questions C 
is a given convex subset of Rn. 
(i) Prove that f is strongly convex over C with parameter u if and only if 
O' 
/(Ax+(t-.rl)y) < .rl/(x)+(t-.rl)/(y)-2.rl(t-.rl)llx-yll2 
for any x,y EC and A E [O, 1]. 
(ii) Prove that a strongly convex function over C is also strictly convex over C. 
(iii) Suppose that f is continuously differentiable over C. Prove that f is strongly 
convex over C with parameter u if and only if 
O' 
f(y) > f (x) + V f (x)T (y-x) + 2llx-yl12 
for any x, y E C. 
(iv) Suppose that f is continuously differentiable over C. Prove that f is strongly 
convex over C with parameter u if and only if 
(V f (x)-V f (y)f (x-y) > ullx-y!l2 
for any x, y E C. 
(v) Suppose that f is twice continuously differentiable over C. Show that f is 
strongly convex over C with parameter u if and only if V2 f ( x) >- u I for any 
xEC. 
7.27. 
(i) Show that the function /(x) = J 1 + llxll2 is strictly convex over Rn but is 
not strongly convex over Rn. 
(ii) Show that the quadratic function f (x) = xT Ax+ 2bT x + c with A = AT E 
Rnxn ,b E Rn ,c E JR is strongly convex if and only if A>- 0, and in that case 
the strong convexity parameter is 2.rlmin(A). 
7.28. Let f E Ci•1(Rn) be a convex function. For a fixed x E Rn define the function 
(i) Prove that x is a minimizer of gx over Rn. 
(ii) Show that for any x,y E Rn 
(iii) Show that for any x,y E Rn 
1 
f (x) < f (y) + V f (x)T (y-x)- 2L llV /(x)-V /(y)ll2• 

Exercises 
145 
(iv) Prove that for any x, y E Rn 
1 
(V/(x)-V/(y),x-y} > L llV/(x)-V/(y)!l2 for any x,yE Rn. 
7.29. Let f: Rn-+ RU {oo} be an extended real-valued function. Show that f is convex 
if and only if epi(f) is convex. 
7 .30. Show that the support function of the set S = { x E Rn : xT Qx < 1 }, where Q >- 0, 
is u s(Y) = J YT Q-ty. 
7.31. Let S = {x E Rn : aT x < b}, where 0 /;a E Rn and b ER. Find the support 
function us. 
7.32. Let p > 1. Show that the support function of S = {x E Rn : llxllp < 1} is u5(y) = 
llYllq' where q is defined by the relation 'i; + ~ = 1. 
7.33. Let fo,ft, ... ,fm be convex functions over Rn and consider the perturbation 
function 
F(b) = min{fo(x): li(x) < h;, i = 1,2, ... , m }. 
x 
Assume that for any b E Rm the minimization problem in the above definition of 
F (b) has an optimal solution. Show that F is convex over Rm. 
7.34. Let CC Rn be a convex set and let </J1, ••• , <Pm be convex functions over C. Let U 
be the following subset of Rm: 
Show that U is a convex set. 
7.35. 
(i) Show that the extreme points of the unit simplex An are the unit-vectors 
et,e2, ••• ,en. 
(ii) Find the optimal solution of the problem 
max 57xi +65xi + 17xi +96x1x2-32x1x3 +8x2x3 +27x1 -84x2 +20x3 
S.t. 
x1 + X2 + X3 = 1 
Xp x2, x3 > 0. 
7.36. Prove that for any x1,x2, ••• ,xn ER+ the following inequality holds: 
°"~ X· 
~ °"~ x2 
~z=l z < 
~z=1 z • 
n 
n 
7 .3 7. Prove that for any x1, x2, ••• , xn E R++ the following inequality holds: 
"~ x~ 
~z=1 I < 
"n 
-
~i=t X; 
7.38. Let x1, x2, ••• , xn > 0 satisfy L?=t X; = 1. Prove that 
~ X· 
~ 
~~='=> --. 
i=l .J 1-X; -
n -1 

146 
Chapter 7. Convex Functions 
7 .39. Prove that for any a, b, c > 0 the following inequality holds: 
__ 
9_<2(-1-+_1_+_1_)· 
a+b+c -
a+b 
b+c 
c+a 
7.40. 
(i) Prove that the function f(x) = t~e" is strictly convex over [O,oo). 
(ii) Prove that for any a1, a2, ••• , an > 1 the inequality 
holds. 

Chapter 8 
Convex Optimization 
8.1 • Definition 
A convex optimization problem (or just a convex problem) is a problem consisting of mini-
mizing a convex function over a closed and convex set. More explicitly, a convex problem 
is of the form 
mm f(x) 
s.t. 
x EC, 
(8.1) 
where C is a closed and convex set and f is a convex function over C. Problem (8.1) is in a 
sense implicit, and we will often consider more explicit formulations of convex problems 
such as convex optimization problems in functional form, which are convex problems of 
the form 
min f(x) 
s.t. 
gi(x) < 0, 
hi(x) = 0, 
i = 1,2, ... ,m, 
(8.2) 
j=1,2, ... ,p, 
where f, g1, ... , gm : Rn --+IR. are convex functions and h1, h2, ••• , hp : Rn --+IR. are affine 
functions. Note that the above problem does fit into the general form (8.1) of convex 
problems. Indeed, the objective function is convex and the feasible set is a convex set 
since it can be written as 
which implies that C is a convex set as an intersection of level sets of convex sets, which 
are necessarily convex sets, and hyperplanes, which are also convex sets. The set C is 
closed as an intersection of hyperplanes and level sets of continuous functions. (Recall 
that a convex function is continuous over the interior of its domain; see Theorem 7.36.) 
The following result shows a very important property of convex problems: all local 
minimum points are also global minimum points. 
Theorem 8.1 (local=global in convex optimization). Let f : C --+ JR. be a convex func-
tion defined on the convex set C. Let x* E C be a local minimum off over C. 1hen x* is a 
global minimum off over C. 
147 

148 
Chapter 8. Convex Optimization 
Proof. Since x* is a local minimum off over C, it follows that there exists r > 0 such that 
/(x) > /(x*) for any x e C satisfying x E B[x*, r]. Now let y EC satisfy y -=f x*. Our 
objective is to show that /(y) > f (x*). Let). E (0, 1] be such that x*+J.(y-x*) E B[x*, r ]. 
An example of such ). is ). = lly~x*ll" Since x* + J.(y- x*) E B[ x*, r] n C, it follows that 
/(x*) < /(x* + J.(y-x*)), and hence by Jensen's inequality 
/(x*) < /(x* + J.(y-x*)) < (1-J.)/(x*) + J./(y). 
Thus, Af (x*) < Af (y), and hence the desired inequality /(x*) < f (y) follows. 
D 
A slight modification of the above result shows that any local minimum of a strictly 
convex function over a convex set is a strict global minimum of the function over the set. 
Theorem 8.2. Let f : C -+ IR be a strictly convex function defined on the convex set C. Let 
x* E C be a local minimum off over C. Then x* is a strict global minimum off over C. 
The optimal set of the convex problem (8.1) is the set of all minimizers, that is, 
argmin{/(x): x EC}. This definition of an optimal set is also valid for general problems. 
An important property of convex problems is that their optimal sets are also convex. 
Theorem 8.3 (convexity of the optimal set in convex optimization). Let f: C-+ IR 
be a convex function defined over the convex set C C Rn. Then the set of optimal solutions of 
the problem 
min{/ (x): x EC}, 
(8.3) 
which we denote by X*, is convex. If, in addition, f is strictly convex over C, then there exists 
at most one optimal solution of the problem (8.3). 
Proof. If X* = 0, the result follows trivially. Suppose that X* -=J 0 and denote the optimal 
value by f*. Let x,y E X* and ). E [O, 1]. Then by Jensen's inequality /(J.x + (1-
J.)y) < .A.f* + (1- J.)/* = f*, and hence Ax+ ( 1- J.)y is also optimal, i.e., belongs to 
X*, establishing the convexity of X*. Suppose now that f is strictly convex and X* is 
nonempty; to show that X* is a singleton, suppose in contradiction that there exist x, y E 
X* such that x -:/:- y. Then ~ x + ~ y E C, and by the strict convexity off we have 
( 1 
1 ) 
1 
1 
1 
1 
f -x + -y < - /(x) + -f (y) = -f* + -f* = f*, 
2 
2 
2 
2 
2 
2 
which is a contradiction to the fact that f * is the optimal value. 
D 
Convex optimization problems consist of minimizing convex functions over convex 
sets, but we will also refer to problems consisting of maximizing concave functions over 
convex sets as convex problems. (Indeed, they can be recast as minimization problems of 
convex functions by multiplying the objective function by minus one.) 
Example 8.4. The problem 
mm 
s.t. 
is convex since the objective function is linear, and thus convex, and the single inequality 
constraint corresponds to the convex function f ( x1, x2) = xf + x;- 3, which is a convex 

8.2. Examples 
149 
quadratic function. On the other hand, the problem 
mm 
s.t. 
is nonconvex. The objective function is convex, but the constraint is a nonlinear equality 
constraint and therefore nonconvex. Note that the feasible set is the boundary of the ball 
with center (0, 0) and radius ./3. 
I 
8.2 • Examples 
8.2.1 • Linear Programming 
A linear programming (LP) problem is an optimization problem consisting of minimizing 
a linear objective function subject to linear equalities and inequalities: 
min 
cT x 
(LP) 
s.t. 
Ax < b, 
Bx=g. 
Here A E Rmxn, b E Rm, BE Rpxn, g ERP, c E Rn. This is of course a convex optimiza-
tion problem since affine functions are convex. An interesting observation concerning LP 
problems is based on the fact that linear functions are both convex and concave. Consider 
the following LP problem: 
max 
cT x 
s.t. 
Ax= b, 
x>O. 
In the literature the latter formulation is many times called the "standard formulation." 
The above problem is on one hand a convex optimization problem as a maximization 
of a concave function over a convex set, but on the other hand, it is also a problem of 
maximizing a convex function over a convex set. We can therefore deduce by Theorem 
7.42 that if the feasible set is nonempty and compact, then there exists at least one opti-
mal solution which is an extreme point of the feasible set. By Theorem 6.34, this means 
that there exists at least one optimal solution which is a basic feasible solution. A more 
general result dropping the compactness assumption is called the "fundamental theorem 
of linear programming," and it states that if the problem has an optimal solution, then it 
necessarily has an optimal basic feasible solution. 
Although the class of LP problems seems to be quite restrictive due to the linearity of 
all the involved functions, it encompasses a huge amount of applications and has a great 
impact on many fields in applied mathematics. Following is an example of a scheduling 
problem that can be recast as an LP problem. 
Example 8.5. For a new position in a company, we need to schedule job interviews for 
n candidates numbered 1, 2, ... , n in this order (candidate i is scheduled to be the i th 
interview). Assume that the starting time of candidate i must he in the interval [ai,(3i], 
where ai < f3i· To assure that the problem is feasible we assume that ai < f3i+t for any 
i = 1, 2, ... , n-1. The objective is to formulate the problem of finding n starting times of 
interviews so that the minimal starting time difference between consecutive interviews is 
maximal. 

150 
Chapter 8. Convex Optimization 
Let ti denote the starting time of interview i. The objective function is the minimal 
difference between consecutive starting times of interviews: 
f (t) =min{ t2 - tp t3 - t2, ••• , tn - tn_.J, 
and the corresponding optimization problem is 
max [ min{t2-tp t3 - t2, ••• , tn -tn_1}] 
s.t. 
ai <ti< (3;, 
i = 1,2, ... ,n. 
Note that we did not incorporate the constraints that t; < ti+t for i = 1,2, ... , n -1 
since the condition ai < f3i+t will guarantee in any case that these constraints will be 
satisfied in an optimal solution. The problem is convex since it consists of maximizing 
a concave function subject to affine (and hence convex) constraints. To show that the 
objective function is indeed concave, note that by Theorem 7 .25 the maximum of convex 
functions is a convex function. The corresponding result for concave functions (that can 
be obtained by simply looking at minus of the function) is that the minimum of concave 
functions is a concave function. Therefore, since the objective function is a minimum of 
linear (and hence concave) functions, it is a concave function. In order to formulate the 
problem as an LP problem, we reformulate the problem as 
maxt,s 
s.t. 
s 
min{ t2 - t1, t3 - t2, ••• , tn - tn_.J = s, 
a;< t; < f3i, 
i = 1,2, ... ,n. 
(8.4) 
We now claim that problem (8.4) is equivalent to the corresponding problem with an 
inequality constraint instead of an equality: 
maxt,s 
s 
s.t. 
min{ t2 - t1, t3 - ti, ... , tn - tn_1} > s, 
(8.5) 
a;< t; < f3i, 
i = 1,2, ... ,n. 
By "equivalent" we mean that any optimal solution of (8.5) satisfies the inequality con-
straint as an equality constraint. Indeed, suppose in contradiction that there exists an 
optimal solution ( t*, s*) of (8.5) that satisfies the inequality constraints strictly, meaning 
that min{ t;- t;, t;- t;, ... , t;- t;_1} > s*. Then we can easily check that the solution 
(t*, S'), wheres= min{ t;-t;, t;-t;, ... , t;-t;_1} is also feasible for (8.5) and has a larger 
objective function value, which is a contradiction to the optimality of (t*, s*). Finally, we 
can rewrite the inequality min{ t2 -
t1, t3 -
ti, ... , tn - tn-.l > s as ti+t - ti > s for any 
i = 1, 2, ... , n -1, and we can therefore recast the problem as the following LP problem: 
m~.s 
s.t. 
s 
t;+i -t; > s, 
a;< t; <(3;, 
i = 1,2, ... , n -1, 
i = 1,2, ... ,n. 
I 
8.2.2 • Convex Quadratic Problems 
Convex quadratic problems are problems consisting of minimizing a convex quadratic 
function subject to affine constraints. A general form of problems of this class can be 
written as 
min xTQx+2hT x 
s.t. 
Ax< c, 
where Q e Rnxn is positive semidefinite, he Rn ,A e Rmxn, and c e Rm. A well-known 
example of a convex quadratic problem arises in the area of linear classification and is 
described in detail next. 

8.2. Examples 
151 
4.5 
"' 
4 
"' 
3.5 
"' 
3 
.. • 
2.5 
2 
1.5 
1 
0.5 
0 
0 
0.5 
1 
1.5 
2 
2.5 
3 
3.5 
4 
4.5 
5 
Figure 8.1. Type A {asterisks) and type B (diamonds) points. 
8.2.3 • Classification via Linear Separators 
Suppose that we are given two types of points in Rn: type A and type B. The type A 
points are given by 
and the type B points are given by 
For example, Figure 8.1 describes two sets of points in R2: the type A points are denoted 
by asterisks and the type B points are denoted by diamonds. The objective is to find a 
linear separator, which is a hyperplane of the form 
H(w,fi) = {x E Rn: WT x+ fi = 0} 
for which the type A and type B points are in its opposite sides: 
WT X; + fi < 0, 
i = 1, 2, ... , m, 
wT X; + fi > 0, 
i = m + 1, m + 2, ... , m + p. 
Our underlying assumption is that the two sets of points are linearly separable, meaning 
that the latter set of inequalities has a solution. The problem is not well-defined in the 
sense that there are many linear separators, and what we seek is in fact a separator that 
is in a sense farthest as possible from all the points. At this juncture we need to define 
the notion of the margin of the separator, which is the distance of the separator from 
the closest point, as illustrated in Figure 8.2. The separation problem will thus consist of 
finding the separator with the largest margin. To compute the margin, we need to have 
a formula for the distance between a point and a hyperplane. The next lemma provides 
such a formula, but its proof is postponed to Chapter 10 (see Lemma 10.12), where more 
general results will be derived. 
Lemma 8.6. Let H(a,b) = {xERn: aT x = b}, whereO tf aERn and b ER LetyERn. 
Then the distance between y and the set H is given by 
laTy-bl 
d(y,H(a, b)) = 
llall 
· 

152 
Chapter 8. Convex Optimization 
4.5 
'* 
4 • 
3.5 
* 
margin 
3 * 
',_/ 
* 
' 
2.5 
~ 
2 
0 
1.5 
1 
0.5 
o.___.____.__.__..__.___.____.__._~.___.__~ 
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 
Figure 8.2. The optimal linear seperator and its margin. 
We therefore conclude that the margin corresponding to a hyperplane H(w,-{3) 
(w ~ 0) is 
. 
mm 
i=1,2, ... ,m+p 
So far, the problem that we consider is therefore 
max 
{ 
· 
lwT x;+,81} 
mmi=1,2, ... ,m+p 
llwll 
s.t. 
WT X; + f3 < 0, 
i = 1, 2, ... , m, 
wT xi + f3 > 0, 
i = m + 1, m + 2, ... , m + p. 
This is a rather bad formulation of the problem since it is not convex and cannot be easily 
handled. Our objective is to find a convex reformulation of the problem. For that, note 
that the problem has a degree of freedom in the sense that if ( w, {3) is an optimal solution, 
then so is any nonzero multiplier of it, that is, (aw,af3) for a~ 0. We can therefore 
decide that 
. min 
lwT X; + {31 = 1, 
i= 1,2, ... ,m+p 
and the problem can then be rewritten as 
max 
s.t. 
{1~!11} 
T 
mmi= l,2,. .. ,m+p lw X; + f31 = 1, 
WT X; + f3 < 0, 
i = 1, 2, ... , m, 
wTxi+f3>0, i=m+1,2, ... ,m+p. 
The combination of the first equality and the other inequality constraints imp.lies that a 
valid reformulation is 
mm 
s.t. 
lllwll2 
2 . 
T 
-
mmi= l,2,. . .,m+p lw Xi + f31 - 1, 
WT X; + f3 <-1, i = 1,2, ... ,m, 
W T X; + f3 > 1, 
i = m + 1, 2, ... , m + p, 

8.2. Examples 
153 
where we also used the fact that maximizing 11;
11 is the same as minimizing I lwl 12 in the 
sense that the optimal set stays the same. Finally, we remove the problematic "min" equal-
ity constraint and obtain the following convex quadratic reformulation of the problem: 
min !llwll2 
s.t. 
WT X; + f3 <-1, 
i = 1,2, ... , m, 
(8.6) 
wT xi + f3 > 1, 
i = m + 1, m + 2, ... , m + p. 
The removal of the "min" constraint is valid since any feasible solution of problem 
(8.6) surely satisfies mini=t,2, ... ,m+p lwT xi + /31 > 1. If (w,{3) is in addition optimal, 
then equality must be satisfied. Otherwise, if mini=t,2,. .. ,m+p lwT xi + /31 > 1, then a 
better solution (i.e., with lower objective function value) will be ~(w,f3), where a = 
mini=t,2, ... ,m+p lwT xi + f31. 
8.2.4 • Chebyshev Center of a Set of Points 
Suppose that we are given m points a1,a2, ••• ,am in Rn. The objective is to find the center 
of the minimum radius closed ball containing all the points. This ball is called the Cheby-
shev ball and the corresponding center is the Chebyshev center. In mathematical terms, the 
problem can be written as (r denotes the radius and xis the center) 
mtll,c,r 
s.t. 
r 
ai E B[x, r ], i=l,2, ... ,m. 
Of course, recalling that B[ x, r] = {y : I ly- xi I < r}, it follows that the problem can be 
written as 
mmx,r 
s.t. 
r 
llx-a;ll < r, 
i = 1,2, ... , m. 
(8.7) 
This is obviously a convex optimization problem since it consists of minimizing a linear 
(and hence convex) function subject to convex inequality constraints: the function I Ix -
ai 11- r is convex as a sum of a translation of the norm function and the linear function 
-r. An illustration of the Chebyshev center and ball is given in Figure 8.3. 
2.5 
2 
----- --
, 
, 
1.5 
' 
, 
' ' 
, 
' 
,, 
' 
1 
I 
.. 
\ 
I 
\ 
\ 
I 
I 
0.5 
I 
* 
I 
I 
I 
I 
0 
I 
I 
... 0 
- 0.5 
I 
I 
.;. 
I 
* 
I 
I 
lf 
I 
- 1 
I 
\ 
I 
I , 
- 1.5 
' 
, 
' 
, 
, 
- 2 
.... 
___ ____ ... 
- 2.5 .__~___.__..___.____._ _ _._____.___. _ _.____, 
- 3 -2.5 - 2 - 1.5 - 1 - 0.5 0 0.5 1 1.5 2 
Figure 8.3. 7be Cheb-yshev center {denoted by a diamond marker) of a set of 10 points {aster-
isks). The boundary of the Cheb-yshev ball is the dashed circle. 

154 
Chapter 8. Convex Optimization 
8.2.5 • Portfolio Selection 
Suppose that an investor wishes to construct a portfolio out of n given assets numbered 
as 1, 2, ... , n. Let Yj (j = 1, 2, ... , n) be the random variable representing the return from 
asset j. We assume that the expected returns are known, 
µj=lE(Yj), 
j=1,2, ... ,n, 
and that the covariances of all the pairs of variables are also known, 
u;,j = COV(Y;, Yj ), i,j = 1,2, ... , n. 
There are n decision variables x1, x2, ... , xn, where xj denotes the proportion of budget 
invested in asset j. The decision variables are constrained to be nonnegative and sum up 
to 1: x E b.n. The overall return is the random variable, 
n 
R= L:xjYj, 
j=1 
whose expectation and variance are given by 
lE(R) = µ7 x, V(R) = x7 Cx, 
whereµ= (µp µ2, ••• , µn)T and C is the cO'Uariance matrix whose elements are given by 
C; ,j = u i ,j for all 1 < i, j < n. It is important to note that the covariance matrix is always 
positive semidefinite. The variance of the portfolio, x7 Cx, is the risk of the suggested 
portfolio x. There are several formulations of the portfolio optimization problem, which 
are all referred to as the "Markowitz model" in honor of Harry Markowitz, who first 
suggested this type of a model in 1952. 
One formulation of the problem is to find a portfolio minimizing the risk under the 
constraint that a minimal return level is guaranteed: 
mm x7 Cx 
s.t 
µ 7x>a, 
(8.8) 
e7x=1, 
x>O, 
where e is the vector of all ones and a is the minimal return value. Another option is to 
maximize the expected return subject to a bounded risk constraint: 
max 
s.t 
µTx 
x7Cx < (3, 
e7X=1, 
x>O, 
(8.9) 
where (3 is the upper bound on the risk. Finally, a third option is to write an objective 
function which is a combination of the expected return and the risk: 
mm -µ7 x+y(x7 Cx) 
s.t 
eT x = 1, 
x>O, 
(8.10) 

8.2. Examples 
155 
where r > 0 is a penalty parameter. Each of the three models (8.8), (8.9), and (8.10) de-
pends on a certain parameter (a,[3, or r) whose value dictates the tradeoff level between 
profit and risk. Determining the value of each of these parameters is not necessarily an 
easy task, and it also depends on the subjective preferences of the investors. The three 
models are all convex optimization problems since xT Cx is a convex function {its associ-
ated matrix C is positive semidefinite). The model (8.10) is a convex quadratic problem. 
8.2.6 • Convex QCQPs 
A quadratically constrained quadratic problem, or QCQP for short, is a problem consist-
ing of minimizing a quadratic function subject to quadratic inequality and equalities: 
xT Aox+2h~ x+c0 
xT A;x+2b. x+c; < 0, 
i = 1,2, ... ,m, 
xT Ajx+2hJ x+cj =0, 
j = m+ 1,m +2, ... ,m+ p. 
. 
mm 
(QCQP) 
s.t. 
Obviously, QCQPs are not necessarily convex problems, but when there are no equal-
ity constrainers (p = 0) and all the matrices are positive semidefinite, A; >- 0 for i = 
0, 1, ... , m, the problem is convex and is therefore called a convex QCQP. 
8.2. 7 • Hidden Convexity in Trust Region Subproblems 
There are several situations in which a certain problem is not convex but nonetheless can 
be recast as a convex optimization problem. This situation is sometimes called "hidden 
convexity." Perhaps the most famous nonconvex problem possessing such a hidden con-
vexity property is the trust region subproblem, consisting of minimizing a quadratic func-
tion (not necessarily convex) subject to an Euclidean norm constraint: 
(fRS) 
min{xT Ax+2bT x+c: llxll2 < 1}. 
Here b E Rn, c E JR., and A is an n x n symmetric matrix which is not necessarily pos-
itive semidefinite. Since the objective function is (possibly) nonconvex, problem (fRS) 
is (possibly) nonconvex. This is an important class of problems arising, for example, as 
a subroutine in trust region methods, hence the name of this class of problems. We will 
now show how to transform (fRS) into a convex optimization problem. First, by the 
spectral decomposition theorem (Theorem 1.10), there exist an orthogonal matrix U and 
a diagonal matrix D = diag( d1, dz, ... , dn) such that A= UDUT, and hence (fRS) can be 
rewritten as 
(8.11) 
where we used the fact that I IUT xi I = I lxl I· Making the linear change of variables y = ur x, 
it follows that (8 .11) reduces to 
min{yr0y+2bruy+c: llYll2<1}. 
Denoting f = urb, we obtain the following formulation of the problem: 
. 
mm 
s.t. 
(8.12) 
The problem is still nonconvex since some of the dis might be negative. At this point, 
we will use the following result stating that the signs of the optimal decision variables are 
actually known in advance. 

156 
Chapter 8. Convex Optimization 
Lemma 8.7. Let y* be an optimal solution of (8.12). Then f;y; < 0 for all i = 1,2, ... , n. 
Proof. We will denote the objective function of problem (8.12) by f(y) = 
l:?=t diyf + 
2 I:?=thYi + c. Let i E {1,2, ... , n }. Define the vector y to be 
"'. = { y;, 
j ii, 
Y, 
-y* 
J. -i 
i' 
-
. 
Then obviously y is also a feasible solution of (8.12), and since y* is an optimal solution 
of (8.12), it follows that 
f (y*) < f (y), 
which is the same as 
n 
n 
n 
n 
L:di(Yn2 +2 L:J;y; +c < L:di(Yi)2 +2 L:hYi +c. 
i=l 
i=l 
i=l 
i=l 
Using the definition of y, the above inequality reduces after much cancelation of terms to 
which implies the desired inequality hYi < 0. 
D 
As a direct result of Lemma 8.7 we have that for any optimal solution y*, the equality 
sgn(yi) = -sgn(f;) holds when/; f; 0 and where the sgn function is defined to be 
( ) 
{ 1, 
x > 0, 
sgn x = 
_ 1 
0 
' x < . 
When f; = 0, we have the property that bothy* and y are optimal (see the proof of Lemma 
8.7), and hence the sign of y* can be chosen arbitrarily. As a consequence, we can make 
the change of variables Yi= -sgn(/;)J'Z;(zi > 0), and problem (8.12) becomes 
mm l:?=t dizi -2l:?=1 l/;l/z; +c 
s.t. 
I:?=t zi < 1, 
Zt,Zz, ... ,Zn> 0. 
(8.13) 
Obviously this is a convex optimization problem since the constraints are linear and the 
objective function is a sum of linear terms and positive multipliers of the convex functions 
-.[Zi.. To conclude, we have shown that the nonconvex trust region subproblem (fRS) 
is equivalent to the convex optimization problem (8.13). 
8.3 • The Orthogonal Projection Operator 
Given a nonempty closed convex set C, the orthogonal projection operator Pc: Rn-.. C 
is defined by 
Pc(x) = argmin{lly-xl!2 :yE C}. 
(8.14) 
The orthogonal projection operator with input x returns the vector in C that is closest 
to x. Note that the orthogonal projection operator is defined as a solution of a convex 
optimization problem, specifically, a minimization of a convex quadratic function subject 

8.3. The Orthogonal Projection Operator 
157 
to a convex feasibility set. The first orthogonal projection theorem states that the orthog-
onal projection operator is in fact well-defined, meaning that the optimization problem 
in (8.14) has a unique optimal solution. 
Theorem 8.8 (first projection theorem). Let C be a nonempty closed convex set. Then 
problem (8.14) has a unique optimal solution. 
Proof. Since the objective function in (8.14) is a quadratic function with a positive definite 
matrix, it follows by Lemma 2.42 that the objective function is coercive and hence, by 
Theorem 2.32, that the problem has at least one optimal solution. In addition, since the 
objective function is strictly convex (again, since the objective function is quadratic with 
positive definite matrix), it follows by Theorem 8.3 that there exists only one optimal 
solution. 
D 
The distance function was already defined in Example 7 .29 as 
d(x, C) = minllx-yll· 
yeC 
Evidently, the distance function can also be written in terms of the orthogonal projection 
as follows: 
d(x, C) = llx-Pc(x)ll· 
Computing the orthogonal projection operator might be a difficult task, but there are 
some examples of simple sets on which the orthogonal projection can be easily computed. 
Example 8.9 (projection on the nonnegative orthant). Let C =JR~. To compute the 
orthogonal projection of x E lRn onto JR~, we need to solve the convex optimization 
problem 
min I:7=1(Yi-xi)2 
s.t. 
Y1,y2, ... ,yn>O. 
(8.15) 
Since this problem is separable, meaning that the objective function is a sum of functions 
of each of the variables, and the constraints are separable in the sense that each of the 
variables has its own constraint, it follows that the i th component of the optimal solution 
y* of problem (8.15) is the optimal solution of the univariate problem 
min{(Yi -xi)2: Yi > O}, 
which is given by y; = [xi]+, where for a real number a E JR, [a]+ is the nonnegative part 
of a: 
[a]+ = { a, a> 0, 
0, a< 0. 
We will extend the definition of the nonnegative part to vectors, and the nonnegative part 
of a vector v E lRn is defined by 
[v]+ = ([v1]+,[v2]+, ... ,[vn]+)T. 
To summarize, the orthogonal projection operator onto lRn is given by 
Example 8.10 (projection on boxes). A box is a subset of lRn of the form 
B = [ l 1, u1] X [ l 2, u2] X "· X [ l n, Un]= {XE lRn : l i < Xi < u;}, 

158 
Chapter 8. Convex Optimization 
where l i < ui for all i = 1,2, ... , n. We will also allow some of the U; 's to be equal to oo 
and some of the l i's to be equal to -oo; in these cases we will assume that oo or -oo are 
not actually contained in the intervals. A similar separability argument as the one used 
in the previous example, shows that the orthogonal projection is given by 
y=P8 (x), 
where 
Yi= { 
U;, 
X; > U;, 
xi, f i <Xi < U;, 
l i, 
X; < f. i' 
for any i = 1, 2, ... , n. 
I 
Example 8.11 (projection onto balls). Let C = B[O, r] = {y: llYll < r }. The optimiza-
tion problem associated with the computation of Pc(x) is given by 
min{lly-xll2: llYll2 < r 2}. 
y 
(8.16) 
If llxll < r, then obviously y =xis the optimal solution of (8.16) since it corresponds to 
the optimal value 0. When llxll > r, then the optimal solution of (8.16) must belong to 
the boundary of the ball since otherwise, by Theorem 2.6, it would be a stationary point 
of the objective function, that is, 2(y-x) = 0, and hence y = x, which is impossible since 
x ~ C. We thus conclude that the problem in this case is equivalent to 
min{lly-xll2: llYll2 = r 2}, 
y 
which can be equivalently written as 
min{-2xT y+ r 2 + llx112: llYll2 = r 2}, 
y 
The optimal solution of the above problem is the same as the optimal solution of 
min{-2x7 y: llYll2 = r 2}. 
y 
By the Cauchy-Schwarz inequality, the objective function can be lower bounded by 
-2x7 y >-2llxllllYll = -2rllxll, 
and on the other hand, this lower bound is attained at y = r 11: 11 , and hence the orthogonal 
projection is given by 
{ x, 
llxll < r, 
I 
PB[O,r] = 
r 11:11' 
llxll > r. 
8.4. cvx 
CVX is a MATLAB-based modeling system for convex optimization. It was created by 
Michael Grant and Stephen Boyd [19]. This MATLAB package is in fact an interface 
to other convex optimization solvers such as SeDuMi and SDPT3. We will explore here 
some of the basic features of the software, but a more comprehensive and complete guide 
can be found at the CVX website (CVXr . com). The basic structure of a CVX program is 
as follows: 

8.4. cvx 
159 
cvx_begin 
{variables declaration} 
minimize({objective function}) or maximize{{objective function}) 
subject to 
{constraints} 
cvx_end 
Variables Declaration 
Atoms 
The variables are declared via the command variable or variables. Thus, for example, 
variable x{4); 
variable z; 
variable Y{2,3); 
declares three variables: 
• x, a column vector of length 4, 
• z, a scalar, 
• Y, a2 x 3 matrix. 
The same declaration can be written as 
variables x{4) z Y(2,3); 
CVX accepts only convex functions as objective and constraint functions. There are sev-
eral basic convex functions, called "atoms," which are embedded in CVX. Some of these 
atoms are given in the following table. 
function 
meamng 
attributes 
norm(x,p) 
\IL?=1 lxi IP(p > 1) 
convex 
square(x) 
x2 
convex 
sum_square(x) 
Ln 
2 
. 1 x. 
i= ' 
convex 
square_pos(x) 
[x]~ 
convex, nondecreasing 
sqrt(x) 
JX 
concave, nondecreasing 
inv_pos(x) 
~(x > 0) 
. 
. 
convex, nomncreasmg 
max(x) 
max{ x1' x2, .•. , xn} 
convex, nondecreasing 
quad_over_lin(x,y) 
M (y>O) 
y 
convex 
quad_form(x,P) 
xTPx(P >-0) 
convex 
In addition, CVX is aware that the function xP for an even integer pis a convex func-
tion and that affine functions are both convex and concave. 
Operations Preserving Convexity 
Atoms can be incorporated by several operations which preserve convexity: 
• addition, 
• multiplication by a nonnegative scalar, 

160 
Chapter 8. Convex Optimization 
• composition of a nondecreasing convex function with a convex function, 
• composition of a convex function with an affine transformation. 
CVX is also aware that minus a convex function is a concave function. The constraints 
that CVX is willing to accept are inequalities of the forms 
f (x)<=g(x) 
g(x)>=f (x) 
where f is convex and g is concave. Equality constraints must be affine, and the syntax 
is (hands are affine functions) 
h(x)==s(x) 
Note that the equality must be written in the format==. Otherwise, it will be interpreted 
as a substitution operation. 
Example 8.12. Suppose that we wish to solve the least squares problem 
minl1Ax-hll2, 
where 
We can find the solution of this least squares problem by the MATLAB commands 
>> A=[l,2;3,4;5,6];b=[7;8;9]; 
>> x=(A'*A)\(A'*b) 
x = 
-6.0000 
6.5000 
To solve this problem via CVX, we can use the function sum_square: 
cvx_begin 
variable x(2) 
minimize(sum_square(A*x-b)) 
cvx_end 
The obtained solution is as expected: 
>> x 
x = 
-6.0000 
6.5000 
We can also solve the problem by noting that 
llAx-bll2 = x7 A 7 Ax-2b7 Ax+ llhll2 

8.4. cvx 
and writing the following commands: 
cvx_begin 
variable x(2} 
minimize(quad_form(x,A'*A}-2*b'*A*X) 
cvx_end 
However, the following program is wrong and CVX will not accept it: 
cvx_begin 
variable x(2) 
minimize(norm(A*x-b)A2) 
cvx_end 
161 
The reason is that the objective function is written as a composition of the square function 
which is not nondecreasing with the function norm (Ax-b). Of course, we know that 
the image of I !Ax - hi I consists only of nonnegative values and that the square function 
is nondecreasing over that domain. However, CVX is not aware of that. If we insist on 
making such a decomposition we can use the function square__pos - the scalar function 
So(x) = max{x,0}2, which is convex and nondecreasing, and write the legitimate CVX 
program: 
cvx_begin 
variable x(2) 
minimize(square__pos(norm(A*X-b))} 
cvx_end 
It is also worth mentioning that since the problem of minimizing the norm is equivalent 
to the problem of minimizing the squared norm in the sense that both problems have the 
same optimal solution, the following CVX program will also find the optimal solution, 
but the optimal value will be the square root of the optimal value of the original problem: 
cvx_begin 
variable x(2} 
minimize(norm(A*x-b}} 
cvx_end 
I 
Example 8.13. Suppose that we wish to write a CVX code that solves the convex opti-
mization problem 
mm Jx;+xi+1+2max{x1,x2,0} 
x2 
s.t. 
lx1I + lx2l + x1 < 5 
2 
.l +x4 <10 
X2 
1 -
(8.17) 
x2> 1 
X1 >O. 
In order to write the above problem in CVX, it is important to understand the reason why 
J x; + x; + 1 is convex since writing sqrt ( x ( 1) A 2 +x ( 2 ) A 2+1) in CVX will result in 
an error message. Since the expression is written as a composition of an increasing concave 
function with a convex function, in general it does not result in a convex function. A valid 
reason why J x; + x; + 1 is convex is that it can be rewritten as ll(x1, x2, l)rll. That is, it 
is a composition of the norm function with an affine transformation. Correspondingly, 

the correct syntax in CVX will be norm { [ x; 1 ] ) . Overall, a CVX program that solves 
(8.17) is 
cvx_begin 
variable x{2) 
minimize(norm([x;1])+2*max(max(x(l),x(2)),0)) 
subject to 
norm(x,l)+quad_over_lin(x(l),x{2))<=5 
inv_pos(x(2))+x(l)A4<=10 
x(2)>=1 
x(l)>=O 
cvx_end 
I 
Example 8.14. Suppose that we wish to find the Chebyshev center of the 5 points 
(-1,3), (-3,10), (-1,0), (5,0), (-1,-5). 
Recall that the problem of finding the Chebyshev center of a set of points a1, a2, ••• ,am is 
given by (see Section 8.2.4) 
m111,c,r 
s.t. 
r 
I Ix - a; II < r, 
i = 1,2, ... ,m, 
and thus the following code will solve the problem: 
A=[-l,-3,-l,5,-1;3,10,0,0,-5]; 
cvx_begin 
variables x(2) r 
minimize(r) 
subject to 
for i=1:5 
norm(x-A{:,i))<=r 
end 
cvx_end 
This results in the optimal solution 
>> x 
x = 
-2.0002 
2.5000 
>> r 
r = 
7.5664 
To plot the 5 points along with the Chebyshev circle and center we can write 
plot(A(l, :),A(2, :), '*') 
hold on 
plot (x ( 1) , x ( 2) , 'd' ) 
t=0:0.001:2*pi; 
xx=x(l)+r*COs(t); 

8.4. cvx 
yy=x(2)+r*sin(t); 
plot(xx,yy) 
axis equal 
axis([-11,7,-6,11]) 
hold off 
The result can be seen in Figure 8.4. 
I 
10 
8 
6 
4 
2 
0 
- 2 
- 4 
<> 
- 6.___._~_.___._~...__,_~..___._~.__~ 
- 10 - 8 - 6 - 4 - 2 0 
2 
4 
6 
Figure 8.4. The Chebyshev center (diamond marker} of 5 points {in asterisks). 
163 
Example 8.15 (robust regression). Suppose that we are given 21 points in R2 generated 
by the MATLAB commands 
randn('seed' ,314); 
x=linspace(-5,5,20) '; 
y=2*x+l+randn(20,1); 
x=[x;5]; 
y=[y;-20]; 
plot (x, y I I* I) 
hold on 
The resulting plot can be seen in Figure 8.5. Note that the point (5,-20) is an outlier; it is 
far away from all the other points and does not seem to :fit into the almost-line structure 
of the other points. The least squares line, also called the regression line, can be found by 
the commands (see also Chapter 3) 
A=[x,ones(21,1)]; 
b=y; 
u=A\b; 
alpha=u(l);beta=u(2); 
plot([-6,6],alpha*[-6,6]+beta); 
hold off 
resulting in the line plotted in Figure 8.6. As can be clearly seen in Figure 8.6, the least 
squares line is very much affected by the single outlier point, which is a known drawback 
of the least squares approach. Another option is to replace the /2-based objective function 
llAx-hll~ with an /1-based objective function; that is, we can consider the optimization 
problem 
min 11Ax-hll1• 

164 
Chapter 8. Convex Optimization 
15 
10 
5 
. . 
0 
. . 
- 5 
• . 
- 10 
- 15 
- 20 
- 6 
- 4 
- 2 
0 
2 
4 
6 
Figure 8.5. 21 points in the plane. 1he point (5, -20) is an outlier. 
15 
10 
5 
0 
-5 
- 10 
-15 
- 20 
-6 
-4 
- 2 
0 
2 
4 
6 
Figure 8.6. 21 points in the plane along with their least squares (regression) line. 
This approach has the advantage that it is less sensitive to outliers since outliers are not 
as severely penalized as they are penalized in the least squares objective function. More 
specifically, in the least squares objective function, the distances to the line are squared, 
while in the 11-based function they are not. To find the line using CVX, we can run the 
commands 
plot(x,y, '*') 
hold on 
cvx_begin 
variable u_l1(2) 
rninirnize(norrn(A*u_ll-b,l)) 
cvx_end 
alpha_ll=u_ll(l); 
beta_ll=u_l1(2); 
plot([-6,6],alpha_ll*[-6,6]+beta_ll}; 
axis([-6,6,-21,15]) 
hold off 
and the corresponding plot is given in Figure 8.7. Note that the resulting line is insensitive 
to the outlier. This is why this line is also called the robust regression line. 
I 

8.4. cvx 
165 
15 
10 
5 
0 
-5 
-10 
-15 
-20 
-6 
-4 
-2 
0 
2 
4 
6 
Figure 8.7. 21 points in the plane along with their robust regression line. 
Example 8.16 (solution of a trust region subproblem). Consider the trust region sub-
problem (see Section 8.2.7) 
mm xi +xi +3x; +4x1x2 +6x1x3 +8x2x3 +x1 +2x2-x3 
s.t. 
xi +xi + x; < 1, 
which is the same as 
mm XT Ax+2bT X 
s.t. 
llxll2 < 1, 
where 
c 
2 ;). b=CJ 
A= 2 1 
3 4 
The problem is nonconvex since the matrix A is not positive definite: 
>> A=[l,2,3;2,l,4;3,4,3]; 
>> b=[0.5;1;-0.5]; 
>> eig(A) 
ans = 
-2.1683 
-0.8093 
7.9777 
It is therefore not possible to solve the problem directly using CVX. Instead, we will use 
the technique described in Section 8.2.7 to convert the problem into a convex problem, 
and then we will be able to solve the transformed problem via CVX. We begin by com-
puting the spectral decomposition of A, 
[U,D]=eig(A); 
and then compute the vectors d and f in the convex reformulation of the problem: 
f=U' *b; 
d=diag (D); 

166 
Chapter 8. Convex Optimization 
We can now use CVX to solve the equivalent problem (8.13): 
cvx_begin 
variable z(3) 
minimize(d'*Z-2*abs(f)'*sqrt(z)) 
subject to 
sum(z)<=l 
z>=O 
cvx_end 
The optimal solution is then computed by Yi= -sgn{f;)JZi and then x =Uy: 
>> y=-sign(f) .*Sqrt(z); 
>> x=U*Y 
x = 
-0.2300 
-0.7259 
0.6482 
I 
Exercises 
8.1. Consider the problem 
min f(x) 
(P) 
s.t. 
g(x) < 0 
xeX, 
where f and g are convex functions over Rn and X C Rn is a convex set. Suppose 
that x* is an optimal solution of (P) that satisfies g(x*) < 0. Show that x* is also an 
optimal solution of the problem 
min f(x) 
s.t. 
xeX. 
8.2. Let C = B[ Xo' r ], where Xo E Rn and r > 0 are given. Find a formula for the 
orthogonal projection operator Pc. 
8.3. Let f be a strictly convex function over Rm and let g be a convex function over 
Rn. Define the function 
h(x) = f (Ax)+ g(x), 
where A E Rmxn. Assume that x* and y* are optimal solutions of the uncon-
strained problem of minimizing h. Show that Ax*= Ay*. 
8.4. For each of the following optimization problems (a) show that it is convex, 
(b) write a CVX code that solves it, and (c) write down the optimal solution (by 
running CVX). 
(i) 
. 
mm 
s.t. 

Exercises 
(ii) 
(iii) 
(iv) 
(v) 
(vi) 
(vii) 
(viii) 
(tx) 
mm 
s.t. 
mm 
s.t. 
max x1 +x2 +x3 +x4 
s.t. 
(x1 -x2)2 +(x3 +2x4) 4 < 5 
x1 +2x2 +3x3 +4x4 < 6 
X1, Xi' X3, X4 > 0. 
167 
l2x1 +3x2 +x3l+x; +xi +xi+ J2x; +4x1x2+7x;+10x2 +6 
x2+t 
2 
2 
2 
7 +2x1 +5x2+10x3 +4x1x2 +2x1x3 +2x2x3 < 7 
2 
X1 >O 
X2>1. 
For this problem also show that the expression inside the square root is always 
nonnegative, i.e., 2x; + 4x1x2+7xi + 10x2 + 6 > 0 for all x1, x2. 
mm 
s.t. 
mm 
s.t. 
1 
+sx2 +4x2 +7x2 + xf+x,+1 
2x2+3x3 
1 
2 
3 
X2+X3 
max{ x1 +x2,xi} +(xi +4x1x2+Sxi+1)2 < 10 
X1, x2, x3 > 0.1. 
J2x; +3xi +x; +4x1x2 +7 +(xi +xi +xi+ 1)2 
(x, +x2)2 +XS < 7 
X3+l 
1 -
x; +xi +4xi +2x1x2 +2x1x3 +2x2x3 < 10 
X1,X2,x3 > 0. 
mm 
s.t. 
. 
mm 
s.t. 

168 
Chapter 8. Convex Optimization 
(x) 
min (x1 + x2 + x3) 8 + x; + x; + 3xi + 2x1x2 +2x2x3 + 2x1 x3 
s.t. 
(lx1-2x21+1)4 +; < 10, 
3 
2x1+2x2 +x3 <1, 
0 < x3 < 1. 
8.5. Suppose that we are given 40 points in the plane. Each of these points belongs to 
one of two classes. Specifically, there are 19 points of class 1 and 21 points of class 
2. The points are generated and plotted by the MATLAB commands 
rand('seed',314); 
x=rand(40,1); 
y=rand ( 4 0 , 1) ; 
class=[2*x<y+0.5]+1; 
Al=[x(find(class==l)),y(find(class==l))]; 
A2 = [x(find(class==2)),y(find(class==2))]; 
plot(Al(:,1),Al(:,2),'*','MarkerSize',6) 
hold on 
plot(A2(:,1),A2(: , 2), 'd','MarkerSize',6) 
hold off 
The plot of the points is given in Figure 8.8. Note that the rows of A1 E JR t9x 2 
are the 19 points of class 1 and the rows of A2 E1R21 x 2 are the 21 points of class 2. 
Write a CVX-based code for finding the maximum-margin line separating the two 
classes of points. 
'" • 
0.9 
• • 
0.8 
0.7 
0.6 
• • . 
0.5 ~ 
• 
0.4 
0 
0.3 
• 
• • 
0.2 
0.1 
• • 
00 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
Figure 8.8. 40 points of two classes: class 1 points are denoted by asterisks, and class 2 points 
are denoted by diamonds. 

Chapter 9 
Optimization over a 
Convex Set 
9.1 • Stationarity 
Throughout this chapter we will consider the constrained optimization problem (P) 
given by 
(P) 
mm f(x) 
s.t. 
XE C, 
where f is a continuously differentiable function and C is a closed and convex set. In 
Chapter 2 we discussed the notion of stationary points of continuously differentiable func-
tions, which are points in which the gradient vanishes. It was shown that stationarity is a 
necessary condition for a point to be an unconstrained local optimum point. The situa-
tion is more complicated when considering constrained problems of the form (P), where 
instead of looking at stationary points of a function, we need to consider the notion of 
stationary points of a problem. 
Definition 9.1 (stationary points of constrained problems). Let f be a continuously 
differentiable function O'Ver a closed convex set C. Ihen x* E C is called a stationary point 
of (P) if '\! f ( x*) T ( x - x*) > 0 for any x E C. 
Stationarity actually means that there are no feasible descent directions of f at x*. 
This suggests that stationarity is in fact a necessary condition for a local minimum of (P). 
Theorem 9 .2 (stationarity as a necessary optimality condition). Let f be a continuously 
differentiable function over a closed convex set C, and let x* be a local minimum of (P). Ihen 
x* is a stationary point of (P). 
Proof. Let x* be a local minimum of (P), and assume in contradiction that x* is not a 
stationary point of (P). Then there exists x EC such that Vf(x*)T(x-x*) < 0. There-
fore, f'(x*;d) < 0 where d = x- x*, and hence, by Lemma 4.2, it follows that there 
exists EE (0, 1) such that f(x* + td) < f (x*) for all t E (O,c ). Since C is convex we have 
that x* + td = ( 1 - t )x* + t x E C, leading to the conclusion that x* is not a local opti-
mum point of (P), in contradiction to the assumption that x* is a local minimum point 
of (P). 
0 
169 

170 
Chapter 9. Optimization over a Convex Set 
Example 9.3 (C =Rn). If C =Rn, then the stationary points of (P) are the points x* 
satisfying 
V'/(x*)T(x-x*) > 0 
(9.1) 
for all x E Rn. Plugging x = x* - V' /(x*) into (9.1), we obtain that -llV' /(x*)ll2 > 0, 
implying that 
V'/(x*) = 0. 
Therefore, it follows that the notion of a stationary point of a function and a stationary 
point of a minimization problem coincide when the problem is unconstrained. 
I 
Example 9.4 (C =IR~). Consider the optimization problem 
(Q) min /(x) 
s.t. 
X; > 0, 
i = 1, 2, ... , n, 
where f is a continuously differentiable function over IR~. A vector x* E IR~ is a station-
ary point of (Q) if and only if 
V' f (x*)T (x-x*) > 0 for all x > 0, 
which is the same as 
V' /(x*)T x-V'/ (x*)T x* > 0 for all x > 0. 
We will now use the following technical result: 
aT x + b > 0 for all x > 0 if and only if a> 0 and b > 0. 
Using this simple result, it follows that (9.2) holds if and only if 
V' /(x*) > 0 and V' /(x*)T x* < 0. 
Since V' /(x*) > 0 and x* > 0, we can conclude that (9.3) holds if and only if 
Bf 
V' /(x*) > 0 and x;-;-(x*) = 0, 
i = 1,2, ... , n. 
OX; 
We can compactly write the above condition as follows: 
Bf (x*) { = o, x; > o, 
I 
0 X; 
> 0, x; = 0. 
(9.2) 
(9.3) 
Example 9.5 (stationarity over the unit-sum set). Consider the optimization problem 
(R) 
min /(x) 
s.t. 
er x = 1, 
where f is a continuously differentiable function over Rn. The feasible set 
U = {x E Rn : eT x = 1} = {x E Rn : t X; = 1} 
t=1 

9 .1 . Stationarity 
is called the unit-sum set . A point x* E U is a stationary point of (R) if and only if 
(I) 
\7 f(x*)T (x-x*) > 0 for all x satisfying eT x = 1. 
171 
We will show that condition (I) is equivalent to the following simple and explicit condi-
uon: 
of 
of 
of 
(II) -(x*) = -(x*) = · · · = -(x*). 
OX1 
OX2 
oxn 
We begin by showing that (II) implies (I). Indeed, assume that x* E U satisfies (II). Then 
for any x E U one has 
n of 
\lf(x*l(x-x*) = L: ~(x*)(xi-x;> 
i=l o X; 
of 
( n 
n 
) 
of 
= ~(x*) L:xi-L:x; = ~(x*)(l-1)=0, 
O X1 
i=l 
i=l 
o X1 
and in particular \7 f (x*f (x- x*) > 0. We have thus shown that (I) is satisfied. To show 
the reverse direction, take x* E U that satisfies (I) and assume in contradiction that (II) 
does not hold. This means that there exist two different indices i f:. j such that 
Define the vector x E U as 
* 
xk' 
k <t { i,j}, 
x~-1, 
z 
k = i, 
xj + 1, k=j. 
Then 
of 
of 
\lf(x*)T(x-x*) = ~(x*)(xi -x;>+ ~(x*)(xi-x~) 
OX· 
OX· 
J 
z 
J 
of 
of 
= --(x*) + -(x*) 
OX· 
OX· 
z 
J 
<0, 
which is a contradiction to the assumption that (I) is satisfied. We thus conclude that (I) 
implies (II). 
I 
Example 9.6 (stationarity over the unit-ball). Consider the optimization problem 
(S) 
min f(x) 
s.t. 
llxll < 1, 
where f is a continuously differetiable function over B[O, 1]. A point x* E B[O, 1] is a 
stationary point of problem (S) if and only if 
\7 f (x*)T (x-x*) > 0 for all x satisfying llxll < 1, 
(9.4) 

172 
Chapter 9. Optimization over a Convex Set 
which is equivalent to 
min{V/(x*)T x-V/(x*)T x*: llxll < 1} > 0. 
We will now use the following fact: 
[A] for any aE Rn the optimal value of the problem min{aT x: llxll < 1} is-llall· 
Indeed, if a= 0, then obviously the optimal value is 0 = -llall· If a f= 0, then by the 
Cauchy-Schwarz inequality we have for any x E B(O, 1] 
so that 
min{ aT x : I lxl I < 1} > -1 lal I· 
On the other hand, the lower bound -llal I is attained at x = -
11:11, and hence fact [A] is 
established. 
Returing to the characterization of stationary points over the unit ball, by fact [A] it 
follows that (9.4) is the same as 
-V/(x*f x* > llV/(x*)ll· 
(9.5) 
However, we have by the Cauchy-Schwarz inequality that the following inequality holds: 
-V f (x*)T x* < llV f (x*)ll · llx*ll < llV /(x*)ll, 
and we conclude that x* is a stationry point of (S) if and only if 
llV/(x*)ll =-V/(x*)T x*. 
(9.6) 
The above condition is a simple and explicit characterization of the stationarity prop-
erty. We can, however, find a more informative characterization of stationarity by the 
following argument: Let x* be a point satisfying (9.6). We have two cases: 
• If V /(x*) = 0, then (9.6) holds automatically. 
• If V /(x*) f= 0, then llx*ll = 1 since otherwise, if llx*ll < 1, we have by (once again!) 
the Cauchy-Schwarz inequality that 
-V f(x*)T x* < llV f (x*)ll · llx*ll < llV /(x*)ll, 
which is a contradiction to (9.6). We therefore conclude that when V /(x*) f= 0, x* 
is a stationary point if and only if I Ix* 11 = 1 and 
-V f (x*f x* = llV /(x*)ll · llx*ll· 
(9.7) 
The latter equality is equivalent to saying that there exists A < 0 such that V f ( x*) = 
Ax*. To show this, note that if there exists such a A, then indeed 
-V /(x*)T x* = -Allx*ll2 J~o llAx*ll · llx*ll = llV /(x*)ll · llx*ll, 
so that (9.7) is satisfied. In the other direction, if (9.7) holds, then this means that 
the Cauchy-Schwarz inequality is satisfied as an equality, and hence, by Lemma 1.5, 
it follows that there exist A E R such that V f ( x*) = Ax*. The parameter A must be 

9.2. Stationarity in Convex Problems 
173 
nonpositive since otherwise the left-hand side of (9.7) would be negative, and the 
right-hand side would be positive, contradicting the equality in (9.7). 
In conclusion, x* is a stationary point of (S) if and only if either V' f (x*) = 0 or llx*ll = 1 
and there exists A < 0 such that V' f ( x*) = Ax*. 
I 
To summarize the four examples, we write explicitly each of the stationarity condi-
tions in the following table. 
feasible set 
explicit stationarity condition 
Rn 
V'f(x*) = 0 
Rn 
Jf (x'}{ = 0, 
x~ >0 
1 
+ 
Tx; 
>O 
x~=O 
-
, 
1 
{ x E Rn : eT x = 1} 
*f<x*) = ... = ;f(x*) 
1 
Xn 
B[O, 1] 
V' f (x*) = 0 or llx*ll = 1and3..l < 0: V' f(x*) = ..lx* 
9.2 • Stationarity in Convex Problems 
Stationarity is a necessary optimality condition for local optimality. However, when the 
objective function is additionally assumed to be convex, stationarity is a necessary and 
sufficient condition for optimality. 
Theorem 9.7. Let f be a continuously differentiable convex function over a closed and 
convex set C C Rn. Tben x* is a stationary point of 
min f(x) 
s.t. 
XE C 
(P) 
if and only if x* is an optimal solution of (P). 
Proof. If x* is an optimal solution of (P), then by Theorem 9 .2, it follows that x* is a 
stationary point of (P). To prove the sufficiency of the stationarity condition, assume 
that x* is a stationary point of (P), and let x E C. Then 
f(x) > f(x*)+ V'f(x*)T(x-x*) > f(x*), 
where the first inequality follows from the gradient inequality for convex functions (f he-
orem 7 .6) and the second inequality follows from the definition of a stationary point. We 
have this shown that x* is the global minimum point of (P), and the reverse direction is 
established. 
0 
9.3 • The Orthogonal Projection Revisited 
We can use the stationarity property in order to establish an important property of the 
orthogonal projection operator. This characterization will be called the second projection 
theorem. Geometrically it states that for a given closed and convex set C, x E Rn, and 
yE C, the angle between x-Pc(x) and y-Pc(x) is greater than or equal to 90 degrees. 
This phenomenon is illustrated in Figure 9.1. 

174 
Chapter 9. Optimization over a Convex Set 
-1 
- 2 
- 3 
~*- · . . 
~ 
.. 
x 
-4 
- 5 
-6 
- 7 
- 4 - 3 - 2 - 1 
0 
1 
2 
3 
4 
5 
Figure 9 .1. The orthogonal projection operator. 
Theorem 9.8 (second projection theorem). Let C be a closed convex set and let x E ffi.n. 
Then z = P c(x) if and only if 
(x-z)T(y-z) < OforanyyE C. 
(9.8) 
Proof. z = Pc(x) if and only if it is the optimal solution of the problem 
min g(y) = 
lly-xll2 
s.t. 
yE C. 
Therefore, by Theorem 9.7 it follows that z = Pc(x) if and only if 
V g(z)T (y-z) > 0 for ally EC, 
which is the same as (9 .8). 
0 
Another important property of the orthogonal projection operator is given in the 
following theorem, which also establishes the so-called nonexpansiveness property of Pc. 
Theorem 9. 9. Let C be a closed and convex set. Then 
1. for any v, w E ffi.n 
(9.9) 
2. (nonexpansiveness) for any v, w E ffi.n 
llPc(v)-Pc(w)ll < llv-wll· 
(9.10) 
Proof. Recall that by Theorem 9 .8 we have that for any x E ffi.n and y E C 
(x-Pc(x))T(y-Pc(~)) < 0. 
(9.11) 
Substituting x = v and y = Pc( w ), we have 
(9.12) 

9.4. The Gradient Projection Method 
Substituting x =wand y = Pc(v), we obtain 
(w-Pc(w)f (Pc(v)-Pc(w)) < 0. 
Adding the two inequalities (9.12) and (9.13) yields 
(Pc(w)-Pc(v)f (v-w+Pc(w)-Pc(v)) < 0, 
and hence, 
(Pc(v)-Pc(w))7(v-w)>11Pc(v)-Pc(w)ll2, 
showing the desired inequality (9.9). 
175 
(9.13) 
To prove (9.10), note that if Pc(v) = Pc(w), the inequality is trivial. If Pc(v) f:. Pc(w), 
then by the Cauchy-Schwarz inequality we have 
which combined with (9. 9) yields the inequality 
llPc{v)-Pc(w)ll2 < llPc(v)-Pc(w)ll · llv-wll· 
Dividing by llPc(v)-Pc(w)ll implies (9.10). 
D 
Coming back to stationarity, the next result describes an additional useful representa-
tion of stationarity in terms of the orthogonal projection operator. 
Theorem 9.10. Let f be a continuously differentiable function defined on the closed and 
convex set C, and let s > 0. Then x* is a stationary point of the problem 
(P) 
mm f(x) 
s.t. 
XE C 
if and only if 
x* = Pc(x* -s'\/f(x*)). 
(9.14) 
Proof. By the second projection theorem (Theorem 9.8), it follows that x* = Pc(x* -
s'\l f(x*)) if and only if 
(x* -s'\/f(x*)-x*)7 (x-x*) < 0 for any x EC. 
However, the latter relation is equivalent to 
'\! f(x*)7 (x-x*) > 0 for any x EC, 
namely to stationarity. 
D 
Note that condition (9.14) seems to depend on the parameters, but by its equivalence 
to stationarity, it is essentially independent of s. 
9.4 • The Gradient Projection Method 
The stationarity condition 
x* = Pc ( x* - s '\! f ( x*)) 
(9.15) 

176 
Chapter 9. Optimization over a Convex Set 
naturally motivates the following algorithm, called the gradient projection method, for solv-
ing problem (P). This algorithm can be seen as a fixed point method for solving the equa-
tion (9.15). 
The Gradient Projection Method 
Input: e > 0 - tolerance parameter. 
Initialization: Pick Xo E C arbitrarily. 
General step: For any k = 0, 1, 2, ... execute the following steps: 
(a) Pick a stepsize tk by a line search procedure. 
(b) Set xk+t = Pc(xk- tk V f(xk)). 
(c) If llxk-xk+tll < e, then STOP, and xk+t is the output. 
Obviously, in the unconstrained case, that is, when C = Rn, the gradient projection 
method is just the gradient method studied in Chapter 4. 
There are several strategies for choosing the stepsizes tk. We will consider two choices: 
• constant stepsize tk = i for all k. 
• backtracking. 
We will elaborate on the specific details of the backtracking procedure in what follows. 
9.4.1 • Sufficient Decrease and the Gradient Mapping 
To establish the convergence of the method, we will prove a sufficient decrease lemma for 
C 1• 1 functions, similarly to the sufficient decrease lemma proved for the gradient method 
(Lemma 4.23). 
Lemma 9.11 (sufficient decrease lemma for constrained problems). Suppose that f E 
Ci·1(C), where C is a closed convex set. Then for any x EC and t E (O,f) the following 
inequa/,ity holds: 
( Lt) 1 
2 
f(x)-f(Pc(x-tVf(x)))>t 1-T t(x-Pc(x-tVf(x))) . 
(9.16) 
Proof. For the sake of simplicity, we will make the notation x+ = Pc(x- tVf(x)). By 
the descent lemma (Lemma 4.22) we have that 
From the second projection theorem (Theorem 9 .8) we have 
(x-tV/(x)-x+,x-x+) < 0, 
from which it follows that 
(v f(x),x+ -x) < -~ llx+ -xll2, 
t 
(9.17) 

9.4. The Gradient Projection Method 
177 
which combined with (9 .17) yields 
Hence, taking into account the definition of x+, the desired result follows. 
0 
The result of Lemma 9 .11 is a generalization of the sufficient decrease property derived 
in the unconstrained setting in Lemma 4.23. In fact, when C =Rn the obtained inequality 
is exactly the same as the one obtained in Lemma 4.23. 
It is convenient to define the gradient mapping as 
where M > 0. We assume that the identities off and C are clear from the context. Note 
that in the unconstrained case GM(x) = Vf(x), so the gradient mapping is an extension 
of the usual gradient operation. In addition, by Theorem 9.10, GM(x) = 0 if and only if 
xis a stationary point of (P). This means that we can look at llGi(x)ll as an optimality 
measure. The result of Lemma 9.11 essentially states that 
f(x)-f(Pc(x-t'il f(x))) > t ( 1-L;) IJG1(x)ll2• 
(9.18) 
This generalized sufficient decrease property allows us to prove similar results to those 
proven in the unconstrained case. Before proceeding to the convergence analysis, we will 
prove an important monotonicity property of the norm of the gradient mapping GM(x) 
with respect to the parameter M. 
Lemma 9 .12. Let f be a continuously differentiable function defined on a closed and convex 
set C. Suppose that L1 > L2• Then 
(9.19) 
and 
(9.20) 
for any x E lRn. 
Proof. Recall that, by the second projection theorem, for any v e JRn and w e C the 
following inequality holds: 
(v-Pc(v),Pc(v)-w} > 0. 
Plugging v = x- J, V/(x) and w = Pc(x- J, Vf(x)) in the latter inequality it follows 
I 
2 
that 
( x-L 
Vf(x)-Pc ( x-L 
Vf(x)),Pc ( x-L 
Vf(x))-Pc ( x-L 
Vf(x))) > 0, 
or 
(
1 
1 
1 
1 
) 
-Gi (x)--Vf(x),-GL (x)--Gi (x) >O. 
L 
I 
L 
L 
2 
L 
I 
-
1 
1 
2 
1 

178 
Chapter 9. Optimization over a Convex Set 
Exchanging the roles of L1 and L2 yields the following inequality: 
( 1 
1 
1 
1 
) 
LGi2(x)- LV/(x), LGL1 (x)-LGr/x) > 0. 
2 
2 
1 
2 
Multiplying the first inequality by L1 and the second by L2 and adding them we obtain 
( Gr (x)-Gi (x), .2_.Gr (x)-.2_.Gr (x)) > 0, 
I 
2 
L2 
2 
L1 
I 
-
which, after some expansion of terms, can be seen to be the same as 
1 
1 
(1 
1) 
Li llGi1 (x)ll2 + L2 llGi2(x)l12 < Li +Li Gr1 (x/ Gi/x). 
Using the Cauchy-Schwarz inequality we obtain that 
1 
1 
(1 1) 
-llGr (x)ll2 + -llGr (x)ll2 < - + -
llGr (x)ll · llGi (x)ll· 
L 
1 
L 
2 
-r L 
I 
2 
1 
2 
1 
2 
(9.21) 
Note that if Gr2(x) = 0, then by the latter inequality, it follows that Gr1 (x) = 0, implying 
that in this case the inequalities (9.19) and (9.20) hold trivially. Assume then that Gr/x)-=J 
d 
fi 
llGLt (x)ll Th 
0, an de ne t = llGi2(x)ll' 
en by (9.21) 
.2_.t2 -(~ + ~) t + ~ < 0. 
L1 
L1 
L2 
L2 -
Since the roots of the quadratic function are t = 1, zi, we obtain that 
2 
showing that 
L 
l<t<-1 , 
-
- L 2 
9.4.2 •Backtracking 
Equipped with the definition of the gradient mapping, we are now able to define the back-
tracking stepsize selection strategy similarly to the definition of the backtracking proce-
dure for descent methods for unconstrained problems (see Chapter 4). The procedure 
requires three parameters (s,a,[3), wheres> O,a E (0, 1),(3E(0,1). The choice of tk is 
done as follows: First, tk is set to be equal to the initial guess s. Then while 
f(xk)-f(Pc(xk- tk V/(xk))) < atkllG.l.(xk)ll2 
t1e 
we set tk := f3 tk. In other words, the stepsize is chosen as tk = s [3i1e, where ik is the 
smallest nonnegative integer for which the condition 
f(xk)-f(Pc(xk-sf3i1e\lf(xk))) > asf3i1e llG-l....(xk)ll2 
sf3'1e 
is satisfied. 

9.4. The Gradient Projection Method 
179 
Note that the backtracking procedure is finite for C 1•1 functions. Indeed, if f E 
C£·1(C), then plugging x = xk into (9.18) we obtain 
Hence, since the inequality t < 2(•za> is the same as 1- L; >a, we conclude that for any 
2(1-a) h · 
l' 
t < -L- t e mequa 1ty 
holds, implying that the backtracking procedure ends when tk is smaller or equal to 2(•za>. 
We can also compute a lower bound on tk: either tk is equal to s or the backtrack-
ing procedure is invoked, meaning that the stepsize ~ did not satisfy the backtracking 
condition given by 
In particular, by the above discussion we can conclude that !p > 2(•za>, so that tk > 
2(t~a)/3. To summarize, in the backtracking procedure, the chosen stepsize tk satisfies 
. { 2(1-a)f3} 
tk >mm s, 
L 
. 
(9.22) 
9.4.3 • Convergence of the Gradient Projection Method 
We will analyze the convergence of the gradient projection method in the case when 
f E C£•1(C). We begin with the following lemma showing the sufficient decrease of 
consecutive function values of the sequence generated by the gradient projection method. 
Lemma 9.13. Consider the problem 
(P) 
min f(x) 
s.t. 
XE C, 
where C C Rn is a closed and convex set and f E C}_'1(C). Let {xkh~o be the sequence 
generated by the gradient projection method for solving problem (P) with either a constant 
stepsize tk = i E (0, f) or with a stepsize chosen by the backtracking procedure with parameters 
(s,a,{3) satisfying s > 0, a E (0, 1),(3E(0,1). Then for any k > 0 
where 
and 
M -{ i ( 1- iJ) 
constant stepsize, 
-
amin{s, 2(t~a)/3} backtracking 
d _ { 1 / i constant stepsize, 
-
1 / s backtracking. 
(9.23) 

180 
Chapter 9. Optimization over a Convex Set 
Proof. The result for the constant stepsize setting follows by plugging t = i and x = xk 
in (9.18). As for the backtracking procedure, by its definition we have 
The result now follows from the lower bound on tk given in (9 .22) and the fact that tk < s, 
which implies by the monotonocity property of the gradient mapping (Lemma 9 .12) that 
llG1/tk (xk)ll > llG1/s(xk)ll. 
D 
We are now ready to prove the convergence of the norm of the gradient mapping to 
zero. 
Theorem 9 .14 (convergence of the gradient projection method). Consider the problem 
(P) 
min f(x) 
s.t. 
XE C, 
where C is a closed and convex set and f E Ci•1(C) is bounded below. Let {xkh~o be the 
sequence generated by the gradient projection method for solving problem (P) with either a 
constant stepsize tk = i E (0, f) or a stepsize chosen by the backtracking procedure with pa-
rameters (s,a,[3) satisfying s > 0,a,[3E(0,1). Then we have the following: 
(a) The sequence {f(xk)} is nonincreasing. In addition, f(xk+i) < f(xk) unless xk is a 
stationary point of (P). 
(b) GJ(xk)--+ 0 ask--+ oo, where dis as defined in Lemma 9.13. 
Proof. (a) By Lemma 9.13 we have that 
(9.24) 
Since M > 0, it readily follows that f(xk) > f(xk+t) and equality holds if and only if 
GJ(xk) = 0, which is the same as saying that xk is a stationary point of (P). 
(b) Since the sequence {f(xk)}k>o is nonincreasing and bounded below, it converges. 
Thus, in particular f(xk)- f(xk+if--+ 0 ask--+ oo, which combined with (9.24) implies 
that llGJ(xk)ll--+ 0 ask--+ oo. 
D 
We can also get an estimate for the rate of convergence of the gradient projection 
method, but in the constrained case it will be in terms of the norm of the gradient mapping. 
Theorem 9.15 (rate of convergence of gradient mapping norms in the gradient pro-
jection method). Under the setting of Theorem 9 .14, let f * be the limit of the convergent 
sequence {f(xk)}k~o· Then for any n = 0, 1,2, ... 
where 
f(Xo)-f* 
M(n+l), 
{ i ( 1- iL) 
constant stepsize, 
M = 
. {2 2(1-a)/3} 
a mm s, 
L 
backtracking 

9.4. The Gradient Projection Method 
and 
d _ { 1 / i constant stepsize, 
-
1/ s backtracking. 
Proof. Sum the inequality (9.23) over k = O, 1, .•. , n and obtain 
n 
f(Xo)-f(xn+t) > M2:11GJ(xk)U2. 
k=O 
Since f (xn+t) > f*, we can thus write 
n 
f(Xo)-f* > MLllGJ(xk)U2. 
k=O 
Finally, using the latter inequality along with the obvious fact that 
it follows that 
implying the desired result. 
0 
9.4.4 • The Convex Case 
181 
When the objective function is assumed to be convex, the constrained problem (P) be-
comes convex, and stronger convergence results can be established. We will concentrate 
on the constant step size setting and show that the sequence { xk} k>o converges to an op-
timal solution of (P). In addition, we will establish a rate of convergence result for the 
sequence of function values {f(xk)}k~o to the optimal value. 
Theorem 9.16 (rate of convergence of the sequence of function values). Consider the 
problem 
(P) 
min f(x) 
s.t. 
xe C, 
where C is a closed and convex set and f E Ci•1(C) is convex over C. Let {xkh~o be the 
sequence generated by the gradient projection method for solving problem (P) with a constant 
stepsize tk = i E ( 0, ± ]. Assume that the set of optimal solutions, denoted by X*, is nonempty, 
and let f * be the optimal value of (P). Then 
(a) for any k > 0 and x* eX* 
2i(f(xk+1)-f(x*)) < llxk-x*ll2-llxk+t -x*ll2, 
(9.25) 
(b) for any n > 0: 
f(x )-f* < 11Xo-=x*ll2 
n 
2tn 
(9.26) 

182 
Chapter 9. Optimization over a Convex Set 
Proof. By the descent lemma we have 
Let x* be an optimal solution of (P). Then the gradient inequality implies that f(xk) < 
f(x*)+ {Vf(xk),xk-x*), which combined with (9.27) yields 
L 
f(xk+l) < f(x*) + {Vf(xk),xk-x*) +{Vf(xk),xk+t -xk)+ 2llxk-xk+tll2· (9.28) 
By the second projection theorem (f heorem 9 .8) we have that 
(xk-iVf(xk)-xk+Px* -xk+t) < 0, 
so that 
Therefore, combining (9 .28), (9 .29) and using the inequality i < ±, we obtain that 
L 
f(xk+1) < f(x*) + {Vf(xk),xk -x*) +{Vf(xk),xk+t -xk) + 2 llxk -xk+1ll2 
= f(x*) +{V f(xk),xk+1 -x*) + ~ llxk-xk+1ll2 
< f(x*) + I{xk -xk+t,xk+1 -x*) + ~ llxk-xk+1ll2 
< f(x*) + !{xk -xk+1'Xk+t -x*) + ~llxk -xk+1ll2 
t 
2t 
1 
= f(x*) +--= (llxk -x*ll2-llxk+1 -x*ll2)' 
2t 
establishing part (a). 
(9.29) 
To prove part {b), sum the inequalities {9.25) fork= 0, 1,2, ... , n -1 and obtain 
n-1 
llxn-x*ll2-11Xo-x*ll2 < 2i~(f(x*)-f(xk+i)) < 2in(f(x*)-f(xn)), 
k=O 
where we used in the last inequality the monotonicity of the functions values of the gen-
erated sequence. Hence, 
as required. 
D 
Note that when the constant stepsize is chosen as i = f, the result {9.26) then reads as 
The rate of convergence of 0(1/n) obtained in Theorem 9.16 is called a sublinear rate 
of convergence. We can also deduce the convergence of the sequence itself, and for that, 

9.5. Sparsity Constrained Problems 
183 
we note that by (9.25) the sequence satisfies a property called Fejer monotonicity, which 
we state explicitly in the following lemma. 
Lemma 9 .17 (Fejer monotonicity of the sequence generated by the gradient projec· 
tion method). Under the setting of Theorem 9 .16 the sequence { xk} k>o satisfies the following 
inequality for any x* E X* and k > 0: 
-
Proof. The proof follows by (9.25) and the fact that f(x*) < f (xk+i>· 
0 
We are now ready to prove the convergence of the sequence { xk h>o to an optimal 
solution. 
-
Theorem 9.18 (convergence of the sequence generated by the gradient projection 
method). Under the setting of Theorem 9 .16 the sequence { xk} k>o generated by the gradient 
projection method with a constant stepsize tk = i E ( 0, ±] conv~es to an optimal solution. 
Proof. The result follows from the Fejer monotonicity of the sequence. First of all, by part 
(b) of Theorem 9 .16 it follows that f ( xk) -+ f *, where f * is the optimal value of problem 
(P), and thus by the continuity off, any limit point of the sequence { xk} k>o is an optimal 
solution. In addition, by the Fejer monotonicity property, we have for any x* e X* that 
llxk-x*ll < llXo-x*ll, implying that the sequence {xkh>o is bounded, establishing the 
fact that the sequence does have at least one limit point.- To prove the convergence of 
the entire sequence, let i be a limit point of the sequence { xk h>o, meaning that there 
exists a subsequence {xk }1">0 such that xk -+ i. Since i EX*, It follows by the Fejer 
J 
-
J 
monotonicity that for any k > 0 
Thus, {llxk -illh>o is a nonincreasing sequence which is bounded below (by zero) and 
hence convergent. -Since I lxk - ii I -+ 0 as j -+ oo, it follows that the whole sequence 
J 
{llxk-illh~o converges to zero, and consequently xk-+ i ask-+ oo. 
0 
9.5 • Sparsity Constrained Problems 
So far we considered problems in which the feasible set C is closed and convex. In this 
section we will study a class of problems in which the constraint set is nonconvex. There-
fore, none of the results derived so far apply for this class of problems, and we will see that 
beside several similarities between the convex and nonconvex cases, there are substantial 
differences in the type of results that can be obtained. 
9.5.1 • Definition and Notation 
In this section we consider the problem of minimizing a continuously differentiable objec-
tive function subject to a sparsity constraint. More specifically, we consider the problem 
(S) 
min f(x) 
s.t. 
llxllo < s, 

184 
Chapter 9. Optimization over a Convex Set 
where f: Rn --. R is bounded below and a continuously differentiable function whose 
gradient is Lipschitz with constant L1, s > 0 is an integer smaller than n, and llxllo is the 
so-called l 0 norm of x, which counts the number of nonzero components in x: 
llxllo = l{i: xi~ O}I. 
It is important to note that the /0 norm is actually not a norm. Indeed, it does not even 
satisfy the homogeneity property since llAxllo = llxllo for A~ 0. We do not assume that f 
is a convex function. The constraint set is of course nonconvex and most of the analysis 
of this chapter does not hold for problem (S). Nevertheless, this type of problem is im-
portant and has many applications in areas such as compressed sensing, and it is therefore 
worthwhile to study it and understand how the "classical" results over convex sets can be 
adjusted in order to be valid for problem (S). First of all, we begin with some notation. 
For a given vector x E Rn, the support set of x is defined by 
and its complement by 
We denote by C5 the set of vectors x that are at most s-sparse, meaning that they have at 
most s nonzero elements: 
Cs = { x : I !xi lo < s} · 
In this notation problem (S) can be written as 
min{f(x): x E C5 }. 
For a vector x E Rn and i E {1,2, ... , n }, the ith largest absolute value component in xis 
denoted by Mi(x), so that in particular 
Also, M1(x) = maxi=l, ... ,n lxi I and Mn(x) = mini=l, ... ,n lxi I· 
Our objective now is to study necessary optimality conditions for problem (S), which 
are similar in nature to the stationarity property. As we will see, the nonconvexity of 
the constraint (S) prohibits the usual stationarity conditions to hold, but we will see that 
other optimality conditions can be constructed. 
9.5.2 • L-Stationarity 
We begin by extending the concept of stationarity to the case of problem (S). We will use 
the characterization of stationarity in terms of the orthogonal projection. Recall that x* 
is a stationary point of the problem of minimizing a continuously differentiable function 
over a closed and convex set C if and only if 
x* =Pc(x*-sV/(x*)), 
(9.30) 
where s is an arbitrary positive number. It is interesting to note (see Theorem 9.10) 
that condition (9.30)-although expressed in terms of the parameter s-does not actu-
ally depend on s. Relation (9.30) is no longer a valid condition when C = Cs since the 

9.5. Sparsity Constrained Problems 
185 
orthogonal projection is not unique on nonconvex sets and is in fact a multivalued map-
ping, meaning that its values form a set given by 
Pcs (x) = argmilly{lly-xll: y E Cs}· 
The existence of vectors in Pc ( x) follows by the closedness of Cs and the coercivity of the 
s 
function lly-xll (with respect toy). To extend (9.30) to the sparsity constrained problem 
(S), we introduce the notion of "L-stationarity.,, 
Definition 9.19. A vector x* E Cs is called an L-stationary point of (S) if it satisfies the 
relation 
[NCL] x' E Pc, ( x' - ~ 'i7 /(x')). 
(9.31) 
As already mentioned, the orthogonal projection operator Pc ( ·) is not single-valued. 
s 
In this case, the orthogonal projection Pc (x) comprises all vectors consisting of the s 
s 
components of x with the largest absolute values and with zeros elsewhere. In general, 
there can be more than one choice to the s largest components in absolute value, and each 
of these choices gives rise to another vector in the set Pc (x). For example 
s 
Below we will show that under our assumption that f e Ci•1(lRn), L-stationarity is a 
f 
necessary condition for optimality whenever L > L 1. 
Before proving this result, we describe a more explicit representation of [NCL]· 
Lemma 9.20. For any L > 0, x* E Cs satisfies [NCL] if and only if 
if i E /0(x*), 
if i E / 1 (x*). 
(9.32) 
Proof. [NCL] => (9.32). Suppose that x* satisfies [NCL]· Note that for any index j E 
{1,2, ... ,n }, the jth component of any vector in Pcs (x*-± V f(x*)) is either zero or equal 
to x~ -
-L1 Bf (x*). Now, since x* E Pc (x*- -L1 \lf(x*)), it follows that if i E / 1(x*), then 
J 
a;; 
s 
x~ = x~--L1 ~(x*), so that ~! (x*) = 0. If i E / 0(x*), then lx~--L1 81. (x*)I < Ms(x*), which 
z 
z 
ox, 
ox, 
z 
ax; 
combined with the fact that x; = 0 implies that l~(x*)I < LMs(x*), and consequently 
• 
(9.32) holds true. 
(9.32) => [NCL]· Suppose that x* satisfies (9.32). If llx*llo < s, then Ms(x*) = 0, and 
by (9.32) it follows that \1 f (x*) = O; therefore, in this case, Pcs (x* - ± V f (x*)) = Pcs (x*) 
is the set {x*}. If llx"'llo = s, then Ms(x*) ~ 0 and j/1(x*)I = s. By (9.32) 
* 
1 a f ( *) { = lx~I, 
i E / 1(x*), 
x --- x 
z 
i 
L Bxi 
<M5(x*), i E /0(x*). 
Therefore, the vector x* - -f V f(x*) contains the s components of x* with the largest 
absolute value and all other components are smaller or equal (in absolute value) to them, 
and consequently [NCL] holds. 
D 

186 
Chapter 9. Optimization over a Convex Set 
Evidently, the condition (9.32) depends on the constant L; the condition is more re-
strictive for smaller values of L. This shows that the state of affairs in the nonconvex case 
is different. Before proceeding to show under which conditions is £-stationarity a neces-
sary optimality condition, we will prove the following technical and useful result. The 
analysis depends heavily on the descent lemma (Lemma 4.22). We recall that the lemma 
states that if f e C£/(Rn) and L > L1, then 
where 
f(y) < hr(y,x), 
L 
hr(y,x) = 
f(x) + (Vf(x),y-x) + -llx-yl!2. 
2 
(9.33) 
Lemma 9.21. Suppose that f e C£/(Rn) and that L > L1. Then for any x e Cs andy E Rn 
satisfying 
we have 
L-L1 
f(x)-f(y)> 
2 
llx-yl!2. 
Proof. Note that (9.34) can be written as 
yeargmin,ec, z-(x- ~V/(x)) 
2 
Since 
L 
hr(z,x) = f(x) + {V f(x),z-x) + -llz-xll2 
2 
L 
( 
1 
) 
2 
1 
=- z- x--Vf(x) 
+f(x)--llV/(x)ll2, 
2 
L 
2L 
constant w.r.t. z 
it follows that the minimization problem (9.36) is equivalent to 
ye argminzec hr(z,x). 
s 
This implies that 
hr(y,x) < hr(x,x) = f(x). 
Now by the descent lemma we have 
f(x)-f(y) > f(x)-hr/y,x), 
which combined with (9.37) and the identity 
L-L1 
hr (x,y) = hr(x,y)-
llx-yll2 
f 
2 
yields (9.35). 
D 
(9.34) 
(9.35) 
(9.36) 
(9.37) 

9.5. Sparsity Constrained Problems 
187 
We are now ready to prove the necessity of the L-stationarity property under the 
condition L > Lr. 
Theorem 9.22 (L-stationarity as a necessary optimality condition). Suppose that f E 
Ci;\~n) and that L >Lr. Let x* be an optimal solution of (S). Then 
(i) x* is an L-stationary point, 
(ii) the set Pc, ( x* - f V f ( x*)) is a singleton. 3 
Proof. We will prove both parts simultaneously. Suppose to the contrary that there exists 
a vector 
yEPc, ( x'- ~ Vf(x')), 
(9.38) 
which is different from x* (y f:. x*). Invoking Lemma 9.21 with x = x*, we have 
L-Lr 
f(x*)-f(y) > 
2 
llx* -yll2, 
contradicting the optimality of x*. We conclude that x* is the only vector in the set 
Pc,(x*-f Vf(x*)). 
0 
To summarize, we have shown that under a Lipschitz condition on V f, L-stationarity 
for any L >Lr is a necessary optimality condition. 
9.5.3 • The Iterative Hard-Thresholding Method 
One approach for solving problem (S) is to employ the following natural generalization 
of the gradient projection algorithm. The method can be seen as a fixed point method 
aimed at "enforcing" the L-stationary condition (9.31): 
(9.39) 
The method is known in the literature as the iterative hard-thresholding (IH1) method, and 
we will adopt this terminology. 
The IHT Method 
Input: a constant L > LI. 
• Initialization: Choose Xo E Cs. 
•General step: xk+1 E Pc, ( xk - t V f (xk)), k = 0, 1,2, .... 
It can be shown that the general step of the IHT method is equivalent to the relation 
(9.40) 
where hr(x,y) is defined by (9.33). (See also the proof of Lemma 9.21.) 
3 A set is called a singleton if it contains exactly one element. 

188 
Chapter 9. Optimization over a Convex Set 
Several basic properties of the IHT method are summarized in the following lemma. 
Lemma 9.23. Suppose that f E Ci/CRn) and that f is lower bounded. Let {xkh~o be the 
sequence generated by the !HT method with a constant stepsize f, where L > L 1. Then 
(a) f(xk)- f(xk+1) > L~Lt llxk-xk+1112, 
(b) {f ( xk) h~o is a nonincreasing sequence, 
(c) llxk-xk+111-+ 0, 
(d) for every k = 0, 1,2, ... , if xk ;j:. xk+1, then f (xk+1) < f (xk). 
Proof. Part (a) follows by substituting x = xk ,y = xk+l into (9.35). Part (b) follows 
immediately from part (a). To prove (c), note that since {f(xk)}k>o is a nonincreasing 
sequence, which is also lower bounded (by the fact that f is bounded below), it follows 
that it converges to some limit l and hence f (xk)- f(xk+l) -+ l - l = 0 ask -+ oo. 
Therefore, by part (a) and the fact that L > L1, the limit llxk -xk+111-+ 0 holds. Finally, 
(d) is a direct consequence of (a). 
0 
As already mentioned, the IHT algorithm can be viewed as a fixed point method for 
solving the condition for L-stationarity. The following theorem states that all accumu-
lation points of the sequence generated by the IHT method with constant stepsize f are 
indeed L-stationary points. 
Theorem 9.24. Let {xkh~o be the sequence generated by the /HT method with stepsize f, 
where L > L f. Then any accumula.tion point of { xk} k~o is an L-stationary point. 
Proof. Suppose that x* is an accumulation point of the sequence. Then there exists a 
subsequence {xkn ln~o that converges to x*. By Lemma 9.23 
L-L1 
f(xkn)-f(xkn+l) > 
2 
llxkn -~n+1112. 
(9.41) 
Since {f(xkn )}n>O and {f(xkn+1)}n>o' as nonincreasing and lower bounded sequences, 
both converge to the same limit, it follows that f (xkn )-f (x_k,,+1)-+ 0 as n -+ oo, which 
combined with (9 .41) yields that 
~,,+l-+ x* as n-+ oo. 
Recall that for all n > 0 
,.R.+t EPc, ( ,.R. - ~ Vf(,.k·)). 
Let i E / 1(x*). By the convergence of x_k,, and .x_kn+1 to x*, it follows that there exists an N 
such that 
and therefore, for n > N, 

Exercises 
Taking n to oo and using the continuity off, we obtain that 
() f 
* 
~(x )=0. 
OX· 
i 
189 
Now let i E /0(x*). If there exist an infinite number of indices kn for which x~n+i f:. 
0, then as in the previous case, we obtain that x~n+l = x~n -
~(.,('ln) for these indices, 
i 
i 
o~ 
implying (by taking the limit) that 4f-(x*) = 0. In particular, l~(x*)I < LMs(x*). On 
o~ 
o~ 
the other hand, if there exists an M > 0 such that for all n > M x~n + 1 = 0, then 
i 
Thus, taking n to infinity, while exploiting the continuity of the function Ms, we ob-
tain that 
;:. (x') <LM,(x'), 
i 
and hence, by Lemma 9 .20, the desired result is established. 
0 
Exercises 
9 .1. Let f be a continuously differentiable convex function over a closed and convex 
set C C R_n. Show that x* E C is an optimal solution of the problem 
(P) 
min{f(x): x EC} 
if and only if 
("V/(x),x*-x) < 0 for all x EC. 
9.2. Consider the Huber function 
H (x)= 
2µ' 
x <µ, 
{ 
11x112 
II II 
µ 
llxll -
~ else, 
whereµ> 0 is a given parameter. Show that Hµ E c1·1• 
µ 
9 .3. Consider the minimization problem 
(Q) 
min 
2x~ + 3xi + 4x; + 2x1 x2 - 2x1 x3 - 8x1 -4x2 - 2x3 
S.t. 
x1, X2, X3 > 0. 
(i) Show that the vector ( 1;, 0, ~) T is an optimal solution of (Q). 
(ii) Employ the gradient projection method via MATLAB with constant stepsize 
t (L being the Lipschitz constant of the gradient of the objective function). 
Show the function values of the first 100 iterations and the produced solution. 

190 
Chapter 9. Optimization over a Convex Set 
9.4. Consider the minimization problem 
where f is a continuously differentiable function over Rn and a ER~+. Show that 
x* satisfying a T x* = 1 is a stationary point of (P) if and only if 
a1 (x*) 
a1 (x*) 
af (x*) 
~ 
ax; 
ax;; 
---= 
=···=---
9.5. Consider the minimization problem 
(P) 
min{f(x): x E An}, 
where f is a continuously differentiable function over An. Prove that x* E An is a 
stationary point of (P) if and only if there exists µ E R such that 
ar (x*){ =µ, x; >0, 
axi 
> µ, x; =0. 
9.6. Let SC Rn be a closed and convex set, and let f E Ci•1(S) be a convex function 
over S. Assume that the optimal value of the problem 
(P) 
min{f (x): x ES}, 
dented by f * is finite. Prove that for any x E S the following inequality holds: 
1 
f(x)-f* > lL llGr(x)l!2, 
where Gr(x) = L[x-P5(x-f Vf(x))]. 
9 .7. Let f be a strongly convex function over a closed convex set S C Rn with strong 
convexity parameter u, that is, 
u 
f(y) > f(x) + (V f(x),y-x} + 2llx-yl!2 for all x,y ES. 
Assume in addition that f E Ci·1(S). Consider the problem 
(P) 
min f(x) 
s.t. 
xES, 
where S is a closed and convex set. Let { xk} k>o be the sequence generated by the 
gradient projection method for solving problem (P) with a constant stepsize tk = 
±. Let x* be the optimal solution of (P), and let f * be the optimal value of of (P). 
Prove that there exists a constant c E (0, 1) such that for any k > 0 
Find an explicit expression for c. 

Chapter 10 
Optimality Conditions for 
Linearly Constrained 
Problems 
In the previous chapter we discussed the notion of stationarity, which is a necessary opti-
mality condition for problems with differentiable objective functions and closed convex 
feasible sets. One of the main drawbacks of this concept is that for most feasible sets, it 
is rather difficult to validate whether this condition is satisfied or not, and it is even more 
difficult to use it in order to actually solve the underlying optimization problem. Our 
main objective in this chapter is to derive an equivalent optimality condition that is much 
easier to handle. We will establish the so-called KKT conditions for the special case of 
linearly constrained problems. 
10.1 • Separation and Alternative Theorems 
We begin with a very simple yet powerful result on convex sets, namely the separation 
theorem between a point and a closed convex set. This result will be the basis for all the 
optimality conditions that will be discussed later on. Given a set S C Rn, a hyperplane 
H = {x E Rn : a7 x = b} (a E JRn\{0}, b E JR) is said to strictly separate a pointy</. S 
from S if 
and 
a7 x < b for all x ES. 
An illustration of a separation between a point and a closed and convex set can be seen 
in Figure 10.1. Our next result shows that a point can always be strictly separated from a 
closed convex set, as long as it does not belong to it. 
Theorem 10.1 (strict separation theorem). Let CC Rn be a closed and convex set, and 
let y </. C. Then there exist p E Rn\ { 0} and a E JR such that 
pTy>a 
and 
p7 x < a for all x E C. 
Proof. By the second projection theorem, the vector i = Pc(Y) EC satisfies 
(y-if (x-i) < 0 for all x EC, 
191 

192 
Chapter 10. Optimality Conditions for Linearly Constrained Problems 
{x : aT x=b} 
Figure 10.1. Strict separation of point from a closed and convex set. 
which is the same as 
(y-i)7 x < (y-i)7i for all XE c. 
Denote p = y-i f:. 0 (since y fl. C) and a= (y-i)7 i. Then we have that p7 x <a for all 
x E C. On the other hand, 
and the result is established. 
D 
As was already mentioned, the latter separation theorem is extremely important since 
it is the basis for all optimality conditions. We begin by using it in order to prove an al· 
ternative theorem, which is known in the literature as Farkas' lemma. We refer to it as an 
alternative theorem since it essentially states that exactly one of two systems ("alterna-
tives") is feasible. 
Lemma 10.2 (Farkas' lemma). Let c E Rn and A E Rmxn. Then exactly one of the 
following systems has a solution: 
I. Ax < 0, c7 x > 0. 
II. A 7 y= c,y> 0. 
Before proceeding to the proof of the lemma, let us begin with an illustration. For 
that, consider the following example: 
Stating that system I is infeasible means that the system Ax < 0 implies the inequality 
c7 x < 0. Thus, the relevant question is whether the inequality -xi+ 9.xi < 0 holds 
whenever the two inequalities 
Xi +5x2 < 0, 
-Xi +2x2 <O 
are satisfied. The answer to this question is affirmative. Indeed, we can see the implication 
by noting that adding twice the second inequality to the first inequality yields the desired 
inequality-xi + 9 x2 < 0. Thus, the argument for showing the implication is that the row 

10.1. Separation and Alternative Theorems 
193 
vector cT can be written as a conic combination of the rows of A, or in other words, that 
c is a conic combination of the columns of AT: 
or 
(!)+{;1)==(~1) 
G 
-;l)G)==(~l). 
~ 
~ 
AT 
c 
The interesting question is whether it is always correct that a system of linear inequali-
ties ("base system") implies another linear inequality ("new inequality") if and only if the 
new inequality can be written as a conic combination of the inequalities in the base sys-
tem. The answer to this question, according to Farkas' lemma, is yes! We will actually 
prefer to state Farkas' lemma in the spirit of this discussion. This can be done since an al-
ternative theorem stating that exactly one of two statements A and B is true is equivalent 
to a result stating that B is equivalent to the denial of A. 
Lemma 10.3 (Farkas' lemma, second formulation). Let c e lRn and A e lRmxn. Then 
the following two claims are equivalent: 
A. The implication Ax < 0 => cT x < 0 holds true. 
B. There exists ye JR~ such that AT y = c. 
Proof. Suppose that system B is feasible, meaning that there exists y E JR~ such that 
AT y = c. To see that the implication A holds, su~pose that Ax < 0 for some x E lRn. 
Then multiplying this inequality from the left by y (a valid operation since y > 0) yields 
yTAx<O. 
Finally, using the fact that cT = yT A, we obtain the desired inequality 
cTx<O. 
The reverse direction is much less obvious. Suppose that the implication A is satisfied, and 
let us show that system B is feasible. Suppose in contradiction that system B is infeasible, 
and consider the following closed and convex set: 
S={xelRn :x=ATyforsomeyelR~}. 
The closedness of the above set follows from Lemma 6.32. The infeasibility of B means 
that c fl. S. By Theorem 10.1, it follows that there exists a vector p E 1Rn\{O} and a E 1R 
such that pT c > a and 
(10.1} 
Since 0 ES, we can conclude that a> 0, and hence also that pT c > 0. In addition, (10.1} 
is equivalent to 

194 
Chapter 10. Optimality Conditions for Linearly Constrained Problems 
or to 
(Ap l y < a for ally> 0, 
(10.2) 
which implies that Ap < 0. Indeed, if there was an index i E { 1, 2, ... , m} such that 
[Ap]i > 0, then for y = {3e;, we would have (Ap)7 y = {3[Ap]i, which is an expression 
that goes to oo as f3 -+ oo. Taking a large enough f3 will contradict (10.2). We have 
thus arrived at a contradiction to the assumption that the implication A holds (using the 
vector p ), and consequently B is satisfied. 
0 
The next alternative theorem, called "Gordan's theorem," is heavily based on Farkas' 
lemma. 
Theorem 10.4 (Gordan's alternative theorem). Let A E Rmxn. Then exactly one of the 
following two systems has a solution: 
A. Ax< 0. 
B. prf O,A7 p=O,p>O. 
Proof. Suppose that system A has a solution. We will prove that system B is infeasible. 
Assume in contradiction that B is feasible, meaning that there exists p rf 0 satisfying 
AT p = 0, p > 0. Multiplying the equality AT p = 0 from the left by xT yields 
(Ax)T p = 0, 
which is impossible since Ax< 0 and 0 rf p > 0. 
Now suppose that system A does not have a solution. Note that system A is equivalent 
to (s is a scalar) 
The latter system can be rewritten as 
Ax+se<O, 
s >0. 
x(;) < o, 
where A = (A e) and c = en+t · The infeasibility of A is thus equivalent to the infeasi-
bility of the system 
Aw<O, 
cTw>O, 
wERn+t. 
By Farkas' lemma, there exists z ER~ such that 
(~;)z=c; 
that is, there exists z E R ~ such that 
AT z= 0, 
Since eT z = 1, it follows in particular that z rf 0, and we have thus shown the existence 
of 0 rf z E R ~ such that AT z = O; that is, that system B is feasible. 
0 

10.2. The KKT conditions 
195 
10.2 • The KKT conditions 
We will now show how Gordan's alternative theorem can be used to establish a very 
useful optimality criterion that is in fact a special case of the so-called Karush-Kuhn-
Tucker (abbreviated KKT) conditions, which will be discussed later on in Chapter 11. 
The new optimality condition follows from the stationarity condition already discussed 
in Chapter 9. 
Theorem 10.5 (KKT conditions for linearly constrained problems; necessary opti-
mality conditions). Consider the minimization problem 
min /(x) 
s.t. 
af x < b;, 
i = 1,2, ... ,m, 
(P) 
where f is continuously differentiable over Rn, a1,a2, ••• ,am E Rn, b1, b2, ••• , bm E JR., and 
let x* be a local minimum point of (P). Then there exist A1, A2, ••• , Am > 0 such that 
m 
V/(x*)+ LAiai = 0 
(10.3) 
i=1 
and 
A;(a[x*-b;)=O, 
i = 1,2, ... ,m. 
(10.4) 
Proof. Since x* is a local minimum point of (P), it follows by Theorem 9.2 that x* is a 
stationary point, meaning that V /(x*)T (x-x*) > 0 for every x E Rn satisfying af x < b; 
for any i = 1,2, ... , m. Let us denote the set of active constraints by 
/(x*) = {i: af x* = b;}. 
Making the change of variables y = x -x*, we obtain that V f (x*)T y > 0 for any y E Rn 
satisfying af (y + x*) < b; for any i = 1, 2, ... , m, that is, for any y E Rn satisfying 
a!y<O, 
'f <b 
T* 
a; y -
i -ai x ' 
i E/(x*), 
i ~/(x*). 
We will show that in fact the second set of inequalities in the latter system can be re-
moved, that is, that the following implication is valid: 
a[ y < 0 for all i E /(x*) => V f (x*)T y > 0. 
Suppose then that y satisfies af y < 0 for all i e /(x*). Since bi -af x* > 0 for all i fl. /(x*), 
it follows that there exists a small enough a > 0 for which4 af ( ay) < bi - af x*. Thus, 
since in addition af (ay) < 0 for any i E /(x*) it follows by the stationarity condition that 
V/(x*f (ay) > 0, and hence that V/(x*)T y> 0. We have thus shown that 
a[ y < 0 for all i E/(x*) => V/(x*f y> 0. 
Thus, by Farkas' lemma it follows that there exist A;> 0, i E /(x*), such that 
-V/(x*) = L A;a;. 
iE/(x*) 
4Indeed, if af y $ 0 for all i ~ J(x*), then we can take a= 1. Otherwise, denote]= {i ~ J(x*): af y > O} 
b 
T • 
dtak 
. 
,-a,x 
an 
e a = nunieJ 
7 
• 
a; y 

196 
Chapter 10. Optimality Conditions for Linearly Constrained Problems 
Defining Ai = 0 for all i <t, /(x*), we get that .A.i(af x* - bi)= 0 for all i = 1,2, ... , m and 
that 
m 
Vf(x*)+ L:A;ai = 0 
i=l 
as required. 
D 
The KKT conditions are necessary optimality conditions, but when the objective 
function is convex, they are both necessary and sufficient global optimality conditions. 
Theorem 10.6 (KKT conditions for convex linearly constrained problems; necessary 
and sufficient optimality conditions). Consider the minimization problem 
(P) 
min f(x) 
s.t. 
af x < bi, i = 1,2, ... , m, 
where f is a convex continuously differentiable function over Rn, a1, a2, ... , am E Rn, 
b1, b2, ••• , b m E R, and let x* be a feasible solution of (P). Then x* is an optimal solution 
of {P) if and only if there exist Ai, A2, ••• , .A.m > 0 such that 
m 
V f (x*) + L: .A.iai = 0 
(10.5) 
i=i 
and 
Ai(a[x*-b;)=O, i=l,2, ... ,m. 
(10.6) 
Proof. If x* is an optimal solution of (P), then by Theorem 10.5 there exist .A.1, A2, ••• , Am > 
0 such that (10.5) and (10.6) are satisfied. To prove the sufficiency, suppose that x* is a 
feasible solution of (P) satisfying (10.5) and (10.6). Let x be any feasible solution of (P). 
Define the function 
m 
h(x) = f(x)+ L:Ai(a[ x-b;)· 
i=i 
Then by {10.5) it follows that V h(x*) = 0, and since h is convex, it follows by Proposition 
7.8 that x* is a minimizer of hover Rn, which combined with (10.6) implies that 
m 
m 
f(x*) = f(x*) + L: A;(af x* - b;) = h(x*) < h(x) = f(x) + L: A;(af x-bi) < f(x), 
i=i 
i=l 
where the last inequality follows from the fact that Ai > 0 and af x - b; < 0 for i = 
1,2, ... , m. We have thus proven that x* is a global optimal solution of (P). 
D 
The scalars Ai, ... , .A.m that appear in the KKT conditions are also called Lagrange mul-
tipliers, and each of the multipliers is associated with a corresponding constraint: A; is the 
multiplier associated with the i th constraint af x < b;. Note that the multipliers asso-
ciated with inequality constraints are nonnegative. The conditions (10.6) are known in 
the literature as the complementary slackness conditions. We can also generalize Theorems 
10.5 and 10.6 to the case where linear equality constraints are also present. The main dif-
ference is that the multipliers associated with equality constraints are not restricted to be 
nonnegative. The proof of the variant that also incorporates equality constraints is based 
on the simple observation that a linear equality constraint aT x = b can be written as two 
inequality constraints, aT x < b and-aT x <-b. 

10.2. The KKT conditions 
197 
Theorem 10.7 (KKT conditions for linearly constrained problems). Consider the min-
imization problem 
min f(x) 
(Q) 
s.t. 
afx<bi, i=l,2, ... ,m, 
cjx=dj, j=l,2, ... ,p, 
where f is a continuously differentiable function ()'()er Rn, ai, ai, .. ., am, c., Ci, ... , cp E JRn, 
b1, h2, ... , bm, d1, di, ... , dp E lR. Then we have the following: 
(a) (necessity of the KKT conditions) If x* is a local minimum point of (Q), then there 
exist Ai, Ai, ... , Am > 0 and µ 1, µi, ... , µ P E JR such that 
m 
p 
Vf(x*)+ LAiai + Lµjcj = 0 
(10.7) 
i=i 
j=i 
and 
A;(af x-h;)=O, 
i = 1,2, ... ,m. 
(10.8) 
(b) (sufficiency in the convex case) If in addition f is convex ()'()er JR n and x* is a feasible 
solution of (Q) for which there exist Ai, Ai, ... , Am > 0andµ 1, µ 2, ••• , µ P E JR such that 
(10.7) and (10.8) are satisfied, then x* is an optimal solution of (Q). 
Proof. (a). Consider the equivalent problem 
mm f(x) 
(Q') 
s.t. 
a; x < b;, 
cj x < dj, 
-c!x<-d-
1 
-
J' 
i = 1,2, ... , m, 
j = 1,2, ... ,p, 
j=l,2, ... ,p. 
Then since x* is an optimal solution of (Q), it is also an optimal solution of ( Q'), and 
thus, by Theorem 10.5, it follows that there exist multipliers A1, Ai, ... , Am > 0 and 
+ -
+ -
+ ->o 
h h 
µ 1 , µ 1 , µ 2 , µ 2 , • .. , µ P , µ P _ 
sue t at 
m 
P 
P 
Vf(x*)+ LA;ai + L:µjcj- Lµjcj =O 
i=1 
j=1 
j=l 
and 
Ai(aj x-bi) = 0, 
i = 1,2, ... ,m, 
µj(cfx-dj)=O, j=l,2, ... ,p, 
µj(-cjx+dj)=O, j=l,2, ... ,p. 
(10.9) 
(10.10) 
(10.11) 
(10.12) 
We thus obtain that (10.7) and (10.8) are satisfied with µ1- = µ-+: - µ-: ,j = 1, 2, ... , p. 
J 
J 
(b) To prove the second part, suppose that x* satisfies (10.7) and (10.8). Then it also 
satisfies (10.9), (10.10), (10.11), and (10.12) with 
µj = [µj]+,µj = [µj]- =-min{µj,O}, 
which by Theorem 10.6 implies that x* is an optimal solution of (Q') and thus also an 
optimal solution of (Q). 
D 

198 
Chapter 1 o. Optimality Conditions for Linearly Constrained Problems 
We note that a feasible point x* is called a KKT point if there exist multipliers for 
which (10.7) and (10.8) are satisfied. 
A very popular representation of the KKT conditions is via the Lagrangian function, 
which we will present in the general setting of general nonlinear programming problems: 
mm f(x) 
(NLP) 
s.t. 
g;(x) < 0, 
i = 1,2, ... , m, 
hj(x)=O, j=l,2, ... ,p. 
Here f, g1, g2, ••• , gm, h1, h2, ••• , h P are all continuously differentiable functions over Rn. 
The associated Lagrangian function takes the form 
m 
p 
L(x,A.,µ)=f(x)+ LA;g;(x)+ Lµjhj(x). 
i=l 
j=l 
In the linearly constrained case of problem (Q), the condition (10.7) is the same as 
m 
p 
VxL(x,A.,µ) = 'Vf(x)+ LA;Vg;(x)+ Lµj Vhj(x) = 0. 
i=l 
j=1 
Back to problem (Q), if in addition we define the matrices A and C and the vectors b 
andd by 
aT 
1 
CT 
1 
b1 
di 
aT 
CT 
b2 
d2 
A= 
2 
C= 
2 
h= 
d= 
' 
' 
' 
aT 
m 
CT 
p 
bm 
dp 
then the constraints of problem (Q) can be written as 
Ax<b, 
Cx=d. 
The Lagrangian can be also written as 
L(x,A.,µ) = f(x)+ AT(Ax-b)+ µT(Cx-d), 
and condition (10.7) takes the form 
Example 10.8. Consider the problem 
min !(x2+x2+x2) 
2 
1 
2 
3 
S.t. 
X1 + x2 + X3 = 3. 
Since the problem is convex, the KKT conditions are necessary and sufficient. The La-
grangian of the problem is 

10.2. The KKT conditions 
The KKT conditions are (we also incorporate feasibility within the KKT system) 
BL 
-=x1+µ=0, 
ax1 
8L 
-a =x2+µ=0, 
X2 
BL 
-a =x3+µ=0, 
X3 
x1 +x2 +x3 = 3. 
199 
By the first three equalities we obtain that x1 = x2 = x3 = -µ. Substituting this in the 
last equation yieldsµ= -1, and we obtained that the unique solution of the KKT system 
is x1 = x2 = x3 = 1, µ = -1. Hence, the unique optimal solution of the problem is 
(x1,x2,x3) = (1, 1, 1). 
I 
Example 10.9. Consider the problem 
min x; + 2x~ + 4x1 x2 
s.t. 
x1 +x2 =1, 
XpX2 > 0. 
The problem is nonconvex, since the matrix associated with the quadratic objective func-
tion A=(!~) is indefinite. However, the KKT conditions are still necessary optimality 
conditions. The Lagrangian of the problem is 
L(x1, x2, µ,Ai, J.2) = x:+2x~ +4x1 x2+ µ(xi +x2-1)-AiXi -A2x2, 
J.1, ..l2 E IR+,µ E IR. 
The KKT conditions are 
8L 
-8 
=2x1 +4x2 +µ-J. 1 =O, 
X1 
BL 
-8 
=4x2 +4x1 +µ-J.2 =0, 
X2 
We will split the analysis into 4 cases. 
A1X1 = o, 
A2X2 = 0, 
x1 +x2 =1, 
x1'x2 > O, 
J.1, J.2 > o. 
• Ai = J.2 = 0. In this case we obtain the three equations 
2x1 + 4x2 + µ = 0, 
4x2 + 4x1 + µ = 0, 
X1+x2 =1, 
whose solution is x1 =O,x2 =1,µ = -4. We thus obtain that (x1,x2) = (0, 1) is a 
KKT point. 

200 
Chapter 10. Optimality Conditions for Linearly Constrained Problems 
• ...l1, ...l2 > 0. In this case, by the complementary slackness conditions we obtain that 
x1 = x2 = 0, which contradicts the constraint x1 + x2 = 1. 
• ...l1 > 0, ...l2 = 0. In this case, by the complementary slackness conditions we have 
x1 = 0 and consequently x2 = 1, which was already shown to be a KKT point. 
• ...l1 = 0, ...l2 > 0. Here x2 = 0 and x1 = 1. The first two equations in the KKT system 
reduce to 
2+µ =0, 
4+ µ-A.2 = 0. 
The solution of this system isµ= -2,A.2 = 2. We thus obtain that (1,0) is also a 
KKT point. 
To summarize, there are two KKT points: (0, 1) and (1,0). Since the problem consists 
of minimizing a continuous function over a compact set it follows by the Weierstrass 
theorem (Theorem 2.30) that it has a global optimal solution. The KKT conditions are 
necessary optimality conditions, and hence the optimal solution is either (0, 1) or (1,0). 
Since the respective objective function values are 2 and 1 respectively, it follows that (1,0) 
is the global optimal solution of the problem. 
I 
Example 10.10 (orthogonal projection onto an affine space). Let C be the affine space 
C '{xERn:Ax=h}, 
where A E Rmxn and b E Rm. We assume that the rows of A are linearly independent. 
Given y E Rn, the optimization problem associated with the problem of finding Pc(Y) is 
min llx-yll2 
s.t. 
Ax=h. 
This is a convex optimization problem, so the KKT conditions are necessary and suffi-
cient. The Lagrangian function is 
Therefore, the KKT conditions are 
2x-2(y-A7 .A)= 0, 
Ax=h. 
The first equation can be written as 
Substituting this expression for x in the second equation yields the equation 
which is the same as 
AA7 A=Ay-b. 
(10.13) 

10.2. The KKT conditions 
201 
Thus, 
where here we used the fact that AA T is nonsingular since the rows of A are linearly 
independent. Plugging the latter expression for .A into (10.13), we obtain that 
Pc(Y) =y-AT(AAT)-1(Ay-b). 
Note that the projection onto an affine space is by itself an affine transformation. 
I 
In the last example we multiplied the Lagrange multiplier in the Lagrangian function 
by 2. This was done to simplify the computations, and it is always allowed to multiply 
the Lagrange multipliers by a positive constant. 
Example 10.11 (orthogonal projection onto hyperplanes). Consider the hyperplane 
H={xERn :aTx=b}. 
where 0 f:. a E Rn and b ER. Since a hyperplane is a spacial case of an affine space, we 
can use the formula obtained in the last example in order to derive an explicit expression 
for the projection onto H: 
As a consequence of the latter example, we can write a result providing an explicit 
expression for the distance between a point and a hyperplane. (fhe result was already 
stated and not proved in Lemma 8.6.) 
Lemma 10.12 (distance of a point from a hyperplane). Let H = {x E Rn : aT x = b }, 
where 0 f:. a E Rn and b E JR. Then 
laTy-bl 
d(y,H) = 
llall 
. 
Proof. 
( 
aTy-b ) 
d(y,H) = lly-PH(Y)ll = y- y-
llall2 a 
Example 10.13 (orthogonal projection onto half-spaces). Let 
H-={xERn :aTx< b}, 
where 0 f:. a E Rn and b E R. The corresponding optimization problem is 
mlllx 
llx -yll2 
s.t. 
aTx<b. 
The Lagrangian of the problem is 
L(x, A)= llx-yll2 + 2A(aT x-b ), 
A> 0, 
D 

202 
Chapter 10. Optimality Conditions for Linearly Constrained Problems 
and the KKT conditions are 
2(x-y) +2Aa = 0, 
A(aT x-b) = 0, 
aT X < b, 
A>O. 
If A= 0, then x = y and the KKT conditions are satisfied when a7 y < b. That is, when 
a7 y < b, the optimal solution is x = y, which is not a surprise since for any closed and 
convex set C, P c(Y) = y whenever y E C. Now assume that A > O; then by the comple-
mentary slackness condition we have that 
(10.14) 
Plugging the first equation x = y-Aa into (10.14), we obtain that 
aT (y-Aa) = b, 
ary-b 
T 
so that A = ~. The multiplier A is indeed positive when a y > b. We have thus 
obtained that when aT y > b, the optimal solution is 
a7y-b 
x=y-
2 a. 
llall 
To summarize, 
{ y, 
a7y< b, 
PH(y) = 
ary--/ 
T 
b 
y-
llall a, a Y > · 
The latter expression can also be compactly written as 
An illustration of the orthogonal projection onto a hyperplane can be found in Figure 
10.2. 
I 
Example 10.14. Consider the optimization problem 
min{x7 Qx + 2c7 x: Ax= b}, 
where Q E Rnxn is a positive definite matrix, c E Rn, b E Rm, and A is an m x n matrix 
with linearly independent rows. The Lagrangian of the problem is 
and the KKT conditions are 
VxL(x,).) = 2[ Qx+c+AT ).] = 0, 
Ax=b. 

10.3. Orthogonal Regression 
1r-~~-.-~~~..---~~-.-~~---,r-~~---.-~~----.,, 
0.8 
0.6 
0.4 
0.2 
0 
- 0.2 
- 0.4 
- 0.6 
- 0.8 
y 
H={ x: a1x s b} 
- 1"-~~-'-~~---''--~~-'-~~~'--~~-'-~~--' 
- 2 
- 1.5 
- 1 
- 0.5 
0 
0.5 
Figure 10.2. A vector y and its orthogonal projection onto a half space. 
The first equation implies that 
Plugging this expression of x into the feasibility constraint we obtain 
so that 
203 
(10.15) 
).=-(AQ-1A7 )-1(b+AQ-1c). 
(10.16) 
The optimal solution of the problem is given by (10.15) with A. as in (10.16). 
I 
10.3 • Orthogonal Regression 
An interesting application to the formula for the distance between a point and a hyper-
plane given in Lemma 10.12 is in the orthogonal regression problem, which we now recall. 
Consider the points at, ... , am in JRn. For a given 0 '/; x E JRn and y E JR, we define the 
hyperplane 
Hx,y := {a E JRn : x7 a= y} . 
In the orthogonal regression problem we seek to find a nonzero vector x E JRn and y E JR 
such that the sum of squared Euclidean distances between the points at, .• • , am to Hx,y is 
minimal; that is, the problem is given by 
(10.17) 
An illustration of the solution to the orthogonal regression problem is given in Figure 
10.3. 
The optimal solution of the orthogonal regression problem is described in the next 
result whose proof strongly relies on the formula of the distance between a point and a 
hyperplane. 

204 
Chapter 1 o. Optimality Conditions for Linearly Constrained Problems 
3 
2.8 
2.6 
2.4 
2.2 
2 
1.8 
1.6 
1.4 
1.2 
1 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
Figure 10.3. A two-dimensional example: given 5 points a1, ••• ,as in the plane, the orthog-
onal regression problem seeks to find the line for which the sum of squared norms of the dashed lines is 
minimal. 
Proposition 10.15. Letap ... ,am ERn andletAbethematrixgivenby 
aT 
1 
A= 
aT 
2 
aT 
m 
Then an optimal solution of problem (10.17) is given by x that is an eigenvector of the matrix 
AT (Im - ! eeT)A associated with the minimum eigenvalue and y = ! L?:t a[ x. Here e is 
them-length vector of ones. The optimal function value of problem (10.17) is Amin[ AT (Im -
!eeT)A]. 
Proof. By Lemma 10.12, the squared Euclidean distance between the point ai to Hx,y is 
given by 
2 - (af x-y)2 
d(ai,Hx,y) -
llx112 
, i = 1, ... ,m. 
It follows that (10.17) is the same as 
• { m ( af x-y )2 
n 
} 
mm L: 
2 
: Of xER ,yER . 
i=t 
llxll 
(10.18) 
Fixing x and minimizing first with respect toy we obtain that the optimal y is given by 
1 ~ 
T 
1 T 
y = -.L..iai x= -e Ax. 
m i=t 
m 
Using the latter expression for y we obtain that 
1;(af x-y)' = it(a;x- ~ e
7 Ax r 
m 
2 
m 
1 
= L:<af x)2- -L:(eT Ax)(af x)+-(eT Ax)2 
i=t 
m i=l 
m 
m 
1 
1 
= L:<af x)2- -(eT Ax)2=11Axll2- -(eT Ax)2 
i=t 
m 
m 
= x7 A 7 (1m - ~ ee7 )Ax. 

Exercises 
205 
Therefore, we arrive at the following reformulation of (10.17) as a problem consisting of 
minimizing a Rayleigh quotient: 
. { xT[AT(lm- !eeT)A]x 
} 
mm 
2 
:x~O . 
x 
llxll 
Therefore, by Lemma 1.12 an optimal solution of the problem is an eigenvector of the 
matrix AT (Im - ! eeT )A corresponding to the minimum eigenvalue; the optimal function 
value is the minimum eigenvalue A.min[AT(Im - !eeT)A]. 
D 
Exercises 
10.1. Show that the dual cone of 
M={xeRn:Ax>O} (AERmxn) 
lS 
M* ={AT v :vER~}. 
10.2. (nonhomogenous Farkas' lemma) Let A E Rmxn, c E Rn, b E Rm, and d E JR.. 
Suppose that there exists y > 0 such that AT y = c. Prove that exactly one of the 
following two systems is feasible: 
A. Ax<b,cT x>d. 
B. Ary= c,bry< d,y> 0. 
10.3. Let A E ]Rmxn and c E ]Rn. Show that exactly one of the following two systems is 
feasible: 
A. Ax> 0, x > 0, er x > 0. 
B. AT y > c, y < 0. 
10.4. Prove Motzkin's theorem of the alternative: the system 
Ad < 
0, 
Bd < 0 
(I) 
has a solution if and only if the system 
ATu+BTy=O, 
u,y>O, 
u~O 
(II) 
does not have a solution (here A E Rmxn ,BE JR.kxn). 
10.5. Prove the following nonhomogenous version of Gordan's alternative theorem: 
Given A E Rmxn, exactly one of these two systems is feasible. 
A. Az<b. 
B. ATy=O,bTy<O,y>O,y~O. 
10.6. Consider the maximization problem 
max x; +2x1x2 +2xi-3x1 +x2 
s.t. 
x1+x2 =1 
X1,X2 > 0. 

206 
Chapter 10. Optimality Conditions for Linearly Constrained Problems 
(i) Is the problem convex? 
(ii) Find all the KKT points of the problem. 
(iii) Find the optimal solution of the problem. 
10.7. Consider the problem 
mm -X1X2X3 
s.t. 
x1 + 3x2 + 6x3 < 48, 
x1, x2, x3 > 0. 
(i) Write the KKT conditions for the problem. 
(ii) Find the optimal solution of the problem. 
10.8. Consider the problem 
mm x:+2xi+x1 
s.t. 
x1 +x2 <a, 
where a E lR is a parameter. 
(i) Prove that for any a E JR, the problem has a unique optimal solution (without 
actually solving it). 
(ii) Solve the problem (the solution will be in terms of the parameter a). 
(iii) Let f(a) be the optimal value of the problem with parameter a. Write an 
explicit expression for f and prove that it is a convex function. 
10.9. Consider the problem 
. 
2 
2 
2 
2 
6 
mm 
xl + x2 + X3 + X1 X2 + X2X3 -
X1 -4X2 -
X3 
s.t. 
x1 +x2 +x3 < 1. 
(i) Is the problem convex? 
(ii) Find all the KKT points of the problem. 
(iii) Find the optimal solution of the problem. 
10.10. Consider the problem 
min x2 +x2 +x2 
1 
2 
3 
s.t. 
x1 + 2x2 + 3x3 > 4 
X3<1. 
(i) Write down the KKT conditions. 
(ii) Without solving the KKT system, prove that the problem has a unique opti-
mal solution and that this solution satisfies the KKT conditions. 
(iii) Find the optimal solution of the problem using the KKT system. 
10.11. Use the KKT conditions in order to solve the problem 
mm 
s.t. 
x2+x2 
1 
2 
-2X1-X2+10 < 0 
Xi> 0. 

Chapter 11 
The KKT Conditions 
In this chapter we will further develop the KKT conditions discussed in Chapter 10, but 
we will consider general constraints and not restrict ourselves to linear constraints. The 
Karush-Kuhn-Tucker (K.KT) conditions were originally named after Harold Kuhn and 
Albert Tucker, who first published the conditions in 1951. Later on it was discovered that 
William Karush developed the necessary conditions in his master's thesis back in 1939, 
and the conditions were thus named after the three researchers. 
11.1 • Inequality Constrained Problems 
We will begin our exploration into the KKT conditions by analyzing the inequality con-
strained problem 
(P) 
min f(x) 
s.t. 
g;(x) < 0, 
i = 1,2, ... , m, 
(11.1) 
where f, g1, ••• , gm are continuously differentiable functions over Rn. Our first task is to 
develop necessary optimality conditions. For that, we will define the concept of a feasible 
descent direction. 
Definition 11.1 (feasible descent directions). Consider the problem 
min h(x) 
s.t. 
XE C, 
where h is continuously differentiable over the closed and convex set C C Rn. Then a vector 
d f:. 0 is called a feasible descent direction at x E C if V h(x? d < 0, and there exists e > 0 
such thatx+ td EC for all t E [O,e ]. 
Obviously, a necessary local optimality condition of a point x is that it does not have 
any feasible descent directions. 
Lemma 11.2. Consider the problem 
(G) 
mm h(x) 
s.t. 
XE C. 
If x* is a local optimal solution of (G), then there are no feasible descent directions at x*. 
207 

208 
Chapter 11. The KKT Conditions 
Proof. The proof is by contradiction. If there is a feasible descent direction, that is, a 
vector d and e- 1 > 0 such that x* + td EC for all t E [O,e-1] and Vf(x*)T d < 0, then by 
the definition of the directional derivative (see also Lemma 4.2) there is an e-2 < e- 1 such 
that f (x* + td) < f (x*) for all t E [O, e-2], which is a contradiction to the local optimality 
of x*. 
D 
We can now write a necessary condition for local optimality in the form of an infea-
sibility of a set of strict linear inequalities. Before doing that, we mention the following 
terminology: Given a set of inequalities 
i = 1,2, ... ,m, 
where gi : Rn -+JR are functions, and a vector i E Rn, the active constraints at i are the 
constraints satisfied as equalities at i. The set of active constraints is denoted by 
/(i) = {i : g;(i) = O}. 
Lemma 11.3. Let x* be a local minimum of the problem 
min f(x) 
s.t. 
gi(x) < 0, 
i = 1,2, ... ,m, 
where f, g1, ••• , gm are continuously differentiable functions over Rn. Let /(x*) be the set of 
active constraints at x*: 
/(x*) = {i: g;(x*) = O}. 
Then there does not exist a vector d E Rn such that 
Vf(x*)T d < 0, 
V gi(x*)T d < 0, 
i E /(x*). 
(11.2) 
Proof. Suppose by contradiction that d satisfies the system of inequalities (11.2). Then 
by Lemma 4.2, it follows that there exists e- 1 > 0 such that f(x* + td) < f(x*) and 
gi(x* + td) < gi(x*) = 0 for any t E (0,e-1) and i E /(x*). For any i <J. /(x*) we have that 
gi(x*) < 0, and hence, by the continuity of g; for all i, it follows that there exists e-2 > 0 
such that gi(x* + td) < 0 for any t E (O,e-2) and i ~ /(x*). We can thus conclude that 
f (x* + td) < f (x*), 
gi(x*+td)<O, i=l,2, ... ,m, 
for all t E ( O, min { e-1, e-2} ), which is a contradiction to the local optimality of x*. 
D 
We have thus shown that a necessary optimality condition for local optimality is 
the infeasibility of a certain system of strict inequalities. We can now invoke Gordan's 
theorem of the alternative (fheorem 10.4) in order to obtain the so-called Fritz-John 
conditions. 
Theorem 11.4 (Fritz-John conditions for inequality constrained problems). Let x* be 
a local minimum of the problem 
mm f(x) 
s.t. 
g;(x)<O, 
i=l,2, ... ,m, 

11.1. Inequality Constrained Problems 
209 
where f, g1, ••• , gm are continuously differentiable functions over Rn. Then there exist mul-
tipliers .Ao, .A1, ••• , ).m > 0, which are not all zeros, such that 
m 
Ao V f (x*) +~A; V g;(x*) = 0, 
(11.3) 
i=l 
).ig;(x*) = 0, 
i = 1,2, ... , m. 
Proof. By Lemma 11.3 it follows that the following system of inequalities does not have 
a solution: 
(S) 
V f (x*)T d < 0, 
V gi(x*)T d < 0, 
i E /(x*), 
where /(x*) = { i : g;(x*) = O} = { i1, i2, ••• , ik }. System (S) can be rewritten as 
Ad<O, 
where 
Vg;/x*)T 
(11.4) 
By Gordan's theorem of the alternative (Theorem 10.4), system (S) is infeasible if and only 
if there exists a vector 'f) = (.Ao, A;1 , ••• , A;k )T f:. 0 such that 
which is the same as 
..iloVf(x*)+ L: .A;Vgi(x*)=O, 
iE/(x*) 
A; > 0, 
i E /(x*). 
Define A; = 0 for any i ~ /(x*), and we obtain that 
m 
Ao V f (x*) + L: A; V g;(x*) = 0 
i=l 
and that ).i g; (x*) = 0 for any i E { 1,2, ... , m} as required. 
D 
A major drawback of the Fritz-John conditions is in the fact that they allow Ao to be 
zero. The case Ao = 0 is not particularly informative since condition {11.3) then becomes 
m L: .A; V g;(x*) = o, 
i=l 
which means that the gradients of the active constraints {Vg;(x*)heI(x*) are linearly de-
pendent. This condition has nothing to do with the objective function, implying that 
there might be a lot of points satisfying the Fritz-John conditions which are not local 
minimum points. If we add an assumption that the gradients of the active constraints are 

210 
Chapter 11. The KKT Conditions 
linearly independent at x*, then we can establish the KKT conditions, which are the same 
as the Fritz-John conditions with Ao= 1. 
Theorem 11.5 (KKT conditions for inequality constrained problems). Letx* bea local 
minimum of the problem 
min f(x) 
s.t. 
gi(x)<O, i=l,2, ... ,m, 
where f, g1, ••• , gm are continuously differentiable functions over Rn. Let 
/(x*) = { i: g;(x*) = O} 
be the set of active constraints. Suppose that the gradients of the active constraints {V gi ( x*)} ie/(x• 
are linearly independent. Then there exist multipliers ...l.1, ...l.2, ••• , Am > 0 such that 
m 
V f(x*) +LA; V gi(x*) = 0, 
i=1 
...ligi(x*)=O, 
i = 1,2, ... ,m. 
(11.5) 
(11.6) 
Proof. By the Fritz-John conditions it follows that there exist 10 , 11, ••• , lm > 0, not all 
zeros, such that 
m 
~ Vf(x*) + L:li Vg;(x*) = 0, 
(11.7) 
i=l 
l;g;(x*)=O, i=l,2, ... ,m. 
(11.8) 
We have that~ f; 0 since otherwise, if lo= 0, by (11.7) and (11.8) it follows that 
L: livgi(x*)=O, 
ieI(r) 
where not all the scalars li, i E J(x*) are zeros, leading to a contradiction to the basic 
assumption that {V g;(x*)Lei(r) are linearly independent, and hence~ > 0. Defining 
A;= t' the result directly follows from (11.7) and (11.8). 
0 
The condition that the gradients of the active constraints are linearly independent 
is one of many types of assumptions that are referred to in the literature as "constraint 
qualifications." 
11.2 • Inequality and Equality Constrained Problems 
By using the implicit function theory, it is possible to generalize the KK.T conditions for 
problems involving also equality constraints. We will state this generalization without a 
proof. 
Theorem 11.6 (KKT conditions for inequality/equality constrained problems). Let 
x* be a local minimum of the problem 
mm f(x) 
s.t. 
gi(x) < 0, 
i = 1,2, ... , m, 
(11.9) 
h;(x)=O, j=l,2, ... ,p. 

11.2. Inequality and Equality Constrained Problems 
211 
where f, g1, ••• ,gm,h1, h2, ••• ,hp are continuously differentiable functions over Rn. Suppose 
that the gradients of the active constraints and the equality constraints 
{V' g;(x*): i E /(x*)} U {V' hj(x*): j = 1,2, ... , p} 
are linearly independent (where as before /(x*) = { i : gi(x*) = O}}. Then there exist multipli-
ers A.1, A.2 ••• , A.m > 0 and µ 1, µ2, ••• , µPE IR such that 
m 
P 
V' f (x*) + L: A.i V' gi(x*) + L: µ j V' hj(x*) = 0, 
i=t 
j=t 
A; gi(x*) = 0, 
i = 1,2, ... , m. 
We will now add to our terminology two concepts: KKT points and regularity. The 
first was already discussed in the previous chapter in the context of linearly constrained 
problems and is now extended. 
Definition 11.7 (KKT points). 
Consider the minimization problem {11.9), where 
f, g1, ••• , gm,h1,h2, ••• ,hp are continuously differentiable functions over Rn. A feasible point 
x* is called a KKT point if there exist A.1, A.2 ••• , Am > 0 and µ 1, µ2, ••• , µ P E IR such that 
m 
p 
V' f(x*) + L: A.i V' gi(x*) + L: µj V' h/x*) = 0, 
i=l 
j=l 
A;gi(x*)=O, 
i=1,2, ... ,m. 
Definition 11.8 (regularity). 
Consider the minimization problem {11. 9), where 
f, g1, ••• ,gm, h1, h2, ••• ,hp are continuously differentiable functions over Rn. A feasible point 
x* is called regular if the gradients of the active constraints among the inequality constraints 
and of the equality constraints 
{V' g; (x*): i E /(x*)} U {V' hj(x*): j = 1,2, ... , p} 
are linearly independent. 
In the terminology of the above definitions, Theorem 11.6 states that a necessary op-
timality condition for local optimality of a regular point is that it is a KKT point. The 
additional requirement of regularity is not required in the linearly constrained case in 
which no such assumption is needed; see Theorem 10.7. 
Example 11.9. Consider the problem 
. 
mm x1 +x2 
s.t. 
x: +x~ = 1. 
Note that this is not a convex optimization problem due to the fact that the equality 
constraint is nonlinear. In addition, since the problem consists of minimizing a contin-
uous function over a nonempty compact set, it follows that the minimizer exists (by the 
Weierstrass theorem, Theorem 2.30). 
Let us first address the issue of whether the KKT conditions are necessary for this prob-
lem. Since by Theorem 11.6 we know that the KKT conditions are necessary optimality 

212 
Chapter 11. The KKT Conditions 
conditions for regular points, we will find the irregular points of the problem. These are 
exactly the points x* for which the set of gradients of active constraints, which is given 
here by { 2( :~ ) } , is a dependent set of vectors; this can happen only when x; = x; = 0, 
2 
that is, at a nonfeasible point. The conclusion is that the problem does not have irregular 
points, and hence the KKT conditions are necessary optimality conditions. 
In order to write the KKT conditions, we will form the Lagrangian: 
L(x., x2, ..l) = x1 + x2 + ..l(x; + xi-1). 
The KKT conditions are 
x;+xi = 1. 
By the first two conditions, A -:/:- 0, and hence x1 = x2 = -fi.. Plugging this expression of 
x1, Xi into the last equation yields 
(-~)2 +(-~)2 =1 
2..l 
2..l 
, 
so that A=± }i· The problem thus has two KKT points: (}i, }i) and (-}i,-}i). 
Since an optimal solution does exist, and the KKT conditions are necessary for this prob-
lem, it follows that at least one of these two points is optimal, and obviously the point 
(-}i, - }i) which has the smaller objective value (-../2) is the optimal solution. 
I 
Example 11.10. Consider a problem which is equivalent to the one given in the previous 
example: 
mm x1 +x2 
s.t. 
(xi+xi-1)2=0. 
Writing the KKT conditions yields 
1 +4..lx1(x; +xi-1) = 0, 
1 +4..lx2(x; +xi-1) = 0, 
( x; +xi- 1 )2 = 0. 
This system is of course infeasible since the combination of the first and third equations 
gives the impossible equation 1 = 0. It is not a surprise that the KKT conditions are not 
satisfied here, since all the feasible points are in fact irregular. Indeed, the gradient of the 
• 
• (4x,(x2+x2-t)) 
h' h . 
h. 
£ 
£ 'bl 
• 
I 
constraint 1s 4 ( ~+ ~-t) , w 1c 1s t e zeros vector ror any eas1 e pomt. 
X2 XI 
X2 
Example 11.11. Consider the optimization problem 
mm 2x1 +3x2-x3 
s.t. 
xi+xi+xi = 1, 
xi +2xi +2x; = 2. 

11.3. The Convex Case 
213 
The problem does have an optimal solution since it consists of minimizing a continuous 
function over a nonempty and compact set. The KK.T system is 
2+2(A+ µ)xi= o, 
3 +2(A+2µ)x2 = o, 
-1+2(A+2µ)x3 =0, 
x: +xi +xi= 1, 
x: +2xi +2xi = 2. 
Obviously, by the first three equations A+µ f:. 0, A+ 2µ f:. 0, and in addition 
3 
x -
2 -
2(A+2µ)' 
1 
Denoting ti= l~µ' t2 = 2(1~2µ)' we obtain that xi =-tpx2 =-3t2,x3 = t2• Substituting 
these expressions into the constraints we obtain that 
t; + 10tf = 1, 
t;+20tf =2. 
implying that t; = 0, which is impossible. We obtain that there are no KKT points. 
Thus, since as was already observed, an optimal solution does exist, it follows that it is an 
irregular point. To find the irregular points, note that the gradients of the constraints are 
given by 
( 2xi ) 
( 2xi ) 
2x2 
, 
4x2 
• 
2x3 
4x3 
The two gradients are linearly dependent in the following two cases. (A) x1 = 0. In this 
case, the problem becomes 
min 3x2-x3 
s.t. 
xi+xi=1, 
whose optimal solution is (x2,x3) = (-)w, )w), and the obtained objective function 
value is -JIO. 
(B) x2 = x3 = 0. However, the constraints in this case reduce to x; = 1, x; = 2, which 
is impossible. 
The conclusion is that the optimal solution of the problem is 
(x1,x,,x,) = ( 0,-Jw. Jw) 
with optimal value -./iO. 
I 
11.3 • The Convex Case 
The KKT conditions are necessary optimality condition under the regularity condition. 
When the problem is convex, the KKT conditions are always sufficient and no further 
conditions are required. 

214 
Chapter 11. The KKT Conditions 
Theorem 11.12 (sufficiency of the KKT conditions for convex optimization prob-
lems). Let x* be a feasible solution of 
min f(x) 
s.t. 
g;(x) < 0, 
h;(x)=O, 
i = 1,2, ... , m, 
{11.10) 
j=l,2, ... ,p, 
where f, g1, ••• , gm are continuously differenti.able convex functions over Rn and h1, h2, ••• , h P 
are affine functions. Suppose that there exist multipliers A1 , A2 ••• , Am > 0 andµ 1, µ 2, ••• , µ P E 
JR such that 
m 
P 
Vf(x*)+ LA;Vg;(x*)+ Lµi\lh;(x*)=O, 
i=l 
j=l 
A;g;(x*)=O, 
i = 1,2, ... ,m. 
Then x* is an optimal solution of{ll.10). 
Proof. Let x be a feasible solution of {11.10). We will show that f(x) > f(x*). Note that 
the function 
m 
p 
s(x) = f(x)+ LAigi(x)+ Lµih;(x) 
i=l 
i=l 
is convex, and since Vs(x*) = Vf(x*)+ L~i A; \7 gi(x*)+ Lf =t µi Vh;(x*) = 0, it follows 
by Proposition 7.8 that x* is a minimizer of s(·) over Rn, and in particular s(x"'.) < s(x). 
We can thus conclude that 
f(x*) = f(x*)+ L~i A;gi(x*) + Lf=t µ;h;(x*) 
(Aig;(x*) = O,h;(x*) = 0) 
= s(x*) 
< s(x) 
=f(x)+ L~t A;gi(x)+ Lf=t µ;h;(x) 
< f(x) 
(Ai> 0, gi(x) < O,hj(x) = 0), 
showing that x* is the optimal solution of (11.10). 
D 
In the convex case we can find a different condition than regularity that guarantees 
the necessity of the K.KT condition. This condition is called Slater's condition. Slater's 
condition, like regularity, is a condition on the constraints of the problem. We will say 
that Slater's condition is satisfied for a set of convex inequalities 
g;(x)<O, i=l,2, ... ,m, 
where gp g2, ••• , gm are given convex functions if there exists i E Rn such that 
g;(i)<O, i=l,2, ... ,m. 
Note that Slater's condition requires that there exists a point that strictly satisfies the 
constraints, and does not require, like in the regularity condition, an a priori knowledge 
on the point that is a candidate to be an optimal solution. This is the reason why checking 
the validity of Slater's condition is usually a much easier task than checking regularity. 
Next, the necessity of the KKT conditions for problems with convex inequalities un-
der Slater's condition is stated and proved. 

11.3. The Convex Case 
215 
Theorem 11.13 (necessity of the KK.T conditions under Slater's condition). Let x* be 
an optimal solution of the problem 
mm f(x) 
s.t. 
gi(x) < 0, 
i = 1,2, ... , m, 
(11.11) 
where f, g1, ••• , gm arecontinuouslydifferentiablefunctionsoverRn. Jn addition, g1' g2, ••• , gm 
are convex functions over Rn. Suppose that there exists x E Rn such that 
gi(x)<O, 
i=1,2, ... ,m. 
Then there exist multipliers A1, A.2 ••• , Am > 0 such that 
m 
"V f(x*) + L: Ai "V gi(x*) = 0, 
i=l 
A.igi(x*)=O, 
i = 1,2, ... ,m. 
(11.12) 
(11.13) 
Proof. As in the proof of the necessity of the KKT conditions (Theorem 11.5), since x* is 
an optimal solution of (11.11), then the Fritz-John conditions are satisfied. That is, there 
exist 1c, A1, ••• , Am > 0, which are not all zeros, such that 
m 
1o "V f (x*) + L: Ai "V gi(x*) = 0, 
i=l 
Ai g;(x*) = 0, 
i = 1,2, ... , m. 
(11.14) 
All that we need to show is that Ao> 0, and then the conditions (11.12) and (11.13) will 
be satisfied with A; = i, i = 1, 2, ... , m. To prove that A0 > 0, assume in contradiction 
that it is zero; then 
m L: A; "V gi(x*) = 0. 
{11.15) 
i=l 
By the gradient inequality we have that for all i = 1,2, ... , m 
Multiplying the ith inequality by A; and summing over i = 1,2, ... , m, we obtain 
(11.16) 
where the inequality is strict since not all the Ai are zero. Plugging the identities {11.14) 
and (11.15) into (11.16), we obtain the impossible statement that 0 > 0, thus establishing 
the result. 
D 
Example 11.14. Consider the convex optimization problem 
. 
mm 
s.t. 

216 
Chapter 11. The KKT Conditions 
The Lagrangian is 
L(xp .xi, A)= x:-x2 + Ax2• 
Since the problem is a linearly constrained convex problem, the KK.T conditions are nec-
essary and sufficient (Theorem 10.7). The conditions are 
2x1 =0, 
-l+A=O, 
X2 = 0, 
and they are satisfied for (xp x2) = (0, 0), which is the optimal solution. 
Now, consider a different reformulation of the problem: 
mm 
s.t. 
Slater's condition is not satisfied since the constraint cannot be satisfied strictly, and there-
fore the KKT conditions are not guaranteed to hold at the optimal solution. The KK.T 
conditions in this case are 
2x1 =0, 
-1+2Ax2 =0, 
Axi = o, 
xi <O, 
A>O. 
The above system is infeasible since x2 = 0, and hence the equality -1 + 2A.:ti = 0 is 
impossible. 
I 
A slightly more refined analysis can show that in the presence of affine constraints, 
one can prove the necessity of the KK.T conditions under a generalized Slater's condi-
tion which states that there exists a point that strictly satisfies all the nonlinear inequality 
constraints as well as satisfies the affine equality and inequality constraints. 
Definition 11.15 (generalized Slater's condition). Consider the system 
g;(x) < 0, 
h/x) < 0, 
sk(x) = 0, 
i = 1,2, ... ,m, 
j=l,2, ... ,p, 
k = 1,2, ... ,q, 
where gi,i = 1,2, ... ,m,areconvexfunctionsandhj,sk,j = 1,2, ... ,p,k = 1,2, ... ,q,are 
affine functions. Then we say that the generalized Slater's condition is satisfied if there 
exists x E 1Rn for which 
g;(x) < o, 
h/x)<O, 
sk(x) = 0, 
i = 1,2, ... ,m, 
j=l,2, ... ,p, 
k=l,2, ... ,q. 
The necessity of the KKT conditions under the generalized Slater's condition is now 
stated. 

11.3. The Convex Case 
217 
Theorem 11.16 (necessity of the KKT conditions under the generalized Slater's con· 
dition). Let x* be an optimal solution of the problem 
mm f(x) 
s.t. 
gi(x) < 0, 
h/x) < 0, 
sk(x) = 0, 
i = 1,2, ... ,m, 
j = 1,2, ... ,p, 
k = 1,2, ... ,q, 
where f, g1, ••• , gm are continuously differentiable convex functions over Rn, and h j, sk, j = 
1,2, ... ,p,k = 1,2, ... ,q, are affine. Suppose that there exists x E Rn such that 
gi(x) < o, 
h/x)< o, 
sk(X.) = 0, 
i = 1,2, ... ,m, 
j = 1,2, ... ,p, 
k = 1,2, ... ,q. 
m 
p 
q 
Vf(x*)+ L:Ai\Jgi(x*)+ L:TJj\Jhj(x*)+ L:µk \Jsk(x*)=O, 
i=l 
j=l 
k=t 
A.igi(x*)=O, 
i=1,2, ... ,m, 
1Jjhj(x*)=O, 
j = 1,2, ... ,p. 
Example 11.17. Consider the convex optimization problem 
min 4x: +xi-x1 -2x2 
s.t. 
2x1 + x2 < 1, 
x: < 1. 
Slater's condition is satisfied with (xp x2) = (0, 0) (2 · 0 + 0 < 1, 02 -
1 < 0), so the KKT 
conditions are necessary and sufficient. The Lagrangian function is 
L(x1,x2,A.1,A2) = 4x: +xi-x1 -2x2 + A1(2x1 + x2-1) + Ai{x:-1), 
and the KKT system is 
We will consider four cases. 
BL 
-a =2x2-2+A.1 =O, 
X2 
A.1 ( 2x1 + x2 - 1) = 0, 
A.z<x:-1) = 0, 
2x1+x2 <1, 
x: < 1, 
A.1' A.2 > o. 
(11.17) 
(11.18) 
Case I: If A.1 = A.2 = 0, then by the first two equations, x1 = l, x2 = 1, which is not a 
feasible solution. 

218 
Chapter 11. The KKT Conditions 
Case II: If At > 0, A2 > 0, then by the complementary slackness conditions 
2xt +x2 =1, 
x: = 1. 
The two solutions of this system are {1,-1),(-1,3). Plugging the first solution into the 
first equation of the KKT system (equation (11.17)) yields 
which is impossible since At, A2 > 0. Plugging the second solution into the second equa-
tion of the KKT system (equation (11.18)) results in the equation 
4+ A1 = o, 
which has no solution since A1>0. 
Case III: If A1 > 0, A2 = 0, then by the complementary slackness conditions we have that 
(11.19) 
which combined with (11.17) and (11.18) yields the set of equations (recalling that ..l2 = 0) 
8x1 -1 + 2A1 = o, 
2x2 - 2 + A1 = o, 
2x1+x2 =1, 
whose unique solution is (x1, Xi' A1) = { 
1~, ~, i), and we obtain that (x1, x2, Ap A2) = 
{ 1~, i, ~,O) satisfies the KKT system. Hence, (x1,x2) = ( 1~, i} is a KKT point, and since 
the problem is convex, it is an optimal solution. In principle, we do not have to check the 
fourth case since we already found an optimal solution, but it might be that there exist 
additional optimal solutions, and if our objective is to find all the optimal solutions, then 
all the cases should be covered. 
Case IV: If A1 = 0, A2 > 0, then by the complementary slackness conditions we have 
x: = 1. By (11.18) we have that x2 =1. The two candidate solutions in this case are there-
fore (1, 1) and (-1, 1). The point (1, 1) does not satisfy the first constraint of the problem 
and is therefore infeasible. Plugging (x1, x2) = (-1, 1) and ..l1 =0 into (11.17) yields 
-9-2A2 =0, 
which is a contradiction to the positivity of A2• 
To conclude, the unique optimal solution of the problem is (x1,x2) = ( 1~, ~). 
I 
11.4 • Constrained Least Squares 
Consider the problem 
(CLS) 
min 11Ax-hll2 
s.t. 
llxll2 < a, 
where A E Rmxn is assumed to be of full column rank, b E Rm, and a > 0. We will refer 
to this problem as a constrained least squares {CLS) problem. In Section 3.3 we considered 

11.4. Constrained Least Squares 
219 
the regularized least squares (RLS) problem which has the form5 min{llAx-bll2+ µllxll2}. 
The two problems are related in the sense that they both regularize the least squares 
solution by a quadratic regularization term. In the RLS problem, the regularization is 
done by a penalty function, while in the CLS problem the regularization is performed by 
incorporating it as a constraint. 
Problem (CLS) is a convex problem and satisfies Slater's condition since x = 0 strictly 
satisfies the constraint of the problem. To solve the problem, we begin by forming the 
Lagrangian: 
L(x, ...l) = 11Ax-bll2 + ...l(llxll2-a) 
(...l > 0). 
The KKT conditions are 
V'xl=2AT(Ax-b)+2...lx = 0, 
...l(llxJl2-a) = 0, 
llxll2 <a, 
...l>O. 
There are two options. In the first, A = 0, and then by the first equation we have that 
This is a KKT point and hence the optimal solution if and only if x15 is feasible, that is, 
if llx15112 <a. This is not a surprising result since it is clear that when the unconstrained 
minimizer (x15) satisfies the constraint, it is also the optimal solution of the constrained 
problem. 
On the other hand, if llx15112 > a, then A > 0. By the complementary slackness 
condition we have that llxll2 =a, and the first equation implies that 
x= X,t =(AT A+...l1)-1Arb. 
The multiplier A> 0 should be thus chosen to satisfy llx..lll2 = a; that is, A is the solu-
tion of 
/(...l) = 
ll(A7 A+AI)-1A7bll2-a = 0. 
(11.20) 
We have /(0) = ll(A7 A)-1A7bll2-a = llx15ll2-a > 0, and it is not difficult to show that 
f is a strictly decreasing function satisfying f ( ...l) -+ -a as A -+ oo. Thus, there exists a 
unique A for which /(...l) = 0, and this A can be found for example by a simple bisection 
procedure. To conclude, the optimal solution of the CLS problem is given by 
where A is the unique root off over [O,oo). We will now construct a MATLAB func-
tion for solving the CLS problem. For that, we need to write a MATLAB function that 
performs a bisection algorithm. The bisection method for finding the root of a scalar 
equation f ( x) = 0 is described below. 
5In Section 3.3 we actually considered a more general model in which the regularizer had the form µl!Dxll2, 
where D is a given matrix. 

220 
Chapter 11. The KKT Conditions 
Bisection 
~ - - -
Input: e - tolerance parameter. a< b - two numbers satisfying/(a)/(b) < 0. 
Initialization: Take 10 =a,u0 = b. 
General step: For any k = 0, 1, 2, ... execute the following steps: 
(a) Take xk = "•;1•. 
(b) If f(lk) · f(xk) > 0, define lk+t = xk,uk+t = uk. Otherwise, define lk+t = 
/k, Uk+l = Xk • 
(c) if uk+t -lk+t < e, then STOP, and xk is the output. 
A MATLAB function implementing the bisection method is given below. 
function z=bisection(f,lb,ub,eps) 
%INPUT 
%================ 
%f ................... a scalar function 
%lb ............ . ..... the initial lower bound 
%ub .................. the initial upper bound 
%eps ................. tolerance parameter 
%OUTPUT 
%================ 
% z .................. a root of the equation f(x)=O 
if (f(lb)*f(ub)>O) 
error('f(lb)*f(ub)>O') 
end 
iter=O; 
while (ub-lb>eps) 
z=(lb+ub)/2; 
iter=iter+l; 
if(f(lb)*f(z)>O) 
lb=z; 
else 
ub=z; 
end 
fprintf('iter_numer = %3d current_sol = %2.6f \n',iter,z); 
end 
Therefore, for example, if we wish to find the square root of 2 with an accuracy of 
10-4, then we can solve the equation x2 - 2 = 0 by the following MATLAB command: 
>> bisection(@(x)xA2-2,l,2,le-4); 
iter_numer = 
1 current_sol = 1.500000 
iter_numer = 
2 current sol = 1.250000 
-
iter_numer = 
3 current sol = 1.375000 
-
iter_numer = 
4 current sol = 1.437500 
-
iter_numer = 
5 current sol = 1.406250 
-

11.4. Constrained Least Squares 
221 
iter_nurner = 
6 current_sol = 1.421875 
iter nurner = 
7 current_ sol = 1.414063 
-
iter_nurner = 
8 current_sol = 1.417969 
iter_nurner = 
9 current sol = 1.416016 
-
iter_nurner = 10 current_ sol = 1.415039 
iter_nurner = 11 current_sol = 1.414551 
iter_nurner = 
12 current_ sol = 1.414307 
iter_nurner = 
13 current_sol = 1.414185 
iter_nurner = 
14 current_sol = 1.414246 
As for the CLS problem, note that the scalar function f given in {11.20) satisfies 
f(O) > 0. Therefore, all that is left is to find a point u > 0 satisfying f (u) < 0. For 
that, we will start with guessing u = 1 and then make the update u +- 2u until f (u) < 0. 
The MATLAB function implementing these ideas is given below. 
function x_cls=cls(A,b,alpha) 
%INPUT 
%================ 
%A ................... an mxn matrix 
%b ................... an m-length vector 
%alpha ............... positive scalar 
%OUTPUT 
%================ 
% x_cls ............ an optimal solution of 
% 
min{ I IA*x-bl I: I lxl I "2<=alpha} 
d=size (A); 
n=d ( 2) ; 
x_ls=A\b; 
if (norm(x_ls)"2<=alpha) 
x_cls=x_ls; 
else 
end 
f=@(lam) norm((A'*A+lam*eye(n))\(A'*b))"2-alpha; 
u=l; 
while (f (u)>O) 
U=2*Ui 
end 
lam=bisection(f,0,u,le-7); 
x_cls=(A'*A+lam*eye(n))\(A'*b); 
For example, assume that we pick A and b as 
A=[l,2;3,1;2,3]; 
b= [ 2; 3; 4] ; 
The least squares solution and its squared norm can be easily found: 
>> x_ls=A\b 
x_ls = 
0.7600 
0.7600 
>> norrn(x_ls)"2 

222 
Chapter 11. The KKT Conditions 
ans = 
1.1552 
If we use the els function with an a which is greater than 1.1552, then we will obviously 
get back the least squares solution: 
>> cls(A,b,1.5) 
ans = 
0.7600 
0.7600 
On the other hand, taking an a with a smaller value than 1.552, will result in a different 
solution (the bisection output was suppressed): 
>> cls(A,b,0.5) 
ans = 
0.5000 
0.5000 
To double check the result, we can run CVX, 
cvx_begin 
variable x_cvx(2) 
minimize(norm(A*x_cvx-b)) 
norm(x_cvx)<=sqrt(0.5) 
cvx_end 
and get the same result: 
>> x_cvx 
x_cvx = 
0.5000 
0.5000 
11.5 • Second Order Optlmallty Conditions 
11.5.1 • Necessary Conditions for Inequality Constrained Problems 
We can also establish necessary second order optimality conditions in the general non-
convex case. We will begin by stating and proving the result for inequality constrained 
problem. 
Theorem 11.18 (second order necessary conditions for inequality constrained prob-
lems). Consider the problem 
min fo(x) 
s.t. 
fi(x) < 0, 
i = 1,2, ... , m, 
(11.21) 
where fo, ft, ... , f m are twice continuously differenti.able over lRn. Let x* be a local minimum 
of problem (11.21), and suj1pose that x* is regular, meaning that the set {V fi(x*) hei(x•) is 

11 .5. Second Order Optimality Conditions 
linearly independent, where 
I ( x*) = { i E { 1, 2, ... , m} : h ( x*) = 0}. 
Denote the Lagrangian by 
m 
L(x,A)=fo(x)+ LA;h(x). 
i=l 
Then there exist A1, .A.2, ••• , Am > 0 such that 
V xL(x*, A)= 0, 
A;h(x*) = 0, 
i = 1,2, ... , m, 
and 
yTV!,L(x', A)y =YT [ V2fo( x') +~A, V2 /;( x')] y > 0 
for ally E A(x*), where 
A(x*) ={de Rn: V !'i(x*f d = 0, i E /(x*)}. 
Proof. Let deD(x*), where 
D(x*) = {d E Rn: V h(x*)7 d < 0, i E /(x*)U {O}}. 
223 
From this point until further notice we will assume that d is fixed. Let z E Rn, and 
i E {O, 1,2, ... , m }. Define 
t2 
x(t) = x* + td+ -z 
2 
and the one-dimensional functions 
g;(t)=!'i(x(t)), 
i e/(x*)U{O}. 
Then 
g;(t) = (d+ tzfV!'i(x(t)), 
g;'(t) = (d + tz)7 V2 !'i(x(t))(d+ tz) + z7 V !'i(x(t)), 
which in particular implies that 
g;(o) = V !'i(x*)7 d, 
g;'(O) = d7 V2!'i(x*)d+ V!'i(x*)7 z. 
By the quadratic approximation theorem (fheorem 1.25) we have 
Therefore, for any i E /(x*)U {O} there are two cases: 
1. V!'i(x*)7 d < 0, and in this case !'i(x(t)) <!'i(x*) for small enough t > 0. 
2. V!'i(x*)7 d = 0. In this case, by (11.22), if V!'i(x*)7 z + d7 V2!'i(x*)d < 0, then 
h ( x( t)) < h ( x*) for small enough t. 

224 
Chapter 11. The KKT Conditions 
As a conclusion, since x* is a local minimum of problem (11.21), the following system of 
strict inequalities in z (recall that d is fixed) does not have a solution: 
V Ji(x*)r z + drv2 fi(x*)d < o, 
i E](x*) u {O}, 
(11.23) 
where 
J(x*) = {i E /(x*): V Ji(x*)T d = O}. 
Indeed, if there was a solution to system (11.23), then for small enough t, the vector x( t) 
would be a feasible solution satisfying fo( x( t)) < fo( x* ), contradicting the local optimality 
of x*. System (11.23) can be written as 
Az<b, 
where A is the matrix whose rows are V Ji(x*)T, i E](x*)U {O}, and bis the vector whose 
rows are -dTV2fi(x*)d, i E](x*) U {O}. By the nonhomogenous Gordan's theorem (see 
Exercise 10.5), we have that there exists y such that AT y = O,br y < O,y > O,y ~ 0, 
meaning that there exist 0 < Yi, i E] ( x*) U { 0}, not all zeros, such that 
and 
that is, 
L 
Yi V /i(x*) = 0 
iE](x0 )u{O} 
L 
Yi(-dTV2/i(x*)d) < 0, 
iE](x0 )U{O} 
dT [ L 
Y;V2fi(x*)]d>O. 
ie](x*)U{O} 
(11.24) 
By the regularity of x* and (11.24), we have that y0 > 0, and hence by defining A; = y, for 
Yo 
i E](x*) and Ai = 0 for i E /(x*)V(x*), we obtain that 
V .fo(x*) + L: A; V Ji(x*) = 0, 
(11.25) 
iE/(x0 ) 
dT[V2.fo(x*)+ L: A;V2/i(x*)]d>O. 
iE/(x*) 
Since {V Ji (x*)} ie/(x*) are linearly independent, equation (11.25) implies that the multipli-
ers A;, i E /(x*), do not depend on the initial choice of d. Therefore, by defining A; = 0 
for any i (j. /(x*), we obtain that 
m 
V .fo(x*) + L: A; V Ji(x*) = 0, 
i=1 
A;/i(x*)=O, 
i=1,2, ... ,m, 
dT [ V2 fo(x') + t. A, V2 /;(x')] d > 0 
(11.26) 

11.5. Second Order Optimality Conditions 
225 
for any d E D(x*). All that is left to prove is that (11.26) is satisfied for all d E A(x*). 
Indeed, if d E A(x*), then either d or-dis in D(x*). Thus, cd E D(x*) for some c E 
{-1, 1}, and as a result 
which is the same as 
dr [ V 2 fo(x')+ t. A; V2 /;( x')] d > 0, 
and the result is established. 
D 
11.5.2 • Necessary Second Order Conditions for Equality and Inequality 
Constrained Problems 
When the problem involves both equality and inequality constraints, a similar result can 
be proved, and it is stated without a proof below. 
Theorem 11.19 (second order necessay conditions for equality and inequality con-
strained problems). Consider the problem 
mm f(x) 
s.t. 
gi(x)<O, i=1,2,. .. ,m, 
(11.27) 
hj(x)=O, j=1,2, ... ,p, 
where f, g1, ••• , gm, h1, ••• , h Pare twice continuously differentiable O'Ver Rn. Let x* be a local 
minimum of problem (11.27), and suppose that x* is regular, meaning that {V gi ( x* ), V h j( x* ), 
i E /(x*),j = 1,2, ... , p} are linearly independent, where 
/(x*) = { i E { 1,2, ... , m} : /;(x*) = O}. 
m 
p 
V f (x*) + L: Ai V gi (x*) + L: µ j V hj(x*) = 0, 
i=l 
j=l 
and 
dr [ V2fo(x')+ t.A; V2g,(x')+ t,µi V2bi(x')] d> 0 
for all d E A(x*) where 
A(x*) = 
{d E Jin : V gi(x*)T d = 0, Vhj(x*)T d = 0, i E /(x*),j = 1,2, ... , p }. 
Example 11.20. Consider the problem 
mm 
s.t. 

226 
Chapter 11. The KKT Conditions 
We first note that since the problem consists of minimizing a coercive objective function 
over a closed set, it follows that an optimal solution does exist. In addition, there are no 
irregular points to the problem since the gradient of the constraint function h is always 
different from the zeros vector. Therefore, the optimal solution is one of the KKT points 
of the problem. The Lagrangian of the problem is 
L(x1, Xi'µ)= (2x1 -1)2 +xi+ µ(-2x1 +xi), 
and the KKT system is 
By the second equation 
2(1+µ)x2 =0, 
and hence there are two cases. In one case, x2 = O; then by the third equation, x1 = 0, 
and by the first equation µ = -2. The second case is when µ = -1, and then by the first 
equation, 4(2x1 -1) =-2, that is, x1 =~'and then x2 = ± }i· We thus obtain that there 
are three KKT points: (x1'x2, µ) = (0,0,-2),(i, }i,-1),(~,-ji,-1). 
Note that 
V!.L(x1,x,,µ) = (~ 2{l ~ µ)). 
Note that for the points (x1,x2,µ) = (~, Jz,-1),(i,-Jz,-1) the Hessian of the La-
grangian is positive semidefinite: 
V!L(x.,x,,µ) = (~ ~) >-O. 
Therefore, these points satisfy the second order necessary conditions. On the other hand, 
for the first point (0, 0) where µ = -2, the Hessian is given by 
v;L(x,,x,.µ) = (~ __'.>2)· 
which is not a positive semidefinite matrix. To check the validity of the second order 
conditions at (0,0), note that 
Vh(x1,x,) = (2~), 
and thus V'h(O,O) =((/).We therefore need to check whether 
dTV'2L(x1' x2, µ)d > 0 for all d satisfying V'h(O,O)T d = 0. 
Since the condition V' h(O, Of d = 0 translates to d1 = 0, we need to check whether 

11.6. Optimality Conditions for the Trust Region Subproblem 
227 
for any di· However, the latter inequality is equivalent to saying that -2di > 0 for any d2, 
which is of course not correct. 
The conClusion is that (0, 0) does not satisfy the second order necessary conditions 
and hence cannot be ~ optimal solution. The optimal solution must be either ( ~, Ji) 
or(~,-}i) (or both). Since the points have the same objective function value, it follows 
that they are the optimal solutions of the problem. 
I 
11.6 • Optimality Conditions for the Trust Region Subproblem 
In Section 8.2.7 we considered the trust region subproblem (TRS) in which one minimizes 
an indefinite quadratic function subject to a norm constraint: 
(TRS): 
min{/(x) = 
xT Ax+2bT x+c: llxll2 <a}, 
where A= ATE Rnxn ,b E Rn ,c ER, and a ER++· Note that here we consider a slight 
extension of the model given in Section 8.2.7 since the norm constraint has a general up-
per bound and not 1. We have seen that the problem can be recast as a convex optimization 
problem. In this section we will look at another aspect of the "easiness" of the problem: 
the problem possesses necessary and sufficient optimality conditions. We will show that 
these optimality conditions can be used to develop an algorithm for solving the problem. 
We begin by stating the necessary and sufficient conditions. 
Theorem 11.21 (necessary and sufficient conditions for problem (TRS)). A vector x* 
is an optimal solution of problem (TRS) if and only if there exists ).* > 0 such that 
(A+ ).*I)x* = -b, 
llx*ll2 <a, 
).*(llx* 112-a)= 0, 
A+ ).*I>- 0. 
(11.28) 
(11.29) 
(11.30) 
(11.31) 
Proof. Sufficiency: To prove the sufficiency, let us assume that x* satisfies (11.28)-(11.31) 
for some).*> 0. Define the function 
h(x) = f (x) + ).*(llxll2-a) = xT (A+ ..l.*I)x + 2bT x + c - a..l.*. 
(11.32) 
Then by (11.31) his a convex quadratic function. By (11.28) it follows that Vh(x*) = 0, 
which combined with the convexity of h implies that x* is an unconstrained minimizer 
of hover Rn (see Proposition 7.8). Let x be a feasible point, that is, llxll2 <a. Then 
/(x) > /(x) + ).*(llxll2-a) 
=h(x) 
> h(x*) 
= f (x*) + ).*(llx*ll2-a) 
=/(x*) 
(..l.* > 0, llxll2-a < 0) 
(by (11.32)) 
(x* is a minimizer of h) 
(by (11.30)) 
and we have established that x* is a global optimal solution of the problem. 
Necessity: To prove the necessity, note that the second order necessary optimality 
conditions are satisfied since all the feasible points of (TRS) are regular. Indeed, the reg-
ularity condition states that when the constraint is active, that is, when llx*ll2 =a, the 

228 
Chapter 11. The KKT Conditions 
gradient of the constraint is not the zeros vector, and indeed, since V h(x*) = 2x*, it is not 
equal to the zeros vector (since llx*ll2 =a). 
If I Ix* 11 < a, then the second order necessary optimality conditions (see Theorem 
11.18) are exactly (11.28)-(11.31). If llx*ll2 =a, then by the second order necessary opti-
mality conditions there exists A* > 0 such that 
(A+ .A*I)x* = -b 
llx*ll2 <a, 
.A*(llx*ll2-a) = 0, 
d7 (A+ .A*I)d > 0 
for all d satisfying d7 x* = 0. 
(11.33) 
(11.34) 
(11.35) 
(11.36) 
All that is left to show is that the inequality (11.36) is true for any d and not only for 
those which are orthogonal to x*. Suppose on the contrary that there exists a d such that 
d7 x* 'f:. 0 and d7 (A+ .A*I)d < 0. Consider the point i = x* + td, where t = -2~~0;. The 
vector i is a feasible point since 
llill2 = llx* + tdW = llx*112 +2tdT x* + t2lldll2 
(dT x*)2 
(dT x*)2 
= llx*ll2- 4 lldll2 + 4 lldll2 
= llx*112 <a. 
In addition, 
f(i)=iT Ai+2bTi+c 
= (x* + td)T A(x* + td) + 2bT (x* + td) + c 
= (x*)T Ax* +2bT x* +c+t2dT Ad+2tdT(Ax* +b) 
f(x*) 
= f(x*)+ t2dT(A+ .A*l)d+2tdT((A+ .A*l)x* + b)-.A*t [tlldll2 +2dT x*] 
= f (x*) + t 2dT (A+ .A*I)d 
<f(x*), 
=0 by (11.33) 
which is a contradiction to the optimality of x*. 
0 
=Cl by def. of t 
Theorem 11.21 can be used in order to construct an algorithm for solving the trust 
region subproblem. We will make the following assumption, which is rather conventional 
in the literature: 
-b ~ Range(A- .Amin(A)I). 
(11.37) 
Under this condition there cannot be a vector x for which (A- .Amin(A)I)x = -b. 
This means that the multiplier .A* from the optimality conditions must be different than 
-.Amin(A). The condition (11.37) is considered to be rather mild in the sense that the range 
space of the matrix A-A.min ( A)I is of rank which is at most n-1. Therefore, at least when 
A and b are generated from a continuous random distribution, the probability that -b 
will not be in this space is 1. 

11.6. Optimality Conditions for the Trust Region Subproblem 
229 
We will consider two cases. 
Case I: A >- 0. Since in this case the problem is convex, x* is an optimal solution of (fRS) 
if and only if there exists A*> 0 such that 
(A+ ).*I)x* = -b, A*(llx*112- a)= 0, 
llx*112 <a. 
If A* = 0, then Ax* = -b, and hence x* = -A-1b. This will be the optimal solution if 
and only if llA-1bll2 <a. If).*> 0, then llx*ll2 =a, and thus the optimal solution is given 
by x* =-(A+ A*I)-1b, where A* is the unique root of the strictly decreasing function 
over (O,oo). 
Case II: A 'f 0 In this case, condition (11.31) combined with the nonnegativity of).* 
is the same as A* > -Amin(A)(> 0). Under Assumption (11.37), A* cannot be equal to 
-Amin(A), and we can thus assume that A* > -Amin(A). In particular, A* > 0 and hence 
we have llx* 112 = a as well as A+ A*I >- 0. Therefore, (11.28) yields 
x* =-(A+ ).*I)-1b, 
(11.38) 
so that llx*ll2 = ll(A + A*I)-1bll2 =a. The optimal solution is therefore given by (11.38), 
where A* is chosen as the unique root of the strictly decreasing function f(A) = ll(A + 
Al)-1bl12-a over (-Amin(A),oo). 
An implementation of the algorithm for solving the trust region subproblem in MAT-
LAB is given below by the function trs. The function uses the bisection method in order 
to find A*. Note that the function uses the fact that f(A) ~ oo as A~ -Amin(A)+ by tak-
ing the initial lower bound as -Amin(A) + E for some small E > 0. 
function x_trs=trs(A,b,alpha) 
%INPUT 
%================ 
%A ................... an men matrix 
%b ................... an n-length vector 
%alpha ............... positive scalar 
%OUTPUT 
%================ 
% x_trs 
% 
n=length (b) ; 
an optimal solution of 
rnin{x' *A*x+2b' *X: I !xi I "2<=alpha} 
f=@(larn) norrn((A+larn*eye(n))\b)"2-alpha; 
[L,p]=chol(A, 'lower'); 
% the case when A is positive definite 
if (p==O) 
x_naive=-L'\(L\b); 
if(norrn(x_naive)"2<=alpha) 
x_trs=x_naive; 
else 
u=l; 
while (f (u)>O) 
U=2*Ui 
end 

230 
else 
end 
lam=bisection(f,O,u,le-7); 
x_trs=-(A+lam*eye(n))\b; 
%when A is not positive definite 
u=max(l,-min(eig(A))+le-7); 
while ( f (u) >0) 
u=2*U; 
Chapter 11. The KKT Conditions 
end 
lam=bisection(f,-min(eig(A))+le-7,u,le-7); 
x_trs=-(A+lam*eye(n))\b; 
end 
We can for example use the MATLAB function trs in order to solve Example 8.16. 
>> A=[l,2,3;2,1,4;3,4,3]; 
>> b=[0.5;1;-0.5]i 
>> x_trs = trs(A,b,l) 
x_trs = 
-0.2300 
-0.7259 
0.6482 
This is of course the same as the solution obtained in Example 8.16. 
11. 7 • Total Least Squares 
Given an approximate linear system Ax~ b (A E Rmxn, b E Rm), the least squares prob-
lem discussed in Chapter 3 can be seen as the problem of finding the minimum norm 
perturbation of the right-hand side of the linear system such that the resulting system is 
consistent: 
ml°w,x 
s.t. 
llwll2 
Ax=h+w, 
we Rm. 
This is a different presentation of the least squares problem from the one given in Chap-
ter 3, but it is totally equivalent since plugging the expression for w (w = Ax- b) into 
the objective function gives the well-known formulation of the problem as one consisting 
of minimizing the function llAx - bll2 over Rn. The least squares problem essentially 
assumes that the right-hand side is unknown and is subjected to noise and that the matrix 
A is known and fixed. However, in many applications the matrix A is not exactly known 
and is also subjected to noise. In these cases it is more logical to consider a different prob-
lem, which is called the total least squares problem, in which one seeks to find a minimal 
norm perturbation to both the right-hand-side vector and the matrix so that the resulting 
perturbed system is consistent: 
(TLS) 
minE,w,x 
s.t. 
llEI I} + Hwll2 
(A+E)x=h+w, 
EeRmxn,weRm. 
Note that we use here the Frobenius norm as a matrix norm. Problem (TLS) is not a 
convex problem since the constraints are quadratic equality constraints. However, despite 

11. 7. Total Least Squares 
231 
the nonconvexity of the problem we can use the KKT conditions in order to simplify it 
considerably and eventually even solve it. The trick is to fix x and solve the problem with 
respect to the variables E and w, giving rise to the problem 
minE,w 
I IEI I} +I lwl 12 
s.t. 
(A+E)x=h+w. 
Problem (Px) is a linearly constrained convex problem and hence the KKT conditions 
are necessary and sufficient (Theorem 10.7). The Lagrangian of problem (Px) is given by 
l(E, w,).) = llEll} + llwll2 +2).T[(A+E)x-b-w]. 
By the KKT conditions, (E, w) is an optimal solution of (Px) if and only if there exists 
). E Rm such that 
2E+2).xT =0 
(VEL=O), 
2w-2).= 0 
(Vwl= 0), 
(A+E)x = h+w (feasibility). 
From (11.40) we have).= w. Substituting this in (11.39) we obtain 
E=-wxr. 
Combining (11.42) with (11.41) we have (A-wxT)x = b +w, so that 
Ax-b 
w= 
' 
llx112+1 
and consequently, by plugging the above into (11.42) we have 
(Ax-b)xT 
E = 
llxll2 + 1 . 
(11.39) 
(11.40) 
(11.41) 
(11.42) 
(11.43) 
(11.44) 
Finally, by substituting (11.43) and (11.44) into the objective function of problem (Px) we 
obtain that the value of problem (Px) is equal to ll~r!r. Consequently, the TLS problem 
reduces to 
(TLS') 
min llAx-hll2. 
xelRn I Ix! 12 + 1 
We have thus proven the following result. 
Theorem 11.22. x is an optimal solution of (TLS') if and only if ( x, E, w) is an optimal 
l . 
,f'(fLS) h 
E 
(Ax-b)xr 
d 
Ax-b 
so utton o1 
w ere = 
llxll2+t an w = llxll2+t' 
The new formulation (TLS') is much simpler than the original one. However, the 
objective function is still nonconvex, and the question that still remains is whether we 
can efficiently find an optimal solution of this simplified formulation. Using the special 
structure of the problem, we will show that the problem can actually be solved efficiently 
by using a homogenization argument. Indeed, problem (TLS') is equivalent to 
mm 
:t=l , 
. 
{ llAx- tbll2 
} 
xelRn,telR 
llx112 + t 2 

232 
Chapter 11. The KKT Conditions 
which is the same as (denoting y = n)) 
* 
. {yTBy 
} 
f = mm 
2 =Yn+t = 1 ' 
yeJRn+l 
llYll 
(11.45) 
where 
( ATA 
-ATb) 
B = -bT A 
llhll2 . 
Now, let us remove the constraint from problem (11.45) and consider the following 
problem: 
* 
. {yTBy 
} 
g=mm 
2 :yf:.O. 
yeJRn+l 
llYll 
(11.46) 
This is a problem consisting of minimizing the so-called Rayleigh quotient (see Section 
1.4) associated with the matrix B, and hence an optimal solution is the vector correspond-
ing to the minimum eigenvalue of B; see Lemma 1.12. Of course, the obtained solution 
is not guaranteed to satisfy the constraint Yn+t = 1, but under a rather mild condition, 
the optimal solution of problem (11.~5) can be extracted. 
Lemma 11.23. Let y* be an optimal solution of (11.46) and assume that Y:+i f:. 0. Then 
y = -f--y* is an optimal solution of (11.45). 
Yn+i 
Proof. Note that since (11.46) is formed from (11.45) by replacing the constraint Yn+t = 1 
with y f:. 0, we have 
f* > g*. 
However, y is a feasible point of problem (11.45) <Yn+t = 1), and we have 
t 
( *)TB * 
<Y::J Y 
Y 
(y*f By* 
* 
--.1--r I ly* 112 
= I ly* 112 = g . 
(yn+l) 
Therefore, y attains the lower bound g* on the optimal value of problem (11.45), and con-
sequently, it is an optimal solution of (11.45) and the optimal values of the two problems 
(11.45) and (11.46) are consequently the same. 
D 
All that is left is to find a computable condition under which Y:+i f:. 0. The follow-
ing theorem presents such a condition and summarizes the solution method of the TLS 
problem. 
Theorem 11.24. Assume that the following condition holds: 
(11.47) 
where 
( ATA 
-ATb) 
B = -bT A 
llbll2 . 
Then the optimal solution of problem (TLS') is given by - 1-v, where y = (y:+i) is an eigen-
Yn+1 
vector corresponding to the minimum eigenvalue of B. 

Exercises 
233 
Proof. By Lemma 11.23 all that we need to prove is that under condition (11.47), an op-
timal solution y* of (11.46) must satisfy y;+t-:/:- 0. Assume on the contrary that y;+t = 0. 
Then 
* 
(y*f By* 
v7 AT Av 
).min(B) = f = lly*ll2 = llvll2 
> ~in(A 7 A), 
which is a contradiction to (11.47). 
0 
Exercises 
11.1. Consider the optimization problem 
mm x1-4x2+x3 
(P) 
s.t. 
x1 + 2x2 + 2x3 = -2, 
x: + x~ + x: < 1. 
(i) Given a KK.T point of problem (P), must it be an optimal solution? 
(ii) Find the optimal solution of the problem using the KK.T conditions. 
11.2. Consider the optimization problem 
(P) 
min{aT x: x7 Qx+2b7 x+c < 0}, 
where Q E Rnxn is positive definite, a(:j:. 0), b E Rn, and c ER. 
(i) For which values of Q, b, c is the problem feasible? 
(ii) For which values of Q, b, c are the KK.T conditions necessary? 
(iii) For which values of Q, b, c are the KKT conditions sufficient? 
(iv) Under the condition of part (ii), find the optimal solution of (P) using the 
KKT conditions. 
11.3. Consider the optimization problem 
mm 
s.t. 
(i) Is the problem convex? 
(ii) Prove that there exists an optimal solution to the problem. 
(iii) Find all the KKT points. For each of the points, determine whether it satisfies 
the second order necessary conditions. 
(iv) Find the optimal solution of the problem. 
11.4. Consider the optimization problem 
mm 
s.t. 
(i) Is the problem convex? 
(ii) Find all the KKT points of the problem. 
(iii) Find the optimal solution of the problem. 

234 
Chapter 11. The KKT Conditions 
11.5. Consider the optimization problem 
mm -2x; + 2xi + 4x1 
s.t. 
x; +xi - 4 < 0, 
x; + xi-4x1 + 3 < 0. 
(i) Prove that there exists an optimal solution to the problem. 
(ii) Find all the KK.T points. 
(iii) Find the optimal solution of the problem. 
11.6. Use the KK.T conditions in order to find an optimal solution of the each of the 
following problems: 
(i) 
(ii) 
mm 
s.t. 
(iii) 
(iv) 
(v) 
mm 
s.t. 
3x2 +x2 
1 
2 
3xi +xi +x1 +x2 +0.1<0, 
X2+10>0. 
mm 2x1 +x2 
s.t. 
4xi +xi-2 < 0, 
4x1 +x2 +3 < 0. 
mm x3+x3 
1 
2 
s.t. 
x;+xi < 1. 
mm x4-x2 
1 
2 
s.t. 
xi +xi< 1, 
2Xi+ 1<0. 
11.7. Let a > 0. Find all the optimal solutions of 
max{x1x2x3 :a2xi+xi+xi<1}. 
11.8. 
(i) Find a formula for the orthogonal projection of a vector y E R3 onto the set 
C = {xeR3 : xi +2xi +3xi < 1}. 
The formula should depend on a single parameter that is a root of a strictly 
decreasing one-dimensional function. 
(ii) Write a MATLAB function whose input is a three-dimensional vector and its 
output is the orthogonal projection of the input onto C. 
11.9. Consider the optimization problem 
mm 2x1x2 +!xi 
(P) 
s.t. 
2x1 x3 + ! xi < 0, 
2x2x3 +!xi< 0. 

Exercises 
235 
(i) Show that the optimal solution of problem (P) is x* = (0,0,0). 
(ii) Show that x* does not satisfy the second order necessary optimality condi-
t10ns. 
11.10. Consider the convex optimization problem 
mm fo(x) 
(P) 
s.t. 
f;(x)<O, i=l,2, ... ,m, 
where fo is a continuously differentiable convex function and fi,fi, ... ,fm are con-
tinuously differentiable strictly convex functions. Let x* be a feasible solution 
of (P). Suppose that the following condition is satisfied: there exist Yi > 0, i E 
{ 0} U I ( x* ), which are not all zeros such that 
y0Vfo(x*)+ L YiVf;(x*)=O. 
ieJ(x*) 
Prove that x* is an optimal solution of (P). 
11.11. Consider the optimization problem 
min 
CTX 
s.t. 
f;(x) < 0, 
i = 1,2, ... , m, 
where cf. 0 and fi,fi, ... ,fm are continuous over lRn. Prove that if x* is a local 
minimum of the problem, then /(x*) f. 0. 
11.12. Consider the QCQP problem 
(QCQP) 
mm 
s.t. 
i = 1,2, ... ,m, 
where Ao,A1, ... ,Am E lRnxn are symmetric matrices, b0,b1, ... ,bm E lRn, and 
c1, c2, ... , cm E JR. Suppose that x* satisfies the following condition: there exist 
A1, A2, ••• , Am> 0 such that 
(Ao+ tA,A}'+(b0+ tA;b;) =O, 
Ai [ (x*)7 A;(x*) + 2bf x* + ci J = 0, 
i = 1,2, ... , m, 
(x*)7 Ai(x*) + 2bf x* + ci < 0, 
i = 1,2, ... , m, 
m 
Ao+ l:AiAi >-0. 
i=l 
Prove that x* is an optimal solution of (QCQP). 

Chapter 12 
Duality 
12.1 • Motivation and Definition 
The dual problem, which we will formally define later on, can be motivated as a way to 
find bounds on a given optimization problem. We will begin with an example. 
Example 12.1. Consider the problem 
mm xf +xi +2xi 
s.t. 
Xi +x2 = 0. 
(P) 
Problem (P) is of course not a difficult problem, and it can be solved by reducing it into a 
one-dimensional problem by eliminating x2 via the relation x2 =-xi, thus transforming 
the objective function to 2xf + 2xi. The (unconstrained) minimizer of the latter function 
is Xi=-~, and the optimal solution is thus(-!,~) with a corresponding optimal value 
off* =-4· 
The theoretical exercise that we wish to make is to find lower bounds on the value of 
the problem by solving unconstrained problems. For example, the unconstrained prob-
lem derived by eliminating the single constraint is 
(P 0) 
minx; +xi + 2xi. 
The optimal value of (P0) is a lower bound on the value of the optimal value of (P). We 
can write this fact by the following notation: 
val(P0) < val(P). 
The optimal solution of the convex problem (P 0) is attained at the stationary point xi = 
-1,x2 = 0 with a corresponding optimal value of -1 (which is indeed a lower bound 
onf*). 
In order to find other lower bounds, we use the following trick. Take a real number 
µ and consider the following problem, which is equivalent to problem (P): 
min xf +xi+ 2xi +µ(xi+ x2) 
s.t. 
Xi + x2 = 0. 
(12.1) 
Now we can eliminate the equality constraint and obtain the unconstrained problem 
(Pµ) 
minx;+xi+2xi +µ(xi +x2). 
237 

238 
Chapter 12. Duality 
We have that for all µ E JR 
val(P µ) < val(P). 
The optimal solution of (P µ) is attained at the stationary point ( x1, x2) = (-1- ~, - ~ ), 
and the corresponding optimal value, which we denote by q(µ ), is 
( 
µ )2 ( µ )2 
( 
µ ) 
µ2 
q(µ)=val(Pµ)= -1- 2 + -2 +2 -1-2 +µ(-l-µ)=-2-µ-1. 
For example, q(O) = -1 is the lower bound obtained by (P0). What interests us the 
most is the best (i.e., largest), lower bound obtained by this technique. The best lower 
bound is the solution of the problem 
(D) 
max{q(µ): µ E JR}. 
This problem will be called the dual problem, and by its construction, its optimal value 
is a lower bound on the optimal value of the original problem (P), which we will call the 
primal problem: 
val(D) < val(P). 
In this case the optimal solution of the dual problem is attained at µ = -1, and the corre-
sponding optimal value of the dual problem is-!, which is exactly f*, meaning that the 
best lower bound obtained by the this technique is actually equal to the optimal value f*. 
Later on, we will refer to this property as "strong duality" and discuss the conditions 
under which it holds. 
I 
12.1.1 • Definition of the Dual Problem 
Consider the general model 
f* =min f(x) 
s.t. 
gi(x) < 0, 
i = 1,2, ... , m, 
h;(x)=O, j = 1,2, ... ,p, 
(12.2) 
xeX, 
where f, gi, h;(i = 1,2, ... , m,j = 1,2, ... ,p) are functions defined on the set X C Rn. 
Problem (12.2) will be referred to as the primal problem. At this point, we do not assume 
anything on the functions (they are not even assumed to be continuous). The Lagrangian 
of the problem is 
m 
p 
L(x,A,µ)=f(x)+ LAigi(x)+ Lµ;h;(x) 
(xEX,AElR~,µeJRP), 
i=l 
j=l 
where A1, A2, Am are nonnegative Lagrange multipliers associated with the inequality con-
straints, and µ 1, µ2, ... , µP are the Lagrange multipliers associated with the equality con-
straints. The dual objective function q: JR~ x ]RP-+ RU {-oo} is defined to be 
q()., µ) = minL(x, A,µ). 
xeX 
(12.3) 
Note that we use the "min" notation even though the minimum is not necessarily attained. 
In addition, the optimal value of the minimization problem in (12.3) is not always finite; 

12.1. Motivation and Definition 
239 
there are values of (A,µ) for which q( A, µ) = -oo. It is therefore natural to define the 
domain of the dual objective function as 
dom(q) = {(l, µ) E JR~ x JRP: q(A, µ) >-oo}. 
The dual problem is given by 
q* = 
max q(l, µ) 
s.t. 
(A,µ) E dom(q). 
(12.4) 
We begin by showing that the dual problem is always convex; it consists of maximizing a 
concave function over a convex feasible set. 
Theorem 12.2 (convexity of the dual problem). Consider problem (12.2) with f,gi,hj 
( i = 1, 2, ... , m, j = 1, 2, ..• , p) 'being finite-valued functions defined on the set X C Rn, and 
let q 'be the function defined in (12.3). Then 
(a) dom(q) is a convex set, 
{b) q is a concave function over dom( q ). 
Proof. (a) To establish the convexity of dom(q), take (11, µ 1),(12, µ 2) E dom(q) and a E 
[O, 1]. Then by the definition of dom(q) we have that 
minL(x, 11, µ 1) >-oo, 
xeX 
minL(x, 12, µ 2) > -oo. 
xeX 
(12.5) 
(12.6) 
Therefore, since the Lagrangian L(x, A,µ) is affine with respect to A,µ, we obtain that 
q(al1 +(1-a)A2,aµ 1 +(1-a)µ 2)=minL(x,a11 +(1-a)A2,aµ 1 +(1-a)µ 2) 
:x:EX 
= ~j-[ aL(x, 11, µ 1) + {1-a)L(x, 12, µ 2) J 
> aminL(x,11,µ 1) + (1-a)minL(x,12, µ 2) 
xeX 
xeX 
= aq(l1,µ 1)+(1-a)q(l2, µ 2) 
>-oo. 
Hence, a( 11, µ 1) + ( 1-a)( 12, µ 2) E dom( q ), and the convexity of dom( q) is established. 
{b) As noted in the proof of part (a), L(x, A,µ) is an affine function with respect to 
(A,µ). In particular, it is a concave function with respect to (A, µ ). Hence, since q( A,µ) is 
the minimum of concave functions, it must be concave. This follows immediately from 
the fact that the maximum of convex functions is a convex function (Theorem 7 .38). 
D 
Note that -q is in fact an extended real-valued convex function over R~ x RP as de-
fined in Section 7.7, and the effective domain of-q is exactly the domain defined in this 
section. The first important result is closely connected to the motivation of the construc-
tion of the dual problem: the optimal dual value is a lower bound on the optimal primal 
value. This result is called the weak duality theorem, and unsurprisingly, its proof is rather 
simple. 

240 
Chapter 12. Duality 
Theorem 12.3 (weak duality theorem). Consider the primal problem (12.2) and its dual 
problem (12.4). Then 
q* <f*, 
where q* ,f* are the optimal dual and primal values respectively. 
Proof. Let us denote the feasible set of the primal problem by 
S = {x EX : &i(x) < 0, hi(x) = 0, i = 1, 2, ... , m,j = 1, 2, ... , p }. 
Then for any {A,µ) e JR~ x ]RP we have 
q(A, µ) = minl(x, A,µ) 
xeX 
< minl(x,A,µ) 
xeS 
=min [/(x)+ i:iigi(x)+ ±µih/x)] 
xeS 
. 1 
. 1 
i= 
J= 
< min/(x), 
xeS 
where the last inequality follows from the fact that Ai > 0 and for any x ES, &i(x) < 
0, hi(x) = 0 (i = 1,2, ... , m,j = 1,2, ... ,p). We thus obtain that 
q(A,µ) < min/(x) 
xeS 
for any (A,µ) E JR~ x JRP. By taking the maximum over (A,µ) E JR~ x JRP, the result 
follows. 
0 
The weak duality theorem states that the dual optimal value is a lower bound on the 
primal optimal value. Example 12.1 illustrated that the lower bound can be tight. How-
ever, the lower bound does not have to be tight, and the next example shows that it can 
be totally uninformative. 
Example 12.4. Consider the problem 
mm 
s.t. 
It is not difficult to solve the problem. Substituting x1 = x~ into the objective function, we 
obtain that the problem is equivalent to the unconstrained one-dimensional minimization 
problem 
An optimal solution exists since the objective function is coercive. The stationary points 
of the latter problem are the solutions to 
6x~-6x2 =0, 
that is, 
6xi(xi-1) = 0. 

12.2. Strong Duality in the Convex Case 
241 
Hence, the stationary points are x2 = 0,±1, and thence the only candidates for the opti-
mal solutions are (0, 0), ( 1, 1 ), (-1, -1 ). Comparing the corresponding objective function 
values, it follows that the optimal solutions of the problem are (1, 1),(-1,-1) with an 
optimal value off* = -2. 
Let us construct the dual problem. The Lagrangian function is 
L(x1,x2,µ) = x:-3xi + µ(x1 -x~) = x: + µx1 -3xi- µx~. 
Obviously, for any µ E JR 
minL(x1, x2, µ) = -oo, 
X1,X2 
and hence the dual optimal value is q* = -oo, which is an extremely poor lower bound 
on the primal optimal value f * = -2. 
I 
12.2 •Strong Duality in the Convex Case 
In the convex case we can prove under rather mild conditions that strong duality holds; 
that is, the primal and dual optimal values coincide. Similarly to the derivation of the 
KKT conditions, we will rely on separation theorems in order to establish the result. 
The strict separation theorem (Theorem 10.1) from Section 10.1 states that a point can 
be strictly separated from any closed and convex set. We will require a variation of this 
result stating that a point can be separated from any convex set, not necessarily closed. 
Note that the separation is not strict, and in fact it also includes the case in which the 
point is on the boundary of the convex set and the theorem is hence called the supporting 
hyperplane theorem. 
Theorem 12.5 (supporting hyperplane theorem). Let C C Rn be a convex set and let 
y <;. C. Then there exists 0 f:. p E JR n such that 
pT x < pTyforany xe C. 
Proof. Since y <;. int(C), it follows that y ~ int(cl(C)). (Recall that by Lemma 6.30 
int(C) = int(cl(C)).) Therefore, there exists a sequence {Ykh>t satisfying Yk <;. cl(C) 
such that Yk --.. y. Since cl(C) is convex by Theorem 6.27 and-closed by its definition, 
it follows by the strict separation theorem (Theorem 10.1) that there exists 0 f:. Pk e JRn 
such that 
T 
T 
pkx<pkyk 
for all x E cl( C). Dividing the latter inequality by I IPk 11 f:. 0, we obtain 
PT 
ll~ll(x-yk) < 0 for any xecl(C). 
(12.7) 
Since the sequence { I l:k 11 } k;?: 1 is bounded, it follows that there exists a subsequence { I l:k 11 } keT 
such that ~ 
--.. p as k .!..+ oo for some p E JRn. Obviously, I IPI I = 1 and hence in particu-
lar pf:. 0. Taking the limit ask.!..+ oo in inequality (12.7), we obtain that 
pT(x-y) < 0 for any XE cl(C), 
which readily implies the result since C C cl( C). 
0 

242 
Chapter 12. Duality 
We can now deduce a separation theorem between two disjoint convex sets. 
Theorem 12.6 (separation of two convex sets). 
Let Cp C2 C Rn be two nonempty 
convex sets such that C1 n C2 = 0. Then there exists 0 f:. p E Rn for which 
pr x < pT y for any xe CpyE C2• 
Proof. The set C1 - C2 is a convex set {by part (a) of Theorem 6.8), and since C1 n C2 = 0, 
it follows that 0 ~ C1 - C2• By the supporting hyperplane theorem (Theorem 12.5), it 
follows that there exists 0 f:. p e Rn such that 
pT(x-y) <pro= O for any xe C1,ye C2, 
which is the same as the desired result. 
D 
We will now derive a result which is a nonlinear version of Farkas lemma. The main 
difference is that a Slater-type condition must be assumed. Later on, this lemma will be 
the key in proving the strong duality result. 
Theorem 12.7 (nonlinear Farkas lemma). 
Let X C Rn be a convex set and let 
f ,g1, g2, ••• , gm be convex functions over X. Assume that there exists i eX such that 
Let c E IR. Then the following two claims are equivalent. 
(a) The following implication holds: 
xeX, 
g;(x)<O, 
i=1,2, ... ,m~f(x)>c. 
{b) There exist A1, Ai, ... , Am> 0 such that 
(12.8) 
Proof. The implication {b) ~ (a) is rather straightforward. Indeed, suppose that there exist 
A1, A2, ... , Am > 0 such that (12.8) holds, and let x EX satisfy gi(x) < 0, i = 1, 2, ... , m. 
Then by {12.8) we have that 
m 
/(x)+ LAig;(x) > c, 
i=1 
and hence, since g;(x) < 0, Ai> 0, 
m 
/(x) > c-LAig;(x) > c. 
i=1 
To prove that (a)~ {b), let us assume that the implication (a) holds. Consider the follow-
mg two sets: 
S={u=(u0,u1, ... ,um):3xeX s.t. /(x)< u0,g;(x)< u;,i = 1,2, ... ,m}, 
T = {(uo,U1, ... ,um): Uo < c,ul < O,u2 < o, ... ,um < O}. 

12.2. Strong Duality in the Convex Case 
243 
Note that S, Tare nonempty and convex and in addition, by the validity of implication 
(a), S n T = 0. Therefore, by Theorem 12.6 (separation of two convex sets), it follows 
that there exists a vector a= (a0,ap ... ,am) f; 0, such that 
(12.9) 
First note that a> 0. This is due to the fact that if there was a negative component, say 
a; < 0, then by taking u; to be a negative number tending to -oo while fixing all the other 
components as zeros, we obtain that the right-hand-side maximum in (12.9) is oo, which 
is impossible. Since a > 0, it follows that the right-hand side is ace, and we thus obtain 
m 
min 
Lajuj >a0c. 
(Uo,Ut , ... ,um)ES ·=0 
,_ 
(12.10) 
Now we will show that a0 > 0. Suppose in contradiction that a0 = 0. Then 
However, since we can take u; = g;(i), i = 1,2, ... , m, we can deduce that 
m 
Laj gj(x) > o, 
j=1 
which is impossible since g/i) < 0 for all j, and there exists at least one nonzero compo-
nent in (a1,ai, ... ,am)· Since ao > 0, we can divide (12.10) by etc to obtain 
where aj = ~- Define 
S = {u = (u0, u1, ... ,um): 3xeX s.t. /(x) = u0, g;(x) = u;, i = 1,2, ... ,m }. 
Then obviously S C S. Therefore, 
which combined with (12.11) yields the desired result 
min{/(x)+ i:ajg/x)} >c. 
D 
xeX 
. 1 
1= 
(12.11) 
We are now able to show a strong duality result in the convex case under a Slater-type 
condition. 

244 
Chapter 12. Duality 
Theorem 12.8 (strong duality of convex problems with inequality constraints). Con· 
sider the optimization problem 
f* =min f(x) 
s.t. 
gi(x)<O, i=l,2, ... ,m, 
xeX, 
(12.12) 
where X is a convex set and f, gi, i = 1, 2, ... , m, are convex functions over X. Suppose that 
there exists x e X for which gi(x) < d, i = 1,2, ... , m. Suppose that problem (12.12) has a 
finite optimal value. Then the optimal value of the dual problem 
q* = max{q(.J.):). e dom(q)}, 
where 
q(.J.) = minL(x, .J.), 
xeX 
is attained, and the optimal values of the primal and dual problems are the same: 
f* =q*. 
(12.13) 
Proof. Since f * > -oo is the optimal value of (12.12), it follows that the following impli-
cation holds: 
xeX, 
g;(x)<O, i=1,2, ... ,m~f(x)>f*, 
and hence by the nonlinear Farkas' lemma (Theorem 12.7) we have that there exist 
11, A2, ... , Am> 0 such that 
q(l)=min{f(x)+ t~jg/x)} > f*, 
xeX 
. 1 
1= 
which combined with the weak duality theorem (Theorem 12.3) yields 
q* > q( l) > f * > q*. 
Hence f * = q* and l is an optimal solution of the dual problem. 
D 
Example 12.9. Consider the problem 
mm 
s.t. 
The problem is convex but does not satisfy Slater's condition. The optimal solution is 
obviously x1 = x2 = 0 and hence f* = 0. The Lagrangian is 
L(xpx2,A)=x:-x2 +Axi (A>O), 
and the dual objective function is 

12.2. Strong Duality in the Convex Case 
245 
The dual problem is therefore 
The dual optimal value is q* = 0, so we do have the equality f * = q*. However, the 
strong duality theorem (Theorem 12.8) states that there exists an optimal solution to the 
dual problem, and this is obviously not the case in this example. The reason why this 
property does not hold is that in this example Slater's condition is not satisfied-there 
does not exist x2 for which xi < o. 
I 
Example 12.10. ([9]) Consider the convex optimization problem 
min e-xz 
S.t. 
v,....x.,,...: _+_x....,,.;- X1 < 0. 
Note that the feasible set is in fact 
The constraint is always satisfied as an equality constraint, and thus Slater's condition is 
not satisfied. Since x2 is necessarily 0, it follows that the optimal value is f * = 1. The 
Lagrangian of the problem is 
L(x1, Xi' ..l.) = e-x2 + ..l.( J x: + xi-x1) 
(..l. > 0). 
The dual objective function is 
We will show that this minimum is 0, no matter what the value of A is. First of all, 
L(x1,x2, ..l.) > 0 for all x1,x2 and hence q(..l.) > 0. On the other hand, for any e > 0, if we 
X2-E2 
take X2 =-lnE,Xt = r, 
We have 
Therefore, 
x2+e2 
2 
2E 
=e. 
X2-E2 
2 
X2-E2 
2 
2E 
2e 
L(x1,x2,..l.) = e-xz + ..l.( J x: +xi-x1) = e + AE = (1 + ..l.)e. 
Consequently, q( ..l.) = 0 for all A> 0. The dual problem is therefore the following "trivial" 
problem: 
max{O:). > O}, 

246 
Chapter 12. Duality 
whose optimal value is obviously q* = 0. We thus obtained that there exists a duality gap 
f* -q* = 1, which is a result of the fact that Slater's condition is not satisfied. 
I 
We can also derive the complementary slackness conditions under the sole assumption 
that q* = f* (without any convexity assumptions). 
Theorem 12.11 (complementary slackness conditions). Consider problem (12.12) and 
assume that f* = q*, where q* is the optimal value of the dual problem given by {12.13). If 
x*, ).* are optimal solutions of the primal and dual problems respectively, then 
Proof. We have 
x* E argminL(x, ).*), 
;.; gi(x*) = 0, i = 1,2, ... , m. 
m 
q* = q().*) = mjp-L(x, ).*) < L(x*, ).*) = f(x*) + L: ;.; gi(x*) < f (x*) = f*, 
x 
i=1 
where the last inequality follows from the fact that ..l; > 0, gi(x*) < 0. Therefore, since 
q* = f*, it follows that all the inequalities in the above chain of inequalities and equali-
ties are satisfied as equalities, meaning that x* 
E 
argm~eXL(x, ).*) and that 
~7:: 1 Aj gi(x*) = 0, which by the fact that ;.; > 0, gi(x*) < 0, implies that ;.; gi(x*) = 0 
for all z = 1,2, ... ,m. 
D 
Finer analysis can show, for example, the following strong duality theorem that deals 
with linear equality and inequality constraints as well as nonlinear constraints. 
Theorem 12.12. Consider the optimization problem 
f* =min f(x) 
s.t. 
gi(x)<O, i=l,2, ... ,m, 
hi(x)<O, j=l,2, ... ,p, 
(12.14) 
sk(x) = 0, 
k = 1,2, ... ,q, 
xEX, 
where X is a convex set and f, gi, i = 1, 2, ... , m, are convex functions over X. The functions 
hi, sk, j = 1, 2, ... , p, k = 1, 2, ... , q, are affine functions. Suppose that there exists x E int(X) 
for which gi(x) < O,i = 1,2, ... ,m,h/x) < O,j = 1,2, ... ,p,sk(x) = O,k = 1,2, ... ,q. 
Then if problem {12.14) has a finite optimal value, then the optimal value of the dual problem 
q* =max{ q()., r;, µ): ()., r;, µ) E dom(q) }, 
where q : IR~ x IR~ x IRq --+IR U {-oo} is given by 
is attained, and the optimal of the primal and dual problems are the same: 
f* = q*. 

12.3. Examples 
247 
Example 12.13. In this example we will demonstrate the fact that there is no "one" dual 
problem for a given primal problem, and in many cases there are several ways to construct 
the dual, which result in different dual problems and dual optimal values. Consider the 
simple two--dimensional problem 
mm 
s.t. 
It is not difficult to verify (for example, by finding all the KKT points) that the optimal 
solution of the problem is (x1,x2) = (~,~)and the optimal primal value is thus f• = 
(/! )3 + (~)3 = ~· We will consider two possible options for constructing a dual problem. 
I we take the underlying set as 
X = {(x1, x2): x1, x2 > O}, 
then the primal problem can be written as 
mm 
s.t. 
Since the objective function is convex over X, it follows that this problem is in fact a 
convex optimization problem. Therefore, since Slater's condition is satisfied here (e.g., 
(x1'x2) = (1, 1) e X satisfy x1 + x2 > 1), we conclude from Theorem 12.8 that strong 
duality will hold. The dual problem in this case is constructed by associating a single 
dual variable A to the linear inequality constraint x1 + x2 > 1. A second option for writ-
ing a dual problem is to take the underlying set X as the entire two-dimensional space 
and associate Lagrange multipliers with each of the three linear constraints. In this case, 
the assumptions of the strong duality theorem do not hold since xi + x~ is not a convex 
function over 1R2• The Lagrangian is therefore (A, 7]1, 7]2 E lR+) 
L(x1, x2, A, 7]1, 7]2) =xi+ xi-..il(x1 + x2 -1)-7]1 x1 -1}2x2• 
Since the minimization of a cubic function over the real line is always -oo, it follows that 
minl(x1, x2, A, 7]1, 7]2) =min[ xi-(..il+ 7]1)x1] +min[ xi-(A + 7]2)x2] +A 
X1,X2 
X1 
X2 
=-00-00+..il=-oo. 
Hence, q• = -oo and the duality gap is infinite. The conclusion is that the way the dual 
problem is constructed is extremely important and may result in very different duality 
gaps. 
I 
12.3 •Examples 
12.3.1 • Linear Programming 
Consider the linear programming problem 
mm 
CTX 
s.t. 
Ax< b, 

248 
Chapter 12. Duality 
where c E Rn ,A E JRmxn, and b E Rm. We will assume that the problem is feasible, and 
under this condition, Slater's condition given in Theorem 12.12 is satisfied so that strong 
duality holds. The Lagrangian is (A > 0) 
L(x,A)=cTx+AT(Ax-h)=(c+AT A)Tx-bT A, 
and the dual objective function is given by 
T T 
T 
{ 
bTA 
c+ATA=O, 
q(A)=minL(x,A)=min(c+A A) x-b A= 
-
' else. 
xe!Rn 
xe!Rn 
-00 
Therefore, the dual problem is 
max -bT A 
s.t. 
AT A =-c, 
A> 0. 
As already mentioned, Slater's condition is satisfied if the primal problem is feasible and 
under the assumption that the optimal value is finite, strong duality holds. 
12.3.2 • Strictly Convex Quadratic Programming 
Consider the following general strictly convex quadratic programming problem 
min xTQx+2fT x 
s.t. 
Ax< b, 
(12.15) 
where Q E JRnxn is positive definite, f E Rn ,A E JRmxn, and b E Rm. The Lagrangian of 
the problem is 
To find the dual objective function we need to minimize the Lagrangian with respect to 
x. The minimizer of the Lagrangian is attained at the stationary point of the Lagrangian 
which is the solution to 
VxL(x*,A) = 2Qx* +2(AT A+f) = 0, 
and hence 
x* = -Q-1(f +AT A). 
(12.16) 
Substituting this value back into the Lagrangian we obtain that 
q(A) = L(x*, A) 
=(f+AT A)TQ-1QQ-1(f+AT A)-2(f+AT A)TQ-1(f+AT A)-2bT A 
=-(f+AT A)TQ-l(f+AT A)-2bT A 
=-AT AQ-1AT A-2fTQ-1AT A-fTQ-=1f-2bT A 
=-AT AQ-1AT A-2(AQ-1f+bf A-fTQ-1f. 
The dual problem is 
max{q(A): A> O}. 

12.3. Examples 
249 
This problem is also a convex quadratic problem. However, its advantage over the primal 
problem (12.15) is that its feasible set is "simpler." In fact, we can develop a method for 
solving problem (12.15) which is based on an orthogonal projection method applied to 
the dual problem. As an illustration, we develop a method for computing the orthogonal 
projection onto a polytope defined by a set of inequalities. 
Example 12.14. Given a polytope 
S = { x E Rn : Ax < b}, 
where A E Rmxn, b E Rm, we wish to compute the orthogonal projection of a given 
pointy. As opposed to affine spaces, on which the orthogonal projection can be computed 
by a simple formula like the one derived in Example 10.10 of Section 10.2, there is no 
simple expression for the orthogonal projection onto polytopes, but using duality we 
will show that a method finding the projection can be derived. For a given y E Rn, the 
problem of computing P5(y) can be written as 
min llx -yll2 
s.t. 
Ax<b. 
This problem fits into the general model (12.15) with Q =I and f = -y. Therefore, the 
dual problem is (omitting constants) 
max -AT AAT A-2(-Ay+bf A 
s.t. 
A> o. 
We assume that S is nonempty, and under this assumption, by Theorem 12.12, strong 
duality holds. We can solve the dual problem by the orthogonal projection method (pre-
sented in Section 9.4). If we use a constant stepsize, then it can be chosen as f, where Lis 
the Lipschitz constant of the gradient of the objective function given by L = 2Amax(AA T). 
The general step of the method would then be 
If the method stops at iteration N, then by (12.16) the primal optimal solution (up to a 
tolerance) is 
x*=y-AT AN. 
A MATLAB function implemeting this dual-based method is described below. 
function x=proj~olytope(y,A,b,N) 
%INPUT 
%================ 
% y 
% A 
% b 
% N 
%OUTPUT 
%================ 
n-length vector 
mxn matrix 
m-length vector 
number of iterations 
% x ................. x=P_S(y), where S = {x: Ax<=b} 

250 
d=size (A); 
m=d (1); 
n=d ( 2) : 
lam= zeros (m, 1) : 
L=2*max(eig(A*A')); 
g=A*y-b; 
for k=l:N 
end 
lam=lam+2/L*(-A*(A'*lam)+g); 
lam=max (lam, O) : 
x=y-A' *lam; 
Consider for example the set 
S = {(x1, x2): x1+x2 <1, x1 >0, x2 > O}. 
Chapter 12. Duality 
This is the triangle in the plane with vertices (0,0),(0, 1),(1,0). Suppose that we wish 
to find the orthogonal projection of (2,-1) onto S. Then taking 100 iterations of the 
dual-based method gives the solution, which is the vertex (1,0): 
>> A=[l,l;-l,0;0,-1]; 
>> b= [ 1 i 0 i 0] i 
>> y=[2;-l]; 
>> x=proj_polytope(y,A,b,100) 
x = 
1.0000 
-0.0000 
An interesting property of the method can be seen when applying only a few iterations. 
For example, after 5 iterations the estimated primal solution is 
>> x=proj_polytope(y,A,b,5) 
x = 
1.5926 
-0.3663 
This is of course not a feasible solution of the primal problem. In fact, all the iterates 
are nonfeasible, as illustrated in Figure 12.1, but they converge to the optimal solution, 
which is of course feasible. This was a demonstration of the fact that dual-based methods 
generate nonfeasible points with respect to the primal problem. 
I 
12.3.3 • Dual of Convex Quadratic Problems 
Now we will consider problem (12.15) when Q is positive semidefinite rather than posi-
tive definite. In this case, since Q is not necessarily invertible, the latter construction of 
the dual problem is not possible. To write a dual, we will use the fact that since Q >- 0, it 
follows that there exists a matrix DE ]RnXn such that Q = oro. Therefore, the problem 
can be recast as 
min xTDTDx+2fT x 
s.t. 
Ax< b. 

12.3. Examples 
. 
. 
....... ·........ 
. . ...... ··-:·· ····. ·····:·· 
1.5 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
· ·· · ·· ··· ···· ··· · ··· ·· ·· ·· ·· ···· .
............................ . 
. 
. 
. 
. 
. 
. 
0.5 .... ......... . 
. 
. 
········>·· ····· ····· ·:· ······· ·· . 
. 
. 
- 0.5 
. 
. 
•. <· .... .... .. ..• ; • • • • . . . . . . ·:- . • 
. 
. 
. 
"'•: 
ii'"' 
. * 
·i·······* ···· 
- 1~~-'-~~--'-~--'~~---'-~---' 
- 0.5 
0 
0.5 
1.5 
2 
251 
Figure 12.1. First 30 iterations of the dual-based method {denoted by asterisks) for finding the 
orthogonal projection. 
We can now rewrite the problem by using an additional variables vector z as 
min~.z 
s.t. 
I lzll2 + 2fT X 
Ax<b, 
z=Dx. 
The Lagrangian of the new reformulation is 
(AE IR~,µ E IRn) 
L(x,z,).,µ) = llzll2 +2fT x+2).T(Ax-b)+2µT(z-Dx) 
= llzll2+2µT z+2(f+AT A-DT µ)T x-2bT ).. 
The Lagrangian is separable with respect to x and z, and we can thus perform the mini-
mizations with respect to x and z separately: 
Hence, 
min(f+AT A-DT µ)T x = { O, 
x 
-00 
min llzll2 + 2µ T z = -11µ112. 
z 
f+AT ).-DT µ = 0, 
else, 
q().,µ) = minl(x,z,).,µ) = { -llµll2-2bT )., f +AT ).-DT µ = 0, 
x,z 
-oo 
else. 
The dual problem is thus 
12.3.4 • Convex QCQPs 
max -llµll2-2bT). 
s.t. 
f+AT A-DT µ = 0, 
). E IR~,µ E IRn. 
Consider the QCQP problem 
mm 
s.t. 
xT A0x+2h~ x+c0 
xT Aix+2bi x+ci < 0, 
i = 1,2,. . .,m, 

252 
Chapter 12. Duality 
where Ai >- 0 is an n x n matrix, bi E IR.n, ci E IR., i = 0, 1, ... , m. We will consider two 
cases. 
Case I: When Ao >- 0, then the dual can be constructed as follows. The Lagrangian is 
m 
(AE JR~) L(x,A) = xT Aox+2bcf x+c0 + 2:.A.i(xT Aix+2hf x+ci) 
i=1 
The minimizer of the Lagrangian with respect to x is attained at the point in which its 
gradient is zero: 
Thus, 
Plugging this expression back into the Lagrangian, we obtain the following expression for 
the dual objective function: 
The dual problem is thus 
max -(ho+ L~ 1 A.ibi)T (Ao+ L~ 1 A.iAi)-1 (ho+ L~ 1 A.ibi)+co+ L~ 1 A.ici 
s.t. 
A.i>O, 
i=l,2, ... ,m. 
Case II: When A0 is not positive definite but still positive semidefinite, the above dual 
is not well-defined since the matrix Ao+ l:~ 1 Ai Ai is not necessarily invertible. However, 
we can construct a different dual by decomposing the positive semidefinite matrices Ai as 
Ai = DfDi (Di E Rnxn) and writing the equivalent formulation 
mm xTDJ'D0x+2hJ' x+c0 
s.t. 
xTDfDix + 2bf x + ci < 0, 
i = 1,2, ... , m, 
Now, we can add additional variables zi E IR.n ( i = 0, 1, 2, ... , m) that will be defined to be 
zi = D;x, giving rise to the formulation 
mm 
s.t. 
llzoll: + 2b~ x + c0 
• _ 
llz;ll +2h; x+c; <O, 
t -1,2, ... ,m, 
zi = D;x, i = 0, 1, ... , m. 

12.3. Examples 
The Lagrangian is (A. E JR~, µi E Rn, i = 0, 1, ... , m) 
L(x, Zo, ... 'zm, A., µo, ... 'µm) 
m 
m 
= llz01!2+2bcf x+c0 + LA;(llz;ll2 +2bf x+ci)+2 L:µf (z; -D;x) 
i=l 
i=O 
m 
+co+ L:ciAi. 
i=l 
Note that for any A E lR+, µ E Rn we have 
{ -~, 
g(A, µ) = 
mjnAllzll2 +2µ 7 z = 
o, 
-oo, 
A>O, 
A=O,µ= 0, 
A=O,µ#O. 
253 
Since the Lagrangian is separable with respect to Z; and x, we can perform the minimiza-
tion with respect to each of the variable vectors, 
~n[llzoll2 +2µcf zo] = g(1,µo) =-llµoll 2, 
°!in[ Ai llzill2 +2µ"{ zi J = g(Ai, µi), 
I 
. (b 
~m ) b 
~m or )T 
{ 0, 
ho+ L~t Ai bi -:E~oDf µi = 0, 
min o + ..1'..ii=t Ai i -
..1'..ii=O 
i µi 
x = 
-oo else, 
and deduce that 
q(.A., µ0, ••• , µm) = min L(x, z0, ••• ,zm, A., µ0, ••• , µm) 
x,:zo, ... ,zm 
- { g( 1, µo) + L~t g( Ai, µi) +Co+ L~1 ci .A.i 
if ho+ L~t A; bi - :E~o Df µi = 0, 
-
-oo 
else. 
The dual problem is therefore 
max g( 1, µ 0) + :E~ 1 g( Ai, µi) + c0 + :E~ 1 C; A; 
s.t. 
b0 + :E~ 1 A; bi - :E~o Df µi = 0, 
A. E JR~' µo, ... 'µm E Rn. 
12.3.5 • Nonconvex QCQPs 
Consider now the QCQP problem 
min x7 A0x+2b[ x+c0 
s.t. 
x7 A;x+2bf x+ci < 0, 
i = 1,2, ... ,m, 
where Ai = Af E Rnxn, b; E Rn, ci E JR, i = 0, 1, ... , m. We do not assume that Ai are 
positive semidefinite, and hence the problem is in general nonconvex, and the techniques 

254 
Chapter 12. Duality 
used in the convex case are not applicable. We begin by forming the Lagrangian (). E JR~): 
m 
L(x,).) =xT A0x+2h6 x+c0 + L:Ai (xT Aix+2bf x+ci) 
i=l 
The main idea is to use the following presentation of the dual objective function: 
q(,l.) = minl(x, ).) =max{ t: L(x, ).) > t for any x E lRn}. 
x 
t 
{12.17) 
The above equation essentially states that the minimal value of L(x, ).) over x E lRn is 
in fact the largest lower bound on the function. We will now use Theorem 2.43 on the 
characterization of the nonnegativity of quadratic functions and deduce that the claim 
L(x, A)> t for all x E lRn 
is equivalent to 
( Ao+ L~1 AiAi 
ho+ L~t Ai bi ) >- 0 
(ho+ L~t Aibi)T 
Co+ L~t Aici -t -
' 
which combined with {12.17) yields that a dual problem is 
t 
s.t. 
( Ao+ L~t AiAi 
ho+ L~1 Ai b; ) >- 0 
(ho+ L~t Aibi)T co+ L~1 Aici - t 
-
' 
{12.18) 
Ai >O, 
i = 1,2, ... ,m. 
The above problem is convex as a dual problem, but since the primal problem is noncon-
vex, strong duality is of course not guaranteed. We also note that the form of the dual 
problem {12.18) is different from all the dual problems derived so far in the sense that 
not all the constraints are presented as inequality or equality constraints but instead are 
of the form B0 + :E~ 1 AiBi >- 0, where B; are given matrices. This type of a constraint 
is called a linear matrix inequality (abbreviated LMI) . Optimization problems consist-
ing of minimization or maximization of a linear function subject to linear inequalities/ 
equalities and LMis are called semidefinite programming (SDP) problems, and they are 
part of a larger class of problems called conic problems; see also Section 12.3.9. 
12.3.6 • Orthogonal Projection onto the Unit-Simplex 
Given a vector y E lRn, we would like to compute the orthogonal projection of the vector 
y onto Jln. The corresponding optimization problem is 
mm llx-yll2 
s.t. 
er x = 1, 
x>O. 
We will associate a Lagrange multiplier A E JR to the linear equality constraint er x = 1 
and obtain the Lagrangian function 
L(x,A) = llx-yll2 +2A(eT x-1) = llxll2-2(y-Aef x+ llYll2-2A 
n 
= ?:<xf-2(yj -A)xj) + llYll2-2A. 
1=1 

12.3. Examples 
255 
The arising problem is therefore saparable with respect to the variables xi and hence 'the 
optimal xi is the solution to the one-dimensional problem 
min[x~-2(y1
- -A)x1-]. 
x >O 
J 
J-
The optimal solution to the above problem is given by 
and the optimal value is-[yi -A]~. The dual problem is therefore 
By the basic proprties of dual problems, the dual objective function is concave. In order 
to actually solve the dual problem, we note that 
Therefore, since -g is a coercive and differentiable function, it follows that there exists 
an optimal solution to the dual problem attained at a point A in which 
g'(..{) = o, 
meaning that 
n 
L:[Yi - A]+ = 1. 
i=l 
The function h(A) = I:j=1[yi -A]+ -1 is a nonincreasing function over JR and is in fact 
strictly decreasing over (-oo, maxi y i ]. In addition, by denoting Ymax = maxi=l,l, ... ,n Yi, Ymin 
= minj=l,2, ... ,n Yi' we have 
h (Ymax) = -1, 
h (Ymin - _:) = .t Yi - nymin + 2-1 > 0, 
n 
. 1 
J= 
and we can therefore invoke a bisection procedure to find the unique root A of the function 
hover the interval [Ymin -1,ymax] and then define P1::.. (y) = [y-Ae]+· The MATLAB 
n 
n 
implementation follows. 
function xp=proj_unit_simplex(y) 
% INPUT 
% ====================== 
% y ........ a column vector 
% OUTPUT 
% ====================== 
% xp ....... the orthogonal projection of y onto the unit simplex 

256 
f=@(lam)sum(max(y-larn,0))-1; 
n=length (y) ; 
lb=min(y)-2/n; 
ub=max (y); 
larn=bisection(f,lb,ub,le-10); 
xp=max (y-larn, 0) ; 
Chapter 12. Duality 
As a sanity check, let us compute the orthogonal projection of the vector (-1, 1,0.3f 
onto A3, 
>> x.p=proj_unit_simplex([-1;1;0.3]) 
xp = 
0 
0.8500 
0.1500 
and compare it with the result of CVX, 
cvx_begin 
variable x(3) 
minimize(norm(x-(-1;1;0.3])) 
sum(x)==l 
x>=O 
cvx_end 
which unsurprisingly is the same: 
>> x 
x = 
0.0000 
0.8500 
0.1500 
12.3. 7 • Dual of the Chebyshev Center Problem 
Recall that in the Chebyshev center problem (see also Section 8.2.4) we are given a set of 
points a1, a2, ... , am E JR n, and we seek to find a point x E Rn, which is the center of the 
minimum radius ball containing the points 
mlflx,r 
s.t. 
r 
llx-adl < r, 
i = 1,2, ... , m. 
Finding a dual problem to this formulation is not an easy task, but we can actually consider 
a different equivalent formulation to which a dual can be constructed in an easier way. The 
problem can be recast as 
mlflx,r r 
s.t. 
llx-aill2 < y, 
i = 1,2,. . .,m. 
where r denotes the squared radius. The problems are equivalent since minimization of 
the radius is equivalent to minimization of the squared radius. The Lagrangian is(). E JR~) 
m 
L(x, y, ).) = r + L:..li(llx-a;ll2-r) 
i=t 

12.3. Examples 
257 
The minimization of the above expression must be -oo unless I:?::1 Ai = 1, and in this 
case we have 
minr(1-i:.Ai) =O. 
r 
i=t 
We are therefore left with the task of :finding the optimal value of 
m 
min~ A-llx-a·ll2 
x.£.....J' 
' 
i=l 
when A. E Am. Since the objective function of the above minimization can be written as 
(12.19) 
it follows that the minimum is attained at the point in which the gradient vanishes, mean-
. 
mg at 
m 
x* = 2: Aiai =AA., 
i=l 
(12.20) 
where A is the n x m matrix whose columns are the vectors a1, a2, ••• , am. Substituting 
this expression back into (12.19), we have that the dual objective function is 
m 
m 
q(A.) = llAA.!12-2(AA.f (AA.)+ 2: A;llaill2 = -llAA.112 + 2: A.;lladl2-
i=t 
i=l 
The dual problem is therefore 
max -llAA.112 + I:?::1 A;lla;ll2 
s.t. 
AE Am. 
We can actually write a MATLAB function that solves this problem. For that, we will use 
the gradient projection method with a constant stepsize J:, where L = 2AmaxCA7 A) is the 
Lipschitz constant of the gradient of the objective function. At each iteration we will also 
use the MATLAB function proj_unit_simplex to find the orthogonal projection 
onto the unit-simplex. Note that the derived method is also a dual-based method and that 
it incorporates another dual-based method for computing the projection. 
function [xp,r]=chebyshev_center(A) 
% INPUT 
% ====================== 
% A ........ a matrix whose columns are points in RArn 
% OUTPUT 
% ====================== 
% xp ........ the Chebyshev center of the points 
% r ......... radius of the Chebyshev circle 
d=size (A) ; 
rn=d(2); 
Q=A' *A; 
L=2*max(eig(Q)); 

258 
b=sum(A."2) '; 
%initialization with the uniform vector 
lam=l/m*ones(m,1); 
old_lam=zeros(m,1); 
while (norm(lam-old_lam)>le-5) 
end 
old_lam=lam; 
lam=proj_unit_simplex(lam+l/L*(-2*0*lam+b)); 
xp=A*lam; 
r=O; 
for i=l:m 
r=max(r,norm(xp-A(:,i))); 
end 
Chapter 12. Duality 
Example 12.15. Returning to Example 8.14, suppose that we wish to find the Chebyshev 
center of the 5 points 
(-1,3), (-3,10), (-1,0), (5,0), (-1,-5). 
For that, we can invoke the MATLAB function chebyshev_center that was just de-
scribed: 
A=[-1,-3,-l,5,-1;3,10,0,0,-5]; 
[xp,r]=chebyshev_center(A); 
The Chebyshev center and radius are 
>> xp 
xp = 
-2.0000 
2.5000 
>> r 
r = 
7.5664 
These are the exact same results obtained by CVX in Example 8.14. 
I 
12.3.8 • Minimization of Sum of Norms 
Consider the problem 
(12.21) 
where A; E JRk, xn, h; E JRk,, i = 1,2, ... , m. At a first glance, it seems that it is not possible 
to find a dual of an unconstrained problem. However, we can use a technique of variable 
decoupling to reformulate the problem as a problem with affine constraints. Specifically, 
problem (12.21) is the same as 
mi°x,y, 
s.t. 
L~111Yill 
Y. =A·x+h· 
Z 
I 
Z' 
i=1,2, ... ,m. 

12.3. Examples 
259 
Associating a Lagrange multiplier vector A; E Rk, with the ith set of constraints Yi = 
A; x +bi, we obtain the following Lagrangian: 
By the separability of the Lagrangian with respect to x, y 1, y 2, ••• , y m, it follows that the 
dual objective function is given by 
Obviously, 
min[-(:E~1 A! A;)7 x] = { O, 
xeJRn 
Z-
z 
-00 
In addition, we have for any a e Rk 
minllYll +aT y = { 0, 
ye!Rk 
-oo, 
~~ 1A! ).. = 0, 
£..Ai= 
z 
z 
else. 
llall < 1, 
llall > 1. 
(12.22) 
{12.23) 
To prove this result, note that when llall < 1, we have by the Cauchy-Schwarz inequality 
that for any y E Rk 
aT Y > -llall · llYll > -llyll 
and hence llYll + a7 y > 0 for any y E Rk, and in addition llOll + a7 0 = 0, implyinp that 
mi°yelRk llYll + aT y = 0. If llall > 1, then taking Ya = -aa we obtain that llYall +a Ya = 
allall(l -llall)-+ -oo as a-+ oo, establishing the result {12.23). We thus conclude that 
for any i = 1,2, ... , m 
min [llY·ll + ;.! Y·] = { o, 
llA;ll < 1' 
Y; e!Rk, 
' 
z 
' 
-oo else, 
which combined with (12.22) implies that the the dual objective function is 
11).dl < 1,i = 1,2, .. .,m, 
L~1 Af A; = o, 
else. 
The dual problem is therefore 
max - L~t bf ).i 
s.t. 
:E~ 1 Af A; = 0, 
11).ill < 1, 
i = 1,2, .. .,m. 

260 
Chapter 12. Duality 
12.3.9 • Conic Duality 
A conic optimization problem is a convex optimization problem of the form 
min a7 x 
(C) 
s.t. 
Ax= b, 
xeK, 
where KC Rn is a closed and convex cone and a E :!Rn ,A E JRmxn, b E :!Rm. Our aim is to 
find an expression for the dual problem. For that, let us associate a Lagrange multipliers 
vector y E JR m with the equality constraints Ax = b, leading to the Lagrangian 
L(x,y)= a7 x-y7 (Ax-b) =(a-A7 y)7 x+b7 y. 
To construct the dual problem, we need to minimize the Lagrangian over x EK: 
q(y) = minL(x,y) = b7 y+min(a-A7 y)7 x. 
xeK 
xeK 
We will now use the following easily verifiable fact: for any d E ]Rn one has 
. dT 
{ 0, 
deK*, 
mm 
x= 
dd:.K* 
xeK 
-00, 
~ 
· 
where K* is the dual cone defined in Exercise 6.11 as 
K* = {z E ]Rn : z7 x > 0 for any x EK}. 
The latter fact is almost a tautology. If d E K*, then by the definition of the dual cone, 
d7 x > 0 for any x E K and also d7 0 = 0 (recall that 0 E K since K is a closed cone), 
and hence minxeK d7 x = 0. On the other hand, if d fl. K*, then it means that there exists 
Xo EK such that d7 Xo < 0. Therefore, taking any a> 0 we have that a:xa EK, and we 
obtain that d7 ( a:xa) = a( d7 Xo) -+ -oo as a tends to oo, and hence minxeK d7 x = -oo. 
To conclude, the dual objective function is 
The dual problem is thus 
-{ b7 y, a-A7 yeK*, 
q(y)-
-oo, a-A7yf/.K*. 
(DC) 
max b7y 
s.t. 
a-A7 yeK*. 
We can now invoke the strong duality theorem for convex problems (Theorem 12.12) and 
state one of the versions of the so-called conic duality theorem. 
Theorem 12.16 (conic duality theorem). Consider the primal and dual problems (C) and 
(DC). Suppose that thare exists x E int(K) such that Ax= band that problem (C) is bounded 
below. Then the dual problem (DC) has an optimal solution, and we have 
val(C) = val(DC). 

12.3. Examples 
261 
12.3.1 O • Denoising 
Suppose that we are given a signal contaminated with noise. In mathematical terms the 
model is 
y=x+w, 
where x E Rn is the noise-free signal, w E Rn is the unknown noise vector, and y E Rn is 
the observed and known vector. An example of "clean" and "noisy" signals can be found 
in Figure 12.2 
1.5 
1 
0.5 
0 
- 0.5 
-1 
-1 .5 0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
1.5 
1 ~ 
0.5 
o . 
- 0.5 
- 1 
- 1.5 0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
Figure 12.2. True signal (top image} and noisy signal {bottom image}. 
The plots were created by the MATLAB commands 
randn{ 'seed' ,314); 
t=linspace{0,1,1000) '; 
n=length ( t) ; 
x=sin{lO*t); 
figure{l) 
plot{t,x) 
axis([0,1,-1.5,1.5]); 
y=x+O.l*randn(size{t)); 
figure{2) 
plot(t,y) 

262 
Chapter 12. Duality 
The objective is to reconstruct the true signal from the observed vector y. A common 
approach for denoising is to use some prior information on the true image. A natural 
information is the smoothness of the signal. This information can be incorporated by 
adding a quadratic penalty function that measures in some sense the smoothness of the 
signal. For example, a standard approach is to solve the optimization problem 
n-1 
min I Ix -yl 12 + A 2:< xi - xi+ 1 )2, 
i=l 
where A > 0 is some predetermined regularization parameter. The problem can also be 
written as 
min llx -yll2 + Al 1Lxll2, 
(12.24) 
where 
1 -1 
0 
0 
0 
0 
0 
1 
-1 
0 
0 
0 
0 
0 
1 
-1 
0 
0 
L= 0 
0 
0 
1 
-1 
0 
0 
0 
0 
0 
0 
1 
-1 
Problem (12.24) is a regularized least squares problem (see Section 3.3), and its optimal 
solution can be derived by writing the stationarity condition 
2(x-y)+2ALTLx= 0. 
Thus, 
(12.25) 
The solution of the problem in the case of A= 1 can thus be obtained by the MATLAB 
commands 
L=sparse (n-1, n) ; 
for i=l:n-1 
L(i,i)=l; 
L(i,i+l)=-1; 
end 
lambda=lOO; 
xde=(speye(n)+lambda*L'*L)\y; 
figure(3) 
plot ( t, xde) ; 
resulting in the relatively good reconstruction given in Figure 12.3. The quadratic regu-
larization method does not work so well for all types of signals. Suppose, for example, 
that we are given a noisy step signal generated by the MATLAB commands 
randn('seed',314); 
x=zeros(1000,1); 
x(1:250)=1; 
x(251:500)=3; 
x(501:750)=0; 
x(751:1000)=2; 

12.3. Examples 
figure(l) 
plot(l:lOOO,x, '. ') 
axis([0,1000,-1,4]); 
y=x+0.05*randn(size(x)); 
figure(2) 
plot(l:lOOO,y, '. ') 
The "true" and "noisy" step signals are given in Figure 12.4. 
1 
0.5 
0 
- 0.5 
- 1 
- 1.5'-------'-------'~---'-~---'-~--"-~-'-~-'-~_._~__,_~~ 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
Figure 12.3. Denoising of the sine signal via quadratic regularization. 
4 ~~~~~~~~~~~~~~~~~~~~~ 
3.5 
3 
2.5 
2 
1.5 
1 1---------
0.5 
0 
- 0.5 
- 1 ~~~~~~~~~~~~~~~~~~--'-~~ 
0 
1 00 
200 
300 
400 
500 
600 
700 
800 
900 1000 
3 . 5 ~~~~~~~~~~~~~~~~~~~~~ 
3 
2.5 
2 
1.5 
0.5 
0 
- 0 . 5 '--~..__~_._~_._~_.__~_._~_.__~_._~_._~_.__~_, 
0 
100 
200 
300 
400 
500 
600 
700 
800 
900 1000 
Figure 12.4. True signal (top image) and noisy signal (bottom image). 
263 

264 
Chapter 12. Duality 
Unfortunately, the quadratic regularization approach does not yield good results, no 
matter what the value of the chosen regularization parameter A is. Indeed, the regular-
ized least squares solution (12.25) is not a good reconstruction since it is unable to deal 
correctly with the three breakpoints. The reason is that the jumps contribute large values 
to the penalty function I ILxl 12 since their values are squared. Therefore, in a sense the regu-
larized least squares solution tries to "smooth" the jumps, resulting in the reconstructions 
in Figure 12.5. 
4.---~-~-~-----.---. 
3 
2 
1 ............. 
0 
..... "' 
;.. ...... ,,. 
4~-~-~-~-----.-~ 
3 
2 
1..-.--
0 
.. . ', 
- 1 0 
200 
400 
600 
800 
1000 
- 1 '---~-~-~-__.-~ 
0 
200 
400 
600 
800 
1000 
4 .---~-~-~-----.---. 
3 
2 
! 
j 
1---
0 
! 
4 ~-~-~-~-----.-~ 
3 
2 
1- - -
0 
( 
\ __ ) 
- 1 '---~-~-~-__.-~ 
- 1
'---~-~-~ 
_ __. _ __, 
0 
200 
400 
600 
800 
1000 
0 
200 
400 
600 
800 
1000 
Figure 12.5. Quadratic regularization with ). = 0.1 (top left), ). = 1 (top right), ). = 10 
(bottom left), ). = 100 {bottom right}. 
Another approach for denoising that is able to overcome this disadvantage is to solve 
the following problem in which the regularization term is in the /1 norm: 
min llx-yll2 + AllLxlli · 
(12.26) 
We would like now to construct a dual to problem (12.26). For that, note that the problem 
is equivalent to the optimization problem 
ml°x,z 
s.t. 
llx-yll2 + Allzll1 
z=Lx. 
The Lagrangian of the problem is 
L(x,z, µ) = llx-yll2 + Allzll1 + µT (Lx-z) 
= llx-yll2 +(LT µ)T x+Allzll1-µT Z. 
The Lagrangian is separable with respect to x and z and thus we can perform the mini-
mization separately. The minimum of I Ix - YI 12 + (LTµ f x over x is attained when the 
gradient vanishes, 
2(x-y)+LT µ=0, 
and hence x = y-! LTµ. Substituting this value back to the x-part of the Lagrangian, we 
obtain 

12.3. Examples 
265 
In addition, 
min .Allzll - µ T z = { 0, 
llµlloo <A, 
z 
1 
-oo else. 
To conclude, the dual objective function is given by 
q(µ)=minL{x,z,µ)= { -~µTLLT µ+µTLy, 
llµlloo <A, 
x,z 
-oo 
else. 
Therefore, the dual problem is 
max 
-~µTLLT µ+µTLy 
s.t. 
I lµlloo < A. 
(12.27) 
Since the feasible set of the dual problem is a box, we can employ the gradient projection 
method in order to solve it. For that, we need to know an upper bound on the Lipschitz 
constant of its gradient. To find such an upper bound, note that 
III.xii' = ~( 
:t; - :t;+ 1 )2 < 2 ( ~ x; + ~ 
x?+i) < 41 lxll'. 
Therefore (see also part (i) of Exercise 1.13) 
AmaxCLLT) = Amax{LTL) < 4. 
Hence, since the Lipschitz constant of the gradient of the objective function of (12.27) is 
! Amax(LLT), it follows that an upper bound on the Lipschitz constant is 2. The conse-
quence is that we can employ the gradient projection method on problem (12.27) with 
constant stepsize ~,and the convergence is guaranteed by Theorems 9.16 and 9.18. Ex-
plicitly, the method will read as 
where 
C = { z E Rn-t : -A< zi < A, i = 1, 2 ... , n -1}. 
If the result of the gradient projection method is µ*, the primal optimal solution (up to 
some tolerance) will be x* = y-~ LTµ*. Following is a short MATLAB code that employs 
1000 iterations of the gradient projection method: 
lambda=l; 
mu=zeros (n-1, 1); 
for i=l:lOOO 
mu=mu-0.25*L*(L'*mu)+0.5*(L*Y); 
mu=lambda*mu./max(abs(mu),lambda); 
xde=y-0.S*L'*ffiU; 
end 
figure(5) 
plot(t,xde, '.'); 
axis([0,1,-1,4]) 
and the result is given in Figure 12.6. This result is much better than any of the quadratic 
regularization reconstructions, and it captures the breakpoints very well. 

266 
Chapter 12. Duality 
4 
I 
I 
I 
I 
I 
I 
I 
I 
I 
3.5 
-
3 
-
2.5 
2 -
1.5 -
1 
0.5 >-
-
0 >-
-
- 0.5 -
-
- 1 
I 
I 
I 
I 
I 
I 
I 
I 
I 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 
Figure 12.6. Result of denoising via an 11 norm regularization. 
12.3.11 • Dual of the Linear Separation Problem 
In Section 8.2.3 we considered the problem of finding a maximal margin hyperplane 
that separates two sets of points. We will assume that the given classified points are 
x1,x2, ••• ,xm E JR.n. For each i, we are given a scalar Yi which is equal to 1 if X; is in 
class A or -1 if it is in class B. The linear separation problem is given by 
min 
~llwll2 
s.t. 
Y;(wT X; + /3) > 1, 
i = 1,2, ... , m. 
(12.28) 
The disadvantage of the formulation (12.28) is that it is only relevant when the two classes 
of points are linearly separable. However, in many practical situations the two classes are 
not linearly separable, and in this case we need to find a formulation in which violation 
of the constraints is allowed and at the same time a penalty term is added to the objective 
function that is equal to the sum of the violations of the constraints. The new formulation 
is as follows: 
mm 
~llwll2+C1:~1 ~; 
s.t. 
Y;(wT X; + /3) > 1-~i' i = 1,2, ... ,m, 
~>O, i=l,2, ... ,m, 
where C > 0 is a given parameter. We will rewrite the problem in a slightly different 
form: 
mm 
~llwll2 + C(eT {) 
s.t. 
Y(Xw + f3e) > e-{, 
{>O, 
where Y = diag(y1, y2, ••• , y m) and X is the m x n matrix whose rows are xf, xJ", ... , x~. 
We begin by constructing the Lagrangian (a E JR~) 
1 
L(w,(3,{,a) = -llwll2+C(eT {)-aT [YXw+ f3Ye-e+{] 
2 
1 
= -llwll2-wT[XTYa]-f3(aTYe)+{T(Ce-a)+aT e. 
2 

12.3. Examples 
267 
The Lagrangian is separable with respect to w,(3 and~ and therefore 
q(a) = [ m,tn~llw1!2-wT[XTYa]] +[ iJn(-/3(aTYe))]+ [ !~~T(Ce-a)] +are. 
Since 
. 1 
1 
~n 2 llwll2-wT[XTYa] = - 2aTYXXTYa, 
min(-(3(aTYe)) = { 0, 
aTYe = 0, 
f3 
-oo else, 
min~ (Ce-a)= 
' 
T 
{ 0 
a< Ce, 
~ ~o 
-oo else, 
it follows that the dual objective function is given by 
{ aTe-laTYXXTYa 
aTYe=O,O<a<Ce, 
q(a) = 
2 
' 
-oo 
else. 
The dual problem is therefore 
max aT e-laTYXXTYa 
2 
aTYe=O, 
s.t. 
O<a<Ce. 
We can also write the dual problem in the following way: 
max 
L~=t ai- ~ L~1Lf=t aiajyiy/xf xi) 
s.t. 
L~1Yiai = 0, 
O<ai<C, i=1,2, ... ,m. 
12.3.12 • A Geometric Programming Example 
A geometric programming problem is an optimization problem of the form 
min /(x) 
s.t. 
g;(x) < 0, 
h/x)=O, 
xeJR~+· 
i = 1,2, ... ,m, 
j=1,2, ... ,p, 
where f, g1, g2, ... , gm are posynomials and h1, h2, ... , h P are monomials . In the context 
of geometric programming, a monomial is a function</>: JR~+-+ JR of the form </>(x) = 
cITj=1x;', where c > 0 and a1'a2, ... ,an E JR. A posynomial is a sum of monomials. 
Geometric programming problems are not convex, but they can be easily transformed 
into convex optimization problems. In addition, their dual can be explicitly derived by an 
elegant argument. Instead of showing the derivations in the most general setting, we will 
illustrate them using a simple example. Consider the geometric programming problem 
mm 
s.t. 

268 
Chapter 12. Duality 
We will make the transformation ti= ex,, which transforms the problem into 
s.t. 
e-X1-X2-X3 + eX2+X3 
eln2+Xt +X3 + eXt +x2 < 4. 
The transformed problem is convex, and we have thus shown how to transform the non-
convex geometric programming problem into a convex problem. To find a dual of this 
problem, we will consider the following equivalent problem: 
mm 
eY1 +eY2 
s.t. 
eYJ + eY4 < 4, 
Yt =-Xi -X2-X3, 
Y2 = X2 +x3, 
y3 = x1 + x3 + ln 2, 
Y4 = X1 +x2· 
(12.29) 
We will now construct the Lagrangian. The first constraint will be associated with the 
nonnegative multiplier w, and with the four linear constraints, we associate the Lagrange 
multipliers u1, u2, u3, u4: 
L{y,x, w, u) = eY1 +eY2 + w (eY3 +eY4 -4)- u1(y1 +x1 + x2 +x3) 
-u2(Y2-X2-X3)-u3{y3-X1 -X3-ln2)-u4{y4-X1 -x2) 
= [eYt - U1Y1] + [eY2 -U2Y2] 
+ [ weY3 - u3y3] + [ weY4 - u4y4] 
-X1(U1 -U3-U4)-x2{U1 -U2-U4)-x3{U1 -U2-U3) 
+{ln2)u3 -4w. 
We will use the following simple and technical lemma. Note that we use the convention 
that OlnO = 0. 
Lemma 12.17. Let).> 0 anda E Ilt Then 
a-aln(j), 
min [ ).eY -ay] = 
yElR 
0, 
-oo, 
-oo, 
If). > 0 and a > 0, then the optimal y is y = ln ( j) . 
). > 0, a> 0, 
A=a =0, 
.A.> 0, a< 0, 
A= o, 
a> o. 
Proof. If ). = 0, then obviously, the minimum is 0 if and only if a = 0, and otherwise it is 
-oo. If ). > 0, then 
min[ ).eY -ay] = Amin[eY - a y], 
yElR 
yElR 
A 
and the optimal solutions of both minimization problems are the same. If a < 0, then the 
minimum is -oo since taking y -+ -oo we obtain that the objective function goes to -oo. 
If a = 0 the (unattained) minimal value is 0. li a > 0, then the optimal solution is attained 
at the stationary point which is the solution to eY = j, that is, at y = ln( j). Substituting 
this expression back to the objective function we obtain that the optimal value is 

12.3. Examples 
Based on Lemma 12.17 and the relations 
min[-x1(u1 -u3-u4)] = { O, 
x, 
-oo 
min[-x2(u1 -u2-u4)] = { O, 
X2 
-00 
min[-x3(u1-u2-u3)J={ O, 
X3 
-00 
we obtain that the dual problem is given by 
U1 -
U3 - u4 = 0, 
else, 
u1-U2-u4 =0, 
else, 
u1 -
U2 -
u3 = 0, 
else, 
269 
max 
u1 -u1 lnu1 + u2-u2lnu2 + u3-u3 ln(~) + u4 -u4 ln(~) +(ln2)u3-4w 
S.t. 
u1 -U3-U4 =0, 
U1 - u2 - u4 = 0, 
Ut -
U2 -
U3 = 0, 
Ut, U2, U3, u4, W > 0. 
To make the problem well-defined and in order to be consistent with Lemma 12.17, the 
function -u ln (:) has the value 0 when u = w = 0 and the value -oo when u > 0, w = 0. 
It is interesting to note that we can actually solve the dual problem. Indeed, noting 
that the expression in the objective function that depends on w is 
we deduce that at an optimal solution w = "3!"4 • The constraints of the dual problem 
imply that u2 = u3 = u4 and that u1=2u2. Therefore, denoting the joint value of u2, u3, u4 
by a(> 0), we conclude that u1 = 2a and w = I. Plugging this into the dual, we obtain 
that the dual problem is reduced to the one-dimensional problem 
max{3(1-ln2)a-3a ln(a): a> O}. 
The optimal solution of this problem is attained at the point at which the derivative van-
ishes, 
3(1-ln2)-3-3lna = 0, 
that is, at a = ! , and hence 
We can also find the optimal solution of the primal problem. For that, we will first com-
pute the optimal y1,y2,y3,y4: 
y1 = In u1 = 0, 
y2 = ln u2 = - ln 2, 
y3 = ln ( : ) = ln 2, 
y 4 = ln ( ~ ) = ln 2. 
Hence, by the constraints of (12.29) we have 
X1 + X2 + X3 = 0, 
X2 + x3 = - ln 2, 
X1 +x3 =0, 
x1 +x2 = ln2, 

270 
Chapter 12. Duality 
whose solution is x1 = ln2,x2 = O,x3 = -ln2. Therefore, the optimal solution of the 
primal problem is 
1 
t -
eX3 -
-
3-
-
. 
2 
Exercises 
12.1. Find a dual problem to the convex problem 
min x: + 0.5x; + X1 X2 - 2X1 - 3X2 
s.t. 
x1+x2 <1. 
Find the optimal solutions of both the dual and primal problems. 
12.2. Write a dual problem to the problem 
Solve the dual problem. 
12.3. Consider the problem 
mm x1 -4x2+xj 
s.t. 
x1 + x2 + x~ < 2 
X1 >O 
x2 >0. 
min xi +2x; +2x1x2+x1 -x2-x3 
s.t. 
x1 +x2 +x3 < 1 
X3<1. 
(i) Is the problem convex? 
(ii) Find an optimal solution of the problem. 
(iii) Find a dual problem and solve it. 
12.4. Consider the primal optimization problem 
4 
2 2 
mm x1 -
x2 -x2 
s.t. 
xi +xi + x2 < 0. 
(i) Is the problem convex? 
(ii) Does there exist an optimal solution to the problem? 
(iii) Write a dual problem. Solve it. 
(iv) Is the optimal value of the dual problem equal to the optimal value of the 
primal problem? Find the optimal solution of the primal problem. 
12.5. Consider the problem 
mm 3xi +x1x2 +2xi 
s.t. 
3xi + X1 X2 + 2xi + X1 - X2 > 1 
x1>2x2. 
(i) Is the problem convex? 
(ii) Find a dual problem. Is the dual problem convex? 

Exercises 
12.6. Find a dual to the convex optimization problem 
where A E Rmxn,b E Rm. 
mm 
s.t. 
"'~ (x. lnx. -x·) 
~i=l ' 
' 
' 
Ax<b, 
x>O, 
12.7. Find a dual problem to the following convex minimization problem: 
. 
mm 
s.t. 
where a,a ER~+,b E Rn. 
12.8. Consider the convex optimization problem 
mm 
s.t. 
where c > O,A E Rmxn, b E Rm. Find a dual problem. 
12.9. Consider the following problem (also called second order cone programming): 
min gT x 
s.t. 
llAix+ hill< cf x+di, 
i = 1,2, ... ,k, 
271 
wheregERn,A; ERm,xn,bi ERm1,ci ERn,di ER,i = 1,2, ... ,k. Find a dual 
problem. 
12.10. Consider the primal optimization problem 
mm 
s.t. 
whereaER~+,cER~+'b ER++· 
"'n 
5., 
~j=l X; 
aT x< b, 
x>O, 
(i) Find a dual problem with a single dual decision variable. 
(ii) Solve the dual and primal problems. 
12.11. Consider the following optimization problem in the variables a E R and q E Rn: 
mm a 
(P) 
s.t. 
Aq = af 
llqll2 < E', 
where A E Rmxn,f E Rm,e- ER++· Assume in addition that the rows of A are 
linearly independent. 
(i) Explain why strong duality holds for problem (P). 
(ii) Find a dual problem to problem (P). (Do not assign a Lagrange multiplier to 
the quadratic constraint.) 

272 
Chapter 12. Duality 
(iii) Solve the dual problem obtained in part (ii) and find the optimal solution of 
problem (P). 
12.12. Lat a E JR~+ and consider the problem 
min L?=1-log(xi +ai) 
s.t. 
L?=t xi = 1 
x>O. 
Find a dual problem with one dual decision variable. Is strong duality satisfied? 
12.13. Let a1,a2,a3 E Rn, and consider the Fermat-Weber problem 
min{llx-adl + llx-~11 + llx-a311}. 
Find a dual problem. 
12.14. Find a dual problem to the following primal one: 
min 
L?=txiln(~)+llxll2+2aTx 
s.t. 
xT Ax+2bT x+ c < 0, 
where a E JR~+' a E Rn, A >- 0, b E Rn, c E JR. Under what condition is strong 
duality guaranteed to hold? Find a condition that is written explicitly in terms of 
the data. 
12.15. Lat a1,a2, ... ,am E lRn and b1, b2, ... , bm E JR and consider the the l,roblem of 
finding the so-called analytic center of the polytope S = { x E Rn : ai x < bi, i = 
1,2, ... , m} given by 
(A) min{-t,ln(b;-a[° x): xe S}. 
Find a dual problem to (A). 
12.16. Let f: Rn-... JR be defined as (k is a positive integer smaller than n) 
k 
/(x) = I:x[i]' 
i=l 
where x[i] is the ith largest values in the vector x. We have seen in Example 7.27 
that f is convex. 
(i) Show that for any x E Rn, we have that /(x) is the optimal value of the prob-
lem 
max xry 
(P) 
s.t. 
er y = k 
O<y<e. 
(ii) For any a E JR show that /(x) <a if and only if there exist A E JR~ and u E JR 
such that 
ku +er A< a,ue+ A> x 
(iii) Let Q E lRnxn be a positive definite matrix. Find a dual to the problem 
mm xTQx 
s.t. 
/(x) <a. 

Exercises 
273 
12.17. Consider the inequality constrained problem 
f* = mm f(x) 
s.t. 
gi(x) < 0, 
i = 1,2, ... , m, 
where f,g1,g2, ... ,gm are convex functions over JRn. Suppose that there exists 
x E JRn such that 
gi(x}<O, 
i = 1,2, ... ,m, 
and that the problem is bounded below, i.e., f * > -oo. Consider also the dual 
problem given by 
max{q(A): AE dom(q}}, 
where q(A) = mi11,cL(x,A},dom(q) ={A E JR~: minL(x,A) > -oo}. Let A* be an 
optimal solution of the dual problem. Prove that 
i:;.; < . f(x)-f* 
,. . 
. 
mm-
12 
(-g-(x)) 
z=l 
i= , , ... ,m 
i 
12.18. Consider the optimization problem (with the convention that OlnO = 0) 
mm x1 + 2x2 + 3x3 + 4x4 + Lt=l xi In xi 
s.t. 
Lt=i xi = 1 
Xp x2, x3, X4 > 0. 
(i) Show that the problem cannot have more than one optimal solution. 
(ii) Find a dual problem in one dual decision variable. 
(iii) Solve the dual problem. 
(iv) Find the optimal solution of the primal problem. 
12.19. Find a dual problem for the optimization problem 
mm x1 + 2x~ + x; + .../ 4x~ + x1 x3 +xi + 2 
s.t. 
x1 +xi +4xi < 5 
x1 +x2 +x3 < 15. 
12.20. Consider the problem 
mm 
s.t. 
(i) Find a dual problem problem. 
(ii) Is the dual problem convex? 
12.21. Consider the optimization problem 
(P) 
mm -6x1 + 2x2 + 4xi 
s.t. 
2x1 +2x2 +x3 < 0 
-2x1 + 4x2 + x; = 0 
x2 >0 

274 
Chapter 12. Duality 
(i) Is the problem convex? 
(ii) Find a dual problem for (P). Do not assign a Lagrange multiplier to the con-
straint x2 > 0. 
(iii) Find the optimal solution of the dual problem. 
12.22. Let a E R++ and define the set 
Ta= {xe1Rn: .txj = 1,0 < 
xj <a}. 
7=1 
(i) For which values of a is the set Ta nonempty? 
(ii) Find a dual problem with one dual decision variable to the problem of finding 
the orthogonal projection of a given vector y E Rn onto Ta. 
(iii) Write a MATLAB function for computing the orthogonal projection onto 
the set Ta based on the dual problem found in part (ii). The call to the func-
tion will be in the form 
function xp=proj_bound(y,alpha) 
where y is the point which should be projected onto Ta and xp is the resulting 
projection. The function should check whether the set Ta is nonempty. 
(iv) Write a function for finding the orthogonal projection onto Ta which is based 
on CVX. The function call will be in the form 
function xp=proj_bound_cvx(y,alpha) 
(v) Compute the orthogonal projection of the vector (0.5, 0.7, 0.1, 0.3, 0.1 )7 onto 
T0.3 using the MATLAB functions constructed in parts (iii) and (iv). 
(vi) Compare the CPU times of the two functions on a problem with 10000 vari-
ables using the following commands: 
rand ( 'seed' , 314) ; 
x=rand(l0000,1); 
tic,y=proj_bound(x,O.Ol);toc 
tic,y=proj_bound_cvx(x,0.0l);toc 
12.23. Consider the following optimization problem: 
(P) 
min { h(x) = llx-dll2 + ..j x: +xi+ ..j xi+ x; + ..j x; + x; + ..j x; + x: : x E R5}, 
where d = (1,2, 3,2, 1)7 
(a) Find an explicit dual problem of (P) with a "simple" constraint set (meaning 
a set on which it is easy to compute the orthogonal projection operator). 
{b) Run 10 iterations of the gradient projection method on the derived dual prob-
lem. Use a constant stepsize ±,where Lis the corresponding Lipschitz con-
stant. You need to show at each iteration both the dual objective function 
value as well as the objective function of the primal problem (use the rela-
tions between the optimal primal and dual solutions to derive at each itera-
tion a primal solution). Finally, write explicitly the optimal solution of the 
problem. 

Bibliographic Notes 
Chapter 1 For a comprehensive treatment of multidimensional calculus and linear alge-
bra, the reader can refer to [24, 29, 33, 34, 35] and also Appendix A of [9]. 
Chapter 2 The topic of optimality conditions in Sections 2.1-2.3 is classical and can also 
be found in many other books such as [1, 30]. The principal minors criterion is also 
known as "Sylvester's criterion," and its proof can be found for example in [24]. 
Chapter 3 A comprehensive study of least squares methods and generalizations can be 
found in [11]. The discussion on circle fitting follows [ 4]. An excellent guide for 
MATLAB is the book [20]. 
Chapter 4 The gradient method is discussed in many books; see, for example, (9, 27, 31]. 
More background and convergence results on the Gauss-Newton method can be found in 
the book [28]. The original paper of Weiszfeld appeared in (39]. The analysis in Section 
4.6 follows [6]. 
Chapter 5 More details and further extensions of Newton's method can be found, for 
example, in [9, 15, 17, 27, 28]. The hybrid Newton's method can be found in [38]. An 
excellent reference for an abundant of optimization methods is the book [28]. 
Chapters 6 and 7 A classical reference for convex analysis is (32]. The book [10] also 
contains a comprehensive treatment of the subject. 
Chapter 8 A large variety of examples of convex optimization problems can be found 
in [13] and also in [8]. The original paper of Markowitz describing the portfolio op-
timization model is [23]. The CVX MATLAB software as well as a user guide can be 
found in [19]. The CVX software uses two conic optimization solvers: SeDeMi [36] and 
SDPT3 (37]. The reformulation of the trust region subproblem as a convex problem de-
scribed in Section 8.2.7 follows the paper (11]. 
Chapter 9 Stationarity is a basic concept in optimization that can be found in many 
sources such as (9]. The gradient mapping is extensively studied in [27]. The analysis 
of the convergence gradient projection method follows [5, 6]. The discussion on sparsity 
constrained problems is based on (3 ]. The iterative hard thresholding method was ini-
tially presented and studied in (12]. 
Chapter 10 Generalization and extensions of separation and alternative theorems can be 
found in [32, 10, 21]. The discussion on the orthogonal regression problem follows [ 4]. 
Chapter 11 A comprehensive treatment of the KK.T conditions, including variants that 
were not presented in this book, can be found in (1, 9]. The derivation of the second or-
der necessary conditions follows the paper (7]. Optimality conditions and algorithms for 
solving the trust region subproblem and its generalizations can be found in [26, 25, 16 ]. 
The total least squares problem was initially presented in [18] and was extensively studied 
and generalized by many authors; see the review paper (22] and references therein. The 
derivation of the reduced form of the total least squares problem via the KKT conditions 
follows [2]. 
275 

276 
Bibliographic Notes 
Chapter 12 Duality is a classic topic and is covered in many other optimization books; 
see, for example, [1, 9, 10, 13, 21, 32]. The dual algorithm for solving the denoising prob-
lem is based on Chambolle's algorithm for solving two-dimensional denoising problems 
with total variation regularization [14]. More on duality in geometric programming can 
be found in (30]. 

Bibliography 
[ 1] M. S. Bazaraa, H. D. Sherali, and C. M. Shetty. Nonlinear programming. Theory and algorithms. 
Wiley-Interscience Uohn Wiley & Sons], Hoboken, NJ, third edition, 2006. 
[2] A. Beck and A. Ben-Tal. On the solution of the Tikhonov regularization of the total least 
squares problem. SIAM J Optim., 17(1):98-118, 2006. 
[ 3] A. Beck and Y. C. Eldar. Sparsity constrained nonlinear optimization: Optimality conditions 
and algorithms. SIAM/ Optim., 23(3):1480-1509, 2013. 
[ 4] A. Beck and D. Pan. On the solution of the GPS localization and circle fitting problems. SIAM 
J Optim., 22(1):108-134, 2012. 
[5] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse 
problems. SIAM/ Imaging Sci., 2(1):183-202, 2009. 
[ 6] A. Beck and M. Teboulle. Gradient-based algorithms with applications to signal-recovery 
problems. In Convex optimization in signal processing and communications, pages 42-88. Cam-
bridge University Press, Cambridge, 2010. 
[7] A. Ben-Tal. Second-order and related extremality conditions in nonlinear programming. 
]. Optim. Theory Appl., 31(2):143-165, 1980. 
[8] A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization. Analysis, algorithms, 
and engineering applications. MPS/SIAM Series on Optimization. Society for Industrial and 
Applied Mathematics (SIAM), Philadelphia, PA, 2001. 
[9] D. P. Bertsekas. Nonlinear programming. Athena Scientific, Belmont, MA, second edition, 
1999. 
[10] D. P. Bertsekas. Convex analysis and optimization. Athena Scientific, Belmont, MA, 2003. 
With Angelia Nedic and Asuman E. Ozdaglar. 
[ 11] A. Bjorck. Numerical methods for least squares problems. Society for Industrial and Applied 
Mathematics (SIAM), Philadelphia, PA, 1996. 
[ 12] T. Blumensath and M. E. Davies. Iterative thresholding for sparse approximations. J Fourier 
Anal. Appl., 14(5-6):629-654, 2008. 
[ 13] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, Cambridge, 
2004. 
[ 14 J A. Chambolle. An algorithm for total variation minimization and applications. J Math. 
Imaging Vision, 20(1-2):89-97, 2004. Special issue on mathematics and image analysis. 
[15] R. Fletcher. Practical methods of optimization. VoL 1. Unconstrained optimization. John Wiley 
& Sons, Ltd., Chichester, 1980. A Wiley-lnterscience Publication. 
277 

278 
Bibliography 
[ 16] C. Fortin and H. Wolkowicz. The trust region subproblem and semidefinite programming. 
Optim. Methods Softw., 19(1):41-67, 2004. 
[ 17] P. E. Gill, W. Murray, and M. H. Wright. Practical optimization. Academic Press, Inc. 
[Harcourt Brace Jovanovich, Publishers], London-New York, 1981. 
[18] G. H. Golub and C. F. Van Loan. An analysis of the total least squares problem. SIAM]. 
Numer. Anal., 17(6):883-893, 1980. 
[19] M. Grant and S. Boyd. CVX: MATLAB software for disciplined convex programming, 
version 2.0 beta. http: I I cvxr. com/ cvx, September 2013. 
[20] D. J. Higham and N. J. Higham. MATLAB guide. 
Society for Industrial and Applied 
Mathematics (SIAM), Philadelphia, PA, second edition, 2005. 
[21] 0. L. Mangasarian. Nonlinear programming. McGraw-Hill Book Co., New York, 1969. 
[22] I. Markovsky and S. Van Huffel. Overview of total least-squares methods. Signal Process., 
87(10):2283-2302, October 2007. 
[23] H. Markowitz. Portfolio selection.]. Finance, 7:77-91, 1952. 
[24] C. Meyer. Matrix analysis and applied linear algebra. Society for Industrial and Applied 
Mathematics (SIAM), Philadelphia, PA, 2000. With 1 CD-ROM (Windows, Macintosh and 
UNIX) and a solutions manual (iv+ 171 pp.). 
[25] J. J. More. Generalizations of the trust region subproblem. Optim. Methods Software, 2:189-
209, 1993. 
[26] J. J. More and D. C. Sorensen. Computing a trust region step. SIAM]. Sci. Statist. Comput., 
4(3):553-572, 1983. 
[ 27] Y. Nesterov. Introductory lectures on convex optimization, volume 87 of Applied Optimization. 
Kluwer Academic Publishers, Boston, MA, 2004. A basic course. 
[28] J. Nocedal and S. J. Wright. Numerical optimization. Springer Series in Operations Research 
and Financial Engineering. Springer, New York, second edition, 2006. 
[ 29] J. M. Ortega and W. C. Rheinholdt. Iterative solution of nonlinear equations in several variables, 
volume 30 of Classics in Applied Mathematics. Society for Industrial and Applied Mathemat-
ics (SIAM), Philadelphia, PA, 2000. Reprint of the 1970 original. 
[30] A. L. Peressini, F. E. Sullivan, and J. J. Uhl, Jr. The mathematics of nonlinear programming. 
Undergraduate Texts in Mathematics. Springer-Verlag, New York, 1988. 
[31] B. T. Polyak. Introduction to optimization. Translations Series in Mathematics and Engineer-
ing. Optimization Software Inc. Publications Division, New York, 1987. Translated from the 
Russian, With a foreword by Dimitri P. Bertsekas. 
[32] R. T. Rockafellar. Convex analysis. Princeton Mathematical Series, No. 28. Princeton Univer-
sity Press, Princeton, N.J., 1970. 
[33] W. Rudin. Real analysis. McGraw-Hill, N. Y., 1976. 
[34] S. Strang. Calculus. Wellesley-Cambridge Press, 1991. 
[35] S. Strang. Introduction to linear algebra. Wellesley-Cambridge Press, fourth edition, 2009. 
[36] J. F. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. 
Optim. Methods Softw., 11-12:625-653, 1999. 

Bibliography 
279 
(37] K. C. Toh, M. J. Todd, and R. H. Tiitiincii. SDPT3-a MATLAB software package for semi-
definite programming, version 1.3. Optim. Methods Softw., 11/12(1-4):545-581, 1999. Interior 
point methods. 
(38] L. Vandenberghe. Applied numerical computing. 2011. Lecture notes, in collaboration with 
S. Boyd. 
[39] E. V. Weiszfeld. Sur le point pour lequel la somme des distances den points donnes est mini-
mum. 7he Tohoku Mathematical Journal, 43:355-386, 1937. 

active constraints, 195, 208 
affine function, 118 
argmax, 14 
argmin, 14 
arithmetic geometric mean 
inequality, 139 
backtracking, 51, 178 
basic feasible solution, 107 
bisection, 219 
boundary point, 7 
boundary set, 8 
bounded below, 36 
bounded set, 8 
Caratheodory theorem, 102 
Cauchy-Schwarz, 3 
Chebyshev ball, 153 
Chebyshev center, 153, 162, 256 
Cholesky factorization, 90 
circle fitting, 45 
classification, 151, 266 
closed ball, 6 
closed line segment, 2 
closed set, 7 
closure, 8 
coercive, 25 
compact set, 8 
complementary slackness, 196, 
246 
concave function, 118 
condition number, 58, 60 
cone, 104 
conic combination, 106 
conic duality theorem, 260 
conic hull, 106 
conic problem, 254 
conic representation theorem, 
106 
constant stepsize, 51 
constrained least squares, 218 
Index 
constraint qualification, 210 
continuously differentiable, 9 
contour set, 7 
convex combination, 101 
convex function, 117 
convex hull, 102 
convex optimization, 147 
convex polytope, 100 
convex problem, 147 
convex set, 97 
cvx, 158 
damped Gauss-Newton method, 
67,68 
damped Newton's method, 88 
data fitting, 39 
denoising, 42 
descent direction, 49 
descent directions method, 50 
descentlemma,74 
descent method, 50 
diagonally dominant matrix, 22 
directional derivative, 8 
distance function, 130, 157 
dot product, 2 
dual cone, 114,205 
dual objective function, 238 
dual problem, 237 
effective domain, 135 
eigenvalue, 5 
eigenvector, 5 
ellipsoid, 99 
epigraph, 136 
Euclidean norm, 3 
exact line search, 51 
extended real-valued function, 
135 
extreme point, 111 
facility location, 69 
Farkas lemma, 192 
281 
feasible descent direction, 207 
feasible set, 13 
feasible solution, 13 
Fejer monotonicity, 183 
Fermat's theorem, 16 
Fermat-Weber problem, 68, 80 
finite-valued, 135 
first projection theorem, 157 
Fritz-John conditions, 208 
Frobenius norm, 4 
full column rank, 37 
function 
distance, 130, 157 
dual objective, 238 
Huber, 189 
quadratic, 32 
Rosenbrock, 60 
generalized Slater's condition, 
216 
geometric programming, 267 
global maximum and minimum, 
13 
global optimum, 13 
Gordan's theorem, 194, 205, 208 
gradient, 9 
gradient inequality, 119 
gradient mapping, 177 
gradient method, 52 
gradient projection method, 175 
Hessian, 10 
hidden convexity, 155, 165 
Holder's inequality, 140 
Huber function, 189 
hyperplane, 98 
identity matrix, 2 
ill-conditioned matrices, 60 
indefinite matrix, 18 
indicator function, 135 
induced matrix norm, 4 

282 
inequality 
Jensen's, 118 
Kantorovich, 59 
linear matrix, 254 
Minkowski's, 140 
strongly convex, 117 
inner product, 2 
interior, 7 
interior point, 7 
iterative hard-thresholding, 187 
Jacobian, 67 
Jensen's inequality, 118 
Kantorovich inequality, 59 
KK.T conditions, 195, 207 
KK.T point, 198, 211 
Krein-Milman theorem, 113 
Lagrange multipliers, 196 
Lagrangian function, 198 
least squares, 37 
linear, 45 
regularized, 41, 219, 262 
total, 230 
level set, 7, 130 
line, 97 
line search, 50 
line segment principle, 108 
linear approximation theorem, 10 
linear fitting, 39 
linear function, 118 
linear least squares, 45 
linear matrix inequality, 254 
lin~ar programming, 107, 149, 
247 
linear rate, 60 
Lipschitz continuity of the 
gradient, 73 
local maximum point, 15 
local minimum point, 15 
log-sum-exp, 124 
Lorenz cone, 105 
matrix norm, 3 
maximal value, 13 
maximizer, 13 
minimial value, 13 
minimizer, 13 
Minkowski functional, 113 
Minkowski's inequality, 140 
monomial, 267 
Motzkin's theorem, 205 
negative definite, 18 
negative semidefinite, 18 
neighborhood, 7 
Newton direction, 83 
Newton's method, 64, 83 
damped,88 
pure, 83 
nonexpansive, 174 
nonlinear Farkas lemma, 242 
nonlinear fitting, 40 
nonlinear least squares, 45, 67 
nonnegative orthant, 1 
nonnegative part, 157 
norm 
diagonally dominant, 22 
identity, 2 
indefinite, 18 
induced matrix, 4 
square root of, 21 
strictly diagonally 
dominant, 22 
normal cone, 114 
normal system, 37 
open ball, 6 
open line segment, 2 
open set, 7 
optimal set, 148 
orthogonal projection, 156 
orthogonal regression, 203 
overdetermined system, 37 
partial derivative, 8 
pointed cone, 114 
positive definite, 17 
positive orthant, 2 
positive semidefinite, 17 
posynomial, 267 
primal problem, 238 
principal minors criterion, 21 
proper, 135 
pure Newton's method, 83 
Q-norm, 35 
QCQP, 155, 235 
quadratic approximation 
theorem, 10 
quadratic function, 32 
quadratic problem, 150 
quadratic-over-linear, 125, 127 
quasi-convex function, 131 
Rayleigh quotient, 5, 205, 
232 
regularity, 211 
regularization parameter, 41 
regularized least squares, 41, 219, 
262 
robust regression, 163 
Rosenbrock function, 60 
saddle point, 24 
Index 
scaled gradient method, 63 
second projection theorem, 173 
semidefinite programming, 254 
separation of two convex sets, 242 
separation theorem, 191 
set 
bounded,8 
open,7 
sgn, 156 
singleton, 187 
Slater's condition, 214 
source localization problem, 80 
sparsity constrained problems, 
183 
spectral decomposition, 5 
spectral norm, 4 
square root of a matrix, 21 
standard basis, 1 
stationary point, 17, 169 
strict global maximum, 13 
strict global minimum, 13 
strict local maximum, 15 
strict local minimum, 15 
strict separation theorem, 191 
strictly concave function, 118 
strictly convex function, 117 
strictly diagonally dominant 
matrix, 22 
strong convexity parameter, 144 
strong duality, 241 
strongly convex function, 144, 
190 
sublinear rate, 182 
sufficient decrease lemma, 75, 176 
support, 184 
support function, 136 
supporting hyperplane theorem, 
241 
total least squares, 230 
trust region subproblem, 155, 227 
unit-simplex, 2, 254 
unit-sum set, 171 
Vandermonde, 40 
weak duality theorem, 239 
Weierstrass theorem, 25 
well-conditioned matrices, 60 
Young's inequality, 140 

This book provides the foundations of the theory of nonlinear optimization as well as 
some related algorithms and presents a variety of applications from diverse areas of 
applied sciences. The author combines three pillars of optimization-theoretical and 
algorithmic foundation, familiarity with various applications, and the ability to apply 
the theory and algorithms on actual problems-and rigorously and gradually builds the 
connection between theory, algorithms, applications, and implementation. 
Readers will find 
• more than 170 theoretical, algorithmic, and numerical exercises that deepen and 
enhance the reader's understanding of the topics; 
• several subjects not typically found in optimization books-for example, optimality 
conditions in sparsity<onstrained optimization and hidden convexity; 
• a large number of applications discussed theoretically and algorithmically, such as 
circle fitting, Chebyshev center, the Fermat-Weber problem, denoising, clustering, 
total least squares, and orthogonal regression; and 
• theoretical and algorithmic topics demonstrated by the MATLAB® toolbox CVX and 
a package of m-files that is posted on the book's web site. 
This book is intended for graduate or advanced undergraduate students of mathematics, 
computer science, and electrical engineering as well as other engineering departments. 
The book will also be of interest to researchers. 
Amir Beck is an Associate Professor in the Department of Industrial Engineering at The 
Technion-Israel Institute of Technology. He has published numerous papers, has given 
invited lectures at international conferences, and was awarded the Salomon 
Simon Mani Award for Excellence in Teaching and the Henry Taub Research 
Prize. He is on the editorial board of Mathematics of Operations Research, 
Operations Research, and Journal of Optimization Theory and Applications. 
His research interests are in continuous optimization, including theory, 
algorithmic analysis, and applications. 
for more information about MOS and SIAM books, journals, 
conferences~ memberships, or activities, contact: 
• 
51BJI... 
Society for Industrial 
aAd Applied Mathematics 
3600 Market Street, 6th FloOr 
Philadelphia, PA 19104-2688 USA 
+1-215-382-9800 • Fax +1-215-386-7999 
siamOsiam.org • www.slam.org 
ISBN 978-1-611973-64 - 8 
Mathematic:al Optimizaticn Society 
3600 Market Street, 6th Floor 
~·· 
PA 19104'2.S USA 
+1·215-382-9800 x319 
Fax +1-215-386-7999 
seMc.eOmathoptorg • www.mathQpt.0131 


168
International Series of Numerical Mathematics
Partial Differential 
Equations: 
Modeling, Analysis 
and Numerical 
Approximation
Hervé Le Dret
Brigitte Lucquin


ISNM
International Series of Numerical Mathematics
Volume 168
Managing Editor
G. Leugering, Erlangen-Nürnberg, Germany
Associate Editors
Z. Chen, Beijing, China
R.H.W. Hoppe, Augsburg, Germany; Houston, USA
N. Kenmochi, Chiba, Japan
V. Starovoitov, Novosibirsk, Russia
Honorary Editor
K.-H. Hoffmann, München, Germany
More information about this series at www.birkhauser-science.com/series/4819

Hervé Le Dret
• Brigitte Lucquin
Partial Differential Equations:
Modeling, Analysis and
Numerical Approximation

Hervé Le Dret
Laboratoire Jacques-Louis Lions
Université Pierre et Marie Curie—Paris VI
Paris
France
Brigitte Lucquin
Laboratoire Jacques-Louis Lions
Université Pierre et Marie Curie—Paris VI
Paris
France
ISSN 0373-3149
ISSN 2296-6072
(electronic)
International Series of Numerical Mathematics
ISBN 978-3-319-27065-4
ISBN 978-3-319-27067-8
(eBook)
DOI 10.1007/978-3-319-27067-8
Library of Congress Control Number: 2015955864
Mathematics Subject Classiﬁcation (2010): 35J20, 35J25, 35K05, 35K20, 35L03, 35L05, 65M06,
65M08, 65M12, 65N30
Springer Cham Heidelberg New York Dordrecht London
© Springer International Publishing Switzerland 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made.
Printed on acid-free paper
Springer International Publishing AG Switzerland is part of Springer Science+Business Media
(www.birkhauser-science.com)

Preface
This book is devoted to the study of partial differential equation problems both from
the theoretical and numerical points of view. A partial differential equation (PDE) is
a relation between the partial derivatives of an unknown function u in several
variables to be satisﬁed by this function, for example ou
ot  ou
ox ¼ 0, where u is a
function in the two variables t and x. Partial differential equations constitute a major
ﬁeld of study in contemporary mathematics. They also arise in other ﬁelds of
mathematics, such as differential geometry or probability theory for example. In
addition, partial differential equations appear in a wide variety of contexts in many
applied ﬁelds, not only in the traditional ﬁelds of physics, mechanics and engi-
neering, but also more recently in chemistry, bioscience, medicine, meteorology,
climatology, ﬁnance and so on.
In all of these applied ﬁelds, numerical simulation is playing an increasingly
prominent role, because even though solutions to such PDE problems can be shown
to exist, there is in general no closed form solution. Therefore, quantitative infor-
mation about the solutions can only be obtained by means of numerical approxi-
mation methods. It is very important not to take numerical simulation results at face
value since they inherently present errors. In order to be able to do this, a com-
prehensive knowledge of all the steps involved is necessary, starting from modeling
to mathematical existence theorems, to numerical approximation methods. This is
one of the main purposes of this book.
A numerical approximation method is a procedure in which the original con-
tinuous unknowns are replaced by a ﬁnite number of discrete, computable
approximate unknowns. It is thus important to be able to properly understand on the
one hand the properties of partial differential equations and, on the other hand, the
properties of the numerical methods that are used to deﬁne approximations of their
solutions and effectively compute such approximations. In particular, it is essential
to quantify the approximation by appropriately deﬁning the error between
approximate and continuous unknowns, and to show that this error goes to zero,
preferably at a known rate, in the continuous limit when the number of approxi-
mated unknowns goes to inﬁnity.
v

The goal of this book is to try and illustrate this program through a rather wide
spectrum of classical or not so classical examples. We thus present some modeling
aspects, develop the theoretical analysis of the partial differential equation problems
thus obtained for the three main classes of partial differential equations, and analyze
several numerical approximation methods adapted to each of these examples.
We have selected three broad families of numerical methods, ﬁnite difference,
ﬁnite element and ﬁnite volumes methods. This is far from being exhaustive, but
these three families of methods are the most widely used and constitute the core
skills for anyone intending to work with numerical simulation of partial differential
equations. There are many other numerical methods that we have chosen not to
develop, in order to keep the size of the book within reasonable bounds.
Parts of the book are accessible to Bachelor students in mathematics or engi-
neering. However, most of the book is better suited to Master students in applied
mathematics or computational engineering. We put emphasis on mathematical
detail and rigor for the analysis of both continuous and discrete problems.
The book is structured globally according to the three major types of partial
differential equations: elliptic equations, parabolic equations and hyperbolic equa-
tions. We mainly consider linear equations, except for a few nonlinear examples.
Each one of the above three types of equations requires a speciﬁc set of mathe-
matical techniques for theoretical analysis, and a speciﬁc set of numerical methods,
i.e., speciﬁc discretization procedures and convergence analyses for approximation.
We follow a path of progressive difﬁculty in each case inasmuch as possible. We
begin with the most elementary approaches either theoretical or numerical, which
also happen to be the earliest ones from the historical viewpoint. We then continue
to more advanced topics that require a more sophisticated mathematical back-
ground, both from the theoretical and numerical points of view, and that are also
more recent than the previous ones. We also give along the way several numerical
illustrations of successes as well as failures of numerical methods, using free
software such as Scilab and FreeFem++.
The book is divided into ten chapters. Chapter 1 is devoted to mathematical
modeling. We give examples ranging from mechanics and physics to ﬁnance.
Starting from concrete situations, we try to present the various steps leading to a
mathematical model involving partial differential equations to the extent possible.
In some cases, it is possible to start from ﬁrst principles and entirely derive the
equations and boundary conditions. In other, more complicated cases, we just give a
few indications concerning the modeling approach and sometimes a historical
account. For some of the examples considered, we also give a short elementary
mathematical study: existence, uniqueness, maximum principle.
In Chap. 2, we present the simplest possible and earliest numerical method, the
so-called ﬁnite difference method. We ﬁrst use the example of a one-dimensional
elliptic problem, already introduced in Chap. 1, which is elementary enough not to
require sophisticated mathematical machinery. We then consider a few general-
izations:
one-dimensional
problems
with
other
boundary
conditions,
two-
dimensional problems, still on a rather elementary mathematical level.
vi
Preface

The ﬁrst two chapters are accessible at the Bachelor level. The remainder of the
book calls for a higher level of mathematics, with the possible exception of parts of
Chaps. 8 and 10. Chapter 3 is thus devised as a toolbox of more advanced math-
ematical analysis techniques that are required to study more general partial differ-
ential equations, especially in more than one dimension: a review of Hilbert space
theory, usual function spaces, properties of open sets in Rd, multidimensional
integration by parts, distributions, and Sobolev spaces. The results are standard and
we sometimes refer to classical references. We have however detailed some of the
proofs for those results that are not always easy to ﬁnd in the literature. Readers
who are more interested in the numerical aspects can leaf through this chapter and
keep it for future reference. For the reader’s convenience, the chapter is concluded
by a summary of the most important results that are used in the sequel.
Chapter 4 is concerned with the variational formulation of multidimensional
elliptic boundary value problems, which is a very powerful way of rewriting certain
partial differential equation problems in an abstract Hilbertian setting, in order to
prove the existence and uniqueness of solutions. We provide many examples of
such problems. Most of these examples are problems of second order, i.e., the
maximum order of derivatives appearing in the partial differential equation is two,
with one fourth order example. We also consider a variety of boundary conditions.
The variational formulation also makes it possible to devise numerical approx-
imation methods in a very natural and uniﬁed fashion. We introduce such methods
in Chap. 5. The approximate problems are set in the same framework as the con-
tinuous problem. This framework also provides a fundamental error estimate.
Of particular interest to us is the ﬁnite element method introduced and analyzed in
detail on simple one-dimensional examples.
In Chap. 6, we study the generalization of the ﬁnite element method to
two-dimensional elliptic problems. We start by a detailed presentation of approx-
imation using rectangular ﬁnite elements. We provide a convergence estimate in the
case of the Lagrange rectangular element of lowest possible degree and give
indications about convergence for higher degrees. We then introduce the concept of
barycentric coordinates and use them to describe Lagrange triangular ﬁnite ele-
ments of degree up to three.
The elliptic problems considered so far correspond to the modeling of static or
equilibrium situations, with no time evolution. We then turn to evolution problems
in Chap. 7 with the theoretical study of the heat equation, which is the prototypical
parabolic equation: maximum principle, existence and uniqueness of regular
solutions, energy estimates, variational formulation and weak solutions. New
mathematical tools are needed, mainly Hilbert space-valued function spaces, which
we introduce as the need arises. We also mention and discuss the heat kernel in the
case of the heat equation in the whole space.
We next consider the numerical approximation of the heat equation in Chap. 8.
We focus on the ﬁnite difference method, already seen for static problems in
Chap. 2. We consider several ﬁnite difference schemes of various precision. The
convergence of such schemes rest on their consistency and stability. The analysis
Preface
vii

of these schemes is signiﬁcantly more complicated in the evolution case than in the
static case, due in particular to rather subtle stability issues, which we analyze in
detail and from several different viewpoints. We also mention other methods such
as the ﬁnite difference-ﬁnite element method, in which time is discretized using
ﬁnite differences and space is discretized using ﬁnite elements.
Chapter 9 is devoted to both theoretical and numerical analyses of another
classical evolution problem, the wave equation. This equation is the prototypical
hyperbolic equation of second order. We ﬁrst study the continuous problem and
prove the existence and uniqueness of regular solutions and then of weak solutions.
We next consider ﬁnite difference schemes for the wave equation. The stability
issues are again signiﬁcantly subtler than in the case of the heat equation, and take
up most of the exposition concerning numerical methods.
Finally, we present the ﬁnite volume method in Chap. 10, again on examples.
Finite volume methods are the most recent of the numerical methods covered in the
book. They are currently widely used in certain areas of applications such as
computational ﬂuid dynamics. We start with the one-dimensional elliptic problem
of Chap. 2 with a description of the ﬁnite volume discretization and a complete
convergence analysis. We then consider the one-dimensional transport equation,
which is the prototypical hyperbolic equation of ﬁrst order. This equation is solved
via the method of characteristics. We then introduce several linear ﬁnite volume
schemes and study their properties of consistency, stability and convergence. We
also present the method on a few examples in the nonlinear case and for the
two-dimensional transport equation.
The contents of this book are signiﬁcantly expanded from a series of ﬁrst year
Master degree classes, taught by the authors at UPMC (Université Pierre et Marie
Curie) in Paris, France, over several years. It is intended to be as self-contained as
possible. It consequently provides more than enough material for a one semester
Master class on the subject. The book can also serve as a wide spectrum monograph
for more advanced readers.
We are indebted to the students who have attended our classes and asked many
questions that contributed to making the text more readable. We would like to thank
our colleagues Muriel Boulakia, Edwige Godlewski, Sidi Mahmoud Kaber and
Gérard Tronel, for carefully reading parts of the manuscript and making numerous
suggestions that improved it signiﬁcantly. We also thank the anonymous referees
for making several constructive comments.
Paris, France
Hervé Le Dret
Brigitte Lucquin
viii
Preface

Contents
1
Mathematical Modeling and PDEs . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
The Elastic String. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
The Elastic Beam. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3
The Elastic Membrane . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.4
The Transport Equation. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.5
The Vibrating String Equation . . . . . . . . . . . . . . . . . . . . . . . .
22
1.6
The Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.7
The Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.8
The Schrödinger Equation . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
1.9
The Black and Scholes Equation . . . . . . . . . . . . . . . . . . . . . .
31
1.10
A Rough Classiﬁcation of PDEs . . . . . . . . . . . . . . . . . . . . . .
33
1.11
What Comes Next . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2
The Finite Difference Method for Elliptic Problems . . . . . . . . . . . .
35
2.1
Approximating Derivatives by Differential Quotients . . . . . . . .
35
2.2
Application to a One-Dimensional Model Problem . . . . . . . . . .
38
2.3
Convergence of the Finite Difference Method . . . . . . . . . . . . .
41
2.4
Neumann Boundary Conditions . . . . . . . . . . . . . . . . . . . . . . .
48
2.5
The Two-Dimensional Case. . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.6
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
3
A Review of Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.1
Basic Hilbert Space Theory . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.2
A Few Basic Function Spaces . . . . . . . . . . . . . . . . . . . . . . . .
72
3.3
Regularity of Open Subsets of Rd . . . . . . . . . . . . . . . . . . . . .
76
3.4
Partitions of Unity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.5
Integration by Parts in Dimension d and Applications. . . . . . . .
87
3.6
Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.7
Sobolev Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
ix

3.8
Properties of Sobolev Spaces in One Dimension . . . . . . . . . . . 105
3.9
Density of Smooth Functions and Trace in Dimension d . . . . . . 108
3.10
A Summary of Important Results . . . . . . . . . . . . . . . . . . . . . . 115
4
The Variational Formulation of Elliptic PDEs . . . . . . . . . . . . . . . . 117
4.1
Model Boundary Value Problems. . . . . . . . . . . . . . . . . . . . . . 117
4.2
Abstract Variational Problems . . . . . . . . . . . . . . . . . . . . . . . . 122
4.3
Application to the Model Problems, and More . . . . . . . . . . . . . 126
4.4
General Second Order Elliptic Problems . . . . . . . . . . . . . . . . . 136
4.5
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
5
Variational Approximation Methods for Elliptic PDEs. . . . . . . . . . 145
5.1
The General Abstract Variational Approximation Scheme . . . . . 145
5.2
The Finite Element Method in Dimension One . . . . . . . . . . . . 149
5.3
Comparison with the Finite Difference Method . . . . . . . . . . . . 157
5.4
A Fourth Order Example. . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
5.5
Neumann and Fourier Conditions . . . . . . . . . . . . . . . . . . . . . . 165
6
The Finite Element Method in Dimension Two . . . . . . . . . . . . . . . 167
6.1
Meshes in 2d. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
6.2
Rectangular Q1 Finite Elements . . . . . . . . . . . . . . . . . . . . . . . 172
6.3
Convergence and Error Estimate for the Q1FEM . . . . . . . . . . . 177
6.4
Assembling the Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.5
General Deﬁnition of a Finite Element . . . . . . . . . . . . . . . . . . 189
6.6
Q2 and Q3 Finite Elements . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.7
Barycentric Coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
6.8
Triangular P1 Lagrange Elements . . . . . . . . . . . . . . . . . . . . . . 204
6.9
Triangular P2 Lagrange Elements . . . . . . . . . . . . . . . . . . . . . . 209
6.10
An Example of 2d-Computation. . . . . . . . . . . . . . . . . . . . . . . 215
7
The Heat Equation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
7.1
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
7.2
The Maximum Principle for the Heat Equation . . . . . . . . . . . . 220
7.3
Construction of a Regular Solution. . . . . . . . . . . . . . . . . . . . . 222
7.4
Spaces of Hilbert Space-Valued Functions. . . . . . . . . . . . . . . . 230
7.5
Energy Estimates, Stability, Uniqueness . . . . . . . . . . . . . . . . . 234
7.6
Variational Formulation and Existence of Weak Solutions. . . . . 237
7.7
The Heat Equation on R . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
8
The Finite Difference Method for the Heat Equation . . . . . . . . . . . 253
8.1
The Explicit Euler Three Point Finite Difference Scheme
for the Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
8.2
The Implicit Euler and Leapfrog Schemes . . . . . . . . . . . . . . . . 259
8.3
General Finite Difference Schemes, Consistency, Stability,
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
8.4
General Criteria for Stability . . . . . . . . . . . . . . . . . . . . . . . . . 269
8.5
Stability for One Time Step Schemes in the 2; h Norms . . . . . . 273
x
Contents

8.6
Stability via the Discrete Fourier Transform. . . . . . . . . . . . . . . 282
8.7
Stability via Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . 286
8.8
Stability via the Continuous Fourier Transform . . . . . . . . . . . . 292
8.9
The Crank–Nicolson Scheme, Stability
via the Energy Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
8.10
Other Approximations of the Heat Equation . . . . . . . . . . . . . . 302
9
The Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
9.1
Regular Solutions of the Wave Equation . . . . . . . . . . . . . . . . . 307
9.2
Variational Formulation and Existence of Weak Solutions. . . . . 313
9.3
The Wave Equation on R . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
9.4
Finite Difference Schemes for the Wave Equation . . . . . . . . . . 320
9.5
Stability via the Fourier Approach . . . . . . . . . . . . . . . . . . . . . 327
9.6
For a Few Schemes More . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
9.7
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
10
The Finite Volume Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
10.1
The Elliptic Case in One Dimension. . . . . . . . . . . . . . . . . . . . 345
10.2
The Transport Equation in One Dimension . . . . . . . . . . . . . . . 351
10.3
Finite Volumes for the Transport Equation . . . . . . . . . . . . . . . 354
10.4
Explicit Three Point Schemes . . . . . . . . . . . . . . . . . . . . . . . . 361
10.5
Examples of Linear Schemes . . . . . . . . . . . . . . . . . . . . . . . . . 370
10.6
Generalizations to Discontinuous Data and Nonlinear
Problems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
10.7
The Transport Equation in an Open Set
in Higher Dimensions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
10.8
Finite Volumes for the Transport Equation
in Two Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
Contents
xi

Chapter 1
Mathematical Modeling and PDEs
In this chapter, we consider several concrete situations stemming from various areas
of applications, the mathematical modeling of which involves partial differential
equation problems. We will not be rigorous mathematically speaking. There will be
quite a few rather brutal approximations, not always convincingly justiﬁed. This is
however the price to be paid if we want to be able to derive mathematical models that
aim to describe the complex phenomena we are dealing with in a way that remains
manageable. At a later stage, we will study some of these models with all required
mathematical rigor.
The simplest examples arise in mechanics. Let us start with the simplest example
of all.
1.1
The Elastic String
Let us ﬁrst consider the situation depicted in Fig.1.1.
What is an elastic string in real life? The term can refer to several different objects,
such as a stretched rubber band, the strings of a musical instrument made of nylon
or steel, or again a cabin lift cable. Up to a certain level of approximation, all these
objects are modeled in the same way. What they all have in common is a very small
aspect ratio: they are three-dimensional, however much thinner in two directions
than in the third one. Thus, the ﬁrst step toward a simple mathematical model is to
simply declare them to be one-dimensional at the onset. Points in a string will thus be
labeled by a single real-valued variable x belonging to a segment [0, L], embedded
in R2 or R3. Another implicit assumption used here is the assumption of continuum
mechanics. We assume that matter is a continuum which can be divided indeﬁnitely.
This is obviously untrue, but it is true enough at the macroscopic scale to be an
extremely effective modeling hypothesis.
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_1
1

2
1
Mathematical Modeling and PDEs
0
L
x
u(x)
applied force
Fig. 1.1 An elastic string stretched between two points and pulled by some vertical force. The
point initially located at x moves by a vertical displacement u(x) to an equilibrium position
Fig. 1.2 Stretching a string with a wheel and a weight
x
y
T−(x)
T+(y)
Fig. 1.3 A piece of string cut at x and y, then kept in equilibrium
We assume that the string is stretched with a tension T > 0. The tension is just
the force that is applied at both extremities 0 and L in order to make the string taut,
for instance by working the tuners of a guitar or by passing the string on a wheel and
suspending a weight at the end with the other end attached to a wall, see Fig.1.2.
In terms of physical units, the tension is measured in Newton (N). If the only force
acting on the string is the tension, then the string settles in an equilibrium position
which is none but the segment [0, L].
We now perform a thought experiment by cutting the string at two points of [0, L],
located at two arbitrary abscissae x and y with x < y. If the piece of string that has
just been cut is going to stay in the same place, it is quite clear that we need to apply
horizontal forces T−(x) < 0 and T+(y) > 0 at the two newly created extremities, in
order to compensate for the disappearance of the rest of the string, see Fig.1.3.
As a matter of fact, T−(x) is the force that was exerted by the [0, x] part of
the string on the segment [x, y] at point x before the string was cut, and T+(y) is
likewise the force formerly exerted by the [y, L] part of the string at point y. The
action-reaction principle immediately shows that we have T−(x) = −T+(x). Let us
thus just set T (x) = T+(x).
Since the cut out piece of string stays in equilibrium and the only forces acting
on it are the above two tensions, Newton’s law of motion implies that the resultant

1.1 The Elastic String
3
force vanishes, that is to say that
T (y) −T (x) = 0.
Now this holds true for all x and y, therefore the tension T (x) inside the string
is constant. Taking x = 0 or x = L, we see that this constant is equal to T , the
string tension applied at the extremities. This is an important—even if obvious—
result, because it holds true irrespective of the physical nature of the string. Whether
a string is made of rubber, nylon or steel, the tension inside is constant and equal to
the applied tension. This is quite remarkable.
From now on, we will work with planar deformations, that is to say, we assume that
the string lives in R2. Of course, a similar model can be derived in three dimensions.
Let us apply other forces to the string, for example its weight or the weight of an
object that is suspended to the string. For simplicity, we assume that this extra force
is perpendicular to the segment—we will say that it is vertical whereas the segment
is considered to be horizontal—and described by a lineic density f . This means that
we are given a function f : [0, L] →R such that the vertical component of the force
applied to a portion [a, b] of the string is equal to the integral
 b
a f (x) dx. Such is the
case of the weight of the string. Assuming the string is homogeneous, then its weight
is represented by the function f (x) = −ρg, where ρ is the string mass per unit
length and g is the gravitational acceleration. If we suspend a weight P to a device
occupying a segment [α, β] of the string, we may take f (x) = −P1[α,β](x), where
1E denotes the characteristic function of a set E: 1E(x) = 1 if x ∈E, 1E(x) = 0
otherwise.
Due to the extra applied force, the string deforms and settles in a new, unknown
equilibrium position that we wish to determine. We assume that any point initially
situated at (x, 0) moves vertically and reaches an equilibrium position (x, u(x)), as
in Fig.1.1. This verticality hypothesis is again a modeling hypothesis. It is not strictly
speaking true. In reality, the point in question also settles a little bit to the left or to
the right of the vertical position. However, this hypothesis is reasonable when the
force is vertical and the displacement is small. In this case, it can be justiﬁed, and we
just admit it here, that the horizontal displacement is negligible in comparison with
the vertical displacement u(x). If the force was slanted, or the displacement large, it
would be an entirely different story.
The deformed string is at this point described by a parametric curve in R2, x →
(x, u(x)) where u is an unknown function to be determined. We now make another
modeling hypothesis, which is that we are only interested in those situations where
the derivative u′(x) has small absolute value compared to 1. In this case, the length
element of the deformed string satisﬁes

1 + u′(x)2 ≈1 + 1
2u′(x)2 ≈1,

4
1
Mathematical Modeling and PDEs
Fig. 1.4 Cutting out a piece
of the deformed string
x
x+δ x
−τ (x)
τ(x+δ x)
since if u′(x) is small, then u′(x)2 is negligible.1 We are thus dealing with situations
in which the string is approximately inextensional, i.e., there are basically no length
changes compared to the initial straight stretched string.
Let us pick up our thought experiment scissors again and cut the string between
abscissae x and x + δx, δx > 0. This time, the string is no longer straight. When we
think about the forces exerted by the rest of the string on the extremities of the part
that was cut, it appears reasonable that these forces should be tangent to the deformed
string at the cut points, see Fig.1.4. This is yet another modeling hypothesis, which
can be justiﬁed by a more reﬁned mechanical analysis.
The thought experimenter thus applies a tension force of the form −T (x)τ(x) at
point (x, u(x)), and a tension force T (x +δx)τ(x +δx) at point (x +δx, u(x +δx)),
where τ is the unit tangent vector
τ(x) =
1

1 + u′(x)2
 1
u′(x)

≈
 1
u′(x)

,
in order to keep the piece that was cut out in equilibrium. The above approximation
of the tangent vector is legitimate in view of our decision to neglect terms involving
u′(x)2 and other quantities of higher order.
We apply Newton’s law of motion again, which yields the vector equation
T (x + δx)τ(x + δx) −T (x)τ(x) +

0
 x+δx
x
f (s) ds

=
0
0

.
The horizontal component of the equation implies that T (x) = T is the same constant
as before. The vertical component then reads
T

u′(x + δx) −u′(x)

+
 x+δx
x
f (s) ds = 0,
using the above approximation for the tangent vector. Dividing everything by δx, we
obtain
1As a general rule, we neglect all terms of order strictly higher than one with respect to u′(x). This
leads to a simpliﬁed linearized model. A model that would take into account such higher order
terms would be by nature nonlinear, and thus a lot more difﬁcult to study from the point of view of
mathematics.

1.1 The Elastic String
5
−T u′(x + δx) −u′(x)
δx
= 1
δx
 x+δx
x
f (s) ds.
Since one of our modeling hypotheses is that matter can be indeﬁnitely divided, the
above relation holds true for any value of δx, no matter how small. We thus let δx tend
to 0. The left-hand side is a differential quotient, the right-hand side is an average
over a small interval, and we thus obtain in the limit δx →0,
−T u′′(x) = f (x),
which can be rewritten as the ﬁrst equation of the following string problem:
⎧
⎨
⎩
−u′′(x) = 1
T f (x) for all x in ]0, L[,
u(0) = u(L) = 0.
(1.1)
The second line of (1.1) expresses the fact that the string is ﬁxed at the endpoints x =
0 and L. These points never move and the displacement is zero there. This condition
is called a boundary condition. Problem (1.1) consists of a differential equation in
an open set (here an ordinary differential equation, since we are in dimension one),
together with a condition on the boundary of the open set. This type of problem is
called a boundary value problem, and we will see many more of them.
If we are somehow capable of solving this problem, then we will have determined
the deformed shape of the string under the action of the applied forces. Indeed,
it is easily checked by following the computations backward, that any solution of
problem (1.1) yields an equilibrium position for the string, at least within the range
of approximations that have been made.
Remark 1.1 In order to appease natural suspicions that it does not feel right to neglect
terms before differentiating them, we can note that

u′(x)

1 + u′(x)2
′
=
u′′(x)

1 + u′(x)2 +
u′(x)2u′′(x)
(1 + u′(x)2)3/2 ≈u′′(x),
therefore it was not so bad, a posteriori.
□
Remark 1.2 Let us admit for the time being that problem (1.1) has a unique solution
for given f and T . If we consider the same string subjected to the same force, but
with different tensions, we see that the displacement is inversely proportional to the
tension: the tauter the string, the more rigid it behaves and conversely. This is in
agreement with daily life.
Let us emphasize again that the string model is independent of the physical nature
of the string, which can be counterintuitive. A rubber string and a steel string stretched
with the same tension have the same behavior insofar as the model is concerned.
Once the model is derived and solved, a natural question arises: how far apart is
the model solution from real life or how large is the modeling error? This is a difﬁcult

6
1
Mathematical Modeling and PDEs
question in all generality. However, a few rules of thumb can be useful. For instance,
if a given modeling hypothesis is clearly not satisﬁed by the model solution, then its
validity is dubious at best. For example, in the case of a string, if we ﬁnd a solution
which is such that |u′(x)| is large for some values of x, in the sense that u′(x)2 can no
longer be neglected, then we know that a more precise model is needed. Of course,
this is still rather vague. An ideal situation would be one in which it was possible to
have an explicit quantitative estimate of the difference in the solutions of models of
various accuracy. Unfortunately, this is rarely possible.
□
It is important to understand that, even though the string model derived here
involves an ordinary differential equation because we are in dimension one, a bound-
ary value problem such as problem (1.1) has strictly nothing to do with the Cauchy
problem for the same ordinary differential equation, either in terms of theory or in
terms of numerical approximation.
In particular, the numerical schemes used for the Cauchy problem, such as the
forward and backward Euler methods or the Runge-Kutta method (see for example
[8, 16, 23, 60, 64]), are of no use to compute approximations of the solution of
problem (1.1) (except in the shooting method, see below). Different, more adapted
numerical methods are needed, and we will discuss several of them later on.
To illustrate this, let us introduce a slightly generalized version of the string
problem. We thus consider the following boundary value problem:

−u′′(x) + c(x)u(x) = f (x) in ]0, L[,
u(0) = A, u(L) = B,
(1.2)
where f and c are two given functions deﬁned on ]0, L[ and A, B are two given
constants. The function c has no speciﬁc mechanical interpretation in the context
of the elastic string. It just adds generality without costing any extra complexity.
The boundary condition in (1.2) is called a Dirichlet boundary condition. In general,
imposing a Dirichlet condition means ascribing given values to the unknown function
u on the boundary. In the case when A = B = 0, it is called a homogeneous Dirichlet
condition.
Let us now see some of the fundamental differences between a boundary value
problem and a seemingly similar Cauchy problem. The Cauchy problem consists in
replacing the boundary conditions in (1.2) by initial conditions of the form u(0) = α,
u′(0) = β and no condition at x = L. We know from the classical theory of ordinary
differential equations (see [16, 23, 60] for example) that this Cauchy problem has
one and only one solution if for example c and f are continuous. The boundary value
problem (1.2) may however not have any solution at all under the same hypotheses!
Let us take the following apparently innocuous example: L = 1, A = B = 0,
c(x) = −π2 and f (x) = 1. We can treat the ﬁrst equation in (1.2) as a linear
ordinary differential equation of second order with constant coefﬁcients that admits
the general solution

1.1 The Elastic String
7
u(x) = λ cos(πx) + μ sin(πx) −1
π2 ,
where λ and μ are two real parameters. If we write down the homogeneous Dirichlet
boundary conditions, we obtain two relations
λ −1
π2 = 0,
−λ −1
π2 = 0,
which are impossible to satisfy simultaneously. Consequently, there is no solution to
this particular boundary value problem.
More generally, let us assume that the problem

−u′′(x) −π2u(x) = f (x) in ]0, 1[,
u(0) = u(1) = 0,
has a regular solution that is for example of class C2. We multiply the differential
equation by sin(πx), which yields
−u′′(x) sin(πx) −π2u(x) sin(πx) = f (x) sin(πx).
We now integrate this equality between 0 and 1. We obtain
−
 1
0
u′′(x) sin(πx) dx −π2
 1
0
u(x) sin(πx) dx =
 1
0
f (x) sin(πx) dx. (1.3)
Now, if we integrate the ﬁrst integral by parts twice, we see that
 1
0
u′′(x) sin(πx) dx = [u′(x) sin(πx)]1
0 −π[u(x) cos(πx)]1
0 −π2
 1
0
u(x) sin(πx) dx.
The ﬁrst two terms vanish because the sine function vanishes at x = 0 and x = 1 for
the ﬁrst one, and u vanishes at the same points for the second one by the homogeneous
Dirichlet condition. The remaining integral cancels out with the second integral in
Eq.(1.3). Finally, we ﬁnd that if the problem has a solution u, then
 1
0
f (x) sin(πx) dx = 0,
which is therefore a necessary condition for existence. By contraposition, if f is such
that
 1
0 sin(πx) f (x) dx ̸= 0, for instance f = 1, then the problem cannot have any
regular solution.
Concerning uniqueness, we note that for f = 0, we have the inﬁnite family of
solutions u(x) = μ sin(πx), μ ∈R. Hence, there is no uniqueness either.
□

8
1
Mathematical Modeling and PDEs
The above trick of multiplying the equation by certain well-chosen functions and
integrating the result by parts will be at the heart of the existence and uniqueness
theory using variational formulations, as well as the basis of such variational approx-
imation methods as the ﬁnite element method that we will encounter later on.
We can already prove a uniqueness result. The problem in the previous example
is that the function c is negative (and take the speciﬁc value −π2).
Theorem 1.1 If c is a continuous, nonnegative function, then problem (1.2) has at
most one solution of class C2([0, L]).
Proof Let u1 and u2 be two solutions of class C2([0, L]), and set w = u2 −u1. It is
easily checked that w solves the homogeneous boundary value problem:

−w′′(x) + c(x)w(x) = 0 in ]0, L[,
w(0) = w(L) = 0.
(1.4)
We multiply the differential equation by w and integrate between 0 and L. This yields
−
 L
0
w′′(x)w(x) dx +
 L
0
c(x)w(x)2 dx = 0.
Integrating the ﬁrst term by parts once, we obtain
 L
0
[w′(x)2 + c(x)w(x)2] dx = 0,
since [w′w]L
0 = 0, given the boundary conditions satisﬁed by w. The integrand is
a continuous function which is nonnegative due to the sign hypothesis for c. Its
integral is zero, hence it is identically zero. In particular, w′(x) = 0, which implies
w(x) = w(0) = 0 for all x, hence u1 = u2, which is the uniqueness result.
□
Let us say a few words about the shooting method alluded to above, applied to
boundary value problem (1.2). The idea is to use the Cauchy problem for the same
ordinary differential equation, −u′′(x) + c(x)u(x) = f (x) on ]0, L[, with initial
values u(0) = A, u′(0) = λ and to try and adjust the parameter λ so as to reach the
target B for x = L, i.e., u(L) = B, see Fig.1.5.
This Cauchy problem has one and only one solution, which we denote by uλ, and
we consider the shooting mapping S : R →R, S(λ) = uλ(L). The boundary value
problem (1.2) thus has a solution for all B if and only if the mapping S is onto. Let
us study this mapping.
Lemma 1.1 The mapping S is afﬁne.
Proof Let λ ∈R. We claim that uλ = λu1 + (1 −λ)u0. Indeed, if we set v =
λu1 + (1 −λ)u0, we see that v(0) = λA + (1 −λ)A = A, v′(0) = λu′
1(0)
+ (1 −λ)u′
0(0) = λ, and

1.1 The Elastic String
9
Fig. 1.5 The shooting
method
0
A
B
L
on target!
missed...
−v′′(x)+c(x)v(x) = λ(−u′′
1(x)+c(x)u1(x))+(1−λ)(−u′′
0(x)+c(x)u0(x)) = f (x)
in ]0, L[, hence the claim by uniqueness of the Cauchy problem. The Lemma follows
then by taking x = L, which shows that S(λ) = λS(1) + (1 −λ)S(0).
□
Theorem 1.2 If c is a continuous, nonnegative function, then problem (1.2) has one
and only one solution of class C2([0, L]).
Proof The mapping S : R →R is afﬁne by Lemma1.1. Let us show that it is one-
to-one and onto when c ≥0. Since S(λ) = (S(1) −S(0))λ + S(0), it is enough to
show S(0) ̸= S(1). Let thus assume that S(0) = S(1). We set w = u1 −u0, which
is thus a solution of problem (1.4). By Theorem1.1, it follows that w = 0, therefore
1 = u′
1(0) = u′
0(0) = 0, contradiction.
□
We have seen that existence and uniqueness fail in the particular case L = 1 and
c = −π2 < 0.
The shooting method can also be used in a numerical context, and in the present
case, it is particularly simple. Indeed, since S is afﬁne, it is sufﬁcient to compute
its values for two different values of λ to determine it entirely. This can be done
numerically using ordinary differential equation schemes such as the forward Euler
scheme or the fourth-order Runge-Kutta scheme for better precision.
The shooting method also applies to nonlinear boundary value problems in one
dimension. In this case, the mapping S is no longer afﬁne, and computing two of
its values would be the initialization step in the application of a bisection method or
a secant method, for example. Now of course, there is no analogue of the shooting
method in dimensions higher than one, which severely restricts its interest.
Such boundary value problems as (1.2) have an important property called the
maximum principle [35, 40]. As we will see shortly, the proof is banal in dimension
one. However, the maximum principle is a profound property in dimensions strictly
greater than one.

10
1
Mathematical Modeling and PDEs
Theorem 1.3 (Maximum Principle) Assume that c ≥0 and that problem (1.2) has
a solution u of class C2. If f ≥0 in ]0, L[, A ≥0 and B ≥0, then we have u ≥0
in [0, L].
Proof We argue by contradiction by assuming that there exists a point x0 such that
u(x0) < 0. Since u(0) = A ≥0 and u(L) = B ≥0, it follows that x0 ∈]0, L[. Now
u is continuous, therefore there is an interval [α, β] such that x0 ∈[α, β] ⊂[0, L]
and u ≤0 on [α, β]. We may assume that u(α) = u(β) = 0 by the intermediate
value theorem.
On the interval [α, β], c and f are positive and u is nonpositive, therefore
u′′(x) = c(x)u(x) −f (x) ≤0.
We deduce from this that the function u is concave on [α, β].
Now as x0 ∈[α, β], there exists λ ∈[0, 1] such that x0 = λα + (1 −λ)β.
Consequently, the concavity of u implies that
u(x0) ≥λu(α) + (1 −λ)u(β) = 0,
which is a contradiction.
□
Remark 1.3 Under the form given above, it is a little hard to see where the maxi-
mum of the principle is. . . because it is hiding. Anyway, one way of understanding
Theorem1.3 is to see it as a monotonicity result. Indeed, the function that maps the
data triple ( f, A, B) to the solution u is monotone. Thus, in the case of the elastic
string, when A = B = 0 and f ≥0, in other words when we pull upwards on the
string, then u ≥0, which means that the string bends upwards too. So we see a very
natural physical interpretation of the maximum principle that is in agreement with
our intuition. This is also the reason why, in mathematics, the operator −u′′, or more
generally −Δu = −d
i=1
∂2u
∂x2
i in higher dimension d, is preferred to the operator
u′′, which has the opposite behavior.
□
1.2
The Elastic Beam
Our second example is also an example taken from mechanics. However, the mathe-
matical modeling of this example is considerably more complicated than that of the
string, and we will not explain it here. We are again dealing with essentially one-
dimensional objects that are a lot more rigid than the previous ones, such as a metal
rod, a concrete pillar or a wooden beam. Such objects exhibit a strong resistance to
bending, as opposed to strings.
If we assume that our beam is clamped in a rigid wall at both ends, see Fig.1.6,
then the following boundary value problem is found for the vertical displacement u:

1.2 The Elastic Beam
11
Fig. 1.6 A beam clamped at
both ends

E I u(4)(x) = f (x) in ]0, L[,
u(0) = u′(0) = u(L) = u′(L) = 0,
where f is again the density of the applied vertical force, E > 0 is a coefﬁcient
which is characteristic of the material of the beam2 and I is a geometric coefﬁcient
depending on the shape of the cross-section of the beam.3 This is in striking contrast
with the string model in which the nature of the string material and the shape of the
cross-section of the string play no role whatsoever. Note also that there is no tension
in a beam.
The differential equation is a fourth order equation (u(4) denotes the fourth deriv-
ative of u), as opposed to a second order equation in the case of the string, and
accordingly, the Dirichlet boundary conditions involve both u and u′.
We can generalize the equation in the same spirit as before by considering the
boundary value problem

u(4)(x) −(a(x)u′(x))′ + c(x)u(x) = f (x) in ]0, L[,
u(0) = u′(0) = u(L) = u′(L) = 0,
(1.5)
where the given functions a and c still have no particular mechanical meaning.
We also have uniqueness of C4 solutions when a and c are nonnegative. Indeed, if
w = u2−u1 is the difference between two solutions, multiplying by w the differential
equation satisﬁed by w, which is the ﬁrst equation in (1.5) with zero right-hand side,
and integrating by parts as many times as needed, we obtain
 L
0
[(w′′(x))2 + a(x)(w′(x))2 + c(x)w(x)2] dx = 0,
whence w′′(x) = 0. Consequently, w is an afﬁne function of the form w(x) = αx +β.
Since it vanishes at both ends, we deduce that w = 0.
Remark 1.4 A word of warning: there is no maximum principle for such problems as
(1.5) in general. The maximum principle is a property of second order boundary value
problems that does not extend to fourth order problems. Physically, this means that
some mechanical systems modeled by a fourth order equation may exhibit the strange
behavior that when pulled downwards, part of such systems may move upwards. □
2The coefﬁcient E is called the Young modulus of the material. It is measured in units of pressure.
The higher the coefﬁcient, the more rigid the material.
3I is an inertia momentum of the cross-section.

12
1
Mathematical Modeling and PDEs
1.3
The Elastic Membrane
Let us now switch to actual PDEs in more than one dimension. The ﬁrst example
is still taken from mechanics. It is the two-dimensional version of the elastic string,
and it is called the elastic membrane. As we will see, many of the characteristics of
the elastic string carry over to the elastic membrane.
To get a feeling for what an elastic membrane is, think of plastic wrap suitable
for food contact that you can ﬁnd in your favorite supermarket. Stretch the ﬁlm up to
the sides of some container in order to seal it before you store it in the fridge. In the
beginning, the stretched part of the plastic ﬁlm is planar. Then, as the temperature
of the air inside the container goes down, the inside air pressure diminishes. At the
same time, the atmospheric pressure inside the fridge remains more or less constant
(you are bound to open the door every once in a while). The pressure differential thus
created pushes on the ﬁlm which bends inwards as a result. We wish to determine
the ﬁnal shape of the ﬁlm in three-dimensional space.
This kitchen example above is by far not the only one. There are many instances
of elastic membranes around: the skin of a drum, a biological cell membrane, the
sails of a boat, a party balloon, and so on.
To model this situation, let us be given an open set Ω of R2, whose boundary ∂Ω
represents the edge of the container opening. Each point x of the closure ¯Ω of Ω
represents a material point of the membrane when it is stretched without any other
applied force. Again, we identify a small aspect ratio, three-dimensional object with
a two-dimensional object ﬁlling ¯Ω.
We now subject the membrane to a given force density, such as the above pressure
differential, which is orthogonal to its plane, and is represented by a given function
f : Ω →R. This time, f is a surfacic force density and the resultant force applied
to a part ω of Ω is given by the integral

ω f (x1, x2) dx1dx2.
As in the case of the elastic string, we make the reasonable albeit approxima-
tive hypothesis that point x is displaced by a quantity u(x) perpendicularly to the
membrane (vertically in Fig.1.7). The displacement u is thus now a function in two
variables u : ¯Ω →R, and the shape of the membrane at equilibrium is a parametric
surface in R3 given by (x1, x2) →(x1, x2, u(x1, x2)).
Sinceweassumethatthemembranestickstotheopeningofthecontainer,wegetat
once a homogeneous Dirichlet boundary condition to be satisﬁed by the displacement
u of the membrane
u(x) = 0 on ∂Ω.
(1.6)
This condition is the exact analogue in two dimensions of the Dirichlet boundary
condition for an elastic string that is attached at both of its ends.
We next need to obtain an equation that will determine the function u in Ω, and
based on our previous one-dimensional experience, we can expect partial differential
equations to play the leading role here.
Figure1.7 represents the graph of the function u(x1, x2) = (1 −x2
1)(1 −x2
2) on
the square Ω = ]−1, 1[2, which is the solution of the as yet unwritten membrane

1.3 The Elastic Membrane
13
Fig. 1.7 An elastic
membrane stretched with
tension T = 1 on the square
]−1, 1[2
problem (1.8) for the vertical force f (x1, x2) = 4 −2(x2
1 + x2
2) and homogeneous
Dirichlet boundary conditions.
The two vectors
a1 =
⎛
⎝
1
0
∂u
∂x1 (x1, x2)
⎞
⎠and a2 =
⎛
⎝
0
1
∂u
∂x2 (x1, x2)
⎞
⎠
form a basis of the tangent plane to the surface at point (x1, x2, u(x1, x2)).
As in the case of the elastic string, we will only consider situations in which
∥∇u∥=
 ∂u
∂x1
2 +
 ∂u
∂x2
2 is small (which is not exactly the case in Fig.1.7!). This
hypothesis leads us to neglect all quantities that are at least quadratic in the partial
derivatives of u. In particular, when we normalize the above tangent basis vectors,
we obtain the approximation
ai
∥ai∥=
1

1 +
 ∂u
∂xi
2 ai ≈ai,
which is analogous to the approximate normalization of the tangent vector to the
deformed elastic string used earlier.
Let us now explain what the word tension means in the case of a membrane.
Because we are in a two-dimensional setting, the situation is a bit more complicated
than for the elastic string. The general principle remains however the same. Let us
consider a part A of the membrane and isolate this part as if it was cut out of the
membrane. Just like the cut piece of string before, what keeps the part A in place must
be forces exerted by the rest of the membrane. It seems reasonable to assume that
these forces are exerted exactly on the boundary ΓA of A relative to the membrane,
since the membrane cannot act at a distance. Now the boundary in question is a
curve, so that the force in question must be given by a lineic density distributed on

14
1
Mathematical Modeling and PDEs
Fig. 1.8 Magniﬁed view of
a small square cut out of the
membrane. A few of the
normal vectors are drawn.
The force density exerted by
the rest of the membrane is
equal to T times these
vectors. We can see it pulling
to stretch the piece of
membrane
ΓA, the resultant force being the integral of the density on ΓA. This is general for all
two-dimensional continuum mechanics models.
In the case of an elastic membrane, as in the case of a string, we assume that the
above force density lies in the tangent plane to the deformed surface, and furthermore,
that it is normal to ΓA in the tangent plane and pointing outwards, see Fig.1.8.
Actually, this assumption can be seen as the very deﬁnition of an elastic membrane.
The tension T > 0 is the norm of this force density—we admit here for simplicity
that this norm is a constant, independent of the point4—it measures the tautness of
the membrane. The physical unit for T is the Newton per meter (N/m).
Let us thus take the scissors out again and cut a small square out of the membrane
around an arbitrary point (¯x, u(¯x)) with ¯x = (¯x1, ¯x2). More precisely, we consider
the square
C ¯x,δx = ]¯x1 −δx, ¯x1 + δx[×]¯x2 −δx, ¯x2 + δx[,
in R2, with δx > 0, and cut out its image by the mapping x →(x, u(x)) in R3, see
Fig.1.8. We also make no distinction between the boundary of the image of C ¯x,δx
in R3 and its boundary as a subset of R2 in the computation of the integrals. This is
because ∥∇u∥is assumed to be small. We already made this approximation in the
case of the string, without mentioning it. . . The exact computations can of course be
performed in order to make sure that this approximation is really justiﬁed.
In order to compute the integral, we number the four sides of the square counter-
clockwise:γ 1
¯x,δx = ]¯x1−δx, ¯x1+δx[×{¯x2−δx},γ 2
¯x,δx = {¯x1+δx}×]¯x2−δx, ¯x2+δx[,
and so on for γ 3
¯x,δx and γ 4
¯x,δx. According to the normal vectors depicted in Fig.1.8,
Newton’s law of motion for the vertical force component then reads
4This can of course be proved with a little more work.

1.3 The Elastic Membrane
15
T

γ 1
¯x,δx
−[a2(x)]3 dγ +

γ 2
¯x,δx
[a1(x)]3 dγ
+

γ 3
¯x,δx
[a2(x)]3 dγ +

γ 4
¯x,δx
−[a1(x)]3 dγ

+

C ¯x,δx
f (x) dx = 0,
(1.7)
where [z]3 denotes the vertical component of vector z. It is a simple exercise to
check that the horizontal components already satisfy Newton’s law. Let us write
each integral separately. We have

γ 1
¯x,δx
[a2(x)]3 dγ =
 ¯x1+δx
¯x1−δx
∂u
∂x2
(x1, ¯x2 −δx) dx1,

γ 2
¯x,δx
[a1(x)]3 dγ =
 ¯x2+δx
¯x2−δx
∂u
∂x1
(¯x1 + δx, x2) dx2,

γ 3
¯x,δx
[a2(x)]3 dγ =
 ¯x1+δx
¯x1−δx
∂u
∂x2
(x1, ¯x2 + δx) dx1,

γ 4
¯x,δx
[a1(x)]3 dγ =
 ¯x2+δx
¯x2−δx
∂u
∂x1
(¯x1 −δx, x2) dx2.
Formula (1.7) can thus be rewritten as
T
 ¯x1+δx
¯x1−δx
 ∂u
∂x2
(x1, ¯x2 + δx) −∂u
∂x2
(x1, ¯x2 −δx)

dx1
+
 ¯x2+δx
¯x2−δx
 ∂u
∂x1
(¯x1 + δx, x2) −∂u
∂x1
(¯x1 −δx, x2)

dx2

+

C ¯x,δx
f (x) dx = 0.
The situation is less transparent than in dimension one, but the idea is the same. We
divide everything by 4(δx)2. This yields
−T 1
2δx
 ¯x1+δx
¯x1−δx
∂u
∂x2 (x1, ¯x2 + δx) −∂u
∂x2 (x1, ¯x2 −δx)
2δx
dx1
+
 ¯x2+δx
¯x2−δx
∂u
∂x1 (¯x1 + δx, x2) −∂u
∂x1 (¯x1 −δx, x2)
2δx
dx2

=
1
4(δx)2

C ¯x,δx
f (x) dx.

16
1
Mathematical Modeling and PDEs
Now the length of each of the segments on which differential quotients of the partial
derivatives ∂u/∂xi are integrated is exactly 2δx, and the area of the square is exactly
4(δx)2. We thus see that all the above quantities are averages over small segments
or squares, which is good in view of letting δx tend to 0 later.
Let us assume that u is of class C2. We can thus write the following Taylor-
Lagrange expansion at ¯x
∂u
∂x2
(x) = ∂u
∂x2
(¯x) +
∂2u
∂x2∂x1
(¯x)(x1 −¯x1) + ∂2u
∂x2
2
(¯x)(x2 −¯x2) + r(x)
where r(x)/∥x −¯x∥→0 when ∥x −¯x∥→0. Therefore
∂u
∂x2 (x1, ¯x2 + δx) −∂u
∂x2 (x1, ¯x2 −δx)
2δx
= ∂2u
∂x2
2
(¯x) + r1(x1, δx)
where r1(x1, δx) →0 when |x1 −¯x1| ≤δx and δx →0. Integrating with respect to
x1, we obtain
1
2δx
 ¯x1+δx
¯x1−δx
∂u
∂x2 (x1, ¯x2 + δx) −∂u
∂x2 (x1, ¯x2 −δx)
2δx
dx1 = ∂2u
∂x2
2
(¯x) + r2(δx)
where r2(δx) →0 when δx →0.
We treat the remaining integral on the boundary in the same fashion and we obtain
1
2δx
 ¯x2+δx
¯x2−δx
∂u
∂x1 (¯x1 + δx, x2) −∂u
∂x1 (¯x1 −δx, x2)
2δx
dx2 →∂2u
∂x2
1
(¯x)
when δx →0. Finally, assuming that f is continuous, we have
1
4(δx)2

C ¯x,δx
f (x) dx →f (¯x) when δx →0,
since the left-hand side is the average of f over the square.
We have thus obtained in the δx →0 limit
∀¯x ∈Ω,
−Δu(¯x) = 1
T f (¯x).
(1.8)
The differential operator Δ =
∂2
∂x2
1 + ∂2
∂x2
2 is called the Laplacian or Laplace operator.
Equation(1.8) is called the elastic membrane equation. It is a second order equation
since it only involves second derivatives, the order of a partial differential equation
being the highest order of derivatives that appear in the equation. The equation must
naturally be complemented by some boundary conditions, such as the homogeneous
Dirichlet condition (1.6).

1.3 The Elastic Membrane
17
The mechanical remarks made in the case of the elastic string also apply to the
elastic membrane and we will not repeat them.
More generally, the boundary value problem in any dimension, Ω ⊂Rd, d ≥1,

−Δu = f in Ω,
u = 0 on ∂Ω,
(1.9)
with Δu = d
i=1
∂2u
∂x2
i , is called the Poisson equation. The Poisson equation shows
up in a surprising number of different areas of mathematics and its applications. For
example, for d = 3, if f represents the density of electrical charge present in Ω
and the boundary of Ω is covered by a perfectly conducting material, then −u is the
electric potential5 inside Ω. The gradient of −u is the electric ﬁeld. More generally,
the Poisson equation is central in all questions relating to the Newtonian potential,
e.g. in electromagnetism, in classical gravity.
There are many other interpretations. Thus, if f represents a density of heat
sources in Ω, say the distribution of radiators in a room and how much heat they
give off, then u is the equilibrium temperature in Ω when the walls of the room ∂Ω
are somehow kept at temperature 0◦. This is why the Poisson equation is sometimes
referred to as the diffusion equation, as it also models the diffusion of heat (and of
other things that may want to diffuse). We will return to the heat equation later on.
There is also a probabilistic interpretation of the Poisson equation, not unrelated
to the diffusion interpretation. For f = 2, u(x) is the expectation of the ﬁrst exit time
from Ω of a standard Brownian motion starting from point x. Roughly speaking, a
particle moving randomly in Rd and starting from a point x in ¯Ω will reach ∂Ω for
the ﬁrst time in an average time u(x).
Finally, when f = 0, the equation is known as the Laplace equation whose
solutions are the harmonic functions (it is clearly better to impose a nonzero boundary
condition to have u ̸= 0, or no condition at all). Harmonic functions are of course
extremely important in many areas.
The Poisson equation satisﬁes the maximum principle, see [20, 25, 61] among
others.
Theorem 1.4 (Maximum Principle) Let Ω be a bounded open subset of Rd and
u ∈C2(Ω) ∩C0( ¯Ω) be a solution of the Poisson equation (1.9) with f ≥0 in Ω.
Then we have u ≥0 in ¯Ω.
This is again a monotonicity result. For instance, in Fig.1.7, the membrane is pulled
upward by the applied force, and consequently bends upwards.
Let us close this section by rapidly mentioning the plate equation. A plate is to
a membrane what a beam is to a string: sheet iron, concrete wall, wood plank. The
clamped plate problem reads
5The minus sign is due to the physical convention that goes contrary to the mathematical convention
in this case.

18
1
Mathematical Modeling and PDEs
⎧
⎨
⎩
Δ2u = f in Ω,
u = ∂u
∂n = 0 on ∂Ω,
(1.10)
where the operator Δ2 = Δ ◦Δ =
∂4
∂x4
1 + 2
∂4
∂x2
1∂x2
2 +
∂4
∂x4
2 is called the bilaplacian
and ∂u
∂n = ∇u · n =
∂u
∂x1 n1 + ∂u
∂x2 n2 is the normal derivative of u on the boundary,
n denoting the unit exterior normal vector (we will go back to this later). This is a
fourth order boundary value problem. A real plate model also contains mechanical
constants which depend on the nature of the material as well as the thickness of the
plate.
All the problems considered up to now are stationary problems in which time
plays no role and only model equilibrium situations. Let us now talk about problems
where time intervenes, that is to say evolution problems.
1.4
The Transport Equation
Let us imagine a kind of gas composed of particles moving in an inﬁnite straight tube
T in R3 of the form R×D, where D is a disk of unit area in the (x2, x3) plane. Instead
of tracking each particle individually, which would be impossible in practice due to
their huge number, we can describe the gas by using a function u : T × R+ →R+,
where u(x, t) measures the quantity of particles, or rather their density at point x
and instant t. This is called a kinetic description. The initial density of particles at
t = 0 is denoted by u0(x) = u(x, 0). We assume it to be given, it is called an initial
condition.
Let us count the total number of particles in a section [y, y + δy] × D of the
tube. We disregard the fact that this number should be an integer. In fact, we consider
cases in which this integer is so large as to appear like a continuous quantity at
the macroscopic scale. Think of the Avogadro number and the fact that quantities
of matter are actually measured in moles. By deﬁnition of a density, at time t, this
quantity is equal to Q(y, δy, t) =
 y+δy
y

D u(x, t) dx1dx2dx3. For simplicity, we
assume that the density u only depends on x1, and we will henceforth drop the
subscript 1, so that Q(y, δy, t) =
 y+δy
y
u(x, t) dx, since the area of D is 1.
Now the question is how does the gas evolve in time? We clearly need to make
hypotheses on the individual motions of particles in order to answer this question.
For maximum simplicity, we assume here that all the particles move at the same
constant speed ae1, where a ∈R, is given. If a > 0, they all move to the right on
Fig.1.9, if a < 0, they all move to the left, and if a = 0, they do not move at all.
Since all particles move as a group at speed ae1, all the particles that were in the
tube section situated between {y} × D and {y + δy} × D at time 0, are going to be
located between {y + at} × D and {y + δy + at} × D at time t, and no other particle
will be there at the same time. Therefore, we have a conservation law: for all y, δy
and t

1.4 The Transport Equation
19
Fig. 1.9 Particles in the tube
Q(y + at, δy, t) = Q(y, δy, 0).
(1.11)
Let us differentiate relation (1.11) with respect to t. We obtain
0 = d
dt Q(y + at, δy, t) = d
dt
 y+δy+at
y+at
u(x, t) dx

= a[u(y + δy + at, t) −u(y + at, t)] +
 y+δy+at
y+at
∂u
∂t (x, t) dx.
Here again we ﬁnd a relation that begs to be divided by δy. So we oblige, then let δy
tend to 0 so that
∂u
∂t (y + at, t) + a ∂u
∂x (y + at, t) = 0.
Now y andt arearbitrary, thereforewecanperformthechangeof variables x = y+at
and obtain the following PDE problem:
⎧
⎨
⎩
∂u
∂t (x, t) + a ∂u
∂x (x, t) = 0 for (x, t) ∈R × R+,
u(x, 0) = u0(x) for x ∈R.
The PDE above is the transport equation (at velocity a), together with an initial
condition. The conjunction of the two form an initial value problem. There is no
boundary condition here because the space variable x ranges over the whole of R.
The PDE itself is of ﬁrst order in time and space.
Let us now proceed to solve the transport equation. Since the particles all move at
the same velocity a, we can look at the variation of u on the trajectory of one particle
t →x + at with x ﬁxed. We thus compute the derivative
d
dt

u(x + at, t)

= a ∂u
∂x (x + at, t) + ∂u
∂t (x + at, t) = 0.

20
1
Mathematical Modeling and PDEs
Fig. 1.10 The
characteristics are the dashed
straight lines with slope 1/a.
If a = 0, they are vertical
and there is no propagation
In other words, u is constant on the trajectories. In particular,
u(x + at, t) = u(x, 0) = u0(x).
(1.12)
The curves t →(x + at, t) in space-time R × R+—which are here straight lines—
are called the characteristics of the equation, and their use to solve the equation is
accordingly called the method of characteristics. They are often drawn in a space-
time diagram as shown on Fig.1.10.
To determine the value of u at a point (x, t) in space-time, it is enough to look at
the unique characteristic going through this point, take the point where it intersects
the t = 0 axis and take the value of u0 at that point, see Fig.1.10. This construction
simply amounts to rewriting formula (1.12) in the form
u(x, t) = u0(x −at),
(1.13)
which proves the uniqueness of the solution, due to an explicit formula!6
We have established the uniqueness of the solution, but have not yet established
its existence. Fortunately, we have an explicit formula, therefore we just need to
check that it actually is a solution. Let us compute the partial derivatives of u given
by formula (1.13), assuming u0 smooth enough. We have
∂u
∂x (x, t) = u′
0(x −at) and ∂u
∂t (x, t) = −au′
0(x −at),
where u′
0 is the ordinary derivative of u0. The PDE is thus clearly satisﬁed. Moreover,
theinitialconditionisalsotriviallysatisﬁedbysettingt = 0 informula (1.13).Hence,
we have found the unique solution.
It is apparent that u propagates or transports the initial data at constant speed a,
hence the name of the equation. This transport of the density is the macroscopic
manifestation of the individual microscopic behavior of the gas particles.
6Explicit solutions are very rare in PDE problems.

1.4 The Transport Equation
21
Fig. 1.11 Propagation of the
initial data u0
x
x0
u0
u(·,t)
x0 +at
If it was possible to animate Fig.1.11 on paper, the solid curve would be seen to
glide to the right at a steady pace (a > 0 in the picture) without changing shape,
after having coincided with the dashed curve at t = 0.
The transport equation has higher dimensional versions, which are much more
complicated than the one-dimensional version. It can also be set in open sets of Rd
instead of on the whole of Rd, see Chap.10. In this case, boundary conditions must
be added in addition to the initial condition, which makes it an initial-boundary value
problem. The boundary value question is delicate depending on whether the transport
velocity, which is then a vector, points inwards or outwards of the open set. Let us
illustrate this on a simple one-dimensional example. To be speciﬁc, we suppose that
the constant speed a is strictly positive and we consider the problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂u
∂t (x, t) + a ∂u
∂x (x, t) = 0 for x ∈]0, 1[, t > 0,
u(x, 0) = u0(x) for x ∈[0, 1],
u(0, t) = g(t) for t > 0,
where g is a given Dirichlet boundary condition at x = 0 such that g(0) = u0(0)
and g′(0) = −au′
0(0). This problem has the explicit solution for t > 0
u(x, t) =

u0(x −at)
for at ≤x ≤1,
g(t −x
a )
for 0 ≤x ≤min(at, 1).
(1.14)
This means that the initial condition is transported along the characteristics in the
region at ≤x ≤1, t > 0, whereas it is the boundary condition at x = 0 that is
transported, still along the characteristics, in the region 0 ≤x ≤min(at, 1), see
Fig.1.12.
In particular, expression (1.14) imposes the value u(1, t) = u0(1 −at) for t < 1
a
and u(1, t) = g

t −1
a

for t ≥1
a at x = 1, so that it is not possible to ascribe a
Dirichlet boundary condition at point x = 1. Note that in the case a < 0, it would
be the other way around: a boundary condition would be expected at x = 1 and no
condition at x = 0. We will go back to the transport equation in higher dimension
in Chap.10.
We can also consider periodic boundary conditions u(0, t) = u(1, t) for all t. In
thatcase,weconsideraperiodicinitialdatau0,whichweextendtoRby1-periodicity.
Let us take a > 0. We can use the previous formulas for the solution for t < 1
a .
We have u(x, t) = u0(x −at) in the lower right triangle of Fig.1.12, that is to say

22
1
Mathematical Modeling and PDEs
Fig. 1.12 The transport
equation in [0, 1] with a > 0
x = at
x
t
u0(x)
g(t)
0
1
x > at
x < at
for at < x ≤1, 0 < t ≤1
a . Therefore, u(1, t) = u0(1 −at) for 0 < t ≤1
a . The
periodic condition u(0, t) = u(1, t) then gives a Dirichlet boundary data at x = 0
for 0 < t ≤1
a , more precisely g(t) = u0(1 −at). We thus obtain an expression for
u(x, t) in the region 0 ≤x ≤at, t ≤1
a
u(x, t) = g

t −x
a

= u0(x + 1 −at) = u0(x −at),
since u0 has been extended by 1-periodicity. For t = 1
a , we thus have u(x, 1
a ) =
u0(x −1) = u0(x), and we can continue by periodicity in time on [ 1
a , 2
a ] and so on.
Note that considering the transport problem on R with the extended initial data u0
directly gives the periodic solution u(x, t) = u0(x −at) by restriction to [0, 1]. How-
ever, the above argument establishes the uniqueness of the solution of the problem
with periodic boundary condition.
The transport equation is relevant in many areas, whenever a spatially distributed
quantity u0 is transported by a velocity ﬁeld, think of a concentration of pollutants
carried away by the wind. A diffusion term of second order is often added, yielding
what are called convection-diffusion problems.
1.5
The Vibrating String Equation
Let us return to the elastic string in the context of dynamics. The displacement u of
the string is now a function of space x and time t. The analysis of applied forces is
exactly the same as in the static case, except that Newton’s law of motion says that
the resultant of the applied forces is equal to the time derivative of the momentum for
each piece that can be cut out of the whole string. There is no point in going through
all the detail again—it is actually a good exercise—and the result is

1.5 The Vibrating String Equation
23
T
∂u
∂x (x + δx, t) −∂u
∂x (x, t)

+
 x+δx
x
f (s, t) ds =
 x+δx
x
ρ ∂2u
∂t2 (s, t) ds,
where T is still the constant tension, ρ is the mass of the string per unit length, which
we assume to be a constant independent of x, i.e., that the string is homogeneous,
and ∂2u
∂t2 (x, t) is the acceleration of the string at point x and time t. Note that the
applied force f can now depend on time as well. Dividing by δx and letting δx tend
to 0, we obtain
T ∂2u
∂x2 (x, t) + f (x, t) = ρ ∂2u
∂t2 (x, t),
which is best rewritten as
∂2u
∂t2 (x, t) −c2 ∂2u
∂x2 (x, t) = 1
ρ f (x, t),
(1.15)
with c =

T
ρ . This partial differential equation, which is also called the one-
dimensional wave equation, is attributed to Jean le Rond d’Alembert. The constant
c is the propagation speed. We will see later that this equation propagates waves to
the right at speed c and to the left at speed −c. This is easily seen experimentally on
a long rope held by two persons. In fact, the vibrating string differential operator is
a composition of two transport operators
∂2
∂t2 −c2 ∂2
∂x2 =
 ∂
∂t −c ∂
∂x
 ∂
∂t + c ∂
∂x

=
 ∂
∂t + c ∂
∂x
 ∂
∂t −c ∂
∂x

,
hence the two propagation directions. Note that the propagation speed increases with
the tension and decreases with the mass of the string.
Equation(1.15) must be complemented by initial conditions that prescribe the
initial shape and initial velocity of the string (this is a problem of second order in
time)
u(x, 0) = u0(x), ∂u
∂t (x, 0) = u1(x) for all x ∈]0, L[,
(1.16)
and by boundary conditions, meaning here that the string is ﬁxed at both endpoints
u(0, t) = u(L, t) = 0, for all t ∈R+.
(1.17)
It should be noted that if a regular solution is expected, then a certain compatibility
between initial data (1.16) and boundary conditions (1.17) must be imposed
u0(0) = u0(L) = 0 and u1(0) = u1(L) = 0,
otherwise a discontinuity in the displacement or velocity will arise at t = 0.

24
1
Mathematical Modeling and PDEs
We will return in Chap.9 to a more in-depth study of the wave equation. For the
time being, let us consider a particular case: harmonic vibrations (see [71, 74] for
example). We are looking for solutions to Eq.(1.15) with right-hand side f = 0 and
by separation of variables, i.e., solutions of the special form u(x, t) = φ(x)ψ(t),
non identically zero and satisfying the boundary condition (1.17). Obviously, in the
case of harmonic vibrations, we cannot impose an arbitrary initial condition.
Let us rewrite the problem in this setting:

φ(x)ψ′′(t) −c2φ′′(x)ψ(t) = 0 for all x ∈]0, L[, t ∈R+,
φ(0)ψ(t) = φ(L)ψ(t) = 0, for all t ∈R+.
Naturally, if ψ = 0 then u = 0, which is not a very interesting solution. We thus
assume that there exists t0 such that ψ(t0) ̸= 0. It is therefore legal to divide by
ψ(t0), so that
⎧
⎨
⎩
−φ′′(x) + ψ′′(t0)
c2ψ(t0)φ(x) = 0 for all x ∈]0, L[,
φ(0) = φ(L) = 0.
This a boundary value problem in the variable x of a kind we have already encoun-
tered, and we know that if ψ′′(t0)
c2ψ(t0) ≥0, then φ = 0 is the unique solution. This again
means that u = 0, which is deﬁnitely not interesting. Let us thus consider the case
when λ = −ψ′′(t0)
c2ψ(t0) > 0 and see under which conditions there could exist a nonzero
solution.
Forgetting the boundary conditions for an instant, we recognize a second order
linear differential equation with constant coefﬁcients, the general solution of which
is of the form
φ(x) = A sin
√
λx

+ B cos
√
λx

.
The boundary condition φ(0) = 0 implies that B = 0. The boundary condition
φ(L) = 0 then either imposes A = 0, but then we are back to φ = 0, hence a trivial
solution u = 0, or sin(
√
λL) = 0, that is to say
√
λL = kπ for some k ∈Z,
or again
λ = k2π2
L2
and φ(x) = A sin
kπ
L x

,
where k is an integer. This gives a nontrivial solution, at last.
Without loss of generality, we take A = 1 and plug u(x, t) = sin
 kπ
L x

ψ(t) back
into the original wave equation, which gives an equation for ψ

1.5 The Vibrating String Equation
25
ψ′′(t) + c2 k2π2
L2 ψ(t) = 0,
that we solve immediately
ψ(t) = α sin
ckπ
L t

+ β cos
ckπ
L t

,
where α and β are arbitrary constants. Finally, we have found
u(x, t) =

α sin
ckπ
L t

+ β cos
ckπ
L t

sin
kπ
L x

,
and it is easily checked that all these functions solve the wave equation with the
homogeneous Dirichlet condition. We thus have found all the separated variable
solutions.
These solutions are harmonic vibrations of frequency νk = ck
2L =

T
ρ
k
2L indexed
by the integer k. The lowest possible frequency is obtained for k = 1. It is called
the fundamental and is the note that is heard from that string. The following integers
correspond to the harmonics of this note: k = 2 double frequency, one octave above
the fundamental, k = 3, k = 4 two octaves above the fundamental, etc., see Fig.1.13.
Naturally, the actual vibration of an ideal musical string is never a separated variable
solution, but a superposition of harmonics. This superposition gives the note its
timbre. From the point of view of mathematics, this is a question of Fourier series,
but we will not pursue this angle here, see Chap.9, Theorem9.2.
To close this section, we deduce from the formula for the frequency that a longer
string will ring a lower note, hence the relative lengths of the necks of a guitar and
Fig. 1.13 Three successive
harmonics: functions φ for
k = 1, 2, 3
0,25
0,5
0,75
-1
-0,5
0,5
1

26
1
Mathematical Modeling and PDEs
a bass and the different notes played on the same string on the frets, that a heavier
string also rings a lower note, hence the mass differences between the strings of a
guitar or piano, and that a higher tension yields a higher note, which is how such
instruments are tuned.
1.6
The Wave Equation
This is the higher dimensional analogue of the vibrating string equation. If we con-
sider a vibrating membrane in dimension two, we easily obtain the problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂2u
∂t2 (x, t) −c2Δu(x, t) = f (x, t) in Ω × R+,
u(x, t) = 0 on ∂Ω × R+,
u(x, 0) = u0(x), ∂u
∂t (x, 0) = u1(x) in Ω,
with c =

T
ρ , T is the tension and ρ the membrane mass per unit area. Note that
compatibility conditions between the boundary and initial conditions must again be
imposed if we expect a regular solution.
The harmonic vibration problem consists in looking for a solution of the form
u(x, t) = eiλtφ(x) (we no longer need to pretend that we do not know what ψ(t)
must be. . .), hence the problem
⎧
⎨
⎩
−Δφ(x) = λ2
c2 φ(x) in Ω,
φ(x) = 0 on ∂Ω,
(1.18)
with φ ̸= 0.
Problem (1.18) is an eigenvalue problem for the linear operator −Δ, that is to
say an inﬁnite dimensional spectral problem. This was already the case in dimension
one, but there was no need for the whole apparatus of self-adjoint compact operator
spectral theory since everything could be done by hand.
What we need to know for now is that the eigenvalues, i.e., the possible values
for μ = λ2
c2 for which problem (1.18) admits a nonzero solution, form an inﬁnite
increasing sequence 0 < μ1 < μ2 ≤μ3 ≤· · · , with μk →+∞when k →+∞,
which depends on the shape of Ω, see for example [5, 26, 28]. The situation is thus a
lot more complex than in dimension one, where the shape of Ω is just characterized
by its length L and we have an explicit formula for the eigenvalues. In particular, the
vibration frequencies λk
2π = c
√μk
2π are no longer proportional to successive integers. If
the ﬁrst eigenvalue still gives the fundamental tone, the following harmonics are not
in rational proportion to each other, and the timbre of the sound is entirely different.

1.6 The Wave Equation
27
This explains why a drum produces a sound that has nothing in common with the
sound produced by a guitar. It is all a question of dimensionality of the source of
vibrations.
A classical problem that was only solved fairly recently was formulated as “Can
you hear the shape of a drum?” The meaning of the question was to know whether
the knowledge of the spectrum, that is to say of the entire sequence (μk)k∈N∗, made
it possible to determine Ω up to a rigid motion. The answer is negative. There are
open sets in R2 of different shapes with exactly the same spectrum. Drums of these
different shapes would thus nonetheless sound exactly the same (up to the modeling
errors).
In higher dimensions, the wave equation is used to model the propagation of sound
waves in the air, the propagation of light waves in the void (the wave equation in this
case is deduced from Maxwell’s equations, the system of PDEs of electromagnetism).
Thereareallsortsofdifferentkindsofwaves,suchasseismicwavesoroceanicwaves,
the propagation of which is governed by equations that are more complex than the
wave equation.
1.7
The Heat Equation
The heat equation is yet another evolution equation, of a totally different nature from
the previous ones. For example, time is reversible in the wave equation: changing
t to −t does not change the equation. The heat equation describes the evolution of
temperature. It thus has a connection with thermodynamics and time can only ﬂow
from the past to the future. From the point of view of mathematics, changing t into
−t modiﬁes the equation and leads to problems with no solution in general.
The heat equation is as follows:
⎧
⎪⎪⎨
⎪⎪⎩
∂u
∂t (x, t) −Δu(x, t) = f (x, t) in Ω × R+,
u(x, t) = 0 on ∂Ω × R+,
u(x, 0) = u0(x) in Ω.
Here Ω ⊂Rd is an open set that represents a material body (for d = 1, 2 and 3) and
u(x, t) is its temperature at point x and time t. We have set all physical constants to
the value 1, as is customary in mathematics, which can in any case be achieved by a
change of units. The equation is of ﬁrst order in time and second order in space with
a boundary condition (of Dirichlet type here) and an initial condition. When f = 0,
the effect of the heat equation is to diffuse the initial condition.
The heat equation was discovered by Joseph Fourier, based on arguments of
exchange of heat between small particles. Let us rephrase in modern terms some of

28
1
Mathematical Modeling and PDEs
Fourier’s quite remarkable modeling arguments, published in his famous 1822, 639
pages long memoir, Théorie analytique de la chaleur7 [38].
Fourier starts by observing that two particles of the same substance at the same
temperature have no mutual effect on each other. However, if one of the particles
is hotter than the other, then there is a transfer of heat, which is a form of energy,
from the warmer to the cooler particle. He then states that the quantity of heat that
is transferred depends on the duration of the exchange, on the distance between the
two particles, which is assumed to be very small, on both temperatures and on the
nature of the substance.
Fourier goes on to assert that the exchange of heat is furthermore proportional
to the temperature difference, based on very precise experiments at the time. He
concludes that the amount of heat exchanged between particle m at temperature
u and particle m′ at temperature u′ during the period of time δt (small enough so
that there is no appreciable change of temperature during that time) is of the form
(u −u′)ϕ(δx)δt, where δx is the distance between the particles and ϕ is a rapidly
decreasing function that tends to 0 when δx →+∞, with the intended meaning that
heat exchange between particles is localized in space. This function depends on the
nature of the substance. He says nothing else about it.
Fourier next considers an inﬁnite homogeneous medium contained between two
parallel planes A and B so that A is kept at constant temperature a and B at constant
temperature b. Assume that A = {x = 0} and B = {x = e} where e > 0 is
the distance between the two planes.8 Fourier claims to show that the equilibrium
temperature proﬁle in the medium is given by u = a + b−a
e x (it is reasonable that the
temperature should only depend on x). What he actually explains is that this proﬁle
is consistent with equilibrium, in the sense that the heat ﬂux9 through all planes
C = {x = c} is independent of c. Hence, considering a slab of material contained
between two arbitrary planes C and C′, there is a perfect balance of incoming heat
on one plane and outgoing heat on the other plane, so that the temperature must stay
stationary.
His argument is as follows. Assume that the former temperature proﬁle is present
in the material, and consider two very close particles m and m′ situated on both sides
of C, say at x = c + ζ and x = c −ζ, and two other very close particles n and n′
situated on both sides of C′ at x = c′ + ζ and x = c′ −ζ, such that the distance d
between m and m′ is equal to the distance between n and n′. The exchanges of heat
between m and m′, and between n and n′, are then both equal to 2 b−a
e ζϕ(d)δt, and
independent of c and c′. Fourier argues that since the total heat ﬂux going through
C (and C′) results from all possible conﬁgurations of such pairs of particles, it is
likewise independent of c (and c′).
Using the same kind of reasoning with two slabs of different thicknesses e and e′,
left temperatures a and a′ and right temperatures b and b′, Fourier determines that
7In which, not only is the heat equation derived and solved in special cases, but Fourier series are
invented, the heat kernel appears, etc.
8e stands for épaisseur in French, i.e. thickness.
9The heat ﬂux is the quantity of heat going through a unit area in the plane during a unit of time.

1.7 The Heat Equation
29
the corresponding heat ﬂuxes F and F′ are such that
F
F′ =
a−b
e
a′−b′
e′
.
Calling K the heat ﬂux necessary to have a slab of unit thickness support a unit
temperature difference, he thus obtains the following law, which is now called the
Fourier law,
F = K a −b
e
= −K du
dx .
The constant K is characteristic of the material, it is called its heat conductivity.
Fourier shows that the ﬂux law actually holds for general, time-dependent temper-
ature repartitions, replacing the derivative du
dx with what we would call today the
gradient ∇u.
For simplicity, let us stay in the one-dimensional case, i.e., an inﬁnite medium
between two parallel planes, and derive the evolution equation in the manner of
Fourier. The temperature is thus an unknown function u of x and t.
Let C be the speciﬁc heat of the material, that is to say, the amount of heat needed
to heat up a unit of mass of the material from temperature 0 to temperature 1. Consider
a parallelepiped bounded by the planes x = x0 and x = x0 +δx, the section of which
has unit area. The total heat balance coming from the outside during a small period
of time δt is thus the ﬂux on the left minus the ﬂux on the right multiplied by the
duration
Q =

F(x, t) −F(x + δx, t)

δt = K
∂u
∂x (x0 + δx, t) −∂u
∂x (x0, t)

δt.
This heat input, which can be positive or negative, results in a change of temperature
u(x0, t + δt) −u(x0, t) =
Q
ρδxC ,
where ρ is the mass density of the material. Indeed, ρδx is the mass of the paral-
lelepiped under consideration. Therefore, we obtain
u(x0, t + δt) −u(x0, t)
δt
= K
ρC
∂u
∂x (x0 + δx, t) −∂u
∂x (x0, t)
δx
.
Letting δx and δt go to 0, we thus obtain the heat equation
∂u
∂t (x, t) = K
ρC
∂2u
∂x2 (x, t).

30
1
Mathematical Modeling and PDEs
Of course, the presence of an external heat source is easily taken into account in
the above balance of ﬂuxes argument and results in a nonzero right-hand side in the
heat equation. In the case of a stationary heat distribution, ∂u
∂t = 0, we recover the
Poisson equation.
Other arguments used by Fourier are actually very close to the ﬁnite difference
method that we will see later on. It is quite remarkable, and Fourier points it out
himself, that it is not necessary to know the ultimate nature of heat, which remained
mysterious at the time but which we now know to be the kinetic energy corresponding
to the random vibrations of molecules, and how it propagates precisely, in order to
be able to derive an extremely accurate macroscopic evolution equation.
1.8
The Schrödinger Equation
The Schrödinger equation is another evolution equation of an again totally different
nature. This time, u is a wave function in the sense of quantum mechanics. It is
complex-valued. The domain is the whole of R3. The equation reads10
i ∂u
∂t (x, t) + Δu(x, t) = 0 in R3 × R+.
The Schrödinger equation is the basic equation of quantum mechanics that governs
the evolution of the wave function of one particle in the absence of any potential,
that is in the void. Physical constants are missing (set to 1), such as Planck’s constant
ℏand the mass of the particle. We need to add an initial condition u(x, 0) = u0(x)
on R3.
Since the square of the modulus of the wave function is interpreted as a presence
probability density, we need to impose

R3 |u(x, t)|2 dx = 1.
Actually, if the initial condition satisﬁes this normalization condition, then the solu-
tion satisﬁes it automatically at all times.
Even though the Schrödinger equation presents a formal similarity with the heat
equation—ﬁrst order in time, second order in space—the presence of the imaginary
factor i gives it radically different properties. In particular, the Schrödinger equation
propagates waves, also not at all in the same way as the wave equation, whereas the
heat equation does not propagate waves (heat waves notwithstanding!).
10Here i2 = −1.

1.8 The Schrödinger Equation
31
LetusnotethatintheSchrödingerequationforasystemof N particles,thevariable
x must belong to R3N, which becomes rapidly difﬁcult for practical purposes when
N is large. . .11
As a general rule, physics is a nearly inexhaustible source of partial differential
equations problems. Let us cite the Dirac equation, a ﬁrst order system of equations
and relativistic version of the Schrödinger equation; Einstein’s equations of gen-
eral relativity, a system of nonlinear PDEs; the Boltzmann equation for the kinetic
description of gases, all the equations of ﬂuid mechanics, Euler, Stokes, Navier-
Stokes, and so on, and so forth. We refer for example to [21, 22, 24, 34] for other
physical models leading to the study of PDEs.
1.9
The Black and Scholes Equation
Physics is not by far the only source of PDEs. PDEs are also playing an increasing
role in diverse areas, such as biology, chemistry, material science, meteorology,
climatology, road trafﬁc modeling, crowd movement modeling, economy, ﬁnance,
among many others. Let us give a famous example in the latter area, the Black and
Scholes equation.
The question is to set the price of a call option. A call option is a contract between
a seller and a buyer, drawn at time t = 0. The contract gives the buyer the right to
buy an asset belonging to the seller, not right away but later and at a price K, the
strike, that is agreed on in advance. The contract has a price paid by the buyer to
the seller at t = 0, otherwise the seller would have no real reason to agree to it. For
the buyer, it is an insurance against future price ﬂuctuations since the strike is ﬁxed.
The price C must be computed in such a way that the game should be fair on
average, or at least seem to be fair. . . The possibility of option pricing hinges on a
modeling of the market and on a hypothesis called no arbitrage opportunity (no free
lunch) meaning that it is impossible to make sure gains without taking risks.
To make things a little more precise, the price of the asset at instant t is denoted St.
It is a continuous time stochastic process. In the case of an american call, the buyer
acquires the right to exercise the option, that is to say to buy the asset for the price
K, at any moment t ∈[0, T ], where T is an expiration date agreed on in advance.
The buyer is under no obligation to do so, and after time T , the option disappears.
Of course, the buyer has no interest in exercising the option at time t if St < K. In
this case, it is better to buy at the market price or not buy at all. On the other hand, the
buyer could also have invested the amount C at a ﬁxed interest rate r without risk.
Therefore, a proﬁt would only be made by exercising the option if St > ertC + K,
which is the decision criterion. The buyer bets this situation will occur before time
T , in which case he or she buys the asset for a price K and sells it back immediately
on the market at price St, thus pocketing the difference St −K. The global balance
11Think Avogadro’s number.

32
1
Mathematical Modeling and PDEs
of the operation is either −C if the option is not exercised or st −K −C if it is
exercised.
The seller always gains C and loses St −K if the buyer exercises the option, in
the sense that he or she could have sold at time t at the market price to somebody
else. Therefore, the bet is that the buyer will not exercise the option. The seller must
also seek to cover losses in case the buyer exercises the option. The price C is meant
to compensate for such potential losses.
The option price C is naturally a function of the asset price, which is represented
by a variable x ∈R+, because a price is nonnegative. It is also useful to introduce
the price at instant t, that is to say, the price the option would have if it was bought at
instant t with the same strike K and expiry date T . The option price is thus a function
in two variables C(x, t) (let us emphasize again that the space variable x is actually
a price).
We want to determine C(x, 0) as a function of x in order to deﬁne the terms of the
contract, since at t = 0, the price of the asset S0 is known and the price of the option
is then C(S0, 0). The price of the option at t = T is obviously C(x, T ) = (x −K)+
sincetheoptionisexercisedat T onlyifthepriceoftheassetislargerthan K,andthere
is no time left to invest C(x, T ) at a ﬁxed interest rate (the notation C+ = max(C, 0)
denotes the positive part of C).
At this point, stochastic modeling is needed in order to describe the evolution of
asset prices and to ensure a viable game, which we do not describe in detail. Anyway,
hypotheses are made concerning the St process. As recent world events have made
quite clear, such hypotheses are not always satisﬁed in real life, but let us proceed
anyway. At the end of this stochastic modeling phase, we end up with a deterministic
PDE for the function C(x, t) of the form
∂C
∂t (x, t) + 1
2σ 2x2 ∂2C
∂x2 (x, t) + μx ∂C
∂x (x, t) −rC(x, t) = 0 in R+ × [0, T ],
with the ﬁnal condition
C(x, T ) = (x −K)+.
This is the Black and Scholes equation. It has a ﬁnal condition and not an initial
condition because of modeling reasons, as we have seen, in fact the initial value is
the unknown quantity of interest. Another reason is that the principal part of the
differential operator is basically similar to a backward heat equation. We have seen
that the heat equation is incapable of going back in time. Therefore, a backward heat
equation needs a ﬁnal condition in order to be well-posed. There is an additional
difﬁculty since the coefﬁcients of the space derivatives are functions of the space
variables that vanish for x = 0. There is thus a degeneracy at the boundary, so
whether or not boundary conditions are in order it is not so clear. The constant σ is
called the asset volatility, a measure of the more or less erratic behavior of the asset
price, and μ is the trend, a sort of average growth rate.

1.9 The Black and Scholes Equation
33
These oddities of the Black and Scholes equation are mostly corrected by a simple
change of variable. Let us set u(y, τ) = C(ey, T −τ), then
∂u
∂τ −1
2σ 2 ∂2u
∂y2 −

μ −1
2σ 2∂u
∂y + ru = 0 in R × [0, T ],
with the initial (since time has been reversed) condition
u(y, 0) = (ey −K)+.
We are comfortably back with an ordinary heat equation with the right time direction,
whose effect is to diffuse the price, corrected by a transport term whose effect is to
make the price drift (in backward time) at speed −(μ −1
2σ 2). The ru term is an
updating term with respect to the interest rate which can be eliminated by a further
change of variables.
To conclude, let us remark that the Black and Scholes equation for one asset
is a two-dimensional equation, one space dimension and one time dimension. The
analogous equation for a portfolio of N assets is in N + 1 dimensions, which is a
source of difﬁculty for numerical approximation even for N moderately large.
1.10
A Rough Classiﬁcation of PDEs
We give a rather informal classiﬁcation of PDEs which is neither very precise, nor
exhaustive, but which has the advantage of giving a general idea of their properties.
Let us start with the Laplace operator Δ =
∂2
∂x2
1 + ∂2
∂x2
2 , and replace
∂
∂xi by multi-
plication by a variable ξi (which is more or less what the Fourier transform does).
The equation Δu = f is thus replaced by an equation of the type ∥ξ∥2 = g which
is the equation of a circle in R2, a special case of an ellipse. We say that the Pois-
son equation is elliptic. More generally, if we repeat the same operation on the
principal part d
i, j=1 ai j
∂2
∂xi x j of a general second order linear operator, we obtain
d
i, j=1 ai jξiξ j = g. If this yields the equation of an ellipsoid in Rd, then we say that
the equation is elliptic. This is the case if the matrix (ai j) is positive deﬁnite.
The same game played on the heat equation, replacing ∂/∂t by ξ0, yields ξ0−ξ 2
1 =
g, which is the equation of a parabola, or a paraboloid in higher dimension. We say
that the heat equation is parabolic.
Finally, in the case of the wave equation, we obtain ξ 2
0 −ξ 2
1 = g, the equation of
a hyperbola. We say that the wave equation is hyperbolic.
In the two-dimensional case, d = 2, the above classiﬁcation reduces to the dis-
criminant criterion, which is as follows. First we can always assume that a12 = a21
and let D = a2
12 −a11a22. Then the equation is elliptic if and only if D < 0, it is
parabolic if and only if D = 0 and it is hyperbolic if and only if D > 0.

34
1
Mathematical Modeling and PDEs
It is possible to give more precise deﬁnitions [24], but this is not useful here. The
important idea is that an elliptic equation (resp. a parabolic equation, resp. a hyper-
bolic equation) has more or less the same properties as the Poisson equation (resp.
the heat equation, resp. the wave equation). The transport equation is considered to
be hyperbolic.
1.11
What Comes Next
Up to now, we have presented a few examples of diverse mathematical models that
involvepartial differential equations. Wehavealsoobtainedexistenceanduniqueness
results for the simplest of these models, in Sect.1.1. The rest of the book is devoted
to the mathematical analysis and numerical approximation of the three main types
of partial differential equations, elliptic, parabolic and hyperbolic.
General elliptic problems exempliﬁed by the elastic membrane equation are
treated in Chap.4 for their mathematical analysis and Chaps.5 and 6 for their numer-
ical approximation. Parabolic equations are the subject of Chaps.7 and 8 for the theo-
retical and numerical aspects respectively. Finally, we consider hyperbolic equations
in Chaps.9 and 10 from both points of view.
In the next chapter, Chap.2, we take the simplest possible elliptic problem, i.e.,
the elastic string problem (1.2). Since the mathematical analysis of this problem has
already been performed, we skip directly to its numerical approximation. We use
this example to present the ﬁnite difference method. This is the earliest and simplest
numerical approximation method and it only requires elementary mathematical tools.
This method has drawbacks for more general elliptic problems, as we will see. We
will however return to the ﬁnite difference method later when dealing with evolution
equations of parabolic or hyperbolic type.

Chapter 2
The Finite Difference Method for Elliptic
Problems
Even though it can be shown that the boundary value problems introduced in Chap.1
admit solutions, they do not admit explicit solutions as a general rule, i.e., solutions
that can be written in closed form. Therefore, in order to obtain quantitative infor-
mation on the solutions, it is necessary to deﬁne approximation procedures that are
effectively computable. We present in this chapter the simplest of all approxima-
tion methods, the ﬁnite difference method. We apply the method to the numerical
approximation of a model problem, the Dirichlet problem for the Laplacian in one
space dimension. We then give some indications for the extension of the method to
Neumann boundary conditions and to two-dimensional problems.
2.1
Approximating Derivatives by Differential Quotients
The ﬁnite difference method is based on an elementary concept which is directly
connected to the deﬁnition of differentiation of functions. The idea is simply to
approximate any derivative by a differential quotient. As opposed to more sophisti-
cated methods, such as the ﬁnite element method that we will present later on, we
can directly treat the boundary value problem instead of a more abstract formula-
tion thereof, the variational formulation, see Chap.4. This explains the popularity
of the ﬁnite difference method, in spite of its shortcomings in particular in higher
dimensions.
Let us explain the method in one dimension. Using the deﬁnition of the derivative
of a function u at point x ∈R, we can write
u′(x) = lim
h→0
u(x + h) −u(x)
h
,
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_2
35

36
2
The Finite Difference Method for Elliptic Problems
and deduce that, when h ̸= 0 is small, the differential quotient [u(x + h) −u(x)]/h
should be a good approximation of the ﬁrst order derivative u′(x), in the sense that
the error induced by this approximation goes to 0 when h goes to 0. If the function
is regular (in a neighborhood of x), we can make a precise estimate of this error by
using a Taylor–Lagrange expansion. Namely, if u is C2 in a neighborhood of x, we
have
u(x + h) = u(x) + hu′(x) + h2
2 u′′(ξ),
(2.1)
where ξ lies between x and x + h. Therefore, taking h0 > 0 and setting C =
supy∈[x−h0,x+h0] |u′′(y)|/2, we see that

u(x + h) −u(x)
h
−u′(x)
 ≤C|h|,
for all h ̸= 0 such that |h| ≤h0, In other words, the error made by replacing the
derivative u′(x) by the differential quotient u(x+h)−u(x)
h
is of the order of h. We say
that we have a consistent approximation of order one of u′ at point x. More generally,
we say that we have a consistent approximation of order p (p > 0) of u′(x) if there
exists a constant C, which does not depend on h, such that this error is bounded by
Chp.
Other consistent approximations are possible. For example, the differential quo-
tient [u(x) −u(x −h)]/h is also a consistent approximation of order one of u′(x).
One way of improving the accuracy is to center the approximation, by using the
points x + h and x −h to form the differential quotient u(x+h)−u(x−h)
2h
. In fact, if u is
C3 in a neighborhood of x, we can write
u(x + h) = u(x) + hu′(x) + h2
2 u′′(x) + h3
6 u(3)(ξ +),
u(x −h) = u(x) −hu′(x) + h2
2 u′′(x) −h3
6 u(3)(ξ −),
where ξ + and ξ −are located between x −h and x + h. Subtracting the two formulas
above and using the intermediate value theorem, we obtain
u(x + h) −u(x −h)
2h
= u′(x) + h2
6 u(3)(ξ),
for some ξ between x −h and x + h. We deduce the following estimate of the error:
For all h, 0 < |h| ≤h0, we have

u(x + h) −u(x −h)
2h
−u′(x)
 ≤Ch2,

2.1 Approximating Derivatives by Differential Quotients
37
where
C = 1
6
sup
y∈[x−h0,x+h0]
|u(3)(y)|.
We have then deﬁned a consistent approximation of order two of u′(x). Note that the
above error estimate is a priori not valid if u is less regular than C3.
The model problem (1.2) of Chap.1 or problem (2.4) below involve second deriva-
tives so that we also need to approximate second derivatives by differential quotients.
Lemma 2.1 Let us suppose that u is C4 on the interval [x −h0, x + h0] (h0 > 0).
Then there exists a constant C such that, for all h, 0 < |h| ≤h0, we have

u(x + h) −2u(x) + u(x −h)
h2
−u′′(x)
 ≤Ch2.
(2.2)
In other words, the differential quotient u(x+h)−2u(x)+u(x−h)
h2
is a consistent approxi-
mation of order two of the second derivative of u at point x.
Proof The proof is again based on Taylor–Lagrange expansions. We have
u(x + h) = u(x) + hu′(x) + h2
2 u′′(x) + h3
6 u(3)(x) + h4
24u(4)(ξ +),
u(x −h) = u(x) −hu′(x) + h2
2 u′′(x) −h3
6 u(3)(x) + h4
24u(4)(ξ −),
where ξ + and ξ −are between x −h and x + h. Adding the two formulas above and
using the intermediate value theorem, we thus obtain
u(x + h) −2u(x) + u(x −h)
h2
= u′′(x) + h2
12u(4)(ξ),
(2.3)
for some ξ between x −h and x + h. It follows that estimate (2.2) holds with
C =
sup
y∈[x−h0,x+h0]
|u(4)(y)|
12
,
for 0 < |h| ≤h0, which completes the proof.
□
Remark 2.1 Let us remark that this error estimate also depends on the regularity of
u. For example, if u is only C3, the Taylor expansion cannot be carried out up to
fourth order, and gives an error estimate of order h. Conversely, any regularity higher
than C4 will not improve the accuracy, which remains of order h2. There is thus no
reason to use a higher order Taylor–Lagrange expansion, see Remark 8.1 of Chap.8.
□

38
2
The Finite Difference Method for Elliptic Problems
Remark 2.2 We remark that
u(x + h) −2u(x) + u(x −h)
h2
= 1
h
u(x + h) −u(x)
h
−u(x) −u(x −h)
h

= D+
h D−
h u(x) = D−
h D+
h u(x),
where the operators D+
h and D−
h are deﬁned by
D+
h u(x) = u(x + h) −u(x)
h
,
D−
h u(x) = u(x) −u(x −h)
h
.
These are precisely the operators, respectively called forward difference operator
and backward difference operator, of order one that we have already met for the
approximation of ﬁrst order derivatives.
□
2.2
Application to a One-Dimensional Model Problem
We consider the model problem consisting in ﬁnding u: [0, 1] →R such that

−u′′(x) + c(x)u(x) = f (x),
x ∈]0, 1[,
u(0) = g0,
u(1) = g1,
(2.4)
where c and f are two given continuous functions, deﬁned on [0, 1], and g0 and g1
are given constants. We know that this problem has a unique solution if c ≥0, by the
shooting method, see Theorem 1.2 of Chap.1. We will thus take c ≥0 in the sequel.
In the particular case c = 0, u has the following expression
u(x) =
 1
0
G(x, y) f (y)dy + g0 + x(g1 −g0),
(2.5)
where G is the Green function, given by
G(x, y) = x(1 −y) if y ≥x,
G(x, y) = y(1 −x) if y ≤x.
In the general case (i.e., c ̸= 0), formula (2.5) still holds with a Green function which
is not explicitly known. The idea is thus to deﬁne an approximation of u.
More precisely, we are going to look for an approximation of u at speciﬁc points xi
of the interval [0, 1] in ﬁnite number. These points are called discretization points or
gridpoints. For simplicity, weassumeherethat thesepoints areuniformlydistributed,
see Fig.2.1, i.e., of the form xi = ih, i ∈{0, . . . , N + 1}, where N is a given positive

2.2 Application to a One-Dimensional Model Problem
39
h
0 = x0
x1
xi−1
xi
···
xi+1
xN+1 = 1
···
Fig. 2.1 A uniform 1d grid and grid points
integer and h = 1/(N + 1) is the grid space step, or grid step in short. Even though
the notation does not make it clear, we see that xi not only depends on i, but also on h
or equivalently on N. Note that 0 ≤h ≤1 and that h goes to 0 when the number
N + 2 of grid points goes to inﬁnity.
We have at the ends of the interval x0 = 0 and xN+1 = 1. The other grid points
xi, for i ∈{1, . . . , N}, are called internal points.
We would like to compute an approximated value ui of the unknown exact value
u(xi) at each point xi, with i ∈{1, . . . , N}. We naturally set at the ends u0 = g0
and uN+1 = g1, i.e., we enforce the exact boundary condition at the discrete level.
For the internal points, the idea is to start from the ﬁrst equation in system (2.4) ex-
pressed at point xi, and approximate u′′(xi) based on the central differential quotient of
Lemma 2.1.
The unknowns of the discrete problem are thus only u1, . . . , uN. Just like xi, we
remark that ui also depends on h or N. Let us denote by Uh the vector in RN with
components ui, for i ∈{1, . . . , N}.
Recall that c ∈C0([0, 1]) and f ∈C0([0, 1]). We start from problem (2.4).
We replace each exact value u(xi) at each grid point by the corresponding approx-
imated value ui, and the second order derivative u′′(xi) at each internal point xi,
i ∈{1, . . . , N}, of the grid by the central difference quotient ui+1−2ui+ui−1
h2
. We thus
get the following discrete system expressed solely in terms of the approximated
discrete values ui:
⎧
⎨
⎩
−ui+1 −2ui + ui−1
h2
+ c(xi)ui = f (xi),
i ∈{1, . . . , N},
u0 = g0,
uN+1 = g1.
(2.6)
At this point, it must be emphasized that there is no indication that ui ≈u(xi),
in any sense whatsoever. We say that we have discretized the problem by a ﬁnite
difference method, using the three point central ﬁnite difference scheme or simply
three point scheme.
The discrete problem has the following equivalent vector formulation:
AhUh = Fh,
(2.7)

40
2
The Finite Difference Method for Elliptic Problems
where Ah is the N × N matrix deﬁned by
Ah = A0
h +
⎛
⎜⎜⎜⎜⎝
c(x1)
0
· · ·
0
0
c(x2) ...
...
...
...
...
0
0
· · ·
0 c(xN)
⎞
⎟⎟⎟⎟⎠
,
(2.8)
with
A0
h = 1
h2
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
2 −1 0 · · · 0
−1 2 −1 ...
...
0
... ... ...
0
...
... −1 2 −1
0 · · · 0 −1 2
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
,
(2.9)
and
Fh =
⎛
⎜⎜⎜⎜⎜⎝
f (x1) + g0
h2
f (x2)
...
f (xN−1)
f (xN) + g1
h2
⎞
⎟⎟⎟⎟⎟⎠
.
(2.10)
In order to compute the discrete solution Uh, we thus have to solve the linear
system (2.7). The ﬁrst point to check is whether or not this system has a solution for
any right-hand side, in other words, whether or not the matrix Ah is invertible. This
is a consequence of the following proposition.
Proposition 2.1 The matrix Ah is symmetric. Furthermore, when c ≥0, it is positive
deﬁnite.
Proof The matrix Ah is clearly symmetric. Let us show it is positive deﬁnite when
c ≥0. Let V be a vector in RN with components vi, i ∈{1, . . . , N}. Let us compute
V TAhV . First, thanks to the assumption c ≥0, we get
V TAhV = V TA0
hV +
N

i=1
c(xi)v2
i ≥V TA0
hV.
It is thus sufﬁcient to show that A0
h is positive deﬁnite. A simple computation gives
h2 V TA0
hV = v2
1 + (v2 −v1)2 + (v3 −v2)2 + · · · + (vN−1 −vN)2 + v2
N,
from which we deduce that V TA0
hV ≥0. Moreover, if V TA0
hV = 0, then each term in
the sum above has to be zero, which implies that 0=v1 =v2 −v1 =· · ·=vN−1 −vN =
vN. Thus v1 = v2 = · · · = vN = 0, i.e. V = 0, which completes the proof.
□

2.2 Application to a One-Dimensional Model Problem
41
It is well-known that positive deﬁnite matrices are invertible, since AhV = 0
implies that V TAhV = 0, thus
Corollary 2.1 The discrete system (2.7) has one and only one solution Uh for any
N and Fh.
It follows that the ﬁnite difference scheme (2.6) is well deﬁned. Note that the
matrix of the linear system (2.7) is sparse, i.e., it has many zero elements. More
precisely, Ah is a tridiagonal matrix in the sense that the only nonzero elements are
either on the diagonal or on the super- or sub-diagonal.
The next question concerns in which sense the discrete system solution Uh is
actually an approximation of the exact solution u of the boundary value problem (2.4).
The two quantities are of different nature since Uh is a vector in RN and u is a function
deﬁned on [0, 1]. The only reasonable way of comparing the two is to compare the
ith component ui of Uh with the value u(xi) of u at the corresponding grid point,
since this is what the ﬁnite difference scheme is intended for. In particular we would
like to know if ui −u(xi) →0 when the grid step h goes to zero and at which rate,
in a sense that is made precise in the next section.
2.3
Convergence of the Finite Difference Method
We study the convergence of the method in the case of the model problem (2.4). The
analysis below applies to more general situations.
In order to compare the discrete solution Uh ∈RN with the exact solution u,
we introduce the grid sampling operator and what is meant by convergence in this
context.
Deﬁnition 2.1 The grid sampling operator Sh : C0([0, 1]) →RN is deﬁned by
Sh(v) =
⎛
⎜⎜⎜⎝
v(x1)
v(x2)
...
v(xN)
⎞
⎟⎟⎟⎠.
We say that the method (2.7) is convergent if
∥Uh −Sh(u)∥∞→0 when h →0.
Moreover, we say that the scheme is convergent of order p if there exists p > 0 and
a constant C which do not depend on h such that, for all h > 0 we have
∥Uh −Sh(u)∥∞≤Chp.

42
2
The Finite Difference Method for Elliptic Problems
Remark 2.3 The norm ∥X∥∞= maxi∈{1,...,N} |Xi| is deﬁned on RN, and thus de-
pends on N. This speciﬁc choice is made for simplicity. We will see more gen-
eral choices of norms in Chap.8 in the study of the numerical approximation of
the heat equation. With the present choice of norm, convergence thus means that
maxi∈{1,...,N} |ui −u(xi)| →0 when N →+∞, which is quite natural. Remember
that h →0 is equivalent to N →+∞and that ui and xi also depend on N and h. □
In order to evaluate the error Uh −Sh(u) of the method, we next introduce the
truncation error of the scheme (2.6) or equivalently (2.7).
Deﬁnition 2.2 The truncation error of the scheme AhUh = Fh is the vector in RN
denoted by εh(u) and deﬁned by
εh(u) = Ah(Sh(u)) −Fh.
We say that the scheme is consistent if
lim
h→0 ∥εh(u)∥∞= 0.
Moreover, we say that the scheme is consistent of order p if there exists p > 0 and a
constant C which do not depend on h such that, for all h > 0 we have the following
error estimate
∥εh(u)∥∞≤Chp.
Remark 2.4 The truncation error is not to be confused with the error of the method
itself. The evaluation of the truncation error is however an important intermediate
step in the evaluation of the error estimate.
□
Let us thus evaluate the truncation error of the scheme. We assume that u is C4
on [0, 1], which means that f is C2. Using formula (2.3), we obtain
εh(u) = −h2
12
⎛
⎜⎜⎜⎝
u(4)(ξ1)
u(4)(ξ2)
...
u(4)(ξN)
⎞
⎟⎟⎟⎠,
where each point ξi is such that ξi ∈]xi−1, xi+1[. We deduce that
∥εh(u)∥∞≤h2
12 max
y∈[0,1] |u(4)(y)|.
(2.11)
We have thus shown the
Proposition 2.2 Let us suppose that the solution u of problem (2.4) is C4 on [0, 1].
Then the scheme (2.6) is consistent of order two.

2.3 Convergence of the Finite Difference Method
43
Remark 2.5 We remark that, if the exact solution u is such that its derivative of order
4 is zero, then the truncation error εh(u) is zero, so that Ah(Uh −Sh(u)) = 0. Since
the matrix Ah is invertible, it follows that Uh = Sh(u), which means that for any
i ∈{0, . . . , N + 1}, we have ui = u(xi). The discrete solution is thus equal to the
exact solution at each grid point (internal or not)! This is very speciﬁc to the case
when the exact solution u happens to be a polynomial function of degree at most 3,
see the proof of Proposition 2.5 where this property is used. Of course, in general
ui ̸= u(xi).
□
Let us now compute the error of the method. By deﬁnition of the scheme on the
one hand, and of the truncation error on the other hand, we have
AhUh = Fh
and
AhSh(u) = Fh + εh(u).
Subtracting one from the other, we deduce that
Ah(Uh −Sh(u)) = −εh(u).
Since Ah is invertible, this implies that the error is given by
Uh −Sh(u) = −(Ah)−1εh(u).
(2.12)
We thus have a decomposition of the error into two independent terms, the truncation
error εh(u) and the matrix term (Ah)−1. We now proceed to show the following
convergence result:
Theorem 2.1 Let us suppose that c ≥0. If the solution u of problem (2.4) is C4 on
[0, 1], then the scheme (2.6) is convergent of order two. More precisely, we have
||Uh −Sh(u)||∞≤h2
96 max
x∈[0,1] |u(4)(x)|.
(2.13)
In order to show this result, we need to brieﬂy introduce a few concepts, and in
particular the deﬁnition of matrix norms. In this chapter all matrices are real. We
refer for example to [6, 18, 53] for details.
Let ∥· ∥be a given norm on RN. For any N × N matrix A, we denote by |||A||| the
associated induced matrix norm or operator norm deﬁned by
|||A||| =
sup
X∈RN,X̸=0
∥AX∥
∥X∥=
sup
X∈RN,∥X∥=1
∥AX∥=
sup
X∈RN,∥X∥≤1
∥AX∥.
The mapping ||| · ||| satisﬁes the usual properties of norms, i.e.,
1. |||A||| ≥0 and |||A||| = 0 if and only if A = 0,
2. for all real λ, |||λA||| = |λ| |||A|||,
3. the triangle inequality |||A + B||| ≤|||A||| + |||B|||, for all matrices A and B.

44
2
The Finite Difference Method for Elliptic Problems
It also satisﬁes the following additional property: For all matrices A and B, we have
|||AB||| ≤|||A||| |||B|||.
We say that a matrix norm is submultiplicative. Finally, by deﬁnition, we have for
all N × N matrices A and for all vectors X in RN,
∥AX∥≤|||A||| ∥X∥.
(2.14)
We note that |||I||| = 1 for any induced matrix norm. In this chapter, we only use the
∥· ∥∞norm on RN. The induced matrix norm is given below.
Proposition 2.3 Let A be a N × N matrix with entries Ai j. We have
|||A|||∞=
max
i∈{1,...,N}
N

j=1
|Ai j|.
(2.15)
Proof Let us set M = maxi∈{1,...,N}
N
j=1 |Ai j|. Let X be an arbitrary vector in RN
with ∥X∥∞= 1. By deﬁnition of ∥· ∥∞, we have
∥AX∥∞=
max
i∈{1,...,N} |
N

j=1
Ai jX j| ≤
max
i∈{1,...,N}
N

j=1
|Ai j| |X j| ≤M,
because for any j ∈{1, . . . N}, |X j| ≤∥X∥∞= 1. We deduce that |||A|||∞≤M.
In order to show (2.15), it is thus sufﬁcient to ﬁnd a vector X ∈RN, with
∥X∥∞= 1, such that ∥AX∥∞= M. By deﬁnition of M, there exists an index
i0 ∈{1, . . . , N} such that M = N
j=1 |Ai0 j|. Let us consider the vector X ∈RN
deﬁned by: X j = 1, if Ai0 j ≥0 and X j = −1 otherwise. We clearly have ∥X∥∞= 1
and (AX)i0 = N
j=1 Ai0 jX j = N
j=1 |Ai0 j| = M ≥0. This shows that |||A|||∞≥M,
which completes the proof.
□
Let us now introduce another useful concept, inverse nonnegative matrices.
Deﬁnition 2.3 We say that a vector X ∈RN is nonnegative, and we write X ≥0, if
all its components Xi are nonnegative, and that a N ×N matrix A is nonnegative, and
we write A ≥0, if all its entries Ai j are nonnegative. A matrix A is said to be inverse
nonnegative or monotone if it is invertible and its inverse matrix is nonnegative.
We also note X ≤0 or A ≤0 whenever −X ≥0 or −A ≥0. We have the
following characterization of inverse nonnegative matrices:
Lemma 2.2 A matrix A is inverse nonnegative if and only if for all vectors X in RN,
we have
if AX ≥0 then X ≥0.
(2.16)

2.3 Convergence of the Finite Difference Method
45
Proof Let us suppose that A is inverse nonnegative. Let X be a vector in RN such
that AX ≥0. The matrix A−1 is nonnegative, therefore the vector X = A−1(AX) is
nonnegative by the usual formula for matrix-vector products.
Conversely, let A satisfy (2.16). We ﬁrst show that A is invertible. Let X be a
vector in RN such that AX = 0. In particular AX ≥0, so that, thanks to (2.16), we
have X ≥0. Likewise A(−X) ≥0, so that −X ≥0. It follows that X = 0, which
shows that A is invertible. Let us next show that the matrix A−1 is nonnegative. It
is sufﬁcient to show that each column of A is nonnegative. Let Ci be one of these
columns. By deﬁnition, Ci = A−1Ei, where Ei is the ith vector of the canonical basis
of RN (i.e., all its components are zero, except the component of index i which is 1).
Since Ei ≥0 and ACi = Ei, we deduce from (2.16) that Ci ≥0, which completes
the proof.
□
We now apply the above deﬁnitions and properties to the discrete problem (2.7).
Proposition 2.4 Let us suppose that c ≥0. The matrix Ah deﬁned by (2.8)–(2.9) is
inverse nonnegative.
Proof We use the above characterization of inverse nonnegative matrices. Let X be
a vector in RN such that AhX ≥0. Let us show that X ≥0. Let i0 be the smallest
index such that Xi0 ≤Xi, for all i ∈{1, . . . , N}. We will show that Xi0 ≥0, which
implies the result.
Let us ﬁrst consider the case when i0 = 1. The ﬁrst component of AhX is nonneg-
ative, therefore
[2 + h2c(x1)]X1 −X2 = (X1 −X2) + [1 + h2c(x1)]X1 ≥0.
As X1 −X2 ≤0 and 1+h2c(x1) ≥1 we deduce that X1 ≥0. The same proof applies
in the case i0 = N.
Let us now consider the case when i0 ∈{2, . . . , N −1}. Since (AhX)i0 ≥0, we
obtain
−Xi0−1 +[2+h2c(xi0)]Xi0 −Xi0+1 = (Xi0 −Xi0−1)+(Xi0 −Xi0+1)+h2c(xi0)Xi0 ≥0,
(2.17)
with Xi0 −Xi0−1 ≤0 and Xi0 −Xi0+1 ≤0 by deﬁnition of i0. If c(xi0) = 0, we get
Xi0 −Xi0−1 = Xi0 −Xi0+1 = 0. In other words, i0 −1 is also an index where the
components of X achieve their minimum, which is a contradiction since i0 −1 < i0.
On the other hand, if c(xi0) > 0, then by (2.17), we see that Xi0 ≥0.
□
Remark 2.6 The above proof shows that if we have AhUh = Fh with Fh ≥0, then
Uh ≥0. This is the discrete maximum principle, which is the discrete analogue of
the maximum principle of Theorem 1.3 of Chap.1.
□
Remark 2.7 We note that the matrix Ah in addition to being inverse nonnegative
or monotone, is also such that its off-diagonal coefﬁcients are nonpositive. Such
matrices are called M-matrices. These matrices have applications in many ﬁelds in

46
2
The Finite Difference Method for Elliptic Problems
addition to the discretization of differential operators, notably in probability theory
and economics among others, see [54, 79].
□
We now are in a position to prove Theorem 2.1. Starting from (2.12) and using
formula (2.14), we deduce the following estimate:
∥Uh −Sh(u)∥∞≤|||(Ah)−1|||∞∥εh(u)∥∞.
We have already estimated the second term ∥εh(u)∥∞, see (2.11). We thus need an
estimate of the quantity |||(Ah)−1|||∞, which is called a stability estimate.
Proposition 2.5 Let us suppose that c ≥0. We have the following estimate, for all
h =
1
N+1,
|||(Ah)−1|||∞≤1
8.
(2.18)
Proof We ﬁrst remark that A−1
h
−(A0
h)−1 = A−1
h [A0
h −Ah](A0
h)−1. Since c ≥0, the
matrix A0
h −Ah is diagonal with nonpositive entries. By Proposition 2.4, A−1
h
≥0
and (A0
h)−1 ≥0. Therefore A−1
h
−(A0
h)−1 ≤0 by the usual formula for matrix
products. Since both A−1
h
and (A0
h)−1 are nonnegative, formula (2.15) then implies
that |||A−1
h |||∞≤|||(A0
h)−1|||∞. In order to get estimate (2.18), it is thus sufﬁcient to
show that
|||(A0
h)−1|||∞≤1
8.
The matrix (A0
h)−1 is nonnegative, therefore by formula (2.15)
|||(A0
h)−1|||∞= ∥(A0
h)−1E∥∞,
where E is the vector in RN all the components of which are equal to 1. We notice
that (A0
h)−1E is precisely the discrete solution Uh of system (2.7) in the particular
case c = 0 and for Fh = E. In other words, it is the discrete solution associated with
the following boundary value problem:

−¯u′′(x) = 1,
x ∈]0, 1[,
¯u(0) = 0,
¯u(1) = 0.
This problem clearly has the explicit solution ¯u(x) = x(1 −x)/2, a second degree
polynomial. Thanks to Remark 2.5, p. 43, we know that, in this particular case, the
discrete solution coincides with the exact solution at each grid point. We thus have
((A0
h)−1E)i = ¯u(xi) = xi(1 −xi)/2. Consequently,
∥(A0
h)−1E∥∞≤sup
x∈[0,1]
|¯u(x)| = ¯u
1
2

= 1
8,
which completes the proof.
□

2.3 Convergence of the Finite Difference Method
47
Fig. 2.2 Plot of ¯u(x) = x(1−x)
2
in solid line and Uh = (A0
h)−1E for N = 10 with ◦marks
Remark 2.8 Note that the convergence proof relies on two fundamental properties:
consistency and stability. We will encounter a very similar idea in the study of
numerical approximations of the heat and wave equations in Chaps.8 and 9.
□
We plot in Fig.2.2 the function u and the discrete solution Uh = (A0
h)−1E in the
particular case used above in the stability estimate when the discrete solution happens
to coincide with the exact solution at the grid points.
Let us illustrate the preceding results with a numerical example. We take c(x) =
1000 sin(10πx)2 and f (x) = 1, and we compute the ﬁnite difference approximations
of the corresponding boundary value problem for N = 19, 29, 49, 99, and 199, i.e.,
h = 0.05, 0.033, 0.02, 0.01, and 0.005, by solving the associated linear systems.
To compare the results, we plot in Fig.2.3 the computed discrete values ui against
xi for these ﬁve cases on the same plot, with different marks for each value of N
and linear interpolations1 in between points. The solid curve with no marks is a
so-called “reference” solution that is meant to represent the exact solution, but that
we actually also computed using ﬁnite differences with N = 8000, since no explicit
formula seems to be available. The computation is performed with Scilab, a free,
general purpose scientiﬁc computing package (http://www.scilab.org/).
1This is for visualization purposes only. The ﬁnite difference method does not compute a function
on [0, 1].

48
2
The Finite Difference Method for Elliptic Problems
0.0000
0.0005
0.0010
0.0015
0.0020
0.0025
0.0030
0.0035
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Fig. 2.3 Convergence of the ﬁnite difference method
2.4
Neumann Boundary Conditions
Let us brieﬂy describe the method in the case of Neumann boundary conditions at
both ends of the interval. The Neumann boundary conditions are different from the
Dirichlet conditions seen up to now. For a one-dimensional second order problem,
they concern the values of the ﬁrst derivative of the unknown function at the ends of
the interval, instead of the values of the function itself. In terms of modeling, in the
heat equation interpretation, they consist in imposing the heat ﬂux at the boundary
instead of imposing the temperature.
We thus consider the problem: Find u: [0, 1] →R solution of

−u′′(x) + c(x)u(x) = f (x),
x ∈]0, 1[,
u′(0) = g0,
u′(1) = g1,
(2.19)
where c and f are two given functions (c ∈C0([0, 1]) and f ∈C0([0, 1])), and g0
and g1 are two given constants. It is easy to prove by the shooting method that such a
function u exists if we suppose, for example, that there exists a constant c0 > 0 such
that c ≥c0 (see also the variational theory in Chap.4). More generally, if c ≥0 is not
identically 0, we have existence and uniqueness. On the other hand, if c = 0 there
is no uniqueness, hence no existence in general. Indeed, if a solution exists, then
we can add any arbitrary constant and still ﬁnd another solution of problem (2.19).
This is one important difference with the Dirichlet problem (2.4) studied before. For
simplicity, we will assume a bound from below c ≥c0 > 0 from now on.
We now proceed to deﬁne a ﬁnite difference approximation of this problem.
We use the same uniform grid in [0, 1] as before, i.e., with grid points xi = ih,
i ∈{0, . . . , N + 1}, where h =
1
N+1 and N is a given positive integer. There will be

2.4 Neumann Boundary Conditions
49
a discrete unknown ui attached to each grid point, including for i = 0 and i = N +1,
since the values of u at the ends of the interval are not prescribed by the continuous
problem as in the case of the Dirichlet problem.
For the internal grid points, i.e. points xi, with i ∈{1, . . . , N}, we will use the
same three point scheme as before to approximate the second order derivative at
this point. The problem is now to approximate the Neumann boundary condition,
since it cannot be satisﬁed exactly—contrarily to the Dirichlet case. The condition
does not even make sense at the discrete level. There are several possibilities. The
simplest one consists in approximating the ﬁrst order derivative by one of the two
decentered difference schemes introduced in Remark 2.2. More precisely, we can
use the forward difference to approximate u′(0) and the backward difference to
approximate u′(1), i.e.,
u′(0) ≈D+
h u(0) = u(h) −u(0)
h
,
u′(1) ≈D−
h u(1) = u(1) −u(1 −h)
h
.
This suggests the following approximation of the boundary conditions:
u0 −u1
h
= −g0,
uN+1 −uN
h
= g1.
The reason for the minus sign in the ﬁrst relation is that the Neumann boundary
condition at x = 0 is more naturally written as −u′(0) = −g0, in terms of integration
by parts and other considerations that we will see later. The discrete problem is thus
equivalent to the linear system
BhUh = Fh,
(2.20)
with
Bh = 1
h2
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
h
−h
0
...
...
0
−1
2 + h2c(x1)
−1
...
...
0
−1
2 + h2c(x2) −1
...
...
0
...
...
...
...
0
...
...
−1
2 + h2c(xN) −1
0
...
...
0
−h
h
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
where the unknown Uh is the vector in RN+2 of components ui, i ∈{0, . . . , N + 1}.
Note that Bh is an (N +2)×(N +2) matrix. The right-hand side of the linear system
is given by

50
2
The Finite Difference Method for Elliptic Problems
Fh =
⎛
⎜⎜⎜⎜⎜⎝
−g0
f (x1)
...
f (xN)
g1
⎞
⎟⎟⎟⎟⎟⎠
.
We ﬁrst remark that the matrix Bh is no longer symmetric due to the ﬁrst and last
lines. Before studying the convergence of the numerical method, let us make a few
comments on the case c = 0. In this case, it is easily seen that the kernel of Bh is the
one-dimensional space spanned by the vector (1, 1, . . . , 1)T, so that the matrix of the
linear system (2.20) is not invertible. Therefore the numerical method does not work
when c = 0, which is perfectly consistent with what happens at the continuous level.
We will see in Proposition 2.7 that when there exists a positive constant c0 such that
c ≥c0, the matrix Bh is invertible, hence the scheme is well deﬁned.
Let us now study the convergence of the method when c ≥c0 > 0, i.e., its
consistency (Proposition 2.6 below) and its stability (Proposition 2.7).
Proposition 2.6 Let us suppose that the solution u of problem (2.19) is C4 on [0, 1].
Then the scheme (2.20) is consistent of order one.
Proof Let εh(u) = BhSh(u) −Fh be the truncation error, where Sh denotes here the
sampling operator on all the grid points, and is thus RN+2-valued. Using the ﬁrst
equation in (2.19) and (2.3), we get for any i ∈{1, . . . , N},
(εh(u))i = −u(xi+1) + 2u(xi) −u(xi−1)
h2
+ c(xi)u(xi) −f (xi) = −h2
12u(4)(ξi),
for some ξi between xi−1 and xi+1. For i = 0, using the left boundary condition in
(2.19) and (2.1), we obtain
(εh(u))0 = u(x0) −u(x1)
h
+ g0 = u(0) −u(h)
h
+ u′(0) = −h
2u′′(ξ0),
for some ξ0 between 0 and h. Similarly
(εh(u))N+1 = −u(xN) + u(xN+1)
h
−g1 = u(1) −u(1 −h)
h
−u′(1) = −h
2u′′(ξN+1),
for some ξN+1 between 1 −h and 1. Let us set
M = max
 1
12 max
y∈[0,1] |u(4)(y)|, 1
2 max
y∈[0,1] |u′′(y)|

.
Since h ≤1, we have h2 ≤h and therefore, for any i ∈{0, . . . , N + 1}, we see that
|(εh(u))i| ≤Mh. In other words,

2.4 Neumann Boundary Conditions
51
∥εh(u)∥∞≤Mh,
(2.21)
which shows that the scheme (2.20) is consistent of order one.
□
Remark 2.9 Note that the terms coming from the approximation of the boundary
conditions are dominant with respect to the second derivative approximation in the
truncation error.
□
We can show the following stability result.
Proposition 2.7 Let us suppose that there exists a constant c0 > 0 such that c ≥c0.
Then, the matrix Bh is inverse nonnegative, thus invertible and we have the following
estimate:
|||(Bh)−1|||∞≤C,
(2.22)
where C is a constant which does not depend on h.
Proof We decompose the matrix as follows
Bh = B0
h +
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
0
...
...
...
...
0
0 c(x1) −c0
0
...
...
0
...
...
c(x2) −c0
...
...
...
...
...
...
...
0
...
...
0 c(xN) −c0 0
0
...
...
...
...
0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
where
B0
h = 1
h2
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
h
−h
0
...
...
0
−1 2 + c0h2 −1 ...
...
0
...
... ...
...
...
...
...
... ...
...
0
...
... −1 2 + c0h2 −1
0
...
...
0
−h
h
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
We ﬁrst show that Bh is inverse nonnegative using the characterization of inverse
nonnegative matrices given in Lemma 2.2. Let X be a vector in RN+2 such that
BhX ≥0. We have to show that X ≥0 and this is equivalent to showing that Xi0 ≥0,
where i0 is the smallest index such that Xi0 ≤Xi, for all i ∈{0, . . . , N + 1}. Let us
ﬁrst consider the case i0 = 0. As (BhX)0 ≥0, we have X0 ≥X1. On the other hand,
by deﬁnition of i0, we also have X0 ≤X1, so that X0 = X1. Now, as (BhX)1 ≥0, it
follows that
−X0 + (2 + c(x1)h2)X1 −X2 = (X0 −X2) + c(x1)h2X0 ≥0.

52
2
The Finite Difference Method for Elliptic Problems
Now X0 −X2 ≤0 and c(x1) ≥c0 > 0, therefore X0 ≥0, which is the expected
result.
Let us now consider the case i0 ∈{1, . . . , N}. The condition (BhX)i0 ≥0 reads
−Xi0−1 +(2+c(xi0)h2)Xi0 −Xi0+1 = (Xi0 −Xi0−1)+(Xi0 −Xi0+1)+c(xi0)h2Xi0 ≥0.
WehaveXi0−Xi0−1 ≤0 andXi0−Xi0+1 ≤0,bydeﬁnitionofi0.Sincec(xi0) ≥c0 > 0,
this implies that Xi0 ≥0.
Finally, the case i0 = N + 1 cannot happen. Indeed, XN+1 ≥XN contradicts the
deﬁnition of i0. We have thus proven that Bh is inverse nonnegative. Since B0
h is a
particular case of Bh, B0
h is also inverse nonnegative.
Let us now prove estimate (2.22). We proceed as in Proposition 2.5. As before,
B−1
h −(B0
h)−1 = B−1
h [B0
h−Bh](B0
h)−1.Sincec ≥c0,thematrixB0
h−Bh isdiagonalwith
nonpositive entries and B−1
h
≥0 and (B0
h)−1 ≥0 as we have just seen. Consequently,
B−1
h
−(B0
h)−1 ≤0. It follows that |||B−1
h |||∞≤|||(B0
h)−1|||∞.
In order to obtain estimate (2.22), it is thus sufﬁcient to show that
|||(B0
h)−1|||∞= ∥(B0
h)−1E∥∞≤C,
where C is a constant which does not depend on h and E the vector in RN+2 with
all components equal to 1. We notice that (B0
h)−1E is precisely the discrete solution
Uh of system (2.20) in the particular case c = c0, f = 1 and −g0 = g1 = 1. In
other words, it is the discrete solution associated with the following boundary value
problem:

−¯u′′(x) + c0¯u(x) = 1,
x ∈]0, 1[,
¯u′(0) = −1,
¯u′(1) = 1.
A simple computation shows that ¯u has the following expression:
¯u(x) =
1
√c0
cosh
√c0

x −1
2

sinh
 √c0
2

+ 1
c0
.
This function is of class C∞, hence C4. By deﬁnition of the truncation error εh(¯u) =
B0
hSh(¯u) −E, we have
(B0
h)−1E = Sh(¯u) −(B0
h)−1εh(¯u).
Consequently,
|||(B0
h)−1|||∞= ∥(B0
h)−1E∥∞≤∥Sh(¯u)∥∞+ |||(B0
h)−1|||∞∥εh(¯u)∥∞.
Now ¯u is C4, hence by Proposition 2.6, ∥εh(¯u)∥∞≤Ch where C does not depend
on h. Therefore, for h sufﬁciently small, this quantity can be bounded from above by
1
2. It follows that

2.4 Neumann Boundary Conditions
53
|||(B0
h)−1|||∞≤2∥Sh(¯u)∥∞≤2
 1
√c0
coth
√c0
2

+ 1
c0

,
due to the expression of ¯u. The values of h that are not sufﬁciently small in the above
sense are only ﬁnite in number, thus this completes the proof.
□
Note that the above estimate gets worse and worse as c0 →0, which is con-
sistent with what happens when c0 = 0 and B0
h is no longer invertible. We deduce
from Proposition 2.6 (consistency) and Proposition 2.7 (stability) the following ﬁnal
convergence result:
Corollary 2.2 Assume that there exists a constant c0 > 0 such that c ≥c0 and that
the solution u of problem (2.19) is C4 on [0, 1]. Then the scheme (2.20) is convergent
of order one. More precisely, we have
∥Uh −Sh(u)∥∞≤Ch,
(2.23)
where C is a constant that does not depend on h.
Proof The proof is the same as for the Dirichlet case. We reproduce it for complete-
ness. By deﬁnition of the scheme and of the truncation error, we have
BhUh = Fh
and BhSh(u) = Fh + εh(u),
from which we deduce that
Bh(Uh −Sh(u)) = −εh(u).
Since Bh is invertible, this implies that the error is given by
Uh −Sh(u) = −(Bh)−1εh(u),
so that we obtain
∥Uh −Sh(u)∥∞≤|||(Bh)−1|||∞∥εh(u)∥∞.
Estimate (2.23) then follows from (2.21) and (2.22).
□
Remark 2.10 The ﬁnal error estimate is only of order one, due to the decentered
scheme chosen for the approximation of the Neumann boundary condition and in
spite of the order two approximation of the second derivative inside the domain.
It could be thought that the order one error remains somehow concentrated near
the boundary and that the approximation is better inside. This is not the case. The
lower order approximation of the boundary condition “pollutes” the discrete solution
everywhere, as we will see on a numerical example below.
□
We can in fact improve the accuracy by choosing a central scheme for the approxima-
tion of the Neumann boundary condition, instead of the decentered approximations

54
2
The Finite Difference Method for Elliptic Problems
used before. In order to do that, we ﬁrst add two ﬁctitious grid points x−1 = −h
and xN+2 = 1 + h, which are outside the interval [0, 1], and we use the following
approximations of the ﬁrst derivative at both ends:
u′(0) ≈u(h) −u(−h)
2h
,
u′(1) ≈u(1 + h) −u(1 −h)
2h
,
assuming u has been adequately extended outside of [0, 1], somehow.2 We also add
two ﬁctitious discrete unknowns, denoted by u−1 and uN+2. In order to have as
many unknowns as equations, we extend equation (2.19) up to the boundary of the
domain, i.e., we discretize it at each grid point xi, internal or not, which ﬁnally gives
the scheme
⎧
⎪⎨
⎪⎩
−ui+1 −2ui + ui−1
h2
+ c(xi)ui = f (xi),
i ∈{0, . . . , N + 1},
u−1 −u1
2h
= −g0,
uN+2 −uN
2h
= g1.
It can be shown that the truncation error of this new scheme is of order two.
In order to illustrate the compared performance of the previous schemes, we plot
the results of a few Scilab computations. We consider the case c(x) = 4, f (x) = 1
and −g0 = g1 = 1, for which we have an exact solution in closed form as seen before.
First we compute the results of the ﬁrst order scheme and plot them in Fig.2.4.
We plot the discrete solutions, again interpolated for easier visualization, for
N = 8, 12, 16, 20, and 24. Clearly, the convergence is fairly slow, and we can see
that the order 1 error is uniformly distributed over the whole domain, even though
it is only due to the approximation of the boundary conditions at the ends of the
interval, and the truncation error inside is of order 2.
Next, we plot the second order scheme for the same data (we do not plot the
ﬁctitious values).
In this case, the marks for the discrete values are virtually on the graph of the
exact solution, thus indicating a much faster convergence. It should be noted that at
ﬁrst glance, the matrices of each method do not look that different from each other.
The results are nonetheless dramatically different, see Fig.2.5.
Fig. 2.4 First order scheme
with × marks. Exact solution
in solid line with no marks
2We skip the details.

2.4 Neumann Boundary Conditions
55
Fig. 2.5 Second order
scheme with × marks. Exact
solution in solid line with no
marks
Remark 2.11 Let us make a ﬁnal remark concerning the resolution of the discrete
problem (2.20) in practice. This must be performed using a numerical method for
linear systems implemented in software. Of course, such numerical methods work in
ﬂoating point arithmetic, hence are subject to round-off errors. This thus raises the
important question of controlling the effect of such errors on the computed solution.
Let us consider a N × N linear system AX = F where A is invertible. We wish
to measure the error δX produced on the solution X by an error δF on the right-
hand side F, i.e., A(X + δx) = F + δF. Subtracting the two equations, we obtain
AδX = δF, from which we deduce that ∥δX∥≤|||A−1||| ∥δF∥(where ∥· ∥is a given
norm on RN and ||| · ||| the associated matrix norm). On the other hand, we also have
∥F∥≤|||A||| ∥X∥, from which we deduce that
1
∥X∥≤
|||A|||
∥F∥when F ̸= 0 and thus
X ̸= 0. Multiplying the two estimates together yields
∥δX∥
∥X∥≤|||A||| |||A−1||| ∥δF∥
∥F∥.
(2.24)
The number cond A = |||A||| |||A−1||| is called the condition number of A (with respect
to the norm ∥· ∥) [6, 18, 53]. It is always larger than 1 since 1 = |||I||| ≤|||A||| |||A−1|||
by submultiplicativity of an induced matrix norm. If it is roughly speaking small, we
say that the matrix is well-conditioned. On the contrary, if it is large compared to 1,
we say that the matrix is ill-conditioned. In the latter case, even a small relative error
on the right-hand side may induce a large relative error on the solution, which may
render the numerical computation meaningless. This is not the case if the matrix
is well-conditioned, on account of estimate (2.24). Of course, different numerical
methods have different abilities to handle ill-conditioned matrices. Some will fail on
some ill-conditioned matrices, whereas others will succeed on the same matrices.
The latter concept is not entirely well deﬁned.
Let us go back to the ﬁrst order scheme for the Neumann problem. We have written
it in a “natural” form that allows for consistency and stability to be established. There
are however inﬁnitely many other different matrix forms, with the same solutions
but different condition numbers.
In fact, we can for instance write this system in the following equivalent form:
B′
hUh = F′
h,
(2.25)

56
2
The Finite Difference Method for Elliptic Problems
with
B′
h = 1
h2
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
−1
0
...
...
0
−1
2 + h2c(x1)
−1
...
...
0
−1
2 + h2c(x2) −1
...
...
0
...
...
...
...
0
...
...
−1
2 + h2c(xN) −1
0
...
...
0
−1
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
and
F′
h =
⎛
⎜⎜⎜⎜⎜⎝
−g0
h
f (x1)
...
f (xN)
g1
h
⎞
⎟⎟⎟⎟⎟⎠
,
where both the ﬁrst and the last equations of system (2.20) have been divided by h.
Eventhoughthetwosystems(2.20)and (2.25)arestrictlyequivalentintermsoflinear
algebra, the new form (2.25) is not suitable for the computation of the truncation
error. It is however better suited for numerical resolution because the matrix B′
h has
a condition number that is smaller than that of matrix Bh.
We ﬁrst plot on Fig.2.6 the two condition numbers for the ∥·∥∞norm, as a function
of N in the case c(x) = 1. Next, Fig.2.7, we plot their ratio, still as a function of N. For
Fig. 2.6 Condition number of Bh top and B′
h bottom

2.4 Neumann Boundary Conditions
57
Fig. 2.7 Ratio cond Bh
cond B′
h
N large, both condition numbers are quite large, but there is a ratio of approximately 3
between the two, which is not spectacular, albeit appreciable. The general problem of
ﬁnding a linear system equivalent to a given linear system but with a better condition
number is called preconditioning.
□
2.5
The Two-Dimensional Case
The ﬁnite difference method also applies to higher-dimensional elliptic problems,
with some limitations. We describe here the approximation of a simple two-
dimensional problem, and the generalization to three-dimensional and higher is easy
to imagine.
Let us thus consider the homogeneous Dirichlet problem in Ω = ]0, 1[ × ]0, 1[

−Δu(x) = f (x) in Ω,
u(x) = 0 on Γ,
(2.26)
where f is a given continuous function on ¯Ω and Γ = ∂Ω. We will see in Chap.4
that this problem has a unique solution. In general, there is no closed form solution,
therefore our goal here is again to deﬁne a ﬁnite difference approximation for it. For

58
2
The Finite Difference Method for Elliptic Problems
Fig. 2.8 A 2d grid
0.2
0.4
0.6
0.8
1
0.2
0.4
0.6
0.8
1
0
xi1,i2
h1
h2
that purpose, we start by placing a uniform grid in ¯Ω as follows. Let us be given N1
and N2 two positive integers. We let h1 = 1/(N1 + 1) be the grid step in direction
x1 and h2 = 1/(N2 + 1) be the grid step in direction x2. The grid points will then be
xi1,i2 = (i1h1, i2h2) for i1 ∈{0, . . . , N1 + 1} and i2 ∈{0, . . . , N2 + 1}. The boundary
grid points correspond to i1 ∈{0, N1 + 1} or i2 ∈{0, N2 + 1}, and the internal grid
points to 1 ≤i1 ≤N1 and 1 ≤i2 ≤N2, see Fig.2.8.
Following the same idea as in the one-dimensional case, we want to compute an
approximation ui1,i2 of u(xi1,i2) for each i1 ∈{0, . . . , N1+1} and i2 ∈{0, . . . , N2+1}.
We naturally enforce the exact Dirichlet boundary condition on the boundary grid
points, i.e., we set ui1,i2 = 0 if i1 ∈{0, N1 + 1} or if i2 ∈{0, N2 + 1}.
We then need to approximate the Laplacian of u at internal grid points. Since
partial derivatives are nothing more than usual one-dimensional derivatives with
all the other variables frozen, we use the now familiar three point scheme for that
purpose. Namely, we approximate
∂2u
∂x2
1
(x1, ·) ≈u(x1 + h1, ·) −2u(x1, ·) + u(x1 −h1, ·)
h2
1
and
∂2u
∂x2
2
(·, x2) ≈u(·, x2 + h2) −2u(·, x2) + u(·, x2 −h2)
h2
2
.
Note that the variable x2 plays no role in the ﬁrst approximation and likewise for x1
in the second approximation. Taking into account that Δu = ∂2u
∂x2
1 + ∂2u
∂x2
2 , this leads to
the following scheme:

2.5 The Two-Dimensional Case
59
Fig. 2.9 The ﬁve point
stencil for the Laplacian
i2 −1
i2
i2 +1
i1
i1 −1
i1 +1
xi1,i2
xi1,i2+1
xi1,i2−1
xi1+1,i2
xi1−1,i2
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
−ui1+1,i2 −2ui1,i2 + ui1−1,i2
h2
1
−ui1,i2+1 −2ui1,i2 + ui1,i2−1
h2
2
= f (xi1,i2), for i1 ∈{1, . . . , N1}, i2 ∈{1, . . . , N2},
ui1,i2 = 0, for i1 ∈{0, N1 + 1} or i2 ∈{0, N2 + 1}.
This scheme is called the ﬁve point scheme for the Laplacian. It is a central scheme.
Indeed, in order to evaluate the discrete approximation of Δu at point xi1,i2, we use
the approximate values of u at the ﬁve grid points centered around xi1,i2, namely the
point xi1,i2 itself and its four neighboring points xi1,i2−1, xi1,i2+1, xi1−1,i2 and xi1+1,i2.
These ﬁve points constitute the stencil of the scheme, see Fig.2.9.
We need to reformulate the above scheme as a linear system. In order to do that,
we ﬁrst have to number the unknowns. For example, we can decide to number them
in the following way u1,1, · · · , uN1,1, u1,2, · · · , uN1,2, · · · , uN1,1, · · · , uN1,N2, i.e., line
by line. Other numberings are possible, for instance column by column.
With this choice of numbering, the scheme has the following equivalent vector
form
Ch1,h2Uh1,h2 = Fh1,h2,
(2.27)
where the N1N2×N1N2 matrix Ch1,h2 has a block structure. More precisely, the matrix
Ch1,h2 is composed of N2
2 blocks, each one of size N1 × N1 and tridiagonal,

60
2
The Finite Difference Method for Elliptic Problems
Ch1,h2 =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
A
−b2I
0
...
...
...
0
−b2I
A
−b2I
...
...
0
−b2I
A
−b2I
...
...
...
...
...
...
...
...
...
...
...
−b2I
A
−b2I
0
...
...
−b2I
A
−b2I
0
...
...
...
0
−b2I
A
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
where A is the N1 × N1 matrix deﬁned by
A =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
a
−b1
0
...
...
0
−b1
a
−b1
...
...
0
...
...
...
...
...
...
...
...
...
...
0
...
... −b1
a
−b1,
0
...
...
0
−b1
a
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
and I is the N1 × N1 identity matrix, where we have set
b1 = 1
h2
1
,
b2 = 1
h2
2
,
and
a = 2(b1 + b2)
for simplicity. Thus the matrix Ch1,h2 is block tridiagonal. There are no more than
ﬁve nonzero elements on each line (or each column) in the matrix Ch1,h2. Moreover,
the matrix Ch1,h2 is symmetric.
The unknown Uh1,h2 and the right-hand side Fh1,h2 have the same block structure
given by
Uh1,h2 =
⎛
⎜⎜⎜⎜⎜⎜⎝
U1
...
Ui2
...
UN2
⎞
⎟⎟⎟⎟⎟⎟⎠
, where Ui2 =
⎛
⎜⎜⎜⎜⎜⎜⎝
u1,i2
...
ui1,i2
...
uN1,i2
⎞
⎟⎟⎟⎟⎟⎟⎠
∈RN1,

2.5 The Two-Dimensional Case
61
and
Fh1,h2 =
⎛
⎜⎜⎜⎜⎜⎜⎝
F1
...
Fi2
...
FN2
⎞
⎟⎟⎟⎟⎟⎟⎠
, where Fi2 =
⎛
⎜⎜⎜⎜⎜⎜⎝
f (h1, i2h2)
...
f (i1h1, i2h2)
...
f (N1h1, i2h2)
⎞
⎟⎟⎟⎟⎟⎟⎠
∈RN1.
We ﬁrst note that the method is well-posed.
Proposition 2.8 The matrix Ch1,h2 is positive deﬁnite, hence invertible.
Proof Same as in Proposition 2.1.
□
Let us now study the convergence of the method. We start with the consistency. We
ﬁrst need to adapt the deﬁnition of the grid sampling operator to the present context.
Here,theoperatorhasvaluesinRN1N2 andissimplydeﬁnedbySh1,h2(v)i1,i2 = v(xi1,i2),
for any v ∈C0( ¯Ω).
Proposition 2.9 Let us suppose that the solution u of problem (2.26) is C4 on ¯Ω.
Then the scheme (2.27) is consistent of order two.
Proof Let us denote by εh1,h2(u) = Ch1,h2Sh1,h2(u)−Fh1,h2 the truncation error. Using
the same numbering as before, it has the block structure
εh1,h2(u) =
⎛
⎜⎜⎜⎜⎜⎜⎝
εh1,h2(u)1
...
εh1,h2(u)i2
...
εh1,h2(u)N2
⎞
⎟⎟⎟⎟⎟⎟⎠
, where εh1,h2(u)i2 =
⎛
⎜⎜⎜⎜⎜⎜⎝
εh1,h2(u)1,i2
...
εh1,h2(u)i1,i2
...
εh1,h2(u)N1,i2
⎞
⎟⎟⎟⎟⎟⎟⎠
∈RN1.
We assume that u is C4 on ¯Ω. We have
εh1,h2(u)i1,i2 = −u(xi1+1,i2) −2u(xi1,i2) + u(xi1−1,i2)
h2
−u(xi1,i2+1) −2u(xi1,i2) + u(xi1,i2−1)
h2
−f (xi1,i2).
We remark that in each differential quotient, one variable is ﬁxed (either the ﬁrst one
or the second one), so that we can still use the Taylor–Lagrange expansion (2.3).
Thanks to the ﬁrst equation in (2.26), we then obtain
(εh1,h2(u))i1,i2 = −1
12

h2
1
∂4u
∂x4
1
(ξi1,i2, i2h2) + h2
2
∂4u
∂x4
2
(i1h1, ξ ′
i1,i2)

,
for some ξi1,i2 ∈](i1 −1)h1, (i1 + 1)h1[ and ξ ′
i1,i2 ∈](i2 −1)h2, (i2 + 1)h2[. Setting
h = max(h1, h2), we deduce that

62
2
The Finite Difference Method for Elliptic Problems
∥εh1,h2(u)∥∞≤Mh2,
where
M = 1
12

max
y∈¯Ω
∂4u
∂x4
1
(y)
 + max
y∈¯Ω
∂4u
∂x4
2
(y)


.
(2.28)
This shows that the scheme (2.27) is consistent of order two.
□
Let us now study the stability of the method.
Proposition 2.10 We have the following estimate, for all h1, h2,
|||(Ch1,h2)−1|||∞≤1
8.
(2.29)
Proof Let Ωh1,h2 be the set of the indices of the grid points which are inside Ω, Γh1,h2
the set of the indices of the grid points which are on Γ and ¯Ωh1,h2 = Ωh1,h2 ∪Γh1,h2 =
{0, . . . , N1 +1}×{0, . . . , N2 +1} the set of all grid indices. We consider the operator
D: R ¯Ωh1,h2 →RΩh1,h2 deﬁned by
(DZ)i1,i2 = aZi1,i2 −b1(Zi1−1,i2 + Zi1+1,i2) −b2(Zi1,i2−1 + Zi1,i2+1).
The operator D is just the discrete ﬁve point Laplacian without boundary conditions.
We ﬁrst establish a discrete maximum principle for D. More precisely, we claim that
if DZ ≤0, then
max
¯Ωh1,h2
Zi1,i2 = max
Γh1,h2
Zi1,i2.
(2.30)
Indeed, if the maximum in question is attained on Γh1,h2, there is nothing to prove.
So let (m1, m2) ∈Ωh1,h2 be an index such that Zm1,m2 ≥Zi1,i2, for all (i1, i2) ∈¯Ωh1,h2.
Since (DZ)m1,m2 ≤0 and a = 2(b1 + b2), we have
aZm1,m2 ≤b1(Zm1+1,m2 + Zm1−1,m2) + b2(Zm1,m2+1 + Zm1,m2−1) ≤aZm1,m2.
Since b1 > 0 and b2 > 0, it follows that
Zm1,m2 = Zm1+1,m2 = Zm1−1,m2 = Zm1,m2+1 = Zm1,m2−1.
In other words, the maximum is also attained for the neighboring indices (m1+1, m2),
(m1 −1, m2), (m1, m2 + 1) and (m1, m2 −1). By an immediate induction, we have
Zm1,m2 = Zm1−1,m2 = · · · = Z0,m2. This means that the maximum is in fact attained
on Γh1,h2 as well and we are done with the claim.
Let us now establish estimate (2.29). Let X be an arbitrary vector in RΩh1,h2 . We
deﬁne a vector X ∈R ¯Ωh1,h2 by Xi1,i2 = Xi1,i2 for (i1, i2) ∈Ωh1,h2 and Xi1,i2 = 0 for
(i1, i2) ∈Γh1,h2. With this deﬁnition, we see that (DX)i1,i2 = (Ch1,h2X)i1,i2, for all
(i1, i2) ∈Ωh1,h2.

2.5 The Two-Dimensional Case
63
Let now Y ∈R ¯Ωh1,h2 be deﬁned by
Yi1,i2 = 1
4

i1 −N1 + 1
2
2
h2
1 +

i2 −N2 + 1
2
2
h2
2

.
By direct inspection, we see that (DY)i1,i2 = −1 for all (i1, i2) ∈Ωh1,h2.
We next choose s = ±1 in such a way that
max
(i1,i2)∈Ωh1,h2
|Xi1,i2| = |Xn1,n2| = sXn1,n2,
for some (n1, n2) ∈Ωh1,h2. This way, we have
max
(i1,i2)∈Ωh1,h2
|Xi1,i2| ≤
max
(i1,i2)∈Ωh1,h2
(sXi1,i2).
Finally, let Z = sX + ∥Ch1,h2X∥∞Y. We note that
∥Ch1,h2X∥∞=
max
(i1,i2)∈Ωh1,h2
|(DX)i1,i2|.
Therefore, we have
(DZ)i1,i2 = s(DX)i1,i2 −∥Ch1,h2X∥∞≤0,
for all (i1, i2) ∈Ωh1,h2. Applying estimate (2.30) to Z, we obtain
max
(i1,i2)∈Ωh1,h2
(sXi1,i2) ≤
max
(i1,i2)∈Ωh1,h2
Zi1,i2
≤
max
(i1,i2)∈Γh1,h2
Zi1,i2
≤
max
(i1,i2)∈Γh1,h2
(sXi1,i2) + ∥Ch1,h2X∥∞
8
,
since 0 ≤Yi1,i2 ≤1
8, for all (i1, i2) ∈¯Ωh1,h2. Now Xi1,i2 = 0 on Γh1,h2, therefore
∥X∥∞=
max
(i1,i2)∈Ωh1,h2
|Xi1,i2| ≤∥Ch1,h2X∥∞
8
,
which completes the proof.
□
Remark 2.12 The proof above shows, with only minor modiﬁcations, that the matrix
Ch1,h2 is inverse nonnegative.
The proof also shows that any Z such that DZ ≤0 and that attains its maximum
in Ωh1,h2 is in fact constant.
As opposed to the one-dimensional case, we do not have a closed form formula
for the solution of −Δ¯u = 1 in Ω, ¯u = 0 on Γ , at our disposal (¯u can however be

64
2
The Finite Difference Method for Elliptic Problems
expressed with Fourier series or approximated using ﬁnite differences, see below),
hencenoclosedformexpressionforitsmaximumvalue.Thisexplainstheroundabout
way of introducing Y to play the same role, without boundary conditions.
As a consequence, the resulting stability estimate is not optimal, contrarily to the
one-dimensional case. Numerical evidence indicates that |||(Ch1,h2)−1|||∞≈0.07356
pretty much independently of h1, h2 in the cases we computed.
□
We deduce from Proposition 2.9 (consistency) and Proposition 2.10 (stability) the
ﬁnal convergence result.
Corollary 2.3 Let us suppose that the solution u of problem (2.26) is C4 on ¯Ω. Then
the scheme (2.27) is convergent of order two. More precisely, we have
∥Uh1,h2 −Sh1,h2(u)∥∞≤M
8 h2,
(2.31)
where M is given by (2.28) and h = max(h1, h2).
Proof Exactly the same as in Corollary 2.2.
□
Remark 2.13 On the surface, it looks like the one-dimensional error estimate (2.13)
and the two-dimensional error estimate (2.31) are basically the same. This is not
actually so. Indeed, an important consideration in numerical methods is that of their
cost. In effect, it is only possible to meaningfully compare two methods if they apply
to data of the same size.
For example, let us assume that Gaussian elimination, see [6, 53, 71], is used to
solve linear systems (we ignore the fact that more efﬁcient methods may exist that are
better adapted to these particular matrices). In the one dimensional case, this would
lead to a compute time T1d = O(N3) = O(h−3), whereas in the two-dimensional
case, we would be looking at a compute time T2d = O(N6) = O(h−6) since the
system to be solved is a N2 × N2 system.3 Thus the time required to achieve a given
error estimate in two dimensions is roughly the square of the time needed to achieve
the same error estimate in one dimension.
This is a general fact: Computations become exponentially costlier and costlier as
the dimension of the problem grows. This is called the curse of dimen-
sionality.
□
Let us illustrate the previous considerations in Fig.2.10 with a plot of the ﬁnite
difference solution of the Poisson problem −Δ¯u = 1 in Ω, ¯u = 0 on Γ , in the unit
square. This plot can be visualized in 3D by squinting toward the middle of the page.
We have described the ﬁnite difference method for the Poisson equation in the
unit square. It immediately generalizes to the case of a rectangle. The case of a more
general two-dimensional domain Ω is more complicated.
3In practice, the cases considered here are all very small, and are solved almost instantaneously on
a personal computer, but the remark applies to more computationally challenging problems.

2.5 The Two-Dimensional Case
65
Fig. 2.10 Cross-eyed autostereogram for 3d visualization of the ﬁnite difference solution for N = 21

66
2
The Finite Difference Method for Elliptic Problems
Fig. 2.11 The point qE is
outside Ω and is replaced by
q′
E as part of the grid
Let us consider the following non homogeneous Dirichlet problem: Find
u: ¯Ω →R solution of

−Δu(x) = f (x),
x ∈Ω,
u(x) = g(x),
x ∈Γ,
where g is the given Dirichlet data on the boundary Γ of Ω.
For simplicity, we assume that h1 = h2 = h and we consider the lattice hZ2 in
the plane. The straight lines parallel to the axes and going through lattice points are
called grid lines. In order to construct a ﬁnite difference grid for the problem, we
ﬁrst retain the lattice points that belong to Ω. The difﬁculty is that in order to impose
the Dirichlet boundary condition, we need points on the boundary, whereas lattice
points have no reason to fall on the boundary. To remedy this situation, the idea is
to replace the lattice points that are closest to Ω outside of Ω by the intersection of
grid lines with Γ . The ﬁnite difference scheme must be modiﬁed accordingly in the
vicinity of such points.
To ﬁx ideas, let us consider one possible conﬁguration in Fig.2.11. The grid point
q is such that its lattice neighbors qS, qW and qN are inside Ω. We keep them in the
grid. On the contrary, the point qE is outside Ω and closest to Γ . We replace it by
point q′
E of intersection of the grid line with Γ . Its distance to point q is h′ < h, as
can be seen on the ﬁgure.
In the case of Fig.2.11, we now look for an approximation of Δu(q) that uses
points q, qS, qW, qN and q′
E. We just have to modify the approximation of the second
order derivative with respect to variable x1 (for the other variable, we can use the
usual three point scheme using q, qS and qN which are inside Ω). Let us explain how
this works. The trick consists in ﬁnding three coefﬁcients α, β and γ such that

2.5 The Two-Dimensional Case
67
αu(qW) + βu(q) + γ u(q′
E) = ∂2u
∂x2
1
(q) + O(h).
Using as usual Taylor–Lagrange expansions of u(qW) and of u(q′
E) in a neighborhood
of q, we get the following system:
α + β + γ = 0,
−αh + γ h′ = 0,
α h2
2 + γ h′2
2 = 1,
which admits a unique solution given by
α =
2
h(h + h′),
β = −2
hh′ ,
γ =
2
h′(h + h′).
We can check that the truncation error is only of order one due to the fact that h′ ̸= h,
hence a lesser quality of approximation compared with the rectangular case. Let us
remark that the matrix of the linear system we have to solve is no longer symmetric.
2.6
Concluding Remarks
To conclude, we see that the ﬁnite difference method is not easily implemented in
arbitrary domains in dimensions higher than 1 or for different boundary conditions.
This is a serious limitation of the method for applications. It works well on domains
with simple geometry, for example domains which are unions of rectangles, with
Dirichlet boundary conditions. However, in the general case, i.e., for arbitrary do-
mains and other boundary conditions, the ﬁnite element method that we will see in
two dimensions in Chap.6 will be generally preferred.
We will return to the ﬁnite difference method in Chap.8 for the heat equation in
one dimension of space and Chap.9 for the wave equation also in one dimension of
space.
The ﬁnite difference method for one-dimensional elliptic problems is mentioned
in many references. The reader is referred to [18, 46, 47, 64, 70, 75] for example, for
various extensions and points of view.

Chapter 3
A Review of Analysis
In order to go beyond the somewhat naive existence theory and ﬁnite difference
method of approximation of elliptic boundary value problems seen in Chaps.1 and
2, we need to develop a more sophisticated point of view. This requires in turn some
elements of analysis pertaining to function spaces in several variables, starting with
some abstract Hilbert space theory. This is the main object of this chapter.
As already mentioned in the preface, this chapter can be read quickly at ﬁrst,
for readers who are not too interested in the mathematical details and constructions
therein. A summary of the important results needed for the subsequent chapters is
thus provided at the end of the chapter.
3.1
Basic Hilbert Space Theory
Let us quickly review basic Hilbert space theory from the abstract viewpoint. Let H
be a real Hilbert space, i.e., a real vector space endowed with a scalar product (·|·)H
and associated norm ∥· ∥H which is complete for this norm. The Cauchy–Schwarz
inequality is really a hilbertian property.
Theorem 3.1 For all u, v ∈H, we have
|(u|v)H| ≤∥u∥H∥v∥H.
One of the most basic results in Hilbert space theory is the orthogonal projection
theorem, see [9, 14, 32, 51] for a proof.
Theorem 3.2 Let C be a non empty, convex, closed subset of H. For all x ∈H,
there exists a unique pC(x) ∈C such that
∥x −pC(x)∥H = inf
y∈C ∥x −y∥H.
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_3
69

70
3
A Review of Analysis
Fig. 3.1 The orthogonal
projection on a closed
convex subset C
x
y
pC(x)
C
The vector pC(x) is called the orthogonal projection of x onC. It is also characterized
by the inequality
∀y ∈C,
(x −pC(x)|y −pC(x))H ≤0.
In addition, if the convex set is a closed vector subspace E of H, then pE is a
continuous linear mapping from H to E which is also characterized by the equality
∀y ∈E,
(x −pE(x)|y)H = 0.
The orthogonal projection of x on C is thus the element of C closest to x and the
angle between x −pC(x) and y −pC(x) is larger than π
2 , see Fig.3.1. In particular, if
x ∈C, then pC(x) = x. An important consequence of the last characterization in the
case of a closed vector subspace E is that we can write H = E ⊕E⊥with continuous
orthogonal projections on each factor. Indeed, we have x = pE(x) + (x −pE(x))
with pE(x) ∈E by construction and x−pE(x) ∈E⊥by the second characterization.
Hence H = E + E⊥. To show that the sum is direct, it sufﬁces to note that E ∩E⊥=
{0} which is obvious since x ∈E ∩E⊥implies 0 = (x|x)H = ∥x∥2
H, see Fig.3.2.
Another important consequence is a characterization of dense subspaces.
Lemma 3.1 A vector subspace E of H is dense in H if and only if E⊥= {0}.
Proof For any vector subspace, it is always true that E⊥= ( ¯E)⊥. Let E be a dense
subspace, i.e., ¯E = H. Then, of course E⊥= H ⊥= {0}. Conversely, if E⊥= {0},
this implies that ( ¯E)⊥= {0} and since H = ( ¯E)⊥⊕¯E, it follows that ¯E = H, and
E is dense in H.
□
The Riesz theorem provides a canonical way of identifying a Hilbert space and
its dual.
Theorem 3.3 (Riesz) Let H be a Hilbert space and ℓan element of its dual H ′.
There exists a unique u ∈H such that

3.1 Basic Hilbert Space Theory
71
Fig. 3.2 The orthogonal
projection on a closed vector
subspace E
E
E⊥
x
pE(x)
pE⊥(x)
0
∀v ∈H,
ℓ(v) = (u|v)H.
Moreover
∥ℓ∥H ′ = ∥u∥H
and the linear mapping σ : H ′ →H, ℓ→u, is an isometry.
Proof For the uniqueness, assume u1 and u2 are two solutions, then for all v ∈H,
(v|u1 −u2)H = 0. This is true in particular for v = u1 −u2, hence u1 = u2.
If ℓ= 0, then we set u = 0 to be the unique u in question. Let ℓ̸= 0. It is
thus a nonzero continuous linear form, hence its kernel ker ℓis a closed hyperplane
of H. Let us choose u0 ∈(ker ℓ)⊥with ∥u0∥H = 1 (this is possible since ker ℓis
not dense). Since u0 /∈ker ℓ, we have ℓ(u0) ̸= 0 and for all v ∈H, we can set
w = v −ℓ(v)
ℓ(u0)u0 so that
ℓ(w) = ℓ

v −ℓ(v)
ℓ(u0)u0

= ℓ(v) −ℓ(v)
ℓ(u0)ℓ(u0) = 0,
and w ∈ker ℓ. Now writing v =
ℓ(v)
ℓ(u0)u0 + w and setting u = ℓ(u0)u0 ∈(ker ℓ)⊥,
we obtain
(v|u)H =
 ℓ(v)
ℓ(u0)u0
u

H + (w|u)H = ℓ(v)(u0|u0)H = ℓ(v),
hence the existence of u.

72
3
A Review of Analysis
The mapping σ is thus well deﬁned and obviously linear. Finally, for the isometry,
we have on the one hand
∥ℓ∥H ′ = sup
∥v∥H ≤1
|ℓ(v)| = sup
∥v∥H ≤1
|(v|u)H| ≤∥v∥H∥u∥H ≤∥u∥H,
by the Cauchy–Schwarz inequality. On the other hand, equality trivially holds for
ℓ= 0, and for ℓ̸= 0, choosing v =
u
∥u∥H yields (v|u)H = ∥u∥H with ∥v∥H = 1,
hence the equality in this case too.
□
Remark 3.1 The Riesz theorem shows that the dual of a Hilbert space is also a Hilbert
space for the scalar product (ℓ1|ℓ2)H ′ = (σℓ1|σℓ2)H which induces the dual norm.
Indeed it is not a priori obvious that the dual norm is hilbertian. It is often used to
identify H and H ′ via the isometry σ or σ −1. This identiﬁcation is not systematic
however. For example, when we have two Hilbert spaces H and V such that V →H
and V is dense in H, the usual identiﬁcation is to let
V →H = H ′ →V ′
using the Riesz theorem for H, which is called the pivot space, but not for V , see
[15, 28].
□
We now turn to the study of more concrete function spaces.
3.2
A Few Basic Function Spaces
Let us rapidly review the most basic function spaces that we will need. All these
function spaces are real valued. In the sequel, Ω denotes an open subset of Rd,
d ≥1. The canonical scalar product of two vectors x and y in Rd will be denoted
x · y = d
i=1 xi yi, where xi (resp. yi) are the components of x (resp. y). The
associated Euclidean norm is ∥x∥= (x · x)1/2. We use the multiindex notation for
partial derivatives. Let α = (α1, α2, . . . , αd) ∈Nd be a multiindex. The integer
|α| = d
i=1 αi is called the length of α and we set
∂αu =
∂|α|u
∂xα1
1 ∂xα2
2 · · · ∂xαd
d
,
whenever the function u is |α|-times differentiable and the partial derivatives com-
mute. The space C0(Ω) is the space of real-valued, continuous functions on Ω, and
for all k ∈N, we deﬁne
Ck(Ω) = {u;
for all α ∈Nd, |α| ≤k, ∂αu ∈C0(Ω)}

3.2 A Few Basic Function Spaces
73
to be the space of k-times continuously differentiable functions on Ω. The space of
indeﬁnitely differentiable functions on Ω is deﬁned by
C∞(Ω) =

k∈N
Ck(Ω).
We do not specify the natural topologies of these vector spaces as we will not need
them. Beware however that these natural topologies are not normed.
The support of a function u, supp u, is the complement of the largest open subset
of Ω on which u vanishes. It is thus a closed subset of Ω. A subset K of Ω is compact
if and only if it is a closed, bounded subset that “does not touch the boundary” in the
sense that d(K, ∁RdΩ) > 0. There is a “security strip” between K and ∂Ω. Functions
with compact support play an important role and deserve a notation of their own:
Ck
c (Ω) = Dk(Ω) = {u ∈Ck(Ω); supp u is compact}
and
C∞
c (Ω) = D(Ω) =

k∈N
Dk(Ω).
Again, these vector spaces are endowed with natural topologies that we will not
describe. We will return to these spaces later when talking about distributions.
Let ¯Ω be the closure of Ω in Rd. The space C0( ¯Ω) is the space of continuous
functions on ¯Ω. If ¯Ω is compact, that is to say, if Ω is bounded, this space is normed
by
∥u∥C0( ¯Ω) = sup
x∈¯Ω
|u(x)| = max
x∈¯Ω |u(x)|.
The convergence associated to this normed topology is just uniform convergence.
Likewise, we deﬁne Ck( ¯Ω) to be the space of functions in Ck(Ω), all the par-
tial derivatives of which up to order k have a continuous extension to ¯Ω. Keep-
ing the same symbol for this extension, the natural norm of this space when Ω is
bounded is
∥u∥Ck( ¯Ω) = max
|α|≤k ∥∂αu∥C0( ¯Ω),
and the convergence of a sequence in this space is the uniform convergence of all
partial derivatives up to order k. All these spaces are Banach spaces, i.e., they are
complete for the metric deﬁned by their norm. We also deﬁne
C∞( ¯Ω) =

k∈N
Ck( ¯Ω),
a space which is endowed with a natural topology that is again not a normed space,
even when ¯Ω is compact.

74
3
A Review of Analysis
For 0 < β ≤1 and Ω bounded, we deﬁne the spaces of Hölder functions
[35, 40] (Lipschitz for β = 1) by
C0,β( ¯Ω) =

u ∈C0( ¯Ω); sup
x,y∈¯Ω
x̸=y
|u(x) −u(y)|
∥x −y∥β
< +∞

and
Ck,β( ¯Ω) = {u ∈Ck( ¯Ω); ∂αu ∈C0,β( ¯Ω) for all α ∈Nd, |α| = k}.
When equipped with the norms
∥u∥Ck,β( ¯Ω) = ∥u∥Ck( ¯Ω) + max
|α|=k

sup
x,y∈¯Ω
x̸=y
|∂αu(x) −∂αu(y)|
∥x −y∥β

,
these spaces also are Banach spaces. There are continuous injections
Ck,β( ¯Ω) →Ck,β′( ¯Ω) →Ck( ¯Ω) →Ck−1,γ ( ¯Ω)
which are compact for β′ < β and γ < 1 by Ascoli’s theorem (the compactness of
the ﬁrst embedding requires some regularity on Ω, see Sect.3.3). A linear mapping
f from a normed space E to a normed space F is continuous if and only if there
exists a constant C such that for all x ∈E, ∥f (x)∥F ≤C∥x∥E. In the continuous
injections above, f is just the identity, i.e., f (u) = u. A mapping is compact if it
transforms bounded sets into relatively compact sets.
The other major family of function spaces that will be useful to us is that of the
Lebesgue spaces (see for example [2, 15, 20, 51]),
L p(Ω) =

u measurable;
	
Ω
|u(x)|p dx < +∞

for 1 ≤p < +∞and
L∞(Ω) =

u measurable; ess sup
Ω
|u| < +∞

,
where
ess sup
Ω
|u| = inf{M; |u| ≤M almost everywhere on Ω}.
Now in these deﬁnitions, u is not strictly speaking a function, but an equivalence class
of functions that are equal almost everywhere with respect to the Lebesgue measure.
However, in practice and outside of very speciﬁc circumstances, it is harmless to

3.2 A Few Basic Function Spaces
75
think of u as just a function and not as an equivalence class. We just need to keep
this fact at the back of our mind, just in case.
When equipped with the norms
∥u∥L p(Ω) =
	
Ω
|u(x)|p dx
 1
p
for 1 ≤p < +∞and
∥u∥L∞(Ω) = ess sup
Ω
|u|,
the Lebesgue spaces are Banach spaces. For p = 2, the space L2(Ω) is a Hilbert
space for the scalar product
(u|v)L2(Ω) =
	
Ω
u(x)v(x) dx,
see Sect.3.1 for general Hilbert space theory. Hölder’s inequality reads
	
Ω
|u(x)v(x)| dx ≤
	
Ω
|u(x)|p dx
 1
p 	
Ω
|v(x)|p′ dx
 1
p′
when 1 < p, p′ < +∞are conjugate exponents, 1
p + 1
p′ = 1 and
	
Ω
|u(x)v(x)| dx ≤(ess sup
Ω
|u|)
	
Ω
|v(x)| dx,
(the integrals do not need to be ﬁnite). In particular, if u ∈L p(Ω) and v ∈L p′(Ω),
then uv ∈L1(Ω) and

	
Ω
u(x)v(x) dx
 ≤∥uv∥L1(Ω) ≤∥u∥L p(Ω)∥v∥L p′(Ω).
For p = 2, we get the Cauchy–Schwarz inequality, which is actually a Hilbert space
property as we have seen before,
|(u|v)L2(Ω)| ≤∥u∥L2(Ω)∥v∥L2(Ω).
When Ω is bounded, there are continuous injections Ck( ¯Ω) →L p(Ω) →
Lq(Ω) whenever q ≤p.1
1This is doubly false if Ω is not bounded.

76
3
A Review of Analysis
The Lebesgue spaces admit local versions
L p
loc(Ω) = {u; u|K ∈L p(K) for all compact K ⊂Ω}.
These vector spaces have a natural topology which is not a normed topology.
Clearly, in view of Hölder’s inequality, we have L p
loc(Ω) ⊂Lq
loc(Ω) whenever
q ≤p. In particular, the space L1
loc(Ω) is the largest of all these spaces, and actually
the largest of all function spaces introduced up to now, which are all continuously
embedded in it.
The following result is of importance [2, 20, 51].
Proposition 3.1 Let u ∈L1
loc(Ω) be such that

Ω uϕ dx = 0 for all ϕ ∈D(Ω).
Then u = 0 almost everywhere.
Proof Note ﬁrst that since ϕ has support in a compact subset K of Ω, so does the
product uϕ. Since ϕ is bounded, it follows that uϕ ∈L1(K) and the integral is
well-deﬁned.
Let x0 ∈Ω and n be large enough so that B

x0, 1
n

⊂Ω. It is possible to construct
a sequence ϕk ∈D(Ω) such that supp ϕk ⊂B

x0, 1
n

and for all x ∈B

x0, 1
n

,
ϕk(x) →1 (we leave the proof as an exercise). Consequently, by the Lebesgue
dominated convergence theorem, we have
0 =
	
B(x0, 1
n)
uϕk dx −→
k→+∞
	
B(x0, 1
n)
u dx.
Hence, since
0 =
1
meas B

x0, 1
n

	
B(x0, 1
n)
u dx −→
n→+∞u(x0)
for almost all x0 by the Lebesgue points theorem, see [68], we obtain the result. □
Remark 3.2 Here we see at work the idea of testing a function u with a test-function
ϕ in order to obtain information on u. The general concept behind it is that of duality
and it will be used in much larger generality in the context of distributions and
variational formulations that we will see later on.
□
3.3
Regularity of Open Subsets of Rd
The structure of the open subsets of Rd for the usual topology is very simple for
d = 1, since every open set is a union of an at most countable family of disjoint
open intervals. In particular, a one-dimensional connected open set is just an open
interval. The situation is more complicated in higher dimensions.
People tend to think of a connected open set of Rd as a potato-shaped object drawn
in R2. This geometrical intuition is basically correct as far as the open set itself is
concerned. It is misleading when the boundary of the open set is involved. In fact,

3.3 Regularity of Open Subsets of Rd
77
Fig. 3.3 An open set in R2 with a relatively wild boundary (imagine an inﬁnity of little spikes
pointing inward the disk)
Fig. 3.4 A zoom on the
complement of the
Mandelbrot set
the boundary of an open set in Rd, d > 1, can be more or less regular, more or less
smooth, as in Fig.3.3.
There is worse: the Mandelbrot set is compact, its complement is open with a very
convoluted boundary, see Fig.3.4.
It is even possible to construct open sets in R2 (or in Rd for any d for that matter),
the boundary of which has strictly positive Lebesgue measure, i.e., a strictly positive
area! PDE problems are posed in open subsets of Rd and we often need a certain
amount of regularity of the boundary of such open sets in order to deal with boundary
conditions.
There are several ways of quantifying the regularity of an open set boundary, or in
short the regularity of that open set. Let us give the deﬁnition that is the most adequate

78
3
A Review of Analysis
for our purposes here. Other deﬁnitions—equivalent or not—may be encountered in
the literature [41, 44].
Deﬁnition 3.1 We say that a bounded open subset of Rd is Lipschitz (resp. of class
Ck,β) if its boundary ∂Ω can be covered by a ﬁnite number of open hypercubes C j,
j = 1, . . . , m, each with an attached system of orthonormal Cartesian coordinates,
y j = (y j
1, y j
2, . . . , y j
d), in such a way that
C j = {y ∈Rd; |y j
i | < a j for i = 1, . . . , d},
and there exists Lipschitz functions (resp. of class Ck,β) ϕ j : Rd−1 →R such that
Ω ∩C j = {y ∈C j; y j
d < ϕ j((y j)′)},
using the notation (y j)′ = (y j
1, y j
2, . . . , y j
d−1) ∈Rd−1.
The meaning of Deﬁnition 3.1 is that locally in C j, Ω consists of those points
located strictly below the graph of ϕ j, in other words, the hypograph of ϕ j, see
Figs.3.5 and 3.7. In particular, such an open set is situated on just one side of its
boundary, which consists of pieces of graphs, since
∂Ω ∩C j = {y ∈C j; y j
d = ϕ j((y j)′)}.
Remark 3.3 It is fairly clear that a bounded polygon is a Lipschitz open set in dimen-
sion 2. None of the wild examples of Figs.3.3 and 3.4 is of class C0,β.
Fig. 3.5 Covering the
boundary with hypercubes

3.3 Regularity of Open Subsets of Rd
79
Fig. 3.6 Simple, however
not Lipschitz
On the other hand, there are also perfectly nice open sets that are not Lipschitz
in the previous sense. We give an example in Fig.3.6, obtained by gluing together
two parallelepipeds one on top of the other, adding the open square of contact. It is
impossible to describe the resulting set as a hypograph at each vertex of that square.
The open set is nonetheless perfectly tame, it is a polyhedron.
□
The boundary of a Lipschitz open set, and a fortiori that of an open set of class
Ck,α, k ≥1, possesses a certain number of useful properties.
Proposition 3.2 Let Ω be a Lipschitz open set. There exists a normal unit exterior
vector n, deﬁned almost everywhere on ∂Ω.
Normal means orthogonal to the boundary, exterior means that it points toward
the complement of Ω. We will go back to the meaning of almost everywhere later.
Proof Let us work in C j and drop all j indices and exponents to simplify notation.
We will admit Rademacher’s theorem, a nontrivial result that says that a Lipschitz
function on Rd−1 is differentiable in the classical sense, almost everywhere with
respect to the Lebesgue measure in Rd−1, see [37].
Let y′ be a point of differentiability of ϕ. At this point, the differentiability implies
that the graph of ϕ has a tangent hyperplane generated by the d −1 vectors

80
3
A Review of Analysis
a1 =
⎛
⎜⎜⎜⎜⎜⎝
1
0
...
0
∂1ϕ(y′)
⎞
⎟⎟⎟⎟⎟⎠
, a2 =
⎛
⎜⎜⎜⎜⎜⎝
0
1
...
0
∂2ϕ(y′)
⎞
⎟⎟⎟⎟⎟⎠
, · · · , ad−1 =
⎛
⎜⎜⎜⎜⎜⎝
0
0
...
1
∂d−1ϕ(y′)
⎞
⎟⎟⎟⎟⎟⎠
,
(for brevity we use here a slightly different notation for partial derivatives, ∂iϕ = ∂ϕ
∂yi ).
The orthogonal straight line is generated by the vector
N =
⎛
⎜⎜⎜⎜⎜⎝
−∂1ϕ(y′)
−∂2ϕ(y′)
...
−∂d−1ϕ(y′)
1
⎞
⎟⎟⎟⎟⎟⎠
.
which is clearly orthogonal to all ai. To conclude, we just need to normalize it and
notice that it points outwards due to the strictly positive last component and Ω lying
under the graph,
n =
1

1 + ∥∇ϕ(y′)∥2
⎛
⎜⎜⎜⎜⎜⎝
−∂1ϕ(y′)
−∂2ϕ(y′)
...
−∂d−1ϕ(y′)
1
⎞
⎟⎟⎟⎟⎟⎠
,
with ∥∇ϕ(y′)∥2 = d−1
i=1 (∂iϕ(y′))2.
□
Remark 3.4 It should be noted that the normal vector n is an object of purely geo-
metric nature that does not depend on the particular system of coordinates used to
compute it. In particular, if we take another admissible covering of the boundary, the
same formulas apply and compute the same vector in different coordinate systems.
This geometrically obvious remark can also be checked by direct computation in two
different coordinate systems, see Fig.3.7.
The “almost everywhere” is meant in the sense of the space Rd−1 associated with
a local coordinate system. We give it an intrinsic meaning just below.
□
If Ω is a Lipschitz subset of Rd, there is a natural measure on ∂Ω that is inherited
in a sense from the Lebesgue measure in Rd, that we will call the boundary measure.
We will not go into all the detail but give a few ideas on how this measure can be
computed.
Let A ⊂∂Ω be a Borel subset of ∂Ω. Since the open sets C j cover the boundary,
we can partition A with Borel sets A j ⊂C j. Let Π j be the orthogonal projection from
C j onto Rd−1 according to the coordinate system associated with C j. The restriction
of the projection to the graph of ϕ j is a homeomorphism, therefore Π j(A j) is a Borel
subset of Rd−1.

3.3 Regularity of Open Subsets of Rd
81
Fig. 3.7 Local aspect of the
boundary of a Lipschitz open
set and the normal unit
exterior vector
We set
Hd−1(A j) =
	
Π j(A j)

1 + ∥∇ϕ j((y j)′)∥2 d(y j)′
and Hd−1(A) =
m

j=1
Hd−1(A j).
It can be checked, although it is quite tedious, that this formula does not depend
on the covering and coordinates chosen to compute it, and that it deﬁnes a Borel
measure on ∂Ω.
In the case when ∇ϕ j is constant, that is to say if the graph is portion of a hyper-
plane, it is also easy to check that the formula above gives the (d −1)-dimensional
Lebesgue measure on the hyperplane, using the same unit of length as in Rd. In this
sense, the boundary measure is inherited from Rd.
The notation Hd−1 alludes to the (d−1)-Hausdorff measure, a much more general
and complicated object that coincides here with our hand-crafted measure. Let us
develop an example.
In the example of Fig.3.8, Ω is the unit square. We have ﬁgured just one square
of the boundary covering, with the attached coordinate system. The part A of ∂Ω
included in it is drawn with a thicker line. It is clearly described by the function
ϕ(y1) = −|y1|, with y1 ∈]−
3
2
√
2,
3
2
√
2[. We have ϕ′(y1) = 1 for y1 < 0 and
ϕ′(y1) = −1 for y1 > 0. Thus
H1(A) =
	
3
2
√
2
−
3
2
√
2
√
1 + 1 dy1 = 3,
which is equal to the length of the thicker line.

82
3
A Review of Analysis
Fig. 3.8 Boundary measure
example
−1.5
−1
−0.5
0.5
1
1.5
2
2.5
3
−1
−0.5
0.5
1
1.5
2
2.5
0
y1
y2
More generally, for d = 2, the boundary of Ω consists of curves and if these curves
are regular, we recognize the length of the parametric curve y1 →(y1, ϕ(y1)). The
same interpretation holds for d = 3 with the area of a parametric surface.
It is now clear that the normal vector is deﬁned almost everywhere with respect to
the boundary measure. In addition, we can now deﬁne L p(∂Ω) spaces and compute
all sorts of integrals on the boundary, using this measure. In order to have a more
economical notation, we will write it dΓ in the integrals. Thus, if g is a function on
the boundary with support in C j, we have
	
∂Ω
g dΓ =
	
Π j(C j)
g(Π−1
j (y′))

1 + ∥∇ϕ j((y j)′)∥2 d(y j)′.
The formula is extended to all functions without condition of support by a partition
of unity, see below.
3.4
Partitions of Unity
Partitions of unity [2] are a basic tool that is used in many contexts whenever the
need arises to localize a function. In what follows, Ω will be a bounded open subset
of Rd with a ﬁnite covering C j, j = 0, . . . , m, of its boundary ∂Ω.2
Let us ﬁrst state a few facts about convolution [15, 51]. Given two functions f
and g in L1(Rd), we deﬁne
2This particular assumption is only because this is the context in which we will use partitions of
unity here. It should be clear from the proof, that the result extends to more general covers.

3.4 Partitions of Unity
83
f ⋆g(x) =
	
Rd f (x −y)g(y) dy.
The function f ⋆g, which is called the convolution of f and g, is well deﬁned and
belongs to L1(Rd). If g is in addition of class C∞with integrable derivatives, so is
f ⋆g, with
∂α( f ⋆g) = f ⋆∂αg
for all multiindices α. If we take a function ρ with support in the unit ball and integral
equal to 1 and let ρn(x) = ndρ(nx), so that ρn has support in the ball of radius 1
n
and integral also equal to 1, then we have
f ⋆ρn →f in L p(Rd) when n →+∞
as soon as f ∈L p(Rd), 1 ≤p < +∞[72]. If ρ is in addition of class C∞, the
sequence ρn is called a mollifying sequence or a sequence of molliﬁers [2]. Indeed,
the functions f ⋆ρn are C∞approximates of f in the L p-norm.
Proposition 3.3 Let C0 be an open set such that ¯C0 ⊂Ω and Ω ⊂∪m
j=0C j. There
exist m + 1 functions ψ j : Rd →[0, 1] of class C∞such that supp ψ j ⊂¯C j and
m
j=0 ψ j = 1 in Ω.
Proof Recall ﬁrst that for any closed set A, the function
x →d(x, A) = inf
y∈A ∥x −y∥
is a continuous function from Rd into R+ that vanishes exactly on A.
We can choose η > 0 small enough so that:
1. The sets Cη
j = {x ∈C j; d(x, ∂C j) > η} still form an open cover of Ω in the
sense that Ω ⊂∪m
j=0Cη
j .
2. We can take an open set Cη
m+1 such that ¯Cη
m+1 ⊂Rd \ Ω in order to cover the
whole of Rd = ∪m+1
j=0 Cη
j , and such that d( ¯Cη
m+1, ¯Ω) > η).
This is possible by compactness of ¯Ω but we omit the (tedious) details.
The functions
ψη
j (x) =
d(x, Rd \ Cη
j )
m+1
k=0 d(x, Rd \ Cη
k )
(3.1)
are continuous on Rd, indeed the denominator never vanishes because of the covering
property. They are [0, 1]-valued and ψη
j has support Cη
j . Finally, it is clear that
m+1
j=0 ψ j(x) = 1 on Rd, with ψη
m+1 identically zero on the set {x; d(x, ¯Ω) ≤η}
which contains Ω.
This family of functions has all the desired properties except that the functions
are not smooth. We thus use the convolution with a molliﬁer ρη with support in the
ball B(0, η). We have

84
3
A Review of Analysis
1 = 1 ⋆ρη =
m+1

j=0
ψη
j

⋆ρη =
m+1

j=0
(ψη
j ⋆ρη).
Each function ψ j = ψη
j ⋆ρη has support in C j for j = 0, . . . , m+1, with ψη
m+1⋆ρη =
0 on Ω (this is the reason why we shrank all open sets by η in the beginning since
the convolution spreads supports by an amount η), and is of class C∞.
□
Let us give an example in dimension 1, see Figs.3.9, 3.10, and 3.11, without
the ﬁnal smoothing step. We take Ω = ]0, 1[, C0 = ] 1
8, 7
8[, C1 = ]−1
4, 1
4[, C2 =
] 3
4, 5
4[, C3 = ]−1
8, 3
8[ and C4 = ]−∞, 0[∪]1, +∞[. All functions can be computed
explicitly. Thus, denoting ξ j(x) = d(x, R \ C j), we have
ξ0(x) = min

x −1
8

+
,
7
8 −x

+

,
and so on.
We note that the set C3 is unnecessary to have a covering of Ω. We just added
it to have a nicer picture. If we had not added it, the partition of unity would have
been piecewise afﬁne and it is a mistake to think the partitions of unity derived from
formula (3.1) are always piecewise afﬁne!
Let us also illustrate an example in dimension 2 (Figs.3.12, 3.13, 3.14, 3.15, and
3.16), Ω is the unit disk covered by three squares of side 2.5, centered at 1, j and
j2 (identifying R2 and C) and rotated so as to form a covering of the boundary as
required. There is no C0, since the three squares already cover Ω.
Corollary 3.1 Let Ω be a bounded open set in Rd and u be a function on Ω. Let
C j, j = 0, . . . , m, be an open cover as in Proposition 3.3. Then we can write
u = m
j=0 u j with supp u j ⊂C j and u j has the same smoothness or integrability
as u.
Fig. 3.9 The ﬁve functions
ξ j, j = 0, . . . , 4
-0,25
0
0,25
0,5
0,75
1
1,25
0,25
0,5
Fig. 3.10 Their sum, i.e.,
the denominator of (3.1),
which never vanishes
-0,25
0
0,25
0,5
0,75
1
1,25
0,25
0,5

3.4 Partitions of Unity
85
-0,5
-0,25
0
0,25
0,5
0,75
1
1,25
1,5
0,25
0,5
0,75
1
y
x
Fig. 3.11 The partition of unity ψ j, j = 0, . . . , 4
Fig. 3.12 The four functions ξ j
Fig. 3.13 Their sum
Proof We use the partition of unity ψ j. Since 1 = m
j=0 ψ j on Ω, we can write
u = u × 1 = u
m

j=0
ψ j =
m

j=0
uψ j
and set u j = uψ j. As supp ψ j ⊂C j, it follows that u j vanishes outside of C j, and
since ψ j is C∞and between 0 and 1, u j is as differentiable or as integrable as u
already is.
□

86
3
A Review of Analysis
Fig. 3.14 The four functions ψ j, drawn separately
Fig. 3.15 The whole
partition of unity

3.4 Partitions of Unity
87
Fig. 3.16 Checking that
ψ1 + ψ2 + ψ3 = 1 on Ω
It is in this sense that partitions of unity are used to localize a function u. Such a
function is decomposed into a sum of functions, each with support in a given open
set of a covering. It is often easier to work with the localized parts u j than with the
function u itself. A prime example is integration by parts in the next section.
3.5
Integration by Parts in Dimension d and Applications
Integration by parts in Rd is a basic formula, that is not often entirely proved. In
what follows, Ω will be an at least Lipschitz open subset of Rd. The most crucial
integration by parts formula, from which all the others follow, is given in the next
Theorem.
Theorem 3.4 Let Ω be a Lipschitz open set in Rd and u ∈C1( ¯Ω). Then we have
	
Ω
∂u
∂xi
dx =
	
∂Ω
uni dΓ,
(3.2)
where ni is the ith component of the normal unit exterior vector.
Proof We will only write the proof in dimension d = 2, which is not a real restriction
as the general case d ≥2 follows from exactly the same arguments, and only in the
case when Ω is of class C1. This is a real restriction: there are additional technical
difﬁculties in the Lipschitz case due to only almost everywhere differentiability.
We start with the partition of unity associated with the given covering C j of the
boundary completed by an open set C0 to cover the interior. We have u = m
j=0 u j
with u j = uψ j, and each u j belongs to C1( ¯Ω) and has support in ¯C j. Consequently,
since formula (3.2) is linear with respect to u, it is sufﬁcient to prove it for each u j.
Let us start with the case j = 0. In this case, u0 is compactly supported in Ω
since ¯C0 ⊂Ω. In particular, it vanishes on ∂Ω, so that

∂Ω u0ni dΓ = 0.

88
3
A Review of Analysis
We extend u0 by 0 to R2, thus yielding a C1(R2) function ˜u0. Since Ω is bounded,
we choose a square that contains it, Ω ⊂Q = ]−M, M[2, for some M. Lettingi′ = 1
if i = 2, i′ = 2 if i = 1, we obtain
	
Ω
∂u0
∂xi
dx =
	
Q
∂˜u0
∂xi
dx =
	 M
−M
	 M
−M
∂˜u0
∂xi
dxi

dxi′ =
	 M
−M
[˜u0]xi=M
xi=−Mdxi′ = 0,
by Fubini’s theorem and the fact that ˜u0 = 0 sur ∂Q. Formula (3.2) is thus established
for u0.
The case j > 0 is a little more complicated. To simplify the notation, we omit all
j indices and exponents. We thus have a function u with support in ¯C. In particular,
u = 0 on ∂C ∩¯Ω. We have
Ω ∩C = {y ∈C; y2 < ϕ(y1)},
see also Fig.3.7. We ﬁrst establish formula (3.2) in the (y1, y2) coordinate system
in which C = ]−a, a[2 for some a. We let ny,i, i = 1, 2, denote the components of
the normal vector in this coordinate system. There are two different computations
depending on the coordinate under consideration.
Case i = 1. We ﬁrst use Fubini’s theorem
	
Ω
∂u
∂y1
dy =
	 a
−a
	 ϕ(y1)
−a
∂u
∂y1
(y1, y2) dy2

dy1,
see again Fig.3.7. Now it is well-known from elementary calculus that
d
dy1
	 ϕ(y1)
−a
u(y1, y2) dy2

=
	 ϕ(y1)
−a
∂u
∂y1
(y1, y2) dy2 + u(y1, ϕ(y1))ϕ′(y1),
(this is where the fact that ϕ is C1 intervenes and where it would be a little harder to
have ϕ only Lipschitz). Consequently,
	
Ω
∂u
∂y1
dy =
	 a
−a
d
dy1
	 ϕ(y1)
−a
u(y1, y2) dy2

dy1 −
	 a
−a
u(y1, ϕ(y1))ϕ′(y1) dy1.
In the ﬁrst integral, we integrate a derivative, so that
	 a
−a
d
dy1
	 ϕ(y1)
−a
u(y1, y2) dy2

dy1
=
	 ϕ(a)
−a
u(a, y2) dy2 −
	 ϕ(−a)
−a
u(−a, y2) dy2 = 0,

3.5 Integration by Parts in Dimension d and Applications
89
since u = 0 on ∂C. We thus see that
	
Ω
∂u
∂y1
dy =
	 a
−a
u(y1, ϕ(y1))
−ϕ′(y1)

1 + ϕ′(y1)2

1 + ϕ′(y1)2 dy1
=
	 a
−a
u(y1, ϕ(y1))ny,1(y1)

1 + ϕ′(y1)2 dy1
=
	
C∩∂Ω
uny,1 dΓ,
by the formulas established in Sect.3.3 for the normal vector components and the
boundary measure. Hence formula (3.2) in this case.
Case i = 2. We start again with Fubini’s theorem
	
Ω
∂u
∂y2
dy =
	 a
−a
	 ϕ(y1)
−a
∂u
∂y2
(y1, y2) dy2

dy1
=
	 a
−a
u(y1, ϕ(y1)) dy1
=
	 a
−a
u(y1, ϕ(y1))
1

1 + ϕ′(y1)2

1 + ϕ′(y1)2 dy1
=
	
C∩∂Ω
uny,2 dΓ,
since u(y1, −a) = 0. This proves the integration by parts formula in the (y1, y2)
system attached to the cube C covering a part of the boundary.
We need to go back to the original coordinate system (x1, x2). Let us write the
change of coordinates formulas
y1
y2

= R
x1 −c1
x2 −c2

or again
x1
x2

= RT
y1
y2

+
c1
c2

,
where R is an orthogonal matrix and (c1, c2) are the (x1, x2) coordinates of the center
of C. Similarly, if vx and vy denote the column-vectors of the components of the same
vector v ∈R2 in each of the coordinate systems, we have
vy = Rvx ⇐⇒vx = RT vy.
This is true in particular for the normal vecteur n, nx = RT ny. Let us note ∇xu and
∇yu the components of the gradient of u in the two coordinate systems, we see by
the chain rule that
(∇xu)i = ∂u
∂xi
=
2

j=1
∂u
∂y j
∂y j
∂xi
=
2

j=1
R ji
∂u
∂y j
= (RT ∇yu)i,

90
3
A Review of Analysis
hence the result for the localized part of the function, by linearity of the integrals,
and then for the whole function again by linearity of the integrals.
□
Once the basic formula is available, many other formulas are easily derived, that
bear various names in the literature. We give below a short selection of such formulas
that will be useful in the sequel. We do not specify the regularity of the functions
below, it is understood that they are sufﬁciently differentiable for all derivatives and
integrals to make sense.
Corollary 3.2 Let Ω be a Lipschitz open set in Rd. We have
(i) Integration by parts strictly speaking
	
Ω
∂u
∂xi
v dx = −
	
Ω
u ∂v
∂xi
dx +
	
∂Ω
uvni dΓ,
(3.3)
(ii) Green’s formula
	
Ω
(Δu)v dx = −
	
Ω
∇u · ∇v dx +
	
∂Ω
∂u
∂n v dΓ,
(3.4)
where ∂u
∂n = ∇u · n = d
i=1
∂u
∂xi ni denotes the normal derivative of u on ∂Ω.
(iii) A slightly more symmetrical version of Green’s formula
	
Ω
(Δu)v dx =
	
Ω
u(Δv) dx +
	
∂Ω
∂u
∂n v −u ∂v
∂n

dΓ,
(3.5)
(iv) Stokes formula
	
Ω
div U dx =
	
∂Ω
U · n dΓ,
(3.6)
where U : Ω →Rd is a vector ﬁeld, its divergence is div U = d
i=1
∂Ui
∂xi and
U · n = d
i=1 Uini is the ﬂux of the vector ﬁeld through the boundary of Ω.
Proof For (i), we apply the basic formula (3.2) to the product uv, and so on.
□
3.6
Distributions
In this section, Ω is an arbitrary open subset of Rd.
It turns out that functions that are differentiable in the classical sense are not
sufﬁcient to work with PDEs. A more general concept is needed, which is called
distributions [72, 73]. As we will see, distributions are a lot more general than
functions. They can always be differentiated indeﬁnitely, even when they correspond
to functions that are not differentiable in the classical sense, and their derivatives
are distributions. This is why distributional solutions to linear PDEs of arbitrary

3.6 Distributions
91
order make sense (with technical conditions on their coefﬁcients). We will also use
distributions to deﬁne an important class of function spaces for PDEs, the Sobolev
spaces.
Let us ﬁrst go back to the space D(Ω) of indeﬁnitely differentiable functions with
compact support encountered in Sect.3.2. It is trivial, but crucial for the sequel, that
the space D(Ω) is stable by differentiation of arbitrary order, i.e., if ϕ ∈D(Ω) then
∂αϕ ∈D(Ω) for any multiindex α.
The function ϕ(x) = e
1
∥x∥2−1 for ∥x∥< 1, ϕ(x) = 0 otherwise, belongs to D(Rd).
It can be scaled to deﬁne a molliﬁer and thus construct inﬁnitely many functions in
D(Rd) by convolution. Let us notice that if Ω contains the unit closed ball, then
the restriction of ϕ to Ω belongs to D(Ω). Conversely, it is quite clear that given a
function in D(Ω) for any Ω, by extending it by 0 to Rd \ Ω we obtain a function in
D(Rd). In this sense, it can be said that a function of D(Ω) vanishes on the boundary
of Ω, even though it is a priori only deﬁned on Ω.
As mentioned before, the vector space D(Ω) has a natural topology that is a little
difﬁcult to understand (technically, it is an LF-space, a strict inductive limit of a
sequence of Fréchet spaces and it is not metrizable, see [13]) and it is not very useful
to master the details of this topology for the applications we have in mind. So we
will just skip it.
The convergence of a sequence in D(Ω) for its natural topology is given by the
following proposition, which we admit, see [72, 73, 78, 80].
Proposition 3.4 A sequence ϕn ∈D(Ω) converges to ϕ ∈D(Ω) in the sense of
D(Ω) if and only if
(i) There exists a compact subset K of Ω such that supp ϕn ⊂K for all n.
(ii) For all α ∈Nd, ∂αϕn →∂αϕ uniformly.
It follows clearly from the above proposition that if ϕn →ϕ in the sense of D(Ω),
then ∂αϕn →∂αϕ in the sense of D(Ω) for all α ∈Nd. In addition, it is easy to see
that D(Ω) ⊂L p(Ω) for all p ∈[1, +∞] and that if ϕn →ϕ in the sense of D(Ω),
then ϕn →ϕ in L p(Ω).
When a real (or complex) vector space is equipped with a topology that makes its
vector space operations continuous, that is when it is a topological vector space, it
makes sense to look at the vector space of continuous linear forms, that is the space of
real (or complex) valued linear mappings that are continuous for the aforementioned
topology. This space is called the topological dual, or in short dual space.
Deﬁnition 3.2 The space of distributions on Ω, D′(Ω), is the dual of the space
D(Ω).
We will indifferently use the notations T (ϕ) = ⟨T, ϕ⟩to denote the value of
a distribution T on a test-function ϕ and the duality pairing between the two. Of
course, since D′(Ω) is a vector space, we can add distributions and multiply them
by a scalar in the obvious way.
Now, not knowing the topology of D(Ω) makes it a little difﬁcult to decide which
linear forms on D(Ω) are continuous and which are not. Fortunately, even though

92
3
A Review of Analysis
the topology in question is not metrizable, the usual sequential criterion happens to
still work in this particular case. We admit the following proposition, see [72, 73, 78,
80].
Proposition 3.5 A linear form T on D(Ω) is a distribution if and only if we have
T (ϕn) →0 for all sequences ϕn ∈D(Ω) such that ϕn →0 in the sense of D(Ω).
Remark 3.5 Let us note that the property T (ϕn) →0 for all sequences ϕn ∈D(Ω)
such that ϕn →0 immediately implies that T (ϕn) →T (ϕ) for all sequences ϕn such
that ϕn →ϕ in the sense of D(Ω), by linearity, hence the sequential continuity of the
linear form T . The difﬁculty is that in a non metrizable topological space, sequential
continuity does not imply continuity in general, even though, in this particular case,
it does.
□
Let us now see in which sense distributions generalize the usual notion of function.
Proposition 3.6 For all f ∈L1
loc(Ω) there exists a distribution ı( f ) on Ω deﬁned
by the formula
⟨ı( f ), ϕ⟩=
	
Ω
f ϕ dx
for all ϕ ∈D(Ω). The mapping ı : L1
loc(Ω) →D′(Ω) is one-to-one.
Proof That the integral is well-deﬁned has already been seen. It clearly deﬁnes a
linear form on D(Ω) by the linearity of the integral. What remains to be established
for ı( f ) to be a distribution, is its continuity. Let us thus be given a sequence ϕn →0
in the sense of D(Ω), and K the associated compact set. We have
|⟨ı( f ), ϕn⟩| =

	
Ω
f ϕn dx
 =

	
K
f ϕn dx

≤
	
K
| f ||ϕn| dx ≤max
K |ϕn|
	
K
| f | dx →0
since f|K ∈L1(K) and ϕn tends to 0 uniformly on K.
Let us now show that the mapping ı is one-to-one. Since it is clearly linear, it
sufﬁces to show that its kernel is reduced to the zero vector. Let f ∈ker ı, which
means that ı( f ) is the zero linear form, or in other words that

Ω f ϕ dx = 0 for
all ϕ in D(Ω). By Proposition 3.1, it follows that we have f = 0, and the proof is
complete.
□
Remark 3.6 The characterization of convergence in D(Ω) of Proposition 3.4 is
implied by the topology of D(Ω). In the proof of Proposition 3.6, we can see the
importance of having a ﬁxed compact K containing all the supports. If it was not the
case, the ﬁnal estimate would break down.
□
Remark 3.7 The mapping ı is not only one-to-one, it is also continuous (for the
topology of D′(Ω) as topological dual of D(Ω), which we also keep shrouded

3.6 Distributions
93
in mystery). The mapping ı thus provides a faithful representation of one type of
objects, L1
loc functions, as objects of a completely different nature, distributions. It is
so faithful that in day-to-day practice, we say that an L1
loc function f is a distribution
and dispense with the notation ı altogether, that is we just write ⟨f, ϕ⟩for the duality
bracket.
Conversely, when a distribution T belongs to the image of ı, that is to say when
there exists f in L1
loc such that ⟨T, ϕ⟩=

Ω f ϕ dx for all ϕ in D(Ω), we just say
that T is a function and we write T = f . Beware however that most distributions
are not functions and that the notation

Ω T ϕ dx is unacceptable for any distribution
that is not in the range of the mapping ı.
Proposition 3.6 is all the more important as it shows that the elements of all the
function spaces introduced up to now actually are distributions, since L1
loc(Ω) is the
largest of all such spaces.
□
Proposition 3.6 gives us our ﬁrst examples of distributions. There are however
many others which are not functions. Let us describe a couple of them.
We choose a point a ∈Ω and deﬁne
⟨δa, ϕ⟩= ϕ(a)
for all ϕ ∈D(Ω). This is clearly a linear form on D(Ω) and we just need to check
its continuity. Let us thus be given again a sequence ϕn →0 in the sense of D(Ω).
In particular, there exists a compact subset K on which it converges uniformly to 0,
and out of which is identically 0. Therefore, the sequence converges uniformly on Ω,
hence pointwise. Thus ϕn(a) →0 and we are done: δa ∈D′(Ω). This distribution
is called the Dirac mass or Dirac distribution at point a. When a = 0, it is often
simply denoted δ. The Dirac mass does not belong to the image of ı, i.e., loosely
speaking, it is not a function.
Let us quickly show this in the case d = 1. Assume that there exists a function
f ∈L1
loc(R) such that, for all ϕ ∈D(R), we have
	
R
f (x)ϕ(x) dx = ϕ(0).
Letting g(x) = x f (x), we have g ∈L1
loc(R), and we see that for all ϕ ∈D(R),
	
R
g(x)ϕ(x) dx =
	
R
x f (x)ϕ(x) dx =
	
R
f (x)(xϕ(x)) dx = 0
since x →xϕ(x) belongs to D(R) and vanishes at x = 0. This implies that g = 0
by Proposition 3.1, and therefore f = 0, which is a contradiction since δ ̸= 0.
Let us give a second example with Ω = R. The function x →1/x almost
everywhere is not in L1
loc(R) because it is not integrable in a neighborhood of 0.
Therefore, it cannot be identiﬁed with a distribution as in Proposition 3.6, which
is rather unfortunate for such a simple function and a concept claiming to widely

94
3
A Review of Analysis
generalize functions. The distribution deﬁned by

vp 1
x , ϕ

= lim
ε→0+
	 −ε
−∞
ϕ(x)
x
dx +
	 +∞
ε
ϕ(x)
x
dx

is called the principal value of 1/x and replaces the function x →1/x for all intents
and purposes (exercise: show that it is a distribution). It is however not a function.
We have hinted at a topology on the space of distributions. Here again, it is not
too important to know the details of this topology. The convergence of sequences is
more than enough and is surprisingly simple. We admit the following result, see [72,
73, 78, 80], which can actually be taken as a deﬁnition.
Proposition 3.7 A sequence Tn ∈D′(Ω) converges to T ∈D′(Ω) in the sense of
D′(Ω) if and only if ⟨Tn, ϕ⟩→⟨T, ϕ⟩for all ϕ ∈D(Ω).
Since distributions are linear forms on the space D(Ω), we see that convergence
in the sense of distributions is actually nothing but simple or pointwise convergence
on D(Ω). This makes it very easy to handle (and unfortunately, very easy to abuse.
Remember, it is not magic!).
This notion of convergence agrees with all previous notions deﬁned on smaller
function spaces. In particular, we have
Proposition 3.8 Let un →u in L p(Ω) for some p ∈[1, +∞]. Then un →u in the
sense of D′(Ω).
Proof For all ϕ ∈D(Ω), we have
|⟨un, ϕ⟩−⟨u, ϕ⟩| ≤
	
Ω
|un −u||ϕ| dx ≤∥un −u∥L p(Ω)∥ϕ∥L p′(Ω) →0
by Hölder’s inequality.
□
We have said earlier that distributions can be differentiated indeﬁnitely, however
in a speciﬁc sense.
Deﬁnition 3.3 Let T be a distribution on Ω. The formula
⟨S, ϕ⟩= −

T, ∂ϕ
∂xi

,
(3.7)
for all ϕ ∈D(Ω), deﬁnes a distribution S, which is called the (distributional) partial
derivative of T with respect to xi and is denoted ∂T
∂xi .
Proof This deﬁnition needs a proof. Formula (3.7) clearly deﬁnes a linear form on
D(Ω). Let us see that it is continuous. Let us be given a sequence ϕn →0 in D(Ω).
It is apparent that ∂ϕn
∂xi →0 in D(Ω). Indeed, the support condition is the same, since
the support of the partial derivative of a function is included in the support of this

3.6 Distributions
95
function, and the uniform convergence of all derivatives trivially holds true as all
derivatives of ∂ϕn
∂xi are derivatives of ϕn. Therefore,
⟨S, ϕn⟩= −

T, ∂ϕn
∂xi

→0
for all sequences ϕn →0 in D(Ω).
□
For example, the derivative of the Dirac mass δ in dimension one is the distribution
⟨δ′, ϕ⟩= −⟨δ, ϕ′⟩= −ϕ′(0),
for all ϕ ∈D(R).
The reason why it is reasonable to call this new distribution a partial derivative is
in the next proposition.
Proposition 3.9 Let u be a function in C1(Ω). Then its distributional partial deriv-
atives coincide with its classical partial derivatives.
Proof Let ϕ ∈D(Ω). The support K of uϕ is bounded and we can include it in a
hypercube C. We deﬁne v on C by v(x) = u(x)ϕ(x) if x ∈K, v(x) = 0 otherwise.
It is easy to check that v ∈C1( ¯C) and v = 0 on ∂C. Since C is a Lipschitz open set,
we can apply the integration by parts formula (3.3)3 and obtain
	
K
∂v
∂xi
dx =
	
C
∂v
∂xi
dx = 0.
Now, on K, we have ∂v
∂xi = u ∂ϕ
∂xi + ∂u
∂xi ϕ, so that
	
Ω
∂u
∂xi
ϕ dx =
	
K
∂u
∂xi
ϕ dx = −
	
K
u ∂ϕ
∂xi
dx = −
	
Ω
u ∂ϕ
∂xi
dx,
since all intervening integrands are zero outside of K, which completes the proof by
using Proposition 3.6.
□
Remark 3.8 Be careful that the same result is false for functions that are only almost
everywhere differentiable. Let us show an example, which is also a showcase example
of how to compute a distributional derivative. Let H be the Heaviside function deﬁned
on R by H(x) = 0 for x ≤0, H(x) = 1 for x > 0. This function is classically
differentiable with zero derivative for x ̸= 0 and has a discontinuity of the ﬁrst
kind at x = 0. It is also in L∞(R), hence in L1
loc(R), hence a distribution. Let us
compute its distributional derivative. Take ϕ ∈D(R) and let R > 0 be such that
supp ϕ ⊂[−R, R]. We have
3Note that there is no regularity or boundedness hypothesis made on Ω itself.

96
3
A Review of Analysis
⟨H ′, ϕ⟩= −⟨H, ϕ′⟩= −
	 R
0
ϕ′(s) ds = ϕ(0) −ϕ(R) = ϕ(0),
since ϕ and ϕ′ vanish for x ≥R. Therefore we see that
H ′ = δ
even though the almost everywhere classical derivative of H is 0. This is an example
of a function that is not differentiable in the classical sense, but that is also a distrib-
ution, hence has a distributional derivative and this derivative is not a function. The
example also shows that H is a distributional primitive of the Dirac mass.
□
Once a distribution is known to have partial derivatives of order one which are
again distributions, it is obvious that the same operation can be repeated indeﬁnitely
and we have, for any distribution T and any multiindex α,
⟨∂αT, ϕ⟩= (−1)|α|⟨T, ∂αϕ⟩
for all ϕ ∈D(Ω), by induction on the length of α.
Differentiation in the sense of distributions is continuous, which is violently false
in most function spaces. We just show here the sequential continuity, which is amply
sufﬁcient for the applications.
Proposition 3.10 Let Tn →T in the sense of D′(Ω). Then, for all multiindices α,
we have ∂αTn →∂αT in the sense of D′(Ω).
Proof For all ϕ ∈D(Ω), we have
⟨∂αTn, ϕ⟩= (−1)|α|⟨Tn, ∂αϕ⟩→(−1)|α|⟨T, ∂αϕ⟩= ⟨∂αT, ϕ⟩,
hence the result.
□
This continuity provides another reason why the partial derivative terminology is
adequate for distributions. Indeed, it can be shown that C∞(Ω) functions are dense in
D′(Ω). For any distribution T , there exists a sequence of indeﬁnitely differentiable
functions ψn that tends to T in the sense of distributions. Therefore, their distribu-
tional partial derivatives of arbitrary order, which coincide with their classical partial
derivatives, also converge in the sense of distributions. So the distributional partial
derivatives of a distribution appear as distributional limits of approximating classical
partial derivatives.
Many other operations usually performed on functions can be extended to distrib-
utions using the same transposition trick as for partial derivatives. Let us just mention
here the multiplication by a smooth function.
Deﬁnition 3.4 Let T be a distribution on Ω and f ∈C∞(Ω). The formula
⟨f T, ϕ⟩= ⟨T, f ϕ⟩,

3.6 Distributions
97
for all ϕ ∈D(Ω), deﬁnes a distribution.
We leave the easy proof as an exercise. Of course, when T ∈L1
loc(Ω), f T coin-
cides with the classical pointwise product and the mapping T →f T is sequentially
continuous on D′(Ω). Note that it is not possible to deﬁne such a product in all gen-
erality by a function that is less smooth than C∞. In particular, there is no product
of two distributions with the reasonable properties to be expected from a product—a
famous theorem by L. Schwartz that limits the usefulness of general distributions in
dealing with nonlinear PDEs.
The partial derivatives of a distribution multiplied by a smooth function follow
the classical Leibniz rule.
Proposition 3.11 Let T beadistributiononΩ and f ∈C∞(Ω).Forallmultiindices
α such that |α| = 1, we have
∂α( f T ) = f ∂αT + ∂αf T.
(3.8)
Proof We just use the deﬁnitions. For all ϕ ∈D(Ω), we have
⟨∂α( f T ), ϕ⟩= −⟨f T, ∂αϕ⟩= −⟨T, f ∂αϕ⟩= −⟨T, ∂α( f ϕ)⟩+ ⟨T, ∂αf ϕ⟩
= ⟨∂αT, f ϕ⟩+ ⟨∂αf T, ϕ⟩= ⟨f ∂αT, ϕ⟩+ ⟨∂αf T, ϕ⟩
= ⟨f ∂αT + ∂αf T, ϕ⟩
by the Leibniz formula for smooth functions.
□
We conclude this very brief review of distribution theory with the following result
[2, 51].
Proposition 3.12 Let Ω be a connected open set of Rd and T a distribution on Ω
such that ∂T
∂xi = 0 for i = 1, . . . , d. Then, there exists a constant c ∈R such that
T = c.
Proof We write the proof in the case Ω = Rd. The general case follows by a localiza-
tion argument. First of all, we claim that if ϕ ∈D(Rd) is such that

Rd ϕ(x) dx = 0,
then there exists ϕi ∈D(Rd), i = 1, . . . , d, such that
ϕ =
d

i=1
∂ϕi
∂xi
.
(3.9)
The proof of the claim is by induction on the dimension d. For d = 1, the result
holds true by taking ϕ1(x1) =

 x1
−∞ϕ(s) ds, which is clearly C∞. In addition, it is
compactly supported. Indeed, let a < b be such that supp ϕ ⊂[a, b]. If x1 < a,
we have ϕ1(x1) = 0 obviously, since the integrand vanishes. If x1 > b, we have
0 =

 +∞
−∞ϕ(s) ds = ϕ1(x1) +

 +∞
x1
ϕ(s) ds = ϕ1(x1) as well.
Assume now that decomposition (3.9) has been established for d −1. Let ϕ be
such that

Rd ϕ(x) dx = 0. We set ψ(x′) =

 +∞
−∞ϕ(x′, s) ds, so that ψ ∈D(Rd−1).
We have

98
3
A Review of Analysis
	
Rd−1 ψ(x′) dx′ =
	
Rd ϕ(x) dx = 0
by Fubini’s theorem.
Let now a < b be such that supp ϕ ⊂Rd−1×[a, b]. We pick a function θ ∈D(R)
such that supp θ ⊂[a, b] and

 +∞
−∞θ(s) ds = 1. Then we let
ϕd(x′, xd) =
	 xd
−∞
ϕ(x′, s) ds −ψ(x′)
	 xd
−∞
θ(s) ds.
It is also clear that ϕd is C∞. Let us check that ϕd is compactly supported. The
variables x′ pose no problem in this regard, so we just have to see what happens
with respect to the variable xd. For xd < a, again obviously ϕd(x′, xd) = 0. For
xd > b, we have on the one hand

 xd
−∞ϕ(x′, s) ds = ψ(x′) and on the other hand

 xd
−∞θ(s) ds = 1, thus ϕd(x′, xd) = 0. This shows that ϕd ∈D(Rd). Now, by
deﬁnition,
∂ϕd
∂xd
(x′, xd) = ϕ(x′, xd) −ψ(x′)θ(xd),
so that
ϕ(x′, xd) = ψ(x′)θ(xd) + ∂ϕd
∂xd
(x′, xd).
The induction hypothesis applies to ψ, thus proving claim (3.9).
Let us now consider a distribution T whose derivatives vanish. Let us pick a
function Θ ∈D(Rd) such that

Rd Θ(x) dx = 1 and for all ϕ ∈D(Rd), let us set
Φ(x) = ϕ(x) −
	
Rd ϕ(y) dy

Θ(x).
Clearly,

Rd Φ(x) dx = 0, so we can apply decomposition (3.9) to Φ and write
ϕ −
	
Rd ϕ(y) dy

Θ =
d

i=1
∂Φi
∂xi
,
so that
ϕ =
	
Rd ϕ(y) dy

Θ +
d

i=1
∂Φi
∂xi
.

3.6 Distributions
99
It follows that
⟨T, ϕ⟩=
	
Rd ϕ(y) dy

⟨T, Θ⟩+
d

i=1

T, ∂Φi
∂xi

=
	
Rd ϕ(y) dy

⟨T, Θ⟩−
d

i=1
 ∂T
∂xi
, Φi

=
	
Rd cϕ(y) dy,
where we have set c = ⟨T, Θ⟩. This shows that T is identiﬁed with the L1
loc(Rd)
function y →c, which happens to be a constant function.
□
This proposition states that distributions behave the same as functions when their
gradient vanishes. There is nothing exotic added in this respect when generalizing
from functions to distributions. In particular, such a T is a function in the sense of
Proposition 3.6 which is equal to the constant c almost everywhere.
3.7
Sobolev Spaces
In this section, Ω is an arbitrary open subset of Rd, unless otherwise speciﬁed. We
now introduce and brieﬂy study an important class of function spaces for PDEs, the
Sobolev spaces. As we have seen, every function in L p(Ω) is actually a distribution,
therefore it has distributional partial derivatives. In general, these derivatives are not
functions, of course. There are however some functions whose distributional deriv-
ative also are functions, even though they may not be differentiable in the classical
sense. These are the functions we are going to be interested in.
Deﬁnition 3.5 Let m ∈N and p ∈[1, +∞]. We deﬁne the Sobolev space
W m,p(Ω) = {u ∈L p(Ω); ∂αu ∈L p(Ω)
for all
α
such that
|α| ≤m}.
When p = 2, we use the notation W m,2(Ω) = H m(Ω).
Note the special case m = 0, where W 0,p(Ω) = L p(Ω) and H 0(Ω) = L2(Ω).
So the notation is hardly ever used for m = 0. In this book, we will mainly use the
H m(Ω) spaces, with special emphasis on H 1(Ω). The natural Sobolev norms are as
follows
∥u∥W m,p(Ω) =
 
|α|≤m
∥∂αu∥p
L p(Ω)
 1
p

100
3
A Review of Analysis
for p < +∞and
∥u∥W m,∞(Ω) = max
|α|≤m ∥∂αu∥L∞(Ω).
In particular, for p = 2, we have
∥u∥H m(Ω) =
 
|α|≤m
∥∂αu∥2
L2(Ω)
 1
2 .
This latter norm is clearly a prehilbertian norm associated with the scalar product
(u|v)H m(Ω) =

|α|≤m
(∂αu|∂αu)L2(Ω).
The notations ∥u∥m,p and ∥u∥m for the W m,p and H m norms are also encountered
in the literature if the context is clear.
Remark 3.9 It follows from the deﬁnition that W m+1,p(Ω) ⊂W m,p(Ω) for all m, p.
Moreover, if Ω is bounded W m,p(Ω) ⊂W m,q(Ω) whenever q ≤p. Also if Ω is
bounded, we have Cm( ¯Ω) ⊂W m,p(Ω). If Ω is Lipschitz, we have in addition that
Cm( ¯Ω) is dense in W m,p(Ω) (we will prove it later on for m = 1, p = 2). We also
refer for example to [2, 25, 35, 40, 61] for other density results in W m,p(Ω). Of
course, there are functions in W m,p(Ω) that are not of class Cm. For example, the
function x →x+ = max(x, 0) is in H 1(]−1, 1[) since (x+)′ = H|]−1,1[ in the sense
of D′ (exercise), but it is not differentiable in the classical sense at x = 0.
Similarly, there are functions in L p that are not in W 1,p, such as the Heaviside
function H whose derivative is δ, which is not a function.
□
Theorem 3.5 The spaces W m,p(Ω) are Banach spaces. In particular, the spaces
H m(Ω) are Hilbert spaces.
Proof We need to show that W m,p(Ω) is complete for its norm. Let us thus be given
a Cauchy sequence (un)n∈N in W m,p(Ω). In view of the deﬁnition of the norm, it
follows that for each multiindex α, |α| ≤m, the sequence of partial derivatives ∂αun
is a Cauchy sequence in L p(Ω). We know that L p(Ω) is complete, therefore there
exists gα ∈L p(Ω) such that ∂αun →gα in L p(Ω). By Proposition 3.8, it follows
that ∂αun →gα in the sense of D′(Ω). Now by Proposition 3.10, we also know
that ∂αun →∂αu in the sense of D′(Ω), where u = g(0,0,...,0) is the limit of the
sequence in L p(Ω). Therefore ∂αu = gα ∈L p(Ω) since the space of distributions
is separated and thus a converging sequence can only have one limit. This shows that
u belongs to W m,p(Ω) on the one hand, and that un →u in W m,p(Ω) since
∥un −u∥p
W m,p(Ω) =

|α|≤m
∥∂αun −∂αu∥p
L p(Ω) =

|α|≤m
∥∂αun −gα∥p
L p(Ω) →0,
for p < +∞and the same for p = +∞. Therefore W m,p(Ω) is complete, and so is
the proof.
□

3.7 Sobolev Spaces
101
From now on, we will mostly consider the case p = 2. Let us introduce an
important subset of H m(Ω).
Deﬁnition 3.6 The closure of D(Ω) in H m(Ω) is denoted H m
0 (Ω).
In other words, H m
0 (Ω) consists exactly of those functions u of H m(Ω) which
can be approximated in the sense of H m(Ω) by indeﬁnitely differentiable func-
tions with compact support, i.e., such that there exists a sequence ϕn ∈D(Ω) with
∥ϕn −u∥H m(Ω) →0. By deﬁnition, it is a closed vector subspace of H m(Ω) and
thus a Hilbert space for the scalar product of H m(Ω).
The following is a very important result. We introduce the semi-norm
|u|H m(Ω) =
 
|α|=m
∥∂αu∥2
L2(Ω)
 1
2 .
This semi-norm just retains the partial derivatives of the highest order compared with
the norm.
Theorem 3.6 (Poincaré’s inequality) Let Ω be a bounded open subset of Rd. There
exists a constant C which only depends on Ω such that for all u ∈H 1
0 (Ω),
∥u∥L2(Ω) ≤C|u|H 1(Ω).
Proof Since Ω is assumed to be bounded, it is included in a strip4 that we may
assume to be of the form
Ω ⊂Sa,b = {(x′, xd); x′ ∈Rd−1, a < xd < b}
for some a and b, without loss of generality, see Fig.3.17.
We argue by density. First let ϕ ∈D(Ω). We extend it by 0 to the whole of Rd
and still call the extension ϕ. Let αd = (0, 0, . . . , 0, 1) so that ∂αdϕ =
∂ϕ
∂xd . Since
ϕ(x′, a) = 0 for all x′ ∈Rd−1 and ϕ is C1 with respect to xd, we can write
ϕ(x′, xd) =
	 xd
a
∂αdϕ(x′, s) ds
for all (x′, xd). In particular, for a ≤xd ≤b, we obtain
ϕ(x′, xd)2 ≤(xd −a)
	 xd
a

∂αdϕ(x′, s)
2 ds ≤(b −a)
	 b
a

∂αdϕ(x′, s)
2 ds
by the Cauchy–Schwarz inequality. We integrate the above inequality with respect
to x′
4It is enough for Poincaré’s inequality to be valid that Ω be included in such a strip although not
necessarily bounded.

102
3
A Review of Analysis
Fig. 3.17 The open set Ω
included in a strip
b
a
x′
xd
	
Rd−1 ϕ(x′, xd)2 dx′ ≤(b −a)
	
Sa,b

∂αdϕ(x)
2 dx
by Fubini’s theorem. Now, because the support of ϕ is included in Ω ⊂Sa,b, it
follows that
	
Sa,b

∂αdϕ(x)
2 dx = ∥∂αdϕ∥2
L2(Ω).
We integrate again with respect to xd between a and b and obtain
∥ϕ∥2
L2(Ω) ≤(b −a)2∥∂αdϕ∥2
L2(Ω),
for the same reasons (Fubini and support of ϕ). Now by deﬁnition of the semi-norm,
it follows that
∥∂αdϕ∥2
L2(Ω) ≤

|α|=1
∥∂αϕ∥2
L2(Ω) = |ϕ|2
H 1(Ω),
hence Poincaré’s inequality for a function ϕ ∈D(Ω) with constant C = (b −a).
We complete the proof by a density argument. Let u ∈H 1
0 (Ω). By deﬁnition of
H 1
0 (Ω) as the closure of D(Ω) in H 1(Ω), there exists a sequence ϕn ∈D(Ω) such
that ϕn →u in H 1(Ω). Inspection of the deﬁnition of the H 1 norm reveals that this
is equivalent to ϕn →u in L2(Ω) and ∂αϕn →∂αu for all |α| = 1 also in L2(Ω).
Since all the L2 norms then converge, we obtain in the limit
∥u∥L2(Ω) ≤(b −a)|u|H 1(Ω),
which is Poincaré’s inequality on H 1
0 (Ω).
□
Remark 3.10 Poincaré’s inequality shows that H 1
0 (Ω) is a strict subspace of H 1(Ω)
when Ω is bounded. Indeed, the constant function u = 1 is in H 1(Ω) but does

3.7 Sobolev Spaces
103
not satisfy the inequality, since all its partial derivatives vanish. It follows that it is
impossible to approximate a non zero constant by a sequence in D(Ω) in the norm
of H 1(Ω).
□
From now on, we will use the gradient notation ∇u to denote the vector of all ﬁrst
order distributional partial derivatives of u. We already used the simpliﬁed notation
∂i =
∂
∂xi for individual ﬁrst order derivatives instead of the multiindex notation.
Similarly, we note ∂i j =
∂2
∂xi∂x j for second order derivatives. When u ∈H 1(Ω), then
we have ∇u ∈L2(Ω; Rd). Poincaré’s inequality has an important corollary.
Corollary 3.3 Let Ω be a bounded subset of Rd. The H 1 semi-norm | · |H 1(Ω) is
a norm on H 1
0 (Ω) that is equivalent to the H 1 norm. It is also a Hilbertian norm
associated with the scalar product
(u|v)H 1
0 (Ω) =
	
Ω
∇u · ∇v dx.
Proof First of all, it is clear that |u|H 1(Ω) ≤∥u∥H 1(Ω) for all u ∈H 1(Ω), hence
all u ∈H 1
0 (Ω), since the norm squared is the semi-norm squared plus the L2 norm
squared.
The converse inequality follows from Poincaré’s inequality. Indeed, for u ∈
H 1
0 (Ω), we have ∥u∥L2(Ω) ≤C|u|H 1(Ω). Therefore
∥u∥H 1(Ω) = (∥u∥2
L2(Ω) + |u|2
H 1(Ω))
1
2 ≤(C2 + 1)
1
2 |u|H 1(Ω).
This shows both that the semi-norm is a norm on H 1
0 (Ω) and that it is equivalent to
the full H 1 norm on H 1
0 (Ω). This also shows that the bilinear form above is positive
deﬁnite, hence a scalar product.
□
Remark 3.11 The fact that the two norms are equivalent implies that H 1
0 (Ω) is also
complete for the semi-norm. Hence, it is also a Hilbert space for the scalar product
corresponding to the semi-norm. Beware however that this is a different Hilbert
structure from the one obtained by restricting the H 1 scalar product to H 1
0 . Indeed,
we now have two different notions of orthogonality, and (at least) two different ways
of identifying the dual of H 1
0 , see Theorem 3.3.
□
Remark 3.12 The above results generalize to H m
0 (Ω) on which the semi-norm
| · |H m(Ω) is equivalent to the full H m norm. They also generalize to the spaces
W m,p
0
(Ω) deﬁned in a obvious way.
□
Let us give yet another way of identifying the dual of H 1
0 (Ω) as a subspace of the
space of distributions, see [2, 25, 35].
Deﬁnition 3.7 Let
H −1(Ω) = {T ∈D′(Ω); ∃C, ∀ϕ ∈D(Ω), |⟨T, ϕ⟩| ≤C|ϕ|H 1(Ω)},
(3.10)

104
3
A Review of Analysis
equipped with the norm
∥T ∥H −1(Ω) = inf{C appearing in formula (3.10)} =
sup
ϕ∈D(Ω)
ϕ̸=0
|⟨T, ϕ⟩|
|ϕ|H 1(Ω)
.
Then H −1(Ω) is isometrically isomorphic to (H 1
0 (Ω))′.
Proof Since D(Ω) ⊂H 1
0 (Ω) by deﬁnition, any linear form ℓon H 1
0 (Ω) deﬁnes a
linear form on D(Ω) by restriction. Moreover, if ϕn →0 in D(Ω), we obviously
have ϕn →0 in H 1
0 (Ω) as well. Hence, if ℓis continuous, that is ℓ∈(H 1
0 (Ω))′, its
restriction to D(Ω) is a distribution T ∈D′(Ω). This distribution clearly belongs
to H −1(Ω).
Conversely, let us be given an element T of H −1(Ω). By deﬁnition, it is a linear
form deﬁned on a dense subspace of H 1
0 (Ω) and continuous with respect to the H 1
0 -
norm. Therefore, it extends to an element ℓof the dual space (H 1
0 (Ω))′, with the
same norm.
□
Remark 3.13 The identiﬁcation of the dual of H 1
0 (Ω) with H −1(Ω) is the one that
leads to the pivot space inclusions of Remark 3.1, V →H →V ′, in the case of
H = L2(Ω) and V = H 1
0 (Ω). Indeed, the scalar product used in the identiﬁcation of
the pivot space with its dual is equal to the duality bracket of an L2 function seen as
a distribution and a D test-function, when the second argument in the scalar product
is such a test-function, i.e., if ϕ ∈D(Ω) and f ∈L2(Ω), we have
⟨f, ϕ⟩=
	
Ω
f ϕ dx.
The two scalar products that H 1
0 (Ω) comes equipped with do not have this property.
Consequently, an identiﬁcation of H 1
0 (Ω) with its dual using either one of the latter
scalar products, even though it is legitimate, does not use the same duality as the one
used to identify a function with a distribution.
□
In order to explain the −1 exponent in the notation, we note the following.
Proposition 3.13 Let f ∈L2(Ω),then∂i f ∈H −1(Ω)and⟨∂i f, ϕ⟩= −

Ω f ∂iϕ dx
for all ϕ ∈D(Ω).
Proof By deﬁnition of distributional derivatives,
⟨∂i f, ϕ⟩= −⟨f, ∂iϕ⟩= −
	
Ω
f ∂iϕ dx,
since f is locally integrable. Thus
|⟨∂i f, ϕ⟩| ≤∥f ∥L2(Ω)∥∂iϕ∥L2(Ω) ≤∥f ∥L2(Ω)|ϕ|H 1
0 (Ω),

3.7 Sobolev Spaces
105
by the Cauchy–Schwarz inequality, hence ∂i f ∈H −1(Ω) with ∥∂i f ∥H −1(Ω) ≤
∥f ∥L2(Ω).
□
Remark 3.14 This shows that the operator ∂i is linear continuous from L2(Ω)
(= H 0(Ω)) into H −1(Ω), just as it is linear continuous from H 1(Ω) into L2(Ω).
Each time, the exponent in the notation gets decremented by 1 as one derivative
is lost.
□
Remark 3.15 When Ω is regular, a distribution in H −1(Ω) whose ﬁrst order partial
derivatives are all in H −1(Ω) is in fact a function in L2(Ω). The latter result is known
as Lions’s lemma.
□
The above bracket formula is also valid for all v ∈H 1
0 (Ω), in the sense that
⟨∂i f, v⟩H −1(Ω),H 1
0 (Ω) =
	
Ω
f ∂iv dx,
by density. In the same vein, we have
Corollary 3.4 The operator −Δ is linear continuous from H 1
0 (Ω) into H −1(Ω)
and for all u, v ∈H 1
0 (Ω), we have
⟨−Δu, v⟩H −1(Ω),H 1
0 (Ω) =
	
Ω
∇u · ∇v dx.
The dual of H m
0 (Ω) is likewise identiﬁed with a subspace H −m(Ω) of D′(Ω).
3.8
Properties of Sobolev Spaces in One Dimension
The one-dimensional case is simple and useful to get acquainted with the properties
of Sobolev spaces in general. For simplicity, we mostly consider H 1(Ω) where
Ω = ]a, b[ is a bounded open interval of R. Let us admit a density result that we
will prove later in arbitrary dimension.
Proposition 3.14 The space C1([a, b]) is dense in H 1(]a, b[).
The density above is meant in the sense that the equivalence classes of elements
of C1([a, b]) are dense in H 1(]a, b[). We have already seen examples of functions
in dimension one that are H 1 but not C1. All one-dimensional H 1 functions however
are continuous, in the sense that each equivalence class contains one continuous
representative. There is even a more precise embedding.
Theorem 3.7 We have that H 1(]a, b[) →C0,1/2([a, b]).
Recall that the hooked arrow means that there is an injection between the two
spaces and that this injection is continuous.

106
3
A Review of Analysis
Proof Let us be given v ∈H 1(]a, b[). The distributional derivative v′ is in L2(a, b)
hence is integrable on [a, b]. For all x ∈[a, b], we thus deﬁne
w(x) =
	 x
a
v′(t) dt.
For all x, y ∈[a, b], we can consequently write (with the convention

 x
y g dt =
−

 y
x g dt)
w(y) −w(x) =
	 y
x
v′(t) dt.
Squaring this relation, we see that
(w(y) −w(x))2 =
	 y
x
v′(t) dt
2
≤|y −x|
	 b
a
(v′(t))2 dt ≤|y −x||v|2
H 1(]a,b[),
by the Cauchy–Schwarz inequality. Therefore, for all x ̸= y, we obtain
|w(y) −w(x)|
|y −x|1/2
≤|v|H 1(]a,b[).
(3.11)
It follows from this that w is Hölder continuous of exponent 1
2 on [a, b]. Therefore,
w is a distribution on ]a, b[. Let us compute its derivative. For all ϕ ∈D(]a, b[), we
have
⟨w′, ϕ⟩= −⟨w, ϕ′⟩= −
	 b
a
w(x)ϕ′(x) dx
= −
	 b
a
	 x
a
v′(t) dt

ϕ′(x) dx = −
	 b
a
	 b
t
ϕ′(x) dx

v′(t) dt
= −
	 b
a
(ϕ(b) −ϕ(t))v′(t) dt =
	 b
a
ϕ(t)v′(t) dt = ⟨v′, ϕ⟩,
since ϕ(b) = 0. The integral interchange is justiﬁed by Fubini’s theorem. Therefore,
we have shown that w′ = v′, from which it follows that there exists a constant c such
that v = w+c by Proposition 3.12. Consequently, v has a representative that belongs
to C0,1/2([a, b]), namely w + c, and we can write
v(x) = v(y) +
	 x
y
v′(t) dt,
(3.12)

3.8 Properties of Sobolev Spaces in One Dimension
107
for all5 x, y in [a, b] by using the deﬁnition of w. Squaring this relation and using
the Cauchy–Schwarz inequality again, we obtain that
v(x)2 ≤2v(y)2 + 2(b −a)
	 b
a
v′(t)2 dt,
which we integrate with respect to y to obtain
(b −a)v(x)2 ≤2∥v∥2
L2(a,b) + 2(b −a)2∥v′∥2
L2(a,b) ≤2 max(1, (b −a)2)∥v∥2
H 1(]a,b[).
As this holds true for all x in [a, b], it follows that
∥v∥C0([a,b]) ≤

2 max

1
b −a , b −a

∥v∥H 1(]a,b[).
(3.13)
Putting estimates (3.11) and (3.13) together, we obtain the announced continuous
embedding.
□
That all H 1 functions are continuous is speciﬁc to dimension one, as we will see
later.
Note also that not all C0,1/2 functions belong to H 1 (consider x →√x on ]0, 1[).
The injection above is nonetheless optimal since for each β > 1/2, there is an H 1
function that is not C0,β (consider x →x
2β+1
4
on ]0, 1[).
An important feature of the one-dimensional case is that pointwise values of
a H 1 function are unambiguously deﬁned as the pointwise value of its continuous
representative. Moreover, such pointwise values depend continuously on the function
in the H 1 norm by estimate (3.13). This is in particular true of the endpoint values
at a and b, which can be surprising because the Sobolev space deﬁnition is based on
the open set ]a, b[, extremities excluded.
Corollary 3.5 The linear mapping H 1(]a, b[) →R2, u →(u(a), u(b)) is contin-
uous.
Proof Obviously max(|u(a)|, |u(b)|) ≤∥u∥C0,1/2([a,b]) ≤C∥u∥H 1(]a,b[).
□
Remark 3.16 This is the one-dimensional version of the trace theorem that we will
prove in all dimensions later on. The linear mapping in question is called the trace
mapping. The result also shows that Dirichlet boundary conditions make sense for
functions of H 1(]a, b[), a fact that was not evident from the start.
Because of the continuity of the trace, it is clear that H 1
0 (]a, b[) is included in the
kernel of the trace {u ∈H 1(]a, b[); u(a) = u(b) = 0}. It sufﬁces to take a sequence
ϕn of D(]a, b[) that tends to u in H 1(]a, b[). Actually, the reverse inclusion holds
true so that
5And not only almost everywhere, since we are now talking about the continuous representative
of v.

108
3
A Review of Analysis
H 1
0 (]a, b[) = {u ∈H 1(]a, b[); u(a) = u(b) = 0},
see Proposition 3.16 in any dimension. The space H 1
0 (]a, b[) is thus adequate for
homogeneous Dirichlet conditions for second order boundary value problems.
□
Remark 3.17 We also have H m(]a, b[) →Cm−1,1/2([a, b]), the trace on H m(]a, b[)
is u →(u(a), u′(a), . . . , u(m−1)(a), u(b), u′(b), . . . , u(m−1)(b)) and H m
0 (]a, b[) is
the set of u such that u(a) = u′(a) = · · · = u(m−1)(a) = u(b) = u′(b) = · · · =
u(m−1)(b) = 0. Similar results can be written for the W m,p(]a, b[) spaces, not with
the same Hölder exponent though (exercise).
□
To conclude the one-dimensional case, let us mention the Rellich compact embed-
ding theorem.
Theorem 3.8 The injection H 1(]a, b[) →L2(a, b), u →u is compact.
Proof A mapping is compact if it transforms bounded sets into relatively compact
sets. Here, it is enough to take the unit ball of H 1(]a, b[) by linearity. By esti-
mates (3.11) and (3.13), this is a bounded subset of C0,1/2([a, b]). Bounded sets
of C0,1/2([a, b]) are equicontinuous, therefore relatively compact in C0([a, b]) by
Ascoli’s theorem [7, 32, 33, 69]. Finally the embedding C0([a, b]) →L2(a, b) is
continuous, thus transforms relatively compact sets into relatively compact sets. □
Remark 3.18 The Rellich theorem is true in arbitrary dimension d, i.e., the embed-
ding H 1(Ω) →L2(Ω) is compact, provided that Ω is bounded and sufﬁciently
regular, for example Lipschitz, see [15, 35, 66].
□
3.9
Density of Smooth Functions and Trace in Dimension d
We have seen that Sobolev functions in dimension one are continuous. This is no
longer true in dimensions 2 and higher. We will concentrate on the space H 1(Ω).
Note however that functions in W 1,p(Ω), where Ω is an open subset of Rd, are
continuous for p > d, this is known as Morrey’s theorem, [15, 35]. See also [2, 58]
for functions in W m,p for mp > d.
Let D be the unit disk in R2. It can be checked (exercise) that the function u : x →
ln(| ln(∥x∥/e)|) is in H 1
0 (D). This function tends to +∞at the origin, thus there is
no continuous function in its equivalence class, see Fig.3.18.
Now we can do much worse! We extend u by 0 to R2, which still is a function in
H 1(R2). Next, let (xi)i∈N be countable, dense set of points in R2. Then the function
v(x) = +∞
i=0 2−iu(x −xi) is in H 1(R2), since ∥u(· −xi)∥H 1(R2) = ∥u∥H 1(R2) and
we have a normally convergent series, but this function tends to +∞at all points xi,
which are dense. Therefore, v is not locally bounded: there is no open set on which it
is bounded. This sounds pretty bad, even though it is a perfectly legitimate, although
hard to mentally picture, function of H 1(R2), see Fig.3.19.

3.9 Density of Smooth Functions and Trace in Dimension d
109
Fig. 3.18 A discontinuous H1-function
Fig. 3.19 An attempt to draw a very bad H1-function (graphics cheat: the spikes should be thinner,
(inﬁnitely) higher and (inﬁnitely) denser)

110
3
A Review of Analysis
In higher dimensions, we can picture such singularities occurring on a dense set of
curves or submanifolds of dimension d −2. In view of this state of things, ascribing
some kind of boundary value to a H 1 function that would be a reasonably deﬁned
continuous extension from the values taken in Ω seems difﬁcult. In PDE problems,
we nonetheless need boundary values, to write Dirichlet conditions for example.
The deﬁnition of a good boundary value for H 1 functions is by means of a mapping
called the trace mapping. This mapping is deﬁned by density of smooth functions, so
let us deal with that ﬁrst. Besides, as should already be quite clear, density arguments
are very useful in Sobolev spaces [2, 25, 35, 40, 61].
Theorem 3.9 Let Ω be a Lipschitz open subset of Rd. Then the space C1( ¯Ω) is
dense in H 1(Ω).
Proof We use the same partition of unity as before. It sufﬁces to construct a C1( ¯Ω)
approximation for each part u j = ψ ju of u. Indeed, we have u j ∈H 1(Ω) for all j
by Proposition 3.11.
We start with the case j = 0. Since u0 is compactly supported in Ω, its extension
by 0 to the whole of Rd belongs to H 1(Rd) as is easily checked. Let us take a molliﬁer
ρ, that is to say a C∞function with compact support in the unit ball B and such that

B ρ(y) dy = 1 as in Sect.3.4.
For all integers n ≥1, we set ρn(y) = ndρ(ny) and u0,n = ρn ⋆u0, where the
star denotes the convolution as usual. By the general properties of convolution, u0,n
is compactly supported in Ω for n sufﬁciently large, and we have u0,n ∈C∞(Rd) ∩
L2(Rd) and u0,n →u0 in L2(Rd) when n →+∞. Moreover, since ∂iu0,n =
ρn⋆∂iu0, the same argument shows that ∂iu0,n →∂iu0 in L2(Rd), whence u0,n →u0
in H 1(Rd) when n →+∞. This settles the case j = 0 because the restriction of
u0,n to ¯Ω is of class C1 (in fact, it is even compactly supported as soon as n is large
enough) and the H 1 norm on Ω is smaller than the H 1 norm on Rd.
The regularity of Ω comes into play for j > 0, in the hypercubes C j that cover the
boundary. We drop again all subscripts or superscripts j for brevity. The difﬁculty
compared with j = 0 is that we cannot extend u by 0 to Rd and remain in H 1(Rd).
For example, it is easy to see that the function equal to 1 in Ω and 0 outside is not
in H 1(Rd). We will use a two step process, ﬁrst a translation, then a convolution.
Let n ∈N∗. We set un(y) = u(y′, yd −1/n), which is a function deﬁned on the
translated set Ωn = {y ∈Rd; (y′, yd −1/n) ∈Ω ∩C}, see Fig.3.20. We extend un
by 0 to Rd and letun denote this extension. Since u is compactly supported in C, and
the translation shifts it upwards, the restriction of un to Ω ∩C is still in H 1(Ω ∩C)
for n large enough.
It can be shown6 that the translation is continuous on L2(Rd) in the sense that
un →u in L2(Rd) when n →+∞, thus by restriction we have un|Ω∩C →u|Ω∩C in
L2(Ω ∩C). Computing the partial derivatives in the sense of distributions shows that
∂i(un|Ω∩C) = (∂iu)n|Ω∩C using the same notation for the translation. Therefore, we
have the same convergence for the partial derivatives, which shows that un|Ω∩C →
6The fairly easy proof uses the density of continuous, compactly supported functions in L2(Rd).

3.9 Density of Smooth Functions and Trace in Dimension d
111
u|Ω∩C in H 1(Ω ∩C) when n →+∞. To conclude, we just need to approximate
un|Ω∩C for any given n by a C1( ¯Ω) function, and use a double limit argument.
We now use the convolution by a molliﬁer again and set un,p = un ⋆ρp. By
construction, un,p is of class C∞on Rd and un,p →un in L2(Rd) when p →+∞.
Now for the subtle point. We do not have L2 convergence of the gradients, because in
general ∂iun is not a function, let alone in L2(Rd). Take for example ϕ = 0 and u = 1,
then ∂dun is a Dirac mass on the hyperplane yd = 1
n , cf. the one-dimensional case.
However, since we have shifted the discontinuity outside of Ω by the translation,
there is hope that the restrictions to Ω still converge.
To see that this is the case, we let 
∂iun denote the extension of ∂iun to Rd by 0.
We have 
∂iun ∈L2(Rd) and 
∂iun ⋆ρp →
∂iun in L2(Rd) when p →+∞by the
properties of convolution again. Of course, as already noted, 
∂iun ̸= ∂iun so that

∂iun ⋆ρp ̸= ∂iun,p. We will show that, for p large enough, we nonetheless have
(
∂iun ⋆ρp)|Ω∩C = (∂iun,p)|Ω∩C. As we have just seen that 
∂iun ⋆ρp converges in
L2(Rd), this will lead to the conclusion that (∂iun,p)|Ω∩C →∂iun in L2(Ω ∩C),
hence un,p|Ω∩C →un|Ω∩C in H 1(Ω ∩C) when p →+∞. Since un,p|Ω∩C ∈
C1(Ω ∩C), we will have our approximation.
To show that the restrictions are equal, we go back to the convolution formula

∂iun ⋆ρp(x) =
	
Rd ρp(x −y)
∂iun(y) dy =
	
B(x,1/p)
ρp(x −y)
∂iun(y) dy
since ρ has support in the unit ball. Now, if for all x ∈Ω ∩C, we had B

x, 1
p

⊂Ωn,
then the only values of 
∂iun in the integral would coincide with those of ∂iun, see
Fig.3.20. Hence the equality of the restrictions since ∂iun,p = ρp ⋆∂iun.
We are thus down to a geometry question, where the regularity of Ω intervenes
(at last). We need to estimate the distance between the graph of ϕ, denoted G, and
the same graph translated upwards by 1
n , denoted Gn. Let L be the Lipschitz constant
of ϕ and take two points x ∈G and y ∈Gn. We have
∥y −x∥2 = ∥y′ −x′∥2 +

ϕ(y′) −ϕ(x′) + 1
n
2
,
using the prime notation to denote the projection on Rd−1 as usual. Now
ϕ(y′) −ϕ(x′) + 1
n ≥1
n −L∥y′ −x′∥,
therefore if ∥y′−x′∥≤
1
2nL , then ∥y−x∥≥
1
2n . On the other hand, if ∥y′−x′∥≥
1
2nL ,
it follows trivially that ∥y −x∥≥
1
2nL . We thus see that
∥y −x∥≥min
 1
2n ,
1
2nL

.

112
3
A Review of Analysis
Fig. 3.20 The translated
open set Ωn and the ball of
radius 1
p used to compute the
convolution at point x. For x
in ¯Ω ∩C, the ball remains
included in Ωn uniformly for
p →+∞, n ﬁxed
If we choose
p >
1
min
 1
2n ,
1
2nL
,
then B

x, 1
p

⊂Ωn, hence the ﬁnal result.
□
Remark 3.19 There is a slight cheat in the geometric part of the above proof, in that
we have ignored what happens on the lateral sides of C. Indeed, there is actually no
problem, since u vanishes there.
□
Remark 3.20 Warning: there are open sets less regular than Lipschitz on which not
only does the above proof not work, but the density result is false (exercise, ﬁnd a
simple example). It is however always true that C1(Ω)∩H 1(Ω) is dense in H 1(Ω),
a weaker result (this is the Meyers–Serrin theorem, see [1]) which is sometimes
sufﬁcient, although not here for the existence of the trace mapping.
□
Once the density of C1( ¯Ω) is established, we can prove the trace theorem.
Theorem 3.10 Let Ω be a Lipschitz open subset of Rd. There exists a unique con-
tinuous linear mapping γ0 : H 1(Ω) →L2(∂Ω) such that for all u ∈C1( ¯Ω), we
have
γ0(u) = u|∂Ω.
In particular, there exists a constant Cγ0 such that, for all u ∈H 1(Ω),
∥γ0(u)∥L2(∂Ω) ≤Cγ0∥u∥H 1(Ω).

3.9 Density of Smooth Functions and Trace in Dimension d
113
In other words, the trace is the unique reasonable way of deﬁning a boundary value
for H 1(Ω) functions, as the continuous extension of the restriction to the boundary
for functions for which this restriction makes sense unambiguously, i.e., functions
in C1( ¯Ω).
Proof We write the proof only in dimension d = 2, but the general case is strictly
identical, up to heavier notation.
Let u ∈C1( ¯Ω). By partition of unity, we consider uψ which is supported in one
of the C j = C for j = 1, . . . , m. Let G = ∂Ω ∩C be the part of the boundary
included in C. By deﬁnition of the boundary measure, we have
∥uψ∥2
L2(G) =
	 a
−a
(uψ)(y1, ϕ(y1))2
1 + ϕ′(y1)2 dy1
=
	 a
−a
	 ϕ(y1)
−a
∂(uψ)
∂y2
(y1, y2) dy2
2
1 + ϕ′(y1)2 dy1,
since ψ(y1, −a) = 0. By the Cauchy–Schwarz inequality, we have
	 ϕ(y1)
−a
∂(uψ)
∂y2
(y1, y2) dy2
2
≤|ϕ(y1) + a|
	 ϕ(y1)
−a
∂(uψ)
∂y2
(y1, y2)
2
dy2,
with |ϕ(y1) + a| ≤2a. Let us set M = max[−a,a]

1 + ϕ′(y1)2. We obtain
∥uψ∥2
L2(G) ≤2aM
	 a
−a
	 ϕ(y1)
−a
∂(uψ)
∂y2
2
dy2dy1 = 2aM
	
Ω∩C
∂(uψ)
∂y2
2
dx.
Now ∂(uψ)
∂y2
= ψ ∂u
∂y2 + u ∂ψ
∂y2 , so that
∥uψ∥2
L2(G) ≤4aM
	
Ω∩C
ψ2 ∂u
∂y2
2
dx +
	
Ω∩C
u2 ∂ψ
∂y2
2
dx

≤4aM
 ∂u
∂y2

2
L2(Ω∩C) + max
Ω∩C
 ∂ψ
∂y2
2
∥u∥2
L2(Ω∩C)
 
≤K 2∥u∥2
H 1(Ω),
(remembering that ψ is [0, 1]-valued) where K = K j is a constant which depends
on j.
We put all the estimates together by partition of unity and the triangle inequality:
∥u∥L2(∂Ω) ≤
m

j=0
∥uψ j∥L2(G j) ≤(m + 1)K∥u∥H 1(Ω),
forallu ∈C1( ¯Ω)where K = max j=1,...,m K j.Thelinearmappingu →u|∂Ω deﬁned
on C1( ¯Ω) is thus continuous in the H 1(Ω) and L2(∂Ω) norms. Since C1( ¯Ω) is dense

114
3
A Review of Analysis
in H 1(Ω), the mapping has a unique continuous extension to H 1(Ω) with values in
L2(∂Ω), which is called the trace mapping γ0.
□
Remark 3.21 Note that if u ∈H 1(Ω) ∩C0( ¯Ω) the trace γ0(u) is equal to the
restriction of the function to ∂Ω, u|∂Ω, even if u is not in C1( ¯Ω).
□
Remark 3.22 Again, there are open sets less regular than Lipschitz on which no trace
mapping can be deﬁned.
□
We now are in a position to extend the integration by parts formula(s) to elements
of Sobolev spaces.
Theorem 3.11 Let Ω be a Lipschitz open set and u, v ∈H 1(Ω). Then we have
	
Ω
∂u
∂xi
v dx = −
	
Ω
u ∂v
∂xi
dx +
	
∂Ω
γ0(u)γ0(v)ni dΓ,
(3.14)
where ni is the ith component of the normal unit exterior vector n.
Proof We argue by density. We already know that formula (3.14) holds true on
C1( ¯Ω), see Corollary 3.2. Let un, vn be sequences in C1( ¯Ω) such that un →u and
vn →v in H 1(Ω) when n →+∞. This means that un →u, vn →v, ∂iun →∂iu
and ∂ivn →∂iu in L2(Ω). Therefore, ∂iunvn →∂iuv and un∂ivn →u∂iv in L1(Ω)
and the left-hand side integral and the ﬁrst integral in the right-hand side pass to the
limit. Secondly, we have γ0(un) →γ0(u) and γ0(vn) →γ0(v) in L2(∂Ω) since the
trace mapping is continuous, hence γ0(un)γ0(vn) →γ0(u)γ0(v) in L1(∂Ω) and the
second integral in the right-hand side also passes to the limit.
□
The various corollaries of the integration by parts formula also hold true, provided
all the integrals make sense. For instance, for all u ∈H 1(Ω),
	
Ω
∂u
∂xi
dx =
	
∂Ω
γ0(u)ni dΓ,
as is seen from taking v = 1.
The formulas that entail second derivatives should be applied to H 2 functions.
Such functions u are in H 1(Ω), thus have a trace γ0(u) and they also have a second
trace γ1(u), called the normal trace, that plays the role of the normal derivative
for a regular function. Indeed, ∂iu ∈H 1(Ω) therefore γ1(u) = d
i=1 γ0(∂iu)ni is
well deﬁned and continuous from H 2(Ω) into L2(∂Ω), because the functions ni are
in L∞(∂Ω). Furthermore, if u ∈C2( ¯Ω), then γ1(u) = ∂u
∂n . We thus establish the
following result (Green’s formula for Sobolev spaces).
Proposition 3.15 Let Ω be a Lipschitz open set. For all u ∈H 2(Ω) and all v ∈
H 1(Ω), we have
	
Ω
(Δu)v dx = −
	
Ω
∇u · ∇v dx +
	
∂Ω
γ1(u)γ0(v) dΓ.
(3.15)

3.9 Density of Smooth Functions and Trace in Dimension d
115
Proof The proof is by density of C2( ¯Ω) in H 2(Ω) and of C1( ¯Ω) in H 1(Ω), starting
from formula (3.4).
□
Proposition 3.16 Let Ω be a Lipschitz open set. Then we have
H 1
0 (Ω) = ker γ0.
Proof One inclusion is easy. The space H 1
0 (Ω) is by deﬁnition the closure of the
space D(Ω) in H 1(Ω). Thus, if u ∈H 1
0 (Ω), then there exist ϕn ∈D(Ω) such that
ϕn →u in H 1(Ω). It is clear by deﬁnition of the trace mapping that γ0(ϕn) = 0,
thus u ∈ker γ0 by continuity of the trace, or in other words H 1
0 (Ω) ⊂ker γ0.
We just give the idea for the reverse inclusion [25, 35, 66]. Take a zero trace
function u, use a partition of unity adapted to the boundary, extend all the parts to the
whole of Rd by 0 (the integration by parts formula (3.14) shows that the extension
remains in H 1(Rd) this time), translate each function downwards7 by a small amount
in its cube then perform the convolution step which provides a compactly supported,
C∞approximation. We leave the details to the reader, all the necessary technical
elements have already been introduced previously.
□
Remark 3.23 By Remark 3.21, if u ∈H 1(Ω) ∩C0( ¯Ω) is such that u|∂Ω = 0, then
u ∈H 1
0 (Ω).
□
Remark 3.24 Similar arguments show that H 2
0 (Ω) = ker γ0 ∩ker γ1 and so on.
Everything we have said in terms of traces can also naturally be done in the spaces
W m,p(Ω).
□
It should be noted that the trace mapping γ0 is not onto.
Proposition 3.17 The image space of the trace mapping, im γ0, is a strict, dense
subspace of L2(∂Ω). This space is denoted H 1/2(∂Ω). The norm
∥g∥H 1/2(∂Ω) =
inf
v∈H 1(Ω)
γ0(v)=g
∥v∥H 1(Ω)
makes H 1/2(∂Ω) into a Hilbert space.
There are other equivalent norms on H 1/2(∂Ω). We do not pursue the study of
the trace space H 1/2(∂Ω) here, see for example [25, 58, 61].
3.10
A Summary of Important Results
Let us now give a quick review of the results of this chapter that are essential for
the following chapters, i.e., everything that concerns variational formulations and
variational approximation methods.
7Instead of upwards for the density result.

116
3
A Review of Analysis
The elementary properties of Hilbert spaces of Sect.3.1 are useful to understand
the abstract variational problems considered in the next chapter.
The Green’s formulas of Corollary 3.2 in the classical case p. 90, and of
Proposition 3.15 in the Sobolev case p. 114, are essential to establish the varia-
tional formulation of elliptic boundary value problems in more than one dimension
of space.
There is no real need for a deep understanding of distribution theory, see [72, 73].
However, the characterization of distributions of Proposition 3.5, p. 92, the deﬁnition
of distributional derivatives of Deﬁnition 3.3, p. 94, and the characterization of the
convergence in the sense of distributions of Proposition 3.7, p. 94 are always good
to know.
The hilbertian Sobolev spaces H 1(Ω) and H 1
0 (Ω) introduced in Deﬁnitions 3.5,
p.99and3.6,p.101,arealsoessentialasthebasicfunctionspacesinwhichvariational
problems are set.
We will have to use Poincaré’s inequality, Theorem 3.6, p. 101, and its conse-
quences, for example Corollary 3.3, p. 103, for homogeneous Dirichlet problems.
The existence and properties of the trace mapping, Theorem 3.10, p. 112 and
Proposition 3.16, p. 115, are also key for what follows.

Chapter 4
The Variational Formulation of Elliptic PDEs
We now begin the theoretical study of elliptic boundary value problems in a context
that is more general than the one-dimensional model problem treated in Chap.1. We
will focus on one approach, which is called the variational approach. There are other
ways of solving elliptic problems, such as working with Green functions as seen in
Chap.2. The variational approach is quite simple and well suited for a whole class
of approximation methods, as we will see later.
4.1
Model Boundary Value Problems
Let us start with a few more model problems. The simplest of all is a slight general-
ization of the Poisson equation with a homogeneous Dirichlet boundary condition.
Let us thus be given an open Lipschitz subset Ω of Rd, a function c ∈L∞(Ω) and
another function f ∈L2(Ω). We are looking for a function u: ¯Ω →R such that
−Δu + cu = f in Ω,
u = 0 on ∂Ω.
(4.1)
We are going to transform the boundary value problem (4.1) into an entirely different
kind of problem that is amenable to an existence and uniqueness theory, as well as
the deﬁnition of approximation methods.
Proposition 4.1 Assume that u ∈H2(Ω) solves the PDE in problem (4.1), i.e., the
ﬁrst equation in (4.1). Then, for all v ∈H1
0(Ω), we have

Ω
∇u · ∇v dx +

Ω
cuv dx =

Ω
f v dx.
(4.2)
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_4
117

118
4
The Variational Formulation of Elliptic PDEs
Proof We take an arbitrary v ∈H1
0(Ω), multiply the equation by v, which yields
−(Δu)v + cuv = f v,
and then integrate the result over Ω. Indeed, every term is integrable. First of all, u ∈
H2(Ω) hence Δu ∈L2(Ω), and v ∈L2(Ω) imply (Δu)v ∈L1(Ω). Moreover, c ∈
L∞(Ω), u ∈L2(Ω) and v ∈L2(Ω) imply cuv ∈L1(Ω). Finally, f ∈L2(Ω) implies
f v ∈L1(Ω). We thus obtain
−

Ω
(Δu)v dx +

Ω
cuv dx =

Ω
f v dx.
We now use Green’s formula (3.15), according to which

Ω
(Δu)v dx = −

Ω
∇u · ∇v dx +

∂Ω
γ1(u)γ0(v) dΓ,
and we conclude since v ∈H1
0(Ω) is equivalent to γ0(v) = 0.
□
Concerning the second equation in (4.1), i.e., the boundary condition, we have to
interpret it in the sense of traces in the Sobolev context. In fact, as we have seen in
the previous chapter, the reasonable way to impose the Dirichlet boundary condition
is to require that γ0(u) = 0, or in other words, that u ∈H1
0(Ω). The conjunction of
(4.2) with the requirement that u ∈H1
0(Ω) is called the variational formulation of
problem (4.1). The functions v are called test-functions.
Let us rewrite the variational formulation in a standard, abstract form. We let
V = H1
0(Ω), it is a Hilbert space. Then we have a bilinear form on V × V
a(u, v) =

Ω
(∇u · ∇v + cuv) dx
and a linear form on V
ℓ(v) =

Ω
f v dx.
The variational formulation then reads: Find u ∈V such that
∀v ∈V,
a(u, v) = ℓ(v),
(4.3)
and we have shown that a solution of the boundary value problem with the additional
regularity u ∈H2(Ω) is a solution of the variational problem (4.3).
Now what about the reverse implication? Does a solution of the variational prob-
lem solve the boundary value problem? The answer is basically yes, the two problems
are equivalent.

4.1 Model Boundary Value Problems
119
Proposition 4.2 Assume that u ∈H1
0(Ω) solves the variational problem (4.3). Then
we have
−Δu + cu = f in the sense o f D′(Ω).
Moreover Δu ∈L2(Ω) and the PDE is also satisﬁed almost everywhere on Ω.
Proof First of all, note that the variational formulation (4.2) makes sense for u ∈
H1
0(Ω). We have D(Ω) ⊂H1
0(Ω), therefore we can take v = ϕ ∈D(Ω) as test-
function in (4.3). Let us examine each term separately.
For the ﬁrst term, we have

Ω
∇u · ∇ϕ dx =

Ω
 d

i=1
∂iu∂iϕ

dx =
d

i=1

Ω
∂iu∂iϕ dx

=
d

i=1
⟨∂iu, ∂iϕ⟩=
d

i=1
−⟨∂iiu, ϕ⟩= −
 d

i=1
∂iiu, ϕ

= −⟨Δu, ϕ⟩,
by deﬁnition of distributional derivatives. Similarly

Ω
cuϕ dx = ⟨cu, ϕ⟩and

Ω
f ϕ dx = ⟨f, ϕ⟩.
Therefore, we have for all ϕ ∈D(Ω)
⟨−Δu + cu −f, ϕ⟩= 0
or
−Δu + cu −f = 0 in the sense of D′(Ω)
andthePDEissatisﬁedinthesenseofdistributions.TheDirichletboundarycondition
is also satisﬁed by the simple fact that u ∈H1
0(Ω), hence the boundary value problem
is solved.
To conclude, we note that Δu = cu −f ∈L2(Ω). This implies that the distrib-
ution Δu is an L2-function and thus that the PDE is satisﬁed almost everywhere in
Ω.
□
Remark 4.1 Note that the condition γ0(u) = 0 also means in a sense that u vanishes
almost everywhere on the boundary ∂Ω.
□
Remark 4.2 The two problems are thus equivalent, except for the fact that we have
assumedu ∈H2(Ω)inonedirection,andonlyrecuperatedΔu ∈L2(Ω)intheother.1
Actually, the assumption u ∈H2(Ω) is somewhat artiﬁcial and made only to make
1The Laplacian is a speciﬁc linear combination of some of the second order derivatives. So it being
in L2 is a priori less than all individual second order derivatives, even those not appearing in the
Laplacian, being in L2, except when d = 1.

120
4
The Variational Formulation of Elliptic PDEs
use of Green’s formula (3.15). It is possible to dispense with it with a little more
work, but that would take us too far.
It should be noted in any case, that if u ∈H1
0(Ω), Δu ∈L2(Ω) and Ω is for exam-
ple of class C2, then u ∈H2(Ω). This is very profound result in elliptic regularity
theory, far beyond the scope of these notes (see [15, 25, 35] for example). We will
come back to this point at the end of the chapter. It is trivial in dimension one though.
Of course, so far we have no indication that either problem has a solution. The
fact is that the variational formulation is signiﬁcantly easier to treat, once the right
point of view is found. And the right point of view is an abstract point of view, as is
often the case, more of this in Sect.4.2.
□
Before we start delving in the abstract, let us give a couple more model problems
of a different kind. First is the higher dimensional analogue of the Neumann boundary
condition already seen in one dimension in Chap.2:
	 −Δu + cu = f in Ω,
∂u
∂n = g on ∂Ω.
(4.4)
When g = 0, it is naturally called a homogeneous Neumann boundary condition.
In terms of modeling, the Neumann condition is a ﬂux condition. For instance, in
the heat equilibrium interpretation, the condition corresponds to an imposed heat
ﬂux through the boundary, as opposed to the Dirichlet condition which imposes a
given temperature on the boundary. The case g = 0 corresponds to perfect thermal
insulation: no heat is allowed to enter or leave Ω.
Let us derive the variational formulation informally. Assume ﬁrst that u ∈H2(Ω),
take v ∈H1(Ω), multiply, integrate and use Green’s formula to obtain
∀v ∈H1(Ω),

Ω
(∇u · ∇v + cuv) dx =

Ω
f v dx +

∂Ω
gγ0(v) dΓ.
Note the different test-function space and the additional boundary term in the right-
hand side.
The converse is more interesting. Let u ∈H2(Ω) be a solution of the above vari-
ational problem. Taking ﬁrst v = ϕ ∈D(Ω), we obtain
−Δu + cu = f in the sense of D′(Ω)
exactly as in the Dirichlet case. Of course, a test-function with compact support
does not see what happens on the boundary, and no information on the Neumann
condition is recovered. Thus, in a second step, we take v arbitrary in H1(Ω). By
Green’s formula again, we have

Ω
∇u · ∇v dx = −

Ω
(Δu)v dx +

∂Ω
γ1(u)γ0(v) dΓ.

4.1 Model Boundary Value Problems
121
Recall that the normal trace γ1(u) plays the role of the normal derivative. Since u is
a solution of the variational problem, it follows that

Ω
(−Δu + cu)v dx +

∂Ω
γ1(u)γ0(v) dΓ =

Ω
f v dx +

∂Ω
gγ0(v) dΓ.
But we already know that

Ω(−Δu + cu)v dx =

Ω f v dx by the previous step, hence
we are left with

∂Ω
γ1(u)γ0(v) dΓ =

∂Ω
gγ0(v) dΓ,
for all v ∈H1(Ω). For simplicity, we assume here that g ∈H1/2(∂Ω), the image
of the trace γ0, see Proposition 3.17 of Chap.3, and that Ω is smooth. Since u ∈
H2(Ω), it follows that γ1(u) = d
i=1 γ0(∂iu)ni ∈H1/2(∂Ω). Therefore, there exists
v ∈H1(Ω) such that γ0(v) = γ1(u) −g. With this choice of v, we obtain

∂Ω
(γ1(u) −g)2 dΓ = 0,
hence γ1(u) = g, which is the Neumann condition. The last hypotheses (u ∈H2(Ω)
and g ∈H1/2(∂Ω)) are made for brevity only. They are not at all necessary to con-
clude.
Another problem of interest is the non homogeneous Dirichlet problem.
−Δu + cu = f in Ω,
u = g on ∂Ω,
with g ∈H3/2(∂Ω).2 This problem is reduced to the homogeneous problem by taking
a function G ∈H2(Ω) such that γ0(G) = g and setting U = u −G. Then clearly
U ∈H1
0(Ω) and −ΔU + cU = −Δu + cu + ΔG −cG = f + ΔG −cG. Then we
just write the variational formulation of the homogeneous problem for U with right-
hand side F = f + ΔG −cG ∈L2(Ω). Note that it is also possible to solve the
problem under the more natural assumption g ∈H1/2(∂Ω).
The Dirichlet and Neumann conditions can be mixed together, but not at the same
place on the boundary, yielding the so-called mixed problem. More precisely, let Γ1
and Γ2 be two subsets of ∂Ω such that Γ1 ∩Γ2 = ∅, ¯Γ1 ∪¯Γ2 = ∂Ω. Then the mixed
problem reads
⎧
⎪⎨
⎪⎩
−Δu + cu = f in Ω,
u = g1 on Γ1,
∂u
∂n = g2 on Γ2.
(4.5)
2The space H3/2(∂Ω) is the space of traces of H2(Ω) functions.

122
4
The Variational Formulation of Elliptic PDEs
The variational formulation for the mixed problem (in the case g1 = 0 for brevity, if
not follow the above route) is to let V = {v ∈H1(Ω); γ0(v) = 0 on Γ1} and
∀v ∈V,

Ω
(∇u · ∇v + cuv) dx =

Ω
f v dx +

Γ2
g2γ0(v) dΓ,
with u ∈V . Note that the mixed problem reduces to the Neumann problem when
meas (Γ1) ̸= 0 and to the Dirichlet problem when meas (Γ2) ̸= 0.
Remark 4.3 An important rule of thumb to be remembered from the above examples
is that (homogeneous) Dirichlet conditions are taken into account in the test-function
space, whereas Neumann boundary conditions are taken into account in the linear
form via boundary integrals and a larger test-function space.
4.2
Abstract Variational Problems
We now describe the general abstract framework for all variational problems. We
have just seen that some boundary value problems can be recast in the following
form. We are given a Hilbert space V (in the examples we have seen before H1
0(Ω)
or H1(Ω)), a bilinear form a on V × V and a linear form ℓon V . The solution of
the boundary value problem is then a solution of problem (4.3). At this point, we
completely abstract the boundary value problem aspect.
Deﬁnition 4.1 An abstract variational problem consists in ﬁnding u ∈V such that
∀v ∈V,
a(u, v) = ℓ(v),
(4.6)
where V is a Hilbert space, a is a bilinear form on V × V and ℓis a linear form
on V .
The basic tool for solving abstract variational problems is the Lax–Milgram the-
orem [55]. This theorem is important, not because it is in any way difﬁcult, which it
is not, but because it has a very wide range of applicability as we will see later.
Theorem 4.1 (Lax–Milgram) Let V be a Hilbert space, a be a bilinear form and ℓ
be a linear form. Assume that
(i) The bilinear form a is continuous, i.e., there exists a constant M such that
|a(u, v)| ≤M∥u∥V ∥v∥V for all u, v ∈V ,
(ii) The bilinear form a is V -elliptic3, i.e., there exists a constant α > 0 such that
a(v, v) ≥α∥v∥2
V for all v ∈V ,
(iii) The linear form ℓis continuous, i.e., there exists a constant C such that
|ℓ(v)| ≤C∥v∥V for all v ∈V .
3This condition is also sometimes called coerciveness.

4.2 Abstract Variational Problems
123
Under the above assumptions, there exists a unique u ∈V that solves the abstract
variational problem (4.6).
Proof Let us start with the uniqueness. Let u1 and u2 be two solutions of prob-
lem (4.6). Since a is linear with respect to its ﬁrst argument, it follows that
a(u1 −u2, v) = 0 for all v ∈V . In particular, for v = u1 −u2, we obtain
0 = a(u1 −u2, u1 −u2) ≥α∥u1 −u2∥2
V ,
so that ∥u1 −u2∥V = 0 since α > 0.
We next prove the existence of a solution. We ﬁrst note that for all u ∈V , the
mapping v →a(u, v) is linear (by bilinearity of a) and continuous (by i) continuity of
a). Therefore, there exists a unique element Au of V ′ such that a(u, v) = ⟨Au, v⟩V ′,V .
Moreover, the bilinearity of a shows that the mapping A: V →V ′ thus deﬁned is
linear. It is also continuous since for all v ∈V with ∥v∥V ≤1,
|⟨Au, v⟩V ′,V | = |a(u, v)| ≤M∥u∥V ∥v∥V ≤M∥u∥V
so that
∥Au∥V ′ = sup
∥v∥V ≤1
|⟨Au, v⟩V ′,V | ≤M∥u∥V .
We rewrite the variational problem as: Find u ∈V such that
∀v ∈V,
⟨Au −ℓ, v⟩V ′,V = 0
or
Au = ℓ,
and this is where the continuity of ℓis used.
Thus, proving the existence is equivalent to showing that the mapping A is onto.4
We do this in two independent steps: we show that im A is closed on the one hand
and that it is dense on the other hand.5 For the closedness of the image, we use
assumption (ii) of V -ellipticity. Let ℓn be a sequence in im A such that ℓn →ℓin V ′.
We want to show that ℓ∈im A, which will imply that im A is closed. The sequence
ℓn is a Cauchy sequence in V ′, and for all n, there exists un ∈V such that Aun = ℓn.
By V -ellipticity,
∥un −um∥2
V ≤1
α a(un −um, un −um) = 1
α ⟨Aun −Aum, un −um⟩V ′,V
= 1
α ⟨ℓn −ℓm, un −um⟩V ′,V ≤1
α ∥ℓn −ℓm∥V ′∥un −um∥V ,
4Since we already know it is one-to-one, it will then be an isomorphism.
5This is a pretty common strategy, to be kept in mind.

124
4
The Variational Formulation of Elliptic PDEs
by the deﬁnition of the dual norm. Therefore, if ∥un −um∥V = 0 we are happy,
otherwise we divide by ∥un −um∥V and in both cases
∥un −um∥V ≤1
α ∥ℓn −ℓm∥V ′,
so that un is a Cauchy sequence in V . Since V is complete, there exists u ∈V such
that un →u in V . Since A is continuous, it follows that ℓn = Aun →Au in V ′. Hence
ℓ= Au ∈im A.
To show the density, we show that (im A)⊥= {0} (according to Lemma 3.1 of
Chap.3). We note that (Au|ℓ)V ′ = (σAu|σℓ)V = ⟨Au, σℓ⟩V ′,V = a(u, σℓ), where σ
is the Riesz isomorphism. Let ℓ∈(im A)⊥. For all u ∈V , we thus have a(u, σℓ) = 0.
In particular, for u = σℓ, we obtain 0 = a(σℓ, σℓ) ≥α∥σℓ∥2
V by V -ellipticity. Since
α > 0, it follows that ℓ= 0.
□
Remark 4.4 There is another classical proof of the Lax–Milgram theorem using the
Banach ﬁxed point theorem [15, 19, 25]. Note that if V is separable, there is yet
another proof based on the Galerkin method which can be generalized to nonlinear
variational problems [57]. Finally, there is a generalization of the Lax–Milgram
theorem, known as the Stampacchia theorem, [15], which is used to solve variational
inequalities, see for example [21].
Remark 4.5 It should be noted that the Lax–Milgram theorem is not a particular
case of the Riesz theorem. It is actually more general, since it applies to bilinear
forms that are not necessarily symmetric, and it implies the Riesz theorem when the
bilinear form is just the scalar product.
Sometimes when the bilinear form a is symmetric, people think it advantageous
to apply Riesz’s theorem in place of the Lax–Milgram theorem. This is usually an
illusion: indeed, if a new scalar product deﬁned by the bilinear form is introduced,
in order to apply Riesz’s theorem, it is necessary to show that the space equipped
with the new scalar product is still a Hilbert space, i.e., is complete. This is done
by V -ellipticity, hence nothing is gained (although this is the part that people who
think they are seeing a good deal usually forget). The continuity of the linear form
for the new norm must also be checked, which amounts to having the bilinear form
and linear form continuous for the original norm, again, no gain.
The only case when Riesz’s theorem can be deemed advantageous over the Lax–
Milgram theorem, is when both above facts to be checked are already known. An
example is the bilinear form a(u, v) =

Ω ∇u · ∇v dx on V = H1
0(Ω).
□
Remark 4.6 In the case of complex Hilbert spaces and complex-valued variational
problems, the Lax–Milgram theorem still holds true for a bilinear or sesquilinear
form. The V -ellipticity assumption can even be relaxed to only involve the real part
of a, i.e., Re (a(u, u)) ≥α∥u∥2 (or the imaginary part), which is rather useful as the
imaginary part can then be pretty arbitrary (exercise).
□
Remark 4.7 Let us emphasize again that the Lax–Milgram theorem only gives sufﬁ-
cient conditions for existence and uniqueness of the solution of an abstract variational

4.2 Abstract Variational Problems
125
problem. More speciﬁcally, V -ellipticity is not necessary. Indeed, when V is ﬁnite
dimensional, V -ellipticity is just the positive deﬁniteness of the operator A (identi-
fying V and V ′ without second thoughts). Obviously, there are more isomorphisms
in L (V ) than just positive deﬁnite linear mappings.
Replacing V -ellipticity with the following inf-sup condition:
inf
u∈V \{0}
sup
v∈V \{0}
a(u, v)
∥u∥V ∥v∥V
> 0,
combined with the requirement that if v is such that a(u, v) = 0 for all u ∈V , then
v = 0, we obtain a set of necessary and sufﬁcient conditions, as is easily seen along
the same lines as before [7, 41]. Both conditions are clearly implied by V -ellipticity.
As a rule, elliptic problems are usually amenable to the Lax–Milgram theorem.
□
The linear form in the right-hand side of a variational problem should be thought
of as data. In this respect, the solution depends continuously on the data.
Proposition 4.3 The mapping V ′ →V , ℓ→u deﬁned by the Lax–Milgram theo-
rem is linear and continuous.
Proof The operator A is linear and invertible, therefore so is A−1. The continuity of
A−1 stems from Banach’s theorem, see [15]. We actually have a more precise result
since
α∥u∥2
V ≤a(u, u) = ℓ(u) ≤∥ℓ∥V ′∥u∥V ,
hence
∥u∥V ≤1
α ∥ℓ∥V ′,
which shows that the continuity constant of A−1 is smaller than the inverse of the
V -ellipticity constant of a.
□
Proposition 4.4 Let the hypotheses of the Lax–Milgram theorem be satisﬁed.
Assume in addition that the bilinear form a is symmetric. Then the solution u of
the variational problem (4.6) is also the unique solution of the minimization prob-
lem:
J(u) = inf
v∈V J(v) with
J(v) = 1
2a(v, v) −ℓ(v).
Proof Let u be the Lax–Milgram solution. For all v ∈V , we let w = v −u and
J(v) = J(u + w) = 1
2a(u, u) + 1
2a(u, w) + 1
2a(w, u) + 1
2a(w, w) −ℓ(u) −ℓ(w)
= J(u) + a(u, w) −ℓ(w) + 1
2a(w, w)
≥J(u),

126
4
The Variational Formulation of Elliptic PDEs
since a(w, w) ≥0. Make note of where the symmetry is used. Hence, u minimizes
J on V .
Conversely, assume that u minimizes J on V . Then, for all λ > 0 and all v ∈V ,
we have J(u + λv) ≥J(u). Expanding the left-hand side, we get
1
2a(u, u) + λa(u, v) + λ2
2 a(v, v) −ℓ(u) −λℓ(v) ≥J(u)
so that dividing by λ
a(u, v) −l(v) + λ
2a(v, v) ≥0.
We then let λ →0, hence
a(u, v) −l(v) ≥0,
and ﬁnally change v in −v to obtain
a(u, v) −l(v) = 0,
for all v ∈V .
□
Remark 4.8 Taking λ > 0, dividing by λ and then letting λ →0 is quite clever, and
known as Minty’s trick.
□
Remark 4.9 When the bilinear form a is not symmetric, we can still deﬁne the
functional J in the same fashion as before and try to minimize it. It is clear from the
above proof that the minimizing element u does not solve the variational problem
associated with a but the variational problem associated with the symmetric part of a.
Note that u exists because we can apply the Lax–Milgram theorem to the symmetric
part of the bilinear form a. Of course, when both variational problems are translated
into PDEs, we get entirely different equations.
□
4.3
Application to the Model Problems, and More
Here again, Ω is a Lipschitz open subset of Rd. We now apply the previous abstract
results to concrete examples. We start with the ﬁrst model problem (4.1).
Proposition 4.5 Let f ∈L2(Ω), c ∈L∞(Ω). Assume that c ≥0. Then the problem:
Find u ∈V = H1
0(Ω) such that
∀v ∈V,

Ω
(∇u · ∇v + cuv) dx =

Ω
f v dx,
has one and only one solution.

4.3 Application to the Model Problems, and More
127
Proof We already know that V is a Hilbert space, for both scalar products that we
deﬁned earlier. Of course
a(u, v) =

Ω
(∇u · ∇v + cuv) dx
clearly deﬁnes a bilinear form on V × V and
ℓ(v) =

Ω
f v dx
a linear form on V . Hence, we have an abstract variational problem. Let us try and
apply the Lax–Milgram theorem. We need to check the theorem hypotheses. For
deﬁniteness, we choose to work with the full H1 norm.
First of all, for all (u, v) ∈V × V ,
|a(u, v)| =


Ω
(∇u · ∇v + cuv) dx

≤

Ω
|∇u · ∇v + cuv| dx
≤

Ω
|∇u · ∇v| dx +

Ω
|cuv| dx
≤∥∇u∥L2(Ω)∥∇v∥L2(Ω) + ∥c∥L∞(Ω)∥u∥L2(Ω)∥v∥L2(Ω)
≤max

1, ∥c∥L∞(Ω)

∥u∥H1(Ω)∥v∥H1(Ω),
by the Cauchy–Schwarz inequality to go from the third line to the fourth line, and
again the Cauchy–Schwarz inequality in R2 to go from the fourth line to the ﬁfth
line, hence the continuity of the bilinear form a.
Next is the V -ellipticity. For all v ∈V , we have
a(v, v) =

Ω
(∥∇v∥2 + cv2) dx ≥

Ω
∥∇v∥2 dx ≥α∥v∥2
H1(Ω)
with α = (C2 + 1)−1/2 > 0 by Corollary 3.3 of Chap.3, where C is the Poincaré
inequality constant, and since c ≥0.
Finally, we check the continuity of the linear form. For all v ∈V ,
|ℓ(v)| =


Ω
f v dx
 ≤∥f ∥L2(Ω)∥v∥L2(Ω) ≤∥f ∥L2(Ω)∥v∥H1(Ω)
by the Cauchy–Schwarz inequality again.
All the hypotheses of the Lax–Milgram theorem are satisﬁed, therefore there is
one and only one solution u ∈V .
□

128
4
The Variational Formulation of Elliptic PDEs
Remark 4.10 Now is a time to celebrate since we have successfully solved our ﬁrst
boundary value problem in arbitrary dimension. Indeed, we have already seen that
any solution of the variational problem is a solution of the PDE in the distributional
sense and in the L2 sense. The solution u depends continuously in H1 on f in L2.
Note that we have also solved the non homogeneous Dirichlet problem at the same
time.
It is an instructive exercise to redo the proof using the H1 semi-norm in place of
the full norm. The same ingredients are used, but not at the same spots.
This is a case of a symmetric bilinear form, therefore the solution u also minimizes
the so-called energy functional
J(v) = 1
2

Ω
(∥∇v∥2 + cv2) dx −

Ω
f v dx
over V .
□
Remark 4.11 It should be noted that the positivity condition c ≥0 is by no means
a necessary condition for existence and uniqueness via the Lax–Milgram theorem.
With a little more work, it is not too hard to allow the function c to take some negative
values. However, we have seen an example at the very beginning of Chap.1 with a
negative function c for which existence and uniqueness fails.
One should also be aware that there is an existence and uniqueness theory that
goes beyond the Lax–Milgram theorem, which only gives a sufﬁcient condition for
existence and uniqueness.
□
Let us now consider the non homogeneous Neumann problem (4.4). The hypothe-
ses are slightly different.
Proposition 4.6 Let f ∈L2(Ω), c ∈L∞(Ω), g ∈L2(∂Ω). Assume that there exists
a constant c0 > 0 such that c ≥c0 almost everywhere. Then the problem: Find u ∈
V = H1(Ω) such that
∀v ∈V,

Ω
(∇u · ∇v + cuv) dx =

Ω
f v dx +

∂Ω
gγ0(v) dx,
has one and only one solution.
Proof We have a different Hilbert space (but known to be Hilbert, nothing to check
here), the same bilinear form and a different linear form
ℓ(v) =

Ω
f v dx +

∂Ω
gγ0(v) dx.

4.3 Application to the Model Problems, and More
129
We have already shown that the bilinear form is continuous in the H1 norm.6
The V -ellipticity is clear since, for all v ∈V ,
a(v, v) =

Ω
(∥∇v∥2 + cv2) dx ≥

Ω
∥∇v∥2 dx + c0

Ω
v2 dx ≥min(1, c0)∥v∥2
H1(Ω),
with min(1, c0) > 0. The continuity of the linear form is also clear
|ℓ(v)| ≤∥f ∥L2(Ω)∥v∥L2(Ω)+∥g∥L2(∂Ω)∥γ0(v)∥L2(∂Ω)
≤(∥f ∥L2(Ω) + Cγ0∥g∥L2(∂Ω))∥v∥H1(Ω)
by the Cauchy–Schwarz inequality, where Cγ0 is continuity constant of the trace
mapping.
□
The mixed problem (4.5) is a nice mixture of the Dirichlet and the Neumann
problems.
Proposition 4.7 Same hypotheses as in Proposition 4.6 and let Γ1 and Γ2 be two
subsets of ∂Ω such that Γ1 ∩Γ2 = ∅, ¯Γ1 ∪¯Γ2 = ∂Ω and meas(Γ1) ̸= 0. Then the
problem: Find u ∈V = {v ∈H1(Ω); γ0(v) = 0 on Γ1} such that
∀v ∈V,

Ω
(∇u · ∇v + cuv) dx =

Ω
f v dx +

Γ2
gγ0(v) dΓ,
has one and only one solution.
Proof The only real difference with Proposition 4.6 lies with the space V , which we
do not know yet to be a Hilbert space. It sufﬁces to show that V is a closed subspace
of H1(Ω). Let vn be a sequence in V such that vn →v in H1(Ω). By continuity
of the trace mapping, we have γ0(vn) →γ0(v) in L2(∂Ω). Therefore, there exists
a subsequence γ0(vnp) that converges to γ0(v) almost everywhere on ∂Ω. Since
γ0(vn) = 0 almost everywhere on Γ1, it follows that γ0(v) = 0 almost everywhere
on Γ1, hence v ∈V , which is thus closed.
□
Proposition 4.7 also holds true under the less demanding hypothesis c ≥0, using
a different argument for V -ellipticity.
A natural question arises about the Neumann problem for c = 0, see Chap.2,
Sect.2.4 in one dimension. Now, this is an entirely different problem from the pre-
vious ones. First we have to ﬁnd the variational formulation of the boundary value
problem and show that it is equivalent to the boundary value problem, then we have
to apply the Lax–Milgram theorem.
Let us thus consider the Neumann problem
−Δu = f in Ω,
∂u
∂n = g on ∂Ω,
(4.7)
6If we had worked with the semi-norm for the Dirichlet problem, we would have had to do the
continuity all over again here…

130
4
The Variational Formulation of Elliptic PDEs
in a Lipschitz open set Ω in Rd. We see right away that things are going to be different
since we do not have uniqueness here. Indeed, if u is a solution, then u + s is also
a solution for any constant s. Furthermore, by Green’s formula (3.15) with v = 1, it
follows that if there is a solution, then, necessarily

Ω
f dx +

∂Ω
g dΓ = 0.
(4.8)
If the data f, g does not satisfy the compatibility condition (4.8), there is thus no
solution. The two remarks, non uniqueness and non existence, are actually dual to
each other.
There are several ways of going around both problems, thus several variational
formulations.7 We choose to set
V =

v ∈H1(Ω);

Ω
v dx = 0

.
(4.9)
This is well deﬁned, since Ω is bounded and we thus have H1(Ω) ⊂L2(Ω) ⊂
L1(Ω). Note that V is the L2-orthogonal in H1(Ω), which also happens to be the
H1-orthogonal in this case, to the one-dimensional space of constant functions. Note
that these functions are precisely the cause of non uniqueness.
Lemma 4.1 The space V is a Hilbert space for the scalar product of H1(Ω).
Proof It sufﬁces to show that V is closed. Let vn be a sequence in V such that vn →v
in H1(Ω). Of course, vn →v in L2(Ω) and by the Cauchy–Schwarz inequality,
vn →v in L1(Ω). Therefore
0 =

Ω
vn dx →

Ω
v dx,
and v ∈V .
□
We introduce the bilinear form a deﬁned on V × V by a(u, v) =

Ω ∇u · ∇v dx
and the linear form ℓdeﬁned on V by ℓ(v) =

Ω f v dx +

∂Ω gγ0(v) dΓ .
Proposition 4.8 Assume that f ∈L2(Ω), g ∈L2(∂Ω) satisfy the compatibility con-
dition(4.8).Then,anysolutionu ∈H2(Ω)oftheNeumannproblem(4.7)isasolution
of the variational problem deﬁned by the triple (V, a, ℓ). Conversely, any solution
u ∈H2(Ω) of the variational problem is a solution of problem (4.7).
Proof Multiplying the PDE by v ∈V and using Green’s formula, we easily see that
if u ∈H2(Ω) solves problem (4.7), then we have for all v ∈V , a(u, v) = ℓ(v).
Conversely, let us be given a function u ∈V ∩H2(Ω) such that for all v ∈V ,
a(u, v) = ℓ(v). We would like to proceed as before and take v ∈D(Ω) to derive the
PDE. This does not work here because D(Ω) ̸⊂V . For all ϕ ∈D(Ω), we set
7We have always said the variational formulation, but there is no evidence that it is unique in general.

4.3 Application to the Model Problems, and More
131
ψ = ϕ −
1
meas Ω

Ω
ϕ(x) dx,
so that ψ ∈V and we can use ψ as a test-function. Now ϕ and ψ differ by a constant
k =
1
meas Ω

Ω ϕ(x) dx, therefore ∇ψ = ∇ϕ. We thus obtain,

Ω
∇u · ∇ϕ dx =

Ω
∇u · ∇ψ dx =

Ω
f ψ dx +

∂Ω
gψ dΓ
=

Ω
f (ϕ + k) dx +

∂Ω
g(ϕ + k) dΓ
=

Ω
f ϕ dx + k

Ω
f dx +

∂Ω
g dΓ

=

Ω
f ϕ dx,
since ϕ vanishes on ∂Ω and f, g satisfy condition (4.8). So we can deduce right
away that −Δu = f in the sense of distributions, and since f ∈L2(Ω) in the sense
of L2(Ω) as well.
We next pick an arbitrary v ∈V and apply Green’s formula again. This yields

Ω
f v dx +

∂Ω
gγ0(v) dΓ = −

Ω
(Δu)v dx +

∂Ω
γ1(u)γ0(v) dΓ.
Hence, taking into account that −Δu = f , we obtain

∂Ω
(g −γ1(u))γ0(v) dΓ = 0
for all v ∈V . Now it is clear that γ0(V ) = H1/2(∂Ω). Indeed, let us pick a θ ∈
D(Ω) such that

Ω θ dx = 1. Then, for all w ∈H1(Ω), v = w −

Ω w dx

θ ∈V
and γ0(v) = γ0(w). Therefore, there are enough test-functions in V to conclude that
γ1(u) = g, since H1/2(∂Ω) is dense in L2(∂Ω).
□
Remark 4.12 It is possible to establish a variational formulation of the Neumann
problem without the artiﬁcial hypothesis u ∈H2(Ω).
□
To apply the Lax–Milgram theorem, we need a new inequality.
Theorem 4.2 (Poincaré–Wirtinger inequality) Assume that Ω is connected. There
exists a constant C such that, for all v ∈H1(Ω),
v −
1
meas Ω

Ω
v dx

L2(Ω) ≤C∥∇v∥L2(Ω).
(4.10)
Proof We use a contradiction argument. Assume that there is no such constant C.
For all n ∈N∗, we can thus ﬁnd vn ∈H1(Ω) such that
vn −
1
meas Ω

Ω
vn dx

L2(Ω) > n∥∇vn∥L2(Ω).

132
4
The Variational Formulation of Elliptic PDEs
In particular, the left-hand side is strictly positive. We let
wn =
vn −
1
meas Ω

Ω vn dx
vn −
1
meas Ω

Ω vn dx

L2(Ω)
.
By construction, we have ∥wn∥L2(Ω) = 1 and wn belongs to the L2-orthogonal of the
one-dimensional space of constant functions, which is closed in L2(Ω).
Moreover, ∇wn = ∇vn and we have
∥∇wn∥L2(Ω) < 1
n →0 when n →+∞.
In particular, the sequence wn is bounded in H1(Ω). By the Rellich theorem, see
Remark 3.18 of Chap.3, it is relatively compact in L2(Ω). We may thus ﬁnd a
subsequence wnp and an element w ∈L2(Ω) such that wnp →w strongly in L2(Ω)
when p →+∞.
On the one hand we have ∥w∥L2(Ω) = 1 and w also belongs to the L2-orthogonal
of the space of constant functions.
On the other hand, ∇wnp →0 strongly in L2(Ω), hence in the sense of distribu-
tions, so that ∇w = 0. As Ω is connected, this implies that w belongs to the space
of constant functions.
We thus see that w belongs to the intersection of one subspace and its orthogonal,
so that w = 0. This contradicts ∥w∥L2(Ω) = 1.
□
Remark 4.13 Even though there is a certain formal similarity with the Poincaré
inequality, there are major differences. In particular, the Poincaré–Wirtinger inequal-
ity fails for open sets that are not regular enough whereas no regularity is needed for
the Poincaré inequality. Note that the contradiction argument above is not construc-
tive. It gives no indication about the actual value of C, as opposed to the proof of the
Poincaré inequality given earlier.
□
Proposition 4.9 Assume that Ω is connected, f ∈L2(Ω) and g ∈L2(∂Ω). Then
the problem: Find u ∈V , V given by (4.9), such that
∀v ∈V,

Ω
∇u · ∇v dx =

Ω
f v dx +

∂Ω
gγ0(v) dΓ,
has one and only one solution.
Proof We have already shown that V is a Hilbert space for the H1 scalar product.
The continuity of both bilinear and linear forms have also already been proved. Only
the V -ellipticity remains.
For all v ∈V , we have

Ω v dx = 0, hence by the Poincaré–Wirtinger inequality
(4.10),
∥v∥2
H1(Ω) = ∥v∥2
L2(Ω) + ∥∇v∥2
L2(Ω) ≤(C2 + 1)∥∇v∥2
L2(Ω).

4.3 Application to the Model Problems, and More
133
Therefore,
a(v, v) = ∥∇v∥2
L2(Ω) ≥α∥v∥2
H1(Ω)
with α =
1
(C2+1) > 0.
□
Remark 4.14 The compatibility condition (4.8) plays no role in the application of
the Lax–Milgram theorem. So exercise: What happens when it is not satisﬁed? What
exactly are we solving then?
□
Remark 4.15 Since the space V is a hyperplane of H1 that is L2 orthogonal to the
constants, it follows that the general solution of the Neumann problem is of the form
v + s, where v ∈V is the unique solution of the variational problem above and s ∈R
is arbitrary.
□
We now introduce a new kind of boundary condition, the Fourier condition (also
called the Robin condition or the third boundary condition). The boundary value
problem reads
	 −Δu + cu = f in Ω,
bu + ∂u
∂n = g on ∂Ω,
(4.11)
where b and c are given functions. When b = 0, we recognize the Neumann problem
(and, in a sense, when b = +∞the Dirichlet problem). This condition is called after
Fourier who introduced it in the context of the heat equation, see Chap.1, Sect.1.7.
In the heat interpretation, ∂u
∂n represents the heat ﬂux through the boundary. Let us
assume that we are modeling a situation in which the boundary is actually a very
thin wall that insulates Ω from the outside where the temperature is 0◦. If g = 0,
the Fourier condition states that ∂u
∂n = −bu, that is to say that the heat ﬂux passing
through the wall is proportional to the temperature difference between the inside and
the outside. For this interpretation to be physically reasonable, it is clearly necessary
that b ≥0, i.e., the heat ﬂows inwards when the outside is warmer than the inside
and conversely. It thus to be expected that the sign of b will play a role.
We follow the same pattern as before: First ﬁnd a variational formulation for the
boundary value problem (4.11), second apply the Lax–Milgram theorem to prove
existence and uniqueness. We introduce the triple
V = H1(Ω),
a(u, v) =

Ω
(∇u · ∇v + cuv) dx +

∂Ω
bγ0(u)γ0(v) dΓ,
ℓ(v) =

Ω
f v dx +

∂Ω
gγ0(v) dΓ.
Proposition 4.10 Assume that we have f ∈L2(Ω), g ∈L2(∂Ω), c ∈L∞(Ω) and
b ∈L∞(∂Ω). Then, any solution u ∈H2(Ω) of the Fourier problem (4.11) is a
solution of the variational problem deﬁned by the triple (V, a, ℓ). Conversely, any
solution u ∈H2(Ω) of the variational problem is a solution of problem (4.11).

134
4
The Variational Formulation of Elliptic PDEs
Proof As always, we multiply the PDE by v ∈V and use Green’s formula,

Ω
(∇u · ∇v + cuv) dx =

Ω
f v dx +

∂Ω
γ1(u)γ0(v) dΓ
=

Ω
f v dx +

∂Ω
(g −bγ0(u))γ0(v) dΓ,
hence

Ω
(∇u · ∇v + cuv) dx +

∂Ω
bγ0(u)γ0(v) dΓ =

Ω
f v dx +

∂Ω
gγ0(v) dΓ,
(4.12)
for all v ∈V .
Conversely, let us be given a solution u ∈H2(Ω) of the variational problem (4.12).
Taking ﬁrst v = ϕ ∈D(Ω), all the boundary integrals vanish and we obtain −Δu +
cu = f exactly as before. Taking then v ∈H1(Ω) arbitrary, using Green’s formula
and the PDE just obtained, we get

∂Ω
γ1(u)γ0(v) dΓ +

∂Ω
bγ0(u)γ0(v) dΓ =

∂Ω
gγ0(v) dΓ,
so that

∂Ω
(γ1(u) + bγ0(u) −g)γ0(v) dΓ = 0,
for all v ∈V = H1(Ω), hence the Fourier boundary condition.
□
Remark 4.16 A natural question to ask is why not keep the term γ1(u) in the bilinear
form? The answer is that, while it is true that γ1(u) exists when u ∈H2(Ω) is a
solution of either the boundary value problem or the variational problem, it does not
exist for a general v ∈H1(Ω), hence cannot appear in a bilinear form that is deﬁned
on H1(Ω) × H1(Ω). Besides, how would b appear otherwise?
□
Let us give a ﬁrst existence and uniqueness result.
Proposition 4.11 Let
f ∈L2(Ω), g ∈L2(∂Ω), c ∈L∞(Ω) and b ∈L∞(∂Ω).
Assume that c ≥c0 > 0 for some constant c0 and that ∥b−∥L∞(∂Ω) < min(1,c0)
C2γ0
,
where Cγ0 is the continuity constant of the trace mapping. Then the problem: Find
u ∈V = H1(Ω) such that
∀v ∈V,

Ω
(∇u · ∇v + cuv) dx +

∂Ω
bγ0(u)γ0(v) dΓ =

Ω
f v dx +

∂Ω
gγ0(v) dΓ,
has one and only one solution.
Here b−= −min(0, b) denotes the negative part of b.

4.3 Application to the Model Problems, and More
135
Proof We check the hypotheses of the Lax–Milgram theorem. We already know that
V is a Hilbert space. The continuity of the bilinear form a has also already been
checked, except for the boundary integral terms


∂Ω
bγ0(u)γ0(v) dΓ
 ≤∥b∥L∞(∂Ω)∥γ0(u)∥L2(∂Ω)∥γ0(v)∥L2(∂Ω)
≤C2
γ0∥b∥L∞(∂Ω)∥u∥H1(Ω)∥v∥H1(Ω)
for all u and v. The linear form is also known to be continuous. Let us check the
V -ellipticity. Obviously b ≥−b−, thus

Ω
(∥∇v∥2 + cv2) dx +

∂Ω
bγ0(v)2 dΓ
≥min(1, c0)∥v∥2
H1(Ω) −∥b−∥L∞(∂Ω)∥γ0(v)∥2
L2(∂Ω)
≥

min(1, c0) −C2
γ0∥b−∥L∞(∂Ω)

∥v∥2
H1(Ω),
hence the V -ellipticity.
□
Remark 4.17 Under the previous hypotheses, we have existence and uniqueness via
the Lax–Milgram theorem provided b is not too negative in some sense.
□
All these hypotheses only give sufﬁcient conditions. Let us give another set of
such hypotheses.
Proposition 4.12 Same hypotheses except that we assume that c ≥0 and that b ≥
μ > 0 for some constant μ. Then the Fourier problem (4.11) has one and only one
solution.
Proof The only point to be established is V -ellipticity. We use a compactness argu-
ment by contradiction based on Rellich’s theorem, see Remark 3.18 of Chap.3 again.
We have

Ω
(∥∇v∥2 + cv2) dx +

∂Ω
bγ0(v)2 dΓ ≥

Ω
∥∇v∥2 dx + μ

∂Ω
γ0(v)2 dΓ.
Let us assume for contradiction that there is no constant α > 0 such that

Ω
∥∇v∥2 dx + μ

∂Ω
γ0(v)2 dΓ ≥α∥v∥2
H1(Ω).
This implies that for all n ∈N∗, there exists vn ∈H1(Ω) such that

Ω
∥∇vn∥2 dx + μ

∂Ω
γ0(vn)2 dΓ < 1
n∥vn∥2
H1(Ω).

136
4
The Variational Formulation of Elliptic PDEs
We can assume without loss of generality that
∥vn∥2
H1(Ω) = 1,
(4.13)
and that we have

Ω
∥∇vn∥2 dx + μ

∂Ω
γ0(vn)2 dΓ →0.
(4.14)
Now vn is bounded in H1(Ω) by (4.13), thus relatively compact in L2(Ω) by Rellich’s
theorem. We may extract a subsequence, still denoted vn, and v ∈L2(Ω) such that
vn →v in L2(Ω). By (4.14), ∥∇vn∥L2(Ω) →0, therefore, since ∇vn →∇v in D′(Ω),
we have ∇v = 0 and v is constant on each connected component of Ω. Therefore
v ∈H1(Ω) and
∥vn −v∥2
H1(Ω) = ∥∇vn∥2
L2(Ω) + ∥vn −v∥2
L2(Ω) →0
(4.15)
so that, by continuity of the trace mapping γ0(vn) →γ0(v) in L2(∂Ω). By (4.14)
again, we also have ∥γ0(vn)∥L2(∂Ω) →0 since μ > 0 and therefore γ0(v) = 0. It fol-
lows that v being a constant with zero trace, it vanishes in each connected component,
i.e., v = 0. We now realize that (4.13) and (4.15) contradict each other, therefore our
premise that there exists no V -ellipticity constant α is false.
□
Remark 4.18 As for the proof of the Poincaré–Wirtinger inequality, this is a typical
compactness-contradiction argument: we can prove that the constant exists but we
have no idea of its value.
□
4.4
General Second Order Elliptic Problems
Up to now, the partial differential operator always was the Laplacian. Let us rapidly
consider more general second order elliptic operators in a Lipschitz open subset Ω of
Rd. We are given a d × d matrix-valued function A(x) = (ai j(x)) with ai j ∈C1( ¯Ω).
Let u ∈C2(Ω) (we can lower this regularity considerably), then A∇u is a vector
ﬁeld with components
(A∇u)i =
d

j=1
ai j∂ju
whose divergence is given by
div (A∇u) =
d

i=1
∂i(A∇u)i
=
d

i, j=1
ai j∂i ju +
d

j=1
 d

i=1
∂iai j

∂ju.

4.4 General Second Order Elliptic Problems
137
The principal part of this operator d
i, j=1 ai j∂i j is of the second order. We will
consider the boundary value problem
⎧
⎨
⎩
−div (A∇u) + cu = f in Ω,
u = h on Γ0,
bu + n · A∇u = g on Γ1,
(4.16)
where c, b, f , g and h are given functions and Γ0, Γ1 a partition of ∂Ω as in the
mixed problem. When A = I, we recognize −div (A∇u) = −Δu and n · A∇u = ∂u
∂n,
so that we are generalizing all the model problems seen up to now. First of all, we
reduce the study to the case h = 0 by subtracting a function with the appropriate
trace, as before.
Proposition 4.13 Assumethat f ∈L2(Ω),g ∈L2(Γ1),c ∈L∞(Ω)andb ∈L∞(Γ1).
Then the triple
V = {v ∈H1(Ω); γ0(v) = 0 on Γ1},
a(u, v) =

Ω
(A∇u · ∇v + cuv) dx +

Γ1
bγ0(u)γ0(v) dΓ,
ℓ(v) =

Ω
f v dx +

Γ1
gγ0(v) dΓ,
deﬁnes a variational formulation for problem (4.16), at least for H2(Ω) solutions.
Proof The proof is routine, but we partially write it down for completeness. It is easy
to check that (A∇u)i ∈H1(Ω) for all i. We multiply the PDE by v ∈V and integrate
by parts. This yields ﬁrst
−

Ω
 d

i=1
∂i(A∇u)i

v dx +

Ω
cuv dx =

Ω
f v dx,
then

Ω
d

i=1
(A∇u)i∂iv dx −

Γ1
 d

i=1
γ0

(A∇u)i

ni

γ0(v) dΓ +

Ω
cuv dx =

Ω
f v dx,
and ﬁnally

Ω
(A∇u · ∇v + cuv) dx +

Γ1
bγ0(u)γ0(v) dΓ =

Ω
f v dx +

Γ1
gγ0(v) dΓ.
We leave the converse argument to the reader.
□

138
4
The Variational Formulation of Elliptic PDEs
Proposition 4.14 Let f ∈L2(Ω), g ∈L2(Γ1), c ∈L∞(Ω) and b ∈L∞(Γ1). We
assume that the matrix A is uniformly elliptic, that is to say that there exists a
constant α > 0 such that
d

i, j=1
ai j(x)ξiξ j ≥α∥ξ∥2
for all x ∈¯Ω and all ξ ∈Rd. We assume in addition that c ≥c0 > 0 for some
constant c0 and that b ≥0. Then the problem: Find u ∈V = {v ∈H1(Ω); γ0(v) =
0 on Γ1} such that
∀v ∈V,

Ω
(A∇u · ∇v + cuv) dx +

Γ1
bγ0(u)γ0(v) dΓ =

Ω
f v dx +

Γ1
gγ0(v) dΓ,
has one and only one solution.
Proof That V is a Hilbert space and that ℓis continuous are already known facts.
The proof of the continuity of the bilinear form, which is implied by the boundedness
of the matrix coefﬁcients ai j(x), is left to the reader. The V -ellipticity is also quite
obvious, since
a(v, v) =

Ω
(A∇v · ∇v + cv2) dx +

Γ1
bγ0(v)2 dΓ
≥α

Ω
∥∇v∥2 dx + c0

Ω
v2 dx
≥min(α, c0)∥v∥2
H1(Ω),
hence the existence, uniqueness and continuous dependence of the solution on the
data by the Lax–Milgram theorem.
□
Remark 4.19 When the matrix A is not symmetric, neither is the bilinear form a,
even though the principal part of the operator is symmetric since d
i, j=1 ai j∂i j =
d
i, j=1
ai j+a ji
2
∂i j due to the fact that ∂i j = ∂ji. When A is symmetric, then so is the
bilinear form and we have an equivalent minimization problem with
J(v) = 1
2

Ω
(A∇v · ∇v + cv2) dx +

Γ1
bγ0(v)2 dΓ

−

Ω
f v dx −

Γ1
gγ0(v) dΓ,
to be minimized over V .
It is quite clear that we can reduce the regularity of A down to L∞without loosing
the existence and uniqueness of the variational problem. The interpretation in terms of
PDEs stops at the divergence form −div (A∇u) + cu = f since we cannot develop
the divergence using Leibniz formula in this case. Such lack of regularity of the
coefﬁcients is useful to model heterogeneous media.
□

4.4 General Second Order Elliptic Problems
139
We now give another example of a non symmetric problem, the convection–
diffusion problem. Let us be given a vector ﬁeld σ. The convection–diffusion problem
reads
−Δu + σ · ∇u + cu = f in Ω,
u = 0 on ∂Ω.
(4.17)
We have a diffusion term −Δu and a transport term σ · ∇u in the same equation that
compete with each other.
Proposition 4.15 Assume that f ∈L2(Ω), σ ∈C1( ¯Ω; Rd) and c ∈L∞(Ω). Then
the triple
V = H1
0(Ω),
a(u, v) =

Ω

∇u · ∇v + (σ · ∇u + cu)v

dx,
ℓ(v) =

Ω
f v dx,
deﬁnes a variational formulation for problem (4.17).
Proof The proof follows the same lines as before and we leave it as an exercise. Note
that the bilinear form a is not symmetric.
□
Proposition 4.16 Let f ∈L2(Ω), σ ∈C1( ¯Ω; Rd) and c ∈L∞(Ω). We assume that
c −1
2 div σ ≥0. Then the problem: Find u ∈V such that
∀v ∈V,

Ω

∇u · ∇v + (σ · ∇u + cu)v

dx =

Ω
f v dx,
has one and only one solution.
Proof We just prove the V -ellipticity. We have for all v ∈V
a(v, v) =

Ω

∥∇v∥2 + cv2 + (σ · ∇v)v

dx.
It can be checked that σiv ∈H1(Ω) and that the Leibniz formula (3.8) holds in this
case for ﬁrst derivatives. Let us integrate the last integral by parts

Ω
(σ · ∇v)v dx =

Ω
 d

i=1
σi∂iv

v dx
= −

Ω
 d

i=1
∂i(σiv)

v dx = −

Ω
 d

i=1
∂iσi

v2 dx −

Ω
 d

i=1
σi∂iv

v dx
= −

Ω
div σv2 dx −

Ω
(σ · ∇v)v dx,

140
4
The Variational Formulation of Elliptic PDEs
since all boundary terms vanish, so that

Ω
(σ · ∇v)v dx = −1
2

Ω
div σv2 dx.
Therefore
a(v, v) =

Ω

∥∇v∥2 +

c −1
2div σ

v2
dx ≥|v|2
H1(Ω),
hence the result by the equivalence of the H1 semi-norm and the H1 norm on H1
0(Ω),
see Corollary3.3 of Chap.3.
□
Remark 4.20 We thus have existence and uniqueness if c = 0 and div σ = 0. The
case div σ = 0 is interesting because if σ represents the velocity ﬁeld of such a ﬂuid
as air or water, the divergence free condition is the expression of the incompressibility
of the ﬂuid. Under usual experimental conditions, both ﬂuids are in fact considered
to be incompressible.
□
Let us now give a fourth order example, even though only second order problems
were advertised in the section title. We consider a slight variant of the plate problem
involving the bilaplacian with homogeneous Dirichlet boundary conditions
⎧
⎪⎪⎨
⎪⎪⎩
Δ2u + cu = f in Ω,
u = 0 on ∂Ω,
∂u
∂n = 0 on ∂Ω.
The derivation of a variational formulation is again fairly routine, but since this
is our ﬁrst (and only) fourth order problem, we give some detail. The variational
space for this Dirichlet problem is V = H2
0(Ω) which incorporates the two boundary
conditions. Assume that u ∈H4(Ω) ∩H2
0(Ω). Then Δu ∈H2(Ω) and we can use
Green’s formula

Ω
(Δ2u)v dx =

Ω
(Δ(Δu))v dx
=

Ω
Δu Δv dx +

∂Ω
(γ0(v)γ1(Δu) −γ1(v)γ0(Δu)) dΓ
=

Ω
Δu Δv dx,
since γ0(v) = γ1(v) = 0 for all v ∈H2
0(Ω). So we have our variational formulation
∀v ∈V,

Ω
(Δu Δv + cuv) dx =

Ω
f v dx,
(4.18)

4.4 General Second Order Elliptic Problems
141
which is easily checked to give rise to a solution of the boundary value problem.
Let ∇2v denote the collection of all second order partial derivatives of v. We let
∥∇2v∥2
L2(Ω) =

1≤i, j≤d
 ∂2v
∂xi∂x j

2
L2(Ω).
We have
Lemma 4.2 The semi-norm ∥∇2v∥L2(Ω) is a norm on H2
0(Ω) that is equivalent to
the H2 norm.
Proof It is enough to establish a bound from below. Let v ∈H2
0(Ω). Then we have
∂iv ∈H1
0(Ω) for all i. Therefore ∥∇(∂iv)∥2
L2(Ω) ≥C2∥∂iv∥2
H1(Ω), as a consequence
of Poincaré’s inequality, and C ≤1. Now of course
∥∂iv∥2
H1(Ω) = ∥∇(∂iv)∥2
L2(Ω) + ∥∂iv∥2
L2(Ω),
so summing over i, we get
∥∇2v∥2
L2(Ω) =
d

i=1
∥∇(∂iv)∥2
L2(Ω) ≥C2
∥∇2v∥2
L2(Ω) + |v|2
H1(Ω)

≥C2∥∇2v∥2
L2(Ω) + C4∥v∥2
H1(Ω) ≥C4∥v∥2
H2(Ω),
since v ∈H1
0(Ω).
□
Proposition 4.17 Let f ∈L2(Ω) and c ∈L∞(Ω). We assume that c ≥0. Then
problem (4.18) has one and only one solution.
Proof We just prove the V -ellipticity. We have
a(v, v) ≥

Ω
(Δv)2 dx.
We argue by density. Let ϕ ∈D(Ω), since Δϕ = d
i=1 ∂iiϕ, we can write

Ω
(Δϕ)2 dx =

Ω
 d

i=1
∂iiϕ
 d

j=1
∂j jϕ

dx =
d

i, j=1

Ω
∂iiϕ∂j jϕ dx
= −
d

i, j=1

Ω
∂iϕ∂i j jϕ dx =
d

i, j=1

Ω
∂i jϕ∂i jϕ dx

142
4
The Variational Formulation of Elliptic PDEs
with two successive integrations by parts, the ﬁrst one with respect to xi and the
second one with respect to x j. Hence, for all ϕ ∈D(Ω), we obtain

Ω
(Δϕ)2 dx =
d

i, j=1

Ω
(∂i jϕ)2 dx = ∥∇2ϕ∥2
L2(Ω).
(4.19)
Now, by deﬁnition, H2
0(Ω) is the closure of D(Ω) in H2(Ω), thus for all v ∈H2
0(Ω),
there exists a sequence ϕn ∈D(Ω) such that ϕn →v in H2(Ω). Passing to the limit
in the above equality, we thus get

Ω
(Δv)2 dx = ∥∇2v∥2
L2(Ω),
since ∂i jϕn →∂i jv in L2(Ω), hence the result by Lemma 4.2.
□
Remark 4.21 Notice the trick used in the above proof. To establish an equality for
H2 functions, we need to use third derivatives, which make no sense as functions in
this context. However, all formulas are valid for smooth functions, for which third
derivatives are not a problem, and since in the end, the resulting equality (4.19) does
not involve any derivatives of order higher than two, it extends to H2 by density.
The formula is actually surprising, since Δv does not contain any derivative ∂i jv
with i ̸= j, and only the sum of all ∂iiv derivatives. Its L2 norm squared is nonetheless
equal to the sum of the L2 norms squared of all individual second derivatives. This
is related to elliptic regularity, which was mentioned in passing before.
□
Remark 4.22 This is another symmetric problem, hence we have an equivalent
energy minimization formulation with
J(v) = 1
2

Ω

(Δv)2 + cv2
dx −

Ω
f v dx,
to be minimized on H2
0(Ω).
□
4.5
Concluding Remarks
To conclude this section, we discuss the general three point strategy for solving
elliptic problems that was repeatedly applied here. First we establish a variational
formulation: (homogeneous) Dirichlet boundary conditions are enforced by the test-
function space, which is included in H1 for second order problems; we multiply the
PDE by a test-function—possibly assuming additional regularity on the solution—
and use integration by parts or Green’s formula to obtain the variational problem.
The bilinear form must be well-deﬁned on the test-function space.

4.5 Concluding Remarks
143
The second point is to check that the variational formulation actually gives rise
to a solution of the boundary value problem. This point is usually itself in two steps:
ﬁrst obtain the PDE in the sense of distributions by using test-functions in D, second
retrieve Neumann or Fourier boundary conditions by using the full test-function
space. The ﬁrst two points can appear somewhat formal because of the assumed
regularity on the solution that is not always easily obtained in the end. This is not a
real problem, since it is possible to write rigorous arguments, at the expense of more
theory than we need here.
The ﬁnal third point is to try and apply the Lax–Milgram theorem, by making
precise regularity and possibly sign assumptions on the data and coefﬁcients in order
to ensure continuity of the linear and bilinear forms as well as V -ellipticity. Here we
prove existence and uniqueness of the solution to the variational problem.
Aquestionthatcanbeaskediswhatistherelevanceofsuchsolutionstoaboundary
value problem, in which the partial derivatives are taken in a rather weak sense. This
is where elliptic regularity theory comes into play. Using elliptic regularity, it is
possible to show that the variational solution given by the Lax–Milgram theorem is
indeed a classical solution, provided the coefﬁcients, right-hand side, boundary of
Ω and so on are smooth enough.
To be a little more precise on these regularity results, let us mention that they
proceed in two steps
• Local regularity, which only depends on the regularity of the coefﬁcients and right-
hand side of the PDE. For example, for the model Dirichlet variational problem
(4.3), with c ∈C∞(Ω), we have for any integer k ≥0
if f ∈Hk
loc(Ω), then u ∈Hk+2
loc (Ω),
where
Hk
loc(Ω) = { f ∈D′(Ω), such that for any ϕ ∈D(Ω), f ϕ ∈Hk(Ω)}.
• Global regularity, i.e., up to the boundary Γ of Ω, which also depends on the
regularity of the boundary and on the boundary conditions, see [3, 4, 15, 25, 35].
We have for example the following result, still for the model Dirichlet problem,
see [44].
Proposition 4.18 Let u be the solution of (4.3) with f ∈L2(Ω). If Γ is of class
C1,1, then u ∈H2(Ω).
The same type of result holds for the Neumann problem of Proposition 4.6 with
g ∈H1/2(Γ ), see [44]. We refer for example to [25] for a counterexample when Ω
is not regular enough.
□
In this chapter, we applied the variational method to solve elliptic boundary value
problems from the theoretical point of view. We are now going to see that the vari-
ational approach is also very well suited to numerical approximation, in particular
via the ﬁnite element method.

Chapter 5
Variational Approximation Methods
for Elliptic PDEs
Oneofthevirtuesofthevariationalapproachisthatitleadsnaturallytoawholefamily
of approximation methods. Let us emphasize again that the reason why approxima-
tion methods for PDEs are needed is that, even though we may be able to prove the
existence of a solution, in general there is no closed form formula for it. In addition,
such approximations must be effectively computable.
There are other approximation methods that are not variational, such as the ﬁnite
difference method seen earlier, the ﬁnite volume method that we will see in Chap.10,
and yet many other methods that we will not consider in this book.
5.1
The General Abstract Variational Approximation
Scheme
As we have seen, boundary value problems naturally take place in inﬁnite dimen-
sional vector spaces. An inﬁnite dimensional space is too large to ﬁt inside a com-
puter, thus the main idea is to build ﬁnite dimensional approximations thereof. Any
approximation method of this kind falls under the general heading of a Galerkin
method [7, 11, 19, 28, 63, 65, 66]. Let us start with a few deﬁnitions that pertain to
the variational case.
Deﬁnition 5.1 Let V be a Hilbert space and (Vn)n∈N be a sequence of ﬁnite dimen-
sional vector subspaces of V . We say that this sequence is a conforming approxima-
tion sequence if for all u ∈V , there exists a sequence (vn)n∈N such that
vn ∈Vn and ∥u −vn∥V →0 when n →+∞.
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_5
145

146
5
Variational Approximation Methods for Elliptic PDEs
Remark 5.1 Note that in general, we do not have Vn ⊂Vn+1, i.e., the approximation
spaces do not need to be nested. The conforming approximation condition implies
that 
n∈N Vn is dense in V .
There are situations in which non conforming approximations are called for, that
is to say Vn ̸⊂V , see for example [11, 19, 45]. Of course, in this case ∥u −vn∥V
does not make sense, and another deﬁnition is needed.
The traditional notation for an approximation sequence is Vh instead of Vn, where
h is a discretization parameter that is assumed to belong to a sequence that tends to
0 instead of having n →+∞, the two being equivalent. We will from now on stick
with the tradition.
□
The main abstract result is the following, also known under the name of Céa’s
lemma, cf. [7, 9, 14].
Theorem 5.1 (Céa’s Lemma) Let V be a Hilbert space, a be a bilinear form and ℓ
be a linear form satisfying the hypotheses of the Lax-Milgram theorem. Let Vh be a
closed subspace of V . Then there exists a unique uh ∈Vh such that
∀vh ∈Vh,
a(uh, vh) = ℓ(vh),
(5.1)
and we have
∥u −uh∥V ≤M
α inf
vh∈Vh ∥u −vh∥V = M
α d(u, Vh),
where M is the continuity constant of a and α its V -ellipticity constant.
Proof Since Vh is closed, it is a Hilbert space for the restriction of the scalar product of
V . The Lax-Milgram hypotheses for the variational problem on Vh are thus satisﬁed
and the existence and uniqueness of uh is assured.
Now we have a(u, v) = ℓ(v) for all v ∈V , thus in particular for v = wh ∈Vh.
On the other hand, we also have a(uh, wh) = ℓ(wh), so that subtracting the two
0 = a(u, wh) −a(uh, wh) = a(u −uh, wh)
(5.2)
for all wh ∈Vh. By V -ellipticity, for all vh ∈Vh,
α∥u −uh∥2
V ≤a(u −uh, u −uh)
≤a(u −uh, u −vh) + a(u −uh, vh −uh)
= a(u −uh, u −vh)
≤M∥u −uh∥V ∥u −vh∥V ,
since wh = vh −uh ∈Vh. The case ∥u −uh∥V = 0 is ideal and nothing needs to be
done. If the norm is non zero, we divide by it and obtain
∥u −uh∥V ≤M
α ∥u −vh∥V

5.1 The General Abstract Variational Approximation Scheme
147
for all vh ∈Vh, thus the theorem by taking the inﬁmum of the right-hand side
over Vh.
□
Remark 5.2 A word of warning about the traditional notation vh ∈Vh. This tradi-
tional notation is rather unfortunate, since vh is not a function of h, but an arbitrary
element of Vh. The subscript h must thus not be understood as a regular subscript,
but just as a typographical reminder that we are talking about an arbitrary element
of Vh. On the other hand, the solution uh can be considered as a function of h insofar
as Vh can be considered as a function of h.
□
We now apply Céa’s lemma to the case of a conforming approximation sequence.
Corollary 5.1 Let Vh be a conforming approximation sequence. Then the sequence
uh ∈Vh of approximated solutions converges to the solution u in V , with the a priori
error estimate
∥u −uh∥V ≤M
α d(u, Vh) →0 when h →0.
Proof Each subspace Vh is ﬁnite dimensional, hence closed. We thus apply Theo-
rem5.1 and obtain the convergence result since d(u, Vh) ≤∥u −vh∥V where vh is
given by the deﬁnition of conforming approximation for this u.
□
Remark 5.3 We also trivially have ∥u −uh∥V ≥d(u, Vh), thus the error estimate is
optimal in terms of order of magnitude when h →0. Now, if the constant M/α is
very large, then the numerical error can be large too with respect to d(u, Vh).
In the case when a is symmetric, the constant in Céa’s lemma can be improved to
√M/α. Indeed, in this case, a deﬁnes a second scalar product on V , for which V is
also a Hilbert space. Equation(5.2) then says that u −uh is orthogonal to Vh for the
new scalar product. Therefore, by the orthogonal projection Theorem3.2 of Chap.3,
it minimizes the new distance to Vh and we thus have
α∥u −uh∥2
V ≤a(u −uh, u −uh) ≤a(u −vh, u −vh) ≤M∥u −vh∥2
V
for all vh ∈Vh. Taking the inﬁmum in the right-hand side with respect to vh yields the
improved estimate. Of course, M ≥α, so this is a real improvement of the constant.
An interesting feature of Céa’s lemma is that it decomposes the error estimate
into two basically independent parts: The constant M/α which only depends on the
bilinear form, i.e., the PDE, and not on the approximation method, and d(u, Vh)
which depends mostly on the approximation properties of the space Vh. In practice,
the second part will be estimated by constructing a linear operator Πh : V →Vh,
writing that
d(u, Vh) ≤∥u −Πhu∥V ≤∥I −Πh∥∥u∥V
and estimating the term ∥I −Πh∥which depends only on Vh.
□
The approximation uh lives in a ﬁnite dimensional space, therefore it is com-
putable, at least in principle. Let us see how to proceed in practice.

148
5
Variational Approximation Methods for Elliptic PDEs
Proposition 5.1 Let Nh = dim Vh and (w1, w2, . . . , wNh) be a basis of Vh. We write
uh = Nh
j=1 uh, jw j. We introduce an Nh × Nh matrix A deﬁned by Ai j = a(w j, wi)
and two vectors B ∈RNh by Bi = ℓ(wi) and X ∈RNh by X j = uh, j. Then the matrix
A is invertible and we have AX = B. Conversely, the solution of this linear system is
the vector of coordinates of uh in the basis (wi).
Proof Let us take vh = wi in the variational formulation of the ﬁnite dimensional
problem. This yields
Bi = ℓ(wi) = a(uh, wi) = a
 Nh

j=1
uh, jw j, wi
=
Nh

j=1
uh, ja(w j, wi) =
Nh

j=1
Ai jX j = (AX)i
for all i. Hence AX = B.
Conversely, if AX = B, then by the above computation, ℓ(wi) = a(uh, wi) where
uh = Nh
j=1 X jw j. For all vh ∈Vh, we have vh = Nh
i=1 vh,iwi, therefore
ℓ(vh) =
Nh

i=1
vh,iℓ(wi) =
Nh

i=1
vh,ia(uh, wi) = a

uh,
Nh

i=1
vh,iwi
= a(uh, vh)
therefore, by the uniqueness of the Lax-Milgram solution, we have uh = uh. Thus
the variational problem and the linear system are equivalent. Since the variational
problem has one and only one solution for any ℓ, it follows that A is invertible.
□
Remark 5.4 The problem of computing the ﬁnite dimensional approximation uh is
thus reduced to that of computing the matrix A and the right-hand side B once a basis
of Vh is chosen, which is called assembling the system, and then of solving the linear
system AX = B. In practical applications, Nh is typically large, ranging from the
thousands to the millions or billions. This is a whole other subject with many facets:
matrix conditioning, efﬁcient algorithms for large linear systems, high performance
computing. We will not touch on this.
It is important not to lose sight of the fact that the size of the matrix A and of the
right-hand side B depend on h, via Nh, even though the notation fails to make this
dependence apparent. In particular, when h →0, we have Nh →+∞.
Do not forget the exchange of indices Ai j = a(w j, wi) and not Ai j = a(wi, w j)!
Of course if a is symmetric, then the matrix A is symmetric. It is also positive,
deﬁnite, with a(vh, vh) = Y TAY where Y is the vector of coordinates of vh ∈Vh in
the basis (wi).
□
We now introduce the main example of variational approximation method, the
ﬁnite element method (FEM). There are other variational approximation methods,
such as the spectral method, see [10, 11, 17, 43, 50, 63, 65] as well as such extensions
as discontinuous Galerkin methods, see [14, 31, 52], which are non conforming
methods, among many others.
In the rest of this chapter, we consider the ﬁnite element method in the one-
dimensional case. The two-dimensional case will be the subject of the next chapter.

5.2 The Finite Element Method in Dimension One
149
5.2
The Finite Element Method in Dimension One
Let Ω = ]a, b[ and consider the model problem
 −u′′ + cu = f in Ω,
u(a) = u(b) = 0.
(5.3)
When f ∈L2(a, b), c ∈L∞(a, b) and c ≥0, we know that this problem has
one and only one solution by using the variational formulation V = H1
0(]a, b[),
a(u, v) =
	
Ω(u′v′ + cuv) dx and ℓ(v) =
	
Ω f v dx.
The idea of the FEM is to take approximation spaces Vh composed of functions
that are piecewise polynomial of low degree, with lots of pieces. In one dimension,
we have H1(]a, b[) ⊂C0([a, b]), thus for the approximation to be conforming, we
need to impose Vh ⊂C0([a, b]) as well.
The FEM is based on the notion of mesh. In one dimension, a mesh is just a
subdivision of ]a, b[ into a ﬁnite number of subintervals. Each one of the small
intervals is called an element. We will only consider uniform meshes for simplicity.
Nonuniform meshes pose no additional conceptual difﬁculty as should become clear.
Let N be a positive integer. We set h = b−a
N+1, which is called the mesh size, and let
xi = a + ih, i = 0, . . . , N + 1, be the nodes of the mesh.
We thus have N + 1 subintervals [xi, xi+1] of length h, N interior nodes xi,
i = 1, . . . , N, and 2 boundary nodes x0 and xN+1. Even though Fig.5.1 is virtually
identical to Fig.2.1 of Chap.2 depicting a one-dimensional grid, the two concepts of
mesh and grid are really different. In a sense, in a grid only the grid points count and
nothing in between, whereas in a mesh both the elements and nodes are of impor-
tance. Of course, the visual difference between grids and meshes is more apparent
in dimensions 2 and higher.
We now deﬁne
Vh = {vh ∈C0([a, b]); vh|[xi,xi+1] is afﬁne for i = 0, . . . , N, vh(a) = vh(b) = 0}.
Note that here, the subscript h in Vh is actually the same h as the mesh size. Since
h →0 when N →+∞, we thus have a sequence of spaces. We ﬁrst need to verify
that these spaces are subspaces of V .
Proposition 5.2 We have Vh ⊂H1
0(]a, b[).
Proof First of all, since Vh ⊂C0([a, b]) with [a, b] compact, we have Vh ⊂L2(a, b).
Let us compute the distributional derivative of an element vh of Vh. Since vh|[xi,xi+1]
Fig. 5.1 A uniform 1d mesh
h
a = x0
x1
x2
x3
···
xN
xN+1 = b

150
5
Variational Approximation Methods for Elliptic PDEs
is an afﬁne function, we can write vh(x) = λix + μi for x ∈[xi, xi+1], where λi, μi
are constants that depend on the subinterval. For all ϕ ∈D(]a, b[), we have
⟨v′
h, ϕ⟩= −⟨vh, ϕ′⟩= −

 b
a
vh(x)ϕ′(x) dx
= −
N

i=0

 xi+1
xi
vh(x)ϕ′(x) dx
= −
N

i=0

 xi+1
xi
(λix + μi)ϕ′(x) dx.
Now we can classically integrate each element integral by parts,
−

 xi+1
xi
(λix + μi)ϕ′(x) dx =

 xi+1
xi
λiϕ(x) dx −[vh(x)ϕ(x)]xi+1
xi
=

 xi+1
xi
λiϕ(x) dx −vh(xi+1)ϕ(xi+1) + vh(xi)ϕ(xi),
since vh is continuous on [a, b] its right and left limits at xi and xi+1 respectively are
just its value at these points.
Now, if we let
g(x) =
N

i=0
λi1]xi,xi+1[(x),
then obviously g is a piecewise constant function, hence is bounded, and thus in
L2(a, b) and
N

i=0

 xi+1
xi
λiϕ(x) dx =

 b
a
g(x)ϕ(x) dx.
On the other hand,
−
N

i=0
[vh(x)ϕ(x)]xi+1
xi
= −vh(x1)ϕ(x1) + vh(x0)ϕ(x0)
−vh(x2)ϕ(x2) + vh(x1)ϕ(x1) −· · ·
· · · −vh(xN+1)ϕ(xN+1) + vh(xN)ϕ(xN) = 0,
since all terms involving interior nodes appear twice with opposite signs, and ϕ(x0) =
ϕ(xN+1) = 0 since ϕ has compact support. Finally, we see that
⟨v′
h, ϕ⟩=

 b
a
g(x)ϕ(x) dx = ⟨g, ϕ⟩,

5.2 The Finite Element Method in Dimension One
151
with g ∈L2(a, b) which shows that vh ∈H1(]a, b[) and v′
h = g. Now all elements
of Vh also satisfy vh(a) = vh(b) = 0 so that vh ∈H1
0(]a, b[).
□
It is fairly clear that the space Vh is ﬁnite dimensional, since any of its elements
is determined by a ﬁnite number of constants λi and μi. The space Vh is therefore
closed, and the general abstract principle applies, i.e., there exists a unique uh ∈Vh
such that a(uh, vh) = ℓ(vh) for all vh ∈Vh, and we have Céa’s lemma error estimate.
Let us see how this estimate can be exploited to quantify the convergence rate. Let
us start with a general purpose lemma concerning Vh.
Lemma 5.1 There exists a unique continuous linear mapping Πh : H1
0(]a, b[) →
Vh, called the Vh-interpolation operator such that for all v in H1
0(]a, b[), v(xi) =
Πhv(xi) for i = 0, . . . , N + 1.
Proof First of all, we note that H1
0(]a, b[) →C0([a, b]), therefore the nodal values
v(xi) are unambiguously deﬁned and v(x0) = v(xN+1) = 0.
Now an afﬁne function on [xi, xi+1] is uniquely determined by its values at xi
and xi+1. Thus, the relations v(xi) = Πhv(xi) for i = 0, . . . , N + 1 deﬁne a unique
piecewise afﬁne function on the mesh, that is continuous and vanishes at both ends,
thus belongs to Vh. Let Πhv be this function. Clearly, the mapping v →Πhv is
linear from H1
0(]a, b[) into Vh. Finally we infer from the fact that the values taken
by an afﬁne function on an interval lie between the values at the endpoints, that
maxx∈[xi,xi+1] |Πhv(x)| = max

|v(xi)|, |v(xi+1)|

, and therefore
∥Πhv∥C0([a,b]) = max
i=0,...,N
max
x∈[xi,xi+1] |Πhv(x)|
= max
i=0,...,N max

|v(xi)|, |v(xi+1)|

≤max
x∈[a,b] |v(x)| = ∥v∥C0([a,b]) ≤C∥v∥H1(]a,b[),
by Theorem3.7 of Chap.3. Consequently, the Vh-interpolation operator is
continuous.
□
Remark 5.5 A picture is in order here. As can be seen on Fig.5.2, Πhv, which we
call the Vh-interpolate of v, is the unique element of Vh that coincides with v at all
nodes of the mesh.
□
Theorem 5.2 Assume c and f are continuous on [a, b]. Then the solution u of
problem (5.3) is of class C2([a, b]) and there exists a constant C independent of u
such that
∥u −uh∥V ≤Ch max
x∈[a,b] |u′′(x)|.
(5.4)
Proof If f and c are continuous, since u is also continuous by Theorem3.7 of Chap.3,
then u′′ = cu −f is continuous on [a, b] and u ∈C2([a, b]). By Céa’s lemma (viz.
Theorem5.1), we have

152
5
Variational Approximation Methods for Elliptic PDEs
Fig. 5.2 The Vh-interpolate
Πhv of a function v
x0
xi
xi+1
xN+1
∥u −uh∥V ≤M
α inf
vh∈Vh ∥u −vh∥V .
We choose vh = Πhu. It follows that
∥u −uh∥V ≤M
α ∥u −Πhu∥V ,
and we are left with estimating the rightmost norm.
Let us take the H1 semi-norm as a norm on V (this makes for simpler computa-
tions). We have
∥u −Πhu∥2
V =

 b
a
((u −Πhu)′(x))2 dx =
N

i=0

 xi+1
xi
(u′ −(Πhu)′(x))2 dx.
Let us consider the function w = u −Πhu on [xi, xi+1]. By deﬁnition of Vh-
interpolation, we have w(xi) = w(xi+1) = 0. Since w is C1 on [xi, xi+1], Rolle’s
theorem applies and there exists c ∈]xi, xi+1[ such that w′(c) = 0. Now, w is also of
class C2 on [xi, xi+1] so that
w′(x) =

 x
c
w′′(t) dt =

 x
c
u′′(t) dt
for all x ∈[xi, xi+1], since Πhu is afﬁne there, thus its second derivative vanishes. It
follows from this equality that
|w′(x)| ≤

 x
c
|u′′(t)| dt ≤

 xi+1
xi
|u′′(t)| dt ≤h
max
t∈[xi,xi+1] |u′′(t)| ≤h max
x∈[a,b] |u′′(x)|,
for all x ∈[xi, xi+1]. Squaring and integrating, we thus see that

5.2 The Finite Element Method in Dimension One
153

 xi+1
xi
((u −(Πhu))′(x))2 dx =

 xi+1
xi
(w′(x))2 dx ≤h2(xi+1 −xi) max
x∈[a,b] |u′′(x)|2.
Now we sum from i = 0 to N
∥u −Πhu∥2
V ≤h2 N

i=0
(xi+1 −xi)

max
x∈[a,b] |u′′(x)|2 = h2(b −a) max
x∈[a,b] |u′′(x)|2.
Finally, we obtain
∥u −uh∥V ≤
M
α
√
b −a

h max
x∈[a,b] |u′′(x)|,
which completes the proof.
□
Remark 5.6 Note that we have not proved that the sequence Vh is a conforming
approximation sequence in the sense of Deﬁnition5.1. Rather, we have exploited
Céa’s error estimate directly, coupled with an additional regularity hypothesis, here
that u be C2 essentially, to obtain an explicit error estimate and a convergence order
in O(h) when h →0, see Fig.5.3. Note that u ∈H2(]a, b[) is sufﬁcient to obtain the
same error estimate, see Theorem6.2 of Chap.6 for a proof in the two-dimensional
case. More precisely, the following inequality holds true
∥u −Πhu∥V ≤Ch∥u∥H2(]a,b[),
(5.5)
which in turns implies the error estimate
∥u −uh∥V ≤Ch∥u∥H2(]a,b[),
(5.6)
Fig. 5.3 A ﬁctitious
computation: the continuous
solution u, the discrete
solution uh, and the
Vh-interpolate Πhu of u used
to control the error between
the former two. Note that
only uh is effectively
computable
x0
xi
xi+1
xN+1
u
uh
hu

154
5
Variational Approximation Methods for Elliptic PDEs
due to Céa’s lemma. This will be a general fact: additional regularity hypotheses on
the solution will be needed for explicit error estimates. Such regularity can however
often be deduced from elliptic regularity theory.
The sequence Vh is in fact a conforming approximation sequence, but this does
not turn out to be too useful, as the convergence toward a generic element of H1 can
be much slower than O(h).
□
Let us now talk about the choice of a basis in Vh. Even though in principle, the
resolution of the ﬁnite dimensional problem should not depend on the basis choice, in
practice this is an extremely important issue since the choice of basis directly impacts
the matrix A. A wrong choice of basis can lead to a linear problem that cannot be
solved numerically (bad conditioning, see Remark2.11 of Chap.2, full matrix) in
the sense that all theoretically convergent algorithms may fail or take too long or use
up too much computer memory. Recall that for a basis (w j), the matrix coefﬁcients
are given by
Ai j = a(w j, wi) =

 b
a
((w j)′(wi)′ + cw jwi) dx.
For numerical purposes, full matrices are to be avoided and sparse matrices pre-
ferred. Now, there is an easy way of making sure that Ai j = 0, given the above
formula, and that is to arrange for the supports of wi and w j to have negligible
intersection. So we want to ﬁnd a basis for Vh for which the supports are as small
as possible, in order to minimize the intersections. Now clearly, the support of any
function of Vh is at least comprised of two elements. We thus deﬁne
Deﬁnition 5.2 For i = 1, . . . , N, let wi
h ∈Vh be deﬁned by wi
h(xi) = 1 and wi
h(x j) =
0 for j = 0, . . . , N + 1, j ̸= i. We call these functions the hat functions or basis
functions for P1 Lagrange interpolation, see Fig.5.4.
Fig. 5.4 The hat function wi
h
with support [xi−1, xi+1]
1
xi
xi−1
xi+1
0

5.2 The Finite Element Method in Dimension One
155
As we have said before, all functions in Vh are determined by their nodal values.
In particular here, wi
h(a) = wi
h(b) = 0, since the endpoints correspond to j = 0 and
j = N + 1, and 1 ≤i ≤N. The term P1 Lagrange interpolation stems from the fact
that afﬁne functions are polynomials of degree at most 1, hence P1, and that these
functions are also used for Lagrange interpolation in Vh, as we will see shortly.
Proposition 5.3 The family (wi
h)i=1,...,N is a basis of Vh. Thus dim Vh = N. More-
over, we have the interpolation property
∀vh ∈Vh,
vh(x) =
N

i=1
vh(xi)wi
h(x),
(5.7)
for all x ∈[a, b].
Proof We use the Kronecker delta symbol: δi j = 1 if i = j, δi j = 0 otherwise. The
hat functions thus satisfy wi
h(x j) = δi j for i = 1, . . . , N and j = 0, . . . , N + 1.
Let us ﬁrst show that the family is linearly independent. Let λi be scalars such
that
N

i=1
λiwi
h = 0.
Evaluating this zero function at point x j yields
0 =
N

i=1
λiwi
h(x j) =
N

i=1
λiδi j = λ j
since in the last sum, the only nonzero term corresponds to i = j. Thus all coefﬁcients
vanish and the family is linearly independent.
Next we show that the family spans Vh. For all vh ∈Vh, we deﬁne
vh =
N

i=1
vh(xi)wi
h ∈Vh.
Now, of course vh −vh ∈Vh and sincevh(x j) = N
i=1 vh(xi)wi
h(x j) = N
i=1 vh(xi)
δi j = vh(x j) (same computation as above), then (vh −vh)(x j) = 0 for all j =
0, . . . , N + 1. For each element [x j, x j+1], we thus see that vh −vh is afﬁne on the
segment and vanishes at both endpoints, hence is identically zero on [x j, x j+1]. As
this is true for all j, we have vh −vh = 0 on [a, b], that is to say vh = vh, which
shows both that the family is spanning and that we have formula (5.7).
The family (wi)i=1,...,N is linearly independent and spanning, thus is a basis of Vh.
It has N elements so that dim Vh = N.
□

156
5
Variational Approximation Methods for Elliptic PDEs
Remark 5.7 The Lagrange interpolation property (5.7) is very important. It shows
that with this speciﬁc choice of basis, the coordinates of a function vh are precisely
its nodal values vh(xi). Hence solving the linear system AX = B is going to directly
provide the nodal values of the discrete solution uh, without any post-processing. The
linear forms vh →vh(xi), which belong to the dual V ∗
h of Vh, are called the degrees
of freedom in the FEM context. From the point of view of linear algebra, they are
just the dual basis of the basis (wi)i=1,...,N.
□
Corollary 5.2 The N × N matrix A is tridiagonal in the hat functions basis.
Proof Indeed, the support of wi
h is [xi−1, xi+1], therefore if |i −j| ≥2, then
xi−1 ≥x j+1 or x j−1 ≥xi+1 and the intersection of both supports is of zero mea-
sure, hence Ai j = 0. Thus, on any given line of the matrix A, we have at most three
nonzero coefﬁcients: Ai,i−1 corresponding to the subdiagonal, Aii corresponding to
the diagonal and Ai,i+1 corresponding to the superdiagonal.
□
Of course, a tridiagonal matrix is the best kind of matrix that can be expected,
apart from a diagonal matrix which cannot occur. This is because the problem is
exceedingly simple. Actually, it is easy to compute all nonzero coefﬁcients.
Proposition 5.4 If c = c0 and f = f0 are constant, then we have
Aii = 2
h + 2h
3 c0,
Ai,i−1 = Ai,i+1 = −1
h + h
6c0 and Bi = h f0.
Proof We start by noticing that wi
h(x) = w1
h(x −xi) (extending w1
h by zero outside
of [a, b]), thus Aii = A11 and Ai,i−1 = Ai,i+1 = A12.
It is easy to see that w1
h(x) =
x
h on [x0, x1] and w1
h(x) = 2 −x
h on [x1, x2],
0 elsewhere. Therefore, (w1
h)′(x) =
1
h on [x0, x1], (w1
h)′(x) = −1
h on [x1, x2], 0
elsewhere.
Thus
A11 =

 x2
x0
((w1
h)′(x)2 + c0w1
h(x)2) dx
=

 x1
x0
((w1
h)′(x)2 + c0w1
h(x)2) dx +

 x2
x1
((w1
h)′(x)2 + c0w1
h(x)2) dx
= 1
h2 × h + c0
h2

 x1
x0
x2 dx + 1
h2 × h + c0
h2

 x2
x1
(2h −x)2 dx
= 2
h + 2h
3 c0.
The intersection of the supports of w1
h and w2
h is [x1, x2], hence

5.2 The Finite Element Method in Dimension One
157
A12 =

 x2
x1
((w1
h)′(x)(w2
h)′(x) + c0w1
h(x)w2
h(x)) dx
= −1
h2 × h + c0
h2

 x2
x1
(2h −x)(x −h) dx
= −1
h + h
6c0.
We leave the last value to the reader.
□
Remark 5.8 When c or f is not constant, the corresponding terms may not necessar-
ily be exactly computable and it may be necessary to resort to numerical integration
[23, 64, 71]. These terms however are corrections to the dominant terms 2
h and −1
h,
so that it can be shown that choosing a sufﬁciently accurate numerical integration
rule does not modify the ﬁnal error estimate.
Numerical methods for linear systems are especially efﬁcient in the case of a
tridiagonal matrix. The LU factorization is very efﬁcient, but other methods can be
used such as the Cholesky factorization (the matrix is symmetric), the conjugate
gradient method, and so on, see [6, 18, 59].
□
Let us now make a rapid comparison between the ﬁnite element method and the
ﬁnite difference method seen in Chap.2.
5.3
Comparison with the Finite Difference Method
We have shown that the ﬁnite element method is of order one in the case of the
model one-dimensional example, cf. estimate (5.4), under the hypotheses c and f
continuous. Under a slightly stronger hypothesis, namely c and f of class C2, the
ﬁnite difference error estimate (2.13) is of order two. It could thus be thought that
the ﬁnite difference method is better than the ﬁnite element method.
It should however be noticed that these errors are not measured in the same norms.
In particular, the ﬁnite difference error estimate does not involve the derivative of u,
whereas the ﬁnite element does. If we do not take the derivative into account, the
ﬁnite element method also yields an error of order two in the L2 norm. This is called
the Aubin-Nitsche duality trick that we now explain.
Proposition 5.5 Assume that c ∈L∞(a, b) and f ∈L2(a, b), then u ∈H2(]a, b[)
and we have
∥u −uh∥L2(a,b) ≤Ch2∥u∥H2(]a,b[).
(5.8)
Proof We have seen that u′′ = cu −f in the sense of D′(]a, b[). Since c ∈L∞(a, b)
and u ∈L2(a, b), it follows that u′′ ∈L2(a, b), hence that u ∈H2(]a, b[). Let us set
eh = u −uh. We deﬁne the adjoint variational problem: Find w ∈V such that

158
5
Variational Approximation Methods for Elliptic PDEs
∀v ∈V,
a(w, v) =

 b
a
eh(x)v(x) dx.
(5.9)
Clearly, w ∈H2(]a, b[) with −w′′ + cw = eh almost everywhere in ]a, b[. By
Proposition4.3 of Chap.4, we thus have ∥w∥V ≤C∥eh∥L2(a,b). Therefore,
∥w′′∥L2(a,b) ≤∥c∥L∞(a,b)∥w∥L2(a,b) + ∥eh∥L2(a,b) ≤C∥eh∥L2(a,b),
where C is a constant that changes from line to line. It follows that ∥w∥H2(]a,b[) ≤
C∥eh∥L2(a,b).1
We know that a(eh, vh) = 0 for all vh ∈Vh. Consequently
∥eh∥2
L2(a,b) =

 b
a
eh(x)eh(x) dx = a(w, eh) = a(eh, w)
= a(eh, w −Πhw) ≤M∥eh∥V ∥w −Πhw∥V ,
where we have used the dual problem (5.9), and the symmetry and continuity of the
bilinear form a. By the error estimate (5.6), we have
∥eh∥V ≤Ch∥u∥H2(]a,b[),
on the one hand, and by the interpolation estimate (5.5), we have
∥w −Πhw∥V ≤Ch∥w∥H2(]a,b[) ≤Ch∥eh∥L2(a,b),
on the other hand. Combining the above estimates, we obtain
∥eh∥2
L2(a,b) ≤Ch2∥u∥H2(]a,b[)∥eh∥L2(a,b),
hence the result.
□
It is interesting to make a numerical comparison of the ﬁnite element method with
the ﬁnite difference method in the present context. When c and f are smooth, we
know from the theory that both methods should give good results of about the same
order, and this is conﬁrmed by numerical experiments. We can make the situation a
little more challenging for the ﬁnite difference method by taking nonsmooth data,
which the ﬁnite element method should be able to handle correctly.
Consider thus the homogeneous Dirichlet problem −u′′ = f in ]0, 1[, u(0) =
u(1) = 0, with f (x) = 20 for x ∈] 1
2 −1
40, 1
2 + 1
40[, f (x) = 0 otherwise.2 We plot in
Fig.5.5 the exact solution, the numerical ﬁnite element solution and the numerical
ﬁnite difference solution.
1This is an elliptic regularity estimate.
2This right-hand side is an approximation of the Dirac mass δ 1
2 .

5.3 Comparison with the Finite Difference Method
159
Estimate (5.4) is not valid since f is not continuous. However, this function is
in L2(0, 1), and estimate (5.6) applies. Now it is not entirely clear whether or not
the ﬁnite difference method converges in this case, but it is obviously having a
signiﬁcantly harder time than the ﬁnite element method.
In particular, we see on Fig.5.5 that the convergence of the ﬁnite element method
is very good, with an excellent agreement even for small values of N. On the other
hand, the behavior of the ﬁnite difference approximation is curious. There is a marked
difference between successive values of N, depending on parity. In any case, the
numerical convergence of the ﬁnite difference approximation is here much slower
than the convergence of the ﬁnite element method.
5.4
A Fourth Order Example
Let us now brieﬂy consider the beam problem
 u(4) + cu = f in Ω,
u(a) = u(b) = u′(a) = u′(b) = 0.
(5.10)
The variational formulation of this problem is set in V = H2
0(]a, b[) and since
H2(]a, b[) →C1([a, b]), the previous P1 ﬁnite element method is not adapted (exer-
cise, show that if v is piecewise afﬁne, then v′′ = N
i=1(λi −λi−1)δxi /∈L2(a, b)). We
need higher degree polynomials to not only match the values, but also the derivatives
at mesh nodes. In the following, Pk denotes the space of polynomials of degree at
most k. We thus deﬁne
Vh = {vh ∈C1([a, b]); vh|[xi,xi+1] ∈P3 for i = 0, . . . , N,
vh(a) = vh(b) = v′
h(a) = v′
h(b) = 0}.
A natural question is why not simply use P2 polynomials. The reason is that even
though in this case, the space Vh is not reduced to {0}, its description is very unwieldy
and it is not clear that it can be used for approximation purposes. This is because the
determination of a P2 polynomial on an interval requires three interpolation data. It
is thus possible to interpolate the value of the polynomial and of its derivative at one
end of an interval, but then, there is only one interpolation value left at the other end,
which makes it difﬁcult to ensure that the piecewise P2 function thus constructed is
globally C1 and satisﬁes the boundary conditions.
The above difﬁculty disappears for degrees k ≥3 as we will see shortly.
Proposition 5.6 We have Vh ⊂H2
0(]a, b[).
Proof Argue as in the proof of Proposition5.2.
□
We thus need C1 functions, and in order to ensure the continuity and derivability
at the endpoints of each element, we need to be able to specify both the value of

160
5
Variational Approximation Methods for Elliptic PDEs
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 19
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 20
0.00
0.05
0.10
0.15
0.20
0.25
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 29
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 30
0.00
0.05
0.10
0.15
0.20
0.25
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 59
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 60
0.00
0.05
0.10
0.15
0.20
0.25
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 199
0.00
0.05
0.10
0.15
0.20
0.25
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
N = 200
Fig. 5.5 Comparison ﬁnite difference (× marks), ﬁnite elements (+ marks) and exact solution
(solid curve), for different values of N

5.4 A Fourth Order Example
161
the polynomial and of its derivative. The simplest way to achieve this is to use P3
Hermite interpolation. Let us rapidly present this interpolation.
Proposition 5.7 For all quadruplets (α0, α1, β0, β1) of scalars, there exists a unique
polynomial P ∈P3 such that
P(0) = α0,
P(1) = α1,
P′(0) = β0,
P′(1) = β1.
This polynomial is given by
P = α0p0 + α1p1 + β0q0 + β1q1,
where the P3 Hermite basis polynomials p0, p1, q0 and q1 are given by
p0(x) = (1 −x)2(1 + 2x), p1(x) =x2(3 −2x),
q0(x) = x(1 −x)2, q1(x) = x2(x −1). (5.11)
Proof The proof of Proposition5.7 follows from a simple dimension argument: we
show that the linear mapping P3 →R4, P →(P(0), P(1), P′(0), P′(1)) is an iso-
morphism. Since P3 is four-dimensional, it sufﬁces to show that its kernel is trivial.
But a polynomial such that P(0) = P(1) = P′(0) = P′(1) = 0 has a double root at
x = 0 and another double root at x = 1, hence a number of roots counting multi-
plicities of at least four. We know that a nonzero polynomial of degree at most three
has at most three roots. Hence P = 0.
The inverse image of the canonical basis of R4 by the previous isomorphism
forms a basis of P3. Its elements are uniquely determined by the following interpo-
lation values: (α0, α1, β0, β1) = (1, 0, 0, 0) for p0, (α0, α1, β0, β1) = (0, 1, 0, 0) for
p1, (α0, α1, β0, β1) = (0, 0, 1, 0) for q0 and (α0, α1, β0, β1) = (0, 0, 0, 1) for q1.
Therefore, any polynomial P of P3 is uniquely written as
P = P(0)p0 + P(1)p1 + P′(0)q0 + P′(1)q1.
Formulas (5.11) can then be checked by hand, see Fig.5.6.
□
In FEM language, the linear forms P →P(0), P →P(1), P →P′(0) and
P →P′(1) are the degrees of freedom of P3 Hermite interpolation on the reference
element [0, 1].
Once we have Hermite interpolation on the reference element [0, 1], we have
Hermite interpolation on any element [xi, xi+1] by a simple afﬁne change of variables:
p0
 x−xi
h

, p1
 x−xi
h

, hq0
 x−xi
h

and hq1
 x−xi
h

.
Lemma 5.2 There exists a unique continuous linear mapping Πh : H2
0(]a, b[) →
Vh, again called the Vh-interpolation operator such that for all v in H2
0(]a, b[), v(xi) =
Πhv(xi) and v′(xi) = (Πhv)′(xi) for i = 0, . . . , N + 1.

162
5
Variational Approximation Methods for Elliptic PDEs
Fig. 5.6 The four P3
Hermite basis polynomials
0
1
1
p0
p1
q0
q1
Proof First of all, we note that H2
0(]a, b[) →C1([a, b]), therefore the nodal values
v(xi) and v′(xi) are unambiguously deﬁned.
Now a P3 polynomial on [xi, xi+1] is uniquely determined by its values and the
values of its derivative at xi and xi+1, by Hermite interpolation. Thus, the relations
v(xi) = Πhv(xi) and v′(xi) = (Πhv)′(xi) for i = 0, . . . , N + 1 deﬁne a unique
piecewise P3 function on the mesh, that is globally C1 and vanishes at both ends
together with its derivatives, thus belongs to Vh. Let Πhv be this function. Clearly,
the mapping v →Πhv is linear from H1
0(]a, b[) into Vh. We leave the continuity to
the reader.
□
The above proof also shows that Vh ̸= {0}. We can now show an error estimate,
along the same lines as before.
Proposition 5.8 Assume c and f are continuous on [a, b]. Then the solution u of
problem (5.10) is of class C4([a, b]) and there exists a constant C independent of u
such that
∥u −uh∥V ≤Ch2 max
x∈[a,b] |u(4)(x)|.
Proof If f and c are continuous, since u is also continuous by Theorem3.7 of Chap.3,
then u(4) = cu −f is continuous on [a, b] and thus u ∈C4([a, b]). By Céa’s lemma,
i.e., Theorem5.1, we have
∥u −uh∥V ≤M
α ∥u −Πhu∥V .
We use the H2 semi-norm as a norm on V . We have
∥u −Πhu∥2
V =

 b
a
((u −Πhu)′′(x))2 dx =
N

i=0

 xi+1
xi
(u′′(x) −(Πhu)′′(x))2 dx.

5.4 A Fourth Order Example
163
Let us consider the function w = u −Πhu on [xi, xi+1]. By deﬁnition of Vh-
interpolation, we have w(xi) = w(xi+1) = 0. Since w is C1 on [xi, xi+1], Rolle’s
theorem applies and there exists c1 ∈]xi, xi+1[ such that w′(c1) = 0. Now, we also
have w′(xi) = w′(xi+1) = 0 by Vh-interpolation, and w′ is also of class C1 so that
Rolle applies again and there exists c2 < c1 < c3 such that w′′(c2) = w′′(c3) = 0.
We apply Rolle one last time since w′′ is C1 and obtain a point c4 ∈[xi, xi+1] such
that w′′′(c4) = 0. Consequently
w′′′(x) =

 x
c4
w(4)(t) dt =

 x
c4
u(4)(t) dt
for all x ∈[xi, xi+1], since Πhu is of degree at most three there, thus its fourth
derivative vanishes. It follows from this equality that
|w′′′(x)| ≤

 x
c4
|u(4)(t)| dt ≤

 xi+1
xi
|u(4)(t)| dt ≤h max
x∈[a,b] |u(4)(x)|,
for all x ∈[xi, xi+1]. We also have
w′′(x) =

 x
c2
w′′′(t) dt,
so that substituting the previous estimate yields
|w′′(x)| ≤h2 max
x∈[a,b] |u(4)(x)|.
Squaring and integrating, we thus see that

 xi+1
xi
(u′′(x) −(Πhu)′′(x))2 dx =

 xi+1
xi
(w′′(x))2 dx ≤h4(xi+1 −xi) max
x∈[a,b] |u(4)(x)|2.
Now we sum from i = 0 to N
∥u −Πhu∥2
V ≤h4(b −a) max
x∈[a,b] |u(4)(x)|2,
which completes the proof.
□
Remark 5.9 Under regularity hypotheses, we thus have convergence of the P3 Her-
mite FEM based on the smallness of the interpolation error. It should be noted that
this kind of proof relying on Rolle’s theorem is not very natural in a Sobolev space
context. There are better proofs using Hilbertian arguments.
□
Let us say a few words about bases and matrices. It is apparent that the operator
Πh only uses the nodal values of the function and its derivatives. Hence, any set of
interpolation data with N elements for the values and N elements for the derivative
values gives rise to one and only one element of Vh. We thus deﬁne

164
5
Variational Approximation Methods for Elliptic PDEs
Fig. 5.7 A wi
h basis function
Fig. 5.8 A zi
h basis function
Deﬁnition 5.3 For i = 1, . . . , N, let wi
h ∈Vh be deﬁned by
wi
h(x j) = δi j and (wi
h)′(x j) = 0,
and zi
h ∈Vh be deﬁned by
zi
h(x j) = 0 and (zi
h)′(x j) = δi j,
for j = 0, . . . , N + 1. We call these functions the basis functions for P3 Hermite
interpolation on the mesh.
The function wi
h is thus equal to 1 at xi and zero at all other nodes, with zero
derivatives at all nodes, whereas the function zi
h has derivative 1 at xi and zero at
all other nodes, with zero values at all nodes, see Figs.5.7 and 5.8. Clearly they are
constructed by pairing together the Hermite basis interpolation polynomials in each
element [xi−1, xi] and [xi, xi+1], which are also called shape functions in the FEM
context.
Proposition 5.9 The family (wi
h, zi
h)i=1,...,N is a basis of Vh. Thus dim Vh = 2N.
Moreover, we have the interpolation property
∀vh ∈Vh,
vh(x) =
N

i=1
vh(xi)wi
h(x) +
N

i=1
v′
h(xi)zi
h(x).
Proof Similar to the proof of Proposition5.3 but using P3 Hermite interpolation in
each element.
□
The supports of the basis functions are again [xi−1, xi+1], thus we can expect lots
of zero coefﬁcients in the matrix. We do not write the detail here. Let us just mention
that there is an issue of numbering. In the P1 Lagrange case, there was a natural
numbering of basis functions, which was that of the nodes. Here we have several
choices, leading to different matrices. If we choose to number the basis elements as
(w1
h, w2
h, . . . , wN
h , z1
h, z2
h, . . . , zN
h ), then the 2N × 2N matrix A is comprised of four

5.4 A Fourth Order Example
165
Fig. 5.9 Matrix structure.
Left block tridiagonal, right
interlaced
N × N blocks, and each one of the blocks is tridiagonal. If on the other hand, we
interlace the basis functions like (w1
h, z1
h, w2
h, z2
h, . . . , wN
h , zN
h ), we obtain a matrix
whose nonzero coefﬁcients are grouped around the diagonal, this is called a band
matrix. More precisely, each row of A has at most six nonzero coefﬁcients resulting
in seven nonzero diagonal rows: the diagonal plus three above the diagonal and three
under the diagonal, see Fig.5.9.
5.5
Neumann and Fourier Conditions
LetusbrieﬂyindicatehowtodealwithNeumannandFourierconditionsforthemodel
second order problem. There are several changes: the test-function space must not
enforce boundary conditions, i.e., V = H1(]a, b[), additional terms come up in the
right hand-side for both problems, and there is an additional term in the bilinear form
for the Fourier condition. Let us just consider the case c ≥c0 > 0. We thus let
Vh = {vh ∈C0([a, b]); vh|[xi,xi+1] ∈P1}.
Compared to the previous version of Vh, we have added two degrees of freedom
vh →v(a) and vh →v(b), hence dim Vh = N + 2. We must accordingly complete
the basis by adding two more basis functions w0
h and wN+1
h
deﬁned by w0
h(x j) = δ0 j
and wN+1
h
(x j) = δN+1, j for all j ∈{0, . . . , N + 1}, see Fig.5.10.
The variational formulation for the Fourier problem (replacing b by d in the
Fourier condition to avoid a conﬂict in notation with the boundary b) is
Fig. 5.10 The two
additional basis functions w0
h
left and wN+1
h
right
x0
x1
xN
xN+1

166
5
Variational Approximation Methods for Elliptic PDEs

 b
a
(u′v′ + cuv) dx + (duv)(a) + (duv)(b) =

 b
a
f v dx + (gv)(a) + (gv)(b).
We just set d = 0 for the Neumann problem. In matrix terms, the (N + 2) × (N + 2)
matrix A is still symmetric tridiagonal, and we have for c and d constants,
A00 = AN+1,N+1 = 1
h + h
3c + d
and
A01 = A10 = AN,N+1 = AN+1,N = −1
h + h
6c.
Of course
A0i = Ai0 = A j,N+1 = AN+1, j = 0
for i ≥2 and j ≤N −1. The other coefﬁcients are unchanged. The right-hand side
has two additional components B0 = h f
2 + g(a) and BN+1 = h f
2 + g(b).
Similar changes must be made to treat a fourth order Neumann problem.
In the next chapter, we consider the ﬁnite element method in two dimensions.
The abstract framework does not change, but the algebraic and geometric aspects are
considerably more elaborate.

Chapter 6
The Finite Element Method in Dimension
Two
It should already be clear that there is no difference between elliptic problems in one
dimensionandellipticproblemsinseveraldimensionsfromthevariationalviewpoint.
The same goes for the abstract part of variational approximations. The difference lies
in the description of the ﬁnite dimensional approximation spaces. The FEM in any
dimension of space is based on the same principle as in one dimension, that is to
say, we consider spaces of piecewise polynomials of low degree, with lots of pieces
for accuracy. Now things are right away quite different, and actually considerably
more complicated, since polynomials have several variables, and open sets are much
more varied than in dimension one. For simplicity, we limit ourselves to the two-
dimensional case.
6.1
Meshes in 2d
Let Ω be an open subset of R2. The idea is to cover ¯Ω with a ﬁnite number of closed
sets Tk of simple shape, ¯Ω = NT
k=1 Tk, with T = {Tk}1≤k≤NT and NT = card T .
This decomposition will be used to decompose integrals over Ω into sums of integrals
overtheTk,thusweimposethatmeas (Tk ∩Tk′) = 0fork ̸= k′.Wewillonlyconsider
two cases:
• The Tk are closed rectangles and ¯Ω is a union of rectangles. For deﬁniteness,
the sides of the rectangles will be parallel to the coordinate axes, without loss of
generality, see Fig.6.1.
• The Tk are closed triangles and ¯Ω is a polygon, see Fig.6.2.
Such a structure will be called a triangulation (even in the case of rectangles …)
or mesh on Ω. The Tk are called the elements, their sides are called the edges, and
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_6
167

168
6
The Finite Element Method in Dimension Two
Fig. 6.1 A rectangular mesh
Tk
Fig. 6.2 A triangular mesh
Tk
their vertices are called mesh nodes.1 The fact that ¯Ω must be a union of rectangles
or more generally a polygon can appear to be unduly restrictive. There are however
ways of going around this restriction and to cover very general domains, see [19, 21]
for example.
In practice, meshes in a domain are not given but must be constructed by computer.
This is a subject all by itself called automatic mesh generation. We will not pursue
this subject here, but refer the reader to [39] for example. See Fig.6.3 for a real-life
example of three-dimensional mesh.
From now on, ¯Ω will always be a polygon in R2.
Given a mesh T = {Tk}1≤k≤NT , we let h(Tk) = diam Tk = supx,y∈Tk ∥x −y∥and
h = max
Tk∈T h(Tk).
1There are often additional mesh nodes, as we will see later.

6.1 Meshes in 2d
169
Fig. 6.3 A real life mesh in
3d. The elements are
tetrahedra that ﬁll the
volume, we just see
triangular faces of those
tetrahedra that touch the
boundary. We will not talk
about 3d problems in this
book, even though most real
life problems occur in 3d.
The conceptual difference
between 3d and 2d is much
less marked than between 2d
and 1d
The scalar h is called the mesh size. The approximation spaces will thus be of the
form
Vh = {v ∈V ; v|Tk is a low degree polynomial},
to be made more precise later. This is again a case of bad traditional notation, since
Vh does not depend solely on h, but on the whole mesh, of which h is but one
characteristic length. Accordingly, when we say h →0, this means that we are given
a sequence (Tn)n∈N of meshes whose mesh size tends to 0 when n →+∞, and we
will use the classical notation Th instead. Naturally in practice, computer calculations
are executed on one or a small number of meshes. The convergence h →0 is only
for theoretical purposes.
In order to be of use, a triangulation must satisfy a certain number of properties.
Deﬁnition 6.1 A mesh is said to be admissible if
(i) For all k ̸= k′, Tk ∩Tk′ is either empty, or consists of exactly one node or of one
entire edge.
(ii) No Tk is of zero measure.
Condition (ii) means that no triangle or rectangle is degenerated, that is to say that
no element has all its vertices on a straight line. Condition (i) is easier to understand
in terms of which situations it precludes. For instance, any one of the three cases
shown on Fig.6.4 is forbidden.
For any triangle T, let ρ(T) be the diameter of the inscribed circle (the center of
the inscribed circle is called the incenter and is located at the intersection of the three
internal angle bissectors, see Fig.6.5).

170
6
The Finite Element Method in Dimension Two
Fig. 6.4 Forbidden meshes
according to rule (i)
Fig. 6.5 Triangle incircle
and diameter
ρ T
h T
T
Deﬁnition 6.2 Let Th be a sequence of triangular meshes whose mesh size h tends
to 0. We say that the sequence is a regular family if there exists a constant C > 0
such that for all h,
max
T∈Th
h(T)
ρ(T) ≤C.
For a sequence of meshes, not to be regular means that there are smaller and
smallertrianglesthatbecomearbitrarilyﬂat.Ofcourse,thedeﬁnitionneedsaninﬁnite
sequence of meshes to make sense. A similar condition for rectangular meshes is that
the ratio of the longer side by the smaller side of each rectangle remains bounded
from above. This property is needed for convergence results, as we will see in the

6.1 Meshes in 2d
171
proof of Lemma 6.4, p. 181. Note moreover that computations on a mesh such that
maxT∈Th
h(T)
ρ(T) is large may run into numerical difﬁculties, so the regularity condition
is also relevant from the practical point of view.
Let us now give a general purpose proposition on piecewise regular functions on
a mesh.
Proposition 6.1 Let T be an admissible mesh on Ω. Deﬁne
Xh = {v ∈C0( ¯Ω); v|Tk ∈C1(Tk) for all Tk ∈T }.
Then we have Xh ⊂H1(Ω) and ∂iv = NT
k=1 ∂i(v|Tk)1Tk for all v ∈Xh.
Proof Let v ∈Xh. Clearly, v ∈L2(Ω) and we just need to compute its partial deriv-
atives in the sense of distributions. Let us thus take an arbitrary function ϕ ∈D(Ω).
We have
⟨∂iv, ϕ⟩= −⟨v, ∂iϕ⟩= −

Ω
v∂iϕ dx = −
NT

k=1

Tk
v∂iϕ dx.
Now v is C1 on each Tk, which is a closed triangle or rectangle, therefore we can use
the integration by parts formula to obtain
−

Tk
v∂iϕ dx =

Tk
∂i(v|Tk)ϕ dx −

∂Tk
vnk,iϕ dΓ
=

Ω
∂i(v|Tk)1Tkϕ dx −

∂Tk
vnk,iϕ dΓ,
where nk denotes the unit exterior normal vector to ∂Tk. Note that since v ∈C0( ¯Ω)
there is no need to take the restriction of v to Tk in the boundary term. Summing on
all triangles or rectangles, we obtain
⟨∂iv, ϕ⟩=

Ω
 NT

k=1
∂i(v|Tk)1Tk

ϕ dx −
NT

k=1

∂Tk
vnk,iϕ dΓ.
Now, each ∂Tk is composed of three or four edges and there are two cases:
1. Either the edge in question is included in ∂Ω and in this case ϕ = 0, hence the
corresponding integral vanishes.
2. Or the edge is included in Ω (except possibly for one node) and in this case, by
condition (i) of mesh admissibility, see Deﬁnition 6.1, this edge is the intersection
of exactly two elements Tk and Tk′. The two integrals corresponding to this edge
cancel out each other, since vϕ is continuous, it takes the same value on ∂Tk ∩∂Tk′
as seen from either side, and nk = −nk′, see Fig.6.6.

172
6
The Finite Element Method in Dimension Two
Fig. 6.6 Pairwise
cancellation of edge integrals
Finally, we see that
NT

k=1

∂Tk
vnk,iϕ dΓ = 0
and since the function NT
k=1 ∂i(v|Tk)1Tk is bounded, it is also in L2(Ω).
□
Remark 6.1 The above proof shows that in fact Xh ⊂W 1,∞(Ω).
□
6.2
Rectangular Q1 Finite Elements
We start over with the model problem
−Δu + cu = f in Ω,
u = 0 on ∂Ω,
(6.1)
with f ∈L2(Ω), c ∈L∞(Ω), c ≥0 and Ω = ]0, 1[×]0, 1[. The variational formula-
tion is of course of the general form (4.6) with V = H1
0(Ω), a(u, v) =
	
Ω(∇u · ∇v + cuv) dx and ℓ(v) =
	
Ω f v dx.
Let us be given two positive integers N1 and N2 and let h1 =
1
N1+1 and h2 =
1
N2+1.
We deﬁne a rectangular mesh on Ω by setting
Rk = {(x1, x2); i1h1 ≤x1 ≤(i1 + 1)h1, i2h2 ≤x2 ≤(i2 + 1)h2,
i1 = 0, . . . , N1, i2 = 0, . . . , N2}.
The elements are rectangles with sides parallel to the coordinate axes and of lengths
h1 and h2, see Fig.6.7. There are NT = (N1 + 1)(N2 + 1) elements. In the above
formula, we have k = 1, . . . , NT . As we will see later on, the actual numbering of
the rectangles, i.e., the function (i1, i2) →k, is largely irrelevant. The mesh size is

6.2 Rectangular Q1 Finite Elements
173
Fig. 6.7 A rectangular mesh
on Ω = ]0, 1[2, 32 elements,
45 nodes
x1
x2
h1
h2
h =

h2
1 + h2
2. Actually, since max(h1, h2) ≤h ≤
√
2 max(h1, h2), we may as well
take h = max(h1, h2). The inscribed circle has diameter min(h1, h2), so the regularity
requirement for a family of such meshes would be that max(h1,h2)
min(h1,h2) ≤C, or roughly
speaking that N1 and N2 be of the same order of magnitude.
The mesh nodes are the points (i1h1, i2h2), i1 = 0, . . . , N1 + 1, , i2 = 0, . . . ,
N2 + 1. There is a total of Ntot = (N1 + 2)(N2 + 2) nodes, including 2(N1 + 1) +
2(N2 + 1) = 2(N1 + N2) + 4 = Nbdy boundary nodes located on ∂Ω and Nint =
N1N2 interior nodes located in Ω. Of course, Ntot = Nint + Nbdy. We will talk about
numbering issues later (numbering of nodes, numbering of elements).
Let us now talk about the discrete approximation space. We ﬁrst state a few facts
about the algebra of polynomials in several variables. First of all, there are several
notions of degree for such polynomials. The total degree of a nonzero monomial
in two variables axn
1xm
2 is n + m (and the obvious generalization for more variables,
that we will not use here). The total degree of a polynomial is the maximum total
degree of its monomials. The partial degree of the same monomial is max(n, m).
The partial degree of a polynomial is the maximum partial degree of its monomials.
Since we are working on an inﬁnite number ﬁeld, R, we can identify polynomials
and polynomial functions on an open set of R2. We will perform this identiﬁcation
freely.
There are two families of spaces of polynomials that will be of interest to us.
Deﬁnition 6.3 For each k ∈N, we denote by Pk the space of polynomials of total
degree less than or equal to k and by Qk the space of polynomials of partial degree
less than or equal to k.

174
6
The Finite Element Method in Dimension Two
Both spaces obviously are vector spaces. It is an easy exercise in algebra to establish
that dim Pk = (k+1)(k+2)
2
and dim Qk = (k + 1)2.
Since the total degree of a polynomial is always larger than its partial degree, it
follows that Pk ⊂Qk. Moreover, as the Qk monomial xk
1xk
2 is clearly of the highest
possible total degree, we also have Qk ⊂P2k. The only value of k for which these
spaces coincide is thus k = 0, with only constant polynomials. The space P1 is the
space of afﬁne functions
P1 = {p; p(x) = a0 + a1x1 + a2x2, ai ∈R}
and the space Q1 is described in terms of its canonical basis
Q1 = {p; p(x) = a0 + a1x1 + a2x2 + a3x1x2, ai ∈R}.
We can now introduce the corresponding approximation spaces. We start with a
version without boundary conditions
Wh = {vh ∈C0( ¯Ω); ∀Rk ∈T , vh|Rk ∈Q1},
and the subspace thereof that includes homogeneous Dirichlet conditions
Vh = {vh ∈Wh; vh = 0 on ∂Ω},
see Remark 3.23 of Chap.3.
The space Wh thus consists of globally continuous functions the restriction of
which to each element coincides with one Q1 polynomial per element. It is the same
ideaasindimension1.SinceQ1 polynomialsareofcourseofclassC1,Proposition6.1
immediately implies
Proposition 6.2 We have Wh ⊂H1(Ω) and Vh ⊂H1
0(Ω).
We now establish interpolation results for Q1 polynomials and piecewise Q1 func-
tions. We start with a uniqueness result.
Proposition 6.3 A function of Wh is uniquely determined by its values at the nodes
of the mesh.
Proof A function vh in Wh is uniquely determined by the values it takes in each
rectangular element, that is to say by the NT polynomials in Q1 that correspond to
each element. It is thus sufﬁcient to argue element by element. Let R be such an
element and Si = (xi
1, xi
2) be its four vertices numbered counterclockwise starting
from the lower left corner, see Fig.6.8.
We have h1 = xi
1 −x1
1 for i = 2, 3 and h2 = xi
2 −x1
2 for i = 3, 4. Since vh is equal
to a Q1 polynomial in R, there exists four constants α j, j = 1, . . . , 4 such that
vh(x) = α1 + α2(x1 −x1
1) + α3(x2 −x1
2) + α4(x1 −x1
1)(x2 −x1
2).

6.2 Rectangular Q1 Finite Elements
175
Fig. 6.8 A generic rectangle
R in the mesh
S3
S2
S4
S1
R
x1
x2
Let us express the values of vh at the four vertices.
vh(S1) = α1
vh(S2) = α1 + α2(x2
1 −x1
1) + α3(x2
2 −x1
2) + α4(x2
1 −x1
1)(x2
2 −x1
2)
= α1 + α2h1
since x2
2 = x1
2,
vh(S4) = α1 + α3h2
vh(S3) = α1 + α2h1 + α3h2 + α4h1h2.
This is a 4 × 4 linear system in the four unknowns α j which we can rewrite in matrix
form
⎛
⎜⎜⎝
1 0 0
0
1 h1 0
0
1 0 h2
0
1 h1 h2 h1h2
⎞
⎟⎟⎠
⎛
⎜⎜⎝
α1
α2
α3
α4
⎞
⎟⎟⎠=
⎛
⎜⎜⎝
vh(S1)
vh(S2)
vh(S4)
vh(S3)
⎞
⎟⎟⎠.
The determinant of the triangular matrix above is h2
1h2
2 ̸= 0, hence the system has
one and only one solution for any given vertex values for vh. Therefore, we have the
announced uniqueness.
□
We also have an existence result.
Proposition 6.4 For any set of values assigned to the nodes of the mesh, there exists
one and only one element vh of Wh that takes these values at the nodes.
Proof The previous proof shows that four values for the four vertices of an element
determine one and only one Q1 polynomial that interpolates the values at the vertices
inside the element. Therefore, if we are given a set of values for each node in the

176
6
The Finite Element Method in Dimension Two
Fig. 6.9 Continuity across
an internal edge [S1, S2]
S2
S1
Rk
Rk′
q′
q
mesh, this set determines one and only one Q1 polynomial per element. The only
thing to be checked is that these polynomials combine into a globally C0 function.
Indeed, discontinuities could arise at internal edges, those that are common to two
elements. We have to show that this is not the case.
Let us thus consider the situation of Fig.6.9, where the common edge between
the two rectangles is parallel to the x2 axis, without loss of generality.
We thus have two Q1 polynomials q and q′ such that q(S1) = q′(S1) and q(S2) =
q′(S2). We can write
(q −q′)(x) = α1 + α2(x1 −x1
1) + α3(x2 −x1
2) + α4(x1 −x1
1)(x2 −x1
2),
for some constants α j, j = 1, . . . , 4.
Now, any point on the segment [S1, S2] is such that x1 = x1
1. Therefore, on this
segment, we have
(q −q′)(x) = α1 + α3(x2 −x1
2).
Now, (q −q′)(S1) = 0 implies α1 = 0 and then (q −q′)(S2) = 0 implies α3 = 0.
Consequently, (q −q′)|[S1,S2] vanishes identically and thus, the function deﬁned by
q(x) if x ∈Rk, q′(x) if x ∈Rk′ \ Rk is continuous on Rk ∪Rk′.
□
Remark 6.2 Notice that, in the above proof, the global continuity follows from the
fact that Q1 polynomials are afﬁne on any segment that is parallel to the coordinate
axes. If two such polynomials coincide at two points of such a segment, they then
coincide on the whole straight line going through the two points. Of course, they are
in general not afﬁne on any segment that is not parallel to the coordinate axes.
□
Corollary 6.1 Let S j, j = 1, . . . , Ntot, be a numbering of the mesh nodes. There
exists a unique family (wi
h)i=1,...,Ntot such that wi
h ∈Wh and wi
h(S j) = δi j. This family
is a basis of Wh, which is of dimension Ntot, and for all vh ∈Wh, we have
vh =
Ntot

i=1
vh(Si)wi
h.
(6.2)
Proof The existence and uniqueness of wi
h follow readily from Propositions 6.3 and
6.4, since for all i, {δi j; 1 ≤j ≤Ntot} is a set of values for all the nodes.

6.2 Rectangular Q1 Finite Elements
177
These propositions also show that the linear mapping Wh →RNtot, vh →(vh(Si))
is an isomorphism, hence dim Wh = Ntot. The family (wi
h)i=1,...,Ntot is the inverse
image of the canonical basis of RNtot by this isomorphism, therefore it is a basis of
Wh. Finally, every element vh of Wh is decomposed on this basis as vh = Ntot
i=1 λiwi
h,
so that taking x = S j, we obtain
vh(S j) =
Ntot

i=1
λiwi
h(S j) =
Ntot

i=1
λiδi j = λ j
which establishes Eq.(6.2).
□
We can now characterize the elements of Vh, i.e., those functions of Wh that vanish
on ∂Ω.
Corollary 6.2 Assume, for convenience only, that the nodes S j, j = 1, . . . , Nint are
theinteriornodes.Thenthefamily(wi
h)i=1,...,Nint isabasisof Vh,and Vh isofdimension
Nint.
Proof If a function is in Vh, then vh(S j) = 0 for j > Nint. Therefore, we necessarily
have
vh =
Nint

i=1
vh(Si)wi
h.
It remains to be seen that wi
h ∈Vh for i ≤Nint. This is clear, since these functions
vanish on all boundary nodes. Hence by the same token as before, they vanish on all
the edges joining boundary nodes, and the whole boundary ∂Ω is composed of such
edges.
□
Remark 6.3 The functions wi
h are called the basis functions for Q1 Lagrange inter-
polation. The linear mappings vh →vh(S j) are again called the degrees of freedom.
It is easy to see that the support of wi
h is composed of the four elements surrounding
Si when Si is an interior node, see Fig.6.14, two elements when it is a boundary node,
but not a vertex of Ω, and just one element when it is one of the four vertices of Ω.
The graph of a basis function corresponding to an interior node over its support
is made of four hyperbolic paraboloid pieces that look like a tent,2 see Fig.6.10. □
In Figs.6.11 and 6.12, we show pictures of elements of Vh.
6.3
Convergence and Error Estimate for the Q1 FEM
The approximation space Vh is ﬁnite-dimensional, therefore closed, hence Céa’s
lemma, i.e., Theorem 5.1 of Chap.5, applies and we denote by uh the solution of
the discrete variational problem (5.1). We thus need to estimate such quantities as
2Which is why they are sometimes called tent-functions.

178
6
The Finite Element Method in Dimension Two
Fig. 6.10 Two views of a Q1
basis function for an interior
node
Fig. 6.11 The graph of a
random element of Vh. The
fact that functions in Vh are
piecewise afﬁne on segments
parallel to the coordinate
axes is apparent, see Sect.6.4
Fig. 6.12 The graph of the
Vh-interpolate of the
function (x1, x2) →
sin(πx1) sin(πx2)
∥u −Πhu∥H1(Ω), where Πh is some interpolation operator with values in Vh in order
to obtain an error estimate and prove convergence. We now encounter a new difﬁculty,
which is that H1 functions are not continuous in two dimensions, therefore, the nodal
values of u a priori do not make any sense and it is not possible to perform any kind
of Lagrange interpolation on H1.
We will thus need to make regularity hypotheses. We will admit the following
particular case of the Sobolev embedding theorems, which is valid in dimension two,
see for example [1].
Theorem 6.1 There is a continuous embedding H2(Ω) →C0( ¯Ω).
With this theorem at hand, we can Vh-interpolate H2 functions.
Let us thus be given a regular family of admissible meshes, that we index by
h = max(h1, h2), regularity meaning here that there exists a constant C such that
max(h1,h2)
min(h1,h2) ≤C. Let u be the solution of problem (6.1) in variational form and uh its

6.3 Convergence and Error Estimate for the Q1 FEM
179
variational approximation on Vh. We will prove the following convergence and error
estimate theorem.
Theorem 6.2 There exists a constant C such that, if u ∈H2(Ω), we have
∥u −uh∥H1(Ω) ≤Ch|u|H2(Ω).
The constant C is naturally not the same constant as a couple of lines higher. Actually,
the proof of Theorem 6.2 will be broken into a series of lemmas, and constants C
will come up that generally vary from line to line. This is what is called a generic
constant…. The important thing is not their actual value, but that they do not depend
on any of the other quantities that appear, in this speciﬁc case, h and u.
Let R = [0, 1] × [0, 1] be the reference rectangle3 or reference element. Its four
verticesS j, j = 1, . . . , 4, are (0, 0), (1, 0), (1, 1) and (0, 1). We let 
Π denote the Q1
interpolation operator on the four vertices S j. The Q1 Lagrange interpolation basis
polynomials, or shape functions, on the reference rectangle are
ˆp1(ˆx) = (1 −ˆx1)(1 −ˆx2), ˆp2(ˆx) = ˆx1(1 −ˆx2),
ˆp3(ˆx) = ˆx1ˆx2, ˆp4(ˆx) = (1 −ˆx1)ˆx2,
(6.3)
as can be checked by hand. For all ˆv ∈C0(R), we thus have

Π ˆv =
4

j=1
ˆv(S j)ˆp j.
(6.4)
Let us begin our series of lemmas.
Lemma 6.1 The operator 
Π is continuous from H2(R) to H1(R).
Proof We equip Q1 with the H1(R) norm (recall that all norms are equivalent on Q1
since it is ﬁnite dimensional). By Theorem 6.1, we have for all ˆv ∈H2(R)
∥ˆv∥C0(R) ≤C∥ˆv∥H2(R),
for some constant C. By formula (6.4), we have
∥
Π ˆv∥H1(R) ≤
4

j=1
|ˆv(S j)|∥ˆp j∥H1(R)
≤
 4

j=1
∥ˆp j∥H1(R)

∥ˆv∥C0(R)
3Ok, it’s a square, and unluckily it happens to look a lot like Ω, although there is no conceptual
connection between the two.

180
6
The Finite Element Method in Dimension Two
≤C
 4

j=1
∥ˆp j∥H1(R)

∥ˆv∥H2(R),
which completes the proof.
□
Lemma 6.2 There exists a constant C such that, for all ˆv ∈H2(R)
∥ˆv −
Π ˆv∥H1(R) ≤C∥∇2ˆv∥L2(R).
Proof We note that P1 ⊂Q1, thus for all p ∈P1, we have 
Πp = p. Therefore
∥ˆv −
Π ˆv∥H1(R) = ∥ˆv −p −
Π(ˆv −p)∥H1(R) ≤∥I −
Π∥L (H2;H1)∥ˆv −p∥H2(R)
for all ˆv ∈H2(R), p ∈P1, by Lemma 6.1. Consequently
∥ˆv −
Π ˆv∥H1(R) ≤C inf
p∈P1 ∥ˆv −p∥H2(R) = C∥ˆv −Pˆv∥H2(R),
where P denotes the H2 orthogonal projection onto P1.
Let us now show that there is a constant C such that
∥ˆv −Pˆv∥H2(R) ≤C∥∇2ˆv∥L2(R),
which will complete the proof of the Lemma. We argue by contradiction and assume
there is no such constant C. In this case, as in the proof of the Poincaré–Wirtinger
inequality, i.e., Theorem 4.2 of Chap.4, there exists a sequence ˆvn ∈H2(R) such that
∥ˆvn −Pˆvn∥H2(R) = 1 and ∥∇2ˆvn∥L2(R) →0,
when n →+∞. Let us set ˆwn = ˆvn −Pˆvn, which belongs to P⊥
1 . The second deriva-
tives of a P1 polynomial vanish, so that ∇2 ˆwn = ∇2ˆvn. By Rellich’s compact embed-
ding theorem, see Remark 3.18 of Chap.3, there exists a sequence, still denoted ˆwn
and a ˆw ∈H1(R) such that ˆwn →ˆw in H1(R). Then, the condition ∥∇2 ˆwn∥L2(R) →0
shows that ˆwn is a Cauchy sequence in H2(R), which is complete. Hence, ˆw ∈H2(R)
and ˆwn →ˆw in H2(R) as well. Now, the space P⊥
1 is a H2 orthogonal, hence is
closed in H2(R), from which it follows that ˆw ∈P⊥
1 . On the other hand, we have
∇2 ˆw = 0, so that ˆw ∈P1. Consequently, ˆw = 0 and ˆwn →0 in H2(R), which con-
tradicts ∥ˆwn∥H2(R) = 1. The proof is complete.
□
Remark 6.4 The above proof does not really use Q1-interpolation, but only P1-
interpolation. It would thus equally apply for triangular elements, which we will
discuss later. The proof is non constructive in the sense that we do not have any idea
of the actual value of the constant.
□
We now perform a change of variable between the reference element R and a
generic element Rk of the mesh.

6.3 Convergence and Error Estimate for the Q1 FEM
181
Fig. 6.13 The afﬁne change
of variable from the
reference element to the
generic element
R
Rk
S1
(0,0)
Lemma 6.3 Let Rk be an element of the mesh. There exists a unique afﬁne bijective
mapping Fk such that Fk(R) = Rk and that maps the vertices counted counterclock-
wise from the lower left corner to their counterparts in Rk.
Proof Consider Fig.6.13.
Inviewoftheﬁgure,itisclearlyenoughtomaptheorigintopointS1 ofcoordinates
(x1(Rk), x2(Rk)) and then to multiply abscissae by h1 and ordinates by h2. This yields
Fk(ˆx) =
x1(Rk) + h1ˆx1
x2(Rk) + h2ˆx2

.
The inverse mapping is given by
F−1
k (y) =
 y1−x1(Rk)
h1
y2−x2(Rk)
h2

.
It is also afﬁne, naturally.
□
Lemma 6.4 There exists a constant C such that for all elements Rk and all v ∈
H1(Rk), setting ˆv(ˆx) = v(Fk(ˆx)), we have

Rk
∥∇v∥2 dx ≤C

R
∥∇ˆv∥2 dˆx.
Proof This is brute force computation. We have v(x) = ˆv(F−1
k (x)) thus
∂v
∂xi
(x) =
2

j=1
∂ˆv
∂ˆx j
(F−1
k (x))∂(F−1
k ) j
∂xi
(x)
= h−1
i
∂ˆv
∂ˆxi
(F−1
k (x)),

182
6
The Finite Element Method in Dimension Two
by the multidimensional chain rule. We also need the Jacobian of the change of
variables x = Fk(ˆx)
dx = | det DFk(ˆx)| dˆx = h1h2 dˆx
to perform the change of variable in the integral. We obtain

Rk
∥∇v∥2 dx =

R

h−2
1
 ∂ˆv
∂ˆx1
2
+ h−2
2
 ∂ˆv
∂ˆx2
2
h1h2 dˆx
≤(min(h1, h2))−2h1h2

R
∥∇ˆv∥2 dˆx.
Now this is where the regularity of the mesh family intervenes. According to
Deﬁnition 6.2, letting h = max(h1, h2) and ρ = min(h1, h2), we have h
ρ ≤C. There-
fore (min(h1, h2))−2h1h2 ≤C2
h2 h2 = C2, and the proof is complete.
□
We now are in a position to prove Theorem 6.2.
Proof of Theorem 6.2. We use the H1 semi-norm. Let Πh be the Vh-interpolation
operator and let vk = (u −Πhu)|Rk and uk = u|Rk. It is important to note that


Πhu|Rk

= 
Π 
u|Rk = 
Π 
uk,
using the same hat notation as in Lemma 6.4 for the change of variables in functions.
This is because an afﬁne change of variables of the form of Fk maps Q1 polynomials
to Q1 polynomials due to their special structure. Moreover, the two sides of the above
equality satisfy the same interpolation conditions at the four vertices of the reference
element, hence are equal everywhere. Therefore, we have
vk = 
uk −
Π 
uk.
We decompose the semi-norm squared as a sum over all elements
|u −Πhu|2
H1(Ω) =
NT

k=1

Rk
∥∇vk∥2 dx ≤C
NT

k=1

R
∥∇vk∥2 dˆx,
by Lemma 6.4.
By Lemma 6.2, we have

R
∥∇vk∥2 dˆx ≤C

R
∥∇2
uk∥2 dˆx
≤C
2

i, j=1

R
 ∂2
uk
∂ˆxi∂ˆx j
2
dˆx

6.3 Convergence and Error Estimate for the Q1 FEM
183
= C
2

i, j=1

Rk

hih j
∂2uk
∂xi∂x j
2
1
h1h2
dx
≤Ch2

Rk
∥∇2uk∥2 dx
by performing the reverse change of variables, and using the regularity of the mesh
family again. It follows that
|u −Πhu|2
H1(Ω) ≤Ch2
NT

k=1

Rk
∥∇2uk∥2 dx = Ch2∥∇2u∥2
L2(Ω),
and the proof is complete since the H1 semi-norm is equivalent to the H1 norm on
H1
0(Ω), see Corollary 3.3 of Chap.3.
□
Remark 6.5 Under the hypothesis u ∈H2(Ω), which is satisﬁed in this particular
case, due to elliptic regularity in a convex polygon, we thus have convergence of the
Q1 FEM when h →0, and we have an error estimate with a constant C that depends
neither on h nor on u. The drawback however is that the proof does not tell us how
large this constant is, see Remark 6.4.
□
6.4
Assembling the Matrix
Let us assume that a numbering of the interior nodes, and thus of the basis functions
of Vh, has been chosen: S j and w j
h, j = 1, . . . , Nint. We have, by Q1 interpolation
uh =
Nint

j=1
uh(S j)w j
h
and the matrix A has coefﬁcients
Ai j = a(w j
h, wi
h) =

Ω
(∇w j
h · ∇wi
h + cw j
hwi
h) dx.
If we set
Ai j(Rk) =

Rk
(∇w j
h · ∇wi
h + cw j
hwi
h) dx,
we see that
Ai j =
NT

k=1
Ai j(Rk),

184
6
The Finite Element Method in Dimension Two
and the coefﬁcients can thus be computed element-wise. The idea is that many of the
numbers Ai j(Rk) do not need to be computed, since it is known that they vanish as
soon as the intersection of the supports of w j
h and wi
h does not meet Rk. This vastly
reduces the computer load.
Likewise, the right-hand side of the linear system can be written as
Bi =

Ω
f wi
h dx =
NT

k=1

Rk
f wi
h dx =
NT

k=1
Bi(Rk),
with only four nonzero terms in the last sum.
Now the restriction of w j
h to Rk is either zero, or one of the four Q1 interpolation
basis polynomials on Rk, which we denote pk
i , i = 1, . . . , 4. Here again, the reference
element R can be used with proﬁt to compute the coefﬁcients of the matrix. The
Q1 Lagrange interpolation basis polynomials, or shape functions, on the reference
rectangle are given by formula (6.3).
We have already noticed that pk
i (x) = ˆpi(F−1
k (x)) because both sides are Q1 and
satisfy the same interpolation conditions at the vertices. Let us give an example of
computation with ˆp3. We thus have
pk
3(x) = ˆp3(F−1
k (x)) =
x1 −x1(Rk)
h1
x2 −x2(Rk)
h2

.
Therefore
∥∇pk
3(x)∥2 =
1
h2
1h2
2

(x1 −x1(Rk))2 + (x2 −x2(Rk))2
,
and assuming Si is the upper right corner of Rk, we obtain by computing the integrals
on Rk
Aii(Rk) = h1h2
3
 1
h2
1
+ 1
h2
2

+ c0
h1h2
9
in the case when c = c0 is a constant. Now there are four such contributions to Aii
coming from the four rectangles that surround Si (see Fig.6.14), which are all equal,
hence
Aii = 4h1h2
3
 1
h2
1
+ 1
h2
2

+ c0
4h1h2
9
.
In the case when h1 = h2 = h, we thus obtain Aii = 8
3 + 4
9c0h2.
The diagonal coefﬁcients do not depend on the node numbering, but the off-
diagonal ones do depend completely on it. So we have to talk about numbering,
since in the 2d case, as opposed to the 1d case, no natural numbering appears at the
onset.
We ﬁrst note that there is a connection between Q1 Lagrange approximation in
two dimensions and P1 Lagrange approximation in one dimension.

6.4 Assembling the Matrix
185
The four basis polynomials on R are given by Eq.(6.3). In one dimension, the
basis polynomials for P1 Lagrange interpolation on [0, 1] are
ℓ1(x) = 1 −x,
ℓ2(x) = x.
Therefore, we see that
ˆp1(x) = ℓ1(ˆx1)ℓ1(ˆx2), ˆp2(x) = ℓ2(ˆx1)ℓ1(ˆx2),
ˆp3(x) = ℓ2(ˆx1)ℓ2(ˆx2), ˆp4(x) = ℓ1(ˆx1)ℓ2(ˆx2).
In this context, we introduce a useful notation. Let f and g be two functions in one
variable. We deﬁne a function in two variables f ⊗g by f ⊗g(x1, x2) = f (x1)g(x2).
This function is called the tensor product of f and g.4 With this notation, we thus
have p1 = ℓ1 ⊗ℓ1 and so on.
ThistensorproductdecompositionextendstothebasisfunctionsonΩ themselves.
Let Si be an interior node of coordinates (i1h1, i2h2) and Ri
k, k = 1, . . . , 4, the four
elements surrounding it.
By direct veriﬁcation of the interpolation relations, we easily check that
wi
h(x) =
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
ℓ2
 x1
h1 −(i1 −1)

ℓ2
 x2
h2 −(i2 −1)

in Ri
1,
ℓ1
 x1
h1 −i1

ℓ2
 x2
h2 −(i2 −1)

in Ri
2,
ℓ1
 x1
h1 −i1

ℓ1
 x2
h2 −i2

in Ri
3,
ℓ2
 x1
h1 −(i1 −1)

ℓ1
 x2
h2 −i2

in Ri
4,
0
elsewhere.
We remark that the support of wi
h is the union of the four rectangles surrounding Si,
see Fig.6.14. Therefore, if wi1
h1 denotes the 1d hat function associated with node i1h1
of the 1d mesh of [0, 1] of mesh size h1, and likewise for wi2
h2, we see that
Fig. 6.14 The support of wi
h
Ri
1
Ri
2
Ri
3
Ri
4
Si
i1h1
(i1 −1)h1
(i1 +1)h1
(i2 +1)h2
(i2 −1)h2
i2h2
4Consider this to be just vocabulary. We do not need to know anything about tensor products in
general.

186
6
The Finite Element Method in Dimension Two
wi
h = wi1
h1 ⊗wi2
h2.
In other words, the basis functions of the Q1 FEM in 2d are nothing but the tensor
products of the one-dimensional P1 basis functions cf. also Fig.6.14.
Let us use this tensor product decomposition to number the basis functions. The
idea is to use the indices i1 and i2 to sweep the rows and then the columns of the
mesh.5 We thus deﬁne a mapping {1, 2, . . . , N1} × {1, 2, . . . , N2} →{1, 2, . . . , Nint}
by
(i1, i2) →i = i1 + (i2 −1)N1.
It is clearly a bijection (recall that Nint = N1N2). To compute the inverse mapping,
we note that i1 −1 is the remainder of the Euclidean division of i −1 by N1, thus
i1 = i −
i −1
N1
 
N1,
i2 =
i −1
N1
 
+ 1.
(6.5)
Now, the support of a tensor product is the Cartesian product of the supports. Thus
supp wi
h = supp wi1
h1 × supp wi2
h2 = [(i1 −1)h1, (i1 + 1)h1] × [(i2 −1)h2, (i2 + 1)h2].
If an index j in the numbering corresponds to a couple ( j1, j2), we thus see that
Ai j ̸= 0 if and only if the supports have non negligible intersection, that is to say
Ai j ̸= 0 ⇐⇒| j1 −i1| ≤1 and | j2 −i2| ≤1.
In view of the numbering formulas above, saying that | j1 −i1| ≤1 is equivalent
to saying that j −i = α + kN1 with α = i1 −j1 = −1, 0 or 1, and k an integer.
Since we also have i2 = i−i1
N1 , it follows that i2 −j2 = k = −1, 0 or 1. Therefore,
for a given i, that is a given row of A, there are at most nine values of j, that is
nine columns, that contain a nonzero coefﬁcient. Of course, not all rows contain
nine nonzero coefﬁcients. For example, the ﬁrst row has four nonzero coefﬁcients,
the second row has six nonzero coefﬁcients, and so on. Rows that correspond to
index pairs (i1, i2) such that 2 ≤i1, i2 ≤N1 −1 do have nine nonzero coefﬁcients
(they correspond to interior nodes with nine neighboring interior nodes, including
themselves). Such a row looks like Fig.6.15.
We see three tridiagonal N1 × N1 blocks emerging, that are themselves arranged
block tridiagonally. The whole (N1N2) × (N1N2) matrix is thus composed of N2
2
blocks Akl, 1 ≤k, l, ≤N2, of size N1 × N1, that are either zero or tridiagonal. Indeed,
if we deﬁne the N1 × N1 matrix Akl to be the block comprised of lines (k −1)N1 + 1
to kN1 and columns (l −1)N1 + 1 to lN1, then using the inverse numbering (6.5),
we see that
Ai j = a(w j
h, wi
h) = a(w j1
h1 ⊗wl
h2, wi1
h1 ⊗wk
h2),
5Or the other way around. But let’s stick to this one here.

6.4 Assembling the Matrix
187
Fig. 6.15 A typical row in
the matrix
j =
i
i+1
i−1
i+N1
i+N1+1
i+N1−1
i−N1
i−N1+1
i−N1−1
for all (i, j) in this block. Therefore we have (Akl)i1 j1 = a(w j1
h1 ⊗wl
h2, wi1
h1 ⊗wk
h2),
thus Akl = 0 as soon as |k −l| ≥2 and is tridiagonal for |k −l| ≤1, for reasons of
supports. We thus have
A =
⎛
⎜⎜⎜⎜⎜⎜⎝
A11 A12 0
· · ·
0
A21 A22 A23
· · ·
0
0 A32 A33
...
0
...
...
...
...
...
0
· · ·
0 AN2−1,N2 AN2N2
⎞
⎟⎟⎟⎟⎟⎟⎠
,
where the block tridiagonal structure appears, see also Fig.6.16.
The sweep columns then rows numbering scheme thus gives rise to a well-
structured matrix for which there exist efﬁcient numerical methods. It is instructive to
see what kind of matrix would result from other numberings that could be considered
just as natural, such as the numbering used to prove that N2 is countable (although
limited to a square here): start from the lower left node, go east one node, then north
west, then north, then south east, etc., see Fig.6.17.
Fig. 6.16 The block
tridiagonal structure of A
(here N1 = N2 = 10 so A is
100 × 100). Black squares
indicate nonzero matrix
coefﬁcients, white areas
zero. The coarse grid shows
the 10 × 10 blocks

188
6
The Finite Element Method in Dimension Two
Fig. 6.17 An alternate node
numbering scheme
1
2
3
4
5
6
···
20
...
For the same 100 × 100 case, we obtain a matrix structure that looks like Fig.6.18.
Of course, the entries of the above matrix are the same as the previous ones after a
permutation, since both matrices are similar to each other via a permutation matrix.
In the case when Ω is not a rectangle, the structure of the matrix is not as regular.
For instance, the matrix associated with the mesh depicted in Fig.6.1, with the sweep
columns then rows numbering, looks like
A =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
■■□□■■□□□□□
■■■□■■■□□□□
□■■■□■■□□□□
□□■■□□■□□□□
■■□□■■□■■□□
■■■□■■■■■□□
□■■■□■■□■□□
□□□□■■□■■■■
□□□□■■■■■■■
□□□□□□□■■■■
□□□□□□□■■■■
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
,
where black squares denote nonzero entries and white squares zero entries.
Let us notice that the structure of the matrix depends solely on the numbering of
nodes, and not on the numbering of elements.

6.5 General Deﬁnition of a Finite Element
189
Fig. 6.18 Structure of the
alternate matrix
6.5
General Deﬁnition of a Finite Element
It is now time to look back and see what are the general characteristics of the ﬁnite
elements we have seen so far, so as to ﬁnally deﬁne what a ﬁnite element is! We let
P = R[x1, x2] denote the space of polynomials in two indeterminates.
Deﬁnition 6.4 A two-dimensional ﬁnite element is a triple (T, P(T), {ϕ1, . . . , ϕd})
where
(1) T is a compact polygon.
(2) P(T) is a ﬁnite dimensional subspace of P, considered as a function space on T.
(3) ϕi, i = 1, . . . , d, are linear forms on P, which are called the degrees of freedom
of the ﬁnite element.
Remark 6.6 In practice, T is either a triangle or a rectangle. The same deﬁnition
applies in dimensions one and three (ﬁnite elements are rarely used in dimensions
higher than four, although it happens). In dimension one, T is an interval. There is
more variety in dimension three, starting with tetrahedra.
The degrees of freedom are attached to T one way or another.
In the literature, in particular the engineering literature, ﬁnite elements are always
presented this way, and not starting with the discrete space Vh and so on, as we have

190
6
The Finite Element Method in Dimension Two
done up to now. We thus start from the top, i.e. from the discrete space, down to the
ﬁnite element, instead of starting from the bottom.
□
Deﬁnition 6.5 We say that a ﬁnite element is unisolvent if for all d-uples of scalars
(α1, . . . , αd), there exists one and only one polynomial p ∈P(T) such that ϕi(p) =
αi, i = 1, . . . , d.
Unisolvence is a generalization of the interpolation property for all kinds of
degrees of freedom.
Proposition 6.5 If a ﬁnite element is unisolvent, then, d = dim P(T).
Proof This is fairly obvious. Assume we want to solve the d equations ϕi(p) = αi.
Since ϕi are linear forms, these equations are linear equations in dim P(T) unknowns,
once we choose a basis of P(T). Hence if the number of equations and the number of
unknowns are different, the system of equations certainly cannot be solved uniquely
for all right-hand sides.
□
Remark 6.7 If we do not have the same number of degrees of freedom as the dimen-
sion of the ﬁnite element space, then the element in question is not unisolvent.
Be careful that unisolvence is not just a question of dimensions, as the following
example shows.
Take T = {|x1| + |x2| ≤1}, P(T) = Q1 and ϕi the values at the four vertices of
T. We have dim Q1 = 4 but this element is not unisolvent, since p(x) = x1x2 is in Q1
and ϕi(p) = 0 for all i even though p ̸= 0.
If on the other hand, T = [0, 1]2, P(T) = Q1 and ϕi the values at the four vertices
of T, then the ﬁnite element is unisolvent. This is the element we have been using so
far in 2d.
Therefore, unisolvence somehow reﬂects the adequacy of the duality between the
polynomial space and the degrees of freedom.
□
In practice, unisolvence is checked using the following result.
Proposition 6.6 A ﬁnite element is unisolvent if and only if d = dim P(T) and there
exists a basis (p j) j=1,...,d of P(T) such that ϕi(p j) = δi j for all i, j.
Proof If the element is unisolvent, we already know that d = dim P(T). Moreover,
choosing αi = δi j for j = 1, . . . , d yields the existence of p j by the very deﬁnition.
The family (p j) is linearly independent, for if
d

j=1
λ jp j = 0,

6.5 General Deﬁnition of a Finite Element
191
applying the linear form ϕi, we obtain
0 =
d

j=1
λ jϕi(p j) =
d

j=1
λ jδi j = λi
for all i. Thus it is a basis of P(T).
Conversely, assume that d = dim P(T) and that we have a basis p j with the above
property. Let us be given scalars αi. Then the polynomial p = d
j=1 α jp j is the only
element of P(T) such that ϕi(p) = αi by the same argument.
□
Remark 6.8 The polynomials p j are called the basis polynomials or shape functions
of the ﬁnite element. They are dual to the degrees of freedom. They are also used to
construct the basis functions of the discrete approximation spaces, as we have seen
already in 1d with the P1 Lagrange and P3 Hermite approximations, and in 2d with
the Q1 Lagrange approximation. In the latter case, the shape functions were already
given in Eq.(6.3) for T = [0, 1]2.
This also indicates that unisolvence is far from being the end of the story in
terms of ﬁnite elements. The basis polynomials must also be such that they can
be combined into globally continuous functions so as to give rise to a conforming
approximation.
□
Speaking of duality, we can also introduce Σ(T) = vect {ϕ1, . . . , ϕd}, the vector
subspaceofP∗spannedbythedegreesoffreedom.InasimilarveinasProposition6.5,
we also have
Proposition 6.7 If a ﬁnite element is unisolvent, then, d = dim Σ(T).
Proof Clear.
□
The basis polynomials and the degrees of freedom are obviously dual bases of
their respective spanned spaces. In the counterexample shown above, the four linear
forms are linearly independent as elements of P∗, but not as elements of Q∗
1.
6.6
Q2 and Q3 Finite Elements
Let us brieﬂy discuss what happens if we want to use higher degree polynomials. We
start with Q2 Lagrange elements for second order problems. The discrete approxi-
mation space is then
Vh = {vh ∈C0( ¯Ω); ∀Rk ∈T , vh|Rk ∈Q2, vh = 0 on ∂Ω},

192
6
The Finite Element Method in Dimension Two
Fig. 6.19 The nine nodes of
the Q2 Lagrange element
S0
S1
S1,2
S2
S4
S3,4
S3
S4,1
S2,3
on the same rectangular mesh as before and the general approximation theory applies
(note that this space is larger than the previous one). We concentrate on the description
of the ﬁnite element ﬁrst. We set R = [0, 1]2, P(R) = Q2 and we need to describe the
degrees of freedom. Since we are going to use Lagrange interpolation, these degrees
of freedom are going to be values at some points of the element. The dimension of
Q2 is nine, therefore nine degrees of freedom are required to deﬁne a unisolvent ﬁnite
element, i.e., nine points or nodes in R. The choice of points must also be guided by
the necessity of deﬁning a global C0 interpolation based on the nodal values on the
mesh. The set of points depicted in Fig.6.19 turns out to satisfy both requirements.
We thus take the four vertices Sk as before, plus the four middles of the edges
Sk,k+, where k+ = k + 1 for k = 1, 2, 3 and k+ = 1 for k = 4, plus the center of
gravity S0. A slight misuse of notation: we let p(S) denote the linear form p →p(S).
Proposition 6.8 Theﬁniteelement(R, Q2, {p(Sk), p(Sk,k+), p(S0), k = 1, . . . , 4})is
unisolvent.
Proof The number of degrees of freedom matches the dimension of the space. It is
thus sufﬁcient to construct the basis polynomials. We will number them the same
way as the node they correspond to. There are three polynomials to be constructed:
p1 from which the other pk are deduced by symmetry, p1,2 from which the other pk,k+
are deduced by symmetry, and p0.
Let us show how to compute p0. The interpolation conditions to be satisﬁed are
p0(S0) = 1 and p0 = 0 on all other eight nodes. Now p0 is zero at points (0, 0), (0, 1
2)
and (0, 1). The restriction of a Q2 polynomial to the line x2 = 0 is a second degree
polynomial in the variable x1, and we have just seen that this polynomial has three
roots. Therefore it vanishes and p0 = 0 on the straight line x2 = 0. It follows that

6.6 Q2 and Q3 Finite Elements
193
p0 is divisible by x2. The same argument shows that it is divisible by x1, (1 −x1)
and (1 −x2). These polynomials are relatively prime, thus p0 is divisible by their
product,
p0(x) = q(x)x1x2(1 −x1)(1 −x2),
for some polynomial q. Now x1x2(1 −x1)(1 −x2) ∈Q2, thus the partial degree of
q is less than 0, i.e., q is a constant C. Evaluating now p0 at point S0 = ( 1
2, 1
2), we
obtain
1 = C × 1
2 × 1
2 × 1
2 × 1
2 = C
16.
Finally, we ﬁnd that
p0(x) = 16x1x2(1 −x1)(1 −x2).
Conversely, it is clear that this particular polynomial is in Q2 and satisﬁes the
required interpolation conditions.
The same arguments, that we leave as an exercise, show that
p1(x) = (1 −x1)(1 −2x1)(1 −x2)(1 −2x2),
p2(x) = −x1(1 −2x1)(1 −x2)(1 −2x2),
p3(x) = x1(1 −2x1)x2(1 −2x2),
p4(x) = −(1 −x1)(1 −2x1)x2(1 −2x2),
and
p1,2(x) = 4x1(1 −x1)(1 −x2)(1 −2x2),
p2,3(x) = −4x1(1 −2x1)x2(1 −x2),
p3,4(x) = −4x1(1 −x1)x2(1 −2x2),
p4,1(x) = 4(1 −2x1)(1 −x1)x2(1 −x2).
The latter three of each group are obtained by considerations of symmetry from the
ﬁrst one of the group.
□
We draw the graphs of the different basis polynomials in Figs.6.20, 6.21 and 6.22.
Let us now consider the whole mesh. The nodes no longer are just the element
vertices, but also the middles of the edges and the centers of gravity of the elements,
see Fig.6.23.
We then have the exact analog of Propositions 6.3 and 6.4.
Proposition 6.9 A function of Vh is uniquely determined by its values at the interior
nodes of the mesh and all sets of values are interpolated by one and only one element
of Vh.
Proof By unisolvence, nine values for the nine nodes of an element determine one
and only one Q2 polynomial that interpolates these nodal values (we take the value

194
6
The Finite Element Method in Dimension Two
Fig. 6.20 The graph of p0
Fig. 6.21 The graph of p1,
with the segments where p1
vanishes
0 for the nodes located on the boundary). Therefore, if we are given a set of values
for each node in the mesh, this set determines one Q2 polynomial per element. Let
us check that these polynomials combine into a globally C0 function.
Let us thus consider the situation depicted in Fig.6.24, without loss of generality.
We thus have two Q2 polynomials q and q′ such that q(S1) = q′(S1), q(S1,2) =
q′(S1,2) and q(S2) = q′(S2). On the segment [S1, S2], which is parallel to the x2
axis, q −q′ is a second degree polynomial in the variable x2 that has three roots.

6.6 Q2 and Q3 Finite Elements
195
Fig. 6.22 The graph of p1,2,
with the segments where p1,2
vanishes
Fig. 6.23 Same mesh as
Fig.6.7, 32 elements, 153
nodes
Therefore, q −q′ = 0 on this segment, and the function deﬁned by q(x) if x ∈Rk,
q′(x) if x ∈Rk′ \ Rk is continuous on Rk ∪Rk′.
□
Corollary 6.3 Let us be given a numbering of the nodes Si, i = 1, . . . , N = (2N1 +
1)(2N2 + 1). There is a basis of Vh composed of the functions wi
h deﬁned by wi
h(S j) =
δi j and for all vh ∈Vh, we have

196
6
The Finite Element Method in Dimension Two
Fig. 6.24 Continuity across
an internal edge, Q2 case
S2
S1,2
S1
Rk
Rk′
q′
q
vh =
N

i=1
vh(Si)wi
h.
Proof Same as before.
□
Figures6.25, 6.26 and 6.27 show pictures of the different types of basis functions,
depending on which kind of node, i.e., center of gravity of an element, element vertex
or edge middle, they are attached to.
Note that the last two basis functions change sign in Ω. This was not the case for
Q1 basis functions.
We do not pursue here matrix assembly and node numbering issues. It is to be
expected that the structure of the matrix is more complicated than in the Q1 case.
The question arises as to why introduce Q2 elements and deal with the added
complexity compared with the Q1 case. One reason is that we thus obtain a higher
order approximation method. Indeed, if u ∈H3(Ω), then we have (exercise) a better
error estimate
∥u −uh∥H1(Ω) ≤Ch2|u|H3(Ω),
than with Q1 elements. The estimate is better in the sense that h2 ≪h when h is small,
even though we do not have any idea of the order of magnitude of the constants and
Fig. 6.25 Basis function
corresponding to an element
center of gravity

6.6 Q2 and Q3 Finite Elements
197
Fig. 6.26 Basis function
corresponding to an element
vertex
Fig. 6.27 Basis function
corresponding to an edge
middle
the norms in the right-hand side. So the extra implementation and computational
costs induced by the extra degree must be balanced against the increased accuracy
that is expected from the higher degree ﬁnite element approximation. For instance,
a cheaper computation may be achieved with Q2 elements with the same accuracy
by taking less elements than with a Q1 computation.
Let us say a few words about Q3 ﬁnite elements. We could deﬁne Q3-Lagrange
elements by taking 16 nodes per element, since dim Q3 = 16. We would need four
nodes per edge to ensure global continuity, hence the four vertices plus two points
on the thirds of each edge. Four more points must be chosen inside, with obvious
simple possibilities.
We can also use Q3 ﬁnite elements for Hermite interpolation that result in C1
functions suitable for conforming approximation of fourth order problems, such as
the plate equation (1.10) for example. In this case, the degrees of freedom must also
include partial derivative values. We would thus take as degrees of freedom the 4
vertex values and the 8 ﬁrst partial derivatives values at the vertices. This would seem
to be enough, as we recognize 1d P3 Hermite interpolation on each edge, and there
is a tensor product structure Q3[X, Y] = P3[X] ⊗P3[Y].
Surprisingly, this is not enough. Indeed, this choice would only provide 12 degrees
of freedom for a 16-dimensional space, and there would be inﬁnitely many different
possible basis polynomials, in the sense that the interpolation relations would be
satisﬁed, since there is inﬁnitely many different ways of adding four more degrees of
freedom. Moreover, it is not clear which choice would guarantee global C1 regularity.
Surprisingly again, if we complete the set of degrees of freedom with the four vertex

198
6
The Finite Element Method in Dimension Two
values of the second derivatives
∂2p
∂x1∂x2 , we obtain a unisolvent element that generates
a C1 approximation. The Q3-Hermite element is well adapted to the approximation
of such fourth order problems.
We have seen an interesting example of the same polynomial space used with
two completely different sets of degrees of freedom and yielding two completely
different approximation spaces, Q3-Lagrange and Q3-Hermite, which are used in
also different contexts.
Let us now switch to triangular ﬁnite elements, which are better adapted for
problems that are posed in open sets that are not just rectangles. First we need a
quick review of afﬁne geometry.
6.7
Barycentric Coordinates
Triangular ﬁnite elements are much easier to work with using a system of coordinates
in the plane that is quite different from the usual Cartesian system, namely barycentric
coordinates. Actually, barycentric coordinates are natural systems of coordinates for
afﬁne geometry.
We will be given three points A1, A2 and A3 in the plane. We ﬁrst deﬁne weighted
barycenters of these points.
Deﬁnition 6.6 Let λ1, λ2 and λ3 be three scalars such that λ1 + λ2 + λ3 = 1. The
barycenter of the points A j with weights λ j is the unique point M in the plane such
that −→
OM = 3
j=1 λ j
−−→
OA j, where O is a given point. This point does not depend on
the choice of O and we thus write
M =
3

j=1
λ jA j.
One statement in this deﬁnition needs to be checked, namely that M does not
depend on O. Indeed, let O′ be another choice of point, and M′ be such that −−→
O′M′ =
3
j=1 λ j
−−→
O′A j. We have
−−→
O′M′ =
3

j=1
λ j(
−−→
O′O +
−−→
OA j) =
 3

j=1
λ j
−−→
O′O +
3

j=1
λ j
−−→
OA j =
−−→
O′O + −→
OM =
−−→
O′M
hence M′ = M.
Now of course, barycenters are likewise deﬁned for any ﬁnite family of points
and weights of sum equal to 1, and in any afﬁne space, but we will only use three
points in the plane.

6.7 Barycentric Coordinates
199
From now on, we assume that the three points A j are not aligned, in which case
they constitute what is called an afﬁne basis of the plane. In this case, we have the
following basic result.
Proposition 6.10 ForallpointsM intheplane,thereexistsauniquetriple(λ1, λ2, λ3)
of real numbers with λ1 + λ2 + λ3 = 1 such that
M =
3

j=1
λ jA j.
The scalars λi = λi(M) are called the barycentric coordinates of M, with respect to
points A1, A2, A3.
Proof We use Cartesian coordinates. Let (x j
1, x j
2) be the Cartesian coordinates of
A j in some Cartesian coordinate system, and (x1, x2) be the Cartesian coordinates
of point M. We have M = 3
j=1 λ jA j if and only if xk = 3
j=1 λ jx j
k for k = 1, 2.
Moreover, we have the condition 1 = 3
j=1 λ j. We thus ﬁnd a system of three linear
equations in the three unknowns λ j
⎧
⎪⎨
⎪⎩
λ1 +
λ2 +
λ3 = 1,
x1
1λ1 + x2
1λ2 + x3
1λ3 = x1,
x1
2λ1 + x2
2λ2 + x3
2λ3 = x2.
The determinant of this system is
Δ =
!!!!!!!
1 1 1
x1
1 x2
1 x3
1
x1
2 x2
2 x3
2
!!!!!!!
=
!!!!!!!
1
0
0
x1
1 x2
1 −x1
1 x3
1 −x1
1
x1
2 x2
2 −x1
2 x3
2 −x1
2
!!!!!!!
= (x2
1 −x1
1)(x3
2 −x1
2) −(x3
1 −x1
1)(x2
2 −x1
2) ̸= 0,
since it is equal to det(
−−→
A2A1,
−−→
A3A1) = 2 area(T), where T is the triangle with vertices
A1 A2 and A3, and area(T) is its algebraic area which is nonzero since the points are
not aligned.
Therefore, for any right-hand side, i.e., for any point M, the system has one and
only one solution.
□
Remark 6.9 Going from barycentric coordinates to Cartesian coordinates is just
done by applying the deﬁnition. Conversely, to compute barycentric coordinates
from Cartesian coordinates, we just need to solve the above linear system.
If the three points are aligned, then we get a system which has a solution only if
M is on the line spanned by the points, and there are inﬁnitely many solutions, and
if the three points are equal, the system only has a solution if M is equal to the other
points, again with an inﬁnity of solutions.
□
Let us give a few miscellaneous properties of barycentric coordinates.

200
6
The Finite Element Method in Dimension Two
Proposition 6.11 We have
(i) λi(A j) = δi j for all i and j.
(ii) The functions λi are afﬁne in (x1, x2) and conversely, (x1, x2) are afﬁne functions
of (λ1, λ2, λ3).
(iii) Let (Ai, A j) denote the straight line passing through Ai and A j for i ̸= j. Then
(Ai, A j) = {M; λk(M) = 0, k ̸= i, k ̸= j}.
(iv) Let T be the closed triangle determined by the three points A j. Then T =
{M, 0 ≤λi(M) ≤1, i = 1, 2, 3}.
Proof (i) We have A1 = 1 × A1 + 0 × A2 + 0 × A3 with 1 + 0 + 0 = 1, hence by
uniqueness of the barycentric coordinates, λi(A1) = δi1.
(ii) Use Cramer’s rule for solving the above linear system.
(iii) The function λk is a nonzero afﬁne function by (i) and (ii), thus it vanishes
on a straight line. By (i), this straight line contains Ai and A j, so it is equal to
(Ai, A j).
(iv) We have just seen by (iii) that λk(M) = 0 is the equation of the straight line
opposite to vertex Ak. Moreover, by (i) the half-plane containing Ak is the
half-plane {M; λk(M) ≥0}. The triangle T is the intersection of these three
half-planes, so it is the set of points whose barycentric coordinates are all
nonnegative. Since their sum is equal to 1, they are also less than or equal to 1.
□
Figure6.28 shows the signs of the barycentric coordinates in the plane. Note that
there is no −−−region, it would be hard to have  λi = 1 in such a region …
Fig. 6.28 Signs of the
barycentric coordinates in
order λ1, λ2, λ3. For
instance, + + −means that
λ1(M) ≥0, λ2(M) ≥0 and
λ3(M) ≤0, and so on

6.7 Barycentric Coordinates
201
Let us give the barycentric coordinates of a few points of interest in a triangle:
• Middle of [A1A2]:
 1
2, 1
2, 0

,
• Middle of [A2A3]:

0, 1
2, 1
2

,
• Middle of [A1A3]:
 1
2, 0, 1
2

,
• Center of gravity of the triangle:
 1
3, 1
3, 1
3

.
Proposition 6.12 The equation of any straight line in barycentric coordinates is of
the form
3

i=1
γiλi(M) = 0,
where the constants γi are not all equal.
Proof Let A and B be two distinct points with barycentric coordinates (α1, α2, α3)
and (β1, β2, β3). This implies not only that (α1, α2, α3) ̸= (β1, β2, β3), but also that
the matrix
α1 α2 α3
β1 β2 β3

is of rank 2. Indeed if this matrix was of rank one, the two
row vectors would be proportional, and since the sum of their coefﬁcients is equal
to one, the proportionality coefﬁcient would also be equal to one, i.e., A = B.
A point M is on the straight line (A, B) passing through A and B if and only if it
is a barycenter of A and B, i.e., if and only if there exists a scalar μ such that
M = μA + (1 −μ)B.
It follows immediately that
λi(M) = μαi + (1 −μ)βi, i = 1, 2, 3,
which is a parametric representation of the straight line in barycentric coordinates
with μ ∈R.
Due to the rank remark above, the existence of μ is then clearly equivalent to the
equation
!!!!!!
λ1(M) λ2(M) λ3(M)
α1
α2
α3
β1
β2
β3
!!!!!!
= 0,
which reads
γ1λ1(M) + γ2λ2(M) + γ3λ3(M) = 0,
where γ1 = α2β3 −α3β2 and so on. Indeed, the vanishing of the above determinant
implies that the ﬁrst line is a linear combination of the other two, or that λi(M) =
μ1αi + μ2βi, i = 1, 2, 3. If we sum over i, we obtain 1 = μ1 + μ2.
It remains to show that the γi are not all equal. This is clear since A ∈(A, B) so
that 3
i=1 γiαi = 0. If we had γi = γ for all i, this would imply that 0 = 3
i=1 γiαi =
γ 3
i=1 αi = γ . This would in turn imply that (α1, α2, α3) = (β1, β2, β3) or A = B.

202
6
The Finite Element Method in Dimension Two
Conversely, let us be given three scalars γi not all equal. The afﬁne function
f : M →3
i=1 γiλi(M) is non constant. Indeed, f (Ai) = γi. It thus vanishes on a
straight line.
□
The above equation is homogeneous, multiplying it by a nonzero constant yields
another equation that obviously describes the same straight line. Conversely, two such
homogeneous equations describe the same straight line if and only if their coefﬁcients
are proportional. Indeed, assume that γi and γ ′
i describe the same straight line. This
implies that the linear system
⎧
⎪⎨
⎪⎩
λ1 +
λ2 +
λ3 = 1,
γ1λ1 + γ2λ2 + γ3λ3 = 0,
γ ′
1λ1 + γ ′
2λ2 + γ ′
3λ3 = 0,
has inﬁnitely many solutions. Hence its determinant is zero.
Let us give an example. Consider the line passing through the middle of [A1A2]
and the middle of [A1A3]. One equation for this line is thus
!!!!!!!
λ1(M) λ2(M) λ3(M)
1
2
1
2
0
1
2
0
1
2
!!!!!!!
= 0,
which reads after multiplication by 4
λ1(M) −λ2(M) −λ3(M) = 0.
We can rewrite the equation in nonhomogeneous form by using the fact that
−λ2(M) −λ3(M) = λ1(M) −1, which yields
2λ1(M) −1 = 0,
in other words, this line is the locus of points such that λ1(M) = 1
2, which is quite
visible on a ﬁgure.
Remark 6.10 Of course, if we are given the equation of a straight line in Cartesian
coordinates, it is immediate to derive an equation for that same line in barycen-
tric coordinates. Indeed, we have seen that the Cartesian coordinates are afﬁne
functions of the barycentric coordinates, x1(λ1, λ2, λ3), x2(λ1, λ2, λ3). Substitut-
ing these expressions in a Cartesian equation ax1 + bx2 + c = 0, we obtain an
expression αλ1 + βλ2 + γ λ3 + δ = 0, which we can rewrite in homogeneous form
(α + δ)λ1 + (β + δ)λ2 + (γ + δ)λ3 = 0. It is as easy to pass from an equation in
barycentric coordinates to an equation in Cartesian coordinates.
□
An important feature of barycentric coordinates is their invariance under afﬁne
transformations. For this we modify the notation a bit by indicating the dependence

6.7 Barycentric Coordinates
203
on the points A j by writing λA1,A2,A3
i
(M), which is admittedly cumbersome, and will
thus not be used after this.
Proposition 6.13 Let F be an bijective afﬁne transformation of the plane. Then we
have
λF(A1),F(A2),F(A3)
i
(F(M)) = λA1,A2,A3
i
(M)
for i = 1, 2, 3 and all M.
Proof This is clear since afﬁne transformations conserve barycenters.
□
Thebarycentriccoordinatesalsohaveanicegeometricalinterpretation.Wechoose
an orientation of the plane such that the loop A1 →A2 →A3 →A1 runs counter-
clockwise. Then, the algebraic area of T, which is equal to 1
2 det(
−−→
A1A2,
−−→
A1A3), is
strictly positive. For i = 1, we let i+ = 2, for i = 2, we let i+ = 3, and for i = 3, we
let i+ = 1. We also let i++ = (i+)+. For any point M in the plane, we denote by Ti(M)
the possibly degenerate, oriented triangle MAi+Ai++, see Fig.6.29. Its algebraic area
is area Ti(M) = 1
2 det(
−−→
MAi+,
−−−→
MAi++).
Proposition 6.14 We have
λi(M) = area Ti(M)
area T
for i = 1, 2, 3 and all M.
Proof Taking O = M in the deﬁnition of barycentric coordinates, we see that
0 =
3

j=1
λ j(M)
−−→
MA j.
Fig. 6.29 Algebraic areas
and barycentric coordinates
M
A1
A2
A3
T1(M)
T2(M)
T3(M)

204
6
The Finite Element Method in Dimension Two
In particular
λ1(M)
−−→
MA1 = −λ2(M)
−−→
MA2 −λ3(M)
−−→
MA3
for instance, so that
λ1(M) det(
−−→
MA1,
−−→
MA2) = det(−λ2(M)
−−→
MA2 −λ3(M)
−−→
MA3,
−−→
MA2)
= −λ3(M) det(
−−→
MA3,
−−→
MA2).
Therefore, we have λ1(M)area T3(M) = λ3(M)area T1(M) and likewise for the other
two possible choices. Hence, there exists a scalar μ such that
⎛
⎝
λ1(M)
λ2(M)
λ3(M)
⎞
⎠= μ
⎛
⎝
area T1(M)
area T2(M)
area T3(M)
⎞
⎠.
Summing over the three lines on both sides, we obtain 1 = μ area T and the propo-
sition is proved.
□
Remark 6.11 In the context of the ﬁnite element method, in each triangle of a mesh,
we will use the barycentric coordinates associated with the vertices of this particular
triangle to compute all the quantities that concern the triangle in question, such as
basis functions and so on.
□
6.8
Triangular P1 Lagrange Elements
Let us return to the model problem (6.1), on a polygonal domain Ω. Let us be
given a triangular mesh T on Ω. We remind the reader that P1 denotes the space
of polynomials of total degree less or equal to 1, i.e., afﬁne functions. We deﬁne the
corresponding approximation spaces
Wh = {vh ∈C0( ¯Ω), vh|Tk ∈P1 for all Tk ∈T },
without boundary conditions and
Vh = {vh ∈Wh, vh = 0 on ∂Ω},
with boundary conditions. The general approximation theory applies and we thus
just need to describe the approximation spaces in terms of ﬁnite elements and basis
functions.
Let T be a triangle with non aligned vertices A1, A2 and A3. We allow the same
misuse of notation as before for the degrees of freedom.

6.8 Triangular P1 Lagrange Elements
205
Proposition 6.15 The ﬁnite element (T, P1, {p(A1), p(A2), p(A3)}) is unisolvent.
Proof We have dim P1 = 3 so the numbers match. The basis polynomials are obvi-
ous: λ1, λ2, λ3, by Proposition 6.11, (i) and (ii).
□
Proposition 6.16 A function of Vh is uniquely determined by its values at the inter-
nal nodes of the mesh and conversely, any set of values for the internal nodes is
interpolated by one and only one element of Vh.
Proof By unisolvence, three values for the three nodes of an element determine one
and only one P1 polynomial that interpolates these nodal values (we take the value
0 for the nodes located on the boundary). Therefore, if we are given a set of values
for each node in the mesh, this set determines one P1 polynomial per element. Let
us check that they combine into a globally C0 function.
Since the mesh is admissible, an edge common to two triangles Tk and Tk′ is
delimited by two vertices A1 and A2 which are also common to both triangles, see
Fig.6.30. We thus have two P1 polynomials p and p′ such that p(A1) = p′(A1) and
p(A2) = p′(A2). We parametrize the segment [A1, A2] as M = μA1 + (1 −μ)A2 with
μ ∈[0, 1]. Then the restriction of p −p′ to this segment is a ﬁrst degree polynomial
in the variable μ that has two roots, μ = 0 and μ = 1. Therefore, p −p′ = 0 on this
segment, and the function deﬁned by p(x) if x ∈Tk, p′(x) if x ∈Tk′ is continuous on
Tk ∪Tk′.
□
Corollary 6.4 Let us be given a numbering of the internal nodes Si, i = 1, . . . , Nint.
There is a basis of Vh composed of the functions wi
h deﬁned by wi
h(S j) = δi j and for
all vh ∈Vh, we have
vh =
Nint

i=1
vh(Si)wi
h.
Proof Same as before, see Fig.6.31.
□
Let us now talk a little bit about matrix assembly. We will not touch on the node
numbering issue, which is clearly more complicated in a triangular mesh than in a
rectangular mesh, especially in an unstructured triangular mesh, such as that shown
in Fig.6.2, in which there is no apparent natural numbering.
Fig. 6.30 Continuity across
an internal edge, P1 case
A2
A1
Tk
Tk′
p′
p

206
6
The Finite Element Method in Dimension Two
Fig. 6.31 A P1 basis
function on a triangular mesh
We will however see how the use of a reference triangle and of barycentric coor-
dinates simpliﬁes the computation of matrix coefﬁcients. We have the same element-
wise decomposition as in the rectangular case
Ai j =
NT

k=1
Ai j(Tk),
with
Ai j(Tk) =

Tk
(∇w j
h · ∇wi
h + cw j
hwi
h)(x) dx.
On each triangle Tk, the basis functions either vanish or are equal to one barycentric
coordinate. So we need to compute the integral of the product of two barycentric
coordinates (for c constant) and the integral of the scalar product of their gradient.
We thus introduce a reference triangle
T = {(ˆx1, ˆx2) ∈R2, ˆx1 ≥0, ˆx2 ≥0, ˆx1 + ˆx2 ≤1}.
Let ˆA1 = (0, 0), ˆA2 = (1, 0) and ˆA3 = (0, 1) be its vertices and ˆλi the corresponding
barycentric coordinates. Let Tk be a generic triangle in the mesh, with vertices A1
k,
A2
k, A3
k. Now, there exists one and only one afﬁne mapping Fk such that Fk(ˆA j) = A j
k,
j = 1, 2, 3. Indeed, since afﬁne mappings conserve barycenters, we simply have
Fk(
M) = ˆλ1(
M)A1
k + ˆλ2(
M)A2
k + ˆλ3(
M)A3
k,
or in other words, λi(Fk(
M)) = ˆλi(
M), where the ﬁrst barycentric coordinates are
taken relative to the vertices of Tk in increasing superscript order.
Now the expression of barycentric coordinates in the reference triangle in terms
of Cartesian coordinates is particularly simple:
ˆλ1 = 1 −ˆx1 −ˆx2,
ˆλ2 = ˆx1,
ˆλ3 = ˆx2,
whereas they are fairly disagreeable in the generic triangle, see Fig.6.32.

6.8 Triangular P1 Lagrange Elements
207
Fig. 6.32 Barycentric
coordinates in the reference
triangle T
0
ˆx1 = ˆλ2
ˆx2 = ˆλ3

M
ˆA1
ˆA2
ˆA3
1
1
Let us give an example of computation with the integral
	
Tk λ2
2 dx. We are going
to use the change of variables x = Fk(ˆx). Since this change of variable is afﬁne, its
Jacobian J is constant, and we have
area Tk =

Tk
dx =

T
J dˆx = J
2
therefore J = 2 area Tk. Now we can compute

Tk
λ2
2(x) dx =

T
ˆλ2
2(ˆx)J dˆx
= 2 area Tk

T
ˆx2
1 dˆx
= 2 area Tk
 1
0
ˆx2
1
 1−ˆx1
0
dˆx2

dˆx1
= 2 area Tk
 1
0
ˆx2
1(1 −ˆx1) dˆx1
= 2 area Tk
1
3 −1
4

= area Tk
6
.
Exchanging the vertices, we ﬁnd
	
Tk λ2
1(x) dx =
	
Tk λ2
3(x) dx = area Tk
6
. A similar
computation shows that
	
Tk λi(x)λ j(x) dx = area Tk
12
for all i ̸= j. Such terms are thus
of the order of h2.

208
6
The Finite Element Method in Dimension Two
Fig. 6.33 Geometric
elements of a generic triangle
Ai+
Ai++
Ai
νi(Tk)
Hi
hi(Tk)
bi(Tk)
Let us now turn to the gradient terms. We ﬁrst need to compute ∇λi, which is a
constant vector.
We introduce hi(Tk) and bi(Tk) respectively the height and base of Tk relative to
Ai, Hi the foot of the altitude of Ai and νi(Tk) the unit vector perpendicular to the
base and pointing from the base toward Ai, see Fig.6.33. Since λi is afﬁne, we have
for all points M
λi(M) = λi(Hi) + ∇λi · −−→
HiM.
Now Hi lies on the straight line (Ai+, Ai++), thus λi(Hi) = 0. Since λi vanishes on
this straight line, it follows that ∇λi = μνi(Tk) for some scalar μ. Taking M = Ai,
we obtain
1 = μνi(Tk) · −−→
HiM = μhi(Tk).
Therefore, we have
∇λi =
1
hi(Tk)νi(Tk) =
bi(Tk)
2 area Tk
νi(Tk).
It follows from instance that
∥∇λi∥2 =
bi(Tk)2
4(area Tk)2 ,

6.8 Triangular P1 Lagrange Elements
209
so that

Tk
∥∇λi∥2 dx = bi(Tk)2
4 area Tk
.
These terms are of the order of 1. We could likewise compute
	
Tk ∇λi · ∇λ j dx
without difﬁculty, with expressions that involve the angles of Tk.
6.9
Triangular P2 Lagrange Elements
Let us go one step up in degree and consider P2 elements. We have dim P2 = 6 as is
shown by its canonical basis (1, x1, x2, x2
1, x1x2, x2
2). This canonical basis is useless
for our purposes and it is again much better to work in barycentric coordinates. The
following result is meant to convince the reader of this fact.
Proposition 6.17 Let T be a triangle with vertices A1, A2, A3 and λ1, λ2, λ3 be the
corresponding barycentric coordinates. The family (λ2
1, λ2
2, λ2
3, λ1λ2, λ2λ3, λ1λ3) is
a basis of P2.
Proof The functions λi are afﬁne, thus products λiλ j belong to P2. We have a family
of 6 vectors in a 6-dimensional space, it thus sufﬁces to show that it is linearly
independent. Let us be given a family of 6 scalars αi j such that
3

i≤j=1
αi jλiλ j = 0.
Evaluating ﬁrst this relation at point Ak, we obtain
0 =
3

i≤j=1
αi jδikδ jk = αkk
for all k. We are thus left with
α12λ1λ2 + α13λ1λ3 + α23λ2λ3 = 0.
We evaluate this relation at point A1+A2
2
, the middle of A1 and A2, for which λ1 =
λ2 = 1
2 and λ3 = 0. Hence
α12
4
= 0,
and similarly α13 = α23 = 0.
□
We need 6 degrees of freedom of Lagrange interpolation. We take the three ver-
tices Ai and the three edge middles Ai,i+, see Fig.6.34. Then we have the following
proposition, using the same misuse of notation as before.

210
6
The Finite Element Method in Dimension Two
Fig. 6.34 The P2 Lagrange
triangle
λ3 = 1
2
λ
2 = 1
2
λ
1 = 1
2
A1
A2
A3
A3,1
A1,2
A2,3
Proposition 6.18 The ﬁnite element

T, P2, {p(Ai), p(Ai,i+)}i=1,2,3

is unisolvent.
Proof We have the right number of degrees of freedom with respect to the dimension
of the polynomial space. It is thus sufﬁcient to construct the basis polynomials.
Everything being invariant by permutation of the vertices, it is clearly sufﬁcient to
construct the basis polynomial corresponding to A1 and that corresponding to A1,2,
for example.
Let us start with A1. We thus need a polynomial p1 ∈P2 such that p1(A1) = 1
and p1 vanishes at all the other nodes. We will freely use the obvious fact that the
restriction of a polynomial of total degree at most n in two variables to a straight
line is a polynomial of degree at most n in any afﬁne parametrization of the straight
line. Here, p1 is of degree at most 2 on (A2, A3), with three roots corresponding to
points A2, A2,3 and A3, thus it vanishes on (A2, A3). The equation of the straight
line is λ1 = 0, hence p1 is divisible by λ1, i.e., there exists a polynomial q such that
p1 = qλ1.
Now λ1 is of degree 1, therefore q is of degree at most one. Moreover, since
λ1(A1,2) = λ1(A3,1) = 1
2 ̸= 0, we have q(A1,2) = q(A3,1) = 0. Therefore, by the
same token, q vanishes on the straight line (A1,2, A3,1), of equation λ1 −1
2 = 0.
Thus q is divisible by λ1 −1
2, so that q = c(λ1 −1
2) with c of degree at most 0, i.e.,
a constant. Finally, the relation p1(A1) = 1 yields 1 = c
2, hence p1 = λ1(2λ1 −1).
Conversely, it is easy—but necessary—to check that this polynomial is in P2 and
satisﬁes the required interpolation relations.
To sum up, we have
p1 = λ1(2λ1 −1),
p2 = λ2(2λ2 −1),
p3 = λ3(2λ3 −1),

6.9 Triangular P2 Lagrange Elements
211
for the basis polynomials associated with the vertices. The basis polynomials are
equivalently rewritten in homogeneous form as
pi = λi(λi −λi+ −λi++),
for i = 1, 2, 3.
Next we deal with A1,2. The polynomial p1,2 has three roots on the line (A1, A3),
where it must thus vanish. Hence it is divisible by λ2 so that there exists q such
that p1,2 = qλ2. Likewise, the polynomial p1,2 must also vanish on the line (A2, A3),
hence be divisible by λ1. Now the polynomials λ1 and λ2 are relatively prime, there-
fore p1,2 = cλ1λ2 where c is a constant. Using p1,2(A1,2) = 1, we obtain c = 4.
Conversely, this polynomial is in P2 and satisﬁes the required interpolation relations.
To sum up, we have
p1,2 = 4λ1λ2,
p2,3 = 4λ2λ3,
p3,1 = 4λ1λ3,
for the basis polynomials associated with the middles of the edges.
Wehavefoundsixbasispolynomials,thereforetheP2 Lagrangetriangularelement
is unisolvent.
□
Figure6.35 shows the graphs of the different P2 basis polynomials.
The approximation space
Vh = {vh ∈C0( ¯Ω); vh|Tk ∈P2, ∀Tk ∈T , vh = 0 on ∂Ω}
is of course endowed with a set of basis functions that interpolate values at all nodes
(vertices and middles). Let us quickly check the continuity across an edge. We thus
have two polynomials of degree at most two, one on each side of the edge, that
coincide at the vertices and the middle, see Fig.6.36. Their restriction to the edge is
Fig. 6.35 The two different
kinds of P2 basis
polynomials

212
6
The Finite Element Method in Dimension Two
Fig. 6.36 Continuity across
an internal edge, P2 case
A2
A1
Tk
Tk′
A1,2
p′
p
Fig. 6.37 A basis function
associated with a vertex
Fig. 6.38 A basis function
associated with a middle
of degree two in one variable, their difference has three roots, hence they are equal
on the edge. The rest follows as before. Figures6.37 and 6.38 show the graphs of
typical P2 basis functions.
Let us say a few words about P3 Lagrange triangles. We have dim P3 = 10, thus 10
interpolation points are needed. We take the 3 vertices plus 2 points per edge, located
at the thirds (this will obviously imply global continuity). That makes 9 points. A
simple choice for the tenth point is then the center of gravity, see Fig.6.39.
Naturally, this ﬁnite element is unisolvent. We list the basis polynomials:
p0 = 27λ1λ2λ3,

6.9 Triangular P2 Lagrange Elements
213
Fig. 6.39 The 10 nodes of a
P3 Lagrange triangle. We use
the notation
Ai,i,i+ = 2
3Ai + 1
3Ai+ and
Ai,i+,i+ = 1
3Ai + 2
3Ai+. The
center of gravity is of course
A0 = 1
3(A1 + A2 + A3)
corresponding to the center of gravity, also called a bubble due to the shape of its
graph,
pi = 1
2λi(3λi −1)(3λi −2), i = 1, 2, 3,
associated with the three vertices, and
pi,i,i+ = 9
2λiλi+(3λi −1), pi,i+,i+ = 9
2λiλi+(3λi+ −1), i = 1, 2, 3,
associated with the six edge nodes. All these formulas can be rewritten in homoge-
neous form. Figure6.40 shows the graphs of the different P3 basis polynomials.
Also of course, the approximation space
Vh = {vh ∈C0( ¯Ω); vh|Tk ∈P3, ∀Tk ∈T , vh = 0 on ∂Ω}
has the usual basis made of basis functions which we picture in Fig.6.41.
As in the rectangular case, the reason for facing the added complexity of using
higher degree polynomials is to achieve faster convergence. Indeed, we have the
following general result [19, 66], for Pk-Lagrange triangular elements corresponding
to the approximation spaces
Vh = {vh ∈C0( ¯Ω); vh|Tl ∈Pk, ∀Tl ∈T , vh = 0 on ∂Ω}
with k ≥1.

214
6
The Finite Element Method in Dimension Two
Fig. 6.40 The three
different kinds of P3 basis
polynomials
Theorem 6.3 Let us be given a regular family of triangulations indexed by h. We
consider Pk Lagrange elements on the triangulations. If u ∈Hk+1(Ω), then we have
∥u −uh∥H1(Ω) ≤Chk|u|Hk+1(Ω).
The proof is along the same lines as the proof in the Q1 case, but with a lot more
technicality due to the afﬁne changes of variables between the reference triangle and
the generic triangle.
All the above Lagrange triangular elements are adequate for H1 approximation
and are adapted to C0 approximation spaces. It is also possible to deﬁne C1 Her-
mite triangular elements for fourth order problems. One possible construction uses
P5 polynomials, hence 21 degrees of freedom. This is the Argyris ﬁnite element,
[14, 19, 21, 27, 82]. More generally, there is a very large diversity of triangular

6.9 Triangular P2 Lagrange Elements
215
Fig. 6.41 A few P3 basis
functions
elements in the literature, sometimes especially crafted for one (class of) boundary
value problem(s). Other generalizations include curvilinear triangles that are used
to approximate curved boundaries. These are called isoparametric elements, see
[19, 21].
6.10
An Example of 2d-Computation
To conclude this chapter, we show an example of computation made with the
FreeFem++ software (a user-friendly free ﬁnite element software package available
at http://www.freefem.org/). We solve the Laplace equation −Δu = 1 with a homo-
geneous Dirichlet boundary condition in the polygonal domain shown in Fig.6.42,
using P1 and P2 Lagrange elements on the same mesh.

216
6
The Finite Element Method in Dimension Two
Fig. 6.42 Domain and
triangular mesh
Fig. 6.43 Isovalue lines of
the approximated solution uh
in the P1 case
In this example, see Figs.6.42, 6.43, 6.44, 6.45 and 6.46, FreeFem++ is given the
boundary nodes (105 such nodes) and constructs a “good” mesh in the domain based
on these boundary nodes using an automatic mesh generator. The mesh has 1,107
triangles and 607 vertices. FreeFem++ then assembles the matrix. It then proceeds
to solve the linear system, then displays the solution and exports various ﬁles for

6.10 An Example of 2d-Computation
217
Fig. 6.44 Isovalue lines of
the approximated solution uh
in the P2 case
Fig. 6.45 3d visualization of
the graph of uh in the P1 case
further use. The second computation uses P2 elements and has 2,320 degrees of
freedom, hence a 2,320×2,320 matrix. Both computations only take a small fraction
of a second on a laptop computer.
The 3d visualizations of the graphs are done with medit (free software available
at http://www.ljll.math.upmc.fr/~frey/software.html).

218
6
The Finite Element Method in Dimension Two
Fig. 6.46 3d visualization of
the graph of uh in the P2 case
This example conﬁrms that the ﬁnite element method is much more versatile than
the ﬁnite difference method in two dimensions seen in Chap.2. In fact, its range of
applications is much wider.

Chapter 7
The Heat Equation
We have so far studied elliptic problems, i.e., stationary problems. We now turn to
evolution problems, starting with the archetypal parabolic equation, namely the heat
equation.
In this chapter, we will present a brief and far from exhaustive theoretical study
of the heat equation. We will mostly work in one dimension of space, some of the
results having an immediate counterpart in higher dimensions, others not. The study
of numerical approximations of the heat equation will be the subject of the next
chapter.
7.1
Overview
In Chap.1, we presented the historical derivation of the heat equation by Fourier. In
the general d-dimensional case, the heat equation is as follows. Let us be given Ω be
an open subset of Rd and T ∈R∗
+. We note Q = Ω × ]0, T [. When all the physical
constants are set to 1, the heat equation with source term f reads
∂u
∂t (x, t) −Δu(x, t) = f (x, t) in Q,
together with an initial condition
u(x, 0) = u0(x) in Ω,
and boundary values, for instance Dirichlet boundary values
u(x, t) = g(x, t) on ∂Ω × ]0, T [,
where f , u0 and g are given functions. The unknown u is a function from ¯Q to R.
This is called an initial-boundary value problem.
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_7
219

220
7
The Heat Equation
When d = 1 and Ω is bounded, we can take Ω = ]0, 1[ without loss of generality.
The problem reads
⎧
⎪⎨
⎪⎩
∂u
∂t (x, t) −∂2u
∂x2 (x, t) = f (x, t) in Q,
u(x, 0) = u0(x) in Ω,
u(0, t) = g(0, t), u(1, t) = g(1, t) in ]0, T [.
(7.1)
Other possible boundary conditions are the Neumann condition and the Fourier
condition. For brevity, we limit ourselves to the Dirichlet case.
7.2
The Maximum Principle for the Heat Equation
We have seen a version of the maximum principle for a second order elliptic equation.
Parabolic equations such as the heat equation also satisfy their own version of the
maximum principle, see for example [5, 35].
Proposition 7.1 We assume that u is a solution of problem (7.1) that belongs to
C0( ¯Q) ∩C2(Q ∪(Ω × {T })). If f ≥0 in Q, then u attains its minimum on
(Ω × {0}) ∪(∂Ω × [0, T ]).
Proof We write the proof for d = 1. Let us ﬁrst assume that f > 0 on Q∪(Ω×{T }).
The set ¯Q is compact and the function u is continuous on ¯Q, thus it attains its
minimum somewhere in ¯Q, say at point (x0, t0).
If (x0, t0) ∈Q which is an open set, then ∂u
∂t (x0, t0) = 0.1 Moreover, ∂2u
∂x2 (x0, t0) ≥
0 since u is C2 in a neighborhood of (x0, t0). Therefore
 ∂u
∂t −∂2u
∂x2

(x0, t0) ≤0, which
contradicts f (x0, t0) > 0.
Therefore (x0, t0) ∈∂Q =

(Ω × {0}) ∪(∂Ω × [0, T ])

∪(Ω × {T }). Assume
that (x0, t0) ∈Ω × {T }, i.e., that x0 ∈Ω and t0 = T . Since as a function in
the variable x for t = T , u is also C2, it follows again that ∂2u
∂x2 (x0, T ) ≥0 so
that ∂u
∂t (x0, T ) = ∂2u
∂x2 (x0, T ) + f (x0, T ) > 0. Thus there exists t < T such that
u(x0, t) < u(x0, T ), which is consequently not a minimum value for u.
The only possibility left is that (x0, t0) ∈K = (Ω × {0}) ∪(∂Ω × [0, T ]).
Consider now the case f ≥0. Let ε > 0 and uε(x, t) = u(x, t) + εx(1 −x). In
particular u(x, t) ≤uε(x, t) in ¯Q. We have
∂uε
∂t −∂2uε
∂x2 = ∂u
∂t −∂2u
∂x2 + 2ε = f + 2ε > 0.
1The fact that ∂u
∂x (x0, t0) = 0 is not useful here.

7.2 The Maximum Principle for the Heat Equation
221
Fig. 7.1 The set K where u
attains its minimum (thicker
line)
1
0
T
¯Q
K
x
t
By the previous argument, uε attains its minimum at a point (xε, tε) of K. We
have
u(x0, t0) ≤u(xε, tε) ≤uε(xε, tε) ≤uε(x0, t0) = u(x0, t0) + εx0(1 −x0).
Therefore,
u(x0, t0) ≤u(xε, tε) ≤u(x0, t0) + εx0(1 −x0).
We now let ε →0. Since K is compact, we may extract a subsequence still
denoted by ϵ such that (xε, tε) →(¯x, ¯t) ∈K. Passing to the limit in the above
inequalities and using the continuity of u, we obtain
u(x0, t0) = u(¯x, ¯t),
with (¯x, ¯t) ∈K and the minimum is therefore attained on K (see Fig.7.1).
□
Remark 7.1 The meaning of the maximum principle is that the minimum tempera-
ture is either attained at t = 0 or on the boundary of Ω at some other time t ∈]0, T ],
but in general not in Ω × ]0, T ]. It is also valid in any dimension d, using the same
proof and the maximum principle for the Laplacian, that we have not proved here.
The result cannot be reﬁned any further since u = 0 is a solution for f = 0, u0 = 0
that attains its minimum at any point in K.
□
The maximum principle has many consequences, some of which we now list.
Corollary 7.1 Under the hypotheses of Proposition7.1, if f ≥0, g ≥0 and u0 ≥0,
then u ≥0 in ¯Q.

222
7
The Heat Equation
Proof This is clear since the minimum of u is either of the form u0(x0), g(0, t0) or
g(1, t0).
□
Remark 7.2 This form of the maximum principle is again a monotonicity result.
The interesting physical interpretation is that if you heat up a room, the walls are
kept at a nonnegative temperature and the initial temperature is nonnegative, then
the temperature in the room stays nonnegative everywhere and at any time.
□
We also have a stability result in the C0 norm.
Corollary 7.2 Under the hypotheses of Proposition7.1, if f = 0 and g = 0, then
∥u∥C0( ¯Q) = ∥u0∥C0( ¯Ω).
Proof Let v+ = u + ∥u0∥C0( ¯Ω). We have
∂v+
∂t −∂2v+
∂x2 = 0,
since f = 0,
v+(0, t) = v+(1, t) = ∥u0∥C0( ¯Ω) ≥0,
since g = 0 and
v+(x, 0) = u0 + ∥u0∥C0( ¯Ω) ≥0.
By Corollary7.1, v+ ≥0 in ¯Q, or in other words u(x, t) ≥−∥u0∥C0( ¯Ω) in ¯Q.
Changing u in −u, we also have u(x, t) ≤∥u0∥C0( ¯Ω) in ¯Q, hence the result.
□
Such a stability result immediately entails a uniqueness result.
Proposition 7.2 Problem (7.1) has at most one solution in C0( ¯Q) ∩C2(Q).
Proof Indeed, if u1 and u2 are two solutions, then v = u1−u2 satisﬁes the hypotheses
of Corollary7.2 on Ω × [0, T −η]) for all η > 0 with an initial value u0 = 0.
□
7.3
Construction of a Regular Solution
We will see several different ways of constructing solutions to the heat equation. Let
us start with an elementary construction using Fourier series. It should be recalled
that Joseph Fourier invented what became Fourier series in the 1800s, exactly for the
purpose of solving the heat equation, see Chap. 1, Sect.1.7.
We consider the case when f = 0, no heat source, and g = 0, homogeneous
Dirichlet boundary condition, the only nonzero data being the initial condition u0.

7.3 Construction of a Regular Solution
223
Proposition 7.3 Letu0 ∈C0([0, 1])bepiecewiseC1 andsuchthatu0(0) = u0(1) =
0. There exists a sequence (bk)k∈N∗of real numbers such that we have
u0(x) =
+∞

k=1
bk sin(kπx)
for all x ∈[0, 1]. Moreover, 	+∞
k=1 |bk| < +∞.
Proof We ﬁrst extend u0 by imparity by setting 
u0(x) = u0(x) for x ∈[0, 1] and

u0(x) = −u0(−x) for x ∈[−1, 0[. The resulting function is odd and continuous on
[−1, 1] by construction since u0(0) = 0 and still piecewise C1.
Secondly, we extend
u0 to R by 2-periodicity by setting
≈u0(x) = 
u0(x −2⌊x+1
2 ⌋),
where ⌊·⌋denotes the ﬂoor function. This function is continuous since 
u0(−1) =

u0(1) = 0, piecewise C1 and 2-periodic by construction. Therefore, by Dirichlet’s
theorem, it can be expanded in Fourier series
≈u0(x) = a0
2 +
+∞

k=1
ak cos(kπx) +
+∞

k=1
bk sin(kπx),
with 	+∞
k=1(|ak| + |bk|) < +∞, hence the series is normally convergent. Now
≈u0 is
also odd by construction, so that all ak Fourier coefﬁcients vanish. Restricting the
above expansion to x ∈[0, 1], we obtain the result.
□
Theorem 7.1 Let u0 be as above. Then the function deﬁned by
u(x, t) =
+∞

k=1
bk sin(kπx)e−k2π2t
belongs to C0(R × [0, +∞[) ∩C∞(R × ]0, +∞[). Its restriction to ¯Q solves the
initial-boundary value problem (7.1) with data f = 0, g = 0.
Proof We ﬁrst need to show that the series above is convergent in some sense and that
its sum belongs to the function spaces indicated in the theorem. Normal convergence
on R × [0, +∞[ is obvious since |bk sin(kπx)e−k2π2t| ≤|bk|, thus u exists and is
continuous on R × [0, +∞[.
Let us now consider differentiability. Now if u is supposed to coincide with u0 at
t = 0, and u0 is only piecewise C1, we cannot expect u to be C∞up to t = 0, hence
the exclusion of t = 0 in the theorem. In order to use theorems on the differentiation
of series, we actually need to stay away from t = 0 as will become clear in the proof.
Let us thus chose ε > 0 and work for t ≥ε. It is convenient to notice that
sin(kπx)e−k2π2t = Im(eikπx−k2π2t),

224
7
The Heat Equation
where Im z denotes the imaginary part of a complex number z. Therefore, for any
nonnegative integers p and q, we have
∂p+q
∂x p∂tq

sin(kπx)e−k2π2t
= (kπ)p(−k2π2)q Im (i peikπx−k2π2t).
Thus
bk
∂p+q
∂x p∂tq

sin(kπx)e−k2π2t ≤|bk|π p+2qk p+2qe−k2π2t
≤|bk|π p+2qk p+2qe−k2π2ε
for t ≥ε. Since bk = 1
2
 1
−1 sin(kπx)
u0(x) dx, we have |bk| ≤∥u0∥C0([0,1]), thus
bk
∂p+q
∂x p∂tq

sin(kπx)e−k2π2t ≤C p,qk p+2qe−k2π2ε,
for some constant C p,q, because t ≥ε. The right-hand side is the general term
of a convergent series due to the e−k2π2ε term with ε > 0, thus the left-hand side
is the general term of a normally, thus uniformly convergent series, for any p and
q. Therefore, u is of class C∞on R × ]ε, +∞[, for all ε > 0, thus belongs to
C∞(R × ]0, +∞[). Moreover, we have
∂p+qu
∂x p∂tq (x, t) =
+∞

k=1
bk
∂p+q
∂x p∂tq

sin(kπx)e−k2π2t
for all (x, t) ∈R × ]0, +∞[ and all p, q. In particular, we have
∂u
∂t (x, t) =
+∞

k=1
bk
∂
∂t

sin(kπx)e−k2π2t
= −
+∞

k=1
bkk2π2 sin(kπx)e−k2π2t
and
∂2u
∂x2 (x, t) =
+∞

k=1
bk
∂2
∂x2

sin(kπx)e−k2π2t
= −
+∞

k=1
bkk2π2 sin(kπx)e−k2π2t
so that
∂u
∂t −∂2u
∂x2 = 0 on R × ]0, +∞[,
and u solves the heat equation.
Concerning the boundary conditions, we note that for all integers k ≥1, we have
sin(kπ × 0) = sin(kπ × 1) = 0, so that
u(0, t) = u(1, t) = 0

7.3 Construction of a Regular Solution
225
for all t ∈R+. Finally,
u(x, 0) =
+∞

k=1
bk sin(kπx)e−k2π2×0 =
+∞

k=1
bk sin(kπx) = u0(x),
and the initial condition is satisﬁed.
□
Remark 7.3 It is worth noticing that both boundary conditions and initial condition
make sense because u is continuous on ¯Q. Moreover, the regularity of u is such that
the previous uniqueness result applies, thus we have found the one and only one
solution in that class.
An important feature of the heat equation, and more generally of parabolic equa-
tions, is that whatever regularity u0 may have, if f = 0, then the solution u becomes
C∞instantly for t > 0. This is a smoothing effect.
For t ≥0 ﬁxed, the series that gives the function x →u(x, t) is also the Fourier
series of the odd and 2-periodic R-extension of this function. The exponential term
e−k2π2t makes the corresponding Fourier coefﬁcients decrease rapidly, which indi-
cates that the sum is smooth (with respect to x), but we knew that already, both in x
and t.
The smoothing effect also tells us why the backward heat equation is ill-posed.
Indeed, there can be no solution to the backward heat equation with an initial condi-
tion that is not C∞, since an initial condition for the backward heat equation is a ﬁnal
condition for the forward heat equation. It is not even clear that all C∞functions can
be reached by the evolution of the heat equation. Therefore, time is irreversible in
the heat equation.
We can see the same effect in the series, since for t < 0, −k2π2t > 0 and the
exponential terms become explosive instead of ensuring extremely fast convergence
of the series. The only way the series can converge for t < 0 is for the Fourier
coefﬁcients bk of the initial condition to be rapidly decreasing, so as to compensate for
the exponential term. Again, a function with rapidly decreasing Fourier coefﬁcients
is very smooth.
□
The above solution of the heat equation exhibits rapid uniform decay in time.
Proposition 7.4 There exists a constant C such that
|u(x, t)| ≤Ce−π2t.
In particular, u(x, t) →0 when t →+∞, uniformly with respect to x.
Proof Indeed, e−k2π2t ≤e−π2t for all k and all t ≥0, so that
|u(x, t)| ≤
+∞

k=1
|bk|e−k2π2t ≤e−π2t
+∞

k=1
|bk|,
hence the result since 	+∞
k=1 |bk| < +∞.
□

226
7
The Heat Equation
Remark 7.4 If we remember the physical interpretation of the heat equation, keeping
the walls of a room at 0 degree is tantamount to having paper-thin walls and a huge
ice cube surrounding the room. If there is no heat source inside the room, it is not
contrary to physical intuition that the temperature inside should drop to 0 degree
pretty quickly, if it was positive at t = 0. All the heat inside the room eventually
ﬂows outside into the ice since the heat ﬂux is proportional to the opposite of the
temperature gradient.
□
Apart from proving the existence of a solution in a particular case, the Fourier
series expansion can also be used as a very precise numerical method, provided the
Fourier coefﬁcients of the initial condition are known with good accuracy.
In effect, we have a coarse error estimate
u(x, t) −
N

k=1
bk sin(kπx)e−k2π2t ≤

+∞

k=N+1
|bk|

e−(N+1)2π2t,
so that truncating the series and retaining only a few terms, we can expect to achieve
excellent precision as soon as t > 0 is noticeably nonzero. Of course, the sine and
exponential functions are already implemented in all computer languages.
The use of Fourier series in a numerical context is the simplest example of spectral
method. Let us give an example of the numerical application of Fourier series. We
consider a simple continuous, piecewise afﬁne initial condition such as depicted in
Fig.7.2. Six terms in the Fourier series already provide a very good approximation of
the solution. Figure7.3 shows several views of the graph of u plotted in (x, t) space.
The grey stripes show the graphs of x →u(x, t) for a discrete sample of values of t.
Fig. 7.2 An admissible
initial value u0
0
0,25
0,5
0,75
1
0,25
0,5
0,75
1
1,25

7.3 Construction of a Regular Solution
227
Fig. 7.3 Various views of the corresponding solution u, using Fourier series, 0 ≤x ≤1, 0 ≤t ≤T
We see the exponential decay in time, the smoothing effect, and also the fact that the
ﬁrst nonzero term in the series rapidly becomes dominant as t increases, as can be
expected from the exponential terms. Note also the continuity as t →0+, and the
fact that the time derivative goes to ±∞when (x, t) tends to a point (x0, 0) where
the second space derivative of the initial condition is in a sense inﬁnite,2 i.e., the ﬁrst
space derivative is discontinuous. We also see that the minimum is attained where
the maximum principle says it must be attained.3
The Fourier expansion even gives quite good results for cases that are not covered
by the preceding analysis, for instance for an initial condition that does not satisfy
2Or more accurately a Dirac mass.
3Which is reassuring.

228
7
The Heat Equation
Fig. 7.4 Fourier series and
discontinuous solutions, 20
terms. The Gibbs
phenomenon is visible in the
neighborhood of
(x, t) = (0, 0) and
(x, t) = (1, 0)
Fig. 7.5 Fourier series and
discontinuous solutions, 100
terms
the Dirichlet boundary condition, such as u0(x) = 1! Figure7.4 shows 20 terms in
the series, and of course a Gibbs phenomenon, i.e., localized oscillations around the
discontinuities.
We also show the same computation with 100 terms in the Fourier series. The
Gibbs phenomenon is still there, see Figs.7.6 and 7.7, but does not show on Fig.7.5
for sampling reasons: It occurs on a length scale that is too small to be captured by
the graphics program. Recall that high frequency oscillations in space are damped
extremely rapidly in time by the exponential term.
Figure7.7 features a few close-ups of the 100 term Fourier series expansion of u
near (x, t) = (0, 0).

7.3 Construction of a Regular Solution
229
0
0,25
0,5
0,75
1
0,25
0,5
0,75
1
1,25
Fig. 7.6 One hundred terms in the Fourier series of 
u0, with Gibbs phenomenon around 0 and 1
Fig. 7.7 Gibbs phenomenon and bad approximation of discontinuity, up close

230
7
The Heat Equation
7.4
Spaces of Hilbert Space-Valued Functions
In order to work with more general solutions and less smooth data, we need to
introduce a few new function spaces. We will actually consider real-valued functions
in the two variables x and t, as functions in the variable t with values in a space
of functions in the variable x, u(t) = u(·, t).4 This is because the time and space
variables do not play the same role for the heat equation.
Let V denote a separable Hilbert space with norm ∥· ∥V . Let T > 0 be given.
The space C0([0, T ]; V ) of continuous functions from [0, T ] with values in V is a
Banach space for its natural norm
∥f ∥C0([0,T ];V ) = max
t∈[0,T ] ∥f (t)∥V .
A V -valued function on ]0, T [ is differentiable at point t ∈]0, T [ if there exists a
vector f ′(t) ∈V such that
 f (t + h) −f (t)
h
−f ′(t)

V →0 when h →0.
Ofcourse, f ′(t)iscalledthederivativeof f atpointt.Afunctionisclearlycontinuous
at all of its points of differentiability. If f is differentiable at all points t, then its
derivative becomes a V -valued function. We can deﬁne
C1([0, T ]; V ) =

f ∈C0([0, T ]; V ); f ′ ∈C0([0, T ]; V )

in the sense that f ′ has a continuous extension at 0 and T . When equipped with its
natural norm
∥f ∥C1([0,T ];V ) = max

∥f ∥C0([0,T ];V ), ∥f ′∥C0([0,T ];V )

,
C1([0, T ]; V ) is a Banach space. More generally, we can deﬁne Ck([0, T ]; V ) for
all positive integers k. All of these notions are perfectly classical and work the same
as in the real-valued case.
Measurability (and integrability) issues are a little trickier in the inﬁnite dimen-
sional valued case than in the ﬁnite dimensional valued case. There are different
types of measurability and integrals when V is a Banach space or a more general
topological vector space. We stick to the simplest notions. Besides we will not use
V -valued integrals here. We equip [0, T ] with the Lebesgue σ-algebra.
Deﬁnition 7.1 A function f : [0, T ] →V is called a simple function if there exists
a ﬁnite measurable partition of [0, T ], (Ei)i=1,...,k, and a ﬁnite set of vectors vi ∈V
such that
4Beware of the slightly ambiguous notation.

7.4 Spaces of Hilbert Space-Valued Functions
231
f (t) =
k

i=1
1Ei(t)vi,
for all t ∈[0, T ].
In other words, f only takes a ﬁnite number of values in V and is equal to vi
exactly on the Lebesgue measurable set Ei. It should be noted that for each t, there
is one and only one nonzero term 1Ei(t) in the sum, due to the fact that the sets Ei
form a partition of [0, T ].
Deﬁnition 7.2 A function f : [0, T ] →V is said to be measurable if there exists
a negligible set N ⊂[0, T ] and a sequence of simple functions fn such that
∥fn(t) −f (t)∥V →0 when n →+∞for all t /∈N.
We also say that f is an almost everywhere limit of simple functions. When
V = R, this notion of measurability coincides with the usual one. It is easy to see
that a continuous function is measurable.
Proposition 7.5 Let f : [0, T ] →V be a measurable function. Then the function
NV f : [0, T ] →R+, NV f (t) = ∥f (t)∥V , is a measurable function in the usual
sense.
Proof Let fn(t) = 	kn
i=1 1En,i(t)vn,i be a sequence of simple functions that converges
a.e. to f . Since the norm of V is continuous from V into R, we have NV fn →NV f
a.e. Now due to the fact that for all t, there is at most one nonzero term in the sum,
we also have ∥fn(t)∥V = 	kn
i=1 1En,i(t)∥vn,i∥V , hence NV fn is a real-valued simple
function. Therefore NV f is measurable.
□
Deﬁnition 7.3 We say that two measurable functions f1, f2 : [0, T ] →V are equal
almost everywhere if there exists a negligible set N ⊂[0, T ] such that f1(t) = f2(t)
for all t /∈N.
Almost everywhere equality is an equivalence relation and from now on, we will
not distinguish between a function and its equivalence class. The V -valued Lebesgue
spaces are deﬁned as would be expected [58]
L p(0, T ; V ) =

f : [0, T ] →V, measurable and such that NV f ∈L p(0, T )

,
for all p ∈[1, +∞]. When equipped with the norms
∥f ∥L p(0,T ;V ) = ∥NV f ∥L p(0,T ),
these spaces are Banach spaces. For p = 2, L2(0, T ; V ) is a Hilbert space for the
scalar product
( f |g)L2(0,T ;V ) =
 T
0

f (t)|g(t)

V dt.

232
7
The Heat Equation
The Hilbert norm reads explicitly
∥f ∥L2(0,T ;V ) =
 T
0
∥f (t)∥2
V dt
1/2
.
Obviously,
C0([0, T ]; V ) 
→L p(0, T ; V ),
for all p ∈[1, +∞].
It is also possible to deﬁne V -valued Sobolev spaces and V -valued distributions,
but we will not use these spaces here.
In the case when V is itself a function space on an open set Ω of Rd, there is a
natural connection between V -valued functions on [0, T ] and real valued functions
on ¯Q = ¯Ω × [0, T ] in d + 1 variables. Let us give an example of this.
Proposition 7.6 The spaces L2(0, T ; L2(Ω)) and L2(Q) are canonically isometric.
Proof We leave aside the measurability questions, which are delicate. First of all,
let us take f ∈L2(Q). We thus have

Q f (x, t)2 dxdt < +∞. By Fubini’s theorem
applied to f 2, we thus have that

Ω
f (x, t)2 dx < +∞for almost all t ∈[0, T ]
and

Q
f (x, t)2 dxdt =
 T
0

Ω
f (x, t)2 dx

dt.
Therefore, if we set 
f (t) = f (·, t), then we see that 
f (t) ∈L2(Ω) for almost all t.
Thus we can let 
f (t) = 0 for those t for which the initial 
f is not in L2(Ω) and 
f
is then an L2(Ω)-valued function. Moreover, the second relation then reads
∥f ∥2
L2(Ω) = ∥
f ∥2
L2(0,T ;L2(Ω)),
hence the isometry.
Conversely, taking 
f ∈L2(0, T ; L2(Ω)), then for almost all t, 
f (t) is a function
in the variable x ∈Ω that belongs to L2(Ω). If we thus set f (x, t) = 
f (t)(x), we
deﬁne a function on Q which is such that
 T
0

Ω f (x, t)2 dx

dt < +∞. By Fubini’s
theorem again, it follows that f ∈L2(Q) and we have the isometry.
□
Itisthuspossibletoswitchbetweenthetwopointsofview:functioninonevariable
with values in a function space on a d dimensional domain and real valued function
in d + 1 variables. If 
f ∈C1([0, T ]; L2(Ω)), then the associated f is in L2(Q)
and it can be shown that its distributional derivative ∂f
∂t is in C0([0, T ]; L2(Ω)) and

∂f
∂t = ( 
f )′. From now on, we will drop the tilde notation for simplicity.

7.4 Spaces of Hilbert Space-Valued Functions
233
We will also encounter such situations as f ∈C0([0, T ]; H) ∩L2(0, T ; V ) with
two (or more) different spaces V ⊂H, meaning that f (t) is unambiguously deﬁned
as an element of H for all t, and continuous with values in H, and the same f (t) is
in V for almost all t and square integrable with values in V . It is allowed to exit V
on a negligible subset of [0, T ].
Let us now be given two Hilbert spaces V1 and V2 and A a continuous linear
operator from V1 to V2. Let ∥A∥denote its operator norm. Given any V1-valued
function f , we can deﬁne a V2-valued function Af by (Af )(t) = A( f (t)). This
deﬁnition commutes with all previous notions.
Proposition 7.7 If f ∈Ck([0, T ]; V1) then Af ∈Ck([0, T ]; V2) with (Af )( j) =
A( f ( j)) for j ≤k, and if f ∈L2(0, T ; V1) then Af ∈L2(0, T ; V2).
Proof We start with the continuity. We have
∥Af (t+h)−Af (t)∥V2 = ∥A( f (t+h)−f (t))∥V2 ≤∥A∥∥f (t+h)−f (t)∥V1 −→
h→0 0.
Therefore Af ∈C0([0, T ]; V2). Moreover, we have ∥Af (t)∥V2 ≤∥A∥∥f (t)∥V1 so
that taking the maximum for t ∈[0, T ] on both sides, we obtain
∥Af ∥C0([0,T ];V2) ≤∥A∥∥f ∥C0([0,T ];V1).
Similarly
 Af (t + h) −Af (t)
h
−Af ′(t)

V2 ≤∥A∥
 f (t + h) −f (t)
h
−f ′(t)

V1 −→
h→0 0
and so on for the successive derivatives and their norms. Finally,
 T
0
∥Af (t)∥2
V2 dt ≤∥A∥2
 T
0
∥f (t)∥2
V1 dt < +∞,
leaving aside measurability issues, which are not difﬁcult here. Of course, the above
inequality is nothing but
∥Af ∥L2(0,T ;V2) ≤∥A∥∥f ∥L2(0,T ;V1),
as with the Ck spaces.
□
To get an idea of how this can be used, just take V1 = H 2(Ω), V2 = L2(Ω) and
A = −Δ.
Later on, we will also use Hilbert bases of V , i.e., total orthonormal families in V
(recall that a total family is a family that spans a dense vector subspace). Such bases
are countable since we only consider separable Hilbert spaces.
Proposition 7.8 Let (en)n∈N be a Hilbert basis of V . Let f be a V -valued function
and for all n ∈N, fn(t) = ( f (t)|en)V .

234
7
The Heat Equation
(i) If f ∈Ck([0, T ]; V ) then fn ∈Ck([0, T ]) for all n and f l
n(t) = ( f (l)(t)|en)V
for all l ≤k.
(ii) If f ∈L2(0, T ; V ) then fn ∈L2(0, T ) for all n.
Proof Let f ∈C0([0, T ]; V ) and take a sequence tp →t in [0, T ]. Then f (tp) →
f (t) in V and taking the scalar product with en, which is continuous, we see that
( f (tp)|en)V →( f (t)|en)V in R. Hence, fn is continuous.
Assume now that f is C1. It similarly follows from the fact that f (tp)−f (t)
tp−t
→f ′(t)
in V , that ( f (tp)|en)V −( f (t)|en)V
tp−t
→( f ′(t)|en)V by linearity and continuity of the scalar
product. Therefore, fn has a derivative at t for all n and ( fn)′ = ( f ′)n, so that ( fn)′
is continuous by the previous case and thus fn ∈C1([0, T ]). The general case of k
derivatives follows by induction on k.
Let now f ∈L2(0, T ; V ). Since ∥f (t)∥2
V = 	
n | fn(t)|2 by Parseval’s identity,
see [68], we see that
∥f ∥2
L2(0,T ;V ) =
 T
0
∥f (t)∥2
V dt =

n
 T
0
| fn(t)|2 dt.
In particular,
 T
0 | fn(t)|2 dt < +∞for all n, or in other words, fn ∈L2(0, T ).
□
7.5
Energy Estimates, Stability, Uniqueness
In this section, we consider solutions of problem (7.1) with data that is considerably
less smooth than in the previous sections. We assume that the solutions considered
are regular enough so that all computations are justiﬁed. As the proof in arbitrary
dimension of space d works the same as in one dimension, we will let Ω ⊂Rd
bounded and Q = Ω × ]0, T [.
We start with a lemma.
Lemma 7.1 Let u ∈C1([0, T ]; L2(Ω)). Then the function t →1
2

Ω(u(t)(x))2 dx
is of class C1([0, T ]) and its derivative is given by t →

Ω[u(t)u′(t)](x) dx.
Proof Let E(t) = 1
2

Ω u(x, t)2 dx. We write
E(t + h) −E(t)
h
= 1
2

Ω

u(x, t + h) + u(x, t)
u(x, t + h) −u(x, t)
h

dx.
Now, by L2-valued continuity, u(t+h) →u(t) in L2(Ω) when h →0. By L2-valued
differentiability, u(t+h)−u(t)
h
→u′(t) in L2(Ω) when h →0. Therefore,
E(t + h) −E(t)
h
→

Ω
[u(t)u′(t)](x) dx

7.5 Energy Estimates, Stability, Uniqueness
235
when h →0 by the Cauchy–Schwarz inequality. By the same inequality, the right-
hand side is a continuous function of t.
□
Remark 7.5 Thisresultcanbeconstruedasakindofdifferentiationundertheintegral
sign, since
d
dt

Ω
u(x, t)2 dx

= 2

Ω
∂u
∂t (x, t)u(x, t) dx =

Ω
∂(u2)
∂t
(x, t) dx,
with the identiﬁcation ∂u
∂t = u′. It can be generalized with weaker assumptions on u,
namely u ∈L2(0, T ; H 1
0 (Ω)), with u′ ∈L2(0, T ; H −1(Ω)), see [35].
□
Proposition 7.9 Assume that g = 0 (homogeneous Dirichlet condition), u0 ∈
L2(Ω) and f ∈L2(Q). Then, if u ∈C1([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)) is
a solution of the problem, then
∥u∥C0([0,T ];L2(Ω)) ≤∥u0∥L2(Ω) + C∥f ∥L2(Q),
(7.2)
where C is the Poincaré inequality constant.
Proof Since u ∈C1([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)), we have that u(·, t) and
∂u
∂t (·, t) belong to L2(Ω) for all t and that u(·, t) belongs to H 1
0 (Ω) for almost
all t. The meaning of the partial differential equation in this context is thus that
u′ −Δu = f where u′ ∈C0([0, T ]; L2(Ω)), f ∈L2(0, T ; L2(Ω)) and Δu ∈
L2(0, T ; H −1(Ω)), see Corollary3.4 of Chap.3, so that the equation is well deﬁned
in this sense. Of course, it also coincides with the distributional equation on Q.
For almost all s ∈[0, T ], both sides of the equation are in H −1(Ω). We thus take
the duality bracket with u and obtain
1
2
d
ds

Ω
u(x, s)2 dx

+

Ω
∥∇u(x, s)∥2 dx =

Ω
f (x, s)u(x, s) dx
≤

Ω
f (x, s)2 dx
 1
2 
Ω
u(x, s)2 dx
 1
2
by Lemma7.1 and by the Cauchy–Schwarz inequality. Because of the homogeneous
Dirichlet condition, we have Poincaré’s inequality

Ω
u(x, s)2 dx
 1
2 ≤C

Ω
∥∇u(x, s)∥2 dx
 1
2 ,
and using Young’s inequality ab ≤1
2a2 + 1
2b2, we obtain
1
2
d
ds

Ω
u(x, s)2 dx

+

Ω
∥∇u(x, s)∥2 dx
≤C2
2

Ω
f (x, s)2 dx + 1
2

Ω
∥∇u(x, s)∥2 dx,

236
7
The Heat Equation
so that
1
2
d
ds

Ω
u(x, s)2 dx

≤1
2
d
ds

Ω
u(x, s)2 dx

+ 1
2

Ω
∥∇u(x, s)∥2 dx
≤C2
2

Ω
f (x, s)2 dx.
We integrate the above inequality between 0 and t with respect to s and obtain

Ω
u(x, t)2 dx−

Ω
u(x, 0)2 dx ≤C2
 t
0

Ω
f (x, s)2 dx ds ≤C2
 T
0

Ω
f (x, s)2 dx ds,
for all t ∈[0, T ] due to Lemma7.1, and since u(x, 0) = u0(x), it follows that
∥u(·, t)∥L2(Ω) ≤

∥u0∥2
L2(Ω) + C2∥f ∥2
L2(Q)
1/2
hence the result, since
√
a2 + b2 ≤a + b for all a, b positive.
□
Remark 7.6 The quantity E(t) = 1
2

Ω u(x, t)2 dx is called the energy (up to phys-
ical constants), hence the term “energy estimate”. It follows from the proof that the
energy is decreasing when f = 0. In addition, it is quite clear also from the proof
that if f ∈L2(Ω × R+), then the energy estimate remains valid for all times, i.e.,
sup
t∈R+
∥u(·, t)∥L2(Ω) ≤∥u0∥L2(Ω) + C∥f ∥L2(Ω×R+),
provided such a solution exists.
Let us note that the energy estimate can be proved under lower regularity hypothe-
ses, namely that u ∈C0([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)). The ﬁrst space in the
intersection gives a precise meaning to the initial condition in L2.
□
As in the case of the maximum principle, the energy estimate has consequences
in terms of uniqueness and stability.
Corollary 7.3 There is at most one solution u belonging to C1([0, T ]; L2(Ω)) ∩
L2(0, T ; H 1(Ω)) to the heat equation with initial data u0 ∈L2(Ω), right-hand side
f ∈L2(Q) and Dirichlet boundary condition g ∈L2(0, T ; H 1/2(∂Ω)).
Proof Let u1 and u2 be two such solutions. Then, their difference u1 −u2 belongs
to C1([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)) and is a solution of the heat equation with
zero right-hand side and initial condition. By estimate (7.2), it follows that we have
u1 −u2 = 0.
□
Again this also holds in C0([0, T ]; L2(Ω)) ∩L2(0, T ; H 1(Ω)). Stability or con-
tinuous dependence on the data is straightforward. We just consider here the homo-
geneous Dirichlet condition g = 0.

7.5 Energy Estimates, Stability, Uniqueness
237
Corollary 7.4 Let ui ∈C1([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)), i = 1, 2, be
solutions corresponding to initial conditions u0,i ∈L2(Ω) and right-hand sides
fi ∈L2(Q). Then
∥u1 −u2∥C0([0,T ];L2(Ω)) ≤∥u0,1 −u0,2∥L2(Ω) + C∥f1 −f2∥L2(Q).
Proof Clear.
□
When in addition there is no heat source, i.e., f = 0, we can expect some kind
of exponential decay as in the regular case. Here, the energy is the relevant quantity.
Proposition 7.10 If f = 0, then we have
E(t) ≤e−2t
C2 E(0) = e−2t
C2
2
∥u0∥2
L2(Ω),
where C is the Poincaré inequality constant.
Proof As before, we have
d
dt
1
2

Ω
u(x, t)2 dx

+

Ω
∥∇u(x, t)∥2 dx = 0.
Thus
dE
dt (t) = −

Ω
∥∇u(x, t)∥2 dx ≤−1
C2

Ω
u(x, t)2 dx = −2
C2 E(t),
byPoincaré’sinequality.Solvingthisdifferentialinequality,weobtaintheannounced
result.
□
Remark 7.7 A function in L2 is not bounded in general, thus we cannot expect
uniform decay of the temperature as in the regular case. However, it can be shown
that u is of class C∞as soon as t > 0, which is the same smoothing effect as before.
Thus, there is also a uniform exponential decay but starting away from t = 0. In
fact, it can be shown that u is C∞on any open subset where f is C∞, in particular
when it is equal to 0. This property of the heat operator is called hypoellipticity,
see [20].
□
7.6
Variational Formulation and Existence of Weak
Solutions
So far, we still have no existence result for the initial-boundary value problem when
f ̸= 0 or f = 0 and u0 ∈L2(Ω). For this, we need to recast the problem in

238
7
The Heat Equation
variational form. We only consider the homogeneous Dirichlet boundary condition,
since a non homogeneous Dirichlet condition can be transformed into a homogeneous
one via an appropriate lift of the boundary data. We start with regularity hypotheses
that are a little too strong, but not by much. As in the elliptic case, we deﬁne the
bilinear form a by a(u, v) =

Ω ∇u · ∇v dx.
Proposition 7.11 Let u0 ∈L2(Ω), f ∈L2(Q). Consider u in C1([0, T ]; L2(Ω))∩
L2(0, T ; H 1
0 (Ω)) such that u′ −Δu = f for almost all t and u(0) = u0. Then we
have, for all v ∈H 1
0 (Ω),
d
dt

(u(t)|v)L2(Ω)

+ a(u(t), v) = ( f (t)|v)L2(Ω)
almost everywhere in [0, T ], and
(u(0)|v)L2(Ω) = (u0|v)L2(Ω).
Conversely, a solution in C1([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)) of the above two
variational equations is a solution of the initial-boundary value problem for the
heat equation with homogeneous Dirichlet boundary condition, initial data u0 and
right-hand side f .
Proof We have already seen that each term in the equation u′ −Δu = f is at worst5
in L2(0, T ; H −1(Ω)). It is therefore meaningful to take the duality bracket of each
one of them with an arbitrary v ∈H 1
0 (Ω), so that we have
⟨u′(t), v⟩H −1(Ω),H 1
0 (Ω) −⟨Δu(t), v⟩H −1(Ω),H 1
0 (Ω) = ⟨f (t), v⟩H −1(Ω),H 1
0 (Ω),
for almost all t.
Arguing as in the proof of Lemma7.1, we see that the real-valued function t →
(u(t)|v)L2(Ω) is of class C1 and that
d
dt

(u(t)|v)L2(Ω)

=

Ω
u′(t)(x)v(x) dx = ⟨u′(t), v⟩H −1(Ω),H 1
0 (Ω),
since u′(t) ∈L2(Ω). Similarly,
⟨f (t), v⟩H −1(Ω),H 1
0 (Ω) =

Ω
f (x, t)v(x) dx.
Finally, by Corollary3.4 of Chap.3, we have
−⟨Δu(t), v⟩H −1(Ω),H 1
0 (Ω) = a(u(t), v),
so that the ﬁrst equation is established. The second equation is trivial.
5In the sense of space regularity.

7.6 Variational Formulation and Existence of Weak Solutions
239
Conversely, let us be given a solution u of the variational problem. Since H 1
0 (Ω)
is dense in L2(Ω), the second equation implies that u(0) = u0. Moreover, the above
calculations can be carried out backwards, so that
⟨u′(t) −Δu(t) −f (t), v⟩H −1(Ω),H 1
0 (Ω) = 0,
for almost all t and all v ∈H 1
0 (Ω). Consequently, for almost all t, u′(t) −Δu(t) −
f (t) = 0 as an element of H −1(Ω), hence the heat equation with right-hand side f
is satisﬁed in this sense.
□
As we said above, the regularity in time assumed above is a bit too high. Indeed,
the variational formulation makes sense in a slightly less regular context. This leads
to the following deﬁnition.
Deﬁnition 7.4 The variational formulation of the heat equation with homogeneous
Dirichlet boundary condition, initial data u0 ∈L2(Ω) and right-hand side f ∈
L2(Q) consists in looking for u ∈C0([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)) such that,
for all v ∈H 1
0 (Ω),
 
(u|v)L2(Ω)
′ + a(u, v) = ( f |v)L2(Ω) in the sense of D′(]0, T [),
(u(0)|v)L2(Ω) = (u0|v)L2(Ω).
(7.3)
Remark 7.8 Let us check that this deﬁnition makes sense. First of all, since
u ∈C0([0, T ]; L2(Ω)) and v does not depend on t, we see that the function
t →(u|v)L2(Ω) is continuous on [0, T ], hence its derivative is a distribution on ]0, T [.
Likewise, since u ∈L2(0, T ; H 1
0 (Ω)), the function t →a(u, v) is in L1(0, T ) by
the Cauchy–Schwarz inequality, hence a distribution on ]0, T [ and the same holds
for t →( f |v)L2(Ω). Therefore, the ﬁrst equation in (7.3) is well deﬁned in the
distributional sense.
We have already seen that the second equation is equivalent to u(0) = u0, and the
continuity of u with respect to t with values in L2(Ω) makes this initial condition
relevant.
□
We use the variational formulation to prove existence and uniqueness of solutions.
We will write the proof in the 1d case, Ω = ]0, 1[. The general case is entirely similar.
For all k ∈N∗, we let φk(x) =
√
2 sin(kπx) and λk = k2π2. It is well-known that
the family (φk)k∈N∗is a Hilbert basis of L2(0, 1) as well as a total orthogonal family
in H 1
0 (]0, 1[) [15, 51]. Moreover, for all w ∈H 1
0 (Ω), we have
a(w, φk) =
 1
0
dw
dx
dφk
dx dx = −
 1
0
wd2φk
dx2 dx = λk(w|φk)L2(Ω).
(7.4)

240
7
The Heat Equation
Theorem 7.2 Let u0 ∈L2(Ω), f ∈L2(Q). There exists a unique solution u ∈
C0([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)) of problem (7.3), which is given by
u(t) =
+∞

k=1
uk(t)φk,
(7.5)
where
uk(t) = (u0|φk)L2(Ω)e−λkt +
 t
0
( f (s)|φk)L2(Ω)e−λk(t−s) ds
(7.6)
and the series converges in C0([0, T ]; L2(Ω)) ∩L2(0, T ; H 1
0 (Ω)).
Proof We start with the uniqueness. Let u ∈C0([0, T ]; L2(Ω))∩L2(0, T ; H 1
0 (Ω))
be a solution of (7.3). For all t ∈[0, T ], u(t) is thus an element of L2(Ω) and can
therefore be expanded on the Hilbert basis (φk)k∈N∗. Consequently, we have for all t
u(t) =
+∞

k=1
uk(t)φk
with
uk(t) = (u(t)|φk)L2(Ω)
for all k ∈N∗and the series converges in L2(Ω). Now φk ∈H 1
0 (Ω) is a legitimate
test-function in problem (7.3). In particular, since u(t) ∈H 1
0 (Ω) almost everywhere,
we have
a(u(t), φk) = λkuk(t)
almost everywhere by (7.4), hence everywhere since the right-hand side is continuous
(the left-hand side is L1). We thus have by Proposition7.8,

u′
k(t) + λkuk(t) = ( f (t)|φk)L2(Ω) in the sense of D′(]0, T [),
uk(0) = (u0|φk)L2(Ω),
for each k ∈N∗. Now this is a Cauchy problem for a linear ordinary differential equa-
tion, and there are no other distributional solutions than the usual solution obtained
by variation of the constant, or Duhamel’s formula:
uk(t) = (u0|φk)L2(Ω)e−λkt +
 t
0
( f (s)|φk)L2(Ω)e−λk(t−s) ds,
which is exactly formula (7.6).6 Hence the uniqueness.
6Observe that the function uk is continuous in t.

7.6 Variational Formulation and Existence of Weak Solutions
241
We now use the above series to prove existence. Since u0 ∈L2(Ω), we have
∥u0∥2
L2(Ω) =
+∞

k=1
(u0|φk)2
L2(Ω)
by Parseval’s identity. Similarly, f ∈L2(Q) and
∥f ∥2
L2(Q) =
 T
0
+∞

k=1
( f (t)|φk)2
L2(Ω) dt.
Let us set u0,k = (u0|φk)L2(Ω) and fk(t) = ( f (t)|φk)L2(Ω). We are going to show
that the series in formula (7.5) converges in both spaces C0(0, T ; L2(Ω)) and
L2(0, T ; H 1
0 (Ω)) and that its sum u is a solution of the variational problem. To do
this, we will show that the partial sums Un(t) = 	n
k=1 uk(t)φk are Cauchy sequences
for both norms. Let p < q be two given integers and let us estimate Up −Uq.
First of all, due to the continuity of t →uk(t), the partial sums Un are continuous
with values in L2(Ω). Moreover, for all t ∈[0, T ], we have
∥Up(t) −Uq(t)∥2
L2(Ω) =

q

k=p+1
uk(t)φk

2
L2(Ω)
=
q

k=p+1
uk(t)2
≤2
q

k=p+1

u2
0,k +
 t
0
| fk(s)| ds
2
≤2
q

k=p+1

u2
0,k + t
 t
0
fk(s)2 ds

≤2
q

k=p+1
u2
0,k + 2T
q

k=p+1
 T
0
fk(s)2 ds
since all the exponential terms are less than 1 and by the Cauchy–Schwarz inequality.
Therefore
∥Up −Uq∥2
C0([0,T ];L2(Ω)) = max
t∈[0,T ] ∥Up(t) −Uq(t)∥2
L2(Ω)
≤2
q

k=p+1
u2
0,k + 2T
q

k=p+1
 T
0
fk(s)2 ds
can be made as small as we wish by taking p large enough, due to the hypotheses
on u0 and f , and the sequence is consequently Cauchy in C0(0, T ; L2(Ω)).

242
7
The Heat Equation
Similarly, the partial sums are obviously in L2(0, T ; H 1
0 (Ω)), in fact they even
are continuous with values in H 1
0 (Ω), although this continuity will not persist in
the limit. We use the H 1
0 seminorm, so that |v|2
H 1
0 (Ω) = a(v, v) (for a more general
parabolic equation, H 1
0 -ellipticity of the bilinear form would here come into play7).
The family (φk)k∈N∗is also orthogonal in H 1
0 (Ω) and we have a(φk, φk) = λk by
(7.4). Therefore, for all v ∈H 1
0 (Ω), it follows that
|v|2
H 1
0 (Ω) =
+∞

k=1
λkv2
k,
(7.7)
where vk = (v|φk)L2(Ω). In particular, we have
|Up(t) −Uq(t)|2
H 1
0 (Ω) =
q

k=p+1
λkuk(t)2,
so that integrating between 0 and T , we obtain
∥Up −Uq∥2
L2(0,T ;H 1
0 (Ω)) =
q

k=p+1
 T
0
λkuk(t)2 dt.
Let us estimate each term in the sum on the right. We have
λkuk(t)2 ≤2λk

u2
0,ke−2λkt + T
 t
0
fk(s)2e−2λk(t−s) ds

,
so that
 T
0
λkuk(t)2 dt ≤2λk

u2
0,k
 T
0
e−2λkt dt + T
 T
0
 t
0
fk(s)2e−2λk(t−s) ds

dt

= (1 −e−2λkT )u2
0,k + 2λkT
 T
0
fk(s)2 T
s
e−2λk(t−s) dt

ds
= (1 −e−2λkT )u2
0,k + T
 T
0
(1 −e−2λk(T −s)) fk(s)2 ds
≤u2
0,k + T
 T
0
fk(s)2 ds.
7Or even more generally, Gårding’s inequality, which reads: for all v ∈H1
0 (Ω), a(v, v) ≥
α|v|2
H1
0 (Ω) −β∥v∥2
L2(Ω) with α > 0.

7.6 Variational Formulation and Existence of Weak Solutions
243
Therefore
∥Up −Uq∥2
L2(0,T ;H 1
0 (Ω)) ≤
q

k=p+1
u2
0,k + T
q

k=p+1
 T
0
fk(s)2 ds,
which can again be made as small as we wish by taking p large enough, and the
sequence is Cauchy in L2(0, T ; H 1
0 (Ω)).
Finally, it remains to be seen that the function u deﬁned by the series and which
belongs to C0(0, T ; L2(Ω))∩L2(0, T ; H 1
0 (Ω)) is a solution of the variational prob-
lem (7.3). The initial condition is obvious even in non variational form since
u(0) =
+∞

k=1

u0,ke0 +
 0
0
fk(s)eλks ds

φk =
+∞

k=1
u0,kφk = u0.
Regarding the evolution equation, we obtain from the ordinary differential equations
for uk that for all v ∈span((φk)k∈N∗)

(u|v)L2(Ω)
′ + a(u, v) = ( f (t)|v)L2(Ω) in the sense of D′(]0, T [),
since any such v is a linear combination of the φk.
Let now v ∈H 1
0 (Ω) be arbitrary and vn ∈span((φk)k∈N∗) be such that vn →v in
H 1
0 (Ω). For any ϕ ∈D(]0, T [), we thus have
−
 T
0
(u(t)|vn)L2(Ω)ϕ′(t) dt +
 T
0
a(u(t), vn)ϕ(t) dt =
 T
0
( f (t)|vn)L2(Ω)ϕ(t) dt.
It is then fairly obvious that each term in the above relation passes to the limit as
n →+∞, thus establishing the evolution equation.
□
Remark 7.9 Note that we do not need any compatibility condition between the initial
condition u0 and the Dirichlet boundary condition, as opposed to the regular case.
Indeed, the expansion u0 = 	+∞
k=1 u0,kφk only holds in the L2 sense.
□
Remark 7.10 Formula (7.5)–(7.6) clearly generalizes the expansion obtained in
Theorem7.1.
□
Remark 7.11 The recovery of a bona-ﬁde solution of the heat equation from the
above variational solution would require the use of Hilbert space valued distributions
and integrals. Let us just say that it can be done. There are other approaches to the
heat equation, for instance using semigroups (see for example [28, 58]).
□
Remark 7.12 The d-dimensional heat equation can be solved along the exact same
lines, replacing the functions φk and scalars λk by the eigenfunctions and eigenvalues
of the minus Laplacian in H 1
0 (Ω), i.e., the solutions of −Δφk = λkφk, φk ∈H 1
0 (Ω),
φk ̸= 0, see [5, 26, 28]. This eigenvalue problem for Ω bounded only has solutions

244
7
The Heat Equation
for λk in a sequence 0 < λ1 < λ2 ≤λ3 ≤· · · such that λk →+∞when k →+∞.
Of course, the eigenvalues and eigenfunctions depend on the shape of Ω, see Chap.1,
Sect.1.6.
□
We also have an energy decay and stability estimate in the present context.
Proposition 7.12 The solution u of problem (7.3) satisﬁes
∥u(t)∥L2(Ω) ≤∥u0∥L2(Ω)e−λ1t +
 t
0
∥f (s)∥L2(Ω)e−λ1(t−s) ds,
(7.8)
for all t ∈[0, T ].
Proof This is a consequence of the series expansion. We ﬁrst observe the following
fact. Let g be a L1-function from [0, T ] to a Euclidean space E (i.e., a ﬁnite dimen-
sional Hilbert space). Then the integral
 t
0 g(s) ds is well deﬁned as a vector of E
by choosing a basis of E and integrating g componentwise. Moreover, since E is
Euclidean, there exists a unit vector e such that

 t
0
g(s) ds

E =
 t
0
g(s) ds

· e =
 t
0
g(s) · e ds ≤
 t
0
∥g(s)∥E ds,
by the Cauchy–Schwarz inequality in E.
We now turn to estimate (7.8). We have
u(t) =
+∞

k=1

u0,ke−λkt +
 t
0
fk(s)e−λk(t−s) ds

φk
so that by the triangle inequality
∥u(t)∥L2(Ω) ≤

+∞

k=1
u0,ke−λktφk

L2(Ω) +

+∞

k=1
 t
0
fk(s)e−λk(t−s) ds

φk

L2(Ω).
For the ﬁrst term, we note that

+∞

k=1
u0,ke−λktφk

L2(Ω) =
+∞

k=1
u2
0,ke−2λkt 1
2 ≤
+∞

k=1
u2
0,ke−2λ1t 1
2 = ∥u0∥L2(Ω)e−λ1t,
since the sequence of eigenvalues λk is increasing. For the second term, we resort to
the observation above with E = span(φ1, . . . , φn) equipped with the L2-norm, and
deduce that

7.6 Variational Formulation and Existence of Weak Solutions
245

n

k=1
 t
0
fk(s)e−λk(t−s) ds

φk

L2(Ω) =

 t
0
 n

k=1
fk(s)e−λk(t−s)φk

ds

L2(Ω)
≤
 t
0

n

k=1
fk(s)e−λk(t−s)φk

L2(Ω) ds
=
 t
0
 n

k=1
fk(s)2e−2λk(t−s) 1
2 ds
≤
 t
0
 n

k=1
fk(s)2 1
2 e−λ1(t−s) ds.
We now let n →+∞and conclude by the convergence of the left-hand side series
in L2(Ω) and by the Lebesgue monotone convergence theorem for the right-hand
side term.
□
Remark 7.13 We recover the exponential decay of the energy when f = 0.
□
Remark 7.14 The energy estimates and existence of weak solutions can be gene-
ralized to parabolic problems that are more general than the heat equation,
see [58, 66].
□
7.7
The Heat Equation on R
Even though it is unphysical, the heat equation on Rd is nonetheless interesting from
the point of view of mathematics. For simplicity, we will only consider the case
d = 1. Let us thus consider the initial value problem
⎧
⎨
⎩
∂u
∂t (x, t) −∂2u
∂x2 (x, t) = f (x, t) in R × ]0, T [,
u(x, 0) = u0(x) on R.
(7.9)
Note that there is no boundary data since R has no boundary. They may be replaced
by some kind of asymptotic behavior at inﬁnity.
Let us now introduce an extremely important function [51, 74].
Deﬁnition 7.5 The function deﬁned on R2 by
E(x, t) =
⎧
⎨
⎩
1
√
4πt
e−x2
4t for t > 0,
0 for t ≤0,
is called the (one-dimensional) heat kernel.

246
7
The Heat Equation
Fig. 7.8 Various views of the graph of the heat kernel
We note that for t > 0 ﬁxed, the function x →E(x, t) is a Gaussian. When
t →0+, the Gaussian becomes increasingly spiked. Indeed, we see that E(x, t) =
1
√
4t E
 x
√
4t , 1
4). In particular, E(0, t) →+∞, whereas E(x, t) →0 for all x ̸= 0
when t →0+, see Figs.7.8 and 7.9.
Proposition 7.13 We have E ∈L1
loc(R2), hence E ∈D′(R2).
Proof Clearly E ∈C∞(R2\{(0, 0)}), therefore the only potential local integrability
problem is in a compact neighborhood of (0, 0). It sufﬁces to integrate |E| on the
square [−a, a]2 for some a > 0. Since E vanishes for t ≤0, only the upper half
square is left. We have

7.7 The Heat Equation on R
247
0,8
1,6
2,4
3,2
4
4,8
0
0,8
1,6
2,4
3,2
4
-2,4
-1,6
-0,8
0
0,8
1,6
2,4
0,8
1,6
2,4
3,2
4
4,8
Fig. 7.9 The heat kernel at t ﬁxed for different values of t (left) and x ﬁxed for different values of
x (right)
 a
−a
 a
0
|E(x, t)| dxdt ≤
 +∞
−∞
 a
0
|E(x, t)| dxdt
=
1
2√π
 +∞
−∞
 a
0
1
√t e−x2
4t dx

dt
=
1
√π
 +∞
−∞
 a
0
e−y2 dy

dt = a < +∞,
where we have performed the change of variables x = 2√t y and because of the
well-known value of the Gaussian integral,
 +∞
−∞e−y2 dy = √π.
□
The heat kernel is the fundamental solution or elementary solution of the heat
equation in the following sense.
Proposition 7.14 We have
∂E
∂t −∂2E
∂x2 = δ0,
where δ0 is the Dirac distribution at (x, t) = (0, 0).
Proof Given ϕ ∈D(R2), our goal is to show that
∂E
∂t −∂2E
∂x2 , ϕ

= ϕ(0, 0).
We have already noticed that E is of class C∞everywhere except at (x, t) =
(0, 0). Its distributional derivatives thus coincide with its classical derivatives on

248
7
The Heat Equation
R2\{(0, 0)}. Let us ﬁrst compute these derivatives using brute force for t > 0 (only
mild force is needed for t < 0). We thus have
∂E
∂t =
1
2√π

−
1
2t3/2 +
x2
4t5/2

e−x2
4t ,
∂E
∂x = −
1
4√π
x
t3/2 e−x2
4t ,
∂2E
∂x2 = −
1
4√π
 1
t3/2 −
x2
2t5/2

e−x2
4t ,
so that ∂E
∂t −∂2E
∂x2 = 0 on R2\{(0, 0)}. Therefore the support of the distribution
∂E
∂t −∂2E
∂x2 is included in {(0, 0)}.
Let us now work in the distributional sense. We take a test-function ϕ ∈D(R2).
We have
∂E
∂t −∂2E
∂x2 , ϕ

= −

E, ∂ϕ
∂t + ∂2ϕ
∂x2

= −
 +∞
−∞
 +∞
0
E(x, t)
∂ϕ
∂t + ∂2ϕ
∂x2

(x, t) dt

dx,
since E is L1
loc and vanishes for t ≤0. The derivatives ∂ϕ
∂t and ∂2ϕ
∂x2 have compact
support, hence E
 ∂ϕ
∂t + ∂2ϕ
∂x2

is in L1(R2) and the Lebesgue dominated convergence
theorem implies that
∂E
∂t −∂2E
∂x2 , ϕ

= −lim
n→+∞
 +∞
−∞
 +∞
1
n
E(x, t)
∂ϕ
∂t + ∂2ϕ
∂x2

(x, t) dt

dx.
Now on the set R×[ 1
n , +∞[, all the functions are C∞and we can integrate by parts,
so that
∂E
∂t −∂2E
∂x2 , ϕ

=
lim
n→+∞
 +∞
−∞
 +∞
1
n
∂E
∂t −∂2E
∂x2

(x, t)ϕ(x, t) dt

dx
+
 +∞
−∞
E(x, n−1)ϕ(x, n−1) dx

.
We have already seen that ∂E
∂t −∂2E
∂x2 = 0 on R × [ 1
n , +∞[ so that the ﬁrst integral
vanishes. Let us study the second integral. We perform the change of variables y =
√nx
2 . This yields
 +∞
−∞
E(x, n−1)ϕ(x, n−1) dx =
1
√π
 +∞
−∞
e−y2ϕ(2n−1/2y, n−1) dy →ϕ(0, 0)

7.7 The Heat Equation on R
249
by the dominated convergence theorem. Therefore, we have shown that
∂E
∂t −∂2E
∂x2 , ϕ

= ϕ(0, 0) = ⟨δ0, ϕ⟩,
and the proposition is proved.
□
The heat kernel can be used to express the solution in various function spaces.
Let us give an example.
Proposition 7.15 Let u0 ∈L1(R) and f ∈L1(R × R+). Then
u(x, t) =
 +∞
−∞
E(x −y, t)u0(y) dy +
 +∞
−∞
 t
0
E(x −y, t −s) f (y, s) ds

dy
is a solution of problem (7.9).
Proof We write the proof for u0 and f continuous and bounded, for simplicity. First
of all, all the integrals make sense and deﬁne a function on R × R∗
+. Moreover,
it is easy to check that all partial derivatives of the heat kernel are integrable on
R × [a, +∞[ for all a > 0. Therefore, we can differentiate under the integral signs
without any problems as soon as the second argument of E stays bounded away from
0. Let us set, for all (x, t) ∈R × R∗
+,
v(x, t) =
 +∞
−∞
E(x −y, t)u0(y) dy
and
w(x, t) =
 +∞
−∞
 t
0
E(x −y, t −s) f (y, s) ds

dy.
By the observation above, we have that
∂v
∂t −∂2v
∂x2

(x, t) =
 +∞
−∞
 ∂
∂t −∂2
∂x2

E(x −y, t)u0(y) dy = 0
for all t > 0. We need to exert a little more care to deal with w. Setting
wn(x, t) =
 +∞
−∞
 t−1/n
0
E(x −y, t −s) f (y, s) ds

dy,
we see that wn →w uniformly. Indeed,
|w(x, t) −wn(x, t)| ≤∥f ∥L∞(R×R+)
 t
t−1
n
 +∞
−∞
E(x −y, t −s) dy

ds = ∥f ∥L∞(R×R+)
n
.

250
7
The Heat Equation
Consequently, wn →w in the sense of D′(R × R∗
+) and therefore
∂wn
∂t −∂2wn
∂x2 →∂w
∂t −∂2w
∂x2 in D′(R × R∗
+).
We have no problem computing the effect of the heat operator on wn:
∂wn
∂t −∂2wn
∂x2

(x, t) =
 +∞
−∞
 t−1/n
0
 ∂
∂t −∂2
∂x2

E(x −y, t −s) f (y, s) ds

dy
+
 +∞
−∞
E(x −y, n−1) f (y, t −n−1) dy
=
 +∞
−∞
E(x −y, n−1) f (y, t −n−1) dy →
n→∞f (x, t)
simply, and even uniformly on compact sets, as we have essentially already seen
before. Hence
∂w
∂t −∂2w
∂x2 = f.
Concerning the initial condition, we obviously have w(x, 0) = 0 and w(x, t) →0
when t →0. Indeed,
 +∞
−∞E(x −y, t −s) f (y, s) dy
 ≤∥f ∥L∞(R×R+). On the
other hand, we have v(x, t) →u0(x) when t →0 by the same change of variable as
above.
□
Remark 7.15 The analysis is a little more difﬁcult when u0 or f are not continuous.
We have not made precise in which space the above solution is unique.
□
Remark 7.16 The solution u is C∞on any open set of R × R∗
+ where f is C∞. This
is again hypoellipticity.
□
Remark 7.17 When u0 is in L∞, the result remains valid. In particular, when u0
is periodic and f = 0, then u is also periodic in x for all t. Moreover if u0 is the
odd periodic extension of an initial condition for the heat equation on a bounded
interval with Dirichlet boundary conditions, we obtain the same solution as the one
obtained via Fourier series by restricting u to a space-time strip based on the interval
in question.
□
Remark 7.18 Notice an interesting phenomenon: When u0 is positive with compact
support and f = 0, we have u(x, t) > 0 for all x ∈R and all t > 0, however small.
In other words, a compactly supported initial distribution of temperature instantly
spreads to the whole of R. Thus, the heat equation propagates energy at inﬁnite
speed, which is strongly non physical. However, the validity of the heat equation
as a model of temperature evolution is still extremely good for all classical physics
and engineering applications. Indeed, assuming that the support of u0 is included in
[−A, A] with A > 0, we have the following coarse estimate for |x| > A:

7.7 The Heat Equation on R
251
|u(x, t)| ≤
A
√πt e−(|x|−A)2
4t
∥u0∥L∞(R),
so that u(x, t) may be nonzero, but it is nonetheless extremely rapidly decreasing at
inﬁnity in x for t > 0.
□
Again, the previous study is far from being exhaustive. There is a lot more to
say about the heat equation, and more generally about parabolic equations, linear
or nonlinear. For the ensuing numerical approximation theory, we are however now
conﬁdent that solutions exist, are unique and sufﬁciently regular under reasonable
hypotheses. Moreover, the energy estimate (7.2) will have important discrete coun-
terparts.

Chapter 8
The Finite Difference Method for the Heat
Equation
We now turn to numerical methods that can be used to approximate the solution of the
heat equation. We develop the ﬁnite difference method in great detail, with particular
emphasis on stability issues, which are delicate. We concentrate on the heat equation
in one dimension of space, with homogeneous Dirichlet boundary conditions.
We also give some indications about ﬁnite difference (in time)-ﬁnite element (in
space) approximation.
8.1
The Explicit Euler Three Point Finite Difference
Scheme for the Heat Equation
We thus consider the problem
⎧
⎪⎨
⎪⎩
∂u
∂t (x, t) −∂2u
∂x2 (x, t) = f (x, t) in Q =]0, 1[×]0, T[,
u(x, 0) = u0(x) in Ω,
u(0, t) = u(1, t) = 0 in ]0, T[.
(8.1)
We assume that the solution u is as regular as we need it to be.
The general idea of ﬁnite difference methods for evolution equations is the same
as for stationary equations that we have already seen in Chap.2. It is simply to replace
derivatives by difference quotients involving approximate discrete unknowns, and
to solve for these unknowns. In the case of an evolution equation, we need a space-
time grid. Let us thus be given two positive integers N and M. We set h = δx =
1
N+1
and xn = nh for n = 0, 1, . . . , N + 1. Similarly, we set k = δt =
T
M+1 and tj = jk for
j = 0, 1, . . . , M + 1. The parameter h is called the space grid step and the parameter
k the time grid step, or time step. The grid points are the points (xn, tj). Eventually,
we will let N and M go to inﬁnity, or equivalently, h and k go to 0. Note that there
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_8
253

254
8
The Finite Difference Method for the Heat Equation
are two independent discretization parameters. This makes the analysis signiﬁcantly
more complicated than in the elliptic case of Chap.2.
The discrete unknowns are scalars uj
n for the above values of n and j, and it is
hoped that uj
n will be an approximation of u(xn, tj), that should become better and
better as N and M are increased. The boundary condition can be enforced exactly by
requiring that
uj
0 = uj
N+1 = 0
for all j = 0, . . . , M + 1. The initial condition is naturally discretized by requiring
that
u0
n = u0(xn)
for all n = 1, . . . , N. If the initial data is consistent with the boundary condition,
the same is true for the discrete unknowns in the sense that u0
0 = u0(0) = u0(1) =
u0
N+1 = 0. The right-hand side of the equation is discretized by setting f j
n = f (xn, tj).
The only values that are left unknown at this stage are thus uj
n for n = 1, . . . , N
and j = 1, . . . , M + 1. We also use the notation1
Uj =
⎛
⎜⎜⎜⎝
uj
1
uj
2...
uj
N
⎞
⎟⎟⎟⎠∈RN
to denote the vector of approximate values on the space grid at time tj. Note again
a fundamental difference with variational approximation methods such as the ﬁnite
element method, which is that the computed approximation is not a function, but a
ﬁnite set of values.
In the explicit Euler three point scheme, the partial derivatives are approximated in
the same way as the simple derivatives were in the stationary case studied in Chap.2.
For the time derivative of the exact solution, we can use for example the forward
differential quotient approximation
∂u
∂t (xn, tj) ≈u(xn, tj+1) −u(xn, tj)
k
,
and for the second order space derivative, by combining a forward and a backward
differential quotient , we obtain the central approximation, see Remark2.2 of Chap.
2,
1For consistency with the notation used in the stationary case, we should denote these vectors Uj
h,k.
We drop the h, k index for brevity, but of course, these vectors crucially depend on h and k.

8.1 The Explicit Euler Three Point Finite Difference Scheme …
255
∂2u
∂x2 (xn, tj) ≈
u(xn+1,tj) −u(xn,tj)
h
−u(xn,tj) −u(xn−1,tj)
h
h
= u(xn+1, tj) −2u(xn, tj) + u(xn−1, tj)
h2
,
where the ≈sign can be given a precise meaning by using Taylor expansions as in
Chap.2 and we will come back to that later. The ﬁnite difference method mimics these
approximations by replacing the exact values of the solution at the grid points by
the discrete unknowns. In this particular case, we end up with the following explicit
Euler three point ﬁnite difference scheme:
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
uj+1
n
−uj
n
k
−uj
n+1 −2uj
n + uj
n−1
h2
= f j
n for n = 1, . . . , N, j = 0, . . . , M,
u0
n = u0(xn) for n = 1, . . . , N,
uj
0 = uj
N+1 = 0 for j = 0, . . . , M + 1.
(8.2)
Of course, at this point, there is no indication that (8.2) has anything to do with (8.1).
The name explicit or forward Euler comes from the fact that the time derivative is
approximated in the same way as it is approximated in the case of the forward Euler
method for ordinary differential equations, whereas the three point name comes from
the three point centered approximation of the second order space derivative already
used for second order elliptic problems in Chap.2.
We may rewrite the ﬁrst N equations of the scheme in vector form as
Uj+1 −Uj
k
+ AhUj = Fj for j = 0, . . . , M,
where Ah is the same N × N tridiagonal matrix as in Chap.2 with c = 0,
Ah = 1
h2
⎛
⎜⎜⎜⎜⎜⎝
2 −1
0
· · ·
0
−1
2
−1
· · ·
0
...
...
...
...
...
0
· · · −1
2 −1
0
· · ·
0
−1
2
⎞
⎟⎟⎟⎟⎟⎠
,
and Fj is the vector
Fj =
⎛
⎜⎜⎜⎝
f j
1
f j
2...
f j
N
⎞
⎟⎟⎟⎠∈RN.
The discrete initial condition is also a vector

256
8
The Finite Difference Method for the Heat Equation
U0 =
⎛
⎜⎜⎜⎝
u0(x1)
u0(x2)
...
u0(xN)
⎞
⎟⎟⎟⎠∈RN.
With this notation, the numerical scheme is equivalent to

Uj+1 = (I −kAh)Uj + kFj for j = 0, . . . , M,
U0 = U0,
where I is the N × N identity matrix.2 This simple recurrence relation shows that
the scheme is well-deﬁned. As opposed to the stationary case of Chap.2, there is no
linear system to be solved. We can also note the appearance of the factor
k
h2 =
δt
δx2
which will play an important role in the sequel.
Let us now introduce, or reintroduce from Chap.2 as the case may be, a few
notions of interest. We will use the notation ut for the function x →u(x, t), for ﬁxed
t ∈R+, and ux for the function t →u(x, t), for ﬁxed x ∈[0, 1].
Deﬁnition 8.1 Let v be a function deﬁned on [0, 1]. We deﬁne the space grid sam-
pling operator Sh by
Sh(v) =
⎛
⎜⎜⎜⎝
v(x1)
v(x2)
...
v(xN)
⎞
⎟⎟⎟⎠∈RN.
Let now u be the solution of problem (8.1). We deﬁne the truncation error of the
present ﬁnite difference method to be the sequence of vectors
εh,k(u)j = Sh(utj+1) −Sh(utj)
k
+ AhSh(utj) −Fj.
To obtain the truncation error, we just take the ﬁnite difference scheme and replace
the discrete unknowns with the corresponding grid samplings of the solution of the
heat equation. Its name stems from the fact that, if we were to ﬁctitiously apply the
numerical scheme with one time step starting from the exact sampling values at tj,
i.e., let
Uj+1 = Sh(utj) −kAhSh(utj) + kFj,
then we would make an error Sh(utj+1) −Uj+1 = kεh,k(u)j. The truncation error is
not however directly related to the actual error between the sampling of the solution
and the discrete unknown, as we will see later. This is because errors accumulate
2Here also, the vectors Fj and U0 depend respectively on h and k, and on h.

8.1 The Explicit Euler Three Point Finite Difference Scheme …
257
from the start at each iteration of the scheme. The truncation error is however an
important intermediate ingredient in the convergence analysis.
In order to analyze the convergence of ﬁnite difference methods, we need to
introduce the function spaces
Cm,n( ¯Q) = {u; ∀t ∈[0, T], ut ∈Cm([0, 1]) and ∀x ∈[0, 1], ux ∈Cn([0, T])
with all derivatives uniformly bounded on ¯Q}.
For the time being, we equip the space RN with the inﬁnity norm as before, except
that the dependence on h is here made explicit,
∥U∥∞,h = max
1≤n≤N |Un|,
where U1, . . . , UN are the components of U ∈RN.
Wehavethefollowingeasyestimateconcerningthetruncationerrorfortheexplicit
Euler three point numerical scheme.
Proposition 8.1 Assume that u ∈C4,2( ¯Q). Then we have
max
0≤j≤M ∥εh,k(u)j∥∞,h ≤C(h2 + k),
where the constant C depends only on u.
We say that the forward Euler three point scheme is consistent for the ∞, h norms,
of order 1 in time and order 2 in space, all these terms to be made precise later on.
Proof We use Taylor–Lagrange expansions exactly as in the one-dimensional elliptic
case. First we use the fact that ux is of class C2. Therefore, for all n and j, there exists
θj
n ∈]tj, tj+1[ such that
u(xn, tj+1) = u(xn, tj) + k ∂u
∂t (xn, tj) + k2
2
∂2u
∂t2 (xn, θj
n),
which we rewrite as
u(xn, tj+1) −u(xn, tj)
k
= ∂u
∂t (xn, tj) + k
2
∂2u
∂t2 (xn, θj
n).
Similarly, ut is of class C4. Therefore, for all n and j, there exists ξ j,+
n
∈]xn, xn+1[,
ξ j,−
n
∈]xn−1, xn[ such that
u(xn+1, tj) = u(xn, tj) + h∂u
∂x (xn, tj) + h2
2
∂2u
∂x2 (xn, tj) + h3
6
∂3u
∂x3 (xn, tj) + h4
24
∂4u
∂x4 (ξj,+
n
, tj),

258
8
The Finite Difference Method for the Heat Equation
and
u(xn−1, tj) = u(xn, tj) −h∂u
∂x (xn, tj) + h2
2
∂2u
∂x2 (xn, tj) −h3
6
∂3u
∂x3 (xn, tj) + h4
24
∂4u
∂x4 (ξj,−
n
, tj),
which we rewrite as
u(xn+1, tj) −2u(xn, tj) + u(xn−1, tj)
h2
= ∂2u
∂x2 (xn, tj) + h2
12
∂4u
∂x4 (ξ j
n, tj),
where ξ j
n ∈]xn−1, xn+1[, thanks to the intermediate value theorem.3 Taking into ac-
count the boundary conditions u(x0, tj) = u(xN+1, tj) = 0, we see that
εh,k(u)j = Sh
∂u
∂t −∂2u
∂x2

tj

−Fj + Rj
with
Rj
n = k
2
∂2u
∂t2 (xn, θj
n) −h2
12
∂4u
∂x4 (ξ j
n, tj).
Now since u is a regular solution of the heat equation, we have Sh
 ∂u
∂t −∂2u
∂x2

tj

−
Fj = 0 due to the deﬁnition of the sampling operator, even for j = 0 by continuity.
Moreover
|Rj
n| ≤max
1
2 max
¯Q
∂2u
∂t2
, 1
12 max
¯Q
∂4u
∂x4


(k + h2),
for all n and j, which concludes the proof of the proposition.
□
Remark 8.1 A natural question is to wonder whether the above order is optimal
and to make sure that we are not missing a better approximation rate. To settle this
question, it is sufﬁcient to pursue the Taylor–Lagrange expansions up to third order
in time and sixth order in space (assuming enough regularity for u) and see what
comes up. The outcome is
εh,k(u)j
n = k
2
∂2u
∂t2 (xn, tj) −h2
12
∂4u
∂x4 (xn, tj)
+ k2
6
∂3u
∂t3 (xn, ˜θj
n) −h4
720
∂6u
∂x6 (˜ξ j,−
n
, tj) + ∂6u
∂x6 (˜ξ j,+
n
, tj)

.
Now, the function (h, k) →Ak + Bh2 is identically 0 if and only if A = B = 0. Of
course, in general both ∂2u
∂t2 (xn, tj) and ∂4u
∂x4 (xn, tj) are nonzero, thus the truncation error
is not of a higher order.
□
3This is the same computation as in Chap.2.

8.2 The Implicit Euler and Leapfrog Schemes
259
8.2
The Implicit Euler and Leapfrog Schemes
Before describing and analyzing general ﬁnite difference schemes, we give two more
simple examples. The ﬁrst example is the implicit or backward Euler three point
scheme, which is associated with the backward differential quotient approximation
of the time derivative
∂u
∂t (xn, tj) ≈u(xn, tj) −u(xn, tj−1)
k
,
also used under the same name in the context of the numerical approximation of
ordinary differential equations. In vector form, this scheme reads
Uj −Uj−1
k
+ AhUj = Fj for j = 1, . . . , M + 1,
or equivalently
Uj+1 −Uj
k
+ AhUj+1 = Fj+1 for j = 0, . . . , M.
This scheme is called implicit, because the above formula is not a simple recurrence
relation. Indeed, Uj+1 appears as the solution of an equation once Uj is known. It is
not a priori clear that this equation is solvable. In this particular case, we have

Uj+1 = (I + kAh)−1(Uj + kFj+1) for j = 0, . . . , M,
U0 = U0,
since it is not hard to see that the matrix I + kAh is symmetric, positive deﬁnite, hence
invertible. In practical terms, the implementation of the backward Euler method
entails the solution of a linear system at each time step, whereas the explicit method
is simply a matrix-vector product and vector addition at each time step. The implicit
method is thus more computationally intensive than the explicit method, but it has
other beneﬁts as we will see later.
The analysis of the truncation error of the implicit Euler scheme is basically the
same as in the explicit case. The method is likewise consistent, of order 1 in time
and order 2 in space.
The second example is the leapfrog or Richardson method, which is associated
with the central differential quotient approximation of the time derivative
∂u
∂t (xn, tj) ≈u(xn, tj+1) −u(xn, tj−1)
2k
,
which leaps over time tj. In vector form, this scheme reads

260
8
The Finite Difference Method for the Heat Equation
Uj+1 −Uj−1
2k
+ AhUj = Fj for j = 1, . . . , M.
This scheme is an explicit two-step method since Uj+1 is explicitly given in terms
of Uj and Uj−1.

Uj+1 = Uj−1 −2kAhUj + 2kFj for j = 1, . . . , M,
U0 = U0, U1 = U1.
Of course, since this is a two-step method, a given value U1 supposed to approximate
Sh(ut1) must somehow be ascribed to U1 in order to initialize the recurrence, in
addition to U0.
The idea behind the leapfrog scheme is that the truncation error is of order 2
in time and order 2 in space, i.e., the truncation error is bounded from above by a
quantity of the form C(h2 + k2), which would seem to be advantageous as compared
to both Euler schemes. Unfortunately, we will see that the improved truncation error
is accompanied by instability, which prevents the method from being convergent. It
is not usable in practice for the heat equation, and this example shows that a naive
approach to ﬁnite difference schemes may very well badly fail.
8.3
General Finite Difference Schemes, Consistency,
Stability, Convergence
In this section, we introduce a general framework for dealing with ﬁnite difference
schemes. A ﬁnite difference scheme for the heat equation, or for any other linear
evolution partial differential equation, is constructed by forming linear combinations
ofpartialdifferentialquotientsandreplicatingtheselinearcombinationsonthepurely
discretelevel.Itcanbecastinthefollowingform:Letusbegiventwopositiveintegers
l and m with l + m ≥1, and a set of l + m + 1 matrices Bi, −m ≤i ≤l, each of size
N × N, the entries of which are functions of h and k. We assume that Bl is invertible.
A general l + m step ﬁnite difference scheme is then a recurrence relation for a
sequence of vectors Uj ∈RN, of the form
BlUj+l + Bl−1Uj+l−1 + · · · + B0Uj + · · · + B−mUj−m = Fj, m ≤j ≤(T/k) −l
(8.3)
with given initial data
U0 = U0, U1 = U1, . . . , Ul+m−1 = Ul+m−1.
The right-hand side vector Fj is to be constructed from f , but is not necessarily
equal to Fj. As before, the intended meaning of Uj with components uj
n is that uj
n is
expected to provide an approximation of u(xn, tj).

8.3 General Finite Difference Schemes, Consistency, Stability, Convergence
261
Deﬁnition 8.2 We say that the scheme (8.3) is explicit if the leading matrix Bl is
diagonal. Otherwise, the scheme is called implicit.
Inanexplicitmethod,thenextvectorUj+l isthusdirectlyobtainedfrompreviously
computed vectors by matrix-vector multiplications and vector additions, whereas an
implicit method entails the actual resolution of a linear system at each time step.
Let us see how the previously introduced schemes ﬁt into this general picture. For
forward Euler, we have
⎧
⎨
⎩
1
k Uj+1 +

−1
k I + Ah

Uj = Fj,
U0 = U0,
(8.4)
so that l = 1, m = 0, B1 = 1
k I, B0 = −1
k I + Ah and Fj = Fj. It is obviously one step
and explicit. Of course, we can also write it with for example B1 = I, B0 = −I + kAh
and Fj = kFj, there is no uniqueness of the general form for a given scheme. The
backward Euler method is
⎧
⎨
⎩
1
k I + Ah

Uj −1
k Uj−1 = Fj,
U0 = U0,
(8.5)
so that l = 0, m = 1, B0 = 1
k I + Ah, B−1 = −1
k I and Fj = Fj. It is obviously one
step and implicit (recall that 1
k I + Ah is invertible). Finally, the leapfrog scheme is
⎧
⎨
⎩
1
2k Uj+1 + AhUj −1
2k Uj−1 = Fj,
U0 = U0, U1 = U1.
(8.6)
so that l = 1, m = 1, B1 =
1
2k I, B0 = Ah, B−1 = −1
2k I and Fj = Fj. It is obviously
two step and explicit.
Remark 8.2 As mentioned before, there is no uniqueness of a general form for a
given scheme. Indeed, given a general form, we can obtain another one by multiplying
everythingbyanarbitraryfunctionof h andk,orevenbyanarbitraryN × N invertible
matrix function of h and k. So the deﬁnition of explicit or implicit scheme as stated
before is attached to a general form and not to the scheme under consideration.
However, it should be quite clear that writing the backward Euler scheme as
Uj −(I + kAh)−1Uj−1 = k(I + kAh)−1Fj

262
8
The Finite Difference Method for the Heat Equation
and thus declaring it explicit, is somehow cheating. Indeed, the matrix (I + kAh)−1
is not know explicitly.4 Thus the real issue is an implementation issue: do we need
to numerically solve a nontrivial linear system to compute the scheme, or not? In the
former case, the scheme is implicit and the latter case, it is explicit.
Different general forms give rise to different truncation errors, see Deﬁnition8.3
below. As a general rule, it is better to choose the form that naturally comes from the
discretization of the partial derivatives by differential quotients.
□
There is nothing in the deﬁnition of a general ﬁnite difference scheme given
above that even alludes to a particular partial differential equation that we might
be interested in approximating. We therefore need a way of comparing the vectors
Uj ∈RN and the function u solution of problem (8.1). As in the stationary case
of Chap.2, an obvious idea is to use the sampling operator already introduced in
Deﬁnition8.1.
Even then, quantitatively comparing two vectors of RN involves the choice of a
norm on RN. We are ultimately interested in letting N →+∞, thus we need a norm
for each value of N. There is no reason at this point to do anything else that to choose
an arbitrary norm ∥· ∥N on RN for each N. Two popular choices are
∥U∥∞,h = max
1≤n≤N |Un| and ∥U∥2,h =
√
h
 N

n=1
U2
n
1/2
,
(recall that h =
1
N+1). The reason for the
√
h factor in the second norm is for com-
parison with the L2 norm in the limit h →0. Of course, it is well-known that any
two norms on RN are equivalent, but the constants in the norm equivalence depend
on N. For instance,
∥U∥2,h ≤∥U∥∞,h ≤
1
√
h
∥U∥2,h
with basically optimal constants. This shows that consistency and convergence (see
Deﬁnitions8.4 and 8.6 below) in the ∞, h norms imply consistency and convergence
in the 2, h norms, but not the converse.
We can now give a few deﬁnitions.
Deﬁnition 8.3 Let u be a sufﬁciently regular solution of problem (8.1). The trunca-
tion error of the general ﬁnite difference method (8.3) is the sequence of vectors
εh,k(u)j = BlSh(utj+l) + · · · + B0Sh(utj) + · · · + B−mSh(utj−m) −Fj,
for m ≤j ≤(T/k) −l.
Again, we just replace the discrete unknown with the grid sampling of the solu-
tion in the ﬁnite difference scheme formula. Note that, since for any given scheme,
4Well, actually it may well be known somewhere in the literature, but let us assume it is not known
for the sake of the argument.

8.3 General Finite Difference Schemes, Consistency, Stability, Convergence
263
there are inﬁnitely many different general formulas describing the same scheme, the
truncation error of a given scheme depends on how it is written in general form. For-
tunately, this is totally irrelevant for the ensuing analysis. We just need to be careful
in the application of the general results in each particular case.
Deﬁnition 8.4 We say that the scheme in general form (8.3) is consistent for the
family of norms ∥· ∥N if
max
m≤j≤(T/k)−l ∥εh,k(u)j∥N →0 when (h, k) →(0, 0).
We say that it is of order p in space and q in time for the family of norms ∥· ∥N if
max
m≤j≤(T/k)−l ∥εh,k(u)j∥N ≤C(hp + kq),
where the constant C only depends on u.
Consistency means that the scheme is trying its best to locally approximate the
right partial differential equation problem in the norm ∥· ∥N. Of course, the above
deﬁnitions depend on the choice of norm and on the choice of general form. A given
scheme may well be consistent for one family of norms and not for another, or be of
some order in one general form and of another order in another general form. It is up to
us to choose the best norm/general form combination. As in the particular cases that
we have already seen, checking consistency and computing time and space orders is
just a matter of having enough patience to write down the relevant Taylor–Lagrange
expansions.
A signiﬁcantly subtler notion is that of stability.
Deﬁnition 8.5 Let S ⊂R∗
+ × R∗
+ be such that (0, 0) ∈
¯
S . We say that the scheme
in general form (8.3) is stable for the family of norms ∥· ∥N under condition S , if
there exists two constants C1(T) and C2(T) which only depend on T such that, for
all (h, k) ∈S , all initial data Uj′, 0 ≤j′ ≤l + m −1, and all right-hand sides Fj′′,
m ≤j′′ ≤(T/k) −l, we have
max
j≤T/k ∥Uj∥N ≤C1(T)
max
0≤j′≤l+m−1 ∥Uj′∥N + C2(T)
max
m≤j′′≤(T/k)−l ∥Fj′′∥N.
(8.7)
If S ⊃]0, a[ × ]0, a[ for some a > 0, we say that the scheme is unconditionally
stable.
Stability makes no reference to the partial differential equation. It is just a property
of the recurrence relation which controls the growth of its solutions in terms of the
initial data and right-hand side. Stability in the present sense is very different from
stability in the stationary case.
Note that in spite of the previous fact, the introduction of stability must be put
in perspective with such continuous energy estimates as estimate (7.2). It is in fact
a discrete version of such estimates in the case of the 2, h norms. We also refer to

264
8
The Finite Difference Method for the Heat Equation
Sect.8.9 for a particularly striking parallel between the stability in the continuous
and discrete cases.
Deﬁnition 8.6 We say that the scheme is convergent for the family of norms ∥· ∥N
if
max
j≤T/k ∥Uj −Sh(utj)∥N →0 when (h, k) →(0, 0), (h, k) ∈S .
Note that this deﬁnition is independent of the general form under which the scheme is
written, as opposed to the two previous deﬁnitions. Remembering that Sh(utj) is just
a notation for u(xn, tj), n = 1, . . . , N, we see that convergence of the scheme means
that |uj
n −u(xn, tj)| tends to 0 (at least if the choice of norms ∥· ∥N is reasonable
enough), or that the computed discrete unknowns uj
n are in effect approximations of
the value of the solution at gridpoints.
The relevance of the above deﬁnitions is clariﬁed by means of the Lax theorem:
Theorem 8.1 (Lax Theorem) Assume that
max
0≤j′≤l+m−1 ∥Uj′ −Sh(utj′ )∥N →0 when (h, k) →(0, 0), (h, k) ∈S .
If the scheme in general form (8.3) is consistent and stable under condition S for
the family of norms ∥· ∥N, then it is convergent for that same family of norms.
Proof Let us compare the formulas for the truncation error and for the scheme.
BlSh(utj+l) + · · · + B0Sh(utj) + · · · + B−mSh(utj−m) = εh,k(u)j + Fj,
BlUj+l + · · · + B0Uj + · · · + B−mUj−m = Fj.
Setting V j = Uj −Sh(utj) and subtracting the above two formulas, we see that
BlV j+l + · · · + B0V j + · · · + B−mV j−m = −εh,k(u)j,
with the initial data
V j′ = Uj′ −Sh(utj′ ) for 0 ≤j′ ≤l + m −1.
By the stability hypothesis, it follows that
max
j≤T/k ∥V j∥N ≤C1
max
0≤j′≤l+m−1 ∥Uj′ −Sh(utj′ )∥N + C2
max
m≤j′′≤(T/k)−l ∥εh,k(u)j′′∥N,
and the right-hand side goes to 0 by consistency and the hypothesis on the initial
data for the scheme.
□
Remark 8.3 We have written here the most useful part of the Lax theorem, i.e.,
consistency plus stability imply convergence, see [7, 29, 67, 81]. There is a less

8.3 General Finite Difference Schemes, Consistency, Stability, Convergence
265
useful converse part, see Proposition8.4 later on. Therefore, we have not missed
anything by focusing on consistency and stability.
□
Corollary 8.1 Assume that the scheme is stable under condition S and of order p
in space and q in time for the family of norms ∥· ∥N, and that
max
0≤j′≤l+m−1 ∥Uj′ −Sh(utj′ )∥N ≤C(hp + kq),
for some constant C. Then
max
j≤T/k ∥Uj −Sh(utj)∥N ≤C′(hp + kq),
where C′ only depends on T and u.
Remark 8.4 High order stable schemes thus result in (in principle) more accurate
approximations than low order schemes. This is conditional on the initial data for
the scheme not destroying this accuracy. If the scheme uses several time steps, the
corresponding initial data must therefore be computed by using some other, equally
accurate method. If the scheme is one time step, then we are at liberty to have exact
initial data (discounting round-off errors).
□
Remark 8.5 Let us emphasize again that all this is highly dependent on the choice
of norms. Assume that, for one outrageous reason or another, we had chosen
∥U∥N = 2−N∥U∥∞,h. Then, it is likely that even the most wildly non consistent
scheme for the ∞, h norms would become consistent for the new norms! Since sta-
bility is not affected by multiplication of the norm by a constant, if the scheme was
stable for the ∞, h norms, then it would also be stable for the ∥· ∥N norms.5 Hence,
by the Lax theorem, it would be convergent for that family of norms. There is how-
ever no contradiction. Indeed, saying that 2−N|uj
n −u(xn, tj)| →0 tells us next to
nothing about what we really are interested in, namely, is uj
n a good approximation
of u(xn, tj) or not. In this respect, the two choices ∥· ∥∞,h and ∥· ∥2,h are much more
natural.
□
Remark 8.6 Itshouldalsobenotedthatthefactthattheunderlyingpartialdifferential
equation is the heat equation plays no role in the Lax theorem. The theorem holds true
for any ﬁnite difference scheme devised to approximate the solution of any evolution
partial differential equation problem in one or several dimensions of space. We will
thus also use it for hyperbolic equations in Chaps.9 and 10.
□
Let us apply the previous results to the explicit Euler scheme. Let ∥· ∥N be a norm
on RN and ||| · |||N the associated induced matrix norm, see Chap.2. For any N × N
real matrix A and vector U ∈RN, we have
∥AU∥N ≤|||A|||N∥U∥N.
5Note that stability is also affected by the general form used to write the scheme, via the term Fj,
see Proposition8.3 and Remark8.12 for a more precise statement.

266
8
The Finite Difference Method for the Heat Equation
In the particular case of the ∞, h norm, we have seen in Proposition2.3 of Chap. 2
that
|||A|||∞,h = max
1≤n≤N
 N

j=1
|Anj|

.
Let us choose the general form
1
k Uj+1 −1
k Uj + AhUj = Fj,
for which we have consistency in the ∥· ∥∞,h norms and Fj = Fj.
Proposition 8.2 Let S =

(h, k) ∈R∗
+ × R∗
+; k
h2 ≤1
2

. The explicit three-point
Euler scheme in this general form is stable under condition S for the norms ∥· ∥∞,h,
hence convergent for these norms.
Proof The above general form can be rewritten as
Uj+1 = Ah,kUj + kFj,
where Ah,k = I −kAh. Therefore
∥Uj+1∥∞,h ≤|||Ah,k|||∞,h∥Uj∥∞,h + k∥Fj∥∞,h.
Let us set r = k
h2 . By direct inspection, we see that
Ah,k =
⎛
⎜⎜⎜⎜⎜⎜⎝
1 −2r
r
0 · · ·
0
r
1 −2r r · · ·
0
...
...
... ...
...
...
...
... ...
r
0
· · ·
0
r 1 −2r
⎞
⎟⎟⎟⎟⎟⎟⎠
.
It follows that
|||Ah,k|||∞,h = |1 −2r| + 2r =

1
if r ≤1
2,
4r −1 if r > 1
2.
Therefore, if r ≤1
2, we have that
∥Uj+1∥∞,h ≤∥Uj∥∞,h + k∥Fj∥∞,h
≤∥Uj∥∞,h + k max
n≤T/k ∥Fn∥∞,h.
Iterating backwards, we obtain that for all j such that j ≤T
k ,

8.3 General Finite Difference Schemes, Consistency, Stability, Convergence
267
Fig. 8.1 Convergence of the
explicit Euler method when
stability is satisﬁed
0.0e+00
5.0e-06
1.0e-05
1.5e-05
2.0e-05
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
∥Uj∥∞,h ≤∥U0∥∞,h + jk max
n≤T/k ∥Fn∥∞,h
≤∥U0∥∞,h + T max
n≤T/k ∥Fn∥∞,h,
hence the stability of the scheme for the norm ∞, h under condition S .
□
We plot6 in Fig.8.1 the discrete values UM+1 with M corresponding to T = 1.1,
for the initial value u0(x) = 4x(1 −x). Each curve corresponds to different values
of N going from 3 to 30, and choosing M in such a way that k
h2 ≈0.49.
Remark 8.7 The above estimates are not sufﬁcient to conclude that the scheme is
not stable when r > 1
2. However, numerical experiments with r > 1
2 quickly show
that the explicit Euler scheme is non convergent for the ∞, h norm. Since it is
consistent, this means it must be unstable. In particular, round-off errors are ampliﬁed
exponentially fast.
To illustrate this, we plot in Fig.8.2 the same sequence of computations as above,
with k
h2 ≈0.51.
Note the extreme and erratic variations in the vertical scale of the above plots
depending on N (and its parity). The explicit Euler scheme appears to be wildly
non convergent for such values of the discretization parameters, due to the failure of
stability even by a very small amount.
□
Remark 8.8 When (h, k) ∈S , we thus have k ≤h2
2 ≪h for h small. For instance,
if we want a modest amount of 1000 points in the space grid, then the time step must
be smaller than 5 × 10−7, i.e., to compute up to a ﬁnal time of T = 1s, we need
at least 2 × 106 iterations in time. Such stability requirements can rapidly make the
scheme too computationally expensive, in spite of its otherwise simplicity.
□
6For an easier visualization, we also plot a linear interpolation of the ﬁnite difference discrete values.

268
8
The Finite Difference Method for the Heat Equation
0e+00
1e-06
2e-06
3e-06
4e-06
5e-06
6e-06
7e-06
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 3, h = 0.25, M = 34, k = 0.0314286, k/h^2 = 0.5028571
0.0e+00
2.0e-06
4.0e-06
6.0e-06
8.0e-06
1.0e-05
1.2e-05
1.4e-05
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 6, h = 0.1428571, M = 105, k = 0.0103774, k/h^2 = 0.5084906
-3e-05
-2e-05
-1e-05
0e+00
1e-05
2e-05
3e-05
4e-05
5e-05
6e-05
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 9, h = 0.1, M = 215, k = 0.0050926, k/h^2 = 0.5092593
0.0e+00
5.0e-06
1.0e-05
1.5e-05
2.0e-05
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 12, h = 0.0769231, M = 364, k = 0.0030137, k/h^2 = 0.5093151
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 15, h = 0.0625, M = 552, k = 0.0019892, k/h^2 = 0.5092224
0.0e+00
5.0e-06
1.0e-05
1.5e-05
2.0e-05
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 18, h = 0.0526316, M = 778, k = 0.0014121, k/h^2 = 0.5097561
-4e+08
-3e+08
-2e+08
-1e+08
0e+00
1e+08
2e+08
3e+08
4e+08
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 21, h = 0.0454545, M = 1043, k = 0.0010536, k/h^2 = 0.5099617
-20
-15
-10
-5
0
5
10
15
20
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 24, h = 0.04, M = 1348, k = 0.0008154, k/h^2 = 0.5096368
-2.0e+19
-1.5e+19
-1.0e+19
-5.0e+18
0.0e+00
5.0e+18
1.0e+19
1.5e+19
2.0e+19
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 27, h = 0.0357143, M = 1690, k = 0.0006505, k/h^2 = 0.5099941
-4e+13
-3e+13
-2e+13
-1e+13
0e+00
1e+13
2e+13
3e+13
4e+13
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = 30, h = 0.0322581, M = 2072, k = 0.0005306, k/h^2 = 0.5099373
Fig. 8.2 Divergence of the explicit Euler method when stability is violated

8.3 General Finite Difference Schemes, Consistency, Stability, Convergence
269
Remark 8.9 The above example shows that it is fairly easy to give sufﬁcient condi-
tions of stability in the ∞, h norms for explicit schemes, which is thus a reasonable
choice of norms for such schemes. In the case of an implicit scheme, determining
stability in the ∞, h norm family can in some cases be obtained by using the discrete
maximum principle, in the absence of the explicit knowledge of the inverse matrix
involved. Another case when the ∞, h norm of the inverse matrix can be estimated
is if there exists δ > 0 such that |Ann| ≥δ + 
j̸=n |Anj| for all n. In this case, it is
easy to show that ∥A−1∥∞,h ≤1
δ . If δ ≥1, then the implicit scheme is stable in the
∞, h norms.
□
Remark 8.10 We will see later that the 2, h norms are more adapted to implicit
schemes since it is possible in certain cases to compute the 2, h norm of an inverse
matrix without computing the inverse in question explicitly.
□
Corollary 8.2 When k
h2 ≤1
2, the explicit three-point Euler scheme satisﬁes the error
estimate
max
n,j |uj
n −u(xn, tj)| ≤Ch2.
Proof This is a consequence of Corollary8.1, Proposition8.1, and the fact that we
have k ≤h2
2 .
□
The second order accuracy is however obtained at the expense of a lot of iterations
in time, see Remark8.8.
8.4
General Criteria for Stability
We have seen that proving consistency is always a matter of combining several
Taylor–Lagrange expansions together, which can be tedious but does not pose much
difﬁculty in principle. Stability is another matter.
Let us consider a general scheme (8.3). We rewrite it as a one time step scheme
of the form
V j+1 = Ah,kV j + Gj,
(8.8)
where V j ∈R(l+m)N is deﬁned as
V j =
⎛
⎜⎜⎜⎜⎜⎜⎝
Uj+l−1
...
Uj
...
Uj−m
⎞
⎟⎟⎟⎟⎟⎟⎠
for m ≤j ≤M + 2 −l,
the matrix Ah,k is the (l + m)N × (l + m)N matrix

270
8
The Finite Difference Method for the Heat Equation
Ah,k =
⎛
⎜⎜⎜⎜⎜⎝
−B−1
l Bl−1 −B−1
l Bl−2
· · ·
· · · −B−1
l B−m
I
0
· · ·
· · ·
0
0
I
0
· · ·
0
...
...
...
...
...
0
· · ·
0
I
0
⎞
⎟⎟⎟⎟⎟⎠
and
Gj =
⎛
⎜⎜⎜⎝
B−1
l Fj
0
...
0
⎞
⎟⎟⎟⎠.
The matrix Ah,k is called the ampliﬁcation matrix of the scheme. It depends on k
and h through its coefﬁcients and it must not be forgotten that its size also depends
on h = 1/(N + 1). Note that the ampliﬁcation matrix Ah,k and the term Gj do not
depend on the general form (8.3), but only on the scheme itself, in the sense that two
general forms for the same scheme will lead to the same iteration (8.8).
In view of the Lax theorem, it is important to prove stability for a general form
(8.3) for which we also have consistency. We give a ﬁrst stability criterion for the
family of norms ∥· ∥N. We associate to this family of norms the family
∥V ∥l+m,N =
max
−m≤k≤l−1 ∥Uk∥N,
with an obvious deﬁnition of V ∈R(l+m)N in terms of Uk ∈RN. For each N, this is
a norm on R(l+m)N. We denote the induced matrix norm by ||| · |||l+m,N.
Proposition 8.3 Let us consider a general scheme in the form (8.3) for which there
exists a constant C0 independent of h and k such that |||B−1
l |||N ≤C0k. Such a scheme
is stable under condition S if and only if there exists a constant C(T) depending
only on T such that for all (h, k) ∈S ,7
max
j≤(T/k)+1−(l+m) |||A j
h,k|||l+m,N ≤C(T).
(8.9)
Proof We remark that, by deﬁnition of the ∥· ∥l+m,N norm, we have
max
j≤T/k ∥Uj∥N =
max
m≤j≤(T/k)+1−l ∥V j∥l+m,N.
(8.10)
Let us ﬁrst assume that the general form of the scheme is stable. This means
that there exist two constants C1(T) and C2(T) such that for any U0 and Fj and all
(h, k) ∈S
7Beware of the notation: up to now V j meant the jth vector in the sequence, but here A j means the
jth power of the matrix A .

8.4 General Criteria for Stability
271
max
j≤T/k ∥Uj∥N ≤C1(T)
max
0≤j′≤l+m−1 ∥Uj′∥N + C2(T)
max
m≤j′′≤(T/k)−l ∥Fj′′∥N.
We take Fj′′ = 0 for all j′′. In this case, V j = A j−m
h,k V m so that we have
max
m≤j≤(T/k)+1−l ∥A j−m
h,k V m∥l+m,N ≤C1(T)∥V m∥l+m,N.
Since this is true for all V m ∈R(l+m)N, it follows that
max
m≤j≤(T/k)+1−l |||A j−m
h,k |||l+m,N ≤C1(T).
Changing j −m into j, we obtain estimate (8.9).
Conversely, assume that estimate (8.9) holds true for all (h, k) ∈S . We can write,
for all m + 1 ≤j ≤(T/k) + 1 −l
V j = Ah,kV j−1 + Gj−1
Ah,kV j−1 = A 2
h,kV j−2 + Ah,kGj−2
...
...
A j−m−1
h,k
V m+1 = A j−m
h,k V m + A j−m−1
h,k
Gm,
so that summing these equations, we obtain
V j = A j−m
h,k V m +
j−1

n=m
A j−n−1
h,k
Gn.
Therefore
∥V j∥l+m,N ≤∥A j−m
h,k V m∥l+m,N +
j−1

n=m
∥A j−n−1
h,k
Gn∥l+m,N
≤C(T)∥V m∥l+m,N + C(T)
j−1

n=m
∥Gn∥l+m,N
≤C(T)∥V m∥l+m,N + (j −m)C(T)
max
m≤n≤j−1 ∥Gn∥l+m,N
≤C(T)∥V m∥l+m,N + T
k C(T)
max
n≤(T/k)−l ∥Gn∥l+m,N
whenever m + 1 ≤j ≤T/k. In addition, the ﬁnal estimate holds trivially for j = m.
Recall now that ∥Gn∥l+m,N = ∥B−1
l Fn∥N. Therefore
∥Gn∥l+m,N ≤|||B−1
l |||N∥Fn∥N ≤C0k∥Fn∥N

272
8
The Finite Difference Method for the Heat Equation
in view of the hypothesis made on B−1
l . We thus obtain the stability under condition
S , with C1(T) = C(T) and C2(T) = TC0C(T), because of Eq.(8.10) and the fact
that ∥V m∥l+m,N = max0≤j′≤l+m−1 ∥Uj′∥N.
□
Remark 8.11 The hypothesis |||B−1
l |||N ≤C0k is essential for the above estimate even
though it seems to be often overlooked in the literature, in which the deﬁnition of
stability is also often different and weaker than the one we present here. Of course,
as was said before, the usefulness of stability is only for general forms for which
consistency holds. In particular, this hypothesis is clearly satisﬁed for the forward
Euler scheme in the form (8.4) for any family of norms since |||I|||N = 1.
□
Remark 8.12 Under the above hypothesis on B−1
l , it follows that stability does not
depend on the right-hand side of the equation. In other words, it is enough to test it
for a zero right-hand side. More precisely, stability in the sense of Deﬁnition8.5 is
implied by the less demanding estimate
max
j≤T/k ∥Uj∥N ≤C1(T)
max
0≤j′≤l+m−1 ∥Uj′∥N,
for all (h, k) ∈S , where Uj is any solution of the scheme with zero right-hand side.
This is called the Dahlquist zero-stability condition, see [49] for example.
□
The criterion given in Proposition8.3 is not too practical in general, since the
quantity maxj |||A j
h,k|||l+m,N is not necessarily easy to estimate. Nonetheless, we have
a sufﬁcient condition as an immediate corollary.
Corollary 8.3 If|||Ah,k|||l+m,N ≤1forall(h, k) ∈S ,thentheschemeisstableunder
condition S .
Proof An operator norm is submultiplicative, i.e., |||AB|||l+m,N ≤|||A |||l+m,N
|||B|||l+m,N for any A and B. Therefore, |||A j
h,k|||l+m,N ≤|||Ah,k|||j
l+m,N ≤1 for all
j, thus in particular for all j smaller than(T/k) + 1 −(l + m).
□
This is what we actually did for the forward Euler scheme and the ∞, h norms
in the proof of Proposition8.2. In the case of the 2, h norms, we will see in the next
section that it is possible to be a little more precise and give a necessary and sufﬁcient
condition of stability for a certain class of ampliﬁcation matrices. This is one of the
main reasons for using these norms, see Sect.8.5.
Let us now show the converse of the Lax theorem.
Proposition 8.4 Let us be given a scheme that is convergent under condition S for
the family of norms ∥· ∥N. Then any general form (8.3) such that |||B−1
l |||N ≤C0k is
stable under condition S in the sense of Deﬁnition8.5.
Proof We ﬁrst rewrite the scheme in the form (8.8). We argue by contradiction. We
assume that there is no constant C(T) such that inequality (8.9) holds in a neigh-
borhood of (0, 0) in S .8 We are thus given a sequence (hn, kn) ∈S such that
8This is the only relevant case.

8.4 General Criteria for Stability
273
(hn, kn) →(0, 0) (so that Nn →+∞). Let Ahn,kn = An be the corresponding se-
quence of ampliﬁcation matrices. By hypothesis, there exists a subsequence n′ and
a sequence jn′ such that jn′ ≤(T/kn′) + 1 −(l + m) with
|||A jn′
n′ |||l+m,Nn′ = λn′ →+∞when n′ →+∞.
By compactness in ﬁnite dimensional spaces, there exists Vn′ ∈R(l+m)Nn′ such that
∥Vn′∥l+m,Nn′ = 1 and ∥A jn′
n′ Vn′∥l+m,Nn′ = λn′.
Let us set Wn′ = Vn′/λn′. It follows that
∥Wn′∥l+m,Nn′ →0 and ∥A jn′
n′ Wn′∥l+m,Nn′ = 1.
The ﬁrst convergence shows that the vectors Wn′ are appropriate discrete initial
conditions for the scheme applied to the case u0 = 0 and f = 0. The scheme being
convergent in this family of norms, it follows that ∥A jn′
n′ Wn′∥l+m,Nn′ →0, which
contradicts the second relation above.
Finally, the hypothesis |||B−1
l |||N ≤C0k combined with estimate (8.9) implies the
stability of the general form by Proposition8.3.
□
From now on, we will concentrate on one time step schemes in the case of the
2, h norms.
8.5
Stability for One Time Step Schemes in the 2, h Norms
Let us consider a general scheme (8.3) with one time step, i.e., l = 1, m = 0 or l = 0,
m = 1. As before, we rewrite the scheme as
Uj+1 = A Uj + Gj,
where
A =

−B−1
1 B0
if l = 1, m = 0,
−B−1
0 B−1
if l = 0, m = 1,
and
Gj =

B−1
1 Fj
if l = 1, m = 0,
B−1
0 Fj+1
if l = 0, m = 1.
The ampliﬁcation matrix A is now a N × N matrix. We henceforth omit the h, k
subscript in ampliﬁcation matrices for notational brevity.

274
8
The Finite Difference Method for the Heat Equation
We ﬁrst need to determine the 2, h induced matrix norms. We let ρ(B) denote the
spectral radius of a matrix B, i.e., ρ(B) = max{|λp|, p = 1, . . . , N}, where λp ∈C
are the eigenvalues of B.
Proposition 8.5 Let A be a real N × N matrix. We have
|||A|||2,h =

ρ(ATA).
Proof For all X ∈RN, we have ∥AX∥2
2,h = h(AX)TAX = hXT(ATA)X. The matrix
ATA is symmetric, hence it is orthogonally diagonalizable: there exists an orthogonal
matrix Q, i.e., a real matrix such that QTQ = QQT = I, such that ATA = QTDQ
where D is a diagonal matrix, the diagonal entries of which are the eigenvalues dp of
ATA. This matrix is also nonnegative, so that these eigenvalues are all nonnegative.
Therefore, ρ(ATA) is simply the largest eigenvalue of ATA.
We can thus write
∥AX∥2
2,h = hXT(QTDQ)X = h(QX)TD(QX).
If we set Y = QX, then ∥Y∥2,h = ∥X∥2,h since Q is orthogonal and
∥AX∥2
2,h = hY TDY = h
N

p=1
dpY 2
p
≤hρ(ATA)
N

p=1
Y 2
p = ρ(ATA)∥Y∥2
2,h = ρ(ATA)∥X∥2
2,h.
Taking the square root and dividing by ∥X∥2,h for X ̸= 0, we thus obtain
|||A|||2,h ≤

ρ(ATA).
Let now X be a unit eigenvector of ATA associated with the eigenvalue ρ(ATA),
ATAX = ρ(ATA)X and ∥X∥2
2,h = hXTX = 1. For this vector, we have
∥AX∥2
2,h = hXT(ATAX) = hρ(ATA)XTX = ρ(ATA),
from which we infer that
|||A|||2,h ≥

ρ(ATA),
and the Proposition is proved.
□
A real matrix A is said to be normal if ATA = AAT. In particular, a real symmetric
matrix is normal.

8.5 Stability for One Time Step Schemes in the 2, h Norms
275
Proposition 8.6 If A is a normal matrix, then ρ(A) = ρ(ATA)1/2 = |||A|||2,h.
Proof A normal matrix A is unitarily similar to a diagonal matrix Λ. Thus there
exists a unitary matrix U, i.e., a complex matrix satisfying U∗U = UU∗= I,9 such
that A = U∗ΛU and therefore the diagonal entries of Λ are the eigenvalues λp ∈C
of A. It follows that
ATA = U∗Λ∗(UU∗)ΛU = U∗Λ∗ΛU,
so that the eigenvalues of ATA are the diagonal entries of Λ∗Λ, namely |λp|2,
p = 1, . . . , N. Thus ρ(A)2 =

max |λp|
2 = ρ(ATA).
□
Of course, for a general non normal matrix A, we only have ρ(A) ≤|||A|||2,h, with
often a strict inequality (take for instance a nonzero nilpotent matrix A for which
ρ(A) = 0). Let us apply the above considerations to ﬁnite difference schemes. In the
sequel, when we say that a scheme is stable, we refer to a scheme written in a general
form satisfying the hypotheses of Proposition8.3.
Proposition 8.7 If the ampliﬁcation matrix A is normal, then the scheme is stable
for the norms ∥· ∥2,h if and only of there exists a constant C′(T) ≥0 depending only
on T such that
ρ(A ) ≤1 + C′(T)k.
(8.11)
Proof Let us ﬁrst assume that there exists C′(T) ≥0 such that ρ(A ) ≤1 + C′(T)k.
By hypothesis, A is normal, therefore A j is also normal and |||A j|||2,h = ρ(A j) =
ρ(A )j. Consequently, for all j ≤T/k,
|||A j|||2,h ≤(1 + C′(T)k)j ≤eC′(T)kj ≤eC′(T)T,
and the constant eC′(T)T depends only on T. Therefore, the scheme is stable according
to Proposition8.3.
Conversely, assume that the scheme is stable. By Proposition8.3 again, this im-
plies that ρ(A )j ≤C(T) or ρ(A ) ≤C(T)1/j for all j ≤T/k. There are two cases.
Either C(T) ≤1 and thus ρ(A ) ≤1 and we are done with C′(T) = 0, or C(T) > 1.
In this case, we take j = T/k so that
ρ(A ) ≤C(T)
k
T = e
k
T ln(C(T)),
with ln(C(T)) > 0. This implies that the function s →es ln(C(T)) is convex on
[0, 1], which in turn implies that for all s ∈[0, 1], es ln(C(T)) ≤(1 −s) + seln(C(T)) =
1 + s(C(T) −1). In particular, for s = k/T, we obtain
9For any matrix A, A∗denotes the adjoint matrix of A, i.e., its conjugate transpose.

276
8
The Finite Difference Method for the Heat Equation
ρ(A ) ≤1 + C(T) −1
T
k,
hence estimate (8.11) with C′(T) = C(T) −1
T
.
□
Remark 8.13 Inspection of the above proof shows that the fact that the matrix is
normal is not used in the converse part of Proposition8.7. Therefore, condition (8.11)
is a necessary condition of stability for any matrix A .
□
Remark 8.14 It is important to stress again that the matrix A is a function of k and h,
and so is its spectral radius. Therefore, the above estimates are by no means obvious.
Note that a sufﬁcient condition for stability in the 2, h norm in the case of a
normal ampliﬁcation matrix, often used in practice, is thus that ρ(A ) ≤1. This is
particularly indicated if we are interested in the computation of long term behavior
of the solution, i.e., T large. Indeed, the less demanding condition (8.11) allows for
exponential growth with T.
We now see that the reason for Remark8.10 is that the eigenvalues of an inverse
matrix are just the inverses of the eigenvalues of that matrix.
□
Let us apply all of the above to both Euler schemes and to the leapfrog scheme. We
ﬁrst need to determine the eigenvalues of the kind of tridiagonal matrices involved
in these schemes.
Lemma 8.1 Consider the N × N matrix
A =
⎛
⎜⎜⎜⎜⎜⎝
a b
0 · · · 0
b a
b · · · 0
... ... ... ... ...
0 · · · b
a b
0 · · · 0
b a
⎞
⎟⎟⎟⎟⎟⎠
,
with a, b ∈R. The eigenvalues of A are given by
λp = a + 2b cos
 pπ
N + 1

, p = 1, . . . , N.
Proof We have A = aI + bB with B =
⎛
⎜⎜⎜⎜⎝
0 1 · · · 0
1 ... ... ...
... ... ... 1
0 · · · 1 0
⎞
⎟⎟⎟⎟⎠
. Of course, AV = λV is equiv-
alent to BV = λ−a
b V . It is thus sufﬁcient to ﬁnd the eigenvalues of B.
Let V ∈RN\{0} be an eigenvector of B associated with the eigenvalue λ. We thus
have

8.5 Stability for One Time Step Schemes in the 2, h Norms
277
V2 = λV1,
V1 + V3 = λV2,
...
Vj−1 + Vj+1 = λVj,
...
VN−1 = λVN.
If we set V0 = VN+1 = 0, we see that the components of V are a solution of the
linear homogeneous recurrence relation with constant coefﬁcients
Vj−1 −λVj + Vj+1 = 0, for j = 0, . . . , N.
It is well known that the general solution of such a recurrence relation depends on
the roots of its characteristic equation r2 −λr + 1 = 0. There are two roots r1 and
r2 which are such that r1 + r2 = λ and r1r2 = 1. If the roots are simple, we have
Vj = αrj
1 + βrj
2, and if there is a double root, Vj = (α + βj)rj, where α and β are
constants to be determined.
Now the question is: given λ, can we ﬁnd α and β such that V0 = VN+1 = 0
with V ̸= 0 (this is a kind of boundary value problem in a sense)? In the case of a
double root, clearly these conditions imply α = β = 0, hence such a λ cannot be an
eigenvalue. Let us thus consider the case of simple roots. We are thus requiring that
0 = V0 = α + β and 0 = VN+1 = αrN+1
1
+ βrN+1
2
.
Therefore, we see that λ is an eigenvalue if and only if rN+1
1
= rN+1
2
. Multiplying
this equation by rN+1
1
, it follows that r2(N+1)
1
= 1. Consequently, r1 and r2 must be
of the form
r1 = ei pπ
N+1 ,
r2 = e−i pπ
N+1
for some integer p. It follows that the eigenvalues are necessarily of the form
λp = ei pπ
N+1 + e−i pπ
N+1 = 2 cos
 pπ
N + 1

.
Conversely, it is easy to check that the vector Vp given by (Vp)j = sin
 jpπ
N+1

, j =
1, . . . , N, is an associated eigenvector. For p = 1, . . . , N, we have thus obtained N
distinct eigenvalues, hence we have found them all.
□
Let us remark that the vectors Vp are eigenvectors for all matrices A independently
of the values of a and b.

278
8
The Finite Difference Method for the Heat Equation
Corollary 8.4 The eigenvalues of Ah are
λp = 4
h2 sin2
pπ
2(N + 1)

, p = 1, . . . , N.
Proof Apply the previous Lemma with a = 2
h2 and b = −1
h2 .
□
We now return to the explicit Euler scheme (8.4).
Proposition 8.8 Let S ⊂R∗
+ × R∗
+. The explicit three-point Euler scheme is stable
for the norms ∥· ∥2,h under condition S if and only if
S ⊂

(h, k); k
h2 cos2πh
2

≤1
2 + Ck

,
(8.12)
for some C ≥0. In this case, it is convergent for these norms and we have the error
estimate
max
j≤T/k ∥Uj −Sh(utj)∥2,h ≤Ch2,
where C depends only on u and T.
Proof First of all, since B−1
1
= kI so that |||B−1
1 |||2,h = k, we can apply Proposition8.3.
We have A = I −kAh. It is a real symmetric matrix, hence a normal matrix. We may
thus apply Proposition8.7. We have
ρ(A ) = max
1≤p≤N
1 −4k
h2 sin2
pπ
2(N + 1)

= max

1 −4k
h2 sin2
π
2(N + 1)

, 4k
h2 sin2
Nπ
2(N + 1)

−1

.
The ﬁrst expression in the maximum is always between 0 and 1. We thus just need
to consider the second expression. Condition (8.12) is then a simple rewriting of
condition (8.11).
□
Remark 8.15 Note that if k
h2 ≤1
2, then k
h2 cos2 πh
2

≤1
2 for all h and k. The region

(h, k); k
h2 ≤1
2

is thus a stability region, as was the case for the ∞, h norm. Besides,
convergence in the ∞, h norm implies convergence in the 2, h norm, so that nothing
new is gained in this case.
We know a little more about instability in the 2, h norm, which on the other hand
does not directly imply instability in the ∞, h norm. However, using the converse part
of the Lax theorem, if the scheme is unstable in the 2, h norm, it is not convergent in
the 2, h norm. Therefore, it is not convergent in the ∞, h norm. Since it is consistent
in the ∞, h norms, it is thus unstable for that same norm.
In this respect, it is in any case better to choose h and k such that
k
h2 ≤1
2, since
in this case, we are assured that |||A |||2,h = ρ(A ) < 1, which is numerically a good
thing. In particular, a spectral radius which is strictly larger than 1 manifests itself

8.5 Stability for One Time Step Schemes in the 2, h Norms
279
Fig. 8.3 The boundaries of a
few stability regions deﬁned
in the (h; k) plane by formula
(8.12) for different values of
C (C = 0, 0.1, 0.2, 0.3, 0.4)
and of the “safe” region
k ≤h2/2 dashed. They are
all tangent at (0, 0)
0
0,5
1
1,5
2
0,5
1
1,5
k
h
as a numerical instability, with oscillations that become very large exponentially fast
with the number of iterations as we will see in a few examples later, even though this
particular choice of values for h and k belongs to a stability region in the previous
sense.
□
We see from Fig.8.3 that there is no practical difference between the different
stability regions where it counts, that is to say in a neighborhood of (0, 0). Let us
now turn to the implicit Euler scheme (8.5).
Proposition 8.9 The implicit Euler three-point scheme is unconditionally stable for
the norms ∥· ∥2,h. It is convergent for these norms and we have the error estimate
max
j≤T/k ∥Uj −Sh(utj)∥2,h ≤C(h2 + k),
where C depends only on u and T.
Proof We have A = (I + kAh)−1, which is real symmetric, hence normal. Its eigen-
values are
λp =
1
1 + 4k
h2 sin2
pπ
2(N+1)
, p = 1, . . . , N,
and are all between 0 and 1. Hence ρ(A ) < 1 for all h and k, and since B−1
0
= kA ,
the scheme is unconditionally stable.
□
Remark 8.16 We see here the great advantage of the implicit Euler scheme over the
explicit Euler scheme. The number of time iterations needed to reach a given time
T is not constrained by the space step.
In this example, we also see why the 2, h norms are easier to work with than the
∞, h norms, for such an implicit scheme.
We plot in Fig.8.4 the solution of the implicit Euler scheme corresponding to
u0(x) = 4x(1 −x) at T = 1.1 and N = M ranging from 120 to 240 by increments
of 12. Only the discrete values are drawn, no linear interpolation.

280
8
The Finite Difference Method for the Heat Equation
Fig. 8.4 Convergence of the
implicit Euler scheme
0.0e+00
5.0e-06
1.0e-05
1.5e-05
2.0e-05
2.5e-05
3.0e-05
3.5e-05
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
In the above computations, k
h2 range from approximately 100 to 200, with no ill
effect of course.
□
We ﬁnally deal with the leapfrog scheme (8.6). The leapfrog scheme is a two time
step scheme with the general form
B1uj+1 + B0Uj + B−1Uj−1 = Fj
with
B1 = 1
2k I, B0 = Ah, B−1 = −1
2k I and Fj = Fj.
As in the general case, we rewrite it as a single time step scheme by setting
V j =
 Uj
Uj−1

∈R2N,
so that
V j+1 = A V j + Gj,
where
A =
−2kAh I
I
0

(8.13)
is the 2N × 2N symmetric ampliﬁcation matrix and Gj ∈R2N. Since |||B−1
1 |||2,h = 2k,
Proposition8.3 applies. However, instead of using the norm used in this proposition,
we use the matrix norm induced by the canonical Euclidean norm on R2N, which is
equivalent. Indeed,
1
√
2
(∥U1∥2
2,h + ∥U2∥2
2,h)1/2 ≤max(∥U1∥2,h, ∥U2∥2,h) ≤(∥U1∥2
2,h + ∥U2∥2
2,h)1/2
for all U1, U2 ∈RN. We thus just need to ﬁnd the spectral radius of the matrix A .

8.5 Stability for One Time Step Schemes in the 2, h Norms
281
Lemma 8.2 Let C be a N × N complex matrix and B the 2N × 2N complex matrix
deﬁned by blocks as
B =
C I
I 0

,
where I (resp. 0) is the N × N identity (resp. zero) matrix. If λ ∈C is an eigenvalue of
B, then λ ̸= 0 and λ −1
λ is an eigenvalue of C. Conversely, if μ ∈C is an eigenvalue
of C, then there exists an eigenvalue λ of B such that μ = λ −1
λ.
Proof Let λ ∈C be such that there exists a vector Y in C2N, Y =
Y1
Y2

, Y ̸= 0, such
that BY = λY. Using the block structure of B, we see that this is equivalent to

CY1 + Y2 = λY1,
Y1 = λY2.
If Y1 = 0, the ﬁrst equation implies that Y2 = 0, which is impossible. Thus Y1 ̸= 0,
which implies λ ̸= 0 by the second equation. We may thus divide by λ so that
Y2 = 1
λY1 and replacing in the ﬁrst equation CY1 =

λ −1
λ

Y1. Since we have already
seen that Y1 ̸= 0, this implies that λ −1
λ is an eigenvalue of C.
Conversely, let μ ∈C be an eigenvalue of C with eigenvector Y1 ∈CN, Y1 ̸= 0.
The polynomial X2 −μX −1 has two roots in C, which are nonzero since their
product is −1. Let λ be one of these roots. Dividing by λ, we see that λ −μ −1
λ = 0,
hence μ = λ −1
λ. Furthermore
B
 Y1
1
λY1

=

CY1 + 1
λY1
Y1

=

μ + 1
λ

Y1
Y1

= λ
 Y1
1
λY1

,
so that λ is an eigenvalue of B.
□
Remark 8.17 If λ is an eigenvalue of B, then −1
λ is also an eigenvalue of B. This
pair corresponds to the same eigenvalue μ of C.
□
Let us now apply this to the leapfrog scheme.
Proposition 8.10 The leapfrog scheme is unstable for the norms ∥· ∥2,h, hence not
convergent for these norms.
Proof The matrix A deﬁned by (8.13) is real symmetric, hence normal. We may
thus apply Proposition8.7 with the equivalence of matrix norms noted before.
The eigenvalues of the matrix −2kAh are
μp = −8k
h2 sin2
pπ
2(N + 1)

, p = 1, . . . , N,
and those of the matrix A

282
8
The Finite Difference Method for the Heat Equation
λ±
p =
μp ±

μ2p + 4
2
according to Lemma8.2. In particular, for p = N, we have
sin2
Nπ
2(N + 1)

= cos2
π
2(N + 1)

≥1
2
since
π
2(N+1) ≤π
4 . Therefore
−μN ≥4k
h2 .
It follows that
ρ(A ) ≥

μN −

μ2
N + 4
2
 =

μ2
N + 4 −μN
2
≥2 + 4k
h2
2
= 1 + 2k
h2 .
Consequently, there is no constant C ≥0 such that ρ(A ) ≤1 + Ck. Indeed, assume
there was such a constant, for (h, k) in some region S . Then we would have 2
h2 ≤C,
which precludes h →0. This is inconsistent with (0, 0) ∈
¯
S .
□
Remark 8.18 The leapfrog scheme is thus not usable in practice for solving the heat
equation. Numerical experiments show that it may diverge very rapidly. We remark
in addition that
ρ(A M+1) ≥

1 + 2k
h2
M+1
≥1 + 2(M + 1)k
h2
= 1 + 2T
h2 →+∞when h →0.
The same is true for any number of iterations needed to reach a ﬁxed
time T > 0.
□
Figure8.5 shows a sequence of plots corresponding to the leapfrog method applied
to u0(x) = 4x(1 −x) at T = 1.1 for N = M ranging from 3 to 18. Notice the vertical
scale on the plots. The maximum of the exact solution u at time T is of the order of
2 × 10−5.
8.6
Stability via the Discrete Fourier Transform
From now on, and for the rest of this chapter, we concentrate on stability criteria
without reference to consistency and convergence, but nonetheless keeping the Lax
theorem in mind. Let us thus present a slightly, although not fundamentally different
way of dealing with stability using the discrete Fourier transform. We consider here
periodic boundary conditions instead of Dirichlet conditions. More precisely, we
are interested in the restriction to [0, 1] × R∗
+ of functions that are 1-periodic in

8.6 Stability via the Discrete Fourier Transform
283
-15000
-10000
-5000
0
5000
10000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = M = 3
-3e+09
-2e+09
-1e+09
0e+00
1e+09
2e+09
3e+09
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = M = 6
-8e+15
-6e+15
-4e+15
-2e+15
0e+00
2e+15
4e+15
6e+15
8e+15
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = M = 9
-8e+22
-6e+22
-4e+22
-2e+22
0e+00
2e+22
4e+22
6e+22
8e+22
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = M = 12
0.0e+00
5.0e+29
1.0e+30
1.5e+30
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = M = 15
-8e+37
-6e+37
-4e+37
-2e+37
0e+00
2e+37
4e+37
6e+37
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
N = M = 18
Fig. 8.5 Divergence of the leapfrog method
space and that satisfy the heat equation on R∗
+ × R with zero right-hand side. Such
functions are regular and thus obey the following system
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∂u
∂t (x, t) −∂2u
∂x2 (x, t) = 0 in Q,
u(x, 0) = u0(x) in Ω,
u(0, t) = u(1, t) in ]0, T[,
∂u
∂x (0, t) = ∂u
∂x (1, t) in ]0, T[.
(8.14)

284
8
The Finite Difference Method for the Heat Equation
It is easy to check that if u0 satisﬁes the same boundary conditions, it is given by
its Fourier series expansion u0(x) = a0
2 + +∞
l=1 al cos(2lπx) + +∞
l=1 bl sin(2lπx).
In this case, the unique solution to (8.14) is given by
u(x, t) = a0
2 +
+∞

l=1
e−4l2π2tal cos(2lπx) +
+∞

l=1
e−4l2π2tbl sin(2lπx),
using similar arguments as in Sect.7.3. It is also possible to write a weak formulation
for this periodic problem.
Up to now, the vectors Uj were always computed in the canonical basis of RN.
It turns out that there is another basis that is better adapted to the study of ﬁnite
difference schemes, provided that we are willing to replace R by C. In this basis, the
Fourier basis, computations are straightforward.
We use the same space-time grid as before, but due to the change of bound-
ary conditions, the forward Euler scheme involves unknowns uj
n, −1 ≤n ≤N + 1,
0 ≤j ≤M, with
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
uj+1
n
−uj
n
k
−uj
n+1 −2uj
n + uj
n−1
h2
= 0 for n = 0, . . . , N, j = 0, . . . , M,
u0
n = u0(xn) for n = 0, . . . , N + 1,
uj
0 = uj
N+1 for j = 0, . . . , M,
uj
−1 = uj
N for j = 0, . . . , M.
(8.15)
The relation uj
0 = uj
N+1 is a discretization of the periodicity condition u(0, t) =
u(1, t), and the relation uj
−1 = uj
N combined with the previous one discretizes
the other periodicity condition ∂u
∂x (0, t) = ∂u
∂x (1, t). Indeed, ∂u
∂x (0, t) ≈
uj
0−uj
−1
h
and
∂u
∂x (1, t) ≈
uj
N+1−uj
N
h
.
We assume of course that u0 is periodic. The corresponding vectors Uj now
live in RN+1 with components (uj
0, uj
1, . . . , uj
N) in the canonical basis. We embed
RN+1 into CN+1 in the canonical fashion. We let (U|V ) = N
m=0 um¯vm denote the
canonical scalar product on CN+1 and introduce the (discrete) Fourier basis of CN+1
as (ωn)n=0,...,N where
(ωn)m = e2iπ nm
N+1 , for m = 0, . . . , N.
Proposition 8.11 The family (ωn)n=0,...,N is an orthogonal basis of CN+1. In fact,
we have
(ωl|ωn) = (N + 1)δln.
(8.16)
Proof It is enough to compute the scalar products (8.16). Indeed

8.6 Stability via the Discrete Fourier Transform
285
(ωl|ωn) =
N

m=0
e2iπ
lm
N+1 e−2iπ nm
N+1 =
N

m=0
e2iπ (l−n)m
N+1 =

N + 1
if l = n,
1−e2iπ(l−n)
1−e2iπ l−n
N+1 = 0 if l ̸= n.
We therefore have an orthogonal, hence linearly independent family of vectors, the
cardinal of which is equal to the dimension of the C-vector space CN+1. It is thus a
basis.
□
We now are at liberty to decompose Uj on this basis
Uj =
N

n=0
cj
nωn with cj
n =
1
N + 1(Uj|ωn) =
1
N + 1
N

m=0
uj
me2iπ nm
N+1 ,
and read the scheme on the discrete Fourier coefﬁcients cj
n.
Proposition 8.12 We have
cj+1
n
=

1 −4k
h2 sin2 πn
N + 1

cj
n,
and therefore
cj
n =

1 −4k
h2 sin2 πn
N + 1
j
c0
n,
where c0
n are the discrete Fourier coefﬁcients of the initial condition.
Proof The key observation here is that the space ﬁnite difference operator acts by
multiplication on the Fourier basis vectors. In other words, these vectors are eigen-
vectors of the ﬁnite difference operator. Indeed, for m = 0, . . . , N,
−(ωl)m−1 + 2(ωl)m −(ωl)m+1
h2
= −e−2iπ
l
N+1 + 2 −e2iπ
l
N+1
h2
e2iπ
lm
N+1
= 2
h2

1 −cos
 2πl
N + 1

(ωl)m,
with (ωl)−1 = (ωl)N and (ωl)N+1 = (ωl)0 by convention, and the corresponding
eigenvalue is
λl = 4
h2 sin2
πl
N + 1

.
The result now follows from the ﬁrst equation in system (8.15) by linearity.
□
Now it is clear that we can analyze other schemes in the case of periodic boundary
conditions, such as the backward Euler scheme or the leapfrog scheme, with the
discrete Fourier transform.
Deﬁnition 8.7 We say that a scheme is stable in the sense of von Neumann if |cj+1
n
| ≤
|cj
n| for all relevant n and j and all initial condition u0.

286
8
The Finite Difference Method for the Heat Equation
We have ∥ωn∥2,h = 1 for all n, therefore ∥Uj∥2
2,h = N
n=0 |cj
n|2. Thus, stability in
the sense of von Neumann implies stability for the 2, h norms for all T and uniformly
with respect to T.
Proposition 8.13 The forward Euler scheme is stable in the sense of von Neumann
if k
h2 ≤1
2.
Proof Assume that k
h2 ≤1
2. In view of Proposition8.12, von Neumann stability oc-
curs if and only if
1 −4k
h2 sin2 πn
N + 1
 ≤1
for all n, that is to say if
2k
h2 sin2 πn
N + 1

≤1
for all n. This is obviously the case under our hypothesis.
□
Similarly, we ﬁnd that the backward Euler scheme is unconditionally von Neu-
mann stable.
8.7
Stability via Fourier Series
Stability for the 2, h norms is closely related to the spectral radius of the ampliﬁcation
matrix, at least when the latter is normal. Unfortunately, it is not always easy to com-
pute the eigenvalues of a given matrix. We now present an alternate way using Fourier
series, which is not directly applicable to the previously introduced schemes—in fact
it applies to a slightly different context—but that still gives stability information in
a much more workable fashion.
We thus now work with the heat equation on R, therefore without boundary
conditions. For deﬁniteness, let us consider the forward Euler scheme
⎧
⎪⎨
⎪⎩
uj+1
n
−uj
n
k
−uj
n+1 −2uj
n + uj
n−1
h2
= 0, n ∈Z,
u0
n = u0(nh), n ∈Z.
(8.17)
As in the previously considered cases, the discrete unknowns uj
n are intended to be
approximations of the exact values of the solution u(xn, tj), n ∈Z, j = 1, . . . , M. We
also write u0 = Sh(u0) to denote the sampling operator on all grid points indexed by
Z.
If we assume that (u0
n)n∈Z ∈ℓ2(Z), i.e., that 
n∈Z |u0
n|2 < +∞, then it is quite
clear that (uj
n)n∈Z is well deﬁned and belongs to ℓ2(Z) for all j. We equip ℓ2(Z) with
the norm

8.7 Stability via Fourier Series
287
∥(vn)n∈Z∥2,h =
√
h

n∈Z
|vn|21/2
for which it is a Hilbert space, using the same notation as in the bounded interval case.
Of course, the forward Euler scheme is also deﬁned on other spaces of Z-indexed
sequences, but we concentrate here on ℓ2. It should be noted that such schemes are
not implementable in practice, since they involve an inﬁnite number of unknowns.
Their interest is purely theoretical.
We introduce the operator T : ℓ2(Z) →ℓ2(Z) deﬁned by
(T v)n = λvn+1 + (1 −2λ)vn + λvn−1,
with λ = k
h2 . Then the scheme reads
uj+1 = T uj,
u0 = Sh(u0),
(8.18)
or
uj = T ju0,
for j = 0, . . . , M.10 We introduce a concept of stability adapted to the present context.
Deﬁnition 8.8 We say that the scheme (8.17) is stable in ℓ2(Z) if there exists a
constant C(T) such that
max
j≤T/k ∥uj∥2,h ≤C(T)∥u0∥2,h,
for all u0 ∈ℓ2(Z).
Now
for
all
v ∈ℓ2(Z; C),
we
deﬁne
Fv ∈L2(0, 2π; C)
by
Fv(s) =

n∈Z vneins. It is well-known that the series converges in L2(0, 2π; C) and that
operator F is an isometry between ℓ2(Z; C) and L2(0, 2π; C), when we equip the
latter with the norm
∥f ∥L2(0,2π;C),h =

h
2π
 2π
0
|f (s)|2 ds
1/2
,
due to Parseval’s formula, see [68]. The coefﬁcients vk are just the Fourier coefﬁcients
of the 2π-periodic L2-function Fv. Conversely, any 2π-periodic L2-function gives
rise to an element of ℓ2(Z; C) by considering its Fourier coefﬁcients. For brevity,
from now on we omit the reference to C in the notation. The next proposition follows
directly from Parseval’s formula.
10Again, beware of the notation: uj is the jth element in the sequence, whereas T j is the jth iterate
of the operator T .

288
8
The Finite Difference Method for the Heat Equation
Proposition 8.14 The scheme (8.17) is stable in ℓ2(Z) if and only if there exists a
constant C(T) such that
max
j≤T/k ∥Fuj∥L2(0,2π),h ≤C(T)∥Fu0∥L2(0,2π),h,
for all u0 ∈ℓ2(Z).
We let G = F ◦T ◦F −1. Since uj = T ju0, it follows that Fuj = G j(Fu0).
Therefore we have the following proposition.
Proposition 8.15 The scheme (8.17) is stable in ℓ2(Z) if there exists a constant C(T)
such that
max
j≤T/k ∥G j∥L (L2(0,2π)) ≤C(T).
Before continuing further, let us discuss the relationship between the initial data
of the continuous problem, the function u0, and the initial data of the discrete scheme,
the sequence u0, in the L2/ℓ2 framework. We need to associate a function in L2(R)
with each sequence of numbers belonging to ℓ2(Z).
Proposition 8.16 For all v ∈ℓ2(Z), we deﬁne a piecewise constant interpolation
Ihv by
∀i ∈Z, ∀x ∈

xn −h
2, xn + h
2
 
,
Ihv(x) = vn.
The interpolation operator Ih is an isometry between ℓ2(Z) equipped with the ∥· ∥2,h
norm and L2(R) equipped with its usual norm.
Proof Indeed,
∥Ihv∥2
L2(R) =

R
Ihv(x)2 dx =

n∈Z
 xn+ h
2
xn−h
2
Ihv(x)2 dx =

n∈Z
hv2
n = ∥v∥2
2,h,
and the proof is complete.
□
We can now see in which sense the stability deﬁnition (Deﬁnition8.8) relates to the
continuous problem, in the sense that Fu0 ∈L2(0, 2π) contains enough information
about the function u0 deﬁned on R, under some mild regularity assumption.
Proposition 8.17 Assume that u0 ∈H1(R), then we have
∥u0 −Ihu0∥L2(R) ≤h
2∥u′
0∥L2(R).
Proof Since u0 ∈H1(R), by formula (3.12), we have for all x ∈R and all n ∈Z
u0(x) −u0(nh) =
 x
nh
u′
0(z) dz.

8.7 Stability via Fourier Series
289
Therefore
 nh+ h
2
nh−h
2
|u0(x) −u0(nh)|2 dx =
 nh+ h
2
nh−h
2

 x
nh
u′
0(z) dz

2
dx
≤
 nh+ h
2
nh−h
2

|x −nh|
 x
nh
|u′
0(z)|2 dz

dx
≤
 nh+ h
2
nh−h
2
|x −nh| dx
  nh+ h
2
nh−h
2
|u′
0(z)|2 dz
= h2
4
 nh+ h
2
nh−h
2
|u′
0(z)|2 dz.
Summing over n ∈Z, we obtain the Proposition.
□
Under the above hypothesis, we thus have
∥Fu0∥L2(0,2π),h = ∥Shu0∥2,h = ∥Ihu0∥L2(R) ≤
√
2∥u0∥H1(R)
for h ≤2. Therefore, if the scheme is stable in ℓ2(Z) in the sense of Deﬁnition8.8,
it follows that
max
j≤T/k ∥uj∥2,h ≤
√
2C(T)∥u0∥H1(R).
The feature of the Fourier series transform that makes it so useful here, in addition
to being an isometry, is that it transforms translations of indices into multiplications
by exponentials. More precisely, if v ∈ℓ2(Z) and m ∈Z, letting (τmv)n = vn+m, then
F(τmv)(s) =

n∈Z
vn+meins =

n∈Z
vnei(n−m)s = e−imsFv(s).
Proposition 8.18 Let a(s) = 1 −4λ sin2 s
2

. Then we have
∥G j∥L (L2(0,2π)) = max
s∈[0,2π] |a(s)|j.
Proof We apply the Fourier series transform to the scheme in the form (8.18). This
yields
F(uj+1)(s) =

1 + λ(eis −2 + e−is)

F(uj)(s) = a(s)F(uj)(s).
Iterating this relation, we obtain for all g ∈L2(0, 2π)11
(G jg)(s) = a(s)jg(s).
11Again, beware of the notation: here G j is the jth iterate of G and aj is the function a to the power j.

290
8
The Finite Difference Method for the Heat Equation
Let now M be a multiplier operator, i.e., an operator on L2(0, 2π) of the form
(Mg)(s) = m(s)g(s),
with m ∈L∞(0, 2π), which is the case of G j above. Let us show that
∥M∥L (L2(0,2π)) = ∥m∥L∞(0,2π).
First of all, for all g ∈L2(0, 2π), we have
∥Mg∥2
L2(0,2π),h = h
2π
 2π
0
|m(s)|2||g(s)|2 ds ≤∥m∥2
L∞(0,2π)∥g∥2
L2(0,2π),h,
so that
∥M∥L (L2(0,2π)) ≤∥m∥L∞(0,2π).
Next, let ∥m∥L∞(0,2π) ≥ε > 0 and A ⊂[0, 2π] be a set of strictly positive measure
such that |m(t)| ≥∥m∥L∞(0,2π) −ε ≥0 on A, assuming m ̸= 0 since the case m = 0
is not difﬁcult. We take g = ( h meas A
2π
)−1/21A ∈L2(0, 2π). Then ∥g∥L2(0,2π),h = 1 and
∥Mg∥2
L2(0,2π),h = h
2π
 2π
0
|m(s)|2|g(s)|2 ds =
1
meas A

A
|m(s)|2 ds
≥(∥m∥L∞(0,2π) −ε)2
meas A

A
ds = (∥m∥L∞(0,2π) −ε)2.
Therefore
∥M∥L (L2(0,2π)) ≥∥m∥L∞(0,2π) −ε
for all ε > 0, and the proposition is proved, since in our particular case, the function
m = aj is continuous on [0, 2π] and its L∞norm is just the maximum of its absolute
value on [0, 2π].
□
Proposition 8.19 TheforwardEulerschemeisstableinℓ2(Z)if k
h2 ≤1
2 andunstable
if k
h2 ≥λ0 > 1
2.
Proof We have a(s) = 1 −4k
h2 sin2 s
2

≤1 for all s ∈[0, 2π] and a(0) = 1. On the
other hand, the minimum of a(s) is attained for s
2 = π
2 and its minimum value is
1 −4k
h2 . Therefore
max
s∈[0,2π] |a(s)| = max

1,
1 −4k
h2


.
Consequently, if k
h2 ≤1
2, then maxs∈[0,2π] |a(s)| = 1 so that ∥G j∥L (L2(0,2π)) = 1 and
the scheme is stable in ℓ2(Z).
If, on the other hand, k
h2 ≥λ0 > 1
2, then

8.7 Stability via Fourier Series
291
max
s∈[0,2π] |a(s)|j ≥(4λ0 −1)j,
so that
max
j≤T/k max
s∈[0,2π] |a(s)|j ≥(4λ0 −1)T/k →+∞when k →0,
hence the scheme is unstable.
□
We can apply the same philosophy to a general single time step ﬁnite differ-
ence scheme and obtain corresponding schemes on ℓ2(Z) which are of the form
F(uj+1)(s) = a(s)F(uj)(s), F(u0) given, in Fourier space. The function a, which
depends onh andk as parameters, is calledtheampliﬁcationcoefﬁcient of thescheme.
Using the same arguments as those used with matrices, it is easy to prove that a
scheme is stable in ℓ2 if and only if there exists a positive constant C that depends
only on T such that |a(s)| ≤1 + Ck for all s. We now introduce the analogue of the
previous notion of von Neumann stability (Deﬁnition8.7) for schemes on ℓ2(Z).
Deﬁnition 8.9 We say that a scheme on ℓ2(Z) is stable in the sense of von Neumann
if maxs∈[0,2π] |a(s)| ≤1.
Clearly, stability in the sense of von Neumann implies stability in ℓ2(Z) for all T
anduniformlywithrespect to T. It is thus asufﬁcient conditionof stability. Obviously,
computations in Fourier space are much easier than evaluations of spectral radii.
We now consider the example of a family of schemes, collectively known as the
θ-scheme. Let us be given a number θ ∈[0, 1]. The θ-scheme is as follows:
uj+1
n
−uj
n
k
−θ
uj+1
n+1 −2uj+1
n
+ uj+1
n−1
h2
−(1 −θ)
uj
n+1 −2uj
n + uj
n−1
h2
= θf j+1
n
+ (1 −θ)f j
n,
(8.19)
with initial conditions. The θ-scheme is thus a weighted average of the explicit Euler
scheme (θ = 0) and the implicit Euler scheme (θ = 1). It is implicit as soon as θ > 0.
Before we can even talk about stability, it is not clear that such an implicit scheme is
actually well-deﬁned on ℓ2(Z). The Fourier transform is also the key here. Indeed,
in Fourier space, we have (with f = 0)
F(uj+1)(s) −F(uj)(s)
k
−θ eis −2 + e−is
h2
F(uj+1)(s)
−(1 −θ)eis −2 + e−is
h2
F(uj)(s) = 0,
which boils down to

1 + θ 4k
h2 sin2 s
2

F(uj+1)(s) =

1 −(1 −θ)4k
h2 sin2 s
2

F(uj)(s).

292
8
The Finite Difference Method for the Heat Equation
Now we see that 1 + θ 4k
h2 sin2 s
2

≥1, hence, its inverse is in L∞and the scheme in
Fourier space can be rewritten as a multiplier operator with ampliﬁcation coefﬁcient
a(s) = 1 −(1 −θ) 4k
h2 sin2 s
2

1 + θ 4k
h2 sin2 s
2

∈L∞(0, 2π).
The fact that the Fourier transform is an isomorphism implies that the discrete scheme
is well-deﬁned.
Now in terms of stability, clearly, a(s) ≤1 for all s ∈[0, 2π]. Stability in the
sense of von Neumann thus depends on whether or not we have a(s) ≥−1 for all
s ∈[0, 2π].
Proposition 8.20 If θ ≥1
2, then the θ-scheme is unconditionally stable in the sense
of von Neumann. If θ < 1
2, it is stable in the sense of von Neumann under the condition
k
h2 ≤
1
2(1−2θ).
Proof After a little bit of computation, it can be checked that a(s) ≥−1 if and only
if 1 + (2θ −1) 2k
h2 sin2 s
2

≥0, hence the result.
□
The case θ = 1
2 is special and is called the Crank–Nicolson scheme. We will go
back to this scheme in detail in Sect.8.9.
8.8
Stability via the Continuous Fourier Transform
We now present yet another approach to stability, using this time the continuous
Fourier transform. Again, this approach is not directly applicable to discrete schemes,
but it makes computations much easier. The ensuing stability analysis turns out to
be very similar to the one done via Fourier series, and we actually basically use the
same notation.
We thus consider again the heat equation on R with zero right-hand side and start
with the forward Euler scheme (8.17) in the ℓ2(Z) context.
Instead of working directly with the above discrete scheme, we introduce a semi-
discrete version of it. In a semi-discrete scheme, only time is fully discretized. Space
is only semi-discretized in the sense that it remains continuous even though we retain
the space step h. We thus consider sequences of functions uj : R →R which are such
that uj is supposed to be an approximation of the function x →u(x, tj).
The semi-discrete version of the forward Euler scheme is as follows:
⎧
⎨
⎩
uj+1(x) −uj(x)
k
−uj(x + h) −2uj(x) + uj(x −h)
h2
= 0,
u0(x) = u0,h(x),
(8.20)
where u0,h is some approximation of u0. So the idea is to use the differential quotient
on which the discrete scheme is based to approximate the space derivative, and the

8.8 Stability via the Continuous Fourier Transform
293
usual discrete difference quotient for the time derivative. This way, any discrete
scheme admits a semi-discrete version.
A good functional setting for this is for example L2(R). Indeed, if u0,h ∈Lp(R),
then clearly, uj is well deﬁned and belongs to Lp(R). In effect, if u0,h ∈L2(R), then we
canwriteuj+1 = G(uj)whereGisthecontinuouslinearoperatorinL (L2(R), L2(R))
deﬁned by
Gv(x) = v(x) + k
h2 (v(x + h) −2v(x) + v(x −h)),
(8.21)
or equivalently
G =

1 −2k
h2

I + k
h2

τh + τ−h

,
where τs denotes the operator of translation by s, τsu(x) = u(x + s). Therefore,
uj = Gj(u0,h) and the properties of the scheme are the properties of the iterates
of the operator G, provided u0,h remains bounded.
Let us discuss the relationship between the fully discrete and semi-discrete points
of view. It turns out that the discrete scheme and the semi-discrete scheme are equiv-
alent when the initial data of the semi-discrete scheme is in the range of the interpo-
lation operator Ih of Proposition8.16.
Proposition 8.21 Let us be given (u0
n)n∈Z ∈ℓ2(Z). If u0,h = Ih

(u0
n)n∈Z

, then
uj = Ih

(uj
n)n∈Z

for all j ∈N.
Proof We prove this by induction on j. The statement is true for j = 0 by hypothesis.
Letusthusassumethatuj = Ih

(uj
n)n∈Z

.Thismeansthatforallx ∈
!
xn −h
2, xn + h
2
"
,
we have uj(x) = uj
n. Therefore, in view of (8.21), for the same values of x, we have
Guj(x) = uj(x) + k
h2 (uj(x + h) −2uj(x) + uj(x −h))
= uj
n + k
h2 (uj
n+1 −2uj
n + uj
n−1) = uj+1
n
,
so that uj+1 = Guj = Ih

(uj+1
n
)n∈Z

.
□
So the idea is that, if we start the semi-discrete scheme with an initial data
constructed by piecewise interpolation from the discrete scheme, the semi-discrete
scheme will construct exactly the same values as the discrete scheme. The advantage
is that the semi-discrete scheme works for much more general initial data, which in
turn makes the study of stability considerably easier.

294
8
The Finite Difference Method for the Heat Equation
Deﬁnition 8.10 We say that the semi-discrete scheme is stable in L2(R) if there
exists a constant C(T) such that
max
j≤T/k ∥uj∥L2(R) ≤C(T)∥u0,h∥L2(R),
for all u0,h ∈L2(R).
Here u0,h is no longer to be thought of as some approximation of u0. Clearly, this is
equivalent to ∥Gj∥L (L2(R),L2(R)) being bounded independently of j, h and k.12 In view
of Propositions8.16 and 8.21, stability of the semi-discrete scheme implies stability
of the discrete scheme in the ∥· ∥2,h norms, hence the interest of the approach.
The reason for singling out L2 among all Lp spaces is that the continuous Fourier
transform is an isometry on L2. Let us brieﬂy state a few facts about the continuous
Fourier transform. When u ∈L1(R), the Fourier transform of u is deﬁned by
#u(ξ) = Fu(ξ) =
1
√
2π
 +∞
−∞
e−ixξu(x) dx.
The function #u is continuous and tends to 0 at inﬁnity. When u ∈L1(R) ∩L2(R),
it can be shown that #u also belongs to L2(R) and that ∥#u∥L2(R) = ∥u∥L2(R), which
is called the Plancherel formula, see [68]. Thus the Fourier transform extends as an
isometry to the whole of L2 by density of L1(R) ∩L2(R) in L2(R) (but not by the
simple Lebesgue integral formula above, which makes no sense in the L2 context).
In addition to being an isometry, the continuous Fourier transform transforms
translations into multiplications by exponentials. More precisely, if u ∈L1(R) and
s ∈R, then
$
τsu(ξ) =
1
√
2π
 +∞
−∞
e−ixξu(x + s) dx =
1
√
2π
 +∞
−∞
e−i(y−s)ξu(y) dy = eisξ#u(ξ),
and the equality $
τsu(ξ) = eisξ#u(ξ) remains true for any u ∈L2(R) by density.
Proposition 8.22 Let a(ξ) = 1 −4k
h2 sin2 hξ
2

. Then we have
∥Gj∥L (L2(R),L2(R)) = sup
ξ∈R
|a(ξ)|j.
Proof We apply the Fourier transform to the semi-discrete scheme (8.20). This yields

uj+1(ξ) −#uj(ξ)
k
−eihξ −2 + e−ihξ
h2
#uj(ξ) = 0,
or

uj+1(ξ) = #uj(ξ) + 2k
h2 (cos(hξ) −1)#uj(ξ),
12Here again, G depends on h and k even though the notation does not make it plain.

8.8 Stability via the Continuous Fourier Transform
295
or again

uj+1(ξ) = a(ξ)#uj(ξ).
Iterating this relation, we obtain13
F(Gju0)(ξ) = #uj(ξ) = a(ξ)j #
u0(ξ).
The conclusion follows as in the proof of Proposition8.18.
□
Proposition 8.23 The forward Euler semi-discrete scheme is stable in L2(R) if
k
h2 ≤1
2 and unstable if k
h2 ≥λ0 > 1
2.
Proof We have a(ξ) = 1 −4k
h2 sin2 hξ
2

for ξ ∈R. As in the proof of Proposi-
tion8.19, we obtain
sup
ξ∈R
|a(ξ)| = max

1,
1 −4k
h2


,
and the conclusion follows along the same lines.
□
Corollary 8.5 The forward Euler discrete scheme is stable in the ∥· ∥2,h norms if
k
h2 ≤1
2.
Any single time step ﬁnite difference scheme has a semi-discrete version, which
is of the form
uj+1(ξ) = a(ξ)#uj(ξ), #
u0 given, in Fourier space. The function a, which
depends on h and k as parameters, is again called the ampliﬁcation coefﬁcient of the
scheme.
Of course, a scheme is stable in L2 if and only if there exists a positive constant C
that depends only on T such that |a(ξ)| ≤1 + Ck for all ξ. There is also a concept
of von Neumann stability for semi-discrete schemes.
Deﬁnition 8.11 We say that a semi-discrete scheme is stable in the sense of von
Neumann if supξ∈R |a(ξ)| ≤1.
Clearly, stability in the sense of von Neumann implies stability in L2(R) for all T
and uniformly with respect to T. It is thus a sufﬁcient condition of stability for both
semi-discrete and discrete schemes.
We now consider the θ-scheme. The semi-discrete version of the θ-scheme (with
0 right-hand side) is
uj+1(x) −uj(x)
k
−θ uj+1(x + h) −2uj+1(x) + uj+1(x −h)
h2
−(1 −θ)uj(x + h) −2uj(x) + uj(x −h)
h2
= 0.
13Again, beware of the notation: uj is the jth function in the sequence, whereas Gj is the jth iterate
of the operator G and aj is the function a to the power j.

296
8
The Finite Difference Method for the Heat Equation
Again the scheme is implicit for θ > 0. We must show that it is well-deﬁned on
L2(R). We use of course the Fourier transform again. Indeed, in Fourier space, we
have

uj+1(ξ) −#uj(ξ)
k
−θ eihξ −2 + e−ihξ
h2

uj+1(ξ)
−(1 −θ)eihξ −2 + e−ihξ
h2
#uj(ξ) = 0,
and the exact same computation as in the Fourier series case yields an ampliﬁcation
coefﬁcient
a(ξ) = 1 −(1 −θ) 4k
h2 sin2 hξ
2

1 + θ 4k
h2 sin2 hξ
2

∈L∞(R).
Proposition 8.24 If θ ≥1
2, then the semi-discrete θ-scheme is unconditionally sta-
ble in the sense of von Neumann. If θ < 1
2, it is stable in the sense of von Neumann
under the condition k
h2 ≤
1
2(1−2θ).
Proof See the proof of Proposition8.20.
□
8.9
The Crank–Nicolson Scheme, Stability via the Energy
Method
Let us ﬁrst talk about consistency and order of the θ-scheme for problem (8.1),
deﬁned by (8.19), of which the Crank–Nicolson scheme is a special case.
Proposition 8.25 The θ-scheme is of order 1 in time and 2 in space for θ ̸= 1
2, and
of order 2 in time and 2 in space for θ = 1
2, for the ∞, h norms.
Proof Let u be a sufﬁciently regular solution of problem (8.1). Let us list the results of
the application of Taylor–Lagrange expansions to the various terms, without writing
the remainders explicitly since we know that they are uniformly bounded in terms
of the relevant parameters. For the time derivative, we have
u(xn, tj+1) −u(xn, tj)
k
= ∂u
∂t (xn, tj) + k
2
∂2u
∂t2 (xn, tj) + O(k2)
= ∂u
∂t (xn, tj+1) −k
2
∂2u
∂t2 (xn, tj+1) + O(k2)
For the space derivatives, we obtain
u(xn+1, tj) −2u(xn, tj) + u(xn−1, tj)
h2
= ∂2u
∂x2 (xn, tj) + O(h2)

8.9 The Crank–Nicolson Scheme, Stability via the Energy Method
297
and
u(xn+1, tj+1) −2u(xn, tj+1) + u(xn−1, tj+1)
h2
= ∂2u
∂x2 (xn, tj+1) + O(h2).
Therefore, combining these relations together, we see that
εh,k(u)j
n = θ
∂u
∂t (xn, tj+1) −∂2u
∂x2 (xn, tj+1)

+(1 −θ)
∂u
∂t (xn, tj) −∂2u
∂x2 (xn, tj)

−θ k
2
∂2u
∂t2 (xn, tj+1) + (1 −θ)k
2
∂2u
∂t2 (xn, tj) + O(k2) + O(h2)
−θf j+1
n
−(1 −θ)f j
n.
Now we can write
∂2u
∂t2 (xn, tj+1) = ∂2u
∂t2 (xn, tj) + O(k).
Canceling all cancelable terms, we thus obtain
εh,k(u)j
n = k
1
2 −θ
∂2u
∂t2 (xn, tj) + O(k2) + O(h2),
and the result follows.
□
Remark 8.19 The θ-scheme for θ = 1
2, or Crank–Nicolson scheme, thus appears to
be particularly attractive: it is unconditionally (von Neumann) stable and of order 2
in time and space on R. Of course, we still have to prove the unconditional stability
of its discrete version on a bounded interval. We rewrite it here in full
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
uj+1
n
−uj
n
k
−
1
2h2

uj+1
n+1 −2uj+1
n
+ uj+1
n−1 + uj
n+1 −2uj
n + uj
n−1

= 1
2

f j+1
n
+ f j
n

u0 = u0,
uj
0 = uj
N+1 = 0.
(8.22)
The scheme is implicit, with the same computational cost as the backward Euler
scheme, since evaluating Uj+1 in terms of Uj entails solving a tridiagonal linear
system with a very similar, invertible matrix.
□
In order to prove the stability of the discrete Crank–Nicolson scheme, we use
an argument that is the discrete analogue of the energy estimate for the continuous
equation, hence the name stability via the energy method. We ﬁrst need a lemma that
is a discrete version of the integration by parts formula.
Let us introduce the forward and backward difference operators
(Df v)n = vn+1 −vn
h
,
(Dbv)n = vn −vn−1
h
,

298
8
The Finite Difference Method for the Heat Equation
which are a priori deﬁned for real-valued, Z-indexed sequences (vn)n∈Z. We clearly
have
Df ◦Db = Db ◦Df = D2,
where D2 is the second order centered difference operator
(D2v)n = vn+1 −2vn + vn−1
h2
that was used to approximate the second order space derivative.
Of course, all these difference operators can be applied to ﬁnite sequences, by
completing them with zeros to the right and to the left, which is appropriate when
dealing with homogeneous Dirichlet boundary conditions. We will be doing this
implicitly in all that follows.
Let us now establish the summation by parts formula, which is an interesting
result by itself with many other applications.
Lemma 8.3 Let v = (vn)n=1,...,N+1 and w = (wn)n=0,...,N+1. We have
N

n=1
(Df v)nwn = −
N+1

n=1
vn(Dbw)n + 1
h(vN+1wN+1 −v1w0).
(8.23)
In particulier, if w0 = wN+1 = 0, then
N

n=1
(Df v)nwn = −
N+1

n=1
vn(Dbw)n.
(8.24)
Proof Let us expand the left-hand side of Eq.(8.23). We obtain
N

n=1
(Df v)nwn = 1
h
N

n=1
(vn+1 −vn)wn
= 1
h
 N

n=1
vn+1wn −
N

n=1
vnwn

= 1
h
N+1

n=2
vnwn−1 −
N

n=1
vnwn

= 1
h
N+1

n=1
vn(wn−1 −wn) −v1w0 + vN+1wN+1

= −
N+1

n=1
vn(Dbw)n + 1
h(vN+1wN+1 −v1w0),
and the result follows.
□

8.9 The Crank–Nicolson Scheme, Stability via the Energy Method
299
Remark 8.20 There are other formulations of the summation by parts, for instance
N

n=1
(Df v)nwn = −
N

n=1
vn(Dbw)n + 1
h(vN+1wN −v1w0).
which is established in the same way.
□
We also need a discrete version of the Poincaré inequality:
Lemma 8.4 Let w = (wn)n=0,...,N be such that w0 = 0. Then we have
N

n=1
w2
n ≤
N

n=1
(Dbw)2
n.
(8.25)
Proof For all n = 1, . . . , N, we can write
wn = wn −wn−1 + wn−1 −wn−2 + · · · + w1 −w0
= h
n

j=1
(Dbw)j.
Therefore, by the Cauchy–Schwarz inequality, it follows that
w2
n ≤h2n
n

j=1
(Dbw)2
j ≤h2N
N

j=1
(Dbw)2
j ≤h
N

j=1
(Dbw)2
j
since h =
1
N+1. Summing the above inequality from n = 1 to N, we obtain the
Lemma.
□
We now are in a position to mimic the continuous energy estimate at the discrete
level.
Proposition 8.26 The Crank–Nicolson scheme (8.22) is unconditionally stable for
the 2, h norms.
Proof We let W j = Uj + Uj+1, Fj = Fj+Fj+1
2
and rewrite the scheme as
Uj+1 −Uj
k
−1
2(Df ◦Db)W j = Fj.
Note that this is exactly the same form as the one for which we have already computed
the orders in time and space in the ∞, h norms, hence in the 2, h norms. We multiply
row n of the above relation by hW i
j and sum with respect to n, or equivalently, take
the 2, h scalar product in RN with W j, and obtain

300
8
The Finite Difference Method for the Heat Equation
1
k

∥Uj+1∥2
2,h −∥Uj∥2
2,h

−1
2

(Df ◦Db)W jW j
2,h =
FjW j
2,h.
Let us estimate the various terms. By the Cauchy–Schwarz inequality and the discrete
Poincaré inequality (8.25), we have
FjW j
2,h ≤∥Fj∥2,h∥W j∥2,h ≤∥Fj∥2,h∥DbW j∥2,h
≤1
2∥Fj∥2
2,h + 1
2∥DbW j∥2
2,h,
indeed, (W j)0 = uj
0 + uj+1
0
= 0 due to the Dirichlet boundary condition.
For the same reason, we also have (W j)N+1 = 0, so that the summation by parts
formula (8.24) implies that
−1
2

(Df ◦Db)W jW j
2,h = 1
2

DbW jDbW j
2,h = 1
2∥DbW j∥2
2,h.
Putting these equalities and estimate together, we obtain
1
k

∥Uj+1∥2
2,h −∥Uj∥2
2,h

≤1
2∥Fj∥2
2,h,
so that
∥Uj+1∥2
2,h ≤∥Uj∥2
2,h + k
2∥Fj∥2
2,h.
Therefore, summing these inequalities from 0 to j −1 ≤M, we obtain
∥Uj∥2
2,h ≤∥U0∥2
2,h + jk
2 max
m≤T/k ∥Fm∥2
2,h
≤∥U0∥2
2,h + (M + 1)k
2
max
m≤T/k ∥Fm∥2
2,h = ∥U0∥2
2,h + T
2 max
m≤T/k ∥Fm∥2
2,h.
This is exactly the stability of the scheme in the 2, h norms, once we apply the
inequality
√
a2 + b2 ≤a + b to the right-hand side.
□
Remark 8.21 Note that when f = 0, the discrete energy 1
2∥Uj∥2
2,h is (in general
strictly) decreasing with j, as it is in the continuous case with respect to continuous
time.
□
We can now put everything together.
Proposition 8.27 The Crank–Nicolson scheme (8.22) is unconditionally convergent
for the 2, h norms, of order 2 in time and space.
For purposes of comparison, we plot in Fig.8.6 the results of the backward Euler
scheme, the Crank–Nicolson scheme with the same discretization parameters h and
k ﬁxed throughout, and the exact solution, for various values of j on the same graphs.

8.9 The Crank–Nicolson Scheme, Stability via the Energy Method
301
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 0, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 1, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 2, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
-0.05
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 3, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
0.00
0.05
0.10
0.15
0.20
0.25
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 4, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 5, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 6, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 7, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 10, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
0.000
0.001
0.002
0.003
0.004
0.005
0.006
0.007
0.008
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 20, h = 0.0476190, k = 0.0238095, k/h^2 = 10.5
Fig. 8.6 Comparison between implicit Euler, (+) Crank–Nicolson (◦), and exact solutions (solid
line)

302
8
The Finite Difference Method for the Heat Equation
The initial condition is u0(x) = sin(πx)/2 + sin(2πx) and the right-hand side f is
zero. In this case, the exact solution is
u(x, t) = 1
2 sin(πx)e−π2t + sin(2πx)e−4π2t,
which makes comparisons possible.
The backward Euler scheme solution is drawn with + marks and linearly inter-
polated, that of the Crank–Nicolson scheme with ◦marks also linearly interpolated,
and the exact solution with a solid line. The vertical scale varies from plot to plot.
Both schemes are stable and the higher order, hence better accuracy, of the Crank–
Nicolson scheme is clearly visible for this particular initial data.
8.10
Other Approximations of the Heat Equation
The ﬁnite difference method seems quite satisfactory in the case of one space dimen-
sion, see also [76] for the ﬁnite difference method for general parabolic equations.
Higher dimensional versions exist, see [75] for example. They however suffer from
the same drawbacks as the ﬁnite difference method for elliptic problems in more
than two space dimensions. Chieﬂy, it is difﬁcult if not downright impossible to
accommodate complex domain geometries in space Ω ⊂Rd.
One way of going around this difﬁculty is to devise methods that combine a ﬁnite
difference approximation in time, since there is an ordinary differential equation
aspect with respect to time, with a ﬁnite element (or other) method in space, since
there is a PDE boundary value aspect with respect to space. Let us quickly introduce
ﬁnite difference-ﬁnite element schemes.
We start from the variational formulation (7.3). Given u0 ∈L2(Ω) and f ∈L2(Q),
the solution u is such that u ∈C0([0, T]; L2(Ω)) ∩L2(0, T; H1
0(Ω)) and for all
v ∈H1
0(Ω),
 
(u|v)L2(Ω)
′ + a(u, v) = (f |v)L2(Ω) in the sense of D′(]0, T[),
(u(0)|v)L2(Ω) = (u0|v)L2(Ω).
The discretization proceeds in two consecutive steps. First comes the space dis-
cretization, exactly in the same spirit as for the abstract variational approximation
methods for elliptic problems. Next, time discretization is performed. Let us start
with the space discretization.
We thus assume that we are given a ﬁnite element subspace Vh of V = H1
0(Ω).
We then let uh ∈C0([0, T]; Vh) be the solution of
 
(uh|vh)L2(Ω)
′ + a(uh, vh) = (f |vh)L2(Ω) in the sense of D′(]0, T[),
(uh(0)|vh)L2(Ω) = (u0,h|vh)L2(Ω),

8.10 Other Approximations of the Heat Equation
303
for all vh ∈Vh, where u0,h ∈Vh is some approximation of u0, for instance its L2-
orthogonal projection on Vh. This is a Cauchy problem for a system of linear ordinary
differential equations, since Vh is ﬁnite dimensional, hence existence and uniqueness
are not a real issue.
More precisely, assume that dim Vh = N and let us be given a basis (wj)j=1,...,N of
Vh consisting of hat functions that are constructed from the shape functions associated
with the underlying ﬁnite element. We can thus write
uh(t) =
N

j=1
uh,j(t)wj
and uh is determined by the N unknown real-valued functions uh,j ∈C0([0, T]).
First of all, the initial condition is obviously equivalent to uh(0) = u0,h, that is in
components uh,j(0) = u0,h,j, for all j.
Next, taking vh = wi for n = 1, . . . , N, we obtain
N

j=1
u′
h,j(t)(wj|wi)L2(Ω) +
N

j=1
uh,j(t)a(wj, wi) = (f |wi)L2(Ω)
for n = 1, . . . , N. Let us rewrite this in vector form by introducing the vectors
Uh(t) =
⎛
⎜⎜⎜⎝
uh,1(t)
uh,2(t)
...
uh,N(t)
⎞
⎟⎟⎟⎠, U0,h =
⎛
⎜⎜⎜⎝
u0,h,1
u0,h,2
...
u0,h,N
⎞
⎟⎟⎟⎠and Fh(t) =
⎛
⎜⎜⎜⎝
(f |w1)L2(Ω)(t)
(f |w2)L2(Ω)(t)
...
(f |wN)L2(Ω)(t)
⎞
⎟⎟⎟⎠,
and the N × N matrices
M =
⎛
⎜⎜⎜⎜⎜⎝
(w1|w1)L2(Ω)
(w2|w1)L2(Ω)
· · ·
(wN|w1)L2(Ω)
(w1|w2)L2(Ω)
(w2|w2)L2(Ω)
· · ·
(wN|w2)L2(Ω)
...
...
...
...
(w1|wN−1)L2(Ω)
· · ·
(wN−1|wN−1)L2(Ω) (wN|wN−1)L2(Ω)
(w1|wN)L2(Ω)
· · ·
(wN−1|wN)L2(Ω)
(wN|wN)L2(Ω)
⎞
⎟⎟⎟⎟⎟⎠
and
A =
⎛
⎜⎜⎜⎜⎜⎝
a(w1|w1)
a(w2|w1)
· · ·
a(wN|w1)
a(w1|w2)
a(w2|w2)
· · ·
a(wN|w2)
...
...
...
...
a(w1|wN−1)
· · ·
a(wN−1|wN−1) a(wN|wN−1)
a(w1|wN)
· · ·
a(wN−1|wN)
a(wN|wN)
⎞
⎟⎟⎟⎟⎟⎠
,

304
8
The Finite Difference Method for the Heat Equation
or in other words, Mnj = (wj|wi)L2(Ω) and Anj = a(wj, wi). Since we are dealing with
a hat-function basis, both matrices are sparse, and we can assume a good numbering
of the degrees of freedom to obtain a band matrix.
The system then becomes
MU′
h(t) + AU(t) = Fh(t),
with the initial condition
Uh(0) = U0,h.
The matrix M is called the mass matrix and the matrix A is called the stiffness matrix.
Since M is the Gram matrix of a basis, it is nonsingular. Therefore, we can write
U′
h(t) + M−1AU(t) = M−1Fh(t),
from which existence and uniqueness are obvious since this is a system of linear
ordinary differential equations. Conversely, it is clear that the solution of this system
of ordinary differential equations gives rise to a solution of the variational problem
on Vh.
So far, only space was discretized. In order to obtain a fully discrete scheme,
we also need to discretize time. Any ordinary differential equation numerical
scheme may a priori be used here: forward Euler, backward Euler, Crank–Nicolson,
4th order Runge–Kutta, linear multistep methods and so on, see [8, 16, 23, 60,
64]. Of course, when using ordinary differential equation schemes, stability crite-
ria such as Dahlquist’s zero-stability, L-stability or A-stability, become essential,
see [12, 48, 49].
For this, we choose an integer M and let k =
T
M+1 be the time step. We denote by
Uj
h an approximation of Uh(jk). The forward Euler scheme then reads
M Uj+1
h
−Uj
h
k
+ AUj
h = Fh(jk),
with the initial condition
U0
h = U0,h,
and so on for the other choices of time ﬁnite difference schemes. We have implicitly
assumed some regularity of f with respect to t. Note that, even for the forward Euler
scheme, we still need to solve a N × N linear system with the mass matrix in order
to compute Uj+1
h
from Uj
h.

8.10 Other Approximations of the Heat Equation
305
Of course, the convergence of these methods when h and k simultaneously go to
0 must be established and we do not pursue in this direction. We refer for example
to [5, 52, 62, 64, 66, 77].
After having discussed the ﬁrst two main classes of problems, namely elliptic
and parabolic problems, we now turn to the third class of PDE problems, hyperbolic
problems. We ﬁrst focus on the wave equation in Chap.9, then on the transport
equation in Chap.10.

Chapter 9
The Wave Equation
In this chapter, we present a short and even more far from exhaustive theoretical study
of the wave equation. We establish the existence and uniqueness of the solution, as
well as the energy estimates. We describe the qualitative behavior of solutions, which
is very different from that of the heat equation. Again, we will mostly work in one
dimension of space.
In the same chapter, we introduce ﬁnite difference methods for the numerical
approximation of the wave equation. Here again, stability issues are prominent, and
signiﬁcantly more delicate than for the heat equation.
9.1
Regular Solutions of the Wave Equation
Recall that the general wave equation reads
∂2u
∂t2 −Δu = f in Q = Ω × ]0, T [,
where Ω is an open subset of Rd and f is a given function on Q, complemented
with boundary and initial conditions, see Chap.1, Sects.1.5 and 1.6. The propagation
speed c is set to 1, which we can always assume after a change of time or length
unit. There are two different settings depending on whether Ω is bounded or not. In
the one-dimensional case, d = 1, we thus have either Ω = ]a, b[ or Ω = R without
loss of generality.1
1Admittedly, there is a third case, Ω = R∗
+, but we will not consider it here.
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_9
307

308
9
The Wave Equation
Let us begin with the bounded case. We are thus looking for a function u : [a, b] ×
[0, T ] →R which solves the initial-boundary value problem
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂2u
∂t2 −∂2u
∂x2 = f in Q,
u(a, t) = u(b, t) = 0 for t ∈[0, T ],
u(x, 0) = u0(x), ∂u
∂t (x, 0) = u1(x) for x ∈]a, b[,
(9.1)
with homogeneous Dirichlet boundary conditions for simplicity and two initial data,
u0 and u1. If we think of the vibrating string interpretation, this means that the string
is ﬁxed at both ends, and that we are given its initial position and initial velocity.
This is quite normal, since the equation is derived from Newton’s law of motion and
is of second order in time.
Deﬁnition 9.1 The quantity
E(t) = 1
2
 b
a
∂u
∂t (x, t)
	2
+
∂u
∂x (x, t)
	2
dx
is called the energy.
Of course, we assume that the solution is regular enough for the above quantity
to make sense. In the vibrating string interpretation, this is exactly the mechanical
energy of the string at time t. The ﬁrst term corresponds to the kinetic energy since
it is half the square of the velocity at point x and time t, integrated along the string.
The second term corresponds to the elastic energy, which can be seen by examining
the work done by the exterior forces, based on the analysis in Chap.1, Sect.1.1. The
initial energy is then
E(0) = 1
2
 b
a
(u1(x)2 + u′
0(x)2) dx.
The initial energy is ﬁnite for u0 ∈H 1(]a, b[) and u1 ∈L2(a, b).
Proposition 9.1 Let u be a smooth enough solution of problem (9.1), then we have
dE
dt (t) =
 b
a
f (x, t)∂u
∂t (x, t) dx.

9.1 Regular Solutions of the Wave Equation
309
Proof By differentiation under the integral sign, we have
dE
dt (t) = 1
2
 b
a
 ∂
∂t
∂u
∂t
	2	
+ ∂
∂t
∂u
∂x
	2	
dx
=
 b
a
∂u
∂t
∂2u
∂t2 + ∂u
∂x
∂2u
∂x∂t

dx.
We integrate the second term by parts
 b
a
∂u
∂x
∂2u
∂x∂t dx =
∂u
∂x
∂u
∂t

b
a −
 b
a
∂2u
∂x2
∂u
∂t dx = −
 b
a
∂2u
∂x2
∂u
∂t dx,
since ∂u
∂t (a, t) = ∂u
∂t (b, t) = 0 due to the Dirichlet boundary condition. Therefore,
dE
dt (t) =
 b
a
∂u
∂t
∂2u
∂t2 −∂2u
∂x2
	
dx =
 b
a
f ∂u
∂t dx,
and the proposition is proved.
□
In the vibrating string interpretation, we thus ﬁnd that the time derivative of the
energy is the power of the applied forces, as is expected from physics.
Corollary 9.1 If the right-hand side f in problem (9.1) vanishes, then the energy is
constant
E(t) = E(0).
Proof Indeed, in this case, dE
dt = 0.
□
Remark 9.1 We note here a sharp contrast with the heat equation, for which the
energy was exponentially decreasing for a zero right-hand side. The heat equation,
which is a parabolic equation, dissipates the energy, whereas the wave equation—a
hyperbolic equation—conserves the energy: a vibrating string keeps vibrating forever
in the absence of dissipation.
□
Corollary 9.2 Problem (9.1) has at most one smooth solution.
Proof Let u1 and u2 be solutions of problem (9.1), and u = u1 −u2. Then u is a
solution of problem (9.1) with right-hand side f = 0, so that E(t) = E(0), and zero
initial data, so that E(0) = 0. It follows from Deﬁnition 9.1 that u = 0.
□
In order to further exploit the energy, we need a general purpose result, known as
Gronwall’s lemma or Gronwall’s inequality.
Theorem 9.1 (Gronwall’s lemma) Let α, β and γ be three continuous functions
deﬁned on [0, T ] such that α is differentiable on ]0, T [. We assume that
α′(t) ≤β(t)α(t) + γ (t) for all t ∈]0, T [.

310
9
The Wave Equation
Then, we have
α(t) ≤e
 t
0 β(s) dsα(0) +
 t
0
e
 t
s β(u) duγ (s) ds.
Proof Let B(t) =
 t
0 β(s) ds and deﬁne δ(t) = e−B(t)α(t). Then δ is differentiable
on ]0, T [ and
δ′(t) = e−B(t)α′(t) −β(t)e−B(t)α(t) = e−B(t)(α′(t) −β(t)α(t))
≤e−B(t)γ (t).
Therefore, by the mean value inequality,
δ(t) −δ(0) ≤
 t
0
e−B(s)γ (s) ds
and we conclude by multiplying the above inequality by eB(t) and by noticing that
B(t) −B(s) =
 t
s β(u) du.
□
Proposition 9.2 We have the energy estimate
sup
t∈[0,T ]
E(t) ≤eT E(0) + 1
2
 T
0
 b
a
eT −s f (x, s)2 dxds.
Proof It follows from Proposition 9.1 that
E′(t) ≤1
2
 b
a
f (x, t)2 dx + 1
2
 b
a
∂u
∂t (x, t)
	2
dx ≤1
2
 b
a
f (x, t)2 dx + E(t).
Thus, by Gronwall’s lemma,
E(t) ≤et E(0) + 1
2
 t
0
 b
a
et−s f (x, s)2 dxds ≤eT E(0) + 1
2
 T
0
 b
a
eT −s f (x, s)2 dxds,
for all t ∈[0, T ].
□
Remark 9.2 The energy estimate provides a stability result in the energy norm, in the
sense of establishing the continuity of the solution with respect to the initial data and
right-hand side. Indeed, if u1 and u2 are two solutions corresponding to right-hand
sides f1 and f2 and initial data u1,0, u1,1 and u2,0, u2,1, applying the energy estimate
to u1 −u2, we obtain
sup
t∈[0,T ]

∥u1 −u2∥2
H 1
0 (]a,b[) +
∂u1
∂t −∂u2
∂t

2
L2(a,b)
	
≤eT (∥u1,0 −u2,0∥2
H 1
0 (]a,b[) + ∥u1,1 −u2,1∥2
L2(a,b) + ∥f1 −f2∥2
L2(Q)).
□

9.1 Regular Solutions of the Wave Equation
311
We now use Fourier series to construct regular solutions of problem (9.1) when
f = 0. For simplicity, we let a = 0, b = 1 and we assume that the initial data are
compatible with the Dirichlet condition, i.e., u0(0) = u0(1) = u1(0) = u1(1) = 0.
As in the case of the heat equation, we expand both functions in Fourier series
u0(x) =
+∞

k=1
b0
k sin(kπx),
u1(x) =
+∞

k=1
b1
k sin(kπx).
Theorem 9.2 Letu0 ∈C4([0, 1])andu1 ∈C3([0, 1])besuchthatu′′
0(0) = u′′
0(1) =
u′′
1(0) = u′′
1(1) = 0. Then the function deﬁned by
u(x, t) =
+∞

k=1

b0
k cos(kπt) + b1
k
kπ sin(kπt)
	
sin(kπx)
(9.2)
belongs to C2([0, 1] × [0, +∞[) and solves problem (9.1) with f = 0.
Proof Under the hypotheses made on u0 and u1, it is easy to see that |b0
k| ≤Ck−4
and |b1
k| ≤Ck−3 for some constant C. Then the series in formula (9.2) as well as the
series of all ﬁrst order and second order derivatives are normally convergent. Hence,
u is of class C2. Moreover, since the functions (x, t) 	→eikπ(t±x) are solutions of the
wave equation with zero right-hand side, it is clear that the normal convergence of
second derivatives implies that u is also a solution of the wave equation.
For t = 0, we have
u(x, 0) =
+∞

k=1
b0
k sin(kπx) = u0(x)
and
∂u
∂t (x, 0) =
+∞

k=1
b1
k sin(kπx) = u1(x),
hence the initial conditions are satisﬁed. Finally, the Dirichlet boundary conditions
are also satisﬁed since sin(kπ) = 0.
□
Remark 9.3 We ﬁnd that the solution is a superposition of harmonics, see Chap.1,
Sect.1.5. Which harmonics are excited depend on the initial conditions. For instance,
for such a musical instrument as the piano, the strings are initially at rest, u0 = 0,
and are hit by a hammer, u1 ̸= 0. In the case of a guitar or a harpsichord, the
strings are typically plucked, u0 ̸= 0, sometimes with no initial velocity, u1 = 0.
Note that other combinations are possible, all resulting in different sounds, see
Figs.9.1 and 9.2.
□

312
9
The Wave Equation
Fig. 9.1 A view of the
evolution in the case of zero
initial velocity u1, u0 has
four nonzero harmonics
Fig. 9.2 A view of the
evolution in the case of zero
initial position, initial
velocity +1 in ]0, 1
2[, −1 in
] 1
2, 1[, two hundred nonzero
terms in the Fourier series
Remark 9.4 The regularity hypotheses made on u0 and u1 are just there to ensure
easy convergence of the series of partial derivatives up to the second order. Indeed,
if the series (9.2) converges in a much weaker sense, its sum is still going to be a
solution of the wave equation in the sense of distributions at least, since differentiation
is continuous in the sense of distributions. The difﬁculty lies in the meaning of the
initial conditions, as some kind of continuity with respect to time is required for them
to make sense.
□
Remark 9.5 A fundamental difference with the heat equation is that the Fourier
coefﬁcients of u(·, t) are not rapidly damped by exponential terms for t > 0, which

9.1 Regular Solutions of the Wave Equation
313
cause the solution of the heat equation to be smooth for t > 0, whatever the initial
data. Here, the wave equation has no smoothing effect whatsoever. The regularity or
lack thereof of the initial conditions is propagated in time without any gain. This is
one of the main differences between parabolic and hyperbolic problems.
□
9.2
Variational Formulation and Existence of Weak
Solutions
We now introduce a variational formulation for the wave equation in a manner that
is quite similar to the one described in Sect.7.6 for the heat equation.
Deﬁnition 9.2 The variational formulation of the wave equation (9.1) with homo-
geneous Dirichlet boundary condition, initial data u0 ∈H 1
0 (Ω), u1 ∈L2(Ω) and
right-hand side f ∈L2(Q) is: Find u ∈C0([0, T ]; H 1
0 (Ω)) ∩C1([0, T ]; L2(Ω))
such that, for all v ∈H 1
0 (Ω),
⎧
⎪⎨
⎪⎩

(u|v)L2(Ω)
′′ + a(u, v) = ( f |v)L2(Ω) in the sense of D′(]0, T [),
(u(0)|v)L2(Ω) = (u0|v)L2(Ω),
(u′(0)|v)L2(Ω) = (u1|v)L2(Ω).
(9.3)
Remark 9.6 This deﬁnition clearly makes sense. The last two equations are a weak
form of the initial conditions u(0) = u0, u′(0) = u1.
□
For simplicity, we work again on Ω = ]0, 1[ with the minus Laplacian eigenfunc-
tions φk(x) =
√
2 sin(kπx) and eigenvalues λk = k2π2, and we have a(w, φk) =
λk(w|φk)L2(Ω) for all w ∈H 1
0 (Ω), see Eq.(7.4).
Theorem 9.3 Let u0 ∈H 1
0 (Ω), u1 ∈L2(Ω), f ∈L2(Q). There exists a unique
solution u ∈C0([0, T ]; H 1
0 (Ω)) ∩C1([0, T ]; L2(Ω)) of the initial-boundary value
problem (9.3), which is given by
u(t) =
+∞

k=1
uk(t)φk,
(9.4)
where
uk(t) = (u0|φk)L2(Ω) cos

λkt

+ (u1|φk)L2(Ω)
√λk
sin

λkt

+
1
√λk
 t
0
( f (s)|φk)L2(Ω) sin

λk(t −s)

ds. (9.5)

314
9
The Wave Equation
Proof We
start
with
the
uniqueness.
Let
u ∈C0([0, T ]; H 1
0 (Ω)) ∩
C1([0, T ]; L2(Ω)) be a solution of (9.3). We expand u(t) on the Hilbert basis
(φk)k∈N∗of L2(Ω) so that, for all t,
u(t) =
+∞

k=1
uk(t)φk
with
uk(t) = (u(t)|φk)L2(Ω)
for all k ∈N∗and the series converges in L2(Ω). Likewise, we set u0 = +∞
k=1 u0,kφk,
u1 = +∞
k=1 u1,kφk and f (t) = +∞
k=1 fk(t)φk. Taking φk ∈H 1
0 (Ω) as a test-function
in problem (9.3), we obtain
⎧
⎪⎨
⎪⎩
u′′
k(t) + λkuk(t) = fk(t) in the sense of D′(]0, T [),
uk(0) = u0,k,
u′
k(0) = u1,k,
for all k ∈N∗. For each k, this is a Cauchy problem for an ordinary differential
equation which has the unique solution
uk(t) = u0,k cos

λkt

+ u1,k
√λk
sin

λkt

+
1
√λk
 t
0
fk(s) sin

λk(t −s)

ds,
hence the uniqueness.
We now use the above series to prove existence. We have that u0 ∈H 1
0 (Ω) and
u1 ∈L2(Ω) by hypothesis, therefore thanks to formula (7.7),
∥u0∥2
H 1(Ω) =
+∞

k=1
(1 + λk)u2
0,k, |u0|2
H 1(Ω) =
+∞

k=1
λku2
0,k and ∥u1∥2
L2(Ω) =
+∞

k=1
u2
1,k.
(9.6)
Similarly, f ∈L2(Q) and
∥f ∥2
L2(Q) =
 T
0
+∞

k=1
fk(t)2 dt.
(9.7)
As before, we consider the sequence of partial sums Un(t) = n
k=1 uk(t)φk and
show that it is Cauchy for both C0([0, T ]; H 1
0 (Ω)) and C1([0, T ]; L2(Ω)) norms.
Let p < q be two given integers and let us estimate Up −Uq in these various norms.

9.2 Variational Formulation and Existence of Weak Solutions
315
First of all, we have for all t
|Up(t) −Uq(t)|2
H 1
0 (Ω) =
q

k=p+1
λkuk(t)2
≤2
q

k=p+1
λk

u2
0,k + 1
λk
u2
1,k + 1
λk
 t
0
| fk(s)| ds
	2
≤2
q

k=p+1
λku2
0,k + 2
q

k=p+1
u2
1,k + 2T
q

k=p+1
 T
0
fk(s)2 ds,
since all the trigonometric terms are less than 1 in absolute value and by the Cauchy–
Schwarz inequality. Therefore
∥Up −Uq∥2
C0([0,T ];H 1
0 (Ω)) ≤2
q

k=p+1
λku2
0,k + 2
q

k=p+1
u2
1,k + 2T
q

k=p+1
 T
0
fk(s)2 ds
can be made as small as we wish by taking p large enough, due to the hypotheses
on u0, u1 and f and formulas (9.6)–(9.7), and the sequence is consequently Cauchy
in C0(0, T ; H 1
0 (Ω)).
It follows from the previous estimate and the Poincaré inequality that the sequence
is also Cauchy in C0(0, T ; L2(Ω)). We need to look at its time derivative. Of course,
U ′
n(t) = n
k=1 u′
k(t)φk with
u′
k(t) = −

λku0,k sin

λkt

+ u1,k cos

λkt

+
 t
0
fk(s) cos

λk(t −s)

ds,
so that
∥U ′
p −U ′
q∥2
C0([0,T ];L2(Ω)) ≤2
q

k=p+1
λku2
0,k + 2
q

k=p+1
u2
1,k + 2T
q

k=p+1
 T
0
fk(s)2 ds
and the sequence U ′
n is Cauchy in C0(0, T ; L2(Ω)), which completes the proof of
the convergence of the series (9.4) in the above-mentioned spaces.
Regarding the wave equation itself, setting Fn(t) = n
k=1 fk(t)φk, we have
n

k=1
u′′
k(t)φk +
n

k=1
λkuk(t)φk = Fn(t).
Foralltest-functionsv ∈H 1
0 (Ω),bytakingthe L2 scalarproductoftheaboveformula
with v = +∞
k=1 vkφk, we thus obtain

316
9
The Wave Equation
n

k=1
u′′
k(t)vk +
n

k=1
λkuk(t)vk = (Fn(t)|v)L2(Ω).
Now n
k=1 uk(t)vk = (Un(t)|v)L2(Ω) →(u(t)|v)L2(Ω) in C0([0, T ]), so that
n

k=1
u′′
k(t)vk =

(Un(t)|v)L2(Ω)
′′ →

(u(t)|v)L2(Ω)
′′ in the sense of D′(]0, T [)
when n →+∞. Similarly
n

k=1
λkuk(t)vk = a(Un(t), v) →a(u(t), v) in C0([0, T ]).
Finally, Fn →f in L2(0, T ; L2(Ω)) and therefore
(Fn(t)|v)L2(Ω) →( f (t)|v)L2(Ω) in L2(0, T ),
and we obtain the variational form of the wave equation in the limit n →+∞.
The initial conditions are obviously satisﬁed by construction.
□
Remark 9.7 For this proof to work, we need the compatibility condition between
the initial condition u0 and the Dirichlet boundary condition, but no such condition
is needed for the initial velocity u1. Formulas (9.4)–(9.5) clearly generalizes the
expansion obtained in Theorem 9.2.
□
Remark 9.8 The d-dimensional wave equation can be solved along the exact same
lines, see [5, 28]. However, here again, other approaches, such as semigroups, are
possible.
□
Remark 9.9 The series estimates above immediately imply stability in the energy
norm for the weak solutions as well, in the sense that given two sets of data with
corresponding solutions u1 and u2, we have
∥u1 −u2∥2
C0([0,T ];H 1
0 (Ω)) + ∥u1 −u2∥2
C1([0,T ];L2(Ω))
≤C(|u1,0 −u2,0|2
H 1
0 (Ω) + ∥u1,1 −u2,1∥2
L2(Ω) + ∥f1 −f2∥2
L2(Q)).
This is also a continuity result of the solution with respect to the initial conditions
and right-hand side.
□
As a consequence, the energy equality of Proposition 9.1 is still valid here, the
energy being deﬁned by E(t) = 1
2

∥u′(t)∥2
L2(Ω) + |u(t)|2
H 1
0 (Ω)

as before. More pre-
cisely,
Proposition 9.3 Let u be the solution given by Theorem 9.3. Then we have
E ∈H 1(]0, T [) with

9.2 Variational Formulation and Existence of Weak Solutions
317
dE
dt (t) =

f (t)|u′(t)

L2(Ω).
Proof We approximate u0, u1 and f by smooth functions un
0, un
1 and f n, in their
respective function spaces. By the stability estimate above, the corresponding solu-
tion un is such that un →u in C0([0, T ]; H 1
0 (Ω)) ∩C1([0, T ]; L2(Ω)). We can
apply Proposition 9.1 to un so that2
dEn
dt (t) =

f n(t)|(un)′(t)

L2(Ω),
where En(t) is the energy of un. Clearly En →E in C0([0, T ]). Moreover,


f n(t)|(un)′(t)

L2(Ω) −

f (t)|u′(t)

L2(Ω)

≤


f n(t) −f (t)|(un)′(t)

L2(Ω)
 +


f (t)|(un)′(t) −u′(t)

L2(Ω)

≤∥f n(t) −f (t)∥L2(Ω)∥(un)′(t)∥L2(Ω)
+ ∥f (t)∥L2(Ω)∥(un)′(t) −u′(t)∥L2(Ω),
so that squaring and integrating in time, we obtain
dEn
dt
−( f |u′)L2(Ω)

2
L2(0,T ) ≤C

∥f n −f ∥2
L2(0,T ;L2(Ω)) + ∥(un)′ −u′∥2
C0([0,T ];L2(Ω))

.
It follows from this that dEn
dt →( f |u′)L2(Ω) in L2(0, T ), and since dEn
dt →dE
dt in the
sense of D′(]0, T [), that dE
dt = ( f |u′)L2(Ω) belongs to L2(0, T ).
□
Remark 9.10 In the case f = 0, we obtain that the energy is also conserved for weak
solutions.
□
9.3
The Wave Equation on R
WenowconsiderthecaseofthewaveequationonRwith f = 0.Thereisnoboundary
condition. In this case, there is an explicit formula for the solution, similar to that
obtained for the transport equation and known as the d’Alembert formula, see [35].
Theorem 9.4 Let u0 ∈C1(R) and u1 ∈C0(R). The solution of problem (9.1) on R
with f = 0 is given by
u(x, t) = 1
2

u0(x + t) + u0(x −t) +
 x+t
x−t
u1(s) ds
	
.
(9.8)
2We admit here that both formulations coincide in the smooth case.

318
9
The Wave Equation
Proof The function given by formula (9.8) is continuous on R × R+, hence is a
distribution on R × R∗
+. It is in fact of class C1 on R × R+, and we can write
u(x, t) = F(x + t) + G(x −t)
(9.9)
with F(y) = 1
2

u0(y) +
 y
0 u1(s) ds

and G(y) = 1
2

u0(y) +
 0
y u1(s) ds

. Let us
set U(x, t) = F(x + t) and V (x, t) = G(x −t). Of course, we have
∂U
∂t (x, t) = F′(x + t),
∂U
∂x (x, t) = F′(x + t)
(9.10)
and
∂V
∂t (x, t) = −G′(x −t),
∂V
∂x (x, t) = G′(x −t).
(9.11)
Let us compute ∂2U
∂t2 −∂2U
∂x2 in the sense of distributions. We thus take ϕ ∈D(R × R∗
+)
and consider the following duality bracket
∂2U
∂t2 −∂2U
∂x2 , ϕ

= −
∂U
∂t −∂U
∂x , ∂ϕ
∂t + ∂ϕ
∂x

= 0,
by Eq.(9.10), and similarly ∂2V
∂t2 −∂2V
∂x2 = 0 in the sense of distributions by Eq.(9.11).
Concerning the initial conditions, of course
u(x, 0) = 1
2

u0(x) + u0(x) +
 x
x
u1(s) ds
	
= u0(x)
and since
∂u
∂t (x, t) = 1
2

u′
0(x + t) −u′
0(x −t) + u1(x + t) + u1(x −t)

,
we have
∂u
∂t (x, 0) = 1
2

u′
0(x) −u′
0(x) + u1(x) + u1(x)

= u1(x).
This proves the theorem.
□
Remark 9.11 Formula (9.8) makes sense for much less regular data, for example u0
and u1 in L1
loc(R), and still gives rise to a solution of the wave equation in the sense
of distributions. In fact, we may even take u0 and u1 in D′(R) by interpreting the
integral as the sum of two primitives. The problem is thus to make sense of the initial
condition in such a nonsmooth context, see Figs.9.3 and 9.4. Some continuity with
respect to time is needed, but we do not pursue in this direction.
Such an explicit formula as (9.8) is speciﬁc to the one-dimensional case. The
solution is not so simple in higher dimensions of space.
□

9.3 The Wave Equation on R
319
Fig. 9.3 A view of the
evolution in the case of
u0 = 1[−1/2,1/2] and zero
initial velocity, with the two
issuing waves propagating
right and left 1
2u0(x −t) and
1
2u0(x + t). Also pictured in
thicker red line the solution
at t = 1
3 and t = 2
Fig. 9.4 A view of the
evolution in the case of
u1 = 1[−1,1] and u0 = 0.
Also pictured in thicker red
line the solution at t = 1
2,
t = 1 and t = 2. It is only
Lipschitz in space and time
Remark 9.12 The solution pictured in Fig.9.3 is discontinuous in space and time.
It is thus meant to be understood as a solution of the wave equation in the sense of
distributions. The interpretation of the initial conditions, in particular for the velocity,
is admittedly a little more delicate.
□
Remark 9.13 Independently of any considerations of initial data, it is easy to see
that all solutions of the wave equation are of the form (9.9). Indeed, the change of
variables w = x + t, z = x −t leads to the equation
∂2u
∂w∂z = 0 whose solutions are
clearly of the form F(w) + G(z). The solution is thus seen as the superposition of
two waves, one traveling to the left at speed −1 (F(x + t)) and the other traveling to
the right at speed +1 (G(x −t)). In the general case, c ̸= 1, the corresponding form
is u(x, t) = F(x + ct) + G(x −ct).
□
Remark 9.14 We also see that the wave equation propagates waves at ﬁnite speed
(±c), as opposed to the heat equation which has inﬁnite speed of propagation. In
particular, if the initial data are compactly supported in [a, b], then the solution at
time t is compactly supported in [a −ct, b + ct]. Another way of seeing this is to
note that the value of the solution at point (x, t) only depends on what happens
in its backward cone of inﬂuence {(y, s) ∈R × R+; s ≤t, |y −x| ≤c(t −s)}, see
Fig.9.5. The information situated outside of the cone of inﬂuence does not have the
time to propagate to point (x, t).
□

320
9
The Wave Equation
Fig. 9.5 The backward cone
of inﬂuence of point (x, t)
x
t
x+ ct
x−ct
Remark 9.15 If u0 is compactly supported and u1 = 0, an observer located at some
point x > 0 initially outside of the support of u0, sees a wave 1
2u0(x −ct) reach
him or her after some time, pass through, and then go back to exactly 0. This is a
feature of the wave equation in odd dimensions of space. This explains why we see
light and hear sounds as we do: a ﬂash of light at some point in space-time results
in a spherical wavefront expanding at the speed of light that an observer experiences
as a single instantaneous ﬂash when reached by the wavefront. The same goes for
sound. This is not true in even dimensions. For example, if we throw a rock on a
lake, the resulting wave on the surface of the lake expands as a circle traveling at the
speed of waves on water, but never goes back to rest inside the disk, even though
the solution is much smaller there. If we lived on the surface of the water, we would
experience a ﬂash followed by a never-ending afterglow… good thing we live in an
odd-dimensional space.
□
Remark 9.16 The wave equation is invariant under the change t →−t. This means
that time is reversible in the wave equation, which is another feature in sharp contrast
with the heat equation.
□
9.4
Finite Difference Schemes for the Wave Equation
The principle of ﬁnite difference methods for the wave equation is exactly the same
as for the heat equation, and the notation is also the same. Since the wave equation is
of second order in time, a natural idea is to consider two time steps ﬁnite difference
schemes, even though we will see that this is not necessarily a good idea. We will
assume the initial conditions U 0 and U 1 to be given in terms of u0 and u1. For
instance, a simple choice could be
U 0 = Sh(u0),
U 1 = U 0 + kSh(u1),
or higher order approximations for U 1.
The most obvious scheme consists in approximating the second time derivative
by means of the usual central difference, which yields

9.4 Finite Difference Schemes for the Wave Equation
321
u j+1
n
−2u j
n + u j−1
n
k2
−u j
n+1 −2u j
n + u j
n−1
h2
= f j
n ,
(9.12)
with the usual boundary conditions and initial data. This is obviously an explicit,
two time steps scheme. In vector form, it reads
U j+1 −2U j + U j−1
k2
+ AhU j = F j,
where Ah is still given by formula (2.8) with c = 0, p. 40 of Chap.2. It should be
quite clear that the scheme is consistent and of order 2 in space and time. Therefore,
its convergence is solely a matter of stability.
We reformulate the above scheme as a single time step scheme by setting
V j =
 U j
U j−1

∈R2N,
and
1
k2 V j+1 =
 2
k2 I −Ah −1
k2 I
1
k2 I
0
  U j
U j−1

+
F j
0

= 1
k2 A V j + G j
with a 2N × 2N ampliﬁcation matrix A =
C −I
I
0

with C = 2I −k2 Ah, and
G j ∈R2N. Unfortunately, the matrix A is not normal. Indeed
A T A =
C2 + I −C
−C
I

̸=
C2 + I C
C
I

= A A T .
Therefore, we only have ρ(A ) ≤|||A |||2,h and the condition ρ(A ) ≤1 + C(T )k is
just a necessary condition for stability, see Remark8.13 in Chap.8, whereas we also
would like to have a sufﬁcient condition for stability. In order to have a necessary
stability condition that is valid for all T , it is easier to require ρ(A ) ≤1. Let us see
what we can say about the spectral radius of A .
Lemma 9.1 Let C be a N × N complex matrix and B the 2N × 2N complex matrix
deﬁned by blocks as
B =
C −I
I
0

.

322
9
The Wave Equation
If λ ∈C is an eigenvalue of B, then λ ̸= 0 and λ + 1
λ is an eigenvalue of C. Con-
versely, if μ ∈C is an eigenvalue of C, then there exists an eigenvalue λ of B such
that μ = λ + 1
λ.
Proof The proof is similar to that of Lemma8.2 of Chap.8.
□
Proposition 9.4 If k
h ≤1, then the necessary stability condition for scheme (9.12)
is satisﬁed.
Proof Recall that stability is meant here in the sense of ρ(A ) ≤1. So we need to ﬁnd
out when all the eigenvalues λ of A are such that |λ| ≤1. According to Lemma 9.1,
the eigenvalues in question are of the form λ± =
μ±√
μ2−4
2
where μ is an eigenvalue
of C = 2I −k2 Ah, hence is real. We thus see that there are two cases:
1. |μ| > 2. In this case, the two eigenvalues λ± are real, distinct, and since their
product is equal to 1, one of them is strictly larger than 1 in absolute value. Hence
this is an unstable case.
2. |μ| ≤2. In this case, λ± are complex conjugate, and since their product is equal
to 1, they are both of modulus 1. The necessary stability condition is thus satisﬁed.
Now we have μ = 2 −4 k2
h2 sin2
pπ
2(N+1)

, p = 1, . . . , N. Clearly, if k
h ≤1, then
we have |μ| ≤2.
□
Remark 9.17 The condition k
h ≤1 is called the Courant–Friedrichs–Lewy or CFL
condition. In the general case, the CFL condition assumes the form k
h ≤1
c. In a sense,
h
k is the numerical velocity needed to reach the neighboring grid points in one time
step starting from one spatial grid point, see Fig.9.6. The CFL condition is that this
numerical velocity must be larger than the propagation velocity.
In other words, the discrete backward cone of inﬂuence of a point (xn, t j) must
contain its continuous backward cone of inﬂuence, in order for the scheme to have
access to all the information needed to compute a relevant approximation at that
point. Of course, this kind of requirement only applies to explicit schemes.
□
Let us plot the result of the explicit scheme with + marks and the exact solution
in solid line in Fig.9.7. We take the same u0 as for the heat equation, i.e., u0(x) =
sin(πx)/2 + sin(2πx), and u1 = 0. We have taken U 1 = U 0, which is actually a
Fig. 9.6 Discrete cone of
inﬂuence versus continuous
cone of inﬂuence
xn
tj
xn + ctj
xn −ct j
xn + jh
xn −jh

9.4 Finite Difference Schemes for the Wave Equation
323
- 1.5
- 1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 0, h = 0.0196078, k = 0.0196078, k/h = 1
- 1.5
- 1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 5, h = 0.0196078, k = 0.0196078, k/h = 1
- 1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 11, h = 0.0196078, k = 0.0196078, k/h = 1
- 1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 16, h = 0.0196078, k = 0.0196078, k/h = 1
- 1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 22, h = 0.0196078, k = 0.0196078, k/h = 1
- 1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 28, h = 0.0196078, k = 0.0196078, k/h = 1
- 1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 34, h = 0.0196078, k = 0.0196078, k/h = 1
- 1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 40, h = 0.0196078, k = 0.0196078, k/h = 1
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
j = 45, h = 0.0196078, k = 0.0196078, k/h = 1
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
j = 50, h = 0.0196078, k = 0.0196078, k/h = 1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Fig. 9.7 Explicit scheme, u0(x) = sin(πx)/2 + sin(2πx), u1(x) = 0

324
9
The Wave Equation
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 0, h = 0.0196078, k = 0.0196078, k/h = 1
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 1, h = 0.0196078, k = 0.0196078, k/h = 1
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 2, h = 0.0196078, k = 0.0196078, k/h = 1
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 3, h = 0.0196078, k = 0.0196078, k/h = 1
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 4, h = 0.0196078, k = 0.0196078, k/h = 1
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j = 5, h = 0.0196078, k = 0.0196078, k/h = 1
Fig. 9.8 Explicit scheme, u0 = 1[ 1
3 , 2
3 ], u1 = 0
second order approximation of the condition u1 = 0, hence the good global accuracy
of the scheme in this particular case.
Of course, the initial condition is very smooth here. If we want to compute a
discontinuous solution with this scheme, we run into trouble with severe unwanted
oscillations, see Fig.9.8. This kind of discontinuous solution is of physical interest in
situations where shock waves occur. Devising numerical schemes capable of reliably
capturing shocks thus requires skills that go beyond the scope of these notes.

9.4 Finite Difference Schemes for the Wave Equation
325
Fig. 9.9 Explicit scheme,
u0(x) =
sin(πx)/2 + sin(2πx),
u1(x) = 0, CFL condition
not satisﬁed
- 1e+ 08
- 8e+ 07
- 6e+ 07
- 4e+ 07
- 2e+ 07
0e+ 00
2e+ 07
4e+ 07
6e+ 07
8e+ 07
1e+ 08
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
j =  40, h =  0.0196078, k =  0.0243902, k/ h =  1.2439024
For the record, Fig.9.9 shows what happens after a few iterations when the CFL
condition is violated, even with the very smooth initial condition used above.
The next obvious scheme is the implicit version of the former one
u j+1
n
−2u j
n + u j−1
n
k2
−u j+1
n+1 −2u j+1
n
+ u j+1
n−1
h2
= f j+1
n
,
(9.13)
with the usual boundary conditions and initial data. In vector form, it reads
 1
k2 I + Ah
	
U j+1 = 2
k2 U j −1
k2 U j−1 + F j+1.
We rewrite it as a single time step scheme
V j+1 = A V j + G j
with
A =
2(I + k2 Ah)−1 −(I + k2 Ah)−1
I
0

.
Again, the matrix A is not normal, but we can look at its spectral radius. Setting C =
(I + k2 Ah)−1, the same kind of arguments as before show that the eigenvalues λ of A
are of the form λ± = μ ±

μ2 −μ where μ is an eigenvalue of C. Now μ ∈]0, 1[,
therefore μ2 −μ < 0 and the eigenvalues λ+ and λ−are complex conjugate, of
modulus √μ. The necessary condition for stability is thus unconditionally satisﬁed.
The implicit scheme does a slightly better job of capturing shocks than the explicit
scheme for the same discretization parameters, but it still has a lot of numerical
diffusion that spreads out the shocks, see Fig.9.10.
A third scheme is the θ-scheme for θ ∈[0, 1
2], written here for f = 0,

326
9
The Wave Equation
Fig. 9.10 Implicit scheme,
u0 = 1[ 1
3 , 2
3 ], u1 = 0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-1.5
-1.0
-0.5
0.0
0.5
1.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
1.5
j = 0, h = 0.0196078, k = 0.0196078, k/h = 1
j = 2, h = 0.0196078, k = 0.0196078, k/h = 1
j = 4, h = 0.0196078, k = 0.0196078, k/h = 1
j = 6, h = 0.0196078, k = 0.0196078, k/h = 1
j = 8, h = 0.0196078, k = 0.0196078, k/h = 1
j = 10, h = 0.0196078, k = 0.0196078, k/h = 1
j = 1, h = 0.0196078, k = 0.0196078, k/h = 1
j = 3, h = 0.0196078, k = 0.0196078, k/h = 1
j = 5, h = 0.0196078, k = 0.0196078, k/h = 1
j = 7, h = 0.0196078, k = 0.0196078, k/h = 1
j = 9, h = 0.0196078, k = 0.0196078, k/h = 1
j = 11, h = 0.0196078, k = 0.0196078, k/h = 1
1.0
1.0
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.0
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.0
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0

9.4 Finite Difference Schemes for the Wave Equation
327
u j+1
n
−2u j
n + u j−1
n
k2
−θ u j+1
n+1 −2u j+1
n
+ u j+1
n−1
h2
−(1 −2θ)u j
n+1 −2u j
n + u j
n−1
h2
−θ u j−1
n+1 −2u j−1
n
+ u j−1
n−1
h2
= 0,
(9.14)
which reduces to the explicit scheme for θ = 0 and is implicit for θ > 0.
9.5
Stability via the Fourier Approach
So far we have only obtained necessary conditions for stability, because the matrices
were not normal. As in the case of numerical schemes for the heat equation, we
can also use the Fourier method to obtain sufﬁcient conditions. Again, we work on
the whole of R. We take f = 0. As was already mentioned, there are no boundary
conditions and let us forget for the moment that the solution is given by an explicit
formula.
Concerning the relationship between discrete and semi-discrete schemes, every
thing said for the heat equation holds true here. In addition, we note that, due to the
ﬁnite speed of propagation, if the initial data is compactly supported in an interval,
a ﬁnite difference scheme on the interval with boundary conditions will compute
exactly the same values as the same scheme on R as long as the wave has not hit the
ends of the interval. Therefore, so does the semi-discrete scheme, and the stability
conditions obtained from the Fourier method actually apply to the scheme on an
interval with boundary conditions (at least under the previous conditions).
Let us ﬁrst consider the explicit scheme (9.12), for which we already have a nec-
essary stability condition, but no sufﬁcient condition. Both Fourier approaches are
more complicated than for the heat equation, since the linear recurrence relations
obtained are two-step relations, which are harder to analyze than the one-step rela-
tions in the heat equation case. Since the two Fourier approaches are very similar to
each other, we ﬁrst concentrate on the Fourier series point of view for brevity, see
Sect.8.7.
Since we are working on the whole space, the scheme (9.12) with zero right-hand
side takes the form
u j+1
n
−2u j
n + u j−1
n
k2
−u j
n+1 −2u j
n + u j
n−1
h2
= 0, for n ∈Z.
(9.15)
In Fourier space, the scheme reads
F(u j+1)(s) −2F(u j)(s) + F(u j−1)(s)
k2
+ 4
h2 sin2s
2
	
F(u j)(s) = 0,
(9.16)
for s ∈[0, 2π]. As was mentioned above, this is a two-step linear recurrence rela-
tion, which we rewrite in vector form by introducing the R2-valued sequence

328
9
The Wave Equation
Z j
n =

u j+1
n
u j
n

, the Fourier series transform of which satisﬁes
F(Z j+1)(s) = A(s)F(Z j)(s)
where
A(s) =
2 −a(s)2 −1
1
0

with a(s) = 2k
h sin
 s
2

. The matrix A is called the ampliﬁcation matrix of the scheme.
We list without proof several properties of such schemes, since they are easy gener-
alizations of former results.
Let A ∈C0([0, 2π]; M2(C)) and consider the multiplier operator L deﬁned on
L2([0, 2π]; C2) with values in L2([0, 2π]; C2) by (LY)(s) = A(s)Y(s), for almost
all s ∈[0, 2π]. The generalization of Eq. (8.7) in this case is
∥L∥L (L2([0,2π];C2)) = max
s∈[0,2π] |||A(s)|||2
(we use the standard hermitian norm on C2), see [67]. It follows that the scheme
(9.16) is stable in L2 if and only if there exists a constant C(T ) such that
max
s∈[0,2π] |||A(s) j|||2 ≤C(T )
for all j ≤T/k. Since ρ(A(s)) ≤|||A(s) j|||1/j
2 , a necessary condition of stability is
that there exists a nonnegative constant C that does not depend on k and h such that
ρ(A(s)) ≤1 + Ck for all s. We say that the scheme is stable in the sense of von
Neumann if ρ(A(s)) ≤1 for all s.
Proposition 9.5 The scheme (9.16) is stable in the sense of von Neumann if and
only if k
h ≤1.
Proof The characteristic polynomial of A(s) is
PA(s)(X) = X2 −(2 −a(s)2)X + 1.
The product of the two roots is 1, thus if they are both real and simple, one of them
is strictly larger than 1 in absolute value. On the other hand, if they are complex
conjugate, they are both of modulus 1. Consequently, von Neumann stability is
equivalent to the discriminant being non positive.
We thus need to see under which condition Δ(s) = a(s)2(a(s)2 −4) ≤0 for all s.
Taking s = π, we see that a necessary condition is k
h ≤1. Conversely, this condition
is clearly sufﬁcient.
□
In the case of a normal ampliﬁcation matrix, the former conditions are also sufﬁ-
cient for L2-stability. Now the matrix A(s) obtained above is not normal in general,

9.5 Stability via the Fourier Approach
329
except for s = π if k
h = 1, thus we need a more general result. We work directly on
the stability of the scheme expressed in Fourier space by the operator L above, since
this is equivalent to L2-stability for the discrete scheme, due to Parseval’s formula.
Stability in L2 is ensured if |||A(s) j|||2 ≤C(T ) for all s and j ≤T/k, where C(T )
does not depend on h or k. It should be noted that this condition is a priori much easier
to check here than it was for (actual) ﬁnite difference schemes, since the matrix in
question is always of the same size, i.e., 2 × 2, whereas the size of the matrix in ﬁnite
difference schemes was N × N with h =
1
N+1 and the norm also depended on h.
Proposition 9.6 The discrete scheme (9.16) is not stable in L2.
Proof It is enough to look at what happens for s = 0. In this case, the ampliﬁcation
matrix has the double eigenvalue 1 and is clearly not diagonalizable. In effect,
A(0) =
2 −1
1 0

= P
1 1
0 1

P−1, where P =
1 1
1 0

,
from which we immediately deduce that
A(0) j =
 j + 1 −j
j
1 −j

.
We thus have |||A(0) j|||2 =

2 j2 + 1 + 2 j

j2 + 1 ≥2 j. It follows in particular
that |||A(0)T/k|||2 ≥2T
k , hence the instability of the discrete scheme.
□
Remark 9.18 This is an unsettling result, since we could have rightfully expected
the very natural scheme (9.15) to be stable under the CFL condition k
h ≤1 or at least
k
h < 1. This is not the case.
The reason for the natural scheme not to be stable is the following. If it was stable
in ℓ2, then having sequences of initial data (u0)m and (u1)m bounded in ℓ2 would
result in a sequence of solutions (u j)m also bounded in ℓ2, uniformly for j ≤T/k.
However, this boundedness could be achieved with (u0)m and (u1)m having strictly
nothing to do with each other. Now we need to remember that u0
n = u0(nh) and u1
n is
supposed to be some approximation of u(nh, k) ≈u0(nh) + k ∂u
∂t (nh, 0). Thus both
initial conditions of the wave equation u(x, 0) = u0(x) and ∂u
∂t (x, 0) = u1(x) must
somehow be taken into account in the discrete initial data u0 and u1 for the scheme
to have any chance to converge. This is not the case if (u0)m and (u1)m can be chosen
independently of each other.
Note that the scheme is unstable in spite of being von Neumann stable, an unhappy
effect of terminology.
□
Remark 9.19 We can see the instability of the scheme on the following example.
Let us consider the initial data u0 = 0 and u1 given by u1
0 = h−1/2 and u1
n = 0 for
n ̸= 0. We have ∥u1∥2,h = 1. Let us show that ∥u j∥2,h is not bounded for j ≤T/k.
In Fourier space, the recurrence relation reads

330
9
The Wave Equation
F(u j+1)(s) = (2 −a2(s))F(u j)(s) −F(u j−1)(s).
The characteristic equation of this recurrence relation X2 −(2 −a2(s))X + 1 = 0
has two roots r± = e±iθ(s), θ(s) = arccos(1 −a2(s)
2 ) for s ̸= 0 and s ̸= 2π, and a
double root r = 1 for s = 0 or 2π.
We have F(u0)(s) = 0 and F(u1)(s) = h−1/2, thus
F(u j)(s) =

h−1/2 sin( jθ(s))
sin(θ(s)) , for 0 < s < 2π,
jh−1/2,
for s = 0 or s = 2π.
We are interested in the L2 norm of the above function. Clearly
∥F(u j)∥2
L2(0,2π),h ≥
 θ−1( π
2 j )
0
sin2( jθ(s))
sin2(θ(s)) ds.
Now, on the interval

0, θ−1 π
2 j

, we have sin( jθ(s))
sin(θ(s)) ≥2 j
π . Consequently
∥F(u j)∥2
L2(0,2π),h ≥4 j2
π2 θ−1 π
2 j
	
.
After a little bit of computation, we ﬁnd that θ−1 π
2 j

∼
π
2
√
2λ j when j →+∞.
Therefore, for j large enough, we obtain
∥F(u j)∥2
L2(0,2π),h ≥
j
π
√
2λ
→+∞when j →+∞.
Going back to the original discrete scheme, it follows that ∥u j∥2,h →+∞when
j →+∞for this particular sequence of bounded initial data.
□
In order to obtain sufﬁcient stability conditions, we actually need to change the
unknowns so as to appropriately take care of the wave equation initial conditions.
First, we rewrite the wave equation: Find u : Q = R × [0, T ] →R such that
⎧
⎪⎪⎨
⎪⎪⎩
∂2u
∂t2 −∂2u
∂x2 = 0 in Q,
u(x, 0) = u0(x), ∂u
∂t (x, 0) = u1(x) for x ∈R,
(9.17)
as a ﬁrst order system.
Proposition 9.7 Let v = ∂u
∂t and w = ∂u
∂x . Problem (9.17) is equivalent to the system
of ﬁrst order PDEs

9.5 Stability via the Fourier Approach
331
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
∂v
∂t −∂w
∂x = 0 in Q,
∂w
∂t −∂v
∂x = 0 in Q,
v(x, 0) = u1(x), w(x, 0) = u′
0(x) for x ∈R,
(9.18)
up to an additive constant.
Proof If u solves problem (9.17), then the ﬁrst equation in (9.18) is just the wave
equation, and the second equation is just
∂2u
∂x∂t =
∂2u
∂t∂x . The initial conditions are
obvious.
Conversely, let (v, w) solve (9.18). Since Q is simply connected, the second
equation implies that there exists u such that v = ∂u
∂t and w = ∂u
∂x . The ﬁrst equation
is the wave equation for u. By the second initial condition, there exists a constant c0
such that u(x, 0) = u0(x) + c0. Hence, u = u −c0 solves (9.17).
□
Remark 9.20 Let us note that, in the case of a bounded interval with Dirichlet con-
ditions, the energy estimate of Corollary 9.1 gives an L2 bound on the variables v
and w, and not directly on u. This explains the choice of these variables for an L2
stability analysis.
□
Weperformasimilaroperationontheﬁnitedifferencescheme.Weusethenotation
xτ = τh for τ ∈R, which agrees with the former notation xn when τ = n ∈Z.
Proposition 9.8 Let
v j
n = u j
n −u j−1
n
k
and w j
n−1/2 = u j
n −u j
n−1
h
,
(9.19)
for n ∈Z and j ∈N∗. If u j is a solution of the ﬁnite difference scheme (9.15), then
(v j, w j) are solution of the ﬁnite difference scheme
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
v j+1
n
−v j
n
k
−
w j
n+1/2 −w j
n−1/2
h
= 0,
w j+1
n−1/2 −w j
n−1/2
k
−v j+1
n
−v j+1
n−1
h
= 0,
(9.20)
with v1
n and w1
n−1/2 given by formula (9.19) in terms of the initial data u0
n and u1
n of
(9.15).
Proof Replacing v j
n = u j
n−u j−1
n
k
and w j
n−1/2 =
u j
n−u j
n−1
h
into (9.20), we see that the sec-
ond relation is satisﬁed by the very deﬁnition of v j
n and w j
n−1
2 , and that the ﬁrst relation
reduces to the original ﬁnite difference scheme. Moreover, the initial conditions are
satisﬁed by construction.
□

332
9
The Wave Equation
Remark 9.21 We note that the scheme (9.20) is explicit. Indeed, assuming that v j
n and
w j
n+1/2 are already known, the ﬁrst relation gives v j+1
n
= v j
n + k
h (w j
n+1/2 −w j
n−1/2)
for all n ∈Z. After that, we see that w j+1
n−1/2 = w j
n−1/2 + k
h (v j+1
n
−v j+1
n−1) for all n ∈Z
by the second relation. Hence, the solution of (9.20) with initial data v1 and w1 exists
and is unique.
□
Remark 9.22 Clearly, v j
n is intended to be an approximation of v(xn, t j) = ∂u
∂t (xn, t j)
and w j
n−1/2 an approximation of w(xn−1/2, t j) = ∂u
∂x (xn−1/2, t j).
In view of this, other initial data are reasonable for (9.20), for example v0
n =
u1(xn) and w0
n−1/2 = u′
0(xn−1
2 ), yielding a different approximation from which an
approximation of u must be reconstructed. These initial conditions directly take into
account the initial conditions of the wave equation.
□
This time, we choose to work in the continuous Fourier transform framework, see
Sect.8.8. We thus introduce the semi-discrete version of the scheme as
⎧
⎪⎪⎨
⎪⎪⎩
v j+1(x) −v j(x)
k
−w j(x + h/2) −w j(x −h/2)
h
= 0,
w j+1(x −h/2) −w j(x −h/2)
k
−v j+1(x) −v j+1(x −h)
h
= 0,
(9.21)
for all x ∈R and with appropriate initial data. Rewriting this in Fourier space, we
obtain
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩

v j+1(ξ) −
v j(ξ)
k
−ei hξ
2 −e−i hξ
2
h

w j(ξ) = 0,
e−i hξ
2

w j+1(ξ) −
w j(ξ)
k
−1 −e−ihξ
h

v j+1(ξ) = 0,
for all ξ ∈R. Writing Y j(x) =
v j(x)
w j(x)

, we obtain

Y j(ξ) = B(ξ)
Y j−1(ξ)
with the ampliﬁcation matrix
B(ξ) =

1
ia(ξ)
ia(ξ) 1 −a(ξ)2

where a(ξ) = 2k
h sin
 hξ
2

.
We see that ampliﬁcation matrices are now complex matrices, we thus need to
generalize the results of Chap.8 to the complex case. First of all, when A is a complex
N × N matrix, its induced matrix norm for the canonical Hermitian norm on CN is
deﬁned as
|||A|||2,h =
sup
X∈CN ,X̸=0
∥AX∥2,h
∥X∥2,h
.

9.5 Stability via the Fourier Approach
333
A complex matrix A is said to be normal if A∗A = AA∗where A∗is the adjoint
matrix. The following results are proved along the same lines as the corresponding
results in the real case.
Proposition 9.9 Let A be a complex N × N matrix. We have
|||A|||2,h =

ρ(A∗A).
In addition, if A is normal then ρ(A) = ρ(A∗A)1/2 = |||A|||2,h.
We now return to the stability of scheme (9.21).
Proposition 9.10 The scheme (9.21) is stable in the sense of von Neumann if and
only if k
h ≤1.
Proof The matrix B(ξ) has the same characteristic polynomial as the matrix A(s)
of Proposition 9.5, therefore the proof is the same.
□
The matrix B(ξ) is not normal in general, thus von Neumann stability is not a
priori sufﬁcient for L2 stability. The following simple matrix result is useful in this
context. Let M ∈M2(C). Every complex matrix is triangularisable, thus we can
write M = PU P−1 with P ∈GL2(C) and U upper-triangular.
Proposition 9.11 For all diagonalizable matrices M ∈M2(C) such that ρ(M) ≤1,
we have
|||M j|||2 ≤|||P|||2|||P−1|||2,
for all j ∈N.
Proof We can write M = PU P−1 with P ∈GL2(C) and U diagonal,
U =
λ1 0
0 λ2

,
where λ1, λ2 are the eigenvalues of M. We have M j = PU j P−1. Therefore
|||M j|||2 ≤|||P|||2|||U j|||2|||P−1|||2.
Now |||U j|||2 = ρ(M) j, which completes the proof.
□
Remark 9.23 The constant |||P|||2|||P−1|||2 appearing in the estimate of M j is nothing
but the condition number (introduced in Remark2.11 of Chap.2) of the change of
basis matrix P.
□
Remark 9.24 If M is not diagonalizable, it has a double eigenvalue λ and we have
U = Λ + N with
Λ =
λ 0
0 λ

,
N =
0 1
0 0

,

334
9
The Wave Equation
by the Jordan decomposition theorem. The matrix N is nilpotent, N 2 = 0, and com-
mutes with Λ. Therefore, by the binomial identity,
U j = Λ j + jλ j−1N,
for all j ∈N. It follows that if ρ(M) < 1,
|||U j|||2 ≤C(ρ(M)),
for all j ∈N, where C is a function of the spectral radius, and if ρ(M) = 1 then
|||M j|||2 →+∞
when j →+∞.
□
Let us now apply Proposition 9.11 to the study of the stability of the scheme (9.21)
by applying the proposition to the matrix M = B(ξ), for any ξ ∈R. We let
B(ξ) = P(ξ)U(ξ)P−1(ξ)
where P(ξ) ∈GL2(C) and U(ξ) ∈M2(C) is upper-triangular for all ξ ∈R. Of
course, all these matrices are also functions of h and k.
Proposition 9.12 Let 0 < λ0 < 1 and assume that k
h ≤λ0. Then, the scheme (9.21)
is stable in L2(R).
Proof In the case a(ξ) = 0, then B(ξ) = I = U(ξ) = P(ξ) and there is nothing to
do. Let us assume that ξ is such that a(ξ) ̸= 0. In this case, Δ(ξ) < 0, there are two
simple eigenvalues and B(ξ) is diagonalizable. We already know that the eigenvalues
of B(ξ) are of modulus 1, so that ρ(B(ξ)) = 1. We have the estimate
|||B j(ξ)|||2 ≤|||P(ξ)|||2|||P−1(ξ)|||2,
by Proposition 9.11. Thus, we only need to bound the condition number of the matrix
P(ξ). Let us note for the record that |a(ξ)| ≤2λ0 < 2 for all ξ.
Computing the eigenvectors of A(ξ), we ﬁnd that the change of basis matrix
P(ξ) =

ia(ξ)
ia(ξ)
λ+(ξ) −1 λ−(ξ) −1

is uniformly bounded since |a(ξ)| < 2. More importantly, since λ±(ξ) −1 =
−a(ξ)2 ± ia(ξ)

4 −a(ξ)2
2
, we clearly have
|||P(ξ)|||2 ≤C|a(ξ)|.

9.5 Stability via the Fourier Approach
335
Moreover,
P(ξ)−1 = det(P(ξ))−1
λ−(ξ) −1 −ia(ξ)
1 −λ+(ξ) ia(ξ)

= det(P(ξ))−1Q(ξ).
We likewise have |||Q(ξ)|||2 ≤C|a(ξ)|. Since
det(P(ξ)) = ia(ξ)(λ−(ξ) −λ+(ξ)) = a(ξ)2
4 −a(ξ)2,
it follows that
| det(P(ξ))| ≥2

1 −λ2
0|a(ξ)|2,
hence
|||P(ξ)−1|||2 ≤C|a(ξ)|−1,
and the result follows.
□
Remark 9.25 We have now established the conditional ℓ2 stability of scheme (9.20).
This means that under the CFL condition, if the initial data v1 and w1 remain in a
bounded set of ℓ2, so do the corresponding solutions v j and w j for j ≤T/k. The
unknowns v and w, which are in a sense quite natural for formulating the wave
equation, are however not the initial unknown u, either continuous or discrete. The
initial values v1 and w1 are supposed to be approximations of ∂u
∂t (x, 0) = u1(x) and
∂u
∂x (x, 0) = u′
0(x) respectively. In this sense, they are independent from each other
as opposed to what happened in the case of the natural scheme, cf. Remark 9.18.
A natural question now is to ask if the ﬁrst order scheme provides some stability
information for the original scheme. We encounter a difﬁculty here. Indeed, if we try
to reconstruct u j from w j, i.e., perform a kind of discrete integration, we see that
u j
n = u j
0 + h
n

l=0
w j
n−l−1/2,
for n ≥0. Now requiring that u j ∈ℓ2(Z) implies that u j
n →0 when n →+∞. So
the partial sums on the right must converge and we must have
u j
n = −h
∞

l=n+1
w j
n−l−1/2,
again for n ≥0. This is not possible in general since w j ∈ℓ2(Z) ̸⊂ℓ1(Z).
What we can say however, is that if we are given u0 ∈ℓ2(Z) and v1 ∈ℓ2(Z), and
if we deﬁne u1 = u0 + kv1 ∈ℓ2(Z) and set w1
n−1/2 =
u1
n−u1
n−1
h
, that is to say if w1 is
the discrete derivative in some sense of an element of ℓ2(Z), then for all j, so is
w j. Indeed, it is simply the discrete derivative of u j obtained by the original scheme

336
9
The Wave Equation
with the above initial conditions, by uniqueness. In this sense, we can say that the
original scheme is stable in ℓ2(Z) for such initial conditions, but with a stability
measured in the norms ∥u j
n−u j−1
n
k
∥2,h + ∥
u j
n−u j
n−1
h
∥2,h, which are more natural in view
of Remark 9.20.
□
Remark 9.26 The proof requires λ0 < 1 to work. Indeed, if k
h = 1, then for ξ = π
h ,
B(ξ) =
 1 2i
2i −3

has the double eigenvalue −1 and is not diagonalizable. Therefore |||B(ξ)T/k|||2 ∼
Ck−1 with C > 0 for k small for this value of ξ and the semi-discrete scheme is
thus unstable in this case. Note that this tells us nothing about the stability of the
discrete scheme when k = h, and this instability occurs even though the scheme is
von Neumann stable.
□
Remark 9.27 If we modify the second relation of the ﬁnite difference scheme as
follows
w j+1
n−1/2 −w j
n−1/2
k
−v j
n −v j
n−1
h
= 0,
which may seem more natural than the scheme above, we get an ampliﬁcation matrix
B(ξ) =

1
ia(ξ)
ia(ξ)
1

.
This matrix is normal and its eigenvalues are 1 ± ia(ξ). They are of modulus strictly
larger than 1 (except when a(ξ) = 0). Hence this scheme is not stable in the sense of
von Neumann, and since the matrix is normal and given the expression of a(ξ), we
see that it is not stable in L2. This shows again that ﬁnite difference schemes must
be chosen with care and that seemingly natural choices may very well fail.
□
Let us now study the stability of the implicit scheme (9.13). The scheme is implicit
and it is not obvious in the ℓ2(Z) context that is even well-deﬁned.
Proposition 9.13 The implicit scheme (9.13) is well-deﬁned with initial data in
ℓ2(Z).
Proof We use the Fourier series argument. For u ∈ℓ2(Z), let T denote the contin-
uous linear operator on ℓ2(Z) deﬁned by
(T u)n = −λun+1 + (1 + 2λ)un −λun−1, n ∈Z,
and λ = k2
h2 . We rewrite the implicit scheme as follows
T u j+1 = v j,

9.5 Stability via the Fourier Approach
337
with v j
n = 2u j
n −u j−1
n
. The question is whether or not the operator T is an isomor-
phism.
Regarding uniqueness, we note that if T u = 0, then u is of the form
un = C1rn
1 + C2rn
2 , n ∈Z,
(9.22)
for some C1 and C2 in C, where r1 and r2 are the roots of the characteristic equation
−λr2 + (1 + 2λ)r −λ = 0. These roots are real, both positive and their product is
1, therefore the only sequence of the form (9.22) that belongs to ℓ2(Z) is such that
C1 = C2 = 0.
We now consider existence. As before, for any v ∈ℓ2(Z; C) we let Fv ∈
L2(0, 2π; C) be deﬁned by Fv(s) = 
n∈Z vneins.
We have
FT u(s) =

n∈Z

−λun+1 + (1 + 2λ)un −λun−1

eins
= −λ

n∈Z
un+1eins + (1 + 2λ)

n∈Z
uneins −λ

n∈Z
un−1eins
=

−λe−is + (1 + 2λ) −λeis
Fu(s)
=

1 + 4λ sin2s
2
		
Fu(s).
Now the function s 	→

1 + 4λ sin2 s
2
−1 is in L∞(0, 2π), therefore
u = F −1

Fv(s)

1 + 4λ sin2 s
2


,
is a solution in ℓ2(Z; C) of T u = v. Of course, by uniqueness, when v is real-valued,
so is u.
□
Remark 9.28 We could not use the semi-discrete version of the scheme in Fourier
space, because the equivalence of this scheme with the discrete scheme for piecewise
constant initial data rests on the existence of the discrete scheme. The Fourier series
approach does not suffer from this drawback.
□
We can now switch to the semi-discrete point of view to study the stability. If we
try to work on the initial formulation of the scheme in Fourier space

u j+1(ξ) −2
u j(ξ) + 
u j−1(ξ)
k2
+ 4
h2 sin2hξ
2
	

u j+1(ξ) = 0,
we encounter the same kind of difﬁculties as with the explicit scheme. Namely, we
obtain an ampliﬁcation matrix

338
9
The Wave Equation
A(ξ) =

2
1+a(ξ)2 −
1
1+a(ξ)2
1
0

.
This matrix is never normal. For ξ = 0, it is the same as in Proposition 9.6, therefore
the semi-discrete scheme is not stable.
Once again, we must change the unknowns and use a ﬁrst order system version
of the scheme in order to be able to conclude. The ﬁrst order scheme is simply
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
v j+1
n
−v j
n
k
−
w j+1
n+1/2 −w j+1
n−1/2
h
= 0,
w j+1
n−1/2 −w j
n−1/2
k
−v j+1
n
−v j+1
n−1
h
= 0.
(9.23)
Writing down the semi-discrete version of this last scheme, we obtain the following
ampliﬁcation matrix
B(ξ) =
1
1 + a(ξ)2

1
ia(ξ)
ia(ξ)
1

.
Now this matrix is normal. Its eigenvalues are λ±(ξ) = 1±ia(ξ)
1+a(ξ)2 , so that
ρ(B(ξ)) =
1

1 + a(ξ)2 ≤1
for all ξ ∈R. We have thus shown
Proposition 9.14 The implicit scheme (9.23) is unconditionally von Neumann stable
and L2 stable.
Let us close this section by saying a few words about the stability of the θ-
scheme (9.14). If we write the semi-discrete version of the scheme, apply the Fourier
transform and rewrite the result in vector form, we obtain an ampliﬁcation matrix
A(ξ) =
−b(ξ) −1
1
0

,
with
b(ξ) = (1 −2θ)a(ξ)2 −2
1 + θa(ξ)2
.
This matrix is not normal. Its eigenvalues are the roots of the polynomial P(X) =
X2 + b(ξ)X + 1. The discriminant reads
Δ(ξ) = a(ξ)2
(1 −4θ)a(ξ)2 −4


1 + θa(ξ)22
.

9.5 Stability via the Fourier Approach
339
If the discriminant is positive for some value of ξ, we thus have two distinct real
roots, the product of which is 1, hence von Neumann instability. If on the other hand,
the discriminant is nonpositive for all ξ, we have two complex conjugate roots of
modulus 1, hence von Neumann stability. Recalling that a(ξ) = 2k
h sin
 hξ
2

, we thus
obtain the following proposition:
Proposition 9.15 The θ-scheme is unconditionally von Neumann stable for θ ≥1
4.
For θ < 1
4, it is von Neumann stable under the condition k
h ≤
1
√1−4θ .
Of course, in terms of L2 stability, we have the exact same problem as before for
ξ = 0, which implies L2 instability of the semi-discrete θ-scheme. We can try to go
around this difﬁculty by using again a system
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎩
v j+1
n
−v j
n
k
−θ
w j+1
n+1/2 −w j+1
n−1/2
h
−(1 −2θ)
w j
n+1/2 −w j
n−1/2
h
−θ
w j−1
n+1/2 −w j−1
n−1/2
h
= 0,
w j+1
n−1/2 −w j
n−1/2
k
−v j+1
n
−v j+1
n−1
h
= 0.
(9.24)
Now on the surface, this scheme appears still to be a two time step scheme, hence
nothing seems to be gained. We can however rewrite it as a one time step scheme
as follows. We ﬁrst apply the Fourier transform to the semi-discrete version of the
scheme
⎧
⎨
⎩

v j+1(ξ) −
v j(ξ) −ia(ξ)

θ 
w j+1(ξ) + (1 −2θ)
w j(ξ) + θ 
w j−1(ξ)

= 0,

w j+1(ξ) −
w j(ξ) −ia(ξ)
v j+1(ξ) = 0.
In addition to a formula for 
w j+1 in terms of 
w j and 
v j+1, the second equation also
yields

w j−1(ξ) = 
w j(ξ) −ia(ξ)
v j(ξ).
We replace these expressions in the ﬁrst equation

v j+1(ξ) −
v j(ξ) −ia(ξ)

θ(
w j(ξ) + ia(ξ)
v j+1(ξ))
+ (1 −2θ)
w j(ξ) + θ(
w j(ξ) −ia(ξ)
v j(ξ))

= 0,
or
(1 + θa(ξ)2)

v j+1(ξ) −
v j(ξ)

−ia(ξ)
w j(ξ) = 0.

340
9
The Wave Equation
This scheme thus corresponds to the ampliﬁcation matrix
B(ξ) =

1
iaθ(ξ)
ia(ξ) 1 −a(ξ)aθ(ξ)

,
with
aθ(ξ) =
a(ξ)
1 + θa(ξ)2 .
This matrix is not normal and has the same eigenvalues as the previous one, hence the
same von Neumann stability. The case a(ξ) = 0 is not a problem anymore however,
since the matrix is then the identity matrix.
Proposition 9.16 The semi-discrete version of the θ-scheme (9.24) is stable in L2
for θ ≥1
4 under the condition k
h ≤M, for any given M. For θ < 1
4, given any
0 < λ0 <
1
√1−4θ , it is L2 stable under the condition k
h ≤λ0.
Proof Let us consider the case θ ≥1
4. First of all, at ξ = 2mπ
h , m ∈Z, we have
a(ξ) = 0 so that nothing needs to be done for these values of ξ, as was already
mentioned. For the other values of ξ, the matrix B(ξ) is diagonalizable with two
distinct, complex conjugate eigenvalues of modulus one, therefore no problem for
the diagonal part either. We have the change of basis matrix
P(ξ) =

−1
2

4aθ(ξ)
a(ξ) −aθ(ξ)2 + iaθ(ξ)
	
1
2

4aθ(ξ)
a(ξ) −aθ(ξ)2 −iaθ(ξ)
	
1
1

,
with 4aθ(ξ)
a(ξ) −aθ(ξ)2 ≥0 since θ ≥1
4.
After a little bit of computer algebra aided manipulations, we obtain the following
value for the condition number of P(ξ):
cond2(P(ξ)) = sign (a)(a + b) +

a2b2 + (a −b)2
√
4ab −a2b2
,
where a = a(ξ) and b = aθ(ξ) for brevity. Replacing b by its value as a function of
a, we obtain
cond2(P(ξ)) = 2 + θa(ξ)2 + |a(ξ)|

1 + θ2a(ξ)2

(4θ −1)a(ξ)2 + 4
≤1 + |a(ξ)|
2
+ a(ξ)2
2
≤1 + M + 2M2,
hence the stability of the scheme. We leave the case θ < 1
4 as an exercise.
□

9.5 Stability via the Fourier Approach
341
Remark 9.29 Proposition 9.16 in the case θ ≥1
4 is a bit of a disappointment. Indeed,
in that case, the scheme is unconditionally von Neumann stable and we only obtain
actual L2 stability under the condition k
h ≤M with M arbitrary. Now in practice,
neither k nor h actually go to 0, and such a condition as k
h ≤M with M arbitrary is
not discernible from unconditional stability.
□
Remark 9.30 Instead of using the Jordan decomposition of B(ξ), we could think
of using the Schur decomposition of B(ξ), B(ξ) = U(ξ)T (ξ)U(ξ)∗, where U(ξ)
is unitary and T (ξ) is upper triangular. The advantage of the Schur decomposition
over the Jordan decomposition in this context, is that |||B(ξ) j|||2 = |||T(ξ) j|||2 for all
j and we lose no information by passing from B(ξ) to T (ξ). Moreover, T (ξ) j is
fairly easy to express explicitly. The disadvantage is that the expression of the upper
right entry of T (ξ) j is even less user-friendly than cond2(P(ξ)) when it comes to
estimating it. We do not pursue this direction here.
□
9.6
For a Few Schemes More
In the previous section, we rewrote the wave equation as the ﬁrst order system (9.18).
This system is of the form
∂U
∂t + ∂( f (U))
∂x
= 0
with
U(x, t) =
v(x, t)
w(x, t)

and f (U) =
 0 −1
−1 0

U.
When U is Rp-valued and f is a general nonlinear function from Rp to Rp, satisfy-
ing certain conditions, this is a (nonlinear) hyperbolic system. Such systems are of
paramount importance in many applications, for example in gas dynamics, and there
is a very large body of numerical schemes that are adapted to the approximation of
the solutions of such systems, see [42] for example.
We present a few of these schemes in the case of our simple R2-valued, linear
hyperbolic system (9.18). We will also return to some of these schemes in the next
chapter. Again, we work on the whole line3 and on the usual (nh, jk) space-time ﬁnite
difference grid for the approximations. We start with the Lax–Friedrichs scheme,
which reads in general
U j+1
n
−1
2(U j
n+1 + U j
n−1)
k
+ f (U j
n+1) −f (U j
n−1)
2h
= 0,
3Boundary conditions are a delicate question for such systems.

342
9
The Wave Equation
and in our particular case
⎧
⎪⎨
⎪⎩
v j+1
n
= 1
2(v j
n+1 + v j
n−1) + k
2h (w j
n+1 −w j
n−1),
w j+1
n
= 1
2(w j
n+1 + w j
n−1) + k
2h (v j
n+1 −v j
n−1).
The scheme is one-step, of order one and explicit. We write the usual semi-discrete
version of the scheme, then apply the Fourier transform, and we obtain
⎧
⎪⎨
⎪⎩

v j+1(ξ) = cos(hξ)
v j(ξ) + i k
h sin(hξ)
w j(ξ),

w j+1(ξ) = cos(hξ)
w j(ξ) + i k
h sin(hξ)
v j(ξ).
Therefore, the ampliﬁcation matrix of the Lax–Friedrichs scheme is
B(ξ) =
 cos(hξ) i k
h sin(hξ)
i k
h sin(hξ)
cos(hξ)

.
This matrix is normal and its spectral radius is ρ(B(ξ)) =

cos2(hξ) +
k2
h2 sin2(hξ)
1/2. It clearly follows that
Proposition 9.17 The Lax–Friedrichs scheme is von Neumann stable and L2 stable
under the condition k
h ≤1.
We consider next the Lax–Wendroff scheme. In our particular case, the scheme
reads
⎧
⎪⎪⎨
⎪⎪⎩
v j+1
n
= v j
n + k
2h (w j
n+1 −w j
n−1) + k2
2h2 (v j
n+1 −2v j
n + v j
n−1),
w j+1
n
= w j
n + k
2h (v j
n+1 −v j
n−1) + k2
2h2 (w j
n+1 −2w j
n + w j
n−1).
The scheme is one-step, of order two and explicit. After Fourier transform, it becomes
⎧
⎪⎪⎨
⎪⎪⎩

v j+1(ξ) =

1 −2k2
h2 sin2hξ
2
		

v j(ξ) + i k
h sin(hξ)
w j(ξ),

w j+1(ξ) =

1 −2k2
h2 sin2hξ
2
		

w j(ξ) + i k
h sin(hξ)
v j(ξ),
hence the ampliﬁcation matrix
B(ξ) =

1 −2k2
h2 sin2 hξ
2

i k
h sin(hξ)
i k
h sin(hξ)

1 −2k2
h2 sin2 hξ
2


.

9.6 For a Few Schemes More
343
Thismatrixisagainnormalanditfollowsfromelementarycomputationsthatithas
a spectral radius ρ(B(ξ)) =

1 −4 k2
h2 (1 −k2
h2 ) sin4 hξ
2
1/2. Therefore, we see that
Proposition 9.18 The Lax–Wendroff scheme is von Neumann stable and L2 stable
under the condition k
h ≤1.
We can also revisit the leapfrog scheme, which reads here
⎧
⎪⎨
⎪⎩
v j+1
n
= v j−1
n
+ k
h (w j
n+1 −w j
n−1),
w j+1
n
= w j−1
n
+ k
h (v j
n+1 −v j
n−1).
Note that it leapfrogs in time as well as in space. The scheme is two-step, of order two
and explicit. To write an ampliﬁcation matrix for it, we need to double the dimension
and consider for example the vectors

v j+1(ξ), 
w j+1(ξ), 
v j(ξ), 
w j(ξ)
T , a choice
which yields the ampliﬁcation matrix
B(ξ) =
⎛
⎜⎜⎝
0
2i k
h sin(hξ)
1
0
2i k
h sin(hξ)
0
0
1
1
0
0
0
0
1
0
0
⎞
⎟⎟⎠.
This matrix is not normal.
Proposition 9.19 The leapfrog scheme is von Neumann stable under the condition
k
h ≤1. It is L2 stable under the condition k
h ≤λ0 < 1.
Proof Let b(ξ) = k
h sin(hξ). If we write B(ξ) as a 2 × 2 block matrix of four 2 × 2
blocks, we see by Lemma8.2 of Chap.8 that its eigenvalues are given by
λ = ±
√
1 −b2 ± ib when k
h ≤1, so that ρ(B(ξ)) = 1 for all ξ.
Concerning the L2-stability, if k
h ≤λ0 < 1, we have four distinct eigenvalues
of modulus 1, so we only need to estimate the condition number of the change of
matrix basis
P(ξ) =
⎛
⎜⎜⎝
1
1
1
1
1
1
−1
−1
√
1 −b2 −ib
−
√
1 −b2 −ib
√
1 −b2 + ib
−
√
1 −b2 + ib
√
1 −b2 −ib
−
√
1 −b2 −ib −
√
1 −b2 −ib
√
1 −b2 −ib
⎞
⎟⎟⎠.
Using computer algebra again, we ﬁnd that
cond2(P(ξ)) =
&
1 + |b(ξ)|
1 −|b(ξ)| ≤
&
2
1 −λ0
,
hence the stability of the scheme.
□

344
9
The Wave Equation
Remark 9.31 Theconditionalstabilityoftheleapfrogschemeistobecontrastedwith
the situation for the heat equation, where the leapfrog scheme is always unstable,
see Proposition8.10 in Chap.8.
□
Remark 9.32 We note that we cannot allow λ0 = 1. Indeed, in this case, there are
values of ξ for which b(ξ) = ±1. For these values of ξ, the matrix B(ξ) has two
double eigenvalues ±i and is not diagonalizable. Therefore |||B(ξ) j|||2 →+∞as
j →+∞, and the scheme is unstable in L2.
□
9.7
Concluding Remarks
To conclude this chapter, we note that there are other issues than just consistency
and stability in the study of ﬁnite difference schemes for hyperbolic systems. Even
though we have not mentioned them at all here, they are important in assessing
the performance of a given scheme. Among these issues are dissipativity, i.e., the
possible damping of wave amplitudes with time, and dispersivity, i.e., the possibility
that numerically approximated waves of different frequencies could travel at different
numerical speeds.
Naturally, there are other numerical methods applicable to the wave equation, for
instance ﬁnite difference-ﬁnite element methods, see for example [65].
We have so far described and analyzed two major classes of numerical methods,
ﬁnite difference methods and ﬁnite element methods, in the contexts of the three
main classes of problems, elliptic, parabolic and hyperbolic. In the last chapter, we
introduce a more recent method, the ﬁnite volume method, on a few elliptic and
hyperbolic examples.

Chapter 10
The Finite Volume Method
The ﬁnite volume method is a more recent method than both ﬁnite difference and
ﬁnite element methods. It is widely used in practice for example in ﬂuid dynamics
computations. We present the method in the simplest possible settings, ﬁrst for one-
dimensional elliptic problems, then for the transport equation in one and two dimen-
sions. For the latter, we return to the method of characteristics already introduced
in Chap.1, to solve one-dimensional nonlinear problems as well as two-dimensional
linear problems.
10.1
The Elliptic Case in One Dimension
Let us consider the elliptic problem −u′′ = f in ]0, 1[, with homogeneous Dirichlet
boundary conditions u(0) = u(1) = 0. The idea of the ﬁnite volume discretization is
tosubdividethedomainintoaﬁnitenumberofsubsetscalledcells,orcontrolvolumes
especially in higher dimension, which are disjoint up to a set of zero measure, and
integrate the equation on each cell. The resulting integrals are then approximated
by computable quantities. The main difference with all previous methods is that the
discrete unknowns to be introduced will be meant to be approximations of the mean
value of the solution on each cell. In a sense, the ﬁnite volume approximation is an
approximation by piecewise constant functions.
It may seem strange to approximate an a priori smooth function, say of class C2 if
f is continuous, that solves a problem involving derivatives, by piecewise constant
functions. In fact, the method does not approximate the differential equation itself
in one form or another, but starts from the integration of the differential equation
over each cell, followed by approximation. So this is a quite different philosophy
compared to both ﬁnite difference and ﬁnite element methods which work on the
PDE itself expressed in different forms.
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8_10
345

346
10
The Finite Volume Method
Fig. 10.1 Finite volume cells in 1D
More precisely, let N ∈N∗be given. In the one-dimensional case, cells are just
going to be subintervals Cn, n = 1, . . . , N. We let xn denote the middle of Cn and
xn−1/2 < xn+1/2 denote its extremities so that Cn = [xn−1/2, xn+1/2]. The local cell
size is hn = xn+1/2 −xn−1/2. It is not a priori a constant, see Fig.10.1.
We also set h0 = h1, hN+1 = hN, x0 = −x1 and xN+1 = 2 −xN. For all
n = 0, . . . , N, we let hn+1/2 = hn+1+hn
2
= xn+1 −xn be the distance between the
centers of two consecutive cells. If we integrate the equation −u′′ = f on Cn, we
obtain
−u′
xn+ 1
2

+ u′
xn−1
2

=

Cn
f (x) dx,
(10.1)
for n = 1, . . . , N. We use the following approximations for the derivatives
u′
xn+ 1
2

≈
1
hn+1

Cn+1 u(x) dx −1
hn

Cn u(x) dx
hn+ 1
2
,
(10.2)
for n = 1, . . . , N −1,
u′
x 1
2

≈
1
h1

C1 u(x) dx
h 1
2
,
(10.3)
and
u′
xN+ 1
2

≈−
1
hN

CN u(x) dx
hN+ 1
2
.
(10.4)
Indeed, assuming that u is of class C2, a Taylor–Lagrange expansion shows that
u(x) = u

xn+ 1
2

+

x −xn+ 1
2

u′
xn+ 1
2

+ O(h2),

10.1 The Elliptic Case in One Dimension
347
where h = maxn hn. Therefore, if n = 1, . . . , N −1
1
hn+1

Cn+1
u(x) dx = u

xn+ 1
2

+ hn+1
2
u′
xn+ 1
2

+ O(h2),
1
hn

Cn
u(x) dx = u

xn+ 1
2

−hn
2 u′
xn+ 1
2

+ O(h2),
(10.5)
and thus

1
hn+1

Cn+1 u(x) dx −1
hn

Cn u(x) dx
hn+ 1
2
−u′
xn+ 1
2

 ≤Ch,
(10.6)
where the constant C only depends on u. For n = 0, we have by the same token
1
h1

C1
u(x) dx = u

x 1
2

+ h1
2 u′
x 1
2

+ O(h2) = h1
2 u′
x 1
2

+ O(h2),
due to the Dirichlet boundary condition, so that

1
h1

C1 u(x) dx
h 1
2
−u′
x 1
2

 ≤Ch,
and likewise for n = N. We have thus shown that
Proposition 10.1 The ﬁnite volume approximation (10.2)–(10.4) of ﬁrst derivatives
is consistent of order 1.
Following a process that is now familiar, we introduce discrete unknowns un,
n = 1, . . . , N, to take the place of the average value of the exact solution on cell Cn,
1
hn

Cn u(x) dx, and replace it in an analogue of Eq.(10.1). We ﬁrst let
Fn+ 1
2 = −un+1 −un
hn+ 1
2
for 2 ≤n ≤N −1,
and
F1
2 = −u1
h 1
2
, FN+ 1
2 =
uN
hN+ 1
2
,
where the latter two relations take the Dirichlet condition into account at the discrete
level. The quantities Fn+1/2 are called the numerical ﬂuxes, since they are meant to
approximate the actual ﬂuxes −u′(xn+1/2). This way, we obtain the ﬁnite volume
scheme
Fn+ 1
2 −Fn−1
2 = fn, n = 1, . . . , N,
(10.7)

348
10
The Finite Volume Method
with
fn =

Cn
f (x) dx.
(10.8)
Rewriting the ﬁnite volume scheme line by line, we obtain for n = 1
 1
h 1
2
+ 1
h 3
2

u1 −1
h 3
2
u2 = f1,
for 2 ≤n ≤N −1,
−
1
hn−1
2
un−1 +

1
hn−1
2
+
1
hn+ 1
2

un −
1
hn+ 1
2
un+1 = fn,
and for n = N,
−
1
hN−1
2
uN−1 +

1
hN−1
2
+
1
hN+ 1
2

uN = fN.
Thus we see that the scheme assumes the form of a linear system
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
h 1
2
+
1
h 3
2
−1
h 3
2
· · ·
· · ·
0
−1
h 3
2
1
h 3
2
+
1
h 5
2
−1
h 5
2
· · ·
0
...
...
...
...
...
0
· · ·
−
1
hN−3
2
1
hN−3
2
+
1
hN−1
2
−
1
hN−1
2
0
· · ·
· · ·
−
1
hN−1
2
1
hN−1
2
+
1
hN+ 1
2
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎝
u1
u2
...
uN
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
f1
f2
...
fN
⎞
⎟⎟⎟⎠,
(10.9)
or in short AhUh = Fh. The matrix Ah of the above linear system bears a strong
resemblance to the matrix by the same name involved in the ﬁnite difference method
in Chap.2. Before we discuss the well-posedness and eventual convergence of the
ﬁnite volume method, let us see that the method is nonetheless fundamentally dif-
ferent from the ﬁnite difference method, in spite of appearances.
Let us thus consider for an instant the linear system (10.9) as stemming from a
ﬁnite difference scheme, that is to say assume that un ≈u(xn). Then the approxi-
mation in the left-hand side of Eq.(10.7) divided by hn, so that the right-hand side
becomes a consistent approximation of f (xn), is not a consistent approximation of
−u′′(xn). Indeed, if we compute the truncation error in this context, we ﬁnd
hnεh(u)n = −
1
hn−1
2
u(xn−1)+

1
hn−1
2
+
1
hn+ 1
2

u(xn)−
1
hn+ 1
2
u(xn+1)−

Cn
f (x) dx,

10.1 The Elliptic Case in One Dimension
349
so that using Taylor–Lagrange expansions, we obtain
εh(u)n =

1 −
hn−1
2 + hn+ 1
2
2hn

u′′(xn) + O(h).
Now, if we are given a sequence of ﬁnite volume meshes indexed by h and such
that h →0, there is no reason in general for hn−1/2+hn+1/2
2hn
to tend to 1, except in very
special cases. Actually, it is easy to construct a sequence that does not exhibit such
a convergence. Therefore, the above truncation error does not tend to 0 in general.
This shows that the ﬁnite volume scheme is quite different from a ﬁnite difference
scheme.
We now return to the study of the ﬁnite volume scheme (10.7)–(10.8). First of all,
the scheme is well-posed.
Proposition 10.2 The matrix Ah is invertible.
Proof We show that the matrix Ah is actually positive deﬁnite, which implies that it
is invertible. Given X ∈RN, we have
X T Ah X = X2
1
h 1
2
+
N−1

n=1
(Xn+1 −Xn)2
hn+ 1
2
+
X2
N
hN+ 1
2
,
from which the result easily follows.
□
Let us now show the convergence of the scheme when h →0.
Proposition 10.3 Assume that f ∈C0([0, 1]). There exists a constant C that does
not depend on h such that
max
1≤i≤N
un −1
hn

Cn
u(x) dx
 ≤Ch.
Proof We let en = un −1
hn

Cn u(x) dx for n = 1, . . . , N and e0 = eN+1 = 0. Let
¯Fn+1/2 = −u′(xn+1/2) denote the actual ﬂuxes. We have by Eq.(10.1)
¯Fn+ 1
2 −¯Fn−1
2 = fn, n = 1, . . . , N,
so that taking Eq.(10.7) into account, it follows that
Dn+ 1
2 −Dn−1
2 = 0, n = 1, . . . , N,
(10.10)
where Dn+1/2 = Fn+1/2 −¯Fn+1/2.

350
10
The Finite Volume Method
Since f is continuous, it follows that u is of class C2, hence estimate (10.6) applies
and we can write
−
1
hn+1

Cn+1 u(x) dx −1
hn

Cn u(x) dx
hn+ 1
2
= ¯Fn+ 1
2 + dn+ 1
2 ,
with
dn+1/2
 ≤Ch, with the obvious modiﬁcations for n = 0 and n = N. Therefore
Dn+ 1
2 = −en+1 −en
hn+ 1
2
+ dn+ 1
2 ,
and the constancy (10.10) of Dn+1/2 implies that
en −en−1
hn−1
2
−dn−1
2 −en+1 −en
hn+ 1
2
+ dn+ 1
2 = 0.
We multiply the previous relation by en and sum for n = 1 to N. This yields
N

n=1
en
en −en−1
hn−1
2
−
N

n=1
en
en+1 −en
hn+ 1
2
= −
N

n=1
dn+ 1
2 en +
N

n=1
dn−1
2 en.
Taking into account the fact that e0 = eN+1 = 0 and reordering the terms, we obtain
N

n=0
(en+1 −en)2
hn+ 1
2
= −
N

n=0
dn+ 1
2 (en −en+1) ≤Ch
N

n=0
en −en+1
.
By the Cauchy–Schwarz inequality, the right-hand side is estimated by
N

n=0
en −en+1
 ≤
 N

n=0
(en+1 −en)2
hn+ 1
2
1/2 N

n=0
hn+ 1
2
1/2
.
Now of course N
n=0 hn+1/2 = 1 and combining the above estimates, we obtain
N

n=0
(en+1 −en)2
hn+ 1
2
≤C2h2.
It follows immediately from the last two estimates that
N

n=0
|en+1 −en| ≤Ch.

10.1 The Elliptic Case in One Dimension
351
To conclude, we notice that en = n−1
j=0(e j+1 −e j), for n = 1, . . . , N, so that
|en| ≤
n−1

j=0
|e j+1 −e j| ≤
N

j=0
|e j+1 −e j| ≤Ch,
by the previous estimate.
□
Theaboveproofshowsthatun iscloseto 1
hn

Cn u(x) dx whenh issmall,uniformly
with respect to n. We can easily infer a ﬁnite difference-like convergence result as
follows.
Corollary 10.1 Under the previous hypotheses, we have
max
1≤i≤N |un −u(xn)| ≤Ch,
where C does not depend on h.
Proof Indeed, for x ∈Cn, we can write u(x) = u(xn) + (x −xn)u′(ξ) for some
ξ ∈Cn. Therefore |u(x) −u(xn)| ≤h maxx∈[0,1] |u′|. Integrating this inequality on
Cn, we obtain
 1
hn

Cn
u(x) dx −u(xn)
 ≤1
hn

Cn
|u(x) −u(xn)| dx ≤h max
x∈[0,1] |u′|,
and the Corollary follows.
□
Remark 10.1 If u is C4, it is possible to obtain a better right-hand side of the error
estimate of Corollary 10.1, namely Ch2.
□
Remark 10.2 Proposition 10.3 still holds true if f is only supposed to be in L1, see
[36].
□
10.2
The Transport Equation in One Dimension
Let us ﬁrst expand a little on the presentation of the theory of the transport equation
of Chap.1, Sect.1.4, before describing the ﬁnite volume method in this context.
The Cauchy problem for the transport, or advection, equation reads
⎧
⎨
⎩
∂u
∂t (x, t) + a ∂u
∂x (x, t) = 0, x ∈R, t > 0,
u(x, 0) = u0(x), x ∈R,
(10.11)
where u(x, t) is the unknown density of some quantity at point x and time t, a
quantity per unit length, and a is a given constant that represents the propagation or

352
10
The Finite Volume Method
advection speed of u. In this interpretation, q(x, t) = au(x, t) represents the ﬂux of
the quantity under consideration passing through x at time t, i.e., a quantity per unit
of time. The initial data u0 is given. We have seen in Chap.1 that when u0 is regular,
there is a unique regular solution given by
u(x, t) = u0(x −at).
(10.12)
As a matter of fact, a crucial property of the transport equation is that the solution
u is constant along the characteristics, i.e., the integral curves of the vector ﬁeld
a (admittedly, a one-dimensional vector ﬁeld in our present case). By deﬁnition, a
characteristic is a curve t →X(t) which satisﬁes the ordinary differential equation
d X
dt (t) = a.
(10.13)
We see that when plotted in space–time, all characteristics, i.e., the curves t →
(X(t), t),1 are straight lines that are parallel to each other. Moreover, given any
(y, s) ∈R2, there is one and only one characteristic passing through point y at time
s, X(t; y, s) = y +a(t −s), t ∈R. Note that if x = X(t; y, s), then y = X(s; x, t),
a relation which holds true for more general right-hand sides in Eq.(10.13), of the
form a(X(t), t). With this notation, the solution of the transport equation (10.12)
becomes
u(x, t) = u0(X(0; x, t)).
The initial data is simply propagated on the characteristics, at constant speed a,
to the right if a > 0, to the left if a < 0, and it is stationary if a = 0. To ﬁnd the
value of the solution u at (x, t), it is enough to look at the characteristic passing
through x at time t, and pick the value of u0 at the point x0 = X(0; x, t) where the
characteristic was at time 0, see Fig.1.10 in Chap.1 where the characteristics are
drawn in a space–time diagram.
The deﬁnition of characteristics can be extended to the case of transport equations
with non constant speed, i.e., a is a given function of x and t,
∂u
∂t (x, t) + a(x, t)∂u
∂x u(x, t) = 0.
The characteristics are again deﬁned by
⎧
⎨
⎩
d
dt X(t; y, s) = a(X(t; y, s), t),
X(s; y, s) = y.
(10.14)
1We call both curves characteristics, even though they do not live in the same space, and switch
between the two meanings without notice.

10.2 The Transport Equation in One Dimension
353
The mapping t →X(t; y, s) is the characteristic going through y at time s. The
characteristics no longer are straight lines in space–time. Local existence of such
curves is ensured by the Picard–Lindelöf or Cauchy–Lipschitz theorem, as soon as
a is sufﬁciently regular (continuous with respect to (x, t), Lipschitz with respect the
space variable x uniformly with respect to t). It is easily seen that the solution u is
still constant on each characteristic. Moreover, due to Cauchy–Lipschitz uniqueness,
different characteristics do not intersect and therefore u(x, t) = u0(X(0; x, t)) as
before, as long as the characteristics exist.
The method of characteristics also makes it possible to consider the case when
there is a source term in the right-hand side, i.e., an equation of the form
∂u
∂t (x, t) + a(x, t)∂u
∂x u(x, t) = f (x, t),
where f is a given function. Integrating this equation along the characteristics, we
obtain
u(x, t) = u0(X(0; x, t)) +
 t
0
f (X(τ; x, t), τ) dτ,
(10.15)
see Proposition 10.13 below for a proof in any dimension.
We have already mentioned nonlinear hyperbolic systems in Chap.9. In the one-
dimensional case, such systems become nonlinear transport equations,
⎧
⎨
⎩
∂u
∂t (x, t) + ∂( f (u))
∂x
(x, t) = 0, x ∈R, t > 0
u(x, 0) = u0(x), x ∈R,
(10.16)
where f is a given function from R to R, called the ﬂux function. The linear case
corresponds to ﬂux functions of the form f (u) = au with a ∈R, and we recover
Eq.(10.11). The simplest nonlinear example is the classical Burgers equation, which
corresponds to f (u) = u2
2 .
The method of characteristics also applies to nonlinear transport equations. In the
nonlinear case, the speed of propagation depends on the solution u, which makes
them much more complicated than linear transport equations. Indeed, if u is a solution
of class C1 of (10.16), we can write
∂u
∂t + f ′(u)∂u
∂x = 0.
For instance, regular solutions of the Burgers equation satisfy
∂u
∂t (x, t) + u(x, t)∂u
∂x (x, t) = 0.

354
10
The Finite Volume Method
In this particular case, we see that when u > 0, the propagation goes to the right with
a speed that increases with u. If we imagine an initial wave proﬁle u0 with a crest to
the left of a trough, the crest will ride faster than the trough and eventually catch up
with it, which leads to serious difﬁculties.
In the general case, the propagation speed is thus f ′(u), which is not known a pri-
ori. In the present context, the characteristics are deﬁned by the ordinary differential
equation
d X
dt (t) = f ′(u(X(t), t)).
It is not too difﬁcult to show that, as long as it remains regular, the solution u is
constant along the characteristics and that the characteristics are therefore straight
lines again. However, the speed of propagation along such a line, i.e., its slope in
space–time, depends on the value of u on this particular characteristic, which in
turn is equal to the value of u0 at the point where the characteristic was at time
0. Consequently, the characteristics are in general no longer parallel to each other.
Difﬁculties arise when these lines cross, see the crest-trough discussion above. We are
not going to study this problem, which involves so-called weak solutions, or solutions
in the distributional sense, and shock waves, any further here. Nevertheless, as long
as the characteristics do not intersect each other, the regular solution is still given by
the same formula
u(x, t) = u0(X(0; x, t)).
10.3
Finite Volumes for the Transport Equation
We now describe the ﬁnite volume method for the transport equation (10.11) in the
linear case with a constant for simplicity. We ﬁrst cover the whole space R by cells
Cn =

xn−1/2, xn+1/2

, with xn−1/2 < xn+1/2, n ∈Z, and we look for a piecewise
constant approximation of the solution u, which is constant on each cell Cn, at some
discretized instants.
We set hn = xn+1/2 −xn−1/2. As in the elliptic case, hn is a local cell size, it is
not necessarily a constant. Let xn denote the middle of Cn. We introduce the mean
value ¯un(t) of u(·, t) on cell Cn, i.e.,
¯un(t) = 1
hn

Cn
u(x, t) dx.
In a ﬁrst step, we want to approximate ¯un(t) by a quantity denoted by un(t) ∈R
for each t and n. Time will be discretized later on. We ﬁrst integrate the equation on
each cell Cn

Cn
∂u
∂t + ∂q
∂x

(x, t) dx = 0,

10.3 Finite Volumes for the Transport Equation
355
where q = au is the ﬂux, then compute both terms. The ﬁrst term is

Cn
∂u
∂t (x, t) dx = d
dt

Cn
u(x, t) dx

= hn
d ¯un
dt (t)
by differentiation under the integral sign, and the second term reads

Cn
∂q
∂x (x, t) dx = q

xn+ 1
2 , t

−q

xn−1
2 , t

,
by direct integration, so that we have
hn
d ¯un
dt (t) + q

xn+ 1
2 , t

−q

xn−1
2 , t

= 0.
So far, there is no approximation, the above relation is an exact consequence of the
transport equation.
Sincethesemi-discreteunknownsun(t)wearelookingforaresupposedtoapprox-
imate the mean values ¯un(t), and not the values of u at the interfaces between cells, we
need an approximation of the ﬂuxes q

xn+1/2, t

= au

xn+1/2, t

that is expressed
only in terms of the semi-discrete unknowns. This is achieved by means of an approx-
imated ﬂux or numerical ﬂux, a function g in two variables which is used to compute
this approximation as follows
gn+ 1
2 (t) = g(un(t), un+1(t)) ≈q

xn+ 1
2 , t

.
Of course, the numerical ﬂux g must be chosen in such a way that the above approx-
imation makes reasonable sense. This yields a semi-discrete scheme
hn
dun
dt (t) + g(un(t), un+1(t)) −g(un−1(t), un(t)) = 0,
(10.17)
for t > 0 and n ∈Z, which is an inﬁnite system of ordinary differential equations.
The properties of the scheme partly result from the choice of the numerical ﬂux
g. We will give a few examples below. If un(t) = un+1(t) for some n, and t, there
is one natural ﬂux between the two cells, namely aun(t) = aun+1(t), which should
naturally be retained. This requirement is fulﬁlled as soon as
∀v ∈R, g(v, v) = av.
(10.18)
From now on, we will make the above assumption on the numerical ﬂux, which is
related to consistency, see Proposition 10.8 below.
We now consider the discretization with respect to time. We use a classical method
of approximation for ordinary differential equations, for example the Euler explicit
scheme. Let M ∈N be an integer and k =
T
M+1 be the time step. We let t j = jk,

356
10
The Finite Volume Method
0 ≤j ≤M + 1. We denote by u j
n the approximation of ¯u j
n =
1
hn

Cn u(x, t j)dx that
we want to compute. Discretizing system (10.17) in time accordingly, we thus obtain
the following explicit scheme:
hn
u j+1
n
−u j
n
k
+ g(u j
n, u j
n+1) −g(u j
n−1, u j
n) = 0,
(10.19)
for j ≤M. Starting from an initial data u0
n given by
u0
n = 1
hn

Cn
u0(x) dx = ¯un(0),
(10.20)
we can thus compute u j
n, n ∈Z for all j ≤M + 1. Other ordinary differential
equation schemes for the discretization with respect to the time variable can also be
used, for example the implicit Euler scheme, the leapfrog scheme, or any Runge–
Kutta scheme. Of course, in practical computations, the set of indices n involved
must somehow be restricted to a ﬁnite set, but we do not pursue this here.
Let us discuss the case of the upwind scheme, a commonly used scheme. In all
the sequel, we assume that h = supn∈Z hn < +∞, infn∈Z hn > 0 and that u is
sufﬁciently regular.
We start from scheme (10.19)–(10.20). We just have to specify the choice of the
numerical ﬂux g, which is used to approximate au(xn+1/2, t j). The simplest choice
consists inapproximatingu(xn+1/2, t j) byoneof theneighboringapproximatedmean
values u j
n or u j
n+1. In the case a > 0, the upwind scheme corresponds to the choice
u j
n on the left and in the case a < 0, to the choice u j
n+1 on the right.
We thus set, depending on the sign of a,
g(u j
n, u j
n+1) = au j
n if a > 0, or g(u j
n, u j
n+1) = au j
n+1 if a < 0.
The case a = 0 is not interesting. This gives the upwind scheme
u j+1
n
−u j
n
k
+ a u j
n −u j
n−1
hn
= 0,
(10.21)
when a > 0 and
u j+1
n
−u j
n
k
+ a u j
n+1 −u j
n
hn
= 0,
(10.22)
when a < 0. In both cases, we enforce the initial condition (10.20). Both choices
satisfy the consistency condition (10.18). The scheme is called upwind, in the sense
that we take into account the direction of the wind, i.e., the exact ﬂux coming from
the left when a > 0 and from the right when a < 0.

10.3 Finite Volumes for the Transport Equation
357
Without loss of generality, we focus on the case a > 0 and consider the upwind
scheme in the form (10.21). We ﬁrst remark that, in the general case of a non constant
cell size, this ﬁnite volume upwind scheme differs from the corresponding ﬁnite
difference scheme. In fact, the ﬁnite difference upwind scheme reads
v j+1
n
−v j
n
k
+ a v j
n −v j
n−1
hn−1
2
= 0,
(10.23)
v0
n = u0(xn),
(10.24)
where hn−1/2 = xn −xn−1 = (hn + hn−1)/2 and v j
n represents an approximation of
u(xn, t j). There are two differences between the ﬁnite volume and ﬁnite difference
schemes. The ﬁrst difference is that the factor hn in (10.21) is replaced by hn−1/2
in (10.23) in order to write a difference quotient. The second difference lies in the
initial conditions, (10.20) versus (10.24). However, in the case of a constant cell size,
i.e., if hn = h for all n ∈Z, then the only difference comes from the discretization
of the initial condition. Moreover, as in the elliptic case, we can remark that the ratio
u j
n−u j
n−1
hn
is not a consistent approximation, in the ﬁnite difference sense, of ∂u
∂x (xn, t j).
Indeed, if we perform the usual Taylor expansions, we obtain
u(xn−1, t j) = u(xn, t j) −hn−1
2
∂u
∂x (xn, t j) + O(h2),
so that
u(xn, t j) −u(xn−1, t j)
hn
=
hn−1
2
hn
∂u
∂x (xn, t j) + O(h),
and hn−1/2/hn ̸→1 in general.
Let us now study the convergence of the ﬁnite volume scheme (10.21)–(10.20).
We ﬁrst have the following ﬁnite difference-like convergence result.
Proposition 10.4 We suppose that u is sufﬁciently regular and that there exists a
constant C ≥0 such that
sup
x∈R,t∈R+
∂u
∂x (x, t)
 ≤C,
sup
x∈R,t∈R+
∂2u
∂x2 (x, t)
 ≤C,
sup
x∈R,t∈R+
∂2u
∂t2 (x, t)
 ≤C.
Then, under the condition
a
k
infn∈Z hn
≤1,
(10.25)

358
10
The Finite Volume Method
there exists a constant C(T ) ≥0 such that, for any j such that j ≤M + 1, we have
sup
n∈Z
u j
n −u

xn+ 1
2 , t j
 ≤C(T )(h + k).
(10.26)
Proof Let us compare u j
n to u(xn+1/2, t j). For this purpose, we introduce
r j
n =
u

xn+ 1
2 , t j+1

−u

xn+ 1
2 , t j

k
+ a
u

xn+ 1
2 , t j

−u

xn−1
2 , t j

hn
.
(10.27)
We use Taylor expansions, ﬁrst with respect to the space variable and then with
respect to the time variable. We ﬁrst have
u

xn−1
2 , t j

= u

xn+ 1
2 , t j

−hn
∂u
∂x

xn+ 1
2 , t j

+ h2
n
2
∂2u
∂x2 (ξn, t j),
where ξn ∈]xn−1/2, xn+1/2[, and secondly
u

xn+ 1
2 , t j+1

= u

xn+ 1
2 , t j

+ k ∂u
∂t

xn+ 1
2 , t j

+ k2
2
∂2u
∂t2

xn+ 1
2 , τ j

,
where τ j ∈]t j, t j+1[. As u satisﬁes the transport equation (10.11), we have
∂u
∂t

xn+ 1
2 , t j

+ a ∂u
∂x

xn+ 1
2 , t j

= 0,
so that
r j
n = −a hn
2
∂2u
∂x2 (ξn, t j) + k
2
∂2u
∂t2

xn+ 1
2 , τ j

.
We deduce that
∥r j∥∞,h = sup
n∈Z
|r j
n | ≤C′(h + k),
(10.28)
where C′ = C
2 max(a, 1).
Let us now introduce the error e j = (e j
n)n∈Z deﬁned by e j
n = u j
n −u(xn+1/2, t j).
Comparing the scheme (10.21) with the deﬁnition (10.27) of r j
n , we obtain
e j+1
n
−e j
n
k
+ a e j
n −e j
n−1
hn
= −r j
n ,
or equivalently
e j+1
n
=

1 −a k
hn

e j
n + a k
hn
e j
n−1 −kr j
n .

10.3 Finite Volumes for the Transport Equation
359
Under condition (10.25), we have 1 −a k
hn ≥0 for all n, and a k
hn ≥0 by hypothesis
on a, so that the ﬁrst two terms form a convex combination of e j
n and e j
n−1. Therefore,
by the triangle inequality
∥e j+1∥∞,h ≤∥e j∥∞,h + k∥r j∥∞,h ≤∥e j∥∞,h + C′k(h + k),
according to (10.28). By an immediate induction argument, we obtain, for any
j ≤M + 1
∥e j∥∞,h ≤∥e0∥∞,h + C′ jk(h + k) ≤∥e0∥∞,h + C′T (h + k).
To conclude, we need to estimate ∥e0∥∞,h. We have
e0
n = v0
n −u(xn+ 1
2 , 0) = 1
hn

Cn
u0(x)dx −u0

xn+ 1
2

.
Now, still using a Taylor expansion (see (10.5)), we have
 1
hn

Cn
u0(x)dx −u0

xn+ 1
2
 ≤C hn
2 ,
so that
∥e0∥∞,h ≤C
2 h.
This gives estimate (10.26) with C(T ) = C
2 + C′T .
□
Remark 10.3 The hypotheses of Proposition 10.4 are satisﬁed provided u0 is regular
and u′
0 and u′′
0 are uniformly bounded on R.
We also deduce the following ﬁnite volume convergence result.
Corollary 10.2 Under the assumptions of Proposition 10.4, there exists a non neg-
ative constant C(T ) such that, for any j ≤M + 1, we have
sup
n∈Z
u j
n −1
hn

Cn
u(x, t j)dx
 ≤C(T )(h + k).
Proof This results from the triangle inequality combined on the one hand with esti-
mate (10.26) and, on the other hand, with the Taylor expansion (10.5) applied to
u(·, t j), from which we deduce
u

xn+ 1
2 , t j

−1
hn

Cn
u(x, t j)dx
 ≤C
2 h,
hence the result.
□

360
10
The Finite Volume Method
Remark 10.4 Condition (10.25) is in fact a CFL stability condition. We will come
back to that later.
Remark 10.5 Proposition 10.4 remains true when a < 0 under the CFL condition
|a|
k
infn∈Z hn
≤1,
which is thus valid for all a, provided that we change xn+1/2 into xn−1/2 in estimate
(10.26) and in the proof when a < 0. Corollary 10.2 remains true without changes.
We can remark that, thanks to the explicit expression (10.12), the exact solution
u of the transport problem satisﬁes a maximum principle, in the sense that if there
exists two constants m∗and m∗such that m∗≤u0 ≤m∗, then the same result holds
at any time t, i.e., we have m∗≤u(·, t) ≤m∗. Let us show that, under the stability
condition (10.25), this property remains valid at the discrete level.
Proposition 10.5 We suppose that there exists two constants m∗and m∗such that
m∗≤u0 ≤m∗. Then, under the stability condition (10.25), we have
for all n,
m∗≤u j
n ≤m∗,
(10.29)
for all j ≤M + 1. Moreover, setting u j = (u j
n)n∈Z, we have
∥u j+1∥∞,h ≤∥u j∥∞,h.
(10.30)
Proof The proof is by induction on j. The double inequality (10.29) is true for j = 0
by hypothesis. Let us suppose that it is true for the index j. We have
u j+1
n
=

1 −a k
hn

u j
n + a k
hn
u j
n−1.
By condition (10.25), we have 1 −a k
hn ≥0 for all n, and a k
hn ≥0 by hypothesis on
a, so that u j+1
n
is a convex combination of u j
n and u j
n−1. It trivially follows that
min(u j
n−1, u j
n) ≤u j+1
n
≤max(u j
n−1, u j
n),
from which m∗≤u j+1
n
≤m∗for all n ensues. Likewise, we obtain (10.30).
□
A natural question that arises is what would happen if we had chosen a down-
wind approximation instead of the upwind approximation, i.e., formula (10.22) when
a > 0. Intuitively, the scheme is looking for information in the opposite direction to
whereitiscomingfrominthecontinuouscase.Sowecanexpectsuchaschemetofail.
Let us give a simple numerical example. We computed the solution on [−2, 2] with
a compactly supported initial data u0 with both schemes. The upwind scheme per-
forms quite well, see Fig.10.2, whereas the downwind scheme gives rise to extreme

10.3 Finite Volumes for the Transport Equation
361
Fig. 10.2 Exact solution and upwind scheme for u0(x) =

(1 −x2)+
2, a = 1
Fig. 10.3 Downwind scheme with the same data
oscillations, see Fig.10.3. This is an instability issue, and we will go back to it later.
Note that if hn = h for all n, if ﬁnite difference initial conditions u0
n = u0(xn) are
chosen and if a k
h = 1, then the upwind scheme computes exact values. Indeed, we
have then
u j
n = u j−1
n−1 = · · · = u0
n−j = u0(xn−j) = u0(xn −jh) = u0(xn −at j).
The computations shown are performed using ﬁnite volume initial data.
10.4
Explicit Three Point Schemes
Let us come back to the general form (10.19)–(10.20) of the scheme. For simplicity,
we suppose from now on that the mesh is uniform, i.e., hn = h for all n ∈Z. We set
λ = k
h ,
and the scheme (10.19) then reads
u j+1
n
= u j
n −λ

g(u j
n, u j
n+1) −g(u j
n−1, u j
n)

.

362
10
The Finite Volume Method
This is a special case of a more general formulation
u j+1
n
= H(u j
n−1, u j
n, u j
n+1),
(10.31)
where H : R3 →R is a given function. This function H must satisfy
H(v, v, v) = v,
(10.32)
for all v ∈R, in order for constant states to be preserved by the scheme. Indeed, if
u0(x) = u0 ∈R for all x ∈R, then obviously u(x, t) = u0 for all x, t.
The scheme (10.31) expresses u j+1
n
explicitly in terms of three values, namely
u j
n−1, u j
n and u j
n+1. We say it is an explicit three point scheme.
Deﬁnition 10.1 The scheme (10.31) can be put in conservation form, or is conser-
vative, if there exists a function g : R2 →R, such that
H(v−1, v0, v1) = v0 −λ[g(v0, v1) −g(v−1, v0)].
The function g is called the numerical ﬂux, it is deﬁned up to an additive constant.
Note that when a three point scheme is conservative, then H(v, v, v) = v, for any
v ∈R.
Remark 10.6 More generally, we can consider (2ℓ+ 1) point schemes, where ℓis a
positive integer, of the form
u j+1
n
= H(u j
n−ℓ, . . . , u j
n, . . . , u j
n+ℓ),
(10.33)
where H : R2ℓ+1 →R is a given function. The deﬁnition of conservative scheme
extends to this case with numerical ﬂux g : R2ℓ→R.
□
Remark 10.7 This deﬁnition can be extended to nonlinear equations with a nonlinear
ﬂux q = f (u). In this case, it is very important to be able to write scheme (10.33)
in conservation form, see [42] for example.
□
In the linear case f (u) = au, the function H is often taken as a linear combination
of values v j, i.e., it is of the form
H(v−1, v0, v1) =
1

l=−1
clvl,
(10.34)
where the coefﬁcients cl only depend on λ and a. In this case, we say that the scheme
itself is linear. According to condition (10.32), we will thus always have
1

l=−1
cl = 1.
(10.35)

10.4 Explicit Three Point Schemes
363
Let us now study the properties of linear three point schemes. Since the cell size
is constant, these schemes can be interpreted as ﬁnite difference schemes with initial
data given by the average values of u0 on the cells. Therefore, we only concentrate
on the properties of consistency, order and stability in spaces ℓp(Z).
We introduce the discrete norms on ℓp(Z),
∥v∥p,h = h1/p
n∈Z
|vn|p1/p
if 1 ≤p < +∞,
∥v∥∞,h = sup
n∈Z
|vn|,
already used for p = 2 in Chaps.8 and 9.2 For simplicity, we adopt the following
deﬁnition of stability in the present context.
Deﬁnition 10.2 We say that a scheme (10.31) is stable for the norm ∥· ∥p,h, 1 ≤
p ≤∞, if the sequence u j = (u j
n)n∈Z satisﬁes, for all j ≥0,
∥u j+1∥p,h ≤∥u j∥p,h,
for all initial data u0 ∈ℓp(Z).
This deﬁnition of stability is signiﬁcantly more demanding than the one used before
in Chaps.8 and 9, see Deﬁnition 33. In practice, only the cases p = 2 and p = +∞
are used.
There are sufﬁcient conditions for ℓ∞stability which are very simple to check.
Proposition 10.6 Let us consider a linear scheme (10.34)–(10.35). The scheme is
stable in ℓ∞if and only if the coefﬁcients cl are all nonnegative.
Proof Let us assume that cl ≥0 for all l. Then by condition (10.35), u j+1
n
is a convex
combination of u j
n−1, u j
n, and u j
n+1 so that
min
l=−1,0,1 u j
n+l ≤u j+1
n
≤
max
l=−1,0,1 u j
n+l
Now we have
−∥u j∥∞,h ≤
min
l=−1,0,1 u j
n+l
and
max
l=−1,0,1 u j
n+l ≤∥u j∥∞,h,
therefore −∥u j∥∞,h ≤u j+1
n
≤∥u j∥∞,h for all n ∈Z. This clearly implies the
stability in ℓ∞.
Assume now that c0 < 0. We thus have c−1 −c0 + c1 = 1 −2c0 > 1. The initial
data u0
n = (−1)n+1 is such that u1
0 = c−1 −c0 + c1, hence ℓ∞stability is violated.
Likewise, if c−1 < 0, then −c−1 + c0 + c1 = 1 −2c−1 > 1, and an initial data such
that u0
−1 = −1, u0
0 = u0
1 = 1 with ∥u0∥∞,h = 1 yields u1
0 = −c−1 + c0 + c1, and
similarly for the case c1 < 0.
□
2Eventhoughthe inﬁnitynormdoesnot involve h,we keepthe h subscript fornotational consistency.

364
10
The Finite Volume Method
Recall that condition (10.35) implies that constant states are preserved by the
scheme. Moreover, in this case, there exists an associated numerical ﬂux, which is
also linear.
Lemma 10.1 Any linear scheme (10.34)–(10.35) admits a numerical ﬂux of the form
g(u, v) = 1
λ(c−1u −c1v).
(10.36)
Proof The three point scheme reads
u j+1
n
= c−1u j
n−1 + c0u j
n + c1u j
n+1.
By (10.35), it follows that
u j+1
n
= u j
n −

(c−1u j
n −c1u j
n+1) −(c−1u j
n−1 −c1u j
n)

.
Hence g is given by (10.36).
□
Deﬁnition 10.3 Alinearschemeissaidtobemonotoneifgiventwoinitialconditions
u0 and w0 such that u0
n ≥w0
n for all n, we have u1
n ≥w1
n for all n.
Obviously, we also have u j
n ≥w j
n, for all j and n. Schemes that satisfy the
hypotheses of Proposition 10.6 are monotone. We have already seen one example of
such schemes, the upwind scheme, under the stability condition (10.25). We will see
another example below, the Lax–Friedrichs scheme.
As before, ℓ2 stability is studied via Fourier series.
Proposition 10.7 A linear scheme (10.34) is stable in ℓ2 if and only if the function
m(s) =
1

l=−1
cle−ils
(10.37)
satisﬁes
∀s ∈[0, 2π],
|m(s)| ≤1.
(10.38)
The function m is the ampliﬁcation coefﬁcient of the scheme and condition (10.38)
is the von Neumann condition.
Proof We use the Fourier series approach, see Sect.8.7 of Chap.8. For any v ∈ℓ2,
deﬁning F(v)(s) = 
n∈Z vneins ∈L2(0, 2π), we obtain
F(u j+1)(s) = m(s)F(u j)(s),

10.4 Explicit Three Point Schemes
365
where m is deﬁned by (10.37). We know that the von Neumann condition (10.38) is
then equivalent to
∥F(u j+1)∥L2(0,2π) ≤∥F(u j)∥L2(0,2π),
(10.39)
for all initial conditions. The Fourier transform is an isometry, thus
∥u j+1∥2,h ≤∥u j∥2,h
and the scheme is stable in ℓ2. Conversely, if the scheme is stable in ℓ2, then inequality
(10.39) holds, which implies the von Neumann condition (10.38).
□
We now consider consistency and order issues. Recall that λ = k
h . To simplify
the analysis, we assume from now on that λ is constant, thus linking the time step
and the cell size. Consistency will thus be studied in the limit k →0.
Deﬁnition 10.4 Let u be a bounded, regular solution of Eq.(10.11). The truncation
error of the scheme is the sequence εk(u) j, deﬁned by
εk(u) j
n = 1
k

u(xn, t j+1) −H(u(xn−1, t j), u(xn, t j), u(xn+1, t j))

,
for all n ∈Z.
Deﬁnition 10.5 We say that the three point scheme (10.31) is consistent in the ℓ∞
norm, if for any regular solution u of Eq.(10.11) with uniformly bounded derivatives,
we have
sup
j≤T/k
∥εk(u) j∥∞,h →0 when k →0.
It is of order p if
sup
j≤T/k
∥εk(u) j∥∞,h = O(k p).
There are similar deﬁnitions for 2ℓ+ 1 schemes. We now give conditions on the
coefﬁcients of a linear scheme for it to be consistent. Let us introduce the Courant
number
c = λa = ak
h ,
which also plays a crucial role in stability, see condition (10.25) above.
Proposition 10.8 A linear scheme (10.34)–(10.35) is consistent and at least of order
one, if and only if
1

l=−1
lcl = −c,

366
10
The Finite Volume Method
or equivalently
g(v, v) = av, for all v ∈R,
(10.40)
with g deﬁned by Eq.(10.36). If in addition we have 1
l=−1 l2cl = c2, then the scheme
is at least of order two.
Proof We use Taylor expansions, assuming that u is regular enough. There exists
τ j, ξn,l such that
u(xn, t j+1) = u(xn, t j) + k ∂u
∂t (xn, t j) + k2
2
∂2u
∂t2 (xn, t j) + k3
6
∂3u
∂t3 (xn, τ j)
and
u(xn+l, t j) = u(xn, t j) + lh ∂u
∂x (xn, t j) + (lh)2
2
∂2u
∂x2 (xn, t j) + (lh)3
6
∂3u
∂x3 (ξn,l, t j),
for l = ±1. Since u is a regular solution of (10.11), we have ∂u
∂t + a ∂u
∂x = 0, but also
∂2u
∂t2 = −a ∂
∂t
∂u
∂x

= −a ∂
∂x
∂u
∂t

= a2 ∂2u
∂x2 .
Since 1
l=−1 cl = 1, the truncation error is therefore such that
εk(u) j
n = −

a +
1

l=−1
lcl
λ
∂u
∂x (xn, t j) + k
2

a2 −
1

l=−1
l2cl
λ2
∂2u
∂x2 (xn, t j) + R j
n(k),
where |R j
n(k)| ≤Ck2 and C does not depend on n, j and k. This gives the expected
results. Relation (10.40) then follows from Lemma 10.1.
□
Remark 10.8 The previous result can be generalized to 2ℓ+ 1 point schemes. A
conservative scheme of the form (10.33) is consistent if g(v, . . . , v) = av for all
v ∈R, and is then at least of order one. In the nonlinear case, consistency reads
g(v, . . . , v) = f (v) for all v.
□
We thus see that consistency or order at least one is equivalent to c−1 −c1 = c
and order at least two to c−1 + c1 = c2. In fact, we can describe all conservative,
consistent three point schemes as follows.
Corollary 10.3 Any conservative, consistent, three point linear scheme is of the
form
u j+1
n
= u j
n −λa
2 (u j
n+1 −u j
n−1) + q
2 (u j
n+1 −2u j
n + u j
n−1).
(10.41)

10.4 Explicit Three Point Schemes
367
for some q ∈R, called the viscosity coefﬁcient of the scheme. Furthermore, the
scheme is of order two if and only if q = c2 = λ2a2. The numerical ﬂux is given by
g(u, v) = a
2(u + v) −q
2λ(v −u).
(10.42)
Proof We start from
u j+1
n
= c−1u j
n−1 + c0u j
n + c1u j
n+1.
with c−1 + c0 + c1 = 1 and c−1 −c1 = c = λa. Letting q = c−1 + c1 = 1 −c0,
then c−1 = (q + c)/2, c1 = (q −c)/2 and the scheme reads
u j+1
n
= q + c
2
u j
n−1 + (1 −q)u j
n + q −c
2
u j
n+1,
(10.43)
from which (10.41) immediately follows. The scheme is of order two if moreover
q = c−1 + c1 = c2. Pursuing the Taylor expansions one step further, we check
that the resulting unique scheme is not of order three. Finally, Eq.(10.42) is just a
rewriting of Eq.(10.36) in terms of the new parameters.
□
The scheme obtained for q = c2 is the Lax–Wendroff scheme, that we have
already encountered in the context of the wave equation, see Chap.9, Sect.9.6. The
name viscosity coefﬁcient comes from the fact that the factor u j
n+1 −2u j
n + u j
n−1 is
directly linked to the three point ﬁnite difference approximation of ∂2u
∂x2 (xn, t j). The
corresponding term in the scheme thus acts as some kind of numerical viscosity or
dissipation.
Remark 10.9 Since we have assumed λ to be a constant, and the constants cl and thus
q only depend on λ and a, there is no discretization parameter left in the expression
of the scheme (10.41). This can be unsettling since how then can we talk about
convergence when h, and thus k, goes to 0, since the scheme apparently depends
on neither one of the two? Two factors are at play here. First of all, we are working
on R with an inﬁnite number of discrete values. If we were working on a bounded
interval with boundary conditions, the number of discrete values would be ﬁnite
and depend on h and k. It would of course go to inﬁnity when h, and thus k, goes
to 0. Secondly, the discretization parameter h is still hidden in the initial data of
the scheme. In a sense, it is the dependence of the initial data on the discretization
parameter that drives the convergence of the scheme when h goes to 0, see also the
proof of Theorem 10.1 below.
□
Formula (10.41) is also useful for stability.
Proposition 10.9 A three point linear scheme (10.41) is stable in ℓ2 if and only if
its viscosity coefﬁcient q and Courant number c satisfy
c2 ≤q ≤1.

368
10
The Finite Volume Method
Fig. 10.4 ℓ2 stability
Stability in ℓ∞is equivalent to
q ≥0 and c2 ≤q2 ≤1.
Proof The ampliﬁcation coefﬁcient of the scheme is given by
m(s) = 1 −c
2(eis −e−is) + q
2 (eis −2 + e−is) = 1 −ic sin s + q(cos s −1).
The image of the mapping s →m(s) in C is an ellipse with vertices 1, 1 −2q,
1−q ±ic. It is included in the unit disk if and only if it is included in the unit disk in
a neighborhood of 1 and that 1−2q belongs to the unit disk, see Fig.10.4. The second
condition implies that 1−2q ≥−1, i.e., q ≤1. For the ﬁrst condition, we notice that
m(s) = 1−q s2
2 −ics+O(s3) when s →0, so that |m(s)|2 = 1−qs2+c2s2+O(s3),
hence the condition c2 −q ≤0.
In view of Eq.(10.43), stability in ℓ∞is ensured if and only if
q ≤1 and
−q ≤c ≤q,
by Proposition 10.6. The double inequality implies that q ≥0, it is thus is equivalent
to c2 ≤q2.
□
The ℓ2 stability condition is thus (λa)2
≤q ≤1, whereas the ℓ∞stability
condition reads (λa)2 ≤q2 ≤1 with q ≥0. The latter is more stringent than the
former since q2 ≤q. In particular, it is not satisﬁed by the Lax–Wendroff scheme
unless |c| = q = 1.

10.4 Explicit Three Point Schemes
369
Inanycase,thestabilityofanexplicitschemeislinkedtoaCFLstabilitycondition,
see also Chap.9, Proposition 9.4, of the form
|a|k
h ≤μ,
with μ ≤1 for a three point scheme, μ ≤ℓfor a 2ℓ+1 point scheme. The scalar |a|k
is the distance covered by the transport phenomenon during one time step at speed
a: the exact solution satisﬁes u(xn, t j+1) = u(xn −ak, t j). The stability condition
|c| ≤1 says that this distance cannot exceed the length of one cell for a three point
scheme (or two cells for a ﬁve point scheme, and so on). This expresses the fact
that the discrete cone of inﬂuence must contain the backward characteristic passing
through (xn, t j+1) (see Fig.9.6 in Chap.9 in the case of the wave equation).
Let us now say a few words about convergence in this context, in ℓ∞for simplicity.
Convergence thus clearly means (recall that λ = k/h is constant)
sup
n∈Z, j≤T/k
|u j
n −u(xn, t j)| →0 when k →0,
when u, i.e., u0, is regular enough. As can be expected, we also have a Lax theorem.
Theorem 10.1 A three point linear scheme that is stable and consistent in ℓ∞is
convergent in ℓ∞.
Proof Let us set v j
n = u(xn, t j) and e j
n = u j
n −v j
n. By deﬁnition of the truncation
error, we have v j+1
n
= H(v j
n−1, v j
n, v j
n+1) + kεk(u) j
n so that
e j+1
n
= H(e j
n−1, e j
n, e j
n+1) −kεk(u) j
n,
e0
n = u0
n −u0(xn).
Now the scheme is stable, therefore

H(e j
n−1, e j
n, e j
n+1)

n∈Z

∞,h ≤∥e j∥∞,h,
so that
∥e j∥∞,h ≤∥e0∥∞,h + k
j−1

l=0
εk(u)l
∞,h ≤∥e0∥∞,h + T sup
l≤T/k
∥εk(u)l∥∞,h,
for all j ≤T/k by a repeated application of the triangle inequality. When k →0
then h →0, and the ﬁrst term in the right-hand side tends to 0 if u0 is regular
enough. The second term tends to 0 by consistency of the scheme in ℓ∞, hence the
convergence result.
□
Naturally, if the scheme is of order p, we have an error estimate
sup
n∈Z, j≤T/k
|u j
n −u(xn, t j)| ≤Ck p,

370
10
The Finite Volume Method
provided u0 is regular enough, if we use ﬁnite difference initial data or an order p
approximation thereof.
Let us now give a few more examples of schemes that can be used for the transport
equation, in either the ﬁnite volume or the ﬁnite difference contexts.
10.5
Examples of Linear Schemes
For all these schemes, we assume that h j = h, λ constant and u0
n = 1
h

Cn u0(x) dx
is given.
Example 10.1 The decentered scheme.
This is the upwind scheme we already studied in the general case h j depending
on j. Let us just give a few more remarks in the light of the previous developments.
First, we have a general expression that is valid whatever the sign of a. We can
write
u j+1
n
= u j
n −λ(a−(u j
n+1 −u j
n) + a+(u j
n −u j
n−1)),
(10.44)
where a−= min(a, 0) = a if a ≤0, 0 otherwise and a+ = max(a, 0) = a if a ≥0,
0 otherwise.3 This corresponds to the numerical ﬂux
g(u, v) = a+u + a−v.
Formula (10.44) thus combines the two cases (10.21) and (10.22) into one. Since
a = a+ + a−, |a| = a+ −a−, rewriting the scheme in the form (10.41), we obtain
u j+1
n
= u j
n −λa
2 (u j
n+1 −u j
n−1) + λ|a|
2 (u j
n+1 −2u j
n + u j
n−1),
hence a viscosity coefﬁcient q = λ|a|. We summarize the main properties of this
scheme.
Proposition 10.10 The CFL condition
λ|a| ≤1,
(10.45)
is a necessary and sufﬁcient condition of stability in both ℓ∞and ℓ2. The scheme is
of order one in ℓ∞, it is thus convergent in ℓ∞. It also satisﬁes the discrete maximum
principle.
3Note that this is the opposite convention to the one used for negative parts, which is a−=
−min(a, 0).

10.5 Examples of Linear Schemes
371
Fig. 10.5 Centered scheme divergence, same data as before
Example 10.2 The centered scheme.
The centered scheme is deﬁned by
u j+1
n
= u j
n −λa
2 (u j
n+1 −u j
n−1).
This corresponds to the numerical ﬂux
g(u, v) = a
2(u + v),
i.e., the mean value of the exact ﬂuxes at the interfaces, and viscosity coefﬁcient
q = 0. The scheme is thus unstable in ℓ2 and ℓ∞as soon as a ̸= 0. The scheme is
not used since it is not convergent, see Fig.10.5.
Example 10.3 The Lax–Friedrichs scheme.
We have already encountered this scheme in the context of the wave equation in
Chap.9. It is given by the following formula
u j+1
n
= u j
n+1 + u j
n−1
2
−λa
2 (u j
n+1 −u j
n−1).
This scheme is associated with the numerical ﬂux
g(u, v) = a
2(u + v) −1
2λ(v −u),
and viscosity coefﬁcient q = 1.
Proposition 10.11 The Lax–Friedrichs scheme is stable both in ℓ∞and in ℓ2 if and
only if the CFL condition (10.45) is satisﬁed. It is then monotone. It is of order one
in ℓ∞.

372
10
The Finite Volume Method
Example 10.4 The Lax–Wendroff scheme.
For completeness, we also record the Lax–Wendroff scheme, that was already
discussed earlier and in Chap.9. The scheme reads
u j+1
n
= u j
n −λa
2 (u j
n+1 −u j
n−1) + λ2a2
2
(u j
n+1 −2u j
n + u j
n−1).
This scheme is associated with the numerical ﬂux
g(u, v) = a
2(u + v) −λa2
2 (v −u),
and viscosity coefﬁcient q = c2.
Proposition 10.12 The Lax–Wendroff scheme is stable in ℓ2 if and only if the CFL
condition (10.45) is satisﬁed. It is unstable in ℓ∞unless c = 1. It is of order two
in ℓ∞.
Recall that the Lax–Wendroff scheme is the only linear three point scheme of
order two.
10.6
Generalizations to Discontinuous Data and Nonlinear
Problems
The above schemes can also be applied when the initial data is discontinuous, see
Figs.10.6 and 10.7. In this case, the exact solution is still given by formula (10.12).
It is thus also discontinuous and must be understood in the distributional sense. As a
consequence, considerations of order of schemes do not apply. The question is more
how accurately do the various numerical schemes capture the jumps in the solution.
We show a computation in which aλ is close to 1 but strictly lower, see Fig.10.6. The
same computation performed with aλ = 1 shows a perfect ﬁt of all three schemes
with the exact solution in Fig.10.7. Indeed, it is easily checked that this should be the
case as the three schemes coincide, in particular with the upwind scheme, which we
Fig. 10.6 Various schemes for a discontinuous initial data u0 = 1[−1,1], aλ < 1

10.6 Generalizations to Discontinuous Data and Nonlinear Problems
373
Fig. 10.7 Same with aλ = 1
already know computes exact values in this case when ﬁnite difference initial values
are used (which we did here for simplicity).
The latter remark could cast legitimate doubts on why bother with all the previous
developments. The reason is that all problems are not one-dimensional, scalar and
linear (anyway, there is an explicit formula for the solution in this instance). The
simple one-dimensional, scalar and linear situation is a preparation for more chal-
lenging problems. We will talk about the two-dimensional case in the next section.
We have already seen a few vector-valued problems in Chap.9. Let us say a couple
of words about the nonlinear case.
All the above schemes have nonlinear versions, the analysis of which is much
more complicated than that of their linear version, see for example [42]. To illustrate
the nonlinear case, we take the example of the Burgers equation, ∂u
∂t + u ∂u
∂x = 0 or
∂u
∂t + ∂
∂x
 u2
2

= 0 in conservation form. We consider two initial conditions, u0(x) =

(1−x2)+
2 (compactly supported in [−2, 2]) and u0(x) = 1+sin(πx)/2 (periodic
on [−2, 2]). We plot the exact solution by using the characteristics at some time T
small enough so that they have not crossed yet, and the corresponding results of the
upwind, Lax–Friedrichs and Lax–Wendroff schemes, see Figs.10.8 and 10.9.
The ﬁrst two schemes assume the same form as in the linear case, with the linear
ﬂux f (u) = au replaced by the nonlinear ﬂux f (u) = u2
2 , namely,
u j+1
n
= u j
n −λ

f (u j
n) −f (u j
n−1)

,
Fig. 10.8 Burgers equation, with initial condition u0(x) =

(1 −x2)+
2

374
10
The Finite Volume Method
Fig. 10.9 Burgers equation, with periodic initial condition u0(x) = 1+sin(πx)/2, hence periodic
solution
for the upwind scheme (the initial conditions are nonnegative, hence the speeds are
nonnegative too), and
u j+1
n
= u j
n+1 + u j
n−1
2
−λ
2( f (u j
n+1) −f (u j
n−1)),
for the Lax–Friedrichs scheme. The Lax–Wendroff scheme is of the form
u j+1
n
= u j
n −λ
2

f (u j
n+1) −f (u j
n−1)

+ λ2
2

A(u j
n+1, u j
n)

f (u j
n+1) −f (u j
n)

−A(u j
n, u j
n−1)

f (u j
n) −f (u j
n−1)

,
where A(u, v) = u+v
2
in the particular case of the Burgers equation.
We also give space–time pictures of the corresponding characteristics before and
after they cross, see Figs.10.10 and 10.11.
Fig. 10.10 Characteristics for u0(x) =

(1 −x2)+
2
Fig. 10.11 Characteristics for u0(x) = 1 + sin(πx)/2

10.6 Generalizations to Discontinuous Data and Nonlinear Problems
375
Note in both cases the already mentioned fact that the crest of the wave travels
faster than the trough (which does not travel at all in the ﬁrst case). One of the main
issues from the theoretical side is what sense to give to a solution after the charac-
teristics have crossed, a situation which results in the formation of a shock wave,
i.e., a jump discontinuity, even if the initial condition is smooth. On the numerical
side, the question is how to reliably compute the shock wave. We leave these difﬁcult
questions aside, see for example [42].
10.7
The Transport Equation in an Open Set in Higher
Dimensions
In the sequel, Ω denotes a bounded open set of Rd, with sufﬁciently regular boundary
Γ = ∂Ω. We here denote by ν the normal unit exterior vector. In this d-dimensional
context, the advection velocity a is a given vector ﬁeld deﬁned on ¯Q = ¯Ω × [0, T ]
of class C1, with values in Rd. Without loss of generality, we can assume that a is the
restriction of a vector ﬁeld deﬁned on Rd ×R with bounded derivatives, still denoted
a, to ¯Q. We again denote by t →X(t; y, s) the characteristic passing through point
y ∈Rd at time s ∈R, still deﬁned by (10.14).
For any t ∈R, we decompose Γ into three different parts
Γ −(t) = {x ∈Γ, a(x, t) · ν(x) < 0},
Γ +(t) = {x ∈Γ, a(x, t) · ν(x) > 0},
Γ 0(t) = {x ∈Γ, a(x, t) · ν(x) = 0}.
The part Γ −(t) is called the incoming part of the boundary at time t, i.e., the part
of the boundary where the advection velocity a points inwards and characteristics
coming from outside Ω enter Ω. Likewise, Γ +(t) is the outgoing part where a
points outwards at time t and characteristics leave Ω. Finally, Γ 0(t) is called the
characteristic part. The velocity and characteristics are tangential to Γ on the char-
acteristic part at time t. When the velocity a is stationary, i.e., does not depend on t,
the above parts also are independent of t and are then denoted by Γ −, Γ + and Γ 0.
For simplicity, we assume that any characteristic passing through a point y at time
s with (y, s) ∈Γ 0(s) does not enter Ω, see Fig.10.12. We consider the following
general transport problem
⎧
⎪⎪⎨
⎪⎪⎩
∂u
∂t (x, t) + a(x, t) · ∇u(x, t) = f (x, t) in Ω × ]0, T [,
u(x, t) = g(x, t) on ∂Q−
u(x, 0) = u0(x) in Ω,
(10.46)
where f is a given source term deﬁned on Q, g a given Dirichlet boundary condition
deﬁned on the set ∂Q−= {(x, t), x ∈Γ −(t), 0 ≤t < T } ⊂Γ × [0, T [ and

376
10
The Finite Volume Method
Fig. 10.12 The characteristic passing through y at time s in the case τin(y, s) > 0 and
τout(y, s) < T. This particular drawing only makes sense if a does not depend on t
u0 a given initial data deﬁned on Ω. This is an initial-boundary value problem. As
already mentioned in Chap.1 in one dimension, no Dirichlet boundary condition is
needed or can even be a priori imposed on the outgoing and characteristic parts of
the boundary.
As in the one-dimensional case, we can express the solution of problem (10.46)
by means of the characteristics, assuming all functions are regular enough. For all
(y, s) ∈
¯Ω × [0, T ], we let [τin(y, s), τout(y, s)] denote the connected compo-
nent containing s of the set of times t such that X(t; y, s) ∈
¯Ω. We remark that
X(τin(y, s); y, s) ∈Γ −(τin(y, s)) if τin(y, s) > 0, due to the hypothesis made on
characteristics leaving Γ 0.
Problem (10.46) thus has an explicit solution, assuming the characteristics are
known. This solution is obtained by integration along the characteristics. More pre-
cisely, we have the following result which generalizes formula (10.15).
Proposition 10.13 If u is a regular solution of (10.46), then
u(x, t) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
u0(X(0; x, t)) +
 t
0
f (X(τ; x, t), τ) dτ, if τin(x, t) = 0,
g(X(τin(x, t); x, t), τin(x, t)) +
 t
τin(x,t)
f (X(τ; x, t), τ) dτ, if τin(x, t) > 0.
(10.47)
Proof Let u be a regular solution of (10.46). Let us pick (x, t) ∈Q. For τ ∈
]τin(x, t), τout(x, t)[, we set y(τ) = X(τ; x, t) and v(τ) = u(y(τ), τ), so that x =
y(t) and v(t) = u(x, t). Using the ﬁrst equation in (10.14) and the ﬁrst equation in
(10.46), we have

10.7 The Transport Equation in an Open Set in Higher Dimensions
377
v′(τ) = ∂u
∂t (y(τ), τ) + y′(τ) · ∇u(y(τ), τ) =
∂u
∂t + a · ∇u

(y(τ), τ) = f (y(τ), τ).
There are now two cases, depending on τin(x, t). If τin(x, t) = 0, we can integrate
the equation between 0 and t. We obtain
u(x, t) = v(t) = v(0) +
 t
0
f (y(τ), τ) dτ
= u(y(0), 0) +
 t
0
f (y(τ), τ) dτ = u0(y(0)) +
 t
0
f (y(τ), τ) dτ,
using the initial condition. Given the deﬁnition of y(τ), this is the ﬁrst expression in
(10.47).
Let us now consider the second case τin(x, t) > 0. In this case, we integrate
between τin(x, t) and t and obtain
u(x, t) = v(t) = v(τin(x, t)) +
 t
τin(x,t)
f (y(τ), τ) dτ
= u(y(τin(x, t)), τin(x, t)) +
 t
τin(x,t)
f (y(τ), τ) dτ
= g(y(τin(x, t)), τin(x, t)) +
 t
τin(x,t)
f (y(τ), τ) dτ,
on account of the incoming boundary condition (second equation in (10.46)). We
thus obtain the second expression in (10.47).
□
Remark 10.10 Conversely, if the function u given by formulas (10.47) is sufﬁciently
regular, it is a solution of the transport equation. Indeed, the transport equation is
clearly satisﬁed along the characteristics issuing from Γ −, which cover all of Q by
the Cauchy–Lipschitz theorem, and the initial and boundary conditions are clearly
satisﬁed. Whether or not u is regular is a question of compatibility between the
functions u0 and g.
□
10.8
Finite Volumes for the Transport Equation
in Two Dimensions
For simplicity, we suppose that f = 0 and that a does not depend on x and t, so
that Γ −, Γ + and Γ 0 are time independent. We also suppose that Ω is polygonal.
We ﬁrst cover ¯Ω by N cells Cn, which are closed polygons such that ¯Ω = ∪N
n=1Cn
whose pairwise intersections are of zero measure. Note that these cells do not need
to be triangles or rectangles, as in the case of the ﬁnite element method. They also
do not need to be all of the same type: we can have triangles, quadrilaterals, and so
on, in the same mesh. For simplicity, we suppose that the mesh is admissible in the

378
10
The Finite Volume Method
Fig. 10.13 An admissible ﬁnite volume mesh of Ω
ﬁnite element sense, i.e., the intersection between two cells is either empty, reduced
to one vertex, or an entire edge, see Fig.10.13.
We denote by νn the normal unit exterior vector to Cn and by An the area of Cn,
so that the area of Ω is N
n=1 An. If two cells Cn and Cm have a common edge, this
interface is denoted by Σnm = Σmn, and νnm denotes the restriction of νn to Σnm (it
thus points into Cm). The length of the interface is lnm = lmn. By construction, we
have νnm + νmn = 0. The boundary of Cn is the union of segments
∂Cn =
M(n)

m=1
Σnm
M−(n)

p=1
Γ −
np
M+(n)

q=1
Γ +
nq
M0(n)

r=1
Γ 0
nr,
where M(n) denotes the number of internal edges Σnm of Cn (i.e., the edges which
are inside Ω), while M−(n) (resp. M+(n), M0(n)) denotes the number of edges of
Cn which are on Γ −(resp. on Γ +, Γ 0).4 The sets
M−(n)

p=1
Γ −
np = ∂Cn ∩Γ −,
M+(n)

q=1
Γ +
nq = ∂Cn ∩Γ +,
M0(n)

r=1
Γ 0
nr = ∂Cn ∩Γ 0
4If one of these integers is 0, the corresponding union is empty.

10.8 Finite Volumes for the Transport Equation in Two Dimensions
379
represent all the boundary edges of Cn depending on whether they are on Γ −, Γ +
or Γ 0. We denote by l−
np (resp. l+
nq, l0
nr) the length of Γ −
np (resp. Γ +
nq, Γ 0
nr). We also
denote by ν−
np (resp. ν+
nq, ν0
nr) the restriction of νn to Γ −
np (resp. Γ +
nq, Γ 0
nr).
As in the 1d-case, we ﬁrst integrate the transport equation on Cn at time t and
obtain
d
dt

Cn
u(x, t) dx

+

Cn
(a · ∇u)(x, t) dx = 0.
(10.48)
We introduce the mean value ¯un(t) of u(., t) on the cell Cn, i.e.,
¯un(t) = 1
An

Cn
u(x, t) dx.
The ﬁrst term in (10.48) thus becomes
d
dt

Cn
u(x, t)dx

= An
d ¯un
dt (t).
We can use the Stokes formula (3.6) of Chap.3 with U(x, t) = u(x, t)a since a is
constant. The second term in (10.48) then reads

Cn
(a · ∇u)(x, t)dx = a ·

∂Cn
u(x, t)νn(x) dΓ

.
Since each Σnm, Γ −
np, Γ +
nq and Γ 0
nr is a segment, we have, taking into account the
incoming boundary condition

Cn
(a · ∇u)(x, t) dx =
M(n)

m=1
(a · νnm)

Σnm
u(x, t) dΓ
+
M−(n)

p=1
(a · ν−
np)

Γ −
np
g(x, t) dΓ +
M+(n)

q=1
(a · ν+
nq)

Γ +
nq
u(x, t) dΓ,
with the convention that, if M−(n) = 0 or M+(n) = 0, the corresponding sums are
equal to 0. Integrating Eq.(10.48) between t j and t j+1, we have for all n = 1, . . . , N
An[¯un(t j+1) −¯un(t j)] +
M(n)

m=1
(a · νnm)
 t j+1
t j

Σnm
u(x, t) dΓ dt
+ k
M−(n)

p=1
(a · ν−
np)l−
np g j
np +
M+(n)

q=1
(a · ν+
nq)
 t j+1
t j

Γ +
nq
u(x, t) dΓ dt = 0,
(10.49)

380
10
The Finite Volume Method
where we have set
g j
np =
1
k l−
np
 t j+1
t j

Γ −
np
g(x, t) dΓ dt.
These relations are exactly satisﬁed, there is no approximation so far.
First, we approximate each integral with respect to time as
 t j+1
t j
φ(t)dt ≈kφ(t j),
which corresponds to the explicit Euler scheme in time. We next have to approximate
the ﬂuxes on Σnm and Γ +
nq at time t j in terms of discrete unknowns u j
n, where u j
n
is meant to be an approximation of ¯un(t j). Let us explain how to proceed for the
computation on Σnm. We still use the fact that the information is transported by
the characteristic lines. If a · νnm = 0, the corresponding term in (10.49) vanishes,
hence there are two cases left depending on the sign of a · νnm (see Fig.10.14). More
precisely, we consider the following upwind approximation:

Σnm
u(x, t j) dΓ ≈

lnmu j
n,
if a · νnm > 0,
lnmu j
m, if a · νnm < 0.
Using the same idea for the ﬂux on Γ +
nq, we obtain

Γ +
nq
u(x, t j) dΓ ≈l+
nqu j
n.
Note that there is only one possible choice for the numerical ﬂux in this case, and
this choice is consistent with the upwind philosophy, since we are on the outgoing
part of the boundary.
Putting all the above approximations together, we obtain the explicit ﬁnite volume
scheme
An
u j+1
n
−u j
n
k
+

1≤m≤M(n)
a·νnm>0
(a · νnm)lnm u j
n +

1≤m≤M(n)
a·νnm<0
(a · νnm)lnm u j
m
+
M−(n)

p=1
(a · ν−
np)l−
np g j
np +
M+(n)

q=1
(a · ν+
nq)l+
nq u j
n = 0.
(10.50)
We complement the scheme with the initial condition
u0
n = 1
An

Cn
u0(x) dx,
(10.51)

10.8 Finite Volumes for the Transport Equation in Two Dimensions
381
Fig. 10.14 The two cases depending on the sign of a · νnm
or an approximation thereof. Once a numbering of the cells is chosen, we can intro-
duce the sequence of vectors U j ∈RN with components u j
n, and the scheme (10.50)–
(10.51) takes the usual form U j+1 = AU j + G j, U 0 given, where A is an N × N
sparse matrix and G j is a given vector in RN corresponding to the Dirichlet condition
on the incoming part of the boundary.

382
10
The Finite Volume Method
We have the following important property:
Proposition 10.14 The scheme is conservative, i.e., the variation of the total mass
is due to what ﬂows in and out at the boundary of Ω
N

n=1
An
u j+1
n
−u j
n
k
+
N

n=1
M−(n)

p=1
(a · ν−
np)l−
np g j
np +
N

n=1
M+(n)

q=1
(a · ν+
nq)l+
nq u j
n = 0.
Proof The proof is straightforward. We have in fact
N

n=1

m∈{1,...,M(n)}
a·νnm>0
(a · νnm)lnm u j
n +
N

n=1

m∈{1,...,M(n)}
a·νnm<0
(a · νnm)lnm u j
m = 0.
Indeed, due to the assumption that the ﬁnite volume mesh is admissible, to each
term in the ﬁrst sum corresponding to an edge Σnm of Cn, there corresponds one
and only one term in the second sum corresponding to the edge Σmn of Cm that
is shared with Cn, with lmn = lnm, νmn = −νnm and the same value u j
n due to the
upwind approximation. The above pairing clearly exhausts all the terms of the second
sum.
□
We do not pursue here the numerical analysis of the above ﬁnite volume scheme
and refer to [30, 36, 56] for a much more comprehensive mathematical analysis of
the ﬁnite volume method. This requires more advanced mathematical techniques, in
particular the use of spaces of functions with bounded variation, which are functions
whose distributional derivatives are not functions but measures, to accommodate
piecewise constant functions.

References
1. R.A. Adams, Sobolev Spaces, vol. 65, Pure and Applied Mathematics (Academic Press, New
York, 1975). A subsidiary of Harcourt Brace Jovanovich, Publishers
2. R.A. Adams, J.J.F. Fournier, Sobolev Spaces, vol. 140, 2nd edn., Pure and Applied Mathematics
(Amsterdam) (Elsevier/Academic Press, Amsterdam, 2003)
3. S. Agmon, A. Douglis, L. Nirenberg, Estimates near the boundary for solutions of elliptic
partial differential equations satisfying general boundary conditions. I. Commun. Pure Appl.
Math. 12, 623–727 (1959)
4. S. Agmon, A. Douglis, L. Nirenberg, Estimates near the boundary for solutions of elliptic
partial differential equations satisfying general boundary conditions. II. Commun. Pure Appl.
Math. 17, 35–92 (1964)
5. G. Allaire, Numerical Analysis and Optimization. An Introduction to Mathematical Modelling
and Numerical Simulation, Numerical Mathematics and Scientiﬁc Computation (Oxford Uni-
versity Press, Oxford, 2007)
6. G. Allaire, S.M. Kaber, Numerical Linear Algebra, vol. 55, Texts in Applied Mathematics
(Springer, New York, 2008)
7. K.E. Atkinson, W. Han, Theoretical Numerical Analysis. A Functional Analysis Framework,
vol. 39, 3rd edn., Texts in Applied Mathematics (Springer, Dordrecht, 2009)
8. K.E. Atkinson, W. Han, D. Stewart, Numerical Solution of Ordinary Differential Equations,
Pure and Applied Mathematics (Wiley, New York, 2009)
9. J.-P. Aubin, Applied Functional Analysis, 2nd edn. Pure and Applied Mathematics (New York)
(Wiley-Interscience, New York, 2000)
10. C. Bernardi, Y. Maday, Spectral methods, in Handbook of Numerical Analysis, vol. V, ed. by
P.G. Ciarlet, J.-L. Lions (North-Holland, Amsterdam, 1997), pp. 209–485
11. C. Bernardi, Y. Maday, F. Rapetti, Discrétisations variationnelles de problèmes aux limites
elliptiques, vol. 45, Mathématiques & Applications (Berlin) [Mathematics & Applications]
(Springer, Berlin, 2004)
12. E. Bohl, Finite Modelle gewöhnlicher Randwertaufgaben, vol. 51, Leitfäden der Angewandten
Mathematik und Mechanik [Guides to Applied Mathematics and Mechanics] (B. G. Teubner,
Stuttgart, 1981). Teubner Studienbücher: Mathematik. [Teubner Study Books: Mathematics]
13. N. Bourbaki, Topological Vector Spaces. Chapters 1–5, Elements of Mathematics (Berlin)
(Springer, Berlin, 1987). Translated from the French by H.G. Eggleston, S. Madan
14. S.C. Brenner, L.R. Scott, The Mathematical Theory of Finite Element Methods, vol. 15, 3rd
edn., Texts in Applied Mathematics (Springer, New York, 2008)
15. H. Brezis, Functional Analysis, Sobolev Spaces and Partial Differential Equations, Universitext
(Springer, New York, 2011)
16. J.C. Butcher, Numerical Methods for Ordinary Differential Equations, 2nd edn. (Wiley, New
York, 2008)
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8
383

384
References
17. C. Canuto, M.Y. Hussaini, A. Quarteroni, T.A. Zang, Spectral Methods. Fundamentals in Single
Domains, Scientiﬁc Computation (Springer, Berlin, 2006)
18. P.G. Ciarlet, Introduction to Numerical Linear Algebra and Optimisation, Cambridge Texts in
Applied Mathematics (Cambridge University Press, Cambridge, 1989)
19. P.G. Ciarlet, The Finite Element Method for Elliptic Problems, Classics in Applied Mathematics
(Society for Industrial and Applied Mathematics (SIAM), Philadelphia, 2002). Reprint of the
1978 original [North-Holland, Amsterdam; MR0520174 (58 #25001)]
20. P.G. Ciarlet, Linear and Nonlinear Functional Analysis With Applications (Society for Indus-
trial and Applied Mathematics, Philadelphia, 2013)
21. P.G. Ciarlet, J.-L. Lions (eds.), Handbook of Numerical Analysis. Finite Element Methods. Part
1, vol. II (North-Holland, Amsterdam, 1991)
22. R. Courant, D. Hilbert, Methods of Mathematical Physics. Partial Differential Equations, vol.
II, Wiley Classics Library (Wiley, New York, 1989). Reprint of the 1962 original, A Wiley-
Interscience Publication
23. M. Crouzeix, A.L. Mignot, Analyse numérique des équations différentielles, Collection Math-
ématiques Appliquées pour la Maîtrise. [Collection of Applied Mathematics for the Master’s
Degree] (Masson, Paris, 1984)
24. R. Dautray, J.-L. Lions, Mathematical Analysis and Numerical Methods for Science and Tech-
nology. Physical Origins and Classical Methods, vol. 1 (Springer, Berlin, 1990). With the
collaboration of P. Bénilan, M. Cessenat, A. Gervat, A. Kavenoky, H. Lanchon
25. R. Dautray, J.-L. Lions, Mathematical Analysis and Numerical Methods for Science and Tech-
nology. Functional and variational methods, vol. 2 (Springer, Berlin, 1988). With the collab-
oration of M. Artola, M. Authier, P. Bénilan, M. Cessenat, J.-M. Combes, H. Lanchon, B.
Mercier, C. Wild, C. Zuily
26. R. Dautray, J.-L. Lions, Mathematical Analysis, Numerical Methods, for Science and Technol-
ogy. Spectral Theory and Applications, vol. 3 (Springer, Berlin, 1990). With the collaboration
of Michel Artola and Michel Cessenat (Translated from the French by J.C. Amson)
27. R. Dautray, J.-L. Lions, Mathematical Analysis and Numerical Methods for Science and Tech-
nology. Integral Equations and Numerical Methods, vol. 4 (Springer, Berlin, 1990). With the
collaboration of M. Artola, P. Bénilan, M. Bernadou, M. Cessenat, J.-C. Nédélec, J. Planchard,
B. Scheurer
28. R. Dautray, J.-L. Lions, Mathematical Analysis and Numerical Methods for Science and
Technology. Evolution Problems. I, vol. 5 (Springer, Berlin, 1992). With the collaboration of
M. Artola, M. Cessenat, H. Lanchon
29. R. Dautray, J.-L. Lions, Mathematical Analysis and Numerical Methods for Science and Tech-
nology. Evolution Problems. II, vol. 6 (Springer, Berlin, 1993). With the collaboration of
C. Bardos, M. Cessenat, A. Kavenoky, P. Lascaux, B. Mercier, O. Pironneau, B. Scheurer,
R. Sentis
30. B. Després, Lois de conservations eulériennes, lagrangiennes et méthodes numériques, vol.
68, Mathématiques & Applications (Berlin) [Mathematics & Applications] (Springer, Berlin,
2010)
31. D.A. Di Pietro, A. Ern, Mathematical Aspects of Discontinuous Galerkin Methods, vol. 69,
Mathématiques & Applications (Berlin) [Mathematics & Applications] (Springer, Heidelberg,
2012)
32. J. Dieudonné, Foundations of Modern Analysis, vol. 10-I, Pure and Applied Mathematics
(Academic Press, New York, 1969). Enlarged and corrected printing
33. N. Dunford, J .T. Schwartz, Linear Operators. Part I. General Theory, Wiley Classics Library
(Wiley, New York, 1988). With the assistance of W. G. Bade, R. G. Bartle. Reprint of the 1958
original, A Wiley-Interscience Publication
34. G. Duvaut, J.-L. Lions, Inequalities in Mechanics and Physics, vol. 219, Grundlehren der
Mathematischen Wissenschaften (Springer, Berlin, 1976)
35. L.C. Evans, Partial Differential Equations, vol. 19, 2nd edn., Graduate Studies in Mathematics
(American Mathematical Society, Providence, 2010)

References
385
36. R. Eymard, R. Herbin, T. Gallouët, Finite volume methods, in Handbook of Numerical Analysis.
Solution of Equations in Rn. Part 3. Techniques of Scientiﬁc Computing, vol. VII, ed. by P.G.
Ciarlet, J.-L. Lions (North-Holland, Amsterdam, 2000), pp. 713–1020
37. H. Federer, Geometric Measure Theory, vol. 153, Die Grundlehren der mathematischen Wis-
senschaften, Band (Springer, New York, 1969)
38. J.-B.J. Fourier, Théorie analytique de la chaleur, Cambridge Library Collection (Cambridge
University Press, Cambridge, 2009). Reprint of the 1822 original, Previously published by
Éditions Jacques Gabay, Paris, 1988 [MR1414430]
39. P.-L. George, Automatic mesh generation and ﬁnite element computation, in Handbook of
Numerical Analysis, ed. by P.G. Ciarlet, J.-L. Lions (North-Holland, Amsterdam, 1996), pp.
69–190
40. D. Gilbarg, N.S. Trudinger, Elliptic Partial Differential Equations of Second Order, Classics
in Mathematics (Springer, Berlin, 2001). Reprint of the 1998 edition
41. V. Girault, P.-A. Raviart., Finite Element Methods for Navier-Stokes Equations. Theory and
Algorithms, vol. 5, Springer Series in Computational Mathematics (Springer, Berlin, 1986)
42. E. Godlewski, P.-A. Raviart, Numerical Approximation of Hyperbolic Systems of Conservation
Laws, vol. 118, Applied Mathematical Sciences (Springer, New York, 1996)
43. D. Gottlieb, S.A. Orszag, Numerical Analysis of Spectral Methods: Theory and Applications,
CBMS-NSF Regional Conference Series in Applied Mathematics (Society for Industrial and
Applied Mathematics, Philadelphia, 1977)
44. P. Grisvard, Elliptic Problems in Nonsmooth Domains, vol. 24, Monographs and Studies in
Mathematics (Pitman (Advanced Publishing Program), Boston, 1985)
45. C.Grossmann,H.-G.Roos,NumericalTreatmentofPartialDifferentialEquations,Universitext
(Springer, Berlin, 2007). Translated and revised from the 3rd (2005) German edition by Martin
Stynes
46. W. Hackbusch, Elliptic Differential Equations. Theory and Numerical Treatment, vol. 18,
Springer Series in Computational Mathematics (Springer, Berlin, 2010). Translated from the
1986 corrected German edition by R. Fadiman, P.D.F. Ion
47. W. Hackbusch, The Concept of Stability in Numerical Mathematics, vol. 45, Springer Series
in Computational Mathematics (Springer, Heidelberg, 2014)
48. E. Hairer, S.P. Nørsett, G. Wanner, Solving Ordinary Differential Equations. I. Nonstiff Prob-
lems, vol. 8, 2nd edn., Springer Series in Computational Mathematics (Springer, Berlin, 1993)
49. E. Hairer, G. Wanner, Solving Ordinary Differential Equations. II. Stiff and Differential-
Algebraic Problems, vol. 14, 2nd edn., Springer Series in Computational Mathematics
(Springer, Berlin, 1996)
50. J.S. Hesthaven, S. Gottlieb, D. Gottlieb, Spectral Methods for Time-Dependent Problems,
vol. 21, Cambridge Monographs on Applied and Computational Mathematics (Cambridge
University Press, Cambridge, 2007)
51. F. Hirsch, G. Lacombe, Elements of Functional Analysis, vol. 192, Graduate Texts in Mathe-
matics (Springer, New York, 1999)
52. C. Johnson, Numerical Solution of Partial Differential Equations by the Finite Element Method
(Dover Publications Inc., Mineola, 2009). Reprint of the 1987 edition
53. R. Kress, Numerical Analysis, vol. 181, Graduate Texts in Mathematics (Springer, New York,
1998)
54. P. Lancaster, M. Tismenetsky, The Theory of Matrices, 2nd edn., Computer Science and Applied
Mathematics (Academic Press Inc, Orlando, 1985)
55. P.D. Lax, A.N. Milgram, Parabolic Equations, Contributions to the Theory of Partial Differen-
tial Equations, vol. 33, Annals of Mathematics Studies (Princeton University Press, Princeton,
1954), pp. 167–190
56. R.J. LeVeque, Finite Volume Methods for Hyperbolic Problems, Cambridge Texts in Applied
Mathematics (Cambridge University Press, Cambridge, 2002)
57. J.-L. Lions, Quelques méthodes de résolution des problèmes aux limites non linéaires (Dunod,
Paris, 1969)

386
References
58. J.-L. Lions, E. Magenes, Non-homogeneous Boundary Value Problems and Applications, vol.
181, Die Grundlehren der mathematischen Wissenschaften (Springer, New York, 1972)
59. B. Lucquin, O. Pironneau, Introduction to Scientiﬁc Computing (Wiley, New York, 1998)
60. R. Mattheij, J. Molenaar, Ordinary Differential Equations in Theory and Practice, vol. 43,
Classics in Applied Mathematics (Society for Industrial and Applied Mathematics (SIAM),
Philadelphia, 2002). Reprint of the 1996 original
61. J. Neˇcas, Les méthodes directes en théorie des équations elliptiques (Masson et Cie, Éditeurs,
Paris, 1967)
62. O. Pironneau, Finite Element Methods for Fluids (Wiley, New York, 1989)
63. A. Quarteroni, Numerical Models for Differential Problems, vol. 2, MS&A. Modeling, Simu-
lation and Applications (Springer, Milan, 2009)
64. A. Quarteroni, R. Sacco, F. Saleri, Numerical Mathematics, vol. 37, 2nd edn., Texts in Applied
Mathematics (Springer, Berlin, 2007)
65. A. Quarteroni, A. Valli, Numerical Approximation of Partial Differential Equations, vol. 23,
Springer Series in Computational Mathematics (Springer, Berlin, 1994)
66. P.-A. Raviart, J.-M. Thomas, Introduction à l’analyse numérique des équations aux dérivées
partielles, Collection Mathématiques Appliquées pour la Maîtrise. [Collection of Applied
Mathematics for the Master’s Degree] (Masson, Paris, 1983)
67. R.D. Richtmyer, K.W. Morton, Difference Methods for Initial-value Problems, vol. 4, 2nd edn.,
Interscience Tracts in Pure and Applied Mathematics (Wiley, New York, 1967)
68. W. Rudin, Real and Complex Analysis, 3rd edn. (McGraw-Hill Book Co., New York, 1987)
69. W. Rudin, Functional Analysis, 2nd edn., International Series in Pure and Applied Mathematics
(McGraw-Hill Inc., New York, 1991)
70. A. Samarski, V. Andréev, Méthodes aux différences pour équations elliptiques, Traduit du russe
par Djilali Embarek (Éditions Mir, Moscow, 1978)
71. M. Schatzman, Numerical Analysis: A Mathematical Introduction (Oxford University Press,
Oxford, 2002). Translated from the French by J. Taylor
72. L. Schwartz, Théorie des distributions. Tome I, Actualités Sci. Ind., no. 1091 = Publ. Inst.
Math. Univ. Strasbourg 9 (Hermann & Cie, Paris, 1950)
73. L. Schwartz, Théorie des distributions. Tome II, Actualités Sci. Ind., no. 1122 = Publ. Inst.
Math. Univ. Strasbourg 10 (Hermann & Cie., Paris, 1951)
74. L. Schwartz, Mathematics for the Physical Sciences (Hermann, Paris, 1966)
75. J.C. Strikwerda, Finite Difference Schemes and Partial Differential Equations, 2nd edn. (Soci-
ety for Industrial and Applied Mathematics (SIAM), Philadelphia, 2004)
76. V. Thomée, Finite difference methods for linear parabolic equations, in Handbook of Numerical
Analysis, I, ed. by P.G. Ciarlet, J.-L. Lions (North-Holland, Amsterdam, 1990), pp. 5–196
77. V. Thomée, Galerkin Finite Element Methods for Parabolic Problems, vol. 25, 2nd edn.,
Springer Series in Computational Mathematics (Springer, Berlin, 2006)
78. F. Trèves, Basic Linear Partial Differential Equations (Dover Publications Inc, Mineola, 2006).
Reprint of the 1975 original
79. G. Windisch, M-Matrices In Numerical Analysis, vol. 115, Teubner-Texte zur Mathematik
[Teubner Texts in Mathematics] (BSB B. G. Teubner Verlagsgesellschaft, Leipzig, 1989). With
German, French and Russian summaries
80. K. Yosida, Functional Analysis, Classics in Mathematics (Springer, Berlin, 1995). Reprint of
the sixth (1980) edition
81. E. Zeidler, Nonlinear Functional Analysis and its Applications. II/A. Linear Monotone Oper-
ators (Springer, New York, 1990)
82. O.C. Zienkiewicz, R.L. Taylor, The Finite Element Method, vol. 1, 5th edn. (Butterworth-
Heinemann, Oxford, 2000)

Index
A
Abstract variational approximation meth-
ods, 145, 302
Abstract variational problem, 122, 123
Adjoint variational problem, 157
Ampliﬁcation coefﬁcient, 291, 295, 364, 368
Ampliﬁcation matrix, 270, 280, 321, 328,
336, 342, 343
Approximation method for ODEs
backward Euler, 6, 304
forward Euler, 6, 9, 255, 304, 380
Runge-Kutta, 6, 9, 304
A priori error estimate, 147, 151, 179
Aubin-Nitsche duality trick, 157
B
Backward differential quotient, 254
Backward Euler three point scheme, see
implicit Euler three point scheme
Backward heat equation, 32, 225
Banach’s theorem, 125
Barycentric coordinates, 198, 199, 206
equation of a straight line in, 201
invariance under afﬁne transformation
of, 202
of points of interest, 201
triangle in, 200
Basis functions
of 1d P1 Lagrange interpolation, 154,
186
of 1d P3 Hermite interpolation, 164
of 2d Q1 Lagrange interpolation, 176,
177, 183, 186
of 2d Q2 Lagrange interpolation, 195
Basis polynomials
of 1d P1 Lagrange interpolation, 185
of 2d P1 Lagrange interpolation, 205
of 2d P2 Lagrange interpolation, 211
of 2d P3 Lagrange interpolation, 213
of 2d Q1 Lagrange interpolation, 179,
185
of 2d Q2 Lagrange interpolation, 193
of a general ﬁnite element, 191
Bilaplacian, 18, 140
Bisection method, 9
Black and Scholes equation, 31
Boundary
characteristic part, 375
incoming part, 375, 381
outgoing part, 375, 380
Boundary condition, 5
Dirichlet, 6, 21, 107, 110, 121, 219, 345,
375, 381
Fourier, 133, 134, 143, 165
homogeneous Dirichlet, 6, 12, 16, 25,
117, 174, 235, 237, 308, 311, 345
homogeneous Neumann, 120
incoming, 377, 379
mixed, 121, 129
Neumann, 48, 120, 128, 129, 143, 165
periodic, 21
Robin, see Fourier boundary condition
third, see Fourier boundary condition
Boundary measure, 80
© Springer International Publishing Switzerland 2016
H. Le Dret and B. Lucquin, Partial Differential Equations:
Modeling, Analysis and Numerical Approximation, International Series
of Numerical Mathematics 168, DOI 10.1007/978-3-319-27067-8
387

388
Index
Boundary value problem, 5, 117
Bubble, 213
Burgers equation, 353, 373
C
Ck([0, T ]; V ) spaces, 230
Ck() spaces, 72
Ck( ¯) spaces, 73
Ck,β( ¯) spaces, 74
Cm,n( ¯Q) spaces, 257
Call option, 31
Cauchy–Lipschitz theorem, 353, 377
Cauchy problem, 6, 8, 240, 314, 351
Cauchy–Schwarz inequality, 69, 72, 75, 101,
105–107, 113, 127, 129, 235, 239,
241, 244, 299, 300, 315, 350
Céa’s lemma, 146, 147, 151, 162, 177
Center of gravity, 192, 193, 212
Central approximation, 254
Central differential quotient, 259
Characteristics, 20, 21, 352–354, 373, 375,
376, 380
backward, 369
method of, 20, 353
Compact mapping, 74
Compactness-contradiction argument, 132,
135
Condition number of a matrix, 55, 333, 334,
340, 343
Cone of inﬂuence
continuous, 319
discrete, 322, 369
Conforming approximation, 145, 154
Conservation law, 18
Consistency
of a ﬁnite difference scheme for the wave
equation, 321
of a ﬁnite volume scheme, 356, 365, 366
of a general scheme for a family of
norms, 263
of the Crank-Nicolson scheme, 297
of the explicit Euler scheme, 257
of the implicit Euler scheme, 259
of the leapfrog method, 260
of the θ-scheme, 296
Consistent approximation, 36, 37, 347
Continuity of the differentiation in the sense
of D′, 96
Control volume, see ﬁnite volume cell
Convection–diffusion problem, 22, 139
Convergence
ﬁnite difference method, 41, 266, 278
ﬁnite volume scheme, 349, 357, 359
in the sense of D(), 91
in the sense of D′(), 94
of a general scheme for a family of
norms, 264
of the Q1 FEM, 179
of the Crank-Nicolson scheme for the
2, h norms, 300
of the explicit Euler scheme for the 2, h
norms, 278
of the explicit Euler scheme for the ∞, h
norms, 266
of the ﬁnite difference method, 43
of the implicit Euler scheme for the 2, h
norms, 279
Convolution, 82, 91, 110, 115
Courant–Friedrichs–Lewy (CFL) condition,
322, 357, 360, 369–372
violation of, 325
Courant number, 365, 367
Crank-Nicolson scheme, 297
consistency, 297
order, 297
unconditional convergence, 300
unconditional stability, 299
D
D() spaces, 73, 91
D′() spaces, 91
Dahlquist’s zero-stability condition, 272,
304
D’Alembert, Jean le Rond, 23
D’Alembert’s formula, 317
Degree of a polynomial
partial, 173
total, 173
Degrees of freedom, 304
1d P1 Lagrange, 156, 165
1d P3 Hermite, 161
2d P1 Lagrange triangle, 204
2d P2 Lagrange triangle, 209
2d P3 Lagrange triangle, 212
2d Q1 Lagrange rectangle, 177
2d Q2 Lagrange rectangle, 192
2d Q3 Hermite rectangle, 198
2d Q3 Lagrange rectangle, 197
general element, 189
Difference quotient, 253
Differential quotient, 35, 36
for the second derivative, 37
Diffusion equation, 17
Dirac mass, 93, 247

Index
389
Dirichlet’s theorem, 223
Discrete Fourier coefﬁcients, 285
Discrete Fourier transform, 282
Discrete maximum principle, 45, 62, 269,
370
Discrete Poincaré inequality, 299, 300
Dispersivity, 344
Dissipativity, 344
Distributional derivative, 149
Distributional partial derivatives, 94, 119,
171, 247
Distributional primitive, 96
Distributions, 90
convergence in the sense of, 94
multiplication by a smooth function, 96
partial derivative, 94
Duhamel’s formula, 240
E
Eigenvalue problem, 26, 243
Eigenvalues, 274, 279, 281, 313, 325, 329,
334, 336, 340, 343
of a tridiagonal matrix, 276
Elastic beam, 10
Elastic membrane equation, 12, 16
Elastic string, 1
Element, 149
generic, 180, 206
reference, 179, 180, 184, 206
Elementary solution, see fundamental solu-
tion
Elliptic equation, 33
Elliptic regularity theory, 120, 142, 143, 154,
183
Energy
conservation (wave eqn.) versus dissipa-
tion (heat eqn.), 309
heat equation, 236
wave equation, 308
Energy estimate
heat equation, 236
wave equation, 310
Energy functional, 128
Equation
Black and Scholes, 31
Burgers, 353, 373
diffusion, 17
elastic membrane, 16
elliptic, 33
heat, 27, 33, 133, 219
hyperbolic, 33, 309
Laplace, 17
Maxwell, 27
parabolic, 33
plate, 17
Poisson, 17, 30, 33, 117
Schrödinger, 30
string, 5
transport, 18, 34, 351, 375
vibrating string, 22
wave, 23, 26, 33, 307
Explicit Euler three point scheme, 254
consistency, 257
convergence for the 2, h norms, 278
convergence for the ∞, h norms, 266
divergence when stability is not satisﬁed,
267
error estimate, 269, 278
stability for the 2, h norms, 278
stability for the ∞, h norms, 266
F
Finite
difference
approximation
of
the
Laplacian, 59, 66
Finite difference method
convergence, 41
convergence of order p, 41
Finite difference scheme
central, 53, 59
consistent, 42, 257, 321
consistent of order p, 42
convergent, 43
explicit, 261
explicit Euler, 254
ﬁve point scheme for the Laplacian, 59
for the heat equation, 253
for the wave equation, 320
general l + m step, 260
implicit, 261
implicit Euler, 259
leapfrog, 259
stencil, 59
θ-scheme, 296, 325, 338
three point scheme, 39, 58, 66
truncation error, 262
Finite difference-ﬁnite element schemes,
302
Finite element method (FEM), 148
in dimension one, 149
in dimension two, 167
Finite elements
1d P1 Lagrange, 149
1d P3 Hermite, 159
2d rectangular Q1 Lagrange, 172, 174,
190

390
Index
2d rectangular Q2 Lagrange, 191
2d rectangular Q3 Hermite rectangle,
197
2d rectangular Q3 Lagrange, 197
2d triangular P1 Lagrange, 204
2d triangular P3 Lagrange, 212
general, 189
Finite volume cell, 345, 346, 354, 377–379,
381
Finite volume method
elliptic case in dimension one, 345
transport equation in dimension one, 354
transport equation in dimension two, 377
Finite volume scheme, 347
centered, 371
conservative, 362, 366, 382
consistency, 365, 366
consistency condition, 356
convergence, 349, 357, 359
convergence in ℓ∞, 369, 370
decentered, see upwind
explicit Euler, 355, 380
explicit three point, 361
in conservation form, see conservative
monotone, 364, 371
nonlinear, 373
order, 365
order of a, 365, 366
stability, 363
stability in ℓ2, 364, 367, 370–372
stability in ℓ∞, 363, 368, 370, 371
truncation error, 349, 365, 366
viscosity coefﬁcient, 367, 370–372
von Neumann condition, 364, 365
Flux, 352, 355, 380
nonlinear, 362, 373
numerical, 347, 355, 356, 362, 364, 366,
367, 371, 372, 380
Flux function, 353
Forward differential quotient, 254
Forward Euler method for ordinary differen-
tial equations, 255, 380
Forward Euler three point scheme, see
explicit Euler three point scheme
Fourier law, 29
Fourier series, 222, 286, 311, 327, 364
Fourier transform, 33, 327
continuous, 294
discrete, 282
Fourier, Joseph, 27
FreeFem++, 215
Fubini’s theorem, 88, 89, 98, 102, 106, 232
Functions with compact support, 73
Fundamental solution, 247
Fundamental tone, 25, 26
G
Galerkin method, 145
Gårding’s inequality, 242
Gaussian, 246
Gaussian elimination, 64
Generic constant, 179
Gibbs phenomenon, 228, 229
Gradient, 103
Green function, 38
Green’s formula, 118, 120, 130, 134, 140,
142
for C1 functions, 90
for Sobolev functions, 114
Grid
points, 38, 58, 149, 253
space step, 39, 58, 253
space-time, 253, 284
time step, 253
Grid sampling operator, 41, 50, 262
Gronwall’s
inequality,
see
Gronwall’s
lemma
Gronwall’s lemma, 309, 310
H
H−1() space, 238
H1/2(∂) space, 115
Hm() spaces, 99
Hm
0 () spaces, 101
Harmonic functions, 17
Harmonic vibrations, 24
Harmonics, 25, 26, 311
Hat functions, 154, 303
Heat conductivity, 29
Heat equation, 27, 33, 133, 219
backward, 32, 225
energy, 236
exponential decay
of the energy, 237, 245
uniform, 225, 227
ﬁnite difference method for the, 253
irreversibility of the, 225
maximum principle, 220, 222, 227
monotonicity, see maximum principle
on R, 245
propagation of the energy at inﬁnite
speed, 250
regular solution, 223
semi-discrete ﬁnite difference scheme,
292

Index
391
smoothing effect, 225, 227
stability
in the C0 norm, 222
in the C0([0, T ]; L2()) norm, 237
stability of a ﬁnite difference scheme in
the sense of von Neumann, 285, 291,
295
stability of a ﬁnite difference scheme via
Fourier series, 286
stability of a ﬁnite difference scheme via
the continuous Fourier transform, 292
stability of a ﬁnite difference scheme via
the discrete Fourier transform, 282
uniqueness, 222, 236
variational formulation, 239
weak solution, 240
Heat ﬂux, 28, 120
Heat kernel, 245, 247, 249
Heaviside function, 95, 100
Hilbert basis, 233, 239, 314
Hilbert-valued function
measurable, 231
of class Ck, 230
simple, 230
Hölder
functions, 74
inequality, 75, 94
Hyperbolic equation, 33, 309
Hyperbolic systems, 341, 353
Hypoellipticity, 237, 250
I
Ill-conditioned matrix, 55
Implicit Euler three point scheme, 259
consistency for the ∞, h norms, 259
convergence for the 2, h norms, 279
error estimate, 279
stability for the 2, h norms, 279
Indeﬁnitely differentiable, compactly sup-
ported functions, 73, 91
Induced matrix norm, 43, 265
2, h, 274
∞, h, 266
Inf-sup condition, 125
Initial-boundary value problem, 21, 219,
223, 237, 307, 313, 376
Initial condition, 18, 219, 254, 255, 308, 320,
356, 357, 376, 380
Initial value problem, 19
Inscribed circle, 169, 173
Integration by parts formula, 171
Integration by parts in Rd
in C1( ¯), 87, 95
in H1(), 114
Interpolation operator, 151, 161, 178
Q1, 179
Interpolation property, 155, 164, 190
Inverse nonnegative matrix, 44, 45, 51, 52,
63
J
Jordan decomposition, 341
K
Kinetic description, 18
L
L p(0, T ; V ) spaces, 231
L p() spaces, 74
L p
loc() spaces, 76
Lagrange interpolation, 155, 156, 178, 192,
209
Laplace equation, 17
Laplace operator, 16, 33
Laplacian, 16
Lax–Friedrichs scheme, 341, 371, 373
Lax–Milgram theorem, 122, 127, 133, 135,
138, 143, 146
Lax–theorem, 264
converse of, 272
Lax–Wendroff scheme, 342, 367, 368, 373
Leapfrog method
heat equation, 259
instability for the 2, h norms, 281
non convergence for the 2, h norms,
281
wave equation, 343
stability, 343
Lebesgue
dominated convergence theorem, 76,
248, 249
monotone convergence theorem, 245
points theorem, 76
Lebesgue spaces
Hilbert-valued, 231
real valued, 74
Leibniz formula, 97, 138
Lions’s lemma, 105
Localization of a function, 87
M
Matrix

392
Index
ampliﬁcation, 270, 321, 328, 336
assembling of the, 148, 183
band, 165
block tridiagonal, 60, 186
condition number, 55
full, 154
Galerkin approximation, 148
ill-conditioned, 55, 154
inverse nonnegative, 44, 45, 51, 52, 63
Jordan form, 341
mass, 304
M-matrix, 45
monotone, 44
nonnegative, 44
normal, 274, 333
orthogonal, 274
positive deﬁnite, 40, 61
sparse, 41, 154, 304, 381
spectral radius, 274, 278, 280
stiffness, 304
tridiagonal, 41, 156, 255, 276
uniformly elliptic, 138
unitary, 275
well-conditioned, 55
Maximum principle, 9, 11
discrete, 45, 62, 269, 370
for the heat equation, 220, 227
for the Poisson equation, 17
for the transport equation, 360
Maxwell’s equations, 27
Medit, 217
Mesh
generation, 168
Mesh 1d, 149
ﬁnite volume, 349, 361
node
boundary, 149
interior, 149
size, 149
uniform, 149
Mesh 2d, 167
admissible, 169, 171, 205, 377
edge, 167, 171, 211, 378, 382
boundary, 177, 379
internal, 176, 196, 378
middle of, 193
element, 167
ﬁnite volume, 377
node, 168, 192, 193
boundary, 173, 177
interior, 173, 177, 183, 193
rectangular, 172
regular family, 170, 173, 178, 182, 183,
214
size, 169, 172
triangular, 204
unstructured triangular, 205
vertex, 168, 193
Meyers-Serrin theorem, 112
Minimization problem, 125, 128, 138, 142
Minty’s trick, 126
M-matrix, 45
Modeling error, 5, 27
Modeling hypothesis, 1, 3, 4, 6, 31
Molliﬁer, 83, 91, 110
Monotone matrix, 44
Multiindex notation for partial derivatives,
72
Multiplier operator, 290, 328
N
Newton’s law, 2, 4, 14, 22, 308
Nodal values, 151, 156, 162, 163, 178, 193
Node, 149, 164, 174, 192, 211
Non conforming approximation spaces, 146
Norm, 265
induced matrix, 43, 265, 274
operator, 43, 265, 274
submultiplicative, 44, 55, 272
Normal derivative, 18
Normal matrix, 274, 333
Normal trace, 114, 121
Normal unit exterior vector, 79, 80, 87, 171,
375, 378
Numbering
of cells, 381
of elements, 173
of nodes, 173, 176, 183, 184, 205
Numericalﬂux,347,355,356,362,364,366,
367, 371, 372, 380
Numerical integration, 157
Numerical methods for linear systems, 157
Numerical viscosity, 367
O
Open subset of Rd
Lipschitz, 78
of class Ck,β, 78
Operator norm, 43, 265, 274
Order
of a ﬁnite volume scheme, 365
of a general scheme for a family of
norms, 263
of the Crank-Nicolson scheme, 297

Index
393
of the explicit Euler scheme, 257
of the implicit Euler scheme, 259
of the leapfrog method, 260
of the θ-scheme, 296
Orthogonal projection
on a closed convex set, 70
on a closed vector subspace, 70
P
P1 Lagrange basis polynomials, 205
P2 Lagrange basis polynomials, 211
P3 Hermite
1d basis polynomials, 161, 164
1d interpolation, 161, 162, 164
P3 Lagrange basis polynomials, 213
Pk space, 173
Parabolic equation, 33
Parseval’s formula, 287, 329
Parseval’s identity, 234, 241
Partitions of unity, 82, 115
PDE, v
Picard–Lindelöf theorem, 353, 377
Piecewise polynomial functions, 149
Pivot space, 72
Plancherel’s formula, 294
Plate equation, 17
Poincaré inequality, 101, 103, 127, 141, 235,
237, 315
discrete, 299, 300
Poincaré–Wirtinger inequality, 131, 132,
136
Poisson equation, 17, 30, 33, 117
probabilistic interpretation of the, 17
Preconditioning, 57
Pricing, 31
Principal value of 1/x, 94
Propagation speed, 23, 307, 351
Q
Q1 Lagrange
basis polynomials, 179, 184
interpolation operator, 179
Q2 Lagrange basis polynomials, 193
Qk space, 173
Quantity of heat, 28
R
Rademacher’s theorem, 79
Rectangular Q1 ﬁnite elements, 172, 174
Rectangular Q2 ﬁnite elements, 191
Rectangular Q3 ﬁnite elements, 197
Reference
element, 161, 179, 180, 184
rectangle, 179
triangle, 206, 214
Rellich theorem, 108, 132, 135, 180
Richardson method, see leapfrog method
Riesz theorem, 71
vs. Lax–Milgram theorem, 124
Rolle’s theorem, 152, 163
S
Scheme
Crank-Nicolson, 297
explicit Euler, 255
implicit Euler, 259
Lax–Friedrichs, 341, 371, 373
Lax–Wendroff, 342, 367, 368, 373
leapfrog, 259
θ, 291, 325
upwind, 356, 357, 360, 364, 370, 373,
374, 380, 382
Schrödinger equation, 30
Schur decomposition, 341
Scilab, 47, 54
Secant method, 9
Second order operator, 137
Semi-discrete scheme
explicit Euler, 292
ﬁnite difference, 292
ﬁnite volume, 355
relation with discrete scheme, 293
stability in L2(R), 294
explicit Euler scheme, 295
θ-scheme, 296
Separation of variables, 24
Shape functions, 303
1d P3 Hermite, 164
2d P1 Lagrange, 205
2d P2 Lagrange, 211
2d P3 Lagrange, 213
2d Q1 Lagrange, 179, 184
2d Q2 Lagrange, 193
general ﬁnite element, 191
Shock wave, 324, 354, 375
Shooting method, 6, 8
Sobolev embedding, 105, 178
Sobolev spaces, 99
density of smooth functions, 110
in one dimension, 105
Space grid sampling operator, 41, 256
Space-time grid, 253, 284
Spaces

394
Index
Ck(), 72
Pk, 173
Qk, 173
Ck([0, T ]; V ), 230
Ck( ¯), 73
Ck,β( ¯), 74
Cm,n( ¯Q), 257
D(), 73, 91
D′(), 91
H1/2(∂), 115
Hm(), 99
Hm
0 (), 101
L p(0, T ; V ), 231
L p(), 74
L p
loc(), 76
W m,p(), 99
Speciﬁc heat, 29
Spectral methods, 148, 226
Spectral radius, 274, 278, 280, 325, 342, 343
Spectral theory, 26
Stability
of a ﬁnite difference scheme for an ellip-
tic problem, 46, 51, 62
of a ﬁnite volume scheme, 363
of a general scheme for a family of
norms, 263
of the θ-scheme on ℓ2(Z), 292
of the Crank-Nicolson scheme for the
2, h norms, 299
of the explicit Euler scheme for the 2, h
norms, 278
of the explicit Euler scheme for the ∞, h
norms, 266
of the explicit Euler scheme on ℓ2(Z),
290
of the implicit Euler scheme for the 2, h
norms, 279
unconditional, 263, 299, 338
via Fourier series, 286, 327, 364
via the continuous Fourier transform,
292, 332
via the discrete Fourier transform, 282
via the energy method, 297
Stability in the sense of von Neumann
of a discrete scheme, 285, 328
of a scheme on ℓ2(Z), 291
of a semi-discrete scheme, 295
of the discrete explicit Euler scheme, 286
of the discrete implicit Euler scheme, 286
of the θ-scheme, 292, 296
Stiffness matrix, 304
Stokes formula, 90, 379
String problem, 5
Summation by parts formula, 298
Support of a function, 73
T
Temperature, 27
Tension
of an elastic membrane, 13
of an elastic string, 2, 5
Tensor product of two functions, 185
Test-function, 104, 118, 131, 142, 143, 240,
314
Test-function space, 120, 142, 143
Tetrahedron, 189
θ-scheme
for the heat equation
consistency, 296
discrete, 291, 295
for θ
=
1
2, see Crank-Nicolson
scheme
order, 296
semi-discrete, 295
for the wave equation, 325, 338
Three point centered approximation, 255
Total family, 233
Trace mapping, 110, 114, 129
continuity of the, 136
in one dimension, 107
Trace space, 115
Trace theorem, 112
in one dimension, 107
Transport equation, 18, 34, 351, 375
nonlinear, 353
Transport operator, 23
Triangular P1 Lagrange ﬁnite elements, 204
Triangular P2 Lagrange ﬁnite elements, 209
Triangular P3 Lagrange ﬁnite elements, 212
Triangulation, see mesh 2d
Truncation error, 42, 54, 67
ﬁnite volume, 349, 365, 366
of a general scheme, 262
of the forward Euler scheme, 256
of the implicit Euler scheme for the ∞, h
norms, 259
of the leapfrog method, 260
U
Unconditional stability, 263, 279, 338
Unisolvence, 190, 193
P1 Lagrange triangle, 205
P2 Lagrange triangle, 210
P3 Lagrange triangle, 212
Q1 Lagrange rectangle, 190

Index
395
Q2 Lagrange rectangle, 192
Upwind scheme, 356, 357, 360, 364, 370,
373, 374, 380, 382
V
V -ellipticity, 122, 124, 125, 127, 129, 132,
135, 139, 141, 143
constant, 125, 146
Vh-interpolate, 151
Vh-interpolation, 163
Vh-interpolation operator, 151, 161, 182
Variation of the constant, 240
Variational formulation, 142
of a general second order problem, 137
of the convection–diffusion problem,
139
of the Dirichlet problem, 118
of the Fourier problem, 133
of the heat equation, 239
of the mixed problem, 122
of the Neumann problem, 120, 130
of the plate problem, 140
of the wave equation, 313
Vibrating string equation, 22, 308
Von Neumann stability, see stability in the
sense of von Neumann
W
W m,p() spaces, 99
completeness of, 100
Wave equation, 23, 26, 33, 307
cone of inﬂuence, 319
discrete cone of inﬂuence, 322
energy, 308
derivative of the, 309, 317
ﬁnite difference method for the, 320
no smoothing effect, 313
on R, 317
propagation at ﬁnite speed, 319
regular solution, 311
reversibility of the, 320
semi-discrete ﬁnite difference scheme,
332
shock, 324
stability in the energy norm, 310, 316
stability of a ﬁnite difference scheme in
the sense of von Neumann, 328
variational formulation, 313
weak solution, 314
Well-conditioned matrix, 55
Y
Young modulus, 11
Young’s inequality, 235


TEAM LinG

A Computational Introduction to Number Theory
and Algebra
(Version 1)
Victor Shoup
TEAM LinG

TEAM LinG

This PDF document contains hyperlinks, and one may navigate through it
by clicking on theorem, deﬁnition, lemma, equation, and page numbers, as
well as URLs, and chapter and section titles in the table of contents; most
PDF viewers should also display a list of “bookmarks” that allow direct
access to chapters and sections.
TEAM LinG

Copyright c⃝2005 by Victor Shoup
<victor@shoup.net>
All rights reserved. The right to publish or distribute this work in print form
belongs exclusively to Cambridge University Press; however, this electronic
version is distributed under the terms and conditions of a Creative Commons
license (Attribution-NonCommercial-NoDerivs 2.0):
You are free to copy, distribute, and display this electronic
version under the following conditions:
Attribution. You must give the original author credit.
Noncommercial. You may not use this electronic version for
commercial purposes.
No Derivative Works. You may not alter, transform, or
build upon this electronic version.
For any reuse or distribution, you must make clear to others
the license terms of this work.
Any of these conditions can be waived if you get permission
from the author.
For more information about the license, visit
creativecommons.org/licenses/by-nd-nc/2.0.
TEAM LinG

Contents
Preface
page x
Preliminaries
xiv
1
Basic properties of the integers
1
1.1
Divisibility and primality
1
1.2
Ideals and greatest common divisors
4
1.3
Some consequences of unique factorization
8
2
Congruences
13
2.1
Deﬁnitions and basic properties
13
2.2
Solving linear congruences
15
2.3
Residue classes
20
2.4
Euler’s phi function
24
2.5
Fermat’s little theorem
25
2.6
Arithmetic functions and M¨obius inversion
28
3
Computing with large integers
33
3.1
Asymptotic notation
33
3.2
Machine models and complexity theory
36
3.3
Basic integer arithmetic
39
3.4
Computing in Zn
48
3.5
Faster integer arithmetic (∗)
51
3.6
Notes
52
4
Euclid’s algorithm
55
4.1
The basic Euclidean algorithm
55
4.2
The extended Euclidean algorithm
58
4.3
Computing modular inverses and Chinese remaindering
62
4.4
Speeding up algorithms via modular computation
63
4.5
Rational reconstruction and applications
66
4.6
Notes
73
v
TEAM LinG

vi
Contents
5
The distribution of primes
74
5.1
Chebyshev’s theorem on the density of primes
74
5.2
Bertrand’s postulate
78
5.3
Mertens’ theorem
81
5.4
The sieve of Eratosthenes
85
5.5
The prime number theorem . . . and beyond
86
5.6
Notes
94
6
Finite and discrete probability distributions
96
6.1
Finite probability distributions: basic deﬁnitions
96
6.2
Conditional probability and independence
99
6.3
Random variables
104
6.4
Expectation and variance
111
6.5
Some useful bounds
117
6.6
The birthday paradox
121
6.7
Hash functions
125
6.8
Statistical distance
130
6.9
Measures of randomness and the leftover hash lemma (∗)
136
6.10 Discrete probability distributions
141
6.11 Notes
147
7
Probabilistic algorithms
148
7.1
Basic deﬁnitions
148
7.2
Approximation of functions
155
7.3
Flipping a coin until a head appears
158
7.4
Generating a random number from a given interval
159
7.5
Generating a random prime
162
7.6
Generating a random non-increasing sequence
167
7.7
Generating a random factored number
170
7.8
The RSA cryptosystem
174
7.9
Notes
179
8
Abelian groups
180
8.1
Deﬁnitions, basic properties, and examples
180
8.2
Subgroups
185
8.3
Cosets and quotient groups
190
8.4
Group homomorphisms and isomorphisms
194
8.5
Cyclic groups
202
8.6
The structure of ﬁnite abelian groups (∗)
208
9
Rings
211
9.1
Deﬁnitions, basic properties, and examples
211
9.2
Polynomial rings
220
TEAM LinG

Contents
vii
9.3
Ideals and quotient rings
231
9.4
Ring homomorphisms and isomorphisms
236
10
Probabilistic primality testing
244
10.1 Trial division
244
10.2 The structure of Z∗
n
245
10.3 The Miller–Rabin test
247
10.4 Generating random primes using the Miller–Rabin test
252
10.5 Perfect power testing and prime power factoring
261
10.6 Factoring and computing Euler’s phi function
262
10.7 Notes
266
11
Finding generators and discrete logarithms in Z∗
p
268
11.1 Finding a generator for Z∗
p
268
11.2 Computing discrete logarithms Z∗
p
270
11.3 The Diﬃe–Hellman key establishment protocol
275
11.4 Notes
281
12
Quadratic residues and quadratic reciprocity
283
12.1 Quadratic residues
283
12.2 The Legendre symbol
285
12.3 The Jacobi symbol
287
12.4 Notes
289
13
Computational problems related to quadratic residues
290
13.1 Computing the Jacobi symbol
290
13.2 Testing quadratic residuosity
291
13.3 Computing modular square roots
292
13.4 The quadratic residuosity assumption
297
13.5 Notes
298
14
Modules and vector spaces
299
14.1 Deﬁnitions, basic properties, and examples
299
14.2 Submodules and quotient modules
301
14.3 Module homomorphisms and isomorphisms
303
14.4 Linear independence and bases
306
14.5 Vector spaces and dimension
309
15
Matrices
316
15.1 Basic deﬁnitions and properties
316
15.2 Matrices and linear maps
320
15.3 The inverse of a matrix
323
15.4 Gaussian elimination
324
15.5 Applications of Gaussian elimination
328
TEAM LinG

viii
Contents
15.6 Notes
334
16
Subexponential-time discrete logarithms and factoring
336
16.1 Smooth numbers
336
16.2 An algorithm for discrete logarithms
337
16.3 An algorithm for factoring integers
344
16.4 Practical improvements
352
16.5 Notes
356
17
More rings
359
17.1 Algebras
359
17.2 The ﬁeld of fractions of an integral domain
363
17.3 Unique factorization of polynomials
366
17.4 Polynomial congruences
371
17.5 Polynomial quotient algebras
374
17.6 General properties of extension ﬁelds
376
17.7 Formal power series and Laurent series
378
17.8 Unique factorization domains (∗)
383
17.9 Notes
397
18
Polynomial arithmetic and applications
398
18.1 Basic arithmetic
398
18.2 Computing minimal polynomials in F[X]/(f) (I)
401
18.3 Euclid’s algorithm
402
18.4 Computing modular inverses and Chinese remaindering
405
18.5 Rational function reconstruction and applications
410
18.6 Faster polynomial arithmetic (∗)
415
18.7 Notes
421
19
Linearly generated sequences and applications
423
19.1 Basic deﬁnitions and properties
423
19.2 Computing minimal polynomials: a special case
428
19.3 Computing minimal polynomials: a more general case
429
19.4 Solving sparse linear systems
435
19.5 Computing minimal polynomials in F[X]/(f) (II)
438
19.6 The algebra of linear transformations (∗)
440
19.7 Notes
447
20
Finite ﬁelds
448
20.1 Preliminaries
448
20.2 The existence of ﬁnite ﬁelds
450
20.3 The subﬁeld structure and uniqueness of ﬁnite ﬁelds
454
20.4 Conjugates, norms and traces
456
TEAM LinG

Contents
ix
21
Algorithms for ﬁnite ﬁelds
462
21.1 Testing and constructing irreducible polynomials
462
21.2 Computing minimal polynomials in F[X]/(f) (III)
465
21.3 Factoring polynomials: the Cantor–Zassenhaus algorithm
467
21.4 Factoring polynomials: Berlekamp’s algorithm
475
21.5 Deterministic factorization algorithms (∗)
483
21.6 Faster square-free decomposition (∗)
485
21.7 Notes
487
22
Deterministic primality testing
489
22.1 The basic idea
489
22.2 The algorithm and its analysis
490
22.3 Notes
500
Appendix: Some useful facts
501
Bibliography
504
Index of notation
510
Index
512
TEAM LinG

Preface
Number theory and algebra play an increasingly signiﬁcant role in comput-
ing and communications, as evidenced by the striking applications of these
subjects to such ﬁelds as cryptography and coding theory. My goal in writ-
ing this book was to provide an introduction to number theory and algebra,
with an emphasis on algorithms and applications, that would be accessible
to a broad audience. In particular, I wanted to write a book that would be
accessible to typical students in computer science or mathematics who have
a some amount of general mathematical experience, but without presuming
too much speciﬁc mathematical knowledge.
Prerequisites. The mathematical prerequisites are minimal: no particular
mathematical concepts beyond what is taught in a typical undergraduate
calculus sequence are assumed.
The computer science prerequisites are also quite minimal: it is assumed
that the reader is proﬁcient in programming, and has had some exposure
to the analysis of algorithms, essentially at the level of an undergraduate
course on algorithms and data structures.
Even though it is mathematically quite self contained, the text does pre-
suppose that the reader is comfortable with mathematical formalism and
has some experience in reading and writing mathematical proofs.
Read-
ers may have gained such experience in computer science courses such as
algorithms, automata or complexity theory, or some type of “discrete math-
ematics for computer science students” course. They also may have gained
such experience in undergraduate mathematics courses, such as abstract or
linear algebra—these courses overlap with some of the material presented
here, but even if the reader already has had some exposure to this material,
it nevertheless may be convenient to have all of the relevant material easily
accessible in one place, and moreover, the emphasis and perspective here
x
TEAM LinG

Preface
xi
will no doubt be diﬀerent than in a typical mathematics course on these
subjects.
Structure of the text. All of the mathematics required beyond basic cal-
culus is developed “from scratch.” Moreover, the book generally alternates
between “theory” and “applications”: one or two chapters on a particular
set of purely mathematical concepts are followed by one or two chapters on
algorithms and applications—the mathematics provides the theoretical un-
derpinnings for the applications, while the applications both motivate and
illustrate the mathematics. Of course, this dichotomy between theory and
applications is not perfectly maintained: the chapters that focus mainly
on applications include the development of some of the mathematics that
is speciﬁc to a particular application, and very occasionally, some of the
chapters that focus mainly on mathematics include a discussion of related
algorithmic ideas as well.
In developing the mathematics needed to discuss certain applications, I
tried to strike a reasonable balance between, on the one hand, presenting
the absolute minimum required to understand and rigorously analyze the
applications, and on the other hand, presenting a full-blown development
of the relevant mathematics. In striking this balance, I wanted to be fairly
economical and concise, while at the same time, I wanted to develop enough
of the theory so as to present a fairly well-rounded account, giving the reader
more of a feeling for the mathematical “big picture.”
The mathematical material covered includes the basics of number theory
(including unique factorization, congruences, the distribution of primes, and
quadratic reciprocity) and abstract algebra (including groups, rings, ﬁelds,
and vector spaces). It also includes an introduction to discrete probability
theory—this material is needed to properly treat the topics of probabilistic
algorithms and cryptographic applications. The treatment of all these topics
is more or less standard, except that the text only deals with commutative
structures (i.e., abelian groups and commutative rings with unity)—this is
all that is really needed for the purposes of this text, and the theory of these
structures is much simpler and more transparent than that of more general,
non-commutative structures.
The choice of topics covered in this book was motivated primarily by
their applicability to computing and communications, especially to the spe-
ciﬁc areas of cryptography and coding theory. For example, the book may
be useful for reference or self-study by readers who want to learn about
cryptography. The book could also be used as a textbook in a graduate
TEAM LinG

xii
Preface
or upper-division undergraduate course on (computational) number theory
and algebra, perhaps geared towards computer science students.
Since this is an introductory textbook, and not an encyclopedic reference
for specialists, some topics simply could not be covered. One such topic
whose exclusion will undoubtedly be lamented by some is the theory of
lattices, along with algorithms for and applications of lattice basis reduction.
Another such topic is that of fast algorithms for integer and polynomial
arithmetic—although some of the basic ideas of this topic are developed in
the exercises, the main body of the text deals only with classical, quadratic-
time algorithms for integer and polynomial arithmetic. As an introductory
text, some topics just had to go; moreover, there are more advanced texts
that cover these topics perfectly well, and these texts should be readily
accessible to students who have mastered the material in this book.
Note that while continued fractions are not discussed, the closely related
problem of “rational reconstruction” is covered, along with a number of in-
teresting applications (which could also be solved using continued fractions).
Using the text. Here are a few tips on using the text.
• There are a few sections that are marked with a “(∗),” indicating
that the material covered in that section is a bit technical, and is not
needed elsewhere.
• There are many examples in the text. These form an integral part of
the text, and should not be skipped.
• There are a number of exercises in the text that serve to reinforce,
as well as to develop important applications and generalizations of,
the material presented in the text. In solving exercises, the reader is
free to use any previously stated results in the text, including those
in previous exercises. However, except where otherwise noted, any
result in a section marked with a “(∗),” or in §5.5, need not and
should not be used outside the section in which it appears.
• There is a very brief “Preliminaries” chapter, which ﬁxes a bit of
notation and recalls a few standard facts. This should be skimmed
over by the reader.
• There is an appendix that contains a few useful facts; where such a
fact is used in the text, there is a reference such as “see §An,” which
refers to the item labeled “An” in the appendix.
Feedback. I welcome comments on the book (suggestions for improvement,
error reports, etc.) from readers. Please send your comments to
victor@shoup.net.
TEAM LinG

Preface
xiii
There is also web site where further material and information relating to
the book (including a list of errata and the latest electronic version of the
book) may be found:
www.shoup.net/ntb.
Acknowledgments. I would like to thank a number of people who vol-
unteered their time and energy in reviewing one or more chapters: Sid-
dhartha Annapureddy, John Black, Carl Bosley, Joshua Brody, Jan Ca-
menisch, Ronald Cramer, Alex Dent, Nelly Fazio, Mark Giesbrecht, Stuart
Haber, Alfred Menezes, Antonio Nicolosi, Roberto Oliveira, and Louis Sal-
vail. Thanks to their eﬀorts, the “bug count” has been signiﬁcantly reduced,
and the readability of the text much improved. I am also grateful to the
National Science Foundation for their support provided under grant CCR-
0310297. Thanks to David Tranah and his colleagues at Cambridge Univer-
sity Press for their progressive attitudes regarding intellectual property and
open access.
New York, January 2005
Victor Shoup
TEAM LinG

Preliminaries
We establish here a few notational conventions used throughout the text.
Arithmetic with ∞
We shall sometimes use the symbols “∞” and “−∞” in simple arithmetic
expressions involving real numbers. The interpretation given to such ex-
pressions is the usual, natural one; for example, for all real numbers x, we
have −∞< x < ∞, x + ∞= ∞, x −∞= −∞, ∞+ ∞= ∞, and
(−∞) + (−∞) = −∞. Some such expressions have no sensible interpreta-
tion (e.g., ∞−∞).
Logarithms and exponentials
We denote by log x the natural logarithm of x. The logarithm of x to the
base b is denoted logb x.
We denote by ex the usual exponential function, where e ≈2.71828 is the
base of the natural logarithm. We may also write exp[x] instead of ex.
Sets and relations
We use the symbol ∅to denote the empty set. For two sets A, B, we use the
notation A ⊆B to mean that A is a subset of B (with A possibly equal to
B), and the notation A ⊊B to mean that A is a proper subset of B (i.e.,
A ⊆B but A ̸= B); further, A ∪B denotes the union of A and B, A ∩B
the intersection of A and B, and A \ B the set of all elements of A that are
not in B.
For sets S1, . . . , Sn, we denote by S1 × · · · × Sn the Cartesian product
xiv
TEAM LinG

Preliminaries
xv
of S1, . . . , Sn, that is, the set of all n-tuples (a1, . . . , an), where ai ∈Si for
i = 1, . . . , n.
We use the notation S×n to denote the Cartesian product of n copies of
a set S, and for x ∈S, we denote by x×n the element of S×n consisting of
n copies of x. (We shall reserve the notation Sn to denote the set of all nth
powers of S, assuming a multiplication operation on S is deﬁned.)
Two sets A and B are disjoint if A ∩B = ∅. A collection {Ci} of sets is
called pairwise disjoint if Ci ∩Cj = ∅for all i, j with i ̸= j.
A partition of a set S is a pairwise disjoint collection of non-empty
subsets of S whose union is S. In other words, each element of S appears
in exactly one subset.
A binary relation on a set S is a subset R of S × S. Usually, one writes
a ∼b to mean that (a, b) ∈R, where ∼is some appropriate symbol, and
rather than refer to the relation as R, one refers to it as ∼.
A binary relation ∼on a set S is called an equivalence relation if for
all x, y, z ∈S, we have
• x ∼x (reﬂexive property),
• x ∼y implies y ∼x (symmetric property), and
• x ∼y and y ∼z implies x ∼z (transitive property).
If ∼is an equivalence relation on S, then for x ∈S one deﬁnes the set
[x] := {y ∈S : x ∼y}. Such a set [x] is an equivalence class. It follows
from the deﬁnition of an equivalence relation that for all x, y ∈S, we have
• x ∈[x], and
• either [x] ∩[y] = ∅or [x] = [y].
In particular, the collection of all distinct equivalence classes partitions the
set S.
For any x ∈S, the set [x] is called the the equivalence class
containing x, and x is called a representative of [x].
Functions
For any function f from a set A into a set B, if A′ ⊆A, then f(A′) :=
{f(a) ∈B : a ∈A′} is the image of A′ under f, and f(A) is simply referred
to as the image of f; if B′ ⊆B, then f−1(B′) := {a ∈A : f(a) ∈B′} is the
pre-image of B′ under f.
A function f : A →B is called one-to-one or injective if f(a) = f(b)
implies a = b. The function f is called onto or surjective if f(A) = B.
The function f is called bijective if it is both injective and surjective; in
this case, f is called a bijection. If f is bijective, then we may deﬁne the
TEAM LinG

xvi
Preliminaries
inverse function f−1 : B →A, where for b ∈B, f−1(b) is deﬁned to be
the unique a ∈A such that f(a) = b.
If f : A →B and g : B →C are functions, we denote by g ◦f their
composition, that is, the function that sends a ∈A to g(f(a)) ∈C. Function
composition is associative; that is, for functions f : A →B, g : B →C,
and h : C →D, we have (h ◦g) ◦f = h ◦(g ◦f). Thus, we can simply
write h ◦g ◦f without any ambiguity. More generally, if we have functions
fi : Ai →Ai+1 for i = 1, . . . , n, where n ≥2, then we may write their
composition as fn ◦· · · ◦f1 without any ambiguity. As a special case of this,
if Ai = A and fi = f for i = 1, . . . , n, then we may write fn ◦· · · ◦f1 as
fn. It is understood that f1 = f, and that f0 is the identity function on A.
If f is a bijection, then so is fn for any non-negative integer n, the inverse
function of fn being (f−1)n, which one may simply write as f−n.
Binary operations
A binary operation ⋆on a set S is a function from S × S to S, where the
value of the function at (a, b) ∈S × S is denoted a ⋆b.
A binary operation ⋆on S is called associative if for all a, b, c ∈S, we
have (a⋆b)⋆c = a⋆(b⋆c). In this case, we can simply write a⋆b⋆c without
any ambiguity. More generally, for a1, . . . , an ∈S, where n ≥2, we can
write a1 ⋆· · · ⋆an without any ambiguity.
A binary operation ⋆on S is called commutative if for all a, b ∈S,
we have a ⋆b = b ⋆a. If the binary operation ⋆is both associative and
commutative, then not only is the expression a1 ⋆· · · ⋆an unambiguous, but
its value remains unchanged even if we re-order the ai.
TEAM LinG

1
Basic properties of the integers
This chapter discusses some of the basic properties of the integers, including
the notions of divisibility and primality, unique factorization into primes,
greatest common divisors, and least common multiples.
1.1 Divisibility and primality
Consider the integers Z := {. . . , −2, −1, 0, 1, 2, . . .}. For a, b ∈Z, we say
that b divides a, or alternatively, that a is divisible by b, if there exists
c ∈Z such that a = bc. If b divides a, then b is called a divisor of a, and
we write b | a. If b does not divide a, then we write b ∤a.
We ﬁrst state some simple facts:
Theorem 1.1. For all a, b, c ∈Z, we have
(i) a | a, 1 | a, and a | 0;
(ii) 0 | a if and only if a = 0;
(iii) a | b and a | c implies a | (b + c);
(iv) a | b implies a | −b;
(v) a | b and b | c implies a | c.
Proof. These properties can be easily derived from the deﬁnition using ele-
mentary facts about the integers. For example, a | a because we can write
a = a · 1; 1 | a because we can write a = 1 · a; a | 0 because we can write
0 = a·0. We leave it as an easy exercise for the reader to verify the remaining
properties. 2
Another simple but useful fact is the following:
Theorem 1.2. For all a, b ∈Z, we have a | b and b | a if and only if a = ±b.
1
TEAM LinG

2
Basic properties of the integers
Proof. Clearly, if a = ±b, then a | b and b | a. So let us assume that a | b and
b | a, and prove that a = ±b. If either of a or b are zero, then part (ii) of the
previous theorem implies that the other is zero. So assume that neither is
zero. Now, b | a implies a = bc for some c ∈Z. Likewise, a | b implies b = ad
for some d ∈Z. From this, we obtain b = ad = bcd, and canceling b from
both sides of the equation b = bcd, we obtain 1 = cd. The only possibility
is that either c = d = −1, in which case a = −b, or c = d = 1, in which case
a = b. 2
Any integer n is trivially divisible by ±1 and ±n. We say that an integer
p is prime if p > 1 and the only divisors of p are the trivial divisors ±1
and ±p. Conversely, an integer n is called composite if n > 1 and it is
not prime. So an integer n > 1 is composite if and only if n = ab for some
integers a, b with 1 < a < n and 1 < b < n. The ﬁrst few primes are
2, 3, 5, 7, 11, 13, 17, . . . .
The number 1 is not considered to be either prime or composite. Also, we
do not consider the negative of a prime (e.g., −2) to be prime (although one
can, and some authors do so).
A basic fact is that any non-zero integer can be expressed as a signed
product of primes in an essentially unique way. More precisely:
Theorem 1.3 (Fundamental theorem of arithmetic). Every non-zero
integer n can be expressed as
n = ±pe1
1 · · · per
r ,
where the pi are distinct primes and the ei are positive integers. Moreover,
this expression is unique, up to a reordering of the primes.
Note that if n = ±1 in the above theorem, then r = 0, and the product
of zero terms is interpreted (as usual) as 1.
To prove this theorem, we may clearly assume that n is positive, since
otherwise, we may multiply n by −1 and reduce to the case where n is
positive.
The proof of the existence part of Theorem 1.3 is easy. This amounts
to showing that every positive integer n can be expressed as a product
(possibly empty) of primes. We may prove this by induction on n. If n = 1,
the statement is true, as n is the product of zero primes. Now let n > 1,
and assume that every positive integer smaller than n can be expressed as
a product of primes. If n is a prime, then the statement is true, as n is the
TEAM LinG

1.1 Divisibility and primality
3
product of one prime; otherwise, n is composite, and so there exist a, b ∈Z
with 1 < a < n, 1 < b < n, and n = ab; by the induction hypothesis, both a
and b can be expressed as a product of primes, and so the same holds for n.
The uniqueness part of Theorem 1.3 is by no means obvious, and most
of the rest of this section and the next section are devoted to developing a
proof of this. We give a quite leisurely proof, introducing a number of other
very important tools and concepts along the way that will be useful later.
An essential ingredient in this proof is the following:
Theorem 1.4 (Division with remainder property). For a, b ∈Z with
b > 0, there exist unique q, r ∈Z such that a = bq + r and 0 ≤r < b.
Proof. Consider the set S of non-negative integers of the form a −zb with
z ∈Z. This set is clearly non-empty, and so contains a minimum. Let r be
the smallest integer in this set, with r = a −qb for q ∈Z. By deﬁnition,
we have r ≥0. Also, we must have r < b, since otherwise, we would have
0 ≤r −b < r and r −b = a −(q + 1)b ∈S, contradicting the minimality of
r.
That proves the existence of r and q. For uniqueness, suppose that a =
bq + r and a = bq′ + r′, where 0 ≤r < b and 0 ≤r′ < b. Then subtracting
these two equations and rearranging terms, we obtain
r′ −r = b(q −q′).
(1.1)
Now observe that by assumption, the left-hand side of (1.1) is less than b in
absolute value. However, if q ̸= q′, then the right-hand side of (1.1) would
be at least b in absolute value; therefore, we must have q = q′. But then by
(1.1), we must have r = r′. 2
In the above theorem, it is easy to see that q = ⌊a/b⌋, where for any real
number x, ⌊x⌋denotes the greatest integer less than or equal to x. We shall
write r = a mod b; that is, a mod b denotes the remainder in dividing a by
b. It is clear that b | a if and only if a mod b = 0.
One can generalize the notation a mod b to all integers a and b, with b ̸= 0:
we deﬁne a mod b := a −bq, where q = ⌊a/b⌋.
In addition to the “ﬂoor” function ⌊·⌋, the “ceiling” function ⌈·⌉is also
useful: for any real number x, ⌈x⌉is deﬁned as the smallest integer greater
than or equal to x.
Exercise 1.1. Let n be a composite integer. Show that there exists a prime
p dividing n, such that p ≤|n|1/2.
TEAM LinG

4
Basic properties of the integers
Exercise 1.2. For integer n and real x, show that n ≤x if and only if
n ≤⌊x⌋.
Exercise 1.3. For real x and positive integer n, show that ⌊⌊x⌋/n⌋= ⌊x/n⌋.
In particular, for positive integers a, b, c, ⌊⌊a/b⌋/c⌋= ⌊a/(bc)⌋.
Exercise 1.4. For real x, show that 2⌊x⌋≤⌊2x⌋≤2⌊x⌋+ 1.
Exercise 1.5. For positive integers m and n, show that the number of
multiples of m among 1, 2, . . . , n is ⌊n/m⌋. More generally, for integer m ≥1
and real x ≥0, show that the number of multiples of m in the interval [1, x]
is ⌊x/m⌋.
Exercise 1.6. For integers a, b with b < 0, show that b < a mod b ≤0.
1.2 Ideals and greatest common divisors
To carry on with the proof of Theorem 1.3, we introduce the notion of an
ideal of Z, which is a non-empty set of integers that is closed under addition,
and under multiplication by an arbitrary integer. That is, a non-empty set
I ⊆Z is an ideal if and only if for all a, b ∈I and all z ∈Z, we have
a + b ∈I and az ∈I.
Note that for an ideal I, if a ∈I, then so is −a, since −a = a · (−1) ∈I.
It is easy to see that any ideal must contain 0: since an ideal I must contain
some element a, and by the closure properties of ideals, we must have 0 =
a + (−a) ∈I. It is clear that {0} and Z are ideals. Moreover, an ideal I is
equal to Z if and only if 1 ∈I —to see this, note that 1 ∈I implies that
for all z ∈Z, z = 1 · z ∈I, and hence I = Z; conversely, if I = Z, then in
particular, 1 ∈I.
For a ∈Z, deﬁne aZ := {az : z ∈Z}; that is, aZ is the set of all integer
multiples of a. It is easy to see that aZ is an ideal: for az, az′ ∈aZ and
z′′ ∈Z, we have az + az′ = a(z + z′) ∈aZ and (az)z′′ = a(zz′′) ∈aZ. The
set aZ is called the ideal generated by a, and any ideal of the form aZ
for some a ∈Z is called a principal ideal.
We observe that for all a, b ∈Z, we have a ∈bZ if and only if b | a.
We also observe that for any ideal I, we have a ∈I if and only if aZ ⊆I.
Both of these observations are simple consequences of the deﬁnitions, as the
reader may verify. Combining these two observations, we see that aZ ⊆bZ
if and only if b | a.
We can generalize the above method of constructing ideals.
For
TEAM LinG

1.2 Ideals and greatest common divisors
5
a1, . . . , ak ∈Z, deﬁne
a1Z + · · · + akZ := {a1z1 + · · · + akzk : z1, . . . , zk ∈Z}.
That is, a1Z + · · · + akZ consists of all linear combinations, with integer
coeﬃcients, of a1, . . . , ak. We leave it to the reader to verify that a1Z+· · ·+
akZ is an ideal and contains a1, . . . , ak; it is called the ideal generated by
a1, . . . , ak. In fact, this ideal is the “smallest” ideal containing a1, . . . , ak, in
the sense that any other ideal that contains a1, . . . , ak must already contain
this ideal (verify).
Example 1.1. Let a := 3 and consider the ideal aZ. This consists of all
integer multiples of 3; that is, aZ = {. . . , −9, −6, −3, 0, 3, 6, 9, . . .}. 2
Example 1.2. Let a1 := 3 and a2 := 5, and consider the ideal a1Z + a2Z.
This ideal contains 2a1 −a2 = 1. Since it contains 1, it contains all integers;
that is, a1Z + a2Z = Z. 2
Example 1.3. Let a1 := 4 and a2 := 6, and consider the ideal a1Z + a2Z.
This ideal contains a2 −a1 = 2, and therefore, it contains all even integers.
It does not contain any odd integers, since the sum of two even integers is
again even. 2
The following theorem says that all ideals of Z are principal.
Theorem 1.5. For any ideal I ⊆Z, there exists a unique non-negative
integer d such that I = dZ.
Proof. We ﬁrst prove the existence part of the theorem. If I = {0}, then
d = 0 does the job, so let us assume that I ̸= {0}. Since I contains non-zero
integers, it must contain positive integers, since if z ∈I then so is −z. Let
d be the smallest positive integer in I. We want to show that I = dZ.
We ﬁrst show that I ⊆dZ. To this end, let c be any element in I. It
suﬃces to show that d | c. Using the division with remainder property, write
c = qd + r, where 0 ≤r < d. Then by the closure properties of ideals, one
sees that r = c −qd is also an element of I, and by the minimality of the
choice of d, we must have r = 0. Thus, d | c.
We next show that dZ ⊆I. This follows immediately from the fact that
d ∈I and the closure properties of ideals.
That proves the existence part of the theorem. As for uniqueness, note
that if dZ = d′Z, we have d | d′ and d′ | d, from which it follows by
Theorem 1.2 that d′ = ±d. 2
For a, b ∈Z, we call d ∈Z a common divisor of a and b if d | a and
TEAM LinG

6
Basic properties of the integers
d | b; moreover, we call such a d a greatest common divisor of a and b if
d is non-negative and all other common divisors of a and b divide d.
Theorem 1.6. For any a, b ∈Z, there exists a unique greatest common
divisor d of a and b, and moreover, aZ + bZ = dZ.
Proof. We apply the previous theorem to the ideal I := aZ + bZ. Let d ∈Z
with I = dZ, as in that theorem. We wish to show that d is a greatest
common divisor of a and b. Note that a, b, d ∈I and d is non-negative.
Since a ∈I = dZ, we see that d | a; similarly, d | b. So we see that d is a
common divisor of a and b.
Since d ∈I = aZ + bZ, there exist s, t ∈Z such that as + bt = d. Now
suppose a = a′d′ and b = b′d′ for a′, b′, d′ ∈Z. Then the equation as+bt = d
implies that d′(a′s + b′t) = d, which says that d′ | d. Thus, any common
divisor d′ of a and b divides d.
That proves that d is a greatest common divisor of a and b.
As for
uniqueness, note that if d′′ is a greatest common divisor of a and b, then
d | d′′ and d′′ | d, and hence d′′ = ±d, and the requirement that d′′ is
non-negative implies that d′′ = d. 2
For a, b ∈Z, we denote by gcd(a, b) the greatest common divisor of a and
b. Note that as we have deﬁned it, gcd(a, 0) = |a|. Also note that when at
least one of a or b are non-zero, gcd(a, b) is the largest positive integer that
divides both a and b.
An immediate consequence of Theorem 1.6 is that for all a, b ∈Z, there
exist s, t ∈Z such that as + bt = gcd(a, b), and that when at least one of
a or b are non-zero, gcd(a, b) is the smallest positive integer that can be
expressed as as + bt for some s, t ∈Z.
We say that a, b ∈Z are relatively prime if gcd(a, b) = 1, which is
the same as saying that the only common divisors of a and b are ±1. It is
immediate from Theorem 1.6 that a and b are relatively prime if and only
if aZ + bZ = Z, which holds if and only if there exist s, t ∈Z such that
as + bt = 1.
Theorem 1.7. For a, b, c ∈Z such that c | ab and gcd(a, c) = 1, we have
c | b.
Proof. Suppose that c | ab and gcd(a, c) = 1. Then since gcd(a, c) = 1, by
Theorem 1.6 we have as+ct = 1 for some s, t ∈Z. Multiplying this equation
by b, we obtain
abs + cbt = b.
(1.2)
TEAM LinG

1.2 Ideals and greatest common divisors
7
Since c divides ab by hypothesis, and since c clearly divides cbt, it follows
that c divides the left-hand side of (1.2), and hence that c divides b. 2
As a consequence of this theorem, we have:
Theorem 1.8. Let p be prime, and let a, b ∈Z. Then p | ab implies that
p | a or p | b.
Proof. Assume that p | ab. The only divisors of p are ±1 and ±p. Thus,
gcd(p, a) is either 1 or p. If p | a, we are done; otherwise, if p ∤a, we must
have gcd(p, a) = 1, and by the previous theorem, we conclude that p | b. 2
An obvious corollary to Theorem 1.8 is that if a1, . . . , ak are integers,
and if p is a prime that divides the product a1 · · · ak, then p | ai for some
i = 1, . . . , k.
This is easily proved by induction on k.
For k = 1, the
statement is trivially true. Now let k > 1, and assume that statement holds
for k −1. Then by Theorem 1.8, either p | a1 or p | a2 · · · ak−1; if p | a1, we
are done; otherwise, by induction, p divides one of a2, . . . , ak−1.
We are now in a position to prove the uniqueness part of Theorem 1.3,
which we can state as follows: if p1, . . . , pr and p′
1, . . . , p′
s are primes (with
duplicates allowed among the pi and among the p′
j) such that
p1 · · · pr = p′
1 · · · p′
s,
(1.3)
then (p1, . . . , pr) is just a reordering of (p′
1, . . . , p′
s). We may prove this by
induction on r. If r = 0, we must have s = 0 and we are done. Now suppose
r > 0, and that the statement holds for r −1. Since r > 0, we clearly must
have s > 0. Also, as p1 is obviously divides the left-hand side of (1.3), it
must also divide the right-hand side of (1.3); that is, p1 | p′
1 · · · p′
s. It follows
from (the corollary to) Theorem 1.8 that p1 | p′
j for some j = 1, . . . , s, and
indeed, since pi and p′
j are both prime, we must have pi = p′
j. Thus, we may
cancel pi from the left-hand side of (1.3) and p′
j from the right-hand side of
(1.3), and the statement now follows from the induction hypothesis. That
proves the uniqueness part of Theorem 1.3.
Exercise 1.7. Let I be a non-empty set of integers that is closed under
addition, that is, a + b ∈I for all a, b ∈I. Show that the condition
−a ∈I for all a ∈I
holds if and only if
az ∈I for all a ∈I, z ∈Z.
TEAM LinG

8
Basic properties of the integers
Exercise 1.8. Let a, b, c be positive integers, with gcd(a, b) = 1 and c ≥ab.
Show that there exist non-negative integers s, t such that c = as + bt.
Exercise 1.9. Show that for any integers a, b with d := gcd(a, b) ̸= 0, we
have gcd(a/d, b/d) = 1.
1.3 Some consequences of unique factorization
The following theorem is a consequence of just the existence part of Theo-
rem 1.3:
Theorem 1.9. There are inﬁnitely many primes.
Proof. By way of contradiction, suppose that there were only ﬁnitely many
primes; call them p1, . . . , pk. Then set n := 1 + k
i=1 pi, and consider a
prime p that divides n. There must be at least one such prime p, since
n ≥2, and every positive integer can be written as a product of primes.
Clearly, p cannot equal any of the pi, since if it did, then p would divide
n −k
i=1 pi = 1, which is impossible. Therefore, the prime p is not among
p1, . . . , pk, which contradicts our assumption that these are the only primes.
2
For a prime p, we may deﬁne the function νp, mapping non-zero integers
to non-negative integers, as follows: for integer n ̸= 0, if n = pem, where
p ∤m, then νp(n) := e. We may then write the factorization of n into primes
as
n = ±

p
pνp(n),
where the product is over all primes p, with all but ﬁnitely many of the
terms in the product equal to 1.
It is also convenient to extend the domain of deﬁnition of νp to include
0, deﬁning νp(0) := ∞. Following standard conventions for arithmetic with
inﬁnity (see Preliminaries), it is easy to see that for all a, b ∈Z, we have
νp(a · b) = νp(a) + νp(b) for all p.
(1.4)
From this, it follows that for all a, b ∈Z, we have
b | a
if and only if
νp(b) ≤νp(a) for all p,
(1.5)
and
νp(gcd(a, b)) = min(νp(a), νp(b)) for all p.
(1.6)
For a, b ∈Z a common multiple of a and b is an integer m such that
TEAM LinG

1.3 Some consequences of unique factorization
9
a | m and b | m; moreover, such an m is the least common multiple of a
and b if m is non-negative and m divides all common multiples of a and b.
In light of Theorem 1.3, it is clear that the least common multiple exists and
is unique, and we denote the least common multiple of a and b by lcm(a, b).
Note that as we have deﬁned it, lcm(a, 0) = 0, and that when both a and
b are non-zero, lcm(a, b) is the smallest positive integer divisible by both a
and b. Also, for all a, b ∈Z, we have
νp(lcm(a, b)) = max(νp(a), νp(b)) for all p,
(1.7)
and
gcd(a, b) · lcm(a, b) = |ab|.
(1.8)
It is easy to generalize the notions of greatest common divisor and least
common multiple from two integers to many integers. For a1, . . . , ak ∈Z,
with k ≥1, we call d ∈Z a common divisor of a1, . . . , ak if d | ai for
i = 1, . . . , k; moreover, we call such a d the greatest common divisor of
a1, . . . , ak if d is non-negative and all other common divisors of a1, . . . , ak
divide d. It is clear that the greatest common divisor of a1, . . . , ak exists
and is unique, and moreover, we have
νp(gcd(a1, . . . , ak)) = min(νp(a1), . . . , νp(ak)) for all p.
(1.9)
Analogously, for a1, . . . , ak ∈Z, with k ≥1, we call m ∈Z a common
multiple of a1, . . . , ak if ai | m for i = 1, . . . , k; moreover, such an m is called
the least common multiple of a1, . . . , ak if m divides all common multiples
of a1, . . . , ak. It is clear that the least common multiple of a1, . . . , ak exists
and is unique, and moreover, we have
νp(lcm(a1, . . . , ak)) = max(νp(a1), . . . , νp(ak)) for all p.
(1.10)
We say that integers a1, . . . , ak are pairwise relatively prime if
gcd(ai, aj) = 1 for all i, j with i ̸= j. Note that if a1, . . . , ak are pairwise rel-
atively prime, then gcd(a1, . . . , ak) = 1; however, gcd(a1, . . . , ak) = 1 does
not imply that a1, . . . , ak are pairwise relatively prime.
Consider now the rational numbers Q := {a/b : a, b ∈Z, b ̸= 0}. Because
of the unique factorization property for Z, given any rational number a/b,
if we set d := gcd(a, b), and deﬁne the integers a′ := a/d and b′ := b/d, then
we have a/b = a′/b′ and gcd(a′, b′) = 1. Moreover, if ˜a/˜b = a′/b′, then we
have ˜ab′ = a′˜b, and so b′ | a′˜b, and since gcd(a′, b′) = 1, we see that b′ | ˜b;
if ˜b = ˜db′, it follows that ˜a = ˜da′. Thus, we can represent every rational
number as a fraction in lowest terms, that is, a fraction of the form a′/b′
TEAM LinG

10
Basic properties of the integers
where a′ and b′ are relatively prime; moreover, the values of a′ and b′ are
uniquely determined up to sign, and every other fraction that represents the
same rational number is of the form ( ˜da′)/( ˜db′), for some non-zero integer ˜d.
Exercise 1.10. Let n be a positive integer. Show that if a, b are relatively
prime integers, each of which divides n, then ab divides n. More generally,
show that if a1, . . . , ak are pairwise relatively prime integers, each of which
divides n, then their product a1 · · · ak divides n.
Exercise 1.11. For positive integer n, let D(n) denote the set of positive
divisors of n. For relatively prime, positive integers n1, n2, show that the
sets D(n1) × D(n2) and D(n1 · n2) are in one-to-one correspondence, via the
map that sends (d1, d2) ∈D(n1) × D(n2) to d1 · d2.
Exercise 1.12. Let p be a prime and k an integer 0 < k < p. Show that
the binomial coeﬃcient
p
k

=
p!
k!(p −k)!,
which is an integer, of course, is divisible by p.
Exercise 1.13. An integer a ∈Z is called square-free if it is not divisible
by the square of any integer greater than 1. Show that any integer n ∈Z
can be expressed as n = ab2, where a, b ∈Z and a is square-free.
Exercise 1.14. Show that any non-zero x ∈Q can be expressed as
x = ±pe1
1 · · · per
r ,
where the pi are distinct primes and the ei are non-zero integers, and that
this expression in unique up to a reordering of the primes.
Exercise 1.15. Show that if an integer cannot be expressed as a square of
an integer, then it cannot be expressed as a square of any rational number.
Exercise 1.16. Show that for all integers a, b, and all primes p, we have
νp(a + b) ≥min{νp(a), νp(b)}, and that if νp(a) < νp(b), then νp(a + b) =
νp(a).
Exercise 1.17. For a prime p, we may extend the domain of deﬁnition of νp
from Z to Q: for non-zero integers a, b, let us deﬁne νp(a/b) := νp(a)−νp(b).
(a) Show that this deﬁnition of νp(a/b) is unambiguous, in the sense that
it does not depend on the particular choice of a and b.
(b) Show that for all x, y ∈Q, we have νp(xy) = νp(x) + νp(y).
TEAM LinG

1.3 Some consequences of unique factorization
11
(c) Show that for all x, y ∈Q, we have νp(x + y) ≥min{νp(x), νp(y)},
and that if νp(x) < νp(y), then νp(x + y) = νp(x).
(d) Show that for all non-zero x ∈Q, we have
x = ±

p
pνp(x),
where the product is over all primes, and all but a ﬁnite number of
terms in the product is 1.
Exercise 1.18. Let n be a positive integer, and let Cn denote the number of
pairs of integers (a, b) such that 1 ≤a ≤n, 1 ≤b ≤n and gcd(a, b) = 1, and
let Fn be the number of distinct rational numbers a/b, where 0 ≤a < b ≤n.
(a) Show that Fn = (Cn + 1)/2.
(b) Show that Cn ≥n2/4. Hint: ﬁrst show that Cn ≥n2(1−
d≥2 1/d2),
and then show that 
d≥2 1/d2 ≤3/4.
Exercise 1.19. This exercise develops a characterization of least common
multiples in terms of ideals.
(a) Arguing directly from the deﬁnition of an ideal, show that if I and J
are ideals of Z, then so is I ∩J.
(b) Let a, b ∈Z, and consider the ideals I := aZ and J := bZ. By part
(a), we know that I ∩J is an ideal. By Theorem 1.5, we know that
I ∩J = mZ for some uniquely determined non-negative integer m.
Show that m = lcm(a, b).
Exercise 1.20. For a1, . . . , ak ∈Z, with k > 1, show that
gcd(a1, . . . , ak) = gcd(gcd(a1, . . . , ak−1), ak)
and
lcm(a1, . . . , ak) = lcm(lcm(a1, . . . , ak−1), ak).
Exercise 1.21. Show that for any a1, . . . , ak ∈Z, if d := gcd(a1, . . . , ak),
then dZ = a1Z + · · · + akZ; in particular, there exist integers s1, . . . , sk such
that
d = a1s1 + · · · + aksk.
Exercise 1.22. Show that for all integers a, b, we have
gcd(a + b, lcm(a, b)) = gcd(a, b).
TEAM LinG

12
Basic properties of the integers
Exercise 1.23. Show that for integers c, a1, . . . , ak, we have
gcd(ca1, . . . , cak) = |c| gcd(a1, . . . , ak).
TEAM LinG

2
Congruences
This chapter introduces the basic properties of congruences modulo n, along
with the related notion of congruence classes modulo n. Other items dis-
cussed include the Chinese remainder theorem, Euler’s phi function, arith-
metic functions and M¨obius inversion, and Fermat’s little theorem.
2.1 Deﬁnitions and basic properties
For positive integer n, and for a, b ∈Z, we say that a is congruent to
b modulo n if n | (a −b), and we write a ≡b (mod n). If n ∤(a −b),
then we write a ̸≡b (mod n).
The relation a ≡b (mod n) is called a
congruence relation, or simply, a congruence. The number n appearing
in such congruences is called the modulus of the congruence. This usage of
the “mod” notation as part of a congruence is not to be confused with the
“mod” operation introduced in §1.1.
A simple observation is that a ≡b (mod n) if and only if there exists an
integer c such that a = b + cn. From this, and Theorem 1.4, the following
is immediate:
Theorem 2.1. Let n be a positive integer. For every integer a, there exists
a unique integer b such that a ≡b (mod n) and 0 ≤b < n, namely, b :=
a mod n.
If we view the modulus n as ﬁxed, then the following theorem says that
the binary relation “· ≡· (mod n)” is an equivalence relation on the set Z:
Theorem 2.2. Let n be a positive integer. For all a, b, c ∈Z, we have:
(i) a ≡a (mod n);
(ii) a ≡b (mod n) implies b ≡a (mod n);
(iii) a ≡b (mod n) and b ≡c (mod n) implies a ≡c (mod n).
13
TEAM LinG

14
Congruences
Proof. For (i), observe that n divides 0 = a −a. For (ii), observe that if n
divides a −b, then it also divides −(a −b) = b −a. For (iii), observe that if
n divides a −b and b −c, then it also divides (a −b) + (b −c) = a −c. 2
A key property of congruences is that they are “compatible” with integer
addition and multiplication, in the following sense:
Theorem 2.3. For all positive integers n, and all a, a′, b, b′ ∈Z, if a ≡
a′ (mod n) and b ≡b′ (mod n), then
a + b ≡a′ + b′ (mod n)
and
a · b ≡a′ · b′ (mod n).
Proof. Suppose that a ≡a′ (mod n) and b ≡b′ (mod n). This means that
there exist integers c and d such that a′ = a+cn and b′ = b+dn. Therefore,
a′ + b′ = a + b + (c + d)n,
which proves the ﬁrst congruence of the theorem, and
a′b′ = (a + cn)(b + dn) = ab + (ad + bc + cdn)n,
which proves the second congruence. 2
Theorems 2.2 and 2.3 allow one to work with congruence relations mod-
ulo n much as one would with ordinary equalities: one can add to, subtract
from, or multiply both sides of a congruence modulo n by the same integer;
also, if x is congruent to y modulo n, one may substitute y for x in any sim-
ple arithmetic expression (more precisely, any polynomial in x with integer
coeﬃcients) appearing in a congruence modulo n.
Example 2.1. Observe that
3 · 5 ≡1 (mod 7).
(2.1)
Using this fact, let us ﬁnd the set of solutions z to the congruence
3z + 4 ≡6 (mod 7).
(2.2)
Suppose that z is a solution to (2.2). Subtracting 4 from both sides of (2.2),
we see that
3z ≡2 (mod 7).
(2.3)
Now, multiplying both sides of (2.3) by 5, and using (2.1), we obtain
z ≡1 · z ≡(3 · 5) · z ≡2 · 5 ≡3 (mod 7).
TEAM LinG

2.2 Solving linear congruences
15
Thus, if z is a solution to (2.2), we must have z ≡3 (mod 7); conversely,
one can verify that if z ≡3 (mod 7), then (2.2) holds. We conclude that
the integers z that are solutions to (2.2) are precisely those integers that are
congruent to 3 modulo 7, which we can list as follows:
. . . , −18, −11, −4, 3, 10, 17, 24, . . .
2
In the next section, we shall give a systematic treatment of the problem
of solving linear congruences, such as the one appearing in the previous
example.
Exercise 2.1. Let x, y, n ∈Z with n > 0 and x ≡y (mod n). Also, let
a0, a1, . . . , ak be integers. Show that
a0 + a1x + · · · + akxk ≡a0 + a1y + · · · + akyk (mod n).
Exercise 2.2. Let a, b, n, n′ ∈Z with n > 0 and n′ | n.
Show that if
a ≡b (mod n), then a ≡b (mod n′).
Exercise 2.3. Let a, b, n, n′ ∈Z with n > 0, n′ > 0, and gcd(n, n′) = 1.
Show that if a ≡b (mod n) and a ≡b (mod n′), then a ≡b (mod nn′).
Exercise 2.4. Let a, b, n ∈Z such that n > 0 and a ≡b (mod n). Show
that gcd(a, n) = gcd(b, n).
Exercise 2.5. Prove that for any prime p and integer x, if x2 ≡1 (mod p)
then x ≡1 (mod p) or x ≡−1 (mod p).
Exercise 2.6. Let a be a positive integer whose base-10 representation is
a = (ak−1 · · · a1a0)10. Let b be the sum of the decimal digits of a; that is, let
b := a0 + a1 + · · · + ak−1. Show that a ≡b (mod 9). From this, justify the
usual “rules of thumb” for determining divisibility by 9 and 3: a is divisible
by 9 (respectively, 3) if and only if the sum of the decimal digits of a is
divisible by 9 (respectively, 3).
Exercise 2.7. Show that there are 14 distinct, possible, yearly (Gregorian)
calendars, and show that all 14 calendars actually occur.
2.2 Solving linear congruences
For a positive integer n, and a ∈Z, we say that a′ ∈Z is a multiplicative
inverse of a modulo n if aa′ ≡1 (mod n).
Theorem 2.4. Let a, n ∈Z with n > 0. Then a has a multiplicative inverse
modulo n if and only if a and n are relatively prime.
TEAM LinG

16
Congruences
Proof. This follows immediately from Theorem 1.6: a and n are relatively
prime if and only if there exist s, t ∈Z such that as + nt = 1, if and only if
there exists s ∈Z such that as ≡1 (mod n). 2
Note that the existence of a multiplicative inverse of a modulo n depends
only on the value of a modulo n; that is, if b ≡a (mod n), then a has an
inverse if and only if b does. Indeed, by Theorem 2.3, if b ≡a (mod n), then
for any integer a′, aa′ ≡1 (mod n) if and only if ba′ ≡1 (mod n). (This
fact is also implied by Theorem 2.4 together with Exercise 2.4.)
We now prove a simple “cancellation law” for congruences:
Theorem 2.5. Let a, n, z, z′ ∈Z with n > 0. If a is relatively prime to n,
then az ≡az′ (mod n) if and only if z ≡z′ (mod n). More generally, if
d := gcd(a, n), then az ≡az′ (mod n) if and only if z ≡z′ (mod n/d).
Proof. For the ﬁrst statement, assume that gcd(a, n) = 1, and let a′ be
a multiplicative inverse of a modulo n. Then, az ≡az′ (mod n) implies
a′az ≡a′az′ (mod n), which implies z ≡z′ (mod n), since a′a ≡1 (mod n).
Conversely, if z ≡z′ (mod n), then trivially az ≡az′ (mod n). That proves
the ﬁrst statement.
For the second statement, let d = gcd(a, n). Simply from the deﬁnition
of congruences, one sees that in general, az ≡az′ (mod n) holds if and only
if (a/d)z ≡(a/d)z′ (mod n/d). Moreover, since a/d and n/d are relatively
prime (see Exercise 1.9), the ﬁrst statement of the theorem implies that
(a/d)z ≡(a/d)z′ (mod n/d) holds if and only if z ≡z′ (mod n/d). That
proves the second statement. 2
Theorem 2.5 implies that multiplicative inverses modulo n are uniquely
determined modulo n; indeed, if a is relatively prime to n, and if aa′ ≡1 ≡
aa′′ (mod n), then we may cancel a from the left- and right-hand sides of
this congruence, obtaining a′ ≡a′′ (mod n).
Example 2.2. Observe that
5 · 2 ≡5 · (−4) (mod 6).
(2.4)
Theorem 2.5 tells us that since gcd(5, 6) = 1, we may cancel the common
factor of 5 from both sides of (2.4), obtaining 2 ≡−4 (mod 6), which one
can also verify directly.
Next observe that
3 · 5 ≡3 · 3 (mod 6).
(2.5)
We cannot simply cancel the common factor of 3 from both sides of (2.5);
TEAM LinG

2.2 Solving linear congruences
17
indeed, 5 ̸≡3 (mod 6). However, gcd(3, 6) = 3, and as Theorem 2.5 guaran-
tees, we do indeed have 5 ≡3 (mod 2). 2
Next, we consider the problem of determining the solutions z to congru-
ences of the form az + c ≡b (mod n), for given integers a, b, c, n. Since
we may both add and subtract c from both sides of a congruence modulo
n, it is clear that z is a solution to the above congruence if and only if
az ≡b −c (mod n). Therefore, it suﬃces to consider the problem of deter-
mining the solutions z to congruences of the form az ≡b (mod n), for given
integers a, b, n.
Theorem 2.6. Let a, b, n ∈Z with n > 0. If a is relatively prime to n, then
the congruence az ≡b (mod n) has a solution z; moreover, any integer z′ is
a solution if and only if z ≡z′ (mod n).
Proof. The integer z := ba′, where a′ is a multiplicative inverse of a modulo
n, is clearly a solution. For any integer z′, we have az′ ≡b (mod n) if
and only if az′ ≡az (mod n), which by Theorem 2.5 holds if and only if
z ≡z′ (mod n). 2
Suppose that a, b, n ∈Z with n > 0, a ̸= 0, and gcd(a, n) = 1. This
theorem says that there exists a unique integer z satisfying
az ≡b (mod n) and 0 ≤z < n.
Setting s := b/a ∈Q, we may generalize the “mod” operation, deﬁning
s mod n to be this value z. As the reader may easily verify, this deﬁnition
of s mod n does not depend on the particular choice of fraction used to
represent the rational number s. With this notation, we can simply write
a−1 mod n to denote the unique multiplicative inverse of a modulo n that
lies in the interval 0, . . . , n −1.
Theorem 2.6 may be generalized as follows:
Theorem 2.7. Let a, b, n ∈Z with n > 0, and let d := gcd(a, n). If d | b,
then the congruence az ≡b (mod n) has a solution z, and any integer z′ is
also a solution if and only if z ≡z′ (mod n/d). If d ∤b, then the congruence
az ≡b (mod n) has no solution z.
Proof. For the ﬁrst statement, suppose that d | b. In this case, by Theo-
rem 2.5, we have az ≡b (mod n) if and only if (a/d)z ≡(b/d) (mod n/d),
and so the statement follows immediately from Theorem 2.6, and the fact
that a/d and n/d are relatively prime.
For the second statement, we show that if az ≡b (mod n) for some
TEAM LinG

18
Congruences
integer z, then d must divide b. To this end, assume that az ≡b (mod n)
for some integer z. Then since d | n, we have az ≡b (mod d). However,
az ≡0 (mod d), since d | a, and hence b ≡0 (mod d); that is, d | b. 2
Example 2.3. The following table illustrates what the above theorem says
for n = 15 and a = 1, 2, 3, 4, 5, 6.
z
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
2z mod 15
0
2
4
6
8
10
12
14
1
3
5
7
9
11
13
3z mod 15
0
3
6
9
12
0
3
6
9
12
0
3
6
9
12
4z mod 15
0
4
8
12
1
5
9
13
2
6
10
14
3
7
11
5z mod 15
0
5
10
0
5
10
0
5
10
0
5
10
0
5
10
6z mod 15
0
6
12
3
9
0
6
12
3
9
0
6
12
3
9
In the second row, we are looking at the values 2z mod 15, and we see
that this row is just a permutation of the ﬁrst row. So for every b, there
exists a unique z such that 2z ≡b (mod 15). We could have inferred this
fact from the theorem, since gcd(2, 15) = 1.
In the third row, the only numbers hit are the multiples of 3, which
follows from the theorem and the fact that gcd(3, 15) = 3. Also note that
the pattern in this row repeats every ﬁve columns; that is also implied by
the theorem; that is, 3z ≡3z′ (mod 15) if and only if z ≡z′ (mod 5).
In the fourth row, we again see a permutation of the ﬁrst row, which
follows from the theorem and the fact that gcd(4, 15) = 1.
In the ﬁfth row, the only numbers hit are the multiples of 5, which follows
from the theorem and the fact that gcd(5, 15) = 5.
Also note that the
pattern in this row repeats every three columns; that is also implied by the
theorem; that is, 5z ≡5z′ (mod 15) if and only if z ≡z′ (mod 3).
In the sixth row, since gcd(6, 15) = 3, we see a permutation of the third
row.
The pattern repeats after ﬁve columns, although the pattern is a
permutation of the pattern in the third row. 2
Next, we consider systems of linear congruences with respect to moduli
that are relatively prime in pairs. The result we state here is known as the
Chinese remainder theorem, and is extremely useful in a number of contexts.
Theorem 2.8 (Chinese remainder theorem). Let n1, . . . , nk be pairwise
relatively prime, positive integers, and let a1, . . . , ak be arbitrary integers.
Then there exists an integer z such that
z ≡ai (mod ni) (i = 1, . . . , k).
TEAM LinG

2.2 Solving linear congruences
19
Moreover, any other integer z′ is also a solution of these congruences if and
only if z ≡z′ (mod n), where n := k
i=1 ni.
Proof. Let n := k
i=1 ni, as in the statement of the theorem. Let us also
deﬁne
n′
i := n/ni (i = 1, . . . , k).
From the fact that n1, . . . , nk are pairwise relatively prime, it is clear that
gcd(ni, n′
i) = 1 for i = 1, . . . , k. Therefore, let
mi := (n′
i)−1 mod ni and wi := n′
imi (i = 1, . . . , k).
By construction, one sees that for i = 1, . . . , k, we have
wi ≡1 (mod ni)
and
wi ≡0 (mod nj) for j = 1, . . . , k with j ̸= i.
That is to say, for i, j = 1, . . . , k, we have wi ≡δij (mod nj), where
δij :=
 1
if i = j,
0
if i ̸= j.
Now deﬁne
z :=
k

i=1
wiai.
One then sees that
z ≡
k

i=1
wiai ≡
k

i=1
δijai ≡aj (mod nj) for j = 1, . . . , k.
Therefore, this z solves the given system of congruences.
Moreover, if z′ ≡z (mod n), then since ni | n for i = 1, . . . , k, we see that
z′ ≡z ≡ai (mod ni) for i = 1, . . . , k, and so z′ also solves the system of
congruences.
Finally, if z′ solves the system of congruences, then z′ ≡z (mod ni)
for i = 1, . . . , k. That is, ni | (z′ −z) for i = 1, . . . , k. Since n1, . . . , nk
are pairwise relatively prime, this implies that n | (z′ −z), or equivalently,
z′ ≡z (mod n). 2
Example 2.4. The following table illustrates what the above theorem says
for n1 = 3 and n2 = 5.
TEAM LinG

20
Congruences
z
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
z mod 3
0
1
2
0
1
2
0
1
2
0
1
2
0
1
2
z mod 5
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
We see that as z ranges from 0 to 14, the pairs (z mod 3, z mod 5) range
over all pairs (a1, a2) with a1 ∈{0, 1, 2} and a2 ∈{0, . . . , 4}, with every pair
being hit exactly once. 2
Exercise 2.8. Let a1, . . . , ak, n, b be integers with n > 0, and let d :=
gcd(a1, . . . , ak, n). Show that the congruence
a1z1 + · · · + akzk ≡b (mod n)
has a solution z1, . . . , zk if and only if d | b.
Exercise 2.9. Find an integer z such that z ≡−1 (mod 100), z ≡
1 (mod 33), and z ≡2 (mod 7).
Exercise 2.10. If you want to show that you are a real nerd, here is an
age-guessing game you might play at a party.
First, prepare 2 cards as
follows:
1
4
7
10
· · ·
94
97
2
5
8
11
· · ·
95
98
and 4 cards as follows:
1
6
11
16
· · ·
91
96
2
7
12
17
· · ·
92
97
3
8
13
18
· · ·
93
98
4
9
14
19
· · ·
94
99
At the party, ask a person to tell you if their age is odd or even, and then
ask them to tell you on which of the six cards their age appears. Show how
to use this information (and a little common sense) to determine their age.
2.3 Residue classes
As we already observed in Theorem 2.2, for any ﬁxed positive integer n, the
binary relation “· ≡· (mod n)” is an equivalence relation on the set Z. As
such, this relation partitions the set Z into equivalence classes. We denote
the equivalence class containing the integer a by [a]n, or when n is clear from
context, we may simply write [a]. Historically, these equivalence classes are
called residue classes modulo n, and we shall adopt this terminology here
as well.
TEAM LinG

2.3 Residue classes
21
It is easy to see from the deﬁnitions that
[a]n = a + nZ := {a + nz : z ∈Z}.
Note that a given residue class modulo n has many diﬀerent “names”; for
example, the residue class [1]n is the same as the residue class [1 + n]n. For
any integer a in a residue class, we call a a representative of that class.
The following is simply a restatement of Theorem 2.1:
Theorem 2.9. For a positive integer n, there are precisely n distinct residue
classes modulo n, namely, [a]n for a = 0, . . . , n −1.
Fix a positive integer n. Let us deﬁne Zn as the set of residue classes
modulo n. We can “equip” Zn with binary operations deﬁning addition and
multiplication in a natural way as follows: for a, b ∈Z, we deﬁne
[a]n + [b]n := [a + b]n,
and we deﬁne
[a]n · [b]n := [a · b]n.
Of course, one has to check this deﬁnition is unambiguous, in the sense
that the sum or product of two residue classes should not depend on which
particular representatives of the classes are chosen in the above deﬁnitions.
More precisely, one must check that if [a]n = [a′]n and [b]n = [b′]n, then
[a op b]n = [a′ op b′]n, for op ∈{+, ·}.
However, this property follows
immediately from Theorem 2.3.
It is also convenient to deﬁne a negation operation on Zn, deﬁning
−[a]n := [−1]n · [a]n = [−a]n.
Having deﬁned addition and negation operations on Zn, we naturally deﬁne
a subtraction operation on Zn as follows: for a, b ∈Z,
[a]n −[b]n := [a]n + (−[b]n) = [a −b]n.
Example 2.5. Consider the residue classes modulo 6. These are as follows:
[0] = {. . . , −12, −6, 0, 6, 12, . . .}
[1] = {. . . , −11, −5, 1, 7, 13, . . .}
[2] = {. . . , −10, −4, 2, 8, 14, . . .}
[3] = {. . . , −9, −3, 3, 9, 15, . . .}
[4] = {. . . , −8, −2, 4, 10, 16, . . .}
[5] = {. . . , −7, −1, 5, 11, 17, . . .}
TEAM LinG

22
Congruences
Let us write down the addition and multiplication tables for Z6. The addi-
tion table looks like this:
+
[0]
[1]
[2]
[3]
[4]
[5]
[0]
[0]
[1]
[2]
[3]
[4]
[5]
[1]
[1]
[2]
[3]
[4]
[5]
[0]
[2]
[2]
[3]
[4]
[5]
[0]
[1]
[3]
[3]
[4]
[5]
[0]
[1]
[2]
[4]
[4]
[5]
[0]
[1]
[2]
[3]
[5]
[5]
[0]
[1]
[2]
[3]
[4]
The multiplication table looks like this:
·
[0]
[1]
[2]
[3]
[4]
[5]
[0]
[0]
[0]
[0]
[0]
[0]
[0]
[1]
[0]
[1]
[2]
[3]
[4]
[5]
[2]
[0]
[2]
[4]
[0]
[2]
[4]
[3]
[0]
[3]
[0]
[3]
[0]
[3]
[4]
[0]
[4]
[2]
[0]
[4]
[2]
[5]
[0]
[5]
[4]
[3]
[2]
[1]
2
These operations on Zn yield a very natural algebraic structure whose
salient properties are as follows:
Theorem 2.10. Let n be a positive integer, and consider the set Zn of
residue classes modulo n with addition and multiplication of residue classes
as deﬁned above. For all α, β, γ ∈Zn, we have
(i) α + β = β + α (addition is commutative),
(ii) (α + β) + γ = α + (β + γ) (addition is associative),
(iii) α + [0]n = α (existence of additive identity),
(iv) α −α = [0]n (existence of additive inverses),
(v) α · β = β · α (multiplication is commutative),
(vi) (α · β) · γ = α · (β · γ) (multiplication is associative),
(vii) α · (β + γ) = α · β + α · γ (multiplication distributes over addition)
(viii) α · [1]n = α (existence of multiplicative identity).
Proof. All of these properties follow easily from the corresponding properties
for the integers, together with the deﬁnitions of addition, subtraction, and
multiplication of residue classes. For example, for (i), we have
[a]n + [b]n = [a + b]n = [b + a]n = [b]n + [a]n,
TEAM LinG

2.3 Residue classes
23
where the ﬁrst and third equalities follow from the deﬁnition of addition
of residue classes, and the second equality follows from the commutativity
property of integer addition. The reader may verify the other properties
using similar arguments. 2
An algebraic structure satisfying the conditions in the above theorem is
known more generally as a “commutative ring with unity,” a notion that we
will discuss in Chapter 9.
Note that while all elements of Zn have an additive inverses, not all el-
ements of Zn have a multiplicative inverse. Indeed, for a ∈Z, the residue
class [a]n ∈Zn has a multiplicative inverse in Zn if and only if a has a
multiplicative inverse modulo n, which by Theorem 2.4, holds if and only
if gcd(a, n) = 1. Since multiplicative inverses modulo n are uniquely deter-
mined modulo n (see discussion following Theorem 2.5), it follows that if
α ∈Zn has a multiplicative inverse in Zn, then this inverse is unique, and
we may denote it by α−1.
One denotes by Z∗
n the set of all residue classes that have a multiplicative
inverse.
It is easy to see that Z∗
n is closed under multiplication; indeed,
if α, β ∈Z∗
n, then (αβ)−1 = α−1β−1.
Also, note that for α ∈Z∗
n and
β, β′ ∈Zn, if αβ = αβ′, we may eﬀectively cancel α from both sides of this
equation, obtaining β = β′ —this is just a restatement of the ﬁrst part of
Theorem 2.5 in the language of residue classes.
For α ∈Zn and positive integer k, the expression αk denotes the product
α · α · · · · · α, where there are k terms in the product. One may extend
this deﬁnition to k = 0, deﬁning α0 to be the multiplicative identity [1]n.
If α has a multiplicative inverse, then it is easy to see that for any integer
k ≥0, αk has a multiplicative inverse as well, namely, (α−1)k, which we may
naturally write as α−k.
In general, one has a choice between working with congruences modulo
n, or with the algebraic structure Zn; ultimately, the choice is one of taste
and convenience, and it depends on what one prefers to treat as “ﬁrst class
objects”: integers and congruence relations, or elements of Zn.
An alternative, and somewhat more concrete, approach to deﬁning Zn is
to simply deﬁne it to consist of the n “symbols” 0, 1, . . . , n −1, with addition
and multiplication deﬁned as
a + b := (a + b) mod n,
a · b := (a · b) mod n,
for a, b = 0, . . . , n−1. Such a deﬁnition is equivalent to the one we have given
here, with the symbol a corresponding to the residue class [a]n. One should
keep this alternative characterization of Zn in mind; however, we prefer the
TEAM LinG

24
Congruences
characterization in terms of residue classes, as it is mathematically more
elegant, and is usually more convenient to work with.
Exercise 2.11. Show that for any positive integer n, and any integer k,
the residue classes [k + a]n, for a = 0, . . . , n −1, are distinct and therefore
include all residue classes modulo n.
Exercise 2.12. Verify the following statements for Zn:
(a) There is only one element of Zn that acts as an additive identity; that
is, if α ∈Zn satisﬁes α + β = β for all β ∈Zn, then α = [0]n.
(b) Additive inverses in Zn are unique; that is, for all α ∈Zn, if α + β =
[0]n, then β = −α.
(c) If α ∈Z∗
n and γ, δ ∈Zn, then there exists a unique β ∈Zn such that
αβ + γ = δ.
Exercise 2.13. Verify the usual “rules of exponent arithmetic” for Zn. That
is, show that for α ∈Zn, and non-negative integers k1, k2, we have
(αk1)k2 = αk1k2 and αk1αk2 = αk1+k2.
Moreover, show that if α ∈Z∗
n, then these identities hold for all integers
k1, k2.
2.4 Euler’s phi function
Euler’s phi function φ(n) is deﬁned for positive integer n as the number
of elements of Z∗
n. Equivalently, φ(n) is equal to the number of integers
between 0 and n −1 that are relatively prime to n. For example, φ(1) = 1,
φ(2) = 1, φ(3) = 2, and φ(4) = 2.
A fact that is sometimes useful is the following:
Theorem 2.11. For any positive integer n, we have

d|n
φ(d) = n,
where the sum is over all positive divisors d of n.
Proof. Consider the list of n rational numbers 0/n, 1/n, . . . , (n −1)/n. For
any divisor d of n and for any integer a with 0 ≤a < d and gcd(a, d) = 1, the
fraction a/d appears in the list exactly once, and moreover, every number in
the sequence, when expressed as a fraction in lowest terms, is of this form.
2
TEAM LinG

2.5 Fermat’s little theorem
25
Using the Chinese remainder theorem, it is easy to get a nice formula
for φ(n) in terms for the prime factorization of n, as we establish in the
following sequence of theorems.
Theorem 2.12. For positive integers n, m with gcd(n, m) = 1, we have
φ(nm) = φ(n)φ(m).
Proof. Consider the map
ρ :
Znm →Zn × Zm
[a]nm →([a]n, [a]m).
First, note that the deﬁnition of ρ is unambiguous, since a ≡a′ (mod nm)
implies a ≡a′ (mod n) and a ≡a′ (mod m). Second, according to the Chi-
nese remainder theorem, the map ρ is one-to-one and onto. Moreover, it is
easy to see that gcd(a, nm) = 1 if and only if gcd(a, n) = 1 and gcd(a, m) = 1
(verify). Therefore, the map ρ carries Z∗
nm injectively onto Z∗
n ×Z∗
m. In par-
ticular, |Z∗
nm| = |Z∗
n × Z∗
m|. 2
Theorem 2.13. For a prime p and a positive integer e, we have φ(pe) =
pe−1(p −1).
Proof. The multiples of p among 0, 1, . . . , pe −1 are
0 · p, 1 · p, . . . , (pe−1 −1) · p,
of which there are precisely pe−1. Thus, φ(pe) = pe −pe−1 = pe−1(p −1). 2
As an immediate consequence of the above two theorems, we have:
Theorem 2.14. If n = pe1
1 · · · per
r is the factorization of n into primes, then
φ(n) =
r

i=1
pei−1
i
(pi −1) = n
r

i=1
(1 −1/pi).
Exercise 2.14. Show that φ(nm) = gcd(n, m) · φ(lcm(n, m)).
2.5 Fermat’s little theorem
Let n be a positive integer, and let a ∈Z with gcd(a, n) = 1. Consider the
sequence of powers of α := [a]n ∈Z∗
n:
[1]n = α0, α1, α2, . . . .
TEAM LinG

26
Congruences
Since each such power is an element of Z∗
n, and since Z∗
n is a ﬁnite set, this
sequence of powers must start to repeat at some point; that is, there must
be a positive integer k such that αk = αi for some i = 0, . . . , k −1. Let
us assume that k is chosen to be the smallest such positive integer. We
claim that i = 0, or equivalently, αk = [1]n. To see this, suppose by way of
contradiction that αk = αi, for some i = 1, . . . , k −1. Then we can cancel
α from both sides of the equation αk = αi, obtaining αk−1 = αi−1, and this
contradicts the minimality of k.
From the above discussion, we see that the ﬁrst k powers of α, that is,
[1]n = α0, α1, . . . , αk−1, are distinct, and subsequent powers of α simply
repeat this pattern.
More generally, we may consider both positive and
negative powers of α—it is easy to see (verify) that for all i, j ∈Z, we have
αi = αj if and only if i ≡j (mod k). In particular, we see that for any
integer i, we have αi = [1]n if and only if k divides i.
This value k is called the multiplicative order of α or the multiplica-
tive order of a modulo n. It can be characterized as the smallest positive
integer k such that
ak ≡1 (mod n).
Example 2.6. Let n = 7. For each value a = 1, . . . , 6, we can compute
successive powers of a modulo n to ﬁnd its multiplicative order modulo n.
i
1
2
3
4
5
6
1i mod 7
1
1
1
1
1
1
2i mod 7
2
4
1
2
4
1
3i mod 7
3
2
6
4
5
1
4i mod 7
4
2
1
4
2
1
5i mod 7
5
4
6
2
3
1
6i mod 7
6
1
6
1
6
1
So we conclude that modulo 7: 1 has order 1; 6 has order 2; 2 and 4 have
order 3; and 3 and 5 have order 6. 2
Theorem 2.15 (Euler’s Theorem). For any positive integer n, and any
integer a relatively prime to n, we have aφ(n) ≡1 (mod n). In particular,
the multiplicative order of a modulo n divides φ(n).
Proof. Let α := [a]n ∈Z∗
n. Consider the map f : Z∗
n →Z∗
n that sends β ∈Z∗
n
to αβ. Observe that f is injective, since if αβ = αβ′, we may cancel α from
both sides of this equation, obtaining β = β′. Since f maps Z∗
n injectively
into itself, and since Z∗
n is a ﬁnite set, it must be the case that f is surjective
TEAM LinG

2.5 Fermat’s little theorem
27
as well. Thus, as β ranges over the set Z∗
n, so does αβ, and we have

β∈Z∗n
β =

β∈Z∗n
(αβ) = αφ(n)
 
β∈Z∗n
β

.
(2.6)
Canceling the common factor 
β∈Z∗n β ∈Z∗
n from the left- and right-hand
side of (2.6), we obtain
αφ(n) = [1]n.
That proves the ﬁrst statement of the theorem. The second follows from
the observation made above that αi = [1]n if and only if the multiplicative
order of α divides i. 2
As a consequence of this, we obtain:
Theorem 2.16 (Fermat’s little theorem). For any prime p, and any
integer a ̸≡0 (mod p), we have ap−1 ≡1 (mod p).
Moreover, for any
integer a, we have ap ≡a (mod p).
Proof. The ﬁrst statement follows from Theorem 2.15, and the fact that
φ(p) = p −1.
The second statement is clearly true if a ≡0 (mod p),
and if a ̸≡0 (mod p), we simply multiply both sides of the congruence
ap−1 ≡1 (mod p) by a. 2
For a positive integer n, we say that a ∈Z with gcd(a, n) = 1 is a
primitive root modulo n if the multiplicative order of a modulo n is
equal to φ(n). If this is the case, then for α := [a]n, the powers αi range
over all elements of Z∗
n as i ranges over the interval 0, . . . , φ(n) −1. Not all
positive integers have primitive roots — we will see in §10.2 that the only
positive integers n for which there exists a primitive root modulo n are
n = 1, 2, 4, pe, 2pe,
where p is an odd prime and e is a positive integer.
Exercise 2.15. Find an integer whose multiplicative order modulo 101 is
100.
Exercise 2.16. Suppose α ∈Z∗
n has multiplicative order k. Show that for
any m ∈Z, the multiplicative order of αm is k/ gcd(m, k).
Exercise 2.17. Suppose α ∈Z∗
n has multiplicative order k, β ∈Z∗
n has
multiplicative order ℓ, and gcd(k, ℓ) = 1. Show that αβ has multiplicative
order kℓ. Hint: use the previous exercise.
TEAM LinG

28
Congruences
Exercise 2.18. Prove that for any prime p, we have
(p −1)! ≡−1 (mod p).
Hint: using the result of Exercise 2.5, we know that the only elements of Z∗
p
that act as their own multiplicative inverse are [±1]n; rearrange the terms
in the product 
β∈Z∗p β so that except for [±1]n, the terms are arranged in
pairs, where each pair consists of some β ∈Z∗
p and its multiplicative inverse.
2.6 Arithmetic functions and M¨obius inversion
A function, such as Euler’s function φ, from the positive integers into the
reals is sometimes called an arithmetic function (actually, one usually
considers complex-valued functions as well, but we shall not do so here).
An arithmetic function f is called multiplicative if f(1) = 1 and for all
positive integers n, m with gcd(n, m) = 1, we have f(nm) = f(n)f(m).
Theorem 2.12 simply says that φ is multiplicative.
In this section, we develop some of the theory of arithmetic functions that
is pertinent to number theory; however, the results in this section will play
only a very minor role in the remainder of the text.
We begin with a simple observation, which the reader may easily verify:
if f is a multiplicative function, and if n = pe1
1 · · · per
r
is the
prime factorization of n, then
f(n) = f(pe1
1 ) · · · f(per
r ).
Next, we deﬁne a binary operation on arithmetic functions that has a
number of interesting properties and applications. Let f and g be arith-
metic functions. The Dirichlet product of f and g, denoted f ⋆g, is the
arithmetic function whose value at n is deﬁned by the formula
(f ⋆g)(n) :=

d|n
f(d)g(n/d),
the sum being over all positive divisors d of n. Another, more symmetric,
way to write this is
(f ⋆g)(n) =

n=d1d2
f(d1)g(d2),
the sum being over all pairs (d1, d2) of positive integers with d1d2 = n. The
Dirichlet product is clearly commutative (i.e., f⋆g = g⋆f), and is associative
TEAM LinG

2.6 Arithmetic functions and M¨obius inversion
29
as well, which one can see by checking that
(f ⋆(g ⋆h))(n) =

n=d1d2d3
f(d1)g(d2)h(d3) = ((f ⋆g) ⋆h)(n),
the sum being over all triples (d1, d2, d3) of positive integers with d1d2d3 = n.
We now introduce three special arithmetic functions: I, J, and µ. The
function I(n) is deﬁned to be 1 when n = 1 and 0 when n > 1. The function
J(n) is deﬁned to be 1 for all n.
The M¨obius function µ is deﬁned for positive integers n as follows:
µ(n) :=
 0
if n is divisible by a square other than 1;
(−1)r
if n is the product of r ≥0 distinct primes.
Thus, if n = pe1
1 · · · per
r is the prime factorization of n, then µ(n) = 0 if ei > 1
for some i, and otherwise, µ(n) = (−1)r. Here are some examples:
µ(1) = 1, µ(2) = −1, µ(3) = −1, µ(4) = 0, µ(5) = −1, µ(6) = 1.
It is easy to see (verify) that for any arithmetic function f, we have
I ⋆f = f and (J ⋆f)(n) =

d|n
f(d).
Also, the functions I, J, and µ are multiplicative (verify). A useful property
of the M¨obius function is the following:
Theorem 2.17. For any multiplicative function f, if n = pe1
1 · · · per
r
is the
prime factorization of n, we have

d|n
µ(d)f(d) = (1 −f(p1)) · · · (1 −f(pr)).
(2.7)
In case r = 0 (i.e., n = 1), the product on the right-hand side of (2.7) is
interpreted (as usual) as 1.
Proof. The non-zero terms in the sum on the left-hand side of (2.7) are those
corresponding to divisors d of the form pi1 · · · piℓ, where pi1, . . . , piℓare dis-
tinct; the value contributed to the sum by such a term is (−1)ℓf(pi1 · · · piℓ) =
(−1)ℓf(pi1) · · · f(piℓ). These are the same as the terms in the expansion of
the product on the right-hand side of (2.7). 2
For example, suppose f(d) = 1/d in the above theorem, and let n =
pe1
1 · · · per
r be the prime factorization of n. Then we obtain:

d|n
µ(d)/d = (1 −1/p1) · · · (1 −1/pr).
(2.8)
TEAM LinG

30
Congruences
As another example, suppose f = J. Then we obtain
(µ ⋆J)(n) =

d|n
µ(d) =
r

i=1
(1 −1),
which is 1 if n = 1, and is zero if n > 1. Thus, we have
µ ⋆J = I.
(2.9)
Theorem 2.18 (M¨obius inversion formula). Let f and F be arithmetic
functions. Then we have F = J ⋆f if and only if f = µ ⋆F.
Proof. If F = J ⋆f, then
µ ⋆F = µ ⋆(J ⋆f) = (µ ⋆J) ⋆f = I ⋆f = f,
and conversely, if f = µ ⋆F, then
J ⋆f = J ⋆(µ ⋆F) = (J ⋆µ) ⋆F = I ⋆F = F. 2
The M¨obius inversion formula says this:
F(n) =

d|n
f(d) for all positive integers n
if and only if
f(n) =

d|n
µ(d)F(n/d) for all positive integers n.
As an application of the M¨obius inversion formula, we can get a diﬀerent
proof of Theorem 2.14, based on Theorem 2.11. Let F(n) := n and f(n) :=
φ(n). Theorem 2.11 says that F = J ⋆f. Applying M¨obius inversion to this
yields f = µ ⋆F, and using (2.8), we obtain
φ(n) =

d|n
µ(d)n/d = n

d|n
µ(d)/d
= n(1 −1/p1) · · · (1 −1/pr).
Of course, one could turn the above argument around, using M¨obius in-
version and (2.8) to derive Theorem 2.11 from Theorem 2.14.
Exercise 2.19. In our deﬁnition of a multiplicative function f, we made
the requirement that f(1) = 1. Show that if we dropped this requirement,
the only other function that would satisfy the deﬁnition would be the zero
function (i.e., the function that is everywhere zero).
TEAM LinG

2.6 Arithmetic functions and M¨obius inversion
31
Exercise 2.20. Let f be a polynomial with integer coeﬃcients, and for
positive integer n deﬁne ωf(n) to be the number of integers z ∈{0, . . . , n−1}
such that f(z) ≡0 (mod n). Show that ωf is multiplicative.
Exercise 2.21. Show that if f and g are multiplicative, then so is f ⋆g.
Exercise 2.22. Deﬁne τ(n) to be the number of positive divisors of n.
(a) Show that τ is a multiplicative function.
(b) Show that
τ(n) = (e1 + 1) · · · (er + 1),
where n = pe1
1 · · · per
r is the prime factorization of n.
(c) Show that

d|n
µ(d)τ(n/d) = 1.
(d) Show that

d|n
µ(d)τ(d) = (−1)r,
where n = pe1
1 · · · per
r is the prime factorization of n.
Exercise 2.23. Deﬁne σ(n) := 
d|n d.
(a) Show that σ is a multiplicative function.
(b) Show that
σ(n) =
r

i=1
pei+1
i
−1
pi −1 ,
where n = pe1
1 · · · per
r is the prime factorization of n.
(c) Show that

d|n
µ(d)σ(n/d) = n.
(d) Show that

d|n
µ(d)σ(d) = (−1)rp1 · · · pr,
where n = pe1
1 · · · per
r is the prime factorization of n.
TEAM LinG

32
Congruences
Exercise 2.24. The Mangoldt function Λ(n) is deﬁned for all positive
integers n by
Λ(n) :=
 log p
if n = pk, where p is prime and k is a positive integer;
0
otherwise.
(a) Show that

d|n
Λ(d) = log n.
(b) Using part (a), show that
Λ(n) = −

d|n
µ(d) log d.
Exercise 2.25. Show that if f is multiplicative, and if n = pe1
1 · · · per
r is the
prime factorization of n, then

d|n
(µ(d))2f(d) = (1 + f(p1)) · · · (1 + f(pr)).
Exercise 2.26. Show that n is square-free (see Exercise 1.13) if and only if

d|n(µ(d))2φ(d) = n.
Exercise 2.27. Show that for any arithmetic function f with f(1) ̸= 0,
there is a unique arithmetic function g, called the Dirichlet inverse of f,
such that f ⋆g = I. Also, show that if f(1) = 0, then f has no Dirichlet
inverse.
Exercise 2.28. Show that if f is a multiplicative function, then so is its
Dirichlet inverse (as deﬁned in the previous exercise).
TEAM LinG

3
Computing with large integers
In this chapter, we review standard asymptotic notation, introduce the for-
mal computational model we shall use throughout the rest of the text, and
discuss basic algorithms for computing with large integers.
3.1 Asymptotic notation
We review some standard notation for relating the rate of growth of func-
tions. This notation will be useful in discussing the running times of algo-
rithms, and in a number of other contexts as well.
Suppose that x is a variable taking non-negative integer or real values,
and let g denote a real-valued function in x that is positive for all suﬃciently
large x; also, let f denote any real-valued function in x. Then
• f = O(g) means that |f(x)| ≤cg(x) for some positive constant c and
all suﬃciently large x (read, “f is big-O of g”),
• f = Ω(g) means that f(x) ≥cg(x) for some positive constant c and
all suﬃciently large x (read, “f is big-Omega of g”),
• f = Θ(g) means that cg(x) ≤f(x) ≤dg(x), for some positive con-
stants c and d and all suﬃciently large x (read, “f is big-Theta of
g”),
• f = o(g) means that f/g →0 as x →∞(read, “f is little-o of g”),
and
• f ∼g means that f/g →1 as x →∞(read, “f is asymptotically
equal to g”).
Example 3.1. Let f(x) := x2 and g(x) := 2x2 −x+1. Then f = O(g) and
f = Ω(g). Indeed, f = Θ(g). 2
Example 3.2. Let f(x) := x2 and g(x) := x2 −2x + 1. Then f ∼g. 2
33
TEAM LinG

34
Computing with large integers
Example 3.3. Let f(x) := 1000x2 and g(x) := x3. Then f = o(g). 2
Let us call a function in x eventually positive if it takes positive values
for all suﬃciently large x. Note that by deﬁnition, if we write f = Ω(g),
f = Θ(g), or f ∼g, it must be the case that f (in addition to g) is eventually
positive; however, if we write f = O(g) or f = o(g), then f need not be
eventually positive.
When one writes “f = O(g),” one should interpret “· = O(·)” as a binary
relation between f with g. Analogously for “f = Ω(g),” “f = Θ(g),” and
“f = o(g).”
One may also write “O(g)” in an expression to denote an anonymous
function f such that f = O(g). As an example, one could write n
i=1 i =
n2/2 + O(n). Analogously, Ω(g), Θ(g), and o(g) may denote anonymous
functions.
The expression O(1) denotes a function bounded in absolute
value by a constant, while the expression o(1) denotes a function that tends
to zero in the limit.
As an even further use (abuse?) of the notation, one may use the big-O,
-Omega, and -Theta notation for functions on an arbitrary domain, in which
case the relevant bound should hold throughout the entire domain.
Exercise 3.1. Show that
(a) f = o(g) implies f = O(g) and g ̸= O(f);
(b) f = O(g) and g = O(h) implies f = O(h);
(c) f = O(g) and g = o(h) implies f = o(h);
(d) f = o(g) and g = O(h) implies f = o(h).
Exercise 3.2. Let f and g be eventually positive functions in x. Show that
(a) f ∼g if and only if f = (1 + o(1))g;
(b) f ∼g implies f = Θ(g);
(c) f = Θ(g) if and only if f = O(g) and f = Ω(g);
(d) f = Ω(g) if and only if g = O(f).
Exercise 3.3. Let f and g be eventually positive functions in x, and suppose
f/g tends to a limit L (possibly L = ∞) as x →∞. Show that
(a) if L = 0, then f = o(g);
(b) if 0 < L < ∞, then f = Θ(g);
(c) if L = ∞, then g = o(f).
Exercise 3.4. Order the following functions in x so that for each adjacent
TEAM LinG

3.1 Asymptotic notation
35
pair f, g in the ordering, we have f = O(g), and indicate if f = o(g), f ∼g,
or g = O(f):
x3, exx2, 1/x,
x2(x + 100) + 1/x, x + √x, log2 x, log3 x, 2x2, x,
e−x, 2x2 −10x + 4, ex+√x, 2x, 3x, x−2, x2(log x)1000.
Exercise 3.5. Suppose that x takes non-negative integer values, and that
g(x) > 0 for all x ≥x0 for some x0. Show that f = O(g) if and only if
|f(x)| ≤cg(x) for some positive constant c and all x ≥x0.
Exercise 3.6. Give an example of two non-decreasing functions f and g,
both mapping positive integers to positive integers, such that f ̸= O(g) and
g ̸= O(f).
Exercise 3.7. Show that
(a) the relation “∼” is an equivalence relation on the set of eventually
positive functions;
(b) for eventually positive functions f1, f2, g2, g2, if f1 ∼f2 and g1 ∼g2,
then f1 ⋆g1 ∼f2 ⋆g2, where “⋆” denotes addition, multiplication, or
division;
(c) for eventually positive functions f1, f2, and any function g that tends
to inﬁnity as x →∞, if f1 ∼f2, then f1 ◦g ∼f2 ◦g, where “◦”
denotes function composition.
Exercise 3.8. Show that all of the claims in the previous exercise also hold
when the relation “∼” is replaced with the relation “· = Θ(·).”
Exercise 3.9. Let f1, f2 be eventually positive functions.
Show that if
f1 ∼f2, then log(f1) = log(f2) + o(1), and in particular, if log(f1) = Ω(1),
then log(f1) ∼log(f2).
Exercise 3.10. Suppose that f and g are functions deﬁned on the integers
k, k + 1, . . ., and that g is eventually positive. For n ≥k, deﬁne F(n) :=
n
i=k f(i) and G(n) := n
i=k g(i). Show that if f = O(g) and G is eventually
positive, then F = O(G).
Exercise 3.11. Suppose that f and g are functions deﬁned on the integers
k, k+1, . . ., both of which are eventually positive. For n ≥k, deﬁne F(n) :=
n
i=k f(i) and G(n) := n
i=k g(i). Show that if f ∼g and G(n) →∞as
n →∞, then F ∼G.
The following two exercises are continuous variants of the previous two
exercises. To avoid unnecessary distractions, we shall only consider functions
TEAM LinG

36
Computing with large integers
that are quite “well behaved.” In particular, we restrict ourselves to piece-
wise continuous functions (see §A3).
Exercise 3.12. Suppose that f and g are piece-wise continuous on [a, ∞),
and that g is eventually positive. For x ≥a, deﬁne F(x) :=
 x
a f(t)dt and
G(x) :=
 x
a g(t)dt. Show that if f = O(g) and G is eventually positive, then
F = O(G).
Exercise 3.13. Suppose that f and g are piece-wise continuous [a, ∞), both
of which are eventually positive. For x ≥a, deﬁne F(x) :=
 x
a f(t)dt and
G(x) :=
 x
a g(t)dt. Show that if f ∼g and G(x) →∞as x →∞, then
F ∼G.
3.2 Machine models and complexity theory
When presenting an algorithm, we shall always use a high-level, and some-
what informal, notation. However, all of our high-level descriptions can be
routinely translated into the machine-language of an actual computer. So
that our theorems on the running times of algorithms have a precise mathe-
matical meaning, we formally deﬁne an “idealized” computer: the random
access machine or RAM.
A RAM consists of an unbounded sequence of memory cells
m[0], m[1], m[2], . . .
each of which can store an arbitrary integer, together with a program. A
program consists of a ﬁnite sequence of instructions I0, I1, . . ., where each
instruction is of one of the following types:
arithmetic This type of instruction is of the form α ←β ⋆γ, where ⋆rep-
resents one of the operations addition, subtraction, multiplication,
or integer division (i.e., ⌊·/·⌋). The values β and γ are of the form c,
m[a], or m[m[a]], and α is of the form m[a] or m[m[a]], where c is an
integer constant and a is a non-negative integer constant. Execution
of this type of instruction causes the value β ⋆γ to be evaluated and
then stored in α.
branching This type of instruction is of the form IF β 3 γ GOTO i, where
i is the index of an instruction, and where 3 is one of the comparison
operations =, ̸=, <, >, ≤, ≥, and β and γ are as above. Execution of
this type of instruction causes the “ﬂow of control” to pass condi-
tionally to instruction Ii.
halt The HALT instruction halts the execution of the program.
TEAM LinG

3.2 Machine models and complexity theory
37
A RAM executes by executing instruction I0, and continues to execute
instructions, following branching instructions as appropriate, until a HALT
instruction is executed.
We do not specify input or output instructions, and instead assume that
the input and output are to be found in memory at some prescribed location,
in some standardized format.
To determine the running time of a program on a given input, we charge
1 unit of time to each instruction executed.
This model of computation closely resembles a typical modern-day com-
puter, except that we have abstracted away many annoying details. How-
ever, there are two details of real machines that cannot be ignored; namely,
any real machine has a ﬁnite number of memory cells, and each cell can
store numbers only in some ﬁxed range.
The ﬁrst limitation must be dealt with by either purchasing suﬃcient
memory or designing more space-eﬃcient algorithms.
The second limitation is especially annoying, as we will want to perform
computations with quite large integers—much larger than will ﬁt into any
single memory cell of an actual machine. To deal with this limitation, we
shall represent such large integers as vectors of digits to some ﬁxed base, so
that each digit is bounded so as to ﬁt into a memory cell. This is discussed in
more detail in the next section. Using this strategy, the only other numbers
we actually need to store in memory cells are “small” numbers represent-
ing array indices, addresses, and the like, which hopefully will ﬁt into the
memory cells of actual machines.
Thus, whenever we speak of an algorithm, we shall mean an algorithm that
can be implemented on a RAM, such that all numbers stored in memory cells
are “small” numbers, as discussed above. Admittedly, this is a bit imprecise.
For the reader who demands more precision, we can make a restriction such
as the following: there exist positive constants c and d, such that at any
point in the computation, if k memory cells have been written to (including
inputs), then all numbers stored in memory cells are bounded by kc + d in
absolute value.
Even with these caveats and restrictions, the running time as we have de-
ﬁned it for a RAM is still only a rough predictor of performance on an actual
machine. On a real machine, diﬀerent instructions may take signiﬁcantly dif-
ferent amounts of time to execute; for example, a division instruction may
take much longer than an addition instruction. Also, on a real machine, the
behavior of the cache may signiﬁcantly aﬀect the time it takes to load or
store the operands of an instruction. Finally, the precise running time of an
TEAM LinG

38
Computing with large integers
algorithm given by a high-level description will depend on the quality of the
translation of this algorithm into “machine code.” However, despite all of
these problems, it still turns out that measuring the running time on a RAM
as we propose here is nevertheless a good “ﬁrst order” predictor of perfor-
mance on real machines in many cases. Also, we shall only state the running
time of an algorithm using a big-O estimate, so that implementation-speciﬁc
constant factors are anyway “swept under the rug.”
If we have an algorithm for solving a certain type of problem, we expect
that “larger” instances of the problem will require more time to solve than
“smaller” instances. Theoretical computer scientists sometimes equate the
notion of an “eﬃcient” algorithm with that of a polynomial-time algo-
rithm (although not everyone takes theoretical computer scientists very se-
riously, especially on this point). A polynomial-time algorithm is one whose
running time on inputs of length n is bounded by nc + d for some constants
c and d (a “real” theoretical computer scientist will write this as nO(1)). To
make this notion mathematically precise, one needs to deﬁne the length of
an algorithm’s input.
To deﬁne the length of an input, one chooses a “reasonable” scheme to
encode all possible inputs as a string of symbols from some ﬁnite alphabet,
and then deﬁnes the length of an input as the number of symbols in its
encoding.
We will be dealing with algorithms whose inputs consist of arbitrary in-
tegers, or lists of such integers. We describe a possible encoding scheme
using the alphabet consisting of the six symbols ‘0’, ‘1’, ‘-’, ‘,’, ‘(’, and ‘)’.
An integer is encoded in binary, with possibly a negative sign. Thus, the
length of an integer x is approximately equal to log2 |x|. We can encode
a list of integers x1, . . . , xn as “(¯x1, . . . , ¯xn)”, where ¯xi is the encoding of
xi. We can also encode lists of lists, and so on, in the obvious way. All of
the mathematical objects we shall wish to compute with can be encoded in
this way. For example, to encode an n × n matrix of rational numbers, we
may encode each rational number as a pair of integers (the numerator and
denominator), each row of the matrix as a list of n encodings of rational
numbers, and the matrix as a list of n encodings of rows.
It is clear that other encoding schemes are possible, giving rise to diﬀerent
deﬁnitions of input length. For example, we could encode inputs in some
base other than 2 (but not unary!) or use a diﬀerent alphabet. Indeed, it
is typical to assume, for simplicity, that inputs are encoded as bit strings.
However, such an alternative encoding scheme would change the deﬁnition
TEAM LinG

3.3 Basic integer arithmetic
39
of input length by at most a constant multiplicative factor, and so would
not aﬀect the notion of a polynomial-time algorithm.
Note that algorithms may use data structures for representing mathe-
matical objects that look quite diﬀerent from whatever encoding scheme
one might choose. Indeed, our mathematical objects may never actually be
written down using our encoding scheme (either by us or our programs)—
the encoding scheme is a purely conceptual device that allows us to express
the running time of an algorithm as a function of the length of its input.
Also note that in deﬁning the notion of polynomial time on a RAM, it
is essential that we restrict the sizes of numbers that may be stored in the
machine’s memory cells, as we have done above. Without this restriction,
a program could perform arithmetic on huge numbers, being charged just
one unit of time for each arithmetic operation—not only is this intuitively
“wrong,” it is possible to come up with programs that solve some problems
using a polynomial number of arithmetic operations on huge numbers, and
these problems cannot otherwise be solved in polynomial time (see §3.6).
3.3 Basic integer arithmetic
We will need algorithms to manipulate integers of arbitrary length. Since
such integers will exceed the word-size of actual machines, and to satisfy the
formal requirements of our random access model of computation, we shall
represent large integers as vectors of digits to some base B, along with a bit
indicating the sign. That is, for a ∈Z, if we write
a = ±
k−1

i=0
aiBi = ±(ak−1 · · · a1a0)B,
where 0 ≤ai < B for i = 0, . . . , k −1, then a will be represented in memory
as a data structure consisting of the vector of base-B digits a0, . . . , ak−1,
along with a “sign bit” to indicate the sign of a. When a is non-zero, the
high-order digit ak−1 in this representation should be non-zero.
For our purposes, we shall consider B to be a constant, and moreover, a
power of 2. The choice of B as a power of 2 is convenient for a number of
technical reasons.
A note to the reader: If you are not interested in the low-level details
of algorithms for integer arithmetic, or are willing to take them on faith,
you may safely skip ahead to §3.3.5, where the results of this section are
summarized.
We now discuss in detail basic arithmetic algorithms for unsigned (i.e.,
TEAM LinG

40
Computing with large integers
non-negative) integers—these algorithms work with vectors of base-B dig-
its, and except where explicitly noted, we do not assume the high-order
digits of the input vectors are non-zero, nor do these algorithms ensure that
the high-order digit of the output vector is non-zero. These algorithms can
be very easily adapted to deal with arbitrary signed integers, and to take
proper care that the high-order digit of the vector representing a non-zero
number is non-zero (the reader is asked to ﬁll in these details in some of the
exercises below). All of these algorithms can be implemented directly in a
programming language that provides a “built-in” signed integer type that
can represent all integers of absolute value less than B2, and that provides
the basic arithmetic operations (addition, subtraction, multiplication, inte-
ger division). So, for example, using the C or Java programming language’s
int type on a typical 32-bit computer, we could take B = 215. The resulting
software would be reasonably eﬃcient, but certainly not the best possible.
Suppose we have the base-B representations of two unsigned integers a
and b. We present algorithms to compute the base-B representation of a+b,
a −b, a · b, ⌊a/b⌋, and a mod b. To simplify the presentation, for integers
x, y with y ̸= 0, we write divmod(x, y) to denote (⌊x/y⌋, x mod y).
3.3.1 Addition
Let a = (ak−1 · · · a0)B and b = (bℓ−1 · · · b0)B be unsigned integers. Assume
that k ≥ℓ≥1 (if k < ℓ, then we can just swap a and b). The sum c := a+b
is of the form c = (ckck−1 · · · c0)B. Using the standard “paper-and-pencil”
method (adapted from base-10 to base-B, of course), we can compute the
base-B representation of a + b in time O(k), as follows:
carry ←0
for i ←0 to ℓ−1 do
tmp ←ai + bi + carry, (carry, ci) ←divmod(tmp, B)
for i ←ℓto k −1 do
tmp ←ai + carry, (carry, ci) ←divmod(tmp, B)
ck ←carry
Note that in every loop iteration, the value of carry is 0 or 1, and the
value tmp lies between 0 and 2B −1.
3.3.2 Subtraction
Let a = (ak−1 · · · a0)B and b = (bℓ−1 · · · b0)B be unsigned integers. Assume
that k ≥ℓ≥1. To compute the diﬀerence c := a −b, we may use the same
TEAM LinG

3.3 Basic integer arithmetic
41
algorithm as above, but with the expression “ai + bi” replaced by “ai −bi.”
In every loop iteration, the value of carry is 0 or −1, and the value of tmp
lies between −B and B−1. If a ≥b, then ck = 0 (i.e., there is no carry out of
the last loop iteration); otherwise, ck = −1 (and b−a = Bk −(ck−1 · · · c0)B,
which can be computed with another execution of the subtraction routine).
3.3.3 Multiplication
Let a = (ak−1 · · · a0)B and b = (bℓ−1 · · · b0)B be unsigned integers, with
k ≥1 and ℓ≥1. The product c := a · b is of the form (ck+ℓ−1 · · · c0)B, and
may be computed in time O(kℓ) as follows:
for i ←0 to k + ℓ−1 do ci ←0
for i ←0 to k −1 do
carry ←0
for j ←0 to ℓ−1 do
tmp ←aibj + ci+j + carry
(carry, ci+j) ←divmod(tmp, B)
ci+ℓ←carry
Note that at every step in the above algorithm, the value of carry lies
between 0 and B −1, and the value of tmp lies between 0 and B2 −1.
3.3.4 Division with remainder
Let a = (ak−1 · · · a0)B and b = (bℓ−1 · · · b0)B be unsigned integers, with
k ≥1, ℓ≥1, and bℓ−1 ̸= 0.
We want to compute q and r such that
a = bq + r and 0 ≤r < b. Assume that k ≥ℓ; otherwise, a < b, and we can
just set q ←0 and r ←a. The quotient q will have at most m := k −ℓ+ 1
base-B digits. Write q = (qm−1 · · · q0)B.
At a high level, the strategy we shall use to compute q and r is the
following:
r ←a
for i ←m −1 down to 0 do
qi ←⌊r/Bib⌋
r ←r −Bi · qib
One easily veriﬁes by induction that at the beginning of each loop itera-
tion, we have 0 ≤r < Bi+1b, and hence each qi will be between 0 and B −1,
as required.
Turning the above strategy into a detailed algorithm takes a bit of work.
TEAM LinG

42
Computing with large integers
In particular, we want an easy way to compute ⌊r/Bib⌋. Now, we could
in theory just try all possible choices for qi —this would take time O(Bℓ),
and viewing B as a constant, this is O(ℓ). However, this is not really very
desirable from either a practical or theoretical point of view, and we can do
much better with just a little eﬀort.
We shall ﬁrst consider a special case; namely, the case where ℓ= 1. In this
case, the computation of the quotient ⌊r/Bib⌋is facilitated by the following,
which essentially tells us that this quotient is determined by the two high-
order digits of r:
Theorem 3.1. Let x and y be integers such that
0 ≤x = x′2n + s and 0 < y = y′2n
for some integers n, s, x′, y′, with n ≥0 and 0 ≤s < 2n. Then ⌊x/y⌋=
⌊x′/y′⌋.
Proof. We have
x
y = x′
y′ +
s
y′2n ≥x′
y′ .
It follows immediately that ⌊x/y⌋≥⌊x′/y′⌋.
We also have
x
y = x′
y′ +
s
y′2n < x′
y′ + 1
y′ ≤
	x′
y′

+ y′ −1
y′

+ 1
y′ .
Thus, we have x/y < ⌊x′/y′⌋+ 1, and hence, ⌊x/y⌋≤⌊x′/y′⌋. 2
From this theorem, one sees that the following algorithm correctly com-
putes the quotient and remainder in time O(k) (in the case ℓ= 1):
carry ←0
for i ←k −1 down to 0 do
tmp ←carry · B + ai
(carry, qi) ←divmod(tmp, b0)
output the quotient q = (qk−1 · · · q0)B and the remainder carry
Note that in every loop iteration, the value of carry lies between 0 and
b0 ≤B −1, and the value of tmp lies between 0 and B ·b0+(B −1) ≤B2−1.
That takes care of the special case where ℓ= 1. Now we turn to the
general case ℓ≥1. In this case, we cannot so easily get the digits qi of
the quotient, but we can still fairly easily estimate these digits, using the
following:
TEAM LinG

3.3 Basic integer arithmetic
43
Theorem 3.2. Let x and y be integers such that
0 ≤x = x′2n + s and 0 < y = y′2n + t
for some integers n, s, t, x′, y′ with n ≥0, 0 ≤s < 2n, and 0 ≤t < 2n.
Further suppose that 2y′ ≥x/y. Then we have
⌊x/y⌋≤⌊x′/y′⌋≤⌊x/y⌋+ 2.
Proof. For the ﬁrst inequality, note that x/y ≤x/(y′2n), and so ⌊x/y⌋≤
⌊x/(y′2n)⌋, and by the previous theorem, ⌊x/(y′2n)⌋= ⌊x′/y′⌋. That proves
the ﬁrst inequality.
For the second inequality, ﬁrst note that from the deﬁnitions, x/y ≥
x′/(y′+1), which is equivalent to x′y−xy′−x ≤0. Now, the inequality 2y′ ≥
x/y is equivalent to 2yy′ −x ≥0, and combining this with the inequality
x′y −xy′ −x ≤0, we obtain 2yy′ −x ≥x′y −xy′ −x, which is equivalent to
x/y ≥x′/y′ −2. It follows that ⌊x/y⌋≥⌊x′/y′⌋−2. That proves the second
inequality. 2
Based on this theorem, we ﬁrst present an algorithm for division with re-
mainder that works assuming that b is appropriately “normalized,” meaning
that bℓ−1 ≥2w−1, where B = 2w. This algorithm is shown in Fig. 3.1.
Some remarks are in order:
1. In line 4, we compute qi, which by Theorem 3.2 is greater than or
equal to the true quotient digit, but exceeds this value by at most 2.
2. In line 5, we reduce qi if it is obviously too big.
3. In lines 6–10, we compute
(ri+ℓ· · · ri)B ←(ri+ℓ· · · ri)B −qib.
In each loop iteration, the value of tmp lies between −(B2 −B) and
B −1, and the value carry lies between −(B −1) and 0.
4. If the estimate qi is too large, this is manifested by a negative value
of ri+ℓat line 10. Lines 11–17 detect and correct this condition: the
loop body here executes at most twice; in lines 12–16, we compute
(ri+ℓ· · · ri)B ←(ri+ℓ· · · ri)B + (bℓ−1 · · · b0)B.
Just as in the algorithm in §3.3.1, in every iteration of the loop in lines
13–15, the value of carry is 0 or 1, and the value tmp lies between 0
and 2B −1.
It is quite easy to see that the running time of the above algorithm is
O(ℓ· (k −ℓ+ 1)).
TEAM LinG

44
Computing with large integers
1.
for i ←0 to k −1 do ri ←ai
2.
rk ←0
3.
for i ←k −ℓdown to 0 do
4.
qi ←⌊(ri+ℓB + ri+ℓ−1)/bℓ−1⌋
5.
if qi ≥B then qi ←B −1
6.
carry ←0
7.
for j ←0 to ℓ−1 do
8.
tmp ←ri+j −qibj + carry
9.
(carry, ri+j) ←divmod(tmp, B)
10.
ri+ℓ←ri+ℓ+ carry
11.
while ri+ℓ< 0 do
12.
carry ←0
13.
for j ←0 to ℓ−1 do
14.
tmp ←ri+j + bi + carry
15.
(carry, ri+j) ←divmod(tmp, B)
16.
ri+ℓ←ri+ℓ+ carry
17.
qi ←qi −1
18.
output the quotient q = (qk−ℓ· · · q0)B
and the remainder r = (rℓ−1 · · · r0)B
Fig. 3.1. Division with Remainder Algorithm
Finally, consider the general case, where b may not be normalized. We
multiply both a and b by an appropriate value 2w′, with 0 ≤w′ < w,
obtaining a′ := a2w′ and b′ := 2w′, where b′ is normalized; alternatively, we
can use a more eﬃcient, special-purpose “left shift” algorithm to achieve
the same eﬀect. We then compute q and r′ such that a′ = b′q + r′, using
the above division algorithm for the normalized case.
Observe that q =
⌊a′/b′⌋= ⌊a/b⌋, and r′ = r2w′, where r = a mod b. To recover r, we simply
divide r′ by 2w′, which we can do either using the above “single precision”
division algorithm, or by using a special-purpose “right shift” algorithm. All
of this normalizing and denormalizing takes time O(k + ℓ). Thus, the total
running time for division with remainder is still O(ℓ· (k −ℓ+ 1)).
Exercise 3.14. Work out the details of algorithms for arithmetic on signed
integers, using the above algorithms for unsigned integers as subroutines.
You should give algorithms for addition, subtraction, multiplication, and
TEAM LinG

3.3 Basic integer arithmetic
45
division with remainder of arbitrary signed integers (for division with re-
mainder, your algorithm should compute ⌊a/b⌋and a mod b). Make sure
your algorithm correctly computes the sign bit of the result, and also strips
leading zero digits from the result.
Exercise 3.15. Work out the details of an algorithm that compares two
signed integers a and b, determining which of a < b, a = b, or a > b holds.
Exercise 3.16. Suppose that we run the division with remainder algorithm
in Fig. 3.1 for ℓ> 1 without normalizing b, but instead, we compute the
value qi in line 4 as follows:
qi ←⌊(ri+ℓB2 + ri+ℓ−1B + ri+ℓ−2)/(bℓ−1B + bℓ−2)⌋.
Show that qi is either equal to the correct quotient digit, or the correct
quotient digit plus 1. Note that a limitation of this approach is that the
numbers involved in the computation are larger than B2.
Exercise 3.17. Work out the details for an algorithm that shifts a given
unsigned integer a to the left by a speciﬁed number of bits s (i.e., computes
b := a · 2s). The running time of your algorithm should be linear in the
number of digits of the output.
Exercise 3.18. Work out the details for an algorithm that shifts a given
unsigned integer a to the right by a speciﬁed number of bits s (i.e., computes
b := ⌊a/2s⌋). The running time of your algorithm should be linear in the
number of digits of the output.
Now modify your algorithm so that it
correctly computes ⌊a/2s⌋for signed integers a.
Exercise 3.19. This exercise is for C/Java programmers. Evaluate the
C/Java expressions
(-17) % 4;
(-17) & 3;
and compare these values with (−17) mod 4.
Also evaluate the C/Java
expressions
(-17) / 4;
(-17) >> 2;
and compare with ⌊−17/4⌋. Explain your ﬁndings.
Exercise 3.20. This exercise is also for C/Java programmers. Suppose
that values of type int are stored using a 32-bit 2’s complement representa-
tion, and that all basic arithmetic operations are computed correctly modulo
232, even if an “overﬂow” happens to occur. Also assume that double pre-
cision ﬂoating point has 53 bits of precision, and that all basic arithmetic
TEAM LinG

46
Computing with large integers
operations give a result with a relative error of at most 2−53. Also assume
that conversion from type int to double is exact, and that conversion from
double to int truncates the fractional part. Now, suppose we are given int
variables a, b, and n, such that 1 < n < 230, 0 ≤a < n, and 0 ≤b < n.
Show that after the following code sequence is executed, the value of r is
equal to (a · b) mod n:
int q;
q
= (int) ((((double) a) * ((double) b)) / ((double) n));
r = a*b - q*n;
if (r >= n)
r = r - n;
else if (r < 0)
r = r + n;
3.3.5 Summary
We now summarize the results of this section. For an integer a, we deﬁne
len(a) to be the number of bits in the binary representation of |a|; more
precisely,
len(a) :=
 ⌊log2 |a|⌋+ 1
if a ̸= 0,
1
if a = 0.
Notice that for a > 0, if ℓ:= len(a), then we have log2 a < ℓ≤log2 a + 1, or
equivalently, 2ℓ−1 ≤a < 2ℓ.
Assuming that arbitrarily large integers are represented as described at
the beginning of this section, with a sign bit and a vector of base-B digits,
where B is a constant power of 2, we may state the following theorem.
Theorem 3.3. Let a and b be arbitrary integers.
(i) We can compute a ± b in time O(len(a) + len(b)).
(ii) We can compute a · b in time O(len(a) len(b)).
(iii) If b ̸= 0, we can compute the quotient q := ⌊a/b⌋and the remainder
r := a mod b in time O(len(b) len(q)).
Note the bound O(len(b) len(q)) in part (iii) of this theorem, which may be
signiﬁcantly less than the bound O(len(a) len(b)). A good way to remember
this bound is as follows: the time to compute the quotient and remainder is
roughly the same as the time to compute the product bq appearing in the
equality a = bq + r.
This theorem does not explicitly refer to the base B in the underlying
TEAM LinG

3.3 Basic integer arithmetic
47
implementation. The choice of B aﬀects the values of the implied big-O
constants; while in theory, this is of no signiﬁcance, it does have a signiﬁcant
impact in practice.
From now on, we shall (for the most part) not worry about the imple-
mentation details of long-integer arithmetic, and will just refer directly this
theorem. However, we will occasionally exploit some trivial aspects of our
data structure for representing large integers. For example, it is clear that
in constant time, we can determine the sign of a given integer a, the bit
length of a, and any particular bit of the binary representation of a; more-
over, as discussed in Exercises 3.17 and 3.18, multiplications and divisions
by powers of 2 can be computed in linear time via “left shifts” and “right
shifts.” It is also clear that we can convert between the base-2 representa-
tion of a given integer and our implementation’s internal representation in
linear time (other conversions may take longer—see Exercise 3.25).
A note on notation: “len” and “log.” In expressing the run-
ning times of algorithms, we generally prefer to write, for exam-
ple, O(len(a) len(b)), rather than O((log a)(log b)). There are two
reasons for this. The ﬁrst is esthetic: the function “len” stresses
the fact that running times should be expressed in terms of the bit
length of the inputs. The second is technical: big-O estimates in-
volving expressions containing several independent parameters, like
O(len(a) len(b)), should be valid for all possible values of the param-
eters, since the notion of “suﬃciently large” does not make sense in
this setting; because of this, it is very inconvenient to have functions,
like log, that vanish or are undeﬁned on some inputs.
Exercise 3.21. Let n1, . . . , nk be positive integers. Show that
k

i=1
len(ni) −k ≤len
 k

i=1
ni

≤
k

i=1
len(ni).
Exercise 3.22. Show that the product n of integers n1, . . . , nk, with each
ni > 1, can be computed in time O(len(n)2). Do not assume that k is a
constant.
Exercise 3.23. Show that given integers n1, . . . , nk, with each ni > 1, and
an integer z, where 0 ≤z < n and n := 
i ni, we can compute the k integers
z mod ni, for i = 1, . . . , k, in time O(len(n)2).
Exercise 3.24. Consider the problem of computing ⌊n1/2⌋for a given non-
negative integer n.
(a) Using binary search, give an algorithm for this problem that runs in
TEAM LinG

48
Computing with large integers
time O(len(n)3). Your algorithm should discover the bits of ⌊n1/2⌋
one at a time, from high- to low-order bit.
(b) Reﬁne your algorithm from part (a), so that it runs in time
O(len(n)2).
Exercise 3.25. Show how to convert (in both directions) between the base-
10 representation and our implementation’s internal representation of an
integer n in time O(len(n)2).
3.4 Computing in Zn
Let n > 1. For α ∈Zn, there exists a unique integer a ∈{0, . . . , n −1} such
that α = [a]n; we call this integer a the canonical representative of α,
and denote it by rep(α). For computational purposes, we represent elements
of Zn by their canonical representatives.
Addition and subtraction in Zn can be performed in time O(len(n)):
given α, β ∈Zn, to compute rep(α + β), we simply compute the integer
sum rep(α) + rep(β), subtracting n if the result is greater than or equal
to n; similarly, to compute rep(α −β), we compute the integer diﬀerence
rep(α) −rep(β), adding n if the result is negative. Multiplication in Zn can
be performed in time O(len(n)2): given α, β ∈Zn, we compute rep(α · β) as
rep(α) rep(β) mod n, using one integer multiplication and one division with
remainder.
A note on notation: “rep,” “mod,” and “[·]n.”
In describ-
ing algorithms, as well as in other contexts, if α, β are elements of
Zn, we may write, for example, γ ←α + β or γ ←αβ, and it is
understood that elements of Zn are represented by their canonical
representatives as discussed above, and arithmetic on canonical rep-
resentatives is done modulo n. Thus, we have in mind a “strongly
typed” language for our pseudo-code that makes a clear distinction
between integers in the set {0, . . . , n −1} and elements of Zn. If
a ∈Z, we can convert a to an object α ∈Zn by writing α ←[a]n,
and if a ∈{0, . . . , n −1}, this type conversion is purely conceptual,
involving no actual computation. Conversely, if α ∈Zn, we can con-
vert α to an object a ∈{0, . . . , n−1}, by writing a ←rep(α); again,
this type conversion is purely conceptual, and involves no actual
computation. It is perhaps also worthwhile to stress the distinction
between a mod n and [a]n —the former denotes an element of the
set {0, . . . , n −1}, while the latter denotes an element of Zn.
Another interesting problem is exponentiation in Zn: given α ∈Zn and
a non-negative integer e, compute αe ∈Zn.
Perhaps the most obvious
way to do this is to iteratively multiply by α a total of e times, requiring
TEAM LinG

3.4 Computing in Zn
49
time O(e len(n)2). A much faster algorithm, the repeated-squaring algo-
rithm, computes αe using just O(len(e)) multiplications in Zn, thus taking
time O(len(e) len(n)2).
This method works as follows. Let e = (bℓ−1 · · · b0)2 be the binary expan-
sion of e (where b0 is the low-order bit). For i = 0, . . . , ℓ, deﬁne ei := ⌊e/2i⌋;
the binary expansion of ei is ei = (bℓ−1 · · · bi)2. Also deﬁne βi := αei for
i = 0, . . . , ℓ, so βℓ= 1 and β0 = αe. Then we have
ei = 2ei+1 + bi and βi = β2
i+1 · αbi for i = 0, . . . , ℓ−1.
This idea yields the following algorithm:
β ←[1]n
for i ←ℓ−1 down to 0 do
β ←β2
if bi = 1 then β ←β · α
output β
It is clear that when this algorithm terminates, we have β = αe, and that
the running-time estimate is as claimed above. Indeed, the algorithm uses
ℓsquarings in Zn, and at most ℓadditional multiplications in Zn.
The following exercises develop some important eﬃciency improvements
to the basic repeated-squaring algorithm.
Exercise 3.26. The goal of this exercise is to develop a “2t-ary” variant of
the above repeated-squaring algorithm, in which the exponent is eﬀectively
treated as a number in base 2t, rather than in base 2.
(a) Show how to modify the repeated squaring so as to compute αe using
ℓ+O(1) squarings in Zn, and an additional 2t+ℓ/t+O(1) multiplica-
tions in Zn. As above, α ∈Zn and len(e) = ℓ, while t is a parameter
that we are free to choose. Your algorithm should begin by building
a table of powers [1], α, . . . , α2t−1, and after that, it should process
the bits of e from left to right in blocks of length t (i.e., as base-2t
digits).
(b) Show that by appropriately choosing the parameter t, we can bound
the number of additional multiplications in Zn by O(ℓ/ len(ℓ)). Thus,
from an asymptotic point of view, the cost of exponentiation is es-
sentially the cost of ℓsquarings in Zn.
(c) Improve your algorithm from part (a), so that it only uses ℓ+ O(1)
squarings in Zn, and an additional 2t−1 + ℓ/t + O(1) multiplications
TEAM LinG

50
Computing with large integers
in Zn. Hint: build a table that contains only the odd powers of α
among [1], α, . . . , α2t−1.
Exercise 3.27. Suppose we are given α1, . . . , αk ∈Zn, along with non-
negative integers e1, . . . , ek, where len(ei) ≤ℓfor i = 1, . . . , k. Show how to
compute
β := αe1
1 · · · αek
k
using ℓ+ O(1) squarings in Zn and an additional ℓ+ 2k + O(1) multiplica-
tions in Zn. Your algorithm should work in two phases: in the ﬁrst phase,
the algorithm uses just the values α1, . . . , αk to build a table of all possible
products of subsets of α1, . . . , αk; in the second phase, the algorithm com-
putes β, using the exponents e1, . . . , ek, and the table computed in the ﬁrst
phase.
Exercise 3.28. Suppose that we are to compute αe, where α ∈Zn, for
many ℓ-bit exponents e, but with α ﬁxed. Show that for any positive integer
parameter k, we can make a pre-computation (depending on α, ℓ, and k)
that uses ℓ+ O(1) squarings in Zn and 2k + O(1) multiplications in Zn, so
that after the pre-computation, we can compute αe for any ℓ-bit exponent e
using just ℓ/k + O(1) squarings and ℓ/k + O(1) multiplications in Zn. Hint:
use the algorithm in the previous exercise.
Exercise 3.29. Let k be a constant, positive integer. Suppose we are given
α1, . . . , αk ∈Zn, along with non-negative integers e1, . . . , ek, where len(ei) ≤
ℓfor i = 1, . . . , k. Show how to compute
β := αe1
1 · · · αek
k
using ℓ+O(1) squarings in Zn and an additional O(ℓ/ len(ℓ)) multiplications
in Zn. Hint: develop a 2t-ary version of the algorithm in Exercise 3.27.
Exercise 3.30. Let m1, . . . , mr be integers, each greater than 1, and let
m := m1 · · · mr. Also, for i = 1, . . . , r, deﬁne m′
i := m/mi. Given α ∈Zn,
show how to compute all of the quantities
αm′
1, . . . , αm′
r
using a total of O(len(r) len(m)) multiplications in Zn. Hint: divide and
conquer.
Exercise 3.31. The repeated-squaring algorithm we have presented here
processes the bits of the exponent from left to right (i.e., from high order
to low order). Develop an algorithm for exponentiation in Zn with similar
complexity that processes the bits of the exponent from right to left.
TEAM LinG

3.5 Faster integer arithmetic (∗)
51
3.5 Faster integer arithmetic (∗)
The quadratic-time algorithms presented in §3.3 for integer multiplication
and division are by no means the fastest possible. The next exercise develops
a faster multiplication algorithm.
Exercise 3.32. Suppose we have two positive, ℓ-bit integers a and b such
that a = a12k + a0 and b = b12k + b0, where 0 ≤a0 < 2k and 0 ≤b0 < 2k.
Then
ab = a1b122k + (a0b1 + a1b0)2k + a0b0.
Show how to compute the product ab in time O(ℓ), given the products a0b0,
a1b1, and (a0 −a1)(b0 −b1). From this, design a recursive algorithm that
computes ab in time O(ℓlog2 3). (Note that log2 3 ≈1.58.)
The algorithm in the previous is also not the best possible. In fact, it is
possible to multiply ℓ-bit integers on a RAM in time O(ℓ), but we do not
explore this any further here (see §3.6).
The following exercises explore the relationship between integer multipli-
cation and related problems. We assume that we have an algorithm that
multiplies two integers of at most ℓbits in time M(ℓ). It is convenient (and
reasonable) to assume that M is a well-behaved complexity function.
By this, we mean that M maps positive integers to positive real numbers,
and
• for all positive integers a and b, we have M(a + b) ≥M(a) + M(b),
and
• for all real c > 1 there exists real d > 1, such that for all positive
integers a and b, if a ≤cb, then M(a) ≤dM(b).
Exercise 3.33. Let α > 0, β ≥1, γ ≥0, δ ≥0 be real constants. Show
that
M(ℓ) := αℓβ len(ℓ)γ len(len(ℓ))δ
is a well-behaved complexity function.
Exercise 3.34. Give an algorithm for Exercise 3.22 that runs in time
O(M(len(n)) len(k)).
Hint: divide and conquer.
Exercise 3.35. We can represent a “ﬂoating point” number ˆz as a pair
(a, e), where a and e are integers — the value of ˆz is the rational number
TEAM LinG

52
Computing with large integers
a2e, and we call len(a) the precision of ˆz. We say that ˆz is a k-bit ap-
proximation of a real number z if ˆz has precision k and ˆz = (1 + ϵ)z for
some |ϵ| ≤2−k+1. Show how to compute — given positive integers b and
k — a k-bit approximation of 1/b in time O(M(k)). Hint: using Newton
iteration, show how to go from a t-bit approximation of 1/b to a (2t −2)-
bit approximation of 1/b, making use of just the high-order O(t) bits of b,
in time O(M(t)).
Newton iteration is a general method of iteratively
approximating a root of an equation f(x) = 0 by starting with an initial ap-
proximation x0, and computing subsequent approximations by the formula
xi+1 = xi −f(xi)/f′(xi), where f′(x) is the derivative of f(x). For this
exercise, apply Newton iteration to the function f(x) = x−1 −b.
Exercise 3.36. Using the result of the previous exercise, given positive
integers a and b of bit length at most ℓ, show how to compute ⌊a/b⌋and
a mod b in time O(M(ℓ)). From this, we see that up to a constant factor,
division with remainder is no harder that multiplication.
Exercise 3.37. Using the result of the previous exercise, give an algorithm
for Exercise 3.23 that runs in time O(M(len(n)) len(k)). Hint: divide and
conquer.
Exercise 3.38. Give an algorithm for Exercise 3.24 that runs in time
O(M(len(n))). Hint: Newton iteration.
Exercise 3.39.
Give algorithms for Exercise 3.25 that run in time
O(M(ℓ) len(ℓ)), where ℓ:= len(n). Hint: divide and conquer.
Exercise 3.40. Suppose we have an algorithm that computes the square of
an ℓ-bit integer in time S(ℓ), where S is a well-behaved complexity function.
Show how to use this algorithm to compute the product of two arbitrary
integers of at most ℓbits in time O(S(ℓ)).
3.6 Notes
Shamir [84] shows how to factor an integer in polynomial time on a RAM,
but where the numbers stored in the memory cells may have exponentially
many bits. As there is no known polynomial-time factoring algorithm on
any realistic machine, Shamir’s algorithm demonstrates the importance of
restricting the sizes of numbers stored in the memory cells of our RAMs to
keep our formal model realistic.
The most practical implementations of algorithms for arithmetic on large
TEAM LinG

3.6 Notes
53
integers are written in low-level “assembly language,” speciﬁc to a partic-
ular machine’s architecture (e.g., the GNU Multi-Precision library GMP,
available at www.swox.com/gmp). Besides the general fact that such hand-
crafted code is more eﬃcient than that produced by a compiler, there is
another, more important reason for using such code. A typical 32-bit ma-
chine often comes with instructions that allow one to compute the 64-bit
product of two 32-bit integers, and similarly, instructions to divide a 64-bit
integer by a 32-bit integer (obtaining both the quotient and remainder).
However, high-level programming languages do not (as a rule) provide any
access to these low-level instructions. Indeed, we suggested in §3.3 using a
value for the base B of about half the word-size of the machine, so as to
avoid overﬂow. However, if one codes in assembly language, one can take B
to be much closer to, or even equal to, the word-size of the machine. Since
our basic algorithms for multiplication and division run in time quadratic
in the number of base-B digits, the eﬀect of doubling the bit-length of B is
to decrease the running time of these algorithms by a factor of four. This
eﬀect, combined with the improvements one might typically expect from us-
ing assembly-language code, can easily lead to a ﬁve- to ten-fold decrease in
the running time, compared to an implementation in a high-level language.
This is, of course, a signiﬁcant improvement for those interested in serious
“number crunching.”
The “classical,” quadratic-time algorithms presented here for integer mul-
tiplication and division are by no means the best possible: there are algo-
rithms that are asymptotically faster.
We saw this in the algorithm in
Exercise 3.32, which was originally invented by Karatsuba [52] (although
Karatsuba is one of two authors on this paper, the paper gives exclusive
credit for this particular result to Karatsuba). That algorithm allows us to
multiply two ℓ-bit integers in time O(ℓlog2 3). The fastest known algorithm
for multiplying two ℓ-bit integers on a RAM runs in time O(ℓ). This algo-
rithm is due to Sch¨onhage, and actually works on a very restricted type of
RAM called a “pointer machine” (see Problem 12, Section 4.3.3 of Knuth
[54]). See Exercise 18.27 later in this text for a much simpler (but heuristic)
O(ℓ) multiplication algorithm.
Another model of computation is that of Boolean circuits. In this model
of computation, one considers families of Boolean circuits (with, say, the
usual “and,” “or,” and “not” gates) that compute a particular function—
for every input length, there is a diﬀerent circuit in the family that computes
the function on inputs of that length. One natural notion of complexity for
such circuit families is the size of the circuit (i.e., the number of gates and
TEAM LinG

54
Computing with large integers
wires in the circuit), which is measured as a function of the input length.
The smallest known Boolean circuit that multiplies two ℓ-bit numbers has
size O(ℓlen(ℓ) len(len(ℓ))). This result is due to Sch¨onhage and Strassen [82].
It is hard to say which model of computation, the RAM or circuits, is
“better.” On the one hand, the RAM very naturally models computers as
we know them today: one stores small numbers, like array indices, coun-
ters, and pointers, in individual words of the machine, and processing such
a number typically takes a single “machine cycle.” On the other hand, the
RAM model, as we formally deﬁned it, invites a certain kind of “cheating,”
as it allows one to stuﬀO(len(ℓ))-bit integers into memory cells. For exam-
ple, even with the simple, quadratic-time algorithms for integer arithmetic
discussed in §3.3, we can choose the base B to have len(ℓ) bits, in which
case these algorithms would run in time O((ℓ/ len(ℓ))2). However, just to
keep things simple, we have chosen to view B as a constant (from a formal,
asymptotic point of view).
In the remainder of this text, unless otherwise speciﬁed, we shall always
use the classical O(ℓ2) bounds for integer multiplication and division, which
have the advantage of being both simple and reasonably reliable predictors of
actual performance for small to moderately sized inputs. For relatively large
numbers, experience shows that the classical algorithms are deﬁnitely not
the best—Karatsuba’s multiplication algorithm, and related algorithms for
division, start to perform signiﬁcantly better than the classical algorithms
on inputs of a thousand bits or so (the exact crossover depends on myriad
implementation details). The even “faster” algorithms discussed above are
typically not interesting unless the numbers involved are truly huge, of bit
length around 105–106. Thus, the reader should bear in mind that for serious
computations involving very large numbers, the faster algorithms are very
important, even though this text does not discuss them at great length.
For a good survey of asymptotically fast algorithms for integer arithmetic,
see Chapter 9 of Crandall and Pomerance [30], as well as Chapter 4 of Knuth
[54].
TEAM LinG

4
Euclid’s algorithm
In this chapter, we discuss Euclid’s algorithm for computing greatest com-
mon divisors. It turns out that Euclid’s algorithm has a number of very nice
properties, and has applications far beyond that purpose.
4.1 The basic Euclidean algorithm
We consider the following problem: given two non-negative integers a and
b, compute their greatest common divisor, gcd(a, b). We can do this using
the well-known Euclidean algorithm, also called Euclid’s algorithm.
The basic idea of Euclid’s algorithm is the following.
Without loss of
generality, we may assume that a ≥b ≥0. If b = 0, then there is nothing to
do, since in this case, gcd(a, 0) = a. Otherwise, if b > 0, we can compute the
integer quotient q := ⌊a/b⌋and remainder r := a mod b, where 0 ≤r < b.
From the equation
a = bq + r,
it is easy to see that if an integer d divides both b and r, then it also divides
a; likewise, if an integer d divides a and b, then it also divides r. From
this observation, it follows that gcd(a, b) = gcd(b, r), and so by performing
a division, we reduce the problem of computing gcd(a, b) to the “smaller”
problem of computing gcd(b, r).
The following theorem develops this idea further:
Theorem 4.1. Let a, b be integers, with a ≥b ≥0. Using the division with
remainder property, deﬁne the integers r0, r1, . . . , rℓ+1, and q1, . . . , qℓ, where
ℓ≥0, as follows:
55
TEAM LinG

56
Euclid’s algorithm
a = r0,
b = r1,
r0 = r1q1 + r2
(0 < r2 < r1),
...
ri−1 = riqi + ri+1
(0 < ri+1 < ri),
...
rℓ−2 = rℓ−1qℓ−1 + rℓ
(0 < rℓ< rℓ−1),
rℓ−1 = rℓqℓ
(rℓ+1 = 0).
Note that by deﬁnition, ℓ= 0 if b = 0, and ℓ> 0, otherwise.
Then we have rℓ= gcd(a, b). Moreover, if b > 0, then ℓ≤log b/ log φ + 1,
where φ := (1 +
√
5)/2 ≈1.62.
Proof. For the ﬁrst statement, one sees that for i = 1, . . . , ℓ, we have ri−1 =
riqi +ri+1, from which it follows that the common divisors of ri−1 and ri are
the same as the common divisors of ri and ri+1, and hence gcd(ri−1, ri) =
gcd(ri, ri+1). From this, it follows that
gcd(a, b) = gcd(r0, r1) = gcd(rℓ, rℓ+1) = gcd(rℓ, 0) = rℓ.
To prove the second statement, assume that b > 0, and hence ℓ> 0. If
ℓ= 1, the statement is obviously true, so assume ℓ> 1. We claim that for
i = 0, . . . , ℓ−1, we have rℓ−i ≥φi. The statement will then follow by setting
i = ℓ−1 and taking logarithms.
We now prove the above claim. For i = 0 and i = 1, we have
rℓ≥1 = φ0 and rℓ−1 ≥rℓ+ 1 ≥2 ≥φ1.
For i = 2, . . . , ℓ−1, using induction and applying the fact the φ2 = φ + 1,
we have
rℓ−i ≥rℓ−(i−1) + rℓ−(i−2) ≥φi−1 + φi−2 = φi−2(1 + φ) = φi,
which proves the claim. 2
Example 4.1. Suppose a = 100 and b = 35. Then the numbers appearing
in Theorem 4.1 are easily computed as follows:
i
0
1
2
3
4
ri
100
35
30
5
0
qi
2
1
6
TEAM LinG

4.1 The basic Euclidean algorithm
57
So we have gcd(a, b) = r3 = 5. 2
We can easily turn the scheme described in Theorem 4.1 into a simple
algorithm, taking as input integers a, b, such that a ≥b ≥0, and producing
as output d = gcd(a, b):
r ←a, r′ ←b
while r′ ̸= 0 do
r′′ ←r mod r′
(r, r′) ←(r′, r′′)
d ←r
output d
We now consider the running time of Euclid’s algorithm. Naively, one
could estimate this as follows. Suppose a and b are k-bit numbers. The
algorithm performs O(k) divisions on numbers with at most k-bits. As each
such division takes time O(k2), this leads to a bound on the running time
of O(k3). However, as the following theorem shows, this cubic running time
bound is well oﬀthe mark.
Theorem 4.2. Euclid’s algorithm runs in time O(len(a) len(b)).
Proof. We may assume that b > 0. The running time is O(τ), where τ :=
ℓ
i=1 len(ri) len(qi). Since ri ≤b for i = 1, . . . , ℓ, we have
τ ≤len(b)
ℓ

i=1
len(qi) ≤len(b)
ℓ

i=1
(log2 qi + 1) = len(b)(ℓ+ log2(
ℓ
i=1
qi)).
Note that
a = r0 ≥r1q1 ≥r2q2q1 ≥· · · ≥rℓqℓ· · · q1 ≥qℓ· · · q1.
We also have ℓ≤log b/ log φ + 1. Combining this with the above, we have
τ ≤len(b)(log b/ log φ + 1 + log2 a) = O(len(a) len(b)),
which proves the theorem. 2
Exercise 4.1. This exercise looks at an alternative algorithm for comput-
ing gcd(a, b), called the binary gcd algorithm.
This algorithm avoids
complex operations, such as division and multiplication; instead, it relies
only on division and multiplication by powers of 2, which assuming a binary
representation of integers (as we are) can be very eﬃciently implemented
using “right shift” and “left shift” operations. The algorithm takes positive
integers a and b as input, and runs as follows:
TEAM LinG

58
Euclid’s algorithm
r ←a, r′ ←b, e ←0
while 2 | r and 2 | r′ do r ←r/2, r′ ←r′/2, e ←e + 1
repeat
while 2 | r do r ←r/2
while 2 | r′ do r′ ←r′/2
if r′ < r then (r, r′) ←(r′, r)
r′ ←r′ −r
until r′ = 0
d ←2e · r
output d
Show that this algorithm correctly computes gcd(a, b), and runs in time
O(ℓ2), where ℓ:= max(len(a), len(b)).
4.2 The extended Euclidean algorithm
Let a and b be non-negative integers, and let d := gcd(a, b). We know by
Theorem 1.6 that there exist integers s and t such that as + bt = d. The
extended Euclidean algorithm allows us to eﬃciently compute s and t.
The following theorem describes the algorithm, and also states a number
of important facts about the relative sizes of the numbers that arise during
the computation—these size estimates will play a crucial role, both in the
analysis of the running time of the algorithm, as well as in applications of
the algorithm that we will discuss later.
Theorem 4.3. Let a, b, r0, r1, . . . , rℓ+1 and q1, . . . , qℓbe as in Theorem 4.1.
Deﬁne integers s0, s1, . . . , sℓ+1 and t0, t1, . . . , tℓ+1 as follows:
s0 := 1,
t0 := 0,
s1 := 0,
t1 := 1,
and for i = 1, . . . , ℓ,
si+1 := si−1 −siqi,
ti+1 := ti−1 −tiqi.
Then
(i) for i = 0, . . . , ℓ+ 1, we have sia + tib = ri; in particular, sℓa + tℓb =
gcd(a, b);
(ii) for i = 0, . . . , ℓ, we have siti+1 −tisi+1 = (−1)i;
(iii) for i = 0, . . . , ℓ+ 1, we have gcd(si, ti) = 1;
(iv) for i = 0, . . . , ℓ, we have titi+1 ≤0 and |ti| ≤|ti+1|; for i = 1, . . . , ℓ,
we have sisi+1 ≤0 and |si| ≤|si+1|;
(v) for i = 1, . . . , ℓ+ 1, we have ri−1|ti| ≤a and ri−1|si| ≤b.
TEAM LinG

4.2 The extended Euclidean algorithm
59
Proof. (i) is easily proved by induction on i. For i = 0, 1, the statement is
clear. For i = 2, . . . , ℓ+ 1, we have
sia + tib = (si−2 −si−1qi−1)a + (ti−2 −ti−1qi−1)b
= (si−2a + ti−2b) −(si−1a + ti−1b)qi
= ri−2 −ri−1qi−1
(by induction)
= ri.
(ii) is also easily proved by induction on i. For i = 0, the statement is
clear. For i = 1, . . . , ℓ, we have
siti+1 −tisi+1 = si(ti−1 −tiqi) −ti(si−1 −siqi)
= −(si−1ti −ti−1si)
(after expanding and simplifying)
= −(−1)i−1 = (−1)i
(by induction).
(iii) follows directly from (ii).
For (iv), one can easily prove both statements by induction on i. The
statement involving the ti is clearly true for i = 0; for i = 1, . . . , ℓ, we have
ti+1 = ti−1 −tiqi, and since by the induction hypothesis ti−1 and ti have
opposite signs and |ti| ≥|ti−1|, it follows that |ti+1| = |ti−1| + |ti|qi ≥|ti|,
and that the sign of ti+1 is the opposite of that of ti. The proof of the
statement involving the si is the same, except that we start the induction
at i = 1.
For (v), one considers the two equations:
si−1a + ti−1b = ri−1,
sia + tib = ri.
Subtracting ti−1 times the second equation from ti times the ﬁrst, applying
(ii), and using the fact that ti and ti−1 have opposite sign, we obtain
a = |tiri−1 −ti−1ri| ≥|ti|ri−1,
from which the inequality involving ti follows. The inequality involving si
follows similarly, subtracting si−1 times the second equation from si times
the ﬁrst. 2
Suppose that a > 0 in the above theorem. Then for i = 1, . . . , ℓ+ 1, the
value ri−1 is a positive integer, and so part (v) of the theorem implies that
|ti| ≤a/ri−1 ≤a and |si| ≤b/ri−1 ≤b. Moreover, if a > 1 and b > 0, then
ℓ> 0 and rℓ−1 ≥2, and hence |tℓ| ≤a/2 and |sℓ| ≤b/2.
Example 4.2. We continue with Example 4.1. The numbers si and ti are
easily computed from the qi:
TEAM LinG

60
Euclid’s algorithm
i
0
1
2
3
4
ri
100
35
30
5
0
qi
2
1
6
si
1
0
1
-1
7
ti
0
1
-2
3
-20
So we have gcd(a, b) = 5 = −a + 3b. 2
We can easily turn the scheme described in Theorem 4.3 into a simple
algorithm, taking as input integers a, b, such that a ≥b ≥0, and producing
as output integers d, s, and t, such that d = gcd(a, b) and as + bt = d:
r ←a, r′ ←b
s ←1, s′ ←0
t ←0, t′ ←1
while r′ ̸= 0 do
q ←⌊r/r′⌋, r′′ ←r mod r′
(r, s, t, r′, s′, t′) ←(r′, s′, t′, r′′, s −s′q, t −t′q)
d ←r
output d, s, t
Theorem 4.4. The extended Euclidean algorithm runs in time
O(len(a) len(b)).
Proof. We may assume that b > 0. It suﬃces to analyze the cost of comput-
ing the sequences {si} and {ti}. Consider ﬁrst the cost of computing all of
the ti, which is O(τ), where τ := ℓ
i=1 len(ti) len(qi). We have t1 = 1 and,
by part (v) of Theorem 4.3, we have |ti| ≤a for i = 2, . . . , ℓ. Arguing as in
the proof of Theorem 4.2, we have
τ ≤len(q1) + len(a)
ℓ

i=2
len(qi) ≤len(q1) + len(a)(ℓ−1 + log2(
ℓ
i=2
qi))
= O(len(a) len(b)),
where we have used the fact that ℓ
i=2 qi ≤b. An analogous argument shows
that one can also compute all of the si in time O(len(a) len(b)), and in fact,
in time O(len(b)2). 2
Another, instructive way to view Theorem 4.3 is as follows.
For i =
1, . . . , ℓ, we have
 ri
ri+1

=
0
1
1
−qi
 ri−1
ri

.
TEAM LinG

4.2 The extended Euclidean algorithm
61
Recursively expanding the right-hand side of this equation, we have for
i = 0, . . . , ℓ,
 ri
ri+1

= Mi
a
b

,
where for i = 1, . . . , ℓ, the matrix Mi is deﬁned as
Mi :=
0
1
1
−qi

· · ·
0
1
1
−q1

.
If we deﬁne M0 to be the 2 × 2 identity matrix, then it is easy to see that
Mi =
 si
ti
si+1
ti+1

,
for i = 0, . . . , ℓ. From this observation, part (i) of Theorem 4.3 is immediate,
and part (ii) follows from the fact that Mi is the product of i matrices, each
of determinant −1, and the determinant of Mi is evidently siti+1 −tisi+1.
Exercise 4.2. One can extend the binary gcd algorithm discussed in Ex-
ercise 4.1 so that in addition to computing d = gcd(a, b), it also computes s
and t such that as + bt = d. Here is one way to do this (again, we assume
that a and b are positive integers):
r ←a, r′ ←b, e ←0
while 2 | r and 2 | r′ do r ←r/2, r′ ←r′/2, e ←e + 1
˜a ←r, ˜b ←r′, s ←1, t ←0, s′ ←0, t′ ←1
repeat
while 2 | r do
r ←r/2
if 2 | s and 2 | t
then s ←s/2, t ←t/2
else s ←(s + ˜b)/2, t ←(t −˜a)/2
while 2 | r′ do
r′ ←r′/2
if 2 | s′ and 2 | t′ then s′ ←s′/2, t′ ←t′/2
else s′ ←(s′ + ˜b)/2, t′ ←(t′ −˜a)/2
if r′ < r then (r, s, t, r′, s′, t′) ←(r′, s′, t′, r, s, t)
r′ ←r′ −r, s′ ←s′ −s, t′ ←t′ −t
until r′ = 0
d ←2e · r, output d, s, t
Show that this algorithm is correct and runs in time O(ℓ2), where ℓ:=
max(len(a), len(b)). In particular, you should verify that all of the divisions
TEAM LinG

62
Euclid’s algorithm
by 2 performed by the algorithm yield integer results. Moreover, show that
the outputs s and t are of length O(ℓ).
4.3 Computing modular inverses and Chinese remaindering
One application of the extended Euclidean algorithm is to the problem of
computing multiplicative inverses in Zn, where n > 1.
Given y ∈{0, . . . , n −1}, in time O(len(n)2), we can determine if y is
relatively prime to n, and if so, compute y−1 mod n, as follows. We run
the extended Euclidean algorithm on inputs a := n and b := y, obtaining
integers d, s, and t, such that d = gcd(n, y) and ns + yt = d. If d ̸= 1, then
y does not have a multiplicative inverse modulo n. Otherwise, if d = 1, then
t is a multiplicative inverse of y modulo n; however, it may not lie in the
range {0, . . . , n−1}, as required. Based on Theorem 4.3 (and the discussion
immediately following it), we know that |t| ≤n/2 < n; therefore, either
t ∈{0, . . . , n −1}, or t < 0 and t + n ∈{0, . . . , n −1}. Thus, y−1 mod n is
equal to either t or t + n.
We also observe that the Chinese remainder theorem (Theorem 2.8) can
be made computationally eﬀective:
Theorem 4.5. Given integers n1, . . . , nk and a1, . . . , ak, where n1, . . . , nk
are pairwise relatively prime, and where ni > 1 and 0 ≤ai < ni for i =
1, . . . , k, we can compute the integer z, such that 0 ≤z < n and z ≡
ai (mod ni) for i = 1, . . . , k, where n := 
i ni, in time O(len(n)2).
Proof. Exercise (just use the formulas in the proof of Theorem 2.8, and see
Exercises 3.22 and 3.23). 2
Exercise 4.3. In this exercise and the next, you are to analyze an “incre-
mental Chinese remaindering algorithm.” Consider the following algorithm,
which takes as input integers z, n, z′, n′, such that
n′ > 1,
gcd(n, n′) = 1,
0 ≤z < n,
and 0 ≤z′ < n′.
It outputs integers z′′, n′′, such that
n′′ = nn′,
0 ≤z′′ < n′′,
z′′ ≡z (mod n),
and z′′ ≡z′ (mod n′).
It runs as follows:
1. Set ˜n ←n−1 mod n′.
2. Set h ←((z′ −z)˜n) mod n′.
TEAM LinG

4.4 Speeding up algorithms via modular computation
63
3. Set z′′ ←z + nh.
4. Set n′′ ←nn′.
5. Output z′′, n′′.
Show that the output z′′, n′′ of the algorithm satisﬁes the conditions stated
above, and estimate the running time of the algorithm.
Exercise 4.4. Using the algorithm in the previous exercise as a subroutine,
give a simple O(len(n)2) algorithm that takes as input integers n1, . . . , nk
and a1, . . . , ak, where n1, . . . , nk are pairwise relatively prime, and where
ni > 1 and 0 ≤ai < ni for i = 1, . . . , k, and outputs integers z and n such
that 0 ≤z < n, n = 
i ni, and z ≡ai (mod ni) for i = 1, . . . , k. The
algorithm should be “incremental,” in that it processes the pairs (ni, ai) one
at a time, using time O(len(n) len(ni)) to process each such pair.
Exercise 4.5. Suppose you are given α1, . . . , αk ∈Z∗
n. Show how to com-
pute α−1
1 , . . . , α−1
k
by computing one multiplicative inverse modulo n, and
performing less than 3k multiplications modulo n. This result is useful, as
in practice, if n is several hundred bits long, it may take 10–20 times longer
to compute multiplicative inverses modulo n than to multiply modulo n.
4.4 Speeding up algorithms via modular computation
An important practical application of the above “computational” version
(Theorem 4.5) of the Chinese remainder theorem is a general algorithmic
technique that can signiﬁcantly speed up certain types of computations in-
volving long integers. Instead of trying to describe the technique in some
general form, we simply illustrate the technique by means of a speciﬁc ex-
ample: integer matrix multiplication.
Suppose we have two m × m matrices A and B whose entries are large
integers, and we want to compute the product matrix C := AB.
If the
entries of A are (ars) and the entries of B are (bst), then the entries (crt)
of C are given by the usual rule for matrix multiplication:
crt =
m

s=1
arsbst.
Suppose further that H is the maximum absolute value of the entries
in A and B, so that the entries in C are bounded in absolute value by
H′ := H2m. Then by just applying the above formula, we can compute
the entries of C using m3 multiplications of numbers of length at most
len(H), and m3 additions of numbers of length at most len(H′), where
TEAM LinG

64
Euclid’s algorithm
len(H′) ≤2 len(H) + len(m). This yields a running time of
O(m3 len(H)2 + m3 len(m)).
(4.1)
If the entries of A and B are large relative to m,
speciﬁcally,
if
len(m)
=
O(len(H)2), then the running time is dominated by the
ﬁrst term above, namely
O(m3 len(H)2).
Using the Chinese remainder theorem, we can actually do much better
than this, as follows.
For any integer n > 1, and for all r, t = 1, . . . , m, we have
crt ≡
m

s=1
arsbst (mod n).
(4.2)
Moreover, if we compute integers c′
rt such that
c′
rt ≡
m

s=1
arsbst (mod n)
(4.3)
and if we also have
−n/2 ≤c′
rt < n/2
and
n > 2H′,
(4.4)
then we must have
crt = c′
rt.
(4.5)
To see why (4.5) follows from (4.3) and (4.4), observe that (4.2) and (4.3)
imply that crt ≡c′
rt (mod n), which means that n divides (crt −c′
rt). Then
from the bound |crt| ≤H′ and from (4.4), we obtain
|crt −c′
rt| ≤|crt| + |c′
rt| ≤H′ + n/2 < n/2 + n/2 = n.
So we see that the quantity (crt −c′
rt) is a multiple of n, while at the
same time this quantity is strictly less than n in absolute value; hence, this
quantity must be zero. That proves (4.5).
So from the above discussion, to compute C, it suﬃces to compute the
entries of C modulo n, where we have to make sure that we compute “bal-
anced” remainders in the interval [−n/2, n/2), rather than the more usual
“least non-negative” remainders.
To compute C modulo n, we choose a number of small integers n1, . . . , nk,
relatively prime in pairs, and such that the product n := n1 · · · nk is just a
bit larger than 2H′. In practice, one would choose the ni to be small primes,
and a table of such primes could easily be computed in advance, so that all
TEAM LinG

4.4 Speeding up algorithms via modular computation
65
problems up to a given size could be handled. For example, the product of
all primes of at most 16 bits is a number that has more than 90, 000 bits.
Thus, by simply pre-computing and storing such a table of small primes,
we can handle input matrices with quite large entries (up to about 45, 000
bits).
Let us assume that we have pre-computed appropriate small primes
n1, . . . , nk. Further, we shall assume that addition and multiplication mod-
ulo any of the ni can be done in constant time. This is reasonable, both from
a practical and theoretical point of view, since such primes easily “ﬁt” into
a memory cell. Finally, we assume that we do not use more of the numbers
ni than are necessary, so that len(n) = O(len(H′)) and k = O(len(H′)).
To compute C, we execute the following steps:
1. For each i = 1, . . . , k, do the following:
(a) compute ˆa(i)
rs ←ars mod ni for r, s = 1, . . . , m,
(b) compute ˆb(i)
st ←bst mod ni for s, t = 1, . . . , m,
(c) For r, t = 1, . . . , m, compute
ˆc(i)
rt ←
m

s=1
ˆa(i)
rsˆb(i)
st mod ni.
2. For each r, t = 1, . . . , m, apply the Chinese remainder theorem to
ˆc(1)
rt , ˆc(2)
rt , . . . , ˆc(k)
rt , obtaining an integer crt, which should be computed
as a balanced remainder modulo n, so that n/2 ≤crt < n/2.
3. Output (crt : r, t = 1, . . . , m).
Note that in Step 2, if our Chinese remainder algorithm happens to be
implemented to return an integer z with 0 ≤z < n, we can easily get a
balanced remainder by just subtracting n from z if z ≥n/2.
The correctness of the above algorithm has already been established. Let
us now analyze its running time. The running time of Steps 1a and 1b is
easily seen (see Exercise 3.23) to be O(m2 len(H′)2). Under our assumption
about the cost of arithmetic modulo small primes, the cost of Step 1c is
O(m3k), and since k = O(len(H′)) = O(len(H) + len(m)), the cost of this
step is O(m3(len(H)+len(m))). Finally, by Theorem 4.5, the cost of Step 2
is O(m2 len(H′)2). Thus, the total running time of this algorithm is easily
calculated (discarding terms that are dominated by others) as
O(m2 len(H)2 + m3 len(H) + m3 len(m)).
Compared to (4.1), we have essentially replaced the term m3 len(H)2 by
m2 len(H)2 + m3 len(H). This is a signiﬁcant improvement: for example,
TEAM LinG

66
Euclid’s algorithm
if len(H) ≈m, then the running time of the original algorithm is O(m5),
while the running time of the modular algorithm is O(m4).
Exercise 4.6. Apply the ideas above to the problem of computing the
product of two polynomials whose coeﬃcients are large integers. First, de-
termine the running time of the “obvious” algorithm for multiplying two
such polynomials, then design and analyze a “modular” algorithm.
4.5 Rational reconstruction and applications
We next state a theorem whose immediate utility may not be entirely ob-
vious, but we quickly follow up with several very neat applications. The
general problem we consider here, called rational reconstruction, is as
follows. Suppose that there is some rational number ˆy that we would like to
get our hands on, but the only information we have about ˆy is the following:
• First, suppose that we know that ˆy may be expressed as r/t for
integers r, t, with |r| ≤r∗and |t| ≤t∗—we do not know r or t, but
we do know the bounds r∗and t∗.
• Second, suppose that we know integers y and n such that n is rela-
tively prime to t, and y = rt−1 mod n.
It turns out that if n is suﬃciently large relative to the bounds r∗and t∗,
then we can virtually “pluck” ˆy out of the extended Euclidean algorithm
applied to n and y. Moreover, the restriction that n is relatively prime to
t is not really necessary; if we drop this restriction, then our assumption is
that r ≡ty (mod n), or equivalently, r = sn + ty for some integer s.
Theorem 4.6. Let r∗, t∗, n, y be integers such that r∗> 0, t∗> 0, n ≥4r∗t∗,
and 0 ≤y < n.
Suppose we run the extended Euclidean algorithm with
inputs a := n and b := y. Then, adopting the notation of Theorem 4.3, the
following hold:
(i) There exists a unique index i = 1, . . . , ℓ+1 such that ri ≤2r∗< ri−1;
note that ti ̸= 0 for this i.
Let r′ := ri, s′ := si, and t′ := ti.
(ii) Furthermore, for any integers r, s, t such that
r = sn + ty,
|r| ≤r∗,
and 0 < |t| ≤t∗,
(4.6)
we have
r = r′α, s = s′α,
and t = t′α,
for some non-zero integer α.
TEAM LinG

4.5 Rational reconstruction and applications
67
Proof. By hypothesis, 2r∗< n = r0. Moreover, since r0, . . . , rℓ, rℓ+1 = 0
is a decreasing sequence, and 1 = |t1|, |t2|, . . . , |tℓ+1| is a non-decreasing
sequence, the ﬁrst statement of the theorem is clear.
Now let i be deﬁned as in the ﬁrst statement of the theorem. Also, let
r, s, t be as in (4.6).
From part (v) of Theorem 4.3 and the inequality 2r∗< ri−1, we have
|ti| ≤
n
ri−1
< n
2r∗.
From the equalities ri = sin + tiy and r = sn + ty, we have the two congru-
ences:
r ≡ty (mod n),
ri ≡tiy (mod n).
Subtracting ti times the ﬁrst from t times the second, we obtain
rti ≡rit (mod n).
This says that n divides rti −rit. Using the bounds |r| ≤r∗and |ti| <
n/(2r∗), we see that |rti| < n/2, and using the bounds |ri| ≤2r∗, |t| ≤t∗,
and 4r∗t∗≤n, we see that |rit| ≤n/2. It follows that
|rti −rit| ≤|rti| + |rit| < n/2 + n/2 = n.
Since n divides rti −rit and |rti −rit| < n, the only possibility is that
rti −rit = 0.
(4.7)
Now consider the two equations:
r = sn + ty
ri = sin + tiy.
Subtracting ti times the ﬁrst from t times the second, and using the identity
(4.7), we obtain n(sti −sit) = 0, and hence
sti −sit = 0.
(4.8)
From (4.8), we see that ti | sit, and since from part (iii) of Theorem 4.3,
we know that gcd(si, ti) = 1, we must have ti | t. So t = tiα for some α, and
we must have α ̸= 0 since t ̸= 0. Substituting tiα for t in equations (4.7)
and (4.8) yields r = riα and s = siα. That proves the second statement of
the theorem. 2
TEAM LinG

68
Euclid’s algorithm
4.5.1 Application: Chinese remaindering with errors
One interpretation of the Chinese remainder theorem is that if we “encode”
an integer z, with 0 ≤z < n, as the sequence (a1, . . . , ak), where ai = z mod
ni for i = 1, . . . , k, then we can eﬃciently recover z from this encoding. Here,
of course, n = n1 · · · nk, and the integers n1, . . . , nk are pairwise relatively
prime.
But now suppose that Alice encodes z as (a1, . . . , ak), and sends this
encoding to Bob; however, during the transmission of the encoding, some
(but hopefully not too many) of the values a1, . . . , ak may be corrupted. The
question is, can Bob still eﬃciently recover the original z from its corrupted
encoding?
To make the problem more precise, suppose that the original, correct
encoding of z is (a1, . . . , ak), and the corrupted encoding is (˜a1, . . . , ˜ak). Let
us deﬁne G ⊆{1, . . . , k} to be the set of “good” positions i with ˜ai = ai,
and B ⊆{1, . . . , k} to be the set of “bad” positions i with ˜ai ̸= ai. We shall
assume that |B| ≤ℓ, where ℓis some speciﬁed parameter.
Of course, if Bob hopes to recover z, we need to build some redundancy
into the system; that is, we must require that 0 ≤z ≤Z for some Z that is
somewhat smaller than n. Now, if Bob knew the location of bad positions,
and if the product of the integers ni at the good positions exceeds Z, then
Bob could simply discard the errors, and reconstruct z by applying the
Chinese remainder theorem to the values ai and ni at the good positions.
However, in general, Bob will not know a priori the location of the bad
positions, and so this approach will not work.
Despite these apparent diﬃculties, Theorem 4.6 may be used to solve the
problem quite easily, as follows. Let P be an upper bound on the product
of any ℓof the integers n1, . . . , nk (e.g., we could take P to be the product
of the ℓlargest ni). Further, let us assume that n ≥4P 2Z.
Now, suppose Bob obtains the corrupted encoding (˜a1, . . . , ˜ak). Here is
what Bob does to recover z:
1. Apply the Chinese remainder theorem, obtaining an integer y, with
0 ≤y < n and y ≡˜ai (mod ni) for i = 1, . . . , k.
2. Run the extended Euclidean algorithm on a := n and b := y, and let
r′, t′ be the values obtained from Theorem 4.6 applied with r∗:= ZP
and t∗:= P.
3. If t′ | r′, output r′/t′; otherwise, output “error.”
We claim that the above procedure outputs z, under our assumption that
the set B of bad positions is of size at most ℓ. To see this, let t := 
i∈B ni.
By construction, we have 1 ≤t ≤P.
Also, let r := tz, and note that
TEAM LinG

4.5 Rational reconstruction and applications
69
0 ≤r ≤r∗and 0 < t ≤t∗. We claim that
r ≡ty (mod n).
(4.9)
To show that (4.9) holds, it suﬃces to show that
tz ≡ty (mod ni)
(4.10)
for all i = 1, . . . , k. To show this, for each index i we consider two cases:
Case 1: i ∈G. In this case, we have ai = ˜ai, and therefore,
tz ≡tai ≡t˜ai ≡ty (mod ni).
Case 2: i ∈B. In this case, we have ni | t, and therefore,
tz ≡0 ≡ty (mod ni).
Thus, (4.10) holds for all i = 1, . . . , k, and so it follows that (4.9) holds.
Therefore, the values r′, t′ obtained from Theorem 4.6 satisfy
r′
t′ = r
t = tz
t = z.
One easily checks that both the procedures to encode and decode a value
z run in time O(len(n)2). If one wanted a practical implementation, one
might choose n1, . . . , nk to be, say, 16-bit primes, so that the encoding of a
value z consisted of a sequence of k 16-bit words.
The above scheme is an example of an error correcting code, and is
actually the integer analog of a Reed–Solomon code.
4.5.2 Application: recovering fractions from their decimal
expansions
Suppose Alice knows a rational number z := s/t, where s and t are integers
with 0 ≤s < t, and tells Bob some of the high-order digits in the decimal
expansion of z. Can Bob determine z? The answer is yes, provided Bob
knows an upper bound T on t, and provided Alice gives Bob enough digits.
Of course, from grade school, Bob probably remembers that the decimal
expansion of z is ultimately periodic, and that given enough digits of z so
as to include the periodic part, he can recover z; however, this technique is
quite useless in practice, as the length of the period can be huge—Θ(T) in
the worst case (see Exercises 4.8–4.10 below). The method we discuss here
requires only O(len(T)) digits.
To be a bit more general, suppose that Alice gives Bob the high-order k
TEAM LinG

70
Euclid’s algorithm
digits in the d-ary expansion of z, for some base d > 1. Now, we can express
z in base d as
z = z1d−1 + z2d−2 + z3d−3 + · · · ,
and the sequence of digits z1, z2, z3, . . . is uniquely determined if we require
that the sequence does not terminate with an inﬁnite run of (d −1)-digits.
Suppose Alice gives Bob the ﬁrst k digits z1, . . . , zk. Deﬁne
y := z1dk−1 + · · · + zk−1d + zk = ⌊zdk⌋.
Let us also deﬁne n := dk, so that y = ⌊zn⌋.
Now, if n is much smaller than T 2, the number z is not even uniquely
determined by y, since there are Ω(T 2) distinct rational numbers of the
form s/t, with 0 ≤s < t ≤T (see Exercise 1.18). However, if n ≥4T 2,
then not only is z uniquely determined by y, but using Theorem 4.6, we can
compute it as follows:
1. Run the extended Euclidean algorithm on inputs a := n and b := y,
and let s′, t′ be as in Theorem 4.6, using r∗:= t∗:= T.
2. Output s′, t′.
We claim that z = −s′/t′. To prove this, observe that since y = ⌊zn⌋=
⌊(ns)/t⌋, if we set r := (ns) mod t, then we have
r = sn −ty and 0 ≤r < t ≤t∗.
It follows that the integers s′, t′ from Theorem 4.6 satisfy s = s′α and
−t = t′α for some non-zero integer α. Thus, s′/t′ = −s/t, which proves the
claim.
We may further observe that since the extended Euclidean algorithm guar-
antees that gcd(s′, t′) = 1, not only do we obtain z, but we obtain z expressed
as a fraction in lowest terms.
It is clear that the running time of this algorithm is O(len(n)2).
Example 4.3. Alice chooses numbers 0 ≤s < t ≤1000, and tells Bob the
high-order seven digits y in the decimal expansion of z := s/t, from which
Bob should be able to compute z. Suppose s = 511 and t = 710. Then
s/t ≈0.71971830985915492958, and so y = 7197183 and n = 107. Running
the extended Euclidean algorithm on inputs a := n and b := y, Bob obtains
the following data:
TEAM LinG

4.5 Rational reconstruction and applications
71
i
ri
qi
si
ti
0
10000000
1
0
1
7197183
1
0
1
2
2802817
2
1
-1
3
1591549
1
-2
3
4
1211268
1
3
-4
5
380281
3
-5
7
6
70425
5
18
-25
7
28156
2
-95
132
8
14113
1
208
-289
9
14043
1
-303
421
10
70
200
511
-710
11
43
1
-102503
142421
12
27
1
103014
-143131
13
16
1
-205517
285552
14
11
1
308531
-428683
15
5
2
-514048
714235
16
1
5
1336627
-1857153
17
0
-7197183
10000000
The ﬁrst ri that meets or falls below the threshold 2r∗= 2000 is at
i = 10, and Bob reads oﬀs′ = 511 and t′ = −710, from which he obtains
z = −s′/t′ = 511/710. 2
Exercise 4.7. Show that given integers s, t, k, with 0 ≤s < t, and k >
0, we can compute the kth digit in the decimal expansion of s/t in time
O(len(k) len(t)2).
For the following exercises, we need a deﬁnition:
a sequence S :=
(z1, z2, z3, . . .) of elements drawn from some arbitrary set is called (k, ℓ)-
periodic for integers k ≥0 and ℓ≥1 if zi = zi+ℓfor all i > k. S is called
ultimately periodic if it is (k, ℓ)-periodic for some (k, ℓ).
Exercise 4.8. Show that if a sequence S is ultimately periodic, then it
is (k∗, ℓ∗)-periodic for some uniquely determined pair (k∗, ℓ∗) for which the
following holds: for any pair (k, ℓ) such that S is (k, ℓ)-periodic, we have
k∗≤k and ℓ∗≤ℓ.
The value ℓ∗in the above exercise is called the period of S, and k∗is
called the pre-period of S. If its pre-period is zero, then S is called purely
periodic.
TEAM LinG

72
Euclid’s algorithm
Exercise 4.9. Let z be a real number whose base-d expansion is an ulti-
mately periodic sequence. Show that z is rational.
Exercise 4.10. Let z = s/t ∈Q, where s and t are relatively prime integers
with 0 ≤s < t, and let d > 1 be an integer.
(a) Show that there exist integers k, k′ such that 0 ≤k < k′ and sdk ≡
sdk′ (mod t).
(b) Show that for integers k, k′ with 0 ≤k < k′, the base-d expansion of
z is (k, k′ −k)-periodic if and only if sdk ≡sdk′ (mod t).
(c) Show that if gcd(t, d) = 1, then the base-d expansion of z is purely
periodic with period equal to the multiplicative order of d modulo t.
(d) More generally, show that if k is the smallest non-negative integer
such that d and t′ := t/ gcd(dk, t) are relatively prime, then the base-
d expansion of z is ultimately periodic with pre-period k and period
equal to the multiplicative order of d modulo t′.
A famous conjecture of Artin postulates that for any integer d, not equal
to −1 or to the square of an integer, there are inﬁnitely many primes t such
that d has multiplicative order t −1 modulo t. If Artin’s conjecture is true,
then by part (c) of the previous exercise, for any d > 1 that is not a square,
there are inﬁnitely many primes t such that the base-d expansion of s/t, for
any 0 < s < t, is a purely periodic sequence of period t −1. In light of these
observations, the “grade school” method of computing a fraction from its
decimal expansion using the period is hopelessly impractical.
4.5.3 Applications to symbolic algebra
Rational reconstruction also has a number of applications in symbolic alge-
bra. We brieﬂy sketch one such application here. Suppose that we want to
ﬁnd the solution v to the equation
vA = w,
where we are given as input a non-singular square integer matrix A and an
integer vector w. The solution vector v will, in general, have rational en-
tries. We stress that we want to compute the exact solution v, and not some
ﬂoating point approximation to it. Now, we could solve for v directly us-
ing Gaussian elimination; however, the intermediate quantities computed by
that algorithm would be rational numbers whose numerators and denomina-
tors might get quite large, leading to a rather lengthy computation (however,
TEAM LinG

4.6 Notes
73
it is possible to show that the overall running time is still polynomial in the
input length).
Another approach is to compute a solution vector modulo n, where n is
a power of a prime that does not divide the determinant of A. Provided n
is large enough, one can then recover the solution vector v using rational
reconstruction. With this approach, all of the computations can be carried
out using arithmetic on integers not too much larger than n, leading to a
more eﬃcient algorithm. More of the details of this procedure are developed
later, in Exercise 15.13.
4.6 Notes
The Euclidean algorithm as we have presented it here is not the fastest
known algorithm for computing greatest common divisors. The asymptot-
ically fastest known algorithm for computing the greatest common divisor
of two numbers of bit length at most ℓruns in time O(ℓlen(ℓ)) on a RAM,
and the smallest Boolean circuits are of size O(ℓlen(ℓ)2 len(len(ℓ))). This
algorithm is due to Sch¨onhage [81]. The same complexity results also hold
for the extended Euclidean algorithm, as well as Chinese remaindering and
rational reconstruction.
Experience suggests that such fast algorithms for greatest common divi-
sors are not of much practical value, unless the integers involved are very
large—at least several tens of thousands of bits in length. The extra “log”
factor and the rather large multiplicative constants seem to slow things down
too much.
The binary gcd algorithm (Exercise 4.1) is due to Stein [95]. The ex-
tended binary gcd algorithm (Exercise 4.2) was ﬁrst described by Knuth
[54], who attributes it to M. Penk. Our formulation of both of these al-
gorithms closely follows that of Menezes, van Oorschot, and Vanstone [62].
Experience suggests that the binary gcd algorithm is faster in practice than
Euclid’s algorithm.
Our exposition of Theorem 4.6 is loosely based on Bach [11]. A somewhat
“tighter” result is proved, with signiﬁcantly more eﬀort, by Wang, Guy, and
Davenport [97].
However, for most practical purposes, the result proved
here is just as good. The application of Euclid’s algorithm to computing a
rational number from the ﬁrst digits of its decimal expansion was observed
by Blum, Blum, and Shub [17], where they considered the possibility of
using such sequences of digits as a pseudo-random number generator—the
conclusion, of course, is that this is not such a good idea.
TEAM LinG

5
The distribution of primes
This chapter concerns itself with the question: how many primes are there?
In Chapter 1, we proved that there are inﬁnitely many primes; however, we
are interested in a more quantitative answer to this question; that is, we
want to know how “dense” the prime numbers are.
This chapter has a bit more of an “analytical” ﬂavor than other chapters
in this text. However, we shall not make use of any mathematics beyond
that of elementary calculus.
5.1 Chebyshev’s theorem on the density of primes
The natural way of measuring the density of primes is to count the number
of primes up to a bound x, where x is a real number. For a real number
x ≥0, the function π(x) is deﬁned to be the number of primes up to x.
Thus, π(1) = 0, π(2) = 1, π(7.5) = 4, and so on. The function π is an
example of a “step function,” that is, a function that changes values only at
a discrete set of points. It might seem more natural to deﬁne π only on the
integers, but it is the tradition to deﬁne it over the real numbers (and there
are some technical beneﬁts in doing so).
Let us ﬁrst take a look at some values of π(x). Table 5.1 shows values of
π(x) for x = 103i and i = 1, . . . , 6. The third column of this table shows
the value of x/π(x) (to ﬁve decimal places). One can see that the diﬀer-
ences between successive rows of this third column are roughly the same—
about 6.9—which suggests that the function x/π(x) grows logarithmically
in x. Indeed, as log(103) ≈6.9, it would not be unreasonable to guess that
x/π(x) ≈log x, or equivalently, π(x) ≈x/ log x.
The following theorem is a ﬁrst—and important—step towards making
the above guesswork more rigorous:
74
TEAM LinG

5.1 Chebyshev’s theorem on the density of primes
75
Table 5.1. Some values of π(x)
x
π(x)
x/π(x)
103
168
5.95238
106
78498
12.73918
109
50847534
19.66664
1012
37607912018
26.59015
1015
29844570422669
33.50693
1018
24739954287740860
40.42045
Theorem 5.1 (Chebyshev’s theorem). We have
π(x) = Θ(x/ log x).
It is not too diﬃcult to prove this theorem, which we now proceed to do
in several steps. Recalling that νp(n) denotes the power to which a prime p
divides an integer n, we begin with the following observation:
Theorem 5.2. Let n be a positive integer. For any prime p, we have
νp(n!) =

k≥1
⌊n/pk⌋.
Proof. This follows immediately from the observation that the numbers
1, 2, . . . , n include exactly ⌊n/p⌋multiplies of p, ⌊n/p2⌋multiplies of p2,
and so on (see Exercise 1.5). 2
The following theorem gives a lower bound on π(x).
Theorem 5.3. π(n) ≥1
2(log 2)n/ log n for all integers n ≥2.
Proof. For positive integer m, consider the binomial coeﬃcient
N :=
2m
m

= (2m)!
(m!)2 .
Note that
N =
m + 1
1
m + 2
2

· · ·
m + m
m

,
from which it is clear that N ≥2m and that N is divisible only by primes p
not exceeding 2m. Applying Theorem 5.2 to the identity N = (2m)!/(m!)2,
we have
νp(N) =

k≥1
(⌊2m/pk⌋−2⌊m/pk⌋).
TEAM LinG

76
The distribution of primes
Each term in this sum is either 0 or 1 (see Exercise 1.4), and for k >
log(2m)/ log p, each term is zero. Thus, νp(N) ≤log(2m)/ log p.
So we have
π(2m) log(2m) =

p≤2m
log(2m)
log p
log p
≥

p≤2m
νp(N) log p = log N ≥m log 2,
where the summations are over the primes p up to 2m. Therefore,
π(2m) ≥1
2(log 2)(2m)/ log(2m).
That proves the theorem for even n. Now consider odd n ≥3, so n =
2m−1 for m ≥2. Since the function x/ log x is increasing for x ≥3 (verify),
and since π(2m −1) = π(2m) for m ≥2, we have
π(2m −1) = π(2m)
≥1
2(log 2)(2m)/ log(2m)
≥1
2(log 2)(2m −1)/ log(2m −1).
That proves the theorem for odd n. 2
As a consequence of the above theorem, we have π(x) = Ω(x/ log x) for
real x →∞. Indeed, for real x ≥2, setting c := 1
2(log 2), we have
π(x) = π(⌊x⌋) ≥c⌊x⌋/ log⌊x⌋≥c(x −1)/ log x = Ω(x/ log x).
To obtain a corresponding upper bound for π(x), we introduce an auxiliary
function, called Chebyshev’s theta function:
ϑ(x) :=

p≤x
log p,
where the sum is over all primes p up to x.
Chebyshev’s theta function is an example of a summation over primes,
and in this chapter, we will be considering a number of functions that are
deﬁned in terms of sums or products over primes. To avoid excessive tedium,
we adopt the usual convention used by number theorists: if not explicitly
stated, summations and products over the variable p are always understood
to be over primes. For example, we may write π(x) = 
p≤x 1.
The next theorem relates π(x) and ϑ(x). Recall the “∼” notation from
§3.1: for two functions f and g such that f(x) and g(x) are positive for all
suﬃciently large x, we write f ∼g to mean that limx→∞f(x)/g(x) = 1, or
TEAM LinG

5.1 Chebyshev’s theorem on the density of primes
77
equivalently, for all ϵ > 0 there exists x0 such that (1 −ϵ)g(x) < f(x) <
(1 + ϵ)g(x) for all x > x0.
Theorem 5.4. We have
π(x) ∼ϑ(x)
log x.
Proof. On the one hand, we have
ϑ(x) =

p≤x
log p ≤log x

p≤x
1 = π(x) log x.
So we have
π(x) ≥ϑ(x)
log x.
On the other hand, for every x > 1 and δ with 0 < δ < 1, we have
ϑ(x) ≥

xδ<p≤x
log p
≥δ log x

xδ<p≤x
1
= δ log x (π(x) −π(xδ))
≥δ log x (π(x) −xδ).
Hence,
π(x) ≤xδ + ϑ(x)
δ log x.
Since by the previous theorem, the term xδ is o(π(x)), we have for all suﬃ-
ciently large x (depending on δ), xδ ≤(1 −δ)π(x), and so
π(x) ≤
ϑ(x)
δ2 log x.
Now, for any ϵ > 0, we can choose δ suﬃciently close to 1 so that 1/δ2 <
1 + ϵ, and for this δ, and for all suﬃciently large x, we have π(x) < (1 +
ϵ)ϑ(x)/ log x, and the theorem follows. 2
Theorem 5.5. ϑ(x) < 2x log 2 for all real numbers x ≥1.
Proof. It suﬃces to prove that ϑ(n) < 2n log 2 for integers n ≥1, since then
ϑ(x) = ϑ(⌊x⌋) < 2⌊x⌋log 2 ≤2x log 2.
For positive integer m, consider the binomial coeﬃcient
M :=
2m + 1
m

= (2m + 1)!
m!(m + 1)!.
TEAM LinG

78
The distribution of primes
One sees that M is divisible by all primes p with m + 1 < p ≤2m + 1.
As M occurs twice in the binomial expansion of (1 + 1)2m+1, one sees that
M < 22m+1/2 = 22m. It follows that
ϑ(2m + 1) −ϑ(m + 1) =

m+1<p≤2m+1
log p ≤log M < 2m log 2.
We now prove the theorem by induction.
For n = 1 and n = 2, the
theorem is trivial. Now let n > 2. If n is even, then we have
ϑ(n) = ϑ(n −1) < 2(n −1) log 2 < 2n log 2.
If n = 2m + 1 is odd, then we have
ϑ(n) = ϑ(2m + 1) −ϑ(m + 1) + ϑ(m + 1)
< 2m log 2 + 2(m + 1) log 2 = 2n log 2. 2
Another way of stating the above theorem is:

p≤x
p < 4x.
Theorem 5.1 follows immediately from Theorems 5.3, 5.4 and 5.5. Note
that we have also proved:
Theorem 5.6. We have
ϑ(x) = Θ(x).
Exercise 5.1. If pn denotes the nth prime, show that pn = Θ(n log n).
Exercise 5.2. For integer n > 1, let ω(n) denote the number of distinct
primes dividing n. Show that ω(n) = O(log n/ log log n).
Exercise 5.3. Show that for positive integers a and b,
a + b
b

≥2min(a,b).
5.2 Bertrand’s postulate
Suppose we want to know how many primes there are of a given bit length,
or more generally, how many primes there are between m and 2m for a given
integer m. Neither the statement, nor the proof, of Chebyshev’s theorem
imply that there are any primes between m and 2m, let alone a useful density
estimate of such primes.
Bertrand’s postulate is the assertion that for all positive integers m,
TEAM LinG

5.2 Bertrand’s postulate
79
there exists a prime between m and 2m. We shall in fact prove a stronger
result, namely, that not only is there one prime, but the number of primes
between m and 2m is Ω(m/ log m).
Theorem 5.7 (Bertrand’s postulate). For any positive integer m, we
have
π(2m) −π(m) >
m
3 log(2m).
The proof uses Theorem 5.5, along with a more careful re-working of the
proof of Theorem 5.3. The theorem is clearly true for m ≤2, so we may
assume that m ≥3. As in the proof of the Theorem 5.3, deﬁne N :=
2m
m

,
and recall that N is divisible only by primes strictly less than 2m, and that
we have the identity
νp(N) =

k≥1
(⌊2m/pk⌋−2⌊m/pk⌋),
(5.1)
where each term in the sum is either 0 or 1. We can characterize the values
νp(N) a bit more precisely, as follows:
Lemma 5.8. Let m ≥3 and N =
2m
m

as above. For all primes p, we have
pνp(N) ≤2m;
(5.2)
if p >
√
2m, then νp(N) ≤1;
(5.3)
if 2m/3 < p ≤m, then νp(N) = 0;
(5.4)
if m < p < 2m, then νp(N) = 1.
(5.5)
Proof. For (5.2), all terms with k > log(2m)/ log p in (5.1) vanish, and hence
νp(N) ≤log(2m)/ log p, from which it follows that pνp(N) ≤2m.
(5.3) follows immediately from (5.2).
For (5.4), if 2m/3 < p ≤m, then 2m/p < 3, and we must also have
p ≥3, since p = 2 implies m < 3. We have p2 > p(2m/3) = 2m(p/3) ≥2m,
and hence all terms with k > 1 in (5.1) vanish. The term with k = 1 also
vanishes, since 1 ≤m/p < 3/2, from which it follows that 2 ≤2m/p < 3,
and hence ⌊m/p⌋= 1 and ⌊2m/p⌋= 2.
For (5.5), if m < p < 2m, it follows that 1 < 2m/p < 2, so ⌊2m/p⌋= 1.
Also, m/p < 1, so ⌊m/p⌋= 0. It follows that the term with k = 1 in (5.1)
is 1, and it is clear that 2m/pk < 1 for all k > 1, and so all the other terms
vanish. 2
We need one more technical fact, namely, a somewhat better lower bound
on N than that used in the proof of Theorem 5.3:
TEAM LinG

80
The distribution of primes
Lemma 5.9. Let m ≥3 and N =
2m
m

as above. We have
N > 4m/(2m).
(5.6)
Proof. We prove this for all m ≥3 by induction on m. One checks by direct
calculation that it holds for m = 3. For m > 3, by induction we have
2m
m

= 22m −1
m
2(m −1)
m −1

> (2m −1)4m−1
m(m −1)
= 2m −1
2(m −1)
4m
2m > 4m
2m. 2
We now have the necessary technical ingredients to prove Theorem 5.7.
Deﬁne
Pm :=

m<p<2m
p,
and deﬁne Qm so that
N = QmPm.
By (5.4) and (5.5), we see that
Qm =

p≤2m/3
pνp(N).
Moreover, by (5.3), νp(N) > 1 for at most those p ≤
√
2m, so there are at
most
√
2m such primes, and by (5.2), the contribution of each such prime
to the above product is at most 2m. Combining this with Theorem 5.5, we
obtain
Qm < (2m)
√
2m · 42m/3.
We now apply (5.6), obtaining
Pm = NQ−1
m > 4m(2m)−1Q−1
m > 4m/3(2m)−(1+
√
2m).
It follows that
π(2m) −π(m) ≥log Pm/ log(2m) >
m log 4
3 log(2m) −(1 +
√
2m)
=
m
3 log(2m) + m(log 4 −1)
3 log(2m)
−(1 +
√
2m).
(5.7)
Clearly, the term (m(log 4 −1))/(3 log(2m)) in (5.7) dominates the term
1 +
√
2m, and so Theorem 5.7 holds for all suﬃciently large m. Indeed, a
simple calculation shows that (5.7) implies the theorem for m ≥13, 000, and
one can verify by brute force (with the aid of a computer) that the theorem
holds for m < 13, 000.
TEAM LinG

5.3 Mertens’ theorem
81
5.3 Mertens’ theorem
Our next goal is to prove the following theorem, which turns out to have a
number of applications.
Theorem 5.10. We have

p≤x
1
p = log log x + O(1).
The proof of this theorem, while not diﬃcult, is a bit technical, and we
proceed in several steps.
Theorem 5.11. We have

p≤x
log p
p
= log x + O(1).
Proof. Let n := ⌊x⌋. By Theorem 5.2, we have
log(n!) =

p≤n

k≥1
⌊n/pk⌋log p =

p≤n
⌊n/p⌋log p +

k≥2

p≤n
⌊n/pk⌋log p.
We next show that the last sum is O(n). We have

p≤n
log p

k≥2
⌊n/pk⌋≤n

p≤n
log p

k≥2
p−k
= n

p≤n
log p
p2
·
1
1 −1/p = n

p≤n
log p
p(p −1)
≤n

k≥2
log k
k(k −1) = O(n).
Thus, we have shown that
log(n!) =

p≤n
⌊n/p⌋log p + O(n).
Further, since ⌊n/p⌋= n/p + O(1), applying Theorem 5.5, we have
log(n!) =

p≤n
(n/p) log p + O(

p≤n
log p) + O(n) = n

p≤n
log p
p
+ O(n). (5.8)
We can also estimate log(n!) using a little calculus (see §A2). We have
log(n!) =
n

k=1
log k =
 n
1
log t dt + O(log n) = n log n −n + O(log n). (5.9)
TEAM LinG

82
The distribution of primes
Combining (5.8) and (5.9), and noting that log x −log n = o(1), we obtain

p≤x
log p
p
= log n + O(1) = log x + O(1),
which proves the theorem. 2
We shall also need the following theorem, which is a very useful tool in
its own right:
Theorem 5.12 (Abel’s identity). Suppose that ck, ck+1, . . . is a sequence
of numbers, that
C(t) :=

k≤i≤t
ci,
and that f(t) has a continuous derivative f′(t) on the interval [k, x]. Then

k≤i≤x
cif(i) = C(x)f(x) −
 x
k
C(t)f′(t) dt.
Note that since C(t) is a step function, the integrand C(t)f′(t) is piece-
wise continuous on [k, x], and hence the integral is well deﬁned (see §A3).
Proof. Let n := ⌊x⌋. We have
n

i=k
cif(i) = C(k)f(k) + [C(k + 1) −C(k)]f(k + 1) + · · ·
+ [C(n) −C(n −1)]f(n)
= C(k)[f(k) −f(k + 1)] + · · · + C(n −1)[f(n −1) −f(n)]
+ C(n)f(n)
= C(k)[f(k) −f(k + 1)] + · · · + C(n −1)[f(n −1) −f(n)]
+ C(n)[f(n) −f(x)] + C(x)f(x).
Observe that for i = k, . . . , n −1, we have C(t) = C(i) for t ∈[i, i + 1), and
so
C(i)[f(i) −f(i + 1)] = −
 i+1
i
C(t)f′(t) dt;
likewise,
C(n)[f(n) −f(x)] = −
 x
n
C(t)f′(t) dt,
from which the theorem directly follows. 2
TEAM LinG

5.3 Mertens’ theorem
83
Proof of Theorem 5.10. For i ≥2, set
ci :=
 (log i)/i
if i is prime,
0
otherwise.
By Theorem 5.11, we have
C(t) :=

2≤i≤t
ci =

p≤t
log p
p
= log t + O(1).
Applying Theorem 5.12 with f(t) = 1/ log t, we obtain

p≤x
1
p = C(x)
log x +
 x
2
C(t)
t(log t)2 dt
=

1 + O(1/ log x)

+
  x
2
dt
t log t + O(
 x
2
dt
t(log t)2 )

= 1 + O(1/ log x) + (log log x −log log 2) + O(1/ log 2 −1/ log x)
= log log x + O(1). 2
Using Theorem 5.10, we can easily show the following:
Theorem 5.13 (Mertens’ theorem). We have

p≤x
(1 −1/p) = Θ(1/ log x).
Proof. Using parts (i) and (iii) of §A1, for any ﬁxed prime p, we have
−1
p2 ≤1
p + log(1 −1/p) ≤0.
(5.10)
Moreover, since

p≤x
1
p2 ≤

i≥2
1
i2 < ∞,
summing the inequality (5.10) over all primes p ≤x yields
−C ≤

p≤x
1
p + log U(x) ≤0,
where C is a positive constant, and U(x) := 
p≤x(1 −1/p). From this, and
from Theorem 5.10, we obtain
log log x + log U(x) = O(1).
This means that
−D ≤log log x + log U(x) ≤D
TEAM LinG

84
The distribution of primes
for some positive constant D and all suﬃciently large x, and exponentiating
this yields
e−D ≤(log x)U(x) ≤eD,
and hence, U(x) = Θ(1/ log x), and the theorem follows. 2
Exercise 5.4. Let ω(n) be the number of distinct prime factors of n, and
deﬁne ω(x) = 
n≤x ω(n), so that ω(x)/x represents the “average” value
of ω. First, show that ω(x) = 
p≤x⌊x/p⌋. From this, show that ω(x) ∼
x log log x.
Exercise 5.5. Analogously to the previous exercise, show that 
n≤x τ(n) ∼
x log x, where τ(n) is the number of positive divisors of n.
Exercise 5.6. Deﬁne the sequence of numbers n1, n2, . . ., where nk is
the product of all the primes up to k.
Show that as k →∞, φ(nk) =
Θ(nk/ log log nk). Hint: you will want to use Mertens’ theorem, and also
Theorem 5.6.
Exercise 5.7. The previous exercise showed that φ(n) could be as small
as (about) n/ log log n for inﬁnitely many n. Show that this is the “worst
case,” in the sense that φ(n) = Ω(n/ log log n) as n →∞.
Exercise 5.8. Show that for any positive integer constant k,
 x
2
dt
(log t)k =
x
(log x)k + O

x
(log x)k+1

.
Exercise 5.9. Use Chebyshev’s theorem and Abel’s identity to show that

p≤x
1
log p = π(x)
log x + O(x/(log x)3).
Exercise 5.10. Use Chebyshev’s theorem and Abel’s identity to prove a
stronger version of Theorem 5.4:
ϑ(x) = π(x) log x + O(x/ log x).
Exercise 5.11. Show that

2<p≤x
(1 −2/p) = Θ(1/(log x)2).
Exercise 5.12. Show that if π(x) ∼cx/ log x for some constant c, then we
must have c = 1. Hint: use either Theorem 5.10 or 5.11.
TEAM LinG

5.4 The sieve of Eratosthenes
85
Exercise 5.13. Strengthen Theorem 5.10, showing that 
p≤x 1/p ∼
log log x + A for some constant A. (Note: A ≈0.261497212847643.)
Exercise 5.14. Strengthen Mertens’ theorem, showing that 
p≤x(1 −
1/p) ∼B1/(log x) for some constant B1.
Hint: use the result from the
previous exercise. (Note: B1 ≈0.561459483566885.)
Exercise 5.15. Strengthen the result of Exercise 5.11, showing that

2<p≤x
(1 −2/p) ∼B2/(log x)2
for some constant B2. (Note: B2 ≈0.832429065662.)
5.4 The sieve of Eratosthenes
As an application of Theorem 5.10, consider the sieve of Eratosthenes.
This is an algorithm for generating all the primes up to a given bound k. It
uses an array A[2 . . . k], and runs as follows.
for n ←2 to k do A[n] ←1
for n ←2 to ⌊
√
k⌋do
if A[n] = 1 then
i ←2n; while i ≤k do { A[i] ←0; i ←i + n }
When the algorithm ﬁnishes, we have A[n] = 1 if and only if n is prime,
for n = 2, . . . , k. This can easily be proven using the fact (see Exercise 1.1)
that a composite number n between 2 and k must be divisible by a prime
that is at most
√
k, and by proving by induction on n that at the beginning
of the nth iteration of the main loop, A[i] = 0 iﬀi is divisible by a prime
less than n, for i = n, . . . , k. We leave the details of this to the reader.
We are more interested in the running time of the algorithm. To analyze
the running time, we assume that all arithmetic operations take constant
time; this is reasonable, since all the quantities computed in the algorithm
are bounded by k, and we need to at least be able to index all entries of the
array A, which has size k.
Every time we execute the inner loop of the algorithm, we perform O(k/n)
steps to clear the entries of A indexed by multiples of n. Naively, we could
bound the running time by a constant times

n≤
√
k
k/n,
TEAM LinG

86
The distribution of primes
which is O(k len(k)), where we have used a little calculus (see §A2) to derive
that
ℓ

n=1
1/n =
 ℓ
1
dy
y + O(1) ∼log ℓ.
However, the inner loop is executed only for prime values of n; thus, the
running time is proportional to

p≤
√
k
k/p,
and so by Theorem 5.10 is Θ(k len(len(k))).
Exercise 5.16. Give a detailed proof of the correctness of the above algo-
rithm.
Exercise 5.17. One drawback of the above algorithm is its use of space:
it requires an array of size k. Show how to modify the algorithm, without
substantially increasing its running time, so that one can enumerate all the
primes up to k, using an auxiliary array of size just O(
√
k).
Exercise 5.18. Design and analyze an algorithm that on input k outputs
the table of values τ(n) for n = 1, . . . , k, where τ(n) is the number of positive
divisors of n. Your algorithm should run in time O(k len(k)).
5.5 The prime number theorem . . . and beyond
In this section, we survey a number of theorems and conjectures related to
the distribution of primes. This is a vast area of mathematical research,
with a number of very deep results. We shall be stating a number of theo-
rems from the literature in this section without proof; while our intent is to
keep the text as self contained as possible, and to avoid degenerating into
“mathematical tourism,” it nevertheless is a good idea to occasionally have
a somewhat broader perspective. In the following chapters, we shall not
make any critical use of the theorems in this section.
5.5.1 The prime number theorem
The main theorem in the theory of the density of primes is the following.
Theorem 5.14 (Prime number theorem). We have
π(x) ∼x/ log x.
TEAM LinG

5.5 The prime number theorem . . . and beyond
87
Proof. Literature—see §5.6. 2
As we saw in Exercise 5.12, if π(x)/(x/ log x) tends to a limit as x →∞,
then the limit must be 1, so in fact the hard part of proving the prime
number theorem is to show that π(x)/(x/ log x) does indeed tend to some
limit.
One simple consequence of the prime number theorem, together with The-
orem 5.4, is the following:
Theorem 5.15. We have
ϑ(x) ∼x.
Exercise 5.19. Using the prime number theorem, show that pn ∼n log n,
where pn denotes the nth prime.
Exercise 5.20. Using the prime number theorem, show that Bertrand’s
postulate can be strengthened (asymptotically) as follows: for all ϵ > 0,
there exist positive constants c and x0, such that for all x ≥x0, we have
π((1 + ϵ)x) −π(x) ≥c
x
log x.
5.5.2 The error term in the prime number theorem
The prime number theorem says that
|π(x) −x/ log x| ≤δ(x),
where δ(x) = o(x/ log x). A natural question is: how small is the “error
term” δ(x)? It turns out that:
Theorem 5.16. We have
π(x) = x/ log x + O(x/(log x)2).
This bound on the error term is not very impressive. The reason is that
x/ log x is not really the best “simple” function that approximates π(x). It
turns out that a better approximation to π(x) is the logarithmic integral,
deﬁned for real x ≥2 by
li(x) :=
 x
2
dt
log t.
It is not hard to show (see Exercise 5.8) that
li(x) = x/ log x + O(x/(log x)2).
TEAM LinG

88
The distribution of primes
Table 5.2. Values of π(x), li(x), and x/ log x
x
π(x)
li(x)
x/ log x
103
168
176.6
144.8
106
78498
78626.5
72382.4
109
50847534
50849233.9
48254942.4
1012
37607912018
37607950279.8
36191206825.3
1015
29844570422669
29844571475286.5
28952965460216.8
1018
24739954287740860
24739954309690414.0
24127471216847323.8
Thus, li(x) ∼x/ log x ∼π(x). However, the error term in the approximation
of π(x) by li(x) is much better. This is illustrated numerically in Table 5.2;
for example, at x = 1018, li(x) approximates π(x) with a relative error just
under 10−9, while x/ log x approximates π(x) with a relative error of about
0.025.
The sharpest proven result is the following:
Theorem 5.17. Let κ(x) := (log x)3/5(log log x)−1/5. Then for some c > 0,
we have
π(x) = li(x) + O(xe−cκ(x)).
Proof. Literature—see §5.6. 2
Note that the error term xe−cκ(x) is o(x/(log x)k) for every ﬁxed k ≥0.
Also note that Theorem 5.16 follows directly from the above theorem and
Exercise 5.8.
Although the above estimate on the error term in the approximation of
π(x) by li(x) is pretty good, it is conjectured that the actual error term is
much smaller:
Conjecture 5.18. For all x ≥2.01, we have
|π(x) −li(x)| < x1/2 log x.
Conjecture 5.18 is equivalent to a famous conjecture called the Riemann
hypothesis, which is an assumption about the location of the zeros of a
certain function, called Riemann’s zeta function. We give a very brief,
high-level account of this conjecture, and its connection to the theory of the
distribution of primes.
For real s > 1, the zeta function is deﬁned as
ζ(s) :=
∞

n=1
1
ns .
(5.11)
TEAM LinG

5.5 The prime number theorem . . . and beyond
89
Note that because s > 1, the inﬁnite series deﬁning ζ(s) converges.
A
simple, but important, connection between the zeta function and the theory
of prime numbers is the following:
Theorem 5.19 (Euler’s identity). For real s > 1, we have
ζ(s) =

p
(1 −p−s)−1,
(5.12)
where the product is over all primes p.
Proof. The rigorous interpretation of the inﬁnite product on the right-hand
side of (5.12) is as a limit of ﬁnite products. Thus, if p1, p2, . . . is the list of
primes, we are really proving that
ζ(s) = lim
r→∞
r

i=1
(1 −p−s
i )−1.
Now, from the identity
(1 −p−s
i )−1 =
∞

e=0
p−es
i
,
we have
r

i=1
(1 −p−s
i )−1 =

1 + p−s
1
+ p−2s
1
+ · · ·

· · ·

1 + p−s
r
+ p−2s
r
+ · · ·

=
∞

e1=0
· · ·
∞

er=0
(pe1
1 · · · per
r )s
=
∞

n=1
gr(n)
ns ,
where
gr(n) :=
 1
if n is divisible only by the primes p1, . . . , pr;
0
otherwise.
Here, we have made use of the fact (see §A5) that we can multiply term-wise
inﬁnite series with non-negative terms.
Now, for any ϵ > 0, there exists n0 such that ∞
n=n0 n−s < ϵ (because
the series deﬁning ζ(s) converges). Moreover, there exists an r0 such that
gr(n) = 1 for all n < n0 and r ≥r0. Therefore, for r ≥r0, we have

∞

n=1
gr(n)
ns
−ζ(s)
 ≤
∞

n=n0
n−s < ϵ.
TEAM LinG

90
The distribution of primes
It follows that
lim
r→∞
∞

n=1
gr(n)
ns
= ζ(s),
which proves the theorem. 2
While Theorem 5.19 is nice, things become much more interesting if one
extends the domain of deﬁnition of the zeta function to the complex plane.
For the reader who is familiar with just a little complex analysis, it is easy
to see that the inﬁnite series deﬁning the zeta function in (5.11) converges
absolutely for complex numbers s whose real part is greater than 1, and that
(5.12) holds as well for such s. However, it is possible to extend the domain
of deﬁnition of ζ even further—in fact, one can extend the deﬁnition of ζ in
a “nice way ” (in the language of complex analysis, analytically continue)
to the entire complex plane (except the point s = 1, where there is a simple
pole). Exactly how this is done is beyond the scope of this text, but assuming
this extended deﬁnition of ζ, we can now state the Riemann hypothesis:
Conjecture 5.20 (Riemann hypothesis). For any complex number s =
x + yi, where x and y are real numbers with 0 < x < 1 and x ̸= 1/2, we
have ζ(s) ̸= 0.
A lot is known about the zeros of the zeta function in the “critical strip,”
consisting of those points s whose real part is greater than 0 and less than
1: it is known that there are inﬁnitely many of them, and there are even
good estimates about their density. It turns out that one can apply standard
tools in complex analysis, like contour integration, to the zeta function (and
functions derived from it) to answer various questions about the distribution
of primes. Indeed, such techniques may be used to prove the prime num-
ber theorem. However, if one assumes the Riemann hypothesis, then these
techniques yield much sharper results, such as the bound in Conjecture 5.18.
Exercise 5.21. For any arithmetic function a, we can form the Dirichlet
series
Fa(s) :=
∞

n=1
a(n)
ns .
For simplicity we assume that s takes only real values, even though such
series are usually studied for complex values of s.
(a) Show that if the Dirichlet series Fa(s) converges absolutely for some
real s, then it converges absolutely for all real s′ ≥s.
TEAM LinG

5.5 The prime number theorem . . . and beyond
91
(b) From part (a), conclude that for any given arithmetic function a,
there is an interval of absolute convergence of the form (s0, ∞),
where we allow s0 = −∞and s0 = ∞, such that Fa(s) converges
absolutely for s > s0, and does not converge absolutely for s < s0.
(c) Let a and b be arithmetic functions such that Fa(s) has an interval
of absolute convergence (s0, ∞) and Fb(s) has an interval of absolute
convergence (s′
0, ∞), and assume that s0 < ∞and s′
0 < ∞.
Let
c := a ⋆b be the Dirichlet product of a and b, as deﬁned in §2.6.
Show that for all s ∈(max(s0, s′
0), ∞), the series Fc(s) converges
absolutely and, moreover, that Fa(s)Fb(s) = Fc(s).
5.5.3 Explicit estimates
Sometimes, it is useful to have explicit estimates for π(x), as well as related
functions, like ϑ(x) and the nth prime function pn. The following theorem
presents a number of bounds that have been proved without relying on any
unproved conjectures.
Theorem 5.21. We have:
(i)
x
log x

1 +
1
2 log x

< π(x) <
x
log x

1 +
3
2 log x

,
for x ≥59;
(ii) n(log n + log log n −3/2) < pn < n(log n + log log n −1/2),
for n ≥20;
(iii) x(1 −1/(2 log x)) < ϑ(x) < x(1 + 1/(2 log x)),
for x ≥563;
(iv) log log x + A −
1
2(log x)2 <

p≤x
1/p < log log x + A +
1
2(log x)2 ,
for x ≥286, where A ≈0.261497212847643;
(v)
B1
log x

1 −
1
2(log x)2

<

p≤x

1 −1
p

<
B1
log x

1 +
1
2(log x)2

,
for x ≥285, where B1 ≈0.561459483566885.
Proof. Literature—see §5.6. 2
5.5.4 Primes in arithmetic progressions
The arithmetic progression of odd numbers 1, 3, 5, . . . contains inﬁnitely
many primes, and it is natural to ask if other arithmetic progressions do
as well. An arithmetic progression with ﬁrst term a and common diﬀerence
d consists of all integers of the form
md + a,
m = 0, 1, 2, . . . .
TEAM LinG

92
The distribution of primes
If d and a have a common factor c > 1, then every term in the progression is
divisible by c, and so there can be no more than one prime in the progression.
So a necessary condition for the existence of inﬁnitely many primes p with
p ≡a (mod d) is that gcd(d, a) = 1. A famous theorem due to Dirichlet
states that this is a suﬃcient condition as well.
Theorem 5.22 (Dirichlet’s theorem). For any positive integer d and
any integer a relatively prime to d, there are inﬁnitely many primes p with
p ≡a (mod d).
Proof. Literature—see §5.6. 2
We can also ask about the density of primes in arithmetic progressions.
One might expect that for a ﬁxed value of d, the primes are distributed
in roughly equal measure among the φ(d) diﬀerent residue classes [a]d with
gcd(a, d) = 1. This is in fact the case. To formulate such assertions, we
deﬁne π(x; d, a) to be the number of primes p up to x with p ≡a (mod d).
Theorem 5.23. Let d > 0 be a ﬁxed integer, and let a ∈Z be relatively
prime to d. Then
π(x; d, a) ∼
x
φ(d) log x.
Proof. Literature—see §5.6. 2
The above theorem is only applicable in the case where d is ﬁxed and
x →∞. But what if we want an estimate on the number of primes p up to
x with p ≡a (mod d), where x is, say, a ﬁxed power of d? Theorem 5.23
does not help us here. The following conjecture does, however:
Conjecture 5.24. For any real x ≥2, integer d ≥2, and a ∈Z relatively
prime to d, we have
π(x; d, a) −li(x)
φ(d)
 ≤x1/2(log x + 2 log d).
The above conjecture is in fact a consequence of a generalization of the
Riemann hypothesis—see §5.6.
Exercise 5.22. Assuming Conjecture 5.24, show that for all α, ϵ, with 0 <
α < 1/2 and 0 < ϵ < 1, there exists an x0, such that for all x > x0, for all
d ∈Z with 2 ≤d ≤xα, and for all a ∈Z relatively prime to d, the number
of primes p ≤x such that p ≡a (mod d) is at least (1 −ϵ) li(x)/φ(d) and at
most (1 + ϵ) li(x)/φ(d).
It is an open problem to prove an unconditional density result analogous
TEAM LinG

5.5 The prime number theorem . . . and beyond
93
to Exercise 5.22 for any positive exponent α. The following, however, is
known:
Theorem 5.25. There exists a constant c such that for all integer d ≥2
and a ∈Z relatively prime to d, the least prime p with p ≡a (mod d) is at
most cd11/2.
Proof. Literature—see §5.6. 2
5.5.5 Sophie Germain primes
A Sophie Germain prime is a prime p such that 2p + 1 is also prime.
Such primes are actually useful in a number of practical applications, and
so we discuss them brieﬂy here.
It is an open problem to prove (or disprove) that there are inﬁnitely
many Sophie Germain primes. However, numerical evidence, and heuristic
arguments, strongly suggest not only that there are inﬁnitely many such
primes, but also a fairly precise estimate on the density of such primes.
Let π∗(x) denote the number of Sophie Germain primes up to x.
Conjecture 5.26. We have
π∗(x) ∼C
x
(log x)2 ,
where C is the constant
C := 2

q>2
q(q −2)
(q −1)2 ≈1.32032,
and the product is over all primes q > 2.
The above conjecture is a special case of a more general conjecture, known
as Hypothesis H. We can formulate a special case of Hypothesis H (which
includes Conjecture 5.26), as follows:
Conjecture 5.27. Let (a1, b1), . . . , (ak, bk) be distinct pairs of integers such
that ai > 0, and for all primes p, there exists an integer m such that
k

i=1
(mai + bi) ̸≡0 (mod p).
Let P(x) be the number of integers m up to x such that mai + bi are simul-
taneously prime for i = 1, . . . , k. Then
P(x) ∼D
x
(log x)k ,
TEAM LinG

94
The distribution of primes
where
D :=

p

1 −1
p
−k
1 −ω(p)
p

,
the product being over all primes p, and ω(p) being the number of distinct
solutions m modulo p to the congruence
k

i=1
(mai + bi) ≡0 (mod p).
The above conjecture also includes (a strong version of) the famous twin
primes conjecture as a special case: the number of primes p up to x such
that p + 2 is also prime is ∼Cx/(log x)2, where C is the same constant as
in Conjecture 5.26.
Exercise 5.23. Show that the constant C appearing in Conjecture 5.26
satisﬁes
2C = B2/B2
1,
where B1 and B2 are the constants from Exercises 5.14 and 5.15.
Exercise 5.24. Show that the quantity D appearing in Conjecture 5.27 is
well deﬁned, and satisﬁes 0 < D < ∞.
5.6 Notes
The prime number theorem was conjectured by Gauss in 1791. It was proven
independently in 1896 by Hadamard and de la Vall´ee Poussin. A proof of
the prime number theorem may be found, for example, in the book by Hardy
and Wright [44].
Theorem 5.21, as well as the estimates for the constants A, B1, and B2
mentioned in that theorem and Exercises 5.13, 5.14, and 5.15, are from
Rosser and Schoenfeld [79].
Theorem 5.17 is from Walﬁsz [96].
Theorem 5.19, which made the ﬁrst connection between the theory of
prime numbers and the zeta function, was discovered in the 18th century
by Euler.
The Riemann hypothesis was made by Riemann in 1859, and
to this day, remains one of the most vexing conjectures in mathematics.
Riemann in fact showed that his conjecture about the zeros of the zeta
function is equivalent to the conjecture that for each ﬁxed ϵ > 0, π(x) =
li(x) + O(x1/2+ϵ). This was strengthened by von Koch in 1901, who showed
TEAM LinG

5.6 Notes
95
that the Riemann hypothesis is true if and only if π(x) = li(x)+O(x1/2 log x).
See Chapter 1 of the book by Crandall and Pomerance [30] for more on
the connection between the Riemann hypothesis and the theory of prime
numbers; in particular, see Exercise 1.36 in that book for an outline of a
proof that Conjecture 5.18 follows from the Riemann hypothesis.
A warning: some authors (and software packages) deﬁne the logarithmic
integral using the interval of integration (0, x), rather than (2, x), which
increases its value by a constant c ≈1.0452.
Theorem 5.22 was proved by Dirichlet in 1837, while Theorem 5.23 was
proved by de la Vall´ee Poussin in 1896. A result of Oesterl´e [69] implies
that Conjecture 5.24 for d ≥3 is a consequence of an assumption about the
location of the zeros of certain generalizations of Riemann’s zeta function;
the case d = 2 follows from the bound in Conjecture 5.18 under the ordinary
Riemann hypothesis. Theorem 5.25 is from Heath-Brown [45].
Hypothesis H is from Hardy and Littlewood [43].
For the reader who is interested in learning more on the topics discussed
in this chapter, we recommend the books by Apostol [8] and Hardy and
Wright [44]; indeed, many of the proofs presented in this chapter are minor
variations on proofs from these two books. Our proof of Bertrand’s postu-
late is based on the presentation in Section 9.2 of Redmond [76]. See also
Bach and Shallit [12] (especially Chapter 8), Crandall and Pomerance [30]
(especially Chapter 1) for a more detailed overview of these topics.
The data in Tables 5.1 and 5.2 was obtained using the computer program
Maple.
TEAM LinG

6
Finite and discrete probability distributions
This chapter introduces concepts from discrete probability theory. We begin
with a discussion of ﬁnite probability distributions, and then towards the end
of the chapter we discuss the more general notion of a discrete probability
distribution.
6.1 Finite probability distributions: basic deﬁnitions
A ﬁnite probability distribution D = (U, P) is a ﬁnite, non-empty set
U, together with a function P that maps u ∈U to P[u] ∈[0, 1], such that

u∈U
P[u] = 1.
(6.1)
The set U is called the sample space and the function P is called the
probability function.
Intuitively, the elements of U represent the possible outcomes of a random
experiment, where the probability of outcome u ∈U is P[u].
Up until §6.10, we shall use the phrase “probability distribution” to mean
“ﬁnite probability distribution.”
Example 6.1. If we think of rolling a fair die, then U := {1, 2, 3, 4, 5, 6},
and P[u] := 1/6 for all u ∈U gives a probability distribution describing the
possible outcomes of the experiment. 2
Example 6.2. More generally, if U is a ﬁnite set, and P[u] = 1/|U| for all
u ∈U, then D is called the uniform distribution on U. 2
Example 6.3. A coin ﬂip is an example of a Bernoulli trial, which is
in general an experiment with only two possible outcomes: success, which
occurs with probability p, and failure, which occurs with probability q :=
1 −p. 2
96
TEAM LinG

6.1 Finite probability distributions: basic deﬁnitions
97
An event is a subset A of U, and the probability of A is deﬁned to be
P[A] :=

u∈A
P[u].
(6.2)
Thus, we extend the domain of deﬁnition of P from outcomes u ∈U to
events A ⊆U.
For an event A ⊆U, let A denote the complement of A in U. We have
P[∅] = 0, P[U] = 1, P[A] = 1 −P[A].
For any events A, B ⊆U, if A ⊆B, then P[A] ≤P[B]. Also, for any events
A, B ⊆U, we have
P[A ∪B] = P[A] + P[B] −P[A ∩B] ≤P[A] + P[B];
(6.3)
in particular, if A and B are disjoint, then
P[A ∪B] = P[A] + P[B].
(6.4)
More generally, for any events A1, . . . , An ⊆U we have
P[A1 ∪· · · ∪An] ≤P[A1] + · · · + P[An],
(6.5)
and if the Ai are pairwise disjoint, then
P[A1 ∪· · · ∪An] = P[A1] + · · · + P[An].
(6.6)
In working with events, one makes frequent use of the usual rules of
Boolean logic. DeMorgan’s law says that for events A and B, we have
A ∪B = A ∩B and A ∩B = A ∪B.
We also have the distributive law: for events A, B, C, we have
A ∩(B ∪C) = (A ∩B) ∪(A ∩C) and A ∪(B ∩C) = (A ∪B) ∩(A ∪C).
In some applications and examples, it is more natural to use the logical
“or” connective “∨” in place of “∪,” and the logical “and” connective “∧”
in place of “∩.”
Example 6.4. Continuing with Example 6.1, the probability of an “odd
roll” A = {1, 3, 5} is 1/2. 2
Example 6.5. More generally, if D is the uniform distribution on a set U
of cardinality n, and A is a subset of U of cardinality k, then P[A] = k/n.
2
Example 6.6. Alice rolls two dice, and asks Bob to guess a value that
appears on either of the two dice (without looking).
Let us model this
TEAM LinG

98
Finite and discrete probability distributions
situation by considering the uniform distribution on {(x, y) : x, y = 1, . . . , 6},
where x represents the value of the ﬁrst die, and y the value of the second.
For x = 1, . . . , 6, let Ax be the event that the ﬁrst die is x, and Bx
the event that the second die is x, Let Cx = Ax ∪Bx be the event that x
appears on either of the two dice. No matter what value x Bob chooses, the
probability that this choice is correct is
P[Cx] = P[Ax ∪Bx] = P[Ax] + P[Bx] −P[Ax ∩Bx]
= 1/6 + 1/6 −1/36 = 11/36. 2
If D1 = (U1, P1) and D2 = (U2, P2) are probability distributions, we can
form the product distribution D = (U, P), where U := U1 × U2, and
P[(u1, u2)] := P1[u1]P2[u2]. It is easy to verify that the product distribution
is also a probability distribution. Intuitively, the elements (u1, u2) of U1×U2
denote the possible outcomes of two separate and independent experiments.
More generally, if Di = (Ui, Pi) for i = 1, . . . , n, we can deﬁne the product
distribution D = (U, P), where U := U1 × · · · × Un, and P[(u1, . . . , un)] :=
P[u1] . . . P[un].
Example 6.7. We can view the probability distribution in Example 6.6 as
the product of two copies of the uniform distribution on {1, . . . , 6}. 2
Example 6.8. Consider the product distribution of n copies of a Bernoulli
trial (see Example 6.3), with associated success probability p and failure
probability q := 1 −p. An element of the sample space is an n-tuple of
success/failure values. Any such tuple that contains, say, k successes and
n −k failures, occurs with probability pkqn−k, regardless of the particular
positions of the successes and failures. 2
Exercise 6.1. This exercise asks you to recast previously established results
in terms of probability theory.
(a) Let k ≥2 be an integer, and suppose an integer n is chosen at random
from among all k-bit integers. Show that the probability that n is
prime is Θ(1/k).
(b) Let n be a positive integer, and suppose that a and b are chosen
at random from the set {1, . . . , n}. Show that the probability that
gcd(a, b) = 1 is at least 1/4.
(c) Let n be a positive integer, and suppose that a is chosen at random
from the set {1, . . . , n}. Show that the probability that gcd(a, n) = 1
is Ω(1/ log log n).
TEAM LinG

6.2 Conditional probability and independence
99
Exercise 6.2. Suppose A, B, C are events such that A ∩C = B ∩C. Show
that |P[A] −P[B]| ≤P[C].
Exercise
6.3.
Generalize
equation
(6.3)
by
proving
the
inclu-
sion/exclusion principle: for events A1, . . . , An, we have
P[A1 ∪· · · ∪An] =

i
P[Ai] −

i<j
P[Ai ∩Aj] +

i<j<k
P[Ai ∩Aj ∩Ak] −· · · + (−1)n−1P[A1 ∩· · · ∩An]
=
n

ℓ=1
(−1)ℓ−1 
i1<···<iℓ
P[Ai1 ∩· · · ∩Aiℓ].
Exercise 6.4. Show that for events A1, . . . , An, we have
P[A1 ∪· · · ∪An] ≥

i
P[Ai] −

i<j
P[Ai ∩Aj].
Exercise 6.5. Generalize inequality (6.5) and the previous exercise by prov-
ing Bonferroni’s inequalities: for events A1, . . . , An, and deﬁning
em := P[A1 ∪· · · ∪An] −
m

ℓ=1
(−1)ℓ−1 
i1<···<iℓ
P[Ai1 ∩· · · ∩Aiℓ]
for m = 1, . . . , n, we have em ≤0 for odd m, and em ≥0 for even m.
6.2 Conditional probability and independence
Let D = (U, P) be a probability distribution.
For any event B ⊆U with P[B] ̸= 0 and any u ∈U, let us deﬁne
P[u | B] :=
 P[u]/P[B]
if u ∈B,
0
otherwise.
Viewing B as ﬁxed, we may view the function P[· | B] as a new probability
function on the sample space U, and this gives rise a new probability distri-
bution DB := (P[· | B], U), called the conditional distribution given B.
Intuitively, DB has the following interpretation: if a random experiment
produces an outcome according to the distribution D, and we learn that the
event B has occurred, then the distribution DB assigns new probabilities to
all possible outcomes, reﬂecting the partial knowledge that the event B has
occurred.
TEAM LinG

100
Finite and discrete probability distributions
As usual, we extend the domain of deﬁnition of P[· | B] from outcomes to
events. For any event A ⊆U, we have
P[A | B] =

u∈A
P[u | B] = P[A ∩B]
P[B]
.
The value P[A | B] is called the conditional probability of A given B.
Again, the intuition is that this is the probability that the event A occurs,
given the partial knowledge that the event B has occurred.
For events A and B, if P[A ∩B] = P[A] · P[B], then A and B are called
independent events. If P[B] ̸= 0, a simple calculation shows that A and B
are independent if and only if P[A | B] = P[A].
A collection A1, . . . , An of events is called pairwise independent if
P[Ai ∩Aj] = P[Ai]P[Aj] for all i ̸= j, and is called mutually independent
if every subset Ai1, . . . , Aik of the collection satisﬁes
P[Ai1 ∩· · · ∩Aik] = P[Ai1] · · · P[Aik].
Example 6.9. In Example 6.6, suppose that Alice tells Bob the sum of
the two dice before Bob makes his guess. For example, suppose Alice tells
Bob the sum is 4. Then what is Bob’s best strategy in this case? Let Sz be
the event that the sum is z, for z = 2, . . . , 12, and consider the conditional
probability distribution given S4. This is the uniform distribution on the
three pairs (1, 3), (2, 2), (3, 1). The numbers 1 and 3 both appear in two
pairs, while the number 2 appears in just one pair. Therefore,
P[C1 | S4] = P[C3 | S4] = 2/3,
while
P[C2 | S4] = 1/3
and
P[C4 | S4] = P[C5 | S4] = P[C6 | S4] = 0.
Thus, if the sum is 4, Bob’s best strategy is to guess either 1 or 3.
Note that the events A1 and B2 are independent, while the events A1 and
S4 are not. 2
Example 6.10. Suppose we toss three fair coins. Let A1 be the event that
the ﬁrst coin is “heads,” let A2 be the event that the second coin is “heads,”
and let A3 be the event that the third coin is “heads.” Then the collection
of events {A1, A2, A3} is mutually independent.
Now let B12 be the event that the ﬁrst and second coins agree (i.e., both
“heads” or both “tails”), let B13 be the event that the ﬁrst and third coins
TEAM LinG

6.2 Conditional probability and independence
101
agree, and let B23 be the event that the second and third coins agree. Then
the collection of events {B12, B13, B23} is pairwise independent, but not mu-
tually independent. Indeed, the probability that any one of the events occurs
is 1/2, and the probability that any two of the three events occurs is 1/4;
however, the probability that all three occurs is also 1/4, since if any two
events occur, then so does the third. 2
Suppose we have a collection B1, . . . , Bn of events that partitions U, such
that each event Bi occurs with non-zero probability. Then it is easy to see
that for any event A,
P[A] =
n

i=1
P[A ∩Bi] =
n

i=1
P[A | Bi] · P[Bi].
(6.7)
Furthermore, if P[A] ̸= 0, then for any j = 1, . . . , n, we have
P[Bj | A] = P[A ∩Bj]
P[A]
=
P[A | Bj]P[Bj]
n
i=1 P[A | Bi]P[Bi].
(6.8)
This equality, known as Bayes’ theorem, lets us compute the conditional
probability P[Bj | A] in terms of the conditional probabilities P[A | Bi].
The equation (6.7) is useful for computing or estimating probabilities by
conditioning on speciﬁc events Bi (i.e., by considering the conditional prob-
ability distribution given Bi) in such a way that the conditional probabilities
P[A | Bi] are easy to compute or estimate. Also, if we want to compute a
conditional probability P[A | C], we can do so by partitioning C into events
B1, . . . , Bn, where each Bi occurs with non-zero probability, and use the
following simple fact:
P[A | C] =
n

i=1
P[A | Bi]P[Bi]/P[C].
(6.9)
Example 6.11. This example is based on the TV game show “Let’s make
a deal,” which was popular in the 1970’s. In this game, a contestant chooses
one of three doors. Behind two doors is a “zonk,” that is, something amusing
but of little or no value, such as a goat, and behind one of the doors is a
“grand prize,” such as a car or vacation package.
We may assume that
the door behind which the grand prize is placed is chosen at random from
among the three doors, with equal probability. After the contestant chooses
a door, the host of the show, Monty Hall, always reveals a zonk behind one
of the two doors not chosen by the contestant. The contestant is then given
a choice: either stay with his initial choice of door, or switch to the other
unopened door. After the contestant ﬁnalizes his decision on which door
TEAM LinG

102
Finite and discrete probability distributions
to choose, that door is opened and he wins whatever is behind the chosen
door. The question is, which strategy is better for the contestant: to stay
or to switch?
Let us evaluate the two strategies. If the contestant always stays with his
initial selection, then it is clear that his probability of success is exactly 1/3.
Now consider the strategy of always switching. Let B be the event that
the contestant’s initial choice was correct, and let A be the event that the
contestant wins the grand prize. On the one hand, if the contestant’s initial
choice was correct, then switching will certainly lead to failure. That is,
P[A | B] = 0.
On the other hand, suppose that the contestant’s initial
choice was incorrect, so that one of the zonks is behind the initially chosen
door. Since Monty reveals the other zonk, switching will lead with certainty
to success. That is, P[A | B] = 1. Furthermore, it is clear that P[B] = 1/3.
So we compute
P[A] = P[A | B]P[B] + P[A | B]P[B] = 0 · (1/3) + 1 · (2/3) = 2/3.
Thus, the “stay” strategy has a success probability of 1/3, while the
“switch” strategy has a success probability of 2/3. So it is better to switch
than to stay.
Of course, real life is a bit more complicated.
Monty did not always
reveal a zonk and oﬀer a choice to switch. Indeed, if Monty only revealed
a zonk when the contestant had chosen the correct door, then switching
would certainly be the wrong strategy. However, if Monty’s choice itself was
a random decision made independent of the contestant’s initial choice, then
switching is again the preferred strategy. 2
Example 6.12. Suppose that the rate of incidence of disease X in the
overall population is 1%. Also suppose that there is a test for disease X;
however, the test is not perfect: it has a 5% false positive rate (i.e., 5% of
healthy patients test positive for the disease), and a 2% false negative rate
(i.e., 2% of sick patients test negative for the disease). A doctor gives the
test to a patient and it comes out positive. How should the doctor advise
his patient? In particular, what is the probability that the patient actually
has disease X, given a positive test result?
Amazingly, many trained doctors will say the probability is 95%, since the
test has a false positive rate of 5%. However, this conclusion is completely
wrong.
Let A be the event that the test is positive and let B be the event that
the patient has disease X. The relevant quantity that we need to estimate
is P[B | A]; that is, the probability that the patient has disease X, given a
TEAM LinG

6.2 Conditional probability and independence
103
positive test result. We use Bayes’ theorem to do this:
P[B | A] =
P[A | B]P[B]
P[A | B]P[B] + P[A | B]P[B] =
0.98 · 0.01
0.98 · 0.01 + 0.05 · 0.99 ≈0.17.
Thus, the chances that the patient has disease X given a positive test result
is just 17%. The correct intuition here is that it is much more likely to get
a false positive than it is to actually have the disease.
Of course, the real world is a bit more complicated than this example
suggests: the doctor may be giving the patient the test because other risk
factors or symptoms may suggest that the patient is more likely to have the
disease than a random member of the population, in which case the above
analysis does not apply. 2
Exercise 6.6. Consider again the situation in Example 6.12, but now sup-
pose that the patient is visiting the doctor because he has symptom Y .
Furthermore, it is known that everyone who has disease X exhibits symp-
tom Y , while 10% of the population overall exhibits symptom Y . Assuming
that the accuracy of the test is not aﬀected by the presence of symptom Y ,
how should the doctor advise his patient should the test come out positive?
Exercise 6.7. Suppose we roll two dice, and let (x, y) denote the outcome
(as in Example 6.6). For each of the following pairs of events A and B,
determine if they are independent or not:
(a) A: x = y; B: y = 1.
(b) A: x ≥y; B: y = 1.
(c) A: x ≥y; B: y2 = 7y −6.
(d) A: xy = 6; B: y = 3.
Exercise 6.8. Let C be an event that occurs with non-zero probability,
and let B1, . . . , Bn be a partition of C, such that each event Bi occurs with
non-zero probability. Let A be an event and let p be a real number with
0 ≤p ≤1. Suppose that for each i = 1, . . . , n, the conditional probability of
A given Bi is ≤p (resp., <, =, >, ≥p). Show that the conditional probability
of A given C is also ≤p (resp., <, =, >, ≥p).
Exercise 6.9. Show that if two events A and B are independent, then so are
A and B. More generally, show that if A1, . . . , An are mutually independent,
then so are A′
1, . . . , A′
n, where each A′
i denotes either Ai or Ai.
Exercise 6.10. This exercise develops an alternative proof, based on prob-
ability theory, of Theorem 2.14. Let n > 1 be an integer and consider an
TEAM LinG

104
Finite and discrete probability distributions
experiment in which a number a is chosen at random from {0, . . . , n −1}.
If n = pe1
1 · · · per
r is the prime factorization of n, let Ai be the event that a
is divisible by pi, for i = 1, . . . , r.
(a) Show that
φ(n)/n = P[A1 ∩· · · ∩Ar],
where φ is Euler’s phi function.
(b) Show that if i1, . . . , iℓare distinct indices between 1 and r, then
P[Ai1 ∩· · · ∩Aiℓ] =
1
pi1 · · · piℓ
.
Conclude that the events Ai are mutually independent, and P[Ai] =
1/pi.
(c) Using part (b) and the result of the previous exercise, show that
P[A1 ∩· · · ∩Ar] =
r

i=1
(1 −1/pi).
(d) Combine parts (a) and (c) to derive the result of Theorem 2.14 that
φ(n) = n
r

i=1
(1 −1/pi).
6.3 Random variables
Let D = (U, P) be a probability distribution.
It is sometimes convenient to associate a real number, or other mathe-
matical object, with each outcome u ∈U. Such an association is called a
random variable; more formally, a random variable X is a function from
U into a set X.
If X is a subset of the real numbers, then X is called
a real random variable. When we speak of the image of X, we sim-
ply mean its image in the usual function-theoretic sense, that is, the set
X(U) = {X(u) : u ∈U}.
One may deﬁne any number of random variables on a given probability
distribution. If X : U →X is a random variable, and f : X →Y is a
function, then f(X) := f ◦X is also a random variable.
Example 6.13. Suppose we ﬂip n fair coins. Then we may deﬁne a ran-
dom variable X that maps each outcome to a bit string of length n, where a
“head” is encoded as a 1-bit, and a “tail” is encoded as a 0-bit. We may de-
ﬁne another random variable Y that is the number of “heads.” The variable
Y is a real random variable. 2
TEAM LinG

6.3 Random variables
105
Example 6.14. If A is an event, we may deﬁne a random variable X as
follows: X := 1 if the event A occurs, and X := 0 otherwise. The variable X
is called the indicator variable for A. Conversely, if Y is any 0/1-valued
random variable, we can deﬁne the event B to be the subset of all possible
outcomes that lead to Y = 1, and Y is the indicator variable for the event
B. Thus, we can work with either events or indicator variables, whichever
is more natural and convenient. 2
Let X : U →X be a random variable. For x ∈X, we write “X = x”
as shorthand for the event {u ∈U : X(u) = x}. More generally, for any
predicate φ, we may write “φ(X)” as shorthand for the event {u ∈U :
φ(X(u))}.
A random variable X deﬁnes a probability distribution on its image X,
where the probability associated with x ∈X is P[X = x]. We call this the
distribution of X. For two random variables X, Y deﬁned on a probability
distribution, Z := (X, Y ) is also a random variable whose distribution is
called the joint distribution of X and Y .
If X is a random variable, and A is an event with non-zero probability,
then the conditional distribution of X given A is a probability distri-
bution on the image X of X, where the probability associated with x ∈X
is P[X = x | A].
We say two random variables X, Y are independent if for all x in the
image of X and all y in the image of Y , the events X = x and Y = y are
independent, which is to say,
P[X = x ∧Y = y] = P[X = x]P[Y = y].
Equivalently, X and Y are independent if and only if their joint distribution
is equal to the product of their individual distributions. Alternatively, X
and Y are independent if and only if for all values x taken by X with non-
zero probability, the conditional distribution of Y given the event X = x is
the same as the distribution of Y .
Let X1, . . . , Xn be a collection of random variables, and let Xi be the image
of Xi for i = 1, . . . , n. We say X1, . . . , Xn are pairwise independent if for
all i, j = 1, . . . , n with i ̸= j, the variables Xi and Xj are independent. We
say that X1, . . . , Xn are mutually independent if for all x1 ∈X1, . . . , xn ∈
Xn, we have
P[X1 = x1 ∧· · · ∧Xn = xn] = P[X1 = x1] · · · P[Xn = xn].
More generally, for k = 2, . . . , n, we say that X1, . . . , Xn are k-wise inde-
pendent if any k of them are mutually independent.
TEAM LinG

106
Finite and discrete probability distributions
Example 6.15. We toss three coins, and set Xi := 0 if the ith coin is
“tails,” and Xi := 1 otherwise. The variables X1, X2, X3 are mutually inde-
pendent. Let us set Y12 := X1 ⊕X2, Y13 := X1 ⊕X3, and Y23 := X2 ⊕X3,
where “⊕” denotes “exclusive or,” that is, addition modulo 2. Then the
variables Y12, Y13, Y23 are pairwise independent, but not mutually indepen-
dent—observe that Y12 ⊕Y13 = Y23. 2
The following is a simple but useful fact:
Theorem 6.1. Let Xi : U →Xi be random variables, for i = 1, . . . , n, and
suppose that there exist functions fi : Xi →[0, 1], for i = 1, . . . , n, such that

xi∈Xi
fi(xi) = 1
(i = 1 . . . n),
and
P[X1 = x1 ∧· · · ∧Xn = xn] = f1(x1) · · · fn(xn)
for all x1 ∈X1, . . . , xn ∈Xn.
Then for any subset of distinct indices
i1, . . . , iℓ∈{1, . . . , n}, we have
P[Xi1 = xi1 ∧· · · ∧Xiℓ= xiℓ] = fi1(xi1) · · · fiℓ(xiℓ)
for all xi1 ∈Xi1, . . . , xiℓ∈Xiℓ.
Proof. To prove the theorem, it will suﬃce to show that we can “eliminate”
a single variable, say Xn, meaning that for all x1, . . . , xn−1, we have
P[X1 = x1 ∧· · · ∧Xn−1 = xn−1] = f1(x1) · · · fn−1(xn−1).
Having established this, we may then proceed to eliminate any number of
variables (the ordering of the variables is clearly irrelevant).
We have
P[X1 = x1 ∧· · · ∧Xn−1 = xn−1]
=

xn∈Xn
P[X1 = x1 ∧· · · ∧Xn−1 = xn−1 ∧Xn = xn]
=

xn∈Xn
f1(x1) · · · fn−1(xn−1)fn(xn)
= f1(x2) · · · fn−1(xn−1)

xn∈Xn
fn(xn)
= f1(x1) · · · fn−1(xn−1). 2
TEAM LinG

6.3 Random variables
107
The following three theorems are immediate consequences of the above
theorem:
Theorem 6.2. Let Xi : U →Xi be random variables, for i = 1, . . . , n, such
that
P[X1 = x1 ∧· · · ∧Xn = xn] =
1
|X1| · · ·
1
|Xn|
(for all x1 ∈X1, . . . , xn ∈Xn).
Then the variables Xi are mutually independent with each Xi uniformly
distributed over Xi.
Theorem 6.3. If X1, . . . , Xn are mutually independent random variables,
then they are k-wise independent for all k = 2, . . . , n.
Theorem 6.4. If Di = (Ui, Pi) are probability distributions for i = 1, . . . , n,
then the projection functions πi : U1 ×· · ·×Un →Ui, where πi(u1, . . . , un) =
ui, are mutually independent random variables on the product distribution
D1 × · · · × Dn.
We also have:
Theorem 6.5. If X1, . . . , Xn are mutually independent random variables,
and g1, . . . , gn are functions, then g1(X1), . . . , gn(Xn) are also mutually in-
dependent random variables.
Proof. The proof is a straightforward, if somewhat tedious, calculation. For
i = 1, . . . , n, let yi be some value in the image of gi(Xi), and let Xi :=
g−1
i
({yi}). We have
P[g1(X1) = y1 ∧· · · ∧gn(Xn) = yn]
= P

(

x1∈X1
X1 = x1) ∧· · · ∧(

xn∈Xn
Xn = xn)

= P
 
x1∈X1
· · ·

xn∈Xn
(X1 = x1 ∧· · · ∧Xn = xn)

=

x1∈X1
· · ·

xn∈Xn
P[X1 = x1 ∧· · · ∧Xn = xn]
=

x1∈X1
· · ·

xn∈Xn
P[X1 = x1] · · · P[Xn = xn]
=
 
x1∈X1
P[X1 = x1]

· · ·
 
xn∈Xn
P[Xn = xn]

TEAM LinG

108
Finite and discrete probability distributions
= P
 
x1∈X1
X1 = x1

· · · P
 
xn∈Xn
Xn = xn

= P[g1(X1) = y1] · · · P[gn(Xn) = yn]. 2
Example 6.16. If we toss n dice, and let Xi denote the value of the ith die
for i = 1, . . . , n, then the Xi are mutually independent random variables. If
we set Yi := X2
i for i = 1, . . . , n, then the Yi are also mutually independent
random variables. 2
Example 6.17. This example again illustrates the notion of pairwise in-
dependence. Let X and Y be independent and uniformly distributed over
Zp, where p is a prime. For a ∈Zp, let Za := aX + Y . Then we claim that
each Za is uniformly distributed over Zp, and that the collection of random
variables {Za : a ∈Zp} is pairwise independent.
To prove this claim, let a, b ∈Zp with a ̸= b, and consider the map
fa,b : Zp ×Zp →Zp ×Zp that sends (x, y) to (ax+y, bx+y). It is easy to see
that fa,b is injective; indeed, if ax + y = ax′ + y′ and bx + y = bx′ + y′, then
subtracting these two equations, we obtain (a −b)x = (a −b)x′, and since
a −b ̸= [0]p, it follows that x = x′, which also implies y = y′. Since fa,b is
injective, it must be a bijection from Zp × Zp onto itself. Thus, since (X, Y )
is uniformly distributed over Zp × Zp, so is (Za, Zb) = (aX + Y, bX + Y ). So
for all z, z′ ∈Zp, we have
P[Za = z ∧Zb = z′] = 1/p2,
and so the claim follows from Theorem 6.2.
Note that the variables Za are not 3-wise independent, since the value of
any two determines the value of all the rest (verify). 2
Example 6.18. We can generalize the previous example as follows. Let
X1, . . . , Xt, Y be mutually independent and uniformly distributed over Zp,
where p is prime, and for a1, . . . , at ∈Zp, let Za1,...,at := a1X1+· · ·+atXt+Y .
We claim that each Za1,...,at is uniformly distributed over Zp, and that the
collection of all such Za1,...,at is pairwise independent.
To prove this claim, it will suﬃce (by Theorem 6.2) to prove that for all
a1, . . . , at, b1, . . . , bt, z, z′ ∈Zp,
subject to (a1, . . . , at) ̸= (b1, . . . , bt), we have
P[Za1,...,at = z ∧Zb1,...,bt = z′] = 1/p2.
(6.10)
TEAM LinG

6.3 Random variables
109
Since (a1, . . . , at) ̸= (b1, . . . , bt), we know that aj ̸= bj for some j = 1, . . . , t.
Let us assume that a1 ̸= b1 (the argument for j > 1 is analogous).
We ﬁrst show that for all x2, . . . , xt ∈Zp, we have
P[Za1,...,at = z ∧Zb1,...,bt = z′ | X2 = x2 ∧· · · ∧Xt = xt] = 1/p2.
(6.11)
To prove (6.11), consider the conditional probability distribution given
X2 = x2 ∧· · · ∧Xt = xt. In this conditional distribution, we have
Za1,...,at = a1X1 + Y + c and Zb1,...,bt = b1X1 + Y + d,
where
c := a2x2 + · · · + atxt and d := b2x2 + · · · + btxt,
and X1 and Y are independent and uniformly distributed over Zp (this
follows from the mutual independence of X1, . . . , Xt, Y before conditioning).
By the result of the previous example, (a1X1 + Y, b1X1 + Y ) is uniformly
distributed over Zp × Zp, and since the function sending (x, y) ∈Zp × Zp to
(x+c, y+d) ∈Zp×Zp is a bijection, it follows that (a1X1+Y +c, b1X1+Y +d)
is uniformly distributed over Zp × Zp. That proves (6.11).
(6.10) now follows easily from (6.11), as follows:
P[Za1,...,at = z ∧Zb1,...,bt = z′]
=

x2,...,xt
P[Za1,...,at = z ∧Zb1,...,bt = z′ | X2 = x2 ∧· · · ∧Xt = xt] ·
P[X2 = x2 ∧· · · ∧Xt = xt]
=

x2,...,xt
1
p2 · P[X2 = x2 ∧· · · ∧Xt = xt]
= 1
p2 ·

x2,...,xt
P[X2 = x2 ∧· · · ∧Xt = xt]
= 1
p2 · 1. 2
Using other algebraic techniques, there are many ways to construct pair-
wise and k-wise independent families of random variables.
Such families
play an important role in many areas of computer science.
Example
6.19.
Suppose we perform an experiment by executing n
Bernoulli trials (see Example 6.3), where each trial succeeds with the same
probability p, and fails with probability q := 1 −p, independently of the
outcomes of all the other trials. Let X denote the total number of successes.
For k = 0, . . . , n, let us calculate the probability that X = k.
To do this, let us introduce indicator variables X1, . . . , Xn, where for
TEAM LinG

110
Finite and discrete probability distributions
i = 1, . . . , n, we have Xi = 1 if the ith trial succeeds, and Xi = 0, otherwise.
By assumption, the Xi are mutually independent. Then we see that X =
X1 + · · · + Xn. Now, consider a ﬁxed value k = 0, . . . , n. Let Ck denote
the collection of all subsets of {1, . . . , n} of size k. For I ∈Ck, let AI be
the event that Xi = 1 for all i ∈I and Xi = 0 for all i /∈I. Since the Xi
are mutually independent, we see that P[AI] = pkqn−k (as in Example 6.8).
Evidently, the collection of events {AI}I∈Ck is a partition of the event that
X = k. Therefore,
P[X = k] =

I∈Ck
P[AI] =

I∈Ck
pkqn−k = |Ck|pkqn−k.
Finally, since
|Ck| =
n
k

,
we conclude that
P[X = k] =
n
k

pkqn−k.
The distribution of the random variable X is called a binomial distri-
bution. 2
Exercise 6.11. Let X1, . . . , Xn be random variables, and let Xi be the image
of Xi for i = 1, . . . , n. Show that X1, . . . , Xn are mutually independent if
and only if for all i = 2, . . . , n, and all x1 ∈X1, . . . , xi ∈Xi, we have
P[Xi = xi | Xi−1 = xi−1 ∧· · · ∧X1 = x1] = P[Xi = xi].
Exercise 6.12. Let A1, . . . , An be events with corresponding indicator vari-
ables X1, . . . , Xn. Show that the events A1, . . . , An are mutually indepen-
dent if and only if the random variables X1, . . . , Xn are mutually indepen-
dent.
Note: there is actually something non-trivial to prove here, since
our deﬁnitions for independent events and independent random variables
superﬁcially look quite diﬀerent.
Exercise 6.13. Let C be an event that occurs with non-zero probability,
and let B1, . . . , Bn be a partition of C, such that each event Bi occurs with
non-zero probability. Let X be a random variable whose image is X, and let
D′ be a probability distribution on X. Suppose that for each i = 1, . . . , n,
the conditional distribution of X given Bi is equal to D′. Show that the
conditional distribution of X given C is also equal to D′.
TEAM LinG

6.4 Expectation and variance
111
Exercise 6.14. Let n be a positive integer, and let X be a random variable
whose distribution is uniform over {0, . . . , n −1}. For each positive divisor
d of n, let use deﬁne the random variable Xd := X mod d. Show that for
any positive divisors d1, . . . , dk of n, the random variables Xd1, . . . , Xdk are
mutually independent if and only if d1, . . . , dk are pairwise relatively prime.
Exercise 6.15. With notation as in the previous exercise, let n := 30, and
describe the conditional distribution of X15 given that X6 = 1.
Exercise 6.16. Let W, X, Y be mutually independent and uniformly dis-
tributed over Zp, where p is prime. For any a ∈Zp, let Za := a2W +aX +Y .
Show that each Za is uniformly distributed over Zp, and that the collection
of all Za is 3-wise independent.
Exercise 6.17. Let Xib, for i = 1, . . . , k and b ∈{0, 1}, be mutually inde-
pendent random variables, each with a uniform distribution on {0, 1}. For
b1, . . . , bk ∈{0, 1}, let us deﬁne the random variable
Yb1···bk := X1b1 ⊕· · · ⊕Xkbk,
where “⊕” denotes “exclusive or.” Show that the 2k variables Yb1···bk are
pairwise independent, each with a uniform distribution over {0, 1}.
6.4 Expectation and variance
Let D = (U, P) be a probability distribution. If X is a real random variable,
then its expected value is
E[X] :=

u∈U
X(u) · P[u].
(6.12)
If X is the image of X, we have
E[X] =

x∈X

u∈X−1({x})
xP[u] =

x∈X
x · P[X = x].
(6.13)
From (6.13), it is clear that E[X] depends only on the distribution of X
(and not on any other properties of the underlying distribution D). More
generally, by a similar calculation, one sees that if X is any random variable
with image X, and f is a real-valued function on X, then
E[f(X)] =

x∈X
f(x)P[X = x].
(6.14)
We make a few trivial observations about expectation, which the reader
may easily verify. First, if X is equal to a constant c (i.e., X(u) = c for all
TEAM LinG

112
Finite and discrete probability distributions
u ∈U), then E[X] = E[c] = c. Second, if X takes only non-negative values
(i.e., X(u) ≥0 all u ∈U), then E[X] ≥0. Similarly, if X takes only positive
values, then E[X] > 0.
A crucial property about expectation is the following:
Theorem 6.6 (Linearity of expectation). For real random variables X
and Y , and real number a, we have
E[X + Y ] = E[X] + E[Y ]
and
E[aX] = aE[X].
Proof. It is easiest to prove this using the deﬁning equation (6.12) for ex-
pectation. For u ∈U, the value of the random variable X + Y at u is by
deﬁnition X(u) + Y (u), and so we have
E[X + Y ] =

u∈U
(X(u) + Y (u))P[u]
=

u∈U
X(u)P[u] +

u∈U
Y (u)P[u]
= E[X] + E[Y ].
For the second part of the theorem, by a similar calculation, we have
E[aX] =

u
(aX(u))P[u] = a

u
X(u)P[u] = aE[X]. 2
More generally, the above theorem implies (using a simple induction ar-
gument) that for any real random variables X1, . . . , Xn, we have
E[X1 + · · · + Xn] = E[X1] + · · · + E[Xn].
So we see that expectation is linear; however, expectation is not in general
multiplicative, except in the case of independent random variables:
Theorem 6.7. If X and Y are independent real random variables, then
E[XY ] = E[X]E[Y ].
Proof. It is easiest to prove this using (6.14). We have
E[XY ] =

x,y
xyP[X = x ∧Y = y]
=

x,y
xyP[X = x]P[Y = y]
TEAM LinG

6.4 Expectation and variance
113
=
 
x
xP[X = x]
 
y
yP[Y = y]

= E[X] · E[Y ]. 2
More generally, the above theorem implies (using a simple induction ar-
gument) that if X1, . . . , Xn are mutually independent real random variables,
then
E[X1 · · · Xn] = E[X1] · · · E[Xn].
The following fact is sometimes quite useful:
Theorem 6.8. If X is a random variable that takes values in the set
{0, 1, . . . , n}, then
E[X] =
n

i=1
P[X ≥i].
Proof. For i = 1, . . . , n, deﬁne the random variable Xi so that Xi = 1 if
X ≥i and Xi = 0 if X < i. Note that E[Xi] = 1 · P[X ≥i] + 0 · P[X < i] =
P[X ≥i]. Moreover, X = X1 + · · · + Xn, and hence
E[X] =
n

i=1
E[Xi] =
n

i=1
P[X ≥i]. 2
The variance of a real random variable X is Var[X] := E[(X −E[X])2].
The variance provides a measure of the spread or dispersion of the distri-
bution of X around its expected value E[X]. Note that since (X −E[X])2
takes only non-negative values, variance is always non-negative.
Theorem 6.9. Let X be a real random variable, and let a and b be real
numbers. Then we have
(i) Var[X] = E[X2] −(E[X])2,
(ii) Var[aX] = a2Var[X], and
(iii) Var[X + b] = Var[X].
Proof. Let µ := E[X]. For part (i), observe that
Var[X] = E[(X −µ)2] = E[X2 −2µX + µ2]
= E[X2] −2µE[X] + E[µ2] = E[X2] −2µ2 + µ2
= E[X2] −µ2,
where in the third equality, we used the fact that expectation is linear, and
TEAM LinG

114
Finite and discrete probability distributions
in the fourth equality, we used the fact that E[c] = c for constant c (in this
case, c = µ2).
For part (ii), observe that
Var[aX] = E[a2X2] −(E[aX])2 = a2E[X2] −(aµ)2
= a2(E[X2] −µ2) = a2Var[X],
where we used part (i) in the ﬁrst and fourth equality, and the linearity of
expectation in the second.
Part (iii) follows by a similar calculation (verify):
Var[X + b] = E[(X + b)2] −(µ + b)2
= (E[X2] + 2bµ + b2) −(µ2 + 2bµ + b2)
= E[X2] −µ2 = Var[X]. 2
A simple consequence of part (i) of Theorem 6.9 is that E[X2] ≥(E[X])2.
Unlike expectation, the variance of a sum of random variables is not equal
to the sum of the variances, unless the variables are pairwise independent:
Theorem 6.10. If X1, . . . , Xn is a collection of pairwise independent real
random variables, then
Var

n

i=1
Xi

=
n

i=1
Var[Xi].
Proof. We have
Var
 
i
Xi

= E

(

i
Xi)2

−

E[

i
Xi]
2
=

i
E[X2
i ] + 2

i,j
j<i
(E[XiXj] −E[Xi]E[Xj]) −

i
E[Xi]2
(by Theorem 6.6 and rearranging terms)
=

i
E[X2
i ] −

i
E[Xi]2
(by pairwise independence and Theorem 6.7)
=

i
Var[Xi]. 2
For any random variable X and event B, with P[B] ̸= 0, we can deﬁne
the conditional expectation of X given B, denoted E[X | B], to be the
TEAM LinG

6.4 Expectation and variance
115
expected value of X in the conditional probability distribution given B. We
have
E[X | B] =

u∈U
X(u) · P[u | B] =

x∈X
xP[X = x | B],
(6.15)
where X is the image of X.
If B1, . . . , Bn is a collection of events that partitions U, where each Bi
occurs with non-zero probability, then it follows from the deﬁnitions that
E[X] =
n

i=1
E[X | Bi]P[Bi].
(6.16)
Example 6.20. Let X be uniformly distributed over {1, . . . , n}. Let us
compute E[X] and Var[X]. We have
E[X] =
n

x=1
x · 1
n = n(n + 1)
2
· 1
n = n + 1
2
.
We also have
E[X2] =
n

x=1
x2 · 1
n = n(n + 1)(2n + 1)
6
· 1
n = (n + 1)(2n + 1)
6
.
Therefore,
Var[X] = E[X2] −(E[X])2 = n2 −1
12
. 2
Example 6.21. Let X denote the value of a die toss. Let A be the event
that X is even. Then in the conditional probability space given A, we see
that X is uniformly distributed over {2, 4, 6}, and hence
E[X | A] = 2 + 4 + 6
3
= 4.
Similarly, in the conditional probability space given A, we see that X is
uniformly distributed over {1, 3, 5}, and hence
E[X | A] = 1 + 3 + 5
3
= 3.
We can compute the expected value of X using these conditional expecta-
tions; indeed, we have
E[X] = E[X | A]P[A] + E[X | A]P[A] = 4 · 1
2 + 3 · 1
2 = 7
2,
which agrees with the calculation in previous example. 2
TEAM LinG

116
Finite and discrete probability distributions
Example 6.22. Suppose that a random variable X takes the value 1 with
probability p, and 0 with probability q := 1 −p. The distribution of X is
that of a Bernoulli trial, as discussed in Example 6.3. Let us compute E[X]
and Var[X]. We have
E[X] = 1 · p + 0 · q = p.
We also have
E[X2] = 12 · p + 02 · q = p.
Therefore,
Var[X] = E[X2] −(E[X])2 = p −p2 = pq. 2
Example 6.23. Suppose that X1, . . . , Xn are mutually independent ran-
dom variables such that each Xi takes the value 1 with probability p and 0
with probability q := 1 −p. Let us set X := X1 + · · · + Xn. Note that the
distribution of each Xi is that of a Bernoulli trial, as in Example 6.3, and
the distribution of X is a binomial distribution, as in Example 6.19. By the
previous example, we have E[Xi] = p and Var[Xi] = pq for i = 1, . . . , n. Let
us compute E[X] and Var[X]. By Theorem 6.6, we have
E[X] =
n

i=1
E[Xi] = np,
and by Theorem 6.10, and the fact that the Xi are mutually independent,
we have
Var[X] =
n

i=1
Var[Xi] = npq. 2
Exercise 6.18. A casino oﬀers you the following four dice games. In each
game, you pay 15 dollars to play, and two dice are rolled. In the ﬁrst game,
the house pays out four times the value of the ﬁrst die (in dollars). In the
second, the house pays out twice the sum of the two dice. In the third,
the house pays the square of the ﬁrst. In the fourth, the house pays the
product of the two dice. Which game should you play? That is, which game
maximizes your expected winnings?
Exercise 6.19. Suppose X and Y are independent real random variables
such that E[X] = E[Y ]. Show that
E[(X −Y )2] = Var[X] + Var[Y ].
TEAM LinG

6.5 Some useful bounds
117
Exercise 6.20. Show that the variance of any 0/1-valued random variable
is at most 1/4.
Exercise 6.21. A die is tossed repeatedly until it comes up “1,” or until it
is tossed n times (whichever comes ﬁrst). What is the expected number of
tosses of the die?
Exercise 6.22. Suppose that 20 percent of the students who took a certain
test were from school A and the average of their scores on the test was
65. Also, suppose that 30 percent of the students were from school B and
the average of their scores was 85. Finally, suppose that the remaining 50
percent of the students were from school C and the average of their scores
was 72. If a student is selected at random from the entire group that took
the test, what is the expected value of his score?
Exercise 6.23. An urn contains r ≥0 red balls and b ≥1 black balls.
Consider the following experiment. At each step in the experiment, a single
ball is removed from the urn, randomly chosen from among all balls that
remain in the urn: if a black ball is removed, the experiment halts, and
if a red ball is removed, the experiment continues (without returning the
red ball to the urn). Show that the expected number of steps performed is
(r + b + 1)/(b + 1).
6.5 Some useful bounds
In this section, we present several theorems that can be used to bound the
probability that a random variable deviates from its expected value by some
speciﬁed amount.
Theorem 6.11 (Markov’s inequality). Let X be a random variable that
takes only non-negative real values. Then for any t > 0, we have
P[X ≥t] ≤E[X]/t.
Proof. We have
E[X] =

x
xP[X = x] =

x<t
xP[X = x] +

x≥t
xP[X = x].
Since X takes only non-negative values, all of the terms in the summation
are non-negative. Therefore,
E[X] ≥

x≥t
xP[X = x] ≥

x≥t
tP[X = x] = tP[X ≥t]. 2
TEAM LinG

118
Finite and discrete probability distributions
Markov’s inequality may be the only game in town when nothing more
about the distribution of X is known besides its expected value. However,
if the variance of X is also known, then one can get a better bound.
Theorem 6.12 (Chebyshev’s inequality). Let X be a real random vari-
able. Then for any t > 0, we have
P[|X −E[X]| ≥t] ≤Var[X]/t2.
Proof. Let Y := (X −E[X])2. Then Y is always non-negative, and E[Y ] =
Var[X]. Applying Markov’s inequality to Y , we have
P[|X −E[X]| ≥t] = P[Y ≥t2] ≤Var[X]/t2. 2
An important special case of Chebyshev’s inequality is the following. Sup-
pose that X1, . . . , Xn are pairwise independent real random variables, each
with the same distribution. Let µ be the common value of E[Xi] and ν the
common value of Var[Xi]. Set
X := 1
n(X1 + · · · + Xn).
The variable X is called the sample mean of X1, . . . , Xn. By the linearity of
expectation, we have E[X] = µ, and since the Xi are pairwise independent,
it follows from Theorem 6.10 (along with part (ii) of Theorem 6.9) that
Var[X] = ν/n. Applying Chebyshev’s inequality, for any ϵ > 0, we have
P[|X −µ| ≥ϵ] ≤
ν
nϵ2 .
(6.17)
The inequality (6.17) says that for all ϵ > 0, and for all δ > 0, there exists n0
(depending on ϵ and δ, as well as the variance ν) such that n ≥n0 implies
P[|X −µ| ≥ϵ] ≤δ.
(6.18)
In words:
As n gets large, the sample mean closely approximates the ex-
pected value µ with high probability.
This fact, known as the law of large numbers, justiﬁes the usual intuitive
interpretation given to expectation.
Let us now examine an even more specialized case of the above situation.
Suppose that X1, . . . , Xn are pairwise independent random variables, each of
which takes the value 1 with probability p, and 0 with probability q := 1−p.
As before, let X be the sample mean of X1, . . . , Xn. As we calculated in
TEAM LinG

6.5 Some useful bounds
119
Example 6.22, the Xi have a common expected value p and variance pq.
Therefore, by (6.17), for any ϵ > 0, we have
P[|X −p| ≥ϵ] ≤pq
nϵ2 .
(6.19)
The bound on the right-hand side of (6.19) decreases linearly in n. If one
makes the stronger assumption that the Xi are mutually independent (so
that X := X1 + · · · + Xn has a binomial distribution), one can obtain a
much better bound that decreases exponentially in n:
Theorem 6.13 (Chernoﬀbound). Let X1, . . . , Xn be mutually indepen-
dent random variables, such that each Xi is 1 with probability p and 0 with
probability q := 1 −p. Assume that 0 < p < 1. Also, let X be the sample
mean of X1, . . . , Xn. Then for any ϵ > 0, we have:
(i) P[X −p ≥ϵ] ≤e−nϵ2/2q;
(ii) P[X −p ≤−ϵ] ≤e−nϵ2/2p;
(iii) P[|X −p| ≥ϵ] ≤2 · e−nϵ2/2.
Proof. First, we observe that (ii) follows directly from (i) by replacing Xi
by 1 −Xi and exchanging the roles of p and q. Second, we observe that (iii)
follows directly from (i) and (ii). Thus, it suﬃces to prove (i).
Let α > 0 be a parameter, whose value will be determined later. Deﬁne
the random variable Z := eαn(X−p). Since the function x →eαnx is strictly
increasing, we have X−p ≥ϵ if and only if Z ≥eαnϵ. By Markov’s inequality,
it follows that
P[X −p ≥ϵ] = P[Z ≥eαnϵ] ≤E[Z]e−αnϵ.
(6.20)
So our goal is to bound E[Z] from above.
For i = 1, . . . , n, deﬁne the random variable Zi := eα(Xi−p). Note that
Z = n
i=1 Zi, that the Zi are mutually independent random variables (see
Theorem 6.5), and that
E[Zi] = eα(1−p)p + eα(0−p)q = peαq + qe−αp.
It follows that
E[Z] = E[

i
Zi] =

i
E[Zi] = (peαq + qe−αp)n.
We will prove below that
peαq + qe−αp ≤eα2q/2.
(6.21)
TEAM LinG

120
Finite and discrete probability distributions
From this, it follows that
E[Z] ≤eα2qn/2.
(6.22)
Combining (6.22) with (6.20), we obtain
P[X −p ≥ϵ] ≤eα2qn/2−αnϵ.
(6.23)
Now we choose the parameter α so as to minimize the quantity α2qn/2−αnϵ.
The optimal value of α is easily seen to be α = ϵ/q, and substituting this
value of α into (6.23) yields (i).
To ﬁnish the proof of the theorem, it remains to prove the inequality
(6.21). Let
β := peαq + qe−αp.
We want to show that β ≤eα2q/2, or equivalently, that log β ≤α2q/2. We
have
β = eαq(p + qe−α) = eαq(1 −q(1 −e−α)),
and taking logarithms and applying parts (i) and (ii) of §A1, we obtain
log β = αq+log(1−q(1−e−α)) ≤αq−q(1−e−α) = q(e−α +α−1) ≤qα2/2.
This establishes (6.21) and completes the proof of the theorem. 2
Thus, the Chernoﬀbound is a quantitatively superior version of the law
of large numbers, although its range of application is clearly more limited.
Example 6.24. Suppose we toss 10,000 coins. The expected number of
heads is 5,000. What is an upper bound on the probability α that we get
6,000 or more heads? Using Markov’s inequality, we get α ≤5/6. Using
Chebyshev’s inequality, and in particular, the inequality (6.19), we get
α ≤
1/4
10410−2 =
1
400.
Finally, using the Chernoﬀbound, we obtain
α ≤e−10410−2/2(0.5) = e−100 ≈10−43.4. 2
Exercise 6.24. You are given a biased coin. You know that if tossed, it will
come up heads with probability at least 51%, or it will come up tails with
probability at least 51%. Design an experiment that attempts to determine
the direction of the bias (towards heads or towards tails). The experiment
should work by ﬂipping the coin some number t times, and it should correctly
determine the direction of the bias with probability at least 99%. Try to
make t as small as possible.
TEAM LinG

6.6 The birthday paradox
121
6.6 The birthday paradox
This section discusses a number of problems related to the following ques-
tion: how many people must be in a room before there is a good chance
that two of them were born on the same day of the year? The answer is
surprisingly few, whence the “paradox.”
To answer this question, we index the people in the room with integers
1, . . . , k, where k is the number of people in the room. We abstract the
problem a bit, and assume that all years have the same number of days,
say n—setting n = 365 corresponds to the original problem, except that
leap years are not handled correctly, but we shall ignore this detail. For
i = 1, . . . , k, let Xi denote the day of the year on which i’s birthday falls.
Let us assume that birthdays are uniformly distributed over {0, . . . , n −1};
this assumption is actually not entirely realistic, as it is well known that
people are somewhat more likely to be born in some months than in others.
So for any i = 1, . . . , k and x = 0, . . . , n −1, we have P[Xi = x] = 1/n.
Let α be the probability that no two persons share the same birthday, so
that 1 −α is the probability that there is a pair of matching birthdays. We
would like to know how big k must be relative to n so that α is not too
large, say, at most 1/2.
We can compute α as follows, assuming the Xi are mutually independent.
There are a total of nk sequences of integers (x1, . . . , xk), with each xi ∈
{0, . . . , n−1}. Among these, there are a total of n(n−1) · · · (n−k +1) that
contain no repetitions: there are n choices for x1, and for any ﬁxed value of
x1, there are n −1 choices for x2, and so on. Therefore
α = n(n−1) · · · (n−k +1)/nk =

1−1
n

1−2
n

· · ·

1−k −1
n

. (6.24)
Using the part (i) of §A1, we obtain
α ≤e−Pk−1
i=1 i/n = e−k(k−1)/2n.
So if k(k −1) ≥(2 log 2)n, we have α ≤1/2. Thus, when k is at least a
small constant times n1/2, we have α ≤1/2, so the probability that two
people share the same birthday is at least 1/2. For n = 365, k ≥23 suﬃces.
Indeed, one can simply calculate α in this case numerically from equation
(6.24), obtaining α ≈0.493. Thus, if there are 23 people in the room, there
is about a 50-50 chance that two people have the same birthday.
The above analysis assumed the Xi are mutually independent. However,
we can still obtain useful upper bounds for α under much weaker indepen-
dence assumptions.
TEAM LinG

122
Finite and discrete probability distributions
For i = 1, . . . , k and j = i + 1, . . . , k, let us deﬁne the indicator variable
Wij :=
 1
if Xi = Xj,
0
if Xi ̸= Xj.
If we assume that the Xi are pairwise independent, then
P[Wij = 1] = P[Xi = Xj] =
n−1

x=0
P[Xi = x ∧Xj = x]
=
n−1

x=0
P[Xi = x]P[Xj = x] =
n−1

x=0
1/n2 = 1/n.
We can compute the expectation and variance (see Example 6.22):
E[Wij] = 1
n,
Var[Wij] = 1
n(1 −1
n).
Now consider the random variable
W :=
k

i=1
k

j=i+1
Wij,
which represents the number of distinct pairs of people with the same birth-
day. There are k(k −1)/2 terms in this sum, so by the linearity of expecta-
tion, we have
E[W] = k(k −1)
2n
.
Thus, for k(k −1) ≥2n, we “expect” there to be at least one pair of
matching birthdays. However, this does not guarantee that the probability
of a matching pair of birthdays is very high, assuming just pairwise inde-
pendence of the Xi. For example, suppose that n is prime and the Xi are
a subset of the family of pairwise independent random variables deﬁned in
Example 6.17. That is, each Xi is of the form aiX + Y , where X and Y
are uniformly and independently distributed modulo n. Then in fact, either
all the Xi are distinct, or they are all equal, where the latter event occurs
exactly when X = [0]n, and so with probability 1/n — “when it rains, it
pours.”
To get a useful upper bound on the probability α that there are no match-
ing birthdays, it suﬃces to assume that the Xi are 4-wise independent. In
this case, it is easy to verify that the variables Wij are pairwise indepen-
dent, since any two of the Wij are determined by at most four of the Xi.
Therefore, in this case, the variance of the sum is equal to the sum of the
TEAM LinG

6.6 The birthday paradox
123
variances, and so
Var[W] = k(k −1)
2n
(1 −1
n) ≤E[W].
Furthermore, by Chebyshev’s inequality,
α = P[W = 0] ≤P[|W −E[W]| ≥E[W]]
≤Var[W]/E[W]2 ≤1/E[W] =
2n
k(k −1).
Thus, if k(k −1) ≥4n, then α ≤1/2.
In many practical applications, it is more important to bound α from
below, rather than from above; that is, to bound from above the probability
1 −α that there are any collisions. For this, pairwise independence of the
Xi suﬃces, since than we have P[Wij = 1] = 1/n, and by (6.5), we have
1 −α ≤
k

i=1
k

j=i+1
P[Wij = 1] = k(k −1)
2n
,
which is at most 1/2 provided k(k −1) ≤n.
Exercise 6.25. Let α1, . . . , αn be real numbers with n
i=1 αi = 1. Show
that
0 ≤
n

i=1
(αi −1/n)2 =
n

i=1
α2
i −1/n,
and in particular,
n

i=1
α2
i ≥1/n.
Exercise 6.26. Let X be a set of size n ≥1, and let X and X′ be indepen-
dent random variables, taking values in X, and with the same distribution.
Show that
P[X = X′] =

x∈X
P[X = x]2 ≥1
n.
Exercise 6.27. Let X be a set of size n ≥1, and let x0 be an arbitrary,
ﬁxed element of X. Consider a random experiment in which a function F is
chosen uniformly from among all nn functions from X into X. Let us deﬁne
random variables Xi, for i = 0, 1, 2, . . . , as follows:
X0 := x0,
Xi+1 := F(Xi) (i = 0, 1, 2, . . .).
TEAM LinG

124
Finite and discrete probability distributions
Thus, the value of Xi is obtained by applying the function F a total of i
times to the starting value x0. Since X has size n, the sequence {Xi} must
repeat at some point; that is, there exists a positive integer k (with k ≤n)
such that Xk = Xi for some i = 0, . . . , k −1. Deﬁne the random variable K
to be the smallest such value k.
(a) Show that for any i ≥0 and any ﬁxed values of x1, . . . , xi ∈X such
that x0, x1, . . . , xi are distinct, the conditional distribution of Xi+1
given that X1 = x1, . . . , Xi = xi is uniform over X.
(b) Show that for any integer k ≥1, we have K ≥k if and only if
X0, X1, . . . , Xk−1 take on distinct values.
(c) From parts (a) and (b), show that for any k = 1, . . . , n, we have
P[K ≥k | K ≥k −1] = 1 −(k −1)/n,
and conclude that
P[K ≥k] =
k−1

i=1
(1 −i/n) ≤e−k(k−1)/2n.
(d) Show that
∞

k=1
e−k(k−1)/2n = O(n1/2)
and then conclude from part (c) that
E[K] =
n

k=1
P[K ≥k] ≤
∞

k=1
e−k(k−1)/2n = O(n1/2).
(e) Modify the above argument to show that E[K] = Ω(n1/2).
Exercise 6.28. The setup for this exercise is identical to that of the previous
exercise, except that now, the function F is chosen uniformly from among
all n! permutations of X.
(a) Show that if K = k, then Xk = X0.
(b) Show that for any i ≥0 and any ﬁxed values of x1, . . . , xi ∈X such
that x0, x1, . . . , xi are distinct, the conditional distribution of Xi+1
given that X1 = x1, . . . , Xi = xi is uniform over X \ {x1, . . . , xi}.
(c) Show that for any k = 2, . . . , n, we have
P[K ≥k | K ≥k −1] = 1 −
1
n −k + 2,
TEAM LinG

6.7 Hash functions
125
and conclude that for all k = 1, . . . , n, we have
P[K ≥k] =
k−2

i=0

1 −
1
n −i

= 1 −k −1
n
.
(d) From part (c), show that K is uniformly distributed over {1, . . . , n},
and in particular,
E[K] = n + 1
2
.
6.7 Hash functions
In this section, we apply the tools we have developed thus far to a par-
ticularly important area of computer science: the theory and practice of
hashing.
The scenario is as follows. We have ﬁnite, non-empty sets A and Z, with
|A| = k and |Z| = n, and a ﬁnite, non-empty set H of hash functions, each
of which map elements of A into Z. More precisely, each element h ∈H
deﬁnes a function that maps a ∈A to an element z ∈Z, and we write
z = h(a); the value z is called the hash code of a (under h), and we say
that a hashes to z (under h). Note that two distinct elements of H may
happen to deﬁne the same function. We call H a family of hash functions
(from A to Z).
Let H be a random variable whose distribution is uniform on H. For any
a ∈A, H(a) denotes the random variable whose value is z = h(a) when
H = h. For any ℓ= 1, . . . , k, we say that H is an ℓ-wise independent
family of hash functions if each H(a) is uniformly distributed over Z, and the
collection of all H(a) is ℓ-wise independent; in case ℓ= 2, we say that H is
a pairwise independent family of hash functions. Pairwise independence
is equivalent to saying that for all a, a′ ∈A, with a ̸= a′, and all z, z′ ∈Z,
P[H(a) = z ∧H(a′) = z′] = 1
n2 .
Example 6.25. Examples 6.17 and 6.18 provide explicit constructions for
pairwise independent families of hash functions.
In particular, from the
discussion in Example 6.17, if n is prime, and we take A := Zn, Z := Zn,
and H := {hx,y : x, y ∈Zn}, where for hx,y ∈H and a ∈A we deﬁne
hx,y(a) := ax+y, then H is a pairwise independent family of hash functions
from A to Z.
Similarly, Example 6.18 yields a pairwise independent family of hash func-
tions from A := Z×t
n to Z := Zn, with H := {hx1,...,xt,y : x1, . . . , xt, y ∈Zn},
TEAM LinG

126
Finite and discrete probability distributions
where for hx1,...,xt,y ∈H and (a1, . . . , at) ∈A, we deﬁne
hx1,...,xt,y(a1, . . . , at) := a1x1 + · · · + atxt + y.
In practice, the inputs to such a hash function may be long bit strings, which
we chop into small pieces so that each piece can be viewed as an element of
Zn. 2
6.7.1 Hash tables
Pairwise independent families of hash functions may be used to implement
a data structure known as a hash table, which in turn may be used to
implement a dictionary.
Assume that H is a family of hash functions from A to Z, where |A| = k
and |Z| = n. A hash function is chosen at random from H; an element
a ∈A is inserted into the hash table by storing the value of a into a bin
indexed by the hash code of a; likewise, to see if a particular value a ∈A
is stored in the hash table, one must search in the bin indexed by the hash
code of a.
So as to facilitate fast storage and retrieval, one typically wants the ele-
ments stored in the hash table to be distributed in roughly equal proportions
among all the bins.
Assuming that H is a pairwise independent family of hash functions, one
can easily derive some useful results, such as the following:
• If the hash table holds q values, then for any value a ∈A, the expected
number of other values that are in the bin indexed by a’s hash code
is at most q/n. This result bounds the expected amount of “work”
we have to do to search for a value in its corresponding bin, which
is essentially equal to the size of the bin. In particular, if q = O(n),
then the expected amount of work is constant.
See Exercise 6.32
below.
• If the table holds q values, with q(q −1) ≤n, then with probability
at least 1/2, each value lies in a distinct bin. This result is useful if
one wants to ﬁnd a “perfect” hash function that hashes q ﬁxed values
to distinct bins: if n is suﬃciently large, we can just choose hash
functions at random until we ﬁnd one that works. See Exercise 6.33
below.
• If the table holds n values, then the expected value of the maximum
number of values in any bin is O(n1/2). See Exercise 6.34 below.
Results such as these, and others, can be obtained using a broader notion
TEAM LinG

6.7 Hash functions
127
of hashing called universal hashing. We call H a universal family of hash
functions if for all a, a′ ∈A, with a ̸= a′, we have
P[H(a) = H(a′)] ≤1
n.
Note that the pairwise independence property implies the universal prop-
erty (see Exercise 6.29 below).
There are even weaker notions that are
relevant in practice; for example, in some applications, it is suﬃcient to
require that P[H(a) = H(a′)] ≤c/n for some constant c.
Exercise 6.29. Show that any pairwise independent family of hash func-
tions is also a universal family of hash functions.
Exercise 6.30. Let A := Z×(t+1)
n
and Z := Zn, where n is prime. Let
H := {hx1,...,xt : x1, . . . , xt ∈Zn} be a family of hash functions from A to
Z, where for hx1,...,xt ∈H, and for (a0, a1, . . . , at) ∈A, we deﬁne
hx1,...,xt(a0, a1, . . . , at) := a0 + a1x1 + · · · + atxt.
Show that H is universal, but not pairwise independent.
Exercise 6.31. Let k be a prime and let n be any positive integer. Let
A := {0, . . . , k −1} and Z := {0, . . . , n −1}. Let
H := {hx,y : x = 1, . . . , k −1, y = 0, . . . , k −1},
be a family of hash functions from A to Z, where for hx,y ∈H and for a ∈A,
we deﬁne
hx,y(a) := ((ax + y) mod k) mod n.
Show that H is universal. Hint: ﬁrst show that for any a, a′ ∈A with a ̸= a′,
the number of h ∈H such that h(a) = h(a′) is equal to the number of pairs
of integers (r, s) such that
0 ≤r < k, 0 ≤s < k, r ̸= s, and r ≡s (mod n).
In the following three exercises, assume that H is a universal family of
hash functions from A to Z, where |A| = k and |Z| = n, and that H is a
random variable uniformly distributed over H.
Exercise 6.32. Let a1, . . . , aq be distinct elements of A, and let a ∈A.
Deﬁne L to be the number of indices i = 1, . . . , q such that H(ai) = H(a).
Show that
E[L] ≤
 1 + (q −1)/n
if a ∈{a1, . . . , aq};
q/n
otherwise.
TEAM LinG

128
Finite and discrete probability distributions
Exercise 6.33. Let a1, . . . , aq be distinct elements of A, and assume that
q(q −1) ≤n. Show that the probability that H(ai) = H(aj) for some i, j
with i ̸= j, is at most 1/2.
Exercise 6.34. Assume k ≥n, and let a1, . . . , an be distinct elements of
A. For z ∈Z, deﬁne the random variable Bz := {ai : H(ai) = z}. Deﬁne
the random variable M := max{|Bz| : z ∈Z}. Show that E[M] = O(n1/2).
Exercise 6.35. A family H of hash functions from A to Z is called ϵ-
universal if for H uniformly distributed over H, and for all a, a′ ∈A with
a ̸= a′, we have P[H(a) = H(a′)] ≤ϵ. Show that if H is ϵ-universal, then we
must have
ϵ ≥
1
|Z| −1
|A|.
Hint: using Exercise 6.26, ﬁrst show that if H, A, A′ are mutually inde-
pendent random variables, with H uniformly distributed over H, and A
and A′ uniformly distributed over A, then P[A ̸= A′ ∧H(A) = H(A′)] ≥
1/|Z| −1/|A|.
6.7.2 Message authentication
Pairwise independent families of hash functions may be used to implement
a message authentication scheme, which is a mechanism to detect if
a message has been tampered with in transit between two parties. Unlike
an error correcting code (such as the one discussed in §4.5.1), a message
authentication scheme should be eﬀective against arbitrary tampering.
As above, assume that H is a family of hash functions from A to Z, where
|A| = k and |Z| = n. Suppose that Alice and Bob somehow agree upon a
hash function chosen at random from H. At some later time, Alice transmits
a message a ∈A to Bob over an insecure network. In addition to sending
a, Alice also sends the hash code z of a. Upon receiving a pair (a, z), Bob
checks that the hash code of a is indeed equal to z: if so, he accepts the
message as authentic (i.e., originating from Alice); otherwise, he rejects the
message.
Now suppose that an adversary is trying to trick Bob into accepting an
inauthentic message (i.e., one not originating from Alice). Assuming that
H is a pairwise independent family of hash functions, it is not too hard
to see that the adversary can succeed with probability no better than 1/n,
regardless of the strategy or computing power of the adversary. Indeed, on
the one hand, suppose the adversary gives Bob a pair (a′, z′) at some time
TEAM LinG

6.7 Hash functions
129
before Alice sends her message. In this case, the adversary knows nothing
about the hash function, and so the correct value of the hash code of a′
is completely unpredictable: it is equally likely to be any element of Z.
Therefore, no matter how clever the adversary is in choosing a′ and z′, Bob
will accept (a′, z′) as authentic with probability only 1/n. On the other hand,
suppose the adversary waits until Alice sends her message, intercepting the
message/hash code pair (a, z) sent by Alice, and gives Bob a pair (a′, z′),
where a′ ̸= a, instead of the pair (a, z). Again, since the adversary does not
know anything about the hash function other than the fact that the hash
code of a is equal to z, the correct hash code of a′ is completely unpredictable,
and again, Bob will accept (a′, z′) as authentic with probability only 1/n.
One can easily make n large enough so that the probability that an ad-
versary succeeds is so small that for all practical purposes it is impossible
to trick Bob (e.g., n ≈2100).
More formally, and more generally, one can deﬁne an ϵ-forgeable mes-
sage authentication scheme to be a family H of hash functions from A
to Z with the following property: if H is uniformly distributed over H, then
(i) for all a ∈A and z ∈Z, we have P[H(a) = z] ≤ϵ, and
(ii) for all a ∈A and all functions f : Z →A and g : Z →Z, we have
P[A′ ̸= a ∧H(A′) = Z′] ≤ϵ,
where Z := H(a), A′ := f(Z), and Z′ := g(Z).
Intuitively, part (i) of this deﬁnition says that it is impossible to guess the
hash code of any message with probability better than ϵ; further, part (ii)
of this deﬁnition says that even after seeing the hash code of one message, it
is impossible to guess the hash code of a diﬀerent message with probability
better than ϵ, regardless the choice of the ﬁrst message (i.e., the value a) and
regardless of the strategy used to pick the second message and its putative
hash code, given the hash code of the ﬁrst message (i.e., the functions f and
g).
Exercise 6.36. Suppose that a family H of hash functions from A to Z is
an ϵ-forgeable message authentication scheme. Show that ϵ ≥1/|Z|.
Exercise 6.37. Suppose that H is a family of hash functions from A to Z
and that |A| > 1. Show that if H satisﬁes part (ii) of the deﬁnition of an
ϵ-forgeable message authentication scheme, then it also satisﬁes part (i) of
the deﬁnition.
TEAM LinG

130
Finite and discrete probability distributions
Exercise 6.38. Let H be a family of hash functions from A to Z. For
ϵ ≥0, we call H pairwise ϵ-predictable if the following holds: for H
uniformly distributed over H, for all a, a′ ∈A, and for all z, z′ ∈Z, we have
P[H(a) = z] ≤ϵ and
P[H(a) = z] > 0 and a′ ̸= a implies P[H(a′) = z′ | H(a) = z] ≤ϵ.
(a) Show that if H is pairwise ϵ-predictable, then it is an ϵ-forgeable
message authentication scheme.
(b) Show that if H is pairwise independent, then it is pairwise 1/|Z|-
predictable. Combining this with part (a), we see that if H is pair-
wise independent, then it is a 1/|Z|-forgeable message authentication
scheme (which makes rigorous the intuitive argument given above).
(c) Give an example of a family of hash functions that is an ϵ-forgeable
message authentication scheme for some ϵ < 1, but is not pairwise
ϵ-predictable for any ϵ < 1.
Exercise 6.39. Give an example of an ϵ-forgeable message authentication
scheme, where ϵ is very small, but where if Alice authenticates two distinct
messages using the same hash function, an adversary can easily forge the
hash code of any message he likes (after seeing Alice’s two messages and their
hash codes). This shows that, as we have deﬁned a message authentication
scheme, Alice should only authenticate a single message per hash function
(t messages may be authenticated using t hash functions).
Exercise 6.40. Let H be an ϵ-universal family of hash functions from A to
Y (see Exercise 6.35), and let H′ be a pairwise independent family of hash
functions from Y to Z. Deﬁne the composed family H′ ◦H of hash functions
from A to Z as H′◦H := {φh′,h : h′ ∈H′, h ∈H}, where φh′,h(a) := h′(h(a))
for φh′,h ∈H′◦H and for a ∈A. Show that H′◦H is an (ϵ+1/|Z|)-forgeable
message authentication scheme.
6.8 Statistical distance
This section discusses a useful measure “distance” between two random
variables.
Although important in many applications, the results of this
section (and the next) will play only a very minor role in the remainder of
the text.
Let X and Y be random variables which both take values on a ﬁnite set
TEAM LinG

6.8 Statistical distance
131
V. We deﬁne the statistical distance between X and Y as
∆[X; Y ] := 1
2

v∈V
|P[X = v] −P[Y = v]|.
Theorem 6.14. For random variables X, Y, Z, we have
(i) 0 ≤∆[X; Y ] ≤1,
(ii) ∆[X; X] = 0,
(iii) ∆[X; Y ] = ∆[Y ; X], and
(iv) ∆[X; Z] ≤∆[X; Y ] + ∆[Y ; Z].
Proof. Exercise. 2
Note that ∆[X; Y ] depends only on the individual distributions of X and
Y , and not on the joint distribution of X and Y . As such, one may speak of
the statistical distance between two distributions, rather than between two
random variables.
Example 6.26. Suppose X has the uniform distribution on {1, . . . , n}, and
Y has the uniform distribution on {1, . . . , n−k}, where 0 ≤k ≤n−1. Let us
compute ∆[X; Y ]. We could apply the deﬁnition directly; however, consider
the following graph of the distributions of X and Y :
1/n
1/(n −k)
0
n −k
n
A
B
C
The statistical distance between X and Y is just 1/2 times the area of
regions A and C in the diagram. Moreover, because probability distributions
sum to 1, we must have
area of B + area of A = 1 = area of B + area of C,
and hence, the areas of region A and region C are the same. Therefore,
∆[X; Y ] = area of A = area of C = k/n. 2
The following characterization of statistical distance is quite useful:
Theorem 6.15. Let X and Y be random variables taking values on a set
TEAM LinG

132
Finite and discrete probability distributions
V. For any W ⊆V, we have
∆[X; Y ] ≥|P[X ∈W] −P[Y ∈W]|,
and equality holds if W is either the set of all v ∈V such that P[X = v] <
P[Y = v], or the complement of this set.
Proof. Suppose we partition the set V into two sets: the set V0 consisting
of those v ∈V such that P[X = v] < P[Y = v], and the set V1 consisting of
those v ∈V such that P[X = v] ≥P[Y = v]. Consider the following rough
graph of the distributions of X and Y , where the elements of V0 are placed
to the left of the elements of V1:
A
B
C
X
Y
V0
V1
Now, as in Example 6.26,
∆[X; Y ] = area of A = area of C.
Further, consider any subset W of V. The quantity |P[X ∈W] −P[Y ∈W]|
is equal to the absolute value of the diﬀerence of the area of the subregion
of A that lies above W and the area of the subregion of C that lies above
W. This quantity is maximized when W = V0 or W = V1, in which case it
is equal to ∆[X; Y ]. 2
We can restate Theorem 6.15 as follows:
∆[X; Y ] = max{|P[φ(X)] −P[φ(Y )]| : φ is a predicate on V}.
This implies that when ∆[X; Y ] is very small, then for any predicate φ, the
events φ(X) and φ(Y ) occur with almost the same probability. Put another
way, there is no “statistical test” that can eﬀectively distinguish between
the distributions of X and Y . For many applications, this means that the
distribution of X is “for all practical purposes” equivalent to that of Y , and
hence in analyzing the behavior of X, we can instead analyze the behavior
of Y , if that is more convenient.
TEAM LinG

6.8 Statistical distance
133
Theorem 6.16. Let X, Y be random variables taking values on a set V, and
let f be a function from V into a set W. Then ∆[f(X); f(Y )] ≤∆[X; Y ].
Proof. By Theorem 6.15, for any subset W′ of W, we have
|P[f(X) ∈W′] −P[f(Y ) ∈W′]| =
|P[X ∈f−1(W′)] −P[Y ∈f−1(W′)]| ≤∆[X; Y ].
In particular, again by Theorem 6.15,
∆[f(X); f(Y )] = |P[f(X) ∈W′] −P[f(Y ) ∈W′]|
for some W′. 2
Example 6.27. Let X be uniformly distributed on the set {0, . . . , n −1},
and let Y be uniformly distributed on the set {0, . . . , m−1}, for m ≥n. Let
f(y) := y mod n. We want to compute an upper bound on the statistical
distance between X and f(Y ). We can do this as follows. Let m = qn −r,
where 0 ≤r < n, so that q = ⌈m/n⌉. Also, let Z be uniformly distributed
over {0, . . . , qn−1}. Then f(Z) is uniformly distributed over {0, . . . , n−1},
since every element of {0, . . . , n −1} has the same number (namely, q) of
pre-images under f which lie in the set {0, . . . , qn −1}. Therefore, by the
previous theorem,
∆[X; f(Y )] = ∆[f(Z); f(Y )] ≤∆[Z; Y ],
and as we saw in Example 6.26,
∆[Z; Y ] = r/qn < 1/q ≤n/m.
Therefore,
∆[X; f(Y )] < n/m. 2
We close this section with two useful theorems.
Theorem 6.17. Let X and Y be random variables taking values on a set V,
and let W be a random variable taking values on a set W. Further, suppose
that X and W are independent, and that Y and W are independent. Then
the statistical distance between (X, W) and (Y, W) is equal to the statistical
distance between X and Y ; that is,
∆[X, W; Y, W] = ∆[X, Y ].
TEAM LinG

134
Finite and discrete probability distributions
Proof. From the deﬁnition of statistical distance,
2∆[X, W; Y, W] =

v,w
|P[X = v ∧W = w] −P[Y = v ∧W = w]|
=

v,w
|P[X = v]P[W = w] −P[Y = v]P[W = w]|
(by independence)
=

v,w
P[W = w]|P[X = v] −P[Y = v]|
= (

w
P[W = w])(

v
|P[X = v] −P[Y = v]|)
= 1 · 2∆[X; Y ]. 2
Theorem 6.18. Let U1, . . . , Uℓ, V1, . . . , Vℓbe mutually independent random
variables. We have
∆[U1, . . . , Uℓ; V1, . . . , Vℓ] ≤
ℓ

i=1
∆[Ui; Vi].
Proof. We introduce random variables W0, . . . , Wℓ, deﬁned as follows:
W0 := (U1, . . . , Uℓ),
Wi := (V1, . . . , Vi, Ui+1, . . . , Uℓ)
for i = 1, . . . , ℓ−1, and
Wℓ:= (V1, . . . , Vℓ).
By deﬁnition,
∆[U1, . . . , Uℓ; V1, . . . , Vℓ] = ∆[W0; Wℓ].
Moreover, by part (iv) of Theorem 6.14, we have
∆[W0; Wℓ] ≤
ℓ

i=1
∆[Wi−1; Wi].
Now consider any ﬁxed index i = 1, . . . , ℓ. By Theorem 6.17, we have
∆[Wi−1; Wi] = ∆[ Ui, (V1, . . . , Vi−1, Ui+1, . . . , Uℓ);
Vi, (V1, . . . , Vi−1, Ui+1, . . . , Uℓ)]
= ∆[Ui; Vi].
The theorem now follows immediately. 2
The technique used in the proof of the previous theorem is sometimes
TEAM LinG

6.8 Statistical distance
135
called a hybrid argument, as one considers the sequence of “hybrid” vari-
ables W0, W1, . . . , Wℓ, and shows that the distance between each consecutive
pair of variables is small.
Exercise 6.41. Let X and Y be independent random variables, each uni-
formly distributed over Zp, where p is prime. Calculate ∆[X, Y ; X, XY ].
Exercise 6.42. Let n be a large integer that is the product of two distinct
primes of roughly the same bit length. Let X be uniformly distributed over
Zn, and let Y be uniformly distributed over Z∗
n.
Show that ∆[X; Y ] =
O(n−1/2).
Exercise 6.43. Let V be a ﬁnite set, and consider any function φ : V →
{0, 1}. Let B be a random variable uniformly distributed over {0, 1}, and
for b = 0, 1, let Xb be a random variable taking values in V, and assume
that Xb and B are independent. Show that
|P[φ(XB) = B] −1
2| = 1
2|P[φ(X0) = 1] −P[φ(X1) = 1]| ≤1
2∆[X0; X1].
Exercise 6.44. Let X, Y be random variables on a probability distribution,
and let B1, . . . , Bn be events that partition of the underlying sample space,
where each Bi occurs with non-zero probability. For i = 1, . . . , n, let Xi
and Yi denote the random variables X and Y in the conditional probability
distribution given Bi; that is, P[Xi = v] = P[X = v | Bi], and P[Yi = v] =
P[Y = v | Bi]. Show that
∆[X; Y ] ≤
n

i=1
∆[Xi; Yi]P[Bi].
Exercise 6.45. Let X and Y be random variables that take the same value
unless a certain event F occurs. Show that ∆[X; Y ] ≤P[F].
Exercise 6.46. Let M be a large integer. Consider three random exper-
iments. In the ﬁrst, we generate a random integer n between 1 and M,
and then a random integer w between 1 and n.
In the second, we gen-
erate a random integer n between 2 and M, and then generate a random
integer w between 1 and n.
In the third, we generate a random integer
n between 2 and M, and then a random integer w between 2 and n. For
i = 1, 2, 3, let Xi denote the outcome (n, w) of the ith experiment. Show
that ∆[X1; X2] = O(1/M) and ∆[X2; X3] = O(log M/M), and conclude
that ∆[X1; X3] = O(log M/M).
TEAM LinG

136
Finite and discrete probability distributions
Exercise 6.47. Show that Theorem 6.17 is not true if we drop the inde-
pendence assumptions.
Exercise 6.48. Show that the hypothesis of Theorem 6.18 can be weakened:
all one needs to assume is that X1, . . . , Xℓare mutually independent, and
that Y1, . . . , Yℓare mutually independent.
Exercise 6.49. Let Y1, . . . , Yℓbe mutually independent random variables,
where each Yi is uniformly distributed on {0, . . . , m −1}. For i = 1, . . . , ℓ,
deﬁne Zi := i
j=1 jYj. Let n be a prime greater than ℓ. Let S be any ﬁnite
subset of Z×ℓ. Let A be the event that for some (a1, . . . , aℓ) ∈S, we have
Zi ≡ai (mod n) for i = 1, . . . , ℓ. Show that
P[A] ≤|S|/nℓ+ ℓn/m.
Exercise 6.50. Let X be a set of size n ≥1. Let F be a random function
from X into X. Let G be a random permutation of X. Let x1, . . . , xℓbe
distinct, ﬁxed elements of X. Show that
∆[F(x1), . . . , F(xℓ); G(x1), . . . , G(xℓ)] ≤ℓ(ℓ−1)
2n
.
Exercise 6.51. Let H be a family hash functions from A to Z such that
(i) each h ∈H maps A injectively into Z, and (ii) there exists ϵ, with
0 ≤ϵ ≤1, such that ∆[H(a); H(a′)] ≤ϵ for all a, a′ ∈A, where H is
uniformly distributed over H. Show that |H| ≥(1 −ϵ)|A|.
6.9 Measures of randomness and the leftover hash lemma (∗)
In this section, we discuss diﬀerent ways to measure “how random” a prob-
ability distribution is, and relations among them. Consider a distribution
deﬁned on a ﬁnite sample space V. In some sense, the “most random” dis-
tribution on V is the uniform distribution, while the least random would be
a “point mass” distribution, that is, a distribution where one point v ∈V in
the sample space has probability 1, and all other points have probability 0.
We deﬁne three measures of randomness. Let X be a random variable
taking values on a set V of size N.
1. We say X is δ-uniform on V if the statistical distance between X
and the uniform distribution on V is equal to δ; that is,
δ = 1
2

v∈V
|P[X = v] −1/N|.
TEAM LinG

6.9 Measures of randomness and the leftover hash lemma (∗)
137
2. The guessing probability γ(X) of X is deﬁned to be
γ(X) := max{P[X = v] : v ∈V}.
3. The collision probability κ(X) of X is deﬁned to be
κ(X) :=

v∈V
P[X = v]2.
Observe that if X is uniformly distributed on V, then it is 0-uniform on V,
and γ(X) = κ(X) = 1/N. Also, if X has a point mass distribution, then it is
(1−1/N)-uniform on V, and γ(X) = κ(X) = 1. The quantity log2(1/γ(X))
is sometimes called the min entropy of X, and the quantity log2(1/κ(X)) is
sometimes called the Renyi entropy of X. The collision probability κ(X)
has the following interpretation: if X and X′ are identically distributed
independent random variables, then κ(X) = P[X = X′] (see Exercise 6.26).
We ﬁrst state some easy inequalities:
Theorem 6.19. Let X be a random variable taking values on a set V of
size N, such that X is δ-uniform on V, γ := γ(X), and κ := κ(X). Then
we have:
(i) κ ≥1/N;
(ii) γ2 ≤κ ≤γ ≤1/N + δ.
Proof. Part (i) is immediate from Exercise 6.26. The other inequalities are
left as easy exercises. 2
This theorem implies that the collision and guessing probabilities are min-
imal for the uniform distribution, which perhaps agrees with ones intuition.
While the above theorem implies that γ and κ are close to 1/N when δ is
small, the following theorem provides a converse of sorts:
Theorem 6.20. If X is δ-uniform on V, κ := κ(X), and N := |V|, then
κ ≥1 + 4δ2
N
.
Proof. We may assume that δ > 0, since otherwise the theorem is already
true, simply from the fact that κ ≥1/N.
For v ∈V, let pv := P[X = v]. We have δ = 1
2

v |pv −1/N|, and hence
TEAM LinG

138
Finite and discrete probability distributions
1 = 
v qv, where qv := |pv −1/N|/(2δ). So we have
1
N ≤

v
q2
v
(by Exercise 6.25)
=
1
4δ2

v
(pv −1/N)2
=
1
4δ2 (

v
p2
v −1/N)
(again by Exercise 6.25)
=
1
4δ2 (κ −1/N),
from which the theorem follows immediately. 2
We are now in a position to state and prove a very useful result which,
intuitively, allows us to convert a “low quality” source of randomness into
a “high quality” source of randomness, making use of a universal family of
hash functions (see §6.7.1).
Theorem 6.21 (Leftover hash lemma). Let H be a universal family of
hash functions from A to Z, where Z is of size n. Let H denote a random
variable with the uniform distribution on H, and let A denote a random
variable taking values in A, and with H, A independent. Let κ := κ(A).
Then (H, H(A)) is δ-uniform on H × Z, where
δ ≤√nκ/2.
Proof. Let Z denote a random variable uniformly distributed on Z, with
H, A, Z mutually independent. Let m := |H| and δ := ∆[H, H(A); H, Z].
Let us compute the collision probability κ(H, H(A)). Let H′ have the
same distribution as H and A′ have the same distribution as A, with
H, H′, A, A′ mutually independent. Then
κ(H, H(A)) = P[H = H′ ∧H(A) = H′(A′)]
= P[H = H′]P[H(A) = H(A′)]
= 1
m

P[H(A) = H(A′) | A = A′]P[A = A′] +
P[H(A) = H(A′) | A ̸= A′]P[A ̸= A′]

≤1
m(P[A = A′] + P[H(A) = H(A′) | A ̸= A′])
TEAM LinG

6.9 Measures of randomness and the leftover hash lemma (∗)
139
≤1
m(κ + 1/n)
=
1
mn(nκ + 1).
Applying Theorem 6.20 to the random variable (H, H(A)), which takes
values on the set H × Z of size N := mn, we see that 4δ2 ≤nκ, from which
the theorem immediately follows. 2
Example 6.28. Suppose A is uniformly distributed over a subset A′ of A,
where |A′| ≥2160, so that κ(A) ≤2−160. Suppose that H is a universal
family of hash functions from A to Z, where |Z| ≤264. If H is uniformly
distributed over H, independently of A, then the leftover hash lemma says
that (H, H(A)) is δ-uniform on H × Z, with
δ ≤
√
2642−160/2 = 2−49. 2
The leftover hash lemma allows one to convert “low quality” sources of
randomness into “high quality” sources of randomness.
Suppose that to
conduct an experiment, we need to sample a random variable Z whose dis-
tribution is uniform on a set Z of size n, or at least δ-uniform for a small
value of δ. However, we may not have direct access to a source of “real”
randomness whose distribution looks anything like that of the desired uni-
form distribution, but rather, only to a “low quality” source of randomness.
For example, one could model various characteristics of a person’s typing
at the keyboard, or perhaps various characteristics of the internal state of a
computer (both its software and hardware) as a random process. We can-
not say very much about the probability distributions associated with such
processes, but perhaps we can conservatively estimate the collision or guess-
ing probability associated with these distributions. Using the leftover hash
lemma, we can hash the output of this random process, using a suitably
generated random hash function. The hash function acts like a “magnifying
glass”: it “focuses” the randomness inherent in the “low quality” source
distribution onto the set Z, obtaining a “high quality,” nearly uniform, dis-
tribution on Z.
Of course, this approach requires a random hash function, which may
be just as diﬃcult to generate as a random element of Z. The following
theorem shows, however, that we can at least use the same “magnifying
glass” many times over, with the statistical distance from uniform of the
output distribution increasing linearly in the number of applications of the
hash function.
TEAM LinG

140
Finite and discrete probability distributions
Theorem 6.22. Let H be a universal family of hash functions from A to Z,
where Z is of size n. Let H denote a random variable with the uniform distri-
bution on H, and let A1, . . . , Aℓdenote random variables taking values in A,
with H, A1, . . . , Aℓmutually independent. Let κ := max{κ(A1), . . . , κ(Aℓ)}.
Then (H, H(A1), . . . , H(Aℓ)) is δ′-uniform on H × Z×ℓ, where
δ′ ≤ℓ√nκ/2.
Proof. Let Z1, . . . , Zℓdenote random variables with the uniform distribution
on Z, with H, A1, . . . , Aℓ, Z1, . . . , Zℓmutually independent. We shall make a
hybrid argument (as in the proof of Theorem 6.18). Deﬁne random variables
W0, W1, . . . , Wℓas follows:
W0 := (H, H(A1), . . . , H(Aℓ)),
Wi := (H, Z1, . . . , Zi, H(Ai+1), . . . , H(Aℓ))
for i = 1, . . . , ℓ−1, and
Wℓ:= (H, Z1, . . . , Zℓ).
We have
δ′ = ∆[W0; Wℓ]
≤
ℓ

i=1
∆[Wi−1; Wi]
(by part (iv) of Theorem 6.14)
≤
ℓ

i=1
∆[H, Z1, . . . , Zi−1, H(Ai), Ai+1, . . . , Aℓ;
H, Z1, . . . , Zi−1,
Zi,
Ai+1, . . . , Aℓ]
(by Theorem 6.16)
=
ℓ

i=1
∆[H, H(Ai); H, Zi]
(by Theorem 6.17)
≤ℓ√nκ/2
(by Theorem 6.21). 2
Another source of “low quality” randomness arises in certain crypto-
graphic applications, where we have a “secret” random variable A that is
distributed uniformly over a large subset of some set A, but we want to
derive from A a “secret key” whose distribution is close to that of the uni-
form distribution on a speciﬁed “key space” Z (typically, Z is the set of all
bit strings of some speciﬁed length). The leftover hash lemma, combined
with Theorem 6.22, allows us to do this using a “public” hash function—
generated at random once and for all, published for all to see, and used over
and over to derive secret keys as needed.
TEAM LinG

6.10 Discrete probability distributions
141
Exercise 6.52. Consider again the situation in Theorem 6.21.
Suppose
that Z = {0, . . . , n −1}, but that we would rather have an almost-uniform
distribution over Z′ = {0, . . . , t −1}, for some t < n.
While it may be
possible to work with a diﬀerent family of hash functions, we do not have
to if n is large enough with respect to t, in which case we can just use the
value H(A) mod t. If Z′ is uniformly distributed over Z′, show that
∆[H, H(A) mod t; H, Z′] ≤√nκ/2 + t/n.
Exercise 6.53. Suppose X and Y are random variables with images X and
Y, respectively, and suppose that for some ϵ, we have P[X = x | Y = y] ≤ϵ
for all x ∈X and y ∈Y. Let H be a universal family of hash functions from
X to Z, where Z is of size n. Let H denote a random variable with the
uniform distribution on H, and Z denote a random variable with the uniform
distribution on Z, where the three variables H, Z, and (X, Y ) are mutually
independent. Show that the statistical distance between (Y, H, H(X)) and
(Y, H, Z) is at most √nϵ/2.
6.10 Discrete probability distributions
In addition to working with probability distributions over ﬁnite sample
spaces, one can also work with distributions over inﬁnite sample spaces.
If the sample space is countable, that is, either ﬁnite or countably inﬁnite,
then the distribution is called a discrete probability distribution. We
shall not consider any other types of probability distributions in this text.
The theory developed in §§6.1–6.5 extends fairly easily to the countably
inﬁnite setting, and in this section, we discuss how this is done.
6.10.1 Basic deﬁnitions
To say that the sample space U is countably inﬁnite simply means that
there is a bijection f from the set of positive integers onto U; thus, we can
enumerate the elements of U as u1, u2, u3, . . . , where ui = f(i).
As in the ﬁnite case, the probability function assigns to each u ∈U a
value P[u] ∈[0, 1].
The basic requirement that the probabilities sum to
one (equation (6.1)) is the requirement that the inﬁnite series ∞
i=1 P[ui]
converges to one. Luckily, the convergence properties of an inﬁnite series
whose terms are all non-negative is invariant under a re-ordering of terms
(see §A4), so it does not matter how we enumerate the elements of U.
Example 6.29. Suppose we ﬂip a fair coin repeatedly until it comes up
TEAM LinG

142
Finite and discrete probability distributions
“heads,” and let the outcome u of the experiment denote the number of coins
ﬂipped. We can model this experiment as a discrete probability distribution
D = (U, P), where U consists of the set of all positive integers, and where
for u ∈U, we set P[u] = 2−u. We can check that indeed ∞
u=1 2−u = 1, as
required.
One may be tempted to model this experiment by setting up a probabil-
ity distribution on the sample space of all inﬁnite sequences of coin tosses;
however, this sample space is not countably inﬁnite, and so we cannot con-
struct a discrete probability distribution on this space. While it is possible
to extend the notion of a probability distribution to such spaces, this would
take us too far aﬁeld. 2
Example 6.30. More generally, suppose we repeatedly execute a Bernoulli
trial until it succeeds, where each execution succeeds with probability p > 0
independently of the previous trials, and let the outcome u of the experiment
denote the number of trials executed. Then we associate the probability
P[u] = qu−1p with each positive integer u, where q := 1 −p, since we have
u −1 failures before the one success. One can easily check that these prob-
abilities sum to 1. Such a distribution is called a geometric distribution.
2
Example 6.31. The series ∞
i=1 1/i3 converges to some positive number
c. Therefore, we can deﬁne a probability distribution on the set of positive
integers, where we associate with each i ≥1 the probability 1/ci3. 2
Example 6.32. More generally, if xi, i = 1, 2, . . . , are non-negative num-
bers, and 0 < c := ∞
i=1 xi < ∞, then we can deﬁne a probability distri-
bution on the set of positive integers, assigning the probability xi/c to i.
2
As in the ﬁnite case, an event is an arbitrary subset A of U. The prob-
ability P[A] of A is deﬁned as the sum of the probabilities associated with
the elements of A—in the deﬁnition (6.2), the sum is treated as an inﬁnite
series when A is inﬁnite. This series is guaranteed to converge, and its value
does not depend on the particular enumeration of the elements of A.
Example 6.33. Consider the geometric distribution discussed in Exam-
ple 6.30, where p is the success probability of each Bernoulli trial, and
q := 1 −p. For integer i ≥1, consider the event A that the number of
trials executed is at least i. Formally, A is the set of all integers greater
than or equal to i. Intuitively, P[A] should be qi−1, since we perform at
least i trials if and only if the ﬁrst i −1 trials fail. Just to be sure, we can
TEAM LinG

6.10 Discrete probability distributions
143
compute
P[A] =

u≥i
P[u] =

u≥i
qu−1p = qi−1p

u≥0
qu = qi−1p ·
1
1 −q = qi−1. 2
It is an easy matter to check that all the statements made in §6.1 carry
over verbatim to the case of countably inﬁnite sample spaces. Moreover, it
also makes sense in the countably inﬁnite case to consider events that are a
union or intersection of a countably inﬁnite number of events:
Theorem 6.23. Let A1, A2, . . . be an inﬁnite sequence of events.
(i) If Ai ⊆Ai+1 for all i ≥1, then P[
i≥1 Ai] = limi→∞P[Ai].
(ii) In general, we have P[
i≥1 Ai] ≤
i≥1 P[Ai].
(iii) If the Ai are pairwise disjoint, then P[
i≥1 Ai] = 
i≥1 P[Ai].
(iv) If Ai ⊇Ai+1 for all i ≥1, then P[
i≥1 Ai] = limi→∞P[Ai].
Proof. For (i), let A := 
i≥1 Ai, and let a1, a2, . . . be an enumeration of the
elements of A. For any ϵ > 0, there exists a value k0 such that k0
i=1 ai >
P[A] −ϵ. Also, there is some k1 such that {a1, . . . , ak0} ⊆Ak1. Therefore,
for any k ≥k1, we have P[A] −ϵ < P[Ak] ≤P[A].
(ii) and (iii) follow by applying (i) to the sequence {i
j=1 Aj}i, and making
use of (6.5) and (6.6), respectively.
(iv) follows by applying (i) to the sequence {Ai}, using (the inﬁnite version
of) DeMorgan’s law. 2
6.10.2 Conditional probability and independence
All of the deﬁnitions and results in §6.2 carry over verbatim to the countably
inﬁnite case. Equation (6.7) as well as Bayes’ theorem (equation 6.8) and
equation (6.9) extend mutatis mutandus to the case of an inﬁnite partition
B1, B2, . . . .
6.10.3 Random variables
All of the deﬁnitions and results in §6.3 carry over verbatim to the countably
inﬁnite case (except Theorem 6.2, which of course only makes sense in the
ﬁnite setting).
TEAM LinG

144
Finite and discrete probability distributions
6.10.4 Expectation and variance
We deﬁne the expected value of a real random variable X exactly as before:
E[X] :=

u∈U
X(u) · P[u],
where, of course, the sum is an inﬁnite series.
However, if X may take
negative values, then we require that the series converges absolutely; that is,
we require that 
u∈U |X(u)| · P[u] < ∞(see §A4). Otherwise, we say the
expected value of X does not exist. Recall from calculus that a series that
converges absolutely will itself converge, and will converge to the same value
under a re-ordering of terms. Thus, if the expectation exists at all, its value
is independent of the ordering on U. For a non-negative random variable
X, if its expectation does not exist, one may express this as “E[X] = ∞.”
All of the results in §6.4 carry over essentially unchanged, except that one
must pay some attention to “convergence issues.”
Equations (6.13) and (6.14) hold, but with the following caveats (verify):
• If X is a real random variable with image X, then its expected value
E[X] exists if and only if the series 
x∈X xP[X = x] converges abso-
lutely, in which case E[X] is equal to the value of the latter series.
• If X is a random variable with image X and f a real-valued function
on X, then E[f(X)] exists if and only if the series 
x∈X f(x)P[X = x]
converges absolutely, in which case E[f(X)] is equal to the value of
the latter series.
Example 6.34. Let X be a random variable whose distribution is as in
Example 6.31.
Since the series  1/n2 converges and the series  1/n
diverges, the expectation E[X] exists, while E[X2] does not. 2
Theorems 6.6 and 6.7 hold under the additional hypothesis that E[X] and
E[Y ] exist.
If X1, X2, . . . is an inﬁnite sequence of real random variables, then the ran-
dom variable X := ∞
i=1 Xi is well deﬁned provided the series ∞
i=1 Xi(u)
converges for all u ∈U. One might hope that E[X] = ∞
i=1 E[Xi]; however,
this is not in general true, even if the individual expectations E[Xi] are non-
negative, and even if the series deﬁning X converges absolutely for all u;
nevertheless, it is true when the Xi are non-negative:
Theorem 6.24. Let X := 
i≥1 Xi, where each Xi takes non-negative val-
ues only. Then we have
E[X] =

i≥1
E[Xi].
TEAM LinG

6.10 Discrete probability distributions
145
Proof. We have

i≥1
E[Xi] =

i≥1

u∈U
Xi(u)P[u] =

u∈U

i≥1
Xi(u)P[u]
=

u∈U
P[u]

i≥1
Xi(u) = E[X],
where we use the fact that we may reverse the order of summation in an
inﬁnite double summation of non-negative terms (see §A5). 2
Using this theorem, one can prove the analog of Theorem 6.8 for countably
inﬁnite sample spaces, using exactly the same argument.
Theorem 6.25. If X is a random variable that takes non-negative integer
values, then
E[X] =
∞

i=1
P[X ≥i].
A nice picture to keep in mind with regards to Theorem 6.25 is the follow-
ing. Let pi := P[X = i] for i = 0, 1, . . . , and let us arrange the probabilities
pi in a table as follows:
p1
p2
p2
p3
p3
p3
...
...
Summing the ith row of this table, we get iP[X = i], and so E[X] is equal
to the sum of all the entries in the table. However, we may compute the
same sum column by column, and the sum of the entries in the ith column
is P[X ≥i].
Example 6.35. Suppose X is a random variable with a geometric distri-
bution, as in Example 6.30, with an associated success probability p and
failure probability q := 1 −p. As we saw in Example 6.33, for all integer
i ≥1, we have P[X ≥i] = qi−1. We may therefore apply Theorem 6.25 to
easily compute the expected value of X:
E[X] =
∞

i=1
P[X ≥i] =
∞

i=1
qi−1 =
1
1 −q = 1
p. 2
Example 6.36. To illustrate that Theorem 6.24 does not hold in general,
consider the geometric distribution on the positive integers, where P[j] = 2−j
for j ≥1. For i ≥1, deﬁne the random variable Xi so that Xi(i) = 2i,
TEAM LinG

146
Finite and discrete probability distributions
Xi(i + 1) = −2i+1, and Xi(j) = 0 for all j /∈{i, i + 1}. Then E[Xi] = 0 for
all i ≥1, and so 
i≥1 E[Xi] = 0. Now deﬁne X := 
i≥1 Xi. This is well
deﬁned, and in fact X(1) = 2, while X(j) = 0 for all j > 1. Hence E[X] = 1.
2
The variance Var[X] of X exists if and only if E[X] and E[(X −E[X])2]
exist, which holds if and only if E[X] and E[X2] exist.
Theorem 6.9 holds under the additional hypothesis that E[X] and E[X2]
exist. Similarly, Theorem 6.10 holds under the additional hypothesis that
E[Xi] and E[X2
i ] exist for each i.
The deﬁnition of conditional expectation carries over verbatim, as do
equations (6.15) and (6.16).
The analog of (6.16) for inﬁnite partitions
B1, B2, . . . does not hold in general, but does hold if X is always non-negative.
6.10.5 Some useful bounds
Both Theorems 6.11 and 6.12 (Markov’s and Chebyshev’s inequalities) hold,
under the additional hypothesis that the relevant expectations and variances
exist.
Exercise 6.54. Suppose X is a random variable taking positive integer
values, and that for some real number q, with 0 ≤q ≤1, and for all integers
i ≥1, we have P[X ≥i] = qi−1. Show that X has a geometric distribution
with associated success probability p := 1 −q.
Exercise 6.55. A gambler plays a simple game in a casino: with each play
of the game, the gambler may bet any number m of dollars; a coin is ﬂipped,
and if it comes up “heads,” the casino pays m dollars to the gambler, and
otherwise, the gambler pays m dollars to the casino. The gambler plays
the game repeatedly, using the following strategy: he initially bets a dollar;
each time he plays, if he wins, he pockets his winnings and goes home, and
otherwise, he doubles his bet and plays again.
(a) Show that if the gambler has an inﬁnite amount of money (so he
can keep playing no matter how many times he looses), then his ex-
pected winnings are one dollar. Hint: model the gambler’s winnings
as a random variable on a geometric distribution, and compute its
expected value.
(b) Show that if the gambler has a ﬁnite amount of money (so that he
can only aﬀord to loose a certain number of times), then his expected
winnings are zero (regardless of how much money he starts with).
TEAM LinG

6.11 Notes
147
Hint: in this case, you can model the gambler’s winnings as a random
variable on a ﬁnite probability distribution.
6.11 Notes
Our Chernoﬀbound (Theorem 6.13) is one of a number of diﬀerent types of
bounds that appear in the literature under the rubric of “Chernoﬀbound.”
Universal and pairwise independent hash functions, with applications to
hash tables and message authentication codes, were introduced by Carter
and Wegman [25, 99].
The leftover hash lemma (Theorem 6.21) was originally stated and proved
by Impagliazzo, Levin, and Luby [46], who use it to obtain an important
result in the theory of cryptography. Our proof of the leftover hash lemma is
loosely based on one by Impagliazzo and Zuckermann [47], who also present
further applications.
TEAM LinG

7
Probabilistic algorithms
It is sometimes useful to endow our algorithms with the ability to generate
random numbers. To simplify matters, we only consider algorithms that
generate random bits. Where such random bits actually come from will not
be of great concern to us here. In a practical implementation, one would
use a pseudo-random bit generator, which should produce bits that “for
all practical purposes” are “as good as random.”
While there is a well-
developed theory of pseudo-random bit generation (some of which builds on
the ideas in §6.9), we will not delve into this here. Moreover, the pseudo-
random bit generators used in practice are not based on this general theory,
and are much more ad hoc in design. So, although we will present a rigorous
formal theory of probabilistic algorithms, the application of this theory to
practice is ultimately a bit heuristic.
7.1 Basic deﬁnitions
Formally speaking, we will add a new type of instruction to our random
access machine (described in §3.2):
random bit This type of instruction is of the form α ←RANDOM, where
α takes the same form as in arithmetic instructions. Execution of
this type of instruction assigns to α a value sampled from the uniform
distribution on {0, 1}, independently from the execution of all other
random-bit instructions.
In describing algorithms at a high level, we shall write “b ←R {0, 1}” to
denote the assignment of a random bit to the variable b, and “s ←R {0, 1}×ℓ”
to denote the assignment of a random bit string of length ℓto the variable s.
In describing the behavior of such a probabilistic or randomized algo-
rithm A, for any input x, we view its running time and output as random
148
TEAM LinG

7.1 Basic deﬁnitions
149
variables, denoted TA(x) and A(x), respectively. The expected running
time of A on input x is deﬁned as the expected value E[TA(x)] of the ran-
dom variable TA(x). Note that in deﬁning expected running time, we are
not considering the input to be drawn from some probability distribution.
One could, of course, deﬁne such a notion; however, it is not always easy to
come up with a distribution on the input space that reasonably models a
particular real-world situation. We do not pursue this issue any more here.
We say that a probabilistic algorithm A runs in expected polynomial
time if there exist constants c, d such that for all n ≥0 and all inputs x
of length n, we have E[TA(x)] ≤nc + d.
We say that A runs in strict
polynomial time if there exist constants c, d such that for all n and all
inputs x of length n, A always halts on input x within nc + d, regardless of
its random choices.
Deﬁning the distributions of TA(x) and A(x) is a bit tricky. Things are
quite straightforward if A always halts on input x after a ﬁnite number
of steps, regardless of the outcomes of its random choices: in this case,
we can naturally view TA(x) and A(x) as random variables on a uniform
distribution over bit strings of some particular length—such a random bit
string may be used as the source of random bits for the algorithm. However,
if there is no a priori bound on the number of steps, things become more
complicated: think of an algorithm that generates random bits one at a time
until it generates, say, a 1 bit—just as in Example 6.29, we do not attempt
to model this as a probability distribution on the uncountable set of inﬁnite
bit strings, but rather, we directly deﬁne an appropriate discrete probability
distribution that models the execution of A on input x.
7.1.1 Deﬁning the probability distribution
A warning to the reader: the remainder of this section is a bit technical,
and you might want to skip ahead to §7.2 on ﬁrst reading, if you are willing
to trust your intuition regarding probabilistic algorithms.
To motivate our deﬁnition, which may at ﬁrst seem a bit strange, consider
again Example 6.29. We could view the sample space in that example to
be the set of all bit strings consisting of zero or more 0 bits, followed by a
single 1 bit, and to each such bit string σ of this special form, we assign the
probability 2−|σ|, where |σ| denotes the length of σ. The “random experi-
ment” we have in mind is to generate random bits one at a time until one of
these special “halting” strings is generated. In developing the deﬁnition of
the probability distribution for a probabilistic algorithm, we simply consider
TEAM LinG

150
Probabilistic algorithms
more general sets of “halting” strings, determined by the algorithm and its
input.
To simplify matters, we assume that the machine produces a stream of
random bits, one with every instruction executed, and if the instruction
happens to be a random-bit instruction, then this is the bit it uses. For
any bit string σ, we can run A on input x for up to |σ| steps, using σ for
the stream of random bits, and observe the behavior of the algorithm. The
reader may wish to visualize σ as a ﬁnite path in an inﬁnite binary tree,
where we start at the root, branching to the left if the next bit in σ is a 0
bit, and branching to the right if the next bit in σ is a 1 bit. In this context,
we call σ an execution path. Some further terminology will be helpful:
• If A halts in at most |σ| steps, then we call σ a complete execution
path;
• if A halts in exactly |σ| steps, then we call σ an exact execution
path;
• if A does not halt in fewer than |σ| steps, then we call σ a partial
execution path.
The sample space S of the probability distribution associated with A on
input x consists of all exact execution paths. Clearly, S is preﬁx free; that
is, no string in S is a proper preﬁx of another.
Theorem 7.1. If S is a preﬁx-free set of bit strings, then 
σ∈S 2−|σ| ≤1.
Proof. We ﬁrst claim that the theorem holds for any ﬁnite preﬁx-free set S.
We may assume that S is non-empty, since otherwise, the claim is trivial.
We prove the claim by induction on the sum of the lengths of the elements
of S. The base case is when S contains just the empty string, in which case
the claim is clear. If S contains non-empty strings, let τ be a string in S of
maximal length, and let τ ′ be the preﬁx of length |τ| −1 of τ. Now remove
from S all strings which have τ ′ as a preﬁx (there are either one or two
such strings), and add to S the string τ ′. It is easy to see (verify) that the
resulting set S′ is also preﬁx-free, and that

σ∈S
2−|σ| ≤

σ∈S′
2−|σ|.
The claim now follows by induction.
For the general case, let σ1, σ2, . . . be a particular enumeration of S, and
consider the partial sums Si = i
j=1 2−|σj| for i = 1, 2, . . . . From the above
claim, each of these partial sums is at most 1, from which it follows that
limi→∞Si ≤1. 2
TEAM LinG

7.1 Basic deﬁnitions
151
From the above theorem, if S is the sample space associated with algo-
rithm A on input x, we have
S :=

σ∈S
2−|σ| ≤1.
Assume that S = 1. Then we say that A halts with probability 1 on
input x, and we deﬁne the distribution DA,x associated with A on input
x to be the distribution on S that assigns the probability 2−|σ| to each bit
string σ ∈S. We also deﬁne TA(x) and A(x) as random variables on the
distribution DA,x in the natural way: for each σ ∈S, we deﬁne TA(x) to be
|σ| and A(x) to be the output produced by A on input x using σ to drive
its execution.
All of the above deﬁnitions assumed that A halts with probability 1 on
input x, and indeed, we shall only be interested in algorithms that halt with
probability 1 on all inputs. However, to analyze a given algorithm, we still
have to prove that it halts with probability 1 on all inputs before we can use
these deﬁnitions and bring to bear all the tools of discrete probability theory.
To this end, it is helpful to study various ﬁnite probability distributions
associated with the execution of A on input x. For every integer k ≥0, let
us consider the uniform distribution on bit strings of length k, and for each
j = 0, . . . , k, deﬁne H(k)
j
to be the event that such a random k-bit string
causes A on input x to halt within j steps.
A couple of observations are in order. First, if S is the set of all exact
execution paths for A on input x, then we have (verify)
P[H(k)
j ] =

σ∈S
|σ|≤j
2−|σ|.
From this it follows that for all non-negative integers j, k, k′ with j ≤
min{k, k′}, we have
P[H(k)
j ] = P[H(k′)
j
].
Deﬁning Hk := P[H(k)
k ], it also follows that the sequence {Hk}k≥0 is non-
decreasing and bounded above by 1, and that A halts with probability 1 on
input x if and only if
lim
k→∞Hk = 1.
A simple necessary condition for halting with probability 1 on a given
input is that for all partial execution paths, there exists some extension that
is a complete execution path. Intuitively, if this does not hold, then with
TEAM LinG

152
Probabilistic algorithms
some non-zero probability, the algorithm falls into an inﬁnite loop. More
formally, if there exists a partial execution path of length j that cannot be
extended to a complete execution path, then for all k ≥j we have
Hk ≤1 −2−j.
This does not, however, guarantee halting with probability 1.
A simple
suﬃcient condition is the following:
There exists a bound ℓ(possibly depending on the input) such
that for every partial execution path σ, there exists a complete
execution path that extends σ and whose length at most |σ|+ℓ.
To see why this condition implies that A halts with probability 1, observe
that if A runs for kℓsteps without halting, then the probability that it does
not halt within (k +1)ℓsteps is at most 1−2−ℓ. More formally, let us deﬁne
Hk := 1 −Hk, and note that for all k ≥0, we have
H(k+1)ℓ= P[H
((k+1)ℓ)
(k+1)ℓ
| H
((k+1)ℓ)
kℓ
] · P[H
((k+1)ℓ)
kℓ
]
≤(1 −2−ℓ)P[H
((k+1)ℓ)
kℓ
]
= (1 −2−ℓ)Hkℓ,
and hence (by an induction argument on k), we have
Hkℓ≤(1 −2−ℓ)k,
from which it follows that
lim
k→∞Hk = 1.
It is usually fairly straightforward to verify this property for a particular
algorithm “by inspection.”
Example 7.1. Consider the following algorithm:
repeat
b ←R {0, 1}
until b = 1
Since every loop is only a constant number of instructions, and since there
is one chance to terminate with every loop iteration, the algorithm halts with
probability 1. 2
Example 7.2. Consider the following algorithm:
TEAM LinG

7.1 Basic deﬁnitions
153
i ←0
repeat
i ←i + 1
s ←R {0, 1}×i
until s = 0×i
For positive integer n, consider the probability pn of executing at least
n loop iterations (each pn is deﬁned using an appropriate ﬁnite probability
distribution). We have
pn =
n−1

i=1
(1 −2−i) ≥
n−1

i=1
e−2−i+1 = e−Pn−2
i=0 2−i ≥e−2,
where we have made use of the estimate (iii) in §A1. As pn does not tend
to zero as n →∞, we may conclude that the algorithm does not halt with
probability 1.
Note that every partial execution path can be extended to a complete
execution path, but the length of the extension is not bounded. 2
The following three exercises develop tools which simplify the analysis of
probabilistic algorithms.
Exercise 7.1. Consider a probabilistic algorithm A that halts with prob-
ability 1 on input x, and consider the probability distribution DA,x on the
set S of exact execution paths. Let τ be a ﬁxed, partial execution path, and
let B ⊆S be the event that consists of all exact execution paths that extend
τ. Show that P[B] = 2−|τ|.
Exercise 7.2. Consider a probabilistic algorithm A that halts with prob-
ability 1 on input x, and consider the probability distribution DA,x on the
set S of exact execution paths. For a bit string σ and an integer k ≥0, let
{σ}k denote the value of σ truncated to the ﬁrst k bits. Suppose that B ⊆S
is an event of the form
B = {σ ∈S : φ({σ}k)}
for some predicate φ and some integer k ≥0. Intuitively, this means that
B is completely determined by the ﬁrst k bits of the execution path. Now
consider the uniform distribution on {0, 1}×k. Let us deﬁne an event B′ in
this distribution as follows. For σ ∈{0, 1}×k, let us run A on input x using
the execution path σ for k steps or until A halts (whichever comes ﬁrst).
If the number of steps executed was t (where t ≤k), then we put σ in B′
if and only if φ({σ}t). Show that the probability that the event B occurs
TEAM LinG

154
Probabilistic algorithms
(with respect to the distribution DA,x) is the same as the probability that
B′ occurs (with respect to the uniform distribution on {0, 1}×k). Hint: use
Exercise 7.1.
The above exercise is very useful in simplifying the analysis of probabilistic
algorithms. One can typically reduce the analysis of some event of interest
into the analysis of a collection of events, each of which is determined by
the ﬁrst k bits of the execution path for some ﬁxed k. The probability of an
event that is determined by the ﬁrst k bits of the execution path may then
be calculated by analyzing the behavior of the algorithm on a random k-bit
execution path.
Exercise 7.3. Suppose algorithm A calls algorithm B as a subroutine. In
the probability distribution DA,x, consider a particular partial execution
path τ that drives A to a point where A invokes algorithm B with a partic-
ular input y (determined by x and τ). Consider the conditional probability
distribution given that τ is a preﬁx of A’s actual execution path. We can
deﬁne a random variable X on this conditional distribution whose value is
the subpath traced out by the invocation of subroutine B. Show that the
distribution of X is the same as DB,y. Hint: use Exercise 7.1.
The above exercise is also very useful in simplifying the analysis of prob-
abilistic algorithms, in that it allows us to analyze a subroutine in isolation,
and use the results in the analysis of an algorithm that calls that subroutine.
Exercise 7.4. Let A be a probabilistic algorithm, and for an input x and
integer k ≥0, consider the experiment in which we choose a random exe-
cution path of length k, and run A on input x for up to k steps using the
selected execution path. If A halts within k steps, we deﬁne Ak(x) to be
the output produced by A, and TAk(x) to be the actual number of steps
executed by A; otherwise, we deﬁne Ak(x) to be the distinguished value
“⊥” and TAk(x) to be k.
(a) Show that if A halts with probability 1 on input x, then for all possible
outputs y,
P[A(x) = y] = lim
k→∞P[Ak(x) = y].
(b) Show that if A halts with probability 1 on input x, then
E[TA(x)] = lim
k→∞E[TAk(x)].
Exercise 7.5. One can generalize the notion of a discrete, probabilistic
process, as follows. Let Γ be a ﬁnite or countably inﬁnite set. Let f be a
TEAM LinG

7.2 Approximation of functions
155
function mapping sequences of one or more elements of Γ to [0, 1], such that
the following property holds:
for
all
ﬁnite
sequences
(γ1, . . . , γi−1),
where
i
≥
1,
f(γ1, . . . , γi−1, γ) is non-zero for at most a ﬁnite number of
γ ∈Γ, and

γ∈Γ
f(γ1, . . . , γi−1, γ) = 1.
Now consider any preﬁx-free set S of ﬁnite sequences of elements of Γ. For
σ = (γ1, . . . , γn) ∈S, deﬁne
P[σ] :=
n

i=1
f(γ1, . . . , γi).
Show that 
σ∈S P[σ] ≤1, and hence we may deﬁne a probability distribu-
tion on S using the probability function P[·] if this sum is 1. The intuition
is that we are modeling a process in which we start out in the “empty” con-
ﬁguration; at each step, if we are in conﬁguration (γ1, . . . , γi−1), we halt if
this is a “halting” conﬁguration, that is, an element of S, and otherwise, we
move to conﬁguration (γ1, . . . , γi−1, γ) with probability f(γ1, . . . , γi−1, γ).
7.2 Approximation of functions
Suppose f is a function mapping bit strings to bit strings. We may have
an algorithm A that approximately computes f in the following sense:
there exists a constant ϵ, with 0 ≤ϵ < 1/2, such that for all inputs x,
P[A(x) = f(x)] ≥1 −ϵ. The value ϵ is a bound on the error probability,
which is deﬁned as P[A(x) ̸= f(x)].
7.2.1 Reducing the error probability
There is a standard “trick” by which one can make the error probability very
small; namely, run A on input x some number, say t, times, and take the
majority output as the answer. Using the Chernoﬀbound (Theorem 6.13),
the error probability for the iterated version of A is bounded by exp[−(1/2−
ϵ)2t/2], and so the error probability decreases exponentially with the number
of iterations.
This bound is derived as follows.
For i = 1, . . . , t, let Xi
be a random variable representing the outcome of the ith iteration of A;
more precisely, Xi = 1 if A(x) ̸= f(x) on the ith iteration, and Xi = 0
otherwise. Let ϵx be the probability that A(x) ̸= f(x). The probability that
the majority output is wrong is equal to the probability that the sample
TEAM LinG

156
Probabilistic algorithms
mean of X1, . . . , Xt exceeds the mean ϵx by at least 1/2 −ϵx. Part (i) of
Theorem 6.13 says that this occurs with probability at most
exp
−(1/2 −ϵx)2t
2(1 −ϵx)

≤exp
−(1/2 −ϵ)2t
2

.
7.2.2 Strict polynomial time
If we have an algorithm A that runs in expected polynomial time, and which
approximately computes a function f, then we can easily turn it into a new
algorithm A′ that runs in strict polynomial time, and also approximates
f, as follows. Suppose that ϵ < 1/2 is a bound on the error probability,
and T(n) is a polynomial bound on the expected running time for inputs of
length n. Then A′ simply runs A for at most tT(n) steps, where t is any
constant chosen so that ϵ + 1/t < 1/2—if A does not halt within this time
bound, then A′ simply halts with an arbitrary output. The probability that
A′ errs is at most the probability that A errs plus the probability that A
runs for more than tT(n) steps. By Markov’s inequality (Theorem 6.11),
the latter probability is at most 1/t, and hence A′ approximates f as well,
but with an error probability bounded by ϵ + 1/t.
7.2.3 Language recognition
An important special case of approximately computing a function is when
the output of the function f is either 0 or 1 (or equivalently, false or true).
In this case, f may be viewed as the characteristic function of the language
L := {x : f(x) = 1}. (It is the tradition of computational complexity theory
to call sets of bit strings “languages.”) There are several “ﬂavors” of proba-
bilistic algorithms for approximately computing the characteristic function
f of a language L that are traditionally considered — for the purposes of
these deﬁnitions, we may restrict ourselves to algorithms that output either
0 or 1:
• We call a probabilistic, expected polynomial-time algorithm an At-
lantic City algorithm for recognizing L if it approximately com-
putes f with error probability bounded by a constant ϵ < 1/2.
• We call a probabilistic, expected polynomial-time algorithm A a
Monte Carlo algorithm for recognizing L if for some constant
δ > 0, we have:
– for any x ∈L, we have P[A(x) = 1] ≥δ, and
– for any x /∈L, we have P[A(x) = 1] = 0.
TEAM LinG

7.2 Approximation of functions
157
• We call a probabilistic, expected polynomial-time algorithm a Las
Vegas algorithm for recognizing L if it computes f correctly on all
inputs x.
One also says an Atlantic City algorithm has two-sided error, a Monte
Carlo algorithm has one-sided error, and a Las Vegas algorithm has zero-
sided error.
Exercise 7.6. Show that any language recognized by a Las Vegas algorithm
is also recognized by a Monte Carlo algorithm, and that any language rec-
ognized by a Monte Carlo algorithm is also recognized by an Atlantic City
algorithm.
Exercise 7.7. Show that if L is recognized by an Atlantic City algorithm
that runs in expected polynomial time, then it is recognized by an Atlantic
City algorithm that runs in strict polynomial time, and whose error proba-
bility is at most 2−n on inputs of length n.
Exercise 7.8. Show that if L is recognized by a Monte Carlo algorithm that
runs in expected polynomial time, then it is recognized by a Monte Carlo
algorithm that runs in strict polynomial time, and whose error probability
is at most 2−n on inputs of length n.
Exercise 7.9. Show that a language is recognized by a Las Vegas algo-
rithm iﬀthe language and its complement are recognized by Monte Carlo
algorithms.
Exercise 7.10. Show that if L is recognized by a Las Vegas algorithm that
runs in strict polynomial time, then L may be recognized in deterministic
polynomial time.
Exercise 7.11. Suppose that for a given language L, there exists a prob-
abilistic algorithm A that runs in expected polynomial time, and always
outputs either 0 or 1. Further suppose that for some constants α and c,
where
• α is a rational number with 0 ≤α < 1, and
• c is a positive integer,
and for all suﬃciently large n, and all inputs x of length n, we have
• if x /∈L, then P[A(x) = 1] ≤α, and
• if x ∈L, then P[A(x) = 1] ≥α + 1/nc.
(a) Show that there exists an Atlantic City algorithm for L.
(b) Show that if α = 0, then there exists a Monte Carlo algorithm for L.
TEAM LinG

158
Probabilistic algorithms
7.3 Flipping a coin until a head appears
In this and subsequent sections of this chapter, we discuss a number of
speciﬁc probabilistic algorithms.
Let us begin with the following simple algorithm (which was already pre-
sented in Example 7.1) that essentially ﬂips a coin until a head appears:
repeat
b ←R {0, 1}
until b = 1
Let X be a random variable that represents the number of loop iterations
made by the algorithm. It should be fairly clear that X has a geometric
distribution, where the associated probability of success is 1/2 (see Exam-
ple 6.30). However, let us derive this fact from more basic principles. Deﬁne
random variables B1, B2, . . . , where Bi represents the value of the bit as-
signed to b in the ith loop iteration, if X ≥i, and ⋆otherwise. Clearly,
exactly one Bi will take the value 1, in which case X takes the value i.
Evidently, for each i ≥1, if the algorithm actually enters the ith loop
iteration, then Bi is uniformly distributed over {0, 1}, and otherwise, Bi = ⋆.
That is:
P[Bi = 0 | X ≥i] = 1/2,
P[Bi = 1 | X ≥i] = 1/2,
P[Bi = ⋆| X < i] = 1.
From this, we see that
P[X ≥1] = 1,
P[X ≥2] = P[B1 = 0 | X ≥1]P[X ≥1] = 1/2,
P[X ≥3] = P[B2 = 0 | X ≥2]P[X ≥2] = (1/2)(1/2) = 1/4,
and by induction on i, we see that
P[X ≥i] = P[Bi−1 = 0 | X ≥i −1]P[X ≥i −1] = (1/2)(1/2i−2) = 1/2i−1,
from which it follows (see Exercise 6.54) that X has a geometric distribution
with associated success probability 1/2.
Now consider the expected value E[X]. By the discussion in Example 6.35,
we have E[X] = 2. If Y denotes the total running time of the algorithm,
then Y ≤cX for some constant c, and hence
E[Y ] ≤cE[X] = 2c,
and we conclude that the expected running time of the algorithm is a con-
stant, the exact value of which depends on the details of the implementation.
TEAM LinG

7.4 Generating a random number from a given interval
159
[Readers who skipped §7.1.1 may also want to skip this paragraph.]
As was argued in Example 7.1, the above algorithm halts with prob-
ability 1.
To make the above argument completely rigorous, we
should formally justify that claim that the conditional distribution
of Bi, given that X ≥i, is uniform over {0, 1}. We do not wish to
assume that the values of the Bi are located at pre-determined posi-
tions of the execution path; rather, we shall employ a more generally
applicable technique. For any i ≥1, we shall condition on a partic-
ular partial execution path τ that drives the algorithm to the point
where it is just about to sample the bit Bi, and show that in this
conditional probability distribution, Bi is uniformly distributed over
{0, 1}. To do this rigorously in our formal framework, let us deﬁne
the event Aτ to be the event that τ is a preﬁx of the execution path.
If |τ| = ℓ, then the events Aτ, Aτ ∧(Bi = 0), and Aτ ∧(Bi = 1) are
determined by the ﬁrst ℓ+1 bits of the execution path. We can then
consider corresponding events in a probabilistic experiment wherein
we observe the behavior of the algorithm on a random (ℓ+1)-bit ex-
ecution path (see Exercise 7.2). In the latter experiment, it is clear
that the conditional probability distribution of Bi, given that the
ﬁrst ℓbits of the actual execution path σ agree with τ, is uniform
over {0, 1}, and thus, the same holds in the original probability dis-
tribution. Since this holds for all relevant τ, it follows (by a discrete
version of Exercise 6.13) that it holds conditioned on X ≥i.
We have analyzed the above algorithm in excruciating detail.
As we
proceed, many of these details will be suppressed, as they can all be handled
by very similar (and completely routine) arguments.
7.4 Generating a random number from a given interval
Suppose we want to generate a number n uniformly at random from the
interval {0, . . . , M −1}, for a given integer M ≥1.
If M is a power of 2, say M = 2k, then we can do this directly as follows:
generate a random k-bit string s, and convert s to the integer I(s) whose
base-2 representation is s; that is, if s = bk−1bk−2 · · · b0, where the bi are
bits, then
I(s) :=
k−1

i=0
bi2i.
In the general case, we do not have a direct way to do this, since we can
only directly generate random bits. However, suppose that M is a k-bit
number, so that 2k−1 ≤M < 2k. Then the following algorithm does the
job:
TEAM LinG

160
Probabilistic algorithms
Algorithm RN:
repeat
s ←R {0, 1}×k
n ←I(s)
until n < M
output n
Let X denote the number of loop iterations of this algorithm, Y its running
time, and N its output.
In every loop iteration, n is uniformly distributed over {0, . . . , 2k−1}, and
the event n < M occurs with probability M/2k; moreover, conditioning on
the latter event, n is uniformly distributed over {0, . . . , M −1}. It follows
that X has a geometric distribution with an associated success probability
p := M/2k ≥1/2, and that N is uniformly distributed over {0, . . . , M −1}.
We have E[X] = 1/p ≤2 (see Example 6.35) and Y ≤ckX for some
implementation-dependent constant c, from which it follows that
E[Y ] ≤ckE[X] ≤2ck.
Thus, the expected running time of Algorithm RN is O(k).
Hopefully, the above argument is clear and convincing. However, as in
the previous section, we can derive these results from more basic principles.
Deﬁne random variables N1, N2, . . . , where Ni represents the value of n in
the ith loop iteration, if X ≥i, and ⋆otherwise.
Evidently, for each i ≥1, if the algorithm actually enters the ith loop
iteration, then Ni is uniformly distributed over {0, . . . , 2k−1}, and otherwise,
Ni = ⋆. That is:
P[Ni = j | X ≥i] = 1/2k (j = 0, . . . , 2k −1),
P[Ni = ⋆| X < i] = 1.
From this fact, we can derive all of the above results.
As for the distribution of X, it follows from a simple induction argument
that P[X ≥i] = qi−1, where q := 1 −p; indeed, P[X ≥1] = 1, and for i ≥2,
we have
P[X ≥i] = P[Ni−1 ≥M | X ≥i −1]P[X ≥i −1] = q · qi−2 = qi−1.
It follows that X has a geometric distribution with associated success prob-
ability p (see Exercise 6.54).
As for the distribution of N, by (a discrete version of) Exercise 6.13, it
suﬃces to show that for all i ≥1, the conditional distribution of N given that
TEAM LinG

7.4 Generating a random number from a given interval
161
X = i is uniform on {0, . . . , M −1}. Observe that for any j = 0, . . . , M −1,
we have
P[N = j | X = i] = P[N = j ∧X = i]
P[X = i]
= P[Ni = j ∧X ≥i]
P[Ni < M ∧X ≥i]
= P[Ni = j | X ≥i]P[X ≥i]
P[Ni < M | X ≥i]P[X ≥i] = 1/2k
M/2k
= 1/M.
[Readers who skipped §7.1.1 may also want to skip this paragraph.]
To make the above argument completely rigorous, we should ﬁrst
show that the algorithm halts with probability 1, and then show
that the conditional distribution of Ni, given that X ≥i, is indeed
uniform on {0, . . . , 2k −1}, as claimed above. That the algorithm
halts with probability 1 follows from the fact that in every loop iter-
ation, there is at least one choice of s that will cause the algorithm
to halt. To analyze the conditional distribution on Ni, one considers
various conditional distributions, conditioning on particular partial
execution paths τ that bring the computation just to the beginning
of the ith loop iteration; for any particular such τ, the ith loop iter-
ation will terminate in at most ℓ:= |τ|+ck steps, for some constant
c. Therefore, the conditional distribution of Ni, given the partial ex-
ecution path τ, can be analyzed by considering the execution of the
algorithm on a random ℓ-bit execution path (see Exercise 7.2). It is
then clear that the conditional distribution of Ni given the partial
execution path τ is uniform over {0, . . . , 2k −1}, and since this holds
for all relevant τ, it follows (by a discrete version of Exercise 6.13)
that the conditional distribution of Ni, given that the ith loop is
entered, is uniform over {0, . . . , 2k −1}.
Of course, by adding an appropriate value to the output of Algorithm
RN, we can generate random numbers uniformly in an interval {A, . . . , B},
for given A and B. In what follows, we shall denote the execution of this
algorithm as
n ←R {A, . . . , B}.
We also mention the following alternative approach to generating a ran-
dom number from an interval.
Given a positive k-bit integer M, and a
parameter t > 0, we do the following:
Algorithm RN′:
s ←R {0, 1}×(k+t)
n ←I(s) mod M
output n
Compared with Algorithm RN, Algorithm RN′ has the advantage that
TEAM LinG

162
Probabilistic algorithms
there are no loops — it halts in a bounded number of steps; however, it
has the disadvantage that its output is not uniformly distributed over the
interval {0, . . . , M −1}. Nevertheless, the statistical distance between its
output distribution and the uniform distribution on {0, . . . , M −1} is at
most 2−t (see Example 6.27 in §6.8). Thus, by choosing t suitably large, we
can make the output distribution “as good as uniform” for most practical
purposes.
Exercise 7.12. Prove that no probabilistic algorithm that always halts in
a bounded number of steps can have an output distribution that is uniform
on {0, . . . , M −1}, unless M is a power of 2.
Exercise 7.13. Let A1 and A2 be probabilistic algorithms such that, for
any input x, the random variables A1(x) and A2(x) take on one of a ﬁnite
number of values, and let δx be the statistical distance between A1(x) and
A2(x). Let B be any probabilistic algorithm that always outputs 0 or 1. For
for i = 1, 2, let Ci be the algorithm that given an input x, ﬁrst runs Ai on
that input, obtaining a value y, then it runs B on input y, obtaining a value
z, which it then outputs. Show that |P[C1(x) = 1] −P[C2(x) = 1]| ≤δx.
7.5 Generating a random prime
Suppose we are given an integer M ≥2, and want to generate a random
prime between 2 and M. One way to proceed is simply to generate random
numbers until we get a prime. This idea will work, assuming the existence
of an eﬃcient algorithm IsPrime that determines whether or not a given
integer n > 1 is prime.
Now, the most naive method of testing if n is prime is to see if any of the
numbers between 2 and n −1 divide n. Of course, one can be slightly more
clever, and only perform this divisibility check for prime numbers between 2
and √n (see Exercise 1.1). Nevertheless, such an approach does not give rise
to a polynomial-time algorithm. Indeed, the design and analysis of eﬃcient
primality tests has been an active research area for many years. There is, in
fact, a deterministic, polynomial-time algorithm for testing primality, which
we shall discuss later, in Chapter 22. For the moment, we shall just assume
we have such an algorithm, and use it as a “black box.”
Our algorithm to generate a random prime between 2 and M runs as
follows:
TEAM LinG

7.5 Generating a random prime
163
Algorithm RP:
repeat
n ←R {2, . . . , M}
until IsPrime(n)
output n
We now wish to analyze the running time and output distribution of
Algorithm RP on input M. Let k := len(M).
First, consider a single iteration of the main loop of Algorithm RP, viewed
as a stand-alone probabilistic experiment. For any ﬁxed prime p between
2 and M, the probability that the variable n takes the value p is precisely
1/(M −1). Thus, every prime is equally likely, and the probability that n
is a prime is precisely π(M)/(M −1).
Let us also consider the expected running time µ of a single loop iteration.
To this end, deﬁne Wn to be the running time of algorithm IsPrime on input
n. Also, deﬁne
W ′
M :=
1
M −1
M

n=2
Wn.
That is, W ′
M is the average value of Wn, for a random choice of n ∈
{2, . . . , M}. Thus, µ is equal to W ′
M, plus the expected running time of
Algorithm RN, which is O(k), plus any other small overhead, which is also
O(k). So we have µ ≤W ′
M + O(k), and assuming that W ′
M = Ω(k), which
is perfectly reasonable, we have µ = O(W ′
M).
Next, let us consider the behavior of Algorithm RP as a whole. From the
above discussion, it follows that when this algorithm terminates, its output
will be uniformly distributed over the set of all primes between 2 and M. If
T denotes the number of loop iterations performed by the algorithm, then
E[T] = (M −1)/π(M), which by Chebyshev’s theorem (Theorem 5.1) is
Θ(k).
So we have bounded the expected number of loop iterations. We now
want to bound the expected overall running time. For i ≥1, let Xi denote
the amount of time (possibly zero) spent during the ith loop iteration of the
algorithm, so that X := 
i≥1 Xi is the total running time of Algorithm RP.
Note that
E[Xi] = E[Xi | T ≥i]P[T ≥i] + E[Xi | T < i]P[T < i]
= E[Xi | T ≥i]P[T ≥i]
= µP[T ≥i],
TEAM LinG

164
Probabilistic algorithms
because Xi = 0 when T < i and E[Xi | T ≥i] is by deﬁnition equal to µ.
Then we have
E[X] =

i≥1
E[Xi] = µ

i≥1
P[T ≥i] = µE[T] = O(kW ′
M).
7.5.1 Using a probabilistic primality test
In the above analysis, we assumed that IsPrime was a deterministic,
polynomial-time algorithm. While such an algorithm exists, there are in
fact simpler and more eﬃcient algorithms that are probabilistic. We shall
discuss such an algorithm in greater depth later, in Chapter 10. This al-
gorithm (like several other algorithms for primality testing) has one-sided
error in the following sense: if the input n is prime, then the algorithm
always outputs true; otherwise, if n is composite, the output may be true
or false, but the probability that the output is true is at most c, where
c < 1 is a constant. In the terminology of §7.2, such an algorithm is essen-
tially a Monte Carlo algorithm for the language of composite numbers. If
we want to reduce the error probability for composite inputs to some very
small value ϵ, we can iterate the algorithm t times, with t chosen so that
ct ≤ϵ, outputting true if all iterations output true, and outputting false
otherwise. This yields an algorithm for primality testing that makes errors
only on composite inputs, and then only with probability at most ϵ.
Let us analyze the behavior of Algorithm RP under the assumption that
IsPrime is implemented by a probabilistic algorithm (such as described
in the previous paragraph) with an error probability for composite inputs
bounded by ϵ. Let us deﬁne Wn to be the expected running time of IsPrime
on input n, and as before, we deﬁne
W ′
M :=
1
M −1
M

n=2
Wn.
Thus, W ′
M is the expected running time of algorithm IsPrime, where the
average is taken with respect to randomly chosen n and the random choices
of the algorithm itself.
Consider a single loop iteration of Algorithm RP. For any ﬁxed prime p
between 2 and M, the probability that n takes the value p is 1/(M −1).
Thus, if the algorithm halts with a prime, every prime is equally likely. Now,
the algorithm will halt if n is prime, or if n is composite and the primality
test makes a mistake; therefore, the the probability that it halts at all is at
least π(M)/(M −1). So we see that the expected number of loop iterations
TEAM LinG

7.5 Generating a random prime
165
should be no more than in the case where we use a deterministic primality
test. Using the same argument as was used before to estimate the expected
total running time of Algorithm RP, we ﬁnd that this is O(kW ′
M).
As for the probability that Algorithm RP mistakenly outputs a composite,
one might be tempted to say that this probability is at most ϵ, the probability
that IsPrime makes a mistake. However, in drawing such a conclusion, we
would be committing the fallacy of Example 6.12—to correctly analyze the
probability that Algorithm RP mistakenly outputs a composite, one must
take into account the rate of incidence of the “primality disease,” as well as
the error rate of the test for this disease.
Let us be a bit more precise. Again, consider the probability distribution
deﬁned by a single loop iteration, and let A be the event that IsPrime
outputs true, and B the event that n is composite.
Let β := P[B] and
α := P[A | B]. First, observe that, by deﬁnition, α ≤ϵ. Now, the probability
δ that the algorithm halts and outputs a composite in this loop iteration is
δ = P[A ∧B] = αβ.
The probability δ′ that the algorithm halts and outputs either a prime or
composite is
δ′ = P[A] = P[A ∧B] + P[A ∧B] = P[A ∧B] + P[B] = αβ + (1 −β).
Now consider the behavior of Algorithm RP as a whole. With T being
the number of loop iterations as before, we have
E[T] = 1
δ′ =
1
αβ + (1 −β),
(7.1)
and hence
E[T] ≤
1
(1 −β) = M −1
π(M) = O(k).
Let us now consider the probability γ that the output of Algorithm RP
is composite. For i ≥1, let Ci be the event that the algorithm halts and
outputs a composite number in the ith loop iteration. The events Ci are
pairwise disjoint, and moreover,
P[Ci] = P[Ci ∧T ≥i] = P[Ci | T ≥i]P[T ≥i] = δP[T ≥i].
So we have
γ =

i≥1
P[Ci] =

i≥1
δP[T ≥i] = δE[T] =
αβ
αβ + (1 −β),
(7.2)
TEAM LinG

166
Probabilistic algorithms
and hence
γ ≤
α
(1 −β) ≤
ϵ
(1 −β) = ϵM −1
π(M) = O(kϵ).
Another way of analyzing the output distribution of Algorithm RP is to
consider its statistical distance ∆from the uniform distribution on the set of
primes between 2 and M. As we have already argued, every prime between
2 and M is equally likely to be output, and in particular, any ﬁxed prime p
is output with probability at most 1/π(M). It follows from Theorem 6.15
that ∆= γ.
7.5.2 Generating a random k-bit prime
Instead of generating a random prime between 2 and M, we may instead
want to generate a random k-bit prime, that is, a prime between 2k−1 and
2k −1. Bertrand’s postulate (Theorem 5.7) tells us that there exist such
primes for every k ≥2, and that in fact, there are Ω(2k/k) such primes.
Because of this, we can modify Algorithm RP, so that each candidate n
is chosen at random from the interval {2k−1, . . . , 2k −1}, and all of the
results of this section carry over essentially without change. In particular,
the expected number of trials until the algorithm halts is O(k), and if a
probabilistic primality test as in §7.5.1 is used, with an error probability of
ϵ, the probability that the output is not prime is O(kϵ).
Exercise 7.14. Design and analyze an eﬃcient probabilistic algorithm that
takes as input an integer M ≥2, and outputs a random element of Z∗
M.
Exercise 7.15. Suppose Algorithm RP is implemented using an imper-
fect random number generator, so that the statistical distance between the
output distribution of the random number generator and the uniform dis-
tribution on {2, . . . , M} is equal to δ (e.g., Algorithm RN′ in §7.4). Assume
that 2δ < π(M)/(M −1). Also, let λ denote the expected number of itera-
tions of the main loop of Algorithm RP, let ∆denote the statistical distance
between its output distribution and the uniform distribution on the primes
up to M, and let k := len(M).
(a) Assuming the primality test is deterministic, show that λ = O(k) and
∆= O(δk).
(b) Assuming the primality test is probabilistic, with one-sided error ϵ,
as in §7.5.1, show that λ = O(k) and ∆= O((δ + ϵ)k).
TEAM LinG

7.6 Generating a random non-increasing sequence
167
Exercise 7.16. Analyze Algorithm RP assuming that the primality test
is implemented by an “Atlantic City” algorithm with error probability at
most ϵ.
Exercise 7.17. Consider the following probabilistic algorithm that takes as
input a positive integer M:
S ←∅
repeat
n ←R {1, . . . , M}
S ←S ∪{n}
until |S| = M
Show that the expected number of iterations of the main loop is ∼M log M.
The following exercises assume the reader has studied §7.1.1.
Exercise 7.18. Consider the following algorithm (which takes no input):
j ←1
repeat
j ←j + 1
n ←R {0, . . . , j −1}
until n = 0
Show that this algorithm halts with probability 1, but that its expected
running time does not exist. (Compare this algorithm with the one in Ex-
ample 7.2, which does not even halt with probability 1.)
Exercise 7.19. Now consider the following modiﬁcation to the algorithm
in the previous exercise:
j ←2
repeat
j ←j + 1
n ←R {0, . . . , j −1}
until n = 0 or n = 1
Show that this algorithm halts with probability 1, and that its expected
running time exists (and is equal to some implementation-dependent con-
stant).
7.6 Generating a random non-increasing sequence
The following algorithm, Algorithm RS, will be used in the next section as
a fundamental subroutine in a beautiful algorithm (Algorithm RFN) that
TEAM LinG

168
Probabilistic algorithms
generates random numbers in factored form. Algorithm RS takes as input
an integer M ≥2, and runs as follows:
Algorithm RS:
n0 ←M
i ←0
repeat
i ←i + 1
ni ←R {1, . . . , ni−1}
until ni = 1
t ←i
Output (n1, . . . , nt)
We analyze ﬁrst the output distribution, and then the running time.
7.6.1 Analysis of the output distribution
Let N1, N2, . . . be random variables denoting the choices of n1, n2, . . . (for
completeness, deﬁne Ni := 1 if loop i is never entered).
A particular output of the algorithm is a non-increasing chain (n1, . . . , nt),
where n1 ≥n2 ≥· · · ≥nt−1 > nt = 1. For any such chain, we have
P[N1 = n1 ∧· · · ∧Nt = nt] = P[N1 = n1]P[N2 = n2 | N1 = n1] · · ·
P[Nt = nt | N1 = n1 ∧· · · ∧Nt−1 = nt−1]
= 1
M · 1
n1
· · · · ·
1
nt−1
.
(7.3)
This completely describes the output distribution, in the sense that we
have determined the probability with which each non-increasing chain ap-
pears as an output. However, there is another way to characterize the output
distribution that is signiﬁcantly more useful. For j = 2, . . . , M, deﬁne the
random variable Ej to be the number of occurrences of j among the Ni.
The Ej determine the Ni, and vice versa. Indeed, EM = eM, . . . , E2 = e2
iﬀthe output of the algorithm is the non-increasing chain
(M, . . . , M



eM times
, M −1, . . . , M −1



eM−1 times
, . . . , 2, . . . , 2
  
e2 times
, 1).
From (7.3), we can therefore directly compute
P[EM = eM ∧. . . ∧E2 = e2] = 1
M
M

j=2
1
jej .
(7.4)
TEAM LinG

7.6 Generating a random non-increasing sequence
169
Notice that we can write 1/M as a telescoping product:
1
M = M −1
M
· M −2
M −1 · · · · · 2
3 · 1
2 =
M

j=2
(1 −1/j),
so we can re-write (7.4) as
P[EM = eM ∧· · · ∧E2 = e2] =
M

j=2
j−ej(1 −1/j).
(7.5)
Notice that for j = 2, . . . , M,

ej≥0
j−ej(1 −1/j) = 1,
and so by (a discrete version of) Theorem 6.1, the variables Ej are mutually
independent, and for all j = 2, . . . , M and integers ej ≥0, we have
P[Ej = ej] = j−ej(1 −1/j).
(7.6)
In summary, we have shown that the variables Ej are mutually indepen-
dent, where for j = 2, . . . , M, the variable Ej+1 has a geometric distribution
with an associated success probability of 1 −1/j.
Another, perhaps more intuitive, analysis of the joint distribution of the
Ej runs as follows. Conditioning on the event EM = eM, . . . , Ej+1 = ej+1,
one sees that the value of Ej is the number of times the value j appears in
the sequence Ni, Ni+1, . . . , where i = eM + · · · + ej+1 + 1; moreover, in this
conditional probability distribution, it is not too hard to convince oneself
that Ni is uniformly distributed over {1, . . . , j}. Hence the probability that
Ej = ej in this conditional probability distribution is the probability of
getting a run of exactly ej copies of the value j in an experiment in which
we successively choose numbers between 1 and j at random, and this latter
probability is clearly j−ej(1 −1/j).
7.6.2 Analysis of the running time
Let T be the random variable that takes the value t when the output is
(n1, . . . , nt).
Clearly, it is the value of T that essentially determines the
running time of the algorithm.
With the random variables Ej deﬁned as above, we see that T = 1 +
M
j=2 Ej. Moreover, for each j, Ej + 1 has a geometric distribution with
TEAM LinG

170
Probabilistic algorithms
associated success probability 1 −1/j, and hence
E[Ej] =
1
1 −1/j −1 =
1
j −1.
Thus,
E[T] = 1 +
M

j=2
E[Ej] = 1 +
M−1

j=1
1
j =
 M
1
dy
y + O(1) ∼log M.
Intuitively, this is roughly as we would expect, since with probability 1/2,
each successive ni is at most one half as large as its predecessor, and so after
O(len(M)) steps, we expect to reach 1.
To complete the running time analysis, let us consider the total number
of times X that the main loop of Algorithm RN in §7.4 is executed. For
i = 1, 2, . . . , let Xi denote the number of times that loop is executed in the
ith loop of Algorithm RS, deﬁning this to be zero if the ith loop is never
reached. So X = ∞
i=1 Xi. Arguing just as in §7.5, we have
E[X] =

i≥1
E[Xi] ≤2

i≥1
P[T ≥i] = 2E[T] ∼2 log M.
To ﬁnish, if Y denotes the running time of Algorithm RS on input M,
then we have Y ≤c len(M)(X + 1) for some constant c, and hence E[Y ] =
O(len(M)2).
Exercise 7.20. Show that when Algorithm RS runs on input M, the ex-
pected number of (not necessarily distinct) primes in the output sequence
is ∼log log M.
Exercise 7.21. For j = 2, . . . , M, let Fj := 1 if j appears in the output
of Algorithm RS on input M, and let Fj := 0 otherwise. Determine the
joint distribution of the Fj. Using this, show that the expected number of
distinct primes appearing in the output sequence is ∼log log M.
7.7 Generating a random factored number
We now present an eﬃcient algorithm that generates a random factored
number. That is, on input M ≥2, the algorithm generates a number r
uniformly distributed over the interval {1, . . . , M}, but instead of the usual
output format for such a number r, the output consists of the prime factor-
ization of r.
As far as anyone knows, there are no eﬃcient algorithms for factoring large
TEAM LinG

7.7 Generating a random factored number
171
numbers, despite years of active research in search of such an algorithm.
So our algorithm to generate a random factored number will not work by
generating a random number and then factoring it.
Our algorithm will use Algorithm RS in §7.6 as a subroutine. In addi-
tion, as we did in §7.5, we shall assume the existence of a deterministic,
polynomial-time primality test IsPrime.
We denote its running time on
input n by Wn, and set W ∗
M := max{Wn : n = 2, . . . , M}.
In the analysis of the algorithm, we shall make use of Mertens’ theorem,
which we proved in Chapter 5 (Theorem 5.13).
On input M ≥2, the algorithm to generate a random factored number
r ∈{1, . . . , M} runs as follows:
Algorithm RFN:
repeat
Run Algorithm RS on input M, obtaining (n1, . . . , nt)
(∗)
Let ni1, . . . , niℓbe the primes among n1, . . . , nt,
including duplicates
(∗∗)
Set r ←ℓ
j=1 nij
If r ≤M then
s ←R {1, . . . , M}
if s ≤r then output ni1, . . . , niℓand halt
forever
Notes:
(∗) For i = 1, . . . , t−1, the number ni is tested for primality
algorithm IsPrime.
(∗∗) We assume that the product is computed by a simple
iterative procedure that halts as soon as the partial
product exceeds M. This ensures that the time spent
forming the product is always O(len(M)2), which sim-
pliﬁes the analysis.
Let us now analyze the running time and output distribution of Algorithm
RFN on input M. Let k := len(M).
To analyze this algorithm, let us ﬁrst consider a single iteration of the
main loop as a random experiment in isolation. Let n = 1, . . . , M be a ﬁxed
integer, and let us calculate the probability that the variable r takes the
particular value n in this loop iteration. Let n = 
p≤M pep be the prime
factorization of n. Then r takes the value n iﬀEp = ep for all primes p ≤M,
TEAM LinG

172
Probabilistic algorithms
which by the analysis in §7.6, happens with probability precisely

p≤M
p−ep(1 −1/p) = U(M)
n
,
where
U(M) :=

p≤M
(1 −1/p).
Now, the probability that this loop iteration produces n as output is equal
to the probability that r takes the value n and s ≤n, which is
U(M)
n
· n
M = U(M)
M
.
Thus, every n is equally likely, and summing over all n = 1, . . . , M, we
see that the probability that this loop iteration succeeds in producing some
output is U(M).
Now consider the expected running time of this loop iteration. From the
analysis in §7.6, it is easy to see that this is O(kW ∗
M). That completes the
analysis of a single loop iteration.
Finally, consider the behavior of Algorithm RFN as a whole. From our
analysis of an individual loop iteration, it is clear that the output distri-
bution of Algorithm RFN is as required, and if H denotes the number of
loop iterations of the algorithm, then E[H] = U(M)−1, which by Mertens’
theorem is O(k). Since the expected running time of each individual loop
iteration is O(kW ∗
M), it follows that the expected total running time is
O(k2W ∗
M).
7.7.1 Using a probabilistic primality test (∗)
Analogous to the discussion in §7.5.1, we can analyze the behavior of Algo-
rithm RFN under the assumption that IsPrime is a probabilistic algorithm
which may erroneously indicate that a composite number is prime with
probability bounded by ϵ. Here, we assume that Wn denotes the expected
running time of the primality test on input n, and set W ∗
M := max{Wn :
n = 2, . . . , M}.
The situation here is a bit more complicated than in the case of Algorithm
RP, since an erroneous output of the primality test in Algorithm RFN could
lead either to the algorithm halting prematurely (with a wrong output),
or to the algorithm being delayed (because an opportunity to halt may be
missed).
Let us ﬁrst analyze in detail the behavior of a single iteration of the main
TEAM LinG

7.7 Generating a random factored number
173
loop of Algorithm RFN. Let A denote the event that the primality test
makes a mistake in this loop iteration, and let δ := P[A]. If T is the number
of loop iterations in a given run of Algorithm RS, it is easy to see that
δ ≤ϵ E[T] = ϵ ℓ(M),
where
ℓ(M) := 1 +
M−1

j=1
1
j ≤2 + log M.
Now, let n = 1, . . . , M be a ﬁxed integer, and let us calculate the probability
αn that the correct prime factorization of n is output in this loop iteration.
Let Bn be the event that the primes among the output of Algorithm RS
multiply out to n.
Then αn = P[Bn ∧A](n/M). Moreover, because of
the mutual independence of the Ej, not only does it follow that P[Bn] =
U(M)/n, but it also follows that Bn and A are independent events: to see
this, note that Bn is determined by the variables {Ej : j prime}, and A is
determined by the variables {Ej : j composite} and the random choices of
the primality test. Hence,
αn = U(M)
M
(1 −δ).
Thus, every n is equally likely to be output.
If C is the event that the
algorithm halts with some output (correct or not) in this loop iteration,
then
P[C] ≥U(M)(1 −δ),
(7.7)
and
P[C ∨A] = U(M)(1 −δ) + δ = U(M) −δU(M) + δ ≥U(M).
(7.8)
The expected running time of a single loop iteration of Algorithm RFN is
also easily seen to be O(kW ∗
M). That completes the analysis of a single loop
iteration.
We next analyze the total running time of Algorithm RFN. If H is the
number of loop iterations of Algorithm RFN, it follows from (7.7) that
E[H] ≤
1
U(M)(1 −δ),
and assuming that ϵℓ(M) ≤1/2, it follows that the expected running time
of Algorithm RFN is O(k2W ∗
M).
Finally, we analyze the statistical distance ∆between the output distri-
bution of Algorithm RFN and the uniform distribution on the numbers 1
TEAM LinG

174
Probabilistic algorithms
to M, in correct factored form. Let H′ denote the ﬁrst loop iteration i for
which the event C ∨A occurs, meaning that the algorithm either halts or
the primality test makes a mistake. Then, by (7.8), H′ has a geometric
distribution with an associated success probability of at least U(M). Let Ai
be the event that the primality makes a mistake for the ﬁrst time in loop
iteration i, and let A∗is the event that the primality test makes a mistake in
any loop iteration. Observe that P[Ai | H′ ≥i] = δ and P[Ai | H′ < i] = 0,
and so
P[Ai] = P[Ai | H′ ≥i]P[H′ ≥i] = δP[H′ ≥i],
from which it follows that
P[A∗] =

i≥1
P[Ai] =

i≥1
δP[H′ ≥i] = δE[H′] ≤δU(M)−1.
Now, if γ is the probability that the output of Algorithm RFN is not in
correct factored form, then
γ ≤P[A∗] = δU(M)−1 = O(k2ϵ).
We have already argued that each value n between 1 and M, in correct
factored form, is equally likely to be output, and in particular, each such
value occurs with probability at most 1/M. It follows from Theorem 6.15
that ∆= γ (verify).
Exercise 7.22. To simplify the analysis, we analyzed Algorithm RFN using
the worst-case estimate W ∗
M on the expected running time of the primality
test. Deﬁne
W +
M :=
M

j=2
Wj
j −1,
where Wn denotes the expected running time of a probabilistic implemen-
tation of IsPrime on input n.
Show that the expected running time of
Algorithm RFN is O(kW +
M), assuming ϵℓ(M) ≤1/2.
Exercise 7.23. Analyze Algorithm RFN assuming that the primality test
is implemented by an “Atlantic City” algorithm with error probability at
most ϵ.
7.8 The RSA cryptosystem
Algorithms for generating large primes, such as Algorithm RP in §7.5, have
numerous applications in cryptography. One of the most well known and
TEAM LinG

7.8 The RSA cryptosystem
175
important such applications is the RSA cryptosystem, named after its inven-
tors Rivest, Shamir, and Adleman. We give a brief overview of this system
here.
Suppose that Alice wants to send a secret message to Bob over an insecure
network. An adversary may be able to eavesdrop on the network, and so
sending the message “in the clear” is not an option.
Using older, more
traditional cryptographic techniques would require that Alice and Bob share
a secret key between them; however, this creates the problem of securely
generating such a shared secret.
The RSA cryptosystem is an example
of a “public key” cryptosystem. To use the system, Bob simply places a
“public key” in the equivalent of an electronic telephone book, while keeping
a corresponding “private key” secret. To send a secret message to Bob, Alice
obtains Bob’s public key from the telephone book, and uses this to encrypt
her message. Upon receipt of the encrypted message, Bob uses his secret
key to decrypt it, obtaining the original message.
Here is how the RSA cryptosystem works.
To generate a public
key/private key pair, Bob generates two very large random primes p and
q. To be secure, p and q should be quite large—typically, they are chosen
to be around 512 bits in length. We require that p ̸= q, but the probability
that two random 512-bit primes are equal is negligible, so this is hardly an
issue. Next, Bob computes n := pq. Bob also selects an integer e > 1 such
that gcd(e, φ(n)) = 1. Here, φ(n) = (p −1)(q −1). Finally, Bob computes
d := e−1 mod φ(n). The public key is the pair (n, e), and the private key is
the pair (n, d). The integer e is called the “encryption exponent” and d is
called the “decryption exponent.”
After Bob publishes his public key (n, e), Alice may send a secret message
to Bob as follows. Suppose that a message is encoded in some canonical
way as a number between 0 and n −1—we can always interpret a bit string
of length less than len(n) as such a number. Thus, we may assume that
a message is an element α of Zn. To encrypt the message α, Alice simply
computes β := αe. The encrypted message is β. When Bob receives β, he
computes γ := βd, and interprets γ as a message. (Note that if Bob stores
the factorization of n, then he may speed up the decryption process using
the algorithm in Exercise 7.28 below.)
The most basic requirement of any encryption scheme is that decryption
should “undo” encryption. In this case, this means that for all α ∈Zn, we
should have
(αe)d = α.
(7.9)
If α ∈Z∗
n, then this is clearly the case, since we have ed = 1 + φ(n)k for
TEAM LinG

176
Probabilistic algorithms
some positive integer k, and hence by Euler’s theorem (Theorem 2.15), we
have
(αe)d = αed = α1+φ(n)k = α · αφ(n)k = α.
Even if α ̸∈Z∗
n, equation (7.9) still holds. To see this, let α = [a]n, with
gcd(a, n) ̸= 1. There are three possible cases. First, if a ≡0 (mod n), then
trivially, aed ≡0 (mod n). Second, if a ≡0 (mod p) but a ̸≡0 (mod q),
then trivially aed ≡0 (mod p), and
aed ≡a1+φ(n)k ≡a · aφ(n)k ≡a (mod q),
where the last congruence follows from the fact that φ(n)k is a multiple of
q −1, which is a multiple of the multiplicative order of a modulo q (again
by Euler’s theorem).
Thus, we have shown that aed ≡a (mod p) and
aed ≡a (mod q), from which it follows that aed ≡a (mod n). The third
case, where a ̸≡0 (mod p) and a ≡0 (mod q), is treated in the same way as
the second. Thus, we have shown that equation (7.9) holds for all α ∈Zn.
Of course, the interesting question about the RSA cryptosystem is
whether or not it really is secure.
Now, if an adversary, given only the
public key (n, e), were able to factor n, then he could easily compute the
decryption exponent d. It is widely believed that factoring n is computation-
ally infeasible, for suﬃciently large n, and so this line of attack is ineﬀective,
barring a breakthrough in factorization algorithms. However, there may be
other possible lines of attack. For example, it is natural to ask whether one
can compute the decryption exponent without having to go to the trouble
of factoring n. It turns out that the answer to this question is no: if one
could compute the decryption exponent d, then ed −1 would be a multiple
of φ(n), and as we shall see later in §10.6, given any multiple of φ(n), we
can easily factor n.
Thus, computing the encryption exponent is equivalent to factoring n, and
so this line of attack is also ineﬀective. But there still could be other lines
of attack. For example, even if we assume that factoring large numbers is
infeasible, this is not enough to guarantee that for a given encrypted message
β, the adversary is unable to compute βd (although nobody actually knows
how to do this without ﬁrst factoring n).
The reader should be warned that the proper notion of security for an
encryption scheme is quite subtle, and a detailed discussion of this is well
beyond the scope of this text. Indeed, the simple version of RSA presented
here suﬀers from a number of security problems (because of this, actual im-
plementations of public-key encryption schemes based on RSA are somewhat
more complicated). We mention one such problem here (others are examined
TEAM LinG

7.8 The RSA cryptosystem
177
in some of the exercises below). Suppose an eavesdropping adversary knows
that Alice will send one of a few, known, candidate messages. For example,
an adversary may know that Alice’s message is either “let’s meet today” or
“let’s meet tomorrow.” In this case, the adversary can encrypt for himself
all of the candidate messages, intercept Alice’s actual encrypted message,
and then by simply comparing encryptions, the adversary can determine
which particular message Alice encrypted. This type of attack works simply
because the encryption algorithm is deterministic, and in fact, any deter-
ministic encryption algorithm will be vulnerable to this type of attack. To
avoid this type of attack, one must use a probabilistic encryption algorithm.
In the case of the RSA cryptosystem, this is often achieved by padding the
message with some random bits before encrypting it.
Exercise 7.24. Alice submits a bid to an auction, and so that other bidders
cannot see her bid, she encrypts it under the public key of the auction service.
Suppose that the auction service provides a public key for an RSA encryption
scheme, with a modulus n. Assume that bids are encoded simply as integers
between 0 and n −1 prior to encryption. Also, assume that Alice submits
a bid that is a “round number,” which in this case means that her bid is a
number that is divisible by 10. Show how an eavesdropper can submit an
encryption of a bid that exceeds Alice’s bid by 10%, without even knowing
what Alice’s bid is. In particular, your attack should work even if the space
of possible bids is very large.
Exercise 7.25. To speed up RSA encryption, one may choose a very small
encryption exponent. This exercise develops a “small encryption exponent
attack” on RSA. Suppose Bob, Bill, and Betty have RSA public keys with
moduli n1, n2, and n3, and all three use encryption exponent 3. Assume
that n1, n2, n3 are pairwise relatively prime. Suppose that Alice sends an
encryption of the same message to Bob, Bill, and Betty — that is, Alice
encodes her message as an integer a, with 0 ≤a < min{n1, n2, n3}, and
computes the three encrypted messages βi := [a3]ni, for i = 1, . . . , 3. Show
how to recover Alice’s message from these three encrypted messages.
Exercise 7.26. To speed up RSA decryption, one might choose a small de-
cryption exponent, and then derive the encryption exponent from this. This
exercise develops a “small decryption exponent attack” on RSA. Suppose
n = pq, where p and q are distinct primes with len(p) = len(q). Let d and e
be integers such that 1 < d < φ(n), 1 < e < φ(n), and de ≡1 (mod φ(n)).
TEAM LinG

178
Probabilistic algorithms
Further, assume that
4d < n1/4.
Show how to eﬃciently compute d, given n and e.
Hint:
since de ≡
1 (mod φ(n)), it follows that de = 1+kφ(n) for an integer k with 0 < k < d;
let r := kn −de, and show that |r| < n3/4; next, show how to recover d
(along with r and k) using Theorem 4.6.
Exercise 7.27. Suppose there is a probabilistic algorithm A that takes as
input an integer n of the form n = pq, where p and q are distinct primes. The
algorithm also takes as input an integer e > 1, with gcd(e, φ(n)) = 1, and
an element β ∈Z∗
n. It outputs either “failure,” or α ∈Z∗
n such that αe = β.
Furthermore, assume that A runs in strict polynomial time, and that for all
n and e of the above form, and for randomly chosen β ∈Z∗
n, A succeeds in
ﬁnding α as above with probability ϵ(n, e). Here, the probability is taken
over the random choice of β, as well as the random choices made during
the execution of A. Show how to use A to construct another probabilistic
algorithm A′ that takes as input n and e as above, as well as β ∈Z∗
n, runs
in expected polynomial time, and that satisﬁes the following property:
if ϵ(n, e) ≥0.001, then for all β ∈Z∗
n, A′ ﬁnds α ∈Z∗
n with
αe = β with probability at least 0.999.
The algorithm A′ in the above exercise is an example of what is called
a random self-reduction, that is, an algorithm that reduces the task of
solving an arbitrary instance of a given problem to that of solving a random
instance of the problem. Intuitively, the fact that a problem is random self-
reducible in this sense means that the problem is no harder in “the worst
case” than in “the average case.”
Exercise 7.28. This exercise develops an algorithm for speeding up RSA
decryption. Suppose that we are given two distinct ℓ-bit primes, p and q, an
element β ∈Zn, where n := pq, and an integer d, where 1 < d < φ(n). Using
the algorithm from Exercise 3.26, we can compute βd at a cost of essentially
2ℓsquarings in Zn.
Show how this can be improved, making use of the
factorization of n, so that the total cost is essentially that of ℓsquarings
in Zp and ℓsquarings in Zq, leading to a roughly four-fold speed-up in the
running time.
TEAM LinG

7.9 Notes
179
7.9 Notes
See Luby [59] for an exposition of the theory of pseudo-random bit genera-
tion.
Our approach in §7.1 to deﬁning the probability distribution associated
with the execution of a probabilistic algorithm is a bit unusual (indeed, it is
a bit unusual among papers and textbooks on the subject to even bother to
formally deﬁne much of anything). There are alternative approaches. One
approach is to deﬁne the output distribution and expected running time of an
algorithm on a given input directly, using the identities in Exercise 7.4, and
avoid the construction of an underlying probability distribution. However,
without such a probability distribution, we would have very few tools at our
disposal to analyze the output distribution and running time of particular
algorithms. Another approach (which we dismissed with little justiﬁcation
early on in §7.1) is to attempt to deﬁne a distribution that models an in-
ﬁnite random bit string. One way to do this is to identify an inﬁnite bit
string with the real number in the unit interval [0, 1] obtained by interpret-
ing the bit string as a number written in base 2, and then use continuous
probability theory (which we have not developed here, but which is covered
in a standard undergraduate course on probability theory), applied to the
uniform distribution on [0, 1]. There are a couple of problems with this ap-
proach. First, the above identiﬁcation of bit strings with numbers is not
quite one-to-one. Second, when one tries to deﬁne the notion of expected
running time, numerous technical problems arise; in particular, the usual
deﬁnition of an expected value in terms of an integral would require us to
integrate functions that are not Riemann integrable. To properly deal with
all of these issues, one would have to develop a good deal of measure theory
(σ-algebras, Lesbegue integration, and so on), at the level normally covered
in a graduate-level course on probability or measure theory.
The algorithm presented here for generating a random factored number is
due to Kalai [50], although the analysis presented here is a bit diﬀerent, and
our analysis using a probabilistic primality test is new. Kalai’s algorithm is
signiﬁcantly simpler, though less eﬃcient than, an earlier algorithm due to
Bach [9], which uses an expected number of O(k) primality tests, as opposed
to the O(k2) primality tests used by Kalai’s algorithm.
The RSA cryptosystem was invented by Rivest, Shamir, and Adleman
[78].
There is a vast literature on cryptography.
One starting point is
the book by Menezes, van Oorschot, and Vanstone [62].
The attack in
Exercise 7.26 is due to Wiener [104]; this attack was recently strengthened
by Boneh and Durfee [19].
TEAM LinG

8
Abelian groups
This chapter introduces the notion of an abelian group. This is an abstrac-
tion that models many diﬀerent algebraic structures, and yet despite the
level of generality, a number of very useful results can be easily obtained.
8.1 Deﬁnitions, basic properties, and examples
Deﬁnition 8.1. An abelian group is a set G together with a binary oper-
ation ⋆on G such that
(i) for all a, b, c ∈G, a ⋆(b ⋆c) = (a ⋆b) ⋆c (i.e., ⋆is associative),
(ii) there exists e ∈G (called the identity element) such that for all
a ∈G, a ⋆e = a = e ⋆a,
(iii) for all a ∈G there exists a′ ∈G (called the inverse of a) such that
a ⋆a′ = e = a′ ⋆a,
(iv) for all a, b ∈G, a ⋆b = b ⋆a (i.e., ⋆is commutative).
While there is a more general notion of a group, which may be deﬁned
simply by dropping property (iv) in Deﬁnition 8.1, we shall not need this
notion in this text. The restriction to abelian groups helps to simplify the
discussion signiﬁcantly. Because we will only be dealing with abelian groups,
we may occasionally simply say “group” instead of “abelian group.”
Before looking at examples, let us state some very basic properties of
abelian groups that follow directly from the deﬁnition:
Theorem 8.2. Let G be an abelian group with binary operation ⋆. Then
we have:
(i) G contains only one identity element;
(ii) every element of G has only one inverse.
180
TEAM LinG

8.1 Deﬁnitions, basic properties, and examples
181
Proof. Suppose e, e′ are both identities. Then we have
e = e ⋆e′ = e′,
where we have used part (ii) of Deﬁnition 8.1, once with e′ as the identity,
and once with e as the identity. That proves part (i) of the theorem.
To prove part (ii) of the theorem, let a ∈G, and suppose that a has two
inverses, a′ and a′′. Then using parts (i)–(iii) of Deﬁnition 8.1, we have
a′ = a′ ⋆e (by part (ii))
= a′ ⋆(a ⋆a′′) (by part (iii) with inverse a′′ of a)
= (a′ ⋆a) ⋆a′′ (by part (i))
= e ⋆a′′ (by part (iii) with inverse a′ of a)
= a′′ (by part (ii)). 2
These uniqueness properties justify use of the deﬁnite article in Deﬁni-
tion 8.1 in conjunction with the terms “identity element” and “inverse.”
Note that we never used part (iv) of the deﬁnition in the proof of the above
theorem.
Abelian groups are lurking everywhere, as the following examples illus-
trate.
Example 8.1. The set of integers Z under addition forms an abelian group,
with 0 being the identity, and −a being the inverse of a ∈Z. 2
Example 8.2. For integer n, the set nZ = {nz : z ∈Z} under addition
forms an abelian group, again, with 0 being the identity, and n(−z) being
the inverse of nz. 2
Example 8.3. The set of non-negative integers under addition does not
form an abelian group, since additive inverses do not exist for positive inte-
gers. 2
Example 8.4. The set of integers under multiplication does not form an
abelian group, since inverses do not exist for integers other than ±1. 2
Example 8.5. The set of integers {±1} under multiplication forms an
abelian group, with 1 being the identity, and −1 its own inverse. 2
Example 8.6. The set of rational numbers Q = {a/b : a, b ∈Z, b ̸= 0}
under addition forms an abelian group, with 0 being the identity, and (−a)/b
being the inverse of a/b. 2
TEAM LinG

182
Abelian groups
Example 8.7. The set of non-zero rational numbers Q∗under multiplica-
tion forms an abelian group, with 1 being the identity, and b/a being the
inverse of a/b. 2
Example 8.8. The set Zn under addition forms an abelian group, where
[0]n is the identity, and where [−a]n is the inverse of [a]n. 2
Example 8.9. The set Z∗
n of residue classes [a]n with gcd(a, n) = 1 under
multiplication forms an abelian group, where [1]n is the identity, and if b is
a multiplicative inverse of a modulo n, then [b]n is the inverse of [a]n. 2
Example 8.10. Continuing the previous example, let us set n = 15, and
enumerate the elements of Z∗
15. They are
[1], [2], [4], [7], [8], [11], [13], [14].
An alternative enumeration is
[±1], [±2], [±4], [±7]. 2
Example 8.11. As another special case, consider Z∗
5. We can enumerate
the elements of this groups as
[1], [2], [3], [4]
or alternatively as
[±1], [±2]. 2
Example 8.12. For any positive integer n, the set of n-bit strings under
the “exclusive or” operation forms an abelian group, where the “all zero”
bit string is the identity, and every bit string is its own inverse. 2
Example 8.13. The set of all arithmetic functions f, such that f(1) ̸= 0,
with multiplication deﬁned by the Dirichlet product (see §2.6) forms an
abelian group, where the special arithmetic function I is the identity, and
inverses are provided by the result of Exercise 2.27. 2
Example 8.14. The set of all ﬁnite bit strings under concatenation does
not form an abelian group. Although concatenation is associative and the
empty string acts as an identity element, inverses do not exist (except for
the empty string), nor is concatenation commutative. 2
Example 8.15. The set of 2 × 2 integer matrices with determinant ±1,
together with the binary operation of matrix multiplication, is an example of
a non-abelian group; that is, it satisﬁes properties (i)–(iii) of Deﬁnition 8.1,
but not property (iv). 2
TEAM LinG

8.1 Deﬁnitions, basic properties, and examples
183
Example 8.16. The set of all permutations on a given set of size n ≥
3, together with the binary operation of function composition, is another
example of a non-abelian group (for n = 1, 2, it is an abelian group). 2
Note that in specifying a group, one must specify both the underlying set
G as well as the binary operation; however, in practice, the binary operation
is often implicit from context, and by abuse of notation, one often refers to
G itself as the group. For example, when talking about the abelian groups
Z and Zn, it is understood that the group operation is addition, while when
talking about the abelian group Z∗
n, it is understood that the group operation
is multiplication.
Typically, instead of using a special symbol like “⋆” for the group oper-
ation, one uses the usual addition (“+”) or multiplication (“·”) operations.
For any particular, concrete abelian group, the most natural choice of no-
tation is clear (e.g., addition for Z and Zn, multiplication for Z∗
n); however,
for a “generic” group, the choice is largely a matter of taste. By conven-
tion, whenever we consider a “generic” abelian group, we shall use additive
notation for the group operation, unless otherwise speciﬁed.
If an abelian group G is written additively, then the identity element is
denoted by 0G (or just 0 if G is clear from context), and the inverse of an
element a ∈G is denoted by −a. For a, b ∈G, a −b denotes a + (−b). If
n is a positive integer, then n · a denotes a + a + · · · + a, where there are n
terms in the sum—note that 1 · a = a. Moreover, 0 · a denotes 0G, and if n
is a negative integer, then n · a denotes (−n)(−a).
If an abelian group G is written multiplicatively, then the identity element
is denoted by 1G (or just 1 if G is clear from context), and the inverse of
an element a ∈G is denoted by a−1 or 1/a. As usual, one may write ab in
place of a · b. For a, b ∈G, a/b denotes a · b−1. If n is a positive integer,
then an denotes a · a · · · · · a, where there are n terms in the product—note
that a1 = a. Moreover, a0 denotes 1G, and if n is a negative integer, then
an denotes (a−1)−n.
An abelian group G may be inﬁnite or ﬁnite. If the group is ﬁnite, we
deﬁne its order to be the number of elements in the underlying set G;
otherwise, we say that the group has inﬁnite order.
Example 8.17. The order of the additive group Zn is n. 2
Example 8.18. The order of the multiplicative group Z∗
n is φ(n), where φ
is Euler’s phi function, deﬁned in §2.4. 2
Example 8.19. The additive group Z has inﬁnite order. 2
TEAM LinG

184
Abelian groups
We now record a few more simple but useful properties of abelian groups.
Theorem 8.3. Let G be an abelian group. Then for all a, b, c ∈G and
n, m ∈Z, we have:
(i) if a + b = a + c, then b = c;
(ii) the equation a + x = b has a unique solution x ∈G;
(iii) −(a + b) = (−a) + (−b);
(iv)
−(−a) = a;
(v) (−n)a = −(na) = n(−a);
(vi) (n + m)a = na + ma;
(vii) n(ma) = (nm)a = m(na);
(viii) n(a + b) = na + nb.
Proof. Exercise. 2
If G1, . . . , Gk are abelian groups, we can form the direct product
G := G1 × · · · × Gk, which consists of all k-tuples (a1, . . . , ak) with
a1 ∈G1, . . . , ak ∈Gk.
We can view G in a natural way as an abelian
group if we deﬁne the group operation component-wise:
(a1, . . . , ak) + (b1, . . . , bk) := (a1 + b1, . . . , ak + bk).
Of course, the groups G1, . . . , Gk may be diﬀerent, and the group operation
applied in the ith component corresponds to the group operation associated
with Gi. We leave it to the reader to verify that G is in fact an abelian
group.
Exercise 8.1. In this exercise, you are to generalize the M¨obius inversion
formula, discussed in §2.6, to arbitrary abelian groups. Let F be the set
of all functions mapping positive integers to integers. Let G be an abelian
group, and let G be the set of all functions mapping positive integers to
elements of G. For f ∈F and g ∈G, we can deﬁne the Dirichlet product
f ⋆g ∈G as follows:
(f ⋆g)(n) :=

d|n
f(d)g(n/d),
the sum being over all positive divisors d of n. Let I, J, µ ∈F be as deﬁned
in §2.6.
(a) Show that for all f, g ∈F and all h ∈G, we have (f ⋆g)⋆h = f ⋆(g⋆h).
(b) Show that for all f ∈G, we have I ⋆f = f.
(c) Show that for all f, F ∈G, we have F = J ⋆f if and only if f = µ⋆F.
TEAM LinG

8.2 Subgroups
185
8.2 Subgroups
We next introduce the notion of a subgroup.
Deﬁnition 8.4. Let G be an abelian group, and let H be a non-empty subset
of G such that
(i) a + b ∈H for all a, b ∈H, and
(ii) −a ∈H for all a ∈H.
Then H is called a subgroup of G.
In words: H is a subgroup of G if it is closed under the group operation
and taking inverses.
Multiplicative notation: if the abelian group G in the above deﬁnition is
written using multiplicative notation, then H is a subgroup if ab ∈H and
a−1 ∈H for all a, b ∈H.
Theorem 8.5. If G is an abelian group, and H is a subgroup of G, then
H contains 0G; moreover, the binary operation of G, when restricted to H,
yields a binary operation that makes H into an abelian group whose identity
is 0G.
Proof. First, to see that 0G ∈H, just pick any a ∈H, and using both
properties of the deﬁnition of a subgroup, we see that 0G = a + (−a) ∈H.
Next, note that by property (i) of Deﬁnition 8.4, H is closed under ad-
dition, which means that the restriction of the binary operation “+” on G
to H induces a well deﬁned binary operation on H. So now it suﬃces to
show that H, together with this operation, satisfy the deﬁning properties
of an abelian group. Associativity and commutativity follow directly from
the corresponding properties for G. Since 0G acts as the identity on G, it
does so on H as well. Finally, property (ii) of Deﬁnition 8.4 guarantees that
every element a ∈H has an inverse in H, namely, −a. 2
Clearly, for an abelian group G, the subsets G and {0G} are subgroups.
These are not very interesting subgroups. An easy way to sometimes ﬁnd
other, more interesting, subgroups within an abelian group is by using the
following two theorems.
Theorem 8.6. Let G be an abelian group, and let m be an integer. Then
mG := {ma : a ∈G} is a subgroup of G.
Proof. For ma, mb ∈mG, we have ma+mb = m(a+b) ∈mG, and −(ma) =
m(−a) ∈mG. 2
TEAM LinG

186
Abelian groups
Theorem 8.7. Let G be an abelian group, and let m be an integer. Then
G{m} := {a ∈G : ma = 0G} is a subgroup of G.
Proof. If ma = 0G and mb = 0G, then m(a + b) = ma + mb = 0G + 0G = 0G
and m(−a) = −(ma) = −0G = 0G. 2
Multiplicative notation: if the abelian group G in the above two theorems
is written using multiplicative notation, then we write the subgroup of the
ﬁrst theorem as Gm := {am : a ∈G}. The subgroup in the second theorem
is denoted in the same way: G{m} := {a ∈G : am = 1G}.
Example 8.20. For every integer m, the set mZ is the subgroup of the
additive group Z consisting of all integer multiples of m. Two such subgroups
mZ and m′Z are equal if and only if m = ±m′. The subgroup Z{m} is equal
to Z if m = 0, and is equal to {0} otherwise. 2
Example 8.21. Let n be a positive integer, let m ∈Z, and consider the
subgroup mZn of the additive group Zn. Now, [b]n ∈mZn if and only if
there exists x ∈Z such that mx ≡b (mod n). By Theorem 2.7, such an
x exists if and only if d | b, where d := gcd(m, n). Thus, mZn consists
precisely of the n/d distinct residue classes
[i · d]n (i = 0, . . . , n/d −1),
and in particular, mZn = dZn.
Now consider the subgroup Zn{m} of Zn. The residue class [x]n is in
Zn{m} if and only if mx ≡0 (mod n). By Theorem 2.7, this happens if
and only if x ≡0 (mod n/d), where d = gcd(m, n) as above. Thus, Zn{m}
consists precisely of the d residue classes
[i · n/d]n (i = 0, . . . , d −1),
and in particular, Zn{m} = Zn{d} = (n/d)Zn. 2
Example 8.22. For n = 15, consider again the table in Example 2.3. For
m = 1, 2, 3, 4, 5, 6, the elements appearing in the mth row of that table
form the subgroup mZn of Zn, and also the subgroup Zn{n/d}, where d :=
gcd(m, n). 2
Because the abelian groups Z and Zn are of such importance, it is a good
idea to completely characterize all subgroups of these abelian groups. As
the following two theorems show, the subgroups in the above examples are
the only subgroups of these groups.
TEAM LinG

8.2 Subgroups
187
Theorem 8.8. If G is a subgroup of Z, then there exists a unique non-
negative integer m such that G = mZ.
Moreover, for two non-negative
integers m1 and m2, we have m1Z ⊆m2Z if and only if m2 | m1.
Proof. Actually, we have already proven this. One only needs to observe
that a subset G of Z is a subgroup if and only if it is an ideal of Z, as
deﬁned in §1.2 (see Exercise 1.7). The ﬁrst statement of the theorem then
follows from Theorem 1.5. The second statement follows easily from the
deﬁnitions, as was observed in §1.2. 2
Theorem 8.9. If G is a subgroup of Zn, then there exists a unique positive
integer d dividing n such that G = dZn. Also, for positive divisors d1, d2 of
n, we have d1Zn ⊆d2Zn if and only if d2 | d1.
Proof. Let ρ : Z →Zn be the map that sends a ∈Z to [a]n ∈Zn. Clearly, ρ
is surjective. Consider the pre-image ρ−1(G) ⊆Z of G.
We claim that ρ−1(G) is a subgroup of Z. To see this, observe that for
a, b ∈Z, if [a]n and [b]n belong to G, then so do [a + b]n = [a]n + [b]n and
−[a]n = [−a]n, and thus a + b and −a belong to the pre-image.
Since ρ−1(G) is a subgroup of Z, by the previous theorem, we have
ρ−1(G) = dZ for some non-negative integer d. Moreover, it is clear that
n ∈ρ−1(G), and hence d | n. That proves the existence part of the theorem.
Next, we claim that for any divisor d of n, we have ρ−1(dZn) = dZ. To see
this, note that ρ−1(dZn) consists of all integers b such that dx ≡b (mod n)
has an integer solution x, and by Theorem 2.7, this congruence admits a
solution if and only if d | b. That proves the claim.
Now consider any two positive divisors d1, d2 of n. Since d1Zn ⊆d2Zn
if and only if ρ−1(d1Zn) ⊆ρ−1(d2Zn), the remaining statements of the
theorem follow from the corresponding statements of Theorem 8.8 and the
above claim. 2
Of course, not all abelian groups have such a simple subgroup structure.
Example 8.23. Consider the group G = Z2 × Z2. For any non-zero α ∈G,
α + α = 0G. From this, it is easy to see that the set H = {0G, α} is a
subgroup of G. However, for any integer m, mG = G if m is odd, and
mG = {0G} if m is even. Thus, the subgroup H is not of the form mG for
any m. 2
Example 8.24. Consider again the group Z∗
n, for n = 15, discussed in
Example 8.10. As discussed there, we have Z∗
15 = {[±1], [±2], [±4], [±7]}.
TEAM LinG

188
Abelian groups
Therefore, the elements of (Z∗
15)2 are
[1]2 = [1], [2]2 = [4], [4]2 = [16] = [1], [7]2 = [49] = [4];
thus, (Z∗
15)2 has order 2, consisting as it does of the two distinct elements
[1] and [4].
Going further, one sees that (Z∗
15)4 = {[1]}. Thus, α4 = [1] for all α ∈Z∗
15.
By direct calculation, one can determine that (Z∗
15)3 = Z∗
15; that is, cubing
simply permutes Z∗
15.
For any integer m, write m = 4q + r, where 0 ≤r < 4. Then for any
α ∈Z∗
15, we have αm = α4q+r = α4qαr = αr. Thus, (Z∗
15)m is either Z∗
15,
(Z∗
15)2, or {[1]}.
However, there are certainly other subgroups of Z∗
15 — for example, the
subgroup {[±1]}. 2
Example 8.25. Consider again the group Z∗
5 from Example 8.11. As dis-
cussed there, Z∗
5 = {[±1], [±2]}. Therefore, the elements of (Z∗
5)2 are
[1]2 = [1], [2]2 = [4] = [−1];
thus, (Z∗
5)2 = {[±1]} and has order 2.
There are in fact no other subgroups of Z∗
5 besides Z∗
5, {[±1]}, and {[1]}.
Indeed, if H is a subgroup containing [2], then we must have H = Z∗
5:
[2] ∈H implies [2]2 = [4] = [−1] ∈H, which implies [−2] ∈H as well. The
same holds if H is a subgroup containing [−2]. 2
Example 8.26. Consider again the group of arithmetic functions f, such
that f(1) ̸= 0, with multiplication deﬁned by the Dirichlet product, dis-
cussed in Example 8.13. By the results of Exercises 2.21 and 2.28, we see
that the subset of all multiplicative arithmetic functions is a subgroup of
this group. 2
The following two theorems may be used to simplify verifying that a subset
is a subgroup.
Theorem 8.10. If G is an abelian group, and H is a non-empty subset of
G such that a −b ∈H for all a, b ∈H, then H is a subgroup of G.
Proof. Since H is non-empty, let c be an arbitrary element of H. Then
0G = c −c ∈H. It follows that for all a ∈H, we have −a = 0G −a ∈H,
and for all a, b ∈H, we have a + b = a −(−b) ∈H. 2
Theorem 8.11. If G is an abelian group, and H is a non-empty, ﬁnite
subset of G such that a + b ∈H for all a, b ∈H, then H is a subgroup of G.
TEAM LinG

8.2 Subgroups
189
Proof. We only need to show that −a ∈H for all a ∈H. Let a ∈H be
given. If a = 0G, then clearly −a = 0G ∈H, so assume that a ̸= 0G, and
consider the set S of all elements of G of the form ma, for m = 1, 2, . . . . Since
H is closed under addition, it follows that S ⊆H. Moreover, since H is
ﬁnite, S must be ﬁnite, and hence there must exist integers m1, m2 such that
m1 > m2 > 0 and m1a = m2a; that is, ra = 0G, where r := m1−m2 > 0. We
may further assume that r > 1, since otherwise a = 0G, and we are assuming
that a ̸= 0G. It follows that a + (r −1)a = 0G, and so −a = (r −1)a ∈S. 2
We close this section with two theorems that provide useful ways to build
new subgroups out of old subgroups.
Theorem 8.12. If H1 and H2 are subgroups of an abelian group G, then
so is
H1 + H2 := {h1 + h2 : h1 ∈H1, h2 ∈H2}.
Proof. Consider two elements in H1 +H2, which we can write as h1 +h2 and
h′
1 + h′
2, where h1, h′
1 ∈H1 and h2, h′
2 ∈H2. Then by the closure properties
of subgroups, h1+h′
1 ∈H1 and h2+h′
2 ∈H2, and hence (h1+h2)+(h′
1+h′
2) =
(h1 + h′
1) + (h2 + h′
2) ∈H1 + H2. Similarly, −(h1 + h2) = (−h1) + (−h2) ∈
H1 + H2. 2
Multiplicative notation: if the abelian group G in the above theorem is
written multiplicatively, then the subgroup deﬁned in the theorem is written
H1 · H2 := {h1h2 : h1 ∈H1, h2 ∈H2}.
Theorem 8.13. If H1 and H2 are subgroups of an abelian group G, then
so is H1 ∩H2.
Proof. If h ∈H1 ∩H2 and h′ ∈H1 ∩H2, then since h, h′ ∈H1, we have
h + h′ ∈H1, and since h, h′ ∈H2, we have h + h′ ∈H2; therefore, h + h′ ∈
H1 ∩H2. Similarly, −h ∈H2 and −h ∈H2, and therefore, −h ∈H1 ∩H2.
2
Exercise 8.2. Show that if H′ is a subgroup of an abelian group G, then a
set H ⊆H′ is a subgroup of G if and only if H is a subgroup of H′.
Exercise 8.3. Let G be an abelian group with subgroups H1 and H2. Show
that any subgroup H of G that contains H1 ∪H2 contains H1 + H2, and
H1 ⊆H2 if and only if H1 + H2 = H2.
Exercise 8.4. Let H1 be a subgroup of an abelian group G1 and H2 a
subgroup of an abelian group G2.
Show that H1 × H2 is a subgroup of
G1 × G2.
TEAM LinG

190
Abelian groups
Exercise 8.5. Let G1 and G2 be abelian groups, and let H be a subgroup
of G1 × G2. Deﬁne
H1 := {h1 ∈G1 : (h1, h2) ∈H for some h2 ∈G2}.
Show that H1 is a subgroup of G1.
Exercise 8.6. Give an example of speciﬁc abelian groups G1 and G2, along
with a subgroup H of G1 × G2, such that H cannot be written as H1 × H2,
where H1 is a subgroup of G1 and H2 is a subgroup of G2.
8.3 Cosets and quotient groups
We now generalize the notion of a congruence relation.
Let G be an abelian group, and let H be a subgroup of G. For a, b ∈G,
we write a ≡b (mod H) if a −b ∈H. In other words, a ≡b (mod H) if and
only if a = b + h for some h ∈H.
Analogously to Theorem 2.2, if we view the subgroup H as ﬁxed, then
the following theorem says that the binary relation “· ≡· (mod H)” is an
equivalence relation on the set G:
Theorem 8.14. Let G be an abelian group and H a subgroup of G. For all
a, b, c ∈G, we have:
(i) a ≡a (mod H);
(ii) a ≡b (mod H) implies b ≡a (mod H);
(iii) a ≡b (mod H) and b ≡c (mod H) implies a ≡c (mod H).
Proof. For (i), observe that H contains 0G = a −a. For (ii), observe that if
H contains a −b, then it also contains −(a −b) = b −a. For (iii), observe
that if H contains a−b and b−c, then it also contains (a−b)+(b−c) = a−c.
2
Since the binary relation “· ≡· (mod H)” is an equivalence relation, it
partitions G into equivalence classes. It is easy to see (verify) that for any
a ∈G, the equivalence class containing a is precisely the set a+H := {a+h :
h ∈H}, and this set is called the coset of H in G containing a, and an
element of such a coset is called a representative of the coset.
Multiplicative notation:
if G is written multiplicatively, then a
≡
b (mod H) means a/b ∈H, and the coset of H in G containing a is
aH := {ah : h ∈H}.
Example 8.27. Let G := Z and H := nZ for some positive integer n. Then
TEAM LinG

8.3 Cosets and quotient groups
191
a ≡b (mod H) if and only if a ≡b (mod n). The coset a + H is exactly the
same thing as the residue class [a]n. 2
Example 8.28. Let G := Z4 and let H be the subgroup 2G = {[0], [2]} of
G. The coset of H containing [1] is {[1], [3]}. These are all the cosets of H
in G. 2
Theorem 8.15. Any two cosets of a subgroup H in an abelian group G
have equal cardinality; that is, there is a bijective map from one coset to the
other.
Proof. It suﬃces to exhibit a bijection between H and a + H for any a ∈G.
The map fa : H →a + H that sends h ∈H to a + h is easily seen to be just
such a bijection. 2
An incredibly useful consequence of the above theorem is:
Theorem 8.16 (Lagrange’s theorem). If G is a ﬁnite abelian group, and
H is a subgroup of G, then the order of H divides the order of G.
Proof. This is an immediate consequence of the previous theorem, and the
fact that the cosets of H in G partition G. 2
Analogous to Theorem 2.3, we have:
Theorem 8.17. Let G be an abelian group and H a subgroup.
For
a, a′, b, b′ ∈G, if a ≡a′ (mod H) and b ≡b′ (mod H), then a + b ≡
a′ + b′ (mod H).
Proof. Now, a ≡a′ (mod H) and b ≡b′ (mod H) means that a′ = a+h1 and
b′ = b+h2 for h1, h2 ∈H. Therefore, a′ +b′ = (a+h1)+(b+h2) = (a+b)+
(h1 + h2), and since h1 + h2 ∈H, this means that a + b ≡a′ + b′ (mod H).
2
Let G be an abelian group and H a subgroup. Theorem 8.17 allows us
to deﬁne a binary operation on the collection of cosets of H in G in the
following natural way: for a, b ∈G, deﬁne
(a + H) + (b + H) := (a + b) + H.
The fact that this deﬁnition is unambiguous follows immediately from The-
orem 8.17. Also, one can easily verify that this operation deﬁnes an abelian
group, where H acts as the identity element, and the inverse of a coset a+H
is (−a) + H. The resulting group is called the quotient group of G mod-
ulo H, and is denoted G/H. The order of the group G/H is sometimes
denoted [G : H] and is called the index of H in G.
TEAM LinG

192
Abelian groups
Multiplicative notation: if G is written multiplicatively, then the deﬁnition
of the group operation of G/H is expressed
(aH) · (bH) := (ab)H.
Theorem 8.18. Let G be a ﬁnite abelian group and H a subgroup. Then
[G : H] = |G|/|H|. Moreover, if H′ is another subgroup of G with H ⊆H′,
then
[G : H] = [G : H′][H′ : G].
Proof. The fact that [G : H] = |G|/|H| follows directly from Theorem 8.15.
The fact that [G : H] = [G : H′][H′ : G] follows from a simple calculation:
[G : H′] = |G|
|H′| = |G|/|H|
|H′|/|H| = [G : H]
[H′ : H]. 2
Example 8.29. For the additive group of integers Z and the subgroup nZ
for n > 0, the quotient group Z/nZ is precisely the same as the additive
group Zn that we have already deﬁned. For n = 0, Z/nZ is essentially just
a “renaming” of Z. 2
Example 8.30. Let G := Z6 and H = 3G be the subgroup of G consisting
of the two elements {[0], [3]}. The cosets of H in G are α := H = {[0], [3]},
β := [1] + H = {[1], [4]}, and γ := [2] + H = {[2], [5]}. If we write out an
addition table for G, grouping together elements in cosets of H in G, then
we also get an addition table for the quotient group G/H:
+
[0]
[3]
[1]
[4]
[2]
[5]
[0]
[0]
[3]
[1]
[4]
[2]
[5]
[3]
[3]
[0]
[4]
[1]
[5]
[2]
[1]
[1]
[4]
[2]
[5]
[3]
[0]
[4]
[4]
[1]
[5]
[2]
[0]
[3]
[2]
[2]
[5]
[3]
[0]
[4]
[1]
[5]
[5]
[2]
[0]
[3]
[1]
[4]
This table illustrates quite graphically the point of Theorem 8.17: for any
two cosets, if we take any element from the ﬁrst and add it to any element
of the second, we always end up in the same coset.
We can also write down just the addition table for G/H:
+
α
β
γ
α
α
β
γ
β
β
γ
α
γ
γ
α
β
TEAM LinG

8.3 Cosets and quotient groups
193
Note that by replacing α with [0]3, β with [1]3, and γ with [2]3, the
addition table for G/H becomes the addition table for Z3. In this sense, we
can view G/H as essentially just a “renaming” of Z3. 2
Example 8.31. Let us return to Example 8.24.
The group Z∗
15, as we
saw, is of order 8. The subgroup (Z∗
15)2 of Z∗
15 has order 2. Therefore, the
quotient group Z∗
15/(Z∗
15)2 has order 4. Indeed, the cosets are α00 = {[1], [4]},
α01 = {[−1], [−4]}, α10 = {[2], [−7]}, and α11 = {[7], [−2]}. In the quotient
group, α00 is the identity; moreover, we have
α2
01 = α2
10 = α2
11 = α00
and
α01α10 = α11, α10α11 = α01, α01α11 = α10.
This completely describes the behavior of the group operation of the quotient
group. Note that this group is essentially just a “renaming” of the group
Z2 × Z2. 2
Example 8.32. As we saw in Example 8.25, (Z∗
5)2 = {[±1]}. Therefore,
the quotient group Z∗
5/(Z∗
5)2 has order 2.
The cosets of (Z∗
5)2 in Z∗
5 are
α0 = {[±1]} and α1 = {[±2]}. In the group Z∗
5/(Z∗
5)2, α0 is the identity,
and α1 is its own inverse, and we see that this group is essentially just a
“renaming” of Z2. 2
Exercise 8.7. Let H be a subgroup of an abelian group G, and let a and
a′ be elements of G, with a ≡a′ (mod H).
(a) Show that −a ≡−a′ (mod H).
(b) Show that na ≡na′ (mod H) for all n ∈Z.
Exercise 8.8. Let G be an abelian group, and let ∼be an equivalence
relation on G. Further, suppose that for all a, a′, b ∈G, if a ∼a′, then
a + b ∼a′ + b. Let H := {a ∈G : a ∼0G}. Show that H is a subgroup of
G, and that for all a, b ∈G, we have a ∼b if and only if a ≡b (mod H).
Exercise 8.9. Let H be a subgroup of an abelian group G.
(a) Show that if H′ is a subgroup of G containing H, then H′/H is a
subgroup of G/H.
(b) Show that if K is a subgroup of G/H, then the set H′ := {a ∈G :
a + H ∈K} is a subgroup of G containing H.
TEAM LinG

194
Abelian groups
8.4 Group homomorphisms and isomorphisms
Deﬁnition 8.19. A group homomorphism is a function ρ from an
abelian group G to an abelian group G′ such that ρ(a + b) = ρ(a) + ρ(b)
for all a, b ∈G.
Note that in the equality ρ(a + b) = ρ(a) + ρ(b) in the above deﬁnition,
the addition on the left-hand side is taking place in the group G while the
addition on the right-hand side is taking place in the group G′.
Two sets play a critical role in understanding a group homomorphism
ρ : G →G′. The ﬁrst set is the image of ρ, that is, the set ρ(G) = {ρ(a) :
a ∈G}. The second set is the kernel of ρ, deﬁned as the set of all elements
of G that are mapped to 0G′ by ρ, that is, the set ρ−1({0G′}) = {a ∈G :
ρ(a) = 0G′}. We introduce the following notation for these sets: img(ρ)
denotes the image of ρ, and ker(ρ) denotes the kernel of ρ.
Example 8.33. For any abelian group G and any integer m, the map that
sends a ∈G to ma ∈G is clearly a group homomorphism from G into
G, since for a, b ∈G, we have m(a + b) = ma + mb. The image of this
homomorphism is mG and the kernel is G{m}. We call this map the m-
multiplication map on G. If G is written multiplicatively, we call this
the m-power map on G, and its image is Gm. 2
Example 8.34. Consider the m-multiplication map on Zn. As we saw in
Example 8.21, if d := gcd(n, m), the image mZn of this map is a subgroup
of Zn of order n/d, while its kernel Zn{m} is a subgroup of order d. 2
Example 8.35. Let G be an abelian group and let a be a ﬁxed element of
G. Let ρ : Z →G be the map that sends z ∈Z to za ∈G. It is easy to see
that this is group homomorphism, since
ρ(z + z′) = (z + z′)a = za + z′a = ρ(z) + ρ(z′). 2
Example 8.36. As a special case of the previous example, let n be a positive
integer and let α be an element of Z∗
n.
Let ρ : Z →Z∗
n be the group
homomorphism that sends z ∈Z to αz ∈Z∗
n. If the multiplicative order of
α is equal to k, then as discussed in §2.5, the image of ρ consists of the k
distinct group elements α0, α1, . . . , αk−1. The kernel of ρ consists of those
integers a such that αa = [1]n. Again by the discussion in §2.5, the kernel
of ρ is equal to kZ. 2
Example 8.37. We may generalize Example 8.35 as follows. Let G be an
abelian group, and let a1, . . . , ak be ﬁxed elements of G. Let ρ : Z×k →G
TEAM LinG

8.4 Group homomorphisms and isomorphisms
195
be the map that sends (z1, . . . , zk) ∈Z×k to z1a1 + · · · + zkak ∈G. The
reader may easily verify that ρ is a group homomorphism. 2
Example 8.38. As a special case of the previous example, let p1, . . . , pk
be distinct primes, and let ρ : Z×k →Q∗be the group homomorphism that
sends (z1, . . . , zk) ∈Z×k to pz1
1 · · · pzk
k ∈Q∗. The image of ρ is the set of all
non-zero fractions whose numerator and denominator are divisible only by
the primes p1, . . . , pk. The kernel of ρ contains only the all-zero tuple 0×k.
2
The following theorem summarizes some of the most important properties
of group homomorphisms.
Theorem 8.20. Let ρ be a group homomorphism from G to G′.
(i) ρ(0G) = 0G′.
(ii) ρ(−a) = −ρ(a) for all a ∈G.
(iii) ρ(na) = nρ(a) for all n ∈Z and a ∈G.
(iv) For any subgroup H of G, ρ(H) is a subgroup of G′.
(v) ker(ρ) is a subgroup of G.
(vi) For all a, b ∈G, ρ(a) = ρ(b) if and only if a ≡b (mod ker(ρ)).
(vii) ρ is injective if and only if ker(ρ) = {0G}.
(viii) For any subgroup H′ of G′, ρ−1(H′) is a subgroup of G containing
ker(ρ).
Proof.
(i) We have
0G′ + ρ(0G) = ρ(0G) = ρ(0G + 0G) = ρ(0G) + ρ(0G).
Now cancel ρ(0G) from both sides (using part (i) of Theorem 8.3).
(ii) We have
0G′ = ρ(0G) = ρ(a + (−a)) = ρ(a) + ρ(−a),
and hence ρ(−a) is the inverse of ρ(a).
(iii) For n = 0, this follows from part (i). For n > 0, this follows from
the deﬁnitions by induction on n. For n < 0, this follows from the
positive case and part (v) of Theorem 8.3.
(iv) For any a, b ∈H, we have a + b ∈H and −a ∈H; hence, ρ(H)
contains ρ(a + b) = ρ(a) + ρ(b) and ρ(−a) = −ρ(a).
TEAM LinG

196
Abelian groups
(v) If ρ(a) = 0G′ and ρ(b) = 0G′, then ρ(a+b) = ρ(a)+ρ(b) = 0G′ +0G′ =
0G′, and ρ(−a) = −ρ(a) = −0G′ = 0G′.
(vi) ρ(a) = ρ(b) iﬀρ(a) −ρ(b) = 0G′ iﬀρ(a −b) = 0G′ iﬀa −b ∈ker(ρ) iﬀ
a ≡b (mod ker(ρ)).
(vii) If ρ is injective, then in particular, ρ−1({0G′}) cannot contain any
other element besides 0G. If ρ is not injective, then there exist two
distinct elements a, b ∈G with ρ(a) = ρ(b), and by part (vi), ker(ρ)
contains the element a −b, which is non-zero.
(viii) This is very similar to part (v). If ρ(a) ∈H′ and ρ(b) ∈H′, then
ρ(a + b) = ρ(a) + ρ(b) ∈H′, and ρ(−a) = −ρ(a) ∈H′. Moreover,
since H′ contains 0G′, we must have ρ−1(H′) ⊇ρ−1({0G′}) = ker(ρ).
2
Part (vii) of the above theorem is particular useful: to check that a group
homomorphism is injective, it suﬃces to determine if ker(ρ) = {0G}. Thus,
the injectivity and surjectivity of a given group homomorphism ρ : G →G′
may be characterized in terms of its kernel and image:
• ρ is injective if and only if ker(ρ) = {0G};
• ρ is surjective if and only if img(ρ) = G′.
The next three theorems establish some further convenient facts about
group homomorphisms.
Theorem 8.21. If ρ : G →G′ and ρ′ : G′ →G′′ are group homomorphisms,
then so is their composition ρ′ ◦ρ : G →G′′.
Proof. For a, b ∈G, we have ρ′(ρ(a + b)) = ρ′(ρ(a) + ρ(b)) = ρ′(ρ(a)) +
ρ′(ρ(b)). 2
Theorem 8.22. Let ρi : G →Gi, for i = 1, . . . , n, be group homo-
morphisms.
Then the map ρ : G →G1 × · · · × Gn that sends a ∈G
to (ρ1(a), . . . , ρn(a)) is a group homomorphism with kernel ker(ρ1) ∩· · · ∩
ker(ρn).
Proof. Exercise. 2
Theorem 8.23. Let ρi : Gi →G, for i = 1, . . . , n, be group homomor-
phisms. Then the map ρ : G1 × · · · × Gn →G that sends (a1, . . . , an) to
ρ1(a1) + · · · + ρn(an) is a group homomorphism.
Proof. Exercise. 2
Consider a group homomorphism ρ : G →G′. If ρ is bijective, then ρ is
TEAM LinG

8.4 Group homomorphisms and isomorphisms
197
called a group isomorphism of G with G′. If such a group isomorphism
ρ exists, we say that G is isomorphic to G′, and write G ∼= G′. Moreover,
if G = G′, then ρ is called a group automorphism on G.
Theorem 8.24. If ρ is a group isomorphism of G with G′, then the inverse
function ρ−1 is a group isomorphism of G′ with G.
Proof. For a′, b′ ∈G′, we have
ρ(ρ−1(a′) + ρ−1(b′)) = ρ(ρ−1(a′)) + ρ(ρ−1(b′)) = a′ + b′,
and hence ρ−1(a′) + ρ−1(b′) = ρ−1(a′ + b′). 2
Because of this theorem, if G is isomorphic to G′, we may simply say that
“G and G′ are isomorphic.”
We stress that a group isomorphism of G with G′ is essentially just a
“renaming” of the group elements—all structural properties of the group
are preserved, even though the two groups might look quite diﬀerent super-
ﬁcially.
Example 8.39. As was shown in Example 8.30, the quotient group G/H
discussed in that example is isomorphic to Z3.
As was shown in Exam-
ple 8.31, the quotient group Z∗
15/(Z∗
15)2 is isomorphic to Z2 × Z2. As was
shown in Example 8.32, the quotient group Z∗
5/(Z∗
5)2 is isomorphic to Z2. 2
Example 8.40. If gcd(n, m) = 1, then the m-multiplication map on Zn is
a group automorphism. 2
The following four theorems provide important constructions of group
homomorphisms.
Theorem 8.25. If H is a subgroup of an abelian group G, then the map
ρ : G →G/H given by ρ(a) = a + H is a surjective group homomorphism
whose kernel is H.
Proof. This really just follows from the deﬁnition of the quotient group. To
verify that ρ is a group homomorphism, note that
ρ(a + b) = (a + b) + H = (a + H) + (b + H) = ρ(a) + ρ(b).
Surjectivity follows from the fact that every coset is of the form a + H for
some a ∈G. The fact that ker(ρ) = H follows from the fact that a + H is
the coset of H in G containing a, and so this is equal to H if and only if
a ∈H. 2
The homomorphism of the above theorem is called the natural map from
G to G/H.
TEAM LinG

198
Abelian groups
Theorem 8.26. Let ρ be a group homomorphism from G into G′. Then
the map ¯ρ : G/ ker(ρ) →img(ρ) that sends the coset a + ker(ρ) for a ∈G
to ρ(a) is unambiguously deﬁned and is a group isomorphism of G/ ker(ρ)
with img(ρ).
Proof. Let K := ker(ρ). To see that the deﬁnition ¯ρ is unambiguous, note
that if a ≡a′ (mod K), then by part (vi) of Theorem 8.20, ρ(a) = ρ(a′). To
see that ¯ρ is a group homomorphism, note that
¯ρ((a + K) + (b + K)) = ¯ρ((a + b) + K) = ρ(a + b) = ρ(a) + ρ(b)
= ¯ρ(a + K) + ¯ρ(b + K).
It is clear that ¯ρ maps onto img(ρ), since any element of img(ρ) is of the form
ρ(a) for some a ∈G, and the map ¯ρ sends a+K to ρ(a). Finally, to see that
¯ρ is injective, suppose that ¯ρ(a + K) = 0G′; then we have ρ(a) = 0G′, and
hence a ∈K; from this, it follows that a+K is equal to K, which is the zero
element of G/K. Injectivity then follows from part (vii) of Theorem 8.20,
applied to ¯ρ. 2
The following theorem is an easy generalization of the previous one.
Theorem 8.27. Let ρ be a group homomorphism from G into G′. Then for
any subgroup H contained in ker(ρ), the map ¯ρ : G/H →img(ρ) that sends
the coset a + H for a ∈G to ρ(a) is unambiguously deﬁned and is a group
homomorphism from G/H onto img(ρ) with kernel ker(ρ)/H.
Proof. Exercise—just mimic the proof of the previous theorem. 2
Theorem 8.28. Let G be an abelian group with subgroups H1, H2. Then
the map ρ : H1 ×H2 →H1 +H2 that sends (h1, h2) to h1 +h2 is a surjective
group homomorphism. Moreover, if H1 ∩H2 = {0G}, then ρ is a group
isomorphism of H1 × H2 with H1 + H2.
Proof. The fact that ρ is a group homomorphism is just a special case
of Theorem 8.23, applied to the inclusion maps ρ1 : H1 →H1 + H2 and
ρ2 : H2 →H1 + H2. One can also simply verify this by direct calculation:
for h1, h′
1 ∈H1 and h2, h′
2 ∈H2, we have
ρ(h1 + h′
1, h2 + h′
2) = (h1 + h′
1) + (h2 + h′
2)
= (h1 + h2) + (h′
1 + h′
2)
= ρ(h1, h2) + ρ(h′
1, ρ′
2).
Moreover, from the deﬁnition of H1 + H2, we see that ρ is in fact surjective.
Now assume that H1 ∩H2 = {0G}. To see that ρ is injective, it suﬃces
TEAM LinG

8.4 Group homomorphisms and isomorphisms
199
to show that ker(ρ) is trivial; that is, it suﬃces to show that for all h1 ∈H1
and h2 ∈H2, h1 + h2 = 0G implies h1 = 0G and h2 = 0G. But h1 + h2 = 0G
implies h1 = −h2 ∈H2, and hence h1 ∈H1 ∩H2 = {0G}, and so h1 = 0G.
Similarly, one shows that h2 = 0G, and that ﬁnishes the proof. 2
Example 8.41. For n ≥1, the natural map ρ from Z to Zn sends a ∈Z to
the residue class [a]n. This map is a surjective group homomorphism with
kernel nZ. 2
Example 8.42. We may restate the Chinese remainder theorem (Theo-
rem 2.8) in more algebraic terms.
Let n1, . . . , nk be pairwise relatively
prime, positive integers. Consider the map from the group Z to the group
Zn1 × · · · × Znk that sends x ∈Z to ([x]n1, . . . , [x]nk). It is easy to see that
this map is a group homomorphism (this follows from Example 8.41 and
Theorem 8.22). In our new language, the Chinese remainder theorem says
that this group homomorphism is surjective and that the kernel is nZ, where
n = k
i=1 ni. Therefore, by Theorem 8.26, the map that sends [x]n ∈Zn
to ([x]n1, . . . , [x]nk) is a group isomorphism of the group Zn with the group
Zn1 × · · · × Znk. 2
Example 8.43. Let n1, n2 be positive integers with n1 > 1 and n1 | n2.
Then the map ¯ρ : Zn2 →Zn1 that sends [a]n2 to [a]n1 is a surjective group
homomorphism, and [a]n2 ∈ker(¯ρ) if and only if n1 | a; that is, ker(¯ρ) =
n1Zn2. The map ¯ρ can also be viewed as the map obtained by applying
Theorem 8.27 with the natural map ρ from Z to Zn1 and the subgroup n2Z
of Z, which is contained in ker(ρ) = n1Z. 2
Example 8.44. Let us reconsider Example 8.21. Let n be a positive in-
teger, let m ∈Z, and consider the subgroup mZn of the additive group
Zn. Let ρ1 : Z →Zn be the natural map, and let ρ2 : Zn →Zn be the
m-multiplication map. The composed map ρ = ρ2 ◦ρ1 from Z to Zn is also
a group homomorphism. The kernel of ρ consists of those integers a such
that am ≡0 (mod n), and so Theorem 2.7 implies that ker(ρ) = (n/d)Z,
where d := gcd(m, n). The image of ρ is mZn. Theorem 8.26 therefore
implies that the map ¯ρ : Zn/d →mZn that sends [a]n/d to [ma]n is a group
isomorphism. 2
Exercise 8.10. Verify that the “is isomorphic to” relation on abelian groups
is an equivalence relation; that is, for all abelian groups G1, G2, G3, we have:
(a) G1 ∼= G1;
(b) G1 ∼= G2 implies G2 ∼= G1;
TEAM LinG

200
Abelian groups
(c) G1 ∼= G2 and G2 ∼= G3 implies G1 ∼= G3.
Exercise 8.11. Let G1, G2 be abelian groups, and let ρ : G1 × G2 →G1
be the map that sends (a1, a2) ∈G1 × G2 to a1 ∈G1. Show that ρ is a
surjective group homomorphism whose kernel is {0G1} × G2.
Exercise 8.12. Suppose that G, G1, and G2 are abelian groups, and that
ρ : G1 × G2 →G is a group isomorphism. Let H1 := ρ(G1 × {0G2}) and
H2 := ρ({0G1} × G2). Show that
(a) H1 and H2 are subgroups of G,
(b) H1 + H2 = G, and
(c) H1 ∩H2 = {0G}.
Exercise 8.13. Let ρ be a group homomorphism from G into G′. Show
that for any subgroup H of G, we have ρ−1(ρ(H)) = H + ker(ρ).
Exercise 8.14. Let ρ be a group homomorphism from G into G′. Show
that the subgroups of G containing ker(ρ) are in one-to-one correspondence
with the subgroups of img(ρ), where the subgroup H of G containing ker(ρ)
corresponds to the subgroup ρ(H) of img(ρ).
Exercise 8.15. Let G be an abelian group with subgroups H ⊆H′.
(a) Show that we have a group isomorphism
G/H′ ∼= G/H
H′/H .
(b) Show that if [G : H] is ﬁnite (even though G itself may have inﬁnite
order), then [G : H] = [G : H′] · [H′ : H].
Exercise 8.16. Show that if G = G1 × G2 for abelian groups G1 and G2,
and H1 is a subgroup of G1 and H2 is a subgroup of G2, then G/(H1×H2) ∼=
G1/H1 × G2/H2.
Exercise 8.17. Let ρ1 and ρ2 be group homomorphisms from G into G′.
Show that the map ρ : G →G′ that sends a ∈G to ρ1(a) + ρ2(a) ∈G′ is
also a group homomorphism.
Exercise 8.18. Let G and G′ be abelian groups. Consider the set H of all
group homomorphisms ρ : G →G′. This set is non-empty, since the map
that sends everything in G to 0G′ is trivially an element of H. We may deﬁne
an addition operation on H as follows: for ρ1, ρ2 ∈H, let ρ1+ρ2 be the map
ρ : G →G′ that sends a ∈G to ρ1(a) + ρ2(a). By the previous exercise, ρ is
TEAM LinG

8.4 Group homomorphisms and isomorphisms
201
also in H, and so this addition operation is a well-deﬁned binary operation
on H. Show that H, together with this addition operation, forms an abelian
group.
Exercise 8.19. This exercise develops an alternative, “quick and dirty”
proof of the Chinese remainder theorem, based on group theory and a count-
ing argument. Let n1, . . . , nk be pairwise relatively prime, positive integers,
and let n := n1 · · · nk. Consider the map ρ : Z →Zn1 × · · · × Znk that sends
x ∈Z to ([x]n1, . . . , [x]nk).
(a) Using the results of Example 8.41 and Theorem 8.22, show (directly)
that ρ is a group homomorphism with kernel nZ.
(b) Using Theorem 8.26, conclude that the map ¯ρ given by that theorem,
which sends [x]n to ([x]n1, . . . , [x]nk), is an injective group homomor-
phism from Zn into Zn1 × · · · × Znk.
(c) Since |Zn| = n = |Zn1 × · · · × Znk|, conclude that the map ¯ρ is
surjective, and so is an isomorphism between Zn and Zn1 ×· · ·×Znk.
Although simple, this proof does not give us an explicit formula for comput-
ing ¯ρ−1.
Exercise 8.20. Let p be an odd prime; consider the squaring map on Z∗
p.
(a) Using Exercise 2.5, show that the kernel of the squaring map on Z∗
p
consists of the two elements [±1]p.
(b) Using the results of this section, conclude that there are (p −1)/2
squares in Z∗
p, each of which has precisely two square roots in Z∗
p.
Exercise 8.21. Consider the group homomorphism ρ : Z × Z × Z →Q∗
that sends (a, b, c) to 2a3b12c. Describe the image and kernel of ρ.
Exercise 8.22. This exercise develops some simple — but extremely use-
ful—connections between group theory and probability theory. Let ρ : G →
G′ be a group homomorphism, where G and G′ are ﬁnite abelian groups.
(a) Show that if g is a random variable with the uniform distribution on
G, then ρ(g) is a random variable with the uniform distribution on
img(ρ).
(b) Show that if g is a random variable with the uniform distribution
on G, and g′ is a ﬁxed element in img(ρ), then the conditional dis-
tribution of g, given that ρ(g) = g′, is the uniform distribution on
ρ−1({g′}).
(c) Show that if g′
1 is a ﬁxed element of G′, g1 is uniformly distributed
TEAM LinG

202
Abelian groups
over ρ−1({g′
1}), g′
2 is a ﬁxed element of G′, and g2 is a ﬁxed element of
ρ−1({g′
2}), then g1 + g2 is uniformly distributed over ρ−1({g′
1 + g′
2}).
(d) Show that if g′
1 is a ﬁxed element of G′, g1 is uniformly distributed
over ρ−1({g′
1}), g′
2 is a ﬁxed element of G′, g2 is uniformly distributed
over ρ−1({g′
2}), and g1 and g2 are independent, then g1 + g2 is uni-
formly distributed over ρ−1({g′
1 + g′
2}).
8.5 Cyclic groups
Let G be an abelian group. For a ∈G, deﬁne ⟨a⟩:= {za : z ∈Z}. It is
easy to see that ⟨a⟩is a subgroup of G—indeed, it is the image of the group
homomorphism discussed in Example 8.35. Moreover, ⟨a⟩is the smallest
subgroup of G containing a; that is, ⟨a⟩contains a, and any subgroup H
of G that contains a must also contain ⟨a⟩. The subgroup ⟨a⟩is called the
subgroup (of G) generated by a. Also, one deﬁnes the order of a to be
the order of the subgroup ⟨a⟩.
More generally, for a1, . . . , ak ∈G, we deﬁne ⟨a1, . . . , ak⟩:= {z1a1 + · · · +
zkak : z1, . . . , zk ∈Z}.
One also veriﬁes that ⟨a1, . . . , ak⟩is a subgroup
of G, and indeed, is the smallest subgroup of G that contains a1, . . . , ak.
The subgroup ⟨a1, . . . , ak⟩is called the subgroup (of G) generated by
a1, . . . , ak.
An abelian group G is said to be cyclic if G = ⟨a⟩for some a ∈G, in
which case, a is called a generator for G. An abelian group G is said to
be ﬁnitely generated if G = ⟨a1, . . . , ak⟩for some a1, . . . , ak ∈G.
Multiplicative notation: if G is written multiplicatively, then ⟨a⟩:= {az :
z ∈Z}, and ⟨a1, . . . , ak⟩:= {az1
1 · · · azk
k : z1, . . . , zk ∈Z}; also, for emphasis
and clarity, we use the term multiplicative order of a.
Classiﬁcation of cyclic groups.
We can very easily classify all cyclic
groups. Suppose that G is a cyclic group with generator a. Consider the
map ρ : Z →G that sends z ∈Z to za ∈G. As discussed in Example 8.35,
this map is a group homomorphism, and since a is a generator for G, it must
be surjective.
Case 1: ker(ρ) = {0}. In this case, ρ is an isomorphism of Z with G.
Case 2: ker(ρ) ̸= {0}. In this case, since ker(ρ) is a subgroup of Z diﬀerent
from {0}, by Theorem 8.8, it must be of the form nZ for some n > 0.
Hence, by Theorem 8.26, the map ¯ρ : Zn →G that sends [z]n to za
is an isomorphism of Zn with G.
So we see that a cyclic group is isomorphic either to the additive group Z
TEAM LinG

8.5 Cyclic groups
203
or the additive group Zn, for some positive integer n. We have thus classiﬁed
all cyclic groups “up to isomorphism.” From this classiﬁcation, we obtain:
Theorem 8.29. Let G be an abelian group and let a ∈G.
(i) If there exists a positive integer m such that ma = 0G, then the least
such positive integer n is the order of a; in this case, we have:
– for any integer z, za = 0G if and only if n | z, and more
generally, for integers z1, z2, z1a = z2a if and only if z1 ≡
z2 (mod n);
– the subgroup ⟨a⟩consists of the n distinct elements
0 · a, 1 · a, . . . , (n −1) · a.
(ii) If G has ﬁnite order, then |G|·a = 0G and the order of a divides |G|.
Proof. Part (i) follows immediately from the above classiﬁcation, along with
part (vi) of Theorem 8.20. Part (ii) follows from part (i), along with La-
grange’s theorem (Theorem 8.16), since ⟨a⟩is a subgroup of G. 2
Example 8.45. The additive group Z is a cyclic group generated by 1. The
only other generator is −1. More generally, the subgroup of Z generated by
m ∈Z is mZ. 2
Example 8.46. The additive group Zn is a cyclic group generated by [1]n.
More generally, for m ∈Z, the subgroup of Zn generated by [m]n is equal
to mZn, which by Example 8.21 has order n/ gcd(m, n). In particular, [m]n
generates Zn if and only if m is relatively prime to n, and hence, the number
of generators of Zn is φ(n). 2
Example 8.47. Consider the additive group G := Zn1 × Zn2, and let α :=
([1]n1, [1]n2) ∈Zn1 × Zn2. For m ∈Z, we have mα = 0G if and only if
n1 | m and n2 | m. This implies that α generates a subgroup of G of order
lcm(n1, n2).
Suppose that gcd(n1, n2) = 1. From the above discussion, it follows that
G is cyclic of order n1n2. One could also see this directly using the Chinese
remainder theorem: as we saw in Example 8.42, the Chinese remainder
theorem gives us an isomorphism of G with the cyclic group Zn1n2.
Conversely, if d := gcd(n1, n2) > 1, then all elements of Zn1 × Zn2 have
order dividing n1n2/d, and so Zn1 × Zn2 cannot be cyclic. 2
Example 8.48. For a, n ∈Z with n > 0 and gcd(a, n) = 1, the deﬁnition
in this section of the multiplicative order of α := [a]n ∈Z∗
n is consistent
TEAM LinG

204
Abelian groups
with that given in §2.5, and is also the same as the multiplicative order of a
modulo n. Indeed, Euler’s theorem (Theorem 2.15) is just a special case of
part (ii) of Theorem 8.29. Also, α is a generator for Z∗
n if and only if a is a
primitive root modulo n. 2
Example 8.49. As we saw in Example 8.24, all elements of Z∗
15 have mul-
tiplicative order dividing 4, and since Z∗
15 has order 8, we conclude that Z∗
15
is not cyclic. 2
Example 8.50. The group Z∗
5 is cyclic, with [2] being a generator:
[2]2 = [4] = [−1],
[2]3 = [−2],
[2]4 = [1]. 2
Example 8.51. Based on the calculations in Example 2.6, we may conclude
that Z∗
7 is cyclic, with both [3] and [5] being generators. 2
The following two theorems completely characterize the subgroup struc-
ture of cyclic groups. Actually, we have already proven the results in these
two theorems, but nevertheless, these results deserve special emphasis.
Theorem 8.30. Let G be a cyclic group of inﬁnite order.
(i) G is isomorphic to Z.
(ii) The subgroups of G are in one-to-one correspondence with the non-
negative integers, where each such integer m corresponds to the cyclic
group mG.
(iii) For any two non-negative integers m, m′, mG ⊆m′G if and only if
m′ | m.
Proof. That G ∼= Z was established in our classiﬁcation of cyclic groups, it
suﬃces to prove the other statements of the theorem for G = Z. It is clear
that for any integer m, the subgroup mZ is cyclic, as m is a generator. This
fact, together with Theorem 8.8, establish all the other statements. 2
Theorem 8.31. Let G be a cyclic group of ﬁnite order n.
(i) G is isomorphic to Zn.
(ii) The subgroups of G are in one-to-one correspondence with the positive
divisors of n, where each such divisor d corresponds to the subgroup
dG; moreover, dG is a cyclic group of order n/d.
(iii) For each positive divisor d of n, we have dG = G{n/d}; that is, the
kernel of the (n/d)-multiplication map is equal to the image of the
d-multiplication map; in particular, G{n/d} has order n/d.
TEAM LinG

8.5 Cyclic groups
205
(iv) For any two positive divisors d, d′ of n, we have dG ⊆d′G if and only
if d′ | d.
(v) For any positive divisor d of n, the number of elements of order d in
G is φ(d).
(vi) For any integer m, we have mG = dG and G{m} = G{d}, where
d := gcd(m, n).
Proof. That G ∼= Zn was established in our classiﬁcation of cyclic groups,
and so it suﬃces to prove the other statements of the theorem for G = Zn.
The one-to-one correspondence in part (ii) was established in Theorem 8.9.
The fact that dZn is cyclic of order n/d can be seen in a number of ways;
indeed, in Example 8.44 we constructed an isomorphism of Zn/d with dZn.
Part (iii) was established in Example 8.21.
Part (iv) was established in Theorem 8.9.
For part (v), the elements of order d in Zn are all contained in Zn{d},
and so the number of such elements is equal to the number of generators of
Zn{d}. The group Zn{d} is cyclic of order d, and so is isomorphic to Zd,
and as we saw in Example 8.46, this group has φ(d) generators.
Part (vi) was established in Example 8.21. 2
Since cyclic groups are in some sense the simplest kind of abelian group,
it is nice to have some suﬃcient conditions under which a group must be
cyclic. The following theorems provide such conditions.
Theorem 8.32. If G is an abelian group of prime order, then G is cyclic.
Proof. Let |G| = p. Let a ∈G with a ̸= 0G, and let k be the order of a. As
the order of an element divides the order of the group, we have k | p, and
so k = 1 or k = p. Since a ̸= 0G, we must have k ̸= 1, and so k = p, which
implies that a generates G. 2
Theorem 8.33. If G1 and G2 are ﬁnite cyclic groups of relatively prime
order, then G1 × G2 is also cyclic.
Proof. This follows from Example 8.47, together with our classiﬁcation of
cyclic groups. 2
Theorem 8.34. Any subgroup of a cyclic group is cyclic.
Proof. This is just a restatement of part (ii) of Theorem 8.30 and part (ii)
of Theorem 8.31 2
Theorem 8.35. If ρ : G →G′ is a group homomorphism, and G is cyclic,
then img(G) is cyclic.
TEAM LinG

206
Abelian groups
Proof. If G is generated by a, then it is easy to see that the image of ρ is
generated by ρ(a). 2
The next three theorems are often useful in calculating the order of a
group element.
Theorem 8.36. Let G be an abelian group, let a ∈G be of ﬁnite order n,
and let m be an arbitrary integer. Then the order of ma is n/ gcd(m, n).
Proof. By our classiﬁcation of cyclic groups, we know that the subgroup ⟨a⟩
is isomorphic to Zn, where under this isomorphism, a corresponds to [1]n and
ma corresponds to [m]n. The theorem then follows from the observations in
Example 8.46. 2
Theorem 8.37. Suppose that a is an element of an abelian group, and for
some prime p and integer e ≥1, we have pea = 0G and pe−1a ̸= 0G. Then
a has order pe.
Proof. If m is the order of a, then since pea = 0G, we have m | pe. So
m = pf for some f = 0, . . . , e. If f < e, then pe−1a = 0G, contradicting the
assumption that pe−1a ̸= 0G. 2
Theorem 8.38. Suppose G is an abelian group with a1, a2 ∈G such that
a1 is of ﬁnite order n1, a2 is of ﬁnite order n2, and gcd(n1, n2) = 1. Then
the order of a1 + a2 is n1n2.
Proof. Let m be the order of a1 + a2. It is clear that n1n2(a1 + a2) = 0G,
and hence m divides n1n2.
We claim that ⟨a1⟩∩⟨a2⟩= {0G}. To see this, suppose a ∈⟨a1⟩∩⟨a2⟩.
Then since a ∈⟨a1⟩, the order of a must divide n1. Likewise, since a ∈⟨a2⟩,
the order of a must divide n2. From the assumption that gcd(n1, n2) = 1,
it follows that the order of a must be 1, meaning that a = 0G.
Since m(a1 + a2) = 0G, it follows that ma1 = −ma2. This implies that
ma1 belongs to ⟨a2⟩, and since ma1 trivially belongs to ⟨a1⟩, we see that
ma1 belongs to ⟨a1⟩∩⟨a2⟩. From the above claim, it follows that ma1 = 0G,
and hence n1 divides m. By a symmetric argument, we see that n2 divides
m. Again, since gcd(n1, n2) = 1, we see that n1n2 divides m. 2
For an abelian group G, we say that an integer k kills G if kG = {0G}.
Consider the set KG of integers that kill G. Evidently, KG is a subgroup of
Z, and hence of the form mZ for a uniquely determined non-negative integer
m. This integer m is called the exponent of G. If m ̸= 0, then we see that
m is the least positive integer that kills G.
We ﬁrst state some basic properties.
TEAM LinG

8.5 Cyclic groups
207
Theorem 8.39. Let G be an abelian group of exponent m.
(i) For any integer k such that kG = {0G}, we have m | k.
(ii) If G has ﬁnite order, then m divides |G|.
(iii) If m ̸= 0, then for any a ∈G, the order of a is ﬁnite, and the order
of a divides m.
(iv) If G is cyclic, then the exponent of G is 0 if G is inﬁnite, and is |G|
is G is ﬁnite.
Proof. Exercise. 2
The next two theorems develop some crucial properties about the struc-
ture of ﬁnite abelian groups.
Theorem 8.40. If a ﬁnite abelian group G has exponent m, then G contains
an element of order m. In particular, a ﬁnite abelian group is cyclic if and
only if its order equals its exponent.
Proof. The second statement follows immediately from the ﬁrst. For the
ﬁrst statement, assume that m > 1, and let m = r
i=1 pei
i
be the prime
factorization of m.
First, we claim that for each i = 1, . . . , r, there exists ai ∈G such that
(m/pi)ai ̸= 0G. Suppose the claim were false: then for some i, (m/pi)a = 0G
for all a ∈G; however, this contradicts the minimality property in the
deﬁnition of the exponent m. That proves the claim.
Let a1, . . . , ar be as in the above claim. Then by Theorem 8.37, (m/pei
i )ai
has order pei
i
for each i = 1, . . . , r. Finally, by Theorem 8.38, the group
element
(m/pe1
1 )a1 + · · · + (m/per
r )ar
has order m. 2
Theorem 8.41. Let G be a ﬁnite abelian group of order n. If p is a prime
dividing n, then G contains an element of order p.
Proof. We can prove this by induction on n.
If n = 1, then the theorem is vacuously true.
Now assume n > 1 and that the theorem holds for all groups of order
strictly less than n. Let a be any non-zero element of G, and let m be the
order of a. Since a is non-zero, we must have m > 1. If p | m, then (m/p)a is
an element of order p, and we are done. So assume that p ∤m and consider
the quotient group G/H, where H is the subgroup of G generated by a.
Since H has order m, G/H has order n/m, which is strictly less than n,
TEAM LinG

208
Abelian groups
and since p ∤m, we must have p | (n/m). So we can apply the induction
hypothesis to the group G/H and the prime p, which says that there is an
element b ∈G such that b + H ∈G/H has order p. If ℓis the order of b,
then ℓb = 0G, and so ℓb ≡0G (mod H), which implies that the order of
b + H divides ℓ. Thus, p | ℓ, and so (ℓ/p)b is an element of G of order p. 2
As a corollary, we have:
Theorem 8.42. Let G be a ﬁnite abelian group. Then the primes dividing
the exponent of G are the same as the primes dividing its order.
Proof. Since the exponent divides the order, any prime dividing the exponent
must divide the order. Conversely, if a prime p divides the order, then since
there is an element of order p in the group, the exponent must be divisible
by p. 2
Exercise 8.23. Let G be an abelian group of order n, and let m be an
integer. Show that mG = G if and only if gcd(m, n) = 1.
Exercise
8.24.
Let G be an abelian group of order mm′,
where
gcd(m, m′) = 1. Consider the map ρ : mG × m′G to G that sends (a, b)
to a + b. Show that ρ is a group isomorphism.
Exercise 8.25. Let G be an abelian group, a ∈G, and m ∈Z, such that
m > 0 and ma = 0G. Let m = pe1
1 · · · per
r
be the prime factorization of m.
For i = 1, . . . , r, let fi be the largest non-negative integer such that fi ≤ei
and m/pfi
i · a = 0G. Show that the order of a is equal to pe1−f1
1
· · · per−fr
r
.
Exercise 8.26. Show that for ﬁnite abelian groups G1, G2 whose exponents
are m1 and m2, the exponent of G1 × G2 is lcm(m1, m2).
Exercise 8.27. Give an example of an abelian group G whose exponent is
zero, but where every element of G has ﬁnite order.
Exercise 8.28. Show how Theorem 2.11 easily follows from Theorem 8.31.
8.6 The structure of ﬁnite abelian groups (∗)
We next state a theorem that classiﬁes all ﬁnite abelian groups up to iso-
morphism.
Theorem 8.43 (Fundamental theorem of ﬁnite abelian groups). A
ﬁnite abelian group (with more than one element) is isomorphic to a direct
TEAM LinG

8.6 The structure of ﬁnite abelian groups (∗)
209
product of cyclic groups
Zpe1
1 × · · · × Zper
r ,
where the pi are primes (not necessarily distinct) and the ei are positive
integers. This direct product of cyclic groups is unique up to the order of
the factors.
An alternative statement of this theorem is the following:
Theorem 8.44. A ﬁnite abelian group (with more than one element) is
isomorphic to a direct product of cyclic groups
Zm1 × · · · × Zmt,
where each mi > 1, and where for i = 1, . . . , t −1, we have mi | mi+1.
Moreover, the integers m1, . . . , mt are uniquely determined, and mt is the
exponent of the group.
Exercise 8.29. Show that Theorems 8.43 and 8.44 are equivalent; that is,
show that each one implies the other. To do this, give a natural one-to-one
correspondence between sequences of prime powers (as in Theorem 8.43)
and sequences of integers m1, . . . , mt (as in Theorem 8.44), and also make
use of Example 8.47.
Exercise 8.30. Using the fundamental theorem of ﬁnite abelian groups
(either form), give short and simple proofs of Theorems 8.40 and 8.41.
We now prove Theorem 8.44, which we break into two lemmas, the ﬁrst
of which proves the existence part of the theorem, and the second of which
proves the uniqueness part.
Lemma 8.45. A ﬁnite abelian group (with more than one element) is iso-
morphic to a direct product of cyclic groups
Zm1 × · · · × Zmt,
where each mi > 1, and where for i = 1, . . . , t −1, we have mi | mi+1;
moreover, mt is the exponent of the group.
Proof. Let G be a ﬁnite abelian group with more than one element, and let
m be the exponent of G. By Theorem 8.40, there exists an element a ∈G of
order m. Let A = ⟨a⟩. Then A ∼= Zm. Now, if A = G, the lemma is proved.
So assume that A ⊊G.
We will show that there exists a subgroup B of G such that G = A + B
and A ∩B = {0}. From this, Theorem 8.28 gives us an isomorphism of G
TEAM LinG

210
Abelian groups
with A × B. Moreover, the exponent of B is clearly a divisor of m, and so
the lemma will follow by induction (on the order of the group).
So it suﬃces to show the existence of a subgroup B as above. We prove
this by contradiction. Suppose that there is no such subgroup, and among
all subgroups B such that A∩B = {0}, assume that B is maximal, meaning
that there is no subgroup B′ of G such that B ⊊B′ and A ∩B′ = {0}. By
assumption C := A + B ⊊G.
Let d be any element of G that lies outside of C. Consider the quotient
group G/C, and let r be the order of d + C in G/C. Note that r > 1 and
r | m.
We shall deﬁne a group element d′ with slightly nicer properties
than d, as follows. Since rd ∈C, we have rd = sa + b for some s ∈Z and
b ∈B. We claim that r | s. To see this, note that 0 = md = (m/r)rd =
(m/r)sa + (m/r)b, and since A ∩B = {0}, we have (m/r)sa = 0, which
can only happen if r | s. That proves the claim. This allows us to deﬁne
d′ := d −(s/r)a. Since d ≡d′ (mod C), we see that d′ + C also has order r
in G/C, but also that rd′ ∈B.
We next show that A∩(B+⟨d′⟩) = {0}, which will yield the contradiction
we seek, and thus prove the lemma. Because A ∩B = {0}, it will suﬃce
to show that A ∩(B + ⟨d′⟩) ⊆B. Now, suppose we have a group element
b′ + xd′ ∈A, with b′ ∈B and x ∈Z. Then in particular, xd′ ∈C, and so
r | x, since d′ + C has order r in G/C. Further, since rd′ ∈B, we have
xd′ ∈B, whence b′ + xd′ ∈B. 2
Lemma 8.46. Suppose that G := Zm1 ×· · ·×Zmt and H := Zn1 ×· · ·×Znt
are isomorphic, where the mi and ni are positive integers (possibly 1) such
that mi | mi+1 for i = 1, . . . , t −1. Then mi = ni for i = 1, . . . , t.
Proof. Clearly, 
i mi = |G| = |H| = 
i ni.
We prove the lemma by
induction on the order of the group. If the group order is 1, then clearly
all mi and ni must be 1, and we are done. Otherwise, let p be a prime
dividing the group order. Now, suppose that p divides mr, . . . , mt but not
m1, . . . , mr−1, and that p divides ns, . . . , nt but not n1, . . . , ns−1, where r ≤t
and s ≤t. Evidently, the groups pG and pH are isomorphic. Moreover,
pG ∼= Zm1 × · · · × Zmr−1 × Zmr/p × · · · × Zmt/p,
and
pH ∼= Zn1 × · · · × Zns−1 × Zns/p × · · · × Znt/p.
Thus, we see that |pG| = |G|/pt−r+1 and |pH| = |H|/pt−s+1, from which it
follows that r = s, and the lemma then follows by induction. 2
TEAM LinG

9
Rings
This chapter introduces the notion of a ring, more speciﬁcally, a commu-
tative ring with unity.
The theory of rings provides a useful conceptual
framework for reasoning about a wide class of interesting algebraic struc-
tures. Intuitively speaking, a ring is an algebraic structure with addition
and multiplication operations that behave like we expect addition and mul-
tiplication should. While there is a lot of terminology associated with rings,
the basic ideas are fairly simple.
9.1 Deﬁnitions, basic properties, and examples
Deﬁnition 9.1. A commutative ring with unity is a set R together with
addition and multiplication operations on R, such that:
(i) the set R under addition forms an abelian group, and we denote the
additive identity by 0R;
(ii) multiplication is associative; that is, for all a, b, c ∈R, we have
a(bc) = (ab)c;
(iii) multiplication distributes over addition; that is, for all a, b, c ∈R, we
have a(b + c) = ab + ac and (b + c)a = ba + ca;
(iv) there exists a multiplicative identity; that is, there exists an element
1R ∈R, such that 1R · a = a = a · 1R for all a ∈R;
(v) multiplication is commutative; that is, for all a, b ∈R, we have ab =
ba.
There are other, more general (and less convenient) types of rings—one
can drop properties (iv) and (v), and still have what is called a ring. We
shall not, however, be working with such general rings in this text. There-
fore, to simplify terminology, from now on, by a “ring,” we shall always
mean a commutative ring with unity.
211
TEAM LinG

212
Rings
Let R be a ring.
Notice that because of the distributive law, for any
ﬁxed a ∈R, the map from R to R that sends b ∈R to ab ∈R is a group
homomorphism with respect to the underlying additive group of R. We call
this the a-multiplication map.
We ﬁrst state some simple facts:
Theorem 9.2. Let R be a ring. Then:
(i) the multiplicative identity 1R is unique;
(ii) 0R · a = 0R for all a ∈R;
(iii) (−a)b = a(−b) = −(ab) for all a, b ∈R;
(iv) (−a)(−b) = ab for all a, b ∈R;
(v) (na)b = a(nb) = n(ab) for all n ∈Z and a, b ∈R.
Proof. Part (i) may be proved using the same argument as was used to prove
part (i) of Theorem 8.2. Parts (ii), (iii), and (v) follow directly from parts
(i), (ii), and (iii) of Theorem 8.20, using appropriate multiplication maps,
discussed above. Part (iv) follows from parts (iii) and (iv) of Theorem 8.3.
2
Example 9.1. The set Z under the usual rules of multiplication and addi-
tion forms a ring. 2
Example 9.2. For n ≥1, the set Zn under the rules of multiplication and
addition deﬁned in §2.3 forms a ring. 2
Example 9.3. The set Q of rational numbers under the usual rules of
multiplication and addition forms a ring. 2
Example 9.4. The set R of real numbers under the usual rules of multipli-
cation and addition forms a ring. 2
Example 9.5. The set C of complex numbers under the usual rules of mul-
tiplication and addition forms a ring. Any α ∈C can be written (uniquely)
as α = a+bi, with a, b ∈R, and i = √−1. If α′ = a′+b′i is another complex
number, with a′, b′ ∈R, then
α + α′ = (a + a′) + (b + b′)i and αα′ = (aa′ −bb′) + (ab′ + a′b)i.
The fact that C is a ring can be veriﬁed by direct calculation; however, we
shall see later that this follows easily from more general considerations.
Recall the complex conjugation operation, which sends α to ¯α := a −
bi. One can verify by direct calculation that complex conjugation is both
additive and multiplicative; that is, α + α′ = ¯α + ¯α′ and α · α′ = ¯α · ¯α′.
TEAM LinG

9.1 Deﬁnitions, basic properties, and examples
213
The norm of α is N(α) := α¯α = a2 + b2.
So we see that N(α) is
a non-negative real number, and is zero iﬀα = 0.
Moreover, from the
multiplicativity of complex conjugation, it is easy to see that the norm is
multiplicative as well: N(αα′) = αα′αα′ = αα′¯α¯α′ = N(α)N(α′). 2
Example 9.6. Consider the set F of all arithmetic functions, that is, func-
tions mapping positive integers to real numbers. We can deﬁne addition
and multiplication operations on F in a natural, point-wise fashion: for
f, g ∈F, let f + g be the function that sends n to f(n) + g(n), and let
f · g be the function that sends n to f(n)g(n). These operations of addition
and multiplication make F into a ring: the additive identity is the function
that is everywhere 0, and the multiplicative identity is the function that is
everywhere 1.
Another way to make F into a ring is to use the addition operation as
above, together with the Dirichlet product, which we deﬁned in §2.6, for
the multiplication operation. In this case, the multiplicative identity is the
function I that we deﬁned in §2.6, which takes the value 1 at 1 and the value
0 everywhere else. The reader should verify that the distributive law holds.
2
Note that in a ring R, if 1R = 0R, then for all a ∈R, we have a = 1R ·a =
0R · a = 0R, and hence the ring R is trivial, in the sense that it consists of
the single element 0R, with 0R + 0R = 0R and 0R · 0R = 0R. If 1R ̸= 0R, we
say that R is non-trivial. We shall rarely be concerned with trivial rings for
their own sake; however, they do sometimes arise in certain constructions.
If R1, . . . , Rk are rings, then the set of all k-tuples (a1, . . . , ak) with ai ∈Ri
for i = 1, . . . , k, with addition and multiplication deﬁned component-wise,
forms a ring. The ring is denoted by R1 × · · · × Rk, and is called the direct
product of R1, . . . , Rk.
The characteristic of a ring R is deﬁned as the exponent of the un-
derlying additive group (see §8.5). Note that for m ∈Z and a ∈R, we
have
ma = m(1R · a) = (m · 1R)a,
so that if m · 1R = 0R, then ma = 0R for all a ∈R. Thus, if the additive
order of 1R is inﬁnite, the characteristic of R is zero, and otherwise, the
characteristic of R is equal to the additive order of 1R.
Example 9.7. The ring Z has characteristic zero, Zn has characteristic n,
and Zn1 × Zn2 has characteristic lcm(n1, n2). 2
For elements a, b in a ring R, we say that b divides a, or alternatively,
TEAM LinG

214
Rings
that a is divisible by b, if there exists c ∈R such that a = bc. If b divides
a, then b is called a divisor of a, and we write b | a. Note Theorem 1.1
holds for an arbitrary ring.
When there is no possibility for confusion, one may write “0” instead of
“0R” and “1” instead of “1R.” Also, one may also write, for example, 2R to
denote 2 · 1R, 3R to denote 3 · 1R, and so on; moreover, where the context
is clear, one may use an implicit “type cast,” so that m ∈Z really means
m · 1R.
For a ∈R and positive integer n, the expression an denotes the product
a · a · · · · · a, where there are n terms in the product. One may extend this
deﬁnition to n = 0, deﬁning a0 to be the multiplicative identity 1R.
Exercise 9.1. Verify the usual “rules of exponent arithmetic” for a ring R.
That is, show that for a ∈R, and non-negative integers n1, n2, we have
(an1)n2 = an1n2 and an1an2 = an1+n2.
Exercise 9.2. Show that the familiar binomial theorem holds in an ar-
bitrary ring R; that is, for a, b ∈R and positive integer n, we have
(a + b)n =
n

i=0
n
i

an−ibi.
Exercise 9.3. Show that
 n

i=1
ai
 m

j=1
bj

=
n

i=1
m

j=1
aibj,
where the ai and bj are elements of a ring R.
9.1.1 Units and ﬁelds
Let R be a ring. We call u ∈R a unit if it divides 1R, that is, if uu′ = 1R
for some u′ ∈R. In this case, it is easy to see that u′ is uniquely determined,
and it is called the multiplicative inverse of u, and we denote it by u−1.
Also, for a ∈R, we may write a/u to denote au−1. It is clear that a unit u
divides every a ∈R.
We denote the set of units by R∗. It is easy to verify that the set R∗
is closed under multiplication, from which it follows that R∗is an abelian
group, called the multiplicative group of units of R. If u ∈R∗, then of
course un ∈R∗for all non-negative integers n, and the multiplicative inverse
TEAM LinG

9.1 Deﬁnitions, basic properties, and examples
215
of un is (u−1)n, which we may also write as u−n (which is consistent with
our notation for abelian groups).
If R is non-trivial and every non-zero element of R has a multiplicative
inverse, then R is called a ﬁeld.
Example 9.8. The only units in the ring Z are ±1. Hence, Z is not a ﬁeld.
2
Example 9.9. For positive integer n, the units in Zn are the residue classes
[a]n with gcd(a, n) = 1. In particular, if n is prime, all non-zero residue
classes are units, and if n is composite, some non-zero residue classes are
not units. Hence, Zn is a ﬁeld if and only if n is prime. Of course, the
notation Z∗
n introduced in this section for the group of units of the ring Zn
is consistent with the notation introduced in §2.3. 2
Example 9.10. Every non-zero element of Q is a unit. Hence, Q is a ﬁeld.
2
Example 9.11. Every non-zero element of R is a unit. Hence, R is a ﬁeld.
2
Example 9.12. For non-zero α = a + bi ∈C, with a, b ∈R, we have c :=
N(α) = a2 + b2 > 0. It follows that the complex number ¯αc−1 = (ac−1) +
(−bc−1)i is the multiplicative inverse of α, since α · ¯αc−1 = (α¯α)c−1 = 1.
Hence, every non-zero element of C is a unit, and so C is a ﬁeld. 2
Example 9.13. For rings R1, . . . , Rk, it is easy to see that the multiplicative
group of units of the direct product R1 × · · · × Rk is equal to R∗
1 × · · · × R∗
k.
Indeed, by deﬁnition, (a1, . . . , ak) has a multiplicative inverse if and only if
each individual ai does. 2
Example 9.14. Consider the rings of arithmetic functions deﬁned in Exam-
ple 9.6. If multiplication is deﬁned point-wise, then an arithmetic function f
is a unit if and only if f(n) ̸= 0 for all n. If multiplication is deﬁned in terms
of the Dirichlet product, then by the result of Exercise 2.27, an arithmetic
function f is a unit if and only if f(1) ̸= 0. 2
9.1.2 Zero divisors and integral domains
Let R be a ring. An element a ∈R is called a zero divisor if a ̸= 0R and
there exists non-zero b ∈R such that ab = 0R.
If R is non-trivial and has no zero divisors, then it is called an integral
domain. Put another way, a non-trivial ring R is an integral domain if
TEAM LinG

216
Rings
and only if the following holds: for all a, b ∈R, ab = 0R implies a = 0R or
b = 0R.
Note that if u is a unit in R, it cannot be a zero divisor (if ub = 0R, then
multiplying both sides of this equation by u−1 yields b = 0R). In particular,
it follows that any ﬁeld is an integral domain.
Example 9.15. Z is an integral domain. 2
Example 9.16. For n > 1, Zn is an integral domain if and only if n is
prime. In particular, if n is composite, so n = n1n2 with 1 < n1 < n and
1 < n2 < n, then [n1]n and [n2]n are zero divisors: [n1]n[n2]n = [0]n, but
[n1]n ̸= [0]n and [n2]n ̸= [0]n. 2
Example 9.17. Q, R, and C are ﬁelds, and hence are also integral domains.
2
Example 9.18. For two non-trivial rings R1, R2, an element (a1, a2) ∈
R1 × R2 is a zero divisor if and only if a1 is a zero divisor, a2 is a zero
divisor, or exactly one of a1 or a2 is zero. In particular, R1 × R2 is not an
integral domain. 2
We have the following “cancellation law”:
Theorem 9.3. If R is a ring, and a, b, c ∈R such that a ̸= 0R and a is not
a zero divisor, then ab = ac implies b = c.
Proof. ab = bc implies a(b −c) = 0R. The fact that a ̸= 0 and a is not a
zero divisor implies that we must have b −c = 0R, and so b = c. 2
Theorem 9.4. If D is an integral domain, then:
(i) for all a, b, c ∈D, a ̸= 0D and ab = ac implies b = c;
(ii) for all a, b ∈D, a | b and b | a if and only if a = bc for some c ∈D∗.
(iii) for all a, b ∈D with b ̸= 0D and b | a, there is a unique c ∈D such
that a = bc, which we may denote as a/b.
Proof. The ﬁrst statement follows immediately from the previous theorem
and the deﬁnition of an integral domain.
For the second statement, if a = bc for c ∈D∗, then we also have b = ac−1;
thus, b | a and a | b. Conversely, a | b implies b = ax for x ∈D, and b | a
implies a = by for y ∈D, and hence b = bxy. If b = 0R, then the equation
a = by implies a = 0R, and so the statement holds for any c; otherwise,
cancel b, we have 1D = xy, and so x and y are units.
For the third statement, if a = bc and a = bc′, then bc = bc′, and cancel
b. 2
TEAM LinG

9.1 Deﬁnitions, basic properties, and examples
217
Theorem 9.5. The characteristic of an integral domain is either zero or a
prime.
Proof. By way of contradiction, suppose that D is an integral domain with
characteristic m that is neither zero nor prime. Since, by deﬁnition, D is
not a trivial ring, we cannot have m = 1, and so m must be composite. Say
m = st, where 1 < s < m and 1 < t < m. Since m is the additive order of
1D, it follows that (s · 1D) ̸= 0D and (t · 1D) ̸= 0D; moreover, since D is an
integral domain, it follows that (s · 1D)(t · 1D) ̸= 0D. So we have
0D = m · 1D = (st) · 1D = (s · 1D)(t · 1D) ̸= 0D,
a contradiction. 2
Theorem 9.6. Any ﬁnite integral domain is a ﬁeld.
Proof. Let D be a ﬁnite integral domain, and let a be any non-zero element
of D. Consider the a-multiplication map that sends b ∈D to ab, which
is a group homomorphism on the additive group of D. Since a is not a
zero-divisor, it follows that the kernel of the a-multiplication map is {0D},
hence the map is injective, and by ﬁniteness, it must be surjective as well.
In particular, there must be an element b ∈D such that ab = 1D. 2
Theorem 9.7. Any ﬁnite ﬁeld F must be of cardinality pw, where p is
prime, w is a positive integer, and p is the characteristic of F.
Proof. By Theorem 9.5, the characteristic of F is either zero or a prime,
and since F is ﬁnite, it must be prime. Let p denote the characteristic. By
deﬁnition, p is the exponent of the additive group of F, and by Theorem 8.42,
the primes dividing the exponent are the same as the primes dividing the
order, and hence F must have cardinality pw for some positive integer w. 2
Of course, for every prime p, Zp is a ﬁnite ﬁeld of cardinality p. As we
shall see later (in Chapter 20), for every prime p and positive integer w,
there exists a ﬁeld of cardinality pw. Later in this chapter, we shall see some
speciﬁc examples of ﬁnite ﬁelds whose cardinality is not prime (Examples
9.35 and 9.47).
Exercise 9.4. Let R be a ring of characteristic m > 0, and let n be any
integer. Show that:
(a) if gcd(n, m) = 1, then n · 1R is a unit;
(b) if 1 < gcd(n, m) < m, then n · 1R is a zero divisor;
(c) otherwise, n · 1R = 0R.
TEAM LinG

218
Rings
Exercise 9.5. Let D be an integral domain, m ∈Z, and a ∈D. Show that
ma = 0D if and only if m is a multiple of the characteristic of D or a = 0D.
Exercise 9.6. For n ≥1, and for all a, b ∈Zn, show that if a | b and b | a,
then a = bc for some c ∈Z∗
n. Thus, part (ii) of Theorem 9.4 may hold for
some rings that are not integral domains.
Exercise 9.7. This exercise depends on results in §8.6. Using the funda-
mental theorem of ﬁnite abelian groups, show that the additive group of a
ﬁnite ﬁeld of characteristic p and cardinality pw is isomorphic to Z×w
p
.
9.1.3 Subrings
Deﬁnition 9.8. A subset S of a ring R is called a subring if
(i) S is a subgroup of the additive group R,
(ii) S is closed under multiplication, and
(iii) 1R ∈S.
It is clear that the operations of addition and multiplication on a ring R
make a subring S of R into a ring, where 0R is the additive identity of S and
1R is the multiplicative identity of S. One may also call R an extension
ring of S.
Some texts do not require that 1R belongs to a subring S, and instead
require only that S contains a multiplicative identity, which may be diﬀerent
than that of R. This is perfectly reasonable, but for simplicity, we restrict
ourselves to the case when 1R ∈S.
Expanding the above deﬁnition, we see that a subset S of R is a subring
if and only if 1R ∈S and for all a, b ∈S, we have
a + b ∈S,
−a ∈S,
and ab ∈S.
If fact, to verify that S is a subring, it suﬃces to show that −1R ∈S and
that S is closed under addition and multiplication; indeed, if −1R ∈S and S
is closed under multiplication, then S is closed under negation, and further,
1R = −(−1R) ∈S.
Example 9.19. Z is a subring of Q. 2
Example 9.20. Q is a subring of R. 2
Example 9.21. R is a subring of C.
Note that for α := a+bi ∈C, with a, b ∈R, we have ¯α = α iﬀa+bi = a−bi
iﬀb = 0. That is, ¯α = α iﬀα ∈R. 2
TEAM LinG

9.1 Deﬁnitions, basic properties, and examples
219
Example 9.22. The set Z[i] of complex numbers of the form a + bi, with
a, b ∈Z, is a subring of C. It is called the ring of Gaussian integers.
Since C is a ﬁeld, it contains no zero divisors, and hence Z[i] contains no
zero divisors. Hence, Z[i] is an integral domain.
Let us determine the units of Z[i]. If α ∈Z[i] is a unit, then there exists
α′ ∈Z[i] such that αα′ = 1. Taking norms, we obtain
1 = N(1) = N(αα′) = N(α)N(α′).
Clearly, the norm of a Gaussian integer is a non-negative integer, and so
N(α)N(α′) = 1 implies N(α) = 1. Now, if α = a + bi, with a, b ∈Z, then
N(α) = a2 + b2, and so N(α) = 1 implies α = ±1 or α = ±i. Conversely, it
is clear that ±1 and ±i are indeed units, and so these are the only units in
Z[i]. 2
Example 9.23. Let m be a positive integer, and let Q(m) be the set of
rational numbers of the form a/b, where a and b are integers, and b is
relatively prime to m. Then Q(m) is a subring of Q, since for any a, b, c, d ∈Z
with gcd(b, m) = 1 and gcd(d, m) = 1, we have
a
b + c
d = ad + bc
bd
and a
b · c
d = ac
bd,
and since gcd(bd, m) = 1, it follows that the sum and product of any two
element of Q(m) is again in Q(m). Clearly, Q(m) contains −1, and so it follows
that Q(m) is a subring of Q. The units of Q(m) are precisely those rational
numbers of the form a/b, where gcd(a, m) = gcd(b, m) = 1. 2
Example 9.24. If R and S are non-trivial rings, then R′ := R × {0S}
is not a subring of R × S: although it satisﬁes the ﬁrst two requirements
of the deﬁnition of a subring, it does not satisfy the third. However, R′
does contain an element that acts as a multiplicative identity of R′, namely
(1R, 0S), and hence could be viewed as a subring of R × S under a more
liberal deﬁnition. 2
Theorem 9.9. Any subring of an integral domain is also an integral do-
main.
Proof. If D′ is a subring of the integral domain D, then any zero divisor in
D′ would itself be a zero divisor in D. 2
Note that it is not the case that a subring of a ﬁeld is always a ﬁeld: the
subring Z of Q is a counter-example. If F ′ is a subring of a ﬁeld F, and F ′
is itself a ﬁeld, then we say that F ′ is a subﬁeld of F, and that F is an
extension ﬁeld of F ′.
TEAM LinG

220
Rings
Example 9.25. Q is a subﬁeld of R, which in turn is a subﬁeld of C. 2
Exercise 9.8. Show that the set Q[i] of complex numbers of the form a+bi,
with a, b ∈Q, is a subﬁeld of C.
Exercise 9.9. Show that if S and S′ are subrings of R, then so is S ∩S′.
Exercise 9.10. Let F be the set of all functions f : R →R, and let C be
the subset of F of continuous functions.
(a) Show that with addition and multiplication of functions deﬁned in the
natural, point-wise fashion, F is a ring, but not an integral domain.
(b) Let a, b ∈F. Show that if a | b and b | a, then there is a c ∈F∗such
that a = bc.
(c) Show that C is a subring of F, and show that all functions in C∗are
either everywhere positive or everywhere negative.
(d) Deﬁne a, b ∈C by a(t) = b(t) = t for t < 0, a(t) = b(t) = 0 for
0 ≤t ≤1, and a(t) = −b(t) = t −1 for t > 1. Show that in the ring
C, we have a | b and b | a, yet there is no c ∈C∗such that a = bc.
Thus, part (ii) of Theorem 9.4 does not hold in a general ring.
9.2 Polynomial rings
If R is a ring, then we can form the ring of polynomials R[X], consisting
of all polynomials a0 + a1X + · · · + akXk in the indeterminate, or “formal”
variable, X, with coeﬃcients in R, and with addition and multiplication
being deﬁned in the usual way.
Example 9.26. Let us deﬁne a few polynomials over the ring Z:
a := 3 + X2, b := 1 + 2X −X3, c := 5, d := 1 + X, e := X, f := 4X3.
We have
a+b = 4+2X+X2−X3, a·b = 3+6X+X2−X3−X5, cd+ef = 5+5X+4X4. 2
As illustrated in the previous example, elements of R are also polynomials.
Such polynomials are called constant polynomials; all other polynomials
are called non-constant polynomials. The set R of constant polynomials
clearly forms a subring of R[X]. In particular, 0R is the additive identity in
R[X] and 1R is the multiplicative identity in R[X].
TEAM LinG

9.2 Polynomial rings
221
For completeness, we present a more formal deﬁnition of the ring R[X].
The reader should bear in mind that this formalism is rather tedious, and
may be more distracting than it is enlightening.
It is technically conve-
nient to view a polynomial as having an inﬁnite sequence of coeﬃcients
a0, a1, a2, . . . , where each coeﬃcient belongs to R, but where only a ﬁnite
number of the coeﬃcients are non-zero. We may write such a polynomial as
an inﬁnite sum ∞
i=0 aiXi; however, this notation is best thought of “syntac-
tic sugar”: there is really nothing more to the polynomial than this sequence
of coeﬃcients. With this notation, if
a =
∞

i=0
aiXi and b =
∞

i=0
biXi,
then
a + b :=
∞

i=0
(ai + bi)Xi,
(9.1)
and
a · b :=
∞

i=0

i

k=0
akbi−k

Xi.
(9.2)
We should ﬁrst verify that these addition and multiplication operations
actually produce coeﬃcient sequences with only a ﬁnite number of non-zero
terms. Suppose that for non-negative integers k and ℓ, we have ai = 0R for
all i > k and bi = 0R for all i > ℓ. Then it is clear that the coeﬃcient of Xi
in a + b is zero for all i > max{k, ℓ}, and it is also not too hard to see that
the coeﬃcient of Xi in a · b is zero for all i > k + ℓ.
We leave it to the reader to verify that R[X], with addition and multipli-
cation deﬁned as above, actually satisﬁes the deﬁnition of a ring—this is
entirely straightforward, but tedious.
For c ∈R, we may identify c with the polynomial ∞
i=0 ciXi, where c0 = c
and ci = 0R for i > 0. Strictly speaking, c and ∞
i=0 ciXi are not the same
mathematical object, but there will certainly be no possible confusion in
treating them as such. Thus, from a narrow, legalistic point of view, R is
not a subring of R[X], but we shall not let such let such annoying details
prevent us from continuing to speak of it as such. As one last matter of
notation, we may naturally write X to denote the polynomial ∞
i=0 aiXi,
where a1 = 1R and ai = 0R for all i ̸= 1.
With all of these conventions and deﬁnitions, we can return to the prac-
tice of writing polynomials as we did in Example 9.26, without any loss of
precision. Note that by deﬁnition, if R is the trivial ring, then so is R[X].
TEAM LinG

222
Rings
9.2.1 Polynomials versus polynomial functions
Of course, a polynomial a = k
i=0 aiXi deﬁnes a polynomial function on R
that sends α ∈R to k
i=0 aiαi, and we denote the value of this function
as a(α). However, it is important to regard polynomials over R as formal
expressions, and not to identify them with their corresponding functions.
In particular, two polynomials are equal if and only if their coeﬃcients are
equal. This distinction is important, since there are rings R over which two
diﬀerent polynomials deﬁne the same function. One can of course deﬁne the
ring of polynomial functions on R, but in general, that ring has a diﬀerent
structure from the ring of polynomials over R.
Example 9.27. In the ring Zp, for prime p, by Fermat’s little theorem
(Theorem 2.16), we have αp −α = [0]p for all α ∈Zp. But consider the
polynomial a := Xp −X ∈Zp[X]. We have a(α) = [0]p for all α ∈Zp, and
hence the function deﬁned by a is the zero function, yet a is deﬁnitely not
the zero polynomial. 2
More generally, if R is a subring of a ring E, a polynomial a = k
i=0 aiXi ∈
R[X] deﬁnes a polynomial function from E to E that sends α ∈E to
k
i=0 aiαi ∈E, and the value of this function is denoted a(α).
If E = R[X], then evaluating a polynomial a ∈R[X] at a point α ∈E
amounts to polynomial composition. For example, if a = X2 + X then
a

X + 1

= (X + 1)2 + (X + 1) = X2 + 3X + 2.
A simple, but important, fact is the following:
Theorem 9.10. Let R be a subring of a ring E. For a, b ∈R[X] and α ∈E,
if p := ab ∈R[X] and s := a + b ∈R[X], then we have
p(α) = a(α)b(α) and s(α) = a(α) + b(α).
Also, if c ∈R[X] is a constant polynomial, then c(α) = c for all α ∈E.
Proof. Exercise. 2
Note that the syntax for polynomial evaluation creates some poten-
tial ambiguities: if a is a polynomial, one could interpret a(b + c) as
either a times b + c, or a evaluated at b + c; usually, the meaning
will be clear from context, but to avoid such ambiguities, if the in-
tended meaning is the former, we shall generally write this as, say,
a · (b + c) or (b + c)a, and if the intended meaning is the latter, we
shall generally write this as a[ b + c ].
So as to keep the distinction between ring elements and indetermi-
nates clear, we shall use the symbol “X” only to denote the latter.
Also, for a polynomial a ∈R[X], we shall in general write this simply
TEAM LinG

9.2 Polynomial rings
223
as “a,” and not as “a(X).” Of course, the choice of the symbol “X”
is arbitrary; occasionally, we may use other symbols, such as “Y,” as
alternatives.
9.2.2 Basic properties of polynomial rings
Let R be a ring. For non-zero a ∈R[X], if a = k
i=0 aiXi with ak ̸= 0R,
then we call k the degree of a, denoted deg(a), we call ak the leading
coeﬃcient of a, denoted lc(a), and we call a0 the constant term of a. If
lc(a) = 1R, then a is called monic.
Suppose a = k
i=0 aiXi and b = ℓ
i=0 biXi are polynomials such that
ak ̸= 0R and bℓ̸= 0R, so that deg(a) = k and lc(a) = ak, and deg(b) = ℓ
and lc(b) = bℓ. When we multiply these two polynomials, we get
ab = a0b0 + (a0b1 + a1b0)X + · · · + akbℓXk+ℓ.
In particular, deg(ab) ≤deg(a) + deg(b). If either of ak or bℓare not zero
divisors, then akbℓis not zero, and hence deg(ab) = deg(a) + deg(b). How-
ever, if both ak and bℓare zero divisors, then we may have akbℓ= 0R,
in which case, the product ab may be zero, or perhaps ab ̸= 0R but
deg(ab) < deg(a) + deg(b).
Example 9.28. Over the ring Z6, consider the polynomials a := [1] + [2]X
and b = [1] + [3]X. We have ab = [1] + [5]X + [6]X2 = [1] + [5]X. Thus,
deg(ab) = 1 < 2 = deg(a) + deg(b). 2
For the zero polynomial, we establish the following conventions: its leading
coeﬃcient and constant term are deﬁned to be 0R, and its degree is deﬁned
to be −∞. With these conventions, we may succinctly state that
for all a, b ∈R[X], we have deg(ab) ≤deg(a) + deg(b), with
equality guaranteed to hold unless the leading coeﬃcients of
both a and b are zero divisors.
In the case where the ring of coeﬃcients is as integral domain, we can say
signiﬁcantly more:
Theorem 9.11. Let D be an integral domain. Then:
(i) for all a, b ∈D[X], we have deg(ab) = deg(a) + deg(b);
(ii) D[X] is an integral domain;
(iii) (D[X])∗= D∗.
Proof. Exercise. 2
TEAM LinG

224
Rings
9.2.3 Division with remainder
An extremely important property of polynomials is a division with remainder
property, analogous to that for the integers:
Theorem 9.12 (Division with remainder property). Let R be a ring.
For a, b ∈R[X] with b ̸= 0R and lc(b) ∈R∗, there exist unique q, r ∈R[X]
such that a = bq + r and deg(r) < deg(b).
Proof. Consider the set S of polynomials of the form a−zb with z ∈R[X]. Let
r = a −qb be an element of S of minimum degree. We must have deg(r) <
deg(b), since otherwise, we would have r′ := r −(lc(r) lc(b)−1Xdeg(r)−deg(b)) ·
b ∈S, and deg(r′) < deg(r), contradicting the minimality of deg(r).
That proves the existence of r and q. For uniqueness, suppose that a =
bq + r and a = bq′ + r′, where deg(r) < deg(b) and deg(r′) < deg(b). This
implies r′ −r = b · (q −q′). However, if q ̸= q′, then
deg(b) > deg(r′ −r) = deg(b · (q −q′)) = deg(b) + deg(q −q′) ≥deg(b),
which is impossible. Therefore, we must have q = q′, and hence r = r′. 2
If a = bq + r as in the above theorem, we deﬁne a mod b := r. Clearly,
b | a if and only if a mod b = 0R. Moreover, note that if deg(a) < deg(b),
then q = 0 and r = a; otherwise, if deg(a) ≥deg(b), then q ̸= 0 and
deg(a) = deg(b) + deg(q).
As a consequence of the above theorem, we have:
Theorem 9.13. For a ring R and a ∈R[X] and α ∈R, a(α) = 0R if and
only if (X −α) divides a.
Proof. If R is the trivial ring, there is nothing to prove, so assume that R is
non-trivial. Let us write a = (X −α)q + r, with q, r ∈R[X] and deg(r) < 1,
which means that r ∈R. Then we have a(α) = (α −α)q(α) + r = r. Thus,
a(α) = 0R if and only if a mod (X−α) = 0R, which holds if and only if X−α
divides a. 2
With R, a, α as in the above theorem, we say that α is a root of a if
a(α) = 0R.
Theorem 9.14. Let D be an integral domain, and let a ∈D[X], with
deg(a) = k ≥0. Then a has at most k roots.
Proof. We can prove this by induction. If k = 0, this means that a is a
non-zero element of D, and so it clearly has no roots.
Now suppose that k > 0. If a has no roots, we are done, so suppose that
TEAM LinG

9.2 Polynomial rings
225
a has a root α. Then we can write a = (X−α)q, where deg(q) = k −1. Now,
for any root β of a with β ̸= α, we have 0D = a(β) = (β −α)q(β), and using
the fact that D is an integral domain, we must have q(β) = 0D. Thus, the
only roots of a are α and the roots of q. By induction, q has at most k −1
roots, and hence a has at most k roots. 2
Theorem 9.14 has many applications, among which is the following beau-
tiful theorem that establishes an important property of the multiplicative
structure of an integral domain:
Theorem 9.15. Let D be an integral domain and G a subgroup of D∗of
ﬁnite order. Then G is cyclic.
Proof. Let n be the order of G, and suppose G is not cyclic.
Then by
Theorem 8.40, we have that the exponent m of G is strictly less than n. It
follows that αm = 1D for all α ∈G. That is, all the elements of G are roots
of the polynomial Xm −1D ∈D[X]. But since a polynomial of degree m over
D has at most m roots, this contradicts the fact that m < n. 2
As a special case of Theorem 9.15, we have:
Theorem 9.16. For any ﬁnite ﬁeld F, the group F ∗is cyclic. In particular,
if p is prime, then Z∗
p is cyclic; that is, there is a primitive root modulo p.
Exercise 9.11. Let D be an inﬁnite integral domain, and let a ∈D[X]. Show
that if a(α) = 0D for all α ∈D, then a = 0D. Thus, for an inﬁnite integral
domain D, there is a one-to-one correspondence between polynomials over
D and polynomial functions on D.
Exercise 9.12. This exercise develops a message authentication scheme
(see §6.7.2) that allows one to hash long messages using a relatively small
set of hash functions. Let F be a ﬁnite ﬁeld of cardinality q and let t be
a positive integer. Let A := F ×t and Z := F. Deﬁne a family H of hash
functions from A to Z as follows: let H := {hα,β : α, β ∈F}, where for all
hα,β ∈H and all (a1, . . . , at) ∈A, we deﬁne
hα,β(a1, . . . , at) := β +
t

i=1
aiαi ∈Z.
Show that H is a t/q-forgeable message authentication scheme. (Compare
this with the second pairwise independent family of hash functions discussed
in Example 6.25, which is much larger, but which is only 1/q-forgeable; in
practice, using the smaller family of hash functions with a somewhat higher
forging probability may be a good trade-oﬀ.)
TEAM LinG

226
Rings
Exercise 9.13. This exercise develops an alternative proof of Theorem 9.15.
Let n be the order of the group. Using Theorem 9.14, show that for all
d | n, there are at most d elements in the group whose multiplicative order
divides d. From this, deduce that for all d | n, the number of elements of
multiplicative order d is either 0 or φ(d). Now use Theorem 2.11 to deduce
that for all d | n (and in particular, for d = n), the number of elements of
multiplicative order d is equal to φ(d).
Exercise 9.14. Let F be a ﬁeld of characteristic other than 2, so that the
2F ̸= 0F . Show that the familiar quadratic formula holds for F. That is,
for a, b, c ∈F with a ̸= 0F , the polynomial f := aX2 + bX + c ∈F[X] has
a root if and only if there exists z ∈F such that z2 = d, where d is the
discriminant of f, deﬁned as d := b2 −4ac, and in this case the roots of f
are
−b ± z
2a
.
Exercise 9.15. Let R be a ring, let a ∈R[X], with deg(a) = k ≥0, and let
α be an element of R.
(a) Show that there exists an integer m, with 0 ≤m ≤k, and a polyno-
mial q ∈R[X], such that
a = (X −α)mq and q(α) ̸= 0R.
(b) Show that the values m and q in part (a) are uniquely determined
(by a and α).
(c) Show that m > 0 if and only if α is a root of a.
Let mα(a) denote the value m in the previous exercise; for completeness,
one can deﬁne mα(a) := ∞if a is the zero polynomial. If mα(a) > 0, then
α is called a root of a of multiplicity mα(a); if mα(a) = 1, then α is called
a simple root of a, and if mα(a) > 1, then α is called a multiple root of
a.
The following exercise reﬁnes Theorem 9.14, taking into account multi-
plicities.
Exercise 9.16. Let D be an integral domain, and let a ∈D[X], with
deg(a) = k ≥0. Show that

α∈D
mα(a) ≤k.
Exercise 9.17. Let D be an integral domain, let a, b ∈D[X], and let α ∈D.
Show that mα(ab) = mα(a) + mα(b).
TEAM LinG

9.2 Polynomial rings
227
Exercise 9.18. Let R be a ring, let a ∈R[X], with deg(a) = k ≥0, let
α ∈R, and let m := mα(a). Show that if we evaluate a at X + α, we have
a

X + α

=
k

i=m
biXi,
where bm, . . . , bk ∈R and bm ̸= 0R.
9.2.4 Formal derivatives
Let R be any ring, and let a ∈R[X] be a polynomial. If a = ℓ
i=0 aiXi, we
deﬁne the formal derivative of a as
D(a) :=
ℓ

i=1
iaiXi−1.
We stress that unlike the “analytical” notion of derivative from calculus,
which is deﬁned in terms of limits, this deﬁnition is purely “symbolic.”
Nevertheless, some of the usual rules for derivatives still hold:
Theorem 9.17. Let R be a ring. For all a, b ∈R[X] and c ∈R, we have
(i) D(a + b) = D(a) + D(b);
(ii) D(ca) = cD(a);
(iii) D(ab) = D(a)b + aD(b).
Proof. Parts (i) and (ii) follow immediately by inspection, but part (iii)
requires some proof. First, note that part (iii) holds trivially if either a or b
are zero, so let us assume that neither are zero.
We ﬁrst prove part (iii) for monomials, that is, polynomials of the form
cXi for non-zero c ∈R and i ≥0.
Suppose a = cXi and b = dXj.
If
i = 0, so a = c, then the result follows from part (ii) and the fact that
D(c) = 0; when j = 0, the result holds by a symmetric argument.
So
assume that i > 0 and j > 0. Now, D(a) = icXi−1 and D(b) = jdXj−1,
and D(ab) = D(cdXi+j) = (i + j)cdXi+j−1. The result follows from a simple
calculation.
Having proved part (iii) for monomials, we now prove it in general on
induction on the total number of monomials appearing in a and b. If the
total number is 2, then both a and b are monomials, and we are in the base
case; otherwise, one of a and b must consist of at least two monomials, and
for concreteness, say it is b that has this property. So we can write b = b1+b2,
where both b1 and b2 have fewer monomials than does b. Applying part (i)
TEAM LinG

228
Rings
and the induction hypothesis for part (iii), we have
D(ab) = D(ab1 + ab2)
= D(ab1) + D(ab2)
= D(a)b1 + aD(b1) + D(a)b2 + aD(b2)
= D(a) · (b1 + b2) + a · (D(b1) + D(b2))
= D(a) · (b1 + b2) + a · D(b1 + b2)
= D(a)b + aD(b). 2
Exercise 9.19. Let R be a ring, let a ∈R[X], and let α ∈R be a root of
a. Show that α is a multiple root of a if and only if α is a root of D(a) (see
Exercise 9.15).
Exercise 9.20. Let R be a ring, let a ∈R[X] with deg(a) = k ≥0, and let
α ∈R. Show that if we evaluate a at X + α, writing
a

X + α

=
k

i=0
biXi,
with b0, . . . , bk ∈R, then we have
i! · bi = (Di(a))(α) for i = 0, . . . , k.
Exercise 9.21. Let F be a ﬁeld such that every non-constant polynomial
a ∈F[X] has a root α ∈F. (The ﬁeld C is an example of such a ﬁeld, an
important fact which we shall not be proving in this text.) Show that for
every positive integer r that is not a multiple of the characteristic of F, there
exists an element ζ ∈F ∗of multiplicative order r, and that every element
in F ∗whose order divides r is a power of ζ.
9.2.5 Multi-variate polynomials
One can naturally generalize the notion of a polynomial in a single variable
to that of a polynomial in several variables. We discuss these ideas brieﬂy
here—they will play only a minor role in the remainder of the text.
Consider the ring R[X] of polynomials over a ring R. If Y is another indeter-
minate, we can form the ring R[X][Y] of polynomials in Y whose coeﬃcients
are themselves polynomials in X over the ring R. One may write R[X, Y]
instead of R[X][Y]. An element of R[X, Y] is called a bivariate polynomial.
TEAM LinG

9.2 Polynomial rings
229
Consider a typical element a ∈R[X, Y], which may be written
a =
ℓ

j=0
 k

i=0
aijXi

Yj.
(9.3)
Rearranging terms, this may also be written as
a =

0≤i≤k
0≤j≤ℓ
aijXiYj,
(9.4)
or as
a =
k

i=0
 ℓ

j=0
aijYj

Xj.
(9.5)
If a is written as in (9.4), the terms aijXiYj with aij ̸= 0R are called
monomials. The total degree of such a monomial aijXiYj is deﬁned to be
i + j, and if a is non-zero, then the total degree of a, denoted Deg(a), is
deﬁned to be the maximum total degree of any monomial appearing in (9.4).
We deﬁne the total degree of the zero polynomial to be −∞. The reader
may verify that for any a, b ∈R[X, Y], we have Deg(ab) ≤Deg(a) + Deg(b),
while equality holds if R is an integral domain.
When a is written as in (9.5), one sees that we can naturally view a as
an element of R[Y][X], that is, as a polynomial in X whose coeﬃcients are
polynomials in Y . From a strict, syntactic point of view, the rings R[Y][X]
and R[X][Y] are not the same, but there is no harm done in blurring this
distinction when convenient. We denote by degX(a) the degree of a, viewed
as a polynomial in X, and by degY(a) the degree of a, viewed as a polynomial
in Y. Analogously, one can formally diﬀerentiate a with respect to either X
or Y, obtaining the “partial” derivatives DX(a) and DY(a).
Example 9.29. Let us illustrate, with a particular example, the three dif-
ferent forms—as in (9.3), (9.4), and (9.5)—of expressing a bivariate poly-
nomial. In the ring Z[X, Y] we have
a = (5X2 −3X + 4)Y + (2X2 + 1)
= 5X2Y + 2X2 −3XY + 4Y + 1
= (5Y + 2)X2 + (−3Y)X + (4Y + 1).
We have Deg(a) = 3, degX(a) = 2, and degY(a) = 1. 2
More generally, if X1, . . . , Xn are indeterminates, we can form the ring
TEAM LinG

230
Rings
R[X1, . . . , Xn] of multi-variate polynomials in n variables over R. For-
mally, we can think of this ring as R[X1][X2] · · · [Xn]. Any multi-variate poly-
nomial can be expressed uniquely as the sum of monomials of the form
cXe1
1 · · · Xen
n for non-zero c ∈R and non-negative integers e1, . . . , en; the total
degree of such a monomial is deﬁned to be 
i ei, and the total degree of
a multi-variate polynomial a, denoted Deg(a), is deﬁned to be the maxi-
mum degree of its monomials. As above, for a, b ∈R[X1, . . . , Xn], we have
Deg(ab) ≤Deg(a) + Deg(b), while equality always holds if R is an integral
domain.
Just as for bivariate polynomials, the order of the indeterminates is not
important, and for any i = 1, . . . , n, one can naturally view any a ∈
R[X1, . . . , Xn] as a polynomial in Xi over the ring R[X1, . . . , Xi−1, Xi+1, . . . , Xn],
and deﬁne degXi(a) to be the degree of a when viewed in this way. Anal-
ogously, one can formally diﬀerentiate a with respect to any variable Xi,
obtaining the “partial” derivative DXi(a).
Just as polynomials in a single variable deﬁne polynomial functions, so do
polynomials in several variables. If R is a subring of E, a ∈R[X1, . . . , Xn],
and α = (α1, . . . , αn) ∈E×n, we deﬁne a(α) to be the element of E ob-
tained by evaluating the expression obtained by substituting αi for Xi in a.
Theorem 9.10 carries over directly to the multi-variate case.
Exercise 9.22. Let R be a ring, and let α1, . . . , αn be elements of R. Show
that any polynomial a ∈R[X1, . . . , Xn] can be expressed as
a = (X1 −α1)q1 + · · · + (Xn −αn)qn + r,
where q1, . . . , qn ∈R[X1, . . . , Xn] and r ∈R.
Moreover, show that the
value of r appearing in such an expression is uniquely determined (by a
and α1, . . . , αn).
Exercise 9.23. This exercise generalizes Theorem 9.14. Let D be an inte-
gral domain, and let a ∈D[X1, . . . , Xn], with Deg(a) = k ≥0. Let T be a
ﬁnite subset of D. Show that the number of elements α ∈T ×n such that
a(α) = 0 is at most k|T|n−1.
Exercise 9.24. Let F be a ﬁnite ﬁeld of cardinality q, and let t be a positive
integer. Let A := F ×t and Z := F. Use the result of the previous exercise to
construct a family H of hash functions from A to Z that is an O(len(t)/q)-
forgeable message authentication scheme, where logq |H| = len(t) + O(1).
(See §6.7.2 and also Exercise 9.12.)
TEAM LinG

9.3 Ideals and quotient rings
231
9.3 Ideals and quotient rings
Deﬁnition 9.18. Let R be a ring. An ideal of R is a subgroup I of the
additive group of R that is closed under multiplication by elements of R, that
is, for all a ∈I and r ∈R, we have ar ∈I.
Expanding the above deﬁnition, we see that a non-empty subset I of R is
an ideal of R if and only if for all a, b ∈I and r ∈R, we have
a + b ∈I,
−a ∈I,
and ar ∈I.
Observe that the condition −a ∈I is redundant, as it is implied by the
condition ar ∈I with r = −1R. Note that in the case when R is the ring Z,
this deﬁnition of an ideal is consistent with that given in §1.2.
Clearly, {0R} and R are ideals of R. From the fact that an ideal I is
closed under multiplication by elements of R, it is easy to see that I = R if
and only if 1R ∈I.
Example 9.30. For m ∈Z, the set mZ is not only a subgroup of the
additive group Z, it is also an ideal of the ring Z. 2
Example 9.31. For m ∈Z, the set mZn is not only a subgroup of the
additive group Zn, it is also an ideal of the ring Zn. 2
Example 9.32. In the previous two examples, we saw that for some rings,
the notion of an additive subgroup coincides with that of an ideal.
Of
course, that is the exception, not the rule. Consider the ring of polynomial
R[X]. Suppose a is a non-zero polynomial in R[X]. The additive subgroup
generated by a consists of polynomials whose degrees are at most that of a.
However, this subgroup is not an ideal, since any ideal containing a must
also contain a · Xi for all i ≥0, and must therefore contain polynomials of
arbitrarily high degree. 2
Let a1, . . . , ak be elements of a ring R. Then it is easy to see that the set
a1R + · · · + akR := {a1r1 + · · · + akrk : r1, . . . , rk ∈R}
is an ideal of R, and contains a1, . . . , ak. It is called the ideal of R gener-
ated by a1, . . . , ak. Clearly, any ideal I of R that contains a1, . . . , ak must
contain a1R + · · · + akR, and in this sense, a1R + · · · + akR is the smallest
ideal of R containing a1, . . . , ak. An alternative notation that is often used
is to write (a1, . . . , ak) to denote the ideal generated by a1, . . . , ak, when the
ring R is clear from context. If an ideal I is of the form aR = {ar : r ∈R}
for some a ∈R, then we say that I is a principal ideal.
TEAM LinG

232
Rings
Note that if I and J are ideals of a ring R, then so are I + J := {x + y :
x ∈I, y ∈J} and I ∩J (verify).
Since an ideal I of a ring R is a subgroup of the additive group R, we may
adopt the congruence notation in §8.3, writing a ≡b (mod I) if and only if
a −b ∈I.
Note that if I = dR, then a ≡b (mod I) if and only if d | (a−b), and as a
matter of notation, one may simply write this congruence as a ≡b (mod d).
Just considering R as an additive group, then as we saw in §8.3, we can
form the additive group R/I of cosets, where (a+I)+(b+I) := (a+b)+I.
By also considering the multiplicative structure of R, we can view R/I as a
ring. To do this, we need the following fact:
Theorem 9.19. Let I be an ideal of a ring R, and let a, a′, b, b′ ∈R. If
a ≡a′ (mod I) and b ≡b′ (mod I), then ab ≡a′b′ (mod I).
Proof. If a′ = a + x for x ∈I and b′ = b + y for y ∈I, then a′b′ =
ab+ay+bx+xy. Since I is closed under multiplication by elements of R, we
see that ay, bx, xy ∈I, and since it is closed under addition, ay+bx+xy ∈I.
Hence, a′b′ −ab ∈I. 2
This theorem is perhaps one of the main motivations for the deﬁnition of
an ideal. It allows us to deﬁne multiplication on R/I as follows: for a, b ∈R,
(a + I) · (b + I) := ab + I.
The above theorem is required to show that this deﬁnition is unambiguous.
Once that is done, it is straightforward to show that all the properties that
make R a ring are inherited by R/I — we leave the details of this to the
reader. In particular, the multiplicative identity of R/I is the coset 1R + I.
The ring R/I is called the quotient ring or residue class ring of R
modulo I.
Elements of R/I may be called residue classes. As a matter of notation,
for a ∈R, we deﬁne [a]I := a + I, and if I = dR, we may write this simply
as [a]d. If I is clear from context, we may also just write [a].
Example 9.33. For n ≥1, the ring Zn is precisely the quotient ring Z/nZ.
2
Example 9.34. Let f be a monic polynomial over a ring R with deg(f) =
ℓ≥0, and consider the quotient ring E := R[X]/fR[X]. By the division with
remainder property for polynomials (Theorem 9.12), for every a ∈R[X],
there exists a unique polynomial b ∈R[X] such that a ≡b (mod f) and
TEAM LinG

9.3 Ideals and quotient rings
233
deg(b) < ℓ. From this, it follows that every element of E can be written
uniquely as [b]f, where b ∈R[X] is a polynomial of degree less than ℓ.
The assumption that f is monic may be relaxed a bit: all that really
matters in this example is that the leading coeﬃcient of f is a unit, so that
the division with remainder property applies. Also, note that in this situa-
tion, we will generally prefer the more compact notation R[X]/(f), instead
of R[X]/fR[X]. 2
Example 9.35. Consider the polynomial f := X2 + X + 1 ∈Z2[X] and the
quotient ring E := Z2[X]/(f). Let us name the elements of E as follows:
00 := [0]f, 01 := [1]f, 10 := [X]f, 11 := [X + 1]f.
With this naming convention, addition of two elements in E corresponds to
just computing the bit-wise exclusive-or of their names. More precisely, the
addition table for E is the following:
+
00
01
10
11
00
00
01
10
11
01
01
00
11
10
10
10
11
00
01
11
11
10
01
00
Note that 00 acts as the additive identity for E, and that as an additive
group, E is isomorphic to the additive group Z2 × Z2.
As for multiplication in E, one has to compute the product of two poly-
nomials, and then reduce modulo f. For example, to compute 10 · 11, using
the identity X2 ≡X + 1 (mod f), one sees that
X · (X + 1) ≡X2 + X ≡(X + 1) + X ≡1 (mod f);
thus, 10 · 11 = 01. The reader may verify the following multiplication table
for E:
·
00
01
10
11
00
00
00
00
00
01
00
01
10
11
10
00
10
11
01
11
00
11
01
10
Observe that 01 acts as the multiplicative identity for E. Notice that every
non-zero element of E has a multiplicative inverse, and so E is in fact a ﬁeld.
By Theorem 9.16, we know that E∗must be cyclic (this fact also follows
from Theorem 8.32, and the fact that |E∗| = 3.) Indeed, the reader may
verify that both 10 and 11 have multiplicative order 3.
TEAM LinG

234
Rings
This is the ﬁrst example we have seen of a ﬁnite ﬁeld whose cardinality is
not prime. 2
Exercise 9.25. Let I be an ideal of a ring R, and let x and y be elements
of R with x ≡y (mod I). Let f ∈R[X]. Show that f(x) ≡f(y) (mod I).
Exercise 9.26. Let p be a prime, and consider the ring Q(p) (see Exam-
ple 9.23). Show that any non-zero ideal of Q(p) is of the form (pi), for some
uniquely determined integer i ≥0.
Exercise 9.27. Let R be a ring. Show that if I is a non-empty subset
of R[X] that is closed under addition, multiplication by elements of R, and
multiplication by X, then I is an ideal of R[X].
For the following three exercises, we need some deﬁnitions. An ideal I of
a ring R is called prime if I ⊊R and if for all a, b ∈R, ab ∈I implies a ∈I
or b ∈I. An ideal I of a ring R is called maximal if I ⊊R and there are
no ideals J of R such that I ⊊J ⊊R.
Exercise 9.28. Let R be a ring. Show that:
(a) an ideal I of R is prime if and only if R/I is an integral domain;
(b) an ideal I of R is maximal if and only if R/I is a ﬁeld;
(c) all maximal ideals of R are also prime ideals.
Exercise 9.29. This exercise explores some examples of prime and maximal
ideals.
(a) Show that in the ring Z, the ideal {0} is prime but not maximal, and
that the maximal ideals are precisely those of the form pZ, where p
is prime.
(b) More generally, show that in an integral domain D, the ideal {0} is
prime, and this ideal is maximal if and only if D is a ﬁeld.
(c) Show that in the ring F[X, Y], where F is a ﬁeld, the ideal (X, Y) is
maximal, while the ideals (X) and (Y) are prime, but not maximal.
Exercise 9.30. It is a fact that all non-trivial rings R contain at least one
maximal ideal. Showing this in general requires some fancy set-theoretic
notions. This exercise develops a proof in the case where R is countable
(i.e., ﬁnite or countably inﬁnite).
(a) Show that if R is non-trivial but ﬁnite, then it contains a maximal
ideal.
TEAM LinG

9.3 Ideals and quotient rings
235
(b) Assume that R is countably inﬁnite, and let a1, a2, a3, . . . be an
enumeration of the elements of R.
Deﬁne a sequence of ideals
I0, I1, I2, . . . , as follows. Set I0 := {0R}, and for i ≥0, deﬁne
Ii+1 :=
 Ii + aiR
if Ii + aiR ⊊R;
Ii
otherwise.
Finally, set
I :=
∞

i=0
Ii.
Show that I is a maximal ideal of R. Hint: ﬁrst show that I is an
ideal; then show that I ⊊R by assuming that 1R ∈I and deriving
a contradiction; ﬁnally, show that I is maximal by assuming that
for some i = 1, 2, . . . , we have I ⊊I + aiR ⊊R, and deriving a
contradiction.
For the following three exercises, we need the following deﬁnition: for
subsets X, Y of a ring R, let X · Y denote the set of all ﬁnite sums of the
form
x1y1 + · · · + xℓyℓ(with xk ∈X, yk ∈Y for k = 1, . . . , ℓ, for some ℓ≥0).
Note that X · Y contains 0R (the “empty” sum, with ℓ= 0).
Exercise 9.31. Let R be a ring, and S a subset of R. Show that S · R is
an ideal of R, and is the smallest ideal of R containing S.
Exercise 9.32. Let I and J be two ideals of a ring R. Show that:
(a) I · J is an ideal;
(b) if I and J are principal ideals, with I = aR and J = bR, then
I · J = abR, and so is also a principal ideal;
(c) I · J ⊆I ∩J;
(d) if I + J = R, then I · J = I ∩J.
Exercise 9.33. Let S be a subring of a ring R. Let I be an ideal of R, and
J an ideal of S. Show that:
(a) I ∩S is an ideal of S, and that (I ∩S) · R is an ideal of R contained
in I;
(b) (J · R) ∩S is an ideal of S containing J.
TEAM LinG

236
Rings
9.4 Ring homomorphisms and isomorphisms
Deﬁnition 9.20. A function ρ from a ring R to a ring R′ is called a ring
homomorphism if it is a group homomorphism with respect to the under-
lying additive groups of R and R′, and if in addition,
(i) ρ(ab) = ρ(a)ρ(b) for all a, b ∈R, and
(ii) ρ(1R) = 1R′.
Expanding the deﬁnition, we see that the requirements that ρ must satisfy
in order to be a ring homomorphism are that for all a, b ∈R, we have
ρ(a + b) = ρ(a) + ρ(b) and ρ(ab) = ρ(a)ρ(b), and that ρ(1R) = 1R′. Note
that some texts do not require that ρ(1R) = 1R′.
Since a ring homomorphism ρ from R to R′ is also an additive group
homomorphism, we may also adopt the notation and terminology for image
and kernel, and note that all the results of Theorem 8.20 apply as well here.
In particular, ρ(0R) = 0R′, ρ(a) = ρ(b) if and only if a ≡b (mod ker(ρ)),
and ρ is injective if and only if ker(ρ) = {0R}. However, we may strengthen
Theorem 8.20 as follows:
Theorem 9.21. Let ρ : R →R′ be a ring homomorphism.
(i) For any subring S of R, ρ(S) is a subring of R′.
(ii) For any ideal I of R, ρ(I) is an ideal of img(ρ).
(iii) ker(ρ) is an ideal of R.
(iv) For any ideal I′ of R′, ρ−1(I′) is an ideal of R.
Proof. Exercise. 2
Theorems 8.21 and 8.22 have natural ring analogs—one only has to show
that the corresponding group homomorphisms are also ring homomorphisms:
Theorem 9.22. If ρ : R →R′ and ρ′ : R′ →R′′ are ring homomorphisms,
then so is their composition ρ′ ◦ρ : R →R′′.
Proof. Exercise. 2
Theorem 9.23. Let ρi : R →Ri, for i = 1, . . . , n, be ring homomorphisms.
Then the map ρ : R →R1 × · · · × Rn that sends a ∈R to (ρ1(a), . . . , ρn(a))
is a ring homomorphism.
Proof. Exercise. 2
If a ring homomorphism ρ : R →R′ is a bijection, then it is called a ring
isomorphism of R with R′. If such a ring isomorphism ρ exists, we say
TEAM LinG

9.4 Ring homomorphisms and isomorphisms
237
that R is isomorphic to R′, and write R ∼= R′. Moreover, if R = R′, then
ρ is called a ring automorphism on R.
Analogous to Theorem 8.24, we have:
Theorem 9.24. If ρ is a ring isomorphism of R with R′, then the inverse
function ρ−1 is a ring isomorphism of R′ with R.
Proof. Exercise. 2
Because of this theorem, if R is isomorphic to R′, we may simply say that
“R and R′ are isomorphic.”
We stress that a ring isomorphism ρ of R with R′ is essentially just a
“renaming” of elements; in particular, ρ maps units to units and zero divisors
to zero divisors (verify); moreover, the restriction of the map ρ to R∗yields
a group isomorphism of R∗with (R′)∗(verify).
An injective ring homomorphism ρ : R →E is called an embedding
of R in E.
In this case, img(ρ) is a subring of E and R ∼= img(ρ).
If
the embedding is a natural one that is clear from context, we may simply
identify elements of R with their images in E under the embedding, and as
a slight abuse of terminology, we shall say that R as a subring of E.
We have already seen an example of this, namely, when we formally de-
ﬁned the ring of polynomials R[X] over R, we deﬁned the map ρ : R →R[X]
that sends c ∈R to the polynomial whose constant term is c, and all other
coeﬃcients zero. This map ρ is clearly an embedding, and it was via this
embedding that we identiﬁed elements of R with elements of R[X], and so
viewed R as a subring of R[X].
This practice of identifying elements of a ring with their images in another
ring under a natural embedding is very common. We shall see more examples
of this later (in particular, Example 9.43 below).
Theorems 8.25, 8.26, and 8.27 also have natural ring analogs—again, one
only has to show that the corresponding group homomorphisms are also ring
homomorphisms:
Theorem 9.25. If I is an ideal of a ring R, then the natural map ρ : R →
R/I given by ρ(a) = a + I is a surjective ring homomorphism whose kernel
is I.
Proof. Exercise. 2
Theorem 9.26. Let ρ be a ring homomorphism from R into R′. Then the
map ¯ρ : R/ ker(ρ) →img(ρ) that sends the coset a + ker(ρ) for a ∈R to
ρ(a) is unambiguously deﬁned and is a ring isomorphism of R/ ker(ρ) with
img(ρ).
TEAM LinG

238
Rings
Proof. Exercise. 2
Theorem 9.27. Let ρ be a ring homomorphism from R into R′.
Then
for any ideal I contained in ker(ρ), the map ¯ρ : R/I →img(ρ) that sends
the coset a + I for a ∈R to ρ(a) is unambiguously deﬁned and is a ring
homomorphism from R/I onto img(ρ) with kernel ker(ρ)/I.
Proof. Exercise. 2
Example 9.36. For n ≥1, the natural map ρ from Z to Zn sends a ∈Z
to the residue class [a]n. In Example 8.41, we noted that this is a surjective
group homomorphism on the underlying additive groups, with kernel nZ;
however, this map is also a ring homomorphism. 2
Example 9.37. As we saw in Example 8.42, if n1, . . . , nk are pairwise
relatively prime, positive integers, then the map from Z to Zn1×· · ·×Znk that
sends x ∈Z to ([x]n1, . . . , [x]nk) is a surjective group homomorphism on the
underlying additive groups, with kernel nZ, where n = k
i=1 ni. However,
this map is also a ring homomorphism (this follows from Example 9.36 and
Theorem 9.23). Therefore, by Theorem 9.26, the map that sends [x]n ∈
Zn to ([x]n1, . . . , [x]nk) is a ring isomorphism of the ring Zn with the ring
Zn1 × · · · × Znk. It follows that the restriction of this map to Z∗
n yields a
group isomorphism of the multiplicative groups Z∗
n and Z∗
n1 × · · · × Z∗
nk (see
Example 9.13). 2
Example 9.38. As we saw in Example 8.43, if n1, n2 are positive integers
with n1 > 1 and n1 | n2, then the map ¯ρ : Zn2 →Zn1 that sends [a]n2 to
[a]n1 is a surjective group homomorphism on the underlying additive groups
with kernel n1Zn2. This map is also a ring homomorphism. The map ¯ρ
can also be viewed as the map obtained by applying Theorem 9.27 with the
natural map ρ from Z to Zn1 and the ideal n2Z of Z, which is contained in
ker(ρ) = n1Z. 2
Example 9.39. Let R be a subring of a ring E, and ﬁx α ∈E.
The
polynomial evaluation map ρ : R[X] →E that sends a ∈R[X] to a(α) ∈E
is a ring homomorphism from R[X] into E (see Theorem 9.10). The image
of ρ consists of all polynomial expressions in α with coeﬃcients in R, and is
denoted R[α]. Note that R[α] is a subring of E containing R ∪{α}, and is
the smallest such subring of E. 2
Example 9.40. We can generalize the previous example to multi-variate
polynomials. If R is a subring of a ring E and α1, . . . , αn ∈E, then the
map ρ : R[X1, . . . , Xn] →E that sends a ∈R[X1, . . . , Xn] to a(α1, . . . , αn) is
TEAM LinG

9.4 Ring homomorphisms and isomorphisms
239
a ring homomorphism. Its image consists of all polynomial expressions in
α1, . . . , αn with coeﬃcients in R, and is denoted R[α1, . . . , αn]. Moreover,
this image is a subring of E containing R ∪{α1, . . . , αn}, and is the smallest
such subring of E. 2
Example 9.41. For any ring R, consider the map ρ : Z →R that sends
m ∈Z to m · 1R in R. This is clearly a ring homomorphism (verify). If
ker(ρ) = {0}, then img(ρ) ∼= Z, and so the ring Z is embedded in R, and
R has characteristic zero. If ker(ρ) = nZ for n > 0, then img(ρ) ∼= Zn, and
so the ring Zn is embedded in R, and R has characteristic n. Note that we
have n = 1 if and only if R is trivial.
Note that img(ρ) is the smallest subring of R; indeed, since any subring
of R must contain 1R and be closed under addition and subtraction, it must
contain img(ρ). 2
Example 9.42. Let R be a ring of prime characteristic p. For any a, b ∈R,
we have (see Exercise 9.2)
(a + b)p =
p

k=0
p
k

ap−kbk.
However, by Exercise 1.12, all of the binomial coeﬃcients are multiples of
p, except for k = 0 and k = p, and hence in the ring R, all of these terms
vanish, leaving us with
(a + b)p = ap + bp.
This result is often jokingly referred to as the “freshman’s dream,” for some-
what obvious reasons.
Of course, as always, we have
(ab)p = apbp and 1p
R = 1R,
and so it follows that the map ρ : R →R that sends a ∈R to ap is a
ring homomorphism. It also immediately follows that for any integer e ≥1,
the e-fold composition ρe : R →R that sends a ∈R to ape is also a ring
homomorphism. 2
Example 9.43. As in Example 9.34, let f be a monic polynomial over a
ring R with deg(f) = ℓ, but now assume that ℓ> 0. Consider the natural
map ρ from R[X] to the quotient ring E := R[X]/(f) that sends a ∈R[X] to
[a]f. If we restrict ρ to the subring R of R[X], we obtain an embedding of R
into E. Since this is a very natural embedding, one usually simply identiﬁes
TEAM LinG

240
Rings
elements of R with their images in E under ρ, and regards R as a subring
of E. Taking this point of view, we see that if a = 
i aiXi, then
[a]f = [

i
aiXi]f =

i
ai([X]f)i = a(η),
where η := [X]f ∈E. Therefore, the map ρ may be viewed as the polynomial
evaluation map, as in Example 9.39, that sends a ∈R[X] to a(η) ∈E. Note
that we have E = R[η]; moreover, every element of E can be expressed
uniquely as b(η) for some b ∈R[X] of degree less than ℓ, and more generally,
for arbitrary a, b ∈R[X], we have a(η) = b(η) if and only if a ≡b (mod f).
2
Example 9.44. As a special case of Example 9.43, let f := X2 + 1 ∈R[X],
and consider the quotient ring R[X]/(f). If we set i := [X]f ∈R[X]/(f), then
every element of R[X]/(f) can be expressed uniquely as a+bi, where a, b ∈R.
Moreover, we have i2 = −1, and more generally, for a, b, a′, b′ ∈R, we have
(a + bi) + (a′ + b′i) = (a + a′) + (b + b′)i
and
(a + bi) · (a′ + b′i) = (aa′ −bb′) + (ab′ + a′b)i.
Thus, the rules for arithmetic in R[X]/(f) are precisely the familiar rules of
complex arithmetic, and so C and R[X]/(f) are essentially the same, as rings.
Indeed, the “algebraically correct” way of deﬁning the complex numbers C
is simply to deﬁne them to be the quotient ring R[X]/(f) in the ﬁrst place.
This will be our point of view from now on. 2
Example 9.45. Consider the polynomial evaluation map ρ : R[X] →C =
R[X]/(X2 + 1) that sends g ∈R[X] to g(−i). For any g ∈R[X], we may write
g = (X2 + 1)q + a + bX, where q ∈R[X] and a, b ∈R. Since (−i)2 + 1 =
i2 + 1 = 0, we have g(−i) = ((−i)2 + 1)q(−i) + a −bi = a −bi. Clearly,
then, ρ is surjective and the kernel of ρ is the ideal of R[X] generated by the
polynomial X2 + 1. By Theorem 9.26, we therefore get a ring automorphism
¯ρ on C that sends a + bi ∈C to a −bi. In fact, ¯ρ it is none other than the
complex conjugation map. Indeed, this is the “algebraically correct” way of
deﬁning complex conjugation in the ﬁrst place. 2
Example 9.46. We deﬁned the ring Z[i] of Gaussian integers in Exam-
ple 9.22 as a subring of C. Let us verify that the notation Z[i] introduced in
Example 9.22 is consistent with that introduced in Example 9.39. Consider
the polynomial evaluation map ρ : Z[X] →C that sends g ∈Z[X] to g(i) ∈C.
TEAM LinG

9.4 Ring homomorphisms and isomorphisms
241
For any g ∈Z[X], we may write g = (X2 + 1)q + a + bX, where q ∈Z[X] and
a, b ∈Z. Since i2 + 1 = 0, we have g(i) = (i2 + 1)q(i) + a + bi = a + bi.
Clearly, then, the image of ρ is the set {a + bi : a, b ∈Z}, and the kernel of
ρ is the ideal of Z[X] generated by the polynomial X2 + 1. This shows that
Z[i] in Example 9.22 is the same as Z[i] in Example 9.39, and moreover,
Theorem 9.26 implies that Z[i] is isomorphic to Z[X]/(X2 + 1).
Thus, we can directly construct the Gaussian integers as the quotient ring
Z[X]/(X2 + 1). Likewise the ﬁeld Q[i] (see Exercise 9.8) can be constructed
directly as Q[X]/(X2 + 1). Such direct constructions are appealing in that
they are purely “elementary,” as they do not appeal to anything so “sophis-
ticated” as the real numbers. 2
Example 9.47. Let p be a prime, and consider the quotient ring E :=
Zp[X]/(X2 + 1). If we set i := [X]X2+1 ∈E, then E = Zp[i] = {a + bi : a, b ∈
Zp}. In particular, E is a ring of cardinality p2. Moreover, the rules for
addition and multiplication in E look exactly the same as they do in C: for
a, b, a′, b′ ∈Zp, we have
(a + bi) + (a′ + b′i) = (a + a′) + (b + b′)i
and
(a + bi) · (a′ + b′i) = (aa′ −bb′) + (ab′ + a′b)i.
Note that E may or may not be a ﬁeld.
On the one hand, suppose that c2 = −1 for some c ∈Zp (for example,
p = 2, p = 5, p = 13). Then (c + i)(c −i) = c2 + 1 = 0, and so E is not an
integral domain.
On the other hand, suppose there is no c ∈Zp such that c2 = −1 (for
example, p = 3, p = 7). Then for any a, b ∈Zp, not both zero, we must have
a2 + b2 ̸= 0; indeed, suppose that a2 + b2 = 0, and that, say, b ̸= 0; then
we would have (a/b)2 = −1, contradicting the assumption that −1 has no
square root in Zp. Since Zp is a ﬁeld, it follows that the same formula for
multiplicative inverses in C applies in E, namely,
(a + bi)−1 = a −bi
a2 + b2 .
This construction provides us with more examples of ﬁnite ﬁelds whose
cardinality is not prime. 2
Example 9.48. If ρ : R →R′ is a ring homomorphism, then we can extend
ρ in a natural way to a ring homomorphism from R[X] to R′[X], by deﬁning
ρ(
i aiXi) := 
i ρ(ai)Xi. We leave it to the reader to verify that this indeed
is a ring homomorphism. 2
TEAM LinG

242
Rings
Exercise 9.34. Verify that the “is isomorphic to” relation on rings is an
equivalence relation; that is, for all rings R1, R2, R3, we have:
(a) R1 ∼= R1;
(b) R1 ∼= R2 implies R2 ∼= R1;
(c) R1 ∼= R2 and R2 ∼= R3 implies R1 ∼= R3.
Exercise 9.35. Let R1, R2 be rings, and let ρ : R1 × R2 →R1 be the map
that sends (a1, a2) ∈R1 × R2 to a1 ∈R1. Show that ρ is a surjective ring
homomorphism whose kernel is {0R1} × R2.
Exercise 9.36. Let ρ be a ring homomorphism from R into R′. Show that
the ideals of R containing ker(ρ) are in one-to-one correspondence with the
ideals of img(ρ), where the ideal I of R containing ker(ρ) corresponds to the
ideal ρ(I) of img(ρ).
Exercise 9.37. Let ρ : R →S be a ring homomorphism.
Show that
ρ(R∗) ⊆S∗, and that the restriction of ρ to R∗yields a group homomorphism
ρ∗: R∗→S∗whose kernel is (1R + ker(ρ)) ∩R∗.
Exercise 9.38. Show that if F is a ﬁeld, then the only ideals of F are {0F }
and F. From this, conclude the following: if ρ : F →R is a ring homomor-
phism from F into a non-trivial ring R, then ρ must be an embedding.
Exercise 9.39. Let n be a positive integer.
(a) Show that the rings Z[X]/(n) and Zn[X] are isomorphic.
(b) Assuming that n = pq, where p and q are distinct primes, show that
the rings Zn[X] and Zp[X] × Zq[X] are isomorphic.
Exercise 9.40. Let n be a positive integer, let f ∈Z[X] be a monic poly-
nomial, and let ¯f be the image of f in Zn[X] (i.e., ¯f is obtained by applying
the natural map from Z to Zn coeﬃcient-wise to f). Show that the rings
Z[X]/(n, f) and Zn[X]/( ¯f) are isomorphic.
Exercise 9.41. Let R be a ring, and let α1, . . . , αn be elements of R. Show
that the rings R and R[X1, . . . , Xn]/(X1 −α1, . . . , Xn −αn) are isomorphic.
Exercise 9.42. Let ρ : R →R′ be a ring homomorphism, and suppose
that we extend ρ, as in Example 9.48, to a ring homomorphism from R[X]
to R′[X]. Show that for any a ∈R[X], we have D(ρ(a)) = ρ(D(a)), where
D(·) denotes the formal derivative.
Exercise 9.43. This exercise and the next generalize the Chinese remainder
theorem to arbitrary rings. Suppose I and J are two ideals of a ring R such
TEAM LinG

9.4 Ring homomorphisms and isomorphisms
243
that I + J = R. Show that the map ρ : R →R/I × R/J that sends a ∈R
to ([a]I, [a]J) is a surjective ring homomorphism with kernel I · J. Conclude
that R/(I · J) is isomorphic to R/I × R/J.
Exercise 9.44. Generalize the previous exercise, showing that R/(I1 · · · Ik)
is isomorphic to R/I1 × · · · × R/Ik, where R is a ring, and I1, . . . , Ik are
ideals of R, provided Ii + Ij = R for all i, j such that i ̸= j.
Exercise 9.45. Let F be a ﬁeld and let d be an element of F that is not a
perfect square (i.e., there does not exist e ∈F such that e2 = d). Let E :=
F[X]/(X2 −d), and let η := [X]X2−d, so that E = F[η] = {a + bη : a, b ∈F}.
(a) Show that the quotient ring E is a ﬁeld, and write down the formula
for the inverse of a + bη ∈E.
(b) Show that the map that sends a + bη ∈E to a −bη is a ring auto-
morphism on E.
Exercise 9.46. Let Q(m) be the subring of Q deﬁned in Example 9.23. Let
us deﬁne the map ρ : Q(m) →Zm as follows. For a/b ∈Q with b relatively
prime to m, ρ(a/b) := [a]m([b]m)−1. Show that ρ is unambiguously deﬁned,
and is a surjective ring homomorphism. Also, describe the kernel of ρ.
Exercise 9.47. Let ρ : R →R′ be a map from a ring R to a ring R′ that
satisﬁes all the requirements of a ring homomorphism, except that we do
not require that ρ(1R) = 1R′.
(a) Give a concrete example of such a map ρ, such that ρ(1R) ̸= 1R′ and
ρ(1R) ̸= 0R′.
(b) Show that img(ρ) is a ring in which ρ(1R) plays the role of the mul-
tiplicative identity.
(c) Show that if R′ is an integral domain, and ρ(1R) ̸= 0R′, then ρ(1R) =
1R′, and hence ρ satisﬁes our deﬁnition of a ring homomorphism.
(d) Show that if ρ is surjective, then ρ(1R) = 1R′, and hence ρ satisﬁes
our deﬁnition of a ring homomorphism.
TEAM LinG

10
Probabilistic primality testing
In this chapter, we discuss some simple and eﬃcient probabilistic tests for
primality.
10.1 Trial division
Suppose we are given an integer n > 1, and we want to determine whether n
is prime or composite. The simplest algorithm to describe and to program
is trial division. We simply divide n by 2, 3, and so on, testing if any of
these numbers evenly divide n. Of course, we don’t need to go any further
than √n, since if n has any non-trivial factors, it must have one that is no
greater than √n (see Exercise 1.1). Not only does this algorithm determine
whether n is prime or composite, it also produces a non-trivial factor of n
in case n is composite.
Of course, the drawback of this algorithm is that it is terribly ineﬃcient:
it requires Θ(√n) arithmetic operations, which is exponential in the binary
length of n. Thus, for practical purposes, this algorithm is limited to quite
small n. Suppose, for example, that n has 100 decimal digits, and that a
computer can perform 1 billion divisions per second (this is much faster than
any computer existing today). Then it would take on the order of 1033 years
to perform √n divisions.
In this chapter, we discuss a much faster primality test that allows 100
decimal digit numbers to be tested for primality in less than a second. Unlike
the above test, however, this test does not ﬁnd a factor of n when n is
composite. Moreover, the algorithm is probabilistic, and may in fact make
a mistake. However, the probability that it makes a mistake can be made
so small as to be irrelevant for all practical purposes. Indeed, we can easily
make the probability of error as small as 2−100 — should one really care
about an event that happens with such a miniscule probability?
244
TEAM LinG

10.2 The structure of Z∗
n
245
10.2 The structure of Z∗
n
Before going any further, we have to have a ﬁrm understanding of the group
Z∗
n, for integer n > 1. As we know, Z∗
n consists of those elements [a]n ∈Zn
such that a is an integer relatively prime to n. Suppose n = pe1
1 · · · per
r is the
factorization of n into primes. By the Chinese remainder theorem, we have
the ring isomorphism
Zn ∼= Zpe1
1 × · · · × Zper
r
which induces a group isomorphism
Z∗
n ∼= Z∗
pe1
1 × · · · × Z∗
per
r .
Thus, to determine the structure of the group Z∗
n for general n, it suﬃces
to determine the structure for n = pe, where p is prime. By Theorem 2.13,
we already know the order of the group Z∗
pe, namely, φ(pe) = pe−1(p −1).
The main result of this section is the following:
Theorem 10.1. If p is an odd prime, then for any positive integer e, the
group Z∗
pe is cyclic. The group Z∗
2e is cyclic for e = 1 or 2, but not for e ≥3.
For e ≥3, Z∗
2e is isomorphic to the additive group Z2 × Z2e−2.
In the case where e = 1, this theorem is a special case of Theorem 9.16,
which we proved in §9.2.3. Note that for e > 1, the ring Zpe is not a ﬁeld,
and so Theorem 9.16 cannot be used directly. To deal with the case e > 1,
we need a few simple facts.
Theorem 10.2. Let p be a prime. For integer e ≥1, if a ≡b (mod pe),
then ap ≡bp (mod pe+1).
Proof. We have a = b + cpe for some c ∈Z. Thus, ap = bp + pbp−1cpe + dp2e
for an integer d. It follows that ap ≡bp (mod pe+1). 2
Theorem 10.3. Let p be a prime. Let e ≥1 be an integer and assume
pe > 2. If a ≡1 + pe (mod pe+1), then ap ≡1 + pe+1 (mod pe+2).
Proof. By Theorem 10.2, ap ≡(1 + pe)p (mod pe+2). Expanding (1 + pe)p,
we have
(1 + pe)p = 1 + p · pe +
p−1

k=2
p
k

pek + pep.
By Exercise 1.12, all of the terms in the sum on k are divisible by p1+2e, and
1 + 2e ≥e + 2 for all e ≥1. For the term pep, the assumption that pe > 2
means that either p ≥3 or e ≥2, which implies ep ≥e + 2. 2
TEAM LinG

246
Probabilistic primality testing
Now consider Theorem 10.1 in the case where p is odd. As we already
know that Z∗
p is cyclic, assume e > 1. Let x ∈Z be chosen so that [x]p
generates Z∗
p. Suppose the multiplicative order of [x]pe ∈Z∗
pe is m. Then
as xm ≡1 (mod pe) implies xm ≡1 (mod p), it must be the case that
p −1 divides m, and so [xm/(p−1)]pe has multiplicative order exactly p −1.
By Theorem 8.38, if we ﬁnd an integer y such that [y]pe has multiplicative
order pe−1, then [xm/(p−1)y]pe has multiplicative order (p −1)pe−1, and we
are done. We claim that y := 1 + p does the job. Any integer between 0
and pe −1 can be expressed as an e-digit number in base p; for example,
y = (0 · · · 0 1 1)p. If we compute successive pth powers of y modulo pe, then
by Theorem 10.3 we have
y mod pe
=
(0
· · ·
0 1 1)p,
yp mod pe
=
(∗
· · ·
∗1 0 1)p,
yp2 mod pe
=
(∗
· · ·
∗1 0 0 1)p,
...
ype−2 mod pe
=
(1 0
· · ·
0 1)p,
ype−1 mod pe
=
(0
· · ·
0 1)p.
Here, “∗” indicates an arbitrary digit. From this table of values, it is clear
(see Theorem 8.37) that [y]pe has multiplicative order pe−1. That proves
Theorem 10.1 for odd p.
We now prove Theorem 10.1 in the case p = 2. For e = 1 and e = 2, the
theorem is easily veriﬁed. Suppose e ≥3. Consider the subgroup G ⊆Z∗
2e
generated by [5]2e. Expressing integers between 0 and 2e−1 as e-digit binary
numbers, and applying Theorem 10.3, we have
5 mod 2e
=
(0
· · ·
0 1 0 1)2,
52 mod 2e
=
(∗
· · ·
∗1 0 0 1)2,
...
52e−3 mod 2e
=
(1 0
· · ·
0 1)2,
52e−2 mod 2e
=
(0
· · ·
0 1)2.
So it is clear (see Theorem 8.37) that [5]2e has multiplicative order 2e−2.
We claim that [−1]2e /∈G. If it were, then since it has multiplicative order
2, and since any cyclic group of even order has precisely one element of
order 2 (see Theorem 8.31), it must be equal to [52e−3]2e; however, it is clear
from the above calculation that 52e−3 ̸≡−1 (mod 2e). Let H ⊆Z∗
2e be the
subgroup generated by [−1]2e. Then from the above, G ∩H = {[1]2e}, and
hence by Theorem 8.28, G × H is isomorphic to the subgroup G · H of Z∗
2e.
TEAM LinG

10.3 The Miller–Rabin test
247
But since the orders of G×H and Z∗
2e are equal, we must have G·H = Z∗
2e.
That proves the theorem.
Exercise 10.1. Show that if n is a positive integer, the group Z∗
n is cyclic
if and only if
n = 1, 2, 4, pe, or 2pe,
where p is an odd prime and e is a positive integer.
Exercise 10.2. Let n = pq, where p and q are distinct primes such that
p = 2p′ + 1 and q = 2q′ + 1, where p′ and q′ are themselves prime. Show
that the subgroup (Z∗
n)2 of squares is a cyclic group of order p′q′.
Exercise 10.3. Let n = pq, where p and q are distinct primes such that
p ∤(q −1) and q ∤(p −1).
(a) Show that the map that sends [a]n ∈Z∗
n to [an]n2 ∈(Z∗
n2)n is a group
isomorphism.
(b) Consider the element α := [1 + n]n2 ∈Z∗
n2; show that for any non-
negative integer k, αk = [1 + kn]n2, and conclude that α has multi-
plicative order n.
(c) Show that the map from Zn × Z∗
n to Z∗
n2 that sends ([k]n, [a]n) to
[(1 + kn)an]n2 is a group isomorphism.
10.3 The Miller–Rabin test
We describe in this section a fast (polynomial time) test for primality, known
as the Miller–Rabin test. The algorithm, however, is probabilistic, and
may (with small probability) make a mistake.
We assume for the remainder of this section that the number n we are
testing for primality is an odd integer greater than 1.
Several probabilistic primality tests, including the Miller–Rabin test, have
the following general structure. Deﬁne Z+
n to be the set of non-zero elements
of Zn; thus, |Z+
n | = n −1, and if n is prime, Z+
n = Z∗
n. Suppose also that we
deﬁne a set Ln ⊆Z+
n such that:
• there is an eﬃcient algorithm that on input n and α ∈Z+
n , determines
if α ∈Ln;
• if n is prime, then Ln = Z∗
n;
• if n is composite, |Ln| ≤c(n −1) for some constant c < 1.
TEAM LinG

248
Probabilistic primality testing
To test n for primality, we set an “error parameter” t, and choose random
elements α1, . . . , αt ∈Z+
n . If αi ∈Ln for all i = 1, . . . , t, then we output
true; otherwise, we output false.
It is easy to see that if n is prime, this algorithm always outputs true, and
if n is composite this algorithm outputs true with probability at most ct. If
c = 1/2 and t is chosen large enough, say t = 100, then the probability that
the output is wrong is so small that for all practical purposes, it is “just as
good as zero.”
We now make a ﬁrst attempt at deﬁning a suitable set Ln. Let us deﬁne
Ln := {α ∈Z+
n : αn−1 = 1}.
Note that Ln ⊆Z∗
n, since if αn−1 = 1, then α has a multiplicative inverse,
namely, αn−2. Using a repeated-squaring algorithm, we can test if α ∈Ln
in time O(len(n)3).
Theorem 10.4. If n is prime, then Ln = Z∗
n. If n is composite and Ln ⊊
Z∗
n, then |Ln| ≤(n −1)/2.
Proof. Note that Ln is the kernel of the (n−1)-power map on Z∗
n, and hence
is a subgroup of Z∗
n.
If n is prime, then we know that Z∗
n is a group of order n −1. Since the
order of a group element divides the order of the group, we have αn−1 = 1
for all α ∈Z∗
n. That is, Ln = Z∗
n.
Suppose that n is composite and Ln ⊊Z∗
n. Since the order of a subgroup
divides the order of the group, we have |Z∗
n| = m|Ln| for some integer m > 1.
From this, we conclude that
|Ln| = 1
m|Z∗
n| ≤1
2|Z∗
n| ≤n −1
2
. 2
Unfortunately, there are odd composite numbers n such that Ln = Z∗
n.
Such numbers are called Carmichael numbers. The smallest Carmichael
number is
561 = 3 · 11 · 17.
Carmichael numbers are extremely rare, but it is known that there are in-
ﬁnitely many of them, so we can not ignore them. The following theorem
puts some constraints on Carmichael numbers.
Theorem 10.5. A Carmichael number n is of the form n = p1 · · · pr, where
the pi are distinct primes, r ≥3, and (pi −1) | (n −1) for i = 1, . . . , r.
TEAM LinG

10.3 The Miller–Rabin test
249
Proof. Let n = pe1
1 · · · per
r be a Carmichael number. By the Chinese remain-
der theorem, we have an isomorphism of Z∗
n with the group
Z∗
pe1
1 × · · · × Z∗
per
r ,
and we know that each group Z∗
pei
i
is cyclic of order pei−1
i
(pi −1). Thus,
the power n −1 kills the group Z∗
n if and only if it kills all the groups Z∗
pei
i ,
which happens if and only if pei−1
i
(pi −1) | (n −1). Now, on the one hand,
n ≡0 (mod pi). On the other hand, if ei > 1, we would have n ≡1 (mod pi),
which is clearly impossible. Thus, we must have ei = 1.
It remains to show that r ≥3. Suppose r = 2, so that n = p1p2. We have
n −1 = p1p2 −1 = (p1 −1)p2 + (p2 −1).
Since (p1 −1) | (n −1), we must have (p1 −1) | (p2 −1). By a symmetric
argument, (p2 −1) | (p1 −1). Hence, p1 = p2, a contradiction. 2
To obtain a good primality test, we need to deﬁne a diﬀerent set L′
n, which
we do as follows. Let n −1 = 2hm, where m is odd (and h ≥1 since n is
assumed odd), and deﬁne
L′
n := {α ∈Z+
n :
αm2h = 1 and
for j = 0, . . . , h −1, αm2j+1 = 1 implies αm2j = ±1}.
The Miller–Rabin test uses this set L′
n, in place of the set Ln deﬁned
above. It is clear from the deﬁnition that L′
n ⊆Ln.
Testing whether a given α ∈Z+
n belongs to L′
n can be done using the
following procedure:
β ←αm
if β = 1 then return true
for j ←0 to h −1 do
if β = −1 then return true
if β = +1 then return false
β ←β2
return false
It is clear that using a repeated-squaring algorithm, this procedure runs
in time O(len(n)3). We leave it to the reader to verify that this procedure
correctly determines membership in L′
n.
Theorem 10.6. If n is prime, then L′
n = Z∗
n.
If n is composite, then
|L′
n| ≤(n −1)/4.
TEAM LinG

250
Probabilistic primality testing
The rest of this section is devoted to a proof of this theorem. Let n −1 =
m2h, where m is odd.
Case 1: n is prime. Let α ∈Z∗
n. Since Z∗
n is a group of order n −1,
and the order of a group element divides the order of the group, we know
that αm2h = αn−1 = 1. Now consider any index j = 0, . . . , h −1 such that
αm2j+1 = 1, and consider the value β := αm2j. Then since β2 = αm2j+1 = 1,
the only possible choices for β are ±1—this is because Z∗
n is cyclic of even
order and so there are exactly two elements of Z∗
n whose multiplicative order
divides 2, namely ±1. So we have shown that α ∈L′
n.
Case 2: n = pe, where p is prime and e > 1. Certainly, L′
n is contained
in the kernel K of the (n −1)-power map on Z∗
n. By Theorem 8.31, |K| =
gcd(φ(n), n −1). Since n = pe, we have φ(n) = pe−1(p −1), and so
|L′
n| ≤|K| = gcd(pe−1(p −1), pe −1) = p −1 =
pe −1
pe−1 + · · · + 1 ≤n −1
4
.
Case 3: n = pe1
1 · · · per
r
is the prime factorization of n, and r > 1. For
i = 1, . . . , r, let Ri denote the ring Zpei
i , and let
θ : R1 × · · · × Rr →Zn
be the ring isomorphism provided by the Chinese remainder theorem.
Also, let φ(pei
i ) = mi2hi, with mi odd, for i = 1, . . . , r, and let ℓ:=
min{h, h1, . . . , hr}. Note that ℓ≥1, and that each R∗
i is a cyclic group
of order mi2hi.
We ﬁrst claim that for any α ∈L′
n, we have αm2ℓ= 1. To prove this,
ﬁrst note that if ℓ= h, then by deﬁnition, αm2ℓ= 1, so suppose that ℓ< h.
By way of contradiction, suppose that αm2ℓ̸= 1, and let j be the largest
index in the range ℓ, . . . , h −1 such that αm2j+1 = 1. By the deﬁnition
of L′
n, we must have αm2j = −1. Since ℓ< h, we must have ℓ= hi for
some particular index i = 1, . . . , r.
Writing α = θ(α1, . . . , αr), we have
αm2j
i
= −1. This implies that the multiplicative order of αm
i
is equal to
2j+1 (see Theorem 8.37). However, since j ≥ℓ= hi, this contradicts the
fact that the order of a group element (in this case, αm
i ) must divide the
order of the group (in this case, R∗
i ).
From the claim in the previous paragraph, and the deﬁnition of L′
n, it
follows that α ∈L′
n implies αm2ℓ−1 = ±1. We now consider an experiment in
which α is chosen at random from Z∗
n (that is, with a uniform distribution),
and show that P[αm2ℓ−1 = ±1] ≤1/4, from which the theorem will follow.
Write α = θ(α1, . . . , αr). As α is uniformly distributed over Z∗
n, each αi is
uniformly distributed over R∗
i , and the collection of all the αi is a mutually
independent collection of random variables.
TEAM LinG

10.3 The Miller–Rabin test
251
For i = 1, . . . , r and j = 0, . . . , h, let Gi(j) denote the image of the (m2j)-
power map on R∗
i . By Theorem 8.31, we have
|Gi(j)| =
mi2hi
gcd(mi2hi, m2j).
Because ℓ≤h and ℓ≤hi, a simple calculation shows that
|Gi(h)| divides |Gi(ℓ)| and 2|Gi(ℓ)| = |Gi(ℓ−1)|.
In particular, |Gi(ℓ−1)| is even and is no smaller than 2|Gi(h)|. The fact
that |Gi(ℓ−1)| is even implies that −1 ∈Gi(ℓ−1).
The event αm2ℓ−1 = ±1 occurs if and only if either
(E1) αm2ℓ−1
i
= 1 for i = 1, . . . , r, or
(E2) αm2ℓ−1
i
= −1 for i = 1, . . . , r.
Since the events E1 and E2 are disjoint, and since the values αm2ℓ−1
i
are
mutually independent, with each value αm2ℓ−1
i
uniformly distributed over
Gi(ℓ−1) (see part (a) of Exercise 8.22), and since Gi(ℓ−1) contains ±1,
we have
P[αm2ℓ−1 = ±1] = P[E1] + P[E2] = 2
r

i=1
1
|Gi(ℓ−1)|,
and since |Gi(ℓ−1)| ≥2|Gi(h)|, we have
P[αm2ℓ−1 = ±1] ≤2−r+1
r

i=1
1
|Gi(h)|.
(10.1)
If r ≥3, then (10.1) directly implies that P[αm2ℓ−1 = ±1] ≤1/4, and we
are done. So suppose that r = 2. In this case, Theorem 10.5 implies that
n is not a Carmichael number, which implies that for some i = 1, . . . , r, we
must have Gi(h) ̸= {1}, and so |Gi(h)| ≥2, and (10.1) again implies that
P[αm2ℓ−1 = ±1] ≤1/4.
That completes the proof of Theorem 10.6.
Exercise 10.4. Show that an integer n > 1 is prime if and only if there
exists an element in Z∗
n of multiplicative order n −1.
Exercise 10.5. Let p be a prime. Show that n := 2p + 1 is a prime if and
only if 2n−1 ≡1 (mod n).
TEAM LinG

252
Probabilistic primality testing
Exercise 10.6. Here is another primality test that takes as input an odd
integer n > 1, and a positive integer parameter t. The algorithm chooses
α1, . . . , αt ∈Z+
n at random, and computes
βi := α(n−1)/2
i
(i = 1, . . . , t).
If (β1, . . . , βt) is of the form (±1, ±1, . . . , ±1), but is not equal to (1, 1, . . . , 1),
the algorithm outputs true; otherwise, the algorithm outputs false. Show
that if n is prime, then the algorithm outputs false with probability at most
2−t, and if n is composite, the algorithm outputs true with probability at
most 2−t.
In the terminology of §7.2, the algorithm in the above exercise is an exam-
ple of an “Atlantic City” algorithm for the language of prime numbers (or
equivalently, the language of composite numbers), while the Miller–Rabin
test is an example of a “Monte Carlo” algorithm for the language of com-
posite numbers.
10.4 Generating random primes using the Miller–Rabin test
The Miller–Rabin test is the most practical algorithm known for testing
primality, and because of this, it is widely used in many applications, espe-
cially cryptographic applications where one needs to generate large, random
primes (as we saw in §7.8). In this section, we discuss how one uses the
Miller–Rabin test in several practically relevant scenarios where one must
generate large primes.
10.4.1 Generating a random prime between 2 and M
Suppose one is given an integer M ≥2, and wants to generate a random
prime between 2 and M. We can do this by simply picking numbers at
random until one of them passes a primality test. We discussed this problem
in some detail in §7.5, where we assumed that we had a primality test
IsPrime. The reader should review §7.5, and §7.5.1 in particular. In this
section, we discuss aspects of this problem that are speciﬁc to the situation
where the Miller–Rabin test is used to implement IsPrime.
To be more precise, let us deﬁne the following algorithm MR(n, t), which
takes as input integers n and t, with n > 1 and t ≥1, and runs as follows:
TEAM LinG

10.4 Generating random primes using the Miller–Rabin test
253
Algorithm MR(n, t):
if n = 2 then return true
if n is even then return false
repeat t times
α ←R {1, . . . , n −1}
if α ̸∈L′
n return false
return true
So we shall implement IsPrime(·) as MR(·, t), where t is an auxiliary
parameter. By Theorem 10.6, if n is prime, the output of MR(n, t) is always
true, while if n is composite, the output is true with probability at most 4−t.
Thus, this implementation of IsPrime satisﬁes the assumptions in §7.5.1,
with ϵ = 4−t.
Let γ(M, t) be the probability that the output of Algorithm RP in §7.5—
using this implementation of IsPrime —is composite. Then as we discussed
in §7.5.1,
γ(M, t) ≤4−t M −1
π(M) = O(4−tk),
(10.2)
where k = len(M). Furthermore, if the output of Algorithm RP is prime,
then every prime is equally likely; that is, conditioning on the event that
the output is prime, the conditional output distribution is uniform over all
primes.
Let us now consider the expected running time of Algorithm RP. As was
shown in §7.5.1, this is O(kW ′
M), where W ′
M is the expected running time
of IsPrime where the average is taken with respect to the random choice of
input n ∈{2, . . . , M} and the random choices of the primality test itself.
Clearly, we have W ′
M = O(tk3), since MR(n, t) executes at most t iterations
of the Miller–Rabin test, and each such test takes time O(k3). This leads to
an expected total running time bound of O(tk4). However, this estimate for
W ′
M is overly pessimistic. Intuitively, this is because when n is composite, we
expect to perform very few Miller–Rabin tests—only when n is prime do we
actually perform all t of them. To make a rigorous argument, consider the
experiment in which n is chosen at random from {2, . . . , M}, and MR(n, t)
is executed. Let Y be the number of times the basic Miller–Rabin test is
actually executed. Conditioned on any ﬁxed, odd, prime value of n, the
value of Y is always t. Conditioned on any ﬁxed, odd, composite value of
n, the distribution of Y is geometric with an associated success probability
of at least 3/4; thus, the conditional expectation of Y is at most 4/3 in this
TEAM LinG

254
Probabilistic primality testing
case. Thus, we have
E[Y ] = E[Y | n prime]P[n prime] + E[Y | n composite]P[n composite]
≤tπ(M)/(M −1) + 4/3.
Thus, E[Y ] ≤4/3 + O(t/k), from which it follows that W ′
M = O(k3 + tk2),
and hence the expected total running time of Algorithm RP is actually
O(k4 + tk3).
Note that the above estimate (10.2) for γ(M, t) is actually quite pes-
simistic. This is because the error probability 4−t is a worst-case estimate;
in fact, for “most” composite integers n, the probability that MR(n, t) out-
puts true is much smaller than this. In fact, γ(M, 1) is very small for large
M. For example, the following is known:
Theorem 10.7. We have
γ(M, 1) ≤exp[−(1 + o(1)) log(M) log(log(log(M)))/ log(log(M))].
Proof. Literature—see §10.7. 2
The bound in the above theorem goes to zero quite quickly—faster than
(log M)−c for any positive constant c. While the above theorem is asymp-
totically very good, in practice, one needs explicit bounds. For example, the
following lower bounds for −log2(γ(2k, 1)) are known:
k
200
300
400
500
600
3
19
37
55
74
Given an upper bound on γ(M, 1), we can bound γ(M, t) for t ≥2 using
the following inequality:
γ(M, t) ≤
γ(M, 1)
1 −γ(M, 1)4−t+1.
(10.3)
To prove (10.3), it is not hard to see that on input M, the output distribution
of Algorithm RP is the same as that of the following algorithm:
repeat
repeat
n ←R {2, . . . , M}
until MR(n, 1)
n1 ←n
until MR(n1, t −1)
output n1
TEAM LinG

10.4 Generating random primes using the Miller–Rabin test
255
Consider for a moment a single execution of the outer loop of the above
algorithm. Let β be the probability that n1 is composite, and let α be the
conditional probability that MR(n1, t −1) outputs true, given that n1 is
composite. Evidently, β = γ(M, 1) and α ≤4−t+1.
Now, using exactly the same reasoning as was used to derive equation
(7.2) in §7.5.1, we ﬁnd that
γ(M, t) =
αβ
αβ + (1 −β) ≤
αβ
1 −β ≤4−t+1γ(M, 1)
1 −γ(M, 1) ,
which proves (10.3).
Given that γ(M, 1) is so small, for large M, Algorithm RP actually
exhibits the following behavior in practice: it generates a random value
n ∈{2, . . . , M}; if n is odd and composite, then the very ﬁrst iteration of
the Miller–Rabin test will detect this with overwhelming probability, and no
more iterations of the test are performed on this n; otherwise, if n is prime,
the algorithm will perform t −1 more iterations of the Miller–Rabin test,
“just to make sure.”
Exercise 10.7. Consider the problem of generating a random Sophie Ger-
main prime between 2 and M (see §5.5.5). One algorithm to do this is as
follows:
repeat
n ←R {2, . . . , M}
if MR(n, t) then
if MR(2n + 1, t) then
output n and halt
forever
Assuming Conjecture 5.26, show that this algorithm runs in expected time
O(k5 +tk4), and outputs a number that is not a Sophie Germain prime with
probability O(4−tk2). As usual, k := len(M).
Exercise 10.8. Improve the algorithm in the previous exercise, so that un-
der the same assumptions, it runs in expected time O(k5+tk3), and outputs
a number that is not a Sophie Germain prime with probability O(4−tk2),
or even better, show that this probability is at most γ(M, t)π∗(M)/π(M) =
O(γ(M, t)k), where π∗(M) is deﬁned as in §5.5.5.
Exercise 10.9. Suppose in Algorithm RFN in §7.7 we implement algorithm
IsPrime(·) as MR(·, t), where t is a parameter satisfying 4−t(2 + log M) ≤
TEAM LinG

256
Probabilistic primality testing
1/2, if M is the input to RFN. Show that the expected running time of
Algorithm RFN in this case is O(k5 + tk4 len(k)). Hint: use Exercise 7.20.
10.4.2 Trial division up to a small bound
In generating a random prime, most candidates n will in fact be composite,
and so it makes sense to cast these out as quickly as possible. Signiﬁcant
eﬃciency gains can be achieved by testing if a given candidate n is divisible
by any small primes up to a given bound s, before we subject n to a Miller–
Rabin test. This strategy makes sense, since for a small, “single precision”
prime p, we can test if p | n essentially in time O(len(n)), while a single
iteration of the Miller–Rabin test takes time O(len(n)3) steps.
To be more precise, let us deﬁne the following algorithm MRS(n, t, s),
which takes as input integers n, t, and s, with n > 1, t ≥1, and s > 1:
Algorithm MRS(n, t, s):
for each prime p ≤s do
if p | n then
if p = n then return true else return false
repeat t times
α ←R {1, . . . , n −1}
if α ̸∈L′
n return false
return true
In an implementation of the above algorithm, one would most likely use
the sieve of Eratosthenes (see §5.4) to generate the small primes.
Note that MRS(n, t, 2) is equivalent to MR(n, t). Also, it is clear that the
probability that MRS(n, t, s) makes a mistake is no more than the prob-
ability that MR(n, t) makes a mistake. Therefore, using MRS in place of
MR will not increase the probability that the output of Algorithm RP is a
composite—indeed, it is likely that this probability decreases signiﬁcantly.
Let us now analyze the impact on the running time. To do this, we need
to estimate the probability τ(M, s) that a randomly chosen number between
2 and M is not divisible by any primes up to s. If M is suﬃciently large
with respect to s, the following heuristic argument can be made rigorous,
as we will discuss below. The probability that a random number is divisible
by a prime p is about 1/p, so the probability that it is not divisible by p is
about 1 −1/p. Assuming that these events are essentially independent for
TEAM LinG

10.4 Generating random primes using the Miller–Rabin test
257
diﬀerent values of p (this is the heuristic part), we estimate
τ(M, s) ≈

p≤s
(1 −1/p) ∼B1/ log s,
where B1 ≈0.56146 is the constant from Exercise 5.14 (see also Theo-
rem 5.21).
Of course, performing the trial division takes some time, so let us also
estimate the expected number κ(M, s) of trial divisions performed.
If
p1, p2, . . . , pr are the primes up to s, then for i = 1, . . . , r, the probabil-
ity that we perform at least i trial divisions is precisely τ(M, pi −1). From
this, it follows (see Theorem 6.8) that
κ(M, s) =

p≤s
τ(M, p −1) ≈

p≤s
B1/ log p.
Using Exercise 5.9 and the Prime number theorem, we obtain
κ(M, s) ≈

p≤s
B1/ log p ∼B1π(s)/ log s ∼B1s/(log s)2.
If k = len(M), for a random n ∈{2, . . . , M}, the expected amount of
time spent within MRS(n, t, s) performing the Miller–Rabin test is now
easily seen to be O(k3/ len(s)+tk2). Further, assuming that each individual
trial division step takes time O(len(n)), the expected running time of trial
division up to s is O(ks/ len(s)2). This estimate does not take into account
the time to generate the small primes using the sieve of Eratosthenes. These
values might be pre-computed, in which case this time is zero, but even if we
compute them on the ﬂy, this takes time O(s len(len(s))), which is dominated
by O(ks/ len(s)2)) for any reasonable value of s (in particular, for s ≤kO(1)).
So provided s = o(k2 len(k)), the running time of MRS will be dominated
by the Miller–Rabin test, which is what we want, of course—if we spend
as much time on trial division as the time it would take to perform a single
Miller–Rabin test, we might as well just perform the Miller–Rabin test. In
practice, one should use a very conservative bound for s, probably no more
than k2, since getting s arbitrarily close to optimal does not really provide
that much beneﬁt, while if we choose s too large, it can actually do signiﬁcant
harm.
From the above estimates, we can conclude that with k ≤s ≤k2, the
expected running time W ′
M of MRS(n, t, s), with respect to a randomly
chosen n between 2 and M, is
W ′
M = O(k3/ len(k) + tk2).
(10.4)
TEAM LinG

258
Probabilistic primality testing
From this, it follows that the expected running time of Algorithm RP on
input M is O(k4/ len(k) + tk3). Thus, we eﬀectively reduce the running
time by a factor proportional to len(k), which is a very real and noticeable
improvement in practice.
The reader may have noticed that in our analysis of MRS, we as-
sumed that computing n mod p for a “small” prime p takes time
O(len(n)). However, if we strictly followed the rules established in
Theorem 3.3, we should charge time O(len(n) len(p)) for this divi-
sion step. To answer this charge that we have somehow “cheated,”
we oﬀer the following remarks.
First, in practice the primes p are so small that they surely will
ﬁt into a single digit in the underlying representation of integers as
vectors of digits, and so estimating the cost as O(len(n)) rather than
O(len(n) len(p)) seems more realistic.
Second, even if one uses the bound O(len(n) len(p)), one can carry
out a similar analysis, obtaining the same result (namely, a speedup
by a factor proportional to len(k)) except that one should choose s
from a slightly smaller range (namely, s = o(k2)).
As we already mentioned, the above analysis is heuristic, but the results
are correct. We shall now discuss how this analysis can be made rigorous;
however, we should remark that any such rigorous analysis is mainly of the-
oretical interest only—in any practical implementation, the optimal choice
of the parameter s is best determined by experiment, with the analysis being
used only as a rough guide. Now, to make the analysis rigorous, we need
prove that the estimate τ(M, s) ≈
p≤s(1 −1/p) is suﬃciently accurate.
Proving such estimates takes us into the realm of “sieve theory.” The larger
M is with respect to s, the easier it is to prove such estimates. We shall
prove only the simplest and most naive such estimate, but it is still good
enough for our purposes, if we do not care too much about hidden big-O
constants.
Before stating any results, let us restate the problem slightly. For real
y ≥0, let us call a positive integer “y-rough” if it is not divisible by any
prime p up to y. For real x ≥0, let us deﬁne R(x, y) to be the number
of y-rough integers up to x. Thus, since τ(M, s) is the probability that a
random integer between 2 and M is s-rough, and 1 is by deﬁnition s-rough,
we have τ(M, s) = (R(M, s) −1)/(M −1).
Theorem 10.8. For any real x ≥0 and y ≥0, we have
R(x, y) −x

p≤y
(1 −1/p)
 ≤2π(y).
Proof. To simplify the notation, we shall use the M¨obius function µ (see
TEAM LinG

10.4 Generating random primes using the Miller–Rabin test
259
§2.6). Also, for a real number u, let us write u = ⌊u⌋+ {u}, where 0 ≤
{u} < 1. Let P be the product of the primes up to the bound y.
Now, there are ⌊x⌋positive integers up to x, and of these, for each prime
p dividing P, precisely ⌊x/p⌋are divisible by p, for each pair p, p′ of distinct
primes dividing P, precisely ⌊x/pp′⌋are divisible by pp′, and so on. By
inclusion/exclusion (see Exercise 6.3), we have
R(x, y) =

d|P
µ(d)⌊x/d⌋=

d|P
µ(d)(x/d) −

d|P
µ(d){x/d}.
Moreover,

d|P
µ(d)(x/d) = x

d|P
µ(d)/d = x

p≤y
(1 −1/p),
and


d|P
µ(d){x/d}
 ≤

d|P
1 = 2π(y).
That proves the theorem. 2
This theorem only says something non-trivial when y is quite small. Nev-
ertheless, using Chebyshev’s theorem on the density of primes, along with
Mertens’ theorem, it is not hard to see that this theorem implies that
τ(M, s) = O(1/ log s) when s = O(log M log log M), which implies the esti-
mate (10.4) above. We leave the details as an exercise for the reader.
Exercise 10.10. Prove the claim made above that τ(M, s) = O(1/ log s)
when s = O(log M log log M). More precisely, show that there exist con-
stants c, d, and s0, such that for all M and d satisfying s0 ≤s ≤
c log M log log M, we have τ(M, s) ≤d/ log s. From this, derive the esti-
mate (10.4) above.
Exercise 10.11. Let f be a polynomial with integer coeﬃcients. For real
x ≥0 and y ≥0, deﬁne Rf(x, y) to be the number of integers m up to x
such that f(m) is y-rough. For positive integer M, deﬁne ωf(M) to be the
number of integers m ∈{0, . . . , M −1} such that f(m) ≡0 (mod M). Show
that
Rf(x, y) −x

p≤y
(1 −ωf(p)/p)
 ≤

p≤y
(1 + ωf(p)).
Exercise 10.12. Consider again the problem of generating a random Sophie
Germain prime, as discussed in Exercises 10.7 and 10.8. A useful idea is to
TEAM LinG

260
Probabilistic primality testing
ﬁrst test if either n or 2n + 1 are divisible by any small primes up to some
bound s, before performing any more expensive tests. Using this idea, design
and analyze an algorithm that improves the running time of the algorithm
in Exercise 10.8 to O(k5/ len(k)2 + tk3)—under the same assumptions, and
achieving the same error probability bound as in that exercise. Hint: ﬁrst
show that the previous exercise implies that the number of positive integers
m up to x such that both m and 2m + 1 are y-rough is at most
x · 1
2

2<p≤y
(1 −2/p) + 3π(y).
Exercise 10.13. Design an algorithm that takes as input a prime q and
a bound M, and outputs a random prime p between 2 and M such that
p ≡1 (mod q). Clearly, we need to assume that M is suﬃciently large
with respect to q. Analyze your algorithm assuming Conjecture 5.24 (and
using the result of Exercise 5.22). State how large M must be with respect
to q, and under these assumptions, show that your algorithm runs in time
O(k4/ len(k)+tk3), and that its output is incorrect with probability O(4−tk).
As usual, k := len(M).
10.4.3 Generating a random k-bit prime
In some applications, we want to generate a random prime of ﬁxed size—
a random 1024-bit prime, for example. More generally, let us consider the
following problem: given integer k ≥2, generate a random k-bit prime, that
is, a prime in the interval [2k−1, 2k).
Bertrand’s postulate (Theorem 5.7) implies that there exists a constant
c > 0 such that π(2k) −π(2k−1) ≥c2k−1/k for all k ≥2.
Now let us modify Algorithm RP so that it takes as input integer k ≥2,
and repeatedly generates a random n in the interval {2k−1, . . . , 2k −1} until
IsPrime(n) returns true. Let us call this variant Algorithm RP′. Further,
let us implement IsPrime(·) as MR(·, t), for some auxiliary parameter t, and
deﬁne γ′(k, t) to be the probability that the output of Algorithm RP′—with
this implementation of IsPrime —is composite.
Then using exactly the same reasoning as above,
γ′(k, t) ≤4−t
2k−1
π(2k) −π(2k−1) = O(4−tk).
As before, if the output of Algorithm RP′ is prime, then every k-bit prime
is equally likely, and the expected running time is O(k4 + tk3). By doing
some trial division as above, this can be reduced to O(k4/ len(k) + tk3).
TEAM LinG

10.5 Perfect power testing and prime power factoring
261
The function γ′(k, t) has been studied a good deal; for example, the fol-
lowing is known:
Theorem 10.9. For all k ≥2, we have
γ′(k, 1) ≤k242−
√
k.
Proof. Literature—see §10.7. 2
Upper bounds for γ′(k, t) for speciﬁc values of k and t have been computed.
The following table lists some known lower bounds for −log2(γ′(k, t)) for
various values of k and t:
t\k
200
300
400
500
600
1
11
19
37
56
75
2
25
33
46
63
82
3
34
44
55
70
88
4
41
53
63
78
95
5
47
60
72
85
102
Using exactly the same reasoning as the derivation of (10.3), one sees that
γ′(k, t) ≤
γ′(k, 1)
1 −γ′(k, 1)4−t+1.
10.5 Perfect power testing and prime power factoring
Consider the following problem: we are given a integer n > 1, and want to
determine if n is a perfect power, which means that n = de for integers d
and e, both greater than 1. Certainly, if such d and e exist, then it must be
the case that 2e ≤n, so we can try all possible candidate values of e, running
from 2 to ⌊log2 n⌋. For each such candidate value of e, we can test if n = de
for some d as follows. Suppose n is a k-bit number, that is, 2k−1 ≤n < 2k.
Then 2(k−1)/e ≤n1/e < 2k/e. So any integer eth root of n must lie in the
set {u, . . . , v −1}, where u := 2⌊(k−1)/e⌋and v := 2⌈k/e⌉. Using u and v as
starting values, we can perform a binary search:
TEAM LinG

262
Probabilistic primality testing
repeat
w ←⌊(u + v)/2⌋
z ←we
if z = n then
declare than n = we is an a perfect eth power, and stop
else if z < n then
u ←w + 1
else
v ←w
until u ≥v
declare that n is not a perfect eth power
If n = de for some integer d, then the following invariant holds (verify):
at the beginning of each loop iteration, we have u ≤d < v. Thus, if n is
a perfect eth power, this will be discovered. That proves the correctness of
the algorithm.
As to its running time, note that with each loop iteration, the length v−u
of the search interval decreases by a factor of at least 2 (verify). Therefore,
after t iterations the interval will be of length at most 2k/e+1/2t, so after
at most k/e + 2 iterations, the interval will be of length less than 1, and
hence of length zero, and the algorithm will halt. So the number of loop
iterations is O(k/e). The power we computed in each iteration is no more
than 2(k/e+1)e = 2k+e ≤22k, and hence can be computed in time O(k2) (see
Exercise 3.22). Hence the overall cost of testing if n is an eth power using
this algorithm is O(k3/e).
Trying all candidate values of e from 1 to ⌊log2 n⌋yields an overall running
time for perfect power testing of O(
e k3/e), which is O(k3 len(k)). To ﬁnd
the largest possible value of e for which n is an eth power, we should examine
the candidates from highest to lowest.
Using the above algorithm for perfect power testing and an eﬃcient pri-
mality test, we can determine if an integer n is a prime power pe, and if so,
compute p and e: we ﬁnd the largest positive integer e (possibly 1) such
that n = de for integer d, and test if d is a prime using an eﬃcient primality
test.
10.6 Factoring and computing Euler’s phi function
In this section, we use some of the ideas developed to analyze the Miller–
Rabin test to prove that the problem of factoring n and the problem of
computing φ(n) are equivalent. By equivalent, we mean that given an eﬃ-
TEAM LinG

10.6 Factoring and computing Euler’s phi function
263
cient algorithm to solve one problem, we can eﬃciently solve the other, and
vice versa.
Clearly, one direction is easy: if we can factor n into primes, so
n = pe1
1 · · · per
r ,
(10.5)
then we can simply compute φ(n) using the formula
φ(n) = pe1−1
1
(p1 −1) · · · per−1
r
(pr −1).
For the other direction, ﬁrst consider the special case where n = pq, for
distinct primes p and q. Suppose we are given n and φ(n), so that we have
two equations in the unknowns p and q:
n = pq and φ(n) = (p −1)(q −1).
Substituting n/p for q in the second equation, and simplifying, we obtain
p2 + (φ(n) −n −1)p + n,
which can be solved using the quadratic formula.
For the general case, it is just as easy to prove a stronger result: given
any non-zero multiple of the exponent of Z∗
n, we can eﬃciently factor n. In
particular, this will show that we can eﬃciently factor Carmichael numbers.
Before stating the algorithm in its full generality, we can convey the main
idea by considering the special case where n = pq, where p and q are distinct
primes, with p ≡q ≡3 (mod 4). Suppose we are given such an n, along
with f ̸= 0 that is a common multiple of p −1 and q −1. The algorithm
works as follows: let f = 2hm, where m is odd; choose a random, non-zero
element α of Zn; test if either gcd(rep(α), n) or gcd(rep(αm) + 1, n) splits n
(recall that rep(α) denotes the canonical representative of α).
The assumption that p ≡3 (mod 4) means that (p−1)/2 is an odd integer,
and since f is a multiple of p −1, it follows that gcd(m, p −1) = (p −1)/2,
and hence the image of Z∗
p under the m-power map is the subgroup of Z∗
p of
order 2, which is {±1}. Likewise, the image of Z∗
q under the m-power map
is {±1}. Let θ : Zp × Zq →Zn be the ring isomorphism from the Chinese
remainder theorem. Now, if α in the above algorithm does not lie in Z∗
n,
then certainly gcd(rep(α), n) splits n. Otherwise, condition on the event
that α ∈Z∗
n. In this conditional probability distribution, α is uniformly
distributed over Z∗
n, and β := αm is uniformly distributed over θ(±1, ±1).
Let us consider each of these four possibilities:
• β = θ(1, 1) implies β + 1 = θ(2, 2), and so gcd(rep(β) + 1, n) = 1;
• β = θ(−1, −1) implies β + 1 = θ(0, 0), and so gcd(rep(β) + 1, n) = n;
TEAM LinG

264
Probabilistic primality testing
• β = θ(−1, 1) implies β + 1 = θ(0, 2), and so gcd(rep(β) + 1, n) = p;
• β = θ(1, −1) implies β + 1 = θ(2, 0), and so gcd(rep(β) + 1, n) = q.
Thus, if β = θ(−1, 1) or β = θ(1, −1), which happens with probability 1/2,
then gcd(rep(β) + 1, n) splits n. Therefore, the overall probability that we
split n is at least 1/2.
We now present the algorithm in its full generality. We ﬁrst introduce
some notation; namely, let λ(n) denote the exponent of Z∗
n. If the prime
factorization of n is as in (10.5), then by the Chinese remainder theorem,
we have
λ(n) = lcm(λ(pe1
1 ), . . . , λ(per
r )).
Moreover, for any prime power pe, by Theorem 10.1, we have
λ(pe) =
 pe−1(p −1)
if p ̸= 2 or e ≤2,
2e−2
if p = 2 and e ≥3.
In particular, if m | n, then λ(m) | λ(n).
Now, returning to our factorization problem, we are given n and a non-
zero multiple f of λ(n), and want to factor n. We may as well assume that
n is odd; otherwise, we can pull out all the factors of 2, obtaining n′ such
that n = 2en′, where n′ is odd and f is a multiple of λ(n′), thus, reducing
to the odd case.
So now, assume n is odd and f is a multiple of λ(n). Assume that f is
of the form f = 2hm, where m is odd. Our factoring algorithm, which we
describe recursively, runs as follows.
if n is a prime power pe then
output e copies of p and return
generate a random, non-zero element α of Zn
d1 ←gcd(rep(α), n)
if d1 ̸= 1, then recursively factor d1 and n/d1 (using the same f),
and return
α ←αm
for j ←0 to h −1 do
d2 ←gcd(rep(α) + 1, n)
if d2 /∈{1, n}, then recursively factor d2 and n/d2
(using the same f), and return
α ←α2
recursively factor n (using the same f)
It is clear that when the algorithm terminates, its output consists of the
TEAM LinG

10.6 Factoring and computing Euler’s phi function
265
list of all primes (including duplicates) dividing n, assuming the primality
test does not make a mistake.
To analyze the running time of the algorithm, assume that the prime
factorization of n is as in (10.5). By the Chinese remainder theorem, we
have a ring isomorphism
θ : Zpe1
1 × · · · × Zper
r →Zn.
Let λ(pei
i ) = mi2hi, where mi is odd, for i = 1, . . . , r, and let ℓ:=
max{h1, . . . , hr}. Note that since λ(n) | f, we have ℓ≤h.
Consider one execution of the body of the recursive algorithm. If n is
a prime power, this will be detected immediately, and the algorithm will
return. Here, even if we are using probabilistic primality test, such as the
Miller–Rabin test, that always says that a prime is a prime, the algorithm
will certainly halt. So assume that n is not a prime power, which means
that r ≥2. If the chosen value of α is not in Z∗
n, then d1 will be a non-
trivial divisor of n. Otherwise, conditioning on the event that α ∈Z∗
n, the
distribution of α is uniform over Z∗
n. Consider the value β := αm2ℓ−1.
We claim that with probability at least 1/2, gcd(rep(β) + 1, n) is a non-
trivial divisor of n. To prove this claim, let us write
β = θ(β1, . . . , βr),
where βi ∈Z∗
pei
i for i = 1, . . . , r. Note that for those i with hi < ℓ, the m2ℓ−1-
power map kills the group Z∗
pei
i , while for those i with hi = ℓ, the image of
Z∗
pei
i under the m2ℓ−1-power map is {±1}. Without loss of generality, assume
that the indices i such that hi = ℓare numbered 1, . . . , r′, where 1 ≤r′ ≤r.
The values βi for i = 1, . . . , r′ are uniformly and independently distributed
over {±1}, while for all i > r′, βi = 1. Thus, the value of gcd(rep(β) + 1, n)
is the product of all prime powers pei
i , with βi = −1, which will be non-
trivial unless either (1) all the βi are 1, or (2) r′ = r and all the βi are −1.
Consider two cases. First, if r′ < r, then only event (1) is possible, and this
occurs with probability 2−r′ ≤1/2. Second, if r′ = r, then each of events
(1) and (2) occurs with probability 2−r, and so the probability that either
occurs is 2−r+1 ≤1/2. That proves the claim.
From the claim, it follows that with probability at least 1/2, we will obtain
a non-trivial divisor d2 of n when j = ℓ−1 (if not before).
So we have shown that with probability at least 1/2, one execution of the
body will succeed in splitting n into non-trivial factors. After at most log2 n
such successes, we will have completely factored n. Therefore, the expected
number of recursive invocations of the algorithm is O(len(n)).
TEAM LinG

266
Probabilistic primality testing
Exercise 10.14. Suppose you are given an integer n of the form n = pq,
where p and q are distinct, ℓ-bit primes, with p = 2p′ + 1 and q = 2q′ + 1,
where p′ and q′ are themselves prime. Suppose that you are also given an
integer m such that gcd(m, p′q′) ̸= 1. Show how to eﬃciently factor n.
Exercise 10.15. Suppose there is a probabilistic algorithm A that takes
as input an integer n of the form n = pq, where p and q are distinct, ℓ-bit
primes, with p = 2p′ + 1 and q = 2q′ + 1, where p′ and q′ are prime. The
algorithm also takes as input α, β ∈(Z∗
n)2. It outputs either “failure,” or
integers x, y, not both zero, such that αxβy = 1. Furthermore, assume that
A runs in strict polynomial time, and that for all n of the above form, and
for randomly chosen α, β ∈(Z∗
n)2, A succeeds in ﬁnding x, y as above with
probability ϵ(n). Here, the probability is taken over the random choice of α
and β, as well as the random choices made during the execution of A. Show
how to use A to construct another probabilistic algorithm A′ that takes as
input n as above, runs in expected polynomial time, and that satisﬁes the
following property:
if ϵ(n) ≥0.001, then A′ factors n with probability at least
0.999.
10.7 Notes
The Miller–Rabin test is due to Miller [63] and Rabin [75]. The paper by
Miller deﬁned the set L′
n, but did not give a probabilistic analysis. Rather,
Miller showed that under a generalization of the Riemann hypothesis, for
composite n, the least positive integer a such that [a]n ∈Zn \ L′
n is at
most O((log n)2), thus giving rise to a deterministic primality test whose
correctness depends on the above unproved hypothesis. The later paper by
Rabin re-interprets Miller’s result in the context of probabilistic algorithms.
Bach [10] gives an explicit version of Miller’s result, showing that under
the same assumptions, the least positive integer a such that [a]n ∈Zn \ L′
n
is at most 2(log n)2; more generally, Bach shows the following holds under
a generalization of the Riemann hypothesis:
For any positive integer n, and any proper subgroup G ⊊Z∗
n,
the least positive integer a such that [a]n ∈Zn \ G is at most
2(log n)2, and the least positive integer b such that [b]n ∈Z∗
n\G
is at most 3(log n)2.
The ﬁrst eﬃcient probabilistic primality test was invented by Solovay and
Strassen [94] (their paper was actually submitted for publication in 1974).
TEAM LinG

10.7 Notes
267
Later, in Chapter 22, we shall discuss a recently discovered, deterministic,
polynomial-time (though not very practical) primality test, whose analysis
does not rely on any unproved hypothesis.
Carmichael numbers are named after R. D. Carmichael, who was the
ﬁrst to discuss them, in work published in the early 20th century.
Al-
ford, Granville, and Pomerance [7] proved that there are inﬁnitely many
Carmichael numbers.
Exercise 10.6 is based on Lehmann [55].
Theorem 10.7, as well as the table of values just below it, are from Kim
and Pomerance [53]. In fact, these bounds hold for the weaker test based
on Ln.
Our analysis in §10.4.2 is loosely based on a similar analysis in §4.1 of
Maurer [61].
Theorem 10.8 and its generalization in Exercise 10.11 are
certainly not the best results possible in this area.
The general goal of
“sieve theory” is to prove useful upper and lower bounds for quantities like
Rf(x, y) that hold when y is as large as possible with respect to x. For
example, using a technique known as Brun’s pure sieve, one can show that
for log y < √log x, there exist β and β′, both of absolute value at most 1,
such that
Rf(x, y) = (1 + βe−√log x)x

p≤y
(1 −ωf(p)/p) + β′√x.
Thus, this gives us very sharp estimates for Rf(x, y) when x tends to inﬁnity,
and y is bounded by any ﬁxed polynomial in log x.
For a proof of this
result, see §2.2 of Halberstam and Richert [42] (the result itself is stated
as equation 2.16). Brun’s pure sieve is really just the ﬁrst non-trivial sieve
result, developed in the early 20th century; even stronger results, extending
the useful range of y (but with larger error terms), have subsequently been
proved.
Theorem 10.9, as well as the table of values immediately below it, are
from Damg˚ard, Landrock, and Pomerance [32].
The algorithm presented in §10.6 for factoring an integer given a multiple
of φ(n) (or, for that matter, λ(n)) is essentially due to Miller [63]. However,
just as for his primality test, Miller presents his algorithm as a deterministic
algorithm, which he analyzes under a generalization of the Riemann hypoth-
esis. The probabilistic version of Miller’s factoring algorithm appears to be
“folklore.”
TEAM LinG

11
Finding generators and discrete logarithms in Z∗
p
As we have seen in Theorem 9.16, for a prime p, Z∗
p is a cyclic group of order
p −1. This means that there exists a generator γ ∈Z∗
p, such that for all
α ∈Z∗
p, α can be written uniquely as α = γx, where x is an integer with
0 ≤x < p −1; the integer x is called the discrete logarithm of α to the
base γ, and is denoted logγ α.
This chapter discusses some computational problems in this setting;
namely, how to eﬃciently ﬁnd a generator γ, and given γ and α, how to
compute logγ α.
More generally, if γ generates a subgroup G of Z∗
p of order q, where q |
(p −1), and α ∈G, then logγ α is deﬁned to be the unique integer x with
0 ≤x < q and α = γx. In some situations it is more convenient to view
logγ α as an element of Zq. Also for x ∈Zq, with x = [a]q, one may write γx
to denote γa. There can be no confusion, since if x = [a′]q, then γa′ = γa.
However, in this chapter, we shall view logγ α as an integer.
Although we work in the group Z∗
p, all of the algorithms discussed in this
chapter trivially generalize to any ﬁnite cyclic group that has a suitably
compact representation of group elements and an eﬃcient algorithm for
performing the group operation on these representations.
11.1 Finding a generator for Z∗
p
There is no eﬃcient algorithm known for this problem, unless the prime
factorization of p −1 is given, and even then, we must resort to the use of
a probabilistic algorithm. Of course, factoring in general is believed to be a
very diﬃcult problem, so it may not be easy to get the prime factorization
of p −1. However, if our goal is to construct a large prime p, together with
a generator for Z∗
p, then we may use Algorithm RFN in §7.7 to generate a
random factored number n in some range, test n + 1 for primality, and then
268
TEAM LinG

11.1 Finding a generator for Z∗
p
269
repeat until we get a factored number n such that p = n + 1 is prime. In
this way, we can generate a random prime p in a given range along with the
factorization of p −1.
We now present an eﬃcient probabilistic algorithm that takes as input an
odd prime p, along with the prime factorization
p −1 =
r

i=1
qei
i ,
and outputs a generator for Z∗
p. It runs as follows:
for i ←1 to r do
repeat
choose α ∈Z∗
p at random
compute β ←α(p−1)/qi
until β ̸= 1
γi ←α(p−1)/qei
i
γ ←r
i=1 γi
output γ
First, let us analyze the correctness of this algorithm. When the ith loop
iteration terminates, by construction, we have
γ
qei
i
i
= 1 but γ
qei−1
i
i
̸= 1.
It follows (see Theorem 8.37) that γi has multiplicative order qei
i . From this,
it follows (see Theorem 8.38) that γ has multiplicative order p −1.
Thus, we have shown that if the algorithm terminates, its output is always
correct.
Let us now analyze the running time of this algorithm.
Consider the
repeat/until loop in the ith iteration of the outer loop, for i = 1, . . . , r, and
let Xi be the random variable whose value is the number of iterations of
this repeat/until loop. Since α is chosen at random from Z∗
p, the value of
β is uniformly distributed over the image of the (p −1)/qi-power map (see
Exercise 8.22), and since the latter is a subgroup of Z∗
p of order qi, we see
that β = 1 with probability 1/qi. Thus, Xi has a geometric distribution with
associated success probability 1−1/qi, and therefore, E[Xi] = 1/(1−1/qi) ≤
2. Set X := X1 + · · · + Xr. Note that E[X] = E[X1] + · · · + E[Xr] ≤2r.
The running time T of the entire algorithm is O(X · len(p)3), and hence
the expected running is E[T] = O(r len(p)3), and since r ≤log2 p, we have
E[T] = O(len(p)4).
TEAM LinG

270
Finding generators and discrete logarithms in Z∗
p
Although this algorithm is quite practical, there are asymptotically faster
algorithms for this problem (see Exercise 11.2).
Exercise 11.1. Suppose we are not given the prime factorization of p −1,
but rather, just a prime q dividing p −1, and we want to ﬁnd an element
of multiplicative order q in Z∗
p. Design and analyze an eﬃcient algorithm to
do this.
Exercise 11.2. Suppose we are given a prime p, along with the prime
factorization p −1 = r
i=1 qei
i .
(a) If, in addition, we are given α ∈Z∗
p, show how to compute the mul-
tiplicative order of α in time O(r len(p)3). Hint: use Exercise 8.25.
(b) Improve the running time bound to O(len(r) len(p)3). Hint: use Ex-
ercise 3.30.
(c) Modifying the algorithm you developed for part (b), show how to
construct a generator for Z∗
p in expected time O(len(r) len(p)3).
Exercise 11.3. Suppose we are given a positive integer n, along with its
prime factorization n = pe1
1 · · · per
r , and that for each i = 1, . . . , r, we are also
given the prime factorization of pi −1. Show how to eﬃciently compute the
multiplicative order of any element α ∈Z∗
n.
Exercise 11.4. Suppose there is an eﬃcient algorithm that takes as input a
positive integer n and an element α ∈Z∗
n, and computes the multiplicative
order of α. Show how to use this algorithm to be build an eﬃcient integer
factoring algorithm.
11.2 Computing discrete logarithms Z∗
p
In this section, we consider algorithms for computing the discrete logarithm
of α ∈Z∗
p to a given base γ. The algorithms we present here are, in the worst
case, exponential-time algorithms, and are by no means the best possible;
however, in some special cases, these algorithms are not so bad.
11.2.1 Brute-force search
Suppose that γ ∈Z∗
p generates a subgroup G of Z∗
p of order q > 1 (not
necessarily prime), and we are given p, q, γ, and α ∈G, and wish to compute
logγ α.
The simplest algorithm to solve the problem is brute-force search:
TEAM LinG

11.2 Computing discrete logarithms Z∗
p
271
β ←1
i ←0
while β ̸= α do
β ←β · γ
i ←i + 1
output i
This algorithm is clearly correct, and the main loop will always halt after
at most q iterations (assuming, as we are, that α ∈G). So the total running
time is O(q len(p)2).
11.2.2 Baby step/giant step method
As above, suppose that γ ∈Z∗
p generates a subgroup G of Z∗
p of order q > 1
(not necessarily prime), and we are given p, q, γ, and α ∈G, and wish to
compute logγ α.
A faster algorithm than brute-force search is the baby step/giant step
method. It works as follows.
Let us choose an approximation m to q1/2. It does not have to be a very
good approximation—we just need m = Θ(q1/2). Also, let m′ = ⌊q/m⌋, so
that m′ = Θ(q1/2) as well.
The idea is to compute all the values γi for i = 0, . . . , m −1 (the “baby
steps”) and to build a “lookup table” L that contains all the pairs (γi, i),
and that supports fast lookups on the ﬁrst component of these pairs. That
is, given β ∈Z∗
p, we should be able to quickly determine if β = γi for some
i = 0, . . . , m −1, and if so, determine the value of i. Let us deﬁne L(β) := i
if β = γi for some i = 0, . . . , m −1; otherwise, deﬁne L(β) := −1.
Using an appropriate data structure, we can build the table L in time
O(q1/2 len(p)2) (just compute successive powers of γ, and insert them in
the table), and we can perform a lookup in time O(len(p)). One such data
structure is a radix tree (also called a search trie); other data structures may
be used (for example, a hash table or a binary search tree), but these may
yield slightly diﬀerent running times for building the table and/or for table
lookup.
After building the lookup table, we execute the following procedure (the
“giant steps”):
TEAM LinG

272
Finding generators and discrete logarithms in Z∗
p
γ′ ←γ−m
β ←α,
j ←0,
i ←L(β)
while i = −1 do
β ←β · γ′,
j ←j + 1,
i ←L(β)
x ←jm + i
output x
To analyze this procedure, suppose that α = γx with 0 ≤x < q. Now, x
can be written in a unique way as x = vm + u, where u and v are integers
with 0 ≤u < m and 0 ≤v ≤m′. In the jth loop iteration, for j = 0, 1, . . . ,
we have
β = αγ−mj = γ(v−j)m+u.
So we will detect i ̸= −1 precisely when j = v, in which case i = u. Thus, the
output will be correct, and the total running time of the algorithm (for both
the “baby steps” and “giant steps” parts) is easily seen to be O(q1/2 len(p)2).
While this algorithm is much faster than brute-force search, it has the
drawback that it requires a table Θ(q1/2) elements of Zp. Of course, there
is a “time/space trade-oﬀ” here: by choosing m smaller, we get a table of
size O(m), but the running time will be proportional to O(q/m). In §11.2.5
below, we discuss an algorithm that runs (at least heuristically) in time
O(q1/2 len(q) len(p)2), but which requires space for only a constant number
of elements of Zp.
11.2.3 Groups of order qe
Suppose that γ ∈Z∗
p generates a subgroup G of Z∗
p of order qe, where q > 1
and e ≥1, and we are given p, q, e, γ, and α ∈G, and wish to compute
logγ α.
There is a simple algorithm that allows one to reduce this problem to the
problem of computing discrete logarithms in the subgroup of Z∗
p of order q.
It is perhaps easiest to describe the algorithm recursively. The base case
is when e = 1, in which case, we use an algorithm for the subgroup of Z∗
p of
order q. For this, we might employ the algorithm in §11.2.2, or if q is very
small, the algorithm in §11.2.1.
Suppose now that e > 1. We choose an integer f with 0 < f < e. Diﬀerent
strategies for choosing f yield diﬀerent algorithms—we discuss this below.
Suppose α = γx, where 0 ≤x < qe. Then we can write x = qfv + u, where
TEAM LinG

11.2 Computing discrete logarithms Z∗
p
273
u and v are integers with 0 ≤u < qf and 0 ≤v < qe−f. Therefore,
αqe−f = γqe−fu.
Note that γqe−f has multiplicative order qf, and so if we recursively compute
the discrete logarithm of αqe−f to the base γqe−f , we obtain u.
Having obtained u, observe that
α/γu = γqfv.
Note also that γqf has multiplicative order qe−f, and so if we recursively
compute the discrete logarithm of α/γu to the base γqf , we obtain v, from
which we then compute x = qfv + u.
Let us put together the above ideas succinctly in a recursive procedure
RDL(p, q, e, γ, α) that runs as follows:
if e = 1 then
return logγ α
// base case: use a diﬀerent algorithm
else
select f ∈{1, . . . , e −1}
u ←RDL(p, q, f, γqe−f , αqe−f )
// 0 ≤u < qf
v ←RDL(p, q, e −f, γqf , α/γu)
// 0 ≤v < qe−f
return qfv + u
To analyze the running time of this recursive algorithm, note that the run-
ning time of the body of one recursive invocation (not counting the running
time of the recursive calls it makes) is O(e len(q) len(p)2). To calculate the
total running time, we have to sum up the running times of all the recursive
calls plus the running times of all the base cases.
Regardless of the strategy for choosing f, the total number of base case
invocations is e. Note that all the base cases compute discrete logarithms
to the base γqe−1. Assuming we implement the base case using the baby
step/giant step algorithm in §11.2.2, the total running time for all the base
cases is therefore O(eq1/2 len(p)2).
The total running time for the recursion (not including the base case
computations) depends on the strategy used to choose the split f.
• If we always choose f = 1 or f = e −1, then the total running time
for the recursion is O(e2 len(q) len(p)2). Note that if f = 1, then the
algorithm is essentially tail recursive, and so may be easily converted
to an iterative algorithm without the need for a stack.
• If we use a “balanced” divide-and-conquer strategy,
choosing
f
≈
e/2,
then the total running time of the recursion is
TEAM LinG

274
Finding generators and discrete logarithms in Z∗
p
O(e len(e) len(q) len(p)2).
To see this, note that the depth of the
“recursion tree” is O(len(e)), while the running time per level of the
recursion tree is O(e len(q) len(p)2).
Assuming we use the faster, balanced recursion strategy, the total running
time, including both the recursion and base cases, is:
O((eq1/2 + e len(e) len(q)) · len(p)2).
11.2.4 Discrete logarithms in Z∗
p
Suppose that we are given a prime p, along with the prime factorization
p −1 =
r

i=1
qei
i ,
a generator γ for Z∗
p, and α ∈Z∗
p. We wish to compute logγ α.
Suppose that α = γx, where 0 ≤x < p−1. Then for i = 1, . . . , r, we have
α(p−1)/qei
i = γ(p−1)/qei
i x.
Note that γ(p−1)/qei
i
has multiplicative order qei
i , and if xi is the discrete
logarithm of α(p−1)/qei
i to the base γ(p−1)/qei
i , then we have 0 ≤xi < qei
i and
x ≡xi (mod qei
i ).
Thus, if we compute the values x1, . . . , xr, using the algorithm in §11.2.3,
we can obtain x using the algorithm of the Chinese remainder theorem (see
Theorem 4.5). If we deﬁne q := max{q1, . . . , qr}, then the running time of
this algorithm will be bounded by q1/2 len(p)O(1).
We conclude that
the diﬃculty of computing discrete logarithms in Z∗
p is deter-
mined by the size of the largest prime dividing p −1.
11.2.5 A space-eﬃcient square-root time algorithm
We present a more space-eﬃcient alternative to the algorithm in §11.2.2, the
analysis of which we leave as a series of exercises for the reader.
The algorithm makes a somewhat heuristic assumption that we have a
function that “behaves” for all practical purposes like a random function.
Such functions can indeed be constructed using cryptographic techniques
under reasonable intractability assumptions; however, for the particular ap-
plication here, one can get by in practice with much simpler constructions.
Let p be a prime, q a prime dividing p −1, γ an element of Z∗
p that
generates a subgroup G of Z∗
p of order q, and α ∈G. Let F be a function
TEAM LinG

11.3 The Diﬃe–Hellman key establishment protocol
275
mapping elements of G to {0, . . . , q −1}. Deﬁne H : G →G to be the
function that sends β to βαγF(β).
The algorithm runs as follows:
i ←1
x ←0, β ←α,
x′ ←F(β), β′ ←H(β)
while β ̸= β′ do
x ←(x + F(β)) mod q, β ←H(β)
x′ ←(x′ + F(β′)) mod q, β′ ←H(β′)
x′ ←(x′ + F(β′)) mod q, β′ ←H(β′)
i ←i + 1
if i < q then
output (x −x′)i−1 mod q
else
output “fail”
To analyze this algorithm, let us deﬁne β1, β2, . . . , as follows: β1 := α and
for i > 1, βi := H(βi−1).
Exercise 11.5. Show that each time the main loop of the algorithm is
entered, we have β = βi = γxαi, and β′ = β2i = γx′α2i.
Exercise 11.6. Show that if the loop terminates with i < q, the value
output is equal to logγ α.
Exercise 11.7. Let j be the smallest index such that βj = βk for some
index k < j. Show that j ≤q + 1 and that the loop terminates with i < j
(and in particular, i ≤q).
Exercise 11.8. Assume that F is a random function, meaning that it is cho-
sen at random, uniformly from among all functions from G into {0, . . . , q−1}.
Show that this implies that H is a random function, meaning that it is uni-
formly distributed over all functions from G into G.
Exercise 11.9. Assuming that F is a random function as in the previous
exercise, apply the result of Exercise 6.27 to conclude that the expected run-
ning time of the algorithm is O(q1/2 len(q) len(p)2), and that the probability
that the algorithm fails is exponentially small in q.
11.3 The Diﬃe–Hellman key establishment protocol
One of the main motivations for studying algorithms for computing discrete
logarithms is the relation between this problem and the problem of break-
TEAM LinG

276
Finding generators and discrete logarithms in Z∗
p
ing a protocol called the Diﬃe–Hellman key establishment protocol,
named after its inventors.
In this protocol, Alice and Bob need never to have talked to each other
before, but nevertheless, can establish a shared secret key that nobody else
can easily compute.
To use this protocol, a third party must provide a
“telephone book,” which contains the following information:
• p, q, and γ, where p and q are primes with q | (p −1), and γ is an
element generating a subgroup G of Z∗
p of order q;
• an entry for each user, such as Alice or Bob, that contains the user’s
name, along with a “public key” for that user, which is an element
of the group G.
To use this system, Alice posts her public key in the telephone book,
which is of the form α = γx, where x ∈{0, . . . , q −1} is chosen by Alice at
random. The value of x is Alice’s “secret key,” which Alice never divulges
to anybody. Likewise, Bob posts his public key, which is of the form β = γy,
where y ∈{0, . . . , q −1} is chosen by Bob at random, and is his secret key.
To establish a shared key known only between them, Alice retrieves Bob’s
public key β from the telephone book, and computes κA := βx. Likewise,
Bob retrieves Alice’s public key α, and computes κB := αy. It is easy to see
that
κA = βx = (γy)x = γxy = (γx)y = αy = κB,
and hence Alice and Bob share the same secret key κ := κA = κB.
Using this shared secret key, they can then use standard methods for
encryption and message authentication to hold a secure conversation. We
shall not go any further into how this is done; rather, we brieﬂy (and only
superﬁcially) discuss some aspects of the security of the key establishment
protocol itself. Clearly, if an attacker obtains α and β from the telephone
book, and computes x = logγ α, then he can compute Alice and Bob’s shared
key as κ = βx —in fact, given x, an attacker can eﬃciently compute any
key shared between Alice and another user.
Thus, if this system is to be secure, it should be very diﬃcult to com-
pute discrete logarithms. However, the assumption that computing discrete
logarithms is hard is not enough to guarantee security. Indeed, it is not
entirely inconceivable that the discrete logarithm problem is hard, and yet
the problem of computing κ from α and β is easy. The latter problem—
computing κ from α and β —is called the Diﬃe–Hellman problem.
As in the discussion of the RSA cryptosystem in §7.8, the reader is warned
that the above discussion about security is a bit of an oversimpliﬁcation. A
TEAM LinG

11.3 The Diﬃe–Hellman key establishment protocol
277
complete discussion of all the security issues related to the above protocol
is beyond the scope of this text.
Note that in our presentation of the Diﬃe–Hellman protocol, we work with
a generator of a subgroup G of Z∗
p of prime order, rather than a generator
for Z∗
p. There are several reasons for doing this: one is that there are no
known discrete logarithm algorithms that are any more practical in G than
in Z∗
p, provided the order q of G is suﬃciently large; another is that by
working in G, the protocol becomes substantially more eﬃcient. In typical
implementations, p is 1024 bits long, so as to protect against subexponential-
time algorithms such as those discussed later in §16.2, while q is 160 bits long,
which is enough to protect against the square-root-time algorithms discussed
in §11.2.2 and §11.2.5. The modular exponentiations in the protocol will run
several times faster using “short,” 160-bit exponents rather than “long,”
1024-bit exponents.
For the following exercise, we need the following notions from complexity
theory.
• We say problem A is deterministic poly-time reducible to prob-
lem B if there exists a deterministic algorithm R for solving problem
A that makes calls to a subroutine for problem B, where the running
time of R (not including the running time for the subroutine for B)
is polynomial in the input length.
• We say that A and B are deterministic poly-time equivalent if
A is deterministic poly-time reducible to B and B is deterministic
poly-time reducible to A.
Exercise 11.10. Consider the following problems.
(a) Given a prime p, a prime q that divides p −1, an element γ ∈Z∗
p
generating a subgroup G of Z∗
p of order q, and two elements α, β ∈G,
compute γxy, where x := logγ α and y := logγ β. (This is just the
Diﬃe–Hellman problem.)
(b) Given a prime p, a prime q that divides p −1, an element γ ∈Z∗
p
generating a subgroup G of Z∗
p of order q, and an element α ∈G,
compute γx2, where x := logγ α.
(c) Given a prime p, a prime q that divides p −1, an element γ ∈Z∗
p
generating a subgroup G of Z∗
p of order q, and two elements α, β ∈G,
with β ̸= 1, compute γxy′, where x := logγ α, y′ := y−1 mod q, and
y := logγ β.
(d) Given a prime p, a prime q that divides p −1, an element γ ∈Z∗
p
TEAM LinG

278
Finding generators and discrete logarithms in Z∗
p
generating a subgroup G of Z∗
p of order q, and an element α ∈G,
with α ̸= 1, compute γx′, where x′ := x−1 mod q and x := logγ α.
Show that these problems are deterministic poly-time equivalent. Moreover,
your reductions should preserve the values of p, q, and γ; that is, if the
algorithm that reduces one problem to another takes as input an instance of
the former problem of the form (p, q, γ, . . .), it should invoke the subroutine
for the latter problem with inputs of the form (p, q, γ, . . .).
Exercise 11.11. Suppose there is a probabilistic algorithm A that takes
as input a prime p, a prime q that divides p −1, and an element γ ∈Z∗
p
generating a subgroup G of Z∗
p of order q. The algorithm also takes as input
α ∈G. It outputs either “failure,” or logγ α. Furthermore, assume that
A runs in strict polynomial time, and that for all p, q, and γ of the above
form, and for randomly chosen α ∈G, A succeeds in computing logγ α with
probability ϵ(p, q, γ). Here, the probability is taken over the random choice
of α, as well as the random choices made during the execution of A. Show
how to use A to construct another probabilistic algorithm A′ that takes as
input p, q, and γ as above, as well as α ∈G, runs in expected polynomial
time, and that satisﬁes the following property:
if ϵ(p, q, γ) ≥0.001, then for all α ∈G, A′ computes logγ α
with probability at least 0.999.
The algorithm A′ in the previous exercise is another example of a random
self-reduction (see discussion following Exercise 7.27).
Exercise 11.12. Let p be a prime, q a prime that divides p −1, γ ∈Z∗
p an
element that generates a subgroup G of Z∗
p of order q, and α ∈G. For δ ∈G,
a representation of δ with respect to γ and α is a pair of integers (r, s),
with 0 ≤r < q and 0 ≤s < q, such that γrαs = δ.
(a) Show that for any δ ∈G, there are precisely q representations (r, s)
of δ with respect to γ and α, and among these, there is precisely one
with s = 0.
(b) Show that given a representation (r, s) of 1 with respect to γ and α
such that s ̸= 0, we can eﬃciently compute logγ α.
(c) Show that given any δ ∈G, along with any two distinct representa-
tions of δ with respect to γ and α, we can eﬃciently compute logγ α.
(d) Suppose we are given access to an “oracle” that, when presented with
any δ ∈G, tells us some representation of δ with respect to γ and α.
Show how to use this oracle to eﬃciently compute logγ α.
TEAM LinG

11.3 The Diﬃe–Hellman key establishment protocol
279
The following two exercises examine the danger of the use of “short”
exponents in discrete logarithm based cryptographic schemes that do not
work with a group of prime order.
Exercise 11.13. Let p be a prime and let p −1 = qe1
1 · · · qer
r
be the prime
factorization of p −1. Let γ be a generator for Z∗
p. Let X, Y be positive
numbers. Let Q be the product of all the prime powers qei
i
with qi ≤Y .
Suppose you are given p, the primes qi dividing p −1 with qi ≤Y , along
with γ and an element α of Z∗
p. Assuming that x := logγ α < X, show how
to compute x in time
(Y 1/2 + (X/Q)1/2) · len(p)O(1).
Exercise 11.14. Continuing with the previous exercise, let Q′ be the prod-
uct of all the primes qi dividing p −1 with qi ≤Y . Note that Q′ | Q. The
goal of this exercise is to heuristically estimate the expected value of log Q′,
assuming p is a large, random prime. The heuristic part is this: we shall
assume that for any prime q ≤Y , the probability that q divides p −1 for
a randomly chosen “large” prime p is ∼1/q. Under this assumption, show
that
E[log Q′] ∼log Y.
The results of the previous two exercises caution against the use of “short”
exponents in cryptographic schemes based on the discrete logarithm problem
for Z∗
p. Indeed, suppose that p is a random 1024-bit prime, and that for
reasons of eﬃciency, one chooses X ≈2160, thinking that a method such
as the baby step/giant step method would require ≈280 steps to recover x.
However, if we choose Y ≈280, then we have reason to expect Q to be at
least about 280, in which case X/Q is at most about 280, and so we can in
fact recover x in roughly 240 steps, which may be a feasible number of steps,
whereas 280 steps may not be. Of course, none of these issues arise if one
works in a subgroup of Z∗
p of large prime order, which is the recommended
practice.
An interesting fact about the Diﬃe–Hellman problem is that there is no
known eﬃcient algorithm to recognize a solution to the problem. Some cryp-
tographic protocols actually rely on the apparent diﬃculty of this decision
problem, which is called the decisional Diﬃe–Hellman problem. The
following three exercises develop a random self-reducibility property for this
decision problem.
Exercise 11.15. Let p be a prime, q a prime dividing p −1, and γ an
TEAM LinG

280
Finding generators and discrete logarithms in Z∗
p
element of Z∗
p that generates a subgroup G of order q. Let α ∈G, and let H
be the subgroup of G×G generated by (γ, α). Let ˜γ, ˜α be arbitrary elements
of G, and deﬁne the map
ρ :
Zq × Zq →G × G
([r]q, [s]q) →(γr˜γs, αr ˜αs).
Show that the deﬁnition of ρ is unambiguous, that ρ is a group homomor-
phism, and that
• if (˜γ, ˜α) ∈H, then img(ρ) = H, and
• if (˜γ, ˜α) /∈H, then img(ρ) = G × G.
Exercise 11.16. For p, q, γ as in the previous exercise, let Dp,q,γ consist
of all triples of the form (γx, γy, γxy), and let Rp,q,γ consist of all triples of
the form (γx, γy, γz). Using the result from the previous exercise, design a
probabilistic algorithm that runs in expected polynomial time, and that on
input p, q, γ, along with a triple Γ ∈Rp,q,γ, outputs a triple Γ∗∈Rp,q,γ such
that
• if Γ ∈Dp,q,γ, then Γ∗is uniformly distributed over Dp,q,γ, and
• if Γ /∈Dp,q,γ, then Γ∗is uniformly distributed over Rp,q,γ.
Exercise 11.17. Suppose that A is a probabilistic algorithm that takes
as input p, q, γ as in the previous exercise, along a triple Γ∗∈Rp,q,γ, and
outputs either 0 or 1. Furthermore, assume that A runs in strict polynomial
time. Deﬁne two random variables, Xp,q,γ and Yp,q,γ, as follows:
• Xp,q,γ is deﬁned to be the output of A on input p, q, γ, and Γ∗, where
Γ∗is uniformly distributed over Dp,q,γ, and
• Yp,q,γ is deﬁned to be the output of A on input p, q, γ, and Γ∗, where
Γ∗is uniformly distributed over Rp,q,γ.
In both cases, the value of the random variable is determined by the random
choice of Γ∗, as well as the random choices made by the algorithm. Deﬁne
ϵ(p, q, γ) :=
P[Xp,q,γ = 1] −P[Yp,q,γ = 1]
.
Using the result of the previous exercise, show how to use A to design a
probabilistic, expected polynomial-time algorithm that takes as input p, q, γ
as above, along with Γ ∈Rp,q,γ, and outputs either “yes” or “no,” so that
if ϵ(p, q, γ) ≥0.001, then for all Γ ∈Rp,q,γ, the probability
that A′ correctly determines whether Γ ∈Dp,q,γ is at least
0.999.
TEAM LinG

11.4 Notes
281
Hint: use the Chernoﬀbound.
The following exercise demonstrates that distinguishing “Diﬃe–Hellman
triples” from “random triples” is hard only if the order of the underlying
group is not divisible by any small primes, which is another reason we have
chosen to work with groups of large prime order.
Exercise 11.18. Assume the notation of the previous exercise, but let us
drop the restriction that q is prime. Design and analyze a deterministic
algorithm A that takes inputs p, q, γ and Γ∗∈Rp,q,γ, that outputs 0 or 1,
and that satisﬁes the following property: if t is the smallest prime dividing
q, then A runs in time (t + len(p))O(1), and the “distinguishing advantage”
ϵ(p, q, γ) for A on inputs p, q, γ is at least 1/t.
11.4 Notes
The probabilistic algorithm in §11.1 for ﬁnding a generator for Z∗
p can be
made deterministic under a generalization of the Riemann hypothesis. In-
deed, as discussed in §10.7, under such a hypothesis, Bach’s result [10] im-
plies that for each prime q | (p −1), the least positive integer a such that
[a]p ∈Z∗
p \ (Z∗
p)q is at most 2 log p.
Related to the problem of constructing a generator for Z∗
p is the question
of how big is the smallest positive integer g such that [g]p is a generator
for Z∗
p; that is, how big is the smallest (positive) primitive root modulo p.
The best bounds on the least primitive root are also obtained using the
same generalization of the Riemann hypothesis mentioned above. Under
this hypothesis, Wang [98] showed that the least primitive root modulo p is
O(r6 len(p)2), where r is the number of distinct prime divisors of p−1. Shoup
[90] improved Wang’s bound to O(r4 len(r)4 len(p)2) by adapting a result of
Iwaniec [48, 49] and applying it to Wang’s proof. The best unconditional
bound on the smallest primitive root modulo p is p1/4+o(1) (this bound is
also in Wang [98]). Of course, just because there exists a small primitive
root, there is no known way to eﬃciently recognize a primitive root modulo
p without knowing the prime factorization of p −1.
As we already mentioned, all of the algorithms presented in this chapter
are completely “generic,” in the sense that they work in any ﬁnite cyclic
group—we really did not exploit any properties about Z∗
p other than the
fact that it is a cyclic group. In fact, as far as such “generic” algorithms
go, the algorithms presented here for discrete logarithms are optimal [67,
93]. However, there are faster, “non-generic” algorithms (though still not
TEAM LinG

282
Finding generators and discrete logarithms in Z∗
p
polynomial time) for discrete logarithms in Z∗
p. We shall examine one such
algorithm later, in Chapter 16.
The “baby step/giant step” algorithm in §11.2.2 is due to Shanks [86].
See, for example, the book by Cormen, Leiserson, Rivest, and Stein [29]
for appropriate data structures to implement the lookup table used in that
algorithm. In particular, see Problem 12-2 in [29] for a brief introduction
to radix trees, which is the data structure that yields the best running time
(at least in principle) for our application.
The algorithms in §11.2.3 and §11.2.4 are variants of an algorithm pub-
lished by Pohlig and Hellman [71]. See Chapter 4 of [29] for details on how
one analyzes recursive algorithms, such as the one presented in §11.2.3; in
particular, Section 4.2 in [29] discusses in detail the notion of a recursion
tree.
The algorithm in §11.2.5 is a variant of an algorithm of Pollard [72]; in
fact, Pollard’s algorithm is a bit more eﬃcient than the one presented here,
but the analysis of its running time depends on stronger heuristics. Pol-
lard’s paper also describes an algorithm for computing discrete logarithms
that lie in a restricted interval—if the interval has width w, this algorithm
runs (heuristically) in time w1/2 len(p)O(1), and requires space for O(len(w))
elements of Zp. This algorithm is useful in reducing the space requirement
for the algorithm of Exercise 11.13.
The key establishment protocol in §11.3 is from Diﬃe and Hellman [33].
That paper initiated the study of public key cryptography, which has
proved to be a very rich ﬁeld of research.
Exercises 11.13 and 11.14 are based on van Oorschot and Wiener [70].
For more on the decisional Diﬃe–Hellman assumption, see Boneh [18].
TEAM LinG

12
Quadratic residues and quadratic reciprocity
12.1 Quadratic residues
For positive integer n, an integer a is called a quadratic residue modulo
n if gcd(a, n) = 1 and x2 ≡a (mod n) for some integer x; in this case, we
say that x is a square root of a modulo n.
The quadratic residues modulo n correspond exactly to the subgroup of
squares (Z∗
n)2 of Z∗
n; that is, a is a quadratic residue modulo n if and only
if [a]n ∈(Z∗
n)2.
Let us ﬁrst consider the case where n = p, where p is an odd prime. In
this case, we know that Z∗
p is cyclic of order p−1 (see Theorem 9.16). Recall
that the subgroups any ﬁnite cyclic group are in one-to-one correspondence
with the positive divisors of the order of the group (see Theorem 8.31). For
any d | (p−1), consider the d-power map on Z∗
p that sends α ∈Z∗
p to αd. The
image of this map is the unique subgroup of Z∗
p of order (p −1)/d, and the
kernel of this map is the unique subgroup of order d. This means that the
image of the 2-power map is of order (p −1)/2 and must be the same as the
kernel of the (p −1)/2-power map. Since the image of the (p −1)/2-power
map is of order 2, it must be equal to the subgroup {±1}. The kernel of the
2-power map is of order 2, and so must also be equal to the subgroup {±1}.
Translating from group-theoretic language to the language of congruences,
we have shown:
Theorem 12.1. For an odd prime p, the number of quadratic residues
a modulo p, with 0 ≤a < p, is (p −1)/2.
Moreover, if x is a square
root of a modulo p, then so is −x, and any square root y of a modulo p
satisﬁes y ≡±x (mod p). Also, for any integer a ̸≡0 (mod p), we have
a(p−1)/2 ≡±1 (mod p), and moreover, a is a quadratic residue modulo p if
and only if a(p−1)/2 ≡1 (mod p).
283
TEAM LinG

284
Quadratic residues and quadratic reciprocity
Now consider the case where n = pe, where p is an odd prime and e > 1.
We also know that Z∗
pe is a cyclic group of order pe−1(p −1) (see Theo-
rem 10.1), and so everything that we said in discussing the case Z∗
p ap-
plies here as well.
In particular, for a ̸≡0 (mod p), a is a quadratic
residue modulo pe if and only if ape−1(p−1)/2 ≡1 (mod pe).
However,
we can simplify this a bit.
Note that ape−1(p−1)/2 ≡1 (mod pe) implies
ape−1(p−1)/2 ≡1 (mod p), and by Fermat’s little theorem, this implies
a(p−1)/2 ≡1 (mod p). Conversely, by Theorem 10.2, a(p−1)/2 ≡1 (mod p)
implies ape−1(p−1)/2 ≡1 (mod pe). Thus, we have shown:
Theorem 12.2. For an odd prime p and integer e > 1, the number of
quadratic residues a modulo pe, with 0 ≤a < pe, is pe−1(p−1)/2. Moreover,
if x is a square root of a modulo pe, then so is −x, and any square root y of
a modulo pe satisﬁes y ≡±x (mod pe). Also, for any integer a ̸≡0 (mod p),
we have ape−1(p−1)/2 ≡±1 (mod p), and moreover, a is a quadratic residue
modulo pe iﬀape−1(p−1)/2 ≡1 (mod pe) iﬀa(p−1)/2 ≡1 (mod p) iﬀa is a
quadratic residue modulo p.
Now consider an arbitrary odd integer n > 1, and let n = r
i=1 pei
i be its
prime factorization. Recall the group isomorphism implied by the Chinese
remainder theorem:
Z∗
n ∼= Z∗
pe1
1 × · · · × Z∗
per
r .
Now,
(α1, . . . , αr) ∈Z∗
pe1
1 × · · · × Z∗
per
r
is a square if and only if there exist β1, . . . , βr with βi ∈Z∗
pei
i
and αi = β2
i
for i = 1, . . . , r, in which case, we see that the square roots of (α1, . . . , αr)
comprise the 2r elements (±β1, . . . , ±βr). Thus we have:
Theorem 12.3. Consider an odd, positive integer n with prime factoriza-
tion n = r
i=1 pei
i .
The number of quadratic residues a modulo n, with
0 ≤a < n, is φ(n)/2r. Moreover, if a is a quadratic residue modulo n,
then there are precisely 2r distinct integers x, with 0 ≤x < n, such that
x2 ≡a (mod n). Also, an integer a is a quadratic residue modulo n if and
only if it is a quadratic residue modulo pi for i = 1, . . . , r.
That completes our investigation of the case where n is odd. We shall
not investigate the case where n is even, as it is a bit messy, and is not of
particular importance.
TEAM LinG

12.2 The Legendre symbol
285
12.2 The Legendre symbol
For an odd prime p and an integer a with gcd(a, p) = 1, the Legendre
symbol (a | p) is deﬁned to be 1 if a is a quadratic residue modulo p, and −1
otherwise. For completeness, one deﬁnes (a | p) = 0 if p | a. The following
theorem summarizes the essential properties of the Legendre symbol.
Theorem 12.4. Let p be an odd prime, and let a, b ∈Z. Then we have
(i) (a | p) ≡a(p−1)/2 (mod p); in particular, (−1 | p) = (−1)(p−1)/2;
(ii) (a | p)(b | p) = (ab | p);
(iii) a ≡b (mod p) implies (a | p) = (b | p);
(iv) (2 | p) = (−1)(p2−1)/8;
(v) if q is an odd prime, then
(p | q) = (−1)
p−1
2
q−1
2 (q | p).
Part (v) of this theorem is called the law of quadratic reciprocity.
Note that when p = q, both (p | q) and (q | p) are zero, and so the statement
of part (v) is trivially true—the interesting case is when p ̸= q, and in this
case, part (v) is equivalent to saying that
(p | q)(q | p) = (−1)
p−1
2
q−1
2 .
Part (i) of this theorem follows from Theorem 12.1. Part (ii) is an imme-
diate consequence of part (i), and part (iii) is clear from the deﬁnition.
The rest of this section is devoted to a proof of parts (iv) and (v) of this
theorem. The proof is completely elementary, although a bit technical.
Theorem 12.5 (Gauss’ lemma). Let p be an odd prime and let a be an
integer not divisible by p. Deﬁne αj := ja mod p for j = 1, . . . , (p−1)/2, and
let n be the number of indices j for which αj > p/2. Then (a | p) = (−1)n.
Proof. Let r1, . . . , rn denote the values αj that exceed p/2, and let s1, . . . , sk
denote the remaining values αj. The ri and si are all distinct and non-zero.
We have 0 < p −ri < p/2 for i = 1, . . . , n, and no p −ri is an sj; indeed,
if p −ri = sj, then sj ≡−ri (mod p), and writing sj = ua mod p and
ri = va mod p, for some u, v = 1, . . . , (p −1)/2, we have ua ≡−va (mod p),
which implies u ≡−v (mod p), which is impossible.
It follows that the sequence of numbers s1, . . . , sk, p−r1, . . . , p−rn is just
TEAM LinG

286
Quadratic residues and quadratic reciprocity
a re-ordering of 1, . . . , (p −1)/2. Then we have
((p −1)/2)! ≡s1 · · · sk(−r1) · · · (−rn)
≡(−1)ns1 · · · skr1 · · · rn
≡(−1)n((p −1)/2)! a(p−1)/2 (mod p),
and canceling the factor ((p −1)/2)!, we obtain a(p−1)/2 ≡(−1)n (mod p),
and the result follows from the fact that (a | p) ≡a(p−1)/2 (mod p). 2
Theorem 12.6. If p is an odd prime and gcd(a, 2p) = 1, then (a | p) =
(−1)t where t = (p−1)/2
j=1
⌊ja/p⌋. Also, (2 | p) = (−1)(p2−1)/8.
Proof. Let a be an integer not divisible by p, but which may be even, and let
us adopt the same notation as in the statement and proof of Theorem 12.5;
in particular, α1, . . . , α(p−1)/2, r1, . . . , rn, and s1, . . . , sk are as deﬁned there.
Note that ja = p⌊ja/p⌋+ αj, for j = 1, . . . , (p −1)/2, so we have
(p−1)/2

j=1
ja =
(p−1)/2

j=1
p⌊ja/p⌋+
n

j=1
rj +
k

j=1
sj.
(12.1)
Also, we saw in the proof of Theorem 12.5 that the integers s1, . . . , sk, p −
r1, . . . , p −rn are a re-ordering of 1, . . . , (p −1)/2, and hence
(p−1)/2

j=1
j =
n

j=1
(p −rj) +
k

j=1
sj = np −
n

j=1
rj +
k

j=1
sj.
(12.2)
Subtracting (12.2) from (12.1), we get
(a −1)
(p−1)/2

j=1
j = p
 (p−1)/2

j=1
⌊ja/p⌋−n

+ 2
n

j=1
rj.
(12.3)
Note that
(p−1)/2

j=1
j = p2 −1
8
,
(12.4)
which together with (12.3) implies
(a −1)p2 −1
8
≡
(p−1)/2

j=1
⌊ja/p⌋−n (mod 2).
(12.5)
TEAM LinG

12.3 The Jacobi symbol
287
If a is odd, (12.5) implies
n ≡
(p−1)/2

j=1
⌊ja/p⌋(mod 2).
(12.6)
If a = 2, then ⌊2j/p⌋= 0 for j = 1, . . . , (p −1)/2, and (12.5) implies
n ≡p2 −1
8
(mod 2).
(12.7)
The theorem now follows from (12.6) and (12.7), together with Theo-
rem 12.5. 2
Note that this last theorem proves part (iv) of Theorem 12.4. The next
theorem proves part (v).
Theorem 12.7. If p and q are distinct odd primes, then
(p | q)(q | p) = (−1)
p−1
2
q−1
2 .
Proof. Let S be the set of pairs of integers (x, y) with 1 ≤x ≤(p−1)/2 and
1 ≤y ≤(q −1)/2. Note that S contains no pair (x, y) with qx = py, so let
us partition S into two subsets: S1 contains all pairs (x, y) with qx > py,
and S2 contains all pairs (x, y) with qx < py. Note that (x, y) ∈S1 if and
only if 1 ≤x ≤(p −1)/2 and 1 ≤y ≤⌊qx/p⌋. So |S1| = (p−1)/2
x=1
⌊qx/p⌋.
Similarly, |S2| = (q−1)/2
y=1
⌊py/q⌋. So we have
p −1
2
q −1
2
= |S| = |S1| + |S2| =
(p−1)/2

x=1
⌊qx/p⌋+
(q−1)/2

y=1
⌊py/q⌋,
and Theorem 12.6 implies
(p | q)(q | p) = (−1)
p−1
2
q−1
2 . 2
12.3 The Jacobi symbol
Let a, n be integers, where n is positive and odd, so that n = q1 · · · qk, where
the qi are odd primes, not necessarily distinct. Then the Jacobi symbol
(a | n) is deﬁned as
(a | n) := (a | q1) · · · (a | qk),
where (a | qj) is the Legendre symbol. Note that (a | 1) = 1 for all a ∈Z.
Thus, the Jacobi symbol essentially extends the domain of deﬁnition of the
Legendre symbol.
Note that (a | n) ∈{0, ±1}, and that (a | n) = 0
TEAM LinG

288
Quadratic residues and quadratic reciprocity
if and only if gcd(a, n) > 1.
Also, note that if a is a quadratic residue
modulo n, then (a | n) = 1; however, (a | n) = 1 does not imply that a
is a quadratic residue modulo n. The following theorem summarizes the
essential properties of the Jacobi symbol.
Theorem 12.8. Let m, n be odd, positive integers, an let a, b be integers.
Then
(i) (ab | n) = (a | n)(b | n);
(ii) (a | mn) = (a | m)(a | n);
(iii) a ≡b (mod n) implies (a | n) = (b | n);
(iv) (−1 | n) = (−1)(n−1)/2;
(v) (2 | n) = (−1)(n2−1)/8;
(vi) (m | n) = (−1)
m−1
2
n−1
2 (n | m).
Proof. Parts (i)–(iii) follow directly from the deﬁnition (exercise).
For parts (iv) and (vi), one can easily verify (exercise) that for odd integers
n1, . . . , nk,
k

i=1
(ni −1)/2 ≡(n1 · · · nk −1)/2 (mod 2).
Part (iv) easily follows from this fact, along with part (ii) of this theorem and
part (i) of Theorem 12.4 (exercise). Part (vi) easily follows from this fact,
along with parts (i) and (ii) of this theorem, and part (v) of Theorem 12.4
(exercise).
For part (v), one can easily verify (exercise) that for odd integers
n1, . . . , nk,

1≤i≤k
(n2
i −1)/8 ≡(n2
1 · · · n2
k −1)/8 (mod 2).
Part (v) easily follows from this fact, along with part (ii) of this theorem,
and part (iv) of Theorem 12.4 (exercise). 2
As we shall see later, this theorem is extremely useful from a computa-
tional point of view—with it, one can eﬃciently compute (a | n), without
having to know the prime factorization of either a or n. Also, in applying
this theorem it is useful to observe that for odd integers m, n,
• (−1)(n−1)/2 = 1 iﬀn ≡1 (mod 4);
• (−1)(n2−1)/8 = 1 iﬀn ≡±1 (mod 8);
• (−1)((m−1)/2)((n−1)/2) = 1 iﬀm ≡1 (mod 4) or n ≡1 (mod 4).
TEAM LinG

12.4 Notes
289
It is sometimes useful to view the Jacobi symbol as a group homomor-
phism. Let n be an odd, positive integer. Deﬁne the Jacobi map
Jn :
Z∗
n →{±1}
[a]n →(a | n).
First, we note that by part (iii) of Theorem 12.8, this deﬁnition is unam-
biguous. Second, we note that since gcd(a, n) = 1 implies (a | n) = ±1, the
image of Jn is indeed contained in {±1}. Third, we note that by part (i) of
Theorem 12.8, Jn is a group homomorphism.
Since Jn is a group homomorphism, it follows that its kernel, ker(Jn), is
a subgroup of Z∗
n.
Exercise 12.1. Let n be an odd, positive integer. Show that [Z∗
n : (Z∗
n)2] =
2r, where r is the number of distinct prime divisors of n.
Exercise 12.2. Let n be an odd, positive integer, and consider the Jacobi
map Jn.
(a) Show that (Z∗
n)2 ⊆ker(Jn).
(b) Show that if n is the square of an integer, then ker(Jn) = Z∗
n.
(c) Show that if n is not the square of an integer, then [Z∗
n : ker(Jn)] = 2
and [ker(Jn) : (Z∗
n)2] = 2r−1, where r is the number of distinct prime
divisors of n.
Exercise 12.3. Let p and q be distinct primes, with p ≡q ≡3 (mod 4),
and let n := pq.
(a) Show that [−1]n ∈ker(Jn) \ (Z∗
n)2, and from this, conclude that
the cosets of (Z∗
n)2 in ker(Jn) are the two distinct cosets (Z∗
n)2 and
[−1]n(Z∗
n)2.
(b) Show that the squaring map on (Z∗
n)2 is a group automorphism.
(c) Let δ ∈Z∗
n\ker(Jn). Show that the map from {0, 1}×{0, 1}×(Z∗
n)2 →
Z∗
n that sends (a, b, γ) to δa(−1)bγ is a bijection.
12.4 Notes
The proof we present here of Theorem 12.4 is essentially the one from Niven
and Zuckerman [68]. Our proof of Theorem 12.8 is essentially the one found
in Bach and Shallit [12].
TEAM LinG

13
Computational problems related to quadratic
residues
13.1 Computing the Jacobi symbol
Suppose we are given an odd, positive integer n, along with an integer a,
and we want to compute the Jacobi symbol (a | n). Theorem 12.8 suggests
the following algorithm:
t ←1
repeat
// loop invariant: n is odd and positive
a ←a mod n
if a = 0
if n = 1 return t else return 0
compute a′, h such that a = 2ha′ and a′ is odd
if h ̸≡0 (mod 2) and n ̸≡±1 (mod 8) then t ←−t
if a′ ̸≡1 (mod 4) and n ̸≡1 (mod 4) then t ←−t
(a, n) ←(n, a′)
forever
That this algorithm correctly computes the Jacobi symbol (a | n) fol-
lows directly from Theorem 12.8. Using an analysis similar to that of Eu-
clid’s algorithm, one easily sees that the running time of this algorithm is
O(len(a) len(n)).
Exercise 13.1. Develop a “binary” Jacobi symbol algorithm, that is, one
that uses only addition, subtractions, and “shift” operations, analogous to
the binary gcd algorithm in Exercise 4.1.
Exercise 13.2. This exercise develops a probabilistic primality test based
290
TEAM LinG

13.2 Testing quadratic residuosity
291
on the Jacobi symbol. For odd integer n > 1, deﬁne
Gn := {α ∈Z∗
n : α(n−1)/2 = [Jn(α)]n},
where Jn : Z∗
n →{±1} is the Jacobi map.
(a) Show that Gn is a subgroup of Z∗
n.
(b) Show that if n is prime, then Gn = Z∗
n.
(c) Show that if n is composite, then Gn ⊊Z∗
n.
(d) Based on parts (a)–(c), design and analyze an eﬃcient probabilistic
primality test that works by choosing a random, non-zero element
α ∈Zn, and testing if α ∈Gn.
13.2 Testing quadratic residuosity
In this section, we consider the problem of testing whether a is a quadratic
residue modulo n, for given integers a and n, from a computational perspec-
tive.
13.2.1 Prime modulus
For an odd prime p, we can test if an integer a is a quadratic residue modulo p
by either performing the exponentiation a(p−1)/2 mod p or by computing the
Legendre symbol (a | p). Assume that 0 ≤a < p. Using a standard repeated
squaring algorithm, the former method takes time O(len(p)3), while using
the Euclidean-like algorithm of the previous section, the latter method takes
time O(len(p)2). So clearly, the latter method is to be preferred.
13.2.2 Prime-power modulus
For an odd prime p, we know that a is a quadratic residue modulo pe if and
only if a is a quadratic residue modulo p. So this case immediately reduces
to the previous case.
13.2.3 Composite modulus
For odd, composite n, if we know the factorization of n, then we can also de-
termine if a is a quadratic residue modulo n by determining if it is a quadratic
residue modulo each prime divisor p of n. However, without knowledge of
this factorization (which is in general believed to be hard to compute), there
is no eﬃcient algorithm known. We can compute the Jacobi symbol (a | n);
TEAM LinG

292
Computational problems related to quadratic residues
if this is −1 or 0, we can conclude that a is not a quadratic residue; otherwise,
we cannot conclude much of anything.
13.3 Computing modular square roots
In this section, we consider the problem of computing a square root of a
modulo n, given integers a and n, where a is a quadratic residue modulo n.
13.3.1 Prime modulus
Let p be an odd prime, and let a be an integer such that 0 < a < p and
(a | p) = 1. We would like to compute a square root of a modulo p. Let
α := [a]p ∈Z∗
p, so that we can restate our problem of that of ﬁnding β ∈Z∗
p
such that β2 = α, given α ∈(Z∗
p)2.
We ﬁrst consider the special case where p ≡3 (mod 4), in which it turns
out that this problem can be solved very easily. Indeed, we claim that in
this case
β := α(p+1)/4
is a square root of α—note that since p ≡3 (mod 4), the number (p + 1)/4
is an integer. To show that β2 = α, suppose α = ˜β2 for some ˜β ∈Z∗
p. We
know that there is such a ˜β, since we are assuming that α ∈(Z∗
p)2. Then
we have
β2 = α(p+1)/2 = ˜βp+1 = ˜β2 = α,
where we used Fermat’s little theorem for the third equality.
Using a
repeated-squaring algorithm, we can compute β in time O(len(p)3).
Now we consider the general case, where we may have p ̸≡3 (mod 4).
Here is one way to eﬃciently compute a square root of α, assuming we are
given, in addition to α, an auxiliary input γ ∈Z∗
p \ (Z∗
p)2 (how one obtains
such a γ is discussed below).
Let us write p −1 = 2hm, where m is odd. For any δ ∈Z∗
p, δm has mul-
tiplicative order dividing 2h. Since α2h−1m = 1, αm has multiplicative order
dividing 2h−1. Since γ2h−1m = −1, γm has multiplicative order precisely
2h. Since there is only one subgroup of Z∗
p of order 2h, it follows that γm
generates this subgroup, and that αm = γmx for 0 ≤x < 2h and x is even.
We can ﬁnd x by computing the discrete logarithm of αm to the base γm,
using the algorithm in §11.2.3. Setting κ = γmx/2, we have
κ2 = αm.
TEAM LinG

13.3 Computing modular square roots
293
We are not quite done, since we now have a square root of αm, and not of
α. Since m is odd, we may write m = 2t + 1 for some non-negative integer
t. It then follows that
(κα−t)2 = κ2α−2t = αmα−2t = αm−2t = α.
Thus, κα−t is a square root of α.
Let us summarize the above algorithm for computing a square root of
α ∈(Z∗
p)2, assuming we are given γ ∈Z∗
p \ (Z∗
p)2, in addition to α:
Compute positive integers m, h such that p −1 = 2hm with m odd
γ′ ←γm, α′ ←αm
Compute x ←logγ′ α′
// note that 0 ≤x < 2h and x is even
β ←(γ′)x/2α−⌊m/2⌋
output β
The total amount of work done outside the discrete logarithm calcu-
lation amounts to just a handful of exponentiations modulo p, and so
takes time O(len(p)3).
The time to compute the discrete logarithm is
O(h len(h) len(p)2). So the total running time of this procedure is
O(len(p)3 + h len(h) len(p)2).
The above procedure assumed we had at hand a non-square γ. If h = 1,
which means that p ≡3 (mod 4), then (−1 | p) = −1, and so we are done.
However, we have already seen how to eﬃciently compute a square root in
this case.
If h > 1, we can ﬁnd a non-square γ using a probabilistic search algorithm.
Simply choose γ at random, test if it is a square, and if so, repeat. The
probability that a random element of Z∗
p is a square is 1/2; thus, the expected
number of trials until we ﬁnd a non-square is 2, and hence the expected
running time of this probabilistic search algorithm is O(len(p)2).
Exercise 13.3. Let p be an odd prime, and let f ∈Zp[X] be a polynomial
with 0 ≤deg(f) ≤2. Design and analyze an eﬃcient, probabilistic algo-
rithm that determines if f has any roots in Zp, and if so, ﬁnds all of the
roots. Hint: see Exercise 9.14.
Exercise 13.4. Show that the following two problems are deterministic,
poly-time equivalent (see discussion just above Exercise 11.10 in §11.3):
(a) Given an odd prime p and α ∈(Z∗
p)2, ﬁnd β ∈Z∗
p such that β2 = α.
(b) Given an odd prime p, ﬁnd an element of Z∗
p \ (Z∗
p)2.
TEAM LinG

294
Computational problems related to quadratic residues
Exercise 13.5. Design and analyze an eﬃcient, deterministic algorithm
that takes as input primes p and q, such that q | (p −1), along with an
element α ∈Z∗
p, and determines whether or not α ∈(Z∗
p)q.
Exercise 13.6. Design and analyze an eﬃcient, deterministic algorithm
that takes as input primes p and q, such that q | (p −1) but q2 ∤(p −1),
along with an element α ∈(Z∗
p)q, and computes a qth root of α, that is, an
element β ∈Z∗
p such that βq = α.
Exercise 13.7. We are given a positive integer n, two elements α, β ∈Zn,
and integers e and f such that αe = βf and gcd(e, f) = 1.
Show how
to eﬃciently compute γ ∈Zn such that γe = β. Hint: use the extended
Euclidean algorithm.
Exercise 13.8. Design and analyze an algorithm that takes as input primes
p and q, such that q | (p−1), along with an element α ∈(Z∗
p)q, and computes
a qth root of α. (Unlike Exercise 13.6, we now allow q2 | (p −1).) Your
algorithm may be probabilistic, and should have an expected running time
that is bounded by q1/2 times a polynomial in len(p). Hint: the previous
exercise may be useful.
Exercise 13.9. Let p be an odd prime, γ be a generator for Z∗
p, and α be
any element of Z∗
p. Deﬁne
B(p, γ, α) :=
 1
if logγ α ≥(p −1)/2;
0
if logγ α < (p −1)/2.
Suppose that there is an algorithm that eﬃciently computes B(p, γ, α) for
all p, γ, α as above.
Show how to use this algorithm as a subroutine in
an eﬃcient, probabilistic algorithm that computes logγ α for all p, γ, α as
above. Hint: in addition to the algorithm that computes B, use algorithms
for testing quadratic residuosity and computing square roots modulo p, and
“read oﬀ” the bits of logγ α one at a time.
13.3.2 Prime-power modulus
Let p be an odd prime, let a be an integer relatively prime to p, and let e > 1
be an integer. We know that a is a quadratic residue modulo pe if and only
if a is a quadratic residue modulo p. Suppose that a is a quadratic residue
modulo p, and that we have found an integer z such that z2 ≡a (mod p),
using, say, one of the procedures described in §13.3.1. From this, we can
easily compute a square root of a modulo pe using the following technique,
which is known as Hensel lifting.
TEAM LinG

13.3 Computing modular square roots
295
More generally, suppose we have computed an integer z such that z2 ≡
a (mod pf), for some f ≥1, and we want to ﬁnd an integer ˆz such that
ˆz2 ≡a (mod pf+1). Clearly, if ˆz2 ≡a (mod pf+1), then ˆz2 ≡a (mod pf),
and so ˆz ≡±z (mod pf). So let us set ˆz = z + pfu, and solve for u. We
have
ˆz2 ≡(z + pfu)2 ≡z2 + 2zpfu + p2fu2 ≡z2 + 2zpfu (mod pf+1).
So we want to ﬁnd integer u such that
2zpfu ≡a −z2 (mod pf+1).
Since pf | (z2 −a), by Theorem 2.5, the above congruence holds if and only
if
2zu ≡a −z2
pf
(mod p).
From this, we can easily compute the desired value u, since gcd(2z, p) = 1.
By iterating the above procedure, starting with a square root of a modulo
p, we can quickly ﬁnd a square root of a modulo pe. We leave a detailed
analysis of the running time of this procedure to the reader.
Exercise 13.10. Suppose you are given a polynomial f ∈Z[X], along with
a prime p and a root z of f modulo p, that is, an integer z such that
f(z) ≡0 (mod p). Further, assume that z is simple root of f modulo p,
meaning that D(f)(z) ̸≡0 (mod p), where D(f) is the formal derivative of
f. Show that for any integer e ≥1, f has a root modulo pe, and give an
eﬃcient procedure to ﬁnd it. Also, show that the root modulo pe is uniquely
determined, in the following sense: if two such roots are congruent modulo
p, then they are congruent modulo pe.
13.3.3 Composite modulus
To ﬁnd square roots modulo n, where n is an odd composite modulus, if we
know the prime factorization of n, then we can use the above procedures
for ﬁnding square roots modulo primes and prime powers, and then use the
algorithm of the Chinese remainder theorem to get a square root modulo n.
However, if the factorization of n is not known, then there is no eﬃcient
algorithm known for computing square roots modulo n. In fact, one can show
that the problem of ﬁnding square roots modulo n is at least as hard as the
problem of factoring n, in the sense that if there is an eﬃcient algorithm for
TEAM LinG

296
Computational problems related to quadratic residues
computing square roots modulo n, then there is an eﬃcient (probabilistic)
algorithm for factoring n.
Here is an algorithm to factor n, using a modular square-root algorithm
as a subroutine. For simplicity, we assume that n is of the form n = pq,
where p and q are distinct, odd primes. Choose β to be a random, non-
zero element of Zn. If d := gcd(rep(β), n) > 1, then output d (recall that
rep(β) denotes the canonical representative of β). Otherwise, set α := β2,
and feed n and α to the modular square-root algorithm, obtaining a square
root β′ ∈Z∗
n of α. If the square-root algorithm returns β′ ∈Z∗
n such that
β′ = ±β, then output “failure”; otherwise, output gcd(rep(β −β′), n), which
is a non-trivial divisor of n.
Let us analyze this algorithm. If d > 1, we split n, so assume that d = 1,
which means that β ∈Z∗
n.
In this case, β is uniformly distributed over
Z∗
n, and α is uniformly distributed over (Z∗
n)2. Let us condition on an a
ﬁxed value of α, and on ﬁxed random choices made by the modular square-
root algorithm (in general, this algorithm may be probabilistic).
In this
conditional probability distribution, the value β′ returned by the algorithm
is completely determined. If θ : Zp × Zq →Zn is the ring isomorphism of
the Chinese remainder theorem, and β′ = θ(β′
1, β′
2), then in this conditional
probability distribution, β is uniformly distributed over the four square roots
of α, which we may write as θ(±β′
1, ±β′
2).
With probability 1/4, we have β = θ(β′
1, β′
2) = β′, and with probability
1/4, we have β = θ(−β′
1, −β′
2) = −β′, and so with probability 1/2, we
have β = ±β′, in which case we fail to factor n. However, with probability
1/4, we have β = θ(−β′
1, β′
2), in which case β −β′ = θ(−2β′
1, 0), and since
2β′
1 ̸= 0, we have p ∤rep(β −β′) and q | rep(β −β′), and so gcd(rep(β −
β′), n) = q. Similarly, with probability 1/4, we have β = θ(β′
1, −β′
2), in
which case β −β′ = θ(0, −2β′
2), and since 2β′
2 ̸= 0, we have p | rep(β −β′)
and q ∤rep(β −β′), and so gcd(rep(β −β′), n) = p. Thus, with probability
1/2, we have β ̸= ±β′, and gcd(rep(β −β′), n) splits n.
Since we split n with probability 1/2 conditioned on any ﬁxed choice α ∈
(Z∗
n)2 and any ﬁxed random choices of the modular square-root algorithm,
it follows that we split n with probability 1/2 conditioned simply on the
event that β ∈Z∗
n. Also, conditioned on the event that β /∈Z∗
n, we split n
with certainty, and so we may conclude that the above algorithm splits n
with probability at least 1/2.
Exercise 13.11. Generalize the algorithm above to eﬃciently factor arbi-
TEAM LinG

13.4 The quadratic residuosity assumption
297
trary integers, given a subroutine that computes arbitrary modular square
roots.
13.4 The quadratic residuosity assumption
Loosely speaking, the quadratic residuosity (QR) assumption is the as-
sumption that it is hard to distinguish squares from non-squares in Z∗
n, where
n is of the form n = pq, and p and q are distinct primes. This assumption
plays an important role in cryptography. Of course, since the Jacobi symbol
is easy to compute, for this assumption to make sense, we have to restrict
our attention to elements of ker(Jn), where Jn : Z∗
n →{±1} is the Jacobi
map. We know that (Z∗
n)2 ⊆ker(Jn) (see Exercise 12.2). Somewhat more
precisely, the QR assumption is the assumption that it is hard to distinguish
a random element in ker(Jn) \ (Z∗
n)2 from a random element in (Z∗
n)2, given
n (but not its factorization!).
To give a rough idea as to how this assumption may be used in cryptog-
raphy, assume that p ≡q ≡3 (mod 4), so that [−1]n ∈ker(Jn) \ (Z∗
n)2, and
moreover, ker(Jn)\(Z∗
n)2 = [−1]n(Z∗
n)2 (see Exercise 12.3). The value n can
be used as a public key in a public-key cryptosystem (see §7.8). Alice, know-
ing the public key, can encrypt a single bit b ∈{0, 1} as β := (−1)bα2, where
Alice chooses α ∈Z∗
n at random. The point is, if b = 0, then β is uniformly
distributed over (Z∗
n)2, and if b = 1, then β is uniformly distributed over
ker(Jn)\(Z∗
n)2. Now Bob, knowing the secret key, which is the factorization
of n, can easily determine if β ∈(Z∗
n)2 or not, and hence deduce the value of
the encrypted bit b. However, under the QR assumption, an eavesdropper,
seeing just n and β, cannot eﬀectively ﬁgure out what b is.
Of course, the above scheme is much less eﬃcient than the RSA cryp-
tosystem presented in §7.8, but nevertheless, has attractive properties; in
particular, its security is very closely tied to the QR assumption, whereas
the security of RSA is a bit less well understood.
Exercise 13.12. Suppose that A is a probabilistic algorithm that takes as
input n of the form n = pq, where p and q are distinct primes such that
p ≡q ≡3 (mod 4). The algorithm also takes as input α ∈ker(Jn), and
outputs either 0 or 1. Furthermore, assume that A runs in strict polynomial
time. Deﬁne two random variables, Xn and Yn, as follows: Xn is deﬁned
to be the output of A on input n and a value α chosen at random from
ker(Jn) \ (Z∗
n)2, and Yn is deﬁned to be the output of A on input n and a
value α chosen at random from (Z∗
n)2. In both cases, the value of the random
variable is determined by the random choice of α, as well as the random
TEAM LinG

298
Computational problems related to quadratic residues
choices made by the algorithm. Deﬁne ϵ(n) := |P[Xn = 1] −P[Yn = 1]|.
Show how to use A to design a probabilistic, expected polynomial time
algorithm A′ that takes as input n as above and α ∈ker(Jn), and outputs
either “square” or “non-square,” with the following property:
if ϵ(n) ≥0.001, then for all α ∈ker(Jn), the probability that
A′ correctly identiﬁes whether α ∈(Z∗
n)2 is at least 0.999.
Hint: use the Chernoﬀbound.
Exercise 13.13. Assume the same notation as in the previous exercise.
Deﬁne the random variable X′
n to be the output of A on input n and a value
α chosen at random from ker(Jn). Show that |P[X′
n = 1] −P[Yn = 1]| =
ϵ(n)/2. Thus, the problem of distinguishing ker(Jn) from (Z∗
n)2 is essentially
equivalent to the problem of distinguishing ker(Jn) \ (Z∗
n)2 from (Z∗
n)2.
13.5 Notes
Exercise 13.2 is based on Solovay and Strassen [94].
The probabilistic algorithm in §13.3.1 for computing square roots modulo
p can be made deterministic under a generalization of the Riemann hypothe-
sis. Indeed, as discussed in §10.7, under such a hypothesis, Bach’s result [10]
implies that the least positive integer that is not a quadratic residue modulo
p is at most 2 log p (this follows by applying Bach’s result with the sub-
group (Z∗
p)2 of Z∗
p). Thus, we may ﬁnd the required element γ ∈Z∗
p \ (Z∗
n)2
in deterministic polynomial time, just by brute-force search. The best un-
conditional bound on the smallest positive integer that is not a quadratic
residue modulo p is due to Burgess [22], who gives a bound of pα+o(1), where
α := 1/(4√e) ≈0.15163.
Goldwasser and Micali [39] introduced the quadratic residuosity assump-
tion to cryptography (as discussed in §13.4). This assumption has subse-
quently been used as the basis for numerous cryptographic schemes.
TEAM LinG

14
Modules and vector spaces
In this chapter, we introduce the basic deﬁnitions and results concerning
modules over a ring R and vector spaces over a ﬁeld F. The reader may
have seen some of these notions before, but perhaps only in the context of
vector spaces over a speciﬁc ﬁeld, such as the real or complex numbers, and
not in the context of, say, ﬁnite ﬁelds like Zp.
14.1 Deﬁnitions, basic properties, and examples
Throughout this section, R denotes a ring.
Deﬁnition 14.1. An R-module is an abelian group M, which we shall write
using additive notation, together with a scalar multiplication operation
that maps a ∈R and α ∈M to an element aα ∈M, such that the following
properties are satisﬁed for all a, b ∈R and α, β ∈M:
(i) a(bα) = (ab)α,
(ii) (a + b)α = aα + bα,
(iii) a(α + β) = aα + aβ,
(iv) 1Rα = α.
One may also call an R-module M a module over R. Elements of R are
often referred to as scalars, and elements of M may be called vectors.
Note that for an R-module M, for ﬁxed a ∈R, the map that sends α ∈M
to aα ∈M is a group homomorphism with respect to the additive group
operation of M; likewise, for ﬁxed α ∈M, the map that sends a ∈R to
aα ∈M is a group homomorphism from the additive group of R into the
additive group of M.
The following theorem summarizes a few basic facts which follow directly
299
TEAM LinG

300
Modules and vector spaces
from the observations in the previous paragraph, and basic facts about group
homomorphisms (see Theorem 8.20):
Theorem 14.2. If M is a module over R, then for all a ∈R, α ∈M, and
m ∈Z, we have:
(i) 0Rα = 0M,
(ii) a0M = 0M,
(iii) (−a)α = −(aα) = a(−α),
(iv) (ma)α = m(aα) = a(mα).
Proof. Exercise. 2
The deﬁnition of a module includes the trivial module, consisting of just
the zero element 0M. If R is the trivial ring, then any R-module is trivial,
since for all α ∈M, we have α = 1Rα = 0Rα = 0M.
Example 14.1. A simple but extremely important example of an R-module
is the set R×n of n-tuples of elements of R, where addition and scalar multi-
plication are deﬁned component-wise—that is, for α = (a1, . . . , an) ∈R×n,
β = (b1, . . . , an) ∈R×n, and a ∈R, we have
α + β = (a1 + b1, . . . , an + bn) and aα = (aa1, . . . , aan). 2
Example 14.2. The ring of polynomials R[X] over R forms an R-module
in the natural way, with addition and scalar multiplication deﬁned in terms
of the addition and multiplication operations of the polynomial ring. 2
Example 14.3. As in Example 9.34, let f be a monic polynomial over R
of degree ℓ≥0, and consider the quotient ring E := R[X]/(f). Then E is
a module over R, with addition deﬁned in terms of the addition operation
of R, and scalar multiplication deﬁned by a[g]f := [ag]f, for a ∈R and
g ∈R[X]. If f = 1, then E is trivial. 2
Example 14.4. If E is any ring containing R as a subring (i.e., E is an
extension ring of R), then E is a module over R, with addition and scalar
multiplication deﬁned in terms of the addition and multiplication operations
of E. 2
Example 14.5. If M1, . . . , Mn are R-modules, then so is the direct product
M1 × · · · × Mn, where addition and scalar product are deﬁned component-
wise. 2
Example 14.6. Any abelian group G, written additively, can be viewed as
TEAM LinG

14.2 Submodules and quotient modules
301
a Z-module, with scalar multiplication deﬁned in terms of the usual integer
multiplication map (see parts (vi)–(viii) of Theorem 8.3). 2
Example 14.7. Let G be any group, written additively, whose exponent
divides n. Then we may deﬁne a scalar multiplication that maps [m]n ∈Zn
and α ∈G to mα. That this map is unambiguously deﬁned follows from the
fact that G has exponent dividing n, so that if m ≡m′ (mod n), we have
mα −m′α = (m −m′)α = 0G, since n | (m −m′). It is easy to check that
this scalar multiplication operation indeed makes G into a Zn-module. 2
Example 14.8. Of course, viewing a group as a module does not depend on
whether or not we happen to use additive notation for the group operation.
If we specialize the previous example to the group G = Z∗
p, where p is prime,
then we may view G as a Zp−1-module. However, since the group operation
itself is written multiplicatively, the “scalar product” of [m]p−1 ∈Zp−1 and
α ∈Z∗
p is the power αm. 2
14.2 Submodules and quotient modules
Again, throughout this section, R denotes a ring. The notions of subgroups
and quotient groups extend in the obvious way to R-modules.
Deﬁnition 14.3. Let M be an R-module. A subset N is a submodule of
M if
(i) N is a subgroup of the additive group M, and
(ii) N is closed under scalar multiplication; that is, for all a ∈R and
α ∈N, we have aα ∈N.
It is easy to see that a submodule N of an R-module M is also an R-
module in its own right, with addition and scalar multiplication operations
inherited from M.
Expanding the above deﬁnition, we see that a subset N of M is a sub-
module if and only if for all a ∈R and all α, β ∈N, we have
α + β ∈N,
−α ∈N,
and aα ∈N.
Observe that the condition −α ∈N is redundant, as it is implied by the
condition aα ∈N with a = −1R.
For m ∈Z, it is easy to see (verify) that not only are mM and M{m}
subgroups of M (see Theorems 8.6 and 8.7), they are also submodules of M.
Moreover, for a ∈R, aM := {aα : α ∈M} and M{a} := {α ∈M : aα =
0M} are also submodules of M (verify).
TEAM LinG

302
Modules and vector spaces
Let α1, . . . , αn be elements of M. In general, the subgroup ⟨α1, . . . , αn⟩
will not be a submodule of M. Instead, let us consider the set ⟨α1, . . . , αn⟩R,
consisting of all R-linear combinations of α1, . . . , αn, with coeﬃcients
taken from R:
⟨α1, . . . , αn⟩R := {a1α1 + · · · + anαn : a1, . . . , an ∈R}.
It is not hard to see (verify) that ⟨α1, . . . , αn⟩R is a submodule of M con-
taining α1, . . . , αn; it is called the submodule spanned or generated by
α1, . . . , αn. Moreover, it is easy to see (verify) that any submodule contain-
ing α1, . . . , αn must contain ⟨α1, . . . , αn⟩R. As a matter of deﬁnition, we
allow n = 0, in which case, the spanned submodule is {0M}.
If N1 and N2 are submodules of M, then N1 + N2 and N1 ∩N2 are not
only subgroups of M, they are also submodules of M (verify).
Example 14.9. For integer ℓ≥0, deﬁne R[X]<ℓto be the set of polynomials
of degree less than ℓ. The reader may verify that R[X]<ℓis a submodule of
the R-module R[X]. If ℓ= 0, then this submodule is the trivial submodule
{0R}. 2
Example 14.10. Let G be an abelian group. As in Example 14.6, we can
view G as a Z-module in a natural way. Subgroups of G are just the same
thing as submodules of G, and for a1, . . . , an ∈G, the subgroup ⟨a1, . . . , an⟩
is the same as the submodule ⟨a1, . . . , an⟩Z. 2
Example 14.11. Any ring R can be viewed as an R-module in the obvious
way, with addition and scalar multiplication deﬁned in terms of the addition
and multiplication operations of R. With respect to this module structure,
ideals of R are just the same thing as submodules of R, and for a1, . . . , an ∈
R, the ideal (a1, . . . , an) is the same as the submodule ⟨a1, . . . , an⟩R. 2
Example 14.12. Let α1, . . . , αn and β1, . . . , βm be elements of an R-
module. Assume that each αi can be expressed as an R-linear combination
of β1, . . . , βm. Then the submodule spanned by α1, . . . , αn is contained in
the submodule spanned by β1, . . . , βm.
One can see this in a couple of diﬀerent ways. First, the assumption that
each αi can be expressed as an R-linear combination of β1, . . . , βm means
that the submodule ⟨β1, . . . , βm⟩R contains the elements α1, . . . , αn, and
so by the general properties sketched above, this submodule must contain
⟨α1, . . . , αn⟩R.
TEAM LinG

14.3 Module homomorphisms and isomorphisms
303
One can also see this via an explicit calculation. Suppose that
αi =
m

j=1
cijβj (i = 1, . . . , n),
where the cij are elements of R. Then for any element γ in the submodule
spanned by α1, . . . , αn, there exist a1, . . . , an ∈R with
γ =
n

i=1
aiαi =
n

i=1
ai
m

j=1
cijβj =
m

j=1
 n

i=1
aicij

βj,
and hence γ is contained in the submodule spanned by β1, . . . , βm. 2
If N is a submodule of M, then in particular, it is also a subgroup of
M, and we can form the quotient group M/N in the usual way (see §8.3).
Moreover, because N is closed under scalar multiplication, we can also deﬁne
a scalar multiplication on M/N in a natural way. Namely, for a ∈R and
α ∈M, we deﬁne
a · (α + N) := (aα) + N.
As usual, one must check that this deﬁnition is unambiguous, that is, if
α ≡α′ (mod N), then aα ≡aα′ (mod N). But this follows from the fact
that N is closed under scalar multiplication (verify). One can also easily
check (verify) that with scalar multiplication deﬁned in this way, M/N is
an R-module; it is called the quotient module of M modulo N.
14.3 Module homomorphisms and isomorphisms
Again, throughout this section, R is a ring. The notion of a group homo-
morphism extends in the obvious way to R-modules.
Deﬁnition 14.4. Let M and M ′ be modules over R. An R-module ho-
momorphism from M to M′ is a map ρ : M →M ′, such that
(i) ρ is a group homomorphism from M to M′, and
(ii) for all a ∈R and α ∈M, we have ρ(aα) = aρ(α).
An R-module homomorphism is also called an R-linear map. We shall
use this terminology from now on. Expanding the deﬁnition, we see that a
map ρ : M →M′ is an R-linear map if and only if ρ(α + β) = ρ(α) + ρ(β)
and ρ(aα) = aρ(α) for all α, β ∈M and all a ∈R.
Since an R-module homomorphism is also a group homomorphism on the
underlying additive groups, all of the statements in Theorem 8.20 apply. In
TEAM LinG

304
Modules and vector spaces
particular, an R-linear map is injective if and only if the kernel is trivial
(i.e., contains only the zero element). However, in the case of R-module
homomorphisms, we can extend Theorem 8.20, as follows:
Theorem 14.5. Let ρ : M →M ′ be an R-linear map.
(i) For any submodule N of M, ρ(N) is a submodule of M′.
(ii) ker(ρ) is a submodule of M.
(iii) For any submodule N ′ of M′, ρ−1(N′) is a submodule of M.
Proof. Exercise. 2
Theorems 8.21, 8.22, and 8.23 have natural R-module analogs:
Theorem 14.6. If ρ : M →M ′ and ρ′ : M′ →M′′ are R-linear maps, then
so is their composition ρ′ ◦ρ : M →M′′.
Proof. Exercise. 2
Theorem 14.7. Let ρi : M →Mi, for i = 1, . . . , n, be R-linear maps. Then
the map ρ : M →M1 × · · · × Mn that sends α ∈M to (ρ1(α), . . . , ρn(α)) is
an R-linear map.
Proof. Exercise. 2
Theorem 14.8. Let ρi : Mi →M, for i = 1, . . . , n, be R-linear maps. Then
the map ρ : M1 × · · · × Mn →M that sends (α1, . . . , αn) to ρ1(α1) + · · · +
ρn(αn) is an R-linear map.
Proof. Exercise. 2
If an R-linear map ρ : M →M ′ is bijective, then it is called an R-module
isomorphism of M with M ′. If such an R-module isomorphism ρ exists,
we say that M is isomorphic to M′, and write M ∼= M′. Moreover, if
M = M′, then ρ is called an R-module automorphism on M.
Analogous to Theorem 8.24, we have:
Theorem 14.9. If ρ is a R-module isomorphism of M with M ′, then the
inverse function ρ−1 is an R-module isomorphism of M′ with M.
Proof. Exercise. 2
Theorems 8.25, 8.26, 8.27, and 8.28 generalize immediately to R-modules:
Theorem 14.10. If N is a submodule of an R-module M, then the natural
map ρ : M →M/N given by ρ(α) = α + N is a surjective R-linear map
whose kernel is N.
TEAM LinG

14.3 Module homomorphisms and isomorphisms
305
Proof. Exercise. 2
Theorem 14.11. Let ρ be an R-linear map from M into M′. Then the map
¯ρ : M/ ker(ρ) →img(ρ) that sends the coset α +ker(ρ) for α ∈M to ρ(α) is
unambiguously deﬁned and is an R-module isomorphism of M/ ker(ρ) with
img(ρ).
Proof. Exercise. 2
Theorem 14.12. Let ρ be an R-linear map from M into M ′. Then for any
submodule N contained in ker(ρ), the map ¯ρ : M/N →img(ρ) that sends the
coset α + N for α ∈M to ρ(α) is unambiguously deﬁned and is an R-linear
map from M/N onto img(ρ) with kernel ker(ρ)/N.
Proof. Exercise. 2
Theorem 14.13. Let M be an R-module with submodules N1, N2. Then
the map ρ : N1 × N2 →N1 + N2 that sends (α1, α2) to α1 + α2 is a surjec-
tive R-linear map. Moreover, if N1 ∩N2 = {0M}, then ρ is an R-module
isomorphism of N1 × N2 with N1 + N2.
Proof. Exercise. 2
Example 14.13. Let M be an R-module, and let m be an integer. Then
the m-multiplication on M is not only a group homomorphism, but it is an
R-linear map. 2
Example 14.14. Let M be an R-module, and let a be an element of R.
The a-multiplication map on M is the map that sends α ∈M to aα ∈M.
This is an R-linear map whose image is aM, and whose kernel is M{a}. The
set of all a ∈R for which aM = {0M} is called the R-exponent of M, and
is easily seen to be an ideal of R (verify). 2
Example 14.15. Let M be an R-module, and let α be an element of M.
Then the map ρ : R →M given by ρ(a) = aα is an R-linear map. The
image of this map is ⟨α⟩R. The kernel of this map is called the R-order of
α, and is easily seen to be an ideal of R (verify). 2
Example 14.16. Consider again the R-module R[X]/(f) discussed in Ex-
ample 14.3, where f is monic of degree ℓ.
As an R-module, R[X]/(f) is
isomorphic to R[X]<ℓ(see Example 14.9). Indeed, based on the observations
in Example 9.34, the map ρ : R[X]<ℓ→R[X]/(f) that sends a polynomial
g ∈R[X] of degree less than ℓto [g]f ∈R[X]/(f) is an isomorphism of R[X]<ℓ
with R[X]/(f). Furthermore, R[X]<ℓis isomorphic as an R-module to R×ℓ.
TEAM LinG

306
Modules and vector spaces
Indeed, the map ρ′ : R[X]<ℓ→R×ℓthat sends g = ℓ−1
i=0 giXi ∈R[X]<ℓto
(g0, . . . , gℓ−1) ∈R×ℓis an isomorphism of R[X]<ℓwith R×ℓ. 2
Example 14.17. Let E and E′ be ring extensions of the ring R. As we
saw in Example 14.4, E and E′ may be viewed as R-modules in a natural
way. Suppose that ρ : E →E′ is a ring homomorphism whose restriction to
R is the identity map (i.e., ρ(a) = a for all a ∈R). Then ρ is an R-linear
map. Indeed, for any a ∈R and α, β ∈E, we have ρ(α + β) = ρ(α) + ρ(β)
and ρ(aα) = ρ(a)ρ(α) = aρ(α). 2
14.4 Linear independence and bases
Throughout this section, R denotes a ring.
Deﬁnition 14.14. We say that an R-module M is ﬁnitely generated
(over R) if it is spanned by a ﬁnite number of elements, which is to say
that M = ⟨α1, . . . , αn⟩R for some α1, . . . , αn ∈M.
We say that a collection of elements α1, . . . , αn in M is linearly de-
pendent (over R) if there exist a1, . . . , an ∈R, not all zero, such that
a1α1 + · · · anαn = 0M; otherwise, we say that α1, . . . , αn are linearly in-
dependent (over R).
We say that a collection α1, . . . , αn of elements in M is a basis for M
(over R) if it is linearly independent and spans M.
Note that in the above deﬁnition, the collection of elements α1, . . . , αn
may contain duplicates; the collection may also be empty (i.e., n = 0),
in which case, by deﬁnition, it is a basis for the trivial submodule {0M}.
Note that the ordering of the elements α1, . . . , αn makes no diﬀerence in
any aspect of the deﬁnition.
Example 14.18. Consider the R-module R×n. Deﬁne α1, . . . , αn ∈R×n
as follows:
α1 := (1, 0, . . . , 0), α2 := (0, 1, 0, . . . , 0), . . . , αn := (0, . . . , 0, 1);
that is, αi has a 1 in position i and is zero everywhere else. It is easy to
see that α1, . . . , αn form a basis for R×n. Indeed, for any a1, . . . , an ∈R,
we have a1α1 + · · · + anαn = (a1, . . . , an), from which it is clear that the αi
span R×n and are linearly independent. The vectors α1, . . . , αn form what
is called the standard basis for R×n. 2
Example 14.19. Consider the Z-module Z×3. In addition to the standard
TEAM LinG

14.4 Linear independence and bases
307
basis
(1, 0, 0), (0, 1, 0), (0, 0, 1),
the vectors
α1 := (1, 1, 1), α2 := (0, 1, 0), α3 := (2, 0, 1)
also form a basis. To see this, ﬁrst observe that for a1, a2, a3, b1, b2, b3 ∈Z,
we have
(b1, b2, b3) = a1α1 + a2α2 + a3α3
if and only if
b1 = a1 + 2a3, b2 = a1 + a2, and b3 = a1 + a3.
(14.1)
If (14.1) holds with b1 = b2 = b3 = 0, then subtracting the equation a1+a3 =
0 from a1 + 2a3 = 0, we see that a3 = 0, from which it easily follows that
a1 = a2 = 0. This shows that the vectors are linearly independent. To show
that they span Z×3, the reader may verify that for any given b1, b2, b3 ∈Z,
the values
a1 := −b1 + 2b3, a2 := b1 + b2 −2b3, a3 := b1 −b3
satisfy (14.1).
The vectors
(1, 1, 1), (0, 1, 0), (1, 0, 1)
do not form a basis, as they are linearly dependent: the third vector is equal
to the ﬁrst minus the second.
The vectors (1, 0, 12), (0, 1, 30), (0, 0, 18) are linearly independent, but do
not span Z×3 — the last component of any Z-linear combination of these
vectors must be divisible by gcd(12, 30, 18) = 6. These vectors do, however,
form a basis for the Q-module Q×3. 2
Example 14.20. If R is non-trivial, the ring of polynomials R[X] is not
ﬁnitely generated as an R-module, since any ﬁnite set of polynomials spans
only polynomials of some bounded degree. 2
Example 14.21. Consider the submodule R[X]<ℓof R[X], where ℓ≥0. If
ℓ= 0, then R[X]<ℓis trivial; otherwise, 1, X, . . . , Xℓ−1 form a basis. 2
Example 14.22. Consider again the ring E = R[X]/(f), where f ∈R[X] is
monic of degree ℓ≥0. If f = 1, then E is trivial; otherwise, 1, η, η2, . . . , ηℓ−1,
where η := [X]f ∈E, form a basis for E over R. 2
The next theorem highlights a critical property of bases:
TEAM LinG

308
Modules and vector spaces
Theorem 14.15. If α1, . . . , αn form a basis for M, then the map ρ : R×n →
M that sends (a1, . . . , an) ∈R×n to a1α1 + · · · + anαn ∈M is an R-module
isomorphism of R×n with M.
In particular, every element of M can be
expressed in a unique way as a1α1 + · · · + anαn, for a1, . . . , an ∈R.
Proof. To show this, one has to show (1) that ρ is an R-linear map, which
follows immediately from the deﬁnitions, (2) that ρ is injective, which follows
immediately from the linear independence of α1, . . . , αn, and (3) that ρ is
surjective, which follows immediately from the fact that α1, . . . , αn span M.
2
The following theorems develop important connections among the notions
of spanning, linear independence, and linear maps.
Theorem 14.16. Suppose that α1, . . . , αn span an R-module M and that
ρ : M →M′ is an R-linear map.
(i) ρ is surjective if and only if ρ(α1), . . . , ρ(αn) span M′.
(ii) If ρ(α1), . . . , ρ(αn) are linearly independent, then ρ is injective.
Proof. Since the αi span M, every element of M can be expressed as 
i aiαi,
where the ai are in R. It follows that the image of ρ consists of all elements
of M ′ of the form ρ(
i aiαi) = 
i aiρ(αi). That is, the image of ρ is the
submodule of M′ spanned by ρ(α1), . . . , ρ(αn), which implies (i).
For (ii), suppose that ρ is not injective.
Then ρ(α) = 0M′ for some
α ̸= 0M, and since the αi span M, we can write α = 
i aiαi, where the ai
are in R. Since α is non-zero, some of the ai must be non-zero. So we have
0M′ = ρ(
i aiαi) = 
i aiρ(αi), and hence ρ(α1), . . . , ρ(αn) are linearly
dependent. 2
Theorem 14.17. Suppose ρ : M →M′ is an injective R-linear map and
that α1, . . . , αn ∈M are linearly independent. Then ρ(α1), . . . , ρ(αn) are
linearly independent.
Proof. Suppose that 0M′ = 
i aiρ(αi) = ρ(
i aiαi). Then, as ker(ρ) =
{0M}, we must have 
i aiαi = 0M, and as the αi are linearly independent,
all the ai must be zero. 2
Theorem 14.18. Let α1, . . . , αn be a basis for an R-module M, and let
ρ : M →M ′ be an R-linear map.
(i) ρ is surjective if and only if ρ(α1), . . . , ρ(αn) span M′.
(ii) ρ is injective if and only if ρ(α1), . . . , ρ(αn) are linearly independent.
(iii) ρ is an isomorphism if and only if ρ(α1), . . . , ρ(αn) form a basis for
M′.
TEAM LinG

14.5 Vector spaces and dimension
309
Proof. (i) follows immediately from part (i) of Theorem 14.16. (ii) follows
from part (ii) of Theorem 14.16 and Theorem 14.17. (iii) follows from (i)
and (ii). 2
Exercise 14.1. Show that if a ﬁnite collection of elements of an R-module
is linearly independent, then any sub-collection is also linearly independent.
Exercise 14.2. Assume R is non-trivial. Show that if a ﬁnite collection of
elements of an R-module contains the zero element, or contains two identical
elements, then it is not linearly independent.
Exercise 14.3. Assume R is trivial and that M is an R-module (which
must also be trivial). Show that any ﬁnite collection of zero or more copies
of 0M is a basis for M.
Exercise 14.4. Let ρ : M →M ′ be an R-linear map.
Show that if
α1, . . . , αn ∈M are linearly dependent, then ρ(α1), . . . , ρ(αn) ∈M ′ are
also linearly dependent.
14.5 Vector spaces and dimension
Throughout this section, F denotes a ﬁeld.
A module over a ﬁeld is also called a vector space. In particular, an
F-module is called an F-vector space, or a vector space over F.
For vector spaces over F, one typically uses the terms subspace and
quotient space, instead of (respectively) submodule and quotient module;
likewise, one usually uses the terms F-vector space homomorphism,
isomorphism and automorphism, as appropriate.
Throughout the rest of this section, V denotes a vector space over F.
We now develop the basic theory of dimension for ﬁnitely generated vector
spaces. The following two theorems provide the keys to this theory.
Theorem 14.19. If V is ﬁnitely generated, then any ﬁnite set of vectors
that spans V contains a subset that is a basis.
Proof. We give an “algorithmic” proof. Let α1, . . . , αn be a given set of
vectors that spans V . Let S0 be the empty set, and for i = 1, . . . , n, do
the following: if αi does not belong to the subspace spanned by Si−1, set
Si := Si−1 ∪{αi}, and otherwise, set Si := Si−1. We claim that Sn is a basis
for V .
First, we show that Sn spans V . To do this, ﬁrst note that for i = 1, . . . , n,
if αi is not in Sn, then by deﬁnition, αi is a linear combination of vectors in
TEAM LinG

310
Modules and vector spaces
Si−1 ⊆Sn. In any case, each αi is a linear combination of the vectors in Sn.
Since any element β of V is a linear combination of α1, . . . , αn, and each
αi is a linear combination of elements of Sn, it follows (see Example 14.12)
that β is a linear combination of elements of Sn.
Second, we show that Sn is linearly independent. Suppose it were not.
Then we could express 0V as a non-trivial linear combination of elements in
Sn. Let us write this as
0V = a1α1 + a2α2 + · · · + anαn,
where the only non-zero coeﬃcients ai are those with αi ∈Sn. If j is the
highest index with aj ̸= 0F , then by deﬁnition αj ∈Sn. However, we see
that αj is in fact in the span of Sj−1; indeed,
αj = (−a−1
j a1)α1 + · · · + (−a−1
j aj−1)αj−1,
and by deﬁnition, the only terms with non-zero coeﬃcients are those corre-
sponding to the vectors in Sj−1. This means that we would not have added
αj to Sj at step j, which means αj is not in Sn, a contradiction. 2
Theorem 14.20. If V has a basis of size n, then any collection of n + 1
elements of V is linearly dependent.
Proof. Let α1, . . . , αn be a basis, and let β1, . . . , βn+1 be any collection of
n + 1 vectors. We wish to show that β1, . . . , βn+1 are linearly dependent.
Since the αi span V , we know that β1 is a linear combination of the αi, say,
β1 = a1α1+· · · anαn. If all the ai were zero, then we would have β1 = 0V , and
so trivially, β1, . . . , βn+1 would be linearly dependent (see Exercise 14.2). So
assume that not all ai are zero, and for convenience, let us say that a1 ̸= 0F .
It follows that α1 is a linear combination of β1, α2, . . . , αn; indeed,
α1 = a−1
1 β1 + (−a−1
1 a2)α2 + · · · + (−a−1
1 an)αn.
It follows that β1, α2, . . . , αn span V (see Example 14.12).
Next, consider β2.
This is a linear combination of β1, α2, . . . , αn, and
we may assume that in this linear combination, the coeﬃcient of one of
α2, . . . , αn is non-zero (otherwise, we ﬁnd a linear dependence among the
βj), and for convenience, let us say that the coeﬃcient of α2 is non-zero. As
in the previous paragraph, it follows that β1, β2, α3, . . . , αn span V .
Continuing in this way, we ﬁnd that β1, . . . , βn are either linearly depen-
dent or they span V . In the latter case, we ﬁnd that βn+1 is a linear com-
bination of β1, . . . , βn, and hence, the vectors β1, . . . , βn, βn+1 are linearly
dependent. 2
TEAM LinG

14.5 Vector spaces and dimension
311
We stress that the proofs of Theorems 14.19 and 14.20 both made critical
use of the assumption that F is a ﬁeld. An important corollary of Theo-
rem 14.20 is the following:
Theorem 14.21. If V is ﬁnitely generated, then any two bases have the
same size.
Proof. If one basis had more elements than another, then Theorem 14.20
would imply that the ﬁrst basis was linearly dependent, which contradicts
the deﬁnition of a basis. 2
Theorem 14.21 allows us to make the following deﬁnition:
Deﬁnition 14.22. If V is ﬁnitely generated, the common size of any basis
is called the dimension of V , and is denoted dimF (V ).
Note that from the deﬁnitions, we have dimF (V ) = 0 if and only if V is
the trivial vector space (i.e., V = {0V }). We also note that one often refers
to a ﬁnitely generated vector space as a ﬁnite dimensional vector space.
We shall give preference to this terminology from now on.
To summarize the main results in this section up to this point: if V is ﬁnite
dimensional, it has a basis, and any two bases have the same size, which is
called the dimension of V . The next theorem is simple consequences of these
results.
Theorem 14.23.
Suppose that V
is of ﬁnite dimension n, and let
α1, . . . , αn ∈V . The following are equivalent:
(i) α1, . . . , αn are linearly independent.
(ii) α1, . . . , αn span V .
(iii) α1, . . . , αn form a basis for V .
Proof. Let W be the subspace spanned by α1, . . . , αn.
First, let us show that (i) implies (ii). Suppose α1, . . . , αn are linearly
independent. Also, by way of contradiction, suppose that W ⊊V . Choose
β ∈V \ W. Then it follows that α1, . . . , αn, β are linearly independent;
indeed, if we had a relation 0V = a1α1 +· · ·+anαn +bβ, then we must have
b = 0F (otherwise, β ∈W), and by the linear independence of α1, . . . , αn,
all the ai must be zero as well. But then we have a set of n + 1 linearly
independent vectors in V , which is impossible by Theorem 14.20.
Second, let us prove that (ii) implies (i). Let us assume that α1, . . . , αn are
linearly dependent, and prove that W ⊊V . By Theorem 14.19, we can ﬁnd a
basis for W among the αi, and since the αi are linearly dependent, this basis
TEAM LinG

312
Modules and vector spaces
must contain strictly fewer than n elements. Hence, dimF (W) < dimF (V ),
and therefore, W ⊊V .
The theorem now follows from the above arguments, and the fact that,
by deﬁnition, (iii) holds if and only if both (i) and (ii) hold. 2
We next examine the dimension of subspaces of ﬁnite dimensional vector
spaces.
Theorem 14.24. If V is ﬁnite dimensional, and W is a subspace of V ,
then W is also ﬁnite dimensional, and dimF (W) ≤dimF (V ). Moreover,
dimF (W) = dimF (V ) if and only if W = V .
Proof. To see this, suppose dimF (V ) = n, and assume that W is non-trivial.
We shall construct a basis α1, . . . , αm for W, where m ≤n. We can take α1
to be any non-zero vector in W, α2 to be any vector in W not in the subspace
spanned by α1, and so on. More generally, at stage i = 1, 2, . . . , we take αi to
be any element of W not in the subspace spanned by α1, . . . , αi−1. It is easy
to see that at each stage i, the vectors α1, . . . , αi are linearly independent:
if we had a relation a1α1 + · · · ajαj = 0V , where j ≤i and aj ̸= 0F , this
would imply that αj lies in the subspace generated by α1, . . . , αj−1, which
contradicts the deﬁnition of how αj was selected. Because of Theorem 14.20,
this process must halt at some stage m ≤n, and since the process does halt,
it must be the case that α1, . . . , αm span W.
That proves that W is ﬁnite dimensional with dimF (W) ≤dimF (V ). It
remains to show that these dimensions are equal if and only if W = V . Now,
if W = V , then clearly dimF (W) = dimF (V ). Conversely, if dimF (W) =
dimF (V ), then by Theorem 14.23, any basis for W must already span V . 2
Theorem 14.25. If V is ﬁnite dimensional, and W is a subspace of V ,
then the quotient space V/W is also ﬁnite dimensional, and
dimF (V/W) = dimF (V ) −dimF (W).
Proof. Suppose that S is a ﬁnite set of vectors that spans V . Then {α +
W : α ∈S} is a ﬁnite set of vectors that spans V/W.
It follows from
Theorem 14.19 that V/W has a basis, say, α1 + W, . . . , αℓ+ W. Suppose
that β1, . . . , βm is a basis for W. The theorem will follow immediately from
the following:
Claim. The vectors
α1, . . . , αℓ, β1, . . . , βm
(14.2)
form a basis for V .
TEAM LinG

14.5 Vector spaces and dimension
313
To see that these vectors span V , consider any element γ of V . Then since
α1 + W, . . . , αℓ+ W span V/W, we have γ ≡
i aiαi (mod W) for some
a1, . . . , aℓ∈F. If we set β := γ −
i aiαi ∈W, then since β1, . . . , βm span
W, we have β = 
j bjβj for some b1, . . . , bm ∈F, and hence γ = 
i aiαi +

j bjβj. That proves that the vectors (14.2) span V . To prove they are
linearly independent, suppose we have a relation of the form 
i aiαi +

j bjβj = 0V , where a1, . . . , aℓ∈F and b1, . . . , bm ∈F. If any of the ai
were non-zero, this would contradict the assumption that α1+W, . . . , αℓ+W
are linearly independent. So assume that all the ai are zero. If any of the
bj were non-zero, this would contradict the assumption that β1, . . . , βm are
linearly independent. Thus, all the ai and all the bj must be zero, which
proves that the vectors (14.2) are linearly independent. That proves the
claim. 2
Theorem 14.26. If V is of ﬁnite dimension, then any linearly independent
set of elements of V can be extended to form a basis for V .
Proof. This is actually implicit in the proof of the previous theorem. Let
β1, . . . , βm ∈V be linearly independent.
Let W be the subspace of V
spanned by β1, . . . , βm, so that β1, . . . , βm form a basis for W. As in the
proof of the previous theorem, we can choose α1, . . . , αℓ∈V such that
α1 + W, . . . , αℓ+ W form a basis for the quotient space V/W, so that
α1, . . . , αℓ, β1, . . . , βm
form a basis for V . 2
Example 14.23. Suppose that F is ﬁnite, say |F| = q, and that V is ﬁnite
dimensional, say dimF (V ) = n. Then clearly |V | = qn. If W is a subspace
with dimF (W) = m, then |W| = qm, and by Theorem 14.25, dimF (V/W) =
n−m, and hence |V/W| = qn−m. Just viewing V and W as additive groups,
we know that the index of W in V is [V : W] = |V/W| = |V |/|W| = qn−m,
which agrees with the above calculations. 2
We next consider the relation between the notion of dimension and linear
maps.
Theorem 14.27. If V is of ﬁnite dimension n, and V is isomorphic to V ′,
then V ′ is also of ﬁnite dimension n.
Proof. If α1, . . . , αn is a basis for V , then by Theorem 14.18, ρ(α1), . . . , ρ(αn)
is a basis for V ′. 2
TEAM LinG

314
Modules and vector spaces
Theorem 14.28. If ρ : V →V ′ is an F-linear map, and if V and V ′ are
ﬁnite dimensional with dimF (V ) = dimF (V ′), then we have:
ρ is injective if and only if ρ is surjective.
Proof. Let α1, . . . , αn be a basis for V . By Theorem 14.18, we know that
ρ is injective if and only if ρ(α1), . . . , ρ(αn) are linearly independent, and
that ρ is surjective if and only if ρ(α1), . . . , ρ(αn) span V ′. Moreover, by
Theorem 14.23, we know that the vectors ρ(α1), . . . , ρ(αn) are linearly inde-
pendent if and only if they span V ′. The theorem now follows immediately.
2
This last theorem turns out to be extremely useful in a number of set-
tings. Generally, of course, if we have a function f : A →B, injectivity does
not imply surjectivity, nor does surjectivity imply injectivity. If A and B
are ﬁnite sets of equal size, then these implications do indeed hold. Theo-
rem 14.28 gives us another important setting where these implications hold,
with ﬁnite dimensionality playing the role corresponding to ﬁniteness.
Theorem 14.28 may be generalized as follows:
Theorem 14.29. If V is ﬁnite dimensional, and ρ : V →V ′ is an F-linear
map, then img(ρ) is a ﬁnite dimensional vector space, and
dimF (V ) = dimF (img(ρ)) + dimF (ker(ρ)).
Proof. As the reader may verify, this follows immediately from Theo-
rem 14.25, together with Theorems 14.27 and 14.11. 2
Intuitively, one way to think of Theorem 14.29 is as a “law of conservation”
for dimension: any “dimensionality” going into ρ that is not “lost” to the
kernel of ρ must show up in the image of ρ.
Exercise 14.5. Show that if V1, . . . , Vn are ﬁnite dimensional vector spaces,
then V1 × · · · × Vn has dimension n
i=1 dimF (Vi).
Exercise 14.6. Show that if V is a ﬁnite dimensional vector space with
subspaces W1 and W2, such that W1 + W2 = V and W1 ∩W2 = {0V }, then
dimF (V ) = dimF (W1) + dimF (W2).
Exercise 14.7. The theory of dimension for ﬁnitely generated vector spaces
is quite elegant and powerful. There is a theory of dimension (of sorts) for
modules over an arbitrary, non-trivial ring R, but it is much more awkward
and limited. This exercise develops a proof of one aspect of this theory: if
an R-module M has a basis at all, then any two bases have the same size.
TEAM LinG

14.5 Vector spaces and dimension
315
To prove this, we need the fact that any non-trivial ring has a maximal ideal
(this was proved in Exercise 9.30 for countable rings). Let n, m be positive
integers, let α1, . . . , αm be elements of R×n, and let I be an ideal of R.
(a) Show that if α1, . . . , αm span R×n, then every element of I×n can be
expressed as a1α1 + · · · amαm, where a1, . . . , am belong to I.
(b) Show that if m > n and I is a maximal ideal, then there exist
a1, . . . , am ∈R, not all in I, such that a1α1 + · · · amαm ∈I×n.
(c) From (a) and (b), deduce that if m > n, then α1, . . . , αm cannot be
a basis for R×n.
(d) From (c), conclude that any two bases for a given R-module M must
have the same size.
TEAM LinG

15
Matrices
In this chapter, we discuss basic deﬁnitions and results concerning matrices.
We shall start out with a very general point of view, discussing matrices
whose entries lie in an arbitrary ring R. Then we shall specialize to the case
where the entries lie in a ﬁeld F, where much more can be said.
One of the main goals of this chapter is to discuss “Gaussian elimination,”
which is an algorithm that allows us to eﬃciently compute bases for the
image and kernel of an F-linear map.
In discussing the complexity of algorithms for matrices over a ring R, we
shall treat a ring R as an “abstract data type,” so that the running times of
algorithms will be stated in terms of the number of arithmetic operations in
R. If R is a ﬁnite ring, such as Zm, we can immediately translate this into a
running time on a RAM (in later chapters, we will discuss other ﬁnite rings
and eﬃcient algorithms for doing arithmetic in them).
If R is, say, the ﬁeld of rational numbers, a complete running time analysis
would require an additional analysis of the sizes of the numbers that appear
in the execution of the algorithm. We shall not attempt such an analysis
here—however, we note that all the algorithms discussed in this chapter do
in fact run in polynomial time when R = Q, assuming we represent rational
numbers as fractions in lowest terms. Another possible approach for dealing
with rational numbers is to use ﬂoating point approximations. While this
approach eliminates the size problem, it creates many new problems because
of round-oﬀerrors. We shall not address any of these issues here.
15.1 Basic deﬁnitions and properties
Throughout this section, R denotes a ring.
For positive integers m and n, an m × n matrix A over a ring R is a
316
TEAM LinG

15.1 Basic deﬁnitions and properties
317
rectangular array
A =





a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
am1
am2
· · ·
amn




,
where each entry aij in the array is an element of R; the element aij is called
the (i, j) entry of A, which we may denote by A(i, j). For i = 1, . . . , m, the
ith row of A is
(ai1, . . . , ain),
which we may denote by A(i), and for j = 1, . . . , n, the jth column of A is





a1j
a2j
...
amj




,
which we may denote by A(·, j). We regard a row of A as a 1 × n matrix,
and a column of A as an m × 1 matrix.
The set of all m × n matrices over R is denoted by Rm×n. Elements of
R1×n are called row vectors (of dimension n) and elements of Rm×1
are called column vectors (of dimension m).
Elements of Rn×n are
called square matrices (of dimension n). We do not make a distinction
between R1×n and R×n; that is, we view standard n-tuples as row vectors.
Also, where there can be no confusion, we may interpret an element of R1×1
simply as an element of R.
We can deﬁne the familiar operations of scalar multiplication, addition,
and multiplication on matrices:
• If A ∈Rm×n and c ∈R, then cA is the m × n matrix whose (i, j)
entry is cA(i, j).
• If A, B ∈Rm×n, then A + B is the m × n matrix whose (i, j) entry
is A(i, j) + B(i, j).
• If A ∈Rm×n and B ∈Rn×p, then AB is the m × p matrix whose
(i, k) entry is
n

j=1
A(i, j)B(j, k).
TEAM LinG

318
Matrices
We can also deﬁne the diﬀerence A −B := A + (−1R)B of matrices of the
same dimension, which is the same as taking the diﬀerence of corresponding
entries. These operations satisfy the usual properties:
Theorem 15.1. If A, B, C ∈Rm×n, U, V ∈Rn×p, Z ∈Rp×q, and c, d ∈R,
then
(i) c(dA) = (cd)A = d(cA),
(ii) (A + B) + C = A + (B + C),
(iii) A + B = B + A,
(iv) c(A + B) = cA + cB,
(v) (c + d)A = cA + dA,
(vi) (A + B)U = AU + BU,
(vii) A(U + V ) = AU + AV ,
(viii) c(AU) = (cA)U = A(cU),
(ix) A(UZ) = (AU)Z.
Proof. All of these are trivial, except the last one which requires just a bit
of computation to show that the (i, ℓ) entry of both A(UZ) and (AU)Z is
(verify)
n

j=1
p

k=1
A(i, j)U(j, k)Z(k, ℓ). 2
Note that while matrix addition is commutative, matrix multiplication in
general is not.
Some simple but useful facts to keep in mind are the following:
• If A ∈Rm×n and B ∈Rn×p, then the kth column of AB is equal to
Av, where v = B(·, k); also, the ith row of AB is equal to wB, where
w = A(i).
• If A ∈Rm×n and u = (u1, . . . , um) ∈R1×m, then
uA =
m

i=1
uiA(i).
In words: uA is a linear combination of the rows of A, with coeﬃcients
taken from the corresponding entries of u.
• If A ∈Rm×n and
v =



v1
...
vn


∈Rn×1,
TEAM LinG

15.1 Basic deﬁnitions and properties
319
then
Av =
n

j=1
vjA(·, j).
In words: Av is a linear combination of the columns of A, with coef-
ﬁcients taken from the corresponding entries of v.
If A ∈Rm×n, the transpose of A, denoted by A⊤, is deﬁned to be the
n × m matrix whose (j, i) entry is A(i, j).
Theorem 15.2. If A, B ∈Rm×n, C ∈Rn×p, and c ∈R, then
(i) (A + B)⊤= A⊤+ B ⊤,
(ii) (cA)⊤= cA⊤,
(iii) (A⊤)⊤= A,
(iv) (AC)⊤= C ⊤A⊤.
Proof. Exercise. 2
An n × n matrix A is called a diagonal matrix if A(i, j) = 0R for i ̸= j,
which is to say that the entries oﬀthe “main diagonal” of A are all zero. A
scalar matrix is a diagonal matrix whose diagonal entries are all the same.
The scalar matrix I, where all the entries on the main diagonal are 1R, is
called the n × n identity matrix. It is easy to see that if A is an n × n
matrix, then AI = IA = A. More generally, if B is an n × m matrix, then
IB = B, and if C is an m × n matrix, then CI = C.
If Ai is an ni × ni+1 matrix, for i = 1, . . . , k, then by associativity of
matrix multiplication (part (ix) of Theorem 15.1), we may write the product
matrix A1 · · · Ak, which is an n1 × nk+1 matrix, without any ambiguity. For
an n × n matrix A, and a positive integer k, we write Ak to denote the
product A · · · A, where there are k terms in the product. Note that A1 = A.
We may extend this notation to k = 0, deﬁning A0 to be the n × n identity
matrix.
One may readily verify the usual rules of exponent arithmetic: for non-
negative integers k1, k2, we have
(Ak1)k2 = Ak1k2 and Ak1Ak2 = Ak1+k2.
It is easy also to see that part (iv) of Theorem 15.2 implies that for all
non-negative integers k, we have
(Ak)
⊤= (A
⊤)k.
TEAM LinG

320
Matrices
Algorithmic issues
For computational purposes, matrices are represented in the obvious way as
arrays of elements of R. As remarked at the beginning of this chapter, we
shall treat R as an “abstract data type,” and not worry about how elements
of R are actually represented; in discussing the complexity of algorithms,
we shall simply count “operations in R,” by which we mean additions, sub-
tractions, multiplications; we shall sometimes also include equality testing
and computing multiplicative inverses as “operations in R.”
In any real
implementation, there will be other costs, such as incrementing counters,
and so on, which we may safely ignore, as long as their number is at most
proportional to the number of operations in R.
The following statements are easy to verify:
• We can multiply an m×n matrix times a scalar using mn operations
in R.
• We can add two m × n matrices using mn operations in R.
• We can multiply an m×n matrix and an n×p matrix using O(mnp)
operations in R.
It is also easy to see that given an m × m matrix A, and a non-negative
integer e, we can adapt the repeated squaring algorithm discussed in §3.4
so as to compute Ae using O(len(e)) multiplications of m×m matrices, and
hence O(len(e)m3) operations in R.
15.2 Matrices and linear maps
Let R be a ring.
For positive integers m and n, we may naturally view R1×m and R1×n
as R-modules. If A is an m × n matrix over R, then the map σ that sends
v ∈R1×m to vA ∈R1×n is easily seen to be an R-linear map. Evidently,
σ is injective if and only if the rows of A are linearly independent, and σ
is surjective if and only if the rows of A span R1×n. Likewise, the map τ
that sends w ∈Rn×1 to Aw ∈Rm×1 is also an R-linear map. Again, τ is
injective if and only if the columns of A are linearly independent, and τ is
surjective if and only if the columns of A span Rm×1.
Thus, the matrix A deﬁnes in a natural way two diﬀerent linear maps,
one deﬁned in terms of multiplying a row vector on the right by A, and the
other in terms multiplying a column vector on the left by A. With either
of these interpretations as a linear map, matrix multiplication has a natural
interpretation as function composition. Let A ∈Rm×n and B ∈Rn×p, and
consider the product matrix C = AB. Let σA, σB, σC be the maps deﬁned
TEAM LinG

15.2 Matrices and linear maps
321
by multiplication on the right by A, B, C, and let τA, τB, τC be the maps
deﬁned by multiplication on the left by A, B, C. Then it easily follows from
the associativity of matrix multiplication that σC = σB◦σA and τC = τA◦τB.
We have seen how matrix/vector multiplication deﬁnes a linear map. Con-
versely, we shall now see that the action of any R-linear map can be viewed
as a matrix/vector multiplication, provided the R-modules involved have
bases (which will always be the case for ﬁnite dimensional vector spaces).
Let M be an R-module, and suppose that A = (α1, . . . , αm), with m >
0, is a basis for M. In this setting, the ordering of the basis elements is
important, and so we refer to A as an ordered basis.
Now, A deﬁnes
a canonical R-module isomorphism ϵ that sends (a1, . . . , am) ∈R1×m to
a1α1 +· · ·+amαm ∈M. Thus, elements of M can be represented concretely
as elements of R1×m; however, this representation depends on the choice A
of the ordered basis. The vector ϵ−1(α) is called the coordinate vector of
α (with respect to A).
Let N be an R-module, and suppose B = (β1, . . . , βn), with n > 0, is an
ordered basis for N. Just as in the previous paragraph, B deﬁnes a canonical
R-module isomorphism δ : R1×n →N.
Now let ρ : M →N be an arbitrary R-linear map. For any α ∈M, if
α = ϵ(a1, . . . , am), then because ρ is R-linear, we have
ρ(α) =
m

i=1
ρ(aiαi) =
m

i=1
aiρ(αi).
Thus, the action of ρ on M is completely determined by its action on the
αi.
Let us now deﬁne an m × n matrix T whose ith row, for i = 1, . . . , m, is
deﬁned to be δ−1(ρ(αi)), that is, the coordinate vector of ρ(αi) with respect
to the ordered basis B. With T deﬁned in this way, then for any α ∈M we
have
δ−1(ρ(α)) = ϵ−1(α)T.
In words: if we multiply the coordinate vector of α on the right by T, we
get the coordinate vector of ρ(α).
A special case of the above is when M = R1×m and N = R1×n, and A
and B are the standard bases for M and N (i.e., for i = 1, . . . , m, the ith
vector of A has a 1 in position i and is zero everywhere else, and similarly
for B). In this case, ρ(v) = vT for all v ∈R1×m.
To summarize, we see that an R-linear map ρ from M to N, together with
particular ordered bases for M and N, uniquely determine a matrix T such
TEAM LinG

322
Matrices
that the action of multiplication on the right by T implements the action
of ρ with respect to the given ordered bases. There may be many ordered
bases for M and N to choose from, and diﬀerent choices will in general
lead to diﬀerent matrices. In any case, from a computational perspective,
the matrix T gives us an eﬃcient way to compute the map ρ, assuming
elements of M and N are represented as coordinate vectors with respect to
the given ordered bases.
Of course, if one prefers, by simply transposing everything, one can equally
well represent the action of ρ in terms of the action of multiplication of a
column vector on the left by a matrix.
Example 15.1. Consider again the ring E = R[X]/(f), where f ∈R[X] is
monic of degree ℓ, and suppose that ℓ> 0 (see Examples 9.34, 9.43, 14.3,
and 14.22). Let f = f0 + f1X + · · · fℓ−1Xℓ−1 + Xℓ, where f0, . . . , fℓ−1 ∈R.
Consider the element η = [X]f ∈E. Let ρ : E →E be the η-multiplication
map, that is, the map that sends α ∈E to ηα ∈E. This is an R-linear
map, and the matrix T ∈Rℓ×ℓthat represents this map with respect to the
ordered basis 1, η, η2, . . . , ηℓ−1 for E over R is readily seen to be
T =







0
1
0
· · ·
0
0
0
1
· · ·
0
...
0
0
0
· · ·
1
−f0
−f1
−f2
· · ·
−fℓ−1







,
where for i = 1, . . . , ℓ−1, the ith row of T contains a 1 in position i + 1,
and is zero everywhere else. This matrix is called the companion matrix
of f. 2
Exercise 15.1. Let F be a ﬁnite ﬁeld, and let A be a non-zero m×n matrix
over F. Suppose one chooses a vector v ∈F 1×m at random. Show that the
probability that vA is the zero vector is at most 1/|F|.
Exercise 15.2. Design and analyze a probabilistic algorithm that takes as
input matrices A, B, C ∈Zm×m
p
, where p is a prime. The algorithm should
run in time O(m2 len(p)2) and should output either “yes” or “no” so that
the following holds:
• if C = AB, then the algorithm should always output “yes”;
• if C ̸= AB, then the algorithm should output “no” with probability
at least 0.999.
TEAM LinG

15.3 The inverse of a matrix
323
15.3 The inverse of a matrix
Let R be a ring.
For a square matrix A ∈Rn×n, we call a matrix X ∈Rn×n an inverse of
A if XA = AX = I, where I is the n × n identity matrix.
It is easy to see that if A has an inverse, then the inverse is unique: if X
and Y were inverses, then multiplying the equation I = AY on the left by
X, we obtain X = X(AY ) = (XA)Y = IY = Y .
Because the inverse of A is uniquely determined, we denote it by A−1.
If A has an inverse, we say that A is invertible, or non-singular. If A
is not invertible, it is sometimes called singular. We will use the terms
“invertible” and “not invertible.” Observe that A is the inverse of A−1; that
is, (A−1)−1 = A.
If A and B are invertible n × n matrices, then so is their product: in fact,
it is easy to see that (AB)−1 = B−1A−1 (verify). It follows that if A is an
invertible matrix, and k is a non-negative integer, then Ak is invertible with
inverse (A−1)k, which we also denote by A−k.
It is also easy to see that A is invertible if and only if the transposed matrix
A⊤is invertible, in which case (A⊤)−1 = (A−1)⊤. Indeed, AX = I = XA
holds if and only if X ⊤A⊤= I = A⊤X ⊤
The following theorem connects invertibility to linear maps.
Theorem 15.3. Let A ∈Rn×n, and let ρ : R1×n →R1×n be the R-linear
map that sends v ∈R1×n to vA. Then A is invertible if and only if ρ is
bijective.
Proof. Suppose A is invertible, and let X ∈Rn×n be its inverse. The map ρ
is surjective, since for any w ∈R1×n, w = wI = wXA = ρ(wX). The map
ρ is injective, since if ρ(v) = 01×n, then v = vI = vAX = ρ(v)X = 01×n.
Suppose ρ is bijective, so that it is an R-module isomorphism. The inverse
map ρ−1 is also an R-module isomorphism. Let X be the matrix representing
ρ−1 with respect to the standard basis for R1×n, so that for w ∈R1×n, we
have wX = ρ−1(w). Since ρ ◦ρ−1 = ρ−1 ◦ρ = the identity map, it follows
that XA = AX = I. 2
We also have:
Theorem 15.4. Let A ∈Rn×n. The following are equivalent:
(i) A is invertible;
(ii) the rows of A form a basis for R1×n;
(iii) the columns of A form a basis for Rn×1.
TEAM LinG

324
Matrices
Proof. The equivalence of (i) and (ii) follows from the previous theorem,
and the fact that the map ρ in that theorem is bijective if and only if the
rows of A form a basis for R1×n. The equivalence of (i) and (iii) follows by
considering the transpose of A. 2
Exercise 15.3. Let R be a ring, and let A be a square matrix over R. Let
us call X a left inverse of A if XA = I, and let us call Y a right inverse
of A if AY = I.
(a) Show that if A has both a left inverse X and a right inverse Y , then
X = Y and hence A is invertible.
(b) Assume that R is a ﬁeld. Show that if A has either a left inverse or
a right inverse, then A is invertible.
Note that part (b) of the previous exercise holds for arbitrary rings, but
the proof of this is non-trivial, and requires the development of the theory
of determinants, which we do not cover in this text.
Exercise 15.4. Show that if A and B are two square matrices over a ﬁeld
such that their product AB is invertible, then both A and B themselves
must be invertible.
Exercise 15.5. Show that if A is a square matrix over an arbitrary ring,
and Ak is invertible for some k > 0, then A is invertible.
15.4 Gaussian elimination
Throughout this section, F denotes a ﬁeld.
A matrix B ∈F m×n is said to be in reduced row echelon form if there
exists a sequence of integers (p1, . . . , pr), with 0 ≤r ≤m and 1 ≤p1 < p2 <
· · · < pr ≤n, such that the following holds:
• for i = 1, . . . , r, all of the entries in row i of B to the left of entry
(i, pi) are zero (i.e., B(i, j) = 0 for j = 1, . . . , pi −1);
• for i = 1, . . . , r, all of the entries in column pi of B above entry (i, pi)
are zero (i.e., B(i′, pi) = 0 for i′ = 1, . . . , i −1);
• for i = 1, . . . , r, we have B(i, pi) = 1;
• all entries in rows r + 1, . . . , m of B are zero (i.e., B(i, j) = 0 for
i = r + 1, . . . , m and j = 1, . . . , n).
It is easy to see that if B is in reduced row echelon form, the sequence
(p1, . . . , pr) above is uniquely determined, and we call it the pivot sequence
of B. Several further remarks are in order:
TEAM LinG

15.4 Gaussian elimination
325
• All of the entries of B are completely determined by the pivot se-
quence, except for the entries (i, j) with 1 ≤i ≤r and j > i with
j /∈{pi+1, . . . , pr}, which may be arbitrary.
• If B is an n × n matrix in reduced row echelon form whose pivot
sequence is of length n, then B must be the n × n identity matrix.
• We allow for an empty pivot sequence (i.e., r = 0), which will be the
case precisely when B = 0m×n.
Example 15.2. The following 4 × 6 matrix B over the rational numbers is
in reduced row echelon form:
B =




0
1
−2
0
0
3
0
0
0
1
0
2
0
0
0
0
1
−4
0
0
0
0
0
0



.
The pivot sequence of B is (2, 4, 5). Notice that the ﬁrst three rows of B
are linearly independent, that columns 2, 4, and 5 are linearly independent,
and that all of other columns of B are linear combinations of columns 2, 4,
and 5. Indeed, if we truncate the pivot columns to their ﬁrst three rows, we
get the 3 × 3 identity matrix. 2
Generalizing the previous example, if a matrix is in reduced row echelon
form, it is easy to deduce the following properties, which turn out to be
quite useful:
Theorem 15.5. If B is a matrix in reduced row echelon form with pivot
sequence (p1, . . . , pr), then
(i) rows 1, 2, . . . , r of B are linearly independent;
(ii) columns p1, . . . , pr of B are linearly independent, and all other
columns of B can be expressed as linear combinations of columns
p1, . . . , pr.
Proof. Exercise—just look at the matrix! 2
Gaussian elimination is an algorithm that transforms an arbitrary m×n
matrix A into a m×n matrix B, where B is a matrix in reduced row echelon
form obtained from A by a sequence of elementary row operations.
There are three types of elementary row operations:
Type I: swap two rows,
Type II: multiply a row by a non-zero scalar,
Type III: add a scalar multiple of one row to a diﬀerent row.
TEAM LinG

326
Matrices
The application of any speciﬁc elementary row operation to an m × n
matrix C can be aﬀected by multiplying C on the left by a suitable m × m
matrix M. Indeed, the matrix M corresponding to a particular elementary
row operation is simply the matrix obtained by applying the same elemen-
tary row operation to the m × m identity matrix. It is easy to see that for
any elementary row operation, the corresponding matrix M is invertible.
We now describe the basic version of Gaussian elimination. The input is
an m × n matrix A.
1.
B ←A, r ←0
2.
for j ←1 to n do
3.
ℓ←0, i ←r
4.
while ℓ= 0 and i ≤m do
5.
i ←i + 1
6.
if B(i, j) ̸= 0 then ℓ←i
7.
if ℓ̸= 0 then
8.
r ←r + 1
9.
swap rows B(r) and B(ℓ)
10.
B(r) ←B(r, j)−1B(r)
11.
for i ←1 to m do
12.
if i ̸= r then
13.
B(i) ←B(i) −B(i, j)B(r)
14.
output B
The algorithm works as follows.
First, it makes a copy B of A (this
is not necessary if the original matrix A is not needed afterwards). The
algorithm proceeds column by column, starting with the left-most column,
so that after processing column j, the ﬁrst j columns of B are in reduced row
echelon form, and the current value of r represents the length of the pivot
sequence. To process column j, in steps 3–6 the algorithm ﬁrst searches
for a non-zero element among B(r + 1, j), . . . , B(m, j); if none is found,
then the ﬁrst j + 1 columns of B are already in reduced row echelon form.
Otherwise, one of these non-zero elements is selected as the pivot element
(the choice is arbitrary), which is then used in steps 8–13 to bring column j
into the required form. After incrementing r, the pivot element is brought
into position (r, j), using a Type I operation in step 9. Then the entry (r, j)
is set to 1, using a Type II operation in step 10. Finally, all the entries above
and below entry (r, j) are set to 0, using Type III operations in steps 11–13.
Note that because columns 1, . . . , j −1 of B were already in reduced row
echelon form, none of these operations changes any values in these columns.
As for the complexity of the algorithm, it is easy to see that it performs
TEAM LinG

15.4 Gaussian elimination
327
O(mn) elementary row operations, each of which takes O(n) operations in
F, so a total of O(mn2) operations in F.
Example 15.3. Consider the execution of the Gaussian elimination algo-
rithm on input
A =


[0]
[1]
[1]
[2]
[1]
[2]
[2]
[2]
[0]

∈Z3×3
3
.
After copying A into B, the algorithm transforms B as follows:


[0]
[1]
[1]
[2]
[1]
[2]
[2]
[2]
[0]

B(1)↔B(2)
−−−−−−−→


[2]
[1]
[2]
[0]
[1]
[1]
[2]
[2]
[0]

B(1)←[2]B(1)
−−−−−−−−→


[1]
[2]
[1]
[0]
[1]
[1]
[2]
[2]
[0]


B(3)←B(3)−[2]B(1)
−−−−−−−−−−−−→


[1]
[2]
[1]
[0]
[1]
[1]
[0]
[1]
[1]

B(1)←B(1)−[2]B(2)
−−−−−−−−−−−−→


[1]
[0]
[2]
[0]
[1]
[1]
[0]
[1]
[1]


B(3)←B(3)−B(2)
−−−−−−−−−−−→


[1]
[0]
[2]
[0]
[1]
[1]
[0]
[0]
[0]


2
Suppose the Gaussian elimination algorithm performs a total of t elemen-
tary row operations. Then as discussed above, the application of the eth
elementary row operation, for e = 1, . . . , t, amounts to multiplying the cur-
rent value of the matrix B on the left by a particular invertible m×m matrix
Me. Therefore, the ﬁnal, output value of B satisﬁes the equation
B = MA where M = MtMt−1 · · · M1.
Since the product of invertible matrices is also invertible, we see that M
itself is invertible.
Although the algorithm as presented does not compute the matrix M, it
can be easily modiﬁed to do so. The resulting algorithm, which we call ex-
tended Gaussian elimination, is the same as plain Gaussian elimination,
except that we initialize the matrix M to be the m×m identity matrix, and
we add the following steps:
• Just before step 9, we add the step: swap rows M(r) and M(ℓ).
• Just before step 10, we add the step: M(r) ←B(r, j)−1M(r).
• Just before step 13, we add the step: M(i) ←M(i) −B(i, j)M(r).
TEAM LinG

328
Matrices
At the end of the algorithm we output M in addition to B.
So we simply perform the same elementary row operations on M that we
perform on B. The reader may verify that the above algorithm is correct,
and that it uses O(mn(m + n)) operations in F.
Example 15.4. Continuing with Example 15.3, the execution of the ex-
tended Gaussian elimination algorithm initializes M to the identity matrix,
and then transforms M as follows:


[1]
[0]
[0]
[0]
[1]
[0]
[0]
[0]
[1]

M(1)↔M(2)
−−−−−−−−→


[0]
[1]
[0]
[1]
[0]
[0]
[0]
[0]
[1]

M(1)←[2]M(1)
−−−−−−−−−→


[0]
[2]
[0]
[1]
[0]
[0]
[0]
[0]
[1]


M(3)←M(3)−[2]M(1)
−−−−−−−−−−−−−−→


[0]
[2]
[0]
[1]
[0]
[0]
[0]
[2]
[1]

M(1)←M(1)−[2]M(2)
−−−−−−−−−−−−−−→


[1]
[2]
[0]
[1]
[0]
[0]
[0]
[2]
[1]


M(3)←M(3)−M(2)
−−−−−−−−−−−−→


[1]
[2]
[0]
[1]
[0]
[0]
[2]
[2]
[1]


2
Exercise 15.6. For each type of elementary row operation, describe the
matrix M which corresponds to it, as well as M −1.
Exercise 15.7. Given a matrix B ∈F m×n in reduced row echelon form,
show how to compute its pivot sequence using O(n) operations in F.
Exercise 15.8. In §4.4, we saw how to speed up matrix multiplication over
Z using the Chinese remainder theorem.
In this exercise, you are to do
the same, but for performing Gaussian elimination over Zp, where p is a
large prime. Suppose you are given an m × m matrix A over Zp, where
len(p) = Θ(m). Straightforward application of Gaussian elimination would
require O(m3) operations in Zp, each of which takes time O(m2), leading to
a total running time of O(m5). Show how to use the techniques of §4.4 to
reduce the running time of Gaussian elimination to O(m4).
15.5 Applications of Gaussian elimination
Throughout this section, A is an arbitrary m × n matrix over a ﬁeld F, and
MA = B, where M is an invertible m × m matrix, and B is in reduced
row echelon form with pivot sequence (p1, . . . , pr). This is precisely the in-
formation produced by the extended Gaussian elimination algorithm, given
TEAM LinG

15.5 Applications of Gaussian elimination
329
A as input (the pivot sequence can easily be “read” directly from B —see
Exercise 15.7).
Let V := F 1×m, W := F 1×n, and ρ : V →W be the F-linear map that
sends v ∈V to vA ∈W.
Computing the image and kernel
Consider ﬁrst the row space of A, that is, the vector space spanned by the
rows of A, or equivalently, the image of ρ.
We claim that the row space of A is the same as the row space of B. To
see this, note that for any v ∈V , since B = MA, we have vB = v(MA) =
(vM)A, and so the row space of B is contained in the row space of A. For the
other containment, note that since M is invertible, we can write A = M−1B,
and apply the same argument.
Further, note that row space of B, and hence that of A, clearly has di-
mension r. Indeed, as stated in Theorem 15.5, the ﬁrst r rows of B form a
basis for the row space of B.
Consider next the kernel of ρ, or what we might call the row null space
of A.
We claim that the last m −r rows of M form a basis for ker(ρ).
Clearly, just from the fact that MA = B and the fact that the last m −r
rows of B are zero, it follows that the last m −r rows of M are contained
in ker(ρ).
Furthermore, as M is invertible, its rows form a basis for V
(see Theorem 15.4), and so in particular, they are linearly independent. It
therefore suﬃces to show that the last m −r rows of M span the entire
kernel. Now, suppose there were a vector v ∈ker(ρ) outside the subspace
spanned by the last m −r rows of M. As the rows of M span V , we may
write v = a1M(1) + · · · + amM(m), where ai ̸= 0 for some i = 1, . . . , r.
Setting ˜v := (a1, . . . , am), we see that v = ˜vM, and so
ρ(v) = vA = (˜vM)A = ˜v(MA) = ˜vB,
and from the fact that the ﬁrst r rows of B are linearly independent and
the last m −r rows of B are zero, we see that ˜vB is not the zero vector
(and because ˜v has a non-zero entry in one its ﬁrst r positions). We have
derived a contradiction, and hence may conclude that the last m −r rows
of M span ker(ρ).
Finally, note that if m = n, then A is invertible if and only if its row space
has dimension m, which holds if and only if r = m, and in the latter case,
B will be the identity matrix, and hence M is the inverse of A.
Let us summarize the above discussion:
TEAM LinG

330
Matrices
• The ﬁrst r rows of B form a basis for the row space of A (i.e., the
image of ρ).
• The last m −r rows of M form a basis for the row null space of A
(i.e., the kernel of ρ).
• If m = n, then A is invertible (i.e., ρ is an isomorphism) if and
only if r = m, in which case M is the inverse of A (i.e., the matrix
representing ρ−1).
So we see that from the output of the extended Gaussian elimination
algorithm, we can simply “read oﬀ” bases for both the image and the kernel,
as well as the inverse (if it exists), of a linear map represented as a matrix
with respect to some ordered bases. Also note that this procedure provides
a “constructive” version of Theorem 14.29.
Example 15.5. Continuing with Examples 15.3 and 15.4, we see that the
vectors ([1], [0], [2]) and ([0], [1], [1]) form a basis for the row space of A, while
the vector ([2], [2], [1]) is a basis for the row null space of A. 2
Solving linear systems of equations
Suppose that in addition to the matrix A, we are given w ∈W, and want
to ﬁnd a solution v (or perhaps describe all solutions v), to the equation
vA = w.
(15.1)
Equivalently, we can phrase the problem as ﬁnding an element (or describing
all elements) of the set ρ−1(w).
Now, if there exists a solution at all, say v ∈V , then since ρ(v) = ρ(˜v)
if and only if v ≡˜v (mod ker(ρ)), it follows that the set of all solutions to
(15.1) is equal to the coset v + ker(ρ). Thus, given a basis for ker(ρ) and
any solution v to (15.1), we have a complete and concise description of the
set of solutions to (15.1).
As we have discussed above, the last m −r rows of M give us a basis for
ker(ρ), so it suﬃces to determine if w ∈img(ρ), and if so, determine a single
pre-image v of w.
Also as we discussed, img(ρ), that is, the row space of A, is equal to the
row space of B, and because of the special form of B, we can quickly and
easily determine if the given w is in the row space of B, as follows. By
deﬁnition, w is in the row space of B iﬀthere exists a vector ¯v ∈V such
that ¯vB = w. We may as well assume that all but the ﬁrst r entries of ¯v
are zero. Moreover, ¯vB = w implies that for i = 1, . . . , r, the ith entry if ¯v
is equal to the pith entry of w. Thus, the vector ¯v, if it exists, is completely
TEAM LinG

15.5 Applications of Gaussian elimination
331
determined by the entries of w at positions p1, . . . , pr. We can construct ¯v
satisfying these conditions, and then test if ¯vB = w. If not, then we may
conclude that (15.1) has no solutions; otherwise, setting v := ¯vM, we see
that vA = (¯vM)A = ¯v(MA) = ¯vB = w, and so v is a solution to (15.1).
One easily veriﬁes that if we implement the above procedure as an algo-
rithm, the work done in addition to running the extended Gaussian elimi-
nation algorithm amounts to O(m(n + m)) operations in F.
A special case of the above procedure is when m = n and A is invertible,
in which case (15.1) has a unique solution, namely, v := wM, since in this
case, M = A−1.
The rank of a matrix
Deﬁne the row rank of A to be the dimension of its row space, which is
dimF (img(ρ)), and deﬁne the column rank of A to be the dimension of its
column space, that is, the space spanned by the columns of A.
Now, the column space A may not be the same as the column space of
B, but from the relation B = MA, and the fact that M is invertible, it
easily follows that these two subspaces are isomorphic (via the isomorphism
that sends v to Mv), and hence have the same dimension. Moreover, by
Theorem 15.5, the column rank of B is r, which is the same as the row rank
of A.
So we may conclude: The column rank and row rank of A are the same.
Because of this, we deﬁne the rank of a matrix to be the common value
of its row and column rank.
The orthogonal complement of a subspace
So as to give equal treatment to rows and columns, one can also deﬁne
the column null space of A to be the kernel of the linear map deﬁned
by multiplication on the left by A. By applying the results above to the
transpose of A, we see that the column null space of A has dimension n −r,
where r is the rank of A.
Let U ⊆W denote the row space of A, and let ¯U ⊆W denote the set of
all vectors ¯u ∈W whose transpose ¯u
⊤belong to the column null space of
A. Now, U is a subspace of W of dimension r and ¯U is a subspace of W of
dimension n −r.
Moreover, if U ∩¯U = {0V }, then by Theorem 14.13 we have an isomor-
phism of U × ¯U with U + ¯U, and since U × ¯U has dimension n, it must be the
TEAM LinG

332
Matrices
case that U + ¯U = W. It follows that every element of W can be expressed
uniquely as u + ¯u, where u ∈U and ¯u ∈¯U.
Now, all of the conclusions in the previous paragraph hinged on the as-
sumption that U ∩¯U = {0V }. The space ¯U consists precisely of all vectors
¯u ∈W which are “orthogonal” to all vectors u ∈U, in the sense that the
“inner product” u¯u⊤is zero. For this reason, ¯U is sometimes called the
“orthogonal complement of U.” The condition U ∩¯U = {0V } is equivalent
to saying that U contains no non-zero “self-orthogonal vectors” u such that
uu⊤= 0F . If F is the ﬁeld of real numbers, then of course there are no
non-zero self-orthogonal vectors, since uu⊤is the sum of the squares of the
entries of u. However, for other ﬁelds, there may very well be non-zero self-
orthogonal vectors. As an example, if F = Z2, then any vector u with an
even number of 1-entries is self orthogonal.
So we see that while much of the theory of vector spaces and matrices car-
ries over without change from familiar ground ﬁelds, like the real numbers,
to arbitrary ground ﬁelds F, not everything does. In particular, the usual
decomposition of a vector space into a subspace and its orthogonal comple-
ment breaks down, as does any other procedure that relies on properties
speciﬁc to “inner product spaces.”
For the following three exercises, as above, A is an arbitrary m×n matrix
over a ﬁeld F, and MA = B, where M is an invertible m × m matrix, and
B is in reduced row echelon form.
Exercise 15.9. Show that the column null space of A is the same as the
column null space of B.
Exercise 15.10. Show how to compute a basis for the column null space of
A using O(r(n −r)) operations in F, given A and B.
Exercise 15.11. Show that the matrix B is uniquely determined by A;
more precisely, show that if M ′A = B′, where M′ is an invertible m × m
matrix, and B′ is in reduced row echelon form, then B′ = B.
In the following two exercises, the theory of determinants could be used;
however, they can all be solved directly, without too much diﬃculty, using
just the ideas developed so far in the text.
Exercise 15.12. Let p be a prime. A matrix A ∈Zm×m is called invertible
modulo p if and only if there exists a matrix X ∈Zm×m such that AX ≡
XA ≡I (mod p), where I is the m × m integer identity matrix. Here, two
matrices are considered congruent with respect to a given modulus if and
TEAM LinG

15.5 Applications of Gaussian elimination
333
only if their corresponding entries are congruent. Show that A is invertible
modulo p if and only if
• A is invertible over Q, and
• the entries of A−1 lie in Q(p) (see Example 9.23).
Exercise 15.13. You are given a matrix A ∈Zm×m and a prime p such
that A is invertible modulo p. Suppose that you are also given w ∈Z1×m.
(a) Show how to eﬃciently compute a vector v ∈Z1×m such that vA =
w (mod p), and that v is uniquely determined modulo p.
(b) Given a vector v as in part (a), along with an integer e ≥1, show
how to eﬃciently compute ˆv ∈Z1×m such that ˆvA = w (mod pe), and
that ˆv is uniquely determined modulo pe. Hint: mimic the “lifting”
procedure discussed in §13.3.2.
(c) Using parts (a) and (b), design and analyze an eﬃcient algorithm
that takes the matrix A and the prime p as input, together with a
bound H on the absolute value of the numerator and denominator of
the entries of the vector v′ that is the unique (rational) solution to
the equation v′A = w. Your algorithm should run in time polynomial
in the length of H, the length of p, and the sum of the lengths of the
entries of A and w. Hint: use rational reconstruction, but be sure to
fully justify its application.
Note that in the previous exercise, one can use the theory of determinants
to derive good bounds, in terms of the lengths of the entries of A and w, on
the size of the least prime p such that A is invertible modulo p (assuming
A is invertible over the rationals), and the length of the numerator and
denominator of the entries of rational solution v′ to the equation v′A = w.
The interested reader who is familiar with the basic theory of determinants
is encouraged to establish such bounds.
The next two exercises illustrate how Gaussian elimination can be
adapted, in certain cases, to work in rings that are not necessarily ﬁelds.
Let R be an arbitrary ring. A matrix B ∈Rm×n is said to be in row eche-
lon form if there exists a pivot sequence (p1, . . . , pr), with 0 ≤r ≤m and
1 ≤p1 < p2 < · · · < pr ≤n, such that the following holds:
• for i = 1, . . . , r, all of the entries in row i of B to the left of entry
(i, pi) are zero;
• for i = 1, . . . , r, we have B(i, pi) ̸= 0;
• all entries in rows r + 1, . . . , m of B are zero.
TEAM LinG

334
Matrices
Exercise 15.14. Let R be the ring Zpe, where p is prime and e > 1. Let
π := [p] ∈R. The goal of this exercise is to develop an eﬃcient algorithm
for the following problem: given a matrix A ∈Rm×n, with m > n, ﬁnd a
vector v ∈R1×m such that vA = 01×n but v ̸∈πR1×m.
(a) Show how to modify the extended Gaussian elimination algorithm
to solve the following problem: given a matrix A ∈Rm×n, compute
M ∈Rm×m and B ∈Rm×n, such that MA = B, M is invertible,
and B is in row echelon form. Your algorithm should run in time
O(mn(m + n)e2 len(p)2). Assume that the input includes the values
p and e. Hint: when choosing a pivot element, select one divisible
by a minimal power of π; as in ordinary Gaussian elimination, your
algorithm should only use elementary row operations to transform
the input matrix.
(b) Using the fact that the matrix M computed in part (a) is invertible,
argue that none of its rows belong to πR1×m.
(c) Argue that if m > n and the matrix B computed in part (a) has
pivot sequence (p1, . . . , pr), then m −r > 0 and if v is any one of the
last m −r rows of M, then vA = 01×n.
(d) Give an example that shows that the ﬁrst r rows of B need not be
linearly independent and that the last m−r rows of M need not span
the kernel of the R-linear map that sends w ∈R1×m to wA ∈R1×n.
Exercise 15.15. Let R be the ring Zℓ, where ℓ> 1 is an integer. You are
given a matrix A ∈Rm×n. Show how to eﬃciently compute M ∈Rm×m
and B ∈Rm×n such that MA = B, M is invertible, and B is in row echelon
form. Your algorithm should run in time O(mn(m + n) len(ℓ)2). Hint: to
zero-out entries, you should use “rotations”—for integers a, b, d, s, t with
d = gcd(a, b) ̸= 0 and as + bt = d,
and for row indices r, i, a rotation simultaneously updates rows r and i of a
matrix C as follows:
(C(r), C(i)) ←(sC(r) + tC(i), −b
dC(r) + a
dC(i));
observe that if C(r, j) = [a]ℓand C(i, j) = [b]ℓbefore applying the rotation,
then C(r, j) = [d]ℓand C(i, j) = [0]ℓafter the rotation.
15.6 Notes
While a trivial application of the deﬁning formulas yields a simple algorithm
for multiplying two m×m matrices over a ring R that uses O(m3) operations
TEAM LinG

15.6 Notes
335
in R, this algorithm is not the best, asymptotically speaking. The currently
fastest algorithm for this problem, due to Coppersmith and Winograd [28],
uses O(mω) operations in R, where ω < 2.376. We note, however, that the
good old O(m3) algorithm is still the only one used in almost any practical
setting.
TEAM LinG

16
Subexponential-time discrete logarithms and
factoring
This chapter presents subexponential-time algorithms for computing dis-
crete logarithms and for factoring. These algorithms are based on a common
technique, which makes essential use of the notion of a smooth number.
16.1 Smooth numbers
If y is a non-negative real number, and m is a positive integer, then we say
that m is y-smooth if all prime divisors of m are at most y.
For 0 ≤y ≤x, let us deﬁne Ψ(y, x) to be the number of y-smooth integers
up to x. The following theorem gives us a lower bound on Ψ(y, x), which will
be crucial in the analysis of our discrete logarithm and factoring algorithms.
Theorem 16.1. Let y be a function of x such that
y
log x →∞and u := log x
log y →∞
as x →∞. Then
Ψ(y, x) ≥x · exp[(−1 + o(1))u log log x].
Proof. Let us write u = ⌊u⌋+δ, where 0 ≤δ < 1. Let us split the primes up
to y into two sets: the set V “very small” primes that are at most yδ/2, and
the other primes W that are greater than yδ/2 but at most y. To simplify
matters, let us also include the integer 1 in the set V .
By Bertrand’s postulate (Theorem 5.7), there exists a constant C > 0
such that |W| ≥Cy/ log y for suﬃciently large y. By the assumption that
y/ log x →∞as x →∞, it follows that |W| ≥2⌊u⌋for suﬃciently large x.
To derive the lower bound, we shall count those integers that can be built
up by multiplying together ⌊u⌋distinct elements of W, together with one
336
TEAM LinG

16.2 An algorithm for discrete logarithms
337
element of V . These products are clearly distinct, y-smooth numbers, and
each is bounded by x, since each is at most y⌊u⌋yδ = yu = x.
If S denotes the set of all of these products, then for x suﬃciently large,
we have
|S| =
|W|
⌊u⌋

· |V |
= |W|(|W| −1) · · · (|W| −⌊u⌋+ 1)
⌊u⌋!
· |V |
≥
|W|
2u
⌊u⌋
· |V |
≥

Cy
2u log y
⌊u⌋
· |V |
=

Cy
2 log x
u−δ
· |V |.
Taking logarithms, we have
log |S| ≥(u −δ)(log y −log log x + log(C/2)) + log |V |
= log x −u log log x + (log |V | −δ log y) +
O(u + log log x).
(16.1)
To prove the theorem, it suﬃces to show that
log |S| ≥log x −(1 + o(1))u log log x.
Under our assumption that u →∞, the term O(u + log log x) in (16.1) is
o(u log log x), and so it will suﬃce to show that the term log |V | −δ log y
is also o(u log log x). But by Chebyshev’s theorem (Theorem 5.1), for some
positive constant D, we have
Dyδ/ log y ≤|V | ≤yδ,
and taking logarithms, and again using the fact that u →∞, we have
log |V | −δ log y = O(log log y) = o(u log log x). 2
16.2 An algorithm for discrete logarithms
We now present a probabilistic, subexponential-time algorithm for comput-
ing discrete logarithms. The input to the algorithm is p, q, γ, α, where p and
q are primes, with q | (p −1), γ is an element of Z∗
p generating a subgroup
G of Z∗
p of order q, and α ∈G.
TEAM LinG

338
Subexponential-time discrete logarithms and factoring
We shall make the simplifying assumption that q2 ∤(p −1), which is
equivalent to saying that q ∤m := (p−1)/q. Although not strictly necessary,
this assumption simpliﬁes the design and analysis of the algorithm, and
moreover, for cryptographic applications, this assumption is almost always
satisﬁed. (Exercises 16.1–16.3 below explore how this assumption may be
lifted, as well as other generalizations.)
At a high level, the main goal of our discrete logarithm algorithm is to
ﬁnd a random representation of 1 with respect to γ and α—as discussed
in Exercise 11.12, this allows us to compute logγ α (with high probability).
More precisely, our main goal is to compute integers r and s in a probabilistic
fashion, such that γrαs = 1 and [s]q is uniformly distributed over Zq. Having
accomplished this, then with probability 1−1/q, we shall have s ̸≡0 (mod q),
which allows us to compute logγ α as −rs−1 mod q.
Let G′ be the subgroup of Z∗
p of order m. Our assumption that q ∤m
implies that G ∩G′ = {1}, since the multiplicative order of any element in
the intersection must divide both q and m, and so the only possibility is
that the multiplicative order is 1. Therefore, the map ρ : G × G′ →Z∗
p that
sends (β, δ) to βδ is injective (Theorem 8.28), and since |Z∗
p| = qm, it must
be surjective as well.
We shall use this fact in the following way: if β is chosen uniformly
at random from G, and δ is chosen uniformly at random from G′ (and
independent of β), then βδ is uniformly distributed over Z∗
p. Furthermore,
since G′ is the image of the q-power map on Z∗
p, we may generate a random
δ ∈G′ simply by choosing ˆδ ∈Z∗
p at random, and setting δ := ˆδq.
The discrete logarithm algorithm uses a “smoothness parameter” y, whose
choice will be discussed below when we analyze the running time of the algo-
rithm; for now, we only assume that y < p. Let p1, . . . , pk be an enumeration
of the primes up to y. Let πi := [pi]p ∈Z∗
p for i = 1, . . . , k.
The algorithm has two stages.
In the ﬁrst stage, we ﬁnd relations of the form
γriαsiδi = πei1
1
. . . πeik
k ,
(16.2)
for integers ri, si, ei1, . . . , eik, and δi ∈G′, and i = 1, . . . , k + 1.
We obtain one such relation by a randomized search, as follows: we choose
ri, si ∈{0, . . . , q −1} at random, as well as ˆδi ∈Z∗
p at random; we then
compute δi := ˆδq
i , βi := γriαsi, and mi := rep(βiδi). Now, the value βi
is uniformly distributed over G, while δi is uniformly distributed over G′;
therefore, the product βiδi is uniformly distributed over Z∗
p, and hence mi
TEAM LinG

16.2 An algorithm for discrete logarithms
339
is uniformly distributed over {1, . . . , p −1}. Next, we simply try to factor
mi by trial division, trying all the primes p1, . . . , pk up to y. If we are lucky,
we completely factor mi in this way, obtaining a factorization
mi = pei1
1
· · · peik
k ,
for some exponents ei1, . . . , eik, and we get the relation (16.2). If we are
unlucky, then we simply try (and try again) until we are lucky.
For i = 1, . . . , k + 1, let vi := (ei1, . . . , eik) ∈Z×k, and let ¯vi denote the
image of vi in Z×k
q
(i.e., ¯vi := ([ei1]q, . . . , [eik]q)). Since Z×k
q
is a vector space
over the ﬁeld Zq of dimension k, the vectors ¯v1, . . . , ¯vk+1 must be linearly de-
pendent. The second stage of the algorithm uses Gaussian elimination over
Zq (see §15.4) to ﬁnd a linear dependence among the vectors ¯v1, . . . , ¯vk+1,
that is, to ﬁnd integers c1, . . . , ck+1 ∈{0, . . . , q −1}, not all zero, such that
(e1, . . . , ek) := c1v1 + · · · ck+1vk+1 ∈qZ×k.
Raising each equation (16.2) to the power ci, and multiplying them all
together, we obtain
γrαsδ = πe1
1 · · · πek
k ,
where
r :=
k+1

i=1
ciri, s :=
k+1

i=1
cisi, and δ :=
k+1

i=1
δci
i .
Now, δ ∈G′, and since each ei is a multiple of q, we also have πei
i
∈G′
for i = 1, . . . , k. It follows that γrαs ∈G′. But since γrαs ∈G as well, and
G ∩G′ = {1}, it follows that γrαs = 1. If we are lucky (and we will be with
overwhelming probability, as we discuss below), we will have s ̸≡0 (mod q),
in which case, we can compute s′ := s−1 mod q, obtaining
α = γ−rs′,
and hence −rs′ mod q is the discrete logarithm of α to the base γ. If we
are very unlucky, we will have s ≡0 (mod q), at which point the algorithm
simply quits, reporting “failure.”
The entire algorithm, called Algorithm SEDL, is presented in Fig. 16.1.
As already argued above, if Algorithm SEDL does not output “failure,”
then its output is indeed the discrete logarithm of α to the base γ. There
remain three questions to answer:
1. What is the expected running time of Algorithm SEDL?
TEAM LinG

340
Subexponential-time discrete logarithms and factoring
i ←0
repeat
i ←i + 1
repeat
choose ri, si ∈{0, . . . , q −1} at random
choose ˆδi ∈Z∗
p at random
βi ←γriαsi,
δi ←ˆδq
i ,
mi ←rep(βiδi)
test if mi is y-smooth (trial division)
until mi = pei1
1
· · · peik
k
for some integers ei1, . . . , eik
until i = k + 1
set vi ←(ei1, . . . , eik) ∈Z×k for i = 1, . . . , k + 1
apply Gaussian elimination over Zq to ﬁnd integers c1, . . . , ck+1 ∈
{0, . . . , q −1}, not all zero, such that
c1v1 + · · · + ck+1vk+1 ∈qZ×k.
r ←k+1
i=1 ciri,
s ←k+1
i=1 cisi
if s ≡0 (mod q) then
output “failure”
else
output −rs−1 mod q
Fig. 16.1. Algorithm SEDL
2. How should the smoothness parameter y be chosen so as to minimize
the expected running time?
3. What is the probability that Algorithm SEDL outputs “failure”?
Let us address these questions in turn. As for the expected running time,
let σ be the probability that a random element of {1, . . . , p−1} is y-smooth.
Then the expected number of attempts needed to produce a single relation
is σ−1, and so the expected number of attempts to produce k + 1 relations
is (k + 1)σ−1. In each attempt, we perform trial division using p1, . . . , pk,
along with a few other minor computations, leading to a total expected
running time in stage 1 of k2σ−1 · len(p)O(1). The running time in stage
2 is dominated by that of the Gaussian elimination step, which takes time
k3 · len(p)O(1). Thus, if T is the total running time of the algorithm, then
we have
E[T] ≤(k2σ−1 + k3) · len(p)O(1).
(16.3)
TEAM LinG

16.2 An algorithm for discrete logarithms
341
Let us assume for the moment that
y = exp[(log p)λ+o(1)]
(16.4)
for some constant λ with 0 < λ < 1. Our ﬁnal choice of y will indeed satisfy
this assumption. Consider the probability σ. We have
σ = Ψ(y, p −1)/(p −1) = Ψ(y, p)/(p −1) ≥Ψ(y, p)/p,
where for the second equality we use the assumption that y < p, so p is not
y-smooth. With our assumption (16.4), we may apply Theorem 16.1 (with
the given value of y and x := p), obtaining
σ ≥exp[(−1 + o(1))(log p/ log y) log log p].
By Chebyshev’s theorem (Theorem 5.1), we know that k = Θ(y/ log y), and
so log k = (1 + o(1)) log y. Moreover, assumption (16.4) implies that the
factor len(p)O(1) in (16.3) is of the form exp[o(min(log y, log p/ log y))], and
so we have
E[T] ≤exp[(1 + o(1)) max{(log p/ log y) log log p + 2 log y, 3 log y}]. (16.5)
Let us ﬁnd the value of y that minimizes the right-hand side of (16.5),
ignoring the “o(1)” terms. Let µ := log y, A := log p log log p, S1 := A/µ +
2µ, and S2 := 3µ. We want to ﬁnd µ that minimizes max{S1, S2}. Using
a little calculus, one sees that S1 is minimized at µ = (A/2)1/2. With this
choice of µ, we have S1 = (2
√
2)A1/2 and S2 = (3/
√
2)A1/2 < S1. Thus,
choosing
y = exp[(1/
√
2)(log p log log p)1/2],
we obtain
E[T] ≤exp[(2
√
2 + o(1))(log p log log p)1/2].
That takes care of the ﬁrst two questions, although strictly speaking, we
have only obtained an upper bound for the expected running time, and we
have not shown that the choice of y is actually optimal, but we shall never-
theless content ourselves (for now) with these results. Finally, we deal with
the third question, on the probability that the algorithm outputs “failure.”
Lemma 16.2. The probability that the algorithm outputs “failure” is 1/q.
Proof. Consider the values ri, si, and βi generated in the inner loop in stage
1. It is easy to see that, as random variables, the values si and βi are inde-
pendent, since conditioned on any ﬁxed choice of si, the value ri is uniformly
distributed over {0, . . . , q −1}, and hence βi is uniformly distributed over
TEAM LinG

342
Subexponential-time discrete logarithms and factoring
G. Turning this around, we see that conditioned on any ﬁxed choice of βi,
the value si is uniformly distributed over {0, . . . , q −1}.
So now let us condition on any ﬁxed choice of values βi and δi, for
i = 1, . . . , k + 1, as determined at the end of stage 1 of the algorithm.
By the remarks in the previous paragraph, we see that in this condi-
tional probability distribution, the variables si are mutually independent
and uniformly distributed over {0, . . . , q −1}, and moreover, the behav-
ior of the algorithm is completely determined, and in particular, the values
c1, . . . , ck+1 are ﬁxed. Therefore, in this conditional probability distribution,
the probability that the algorithm outputs failure is just the probability that

i sici ≡0 (mod q), which is 1/q, since not all the ci are zero modulo q.
Since this equality holds for every choice of βi and δi, the lemma follows. 2
Let us summarize the above discussion in the following theorem.
Theorem 16.3. With the smoothness parameter set as
y := exp[(1/
√
2)(log p log log p)1/2],
the expected running time of Algorithm SEDL is
exp[(2
√
2 + o(1))(log p log log p)1/2].
The probability that Algorithm SEDL outputs “failure” is 1/q.
In the description and analysis of Algorithm SEDL, we have assumed that
the primes p1, . . . , pk were pre-computed. Of course, we can construct this
list of primes using, for example, the sieve of Eratosthenes (see §5.4), and
the running time of this pre-computation will be dominated by the running
time of Algorithm SEDL.
In the analysis of Algorithm SEDL, we relied crucially on the fact that
in generating a relation, each candidate element γriαsiδi was uniformly dis-
tributed over Z∗
p. If we simply left out the δi, then the candidate element
would be uniformly distributed over the subgroup G, and Theorem 16.1
simply would not apply.
Although the algorithm might anyway work as
expected, we would not be able to prove this.
Exercise 16.1. Using the result of Exercise 15.14, show how to modify
Algorithm SEDL to work in the case where p −1 = qem, e > 1, q ∤m, γ
generates the subgroup G of Z∗
p of order qe, and α ∈G. Your algorithm
should compute logγ α with roughly the same expected running time and
success probability as Algorithm SEDL.
TEAM LinG

16.2 An algorithm for discrete logarithms
343
Exercise 16.2. Using the algorithm of the previous exercise as a subroutine,
design and analyze an algorithm for the following problem. The input is
p, q, γ, α, where p is a prime, q is a prime dividing p −1, γ generates the
subgroup G of Z∗
p of order q, and α ∈G; note that we may have q2 | (p −1).
The output is logγ α. Your algorithm should always succeed in computing
this discrete logarithm, and its expected running time should be bounded by
a constant times the expected running time of the algorithm of the previous
exercise.
Exercise 16.3. Using the result of Exercise 15.15, show how to modify
Algorithm SEDL to solve the following problem: given a prime p, a generator
γ for Z∗
p, and an element α ∈Z∗
p, compute logγ α. Your algorithm should
work without knowledge of the factorization of p −1; its expected running
time should be roughly the same as that of Algorithm SEDL, but its success
probability may be lower. In addition, explain how the success probability
may be signiﬁcantly increased at almost no cost by collecting a few extra
relations.
Exercise 16.4. Let n = pq, where p and q are distinct, large primes. Let e
be a prime, with e < n and e ∤(p −1)(q −1). Let x be a positive integer,
with x < n. Suppose you are given n (but not its factorization!) along with
e and x. In addition, you are given access to two “oracles,” which you may
invoke as often as you like.
• The ﬁrst oracle is a “challenge oracle”: each invocation of the oracle
produces a “challenge” a ∈{1, . . . , x} — distributed uniformly and
independently of all other challenges.
• The second oracle is a “solution oracle”: you invoke this oracle with
the index of a previous challenge oracle; if the corresponding challenge
was a, the solution oracle returns the eth root of a modulo n; that
is, the solution oracle returns b ∈{1, . . . , n −1} such that be ≡
a (mod n)—note that b always exists and is uniquely determined.
Let us say that you “win” if you are able to compute the eth root modulo
n of any challenge, but without invoking the solution oracle with the cor-
responding index of the challenge (otherwise, winning would be trivial, of
course).
(a) Design a probabilistic algorithm that wins the above game, using an
expected number of
exp[(c + o(1))(log x log log x)1/2] · len(n)O(1)
steps, for some constant c, where a “step” is either a computation step
TEAM LinG

344
Subexponential-time discrete logarithms and factoring
or an oracle invocation (either challenge or solution). Hint: Gaussian
elimination over the ﬁeld Ze.
(b) Suppose invocations of the challenge oracle are “cheap,” while invo-
cations of the solution oracle are relatively “expensive.” How would
you modify your strategy in part (a)?
Exercise 16.4 has implications in cryptography.
A popular way of im-
plementing a public-key primitive known as a “digital signature” works as
follows: to digitally sign a message M (which may be an arbitrarily long
bit string), ﬁrst apply a “hash function” or “message digest” H to M, ob-
taining an integer a in some ﬁxed range {1, . . . , x}, and then compute the
signature of M as the eth root b of a modulo n. Anyone can verify that such
a signature b is correct by checking that be ≡H(M) (mod n); however, it
would appear to be diﬃcult to “forge” a signature without knowing the fac-
torization of n. Indeed, one can prove the security of this signature scheme
by assuming that it is hard to compute the eth root of a random number
modulo n, and by making the heuristic assumption that H is a random
function (see §16.5). However, for this proof to work, the value of x must
be close to n; otherwise, if x is signiﬁcantly smaller than n, as the result of
this exercise, one can break the signature scheme at a cost that is roughly
the same as the cost of factoring numbers around the size of x, rather than
the size of n.
16.3 An algorithm for factoring integers
We now present a probabilistic, subexponential-time algorithm for factor-
ing integers. The algorithm uses techniques very similar to those used in
Algorithm SEDL in §16.2.
Let n > 1 be the integer we want to factor. We make a few simplifying
assumptions. First, we assume that n is odd—this is not a real restriction,
since we can always pull out any factors of 2 in a pre-processing step. Second,
we assume that n is not a perfect power, that is, not of the form ab for
integers a > 1 and b > 1—this is also not a real restriction, since we can
always partially factor n using the algorithm in §10.5 if n is a perfect power.
Third, we assume that n is not prime — this may be eﬃciently checked
using, say, the Miller–Rabin test (see §10.3). Fourth, we assume that n is
not divisible by any primes up to a “smoothness parameter” y — we can
ensure this using trial division, and it will be clear that the running time of
this pre-computation is dominated by that of the algorithm itself.
TEAM LinG

16.3 An algorithm for factoring integers
345
With these assumptions, the prime factorization of n is of the form
n = qf1
1 · · · qfw
w ,
where the qi are distinct, odd primes, all greater than y, the fi are positive
integers, and w > 1.
The main goal of our factoring algorithm is to ﬁnd a random square root
of 1 in Zn. Let
θ : Zqf1
1 × · · · × Zqfw
w →Zn
be the ring isomorphism of the Chinese remainder theorem. The square
roots of 1 in Zn are precisely those elements of the form θ(±1, . . . , ±1), and
if β is a random square root of 1, then with probability 1 −2−w+1 ≥1/2, it
will be of the form β = θ(β1, . . . , βw), where the βi are neither all 1 nor all
−1 (i.e., β ̸= ±1). If this happens, then β −1 = θ(β1 −1, . . . , βw −1), and
so we see that some, but not all, of the values βi −1 will be zero. The value
of gcd(rep(β −1), n) is precisely the product of the prime powers qfi
i
such
that βi −1 = 0, and hence this gcd will yield a non-trivial factorization of
n, unless β = ±1.
Let p1, . . . , pk be the primes up to the smoothness parameter y mentioned
above. Let πi := [pi]n ∈Z∗
n for i = 1, . . . , k.
We ﬁrst describe a simpliﬁed version of the algorithm, after which we
modify the algorithm slightly to deal with a technical problem. Like Algo-
rithm SEDL, this algorithm proceeds in two stages. In the ﬁrst stage, we
ﬁnd relations of the form
α2
i = πei1
1
· · · πeik
k ,
(16.6)
for αi ∈Z∗
n, and i = 1, . . . , k + 1.
We can obtain such a relation by randomized search, as follows: we select
αi ∈Z∗
n at random, square it, and try to factor mi := rep(α2
i ) by trial
division, trying all the primes p1, . . . , pk up to y. If we are lucky, we obtain
a factorization
mi = pei1
1
· · · peik
k ,
for some exponents ei1, . . . , eik, yielding the relation (16.6); if not, we just
keep trying.
For i = 1, . . . , k + 1, let vi := (ei1, . . . , eik) ∈Z×k, and let ¯vi denote the
image of vi in Z×k
2
(i.e., ¯vi := ([ei1]2, . . . , [eik]2)). Since Z×k
2
is a vector space
over the ﬁeld Z2 of dimension k, the vectors ¯v1, . . . , ¯vk+1 must be linearly
dependent. The second stage of the algorithm uses Gaussian elimination
TEAM LinG

346
Subexponential-time discrete logarithms and factoring
over Z2 to ﬁnd a linear dependence among the vectors ¯v1, . . . , ¯vk+1, that is,
to ﬁnd integers c1, . . . , ck+1 ∈{0, 1}, not all zero, such that
(e1, . . . , ek) := c1v1 + · · · ck+1vk+1 ∈2Z×k.
Raising each equation (16.6) to the power ci, and multiplying them all to-
gether, we obtain
α2 = πe1
1 · · · πek
k ,
where
α :=
k+1

i=1
αci
i .
Since each ei is even, we can compute
β := πe1/2
1
· · · πek/2
k
α−1,
and we see that β is a square root of 1 in Zn. A more careful analysis (see
below) shows that in fact, β is uniformly distributed over all square roots of
1, and hence, with probability at least 1/2, if we compute gcd(rep(β −1), n),
we get a non-trivial factor of n.
That is the basic idea of the algorithm. There is, however, a technical
problem. Namely, in the method outlined above for generating a relation,
we attempt to factor mi := rep(α2
i ). Thus, the running time of the algorithm
will depend in a crucial way on the probability that a random square modulo
n is y-smooth. Unfortunately for us, Theorem 16.1 does not say anything
about this situation — it only applies to the situation where a number is
chosen at random from an interval [1, x]. There are (at least) three diﬀerent
ways to address this problem:
1. Ignore it, and just assume that the bounds in Theorem 16.1 apply to
random squares modulo n (taking x := n in the theorem).
2. Prove a version of Theorem 16.1 that applies to random squares mod-
ulo n.
3. Modify the factoring algorithm, so that Theorem 16.1 applies.
The ﬁrst choice, while not completely unreasonable, is not very satisfying
mathematically. It turns out that the second choice is a indeed a viable
option (i.e., the theorem is true and is not so diﬃcult to prove), but we opt
for the third choice, as it is somewhat easier to carry out, and illustrates a
probabilistic technique that is more generally useful.
TEAM LinG

16.3 An algorithm for factoring integers
347
So here is how we modify the basic algorithm.
Instead of generating
relations of the form (16.6), we generate relations of the form
α2
i δ = πei1
1
· · · πeik
k ,
(16.7)
for δ ∈Z∗
n, αi ∈Z∗
n, and i = 1, . . . , k + 2. Note that the value δ is the same
in all relations.
We generate these relations as follows. For the very ﬁrst relation (i.e.,
i = 1), we repeatedly choose α1 and δ in Z∗
n at random, until rep(α2
1δ) is
y-smooth. Then, after having found the ﬁrst relation, we ﬁnd subsequent
relations (i.e., for i > 1) by repeatedly choosing αi in Z∗
n at random until
rep(α2
i δ) is y-smooth, where δ is the same value that was used in the ﬁrst
relation. Now, Theorem 16.1 will apply directly to determine the success
probability of each attempt to generate the ﬁrst relation.
Having found
this relation, the value α2
1δ will be uniformly distributed over all y-smooth
elements of Z∗
n (i.e., elements whose integer representations are y-smooth).
Consider the various cosets of (Z∗
n)2 in Z∗
n.
Intuitively, it is much more
likely that a random y-smooth element of Z∗
n lies in a coset that contains
many y-smooth elements, rather than a coset with very few, and indeed,
it is reasonably likely that the fraction of y-smooth elements in the coset
containing δ is not much less than the overall fraction of y-smooth elements
in Z∗
n. Therefore, for i > 1, each attempt to ﬁnd a relation should succeed
with reasonably high probability.
This intuitive argument will be made
rigorous in the analysis to follow.
The second stage is then modiﬁed as follows. For i = 1, . . . , k + 2, let
vi := (ei1, . . . , eik, 1) ∈Z×(k+1), and let ¯vi denote the image of vi in Z×(k+1)
2
.
Since Z×(k+1)
2
is a vector space over the ﬁeld Z2 of dimension k+1, the vectors
¯v1, . . . , ¯vk+2 must be linearly dependent. Therefore, we use Gaussian elimi-
nation over Z2 to ﬁnd a linear dependence among the vectors ¯v1, . . . , ¯vk+2,
that is, to ﬁnd integers c1, . . . , ck+2 ∈{0, 1}, not all zero, such that
(e1, . . . , ek+1) := c1v1 + · · · + ck+2vk+2 ∈2Z×(k+1).
Raising each equation (16.7) to the power ci, and multiplying them all to-
gether, we obtain
α2δek+1 = πe1
1 · · · πek
k ,
where
α :=
k+2

i=1
αci
i .
TEAM LinG

348
Subexponential-time discrete logarithms and factoring
i ←0
repeat
i ←i + 1
repeat
choose αi ∈Z∗
n at random
if i = 1 then choose δ ∈Z∗
n at random
mi ←rep(α2
i δ)
test if mi is y-smooth (trial division)
until mi = pei1
1
· · · peik
k
for some integers ei1, . . . , eik
until i = k + 2
set vi ←(ei1, . . . , eik, 1) ∈Z×(k+1) for i = 1, . . . , k + 2
apply Gaussian elimination over Z2 to ﬁnd integers c1, . . . , ck+2 ∈
{0, 1}, not all zero, such that
(e1, . . . , ek+1) := c1v1 + · · · + ck+2vk+2 ∈2Z×(k+1).
α ←k+2
i=1 αci
i ,
β ←πe1/2
1
· · · πek/2
k
δ−ek+1/2α−1
if β = ±1 then
output “failure”
else
output gcd(rep(β −1), n)
Fig. 16.2. Algorithm SEF
Since each ei is even, we can compute
β := πe1/2
1
· · · πek/2
k
δ−ek+1/2α−1,
which is a square root of 1 in Zn.
The entire algorithm, called Algorithm SEF, is presented in Fig. 16.2.
Now the analysis. From the discussion above, it is clear that Algorithm
SEF either outputs “failure,” or outputs a non-trivial factor of n. So we have
the same three questions to answer as we did in the analysis of Algorithm
SEDL:
1. What is the expected running time of Algorithm SEF?
2. How should the smoothness parameter y be chosen so as to minimize
the expected running time?
3. What is the probability that Algorithm SEF outputs “failure”?
To answer the ﬁrst question, let σ denote the probability that (the
TEAM LinG

16.3 An algorithm for factoring integers
349
canonical representative of) a random element of Z∗
n is y-smooth.
For
i = 1, . . . , k + 2, let Xi denote the number iterations of the inner loop
of stage 1 in the ith iteration of the main loop; that is, Xi is the number of
attempts made in ﬁnding the ith relation.
Lemma 16.4. For i = 1, . . . , k + 2, we have E[Xi] = σ−1.
Proof. We ﬁrst compute E[X1].
As δ is chosen uniformly from Z∗
n and
independent of α1, at each attempt to ﬁnd a relation, α2
1δ is uniformly
distributed over Z∗
n, and hence the probability that the attempt succeeds is
precisely σ. This means E[X1] = σ−1.
We next compute E[Xi] for i > 1. To this end, let us denote the cosets
of (Z∗
n)2 by Z∗
n as C1, . . . , Ct. As it happens, t = 2w, but this fact plays no
role in the analysis. For j = 1, . . . , t, let σj denote the probability that a
random element of Cj is y-smooth, and let τj denote the probability that
the ﬁnal value of δ belongs to Cj.
We claim that for j = 1, . . . , t, we have τj = σjσ−1t−1. To see this, note
that each coset Cj has the same number of elements, namely, |Z∗
n|t−1, and
so the number of y-smooth elements in Cj is equal to σj|Z∗
n|t−1. Moreover,
the ﬁnal value of α2
1δ is equally likely to be any one of the y-smooth numbers
in Z∗
n, of which there are σ|Z∗
n|, and hence
τj = σj|Z∗
n|t−1
σ|Z∗n|
= σjσ−1t−1,
which proves the claim.
Now, for a ﬁxed value of δ and a random choice of αi ∈Z∗
n, one sees
that α2
i δ is uniformly distributed over the coset containing δ. Therefore, for
j = 1, . . . , t, we have
E[Xi | δ ∈Cj] = σ−1
j .
It follows that
E[Xi] =
t

j=1
E[Xi | δ ∈Cj] · P[δ ∈Cj]
=
t

j=1
σ−1
j
· τj =
t

j=1
σ−1
j
· σjσ−1t−1 = σ−1,
which proves the lemma. 2
So in stage 1, the expected number of attempts made in generating a
single relation is σ−1, each such attempt takes time k · len(n)O(1), and we
have to generate k + 2 relations, leading to a total expected running time in
TEAM LinG

350
Subexponential-time discrete logarithms and factoring
stage 1 of σ−1k2 · len(n)O(1). Stage 2 is dominated by the cost of Gaussian
elimination, which takes time k3 · len(n)O(1). Thus, if T is the total running
time of the algorithm, we have
E[T] ≤(σ−1k2 + k3) · len(n)O(1).
By our assumption that n is not divisible by any primes up to y, all y-
smooth integers up to n −1 are in fact relatively prime to n. Therefore, the
number of y-smooth elements of Z∗
n is equal to Ψ(y, n−1), and since n itself
is not y-smooth, this is equal to Ψ(y, n). From this, it follows that
σ = Ψ(y, n)/|Z∗
n| ≥Ψ(y, n)/n.
The rest of the running time analysis is essentially the same as in the anal-
ysis of Algorithm SEDL; that is, assuming y = exp[(log n)λ+o(1)] for some
constant 0 < λ < 1, we obtain
E[T] ≤exp[(1 + o(1)) max{(log n/ log y) log log n + 2 log y, 3 log y}]. (16.8)
Setting y = exp[(1/
√
2)(log n log log n)1/2], we obtain
E[T] ≤exp[(2
√
2 + o(1))(log n log log n)1/2].
That basically takes care of the ﬁrst two questions. As for the third, we
have:
Lemma 16.5. The probability that the algorithm outputs “failure” is
2−w+1 ≤1/2.
Proof. Let ρ be the squaring map on Z∗
n. By part (b) of Exercise 8.22, if
we condition on any ﬁxed values of δ, α2
1, . . . , α2
k+2, as determined at the
end of stage 1 of the algorithm, then in the resulting conditional probability
distribution, the values α1, . . . , αk+2 are mutually independent, with each
αi uniformly distributed over ρ−1({α2
i }). Moreover, these ﬁxed values of
δ, α2
1, . . . , α2
k+2 completely determine the behavior of the algorithm, and in
particular, the values of c1, . . . , ck+2, α2, and e1, . . . , ek+1. By part (d) of
Exercise 8.22, it follows that α is uniformly distributed over ρ−1({α2}), and
also that β is uniformly distributed over ρ−1({1}). Thus, in this conditional
probability distribution, β is a random square root of 1, and so β = ±1 with
probability 2−w+1. Since this holds conditioned on all relevant choices of
δ, α2
1, . . . , α2
k+2, it also holds unconditionally. Finally, since we are assuming
that w > 1, we have 2−w+1 ≤1/2. 2
Let us summarize the above discussion in the following theorem.
TEAM LinG

16.3 An algorithm for factoring integers
351
Theorem 16.6. With the smoothness parameter set as
y := exp[(1/
√
2)(log n log log n)1/2],
the expected running time of Algorithm SEF is
exp[(2
√
2 + o(1))(log n log log n)1/2].
The probability that Algorithm SEF outputs “failure” is at most 1/2.
Exercise 16.5. It is perhaps a bit depressing that after all that work, Al-
gorithm SEF only succeeds (in the worst case) with probability 1/2. Of
course, to reduce the failure probability, we can simply repeat the entire
computation—with ℓrepetitions, the failure probability drops to 2−ℓ. How-
ever, there is a better way to reduce the failure probability. Suppose that in
stage 1, instead of collecting k + 2 relations, we collect k + 1 + ℓrelations,
where ℓ≥1 is an integer parameter.
(a) Show that in stage 2, we can use Gaussian elimination over Z2 to ﬁnd
integer vectors
c(j) = (c(j)
1 , . . . , c(j)
k+1+ℓ) ∈{0, 1}×(k+1+ℓ) (j = 1, . . . , ℓ)
such that
– over the ﬁeld Z2, the images of the vectors c(1), . . . , c(ℓ) in
Z×(k+1+ℓ)
2
are linearly independent, and
– for j = 1, . . . , ℓ, we have
c(j)
1 v1 + · · · + c(j)
k+1+ℓvk+1+ℓ∈2Z×(k+2).
(b) Show that given vectors c(1), . . . , c(ℓ) as in part (a), if for j = 1, . . . , ℓ,
we set
(e(j)
1 , . . . , e(j)
k+1) ←c(j)
1 v1 + · · · + c(j)
k+1+ℓvk+1+ℓ,
α(j) ←
k+1+ℓ

i=1
α
c(j)
i
i
,
and
β(j) ←πe(j)
1 /2
1
· · · π
e(j)
k /2
k
δ−e(j)
k+1/2(α(j))−1,
then the values β(1), . . . , β(ℓ) are independent and uniformly dis-
tributed over the set of all square roots of 1 in Zn, and hence at
least one of gcd(rep(β(j) −1), n) splits n with probability at least
1 −2−ℓ.
TEAM LinG

352
Subexponential-time discrete logarithms and factoring
So, for example, if we set ℓ= 20, then the failure probability is reduced to
less than one in a million, while the increase in running time over Algorithm
SEF will hardly be noticeable.
16.4 Practical improvements
Our presentation and analysis of algorithms for discrete logarithms and fac-
toring were geared towards simplicity and mathematical rigor. However, if
one really wants to compute discrete logarithms or factor numbers, then a
number of important practical improvements should be considered. In this
section, we brieﬂy sketch some of these improvements, focusing our atten-
tion on algorithms for factoring numbers (although some of the techniques
apply to discrete logarithms as well).
16.4.1 Better smoothness density estimates
From an algorithmic point of view, the simplest way to improve the running
times of both Algorithms SEDL and SEF is to use a more accurate smooth-
ness density estimate, which dictates a diﬀerent choice of the smoothness
bound y in those algorithms, speeding them up signiﬁcantly.
While our
Theorem 16.1 is a valid lower bound on the density of smooth numbers, it
is not “tight,” in the sense that the actual density of smooth numbers is
somewhat higher. We quote from the literature the following result:
Theorem 16.7. Let y be a function of x such that for some ϵ > 0, we have
y = Ω((log x)1+ϵ) and u := log x
log y →∞
as x →∞. Then
Ψ(y, x) = x · exp[(−1 + o(1))u log u].
Proof. See §16.5. 2
Let us apply this result to the analysis of Algorithm SEF. Assume that
y = exp[(log n)1/2+o(1)]—our choice of y will in fact be of this form. With
this assumption, we have log log y = (1/2 + o(1)) log log n, and using Theo-
rem 16.7, we can improve the inequality (16.8), obtaining instead (verify)
E[T] ≤exp[(1 + o(1)) max{(1/2)(log n/ log y) log log n + 2 log y, 3 log y}].
From this, if we set
y := exp[(1/2)(log n log log n)1/2)],
TEAM LinG

16.4 Practical improvements
353
we obtain
E[T] ≤exp[(2 + o(1))(log n log log n)1/2].
An analogous improvement can be obtained for Algorithm SEDL.
Although this improvement reduces the constant 2
√
2 ≈2.828 to 2, the
constant is in the exponent, and so this improvement is not to be scoﬀed at!
16.4.2 The quadratic sieve algorithm
We now describe a practical improvement to Algorithm SEF. This algorithm,
known as the quadratic sieve, is faster in practice than Algorithm SEF;
however, its analysis is somewhat heuristic.
First, let us return to the simpliﬁed version of Algorithm SEF, where
we collect relations of the form (16.6). Furthermore, instead of choosing
the values αi at random, we will choose them in a special way, as we now
describe. Let
˜n := ⌊√n⌋,
and deﬁne the polynomial
F := (X + ˜n)2 −n ∈Z[X].
In addition to the usual “smoothness parameter” y, we need a “sieving
parameter” z, whose choice will be discussed below. We shall assume that
both y and z are of the form exp[(log n)1/2+o(1)], and our ultimate choices
of y and z will indeed satisfy this assumption.
For all s = 1, 2, . . . , ⌊z⌋, we shall determine which values of s are “good,”
in the sense that the corresponding value F(s) is y-smooth. For each good
s, since we have F(s) ≡(s+ ˜n)2 (mod n), we obtain one relation of the form
(16.6), with αi := [s + ˜n]n. If we ﬁnd at least k + 1 good values of s, then
we can apply Gaussian elimination as usual to ﬁnd a square root β of 1 in
Zn. Hopefully, we will have β ̸= ±1, allowing us to split n.
Observe that for 1 ≤s ≤z, we have
1 ≤F(s) ≤z2 + 2zn1/2 ≤n1/2+o(1).
Now, although the values F(s) are not at all random, we might expect
heuristically that the number of good s up to z is roughly equal to ˆσz,
where ˆσ is the probability that a random integer in the interval [1, n1/2] is
y-smooth, and by Theorem 16.7, we have
ˆσ = exp[(−1/4 + o(1))(log n/ log y) log log n].
TEAM LinG

354
Subexponential-time discrete logarithms and factoring
If our heuristics are valid, this already gives us an improvement over Al-
gorithm SEF, since now we are looking for y-smooth numbers near n1/2,
which are much more common than y-smooth numbers near n. But there
is another improvement possible; namely, instead of testing each individual
number F(s) for smoothness using trial division, we can test them all at
once using the following “sieving procedure”:
Create a vector v[1 . . . ⌊z⌋], and initialize v[s] to F(s), for 1 ≤
s ≤z. For each prime p up to y, do the following:
1. Compute the roots of the polynomial F modulo p.
This can be done quite eﬃciently, as follows. For p = 2,
F has exactly one root modulo p, which is determined
by the parity of ˜n.
For p > 2, we may use the fa-
miliar quadratic formula together with an algorithm for
computing square roots modulo p, as discussed in Exer-
cise 13.3. A quick calculation shows that the discrimi-
nant of F is n, and thus, F has a root modulo p if and
only if n is a quadratic residue modulo p, in which case
it will have two roots (under our usual assumptions, we
cannot have p | n).
2. Assume that the distinct roots of F modulo p lying in
the interval [1, p] are ri, for i = 1, . . . , vp.
Note that vp = 1 for p = 2 and vp ∈{0, 2} for p > 2.
Also note that F(s) ≡0 (mod p) if and only if s ≡
ri (mod p) for some i = 1, . . . , vp.
For i = 1, . . . , vp, do the following:
s ←ri
while s ≤z do
repeat
v[s] ←v[s]/p
until p ∤v[s]
s ←s + p
At the end of this sieving procedure, the good values of s may be identiﬁed
as precisely those such that v[s] = 1.
The running time of this sieving
procedure is at most len(n)O(1) times

p≤y
z
p = z

p≤y
1
p = O(z log log y) = z1+o(1).
Here, we have made use of Theorem 5.10, although this is not really nec-
essary—for our purposes, the bound 
p≤y(1/p) = O(log y) would suﬃce.
TEAM LinG

16.4 Practical improvements
355
Note that this sieving procedure is a factor of k1+o(1) faster than the method
for ﬁnding smooth numbers based on trial division. With just a little extra
book-keeping, we can not only identify the good values of s, but we can also
compute the factorization of F(s) into primes.
Now, let us put together all the pieces. We have to choose z just large
enough so as to ﬁnd at least k + 1 good values of s up to z. So we should
choose z so that z ≈k/ˆσ—in practice, we could choose an initial estimate
for z, and if this choice of z does not yield enough relations, we could keep
doubling z until we do get enough relations. Assuming that z ≈k/ˆσ, the
cost of sieving is (k/ˆσ)1+o(1), or
exp[(1 + o(1))(1/4)(log n/ log y) log log n + log y].
The cost of Gaussian elimination is still O(k3), or
exp[(3 + o(1)) log y].
Thus, if T is the running time of the entire algorithm, we have
T ≤exp[(1 + o(1)) max{(1/4)(log n/ log y) log log n + log y, 3 log y}].
Let µ := log y, A := (1/4) log n log log n, S1 := A/µ + µ and S2 := 3µ, and
let us ﬁnd the value of µ that minimizes max{S1, S2}. Using a little calculus,
one ﬁnds that S1 is minimized at µ = A1/2. For this value of µ, we have
S1 = 2A1/2 and S2 = 3A1/2 > S1, and so this choice of µ is a bit larger
than optimal. For µ < A1/2, S1 is decreasing (as a function of µ), while S2
is always increasing. It follows that the optimal value of µ is obtained by
setting
A/µ + µ = 3µ
and solving for µ. This yields µ = (A/2)1/2. So setting
y = exp[(1/(2
√
2))(log n log log n)1/2],
we have
T ≤exp[(3/(2
√
2) + o(1))(log n log log n)1/2].
Thus, we have reduced the constant in the exponent from 2, for Algorithm
SEF (using the more accurate smoothness density estimates), to 3/(2
√
2) ≈
1.061.
We mention one ﬁnal improvement. The matrix to which we apply Gaus-
sian elimination in stage 2 is “sparse”; indeed, since any integer less than
n has O(log n) prime factors, the total number of non-zero entries in the
TEAM LinG

356
Subexponential-time discrete logarithms and factoring
matrix is k1+o(1). In this case, there are special algorithms for working with
such sparse matrices, which allow us to perform stage 2 of the factoring
algorithm in time k2+o(1), or
exp[(2 + o(1)) log y].
This gives us
T ≤exp[(1 + o(1)) max{(1/4)(log n/ log y) log log n + log y, 2 log y}],
and setting
y = exp[(1/2)(log n log log n)1/2]
yields
T ≤exp[(1 + o(1))(log n log log n)1/2].
Thus,
this improvement reduces the constant in the exponent from
3/(2
√
2) ≈1.061 to 1. Moreover, the special algorithms designed to work
with sparse matrices typically use much less space than ordinary Gaussian
elimination—even if the input to Gaussian elimination is sparse, the inter-
mediate matrices will not be. We shall discuss in detail later, in §19.4, one
such algorithm for solving sparse systems of linear equations.
The quadratic sieve may fail to factor n, for one of two reasons: ﬁrst, it
may fail to ﬁnd k + 1 relations; second, it may ﬁnd these relations, but in
stage 2, it only ﬁnds a trivial square root of 1. There is no rigorous theory
to say why the algorithm should not fail for one of these two reasons, but
experience shows that the algorithm does indeed work as expected.
16.5 Notes
Many of the algorithmic ideas in this chapter were ﬁrst developed for the
problem of factoring integers, and then later adapted to the discrete log-
arithm problem.
The ﬁrst (heuristic) subexponential-time algorithm for
factoring integers, called the continued fraction method (not discussed
here), was introduced by Lehmer and Powers [56], and later reﬁned and
implemented by Morrison and Brillhart [66].
The ﬁrst rigorously ana-
lyzed subexponential-time algorithm for factoring integers was introduced
by Dixon [34]. Algorithm SEF is a variation of Dixon’s algorithm, which
works the same way as Algorithm SEF, except that it generates relations
of the form (16.6) directly (and indeed, it is possible to prove a variant of
TEAM LinG

16.5 Notes
357
Theorem 16.1, and for that matter, Theorem 16.7, for random squares mod-
ulo n). Algorithm SEF is based on an idea suggested by Rackoﬀ(personal
communication).
Theorem 16.7 was proved by Canﬁeld, Erd˝os, and Pomerance [23]. The
quadratic sieve was introduced by Pomerance [74]. Recall that the quadratic
sieve has a heuristic running time of
exp[(1 + o(1))(log n log log n)1/2].
This running time bound can also be achieved rigorously by a result of
Lenstra and Pomerance [58], and to date, this is the best rigorous running
time bound for factoring algorithms. We should stress, however, that most
practitioners in this ﬁeld are not so much interested in rigorous running time
analyses as they are in actually factoring integers, and for such purposes,
heuristic running time estimates are quite acceptable. Indeed, the quadratic
sieve is much more practical than the algorithm in [58], which is mainly of
theoretical interest.
There are two other factoring algorithms not discussed here, but that
should anyway at least be mentioned.
The ﬁrst is the elliptic curve
method, introduced by Lenstra [57].
Unlike all of the other known
subexponential-time algorithms, the running time of this algorithm is sen-
sitive to the sizes of the factors of n; in particular, if p is the smallest prime
dividing n, the algorithm will ﬁnd p (heuristically) in expected time
exp[(
√
2 + o(1))(log p log log p)1/2] · len(n)O(1).
This algorithm is quite practical, and is the method of choice when it is
known (or suspected) that n has some small factors. It also has the ad-
vantage that it uses only polynomial space (unlike all of the other known
subexponential-time factoring algorithms).
The second is the number ﬁeld sieve, the basic idea of which was intro-
duced by Pollard [73], and later generalized and reﬁned by Buhler, Lenstra,
and Pomerance [21], as well as by others. The number ﬁeld sieve will split
n (heuristically) in expected time
exp[(c + o(1))(log n)1/3(log log n)2/3],
where c is a constant (currently, the smallest value of c is 1.902, a result
due to Coppersmith [27]). The number ﬁeld sieve is currently the asymp-
totically fastest known factoring algorithm (at least, heuristically), and it is
also practical, having been used to set the latest factoring record—the fac-
torization of a 576-bit integer that is the product of two primes of about the
TEAM LinG

358
Subexponential-time discrete logarithms and factoring
same size. See the web page www.rsasecurity.com/rsalabs/challenges/
factoring/rsa576.html for more details.
As for subexponential-time algorithms for discrete logarithms, Adleman
[1] adapted the ideas used for factoring to the discrete logarithm problem,
although it seems that some of the basic ideas were known much earlier.
Algorithm SEDL is a variation on this algorithm, and the basic technique
is usually referred to as the index calculus method. The basic idea of
the number ﬁeld sieve was adapted to the discrete logarithm problem by
Gordon [40]; see also Adleman [2] and Schirokauer, Weber, and Denny [80].
For many more details and references for subexponential-time algorithms
for factoring and discrete logarithms, see Chapter 6 of Crandall and Pomer-
ance [30]. Also, see the web page www.crypto-world.com/FactorWorld.
html for links to research papers and implementation reports.
For more details regarding the security of signature schemes, as discussed
following Exercise 16.4, see the paper by Bellare and Rogaway [13].
Last, but not least, we should mention the fact that there are in fact
polynomial-time algorithms for factoring and discrete logarithms; however,
these algorithms require special hardware, namely, a quantum computer.
Shor [87, 88] showed that these problems could be solved in polynomial time
on such a device; however, at the present time, it is unclear when and if such
machines will ever be built. Much, indeed most, of modern-day cryptogra-
phy will crumble if this happens, or if eﬃcient “classical” algorithms for
these problems are discovered (which is still a real possibility).
TEAM LinG

17
More rings
This chapter develops a number of other concepts concerning rings. These
concepts will play important roles later in the text, and we prefer to discuss
them now, so as to avoid too many interruptions of the ﬂow of subsequent
discussions.
17.1 Algebras
Let R be a ring. An R-algebra (or algebra over R) is a ring E, together
with a ring homomorphism τ : R →E. Usually, the map τ will be clear
from context, as in the following examples.
Example 17.1. If E is a ring that contains R as a subring, then E is an
R-algebra, where the associated map τ : R →E is just the inclusion map.
2
Example 17.2. Let E1, . . . , En be R-algebras, with associated maps τi :
R →Ei, for i = 1, . . . , n. Then the direct product ring E := E1 × · · · × En
is naturally viewed as an R-algebra, via the map τ that sends a ∈R to
(τ1(a), . . . , τn(a)) ∈E. 2
Example 17.3. Let E be an R-algebra, with associated map τ : R →E,
and let I be an ideal of E. Consider the quotient ring E/I. If ρ is the
natural map from E onto E/I, then the homomorphism ρ ◦τ makes E/I
into an R-algebra, called the quotient algebra of E modulo I. 2
Example 17.4. As a special case of the previous example, consider the ring
R[X], viewed as an R-algebra via inclusion, and the ideal of R generated by
f, where f is a monic polynomial. Then R[X]/(f) is naturally viewed as an
R-algebra, via the map τ that sends c ∈R to [c]f ∈R[X]/(f). If deg(f) > 0,
359
TEAM LinG

360
More rings
then τ is an embedding of R in R[X]/(f); if deg(f) = 0, then R[X]/(f) is the
trivial ring, and τ maps everything to zero. 2
In some sense, an R-algebra is a generalization of the notion of an exten-
sion ring. When the map τ : R →E is a canonical embedding, the language
of R-algebras can be used if one wants to avoid the sloppiness involved in
“identifying” elements of R with their image under τ in E, as we have done
on occasion.
In this text, we will be particularly interested in the situation where E is
an algebra over a ﬁeld F. In this case, E either contains a copy of F, or
is itself the trivial ring. To see this, let τ : F →E be the associated map.
Then since the kernel of τ is an ideal of F, it must either be {0F } or F. In
the former case, τ is injective, and so E contains an isomorphic copy of F.
In the latter case, our requirement that τ(1F ) = 1E implies that 1E = 0E,
and so E is trivial.
Subalgebras
Let E be an R-algebra with associated map τ : R →E. A subset S of E is
a subalgebra if S is a subring containing img(τ). As an important special
case, if τ is just the inclusion map, then a subring S of E is a subalgebra if
and only if S contains R.
R-algebra homomorphisms
There is, of course, a natural notion of a homomorphism for R-algebras.
Indeed, it is this notion that is our main motivation for introducing R-
algebras in this text. If E and E′ are R-algebras, with associated maps
τ : R →E and τ ′ : R →E′, then a map ρ : E →E′ is called an R-algebra
homomorphism if ρ is a ring homomorphism, and if for all a ∈R, we have
ρ(τ(a)) = τ ′(a).
As usual, if ρ is bijective, then it is called an R-algebra isomorphism,
and if R = R′, it is called an R-algebra automorphism.
As an important special case, if τ and τ ′ are just inclusion maps, then a
ring homomorphism ρ : E →E′ is an R-algebra homomorphism if and only
if the restriction of ρ to R is the identity map.
The reader should also verify the following facts.
First, an R-algebra
homomorphism maps subalgebras to subalgebras. Second, Theorems 9.22,
9.23, 9.24, 9.25, 9.26, and 9.27 carry over mutatis mutandis from rings to
R-algebras.
TEAM LinG

17.1 Algebras
361
Example 17.5. Since C contains R as a subring, we may naturally view C
as an R-algebra. The complex conjugation map on C that sends a + bi to
a −bi, for a, b ∈R, is an R-algebra automorphism on C (see Example 9.5).
2
Example 17.6. Let p be a prime, and let F be the ﬁeld Zp. If E is an
F-algebra, with associated map τ : F →E, then the map ρ : E →E that
sends α ∈E to αp is an F-algebra homomorphism. To see this, note that
E is either trivial, or contains a copy of Zp. In the former case, there is
nothing really to prove. In the latter case, E has characteristic p, and so
the fact that ρ is a ring homomorphism follows from Example 9.42 (the
“freshman’s dream”); moreover, by Fermat’s little theorem, for all a ∈F,
we have τ(a)p = τ(ap) = τ(a). 2
Polynomial evaluation
Let E be an R-algebra with associated map τ : R →E. Any polynomial
g ∈R[X] naturally deﬁnes a function on E: if g = 
i giXi, with each gi ∈R,
and α ∈E, then
g(α) :=

i
τ(gi)αi.
For ﬁxed α ∈E, the polynomial evaluation map ρ : R[X] →E sends
g ∈R[X] to g(α) ∈E. It is easily veriﬁed that ρ is an R-algebra homomor-
phism (where we naturally view R[X] as an R-algebra via inclusion). The
image of ρ is denoted R[α], and is a subalgebra of E. Indeed, R[α] is the
smallest subalgebra of E containing α.
Note that if E contains R as a subring, then the notation R[α] has the
same meaning as that introduced in Example 9.39.
We next state a very simple, but extremely useful, fact:
Theorem 17.1. Let ρ : E →E′ be an R-algebra homomorphism. Then for
any g ∈R[X] and α ∈E, we have
ρ(g(α)) = g(ρ(α)).
Proof. Let τ : R →E and τ ′ : R →E′ be the associated maps.
Let
TEAM LinG

362
More rings
g = 
i giXi ∈R[X]. Then we have
ρ(g(α)) = ρ(

i
τ(gi)αi) =

i
ρ(τ(gi)αi)
=

i
ρ(τ(gi))ρ(αi) =

i
τ ′(gi)ρ(α)i
= g(ρ(α)). 2
As a special case of Theorem 17.1, if E = R[η] for some η ∈E, then every
element of E can be expressed as g(η) for some g ∈R[X], and ρ(g(η)) =
g(ρ(η)); hence, the action of ρ is completely determined by its action on η.
Example 17.7. Let E := R[X]/(f) for some monic polynomial f ∈R[X], so
that E = R[η], where η := [X]f, and let E′ be any R-algebra.
Suppose that ρ : E →E′ is an R-algebra homomorphism, and that η′ :=
ρ(η). The map ρ sends g(η) to g(η′), for g ∈R[X]. Also, since f(η) = 0E,
we have 0E′ = ρ(f(η)) = f(η′). Thus, η′ must be a root of f.
Conversely, suppose that η′ ∈E′ is a root of f. Then the polynomial
evaluation map from R[X] to E′ that sends g ∈R[X] to g(η′) ∈E′ is an R-
algebra homomorphism whose kernel contains f, and this gives rise to the
R-algebra homomorphism ρ : E →E′ that sends g(η) to g(η′), for g ∈R[X].
One sees that complex conjugation is just a special case of this construction
(see Example 9.44). 2
R-algebras as R-modules
If E is an R-algebra, with associated map τ : R →E, we may naturally
view E as an R-module, where we deﬁne a scalar multiplication operation
as follows: for a ∈R and α ∈E, deﬁne
a · α := τ(a)α.
The reader may easily verify that with scalar multiplication so deﬁned, E is
an R-module.
Of course, if E is an algebra over a ﬁeld F, then it is also a vector space
over F.
Exercise 17.1. Show that any ring E may be viewed as a Z-algebra.
Exercise 17.2. Show that the only R-algebra homomorphisms from C into
itself are the identity map and the complex conjugation map.
TEAM LinG

17.2 The ﬁeld of fractions of an integral domain
363
Exercise 17.3. Let E be an R-algebra, viewed as an R-module as discussed
above.
(a) Show that for all a ∈R and α, β ∈E, we have a · (αβ) = (a · α)β.
(b) Show that a subring S of E is a subalgebra if and only if it is also
submodule.
(c) Show that if E′ is another R-algebra, then a ring homomorphism
ρ : E →E′ is an R-algebra homomorphism if and only if it is an
R-linear map.
Exercise 17.4. This exercise develops an alternative characterization of
R-algebras. Let R be a ring, and let E be a ring, together with a scalar
multiplication operation, that makes E into an R-module. Further suppose
that for all a ∈R and α, β ∈E, we have a(αβ) = (aα)β.
Deﬁne the
map τ : R →E that sends a ∈R to a · 1E ∈E. Show that τ is a ring
homomorphism, so that E is an R-algebra, and also show that τ(a)α = aα
for all a ∈R and α ∈E.
17.2 The ﬁeld of fractions of an integral domain
Let D be any integral domain. Just as we can construct the ﬁeld of rational
numbers by forming fractions involving integers, we can construct a ﬁeld
consisting of fractions whose numerators and denominators are elements of
D. This construction is quite straightforward, though a bit tedious.
To begin with, let S be the set of all pairs of the form (a, b), with a, b ∈D
and b ̸= 0D.
Intuitively, such a pair (a, b) is a “formal fraction,” with
numerator a and denominator b. We deﬁne a binary relation ∼on S as
follows: for (a1, b1), (a2, b2) ∈S, we say (a1, b1) ∼(a2, b2) if and only if
a1b2 = a2b1. Our ﬁrst task is to show that this is an equivalence relation:
Lemma 17.2. For all (a1, b1), (a2, b2), (a3, b3) ∈S, we have
(i) (a1, b1) ∼(a1, b1);
(ii) (a1, b1) ∼(a2, b2) implies (a2, b2) ∼(a1, b1);
(iii) (a1, b1) ∼(a2, b2) and (a2, b2) ∼(a3, b3) implies (a1, b1) ∼(a3, b3).
Proof. (i) and (ii) are rather trivial, and we do not comment on these any
further. As for (iii), assume that a1b2 = a2b1 and a2b3 = a3b2. Multiplying
the ﬁrst equation by b3 we obtain a1b3b2 = a2b3b1 and substituting a3b2 for
a2b3 on the right-hand side of this last equation, we obtain a1b3b2 = a3b2b1.
Now, using the fact that b2 is non-zero and that D is an integral domain,
we may cancel b2 from both sides, obtaining a1b3 = a3b1. 2
TEAM LinG

364
More rings
Since ∼is an equivalence relation, it partitions S into equivalence classes,
and for (a, b) ∈S, we denote by [a, b] the equivalence class containing (a, b),
and we denote by K the collection of all such equivalence classes.
Our
next task is to deﬁne addition and multiplication operations on equivalence
classes, mimicking the usual rules of arithmetic with fractions. We want
to deﬁne the sum of [a1, b1] and [a2, b2] to be [a1b2 + a2b1, b1b2], and the
product of [a1, b1] and [a2, b2] to be [a1a2, b1b2]. Note that since D is an
integral domain, if b1 and b2 are non-zero, then so is the product b1b2, and
therefore [a1b2 + a2b1, b1b2] and [a1a2, b1b2] are indeed equivalence classes.
However, to ensure that this deﬁnition is unambiguous, and does not depend
on the particular choice of representatives of the equivalence classes [a1, b1]
and [a2, b2], we need the following lemma.
Lemma 17.3. For (a1, b1), (a′
1, b′
1), (a2, b2), (a′
2, b′
2) ∈S with (a1, b1) ∼
(a′
1, b′
1) and (a2, b2) ∼(a′
2, b′
2), we have
(a1b2 + a2b1, b1b2) ∼(a′
1b′
2 + a′
2b′
1, b′
1b′
2)
and
(a1a2, b1b2) ∼(a′
1a′
2, b′
1b′
2).
Proof. This is a straightforward calculation. Assume that a1b′
1 = a′
1b1 and
a2b′
2 = a′
2b2. Then we have
(a1b2 + a2b1)b′
1b′
2 = a1b2b′
1b′
2 + a2b1b′
1b′
2 = a′
1b2b1b′
2 + a′
2b1b′
1b2
= (a′
1b′
2 + a′
2b′
1)b1b2
and
a1a2b′
1b′
2 = a′
1a2b1b′
2 = a′
1a′
2b1b2. 2
In light of this lemma, we may unambiguously deﬁne addition and multi-
plication on K as follows: for [a1, b1], [a2, b2] ∈K, we deﬁne
[a1, b1] + [a2, b2] := [a1b2 + a2b1, b1b2]
and
[a1, b1] · [a2, b2] := [a1a2, b1b2].
The next task is to show that K is a ring—we leave the details of this
(which are quite straightforward) to the reader.
Lemma 17.4. With addition and multiplication as deﬁned above, K is a
ring, with additive identity [0D, 1D] and multiplicative identity [1D, 1D].
TEAM LinG

17.2 The ﬁeld of fractions of an integral domain
365
Proof. Exercise. 2
Finally, we observe that K is in fact a ﬁeld: it is clear that [a, b] is a non-
zero element of K if and only if a ̸= 0D, and hence any non-zero element
[a, b] of K has a multiplicative inverse, namely, [b, a].
The ﬁeld K is called the ﬁeld of fractions of D. Consider the map
τ : D →K that sends a ∈D to [a, 1D] ∈K. It is easy to see that this map
is a ring homomorphism, and one can also easily verify that it is injective.
So, starting from D, we can synthesize “out of thin air” its ﬁeld of fractions
K, which essentially contains D as a subring, via the canonical embedding
τ : D →K.
Now suppose that we are given a ﬁeld L that contains D as a subring.
Consider the set K′ consisting of all elements in L of the form ab−1, where
a, b ∈D and b ̸= 0—note that here, the arithmetic operations are performed
using the rules for arithmetic in L.
One may easily verify that K′ is a
subﬁeld of L that contains D, and it is easy to see that this is the smallest
subﬁeld of L that contains D. The subﬁeld K′ of L may be referred to as
the ﬁeld of fractions of D within L. One may easily verify that the map
ρ : K →L that sends [a, b] ∈K to ab−1 ∈L is an unambiguously deﬁned
ring homomorphism that maps K injectively onto K′; in particular, K is
isomorphic as a ring to K′. It is in this sense that the ﬁeld of fractions K is
the smallest ﬁeld containing D as a subring.
Somewhat more generally, suppose that L is a ﬁeld, and that τ ′ : D →L
is an embedding. One may easily verify that the map ρ : K →L that sends
[a, b] ∈K to τ ′(a)τ ′(b)−1 ∈L is an unambiguously deﬁned, injective ring
homomorphism. Moreover, we may view K and L as D-algebras, via the
embeddings τ : D →K and τ ′ : D →L, and the map ρ is seen to be a
D-algebra homomorphism.
From now on, we shall simply write an element [a, b] of K as a fraction,
a/b. In this notation, the above rules for addition, multiplication, and testing
equality in K now look quite familiar:
a1
b1
+ a2
b2
= a1b2 + a2b1
b1b2
, a1
b1
· a2
b2
= a1a2
b1b2
,
and a1
b1
= a2
b2
iﬀa1b2 = a2b1.
Observe that for a, b ∈D, with b ∈0D and b | a, so that a = bc for
some c ∈D, then the fraction a/b ∈K is equal to the fraction c/1D ∈K,
and identifying the element c ∈D with its canonical image c/1D ∈K, we
may simply write c = a/b. Note that this notation is consistent with that
introduced in part (iii) of Theorem 9.4. A special case of this arises when
b ∈D∗, in which case c = ab−1.
TEAM LinG

366
More rings
Function ﬁelds
An important special case of the above construction for the ﬁeld of fractions
of D is when D = F[X], where F is a ﬁeld. In this case, the ﬁeld of fractions is
denoted F(X), and is called the ﬁeld of rational functions (over F). This
terminology is a bit unfortunate, since just as with polynomials, although
the elements of F(X) deﬁne functions, they are not (in general) in one-to-one
correspondence with these functions.
Since F[X] is a subring of F(X), and since F is a subring of F[X], we see
that F is a subﬁeld of F(X).
More generally, we may apply the above construction to the ring D =
F[X1, . . . , Xn] of multi-variate polynomials over a ﬁeld F, in which case the
ﬁeld of fractions is denoted F(X1, . . . , Xn), and is also called the ﬁeld of
rational functions (over F, in the variables X1, . . . , Xn).
Exercise 17.5. Let F be a ﬁeld of characteristic zero. Show that F contains
an isomorphic copy of Q.
Exercise 17.6. Show that the ﬁeld of fractions of Z[i] within C is Q[i]. (See
Example 9.22 and Exercise 9.8.)
17.3 Unique factorization of polynomials
Throughout this section, F denotes a ﬁeld.
Like the ring Z, the ring F[X] of polynomials is an integral domain, and
because of the division with remainder property for polynomials, F[X] has
many other properties in common with Z. Indeed, essentially all the ideas
and results from Chapter 1 can be carried over almost verbatim from Z to
F[X], and in this section, we shall do just that.
Recall that for a, b ∈F[X], we write b | a if a = bc for some c ∈F[X], and
in this case, note that deg(a) = deg(b) + deg(c).
The units of F[X] are precisely the units F ∗of F, that is, the non-zero
constants.
We call two polynomials a, b ∈F[X] associate if a = ub for
u ∈F ∗. It is easy to see that a and b are associate if and only if a | b
and b | a—indeed, this follows as a special case of part (ii) of Theorem 9.4.
Clearly, any non-zero polynomial a is associate to a unique monic polynomial
(i.e., with leading coeﬃcient 1), called the monic associate of a; indeed,
the monic associate of a is lc(a)−1 · a.
We call a polynomial p irreducible if it is non-constant and all divisors
of p are associate to 1 or p. Conversely, we call a polynomial n reducible
if it is non-constant and is not irreducible. Equivalently, non-constant n is
TEAM LinG

17.3 Unique factorization of polynomials
367
reducible if and only if there exist polynomials a, b ∈F[X] of degree strictly
less that n such that n = ab.
Clearly, if a and b are associate polynomials, then a is irreducible if and
only if b is irreducible.
The irreducible polynomials play a role similar to that of the prime num-
bers. Just as it is convenient to work with only positive prime numbers, it
is also convenient to restrict attention to monic irreducible polynomials.
Corresponding to Theorem 1.3, every non-zero polynomial can be ex-
pressed as a unit times a product of monic irreducibles in an essentially
unique way:
Theorem 17.5. Every non-zero polynomial n ∈F[X] can be expressed as
n = u · pe1
1 · · · per
r ,
where u ∈F ∗, the pi are distinct monic irreducible polynomials, and the ei
are positive integers. Moreover, this expression is unique, up to a reordering
of the pi.
To prove this theorem, we may assume that n is monic, since the non-
monic case trivially reduces to the monic case.
The proof of the existence part of Theorem 17.5 is just as for Theorem 1.3.
If n is 1 or a monic irreducible, we are done. Otherwise, there exist a, b ∈
F[X] of degree strictly less than n such that n = ab, and again, we may
assume that a and b are monic. By induction on degree, both a and b can
be expressed as a product of monic irreducible polynomials, and hence, so
can n.
The proof of the uniqueness part of Theorem 17.5 is almost identical
to that of Theorem 1.3. As a special case of Theorem 9.12, we have the
following division with remainder property, analogous to Theorem 1.4:
Theorem 17.6. For a, b ∈F[X] with b ̸= 0, there exist unique q, r ∈F[X]
such that a = bq + r and deg(r) < deg(b).
Analogous to Theorem 1.5, we have:
Theorem 17.7. For any ideal I ⊆F[X], there exists a unique polynomial d
such that I = dF[X], where d is either zero or monic.
Proof. We ﬁrst prove the existence part of the theorem. If I = {0}, then
d = 0 does the job, so let us assume that I ̸= {0}.
Let d be a monic
polynomial of minimal degree in I. We want to show that I = dF[X].
We ﬁrst show that I ⊆dF[X]. To this end, let c be any element in I. It
TEAM LinG

368
More rings
suﬃces to show that d | c. Using Theorem 17.6, we may write c = qd + r,
where deg(r) < deg(d). Then by the closure properties of ideals, one sees
that r = c −qd is also an element of I, and by the minimality of the degree
of d, we must have r = 0. Thus, d | c.
We next show that dF[X] ⊆I. This follows immediately from the fact
that d ∈I and the closure properties of ideals.
That proves the existence part of the theorem. As for uniqueness, note
that if dF[X] = d′F[X], we have d | d′ and d′ | d, from which it follows that d
and d′ are associate, and so if d and d′ are both either monic or zero, they
must be equal. 2
For a, b ∈F[X], we call d ∈F[X] a common divisor of a and b if d | a
and d | b; moreover, we call such a d a greatest common divisor of a and
b if d is monic or zero, and all other common divisors of a and b divide d.
Analogous to Theorem 1.6, we have:
Theorem 17.8. For any a, b ∈F[X], there exists a unique greatest common
divisor d of a and b, and moreover, aF[X] + bF[X] = dF[X].
Proof. We apply the previous theorem to the ideal I := aF[X] + bF[X]. Let
d ∈F[X] with I = dF[X], as in that theorem. Note that a, b, d ∈I and d is
monic or zero.
It is clear that d is a common divisor of a and b. Moreover, there exist
s, t ∈F[X] such that as+bt = d. If d′ | a and d′ | b, then clearly d′ | (as+bt),
and hence d′ | d.
Finally, for uniqueness, if d′′ is a greatest common divisor of a and b, then
d | d′′ and d′′ | d, and hence d′′ is associate to d, and the requirement that
d′′ is monic or zero implies that d′′ = d. 2
For a, b ∈F[X], we denote by gcd(a, b) the greatest common divisor of
a and b. Note that as we have deﬁned it, lc(a) gcd(a, 0) = a. Also note
that when at least one of a or b are non-zero, gcd(a, b) is the unique monic
polynomial of maximal degree that divides both a and b.
An immediate consequence of Theorem 17.8 is that for all a, b ∈F[X],
there exist s, t ∈F[X] such that as + bt = gcd(a, b), and that when at
least one of a or b are non-zero, gcd(a, b) is the unique monic polynomial of
minimal degree that can be expressed as as + bt for some s, t ∈F[X].
We say that a, b ∈F[X] are relatively prime if gcd(a, b) = 1, which is
the same as saying that the only common divisors of a and b are units. It is
immediate from Theorem 17.8 that a and b are relatively prime if and only
if aF[X]+bF[X] = F[X], which holds if and only if there exist s, t ∈F[X] such
that as + bt = 1.
TEAM LinG

17.3 Unique factorization of polynomials
369
Analogous to Theorem 1.7, we have:
Theorem 17.9. For a, b, c ∈F[X] such that c | ab and gcd(a, c) = 1, we
have c | b.
Proof. Suppose that c | ab and gcd(a, c) = 1. Then since gcd(a, c) = 1, by
Theorem 17.8 we have as + ct = 1 for some s, t ∈F[X]. Multiplying this
equation by b, we obtain abs + cbt = b. Since c divides ab by hypothesis, it
follows that c | (abs + cbt), and hence c | b. 2
Analogous to Theorem 1.8, we have:
Theorem 17.10. Let p ∈F[X] be irreducible, and let a, b ∈F[X]. Then
p | ab implies that p | a or p | b.
Proof. Assume that p | ab. The only divisors of p are associate to 1 or p.
Thus, gcd(p, a) is either 1 or the monic associate of p. If p | a, we are done;
otherwise, if p ∤a, we must have gcd(p, a) = 1, and by the previous theorem,
we conclude that p | b. 2
Now to prove the uniqueness part of Theorem 17.5. Suppose we have
p1 · · · pr = p′
1 · · · p′
s,
where p1, . . . , pr and p′
1, . . . , p′
s are monic irreducible polynomials (duplicates
are allowed among the pi and among the p′
j). If r = 0, we must have s = 0
and we are done. Otherwise, as p1 divides the right-hand side, by inductively
applying Theorem 17.10, one sees that p1 is equal to p′
j for some j. We can
cancel these terms and proceed inductively (on r).
That completes the proof of Theorem 17.5.
Analogous to Theorem 1.9, we have:
Theorem 17.11. There are inﬁnitely many monic irreducible polynomials
in F[X].
If F is inﬁnite, then this theorem is true simply because there are inﬁnitely
many monic, linear polynomials; in any case, one can also just prove this
theorem by mimicking the proof of Theorem 1.9 (verify).
For a monic irreducible polynomial p, we may deﬁne the function νp, map-
ping non-zero polynomials to non-negative integers, as follows: for polyno-
mial n ̸= 0, if n = pem, where p ∤m, then νp(n) := e. We may then write
the factorization of n into irreducibles as
n = u

p
pνp(n),
TEAM LinG

370
More rings
where the product is over all monic irreducible polynomials p, with all but
ﬁnitely many of the terms in the product equal to 1.
Just as for integers, we may extend the domain of deﬁnition of νp to
include 0, deﬁning νp(0) := ∞. For all polynomials a, b, we have
νp(a · b) = νp(a) + νp(b) for all p.
(17.1)
From this, it follows that for all polynomials a, b, we have
b | a
if and only if
νp(b) ≤νp(a) for all p,
(17.2)
and
νp(gcd(a, b)) = min(νp(a), νp(b)) for all p.
(17.3)
For a, b ∈F[X] a common multiple of a and b is a polynomial m such
that a | m and b | m; moreover, such an m is the least common multiple
of a and b if m is monic or zero, and m divides all common multiples of a
and b. In light of Theorem 17.5, it is clear that the least common multiple
exists and is unique, and we denote the least common multiple of a and
b by lcm(a, b). Note that as we have deﬁned it, lcm(a, 0) = 0, and that
when both a and b are non-zero, lcm(a, b) is the unique monic polynomial
of minimal degree that is divisible by both a and b. Also, for all a, b ∈F[X],
we have
νp(lcm(a, b)) = max(νp(a), νp(b)) for all p,
(17.4)
and
lc(ab) · gcd(a, b) · lcm(a, b) = ab.
(17.5)
Just as in §1.3, the notions of greatest common divisor and least common
multiple generalize naturally from two to any number of polynomials. We
also say that polynomials a1, . . . , ak ∈F[X] are pairwise relatively prime
if gcd(ai, aj) = 1 for all i, j with i ̸= j.
Also just as in §1.3, any rational function a/b ∈F(X) can be expressed
as a fraction a′/b′ in lowest terms, that is, a/b = a′/b′ and gcd(a′, b′) = 1,
and this representation is unique up to multiplication by units.
Many of the exercises in Chapter 1 carry over naturally to polynomials—
the reader is encouraged to look over all of the exercises in that chapter,
determining which have natural polynomial analogs, and work some of these
out.
Exercise 17.7. Show that for f ∈F[X] of degree 2 or 3, we have f irre-
ducible if and only if f has no roots in F.
TEAM LinG

17.4 Polynomial congruences
371
17.4 Polynomial congruences
Throughout this section, F denotes a ﬁeld.
Specializing the congruence notation introduced in §9.3 for arbitrary rings
to the ring F[X], for polynomials a, b, n ∈F[X], we write a ≡b (mod n) when
n | (a−b). Because of the division with remainder property for polynomials,
we have the analog of Theorem 2.1:
Theorem 17.12. Let n ∈F[X] be a non-zero polynomial. For every a ∈
F[X], there exists a unique b ∈F[X] such that a ≡b (mod n) and deg(b) < n,
namely, b := a mod n.
For a non-zero n ∈F[X], and a ∈F[X], we say that a′ ∈F[X] is a multi-
plicative inverse of a modulo n if aa′ ≡1 (mod n).
All of the results we proved in §2.2 for solving linear congruences over the
integers carry over almost identically to polynomials. As such, we do not
give proofs of any of the results here. The reader may simply check that the
proofs of the corresponding results translate almost directly.
Theorem 17.13. Let a, n ∈F[X] with n ̸= 0. Then a has a multiplicative
inverse modulo n if and only if a and n are relatively prime.
Theorem 17.14. Let a, n, z, z′ ∈F[X] with n ̸= 0. If a is relatively prime
to n, then az ≡az′ (mod n) if and only if z ≡z′ (mod n). More generally,
if d := gcd(a, n), then az ≡az′ (mod n) if and only if z ≡z′ (mod n/d).
Theorem 17.15. Let a, b, n ∈F[X] with n ̸= 0. If a is relatively prime
to n, then the congruence az ≡b (mod n) has a solution z; moreover, any
polynomial z′ is a solution if and only if z ≡z′ (mod n).
As for integers, this theorem allows us to generalize the “mod” operation
as follows: if n ∈F[X] is a non-zero polynomial, and s ∈F(X) is a rational
function of the form b/a, where a, b ∈F[X], a ̸= 0, and gcd(a, n) = 1, then
s mod n denotes the unique polynomial z satisfying
az ≡b (mod n) and
deg(z) < deg(n).
With this notation, we can simply write a−1 mod n to denote the unique
multiplicative inverse of a modulo n with deg(a) < deg(n).
Theorem 17.16. Let a, b, n ∈F[X] with n ̸= 0, and let d := gcd(a, n).
If d | b, then the congruence az ≡b (mod n) has a solution z, and any
polynomial z′ is also a solution if and only if z ≡z′ (mod n/d). If d ∤b,
then the congruence az ≡b (mod n) has no solution z.
TEAM LinG

372
More rings
Theorem 17.17 (Chinese remainder theorem). Let n1, . . . , nk ∈F[X]
be pairwise relatively prime, non-zero polynomials, and let a1, . . . , ak ∈F[X]
be arbitrary polynomials. Then there exists a polynomial z ∈F[X] such that
z ≡ai (mod ni) (i = 1, . . . , k).
Moreover, any other polynomial z′ ∈F[X] is also a solution of these congru-
ences if and only if z ≡z′ (mod n), where n := k
i=1 ni.
Note that the Chinese remainder theorem (with Theorem 17.12) implies
that there exists a unique solution z ∈F[X] to the given congruences with
deg(z) < deg(n).
The Chinese remainder theorem also has a more algebraic interpreta-
tion. Deﬁne quotient rings Ei := F[X]/(ni) for i = 1, . . . , k, which we may
naturally view as F-algebras (see Example 17.4), along with the product
F-algebra E := E1 × · · · × Ek (see Example 17.2). The map ρ from F[X] to
E that sends z ∈F[X] to ([z]n1, . . . , [z]nk) ∈E is an F-algebra homomor-
phism. The Chinese remainder theorem says that ρ is surjective, and that
the kernel of ρ is the ideal of F[X] generated by n, giving rise to an F-algebra
isomorphism of F[X]/(n) with E.
Let us recall the formula for the solution z (see proof of Theorem 2.8).
We have
z :=
k

i=1
wiai,
where
wi := n′
imi,
n′
i := n/ni,
mi := (n′
i)−1 mod ni (i = 1, . . . , k).
Now, let us consider the special case of the Chinese remainder theorem
where ai ∈F and ni = (X −bi) with bi ∈F, for i = 1, . . . , k. The condition
that the ni are pairwise relatively prime is equivalent to the condition that
the bi are all distinct. A polynomial z satisﬁes the system of congruences if
and only if z(bi) = ai for i = 1, . . . , k. Moreover, we have n′
i = 
j̸=i(X−bj),
and mi = 1/ 
j̸=i(bi −bj) ∈F. So we get
z =
k

i=1
ai

j̸=i(X −bj)

j̸=i(bi −bj).
The reader will recognize this as the usual Lagrange interpolation for-
mula. Thus, the Chinese remainder theorem for polynomials includes La-
grange interpolation as a special case.
TEAM LinG

17.4 Polynomial congruences
373
Let us consider this situation from the point of view of vector spaces.
Consider the map σ : F[X]<k →F ×k that sends z ∈F[X] of degree less than
k to (z(b1), . . . , z(bk)) ∈F ×k, where as above, b1, . . . , bk are distinct elements
of F. We see that σ is an F-linear map, and by the Chinese remainder
theorem, it is bijective. Thus, σ is an F-vector space isomorphism of F[X]<k
with F ×k.
We may encode elements of F[X]<k as row vectors in a natural way, encod-
ing the polynomial z = k−1
i=0 ziXi as the row vector (z0, . . . , zk−1) ∈F 1×k.
With this encoding, we have
σ(z) = (z0, . . . , zk−1)V,
where V is the k × k matrix
V :=





1
1
1
b1
b2
bk
...
...
· · ·
...
bk−1
1
bk−1
2
· · ·
bk−1
k




.
The matrix V (well, actually its transpose) is known as a Vandermonde
matrix.
Because σ is an isomorphism, it follows that the matrix V is
invertible.
More generally, consider any ﬁxed elements b1, . . . , bℓof F, where ℓ≤k,
and consider the F-linear map σ : F[X]<k →F ×ℓthat sends z ∈F[X]<k to
(z(b1), . . . , z(bℓ)). If z = k−1
i=0 ziXi, then
σ(z) = (z0, . . . , zk−1)W,
where W is the k × ℓmatrix
W :=





1
1
1
b1
b2
bℓ
...
...
· · ·
...
bk−1
1
bk−1
2
· · ·
bk−1
ℓ




.
Now, if bi = bj for some i ̸= j, then the columns of W are linearly dependent,
and hence the column rank of W is less than ℓ. Since the column rank of
W is equal to its row rank, the dimension of the row space of W is less
than ℓ, and hence, σ is not surjective. Conversely, if the bi are all distinct,
then since the submatrix of W consisting of its ﬁrst ℓrows is an invertible
Vandermonde matrix, we see that the rank of W is equal to ℓ, and hence σ
is surjective.
TEAM LinG

374
More rings
17.5 Polynomial quotient algebras
Throughout this section, F denotes a ﬁeld.
Let f ∈F[X] be a monic polynomial, and consider the quotient ring
E := F[X]/(f). As discussed in Example 17.4, we may naturally view E
as an F-algebra via the map τ that sends c ∈R to [c]f ∈E. Moreover,
if deg(f) > 0, then τ is an embedding of F in F[X]/(f), and otherwise, if
f = 1, then E is the trivial ring, and τ maps everything to zero.
Suppose that ℓ:= deg(f) > 0. Let η := [X]f ∈E. Then E = F[η], and as
an F-vector space, E has dimension ℓ, with 1, η, . . . , ηℓ−1 being a basis (see
Examples 9.34, 9.43, 14.3, and 14.22). That is, every element of E can be
expressed uniquely as g(η) for g ∈F[X] of degree less than ℓ.
Now, if f is irreducible, then every polynomial a ̸≡0 (mod f) is relatively
prime to f, and hence invertible modulo f; therefore, it follows that E is a
ﬁeld. Conversely, if f is not irreducible, then E cannot be a ﬁeld—indeed,
if g is a non-trivial factor of f, then g(η) is a zero divisor.
If F = Zp for a prime number p, and f is irreducible, then we see that E
is a ﬁnite ﬁeld of cardinality pℓ. In the next chapter, we shall see how one
can perform arithmetic in such ﬁelds eﬃciently, and later, we shall also see
how to eﬃciently construct irreducible polynomials of any given degree over
a ﬁnite ﬁeld.
Minimal polynomials. Now suppose that E is any F-algebra, and let α
be an element of E. Consider the polynomial evaluation map ρ : F[X] →E
that sends g ∈F[X] to g(α). The kernel of ρ is an ideal of F[X], and since
every ideal of F[X] is principal, it follows that there exists a polynomial
φ ∈F[X] such that ker(ρ) is the ideal of F[X] generated by φ; moreover,
we can make the choice of φ unique by insisting that it is monic or zero.
The polynomial φ is called the minimal polynomial of α (over F). If
φ = 0, then ρ is injective, and hence the image F[α] of ρ is isomorphic (as
an F-algebra) to F[X]. Otherwise, F[α] is isomorphic (as an F-algebra) to
F[X]/(φ); moreover, since any polynomial that is zero at α is a polynomial
multiple of φ, we see that φ is the unique monic polynomial of smallest
degree that is zero at α.
If E has ﬁnite dimension, say n, as an F-vector space, then any element α
of E has a non-zero minimal polynomial. Indeed, the elements 1E, α, . . . , αn
must be linearly dependent (as must be any n + 1 vectors in a vector space
of dimension n), and hence there exist c0, . . . , cn ∈F, not all zero, such that
c01E + c1α + · · · + cnαn = 0E,
and therefore, the non-zero polynomial g := 
i ciXi is zero at α.
TEAM LinG

17.5 Polynomial quotient algebras
375
Example 17.8. The polynomial X2 +1 is irreducible over R, since if it were
not, it would have a root in R (see Exercise 17.7), which is clearly impossible,
since −1 is not the square of any real number. It follows immediately that
C = R[X]/(X2 + 1) is a ﬁeld, without having to explicitly calculate a formula
for the inverse of a non-zero complex number. 2
Example 17.9. Consider the polynomial f := X4+X3+1 over Z2. We claim
that f is irreducible. It suﬃces to show that f has no irreducible factors of
degree 1 or 2.
If f had a factor of degree 1, then it would have a root; however, f(0) =
0 + 0 + 1 = 1 and f(1) = 1 + 1 + 1 = 1. So f has no factors of degree 1.
Does f have a factor of degree 2? The polynomials of degree 2 are X2,
X2 + X, X2 + 1, and X2 + X + 1. The ﬁrst and second of these polynomials
are divisible by X, and hence not irreducible, while the third has a 1 as a
root, and hence is also not irreducible. The last polynomial, X2 + X + 1, has
no roots, and hence is the only irreducible polynomial of degree 2 over Z2.
So now we may conclude that if f were not irreducible, it would have to be
equal to
(X2 + X + 1)2 = X4 + 2X3 + 3X2 + 2X + 1 = X4 + X2 + 1,
which it is not.
Thus, E := Z2[X]/(f) is a ﬁeld with 24 = 16 elements. We may think of
elements E as bit strings of length 4, where the rule for addition is bit-wise
“exclusive-or.” The rule for multiplication is more complicated: to multiply
two given bit strings, we interpret the bits as coeﬃcients of polynomials
(with the left-most bit the coeﬃcient of X3), multiply the polynomials, reduce
the product modulo f, and write down the bit string corresponding to the
reduced product polynomial. For example, to multiply 1001 and 0011, we
compute
(X3 + 1)(X + 1) = X4 + X3 + X + 1,
and
(X4 + X3 + X + 1) mod (X4 + X3 + 1) = X.
Hence, the product of 1001 and 0011 is 0010.
Theorem 9.16 says that E∗is a cyclic group. Indeed, the element η :=
0010 (i.e., η = [X]f) is a generator for E∗, as the following table of powers
shows:
TEAM LinG

376
More rings
i
ηi
i
ηi
1
0010
8
1110
2
0100
9
0101
3
1000
10
1010
4
1001
11
1101
5
1011
12
0011
6
1111
13
0110
7
0111
14
1100
15
0001
Such a table of powers is sometimes useful for computations in small
ﬁnite ﬁelds such as this one.
Given α, β ∈E∗, we can compute αβ by
obtaining (by table lookup) i, j such that α = ηi and β = ηj, computing
k := (i + j) mod 15, and then obtaining αβ = ηk (again by table lookup).
2
Exercise 17.8. In the ﬁeld E is Example 17.9, what is the minimal poly-
nomial of 1011 over Z2?
Exercise 17.9. Show that if the factorization of f over F[X] into irreducibles
is as f = fe1
1 · · · fer
r , and if α = [h]f ∈F[X]/(f), then the minimal polynomial
φ of α over F is lcm(φ1, . . . , φr), where each φi is the minimal polynomial
of [h]fei
i
∈F[X]/(fei
i ) over F.
17.6 General properties of extension ﬁelds
We now discuss a few general notions related to extension ﬁelds. These are
all quite simple applications of the theory developed so far. Recall that if F
and E are ﬁelds, with F being a subring of E, then E is called an extension
ﬁeld of F. As usual, we shall blur the distinction between a subring and a
canonical embedding; that is, if τ : F →E is an canonical embedding, we
shall simply identify elements of F with their images in E under τ, and in
so doing, we may view E as an extension ﬁeld of F. Usually, the map τ
will be clear from context; for example, if E = F[X]/(φ) for some irreducible
polynomial φ ∈F[X], then we shall simply say that E is an extension ﬁeld of
F, although strictly speaking, F is embedded in E via the map that sends
a ∈F to [a]φ ∈E.
Let E be an extension ﬁeld of a ﬁeld F. Then E is an F-algebra, and in
particular, an F-vector space. If E is a ﬁnite dimensional F-vector space,
then we say that E is a ﬁnite extension of F, and dimF (E) is called the
TEAM LinG

17.6 General properties of extension ﬁelds
377
degree of the extension, and is denoted (E : F); otherwise, we say that E
is an inﬁnite extension of F.
An element α ∈E is called algebraic over F if there exists a non-zero
polynomial f ∈F[X] such that f(α) = 0; otherwise, α is called transcen-
dental over F. If all elements of E are algebraic over F, then we call E an
algebraic extension of F. From the discussion on minimal polynomials
in §17.5, we may immediately state:
Theorem 17.18. If E is a ﬁnite extension of F, then E is also an algebraic
extension of F.
Suppose α ∈E is algebraic over F. Let φ be its minimal polynomial,
so that F[X]/(φ) is isomorphic (as an F-algebra) to F[α]. Since F[α] is a
subring of a ﬁeld, it must be an integral domain, which implies that φ is
irreducible, which in turn implies that F[α] is a subﬁeld of E. Moreover,
the degree (F[α] : F) is equal to the degree of φ, and this number is called
the degree of α (over F). It is clear that if E is ﬁnite dimensional, then
the degree of α is at most (E : F).
Suppose that α ∈E is transcendental over F. Consider the “rational
function evaluation map” that sends f/g ∈F(X) to f(α)/g(α) ∈E. Since
no non-zero polynomial over F vanishes at α, it is easy to see that this map
is well deﬁned, and is in fact an injective F-algebra homomorphism from
F(X) into E. The image is denoted F(α), and this is clearly a subﬁeld of
E containing F and α, and it is plain to see that it is the smallest such
subﬁeld. It is also clear that F(α) has inﬁnite dimension over F, since it
contains an isomorphic copy of the inﬁnite dimensional vector space F[X].
More generally, for any α ∈E, algebraic or transcendental, we can deﬁne
F(α) to be the set consisting of all elements of the form f(α)/g(α) ∈E,
where f, g ∈F[X] and g(α) ̸= 0. It is clear that F(α) is a ﬁeld, and indeed,
it is the smallest subﬁeld of E containing F and α. If α is algebraic, then
F(α) = F[α], and is isomorphic (as an F-algebra) to F[X]/(φ), where φ is
the minimal polynomial of α over F; otherwise, if α is transcendental, then
F(α) is isomorphic (as an F-algebra) to the rational function ﬁeld F(X).
Example 17.10. If f ∈F[X] is monic and irreducible, E = F[X]/(f), and
η := [X]f ∈E, then η is algebraic over F, its minimal polynomial over F is
f, and its degree over F is equal to deg(f). Also, we have E = F[η], and
any element α ∈E is algebraic of degree at most deg(f). 2
Exercise 17.10. In the ﬁeld E is Example 17.9, ﬁnd all the elements of
degree 2 over Z2.
TEAM LinG

378
More rings
Exercise 17.11. Show that if E is a ﬁnite extension of F, with a basis
α1, . . . , αn over F, and K is a ﬁnite extension of E, with a basis β1, . . . , βm
over E, then
αiβj
(i = 1, . . . , n; j = 1, . . . , m)
is a basis for K over F, and hence K is a ﬁnite extension of F and
(K : F) = (K : E)(E : F).
Exercise 17.12. Show that if E is an algebraic extension of F, and K is
an algebraic extension of E, then K is an algebraic extension of F.
Exercise 17.13. Let E be an extension of F.
Show that the set of all
elements in E that are algebraic over F is a subﬁeld of E containing F.
We close this section with a discussion of a splitting ﬁeld — a ﬁnite
extension of the coeﬃcient ﬁeld in which a given polynomial splits completely
into linear factors. As the next theorem shows, splitting ﬁelds always exist.
Theorem 17.19. Let F be a ﬁeld, and f ∈F[X] a monic polynomial of
degree ℓ. Then there exists a ﬁnite extension K of F in which f factors as
f = (X −α1)(X −α2) · · · (X −αℓ),
with α1, . . . , αℓ∈K.
Proof. We prove the existence of K by induction on the degree ℓof f. If
ℓ= 0, then the theorem is trivially true. Otherwise, let g be an irreducible
factor of f, and set E := F[X]/(g), so that α := [X]g is a root of g, and hence
of f, in E. So over the extension ﬁeld E, f factors as
f = (X −α)h,
where h ∈E[X] is a polynomial of degree ℓ−1. Applying the induction
hypothesis, there exists a ﬁnite extension K of E such that h splits into
linear factors over K. Thus, over K, f splits into linear factors, and by
Exercise 17.11, K is a ﬁnite extension of F. 2
17.7 Formal power series and Laurent series
We discuss generalizations of polynomials that allow an inﬁnite number of
non-zero coeﬃcients. Although we are mainly interested in the case where
the coeﬃcients come from a ﬁeld F, we develop the basic theory for general
rings R.
TEAM LinG

17.7 Formal power series and Laurent series
379
17.7.1 Formal power series
The ring R[[X]] of formal power series over R consists of all formal ex-
pressions of the form
a = a0 + a1X + a2X2 + · · · ,
where a0, a1, a2, . . . ∈R. Unlike ordinary polynomials, we allow an inﬁnite
number of non-zero coeﬃcients. We may write such a formal power series
as
a =
∞

i=0
aiXi.
The rules for addition and multiplication of formal power series are exactly
the same as for polynomials. Indeed, the formulas (9.1) and (9.2) in §9.2
for addition and multiplication may be applied directly—all of the relevant
sums are ﬁnite, and so everything is well deﬁned.
We shall not attempt to interpret a formal power series as a function, and
therefore, “convergence” issues shall simply not arise.
Clearly, R[[X]] contains R[X] as a subring. Let us consider the group of
units of R[[X]].
Theorem 17.20. Let a = ∞
i=0 aiXi ∈R[[X]]. Then a ∈(R[[X]])∗if and only
if a0 ∈R∗.
Proof. If a0 is not a unit, then it is clear that a is not a unit, since the
constant term of a product formal power series is equal to the product of
the constant terms.
Conversely, if a0 is a unit, we show how to deﬁne the coeﬃcients of the
inverse b = ∞
i=0 biXi of a. Let ab = c = ∞
i=0 ciXi. We want c = 1, meaning
that c0 = 1 and ci = 0 for all i > 0. Now, c0 = a0b0, so we set b0 := a−1
0 .
Next, we have c1 = a0b1 + a1b0, so we set b1 := −a1b0 · a−1
0 . Next, we have
c2 = a0b2 + a1b1 + a2b0, so we set b2 := −(a1b1 + a2b0) · a−1
0 . Continuing in
this way, we see that if we deﬁne bi := −(a1bi−1 + · · · + aib0) · a−1
0
for i ≥1,
then ab = 1. 2
Example 17.11. In the ring R[[X]], the multiplicative inverse of 1 −X is
∞
i=0 Xi. 2
Exercise 17.14. For a ﬁeld F, show that any non-zero ideal of F[[X]] is of
the form (Xm) for some uniquely determined integer m ≥0.
TEAM LinG

380
More rings
17.7.2 Formal Laurent series
One may generalize formal power series to allow a ﬁnite number of negative
powers of X. The ring R((X)) of formal Laurent series over R consists of
all formal expressions of the form
a = amXm + am+1Xm+1 + · · · ,
where m is allowed to be any integer (possibly negative), and am, am+1, . . . ∈
R. Thus, elements of R((X)) may have an inﬁnite number of terms involving
positive powers of X, but only a ﬁnite number of terms involving negative
powers of X. We may write such a formal Laurent series as
a =
∞

i=m
aiXi.
The rules for addition and multiplication of formal Laurent series are just
as one would expect: if
a =
∞

i=m
aiXi and b =
∞

i=m
biXi,
then
a + b :=
∞

i=m
(ai + bi)Xi,
(17.6)
and
a · b :=
∞

i=2m
i−m

k=m
akbi−k

Xi.
(17.7)
We leave it to the reader to verify that R((X)) is a ring containing R[[X]].
Theorem 17.21. If D is an integral domain, then D((X)) is an integral
domain.
Proof. Let a = ∞
i=m aiXi and b = ∞
i=n biXi, where am ̸= 0 and bn ̸= 0.
Then ab = ∞
i=m+n ci, where cm+n = ambn ̸= 0. 2
Theorem 17.22. Let a ∈R((X)), and suppose that a ̸= 0 and a = ∞
i=m aiXi
with am ∈R∗. Then a has a multiplicative inverse in R((X)).
Proof. We can write a = Xmb, where b is a formal power series whose constant
term is a unit, and hence there is a formal power series c such that bc = 1.
Thus, X−mc is the multiplicative inverse of a in R((X)). 2
As an immediate corollary, we have:
TEAM LinG

17.7 Formal power series and Laurent series
381
Theorem 17.23. If F is a ﬁeld, then F((X)) is a ﬁeld.
Exercise 17.15. Show that for a ﬁeld F, F((X)) is the ﬁeld of fractions of
F[[X]]; that is, there is no proper subﬁeld of F((X)) that contains F[[X]].
17.7.3 Reversed formal Laurent series
While formal Laurent series are useful in some situations, in many others,
it is more useful and natural to consider reversed formal Laurent series
over R. These are formal expressions of the form
a =
m

i=−∞
aiXi,
where am, am−1, . . . ∈R. Thus, in a reversed formal Laurent series, we allow
an inﬁnite number of terms involving negative powers of X, but only a ﬁnite
number of terms involving positive powers of X.
The rules for addition and multiplication of reversed formal Laurent series
are just as one would expect: if
a =
m

i=−∞
aiXi and b =
m

i=−∞
biXi,
then
a + b :=
m

i=−∞
(ai + bi)Xi,
(17.8)
and
a · b :=
2m

i=−∞

m

k=i−m
akbi−k

Xi.
(17.9)
The ring of all reversed formal Laurent series is denoted R((X−1)), and as
the notation suggests, the map that sends X to X−1 (and acts as the identity
on R) is an isomorphism of R((X)) with R((X−1)).
Now, for any a = m
i=−∞aiXi ∈R((X−1)) with am ̸= 0, let us deﬁne the
degree of a, denoted deg(a), to be the value m, and the leading coeﬃ-
cient of a, denoted lc(a), to be the value am. As for ordinary polynomials,
we deﬁne the degree of 0 to be −∞, and the leading coeﬃcient of 0 to be 0.
Note that if a happens to be a polynomial, then these deﬁnitions of degree
and leading coeﬃcient agree with that for ordinary polynomials.
TEAM LinG

382
More rings
Theorem 17.24. For a, b ∈R((X−1)), we have deg(ab) ≤deg(a) + deg(b),
where equality holds unless both lc(a) and lc(b) are zero divisors.
Fur-
thermore, if b ̸= 0 and lc(b) is a unit, then b is a unit, and we have
deg(ab−1) = deg(a) −deg(b).
Proof. Exercise. 2
It is also natural to deﬁne a ﬂoor function for reversed formal Laurent
series: for a ∈R((X−1)) with a = m
i=−∞aiXi, we deﬁne
⌊a⌋:=
m

i=0
aiXi ∈R[X];
that is, we compute the ﬂoor function by simply throwing away all terms
involving negative powers of X.
Now, let a, b ∈R[X] with b ̸= 0 and lc(b) a unit, and using the usual
division with remainder property for polynomials, write a = bq + r, where
q, r ∈R[X] with deg(r) < deg(b). Let b−1 denote the multiplicative inverse of
b in R((X−1)). It is not too hard to see that ⌊ab−1⌋= q; indeed, multiplying
the equation a = bq+r by b−1, we obtain ab−1 = q+rb−1, and deg(rb−1) < 0,
from which it follows that ⌊ab−1⌋= q.
Let F be a ﬁeld. Since F((X−1)) is isomorphic to F((X)), and the latter
is a ﬁeld, it follows that F((X−1)) is a ﬁeld. Now, F((X−1)) contains F[X]
as a subring, and hence contains (an isomorphic copy) of F(X).
Just as
F(X) corresponds to the ﬁeld of rational numbers, F((X−1)) corresponds to
the ﬁeld real numbers. Indeed, we can think of real numbers as decimal
numbers with a ﬁnite number of digits to the left of the decimal point
and an inﬁnite number to the right, and reversed formal Laurent series
have a similar “syntactic” structure. In many ways, this syntactic similarity
between the real numbers and reversed formal Laurent series is more than
just superﬁcial.
Exercise 17.16. Write down the rule for determining the multiplicative
inverse of an element of R((X−1)) whose leading coeﬃcient is a unit in R.
Exercise 17.17. Let F be a ﬁeld of characteristic other than 2. Show that
a non-zero z ∈F((X−1)) has a square-root in z ∈F((X−1)) if and only if
deg(z) is even and lc(z) has a square-root in F.
Exercise 17.18. Let R be a ring, and let α ∈R. Show that the multiplica-
tive inverse of X −α in R((X−1)) is ∞
j=1 αj−1X−j.
TEAM LinG

17.8 Unique factorization domains (∗)
383
Exercise 17.19. Let R be an arbitrary ring, let α1, . . . , αℓ∈R, and let
f := (X −α1)(X −α2) · · · (X −αℓ) ∈R[X].
For j ≥0, deﬁne the “power sum”
sj :=
ℓ

i=1
αj
i.
Show that in the ring R((X−1)), we have
D(f)
f
=
ℓ

i=1
1
(X −αi) =
∞

j=1
sj−1X−j,
where D(f) is the formal derivative of f.
Exercise 17.20. Continuing with the previous exercise, derive Newton’s
identities, which state that if f = Xℓ+f1Xℓ−1+· · ·+fℓ, with f1, . . . , fℓ∈R,
then
s1 + f1 = 0
s2 + f1s1 + 2f2 = 0
s3 + f1s2 + f2s1 + 3f3 = 0
...
sℓ+ f1sℓ−1 + · · · + fℓ−1s1 + ℓfℓ= 0
sj+ℓ+ f1sj+ℓ−1 + · · · + fℓ−1sj+1 + fℓsj = 0 (j ≥1).
17.8 Unique factorization domains (∗)
As we have seen, both the integers and the ring F[X] of polynomials over
a ﬁeld enjoy a unique factorization property. These are special cases of a
more general phenomenon, which we explore here.
Throughout this section, D denotes an integral domain.
We call a, b ∈D associate if a = ub for some u ∈D∗. Equivalently, a
and b are associate if and only if a | b and b | a. A non-zero element p ∈D
is called irreducible if it is not a unit, and all divisors of p are associate to
1 or p. Equivalently, a non-zero, non-unit p ∈D is irreducible if and only if
it cannot be expressed as p = ab where neither a nor b are units.
TEAM LinG

384
More rings
Deﬁnition 17.25. We call D a unique factorization domain (UFD)
if
(i) every non-zero element of D that is not a unit can be written as a
product of irreducibles in D, and
(ii) such a factorization into irreducibles is unique up to associates and
the order in which the factors appear.
Another way to state part (ii) of the above deﬁnition is that if p1 · · · pr and
p′
1 · · · p′
s are two factorizations of some element as a product of irreducibles,
then r = s, and there exists a permutation π on the indices {1, . . . , r} such
that pi and p′
π(i) are associate.
As we have seen, both Z and F[X] are UFDs. In both of those cases,
we chose to single out a distinguished irreducible element among all those
associate to any given irreducible: for Z, we always chose p to be positive,
and for F[X], we chose p to be monic. For any speciﬁc unique factorization
domain D, there may be such a natural choice, but in the general case, there
will not be (see Exercise 17.21 below).
Example 17.12. Having already seen two examples of UFDs, it is perhaps
a good idea to look at an example of an integral domain that is not a UFD.
Consider the subring Z[√−3] of the complex numbers, which consists of all
complex numbers of the form a+b√−3, where a, b ∈Z. As this is a subring
of the ﬁeld C, it is an integral domain (one may also view Z[√−3] as the
quotient ring Z[X]/(X2 + 3)).
Let us ﬁrst determine the units in Z[√−3]. For a, b ∈Z, we have N(a +
b√−3) = a2 + 3b2, where N is the usual norm map on C (see Example 9.5).
If α ∈Z[√−3] is a unit, then there exists α′ ∈Z[√−3] such that αα′ = 1.
Taking norms, we obtain
1 = N(1) = N(αα′) = N(α)N(α′).
Since the norm of an element of Z[√−3] is a non-negative integer, this
implies that N(α) = 1. If α = a+b√−3, with a, b ∈Z, then N(α) = a2+3b2,
and it is clear that N(α) = 1 if and only if α = ±1. We conclude that the
only units in Z[√−3] are ±1.
Now consider the following two factorizations of 4 in Z[√−3]:
4 = 2 · 2 = (1 +
√
−3)(1 −
√
−3).
(17.10)
We claim that 2 is irreducible.
For suppose, say, that 2 = αα′, for
α, α′ ∈Z[√−3], with neither a unit. Taking norms, we have 4 = N(2) =
N(α)N(α′), and therefore, N(α) = N(α′) = 2—but this is impossible, since
TEAM LinG

17.8 Unique factorization domains (∗)
385
there are no integers a and b such that a2 +3b2 = 2. By the same reasoning,
since N(1+√−3) = N(1−√−3) = 4, we see that 1+√−3 and 1−√−3 are
both irreducible. Further, it is clear that 2 is not associate to either 1+√−3
or 1 −√−3, and so the two factorizations of 4 in (17.10) are fundamentally
diﬀerent. 2
For a, b ∈D, we call d ∈D a common divisor of a and b if d | a and
d | b; moreover, we call such a d a greatest common divisor of a and
b if all other common divisors of a and b divide d. We say that a and b
are relatively prime if the only common divisors of a and b are units.
It is immediate from the deﬁnition of a greatest common divisor that it is
unique, up to multiplication by units, if it exists at all. Unlike in the case of
Z and F[X], in the general setting, greatest common divisors need not exist;
moreover, even when they do, we shall not attempt to “normalize” greatest
common divisors, and we shall speak only of “a” greatest common divisor,
rather than “the” greatest common divisor.
Just as for integers and polynomials, we can generalize the notion of a
greatest common divisor in an arbitrary integral domain D from two to any
number of elements of D, and we can also deﬁne a least common multiple
of any number of elements as well.
Although these greatest common divisors and least common multiples
need not exist in an arbitrary integral domain D, if D is a UFD, they will
always exist. The existence question easily reduces to the question of the
existence of a greatest common divisor and least common multiple of a and
b, where a and b are non-zero elements of D. So assuming that D is a UFD,
we may write
a = u
r

i=1
pei
i
and b = v
r

i=1
pfi
i ,
where u and v are units, p1, . . . , pr are non-associate irreducibles, and the
ei and fi are non-negative integers, and it is easily seen that
r

i=1
pmin(ei,fi)
is a greatest common divisor of a and b, while
r

i=1
pmax(ei,fi)
is a least common multiple of a and b.
It is also evident that in a UFD D, if c | ab and c and a are relatively
TEAM LinG

386
More rings
prime, then c | b. In particular, if p is irreducible and p | ab, then p | a or
p | b. From this, we see that if p is irreducible, then the quotient ring D/pD
is an integral domain, and so the ideal pD is a prime ideal (see discussion
above Exercise 9.28).
In a general integral domain D, we say that an element p ∈D is prime
if for all a, b ∈D, p | ab implies p | a or p | b (which is equivalent to saying
that the ideal pD is prime). Thus, if D is a UFD, then all irreducibles are
primes; however, in a general integral domain, this may not be the case.
Here are a couple of simple but useful facts whose proofs we leave to the
reader.
Theorem 17.26. Any prime element in D is irreducible.
Proof. Exercise. 2
Theorem 17.27. Suppose D satisﬁes part (i) of Deﬁnition 17.25. Also,
suppose that all irreducibles in D are prime. Then D is a UFD.
Proof. Exercise. 2
Exercise 17.21.
(a) Show that the “is associate to” relation is an equiv-
alence relation.
(b) Consider an equivalence class C induced by the “is associate to”
relation. Show that if C contains an irreducible element, then all
elements of C are irreducible.
(c) Suppose that for every equivalence class C that contains irreducibles,
we choose one element of C, and call it a distinguished irreducible.
Show that D is a UFD if and only if every non-zero element of D can
be expressed as u · pe1
1 · · · per
r , where u is a unit, p1, . . . , pr are distin-
guished irreducibles, and this expression is unique up to a reordering
of the pi.
Exercise 17.22. Show that the ring Z[√−5] is not a UFD.
Exercise 17.23. Let D be a UFD and F its ﬁeld of fractions. Show that
(a) every element x ∈F can be expressed as x = a/b, where a, b ∈D are
relatively prime, and
(b) that if x = a/b for a, b ∈D relatively prime, then for any other
a′, b′ ∈D with x = a′/b′, we have a′ = ca and b′ = cb for some c ∈D.
Exercise 17.24. Let D be a UFD and let p ∈D be irreducible. Show that
there is no prime ideal Q of D with {0D} ⊊Q ⊊pD.
TEAM LinG

17.8 Unique factorization domains (∗)
387
17.8.1 Unique factorization in Euclidean and principal ideal
domains
Our proofs of the unique factorization property in both Z and F[X] hinged
on the division with remainder property for these rings. This notion can be
generalized, as follows.
Deﬁnition 17.28. D is said to be a Euclidean domain if there is a “size
function” S mapping the non-zero elements of D to the set of non-negative
integers, such that for a, b ∈D with b ̸= 0, there exist q, r ∈D, with the
property that a = bq + r and either r = 0 or S(r) < S(b).
Example 17.13. Both Z and F[X] are Euclidean domains. In Z, we can
take the ordinary absolute value function |·| as a size function, and for F[X],
the function deg(·) will do. 2
Example 17.14. Recall again the ring
Z[i] = {a + bi : a, b ∈Z}
of Gaussian integers from Example 9.22. Let us show that this is a Euclidean
domain, using the usual norm map N on complex numbers (see Example 9.5)
for the size function. Let α, β ∈Z[i], with β ̸= 0. We want to show the
existence of ξ, ρ ∈Z[i] such that α = βξ + ρ, where N(ρ) < N(β). Suppose
that in the ﬁeld C, we compute αβ−1 = r + si, where r, s ∈Q. Let m, n be
integers such that |m −r| ≤1/2 and |n −s| ≤1/2—such integers m and n
always exist, but may not be uniquely determined. Set ξ := m + ni ∈Z[i]
and ρ := α −βξ. Then we have
αβ−1 = ξ + δ,
where δ ∈C with N(δ) ≤1/4 + 1/4 = 1/2, and
ρ = α −βξ = α −β(αβ−1 −δ) = δβ,
and hence
N(ρ) = N(δβ) = N(δ)N(β) ≤1
2N(β). 2
Theorem 17.29. If D is a Euclidean domain and I is an ideal of D, then
there exists d ∈D such that I = dD.
Proof. If I = {0}, then d = 0 does the job, so let us assume that I ̸= {0}.
Let d be an non-zero element of I such that S(d) is minimal, where S is a
size function that makes D into a Euclidean domain. We claim that I = dD.
It will suﬃce to show that for all c ∈I, we have d | c. Now, we know
TEAM LinG

388
More rings
that there exists q, r ∈D such that c = qd + r, where either r = 0 or
S(r) < S(d). If r = 0, we are done; otherwise, r is a non-zero element of I
with S(r) < S(d), contradicting the minimality of S(d). 2
Recall that an ideal of the form I = dD is called a principal ideal. If
all ideals of D are principal, then D is called a principal ideal domain
(PID). Theorem 17.29 says that any Euclidean domain is a PID.
PIDs enjoy many nice properties, including:
Theorem 17.30. If D is a PID, then D is a UFD.
For the rings Z and F[X], the proof of part (i) of Deﬁnition 17.25 was
a quite straightforward induction argument (as it also would be for any
Euclidean domain). For a general PID, however, this requires a diﬀerent
sort of argument. We begin with the following fact:
Theorem 17.31. If D is a PID, and I1 ⊆I2 ⊆· · · is an ascending chain
of ideals of D, then there exists an integer k such that Ik = Ik+1 = · · · .
Proof. Let I := ∞
i=1 Ii. It is easy to see that I is an ideal. Thus, I = dD for
some d ∈D. But d ∈∞
i=1 Ii implies that d ∈Ik for some k, which shows
that I = dD ⊆Ik. It follows that I = Ik = Ik+1 = · · · . 2
We can now prove the existence part of Theorem 17.30:
Theorem 17.32. If D is a PID, then every non-zero, non-unit element of
D can be expressed as a product of irreducibles in D.
Proof. Let n ∈D, n ̸= 0, and n not a unit. If n is irreducible, we are done.
Otherwise, we can write n = ab, where neither a nor b are units. As ideals,
we have nD ⊊aD and nD ⊊bD. If we continue this process recursively,
building up a “factorization tree” where n is at the root, a and b are the
children of n, and so on, then the recursion must stop, since any inﬁnite
path in the tree would give rise to a chain of ideals
nD = I1 ⊊I2 ⊊· · · ,
contradicting Theorem 17.31. 2
The proof of the uniqueness part of Theorem 17.30 is essentially the same
as for proofs we gave for Z and F[X].
Analogous to Theorems 1.6 and 17.8, we have:
Theorem 17.33. Let D be a PID. For any a, b ∈D, there exists a greatest
common divisor d of a and b, and moreover, aD + bD = dD.
TEAM LinG

17.8 Unique factorization domains (∗)
389
Proof. Exercise. 2
As an immediate consequence of the previous theorem, we see that in a
PID D, for all a, b ∈D with greatest common divisor d, there exist s, t ∈D
such that as + bt = d; moreover, a, b ∈D are relatively prime if and only if
there exist s, t ∈D such that as + bt = 1.
Analogous to Theorems 1.7 and 17.9, we have:
Theorem 17.34. Let D be a PID. For a, b, c ∈D such that c | ab and a
and c are relatively prime, we have c | b.
Proof. Exercise. 2
Analogous to Theorems 1.8 and 17.10, we have:
Theorem 17.35. Let D be a PID. Let p ∈D be irreducible, and let a, b ∈D.
Then p | ab implies that p | a or p | b. That is, all irreducibles in D are
prime.
Proof. Exercise. 2
Theorem 17.30 now follows immediately from Theorems 17.32, 17.35, and
17.27.
Exercise 17.25. Show that Z[√−2] is a Euclidean domain.
Exercise 17.26. Consider the polynomial
X3 −1 = (X −1)(X2 + X + 1).
Over C, the roots of X3 −1 are 1, (−1 ± √−3)/2. Let ω := (−1 + √−3)/2,
and note that ω2 = −1 −ω = (−1 −√−3)/2, and ω3 = 1.
(a) Show that the ring Z[ω] consists of all elements of the form a + bω,
where a, b ∈Z, and is an integral domain. This ring is called the ring
of Eisenstein integers.
(b) Show that the only units in Z[ω] are ±1, ±ω, and ±ω2.
(c) Show that Z[ω] is a Euclidean domain.
Exercise 17.27. Show that in a PID, all non-zero prime ideals are maximal.
Recall that for a complex number α = a + bi, with a, b ∈R, the norm of
α was deﬁned as N(α) = α¯α = a2 + b2 (see Example 9.5). There are other
measures of the “size” of a complex number that are useful. The absolute
value of α is deﬁned as |α| :=
"
N(α) =
√
a2 + b2. The max norm of α is
deﬁned as M(α) := max{|a|, |b|}.
TEAM LinG

390
More rings
Exercise 17.28. Let α, β ∈C. Prove the following statements.
(a) |αβ| = |α||β|.
(b) |α + β| ≤|α| + |β|.
(c) N(α + β) ≤2(N(α) + N(β)).
(d) M(α) ≤|α| ≤
√
2M(α).
The following exercises develop algorithms for computing with Gaussian
integers.
We shall assume that for computational purposes, a Gaussian
integer α = a + bi, with a, b ∈Z, is represented as the pair of integers (a, b).
Exercise 17.29. Let α, β ∈Z[i].
(a) Show how to compute M(α) in time O(len(M(α))) and N(α) in time
O(len(M(α))2).
(b) Show how to compute α + β in time O(len(M(α)) + len(M(β))).
(c) Show how to compute α · β in time O(len(M(α)) · len(M(β))).
(d) Assuming β ̸= 0, show how to compute ξ, ρ ∈Z[i] such that α = βξ+
ρ, N(ρ) ≤1
2N(β), and N(ξ) ≤4N(α)/N(β). Your algorithm should
run in time O(len(M(α))·len(M(β))). Hint: see Example 17.14; also,
to achieve the stated running time bound, your algorithm should ﬁrst
test if M(β) ≥2M(α).
Exercise 17.30. Using the division with remainder algorithm from part
(d) of the previous exercise, adapt the Euclidean algorithm for (ordinary)
integers to work with Gaussian integers. On inputs α, β ∈Z[i], your algo-
rithm should compute a greatest common divisor δ ∈Z[i] of α and β in time
O(ℓ3), where ℓ:= max{len(M(α)), len(M(β))}.
Exercise 17.31. Extend the algorithm of the previous exercise, so that it
computes σ, τ ∈Z[i] such that ασ + βτ = δ. Your algorithm should run in
time O(ℓ3), and it should also be the case that len(M(σ)) and len(M(τ))
are O(ℓ).
The algorithms in the previous two exercises for computing greatest com-
mon divisors in Z[i] run in time cubic in the length of their input, whereas
the corresponding algorithms for Z run in time quadratic in the length of
their input. This is essentially because the running time of the algorithm
for division with remainder discussed in Exercise 17.29 is insensitive to the
size of the quotient.
To get a quadratic-time algorithm for computing greatest common divisors
in Z[i], in the following exercises we shall develop an analog of the binary
gcd algorithm for Z.
TEAM LinG

17.8 Unique factorization domains (∗)
391
Exercise 17.32. Let π := 1 + i ∈Z[i].
(a) Show that 2 = π¯π = −iπ2, that N(π) = 2, and that π is irreducible
in Z[i].
(b) Let α ∈Z[i], with α = a+bi for a, b ∈Z. Show that π | α if and only
if a −b is even, in which case
α
π = a + b
2
+ b −a
2
i.
(c) Show that for any α ∈Z[i], we have α ≡0 (mod π) or α ≡1 (mod π).
(d) Show that the quotient ring Z[i]/πZ[i] is isomorphic to the ring Z2.
(e) Show that for any α ∈Z[i] with α ≡1 (mod π), there exists a unique
ϵ ∈{±1, ±i} such that α ≡ϵ (mod 2π).
(f) Show that for any α, β ∈Z[i] with α ≡β ≡1 (mod π), there exists a
unique ϵ ∈{±1, ±i} such that α ≡ϵβ (mod 2π).
Exercise 17.33. We now present a “(1+i)-ary gcd algorithm” for Gaussian
integers. Let π := 1 + i ∈Z[i]. The algorithm takes non-zero α, β ∈Z[i] as
input, and runs as follows:
ρ ←α, ρ′ ←β, e ←0
while π | ρ and π | ρ′ do ρ ←ρ/π, ρ′ ←ρ′/π, e ←e + 1
repeat
while π | ρ do ρ ←ρ/π
while π | ρ′ do ρ′ ←ρ′/π
if M(ρ′) < M(ρ) then (ρ, ρ′) ←(ρ′, ρ)
determine ϵ ∈{±1, ±i} such that ρ′ ≡ϵρ (mod 2π)
(∗)
ρ′ ←ρ′ −ϵρ
until ρ′ = 0
δ ←πe · ρ
output δ
Show that this algorithm correctly computes a greatest common divisor
of α and β, and can be implemented so as to run in time O(ℓ2), where
ℓ:= max(len(M(α)), len(M(β))). Hint: to analyze the running time, for
i = 1, 2, . . . , let vi (respectively, v′
i) denote the value of |ρρ′| just before
(respectively, after) the execution of the line marked (∗) in loop iteration i,
and show that
v′
i ≤(1 +
√
2)vi and vi+1 ≤v′
i/2
√
2.
Exercise 17.34. Extend the algorithm of the previous exercise, so that it
computes σ, τ ∈Z[i] such that ασ + βτ = δ. Your algorithm should run in
TEAM LinG

392
More rings
time O(ℓ2), and it should also be the case that len(M(σ)) and len(M(τ))
are O(ℓ). Hint: adapt the algorithm in Exercise 4.2.
Exercise 17.35. In Exercise 17.32, we saw that 2 factors as −iπ2 in Z[i],
where π := 1 + i is irreducible. This exercise examines the factorization in
Z[i] of prime numbers p > 2.
(a) Suppose −1 is not congruent to the square of any integer modulo p.
Show that p is irreducible in Z[i].
(b) Suppose that c2 ≡−1 (mod p) for some c ∈Z. Let γ := c + i ∈Z[i]
and let δ be a greatest common divisor in Z[i] of γ and p. Show that
p = δ¯δ, and that δ and ¯δ are non-associate, irreducible elements of
Z[i].
17.8.2 Unique factorization in D[X]
In this section, we prove the following:
Theorem 17.36. If D is a UFD, then so is D[X].
This theorem implies, for example, that Z[X] is a UFD. Applying the
theorem inductively, one also sees that for any ﬁeld F, the ring F[X1, . . . , Xn]
of multi-variate polynomials over F is also a UFD.
We begin with some simple observations. First, recall that for an integral
domain D, D[X] is an integral domain, and the units in D[X] are precisely the
units in D. Second, it is easy to see that an element of D is irreducible in D if
and only if it is irreducible in D[X]. Third, for c ∈D and f = 
i aiXi ∈D[X],
we have c | f if and only if c | ai for all i.
We call a non-zero polynomial f ∈D[X] primitive if the only elements
in D that divide f are units.
If D is a UFD, then given any non-zero
polynomial f ∈D[X], we can write it as f = cf′, where c ∈D and f′ ∈D[X]
is a primitive polynomial: just take c to be a greatest common divisor of all
the coeﬃcients of f.
It is easy to prove the existence part of Theorem 17.36:
Theorem 17.37. Let D be a UFD. Any non-zero, non-unit element of D[X]
can be expressed as a product of irreducibles in D[X].
Proof. Let f be a non-zero, non-unit polynomial in D[X]. If f is a constant,
then because D is a UFD, it factors into irreducibles in D. So assume f
is not constant. If f is not primitive, we can write f = cf′, where c is a
non-zero, non-unit in D, and f′ is a primitive, non-constant polynomial in
D[X]. Again, as D is a UFD, c factors into irreducibles in D.
TEAM LinG

17.8 Unique factorization domains (∗)
393
From the above discussion, it suﬃces to prove the theorem for non-
constant, primitive polynomials f ∈D[X]. If f is itself irreducible, we are
done. Otherwise, then we can write f = gh, where g, h ∈D[X] and nei-
ther g nor h are units. Further, by the assumption that f is a primitive,
non-constant polynomial, both g and h must also be primitive, non-constant
polynomials; in particular, both g and h have degree strictly less than deg(f),
and the theorem follows by induction on degree. 2
The uniqueness part of Theorem 17.36 is (as usual) more diﬃcult. We
begin with the following fact:
Theorem 17.38. Let D be a UFD, let p be an irreducible in D, and let
f, g ∈D[X]. Then p | fg implies p | f or p | g.
Proof. Consider the quotient ring D/pD, which is an integral domain (be-
cause D is a UFD), and the corresponding ring of polynomials (D/pD)[X],
which is also an integral domain. Consider the natural map from D[X] to
(D/pD)[X] that sends a ∈D[X] to the polynomial ¯a ∈(D/pD)[X] obtained
by mapping each coeﬃcient of a to its residue class modulo p. If p | fg, then
we have
0 = fg = ¯f¯g,
and since (D/pD)[X] is an integral domain, it follows that ¯f = 0 or ¯g = 0,
which means that p | f or p | g. 2
Theorem 17.39. Let D be a UFD. The product of two primitive polynomi-
als in D[X] is also primitive.
Proof. Let f, g ∈D[X] be primitive polynomials, and let h := fg. If h is
not primitive, then m | h for some non-zero, non-unit m ∈D, and as D is a
UFD, there is some irreducible element p ∈D that divides m, and therefore,
divides h as well. By Theorem 17.38, it follows that p | f or p | g, which
implies that either f is not primitive or g is not primitive. 2
Suppose that D is a UFD and that F is its ﬁeld of fractions. Any non-zero
polynomial f ∈F[X] can always be written as f = (c/d)f′, where c, d ∈D,
with d ̸= 0, and f′ ∈D[X] is primitive. To see this, clear the denominators
of the coeﬃcients of f, writing df = f′′, where 0 ̸= d ∈D and f′′ ∈D[X].
Then take c to be a greatest common divisor of the coeﬃcients of f′′, so
that f′′ = cf′, where f′ ∈D[X] is primitive. Then we have f = (c/d)f′, as
required. Of course, we may assume that c and d are relatively prime—if
not, we may divide c and d by a greatest common divisor.
As a consequence of the previous theorem, we have:
TEAM LinG

394
More rings
Theorem 17.40. Let D be a UFD and let F be its ﬁeld of fractions. Let
f, g ∈D[X] and h ∈F[X] be non-zero polynomials such that f = gh and g is
primitive. Then h ∈D[X].
Proof. Write h = (c/d)h′, where c, d ∈D and h′ ∈D[X] is primitive. Let us
assume that c and d are relatively prime. Then we have
d · f = c · gh′.
(17.11)
We claim that d ∈D∗. To see this, note that (17.11) implies that d |
(c · gh′), and the assumption that c and d are relatively prime implies that
d | gh′. But by Theorem 17.39, gh′ is primitive, from which it follows that
d is a unit. That proves the claim.
It follows that c/d ∈D, and hence h = (c/d)h′ ∈D[X]. 2
Theorem 17.41. Let D be a UFD and F its ﬁeld of fractions. If f ∈D[X]
with deg(f) > 0 is irreducible, then f is also irreducible in F[X].
Proof. Suppose that f is not irreducible in F[X], so that f = gh for non-
constant polynomials g, h ∈F[X], both of degree strictly less than that of f.
We may write g = (c/d)g′, where c, d ∈D and g′ ∈D[X] is primitive. Set
h′ := (c/d)h, so that f = gh = g′h′. By Theorem 17.40, we have h′ ∈D[X],
and this shows that f is not irreducible in D[X]. 2
Theorem 17.42. Let D be a UFD. Let f ∈D[X] with deg(f) > 0 be
irreducible, and let g, h ∈D[X].
If f divides gh in D[X], then f divides
either g or h in D[X].
Proof. Suppose that f ∈D[X] with deg(f) > 0 is irreducible. This implies
that f is a primitive polynomial. By Theorem 17.41, f is irreducible in F[X],
where F is the ﬁeld of fractions of D. Suppose f divides gh in D[X]. Then
because F[X] is a UFD, f divides either g or h in F[X]. But Theorem 17.40
implies that f divides either g or h in D[X]. 2
Theorem 17.36 now follows immediately from Theorems 17.37, 17.38, and
17.42, together with Theorem 17.27.
In the proof of Theorem 17.36, there is a clear connection between factor-
ization in D[X] and F[X], where F is the ﬁeld of fractions of D. We should
perhaps make this connection more explicit. Suppose f ∈D[X] factors into
irreducibles in D[X] as
f = ca1
1 · · · car
r hb1
1 · · · hbs
s .
where the ci are non-associate, irreducible constants, and the hi are non-
TEAM LinG

17.8 Unique factorization domains (∗)
395
associate, irreducible, non-constant polynomials (and in particular, primi-
tive). By Theorem 17.41, the hi are irreducible in F[X]. Moreover, by The-
orem 17.40, the hi are non-associate in F[X]. Therefore, in F[X], f factors
as
f = chb1
1 · · · hbs
s ,
where c := ca1
1 · · · car
r
is a unit in F, and the hi are non-associate irreducible
polynomials in F[X].
Example 17.15. It is important to keep in mind the distinction between
factorization in D[X] and F[X]. Consider the polynomial 2X2 −2 ∈Z[X].
Over Z[X], this polynomial factors as 2(X −1)(X + 1), where each of these
three factors are irreducible in Z[X]. Over Q[X], this polynomial has two
irreducible factors, namely, X −1 and X + 1. 2
The following theorem provides a useful criterion for establishing that a
polynomial is irreducible.
Theorem 17.43 (Eisenstein’s criterion). Let D be a UFD and F its
ﬁeld of fractions. Let f = fnXn + fn−1Xn−1 + · · · + f0 ∈D[X]. If there exists
an irreducible p ∈D such that
p ∤fn, p | fn−1, · · · , p | f0, p2 ∤f0,
then f is irreducible over F.
Proof. Let f be as above, and suppose it were not irreducible in F[X]. Then
by Theorem 17.41, we could write f = gh, where g, h ∈D[X], both of degree
strictly less than that of f. Let us write
g = grXr + · · · + g0 and h = hsXs + · · · + h0,
where gr ̸= 0 and hs ̸= 0, so that 0 < r < n and 0 < s < n.
Now,
since fn = grhs, and p ∤fn, it follows that p ∤gr and p ∤hs. Further, since
f0 = g0h0, and p | f0 but p2 ∤f0, it follows that p divides one of g0 or h0, but
not both—for concreteness, let us assume that p | g0 but p ∤h0. Also, let t
be the smallest positive integer such that p ∤gt—note that 0 < t ≤r < n.
Now consider the natural map that sends c ∈D to ¯c ∈D/pD, which we
can extend coeﬃcient-wise to the map that sends a ∈D[X] to ¯a ∈(D/pD)[X].
Because D is a UFD and p is irreducible, both D/pD and (D/pD)[X] are
integral domains. Since f = gh, we have
¯fnXn = ¯f = ¯g¯h = (¯grXr + · · · + ¯gtXt)(¯hsXs + · · · + ¯h0).
(17.12)
TEAM LinG

396
More rings
But notice that when we multiply out the two polynomials on the right-
hand side of (17.12), the coeﬃcient of Xt is ¯gt¯h0 ̸= 0, and as t < n, this
clearly contradicts the fact that the coeﬃcient of Xt in the polynomial on
the left-hand side of (17.12) is zero. 2
As an application of Eisenstein’s criterion, we have:
Theorem 17.44. For any prime number q, the qth cyclotomic polynomial
Φq := Xq −1
X −1 = Xq−1 + Xq−2 + · · · + 1
is irreducible over Q.
Proof. Let
f := Φq

X + 1

= (X + 1)q −1
(X + 1) −1 .
It is easy to see that
f =
q−1

i=0
aiXi,
where ai =
 q
i + 1

(i = 0, . . . , q −1).
Thus, aq−1 = 1, a0 = q, and for 0 < i < q −1, we have q | ai (see
Exercise 1.12). Theorem 17.43 therefore applies, and we conclude that f is
irreducible over Q. It follows that Φq is irreducible over Q, since if Φq = gh
were a non-trivial factorization of Φq, then f = Φq

X + 1

= g

X + 1

·
h

X + 1

would be a non-trivial factorization of f. 2
Exercise 17.36. Show that neither Z[X] nor F[X, Y] (where F is a ﬁeld) are
PIDs (even though they are UFDs).
Exercise 17.37. Let f ∈Z[X] be a monic polynomial. Show that if f has a
root α ∈Q, then α ∈Z, and α divides the constant term of f.
Exercise 17.38. Let a be a non-zero, square-free integer, with a ̸∈{±1}.
For integer n ≥1, show that the polynomial Xn −a is irreducible in Q[X].
Exercise 17.39. Show that the polynomial X4 + 1 is irreducible in Q[X].
Exercise 17.40. Let F be a ﬁeld, and consider the ring of bivariate polyno-
mials F[X, Y]. Show that in this ring, the polynomial X2+Y2−1 is irreducible,
provided F does not have characteristic 2. What happens if F has charac-
teristic 2?
TEAM LinG

17.9 Notes
397
Exercise 17.41. Design and analyze an eﬃcient algorithm for the following
problem. The input is a pair of polynomials a, b ∈Z[X], along with their
greatest common divisor d in the ring Q[X].
The output is the greatest
common divisor of a and b the ring Z[X].
Exercise 17.42. Let a, b ∈Z[X] be non-zero polynomials with d :=
gcd(a, b) ∈Z[X]. Show that for any prime p not dividing lc(a) lc(b), we have
¯d | gcd(¯a,¯b), and except for ﬁnitely many primes p, we have ¯d = gcd(¯a,¯b).
Here, ¯d, ¯a, and ¯b denote the images of d, a, and b in Zp[X].
Exercise 17.43. Let F be a ﬁeld, and let f, g ∈F[X, Y]. Deﬁne V (f, g) :=
{(x, y) ∈F × F : f(x, y) = g(x, y) = 0F }.
Show that if f and g are
relatively prime, then V (f, g) is a ﬁnite set. Hint: consider the rings F(X)[Y]
and F(Y)[X].
17.9 Notes
The “(1 + i)-ary gcd algorithm” in Exercise 17.33 for computing greatest
common divisors of Gaussian integers is based on algorithms in Weilert
[100] and Damg˚ard and Frandsen [31].
The latter paper also develops a
corresponding algorithm for Eisenstein integers (see Exercise 17.26). Weilert
[101] presents an asymptotically fast algorithm that computes the greatest
common divisor of ℓ-bit Gaussian integers in time O(ℓ1+o(1)).
TEAM LinG

18
Polynomial arithmetic and applications
In this chapter, we study algorithms for performing arithmetic on poly-
nomials. Initially, we shall adopt a very general point of view, discussing
polynomials whose coeﬃcients lie in an arbitrary ring R, and then specialize
to the case where the coeﬃcient ring is a ﬁeld F.
There are many similarities between arithmetic in Z and in R[X], and the
similarities between Z and F[X] run even deeper. Many of the algorithms
we discuss in this chapter are quite similar to the corresponding algorithms
for integers.
As we did in Chapter 15 for matrices, we shall treat R as an “abstract
data type,” and measure the complexity of algorithms for polynomials over
a ring R by counting “operations in R.”
18.1 Basic arithmetic
Throughout this section, R denotes a non-trivial ring.
For computational purposes,
we assume that a polynomial a
=
k−1
i=0 aiXi ∈R[X] is represented as a coeﬃcient vector (a0, a1, . . . , ak−1).
Further, when a is non-zero, the coeﬃcient ak−1 should be non-zero.
The basic algorithms for addition, subtraction, multiplication, and divi-
sion of polynomials are quite straightforward adaptations of the correspond-
ing algorithms for integers. In fact, because of the lack of “carries,” these
algorithms are actually much simpler in the polynomial case. We brieﬂy
discuss these algorithms here—analogous to our treatment of integer arith-
metic, we do not discuss the details of “stripping” leading zero coeﬃcients.
For addition and subtraction, all we need to do is to add or subtract
coeﬃcient vectors.
For multiplication, let a = k−1
i=0 aiXi ∈R[X] and b = ℓ−1
i=0 biXi ∈R[X],
398
TEAM LinG

18.1 Basic arithmetic
399
where k ≥1 and ℓ≥1. The product c := a·b is of the form c = k+ℓ−2
i=0
ciXi,
and can be computed using O(kℓ) operations in R as follows:
for i ←0 to k + ℓ−2 do ci ←0
for i ←0 to k −1 do
for j ←0 to ℓ−1 do
ci+j ←ci+j + ai · bj
For division, let a = k−1
i=0 aiXi ∈R[X] and b = ℓ−1
i=0 biXi ∈R[X], where
bℓ−1 ∈R∗. We want to compute polynomials q, r ∈R[X] such that a = bq+r,
where deg(r) < ℓ−1. If k < ℓ, we can simply set q ←0 and r ←a; otherwise,
we can compute q and r using O(ℓ· (k −ℓ+ 1)) operations in R using the
following algorithm:
t ←b−1
ℓ−1 ∈R
for i ←0 to k −1 do ri ←ai
for i ←k −ℓdown to 0 do
qi ←t · ri+ℓ−1
for j ←0 to ℓ−1 do
ri+j ←ri+j −qi · bj
q ←k−ℓ
i=0 qiXi, r ←ℓ−2
i=0 riXi
With these simple algorithms, we obtain the polynomial analog of The-
orem 3.3. Let us deﬁne the length of a ∈R[X], denoted len(a), to be the
length of its coeﬃcient vector; more precisely, we deﬁne
len(a) :=
 deg(a) + 1
if a ̸= 0,
1
if a = 0.
It is sometimes more convenient to state the running times of algorithms in
terms of len(a), rather than deg(a) (the latter has the inconvenient habit of
taking on the value 0, or worse, −∞).
Theorem 18.1. Let a and b be arbitrary polynomials in R[X].
(i) We can compute a ± b with O(len(a) + len(b)) operations in R.
(ii) We can compute a · b with O(len(a) len(b)) operations in R.
(iii) If b ̸= 0 and lc(b) is a unit in R, we can compute q, r ∈R[X] such
that a = bq + r and deg(r) < deg(b) with O(len(b) len(q)) operations
in R.
Analogous to algorithms for modular integer arithmetic, we can also do
arithmetic in the residue class ring R[X]/(n), where n ∈R[X] is a polynomial
TEAM LinG

400
Polynomial arithmetic and applications
of degree ℓ> 0 whose leading coeﬃcient lc(n) is a unit (in most applications,
we may in fact assume that n is monic). For α ∈R[X]/(n), there exists a
unique polynomial a ∈R[X] with deg(a) < ℓand α = [a]n; we call this
polynomial a the canonical representative of α, and denote it by rep(α).
For computational purposes, we represent elements of R[X]/(n) by their
canonical representatives.
With this representation, addition and subtraction in R[X]/(n) can be
performed using O(ℓ) operations in R, while multiplication takes O(ℓ2) op-
erations in R.
The repeated-squaring algorithm for computing powers works equally well
in this setting: given α ∈R[X]/(n) and a non-negative exponent e, we can
compute αe using O(len(e)) multiplications in R[X]/(n), and so a total of
O(len(e) ℓ2) operations in R.
The following exercises deal with arithmetic with polynomials R[X] over
a ring R.
Exercise 18.1. State and re-work the polynomial analog of Exercise 3.22.
Exercise 18.2. State and re-work the polynomial analog of Exercise 3.23.
Assume n1, . . . , nk are monic polynomials.
Exercise 18.3. Given a polynomial g ∈R[X] and an element α ∈E, where
R is a subring of E, we may wish to compute g(α) ∈E. A particularly
elegant and eﬃcient way of doing this is called Horner’s rule. Suppose
g = k−1
i=0 giXi, where k ≥0 and gi ∈R for i = 0, . . . , k −1. Horner’s rule
computes g(α) as follows:
β ←0E
for i ←k −1 down to 0 do
β ←β · α + ai
output β
Show that this algorithm correctly computes g(α) using k multiplications in
E and k additions in E.
Exercise 18.4. Let f ∈R[X] be a monic polynomial of degree ℓ> 0, and
let E := R[X]/(f). Suppose that in addition to f, we are given a polynomial
g ∈R[X] of degree less than k and an element α ∈E, and we want to
compute g(α) ∈E.
(a) Show that a straightforward application of Horner’s rule yields an
algorithm that uses O(kℓ2) operations in R, and requires space for
storing O(ℓ) elements of R.
TEAM LinG

18.2 Computing minimal polynomials in F[X]/(f) (I)
401
(b) Show how to compute g(α) using just O(kℓ+ k1/2ℓ2) operations in
R, at the expense of requiring space for storing O(k1/2ℓ) elements of
R. Hint: ﬁrst compute a table of powers 1, α, . . . , αm, for m ≈k1/2.
Exercise 18.5. Given polynomials g, h ∈R[X], show how to compute the
composition g(h) ∈R[X] using O(len(g)2 len(h)2) operations in R.
Exercise 18.6. Suppose you are given three polynomials f, g, h ∈Zp[X],
where p is a large prime, in particular, p ≥2 deg(g) deg(h).
Design an
eﬃcient probabilistic algorithm that tests if f = g(h) (i.e., if f equals g
composed with h). Your algorithm should have the following properties: if
f = g(h), it should always output “true,” and otherwise, it should output
“false” with probability at least 0.999. The expected running time of your
algorithm should be O((len(f) + len(g) + len(h)) len(p)2).
18.2 Computing minimal polynomials in F[X]/(f) (I)
In this section, we shall examine a computational problem to which we
shall return on several occasions, as it will serve to illustrate a number of
interesting algebraic and algorithmic concepts.
Let F be a ﬁeld, f ∈F[X] a monic polynomial of degree ℓ> 0, and let
E := F[X]/(f). E is an F-algebra, and in particular, an F-vector space.
As an F-vector space, it has dimension ℓ. Suppose we are given an element
α ∈E, and want to eﬃciently compute the minimal polynomial of α over F,
that is, the monic polynomial φ ∈F[X] of least degree such that φ(α) = 0,
which we know has degree at most ℓ(see §17.5).
We can solve this problem using polynomial arithmetic and Gaussian
elimination, as follows. Consider the F-linear map ρ : F[X]≤ℓ→E that
sends a polynomial h ∈F[X] of degree at most ℓto h(α). Let us ﬁx ordered
bases for F[X]≤ℓand E: for F[X]≤ℓ, let us take Xℓ, Xℓ−1, . . . , 1, and for E, let
us take 1, η, . . . , ηℓ−1, where η := [X]f ∈E. The matrix A representing the
map ρ (via multiplication on the right by A), is the (ℓ+ 1) × ℓmatrix A
whose ith row, for i = 1, . . . , ℓ+ 1, is the coordinate vector of αℓ+1−i.
We apply Gaussian elimination to A to ﬁnd a set of row vectors v1, . . . , vs
that are coordinate vectors for a basis for the kernel of ρ. Now, the co-
ordinate vector of the minimal polynomial of α is a linear combination of
v1, . . . , vs. To ﬁnd it, we form the s × (ℓ+ 1) matrix B whose rows consist
of v1, . . . , vs, and apply Gaussian elimination to B, obtaining an s × (ℓ+ 1)
matrix B′ in reduced row echelon form whose row space is the same as that
of B. Let g be the polynomial whose coordinate vector is the last row of
B′. Because of the choice of ordered basis for F[X]≤ℓ, and because B′ is in
TEAM LinG

402
Polynomial arithmetic and applications
reduced row echelon form, it is clear that no non-zero polynomial in ker(ρ)
has degree less than that of g. Moreover, as g is already monic (again, by
the fact that B′ is in reduced row echelon form), it follows that g is in fact
the minimal polynomial of α over F.
The total amount of work performed by this algorithm is O(ℓ3) opera-
tions in F to build the matrix A (this just amounts to computing ℓsuc-
cessive powers of α, that is, O(ℓ) multiplications in E, each of which takes
O(ℓ2) operations in F), and O(ℓ3) operations in F to perform both Gaussian
elimination steps.
18.3 Euclid’s algorithm
In this section, F denotes a ﬁeld, and we consider the computation of great-
est common divisors in F[X].
The basic Euclidean algorithm for integers is easily adapted to compute
gcd(a, b), for polynomials a, b ∈F[X]. Analogous to the integer case, we
assume that deg(a) ≥deg(b); however, we shall also assume that a ̸= 0.
This is not a serious restriction, of course, as gcd(0, 0) = 0, and making
this restriction will simplify the presentation a bit. Recall that we deﬁned
gcd(a, b) to be either zero or monic, and the assumption that a ̸= 0 means
that gcd(a, b) is non-zero, and hence monic.
The following is the analog of Theorem 4.1.
Theorem 18.2. Let a, b ∈F[X], with deg(a) ≥deg(b) and a ̸= 0. Deﬁne
the polynomials r0, r1, . . . , rℓ+1 ∈F[X], and q1, . . . , qℓ∈F[X], where ℓ≥0,
as follows:
a = r0,
b = r1,
r0 = r1q1 + r2
(0 ≤deg(r2) < deg(r1)),
...
ri−1 = riqi + ri+1
(0 ≤deg(ri+1) < deg(ri)),
...
rℓ−2 = rℓ−1qℓ−1 + rℓ
(0 ≤deg(rℓ) < deg(rℓ−1)),
rℓ−1 = rℓqℓ
(rℓ+1 = 0).
Note that by deﬁnition, ℓ= 0 if b = 0, and ℓ> 0 otherwise; moreover,
rℓ̸= 0.
Then we have rℓ/ lc(rℓ) = gcd(a, b), and if b ̸= 0, then ℓ≤deg(b) + 1.
TEAM LinG

18.3 Euclid’s algorithm
403
Proof. Arguing as in the proof of Theorem 4.1, one sees that
gcd(a, b) = gcd(r0, r1) = gcd(rℓ, rℓ+1) = gcd(rℓ, 0) = rℓ/ lc(rℓ).
That proves the ﬁrst statement.
For the second statement, if b ̸= 0, then the degree sequence
deg(r1), deg(r2), . . . , deg(rℓ)
is strictly decreasing, with deg(rℓ) ≥0, from which it follows that deg(b) =
deg(r1) ≥ℓ−1. 2
This gives us the following Euclidean algorithm for polynomials, which
takes as input polynomials a, b ∈F[X] with deg(a) ≥deg(b) and a ̸= 0, and
which produces as output d = gcd(a, b) ∈F[X].
r ←a, r′ ←b
while r′ ̸= 0 do
r′′ ←r mod r′
(r, r′) ←(r′, r′′)
d ←r/ lc(r)
// make monic
output d
Theorem 18.3. Euclid’s algorithm for polynomials uses O(len(a) len(b))
operations in F.
Proof. The proof is almost identical to that of Theorem 4.2. Details are left
to the reader. 2
Just as for integers, if d = gcd(a, b), then aF[X] + bF[X] = dF[X], and so
there exist polynomials s and t such that as + bt = d. The procedure to
calculate s and t is precisely the same as in the integer case; however, in
the polynomial case, we can be much more precise about the relative sizes
of the objects involved in the calculation.
Theorem 18.4. Let a, b, r0, r1, . . . , rℓ+1 and q1, . . . , qℓbe as in Theo-
rem 18.2. Deﬁne polynomials s0, s1, . . . , sℓ+1 ∈F[X] and t0, t1, . . . , tℓ+1 ∈
F[X] as follows:
s0 := 1,
t0 := 0,
s1 := 0,
t1 := 1,
and for i = 1, . . . , ℓ,
si+1 := si−1 −siqi,
ti+1 := ti−1 −tiqi.
TEAM LinG

404
Polynomial arithmetic and applications
Then:
(i) for i = 0, . . . , ℓ+ 1, we have sia + tib = ri; in particular, sℓa + tℓb =
lc(rℓ) gcd(a, b);
(ii) for i = 0, . . . , ℓ, we have siti+1 −tisi+1 = (−1)i;
(iii) for i = 0, . . . , ℓ+ 1, we have gcd(si, ti) = 1;
(iv) for i = 1, . . . , ℓ+ 1, we have
deg(ti) = deg(a) −deg(ri−1),
and for i = 2, . . . , ℓ+ 1, we have
deg(si) = deg(b) −deg(ri−1).
Proof. (i), (ii), and (iii) are proved just as in the corresponding parts of
Theorem 4.3.
For (iv), the proof will hinge on the following facts:
• For i = 1, . . . , ℓ, we have deg(ri−1) ≥deg(ri), and since qi is the
quotient in dividing ri−1 by ri, we have deg(qi) = deg(ri−1)−deg(ri).
• For i = 2, . . . , ℓ, we have deg(ri−1) > deg(ri).
We prove the statement involving the ti by induction on i, and leave the
proof of the statement involving the si to the reader.
One can see by inspection that this statement holds for i = 1, since
deg(t1) = 0 and r0 = a. If ℓ= 0, there is nothing more to prove, so assume
that ℓ> 0 and b ̸= 0.
Now, for i = 2, we have t2 = 0 −1 · q1 = −q1. Thus, deg(t2) = deg(q1) =
deg(r0) −deg(r1) = deg(a) −deg(r1).
Now for the induction step. Assume i ≥3. Then we have
deg(ti−1qi−1) = deg(ti−1) + deg(qi−1)
= deg(a) −deg(ri−2) + deg(qi−1)
(by induction)
= deg(a) −deg(ri−1)
(since deg(qi−1) = deg(ri−2) −deg(ri−1))
> deg(a) −deg(ri−3)
(since deg(ri−3) > deg(ri−1))
= deg(ti−2)
(by induction).
By deﬁnition, ti = ti−2 −ti−1qi−1, and from the above reasoning, we see
that
deg(a) −deg(ri−1) = deg(ti−1qi−1) > deg(ti−2),
from which it follows that deg(ti) = deg(a) −deg(ri−1). 2
TEAM LinG

18.4 Computing modular inverses and Chinese remaindering
405
Note that part (iv) of the theorem implies that for i = 1, . . . , ℓ+ 1, we
have deg(ti) ≤deg(a) and deg(si) ≤deg(b). Moreover, if deg(a) > 0 and
b ̸= 0, then ℓ> 0 and deg(rℓ−1) > 0, and hence deg(tℓ) < deg(a) and
deg(sℓ) < deg(b).
We can easily turn the scheme described in Theorem 18.4 into a simple
algorithm, taking as input polynomials a, b ∈F[X], such that deg(a) ≥
deg(b) and a ̸= 0, and producing as output polynomials d, s, t ∈F[X] such
that d = gcd(a, b) and as + bt = d:
r ←a, r′ ←b
s ←1, s′ ←0
t ←0, t′ ←1
while r′ ̸= 0 do
Compute q, r′′ such that r = r′q + r′′, with deg(r′′) < deg(r′)
(r, s, t, r′, s′, t′) ←(r′, s′, t′, r′′, s −s′q, t −t′q)
c ←lc(r)
d ←r/c, s ←s/c, t ←t/c
// make monic
output d, s, t
Theorem 18.5. The extended Euclidean algorithm for polynomials uses
O(len(a) len(b)) operations in F.
Proof. Exercise. 2
18.4 Computing modular inverses and Chinese remaindering
In this and the remaining sections of this chapter, we explore various appli-
cations of Euclid’s algorithm for polynomials. Most of these applications are
analogous to their integer counterparts, although there are some diﬀerences
to watch for. Throughout this section, F denotes a ﬁeld.
We begin with the obvious application of the extended Euclidean algo-
rithm for polynomials to the problem of computing multiplicative inverses
in F[X]/(n), where n ∈F[X] with ℓ:= deg(n) > 0.
Given y ∈F[X] with deg(y) < ℓ, using O(ℓ2) operations in F, we can
determine if y is relatively prime to n, and if so, compute y−1 mod n as
follows. We run the extended Euclidean algorithm on inputs a := n and
b := y, obtaining polynomials d, s, t such that d = gcd(n, y) and ns+yt = d.
If d ̸= 1, then y does not have a multiplicative inverse modulo n. Otherwise,
if d = 1, then t is a multiplicative inverse of y modulo n. Moreover, by
Theorem 18.4, and the discussion immediately following, deg(t) < ℓ, and so
t = y−1 mod n.
TEAM LinG

406
Polynomial arithmetic and applications
If the polynomial n is irreducible, then F[X]/(n) is a ﬁeld, and the ex-
tended Euclidean algorithm, together with the basic algorithms for addition,
subtraction, and multiplication modulo n, yield eﬃcient algorithms for per-
forming addition, subtraction, multiplication and division in the extension
ﬁeld F[X]/(n).
We also observe that the Chinese remainder theorem for polynomials
(Theorem 17.17) can be made computationally eﬀective as well:
Theorem 18.6. Given polynomials n1, . . . , nk ∈F[X] and a1, . . . , ak ∈F[X],
where n1, . . . , nk are pairwise relatively prime, and where deg(ni) > 0 and
deg(ai) < deg(ni) for i = 1, . . . , k, we can compute the polynomial z ∈F[X],
such that deg(z) < deg(n) and z ≡ai (mod ni) for i = 1, . . . , k, where
n := 
i ni, using O(len(n)2) operations in F.
Proof. Exercise (just use the formulas in the proof of Theorem 2.8, which
are repeated below the statement of Theorem 17.17). 2
18.4.1 Chinese remaindering and polynomial interpolation
We remind the reader of the discussion following Theorem 17.17, where the
point was made that when ni = (X −bi) for i = 1, . . . , k, then the Chinese
remainder theorem for polynomials reduces to Lagrange interpolation. Thus,
Theorem 18.6 says that given distinct elements b1, . . . , bk ∈F, along with
elements a1, . . . , ak ∈F, we can compute the unique polynomial z ∈F[X] of
degree less than k such that
z(bi) = ai
(i = 1, . . . , k),
using O(k2) operations in F.
It is perhaps worth noting that we could also solve the polynomial interpo-
lation problem using Gaussian elimination, by inverting the corresponding
Vandermonde matrix. However, this algorithm would use O(k3) operations
in F. This is a speciﬁc instance of a more general phenomenon: there are
many computational problems involving polynomials over ﬁelds that can be
solved using Gaussian elimination, but which can be solved more eﬃciently
using more specialized algorithmic techniques.
Exercise 18.7. State and re-work the polynomial analog of Exercises 4.3
and 4.4. In the special case of polynomial interpolation, this algorithm is
called Newton interpolation.
TEAM LinG

18.4 Computing modular inverses and Chinese remaindering
407
18.4.2 Mutual independence and secret sharing
As we also saw in the discussion following Theorem 17.17, for ℓ≤k
and ﬁxed and distinct b1, . . . , bℓ∈F, the “multi-point evaluation” map
σ : F[X]<k →F ×ℓthat sends a polynomial z ∈F[X] of degree less than k to
(z(b1), . . . , z(bℓ)) ∈F ×ℓis a surjective F-linear map.
If F is a ﬁnite ﬁeld, then this has the following probabilistic interpreta-
tion: if the coeﬃcient vector (z0, . . . , zk−1) of z is a random variable, uni-
formly distributed over F ×k, then the random variable (z(b1), . . . , z(bℓ)) is
uniformly distributed over F ×ℓ(see part (a) of Exercise 8.22). Put another
way, the collection {z(b) : b ∈F} of random variables is ℓ-wise independent,
where each individual z(b) is uniformly distributed over F. Clearly, given
z and b, we can eﬃciently compute the value of z(b), so this construction
gives us a nice way to build eﬀectively constructible, ℓ-wise independent
collections of random variables for any ℓ, thus generalizing the construc-
tions in Example 6.17 and Exercise 6.16 of pairwise and 3-wise independent
collections.
As a particular application of this idea, we describe a simple secret shar-
ing scheme. Suppose Alice wants to share a secret among some number
m of parties, call them P1, . . . , Pm, in such a way that if less than k parties
share their individual secret shares with one another, then Alice’s secret is
still well hidden, while any subset of k parties can reconstruct Alice’s secret.
She can do this as follows. Suppose her secret s is (or can be encoded as)
an element of a ﬁnite ﬁeld F, and that b0, b1, . . . , bm are some ﬁxed, distinct
elements of F, where b0 = 0. This presumes, of course, that |F| ≥m+1. To
share her secret s, Alice chooses z1, . . . , zk−1 ∈F at random, and sets z0 :=
s. Let z ∈F[X] be the polynomial whose coeﬃcient vector is (z0, . . . , zk−1);
that is,
z =
k−1

i=0
ziXi.
For i = 1, . . . , m, Alice gives party Pi its share
ai := z(bi).
For the purposes of analysis, it is convenient to deﬁne
a0 := z(b0) = z(0) = z0 = s.
Clearly, if any k parties pool their shares, they can reconstruct Alice’s
secret by interpolating a polynomial of degree less than k at k points—the
constant term of this polynomial is equal to Alice’s secret s.
TEAM LinG

408
Polynomial arithmetic and applications
It remains to show that Alice’s secret remains well hidden provided less
than k parties pool their shares. To do this, ﬁrst assume that Alice’s secret
s is uniformly distributed over F, independently of z1, . . . , zk−1 (we will
relax this assumption below).
With this assumption, z0, z1, . . . , zk−1 are
independently and uniformly distributed over F. Now consider any subset
of k −1 parties; to simplify notation, assume the parties are P1, . . . , Pk−1.
Then the random variables a0, a1, . . . , ak−1 are mutually independent. The
variables a1, . . . , ak−1 are of course the shares of P1, . . . , Pk−1, while a0 is
equal to Alice’s secret (the fact that a0 has two interpretations, one as the
value of z at a point, and one as a coeﬃcient of z, plays a crucial role
in the analysis). Because of mutual independence, the distribution of a0,
conditioned on ﬁxed values of the shares a1, . . . , ak−1, is still uniform over
F, and so even by pooling their shares, these k −1 parties would have
no better chance of guessing Alice’s secret than they would have without
pooling their shares.
Continuing the analysis of the previous paragraph, consider the condi-
tional probability distribution in which we condition on the event that a0 = s
for some speciﬁc, ﬁxed value of s ∈F. Because the z0, z1, . . . , zk−1 were ini-
tially independently and uniformly distributed over F, and because z0 = a0,
in this conditional probability distribution, we have z0 = s and z1, . . . , zk−1
are independently and uniformly distributed over F. So this conditional
probability distribution perfectly models the secret sharing algorithm per-
formed by Alice for a speciﬁc secret s, without presuming that s is drawn
from any particular distribution.
Moreover, because the a0, a1, . . . , ak−1
were initially independently and uniformly distributed over F, when we con-
dition on the event a0 = s, the variables a1, . . . , ak−1 are still independently
and uniformly distributed over F.
The argument in the previous two paragraphs shows that
for any ﬁxed secret s, the shares a1, . . . , am are (k−1)-wise in-
dependent, with each individual share ai uniformly distributed
over F.
This property ensures that Alice’s secret is perfectly hidden, provided that
less than k parties pool their shares: for any secret s, these parties just see
a bunch of random values in F, with no particular bias that would give any
hint whatsoever as to the actual value of s.
Secret sharing has a number of cryptographic applications, but one simple
motivation is the following. Alice may have some data that she wants to
“back up” on some ﬁle servers, who play the role of the parties P1, . . . , Pm.
TEAM LinG

18.4 Computing modular inverses and Chinese remaindering
409
To do this, Alice gives each server a share of her secret data (if she has a
lot of data, she can break it up into many small blocks, and process each
block separately). If at a later time, Alice wants to restore her data, she
contacts any k servers who will give Alice their shares, from which Alice
can reconstruct the original data. In using a secret sharing scheme in this
way, Alice trusts that the servers are reliable to the extent that they do
not modify the value of their shares (as otherwise, this would cause Alice to
reconstruct the wrong data). We shall discuss later in this chapter how one
can relax this trust assumption. But even with this trust assumption, Alice
does gain something above and beyond the simpler solution of just backing
up her data on a single server, namely:
• even if some of the servers crash, or are otherwise unreachable, she
can still recover her data, as long as at least k are available at the
time she wants to do the recovery;
• even if the data on some (but strictly less than k) of the servers is
“leaked” to some outside attacker, the attacker gains no information
about Alice’s data.
Exercise 18.8. Suppose that Alice shares secrets s1, . . . , st ∈F with parties
P1, . . . , Pm, so that each Pi has one share of each sj.
At a later time,
Alice obtains all the shares held by k of the parties. Show how Alice can
reconstruct all of the secrets s1, . . . , st using O(k2 + tk) operations in F.
Exercise 18.9. Suppose that Alice shares secrets s1, . . . , st ∈F with parties
P1, . . . , Pm, so that each Pi has one share of each sj. Moreover, Alice does
not want to trust that the parties do not maliciously (or accidentally) modify
their shares. Show that if Alice has a small amount of secure storage, namely,
space for O(m) elements of F that cannot be read or modiﬁed by the other
parties, then she can eﬀectively protect herself from malicious parties, so
that if any particular party tries to give her modiﬁed shares, Alice will
fail to detect this with probability at most t/|F|. If |F| is very large (say,
|F| = 2128), and t is any realistic value (say, t ≤240), this failure probability
will be acceptably small for all practical purposes. Hint: see Exercise 9.12.
18.4.3 Speeding up algorithms via modular computation
In §4.4, we discussed how the Chinese remainder theorem could be used to
speed up certain types of computations involving integers. The example we
gave was the multiplication of integer matrices. We can use the same idea to
speed up certain types of computations involving polynomials. For example,
TEAM LinG

410
Polynomial arithmetic and applications
if one wants to multiply two matrices whose entries are elements of F[X], one
can use the Chinese remainder theorem for polynomials to speed things up.
This strategy is most easily implemented if F is suﬃciently large, so that we
can use polynomial evaluation and interpolation directly, and do not have
to worry about constructing irreducible polynomials. We leave the details
as an exercise.
Exercise 18.10. You are give two matrices A, B ∈F[X]ℓ×ℓ. All entries of
A and B are polynomials of degree at most M. Assume that |F| ≥2M + 1.
Using polynomial evaluation and interpolation, show how to compute the
product matrix C = A · B using O(ℓ2M2 + ℓ3M) operations in F. Compare
this to the cost of computing C directly, which would be O(ℓ3M2).
18.5 Rational function reconstruction and applications
We next state and prove the polynomial analog of Theorem 4.6. As we are
now “reconstituting” a rational function, rather than a rational number,
we call this procedure rational function reconstruction.
Because of
the relative simplicity of polynomials compared to integers, the rational
reconstruction theorem for polynomials is a bit “sharper” than the rational
reconstruction theorem for integers. Throughout this section, F denotes a
ﬁeld.
Theorem 18.7. Let r∗, t∗be non-negative integers, and let n, y ∈F[X] be
polynomials such that r∗+ t∗≤deg(n) and deg(y) < deg(n). Suppose we
run the extended Euclidean algorithm with inputs a := n and b := y. Then,
adopting the notation of Theorem 18.4, the following hold:
(i) There exists a unique index i = 1, . . . , ℓ+1, such that deg(ri) < r∗≤
deg(ri−1), and for this i, we have ti ̸= 0.
Let r′ := ri, s′ := si, and t′ := ti.
(ii) Furthermore, for any polynomials r, s, t ∈F[X] such that
r = sn + ty,
deg(r) < r∗,
0 ≤deg(t) ≤t∗,
(18.1)
we have
r = r′α, s = s′α, t = t′α,
for some non-zero polynomial α ∈F[X].
Proof. By hypothesis, 0 ≤r∗≤deg(n) = deg(r0). Moreover, since
deg(r0), . . . , deg(rℓ), deg(rℓ+1) = −∞
TEAM LinG

18.5 Rational function reconstruction and applications
411
is a decreasing sequence, and ti ̸= 0 for i = 1, . . . , ℓ+ 1, the ﬁrst statement
of the theorem is clear.
Now let i be deﬁned as in the ﬁrst statement of the theorem. Also, let
r, s, t be as in (18.1).
From part (iv) of Theorem 18.4 and the inequality r∗≤deg(ri−1), we
have
deg(ti) = deg(n) −deg(ri−1) ≤deg(n) −r∗.
From the equalities ri = sin + tiy and r = sn + ty, we have the two congru-
ences:
r ≡ty (mod n),
ri ≡tiy (mod n).
Subtracting ti times the ﬁrst from t times the second, we obtain
rti ≡rit (mod n).
This says that n divides rti−rit; however, using the bounds deg(r) < r∗and
deg(ti) ≤deg(n) −r∗, we see that deg(rti) < deg(n), and using the bounds
deg(ri) < r∗, deg(t) ≤t∗, and r∗+ t∗≤deg(n), we see that deg(rit) <
deg(n); it immediately follows that
deg(rti −rit) < deg(n).
Since n divides rti −rit and deg(rti −rit) < deg(n), the only possibility is
that
rti −rit = 0.
The rest of the proof runs exactly the same as the corresponding part of
the proof of Theorem 4.6, as the reader may easily verify. 2
18.5.1 Application: polynomial interpolation with errors
We now discuss the polynomial analog of the application in §4.5.1.
If we “encode” a polynomial z ∈F[X], with deg(z) < k, as the sequence
(a1, . . . , ak) ∈F ×k, where ai = z(bi), then we can eﬃciently recover z from
this encoding, using an algorithm for polynomial interpolation.
Here, of
course, the bi are distinct elements of F, and F is a ﬁnite ﬁeld (which must
have at least k elements, of course).
Now suppose that Alice encodes z as (a1, . . . , ak), and sends this encoding
to Bob, but that some, say at most ℓ, of the ai may be corrupted during
transmission. Let (˜a1, . . . , ˜ak) denote the vector actually received by Bob.
TEAM LinG

412
Polynomial arithmetic and applications
Here is how we can use Theorem 18.7 to recover the original value of z
from (˜a1, . . . , ˜ak), assuming:
• the original polynomial z has degree less than k′,
• at most ℓerrors occur in transmission, and
• k ≥2ℓ+ k′.
Let us set ni := (X−bi) for i = 1, . . . , k, and n := n1 · · · nk. Now, suppose
Bob obtains the corrupted encoding (˜a1, . . . , ˜ak). Here is what Bob does to
recover z:
1. Interpolate, obtaining a polynomial y, with deg(y) < k and y(bi) = ˜ai
for i = 1, . . . , k.
2. Run the extended Euclidean algorithm on a := n and b := y, and let
r′, t′ be the values obtained from Theorem 18.7 applied with r∗:=
k′ + ℓand t∗:= ℓ.
3. If t′ | r′, output r′/t′; otherwise, output “error.”
We claim that the above procedure outputs z, under the assumptions
listed above. To see this, let t be the product of the ni for those values of i
where an error occurred. Now, assuming at most ℓerrors occurred, we have
deg(t) ≤ℓ. Also, let r := tz, and note that deg(r) < k′ + ℓ. We claim that
r ≡ty (mod n).
(18.2)
To show that (18.2) holds, it suﬃces to show that
tz ≡ty (mod ni)
(18.3)
for all i = 1, . . . , k.
To show this, consider ﬁrst an index i at which no
error occurred, so that ai = ˜ai. Then tz ≡tai (mod ni) and ty ≡t˜ai ≡
tai (mod ni), and so (18.3) holds for this i.
Next, consider an index i
for which an error occurred. Then by construction, tz ≡0 (mod ni) and
ty ≡0 (mod ni), and so (18.3) holds for this i. Thus, (18.2) holds, from
which it follows that the values r′, t′ obtained from Theorem 18.7 satisfy
r′
t′ = r
t = tz
t = z.
One easily checks that both the procedures to encode and decode a value z
run in time O(k2). The above scheme is an example of an error correcting
code called a Reed–Solomon code. Note that we are completely free to
choose the ﬁnite ﬁeld F however we want, just so long as it is big enough.
An attractive choice in some settings is to choose F = Z2[Y]/(f), where
f ∈Z2[Y] is an irreducible polynomial; with this choice, elements of F may
be encoded as bit strings of length deg(f).
TEAM LinG

18.5 Rational function reconstruction and applications
413
One can combine the above error correction technique with the idea of
secret sharing (see §18.4.2) to obtain a secret sharing scheme that is robust,
even in the presence of erroneous (as opposed to just missing) shares. More
precisely, Alice can share a secret s ∈F among parties P1, . . . , Pm, in such
a way that (1) if less than k′ parties pool their shares, Alice’s secret remains
well hidden, and (2) from any k shares, we can correctly reconstruct Alice’s
secret, provided at most ℓof the shares are incorrect, and k ≥2ℓ+ k′.
To do this, Alice chooses z1, . . . , zk′−1 ∈F at random, sets z0 := s, and
z := k′−1
i=0 ziXi ∈F[X], and computes the ith share as ai := z(bi), for
i = 1, . . . , m. Here, we assume that the bi are distinct, non-zero elements of
F. Now, just as in §18.4.2, as long as less than k′ parties pool their shares,
Alice’s secret remains well hidden; however, provided k ≥2ℓ+ k′, we can
correctly and eﬃciently reconstruct Alice’s secret given any k values ˜ai, as
long as at most ℓof the ˜ai diﬀer from the corresponding value of ai.
18.5.2 Application: recovering rational functions from their
reversed formal Laurent series
We now discuss the polynomial analog of the application in §4.5.2. This is an
entirely straightforward translation of the results in §4.5.2, but we shall see
in the next chapter that this problem has its own interesting applications.
Suppose Alice knows a rational function z = s/t ∈F(X), where s and t
are polynomials with deg(s) < deg(t), and tells Bob some of the high-order
coeﬃcients of the reversed formal Laurent series (see §17.7) representing z
in F((X−1)). We shall show that if deg(t) ≤M and Bob is given the bound
M on deg(t), along with the high-order 2M coeﬃcients of z, then Bob can
determine z, expressed as a rational function in lowest terms.
So suppose that z = s/t = ∞
i=1 ziX−i, and that Alice tells Bob the
coeﬃcients z1, . . . , z2M. Equivalently, Alice gives Bob the polynomial
y := z1X2M−1 + · · · + z2M−1X + z2M = ⌊zX2M⌋.
Let us deﬁne n := X2M, so that y = ⌊zn⌋.
Here is Bob’s algorithm for
recovering z:
1. Run the extended Euclidean algorithm on inputs a := n and b := y,
and let s′, t′ be as in Theorem 18.7, using r∗:= M and t∗:= M.
2. Output s′, t′.
We claim that z = −s′/t′. To prove this, observe that since y = ⌊zn⌋=
⌊(ns)/t⌋, if we set r := (ns) mod t, then we have
r = sn −ty, deg(r) < r∗, 0 ≤deg(t) ≤t∗, and r∗+ t∗≤deg(n).
TEAM LinG

414
Polynomial arithmetic and applications
It follows that the polynomials s′, t′ from Theorem 18.7 satisfy s = s′α and
−t = t′α for some non-zero polynomial α. Thus, s′/t′ = −s/t, which proves
the claim.
We may further observe that since the extended Euclidean algorithm guar-
antees that gcd(s′, t′) = 1, not only do we obtain z, but we obtain z expressed
as a fraction in lowest terms.
It is clear that this algorithm takes O(M2) operations in F.
The following exercises are the polynomial analogs of Exercises 4.7, 4.9,
and 4.10.
Exercise 18.11. Let F be a ﬁeld.
Show that given polynomials s, t ∈
F[X] and integer k, with deg(s) < deg(t) and k > 0, we can compute the
kth coeﬃcient in the reversed formal Laurent series representing s/t using
O(len(k) len(t)2) operations in F.
Exercise 18.12. Let F be a ﬁeld. Let z ∈F((X−1)) be a reversed formal
Laurent series whose coeﬃcient sequence is ultimately periodic. Show that
z ∈F(X).
Exercise 18.13. Let F be a ﬁeld. Let z = s/t, where s, t ∈F[X], deg(s) <
deg(t), and gcd(s, t) = 1. Let d > 1 be an integer.
(a) Show that if F is ﬁnite, there exist integers k, k′ such that 0 ≤k < k′
and sdk ≡sdk′ (mod t).
(b) Show that for integers k, k′ with 0 ≤k < k′, the sequence of coef-
ﬁcients of the reversed Laurent series representing z is (k, k′ −k)-
periodic if and only if sdk ≡sdk′ (mod t).
(c) Show that if F is ﬁnite and X ∤t, then the reversed Laurent series rep-
resenting z is purely periodic with period equal to the multiplicative
order of [X]t ∈(F[X]/(t))∗.
(d) More generally, show that if F is ﬁnite and t = Xkt′, with X ∤t′,
then the reversed Laurent series representing z is ultimately periodic
with pre-period k and period equal to the multiplicative order of
[X]t′ ∈(F[X]/(t′))∗.
18.5.3 Applications to symbolic algebra
Rational function reconstruction has applications in symbolic algebra, anal-
ogous to those discussed in §4.5.3. In that section, we discussed the appli-
cation of solving systems of linear equations over the integers using rational
TEAM LinG

18.6 Faster polynomial arithmetic (∗)
415
reconstruction. In exactly the same way, one can use rational function re-
construction to solve systems of linear equations over F[X]—the solution to
such a system of equations will be a vector whose entries are elements of
F(X), the ﬁeld of rational functions.
18.6 Faster polynomial arithmetic (∗)
The algorithms discussed in §3.5 for faster integer arithmetic are easily
adapted to polynomials over a ring. Throughout this section, R denotes
a non-trivial ring.
Exercise 18.14. State and re-work the analog of Exercise 3.32 for R[X].
Your algorithm should multiply two polynomials over R of length at most ℓ
using O(ℓlog2 3) operations in R.
It is in fact possible to multiply polynomials over R of length at most ℓ
using O(ℓlen(ℓ) len(len(ℓ))) operations in R—we shall develop some of the
ideas that lead to such a result below in Exercises 18.23–18.26 (see also the
discussion in §18.7).
In Exercises 18.15–18.21 below, assume that we have an algorithm that
multiplies two polynomials over R of length at most ℓusing at most M(ℓ)
operations in R, where M is a well-behaved complexity function (as deﬁned
in §3.5).
Exercise 18.15. State and re-work the analog of Exercise 3.34 for R[X].
Exercise 18.16. This problem is the analog of Exercise 3.35 for R[X]. Let
us ﬁrst deﬁne the notion of a “ﬂoating point” reversed formal Laurent series
ˆz, which is represented as a pair (a, e), where a ∈R[X] and e ∈Z — the
value of ˆz is aXe ∈R((X−1)), and we call len(a) the precision of ˆz. We
say that ˆz is a length k approximation of z ∈R((X−1)) if ˆz has precision
k and ˆz = (1 + ϵ)z for ϵ ∈R((X−1)) with deg(ϵ) ≤−k, which is the same
as saying that the high-order k coeﬃcients of ˆz and z are equal.
Show
how to compute—given monic b ∈R[X] and positive integer k —a length
k approximation of 1/b ∈R((X−1)) using O(M(k)) operations in R. Hint:
using Newton iteration, show how to go from a length t approximation
of 1/b to a length 2t approximation, making use of just the high-order 2t
coeﬃcients of b, and using O(M(t)) operations in R.
Exercise 18.17. State and re-work the analog of Exercise 3.36 for R[X].
Assume that b is a monic polynomial.
Exercise 18.18. State and re-work the analog of Exercise 3.37 for R[X].
TEAM LinG

416
Polynomial arithmetic and applications
Conclude that a polynomial of length ℓcan be evaluated at ℓpoints using
O(M(ℓ) len(ℓ)) operations in R.
Exercise 18.19. State and re-work the analog of Exercise 3.38 for R[X],
assuming that R is a ﬁeld of odd characteristic.
Exercise 18.20. State and re-work the analog of Exercise 3.40 for R[X].
Assume that 2R ∈R∗.
The next two exercises develop a useful technique known as Kronecker
substitution.
Exercise 18.21. Let E := R[X]. Let a, b ∈E[Y] with a = m−1
i=0 aiYi and
b = m−1
i=0 biYi, where each ai and bi is a polynomial in X of degree less
than k. The product c := ab ∈E[Y] may be written c = 2m−2
i=0
ciYi, where
each ci is a polynomial in X. Show how to compute c, given a and b, using
O(M(km)) operations in R. Hint: for an appropriately chosen integer t > 0,
ﬁrst convert a, b to ˜a,˜b ∈R[X], where ˜a := m−1
i=0 aiXti and ˜b := m−1
i=0 biXti;
next, compute ˜c := ˜a˜b ∈R[X]; ﬁnally, “read oﬀ” the values ci from the
coeﬃcients of ˜c.
Exercise 18.22. Assume that ℓ-bit integers can be multiplied in time
¯
M(ℓ), where ¯
M is a well-behaved complexity function. Let a, b ∈Z[X] with
a = m−1
i=0 aiXi and b = m−1
i=0 biXi, where each ai and bi is a non-negative
integer, strictly less than 2k. The product c := ab ∈Z[X] may be written
c = 2m−2
i=0
ciXi, where each ci is a non-negative integer. Show how to com-
pute c, given a and b, using O( ¯
M((k + len(m))m)) operations in R. Hint:
for an appropriately chosen integer t > 0, ﬁrst convert a, b to ˜a,˜b ∈Z, where
˜a := m−1
i=0 ai2ti and ˜b := m−1
i=0 bi2ti; next, compute ˜c := ˜a˜b ∈Z; ﬁnally,
“read oﬀ” the values ci from the bits of ˜c.
The following exercises develop an important algorithm for multiplying
polynomials in almost-linear time. For integer n ≥0, let us call ω ∈R a
primitive 2nth root of unity if n ≥1 and ω2n−1 = −1R, or n = 0 and
ω = 1R; if 2R ̸= 0R, then in particular, ω has multiplicative order 2n. For
n ≥0, and ω ∈R a primitive 2nth root of unity, let us deﬁne the R-linear
map En,ω : R×2n →R×2n that sends the vector (g0, . . . , g2n−1) to the vector
(g(1R), g(ω), . . . , g(ω2n−1)), where g := 2n−1
i=0
giXi ∈R[X].
Exercise 18.23. Suppose 2R ∈R∗and ω ∈R is a primitive 2nth root of
unity.
(a) Let k be any integer, and consider gcd(k, 2n), which must be of the
TEAM LinG

18.6 Faster polynomial arithmetic (∗)
417
form 2m for some m = 0, . . . , n. Show that ωk is a primitive 2n−mth
root of unity.
(b) Show that if n ≥1, then ω −1R ∈R∗.
(c) Show that ωk −1R ∈R∗for all integers k ̸≡0 (mod 2n).
(d) Show that for any integer k, we have
2n−1

i=0
ωki =
 2n
R
if k ≡0 (mod 2n),
0R
if k ̸≡0 (mod 2n).
(e) Let M2 be the 2-multiplication map on R×2n, which is a bijective,
R-linear map. Show that
En,ω ◦En,ω−1 = Mn
2 = En,ω−1 ◦En,ω,
and conclude that En,ω is bijective, with M−n
2
◦En,ω−1 being its inverse.
Hint: write down the matrices representing the maps En,ω and En,ω−1.
Exercise 18.24. This exercise develops a fast algorithm, called the fast
Fourier transform or FFT, for computing the function En,ω.
This is
a recursive algorithm FFT(n, ω; g0, . . . , g2n−1) that takes as inputs integer
n ≥0, a primitive 2nth root of unity ω ∈R, and elements g0, . . . , g2n−1 ∈R,
and runs as follows:
if n = 0 then
return g0
else
(α0, . . . , α2n−1−1) ←FFT(n −1, ω2; g0, g2, . . . , g2n−2)
(β0, . . . , β2n−1−1) ←FFT(n −1, ω2; g1, g3, . . . , g2n−1)
for i ←0 to 2n−1 −1 do
γi ←αi + βiωi, γi+2n−1 ←αi −βiωi
return (γ0, . . . , γ2n−1)
Show that this algorithm correctly computes En,ω(g0, . . . , g2n−1) using
O(2nn) operations in R.
Exercise 18.25. Assume 2R ∈R∗. Suppose that we are given two polyno-
mials a, b ∈R[X] of length at most ℓ, along with a primitive 2nth root of unity
ω ∈R, where 2ℓ≤2n < 4ℓ. Let us “pad” a and b, writing a = 2n−1
i=0
aiXi
and b = 2n−1
i=0
biXi, where ai and bi are zero for i ≥ℓ. Show that the follow-
ing algorithm correctly computes the product of a and b using O(ℓlen(ℓ))
operations in R:
TEAM LinG

418
Polynomial arithmetic and applications
(α0, . . . , α2n−1) ←FFT(n, ω; a0, . . . , a2n−1)
(β0, . . . , β2n−1) ←FFT(n, ω; b0, . . . , b2n−1)
(γ0, . . . , γ2n−1) ←(α0β0, . . . , α2n−1β2n−1)
(c0, . . . , c2n−1) ←2−n
R FFT(n, ω−1; γ0, . . . , γ2n−1)
output 2ℓ−2
i=0 ciXi
Also, argue more carefully that the algorithm performs O(ℓlen(ℓ)) addi-
tions/subtractions in R, O(ℓlen(ℓ)) multiplications in R by powers of ω,
and O(ℓ) other multiplications in R.
Exercise 18.26. Assume 2R ∈R∗. In this exercise, we use the FFT to
develop an algorithm that multiplies polynomials over R of length at most
ℓusing O(ℓlen(ℓ)β) operations in R, where β is a constant. Unlike as in
the previous exercise, we do not assume that R contains any particular
primitive roots of unity; rather, the algorithm will create them “out of thin
air.” Suppose that a, b ∈R[X] are of length at most ℓ. Set k := ⌊
"
ℓ/2⌋,
m := ⌈ℓ/k⌉. We may write a = m−1
i=0 aiXki and b = m−1
i=0 biXki, where
the ai and bi are polynomials of length at most k. Let n be the integer
determined by 2m ≤2n < 4m. Let f := X2n−1 + 1R ∈R[X], E := R[X]/(f),
and ω := [X]f ∈E.
(a) Show that ω is a primitive 2nth root of unity in E, and that given an
element δ ∈E and an integer i between 0 and 2n−1, we can compute
δωi ∈E using O(ℓ1/2) operations in R.
(b) Let ¯a := m−1
i=0 [ai]fYi ∈E[Y] and ¯b := m−1
i=0 [bi]fYi ∈E[Y]. Using
the FFT (over E), show how to compute ¯c := ¯a¯b ∈E[Y] by computing
O(ℓ1/2) products in R[X] of polynomials of length O(ℓ1/2), along with
O(ℓlen(ℓ)) additional operations in R.
(c) Show how to compute the coeﬃcients of c := ab ∈R[X] from the
value ¯c ∈E[Y] computed in part (b), using O(ℓ) operations in R.
(d) Based on parts (a)–(c), we obtain a recursive multiplication algo-
rithm: on inputs of length at most ℓ, it performs at most α0ℓlen(ℓ)
operations in R, and calls itself recursively on at most α1ℓ1/2 sub-
problems, each of length at most α2ℓ1/2; here, α0, α1 and α2 are
constants. If we just perform one level of recursion, and immediately
switch to a quadratic multiplication algorithm, we obtain an algo-
rithm whose operation count is O(ℓ1.5). If we perform two levels of
recursion, this is reduced to O(ℓ1.25). For practical purposes, this is
probably enough; however, to get an asymptotically better complex-
ity bound, we can let the algorithm recurse all the way down to inputs
of some (appropriately chosen) constant length. Show that if we do
TEAM LinG

18.6 Faster polynomial arithmetic (∗)
419
this, the operation count of the recursive algorithm is O(ℓlen(ℓ)β) for
some constant β (whose value depends on α1 and α2).
The approach used in the previous exercise was a bit sloppy. With a bit
more care, one can use the same ideas to get an algorithm that multiplies
polynomials over R of length at most ℓusing O(ℓlen(ℓ) len(len(ℓ))) opera-
tions in R, assuming 2R ∈R∗. The next exercise applies similar ideas, but
with a few twists, to the problem of integer multiplication.
Exercise 18.27. This exercise uses the FFT to develop a linear-time al-
gorithm for integer multiplication; however, a rigorous analysis depends on
an unproven conjecture (which follows from a generalization of the Riemann
hypothesis). Suppose we want to multiply two ℓ-bit, positive integers a and b
(represented internally using the data structure described in §3.3). Through-
out this exercise, assume that all computations are done on a RAM, and
that arithmetic on integers of length O(len(ℓ)) takes time O(1). Let k be an
integer parameter with k = Θ(len(ℓ)), and let m := ⌈ℓ/k⌉. We may write
a = m−1
i=0 ai2ki and b = m−1
i=0 bi2ki, where 0 ≤ai < 2k and 0 ≤bi < 2k.
Let n be the integer determined by 2m ≤2n < 4m.
(a) Assuming Conjecture 5.24 (and the result of Exercise 5.22), and as-
suming a deterministic, polynomial-time primality test (such as the
one to be presented in Chapter 22), show how to eﬃciently generate
a prime p ≡1 (mod 2n) and an element ω ∈Z∗
p of multiplicative
order 2n, such that
22km < p ≤ℓO(1).
Your algorithm should be probabilistic, and run in expected time
polynomial in len(ℓ).
(b) Assuming you have computed p and ω as in part (a), let ¯a :=
m−1
i=0 [ai]pXi ∈Zp[X] and ¯b := m−1
i=0 [bi]pXi ∈Zp[X], and show how
to compute ¯c := ¯a¯b ∈Zp[X] in time O(ℓ) using the FFT (over Zp).
Here, you may store elements of Zp in single memory cells, so that
operations in Zp take time O(1).
(c) Assuming you have computed ¯c ∈Zp[X] as in part (b), show how to
obtain c := ab in time O(ℓ).
(d) Conclude that assuming Conjecture 5.24, we can multiply two ℓ-bit
integers on a RAM in time O(ℓ).
Note that even if one objects to our accounting practices, and insists on
charging O(len(ℓ)2) time units for arithmetic on numbers of length O(len(ℓ)),
TEAM LinG

420
Polynomial arithmetic and applications
the algorithm in the previous exercise runs in time O(ℓlen(ℓ)2), which is
“almost” linear time.
Exercise 18.28. Continuing with the previous exercise:
(a) Show how the algorithm presented there can be implemented on a
RAM that has only built-in addition, subtraction, and branching
instructions, but no multiplication or division instructions, and still
run in time O(ℓ). Also, memory cells should store numbers of length
at most len(ℓ) + O(1). Hint: represent elements of Zp as sequences
of base-2t digits, where t ≈α len(ℓ) for some constant α < 1; use
table lookup to multiply t-bit numbers, and to perform 2t-by-t-bit
divisions—for α suﬃciently small, you can build these tables in time
o(ℓ).
(b) Using Theorem 5.25, show how to make this algorithm fully deter-
ministic and rigorous, provided that on inputs of length ℓ, it is pro-
vided with a certain bit string σℓof length O(len(ℓ)) (this is called a
non-uniform algorithm).
Exercise 18.29. This exercise shows how the algorithm in Exercise 18.27
can be made quite concrete, and fairly practical, as well.
(a) The number p := 25927 + 1 is a 64-bit prime. Show how to use this
value of p in conjunction with the algorithm in Exercise 18.27 with
k = 20 and any value of ℓup to 227.
(b) The numbers p1 := 2303 + 1, p2 := 22813 + 1, and p3 := 22729 + 1
are 32-bit primes. Show how to use the Chinese remainder theorem
to modify the algorithm in Exercise 18.27, so that it uses the three
primes p1, p2, p3, and so that it works with k = 32 and any value of
ℓup to 231. This variant may be quite practical on a 32-bit machine
with built-in instructions for 32-bit multiplication and 64-by-32-bit
division.
The previous three exercises indicate that we can multiply integers in
essentially linear time, both in theory and in practice. As mentioned in §3.6,
there is a diﬀerent, fully deterministic and rigorously analyzed algorithm
that multiplies integers in linear time on a RAM. In fact, that algorithm
works on a very restricted type of machine called a “pointer machine,” which
can be simulated in “real time” on a RAM with a very restricted instruction
set (including the type in the previous exercise). That algorithm works with
ﬁnite approximations to complex roots of unity, rather than roots of unity
in a ﬁnite ﬁeld.
TEAM LinG

18.7 Notes
421
We close this section with a cute application of fast polynomial multipli-
cation to the problem of factoring integers.
Exercise 18.30. Let n be a large, positive integer. We can factor n using
trial division in time n1/2+o(1); however, using fast polynomial arithmetic
in Zn[X], one can get a simple, deterministic, and rigorous algorithm that
factors n in time n1/4+o(1). Note that all of the factoring algorithms dis-
cussed in Chapter 16, while faster, are either probabilistic, or deterministic
but heuristic. Assume that we can multiply polynomials in Zn[X] of length
at most ℓusing M(ℓ) operations in Zn, where M is a well-behaved complex-
ity function, and M(ℓ) = ℓ1+o(1) (the algorithm from Exercise 18.26 would
suﬃce).
(a) Let ℓbe a positive integer, and for i = 1, . . . , ℓ, let
ai :=
ℓ−1

j=0
(iℓ−j) mod n.
Using fast polynomial arithmetic, show how to compute all of the
integers a1, . . . , aℓin time ℓ1+o(1) len(n)O(1).
(b) Using the result of part (a), show how to factor n in time n1/4+o(1)
using a deterministic algorithm.
18.7 Notes
Exercise 18.4 is based on an algorithm of Brent and Kung [20]. Using fast
matrix arithmetic, Brent and Kung show how this problem can be solved
using O(ℓ(ω+1)/2) operations in R, where ω is the exponent for matrix mul-
tiplication (see §15.6), and so (ω + 1)/2 < 1.7.
The interpretation of Lagrange interpolation as “secret sharing” (see
§18.4.2), and its application to cryptography, was made by Shamir [85].
Reed–Solomon codes were ﬁrst proposed by Reed and Solomon [77], al-
though the decoder presented here was developed later. Theorem 18.7 was
proved by Mills [64]. The Reed–Solomon code is just one way of detecting
and correcting errors—we have barely scratched the surface of this subject.
Just as in the case of integer arithmetic, the basic “pencil and paper”
quadratic-time algorithms discussed in this chapter for polynomial arith-
metic are not the best possible. The fastest known algorithms for multipli-
cation of polynomials of length ℓover a ring R take O(ℓlen(ℓ) len(len(ℓ)))
operations in R. These algorithms are all variations on the basic FFT al-
gorithm (see Exercise 18.25), but work without assuming that 2R ∈R∗or
TEAM LinG

422
Polynomial arithmetic and applications
that R contains any particular primitive roots of unity (we developed some
of the ideas in Exercise 18.26). The Euclidean and extended Euclidean al-
gorithms for polynomials over a ﬁeld F can be implemented so as to take
O(ℓlen(ℓ)2 len(len(ℓ))) operations in F, as can the algorithms for Chinese
remaindering and rational function reconstruction.
See the book by von
zur Gathen and Gerhard [37] for details (as well for an analysis of the Eu-
clidean algorithm for polynomials over the ﬁeld of rational numbers and
over function ﬁelds). Depending on the setting and many implementation
details, such asymptotically fast algorithms for multiplication and division
can be signiﬁcantly faster than the quadratic-time algorithms, even for quite
moderately sized inputs of practical interest. However, the fast Euclidean
algorithms are only useful for signiﬁcantly larger inputs.
TEAM LinG

19
Linearly generated sequences and applications
In this chapter, we develop some of the theory of linearly generated se-
quences.
As an application, we develop an eﬃcient algorithm for solv-
ing sparse systems of linear equations, such as those that arise in the
subexponential-time algorithms for discrete logarithms and factoring in
Chapter 16. These topics illustrate the beautiful interplay between the arith-
metic of polynomials, linear algebra, and the use of randomization in the
design of algorithms.
19.1 Basic deﬁnitions and properties
Let F be a ﬁeld, let V be an F-vector space, and consider an inﬁnite sequence
S = (α0, α1, α2, . . .),
where αi ∈V for i = 0, 1, 2 . . . . We say that S is linearly generated (over
F) if there exist scalars a0, . . . , ak−1 ∈F such that the following recurrence
relation holds:
αk+i =
k−1

j=0
ajαj+i
(for i = 0, 1, 2, . . .).
In this case, all of the elements of the sequence S are determined by the initial
segment α0, . . . , αk−1, together with the coeﬃcients a0, . . . , ak−1 deﬁning the
recurrence relation.
The general problem we consider is this: how to determine the coeﬃcients
deﬁning such a recurrence relation, given a suﬃciently long initial segment
of S. To study this problem, it turns out to be very useful to rephrase the
problem slightly. Let g ∈F[X] be a polynomial of degree, say, k, and write
423
TEAM LinG

424
Linearly generated sequences and applications
g = k
j=0 gjXj. Next, deﬁne
g ⋆S :=
k

j=0
gjαj.
Then it is clear that S is linearly generated if and only if there exists a
non-zero polynomial g such that
(Xig) ⋆S = 0
(for i = 0, 1, 2, . . .).
(19.1)
Indeed, if there is such a non-zero polynomial g, then we can take
a0 := −(g0/gk), a1 := −(g1/gk), . . . , ak−1 := −(gk−1/gk)
as coeﬃcients deﬁning the recurrence relation for S. We call a polynomial g
satisfying (19.1) a generating polynomial for S. The sequence S will in
general have many generating polynomials. Note that the zero polynomial is
technically considered a generating polynomial, but is not a very interesting
one.
Let G(S) be the set of all generating polynomials for S.
Theorem 19.1. G(S) is an ideal of F[X].
Proof. First, note that for any two polynomials f, g, we have (f + g) ⋆S =
(f ⋆S) + (g ⋆S) — this is clear from the deﬁnitions. It is also clear that
for any c ∈F and f ∈F[X], we have (cf) ⋆S = c · (f ⋆S). From these
two observations, it is immediately clear that G(S) is closed under addition
and scalar multiplication. It is also clear from the deﬁnition that G(S) is
closed under multiplication by X; indeed, if (Xif) ⋆S = 0 for all i ≥0, then
certainly, (Xi(Xf)) ⋆S = (Xi+1f) ⋆S = 0 for all i ≥0. But any non-empty
subset of F[X] that is closed under addition, multiplication by elements of
F, and multiplication by X is an ideal of F[X] (see Exercise 9.27). 2
Since all ideals of F[X] are principal, it follows that G(S) is the ideal of
F[X] generated by some polynomial φ ∈F[X]—we can make this polyno-
mial unique by choosing the monic associate (if it is non-zero), and we call
this polynomial the minimal polynomial of S. Note that S is linearly
generated if and only if φ ̸= 0.
We can now restate our main objective as follows: given a suﬃciently
long initial segment of a linearly generated sequence, determine its minimal
polynomial.
Example 19.1. Of course, one can always deﬁne a linearly generated se-
quence by simply choosing an initial sequence α0, α1, . . . , αk−1, along with
TEAM LinG

19.1 Basic deﬁnitions and properties
425
the coeﬃcients g0, . . . , gk−1 of a generating polynomial g := g0 + g1X + · · · +
gk−1Xk−1 + Xk. One can enumerate as many elements of the sequence as
one wants by using storage for k elements of V , along with storage for the
coeﬃcients of g, as follows:
(β0, . . . , βk−1) ←(α0, . . . , αk−1)
repeat
output β0
β′ ←−k−1
j=0 gjβj
(β0, . . . , βk−1) ←(β1, . . . , βk−1, β′)
forever
Because of the structure of the above algorithm, linearly generated se-
quences are sometimes also called shift register sequences. Also observe
that if F is a ﬁnite ﬁeld, and V is ﬁnite dimensional, the value stored in
the “register” (β0, . . . , βk−1) must repeat at some point, from which it fol-
lows that the linearly generated sequence must be ultimately periodic (see
deﬁnitions above Exercise 4.8). 2
Example 19.2. Linearly generated sequences can also arise in a natural
way, as this example and the next illustrate.
Let E := F[X]/(f), where
f ∈F[X] is a monic polynomial of degree ℓ> 0, and let α be an element
of E. Consider the sequence S := (1, α, α2, · · · ) of powers of α. For any
polynomial g = k
j=0 gjXj ∈F[X], we have
g ⋆S =
k

j=0
gjαj = g(α).
Now, if g(α) = 0, then clearly (Xig)⋆S = αig(α) = 0 for all i ≥0. Conversely,
if (Xig) ⋆S = 0 for all i ≥0, then in particular, g(α) = 0. Thus, g is a
generating polynomial for S if and only if g(α) = 0. It follows that the
minimal polynomial φ of S is the same as the minimal polynomial of α over
F, as deﬁned in §17.5. Furthermore, φ ̸= 0, and the degree m of φ may be
characterized as the smallest positive integer m such that 1, α, . . . , αm are
linearly dependent; moreover, as E has dimension ℓover F, we must have
m ≤ℓ. 2
Example 19.3. Let V be a vector space over F of dimension ℓ> 0, and let
τ : V →V be an F-linear map. Let β ∈V , and consider the sequence S :=
(α0, α1, . . .), where αi = τ i(β); that is, α0 = β, α1 = τ(β), α2 = τ(τ(β)),
TEAM LinG

426
Linearly generated sequences and applications
and so on. For any polynomial g = k
j=0 gjXj ∈F[X], we have
g ⋆S =
k

j=0
gjτ j(β),
and for any i ≥0, we have
(Xig) ⋆S =
k

j=0
gjτ i+j(β) = τ i
 k

j=0
gjτ j(β)

= τ i(g ⋆S).
Thus, if g ⋆S = 0, then clearly (Xig)⋆S = τ i(g ⋆S) = τ i(0) = 0 for all i ≥0.
Conversely, if (Xig) ⋆S = 0 for all i ≥0, then in particular, g ⋆S = 0. Thus,
g is a generating polynomial for S if and only if g ⋆S = 0. The minimal
polynomial φ of S is non-zero and its degree m is at most ℓ; indeed, m may be
characterized as the least non-negative integer such that β, τ(β), . . . , τm(β)
are linearly dependent, and since V has dimension ℓover F, we must have
m ≤ℓ.
The previous example can be seen as a special case of this one, by taking
V to be E, τ to be the α-multiplication map on E, and setting β to 1. 2
The problem of computing the minimal polynomial of a linearly generated
sequence can always be solved by means of Gaussian elimination. For exam-
ple, the minimal polynomial of the sequence discussed in Example 19.2 can
be computed using the algorithm described in §18.2. The minimal polyno-
mial of the sequence discussed in Example 19.3 can be computed in a similar
manner. Also, Exercise 19.3 below shows how one can reformulate another
special case of the problem so that it is easily solved by Gaussian elimination.
However, in the following sections, we will present algorithms for computing
minimal polynomials for certain types of linearly generated sequences that
are much more eﬃcient than any algorithm based on Gaussian elimination.
Exercise 19.1. Show that the only sequence for which 1 is a generating
polynomial is the “all zero” sequence.
Exercise 19.2. Let S = (α0, α1, . . .) be a sequence of elements of an F-
vector space V . Further, suppose that S has non-zero minimal polynomial
φ.
(a) Show that for any polynomials g, h ∈F[X], if g ≡h (mod φ), then
g ⋆S = h ⋆S.
(b) Let m := deg(φ).
Show that if g ∈F[X] and (Xig) ⋆S = 0 for
i = 0, . . . , m −1, then g is a generating polynomial for S.
TEAM LinG

19.1 Basic deﬁnitions and properties
427
Exercise 19.3. This exercise develops an alternative characterization lin-
early generated sequences. Let S = (z0, z1, . . .) be a sequence of elements
of F. Further, suppose that S has minimal polynomial φ = m
j=0 cjXj with
m > 0 and cm = 1. Deﬁne the matrix
A :=





z0
z1
· · ·
zm−1
z1
z2
· · ·
zm
...
...
...
...
zm−1
zm
· · ·
z2m−2




∈F m×m
and the vector
w := (zm, . . . , z2m−1) ∈F 1×m.
Show that
v = (−c0, . . . , −cm−1) ∈F 1×m
is the unique solution to the equation
vA = w.
Hint: show that the rows of A are linearly independent by making use of
Exercise 19.2 and the fact that no polynomial of degree less than m is a
generating polynomial for S.
Exercise 19.4.
Suppose that you are given a0, . . . , ak−1
∈
F
and
z0, . . . , zk−1 ∈F. Suppose that for all i ≥0, we deﬁne
zk+i :=
k−1

j=0
ajzj+i.
Given n ≥0, show how to compute zn using O(len(n)k2) operations in F.
Exercise 19.5. Let V be a vector space over F, and consider the set V ×∞
of all inﬁnite sequences (α0, α1, . . .), where the αi are in V . Let us deﬁne
the scalar product of g ∈F[X] and S ∈V ×∞as
g · S = (g ⋆S, (Xg) ⋆S, (X2g) ⋆S, . . .) ∈V ×∞.
Show that with this scalar product, V ×∞is an F[X]-module, and that a
polynomial g ∈F[X] is a generating polynomial for S ∈V ×∞if and only if
g · S = 0.
TEAM LinG

428
Linearly generated sequences and applications
19.2 Computing minimal polynomials: a special case
We now tackle the problem of computing the minimal polynomial of a lin-
early generated sequence from a suﬃciently long initial segment.
We shall ﬁrst address a special case of this problem, namely, the case
where the vector space V is just the ﬁeld F. In this case, we have
S = (z0, z1, z2, . . .),
where zi ∈F for i = 0, 1, 2, . . . .
Suppose that we do not know the minimal polynomial φ of S, but we
know an upper bound M ≥0 on its degree. Then it turns out that the
initial segment z0, z1, . . . z2M−1 completely determines φ, and moreover, we
can very eﬃciently compute φ given the bound M and this initial segment.
The following theorem provides the essential ingredient.
Theorem 19.2. Let S = (z0, z1, . . .) be a sequence of elements of F, and
deﬁne the reversed formal Laurent series
z :=
∞

i=0
ziX−(i+1) ∈F((X−1)),
whose coeﬃcients are the elements of the sequence S. Then for any g ∈F[X],
we have g ∈G(S) if and only if gz ∈F[X]. In particular, S is linearly
generated if and only if z is a rational function, in which case, its minimal
polynomial is the denominator of z when expressed as a fraction in lowest
terms.
Proof. Observe that for any polynomial g ∈F[X] and any integer i ≥0,
the coeﬃcient of X−(i+1) in the product gz is equal to Xig ⋆S —just look
at the formulas deﬁning these expressions! It follows that g is a generating
polynomial for S if and only if the coeﬃcients of the negative powers of X in
gz are all zero, which is the same as saying that gz ∈F[X]. Further, if g ̸= 0
and h := gz ∈F[X], then deg(h) < deg(g)—this follows simply from the fact
that deg(z) < 0 (together with the fact that deg(h) = deg(g) + deg(z)). All
the statements in the theorem follow immediately from these observations.
2
By virtue of Theorem 19.2, we can compute the minimal polynomial φ of S
using the algorithm in §18.5.2 for computing the numerator and denominator
of a rational function from its reversed Laurent series expansion.
More
precisely, we can compute φ given the bound M on its degree, along with
the ﬁrst 2M elements z0, . . . , z2M−1 of S, using O(M2) operations in F.
Just for completeness, we write down this algorithm:
TEAM LinG

19.3 Computing minimal polynomials: a more general case
429
1. Run the extended Euclidean algorithm on inputs
a := X2M and b := z0X2M−1 + z1X2M−2 + · · · + z2M−1,
and let s′, t′ be as in Theorem 18.7, using r∗:= M and t∗:= M.
2. Output φ := t′/ lc(t′).
The characterization of linearly generated sequences provided by Theo-
rem 19.2 is also very useful in other ways. For example, suppose the ﬁeld
F is ﬁnite. As we already saw in Example 19.1, any linearly generated se-
quence S := (z0, z1, . . .), where the zi are in F, must be ultimately periodic.
However, Theorem 19.2, together with the result of Exercise 18.13, tells us
much more; for example, if the minimal polynomial φ of S is not divisible
by X, then S is purely periodic with period equal to the multiplicative order
of [X]φ ∈(F[X]/(φ))∗.
19.3 Computing minimal polynomials: a more general case
Having dealt with the problem of ﬁnding the minimal polynomial of a se-
quence S of elements of F, we address the more general problem, where the
elements of S lie in a vector space V over F. We shall only deal with a
special case of this problem, but it is one which has useful applications:
• First, we shall assume that V has ﬁnite dimension ℓ> 0 over F.
• Second, we shall assume that the sequence S = (α0, α1, . . .) has full
rank, by which we mean the following: if the minimal polynomial φ
of S over F has degree m, then the vectors α0, . . . , αm−1 are linearly
independent. The sequences considered in Examples 19.2 and 19.3
are of this type.
• Third, we shall assume that F is a ﬁnite ﬁeld.
The Dual Space.
To develop the theory behind the approach we are
going to present, we need to discuss the dual space DF (V ) of V (over F),
which consists of all F-linear maps from V into F. We may sometimes refer
to elements of DF (V ) as projections. Now, as was discussed in §15.2, if
we ﬁx an ordered basis γ1, . . . , γℓfor V , the elements of V are in one-to-
one correspondence with the coordinate vectors F 1×ℓ, where the element
a1γ1 + . . . + aℓγℓ∈V corresponds to the coordinate vector (a1, . . . , aℓ) ∈
F 1×ℓ. The elements of DF (V ) are in one-to-one correspondence with F ℓ×1,
where the map π ∈DF (V ) corresponds to the column vector whose jth
coordinate is π(γj), for j = 1, . . . , ℓ. It is natural to call the column vector
corresponding to π its coordinate vector. A projection π ∈DF (V ) may
TEAM LinG

430
Linearly generated sequences and applications
be evaluated at a point δ ∈V by taking the product of the coordinate vector
of δ with the coordinate vector of π.
One may also impose a vector space structure on DF (V ), in a very natural
way: for π, π′ ∈DF (V ), the map π + π′ sends δ ∈V to π(δ) + π′(δ), and
for c ∈F, the map cπ sends δ ∈V to cπ(δ). By the observations in the
previous paragraph, DF (V ) is an F-vector space of dimension ℓ; indeed, the
sum and scalar multiplication operations on DF (V ) correspond to analogous
operations on coordinate vectors.
One last fact we need about the dual space is the following:
Theorem 19.3. Let V be an F-vector space of ﬁnite dimension ℓ> 0. For
any linearly independent vectors δ1, . . . , δm ∈V , and any a1, . . . , am ∈F,
there exists π ∈DF (V ) such that π(δi) = ai for i = 1, . . . , m.
Proof. Fix any ordered basis for V , and let M be the m×ℓmatrix whose ith
row is the coordinate vector of δi with respect to this ordered basis. Let v
be the m×1 column vector whose ith coordinate is ai. As the δi are linearly
independent, the rows of M must also be linearly independent. Therefore,
the F-linear map that sends w ∈F ℓ×1 to Mw ∈F m×1 is surjective. It
follows that any solution w to the equation v = Mw is the coordinate
vector of a map π ∈DF (V ) that satisﬁes the requirements of the theorem.
2
That completes our digression on the dual space. We now return to the
problem of computing the minimal polynomial φ of the linearly generated
sequence S = (α0, α1, . . .). Assume we have a bound M on the degree of φ.
As we are assuming S has full rank, we may assume that M ≤ℓ. For any π ∈
DF (V ), we may consider the projected sequence Sπ = (π(α0), π(α1), . . .).
Observe that φ is a generating polynomial for Sπ; indeed, for any polynomial
g ∈F[X], we have g ⋆Sπ = π(g ⋆S), and hence, for all i ≥0, we have
(Xiφ) ⋆Sπ = π((Xiφ) ⋆S) = π(0) = 0. Let φπ ∈F[X] denote the minimal
polynomial of Sπ. Since φπ divides any generating polynomial of Sπ, and
since φ is a generating polynomial for Sπ, it follows that φπ is a divisor of
φ.
This suggests the following algorithm for eﬃciently computing the mini-
mal polynomial of S:
TEAM LinG

19.3 Computing minimal polynomials: a more general case
431
Algorithm MP:
g ←1 ∈F[X]
repeat
choose π ∈DF (V ) at random
compute the ﬁrst 2M terms of the projected sequence Sπ
use the algorithm in §19.2 to compute the minimal polynomial
φπ of Sπ
g ←lcm(g, φπ)
until g ⋆S = 0
output g
A few remarks on the above procedure are in order:
• in every iteration of the main loop, g is the least common multiple of
a number of divisors of φ, and hence is itself a divisor of φ;
• under our assumption that S has full rank, and since g is a monic
divisor of φ, if g ⋆S = 0, we may safely conclude that g = φ;
• under our assumption that F is ﬁnite, choosing a random element π
of DF (V ) amounts to simply choosing at random the entries of the
coordinate vector of π, relative to some ordered basis for V ;
• we also assume that elements of V are represented as coordinate
vectors, so that applying a projection π ∈DF (V ) to a vector in V
takes O(ℓ) operations in F;
• similarly, adding two elements of V , or multiplying an element of V
times a scalar, takes O(ℓ) operations in F.
Based on the above observations, it follows that when the algorithm halts,
its output is correct, and that the cost of each loop iteration is O(Mℓ)
operations in F. The remaining question to be answered is this: what is
the expected number of iterations of the main loop? The answer to this
question is O(1), which leads to a total expected cost of Algorithm MP of
O(Mℓ) operations in F.
The key to establishing that the expected number of iterations of the main
loop is constant is provided by the following theorem.
Theorem 19.4. Let S = (α0, α1, . . .) be a linearly generated sequence over
the ﬁeld F, where the αi are elements of a vector space V of ﬁnite dimension
ℓ> 0. Let φ be the minimal polynomial of S over F, let m := deg(φ), and
assume that S has full rank (i.e., α0, . . . , αm−1 are linearly independent).
Under the above assumptions, there exists a surjective F-linear map σ :
DF (V ) →F[X]<m such that for all π ∈DF (V ), the minimal polynomial φπ
TEAM LinG

432
Linearly generated sequences and applications
of the projected sequence Sπ := (π(α0), π(α1), . . .) satisﬁes
φπ =
φ
gcd(σ(π), φ).
Recall that F[X]<m denotes the m-dimensional vector space of polynomials
in F[X] of degree less than m.
Proof. While the statement of this theorem looks a bit complicated, its proof
is quite straightforward, given our characterization of linearly generated
sequences in Theorem 19.2 in terms of rational functions.
We build the
linear map σ as the composition of two linear maps, σ0 and σ1.
Let us deﬁne the map
σ0 :
DF (V ) →F((X−1))
π →
∞

i=0
π(αi)X−(i+1).
We also deﬁne the map σ1 to be the φ-multiplication map on F((X−1)), that
is, the map that sends z ∈F((X−1)) to φ · z ∈F((X−1)). The map σ is just
the composition σ = σ1 ◦σ0. It is clear that both σ0 and σ1 are F-linear
maps, and hence, so is σ.
First, observe that for π ∈DF (V ), the series z := σ0(π) is the series
associated with the projected sequence Sπ, as in Theorem 19.2. Let φπ be
the minimal polynomial of Sπ. Since φ is a generating polynomial for S,
it is also a generating polynomial for Sπ. Therefore, Theorem 19.2 tells us
that
h := σ(π) = φ · z ∈F[X]<m,
and that φπ is the denominator of z when expressed as a fraction in lowest
terms. Now, we have z = h/φ, and it follows that φπ = φ/ gcd(h, φ) is this
denominator.
Second, the hypothesis that α0, . . . , αm−1 are linearly independent, to-
gether with Theorem 19.3, implies that dimF (img(σ0)) ≥m.
Also, ob-
serve that σ1 is an injective map (indeed, it is surjective as well). There-
fore, dimF (img(σ)) ≥m.
In the previous paragraph, we observed that
img(σ) ⊆F[X]<m, and since dimF (F[X]<m) = m, we may conclude that
img(σ) = F[X]<m. That proves the theorem. 2
Given the above theorem, we can analyze the expected number of itera-
tions of the main loop of Algorithm MP.
First of all, we may as well assume that the degree m of φ is greater than
0, as otherwise, we are sure to get φ in the very ﬁrst iteration. Let π1, . . . , πs
TEAM LinG

19.3 Computing minimal polynomials: a more general case
433
be the random projections chosen in the ﬁrst s iterations of Algorithm MP.
By Theorem 19.4, the polynomials σ(π1), . . . , σ(πs) are uniformly and inde-
pendently distributed over F[X]<m, and we have g = φ at the end of loop
iteration s if and only if gcd(φ, σ(π1), . . . , σ(πs)) = 1.
Let us deﬁne Λφ
F (s) to be the probability that gcd(φ, f1, . . . , fs) = 1, where
f1, . . . , fs are randomly chosen from F[X]<m. Thus, the probability that we
have g = φ at the end of loop iteration s is equal to Λφ
F (s). While one
can analyze the quantity Λφ
F (s), it turns out to be easier, and suﬃcient for
our purposes, to analyze a diﬀerent quantity. Let us deﬁne Λm
F (s) to be the
probability that gcd(f1, . . . , fs) = 1, where f1, . . . , fs are randomly chosen
from F[X]<m. Clearly, Λφ
F (s) ≥Λm
F (s).
Theorem 19.5. If F is a ﬁnite ﬁeld of cardinality q, and m and s are
positive integers, then we have
Λm
F (s) = 1 −1/qs−1 + (q −1)/qsm.
Proof. For any positive integer n, let Un be the set of all tuples of polynomials
(f1, . . . , fs) ∈F[X]×s
<n with gcd(f1, . . . , fs) = 1, and let un = |Un|. First, let
h be any monic polynomial with k := deg(h) < n. The set Un,h of all s-
tuples of polynomials of degree less than n whose gcd is h is in one-to-one
correspondence with Un−k, via the map that sends (f1, . . . , fs) ∈Un,h to
(f1/h, . . . , fs/h) ∈Un−k. As there are qk possible choices for h of degree
k, we see that the set Vn,k, consisting of tuples (f1, . . . , fs) ∈F[X]×s
<n with
deg(gcd(f1, . . . , fs)) = k, has cardinality qkun−k. Every non-zero element of
F[X]×s
<n appears in exactly one of the sets Vn,k, for k = 0, . . . , n −1. Taking
into account the zero polynomial, it follows that
qsn = 1 +
n−1

k=0
qkun−k,
(19.2)
which holds for all n ≥1. Replacing n by n −1 in (19.2), we obtain
qs(n−1) = 1 +
n−2

k=0
qkun−1−k,
(19.3)
which holds for all n ≥2, and indeed, holds for n = 1 as well. Subtracting
q times (19.3) from (19.2), we deduce that for n ≥1,
qsn −qsn−s+1 = 1 + un −q,
and rearranging terms:
un = qsn −qsn−s+1 + q −1.
TEAM LinG

434
Linearly generated sequences and applications
Therefore,
Λm
F (s) = um/qsm = 1 −1/qs−1 + (q −1)/qsm. 2
From the above theorem, it follows that for s ≥1, the probability Ps that
Algorithm MP runs for more than s loop iterations is at most 1/qs−1. If T
is the total number of loop iterations, then
E[T] =

i≥1
P[T ≥i] = 1 +

s≥1
Ps ≤1 +

s≥1
1/qs−1 = 1 +
q
q −1 = O(1).
Let us summarize all of the above analysis with the following:
Theorem 19.6. Let S be a sequence of elements of an F-vector space V of
ﬁnite dimension ℓ> 0 over F, where F is a ﬁnite ﬁeld. Assume that S is
linearly generated over F with minimal polynomial φ ∈F[X] of degree m, and
that S has full rank (i.e., the ﬁrst m elements of S are linearly independent).
Then given an upper bound M on m, along with the ﬁrst 2M elements of
S, Algorithm MP correctly computes φ using an expected number of O(Mℓ)
operations in F.
We close this section with the following observation.
Suppose the se-
quence S is of the form (β, τ(β), τ 2(β), . . .), where β ∈V and τ : V →V is
an F-linear map. Suppose that with respect to some ordered basis for V , el-
ements of V are represented as elements of F 1×ℓ, and elements of DF (V ) are
represented as elements of F ℓ×1. The linear map τ also has a corresponding
representation as a matrix A ∈F ℓ×ℓ, so that evaluating τ at a point α in
V corresponds to multiplying the coordinate vector of α on the right by A.
Now, suppose β ∈V has coordinate vector b ∈F 1×ℓand that π ∈DF (V )
has coordinate vector c⊤∈F ℓ×1. Then if ˜S is the sequence of coordinate
vectors of the elements of S, we have
˜S = (bAi)∞
i=0 and Sπ = (bAic
⊤)∞
i=0.
This more concrete, matrix-oriented point of view is sometimes useful; in
particular, it makes quite transparent the symmetry of the roles played by
β and π in forming the projected sequence.
Exercise 19.6. If |F| = q and φ ∈F[X] is monic and factors into monic
irreducible polynomials in F[X] as φ = pe1
1 · · · per
r , show that
Λφ
F (1) =
r

i=1
(1 −q−deg(pi)) ≥1 −
r

i=1
q−deg(pi).
From this, conclude that the probability that Algorithm MP terminates
TEAM LinG

19.4 Solving sparse linear systems
435
after just one loop iteration is 1 −O(m/q), where m = deg(φ). Thus, if q
is very large relative to m, it is highly likely that Algorithm MP terminates
after just one iteration of the main loop.
19.4 Solving sparse linear systems
Let V be a vector space of ﬁnite dimension ℓ> 0 over a ﬁnite ﬁeld F, and
let τ : V →V be an F-linear map. The goal of this section is to develop
time- and space-eﬃcient algorithms for solving equations of the form
τ(γ) = δ;
(19.4)
that is, given τ and δ ∈V , ﬁnd γ ∈V satisfying (19.4). The algorithms we
develop will have the following properties: they will be probabilistic, and
will use an expected number of O(ℓ2) operations in F, an expected number
of O(ℓ) evaluations of τ, and space for O(ℓ) elements of F. By an “evaluation
of τ,” we mean the computation of τ(α) for some α ∈V .
We shall assume that elements of V are represented as coordinate vectors
with respect to some ﬁxed ordered basis for V . Now, if the matrix rep-
resenting τ with respect to the given ordered basis is sparse, having, say,
ℓ1+o(1) non-zero entries, then the space required to represent τ is ℓ1+o(1)
elements of F, and the time required to evaluate τ is ℓ1+o(1) operations in
F. Under these assumptions, our algorithms to solve (19.4) use an expected
number of ℓ2+o(1) operations in F, and space for ℓ1+o(1) elements of F. This
is to be compared with standard Gaussian elimination: even if the original
matrix is sparse, during the execution of the algorithm, most of the entries
in the matrix may eventually be “ﬁlled in” with non-zero ﬁeld elements,
leading to a running time of Ω(ℓ3) operations in F, and a space requirement
of Ω(ℓ2) elements of F. Thus, the algorithms presented here will be much
more eﬃcient than Gaussian elimination when the matrix representing τ is
sparse.
We hasten to point out that the algorithms presented here may be more
eﬃcient than Gaussian elimination in other cases, as well. All that matters
is that τ can be evaluated using o(ℓ2) operations in F and/or represented
using space for o(ℓ2) elements of F —in either case, we obtain a time and/or
space improvement over Gaussian elimination. Indeed, there are applica-
tions where the matrix of the linear map τ may not be sparse, but never-
theless has special structure that allows it to be represented and evaluated
in subquadratic time and/or space.
We shall only present algorithms that work in two special, but important,
cases:
TEAM LinG

436
Linearly generated sequences and applications
• the ﬁrst case is where τ is invertible,
• the second case is where τ is not invertible, δ = 0, and a non-zero
solution γ to (19.4) is required (i.e., we are looking for a non-zero
element of ker(τ)).
In both cases, the key will be to use Algorithm MP in §19.3 to ﬁnd the
minimal polynomial φ of the linearly generated sequence
S := (α0, α1, . . .),
(αi = τ i(β), i = 0, 1, . . .),
(19.5)
where β is a suitably chosen element of V . From the discussion in Exam-
ple 19.3, this sequence has full rank, and so we may use Algorithm MP. We
may use M := ℓas an upper bound on the degree of φ (assuming we know
nothing more about τ and β that would allow us to use a smaller upper
bound). In using Algorithm MP in this application, note that we do not
want to store α0, . . . , α2ℓ−1—if we did, we would not satisfy our stated space
bound. Instead of storing the αi in a “warehouse,” we use a “just in time”
strategy for computing them, as follows:
• In the body of the main loop of Algorithm MP, where we calculate the
values ai := π(αi), for i = 0 . . . 2ℓ−1, we perform the computation
as follows:
α ←β
for i ←0 to 2ℓ−1 do
ai ←π(α), α ←τ(α)
• In the test at the bottom of the main loop of Algorithm MP, if g =
k
j=0 gjXj, we compute ν := g ⋆S ∈V as follows:
ν ←0, α ←β
for j ←0 to k do
ν ←ν + gj · α, α ←τ(α)
Alternatively, one could use a Horner-like algorithm:
ν ←0
for j ←k down to 0 do
ν ←τ(ν) + gj · β
With this implementation, Algorithm MP uses an expected number of O(ℓ2)
operations in F, an expected number of O(ℓ) evaluations of τ, and space
for O(ℓ) elements of F. Of course, the “warehouse” strategy is faster than
the “just in time” strategy by a constant factor, but it uses about ℓtimes
as much space; thus, for large ℓ, using the “just in time” strategy is a very
good time/space trade-oﬀ.
TEAM LinG

19.4 Solving sparse linear systems
437
The invertible case. Now consider the case where τ is invertible, and
we want to solve (19.4) for a given δ ∈V . We may as well assume that
δ ̸= 0, since otherwise, γ = 0 is the unique solution to (19.4). We proceed
as follows.
First, using Algorithm MP as discussed above, compute the
minimal polynomial φ of the sequence S deﬁned in (19.5), using β := δ. Let
φ = m
j=0 cjXj, where cm = 1 and m > 0. Then we have
c0δ + c1τ(δ) + · · · + cmτ m(δ) = 0.
(19.6)
We claim that c0 ̸= 0. To prove the claim, suppose that c0 = 0. Then
applying τ −1 to (19.6), we would obtain
c1δ + · · · + cmτ m−1(δ) = 0,
which would imply that φ/X is a generating polynomial for S, contradicting
the minimality of φ. That proves the claim.
Since c0 ̸= 0, we can apply τ −1 to (19.6), and solve for γ = τ −1(δ) as
follows:
γ = −c−1
0 (c1δ + · · · + cmτ m−1(δ)).
To actually compute γ, we use the same “just in time” strategy as was
used in the implementation of the computation of g ⋆S in Algorithm MP,
which costs O(ℓ2) operations in F, O(ℓ) evaluations of τ, and space for O(ℓ)
elements of F.
The non-invertible case. Now consider the case where τ is not invertible,
and we want to ﬁnd non-zero vector γ ∈V such that τ(γ) = 0. The idea
is this. Suppose we choose an arbitrary, non-zero element β of V , and use
Algorithm MP to compute the minimal polynomial φ of the sequence S
deﬁned in (19.5), using this value of β. Let φ = m
j=0 cjXj, where m > 0
and cm = 1. Then we have
c0β + c1τ(β) + · · · + cmτ m(β) = 0.
(19.7)
Let
γ := c1β + · · · cmτ m−1(β).
We must have γ ̸= 0, since γ = 0 would imply that ⌊φ/X⌋is a non-zero
generating polynomial for S, contradicting the minimality of φ. If it happens
that c0 = 0, then equation (19.7) implies that τ(γ) = 0, and we are done.
As before, to actually compute γ, we use the same “just in time” strategy
as was used in the implementation of the computation of g ⋆S in Algorithm
MP, which costs O(ℓ2) operations in F, O(ℓ) evaluations of τ, and space for
O(ℓ) elements of F.
TEAM LinG

438
Linearly generated sequences and applications
The above approach fails if c0 ̸= 0. However, in this “bad” case, equation
(19.7) implies that β = −c−1
0 τ(γ); that is, β ∈img(τ). One way to avoid
such a “bad” β is to randomize: as τ is not surjective, the image of τ is a
subspace of V of dimension strictly less than ℓ, and therefore, a randomly
chosen β lies in the image of τ with probability at most 1/|F|. So a simple
technique is to choose repeatedly β at random until we get a “good” β.
The overall complexity of the resulting algorithm will be as required: O(ℓ2)
expected operations in F, O(ℓ) expected evaluations of τ, and space for O(ℓ)
elements of F.
As a special case of this situation, consider the problem that arose in
Chapter 16 in connection with algorithms for computing discrete logarithms
and factoring. We had to solve the following problem: given an ℓ× (ℓ−1)
matrix M with entries in a ﬁnite ﬁeld F, containing ℓ1+o(1) non-zero entries,
ﬁnd a non-zero vector v ∈F 1×ℓsuch that vM = 0. To solve this problem,
we can augment the matrix M, adding an extra column of zeros, to get an
ℓ× ℓmatrix M′. Now, let V = F 1×ℓand let τ be the F-linear map on V
that sends γ ∈V to γM′. A non-zero solution γ to the equation τ(γ) = 0
will provide us with the solution to our original problem; thus, we can apply
the above technique directly, solving this problem using ℓ2+o(1) expected
operations in F, and space for ℓ1+o(1) elements of F. As a side remark, in
this particular application, we can choose a “good” β in the above algorithm
without randomization: just choose β := (0, . . . , 0, 1), which is clearly not
in the image of τ.
19.5 Computing minimal polynomials in F[X]/(f) (II)
Let us return to the problem discussed in §18.2: F is a ﬁeld, f ∈F[X] is
a monic polynomial of degree ℓ> 0, and E := F[X]/(f) = F[η], where
η := [X]f; we are given an element α ∈E, and want to compute the minimal
polynomial φ ∈F[X] of α over F. As discussed in Example 19.2, this problem
is equivalent to the problem of computing the minimal polynomial of the
sequence
S := (α0, α1, . . .)
(αi := αi, i = 0, 1, . . .),
and the sequence has full rank; therefore, we can use Algorithm MP in §19.3
directly to solve this problem, assuming F is a ﬁnite ﬁeld.
If we use the “just in time” strategy in the implementation of Algorithm
MP, as was used in §19.4, we get an algorithm that computes the minimal
polynomial of α using O(ℓ3) expected operations in F, but space for just
O(ℓ2) elements of F. Thus, in terms of space, this approach is far superior
TEAM LinG

19.5 Computing minimal polynomials in F[X]/(f) (II)
439
to the algorithm in §18.2, based on Gaussian elimination. In terms of time
complexity, the algorithm based on linearly generated sequences is a bit
slower than the one based on Gaussian elimination (but only by a constant
factor). However, if we use any subquadratic-time algorithm for polynomial
arithmetic (see §18.6 and §18.7), we immediately get an algorithm that runs
in subcubic time, while still using linear space. In the exercises below, you
are asked to develop an algorithm that computes the minimal polynomial
of α using just O(ℓ2.5) operations in F, at the expense of requiring space
for O(ℓ1.5) elements of F —this algorithm does not rely on fast polynomial
arithmetic, and can be made even faster if such arithmetic is used.
Exercise 19.7. Let f ∈F[X] be a monic polynomial of degree ℓ> 0 over a
ﬁeld F, and let E := F[X]/(f). Also, let η := [X]f ∈E. For computational
purposes, we assume that elements of E and DF (E) are represented as co-
ordinate vectors with respect to the usual “polynomial” basis 1, η, . . . , ηℓ−1.
For β ∈E, let Mβ denote the β-multiplication map on E that sends α ∈E
to αβ ∈E, which is an F-linear map from E into E.
(a) Show how to compute — given as input the polynomial f deﬁning
E, along with a projection π ∈DF (E) and an element β ∈E —the
projection π ◦Mβ ∈DF (E), using O(ℓ2) operations in F.
(b) Show how to compute — given as input the polynomial f deﬁning
E, along with a projection π ∈DF (E), an element α ∈E, and a
parameter k > 0—all of the k values
π(1), π(α), . . . , π(αk−1)
using just O(kℓ+ k1/2ℓ2) operations in F, and space for O(k1/2ℓ)
elements of F. Hint: use the same hint as in Exercise 18.4.
Exercise 19.8. Let f ∈F[X] be a monic polynomial over a ﬁnite ﬁeld F
of degree ℓ> 0, and let E := F[X]/(f). Show how to use the result of the
previous exercise, as well as Exercise 18.4, to get an algorithm that computes
the minimal polynomial of α ∈E over F using O(ℓ2.5) expected operations
in F, and space for O(ℓ1.5) operations in F.
Exercise 19.9. Let f ∈F[X] be a monic polynomial of degree ℓ> 0 over
a ﬁeld F (not necessarily ﬁnite), and let E := F[X]/(f). Further, suppose
that f is irreducible, so that E is itself a ﬁeld.
Show how to compute
the minimal polynomial of α ∈E over F deterministically, satisfying the
following complexity bounds:
(a) O(ℓ3) operations in F and space for O(ℓ) elements of F;
TEAM LinG

440
Linearly generated sequences and applications
(b) O(ℓ2.5) operations in F and space for O(ℓ1.5) elements of F.
19.6 The algebra of linear transformations (∗)
Throughout this chapter, one could hear the whispers of the algebra of linear
transformations. We develop some of the aspects of this theory here, leaving
a number of details as exercises. It will not play a role in any material that
follows, but it serves to provide the reader with a “bigger picture.”
Let F be a ﬁeld and V be a non-trivial F-vector space. We denote by
LF (V ) the set of all F-linear maps from V into V .
Elements of LF (V )
are called linear transformations. We can make LF (V ) into an F-vector
space by deﬁning addition and scalar multiplication as follows: for τ, τ ′ ∈
LF (V ), deﬁne τ + τ ′ to be the map that sends α ∈V to τ(α) + τ ′(α); for
c ∈F and τ ∈LF (V ), deﬁne cτ to be the map that sends α ∈V to cτ(α).
Exercise 19.10.
(a) Verify that with addition and scalar multiplication
deﬁned as above, LF (V ) is an F-vector space.
(b) Suppose that V has ﬁnite dimension ℓ> 0. By identifying elements
of LF (V ) with ℓ×ℓmatrices over F, show that LF (V ) has dimension
ℓ2.
As usual, for τ, τ ′ ∈LF (V ), the composed map, τ ◦τ ′ that sends α ∈
V to τ(τ ′(α)) is also an element of LF (V ) (verify). As always, function
composition is associative (i.e., for τ, τ ′, τ ′′ ∈LF (V ), we have τ ◦(τ ′ ◦τ ′′) =
(τ ◦τ ′) ◦τ ′′); however, function composition is not in general commutative
(i.e., we may have τ ◦τ ′ ̸= τ ′◦τ for some τ, τ ′ ∈LF (V )). For any τ ∈LF (V )
and an integer i ≥0, the map τ i (i.e., the i-fold composition of τ) is also an
element of LF (V ). Note that for any τ ∈LF (V ), the map τ 0 is by deﬁnition
just the identity map on V .
For any τ ∈LF (V ), and for any polynomial f ∈F[X], with f = 
i aiXi,
we denote by f(τ) the linear transformation
f(τ) :=

i
aiτ i.
Exercise 19.11. Verify the following properties of LF (V ). For all τ, τ ′, τ ′′ ∈
LF (V ), for all c ∈F, and all f, g ∈F[X]:
(a) τ ◦(τ ′ + τ ′′) = τ ◦τ ′ + τ ◦τ ′′;
(b) (τ ′ + τ ′′) ◦τ = τ ′ ◦τ + τ ′′ ◦τ;
(c) c(τ ◦τ ′) = (cτ) ◦τ ′ = τ ◦(cτ ′);
(d) f(τ) ◦g(τ) = (fg)(τ) = g(τ) ◦f(τ);
TEAM LinG

19.6 The algebra of linear transformations (∗)
441
(e) f(τ) + g(τ) = (f + g)(τ).
Under the addition operation of the vector space LF (V ), and deﬁning
multiplication on LF (V ) using the “◦” operation, we get an algebraic struc-
ture that satisﬁes all the properties of Deﬁnition 9.1, with the exception of
property (v) of that deﬁnition (commutativity). Thus, we can view LF (V )
as a non-commutative ring with unity (the identity map acts as the multi-
plicative identity).
For a ﬁxed τ ∈LF (V ), we may consider the subset of LF (V ),
F[τ] := {f(τ) : f ∈F[X]},
which does in fact satisfy all the properties of Deﬁnition 9.1. Moreover, we
can view F as a subring of F[τ] by identifying c ∈F with cτ 0 ∈F[τ]. With
this convention, for f ∈F[X], the expression f(τ) has its usual meaning as
the value of f evaluated at the point τ in the extension ring F[τ] of F. Let
φτ is the minimal polynomial of τ over F, so that F[τ] is isomorphic as an
F-algebra to F[X]/(φτ). We can also characterize φτ as follows (verify):
if there exists a non-zero polynomial f ∈F[X] such that f(τ) =
0, then φτ is the monic polynomial of least degree with this
property; otherwise, φτ = 0.
Another way to characterize φ is as follows (verify):
φτ is the minimal polynomial of the sequence (1, τ, τ 2, . . .).
Note that φτ is never 1 — this follows from the assumption that V is
non-trivial.
It is easy to see that if V happens to be ﬁnite dimensional, with ℓ:=
dimF (V ), then by Exercise 19.10, LF (V ) has dimension ℓ2. Therefore, there
must be a linear dependence among 1, τ, . . . , τ ℓ2, which implies that the
minimal polynomial of τ is non-zero with degree at most ℓ2. We shall show
below that in this case, the minimal polynomial of τ actually has degree at
most ℓ.
For a ﬁxed τ ∈LF (V ), we can deﬁne a “scalar multiplication” operation
⊙, that maps f ∈F[X] and α ∈V to
f ⊙α := f(τ)(α) ∈V ;
that is, if f = 
i aiXi, then
f ⊙α =

i
aiτ i(α).
TEAM LinG

442
Linearly generated sequences and applications
Exercise 19.12. Show that the scalar multiplication ⊙, together with the
usual addition operation on V , makes V into an F[X]-module; that is, show
that for all f, g ∈F[X] and α, β ∈V , we have
f ⊙(g ⊙α) = (fg) ⊙α, (f + g) ⊙α = f ⊙α + g ⊙α,
f ⊙(α + β) = f ⊙α + f ⊙β, 1 ⊙α = α.
Note that each choice of τ gives rise to a diﬀerent F[X]-module structure,
but all of these structures are extensions of the usual vector space structure,
in the sense that for all c ∈F and α ∈V , we have c ⊙α = cα.
Now, for ﬁxed τ ∈LF (V ) and α ∈V , consider the F[X]-linear map
ρτ,α : F[X] →V that sends f ∈F[X] to f ⊙α = f(τ)(α). The kernel of this
map must be a submodule, and hence an ideal, of F[X]; since every ideal
of F[X] is principal, it follows that ker(ρτ,α) is the ideal of F[X] generated
by some polynomial φτ,α, which we can make unique by insisting that it is
monic or zero. We call φτ,α the minimal polynomial of α under τ.We
can also characterize φτ,α as follows (verify):
if there exists a non-zero polynomial f ∈F[X] such that
f(τ)(α) = 0, then φτ,α the monic polynomial of least degree
with this property; otherwise, φτ,α = 0.
Another way to characterize φτ,α is as follows (verify):
φτ,α is the minimal polynomial of the sequence
(α, τ(α), τ 2(α), . . .).
Note that since φτ(τ) is the zero map, we have
φτ ⊙α = φτ(τ)(α) = 0,
and hence φτ ∈ker(ρτ,α), which means that φτ,α | φτ.
Now consider the image of ρτ,α, which we shall denote by ⟨α⟩τ. As an F[X]-
module, ⟨α⟩τ is isomorphic to F[X]/(φτ,α). In particular, if φτ,α is non-zero
and has degree m, then ⟨α⟩τ is a vector space of dimension m over F; indeed,
the vectors α, τ(α), . . . , τ m−1(α) form a basis for ⟨α⟩τ over F; moreover, m
is the smallest non-negative integer such that α, τ(α), . . . , τ m(α) are linearly
dependent.
Observe that for any β ∈⟨α⟩τ, we have φτ,α ⊙β = 0; indeed, if β = f ⊙α,
then
φτ,α ⊙(f ⊙α) = (φτ,αf) ⊙α = f ⊙(φτ,α ⊙α) = f ⊙0 = 0.
In the following three exercises, τ is an element of LF (V ), and ⊙is the
associated scalar multiplication that makes V into an F[X]-module.
TEAM LinG

19.6 The algebra of linear transformations (∗)
443
Exercise 19.13. Let α ∈V have minimal polynomial f ∈F[X] under τ,
and let β ∈V have minimal polynomial g ∈F[X] under τ. Show that if
gcd(f, g) = 1, then
(a) ⟨α⟩τ ∩⟨β⟩τ = {0}, and
(b) α + β has minimal polynomial f · g under τ.
Exercise 19.14. Let α ∈V . Let q ∈F[X] be a monic irreducible polynomial
such that qe ⊙α = 0 but qe−1 ⊙α ̸= 0 for some integer e ≥1. Show that qe
is the minimal polynomial of α under τ.
Exercise 19.15. Let α ∈V , and suppose that α has minimal polynomial
f ∈F[X] under τ, with f ̸= 0. Let g ∈F[X]. Show that g ⊙α has minimal
polynomial f/ gcd(f, g) under τ.
We are now ready to state the main result of this section, whose statement
and proof are analogous to that of Theorem 8.40:
Theorem 19.7. Let τ ∈LF (V ), and suppose that τ has non-zero minimal
polynomial φ. Then there exists β ∈V such that the minimal polynomial of
β under τ is φ.
Proof. Let ⊙be the scalar multiplication associated with τ.
Let φ =
pe1
1 · · · per
r
be the factorization of φ into monic irreducible polynomials in
F[X].
First, we claim that for each i = 1, . . . , r, there exists αi ∈V such that
φ/pi ⊙αi ̸= 0. Suppose the claim were false: then for some i, we would
have φ/pi ⊙α = 0 for all α ∈V ; however, this means that (φ/pi)(τ) =
0, contradicting the minimality property in the deﬁnition of the minimal
polynomial φ. That proves the claim.
Let α1, . . . , αr be as in the above claim. Then by Exercise 19.14, each
φ/pei
i ⊙αi has minimal polynomial pei
i
under τ.
Finally, by part (b) of
Exercise 19.13, the vector
β := φ/pe1
1 ⊙α1 + · · · + φ/per
r ⊙αr
has minimal polynomial φ under τ. 2
Theorem 19.7 says that if τ has minimal polynomial φ of degree m ≥0,
then there exists β ∈V such that
β, τ(β), . . . , τ m−1(β)
are linearly independent. From this, it immediately follows that:
TEAM LinG

444
Linearly generated sequences and applications
Theorem 19.8. If V has ﬁnite dimension ℓ> 0, then for any τ ∈LF (V ),
the minimal polynomial of τ is non-zero of degree at most ℓ.
We close this section a simple observation. Let V be an arbitrary, non-
trivial F[X]-module with scalar multiplication ⊙. Restricting the scalar mul-
tiplication from F[X] to F, we can naturally view V as an F-vector space.
Let τ : V →V be the map that sends α ∈V to X ⊙α. It is easy to see that
τ ∈LF (V ), and that for all polynomials f ∈F[X], and all α ∈V , we have
f ⊙α = f(τ)(α). Thus, instead of starting with a vector space and deﬁning
an F[X]-module structure in terms of a given linear map, we can go the other
direction, starting from an F[X]-module and obtaining a corresponding lin-
ear map. Furthermore, using the language introduced in Examples 14.14
and 14.15, we see that the F[X]-exponent of V is the ideal of F[X] generated
by the minimal polynomial of τ, and the F[X]-order of any element α ∈V is
the ideal of F[X] generated by the minimal polynomial of α under τ. The-
orem 19.7 says that there exists an element in V whose F[X]-order is equal
to the F[X]-exponent of V , assuming the latter is non-zero.
So depending on one’s mood, one can place emphasis either on the linear
map τ, or just talk about F[X]-modules without mentioning any linear maps.
Exercise 19.16. Let τ ∈LF (V ) have non-zero minimal polynomial φ of
degree m, and let φ = pe1
1 · · · per
r
be the factorization of φ into monic irre-
ducible polynomials in F[X]. Let ⊙be the scalar multiplication associated
with τ. Show that β ∈V has minimal polynomial φ under τ if and only if
φ/pi ⊙β ̸= 0 for i = 1, . . . , r.
Exercise 19.17. Let τ ∈LF (V ) have non-zero minimal polynomial φ. Show
that τ is an invertible map if and only if X ∤φ.
Exercise 19.18. Let F be a ﬁnite ﬁeld, and let V have ﬁnite dimension
ℓ> 0 over F. Let τ ∈LF (V ) have minimal polynomial φ, with deg(φ) = m
(and of course, by Theorem 19.8, we have m ≤ℓ). Suppose that α1, . . . , αs
are randomly chosen elements of V . Let gj be the minimal polynomial of αj
under τ, for j = 1, . . . , s. Let Q be the probability that lcm(g1, . . . , gs) = φ.
The goal of this exercise is to show that Q ≥Λφ
F (s), where Λφ
F (s) is as
deﬁned in §19.3.
(a) Using Theorem 19.7 and Exercise 19.15, show that if m = ℓ, then
Q = Λφ
F (s).
(b) Without the assumption that m = ℓ, things are a bit more challeng-
ing. Adopting the matrix-oriented point of view discussed at the end
of §19.3, and transposing everything, show that
TEAM LinG

19.6 The algebra of linear transformations (∗)
445
– there exists π ∈DF (V ) such that the sequence (π ◦τ i)∞
i=0 has
minimal polynomial φ, and
– if, for j = 1, . . . , s, we deﬁne hj to be the minimal polyno-
mial of the sequence (π(τ i(αj)))∞
i=0, then the probability that
lcm(h1, . . . , hs) = φ is equal to Λφ
F (s).
(c) Show that hj | gj, for j = 1, . . . , s, and conclude that Q ≥Λφ
F (s).
Exercise 19.19. Let f, g ∈F[X] with f ̸= 0, and let h := f/ gcd(f, g).
Show that g · F[X]/(f) and F[X]/(h) are isomorphic as F[X]-modules.
Exercise 19.20. In this exercise, you are to derive the fundamental theo-
rem of ﬁnite dimensional F[X]-modules, which is completely analogous
to the fundamental theorem of ﬁnite abelian groups. Both of these results
are really special cases of a more general decomposition theorem for mod-
ules over a principal ideal domain. Let V be an F[X]-module. Assume that
as an F-vector space, V has ﬁnite dimension ℓ> 0, and that the F[X]-
exponent of V is generated by the monic polynomial φ ∈F[X] (note that
1 ≤deg(φ) ≤ℓ). Show that there exist monic, non-constant polynomials
φ1, . . . , φt ∈F[X] such that
• φi | φi+1 for i = 1, . . . , t −1, and
• V is isomorphic, as an F[X]-module, to the direct product of F[X]-
modules
V ′ := F[X]/(φ1) × · · · × F[X]/(φt).
Moreover, show that the polynomials φ1, . . . , φt satisfying these conditions
are uniquely determined, and that φt = φ. Hint: one can just mimic the
proof of Theorem 8.44, where the exponent of a group corresponds to the
F[X]-exponent of an F[X]-module, and the order of a group element cor-
responds to the F[X]-order of an element of an F[X]-module — everything
translates rather directly, with just a few minor, technical diﬀerences, and
the previous exercise is useful in proving the uniqueness part of the theorem.
Exercise 19.21. Let us adopt the same assumptions and notation as in
Exercise 19.20, and let τ ∈LF (V ) be the map that sends α ∈V to X ⊙α.
Further, let σ : V →V ′ be the isomorphism of that exercise, and let τ ′ ∈
LF (V ′) be the X-multiplication map on V ′.
(a) Show that σ ◦τ = τ ′ ◦σ.
(b) From part (a), derive the following: there exists an ordered basis for
V over F, with respect to which the matrix representing τ is the
TEAM LinG

446
Linearly generated sequences and applications
“block diagonal” matrix
T =





C1
C2
...
Ct




,
where each Ci is the companion matrix of φi (see Example 15.1).
Exercise 19.22. Let us adopt the same assumptions and notation as in
Exercise 19.20.
(a) Using the result of that exercise, show that V is isomorphic, as an
F[X]-module, to a direct product of F[X]-modules
F[X]/(pe1
1 ) × · · · × F[X]/(per
r ),
where the pi are monic irreducible polynomials (not necessarily dis-
tinct) and the ei are positive integers, and this direct product is
unique up to the order of the factors.
(b) Using part (a), show that there exists an ordered basis for V over
F, with respect to which the matrix representing τ is the “block
diagonal” matrix
T ′ =





C′
1
C′
2
...
C′
r




,
where each C′
i is the companion matrix of pei
i .
Exercise 19.23. Let us adopt the same assumptions and notation as in
Exercise 19.20.
(a) Suppose α ∈V corresponds to ([f1]φ1, . . . , [ft]φt) ∈V ′ under the iso-
morphism of that exercise. Show that the F[X]-order of α is generated
by the polynomial
lcm(φ1/ gcd(f1, φ1), . . . , φt/ gcd(ft, φt)).
(b) Using part (a), give a short and simple proof of the result of Exer-
cise 19.18.
TEAM LinG

19.7 Notes
447
19.7 Notes
Berlekamp [15] and Massey [60] discuss an algorithm for ﬁnding the mini-
mal polynomial of a linearly generated sequence that is closely related to the
one presented in §19.2, and which has a similar complexity. This connection
between Euclid’s algorithm and ﬁnding minimal polynomials of linearly gen-
erated sequences has been observed by many authors, including Mills [64],
Welch and Scholtz [102], and Dornstetter [35].
The algorithm presented in §19.3, is due to Wiedemann [103], as are the
algorithms for solving sparse linear systems in §19.4, as well as the statement
and proof outline of the result in Exercise 19.18.
Our proof of Theorem 19.5 is based on an exposition by Morrison [65].
Using fast matrix and polynomial arithmetic, Shoup [91] shows how to
implement the algorithms in §19.5 so as to use just O(ℓ(ω+1)/2) operations
in F, where ω is the exponent for matrix multiplication (see §15.6), and so
(ω + 1)/2 < 1.7.
TEAM LinG

20
Finite ﬁelds
This chapter develops some of the basic theory of ﬁnite ﬁelds. As we already
know (see Theorem 9.7), every ﬁnite ﬁeld must be of cardinality pw, for some
prime p and positive integer w. The main results of this chapter are:
• for any prime p and positive integer w, there exists a ﬁnite ﬁeld of
cardinality pw, and
• any two ﬁnite ﬁelds of the same cardinality are isomorphic.
20.1 Preliminaries
In this section, we prove a few simple facts that will be useful in this and
later chapters; also, for the reader’s convenience, we recall a few basic alge-
braic concepts that were discussed in previous chapters, but which will play
important roles in this chapter.
Theorem 20.1. Let F be a ﬁeld, and let k, ℓbe positive integers. Then
Xk −1 divides Xℓ−1 if and only if k divides ℓ.
Proof. Let ℓ= kq + r, with 0 ≤r < k. We have
Xℓ≡XkqXr ≡Xr (mod Xk −1),
and Xr ≡1 (mod Xk −1) if and only if r = 0. 2
Theorem 20.2. Let a ≥2 be an integer and let k, ℓbe positive integers.
Then ak −1 divides aℓ−1 if and only if k divides ℓ.
Proof. The proof is analogous to that of Theorem 20.1. We leave the details
to the reader. 2
One may combine these two theorems, obtaining:
448
TEAM LinG

20.1 Preliminaries
449
Theorem 20.3. Let a ≥2 be an integer, k, ℓbe positive integers, and F a
ﬁeld. Then Xak −X divides Xaℓ−X if and only if k divides ℓ.
Proof. We have Xak −X divides Xaℓ−X iﬀXak−1 −1 divides Xaℓ−1 −1, and by
Theorem 20.1, this happens iﬀak −1 divides aℓ−1, which by Theorem 20.2
happens iﬀk divides ℓ. 2
Let F be a ﬁeld. A polynomial f ∈F[X] is called square-free if it is not
divisible by the square of any polynomial of degree greater than zero. Using
formal derivatives, we obtain the following useful criterion for establishing
that a polynomial is square-free:
Theorem 20.4. If F is a ﬁeld, and f ∈F[X] with gcd(f, D(f)) = 1, then
f is square-free.
Proof. Suppose f is not square-free, and write f = g2h, for g, h ∈F[X] with
deg(g) > 0. Taking formal derivatives, we have
D(f) = 2gD(g)h + g2D(h),
and so clearly, g is a common divisor of f and D(f). 2
We end this section by recalling some concepts discussed earlier, mainly
in §17.1, §17.5, and §17.6.
Suppose F is a ﬁeld, and E is an extension ﬁeld of F; that is, F is a
subﬁeld of E, or F is embedded in E via some canonical embedding, and
we identify elements of F with their images in E under this embedding. We
may naturally view E as an F-vector space. Assume that as an F-vector
space, E has ﬁnite dimension ℓ> 0. This dimension ℓis called the degree
of E over F, and is denoted (E : F); moreover, E is called a ﬁnite extension
of F.
We may also naturally view E as an F-algebra, either via the inclusion
map or via some canonical embedding. Let E′ be another ﬁeld extension
of F, and let ρ : E →E′ be a ring homomorphism (which in fact, must be
injective). Then ρ is an F-algebra homomorphism if and only if ρ(a) = a
for all a ∈F.
For any α ∈E, the set F[α] = {g(α) : g ∈F[X]} is a subﬁeld of E
containing F. Moreover, there exists a non-zero polynomial g of degree at
most ℓsuch that g(α) = 0. The monic polynomial φ of least degree such that
φ(α) = 0 is called the minimal polynomial of α over F, and this polynomial
is irreducible over F. The ﬁeld F[X]/(φ) is isomorphic, as an F-algebra,
to F[α], via the map that sends [g]φ ∈F[X]/(φ) to g(α) ∈F[α]. We have
(F[α] : F) = deg(φ), and this value is called the degree of α over F. If E′ is
TEAM LinG

450
Finite ﬁelds
an extension ﬁeld of F, and if ρ : F[α] →E′ is an F-algebra homomorphism,
then the action of ρ is completely determined by its action on α; indeed, for
any g ∈F[X], we have ρ(g(α)) = g(ρ(α)).
20.2 The existence of ﬁnite ﬁelds
Let F be a ﬁnite ﬁeld. As we saw in Theorem 9.7, F must have cardinality
pw, where p is prime and w is a positive integer, and p is the characteristic of
F. However, we can say a bit more than this. As discussed in Example 9.41,
the ﬁeld Zp is embedded in F, and so we may simply view Zp as a subﬁeld
of F. Moreover, it must be the case that w is equal to (F : Zp).
We want to show that there exist ﬁnite ﬁelds of every prime-power cardi-
nality. Actually, we shall prove a more general result:
If F is a ﬁnite ﬁeld, then for every integer ℓ≥1, there exists
an extension ﬁeld E of degree ℓover F.
For the remainder of this section, F denotes a ﬁnite ﬁeld of cardinality
q = pw, where p is prime and w ≥1.
Suppose for the moment that E is an extension of degree ℓover F. Let
us derive some basic facts about E. First, observe that E has cardinality
qℓ. By Theorem 9.16, E∗is cyclic, and the order of E∗is qℓ−1. If γ ∈E∗
is a generator for E∗, then every non-zero element of E can be expressed
as a power of γ; in particular, every element of E can be expressed as a
polynomial in γ with coeﬃcients in F; that is, E = F[γ]. Let φ ∈F[X] be
the minimal polynomial of γ over F, which is an irreducible polynomial of
degree ℓ. It follows that F is isomorphic (as an F-algebra) to F[X]/(φ).
So we have shown that any extension of F of degree ℓmust be isomorphic,
as an F-algebra, to F[X]/(φ) for some irreducible polynomial φ ∈F[X] of
degree ℓ. Conversely, given any irreducible polynomial φ over F of degree ℓ,
we can construct the ﬁnite ﬁeld F[X]/(φ), which has degree ℓover F. Thus,
the question of the existence of a ﬁnite ﬁelds of degree ℓover F reduces to
the question of the existence of an irreducible polynomial over F of degree ℓ.
We begin with a simple generalization Fermat’s little theorem:
Theorem 20.5. For any a ∈F ∗, we have aq−1 = 1, and for any a ∈F, we
have aq = a.
Proof. The multiplicative group of units F ∗of F has order q −1, and hence,
every a ∈F ∗satisﬁes the equation aq−1 = 1. Multiplying this equation by
a yields aq = a for all a ∈F ∗, and this latter equation obviously holds for
a = 0 as well. 2
TEAM LinG

20.2 The existence of ﬁnite ﬁelds
451
Theorem 20.6. We have
Xq −X =

a∈F
(X −a).
Proof. The polynomial
(Xq −X) −

a∈F
(X −a)
has degree less than q, but has q distinct roots (namely, every element of
F), and hence must be the zero polynomial. 2
The following theorem generalizes Example 17.6:
Theorem 20.7. Let E be an F-algebra. Then the map ρ : E →E that
sends α ∈E to αq is an F-algebra homomorphism.
Proof. Recall that E being an F-algebra simply means that E is a ring
and that there is a ring homomorphism τ : F →E, and because F is a
ﬁeld, either τ is injective or E is trivial. Also, recall that ρ being an F-
algebra homomorphism simply means that ρ is a ring homomorphism and
ρ(τ(a)) = τ(a) for all a ∈F.
Now, if E is trivial, there is nothing to prove. Otherwise, as E contains a
copy of F, it must have characteristic p. Since q is a power of the character-
istic, the fact that ρ is a ring homomorphism follows from the discussion in
Example 9.42. Moreover, by Theorem 20.5, we have τ(a)q = τ(aq) = τ(a)
for all a ∈F. 2
Theorem 20.8. Let E be a ﬁnite extension of F, and consider the map σ :
E →E that sends α ∈E to αq ∈E. Then σ is an F-algebra automorphism
on E. Moreover, if α ∈E is such that σ(α) = α, then α ∈F.
Proof. The fact that σ is an F-algebra homomorphism follows from the pre-
vious theorem. Any ring homomorphism from a ﬁeld into a ﬁeld is injective
(see Exercise 9.38). Surjectivity follows from injectivity and ﬁniteness.
For the second statement, observe that σ(α) = α if and only if α is a root
of the polynomial Xq −X, and since all q elements of F are already roots of
this polynomial, there can be no other roots. 2
The map σ deﬁned in Theorem 20.8 is called the Frobenius map on E
over F. As it plays a fundamental role in the study of ﬁnite ﬁelds, let us
develop a few simple properties right away.
Since the composition of two F-algebra automorphisms is also an F-
algebra automorphism, for any i ≥0, the i-fold composition σi that sends
α ∈E to αqi is also an F-algebra automorphism.
TEAM LinG

452
Finite ﬁelds
Since σ is an F-algebra automorphism, the inverse function σ−1 is also
an F-algebra automorphism. Hence, σi is an F-algebra automorphism for
all i ∈Z. If E has degree ℓover F, then applying Theorem 20.5 to the ﬁeld
E, we see that σℓis the identity map, from which it follows that σ−1 =
σℓ−1. More generally, we see that for any i ∈Z, we have σi = σj, where
j = i mod ℓ.
Thus, in considering integer powers of σ, we need only consider the powers
σ0, σ1, . . . , σℓ−1. Furthermore, the powers σ0, σ1, . . . , σℓ−1 are all distinct
maps. To see this, assume that σi = σj for some i, j with 0 ≤i < j < ℓ.
Then σj−i would be the identity map, which would imply that all of the qℓ
elements of E were roots of the polynomial Xqj−i −X, which is a non-zero
polynomial of degree less that qℓ, and this yields a contradiction.
The following theorem generalizes Theorem 20.6:
Theorem 20.9. For k ≥1, let Pk denote the product of all the monic
irreducible polynomials in F[X] of degree k. For all positive integers ℓ, we
have
Xqℓ−X =

k|ℓ
Pk,
where the product is over all positive divisors k of ℓ.
Proof. First, we claim that the polynomial Xqℓ−X is square-free. This follows
immediately from Theorem 20.4, since D(Xqℓ−X) = qℓXqℓ−1 −1 = −1.
So we have reduced the proof to showing that if f is a monic irreducible
polynomial of degree k, then f divides Xqℓ−X if and only if k | ℓ.
Let
E := F[X]/(f), and let η := [X]f ∈E, which is a root of f.
For the ﬁrst implication, assume that f divides Xqℓ−X. We want to show
that k | ℓ. Now, if Xqℓ−X = fg, then ηqℓ−η = f(η)g(η) = 0, so ηqℓ= η.
Therefore, if σ is the Frobenius map on E over F, then we have σℓ(η) = η.
We claim that σℓ(α) = α for all α ∈E. To see this, recall from Theorem 17.1
that for all h ∈F[X] and β ∈E, we have σℓ(h(β)) = h(σℓ(β)). Moreover,
any α ∈E can be expressed as h(η) for some h ∈F[X], and so
σℓ(α) = σℓ(h(η)) = h(σℓ(η)) = h(η) = α.
That proves the claim.
From the claim, it follows that every element of E is a root of Xqℓ−X.
That is, 
α∈E(X −α) divides Xqℓ−X. Applying Theorem 20.6 to the ﬁeld
E, we see that 
α∈E(X −α) = Xqk −X, and hence Xqk −X divides Xqℓ−X.
By Theorem 20.3, this implies k divides ℓ.
TEAM LinG

20.2 The existence of ﬁnite ﬁelds
453
For the second implication, suppose that k | ℓ. We want to show that
f | Xqℓ−X. Since f is the minimal polynomial of η, and since η is a root
of Xqk −X, we must have that f divides Xqk −X. Since k | ℓ, and applying
Theorem 20.3 once more, we see that Xqk −X divides Xqℓ−X. That proves
the second implication, and hence, the theorem. 2
For ℓ≥1, let Π(ℓ) denote the number of monic irreducible polynomials
of degree ℓin F[X].
Theorem 20.10. For all ℓ≥1, we have
qℓ=

k|ℓ
kΠ(k).
(20.1)
Proof. Just equate the degrees of both sides of the identity in Theorem 20.9.
2
From Theorem 20.10 it is easy to deduce that Π(ℓ) > 0 for all ℓ, and in
fact, one can prove a density result—essentially a “prime number theorem”
for polynomials over ﬁnite ﬁelds:
Theorem 20.11. For all ℓ≥1, we have
qℓ
2ℓ≤Π(ℓ) ≤qℓ
ℓ,
(20.2)
and
Π(ℓ) = qℓ
ℓ+ O
qℓ/2
ℓ

.
(20.3)
Proof. First, since all the terms in the sum on the right hand side of (20.1)
are non-negative, and ℓΠ(ℓ) is one of these terms, we may deduce that
ℓΠ(ℓ) ≤qℓ, which proves the second inequality in (20.2). Since this holds
for all ℓ, we have
ℓΠ(ℓ) = qℓ−

k|ℓ
k<ℓ
kΠ(k) ≥qℓ−

k|ℓ
k<ℓ
qk ≥qℓ−
⌊ℓ/2⌋

k=1
qk.
Let us set
S(q, ℓ) :=
⌊ℓ/2⌋

k=1
qk =
q
q −1(q⌊ℓ/2⌋−1),
so that ℓΠ(ℓ) ≥qℓ−S(q, ℓ). It is easy to see that S(q, ℓ) = O(qℓ/2), which
proves (20.3).
For the ﬁrst inequality of (20.2), it suﬃces to show that
TEAM LinG

454
Finite ﬁelds
S(q, ℓ) ≤qℓ/2. One can check this directly for ℓ∈{1, 2, 3} (verify), and for
ℓ≥4, we have
S(q, ℓ) ≤qℓ/2+1 ≤qℓ−1 ≤qℓ/2. 2
We note that the inequalities in (20.2) are tight, in the sense that Π(ℓ) =
qℓ/2ℓwhen q = 2 and ℓ= 2, and Π(ℓ) = qℓwhen ℓ= 1. The ﬁrst inequality
in (20.2) implies not only that Π(ℓ) > 0, but that the fraction of all monic
degree ℓpolynomials that are irreducible is at least 1/2ℓ, while (20.3) says
that this fraction gets arbitrarily close to 1/ℓas either q or ℓare suﬃciently
large.
Exercise 20.1. Starting from Theorem 20.10, show that
Π(ℓ) = ℓ−1 
k|ℓ
µ(k)qℓ/k,
where µ is the M¨obius function (see §2.6).
Exercise 20.2. How many irreducible polynomials of degree 30 over Z2 are
there?
20.3 The subﬁeld structure and uniqueness of ﬁnite ﬁelds
We begin with a result that holds for ﬁeld extensions in general.
Theorem 20.12. Let E be an extension of a ﬁeld F, and let σ be an F-
algebra automorphism on E. Then the set E′ := {α ∈E : σ(α) = α} is a
subﬁeld of E containing F.
Proof. By deﬁnition, σ acts as the identity function on F, and so F ⊆E′.
To show that E′ is a subring of E, it suﬃces to show that E′ is closed under
addition and multiplication. To show that E′ is closed under addition, let
α, β ∈E′. Then σ(α + β) = σ(α) + σ(β) = α + β, and hence α + β ∈E′.
Replacing “+” by “·” in the above argument shows that E′ is closed under
multiplication. We conclude that E′ is a subring of E.
To complete the proof that E′ is a subﬁeld of E, we need to show that if
0 ̸= α ∈E′ and β ∈E with αβ = 1, then β ∈E′. We have
αβ = 1 = σ(1) = σ(αβ) = σ(α)σ(β) = ασ(β),
and hence αβ = ασ(β); canceling α, we obtain β = σ(β), and so β ∈E′. 2
The subﬁeld E′ in the above theorem is called the subﬁeld of E ﬁxed
TEAM LinG

20.3 The subﬁeld structure and uniqueness of ﬁnite ﬁelds
455
by σ. Turning our attention again to ﬁnite ﬁelds, the following theorem
completely characterizes the subﬁeld structure of a ﬁnite ﬁeld.
Theorem 20.13. Let E be an extension of degree ℓof a ﬁnite ﬁeld F, and
let σ be the Frobenius map on E over F. Then the intermediate ﬁelds E′,
with F ⊆E′ ⊆E, are in one-to-one correspondence with the divisors k of ℓ,
where the divisor k corresponds to the subﬁeld of E ﬁxed by σk, which has
degree k over F.
Proof. Let q be the cardinality of F.
Let k be a divisor of ℓ.
Now, by
Theorem 20.6, the polynomial Xqℓ−X splits into distinct linear factors over
E, and by Theorem 20.3, the polynomial Xqk −X divides Xqℓ−X. Hence,
Xqk −X also splits into distinct linear factors over E. This says that the
subﬁeld of E ﬁxed by σk, which consists of the roots of Xqk −X, has precisely
qk elements, and hence is an extension of degree k over F. That proves the
existence part of the theorem.
As for uniqueness, we have to show that any intermediate ﬁeld is of this
type. Let E′ be an intermediate ﬁeld of degree k over F. By Theorem 20.6,
we have Xqk −X = 
α∈E′(X −α) and Xqℓ−X = 
α∈E(X −α), from which it
follows that Xqk −X divides Xqℓ−X, and so by Theorem 20.3, we must have
k | ℓ. There can be no other intermediate ﬁelds of the same degree k over
F, since the elements of such a ﬁeld would also be roots of Xqk −X. 2
The next theorem shows that up to isomorphism, there is only one ﬁnite
ﬁeld of a given cardinality.
Theorem 20.14. Let E, E′ be extensions of the same degree over a ﬁnite
ﬁeld F. Then E and E′ are isomorphic as F-algebras.
Proof. Let q be of cardinality F, and let ℓbe the degree of the extensions.
As we have argued before, we have E′ = F[α′] for some α′ ∈E′, and so E′ is
isomorphic as an F-algebra to F[X]/(φ), where φ is the minimal polynomial
of α′ over F. As φ is an irreducible polynomial of degree ℓ, by Theorem 20.9,
φ divides Xqℓ−X, and by Theorem 20.6, Xqℓ−X = 
α∈E(X −α), from which
it follows that φ has a root α ∈E. Since φ is irreducible, φ is the minimal
polynomial of α over F, and hence F[α] is isomorphic as an F-algebra to
F[X]/(φ). Since α has degree ℓover F, we must have E = F[α]. 2
Exercise 20.3. This exercise develops an alternative proof for the existence
of ﬁnite ﬁelds—however, it does not yield a density result for irreducible
polynomials. Let F be a ﬁnite ﬁeld of cardinality q, and let ℓ≥1 be an
integer. Let E be a splitting ﬁeld for the polynomial Xqℓ−X ∈F[X] (see
TEAM LinG

456
Finite ﬁelds
Theorem 17.19), and let σ be the Frobenius map on E over F. Let E′ be
the subﬁeld of E ﬁxed by σℓ. Show that E′ is an extension of F of degree ℓ.
Exercise 20.4. Let E be an extension of degree ℓover a ﬁnite ﬁeld F of
cardinality q. Show that at least half the elements of E have degree ℓover
F, and that the total number of elements of degree ℓover F is qℓ+ O(qℓ/2).
20.4 Conjugates, norms and traces
Throughout this section, F denotes a ﬁnite ﬁeld of cardinality q, E denotes
an extension over F of degree ℓ, and σ denotes the Frobenius map on E
over F.
Consider an element α ∈E. We say that β ∈E is conjugate to α (over
F) if β = σi(α) for some i ∈Z. The reader may verify that the “conjugate
to” relation is an equivalence relation. We call the equivalence classes of
this relation conjugacy classes, and we call the elements of the conjugacy
class containing α the conjugates of α.
Starting with α, we can start listing conjugates:
α, σ(α), σ2(α), . . . .
As σℓis the identity map, this list will eventually start repeating.
Let
k be the smallest positive integer such that σk(α) = σi(α) for some i =
0, . . . , k −1.
It must be the case that i = 0 — otherwise, applying σ−1
to the equation σk(α) = σi(α) would yield σk−1(α) = σi−1(α), and since
0 ≤i −1 < k −1, this would contradict the minimality of k.
Thus, α, σ(α), . . . , σk−1(α) are all distinct, and σk(α) = α. Moreover,
for any i ∈Z, we have σi(α) = σj(α), where j = i mod k, and so
α, σ(α), . . . , σk−1(α) are all the conjugates of α. Also, σi(α) = α if and
only if k divides i. Since σℓ(α) = α, it must be the case that k divides ℓ.
With α and k as above, consider the polynomial
φ :=
k−1

i=0
(X −σi(α)).
The coeﬃcients of φ obviously lie in E, but we claim that in fact, they lie
in F.
This is easily seen as follows.
Consider the extension of the map
σ from E to E[X] that applies σ coeﬃcient-wise to polynomials. This was
discussed in Example 9.48, where we saw that the extended map, which we
also denote by σ, is a ring homomorphism from E[X] into E[X]. Applying σ
TEAM LinG

20.4 Conjugates, norms and traces
457
to φ, we obtain
σ(φ) =
k−1

i=0
σ(X −σi(α)) =
k−1

i=0
(X −σi+1(α)) =
k−1

i=0
(X −σi(α)),
since σk(α) = α. Thus we see that σ(φ) = φ. Writing φ = 
i aiXi, we see
that σ(ai) = ai for all i, and hence by Theorem 20.8, ai ∈F for all i. Hence
φ ∈F[X]. We further claim that φ is the minimal polynomial of α. To see
this, let f ∈F[X] be any polynomial over F for which α is a root. Then for
any integer i, by Theorem 17.1, we have
0 = σi(0) = σi(f(α)) = f(σi(α)).
Thus, all the conjugates of α are also roots of f, and so φ divides f. That
proves that φ is the minimal polynomial of α. Since φ is the minimal poly-
nomial of α and deg(φ) = k, it follows that the number k is none other than
the degree of α over F.
Let us summarize the above discussion as follows:
Theorem 20.15. Let α ∈E be of degree k over F, and let φ be the minimal
polynomial of α over F. Then k is the smallest positive integer such that
σk(α) = α, the distinct conjugates of α are α, σ(α), . . . , σk−1(α), and φ
factors over E (in fact, over F[α]) as
φ =
k−1

i=0
(X −σi(α)).
Another useful way of reasoning about conjugates is as follows. First,
if α = 0, then the degree of α over F is 1, and there is nothing more to
say, so let us assume that α ∈E∗. If r is the multiplicative order of α,
then note that any conjugate σi(α) also has multiplicative order r — this
follows from the fact that for any positive integer s, αs = 1 if and only if
(σi(α))s = 1. Also, note that we must have r | |E∗| = qℓ−1, or equivalently,
qℓ≡1 (mod r). Focusing now on the fact that σ is the q-power map, we
see that the degree k of α is the smallest positive integer such that αqk = α,
which holds iﬀαqk−1 = 1, which holds iﬀqk ≡1 (mod r). Thus, the degree
of α over F is simply the multiplicative order of q modulo r. Again, we
summarize these observations as a theorem:
Theorem 20.16. If α ∈E∗has multiplicative order r, then the degree of α
over F is equal to the multiplicative order of q modulo r.
TEAM LinG

458
Finite ﬁelds
Let us deﬁne the polynomial
χ :=
ℓ−1

i=0
(X −σi(α)).
It is easy to see, using the same type of argument as above, that χ ∈F[X],
and indeed, that
χ = φℓ/k.
The polynomial χ is called the characteristic polynomial of α (from E
to F).
Two functions that are often useful are the “norm” and “trace.” The
norm of α (from E to F) is deﬁned as
NE/F (α) :=
ℓ−1

i=0
σi(α),
while the trace of α (from E to F) is deﬁned as
TrE/F (α) :=
ℓ−1

i=0
σi(α).
It is easy to see that both the norm and trace of α are elements of F,
as they are ﬁxed by σ; alternatively, one can see this by observing that
they appear, possibly with a minus sign, as coeﬃcients of the characteristic
polynomial χ—indeed, the constant term of χ is equal to (−1)ℓNE/F (α),
and the coeﬃcient of Xℓ−1 in χ is −TrE/F (α).
The following two theorems summarize the most important facts about
the norm and trace functions.
Theorem 20.17. The function NE/F , restricted to E∗, is a group homo-
morphism from E∗onto F ∗.
Proof. We have
NE/F (α) =
ℓ−1

i=0
αqi = α
Pℓ−1
i=0 qi = α(qℓ−1)/(q−1).
Since E∗is a cyclic group of order qℓ−1, the image of the (qℓ−1)/(q −1)-
power map on E∗is the unique subgroup of E∗of order q −1 (see Theo-
rem 8.31). Since F ∗is a subgroup of E∗of order q −1, it follows that the
image of this power map is F ∗. 2
Theorem 20.18. The function TrE/F is an F-linear map from E onto F.
TEAM LinG

20.4 Conjugates, norms and traces
459
Proof. The fact that TrE/F is an F-linear map is a simple consequence of
the fact that σ is an F-algebra automorphism (verify). As discussed above,
TrE/F maps into F. Since the image of TrE/F is a subspace of F, the image
is either {0} or F, and so it suﬃces to show that TrE/F does not map all of
E to zero. But an element α ∈E is in the kernel of TrE/F if and only if α
is a root of the polynomial
X + Xq + · · · + Xqℓ−1,
which has degree qℓ−1. Since E contains qℓelements, not all elements of E
can lie in the kernel of TrE/F . 2
Example 20.1. As an application of some of the above theory, let us in-
vestigate the factorization of the polynomial Xr −1 over F, a ﬁnite ﬁeld of
cardinality q. Let us assume that r > 0 and is relatively prime to q. Let
E be a splitting ﬁeld of Xr −1 (see Theorem 17.19), so that E is a ﬁnite
extension of F in which Xr −1 splits into linear factors:
Xr −1 =
r

i=1
(X −αi).
We claim that the roots αi of Xr −1 are distinct—this follows from the
Theorem 20.4 and the fact that gcd(Xr −1, rXr−1) = 1.
Next, observe that the r roots of Xr −1 in E actually form a subgroup
of E∗, and since E∗is cyclic, this subgroup must be cyclic as well. So the
roots of Xr −1 form a cyclic subgroup of E∗of order r. Let ζ be a generator
for this group. Then all the roots of Xr −1 are contained in F[ζ], and so we
may as well assume that E = F[ζ].
Let us compute the degree of ζ over F. By Theorem 20.16, the degree ℓ
of ζ over F is the multiplicative order of q modulo r. Moreover, the φ(r)
roots of Xr −1 of multiplicative order r are partitioned into φ(r)/ℓconjugacy
classes, each of size ℓ; indeed, as the reader is urged to verify, these conjugacy
classes are in one-to-one correspondence with the cosets of the subgroup of
Z∗
r generated by [q]r, where each such coset C ⊆Z∗
r corresponds to the
conjugacy class {ζa : [a]r ∈C}.
More generally, for any s | r, any root of Xr −1 whose multiplicative order
is s has degree k over F, where k is the multiplicative order of q modulo
s. As above, the φ(s) roots of multiplicative order s are partitioned into
φ(s)/k conjugacy classes, which are in one-to-one correspondence with the
cosets of the subgroup of Z∗
s generated by [q]s.
This tells us exactly how Xr −1 splits into irreducible factors over F.
Things are a bit simpler when r is prime, in which case, from the above
TEAM LinG

460
Finite ﬁelds
discussion, we see that
Xr −1 = (X −1)
(r−1)/ℓ

i=1
fi,
where each fi is an irreducible polynomial of degree ℓ, and ℓis the multi-
plicative order of q modulo r.
In the above analysis, instead of constructing the ﬁeld E using Theo-
rem 17.19, one could instead simply construct E as F[X]/(φ), where φ is any
irreducible polynomial of degree ℓ, and where ℓis the multiplicative order
of q modulo r. We know that such a polynomial φ exists by Theorem 20.11,
and since E has cardinality qℓ, and r | (qℓ−1) = |E∗|, and E∗is cyclic, we
know that E∗contains an element ζ of multiplicative order r, and each of
the r distinct powers of ζ are roots of Xr −1, and so this E is a splitting
ﬁeld Xr −1 over F. 2
Exercise 20.5. Let E be a ﬁnite extension of a ﬁnite ﬁeld F. Show that
for a ∈F, we have NE/F (a) = aℓand TrE/F (a) = ℓa.
Exercise 20.6. Let E be a ﬁnite extension of a ﬁnite ﬁeld F. Let E′ be an
intermediate ﬁeld, F ⊆E′ ⊆E. Show that
(a) NE/F (α) = NE′/F (NE/E′(α)), and
(b) TrE/F (α) = TrE′/F (TrE/E′(α)).
Exercise 20.7. Let F be a ﬁnite ﬁeld, and let f ∈F[X] be a monic irre-
ducible polynomial of degree ℓ. Let E = F[X]/(f) = F[η], where η := [X]f.
(a) Show that
D(f)
f
=
∞

j=1
TrE/F (ηj−1)X−j.
(b) From part (a), deduce that the sequence
TrE/F (ηj−1)
(j = 1, 2, . . .)
is linearly generated over F with minimal polynomial f.
(c) Show that one can always choose a polynomial f so that sequence in
part (b) is purely periodic with period qℓ−1.
Exercise 20.8. Let F be a ﬁnite ﬁeld, and f ∈F[X] an irreducible polyno-
mial of degree k over F. Let E be an extension of degree ℓover F. Show
that over E, f factors as the product of d distinct irreducible polynomials,
each of degree k/d, where d = gcd(k, ℓ).
TEAM LinG

20.4 Conjugates, norms and traces
461
Exercise 20.9. Let E be a ﬁnite extension of a ﬁnite ﬁeld F of characteristic
p. Show that if α ∈E and 0 ̸= a ∈F, and if α and α + a are conjugate over
F, then p divides the degree of α over F.
Exercise 20.10. Let F be a ﬁnite ﬁeld of characteristic p.
For a ∈F,
consider the polynomial f := Xq −X −a ∈F[X].
(a) Show that if F = Zp and a ̸= 0, then f is irreducible.
(b) More generally, show that if TrF/Zp(a) ̸= 0, then f is irreducible, and
otherwise, f splits into distinct linear factors over F.
Exercise 20.11. Let E be a ﬁnite extension of a ﬁnite ﬁeld F. Let α, β ∈E,
where α has degree a over F, β has degree b over F, and gcd(a, b) = 1. Show
that α + β has degree ab over F.
Exercise 20.12. Let E be a ﬁnite extension of a ﬁnite ﬁeld F. Show that
any F-algebra automorphism on E must be a power of a the Frobenius map
on E over F.
Exercise 20.13. Show that for all primes p, the polynomial X4 + 1 is re-
ducible in Zp[X]. (Contrast this to the fact that this polynomial is irreducible
in Q[X], as discussed in Exercise 17.39.)
Exercise 20.14. This exercise depends on the concepts and results in §19.6.
Let F be a ﬁnite ﬁeld and let E be an extension of degree ℓ. Let σ be the
Frobenius map on E over F.
(a) Show that the minimal polynomial of σ over F is Xℓ−1.
(b) Show that there exists β ∈E such that the minimal polynomial of β
under σ is Xℓ−1.
(c) Conclude that β, σ(β), . . . , σℓ−1(β) is a basis for E over F. This type
of basis is called a normal basis.
TEAM LinG

21
Algorithms for ﬁnite ﬁelds
This chapter discusses eﬃcient algorithms for factoring polynomials over
ﬁnite ﬁelds, and related problems, such as testing if a given polynomial is
irreducible, and generating an irreducible polynomial of given degree.
Throughout this chapter, F denotes a ﬁnite ﬁeld of character-
istic p and cardinality q = pw.
In addition to performing the usual arithmetic and comparison operations
in F, we assume that our algorithms have access to the numbers p, w, and
q, and have the ability to generate random elements of F. Generating such
a random ﬁeld element will count as one “operation in F,” along with the
usual arithmetic operations. Of course, the “standard” ways of representing
F as either Zp (if w = 1), or as the ring of polynomials modulo an irreducible
polynomial over Zp of degree w (if w > 1), satisfy the above requirements,
and also allow for the implementation of arithmetic operations in F that
take time O(len(q)2) on a RAM (using simple, quadratic-time arithmetic
for polynomials and integers).
21.1 Testing and constructing irreducible polynomials
Let f ∈F[X] be a monic polynomial of degree ℓ> 0. We develop here an
eﬃcient algorithm that determines if f is irreducible.
The idea is a simple application of Theorem 20.9. That theorem says that
for any integer k ≥1, the polynomial Xqk −X is the product of all monic
irreducibles whose degree divides k. Thus, gcd(Xq −X, f) is the product of all
the distinct linear factors of f. If f has no linear factors, then gcd(Xq2 −X, f)
is the product of all the distinct quadratic irreducible factors of f. And so
on. Now, if f is not irreducible, it must be divisible by some irreducible
polynomial of degree at most ℓ/2, and if g is an irreducible factor of f
462
TEAM LinG

21.1 Testing and constructing irreducible polynomials
463
of minimal degree, say k, then we have k ≤ℓ/2 and gcd(Xqk −X, f) ̸=
1. Conversely, if f is irreducible, then gcd(Xqk −X, f) = 1 for all positive
integers k up to ℓ/2. So to test if f is irreducible, it suﬃces to check if
gcd(Xqk −X, f) = 1 for all positive integers k up to ℓ/2 — if so, we may
conclude that f is irreducible, and otherwise, we may conclude that f is
not irreducible. To carry out the computation eﬃciently, we note that if
h ≡Xqk (mod f), then gcd(h −X, f) = gcd(Xqk −X, f).
The above observations suggest the following algorithm, which takes as
input a monic polynomial f ∈F[X] of degree ℓ> 0, and outputs true if f is
irreducible, and false otherwise:
Algorithm IPT:
h ←X mod f
for k ←1 to ⌊ℓ/2⌋do
h ←hq mod f
if gcd(h −X, f) ̸= 1 then return false
return true
The correctness of Algorithm IPT follows immediately from the above
discussion. As for the running time, we have:
Theorem 21.1. Algorithm IPT uses O(ℓ3 len(q)) operations in F.
Proof. Consider an execution of a single iteration of the main loop. The cost
of the qth-powering step (using a standard repeated-squaring algorithm) is
O(len(q)) multiplications modulo f, and so O(ℓ2 len(q)) operations in F.
The cost of the gcd computation is O(ℓ2) operations in F. Thus, the cost of
a single loop iteration is O(ℓ2 len(q)) operations in F, from which it follows
that the cost of the entire algorithm is O(ℓ3 len(q)) operations in F. 2
Algorithm IPT is a “polynomial time” algorithm, since the length of the
binary encoding of the input is about ℓlen(q), and so the algorithm runs in
time polynomial in its input length, assuming that arithmetic operations in
F take time polynomial in len(q). Indeed, using a standard representation
for F, each operation in F takes time O(len(q)2) on a RAM, and so the
running time on a RAM for the above algorithm would be O(ℓ3 len(q)3),
that is, cubic in the bit-length of the input.
Let us now consider the related problem of constructing an irreducible
polynomial of speciﬁed degree ℓ> 0. To do this, we can simply use the
result of Theorem 20.11, which has the following probabilistic interpretation:
if we choose a random, monic polynomial f of degree ℓover F, then the
TEAM LinG

464
Algorithms for ﬁnite ﬁelds
probability that f is irreducible is at least 1/2ℓ. This suggests the following
probabilistic algorithm:
Algorithm RIP:
repeat
choose a0, . . . , aℓ−1 ∈F at random
set f ←Xℓ+ ℓ−1
i=0 aiXi
test if f is irreducible using Algorithm IPT
until f is irreducible
output f
Theorem 21.2. Algorithm RIP uses an expected number of O(ℓ4 len(q))
operations in F, and its output is uniformly distributed over all monic irre-
ducibles of degree ℓ.
Proof. Because of Theorem 20.11, the expected number of loop iterations
of the above algorithm is O(ℓ). Since Algorithm IPT uses O(ℓ3 len(q)) op-
erations in F, the statement about the running time of Algorithm RIP is
immediate. The statement about its output distribution is clear. 2
The expected running-time bound in Theorem 21.2 is actually a bit of
an over-estimate. The reason is that if we generate a random polynomial
of degree ℓ, it is likely to have a small irreducible factor, which will be
discovered very quickly by Algorithm IPT. In fact, it is known (see §21.7)
that the expected value of the degree of the least degree irreducible factor
of a random monic polynomial of degree ℓover F is O(len(ℓ)), from which it
follows that the expected number of operations in F performed by Algorithm
RIP is actually O(ℓ3 len(ℓ) len(q)).
Exercise 21.1. Let f ∈F[X] be a monic polynomial of degree ℓ> 0. Also,
let η := [X]f ∈E, where E is the F-algebra E := F[X]/(f).
(a) Show how to compute—given as input α ∈E and ηqm ∈E (for some
integer m > 0)—the value αqm ∈E, using just O(ℓ2.5) operations in
F, and space for O(ℓ1.5) elements of F. Hint: see Theorems 17.1 and
20.7, as well as Exercise 18.4.
(b) Show how to compute—given as input ηqm ∈E and ηqm′
∈E, where
m and m′ are positive integers—the value ηqm+m′
∈E, using O(ℓ2.5)
operations in F, and space for O(ℓ1.5) elements of F.
(c) Show how to compute—given as input ηq ∈E and a positive integer
m — the value ηqm ∈E, using O(ℓ2.5 len(m)) operations in F, and
TEAM LinG

21.2 Computing minimal polynomials in F[X]/(f) (III)
465
space for O(ℓ1.5) elements of F. Hint: use a repeated-squaring-like
algorithm.
Exercise 21.2. This exercise develops an alternative irreducibility test.
(a) Show that a monic polynomial f ∈F[X] of degree ℓ> 0 is irreducible
if and only if Xqℓ≡X (mod f) and gcd(Xqℓ/s −X, f) = 1 for all primes
s | ℓ.
(b) Using part (a) and the result of the previous exercise, show how
to determine if f is irreducible using O(ℓ2.5 len(ℓ)ω(ℓ) + ℓ2 len(q))
operations in F, where ω(ℓ) is the number of distinct prime factors
of ℓ.
(c) Show that the operation count in part (b) can be reduced to
O(ℓ2.5 len(ℓ) len(ω(ℓ)) + ℓ2 len(q)). Hint: see Exercise 3.30.
Exercise 21.3. Design and analyze a deterministic algorithm that takes as
input a list of irreducible polynomials f1, . . . , fr ∈F[X], where ℓi := deg(fi)
for i = 1, . . . , r. Assuming that the degrees ℓ1, . . . , ℓr are pairwise relatively
prime, your algorithm should output an irreducible polynomial f ∈F[X] of
degree ℓ:= r
i=1 ℓi using O(ℓ3) operations in F.
Exercise 21.4. Design and analyze a probabilistic algorithm that, given
a monic irreducible polynomial f ∈F[X] of degree ℓas input, generates
as output a random monic irreducible polynomial g ∈F[X] of degree ℓ
(i.e., g should be uniformly distributed over all such polynomials), using an
expected number of O(ℓ2.5) operations in F. Hint: use Exercise 19.8 (or
alternatively, Exercise 19.9).
Exercise 21.5. Let f ∈F[X] be a monic irreducible polynomial of degree ℓ,
let E := F[X]/(f), and let η := [X]f ∈E. Design and analyze a deterministic
algorithm that takes as input the polynomial f deﬁning the extension E,
and outputs the values
sj := TrE/F (ηj) ∈F
(j = 0, . . . , ℓ−1),
using O(ℓ2) operations in F.
Here, TrE/F is the trace from E to F
(see §20.4). Show that given an arbitrary α ∈E, along with the values
s0, . . . , sℓ−1, one can compute TrE/F (α) using just O(ℓ) operations in F.
21.2 Computing minimal polynomials in F[X]/(f) (III)
We consider, for the third and ﬁnal time, the problem considered in §18.2
and §19.5: f ∈F[X] is a monic polynomial of degree ℓ> 0, and E :=
TEAM LinG

466
Algorithms for ﬁnite ﬁelds
F[X]/(f) = F[η], where η := [X]f; we are given an element α ∈E, and
want to compute the minimal polynomial φ ∈F[X] of α over F. We develop
an alternative algorithm, based on the theory of ﬁnite ﬁelds. Unlike the
algorithms in §18.2 and §19.5, this algorithm only works when F is ﬁnite
and the polynomial f is irreducible, so that E is also a ﬁnite ﬁeld.
From Theorem 20.15, we know that the degree of α over F is the smallest
positive integer k such that αqk = α. By successive qth powering, we can
compute the conjugates of α, and determine the degree k, using O(k len(q))
operations in E, and hence O(kℓ2 len(q)) operations in F.
Now, we could simply compute the minimal polynomial φ by directly
using the formula
φ(Y) =
k−1

i=0
(Y −αqi).
(21.1)
This would involve computations with polynomials in the variable Y whose
coeﬃcients lie in the extension ﬁeld E, although at the end of the compu-
tation, we would end up with a polynomial all of whose coeﬃcients lie in
F. The cost of this approach would be O(k2) operations in E, and hence
O(k2ℓ2) operations in F.
A more eﬃcient approach is the following. Substituting η for Y in the
identity (21.1), we have
φ(η) =
k−1

i=0
(η −αqi).
Using this formula, we can compute (given the conjugates of α) the value
φ(η) ∈E using O(k) operations in E, and hence O(kℓ2) operations in F.
Now, φ(η) is an element of E, and for computational purposes, it is repre-
sented as [g]f for some polynomial g ∈F[X] of degree less than ℓ. Moreover,
φ(η) = [φ]f, and hence φ ≡g (mod f). In particular, if k < ℓ, then g = φ;
otherwise, if k = ℓ, then g = φ −f. In either case, we can recover φ from g
with an additional O(ℓ) operations in F.
Thus, given the conjugates of α, we can compute φ using O(kℓ2) opera-
tions in F. Adding in the cost of computing the conjugates, this gives rise to
an algorithm that computes the minimal polynomial of α using O(kℓ2 len(q))
operations in F.
In the worst case, then, this algorithm uses O(ℓ3 len(q)) operations in
F. A reasonably careful implementation needs space for storing a constant
number of elements of E, and hence O(ℓ) elements of F. For very small
values of q, the eﬃciency of this algorithm will be comparable to that of
TEAM LinG

21.3 Factoring polynomials: the Cantor–Zassenhaus algorithm
467
the algorithm in §19.5, but for large q, it will be much less eﬃcient. Thus,
this approach does not really yield a better algorithm, but it does serve to
illustrate some of the ideas of the theory of ﬁnite ﬁelds.
21.3 Factoring polynomials: the Cantor–Zassenhaus algorithm
In the remaining sections of this chapter, we develop eﬃcient algorithms for
factoring polynomials over the ﬁnite ﬁeld F.
The algorithm we discuss in this section is due to Cantor and Zassenhaus.
It has two stages:
Distinct Degree Factorization: The input polynomial is decomposed
into factors so that each factor is a product of distinct irreducibles
of the same degree (and the degree of those irreducibles is also de-
termined).
Equal Degree Factorization: Each of the factors produced in the dis-
tinct degree factorization stage are further factored into their irre-
ducible factors.
The algorithm we present for distinct degree factorization is a determinis-
tic, polynomial-time algorithm. The algorithm we present for equal degree
factorization is a probabilistic algorithm that runs in expected polynomial
time (and whose output is always correct).
21.3.1 Distinct degree factorization
The problem, more precisely stated, is this: given a monic polynomial f ∈
F[X] of degree ℓ> 0, produce a list of polynomial/integer pairs (g, k), where
• each g is a product of distinct monic irreducible polynomials of degree
k, and
• the product of all the polynomials g in the list is equal to f.
This problem can be easily solved using Theorem 20.9, using a simple
variation of the algorithm we discussed in §21.1 for irreducibility testing.
The basic idea is this. We can compute g := gcd(Xq −X, f), so that g is the
product of all the distinct linear factors of f. We can remove the factor g
from f, but after doing so, f may still contain some linear factors (if the
original polynomial was not square-free), and so we have to repeat the above
step until no linear factors are discovered. Having removed all linear factors
from f, we next compute gcd(Xq2 −X, f), which will be the product of all
the distinct quadratic irreducibles dividing f, and we can remove these from
f —although Xq2 −X is the product of all linear and quadratic irreducibles,
TEAM LinG

468
Algorithms for ﬁnite ﬁelds
since we have already removed the linear factors from f, the gcd will give us
just the quadratic factors of f. As above, we may have to repeat this a few
times to remove all the quadratic factors from f. In general, for k = 1, . . . , ℓ,
having removed all the irreducible factors of degree less than k from f, we
compute gcd(Xqk −X, f) to obtain the product of all the distinct irreducible
factors of f of degree k, repeating as necessary to remove all such factors.
The above discussion leads to the following algorithm for distinct degree
factorization, which takes as input a monic polynomial f ∈F[X] of degree
ℓ> 0:
Algorithm DDF:
h ←X mod f
k ←1
while f ̸= 1 do
h ←hq mod f
g ←gcd(h −X, f)
while g ̸= 1 do
output (g, k)
f ←f/g
h ←h mod f
g ←gcd(h −X, f)
k ←k + 1
The correctness of Algorithm DDF follows from the discussion above. As
for the running time:
Theorem 21.3. Algorithm DDF uses O(ℓ3 len(q)) operations in F.
Proof. Note that the body of the outer loop is executed at most ℓtimes,
since after ℓiterations, we will have removed all the factors of f. Thus,
we perform at most ℓqth-powering steps, each of which takes O(ℓ2 len(q))
operations in F, and so the total contribution to the running time of these is
O(ℓ3 len(q)) operations in F. We also have to take into account the cost of
the gcd computations. We perform one gcd computation in every iteration
of the main loop, for a total of ℓsuch computations.
We also perform
an “extra” gcd computation whenever we discover a non-trivial factor of
f; however, since we only discover at most ℓsuch non-trivial factors, we
perform at most ℓsuch “extra” gcd computations. So the total number of
gcd computations is at most 2ℓ, and as each of these takes O(ℓ2) operations
in F, they contribute a term of O(ℓ3) to the total operation count. This
TEAM LinG

21.3 Factoring polynomials: the Cantor–Zassenhaus algorithm
469
term is dominated by the cost of the qth-powering steps (as is the cost of
the division step in the inner loop), and so the total cost of Algorithm DDF
is O(ℓ3 len(q)) operations in F. 2
21.3.2 Equal degree factorization
The problem, more precisely stated, is this: given a monic polynomial g ∈
F[X] of degree ℓ> 0, and an integer k > 0, such that g is of the form
g = g1 · · · gr
for distinct monic irreducible polynomials g1, . . . , gr, each of degree k, com-
pute these irreducible factors of g. Note that given g and k, the value of r
is easily determined, since r = ℓ/k.
We begin by discussing the basic mathematical ideas that will allow us
to eﬃciently split g into two non-trivial factors, and then we present a
somewhat more elaborate algorithm that completely factors g.
By the Chinese remainder theorem, we have an F-algebra isomorphism
θ : E1 × · · · × Er →E,
where for i = 1, . . . , r, Ei is the extension ﬁeld F[X]/(gi) of degree k over F,
and E is the F-algebra F[X]/(g).
Recall that q = pw. We have to treat the cases p = 2 and p > 2 separately.
We ﬁrst treat the case p = 2. Let us deﬁne the polynomial
Mk :=
wk−1

j=0
X2j ∈F[X].
(21.2)
(The algorithm in the case p > 2 will only diﬀer in the deﬁnition of Mk.)
For α ∈E, if α = θ(α1, . . . , αr), then we have
Mk(α) = θ(Mk(α1), . . . , Mk(αr)).
Note that each Ei is an extension of Z2 of degree wk, and that
Mk(αi) =
wk−1

j=0
α2j
i
= TrEi/Z2(αi),
where TrEi/Z2 : Ei →Z2 is the trace from Ei to Z2, which is a surjective,
Z2-linear map (see §20.4).
Now, suppose we choose α ∈E at random. Then if α = θ(α1, . . . , αr),
the αi will be independently distributed, with each αi uniformly distributed
TEAM LinG

470
Algorithms for ﬁnite ﬁelds
over Ei. It follows that the values Mk(αi) will be independently and uni-
formly distributed over Z2. Thus, if a := rep(Mk(α)) (i.e., a ∈F[X] is the
polynomial of degree less than ℓsuch that Mk(α) = [a]g), then gcd(a, g) will
be the product of those factors gi of g such that Mk(αi) = 0. We will fail
to get a non-trivial factorization only if the Mk(αi) are either all 0 or all 1,
which for r ≥2 happens with probability at most 1/2 (the worst case being
when r = 2).
That is our basic splitting strategy. The algorithm for completely factor-
ing g works as follows. The algorithm proceeds in stages. At any stage, we
have a partial factorization g = 
h∈H h, where H is a set of non-constant,
monic polynomials. Initially, H = {g}. With each stage, we attempt to get
a ﬁner factorization of g by trying to split each h ∈H using the above split-
ting strategy—if we succeed in splitting h into two non-trivial factors, then
we replace h by these two factors. We continue in this way until |H| = r.
Here is the full equal degree factorization algorithm. It takes as input a
monic polynomial g ∈F[X] of degree ℓ> 0, and an integer k > 0, such that
g is the product of r := ℓ/k distinct monic irreducible polynomials, each of
degree k. With Mk as deﬁned in (21.2), the algorithm runs as follows:
Algorithm EDF:
H ←{g}
while |H| < r do
H′ ←∅
for each h ∈H do
choose α ∈F[X]/(h) at random
d ←gcd(rep(Mk(α)), h)
if d = 1 or d = h
then H′ ←H′ ∪{h}
else H′ ←H′ ∪{d, h/d}
H ←H′
output H
The correctness of the algorithm is clear from the above discussion. As
for its expected running time, we can get a quick-and-dirty upper bound as
follows:
• For a given h, the cost of computing Mk(α) for α ∈F[X]/(h) is
O(k deg(h)2 len(q)) operations in F, and so the number of operations
in F performed in each iteration of the main loop is at most a constant
TEAM LinG

21.3 Factoring polynomials: the Cantor–Zassenhaus algorithm
471
times
k len(q)

h∈H
deg(h)2 ≤k len(q)

h∈H
deg(h)
2
= kℓ2 len(q).
• The expected number of iterations of the main loop until we get some
non-trivial split is O(1).
• The algorithm ﬁnishes after getting r −1 non-trivial splits.
• Therefore, the total expected cost is O(rkℓ2 len(q)), or O(ℓ3 len(q)),
operations in F.
This analysis gives a bit of an over-estimate—it does not take into account
the fact that we expect to get fairly “balanced” splits. For the purposes
of analyzing the overall running time of the Cantor–Zassenhaus algorithm,
this bound suﬃces; however, the following analysis gives a tight bound on
the complexity of Algorithm EDF.
Theorem 21.4. In the case p = 2, Algorithm EDF uses an expected number
of O(kℓ2 len(q)) operations in F.
Proof. We may assume r ≥2. Let L be a random variable that denotes the
number of iterations of the main loop of the algorithm.
We claim that E[L] = O(len(r)). To prove this claim, we make use of the
fact (see Theorem 6.25) that
E[L] =

t≥1
P[L ≥t].
For i = 1, . . . , r and j = i+1, . . . , r, deﬁne Lij to be the number of iterations
of the main loop in which the factors gi and gj remain unseparated at the
beginning of the loop. Now, if gi and gj have not been separated at the
beginning of one loop iteration, then they will be separated at the beginning
of the next with probability 1/2. It follows that
P[Lij ≥t] ≤2−(t−1).
Also note that L ≥t implies that Lij ≥t for some i, j, and hence
P[L ≥t] ≤
r

i=1
r

j=i+1
P[Lij ≥t] ≤r22−t.
TEAM LinG

472
Algorithms for ﬁnite ﬁelds
So we have
E[L] =

t≥1
P[L ≥t]
=

t≤2 log2 r
P[L ≥t] +

t>2 log2 r
P[L ≥t]
≤2 log2 r +

t>2 log2 r
r22−t
≤2 log2 r +

t≥0
2−t
= 2 log2 r + 2.
That proves the claim.
As discussed in the paragraph above this theorem, the cost of each it-
eration of the main loop is O(kℓ2 len(q)) operations in F. Combining this
with the fact that E[L] = O(len(r)), it follows that the expected number
of operations in F for the entire algorithm is O(len(r)kℓ2 len(q)). This is
signiﬁcantly better than the above quick-and-dirty estimate, but is not quite
the result we are after—we have to get rid of the factor len(r). There are a
number of ways to do this. We sketch one such way, which is a bit ad hoc,
but suﬃcient for our purposes.
Let us deﬁne
S :=
r

i=1
r

j=i+1
Lij.
We claim that the total work performed by the algorithm in attempting to
split non-irreducible factors of g is
O(Sk3 len(q)).
To see why this is so, consider one iteration of the inner loop of the algorithm,
where we are trying to split a factor h of g, where h is the product of
two or more irreducible factors of g.
Let us write h = gi1 · · · gin, where
2 ≤n ≤r. On the one hand, the number of operations in F performed
in this step is at most ck deg(h)2 len(q) for some constant c, which we may
write as cn2 · k3 len(q). On the other hand, each pair of indices (ij, ij′), with
1 ≤j < j′ ≤n, contributes 1 to the sum deﬁning S, for a total contribution
from pairs at this step of n(n −1)/2 ≥n2/4. The claim now follows.
Algorithm EDF is a little silly in that it wastes time trying to split irre-
ducible factors (and although it would be trivial to modify the algorithm to
avoid this, the asymptotic running time would not be aﬀected signiﬁcantly).
TEAM LinG

21.3 Factoring polynomials: the Cantor–Zassenhaus algorithm
473
It is easy to see that attempting to split a single irreducible factor takes
O(k3 len(q)) operations in F, and hence the total amount of work wasted in
this way is O(Lrk3 len(q)).
We next claim that E[Lij] = O(1), for all i, j. Indeed,
E[Lij] =

t≥1
P[Lij ≥t] ≤

t≥1
2−(t−1) = 2.
It follows that
E[S] =

ij
E[Lij] = O(r2).
Therefore, the expected number of operations in F performed by the algo-
rithm is at most a constant times
E[S]k3 len(q) + E[L]rk3 len(q) = O(r2k3 len(q) + r len(r)k3 len(q)),
which is O(kℓ2 len(q)). 2
That completes the discussion of Algorithm EDF in the case p = 2.
The case p > 2
Now assume that p > 2, so that p, and hence also q, is odd. Algorithm EDF
in this case is exactly the same as above, except that in this case, we deﬁne
the polynomial Mk as
Mk := X(qk−1)/2 −1 ∈F[X].
(21.3)
Just as before, for α ∈E with α = θ(α1, . . . , αr), we have
Mk(α) = θ(Mk(α1), . . . , Mk(αr)).
Note that each group E∗
i is a cyclic group of order qk −1, and therefore, the
image of the (qk −1)/2-power map on E∗
i is {±1}.
Now, suppose we choose α ∈E at random. Then if α = θ(α1, . . . , αr), the
αi will be independently distributed, with each αi uniformly distributed over
Ei. It follows that the values Mk(αi) will be independently distributed. If
αi = 0, which happens with probability 1/qk, then Mk(αi) = −1; otherwise,
α(qk−1)/2
i
is uniformly distributed over {±1}, and so Mk(αi) is uniformly
distributed over {0, −2}. That is to say,
Mk(αi) =



0
with probability (qk −1)/2qk,
−1
with probability 1/qk,
−2
with probability (qk −1)/2qk.
Thus, if a := rep(Mk(α)), then gcd(a, g) will be the product of those factors
TEAM LinG

474
Algorithms for ﬁnite ﬁelds
gi of g such that Mk(αi) = 0. We will fail to get a non-trivial factorization
only if the Mk(αi) are either all zero or all non-zero. Assume r ≥2. Consider
the worst case, namely, when r = 2. In this case, a simple calculation shows
that the probability that we fail to split these two factors is
qk −1
2qk
2
+
qk + 1
2qk
2
= 1
2(1 + 1/q2k).
The (very) worst case is when qk = 3, in which case the probability of failure
is at most 5/9.
The same quick-and-dirty analysis given just above Theorem 21.4 applies
here as well, but just as before, we can do better:
Theorem 21.5. In the case p > 2, Algorithm EDF uses an expected number
of O(kℓ2 len(q)) operations in F.
Proof. The analysis is essentially the same as in the case p = 2, except that
now the probability that we fail to split a given pair of irreducible factors
is at most 5/9, rather than equal to 1/2. The details are left as an exercise
for the reader. 2
21.3.3 Analysis of the whole algorithm
Given an arbitrary polynomial f ∈F[X] of degree ℓ> 0, the distinct degree
factorization step takes O(ℓ3 len(q)) operations in F. This step produces
a number of polynomials that must be further subjected to equal degree
factorization. If there are s such polynomials, where the ith polynomial has
degree ℓi, for i = 1, . . . , s, then s
i=1 ℓi = ℓ. Now, the equal degree factor-
ization step for the ith polynomial takes an expected number of O(ℓ3
i len(q))
operations in F (actually, our initial, “quick and dirty” estimate is good
enough here), and so it follows that the total expected cost of all the equal
degree factorization steps is O(
i ℓ3
i len(q)), which is O(ℓ3 len(q)), opera-
tions in F. Putting this all together, we conclude:
Theorem 21.6. The Cantor–Zassenhaus factoring algorithm uses an ex-
pected number of O(ℓ3 len(q)) operations in F.
This bound is tight, since in the worst case, when the input is irreducible,
the algorithm really does do this much work.
Exercise 21.6. Show how to modify Algorithm DDF so that the main loop
halts as soon as 2k > deg(f).
TEAM LinG

21.4 Factoring polynomials: Berlekamp’s algorithm
475
Exercise 21.7. This exercise extends the techniques developed in Exer-
cise 21.1. Let f ∈F[X] be a monic polynomial of degree ℓ> 0, and let
η := [X]f ∈E, where E := F[X]/(f). For integer m > 0, deﬁne polynomials
Tm := X + Xq + · · · + Xqm−1 ∈F[X] and Nm := X · Xq · · · · · Xqm−1 ∈F[X].
(a) Show how to compute—given as input ηqm ∈E and ηqm′
, where m
and m′ are positive integers, along with Tm(α) and Tm′(α), for some
α ∈E —the values ηqm+m′
and Tm+m′(α), using O(ℓ2.5) operations
in F, and space for O(ℓ1.5) elements of F.
(b) Using part (a), show how to compute—given as input ηq ∈E, α ∈
E, and a positive integer m—the value Tm(α), using O(ℓ2.5 len(m))
operations in F, and space for O(ℓ1.5) elements of F.
(c) Repeat parts (a) and (b), except with “N” in place of “T.”
Exercise 21.8. Using the result of the previous exercise, show how to im-
plement Algorithm EDF so that it uses an expected number of
O(len(k)ℓ2.5 + ℓ2 len(q))
operations in F, and space for O(ℓ1.5) elements of F.
Exercise 21.9. This exercise depends on the concepts and results in §19.6.
Let E be an extension ﬁeld of degree ℓover F, speciﬁed by an irreducible
polynomial of degree ℓover F. Design and analyze an eﬃcient probabilis-
tic algorithm that ﬁnds a normal basis for E over F (see Exercise 20.14).
Hint: there are a number of approaches to solving this problem; one way
is to start by factoring Xℓ−1 over F, and then turn the construction in
Theorem 19.7 into an eﬃcient probabilistic procedure; if you mimic Ex-
ercise 11.2, your entire algorithm should use O(ℓ3 len(ℓ) len(q)) operations
in F (or O(len(r)ℓ3 len(q)) operations, where r is the number of distinct
irreducible factors of Xℓ−1 over F).
21.4 Factoring polynomials: Berlekamp’s algorithm
We now develop an alternative algorithm, due to Berlekamp, for factoring
a polynomial over the ﬁnite ﬁeld F.
This algorithm usually starts with a pre-processing phase to reduce the
problem to that of factoring square-free polynomials. There are a number
of ways to carry out this step. We present a simple-minded method here
that is suﬃcient for our purposes.
TEAM LinG

476
Algorithms for ﬁnite ﬁelds
21.4.1 A simple square-free decomposition algorithm
Let f ∈F[X] be a monic polynomial of degree ℓ> 0. Suppose that f is
not square-free. According to Theorem 20.4, d := gcd(f, D(f)) ̸= 1, and
so we might hope to get a non-trivial factorization of f by computing d;
however, we have to consider the possibility that d = f. Can this happen?
The answer is “yes,” but if it does happen that d = f, we can still get a
non-trivial factorization of f by other means:
Theorem 21.7. Suppose that f ∈F[X] is a polynomial of degree ℓ> 0, and
that gcd(f, D(f)) = f. Then f = g(Xp) for some g ∈F[X]. Moreover, if
g = 
i biXi, then f = hp, where h = 
i bp(w−1)
i
Xi.
Proof. Since deg(D(f)) < deg(f), if gcd(f, D(f)) = f, then we must have
D(f) = 0. If f = ℓ
i=0 aiXi, then D(f) = ℓ
i=1 iaiXi−1. Since this deriva-
tive must be zero, it follows that all the coeﬃcients ai with i ̸≡0 (mod p)
must be zero to begin with. That proves that f = g(Xp) for some g ∈F[X].
Furthermore, if h is deﬁned as above, then
hp =

i
bp(w−1)
i
Xi
p
=

i
bpw
i Xip =

i
bi(Xp)i = g(Xp) = f. 2
This suggests the following recursive algorithm. The input is the polyno-
mial f as above, and a parameter s, which is set to 1 on the initial invoca-
tion. The output is a list of pairs (gi, si) such that each gi is a square-free,
non-constant polynomial over F and f = 
i gsi
i .
Algorithm SFD:
d ←gcd(f, D(f))
if d = 1 then
output (f, s)
else if d ̸= f then
recursively process (d, s) and (f/d, s)
else
let f = Xℓ+ ℓ−1
i=0 aiXi
// note that ai = 0 except when p | i
set h ←Xℓ/p + ℓ/p−1
i=0
(api)pw−1Xi
// note that h = f1/p
recursively process (h, ps)
The correctness of Algorithm SFD follows from the discussion above. As
for its running time:
Theorem 21.8. Algorithm SFD uses O(ℓ3 + ℓ(w −1) len(p)/p) operations
in F.
TEAM LinG

21.4 Factoring polynomials: Berlekamp’s algorithm
477
Proof. For input polynomial f with deg(f) > 0, let R(f) denote the number
of recursive invocations of the algorithm, and let P(f) denote the number
of pw−1th powers in F computed by the algorithm. It is easy to see that the
number of operations in F performed by the algorithm is
O(R(f) deg(f)2 + P(f)(w −1) len(p)).
The theorem will therefore follow from the following two inequalities:
R(f) ≤2 deg(f) −1
(21.4)
and
P(f) ≤2 deg(f)/p.
(21.5)
We prove (21.4) by induction of deg(f). We assume (21.4) holds for all
input polynomials of degree less than that of f, and prove that it holds for
f. Let d := gcd(f, D(f)). If d = 1, then R(f) = 1 ≤2 deg(f) −1. If d ̸= 1
and d ̸= f, then applying the induction hypothesis, we have
R(f) = 1 + R(d) + R(f/d) ≤1 + (2 deg(d) −1) + (2 deg(f/d) −1)
= 2 deg(f) −1.
Finally, if d = f, then again applying the induction hypothesis, we have
R(f) = 1 + R(f 1/p) ≤1 + (2 deg(f)/p −1) ≤deg(f) ≤2 deg(f) −1.
The inequality (21.5) is proved similarly by induction. We assume (21.5)
holds for all input polynomials of degree less than that of f, and prove that
it holds for f. Let d := gcd(f, D(f)). If d = 1, then P(f) = 0 ≤2 deg(f)/p.
If d ̸= 1 and d ̸= f, then applying the induction hypothesis, we have
P(f) = P(d) + P(f/d) ≤2 deg(d)/p + 2 deg(f/d)/p = 2 deg(f)/p.
Finally, if d = f, then again applying the induction hypothesis, we have
P(f) = deg(f)/p + P(f1/p) ≤deg(f)/p + 2 deg(f)/p2 ≤2 deg(f)/p. 2
The running-time bound in Theorem 21.8 is essentially tight (see Exer-
cise 21.10 below). Although it suﬃces for our immediate purpose as a pre-
processing step in Berlekamp’s factoring algorithm, Algorithm SFD is by no
means the most eﬃcient algorithm possible for square-free decomposition of
polynomials. We return to this issue below, in §21.6.
TEAM LinG

478
Algorithms for ﬁnite ﬁelds
21.4.2 The main factoring algorithm
Let us now assume we have a monic square-free polynomial f of degree ℓ> 0
that we want to factor into irreducibles, such as is output by the square-free
decomposition algorithm above. We ﬁrst present the mathematical ideas
underpinning the algorithm.
Let E be the F-algebra F[X]/(f). We deﬁne a subset B of E as follows:
B := {α ∈E : αq = α}.
It is easy to see that B is a subalgebra of E. Indeed, for α, β ∈B, we have
(α+β)q = αq +βq = α+β, and similarly, (αβ)q = αqβq = αβ. Furthermore,
one sees that cq = c for all c ∈F, and hence B is a subalgebra.
The subalgebra B is called the Berlekamp subalgebra of E. Let us
take a closer look at it. Suppose that f factors into irreducibles as
f = f1 · · · fr,
and let
θ : E1 × · · · × Er →E
be the F-algebra isomorphism from the Chinese remainder theorem, where
Ei := F[X]/(fi) is an extension ﬁeld of F of ﬁnite degree for i = 1, . . . , r.
Now, for α = θ(α1, . . . , αr) ∈E, we have αq = α if and only if αq
i = αi for
i = 1, . . . , r; moreover, by Theorem 20.8, we know that for any αi ∈Ei, we
have αq
i = αi if and only if αi ∈F. Thus, we may characterize B as follows:
B = {θ(c1, . . . , cr) : c1, . . . , cr ∈F}.
Since B is a subalgebra of E, then as F-vector spaces, B is a subspace of
E. Of course, E has dimension ℓover F, with the natural basis 1, η, . . . , ηℓ−1,
where η := [X]f. As for the Berlekamp subalgebra, from the above charac-
terization of B, it is evident that
θ(1, 0, . . . , 0), θ(0, 1, 0, . . . , 0), . . . , θ(0, . . . , 0, 1)
is a basis for B over F, and hence, B has dimension r over F.
Now we come to the actual factoring algorithm.
Stage 1: Construct a basis for B
The ﬁrst stage of Berlekamp’s factoring algorithm constructs a basis for B
over F. We can easily do this using Gaussian elimination, as follows. Let
ρ : E →E be the map that sends α ∈E to αq −α. Since the qth power map
on E is an F-algebra homomorphism (see Theorem 20.7)—and in particular,
an F-linear map—the map ρ is also F-linear. Moreover, the kernel of ρ is
TEAM LinG

21.4 Factoring polynomials: Berlekamp’s algorithm
479
none other than the Berlekamp subalgebra B. So to ﬁnd a basis for B, we
simply need to ﬁnd a basis for the kernel of ρ using Gaussian elimination
over F, as in §15.4.
To perform the Gaussian elimination, we need to choose an ordered basis
for E over F, and construct a matrix Q ∈F ℓ×ℓthat represents ρ with
respect to that ordered basis as in §15.2, so that evaluation of ρ corresponds
to multiplying a row vector on the right by Q.
We are free to choose
an ordered basis in any convenient way, and the most convenient ordered
basis, of course, is (1, η, . . . , ηℓ−1), as this directly corresponds to the way
we represent elements of E for computational purposes. Let us deﬁne the
F-vector space isomorphism
ϵ :
F 1×ℓ→E
(a0, . . . , aℓ−1) →a0 + a1η + · · · + aℓ−1ηℓ−1.
(21.6)
The maps ϵ and ϵ−1 are best thought of as “type conversion operators”
that require no actual computation to evaluate. The matrix Q, then, is the
ℓ× ℓmatrix whose ith row, for i = 1, . . . , ℓ, is ϵ−1(ρ(ηi−1)). Note that if
α := ηq, then ρ(ηi−1) = (ηi−1)q −ηi−1 = (ηq)i−1 −ηi−1 = αi−1 −ηi−1. This
observation allows us to construct the rows of Q by ﬁrst computing α as ηq
via repeated squaring, and then just computing successive powers of α.
After we construct the matrix Q, we apply Gaussian elimination to get
row vectors v1, . . . , vr that form a basis for the row null space of Q. It is at
this point that our algorithm actually discovers the number r of irreducible
factors of f. We can then set βi := ϵ(vi) for i = 1, . . . , r to get our basis for B.
Putting this altogether, we have the following algorithm to compute a
basis for the Berlekamp subalgebra. It takes as input a monic square-free
polynomial f of degree ℓ> 0. With E := F[X]/(f), η := [X]f ∈E, and ϵ as
deﬁned in (21.6), the algorithm runs as follows:
Algorithm B1:
let Q be an ℓ× ℓmatrix over F (initially with undeﬁned entries)
compute α ←ηq using repeated squaring
β ←1E
for i ←1 to ℓdo
// invariant: β = αi−1 = (ηi−1)q
Q(i) ←ϵ−1(β), Q(i, i) ←Q(i, i) −1, β ←βα
compute a basis v1, . . . , vr of the row null space of Q using
Gaussian elimination
for i = 1, . . . , r do βi ←ϵ(vi)
output β1, . . . , βr
TEAM LinG

480
Algorithms for ﬁnite ﬁelds
The correctness of Algorithm B1 is clear from the above discussion. As
for the running time:
Theorem 21.9. Algorithm B1 uses O(ℓ3 + ℓ2 len(q)) operations in F.
Proof. This is just a matter of counting.
The computation of α takes
O(len(q)) operations in E using repeated squaring, and hence O(ℓ2 len(q))
operations in F. To build the matrix Q, we have to perform an additional
O(ℓ) operations in E to compute the successive powers of α, which trans-
lates into O(ℓ3) operations in F. Finally, the cost of Gaussian elimination
is an additional O(ℓ3) operations in F. 2
Stage 2: Splitting with B
The second stage of Berlekamp’s factoring algorithm is a probabilistic proce-
dure that factors f using a basis β1, . . . , βr for B. As we did with Algorithm
EDF in §21.3.2, we begin by discussing how to eﬃciently split f into two
non-trivial factors, and then we present a somewhat more elaborate algo-
rithm that completely factors f.
Let M1 ∈F[X] be the polynomial deﬁned by (21.2) and (21.3); that is,
M1 :=
& w−1
j=0 X2j
if p = 2,
X(q−1)/2 −1
if p > 2.
Using our basis for B, we can easily generate a random element β of B
by simply choosing c1, . . . , cr at random, and computing β := 
i ciβi. If
β = θ(b1, . . . , br), then the bi will be uniformly and independently distributed
over F. Just as in Algorithm EDF, gcd(rep(M1(β)), f) will be a non-trivial
factor of f with probability at least 1/2, if p = 2, and probability at least
4/9, if p > 2.
That is the basic splitting strategy. We turn this into an algorithm to
completely factor f using the same technique of iterative reﬁnement that
was used in Algorithm EDF. That is, at any stage of the algorithm, we have
a partial factorization f = 
h∈H h, which we try to reﬁne by attempting
to split each h ∈H using the strategy outlined above. One technical dif-
ﬁculty is that to split such a polynomial h, we need to eﬃciently generate
a random element of the Berlekamp subalgebra of F[X]/(h). A particularly
eﬃcient way to do this is to use our basis for the Berlekamp subalgebra
of F[X]/(f) to generate a random element of the Berlekamp subalgebra of
F[X]/(h) for all h ∈H simultaneously. Let gi := rep(βi) for i = 1, . . . , r.
If we choose c1, . . . , cr ∈F at random, and set g := c1g1 + · · · + crgr, then
[g]f is a random element of the Berlekamp subalgebra of F[X]/(f), and by
TEAM LinG

21.4 Factoring polynomials: Berlekamp’s algorithm
481
the Chinese remainder theorem, it follows that the values [g]h for h ∈H
are independently distributed, with each [g]h uniformly distributed over the
Berlekamp subalgebra of F[X]/(h).
Here is the algorithm for completely factoring a polynomial, given a basis
for the corresponding Berlekamp subalgebra.
It takes as input a monic,
square-free polynomial f of degree ℓ> 0, together with a basis β1, . . . , βr for
the Berlekamp subalgebra of F[X]/(f). With gi := rep(βi) for i = 1, . . . , r,
the algorithm runs as follows:
Algorithm B2:
H ←{f}
while |H| < r do
choose c1, . . . , cr ∈F at random
g ←c1g1 + · · · + crgr ∈F[X]
H′ ←∅
for each h ∈H do
β ←[g]h ∈F[X]/(h)
d ←gcd(rep(M1(β)), h)
if d = 1 or d = h
then H′ ←H′ ∪{h}
else H′ ←H′ ∪{d, h/d}
H ←H′
output H
The correctness of the algorithm is clear. As for its expected running
time, we can get a quick-and-dirty upper bound as follows:
• The cost of generating g in each loop iteration is O(rℓ) operations
in F. For a given h, the cost of computing β := [g]h ∈F[X]/(h)
is O(ℓdeg(h)) operations in F, and the cost of computing M1(β) is
O(deg(h)2 len(q)) operations in F. Therefore, the number of opera-
tions in F performed in each iteration of the main loop is at most a
constant times
rℓ+ ℓ

h∈H
deg(h) + len(q)

h∈H
deg(h)2
≤2ℓ2 + len(q)

h∈H
deg(h)
2
= O(ℓ2 len(q)).
• The expected number of iterations of the main loop until we get some
non-trivial split is O(1).
TEAM LinG

482
Algorithms for ﬁnite ﬁelds
• The algorithm ﬁnishes after getting r −1 non-trivial splits.
• Therefore, the total expected cost is O(rℓ2 len(q)) operations in F.
A more careful analysis reveals:
Theorem 21.10. Algorithm B2 uses an expected number of
O(len(r)ℓ2 len(q))
operations in F.
Proof. The proof follows the same line of reasoning as the analysis of Al-
gorithm EDF. Indeed, using the same argument as was used there, the
expected number of iterations of the main loop is O(len(r)). As discussed in
the paragraph above this theorem, the cost per loop iteration is O(ℓ2 len(q))
operations in F. The theorem follows. 2
The bound in the above theorem is tight (see Exercise 21.11 below): unlike
Algorithm EDF, we cannot make the multiplicative factor of len(r) go away.
21.4.3 Analysis of the whole algorithm
Putting together Algorithm SFD with algorithms B1 and B2, we get
Berlekamp’s complete factoring algorithm. The running time bound is easily
estimated from the results already proved:
Theorem 21.11. Berlekamp’s factoring algorithm uses an expected number
of O(ℓ3 + ℓ2 len(ℓ) len(q)) operations in F.
So we see that Berlekamp’s algorithm is in fact faster than the Cantor–
Zassenhaus algorithm, whose expected operation count is O(ℓ3 len(q)). The
speed advantage of Berlekamp’s algorithm grows as q gets large. The one
disadvantage of Berlekamp’s algorithm is space: it requires space for Θ(ℓ2)
elements of F, while the Cantor–Zassenhaus algorithm requires space for
only O(ℓ) elements of F. One can in fact implement the Cantor–Zassenhaus
algorithm so that it uses O(ℓ3 +ℓ2 len(q)) operations in F, while using space
for only O(ℓ1.5) elements of F —see Exercise 21.13 below.
Exercise 21.10. Give an example of a family of input polynomials f that
cause Algorithm SFD to use at least Ω(ℓ3) operations in F, where ℓ:=
deg(f).
Exercise 21.11. Give an example of a family of input polynomials f that
cause Algorithm B2 to use an expected number of at least Ω(ℓ2 len(ℓ) len(q))
operations in F, where ℓ:= deg(f).
TEAM LinG

21.5 Deterministic factorization algorithms (∗)
483
Exercise 21.12. Using the ideas behind Berlekamp’s factoring algorithm,
devise a deterministic irreducibility test that, given a monic polynomial of
degree ℓover F, uses O(ℓ3 + ℓ2 len(q)) operations in F.
Exercise 21.13. This exercise develops a variant of the Cantor–Zassenhaus
algorithm that uses O(ℓ3 + ℓ2 len(q)) operations in F, while using space for
only O(ℓ1.5) elements of F. By making use of Algorithm SFD (which with
a bit of care can be implemented so as to use space for O(ℓ) elements of F)
and the variant of Algorithm EDF discussed in Exercise 21.8, our problem
is reduced to that of implementing Algorithm DDF within the stated time
and space bounds, assuming that the input polynomial is square-free.
(a) For non-negative integers i, j, with i ̸= j, show that the irreducible
polynomials in F[X] that divide Xqi −Xqj are precisely those whose
degree divides i −j.
(b) Let f ∈F[X] be a monic polynomial of degree ℓ> 0, and let m ≈ℓ1/2.
Let η := [X]f ∈E, where E := F[X]/(f). Show how to compute
ηq, ηq2, . . . , ηqm−1 ∈E and ηqm, ηq2m, . . . , ηq(m−1)m ∈E
using O(ℓ3+ℓ2 len(q)) operations in F, and space for O(ℓ1.5) elements
of F.
(c) Combine the results of parts (a) and (b) to implement Algorithm
DDF on square-free inputs of degree ℓ, so that it uses O(ℓ3+ℓ2 len(q))
operations in F, and space for O(ℓ1.5) elements of F.
21.5 Deterministic factorization algorithms (∗)
The algorithms of Cantor and Zassenhaus and of Berlekamp are probabilis-
tic.
The exercises below develop a deterministic variant of the Cantor–
Zassenhaus algorithm.
(One can also develop deterministic variants of
Berlekamp’s algorithm, with similar complexity.)
This algorithm is only practical for ﬁnite ﬁelds of small characteristic, and
is anyway mainly of theoretical interest, since from a practical perspective,
there is nothing wrong with the above probabilistic method. In all of these
exercises, we assume that we have access to a basis ϵ1, . . . , ϵw for F as a
vector space over Zp.
To make the Cantor–Zassenhaus algorithm deterministic, we only need
to develop a deterministic variant of Algorithm EDF, as Algorithm DDF is
already deterministic.
TEAM LinG

484
Algorithms for ﬁnite ﬁelds
Exercise 21.14. Let g = g1 · · · gr, where the gi are distinct monic irre-
ducible polynomials in F[X]. Assume that r > 1, and let ℓ:= deg(g). For
this exercise, the degrees of the gi need not be the same. For an intermediate
ﬁeld F ′, with Zp ⊆F ′ ⊆F, let us call a set S = {λ1, . . . , λs} of polynomials
in F[X]<ℓa separating set for g over F ′ if the following conditions hold:
• for i = 1, . . . , r and u = 1, . . . , s, there exists cui ∈F ′ such that
λu ≡cui (mod gi), and
• for any distinct pair of indices i, j, with 1 ≤i < j ≤r, there exists
u = 1, . . . , s such that cui ̸= cuj.
Show that if S is a separating set for g over Zp, then the following algorithm
completely factors g using O(p|S|ℓ2) operations in F.
H ←{g}
for each λ ∈S do
for each a ∈Zp do
H′ ←∅
for each h ∈H do
d ←gcd(λ −a, h)
if d = 1 or d = h
then H′ ←H′ ∪{h}
else H′ ←H′ ∪{d, h/d}
H ←H′
output H
Exercise 21.15. Let g be as in the previous exercise. Show that if S is a
separating set for g over F, then the set
S′ := {
w−1

i=0
(ϵjλ)pi mod g : 1 ≤j ≤w, λ ∈S}
is a separating set for g over Zp.
Show how to compute this set using
O(|S|ℓ2 len(p)w(w −1)) operations in F.
Exercise 21.16. Let g be as in the previous two exercises, but further
suppose that each irreducible factor of g is of the same degree, say k. Let
E := F[X]/(g) and η := [X]g ∈E. Deﬁne the polynomial φ ∈E[Y] as follows:
φ :=
k−1

i=0
(Y −ηqi).
If
φ = Yk + αk−1Yk−1 + · · · + α0,
TEAM LinG

21.6 Faster square-free decomposition (∗)
485
with α0, . . . , αk−1 ∈E, show that the set
S := {rep(αi) : 0 ≤i ≤k −1}
is a separating set for g over F, and can be computed deterministically using
O(k2 + k len(q)) operations in E, and hence O(k2ℓ2 + kℓ2 len(q)) operations
in F.
Exercise 21.17. Put together all of the above pieces, together with Algo-
rithm DDF, so as to obtain a deterministic algorithm for factoring polyno-
mials over F that runs in time at most p times a polynomial in the input
length, and make a careful estimate of the running time of your algorithm.
Exercise 21.18. It is a fact that when our prime p is odd, then for all
integers a, b, with a ̸≡b (mod p), there exists a non-negative integer
i ≤p1/2 log2 p such that (a + i | p) ̸= (b + i | p) (here, “(· | ·)” is the
Legendre symbol). Using this fact, design and analyze a deterministic algo-
rithm for factoring polynomials over F that runs in time at most p1/2 times
a polynomial in the input length.
The following two exercises show that the problem of factoring polyno-
mials over F reduces in deterministic polynomial time to the problem of
ﬁnding roots of polynomials over Zp.
Exercise 21.19. Let g be as in Exercise 21.14.
Suppose that S =
{λ1, . . . , λs} is a separating set for g over Zp, and φu ∈F[X] is the min-
imal polynomial over F of [λu]g ∈F[X]/(g) for u = 1, . . . , s. Show that each
φu is the product of linear factors over Zp, and that given S along with
the roots of all the φu, we can deterministically factor g using (|S| + ℓ)O(1)
operations in F. Hint: see Exercise 17.9.
Exercise 21.20. Using the previous exercise, show that the problem of fac-
toring a polynomial over a ﬁnite ﬁeld F reduces in deterministic polynomial
time to the problem of ﬁnding roots of polynomials over the prime ﬁeld of F.
21.6 Faster square-free decomposition (∗)
The algorithm presented in §21.4.1 for square-free decomposition was simple
and suitable for our immediate purposes, but is certainly not the most eﬃ-
cient algorithm possible. The following exercises develop a faster algorithm
for this problem.
We begin with an exercise that more fully develops the connection be-
TEAM LinG

486
Algorithms for ﬁnite ﬁelds
tween square-free polynomials and formal derivatives for polynomials over
arbitrary ﬁelds:
Exercise 21.21. Let K be a ﬁeld, and let f ∈K[X] with deg(f) > 0.
(a) Show that if D(f) = 0, then the characteristic of K must be a prime
p, and f must be of the form f = g(Xp) for some g ∈K[X].
(b) Show that if K is a ﬁnite ﬁeld or a ﬁeld of characteristic zero, then
f is square-free if and only if d := gcd(f, D(f)) = 1; moreover, if
d ̸= 1, then either deg(d) < deg(f), or K has prime characteristic p
and f = hp for some h ∈K[X].
(c) Give an example of a ﬁeld K of characteristic p and an irreducible
polynomial f ∈K[X] such that f = g(Xp) for some g ∈K[X].
Next, we consider the problem of square-free decomposition of polynomi-
als over ﬁelds of characteristic zero, which is simpler than the corresponding
problem over ﬁnite ﬁelds.
Exercise 21.22. Let f ∈K[X] be a monic polynomial over a ﬁeld K of
characteristic zero. Suppose that the factorization of f into irreducibles is
f = fe1
1 · · · fer
r .
Show that
f
gcd(f, D(f)) = f1 · · · fr.
Exercise 21.23. Let K be a ﬁeld of characteristic zero. Consider the fol-
lowing algorithm that takes as input a monic polynomial f ∈K[X] of degree
ℓ> 0:
j ←1, g ←f/ gcd(f, D(f))
repeat
f ←f/g, h ←gcd(f, g), m ←g/h
if m ̸= 1 then output (m, j)
g ←h, j ←j + 1
until g = 1
Using the result of the previous exercise, show that this algorithm outputs
a list of pairs (gi, si), such that each gi is square-free, f = 
i gsi
i , and the gi
are pairwise relatively prime. Furthermore, show that this algorithm uses
O(ℓ2) operations in K.
We now turn our attention to square-free decomposition over ﬁnite ﬁelds.
TEAM LinG

21.7 Notes
487
Exercise 21.24. Let f ∈F[X] be a monic polynomial over F (which, as
usual, has characteristic p and cardinality q = pw). Suppose that the fac-
torization of f into irreducibles is
f = fe1
1 · · · fer
r .
Show that
f
gcd(f, D(f)) =

1≤i≤r
ei̸≡0 (mod p)
fi.
Exercise 21.25. Consider the following algorithm that takes as input a
monic polynomial f ∈F[X] of degree ℓ> 0:
s ←1
repeat
j ←1, g ←f/ gcd(f, D(f))
repeat
f ←f/g, h ←gcd(f, g), m ←g/h
if m ̸= 1 then output (m, js)
g ←h, j ←j + 1
until g = 1
if f ̸= 1 then // f is a pth power
// we compute a pth root as in Algorithm SFD
f ←f1/p, s ←ps
until f = 1
Using the result of the previous exercise, show that this algorithm outputs
a list of pairs (gi, si), such that each gi is square-free, f = 
i gsi
i , and the gi
are pairwise relatively prime. Furthermore, show that this algorithm uses
O(ℓ2 + ℓ(w −1) len(p)/p) operations in F.
21.7 Notes
The average-case analysis of Algorithm IPT, assuming its input is random,
and the application to the analysis of Algorithm RIP, is essentially due to
Ben-Or [14]. If one implements Algorithm RIP using fast polynomial arith-
metic, one gets an expected cost of O(ℓ2+o(1) len(q)) operations in F. Note
that Ben-Or’s analysis is a bit incomplete—see Exercise 32 in Chapter 7 of
Bach and Shallit [12] for a complete analysis of Ben-Or’s claims.
The asymptotically fastest probabilistic algorithm for constructing an ir-
reducible polynomial over F of degree ℓis due to Shoup [91]. That algorithm
uses an expected number of O(ℓ2+o(1) + ℓ1+o(1) len(q)) operations in F, and
TEAM LinG

488
Algorithms for ﬁnite ﬁelds
in fact does not follow the “generate and test” paradigm of Algorithm RIP,
but uses a completely diﬀerent approach. Exercise 21.2 is based on [91].
As far as deterministic algorithms for constructing irreducible polynomials
of given degree over F, the only known methods are eﬃcient when the
characteristic p of F is small (see Chistov [26], Semaev [83], and Shoup
[89]), or under a generalization of the Riemann hypothesis (see Adleman and
Lenstra [4]). Shoup [89] in fact shows that the problem of constructing an
irreducible polynomial of given degree over F is deterministic, polynomial-
time reducible to the problem of factoring polynomials over F.
The algorithm in §21.2 for computing minimal polynomials over ﬁnite
ﬁelds is due to Gordon [41].
The Cantor–Zassenhaus algorithm was initially developed by Cantor and
Zassenhaus [24], although many of the basic ideas can be traced back quite
a ways. A straightforward implementation of this algorithm using fast poly-
nomial arithmetic uses an expected number of O(ℓ2+o(1) len(q)) operations
in F.
Berlekamp’s algorithm was initially developed by Berlekamp [15, 16],
but again, the basic ideas go back a long way. A straightforward imple-
mentation using fast polynomial arithmetic uses an expected number of
O(ℓ3 + ℓ1+o(1) len(q)) operations in F; the term ℓ3 may be replaced by ℓω,
where ω is the exponent of matrix multiplication (see §15.6).
There are no known eﬃcient, deterministic algorithms for factoring poly-
nomials over F when the characteristic p of F is large (even under a gener-
alization of the Riemann hypothesis, except in certain special cases).
The square-free decomposition of a polynomial over a ﬁeld K of character-
istic zero can be computed using an algorithm of Yun [105] using O(ℓ1+o(1))
operations in K. Yun’s algorithm can be adapted to work over ﬁnite ﬁelds
as well (see Exercise 14.30 in von zur Gathen and Gerhard [37]).
The asymptotically fastest algorithms for factoring polynomials over a
ﬁnite ﬁeld F are due to von zur Gathen, Kaltofen, and Shoup: the algorithm
of von zur Gathen and Shoup [38] uses an expected number of O(ℓ2+o(1) +
ℓ1+o(1) len(q)) operations in F; the algorithm of Kaltofen and Shoup [51]
has a cost that is subquadratic in the degree—it uses an expected number
of O(ℓ1.815 len(q)0.407) operations in F. Exercises 21.1, 21.7, and 21.8 are
based on [38]. Although the “fast” algorithms in [38] and [51] are mainly
of theoretical interest, a variant in [51], which uses O(ℓ2.5 + ℓ1+o(1) len(q))
operations in F, and space for O(ℓ1.5) elements of F, has proven to be quite
practical (Exercise 21.13 develops some of these ideas; see also Shoup [92]).
TEAM LinG

22
Deterministic primality testing
For many years, despite much research in the area, there was no known
deterministic, polynomial-time algorithm for testing whether a given integer
n > 1 is a prime. However, that is no longer the case—the breakthrough
algorithm of Agrawal, Kayal, and Saxena, or AKS algorithm for short, is just
such an algorithm. Not only is the result itself remarkable, but the algorithm
is striking in both its simplicity, and in the fact that the proof of its running
time and correctness are completely elementary (though ingenious).
We should stress at the outset that although this result is an important
theoretical result, as of yet, it has no real practical signiﬁcance: probabilistic
tests, such as the Miller–Rabin test discussed in Chapter 10, are much more
eﬃcient, and a practically minded person should not at all bothered by the
fact that such algorithms may in theory make a mistake with an incredibly
small probability.
22.1 The basic idea
The algorithm is based on the following fact:
Theorem 22.1. Let n > 1 be an integer. If n is prime, then for all a ∈Zn,
we have the following identity in the ring Zn[X]:
(X + a)n = Xn + a
(22.1)
Conversely, if n is composite, then for all a ∈Z∗
n, the identity (22.1) does
not hold.
Proof. Note that
(X + a)n = Xn + an +
n−1

i=1
n
i

aiXn−i.
489
TEAM LinG

490
Deterministic primality testing
If n is prime, then by Fermat’s little theorem (Theorem 2.16), we have
an = a, and by Exercise 1.12, all of the binomial coeﬃcients
n
i

, for i =
1, . . . , n−1, are divisible by n, and hence their images in the ring Zn vanish.
That proves that the identity (22.1) holds when n is prime.
Conversely, suppose that n is composite and that a ∈Z∗
n. Consider any
prime factor p of n, and suppose n = pkm, where p ∤m.
We claim that pk ∤
n
p

. To prove the claim, one simply observes that
n
p

= n(n −1) · · · (n −p + 1)
p!
,
and the numerator of this fraction is an integer divisible by pk, but no higher
power of p, and the denominator is divisible by p, but no higher power of p.
That proves the claim.
From the claim, and the fact that a ∈Z∗
n, it follows that the coeﬃcient of
Xn−p in (X + a)n is not zero, and hence the identity (22.1) does not hold. 2
Of course, Theorem 22.1 does not immediately give rise to an eﬃcient
primality test, since just evaluating the left-hand side of the identity (22.1)
takes time Ω(n) in the worst case. The key observation of Agrawal, Kayal,
and Saxena is that if (22.1) holds modulo Xr −1 for a suitably chosen value
of r, and for suﬃciently many a, then n must be prime. To make this idea
work, one must show that a suitable r exists that is bounded by a polynomial
in len(n), and that the number of diﬀerent values of a that must be tested
is also bounded by a polynomial in len(n).
22.2 The algorithm and its analysis
The algorithm is shown in Fig. 22.1. It takes as input an integer n > 1.
A few remarks on implementation are in order:
• In step 1, we can use the algorithm for perfect-power testing discussed
in §10.5, which is a deterministic, polynomial-time algorithm.
• The search for r in step 2 can just be done by brute-force search;
likewise, the determination of the multiplicative order of [n]r ∈Z∗
r can
be done by brute force: after verifying that gcd(n, r) = 1, compute
successive powers of n modulo r until we get 1.
We want to prove that Algorithm AKS runs in polynomial time and is
correct.
To prove that it runs in polynomial time, it clearly suﬃces to
prove that there exists an integer r satisfying the condition in step 2 that
is bounded by a polynomial in len(n), since all other computations can be
TEAM LinG

22.2 The algorithm and its analysis
491
1.
if n is of the form ab for integers a > 1 and b > 1 then
return false
2.
ﬁnd the smallest integer r > 1 such that either
gcd(n, r) > 1
or
gcd(n, r) = 1 and
[n]r ∈Z∗
r has multiplicative order > 4 len(n)2
3.
if r = n then return true
4.
if gcd(n, r) > 1 then return false
5.
for j ←1 to 2 len(n)⌊r1/2⌋+ 1 do
if (X + j)n ̸≡Xn + j (mod Xr −1) in the ring Zn[X] then
return false
6.
return true
Fig. 22.1. Algorithm AKS
carried out in time (r + len(n))O(1). Correctness means that it outputs true
if and only if n is prime.
22.2.1 Running time analysis
The question of the running time of Algorithm AKS is settled by the fol-
lowing fact:
Theorem 22.2. For integers n > 1 and m ≥1, the least prime r such
that r ∤n and the multiplicative order of [n]r ∈Z∗
r is greater than m is
O(m2 len(n)).
Proof. Call a prime r “good” if r ∤n and the multiplicative order of [n]r ∈Z∗
r
is greater than m, and otherwise call r “bad.” If r is bad, then either r | n
or r | (nd −1) for some d = 1, . . . , m. Thus, any bad prime r satisﬁes
r | n
m

d=1
(nd −1).
If all primes r up to some given bound x ≥2 are bad, then the product of
all primes up to x divides n m
d=1(nd −1), and so in particular,

r≤x
r ≤n
m

d=1
(nd −1),
TEAM LinG

492
Deterministic primality testing
where the ﬁrst product is over all primes r up to x. Taking logarithms, we
obtain

r≤x
log r ≤log

n
m

d=1
(nd −1)

≤(log n)

1 +
m

d=1
d

= (log n)(1 + m(m + 1)/2).
But by Theorem 5.6, we have

r≤x
log r ≥cx
for some constant c > 0, from which it follows that
x ≤c−1(log n)(1 + m(m + 1)/2),
and the theorem follows. 2
From this theorem, it follows that the value of r found in step 2—which
need not be prime—will be O(len(n)5). From this, we obtain:
Theorem 22.3. Algorithm AKS can be implemented so as to run in time
O(len(n)16.5).
Proof. As discussed above, the value of r determined in step 2 will be
O(len(n)5). It is fairly straightforward to see that the running time of the
algorithm is dominated by the running time of step 5. Here, we have to per-
form O(r1/2 len(n)) exponentiations to the power n in the ring Zn[X]/(Xr−1).
Each of these exponentiations takes O(len(n)) operations in Zn[X]/(Xr −1),
each of which takes O(r2) operations in Zn, each of which takes time
O(len(n)2). This yields a running time bounded by a constant times
r1/2 len(n) × len(n) × r2 × len(n)2 = r2.5 len(n)4.
Substituting the bound O(len(n)5) for r, we obtain the stated bound in the
theorem. 2
22.2.2 Correctness
As for the correctness of Algorithm AKS, we ﬁrst show:
Theorem 22.4. If the input to Algorithm AKS is prime, then the output
is true.
Proof. Assume that the input n is prime. The test in step 1 will certainly
fail. If the algorithm does not return true in step 3, then certainly the test
TEAM LinG

22.2 The algorithm and its analysis
493
in step 4 will fail as well. If the algorithm reaches step 5, then all of the
tests in the loop in step 5 will fail—this follows from Theorem 22.1. 2
The interesting case is the following:
Theorem 22.5. If the input to Algorithm AKS is composite, then the output
is false.
The proof of this theorem is rather long, and is the subject of the remain-
der of this section.
Suppose the input n is composite. If n is a prime power, then this will be
detected in step 1, so we may assume that n is not a prime power. Assume
that the algorithm has found a suitable value of r in step 2. Clearly, the test
in 3 will fail. If the test in step 4 passes, we are done, so we may assume
that this test fails; that is, we may assume that all prime factors of n are
greater than r. Our goal now is to show that one of the tests in the loop in
step 5 must pass. The proof will be by contradiction: we shall assume that
none of the tests pass, and derive a contradiction.
The assumption that none of the tests in step 5 fail means that in the
ring Zn[X], the following congruences hold:
(X + j)n ≡Xn + j (mod Xr −1)
(j = 1, . . . , 2 len(n)⌊r1/2⌋+ 1).
(22.2)
For the rest of the proof, we ﬁx any particular prime divisor p of n—the
choice does not matter. Since p | n, we have a natural ring homomorphism
from Zn[X] to Zp[X] (see Example 9.48), which implies that the congruences
(22.2) hold in the ring of polynomials over Zp as well. From now on, we
shall work exclusively with polynomials over Zp.
Let us state in somewhat more abstract terms the precise assumptions we
are making in order to derive our contradiction:
(A0) n > 1, r > 1, and ℓ≥1 are integers, p is a prime
dividing n, and gcd(n, r) = 1;
(A1) n is not a prime power;
(A2) p > r;
(A3) the congruences
(X + j)n ≡Xn + j (mod Xr −1)
(j = 1, . . . , ℓ)
hold in the ring Zp[X];
(A4) the multiplicative order of [n]r ∈Z∗
r is greater than
4 len(n)2;
(A5) ℓ> 2 len(n)⌊r1/2⌋.
TEAM LinG

494
Deterministic primality testing
The rest of the proof will rely only on these assumptions, and not on any
other details of Algorithm AKS. From now on, only assumption (A0) will
be implicitly in force. The other assumptions will be explicitly invoked as
necessary. Our goal is to show that assumptions (A1), (A2), (A3), (A4),
and (A5) cannot all be true simultaneously.
Deﬁne the Zp-algebra E := Zp[X]/(Xr−1), and let η := [X]Xr−1 ∈E, so that
E = Zp[η]. Every element of E can be expressed uniquely as g(η) = [g]Xr−1,
for g ∈Zp[X] of degree less than r, and for an arbitrary polynomial g ∈Zp[X],
we have g(η) = 0 if and only if (Xr −1) | g. Note that η ∈E∗and has
multiplicative order r: indeed, ηr = 1, and ηs −1 cannot be zero for s < r,
since Xs −1 has degree less than r.
Assumption (A3) implies that we have a number of interesting identities
in the Zp-algebra E:
(η + j)n = ηn + j
(j = 1, . . . , ℓ).
For the polynomials gj := X + j ∈Zp[X], with j in the given range, these
identities say that gj(η)n = gj(ηn).
In order to exploit these identities, we study more generally functions
σk, for various integer values k, that send g(η) ∈E to g(ηk), for arbitrary
g ∈Zp[X], and we investigate the implications of the assumption that such
functions behave like the kth power map on certain inputs. To this end, let
Z(r) denote the set of all positive integers k such that gcd(r, k) = 1. Note
that the set Z(r) is multiplicative; that is, 1 ∈Z(r), and for all k, k′ ∈Z(r),
we have kk′ ∈Z(r). Also note that because of our assumption (A0), both n
and p are in Z(r). For integer k ∈Z(r), let ˆσk : Zp[X] →E be the polynomial
evaluation map that sends g ∈Zp[X] to g(ηk). This is of course a Zp-algebra
homomorphism, and we have:
Lemma 22.6. For all k ∈Z(r), the kernel of ˆσk is (Xr −1), and the image
of ˆσk is E.
Proof. Let J := ker(ˆσk), which is an ideal of Zp[X]. Let k′ be a positive
integer such that kk′ ≡1 (mod r), which exists because gcd(r, k) = 1.
To show that J = (Xr −1), we ﬁrst observe that
ˆσk(Xr −1) = (ηk)r −1 = (ηr)k −1 = 1k −1 = 0,
and hence (Xr −1) ⊆J.
Next, we show that J ⊆(Xr −1). Let g ∈J. We want to show that
(Xr −1) | g. Now, g ∈J means that g(ηk) = 0. If we set h := g(Xk),
TEAM LinG

22.2 The algorithm and its analysis
495
this implies that h(η) = 0, which means that (Xr −1) | h. So let us write
h = (Xr −1)f, for some f ∈Zp[X]. Then
g(η) = g(ηkk′) = h(ηk′) = (ηk′r −1)f(ηk′) = 0,
which implies that (Xr −1) | g.
That ﬁnishes the proof that J = (Xr −1).
Finally, to show that ˆσk is surjective, suppose we are given an arbitrary
element of E, which we can express as g(η) for some g ∈Zp[X]. Now set
h := g(Xk′), and observe that
ˆσk(h) = h(ηk) = g(ηkk′) = g(η). 2
Because of lemma 22.6, then by Theorem 9.26, the map σk : E →E
that sends g(η) ∈E to g(ηk), for g ∈Zp[X], is well deﬁned, and is a ring
automorphism—indeed, a Zp-algebra automorphism—on E. Note that for
any k, k′ ∈Z(r), we have
• σk = σk′ if and only if ηk = ηk′ if and only if k ≡k′ (mod r), and
• σk ◦σk′ = σk′ ◦σk = σkk′.
So in fact, the set of all σk forms an abelian group (with respect to compo-
sition) that is isomorphic to Z∗
r.
Remark. It is perhaps helpful (but not necessary for the proof) to
examine the behavior of the map σk in a bit more detail. Let α ∈E,
and let
α =
r−1

i=0
giηi
be the canonical representation of α. Since gcd(r, k) = 1, the map
π : {0, . . . , r −1} →{0, . . . , r −1} that sends i to ki mod r is a
permutation whose inverse is the permutation π′ that sends i to
k′i mod r, where k′ is a multiplicative inverse of k modulo r. Then
we have
σk(α) =
r−1

i=0
giηki =
r−1

i=0
giηπ(i) =
r−1

i=0
gπ′(i)ηi.
Thus,
the action of σk is to permute the coordinate vector
(g0, . . . , gr−1) of α, sending α to the element in E whose coordinate
vector is (gπ′(0), . . . , gπ′(r−1)). So we see that although we deﬁned
the maps σk in a rather “highbrow” algebraic fashion, their behavior
in concrete terms is actually quite simple.
Recall that the pth power map on E is a Zp-algebra homomorphism (see
Theorem 20.7), and so for all α ∈E, if α = g(η) for g ∈Zp[X], then (by
TEAM LinG

496
Deterministic primality testing
Theorem 17.1) we have
αp = g(η)p = g(ηp) = σp(α).
Thus, σp acts just like the pth power map on all elements of E.
We can restate assumption (A3) as follows:
σn(η + j) = (η + j)n
(j = 1, . . . , ℓ).
That is to say, the map σn acts just like the nth power map on the elements
η + j for j = 1, . . . , ℓ.
Now, although the σp map must act like the pth power map on all of
E, there is no good reason why the σn map should act like the nth power
map on any particular element of E, and so the fact that it does so on all
the elements η + j for j = 1, . . . , ℓlooks decidedly suspicious. To turn our
suspicions into a contradiction, let us start by deﬁning some notation. For
α ∈E, let us deﬁne
C(α) := {k ∈Z(r) : σk(α) = αk},
and for k ∈Z(r), let us deﬁne
D(k) := {α ∈E : σk(α) = αk}.
In words: C(α) is the set of all k for which σk acts like the kth power map
on α, and D(k) is the set of all α for which σk acts like the kth power map
on α. From the discussion above, we have p ∈C(α) for all α ∈E, and it is
also clear that 1 ∈C(α) for all α ∈E. Also, it is clear that α ∈D(p) for all
α ∈E, and 1E ∈D(k) for all k ∈Z(r).
The following two simple lemmas say that the sets C(α) and D(k) are
multiplicative.
Lemma 22.7. For any α ∈E, if k ∈C(α) and k′ ∈C(α), then kk′ ∈C(α).
Proof. If σk(α) = αk and σk′(α) = αk′, then
σkk′(α) = σk(σk′(α)) = σk(αk′) = (σk(α))k′ = (αk)k′ = αkk′,
where we have made use of the homomorphic property of σk. 2
Lemma 22.8. For any k ∈Z(r), if α ∈D(k) and β ∈D(k), then αβ ∈
D(k).
Proof. If σk(α) = αk and σk(β) = βk, then
σk(αβ) = σk(α)σk(β) = αkβk = (αβ)k,
where again, we have made use of the homomorphic property of σk. 2
TEAM LinG

22.2 The algorithm and its analysis
497
Let us deﬁne
• s to be the multiplicative order of [p]r ∈Z∗
r, and
• t to be the order of the subgroup of Z∗
r generated by [p]r and [n]r.
Since r | (ps −1), if we take any extension ﬁeld F of degree s over Zp
(which we know exists by Theorem 20.11), then since F ∗is cyclic (Theo-
rem 9.15) and has order ps −1, we know that there exists an element ζ ∈F ∗
of multiplicative order r (Theorem 8.31). Let us deﬁne the polynomial eval-
uation map ˆτ : Zp[X] →F that sends g ∈Zp[X] to g(ζ) ∈F. Since Xr −1 is
clearly in the kernel of ˆτ, then by Theorem 9.27, the map τ : E →F that
sends g(η) to g(ζ), for g ∈Zp[X], is a well-deﬁned ring homomorphism, and
actually, it is a Zp-algebra homomorphism.
For concreteness, one could think of F as Zp[X]/(φ), where φ is an irre-
ducible factor of Xr −1 of degree s. In this case, we could simply take ζ to
be [X]φ (see Example 20.1), and the map ˆτ above would be just the natural
map from Zp[X] to Zp[X]/(φ).
The key to deriving our contradiction is to examine the set S := τ(D(n)),
that is, the image under τ of the set D(n) of all elements α ∈E for which
σn acts like the nth power map.
Lemma 22.9. Under assumption (A1), we have
|S| ≤n2⌊t1/2⌋.
Proof. Consider the set of integers
I := {nupv : u, v = 0, . . . , ⌊t1/2⌋}.
We ﬁrst claim that |I| > t. To prove this, we ﬁrst show that each distinct
pair (u, v) gives rise to a distinct value nupv. To this end, we make use of
our assumption (A1) that n is not a prime power, and so is divisible by some
prime q other than p. Thus, if (u′, v′) ̸= (u, v), then either
• u ̸= u′, in which case the power of q in the prime factorization of
nupv is diﬀerent from that in nu′pv′, or
• u = u′ and v ̸= v′, in which case the power of p in the prime factor-
ization of nupv is diﬀerent from that in nu′pv′.
The claim now follows from the fact that both u and v range over a set of
size ⌊t1/2⌋+ 1 > t1/2, and so there are strictly more than t such pairs (u, v).
Next, recall that t was deﬁned to be the order of the subgroup of Z∗
r
generated by [n]r and [p]r; equivalently, t is the number of distinct residue
classes of the form [nupv]r, where u and v range over all non-negative in-
tegers. Since each element of I is of the form nupv, and |I| > t, we may
TEAM LinG

498
Deterministic primality testing
conclude that there must be two distinct elements of I, call them k and k′,
that are congruent modulo r. Furthermore, any element of I is a product of
two positive integers each of which is at most n⌊t1/2⌋, and so both k and k′
lie in the range 1, . . . , n2⌊t1/2⌋.
Now, let α ∈D(n). This is equivalent to saying n ∈C(α). We always
have 1 ∈C(α) and p ∈C(α), and so by lemma 22.7, we have nupv ∈C(α)
for all non-negative integers u, v, and so in particular, k, k′ ∈C(α).
Since both k and k′ are in C(α), we have
σk(α) = αk and σk′(α) = αk′.
Since k ≡k′ (mod r), we have σk = σk′, and hence
αk = αk′.
Now apply the homomorphism τ, obtaining
τ(α)k = τ(α)k′.
Since this holds for all α ∈D(n), we conclude that all elements of S are
roots of the polynomial Xk −Xk′. Since k ̸= k′, we see that Xk −Xk′ is a
non-zero polynomial of degree at most max{k, k′} ≤n2⌊t1/2⌋, and hence can
have at most n2⌊t1/2⌋roots in the ﬁeld F (Theorem 9.14). 2
Lemma 22.10. Under assumptions (A2) and (A3), we have
|S| ≥2min(t,ℓ) −1.
Proof. Let m := min(t, ℓ). Under assumption (A3), we have η + j ∈D(n)
for j = 1, . . . , m. Under assumption (A2), we have p > r > t ≥m, and
hence the integers j = 1, . . . , m are distinct modulo p. Deﬁne
P :=
 m

j=1
(X + j)ej ∈Zp[X] : ej ∈{0, 1} for j = 1, . . . , m, and
m

j=1
ej < m

.
That is, we form P by taking products over all subsets S ⊊{X + j : j =
1, . . . , m}. Clearly, |P| = 2m −1.
Deﬁne P(η) := {f(η) ∈E : f ∈P} and P(ζ) := {f(ζ) ∈F : f ∈P}.
Note that τ(P(η)) = P(ζ), and that by lemma 22.8, P(η) ⊆D(n).
Therefore, to prove the lemma, it suﬃces to show that |P(ζ)| = 2m −1.
Suppose that this is not the case. This would give rise to distinct polynomials
g, h ∈Zp[X], both of degree at most t −1, such that
g(η) ∈D(n), h(η) ∈D(n), and τ(g(η)) = τ(h(η)).
So we have n ∈C(g(η)) and (as always) 1, p ∈C(g(η)). Likewise, we have
TEAM LinG

22.2 The algorithm and its analysis
499
1, n, p ∈C(h(η)). By lemma 22.7, for all integers k of the form nupv, where
u and v range over all non-negative integers, we have
k ∈C(g(η)) and k ∈C(h(η)).
For any such k, since τ(g(η)) = τ(h(η)), we have τ(g(η))k = τ(h(η))k, and
hence
0 = τ(g(η))k −τ(h(η))k
= τ(g(η)k) −τ(h(η)k)
(τ is a homomorphism)
= τ(g(ηk)) −τ(h(ηk))
(k ∈C(g(η)) and k ∈C(h(η)))
= g(ζk) −h(ζk)
(deﬁnition of τ).
Thus, the polynomial f := g −h ∈Zp[X] is a non-zero polynomial of degree
at most t −1, having roots ζk in the ﬁeld F for all k of the form nupv.
Now, t is by deﬁnition the number of distinct residue classes of the form
[nupv]r ∈Z∗
r. Also, since ζ has multiplicative order r, for integers k, k′, we
have ζk = ζk′ if and only if k ≡k′ (mod r). Therefore, as k ranges over
all integers of the form nupv, ζk ranges over precisely t distinct values in F.
But since all of these values are roots of the polynomial f, which is non-zero
and of degree at most t −1, this is impossible (Theorem 9.14). 2
We are now (ﬁnally!) in a position to complete the proof of Theorem 22.5.
Under assumptions (A1), (A2), and (A3), Lemmas 22.9 and 22.10 imply that
2min(t,ℓ) −1 ≤|S| ≤n2⌊t1/2⌋.
(22.3)
The contradiction is provided by the following:
Lemma 22.11. Under assumptions (A4) and (A5), we have
2min(t,ℓ) −1 > n2⌊t1/2⌋.
Proof. Observe that log2 n ≤len(n), and so it suﬃces to show that
2min(t,ℓ) −1 > 22 len(n)⌊t1/2⌋,
and for this, it suﬃces to show that
min(t, ℓ) > 2 len(n)⌊t1/2⌋,
since for any integers a, b with a > b ≥1, we have 2a > 2b + 1.
To show that t > 2 len(n)⌊t1/2⌋, it suﬃces to show that t > 2 len(n)t1/2,
or equivalently, that t > 4 len(n)2. But observe that by deﬁnition, t is the
order of the subgroup of Z∗
r generated by [n]r and [p]r, which is at least as
TEAM LinG

500
Deterministic primality testing
large as the multiplicative order of [n]r in Z∗
r, and by assumption (A4), this
is larger than 4 len(n)2.
Finally, directly by assumption (A5), we have ℓ> 2 len(n)⌊t1/2⌋. 2
That concludes the proof of Theorem 22.5.
Exercise 22.1. Show that if Conjecture 5.26 is true, then the value of r
discovered in step 2 of Algorithm AKS satisﬁes r = O(len(n)2).
22.3 Notes
The algorithm presented here is due to Agrawal, Kayal, and Saxena. The
paper is currently available only on the Internet [6]. The analysis in the
original version of the paper made use of a deep number-theoretic result of
Fouvry [36], but it was subsequently noticed that the algorithm can be fully
analyzed using just elementary arguments (as we have done here).
If fast algorithms for integer and polynomial arithmetic are used, then
using the analysis presented here, it is easy to see that the algorithm runs
in time O(len(n)10.5+o(1)). More generally, it is easy to see that the algo-
rithm runs in time O(r1.5+o(1) len(n)3+o(1)), where r is the value determined
in step 2 of the algorithm. In our analysis of the algorithm, we were able
to obtain the bound r = O(len(n)5), leading to the running-time bound
O(len(n)10.5+o(1)). Using Fouvry’s result, one can show that r = O(len(n)3),
leading to a running-time bound of O(len(n)7.5+o(1)). Moreover, if Conjec-
ture 5.26 on the density of Sophie Germain primes is true, then one could
show that r = O(len(n)2) (see Exercise 22.1), which would lead to a running-
time bound of O(len(n)6+o(1)).
Prior to this algorithm, the fastest deterministic, rigorously proved pri-
mality test was one introduced by Adleman, Pomerance, and Rumely [5],
called the Jacobi sum test, which runs in time
O(len(n)c len(len(len(n))))
for some constant c. Note that for numbers n with less than 2256 bits, the
value of len(len(len(n))) is at most 8, and so this algorithm runs in time
O(len(n)8c) for any n that one could ever actually write down.
We also mention the earlier work of Adleman and Huang [3], who gave a
probabilistic algorithm whose output is always correct, and which runs in
expected polynomial time (i.e., a Las Vegas algorithm, in the parlance of
§7.2).
TEAM LinG

Appendix: Some useful facts
A1. Some handy inequalities. The following inequalities involving expo-
nentials and logarithms are very handy.
(i) For all real x, we have
1 + x ≤ex,
or, taking logarithms,
log(1 + x) ≤x.
(ii) For all real x ≥0, we have
e−x ≤1 −x + x2/2,
or, taking logarithms,
−x ≤log(1 −x + x2/2).
(iii) For all real x with 0 ≤x ≤1/2, we have
1 −x ≥e−x−x2 ≥e−2x,
or, taking logarithms,
log(1 −x) ≥−x −x2 ≥−2x.
A2. Estimating sums by integrals. Using elementary calculus, it is easy
to estimate sums over a monotone sequences in terms of a deﬁnite
integral, by interpreting the integral as the area under a curve. Let
f be a real-valued function that is continuous and monotone on the
closed interval [a, b], where a and b are integers. Then we have
min(f(a), f(b)) ≤
b

i=a
f(i) −
 b
a
f(x)dx ≤max(f(a), f(b)).
501
TEAM LinG

502
Appendix: Some useful facts
A3. Integrating piece-wise continuous functions. In discussing the Rie-
mann integral
 b
a f(x)dx, many introductory calculus texts only dis-
cuss in any detail the case where the integrand f is continuous on
the closed interval [a, b], in which case the integral is always well de-
ﬁned. However, the Riemann integral is well deﬁned for much broader
classes of functions. For our purposes in this text, it is convenient
and suﬃcient to work with integrands that are piece-wise contin-
uous on [a, b], that is, there exist real numbers x0, x1, . . . , xk and
functions f1, . . . , fk, such that a = x0 ≤x1 ≤· · · ≤xk = b, and
for i = 1, . . . , k, the function fi is continuous on the closed interval
[xi−1, xi], and agrees with f on the open interval (xi−1, xi). In this
case, f is integrable on [a, b], and indeed
 b
a
f(x)dx =
k

i=1
 xi
xi−1
fi(x)dx.
It is not hard to prove this equality, using the basic deﬁnition of the
Riemann integral; however, for our purposes, we can also just take
the value of the expression on the right-hand side as the deﬁnition of
the integral on the left-hand side.
We also say that f is piece-wise continuous on [a, ∞) if for all b ≥a,
f is piece-wise continuous on [a, b]. In this case, we may deﬁne the
improper integral
 ∞
a f(x)dx as the limit, as b →∞, of
 b
a f(x)dx,
provided the limit exists.
A4. Inﬁnite series.
It is a basic fact from calculus that if an inﬁnite
series ∞
i=1 xi of non-negative terms converges to a value y, then any
inﬁnite series whose terms are a rearrangement of the xi converges
to the same value y.
An inﬁnite series ∞
i=1 xi, where now some of the xi may be negative,
is called absolutely convergent if the series ∞
i=1 |xi| is convergent.
It is a basic fact from calculus that if an inﬁnite series ∞
i=1 xi is
absolutely convergent, then not only does the series itself converge to
some value y, but any inﬁnite series whose terms are a rearrangement
of the xi also converges to the same value y.
A5. Double inﬁnite series. The topic of double inﬁnite series may not
be discussed in a typical introductory calculus course; we summa-
rize here the basic facts that we need. We state these facts without
proof, but all of them are fairly straightforward applications of the
deﬁnitions.
Suppose that xij, i, j = 1, 2, . . . are non-negative real numbers. The
TEAM LinG

Appendix: Some useful facts
503
ith row gives a series 
j xij, and if each of these converges, one can
form the double inﬁnite series 
i

j xij. Similarly, one may form
the double inﬁnite series 
j

i xij One may also arrange the terms
xij in a single inﬁnite series 
ij xij, using some enumeration of the
set of pairs (i, j). Then these three series either all diverge or all
converge to the same value.
If we drop the requirement that the xij are non-negative, but instead
require that the single inﬁnite series 
ij xij is absolutely convergent,
then these three series all converge to the same value.
As a special application of the above discussion, if the series 
i ai
is absolutely convergent and converges to A, and if the series 
j bj
is absolutely convergent and converges to B, then if we arrange the
terms aibj in any way in a single inﬁnite series 
ij aibj, this latter
series is absolutely convergent and converges to AB.
TEAM LinG

Bibliography
[1] L. M. Adleman. A subexponential algorithm for the discrete logarithm prob-
lem with applications to cryptography. In 20th Annual Symposium on Foun-
dations of Computer Science, pages 55–60, 1979.
[2] L. M. Adleman. The function ﬁeld sieve. In Proc. 1st International Sympo-
sium on Algorithmic Number Theory (ANTS-I), pages 108–121, 1994.
[3] L. M. Adleman and M.-D. Huang.
Primality Testing and Two Dimen-
sional Abelian Varieties over Finite Fields (Lecture Notes in Mathematics
No. 1512). Springer-Verlag, 1992.
[4] L. M. Adleman and H. W. Lenstra, Jr. Finding irreducible polynomials over
ﬁnite ﬁelds. In 18th Annual ACM Symposium on Theory of Computing, pages
350–355, 1986.
[5] L. M. Adleman, C. Pomerance, and R. S. Rumely. On distinguishing prime
numbers from composite numbers.
Annals of Mathematics, 117:173–206,
1983.
[6] M. Agrawal, N. Kayal, and N. Saxena. PRIMES is in P. Manuscript, www.
cse.iitk.ac.in/news/primality.html, 2002.
[7] W. Alford, A. Granville, and C. Pomerance.
There are inﬁntely many
Carmichael numbers. Annals of Mathematics, 140:703–722, 1994.
[8] T. M. Apostol. Introduction to Analytic Number Theory. Springer-Verlag,
1973.
[9] E. Bach.
How to generate factored random numbers.
SIAM Journal on
Computing, 17:179–193, 1988.
[10] E. Bach. Explicit bounds for primality testing and related problems. Mathe-
matics of Computation, 55:355–380, 1990.
[11] E. Bach. Eﬃcient prediction of Marsaglia-Zaman random number generators.
IEEE Transactions on Information Theory, IT-44:1253–1257, 1998.
[12] E. Bach and J. Shallit. Algorithmic Number Theory, volume 1. MIT Press,
1996.
[13] M. Bellare and P. Rogaway. Random oracles are practical: a paradigm for
designing eﬃcient protocols.
In First ACM Conference on Computer and
Communications Security, pages 62–73, 1993.
[14] M. Ben-Or. Probabilistic algorithms in ﬁnite ﬁelds. In 22nd Annual Sympo-
sium on Foundations of Computer Science, pages 394–398, 1981.
504
TEAM LinG

Bibliography
505
[15] E. R. Berlekamp. Algebraic Coding Theory. McGraw-Hill, 1968.
[16] E. R. Berlekamp. Factoring polynomials over large ﬁnite ﬁelds. Mathematics
of Computation, 24(111):713–735, 1970.
[17] L. Blum, M. Blum, and M. Shub. A simple unpredictable pseudo-random
number generator. SIAM Journal on Computing, 15:364–383, 1986.
[18] D. Boneh. The Decision Diﬃe-Hellman Problem. In Proc. 3rd International
Symposium on Algorithmic Number Theory (ANTS-III), pages 48–63, 1998.
Springer LNCS 1423.
[19] D. Boneh and G. Durfee. Cryptanalysis of RSA with private key d less than
N 0.292. IEEE Transactions on Information Theory, IT-46:1339–1349, 2000.
[20] R. P. Brent and H. T. Kung. Fast algorithms for manipulating formal power
series. Journal of the ACM, 25:581–595, 1978.
[21] J. P. Buhler, H. W. Lenstra, Jr., and C. Pomerance. Factoring integers with
the number ﬁeld sieve. In A. K. Lenstra and H. W. Lenstra, Jr., editors, The
Development of the Number Field Sieve, pages 50–94. Springer-Verlag, 1993.
[22] D. A. Burgess. The distribution of quadratic residues and non-residues. Math-
ematika, 4:106–112, 1957.
[23] E. Canﬁeld, P. Erd˝os, and C. Pomerance.
On a problem of Oppenheim
concerning ‘Factorisatio Numerorum’. Journal of Number Theory, 17:1–28,
1983.
[24] D. G. Cantor and E. Kaltofen. On fast multiplication of polynomials over
arbitrary rings. Acta Informatica, 28:693–701, 1991.
[25] J. L. Carter and M. N. Wegman. Universal classes of hash functions. Journal
of Computer and System Sciences, 18:143–154, 1979.
[26] A. L. Chistov. Polynomial time construction of a ﬁnite ﬁeld. In Abstracts
of Lectures at 7th All-Union Conference in Mathematical Logic, Novosibirsk,
page 196, 1984. In Russian.
[27] D. Coppersmith. Modiﬁcations to the number ﬁeld sieve. Journal of Cryp-
tology, 6:169–180, 1993.
[28] D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic pro-
gressions. Journal of Symbolic Computation, 9(3):23–52, 1990.
[29] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to
Algorithms. MIT Press, second edition, 2001.
[30] R. Crandall and C. Pomerance. Prime Numbers: A Computational Perspec-
tive. Springer, 2001.
[31] I. Damg˚ard and G. Frandsen. Eﬃcient algorithms for gcd and cubic residu-
osity in the ring of Eisenstein integers. In 14th International Symposium on
Fundamentals of Computation Theory, Springer LNCS 2751, pages 109–117,
2003.
[32] I. Damg˚ard, P. Landrock, and C. Pomerance. Average case error estimates
for the strong probable prime test. Mathematics of Computation, 61:177–194,
1993.
[33] W. Diﬃe and M. E. Hellman. New directions in cryptography. IEEE Trans-
actions on Information Theory, IT-22:644–654, 1976.
[34] J. Dixon. Asymptotocally fast factorization of integers. Mathematics of Com-
putation, 36:255–260, 1981.
TEAM LinG

506
Bibliography
[35] J. L. Dornstetter.
On the equivalence between Berlekamp’s and Euclid’s
algorithms. IEEE Transactions on Information Theory, IT-33:428–431, 1987.
[36] E. Fouvry. Th´eor`eme de Brun-Titchmarsh; application au th´eor`eme de Fer-
mat. Inventiones Mathematicae, 79:383–407, 1985.
[37] J. von zur Gathen and J. Gerhard. Modern Computer Algebra. Cambridge
University Press, 1999.
[38] J. von zur Gathen and V. Shoup. Computing Frobenius maps and factoring
polynomials. Computational Complexity, 2:187–224, 1992.
[39] S. Goldwasser and S. Micali. Probabilistic encryption. Journal of Computer
and System Sciences, 28:270–299, 1984.
[40] D. M. Gordon. Discrete logarithms in GF(p) using the number ﬁeld sieve.
SIAM Journal on Discrete Mathematics, 6:124–138, 1993.
[41] J. Gordon. Very simple method to ﬁnd the minimal polynomial of an arbitrary
non-zero element of a ﬁnite ﬁeld. Electronic Letters, 12:663–664, 1976.
[42] H. Halberstam and H. Richert. Sieve Methods. Academic Press, 1974.
[43] G. H. Hardy and J. E. Littlewood. Some problems of partito numerorum.
III. On the expression of a number as a sum of primes. Acta Mathematica,
44:1–70, 1923.
[44] G. H. Hardy and E. M. Wright. An Introduction to the Theory of Numbers.
Oxford University Press, ﬁfth edition, 1984.
[45] D. Heath-Brown. Zero-free regions for Dirichlet L-functions and the least
prime in an arithmetic progression. Proceedings of the London Mathematical
Society, 64:265–338, 1992.
[46] R. Impagliazzo, L. Levin, and M. Luby. Pseudo-random number generation
from any one-way function. In 21st Annual ACM Symposium on Theory of
Computing, pages 12–24, 1989.
[47] R. Impagliazzo and D. Zuckermann. How to recycle random bits. In 30th An-
nual Symposium on Foundations of Computer Science, pages 248–253, 1989.
[48] H. Iwaniec. On the error term in the linear sieve. Acta Arithmetica, 19:1–30,
1971.
[49] H. Iwaniec.
On the problem of Jacobsthal.
Demonstratio Mathematica,
11:225–231, 1978.
[50] A. Kalai. Generating random factored numbers, easily. In Proc. 13th ACM-
SIAM Symposium on Discrete Algorithms, page 412, 2002.
[51] E. Kaltofen and V. Shoup. Subquadratic-time factoring of polynomials over
ﬁnite ﬁelds. In 27th Annual ACM Symposium on Theory of Computing, pages
398–406, 1995.
[52] A. A. Karatsuba and Y. Ofman. Multiplication of multidigit numbers on
automata. Soviet Physics Doklady, 7:595–596, 1963.
[53] S. H. Kim and C. Pomerance. The probability that a random probable prime
is composite. Mathematics of Computation, 53(188):721–741, 1989.
[54] D. E. Knuth. The Art of Computer Programming, volume 2. Addison-Wesley,
second edition, 1981.
[55] D. Lehmann. On primality tests. SIAM Journal on Computing, 11:374–375,
1982.
TEAM LinG

Bibliography
507
[56] D. Lehmer and R. Powers. On factoring large numbers. Bulletin of the AMS,
37:770–776, 1931.
[57] H. W. Lenstra, Jr. Factoring integers with elliptic curves. Annals of Mathe-
matics, 126:649–673, 1987.
[58] H. W. Lenstra, Jr. and C. Pomerance. A rigorous time bound for factoring
integers. Journal of the AMS, 4:483–516, 1992.
[59] M. Luby. Pseudorandomness and Cryptographic Applications. Princeton Uni-
versity Press, 1996.
[60] J. Massey. Shift-register synthesis and BCH coding. IEEE Transactions on
Information Theory, IT-15:122–127, 1969.
[61] U. Maurer. Fast generation of prime numbers and secure public-key crypto-
graphic parameters. Journal of Cryptology, 8:123–155, 1995.
[62] A. Menezes, P. van Oorschot, and S. Vanstone. Handbook of Applied Cryp-
tography. CRC Press, 1997.
[63] G. L. Miller. Riemann’s hypothesis and tests for primality. Journal of Com-
puter and System Sciences, 13:300–317, 1976.
[64] W. Mills. Continued fractions and linear recurrences. Mathematics of Com-
putation, 29:173–180, 1975.
[65] K. Morrison.
Random polynomials over ﬁnite ﬁelds.
Manuscript, www.
calpoly.edu/~kmorriso/Research/RPFF.pdf, 1999.
[66] M. Morrison and J. Brillhart. A method of factoring and the factorization of
F7. Mathematics of Computation, 29:183–205, 1975.
[67] V. I. Nechaev. Complexity of a determinate algorithm for the discrete log-
arithm. Mathematical Notes, 55(2):165–172, 1994. Translated from Matem-
aticheskie Zametki, 55(2):91–101, 1994.
[68] I. Niven and H. Zuckerman. An Introduction to the Theory of Numbers. John
Wiley and Sons, Inc., second edition, 1966.
[69] J. Oesterl´e. Versions eﬀectives du th´eor`eme de Chebotarev sous l’hypoth`ese
de Riemann g´en´eralis´ee. Ast´erisque, 61:165–167, 1979.
[70] P. van Oorschot and M. Wiener. On Diﬃe-Hellman key agreement with short
exponents. In Advances in Cryptology–Eurocrypt ’96, Springer LNCS 1070,
pages 332–343, 1996.
[71] S. Pohlig and M. Hellman. An improved algorithm for computing logarithms
over GF(p) and its cryptographic signiﬁcance. IEEE Transactions on Infor-
mation Theory, IT-24:106–110, 1978.
[72] J. M. Pollard. Monte Carlo methods for index computation mod p. Mathe-
matics of Computation, 32:918–924, 1978.
[73] J. M. Pollard. Factoring with cubic integers. In A. K. Lenstra and H. W.
Lenstra, Jr., editors, The Development of the Number Field Sieve, pages 4–10.
Springer-Verlag, 1993.
[74] C. Pomerance. Analysis and comparison of some integer factoring algorithms.
In H. W. Lenstra, Jr. and R. Tijdeman, editors, Computational Methods in
Number Theory, Part I, pages 89–139. Mathematisch Centrum, 1982.
[75] M. O. Rabin. Probabilistic algorithms. In Algorithms and Complexity, Recent
Results and New Directions, pages 21–39. Academic Press, 1976.
[76] D. Redmond. Number Theory — An Introduction. Marcel Dekker, 1996.
TEAM LinG

508
Bibliography
[77] I. Reed and G. Solomon. Polynomial codes over certain ﬁnite ﬁelds. SIAM
Journal on Applied Mathematics, pages 300–304, 1960.
[78] R. L. Rivest, A. Shamir, and L. M. Adleman. A method for obtaining digi-
tal signatures and public-key cryptosystems. Communications of the ACM,
21(2):120–126, 1978.
[79] J. Rosser and L. Schoenfeld. Approximate formulas for some functions of
prime numbers. Illinois Journal of Mathematics, 6:64–94, 1962.
[80] O. Schirokauer, D. Weber, and T. Denny. Discrete logarithms: the eﬀective-
ness of the index calculus method. In Proc. 2nd International Symposium on
Algorithmic Number Theory (ANTS-II), pages 337–361, 1996.
[81] A. Sch¨onhage.
Schnelle Berechnung von Kettenbruchentwicklungen.
Acta
Informatica, 1:139–144, 1971.
[82] A. Sch¨onhage and V. Strassen. Schnelle Multiplikation grosser Zahlen. Com-
puting, 7:281–282, 1971.
[83] I. A. Semaev. Construction of irreducible polynomials over ﬁnite ﬁelds with
linearly independent roots. Mat. Sbornik, 135:520–532, 1988. In Russian;
English translation in Math. USSR–Sbornik, 63(2):507–519, 1989.
[84] A. Shamir.
Factoring numbers in O(log n) arithmetic steps.
Information
Processing Letters, 8:28–31, 1979.
[85] A. Shamir. How to share a secret. Communications of the ACM, 22:612–613,
1979.
[86] D. Shanks. Class number, a theory of factorization, and genera. In Proceedings
of Symposia in Pure Mathematics, volume 20, pages 415–440, 1969.
[87] P. Shor. Algorithms for quantum computation: discrete logarithms and fac-
toring.
In 35th Annual Symposium on Foundations of Computer Science,
pages 124–134, 1994.
[88] P. Shor.
Polynomial-time algorithms for prime factorization and discrete
logarithms on a quantum computer. SIAM Review, 41:303–332, 1999.
[89] V. Shoup.
New algorithms for ﬁnding irreducible polynomials over ﬁnite
ﬁelds. Mathematics of Computation, 54(189):435–447, 1990.
[90] V. Shoup.
Searching for primitive roots in ﬁnite ﬁelds.
Mathematics of
Computation, 58:369–380, 1992.
[91] V. Shoup.
Fast construction of irreducible polynomials over ﬁnite ﬁelds.
Journal of Symbolic Computation, 17(5):371–391, 1994.
[92] V. Shoup. A new polynomial factorization algorithm and its implementation.
Journal of Symbolic Computation, 20(4):363–397, 1995.
[93] V. Shoup. Lower bounds for discrete logarithms and related problems. In
Advances in Cryptology–Eurocrypt ’97, pages 256–266, 1997.
[94] R. Solovay and V. Strassen. A fast Monte-Carlo test for primality. SIAM
Journal on Computing, 6:84–85, 1977.
[95] J. Stein. Computational problems associated with Racah algebra. Journal of
Computational Physics, 1:397–405, 1967.
[96] A. Walﬁsz. Weylsche Exponentialsummen in der neueren Zahlentheorie. VEB
Deutscher Verlag der Wissenschaften, 1963.
[97] P. Wang, M. Guy, and J. Davenport. p-adic reconstruction of rational num-
bers. SIGSAM Bulletin, 16:2–3, 1982.
TEAM LinG

Bibliography
509
[98] Y. Wang. On the least primitive root of a prime. Scientia Sinica, 10(1):1–14,
1961.
[99] M. N. Wegman and J. L. Carter. New hash functions and their use in au-
thentication and set equality.
Journal of Computer and System Sciences,
22:265–279, 1981.
[100] A. Weilert. (1+i)-ary GCD computation in Z[i] as an analogue to the binary
GCD algorithm. Journal of Symbolic Computation, 30:605–617, 2000.
[101] A. Weilert.
Asymptotically fast GCD computation in Z[i].
In Proc. 4th
International Symposium on Algorithmic Number Theory (ANTS-IV), pages
595–613, 2000.
[102] L. Welch and R. Scholtz. Continued fractions and Berlekamp’s algorithm.
IEEE Transactions on Information Theory, IT-25:19–27, 1979.
[103] D. Wiedemann. Solving sparse linear systems over ﬁnite ﬁelds. IEEE Trans-
actions on Information Theory, IT-32:54–62, 1986.
[104] M. Wiener. Cryptanalysis of short RSA secret exponents. IEEE Transactions
on Information Theory, IT-44:553–558, 1990.
[105] D. Y. Y. Yun.
On square-free decomposition algorithms.
In Proc. ACM
Symposium on Symbolic and Algebraic Computation, pages 26–35, 1976.
TEAM LinG

Index of notation
Entries are listed in order of appearance.
∞: arithmetic with inﬁnity, xiv
log: natural logarithm, xiv
exp: exponential function, xiv
∅: the empty set, xiv
A ∪B: union of two sets, xiv
A ∩B: intersection of two sets, xiv
A \ B: diﬀerence of two sets, xiv
S1 × · · · × Sn: Cartesian product, xv
S×n: n-wise Cartesian product, xv
f(S): image of a set, xv
f−1(S): pre-image of a set, xv
f ◦g: function composition, xvi
Z: the integers, 1
b | a: b divides a, 1
⌊x⌋: ﬂoor of x, 3
a mod b: integer remainder, 3
⌈x⌉: ceiling of x, 3
aZ: ideal generated by a, 4
a1Z + · · · + akZ: ideal generated by a1, . . . , ak,
5
gcd: greatest common divisor, 6
νp(n): largest power to which p divides n, 8
lcm: least common multiple, 9
Q: the rational numbers, 9
a ≡b (mod n): a congruent to b modulo n, 13
b/a mod n: integer remainder, 17
a−1 mod n: integer modular inverse, 17
Zn: residue classes modulo n, 21
φ: Euler’s phi function, 24
µ: M¨obius function, 29
O, Ω, Θ, o, ∼: asymptotic notation, 33
len: length (in bits) of an integer, 46
rep(α): canonical representative of α ∈Zn, 48
π(x): number of primes up to x, 74
ϑ: Chebyshev’s theta function, 76
li: logarithmic integral, 87
ζ: Riemann’s zeta function, 88
P: probability function, 96
P[A | B]: conditional probability of A given B,
100
E[X]: expected value of X, 111
Var[X]: variance of X, 113
E[X | B]: conditional expectation of X given
B, 114
∆[X; Y ]: statistical distance, 131
mG: {ma : a ∈G}, 185
G{m}: {a ∈G : ma = 0G}, 186
Gm: {am : a ∈G}, 186
H1 + H2: {h1 + h2 : h1 ∈H1, h2 ∈H2}, 189
H1 · H2: {h1h2 : h1 ∈H1, h2 ∈H2}, 189
a ≡b (mod H): a −b ∈H, 190
a + H: coset of H containing a, 190
aH: coset of H containing a (multiplicative
notation), 190
G/H: quotient group, 191
[G : H]: index, 191
ker(ρ): kernel, 194
img(ρ): image, 194
G ∼
= G′: isomorphic groups, 197
⟨a⟩: subgroup generated by a, 202
⟨a1, . . . , ak⟩: subgroup generated by
a1, . . . , ak, 202
R: real numbers, 212
C: complex numbers, 212
¯α: complex conjugate of α, 212
N(α): norm of α ∈C, 213
b | a: b divides a, 214
R∗: multiplicative group of units of R, 214
Z[i]: Gaussian integers, 219
Q(m): {a/b : gcd(b, m) = 1}, 219
R[X]: ring of polynomials, 220
deg(a): degree of a polynomial, 223
lc(a): leading coeﬃcient of a polynomial, 223
a mod b: polynomial remainder, 224
D(a): formal derivative of a, 227
510
TEAM LinG

Index of notation
511
a1R + · · · + akR: ideal generated by
a1, . . . , ak, 231
(a1, . . . , ak): ideal generated by a1, . . . , ak, 231
R/I: quotient ring, 232
[a]I: the coset a + I, 232
[a]d: the coset a + dR, 232
R ∼
= R′: isomorphic rings, 237
logγ α: discrete logarithm, 268
(a | p): Legendre symbol, 285
(a | n): Jacobi symbol, 287
Jn: Jacobi map, 289
aM: {aα : α ∈M}, 301
M{a}: {α ∈M : aα = 0M}, 301
⟨α1, . . . , αn⟩R: submodule spanned by
α1, . . . , αn, 302
R[X]<ℓ: polynomials of degree less than ℓ, 302
M/N: quotient module, 303
M ∼
= M′: isomorphic modules, 304
dimF (V ): dimension, 311
A(i, j): (i, j) entry of A, 317
A(i): ith row of A, 317
A(·, j): jth column of A, 317
Rm×n: m × n matrices over R, 317
A⊤: transpose of A, 319
Ψ(y, x): number of y-smooth integers up to x,
336
gcd: greatest common divisor (polynomial),
368
lcm: least common multiple (polynomial), 370
b/a mod n: polynomial remainder, 371
a−1 mod n: polynomial modular inverse, 371
(E : F): degree of an extension, 377
R[[X]]: formal power series, 379
R((X)): formal Laurent series, 380
R((X−1)): reversed formal Laurent series, 381
deg(a): degree of a ∈R((X−1)), 381
lc(a): leading coeﬃcient of a ∈R((X−1)), 381
⌊a⌋: ﬂoor of a ∈R((X−1)), 382
len: length of a polynomial, 399
rep(α): canonical representative of
α ∈R[X]/(n), 400
DF (V ): dual space, 429
LF (V ): space of linear transformations, 440
NE/F (α): norm, 458
TrE/F (α): trace, 458
TEAM LinG

Index
Abel’s identity, 82
abelian group, 180
Adleman, L. M., 175, 179, 358, 488, 500
Agrawal, M., 489, 500
Alford, W., 267
algebra, 359
algebraic
element, 377
extension, 377
Apostol, T. M., 95
approximately computes, 155
arithmetic function, 28
Artin’s conjecture, 72
associate
elements of an integral domain, 383
polynomials, 366
associative binary operation, xvi
asymptotic notation, 33
Atlantic City algorithm, 156
automorphism
algebra, 360
group, 197
module, 304
ring, 237
vector space, 309
baby step/giant step method, 271
Bach, E., 73, 95, 179, 266, 281, 289, 298, 487
basis, 306
Bayes’ theorem, 101
Bellare, M., 358
Ben-Or, M., 487
Berlekamp subalgebra, 478
Berlekamp’s algorithm, 475
Berlekamp, E. R., 447, 488
Bernoulli trial, 96
Bertrand’s postulate, 78
big-O, -Omega, -Theta, 33
bijection, xv
bijective, xv
binary gcd algorithm, 57
binary operation, xvi
binary relation, xv
binomial distribution, 110, 116
binomial theorem, 214
birthday paradox, 121
bivariate polynomial, 228
Blum, L., 73
Blum, M., 73
Boneh, D., 179, 282
Bonferroni’s inequalities, 99
Boolean circuits, 53
Brent, R. P., 421
Brillhart, J., 356
Buhler, J. P., 357
Burgess, D. A., 298
C, 212
cancellation law
for integer congruences, 16
for polynomial congruences, 371
in an integral domain, 216
Canﬁeld, E., 357
canonical representative
integer, 48
polynomial, 400
Cantor, D. G., 488
Cantor–Zassenhaus algorithm, 467
Carmichael number, 248
Carmichael, R. D., 267
Carter, J. L., 147
Cartesian product, xiv
ceiling function, 3
characteristic of a ring, 213
characteristic polynomial, 458
Chebyshev’s inequality, 118
Chebyshev’s theorem, 74
Chebyshev’s theta function, 76
Chernoﬀbound, 119
Chinese remainder theorem
general, 242
integer, 18, 62
polynomial, 372, 406
Chistov, A. L., 488
classiﬁcation of cyclic groups, 202
collision probability, 137
512
TEAM LinG

Index
513
column null space, 331
column rank, 331
column space, 331
column vector, 317
common divisor
in an integral domain, 385
integer, 5
polynomial, 368
common multiple
in an integral domain, 385
integer, 8
polynomial, 370
commutative binary operation, xvi
commutative ring with unity, 211
companion matrix, 322
complex conjugation, 212, 240
composite, 2
conditional distribution, 99, 105
conditional expectation, 114
conditional probability, 100
congruence, 13, 190
conjugacy class, 456
conjugate, 456
constant polynomial, 220
constant term, 223
continued fraction method, 356
coordinate vector, 321
of a projection, 429
Coppersmith, D., 357
Cormen, T. H., 282
coset, 190
Crandall, R., 54, 95, 358
cyclic, 202
Damg˚ard, I., 267, 397
Davenport, J., 73
decisional Diﬃe–Hellman problem, 279
degree
of a polynomial, 223
of a reversed formal Laurent series, 381
of an element in an extension ﬁeld, 377
of an extension, 377
δ-uniform, 136
Denny, T., 358
derivative, 227
deterministic poly-time equivalent, 277
deterministic poly-time reducible, 277
diagonal matrix, 319
dictionary, 126
Diﬃe, W., 282
Diﬃe–Hellman key establishment protocol, 276
Diﬃe–Hellman problem, 276
dimension, 311
direct product
of algebras, 359
of groups, 184
of modules, 300
of rings, 213
Dirichlet inverse, 32
Dirichlet product, 28
Dirichlet series, 90
Dirichlet’s theorem, 92
Dirichlet, G., 95
discrete logarithm, 268
algorithm for computing, 270, 337
discrete probability distribution, 141
discriminant, 226
disjoint, xv
distinct degree factorization, 467, 483
divides, 1, 213
divisible by, 1, 214
division with remainder property
integer, 3
polynomial, 224, 367
divisor, 1, 214
Dixon, J., 356
Dornstetter, J. L., 447
dual space, 429
Durfee, G., 179
Eisenstein integers, 389
Eisenstein’s criterion, 395
elementary row operation, 325
elliptic curve method, 357
equal degree factorization, 469, 475
equivalence class, xv
equivalence relation, xv
Eratosthenes
sieve of, 85
Erd˝os, P., 357
error correcting code, 69, 412
error probability, 155
Euclidean algorithm
extended
integer, 58
polynomial, 403
integer, 55
polynomial, 402
Euclidean domain, 387
Euler’s identity, 89
Euler’s phi function, 24
and factoring, 263
Euler’s theorem, 26
Euler, L., 94
event, 97
execution path, 150
exp, xiv
expected polynomial time, 149
expected running time, 149
expected value, 111
exponent, 206
module, 305
extended Euclidean algorithm
integer, 58
polynomial, 403
extended Gaussian elimination, 327
extension ﬁeld, 219, 376
extension ring, 218
factoring
and Euler’s phi function, 263
factoring algorithm
TEAM LinG

514
Index
integer, 344, 352
deterministic, 421
polynomial, 467, 475
deterministic, 483
fast Fourier transform, 417
Fermat’s little theorem, 27
FFT, 417
ﬁeld, 215
ﬁeld of fractions, 363
ﬁnite dimensional, 311
ﬁnite extension, 376
ﬁnite ﬁelds
existence, 450
subﬁeld structure, 454
uniqueness, 454
ﬁnite probability distribution, 96
ﬁnitely generated
abelian group, 202
module, 306
ﬁxed ﬁeld, 455
ﬂoor function, 3
reversed formal Laurent series, 382
formal derivative, 227
formal Laurent series, 380
formal power series, 379
Fouvry, E., 500
Frandsen, G., 397
Frobenius map, 451
fundamental theorem of arithmetic, 2
fundamental theorem of ﬁnite abelian groups,
208
fundamental theorem of ﬁnite dimensional
F[X]-modules, 445
von zur Gathen, J., 422, 488
Gauss’ lemma, 285
Gaussian elimination, 325
Gaussian integers, 219, 240, 387, 390
gcd
integer, 6
polynomial, 368
generating polynomial, 424
generator, 202
algorithm for ﬁnding, 268
geometric distribution, 142, 145
Gerhard, J., 422, 488
Goldwasser, S., 298
Gordon, D. M., 358
Gordon, J., 488
Granville, A., 267
greatest common divisor
in an integral domain, 385
integer, 6
polynomial, 368
group, 180
guessing probability, 137
Guy, M., 73
Hadamard, J., 94
Halberstam, H., 267
Hardy, G. H., 94, 95
hash function, 125
universal, 127
hash table, 126
Heath-Brown, D., 95
Hellman, M., 282
Hensel lifting, 294
homomorphism
algebra, 360
group, 194
module, 303
ring, 236
vector space, 309
Horner’s rule, 400
Huang, M.-D., 500
hybrid argument, 135
Hypothesis H, 93
ideal, 4, 231
generated by, 4, 231
maximal, 234
prime, 234
principal, 4, 231
identity element, 180
identity matrix, 319
image, xv
image of a random variable, 104
Impagliazzo, R., 147
inclusion/exclusion principle, 99
index, 191
index calculus method, 358
indicator variable, 105
inﬁnite extension, 377
inﬁnite order, 183
injective, xv
integral domain, 215
inverse
multiplicative, 214
of a group element, 180
of a matrix, 323
inverse function, xvi
invertible matrix, 323
irreducible element, 383
irreducible polynomial, 366
algorithm for generating, 464
algorithm for testing, 462
number of, 453
isomorphism
algebra, 360
group, 197
module, 304
ring, 236
vector space, 309
Iwaniec, H., 281
Jacobi map, 289
Jacobi sum test, 500
Jacobi symbol, 287
algorithm for computing, 290
joint distribution, 105
k-wise independent, 105
TEAM LinG

Index
515
Kalai, A., 179
Kaltofen, E., 488
Karatsuba, A. A., 53
Kayal, N., 489, 500
kernel, 194
kills, 206
Kim, S. H., 267
Knuth, D. E., 53, 54, 73
von Koch, H., 94
Kronecker substitution, 416
Kung, H. T., 421
Lagrange interpolation formula, 372
Las Vegas algorithm, 157
law of large numbers, 118
law of quadratic reciprocity, 285
lcm
integer, 9
polynomial, 370
leading coeﬃcient, 223
of a reversed formal Laurent series, 381
least common multiple
in an integral domain, 385
integer, 9
polynomial, 370
leftover hash lemma, 138
Legendre symbol, 285
Lehmann, D., 267
Lehmer, D., 356
Leiserson, C. E., 282
len, 46, 399
length
of a polynomial, 399
of an integer, 46
Lenstra, Jr., H. W., 357, 488
Levin, L., 147
li, 87
linear map, 303
linear transformation, 440
linearly dependent, 306
linearly generated sequence, 423
minimal polynomial of, 424
of full rank, 429
linearly independent, 306
little-o, 33
Littlewood, J. E., 95
log, xiv
logarithmic integral, 87
lowest terms, 9
Luby, M., 147, 179
Markov’s inequality, 117
Massey, J., 447
matrix, 316
Maurer, U., 267
maximal ideal, 234
memory cells, 36
Menezes, A., 73, 179
Mertens’ theorem, 83
message authentication scheme, 128
Micali, S., 298
Miller, G. L., 266, 267
Miller–Rabin test, 247
Mills, W., 421, 447
min entropy, 137
minimal polynomial, 374
algorithm for computing, 401, 438, 466
of a linear transformation, 441
of a linearly generated sequence, 424
of a vector under a linear transformation,
442
M¨obius function (µ), 29
M¨obius inversion formula, 30
mod, 3, 13, 17, 224, 371
modular square root, 283
algorithm for computing, 292
module, 299
modulus, 13
monic associate, 366
monic polynomial, 223
monomial, 227, 229, 230
Monte Carlo algorithm, 156
Morrison, K., 447
Morrison, M., 356
multi-variate polynomial, 230
multiple root, 226
multiplication map, 194, 212, 305
multiplicative function, 28
multiplicative group of units, 214
multiplicative inverse, 23
in a ring, 214
modulo integers, 15
modulo polynomials, 371
multiplicative order, 26, 202
multiplicative order modulo n, 26
multiplicity, 226
mutually independent
events, 100
random variables, 105
natural map, 197
Newton interpolation, 406
Newton’s identities, 383
Niven, I., 289
non-constant polynomial, 220
non-trivial ring, 213
norm, 213, 458
normal basis, 461
number ﬁeld sieve, 357
Oesterl´e, J., 95
one-sided error, 157
van Oorschot, P., 73, 179, 282
order
in a module, 305
of a group element, 202
of an abelian group, 183
ordered basis, 321
pairwise disjoint, xv
pairwise independent
events, 100
TEAM LinG

516
Index
hash function, 125
random variables, 105
pairwise relatively prime
integer, 9
polynomial, 370
partition, xv
Penk, M., 73
perfect power, 261
period, 71
periodic sequence, 71
phi function of Euler, 24
PID, 388
pivot element, 326
pivot sequence, 324
Pohlig, S., 282
Pollard, J. M., 282, 357
polynomial
associate, 366
irreducible, 366
monic, 223
primitive, 392
reducible, 366
polynomial evaluation map, 238, 361
polynomial time, 38
expected, 149
strict, 149
Pomerance, C., 54, 95, 267, 357, 358, 500
de la Vall´ee Poussin, C.-J., 94, 95
power map, 194
pre-image, xv
pre-period, 71
preﬁx free, 150
primality test
deterministic, 489
probabilistic, 244
prime
ideal, 234
in an integral domain, 386
number, 2
prime number theorem, 86
irreducible polynomials over a ﬁnite ﬁeld,
453
primitive polynomial, 392
principal ideal, 4, 231
principal ideal domain, 388
probabilistic algorithm, 148
probability distribution
conditional, 99
discrete, 141
ﬁnite, 96
probability function, 96
product distribution, 98
program, 36
projection, 429
public key cryptography, 282
purely periodic, 71
Q, 9
quadratic formula, 226
quadratic reciprocity, 285
quadratic residue, 283
quadratic residuosity
algorithm for testing, 291
assumption, 297
quadratic sieve, 353
quantum computer, 358
quotient algebra, 359
quotient group, 191
quotient module, 303
quotient ring, 232
quotient space, 309
R, 212
Rabin, M. O., 266
Rackoﬀ, C., 357
RAM, 36
random access machine, 36
random self-reduction, 178
random variable, 104
conditional distribution of, 105
conditional expectation, 114
distribution of, 105
expected value, 111
image, 104
independent, 105
joint distribution, 105
k-wise independent, 105
mutually independent, 105
pairwise independent, 105
real, 104
variance, 113
randomized algorithm, 148
rank, 331
rational function ﬁeld, 366
rational function reconstruction, 410
rational reconstruction, 66
real random variable, 104
recursion tree, 282
Redmond, D., 95
reduced row echelon form, 324
reducible polynomial, 366
Reed, I., 421
Reed–Solomon code, 69, 412
relatively prime
in an integral domain, 385
integers, 6
polynomials, 368
Renyi entropy, 137
rep, 48, 400
repeated-squaring algorithm, 49
representation, 278
representative
of a coset, 190
of a residue class, 21
of an equivalence class, xv
residue class, 20, 232
residue class ring, 232
reversed formal Laurent series, 381
Richert, H., 267
Riemann hypothesis, 88, 90, 92, 94, 266, 267,
281, 298, 419, 488
Riemann’s zeta function, 88
TEAM LinG

Index
517
Riemann, B., 94
ring, 211
ring of polynomials, 220
Rivest, R. L., 175, 179, 282
Rogaway, P., 358
root of a polynomial, 224
Rosser, J., 94
row echelon form, 333
row null space, 329
row rank, 331
row space, 329
row vector, 317
RSA cryptosystem, 175
Rumely, R. S., 500
sample mean, 118
sample space, 96
Saxena, N., 489, 500
scalar, 299
scalar matrix, 319
scalar multiplication, 299
Schirokauer, O., 358
Schoenfeld, L., 94
Sch¨onhage, A., 53, 54, 73
Scholtz, R., 447
secret sharing scheme, 407
Semaev, I. A., 488
separating set, 484
Shallit, J., 95, 289, 487
Shamir, A., 52, 175, 179, 421
Shanks, D., 282
shift register sequence, 425
Shor, P., 358
Shoup, V., 281, 447, 487, 488
Shub, M., 73
sieve of Eratosthenes, 85
simple root, 226
smooth number, 336, 352
Solomon, G., 421
Solovay, R., 266, 298
solving linear congruences
integer, 17
polynomial, 371
Sophie Germain prime, 93
splitting ﬁeld, 378
square root (modular), 283
algorithm for computing, 292
square-free
integer, 10
polynomial, 449
square-free decomposition algorithm, 476, 485
standard basis, 306
statistical distance, 131
Stein, C., 282
Stein, J., 73
Strassen, V., 54, 266, 298
strict polynomial time, 149
subalgebra, 360
subﬁeld, 219
subgroup, 185
generated by, 202
submodule, 301
generated (or spanned) by, 302
subring, 218
subspace, 309
surjective, xv
theta function of Chebyshev, 76
total degree, 229, 230
trace, 458
transcendental element, 377
transpose, 319
trial division, 244
trivial ring, 213
twin primes conjecture, 94
two-sided error, 157
UFD, 384
ultimately periodic sequence, 71
unique factorization
in a Euclidean domain, 387
in a PID, 388
in D[X], 392
in F[X], 367
in Z, 2
unique factorization domain, 384
unit, 214
universal family of hash functions, 127
Vandermonde matrix, 373
Vanstone, S., 73, 179
variance, 113
vector, 299
vector space, 309
Walﬁsz, A., 94
Wang, P., 73
Wang, Y., 281
Weber, D., 358
Wegman, N. M., 147
Weilert, A., 397
Welch, L., 447
well-behaved complexity function, 51
Wiedemann, D., 447
Wiener, M., 179, 282
Wright, E. M., 94, 95
Yun, D. Y. Y., 488
Z, 1
Zassenhaus, H., 488
zero divisor, 215
zero-sided error, 157
zeta function of Riemann, 88
Zuckerman, H., 289
Zuckermann, D., 147
TEAM LinG


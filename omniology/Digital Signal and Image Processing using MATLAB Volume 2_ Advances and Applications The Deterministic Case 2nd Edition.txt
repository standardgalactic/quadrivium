Digital Signal
and Image Processing
using MATLAB®
2nd Edition Revised and Updated
Volume 2  – Advances and Applications
The Deterministic Case
Gérard Blanchet and Maurice Charbit  
DIGITAL SIGNAL AND IMAGE PROCESSING SERIES

 
 
 

Digital Signal and Image Processing using MATLAB® 
 
 

 
 
 

 
Revised and Updated 2nd Edition 
Digital Signal and Image 
Processing using MATLAB® 
 
 
 
Volume 2 
Advances and Applications:  
The Deterministic Case 
 
 
 
 
 
Gérard Blanchet  
Maurice Charbit  
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
First published 2015 in Great Britain and the United States by ISTE Ltd and John Wiley & Sons, Inc. 
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as 
permitted under the Copyright, Designs and Patents Act 1988, this publication may only be reproduced, 
stored or transmitted, in any form or by any means, with the prior permission in writing of the publishers, 
or in the case of reprographic reproduction in accordance with the terms and licenses issued by the  
CLA. Enquiries concerning reproduction outside these terms should be sent to the publishers at the 
undermentioned address: 
ISTE Ltd  
John Wiley & Sons, Inc.  
27-37 St George’s Road  
111 River Street 
London SW19 4EU 
Hoboken, NJ 07030 
UK  
USA  
www.iste.co.uk  
www.wiley.com 
© ISTE Ltd 2015 
The rights of Gérard Blanchet and Maurice Charbit to be identified as the authors of this work have been 
asserted by them in accordance with the Copyright, Designs and Patents Act 1988. 
Library of Congress Control Number:  2014958256 
 
British Library Cataloguing-in-Publication Data 
A CIP record for this book is available from the British Library  
ISBN 978-1-84821-641-9 
 
MATLAB®is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does not 
warrant the accuracy of the text or exercises in this book. This book’s use or discussion of MATLAB® 
software does not constitute endorsement or sponsorship by The MathWorks of a particular pedagogical 
approach or use of the MATLAB® software. 

Contents
Foreword
ix
Notations and Abbreviations
xi
Chapter 1
Recap on Digital Signal Processing
1
1.1
The sampling theorem . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Spectral contents . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.1
Discrete-time Fourier transform (DTFT) . . . . . . . . .
7
1.2.2
Discrete Fourier transform (DFT)
. . . . . . . . . . . .
8
1.3
Case of random signals . . . . . . . . . . . . . . . . . . . . . . .
10
1.4
Example of the Dual Tone Multi-Frequency (DTMF) . . . . . .
11
Chapter 2
Additional Information About Filtering
15
2.1
Filter implementation
. . . . . . . . . . . . . . . . . . . . . . .
15
2.1.1
Examples of ﬁlter structures . . . . . . . . . . . . . . . .
16
2.1.2
Distributing the calculation load in an FIR ﬁlter . . . .
20
2.1.3
FIR block ﬁltering . . . . . . . . . . . . . . . . . . . . .
21
2.1.4
FFT ﬁltering . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2
Filter banks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.2.1
Decimation and expansion . . . . . . . . . . . . . . . . .
30
2.2.2
Filter banks . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.3
Ripple control . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.3.1
Principle
. . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.3.2
Programming . . . . . . . . . . . . . . . . . . . . . . . .
44
Chapter 3
Image Processing
51
3.1
A little geometry . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.1.1
3D object . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.1.2
Calibration of cameras . . . . . . . . . . . . . . . . . . .
53
3.2
Pyramidal decompositions . . . . . . . . . . . . . . . . . . . . .
62
3.2.1
Pyramidal
decomposition
given
by
Burt
and
Adelson . . . . . . . . . . . . . . . . . . . . . . . . . . .
64

vi
Digital Signal and Image Processing using MATLAB®
3.2.2
Pyramidal decomposition using a Haar tranformation
.
65
3.2.3
Stepwise decomposition (lifting scheme) . . . . . . . . .
66
Chapter 4
Numerical Calculus and Simulation
71
4.1
Simulation of continuous-time systems . . . . . . . . . . . . . .
71
4.1.1
Simulation by approximation . . . . . . . . . . . . . . .
71
4.1.2
Exact model simulation . . . . . . . . . . . . . . . . . .
72
4.2
Solving of ordinary diﬀerential equations
(ODEs)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.2.1
Conversion from continuous to discrete time . . . . . . .
76
4.2.2
Linear case, continuous-time solution . . . . . . . . . . .
78
4.2.3
Remarks on the Runge–Kutta methods
. . . . . . . . .
81
4.3
Systems of equations and zero-seeking . . . . . . . . . . . . . .
88
4.3.1
Zeros of a function using the Newton method . . . . . .
88
4.3.2
Roots of a polynomial with the Newton–Raphson method
89
4.3.3
Systems of nonlinear equations . . . . . . . . . . . . . .
90
4.4
Interpolation
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.4.1
Thiele’s interpolation
. . . . . . . . . . . . . . . . . . .
92
4.4.2
Another decomposition in continuous fractions
. . . . .
95
4.4.3
Natural cubic splines . . . . . . . . . . . . . . . . . . . .
96
4.5
Solving of linear systems . . . . . . . . . . . . . . . . . . . . . .
100
4.5.1
Jacobi method
. . . . . . . . . . . . . . . . . . . . . . .
100
4.5.2
Relaxation method . . . . . . . . . . . . . . . . . . . . .
101
4.5.3
Cholesky factorization . . . . . . . . . . . . . . . . . . .
102
Chapter 5
Speech Processing
105
5.1
A speech signal model . . . . . . . . . . . . . . . . . . . . . . .
105
5.1.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . .
105
5.1.2
A typology of vocal sounds
. . . . . . . . . . . . . . . .
106
5.1.3
The AR model of speech production . . . . . . . . . . .
107
5.1.4
Compressing a speech signal . . . . . . . . . . . . . . . .
113
5.2
Dynamic Time Warping . . . . . . . . . . . . . . . . . . . . . .
116
5.2.1
The DTW algorithm . . . . . . . . . . . . . . . . . . . .
117
5.2.2
Examples of pathﬁnding rules . . . . . . . . . . . . . . .
118
5.2.3
Cepstral coeﬃcients
. . . . . . . . . . . . . . . . . . . .
119
5.3
Modifying the duration of an audio signal . . . . . . . . . . . .
120
5.3.1
PSOLA . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
5.3.2
Phase vocoder
. . . . . . . . . . . . . . . . . . . . . . .
123
5.4
Eliminating the impulse noise . . . . . . . . . . . . . . . . . . .
124
5.4.1
The signal model . . . . . . . . . . . . . . . . . . . . . .
125
5.4.2
Click detection . . . . . . . . . . . . . . . . . . . . . . .
126
5.4.3
Restoration . . . . . . . . . . . . . . . . . . . . . . . . .
128

Contents
vii
Chapter 6
Selected Topics
131
6.1
Tracking the cardiac rhythm of the fetus . . . . . . . . . . . . .
131
6.1.1
Objectives . . . . . . . . . . . . . . . . . . . . . . . . . .
131
6.1.2
Separating the EKG signals . . . . . . . . . . . . . . . .
132
6.1.3
Estimating cardiac rhythms . . . . . . . . . . . . . . . .
136
6.2
Extracting the contour of a coin
. . . . . . . . . . . . . . . . .
142
6.3
Constrained optimization and Lagrange
multipliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
6.3.1
Equality-constrained optimization
. . . . . . . . . . . .
146
6.3.2
Quadratic problem with linear inequality constraints . .
149
6.3.3
Portfolio optimization . . . . . . . . . . . . . . . . . . .
153
6.4
Principal Component Analysis (PCA)
. . . . . . . . . . . . . .
163
6.4.1
Determining the principal components . . . . . . . . . .
164
6.4.2
2-Dimension PCA
. . . . . . . . . . . . . . . . . . . . .
168
6.4.3
Linear Discriminant Analysis . . . . . . . . . . . . . . .
170
6.5
GPS positioning
. . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.6
The Viterbi algorithm . . . . . . . . . . . . . . . . . . . . . . .
178
6.6.1
Convolutional non-recursive encoder . . . . . . . . . . .
179
6.6.2
Decoding and hard decision . . . . . . . . . . . . . . . .
181
Chapter 7
Hints and Solutions
187
H1
Reminders on digital signal-processing . . . . . . . . . . . . . .
187
H2
Additional information on ﬁltering . . . . . . . . . . . . . . . .
189
H3
Image Processing . . . . . . . . . . . . . . . . . . . . . . . . . .
194
H4
Numerical calculus . . . . . . . . . . . . . . . . . . . . . . . . .
197
H5
Speech processing . . . . . . . . . . . . . . . . . . . . . . . . . .
215
H6
Selected topics
. . . . . . . . . . . . . . . . . . . . . . . . . . .
231
Chapter 8
Appendix
243
A1
A few properties of a matrix . . . . . . . . . . . . . . . . . . . .
243
A2
A few relations for matrices . . . . . . . . . . . . . . . . . . . .
246
Bibliography
247
Index
251


Foreword
This book represents the continuation to Digital Signal and Image Process-
ing: Fundamentals. It is assumed that the reader possesses a good knowledge
of the programming language MATLAB® and a command of the fundamen-
tal elements of digital signal processing: the usual transforms (the Discrete
Time Fourier Transform (DTFT), the Discrete Fourier Transform and the
z-Transform), the properties of deterministic and random signals, and digi-
tal ﬁltering. Readers will also need to be familiar with the fundamentals of
continuous-spectrum spectral analysis and have a certain amount of mathe-
matical knowledge concerning vector spaces.
In order to prevent the reading becoming a penance, we will oﬀer a few
reminders of the basics wherever necessary. This book is essentially a collection
of examples, exercises and case studies. It also presents applications of digital
signal- or image processing, and techniques which were not touched upon in
the previous volume.
Recap on digital signal processing
This section is devoted to the deﬁnitions and properties of the fundamental
transforms used in digital signal processing: Fourier transform, discrete time
Fourier transform and discrete Fourier transform. It concludes with a classic
example which enables us to put some known results into practice.
Filter implementation
This section deals with the structures of ﬁlters, the introduction of parallelism
into the ﬁltering operations (block ﬁltering and ﬁlter banks) and, by way of an
example, the Parks–McClellan method for FIR ﬁlter synthesis (ﬁnite impulse
response).
Image processing
The section given over to images oﬀers a few geometrical concepts relating to
the representation of 3D objects in a 2D space. Therein, we deal with problems

x
Digital Signal and Image Processing using MATLAB®
of calibration of cameras. In addition, image compression is also discussed, with
the use of examples (pyramidal decompositions, lifting scheme).
Digital calculus and simulation
This section deals with the algorithms used in most domains in digital process-
ing, and therefore far beyond mere signal processing. It only touches on the
domain using a few examples of methods applied to problems of simulation,
resolution of diﬀerential equations, zero-seeking, interpolation and iterative
methods for solving linear systems.
Speech processing
After a brief introduction to speech production, we will discuss the represen-
tation of a speech signal by an autoregressive model, and its application to
compression. Next we will give the descriptions of the techniques widely used
in this ﬁeld (Dynamic Time Warping and PSOLA) and, ﬁnally, an example of
application with “decrackling” for audio recordings.
Selected topics
This last chapter presents case studies that go a little further in depth than
the examples described in the previous sections. “Tracking the cardiac rhythm
of the fetus” and “Extracting the contour of a coin” are classic examples of
the application of the least squares method. Principal component analysis and
linear discriminant analysis are basic methods for the classiﬁcation of objects
(in a very broad sense).
The section devoted to optimization under constraints could have been part
of the section on numerical methods. The method of Lagrange multipliers is
encountered in a multitude of applications. In terms of applications, we present
the case of optimization of a stock portfolio.
We conclude with the example of the Viterbi algorithm for the hard de-
coding of convolutional codes. This algorithm is, in fact, a particular case for
searching for the shortest possible path in a lattice.

Notations and
Abbreviations
∅
empty set
∑
k,n
=
∑
k
∑
n
rectT (t)
=
{
1
when
|t| < T/2
0
otherwise
sinc(x)
=
sin(πx)
πx
1(x ∈A)
=
{
1
when x ∈A
0
otherwise
(indicator function of A)
(a, b]
=
{x : a < x ≤b}
δ(t)
{
Dirac distribution when t ∈R
Kronecker symbol when t ∈Z
Re(z)
real part of z
Im(z)
imaginary part of z
⌊x⌋
integer part of x
i or j
=
√
−1
x(t) ⇌X(f)
Fourier transform
(x ⋆y)(t)
continuous time convolution
=
∫
R x(u)y(t −u)du
(x ⋆y)(t)
discrete time convolution
=
∑
u∈Z x(u)y(t −u) = ∑
u∈Z x(t −u)y(u)
y(n)(t)
=
dny(t)
dtn
, nth order derivative

xii
Digital Signal and Image Processing using MATLAB®
x or x
vector x
IN
(N × N)-dimension identity matrix
A∗
complex conjugate of A
AT
transpose of A
AH
transpose-conjugate of A
A−1
inverse matrix of A
A#
pseudo-inverse matrix of A
P {X ∈A}
probability that X ∈A
E {X}
expectation value of X
Xc = X −E {X}
zero-mean random variable
var {X} = E
{
|Xc|2}
variance of X
E {X|Y }
conditional expectation of X given Y
ADC
Analog to Digital Converter
ADPCM
Adaptive Diﬀerential PCM
AR
Autoregressive
ARMA
AR and MA
BER
Bit Error Rate
bps
Bits per second
cdf
Cumulative distribution function
CF
Clipping Factor
CZT
Causal z-Transform
DAC
Digital to Analog Converter
DCT
Discrete Cosine Transform
d.e./de
Diﬀerence equation
DFT
Discrete Fourier Transform
DTFT
Discrete Time Fourier Transform
DTMF
Dual Tone Multi-Frequency
dsp
Digital signal processing/processor
e.s.d./esd
Energy spectral density
FIR
Finite Impulse Response
FFT
Fast Fourier Transform
FT
Continuous Time Fourier Transform

Notations and Abbreviations
xiii
IDFT
Inverse Discrete Fourier Transform
i.i.d./iid
Independent and Identically Distributed
IIR
Inﬁnite Impulse Response
ISI
InterSymbol Interference
LDA
Linear discriminant analysis
lms
Least mean squares
MA
Moving Average
MAC
Multiplication ACcumulation
OTF
Optical Transfer Function
PAM
Pulse Amplitude Modulation
PCA
Principal Component Analysis
p.d.
Probability Distribution
ppi
Points per Inch
p.s.d./PSD
Power Spectral Density
PSF
Point Spread Function
PSK
Phase Shift Keying
QAM
Quadrature Amplitude Modulation
rls
Recursive least squares
rms
Root mean square
r.p./rp
Random process
SNR
Signal to Noise Ratio
r.v./rv
Random variable
STFT
Short Term Fourier Transform
TF
Transfer Function
WSS
Wide (Weak) Sense Stationary (Second Order) Process
ZOH
Zero-Order Hold
ZT
z-Transform


Chapter 1
Recap on Digital Signal
Processing
Signal processing consists of handling data in order to extract information
considered relevant, or to modify them so as to give them useful properties:
extracting, for example, information on a plane’s speed or distance from a
RADAR signal, making an old and decayed sound recording clearer, synthe-
sizing a sentence on an answering machine, transmitting information through
a communication channel, etc.
The processing is called digital if it deals with a discrete sequence of values
{x1, x2, . . .}. There are two types of scenario: either the observation is already
a sequence of numbers, as is the case for example for economic data, either the
observed phenomenon is “continuous-time”, and the signal’s value x(t) must
then be measured at regular intervals.
This second scenario has tremendous practical applications. This is why an
entire section of this chapter is devoted to the operation called sampling.
The acquisition chain is described in Figure 1.1.
continuous-time
signal
discrete-time
signal
(sampling
period)
acquisition,
measure
supply voltage
(references)
Figure 1.1 – Digital signal acquisition

2
Digital Signal and Image Processing using MATLAB®
The essential part of the acquisition device is usually the analog-to-digital
converter, or ADC, which samples the value of the input voltage at regular
intervals – every Ts seconds – and provides a coded representation at the output.
To be absolutely correct, this coded value is not exactly equal to the value
of x(nTs). However, in the course of this chapter, we will assume that xs(n) =
x(nTs). The sequence of these numerical values will be referred to as the digital
signal, or more plainly as the signal.
Ts is called the sampling period and Fs = 1/Ts the sampling frequency. The
gap between the actual value and the coded value is called quantization noise.
Obviously, the sampling frequency must be high enough “in order not to
lose too much information” – a concept we will discuss later on – from the orig-
inal signal, and there is a connection between this frequency and the sampled
signal’s “frequential content”. Anybody who conducts experiments knows this
“graph plotting principle”: when the signal’s value changes quickly (presence
of “high frequencies”), “many” points have to be plotted (it would actually be
preferable to use the phrase “high point density”), whereas when the signal’s
value changes slowly (presence of low frequencies), fewer points need to be
plotted.
To sum up, the signal sampling must be done in such a way that the nu-
merical sequence {xs(n)} alone is enough to reconstruct the continuous-time
signal. The sampling theorem speciﬁes the conditions that need to be met for
perfect reconstruction to be possible.
1.1
The sampling theorem
Let x(t) be a continuous signal, with X(F) its Fourier transform, which will
also be called the spectrum. The sample sequence measured at the frequency
Fs = 1/Ts is denoted by xs(n) = x(nTs).
Deﬁnition 1.1 When X(F) ̸= 0 for F ∈(B1, B2) and X(F) = 0 everywhere
else, x(t) is said to be (B1, B2) band-limited. If x(t) is real, its Fourier transform
has a property called Hermitian symmetry, meaning that X(F) = X∗(−F), and
the frequency band’s expression is (−B, +B). A common misuse of language
consists of referring to the signal as a B-band signal.
Perfect reconstruction
Our goal is to reconstruct x(t), at every time t, using the sampling sequence
xs(n) = x(nTs), while imposing a “reconstruction scheme” deﬁned by the ex-
pression (1.1):
y(t) =
+∞
∑
n=−∞
x(nTs)h(t −nTs)
(1.1)

Recap on Digital Signal Processing
3
Figure 1.2 – Spectra of band-limited signals
where h(t) is called a reconstruction function. Notice that (1.1) is linear with
respect to x(nTs). In order to reach this objective, two questions have to be
answered:
1. is there a class of signals x(t) large enough for y(t) to be identical to x(t)?
2. if that is the case, what is the expression of h(t)?
The answers to these questions are provided by the sampling theorem (1.1).
Theorem 1.1 (Sampling theorem)
Let x(t) be a (B1, B2) band-limited signal, real or complex, and let {x(nTs)} be
its sample sequence, then there are two possible cases:
1. If Fs = 1/Ts is such that Fs ≥B2 −B1, then x(t) can be perfectly
reconstructed from its samples x(nTs) using the expression:
x(t)
=
+∞
∑
n=−∞
x(nTs)h(B1,B2)(t −nTs)
(1.2)
where the FT of the reconstruction function h(B1,B2)(t) is:
H(B1,B2)(F) = 1
Fs
1(F ∈(B1, B2))
(1.3)
2. If Fs = 1/Ts < B2 −B1, perfect reconstruction turns out to be impossible
because of the “spectrum aliasing” phenomenon.
The proof uses the Poisson summation formula which gives the relation
between X(F) and the values of x(t) at sampling times nTs, and makes it
possible to determine the expression of the spectrum of the signal y(t) deﬁned
by equation (1.1).

4
Digital Signal and Image Processing using MATLAB®
Lemma 1.1 (Poisson formula) Let x(t) be a signal, and X(F) its Fourier
transform. Then for any Ts:
1
Ts
+∞
∑
k=−∞
X(F −kFs) =
+∞
∑
n=−∞
x(nTs) exp(−2jπnFTs)
(1.4)
where the left member is assumed to be a continuous function of F.
We will use the following deﬁnition for the discrete-time Fourier transform.
We will see another completely equivalent expression of it (deﬁnition 1.3), but
more frequently used in the case of numerical sequences.
Deﬁnition 1.2 (DTFT) The sum ∑+∞
n=−∞x(nTs) exp(−2jπnFTs) is called
the Discrete-Time Fourier Transform (DTFT) of the sequence {x(nTs)}.
We now go back to the sampling theorem. By using the fact that the Fourier
transform of h(t−nTs) is H(F)e−2jπnF Ts, the Fourier transform of y(t), deﬁned
by (1.1), can be written:
Y (F)
=
+∞
∑
n=−∞
x(nTs) × H(F)e−2jπnF Ts
=
H(F)
Ts
+∞
∑
k=−∞
X(F −kFs)
(1.5)
Therefore, if Fs ≥B2 −B1, the diﬀerent contributions X(F −kFs) do
not overlap, and by simply assuming H(B1,B2)(F) = Ts1(F ∈(B1, B2)), Y (F)
coincides exactly with X(F). Figure 1.3 illustrates this case for a real signal.
In this case, B1 = −B and B2 = B.
transition band
Figure 1.3 – Real signal reconstruction

Recap on Digital Signal Processing
5
Except if speciﬁed otherwise, we will assume from now on that x(t) is real.
The suﬃcient reconstruction condition can be written as follows:
Fs ≥2B
(1.6)
The limit frequency 2B is called the Nyquist frequency. Still in the same
case, the Fourier transform of a possible reconstruction function is HB(F) =
Tsrect2B(F), and therefore:
hB(t) = sin(2πBt)
πFst
(1.7)
It should be noted that the function HB(F) = Tsrect2B(F) is not the only
possible function. If Fs is assumed to be strictly greater than 2B, then we can
choose a ﬁlter with larger transition bands (see Figure 1.3), making it easier to
design.
When there is no possible doubt, we will not indicate the dependence on
B, and simply write h(t) instead of hB(t).
Anti-aliasing ﬁlter
The reconstruction formula (1.1), is, according to the Poisson formula (1.4),
associated with the periodization of the spectrum X(F) with the period Fs. It
follows that, for Fs < 2B, the diﬀerent non-zero parts of the spectrum over-
lap, making perfect reconstruction impossible. The overlapping phenomenon
is called spectrum aliasing.
Figure 1.4 illustrates the spectrum aliasing phenomenon for a real signal
whose frequential content is of the “low-pass” type, implicitly meaning that it
“ﬁlls up” the band (−Fs/2, +Fs/2).
Except in some particular cases, we will assume that spectrum signals are
of this type, or that they can be modiﬁed to ﬁt this description.
Figure 1.4 – The aliasing phenomenon

6
Digital Signal and Image Processing using MATLAB®
For a real signal, showing aliasing means that the frequencies beyond the
frequency Fs/2 can be “brought back” to the (−Fs/2, +Fs/2) band.
In practice, the following cases will occur:
1. the sampling frequency is imposed: if, knowing how the data is used, the
aliasing phenomenon is considered to “cause damage”, the appropriate
procedure for sampling a real signal requires the use of low-pass ﬁlter-
ing called anti-aliasing ﬁltering which eliminates the components of the
frequencies higher than Fs/2;
2. the sampling frequency is not imposed: in this case, it can be chosen
high enough so that the aliased components of the signal do not alter
the expected results. If this is not possible, Fs is set, and the situation
becomes the same as in the ﬁrst case.
Speech signals are a good example. If they are sampled at 8,000 Hz, an ex-
tremely common value, high enough to make the person speaking recognizable
and understandable, and if no anti-aliasing ﬁltering is done, the reconstructed
signal contains a “hissing” noise. This alone justiﬁes the use of an anti-aliasing
ﬁlter. The irretrievable loss of high frequency components is actually better
than the presence of aliasing.
Figure 1.5 illustrates the case of a “low-pass”, preﬁltered, real signal to
prevent aliasing.
prefiltered
signal
B0
−B0
Figure 1.5 – Absence of aliasing after [−B0, +B0] ﬁltering
In general, it is important to understand that anti-aliasing ﬁltering must
be done in the band that is considered essential (useful band) to the unaliased
signal reconstruction. The low-pass ﬁltering mentioned here corresponds to a
low-pass sampled signal.
The following general rule can be stated:

Recap on Digital Signal Processing
7
The sampling operation of a signal at the frequency Fs must be preceded
by an anti-aliasing ﬁltering with a gain equal to 1 and with a width of
Fs in the useful band.
Spectrum aliasing and ambiguity
For a given signal, for any integer k, it is not possible to distinguish F0 from
F1 = F0 + kFs, k ∈Z, which is called the image frequency of F0 relative to
Fs. Hence, x1(t) = sin(2πF0t) and x2(t) = sin(2π(F0 + kFs)t), with k ∈Z
take exactly the same values if both are collected at frequency Fs. This is the
ambiguity due to the spectrum aliasing phenomenon (or generally speaking to
the Poisson formula).
1.2
Spectral contents
1.2.1
Discrete-time Fourier transform (DTFT)
The sampling period Ts appears in the DTFT’s expression in deﬁnition 1.3.
Deﬁnition 1.3 (DTFT) The discrete-time Fourier transform of a sequence
{x(n)} is the function of the real variable f, periodic with period 1, deﬁned by:
X(f) =
+∞
∑
n=−∞
x(n) exp(−2jπnf)
(1.8)
As you can see, we need only impose FTs = f and replace x(nTs) by x(n)
to go from (1.4) to(1.8)(1).
Deﬁnition (1.3) calls for a few comments: it can be proven that if {x(n)} is
summable (∑
n |x(n)| < +∞), the series (1.8) converges uniformly to a continu-
ous function X(f). However, if {x(n)} is square summable (∑
n |x(n)|2 < +∞)
without having a summable modulus, then the series converges in quadratic
mean. There can be no uniform convergence.
Because of its periodicity, the DTFT is plotted on an interval of length 1,
most often the intervals (−1/2, +1/2) or (0, 1).
Starting oﬀfrom X(f), how can we go back to x(n)? One possible answer
is given in the following result.
Theorem 1.2 (Inverse DTFT) If X(f) is a periodic function with period
1, and if
∫1
0 |X(f)|2df < +∞, then X(f) = ∑
n x(n)e−2jπnf, where the x(n)
coeﬃcients are given by:
x(n) =
∫1/2
−1/2
X(f)e2jπnfdf
(1.9)
(1)X(F), which refers to the FT in (1.4) must not be confused with X(f), the DTFT.

8
Digital Signal and Image Processing using MATLAB®
As in the continuous-time case, we have the Parseval formula:
+∞
∑
n=−∞
|x(n)|2 =
∫1/2
−1/2
|X(f)|2df
(1.10)
and the conservation of the dot product:
+∞
∑
n=−∞
x(n)y∗(n) =
∫1/2
−1/2
X(f)Y ∗(f)df
(1.11)
Because the left member of (1.10) is, by deﬁnition, the signal’s energy,
|X(f)|2 represents the energy’s distribution along the frequency axis.
It is
therefore called the energy spectral density (esd), or spectrum. In the literature,
this last word is associated with the function |X(f)|.
If X(f) is included,
this adds up to three deﬁnitions for the same word. But in practice, this is
not important, as the context is often enough to clear up any ambiguity. It
should be pointed out that the two expressions |X(f)| and |X(f)|2 become
proportional if the decibel scale is used, by imposing:
SdB(f) = 20 log10 |X(f)|
(1.12)
1.2.2
Discrete Fourier transform (DFT)
Deﬁnition of the discrete Fourier transform
A computer calculation of the DTFT, based on the values of the samples x(n),
imposes an inﬁnite workload, because the sequence is made up of an inﬁnity of
terms, and because the frequency f varies continuously on the interval (0, 1).
This is why, digitally speaking, the DTFT does not stand a chance against the
Discrete Fourier Transform, or DFT. The DFT calculation is limited to a ﬁnite
number of values of n, and a ﬁnite number of values of f.
The digital use of the DFT has acquired an enormous and undisputed prac-
tical importance with the discovery of a fast calculation method known as the
Fast Fourier Transform, or FFT.
Consider the ﬁnite sequence {x(0), . . ., x(P −1)}. Using deﬁnition (1.8),
its DTFT is expressed X(f) = ∑P −1
n=0 x(n)e−2jπnf where f ∈(0, 1). In order
to obtain the values of X(f) using a calculator, only a ﬁnite number N of
values for f are taken. The ﬁrst idea that comes to mind is to take N values,
uniformly spaced-out in [0, 1[, meaning that f = k/N with k ∈{0, . . ., N −1}.
This gives us the N values:
X(k/N) =
P −1
∑
n=0
x(n)e−2jπnk/N
(1.13)

Recap on Digital Signal Processing
9
In this expression, P and N play two very diﬀerent roles: N is the number
of points used to calculate the DTFT, and P is the number of observed points
of the temporal sequence. N inﬂuences the precision of the plotting of X(f),
whereas P is related to what is called the frequency resolution.
In practice, P and N are chosen so that N ≥P. We then impose:
˜x(n) =
{ x(n)
for n ∈{0, . . . , P −1}
0
for n ∈{P, . . . , N −1}
Obviously:
X(k/N) =
P −1
∑
n=0
x(n)e−2jπnk/N =
N−1
∑
n=0
˜x(n)e−2jπnk/N
Because the sequence x(n) is completed with (N −P) zeros, an operation
called zero-padding, in the end we have as many points for the sequence ˜x(n)
as we do for X(k/N). Choosing to take as many points for both the temporal
sequence and the frequential sequence does not restrict in any way the concepts
we are trying to explain. This leads to the deﬁnition of the discrete Fourier
transform.
Deﬁnition 1.4 Let {x(n)} be a N-length sequence. Its discrete Fourier trans-
form or DFT is deﬁned by:
X(k) =
N−1
∑
n=0
x(n)W nk
N ,
k ∈(0, 1, . . . N −1)
(1.14)
where WN = e−2jπ/N
(1.15)
is an N-th root of unity, that is to say such that W N
N = 1. The inverse formula,
leading from the sequence {X(k)} to the sequence {x(n)}, is:
x(n) = 1
N
N−1
∑
k=0
X(k)W −nk
N
(1.16)
Properties of the DFT
The properties of the DFT show strong similarities with those of the DTFT.
However, there is an essential diﬀerence. In the formulas associated with the
DFT, all the index calculations are done modulo N.
Fast Fourier transform
The fast Fourier transform, or FFT, ﬁrst published in 1965 by J. W. Cooley
and J. W. Tuckey [8], is a fast DFT calculation technique. The basic algorithm,

10
Digital Signal and Image Processing using MATLAB®
many versions of which can be found, calculates a number of points N, equal to
a power of 2, and the time saved compared with a direct calculation is roughly:
gain =
N
log2(N)
To get a better idea, if N = 1,024, the FFT is about 100 times faster than
the direct calculation based on the deﬁnition of the DFT.
1.3
Case of random signals
In the case of a random process, also referred to as a “time series”, the notion
of “spectral content” needs to be treated with caution. More speciﬁcally, the
Fourier transform of a single random process trajectory generally does not exist.
However, theoretical developments [3] lead us to deﬁne the spectrum associated
with a random process by the DTFT of the covariance function:
SXX(f) = DTFT (RXX(n)) =
+∞
∑
n=−∞
RXX(n)e−2πjnf
(1.17)
SXX(f) is called the power spectral density (psd).
A fairly simple alternative idea is to use the Fourier transform of that sin-
gle record of length N. This leads us to the deﬁnition of a periodogram. A
periodogram is the random function of f ∈(0, 1) deﬁned by:
IN(f) = 1
N

N−1
∑
n=0
X(n)e−2jπnf

2
(1.18)
Unfortunately the periodogram is not a good estimator of the spectral den-
sity [3]. However a consistent estimate can be derived by averaging or smoothing
periodograms.
The ﬁltering equations also exhibit a fundamental diﬀerence in comparison
to the deterministic case (Y (f) = H(f)X(f)). In the random case, the psds
are linked by the relation:
SY Y (f) = |H(f)|2SXX(f)
(1.19)
Note that there is a second ﬁltering equation which can be used:
SY X(f) = H(f)SXX(f)
(1.20)
where SY X(f), which is called the interspectrum, refers to the Fourier transform
of {RY X(n)}.

Recap on Digital Signal Processing
11
1.4
Example of the Dual Tone Multi-Frequency
(DTMF)
On a Dual Tone Multi-Frequency (DTMF) phone keyboard, each key is asso-
ciated with the sending of a signal. This signal is the sum of two sines the
frequencies (in Hz) of which are given in the correspondence Table 1.1.
Hz
1,209
1,336
1,477
697
1
2
3
770
4
5
6
852
7
8
9
941
⋆
0
#
Table 1.1 – Frequency correspondence table
This means that when you dial “5” on your phone, the signal x(t) =
cos(2π × 1,336 × t) + cos(2π × 770 × t) is sent through the phone line.
Comments:
– these frequencies belong to the (300 Hz - 3,400 Hz) band, the phone band
for the switched network (ﬁxed phones). The frequencies associated with
the columns are all greater than those associated with the lines. This lay-
out can help to ﬁnd the phone number using the signal. Finally, the fre-
quencies were chosen so as not to have frequency ratios equal to integers.
As we have seen, a non-linear operation can cause multiples (harmonics)
of the fundamental frequency to appear, causing some confusion;
– the keyboard is designed to always send signals for periods longer than
τ1 = 65 milliseconds. This value was chosen so that the two frequencies
contained in the signal could easily be separated. In the worst case, the
diﬀerence in frequency is ∆Fmin = 1,209 −941 = 268 Hz (corresponding
to the ⋆key), therefore τ1 needs to be such that ∆Fminτ1 ≫1. With the
values that were chosen, ∆Fminτ1 > 17;
– ﬁnally, it must be possible to tell the diﬀerence between the number C
being sent for a duration of T and the number CC being sent for the
same duration. This is why, after each key is released, a zero signal is
sent for at least 80 ms (even if you can dial faster than that!).
Exercise 1.1 (DTMF signal processing) (see p. 187)
We are going to try to ﬁnd a 10 digit phone number using the signal sent

12
Digital Signal and Image Processing using MATLAB®
by the phone. We will start by sampling the signal at a frequency of 8,000
samples per second, a speed much higher than twice the highest frequency,
that is 2 × 1,477 = 2,954 Hz.
The following program creates such a signal:
%===== genekey.m
clear
Fs=8000;
% sampling freq.
tel='0145817178'; lt=length(tel); % seq. of numbers
%===== coding table
keys='123456789*0#'; nbkeys=length(keys);
FreqB=[697 770 852 941]; FreqH=[1209 1336 1477];
Freqskeys=...
[FreqB(1) FreqH(1); FreqB(1) FreqH(2); % 1 et 2
FreqB(1) FreqH(3); FreqB(2) FreqH(1); % 3 et 4
FreqB(2) FreqH(2); FreqB(2) FreqH(3); % 5 et 6
FreqB(3) FreqH(1); FreqB(3) FreqH(2); % 7 et 8
FreqB(3) FreqH(3); FreqB(4) FreqH(1); % 9 et *
FreqB(4) FreqH(2); FreqB(4) FreqH(3)];% 0 et #
%===== constraints
tton=0.065; tsil=0.080;
% in seconds
%==== construction of the seq. of frequencies
Freqs=zeros(lt,2);
for k=1:lt
ind=find(keys==tel(k));
% test of the number
Freqs(k,:)=Freqskeys(ind,:);
% associated Freq.
end
freqs=Freqs/Fs;
% normalized freq.
%===== construction of the signal
y=zeros(100+fix(100*rand),1);
% starting with signal=0
dton=fix(1000*rand(lt,1)+tton*Fs); % number duration
dsil=fix(1000*rand(lt,1)+tsil*Fs); % silence duration
for k=1:lt
sigu=cos(2*pi*(0:dton(k))'*freqs(k,:))*ones(2,1);
y=[y;sigu;zeros(dsil(k),1)];
end
%===== some noise is added
lx=length(y); py=y'*y/lx;
SNR=30; % signal to noise ratio
pb=py*10^(-SNR/10); x=y+sqrt(pb)*randn(lx,1);
%===== plotting of the signal
tps=(0:lx-1)/Fs; plot(tps,x); grid
set(gca,'xlim',[0 (lx-1)/Fs])
In order to simulate the perturbations on an actual phone call, the program
adds noise created by sqrt(pb)*randn(L,1). SNR is the signal-to-noise ratio
(in dB) chosen equal to 30 dB. The resulting signal is shown in Figure 1.6.
We are going to ﬁnd the 10 digit number in this signal in two steps. First,
we will determine the beginning and the end of the signal’s active zones, then

Recap on Digital Signal Processing
13
0
0.5
1
1.5
2
2.5
−2
−1
0
1
2
Figure 1.6 – DTMF Signal
we will analyze each of the intervals to extract the frequencies and therefore
the corresponding digit. To determine the beginning and the end of the active
zones of the signal, we are going to estimate the “instantaneous power” and
compare it to a threshold value.
We will see later on as we study random
phenomena what we mean exactly by “estimating the instantaneous power”.
Here, we will merely be considering the quantity:
Pn = 1
N
n
∑
k=n−N+1
x2
k
(1.21)
which gives a relevant indication on the signal’s ﬂuctuations. The choice of N
is done as a compromise. Consider, for example,the signal x(n) represented in
Figure 1.6. If N is very small, Pn will be very close to the amplitude x2
n. The
risk would be to make the conclusion that the power is equal to zero whenever
the amplitude is close to 0 (which happens every period). If, on the contrary,
N is very high, we might include a silence and miss the beginning or the end
of an active part. Quantitatively, N must therefore be much greater than the
longest of the periods of the active parts, and much smaller than the duration
of the wanted signal, that is to say 65 ms. This can be expressed:
Fs
697 ≪N ≪65 × 10−3Fs
For Fs = 8,000, and with N = 100, this double inequality is satisﬁed.
1. write a program that measures the “instantaneous power” and determines
the beginning and the end of the signals associated with a digit;
2. write a program that determines the digit associated with each portion
of the signal.


Chapter 2
Additional Information
About Filtering
Mathematically speaking, using the ﬁlter with the transfer function H(z) for
ﬁltering the sequence x(n) leads to a perfectly determined result. However,
depending on the practical implementation of the ﬁlter, the results can vary
in terms of precision, speed, etc. This chapter deals with the technical aspects
of ﬁltering. If you restrict yourself to a “simulation” approach, as we have up
until now, the filter function is everything you will ever need. However, if
this ﬁltering operation has to be implemented, its eﬀectiveness requires some
additional knowledge that will be detailed in this chapter.
2.1
Filter implementation
Linear invariant ﬁlters as we consider in this section perform the convolution of
the input sequence {x(n)} with a sequence {h(n)} called the impulse response
of the ﬁlter:
y(n) = (x ⋆h)(n) =
+∞
∑
k=−∞
x(k)h(n −k) =
+∞
∑
k=−∞
x(n −k)h(k)
(2.1)
When the series {h(n)} is of ﬁnite length, we speak of a ﬁnite impulse
response ﬁlter or FIR ﬁlter; otherwise, we have an inﬁnite impulse response
ﬁlter or IIR ﬁlter.
The DTFT of {h(n)} is called the complex gain of the ﬁlter, and its modulus
is the gain.
The ZT of {h(n)} is called the transfer function.

16
Digital Signal and Image Processing using MATLAB®
2.1.1
Examples of ﬁlter structures
In this section, we will study the implementation of the ﬁltering function, in
other words its programming. Figure 2.1 shows a particular implementation
called the canonical direct form of a general recursive ﬁlter with the transfer
function:
H(z) = b0 + b1z−1 + · · · + bpz−p
1 + a1z−1 + · · · + apz−p
Choosing the same degree for both the numerator and the denominator does
not restrict us in any way; you need only consider that some of the coeﬃcients
can be equal to zero.
z−1
z−1
z−1
z−1
b1
b2
x1
x2
ap
bp
a2
a1
b0
t(n)
i(n)
o(n)
+
−
+
+
+
+
+
+
+
+
+
+
+
ap−1
bp−1
+
+
+
xp−1
x0
Figure 2.1 – Processing architecture
This “implementation” ﬁrst performs the calculation of:
t(n) = i(n) −a1t(n −1) −· · · −apt(n −p)
then the calculation of:
o(n) = b0t(n) + b1t(n −1) + · · · + bpt(n −p)
where {i(n)} and {o(n)} are the input and output sequences respectively.
For this algorithm, the vector x(n)
∆=
[x0(n)
x1(n)
. . .
xp−1(n)]T (p×
1) is called the ﬁlter state:
x(n) =
[
t(n)
t(n −1)
. . .
t(n −p + 1)
]T
Its components, referred to as state variables, are the input values of the
“delay” cells denoted z−1 in Figure 2.1. By introducing the vectors:
a = [a1
a2
. . .
ap]T and b = [b1
b2
. . .
bp]T

Additional Information About Filtering
17
we get the following expression for the algorithm:



t(n) = i(n) −aT x(n −1)
o(n) = b0t(n) + bT x(n −1)
x(n) =
[t(n)
x0(n −1)
x1(n −1)
· · ·
xp−2(n −1)]
(2.2)
The following filtrer function implements this algorithm:
function [ys,xs] = filtrer(num,den,xe,xi)
%!==========================================!
%! Filter (direct canonical structure)
!
%! SYNOPSIS: [ys,xs]=FILTRER(num,den,xe,xi) !
%!
num = [b0 b1 ... bP]
!
%!
den = [1 a1 a2 ... aP]
!
%!
xe
= input sequence
!
%!
xi
= initial state
!
%!
ys
= output sequence
!
%!
xs
= final state
!
%!==========================================!
lden=length(den); lnum=length(num);
if (lden<lnum), den(lnum)=0; lden=lnum; end
if (lnum<lden), num(lden)=0; end
ld=lden-1; N=length(xe);
av=zeros(1,ld); bv=av;
av(:)=den(2:lden); bv(:)=num(2:lden);
if (nargin==3), zzi=zeros(ld,1); end
if (nargin==4),
if length(xi)<ld, xi(ld)=0; end
zzi=zeros(ld,1); zzi(:)=xi;
end
b0=num(1); xs = zzi; ys=zeros(ld,1);
for k=1:N,
x0n=xe(k) - av * xs;
ys(k)=b0 * x0n + bv * xs;
xs=[x0n ; xs(1:ld-1)];
% new state
end
Determining the initial state leading to a given input-output sequence is
another problem altogether. Using the recursive equations (2.2) that lead to
t(n) and o(n), we can also write:


o(p −1)
i(p −1)
...
o(0)
i(0)


=


b0
b1
· · ·
bp
0
· · ·
· · ·
0
1
a1
· · ·
ap
0
· · ·
· · ·
0
0
b0
· · ·
bp−1
bp
0
· · ·
0
0
1
· · ·
...
0
· · ·
· · ·
0
b0
b1
· · ·
bp
0
· · ·
· · ·
0
1
a1
· · ·
ap




t(p −1)
...
t(−1)
...
t(−p)


= OT

18
Digital Signal and Image Processing using MATLAB®
This expression shows that the initial state T = [t(−1) . . . t(−p)]T can
be reconstructed so long as the matrix O is invertible.
The system theory
demonstrates that the possibility of reconstruction is related to the concept of
observability. A great number of observability criteria exist, based on the state
representations associated with a system [22].
The filtric function detailed hereafter carries out the reconstruction of
the state associated with the processing architecture implemented by filter:
function zi=filtric(num,den,xi,yo)
%!===========================================!
%! Initial state reconstruction for a direct !
%! canonical structure
!
%! SYNOPSIS: zi=FILTRIC(num,den,xi,yo)
!
%!
num = [b0 b1 ... bP]
!
%!
den = [1 a1 a2 ... aP]
!
%!
xi
= input sequence
!
%!
yo
= output sequence
!
%!
zi
= reconstructed initial state !
%!===========================================!
lden=length(den); lnum=length(num);
if (lden<lnum), den(lnum)=0; lden=lnum; end
if (lnum<lden), num(lden)=0; end
ld=lden-1;
numv=zeros(lden,1); denv=numv;
numv(:)=num; denv(:)=den;
lx=length(xi); ly=length(yo);
if lx<ld, xi(ld)=0; end
if ly<ld, yo(ld)=0; end
ysv=zeros(1,ld); xev=zeros(1,ld);
ysv(:)=yo(ld:-1:1); xev(:)=xi(ld:-1:1);
x=[ysv;xev];
vec=zeros(2*ld,1); vec(:)=x;
v0=[numv; zeros(ld-1,1); denv; zeros(ld,1)];
A=[]; for ii=1:ld, A=[A v0]; end
A=A(1:4*ld*ld);
Ax=zeros(2*ld,2*ld); Ax(:)=A; Ax=Ax';
zzi=inv(Ax) * vec; zi=zzi(ld+1:2*ld);
The state reconstruction function is inseparably related to the ﬁltering
function, which is itself based on a particular processing architecture.
The reconstruction function is rarely used. The state vector xs, ﬁnal state
of the ﬁrst block’s processing, is transmitted as the initial state of the second
block. The result yp is identical to the one obtained for the ﬁltering of the
entire block etot.
%===== fil2blocks.m
inp1=randn(100,1); inp2=randn(100,1); etot=[inp1;inp2];
b=[1 .3]; a=[1 -.8 .9];

Additional Information About Filtering
19
%===== global filtering
(null initial state)
y=filtrer(b,a,etot);
%===== filtering the 2 blocks
[y1 xs]=filtrer(b,a,inp1);
% null initial state
y2=filtrer(b,a,inp2,xs);
% initial state xs
yp=[y1;y2];
%===== drawing for the transition between 2 blocks
[y(90:110) yp(90:110)]
MATLAB®’s ﬁltering function, filter, uses the Transpose-Form IIR struc-
ture [28], diﬀerent from the previous one, represented in Figure 2.2.
As in
our example, filter transmits the state vector. The reconstruction function
filtic.m is available as part of the Signal Toolbox. Exercise 2.1 is a study of
this structure.
Exercise 2.1 (Filter architecture) (see p. 189)
Consider the Transpose-Form IIR structure (Figure 2.2) of a rational ﬁlter.
z−1
z−1
z−1
x1(n)
xp(n)
ap
a1
i(n)
o(n)
+
+
−
+
+
+
+
bp
bp−1
−
+
ap−1
b0
+
xp−1(n)
x1(n−1)
b1
+
+
+
−
Figure 2.2 – Transpose-Form IIR structure
1. Determine the ﬁlter’s transfer function.

20
Digital Signal and Image Processing using MATLAB®
2. By deﬁning the state x = [x1(n) . . . xp(n)] at the time n, determine the
state representation and express it as follows:
{ x(n) = Ax(n −1) + bi(n)
o(n) = cT x(n −1) + di(n)
Use this to ﬁnd the ﬁltering program. It might be useful to notice that
the matrix A is the transpose of the companion matrix (compan function)
associated with the denominator polynomial [1 a1 a2 . . . ap].
3. Find the associated reconstruction function using only the ﬁltering func-
tion. In order to do this, express xk(0) as the sum of an input ﬁltering
and an output ﬁltering.
2.1.2
Distributing the calculation load in an FIR ﬁlter
We wish to distribute the calculation load for an FIR ﬁltering algorithm among
several processors. Only two methods will be presented. The ﬁrst one consists
of distributing the number of multiplication/accumulation operations (MAC
operations) among M branches without changing the processing rate.
The
second one consists of organizing the calculation in diﬀerent units, so as to
reduce this speed, at the cost of a certain delay.
Paralleled calculations
Consider the ﬁltering equation y(n) = ∑+∞
k=−∞h(k)x(n −k). For a given M,
we deﬁne k = mM + r where r ∈{0, . . ., M −1}. We obtain:
y(n) =
M−1
∑
r=0
+∞
∑
m=−∞
h(mM + r)x(n −mM −r)
This expression shows y(n) as the sum of M terms:
r = 0
+∞
∑
m=−∞
h(mM)x(n −mM)
r = 1
+∞
∑
m=−∞
h(mM + 1)x(n −mM −1)
...
r = M −1
+∞
∑
m=−∞
h(mM + M −1)x(n −mM −M + 1)
The ﬁrst term is the ﬁltering of a sequence . . . , x(n−M), x(n), x(n+M), . . .
by the ﬁlter with the impulse response h(0), h(M), . . . The next terms cor-
respond to translated sequences ﬁltered by the ﬁlters hr(m) = {h(r), . . .,

Additional Information About Filtering
21
h(r + mM), . . . }. The ﬁlter hr(m) is called the r-th M-polyphase component
of h(n).
Figure 2.3 illustrates this processing method.
h(1)
x(n)
x(n−1)
x(n−2)
x(n−3)
x(n−4)
x(n−5)
x(n−6)
h(0)
h(3)
h(6)
h(2)
h(4)
h(5)
+
+
+
{y(n)}
M=3
Figure 2.3 – A representation of the paralleled process
Exercise 2.2 (Parallel implementation of the FIR ﬁltering) (see p.
192)
Write a program designed to simulate the process described by Figure 2.3.
Choose M = 4 and a low-pass, (−0.3, +0.3) band FIR ﬁlter with 25 coeﬃcients.
The result will be compared to the one obtained through direct ﬁltering.
This method for paralleling does not reduce the processing speed in inter-
mediate ﬁlters. Only the number of multiplications per ﬁlter is reduced.
2.1.3
FIR block ﬁltering
Let us again consider the FIR ﬁltering equation y(k) = h(0)x(k) + h(1)x(k −
1) + · · · + h(P)x(k −P). Let:
y
=


y(nN)
y(nN −1)
...
y(nN −(N −1))

= X ×


h(0)
h(1)
...
h(P)


with:
X =


x(nN)
x(nN −1)
· · ·
x(nN −P)
x(nN −1)
x(nN −2)
· · ·
x(nN −1 −P)
...
x(nN −(N −1))
x(nN −N)
· · ·
x(nN −(N −1) −P)



22
Digital Signal and Image Processing using MATLAB®
By organizing the inputs modulo M, we obtain:
y =


[x(nN)
x(nN −M)
· · ·
]
[x(nN −1)
x(nN −1 −M)
· · ·
]
...
[x(nN −(N −1))
x(nN −(N −1) −M)
· · ·
]
. . .
. . .
[x(nN −1)
x(nN −1 −M)
· · ·
]
· · ·
[x(nN −2)
x(nN −2 −M)
· · ·
]
· · ·
...
[x(nN −N)
x(nN −N −M)
· · ·
]
· · ·

×




h(0)
h(M)
...




h(1)
h(M + 1)
...


...


By restricting ourselves to the case M = N = 2, the previous expression
can be written:
[
y(2n)
y(2n −1)
]
=
[
[x(2n)
x(2n −2)
· · ·
]
[x(2n −1)
x(2n −3)
· · ·
]
[x(2n −1)
x(2n −3)
· · ·
]
[x(2n −2)
x(2n −4)
· · ·
]
]




h(0)
h(2)
...




h(1)
h(3)
...




If we assume x0(n) = [x(2n), x(2n−2), . . . ]T and x1(n) = [x(2n−1), x(2n−
3), . . . ]T , we can also write:
[
y(2n)
y(2n −1)
]
=
[x0(n)
x1(n)
x1(n)
x0(n −1)
]
×
[h0
h1
]
where h0 = [h(0), h(2), . . . ]T and h1 = [h(1), h(3), . . . ]T . If we develop y(2n)
and y(2n −1), we obtain:
y(2n)
=
x0(n)h0 + x1(n)h1
=
x1(n)(h0 + h1) + (x0(n) −x1(n))h0
y(2n −1)
=
x1(n)h0 + x0(n −1)h1
=
x1(n)(h0 + h1) + (x0(n −1) −x1(n))h1
Therefore, the calculation of the two terms y(2n) and y(2n −1) requires
the calculation of a total of four terms. However, one of them, x1(n)(h0 + h1),

Additional Information About Filtering
23
appears twice. If P refers to the length of the ﬁlter h, the lengths of h0 and
h1 are at the most equal to P/2. Hence the three terms of the calculation of
y(2n) and y(2n −1) correspond to P/2 length ﬁltering. Figure 2.4 illustrates
all these calculations.
{x(n)}
{x(2n)}
2
z−1
z−1
2
2
{x(2n−1)}
{x(2n−2)}
H0+H1
H1
H0
z−1
2
2
{y(n)}
+
−
−
+
+
+
+
+
Figure 2.4 – Block ﬁltering: the case where M = N = 2
To sum up, in order to calculate y(2n) and y(2n −1), the number of MAC
operations is roughly 3 × P/2. This value should be compared to the 2 × P
MAC operations of the direct calculations. You may, as an exercise, simulate
the process described in Figure 2.4. There is more than one method organizing
the process. Consider for example:
y(2n)
=
x0(n)h0 + x1(n)h1
=
(x0(n) + x1(n))(h0 + h1) −x0(n)h1 −x1(n)h0
y(2n −1)
=
x1(n)h0 + x0(n −1)h1
Notice that x0(n −1)h1 was calculated previously. Hence there are indeed
only three MAC operations at this stage of the calculation. We can also consider
parallel block processing for values of M and N diﬀerent from 2.
2.1.4
FFT ﬁltering
A possibility for accelerating ﬁltering operations is to work in the frequency
domain, using Fourier transforms to take advantage of the FFT algorithm’s
speed. Unfortunately, this is not as simple as it seems, because linear ﬁltering
uses a linear convolution:
y(n) = ∑+∞
m=−∞x(m)h(n −m)
(2.3)
the DTFT of which is H(f)X(f), whereas the product of the DFTs is the DFT
of the circular convolution. As a reminder, here is its expression (2.4):
∑N−1
m=0 x(m)h((n −m) mod N)
(2.4)

24
Digital Signal and Image Processing using MATLAB®
A simple calculation shows that expressions (2.3) and (2.4) lead to com-
pletely diﬀerent results. Consider a ﬁnite impulse response ﬁlter {hN(n)} and
a sequence {x(n)}. The output value at the time n is:
y(n) = hN(0)x(n) + · · · + hN(N −1)x(n −N + 1)
(2.5)
For a clearer picture, let us assume N = 8.
We are going to calcu-
late the terms resulting from a circular convolution of the length 8 block
{x(n), . . . , x(n −7)} with a ﬁlter with the coeﬃcients {h(0), . . . , h(7)}. The
following table describes the operations modulo 8.
m
0
1
2
3
4
5
6
7
hm
h0
h1
h2
h3
h4
h5
h6
h7
x
xn−7
xn−6
xn−5
xn−4
xn−3
xn−2
xn−1
xn
h−m
mod
8
h0
h7
h6
h5
h4
h3
h2
h1
h1−m
mod
8
h1
h0
h7
h6
h5
h4
h3
h2
...
h7−m
mod
8
h7
h6
h5
h4
h3
h2
h1
h0
Notice that among the 8 results of the circular convolution, only the last
one, h0xn + h1xn−1 + · · · + h7xn−7, corresponds to one of the terms from the
linear convolution, making this approach completely hopeless. This is actually
downright wrong, as we are going to see now.
The overlap-save algorithm
Consider an N = 5 length ﬁlter with its coeﬃcients h(0), . . ., h(4) completed
by three zeros. Let {x(n), . . ., x(n −7)} be the L = 8 length input block. As
we did before, we can build the sequence of the 8 output values by using the
following table:
m
0
1
2
3
4
5
6
7
hm
h0
h1
h2
h3
h4
0
0
0
x
xn−7
xn−6
xn−5
xn−4
xn−3
xn−2
xn−1
xn
h−m
mod
8
h0
0
0
0
h4
h3
h2
h1
h1−m
mod
8
h1
h0
0
0
0
h4
h3
h2
...
h7−m
mod
8
0
0
0
h4
h3
h2
h1
h0

Additional Information About Filtering
25
The resulting circular convolution outputs are:

yc(0) = h0xn−7 + h4xn−3 + h3xn−2 + h2xn−1 + h1xn
yc(1) = h1xn−7 + h0xn−6 + h4xn−2 + h3xn−1 + h2xn
yc(2) = h2xn−7 + h1xn−6 + h0xn−5 + h4xn−1 + h3xn
yc(3) = h3xn−7 + h2xn−6 + h1xn−5 + h0xn−4 + h4xn
yc(4) = h4xn−7 + h3xn−6 + h2xn−5 + h1xn−4 + h0xn−3
yc(5) = h4xn−6 + h3xn−5 + h2xn−4 + h1xn−3 + h0xn−2
yc(6) = h4xn−5 + h3xn−4 + h2xn−3 + h1xn−2 + h0xn−1
yc(7) = h4xn−4 + h3xn−3 + h2xn−2 + h1xn−1 + h0xn
Notice that the four last expressions correspond to terms found with the
linear convolution:

yc(4) = y(n −3)
yc(5) = y(n −2)
yc(6) = y(n −1)
yc(7) = y(n)
In order to calculate the next four values, we have to choose {x(n + 4), . . .,
x(n −3)} as our input block. This block partly overlaps the previous one (see
Figure 2.5), hence the word overlap in the name “overlap-save algorithm”.
x(n)
x(n−L+1)
N−1 erroneous values
L
N
h(0)
h(N−1)
Block p
Block  p+1
Save p
Save p+1
Figure 2.5 – Overlap-save algorithm
All of the operations can be summed up as follows:

26
Digital Signal and Image Processing using MATLAB®
Overlap-save
1. calculation (performed only once) of the DFT of the L length se-
quence h(n) completed by (L −N) zeros. L (the DFT’s length) is
usually a power of 2;
2. calculation of the DFT of an L length block extracted from the
input data with an overlap of the (N −1) last values of the previous
block;
3. term-by-term multiplication of the two DFTs, followed by an
IDFT;
4. the (L−N +1) terms corresponding to the linear convolution (2.3)
are saved.
Let us compare the number of operations for the overlap-save algorithm
with that of a direct calculation. The direct calculation of one convolution
point for an N length impulse response requires a loop comprising N MAC
operations.
Using the overlap-save algorithm, the impulse response’s DFT is calculated
in advance. There are two L length FFTs left for each step (one direct, one
inverse) and L complex multiplications, in all a calculation load of roughly
2 × L log2(L) + L = 2L log2(L
√
2) MAC operations. This calculation provides
us with a block of (L−N +1) convolution points, equivalent to a load of about
2L log2(L)/(L −N + 1) MAC operations per calculation point. Hence the gain
is roughly:
G(N, L) = (L −N + 1)N
2L log2(L
√
2)
Thus, for N = L/2 and N ≥32, the FFT technique is quicker.
Other parameters have to be considered. The FFT calculation implies the
use of array pointers, which cause a considerable increase in the calculation
time. The FFT also requires memory space to save the data arrays that are
too large for the ﬁlter’s memory. This is why convolution calculations that
use the FFT are usually undertaken only with ﬁlters with a length of more
than a hundred coeﬃcients. In acoustics, impulse response of a quarter-second
sampled at 8,000 Hz lead to lengths of 2,000 samples. You also have to add
to that the delay caused by block processing, a delay roughly equal the block’s
length. For some applications, this delay is reason enough to discard these
techniques.

Additional Information About Filtering
27
Overlap-add algorithm
Consider once again the previous example of a ﬁlter {h(0), . . . , h(4)}. We still
hope to obtain the output:
y(n) = h0xn + h1xn−1 + h2xn−2 + h3xn−3 + h4xn−4
Consider the convolutions concerning two consecutive length 8 blocks la-
belled xp and xp+1.
m
0
1
2
3
4
5
6
7
hm
h0
h1
h2
h3
h4
0
0
0
xp
xn−3
xn−2
xn−1
xn
0
0
0
0
xp+1
xn+1
xn+2
xn+3
xn+4
0
0
0
0
h−m
mod
8
h0
0
0
0
h4
h3
h2
h1
h1−m
mod
8
h1
h0
0
0
0
h4
h3
h2
...
h7−m
mod
8
0
0
0
h4
h3
h2
h1
h0
The values obtained from the ﬁrst block are:
yc,p(0) = h0xn−3
yc,p(1) = h1xn−3 + h0xn−2
yc,p(2) = h2xn−3 + h1xn−2 + h0xn−1
yc,p(3) = h3xn−3 + h2xn−2 + h1xn−1 + h0xn
yc,p(4) = h4xn−3 + h3xn−2 + h2xn−1 + h1xn
yc,p(5) = h4xn−2 + h3xn−1 + h2xn
yc,p(6) = h4xn−1 + h3xn
yc,p(7) = h4xn
and the values obtained from the next block are:
yc,p+1(0) = h0xn+1
yc,p+1(1) = h1xn+1 + h0xn+2
yc,p+1(2) = h2xn+1 + h1xn+2 + h0xn+3
yc,p+1(3) = h3xn+1 + h2xn+2 + h1xn+3 + h0xn+4
yc,p+1(4) = h4xn+1 + h3xn+2 + h2xn+3 + h1xn+4
yc,p+1(5) = h4xn+2 + h3xn+3 + h2xn+4
yc,p+1(6) = h4xn+3 + h3xn+4
yc,p+1(7) = h4xn+4

28
Digital Signal and Image Processing using MATLAB®
As you can see:
y(n + 1) = yc,p(4) + yc,p+1(0)
y(n + 2) = yc,p(5) + yc,p+1(1)
y(n + 3) = yc,p(6) + yc,p+1(2)
y(n + 4) = yc,p(7) + yc,p+1(3)
The conclusion is that if the input block is completed with N −1 = 4 zeros,
the circular convolution will calculate incomplete sums. These sums will then
be completed with values obtained from the next block translated by N −1
values. The sequence of operations can be summed up in the following way:
Overlap-add
1. calculation of the DFT of the N length sequence h(n) completed
with (L −N) zeros. Usually the length L of the DFT is 2P ;
2. calculation of the DFT of a length (L−N +1) block extracted from
the input data without any overlap and completed with (N −1)
zeros;
3. term-by-term multiplication of the two DFTs, followed by an
IDFT;
4. sum of the current block and of the next block with an overlap of
(N −1) values.
You can check that the overlap algorithm leads to basically the same cal-
culation load as the overlap-save algorithm.
To sum everything up, the overlap-save performs an overlap on the inputs
then delivers the result, whereas the overlap-add technique performs an overlap
not on the input but on the output (see Figure 2.6).
Exercise 2.3 (FFT ﬁltering) (see p. 192)
Let x(n) be a signal such that x(n) = sin(2πf0n)+sin(2πf1n), where f0 = 0.15
and f1 = 0.3 and let h(n) be the following impulse response ﬁlter:
h(n)=[0.0002
0.0134
0.0689
0.1676
0.2498
0.2498
0.1676
0.0689
0.0134 0.0002].
1. Normalize the ﬁlter’s coeﬃcients so as to have the gain at the frequency
0 equal to 1.
2. Display on the same graph the original signal and the ﬁltered signal ob-
tained with the filter function.
3. Display the ﬁlter’s complex gain and the spectra of the original signal
and of the ﬁltered signal.

Additional Information About Filtering
29
x(n)
x(n−L+N)
L
N
h(0)
h(N−1)
Block p
Block p+1
Sum p
Sum p+1
0
0
N−1 null values
0
0
Figure 2.6 – Overlap-add algorithm
4. Perform the ﬁltering using an FFT on the entire signal.
5. Perform the same process with length 32 blocks. Notice that this requires
an overlap of consecutive blocks.
2.2
Filter banks
The idea of using several parallel ﬁlters to “simultaneously” analyze several
frequency bands is very old. That is how some analog spectrum analyzers work.
Several ﬁlters, forming what is called a “ﬁlter bank”, with slightly overlapping
frequency responses, cover the entire extent of the frequency band we wish to
analyze. The short term Fourier transform time-frequency analysis is another
example.
Signal spectrum analysis is not the only application of ﬁlter banks. For the
purpose of processing improvements, we can imagine performing operations on
signals coming from diﬀerent ﬁlters. This is what is represented in Figure 2.7.
There are two advantages to this method: on the one hand, the calculations
are parallel, and on the other hand, the processes can be adapted to the vari-
ous channels. Among the main applications of subband ﬁltering techniques, a
few are worth mentioning, such as subband coding, multicarrier modulations,
analog-to-digital conversion (Σ-∆), etc. In this section, we will only present
some results concerning ﬁlter banks that can be encountered when dealing with
processing architectures.

30
Digital Signal and Image Processing using MATLAB®
M
M
M
x(n)
G0(z)
Gm(z)
GM−1(z)
M
M
M
H0(z)
Hm(z)
HM−1(z)
Sub-band
processing
z(n)
analysis
bank
synthesis
bank
zm(n)
xm(n)
Figure 2.7 – How ﬁlter banks work
In digital processing, the fact that each channel operates on narrower fre-
quency bands allows the possibility, according to the sampling theorem, of
reducing the sampling rate at the ﬁlter’s output, as it is shown in Figure 2.7.
We end up with a system for which diﬀerent processing frequencies are used
simultaneously in diﬀerent points of the calculation chain. The oversampling
and undersampling operations are examples. Two operations form the basis of
these techniques: decimation and expansion.
2.2.1
Decimation and expansion
Decimation: an operation that takes one out of every M samples. Symboli-
cally, it is represented by an arrow pointing down (Figure 2.8). We have:
X↓M(z) = 1
M
M−1
∑
k=0
X(z1/MW k
M) where WM = exp(−2jπ/M) (2.6)
Expansion: an operation that inserts M −1 zeros between two samples of the
original sequence. Symbolically, it is represented by an arrow pointing
up (Figure 2.9). We have:
X↑M(z) = X(zM)
(2.7)
Property 2.1 (Filtering and decimation)
We have the property illustrated by Figure 2.8.
Hints: on the right-hand side of Figure 2.8, we have:
Y (z) = U↓M(z) = 1
M
M−1
∑
n=0
U(z1/MW n
M)

Additional Information About Filtering
31
M
H(z)
x(n)
y(n)
H(zM)
x(n)
y(n)
M
u(n)
Figure 2.8 – Equivalence implicating a ﬁltering and a decimation
By choosing U(z) = H(zM)X(z), we obtain:
Y (z)
=
1
M
M−1
∑
n=0
X(z1/MW −n
M )H[(z1/MW −n
M )M]
=
M−1
∑
n=0
X(z1/MW −n
M )H(z) = H(z)X↓M(z)
(2.8)
which corresponds to the expression for the process on the left-hand
side of Figure 2.8.
Property 2.2 (Filtering and expansion)
We have the properties illustrated by Figure 2.9.
M
H(z)
x(n)
y(n)
x(n)
y(n)
M
H(zM)
u(n)
Figure 2.9 – Equivalence implicating a ﬁltering and an expansion
Hints: starting oﬀwith the diagram on the left of Figure 2.9, and
with U(z) = H(z)X(z), we have:
Y (z) = U(zM) = H(zM)X(zM) = H(zM)X↑M(z)
(2.9)
corresponding to the expression for the process on the right of
Figure 2.9.
Application: the comb decimation ﬁlter
Consider the ﬁlter represented in Figure 2.10. It is composed of the ﬁlter with
the transfer function 1/(1 −z−1) cascaded with the ﬁlter with the transfer
function (1 −z−M).
Hence its transfer function has the expression:
Hz(z) =
1
1 −z−1 × (1 −z−M)

32
Digital Signal and Image Processing using MATLAB®
−
z−1
x(n)
z−1
z−1
M
+
+
z−1
y(n)
+
Figure 2.10 – Comb ﬁlter composed of the ﬁlter 1/(1 −z−1) followed by the ﬁlter
(1 −z−M)
A simpliﬁcation leads to Hz(z) = 1+z−1 +· · ·+z−(M−1). This is therefore
an FIR ﬁlter with the impulse response h(n) = 1 for n ∈{0, . . ., (M −1)} and
0 otherwise. Notice that 1/(1 −z−1) has a pole in z = 1 (zero frequency) and
that (1 −z−M) has M zeros placed on the unit circle in Wm = e2jπm/M where
m ∈{0, . . ., (M −1)}.
Because of the location of the zeros of (1 −z−M), regularly spread out on
the unit circle, the ﬁlter is called a comb ﬁlter. After a simpliﬁcation, Hz(z)
has (M −1) zeros in e2jπm/M where m ∈{1, . . ., (M −1)}, and no poles.
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
12
14
16
18
m=3
Figure 2.11 – Frequency response of a comb ﬁlter for M = 16
The frequency response of the ﬁlter Hz(z) is represented in Figure 2.11 for
M = 16. It has a main lobe centered at 0 with a width of 2/M. It is therefore
a low-pass ﬁlter. By placing the ﬁrst cell’s pole on another of the second cell’s
zeros, we can obtain a low-pass ﬁlter (see exercise (2.4)).
Exercise 2.4 (Band-pass ﬁlter based on a comb ﬁlter) (see p. 193)
In a method similar to the one used for the low-pass comb ﬁlter in Figure 2.10,
design a real band-pass ﬁlter centered at the frequency fm = m/M.

Additional Information About Filtering
33
Although the ﬁlter in Figure 2.11 is, after simpliﬁcation, an FIR ﬁlter that
can therefore be achieved by a stable structure, the diagram in Figure 2.10 is
unstable because the ﬁrst cell can produce an unbounded output even if the
input is bounded. For systems that use comb ﬁlters (see Figure 2.14), it is
ensured that this never happens. We will now see an application of the comb
ﬁlter.
The comb ﬁlter represented in Figure 2.10 can be used for designing a
low-pass ﬁlter (but not a very selective one) in the undersampling operation
by placing it before an M order decimator.
The result is the ﬁrst system
represented in Figure 2.12.
1−z−1
1−z−M
1
M
1−z−1
1−z−1
1
M
decimation
prefiltering
{
Figure 2.12 – Permutation of the decimator and the derivative ﬁlter
According to property 2.1, the ﬁltering and decimation operations can be
performed in any order. We end up with the second system in Figure 2.12.
In this system I(z) = 1/(1 −z−1) performs the operation that associates the
output u(n) with the input x(n) as follows:
u(n) = u(n −1) + x(n)
which is an accumulation/integration and D(z) = 1 −z−1 performs the opera-
tion that associates the output y(n) with the input v(n) as follows:
y(n) = v(n) −v(n −1)
which can be seen as the approximation of a derivative ﬁlter.
The selectivity can be increased by cascading several integrators and several
derivative ﬁlters, as is done on certain audio CD players using what is called
the one-bit stream technique. Figure 2.13 explains how this works.
Starting oﬀwith a binary ﬂux at 6.4 MHz, we accumulate input values, 0
or 1, then we decimate by a factor of 4. We obtain a ﬂux of values represented
on 20 bits and sampled at 400 kHz. As we have already said, the integrator
cascade is, by nature, unstable.
However, it can be shown that if we use
a modulo M summer, the “integrator, decimator, derivative ﬁlter” set does
not cause any overﬂow, so long as the summer contains M bits. Because the
cascade is comprised of four of these systems, an M + 4 = 20 bits summer is

34
Digital Signal and Image Processing using MATLAB®
(1−z−1)4
(1−z−1)4
1
M
6,4 MHz
1 bit
M=16
400
kHz
4
100 kHz
20 bits
Figure 2.13 – Undersampling ﬁlter
used. Finally, the calculations performed by the fourth band output ﬁlter are
processed with 38 bits.
2.2.2
Filter banks
An analysis ﬁlter bank is a group of parallel digital ﬁlters, the input signals of
which are x(n), that cuts up the frequency band in K subbands. The synthesis
ﬁlter bank is a group of K ﬁlters placed after the the analysis ﬁlter bank and
generating the signal ˆx(n).
The processing system can be represented by a group of ﬁlters connected
by undersampling and expansion operators as shown in Figure 2.14.
M
M
M
x(n)
G0(z)
Gm(z)
GM−1(z)
M
M
M
H0(z)
Hm(z)
HM−1(z)
Sub-band
processing
z(n)
analysis
bank
synthesis
bank
zm(n)
xm(n)
Figure 2.14 – How the ﬁlter bank works
We now reconsider the problem of perfect reconstruction: is there a ﬁlter
bank such that the aliasing eﬀects for each band compensate each other exactly
on the entire band? This question is justiﬁed by the fact that if no processing
is done, the least that can be expected of this structure is to produce an output
signal identical to the input signal. To make the rest of this discussion simpler,
we will consider the case where M = 2 according to Figure 2.15, a case of
important practical use.

Additional Information About Filtering
35
2
2
G0(z)
G1(z)
2
2
H0(z)
H1(z)
z1(n)
x1(n)
x0(n)
z0(n)
y1(n)
y0(n)
Figure 2.15 – Two channel ﬁlter bank
Obvious solutions
Note that the perfect reconstruction problem has at least two obvious solutions.
The ﬁrst one consists of taking two ideal low-pass ﬁlters in the (0, 1/4) band
for G0(z) H0(z), and two ideal high-pass ﬁlters in the (1/4, 1/2) band for G1(z)
H1(z).
The second solution simply consists of choosing G0(z) = H1(z) = 1 and
G1(z) = H0(z) = z−1. In this case, the analysis ﬁlter bank is merely a de-
multiplexer distributing the even index values of x(n) to one channel and the
odd index values to the other channel, while the synthesis ﬁlter bank is a mul-
tiplexer that interlaces the two channels. This solution uses FIR ﬁlters (with
only one coeﬃcient!). However, these ﬁlters are unfortunately band-pass ﬁlters
with a gain of 1 without any frequency selectivity.
One of the major problems we are faced with when using ﬁlter banks is
the diﬃculty of ﬁnding a solution that has a ﬁnite impulse response, a good
selectivity and the ability to perform perfect reconstruction, all at once.
Perfect reconstruction equations
By referring to the diagram in Figure 2.15, and by using formulae (2.6) and
(2.7), we can write:
Z0(z) = (G0X)↓2↑2
=
[G0(z)X(z) + G0(−z)X(−z)]/2
Z1(z) = (G1X)↓2↑2
=
[G1(z)X(z) + G1(−z)X(−z)]/2
and:
Y0(z)
=
H0(z)[G0(z)X(z) + G0(−z)X(−z)]/2
Y1(z)
=
H1(z)[G1(z)X(z) + G1(−z)X(−z)]/2
This leads us to the reconstructed sequence ˆx(n) by b
X(z) = Y0(z) + Y1(z)
and therefore:
2 b
X(z)
=
[G0(z)H0(z) + G1(z)H1(z)] X(z) +
[G0(−z)H0(z) + G1(−z)H1(z)] X(−z)
=
T(z)X(z) + A(z)X(−z)

36
Digital Signal and Image Processing using MATLAB®
Perfect reconstruction is ensured when b
X(z) = z−rX(z). This gives us the
following two conditions:
T(z) = G0(z)H0(z) + G1(z)H1(z) = 2z−r
(2.10)
A(z) = G0(−z)H0(z) + G1(−z)H1(z) = 0
(2.11)
The ﬁrst condition ((2.10)) expresses the absence of distortion, due to the
fact that the transfer function T(z) has a gain equal to 1 and a linear phase. The
second one ((2.11)) ensures that the term X(−z), characterizing the spectrum
aliasing, is zeroed out.
Quadrature ﬁlters
A ﬁrst solution consists of imposing:
H0(z) = G1(−z) and H1(z) = −G0(−z)
(2.12)
which ensures condition (2.11). Condition (2.10) then becomes:
H0(z)G0(z) −G0(−z)H0(−z) = 2z−r
(2.13)
If H0(z) and G0(z) are two polynomials in z−1, equation (2.13) can be
expressed α(z) −α(−z) = 2z−r, where we have deﬁned α(z) = H0(z)G0(z).
In α(z) −α(−z), only the odd degree coeﬃcients of α(z) remain. Therefore,
all the odd degree coeﬃcients of α(z) must be equal to 0, except for the r-th
degree coeﬃcient. This implies, incidentally, that r is odd. We then have to
factorize α(z) = H0(z)G0(z). Because the two constraints (2.12) are supposed
to be obeyed at all times, two simple solutions arise:
– we impose G0(z) = G1(−z). If we change over to the DTFTs, we get
G0(e2jπf) = G1(e2jπ(f−1/2)). Because the ﬁlters are real, this expres-
sion implies that the frequency responses of the ﬁlters have a “mir-
ror” symmetry about the frequency 1/4.
This is called a QMF ﬁlter
bank, short for Quadrature Mirror Filters.
Unfortunately, there are
very few solutions, and they are not selective.
In order to show this,
we replace G0(z) = G1(−z) in the ﬁrst expression of (2.12), meaning
H0(z) = G1(−z), and we get G0(z) = H0(z). Replacing it in (2.13) leads
us to:
H2
0(z) −H2
0(−z) = 2z−r
For this solution to be satisﬁed, H0(z) cannot have more than two non-
zero coeﬃcients, in other words H0(z) = h0z−k0 + h1z−k1. If we identify
the terms, we obtain:
4h0h1z−(k0+k1) = 2z−r

Additional Information About Filtering
37
and therefore k0 and k1 can have any value so long as the sum is odd, for
example, k0 = 0 and k1 = 1, and h0h1 = 1/2. By imposing that the ﬁlters
be linear-phase ﬁlters, and therefore h0 = h1, we obtain h0 = h1 = 1/
√
2.
This result is not satisfactory because the obtained ﬁlters are very poorly
selective. Thus, the frequency response of H0(z), for k0 = 0 and k1 = 1,
is |H0(e2jπf)|2 = cos2(πf)/2;
– the
condition
G1(z)
=
G0(−z)
is
now
replaced
by
G1(z)
=
(−z)−NG0(−z−1) or g1(n) = (−1)ng0(N −n), which is equivalent. This
is called a CQF ﬁlter bank, short for Conjugate Quadrature Filters. By
replacing G1(z) = (−z)−NG0(−z−1) in (2.12), that is to say H0(z) =
G1(−z), we have H0(z) = (−z)−NG0(z−1). Replacing it in (2.13) leads
us to:
z−N (
G0(z)G0(z−1) + G0(−z)G0(−z−1)
)
and a suﬃcient condition on the phase is provided by:
G0(z)G0(z−1) + G0(−z)G0(−z−1) = 1
The transfer function D(z) = G0(z)G0(z−1) is sometimes called a zero-
phase half-band. Because G0(e2jπf) satisﬁes |G0(e2jπf)|2 + |G0(e2jπ(f−1/2)|2 =
1, G0 is said to be “power symmetric”. Searching for a solution can be summed
up as follows:
Steps:
1. Find an odd order, “power symmetric” ﬁlter D(z), approximately
half-band.
In order to do this, we can start with the window method (using
a triangular window for which positivity is ensured), or with an
iterative method such as the Parks-McClellan algorithm [2] (the
problem is that there is no guarantee that the phase will be linear).
2. Perform a spectral decomposition of D(z) in G0(z)G0(z−1).
3. Construct the ﬁlter bank using G1(z) = (−z)−NG0(−z−1), then:
H0(z) = G1(−z) H1(z) = −G0(−z)

38
Digital Signal and Image Processing using MATLAB®
Orthogonal ﬁlters
We will again use equations (2.10) and (2.11), written below:
{G0(z)H0(z) + G1(z)H1(z) = 2z−r
G0(−z)H0(z) + G1(−z)H1(z) = 0
and solve this linear system in order to determine the expressions of H0(z) and
H1(z) as an expression of G0(z) and G1(z). We obtain:
H0(z) =
2z−rG1(−z)
G0(z)G1(−z) −G1(z)G0(−z)
(2.14)
H1(z) = −
2z−rG0(−z)
G0(z)G1(−z) −G1(z)G0(−z)
Property 2.3 For the two channel ﬁlter bank, the perfect reconstruction prop-
erty is obtained if and only if:







∑
k g0(k)h0(2n −k) = δ(2n −r)
∑
k g1(k)h1(2n −k) = δ(2n −r)
∑
k g1(k)h0(2n −k) = 0
Hints: let P(z) = H0(z)G0(z). Using (2.14), we have:
P(z) = 2z−rG1(−z)G0(z)/D(z)
where D(z) refers to the denominator of H0(z) in (2.14). Likewise:
H1(z)G1(z) = −2z−rG0(−z)G1(z)/D(z)
Because D(z) = −D(−z), we have H1(z)G1(z) = P(−z), and we
can write:
P(z) + P(−z) = 2z−r
This condition implies that r is even, and that p(2n) = δ(2n −
r). By noticing that P(z) = H0(z)G0(z) is the z-transform of the
convolution product of h0(n) with g0(n), we obtain:
∑
k
g0(k)h0(2n −k) = δ(2n −r)
Now let Q0(z) = H1(z)G0(z):
Q0(z) = −2z−rG0(z)G0(−z)/D(z)

Additional Information About Filtering
39
Because r is even, Q0(z) is odd. And hence q0(2n) = 0. By noticing
that Q0(z) = H1(z)G0(z) is the z-transform of the convolution of
h1(n) with g0(n), we get ∑
k g1(k)h0(2n −k) = 0.
The sequences g0(n) g1(n) on one hand, and h0(n) and h1(n) on the other,
lead to the deﬁnition of two sets of orthogonal sequences. Let:
ϕ2n(k) = g0(2n −k),
ϕ2n+1(k) = g1(2n −k)
ψ2n(k) = h0(k −2n),
ψ2n+1(k) = h1(k −2n)
Property (2.3) shows that the two sequences {ϕn(k)} and {ψn(k)} verify
for any n ̸= n′:
∑
k
ϕn(k)ψn′(k) = 0
The two sets {ϕ(n)} and {ψ(n)} are said to have the bi-orthogonality
property. This is the equivalent for inﬁnite dimension of the property of two
matrices such that ΨT Φ = diag(d1, . . . , dK) where diag(d1, . . . , dK) is a diago-
nal matrix, the identity being a particular case.
We will now discuss the orthogonal case [40], where the sequences ϕn(k)
and ψn(k) coincide, that is where one is equal to the other translated.
A
suﬃcient condition is to have h0(n) = g0(r −n) and h1(n) = g1(r −n). Hence
perfect reconstruction and orthogonality require the impulse responses of the
synthesis ﬁlters to be reversed copies of the impulse responses of the analysis
ﬁlters. Changing over to the z-transforms, this leads to:
Hi(z) = z−rGi(1/z) where i = {0, 1}
(2.15)
This means, ﬁrst of all, that P(z), deﬁned by P(z) = H0(z)G0(z) can be
written:
P(z) = z−rG0(1/z)G0(z)
This relation implies that if z0 is a root of P(z), then 1/z0 is also a root of
P(z). Hence, the roots of P(z) are pairs of inverse values, one inside and one
outside the unit circle.
By replacing (2.15) in the second equation of (2.14), we get:
G0(−1/z)G0(z) + G1(−1/z)G1(z) = 0
If the polynomials G0(z) and G1(z) share the same ﬁnite degree (FIR ﬁlters
of the same length) and are diﬀerent from one another, then the roots of G1(z)
have to be roots of G0(−1/z). Therefore, G1(z) = −z2K−1G0(−1/z). This
relation can be expressed, in the temporal domain, as:
g1(n) = (−1)ng0(2K −1 −n)

40
Digital Signal and Image Processing using MATLAB®
To sum up, calculating analysis ﬁlter banks using orthogonal ﬁlters is
achieved using the following method: starting oﬀwith P(z) = G0(z)G0(1/z)
which veriﬁes P(z) + P(−z) = 2:
– we associate with G0(z) the roots of P(z) that are inside the unit circle,
then we calculate g0(n);
– we calculate h0(n) = g0(−n);
– we calculate g1(n) = (−1)ng0(2K −1 −n);
– we calculate h1(n) = g1(−n).
We still have to ﬁnd a function P(z) = G0(z)G0(1/z) such that P(z) +
P(−z) = 2.
A ﬁrst crude method consists of imposing the relation P(z) + P(−z) = 2
by choosing a sequence p(n) such that p(2n) = 0 in the following manner: we
start oﬀwith an even sequence wn, for example, the one obtained by the FIR
ﬁlter design method (window method, Parks-McClellan method), and all the
even index terms are replaced by zero, except for the zero index term. This
can be expressed as follows:
p(n) = wncn
where c2n = δ(n). However, this does not guarantee that P(z) can be expressed
as G0(z)G0(1/z), or that P(e2jπf) is positive, which is equivalent. We can
then determine the sequence cn such that P(e2jπf) > 0. As a consequence, the
relation P(z) + P(−z) = 2 is not quite true anymore, and becomes even less
true as the minimum negative value of the DTFT of wn becomes smaller.
Let us now see an important example related to the Daubechies wavelets.
We start with a polynomial P(z), such that it is at the frequency 1/2. As a
consequence, this introduces in the sequence p(n) a kind of regularity similar
to the signal smoothing property when the energy of the high frequencies is
reduced, hence the idea to place a great number of zeros in z = −1. For this
we assume:
P(z) = (1 + z)k(1 + z−1)kR(z)
where R(z) can be expressed as R1(z)R1(1/z). R(z) is therefore a symmetrical
polynomial for which the degrees of its terms vary from −s to +s. Therefore,
P(z) has 2k + 2s roots and is dependent on 2k + 2s + 1 coeﬃcients, (k + s) of
which have to be equal to zero (p(2n) = 2δ(k)). This leads to (k+s) equations.
Yet we have 2s + 1 linearly independent coeﬃcients in R(z). This means we
have to set k + s = 2s + 1, or s = k −1. Thus, for k = 2, we get s = 1. Hence
the length of the ﬁlter G0(z) is 4. Generally speaking, this method leads to
FIR ﬁlters with lengths of L = 2k.

Additional Information About Filtering
41
Let us calculate the coeﬃcients for k = 2.
For this we assume R(z) =
(αz + β + αz−1). The expression of the condition P(z) + P(−z) = 2 will give
us two equations with two unknowns α and β. First we have:
P(z)
=
αz−3 + (4α + β)z−2 + (4β + 7α)z−1 + (8α + 6β)
+(4β + 7α)z + (4α + β)z2 + αz3
The condition:
P(z) + P(−z) = 2((4α + β)z−2 + (8α + 6β) + (4α + β)z2) = 2
is met if 4α + β = 0 and 8α + 6β = 1. This leads to α = −1/16 and β = 1/4.
If we factorize R(z), then associate with G0(z) the roots inside the unit circle,
we get:
G0(z) =
1
4
√
2
(
(1 +
√
3) + (3 +
√
3)z−1 + (3 −
√
3)z−2 + (1 −
√
3)z−3)
This leads to h0(n) = g0(−n), then g1(n) = (−1)ng0(3 −n) and h1(n) =
g1(−n).
The following program calculates the coeﬃcients of G0(z), plots the gains
of the analysis ﬁlters, and checks the perfect reconstruction property on a
trajectory.
%===== daub4.m
clear
r=4;
% delay due to the bank
g0=[1+sqrt(3);3+sqrt(3);3-sqrt(3);1-sqrt(3)]/4/sqrt(2);
h0=g0(r:-1:1); g1=-h0 .* ((-1) .^ (0:r-1)');
h1=g1(r:-1:1);
%===== gains
Lfft=1024; freq=[0:Lfft-1]/Lfft;
G0f=abs(fft(h0,Lfft)); G1f=abs(fft(h1,Lfft));
subplot(311); plot(freq,[G0f G1f]); grid;
set(gca,'Xlim',[0 .5])
%===== verification
N=1000; x=randn(N,1);
%===== analysis
x0=filter(g0,1,x); x1=filter(g1,1,x);
%===== decimation/expansion
v0=x0; v0(1:2:N)=zeros(N/2,1);
v1=x1; v1(1:2:N)=zeros(N/2,1);
%===== synthesis
y0=filter(h0,1,v0); y1=filter(h1,1,v1);
xchap=y0+y1; max(abs(xchap(r:N)-x(1:N-r+1)))
subplot(312); plot(x(100:120)); grid
subplot(313); plot(xchap(100+r-1:120+r-1)); grid

42
Digital Signal and Image Processing using MATLAB®
Comments
– We often only restrict ourselves to the two-branch symmetrical ﬁlter,
because the same segmentation can be applied to both branches (Figure
2.16).
2
2
x(n)
G0(z)
G1(z)
2
2
G00(z)
G01(z)
2
2
G10(z)
G11(z)
Figure 2.16 – Decomposition of each branch
A particular decomposition in octaves (Figure 2.17) can be associated
with the wavelets using multi-scale analysis.
2
2
x(n)
G0(z)
G1(z)
2
2
G10(z)
G11(z)
2
2
G110(z)
G111(z)
Figure 2.17 – Decomposition in octaves
– A commonly used approach in sub-band decomposition techniques uses
the FFT calculation structure. Analysis and synthesis ﬁlter banks con-
sist of inverse and direct “FFT blocks”.
Any reader curious for more
information on this method should read [9, 39].
2.3
Ripple control
2.3.1
Principle
The Remez algorithm [32] [33] [31], and its subsequent application to the syn-
thesis of FIR ﬁlters by Parks and McClellan [29], is based on the alternance
theorems (Borel, 1905) [7] [17] and an optimal error control (La Vallée Poussin,

Additional Information About Filtering
43
1910) at each step of an iterative calculation. The objective is to obtain as faith-
ful an approximation as possible of the template we can use for the synthesis of
an FIR ﬁlter (1), whose frequency response is a polynomial in e2jπf. MATLAB®
provides the function firpm in its signal toolbox to perform a synthesis of this
type.
Hereinafter, we examine the approximation of continuous functions f(x)
over the interval [a, b], and use the notation ||.|| for the norm sup. We look for
a polynomial p(x) which minimizes the error ε(x) = p(x) −f(x).
The polynomial approximation theorem establishes the fact that there is a
unique polynomial pn of degree n and a subdivision of at least n + 2 points xk,
a ≤x0 < x1 < · · · < xn+1 ≤b, such that, if, ∀k, 0 ≤k ≤n:
f(xk+1) −pn(xk+1)
f(xk) −pn(xk)
= −1
(2.16)
and |f(xk) −pn(xk)| = ||f −pn||
then pn is a better uniform approximation of f in the sense of the norm sup.
We denote it as p∗
n.
These are suﬃcient and also necessary conditions, and pn is unique (Cheby-
shev theorem).
Figure 2.18 – Polynomial approximation and alternance
The approximation of the error is expressed as follows: the error ε(x) =
p(x) −f(x) exhibits at least n + 2 oscillations. Let x0, x1, . . ., xn+1 be the
abscissa values of the local maxima of the error, and suppose that the expression
(2.16) is satisﬁed (alternance). It can be shown that the optimal error µ is such
that:
min
k |p(xk) −f(xk)| ≤µ ≤max
k
|p(xk) −f(xk)|
(2.17)
More generally speaking, we can introduce a weighting function W(x) for
the criterion over the interval [a, b], and thus control the ripples of W(x)(p(x)−
f(x)).
(1)eeweb.poly.edu/iselesni/EL713/remez/remez.pdf

44
Digital Signal and Image Processing using MATLAB®
2.3.2
Programming
We will look at the case of a low-pass FIR ﬁlter with 2n+1 coeﬃcients hk such
that hk = h−k. The frequency response is of the form:
H(f) =
+n
∑
k=−n
hk exp(2kπjf) = h0 + 2
n
∑
k=1
hk cos(2kπf)
(2.18)
which we can write as H(f) = ∑n
k=0 αk cosk(2πf). We will use the notation
G(f) for the desired frequency response, i.e. G(f) = 1 for 0 ≤f < fc and
G(f) = 0 for fc < f ≤1/2.
If the polynomial is optimal, the ripples have the same value δa for the
frequencies f0, f1, . . ., fn+1. If we introduce a weighting function W(f), this
gives us:
G(f0)
=
h0 + · · · + 2hn cos(2nπf0) +
δa
W(f0)
G(f1)
=
h0 + · · · + 2hn cos(2nπf1) −
δa
W(f1)
(2.19)
...
G(fn+1)
=
h0 + · · · + 2hn cos(2nπfn+1) + (−1)n+1
δa
W(fn+1)
system of n + 2 equations with n + 2 unknowns (the hk and δa) which we can
solve (see function calcak). It may actually be possible to calculate δa directly.
In fact, we do not know the series of fk. We take n + 2 values including the
terminal values (0, 1/2, the limits of the transition band fc −∆f and fc + ∆f)
and the result of the system (2.19) will lead to a polynomial whose ripples are
greater than δa/W(f). However, we know that the optimal error µ is such
that:
|δa/W| ≤µ ≤||H −G|| = M
(2.20)
Therefore, we need to choose the fk values such that |δa/W| is maximum.
If, during the calculation, |δa/W| and ||H −G|| are suﬃciently close to one
another, then we can consider that the optimum value has been reached.
The stop test for the computational procedure may use a quality factor qa
as follows. Consider the relative errors. On the basis of (2.20), we obtain the
relation:
||H −G|| −µ
µ
≤||H −G|| −ε
ε
= qa
(2.21)

Additional Information About Filtering
45
When qa falls below a pre-deﬁned value, the procedure is halted.
It is
noteworthy that, on the basis of (2.20) and (2.21), we also obtain:
||H∗−G|| ≤||H −G|| ≤||H∗−G||(1 + qa)
(2.22)
This relation was contributed by de la Vallée Poussin.
Example 2.1 (Synthesis of a low-pass ﬁlter) Here we initialize the n + 2
frequencies by taking f = 0, f = 1/2, fc −∆f and fc + ∆f where [−∆f, +∆f]
deﬁnes the width of the transition band. We also suppose that the weighting
function is equal to 1 in the passband and in the attenuation band, equal to 0
in ]fc −∆f, fc + ∆f[. At step 1, the result obtained for the calculation of the
polynomial is illustrated in Figure 2.19. We can see that for the frequencies
fk we have an error equal to δa and that error is not optimal. The maximum
value of the error M is > δa.
Figure 2.19 – Result of the initial calculation for fc = 0.2, n = 6 and ∆f = 0.02
The error function is forced at 0 over the interval ] −fc, fc[. The choice of
a new sequence {fk} respects the following principles (Figure 2.20):
– the sign of the error changes for each new point of frequency fk;
– the error for each fk is |ε(fk)| ≥|δ|;
– |ε(fk)| = |εk| > |δ| for at least one of the fk.
At step 1 of the algorithm, the alternance conditions are fulﬁlled (Figure 2.20).
The new frequencies are indicated by
corresponding to the maximum and
minimum points of the error function.
The maximum values are obtained from the zeros (Figure 2.21) of the deriva-
tive of the gain.

46
Digital Signal and Image Processing using MATLAB®
0.1266768
0.5
0.4
0.3
0.2
0.1
0
−0.5
−0.4
−0.3
−0.2
−0.1
Figure 2.20 – Error function and choice of new frequencies
function [ffreq,fk,gainfk,mrk,nn]=selnewfr(ir,n,fc,Df)
%!=========================================================!
%! SYNOPSIS: [ffreq,fk,gainfk,mrk,nn]=SELNEWFR(ir,n,fc,Df) !
%! ir
= coeff. of FIR
!
%! fc
= cutoff freq.
!
%! Df
= defines the transition band [fc-Df,fc+Df]
!
%! ffreq
= freq. sequence <> edge freq.
!
%! fk
= new freq. sequence
!
%! gainfk = gain(fk)
!
%! mrk
= error(fk)
!
%!=========================================================!
myeps=10^(-10);
fk=[0;1/2;fc-Df;fc+Df]; % initial values (edge freq.)
%==========
nt=fix(length(ir)/2);
pdiffcoef=ir .* [nt:-1:-nt]'*2*pi*1i; % dP/df
rtspol=roots(pdiffcoef); nn=nt;
icond=(1-myeps < abs(rtspol)) & (abs(rtspol) < 1+myeps);
idx=find(icond); rr=rtspol(idx);
ffreq=atan2(imag(rr),real(rr))/pi/2;
idx=find(ffreq>myeps & ffreq<1/2-myeps);
ffreq=ffreq(idx);
fk=[fk;ffreq]; fk=sort(fk);
expc=exp(-2*pi*1i*fk);
gainfk=polyval(ir,expc)
.* exp(2*pi*1i*nt*fk);
mrk=real(gainfk); idx=find(fk<=fc-Df); mrk(idx)=mrk(idx)-1;
if length(fk)~= n+2
if sign(mrk(1))==sign(mrk(2))
fk(1)=[]; mrk(1)=[];
elseif sign(mrk(end))==sign(mrk(end-1))

Additional Information About Filtering
47
fk(end)=[]; mrk(end)=[];
end
nn=length(fk)-2;
end
Figure 2.21 – Roots of the polynomial pdiffcoef (function selnewfr)
At step 2 of the algorithm we have an additional maximum (Figure 2.22)
and alternance is no longer assured. The ﬁrst frequency is “eliminated”. If
alternance were veriﬁed, the order of the ﬁlter would be altered (n →n + 1).
The value of n can be initialized by using Kaiser’s approximation, which
gives the theoretical length Nt of the ﬁlter:
Nt = −10 log10(δpδs) −13
14.6 × 2 × ∆f
+ 1;
(2.23)
where δp and δs are the ripples in the passband and attenuation band. In our
case, δp = δs = δ. If we arbitrarily take δ = 0.1:
delta=.1; mdelta=1.177479e-01;
Nt=fix((-20*log10(abs(mdelta)) -13)/14.6/2/Df) + 1
which gives Nt ≈13 and n = 6. This generally constitutes a minimum value.
%===== testremes.m
% low-pass filter

48
Digital Signal and Image Processing using MATLAB®
0.1675315
0.3
0.2
0.1
0
−0.3
−0.2
−0.1
Figure 2.22 – Error function at the second step of the algorithm
% Weighting function W=Wp=Ws=1
clear
verb=true;
nf=2048; nfs2=nf/2; fr=[0:nfs2-1]/nf;
fc=.2; Df=.01; n=15; qf0=10^(-3);
[ir,lpcnt,mdelta,gainmat]=LPremes(n,nf,fc,Df,qf0,verb);
sprintf('loop count: %d',lpcnt-1)
sprintf('delta: %d, %d dB',abs(mdelta),20*log10(abs(mdelta)))
sprintf('FIR order: %d',length(ir))
if verb
plot(fr,gainmat(:,end)); grid on
hold on
plot([fc-Df fc-Df],[0 1+abs(mdelta)],'r')
plot([fc+Df fc+Df],[0 1+abs(mdelta)],'r')
plot([0 fc+Df],[1+mdelta 1+mdelta],'r')
plot([0 fc+Df],[1-mdelta 1-mdelta],'r')
plot([fc-Df 1/2],abs([mdelta mdelta]),'r')
hold off
end
function [ir,lpcnt,mdelta,gainmat]=LPremes(n,nf,fc,Df,qf0,verb)
%!=============================================================!
%! Low-pass filter
!
%! SYNOPSIS:
!
%!
[ir,lpcnt,mdelta,gainmat]=LPREMES(n,nf,fc,Df,qf0,verb) !
%! fc
= cutoff freq.
!
%! Df
= defines the transition band [fc-Df,fc+Df]
!
%! nf
= number of FFT points
!
%! n
= 2n+1 FIR coefficients
!
%! lpcnt
= loop count
!
%! mdelta
= ripple amplitude
!
%! gainmat = (nf/2,lpcnt) gain matrix
!

Additional Information About Filtering
49
%!=============================================================!
fr0=[0:nf-1]/nf; fr=fr0(1:nf/2);
%===== initial freq. sequence f
n1=fix(2*fc*(n+2)); st1=(fc-Df)/(n1-1);
f1=[0:st1:fc-Df];
n2=(n+2-n1); st2=(1/2-fc-Df)/(n2-1);
f2=[fc+Df:st2:1/2];
f=[f1,f2]'; H=[ones(1,n1),zeros(1,n2)];
fk=f;
Hf=zeros(1,nf/2); idx=find(fr<=fc); Hf(idx)=1;
%===========
qfact=inf; gainmat=[]; lpcnt=1;
%===========
while (qfact>qf0)
%===== best approximation
[ak,mdelta]=calcak(n,f,fc,Df);
ir=[ak(n+1:-1:2);ak]; Lir=length(ir); % impulse resp.
gainf=fft(ir,nf);
gainf=real(gainf .* exp(2*pi*1i*n*fr0'));
if verb, gainmat=[gainmat,abs(gainf(1:nf/2))]; end
lpcnt=lpcnt+1;
dk=abs(mdelta);
%=====
merrf=gainf(1:nf/2)-Hf.';
idx=find(fr>=fc-Df & fr<=fc+Df); merrf(idx)=0;
%===== selection of a new sequence
[ffreq,fk,gk,mrk,nn]=selnewfr(ir,n,fc,Df);
n=nn; M=max(abs(mrk)); f=fk;
%==========
qfact=(M-dk)/M;
end
1.2
1
0.8
0.6
0.4
0.2
0
0
0.05
0.1
0.15 0.2
0.25
0.3
0.35
0.4
0.45
0.5
Figure 2.23 – Result for an order equal to 33 with ripples in amplitude −18 dB,
∆f = 0.01, a quality factor of 10−3 and ﬁve iterations (see program testremes)

50
Digital Signal and Image Processing using MATLAB®
function [ak,mdelta]=calcak(n,f,fc,Df)
calt=(-1).^(1:n+2)';
mM=[cos(2 * pi * f * [0:n]),calt];
idx=find(f<=fc-Df); n1=length(idx); n2=(n+2-n1);
H=[ones(1,n1),zeros(1,n2)];
coeffs=mM \ H'; ak=coeffs(1:end-1);
ak(2:end)=ak(2:end)/2; mdelta=coeffs(end);
end
0
10
20
30
40
50
60
0
0.1
0.2
0.3
0.4
0
10
20
30
40
50
60
0
50
100
150
n
n
Figure 2.24 – Evolutions of δ (top ﬁgure) and of the real number of coeﬃcients of
the ﬁlter (bottom ﬁgure) as a function of n

Chapter 3
Image Processing
The basic version of MATLAB® oﬀers a number of functions for the processing
of images without actually needing to have the toolbox image processing. In
particular, it includes the functions image and imagesc, filter2 and conv2,
fft2, etc., and the software is also able to deal with numerous formats for
opening and saving images.
3.1
A little geometry
3.1.1
3D object
This section discusses the representation of a “3D” object on a screen. The
basic idea is to “project” the object onto a screen P placed between the object
itself and the observer’s eye (point C in Figure 3.1).
We choose a reference system (x, y, z), and suppose that the screen is or-
thogonal to the z axis. Given a point M(xM, yM, zM), the point projected onto
the screen is P(xP , yP , zP ), with zP = Dr −Ds. The eye C of the observer is
at (0, 0, Dr). We deﬁne δ:
δ =
Ds
Dr −zM
⇒xP = δ × xM, yP = δ × yM
(3.1)
Exercise 3.1 (Projection onto a screen (see p. 194))
Consider a 3D object deﬁned by the function newimg. That function deﬁnes a
ﬁlter referred to as a DoG (Diﬀerence of Gaussians) ﬁlter.
function [tx,ty,tz,zz,xx]=newimg()
tx = linspace(-8,8,32)'; tz=tx;
[zz,xx] = meshgrid(tz,tx);
yy=(-zz .^ 2 - xx .^ 2);

52
Digital Signal and Image Processing using MATLAB®
OO
M
x
C
z
y
P
Dr
Ds
Figure 3.1 – Conical perspective (projection perspective) P of the point M onto the
P plane
ty = 10*exp(yy/4) - 4*exp(yy/10); % (DoG)
end
We are not seeking to obtain a realistic view of the object. Rather we will
content ourselves with merely projecting the points which deﬁne the object,
without worrying about the “facets” which may make it up.
1. Write a function which projects the object onto the screen (Figure 3.2).
We will take Dr = 20 and Ds = 12. The synopsis will be as follows:
function [pos2D,lm]=projconique(Ds,Dr,tb3N)
%!==================================================!
%! Dr = distance object reference origin - observer!
%! Ds = distance from screen to observer!
%! tb3N = (3*N)-array [[x1;y1;z1],...,[xN;yN;zN]]!
%! pos2D = coordinates in the P-plane!
%!==================================================!
2. Rather than modifying the position of the observer or of the screen, we
will simply move the object. Give the three matrices of rotation around
the three axes as a function of the angles φx, φy and φz, expressed in
degrees. Write three functions implementing these transformations. Take
the following description as a starting point:

Image Processing
53
x
Ds
Dr
y
z
Figure 3.2 – Construction of the projection
function vxp=rotx(vp,phix)
%!=========================================!
%! phix = rotation around x-axis (degrees)!
%! vp=(3*N)-array, vxp=(3*N)-array!
%!=========================================!
3. Write a program which moves the 3D object (rotational and translational
motion) and projects its image onto the screen.
3.1.2
Calibration of cameras
Numerous domains – e.g. robotics or face recognition – use cameras which
provide 2-dimensional images which are used to reconstruct a 3D image of an
object (Figure 3.3).

54
Digital Signal and Image Processing using MATLAB®
x
z
y
camera 3
camera 2
camera 1
ra
e
rer
Figure 3.3 – System with three cameras: each camera k is modeled by an optical
center Ck and a projection plane Pk representing the sensor. The point M is pro-
jected into m1, m2 and m3 on the sensors P1, P2 and P3 of the cameras
Calibration parameters
From the moment an image is captured, eﬀorts are made to deal, as best we
can, with the deformations due to the projection of real images on the CCD or
CMOS sensor used. The problems may be due:
– to deformations caused by the lens system;
– to the spatial and temporal image sampling rates;
– to diﬀerent interpretations of the coordinates on each camera, etc.
Hence, before using them, it is helpful to “calibrate the cameras” (in fact
we identify the projection model). There are numerous methods in existence
[38, 13, 18, 14], as well as numerous software packages, devoted to the operation
of calibration.
The projection model uses a optical center C which plays the role of the eye
mentioned in section 3.1.1. This model, known as the pinhole camera model,
simpliﬁes the role of the lenses of the objective somewhat, but in practice the
results that it yields are acceptable. The distance between the optical center
and the image plane is the focal length f, which we have previously denoted
Ds in section 3.1.1.

Image Processing
55
The calibration parameters come in the form of intrinsic parameters – i.e.
those linked to the camera itself – and extrinsic parameters – which correspond
to the camera’s position:
– Intrinsic parameters: if we consider a reference space linked to the camera
with the origin C and the camera’s own axis as its axis Oz (Figure 3.4),
we obtain:
f
zM
= xm
xM
= ym
yM
M
x
y
m
f
C
z
Figure 3.4 – Frame of reference linked to the camera
By using homogeneous coordinates, we ﬁnd:
zM


xm
ym
1

=


χm
ψm
ζm

=


f
0
0
0
0
f
0
0
0
0
1
0




xM
yM
zM
1


(3.2)
At the point of the sensor, the “sensor coordinates” (Figure 3.4), ex-
pressed in pixels, are given in the form:
{ xs = kxxm + x0
ys = kyym + y0
so that


xs
ys
1

=


kx
0
x0
0
ky
y0
0
0
1

×


xm
ym
1

(3.3)
where kx and ky are scaling factors and (x0, y0) is the origin of the coor-
dinates in the image plane (sensor). In addition, in order to take account
of the “skew” deformations, we introduce a parameter s/f into the above

56
Digital Signal and Image Processing using MATLAB®
matrix:


χs
ψs
ζs

= zM


kx
s/f
x0
0
ky
y0
0
0
1




xm
ym
1

=


kx
s/f
x0
0
ky
y0
0
0
1




χm
ψm
ζm


(3.4)
We obtain the relation between the coordinates of the point M in the
frame of reference (C, x, y, z) and the sensor coordinates in that same
frame of reference:


χs
ψs
ζs

=


fkx
s
x0
0
0
fky
y0
0
0
0
1
0




xM
yM
zM
1

=


αx
s
x0
0
0
αy
y0
0
0
0
1
0




xM
yM
zM
1

(3.5)
We rewrite this as follows:


αx
s
x0
0
0
αy
y0
0
0
0
1
0


=


αx
s
x0
0
αy
y0
0
0
1

×


1
0
0
0
0
1
0
0
0
0
1
0


=
K ×
[ I3
0
]
(3.6)
K is called the calibration matrix.
– Extrinsic parameters:
we still need to express the relation giving
[xM
yM
zM
]T as a function of
[XM
YM
ZM
]T given in a diﬀer-
ent frame of reference from that of the camera (Figure 3.5).
M
X
Y
Z
x
y
C
O
z
Figure 3.5 – Frames of reference of the camera – (C, i, j, k) – and of the surround-
ing universe – (O, I, J, K)
We suppose that the frames of reference are orthonormal. The conversion
from one frame of reference to the other is made by three rotations – by

Image Processing
57
angles α around OX, β around OY and γ around OZ – followed by a
translation. We use the notations Rx, Ry and Rz to represent the three
rotation matrices associated with these motions.
For the rotations, we have:


xM
yM
zM


=
RzRyRx


XM
YM
ZM

= R


XM
YM
ZM


=


cos(γ)
sin(γ)
0
−sin(γ)
cos(γ)
0
0
0
1




cos(β)
0
−sin(β)
0
1
0
sin(β)
0
cos(β)

×


1
0
0
0
cos(α)
sin(α)
0
−sin(α)
cos(α)




XM
YM
ZM


(3.7)
Taking account of the translation −−→
OC, writing vT
M =
[xM
yM
zM
]
,
V T
M =
[XM
YM
ZM
]
and V T
C =
[XC
YC
ZC
]
, and in homogeneous
coordinates we obtain:
[vM
1
]
=
[R
−RV C
0T
1
] [V M
1
]
(3.8)
In summary:


XM
YM
ZM
1


[R
−RV C
0T
1
] 

xM
yM
zM
1




f
0
0
0
0
f
0
0
0
0
1
0



χm
ψm
ζm




kx
s/f
x0
0
ky
y0
0
0
1



χs
ψs
ζs


where χs, ψs and ζs are the coordinates in the sensor plane ((3.5)).
From (3.6) and (3.8) we deduce:


χs
ψs
ζs


=
K ×
[ I3
0 ]
×
[R
−RV C
0T
1
]


XM
YM
ZM
1


=
KR
[ I3
−V C
] [V M
1
]
= P
[V M
1
]
(3.9)
It is noteworthy that, in the product M = KR, the last row is given by
the last row of R which gives the three components of the vector k in the frame

58
Digital Signal and Image Processing using MATLAB®
of reference (O, I, J, K) (and thus
[
kT I
kT J
kT K
]
). As k is a unit vector,
we must have m2
31 + m2
32 + m2
33 = 1. In light of (3.9) we deduce that we also
have p2
31 + p2
32 + p2
33 = 1.
Thus, the problem of calibration involves estimating the components pij of
the matrix P = KR
[ I3
−V C
]
.
In total, there are 11 unknowns: ﬁve for the matrix K, three for the rotation
of the axes (three angles), and three for the translation.
In our discussion above, we have ignored numerous aspects – particularly
the nonlinear eﬀects such as the distorsions introduced by the optical system.
There are many others, including faults in the centering of the lenses, lack of
perpendicularity of the optical axis and the sensor, etc.
Computation of the calibration parameters
There are many ways to ﬁnd the calibration parameters. All these methods
use “test patterns” which may include checkered patterns, circles, ellipses, etc.
The reference [38] describes a calibration grid known as a Tsai grid, often used
to calculate the parameters of the system on the basis of the coordinates of the
“corners” of each tile on the grid.
It is possible, using a “least-squares’-type method’, to obtain the pij of
the matrix P . This method, known as the Faugeras and Toscani method, is
presented in [13].
Based on (3.9), we can write:
xs
=
χs
ζs
= p11XM + p12YM + p13ZM + p14
p31XM + p32YM + p33ZM + p34
ys
=
ψs
ζs
= p21XM + p22YM + p23ZM + p24
p31XM + p32YM + p33ZM + p34
As we have 11 elements to evaluate, we need at least 6 couples (mi, Mi).
Each such couple gives two linear equations with respect to the parameters pij:
p11XM + p12YM + p13ZM + p14 −xsp34
−xs(p31XM + p32YM + p33ZM) = 0
p21XM + p22YM + p23ZM + p24 −ysp34
−ys(p31XM + p32YM + p33ZM) = 0
However, it should not be forgotten that the data obtained are subject to
noise – estimation of the positions, spatial quantiﬁcation in the sensors, etc. In

Image Processing
59
practice, we take around thirty points (Mn ≈30).


XM1
YM1
ZM1
1
0
0
0
0
−xs1
0
0
0
0
XM1
YM1
ZM1
1
−ys1
...
...
XMn
YMn
ZMn
1
0
0
0
0
−xsn
0
0
0
0
XMn
YMn
ZMn
1
−ysn




p11
p12
p13
p14
p21
p22
p23
p24
p34


+


−xs1XM1
−xs1YM1
−xs1ZM1
−ys1XM1
−ys1YM1
−ys1ZM1
...
...
−xsnXMn
−xsnYMn
−xsnZMn
−ysnXMn
−ysnYMn
−ysnZMn




p31
p32
p33

= 02n
which we write as:
Ap + Bq = 02n with qT q = 1
(3.10)
We seek to minimize (Ap + Bq)T (Ap + Bq) with the constraint qT q = 1.
For the sake of simplicity of the formulation, we shall set:
C =
[A
B]
, M =
[0
0
0
I
]
, r =
[p
q
]
The problem boils down to looking for r such that ||Cr|| is minimal, where
||Mr|| = 1.
We apply the Lagrange multiplier method, which consists of
seeking the extreme values of the function L (r, λ) (see section (6.3)):
L (r, λ) = rT CT Cr + λ(1 −rT Mr)
(3.11)
By derivation, we obtain:
∂L
∂r = 2CT Cr −2λMr = 0
(3.12)
We put the value of λ found into the criterion L :
L (r, λ) = rT λMr + λ(1 −rT Mr) = λ
(3.13)
Hence, our problem involves minimizng λ. With the notations used:
AT Ap + AT Bq = 0
BT Bq + BT Ap −λq = 0

60
Digital Signal and Image Processing using MATLAB®
which gives us:
λq
=
BT Bq + BT Ap
(3.14)
=
BT Bq −BT A(AT A)−1AT Bq
=
BT [
I −A(AT A)−1AT )
]
Bq
We can verify that λ is positive. Indeed, by multiplying (3.14) on the left
by qT , we obtain:
qT λq = λ = qT BT Bq + qT BT Ap = qT BT Bq + pT AT Ap > 0
The expression obtained by (3.14) shows that λ is an eigenvalue of the
matrix BT [
I −A(AT A)−1AT )
]
B and that therefore we need to choose the
smallest of them.
The corresponding eigenvector gives us q, and then p =
−(AT A)−1AT Bq.
function [r]=calibparam(m,M)
%!================================================!
%! SYNOPSIS [r]=CALIBPARAM(m,M)
!
%! m : (2*N) (xs(k);ys(k)), k=1:N matrix (pixels) !
%! M : (3*M) (XM(k);YM(k);ZM(k)), k=1:N matrix
!
%! r : (11,1) parameter vector
!
%!================================================!
n=size(M,2);
A=zeros(2*n,8);
for k=1:n
A(2*k-1:2*k,:)=kron(eye(2,2),[M(:,k);1]');
end
MM=[M;M]; MMM=zeros(3,2*n); MMM(:)=MM; MMM=MMM';
mm=-reshape(m,2*n,1)*ones(1,3); B=mm.*MMM;
Q=inv(A'*A)*A';
P=B'*(eye(2*n,2*n)-A*Q)*B;
[V,D]=eig(P); [Y,idx]=min(diag(D));
q=V(:,idx); p=-Q*B*q;
r=[p;q];
Separation of the intrinsic and extrinsic parameters
It is possible to obtain V C, K and R in the following manner:
– given P , we obtain V C by using (3.9):
P
[V C
1
]
= 0 ⇒V C = −P −1
(1:3,1:3)P (:,4)
using MATLAB®-type notations;

Image Processing
61
– given P = KR
[ I3
−V C
]
we wish to obtain K and R. Taking the
M (3×3) matrix formed by the ﬁrst three columns of P , we must perform
the factorization M = KR, where K is an upper triangular matrix and
R an orthogonal base-change matrix whose expression we have already
seen R = RzRyRx ((3.7)).
We can also write K = MRT = MRT
x RT
y RT
z . We have, successively:
1. k32 = 0 = m32 cos(α) + m33 sin(α):
MRT
x = M ×


1
0
0
0
cos(α)
−sin(α)
0
sin(α)
cos(α)

= M ′ =


•
•
•
•
•
•
•
0
•


so that:
cos(α) =
m33
√
m2
32 + m2
33
et sin(α) = −
m32
√
m2
32 + m2
33
(3.15)
2. k31 = 0 = m′
31 cos(β) −m′
33 sin(β):
M ′RT
y = M ′×


cos(β)
0
sin(β)
0
1
0
−sin(β)
0
cos(β)

= M” =


•
•
•
•
•
•
0
0
•

(3.16)
thus:
cos(β) =
m′
33
√
m′2
31 + m′2
33
et sin(β) =
m′
31
√
m′2
31 + m′2
33
(3.17)
3. k21 = 0 = m′′
21 cos(γ) + m′′
22 sin(γ):
M”RT
z = M” ×


cos(γ)
−sin(γ)
0
sin(γ)
cos(γ)
0
0
0
1

=


•
•
•
0
•
•
0
0
1

(3.18)
hence:
cos(γ) =
m′′
22
√
m′′2
21 + m′′2
22
and sin(γ) = −
m′′
21
√
m′′2
21 + m′′2
22
(3.19)
We obtain:
K = MRT
x RT
y RT
z with R = RzRyRx
(3.20)

62
Digital Signal and Image Processing using MATLAB®
function [K,R]=KRdecomp(M)
%!===========================================!
%! SYNOPSIS [K,R]=KRDECOMP(M)
!
%! M : (3*3) matrix with sum(M(3,:).^2)=1
!
%! K : (3*3) upper triangular matrix
!
%! R : (3*3) orthogonal matrix
!
%!===========================================!
cte1=1/sqrt(M(3,2)*M(3,2)+M(3,3)*M(3,3));
sina=-cte1*M(3,2); cosa=cte1*M(3,3);
RxT=[1
0
0;
0
cosa -sina;
0
sina cosa];
M1=M*RxT;
cte2=1/sqrt(M1(3,1)*M1(3,1)+M1(3,3)*M1(3,3));
sinb=cte2*M1(3,1); cosb=cte2*M1(3,3);
RyT=[cosb
0 sinb;
0
1 0;
-sinb 0 cosb];
M2=M1*RyT;
cte3=1/sqrt(M2(2,1)*M2(2,1)+M2(2,2)*M2(2,2));
sinc=-cte3*M2(2,1); cosc=cte3*M2(2,2);
RzT=[cosc -sinc 0;
sinc cosc
0;
0
0
1];
R=RzT'*RyT'*RxT'; K=M*R';
return
3.2
Pyramidal decompositions
The pixels in an image are generally strongly correlated with their neighbors.
It is easy to see that, in order to generate an acceptable representation of an
image, it is suﬃcient to use an under-sampled version of that image.
The
operation of under-sampling causes the loss of information, which it is possible
to compensate for by using the error rate committed during that operation.
Hence, we obtain the scheme of the principle shown in Figure 3.6.
+
−
+
−
+
−
Figure 3.6 – Decomposition by iterative ﬁltering
The symbol
represents a decimation by a factor 1/2. Based on the image

Image Processing
63
P0, which is under-sampled by a factor 2, we obtain P1, and the prediction error
L0 = P0 −Q0 which contains the high-frequency data from the original image.
The process can be repeated in successive iterations.
One of the advantages to this type of operation (though not the only one)
is image compression. The image Pn is relatively small, and the “errors” Lp
can be encoded on a reduced number of bits.
The operation of reconstruction (Figure 3.7) introduces errors due to the
interpolations (the symbol
represents an interpolation of ratio 2).
Figure 3.7 – Reconstruction
The program tstpyramide.m illustrates the decomposition with the “low-
frequency” and “high-frequency” parts represented in Figure 3.8.
%===== tstpyramide.m
ww=BurtAdelsonInit; C=ww.cH; Ct=ww.cHt; ri=C' * Ct;
load lena, diml=size(pixc); pixc0=pixc;
%=====
figure(1), clf
ha=axes('units','pix','position',[30 30 fliplr(diml)]);
imagesc(pixc), colormap(cmap), axis('image')
nx=diml(1)+60; Nlevel=3;
L=zeros(diml(1),diml(2),Nlevel+1);
for k=1:Nlevel
pixcf=filter2(ri,pixc);
%===== on n'estime pas l'interpolee
L(1:diml(1),1:diml(2),k)=pixc-pixcf;
hb=axes('units','pix',...
'position',[nx-diml(1) 60+diml(2) fliplr(diml)]);
imagesc(L(1:diml(1),1:diml(2),k))
pixcf=pixcf(1:2:end,1:2:end); diml=size(pixcf);
ha=axes('units','pix','position',[nx 30 fliplr(diml)]);
nx=nx+diml(1)+30;
pixc=pixcf; imagesc(pixc)
end
L(1:diml(1),1:diml(2),k)=pixcf;
The coeﬃcients of the low-pass ﬁlter are given by the initialization function
BurtAdelsonInit.m:
function wfltrBurtAdelson=BurtAdelsonInit
%============================
% Un des filtres passe-bas

64
Digital Signal and Image Processing using MATLAB®
%============================
rac2=sqrt(2);
cHBurtAdelson = [rac2*[-1 5 12 5 -1]/20];
cHtildeBurtAdelson = [rac2*[-3 -15 73 170 73 -15 -3]/280];
wfltrBurtAdelson=struct('cH',cHBurtAdelson,...
'cHt',cHtildeBurtAdelson);
50
100
150
200
250
50
100
150
200
250
50
100
150
200
250
50
100
150
200
250
20
40
60
80 100 120
20
40
60
80
100
120
20
40
60 80 100 120
20
40
60
80
100
120
20 40
60
20
40
60
20
40
60
20
40
60
102030
10
20
30
Figure 3.8 – Pyramidal decomposition
3.2.1
Pyramidal
decomposition
given
by
Burt
and
Adelson
The construction scheme presented above does not take account of the fact
that it is not useful to actually compute all of the points (low-pass ﬁltering is

Image Processing
65
applied to the whole image). It is suﬃcient to compute only those points which
are necessary [4]. However, the “high-frequency” part cannot be performed
directly. Hence, the result must be interpolated as shown in Figure 3.9.
P0
Q0
L0
L1
L2
P1
Q1
P2
Q2
P3=L3
+
−
+
−
+
−
Figure 3.9 – Decomposition according to Burt and Adelson
P2
L2
P1
L1
P0
L0
P3=L3
P3
P2
P1
Figure 3.10 – Reconstruction
3.2.2
Pyramidal decomposition using a Haar tranforma-
tion
We apply a transformation which tends to decompose the image into four fre-
quency bands. Consider the transformation:
T =
1
√
2
[1]
Each X(2 × 2) block is transformed as follows:
Y = T XT T = T
[a
b
c
d
]
T T =
[ a + b + c + d
(a + c) −(b + d)
(a + b) −(c + d)
(a + d) −(b + c)
]
/2
Here, we notice four terms corresponding to a mean (a + b + c + d), a
horizontal derivative ((a + c) −(b + d)), a vertical derivative ((a + c) −(b + d))
and a second derivative ((a + d) −(b + c)). The program dwtproc.m gives a
direct realization of this.
%===== dwtproc.m =====
figure(1), clf
pixc=imread('boats.jpg','JPG'); diml=size(pixc);
cmap=[0:1/255:1]'*ones(1,3);
ha=axes('units','pix','position',[30 30 fliplr(diml)]);

66
Digital Signal and Image Processing using MATLAB®
imagesc(pixc), axis('square'), colormap(cmap)
%===== Construction directe
A=double(pixc(1:2:end,1:2:end)); B=double(pixc(1:2:end,2:2:end));
C=double(pixc(2:2:end,1:2:end)); D=double(pixc(2:2:end,2:2:end));
pixcLL=(A+B+C+D)/2; pixcHL=(A+C-B-D)/2;
pixcLH=(A+B-C-D)/2; pixcHH=(A-C-B+D)/2;
pixcf=[normM(pixcLL,cmap),normM(pixcHL,cmap);...
normM(pixcLH,cmap),normM(pixcHH,cmap)];
figure(2), clf
hc=axes('units','pix','position',[30 30 fliplr(diml)]);
imagesc(pixcf), axis('square'), colormap(cmap)
In writing this program, it would have been possible to use the 2D ﬁltering
functions in MATLAB®, but the result would have taken 4-5 times as long to
obtain. The results are normalized so they can be displayed in a single table:
%===== dwtprocf.m =====
pixc=imread('boats.jpg','JPG'); diml=size(pixc);
cmap=[0:1/255:1]'*ones(1,3);
%===== Construction par filter2
T=[1 1;-1 -1]; Tt=T';
pixcLL=filter2(ones(2,2),pixc); pixcLH=filter2(T,pixc);
pixcHL=filter2(Tt,pixc); pixcHH=filter2(T*Tt,pixc);
%===== Normalisation pour affichage
pixcLL=normM(pixcLL,cmap); pixcHL=normM(pixcHL,cmap);
pixcLH=normM(pixcLH,cmap); pixcHH=normM(pixcHH,cmap);
pixc3=[pixcLL(1:2:end,1:2:end) pixcHL(1:2:end,1:2:end);...
pixcLH(1:2:end,1:2:end) pixcHH(1:2:end,1:2:end)];
clf
hb=axes('units','pix','position',[30 30 fliplr(diml)]);
imagesc(pixc3), axis('square'), colormap(cmap)
function pixct=normM(pixc,cmap)
Lc=size(cmap,1);
Mx=max(max(pixc)); mx=min(min(pixc));
pixct=1+(pixc-mx)./(Mx-mx)*(Lc-1);
3.2.3
Stepwise decomposition (lifting scheme)
Up until now, the decomposition has been based on the spectral content. We
have separated out the low- and high-frequency components. The lifting scheme
is based on a spatial decomposition (e.g. a decimation). The two separate
elements are generally strongly correlated. By exploiting this correlation, we
are able to “compress” the representation.
Consider P0 and the two components P1 and Q1. We can use P1 to predict
Q1. Rather than predicting Q1, we predict P 1 = Q1−P(P1), where P denotes
the predictor. We then attain the setup illustrated in Figure 3.13.

Image Processing
67
Figure 3.11 – Original image
The scheme (3.13) yields an image Pn obtained on the basis of successive
separations which exhibit a phenomenon of aliasing.
In order to limit that
aliasing, we “update” the separate component on the basis of the prediction
error (operator U in Figure 3.14).
Example 3.1 (Lifting scheme)
[36] gives the following example, in which the operator P is a simple interpo-
lation (the mean of two neighbors):
R0(k)
=
P0(2k)
Q0(k)
=
P0(2k + 1)
P 1(k)
=
Q0(k) −(R0(k) + R0(k + 1)) /2
=
P0(2k + 1) −(P0(2k) + P0(2k + 2)) /2
(3.21)
which is tantamount to ﬁltering the sequence P0 with the impulse response
ﬁlter {−1
2, 1, −1
2} (high-pass).

68
Digital Signal and Image Processing using MATLAB®
Figure 3.12 – Pyramidal decomposition using a Haar transformation
+
+
−
−
+
−
Figure 3.13 – Setup using only prediction (S performs the separation)
+
+
+
−
+
+
+
−
+
+
+
−
Figure 3.14 – Setup using prediction and updating

Image Processing
69
Updating consists of using the P 1(k) and P 1(k −1) as follows:
P1(k)
=
R0(k) + A
(
P 1(k) + P 1(k −1)
)
(3.22)
=
−A
2 P0(2k + 2) + AP0(2k + 1) +
(1 −A)P0(2k) + AP0(2k −1) −A
2 P0(2k −2)
corresponding to an impulse response ﬁlter {−A
2 , A, (1 −A), A, −A
2 } (low-
pass). The choice of A is made by supposing that the sum σ = ∑
k P0(k) is
kept constant. Using (3.22) and then (3.21):
∑
k
P0(k)
=
∑
k
P1(k) =
∑
k
R0(k) + 2A
∑
k
P 1(k)
=
∑
k
P0(2k) + 2A
∑
k
P0(2k + 1) −
A
∑
k
P0(2k) −A
∑
k
P0(2k + 2)
=
(1 −2A)
∑
k
P0(2k) + 2A
∑
k
P0(2k + 1)
Equality is assured only if A = 1/4. The gains of the two ﬁltering operations
are given by Figure 3.15.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.5
1
1.5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.5
1
Figure 3.15 – Gains of prediction and updating ﬁlters
From a spectral point of view, the operations of decimation and reconstruc-

70
Digital Signal and Image Processing using MATLAB®
tion can be described by:
R0(z) = P0(z1/2) + P0(−z1/2)
2
or R0(f) = P0(f/2) + P0((f −1)/2)
2
Q0(z) = z1/2
2
(
P0(z1/2) −P0(−z1/2)
)
or Q0(f) = eπjf
2
(P0(f/2) −P0((f −1)/2))
The reconstruction J is expressed as:
P0(z) = R0(z2) + z−1Q0(z2)
or
P0(f) = R0(2f) + e−2πjfQ0(2f)
We can write (we suppose that P and U are Finite Impulse Responses
(FIRs), with the respective transfer functions B(z) = ∑M−1
k=0 bkzk and A (z) =
∑N−1
k=0 akz−k:
P 1(z) = Q0(z) −
(M−1
∑
k=0
bkzk
)
R0(z) = Q0(z) −B(z)R0(z)
and
P1(z) = R0(z) +
(N−1
∑
k=0
akz−k
)
P 1(z) = R0(z) + A (z)P 1(z)
+
+
+
−
Figure 3.16 – Separation/reconstruction
If the construction were perfect, then for any value of P0(z) we should have:
P0(z)
=
P1(z2) + z−1P 1(z2)
=
R0(z2) +
(
A (z2) + z−1)
P 1(z2)
=
R0(z2) +
(
A (z2) + z−1) (
Q0(z2) −B(z2)R0(z2)
)
=
R0(z2)
[
1 −B(z2)
(
A (z2) + z−1)]
+ Q0(z2)
(
A (z2) + z−1)
so:
P0(z)
=
1
2 (P0(z) + P0(−z))
[
1 −B(z2)
(
A (z2) + z−1)]
+z
2 (P0(z) −P0(−z))
[
A (z2) + z−1]

Chapter 4
Numerical Calculus and
Simulation
This chapter is devoted to the problems of numerical calculus. Although nu-
merous functions – the conversion from a continuous-time system to a discrete-
time system, seeking of zeros, of minima and maxima, solving of diﬀerential
equations, etc. – are provided in the basic version of MATLAB®, it is useful to
have some understanding of the methods used. An abundant body of literature
is available on the subject, as are numerous publications in FORTRAN, C or
other languages. Thus, we will content ourselves here with describing a number
of techniques commonly used in devoted software packages.
4.1
Simulation of continuous-time systems
4.1.1
Simulation by approximation
Design methods based on continuous-discrete time changes actually consist of
constructing a discrete-time simulator of a linear diﬀerential equation. This
method provides satisfying results because the simulated systems are linear.
Many precautions would have been needed had they not.
Exercise 4.1 illustrates the implementation of an RC ﬁlter simulator sub-
jected to a periodic input.
Exercise 4.1 (Full-wave rectiﬁer and simulation) (see p. 197)
Consider a full-wave rectiﬁer followed by an RC ﬁlter (Figure 4.1).
1. The input signal s(t) = A sin(2πF0t) with F0 = 50 Hz is fed to the
rectiﬁer. Determine the Fourier series expansion of the rectiﬁed signal

72
Digital Signal and Image Processing using MATLAB®
Full-wave
rectifier
R
C
s(t)
x(t)
y(t)
Figure 4.1 – Full-wave rectiﬁer
x(t) = |s(t)|. What is the amplitude of the continuous component of
x(t)?
2. The output voltage y(t) of the RC ﬁlter veriﬁes the diﬀerential equation:
RC dy(t)
dt
+ y(t) = x(t)
Using the properties of the Fourier transform, determine the expression
of this ﬁlter’s complex gain H(F);
3. 1/RC is chosen to be much greater than F0 so that only the continuous
component and the ﬁrst harmonics remain in the output signal. What is,
in this case, the expression of y(t)?
4. We wish to simulate this system. In order to do so, we perform a sharp
enough discretization of time by choosing a sampling frequency Fs =
1/Ts = 5,000 Hz much greater than the input sine’s frequency.
We deﬁne xs(n) = x(nTs) ys(n) = y(nTs). By using the Euler approxi-
mation dy/dt ≃(y(nTs) −y((n −1)Ts)) × Fs, show that the diﬀerential
equation is equivalent to the recursive equation ys(n)+(τ −1)ys(n−1) =
τxs(n). Determine the expression of τ as a function of RC and Ts.
5. Write a program that simulates the system’s output voltage when the
input is a sinusoidal voltage with a frequency of 50 Hz and an RMS (Root-
Mean-Square) voltage of 220V. The ﬁlter’s time constant is RC = 0.01 s.
The filter function will be used to generate the output signal.
4.1.2
Exact model simulation
Consider a continuous-time ﬁlter with a frequency response that tends to 0
when the frequency tends to +∞(this is called a strictly proper ﬁlter). The
relation between the input i(t) and the output o(t) is then assumed to be

Numerical Calculus and Simulation
73
described by a constant coeﬃcient linear diﬀerential equation. This system
can be represented with the use of state equations as follows:



dx(t)
dt
= Ax(t) + bi(t)
o(t) = cT x(t) + di(t)
(4.1)
where b, c and x(t) are n×1 vectors and d is a scalar, equal to zero. A = [aij]
is an n×n matrix with its [aij] time-independent. x(t) is called the state vector
of this representation, which is far from being the only possible one.
It can be shown that the solution to the ﬁrst of the system’s equations is:
x(t) = eA(t−t0)x(t0) +
∫t
t0
eA(t−u)bi(u)du
(4.2)
where the matrix exponential (the MATLAB® function expm) is given by:
eAt = I + t
1!A + t2
2!A2 + · · · + tn
n!An + · · ·
Given this deﬁnition, you can check that:
deAt
dt
= AeAt = eAtA
and:
A
∫t
0
eAudu =
∫t
0
eAudu × A =
(
eAt −I
)
The input-output relation is obtained by applying the Laplace transform to
the system (4.1). We get:
O(s) = cT (sI −A)−1bI(s)
(4.3)
where I(s) and O(s) are the Laplace transforms of i(t) and o(t) respectively.
Example 4.1 (Second order systems) Consider a continuous-time system,
with the input i(t) and the output o(t), both scalar, described by the diﬀerential
equation:
d2o(t)
dt2
+ a1
do(t)
dt
+ a2o(t) = i(t)
(4.4)
Let:
x(t) =
[
o(t)
do(t)
dt
]T

74
Digital Signal and Image Processing using MATLAB®
With this choice of the state vector, equation (4.4) leads us to a state
representation:









dx(t)
dt
=

0
1
−a2
−a1

x(t) +

0
1

i(t)
o(t) = [1
0]x(t)
similar to that of (4.1). We have to check that the input-output relation is:
O(s) =
1
s2 + a1s + a2
I(s)
by applying the Laplace transform to (4.4) (the initial conditions are assumed
to be equal to zero) or by (4.3).
Exercise 4.2 illustrates the implementation, in the particular case of a simu-
lation based on a state representation. Such a simulation is particularly useful
in automatic control where the ﬁlter’s output (called a compensator in this
context) is applied to a continuous-time process through a digital-to-analog
converter.
Exercise 4.2 (Simulation in the presence of a ZOH) (see p. 200)
We wish to simulate the behavior of a continuous-time system described by
a state representation. The input signal is obtained with a ZOH (Zero-Order
Hold) DAC that maintains the input value during the sampling period (Figure
4.2).
i(kT)
Continuous
time
system
DAC
i(t)
o(t)
Figure 4.2 – Continuous-time system fed through a ZOH DAC
1. Give, based on expression (4.2), the relation between the state vector at
the time (k + 1)T and the state vector at the time kT. Show that we can
ﬁnd a relation similar to:
x((k + 1)T) = Φ(T)x(kT) + i(kT)Ψ(T)b
where Ψ(T) is obtained as a k-independent integral of Φ(t) = eAt. Notice
that the integral
∫t
0 eAudu does not require the calculation of A−1, the

Numerical Calculus and Simulation
75
invertibility of which is not certain. It can actually be calculated directly
by taking the exponential exp(Aet) with:
Ae =
[A
b
0T
0
]
2. Give x(t) as a function of Φ(t) and Ψ(t), for t ∈(kT, (k + 1)T).
3. Consider the ﬁlter deﬁned by the diﬀerential equation (4.5):
d2o(t)
dt2
+ 1.4do(t)
dt
+ o(t) = 1(t ∈[0, +∞))
(4.5)
with the unit step as its input. Starting with the initial conditions:
o(0) = 0 do(t)
dt

t=0
= 0
simulate the response to the unit step.
Notice that, in this exercise, the unit step response given in question 3 is
correct. This means that the calculated values of the output sequence coincide
with the values o(nT) for all T.
Any other technique involving a transfor-
mation (Euler or bilinear) on the continuous-time system can only yield an
approximation.
Exercise 4.3 (Non-minimal system) (see p. 202)
The use of linear, time-invariant, continuous-time models in the form of state
representations has led us the following expressions:















A =


−11/4
−11/8
−5/4
27/4
11/8
21/4
15/8
19/16
5/8


b =


1
−1
−1/2

, cT =
[3/8
1/2
−1/4]
, d = 0
(4.6)
1. We wish to “digitally” simulate the response of this system when fed by
a unit step through a ZOH. Perform this simulation using the results
of the previous exercise, for several sampling values and for a minimum
simulation duration of 10 s.
2. Observe the evolution of the ||.||∞norm of the state vector during the
simulation. What conjecture can be made concerning the stability?

76
Digital Signal and Image Processing using MATLAB®
3. Find the transfer function (Laplace transform) associated with the system
(4.6). What can be said of the stability? Comment on the role played
by initial conditions in a system’s behavior (we have the same result in
continuous and discrete time cases).
4. Perform the simulation for a period of about 3 minutes. What happens?
What is this phenomenon caused by? It can be veriﬁed that the discrete
TF can be calculated by poly(phi-psib*c')/poly(phi) - 1.
4.2
Solving of ordinary diﬀerential equations
(ODEs)
So-called ordinary diﬀerential equations of order n are deﬁned by an equation
such that:
F
(
x, y, y′, . . . , y(n))
= 0
(4.7)
where y is a vector in normed vector space E, F is continuous over an opening
in R × En+1.
4.2.1
Conversion from continuous to discrete time
The conversion from a continuous to a discrete regime can be made by way
of a bilinear transformation. Control engineers use the term “Tustin method”
to speak of that conversion. The Tustin method involves replacing s in the
transfer function (Laplace transform) with (2/T)(1 −z−1)/(1 + z−1), where
T is the computation step and z−1 is the delay operator. This approximation
corresponds to the approximate computation of an integral by the trapeze
method. Consider the linear equation:
τ dy
dt + y(t) = x(t)
(4.8)
with which the following transfer function is associated:
G(s) =
1
1 + τs
(4.9)
Changing the variables gives:
H(z) =
T(1 + z−1)
T + 2τ + (T −2τ)z−1
(4.10)
which yields the recurrent equation:
(T + 2τ)yn + (T −2τ)yn−1 = Txn + Txn−1
(4.11)
It can be implemented in the following simple way:

Numerical Calculus and Simulation
77
function ctime2dtime()
% continuous time to discrete time
%=====
tau=0.6; T=.03; t=(0:T:7);
%===== discrete time linear system
numd=[T,T], dend=[(T+2*tau),(T-2*tau)]
%=====
[myinp]=f(t);
y=filter(numd,dend,myinp);
clf, plot(t,myinp,'r',t,y,'b'), grid
end
%===== input
function myinp=f(t)
f0=0.5; y=sin(2*pi*f0*t); myinp=sign(y);
end
or, using the function nbilin (see [1]):
function [B,A]=nbilin(pol,Ts)
%!==================================================!
%! Bilinear transform of a polynomial
!
%! SYNOPSIS: [B,A]=NBILIN(pol,Ts)
!
%!
pol = polynomial (decreasing powers of s)
!
%!
= a0 s^n+a1 s^(n-1)+ ... +aN
!
%!
Ts
= sampling period
!
%!
B,A = numerator and denominator of the result !
%!==================================================!
if nargin<2, Ts=1; end
NX=[1 -1]*2/Ts; DX=[1 1];
nP=length(pol); PP=zeros(nP,1); PP(:)=pol;
B=pol(1); A=[1];
for k=2:nP
A=conv(A,DX); B=conv(B,NX) + pol(k)*A;
end
with the test program:
%===== testbilin.m
function testbilin()
tau=.6; Ts=.03; t=(0:Ts:7);
num=[1]; den=[tau,1];
%===== bilinear transform
[Bn,An]=nbilin(num,Ts); [Bd,Ad]=nbilin(den,Ts);
B=conv(Bn,Ad); A=conv(An,Bd);
%===== simulation
myinp=f(t); y=filter(B,A,myinp);
plot(t,myinp,'-r',t,y,'-b'), grid
end
%=====
function myinp=f(t)
f0=0.5; y=sin(2*pi*f0*t); myinp=sign(y);

78
Digital Signal and Image Processing using MATLAB®
end
The transformation may be modiﬁed somewhat. The modiﬁcation is known
as pre-warping, and is performed as follows:
2
T
1 −z−1
1 + z−1 −→1 −z−1
1 + z−1 ×
ωa
tan
(
ωa T
2
)
where ωa is called the critical frequency.
Remarks:
– the functions of continuous-to-discrete-time conversion are available in
toolboxes linked to the ﬁeld of automation engineering;
– the bilinear transform is available in the toolbox robust control with the
function bilin and the toolbox bilinear with bilinear;
– the solution in the above program is obtained by numerical ﬁltering (with
the function filter). This function solves the recurrent equation ob-
tained for the given input. Here, we apply a “square wave” signal, con-
structed by sign(sin(2*pi*f0*t));
– this method remains problematic when there are discontinuities, either
in the system or in the input signal, as is the case here.
4.2.2
Linear case, continuous-time solution
MATLAB® has numerous functions for solving diﬀerential equations, be they
linear or otherwise. The function ode45 is one such tool, and can be used to
solve the diﬀerential equation dy/dt = f(t, y).
Example 4.2 (Use of the function ode45) At the input to an RC ﬁlter, we
apply a “square wave” signal, and we wish to display the output. The transfer
function for the ﬁlter is:
G(s) =
1
1 + RCs =
1
1 + τs
(4.12)
In the linear case, the functions tf2ss and ss2tf can be used to make the
conversion between transfer functions and representations of state:
dv
dt
=
Av + Bx
y
=
Cv + Dx
where v(N × 1) is the state vector, x(M × 1) the input vector and y(P × 1)
the output vector. The matrix A(N × N) is the state matrix.
In the above example, the order of the system is N = 1.

Numerical Calculus and Simulation
79
>> [A,B,C,D]=tf2ss(num,den)
A =
-1.6667
B =
1
C =
1.6667
D =
0
The simulation can be performed as follows:
%===== repcren.m
%===== linear system
num=1; tau=0.6; den=[tau 1];
[A,B,C,D]=tf2ss(num,den); % CT linear system
y0=sqrt(tau)/2;
%===== simulation
[t,y,myinp]=solve_myls(A,B,y0);
%===== drawing
myoutp=C*y+D*myinp;
plot(t,myinp,'r',t,myoutp,'b'), grid on
which uses the function:
function [t,y,myinp]=solve_myls(A,B,y0)
%===== simulation
tspan=[0:.03:7]'; myinp=sign(sin(pi*tspan));
[t,y] = ode45(@myls,tspan,y0);
%==== local function
function yp=myls(t,y)
yp=A*y+B*sign(sin(pi*t));
end
end
The function solve_myls uses a pointer, @myls, to the function describing
the ODE. The input parameters for that pointer, @myls, are a vector y and a
scalar t. We can see that the function myls is local in solve_myls. Thus, it is
able to pass the relevant parameters (here A and B) to myls.
In the program, we have deﬁned an initial state which renders a certain
output myoutp equal to 1/2 at the initial time. Here, we have a ﬁrst order,
and the scalar linking the output to the state can be obtained in the following
manner:
>> [A,B,C,D]% state representation
ans =
-1.6667
1.0000
1.6667
0

80
Digital Signal and Image Processing using MATLAB®
1
0.8
0.6
0.4
0.2
0
−0.2
−0.4
−0.6
−0.8
0
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
7
0.5
−1
Figure 4.3 – Response to a rectangular pulse signal
>> [myoutp(1)/C,sqrt(tau)/2]
ans =
0.3873
0.3873
It is possible to verify that the constant linking the output to the state is
1/√τ. Indeed, the state representation used is:
{
dx
dt = −1
τ x(t) +
1
√τ u(t)
y(t) =
1
√τ x(t)
(4.13)
which is obtained by deﬁning the state by x(t) = √τy(t).
Example 4.3 (Solving of a “predator–prey” system) Consider the follow-
ing system of nonlinear equations, known as Lotka–Volterra equations, to be
solved:
{
dx1
dt = ax1 −bx1x2
dx2
dt = −cx2 + dx1x2
(4.14)
with a, b, c and d > 0. It is possible to simulate the system for a = b = c =
d = 1 as follows:
%===== testode05.m
% fixed points: [0;0] and [c/d;a/b] (omeg^2=ac)
a=1; b=1; c=1; d=1;
xinit=[3/2 1.1 1.1 1 1;3 1.3 1.1 2 4];
t0=0; t=[0:0.1:2.35*pi];
for k=1:size(xinit,2)
y0=xinit(:,k);
[t,y] = solve_lv(a,b,c,d,y0);
plot(y(:,1),y(:,2),'b',y0(1),y0(2),'or'), hold on
end
set(gca,'xlim',[0 4.5],'ylim',[0 4.5]), grid on; hold off

Numerical Calculus and Simulation
81
where the function solve_lv is:
function [t,y]=solve_lv(a,b,c,d,y0)
tspan=[0:0.1:2.35*pi]';
[t,y] = ode45(@lv,tspan,y0);
%==== Lotka-Volterra equation
function ydot=lv(t,y)
ydot=zeros(2,1);
ydot(1)=a*y(1)-b*y(1)*y(2);
ydot(2)=-c*y(2)+d*y(1)*y(2);
end
end
We obtain Figure 4.4 for several initial values.
0
1
2
3
4
Figure 4.4 – Phase trajectories for various initial values
4.2.3
Remarks on the Runge–Kutta methods
Here, again, the problem is to calculate the solution to dy/dt = f(t, y) by esti-
mation of y(kT + T) as a function of y(kT). The Runge–Kutta (RK) methods
pq [5] consist of:
1. Estimating the value of the function y(t) at intermediary points kT + h1,
kT + h2, . . ., kT + hq = (k + 1)T (h0 = 0 and hq = T and the hn are not

82
Digital Signal and Image Processing using MATLAB®
necessarily distinct), in the range [kT, (k + 1)T], as follows:
y(kT + h1)
=
y(kT) + a1,0f(kT, y(kT))
y(kT + h2)
=
y(kT) + a2,0f(kT, y(kT)) + a2,1f(kT + h1, y(kT + h1))
...
y(kT + hq)
=
y(kT) + aq,0f(kT, y(kT)) + · · ·
+aq,q−1f(kT + hq−1, y(kT + hq−1))
given that y(kT + hq) = y((k + 1)T) is what we are seeking to estimate.
2. Ensuring that the p-order Taylor expansions are veriﬁed at all those in-
termediary points (kT + hn):
y(kT + hn) = y(kT) + hn ˙y(kT) + · · · + hp
n
p! y(p)(kT) + O(hp+1
n
)
RK22 or RK-2 algorithms
The application to the case p = q = 2 begins by writing:
y(kT + h1)
=
y(kT) + a1,0f(kT, y(kT)) = y(kT) + h1f(kT, y(kT))
y(kT + h2)
=
y(kT) + a2,0f(kT, y(kT)) + a2,1f(kT + h1, y(kT + h1))
=
y(kT) + a2,0
dy
dt (kT) + a2,1
dy
dt (kT + h1)
=
y((k + 1)T)
The ﬁrst expression is already in the form of a ﬁrst-order Taylor expansion.
The second should give:
y(kT + h2)
=
y(kT) + T ˙y(kT) + T 2
2 y(2)(kT) + O(T 3)
We have:
d2y
dt2 = df(t, y)
dt
= ∂f
∂t + ∂f
∂y
dy
dt
dy
dt (kT + h1) = dy
dt (kT) + h1
d2y
dt2 (kT) + h2
1
2
d3y
dt3 (kT) + · · ·
and:
y((k + 1)T)
=
y(kT) + a2,0
dy
dt (kT) + a2,1
(dy
dt (kT) + h1
d2y
dt2 (kT) + · · ·
)
=
y(kT) + T dy
dt (kT) + T 2
2
d2y
dt2 (kT) + T 3
6
d3y
dt3 (kT) + · · ·

Numerical Calculus and Simulation
83
By identiﬁcation we obtain:
a2,0 + a2,1 = T and a2,1h1 = T 2
2
⇒
a2,1 = T 2
2h1
and a2,0 = T −T 2
2h1
This ﬁnally gives us:
y(kT + h1)
=
y(kT) + h1f(kT, y(kT))
y(kT + T)
=
y(kT) +
(
T −T 2
2h1
)
f(kT, y(kT)) . . .
+ T 2
2h1
f(kT + h1, y(kT + h1))
There are three classic versions of the algorithm: improved tangent algo-
rithm for h1 = T/2, Euler–Cauchy for h1 = T and Heun for h1 = 2T/3.
Exercise 4.4 (RK-2 solver and ode15i function) (see p. 203) Returning
to the example 4.3:
1. process the diﬀerential equation from example 4.4 using Heun’s algorithm,
and compare the result to the solution given by ode45.
2. instead of ode45, use ode15i to perform the same process.
RK44 or RK-4 algorithms
Let us begin with an example relating to the widely-used “M = 4th-order
Runge–Kutta method”. The presentation of the algorithm is slightly diﬀerent
from what we have seen before.
Example 4.4 (Fourth-order Runge–Kutta, or RK-4, method) Consider
the scalar case. We can write:
y(kT + T)
=
y(kT) +
∫1
0
f(kT + uT, y(kT + uT))Tof
=
y(kT) + T
N
∑
1
αnf(kT + unT, y(kT + unT))
(4.15)
with ∑N
1 αn = 1. In certain conditions the error committed by the sum (4.15)
is at T M+1.
In the program RK4.m, we solve the equation ˙y(t) = y2(t) −y sin(t) + cos(t)
using two fourth-order methods:

84
Digital Signal and Image Processing using MATLAB®
1. In the basic method we estimate the derivative at kT + T/2 twice. We
successively calculate:
– the derivative k1 at point (kT, y(kT)), so that k1 = f(kT, y(k)),
– the derivative k2 at point ((kT + T/2), y(k) + k1T/2) using the pre-
vious result, so that k2 = f((k + 1/2)T, y(k) + k1T/2),
– the derivative k3 at point ((kT + T/2), y(k) + k2T/2) using the pre-
vious result, so that k3 = f((k + 1/2)T, y(k) + k2T/2),
– the derivative k4 at point kT + T using the previous result, so that
k4 = f(kT + T, y(k) + k3T),
Then we estimate a weighted-sum derivative of k1, k2, k3 and k4 by
[1]
/6
(in the initial presentation, it is the series aq,0, aq,1, . . ., aq−1,q−1), from
which we deduce an estimation for y(k + 1).
2. In the second so-called “Kutta” method, we calculate:
– the derivative k1 at point kT, y(kT) so that k1 = f(kT, y(kT)),
– the derivative at point (k + 1/3)T, y((k + 1/3)T) using the previous
result, so that k2 = f((k + 1/3)T, y(k) + k1T/3),
– the term k3 by f((k + 2/3)T, y(k) −k1T/3 + k2T),
– the term k4 by f((k + 1)T, y(k) + k1T −k2T + k3T)
with a weighting function
[1
3
3
1]
/8, from which we deduce an
estimation for y(k + 1).
function RK4()
y0=0; h=.1; t=[0:h:2*pi]; Lt=length(t);
[t,y]=ode45(@fexple,t,y0); y1=y;
clf, subplot(211), plot(t,y1); grid on, hold on
%===== solution standard RK4
hs2=h/2; hw=[1;2;2;1]/6;
y(1)=y0;
for k=1:Lt-1
k1=fexple(t(k),y(k));
k2=fexple(t(k)+hs2,y(k)+hs2*k1);
k3=fexple(t(k)+hs2,y(k)+hs2*k2);
k4=fexple(t(k)+h,y(k)+h*k3);
% weighting
y(k+1)=y(k)+[k1,k2,k3,k4]*hw*h;
end
plot(t,y,'ro'); y2=y;
%===== solution RK4 - 3/8 rule
hw=[1;3;3;1]/8;
y(1)=y0; hs3=h/3; dhs3=2*hs3;

Numerical Calculus and Simulation
85
for k=1:Lt-1
k1=fexple(t(k),y(k));
k2=fexple(t(k)+hs3,y(k)+k1*hs3);
k3=fexple(t(k)+dhs3,y(k)-k1*hs3+k2*h);
k4=fexple(t(k)+h,y(k)+k1*h-k2*h+k3*h);
% weighting
y(k+1)=y(k)+[k1,k2,k3,k4]*hw*h;
end
plot(t,y,'xk'), legend('ode45','standard RK4','RK4 3/8')
subplot(212), plot(t,y1'-y2','b',t,y1'-y','r');
legend('Diff. ode45-RK4 standard','Diff. ode45-RK4 3/8')
grid on
end
%=====
function ydot=fexple(t,y)
ydot=y^2-y*sin(t)+cos(t);
end
Modiﬁcation of the order of the error and estimation of the derivative
Here, we consider(1) a quantity A whose approximate expression An−1(ε) of
order (n −1) is given by:
A
=
An−1(ε) + k0εn + k1εn+1 + · · ·
=
An−1(ε) + k0εn + O(εn+1)
(4.16)
The Richardson method – also known as extrapolation – enables us to obtain
an approximate n-order expression which we will denote as An(ε) or Rn(ε, r)
[34].
Consider the parameter r; we can write (4.16) as follows:
A = An−1(ε/r) + k0 × (ε/r)n + O(εn+1)
(4.17)
(rn×((4.17))−((4.16))) gives us:
(rn −1)A = rnAn−1(ε/r) −An−1(ε) + O(εn+1)
(4.18)
so:
A = rnAn−1(ε/r) −An−1(ε)
rn −1
+ O(εn+1)
(4.19)
The expression of A uses the approximation Rn(ε, r) of order n:
Rn(ε, r) = rnAn−1(ε/r) −An−1(ε)
rn −1
(4.20)
at the cost of the calculation of An−1(ε/r).
(1)http://www.math.ubc.ca/~feldman/m256/richard.pdf

86
Digital Signal and Image Processing using MATLAB®
Example 4.5 (Estimation of the derivative) The derivative at a point t0
can be estimated on the basis of the Taylor expansion:
f(t0 + ε)
=
f(t0) + ε ˙f(t0) + ε2
2
¨f(t0) + O(ε3)
⇒
˙f(t0) = f(t0 + ε) −f(t0)
ε
−ε
2
¨f(t0) + O(ε2)
The estimation A0(ε) = f(t0+ε)−f(t0)
ε
corresponds to the Euler approxima-
tion. Take r = 2. In this case:
R1(ε, 2)
=
2A0(ε/2) −A0(ε)
(= A2(ε))
=
2f(t0 + ε/2) −f(t0)
ε/2
−f(t0 + ε) −f(t0)
ε
=
4f(t0 + ε/2)
ε
−3f(t0)
ε
−f(t0 + ε)
ε
In the following order:
R2(ε, 2)
=
1
3 [4A1(ε/2) −A1(ε)]
=
1
3 (8A0(ε/4) −6A0(ε/2) + A0(ε))
The approximate value of the derivative is then obtained by:
R2(ε, 2)
=
8
3A0(ε/4) −2A0(ε/2) + 1
3A0(ε)
=
8
3
f(t0 + ε/4) −f(t0)
ε/4
−2f(t0 + ε/2) −f(t0)
ε/2
+ f(t0 + ε) −f(t0)
3ε
=
−7
εf(t0) + 32
3εf(t0 + ε/4) −4
εf(t0 + ε/2) + 1
3εf(t0 + ε)
R3(ε, 2)
=
1
7 [8A2(ε/2) −A2(ε)]
=
1
21 [64A0(ε/8) −56A0(ε/4) + 14A0(ε/2) −A0(ε)] ...
We can summarize the above in the table:
(r = 2)
ε
16
ε
8
ε
4
ε
2
ε
n = 1
0
0
0
2
−1
n = 2
0
0
8
3
−2
1
3
n = 3
0
64
21
−8
3
2
3
−1
21
n = 4
1024
315
−64
21
8
9
−2
21
1
315

Numerical Calculus and Simulation
87
and for the derivative:
(r = 2)
0
ε
16
ε
8
ε
4
ε
2
ε
n = 0
−1
ε
0
0
0
0
1
ε
n = 1
−3
ε
0
0
0
4
ε
−1
ε
n = 2
−7
ε
0
0
32
3ε
−4
ε
1
3ε
n = 3
. . .
The calculation of the coeﬃcients for the derivative derivestimc and con-
struction calcrn and derivestimc of the corresponding tables are:
%===== estimderiv.m
n=2; h=1; r=2;
mcoff=derivestimc(n,h,r);
disp(mcoff)
function [Rn,ve]=calcrn(n,h,r)
%!=================================!
%! SYNOPSIS [Rn,ve]=CALCRN(n,h,r)
!
%! n
= expansion order
!
%! h
= step value
!
%! r
= parameter used to divide h !
%! Rn = coefficients array
!
%! ve = vector [h/r^n ... h/r h]
!
%!=================================!
Rn=zeros(n,n+1);
vr=ones(1,n+1) ./ r.^(n:-1:0); ve=h * vr;
Rn(1,n)=r; Rn(1,n+1)=-1;
for k=2:n
Rn(k,:)=(r^k * [Rn(k-1,2:end),0]-Rn(k-1,:))/(r^k-1);
end
function [mcoff,ve]=derivestimc(n,h,r)
%!================================================!
%! SYNOPSIS [mcoff,ve]=DERIVESTIMC(n,h,r)
!
%! n
=
expansion order
!
%! h
=
step value
!
%! r
=
parameter used to divide h
!
%! mcoff = coefficients for derivative estimation !
%! mcoff(1)*f(nT) + mcoff(2)*f(nT+h/r^n)+...
!
%! ve = vector [h/r^n ... h/r h]
!
%!================================================!
[Rn,ve]=calcrn(n,h,r);
Re=Rn(n,:) ./ ve;
tt=toeplitz(zeros(n+1,1),[0 1 zeros(1,n)]);
tt(:,1)=-1; mm=(Re.' * ones(1,n+2)) .* tt;
mcoff=sum(mm,1);

88
Digital Signal and Image Processing using MATLAB®
4.3
Systems of equations and zero-seeking
4.3.1
Zeros of a function using the Newton method
The principle of the method is illustrated by Figure 4.5. Given the graph G of
y = f(x), we construct a series of values xn in the following way: we consider
the tangent (∆n) in xn at G and its intersection with the Ox axis which deﬁnes
xn+1. This process is repeated iteratively until |xn+1 −xn| < ε.
Figure 4.5 – Diagram of recurrence in the Newton method, moving from the point
Mn to the point Mn+1
The convergence of the method depends on several conditions related to
the function h(x) = x −f(x)/ ˙f(x) and on the choice of the initial value x0.
We choose to take that value in the vicinity (V ) containing a solution to the
equation, ensuring that the function y is doubly derivable and with non-null
ﬁrst and second derivatives.
Exercise 4.5 (Newton’s method) (see p. 205)
Here we consider the case of the polynomials y = f(x). Given x = xn, we use
the notation yn = f(xn) and y′
n for the derivative at x = xn:
1. give the recurrence relation linking xn+1 to xn;
2. we take a polynomial myp by its coeﬃcients:
myp=[1,(-4-sqrt(2)),(3+4{*}sqrt(2)),-3{*}sqrt(2)];
Write a program calculating the roots of that polynomial using the New-
ton method. Note that the polynomial division is performed by the func-
tion deconv and the derivation by polyder. If the derivative becomes
null during the course of the execution of the program, we will simply
take a diﬀerent initial condition.

Numerical Calculus and Simulation
89
4.3.2
Roots of a polynomial with the Newton–Raphson
method
The Newton–Raphson method applies to zero-seeking for F(z) = 0. We will
use it here to calculate the roots of a polynomial P(z). The principle is as
follows: we take an initial value z = x0 + jy0. Then we vary the value of z
by ∆z = ∆x + j∆y, ∆x and ∆y, given by Newton’s equations, so that P(z)
approaches the origin (see Figure 4.6). The stop test is given by |∆x|+|∆y| < ε.
R
I
Figure 4.6 – Principle of the method
Thus, consider a polynomial P(z) of degree D:
P(z) =
D
∑
n=0
a(n)zn
(4.21)
The steps involved in the method are as follows:
1. During the calculation, zn is evaluated by recurrence. We set zn = (x +
jy)n = Xn + jYn and the Xn and Yn are obtained by the recurrences:
Xn = xXn−1 −yYn−1
and
Yn = xYn−1 + yXn−1
(4.22)
2. We set P(z) = U + jV . We have:
P(z) =
D
∑
n=0
a(n)zn =
D
∑
n=0
a(n)Xn + j
D
∑
n=0
a(n)Yn
(4.23)
from which we deduce:
∂U
∂x = ∂V
∂y =
D
∑
n=1
na(n)Xn−1
(4.24)
∂V
∂x = −∂U
∂y =
D
∑
n=1
na(n)Yn−1
(4.25)

90
Digital Signal and Image Processing using MATLAB®
3. Newton’s equations are obtained with the proviso that we are seeking to
bring the point (Uk, Vk) closer to the origin (Figure 4.6):
{
∆U = −U = ∂U
∂x ∆x + ∂U
∂y ∆y
∆V = −V = ∂V
∂x ∆x + ∂V
∂y ∆y
⇒
[∆x
∆y
]
= −
[
∂U
∂x
∂U
∂y
∂V
∂x
∂V
∂y
]−1 [U
V
]
(4.26)
and from expressions (4.24) and (4.25):
∆x = −U ∂U
∂x + V ∂V
∂x
( ∂U
∂x
)2 +
( ∂V
∂x
)2
and
∆y =
U ∂V
∂x −V ∂U
∂x
( ∂U
∂x
)2 +
( ∂V
∂x
)2
(4.27)
The expression (4.26) extends to all equations, linear or nonlinear, as we
will see in section 6.5.
Exercise 4.6 (The Newton–Raphson method) (see p. 206)
We apply the Newton–Raphson method to the calculation of the roots of a
polynomial:
1. Write a function to calculate a root [xc,yc,cErr] = CalcXY(pol,x0,y0,
nblps) where pol is the polynomial given in the form of a vector of its
coeﬃcients (in order of decreasing power), (x0,y0) is the starting point,
teps the ε the end-of-loop test, nblps the maximum acceptable number of
loops, (xc,yc) the solution found and cErr an error code (e.g. denoting
an “excessive” number of iterations);
2. Write a function to calculate all the roots of a polynomial.
4.3.3
Systems of nonlinear equations
The function fzero, which is available in the basic version of MATLAB®, can
be used to solve the equation F(x) = 0 when x is scalar. fsolve from the
optim toolbox is able to solve F(x) = 0 in the multivariable case.
The call syntax for fzero is:
[x,fval,exitflag,output] = fzero(@myfunc,x0,options)
Example 4.6 (Solving of nonlinear equations)
Consider the following equation to be solved:
x + 1 + atan(x) = 0
(4.28)
Type:

Numerical Calculus and Simulation
91
%===== explefzero.m
x0=-1; % init value
%==== using an anonymous function
[x,fval,exitflag,output]=fzero(@(x)x+1+atan(x),x0)
The execution of the program explefzero gives:
>> explefzero
x =
-0.5203
fval =
5.5511e-17
exitflag =
1
output =
intervaliterations: 10
iterations: 6
funcCount: 26
algorithm: 'bisection, interpolation'
message: 'Zero found in the interval [-0.36, -1.45255]'
Instead of giving an initial value, it is possible to deﬁne an interval within
which a solution is sought.
Thus, in the program explefzero2, we seek a
solution between π/4 and π/2:
%===== explefzero2.m
x0=[pi/4 pi/2]; % interval
[x,fval,exitflag,output]=fzero(@(x)x+1-tan(x),x0)
4.4
Interpolation
The word interpolation is used to denote many diﬀerent types of operations.
We sometimes also speak of curve ﬁtting. In signal-processing, interpolation
is generally performed by zero-insertion, followed by low-pass ﬁltering. The
series obtained does not pass through the same points as the original series
S = {. . . (x0, f(x0)), (x1, f(x1)), . . .}. However, the spectra of the trajecto-
ries are practically identical. There are a wide range of other methods. The
most conventional techniques use polynomials: Lagrange basis polynomials,
Newton’s equations, cubic spline interpolation, etc. In this case, the trajectory
obtained passes through the points of S. Functions other than polynomials can
also be used: trigonometric functions, complex exponentials, Gaussians, etc.
Here, we are going to examine a method using a rational fraction written
in the form of a continued fraction.

92
Digital Signal and Image Processing using MATLAB®
4.4.1
Thiele’s interpolation
Principle
Knowing a set of N + 1 points of a trajectory given in the form {(x0, f(x0)),
(x1, f(x1)), . . ., (xN, f(xN))}, we seek to approximate f(x) as closely as possi-
ble. We write S = {x0, x1, . . . , xN}. The so-called continued fractions decom-
position or interpolation by rational interpolants proposed by Thiele [37] is of
the form:
f(x) = f(x0) +
x −x0
ρ1(x0, x1) +
x −x1
ρ2(x0, x1, x2) +
x −x2
ρ3(x0, x1, x2, x3) + . . .
(4.29)
We use the notation ρ0(x) = f(x) = y and ρn(xi, . . . , xi+n) = ρn(xi:i+n)
where ρn(.) = 0 if n < 0. We have the relation:
ρn(x0:n) =
xn−1 −xn
ρn−1(x0:n−1) −ρn−1(x0:n−2, xn)
(4.30)
and the fraction is constructed by the recurrence relation:
F0(x)
=
f(x)
Fn(x)
=
ρn−1(x0:n−1) + x−xn−1
Fn+1(x)
FN(x)
=
1
(4.31)
Thus, we have:
F0(x)
=
ρ0(x0),
F1(x)
=
ρ0(x0) + x −x0
F2(x) ,
F2(x)
=
ρ1(x0, x1) + x −x1
F3(x) ,
F3(x)
=
ρ2(x0, x1, x2) + x −x2
F4(x) . . .
The ﬁrst values of the ρn(.) are:
ρ1(x0, x1)
=
x0 −x1
y0 −y1
,
ρ1(x0, x2)
=
x0 −x2
y0 −y2
ρ2(x0, x1, x2)
=
x1 −x2
ρ1(x0, x1) −ρ1(x0, x2)

Numerical Calculus and Simulation
93
Indications regarding the method
Consider the rational fraction:
R(M,N)(x) = a0 + a1x + · · · + aMxM
b0 + b1x + · · · + bNxM = pM(x)
qN(x)
(4.32)
We take a series of points {(x0, y0), (x1, y1), . . ., (x2N, y2N))}. The problem
at hand is that of ﬁnding R(N,N)(x) such that:
R(N,N)(xk) = pN(x)
qN(x) = yk for k = 0, . . . , 2N
(4.33)
We consider the ﬁrst point in the series, and we have:
pN(x0)
qN(x0) = y0
and we write pN(x)/qN(x) in the form:
pN(x)
qN(x) = y0 + (x −x0)pN−1(x)
qN(x)
= y0 +
x −x0
ρ1(x0, x)
ρ1(x0, x) is called the inverse diﬀerence. If we attribute to x the successive
values xk, k = 1 . . . 2N, we can write:
xk −x0
yk −y0
=
qN(xk)
pN−1(xk) = ρ1(x0, xk)
We can continue with x1:
qN(x1)
pN−1(x1) = ρ1(x0, x1) ⇒
qN(x)
pN−1(x) = ρ1(x0, x1) + (x −x1)qN−1(x)
pN−1(x) (4.34)
If we substitute x = xk, k = 2 . . . 2N back into (4.34):
qN(xk)
pN−1(xk) = ρ1(x0, xk) = ρ1(x0, x1) + (xk −x1)qN−1(xk)
pN−1(xk)
(4.35)
⇒pN−1(xk)
qN−1(xk) =
xk −x1
ρ1(x0, xk) −ρ1(x0, x1) = ρ2(x0, x1, xk)
We continue, noting that:
pN−1(x2)
qN−1(x2) −ρ2(x0, x1, x2) = 0 ⇒pN−1(x)
qN−1(x) = ρ2(x0, x1, x2)+(x−x2)pN−2(x)
qN−1(x)
which enables us to obtain the construction scheme (4.29). It should be noted,
however, that the values of ρn must not reach zero. If they do, we say that the
associated points are unattainable and that interpolation is not possible, taking
the series of points S in the order in which it is given. Werner’s algorithm
[42] provides a solution which, in certain cases, is able to yield a permutation
to help calculate the fraction.

94
Digital Signal and Image Processing using MATLAB®
Programming
For programming, we organize the calculation of the ρ(i)
n values using a table:
x
ρ0(.)
ρ1(.)
ρ2(.)
. . .
x0
y0
x1
y1
ρ1(x0, x1)
x2
y2
ρ1(x0, x2)
ρ2(x0:2)
x3
y3
ρ1(x0, x3)
ρ2(x0:1, x3)
. . .
...
...
...
xN
yN
ρ1(x0, xN)
ρ2(x0:1, xN)
. . .
. . .
ρN(x0:N)
then we calculate the Fk values using the relation (4.31).
Example 4.7 (Thiele decomposition) We consider the points (0, 0), (1, 1/2),
(2, 2), (3, 1). The computed table gives us:
x
y
ρ(x0; xk)
ρ(x0, x1; xk)
ρ(x0, x1, x2; xk)
0
0
ϕ
ϕ
ϕ
1
1/2
2
ϕ
ϕ
2
2
1
−1
ϕ
3
1
3
2
1/3
and the approximation function:
f(x) =
2x
4 + (x −1)(3x −8)
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
−0.5
0
0.5
1
1.5
2
2.5
Figure 4.7 – Interpolation by “rational interpolants”

Numerical Calculus and Simulation
95
%===== testth01.m
clear all
xk=(0:3)'; yk=[0;1/2;2;1]; N=length(xk);
%===== calculating of rho constants
mt=initthiele(xk,yk); dmt=diag(mt,1); disp(mt)
%===== verification
[yk,(2*xk) ./ (4+(xk-1).*(3*xk-8))]
x=[0:.05:3]; fx=(2*x) ./ (4+(x-1).*(3*x-8));
%===== calculating of interpolant function
Nx=length(x); ye=zeros(Nx,1);
for k=1:Nx
ye(k)=calcthiele(x(k),xk,dmt);
end
plot(x,fx,'-r',x,ye,'.y',xk,yk,'xk'), grid
axis([-.5,3.5,-.5,2.5])
function mt=initthiele(xk,yk)
%!======================================================!
%! Calculating rho functions for thiele's interpolation !
%! (xk,yk) are the original points to be
!
%! interpolated by a continued fraction
!
%!======================================================!
N=length(xk); mt=ones(N,N+1)*NaN;
mt(:,1)=xk; mt(:,2)=yk;
mt(2:N,3)=(mt(1,1)-mt(2:N,1))./(mt(1,2)-mt(2:N,2));
for ll=3:N
for cc=4:ll+1
mt(ll,cc)=...
(mt(ll,1)-mt(cc-2,1))/(mt(ll,cc-1)-mt(cc-2,cc-1));
end
end
function y=calcthiele(x,xk,dmt)
%!=============================================!
%! Calculating of the continued fraction for x !
%! x
= value for calculating of f(x)
!
%! xk
= vector of the original abscissa
!
%! dmt = rho(x0,x1,...xN) constants
!
%!=============================================!
N=length(dmt); y=1;
for k=N:-1:1, y=dmt(k)+(x-xk(k))/y; end
4.4.2
Another decomposition in continuous fractions
Look at the function f(x) and the series S = {x1, x2, . . . , xN} which we write
as x1:N. We suppose that the xk values are all distinct, that f(x) does not
cancel within S and that it is derivable over S . We use the notation ρ0(x) =
f(x) and write [25]:
f(x) =
ρ0(x1)
1 −(x −x1)ρ1(x1, x) ⇒ρ1(x1, x) =
1
ρ0(x)
ρ0(x) −ρ0(x1)
x −x1

96
Digital Signal and Image Processing using MATLAB®
We note that:
ρ1(x1, x1) =
1
f(x1)
df(x)
dx

x=x1
=
1
ρ0(x1)
dρ0(x)
dx

x=x1
We then posit:
ρ1(x1, x) =
ρ1(x1, x2)
1 −(x −x2)ρ2(x1:2, x)
⇒ρ2(x1:2, x) = ρ1(x1, x) −ρ1(x1, x2)
x −x2
1
ρ1(x1, x)
noting that:
ρ2(x1:2, x1)
=
ρ1(x1, x1) −ρ1(x1, x2)
x1 −x2
1
ρ1(x1, x1)
ρ2(x1:2, x2)
=
dρ1(x1, x)
dx

x=x2
1
ρ1(x1, x2)
More generally speaking:
ρn−1(x1:n−1, x)
=
ρn−1(x1:n−1, xn)
1 −(x −xn)ρn(x1:n, x)
⇒ρn(x1:n, x)
=
ρn−1(x1:n−1, x) −ρn−1(x1:n−1, xn)
x −xn
1
ρn−1(x1:n−1, x)
By way of example, let us consider the expression of f(x) for N points:
f(x) =
ρ0(x1)
1 −(x −x1)
ρ1(x1, x2)
1 −(x −x2)
ρ2(x1:2, x3)
1 −(x −x3)
...
. . .
ρN−2(x1:N−2,xN−1)
1−(x−xN−1)ρN−1(x1:N−1,x)
To
obtain
f(x),
we
replace
ρN−1(x1:N−1, x)
with
the
quantity
ρN−1(x1:N−1, xN). ρN multiplied by (x −xN) then gives us the relative er-
ror committed regarding ρN−1 during this approximation.
4.4.3
Natural cubic splines
We consider a series of points {Pk}, k ∈[0, N −1], known as control points,
with aﬃx pk ∈C, and the trajectory “segments” (Sk) between two successive
points Pk and Pk+1 (Figure (4.8)). pk(t) ∈C is the current point in the segment
(Pk −Pk+1), and we use the notation ˙pk(t) and ¨pk(t), respectively, for the
ﬁrst and second derivatives of pk(t).

Numerical Calculus and Simulation
97
Figure 4.8 – Series of points {Pk} and segments of trajectories (Sk)
We suppose that ¨pk(t) can be written:
¨pk(t) = ¨pk(0)(1 −t) + ¨pk(1)t
(4.36)
where t is a parameter ∈[0, 1]. From (4.36), we deduce ˙pk(t) and pk(t).



˙pk(t) = ¨pk(0)
(
t −t2
2
)
+ ¨pk(1) t2
2 + α
pk(t) = ¨pk(0)
(
t2
2 −t3
6
)
+ ¨pk(1) t3
6 + αt + β
(4.37)
We ﬁrst consider the natural cubic splines for which the second derivatives
at the endpoints P0 and PN−1 are null.
We also suppose that the second
derivative remains continuous at each of the points Pk. Thus, we can write
these conditions in the form:



¨p0(0) = 0,
¨pN−2(1) = 0
¨pk(1) = ¨pk+1(0) for k = 0 . . . N −3
(4.38)
Using the fact that pk(0) = pk and pk(1) = pk+1, we deduce α and β and
then the expression for the trajectory pk(t):
pk(t)
=
¨pk(0)
(t2
2 −t3
6 −t
3
)
+ ¨pk(1)
(t3
6 −t
6
)
+ (pk+1 −pk)t + pk
=
(¨pk+1(0) −¨pk(0)) t3
6
+¨pk(0)t2
2 +
(
pk+1 −pk −¨pk(0)
3
−¨pk+1(0)
6
)
t + pk
(4.39)
The continuity conditions (4.38) involve N −2 relations for 2(N −2) un-
knowns ¨pk(0) and ¨pk(1). In order to be able to solve this system, we will express
the continuity of the derivative at points Pk.
˙pk(t) = ¨pk(0)
(
t −t2
2
)
+ ¨pk(1)t2
2 + pk+1 −pk −¨pk(0)1
3 −¨pk(1)1
6 (4.40)

98
Digital Signal and Image Processing using MATLAB®
which gives us:



˙pk(0) = pk+1 −pk −¨pk(0)
3
−¨pk(1)
6
˙pk(1) = pk+1 −pk + ¨pk(0)
6
+ ¨pk(1)
3
(4.41)
The conditions ˙pk(1) = ˙pk+1(0) with the relations (4.38) give:
¨pk+2(0)
6
+ 2¨pk+1(0)
3
+ ¨pk(0)
6
= pk+2 −2pk+1 + pk
(4.42)
All the conditions expressed above can be written in the form (4.43):



























¨p0(0) = 0
¨pN−1(0) = 0
N −2















¨p2(0)
6
+ 2¨p1(0)
3
= p2 −2p1 + p0
¨p3(0)
6
+ 2¨p2(0)
3
+ ¨p1(0)
6
= p3 −2p2 + p1
...
2¨pN−2(0)
3
+ ¨pN−3(0)
6
= pN−1 −2pN−2 + pN−3
(4.43)
Solving this tridiagonal system gives us the second derivatives, thus enabling
us to go on to construct each of the Sk using (4.39).


2/3
1/6
0
0
. . .
0
1/6
2/3
1/6
0
. . .
0
0
...
...
...
...
...
...
...
...
...
0
...
1/6
2/3
1/6
0
. . .
0
0
1/6
2/3




¨p1(0)
¨p2(0)
...
¨pN−3(0)
¨pN−2(0)


=


p2 −2p1 + p0
p3 −2p2 + p1
...
pN−1 −2pN−2 + pN−3


(4.44)
We are led to the solution of 4.44, a linear system Ax = b of N −2 equations
with N −2 unknowns, wherein the matrix A is tridiagonal symmetrical and
positive deﬁnite:
A =


4
1
1
4
1
...
...
...
1
4
1
1
4


/6
(4.45)

Numerical Calculus and Simulation
99
−0
−0
−0
−0
0
0
0
0
Figure 4.9 – Example of a natural cubic spline
and b is constructed by
b=toeplitz(Pk(3:N),[Pk(3) Pk(2) Pk(1)])*[1;-2;1];
Exercise 4.7 (Plot of the natural cubic spline) (see p. 208):
Remark: we consider a series of N pairs (xi, yi) and we note S2[a, b] all of the
twice diﬀerentiable functions deﬁned on [a, b]. Thus [19], the function g that
minimizes on S2[a, b] the quantity:
K
∑
i=1
(yi −g(xi))2 + α
∫
|g′′(u)|2 du
(4.46)
for α > 0 is a natural cubic spline. The criterion to be minimized can be seen
as the Lagrangian of a quadratic problem with a constraint on the quadratic
ﬂuctuations of the second derivative which can be interpreted as a smoothing
constraint (see section (6.3)).
Exercise 4.8 (Plot of natural cubic spline) (see p. 208)
We wish to plot a natural cubic spline passing through the points A(0, 0),
B(0, 2), C(2, 0), D(3, 2) and E(1, 1).
1. Write a function giving the second derivatives on the basis of the control
points A, B, C, D and E.
2. Write a program to plot the associated graph.

100
Digital Signal and Image Processing using MATLAB®
4.5
Solving of linear systems
There are many methods in existence for solving the linear system Ax = b.
Many of the algorithms available exploit the structure of the matrix A. In
signal-processing, the Levinson algorithm, based on the Toëplitz structure of
A, is a well-known example of this. Here, we shall look at iterative methods,
which involve constructing an array x(n) that converges toward the solution x.
Obviously, these methods are not limited to the case of linear systems.
The method entails writing A = M −N; then, with the initial value x(0)
being given, we execute the iteration:
x(n+1)
=
M −1Nx(n) + M −1b
(4.47)
We know that if the array x(n+1) converges toward a limit x(∞), then
that limit satisﬁes the recurrent equation (4.47), and from this we deduce that
Ax(∞) = b.
The convergence of this recurrent equation is linked to the eigenvalues λk
of the matrix M −1N, whose modulus must never exceed 1:
|λk| < 1
(4.48)
The largest of the eigenvalues is called the spectral radius ρ = maxk |λk|.
Let us write the matrix A in the form:
A = D −L −U
(4.49)
where D is diagonal, L is strictly lower triangular and U strictly upper trian-
gular. Using the notation A = [aℓ,c] this is written as follows:
D = [aℓ,ℓ], L = −[aℓ,c] with c < ℓand U = −[aℓ,c] with c > ℓ
4.5.1
Jacobi method
In the Jacobi method, we set M = D and N = L + U:
x(n+1) = Jx(n) + D−1b
(4.50)
and hence J = D−1(L + U).
function [x,rho]=jacobimethod(A,b,x0,stcr0,maxit)
%!====================================================!
%! SYNOPSIS: [x,rho]=JACOBIMETHOD(A,b,x0,stcr0,maxit) !
%! x0
= initial state
!
%! stcr0
= |x(n+1)-x(n)| limit
!
%! maxit =
max number of loops
!
%! x
= solution of Ax=b
!
%! rho
= spectral radius
!
%!====================================================!

Numerical Calculus and Simulation
101
n=size(A,1);
U=-triu(A,1); L=-tril(A,-1); N=U+L;
D=diag(A); Dm1=1./D; Mm1=diag(Dm1);
rho=max(abs(eig(Mm1*N)));
if rho>=1, rho, error('the method is not convergent'); end
x=x0; stcr=1; k=0;
while stcr>stcr0 && k<maxit
xp=(N*x+b).*Dm1; stcr=max(abs(xp-x));
x=xp; k=k+1;
end
4.5.2
Relaxation method
In the relaxation method, we set:
M = D
ω −L and N = 1 −ω
ω
D + U
and hence J =
( D
ω −L
)−1 ( 1−ω
ω D + U
)
.
function [x,rho]=relaxmethod(A,b,x0,w,stcr0,maxit)
%!====================================================!
%! SYNOPSIS [x,rho]=RELAXMETHOD(A,b,x0,w,stcr0,maxit) !
%! x0
= initial xn
!
%! stcr0 = |x(n+1)-x(n)| limit
!
%! maxit =
max number of loops
!
%! w
= (1 for Gauss-Seidel)
!
%! x
= solution of Ax=b
!
%! rho
= spectral radius
!
%!====================================================!
n=size(A,1);
U=-triu(A,1); L=-tril(A,-1); D=diag(A);
N=(1-w)*diag(D)/w+U; M=diag(D)/w-L;
Mm1=inv(M); rho=max(abs(eig(Mm1*N)));
if rho>=1, rho, error('the method is not convergent'); end
x=zeros(n,1); stcr=1; t1=tic; k=0;
while stcr>stcr0 && k<maxit
xp=Mm1*(N*x+b); stcr=max(abs(xp-x));
x=xp; k=k+1;
end
Remarks:
– the Gauss–Seidel method corresponds to the case ω = 1 in the relaxation
method, i.e. M = D −L and N = U :
J = (D −L)−1U
(4.51)

102
Digital Signal and Image Processing using MATLAB®
– the relaxation method entails, at each iteration, calculating a weighted
mean between the result yielded by the Gauss–Seidel method x(n) and
the previous result x(n−1), so that:
x(n) = ωx(n) + (1 −ω)x(n−1)
(4.52)
In matrix form, this is written as:
x(n)
=
ω(D −L)−1 (
Ux(n−1) + b
)
+ (1 −ω)x(n−1)
=
(D −L)−1 [ωU + (1 −ω)(D −L)] x(n−1) + ω(D −L)−1b
Exercise 4.9 (Relaxation method) (see p. 209)
Consider equation (4.44) with the list of points A(0, 0), B(0, 2), C(2, 0),
D(3, 2) and E(1, 1) from exercise 4.8.
1. The symbol ρgs denotes the spectral radius in the Gauss–Seidel method,
whilst ρj is the spectral radius in the Jacobi method. Show that ρ2
j = ρgs.
2. Demonstrate that the convergence of the relaxation method necessitates
that 0 ≤ω < 2.
3. Perform a simulation using the function relaxmethod, viewing the error
and the spectral radius as functions of ω. Accepting that for any positive-
deﬁnite, tridiagonal symmetrical matrix A, the parameter ω ∈]0, 2[ which
minimizes the spectral radius of the matrix Rω is given by:
ω = 2/
(
1 +
√
1 −ρ2
j
)
(4.53)
verify this value on the plots obtained.
4.5.3
Cholesky factorization
Let A be a Hermitian matrix. We seek a factorization of the form L × LH
where L is lower triangular. The uniqueness of this factorization is a necessary
and suﬃcient condition to ensure that M is positive deﬁnite.
By way of example, we shall now present a recursive realization of this
factorization. It can be represented by the calculation scheme:

Numerical Calculus and Simulation
103
Ln × LH
n
=


ℓ11
0
0
. . .
0
ℓ21
ℓ22
0
0
...
...
...
...
...
0
ℓn1
. . .
. . .
. . .
ℓnn


×


ℓ11
ℓ21
. . .
ℓn1
0
ℓ22
ℓn2
...
...
...
...
...
...
0
. . .
. . .
0
ℓnn


∗
=
[ ℓ11
0T
v
Ln−1
]
×
[ ℓ11
vH
0
LH
n−1
]
=
[ ℓ2
11
ℓ11vH
ℓ11v
vvH + Ln−1LH
n−1
]
=
[
a11
AH
(2:n,1)
A(2:n,1)
A(2:n,2:n)
]
From this, we deduce that:



ℓ11 = √a11
v = A(2:n,1)/ℓ11
Ln−1LH
n−1 = A(2:n,2:n) −vvH
It should be noted that, in terms of performances, MATLAB®, like all
interpreters, is ill-suited for recursive programming. Nevertheless, it can be
used to test the algorithm before writing a program to execute it in a more
suitable language, such as C.
Exercise 4.10 (Cholesky factorization) (see p. 214)
Write a function to perform the Cholesky factorization using a recursive algo-
rithm.


Chapter 5
Speech Processing
Speech is an important ﬁeld for the applications of digital signal processing.
This ﬁrst paragraph deals with how transporting and storing a signal can be fa-
cilitated by analyzing and compressing it. The arguments outlined below make
use of certain elementary concepts regarding random processes – particularly
covariances and autoregressive modeling.
5.1
A speech signal model
5.1.1
Overview
The ﬁrst issue is the choice of the sampling frequency.
When it comes to
telephone communications, there usually are two constraints: the message must
be comprehensible, and it must be possible to identify the person speaking.
These constraints mean that the frequency band can be restrained to the [300-
3,400] Hz interval, which implies a Nyquist frequency of 8 kHz. Figure 5.1
shows 2,000 values, or 0.25 s of a speech signal sampled at that frequency.
0
200
400
600
800 1,000 1,200 1,400 1,600 1,800 2,000
−10
0
10
Voiced signal
Unvoiced signal
Figure 5.1 – Speech signal sampled at 8,000 Hz. The x-coordinates correspond to
the number of samples
This 300-3,400 Hz band is called the telephone-band. Of course, the larger

106
Digital Signal and Image Processing using MATLAB®
the band, the better the quality. However, as the width of the band increases,
so does the amount of informations sent per unit of time!
In order to enhance the quality, a larger band is considered that goes from
150 Hz up to 7 kHz; this band is devoted to the wideband digital handsets
telephones. It works well for speech signals but is still insuﬃcient for musical
signals. In the case of music, usually two qualities are considered: the FM
band (short for Frequency Modulation) quality used for frequency modulated
radio broadcasts, which goes up to 15 kHz, and the HIFI band quality, used for
example for compact discs, which goes up to 22 kHz. In any case, the speech
signal must be sampled, according to the sampling theorem, at a frequency at
least twice that of the band used.
Quality
Maximum frequency
Sampling frequency
Telephone-band
300-3,400 Hz
8,000 samples/s
Wideband
150-7,000 Hz
16,000 samples/s
FM band
50 Hz-15,000 Hz
32,000 samples/s
HIFI band
<22,050 Hz
44,110 samples/s
5.1.2
A typology of vocal sounds
Consider the part of the signal shown in Figure 5.1. As you can see, there are
two clearly distinct sections corresponding to two types of sounds:
– the sounds that have the aspect of a harmonic vibration and that are said
to be voiced. Vowels are a perfect illustration of this type of sound. An
example is shown in Figure 5.1 in the window with the indices from 0 to
1,200;
– the sounds we interpret more as noise, and that are said to be unvoiced.
An example is shown in Figure 5.1 in the window with the indices from
1,200 to 1,800.
Vowels generally last longer than consonants. They can easily be recognized
by their harmonic aspect. Consonants are divided in the following categories:
– the nasal consonants/m/, /n/. . . for which the “oral cavity + pharynx”
system forms a closed resonant cavity, with the air going the nostrils;
– the unvoiced fricatives /f/, /s/, /ch/. . . produced by turbulence of a con-
tinuous air ﬂow in the oral cavity.
The cavity is divided in two sub-
cavities, the one in the back causing the “zeros” in the transfer function;
– the voiced fricatives /v/, /z/. . . which can be described in the same way
as the unvoiced fricatives but with vibrations of the vocal folds;
– the voice plosives /b/, /d/. . . which are transitions caused by the sudden
opening of the oral cavity following a rise in pressure.
They strongly
depend on the vowels they are pronounced with;

Speech Processing
107
– the voiced plosives /p/, /t/, etc.
Figure 5.2 illustrates the case of the sounds “sh” and “ee”.
0
2,000
4,000
6,000
8,000
0
Vowel /ee/
0
500
1,000
Fricative /sh/
Figure 5.2 – Temporal shapes of a speech signal sampled at 8,000 Hz: top graph: an
unvoiced sound; bottom graph: voiced sound
5.1.3
The AR model of speech production
The production of sounds is a very complex phenomenon that cannot be easily
described by a model. It is bound to the anatomy of the vocal tract, represented
in Figure 5.3.
A functional represents [15] is shown in Figure 5.4.
The vocal tract is
simpliﬁed as a series of cavities, the shapes of which change with time as air,
coming from the lungs, ﬂows through them.
Studies conducted on the vocal tract show that the two type of sounds,
voiced and unvoiced, can be described using as a model the output of a all pole
linear ﬁlter of the type 1/A(z), the order of which is between 10 and 50, and
with, as the input:
– a white noise for unvoiced sounds;
– and an impulse sequence for voiced sounds.
The impulse sequence associated with voiced sounds corresponds to the
derivative of the volume of the air ﬂowing through the glottis (the space be-
tween the vocal cords) which opens and closes periodically, with opening phases

108
Digital Signal and Image Processing using MATLAB®
Vocal folds
Windpipe (trachea)
Tongue
Uvula
Velum
(soft
palate)
Oral cavity
Labial 
cavity
Nasal
Cavity
Palate
Pharyngeal
cavity
Esophagus
Epiglottis
Larynx
Glottis
Figure 5.3 – Anatomy of the vocal tract
Lungs
Nasal cavity
Oral cavity
(Tongue-palate)
Soft palate
Vocal
folds
Trachea and
bronchi
Larynx
Pharynx
Figure 5.4 – Elements of the vocal tract
that last longer than the closing phases. This leads to a sudden decrease, dur-
ing the closing phase, of the air ﬂow which causes by derivation a very brief
negative impulse (see Figure 5.5). Although it is sometimes suﬃcient to crudely
approximate the sequence produced by the glottis by a simple sequence of ideal
impulses, there are more sophisticated glottal excitation models [21], such as
those of Rosenberg or Liljencrants-Fant (see Figure 5.5).
The fundamental frequency of the periodic sequence of glottal impulses is
called the pitch. This frequency goes from about 70 Hz for a very low voice to
450 Hz for very high one. For a man, it goes basically from 70 to 200 Hz, for
a woman from 140 to 350 Hz, and for a child from 180 to 450 Hz. In any case,
for a given person, the pitch varies in the course of a conversation. For voiced
sounds, the sequence of periodic impulses produced by the vocal folds acts as
a frequency analyzer and causes resonant frequencies to appear in the vocal

Speech Processing
109
0
50
100
0
0.2
0.4
0.6
0.8
1
0
50
100
−8
−6
−4
−2
0
2
4
0
50
100
0
0.2
0.4
0.6
0.8
1
0
50
100
−10
−5
0
5
Rosenberg
Rosenberg
Liljencrants-Fant 
Liljencrants-Fant 
Figure 5.5 – Typical shapes of the glottal excitation signal. Above ﬁgure: air ﬂow
model. Bottom ﬁgure: derivative of the air ﬂow model
tract. These frequencies are called formants. As you can see in Figure 5.6, four
formants are found by following the spectrum’s envelope shown on the right.
0
2,000 4,000 6,000 8,000
0
0
0.1
0.2
0.3
0.4
0.5 (Fs)
Vowel /i/
70
150
100
Figure 5.6 – Temporal shape and spectrum of a speech signal: the graph on the
right shows four formants (sampling frequency Fs = 8,000 Hz)
Let 1/A(z) be the transfer function of the ﬁlter used as a model for the
vocal tract with A(z) = 1 + a1z−1 + · · · + aP z−P . In the case of an unvoiced
sound, the input can be seen as a white noise, and therefore the speech signal
{s(n)} is an autoregressive process:
s(n) = −a1s(n −1) −· · · −aP s(n −P) + w(n)
We can then estimate the coeﬃcients of A(z) from an unvoiced sound win-
dow, using the results obtained for AR processes with the Yule-Walker equa-

110
Digital Signal and Image Processing using MATLAB®
tions:


R(0)
R∗(1)
. . .
R∗(P)
R(1)
R(0)
. . .
R∗(P −1)
...
...
R(P)
R(P −1)
. . .
R(0)




1
a1
...
aP

=


σ2
0
...
0


(5.1)
where R(k) is the covariance function and σ2 the power of w(n).
Then, when the unvoiced signal is applied to the FIR ﬁlter with the transfer
function A(z), we get an estimate of the white noise input.
{s(n)} −→A(z) −→{w(n)}
In the case of a voiced sound, the glottis signal is assumed to be a sequence
of periodic impulses with the pitch frequency Fp. If the vocal tract is described
as an all pole ﬁlter 1/A(z) and if we assume that:
g(n) =
∑
k
i(n −kM) ≈
∑
k
Aδ(n −kM)
(5.2)
provides a good approximation of the glottis signal with MTs ≈Tp = 1/Fp,
the voiced signal s(n) is a periodic signal containing the same frequencies. If
A(z) is a P degree polynomial, s(n) obeys the ﬁltering equation:
s(n) + a1s(n −1) + · · · + aP s(n −P) = g(n)
(5.3)
Based on N observations, we get:


s(P + 1)
s(P)
· · ·
s(1)
...
...
...
...
s(N)
s(N −1)
· · ·
s(N −P)




1
a1
...
aP

=


g(P + 1)
...
...
g(N)


which is written in matrix form:
[s
S] [1
a
]
= g
(5.4)
S is a Toeplitz matrix. The vector g is a periodic vector containing an
A followed by (M −1) zeros (see approximation (5.2)). Typically, for a pitch
frequency of Fp = 190 Hz, the pitch period is Tp ≈5 ms. If Tp is assumed to be
much greater than the glottal impulse duration, the vector g contains almost
nothing but zeros. Equation (5.4) can then be written:
[s
S] [1
a
]
≈0

Speech Processing
111
and a can be estimated using a least squares approach by minimizing this vec-
tor. The advantage of this method is its simplicity(1) because it requires neither
the estimate of the period M nor the estimate of the phase corresponding to
the exact moment when the glottis close. The minimization leads to:
ˆa = −(ST S)−1ST s
an expression similar to the expression a = −R−1r found with a Yule-Walker
equation which relates the parameters of an AR process with the covariance
coeﬃcients. Once ˆa has been estimated, we can then calculate g(n) with the
use of expression (5.3), that is simply by feeding the speech signal {s(n)} into
the input of the FIR ﬁlter with the transfer function A(z).
{s(n)} −→A(z) −→{g(n)}
As a conclusion, whether the sound is voiced or not, the estimate of the
residual signal, glottis impulses in the ﬁrst case and white noise in the second,
is obtained by ﬁltering the signal by the ﬁlter with the transfer function A(z),
the coeﬃcients of which are estimated as the parameters of an autoregressive
process. The following example allows you to experimentally check the shape
of the residual signal for a speech signal.
Example 5.1 (Observation of a speech signal’s residual) First record a
voiced sound /ee/ and an unvoiced sound /sh/ at 8 kHz.
Then create a
program:
– that estimates the coeﬃcients {ak} of a 20th order all pole model for an
analysis window with a duration of 30 ms, or 240 samples;
– that performs the ﬁltering with the transfer function A(z) of the same
speech signal block.
Apply this process to both the voiced and the unvoiced sound.
Hints: type the following program:
%===== residuAR.m
clear
load voye;
% voyel
load conch;
% consonant
Fs=8000; tbloc=.05; % block size (ms)
modorder=20; nb=tbloc*Fs; mtime=(0:nb-1)/Fs;
sigi=ltre(1+1600:nb+1600); sigch=ltrch(1:nb);
(1)When the glottal impulses have to be reconstructed with a higher accuracy, for example
when it is used to perform a medical diagnosis, the least squares method used here can give
insuﬃcient results [15].

112
Digital Signal and Image Processing using MATLAB®
% estimating the covariances
for k=1:modorder+1,
ri(k)=sigi(k:nb)'*sigi(1:nb-k+1)/nb;
rch(k)=sigch(k:nb)'*sigch(1:nb-k+1)/nb;
end
% solving the Yule-Walker equation
RRi=toeplitz(ri); asi=-RRi \ [1;zeros(modorder,1)];
RRch=toeplitz(rch); asch=-RRch \ [1;zeros(modorder,1)];
ai=asi/asi(1); resi=filter(ai,1,sigi);
ach=asch/asch(1); resch=filter(ach,1,sigch);
subplot(411); plot(mtime,sigi/max(abs(sigi))); grid
subplot(412); plot(mtime,-resi/max(abs(resi))); grid
subplot(413); plot(mtime,sigch/max(abs(sigch))); grid
subplot(414); plot(mtime,resch/max(abs(resch))); grid
The results are shown in Figures 5.7 and 5.8. The graphs give the
shape of the residual signal for a voiced sound and for an unvoiced
sound. Very short impulses are clearly visible for the residual signal
of a voiced signal, with a period of about 10 ms, that is about 100
Hz, corresponding to the closing frequency of the glottis.
0
0.005
0.01
0.015
0.02
0.025
0.03 s
0
0
Figure 5.7 – Temporal shape of a voiced sound (/ee/) and of the residual
Remember that the Toeplitz nature of the estimate of a covariance
matrix ensures that the ﬁlter with the transfer function 1/A(z) is
causal and stable. However, in the analysis problem, which consists
of constructing, as we have just done, the input signal based on
the observed signal, this property is not crucial since the ﬁlter A(z)
is a FIR ﬁlter, and is therefore stable. On the other hand, in the
synthesis problem, which consists of reconstructing the speech signal
based on the input signal, this property is of course essential.

Speech Processing
113
0
0.005
0.01
0.015
0.02
0.025
0.03 s
0
0
Figure 5.8 – Temporal shape of an unvoiced sound (/sh/) and of the residual
5.1.4
Compressing a speech signal
We are going to present in this paragraph an example of speech signal com-
pression. The word compression is used for the digitization of a signal as a
bitstream with a rate as low as possible for a given level of distortion. Com-
pression is too diﬃcult a problem to be discussed here thoroughly. The solution
we present is merely an introductory example, inspired from a encoder chosen
a very wide choice of existing encoders [26, 15].
The simplest form of signal digitization is the b-bit uniform quantization of
the values sampled at the frequency Fs. Typically, for Fs = 8 kHz and b = 8
bits, the result is a 64 kbits/s stream called PCM (Pulse Code Modulation).
To achieve compression, in other words to use less bits, while maintaining
a low level of distortion, a ﬁrst idea would be to quantize the diﬀerence δ(n) =
s(n) −s(n −1) in place of the samples s(n). The result should be smaller
than s(n) and hence the same accuracy would be achieved with less bits. This
approach, which uses the correlations contained in the consecutive samples can
easily be generalized by quantizing the residual signal ϵ(n) = s(n)−(α1s(n−1)+
· · ·+αP s(n−P)), where the sequence αp is the one that minimizes E
{
|ϵ(n)|2}
.
As you may have noticed, we are once again faced with the linear prediction
problem. This approach, called DPCM, for diﬀerential PCM has the advantage
of being applicable to any signal for what we call waveform coding.
A second approach consists of considering the speech creating system and
describing it using a model comprising a small number of parameters we are
going to estimate. Then, all we have to do is reconstruct the signal from these
parameters to obtain a signal that “sounds” like the original one. This type
of encoder, called a vocoder, does not follow the shape of the original signal.
This is why it cannot be used to compress a signal originating for example from
a modem operating in the phone frequency band. However, the bitstream is
usually lower, for the same quality, than the ones obtained in the ﬁrst approach.

114
Digital Signal and Image Processing using MATLAB®
The example we chose belongs to the second category.
The compression principle we decided on is based on representing the speech
production system as an all pole ﬁlter, the input of which is either a sequence
of periodic impulses, in the case of voiced sounds, or a white noise in the case
of unvoiced sounds. The result is the synthesis diagram shown in Figure 5.9.
With this approach, we assume that the signal is stationary, which is almost
true for speech signals with a duration of about 10 to 20 milliseconds.
To have a better idea of this model, consider a signal sampled at 8,000
Hz with 8 bits, which corresponds to a rate of 64,000 bits/s, and assume that
for each block of N = 180 samples, corresponding to a duration of 22.5 ms,
P = 22 parameters are extracted. If we use 8 bits to represent each of the 22
parameters, 180 × 8 = 1,440 bits are replaced with 22 × 8 = 176 bits, which
means we have a compression factor of about 8. In terms of rate, everything
happens as if only 1 bit of each sample was kept.
This would be the rate
obtained if, for example, we only kept the sign bit of the samples s(n). You
can run the following program:
signs=sign(s);
soundsc(signs,8000);
where s represents the sample sequence of a speech signal sampled at 8,000 Hz.
1
A(z)
Unvoiced sound
Voiced sound
Figure 5.9 – Creating voiced and unvoiced sounds with all pole ﬁltering
Exercise 5.1 (Compression of a speech signal) (see p. 215)
The task you are supposed to do is divided in three parts:
1. Detecting whether a sound is voiced or unvoiced, and pitch measurement:
when a signal is periodic, its autocorrelation function, deﬁned by the
normalized covariance:
ρXX(n1, n2) =
E {Xc(n1)X∗
c (n2)}
√
E {|Xc(n1)|2} E {|Xc(n2)|2}
shows maxima distant from each other by one period of the fundamen-
tal. Hence the idea to detect whether or not the sound is voiced and to

Speech Processing
115
measure the pitch based on the computation of the estimate J(k) of the
autocorrelation function:
J(k) =
∑x(n)x(n −k)
(∑x2(n))1/2(∑x2(n −k))1/2
(5.5)
The use of this function, incidentally, will be justiﬁed by a least squares
approach when we deal with the similar problem of cardiac rhythm esti-
mation (see page 136).
As we have seen, the pitch belongs, in practice, to an interval (Fmin, Fmax).
You can therefore restrain the computation of the function J(k) to the val-
ues of k greater than kmin = Fs/Fmax and smaller than kmax = Fs/Fmin,
where Fs refers to the sampling frequency. If the maximum of J(k) goes
beyond a certain threshold, typically 0.6, the sound is considered voiced,
and the pitch period is then given by the value k0 corresponding to the
maximum of J(k).
Write a function that detects sound activity, determines whether the
sound is voiced or unvoiced and measures, in the case of a voiced sound,
the pitch period. Use this function to divide the signal voiced and un-
voiced areas, using windows of 240 points, that is 30 ms of signal, with
an overlap rate of 25%.
2. All pole ﬁlter parameter extraction based on the previous partitioning:
– use the xtoa function:
function [a,sigma2]=xtoa(x,P)
%!=========================================!
%! SYNOPSIS: [a,sigma2]=XTOA(x,P)
!
%! x
= signal Coefficients array
!
%! P
= model order
!
%! a
= [1 a_1 a_2 ... a_P]
!
%! sigma2 = power of the input white noise !
%!=========================================!
N=length(x); x=x(:); x=x-mean(x);
for k=1:P+1
%===== biased estimate of R^*(k)
rconj(k)=x(k:N)' * x(1:N-k+1) / N;
end
Rc=toeplitz(rconj); vaux=Rc \ eye(P+1,1);
a=vaux / vaux(1); sigma2=1 / vaux(1);
to extract the P coeﬃcients of the all pole ﬁlter associated with each
analysis window. You can choose P = 20 as the model order for a
voiced sound window and P = 10 for an unvoiced sound window;
– create a coeﬃcient ﬁle containing for each block the coeﬃcients of
the all pole model, as well as the value of the pitch when the signal
is voiced. Set the pitch value to zero to indicate an unvoiced sound.

116
Digital Signal and Image Processing using MATLAB®
3. Synthesis: write a program that achieves the synthesis. To tone down
the possible sudden variations from the coeﬃcients of one window to
those of the next, set the overlap to 25% for the consecutive outputs
calculated over time intervals of 30 ms. Use a white Gaussian noise or a
simple sequence of impulse with identical amplitudes as the ﬁlter’s input,
depending on whether the sound is unvoiced or voiced.
In Exercise 5.1, we did not quite perform the encoding operation we said we
would. What we should have done is use 176 bits to encode all 22 parameters for
every time of analysis, but that is not as simple as it seems, because we cannot
just simply associate 8 bits to each of the 22 parameters, the bit distribution
has to be optimal. A classic approach is to study, using a speech database, the
parameter distribution and to create a codebook of representative elements by
a vector quantization.
A voice activity detector usually precedes the whole encoding system. Its
role is to determine the segments containing “silence”, for which no parameter
estimation is done. When reconstructing the signal, a faint noise is added to
these segments of silence for purposes of listening comfort.
5.2
Dynamic Time Warping
In signal processing, and particularly in the ﬁeld of speech, diﬀerent series of
measurement conducted in seemingly identical conditions can provide record-
ings that actually show signiﬁcant diﬀerences in terms of amplitude, duration,
utterance speed, etc. The algorithm, which will be detailed later, performs a
time-alignment of two observation sequences independently from possible dif-
ferences in amplitude, duration, or utterance speed. It is commonly called the
DTW algorithm, short for Dynamic Time Warping.
In 1975, Itakura [12] suggested using the DTW for speech recognition. In
the particular case of the recognition of isolated words, a dictionary is used,
containing the sound recordings of the words to be recognized, and during the
recognition operation, it has to be decided which word has been pronounced
based on the observed sound signal. If the sound signal corresponding to a given
word were perfectly reproduced, we would simply have to subtract, sample by
sample, the sample signal from the reference signal: a word would then be
recognized if the diﬀerence is equal to zero. Unfortunately, this is never the
case. So, in order to perform the comparison, we are going to associate as best
we can the consecutive phases of the signal to be recognized with those of the
diﬀerent dictionary signals and ﬁnd a value for the discrepancy. The DTW
performs both those operations.

Speech Processing
117
5.2.1
The DTW algorithm
Consider two sequences of length d vectors {x1, . . . , xI} and {y1, . . . , yJ}. Let:
d(i, j) =
√∑d
ℓ=1(xi,ℓ−yj,ℓ)2
with 1 ≤i ≤I and 1 ≤j ≤J, be the distance between the two vectors xi and
yj.
A solution to the time alignment problem consists of taking one by one the
indices of the sequences {xi} and {yj} using a pair of functions ϕ = (ϕx, ϕy)
deﬁned on {1, . . . , T} and within the range {1, . . . , I} × {1, . . . , J}, and to
calculate the cumulated sum of the distances associated with ϕ:
dϕ(I, J) = M −1
ϕ
T
∑
k=1
mkd(ϕx(k), ϕy(k))
where the mk are positive weighting coeﬃcients and Mϕ is a normalization
constant given by:
Mϕ =
T
∑
k=1
mk
It seems Mϕ should depend on the choice of ϕ. In practice, the mk are
chosen such that Mϕ is independent of ϕ. This can be achieved for example by
choosing:
mk = (ϕx(k) −ϕx(k −1)) + (ϕy(k) −ϕy(k −1))
(5.6)
The function ϕ satisﬁes diﬀerent types of constraints, such as:
– initial and ﬁnal values: ϕx(1) = 1, ϕx(T) = I, ϕy(1) = 1, ϕy(T) = J;
– trajectory monotony and continuity: 0 ≤ϕx(k) −ϕx(k −1) ≤1 and
0 ≤ϕy(k) −ϕy(k −1) ≤1;
– local continuity: ﬁnally, the pair (ϕx(k), ϕy(k)) satisﬁes certain pathﬁnd-
ing rules.
The comparison is performed on all the elements in the dictionary. Figure
5.10 illustrates the comparison of {x1, . . . , xI} with {y(1)
1 , . . . , y(1)
J1 }, {y(2)
1 , . . . , y(2)
J2 },
. . ., {y(N)
1
, . . . , y(N)
JN }.

118
Digital Signal and Image Processing using MATLAB®
Figure 5.10 – Comparison calculation with the N elements from the dictionary with
lengths J1, J2, . . ., JN by calculating of minimal distances in each distance matrix
1
1
i
i−1
j
j−1
2
Figure 5.11 – Pathﬁnding constraints: the numbers indicate the weight associated
with the considered paths
1
0,5
0,5
0,5
0,5
i
j
i−1
Figure 5.12 – Pathﬁnding constraints: the numbers indicate the weight associated
with the considered paths
5.2.2
Examples of pathﬁnding rules
To ensure local continuity, a sequence of possible paths is deﬁned by a graph
such as the ones shown in Figures 5.11 and 5.12: the arrows in these ﬁgures
show the only possible paths that can be taken to reach the ﬁnal point.
The goal of the DTW algorithm is to determine:
D(I, J) = min
ϕ∈Φ dϕ(I, J)

Speech Processing
119
where Φ is the set of functions that satisfy the constraint.
Dynamic programming is a recursive approach introduced by R. Bellmann
which allows the previous criterion to be minimized using the following prop-
erty: consider two sequences {x1, . . . , xI} and {y1, . . . , yJ} and let C(I, J) be
the minimal length DTW path associated with these two sequences. Then the
sub-path of the path C(I, J) which reaches the point with coordinates (xi, yj)
is optimal for the two sub-sequences {x1, . . . , xi} and {y1, . . . , yj}, because
among all the paths that lead to the point with coordinates (i, j), minimiza-
tion means we only keep the shortest one and therefore:
D(i, j) = min
ϕ∈Φ dϕ(i, j)
Thus, for the constraint graph shown in Figure 5.11, we infer that:
D(i, j) = min









D(i −1, j) + d(i, j)
D(i −1, j −1) + 2d(i, j)
D(i, j −1) + d(i, j)









(5.7)
Likewise, for the constraint graph shown in Figure 5.12, we have:
D(i, j) = min









D(i −2, j −1) + 0.5d(i −1, j) + 0.5d(i, j)
D(i −1, j −1) + d(i, j)
D(i −1, j −2) + 0.5d(i, j −1) + 0.5d(i, j)









(5.8)
Exercise 5.2 (DTW) (see p. 219) Write a function that implements the DTW
algorithm by taking the pathﬁnding constraints found in the graph in Figure
5.11. Use the two sequences of vectors as the input and as the output the
array of the cumulated sums D(i, j), as well as the value D(I, J)/(I + J) as
the output.
Exercise 5.3 uses the function obtained in Exercise 5.2 to recognize a word
in a given list of words. We will ﬁrst brieﬂy describe the cepstral analysis used
to provide the characteristic sound features the DTW alignment is based on.
5.2.3
Cepstral coeﬃcients
We know that the phonetic content of a speech signal can be characterized
in a satisfactory way by a short-term spectral representation. This is why in
speech recognition the ﬁrst signal is usually performed, consisting of such an
analysis over windows of about 20 milliseconds with overlap. Most of the time,
the extracted characteristics are the ﬁrst values of the short-term cepstrum,

120
Digital Signal and Image Processing using MATLAB®
deﬁned as the inverse Fourier transform of the logarithm of the modulus of
the signal’s DFT. If x(n), with n ∈{0, . . . , N −1}, refers to a portion of a
signal with a duration of N, possibly weighted by a window, then the cepstral
coeﬃcients have the expression:
c(k) = 1
L
L−1
∑
ℓ=0
S(ℓ)e2jπnℓ/L with S(ℓ) = log

N−1
∑
n=0
x(n)e−2jπnℓ/L

It can be proven that the ﬁrst cepstral coeﬃcient represents the energy of
the signal segment and that the following d coeﬃcients, where d corresponds to
a duration of a few milliseconds, characterize the shape of the vocal tract. In
speech recognition, the relevant information is essentially characterized by the
shape of the vocal tract. This is why most recognition methods are based on the
use of the ﬁrst cepstral coeﬃcients. Most of the time, the cepstral coeﬃcients
are calculated on a logarithmic frequency scale; this is called MFCC, for Mel
Frequency Cepstral Coeﬃcients. Exercise 5.3 only uses a linear frequency scale,
leading to a sequence of length d vectors that will undergo the recognition
processes.
Exercise 5.3 (DTW word recognition) (see p. 221)
1. Write a function that extracts the short-term cepstrum from a speech
signal sampled at 8,000 Hz using the operations:
– multiplication of the time window by a Hamming window: choose
a window duration corresponding to 20 ms with a temporal overlap
of 50%;
– computation of the logarithm of the modulus of the Fourier trans-
form over L = 256 points;
– computation of the inverse Fourier transform;
– extraction of the 12 values useful to recognition.
2. Write a program that uses the DTW to compute, based on the sequences
of cesptral coeﬃcients, the “distance” between two recordings of the
same pronounced word, and between two recordings of two diﬀerent pro-
nounced words.
5.3
Modifying the duration of an audio signal
Modifying the temporal scale of a sound has applications in many ﬁelds, such as
solutions for the hearing impaired, speech design and recognition, movies, TV
and radio advertisement, etc. A simple way of performing this modiﬁcation is
to reconstruct the signal with a sampling frequency diﬀerent from the one used

Speech Processing
121
for the acquisition. Unfortunately, this method causes frequency distortion,
because, as we know, if X(f) represents the Fourier transform of x(t), then
the Fourier transform of x(γt), with γ > 0, is given by γ−1X(f/γ). Therefore,
the frequency axis is dilated or contracted, depending on whether γ is greater
or smaller than 1. You can observe the eﬀects on a speech signal with the
use of the MATLAB® function sound and by trying diﬀerent reconstruction
frequencies: when the frequency is greater than the acquisition frequency, the
pitch seems higher. To prevent this type of distortion, several techniques have
been suggested. The most popular one is probably the technique called PSOLA,
for Pitch Synchronous Overlapping Addition, which works in the time domain
[27]. Another one is referred to as the “phase vocoder”, which works in the
frequency domain [16]. One of the drawbacks of PSOLA is the addition of
unwanted noises. As for the phase vocoder, it causes a reverberation eﬀect.
5.3.1
PSOLA
To reduce the total duration of the signal, while preserving the frequency scale,
we can simply eliminate small segments of the signal throughout the recording.
Conversely, to increase the total duration of the signal, we can duplicate some
of its segments. However, these segments have to be long enough to prevent
spectrum aliasing, but short enough compared with the duration of the basic
acoustic units: with PSOLA [27], the durations of the segments have to be cho-
sen equal to the “instantaneous” pitch period. PSOLA is essentially comprised
of two steps:
– analysis: the signal s(t) is time-“marked” according to a sequence of
analysis times ta(i) such that ta(i) = ta(i −1) + Pa, where Pa refers
to the “instantaneous” fundamental period (the pitch) estimated with a
window long enough, starting at the time ta(i −1);
– synthesis: the modiﬁed signal ˜s(t) is constructed by an overlap-add
operation on the basic segments si(t) of the original signal relocated at
synthesis times ts(j) according to the expression:
˜s(t) =
∑
n
˜si(t −ts(n))
where ˜si(t −ts(n)) = h(t −ta(i(n)))s(t) is a signal segment centered in
ta(i(n)). The synthesis times are such that ts(n) = ts(n −1) + Pa(i(n)).
If γ refers to the speed modiﬁcation rate, the index i(n) is the closest
integer to the value nγ.

122
Digital Signal and Image Processing using MATLAB®
Exercise 5.4 (PSOLA) (see p. 222)
1. Analysis: use the f0cor.m function (page 139), which estimates a sig-
nal’s pitch, to construct the sequence of analysis times ta(n) in the fol-
lowing way:
– the initial values are set as ta(1) = 1 and Pa = L10 where L10 is the
number of samples corresponding to 10 ms;
– the window starting at the time ta(n) and lasting at least two pitch
periods Pa is extracted. In practice, the duration has to be chosen
equal to twice the smallest period expected in the signal;
– pitch detection is performed on the window. If the signal is voiced,
we get the pitch period Pa and the analysis time ta(n+1) = ta(n)+
Pa. If the signal is unvoiced, the value ta(n + 1) = ta(n) + L10 is
chosen.
At the end of this ﬁrst process, we have a sequence of Na analysis times
(positions in the ﬁle) ta(n) that are synchronous with the pitch period;
2. Synthesis: we wish to modify the prosodic speed by a factor γ. This is
done by generating the sequence of synthesis times ts as follows:
– initially, ts = 1;
– the following synthesis time is calculated by executing:
te=te+gamma;
ie=ceil(te);
The index is used to point out the analysis time after which the
segment of the signal used for creating the desired signal is extracted.
Hence, if γ < 1, which corresponds to a slower utterance, the value
of ie after n iterations will be smaller than n. Hence, some segments
of the signal will be repeated and the signal created will last longer;
– the sequence of synthesis times is generated by executing:
ts=ts+(ta(ie+1)-ta(ie));
This is equivalent to generating a window with a length equal to the
pitch (hence the phrase Pitch Synchronous), because, using obvious
notations, we can write:
ts(n + 1) = ts(n) + (ta(ie(n) + 1) −ta(ie(n)))
|
{z
}
Pa
Table 5.1 shows some of the values for the sequences that were found for
γ = 0.8. As you can see, the portion centered in 7,668 in the original
signal is in position 9,610 in the synthesis signal. Notice also that, because

Speech Processing
123
n
1
. . .
199
200
201
202
. . .
te(n)
1
. . .
159.4
160.2
161.0
161.8
. . .
ie(n)
1
. . .
160
161
161
162
. . .
ts(n)
1
. . .
9,493
9,532
9,571
9,610
. . .
ta(ie(n))
1
. . .
7,590
7,629
7,629
7,668
. . .
Table 5.1 – Sequences of synthesis and analysis times corresponding to γ = 0.8, that
is a slower utterance
the signal is slowed down, as we wanted it to be, we have to duplicate
the portion centered on 7,629, once in position 9,532, and once more in
the following position 9,571.
The signal synthesis is performed as follows:
– the segment centered in ta(ie) and with a length of 2(ta(ie + 1) −
ta(ie)) is extracted from the original signal;
– the extracted segment is multiplied by a Hann window then added
to the previous portion with a 50% overlap.
Write a function that consecutively performs the two steps of the process.
5.3.2
Phase vocoder
The basic idea behind the phase vocoder [16] is to perform a short-term Fourier
analysis. If the spectrum is comprised of narrow band spectral components,
which means that the sound is closer to a voiced sound than it is to an unvoiced
sound, and that the analysis window is much longer than the pitch period, then
the values of every pitch harmonic will be clearly identiﬁed. In this case, if the
spectral characteristics are maintained, for a duration slightly lower or slightly
greater than the original one, then the value of the pitch is preserved.
The only diﬃculty is that the phases associated with each frequency compo-
nent in the modiﬁed signal’s spectrum have to be calculated in such a way as to
ensure the proper alignment of the consecutive phases during the overlap-add
operation.
The following exercise shows that the sum of overlapping Hann windows
remains constant. This property is used in exercise 5.6.
Exercise 5.5 (Hann window) (see p. 223)
Consider the sequence (called a Hann window):
h(n) =
{ 0.5(cos(2πn/L) + 1) = sin2(πn/L) for n ∈{0, . . . , L −1}
0 otherwise
Let α ∈(0, 1), and(2) n0 = ⌊αL⌋. Write a program that plots against n the
(2)⌊αL⌋refers to the integer part of αL.

124
Digital Signal and Image Processing using MATLAB®
sequence:
x(n) =
∑
k
h2(n −kn0)
(5.9)
Notice that the sequence h(n) has a ﬁnite length. Therefore, to construct
x(n), all you need to do is to calculate the sum of a ﬁnite number of segments
distant from each other by h2(n).
What happens when L varies, when α varies? Try other powers of h(n) in
expression (5.9).
Exercise 5.6 (Phase vocoder) (see p. 224)
Write a program that performs the following operation:
– calculation of the DFTs of the signal segments, each segment weighted
by a length L Hann window h(n). The consecutive windows are distant
from each other by a number of points n0 = ⌊αL⌋where α ∈(0, 1). With
such an overlap, we have:
∑
k
g(n −kn0) = C
where g(n) = h2(n) and where C is constant depending on α that you
will determine. According to what we saw in Exercise 5.5, if L is a power
of 2, the distance between the windows has to be in the form n0 = L/2m;
– generating the sequence of synthesis times with a factor γ compared with
the original sequence according to the method suggested in Exercise 5.4;
– calculation of the modiﬁed DFTs by performing an amplitude interpo-
lation, proportional to the original DFTs on the interval containing the
synthesis time. For the phase calculations, sum the phase increases of
the original DFTs;
– calculation of the inverse DFTs of the modiﬁed DFTs, each DFT being
once more weighted by a length L Hann window. For γ = 1, everything
happens as if the window g(n) = h2(n) were applied and, therefore, the
correct amplitudes can theoretically be found by dividing the obtained
signal by the constant C.
5.4
Eliminating the impulse noise
We are now going to look into the restoration of recordings containing errors
with a relatively large amplitude and with a very brief duration (less than a
millisecond), referred to as clicks, and assumed to be in small numbers. Thus,

Speech Processing
125
the signal represented in Figure 5.13, originating from record, shows a scratch
spread over a few samples. Cracks are not always as “visible” as this one, and
the recording often has to be listened to in order to detect them.
0.79
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88 (s)
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Click
Figure 5.13 – Signal originating from a record and containing a click
The considered restoration method is comprised of two separate steps:
1. the detection of clicks in the signal;
2. the restoration of corrupted samples.
5.4.1
The signal model
When observing the signal s(n) shown in Figure 5.13, what makes us decide
that there is a click in position n0 is that the value in n0 is noticeably diﬀerent
from what would seem predictable to us based on the past signal. Therefore,
if we know how to calculate a prediction ˆs(n) based on the last K values
s(n −1), . . . , s(n −K), the procedure can be made automatic, by deciding the
presence of a click in position n if the diﬀerence between ˆs(n) and s(n) is above
a threshold that can be determined experimentally.
For an WSS process, the best K order linear predictor:
ˆs(n) = α1s(n −1) + · · · + αKs(n −K)
(5.10)
in the least squares sense is obtained by choosing the solutions to the Yule-
Walker equation (5.1) as the coeﬃcients αi. The expression of the prediction
error σ2 is given by:
ε2 = R(0) −
P
∑
k=1
αkR(−k)
(5.11)

126
Digital Signal and Image Processing using MATLAB®
If s(n) is a K order AR process, the prediction process s(n) −ˆs(n), also
called the residual signal, is white. Because we are going to use this property
in click detection, we will assume that this is the case: in the absence of clicks,
the signal s(n) is a K order AR process. From now on, K is assumed to be
known. In practice, its value is set by examining the results.
If the number of clicks is small, we can assume that the eﬀects caused by
their presence in the estimation window are negligible. We can then estimate
the parameters a1, . . . , aK and σ2 of the K order AR process directly on the
signal’s window. Once these values have been estimated, the FIR ﬁlter with
the transfer function A(z) = 1 + a1z−1 + · · · + aKz−K can be applied to the
signal x(n). The signal y(n) is obtained. Because s(n) is a K order AR, then
in the absence of clicks, this signal y(n) is a white noise with the variance σ2.
If, in addition to that, s(n) is Gaussian, then y(n) is itself Gaussian. Let us
now see how to detect the presence of clicks.
5.4.2
Click detection
We now assume that a click is described as an impulse i(n) = A0δ(n−n0) added
to the useful sound signal s(n). The observed signal is then x(n) = s(n)+i(n).
We are going to try to detect the possible presence of i(n) using a linear ﬁlter,
then by comparing the ﬁlter’s output with a threshold. The following exercise
shows how the methods works.
Exercise 5.7 (Detecting impulse clicks) (see p. 226)
Let d(n) be a signal with a known shape, corrupted by a noise b(n). “Detecting”
the signal d(n) means we have to choose a decision rule to be able to say if the
signal d(n) is or not in the observed signal y(n). Hence we have two hypotheses
that can be summed up as follows:
– in the absence of the signal, what we see is y(n) = b(n);
– in the presence of the signal d(n), what we see is y(n) = d(n) + b(n).
We will assume that b(n) is a white noise, with the variance σ2. To conduct
the processing, we are going to impose that the detector is comprised of a linear
ﬁlter followed by a threshold comparator, and we will ﬁnd the optimal settings
for the the ﬁlter and the threshold value.
d(n)
+
+
b(n)
y(n)
z(n)=zd(n)+zb(n)
g(n)
Figure 5.14 – Matched ﬁlter

Speech Processing
127
Let zd(n) and zb(n) be the outputs of the ﬁlter g(n) we are trying to de-
termine, the inputs of which are the deterministic signal d(n) and the noise
b(n).
1. Show
that
the
signal-to-noise
ratio
(SNR)
deﬁned
by
ρ
=
|zd(n)|2/E
{
|zb(n)|2}
has the expression:
ρ = 1
σ2
∑+∞
u=−∞g(u)d(n −u)

2
∑+∞
u=−∞|g(u)|2
2. Using the Schwartz inequality, ﬁnd the expression of the impulse response
of the ﬁlter g(n) that maximizes the value of ρ. In the literature, this
ﬁlter is called a matched ﬁlter (the phrase implies that the ﬁlter is matched
with the signal d(n)).
3. Consider now the problem of detecting, with a linear ﬁlter h(n), an im-
pulse δ(n) in a K order AR signal deﬁned by s(n) + a1s(n −1) + · · · +
aKs(n −K) = w(n) where w(n) is a white noise with the variance σ2.
To achieve such a detection, we can decompose h(n) in a cascade of two
linear ﬁlters h1(n) and h2(n), the ﬁrst one a FIR ﬁlter with the impulse
response h1(0) = 1, h1(1) = a1, . . ., h1(K) = aK (Figure 5.15). Let y(n)
be this ﬁlter’s output.
+
+
Figure 5.15 – Signal whitening before detection
By applying the result of the previous question, determine the ﬁlter h2(n)
that maximizes the detection signal-to-noise ratio. Let z(n) be the output
of the ﬁlter h2(n).
4. In the absence of clicks, determine as a function of a1, . . ., aK and σ2 the
expression of the variance Pz at the output of the ﬁlter h2(n).
5. A threshold s is set, and the following decision rule is adopted: if |z(n)| >
s, the presence of a click is decided at the time n. Under the hypothesis
that s is Gaussian, use the decision rule to show that the expression of the
threshold that ensures a probability equal to α of deciding the presence
of a click where there isn’t one, is s = λ(α)√Pz. For α = 0.01, we have
λ = 3.

128
Digital Signal and Image Processing using MATLAB®
6. Simulation: write a program that generates 500 samples of a 10 order AR
process, simulating the useful signal s(n). Use, as the parameters of the
AR-10 process, the values σ2 = 1 and a1, . . . , a10 given by:
%===== AA.M
a= [1 -1.6507 0.6711 -0.1807 0.6130 -0.6085 0.3977 ...
-0.6122 0.5412 0.1321 -0.2393];
Let se be the root mean square value of s(n), an estimate of which is
given under MATLAB® by seff=sqrt(s*s'/N). Add ﬁve clicks with am-
plitudes equal to ±1.5se at arbitrary times. Let us assume that the order
of the ﬁlter is known, but that, despite the presence of a few clicks,
the model’s parameters can be estimated with the xtoa function. Suc-
cessively estimate the model’s parameters, the whitening, the matched
ﬁltering, then the comparison with the previously determined threshold.
The results obtained with the previous program show that a given click
can lead to several close positions detected around the real value.
In
practice, it is preferable to “group” these positions together. Describe a
processing algorithm corresponding to several positions detected for the
same click.
In Summary: the previous results lead us to summarize click detection in
the following operations:
– estimation of the K order AR model’s K + 1 parameters a1, . . ., aK and
σ2 based on the length N window x(n);
– ﬁltering of x(n) by the ﬁlter with the impulse response {1, a1, . . . , aK};
– ﬁltering of the previous signal by the matched ﬁlter with impulse response
{aK, aK−1, . . . , a1, 1}. The output signal is denoted by z(n);
– comparison of |z(n)| with the threshold:
s = λ
√
σ2(1 + a2
1 + · · · + a2
K)
where the parameter λ ≈3 is set experimentally.
5.4.3
Restoration
Once a click is detected in position n0, you would think that only one value
has to be reconstructed: that of the altered sample. But in fact, the error is
rarely found on only one point of the signal. This is why it is better to consider
several samples on both sides of the detected position to be errors. This means

Speech Processing
129
we have a corrupt area of m consecutive values. Typically, m is chosen between
9 and 15 (see Figure 5.16).
The idea, which consists of performing a simple linear interpolation based
on the values placed on either side of the corrupt area, does not take into
account the correlations among the points of the signal. The solution we are
going to use consists of using the prediction:
ˆx(n) = α1x(n −1) + · · · + αKx(n −K)
(5.12)
of a K order AR process.
Exercise 5.8 (Restoring “missing values”) (see p. 229)
Let us assume the parameters of the AR model have been estimated, and
that, in the considered block, the corrupt zone goes from position ℓto position
ℓ+ m −1. We are going to try, by minimizing the square deviation, to search
for the best values of x(ℓ), . . . , x(ℓ+ m −1) (Figure 5.16).
+m−1
x(n)
{
Figure 5.16 – Several values are restored around the detected position
1. Show that estimating the m unknown values can be seen as a linear
problem depending on the non-corrupt values and the model’s coeﬃcients.
2. Use this result to estimate, using the least squares method, by of the
corrupt zone y = [x(ℓ), . . . , x(ℓ+ m −1)]T .
Write the expression of by in the form:
by = −(BT B)−1BT (A0x0 + A1x1)
(5.13)
where A0, A1 and B are matrices build from the model’s coeﬃcients
(a1, . . . , aK) and x0 and x1 are vectors constructed from the non-corrupt
observations x1, . . . , xℓ−1, xℓ+m, . . . , xN.
The plots in Figure 5.17 were obtained using the following program. They
show the signal containing the clicks and the signal after restoration:

130
Digital Signal and Image Processing using MATLAB®
%===== RESTAU.M
% Signal reconstruction
% Run the detection program, then
% select a position to restore in the list of
% positions detected by detect.m
pos=input('Click position: ');
lsig=length(sig); tps=[0:lsig-1]; sig=reshape(sig,lsig,1);
m=15; ell=pos-7;
X0=sig(ell-K:ell-1); X1=sig(ell+m:ell+m+K-1);
colT=[aest(K);zeros(m+K-1,1)];
ligT=[aest(K:-1:1)' zeros(1,m+K)];
T=toeplitz(colT,ligT);
A0=T(:,1:K); B=T(:,K+1:K+m);
A1=T(:,K+m+1:2*K+m); X=A0*X0+A1*X1;
%===== solving the system
Y=-B \ X; sigr=sig; sigr(ell:ell+m-1)=Y;
plot(tps,sig,'-r', tps,s,'b', tps, sigr,':y'); grid;
270
280
290
300
310
320
−20
−15
−10
−5
0
5
10
Detected click
Restored signal
Original signal
Figure 5.17 – Clicks (dashed line), restored signal (full line)
The method presented here is successfully used to clean up recordings, such
as the ones stored on old records. It requires a few adjustments to determine
experimentally the window size, the AR order and the threshold value that
provide the best acoustic results.

Chapter 6
Selected Topics
6.1
Tracking the cardiac rhythm of the fetus
6.1.1
Objectives
The human heart’s activity produces electrical currents that spread through
the tissue and can be measured with the use of electrodes attached to the skin.
These signals are called an electrocardiogram (ECG, or EKG). In obstetrics,
these signals can be used to keep watch over the cardiac condition of the fetus, in
particular during the delivery, which allows doctors to detect possible anomalies
very early on, and hence to treat them more rapidly.
There are essentially two methods that are used to track the cardiac rhythm
of the fetus. The ﬁrst one, which can only be carried out during the delivery,
consists of measuring the electrocardiogram (ECG or EKG) directly oﬀthe
fetus by placing an electrode on its scalp. The second one, the main advantage
of which is to be non-invasive, consists of placing electrodes on the mother to
pick up signals from which the fetal cardiac signal will be extracted.
In the second method, the use of a single sensor placed on the mother’s ab-
domen is not suﬃcient because the amplitude of the fetal EKG can be several
times less than the noise produced by diﬀerent sources of interference such as
the mother’s EKG, but also the signals caused by her muscle activity (elec-
tromyogram) or the ones related to her breathing. Therefore, the observation
is diﬃcult. However, the use of several sensors makes it possible to separate
the signals (Figure 6.1).
The data we are going to process in this work come from measurements
done on EKG signals samples at Fs = 300 Hz over a duration of a few seconds.
Many websites make such signals available to the public. From now on, the
signal originating from the sensor on the chest will be denoted by xp and the
one originating from the sensor on the abdomen by xv (Figure 6.2).

132
Digital Signal and Image Processing using MATLAB®
Chest sensor
Abdominal sensor
The mother's
heart
The fetus's
heart
Figure 6.1 – Picking up the EKGs
chest
sensor
abdominal
sensor
the fetus
heart
xp
xv
g
f
k=1
vF
vM
h
cM
cF
the mother
heart
Figure 6.2 – The model used for describing EKG tracking
A reading of such signals is shown in Figure 6.3.
Two processings have to be conducted. First, we have to extract the fe-
tus’s EKG from the signals xp(n) and xv(n) and, second, estimate the cardiac
rhythm of the fetus based on the obtained signal.
6.1.2
Separating the EKG signals
Theoretically, the signal observed with the chest sensor represents the mother’s
cardiac signal noised by the fetus by the signal originating from the heart of the
fetus. However, because this sensor is located far away from the heart of the
fetus, i.e. because its heartbeat is faint, we will assume that the signal xp(n)
represents the mother’s heart ﬁltered by traveling through the thoracic tissue.
If the mother’s cardiac signal is denoted by cM(n), and if traveling through the
tissue acts as a linear ﬁlter with a length Kp ﬁnite impulse response, we have:
xp(n) = g1cM(n −np) + · · · + gKpcM(n −np −Kp + 1)
(6.1)
where anyone np accounts for the overall propagation delay through the chest.
In practice, this value turns out to be small.
As for the signal xv(n), it is the sum of the cardiac signal of the fetus, ﬁltered
by traveling through the abdomen tissue and a signal originating from the

Selected Topics
133
0
500
1,000
1,500
2,000
2,500
−500
0
500
1,000
0
500
1,000
1,500
2,000
2,500
−60
−40
−20
0
20
40
ECG mother
EKG fetus+mother
Figure 6.3 – Mother-fetus EKG signals. Graph on the top: signal measured oﬀthe
chest, and which can be considered as the signal originating from the mother’s heart.
Graph on the bottom: signal measured oﬀthe abdomen containing both cardiac sig-
nals
mother’s heart. However, this last signal is not quite the signal xp(n) picked up
by the chest sensor, because the signal produced by the mother’s heart reaches
the abdomen sensor after traveling through the abdomen. Therefore, it has
undergone some transformations. By assuming once more that the abdominal
transfer behaves like a linear ﬁlter with a length Ka ﬁnite impulse response
ﬁlter, the signal originating from the mother’s heart can be written (in fact the
FIR hypothesis should be related to the comments made on the approximation
of an IIR by a FIR):
vM(n) = f1cM(n −na) + · · · + fKacM(n −na −Ka + 1)
where na accounts for the overall propagation delay through the abdomen.
By replacing this expression in (6.1) we end up with a relation between the
disruptive signal from the abdomen sensor and the signal from the chest sensor,
which is written:
vM(n)
=
h−Mxp(n + M) + · · · + h−1xp(n + 1)
+h0xp(n) + · · · + hL−1xp(n −(L −1))
This expression calls for a few comments. The signal xp(n) is not exactly the
one produced by the mother’s heart, because the latter is subjected, according
to (6.1), to a delay np due to the propagation through the thoracic tissue.

134
Digital Signal and Image Processing using MATLAB®
Because the propagation times are unknown, np may very well be greater than
na.
In that case, xp(n) is delayed with respect to vM(n).
This is why we
planned for an “anticausal” part represented by the terms h(−M), . . ., h(−1).
However, if np < na, which certainly is the case in this experiment, the signal
vm(n) is delayed with respect to xp(n) and we have to make sure that the
coeﬃcients h(−M), . . ., h(−1) are also almost null.
In the end, the two signals xp(n) and xv(n), picked up oﬀthe mother’s
chest and abdomen, are such that:
xv(n)
=
h−Mxp(n + M) + · · · + h−1xp(n + 1)
+h0xp(n) + · · · + hL−1xp(n −L + 1) + cF (n)
where cF (n) represents the signal we are trying to determine, originating from
the heart of the fetus. We are going to estimate the coeﬃcients hi in such a
way as to extract cF (n) from the signals xp(n) and xv(n).
Example 6.1 (Extracting the signal cF (n))
The ﬁrst part of our work consists of identifying the abdominal transfer:
1. Establish that estimating the linear sequence h = [h−M . . . h0 . . . hL−1]
is equivalent to a linear problem of the type:
xv = Xph + cF
where xv and Xp are respectively a vector and a matrix with the adequate
sizes, constructed from the observations. cF is a vector that represents
the signal originating from the heart of the fetus.
2. Using the ordinary least squares method, ﬁnd the expression of an esti-
mate of h. Use the result to ﬁnd the estimate of the signal cF (n).
3. Based on the previous result, write a function extract(Xp,Xv,M,L) that
estimates h−M, . . ., h0, . . ., hL−1 and infers an estimate of the signal
cF (n) originating from the heart of the fetus.
Comment: The method considered for the extraction of the signal cF (n)
assumes that the useful information concerning the signal cF (n) (such as its
frequency, but also other characteristics useful to the practitioner) is not con-
tained in the space generated by the columns of the matrix Xp. The likelihood
of this hypothesis can really be evaluated only by whether or not the results
are relevant. If, for whatever reason, a part of the signal originating from the
heart of the fetus actually does belong to the space generated by the columns
of Xp, then this signal is impossible to extract using the suggested method.
If this signal happens to be useful to the practitioner, then another separation
method will have to be considered. In this case, for the estimation of the car-
diac frequency, the results obtained are quite satisfactory.

Selected Topics
135
Hints:
1. The observed sequences are indexed from 1 to N. If we stack
the expressions:
xv(n)
=
h−Mxp(n + M) + · · · + h−1xp(n + 1)
+h0xp(n) + · · · + hL−1xp(n −L + 1) + cF (n)
for n ∈{L, L + 1, . . . , N −M} and if we use a vector notation,
we get:
xv = Xph + cF
where xv = [xv(L)
· · ·
xv(N −M)]T and where:
Xp =


xp(L + M)
· · ·
xp(1)
...
...
...
xp(N)
· · ·
xp(N −L −M + 1)


is a Toeplitz matrix constructed from the observations xp(n).
2. We infer that h = X#
p xv. X#
p refers to the pseudo-inverse
of Xp which can be obtained, either by using the pinv(Xp)
function, or by typing h=Xp \ Xv. Once h has been estimated,
we can ﬁnd an estimation of cF using the expression cF =
xv −XpX#
p xv.
The processing is performed by the program:
%===== separecg.m
% The file FOETUS.DAT contains:
%
xp(2500 * 1) (chest sensor),
%
almost equal to the mother's EKG,
%
xv(2500 * 1) (abdominal sensor)
%
equal to the fetus's EKG plus
%
the filterd mother's EKG: Xv = h * Xp + cf
%
Sampling frequency = 300 Hz
load fetus.dat
xp=fetus(:,1)-mean(fetus(:,1));
xv=fetus(:,2)-mean(fetus(:,2));
N=length(xv);
%===== estimation of h
L=20;
% L causal
M=3;
% M anticausal
Xv=xv(L:N-M);
col=xp(L+M:N); lig=xp(M+L:-1:1);
Xp=toeplitz(col,lig);
h=Xp \ Xv;
% resolution
cf=Xv-Xp*h;
% fetal heart beats
%===== displaying the results

136
Digital Signal and Image Processing using MATLAB®
Nmax=1000; indx=[1:Nmax];
subplot(311); plot(xp(indx)); grid
subplot(312); plot(xv(indx)); grid
subplot(313); plot([zeros(L-1,1);cf(L:Nmax)]); grid
We represented in Figure 6.4, in the bottom graph, the signal ex-
tracted after processing. As you can see, the signal originating from
the mother has been correctly extracted from the signal picked up
by the abdomen sensor (middle graph). However, you can make out
in some places the presence of a very faint residue of the mother’s
heart beats in the bottom graph.
0
100
200
300
400
500
600
700
800
900
1,000
−500
0
500
1,000
0
100
200
300
400
500
600
700
800
900
1,000
−50
0
50
0
100
200
300
400
500
600
700
800
900
1,000
−40
−20
0
20
Figure 6.4 – EKG signals: at the bottom, the EKG signal of the fetus, extracted
from the abdominal signal (middle graph) using the mother’s EKG (top graph)
6.1.3
Estimating cardiac rhythms
We now have to estimate the fundamental frequency of the EKG signal assumed
to be periodic. We have already encountered this problem with pitch detection
in speech, and we used a correlation measurement to perform this estimation.
Here, we are going to give a theoretical justiﬁcation for a least squares approach.
Theoretically, if a centered signal s(n) is periodic with period P, the func-

Selected Topics
137
tion deﬁned by:
g(k) =
lim
K→+∞
∑+K
n=−K s(n + k)s(n)
√∑+K
n=−K s2(n)
√∑+K
n=−K s2(n + k)
(6.2)
is itself periodic with period P. It reaches its maximum in k = 0, and reaches
it again for the various multiples of P.
Example 6.2 (Measuring the fundamental frequency)
Consider a signal s(n), with n = 1 . . . N, periodic with period P, and let
us construct the two length L vectors v0 = [s(1)
. . .
s(L)]T and vk =
[s(k + 1)
. . .
s(L + k)]T . If k is a multiple of the period P, we will have:
vk
∥vk∥= r(k) v0
∥v0∥+ ε
where the coeﬃcient r(k), which is equal to +1 in the ideal case, is used in
the sequence to account for the slight variability of the signal’s amplitude. Let
w0 = v0/∥v0∥and wk = vk/∥vk∥. The previous equation used as a model for
the periodicity of the signal s(n) can be written:
wk = r(k)w0 + ε
(6.3)
Notice that ∥w0∥= ∥wk∥= 1.
1. Using the least squares method, determine the estimator ˆr(k) of r(k) that
minimizes the norm of the error ε.
2. Show that |r(k)| < 1. What does the limit case where r(k) ≲+1 signify?
Use this interpretation to determine a method for estimating the period
P of the signal s(n). How must the length L of the vector v0 be chosen?
3. If the rhythm that has to be estimated is between 80 and 200 beats per
minute, and if the sampling period is equal to 1/300 s, determine the
range of values that must be chosen for P.
4. The previous estimate leads to a value of the period, expressed in seconds,
which is a multiple of the sampling period. How can this accuracy be
improved?
5. Write a MATLAB® function that estimates, using the previous method,
a signal’s fundamental frequency.

138
Digital Signal and Image Processing using MATLAB®
Hints:
1. Directly applying the least squares formula to equation (6.3)
leads to the following estimation of r(k):
ˆr(k)
=
1
wT
0 w0
wT
0 wk =
vT
0 vk
∥v0∥∥vk∥
=
∑L
n=1 s(n + k)s(n)
√∑L
n=1 s2(n)
√∑L
n=1 s2(n + k)
This expression should be compared with the expression (6.2)
deﬁning g(k). We can infer a method for estimating P:
(a) Calculate the sequence ˆr(k) by varying k over a range of
values between Pm and PM.
(b) The maximum of the resulting sequence is determined. If
this maximum is close enough to 1, the signal is said to
be periodic, the maximum’s argument is chosen as the
period.
The choice of L is essentially related with the practical dura-
tion N for which it is reasonable to assume that the signal is
periodic in a stationary sense. Obviously, if we wish to detect
the possible ﬂuctuations of the rhythm, the windows have to
be chosen smaller. Once N is chosen, the square deviation of
the estimation decreases as L increases. Hence, if PM refers
to the maximum value for the period analysis, we can choose
L = N −PM.
2. Using the Schwarz inequality, we have to check that r(k) has a
modulus smaller than 1. When r(k) ≲+1, v0 ≈+vk. We can
then consider that the signal is periodic and that the period is
a sub-multiple of k. In the opposite case, the signal is likely not
to be periodic. This result leads to a procedure for detecting
and estimating the pitch. r(k) is calculated with k belonging
to an exploration range (Pm, PM) of possible values. If the
maximum value of r(k) is greater than a threshold ρ chosen
beforehand, the signal is considered periodic. In this case, the
ﬁrst maximum of r(k), greater then ρ, leads to an estimation of
the period P. The value of ρ is chosen experimentally based on
a large number of observations of the signals to be processed.
3. For a rhythm of B beats per minute, that is b = B/60 beats
per second, the period P expressed as a number of points
is given by the integer part of bFs, where Fs refers to the
sampling frequency.
As such, to explore the pulse range
(Bm, BM), we have to explore the range of values of k deﬁned
by (fix(Bm*Fs/60):fix(BM*Fs/60)).

Selected Topics
139
4. Notice that the values that can be obtained for the period,
expressed in seconds, are multiples of 1/Fs.
Therefore, the
determination accuracy of the correlation function’s maximum
depends on the choice of the sampling rate, but because the
signal is assumed to have been properly sampled, it can be
interpolated with a rate R and thus increase the evaluation
accuracy of the maximum.
Of course, in the same manner
as with the frequency resolution, this increases the accuracy
with which the x-coordinate of the maximum is obtained, but
in no way does it enhance, in the presence of noise, the mean
square error, which is related to the random position of the
maximum.
5. The f0cor.m function detects and estimates the fundamental
frequency:
function [F0,corr]=f0cor(sn,Fs,R,thr,Fmin,Fmax)
%!==================================================!
%! SYNOPSIS: [F0,corr]=F0COR(sn,Fs,R,thr,Fmin,Fmax) !
%!
sn
= signal from which the freq. is extracted !
%!
Fs
= sampling frequency (Hz)
!
%!
R
= oversampling factor
!
%!
thr
= threshold
!
%!
Fmin = min. frequeny (Hz)
!
%!
otherwise Fmin=2*Fs/longueur(sn);
!
%!
Fmax = max. frequency (Hz)
!
%!
otherwise Fmax=Fs/2-Fmin;
!
%!
F0
= fundamental frequency (Hz)
!
%!
corr = correlation sequence
!
%!==================================================!
sn=interp(sn,R); Fs=R*Fs;
N=length(sn); sn=sn(:); sn=sn-mean(sn);
lagmin=fix(Fs/Fmax); lagmax=fix(Fs/Fmin);
corr=zeros(1,lagmax-lagmin+1);
%===== the effects of the window's size can be tested
%
by taking wlg<wlgmax=N-lagmax
wlg=N-lagmax; v0=sn(1:wlg);
for ii=lagmin:lagmax
vP=sn(ii:ii+wlg-1);
corr(ii-lagmin+1)=(v0'*vP)/sqrt((v0'*v0)*(vP'*vP));
end
[niv1, indmax]=max(corr);
if niv1<thr
pf0=0; F0=NaN;
return
else
for ii=lagmin+1:lagmax
if corr(ii-lagmin+1)>niv1*0.9
while corr(ii-lagmin+1)>corr(ii-lagmin)

140
Digital Signal and Image Processing using MATLAB®
ii=ii+1;
end
pf0=ii-2; F0=Fs/pf0;
return
else
F0=Fs/(indmax+lagmin-1);
end
end
end
In the method considered for measuring the pitch in this ex-
ample, the correlation function shows maxima in the multiples
of the period we wish to determine. Therefore, when scanning
the range of possible values, several of these maxima can be
encountered. Because of measurement uncertainty, it is possi-
ble that the highest of these maxima does not correspond to
the period. Therefore, we must search for the possible maxima
at sub-multiples of the one corresponding the highest of these
maxima. The f0cor.m function we have given performs such
an operation by searching for other possible maxima greater
than 0.9 times the highest maximum.
Other more eﬃcient
processes can be designed to solve this problem (see further on
in this paragraph).
Figure 6.5 shows the levels of the correlation functions, in
their respective exploration ranges, for the mother’s EKG (top
graph), and for the EKG of the fetus (bottom graph).
These graphs were obtained using the following program:
%===== extrythm.m
% Rhythm estimation
% This program uses signals from separeg.m
% with the file fetus.mat
% Uses: f0cor.m
pulsemin=50;
%==== beats per mn
pulsemax=300;
%==== beats per mn
%===== oversampling rati
R=2;
maxcor_apriori=0.25;
[F_mother corr_mother]=...
f0cor(xp,Fe,R,maxcor_apriori,pulsemin/60,pulsemax/60);
[F_fetus corr_fetus]=...
f0cor(cf,Fe,R,maxcor_apriori,pulsemin/60,pulsemax/60);
%===== displaying the results
disp('*****************************')
disp(sprintf('* Pulses (mother): %5.2f',60*F_mother));
disp(sprintf('* Pulses (fetus) : %5.2f',60*F_fetus));
disp('*****************************')
%===== displaying the results

Selected Topics
141
50
100
150
200
250
300
350
400
−0.2
−0.1
0
0.1
0.2
0.3
50
100
150
200
250
300
350
400
−0.2
0
0.2
0.4
0.6
Figure 6.5 – Levels of the correlation function for the mother’s EKG (top graph),
and for the EKG of the fetus (bottom graph)
lagminM=fix(60*Fe*R/pulsemax);
lagminF=fix(60*Fe*R/pulsemax);
MinCM=min(corr_mother);
LCM=length(corr_mother); MaxCM=max(corr_mother);
subplot(211);
plot((lagminM:LCM+lagminM-1)/R,corr_mother)
grid; hold on;
plot(Fe/F_mother*[1 1]+1,[min(corr_mother) MaxCM],':');
hold off
%=====
MinCF=min(corr_fetus); MaxCF=max(corr_fetus);
subplot(212);
plot((lagminF:length(corr_fetus)+lagminF-1)/R,...
corr_fetus)
grid; hold on;
plot(Fe/F_fetus*[1 1]+1,[MinCF MaxCF],':');
hold off
Running the program returns:
****************************
*
Pulse (mother): 97.56
*
Pulse (fetus) :
160.71
****************************
We saw previously that the presence of several maxima in the correlation we

142
Digital Signal and Image Processing using MATLAB®
are measuring can pose a problem. One way of eliminating this error consists
of calculating the spectral product deﬁned by:
P(f) =
K
∏
k=1
X(e2jπkf)
2 where f ≤1/2K
(6.4)
where X(f) is the DTFT of a block of the signal. If the latter is periodic with
the fundamental frequency f0 = F0/Fs, its spectrum will show high amplitude
peaks in frequencies that are multiples of the frequency f0. Hence the product
P(f0/2) of the spectrum’s values calculated in multiples of f0/2 will have a
small value. This is due to the almost null values associated with the frequencies
that are odd multiples of f0/2.
This is not the case of the product P(f0)
calculated in f0, which accumulates the spectrum’s amplitudes in f0, 2f0, etc.
Therefore, the function P(f) allows us to do away with the ambiguity in f0/2:
if the frequency estimate ˆf is close to f0/2, the value of P( ˆf) will be much
smaller than P(2 ˆf). Comparing these two values then allows us to choose 2 ˆf
as the fundamental frequency. However, in order for this method to work well,
it requires precise calculation of the spectrum, especially if the spectrum has
very sharp peaks, which implies long calculation times.
Comment: when the signal we wish to estimate the pitch of contains much
higher frequencies than the pitch, we can also, as it is done with some speech
encoders, perform a low-pass ﬁltering before estimating the pitch. For a narrow
band signal around a central frequency Fc, that is a signal in the form of a brief
oscillating impulse, we can also replace the signal x with its envelope.
6.2
Extracting the contour of a coin
We are going to try to determine the ellipse representing the contour of a coin
(Figure 6.6). The process can be achieved in two very diﬀerent ways: the ﬁrst
one performs an approximate extraction of the contour, then applies the least
squares method. The second performs an approximate extraction of the ellip-
tical disk representing the coin, then extracts the contour using a correlation
method.
Example 6.3 (Extracting the elliptical disk) Based on the image of a coin
that you will save in “levels of gray”, write a program that approximately ex-
tracts the elliptical disk corresponding to the inside of the coin, as it is shown
in Figure 6.6.
Hints: type:
%===== preprocesscoin.m
clear all; close all;

Selected Topics
143
load mcoin2; % or imread...
figure(1); set(gcf,'color',[1 1 1])
subplot(221); imagesc(pixc); Spix=size(pixc);
colormap('gray'); axis('image'); title('Coin')
set(gca,'Xcolor',[0 0 0],'Ycolor',[0 0 0])
%===== threshold
minpix=100; ipx0=find(pixc < minpix); yim0=ones(Spix)*255;
yim0(ipx0)=zeros(size(ipx0));
subplot(222); imagesc(yim0); colormap('gray');
axis('image'); title('Threshold')
set(gca,'Xcolor',[0 0 0],'Ycolor',[0 0 0])
%===== gaussian filtering
hg=moygauss(5); yim0g=filter2(hg,yim0);
subplot(223); imagesc(yim0g); title('Gaussian filter')
colormap('gray'); axis('image')
set(gca,'Xcolor',[0 0 0],'Ycolor',[0 0 0])
%===== threshold
minpix2=170; ipx02=find(yim0g < minpix2);
yim02=ones(Spix)*255; yim02(ipx02)=zeros(size(ipx02));
subplot(224); imagesc(yim02); colormap('gray');
axis('image'); title('Threshold')
set(gca,'Xcolor',[0 0 0],'Ycolor',[0 0 0])
save('yim02','yim02')
with:
function hg=moygauss(sigma)
%!==============================!
%! LOW-Pass Gaussian filter
!
%! SYNOPSIS: hg=MOYGAUSS(sigma) !
%!
sigma = standard deviation !
%!
hg
= coefficients
!
%!==============================!
xx=-sigma*5:sigma*5;
g=exp(-xx.*xx / (2*sigma*sigma)); % gaussian
%===== keeping only significant values
hg=g(g>max(g)*.005); hg=hg/sum(hg(:));
hg=hg' * hg;
return
Exercise 6.1 (Ellipse contour: the least squares method) (see p. 231)
Remember that an ellipse can be described by the equation:
ax2
1 + bx2
2 + cx1x2 + dx1 + ex2 −1 = 0 with ab −c2/4 > 0
(6.5)

144
Digital Signal and Image Processing using MATLAB®
Figure 6.6 – Result of pre-processing
where (x1, x2) represents the point in the plane with the x-coordinate x1 and
the y-coordinate x2. A ﬁrst process leads to the least squares estimation over
N points of the ellipse, to the coeﬃcients a, b, c, d and e of equation (6.5).
1. Write a program, based on the program in example 6.3, which extracts an
approximate contour of the coin. You can use the “numerical diﬀerentia-
tion” function (diff) or a Gaussian diﬀerentiation function (dergauss).
function hd=dergauss(sigma)
%!=================================!
%! Gaussian derivative filter
!
%! SYNOPSIS: hd = DERGAUSS(sigma)
!
%!
sigma = Standard deviation
!
%!
hd
= filter with (N*N)-PSF !
%!=================================!
rho=[-sigma*3:sigma*3]; N=length(rho);
rp=1.4; s2=2*sigma^2; s22=s2*rp*rp;
idx= ([1:N]-(N+1)/2)' * ones(1,N); idy=idx';
idxa=[1:N]' * ones(1,N); idya=idxa';
%=====
inds(1,:)=reshape(idx,1,N*N); inda(1,:)=reshape(idxa,1,N*N);
inds(2,:)=reshape(idy,1,N*N); inda(2,:)=reshape(idya,1,N*N);
rho2=sum(inds .* inds); rho=sqrt(rho2);
for k=1:N*N
g1=(1/sigma)*exp(-rho2(k) / s2);
g2=(1/sigma/rp)*exp(-rho2(k) / s22);
hd(inda(2,k),inda(1,k))=g1-g2;

Selected Topics
145
end
hd=hd-sum(sum(hd))/N/N;
2. Using the “least squares method”, determine the coeﬃcients a, b, c, d,
and e of the ellipse closest to the contour that we found. Write a function
that plots the ellipse deﬁned by equation (6.5).
Do so by rewriting the equation of the ellipse in the form (x−x0)T E(x−
x0) = γ, where x = [x1
x2]T , then determine the expressions of E, x0
and γ as functions of a, b, c, d, and e.
Exercise 6.2 (Ellipse contour: the covariance method) (see p. 232)
Consider a sequence of N points on the plane, described by N random vari-
ables x1, . . . , xN assumed to be independent and uniformly distributed on the
elliptical disk deﬁned by its contour:
(x −µ)T M −1(x −µ) = 1
where M refers to a positive matrix and µ is the center of the ellipse. Let:
yn = M −1/2(xn −µ)
(6.6)
You can easily check that the sequence yn constitutes a sequence of in-
dependent random variables uniformly distributed on the circular disk with a
unit radius.
1. Determine the expression of the vector E {y1} and of the matrix E
{
y1yT
1
}
.
2. According to the law of large numbers, if yn refers to a sequence of
independent random vectors with the mean ν and the covariance C then,
when N tends to inﬁnity:
1
N
N
∑
n=1
(yn −ˆνN) (yn −ˆνN)T
a.s.
−→C
where ˆνN = N −1 ∑N
n=1 yn. By applying the law of large numbers to the
sequence y1, . . . , yN, infer that:
M −1 ≈4
N
N
∑
n=1
(xn −ˆµN) (xn −ˆµN)T where ˆµN = 1
N
N
∑
n=1
xn
3. Apply these results to the problem of determining the ellipse closest to
the contour of the coin.

146
Digital Signal and Image Processing using MATLAB®
6.3
Constrained optimization and Lagrange
multipliers
This section constitutes an introduction to the problem of constrained opti-
mization and Lagrange multipliers. To illustrate the issue, we have chosen to
present the Modern Portfolio Theory put forward by H. Markowitz.
In this section, the notation h(x) ≤0p is used for:











h1(x1, . . . , xn) ≤0
...
hp(x1, . . . , xn) ≤0
(6.7)
6.3.1
Equality-constrained optimization
Consider the following problem:



min J(x)
h(x) = 0p
(6.8)
where J is a function of Rn in R, and h(x) a function of Rn in Rp. We write:
L (x, λ) = J(x) + λT h(x)
(6.9)
where λ ∈Rp. The function L is called the Lagrangian of the problem and
λ the Lagrangian multiplier vector. Under very general regularity conditions,
e.g. when J is convex, we can show [35] that if xo is a local minimum – it
satisﬁes:
∃λo ∈Rp, xo ∈Rn
s.t.
{ ∂L
∂x (xo) = ∂J
∂x(xo) + λo
∂h
∂x(xo) = 0
h(xo) = 0
(6.10)
It is worth noting that, according to the ﬁrst equation in (6.10), the normals
to the functions J and h coincide at the point xo. Note that the second equation
can be seen as the cancellation of L with respect to λ.
For simplicity’s sake, the optimum will be denoted either by an asterisk (*),
or by the letter o.

Selected Topics
147
Example 6.4 (Production of goods)
Let G be the gain (or proﬁt) obtained by the sale of two goods with respective
unit prices p1 and p2. Thus, we have G = p1x1 + p2x2. We seek to maximize a
function of the form xα
1 xβ
2 (Cobb–Douglas function [10]), and therefore mini-
mize J(x1, x2) = −xα
1 xβ
2, under the constraint G−p1x1 −p2x2 = 0. It is shown
that the function J is convex if α ≥0, β ≥0 and α + β ≤1. The Lagrangian
is written:
L (x1, x2, λ) = −xα
1 xβ
2 + λ(G −p1x1 −p2x2)
At the minimum, we have:
−αx∗α−1
1
x∗β
2 −p1λ∗= 0 ⇒λ∗= −αx∗α−1
1
x∗β
2
p1
−βx∗α
1 x∗β−1
2
−p2λ∗= 0 ⇒λ∗= −βx∗α
1 x∗β−1
2
p2
G −p1x∗
1 −p2x∗
2 = 0
From the ﬁrst two expressions, we can deduce:
αx∗α−1
1
x∗β
2
p1
= βx∗α
1 x∗β−1
2
p2
⇒x∗
2 = x∗
1
β
α
p1
p2
With the constraint, we obtain (Figure 6.7):
x∗
1 =
αG
(α + β)p1
and x∗
2 =
βG
(α + β)p2
In Figure 6.7, we have plotted the curves deﬁned by J(x1, x2) = constant
and also the normal to the constraint line the equation of which is p1x1 +
p2x2 = G. This normal is normal only to one of the curves, which is deﬁned
by Jo = 40.4886 at the coordinate x1 = 7.2 and x2 = 72.
Example 6.5 (Minimization with quadratic constraints)
Consider the following problem:



min J(x) = 1
2xT Qx
with xT x = 1
where Q is a positive, invertible matrix, of dimension n and x ∈Rn. Determine
the solution.
Hints: the Lagrangian of the problem is written as:
L (x) = 1
2xT Qx + λ(1 −xT x)

148
Digital Signal and Image Processing using MATLAB®
0
5
10
15
20
J=20
80
120
25
0
50
100
150
200
250
300
350
Figure 6.7 – Optimization of utility function; α = 0.25, β = 0.75, p1 = 10, p2 = 3,
G = 288
and therefore :
∂xL (x) = Qx −λx = 0
Consequently, Qx = λx, so x is one of the eigenvectors of Q. We deduce from
this that the minimum of J(x) must be equal to 1
2λ and therefore 2J(x) is
necessarily equal to an eigenvalue. As Q is positive, it is diagonalizable and its
eigenvalues are positive. It follows from this that the minimum we seek is given
by the lowest eigenvalue of Q, and x is the eigenvector associated therewith.
We can verify that these expressions are still valid when x ∈Cn.
Example 6.6 (Maximum entropy in a system under mean constraints)
Consider a sequence pk ≥0 where k ∈N and such that ∑
k pk = 1. The series
p = {pk} is interpreted as a probability distribution. We use the term “entropy
of the series” for the concave function:
h(p) = −
+∞
∑
k=0
pk log pk
(6.11)
We note that 0 ≤pk ≤1 and therefore h ≥0. We wish to solve the following
problem:









max h(p)
∑+∞
k=0 pk = 1
∑+∞
k=0 kpk = m

Selected Topics
149
Note that m ≥0. Maximizing h(p) is tantamount to minimizing the convex
function J(p) = −h(p).
Hints: the Lagrangian is written as:
L (p, λ) =
∑
k
pk log pk + λ1
(
1 −
∑
k
pk
)
+ λ2
(
m −
∑
k
kpk
)
The cancellation of the k-th component of the gradient with respect to p is
written as:
∂pkL (p, λ) = 1 + log pk −λ1 −λ2k = 0
hence:
pk = e−1+λ1+λ2k = µγk
µ and γ are determined by satisfying the constraints:
∑
k
pk = µ
1
1 −γ = 1 and
∑
k
kpk = µ
γ
(1 −γ)2 = m
Therefore, γ =
m
m + 1, µ = (1 −γ) and pk = (1 −γ)γk.
6.3.2
Quadratic problem with linear inequality constraints
Consider the following quadratic problem:











min
(1
2xT Qx + wT x
)
Ax + b = 0,
Cx + d ≤0
(6.12)
where Q is a strictly positive matrix of dimensions n×n, w is a vector of length
n, A a positive matix of dimensions p × n, b a vector of length p, C a matrix
of dimensions q × n and d a vector of length q.
The Lagrangian is:
L (x, λ, µ) = 1
2xT Qx + wT x + λT (Ax + b) + µT (Cx + d)
(6.13)
where λ ∈Rp and µ ∈Rq+.
We can show that xo is a solution to the problem if and only if:
{
∃λo ∈Rp, µo ∈Rq+, xo ∈Rn? = 0
for all
i = 1, . . . , q
(6.14)
where Ci denotes the row i in the matrix C.

150
Digital Signal and Image Processing using MATLAB®
The set of equations (6.13) are called the Karush–Kuhn–Tucker (KKT)
conditions. The condition µi,o ≥0 is called the duality constraint. The last
condition, known as complementarity, implies that if µo,i is strictly positive,
then the constraint of inequality becomes one of equality. The constraint is
said to be saturated or active.
Example 6.7 Determine the solution to the problem:







min J(x) = 1
2xT Qx
xT u = 1,
xT v ≤a
where a ∈R, where x ∈R3, where u =
[1
1
1]T , where v ∈R3, and where
Q is a strictly-positive matrix of dimensions 3 × 3, with real elements. Note
that we can replace x3 with (1 −x1 −x2) in J, numerically calculate J in
the plane (x1, x2) and seek the minimum which is below the line deﬁned by
v1x1 + v2x2 + v3(1 −x1 −x2) = a.
Hints: The Lagrangian is written as:
L (x, λ, µ) = 1
2xT Qx + λ(xT u −1) + µ(xT v −a)
Using the KKT conditions, we obtain:
∃λ ∈R, µ ∈R+and x ∈R3
s.t.









Qx + λu + µv = 0
λ(xT u −1) = 0
µ(xT v −a) = 0
(6.15)
and therefore:
x = −(λQ−1u + µQ−1v)
We determine µ by applying the condition of complementarity. Thus, we
must distinguish two cases:
– the constraint xT v −a ≤0 is not active, and therefore µ = 0 and x =
−λQ−1u.
λ is determined by writing that uT x = 1, which gives us
x = (uT Q−1u)−1Q−1u, and 2J1 = (uT Q−1u)−1 > 0.
– the constraint xT v −a ≤0 is made active, so xT v −a = 0 and:
x = −(λQ−1u + µQ−1v)

Selected Topics
151
and to determine λ and µ we apply the equality constraint and active
inequality constraint, which gives:
{ λuT Q−1u + µuT Q−1v = −1
λvT Q−1u + µvT Q−1v = −a
{ λA + µB = −1
λB + µC = −a
where we have set:
A = uT Q−1u,
B = vT Q−1u
and
C
=
vT Q−1v
Note that A ≥0, C ≥0 and that, according to Schwarz’s inequality,
AC −B2 > 0. By substituting the value into J, the vector Qx given by
the ﬁrst expression of (6.14), we obtain:
2J2
=
xT Qx
=
(λQ−1u + µQ−1v)T (λu + µv)
=
(λ2A + µ2C + 2λµB)
=
−(λ + µa)
=
[1
a] [A
B
B
C
]−1 [1
a
]
> 0
To conclude, we need to take the lower of the two values 2J1 = 1/A and 2J2.
In Figure 6.8, we have illustrated the function J in relation to the plane
xT u = 1 and calculated on a value grid.
−4
−2
0
2
−2
−1
0
1
2
3
Figure 6.8 – Function J from example 6.7 in relation to the plane (x1, x2) where
x3 = 1 −x1 −x2. The circle indicates the minimum obtained analytically by
the Lagrangian and the cross denotes the minimum obtained on a grid in the plane
xT u = 1

152
Digital Signal and Image Processing using MATLAB®
Type:
%===== exeminQ3linineg.m
clear all
u = [1;1;1]; v = [2 ;-3.83;4.9]; a = 4;
%=====
Q positive matrix
Q=[1.2 1.1 -3.8;...
1.1 3.5 -3.3;...
-3.8 -3.3 19.6];
%=====
A = u'*(Q\u); B = u'*(Q\v); C = v'*(Q\v);
M = [A B;B C];
%===== solution with x^Tv<a (non active)
K1 = 1/A;
xo1 = (Q\u)/A;
% solution with x^Tv=a (active)
mu = M\[-1;-a];
xo2 = -(Q\[u v])*mu;
K2 = -[1 a]*mu;
%===== numerical computation in the plane
%
(x_1,x_2) with x_3=1-x1-x_2
L1 = 200; L2 = 300;
lx1 = linspace(-5,3,L1);
lx2 = linspace(-2,3,L2);
K = zeros(L2,L1);
for i1=1:L1
for i2=1:L2
x = [lx1(i1);lx2(i2);1-lx1(i1)-lx2(i2)];
K(i2,i1) = x'*Q*x;
end
end
minK = min(min(K));
[locmin2,locmin1] = find(K==minK);
imagesc(lx1,lx2,log(K))
hold on
%===== inequality constraint x^Tv < a
plot(lx1,(a-v(3)-(v(1)-v(3))*lx1)/(v(2)-v(3)),'w')
plot(xo1(1),xo1(2),'ow')
plot(lx1(locmin1),lx2(locmin2),'xw','markers',12)
hold off
xlabel('x_1','fontn','times')
ylabel('x_2','fontn','times','rot',0), grid on
We can see in example 6.7 that, when we have inequality constraints, we
must review all possible cases of activation of the constraints. If n is large, this
may lead to an impossible exhaustive analysis. This is why numerical methods
have had to be developed. However, the study of these algorithms goes beyond
the bounds of this book.

Selected Topics
153
6.3.3
Portfolio optimization
The placement of investments on the ﬁnancial markets raises the following two
questions:
1. Which stocks should we choose?
2. What proportion of our portfolio should we assign to each stock holding?
The purpose of the portfolio theory is to answer these two questions. It was a
long time before ﬁnancial theory ventured into the notion of uncertainty. The
indisputable pioneer in this area was Harry Markowitz: his ﬁrst article on the
subject was published in 1952 [23] and his ﬁrst book in 1959 [24]. For his work,
he received the Nobel Memorial Prize in Economic Sciences in 1990.
Markovitz’s work was opposed to the idea which was widely accepted at the
time, i.e. that the investor chooses the portfolio distribution which will generate
the highest revenue. Indeed, if this were the case, then investors would only
ever choose one investment: the one with the highest rate of return. This runs
counter, though, to the phenomenon observed in real life, which is the diversi-
ﬁcation of portfolios. To explain this mechanism of diversiﬁcation, Markovitz
introduced a probabilistic model which introduces the notions of return and
risk. On the mathematical level, he constructed a fairly straightforward prob-
lem of quadratic optimization. The originality of the work essentially lies in
the application of this engineering model to the world of ﬁnance.
Generally speaking, all investors are, by nature, “risk-averse”.
However,
they are prepared to take more risks in exchange for a better return. Markowitz’s
idea was to link the risk to the variability of the assets.
The model is static, in the sense that we consider only two periods:
– the initial period, 0, when the investor is constructing his or her portfolio;
– and the ﬁnal period, T, when the yields from the assets are recorded and
the ﬁnal value of the portfolio is determined.
We use the following notation:
– n for the number of assets (shares) making up the ﬁnancial market;
– B0,i and BT,i for the respective prices of the asset i, at moments which
are a length of time T apart;
– the return ρi on the asset i for the period T is then deﬁned by:
BT,i = B0,i(1 + ρi)T ⇒ρi = −1 +
(BT,i
B0,i
)1/T
(6.16)

154
Digital Signal and Image Processing using MATLAB®
Sometimes we add to BT,i any revenue perceived in the form of dividends.
In any case, ρi is a number which may be positive or negative. Evidently,
its modulus may be greater than 1. However, over short lengths of time,
this number is generally small. It is typically to multiply it by 100 so as
to be able to express it as a percentage.
Markowitz’s model states that the returns ρi of assets on the market are
random variables. We note the mean vector:
m=E {ρ} =
[E {ρ1}
. . .
E {ρn}]T ∈Rn
(6.17)
and covariance matrix:
Q =


cov(ρ1, ρ1)
cov(ρ1, ρ2)
. . .
cov(ρ1, ρn)
cov(ρ2, ρ1)
cov(ρ2, ρ1)
. . .
cov(ρ2, ρn)
...
cov(ρn, ρ1)
cov(ρn, ρ2)
. . .
cov(ρn, ρn)


Remember that a covariance matrix is positive – particularly its determi-
nant and its eigenvalues are positive. If Q is strictly positive, its determinant
is strictly positive and its inverse exists, which we will assume.
Hereafter, we will consider that the vector m and the matrix Q are known.
In practice, these quantities are estimated on the basis of observations of the
market over long periods.
Let W be the investor’s original wealth and Bi ≥0 the sum invested in the
asset i. The proportion of the asset Bi in the portfolio is deﬁned by:
xi = Bi
W ∈(0, 1)
(6.18)
with the budgetary constraint:
∑
i
xi = 1, or
xT u = 1
(6.19)
where x =
[x1
. . .
xn
]T and where u is a column vector whose n components
have the value of 1.
The problem of the construction of a portfolio therefore lies in determining
the proportions xi. The rate of return of a portfolio, comprising the proportions
xi of the shares on the market, is given by the random variable:
Θ =
n
∑
i=1
xiρi
(6.20)
whose average is:
E {Θ} = xT m
(6.21)

Selected Topics
155
and whose variance or volatility has the expression:
var {Θ} = xT Qx
(6.22)
H. Markowitz stipulates that the variability σ =
√
xT Qx is the measure-
ment of the risk. The greater the value of σ, the greater the risk, but also the
higher the potential return. Consequently, the greater the risk that investors
are willing to accept, the greater will be the expected gain. In practice, a tai-
lored survey is used to estimate the level of risk that an individual is willing to
take.(1) For people who have no taste for risk, the market generally includes
an investment with a ﬁxed return, risk-free, i.e. with zero volatility. Of course,
the return on such an investment is low.
The optimal portfolio is deﬁned as the solution to the problem which mini-
mizes the risk with a level of expected return which is higher than or equal to a
given level ρe. The optimal portfolio therefore needs to be found among all the
portfolios which are acceptable solutions to the problem – known as eﬃcient
portfolios:











min J(x) = 1
2 xT Qx
xT u = 1
xT m≥ρe
∀i, xi ≥0
(6.23)
It is important to note that Q is positive, because it is a covariance matrix.
Hereafter, we will suppose that it is strictly positive, and therefore invertible.
Given the positive nature of Q, the problem of minimization is convex and,
therefore, has a global minimum which satisﬁes the KKT conditions (6.13).
Optimal portfolios are calculated on the basis of the market data, i.e. the
vector m of the average returns and the covariance matrix Q. For each value
of the return ρe in which we are interested, we can associate with it the dis-
tribution xe and the risk σe of the portfolio. A pair (σe, ρe) constitutes an
eﬃcient portfolio and the set of eﬃcient portfolios forms a curve called the
market frontier. Each point on the curve is indexed by its distribution xe.
On the basis of the market frontier, the investor determines the construction
of his/her portfolio of assets depending on his/her taste for risk.
Example 6.8 We use the notation Jm for the solution to the problem (6.23)
and Km for that of the following problem:



min 1
2xtQx
ut x = 1
(1)On the Internet, there are many sites where users can evaluate their own risk-acceptance.

156
Digital Signal and Image Processing using MATLAB®
Show that:
Jm ≥
1
utQ−1u
Conclude.
Hints: Given that the problem (6.23) is subject to more stringent con-
straints than the problem at hand here, we have Jm ≥Km. The Lagrangian
of the current problem is: L (x, λ) = 1
2xT Qx + λ(xT u −1). By deriving in
relation to x we obtain:
Qxo = −λu ⇒xo = −λQ−1u
By applying the stress uT x = 1, we ﬁnd λ = −1/(uT Q−1u) and then
xo = Q−1u/(utQ−1u) and therefore
Km =
1
utQ−1u ⇒Jm ≥
1
utQ−1u
From this, we conclude that there is a risk which depends only on the market
(by way of the matrix Q), below which it is not possible to go.
Thus, we can expect a decrease of the risk:
– when the number of assets n in the portfolio increases;
– when we choose a pair of assets which vary in an inverse direction – i.e.
which have a negative correlation. This is what we can see in the exercise
6.4. For a long time, investors have been using such pairs where the gain
on one of the assets compensates for the losses on another. For exam-
ple, we can cite the negative correlation of the share/obligation couple.
Negative correlations are the cornerstone of diversiﬁcation.
Exercise 6.3 (Optimization of a portfolio with two assets) (see p. 233)
Consider a portfolio containing two assets. Q is the risk matrix deﬁned by:
Q =
[
q1
ρ√q1q2
ρ√q1q2
q2
]
where q1 and q2 are positive and −1 < ρ < 1. We take m=
[m1
m2
]T . We
set:
A
=
uT Q−1u
B
=
mT Q−1u
C
=
mT Q−1m
Note that A ≥0, C ≥0 and that, in view of Schwarz’s inequality, AC−B2 >
0. These quantities can be pre-calculated.

Selected Topics
157
1. Solve the problem (6.23) of portfolio optimization.
2. Write a program to determine the market frontier.
Exercise 6.4 (Negative correlation) (see p. 236)
Look again at the statement of exercise 6.3 and write a program which demon-
strates the decrease in risk when ρ becomes negative.
Market containing a risk-free investment
Consider the case where the market includes a risk-free investment – i.e. with
zero variance and return ρf. We set x =
[
xf
xT
a
]
where xf represents the
proportion of the portfolio invested in the share with a ﬁxed return rate of ρf
and xa =
[xa,1
. . .
xa,n
]T the vector of the n components of the portions
invested in the risky shares. The symbol ma represents the average yield vector
for the risky shares. The problem of optimization can be rewritten as:









































min J(x) = 1
2
[
xf
xT
a
]

0
0T
n
0n
Q



xf
xa


[
xf
xT
a
]

1
u

= 1
[
xf
xT
a
]

ρf
ma

≥ρe
∀i = 1 · · · n,
xa,i ≥0
xf ≥0
⇔























min J(x) = 1
2 xT
a Qxa
xf + xT
a u = 1
xT
a ma + xfρf ≥ρe
∀i = 1 · · · n,
xa,i ≥0
xf ≥0
(6.24)
It should be noted that the constraint xf + xT
a u = 1, and the constraints
xa,i ≥0 and xf ≥0, imply that xa,i ≤1 and xf ≤1.
If the client is particularly risk-averse then the optimal portfolio comprises
only the risk-free investment and, in this case, xf = 1 and the return achieved
is ρf. The frontier is reduced to the single point (0, ρf). Hereafter we will
therefore suppose that the expected return ρe ≥ρf.
Ma = ma(jM) is the maximum value of the returns on the risky investments.
We suppose that Ma > ρf, or else the optimal solution is xa = 0, xf = 1 and
the return is ρf and the risk is nonexistent. Obviously, the return which is
achieved satisﬁes:
xfρf + xT
a ma ≤xfρf + xT
a uMa ≤xfMa + xT
a uMa = Ma
In conclusion, the desired return ρe satisﬁes ρf ≤ρe ≤Ma. When the
desired return is ρf then xf = 1, when the desired return is Ma, xf = 0 and

158
Digital Signal and Image Processing using MATLAB®
xa has a single non-null component equal to 1, in position jM (which is an
admissible point in the sense that it satisﬁes the constraints and activates the
constraint of return).
If xf > 0, the optimal portfolio shifts along a line running from the point
(0, xf) to a curve which depends only on the assets on the market.
Let
(xa,o, xf,o) be the optimal solution to the problem (6.24). The associated risk
is written as:
σo =
√
xTa,oQxa,o
We set:
za,o = xa,o
xTa u =
xa,o
1 −xf,o
,
⇒zT
a,ou = 1
(6.25)
za,o has a clear meaning: it represents the distribution of the part (1−xf,o)
of the portfolio given over to risky investments.
With these notations, the
expression of the risk is:
σo = (1 −xf,o)
√
zTa,oQza,o
and the associated return is:
ρo
=
xf,oρf + xT
a,oma = xf,oρf + (1 −xf,o)zT
a,oma,o
=
ρf + (zT
a,oma −ρf)
σa,o
σo
(6.26)
where we have set σ2
a,o = zT
a,oQza,o. Thus, we can see that the optimal return
as a function of the risk is on a line with the slope:
ρ = (zT
a,oma −ρf)
σa,o
called Sharpe’s ratio. The straight line deﬁned by equation (6.26) is called the
Capital Allocation Line (CAL). Using the fact that zT
a,ou = 1, we can rewrite
Sharpe’s ratio thus:
ρ = zT
a,o(ma −ρfu)
σa,o
(6.27)
Note that ρo ≥ρf. Indeed, the portfolio (xf = 1, xa = 0n) is acceptable,
its risk is null (0 is an absolute minimum) and its return is ρf. The positive
diﬀerence ρo −ρf is called the risk premium. From this, it follows that, in light
of equation (6.26), za,oma ≥ρf. Finally, if xf,o = 0, ρo = zT
a,oma. We will
now maximize Sharpe’s ratio.

Selected Topics
159
Maximization of Sharpe’s ratio
The problem of maximization of Sharpe’s ratio is written as:
P1 :





max J(za) = zT
a (ma −ρfu)
√
zTa Qza
za ∈D
(6.28)
where:
D =
{
za ∈Rn
s.t. zT
a u = 1, zT
a ma ≥0, za ≥0
}
(6.29)
Note that J(za/s) = J(za) for all values of s ̸= 0. Thus, let us consider
the domain:
¯D =
{
(za, s) ∈Rn × R
s.t. za
s ∈D
}
∪{0n, 0}
(6.30)
We have the following property:
P1 :
{ max J(za)
za ∈D
⇔
P2 :
{ max J(za)
za ∈D
Indeed, if we use the notation za,o for an optimal solution to P1 and
(z#
a,o, s#) for an optimal solution to P2, we have:
– (sza,o, s) ∈D, therefore J(za,os/s) = J(za,o) ≤J(z#
a,o);
– z#
a,o/s# ∈D, so J(z#
a,os#/s#) = J(z#
a,o) ≤J(za,o).
and hence J(za,o) = J(z#
a,o).
In the problem P2, we can now choose the parameter s in such a way that
(ma−ρfu)tza will be equal to 1. Indeed, by dividing za by s = (ma−ρfu)tza,
we obtain ˜za = za/s which satisﬁes (ma−ρfu)T ˜za = 1 and such that J(za) =
J(˜za).
Consequently, we can replace the problem P2, and therefore problem P1 as
well, with the following problem, which is easier to solve:









min J(x) = 1
2zT
a Q za(ma −ρfu)T za = 1
maT za ≥0
za ≥0
(6.31)
These results are shown in Figure 6.9.
The CAL is the tangent to the
frontier of the risky investment market, passing through the point (0, ρf). For
a degree of risk less than σ∗
a (see Figure 6.9) the optimal portfolio lies on the

160
Digital Signal and Image Processing using MATLAB®
0
0.02
0.04
0.06
0.08
0.1
0
1
2
3
4
5
Risk
Return
Figure 6.9 – Optimal portfolios: the point of intersection between the Oy axis and
the line has the coordinates (0, ρf), to which xf = 1 corresponds. The point of con-
tact (σa, ρa) between the line and the curve also corresponds to xf = 1. Any given
point on the line is associated with 0 < xf < 1
CAL in accordance with a distribution given by equation (6.26). For a degree
of risk greater than σ∗
a, xf = 0 and the optimal portfolio contains solely risky
investments and follows the frontier which is a solution to the problem (6.24).
The fundamental property is that the solution to the problem (6.31) depends
solely on the market parameters, i.e. ma, ρf and Q, and thus is independent of
the expected return ρe. In that sense, this solution constitutes a market index.
Consequently, if certain ﬁnancial organizations reproduce this indicator, it will
be suﬃcient for the investor to refer to those organizations (this is known as
passive management) and, on the basis of his/her taste for risk, decide upon
the proportion xf and (1−xf) of his/her portfolio to be attached, respectively,
to the risk-free investment and the risky investments.
We show, under general conditions [6], that in the presence of two uncor-
related investments funds, the optimal portfolio is a linear combination of two
optimal portfolios associated, respectively, with each fund. This constitutes
what is know as the two-fund separation theorem. Equation (6.26) represents
a special case, where one of the two funds is risk-free.
Example 6.9 Consider a market with nine shares, whose frontier over the
space of a month is illustrated in Figure 6.10 and whose distribution za,o are
given in Table 6.1. These elements are market data.
Suppose that the risk-free investment has an annual return of 0.01 and the
investor’s risk-acceptance is σo = 0.03. Determine the optimal portfolio over a
month.

Selected Topics
161
0
0.02
0.04
0.06
0.08
0.1
0
1
2
3
4
5
Risk
Return
Figure 6.10 – Optimal portfolios on the share market (the curve illustrates the ﬁrst
two columns from Table 6.1)
Hints:
– the monthly return of the risk-free investment is:
ρf = (1 + 0.01)1/12 −1 = 8.3 × 10−4
We plot the tangent from the point (0, ρf) which touches the curve at
the point (σa = 0.066, ρa = 0.0036).
– from the value σo = 0.03 we deduce the optimal portfolio which is on the
CAL and a return rate of ρo = 0.0021;
– from the table we deduce the value ρa = 0.0036 that the distribution za,o
is given by the 19th row in the table;
– the non-risk/risk distribution is associated with xf by the relation:
xf = 1 −σo
σa
= 0.55
and therefore xa = 0.45za,o. In summary, the portfolio repartition is:
xf = 0.55, xa,1 = 0.45 × 0.0666, . . . , xa,9 = 0.45 times0.0225
which presents the return of 21 × 10−4 instead of the 8.3 × 10−4 return
of the non risky asset.
As previously stated, the higher the number of assets, the more complicated it
becomes to take account of inequalities. For this reason, in exercise 6.5 we use
the function fmincon from the toolbox optim in MATLAB®.

162
Digital Signal and Image Processing using MATLAB®
ρa
σa
za,1
za,2
za,3
za,4
0.0000
0.0354
0.0434
0.3308
0.0086
0.0000
0.0002
0.0354
0.0434
0.3308
0.0086
0.0000
0.0004
0.0354
0.0434
0.3308
0.0086
0.0000
0.0006
0.0354
0.0434
0.3308
0.0086
0.0000
0.0008
0.0354
0.0434
0.3308
0.0086
0.0000
0.0010
0.0354
0.0434
0.3308
0.0086
0.0000
0.0012
0.0354
0.0434
0.3308
0.0086
0.0000
0.0014
0.0354
0.0434
0.3308
0.0086
0.0000
0.0016
0.0354
0.0434
0.3308
0.0086
0.0000
0.0018
0.0359
0.0500
0.3338
0.0622
0.0021
0.0020
0.0376
0.0537
0.3247
0.0810
0.0361
0.0022
0.0398
0.0574
0.3157
0.0999
0.0701
0.0024
0.0425
0.0611
0.3066
0.1187
0.1041
0.0026
0.0456
0.0648
0.2975
0.1376
0.1381
0.0028
0.0490
0.0671
0.2780
0.1690
0.1733
0.0030
0.0530
0.0693
0.2581
0.2009
0.2086
0.0032
0.0573
0.0715
0.2382
0.2328
0.2439
0.0034
0.0618
0.0737
0.2182
0.2647
0.2792
0.0036
0.0666
0.0759
0.1983
0.2966
0.3145
0.0038
0.0716
0.0772
0.1747
0.3272
0.3500
0.0040
0.0768
0.0752
0.1391
0.3534
0.3862
0.0042
0.0821
0.0728
0.1024
0.3795
0.4226
0.0044
0.0876
0.0643
0.0533
0.4040
0.4615
0.0046
0.0932
0.0558
0.0043
0.4285
0.5004
0.0048
0.0990
0.0000
0.0000
0.4503
0.5497
0.0050
0.1077
0.0000
0.0000
0.2974
0.7026
Table 6.1 – Risk (σa) and return (ρa) of the optimal portfolio on nine assets. The
other four columns represent the proportion of the portfolio invested in the four as-
sets. The other ﬁve are given in Table 6.1
Exercise 6.5 (Optimization of a portfolio) (see p. 237)
Consider a market including one risk-free investment with a return of 0.01 and
6 investments with respective returns of 0.03, 0.04, 0.08, −0.02, 0.06 and 0.08.
Its volatility matrix is:
Q = 0.01


4
2
−1
−3
0
2
2
7
0
1
4
−1
−1
0
20
7
−1
13
−3
1
7
10
−2
4
0
4
−1
−2
11
−7
2
−1
13
4
−7
18


Using the function fmincon in MATLAB®, plot the market frontier.

Selected Topics
163
ρa
σa
za,5
za,6
za,7
za,8
za,9
0.0000
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0002
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0004
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0006
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0008
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0010
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0012
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0014
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0016
0.0354
0.1556
0.0000
0.1895
0.0546
0.2174
0.0018
0.0359
0.1271
0.0000
0.1685
0.0525
0.2038
0.0020
0.0376
0.0956
0.0000
0.1607
0.0530
0.1950
0.0022
0.0398
0.0641
0.0000
0.1530
0.0536
0.1862
0.0024
0.0425
0.0327
0.0000
0.1453
0.0541
0.1774
0.0026
0.0456
0.0012
0.0000
0.1375
0.0547
0.1686
0.0028
0.0490
0.0000
0.0000
0.1212
0.0514
0.1400
0.0030
0.0530
0.0000
0.0000
0.1046
0.0479
0.1106
0.0032
0.0573
0.0000
0.0000
0.0880
0.0444
0.0813
0.0034
0.0618
0.0000
0.0000
0.0713
0.0410
0.0519
0.0036
0.0666
0.0000
0.0000
0.0547
0.0375
0.0225
0.0038
0.0716
0.0000
0.0000
0.0374
0.0336
0.0000
0.0040
0.0768
0.0000
0.0000
0.0179
0.0282
0.0000
0.0042
0.0821
0.0000
0.0000
0.0000
0.0227
0.0000
0.0044
0.0876
0.0000
0.0000
0.0000
0.0169
0.0000
0.0046
0.0932
0.0000
0.0000
0.0000
0.0110
0.0000
0.0048
0.0990
0.0000
0.0000
0.0000
0.0000
0.0000
0.0050
0.1077
0.0000
0.0000
0.0000
0.0000
0.0000
Table 6.1 – (continued)
6.4
Principal Component Analysis (PCA)
The principal component analysis (or PCA) provides a simple way of reducing
a complex set of data by projecting it onto a space with a small dimension while
preserving as much of the variability as possible. This method has the advan-
tage of being linear, and makes no hypothesis concerning the data distribution.
It is widely used in a number of applications.

164
Digital Signal and Image Processing using MATLAB®
6.4.1
Determining the principal components
Consider N length d vectors x1, . . ., xN each one of them associated with a
set of d measurements, and X the matrix:
X =
[x1
. . .
xN
]
=


x1,1
x1,2
. . .
x1,N
...
...
...
xd,1
xd,2
. . .
xd,N


X can be understood from two points of view:
– either as a set of N columns xn, each one of them representing d factors
associated with a same individual. This leads to N points in the space
Rd;
– or as d lines, each one of them representing the same factor for N indi-
viduals. This leads to d points in the space RN.
We wish to reduce the number of factors to keep only the k < d most
signiﬁcant ones.
First consider the case where k = 1. We have to search in Rd the direction
of the unit vector v such that the projection of the set of the xn onto this
direction leads to the scatter of N points with the highest dispersion (Figure
(6.11) for d = 2).
Figure 6.11 – Projecting the samples for the directions v1 and v2: the dispersion of
the projected points is more favorable to an analysis for the vector v1 than it is for
v2
This can be interpreted as follows: if we can only keep one component, it
might as well be the one that best separates all of the points. A classic criterion
to evaluate this dispersion is to consider the sum of the distances between all of
the projected points. Remember that the projection of xn is given by vvT xn

Selected Topics
165
and that the distance between any two points is written ∥vvT xi −vvT xj∥2 =
vT (xi −xj)(xi −xj)T v. The criterion to be minimized is then written:
J(v) =
1
2N 2 vT


N
∑
i=1
N
∑
j=1
(xi −xj)(xi −xj)T

v
Let ¯x = N −1 ∑N
j=1 xj. Therefore:
J(v)
=
1
2N 2 vT


N
∑
i=1
N
∑
j=1
[(xi −¯x) −(xj −¯x)][(xi −¯x) −(xj −¯x)]T

v
=
vT RNv
where we have:
RN = 1
N
N
∑
n=1
(xn −¯x)(xn −¯x)T
(6.32)
Notice that the (d×d) matrix RN can be interpreted as a covariance matrix.
Hence the problem can be laid out as follows:



maxv J(v)
with
vT v −1 = 0
The Lagrange multipliers lead us to the following equivalent problem:



maxv
(
vT RNv −λ(vT v −1)
)
vT v −1 = 0
which leads, by setting to zero the gradient with respect to v:



RNv −λv = 0
vT v −1 = 0
The ﬁrst equation means that v is an eigenvector of the matrix RN. If the
eigenvalue associated with v is denoted by λ, then J(v) = λ ≥0. Hence the
maximum is reached when v is chosen as the eigenvector of RN associated with
the highest eigenvalue: hence the name of this method, Principal Component
Analysis (or PCA).
By calculating the product vT X, we get a line vector of RN. This line vector
can be interpreted as the “mean” factor that best characterizes, by itself, the
N individuals (it results in a good separation). However, this “factor” does

166
Digital Signal and Image Processing using MATLAB®
not really exist, it is merely a linear combination of factors that are actually
observed.
The previous result can easily be generalized: the k principal directions are
the k eigendirections of the highest eigenvalues.
Implementing the previous calculations can pose a problem in the case of
image processing. Let us assume for example that we are dealing with images
comprised of 100 × 100 pixels represented in the form of length 104 vectors.
The matrix RN associated with these vectors is a (104 × 104) matrix! The
following property can therefore be useful when d ≫N.
Property 6.1 Let A be a (d × N) matrix. Let {λ1, . . . , λN} be the N eigen-
values of the (N × N) matrix AHA. Then the d eigenvalues of the (d × d)
matrix AAH are:
{λ1, . . . , λN, 0, . . . , 0
| {z }
d−N
}
Hints: Let λ be an eigenvalue of AHA associated with the eigen-
vector v,which is written AHAv = λv. If we multiply both sides
of this equality on the left by A, we get:
AAHAv = λAv
Letting w = Av, we get AAHw = λw. Therefore λ is the eigen-
value of AAH associated with the eigenvector w = Av.
Example 6.10 (Eigenfaces) Construct a database containing front views of
the faces of diﬀerent people(2).
We can use the ORL database, available to
anyone on AT&T’s web site. This database contains photographs showing the
faces of 40 people. Each one of them was photographed 10 times. These photos
are stored as images in levels of grey with 112 × 92 pixels. In our example, we
constructed a catalog called orlfaces, comprised of the catalogs named s1,
s2, . . . , s40, each one of them containing the 10 photographs we are going to
process.
Write a MATLAB® program:
– that changes each (d1 = 112) × (d2 = 92) photograph into a vector;
– that constructs, using a photograph of each of the N people, a subspace
H the dimension of which is less than or equal to N, and such as to
have the maximum dispersion of the N projections (think of using the
property (6.1));
(2)When using PCA for face recognition, it is important for the photos corresponding to
the same person to be taken in approximately the same position and the same lighting. Oth-
erwise, an alignment and a calibration are often unavoidable to achieve satisfactory results.

Selected Topics
167
– that checks the identity corresponding to a photograph by determining its
projection onto H then by comparing the distances of this projection with
respect to the N projections obtained with the N previous photographs;
– which constructs the confusion, for which the element (i, j) represents the
number of times the person i was chosen as being the person j.
Hints: Type:
%===== exeigenfaces.m
clear all
d1=112; d2=92; d=d1*d2; figure(1); colormap('gray')
imagesNb=10; peopleNb=10; images=cell(peopleNb,imagesNb);
matX=zeros(d,peopleNb,imagesNb);
eigenfaces1=zeros(d,peopleNb);
for ni=1:peopleNb
for kimg=1:imagesNb
filename=sprintf('orlfaces/s%i/%i.png',ni,kimg);
images{ni,kimg}=imread(filename);
aux=images{ni,kimg};
matX(:,ni,kimg)=reshape(aux,d,1);
end
end
%===== Training of the eigenfaces using the first image
%
of each person
moymatX=matX(:,:,1)*ones(peopleNb,1)/peopleNb;
nbeig=6; eigenfaces=zeros(d,nbeig); matXc=zeros(d,peopleNb);
for ni=1:peopleNb, matXc(:,ni)=matX(:,ni,1)-moymatX; end
RR=matXc'*matXc; [UU, DD, VV]=svd(RR); DD=diag(DD);
UUpbT=matXc(:,:,1)*VV(:,1:nbeig);
UUpbT=UUpbT*diag(1./sqrt(DD(1:nbeig)));
for ni=1:peopleNb,
cni(ni,:)=matXc(:,ni,1)'*UUpbT;
vi=reshape(UUpbT*cni(ni,:)',d1,d2);
subplot(211);imagesc(reshape(matX(:,ni),d1,d2));
axis('image'); subplot(212);
imagesc(vi); axis('image'); pause
end
%===== Testing the pictures of the person
matriceconf=zeros(peopleNb,peopleNb);
for testedNb=1:peopleNb
for ii=2:imagesNb
aux1=(matX(:,testedNb,ii)-moymatX)'*UUpbT;
for ni=1:peopleNb
aux2(ni)=norm(aux1-cni(ni,:));
end
[aa zz]=min(aux2);
matriceconf(zz,testedNb)=matriceconf(zz,testedNb)+1;

168
Digital Signal and Image Processing using MATLAB®
end
end
matriceconf
6.4.2
2-Dimension PCA
Consider N images of the same size d1 ×d2. Let Aℓbe the matrix representing
the image ℓand
Ac,ℓ= Aℓ−N −1
N
∑
j=1
Ai
be the centered image.
We need to determine k1 length d1 unit vectors v
such that the vectors yℓ= vT Ac,ℓare maximally dispersed. This is done by
maximizing the quantity:
N
∑
ℓ=1
vT Ac,ℓAT
c,ℓv
A calculation similar to the one done in the previous paragraph, for length
1 vectors, proves that the vectors we are trying to determine are the k1 eigen-
vectors of the (d1 × d1) square matrix R1:
R1 = N −1
N
∑
ℓ=1
Ac,ℓAT
c,ℓ
Let V be the (d1 × k1) matrix obtained by compiling these k1 vectors
according to the expression:
V =
[v1
. . .
vk
]
(6.33)
Likewise, if we wish to determine the k2 length d2 unit vectors such that
the vectors yℓ= Ac,ℓw are maximally dispersed, we end up ﬁnding that these
vectors are the k2 eigenvectors of the (d2 × d2) square matrix R2:
R2 = N −1
N
∑
ℓ=1
AT
c,ℓAc,ℓ
Let W be the (d2 × k2) matrix obtained by compiling these k2 vectors
according to the expression:
W =
[w1
. . .
wk
]
(6.34)
In practice, k1 and k2 are chosen such that k1 < d1 and k2 < d2, leading to a
sequence of N “reduced” images of the size k1×k2, according to the expression:
Bℓ= V T AℓW
(6.35)

Selected Topics
169
Example 6.11 (2D-PCA) Write a program that extracts from a set of N
(d1 × d2) images the matrices V and W that allow us, according to expression
(6.35), to obtain N (k1 × k2) images. Have the program display the N images
in the form of an array of cells, each cell representing a (d1 × d2) image, and
return the matrices V and W .
Hints: type the following function:
function [V,W] = PCA2D(matXcell,k1,k2)
%!=========================================!
%! SYNOPSIS: [V,W] = PCA2D(matXcell,k1,k2) !
%!
matXcell : dimension N cell array
!
%!
a cell is a dimension (d1 x d2) array !
%!
k1 = reduced nomber of rows
!
%!
k2 = reduced nomber of columns
!
%! Output:
!
%!
V = dimension (k1 x d1) array
!
%!
W = dimension (k2 x d2) array
!
%!=========================================!
N=length(matXcell); gXcellc=cell(1,N);
[d1,d2]=size(matXcell{1});
moy_image=zeros(d1,d2); GG=zeros(d1,d2);
for ii=1:N,
moy_image=moy_image+double(matXcell{ii});
end
moy_image=moy_image/N;
%===== centered images
for ii=1:N,
gXcellc{ii}=double(matXcell{ii})-moy_image;
end
gR=zeros(d1,d1);
for ii=1:N, gR=gR+gXcellc{ii}*gXcellc{ii}'; end
%===== a covariance
gR=gR/N; [gU, lambda, gV]=svd(gR);
ap=lambda(1:k1,1:k1); V=gU(:,1:k1);
%===== the other covariance
gR=zeros(d2,d2);
for ii=1:N, gR=gR+gXcellc{ii}'*gXcellc{ii}; end
gR=gR/N; [gU, lambda, gV]=svd(gR);
W=gU(:,1:k2);
This function is used in exercise 6.6.
When trying to recognize someone among g individuals, the PCA approach
makes you determine separately, during the training phase, the principal direc-
tions for each group of individuals. The approach we are now going to see makes
it possible to simultaneously optimize the choice of the principal directions and
the separation into groups.

170
Digital Signal and Image Processing using MATLAB®
6.4.3
Linear Discriminant Analysis
We now consider g groups of individuals, each group comprised of Nℓindivid-
uals for which d factors were measured. The data can then be represented in
the form of g matrices of the type:
Xℓ=
[xℓ,1
. . .
xℓ,Nℓ
]
=


x1,1
x1,2
. . .
x1,Nℓ
...
...
...
xd,1
x1,2
. . .
xd,Nℓ


with ℓfrom 1 to g.
In the space Rd we obtain g scatters containing respectively N1, . . ., Ng
points. We let N = ∑g
ℓ=1 Nℓ.
The goal of the Linear Discriminant Analysis (or LDA) is to ﬁnd the best
separation for these g scatters of points. To achieve this, we must ﬁrst introduce
the following deﬁnitions:
– the mean, or barycenter, of a group:
mℓ= 1
Nℓ
Nℓ
∑
j=1
xℓ,j
– the overall mean of the g groups:
m = 1
N
g
∑
ℓ=1
Nℓ
∑
j=1
xℓ,j =
∑g
ℓ=1 Nℓmℓ
∑g
ℓ=1 Nℓ
(6.36)
– the intraclass covariance (internal to the considered class) deﬁned by:
RI =
∑g
ℓ=1 NℓRℓ
∑g
ℓ=1 Nℓ
with Rℓ= 1
Nℓ
Nℓ
∑
j=1
(xℓ,j −mℓ)(xℓ,j −mℓ)T
which leads us to:
RI = 1
N
g
∑
ℓ=1
Nℓ
∑
j=1
xℓ,jxT
ℓ,j −1
N
g
∑
ℓ=1
NℓmℓmT
ℓ
which can be interpreted as the mean of the dispersions inside of each
group;

Selected Topics
171
– the extraclass covariance deﬁned by:
RE = 1
N
g
∑
ℓ=1
Nℓ(mℓ−m)(mℓ−m)T
which can be interpreted as the dispersion of the barycenters of each class
with respect to the overall mean;
– the total covariance deﬁned by:
R = 1
N
g
∑
ℓ=1
Nℓ
∑
j=1
(xℓ,j −m)(xℓ,j −m)T = 1
N
g
∑
ℓ=1
Nℓ
∑
j=1
xℓ,jxT
ℓ,j −mmT
These covariance matrices are (d×d) matrices. It can easily be proved that:
R = RI + RE
Property 6.2 RE is of rank r = min(d, g −1).
Hints: Indeed
NRE =
g
∑
ℓ=1
Nℓ(mℓ−m)(mℓ−m)T
where m is given by expression (6.36). We let
M =
[√N1(m1 −m)
. . .
√
Ng(mg −m)
]
The d × g matrix M veriﬁes MM T = ∑g
ℓ=1 Nℓ(mℓ−m)(mℓ−
m)T = NRE. On the other hand
M


√N1
...
√
Ng

=
g
∑
j=1
Njmj −mN = 0
Therefore the rank of M is at most equal to min(d, g −1). That
means that the space spanned by the columns of M is at most of
dimension min(d, g −1). That is also true for NRE = MM T .

172
Digital Signal and Image Processing using MATLAB®
From property 6.2, it follows that, if g −1 < d, RE is not invertible.
We are now going to ﬁnd the direction, parallel to the vector v, such that
the intraclass dispersion is minimal and the interclass dispersion is maximal:
graphically speaking, the scatters are farther away from each other, and more
compact. To achieve this objective, one possible criterion is to minimize the
evaluation function deﬁned by:
J(v) = vT RIv
vT REv
This amounts to searching v such that:
min
v vT RIv with 1 −vT REv = 0
(6.37)
Using the Lagrange multiplier method (section (6.3)), we end up with the
following equivalent problem:



minv
(
vT RIv + λ(1 −vT REv)
)
with
1 −vT REv = 0
By setting the gradient with respect to v to zero, we ﬁnd that v is given by:
RIv = λREv
(6.38)
Notice that if v is a solution of equation (6.38), then J(v) = λ with λ ≥0.
Now we have to solve the equation (6.38) with respect to v and choose v
associated to the minimal value of λ. The resolution of (6.38) is a well known
problem which is called the generalized eigenvalue problem. In our case RE and
RI are positive, therefore it exists a basis of generalized eigenvectors which are
solution of (6.38).
Remark: if RI is invertible, then the equation (6.38) can be written in the
form:
R−1
I REv = 1
λv
(6.39)
which is a standard eigenvalue problem. However, in most situations it is prefer-
able to avoid the inversion operation. Fortunately, the generalized eigenvalue
problem is solved in MATLAB® using the function eig.m.
To increase the capability of classiﬁcation, we can choose more than one
eigenvector verifying equation (6.38). Let v1, . . ., vk be the k eigenvectors
associated with the k lowest generalized eigenvalues of equation (6.38). By
compiling these k vectors, we get the (d × k) matrix:
V =
[v1
. . .
vk
]
(6.40)

Selected Topics
173
It is worth noting that, because R−1
I RE is not in general positive, the
eigenvectors v1, . . ., vk are not orthogonal. This means that for each of the g
families, the vectors:
yℓ,j = (V T V )−1V T xℓ,j
(6.41)
give the representative points in Rk of each image. Theoretically, each of the
g scatter has a minimal dispersion, and all of the scatters are as far away from
each other as possible.
Example 6.12 (LDA) Write a function that performs the linear discriminant
analysis of g groups, each group containing Nℓvectors of the same length d.
We will be using g cells as the data format with MATLAB®, where each cell
is a (d × Nℓ) matrix. The program returns the matrix V deﬁned by equation
(6.40) which is used for changing the g scatters in Rd into g “well” separated
scatters in Rk.
Hints: Type the following function:
function VlowestEIG = LDA(gX,kk)
%!======================================================!
%! SYNOPSIS: VlowestEIG=LDA(gX,kk)
!
%!
gX = array of cells. Each cell is a (d x Nell)
!
%!
matrix associated to the Nell vectors of a class !
%!
kk = reduced dimension (kk<d)
!
%!
VlowestEIG
= (kk x d) matrix used to
!
%!
reduce X with Xreduced=V'X
!
%!======================================================!
d
= size(gX{1},1);
G
= length(gX);
moyell
= cell(1,G);
Nell
= cell(1,G);
gXc
= cell(1,G);
moyT
= zeros(d,1);
NT
= 0;
RI
= zeros(d,d);
for ell=1:G
gXell
= gX{ell};
Nell{ell}
= size(gXell,2);
NT
= NT+Nell{ell};
moyell{ell} = mean(gXell,2);
moyT
= moyT+Nell{ell}*moyell{ell};
gXellc
= gXell-double(moyell{ell})* ...
ones(1,Nell{ell});
gXc{ell}
= gXellc;
RI
= RI+gXellc*gXellc';
end
RI = RI/NT; moyT = moyT/NT; RE = zeros(d,d);

174
Digital Signal and Image Processing using MATLAB®
for ell=1:G
vaux
= moyell{ell}-moyT;
RE
= RE+Nell{ell}*(vaux*vaux');
end
RE
= RE/NT;
%=====
% [V,D]
= eig(RI,RE)
% solves RI*V = lambda*RE*V
%=====
[gV, lambda]
= eig(RI,RE);
[oo, indsort] = sort(diag(lambda),'ascend');
gV = gV(:,indsort); VlowestEIG = gV(:,1:kk);
Exercise 6.6 (Face recognition) (see p.
238) Consider a database com-
prised of g groups of photographs in levels of grey of the same person’s face,
taken from the front. We can once again use the ORL database, available on
AT&T’s website.
In this exercise, the database of ten photos is divided in
two: three photographs are used for training, and the seven others are used for
recognitions tests.
Training: Write a program:
– that determines, using the function in example 6.11, the matrices
V and W associated with each person (try for instance k1 = 7 and
k2 = 7);
– that determines the k = k1k2 vector characterizing each person and
each photograph;
– that learns from the g previous groups of vectors the matrices of the
linear discriminant analysis. Use the function from example (6.12).
By choosing the dimension d = 2, you can display representative
barycenters in the plane, but in order to obtain good recognition
results, you will need a higher value for d, for example d = 20, re-
quiring that the maximal value of d is k. When d becomes too large,
for example d = 40 with k1 = k2 = 7, we observe that performance
worsens. The reason is that, when you increase the number of pa-
rameters, the learning step memorizes too much unnecessary detail
on the training examples. This phenomena is called overﬁtting;
– that ﬁnds the barycenters characterizing each individual.
Recognition: Write a program that successively determines from expressions
(6.35) and (6.41) the position of the test images in the space Rd (d = 2).
Note that, usually, the person’s identity is not known. Therefore the ob-
tained position has to be compared with the scatters characterizing each
person. As a test function, you can use, for example, the distance be-
tween obtained position and the barycenter of the scatter, and construct
the confusion matrix.

Selected Topics
175
The programs developed in exercise 6.6 can also be used for character recog-
nition. The program geneBDDgene.m allows you to generate a database of the
10 digits in printing characters, each digit being recorded in 12 diﬀerent copies.
After having created the folders s0, . . ., s9 in the folder mdigits, type:
%===== digitbddgene.m
clear all; close all
name={'times','courrier','verdana'}; sizech=[120 100 60];
angle={'normal','italic'}; bolfCh={'normal','bold'};
for ii=0:9
for jj=1:3
figure(1); tt=sprintf('%i',ii);
set(gcf,'color',[1 1 1],'position',[60 500 180 180])
plot(0,0); set(gca,'unit','pixel')
set(gca,'xtick',[],'xticklabel',[])
set(gca,'ytick',[],'yticklabel',[])
set(gca,'box','off','xcolor',[1 1 1],'ycolor',[1 1 1])
ff=text(-1,0,tt);
set(ff,'fontsize',sizech(jj),'fontname',name{jj})
for kk=1:2
set(ff,'fontangle',angle{kk})
for mm=1:2
set(ff,'fontweight',bolfCh{mm});
num=2*2*(jj-1)+2*(kk-1)+mm;
%
cde=sprintf(...
%
'print -dtiffnocompression mdigits/s%i/%i.tif',...
%
ii,num);
cde=sprintf(...
'print -dpng mdigits/s%i/%i.png',ii,num);
% Windows only:
%
cde=sprintf(...
%
'print -dbitmap mdigits/s%i/%i.bmp',ii,num);
eval(cde)
end
end
end
end
Use the program LDAPCAtest.m while adapting the values of the param-
eters.
Try it in particular with nbimages_A=4;, dim_barycenter=30; and
k1=6; k2=6; (set in LDAPCAtraining.m). The -dbitmap parameter is only
supported by the MS-Windows operating system.
The called functions will
have to be modiﬁed accordingly. It is usually preferable to use the options
dtiffnocompression or dpng.
6.5
GPS positioning
GPS is the US-designed satellite positioning system (Global Navigation Satel-
lite System, GNSS). It comprises a network of 30 satellites in quasi-circular

176
Digital Signal and Image Processing using MATLAB®
orbit at an altitude of around 20,000 km, which takes a little less than half an
Earth day to complete. In practice, it is hoped that at least 4 of these satellites
will be reachable at any given time from any point on the Earth’s surface.
In an arbitrary frame of reference, x is the receiver’s true position, ξi the
position of the satellite i and (ai, ei) the azimuth/elevation coordinates which
characterize the satellite’s position i in relation to the receiver.
Assume that the receiver can see I satellites and that the position of those
satellites is perfectly known at all times (Figure 6.12).
U
N
a
e
E
GPS satellite
Figure 6.12 – Satellite location
The term “pseudo-distance” is used for the time taken, in seconds, for the
signal from the satellite to reach the receiver. By multiplying this ﬁgure by the
celerity of light c, it is possible to transform these time values into distances.
Thus, the pseudo-distance between the receiver and the satellite i is written as:
pi = c−1d(x, ξi) + τ + ei
where d(x, ξi) =
√
(x1 −ξ1,i)2 + (x2 −ξ2,i)2 + (x3 −ξ3,i)2 is the distance be-
tween the receiver and the satellite i; τ an unknown time diﬀerence between
the receiver’s clock and those of the satellites, which are supposed to be syn-
chronized; and ei an error due to noise which is supposed to be centered.
The pseudo-distances are measured by observing the time taken for signals
to travel from the satellites to the receiver. Thus, if two signals are received at
exactly the same moment, this means that the receiver is at an equal distance
from the two satellites.
The estimations of the position x and the delay τ on the basis of the mea-
surements pi are obtained by a least-squares method, i.e. as a solution to the
problem:
[ˆx1
ˆx2
ˆx3
ˆτ]T = arg min
x,t
I
∑
i=1
(pi −c−1d(x, ξi) −t)2
(6.42)

Selected Topics
177
By adopting matrix notation, we can also write:
[ˆx1
ˆx2
ˆx3
ˆτ]T = arg
min
x1,x2,x3,t ∥p −f(x1, x2, x3, t)∥2
where
p
=
[p1
. . .
pI
]T
is
the
observations
vector
and
f
:
[x1
x2
x3
t]T 7→
[f1
. . .
fI
]T is a function of R4 in RI, whose i-th
component has the expression:
fi(x1, x2, x3, t)
=
c−1√
(x1 −ξ1,i)2 + (x2 −ξ2,i)2 + (x3 −ξ3,i)2 + t
=
c−1di + t
where di = fi(x1, x2, x3, 0) represents the distance between the satellite i and
the receiver. The solution to the problem (6.42) satisﬁes:
−∇f T (ˆx1, ˆx2, ˆx3, ˆτ) (p −f(ˆx1, ˆx2, ˆx3, ˆτ)) = 0
(6.43)
where:
∇f(x1, x2, x3, t)
=


∂f1
∂x1
∂f1
∂x2
∂f1
∂x3
1
...
...
...
...
∂fI
∂x1
∂fI
∂x2
∂fI
∂x3
1


=
c−1


x1−ξ1,1
d1
x2−ξ1,2
d1
x3−ξ1,3
d1
c
...
...
...
...
x1−ξI,1
dI
x2−ξI,2
dI
x3−ξI,3
dI
c


Note that ∇f(x1, x2, x3, t) depends neither on t nor on the distances be-
tween the receiver and the satellites, but only from the angles from which
the receiver sees the satellites. Unfortunately, the system of equations (6.43)
to solve is highly nonlinear.
In practice, we solve equation (6.43) with a
Newton–Raphson-type iterative algorithm (see expression (4.26)).
Remem-
ber that the Newton–Raphson method is used to seek the zeros in equation
g(θ) = 0 where g : θ ∈Rp 7→Rq. It is written as:
θ(p+1)
=
θ(p) −(∇θg(θ(p)))−1g(θ(p))
where:
∇θg(θ(p)) =


∂g1
∂θ1
∂g1
∂θ2
· · ·
∂g1
∂θp
...
...
...
...
∂gq
∂θ1
∂gq
∂θ2
· · ·
∂gq
∂θp


It is associated with problems of minimization when the function to be
canceled is the gradient of a function J(θ), θ ∈Rp 7→R.

178
Digital Signal and Image Processing using MATLAB®
Exercise 6.7 (GPS location) (see p. 240)
Write a program which uses the Newton–Raphson method to determine the
receiver’s position based on the data provided by the following program:
%===== data4exeGPS.m
%===== light velocity in m/s
c_mps = 299792458;
%===== true location in meter
% for comparison with the estimated location
true_location_m = [248645.5722;-4828261.0758;4146460.5047];
%===== location in meter of the 6 satelites
sat_location_m =
1.0e+07 * ...
[-1.731978829800000
-1.875361540500000
0.728987051300000;...
-1.362041209000000
-0.813947623800000
2.149113287300000;...
0.809023521200000
-2.399402432700000
-0.752593941400000;...
1.929942650600000
-0.960140862800000
1.585822443500000;...
2.345122552599999
-0.672073953600000
1.090961123000000;...
1.165884015800000
-1.341861781800000
1.940808680800000];
% pseudo-ranges in second
pseudorange_s = [ ...
0.075522044080650;...
0.074908679917403;...
0.079304017358102;...
0.076286737020851;...
0.080875081503129;...
0.069734492372016];
6.6
The Viterbi algorithm
Digital Signal Processor (DSP) architectures were initially designed for ﬁltering.
Afterwards, abilities for calculations involved in digital signal processing were
added: power, spectrum, FFT, etc. Subsequently, with the development of
mobile phones, the processing of frames received by such devices was also added.
Below, we use an example to explain the processing of “hard decision” in the
Viterbi algorithm and its implementation in common digital signal processors.
MATLAB® programs are given to illustrate the coding and decoding processes.
A simpliﬁed communication channel is illustrated by Figure 6.13. Coding
of the binary symbols bk with values in the range {0, 1} provides binary vectors
denoted by sk with values in the range {0, 1} (see Figure 6.14). The baseband
vectors thus emitted are denoted by ˜sk, with each component being 1 or −1
given by the equation ˜sk = 2sk −1.

Selected Topics
179
+
+
Figure 6.13 – An example of a communication channel
The noise is denoted by {nk} and the vectors ˜rk are coded over 1 bit (hard
decision) or n > 1 bits (soft decision).
6.6.1
Convolutional non-recursive encoder
Among the numerous methods which exist for the coding of data frames, con-
volutional codes are widely used [11]. They exhibit performances which dete-
riorate progressively with the error rate, and decoding can be done over data
blocks of arbitrary length. Most recent DSPs provide processing units which
simplify the implementation of the Viterbi algorithm.
Let us consider as an example the convolutional non-recursive encoder [20]
shown in Figure 6.14. This encoder is described as a “1/3 rate convolutional
non-recursive encoder”. For each binary symbol bn, three bits – s0(n), s1(n)
and s2(n) – are generated, and the output stream is {. . . s2(n −1), s1(n −1),
s0(n −1), s2(n), s1(n), s0(n), s2(n + 1), s1(n + 1), s0(n + 1) . . .}.
Figure 6.14 – Example of a convolutional non-recursive encoder
The symbols ⊕denote modulo-2 sums. They will be replaced by “+” when
there is no ambiguity.
The parameters used to describe such an encoder are:
1. The Constraint Length K, which is also the sequential automaton state
space dimension +1, here K = 3.
2. The Code Rate R, which is the ratio between the number n of information
bits to be coded and the code length k, here R = n/k = 1/3.

180
Digital Signal and Image Processing using MATLAB®
3. The generator polynomials which deﬁne the code – here G0 = 100, G1 =
101 and G2 = 111 where Gk = {a0, a1, a2} corresponds to the sum a0bn⊕
a1bn−1 ⊕a2bn−2.
Example:
in some of the DSPs of the TMS320C6000TM family, the
TMS320C64x DSP Viterbi-Decoder Coprocessor unit (VCP) [20] available pa-
rameters are K = 5 to 9 and R = 1/2, 1/3 or 1/4.
Example: in the GSM (Global System for Mobile communications), voice
coding uses the parameters: R = 1/2, K = 5, a frame length of N = 189
bits and 50 frames per second. Parameters for data transmission are slightly
diﬀerent and puncturing is used, facilitating a code rate equal to R = 57/61
(puncturing deletes bits of the emitted frame and decoding necessitates a soft
decision algorithm).
The state transition graph corresponding to Figure 6.14 can be represented
by Figure 6.15.
1
0
Figure 6.15 – The state transition graph (transition: b(n)/(s2s1s0)(n)) of the en-
coder shown in Figure 6.14
The encoding can also be represented by Figure 6.16 corresponding to a
step in the encoding process.
00
10
11
01
00
10
11
01
bn-1bn-2
bnbn-1
bn/s2s1s0
0/000
1/101
0/010
0/110
1/001
1/111
1/011
0/100
Figure 6.16 – A step in the encoding process

Selected Topics
181
The coding of states by (“bnbn−1”) is choosen in order to make the pro-
gramming easier. With such a code, the decoding is straightforward when the
sequence of states is known.
Hereafter, we shall denote the emitted vector
corresponding to the binary symbol bn as s(n) =
[s2(n)
s1(n)
s0(n)]
.
Coding
Let Sn be the state at time n. xn and yn deﬁne the two-bit code for Sn:
Sn =
[xn
yn
]T
(6.44)
One has a state representation (the modulo-2 sum can be replaced by a
sum):
Sn+1 =
[0
1
0
0
]
Sn ⊕
[0
1
]
bn =
[0
1
0
0
]
Sn +
[0
1
]
bn
(6.45)
In the program coderexple.m the coded sequence can be generated more
eﬃciently (avoiding a matrix-vector multiplication) as follows:
%===== coderexple =====
clear all, N=1000; n=(0:N-1);
prcodes=[[0;0;0],[1;1;0],[1;0;0],[0;1;0],...
[1;1;1],[0;0;1],[0;1;1],[1;0;1]];
%===== Mersenne twister
s=RandStream.create('mt19937ar','seed',134);
RandStream.setDefaultStream(s);
bn=randi([0 1],1,N);
Rn=zeros(3,N); % coded sequence
Sn=zeros(1,N); % state
for k=1:N-1
kn=4*bn(k)+Sn(k);
Rn(:,k)=prcodes(:,kn+1);
Sn(k+1)=floor(kn/2); % transition
end
plot(n,Sn,'o',n,Sn,'-'); grid
Rnt=reshape(Rn,1,3*N);
save codedseq bn Rnt -V6
6.6.2
Decoding and hard decision
We suppose in our simulations below that the received sequence is coded with
binary values 0 and 1.
Decoding with a majority function
Upon receipt of the coded sequence, that sequence includes 3-bit words from
which one has to reconstruct b(n). Let b(n|k) the reconstructed bit at time n

182
Digital Signal and Image Processing using MATLAB®
knowing r0(k), r1(k) and r2(k). Decoding provides:



bn|n = r0(n)
bn−1|n = r1(n) + r2(n)
bn−2|n = r0(n) + r1(n)
With the blocks number n + 1 and n + 2, there are two other solutions for
reconstructing of bn:



bn+1|n+1 = r0(n + 1)
bn|n+1 = r1(n + 1) + r2(n + 1)
bn−1|n+1 = r0(n + 1) + r1(n + 1)
and



bn+2|n+2 = r0(n + 2)
bn+1|n+2 = r1(n + 2) + r2(n + 2)
bn|n+2 = r0(n + 2) + r1(n + 2)
z-1
z-1
z-1
z-1
z-1
z-1
r2(n+1)
r1(n+1)
r0(n+1)
r2(n)
r1(n)
b(n|n)
b(n|n+1)
r0(n)
z-1
z-1
z-1
r2(n+2)
r1(n+2)
r0(n+2)
b(n|n+2)
Figure 6.17 – Decoding the sequence. We use the notation bn|k for the bit recon-
structed using r0(k), r1(k) . . .
We use a majority function to get the estimated value ˜bn:
˜bn = bn|nbn|n+1 + bn|n+1bn|n+2 + bn|n+2bn|n
(6.46)
The program decoderexple.m gives an example of decoding of the sequence
provided by coderexple.m.
%===== decoderexple.m =====
clear all, load codedseq
P=3; Nc=length(Rnt); N=Nc/P;
nberr=10; % error number
idxerr=randi([1,Nc],1,nberr);
serr=zeros(1,Nc); serr(idxerr)=1;
Rnte=mod(Rnt+serr,2); % received seq.
%=====
for k=1:N-P+1
idxd=P*(k-1)+1; idxf=idxd+P*P-1;
bl=reshape(Rnte(idxd:idxf),P,P)';
%===== b(n|n)
b0=bl(1,3);
%===== b(n|n+1)
b1=mod(bl(2,1)+bl(2,2),2);

Selected Topics
183
%===== b(n|n+2)
b2=mod(bl(3,2)+bl(3,3),2);
%===== majority function
b(k)=mod(b0*b1+b0*b2+b1*b2,2);
end
Viterbi Algorithm
The principle of the Viterbi algorithm [41] is based on the minimization of a
distance between the estimated and received sequences. Consider the trellis
(Figure 6.18) corresponding to the encoder shown in Figure 6.14.
00
10
11
01
bn+1bn
bnbn-1 r2r1r0
000
010
110
001
111
011
100
101
00
10
11
01
00
10
11
01
bn-1bn-2 r2r1r0
000
010
110
001
111
011
100
101
00
10
11
01
r2r1r0
000
010
110
001
111
011
100
101
00
10
r2r1r0
000
111
00
r2r1r0
000
111
011
100
a1a2
b0a1
b1b0
Figure 6.18 – The trellis of the encoder; the initial state is chosen to be 00
We execute the following steps:
1. Encoding of the sequence: starting from the state 00 with a sequence
0 1 1 0 1, we obtain (Figure 6.19) the emitted sequence 000 111 011 010
001.
b2b1
b4b3
b3b2
00
10
11
01
000
010
110
001
111
011
100
101
00
10
11
01
00
10
11
01
000
010
110
001
111
011
100
101
00
10
11
01
000
010
110
001
111
011
100
101
00
10
000
111
00
r2r1r0
000
111
011
100
a1a2
b0a1
b1b0
Figure 6.19 – Encoding the sequence 0 1 1 0
2. Addition of noise to the sequence: the emitted sequence suﬀers from
errors. Let us suppose the following sequence is received: 000 110 111

184
Digital Signal and Image Processing using MATLAB®
010 000 (corresponding to the error vector 000 001 100 000 001). From the
initial state 00, we mark each state with its minimal Hamming distance
from the initial state and label it with the previous state number on the
shortest path to it (Figure 6.20).
(3,A)
(0,A)
(2,A)
(1,A)
(5,C)
(4,C)
(5,A)
(2,A)
(2,C)
(3,C)
(4,B)
(5,B)
(3,C)
(2,D)
(4,A)
(3,B)
(5,D)
(4,D)
A
B
C
D
000
010
111
110
000
b2b1
b4b3
b3b2
00
10
11
01
000
010
110
001
111
011
100
101
00
10
11
01
00
10
11
01
000
010
110
001
111
011
100
101
00
10
11
01
000
010
110
001
111
011
100
101
00
10
000
111
00
r2r1r0
000
111
011
100
a1a2
b0a1
b1b0
received triplets
Figure 6.20 – Decoding the sequence; A, B, C, D refer to the states (0, 0), (0, 1),
(1, 0) and (1, 1) at any step of the algorithm
3. Marking the path: at each step an update of the distance (metric update)
is carried out for each state, a comparison and a marking of the previous
state, given the minimal distance. This is called an ACS (Add, Compare,
Select) operation. Hence, the state 00 (b4b3) from Figure 6.20 is marked
with a distance equal to 4 and a previous state A.
4. Reading backwards the path of best length: the path (B, D, C, A, A)
which leads to the state with the shortest length gives us the emitted
sequence 0 1 1 0 1. The minimum value 3 associated with the state C
gives the number of errors in the sequence.
Simulation
Let b be the edge characterized by the triplet r(b)
0 , r(b)
1 , r(b)
2 . Let R0, R1, R2 be
the received triplet. The Hamming distance is given by:
D =
2
∑
k=0
r(b)
k
⊕Rk
(6.47)
In the program decoderv.m, the calculation of the distances on eight edges
is carried out by delta=sum(xor(prcodes,bl)); where prcodes gives the
eight triplets (the 3-bit codes provided by the encoder) labeling the eight edges.

Selected Topics
185
%===== decoderv.m =====
clear all
%==== Test sequence
Rnte=[0 0 0 1 1 0 1 1 1 0 1 0 0 0 0];
P=3; N=length(Rnte)/P;
prcodes=[[0;0;0],[1;1;0],[1;0;0],[0;1;0],...
[1;1;1],[0;0;1],[0;1;1],[1;0;1]];
pstate=[1 3 1 3;2 4 2 4]; ostate=zeros(4,N-P+2);
wght=zeros(4,1); st=[];
%===== level 1
idxd=1; idxf=3;
bl=Rnte(idxd:idxf)'*ones(1,2);
delta=sum(xor([[0;0;0] [1;1;1]],bl));
wght=wght+[delta(1);0;delta(2);0];
ostate(:,1)=[1;0;1;0]; st=[st wght];
%===== level 2
idxd=4; idxf=6;
bl=Rnte(idxd:idxf)'*ones(1,4);
delta=sum(xor([[0;0;0] [1;0;0]...
[1;1;1] [0;1;1]],bl));
wght=[wght(1)+delta(1); wght(3)+delta(2);...
wght(1)+delta(3); wght(3)+delta(4)];
st=[st wght]; ostate(:,2)=[1;3;1;3];
%===== level 3...
for k=3:N
idxd=P*(k-1)+1; idxf=idxd+2;
bl=Rnte(idxd:idxf)' * ones(1,8);
delta=sum(xor(prcodes,bl));
[td,ti]=min(reshape(delta+[wght' wght'],2,4));
wght=td'; st=[st wght];
ostate(:,k)=pstate(ti+[0:2:6])';
end
%===== backtracking
mpath=zeros(1,N+1); rseq=mpath; mbits=zeros(1,N);
[mmin,idx]=min(st(:,end));
idxm=idx(1); mpath(N+1)=idxm;
for k=N:-1:1
mpath(k)=ostate(idxm,k);
mbits(k)=floor((idxm-1)/2);
idxm=ostate(idxm,k);
end
st, ostate, mpath, mbits,
Running the program decoderv.m yields:
st =
0
2
5
4
4
0
4
3
2
4
3
1
2
5
3
0
5
2
3
5

186
Digital Signal and Image Processing using MATLAB®
ostate =
1
1
1
2
1
0
3
3
4
4
1
1
1
2
2
0
3
3
3
4
mpath =
1
1
3
4
2
3
mbits =
0
1
1
0
1
where:
– st(i,k), i = 1 to 4, k = 1 to N, gives the length from state (0, 0) to the
current state i at step k on the “best” path to i;
– ostate(i,k), i = 1 to 4, k = 1 to N, gives the state from which it is
reached on the path from (0, 0) to i at step k (0 if no previous state);
– mpath gives the sequence of states on the minimum-length path;
– mbits gives the reconstructed sequence bits.
The states are numbered from 1 to 4. The array ostate gives the original
state during the construction of the trellis. This enables us to backtrack when
reconstructing the emitted sequence.
For very long sequences, decoding is done not on the whole received sequence
but on blocks (e.g. 189 bits for voice data in the GSM norm).

Chapter 7
Hints and Solutions
H1
Reminders on digital signal-processing
H1.1 (DTMF signal processing) (see p. 11)
1. The expression (1.21) for the power can be seen as the result of the
ﬁltering of the positive signal yn = x2
n by the ﬁlter with the ﬁnite impulse
response h(k) = 1/N for k ∈{0, . . ., N −1}, and 0 otherwise. The filter
function is used as follows to achieve this ﬁltering operation:
hpb=ones(N,1)/N; pn=filter(hpb,1,x .* x);
The program that performs the complete signal activity detection opera-
tion is the following:
%===== detectkey.m
% input x=DTMF signal
N=100; hpb=ones(N,1)/N;
% estimation
pn=filter(hpb,1,(x .* x)); % power for N points
pmax=max(pn); mthresh=0.5*pmax;
marker=(pn>mthresh); lmarker=length(marker);
begend=marker(2:lmarker)-marker(1:lmarker-1);
begs=find(begend==+1)-N/2; % start indices
ends=find(begend==-1)-N/2; % stop indices
%===== plotting of the signal
subplot(211),plot(tps,x); set(gca,'xlim',[0 (lx-1)/Fs])
%===== plotting of the transitions
hold on
for k=1:length(begs)

188
Digital Signal and Image Processing using MATLAB®
plot(begs(k)/Fs*[1 1],4*[-1 1],'-r');
end
for k=1:length(ends)
plot(ends(k)/Fs*[1 1],4*[-1 1],'-');
end
hold off; grid
%===== plotting of the instantaneous
%
power and threshold
subplot(212); plot(tps,pn)
set(gca,'xlim',[0 (lx-1)/Fs],'ylim',[0 1.2*pmax])
hold on; plot([0 (lx-1)/Fs],[mthresh mthresh],'-');
hold off; grid
The ﬁlter output is compared to a threshold chosen equal to half of the
maximum instantaneous power. Using the logical expression pn>mthresh,
we then determine the sequence of the parts of the signal where Pn
is above the threshold value. By subtracting this sequence from itself
translated by 1, we get of sequence of values, where +1 indicates the
beginning of a signal and −1 the end of a signal. This operation corre-
sponds to a numerical derivative that could just as well have been written
begend=marker(2:lmarker)-marker(1:lmarker-1);
or
begend=diff(marker). In expression (1.21), the calculated power corre-
sponds to the signal portion going from the indices (n−N +1) to n. It is
therefore better to consider this measure as the median position n−N/2.
This is why we subtracted N/2 to the positions that were found, which
is done by the two following program lines:
begs=find(begend==+1)-N/2;
ends=find(begend==-1)-N/2;
We represented in Figure H1.1 the estimated instantaneous power.
0
0.5
1
1.5
2
2.5  s
0
0.2
0.4
0.6
0.8
1
1.2
Threshold
Figure H1.1 – Estimated instantaneous power
2. To determine the digit based on an active portion of the signal, the two
frequencies have to be extracted. The correspondence table provides the

Hints and Solutions
189
corresponding number. All the frequencies extracted are already known,
so there is no use doing a spectral study on the entire frequency axis. It is
suﬃcient to evaluate the spectrum at frequencies that might be contained
in the signal, by calculating these four quantities:
Q =

∑
n
x(n)e2jπfn

(7.1)
for f ∈{697/Fs, 770/Fs, 852/Fs, 941/Fs}, with Fs = 8,000 Hz, and to
choose the frequency that corresponds to the highest value. The same is
done with the group of three high frequencies.
The program detectnum.m determines the phone number associated with
the signal:
%===== detectnum.m
nbDigitdet=min([length(begs) length(ends)]);
foundNum=[];
for k=1:nbDigitdet
QB=zeros(4,1); QH=zeros(3,1);
sig=x(begs(k):ends(k)); % signal associated with a number
lsig=length(sig);
%===== for each of the 4 freq.: correlation
for n=1:length(FreqB)
% for each freq.
ps=sig .* exp(2*j*pi*FreqB(n)*(1:lsig)'/Fs);
QB(n)=abs(sum(ps));
end
%===== for each of the 3 freq.: correlation
for n=1:length(FreqH)
ps=sig .* exp(2*j*pi*FreqH(n)*(1:lsig)'/Fs);
QH(n)=abs(sum(ps));
end
%===== maxima
[bid, indB]=max(QB); [bid, indH]=max(QH);
detF=[FreqB(indB) FreqH(indH)];
%===== table look-up
jj=1;
while sum(Freqskeys(jj,:)~=detF), jj=jj+1; end
foundNum=[foundNum keys(jj)];
end
disp(sprintf('******* The number is : %s',foundNum))
H2
Additional information on ﬁltering
H2.1 (Filter architecture) (see p. 19)

190
Digital Signal and Image Processing using MATLAB®
1. We can write:













xp(n)
=
bpi(n) −apo(n)
xp−1(n)
=
bp−1i(n) −ap−1o(n) + xp(n −1)
...
...
x1(n)
=
b1i(n) −a1o(n) + x2(n −1)
0
=
b0i(n) −o(n) + x1(n −1)
⇒



x1(n) = b1i(n) −a1o(n) + b2i(n −1) −· · ·
· · · + bpi(n −p + 1) −apo(n −p + 1)
x1(n −1) + b0i(n) = o(n)
In terms of the z-transform, we get, as expected, the transfer function:
H(z) = b0 + b1z−1 + · · · + bpz−p
1 + a1z−1 + · · · + apz−p
2. The state representation associated with this architecture is:

































x1(n)
...
xp(n)


=


−a1
1
0
· · ·
0
−a2
0
...
...
...
...
...
...
...
0
...
...
...
1
−ap
0
· · ·
· · ·
0




x1(n −1)
...
xp(n −1)


+


b1 −b0a1
...
bp −b0ap


i(n)
o(n) =
[
1
0
· · ·
0
]
x(n −1) + b0i(n)
The corresponding ﬁltering program is given below. Of course, this design
is far from being the optimal one in terms of execution.
It would be
preferable to have a “mex” function. You can check that it leads to the
same output sequence as the one obtained with the ﬁltering function
described in the text:
function [xout,zs]=filtrerII(num,den,xinp,zi)
%!=================================================!
%! Filtering (Transpose-form IIR filter structure) !
%! SYNOPSIS: [xout,zs]=FILTRERII(num,den,xinp,zi)
!
%!
num
= [b0 b1 ... bP]
!
%!
den
= [1 a1 a2 ... aP]
!
%!
xinp = input sequence
!

Hints and Solutions
191
%!
zi
= initial state
!
%!
xout = output sequence
!
%!
zs
= final state
!
%!=================================================!
lden=length(den); lnum=length(num);
if lden < lnum, den(lnum)=0; lden=lnum; end
if lnum < lden, num(lden)=0; end
ld=lden-1; N=length(xinp); av=zeros(ld,1); bv=av;
av(:)=den(2:lden); bv(:)=num(2:lden);
if nargin==3, zi=zeros(ld,1); end;
if length(zi)<ld, zi(ld)=0; end
zzi=zeros(ld,1); zzi(:)=zi; zs=zzi;
%===== state representation
b0=num(1); ma=compan([1;av])';
vb=bv - b0 * av; vc=[1 zeros(1,ld-1)]; cd=b0;
%===== filtering
for k=1:N,
zsn =ma * zs + vb * xinp(k);
xout(k)=vc * zs + cd * xinp(k); zs=zsn;
end
3. We can express the intial state reconstruction by:
xk(0) = −
k−1
∑
α=0
bαi(k −α) +
k−1
∑
α=0
aαo(k −α)
This leads us to the state reconstruction program:
function zi=filtricII(num,den,xinp,xout)
%!===========================================!
%! Reconstruction of the initial state for a !
%! Transpose-Form IIR structure
!
%! SYNOPSIS: zi=FILTRICII(num,den,xinp,xout) !
%!
num
= [b0 b1 ... bP]
!
%!
den
= [1 a1 a2 ... aP]
!
%!
xinp = input sequence
!
%!
xout = output sequence
!
%!
zi
= reconstructed initial state !
%!===========================================!
lden = length(den); lnum = length(num);
if lden<lnum, den(lnum)=0; lden=lnum; end
if lnum<lden, num(lden)=0; end
ld=lden-1; numv=zeros(lden,1); denv=numv;
numv(:)=num; denv(:)=den;
%=====
lx = length(xinp); ly = length(xout);
if lx<ld, xinp(ld)=0; end
if ly<ld, xout(ld)=0; end

192
Digital Signal and Image Processing using MATLAB®
ysv=zeros(1,ld); xev=ysv; ysv(:)=xout(1:ld);
xev(:)=xinp(1:ld);
zi=filtrerII(denv,1,ysv)+filtrerII(-numv,1,xev);
H2.2 (Parallel implementation of the FIR ﬁltering) (see p. 21)
Type:
%===== polyphase.m
x0=[1:103]; lx0=length(x0); M=4;
b=0.3; N=25; h=rif(N,b);
%===== M-polyphase filters (with insertion of zeros
%
for the processing)
hp=zeros(M,N);
for k=1:M, hp(k,1:M:N-k+1)=h(k:M:N); end
%===== result of the filtering without polyphase
z1=filter(h,1,x0);
%===== polyphase processing
z2=zeros(M,lx0);
for k=1:M,
xx = [zeros(1,k-1) x0(1:lx0-k+1)];
z2(k,:)=filter(hp(k,:),1,xx);
end
xx = sum(z2); [xx(1:lx0)' z1(1:lx0)']
H2.3 (FFT ﬁltering) (see p. 28)
1. The gain at the frequency 0 is equal to the sum of the impulse response
coeﬃcients.
2. Type:
%===== filtragefft1.m
nfft=256; freq=[0:nfft-1]/nfft;
hn=[0.0002 0.0134 0.0689 0.1676 0.2498 ...
0.2498 0.1676 0.0689 0.0134 0.0002];
nh = length(hn);
N=128-nh; temps=[0:N-1]; f0=.15; f1=.3;
x=sin(2*pi*f0*temps) + sin(2*pi*f1*temps);
%===== processing using the convolution
hn = hn / sum(hn); y=filter(hn,1,x);
subplot(311); plot(temps,[x' y'])
subplot(312); plot(freq, abs(fft(hn,nfft)));
subplot(313); plot(freq, abs(fft([x' y'],nfft)));
3. Then type:
%===== filtragefft2.m
xcompl = [zeros(1,nh) x]; nxc = length(xcompl);

Hints and Solutions
193
hns = fft(hn,nxc); xcs = fft(xcompl,nxc);
yns = xcs .* hns; yn = real(ifft(yns));
figure(3)
plot(y); hold; plot(yn(1+nh:nxc),'or'); hold;
4. Then type:
%===== filtragefft3.m
% Processing with blocks of length P
clear hns; clear yns; P=32; hns = fft(hn,P);
kp=floor(nxc / (P-nh)); % number of blocks
y=[];
for k =0:kp-1,
kdb=(P-nh)*k;
xbloc=xcompl(kdb+1:kdb+P); % overlap
xbs=fft(xbloc,P); yns=hns .* xbs;
yn = real(ifft(yns)); y=[y yn(nh+1:P)];
end
hold; plot(y,'ob'); hold; grid
H2.4 (Band-pass ﬁlter based on a comb ﬁlter) (see p. 32)
A pole was placed in the ﬁrst cell of the low-pass ﬁlter from Figure 2.10 in
order to cancel the zero placed at the frequency 0 in the second cell. Therefore,
all we need to do to design a real pass-band ﬁlter around the frequency m/M
is to precede the ﬁlter with another ﬁlter with the transfer function:
Fm(z) =
1
(1 −wmz−1)(1 −w∗mz−1) =
1
1 −2 cos(2πm/M)z−1 + z−2
This leads to Hz(z) = Fm(z)(1 −z−M) which is still an FIR ﬁlter. Figure
H2.1 shows, for M = 16, the frequency response of the low-pass ﬁlter and of
the band-pass ﬁlter for m = 3.
The following program plots the frequency responses of the two ﬁlters:
%===== mcomb.m
M=16; m=(0:M-1); Lfft=512; fq=(0:Lfft-1)/Lfft-1/2;
fq1=(Lfft/2+1:Lfft);fq2=(1:Lfft/2);
%===== low-pass and band-pass comb filters
ht=ones(1,M); k=4; gt=2 * ht .* cos(2*pi*k*m/M);
hf=abs(fft(ht,Lfft)); hf=10*log10(hf / max(abs(hf)));
gf=abs(fft(gt,Lfft)); gf=10*log10(gf / max(abs(gf)));
subplot(211); plot(fq,[hf(fq1) hf(fq2)]);

194
Digital Signal and Image Processing using MATLAB®
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
12
14
16
m=3
m=0
Figure H2.1 – Frequency response: m = 0 (low-pass) and m = 3 (band-pass)
set(gca,'ylim',[-20 0]); grid
subplot(212); plot(fq,[gf(fq1) gf(fq2)]);
set(gca,'ylim',[-20 0]); grid
H3
Image Processing
H3.1 (Projection onto a screen (text on page 51))
1. Calculating the projection:
function [pos2D,lm]=projconique(Ds,Dr,tb3N)
%!==================================================!
%! Dr = distance object reference origin - observer !
%! Ds = distance from screen to observer
!
%! tb3N
= array(3,N) [[x1;y1;z1],...,[xN;yN;zN]]
!
%! pos2D = coordinates in the plane
!
%!==================================================!
delta=Ds ./ (Dr-tb3N(1,:,:));
pos2D = [tb3N(2,:,:) .* delta ; tb3N(3,:,:) .* delta];
2. The rotations around the axes by an angle φx, φy where φz are repre-
sented by the three matrices:
Rx =


1
0
0
0
cos φx
−sin φx
0
sin φx
cos φx

,
Ry =


cos φy
0
sin φy
0
1
0
−sin φy
0
cos φy



Hints and Solutions
195
2
2
3
3
4
4
5
6
7
8
1
1
0
0
−1
−1
−2
−3
−4
−5
−6
−7
−8
Figure H3.1 – Example of projection onto the plane orthogonal to Ox
and Rz =


cos φz
−sin φz
0
sin φz
cos φz
0
0
0
1


which give us the coordinates of the points transformed into the chosen
system of axes.
Functions of rotation:
function vxp=rotx(vp,phix)
%!=========================================!
%! phix = rotation around x-axis (degrees) !
%! vp=(3,N)-matrix, vxp=(3,N)-matrix
!
%!=========================================!
phix=phix * pi / 180;
Rx=[1
0
0;...
0 cos(phix) -sin(phix);...
0 sin(phix)
cos(phix)];
vxp=Rx*vp;
return
function vyp=roty(vp,phiy)
%!=========================================!
%! phiy = rotation around y-axis (degrees) !
%! vp=(3,N)-matrix, vyp=(3,N)-matrix
!
%!=========================================!
phiy=phiy * pi / 180;
Ry=[cos(phiy)
0 sin(phiy);...
0
1
0;...
-sin(phiy) 0 cos(phiy)];
vyp=Ry*vp;
return

196
Digital Signal and Image Processing using MATLAB®
function vzp=rotz(vp,phiz)
%!=========================================!
%! phiz = rotation around z-axis (degrees) !
%! vp=(3,N)-matrix, vzp=(3,N)-matrix
!
%!=========================================!
phiz=phiz * pi / 180;
Rz=[cos(phiz) -sin(phiz) 0;...
sin(phiz)
cos(phiz) 0;...
0
0
1];
vzp=Rz*vp;
return
3. The diﬃculty stems from the fact that the function surf accepts only the
functions deﬁned by z = f(x, y). The test program giving Figure H3.2
is:
%===== tstproj3D.m
xrate=2.5; sapert=24;
%===== original image
[tx,ty,tz,zz,xx]=newimg(); L=length(tx);
subplot(221), surf(tz,tx,ty);
xlabel('z'), ylabel('x'), zlabel('y')
view(135,30)
%==========
pos2D=[]; tzm=max(tz); Dr=xrate*tzm; Ds=sapert;
tb3=zeros(3,L,L);
tb3(1,:,:)=zz; tb3(2,:,:)=xx; tb3(3,:,:)=ty;
tb3N=reshape(tb3,3,L*L);
%===== conic projection
subplot(222)
[pos2D]=conicproj(Ds,Dr,tb3N);
plot(pos2D(1,:),pos2D(2,:),'.'); grid on
%===== translation + rotation / Ox
vyp=roty([tb3N(1,:);tb3N(2,:)-7;tb3N(3,:)],30);
[pos2D]=conicproj(Ds,Dr,vyp);
subplot(223), plot(pos2D(1,:),pos2D(2,:),'.')
grid on
%===== using rotate function
alpha=20; % degrees
subplot(224), hh=surf(tz,tx,ty);
zdir=[0 0]; % phi=0, theta=0
rotate(hh,zdir,alpha);

Hints and Solutions
197
−10
0
10
−10
0
10
−5
0
5
10
−20
−10
0
10
20
−2
0
2
4
6
8
−30
−20
−10
0
10
−10
−5
0
5
10
−10
0
10
−10
0
10
−5
0
5
10
x
z
(a)
(c)
(b)
(d)
y
Figure H3.2 – Projection after translation and rotation: (a) original image, (b)
projection onto the plane P, (c) rotation around the Ox axis with translation along
that same axis, (d) rotation of the surface by the function rotate in MATLAB®
H4
Numerical calculus
H4.1 (Full-wave rectiﬁer and simulation) (page 71)
1. The full-wave rectiﬁer is periodic with period Td = T0/2 where T0 = 1/F0.
It is expandable in a Fourier series. The Fourier coeﬃcients are:
Xn
=
A
Td
∫Td
0
sin(2πF0t)e−2jπnt/Tddt
=
1
Td
∫+∞
−∞
xT (t)e−2jπnt/Tddt = 1
Td
XT (n/Td)
where xT (t) = ArectTd(t −Td/2) sin(2πF0t). The Fourier transform of
xT (t) is denoted by XT (F). We can write:
xT (t) = ArectTd(t −Td/2) 1
2j (exp(2jπF0t) −exp(−2jπF0t))
If we use the modulation property satisﬁed by the Fourier transform of

198
Digital Signal and Image Processing using MATLAB®
rectTd(t −Td/2), we get:
XT (F) =
Asin(π(F −F0)Td)
2jπ(F −F0)
e−jπ(F −F0)Td
−Asin(π(F + F0)Td)
2jπ(F + F0)
e−jπ(F +F0)Td
and therefore:
Xn = 1
Td
XT (n/Td) = −2A
π
1
4n2 −1
The continuous component is X0 = 2A/π.
2. If we apply the Fourier transform to the diﬀerential equation, we get:
2jπRCFY (F) + Y (F) = X(F)
⇒
H(F) =
1
1 + 2jπRCF
3. If the RC ﬁlter has a constant RC ≫1/F0 such that the components
with the frequency ±2kF0, where k ≥2, are negligible, the continous
component and the ﬁrst harmonic are all that is left in the output.
By remembering that the signal exp(2jπFt), after ﬁltering, leads to
H(F) exp(2jπFt), the output signal has the expression:
y(t)
≈
H(0)X0 + H(−2F0)X−1e−4jπF0t + H(2F0)X1e4jπF0t
=
2A
π + 2|H(2F0)||X1| cos(4πF0t + Φ)
4. Because Ts ≪RC, approximating the derivative by:
∆y(t)
∆t
= Fs(ys(n) −ys(n −1))
is acceptable. The diﬀerential equation becomes the recursive equation:
Fs(ys(n) −ys(n −1)) +
1
RC ys(n) =
1
RC xs(n)
Hence the simulation is equivalent to the ﬁltering:
ys(n)(1 + τ) −ys(n −1) = τxs(n)
if we choose τ = Ts/RC.

Hints and Solutions
199
5. The following program illustrates the cases of the half-wave and full-wave
rectiﬁers (Figure H4.1):
%===== C2altern.m
% Rectifiers
Fs=5000; Ts=1/Fs; N=800; fa=50; A=220*sqrt(2);
t=(0:N-1)/Fs; xa=A*sin(2*pi*fa*t);
xrS=.5 *(sign(xa)+1) .* xa; % half-wave
xrD=abs(xa);
% full-wave
%===== simulation of RC(dy/dt) + y = x
RC=.02; tau=Ts/RC; mu=1/(1+tau); nu=tau*mu;
yS=filter(nu,[1 -mu],xrS); yD=filter(nu,[1 -mu],xrD);
subplot(211); plot(t,xrS,'-',t,yS,[0 max(t)],[A/pi A/pi]);
grid; subplot(212); plot(t,xrD,'-',t,yD,'-',...
[0 max(t)],[(2*A)/pi (2*A)/pi]); grid
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0
100
200
300
400
0
100
200
300
400
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Figure H4.1 – Simulation of the half-wave and full-wave rectiﬁers

200
Digital Signal and Image Processing using MATLAB®
H4.2 (Simulation in the presence of a ZOH) (see p. 74)
1. If we let Φ(t) = eAt and notice that e(u) remains constant between kT
and (k + 1)T, we have:
x((k + 1)T)
=
Φ(T)x(kT) +
∫(k+1)T
kT
Φ((k + 1)T −u)be(kT)du
=
Φ(T)x(kT) + e(kT)ΨkT b
with:
ΨkT =
∫(k+1)T
kT
Φ((k + 1)T −u)du =
∫T
0
Φ(u)du = Ψ(T)
The output at the sampling times is given by s(kT) = cT x(kT).
2. The inter-sample response is given by:
{ x(t) = Φ(t −kT)x(kT) + e(kT)Ψ(t)b
s(t) = cT x(t)
with Ψ(t) =
∫t−kT
0
Φ(u)du.
In order to know the system’s behavior
between kT and (k + 1)T, we need to know x(kT) and Ψ(t).
3. Type the following program:
%===== Crepetats.m
% Sampling frequency (1/T)=10 Hz
% Simulation duration tmax=10 s
% Initial conditions x0
A=[0 1;-1 -1.4]; b=[0;1]; c=[1 0];
T=.1; tmax=10; x0=zeros(2,1);
[t,s]=Crepind(A,b,c,T,tmax,x0);
plot(t,s,'x'); grid
The Crepind function computes the system’s step response:
function [realt,xout] = Crepind(A,b,c,Ts,tmax,x0)
%!==================================================!
%! Step response of a linear system
!
%! SYNOPSIS: [realt,xout]=CREPIND(A,b,c,Ts,tmax,x0) !
%! Input: (A,b,c) = state representation
!
%!
Ts
= sampling frequency
!
%!
tmax
= observation duration
!
%!
x0
= initial state
!
%! Output: realt
= real time
!
%!
xout
= response
!

Hints and Solutions
201
%!==================================================!
npts=floor(tmax/Ts); [N,N]=size(A);
% system order
%===== sampling frequency = 1/Ts Hz
Ae=[A b;zeros(1,N+1)]*Ts; Aexp=expm(Ae);
phi=Aexp(1:N,1:N); psib=Aexp(1:N,N+1);
tps=[0:npts-1]; realt=tps * Ts; xout=zeros(1,npts);
xx=x0;
% initial conditions
xout(1)=c*xx;
for k=2:npts
xx=phi * xx + psib; xout(k)=c * xx;
end
4. Changing from continuous-time over to discrete-time requires the use of
the bilinear transform obtained with the nbilin function:
%===== Cetatscomp.m
% Crepetats.m must be run before
tmax=10;
%===== direct calculation
-> continuous time
A=[0 1;-1 -1.4]; b=[0;1]; c=[1 0]; x0=zeros(2,1);
[tps,s]=Crepind(A, b, c, T, tmax, x0);
%===== calculating using the bilinear transform (T=0.1)
pol=[1 1.4 1]; T=.1; [DX,NX]=nbilin(pol,T);
N=floor(tmax / T); repind=filter(NX,DX,ones(1,N));
%===== calculation using the bilinear transform (T=0.4)
T=.4; N=floor(tmax / T); [DX,NX]=nbilin(pol,T);
repind2=filter(NX,DX,ones(1,N));
tps2=[0:N-1] * T;
plot(tps,s,'-', tps,repind,'x', tps2,repind2,'o'); grid
function [B,A]=nbilin(pol,Ts)
%!==================================================!
%! Bilinear transform of a polynomial
!
%! SYNOPSIS: [B,A]=NBILIN(pol,Ts)
!
%!
pol = polynomial (decreasing powers of s)
!
%!
= a0 s^n+a1 s^(n-1)+ ... +aN
!
%!
Ts
= sampling period
!
%!
B,A = numerator and denominator of the result !
%!==================================================!
if nargin<2, Ts=1; end
NX=[1 -1]*2/Ts; DX=[1 1];
nP=length(pol); PP=zeros(nP,1); PP(:)=pol;
B=pol(1); A=[1];
for k=2:nP
A=conv(A,DX); B=conv(B,NX) + pol(k)*A;
end

202
Digital Signal and Image Processing using MATLAB®
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
1.2
T=0.1 s
T=0.4 s
Continuous-time
Figure H4.2 – Comparing step response calculations: in continuous-time and in
discrete-time using the bilinear transform
H4.3 (Non-minimal system) (see p. 75)
1. Simulation:
%===== repetat.m
clear
%===== system definition
atc=[-11/4 -11/8 -5/4;27/4 11/8 21/4;15/8 19/16 5/8];
btc=[1;-1;-1/2]; ctc=[3/8 1/2 -1/4];
%===== parameters
T=.2;
% sampling period
x0=[0;0;0];
% initial state
tpm=130;
% duration of the simulation
Npts=tpm/T; tps=[0:Npts]*T; e=ones(1,Npts);
%===== discrete time equivalence
atd=expm(atc*T); abs(eig(atd))
btd=inv(atc)*(atd-eye(3,3))*btc;
ctd=ctc;
%===== simulation
x=x0; s=[ctd*x0]; m=[max(max(abs(x0)))]; zz=[x0'];
for k=1:Npts
x=atd*x+btd*e(k); s(k+1)=ctd*x;
zz=[zz;x']; m(k+1)=max(max(abs(x)));
end
figure(1);
title('Norm of the state vector')
plot(tps,s,'-'); grid
2. The state vector’s evolution indicates an instability.

Hints and Solutions
203
3. The transfer function calculation ends up with:
G(s) = cT (sI −A)−1b =
1
s2 + s + 1
An “unstable” pair is eliminated. From an input-output point of view,
the system is stable. However, the presence of initial conditions causes
the output to diverge because the unstable pole is not simpliﬁed in the
free part of the response. It can be shown that the discrete time system
has the same property.
4. The system diverges because of the computation noise (Figure H4.3). Cal-
culating the roots of poly(phi-psib*c')-poly(phi) and of poly(phi)
shows that there are a pole and a zero which are almost identical. As
there is not really – physically – a simpliﬁcation, this is another reason
for the divergence of the output.
In the area of the control of a sys-
tem, here the ﬁlter, we can show that the simpliﬁcation is linked to the
nonobservable and/or noncontrollable nature of the system.
0
20
40
60
80
100
120
140
−8
−6
−4
−2
0
2
Figure H4.3 – Response for a longer simulation time (tpm=130)
H4.4 (RK-2 solver and ode15i function) (see p. 83)
1. For Heun’s algorithm, we have h1 = 3T/4, from which we obtain a2,0 =
T/4 and a2,1 = 3T/4.
%===== testheun.m
% fixed points: [0;0] and [c/d;a/b] (omeg^2=ac)
a=1; b=1; c=1; d=1;
y0=[3/2;3]; t0=0;
T=0.1; t=[0:T:2.35*pi]; Lt=length(t);
y=[y0,zeros(2,Lt-1)];
for k=1:Lt-1

204
Digital Signal and Image Processing using MATLAB®
ydot=lv(a,b,c,d,t(k),y(:,k));
ykp1=rk2lv(a,b,c,d,y(:,k),ydot,t(k),T);
y(:,k+1)=ykp1;
end
plot(y(:,1),y(:,2),'b',y0(1),y0(2),'or')
set(gca,'xlim',[0 4.5],'ylim',[0 4.5]), grid on; hold off
function ykp1=rk2lv(a,b,c,d,yk,ydot,tk,T)
%!==================
%! h1=2*T/3, h2=T : Heun algorithm
%!==================
h1=2*T/3;
ykt1=yk+h1*lv(a,b,c,d,tk,yk);
% y(kT+h1)
ykp1=yk+lv(a,b,c,d,tk,yk)*T/4+...
lv(a,b,c,d,tk+h1,ykt1)*3*T/4; % y(kT+h2)
end
function ydot=lv(a,b,c,d,t,y)
ydot=zeros(2,1);
ydot(1)=a*y(1)-b*y(1)*y(2);
ydot(2)=-c*y(2)+d*y(1)*y(2);
end
2. use of ode15i:
%===== testlv2.m
% fixed points: [0;0] and [c/d;a/b] (omeg^2=ac)
a=1; b=1; c=1; d=1;
xinit=[3/2 1.1 1.1 1 1;3 1.3 1.1 2 4];
t0=0; t=[0:0.1:2.35*pi];
for k=1:size(xinit,2)
y0=xinit(:,k);
[t,y] = solve_lv2(a,b,c,d,y0); hold on
plot(y(:,1),y(:,2))
plot(y0(1),y0(2),'or')
end
set(gca,'xlim',[0 4.5],'ylim',[0 4.5])
grid on; hold off
function [t,y]=solve_lv2(a,b,c,d,y0)
tspan=[0:0.1:2.35*pi]'; yp0=zeros(2,1);
options = odeset('RelTol',1e-4,'AbsTol',[1e-6 1e-10]);
[t,y] = ode15i(@lv,tspan,y0,yp0,options);
%==== Lotka-Volterra equation
function res=lv(t,y,yp)

Hints and Solutions
205
0
1
2
3
Figure H4.4 – Comparison of the result obtained with Heun’s algorithm and that
obtained with ode45 (exercise 4.3)
res=[yp(1)-a*y(1)+b*y(1)*y(2);
yp(2)+c*y(2)-d*y(1)*y(2)];
end
end
H4.5 (Newton’s method (see p. 88))
1. Recurrence relation;
xn+1 = xn −yn
y′n
Note that there is convergence – ﬁxed-point theorem – if 0 < |˙h(x)| <
A < 1 on (V ) (h(x) = x −f(x)/ ˙f(x)). Then, the solution is given by
h(x) = x, therefore f(x) = 0. The condition on ˙h(x) is also written as:
0 <
f(x)
¨f(x)
˙f 2(x)
 < A < 1
(7.2)
which causes the ﬁrst and second derivatives not to take a value of zero
over (V ) and therefore the concavity does not change.

206
Digital Signal and Image Processing using MATLAB®
function xnp1=recn(xn,mp)
mpdot=polyder(mp); % derivative
yn=polyval(mp,xn); ydotn=polyval(mpdot,xn);
if ydotn==0 then
error('Divide by zero, modify the initial value')
end
xnp1=xn-yn/ydotn;
end
2. Program:
%===== exonewton.m
% roots: 1,sqrt(2),3
myp=[1,(-4-sqrt(2)),(3+4*sqrt(2)),-3*sqrt(2)];
mdeg=length(myp)-1;
nrts=zeros(1,mdeg); % estimated roots
xn=3; xnp1=2;
% initial value
myeps=2*eps;
for k=1:mdeg
disp('Poly='),disp(myp)
while abs(xnp1-xn)>myeps
xn=xnp1; xnp1=recn(xn,myp);
end
nrts(k)=xnp1; myp=deconv(myp,[1,-xnp1]);
xn=3; xnp1=2;
end
disp('Poly='), disp(myp)
disp('Roots='), disp(nrts)
H4.6 ( Newton–Raphson method (see p. 90))
function [xc,yc,cErr,poss,k,dv]=CalcXY(pol,x0,y0,teps,nblps,verb)
%!=========================================================!
%! pol
= polynomial, coefficients in descending powers !
%! (x0,y0) = initial value (x0+i*y0)
!
%! teps
= used by the test: |Delta x|+|Delta y|<teps
!
%! nblps
= max. number of loops
!
%! verb
= true or false, for debugging purposes
!
%! (xc,yc) = one root (xc+i*yc)
!
%! cerr
= error code
!
%! k
= number of loops
!
%! dv
= value of |Delta x|+|Delta y| when exiting
!
%!=========================================================!
if nargin<6, verb=false; end
deg=length(pol)-1; P=zeros(1,deg+1); P(:)=pol;
pp=P; ppd=[deg:-1:1] .* P(1:end-1); % deriv. (see polyder)
df=1; dv=1; poss=zeros(nblps,4);

Hints and Solutions
207
k=1; xk=x0; yk=y0;
Pz=polyval((pp),x0+1i*y0); Uk=real(Pz); Vk=imag(Pz);
if verb, poss(k,:)=[x0,y0,Uk,Vk]; end
while (dv>teps) && (k<nblps)
Pk=polyval((ppd),xk+1i*yk); dUsdx=real(Pk); dVsdx=imag(Pk);
den2=dUsdx * dUsdx + dVsdx * dVsdx;
Dxk=-(Uk * dUsdx + Vk * dVsdx)/den2;
Dyk=(Uk * dVsdx - Vk * dUsdx)/den2;
dv=abs(Dxk)+abs(Dyk);
%===== x(k+1), y(k+1)
xk=xk+Dxk; yk=yk+Dyk;
%===== U(k+1), V(k+1)
Pz=polyval((pp),xk+1i*yk); Uk=real(Pz); Vk=imag(Pz);
k=k+1;
if verb, poss(k,:)=[xk,yk,Uk,Vk]; end
end
xc=xk; yc=yk;
if (dv<teps)
cErr=0; % no error
else
cErr=1; % max counter reached
end
Evidently, in terms of processing speed, improvements can certainly be
made. An interpreter is not ideal for implementing this type of computation.
However, it is an excellent way to “prototype” a version of a program written
in a diﬀerent language.
%===== tstCalcXY.sce
% calculates one root (for others polynomial division
% is implemented by the deconv function)
clear all
pol=[2 -3 1]; % Pol(z)=2z^2-3z+1
verb=true;
deg=length(pol)-1;
m=1; mroots=zeros(deg,2);
while deg>=1
[xc,yc,cErr,poss,k,dv]=CalcXY(pol,pi,exp(1),10^(-5),100,verb);
mroots(m,1)=xc; mroots(m,2)=yc;
if verb,
xc,yc,cErr,poss,k,dv
subplot(211); plot(poss(1:k,1),poss(1:k,2),'-'), hold on
plot(poss(1:k,1),poss(1:k,2),'x'), grid on
subplot(212); plot(poss(1:k,3),poss(1:k,4),'-'), hold on
plot(poss(1:k,3),poss(1:k,4),'x'), grid on
pause
end
m=m+1;
pol=deconv(pol,[1 (-xc-1i*yc)]); % pol. division

208
Digital Signal and Image Processing using MATLAB®
deg=deg-1;
end
mroots
0.5
1
1.5
2
2.5
3
3.5       
−1
0
1
2
3
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
−10
0
10
20
30
1
1.0002 1.0004 1.0006 1.0008
1.001
1.0012 1.0014
−1
0
1
2
Figure H4.5 – Evolution of z and P(z) during the calculation of the root zc = 1 for
the chosen initial condition
H4.7 (Plot of a natural cubic spline) (see p. 99)
1. Main program:
%===== natcubicspline.m
pts=[0;0;2;3;1]+1i*[0;2;0;2;1];
msd=secondderiv(pts);
figure(1)
npts=20; drawcs(pts,msd,npts)
grid on
Calculation of the second derivatives on the basis of the control points A,
B, C, D and E:
function msd=secondderiv(ptlist)
%!=============================================!
%! SYNOPSIS: msd=SECONDDERIV(ptlist)
!

Hints and Solutions
209
%! ptlist = vector (N*1) of control points
!
%!
(complex)
!
%! msd
= vector (N*1) of second derivatives !
%!
(msd(1)=0, msd(N)=0)
!
%!=============================================!
N=length(ptlist);
tt=toeplitz(ptlist(3:N),...
[ptlist(3),ptlist(2),ptlist(1)]);
b=tt*[1;-2;1];
C=[4;1;zeros(N-4,1)]; R=C.';
A=toeplitz(C,R)/6;
msd=A\b; msd=[0;msd;0];
The derivatives can be calculated by:
function md=calcderiv(ptlist,msd)
%!=========================================!
%! SYNOPSIS: CALCDERIV(ptlist,msd)
!
%! ptlist = vector (N,1) of control points !
%! msd
= second derivatives
!
%!=========================================!
N=length(ptlist); md=zeros(N,1);
md=diff(ptlist)+diff(msd)/3-msd(2:N)/2;
md(N)=ptlist(N)-ptlist(N-1)+msd(N-1)/6;
2. Plot function:
function drawcs(ptlist,msd,npts)
%!=========================================!
%! SYNOPSIS: DRAWCS(ptlist,msd,npts)
!
%! ptlist = vector (N,1) of control points !
%! msd
= second derivatives
!
%! npts
= number of points for each
!
%!
segment P(k),P(k+1)
!
%!=========================================!
N=length(ptlist); t=[0:npts]/npts;
t2=t.*t; t3=t2.*t;
hold on
for k=1:N-1
pt=(msd(k+1)-msd(k))*t3/6 + msd(k)*t2/2 + ptlist(k);
pt=pt+(ptlist(k+1)-ptlist(k)-msd(k)/3-msd(k+1)/6)*t;
plot(real(pt),imag(pt),':r')
plot(real(ptlist(k)),imag(ptlist(k)),'o')
end
plot(real(ptlist(N)),imag(ptlist(N)),'o')
hold off
H4.8 (Relaxation method) (see page 102)

210
Digital Signal and Image Processing using MATLAB®
4
3
2
1
0
−1
−2
−
Figure H4.6 – Veriﬁcation with visualization of the tangents
1. In the Jacobi method, we have M = D and N = L + U and, in the
Gauss–Seidel method, M = D −L and N = U.
∆j
=
det(λI −(D)−1(L + U))
=
det(D−1)det(λD −L −U)
(7.3)
and
∆gs
=
det(λI −(D −L)−1U)
=
det((D −L)−1)det(λD −λL −U)
(7.4)
Example: in the case of N = 5, the dimension of the matrix is (N −
2) × (N −2) = (3 × 3). If we write dj = det(λD −L −U) and dgs =
det(λD −λL −U):
dj = det


4λ
1
0
1
4λ
1
0
1
4λ

and dgs = det


4λ
1
0
λ
4λ
1
0
λ
4λ


and we verify the relation between spectral radii:
dj
=
4λ(16λ2 −1) −4λ = 8λ(8λ2 −1) ⇒ρj =
√
1/8
dgs
=
4λ(16λ2 −λ) −4λ2 = 8λ2(8λ −1) ⇒ρgs = 1/8

Hints and Solutions
211
The program testridiag.m:
%===== testtridiag.m
for P=3:8
[pJ,pGS,rhoJ,rhoGS]=dettridiag(P);
[rhoGS,rhoJ^2]
pJ,pGS
end
with the function dettridiag:
function [pJ,pGS,rhoJ,rhoGS]=dettridiag(P)
%!=============================================!
%! SYNOPSIS [pJ,pGS,rhoJ,rhoGS]=dettridiag(P)
!
%! P
: system dimension
!
%! pJ,pGS : polynomials (Jacobi, Gauss-Seidel) !
%! rhoJ,rhoGS : spectral radius
!
%!=============================================!
R=[4 1 zeros(1,P-2)];
A=toeplitz(R',R)/6;
D=diag(diag(A)); L=-tril(A,-1); U=-triu(A,1);
%===== Jacobi
M=D; N=L+U; mM=inv(M)*N;
vJ=eig(mM); rhoJ=max(abs(vJ));
pJ=poly(mM)*(4^P);
%===== Gauss-Seidel
M=D-L; N=U; mM=inv(M)*N;
vGS=eig(mM); rhoGS=max(abs(vGS));
pGS=poly(mM)*(4^P);
gives:
n = 3
dj
64λ3 −8λ
dgs
64λ3 −8λ2
n = 4
dj
256λ4 −48λ2 + 1
dgs
256λ4 −48λ3 + λ2
n = 5
dj
1024λ5 −256λ3 + 12λ
dgs
1024λ5 −256λ4 + 12λ3
n = 6
dj
4096λ6 −1280λ4 + 96λ2
dgs
4096λ6 −1280λ5 + 96λ4 −λ3
n = 7
dj
16384λ7 −6144λ5 + 640λ3 −16λ
dgs
16384λ7 −6144λ6 + 640λ5 −16λ4
n = 8
dj
65536x8 −28672x6 + 3840x4 −160x2 + 1
dgs
65536x8 −28672x7 + 3840x6 −160x5 + x4

212
Digital Signal and Image Processing using MATLAB®
◦More generally, consider a matrix of the form 7.3 and of dimensions
(P × P). The notation d(n)
j
represents the determinant of a submatrix of
dimension (n × n). We have:
d(n)
j
= 4λd(n−1)
j
−d(n−2)
j
with d(1)
j
= 4λ and d(2)
j
= 16λ2 −1
The expression of d(n)
j
is:
d(n)
j
= α
(
2λ +
√
4λ2 −1
)n
+ β
(
2λ −
√
4λ2 −1
)n
We know that d(1)
j
= 4λ and d(2)
j
= 16λ2 −1. By setting ∆= 4λ2 −1,
we obtain:
d(n)
j
=
(1
2 +
λ
√
∆
) (
2λ +
√
∆
)n
+
(1
2 −
λ
√
∆
) (
2λ −
√
∆
)n
=
[(
2λ +
√
∆
)n+1
−
(
2λ −
√
∆
)n+1]
1
2
√
∆
(7.5)
◦The determinant of the (n×n) matrix associated with the Gauss–Seidel
method satisﬁes:
d(n)
gs = 4λ(n−1)
gs
−λd(n−2)
gs
with d(1)
gs = 4λ and d(2)
j
= 16λ2 −λ
The expression of d(n)
gs is:
d(n)
gs = α
(
2λ +
√
4λ2 −λ
)n
+ β
(
2λ −
√
4λ2 −λ
)n
By setting ∆′ = 4λ2 −λ, we ﬁnd:
d(n)
gs
=
(1
2 +
λ
√
∆′
) (
2λ +
√
∆′
)n
+
(1
2 −
λ
√
∆′
) (
2λ −
√
∆′
)n
=
[(
2λ +
√
∆′
)n+1
−
(
2λ −
√
∆′
)n+1]
1
2
√
∆′
(7.6)
The eigenvalues are given by d(P )
j
(λ) = 0 and d(P )
gs (λ) = 0 with the
expressions 7.5 and 7.6:
λ(k)
j
=
cos(kπ/(P + 1))
2
, k = 1 : P
λ(k)
gs
=
cos2(kπ/(P + 1))
4
, k = 1 : ⌊P/2⌋

Hints and Solutions
213
The eigenvalues of d(P )
gs
are either positive or null. Type:
%===== spectralradius.m
P=8; [pJ,pGS,rhoJ,rhoGS]=dettridiag(P);
%===== Jacobi
rr=roots(pJ); rrt=cos([1:P]'*pi/(P+1))/2;
[rr,abs(rr),rrt]
%====== Gauss-Seidel
rr3=zeros(P,1); mP=fix(P/2);
rr3(1:mP)=rrt(1:mP).^2;
rr2=roots(pGS);
[rr2,abs(rr2),rr3]
2. Convergence of the relaxation method: the state change matrix is given
by:
(D
ω −L
)−1 (1 −ω
ω
D + U
)
(7.7)
whose determinant is (1 −ω)P . From this, we deduce that the spectral
radius ρr is ≥(1 −ω).
The condition (4.48) (|ρr| < 1) gives us 0 ≤ω < 2.
3. Programming:
%===== relaxtest.m =====
clear
N=5; eps=1e-6; maxiter=100;
A = 4*eye(N) + diag(ones(1,N-1),1) + diag(ones(1,N-1),-1);
A(1,1)=2; A(N,N)=2;
xex=ones(N,1); b=A*xex;
x0=zeros(N,1); maxit=100;
stcr0=1e-6; merr=[]; w0=(.5:.025:1.74); rhot=[];
for w=w0
[x,rho,eltime]=relaxmethod(A,b,x0,w,stcr0,maxit);
merr=[merr;sqrt((x-xex)'*(x-xex))];
rhot=[rhot;rho];
end
subplot(211), plot(w0,merr), grid
subplot(212),
plot(w0,rhot,'-',w0,rhot,'xr'), grid

214
Digital Signal and Image Processing using MATLAB®
1
2
3
4
5
0
0.4
0.4
0.2
0
0.6
0.6
0.8
0.8
1
1.2
1.4
1.6
1.8
2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Figure H4.7 – Top: Evolution of the error, and bottom: evolution of the spectral
radius, as a function of ω
H4.9 (Cholesky factorization) (see p. 103)
Type:
function L=CHOfact(A,L,N0)
%!===================================================!
%! CHOfact calculates in L a lower triangular matrix !
%! so that A=L*L^H (Cholesky factorization)
!
%! with A symmetric definite positive
!
%! SYNOPSIS: L=CHOFACT(A,L,N0)
!
%!
N0=size(A,1)
!
%!
A = symmetric definite positive
!
%!
L = used as an input allows
!
%!
the recursivity (initial value [])
!
%! EXAMPLE: A=rand(4,4); A=A*A.'; N0=size(A,1);
!
%!
L=[]; L=CHOfact(A,L,N0);
!
%!===================================================!
N=size(A,1); if N==0, return, end
Lp=zeros(N,1); Lp(1)=sqrt(A(1,1));
Lp(2:N)=A(2:N,1)/Lp(1); vec=Lp(2:N);
L=[L,[zeros(N0-N,1);Lp]];
B=A(2:N,2:N)-(vec * vec');
L=CHOfact(B,L,N0);

Hints and Solutions
215
H5
Speech processing
H5.1 (Compression of a speech signal) (see page 114)
1. Pitch detection:
function [vnv,pitch]=detectpitch(sig,trhld,tmin,tmax,energm)
%!=========================================================!
%! Pitch detection using correlation
!
%! SYNOPSIS:
!
%!
[vnv,pitch]=DETECTPITCH(sig,trhld,tmin,tmax,energm) !
%!
sig
= signal block
!
%!
trhld
= correlation treshold
!
%!
tmin,tmax = correlation window
!
%!
energm
= energy threshold
!
%!
vnv
= TRUE if voiced, otherwise FALSE
!
%!
pitch
= pitch period
!
%!=========================================================!
nfa=length(sig); x=zeros(nfa,1); x(:)=sig; ae=x'*x;
if (ae > energm),
% energy>trhld
for T=tmin:tmax
stmT=x(T:nfa); s0T=x(1:nfa-T+1);
autoc=stmT'*s0T; etmT=stmT'*stmT; e0T=s0T'*s0T;
correl(T-tmin+1)=autoc/sqrt(etmT*e0T);
end
[corrmax,imax]=max(correl); tfond = imax+tmin-1;
if (corrmax < trhld),
vnv=(0==1); pitch=0; return;
else
pitch=tfond; vnv=(0==0);
end
else
pitch=-1; vnv=(0==1);
end
Figure H5.1 shows the shape of the autocorrelation for a block.
The
maximum is located in T = 82, which means that the pitch frequency is
roughly f0 = 8, 000/82 ≈97.5 Hz. Determining T can become a diﬃcult
task when there are maxima present at the multiples of the pitch period.
One solution is to check for the possible presence of a high maximum
at sub-multiples of the x-coordinate found for the maximum. We can
also study the evolution over diﬀerent consecutive windows by comparing
the obtained fundamental frequencies. Bear in mind, ﬁnally, that the
accuracy can be improved by oversampling the signal beforehand.

216
Digital Signal and Image Processing using MATLAB®
50
60
70
80
90
100
110
120
130
140
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
T=82
Figure H5.1 – Example of autocorrelation graph
2. Coding program (using the xtoa function from page 115):
%===== CODE.M
%!==========================================================!
%! code.m: Coding a speech signal based on an AR-model
!
%! INPUT:
!
%!
Signal sampled at 8000 Hz
!
%! OUTPUT: array tab_cod(N,XX):
!
%!
tab_cod(N,1): energy in the block of signal
!
%!
tab_cod(N,2): pitch period
!
%!
tab_cod(N,3:12): AR coefficients (AR-ordv if voiced
!
%!
sound, AR-ordnv otherwise) or reflection coeffs. !
%! Each block has a 240 sample length (30 ms) with an
!
%! overlap of 60 samples.
!
%! Uses: xtoa
: AR-model coeffts
!
%!
detectpitch : pitch detection
!
%!
ai2ki
: reflection coeffs
!
%!==========================================================!
clear
load phrase;
%===== vector y
enerm=std(y)^2*.1;
% AR-model orders for voiced and non voiced sounds
ordv=20; ordnv=10;
NbParam=ordv+2;
phrase=y-mean(y);
%===== parameters
lbloc=240;
% block length
recouv=60;
% overlap
ltr=lbloc-recouv;
nblocs=floor((length(phrase)-recouv)/ltr); % nb of blocks
reste=rem(length(phrase)-recouv,ltr);

Hints and Solutions
217
phrase=phrase(1:length(phrase)-reste);
tmin=40; tmax=150; seuil=0.7;
% for pitch detection
%=====
vnv=zeros(1,nblocs);
% boolean "voiced/non voiced"
pitch=zeros(1,nblocs);
% pitch period
tab_cod=zeros(nblocs,NbParam); % coeffts of the model
%===== detection "voiced/non voiced"
sprintf('"Voiced/non voiced" on %5.0f blocks', nblocs)
TIC
for k=1:nblocs,
ind=(k-1)*ltr;
blocan=phrase(ind+1:ind+lbloc);
% analysis block
[vnv(k) pitch(k)]=detectpitch(blocan,seuil,tmin,tmax,enerm);
end;
TOC
%===== AR-model
sprintf('AR-model')
TIC
preacpar=filter([1 -0.9375],1,phrase); % pre-emphasis
for k=2:(nblocs-1),
%========
if (vnv(k-1) == vnv(k+1)),
% correction of
vnv(k)=vnv(k-1);
% errors of detection
if (vnv(k)==1)
%==== "voiced" with pitch=mean
pitch(k)=floor((pitch(k-1)+pitch(k+1))/2);
else
%==== "non voiced" with pitch=0
pitch(k)=0;
end
end
%===== analysis block
sigbloc=preacpar((k-1)*ltr+1:(k-1)*ltr+lbloc);
if (vnv(k)==1)
[pcoeff,enrg]=xtoa(sigbloc,ordv);
%=======
%=====> coeff_refl=ai2ki(pcoeff);
% reflection
%=====> tab_cod(k,3:NbParam)=coeff_refl; % coeffts
tab_cod(k,3:NbParam)=pcoeff(2:ordv+1)';
tab_cod(k,1)=enrg;
tab_cod(k,2)=pitch(k);
else
[pcoeff, enrg]=xtoa(sigbloc,ordnv);
%=====> coeff_refl=ai2ki(pcoeff);
% reflection
%=====> tab_cod(k,3:NbParam)= coeff_refl; % coeffts
tab_cod(k,1)=enrg;
tab_cod(k,2)=0;
tab_cod(k,3:NbParam)=[pcoeff(2:ordnv+1)' ...
zeros(1,ordv-ordnv)];
end;
end;

218
Digital Signal and Image Processing using MATLAB®
TOC
sprintf('Writing array in tab_cod.mat')
save tab_cod tab_cod
Notice the presence of a high-pass type pre-emphasis ﬁlter preceding the
operations for estimating the model’s parameters.
The prediction coeﬃcients, obtained by analyzing the signal, are stored
as “double ﬂoating point” numbers (8 bytes). If we had to use a “ﬁxed-
point” processor, it might be better to consider the reﬂection coeﬃcients
with values between −1 and +1. Speech coders also use what are called
lsp coeﬃcients, short for Line Spectrum Pair ([26]).
3. Decoding program:
%===== decode.m
% Decoding the file tab_cod.mat
clear
TIC
load tab_cod;
% tab_cod(nblocs,XX);
excgl=eye(1,40);
% glottal signal
lbloc=240;
% block length
recouv=60;
% overlap
ltr=lbloc-recouv;
OvlRec=lbloc/3;
% overlap reconstruction(1/3)
LBrec=lbloc+2*(OvlRec-recouv); % reconstructed block length
nblocs=size(tab_cod,1); NbParam=size(tab_cod,2);
outsig=[]; finalsig=zeros(1,nblocs*ltr+OvlRec);
%===== Reconstruction window
fen_rec=[(1:OvlRec)/OvlRec ones(1,lbloc-2*recouv) ...
(OvlRec:-1:1)/OvlRec];
ImpGlPtr=0;
LgExcGl=length(excgl);
NbSmpTot=LBrec+ LgExcGl;
% because of the filtering
drap_vnv=0;
%=====
for k=2:(nblocs-1),
if (tab_cod(k,2)~=0)
%===== voiced block
if (drap_vnv==1)
% the previous one is voiced
%==== continuity of the input signal
trame=[TmpSig(ltr+1:NbSmpTot), zeros(1,ltr)];
NbSmp=NbSmpTot-ltr+ImpGlPtr;
else
% The previous one is not voiced
trame=zeros(1,NbSmpTot); NbSmp= 0;
end
PitchPeriod=tab_cod(k,2);
% block pitch
while (NbSmp<LBrec),
trame((NbSmp+1):(NbSmp+LgExcGl))=excgl;
NbSmp=NbSmp+PitchPeriod;

Hints and Solutions
219
end
drap_vnv=1; ImpGlPtr=NbSmp-NbSmpTot;
TmpSig=trame; trame=trame(1:LBrec);
trame=trame/std(trame);
% normalization
else
%===== non voiced
ImpGlPtr=0;
drap_vnv=0;
% gaussian
trame=randn(1,LBrec);
% white noise
end;
trame=sqrt(tab_cod(k,1))*trame;
% power
%den=ki2ai(tab_cod(k,3:NbParam)); %<===========
den=[1 tab_cod(k,3:NbParam)];
outsig=filter(1,den,trame); outsig=fen_rec .* outsig;
st=(k-1)*ltr;
%==== construction with an overlap
finalsig((st+1):(st+LBrec))=...
finalsig((st+1):(st+LBrec)) + outsig;
end;
finalsig=filter(1,[1 -0.9375],finalsig);
% de-emphasis
TOC, soundsc(finalsig,8000);
The length of the block can be modiﬁed in the instruction lbloc=240 of
the decode.m program. By typing for example lbloc=180, the reconstructed
windows are shorter and the same sentences are uttered faster. In both cases,
this modiﬁcation of the utterance speed occurs without a change in the timbre
of the voice, which keeps its original, natural aspect. This is no longer the case
if the sampling frequency is modiﬁed suddenly, with the same ratio by typing
for example soundsc(y,Fe*3/4) to slow down the sentence. You can listen to
the results and compare.
H5.2 (DTW) (see p. 119)
Type the following function:
function [Dmin,DTWway,CD]=DTW1(xx,yy)
%!=============================================!
%! Synopsis: [Dmin,DTWway,CD]=DTW1(xx,yy)
!
%!
xx,yy
= cepstrum of x and y signals
!
%!
Dmin
= minimal cumulative distance
!
%!
DTWway = DTW way
!
%!
CD
= array of cumulative distances !
%!=============================================!
mxx = 1; mxy = 2; myy = 1;
dd=size(xx,1); Ix=size(xx,2); Jy=size(yy,2);
distance=zeros(Ix,Jy);
for ix=1:Ix
for jy=1:Jy
diffe=xx(:,ix)-yy(:,jy);
distance(ix,jy)=sqrt(diffe'*diffe);

220
Digital Signal and Image Processing using MATLAB®
end
end
%===== cumulative distance
CD = zeros(Ix, Jy);
%===== parent to keep
Parent = zeros(Ix,Jy,2);
%===== CD initialization
CD(1,1) = distance(1,1);
Parent(1,1,:) = [1 1];
for ix = 2:Ix
CD(ix,1) = distance(ix,1)+mxx*CD(ix-1,1);
Parent(ix,1,:)= [ix-1 1];
end
for jy = 2:Jy
CD(1,jy) = distance(1,jy)+myy*CD(1,jy-1);
Parent(1,jy,:) = [1 jy-1];
end
%===== main loop
nT = min(Ix, Jy);
for tt = 2:nT
for ix = tt:Ix
DD = [CD(ix-1,tt)+mxx*distance(ix,tt), ...
CD(ix-1,tt-1)+mxy*distance(ix,tt), ...
CD(ix,tt-1)+myy*distance(ix,tt)];
[val,ind] = min(DD);
CD(ix,tt) = val;
switch ind
case 1
Parent(ix,tt,:)=[ix-1,tt];
case 2
Parent(ix,tt,:)=[ix-1,tt-1];
case 3
Parent(ix,tt,:)=[ix,tt-1];
end
end
for jy = tt+1:Jy
DD = [CD(tt-1,jy)+mxx*distance(tt,jy), ...
CD(tt-1,jy-1)+mxy*distance(tt,jy), ...
CD(tt,jy-1)+myy*distance(tt,jy)];
[val,ind] = min(DD);
CD(tt,jy) = val;
switch ind
case 1
Parent(tt,jy,:)=[tt-1,jy];
case 2
Parent(tt,jy,:)=[tt-1,jy-1];
case 3
Parent(tt,jy,:)=[tt,jy-1];
end
end
end

Hints and Solutions
221
%===== normalization
%===== minimal sum
Dmin = CD(Ix,Jy);
%===== backtracking inverse
bi=zeros(nT+1,2);
%===== we start at the end
bi(1,:)=[Ix Jy]; k=2;
while (Parent(bi(k-1,1),bi(k-1,2),1) ~= 1 ...
& Parent(bi(k-1,1),bi(k-1,2),2) ~= 1)
bi(k,:) = Parent(bi(k-1,1),bi(k-1,2),:);
k = k + 1;
end
DTWway = bi(k-1:-1:1,:);
H5.3 (DTW word recognition) (see page 120)
1. Type the following function:
function cepstre=extractCEPSTRE(xt,Fe)
%!=========================================!
%! Synopsis: cepstre=EXTRACTCEPSTRE(xt,Fe) !
%!
xt
= audio signal
!
%!
Fe
= sampling frequency
!
%!
cepstre = cepstral coefficients
!
%!=========================================!
xt=xt(:);
%===== parameters
pp=10;
% cepstrum order
duree=15;
% window duration in ms
Lfen=fix(Fe*duree/1000); % window size
decal=fix(Lfen/2);
% shift for overlapping
Lfft=2^nextpow2(Lfen);
% FFT size
hamm=0.5-0.5*cos(2*pi*(0:Lfen-1)'/Lfen);
Lx=length(xt);
nbfen=fix(Lx/decal);
cepstre = zeros(pp,nbfen);
for k = 1:nbfen-1
inddeb=(k-1)*decal+1;indfin=inddeb+Lfen-1;
xaux=xt(inddeb:indfin).*hamm;
%===== compute standard cepstral coefficients
Sx = log(abs(fft(xaux,Lfft)));
%===== power cepstrum
Cx = real(ifft(Sx));
cepstre(:,k)=Cx(2:pp+1); % without energy
end

222
Digital Signal and Image Processing using MATLAB®
2. Type the following program:
%===== DTWtry.m
clear all
[x,fe]=wavread('utter2.wav');
[y,fe]=wavread('utter4.wav');
cepx=extractCEPSTRE(x,fe);
cepy=extractCEPSTRE(y,fe);
[Dmin,wayDTW,CD]=DTW1(cepx,cepy); Dmin
figure(1); imagesc(CD');
set(gca,'ydir','normal')
hold on
plot(wayDTW(:,1),wayDTW(:,2),'k'); hold off
H5.4 (PSOLA) (see page 122)
Type the program:
%===== PSOLAtry.M
clear all;
[x,Fs]=wavread('desgens.wav');
gamma=0.8;
x_m=psola(x,Fs,gamma);
soundsc(x_m,Fs);
which calls the following function:
function s_synt=psola(s_orig,Fs,gamma)
%!=========================================!
%! SYNOPSIS: s_synt=PSOLA(s_orig,Fs,gamma) !
%!
s_orig
= signal
!
%!
Fs
= sampling Frequency (Hz)
!
%!
gamma
= modification rate
!
%!
s_synt
= modified Signal
!
%! Uses the F0cor function
!
%!=========================================!
seuil_pitch=0.7;
Rsurech=1;
% for improving the pitch's evaluation
L10ms=fix(Fs/100); % constant size window (10 ms)
fp_min=70; fp_max=400; Lfen=2*fix(Fs/fp_min);
%=====
Ns=length(s_orig); Namax=fix(Ns*fp_max/Fs);
ta=zeros(Namax,1); ta(1)=1;
Pa=L10ms; inda=1;
%===== analysis
while ta(inda)<Ns-Lfen
indsdeb=ta(inda);
% The length Lfen must be large enough
% to allow the estimation of the lowest frequency
indsfin=indsdeb+Lfen; sextrait=s_orig(indsdeb:indsfin);

Hints and Solutions
223
[Fpitch, corr]=...
f0cor(sextrait,Fs,Rsurech,seuil_pitch,fp_min,fp_max);
if isnan(Fpitch)
Pa=L10ms;
else
Pa=fix(Fs/Fpitch);
end;
inda=inda+1; ta(inda)=ta(inda-1)+Pa;
end
ta=ta(1:inda);Na=length(ta);
%===== time scale modification and Synthesis
s_synt=zeros(fix(Ns/gamma),1);
ii=1;ts=1;ie=1;te=1;
while ie<Na-2
ii=ii+1;
te=te+gamma; ie=ceil(te);
Pa=ta(ie+1)-ta(ie); ts=ts+Pa;
winHann=sin(pi*(0:2*Pa)'/(2*Pa)) .^2;
sola=s_orig(ta(ie):ta(ie)+2*Pa) .* winHann;
s_synt(ts-Pa:ts+Pa)= s_synt(ts-Pa:ts+Pa)+sola;
end
H5.5 (Hann window) (see page 123)
Type the following program:
%===== fenHann.m
L=300; alpha=1/6;
n0=fix(alpha*L);
%===== Hann window
hn=abs(sin(pi*(0:L-1)'/L)) .^2;
gn=hn.^2; pp=500; x=zeros(pp*n0,1);
for k=0:pp-ceil(L/n0)
id1=k*n0;
x(id1+1:id1+L)=x(id1+1:id1+L)+gn;
end
figure(1);plot(x)
max(x), sum(gn)/n0
%===== plotting the DTFT of gn
figure(2); Lfft=4*1024; Gf=abs(fft(gn,Lfft));
plot((0:Lfft-1)/Lfft,20*log10(Gf))
set(gca,'xlim',[0 0.2]); hold on;
plot(ones(2,1)*(1:5)/n0,[-140*ones(1,5);40*ones(1,5)],':');
hold off
As you can see on the resulting graph, the sequence x(n) is constant for
any value L. The constant is equal to the sum of the elements of the sequence
g(n) divided by n0. Therefore it depends on α. Notice that the property is still
true for other powers of h(n). By using the Poisson formula, we can show that
this property is equivalent to the fact that the sequence g(n) is equal to zero

224
Digital Signal and Image Processing using MATLAB®
for the multiples of 1/n0. This amounts to choosing the inverse of an integer
as the value of α and to choosing L so as to have Lα equal to an integer.
H5.6 (Phase vocoder) (see page 124)
Type the program:
%===== PHcoder.m
clear all
[x,Fe]=wavread('desgens.wav');
gamma=0.8;
Lfft=256;
x_m = phasevoc(x, gamma, Lfft, Fe);
soundsc(x_m,Fe)
which uses the following functions:
function s_synt=phasevoc(s_orig,gamma,Lfft)
%!==============================================!
%! SYNOPSIS: s_synt=PHASEVOC(s_orig,gamma,Lfft) !
%!
s_orig = audio source
!
%!
gamma
= modification rate
!
%!
Lfft
= FFT length
!
%!
s_synt = modified audio signal
!
%! Uses stft.m, specinterp.m and spec2sig.m
!
%===============================================!
% alpha is the shift rate relative to the FFT length
unsuralpha=8;
% power of 2
n0=fix(Lfft/unsuralpha);
win=hann(Lfft);
%===== initial STFT
spec_a = stft(s_orig,Lfft,n0,win);
%===== calculus of the modified DTFT
spec_s = specinterp(spec_a, gamma);
%===== inversion
s_synt = spec2sig(spec_s, n0,win);
function sig=spec2sig(spec,n0,win)
%!============================================!
%! Synthesis of a signal from its spectrogram !
%! SYNOPSIS: sig=SPEC2SIG(spec,ovlap,win)
!
%!
spec = spectrogram
!
%!
n0
= shift value
!
%!
win
= weighting window
!
%!
sig
= signal
!
%!============================================!
[Lfft,nbcol] = size(spec);
ispec=real(ifft(spec));

Hints and Solutions
225
sig = zeros(Lfft+(nbcol-1)*n0,1);
%===== re-synthesis using a window
for icol = 1:nbcol
sigfen = ispec(:,icol) .* win;
%===== overlap-Add
ixi = (icol-1)*n0+1;
sig(ixi:ixi+Lfft-1) = sig(ixi:ixi+Lfft-1) + sigfen;
end
function spec_s=specinterp(spec_a, gamma)
%!===================================================!
%! Interpolation of a Short Term FT array
!
%! SYNOPSIS: spec_s=SPECINTERP(spec_a, gamma)
!
%!
spec_a = original spectrogram (Short Term FT) !
%!
gamma
= temporal modification rate
!
%!
spec_s = modified spectrogram
!
%!===================================================!
[Lfft,nbcol] = size(spec_a);
ts=1:gamma:nbcol-1;
spec_s = zeros(Lfft,length(ts));
%===== phase and phase increase
phase_a = angle(spec_a);
module_a = abs(spec_a);
diffp = zeros(Lfft,1);
phase_s=phase_a(:,1);
indcol = 1;
for tt = ts
%===== two adjacent columns
ta_min=floor(tt); ta_max=floor(tt)+1;
%===== weighted Mean
pond = tt - floor(tt);
modul = (1-pond)*module_a(:,ta_min) +...
pond *module_a(:,ta_max);
spec_s(:,indcol) = modul .* exp(j*phase_s);
%===== phase diff and accumulation
diffp = phase_a(:,ta_max)-phase_a(:,ta_min);
phase_s = phase_s + diffp;
indcol = indcol+1;
end
function xspec = stft(x,Lfft,ns,win)
%!======================================!
%! SYNOPSIS xspec = STFT(x,Lfft,ns,win) !
%!
x
: signal (N*1)
!
%!
Lfft
: FFT length
!
%!
ns
: shift
!
%!
win
: weighting window (L*1)
!

226
Digital Signal and Image Processing using MATLAB®
%!
xspec : spectrogram
!
%!======================================!
N=length(x); L=length(win);
nbb=fix((N-L)/ns);
xspec=zeros(Lfft,nbb);
for k=1:nbb
idb=k*ns+1; idf=idb+L-1;
xw=x(idb:idf) .* win;
xspec(:,k)=fft(xw,Lfft);
end
H5.7 (Detecting impulse clicks) (see page 126)
1. We have:
ρ
=
|zd(n)|2
E (|zb(n)|2) =
∑+∞
u=−∞g(u)d(n −u)

2
σ2 ∫1/2
−1/2 |G(f)|2df
=
1
σ2
∑+∞
u=−∞g(u)d(n −u)

2
∑+∞
u=−∞g2(u)
d(n)
+
+
b(n)
y(n)
z(n)=zd(n)+zb(n)
g(n)
Figure H5.2 – Matched ﬁlter
2. If we apply the Schwarz inequality to the numerator, we get:

+∞
∑
u=−∞
g(u)d(n −u)

2
≤
+∞
∑
u=−∞
g2(u)
+∞
∑
u=−∞
d2(u)
and therefore ρ ≤Ed/σ2 where Ed = ∑+∞
u=−∞d2(u). The resulting upper
bound is reached if we assume g(u) = d(n −u).
It is therefore the
maximum with respect to g(u). Note that the optimal solution is the
reversed copy of the signal g(n).
In the case where d(u) has a ﬁnite
duration k, we will assume g(u) = d(k −u) in order for the ﬁlter g(n) to
be causal.

Hints and Solutions
227
3. The ﬁlter with the transfer function A(z) = 1 + a1z−1 + · · · + aKz−K
is a linear ﬁlter with the ﬁnite impulse response {h1(n)} = {h1(0) = 1,
h1(1) = a1, . . . , h1(K) = aK}. If we feed the signal x(n) = δ(n) + s(n)
into this ﬁlter’s input, we get the signal y(n) = h1(n) + w(n), which is
the sum of the deterministic signal h1(n) and a white noise.
If we apply the result of the previous question, the conclusion is that
we have to ﬁlter the signal y(n) by the ﬁlter with the impulse response
h1(−n). Aside from a K sample delay, we get the causal ﬁlter with the
impulse response {h2(n)} = {h2(0) = aK, h2(1) = aK−1, . . . , h2(K) =
1}.
4. In the absence of clicks, the input signal y(n) of the matched ﬁlter is a
white noise with the variance σ2. Hence the output signal is centered and
the output spectral density has the expression S(e2jπf) = σ2|A(e2jπf)|2.
The output power is obtained by integrating the spectral density. Using
the Parseval formula, we get:
Pz = σ2(1 + a2
1 + · · · + a2
K)
(7.8)
5. In the absence of clicks, the output signal z(n) of the matched ﬁlter h2(n)
is a centered, Gaussian noise with the variance Pz. The probability of
deciding the presence of a click is therefore given by:
Pr(|z(n)| > s|H0)
=
2
∫+∞
s
1
√2πPz
exp(−u2/2Pz)du
=
2
∫+∞
s/√Pz
1
√
2π exp(−v2/2)dv
=
2Q(s/
√
Pz)
where Q(c) is the integral function of the centered, Gaussian distribution
with the variance 1.
If we choose Q(c) = 0.005, we have c ≈3 and
therefore:
s = 3
√
Pz
This threshold guarantees that the probability of deciding in favor of the
presence of a click, when there is no click, is less than 1%: this is called
the probability of false alarm. In order to set this level to satisfy a sound
criterion, we have to compare the matched ﬁlter’s output with a threshold
of the type λ√Pz. The choice of λ will then be done by listening to the
denoised signal. Pz can be estimated using expression (7.8).

228
Digital Signal and Image Processing using MATLAB®
6. The following program generates the useful signal comprising 500 samples
of an AR-10. The impulses used to simulate clicks have an amplitude
equal to 1.5 times the square deviation of the signal. The program then
estimates the parameters, and computes the residual and the matched
ﬁlter. Finally, the resulting signal is compared to a threshold (Figure
H5.3):
%===== Ccraq.m
clear
%===== original signal (order 10-AR)
a= [1 -1.6507 0.6711 -0.1807 0.6130 -0.6085 0.3977 ...
-0.6122 0.5412 0.1321 -0.2393];
K=length(a); N=500; w=randn(1,N);
s=filter(1,a,w); srms=sqrt(s*s'/N);
%===== NBCRAC clicks with an amplitude +/-1.5 srms
nbcrac=5; poscrac=[73 193 249 293 422];
ampcrac=1.5*srms*(2*round(rand(1,nbcrac))-1);
sig=s; sig(poscrac)=s(poscrac)+ampcrac;
subplot(311); plot(s); grid
subplot(312); plot(sig); grid
%===== detection of the clicks
[aest sw2est]=xtoa(sig,K);
% estimation of the AR
y=filter(aest,1,sig);
% whitening: estim. of residual
z=filter(aest(K:-1:1),1,y); % matched filtering
subplot(313); plot(z); grid
V0eff=sqrt(sw2est*aest'*aest);
lambda=3; threshold=lambda*V0eff;
izthreshold=find(abs(z)>threshold); % threshold
izthreshold=izthreshold-K;
% filter delay
lzs=length(izthreshold);
%===== extraction of the maxima (3 samples from each other)
dist=izthreshold - [0 izthreshold(1:lzs-1)];
mpl3=find(dist>3); lm3=length(mpl3); mpl3=[mpl3 lzs+1];
for ii=1:lm3
t1=izthreshold(mpl3(ii)); t2=izthreshold(mpl3(ii+1)-1);
[zmax(ii) im]=max(z(t1:t2));
posEstim(ii)=im+t1;
end
izthreshold,poscrac,posEstim
Note that the instruction izthreshold=izthreshold-K, which substracts
K from the detected positions, takes into account the K sample delay
caused by the causal implementation of the matched ﬁlter h2(n).

Hints and Solutions
229
0
50
100
150
200
250
300
350
400
450
500
−40
−20
0
20
40
0
50
100
150
200
250
300
350
400
450
500
−40
−20
0
20
40
0
50
100
150
200
250
300
350
400
450
500
−40
−20
0
20
40
Figure H5.3 – Signals obtained by declicking. The signal contains clicks that can
clearly be located on the residual
H5.8 (Restoring “missing values”) (see page 129)
1. Let K be the order of the AR model and N the sample size. We wish to
minimize the square deviation between the sequence of values x(n) and
the sequence of predicted values ˆx(n) = −a1x(n −1) −· · · −aKx(n −K),
for n from 1 to N, that is to say the quantity:
N−K
∑
u=1
(x(u) + a1x(u −1) + · · · + aKx(u −K))2
The minimization is done with respect to m unknown values, with indices
from ℓto ℓ+ m −1 (Figure H5.4).
These values are only involved in a limited number of terms of this sum
which are:
J =
ℓ+m+K−1
∑
u=ℓ
(x(u) + a1x(u −1) + · · · + aKx(n −K))2
Notice that J appears as the norm of the vector:
e = T x

230
Digital Signal and Image Processing using MATLAB®
+m−1
x(n)
{
Figure H5.4 – Several values are restored around the detected position
where T is an (m + K) × (m + 2K) Toeplitz matrix constructed from the
coeﬃcients (a1, . . ., aK):
T =


aK
aK−1
· · ·
a1
1
0
· · ·
· · ·
0
aK
aK−1
· · ·
a1
1
0
· · ·
...


and x is a size (m + 2K) vector deﬁned by:
x
=
[x(ℓ−K), · · · , x(ℓ−1)
|
{z
}
x0 : known
, x(ℓ), · · · , x(ℓ+ m −1)
|
{z
}
y : unknown
,
x(ℓ+ m), · · · , x(ℓ+ m + K −1)
|
{z
}
x1 : known
]T
The size k vectors x0 and x1 are comprised of values that are known.
The size m vector y is comprised of values we have to reconstruct.
Hence we can partition T in three matrices of the adequate size such that
e = A0x0 + A1x1 + By, which we can also write x = A0x0 + A1x1 =
−By + e. In the end:
x = −By + e
2. Minimizing the norm of e with respect to y involves the usual formulation
of a least squares problem. The solution is:
by = −(BT B)−1BT x = −(BT B)−1BT (A0x0 + A1x1)

Hints and Solutions
231
H6
Selected topics
H6.1 (Contour ellipse: the least squares method) (see page 143)
1. Type:
%===== detell.m
% run preprocesscoin before
function detell()
load yim02 %===== from preprocesscoin
yim=yim02(5:145,8:295);
[ligyim,colyim]=size(yim);
imagesc(yim), colormap('gray')
dd=abs(diff(yim)); vv=find(dd>00);
yy1=rem(vv,(ligyim-1)); xx1=fix((vv-yy1)/(ligyim-1));
%===== undersampling
xx1=xx1(1:15:end); yy1=yy1(1:15:end);
N=length(xx1); hold on, plot(xx1,yy1,'ob')
xy=[xx1';yy1']; % observations
GOBS=[xy(1,:).^2;xy(2,:).^2;xy(1,:).*xy(2,:);...
xy(1,:);xy(2,:)];
%===== coefficients
alphak= - GOBS' \ ones(N,1);
draw_ellipse(alphak,N); hold off
%===== drawing an ellipse
function X=draw_ellipse(alpha,N)
M=zeros(2,2);
M(1,1)=alpha(1); M(2,2)=alpha(2);
M(1,2)=alpha(3)/2; M(2,1)=alpha(3)/2;
center=-inv(M)*[alpha(4);alpha(5)]/2;
rho2=center'*M*center-1;
%=====
theta = (0:N-1) * (2*pi) ./ (N-1) ;
Y = sqrt(rho2)*[cos(theta);sin(theta)];
X = diag(center)*ones(2,N)+sqrtm(M)\Y;
hpl=plot(X(1,:),X(2,:),'y'); set(hpl,'linewidth',.5)
2. Theoretically the points of the ellipse obey the equation ax2
1 + bx2
2 +
cx1x2 + dx1 + ex2 −1 = 0.
Hence the idea of estimating the coef-
ﬁcients based on N pairs {x1(n), x2(n)} by determining the value of
θ = [a
b
c
d
e]T that minimizes:
(Xθ −u)T (Xθ −u)
where X is the N × 5 matrix constructed from the sequences x1(n) and
x2(n), and where u refers to the length N vector containing nothing but

232
Digital Signal and Image Processing using MATLAB®
the components 1. The solution is given by:
θ = X#u
Once θ has been estimated, we draw the ellipse using the equation (x −
x0)T E(x −x0) −γ = 0.
In order to do this, we have to determine
the expressions which lead from θ to the parameters x0, E and γ. By
expanding (x−x0)T E(x−x0)−γ = 0, we get e11x2
1 +e22x2
2 +2e12x1x2 −
2xT Ex0+xT
0 Ex0−γ = 0 (eij refers to the generating element of E where
E = ET ). If we identify this expansion as ax2
1 + bx2
2 + cx1x2 + dx1 +
ex2 −1 = 0, we ﬁrst have e11 = a, e22 = b and e12 = e21 = c/2. Then, for
any pair x = [x1
x2]T , −2xT Ex0 = dx1 + ex2 = xT [d
e]T , meaning
that:
x0 = −1
2E−1
[d
e
]
Finally, we have xT
0 Ex0 −γ = −1.
H6.2 (Contour ellipse: the covariance method) (see page 145)
1. Let U1 and V1 be the two components of y1. The mean vector E {y1}
has two components. The ﬁrst one is:
E {U1} =
∫
R2 u1pY (u1, u2)du1du2 = 1
π
∫
C
u1du1du2
If we assume u1 = ρ cos(θ), we get:
E {U1} = 1
π
∫2π
0
∫1
0
ρ cos(θ)ρdρdθ = 0
We also have to check that the second component E {V1} = 0. In the
end, E {y1} = 0.
The covariance matrix E
{
y1yT
1
}
requires the calculation of three quan-
tities E
{
U 2
1
}
, E
{
V 2
1
}
and E {U1V1}. If we assume u1 = ρ cos(θ), we
get:
E
{
U 2
1
}
=
∫
R2 u2
1pY (u1, u2)du1du2 = 1
π
∫2π
0
∫1
0
ρ2 cos2(θ)ρdρdθ = 1
4
Likewise, we have E
{
V 2
1
}
= 1/4 et E {U1V1} = 0. Therefore:
E
{
y1yT
1
}
= 1
4I2

Hints and Solutions
233
2. The law of large numbers states that when N tends to inﬁnity:
1
N
N
∑
n=1
(yn −ˆνN) (yn −ˆνN)T
a.s.
−→1
4I2
If we multiply on the left and on the right by M 1/2 and according to
(6.6) we infer that:
1
N
N
∑
n=1
(xn −ˆµN)(xn −ˆµN)T
a.s.
−→M/4
where ˆµN = N −1 ∑N
n=1 xn.
3. Type:
%===== rechellipsecovar.m
clear all, close all
load mcoin2
[nblign nbcol]=size(pixc); [yy1,xx1]=find(pixc<70);
%=====
points=[xx1 yy1]; NN=length(xx1);
mymean=mean(points);
points_centres=points-ones(NN,1)*mymean;
RR=points_centres'*points_centres/NN;
image(pixc); colormap('gray'), hold on
MM=4*RR; E=inv(MM); ellipse(mymean,E,1)
hold off
H6.3 (Optimization of a portfolio with two assets) (see page 156)
Preliminary remark: due to the fact that there are only two assets and that
x1 + x2 = 1, we can replace x2 with 1 −x1 in J and plot J as a function of
x1 ∈(0, 1). However, for pedagogical reasons, we propose to solve the problem
by using the KKT conditions. The Lagrangian is written as:
L (x, λ, µ)
=
1
2xT Qx + λT (x1 + x2 −1) + µ1(Re −m1x1 −m2x2)
−µ2x1 −µ3x2
1. To solve the problem, we need to look at the diﬀerent cases as a function
of the conditions of complementarity:
(a) If x1 > 0, x2 > 0 and xT m > Re, then µ1 = µ2 = µ3 = 0. The
Lagrangian is written:
L = 1
2xT Qx + λ(1 −xT u)

234
Digital Signal and Image Processing using MATLAB®
By canceling the derivative with respect to x, we ﬁnd
∂xL = Qx −λu = 0
This gives us
x = λQ−1u
If we write that xT u = 1, we ﬁnd
xo =
1
uT Q−1uQ−1u
on condition that both components are positive and that
mT xo =
1
uT Q−1umT Q−1u > Re
i.e. on condition that Re < B/A. In this case, the risk is given by
1/uT Q−1u and is therefore constant.
(b) x1 > 0, x2 > 0 and xT m = Re in this case µ2 = µ3 = 0. The
Lagrangian is written
L = 1
2xT Qx + λ(1 −xT u) + µ1(Re −xT m)
By canceling the derivative with respect to x, we ﬁnd
∂xL = Qx −λu −µ1m = 0
This gives
x = λQ−1u + µ1Q−1m
By writing that xT u = 1 and that xT m = Re, we obtain the two
equations with two unknowns:
{
λu1Q−1u + µ1uT Q−1m = 1
λmT Q−1u + µ1mT Q−1m = Re
⇔
{
Aλ + Bµ1 = 1
Bλ + Cµ1 = Re
From this, we deduce that







λ = C −BRe
AC −B2
µ1 = ARe −B
AC −B2
(7.9)
By substituting λ and µ1 into x, we obtain the solution on condition
that µ1 ≥0 – i.e. Re ≥B/A – and that x1 and x2 are positive.
(c) If x1 = 0, then x2 = 1: the risk is equal to √q2 and the return is
xT m = m2.
Similarly, if x2 = 0, then x1 = 1: the risk is equal to √q1 and the
return is xT m = m1.
The two solutions must be compared to the values found previously
as a function of the numerical values of Q, m an Re.

Hints and Solutions
235
Figure H6.1 represents the curve of optimal portfolios. The linear part of
the curves corresponds to the case Re < B/A where the constraint Re −
xT ma < 0 – i.e. non-active. Its abscissa value is given by 1/
√
uT Q−1u,
which is the minimum possible risk according to example 6.8. The circle
indicates the portfolio (0, 1) and the triangle the portfolio (1, 0). That
portfolio is not optimal because it is strictly below the curve.
0.
0
0.
0
Figure H6.1 – Market frontier for a market with 2 assets
2. Type:
%===== twoassets.m
clear all
q1
= 1;
q2
= 2;
rho
= 0.3;
Q
= [q1 rho*sqrt(q1*q2);rho*sqrt(q1*q2) q2];
m
= [0.1;0.2];
u
= ones(2,1);
A
= u'*(Q\u);
B
= m'*(Q\u);
C
= m'*(Q\m);
Rebound = B/A;
M
= [A B;B C];
nbvalRe = 20;
listRe
= linspace(0,max(m),nbvalRe);
sigmao
= zeros(nbvalRe,1);
xo
= zeros(2,nbvalRe);
for ir
= 1:nbvalRe
Re
= listRe(ir);
if Re < Rebound
xo(:,ir)
= (Q\u) /A;

236
Digital Signal and Image Processing using MATLAB®
sigmao(ir) = sqrt(xo(:,ir)'*Q*xo(:,ir));
else
gamma
= M\[1;Re];
lambda
= gamma(1);
mu
= gamma(2);
xo(:,ir)
= lambda*(Q\u) + mu*(Q\m);
sigmao(ir) = sqrt(xo(:,ir)'*Q*xo(:,ir));
end
end
xo0 = [0;1];
Re0 = m(2);
sigmao0 = sqrt(xo0'*Q*xo0);
xo1 = [1;0];
Re1 = m(1);
sigmao1 = sqrt(xo1'*Q*xo1);
plot(sigmao,listRe,'.-')
hold on
plot(sigmao0, Re0,'or','markerf','r','markers',10)
plot(sigmao1, Re1,'vg','markerf','g','markers',10)
hold off
grid on
H6.4 (Negative correlation) (see page 157) Type:
%========= negative correlation
clear all
q1
= 1;
q2
= 2;
nbrho
= 10;
listrho = linspace(-0.7,0.3,nbrho);
m
= [0.05;0.3];
u
= ones(2,1);
nbvalRe = 10;
listRe
= linspace(min(m),max(m),nbvalRe);
sigmao
= zeros(nbvalRe,1);
xo
= zeros(2,nbvalRe);
for irho
= 1:nbrho
rho = listrho(irho);
Q = [q1 rho*sqrt(q1*q2);rho*sqrt(q1*q2) q2];
A = u'*(Q\u);
B = m'*(Q\u);
C = m'*(Q\m);
Rebound = B/A;
M = [A B;B C];
for ir = 1:nbvalRe
Re = listRe(ir);
if Re < Rebound
xo(:,ir) = (Q\u) /A;
sigmao(ir) = sqrt(xo(:,ir)'*Q*xo(:,ir));

Hints and Solutions
237
else
gamma = M\[1;Re];
lambda = gamma(1);
mu = gamma(2);
xo(:,ir) = lambda * (Q\u) + mu * (Q\m);
sigmao(ir) = sqrt(xo(:,ir)'*Q*xo(:,ir));
end
end
plot(sigmao,listRe,'.-')
hold on
end
hold off
grid on
We have shown, in Figure H6.2, the Market boundaries for diﬀerent values
of ρ. We observe that, for a given level of risk (on the abscissa axis), the more
negative the correlation is, the better the return becomes.
The linear part of the curves corresponds to the case Re < B/A where the
constraint Re −xT ma < 0 – i.e. non-active. Its abscissa value is given by
1/
√
uT Q−1u, which is the minimum according to example 6.8.
0
0.5
1
1.5
2
0.05
0.1
0.15
0.2
0.25
0.3
Figure H6.2 – market boundaries for a market with 2 assets and for diﬀerent val-
ues of the correlation. From left to right, the correlation runs from −0.7 to  0.3. We
can see that for a given level of risk (on the abscissa axis), the more the correlation
ρ approaches −1, the higher the return
H6.5 (Optimization of a portfolio) (see page 162)
Type the program:
%==== optimFolio
clear all
Q = 1e-2*[
...
4 2 -1 -3 0 2 ; 2 7 0 1 4 -1;...
-1 0 20 7 -1 13 ; -3 1 7 10 -2 4;...
0 4 -1 -2 11 -7 ; 2 -1 13 4 -7 18];
ma
= [0.03;0.04;0.08;-0.02;0.06;0.08];
U
= ones(6,1);

238
Digital Signal and Image Processing using MATLAB®
alpha
= U'*(Q\U);
beta
= U'*(Q\ma);
[bmax, imax] = max(ma);
Rf
= 0.02;
xi
= zeros(6,1);
xi(imax) = 1;
J
= @(x,Q) x'*Q*x;
listRe
= linspace(Rf,max(ma),20);
nbRe
= length(listRe);
s2
= zeros(nbRe,1);
s2za
= zeros(nbRe,1);
xa
= zeros(6,nbRe);
za
= zeros(6,nbRe);
xf
= zeros(nbRe,1);
Rp
= zeros(nbRe,1);
A
= [U';(Rf*U-ma)'];
for ire
= 1:nbRe
Re
= listRe(ire);
B
= [1;(Rf-Re)];
[xa(:,ire), fval] =...
fmincon(@(x) J(x,Q), xi,A,B,[],[],zeros(6,1));
xf(ire) = 1-sum(xa(:,ire));
Rp(ire) = ma'*xa(:,ire)+xf(ire)*Rf;
s2(ire) = xa(:,ire)'*Q*xa(:,ire);
za(:,ire) = xa(:,ire)/sum(xa(:,ire));
s2za(ire) = za(:,ire)'*Q*za(:,ire);
end
%=====
figure(1), plot(sqrt(s2),Rp,'.-b')
hold on, plot(sqrt(s2za),Rp,'.-r'), hold off
H6.6 (Face recognition) (see page 174)
First, make a catalog of the data corresponding to the photographs in levels of
gray. In our example, the catalog called orlfaces contains the catalogs called
s1, s2, . . . , each one containing the 10 photographs that have to be processed.
Type the training program LDAPCAtraining.m. It is used by the test program
LDAPCAtest.m:
%===== LDAPCAtraining.m
XredPCA2D
= cell(1,nbindiv);
XredLDA
= cell(1,nbindiv);
V
= cell(nbindiv,1);
W
= cell(nbindiv,1);
for ii=1:nbindiv
grandX
= zeros(d,nbimages_L);
grandXcell = cell(nbimages_L,1);
for kimg=1:nbimages_L
pkimg = permutind(kimg);
filename = sprintf('%s/s%i/%i.png',ImageFile,ii,pkimg);

Hints and Solutions
239
imge
= imread(filename);
grandXcell{kimg} = imge;
end
[V{ii}, W{ii}]
= PCA2D(grandXcell,k1,k2);
XredPCA2D{ii}
= zeros(k1*k2,nbimages_L);
for kimg=1:nbimages_L
VTGW
= V{ii}'*double(grandXcell{kimg})*W{ii};
XredPCA2D{ii}(:,kimg) = reshape(VTGW,k1*k2,1);
end
end
gLDA
= LDA(XredPCA2D,dim_barycenter);
barycenter_nua
= zeros(dim_barycenter,nbindiv);
for ii=1:nbindiv
XredLDA{ii} = gLDA\XredPCA2D{ii};
barycenter_nua(:,ii)=XredLDA{ii}*ones(nbimages_L,1)/...
nbimages_L;
end
Type the following recognition program:
%==== LDAPCAtest.m
ImageFile
= '../orlfaces';
nbindiv
= 40;
% number of individuals
nbimages
= 10;
% number of photos
nbimages_L = 4;
% number of photos for training
d
= 112*92; % photo size
k1
= 7;
k2
= 7;
%===== dimension of the LDA less than k1*k2
dim_barycenter
= 20; cp=0; Lruns=10;
for irun=1:Lruns
%===== training
% random permutation over the NBIMAGES images
% to use different training databases.
% The remaining images will be used for testing
%=====
permutind
= randperm(nbimages);
LDAPCAtraining
%===== testing
matriceconf = zeros(nbindiv); aux = zeros(nbindiv,1);
for ii=1:nbindiv
for jj
= nbimages_L+1:nbimages
pjj = permutind(jj);
filename = sprintf([ImageFile '/s%i/%i.png'],ii,pjj);
grandXcell_T = double(imread(filename));
AA=double(V{ii})'*grandXcell_T*double(W{ii});
Xred_T=reshape(AA,k1*k2,1);

240
Digital Signal and Image Processing using MATLAB®
XredLDA_T=gLDA\Xred_T;
for kk=1:nbindiv
aux(kk)=norm(barycenter_nua(:,kk)-XredLDA_T,'fro');
end
[minaux,indaux] = min(aux);
matriceconf(ii,indaux) = matriceconf(ii,indaux)+1;
end
end
% idxsrt=sprintf('%i, ',sort(permutind(1:nbimages_L)));
% txtL = sprintf('Learning on images %s',idxsrt);
% figure(1)
% imagesc(matriceconf)
% title(txtL,'fontsize',16)
if any(diag(matriceconf)~=nbimages-nbimages_L),
disp('alarm');cp=cp+1;
end
end
cp/Lruns
H6.7 (GPS location) (see page 178)
Type the program:
%===== exeGPS.m
clear all
toler = 0.1;
c_mps = 299792458;
true_loc_m = [248645.5722;-4828261.0758;4146460.5047];
sat_loc_m =
1.0e+07 * ...
[-1.731978829800000
-1.875361540500000
0.728987051300000;...
-1.362041209000000
-0.813947623800000
2.149113287300000;...
0.809023521200000
-2.399402432700000
-0.752593941400000;...
1.929942650600000
-0.960140862800000
1.585822443500000;...
2.345122552599999
-0.672073953600000
1.090961123000000;...
1.165884015800000
-1.341861781800000
1.940808680800000];
pseudorange_s = [ ...
0.075522044080650;...
0.074908679917403;...
0.079304017358102;...
0.076286737020851;...
0.080875081503129;...
0.069734492372016];
L_m = c_mps*pseudorange_s;
%===== initialization
init_loc = zeros(3,1);
[estim_loc_m, ttestime, delta] = ...
GPSloc(L_m, init_loc, sat_loc_m, toler);
disterr = norm(true_loc_m-estim_loc_m);
format long, disterr, format
and the function:

Hints and Solutions
241
function [receiver_loc, tt, delta] = ...
GPSloc(distance_m, init_loc_m, sat_loc_m, toler)
%!==============================================!
%! distance_m : pseudo-range (s) * c (m/s)
!
%! init_loc_m : initial location algorithm
!
%! sat_loc_m
: satellite location (m) (I by 3) !
%! toler
: arret algorithme (typ. 0.1)
!
%!==============================================!
tt
= 0; % init value of the time shift tau
receiver_loc = init_loc_m;
c_mps = 299792458;
I = length(distance_m); % nb_sat
max_delta = 1; iter = 0; F = zeros(I,1);
J = zeros(I,4); delta = zeros(4,1000);
while max_delta>toler
iter = iter + 1;
mv=ones(I,1)*receiver_loc'-sat_loc_m;
dmat=sum(mv.^2,2);
F=sqrt(dmat)+c_mps*tt; % dist. rec-sat
J=[mv ./ (F*ones(1,3)),c_mps*ones(I,1)];
Df = distance_m-F;
delta(:,iter)= J \ Df;
receiver_loc = receiver_loc + delta(1:3,iter);
tt = tt + delta(4,iter);
max_delta = max(abs(delta(:,iter)));
end
delta=delta(:,1:iter);
return


Chapter 8
Appendix
A1
A few properties of a matrix
Eigendecomposition
We consider a N square matrix A of complex values. A non-zero vector v is
an eigenvector of A if and only if it exists a scalar λ such that:
Av = λv
⇔
(A −λI)v = 0
(8.1)
λ is called the eigenvalue associated to v. It is clear that if v ∈CN is an
eigenvector then, for any α ∈C, αv is an eigenvector. It would be better to
speak of eigendirection. The equation (8.4) leads to the eigenvalues equation:
P(λ) = det(A −λI) = 0
P(λ) is the characteristic polynomial. It has N roots, which are distinct
and/or multiple, and some of them may be zero. For each distinct root λi we
can solve with respect to v the equation:
(A −λiI)v = 0
The set of solutions associated to λi spans a subspace of CN. In the case
where the multiplicity order of all eigenvalue is equal to the dimension of the
associated subspace, the matrix is said diagonalizable and we have
A = P DP −1
where D is a diagonal matrix whose diagonal elements are the eigenvalues and
where P is invertible.
It is shown that if A = AH, A is diagonalizable. In particular if A is posi-
tive it is diagonalizable, its eigenvalues are positive or null and the eigenvectors

244
Digital Signal and Image Processing using MATLAB®
are orthogonal with
A
=
UDU H
(8.2)
=
[u1
. . .
uN
]


λ1
· · ·
0
...
...
...
0
· · ·
λN


[u1
. . .
uN
]H
where U is unitary, that is UU H = U HU = IN.
If we denote U =
[u1
. . .
uN
]
equation (8.2) can be rewritten
A = ∑N
i=1 λiuiuH
i
(8.3)
Generalized eigendecomposition
A non-zero vector v is an generalized eigenvector of the pair (A, B) if and only
if it exists a scalar λ such that
Av = λBv
⇔
(A −λB)v = 0
(8.4)
Similarly to the standard case where B = I, we can deﬁne subspaces asso-
ciated with the generalized eigenvalues solutions of P(λ) = det(A −λB) = 0.
If A and B are positive, it is shown that the set of the eigenvectors spans
the all space CN and we have
A = BUDU H
where D is a diagonal matrix whose diagonal elements are the eigenvalues and
where U is an unitary matrix.
Singular value decomposition
We consider a N × M matrix A of complex values. It can be shown that there
exists U, V and D such that:
N ≥M : A
=
UDV H
=
[u1
. . .
uN
]


σ1
· · ·
0
...
...
...
0
· · ·
σM
0
· · ·
0
...
...
...
0
· · ·
0


[v1
. . .
vM
]H
N ≤M : A
=
UDV H
=
[u1
. . .
uN
]


σ1
· · ·
0
0
· · ·
0
...
...
...
...
...
...
0
· · ·
σN
0
· · ·
0


[v1
. . .
vM
]H

Appendix
245
where U is an N square unitary matrix, V an M square unitary matrix and D
an N × M diagonal matrix such that the diagonal entry σi ≥0. The diagonal
elements σi are called the singular values. If r is the rank of A, only r singular
values are non-null.
It is shown that the squares of the singular values are equal to
– the N eigenvalues of AAH if N ≤M,
– the M eigenvalues of AHA if N ≥M.
If A has rank r and if we denote (u1, v1), . . ., (ur, vr) the r singular vector
pair associated to the non-null singular values σ1, . . ., σr, then
A =
r
∑
i=1
σiuivH
i
(8.5)
Pseudo-inverse
We consider a N × M matrix A of complex values. The pseudo inverse is the
unique M × N matrix A# such that the four following conditions are veriﬁed:
−
AA#A
=
A
−
A#AA#
=
A#
−
(A#A)∗
=
A#A
−
(AA#)∗
=
AA#
We have the following properties:
– if A is invertible, A# = A−1;
– if the rank of A is the number of rows N of A, then A# = AH(AAH)−1;
– if the rank of A is the number of columns M of A, then A# = (AHA)−1AH;
– if A = UDV H is the svd decomposition of A, then A# = UD#V H,
where D# is a diagonal matrix whose non-zero elements are the reciprocal
of the non-zero singular values.
Condition number
Let us denote respectively σmax and σmin the highest and the lowest singular
values. The ratio r = σmax/σmin is called the condition number. The higher the
condition number, the more diﬃcult the numerical calculation of the pseudo-
inverse of a given matrix is.

246
Digital Signal and Image Processing using MATLAB®
A2
A few relations for matrices
For more results see [30].
◦Let A nonsingular, and B, C and D may be rectangular:
(A + CBD)−1 = A−1 −A−1C
(
I + BDA−1C
)−1 BDA−1
(8.6)
◦Woodbury:
(
A + CBCT )−1
= A−1 −A−1C
(
B−1 + CT A−1C
)−1
CT A−1
(8.7)
◦Kailath:
(A + BC)−1 = A−1 −A−1B
(
I + CA−1B
)−1 CA−1
(8.8)
◦Bass-Gura:
det
[A
B
C
D
]
= det(A)det(D −CA−1B)
(8.9)
⇒det(In −AB) = det(Im −BA)
(8.10)
◦Searle’s identities:
(
I + A−1)−1 = A (A + I)−1
(8.11)
(
A + BBT )−1
B = A−1B
(
I + BT A−1B
)−1
(8.12)
(
A−1 + B−1)−1 = A (A + B)−1 B = B (A + B)−1 A
(8.13)
A−1 + B−1 = A−1 (A + B) B−1
(8.14)
(I + AB)−1 = I −A (I + BA)−1 B
(8.15)
(I + AB)−1 A = A (I + BA)−1
(8.16)

Bibliography
[1] G. Blanchet and M. Charbit. Digital Signal and Image Processing using
MATLAB, Volume 1: Fundamentals. ISTE, London and John Wiley &
Sons, New York, July 2014.
[2] R. Boite and H. Leich. Les Filtres Numériques. Masson, Collection CNET-
ENST, 1980.
[3] P. Brockwell and R. Davies. Time Series: Theory and Methods. Springer
Verlag, 1990.
[4] P. J. Burt and E. H. Adelson. The laplacian pyramid as a compact image
code. IEEE Trans. on communications, COM-31(4), April 1983.
[5] J. C. Butcher. Numerical Methods for Ordinary Diﬀerential Equations.
John Wiley & Sons, 2003.
[6] D. Cass and J. Stiglitz. “The structure of investor preferences and asset
returns, and separability in portfolio allocation.”
Journal of Economic
Theory, 2:122–160, 1970.
[7] P.L. Chebyshev. Questions on smallest quantities connected with the ap-
proximate representation of functions (1859). Collected works , 2 , Moscow-
Leningrad, pages 151–238, 1947.
[8] J.W. Cooley and J.W. Tuckey. “An Algorithm for the Machine Calculation
of Complex Fourier Series”. Math. of Comp., 19:297–301, April 1965.
[9] J.P. Delmas.
Eléments de Théorie du Signal:
Signaux Déterministes.
Collection Pédagogique de Télécommunication. Ellipses, 1995.
[10] P. Douglas and C. Cobb. A theory of production. American Economic
Review, 18, 1928.
[11] P. Elias. Coding for noisy channels. IRE Conv. Rec., 1955.

248
Digital Signal and Image Processing using MATLAB®
[12] F. Itakura. “Minimum Prediction Residual Principle Applied to Speech
Recognition”. IEEE Transactions on Acoustics Speech and Signal Process-
ing, AS23.1:67–72, 1975.
[13] O. Faugeras.
Three-Dimentional Computer Vision:
A Geometric Ap-
proach. MIT Press, 1996.
[14] O. Faugeras and Q-T Luong. The Geometry of Multiple Images. MIT
Press, 2001.
[15] J. L. Flanagan.
Speech Analysis, Synthesis, and Perception.
Springer
Verlag, New York, 1972.
[16] J. L. Flanagan and R. M. Golden. “Phase Vocoder”. Bell System Technical
Journal, pages 1493–1509, November 1966.
[17] G. Hacques. Mathématiques pour l’informatique, algorithmique numérique,
volume 1 of collection U, série “Informatique”. Armand Colin, 1971.
[18] R. Hartley and A. Zisserman. Multiple View in Computer Vision. Cam-
bridge University Press, 2000.
[19] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical
Learning, Data Mining, Inference, and Prediction. Springer, 2008.
[20] Texas Intruments. Tms320c64x dsp viterbi-decoder coprocessor (vcp) ref-
erence guide. SPRU533D reference manual, September 2004.
[21] M.R. Iseli and A. Alwan. “Inter- and Intra-speaker Variability of Glottal
Flow Derivative using the LF Model”. 6th International Conference on
Spoken Language Processing, pages 477–480, 2000.
[22] T. Kailath. Linear Systems. Prentice Hall, Englewood Cliﬀs, 1980.
[23] H. M. Markowitz. Portfolio selection. The Journal of Finance, 7(1):77–91,
March 1952.
[24] H. M. Markowitz. Portfolio Selection: Eﬃcient Diversiﬁcation of Invest-
ments. John Wiley & Sons, Inc., New York, 1959.
[25] B. Michalik.
A new interpolation formula in the form of a continued
fraction. Czechoslovak Journal of Physics, 33(7), 1983.
[26] N. Moreau. Techniques de Compression des Signaux. Masson, 1995.
[27] E. Moulines and F. Charpentier. “Pitch-synchronous waveform processing
techniques for text-to-speech synthesis using diphones”. Speech Commu-
nication, 5-6(9):453–467, December 1990.

Bibliography
249
[28] A. Oppenheimer and R. Shafer. Discrete-Time Signal Processing. Prentice
Hall, 1989.
[29] T. Parks and J. McClellan. Chebyshev approximation for nonrecursive
digital ﬁlters with linear phase. IEEE Transactions on Circuit Theory,
19(2):189–194, March 1972.
[30] K. B. Petersen and M. S. Pedersen. The Matrix Cookbook. Technical
University of Denmark, 2012.
[31] E. Remes. “Sur la détermination des polynômes d’approximation de degré
donné.” Commun. Soc. Math. Kharkov, 1934.
[32] E. Remes.
“Sur le calcul eﬀectif des polynômes d’approximation de
Tchebychef.” C. R. Acad. Sci., 199:337–340, 1934.
[33] E. Remes.
Sur un procédé convergent d’approximations successives
pour déterminer les polynômes d’approximation.
C. R. Acad. Sci.,
198:2063–2065, 1934.
[34] L. F. Richardson. The approximate arithmetical solution by ﬁnite diﬀer-
ences of physical problems including diﬀerential equations, with an appli-
cation to the stresses in a masonry dam, volume A 210. Philosophical
Transactions of the Royal Society, 1911.
[35] R. Tyrrel Rockafellar. “Lagrange multipliers and optimality.” SIAM Re-
view, 35(2):183–238, June 1993.
[36] W. Sweldens. “The lifting scheme: A construction of second generation
wavelets.” SIAM J. Math. Anal, 29, 1997.
[37] T. N. Thiele. Interpolationsrechnung. Leipzig, B.G. Teubner, 1909.
[38] R. Y. Tsai.
“A versatile camera calibration technique for 3d machine
vision.” IEEE J. Robotics and Automation, RA-3(4):323–344, 1987.
[39] P. P. Vaidyanathan. Multirate Systems and Filter Banks. Prentice Hall,
Englewood Cliﬀs, 1993.
[40] M. Vetterli and J. Kovˇcevi´c. Wavelets and Subband Coding. Prentice Hall,
Englewood Cliﬀs, 1995.
[41] A. J. Viterbi. Error bounds for convolutional codes and an asymptotically
optimum decoding algorithm. IEEE Transactions on Information Theory,
IT-13:260–269, April 1967.
[42] H. Werner. A reliable method for rational interpolation. In L. Wuytack,
editor, Padé Approximation and its Applications, volume 765 of Lecture
Notes in Mathematics. Springer, Berlin, Heidelberg, 1979.


Index
ADC, 2
algorithm
overlap-add, 28
overlap-save, 25
aliasing, 5
all-pole, 111
ambiguity, 7
analog-to-digital converter (ADC), 2
analysis (principal components), 164
anti-aliasing, 7
AR, 106, 126, 218, 229
autoregressive, 106, 109, 126, 218, 229
band
useful, 6
band-limited
signal, 2
bi-orthogonality, 39
CAL, 159
capital allocation line, 159
cardiac rhythm, 131
CD-audio, 34
cepstral coeﬃcients, 120
cepstrum, 120, 221
Cholevsky, 102
clicks, 125
code rate, 181
comb, 34
ﬁlter, 32
compensator, 74
complex gain, 15
constraint length, 181
continued fraction, 91
control points (spline), 96
convergence
DTFT, 7
conversion
analog-to-digital, 2
convolution, 15
circular, 23
linear, 23
critical frequency, 78
cubic spline, 96
Daubechies, 40
decibel, 8
decimation, 30
declicking, 125
DFT, 8, 9
digital processing, 1
direct form (ﬁlter), 16
discrete Fourier transform, 8
dot product, 8
DTFT, 4, 7
DTMF, 11
DTW, 116, 117, 221
dual tone multi-frequency, 11
dynamic time warping, 116
ECG, 131
eigenfaces, 167
EKG, 131
energy spectral density, 8
equation
state, 16
Yule-Walker, 110, 111, 125
esd, 8
extrinsic parameters (calibration), 56

252
Digital Signal and Image Processing using MATLAB®
factorization
Cholevsky, 102
fast Fourier transform, 8, 10
fetus (EKG), 131
FFT, 8
ﬁlter
(Daubechies), 40
all-pole, 111
bank, 29
deﬁnition, 15
perfect reconstruction, 3
FIR, 20
FIR ﬁlter, 15
formant, 109
Fourier
discrete-time transform, 4
frequency
fundamental, 138
fundamental (measuring the), 137
image, 7
frequency resolution, 9
fricatives, 106
function
BurtAdelsonInit, 63
calcak, 49
calcderiv, 211
calcrn, 87
CalcXY, 208
calibparam, 60
CHOfact, 216
compan, 20
Crepind, 202
dergauss, 144
derivestimc, 87
detectpitch, 217
dettridiag, 213
drawcs, 211
DTW1, 221
expm, 73
extractCEPSTRE, 223
f0cor, 139
filter, 18
filtic, 18
filtrer, 17
filtrerII, 192
filtric, 18
filtricII, 193
fsolve (non-linear equations), 90
fzero (non-linear equations), 90
KRdecomp, 61
LDA, 173
LPremes, 48
moygauss, 143
nbilin.m (continuous →discrete
time), 77
nbilin, 203
newimg.m, 51
normM, 66
ode45 (linear system simulation),
78
PCA2D, 169
phasevoc, 226
projconique.m, 196
psola, 224
recn.m, 207
reconstruction, 3, 5
rotate, 198
rotx.m, 197
roty.m, 197
rotz.m, 198
secondderiv, 210
selnewfr, 45
solve_lv2, 206
solve_myls.m, 79
spec2sig, 226
specinterp, 227
stft, 227
xtoa, 115
gain
ﬁlter, 15
Gauss-Seidel, 101
generating codes, 181
global positioning system (GPS), 176
glottal excitation, 108
GPS, 176

Index
253
heart, 131
IDFT, 9
IIR ﬁlter, 15
insertion, 31
instantaneous
power, 13
intrinsic parameters (calibration), 55
inverse DFT, 9
inverse diﬀerence (Thiele’s interpola-
tion), 93
Jacobi, 100
Karush-Kuhn-Tucker, 149
KKT, 149
Lagrange multipliers, 166
Lagrangian, 146
LDA, 170
least squares (calibration), 58
lifting scheme, 66
line spectrum pair, 220
linear discriminant analysis, 170
linear ﬁlter
anti-aliasing, 7
by FFT, 23
comb, 32, 34
implementation, 16
matched, 127
state, 16
linear prediction, 125
Lotka-Volterra, 80
lsp, 220
MAC operation, 20, 26
matched
linear ﬁlter, 127
matrix
companion, 20
confusion, 167, 175
mel frequency cepstral coeﬃcients, 120
method
Gauss-Seidel, 101
Jacobi, 100
Newton, 88
Newton-Raphson, 89
relaxation, 101
Richardson (extrapolation), 85
Runge-Kutta, 83
MFCC, 120
multifrequency, 29
multiplication-accumulation (MAC), 20,
23
Newton-Raphson, 89
noise
impulse, 125
quantization, 2
ode, 76
one-bit stream, 34
ordinary diﬀerential equation, 76
overlap, 25, 29
overlap-add, 27
overlap-save, 25
Parks-McClellan method, 43
Parseval
formula, 8
PCA, 164, 166, 169
PCM, 113
perfect reconstruction, 3
periodogram, 10
averaged, 10
smoothed, 10
perspective projection, 51
phase
vocoder, 123
phase vocoder, 121
phone dialing, 11
pinhole camera model, 54
pitch, 108
plosives, 106
Poisson
formula, 4
polynomial approximation, 43
polyphase, 21

254
Digital Signal and Image Processing using MATLAB®
polyphases components, 21
power spectral density, 10
predator-prey equation, 80
principal component analysis, 164, 166
processing (multifrequency), 29
program
C2altern.m, 201
Crepetats.m, 202
aa.m, 128
Ccraq.m, 230
Cetatscomp.m, 203
code.m, 218
correlnegative.m, 239
ctime2dtime.m (continuous →dis-
crete time), 76
data4exeGPS.m, 179
daub4.m, 41
decode.m, 220
detectkey.m, 189
detectnum.m, 191
detell.m, 233
digitBDDgene.m, 175
DTWtry.m, 224
dwtproc.m, 65
dwtprocf.m, 66
estimderiv.m, 87
exeigenfaces.m, 167
exeminQ3linineg.m, 152
exonewton.m, 208
explefzero.m, 90
explefzero2.m, 91
extrythm.m, 140
fenHann.m, 225
fil2blocs.m, 18
filtragefft1.m, 194
filtragefft2.m, 194
filtragefft3.m, 195
genekey.m, 12
LDAPCAtest.m, 242
LDAPCAtraining.m, 241
lv.m, 206
mcomb.m, 195
natcubicspline.m, 210
optimFolio.m, 240
PHcoder.m, 226
polyphase.m, 194
preprocesscoin.m, 142
PSOLAtry.m, 224
rechellipsecovar.m, 235
relaxtest.m, 215
repcren.m (RC ﬁlter), 79
residuAR.m, 111
restau.m, 130
rk2lv.m, 206
RK4.m, 84
separecg.m, 135
solve_lv.m, 81
spectralradius.m, 215
testbilin.m (continuous →dis-
crete time), 77
testheun.m, 205
testlv2.m, 206
testode05.m, 80
testremes.m, 47
testtridiag.m, 213
tstCalcXY.m, 209
tstproj3D.m (3D projection), 198
tstpyramide.m, 63
twoassets.m, 237
psd, 10
PSOLA, 121
pulse code modulation, 113
quality factor (Remez), 44
random
sequence, 10
signal, 10
random process
AR, 106, 126, 218, 229
autoregressive, 109
reconstruction (of a state), 18
reconstruction function, 3
reﬂection coeﬃcients, 220
relaxation, 101
Remes, 43
residual, 111

Index
255
residual signal (prediction), 126
resolution
frequency, 9
Runge-Kutta, 83
sampling
deterministic signal, 2
frequency, 2
period, 2
satellite positioning, 176
Sharpe’s ratio, 159
signal
digital, 2
low-pass, 6
periodic, 136
residual, 111
speech, 106
signal-to-noise ratio, 127
simulation, 74
SNR, 127
sound
unvoiced, 106
voiced, 106, 111
spectral radius, 100, 213
spectrum, 2, 8
aliasing, 3, 5
random signal, 10
speech
AR model, 107
model, 105, 106
typology of sounds, 106
spline
cubic, 96
spline curve, 96
state
of a ﬁlter, 16
reconstruction, 18
representations, 73
Thiele’s interpolation, 92
transfert function, 15
transform
discrete Fourier, 8, 9
vocal folds, 106
vocoder, 113
voice activity, 116
vowels, 106
waveform coding, 113
wavelet, 40
window
Hann, 123
Yule-Walker
equation, 110, 111, 125
zero-order hold, 74
zero-padding, 9
ZOH, 74


Other titles from  
 
in 
Digital Signal and Image Processing 
2014 
AUGER François 
Signal Processing with Free Software: Practical Experiments 
BLANCHET Gérard, CHARBIT Maurice 
Digital Signal and Image Processing using MATLAB 
Volume 1 – Fundamentals – 2nd edition 
FANET Hervé 
Medical Imaging Based on Magnetic Fields and Ultrasounds 
MOUKADEM Ali, OULD Abdeslam Djaffar, DIETERLEN Alain 
Time-Frequency Domain for Segmentation and Classification of Non-
stationary Signals: The Stockwell Transform Applied on Bio-signals and 
Electric Signals 
NDAGIJIMANA Fabien 
Signal Integrity: From High Speed to Radiofrequency Applications 
PINOLI Jean-Charles 
Mathematical Foundations of Image Processing and Analysis –  
Volumes 1 and 2 

TUPIN Florence, INGLADA Jordi, NICOLAS Jean-Marie 
Remote Sensing Imagery 
VLADEANU Calin, EL ASSAD Safwan 
Nonlinear Digital Encoders for Data Communications 
2013 
GOVAERT Gérard, NADIF Mohamed 
Co-Clustering 
DAROLLES Serge, DUVAUT Patrick, JAY Emmanuelle 
Multi-factor Models and Signal Processing Techniques: Application to 
Quantitative Finance 
LUCAS Laurent, LOSCOS Céline, REMION Yannick 
3D Video: From Capture to Diffusion 
MOREAU Eric, ADALI Tulay 
Blind Identification and Separation of Complex-valued Signals 
PERRIN Vincent 
MRI Techniques 
WAGNER Kevin, DOROSLOVACKI Milos 
Proportionate-type Normalized Least Mean Square Algorithms 
FERNANDEZ Christine, MACAIRE Ludovic, ROBERT-INACIO Frédérique 
Digital Color Imaging 
FERNANDEZ Christine, MACAIRE Ludovic, ROBERT-INACIO Frédérique 
Digital Color: Acquisition, Perception, Coding and Rendering 
NAIT-ALI Amine, FOURNIER Régis 
Signal and Image Processing for Biometrics 
OUAHABI Abdeljalil 
Signal and Image Multiresolution Analysis 

2011 
CASTANIÉ Francis 
Digital Spectral Analysis: Parametric, Non-parametric and Advanced 
Methods 
DESCOMBES Xavier 
Stochastic Geometry for Image Analysis 
FANET Hervé 
Photon-based Medical Imagery 
MOREAU Nicolas 
Tools for Signal Compression 
2010 
NAJMAN Laurent, TALBOT Hugues 
Mathematical Morphology 
2009 
BERTEIN Jean-Claude, CESCHI Roger 
Discrete Stochastic Processes and Optimal Filtering / 2nd edition 
CHANUSSOT Jocelyn et al. 
Multivariate Image Processing 
DHOME Michel 
Visual Perception through Video Imagery 
GOVAERT Gérard 
Data Analysis 
GRANGEAT Pierre 
Tomography 
MOHAMAD-DJAFARI Ali 
Inverse Problems in Vision and 3D Tomography 
SIARRY Patrick 
Optimisation in Signal and Image Processing 

2008 
ABRY Patrice et al.  
Scaling, Fractals and Wavelets 
GARELLO René 
Two-dimensional Signal Analysis 
HLAWATSCH Franz et al. 
Time-Frequency Analysis 
IDIER Jérôme 
Bayesian Approach to Inverse Problems 
MAITRE Henri 
Processing of Synthetic Aperture Radar (SAR) Images 
MAITRE Henri 
Image Processing 
NAIT-ALI Amine, CAVARO-MENARD Christine 
Compression of Biomedical Images and Signals 
NAJIM Mohamed 
Modeling, Estimation and Optimal Filtration in Signal Processing 
QUINQUIS André 
Digital Signal Processing Using Matlab 
2007 
BERTEIN Jean-Claude, CESCHI Roger 
Discrete Stochastic Processes and Optimal Filtering 
BLOCH Isabelle 
Information Fusion in Signal and Image Processing 
GLAVIEUX Alain 
Channel Coding in Communication Networks 
OPPENHEIM Georges et al. 
Wavelets and their Applications 

2006 
CASTANIÉ Francis 
Spectral Analysis 
NAJIM Mohamed 
Digital Filters Design for Signal and Image Processing 
 












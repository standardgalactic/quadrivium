ELEMENTARY
LINEAR ALGEBRA
K. R. MATTHEWS
DEPARTMENT OF MATHEMATICS
UNIVERSITY OF QUEENSLAND
Second Online Version, December 1998
Comments to the author at krm@maths.uq.edu.au

Contents
1
LINEAR EQUATIONS
1
1.1
Introduction to linear equations . . . . . . . . . . . . . . . . .
1
1.2
Solving linear equations . . . . . . . . . . . . . . . . . . . . .
6
1.3
The Gaussâ€“Jordan algorithm
. . . . . . . . . . . . . . . . . .
8
1.4
Systematic solution of linear systems.
. . . . . . . . . . . . .
9
1.5
Homogeneous systems . . . . . . . . . . . . . . . . . . . . . .
16
1.6
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2
MATRICES
23
2.1
Matrix arithmetic . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2
Linear transformations . . . . . . . . . . . . . . . . . . . . . .
27
2.3
Recurrence relations . . . . . . . . . . . . . . . . . . . . . . .
31
2.4
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.5
Nonâ€“singular matrices . . . . . . . . . . . . . . . . . . . . . .
36
2.6
Least squares solution of equations . . . . . . . . . . . . . . .
47
2.7
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3
SUBSPACES
55
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.2
Subspaces of F n
. . . . . . . . . . . . . . . . . . . . . . . . .
55
3.3
Linear dependence . . . . . . . . . . . . . . . . . . . . . . . .
58
3.4
Basis of a subspace . . . . . . . . . . . . . . . . . . . . . . . .
61
3.5
Rank and nullity of a matrix
. . . . . . . . . . . . . . . . . .
64
3.6
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4
DETERMINANTS
71
4.1
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . .
85
i

5
COMPLEX NUMBERS
89
5.1
Constructing the complex numbers . . . . . . . . . . . . . . .
89
5.2
Calculating with complex numbers . . . . . . . . . . . . . . .
91
5.3
Geometric representation of C . . . . . . . . . . . . . . . . . .
95
5.4
Complex conjugate . . . . . . . . . . . . . . . . . . . . . . . .
96
5.5
Modulus of a complex number
. . . . . . . . . . . . . . . . .
99
5.6
Argument of a complex number . . . . . . . . . . . . . . . . . 103
5.7
De Moivreâ€™s theorem . . . . . . . . . . . . . . . . . . . . . . . 107
5.8
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6
EIGENVALUES AND EIGENVECTORS
115
6.1
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.2
Deï¬nitions and examples . . . . . . . . . . . . . . . . . . . . . 118
6.3
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7
Identifying second degree equations
129
7.1
The eigenvalue method . . . . . . . . . . . . . . . . . . . . . . 129
7.2
A classiï¬cation algorithm
. . . . . . . . . . . . . . . . . . . . 141
7.3
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . . 147
8
THREEâ€“DIMENSIONAL GEOMETRY
149
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
8.2
Threeâ€“dimensional space . . . . . . . . . . . . . . . . . . . . . 154
8.3
Dot product . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
8.4
Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
8.5
The angle between two vectors
. . . . . . . . . . . . . . . . . 166
8.6
The crossâ€“product of two vectors . . . . . . . . . . . . . . . . 172
8.7
Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
8.8
PROBLEMS
. . . . . . . . . . . . . . . . . . . . . . . . . . . 185
9
FURTHER READING
189
ii

List of Figures
1.1
Gaussâ€“Jordan algorithm . . . . . . . . . . . . . . . . . . . . .
10
2.1
Reï¬‚ection in a line . . . . . . . . . . . . . . . . . . . . . . . .
29
2.2
Projection on a line
. . . . . . . . . . . . . . . . . . . . . . .
30
4.1
Area of triangle OPQ. . . . . . . . . . . . . . . . . . . . . . .
72
5.1
Complex addition and subtraction
. . . . . . . . . . . . . . .
96
5.2
Complex conjugate . . . . . . . . . . . . . . . . . . . . . . . .
97
5.3
Modulus of a complex number
. . . . . . . . . . . . . . . . .
99
5.4
Apollonius circles . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.5
Argument of a complex number . . . . . . . . . . . . . . . . . 104
5.6
Argument examples
. . . . . . . . . . . . . . . . . . . . . . . 105
5.7
The nth roots of unity. . . . . . . . . . . . . . . . . . . . . . . 108
5.8
The roots of zn = a. . . . . . . . . . . . . . . . . . . . . . . . 109
6.1
Rotating the axes . . . . . . . . . . . . . . . . . . . . . . . . . 116
7.1
An ellipse example . . . . . . . . . . . . . . . . . . . . . . . . 135
7.2
ellipse: standard form
. . . . . . . . . . . . . . . . . . . . . . 137
7.3
hyperbola: standard forms . . . . . . . . . . . . . . . . . . . . 138
7.4
parabola: standard forms (i) and (ii) . . . . . . . . . . . . . . 138
7.5
parabola: standard forms (iii) and (iv) . . . . . . . . . . . . . 139
7.6
1st parabola example . . . . . . . . . . . . . . . . . . . . . . . 140
7.7
2nd parabola example . . . . . . . . . . . . . . . . . . . . . . 141
8.1
Equality and addition of vectors
. . . . . . . . . . . . . . . . 150
8.2
Scalar multiplication of vectors. . . . . . . . . . . . . . . . . . 151
8.3
Representation of threeâ€“dimensional space . . . . . . . . . . . 155
8.4
The vector
-
AB. . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.5
The negative of a vector. . . . . . . . . . . . . . . . . . . . . . 157
iii

1
8.6
(a) Equality of vectors; (b) Addition and subtraction of vectors.157
8.7
Position vector as a linear combination of i, j and k. . . . . . 158
8.8
Representation of a line. . . . . . . . . . . . . . . . . . . . . . 162
8.9
The line AB. . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
8.10 The cosine rule for a triangle. . . . . . . . . . . . . . . . . . . 167
8.11 Pythagorasâ€™ theorem for a rightâ€“angled triangle.
. . . . . . . 168
8.12 Distance from a point to a line. . . . . . . . . . . . . . . . . . 169
8.13 Projecting a segment onto a line. . . . . . . . . . . . . . . . . 171
8.14 The vector crossâ€“product. . . . . . . . . . . . . . . . . . . . . 174
8.15 Vector equation for the plane ABC.
. . . . . . . . . . . . . . 177
8.16 Normal equation of the plane ABC.
. . . . . . . . . . . . . . 178
8.17 The plane ax + by + cz = d. . . . . . . . . . . . . . . . . . . . 179
8.18 Line of intersection of two planes. . . . . . . . . . . . . . . . . 182
8.19 Distance from a point to the plane ax + by + cz = d. . . . . . 184

2

Chapter 1
LINEAR EQUATIONS
1.1
Introduction to linear equations
A linear equation in n unknowns x1, x2, Â· Â· Â· , xn is an equation of the form
a1x1 + a2x2 + Â· Â· Â· + anxn = b,
where a1, a2, . . . , an, b are given real numbers.
For example, with x and y instead of x1 and x2, the linear equation
2x + 3y = 6 describes the line passing through the points (3, 0) and (0, 2).
Similarly, with x, y and z instead of x1, x2 and x3, the linear equa-
tion 2x + 3y + 4z = 12 describes the plane passing through the points
(6, 0, 0), (0, 4, 0), (0, 0, 3).
A system of m linear equations in n unknowns x1, x2, Â· Â· Â· , xn is a family
of linear equations
a11x1 + a12x2 + Â· Â· Â· + a1nxn
=
b1
a21x1 + a22x2 + Â· Â· Â· + a2nxn
=
b2
...
am1x1 + am2x2 + Â· Â· Â· + amnxn
=
bm.
We wish to determine if such a system has a solution, that is to ï¬nd
out if there exist numbers x1, x2, Â· Â· Â· , xn which satisfy each of the equations
simultaneously. We say that the system is consistent if it has a solution.
Otherwise the system is called inconsistent.
1

2
CHAPTER 1. LINEAR EQUATIONS
Note that the above system can be written concisely as
n
X
j=1
aijxj = bi,
i = 1, 2, Â· Â· Â· , m.
The matrix
ï£®
ï£¯ï£¯ï£¯ï£°
a11
a12
Â· Â· Â·
a1n
a21
a22
Â· Â· Â·
a2n
...
...
am1
am2
Â· Â· Â·
amn
ï£¹
ï£ºï£ºï£ºï£»
is called the coeï¬ƒcient matrix of the system, while the matrix
ï£®
ï£¯ï£¯ï£¯ï£°
a11
a12
Â· Â· Â·
a1n
b1
a21
a22
Â· Â· Â·
a2n
b2
...
...
...
am1
am2
Â· Â· Â·
amn
bm
ï£¹
ï£ºï£ºï£ºï£»
is called the augmented matrix of the system.
Geometrically, solving a system of linear equations in two (or three)
unknowns is equivalent to determining whether or not a family of lines (or
planes) has a common point of intersection.
EXAMPLE 1.1.1 Solve the equation
2x + 3y = 6.
Solution.
The equation 2x + 3y = 6 is equivalent to 2x = 6 âˆ’3y or
x = 3 âˆ’3
2y, where y is arbitrary. So there are inï¬nitely many solutions.
EXAMPLE 1.1.2 Solve the system
x + y + z
=
1
x âˆ’y + z
=
0.
Solution. We subtract the second equation from the ï¬rst, to get 2y = 1
and y = 1
2. Then x = y âˆ’z = 1
2 âˆ’z, where z is arbitrary. Again there are
inï¬nitely many solutions.
EXAMPLE 1.1.3 Find a polynomial of the form y = a0+a1x+a2x2+a3x3
which passes through the points (âˆ’3, âˆ’2), (âˆ’1, 2), (1, 5), (2, 1).

1.1. INTRODUCTION TO LINEAR EQUATIONS
3
Solution. When x has the values âˆ’3, âˆ’1, 1, 2, then y takes corresponding
values âˆ’2, 2, 5, 1 and we get four equations in the unknowns a0, a1, a2, a3:
a0 âˆ’3a1 + 9a2 âˆ’27a3
=
âˆ’2
a0 âˆ’a1 + a2 âˆ’a3
=
2
a0 + a1 + a2 + a3
=
5
a0 + 2a1 + 4a2 + 8a3
=
1.
This system has the unique solution a0 = 93/20, a1 = 221/120, a2 =
âˆ’23/20,
a3 = âˆ’41/120. So the required polynomial is
y
=
93
20 + 221
120x âˆ’23
20x2 âˆ’41
120x3.
In [26, pages 33â€“35] there are examples of systems of linear equations
which arise from simple electrical networks using Kirchhoï¬€â€™s laws for elec-
trical circuits.
Solving a system consisting of a single linear equation is easy. However if
we are dealing with two or more equations, it is desirable to have a systematic
method of determining if the system is consistent and to ï¬nd all solutions.
Instead of restricting ourselves to linear equations with rational or real
coeï¬ƒcients, our theory goes over to the more general case where the coef-
ï¬cients belong to an arbitrary ï¬eld. A ï¬eld F is a set F which possesses
operations of addition and multiplication which satisfy the familiar rules of
rational arithmetic. There are ten basic properties that a ï¬eld must have:
THE FIELD AXIOMS.
1. (a + b) + c = a + (b + c) for all a, b, c in F;
2. (ab)c = a(bc) for all a, b, c in F;
3. a + b = b + a for all a, b in F;
4. ab = ba for all a, b in F;
5. there exists an element 0 in F such that 0 + a = a for all a in F;
6. there exists an element 1 in F such that 1a = a for all a in F;

4
CHAPTER 1. LINEAR EQUATIONS
7. to every a in F, there corresponds an additive inverse âˆ’a in F, satis-
fying
a + (âˆ’a) = 0;
8. to every nonâ€“zero a in F, there corresponds a multiplicative inverse
aâˆ’1 in F, satisfying
aaâˆ’1 = 1;
9. a(b + c) = ab + ac for all a, b, c in F;
10. 0 Ì¸= 1.
With standard deï¬nitions such as a âˆ’b = a + (âˆ’b) and a
b = abâˆ’1 for
b Ì¸= 0, we have the following familiar rules:
âˆ’(a + b)
=
(âˆ’a) + (âˆ’b),
(ab)âˆ’1 = aâˆ’1bâˆ’1;
âˆ’(âˆ’a)
=
a,
(aâˆ’1)âˆ’1 = a;
âˆ’(a âˆ’b)
=
b âˆ’a,
(a
b )âˆ’1 = b
a;
a
b + c
d
=
ad + bc
bd
;
a
b
c
d
=
ac
bd;
ab
ac
=
b
c,
a
Â¡ b
c
Â¢ = ac
b ;
âˆ’(ab)
=
(âˆ’a)b = a(âˆ’b);
âˆ’
Â³a
b
Â´
=
âˆ’a
b
= a
âˆ’b;
0a
=
0;
(âˆ’a)âˆ’1
=
âˆ’(aâˆ’1).
Fields which have only ï¬nitely many elements are of great interest in
many parts of mathematics and its applications, for example to coding the-
ory. It is easy to construct ï¬elds containing exactly p elements, where p is
a prime number. First we must explain the idea of modular addition and
modular multiplication. If a is an integer, we deï¬ne a (mod p) to be the
least remainder on dividing a by p: That is, if a = bp + r, where b and r are
integers and 0 â‰¤r < p, then a (mod p) = r.
For example, âˆ’1 (mod 2) = 1, 3 (mod 3) = 0, 5 (mod 3) = 2.

1.1. INTRODUCTION TO LINEAR EQUATIONS
5
Then addition and multiplication mod p are deï¬ned by
a âŠ•b
=
(a + b) (mod p)
a âŠ—b
=
(ab) (mod p).
For example, with p = 7, we have 3 âŠ•4 = 7 (mod 7) = 0 and 3 âŠ—5 =
15 (mod 7) = 1. Here are the complete addition and multiplication tables
mod 7:
âŠ•
0
1
2
3
4
5
6
0
0
1
2
3
4
5
6
1
1
2
3
4
5
6
0
2
2
3
4
5
6
0
1
3
3
4
5
6
0
1
2
4
4
5
6
0
1
2
3
5
5
6
0
1
2
3
4
6
6
0
1
2
3
4
5
âŠ—
0
1
2
3
4
5
6
0
0
0
0
0
0
0
0
1
0
1
2
3
4
5
6
2
0
2
4
6
1
3
5
3
0
3
6
2
5
1
4
4
0
4
1
5
2
6
3
5
0
5
3
1
6
4
2
6
0
6
5
4
3
2
1
If we now let Zp = {0, 1, . . . , p âˆ’1}, then it can be proved that Zp forms
a ï¬eld under the operations of modular addition and multiplication mod p.
For example, the additive inverse of 3 in Z7 is 4, so we write âˆ’3 = 4 when
calculating in Z7. Also the multiplicative inverse of 3 in Z7 is 5 , so we write
3âˆ’1 = 5 when calculating in Z7.
In practice, we write aâŠ•b and aâŠ—b as a+b and ab or aÃ—b when dealing
with linear equations over Zp.
The simplest ï¬eld is Z2, which consists of two elements 0, 1 with addition
satisfying 1+1 = 0. So in Z2, âˆ’1 = 1 and the arithmetic involved in solving
equations over Z2 is very simple.
EXAMPLE 1.1.4 Solve the following system over Z2:
x + y + z
=
0
x + z
=
1.
Solution. We add the ï¬rst equation to the second to get y = 1. Then x =
1 âˆ’z = 1 + z, with z arbitrary. Hence the solutions are (x, y, z) = (1, 1, 0)
and (0, 1, 1).
We use Q and R to denote the ï¬elds of rational and real numbers, re-
spectively. Unless otherwise stated, the ï¬eld used will be Q.

6
CHAPTER 1. LINEAR EQUATIONS
1.2
Solving linear equations
We show how to solve any system of linear equations over an arbitrary ï¬eld,
using the GAUSSâ€“JORDAN algorithm. We ï¬rst need to deï¬ne some terms.
DEFINITION 1.2.1 (Rowâ€“echelon form) A matrix is in rowâ€“echelon
form if
(i) all zero rows (if any) are at the bottom of the matrix and
(ii) if two successive rows are nonâ€“zero, the second row starts with more
zeros than the ï¬rst (moving from left to right).
For example, the matrix
ï£®
ï£¯ï£¯ï£°
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
ï£¹
ï£ºï£ºï£»
is in rowâ€“echelon form, whereas the matrix
ï£®
ï£¯ï£¯ï£°
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
ï£¹
ï£ºï£ºï£»
is not in rowâ€“echelon form.
The zero matrix of any size is always in rowâ€“echelon form.
DEFINITION 1.2.2 (Reduced rowâ€“echelon form) A matrix is in re-
duced rowâ€“echelon form if
1. it is in rowâ€“echelon form,
2. the leading (leftmost nonâ€“zero) entry in each nonâ€“zero row is 1,
3. all other elements of the column in which the leading entry 1 occurs
are zeros.
For example the matrices
Â· 1
0
0
1
Â¸
and
ï£®
ï£¯ï£¯ï£°
0
1
2
0
0
2
0
0
0
1
0
3
0
0
0
0
1
4
0
0
0
0
0
0
ï£¹
ï£ºï£ºï£»

1.2. SOLVING LINEAR EQUATIONS
7
are in reduced rowâ€“echelon form, whereas the matrices
ï£®
ï£°
1
0
0
0
1
0
0
0
2
ï£¹
ï£»
and
ï£®
ï£°
1
2
0
0
1
0
0
0
0
ï£¹
ï£»
are not in reduced rowâ€“echelon form, but are in rowâ€“echelon form.
The zero matrix of any size is always in reduced rowâ€“echelon form.
Notation. If a matrix is in reduced rowâ€“echelon form, it is useful to denote
the column numbers in which the leading entries 1 occur, by c1, c2, . . . , cr,
with the remaining column numbers being denoted by cr+1, . . . , cn, where
r is the number of nonâ€“zero rows. For example, in the 4 Ã— 6 matrix above,
we have r = 3, c1 = 2, c2 = 4, c3 = 5, c4 = 1, c5 = 3, c6 = 6.
The following operations are the ones used on systems of linear equations
and do not change the solutions.
DEFINITION 1.2.3 (Elementary row operations) There are three
types of elementary row operations that can be performed on matrices:
1. Interchanging two rows:
Ri â†”Rj interchanges rows i and j.
2. Multiplying a row by a nonâ€“zero scalar:
Ri â†’tRi multiplies row i by the nonâ€“zero scalar t.
3. Adding a multiple of one row to another row:
Rj â†’Rj + tRi adds t times row i to row j.
DEFINITION 1.2.4 [Row equivalence]Matrix A is rowâ€“equivalent to ma-
trix B if B is obtained from A by a sequence of elementary row operations.
EXAMPLE 1.2.1 Working from left to right,
A =
ï£®
ï£°
1
2
0
2
1
1
1
âˆ’1
2
ï£¹
ï£»
R2 â†’R2 + 2R3
ï£®
ï£°
1
2
0
4
âˆ’1
5
1
âˆ’1
2
ï£¹
ï£»
R2 â†”R3
ï£®
ï£°
1
2
0
1
âˆ’1
2
4
âˆ’1
5
ï£¹
ï£»
R1 â†’2R1
ï£®
ï£°
2
4
0
1
âˆ’1
2
4
âˆ’1
5
ï£¹
ï£»= B.

8
CHAPTER 1. LINEAR EQUATIONS
Thus A is rowâ€“equivalent to B. Clearly B is also rowâ€“equivalent to A, by
performing the inverse rowâ€“operations R1 â†’1
2R1, R2 â†”R3, R2 â†’R2âˆ’2R3
on B.
It is not diï¬ƒcult to prove that if A and B are rowâ€“equivalent augmented
matrices of two systems of linear equations, then the two systems have the
same solution sets â€“ a solution of the one system is a solution of the other.
For example the systems whose augmented matrices are A and B in the
above example are respectively
ï£±
ï£²
ï£³
x + 2y
=
0
2x + y
=
1
x âˆ’y
=
2
and
ï£±
ï£²
ï£³
2x + 4y
=
0
x âˆ’y
=
2
4x âˆ’y
=
5
and these systems have precisely the same solutions.
1.3
The Gaussâ€“Jordan algorithm
We now describe the GAUSSâ€“JORDAN ALGORITHM. This is a process
which starts with a given matrix A and produces a matrix B in reduced rowâ€“
echelon form, which is rowâ€“equivalent to A. If A is the augmented matrix
of a system of linear equations, then B will be a much simpler matrix than
A from which the consistency or inconsistency of the corresponding system
is immediately apparent and in fact the complete solution of the system can
be read oï¬€.
STEP 1.
Find the ï¬rst nonâ€“zero column moving from left to right, (column c1)
and select a nonâ€“zero entry from this column. By interchanging rows, if
necessary, ensure that the ï¬rst entry in this column is nonâ€“zero. Multiply
row 1 by the multiplicative inverse of a1c1 thereby converting a1c1 to 1. For
each nonâ€“zero element aic1, i > 1, (if any) in column c1, add âˆ’aic1 times
row 1 to row i, thereby ensuring that all elements in column c1, apart from
the ï¬rst, are zero.
STEP 2. If the matrix obtained at Step 1 has its 2nd, . . . , mth rows all
zero, the matrix is in reduced rowâ€“echelon form. Otherwise suppose that
the ï¬rst column which has a nonâ€“zero element in the rows below the ï¬rst is
column c2. Then c1 < c2. By interchanging rows below the ï¬rst, if necessary,
ensure that a2c2 is nonâ€“zero. Then convert a2c2 to 1 and by adding suitable
multiples of row 2 to the remaing rows, where necessary, ensure that all
remaining elements in column c2 are zero.

1.4. SYSTEMATIC SOLUTION OF LINEAR SYSTEMS.
9
The process is repeated and will eventually stop after r steps, either
because we run out of rows, or because we run out of nonâ€“zero columns. In
general, the ï¬nal matrix will be in reduced rowâ€“echelon form and will have
r nonâ€“zero rows, with leading entries 1 in columns c1, . . . , cr, respectively.
EXAMPLE 1.3.1
ï£®
ï£°
0
0
4
0
2
2
âˆ’2
5
5
5
âˆ’1
5
ï£¹
ï£»
R1 â†”R2
ï£®
ï£°
2
2
âˆ’2
5
0
0
4
0
5
5
âˆ’1
5
ï£¹
ï£»
R1 â†’1
2R1
ï£®
ï£°
1
1
âˆ’1
5
2
0
0
4
0
5
5
âˆ’1
5
ï£¹
ï£»
R3 â†’R3 âˆ’5R1
ï£®
ï£°
1
1
âˆ’1
5
2
0
0
4
0
0
0
4
âˆ’15
2
ï£¹
ï£»
R2 â†’1
4R2
ï£®
ï£°
1
1
âˆ’1
5
2
0
0
1
0
0
0
4
âˆ’15
2
ï£¹
ï£»
Â½ R1 â†’R1 + R2
R3 â†’R3 âˆ’4R2
ï£®
ï£°
1
1
0
5
2
0
0
1
0
0
0
0
âˆ’15
2
ï£¹
ï£»
R3 â†’âˆ’2
15 R3
ï£®
ï£°
1
1
0
5
2
0
0
1
0
0
0
0
1
ï£¹
ï£»
R1 â†’R1 âˆ’5
2R3
ï£®
ï£°
1
1
0
0
0
0
1
0
0
0
0
1
ï£¹
ï£»
The last matrix is in reduced rowâ€“echelon form.
REMARK 1.3.1 It is possible to show that a given matrix over an ar-
bitrary ï¬eld is rowâ€“equivalent to precisely one matrix which is in reduced
rowâ€“echelon form.
A ï¬‚owâ€“chart for the Gaussâ€“Jordan algorithm, based on [1, page 83] is pre-
sented in ï¬gure 1.1 below.
1.4
Systematic solution of linear systems.
Suppose a system of m linear equations in n unknowns x1, Â· Â· Â· , xn has aug-
mented matrix A and that A is rowâ€“equivalent to a matrix B which is in
reduced rowâ€“echelon form, via the Gaussâ€“Jordan algorithm. Then A and B
are m Ã— (n + 1). Suppose that B has r nonâ€“zero rows and that the leading
entry 1 in row i occurs in column number ci, for 1 â‰¤i â‰¤r. Then
1 â‰¤c1 < c2 < Â· Â· Â· , < cr â‰¤n + 1.

10
CHAPTER 1. LINEAR EQUATIONS
START
?
Input A, m, n
?
i = 1, j = 1
?
-

?
Are the elements in the
jth column on and below
the ith row all zero?
j = j + 1
@
@
@
@@
R Yes
No
?
Is j = n?
Yes
No
-
6
Let apj be the ï¬rst nonâ€“zero
element in column j on or
below the ith row
?
Is p = i?
Yes
?
PPPPP
q No
Interchange the
pth and ith rows
Â©
Â©
Â©
Â©
Â©
Â©
Â©

Divide the ith row by aij
?
Subtract aqj times the ith
row from the qth row for
for q = 1, . . . , m (q Ì¸= i)
?
Set ci = j
?
Is i = m?



+
Is j = n?

i = i + 1
j = j + 1
6
No
No
Yes
Yes -
-
6
?
Print A,
c1, . . . , ci
?
STOP
Figure 1.1: Gaussâ€“Jordan algorithm.

1.4. SYSTEMATIC SOLUTION OF LINEAR SYSTEMS.
11
Also assume that the remaining column numbers are cr+1, Â· Â· Â· , cn+1, where
1 â‰¤cr+1 < cr+2 < Â· Â· Â· < cn â‰¤n + 1.
Case 1: cr = n + 1. The system is inconsistent. For the last nonâ€“zero
row of B is [0, 0, Â· Â· Â· , 1] and the corresponding equation is
0x1 + 0x2 + Â· Â· Â· + 0xn = 1,
which has no solutions. Consequently the original system has no solutions.
Case 2: cr â‰¤n. The system of equations corresponding to the nonâ€“zero
rows of B is consistent. First notice that r â‰¤n here.
If r = n, then c1 = 1, c2 = 2, Â· Â· Â· , cn = n and
B =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
1
0
Â· Â· Â·
0
d1
0
1
Â· Â· Â·
0
d2
...
...
0
0
Â· Â· Â·
1
dn
0
0
Â· Â· Â·
0
0
...
...
0
0
Â· Â· Â·
0
0
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
.
There is a unique solution x1 = d1, x2 = d2, Â· Â· Â· , xn = dn.
If r < n, there will be more than one solution (inï¬nitely many if the
ï¬eld is inï¬nite).
For all solutions are obtained by taking the unknowns
xc1, Â· Â· Â· , xcr as dependent unknowns and using the r equations correspond-
ing to the nonâ€“zero rows of B to express these unknowns in terms of the
remaining independent unknowns xcr+1, . . . , xcn, which can take on arbi-
trary values:
xc1
=
b1 n+1 âˆ’b1cr+1xcr+1 âˆ’Â· Â· Â· âˆ’b1cnxcn
...
xcr
=
br n+1 âˆ’brcr+1xcr+1 âˆ’Â· Â· Â· âˆ’brcnxcn.
In particular, taking xcr+1 = 0, . . . , xcnâˆ’1 = 0 and xcn = 0, 1 respectively,
produces at least two solutions.
EXAMPLE 1.4.1 Solve the system
x + y
=
0
x âˆ’y
=
1
4x + 2y
=
1.

12
CHAPTER 1. LINEAR EQUATIONS
Solution. The augmented matrix of the system is
A =
ï£®
ï£°
1
1
0
1
âˆ’1
1
4
2
1
ï£¹
ï£»
which is row equivalent to
B =
ï£®
ï£°
1
0
1
2
0
1
âˆ’1
2
0
0
0
ï£¹
ï£».
We read oï¬€the unique solution x = 1
2, y = âˆ’1
2.
(Here n = 2, r = 2, c1 = 1, c2 = 2. Also cr = c2 = 2 < 3 = n + 1 and
r = n.)
EXAMPLE 1.4.2 Solve the system
2x1 + 2x2 âˆ’2x3
=
5
7x1 + 7x2 + x3
=
10
5x1 + 5x2 âˆ’x3
=
5.
Solution. The augmented matrix is
A =
ï£®
ï£°
2
2
âˆ’2
5
7
7
1
10
5
5
âˆ’1
5
ï£¹
ï£»
which is row equivalent to
B =
ï£®
ï£°
1
1
0
0
0
0
1
0
0
0
0
1
ï£¹
ï£».
We read oï¬€inconsistency for the original system.
(Here n = 3, r = 3, c1 = 1, c2 = 3. Also cr = c3 = 4 = n + 1.)
EXAMPLE 1.4.3 Solve the system
x1 âˆ’x2 + x3
=
1
x1 + x2 âˆ’x3
=
2.

1.4. SYSTEMATIC SOLUTION OF LINEAR SYSTEMS.
13
Solution. The augmented matrix is
A =
Â· 1
âˆ’1
1
1
1
1
âˆ’1
2
Â¸
which is row equivalent to
B =
Â· 1
0
0
3
2
0
1
âˆ’1
1
2
Â¸
.
The complete solution is x1 = 3
2, x2 = 1
2 + x3, with x3 arbitrary.
(Here n = 3, r = 2, c1 = 1, c2 = 2. Also cr = c2 = 2 < 4 = n + 1 and
r < n.)
EXAMPLE 1.4.4 Solve the system
6x3 + 2x4 âˆ’4x5 âˆ’8x6
=
8
3x3 + x4 âˆ’2x5 âˆ’4x6
=
4
2x1 âˆ’3x2 + x3 + 4x4 âˆ’7x5 + x6
=
2
6x1 âˆ’9x2 + 11x4 âˆ’19x5 + 3x6
=
1.
Solution. The augmented matrix is
A =
ï£®
ï£¯ï£¯ï£°
0
0
6
2
âˆ’4
âˆ’8
8
0
0
3
1
âˆ’2
âˆ’4
4
2
âˆ’3
1
4
âˆ’7
1
2
6
âˆ’9
0
11
âˆ’19
3
1
ï£¹
ï£ºï£ºï£»
which is row equivalent to
B =
ï£®
ï£¯ï£¯ï£°
1
âˆ’3
2
0
11
6
âˆ’19
6
0
1
24
0
0
1
1
3
âˆ’2
3
0
5
3
0
0
0
0
0
1
1
4
0
0
0
0
0
0
0
ï£¹
ï£ºï£ºï£».
The complete solution is
x1 = 1
24 + 3
2x2 âˆ’11
6 x4 + 19
6 x5,
x3 = 5
3 âˆ’1
3x4 + 2
3x5,
x6 = 1
4,
with x2, x4, x5 arbitrary.
(Here n = 6, r = 3, c1 = 1, c2 = 3, c3 = 6; cr = c3 = 6 < 7 = n + 1; r < n.)

14
CHAPTER 1. LINEAR EQUATIONS
EXAMPLE 1.4.5 Find the rational number t for which the following sys-
tem is consistent and solve the system for this value of t.
x + y
=
2
x âˆ’y
=
0
3x âˆ’y
=
t.
Solution. The augmented matrix of the system is
A =
ï£®
ï£°
1
1
2
1
âˆ’1
0
3
âˆ’1
t
ï£¹
ï£»
which is rowâ€“equivalent to the simpler matrix
B =
ï£®
ï£°
1
1
2
0
1
1
0
0
t âˆ’2
ï£¹
ï£».
Hence if t Ì¸= 2 the system is inconsistent. If t = 2 the system is consistent
and
B =
ï£®
ï£°
1
1
2
0
1
1
0
0
0
ï£¹
ï£»â†’
ï£®
ï£°
1
0
1
0
1
1
0
0
0
ï£¹
ï£».
We read oï¬€the solution x = 1, y = 1.
EXAMPLE 1.4.6 For which rationals a and b does the following system
have (i) no solution, (ii) a unique solution, (iii) inï¬nitely many solutions?
x âˆ’2y + 3z
=
4
2x âˆ’3y + az
=
5
3x âˆ’4y + 5z
=
b.
Solution. The augmented matrix of the system is
A =
ï£®
ï£°
1
âˆ’2
3
4
2
âˆ’3
a
5
3
âˆ’4
5
b
ï£¹
ï£»

1.4. SYSTEMATIC SOLUTION OF LINEAR SYSTEMS.
15
Â½ R2 â†’R2 âˆ’2R1
R3 â†’R3 âˆ’3R1
ï£®
ï£°
1
âˆ’2
3
4
0
1
a âˆ’6
âˆ’3
0
2
âˆ’4
b âˆ’12
ï£¹
ï£»
R3 â†’R3 âˆ’2R2
ï£®
ï£°
1
âˆ’2
3
4
0
1
a âˆ’6
âˆ’3
0
0
âˆ’2a + 8
b âˆ’6
ï£¹
ï£»= B.
Case 1. a Ì¸= 4. Then âˆ’2a + 8 Ì¸= 0 and we see that B can be reduced to
a matrix of the form
ï£®
ï£°
1
0
0
u
0
1
0
v
0
0
1
bâˆ’6
âˆ’2a+8
ï£¹
ï£»
and we have the unique solution x = u, y = v, z = (b âˆ’6)/(âˆ’2a + 8).
Case 2. a = 4. Then
B =
ï£®
ï£°
1
âˆ’2
3
4
0
1
âˆ’2
âˆ’3
0
0
0
b âˆ’6
ï£¹
ï£».
If b Ì¸= 6 we get no solution, whereas if b = 6 then
B =
ï£®
ï£°
1
âˆ’2
3
4
0
1
âˆ’2
âˆ’3
0
0
0
0
ï£¹
ï£»
R1 â†’R1 + 2R2
ï£®
ï£°
1
0
âˆ’1
âˆ’2
0
1
âˆ’2
âˆ’3
0
0
0
0
ï£¹
ï£». We
read oï¬€the complete solution x = âˆ’2 + z, y = âˆ’3 + 2z, with z arbitrary.
EXAMPLE 1.4.7 Find the reduced rowâ€“echelon form of the following ma-
trix over Z3:
Â· 2
1
2
1
2
2
1
0
Â¸
.
Hence solve the system
2x + y + 2z
=
1
2x + 2y + z
=
0
over Z3.
Solution.

16
CHAPTER 1. LINEAR EQUATIONS
Â· 2
1
2
1
2
2
1
0
Â¸
R2 â†’R2 âˆ’R1
Â· 2
1
2
1
0
1
âˆ’1
âˆ’1
Â¸
=
Â· 2
1
2
1
0
1
2
2
Â¸
R1 â†’2R1
Â· 1
2
1
2
0
1
2
2
Â¸
R1 â†’R1 + R2
Â· 1
0
0
1
0
1
2
2
Â¸
.
The last matrix is in reduced rowâ€“echelon form.
To solve the system of equations whose augmented matrix is the given
matrix over Z3, we see from the reduced rowâ€“echelon form that x = 1 and
y = 2 âˆ’2z = 2 + z, where z = 0, 1, 2. Hence there are three solutions
to the given system of linear equations: (x, y, z) = (1, 2, 0), (1, 0, 1) and
(1, 1, 2).
1.5
Homogeneous systems
A system of homogeneous linear equations is a system of the form
a11x1 + a12x2 + Â· Â· Â· + a1nxn
=
0
a21x1 + a22x2 + Â· Â· Â· + a2nxn
=
0
...
am1x1 + am2x2 + Â· Â· Â· + amnxn
=
0.
Such a system is always consistent as x1 = 0, Â· Â· Â· , xn = 0 is a solution.
This solution is called the trivial solution. Any other solution is called a
nonâ€“trivial solution.
For example the homogeneous system
x âˆ’y
=
0
x + y
=
0
has only the trivial solution, whereas the homogeneous system
x âˆ’y + z
=
0
x + y + z
=
0
has the complete solution x = âˆ’z, y = 0, z arbitrary. In particular, taking
z = 1 gives the nonâ€“trivial solution x = âˆ’1, y = 0, z = 1.
There is simple but fundamental theorem concerning homogeneous sys-
tems.
THEOREM 1.5.1 A homogeneous system of m linear equations in n un-
knowns always has a nonâ€“trivial solution if m < n.

1.6. PROBLEMS
17
Proof. Suppose that m < n and that the coeï¬ƒcient matrix of the system
is rowâ€“equivalent to B, a matrix in reduced rowâ€“echelon form. Let r be the
number of nonâ€“zero rows in B. Then r â‰¤m < n and hence n âˆ’r > 0 and
so the number n âˆ’r of arbitrary unknowns is in fact positive. Taking one
of these unknowns to be 1 gives a nonâ€“trivial solution.
REMARK 1.5.1 Let two systems of homogeneous equations in n un-
knowns have coeï¬ƒcient matrices A and B, respectively. If each row of B is
a linear combination of the rows of A (i.e. a sum of multiples of the rows
of A) and each row of A is a linear combination of the rows of B, then it is
easy to prove that the two systems have identical solutions. The converse is
true, but is not easy to prove. Similarly if A and B have the same reduced
rowâ€“echelon form, apart from possibly zero rows, then the two systems have
identical solutions and conversely.
There is a similar situation in the case of two systems of linear equations
(not necessarily homogeneous), with the proviso that in the statement of
the converse, the extra condition that both the systems are consistent, is
needed.
1.6
PROBLEMS
1. Which of the following matrices of rationals is in reduced rowâ€“echelon
form?
(a)
ï£®
ï£°
1
0
0
0
âˆ’3
0
0
1
0
4
0
0
0
1
2
ï£¹
ï£»
(b)
ï£®
ï£°
0
1
0
0
5
0
0
1
0
âˆ’4
0
0
0
âˆ’1
3
ï£¹
ï£»
(c)
ï£®
ï£°
0
1
0
0
0
0
1
0
0
1
0
âˆ’2
ï£¹
ï£»
(d)
ï£®
ï£¯ï£¯ï£°
0
1
0
0
2
0
0
0
0
âˆ’1
0
0
0
1
4
0
0
0
0
0
ï£¹
ï£ºï£ºï£»
(e)
ï£®
ï£¯ï£¯ï£°
1
2
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
ï£¹
ï£ºï£ºï£»
(f)
ï£®
ï£¯ï£¯ï£°
0
0
0
0
0
0
1
2
0
0
0
1
0
0
0
0
ï£¹
ï£ºï£ºï£»
(g)
ï£®
ï£¯ï£¯ï£°
1
0
0
0
1
0
1
0
0
2
0
0
0
1
âˆ’1
0
0
0
0
0
ï£¹
ï£ºï£ºï£». [Answers: (a), (e), (g)]
2. Find reduced rowâ€“echelon forms which are rowâ€“equivalent to the following
matrices:
(a)
Â· 0
0
0
2
4
0
Â¸
(b)
Â· 0
1
3
1
2
4
Â¸
(c)
ï£®
ï£°
1
1
1
1
1
0
1
0
0
ï£¹
ï£»
(d)
ï£®
ï£°
2
0
0
0
0
0
âˆ’4
0
0
ï£¹
ï£».

18
CHAPTER 1. LINEAR EQUATIONS
[Answers:
(a)
Â· 1
2
0
0
0
0
Â¸
(b)
Â· 1
0
âˆ’2
0
1
3
Â¸
(c)
ï£®
ï£°
1
0
0
0
1
0
0
0
1
ï£¹
ï£»
(d)
ï£®
ï£°
1
0
0
0
0
0
0
0
0
ï£¹
ï£».]
3. Solve the following systems of linear equations by reducing the augmented
matrix to reduced rowâ€“echelon form:
(a)
x + y + z
=
2
(b)
x1 + x2 âˆ’x3 + 2x4
=
10
2x + 3y âˆ’z
=
8
3x1 âˆ’x2 + 7x3 + 4x4
=
1
x âˆ’y âˆ’z
=
âˆ’8
âˆ’5x1 + 3x2 âˆ’15x3 âˆ’6x4
=
9
(c)
3x âˆ’y + 7z
=
0
(d)
2x2 + 3x3 âˆ’4x4
=
1
2x âˆ’y + 4z
=
1
2
2x3 + 3x4
=
4
x âˆ’y + z
=
1
2x1 + 2x2 âˆ’5x3 + 2x4
=
4
6x âˆ’4y + 10z
=
3
2x1 âˆ’6x3 + 9x4
=
7
[Answers: (a) x = âˆ’3, y = 19
4 , z = 1
4; (b) inconsistent;
(c) x = âˆ’1
2 âˆ’3z, y = âˆ’3
2 âˆ’2z, with z arbitrary;
(d) x1 = 19
2 âˆ’9x4, x2 = âˆ’5
2 + 17
4 x4, x3 = 2 âˆ’3
2x4, with x4 arbitrary.]
4. Show that the following system is consistent if and only if c = 2a âˆ’3b
and solve the system in this case.
2x âˆ’y + 3z
=
a
3x + y âˆ’5z
=
b
âˆ’5x âˆ’5y + 21z
=
c.
[Answer: x = a+b
5
+ 2
5z, y = âˆ’3a+2b
5
+ 19
5 z, with z arbitrary.]
5. Find the value of t for which the following system is consistent and solve
the system for this value of t.
x + y
=
1
tx + y
=
t
(1 + t)x + 2y
=
3.
[Answer: t = 2; x = 1, y = 0.]

1.6. PROBLEMS
19
6. Solve the homogeneous system
âˆ’3x1 + x2 + x3 + x4
=
0
x1 âˆ’3x2 + x3 + x4
=
0
x1 + x2 âˆ’3x3 + x4
=
0
x1 + x2 + x3 âˆ’3x4
=
0.
[Answer: x1 = x2 = x3 = x4, with x4 arbitrary.]
7. For which rational numbers Î» does the homogeneous system
x + (Î» âˆ’3)y
=
0
(Î» âˆ’3)x + y
=
0
have a nonâ€“trivial solution?
[Answer: Î» = 2, 4.]
8. Solve the homogeneous system
3x1 + x2 + x3 + x4
=
0
5x1 âˆ’x2 + x3 âˆ’x4
=
0.
[Answer: x1 = âˆ’1
4x3, x2 = âˆ’1
4x3 âˆ’x4, with x3 and x4 arbitrary.]
9. Let A be the coeï¬ƒcient matrix of the following homogeneous system of
n equations in n unknowns:
(1 âˆ’n)x1 + x2 + Â· Â· Â· + xn
=
0
x1 + (1 âˆ’n)x2 + Â· Â· Â· + xn
=
0
Â· Â· Â·
=
0
x1 + x2 + Â· Â· Â· + (1 âˆ’n)xn
=
0.
Find the reduced rowâ€“echelon form of A and hence, or otherwise, prove that
the solution of the above system is x1 = x2 = Â· Â· Â· = xn, with xn arbitrary.
10. Let A =
Â· a
b
c
d
Â¸
be a matrix over a ï¬eld F. Prove that A is rowâ€“
equivalent to
Â· 1
0
0
1
Â¸
if ad âˆ’bc Ì¸= 0, but is rowâ€“equivalent to a matrix
whose second row is zero, if ad âˆ’bc = 0.

20
CHAPTER 1. LINEAR EQUATIONS
11. For which rational numbers a does the following system have (i) no
solutions (ii) exactly one solution (iii) inï¬nitely many solutions?
x + 2y âˆ’3z
=
4
3x âˆ’y + 5z
=
2
4x + y + (a2 âˆ’14)z
=
a + 2.
[Answer: a = âˆ’4, no solution; a = 4, inï¬nitely many solutions; a Ì¸= Â±4,
exactly one solution.]
12. Solve the following system of homogeneous equations over Z2:
x1 + x3 + x5
=
0
x2 + x4 + x5
=
0
x1 + x2 + x3 + x4
=
0
x3 + x4
=
0.
[Answer: x1 = x2 = x4 + x5, x3 = x4, with x4 and x5 arbitrary elements of
Z2.]
13. Solve the following systems of linear equations over Z5:
(a)
2x + y + 3z
=
4
(b)
2x + y + 3z
=
4
4x + y + 4z
=
1
4x + y + 4z
=
1
3x + y + 2z
=
0
x + y
=
3.
[Answer: (a) x = 1, y = 2, z = 0; (b) x = 1 + 2z, y = 2 + 3z, with z an
arbitrary element of Z5.]
14. If (Î±1, . . . , Î±n) and (Î²1, . . . , Î²n) are solutions of a system of linear equa-
tions, prove that
((1 âˆ’t)Î±1 + tÎ²1, . . . , (1 âˆ’t)Î±n + tÎ²n)
is also a solution.
15. If (Î±1, . . . , Î±n) is a solution of a system of linear equations, prove that
the complete solution is given by x1 = Î±1 + y1, . . . , xn = Î±n + yn, where
(y1, . . . , yn) is the general solution of the associated homogeneous system.

1.6. PROBLEMS
21
16. Find the values of a and b for which the following system is consistent.
Also ï¬nd the complete solution when a = b = 2.
x + y âˆ’z + w
=
1
ax + y + z + w
=
b
3x + 2y +
aw
=
1 + a.
[Answer: a Ì¸= 2 or a = 2 = b; x = 1 âˆ’2z, y = 3z âˆ’w, with z, w arbitrary.]
17. Let F = {0, 1, a, b} be a ï¬eld consisting of 4 elements.
(a) Determine the addition and multiplication tables of F. (Hint: Prove
that the elements 1+0, 1+1, 1+a, 1+b are distinct and deduce that
1 + 1 + 1 + 1 = 0; then deduce that 1 + 1 = 0.)
(b) A matrix A, whose elements belong to F, is deï¬ned by
A =
ï£®
ï£°
1
a
b
a
a
b
b
1
1
1
1
a
ï£¹
ï£»,
prove that the reduced rowâ€“echelon form of A is given by the matrix
B =
ï£®
ï£°
1
0
0
0
0
1
0
b
0
0
1
1
ï£¹
ï£».

22
CHAPTER 1. LINEAR EQUATIONS

Chapter 2
MATRICES
2.1
Matrix arithmetic
A matrix over a ï¬eld F is a rectangular array of elements from F. The sym-
bol MmÃ—n(F) denotes the collection of all m Ã— n matrices over F. Matrices
will usually be denoted by capital letters and the equation A = [aij] means
that the element in the iâ€“th row and jâ€“th column of the matrix A equals
aij. It is also occasionally convenient to write aij = (A)ij. For the present,
all matrices will have rational entries, unless otherwise stated.
EXAMPLE 2.1.1 The formula aij = 1/(i + j) for 1 â‰¤i â‰¤3, 1 â‰¤j â‰¤4
deï¬nes a 3 Ã— 4 matrix A = [aij], namely
A =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
1
2
1
3
1
4
1
5
1
3
1
4
1
5
1
6
1
4
1
5
1
6
1
7
ï£¹
ï£ºï£ºï£ºï£ºï£»
.
DEFINITION 2.1.1 (Equality of matrices) Matrices A and B are said
to be equal if A and B have the same size and corresponding elements are
equal; that is A and B âˆˆMmÃ—n(F) and A = [aij], B = [bij], with aij = bij
for 1 â‰¤i â‰¤m, 1 â‰¤j â‰¤n.
DEFINITION 2.1.2 (Addition of matrices) Let A = [aij] and B =
[bij] be of the same size. Then A + B is the matrix obtained by adding
corresponding elements of A and B; that is
A + B = [aij] + [bij] = [aij + bij].
23

24
CHAPTER 2. MATRICES
DEFINITION 2.1.3 (Scalar multiple of a matrix) Let A = [aij] and
t âˆˆF (that is t is a scalar). Then tA is the matrix obtained by multiplying
all elements of A by t; that is
tA = t[aij] = [taij].
DEFINITION 2.1.4 (Additive inverse of a matrix) Let A = [aij] .
Then âˆ’A is the matrix obtained by replacing the elements of A by their
additive inverses; that is
âˆ’A = âˆ’[aij] = [âˆ’aij].
DEFINITION 2.1.5 (Subtraction of matrices) Matrix subtraction is
deï¬ned for two matrices A = [aij] and B = [bij] of the same size, in the
usual way; that is
A âˆ’B = [aij] âˆ’[bij] = [aij âˆ’bij].
DEFINITION 2.1.6 (The zero matrix) For each m, n the matrix in
MmÃ—n(F), all of whose elements are zero, is called the zero matrix (of size
m Ã— n) and is denoted by the symbol 0.
The matrix operations of addition, scalar multiplication, additive inverse
and subtraction satisfy the usual laws of arithmetic. (In what follows, s and
t will be arbitrary scalars and A, B, C are matrices of the same size.)
1. (A + B) + C = A + (B + C);
2. A + B = B + A;
3. 0 + A = A;
4. A + (âˆ’A) = 0;
5. (s + t)A = sA + tA,
(s âˆ’t)A = sA âˆ’tA;
6. t(A + B) = tA + tB,
t(A âˆ’B) = tA âˆ’tB;
7. s(tA) = (st)A;
8. 1A = A,
0A = 0,
(âˆ’1)A = âˆ’A;
9. tA = 0 â‡’t = 0 or A = 0.
Other similar properties will be used when needed.

2.1. MATRIX ARITHMETIC
25
DEFINITION 2.1.7 (Matrix product) Let A = [aij] be a matrix of
size m Ã— n and B = [bjk] be a matrix of size n Ã— p; (that is the number
of columns of A equals the number of rows of B). Then AB is the m Ã— p
matrix C = [cik] whose (i, k)â€“th element is deï¬ned by the formula
cik =
n
X
j=1
aijbjk = ai1b1k + Â· Â· Â· + ainbnk.
EXAMPLE 2.1.2
1.
Â· 1
2
3
4
Â¸ Â· 5
6
7
8
Â¸
=
Â· 1 Ã— 5 + 2 Ã— 7
1 Ã— 6 + 2 Ã— 8
3 Ã— 5 + 4 Ã— 7
3 Ã— 6 + 4 Ã— 8
Â¸
=
Â· 19
22
43
50
Â¸
;
2.
Â· 5
6
7
8
Â¸ Â· 1
2
3
4
Â¸
=
Â· 23
34
31
46
Â¸
Ì¸=
Â· 1
2
3
4
Â¸ Â· 5
6
7
8
Â¸
;
3.
Â· 1
2
Â¸ Â£
3
4
Â¤
=
Â· 3
4
6
8
Â¸
;
4.
Â£
3
4
Â¤ Â· 1
2
Â¸
=
Â£
11
Â¤
;
5.
Â· 1
âˆ’1
1
âˆ’1
Â¸ Â· 1
âˆ’1
1
âˆ’1
Â¸
=
Â· 0
0
0
0
Â¸
.
Matrix multiplication obeys many of the familiar laws of arithmetic apart
from the commutative law.
1. (AB)C = A(BC) if A, B, C are m Ã— n, n Ã— p, p Ã— q, respectively;
2. t(AB) = (tA)B = A(tB),
A(âˆ’B) = (âˆ’A)B = âˆ’(AB);
3. (A + B)C = AC + BC if A and B are m Ã— n and C is n Ã— p;
4. D(A + B) = DA + DB if A and B are m Ã— n and D is p Ã— m.
We prove the associative law only:
First observe that (AB)C and A(BC) are both of size m Ã— q.
Let A = [aij], B = [bjk], C = [ckl]. Then
((AB)C)il
=
p
X
k=1
(AB)ikckl =
p
X
k=1
ï£«
ï£­
n
X
j=1
aijbjk
ï£¶
ï£¸ckl
=
p
X
k=1
n
X
j=1
aijbjkckl.

26
CHAPTER 2. MATRICES
Similarly
(A(BC))il =
n
X
j=1
p
X
k=1
aijbjkckl.
However the double summations are equal. For sums of the form
n
X
j=1
p
X
k=1
djk
and
p
X
k=1
n
X
j=1
djk
represent the sum of the np elements of the rectangular array [djk], by rows
and by columns, respectively. Consequently
((AB)C)il = (A(BC))il
for 1 â‰¤i â‰¤m, 1 â‰¤l â‰¤q. Hence (AB)C = A(BC).
The system of m linear equations in n unknowns
a11x1 + a12x2 + Â· Â· Â· + a1nxn
=
b1
a21x1 + a22x2 + Â· Â· Â· + a2nxn
=
b2
...
am1x1 + am2x2 + Â· Â· Â· + amnxn
=
bm
is equivalent to a single matrix equation
ï£®
ï£¯ï£¯ï£¯ï£°
a11
a12
Â· Â· Â·
a1n
a21
a22
Â· Â· Â·
a2n
...
...
am1
am2
Â· Â· Â·
amn
ï£¹
ï£ºï£ºï£ºï£»
ï£®
ï£¯ï£¯ï£¯ï£°
x1
x2
...
xn
ï£¹
ï£ºï£ºï£ºï£»=
ï£®
ï£¯ï£¯ï£¯ï£°
b1
b2
...
bm
ï£¹
ï£ºï£ºï£ºï£»,
that is AX = B, where A = [aij] is the coeï¬ƒcient matrix of the system,
X =
ï£®
ï£¯ï£¯ï£¯ï£°
x1
x2
...
xn
ï£¹
ï£ºï£ºï£ºï£»is the vector of unknowns and B =
ï£®
ï£¯ï£¯ï£¯ï£°
b1
b2
...
bm
ï£¹
ï£ºï£ºï£ºï£»is the vector of
constants.
Another useful matrix equation equivalent to the above system of linear
equations is
x1
ï£®
ï£¯ï£¯ï£¯ï£°
a11
a21
...
am1
ï£¹
ï£ºï£ºï£ºï£»+ x2
ï£®
ï£¯ï£¯ï£¯ï£°
a12
a22
...
am2
ï£¹
ï£ºï£ºï£ºï£»+ Â· Â· Â· + xn
ï£®
ï£¯ï£¯ï£¯ï£°
a1n
a2n
...
amn
ï£¹
ï£ºï£ºï£ºï£»=
ï£®
ï£¯ï£¯ï£¯ï£°
b1
b2
...
bm
ï£¹
ï£ºï£ºï£ºï£».

2.2. LINEAR TRANSFORMATIONS
27
EXAMPLE 2.1.3 The system
x + y + z
=
1
x âˆ’y + z
=
0.
is equivalent to the matrix equation
Â· 1
1
1
1
âˆ’1
1
Â¸ ï£®
ï£°
x
y
z
ï£¹
ï£»=
Â· 1
0
Â¸
and to the equation
x
Â· 1
1
Â¸
+ y
Â·
1
âˆ’1
Â¸
+ z
Â· 1
1
Â¸
=
Â· 1
0
Â¸
.
2.2
Linear transformations
An nâ€“dimensional column vector is an n Ã— 1 matrix over F. The collection
of all nâ€“dimensional column vectors is denoted by F n.
Every matrix is associated with an important type of function called a
linear transformation.
DEFINITION 2.2.1 (Linear transformation) With A âˆˆMmÃ—n(F), we
associate the function TA : F n â†’F m deï¬ned by TA(X) = AX for all
X âˆˆF n. More explicitly, using components, the above function takes the
form
y1
=
a11x1 + a12x2 + Â· Â· Â· + a1nxn
y2
=
a21x1 + a22x2 + Â· Â· Â· + a2nxn
...
ym
=
am1x1 + am2x2 + Â· Â· Â· + amnxn,
where y1, y2, Â· Â· Â· , ym are the components of the column vector TA(X).
The function just deï¬ned has the property that
TA(sX + tY ) = sTA(X) + tTA(Y )
(2.1)
for all s, t âˆˆF and all nâ€“dimensional column vectors X, Y . For
TA(sX + tY ) = A(sX + tY ) = s(AX) + t(AY ) = sTA(X) + tTA(Y ).

28
CHAPTER 2. MATRICES
REMARK 2.2.1 It is easy to prove that if T : F n â†’F m is a function
satisfying equation 2.1, then T = TA, where A is the m Ã— n matrix whose
columns are T(E1), . . . , T(En), respectively, where E1, . . . , En are the nâ€“
dimensional unit vectors deï¬ned by
E1 =
ï£®
ï£¯ï£¯ï£¯ï£°
1
0
...
0
ï£¹
ï£ºï£ºï£ºï£»,
. . .
, En =
ï£®
ï£¯ï£¯ï£¯ï£°
0
0
...
1
ï£¹
ï£ºï£ºï£ºï£».
One wellâ€“known example of a linear transformation arises from rotating
the (x, y)â€“plane in 2-dimensional Euclidean space, anticlockwise through Î¸
radians.
Here a point (x, y) will be transformed into the point (x1, y1),
where
x1
=
x cos Î¸ âˆ’y sin Î¸
y1
=
x sin Î¸ + y cos Î¸.
In 3â€“dimensional Euclidean space, the equations
x1 = x cos Î¸ âˆ’y sin Î¸, y1 = x sin Î¸ + y cos Î¸, z1 = z;
x1 = x, y1 = y cos Ï† âˆ’z sin Ï†, z1 = y sin Ï† + z cos Ï†;
x1 = x cos Ïˆ âˆ’z sin Ïˆ, y1 = y, z1 = x sin Ïˆ + z cos Ïˆ;
correspond to rotations about the positive z, x, yâ€“axes, anticlockwise through
Î¸, Ï†, Ïˆ radians, respectively.
The product of two matrices is related to the product of the correspond-
ing linear transformations:
If A is mÃ—n and B is nÃ—p, then the function TATB : F p â†’F m, obtained
by ï¬rst performing TB, then TA is in fact equal to the linear transformation
TAB. For if X âˆˆF p, we have
TATB(X) = A(BX) = (AB)X = TAB(X).
The following example is useful for producing rotations in 3â€“dimensional
animated design. (See [27, pages 97â€“112].)
EXAMPLE 2.2.1 The linear transformation resulting from successively
rotating 3â€“dimensional space about the positive z, x, yâ€“axes, anticlockwise
through Î¸, Ï†, Ïˆ radians respectively, is equal to TABC, where

2.2. LINEAR TRANSFORMATIONS
29
Î¸
l
(x, y)
(x1, y1)
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
@
@
@
@
@
@
@
Figure 2.1: Reï¬‚ection in a line.
C =
ï£®
ï£°
cos Î¸
âˆ’sin Î¸
0
sin Î¸
cos Î¸
0
0
0
1
ï£¹
ï£»,
B =
ï£®
ï£°
1
0
0
0
cos Ï†
âˆ’sin Ï†
0
sin Ï†
cos Ï†
ï£¹
ï£».
A =
ï£®
ï£°
cos Ïˆ
0
âˆ’sin Ïˆ
0
1
0
sin Ïˆ
0
cos Ïˆ
ï£¹
ï£».
The matrix ABC is quite complicated:
A(BC) =
ï£®
ï£°
cos Ïˆ
0
âˆ’sin Ïˆ
0
1
0
sin Ïˆ
0
cos Ïˆ
ï£¹
ï£»
ï£®
ï£°
cos Î¸
âˆ’sin Î¸
0
cos Ï† sin Î¸
cos Ï† cos Î¸
âˆ’sin Ï†
sin Ï† sin Î¸
sin Ï† cos Î¸
cos Ï†
ï£¹
ï£»
=
ï£®
ï£°
cos Ïˆ cos Î¸ âˆ’sin Ïˆ sin Ï† sin Î¸
âˆ’cos Ïˆ sin Î¸ âˆ’sin Ïˆ sin Ï† sin Î¸
âˆ’sin Ïˆ cos Ï†
cos Ï† sin Î¸
cos Ï† cos Î¸
âˆ’sin Ï†
sin Ïˆ cos Î¸ + cos Ïˆ sin Ï† sin Î¸
âˆ’sin Ïˆ sin Î¸ + cos Ïˆ sin Ï† cos Î¸
cos Ïˆ cos Ï†
ï£¹
ï£».
EXAMPLE 2.2.2 Another example of a linear transformation arising from
geometry is reï¬‚ection of the plane in a line l inclined at an angle Î¸ to the
positive xâ€“axis.
We reduce the problem to the simpler case Î¸ = 0, where the equations
of transformation are x1 = x, y1 = âˆ’y. First rotate the plane clockwise
through Î¸ radians, thereby taking l into the xâ€“axis; next reï¬‚ect the plane in
the xâ€“axis; then rotate the plane anticlockwise through Î¸ radians, thereby
restoring l to its original position.

30
CHAPTER 2. MATRICES
Î¸
l
(x, y)
(x1, y1)
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
@
@
@
Figure 2.2: Projection on a line.
In terms of matrices, we get transformation equations
Â· x1
y1
Â¸
=
Â· cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¸ Â· 1
0
0
âˆ’1
Â¸ Â· cos (âˆ’Î¸)
âˆ’sin (âˆ’Î¸)
sin (âˆ’Î¸)
cos (âˆ’Î¸)
Â¸ Â· x
y
Â¸
=
Â· cos Î¸
sin Î¸
sin Î¸
âˆ’cos Î¸
Â¸ Â·
cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸
Â¸ Â· x
y
Â¸
=
Â· cos 2Î¸
sin 2Î¸
sin 2Î¸
âˆ’cos 2Î¸
Â¸ Â· x
y
Â¸
.
The more general transformation
Â· x1
y1
Â¸
= a
Â· cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¸ Â· x
y
Â¸
+
Â· u
v
Â¸
,
a > 0,
represents a rotation, followed by a scaling and then by a translation. Such
transformations are important in computer graphics. See [23, 24].
EXAMPLE 2.2.3 Our last example of a geometrical linear transformation
arises from projecting the plane onto a line l through the origin, inclined
at angle Î¸ to the positive xâ€“axis.
Again we reduce that problem to the
simpler case where l is the xâ€“axis and the equations of transformation are
x1 = x, y1 = 0.
In terms of matrices, we get transformation equations
Â· x1
y1
Â¸
=
Â· cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¸ Â· 1
0
0
0
Â¸ Â· cos (âˆ’Î¸)
âˆ’sin (âˆ’Î¸)
sin (âˆ’Î¸)
cos (âˆ’Î¸)
Â¸ Â· x
y
Â¸

2.3. RECURRENCE RELATIONS
31
=
Â· cos Î¸
0
sin Î¸
0
Â¸ Â·
cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸
Â¸ Â· x
y
Â¸
=
Â·
cos2 Î¸
cos Î¸ sin Î¸
sin Î¸ cos Î¸
sin2 Î¸
Â¸ Â· x
y
Â¸
.
2.3
Recurrence relations
DEFINITION 2.3.1 (The identity matrix) The n Ã— n matrix In =
[Î´ij], deï¬ned by Î´ij = 1 if i = j, Î´ij = 0 if i Ì¸= j, is called the n Ã— n identity
matrix of order n. In other words, the columns of the identity matrix of
order n are the unit vectors E1, Â· Â· Â· , En, respectively.
For example, I2 =
Â· 1
0
0
1
Â¸
.
THEOREM 2.3.1 If A is m Ã— n, then ImA = A = AIn.
DEFINITION 2.3.2 (kâ€“th power of a matrix) If A is an nÃ—n matrix,
we deï¬ne Ak recursively as follows: A0 = In and Ak+1 = AkA for k â‰¥0.
For example A1 = A0A = InA = A and hence A2 = A1A = AA.
The usual index laws hold provided AB = BA:
1. AmAn = Am+n,
(Am)n = Amn;
2. (AB)n = AnBn;
3. AmBn = BnAm;
4. (A + B)2 = A2 + 2AB + B2;
5. (A + B)n =
n
X
i=0
Â¡n
i
Â¢
AiBnâˆ’i;
6. (A + B)(A âˆ’B) = A2 âˆ’B2.
We now state a basic property of the natural numbers.
AXIOM 2.3.1 (PRINCIPLE OF MATHEMATICAL INDUCTION)
If for each n â‰¥1, Pn denotes a mathematical statement and
(i) P1 is true,

32
CHAPTER 2. MATRICES
(ii) the truth of Pn implies that of Pn+1 for each n â‰¥1,
then Pn is true for all n â‰¥1.
EXAMPLE 2.3.1 Let A =
Â·
7
4
âˆ’9
âˆ’5
Â¸
. Prove that
An =
Â· 1 + 6n
4n
âˆ’9n
1 âˆ’6n
Â¸
if n â‰¥1.
Solution. We use the principle of mathematical induction.
Take Pn to be the statement
An =
Â· 1 + 6n
4n
âˆ’9n
1 âˆ’6n
Â¸
.
Then P1 asserts that
A1 =
Â· 1 + 6 Ã— 1
4 Ã— 1
âˆ’9 Ã— 1
1 âˆ’6 Ã— 1
Â¸
=
Â·
7
4
âˆ’9
âˆ’5
Â¸
,
which is true. Now let n â‰¥1 and assume that Pn is true. We have to deduce
that
An+1 =
Â· 1 + 6(n + 1)
4(n + 1)
âˆ’9(n + 1)
1 âˆ’6(n + 1)
Â¸
=
Â·
7 + 6n
4n + 4
âˆ’9n âˆ’9
âˆ’5 âˆ’6n
Â¸
.
Now
An+1
=
AnA
=
Â· 1 + 6n
4n
âˆ’9n
1 âˆ’6n
Â¸ Â·
7
4
âˆ’9
âˆ’5
Â¸
=
Â·
(1 + 6n)7 + (4n)(âˆ’9)
(1 + 6n)4 + (4n)(âˆ’5)
(âˆ’9n)7 + (1 âˆ’6n)(âˆ’9)
(âˆ’9n)4 + (1 âˆ’6n)(âˆ’5)
Â¸
=
Â·
7 + 6n
4n + 4
âˆ’9n âˆ’9
âˆ’5 âˆ’6n
Â¸
,
and â€œthe induction goes throughâ€.
The last example has an application to the solution of a system of re-
currence relations:

2.4. PROBLEMS
33
EXAMPLE 2.3.2 The following system of recurrence relations holds for
all n â‰¥0:
xn+1
=
7xn + 4yn
yn+1
=
âˆ’9xn âˆ’5yn.
Solve the system for xn and yn in terms of x0 and y0.
Solution. Combine the above equations into a single matrix equation
Â· xn+1
yn+1
Â¸
=
Â·
7
4
âˆ’9
âˆ’5
Â¸ Â· xn
yn
Â¸
,
or Xn+1 = AXn, where A =
Â·
7
4
âˆ’9
âˆ’5
Â¸
and Xn =
Â· xn
yn
Â¸
.
We see that
X1
=
AX0
X2
=
AX1 = A(AX0) = A2X0
...
Xn
=
AnX0.
(The truth of the equation Xn = AnX0 for n â‰¥1, strictly speaking
follows by mathematical induction; however for simple cases such as the
above, it is customary to omit the strict proof and supply instead a few
lines of motivation for the inductive statement.)
Hence the previous example gives
Â· xn
yn
Â¸
= Xn
=
Â· 1 + 6n
4n
âˆ’9n
1 âˆ’6n
Â¸ Â· x0
y0
Â¸
=
Â·
(1 + 6n)x0 + (4n)y0
(âˆ’9n)x0 + (1 âˆ’6n)y0
Â¸
,
and hence xn = (1+6n)x0 +4ny0 and yn = (âˆ’9n)x0 +(1âˆ’6n)y0, for n â‰¥1.
2.4
PROBLEMS
1. Let A, B, C, D be matrices deï¬ned by
A =
ï£®
ï£°
3
0
âˆ’1
2
1
1
ï£¹
ï£»,
B =
ï£®
ï£°
1
5
2
âˆ’1
1
0
âˆ’4
1
3
ï£¹
ï£»,

34
CHAPTER 2. MATRICES
C =
ï£®
ï£°
âˆ’3
âˆ’1
2
1
4
3
ï£¹
ï£»,
D =
Â· 4
âˆ’1
2
0
Â¸
.
Which of the following matrices are deï¬ned? Compute those matrices
which are deï¬ned.
A + B, A + C, AB, BA, CD, DC, D2.
[Answers: A + C, BA, CD, D2;
ï£®
ï£°
0
âˆ’1
1
3
5
4
ï£¹
ï£»,
ï£®
ï£°
0
12
âˆ’4
2
âˆ’10
5
ï£¹
ï£»,
ï£®
ï£°
âˆ’14
3
10
âˆ’2
22
âˆ’4
ï£¹
ï£»,
Â· 14
âˆ’4
8
âˆ’2
Â¸
.]
2. Let A =
Â· âˆ’1
0
1
0
1
1
Â¸
. Show that if B is a 3 Ã— 2 such that AB = I2,
then
B =
ï£®
ï£°
a
b
âˆ’a âˆ’1
1 âˆ’b
a + 1
b
ï£¹
ï£»
for suitable numbers a and b. Use the associative law to show that
(BA)2B = B.
3. If A =
Â· a
b
c
d
Â¸
, prove that A2 âˆ’(a + d)A + (ad âˆ’bc)I2 = 0.
4. If A =
Â· 4
âˆ’3
1
0
Â¸
, use the fact A2 = 4A âˆ’3I2 and mathematical
induction, to prove that
An = (3n âˆ’1)
2
A + 3 âˆ’3n
2
I2
if n â‰¥1.
5. A sequence of numbers x1, x2, . . . , xn, . . . satisï¬es the recurrence rela-
tion xn+1 = axn+bxnâˆ’1 for n â‰¥1, where a and b are constants. Prove
that
Â· xn+1
xn
Â¸
= A
Â·
xn
xnâˆ’1
Â¸
,

2.4. PROBLEMS
35
where A =
Â· a
b
1
0
Â¸
and hence express
Â· xn+1
xn
Â¸
in terms of
Â· x1
x0
Â¸
.
If a = 4 and b = âˆ’3, use the previous question to ï¬nd a formula for
xn in terms of x1 and x0.
[Answer:
xn = 3n âˆ’1
2
x1 + 3 âˆ’3n
2
x0.]
6. Let A =
Â· 2a
âˆ’a2
1
0
Â¸
.
(a) Prove that
An =
Â· (n + 1)an
âˆ’nan+1
nanâˆ’1
(1 âˆ’n)an
Â¸
if n â‰¥1.
(b) A sequence x0, x1, . . . , xn, . . . satisï¬es the recurrence relation xn+1 =
2axn âˆ’a2xnâˆ’1 for n â‰¥1. Use part (a) and the previous question
to prove that xn = nanâˆ’1x1 + (1 âˆ’n)anx0 for n â‰¥1.
7. Let A =
Â· a
b
c
d
Â¸
and suppose that Î»1 and Î»2 are the roots of the
quadratic polynomial x2âˆ’(a+d)x+adâˆ’bc. (Î»1 and Î»2 may be equal.)
Let kn be deï¬ned by k0 = 0, k1 = 1 and for n â‰¥2
kn =
n
X
i=1
Î»nâˆ’i
1
Î»iâˆ’1
2
.
Prove that
kn+1 = (Î»1 + Î»2)kn âˆ’Î»1Î»2knâˆ’1,
if n â‰¥1. Also prove that
kn =
Â½ (Î»n
1 âˆ’Î»n
2)/(Î»1 âˆ’Î»2)
if Î»1 Ì¸= Î»2,
nÎ»nâˆ’1
1
if Î»1 = Î»2.
Use mathematical induction to prove that if n â‰¥1,
An = knA âˆ’Î»1Î»2knâˆ’1I2,
[Hint: Use the equation A2 = (a + d)A âˆ’(ad âˆ’bc)I2.]

36
CHAPTER 2. MATRICES
8. Use Question 7 to prove that if A =
Â· 1
2
2
1
Â¸
, then
An = 3n
2
Â· 1
1
1
1
Â¸
+ (âˆ’1)nâˆ’1
2
Â· âˆ’1
1
1
âˆ’1
Â¸
if n â‰¥1.
9. The Fibonacci numbers are deï¬ned by the equations F0 = 0, F1 = 1
and Fn+1 = Fn + Fnâˆ’1 if n â‰¥1. Prove that
Fn =
1
âˆš
5
ÃƒÃƒ
1 +
âˆš
5
2
!n
âˆ’
Ãƒ
1 âˆ’
âˆš
5
2
!n!
if n â‰¥0.
10. Let r > 1 be an integer. Let a and b be arbitrary positive integers.
Sequences xn and yn of positive integers are deï¬ned in terms of a and
b by the recurrence relations
xn+1
=
xn + ryn
yn+1
=
xn + yn,
for n â‰¥0, where x0 = a and y0 = b.
Use Question 7 to prove that
xn
yn
â†’âˆšr
as n â†’âˆ.
2.5
Nonâ€“singular matrices
DEFINITION 2.5.1 (Nonâ€“singular matrix)
A square matrix A âˆˆMnÃ—n(F) is called nonâ€“singular or invertible if
there exists a matrix B âˆˆMnÃ—n(F) such that
AB = In = BA.
Any matrix B with the above property is called an inverse of A. If A does
not have an inverse, A is called singular.

2.5. NONâ€“SINGULAR MATRICES
37
THEOREM 2.5.1 (Inverses are unique)
If A has inverses B and C, then B = C.
Proof. Let B and C be inverses of A. Then AB = In = BA and AC =
In = CA. Then B(AC) = BIn = B and (BA)C = InC = C. Hence because
B(AC) = (BA)C, we deduce that B = C.
REMARK 2.5.1 If A has an inverse, it is denoted by Aâˆ’1. So
AAâˆ’1 = In = Aâˆ’1A.
Also if A is nonâ€“singular, it follows that Aâˆ’1 is also nonâ€“singular and
(Aâˆ’1)âˆ’1 = A.
THEOREM 2.5.2 If A and B are nonâ€“singular matrices of the same size,
then so is AB. Moreover
(AB)âˆ’1 = Bâˆ’1Aâˆ’1.
Proof.
(AB)(Bâˆ’1Aâˆ’1) = A(BBâˆ’1)Aâˆ’1 = AInAâˆ’1 = AAâˆ’1 = In.
Similarly
(Bâˆ’1Aâˆ’1)(AB) = In.
REMARK 2.5.2 The above result generalizes to a product of m nonâ€“
singular matrices: If A1, . . . , Am are nonâ€“singular n Ã— n matrices, then the
product A1 . . . Am is also nonâ€“singular. Moreover
(A1 . . . Am)âˆ’1 = Aâˆ’1
m . . . Aâˆ’1
1 .
(Thus the inverse of the product equals the product of the inverses in the
reverse order.)
EXAMPLE 2.5.1 If A and B are n Ã— n matrices satisfying A2 = B2 =
(AB)2 = In, prove that AB = BA.
Solution. Assume A2 = B2 = (AB)2 = In. Then A, B, AB are nonâ€“
singular and Aâˆ’1 = A, Bâˆ’1 = B, (AB)âˆ’1 = AB.
But (AB)âˆ’1 = Bâˆ’1Aâˆ’1 and hence AB = BA.

38
CHAPTER 2. MATRICES
EXAMPLE 2.5.2 A =
Â· 1
2
4
8
Â¸
is singular. For suppose B =
Â· a
b
c
d
Â¸
is an inverse of A. Then the equation AB = I2 gives
Â· 1
2
4
8
Â¸ Â· a
b
c
d
Â¸
=
Â· 1
0
0
1
Â¸
and equating the corresponding elements of column 1 of both sides gives the
system
a + 2c
=
1
4a + 8c
=
0
which is clearly inconsistent.
THEOREM 2.5.3 Let A =
Â· a
b
c
d
Â¸
and âˆ†= ad âˆ’bc Ì¸= 0. Then A is
nonâ€“singular. Also
Aâˆ’1 = âˆ†âˆ’1
Â·
d
âˆ’b
âˆ’c
a
Â¸
.
REMARK 2.5.3 The expression ad âˆ’bc is called the determinant of A
and is denoted by the symbols det A or
Â¯Â¯Â¯Â¯
a
b
c
d
Â¯Â¯Â¯Â¯.
Proof. Verify that the matrix B = âˆ†âˆ’1
Â·
d
âˆ’b
âˆ’c
a
Â¸
satisï¬es the equation
AB = I2 = BA.
EXAMPLE 2.5.3 Let
A =
ï£®
ï£°
0
1
0
0
0
1
5
0
0
ï£¹
ï£».
Verify that A3 = 5I3, deduce that A is nonâ€“singular and ï¬nd Aâˆ’1.
Solution. After verifying that A3 = 5I3, we notice that
A
Âµ1
5A2
Â¶
= I3 =
Âµ1
5A2
Â¶
A.
Hence A is nonâ€“singular and Aâˆ’1 = 1
5A2.

2.5. NONâ€“SINGULAR MATRICES
39
THEOREM 2.5.4 If the coeï¬ƒcient matrix A of a system of n equations
in n unknowns is nonâ€“singular, then the system AX = B has the unique
solution X = Aâˆ’1B.
Proof. Assume that Aâˆ’1 exists.
1. (Uniqueness.) Assume that AX = B. Then
(Aâˆ’1A)X
=
Aâˆ’1B,
InX
=
Aâˆ’1B,
X
=
Aâˆ’1B.
2. (Existence.) Let X = Aâˆ’1B. Then
AX
=
A(Aâˆ’1B) = (AAâˆ’1)B = InB = B.
THEOREM 2.5.5 (Cramerâ€™s rule for 2 equations in 2 unknowns)
The system
ax + by
=
e
cx + dy
=
f
has a unique solution if âˆ†=
Â¯Â¯Â¯Â¯
a
b
c
d
Â¯Â¯Â¯Â¯ Ì¸= 0, namely
x = âˆ†1
âˆ†,
y = âˆ†2
âˆ†,
where
âˆ†1 =
Â¯Â¯Â¯Â¯
e
b
f
d
Â¯Â¯Â¯Â¯
and
âˆ†2 =
Â¯Â¯Â¯Â¯
a
e
c
f
Â¯Â¯Â¯Â¯ .
Proof. Suppose âˆ†Ì¸= 0. Then A =
Â· a
b
c
d
Â¸
has inverse
Aâˆ’1 = âˆ†âˆ’1
Â·
d
âˆ’b
âˆ’c
a
Â¸
and we know that the system
A
Â· x
y
Â¸
=
Â· e
f
Â¸

40
CHAPTER 2. MATRICES
has the unique solution
Â· x
y
Â¸
= Aâˆ’1
Â· e
f
Â¸
=
1
âˆ†
Â·
d
âˆ’b
âˆ’c
a
Â¸ Â· e
f
Â¸
=
1
âˆ†
Â·
de âˆ’bf
âˆ’ce + af
Â¸
= 1
âˆ†
Â· âˆ†1
âˆ†2
Â¸
=
Â· âˆ†1/âˆ†
âˆ†2/âˆ†
Â¸
.
Hence x = âˆ†1/âˆ†, y = âˆ†2/âˆ†.
COROLLARY 2.5.1 The homogeneous system
ax + by
=
0
cx + dy
=
0
has only the trivial solution if âˆ†=
Â¯Â¯Â¯Â¯
a
b
c
d
Â¯Â¯Â¯Â¯ Ì¸= 0.
EXAMPLE 2.5.4 The system
7x + 8y
=
100
2x âˆ’9y
=
10
has the unique solution x = âˆ†1/âˆ†, y = âˆ†2/âˆ†, where
âˆ†=
Â¯Â¯Â¯Â¯
7
8
2
âˆ’9
Â¯Â¯Â¯Â¯ = âˆ’79, âˆ†1 =
Â¯Â¯Â¯Â¯
100
8
10
âˆ’9
Â¯Â¯Â¯Â¯ = âˆ’980, âˆ†2 =
Â¯Â¯Â¯Â¯
7
100
2
10
Â¯Â¯Â¯Â¯ = âˆ’130.
So x = 980
79 and y = 130
79 .
THEOREM 2.5.6 Let A be a square matrix. If A is nonâ€“singular, the
homogeneous system AX = 0 has only the trivial solution. Equivalently,
if the homogenous system AX = 0 has a nonâ€“trivial solution, then A is
singular.
Proof. If A is nonâ€“singular and AX = 0, then X = Aâˆ’10 = 0.
REMARK 2.5.4 If Aâˆ—1, . . . , Aâˆ—n denote the columns of A, then the equa-
tion
AX = x1Aâˆ—1 + . . . + xnAâˆ—n
holds. Consequently theorem 2.5.6 tells us that if there exist scalars x1, . . . , xn,
not all zero, such that
x1Aâˆ—1 + . . . + xnAâˆ—n = 0,

2.5. NONâ€“SINGULAR MATRICES
41
that is, if the columns of A are linearly dependent, then A is singular. An
equivalent way of saying that the columns of A are linearly dependent is that
one of the columns of A is expressible as a sum of certain scalar multiples
of the remaining columns of A; that is one column is a linear combination
of the remaining columns.
EXAMPLE 2.5.5
A =
ï£®
ï£°
1
2
3
1
0
1
3
4
7
ï£¹
ï£»
is singular. For it can be veriï¬ed that A has reduced rowâ€“echelon form
ï£®
ï£°
1
0
1
0
1
1
0
0
0
ï£¹
ï£»
and consequently AX = 0 has a nonâ€“trivial solution x = âˆ’1, y = âˆ’1, z = 1.
REMARK 2.5.5 More generally, if A is rowâ€“equivalent to a matrix con-
taining a zero row, then A is singular. For then the homogeneous system
AX = 0 has a nonâ€“trivial solution.
An important class of nonâ€“singular matrices is that of the elementary
row matrices.
DEFINITION 2.5.2 (Elementary row matrices) There are three types,
Eij, Ei(t), Eij(t), corresponding to the three kinds of elementary row oper-
ation:
1. Eij, (i Ì¸= j) is obtained from the identity matrix In by interchanging
rows i and j.
2. Ei(t), (t Ì¸= 0) is obtained by multiplying the iâ€“th row of In by t.
3. Eij(t), (i Ì¸= j) is obtained from In by adding t times the jâ€“th row of
In to the iâ€“th row.
EXAMPLE 2.5.6 (n = 3.)
E23 =
ï£®
ï£°
1
0
0
0
0
1
0
1
0
ï£¹
ï£», E2(âˆ’1) =
ï£®
ï£°
1
0
0
0
âˆ’1
0
0
0
1
ï£¹
ï£», E23(âˆ’1) =
ï£®
ï£°
1
0
0
0
1
âˆ’1
0
0
1
ï£¹
ï£».

42
CHAPTER 2. MATRICES
The elementary row matrices have the following distinguishing property:
THEOREM 2.5.7 If a matrix A is preâ€“multiplied by an elementary rowâ€“
matrix, the resulting matrix is the one obtained by performing the corre-
sponding elementary rowâ€“operation on A.
EXAMPLE 2.5.7
E23
ï£®
ï£°
a
b
c
d
e
f
ï£¹
ï£»=
ï£®
ï£°
1
0
0
0
0
1
0
1
0
ï£¹
ï£»
ï£®
ï£°
a
b
c
d
e
f
ï£¹
ï£»=
ï£®
ï£°
a
b
e
f
c
d
ï£¹
ï£».
COROLLARY 2.5.2 The three types of elementary rowâ€“matrices are nonâ€“
singular. Indeed
1. Eâˆ’1
ij = Eij;
2. Eâˆ’1
i
(t) = Ei(tâˆ’1);
3. (Eij(t))âˆ’1 = Eij(âˆ’t).
Proof.
Taking A = In in the above theorem, we deduce the following
equations:
EijEij
=
In
Ei(t)Ei(tâˆ’1)
=
In = Ei(tâˆ’1)Ei(t)
if t Ì¸= 0
Eij(t)Eij(âˆ’t)
=
In = Eij(âˆ’t)Eij(t).
EXAMPLE 2.5.8 Find the 3 Ã— 3 matrix A = E3(5)E23(2)E12 explicitly.
Also ï¬nd Aâˆ’1.
Solution.
A = E3(5)E23(2)
ï£®
ï£°
0
1
0
1
0
0
0
0
1
ï£¹
ï£»= E3(5)
ï£®
ï£°
0
1
0
1
0
2
0
0
1
ï£¹
ï£»=
ï£®
ï£°
0
1
0
1
0
2
0
0
5
ï£¹
ï£».
To ï¬nd Aâˆ’1, we have
Aâˆ’1
=
(E3(5)E23(2)E12)âˆ’1
=
Eâˆ’1
12 (E23(2))âˆ’1 (E3(5))âˆ’1
=
E12E23(âˆ’2)E3(5âˆ’1)

2.5. NONâ€“SINGULAR MATRICES
43
=
E12E23(âˆ’2)
ï£®
ï£°
1
0
0
0
1
0
0
0
1
5
ï£¹
ï£»
=
E12
ï£®
ï£°
1
0
0
0
1
âˆ’2
5
0
0
1
5
ï£¹
ï£»=
ï£®
ï£°
0
1
âˆ’2
5
1
0
0
0
0
1
5
ï£¹
ï£».
REMARK 2.5.6 Recall that A and B are rowâ€“equivalent if B is obtained
from A by a sequence of elementary row operations. If E1, . . . , Er are the
respective corresponding elementary row matrices, then
B = Er (. . . (E2(E1A)) . . .) = (Er . . . E1)A = PA,
where P = Er . . . E1 is nonâ€“singular. Conversely if B = PA, where P is
nonâ€“singular, then A is rowâ€“equivalent to B. For as we shall now see, P is
in fact a product of elementary row matrices.
THEOREM 2.5.8 Let A be nonâ€“singular n Ã— n matrix. Then
(i) A is rowâ€“equivalent to In,
(ii) A is a product of elementary row matrices.
Proof. Assume that A is nonâ€“singular and let B be the reduced rowâ€“echelon
form of A. Then B has no zero rows, for otherwise the equation AX = 0
would have a nonâ€“trivial solution. Consequently B = In.
It follows that there exist elementary row matrices E1, . . . , Er such that
Er (. . . (E1A) . . .) = B = In and hence A = Eâˆ’1
1
. . . Eâˆ’1
r , a product of
elementary row matrices.
THEOREM 2.5.9 Let A be n Ã— n and suppose that A is rowâ€“equivalent
to In. Then A is nonâ€“singular and Aâˆ’1 can be found by performing the
same sequence of elementary row operations on In as were used to convert
A to In.
Proof. Suppose that Er . . . E1A = In. In other words BA = In, where
B = Er . . . E1 is nonâ€“singular. Then Bâˆ’1(BA) = Bâˆ’1In and so A = Bâˆ’1,
which is nonâ€“singular.
Also Aâˆ’1 =
Â¡
Bâˆ’1Â¢âˆ’1 = B = Er ((. . . (E1In) . . .), which shows that Aâˆ’1
is obtained from In by performing the same sequence of elementary row
operations as were used to convert A to In.

44
CHAPTER 2. MATRICES
REMARK 2.5.7 It follows from theorem 2.5.9 that if A is singular, then
A is rowâ€“equivalent to a matrix whose last row is zero.
EXAMPLE 2.5.9 Show that A =
Â· 1
2
1
1
Â¸
is nonâ€“singular, ï¬nd Aâˆ’1 and
express A as a product of elementary row matrices.
Solution. We form the partitioned matrix [A|I2] which consists of A followed
by I2. Then any sequence of elementary row operations which reduces A to
I2 will reduce I2 to Aâˆ’1. Here
[A|I2] =
Â· 1
2
1
0
1
1
0
1
Â¸
R2 â†’R2 âˆ’R1
Â· 1
2
1
0
0
âˆ’1
âˆ’1
1
Â¸
R2 â†’(âˆ’1)R2
Â· 1
2
1
0
0
1
1
âˆ’1
Â¸
R1 â†’R1 âˆ’2R2
Â· 1
0
âˆ’1
2
0
1
1
âˆ’1
Â¸
.
Hence A is rowâ€“equivalent to I2 and A is nonâ€“singular. Also
Aâˆ’1 =
Â· âˆ’1
2
1
âˆ’1
Â¸
.
We also observe that
E12(âˆ’2)E2(âˆ’1)E21(âˆ’1)A = I2.
Hence
Aâˆ’1
=
E12(âˆ’2)E2(âˆ’1)E21(âˆ’1)
A
=
E21(1)E2(âˆ’1)E12(2).
The next result is the converse of Theorem 2.5.6 and is useful for proving
the nonâ€“singularity of certain types of matrices.
THEOREM 2.5.10 Let A be an n Ã— n matrix with the property that
the homogeneous system AX = 0 has only the trivial solution. Then A is
nonâ€“singular. Equivalently, if A is singular, then the homogeneous system
AX = 0 has a nonâ€“trivial solution.

2.5. NONâ€“SINGULAR MATRICES
45
Proof. If A is n Ã— n and the homogeneous system AX = 0 has only the
trivial solution, then it follows that the reduced rowâ€“echelon form B of A
cannot have zero rows and must therefore be In. Hence A is nonâ€“singular.
COROLLARY 2.5.3 Suppose that A and B are n Ã— n and AB = In.
Then BA = In.
Proof. Let AB = In, where A and B are n Ã— n. We ï¬rst show that B
is nonâ€“singular. Assume BX = 0. Then A(BX) = A0 = 0, so (AB)X =
0, InX = 0 and hence X = 0.
Then from AB = In we deduce (AB)Bâˆ’1 = InBâˆ’1 and hence A = Bâˆ’1.
The equation BBâˆ’1 = In then gives BA = In.
Before we give the next example of the above criterion for non-singularity,
we introduce an important matrix operation.
DEFINITION 2.5.3 (The transpose of a matrix) Let A be an m Ã— n
matrix. Then At, the transpose of A, is the matrix obtained by interchanging
the rows and columns of A. In other words if A = [aij], then
Â¡
AtÂ¢
ji = aij.
Consequently At is n Ã— m.
The transpose operation has the following properties:
1.
Â¡
AtÂ¢t = A;
2. (A Â± B)t = At Â± Bt if A and B are m Ã— n;
3. (sA)t = sAt if s is a scalar;
4. (AB)t = BtAt if A is m Ã— n and B is n Ã— p;
5. If A is nonâ€“singular, then At is also nonâ€“singular and
Â¡
AtÂ¢âˆ’1 =
Â¡
Aâˆ’1Â¢t ;
6. XtX = x2
1 + . . . + x2
n if X = [x1, . . . , xn]t is a column vector.
We prove only the fourth property. First check that both (AB)t and BtAt
have the same size (p Ã— m).
Moreover, corresponding elements of both
matrices are equal. For if A = [aij] and B = [bjk], we have
Â¡
(AB)tÂ¢
ki
=
(AB)ik
=
n
X
j=1
aijbjk

46
CHAPTER 2. MATRICES
=
n
X
j=1
Â¡
BtÂ¢
kj
Â¡
AtÂ¢
ji
=
Â¡
BtAtÂ¢
ki .
There are two important classes of matrices that can be deï¬ned concisely
in terms of the transpose operation.
DEFINITION 2.5.4 (Symmetric matrix) A real matrix A is called sym-
metric if At = A. In other words A is square (n Ã— n say) and aji = aij for
all 1 â‰¤i â‰¤n, 1 â‰¤j â‰¤n. Hence
A =
Â· a
b
b
c
Â¸
is a general 2 Ã— 2 symmetric matrix.
DEFINITION 2.5.5 (Skewâ€“symmetric matrix) A real matrix A is called
skewâ€“symmetric if At = âˆ’A. In other words A is square (n Ã— n say) and
aji = âˆ’aij for all 1 â‰¤i â‰¤n, 1 â‰¤j â‰¤n.
REMARK 2.5.8 Taking i = j in the deï¬nition of skewâ€“symmetric matrix
gives aii = âˆ’aii and so aii = 0. Hence
A =
Â·
0
b
âˆ’b
0
Â¸
is a general 2 Ã— 2 skewâ€“symmetric matrix.
We can now state a second application of the above criterion for nonâ€“
singularity.
COROLLARY 2.5.4 Let B be an n Ã— n skewâ€“symmetric matrix. Then
A = In âˆ’B is nonâ€“singular.
Proof. Let A = In âˆ’B, where Bt = âˆ’B. By Theorem 2.5.10 it suï¬ƒces to
show that AX = 0 implies X = 0.
We have (In âˆ’B)X = 0, so X = BX. Hence XtX = XtBX.
Taking transposes of both sides gives
(XtBX)t
=
(XtX)t
XtBt(Xt)t
=
Xt(Xt)t
Xt(âˆ’B)X
=
XtX
âˆ’XtBX
=
XtX = XtBX.
Hence XtX = âˆ’XtX and XtX = 0. But if X = [x1, . . . , xn]t, then XtX =
x2
1 + . . . + x2
n = 0 and hence x1 = 0, . . . , xn = 0.

2.6. LEAST SQUARES SOLUTION OF EQUATIONS
47
2.6
Least squares solution of equations
Suppose AX = B represents a system of linear equations with real coeï¬ƒ-
cients which may be inconsistent, because of the possibility of experimental
errors in determining A or B. For example, the system
x
=
1
y
=
2
x + y
=
3.001
is inconsistent.
It can be proved that the associated system AtAX = AtB is always
consistent and that any solution of this system minimizes the sum r2
1 +. . .+
r2
m, where r1, . . . , rm (the residuals) are deï¬ned by
ri = ai1x1 + . . . + ainxn âˆ’bi,
for i = 1, . . . , m. The equations represented by AtAX = AtB are called the
normal equations corresponding to the system AX = B and any solution
of the system of normal equations is called a least squares solution of the
original system.
EXAMPLE 2.6.1 Find a least squares solution of the above inconsistent
system.
Solution. Here A =
ï£®
ï£°
1
0
0
1
1
1
ï£¹
ï£», X =
Â· x
y
Â¸
, B =
ï£®
ï£°
1
2
3.001
ï£¹
ï£».
Then AtA =
Â· 1
0
1
0
1
1
Â¸ ï£®
ï£°
1
0
0
1
1
1
ï£¹
ï£»=
Â· 2
1
1
2
Â¸
.
Also AtB =
Â· 1
0
1
0
1
1
Â¸ ï£®
ï£°
1
2
3.001
ï£¹
ï£»=
Â· 4.001
5.001
Â¸
.
So the normal equations are
2x + y
=
4.001
x + 2y
=
5.001
which have the unique solution
x = 3.001
3
,
y = 6.001
3
.

48
CHAPTER 2. MATRICES
EXAMPLE 2.6.2 Points (x1, y1), . . . , (xn, yn) are experimentally deter-
mined and should lie on a line y = mx + c. Find a least squares solution to
the problem.
Solution. The points have to satisfy
mx1 + c
=
y1
...
mxn + c
=
yn,
or Ax = B, where
A =
ï£®
ï£¯ï£°
x1
1
...
...
xn
1
ï£¹
ï£ºï£», X =
Â· m
c
Â¸
, B =
ï£®
ï£¯ï£°
y1
...
yn
ï£¹
ï£ºï£».
The normal equations are given by (AtA)X = AtB. Here
AtA =
Â· x1
. . .
xn
1
. . .
1
Â¸
ï£®
ï£¯ï£°
x1
1
...
...
xn
1
ï£¹
ï£ºï£»=
Â· x2
1 + . . . + x2
n
x1 + . . . + xn
x1 + . . . + xn
n
Â¸
Also
AtB =
Â· x1
. . .
xn
1
. . .
1
Â¸
ï£®
ï£¯ï£°
y1
...
yn
ï£¹
ï£ºï£»=
Â· x1y1 + . . . + xnyn
y1 + . . . + yn
Â¸
.
It is not diï¬ƒcult to prove that
âˆ†= det (AtA) =
X
1â‰¤i<jâ‰¤n
(xi âˆ’xj)2,
which is positive unless x1 = . . . = xn. Hence if not all of x1, . . . , xn are
equal, AtA is nonâ€“singular and the normal equations have a unique solution.
This can be shown to be
m = 1
âˆ†
X
1â‰¤i<jâ‰¤n
(xi âˆ’xj)(yi âˆ’yj), c = 1
âˆ†
X
1â‰¤i<jâ‰¤n
(xiyj âˆ’xjyi)(xi âˆ’xj).
REMARK 2.6.1 The matrix AtA is symmetric.

2.7. PROBLEMS
49
2.7
PROBLEMS
1. Let A =
Â·
1
4
âˆ’3
1
Â¸
.
Prove that A is nonâ€“singular, ï¬nd Aâˆ’1 and
express A as a product of elementary row matrices.
[Answer: Aâˆ’1 =
Â·
1
13
âˆ’4
13
3
13
1
13
Â¸
,
A = E21(âˆ’3)E2(13)E12(4) is one such decomposition.]
2. A square matrix D = [dij] is called diagonal if dij = 0 for i Ì¸= j. (That
is the oï¬€â€“diagonal elements are zero.) Prove that preâ€“multiplication
of a matrix A by a diagonal matrix D results in matrix DA whose
rows are the rows of A multiplied by the respective diagonal elements
of D. State and prove a similar result for postâ€“multiplication by a
diagonal matrix.
Let diag (a1, . . . , an) denote the diagonal matrix whose diagonal ele-
ments dii are a1, . . . , an, respectively. Show that
diag (a1, . . . , an)diag (b1, . . . , bn) = diag (a1b1, . . . , anbn)
and deduce that if a1 . . . an Ì¸= 0, then diag (a1, . . . , an) is nonâ€“singular
and
(diag (a1, . . . , an))âˆ’1 = diag (aâˆ’1
1 , . . . , aâˆ’1
n ).
Also prove that diag (a1, . . . , an) is singular if ai = 0 for some i.
3. Let A =
ï£®
ï£°
0
0
2
1
2
6
3
7
9
ï£¹
ï£». Prove that A is nonâ€“singular, ï¬nd Aâˆ’1 and
express A as a product of elementary row matrices.
[Answers: Aâˆ’1 =
ï£®
ï£°
âˆ’12
7
âˆ’2
9
2
âˆ’3
1
1
2
0
0
ï£¹
ï£»,
A = E12E31(3)E23E3(2)E12(2)E13(24)E23(âˆ’9) is one such decompo-
sition.]

50
CHAPTER 2. MATRICES
4. Find the rational number k for which the matrix A =
ï£®
ï£°
1
2
k
3
âˆ’1
1
5
3
âˆ’5
ï£¹
ï£»
is singular. [Answer: k = âˆ’3.]
5. Prove that A =
Â·
1
2
âˆ’2
âˆ’4
Â¸
is singular and ï¬nd a nonâ€“singular matrix
P such that PA has last row zero.
6. If A =
Â·
1
4
âˆ’3
1
Â¸
, verify that A2 âˆ’2A + 13I2 = 0 and deduce that
Aâˆ’1 = âˆ’1
13(A âˆ’2I2).
7. Let A =
ï£®
ï£°
1
1
âˆ’1
0
0
1
2
1
2
ï£¹
ï£».
(i) Verify that A3 = 3A2 âˆ’3A + I3.
(ii) Express A4 in terms of A2, A and I3 and hence calculate A4
explicitly.
(iii) Use (i) to prove that A is nonâ€“singular and ï¬nd Aâˆ’1 explicitly.
[Answers: (ii) A4 = 6A2 âˆ’8A + 3I3 =
ï£®
ï£°
âˆ’11
âˆ’8
âˆ’4
12
9
4
20
16
5
ï£¹
ï£»;
(iii) Aâˆ’1 = A2 âˆ’3A + 3I3 =
ï£®
ï£°
âˆ’1
âˆ’3
1
2
4
âˆ’1
0
1
0
ï£¹
ï£».]
8.
(i) Let B be an nÃ—n matrix such that B3 = 0. If A = In âˆ’B, prove
that A is nonâ€“singular and Aâˆ’1 = In + B + B2.
Show that the system of linear equations AX = b has the solution
X = b + Bb + B2b.
(ii) If B =
ï£®
ï£°
0
r
s
0
0
t
0
0
0
ï£¹
ï£», verify that B3 = 0 and use (i) to determine
(I3 âˆ’B)âˆ’1 explicitly.

2.7. PROBLEMS
51
[Answer:
ï£®
ï£°
1
r
s + rt
0
1
t
0
0
1
ï£¹
ï£».]
9. Let A be n Ã— n.
(i) If A2 = 0, prove that A is singular.
(ii) If A2 = A and A Ì¸= In, prove that A is singular.
10. Use Question 7 to solve the system of equations
x + y âˆ’z
=
a
z
=
b
2x + y + 2z
=
c
where a, b, c are given rationals. Check your answer using the Gaussâ€“
Jordan algorithm.
[Answer: x = âˆ’a âˆ’3b + c, y = 2a + 4b âˆ’c, z = b.]
11. Determine explicitly the following products of 3 Ã— 3 elementary row
matrices.
(i) E12E23
(ii) E1(5)E12
(iii) E12(3)E21(âˆ’3)
(iv) (E1(100))âˆ’1
(v) Eâˆ’1
12
(vi) (E12(7))âˆ’1
(vii) (E12(7)E31(1))âˆ’1.
[Answers: (i)
ï£®
ï£°
0
0
1
1
0
0
0
1
0
ï£¹
ï£»(ii)
ï£®
ï£°
0
5
0
1
0
0
0
0
1
ï£¹
ï£»(iii)
ï£®
ï£°
âˆ’8
3
0
âˆ’3
1
0
0
0
1
ï£¹
ï£»
(iv)
ï£®
ï£°
1
100
0
0
0
1
0
0
0
1
ï£¹
ï£»(v)
ï£®
ï£°
0
1
0
1
0
0
0
0
1
ï£¹
ï£»(vi)
ï£®
ï£°
1
âˆ’7
0
0
1
0
0
0
1
ï£¹
ï£»(vii)
ï£®
ï£°
1
âˆ’7
0
0
1
0
âˆ’1
7
1
ï£¹
ï£».]
12. Let A be the following product of 4 Ã— 4 elementary row matrices:
A = E3(2)E14E42(3).
Find A and Aâˆ’1 explicitly.
[Answers: A =
ï£®
ï£¯ï£¯ï£°
0
3
0
1
0
1
0
0
0
0
2
0
1
0
0
0
ï£¹
ï£ºï£ºï£», Aâˆ’1 =
ï£®
ï£¯ï£¯ï£°
0
0
0
1
0
1
0
0
0
0
1
2
0
1
âˆ’3
0
0
ï£¹
ï£ºï£ºï£».]

52
CHAPTER 2. MATRICES
13. Determine which of the following matrices over Z2 are nonâ€“singular
and ï¬nd the inverse, where possible.
(a)
ï£®
ï£¯ï£¯ï£°
1
1
0
1
0
0
1
1
1
1
1
1
1
0
0
1
ï£¹
ï£ºï£ºï£»
(b)
ï£®
ï£¯ï£¯ï£°
1
1
0
1
0
1
1
1
1
0
1
0
1
1
0
1
ï£¹
ï£ºï£ºï£».
[Answer: (a)
ï£®
ï£¯ï£¯ï£°
1
1
1
1
1
0
0
1
1
0
1
0
1
1
1
0
ï£¹
ï£ºï£ºï£».]
14. Determine which of the following matrices are nonâ€“singular and ï¬nd
the inverse, where possible.
(a)
ï£®
ï£°
1
1
1
âˆ’1
1
0
2
0
0
ï£¹
ï£»
(b)
ï£®
ï£°
2
2
4
1
0
1
0
1
0
ï£¹
ï£»
(c)
ï£®
ï£°
4
6
âˆ’3
0
0
7
0
0
5
ï£¹
ï£»
(d)
ï£®
ï£°
2
0
0
0
âˆ’5
0
0
0
7
ï£¹
ï£»(e)
ï£®
ï£¯ï£¯ï£°
1
2
4
6
0
1
2
0
0
0
1
2
0
0
0
2
ï£¹
ï£ºï£ºï£»(f)
ï£®
ï£°
1
2
3
4
5
6
5
7
9
ï£¹
ï£».
[Answers: (a)
ï£®
ï£°
0
0
1
2
0
1
1
2
1
âˆ’1
âˆ’1
ï£¹
ï£»(b)
ï£®
ï£°
âˆ’1
2
2
1
0
0
1
1
2
âˆ’1
âˆ’1
ï£¹
ï£»(d)
ï£®
ï£°
1
2
0
0
0
âˆ’1
5
0
0
0
1
7
ï£¹
ï£»
(e)
ï£®
ï£¯ï£¯ï£°
1
âˆ’2
0
âˆ’3
0
1
âˆ’2
2
0
0
1
âˆ’1
0
0
0
1
2
ï£¹
ï£ºï£ºï£».]
15. Let A be a nonâ€“singular n Ã— n matrix. Prove that At is nonâ€“singular
and that (At)âˆ’1 = (Aâˆ’1)t.
16. Prove that A =
Â· a
b
c
d
Â¸
has no inverse if ad âˆ’bc = 0.
[Hint: Use the equation A2 âˆ’(a + d)A + (ad âˆ’bc)I2 = 0.]

2.7. PROBLEMS
53
17. Prove that the real matrix A =
ï£®
ï£°
1
a
b
âˆ’a
1
c
âˆ’b
âˆ’c
1
ï£¹
ï£»is nonâ€“singular by
proving that A is rowâ€“equivalent to I3.
18. If P âˆ’1AP = B, prove that P âˆ’1AnP = Bn for n â‰¥1.
19. Let A =
Â· 2
3
1
4
1
3
3
4
Â¸
, P =
Â·
1
3
âˆ’1
4
Â¸
. Verify that P âˆ’1AP =
Â·
5
12
0
0
1
Â¸
and deduce that
An = 1
7
Â· 3
3
4
4
Â¸
+ 1
7
Âµ 5
12
Â¶n Â·
4
âˆ’3
âˆ’4
3
Â¸
.
20. Let A =
Â· a
b
c
d
Â¸
be a Markov matrix; that is a matrix whose elements
are nonâ€“negative and satisfy a+c = 1 = b+d. Also let P =
Â· b
1
c
âˆ’1
Â¸
.
Prove that if A Ì¸= I2 then
(i) P is nonâ€“singular and P âˆ’1AP =
Â· 1
0
0
a + d âˆ’1
Â¸
,
(ii) An â†’
1
b + c
Â· b
b
c
c
Â¸
as n â†’âˆ, if A Ì¸=
Â· 0
1
1
0
Â¸
.
21. If X =
ï£®
ï£°
1
2
3
4
5
6
ï£¹
ï£»and Y =
ï£®
ï£°
âˆ’1
3
4
ï£¹
ï£», ï¬nd XXt, XtX, Y Y t, Y tY .
[Answers:
ï£®
ï£°
5
11
17
11
25
39
17
39
61
ï£¹
ï£»,
Â· 35
44
44
56
Â¸
,
ï£®
ï£°
1
âˆ’3
âˆ’4
âˆ’3
9
12
âˆ’4
12
16
ï£¹
ï£», 26.]
22. Prove that the system of linear equations
x + 2y
=
4
x + y
=
5
3x + 5y
=
12
is inconsistent and ï¬nd a least squares solution of the system.
[Answer: x = 6, y = âˆ’7/6.]

54
CHAPTER 2. MATRICES
23. The points (0, 0), (1, 0), (2, âˆ’1), (3, 4), (4, 8) are required to lie on a
parabola y = a + bx + cx2. Find a least squares solution for a, b, c.
Also prove that no parabola passes through these points.
[Answer: a = 1
5, b = âˆ’2, c = 1.]
24. If A is a symmetric nÃ—n real matrix and B is nÃ—m, prove that BtAB
is a symmetric m Ã— m matrix.
25. If A is m Ã— n and B is n Ã— m, prove that AB is singular if m > n.
26. Let A and B be n Ã— n. If A or B is singular, prove that AB is also
singular.

Chapter 3
SUBSPACES
3.1
Introduction
Throughout this chapter, we will be studying F n, the set of all nâ€“dimensional
column vectors with components from a ï¬eld F. We continue our study of
matrices by considering an important class of subsets of F n called subspaces.
These arise naturally for example, when we solve a system of m linear ho-
mogeneous equations in n unknowns.
We also study the concept of linear dependence of a family of vectors.
This was introduced brieï¬‚y in Chapter 2, Remark 2.5.4. Other topics dis-
cussed are the row space, column space and null space of a matrix over F,
the dimension of a subspace, particular examples of the latter being the rank
and nullity of a matrix.
3.2
Subspaces of F n
DEFINITION 3.2.1 A subset S of F n is called a subspace of F n if
1. The zero vector belongs to S; (that is, 0 âˆˆS);
2. If u âˆˆS and v âˆˆS, then u + v âˆˆS; (S is said to be closed under
vector addition);
3. If u âˆˆS and t âˆˆF, then tu âˆˆS; (S is said to be closed under scalar
multiplication).
EXAMPLE 3.2.1 Let A âˆˆMmÃ—n(F). Then the set of vectors X âˆˆF n
satisfying AX = 0 is a subspace of F n called the null space of A and is
denoted here by N(A). (It is sometimes called the solution space of A.)
55

56
CHAPTER 3. SUBSPACES
Proof. (1) A0 = 0, so 0 âˆˆN(A); (2) If X, Y âˆˆN(A), then AX = 0 and
AY = 0, so A(X + Y ) = AX + AY = 0 + 0 = 0 and so X + Y âˆˆN(A); (3)
If X âˆˆN(A) and t âˆˆF, then A(tX) = t(AX) = t0 = 0, so tX âˆˆN(A).
For example, if A =
Â· 1
0
0
1
Â¸
, then N(A) = {0}, the set consisting of
just the zero vector. If A =
Â· 1
2
2
4
Â¸
, then N(A) is the set of all scalar
multiples of [âˆ’2, 1]t.
EXAMPLE 3.2.2 Let X1, . . . , Xm âˆˆF n. Then the set consisting of all
linear combinations x1X1 + Â· Â· Â· + xmXm, where x1, . . . , xm âˆˆF, is a sub-
space of F n. This subspace is called the subspace spanned or generated by
X1, . . . , Xm and is denoted here by âŸ¨X1, . . . , XmâŸ©. We also call X1, . . . , Xm
a spanning family for S = âŸ¨X1, . . . , XmâŸ©.
Proof.
(1) 0 = 0X1 + Â· Â· Â· + 0Xm, so 0 âˆˆâŸ¨X1, . . . , XmâŸ©; (2) If X, Y âˆˆ
âŸ¨X1, . . . , XmâŸ©, then X = x1X1 + Â· Â· Â· + xmXm and Y = y1X1 + Â· Â· Â· + ymXm,
so
X + Y
=
(x1X1 + Â· Â· Â· + xmXm) + (y1X1 + Â· Â· Â· + ymXm)
=
(x1 + y1)X1 + Â· Â· Â· + (xm + ym)Xm âˆˆâŸ¨X1, . . . , XmâŸ©.
(3) If X âˆˆâŸ¨X1, . . . , XmâŸ©and t âˆˆF, then
X
=
x1X1 + Â· Â· Â· + xmXm
tX
=
t(x1X1 + Â· Â· Â· + xmXm)
=
(tx1)X1 + Â· Â· Â· + (txm)Xm âˆˆâŸ¨X1, . . . , XmâŸ©.
For example, if A âˆˆMmÃ—n(F), the subspace generated by the columns of A
is an important subspace of F m and is called the column space of A. The
column space of A is denoted here by C(A). Also the subspace generated
by the rows of A is a subspace of F n and is called the row space of A and is
denoted by R(A).
EXAMPLE 3.2.3 For example F n = âŸ¨E1, . . . , EnâŸ©, where E1, . . . , En are
the nâ€“dimensional unit vectors. For if X = [x1, . . . , xn]t âˆˆF n, then X =
x1E1 + Â· Â· Â· + xnEn.
EXAMPLE 3.2.4 Find a spanning family for the subspace S of R3 deï¬ned
by the equation 2x âˆ’3y + 5z = 0.

3.2. SUBSPACES OF F N
57
Solution. (S is in fact the null space of [2, âˆ’3, 5], so S is indeed a subspace
of R3.)
If [x, y, z]t âˆˆS, then x = 3
2y âˆ’5
2z. Then
ï£®
ï£°
x
y
z
ï£¹
ï£»=
ï£®
ï£°
3
2y âˆ’5
2z
y
z
ï£¹
ï£»= y
ï£®
ï£°
3
2
1
0
ï£¹
ï£»+ z
ï£®
ï£°
âˆ’5
2
0
1
ï£¹
ï£»
and conversely. Hence [ 3
2, 1, 0]t and [âˆ’5
2, 0, 1]t form a spanning family for
S.
The following result is easy to prove:
LEMMA 3.2.1 Suppose each of X1, . . . , Xr is a linear combination of
Y1, . . . , Ys. Then any linear combination of X1, . . . , Xr is a linear combi-
nation of Y1, . . . , Ys.
As a corollary we have
THEOREM 3.2.1 Subspaces âŸ¨X1, . . . , XrâŸ©and âŸ¨Y1, . . . , YsâŸ©are equal if
each of X1, . . . , Xr is a linear combination of Y1, . . . , Ys and each of Y1, . . . , Ys
is a linear combination of X1, . . . , Xr.
COROLLARY 3.2.1 Subspaces âŸ¨X1, . . . , Xr, Z1, . . . , ZtâŸ©and âŸ¨X1, . . . , XrâŸ©
are equal if each of Z1, . . . , Zt is a linear combination of X1, . . . , Xr.
EXAMPLE 3.2.5 If X and Y are vectors in Rn, then
âŸ¨X, Y âŸ©= âŸ¨X + Y, X âˆ’Y âŸ©.
Solution. Each of X + Y and X âˆ’Y is a linear combination of X and Y .
Also
X = 1
2(X + Y ) + 1
2(X âˆ’Y )
and
Y = 1
2(X + Y ) âˆ’1
2(X âˆ’Y ),
so each of X and Y is a linear combination of X + Y and X âˆ’Y .
There is an important application of Theorem 3.2.1 to row equivalent
matrices (see Deï¬nition 1.2.4):
THEOREM 3.2.2 If A is row equivalent to B, then R(A) = R(B).
Proof. Suppose that B is obtained from A by a sequence of elementary row
operations. Then it is easy to see that each row of B is a linear combination
of the rows of A. But A can be obtained from B by a sequence of elementary
operations, so each row of A is a linear combination of the rows of B. Hence
by Theorem 3.2.1, R(A) = R(B).

58
CHAPTER 3. SUBSPACES
REMARK 3.2.1 If A is row equivalent to B, it is not always true that
C(A) = C(B).
For example, if A =
Â· 1
1
1
1
Â¸
and B =
Â· 1
1
0
0
Â¸
, then B is in fact the
reduced rowâ€“echelon form of A. However we see that
C(A) =
Â¿Â· 1
1
Â¸
,
Â· 1
1
Â¸Ã€
=
Â¿Â· 1
1
Â¸Ã€
and similarly C(B) =
Â¿Â· 1
0
Â¸Ã€
.
Consequently C(A) Ì¸= C(B), as
Â· 1
1
Â¸
âˆˆC(A) but
Â· 1
1
Â¸
Ì¸âˆˆC(B).
3.3
Linear dependence
We now recall the deï¬nition of linear dependence and independence of a
family of vectors in F n given in Chapter 2.
DEFINITION 3.3.1 Vectors X1, . . . , Xm in F n are said to be linearly
dependent if there exist scalars x1, . . . , xm, not all zero, such that
x1X1 + Â· Â· Â· + xmXm = 0.
In other words, X1, . . . , Xm are linearly dependent if some Xi is expressible
as a linear combination of the remaining vectors.
X1, . . . , Xm are called linearly independent if they are not linearly depen-
dent. Hence X1, . . . , Xm are linearly independent if and only if the equation
x1X1 + Â· Â· Â· + xmXm = 0
has only the trivial solution x1 = 0, . . . , xm = 0.
EXAMPLE 3.3.1 The following three vectors in R3
X1 =
ï£®
ï£°
1
2
3
ï£¹
ï£»,
X2 =
ï£®
ï£°
âˆ’1
1
2
ï£¹
ï£»,
X3 =
ï£®
ï£°
âˆ’1
7
12
ï£¹
ï£»
are linearly dependent, as 2X1 + 3X2 + (âˆ’1)X3 = 0.

3.3. LINEAR DEPENDENCE
59
REMARK 3.3.1 If X1, . . . , Xm are linearly independent and
x1X1 + Â· Â· Â· + xmXm = y1X1 + Â· Â· Â· + ymXm,
then x1 = y1, . . . , xm = ym. For the equation can be rewritten as
(x1 âˆ’y1)X1 + Â· Â· Â· + (xm âˆ’ym)Xm = 0
and so x1 âˆ’y1 = 0, . . . , xm âˆ’ym = 0.
THEOREM 3.3.1 A family of m vectors in F n will be linearly dependent
if m > n. Equivalently, any linearly independent family of m vectors in F n
must satisfy m â‰¤n.
Proof. The equation
x1X1 + Â· Â· Â· + xmXm = 0
is equivalent to n homogeneous equations in m unknowns. By Theorem 1.5.1,
such a system has a nonâ€“trivial solution if m > n.
The following theorem is an important generalization of the last result
and is left as an exercise for the interested student:
THEOREM 3.3.2 A family of s vectors in âŸ¨X1, . . . , XrâŸ©will be linearly
dependent if s > r. Equivalently, a linearly independent family of s vectors
in âŸ¨X1, . . . , XrâŸ©must have s â‰¤r.
Here is a useful criterion for linear independence which is sometimes
called the leftâ€“toâ€“right test:
THEOREM 3.3.3 Vectors X1, . . . , Xm in F n are linearly independent if
(a) X1 Ì¸= 0;
(b) For each k with 1 < k â‰¤m, Xk is not a linear combination of
X1, . . . , Xkâˆ’1.
One application of this criterion is the following result:
THEOREM 3.3.4 Every subspace S of F n can be represented in the form
S = âŸ¨X1, . . . , XmâŸ©, where m â‰¤n.

60
CHAPTER 3. SUBSPACES
Proof. If S = {0}, there is nothing to prove â€“ we take X1 = 0 and m = 1.
So we assume S contains a nonâ€“zero vector X1; then âŸ¨X1âŸ©âŠ†S as S is a
subspace. If S = âŸ¨X1âŸ©, we are ï¬nished. If not, S will contain a vector X2,
not a linear combination of X1; then âŸ¨X1, X2âŸ©âŠ†S as S is a subspace. If
S = âŸ¨X1, X2âŸ©, we are ï¬nished. If not, S will contain a vector X3 which is
not a linear combination of X1 and X2. This process must eventually stop,
for at stage k we have constructed a family of k linearly independent vectors
X1, . . . , Xk, all lying in F n and hence k â‰¤n.
There is an important relationship between the columns of A and B, if
A is rowâ€“equivalent to B.
THEOREM 3.3.5 Suppose that A is row equivalent to B and let c1, . . . , cr
be distinct integers satisfying 1 â‰¤ci â‰¤n. Then
(a) Columns Aâˆ—c1, . . . , Aâˆ—cr of A are linearly dependent if and only if the
corresponding columns of B are linearly dependent; indeed more is
true:
x1Aâˆ—c1 + Â· Â· Â· + xrAâˆ—cr = 0 â‡”x1Bâˆ—c1 + Â· Â· Â· + xrBâˆ—cr = 0.
(b) Columns Aâˆ—c1, . . . , Aâˆ—cr of A are linearly independent if and only if the
corresponding columns of B are linearly independent.
(c) If 1 â‰¤cr+1 â‰¤n and cr+1 is distinct from c1, . . . , cr, then
Aâˆ—cr+1 = z1Aâˆ—c1 + Â· Â· Â· + zrAâˆ—cr â‡”Bâˆ—cr+1 = z1Bâˆ—c1 + Â· Â· Â· + zrBâˆ—cr.
Proof. First observe that if Y = [y1, . . . , yn]t is an nâ€“dimensional column
vector and A is m Ã— n, then
AY = y1Aâˆ—1 + Â· Â· Â· + ynAâˆ—n.
Also AY = 0 â‡”BY = 0, if B is row equivalent to A. Then (a) follows by
taking yi = xcj if i = cj and yi = 0 otherwise.
(b) is logically equivalent to (a), while (c) follows from (a) as
Aâˆ—cr+1 = z1Aâˆ—c1 + Â· Â· Â· + zrAâˆ—cr
â‡”
z1Aâˆ—c1 + Â· Â· Â· + zrAâˆ—cr + (âˆ’1)Aâˆ—cr+1 = 0
â‡”
z1Bâˆ—c1 + Â· Â· Â· + zrBâˆ—cr + (âˆ’1)Bâˆ—cr+1 = 0
â‡”
Bâˆ—cr+1 = z1Bâˆ—c1 + Â· Â· Â· + zrBâˆ—cr.

3.4. BASIS OF A SUBSPACE
61
EXAMPLE 3.3.2 The matrix
A =
ï£®
ï£°
1
1
5
1
4
2
âˆ’1
1
2
2
3
0
6
0
âˆ’3
ï£¹
ï£»
has reduced rowâ€“echelon form equal to
B =
ï£®
ï£°
1
0
2
0
âˆ’1
0
1
3
0
2
0
0
0
1
3
ï£¹
ï£».
We notice that Bâˆ—1, Bâˆ—2 and Bâˆ—4 are linearly independent and hence so are
Aâˆ—1, Aâˆ—2 and Aâˆ—4. Also
Bâˆ—3
=
2Bâˆ—1 + 3Bâˆ—2
Bâˆ—5
=
(âˆ’1)Bâˆ—1 + 2Bâˆ—2 + 3Bâˆ—4,
so consequently
Aâˆ—3
=
2Aâˆ—1 + 3Aâˆ—2
Aâˆ—5
=
(âˆ’1)Aâˆ—1 + 2Aâˆ—2 + 3Aâˆ—4.
3.4
Basis of a subspace
We now come to the important concept of basis of a vector subspace.
DEFINITION 3.4.1 Vectors X1, . . . , Xm belonging to a subspace S are
said to form a basis of S if
(a) Every vector in S is a linear combination of X1, . . . , Xm;
(b) X1, . . . , Xm are linearly independent.
Note that (a) is equivalent to the statement that S = âŸ¨X1, . . . , XmâŸ©as we
automatically have âŸ¨X1, . . . , XmâŸ©âŠ†S. Also, in view of Remark 3.3.1 above,
(a) and (b) are equivalent to the statement that every vector in S is uniquely
expressible as a linear combination of X1, . . . , Xm.
EXAMPLE 3.4.1 The unit vectors E1, . . . , En form a basis for F n.

62
CHAPTER 3. SUBSPACES
REMARK 3.4.1 The subspace {0}, consisting of the zero vector alone,
does not have a basis. For every vector in a linearly independent family
must necessarily be nonâ€“zero. (For example, if X1 = 0, then we have the
nonâ€“trivial linear relation
1X1 + 0X2 + Â· Â· Â· + 0Xm = 0
and X1, . . . , Xm would be linearly dependent.)
However if we exclude this case, every other subspace of F n has a basis:
THEOREM 3.4.1 A subspace of the form âŸ¨X1, . . . , XmâŸ©, where at least
one of X1, . . . , Xm is nonâ€“zero, has a basis Xc1, . . . , Xcr, where 1 â‰¤c1 <
Â· Â· Â· < cr â‰¤m.
Proof. (The leftâ€“toâ€“right algorithm). Let c1 be the least index k for which
Xk is nonâ€“zero. If c1 = m or if all the vectors Xk with k > c1 are linear
combinations of Xc1, terminate the algorithm and let r = 1. Otherwise let
c2 be the least integer k > c1 such that Xk is not a linear combination of
Xc1.
If c2 = m or if all the vectors Xk with k > c2 are linear combinations
of Xc1 and Xc2, terminate the algorithm and let r = 2.
Eventually the
algorithm will terminate at the râ€“th stage, either because cr = m, or because
all vectors Xk with k > cr are linear combinations of Xc1, . . . , Xcr.
Then it is clear by the construction of Xc1, . . . , Xcr, using Corollary 3.2.1
that
(a) âŸ¨Xc1, . . . , XcrâŸ©= âŸ¨X1, . . . , XmâŸ©;
(b) the vectors Xc1, . . . , Xcr are linearly independent by the leftâ€“toâ€“right
test.
Consequently Xc1, . . . , Xcr form a basis (called the leftâ€“toâ€“right basis) for
the subspace âŸ¨X1, . . . , XmâŸ©.
EXAMPLE 3.4.2 Let X and Y be linearly independent vectors in Rn.
Then the subspace âŸ¨0, 2X, X, âˆ’Y, X +Y âŸ©has leftâ€“toâ€“right basis consisting
of 2X, âˆ’Y .
A subspace S will in general have more than one basis. For example, any
permutation of the vectors in a basis will yield another basis. Given one
particular basis, one can determine all bases for S using a simple formula.
This is left as one of the problems at the end of this chapter.
We settle for the following important fact about bases:

3.4. BASIS OF A SUBSPACE
63
THEOREM 3.4.2 Any two bases for a subspace S must contain the same
number of elements.
Proof.
For if X1, . . . , Xr and Y1, . . . , Ys are bases for S, then Y1, . . . , Ys
form a linearly independent family in S = âŸ¨X1, . . . , XrâŸ©and hence s â‰¤r by
Theorem 3.3.2. Similarly r â‰¤s and hence r = s.
DEFINITION 3.4.2 This number is called the dimension of S and is
written dim S. Naturally we deï¬ne dim {0} = 0.
It follows from Theorem 3.3.1 that for any subspace S of F n, we must have
dim S â‰¤n.
EXAMPLE 3.4.3 If E1, . . . , En denote the nâ€“dimensional unit vectors in
F n, then dim âŸ¨E1, . . . , EiâŸ©= i for 1 â‰¤i â‰¤n.
The following result gives a useful way of exhibiting a basis.
THEOREM 3.4.3 A linearly independent family of m vectors in a sub-
space S, with dim S = m, must be a basis for S.
Proof.
Let X1, . . . , Xm be a linearly independent family of vectors in a
subspace S, where dim S = m. We have to show that every vector X âˆˆS is
expressible as a linear combination of X1, . . . , Xm. We consider the following
family of vectors in S: X1, . . . , Xm, X. This family contains m+1 elements
and is consequently linearly dependent by Theorem 3.3.2. Hence we have
x1X1 + Â· Â· Â· + xmXm + xm+1X = 0,
(3.1)
where not all of x1, . . . , xm+1 are zero. Now if xm+1 = 0, we would have
x1X1 + Â· Â· Â· + xmXm = 0,
with not all of x1, . . . , xm zero, contradictiong the assumption that X1 . . . , Xm
are linearly independent. Hence xm+1 Ì¸= 0 and we can use equation 3.1 to
express X as a linear combination of X1, . . . , Xm:
X = âˆ’x1
xm+1
X1 + Â· Â· Â· + âˆ’xm
xm+1
Xm.

64
CHAPTER 3. SUBSPACES
3.5
Rank and nullity of a matrix
We can now deï¬ne three important integers associated with a matrix.
DEFINITION 3.5.1 Let A âˆˆMmÃ—n(F). Then
(a) column rank A = dim C(A);
(b) row rank A = dim R(A);
(c) nullity A = dim N(A).
We will now see that the reduced rowâ€“echelon form B of a matrix A allows
us to exhibit bases for the row space, column space and null space of A.
Moreover, an examination of the number of elements in each of these bases
will immediately result in the following theorem:
THEOREM 3.5.1 Let A âˆˆMmÃ—n(F). Then
(a) column rank A = row rank A;
(b) column rank A+ nullity A = n.
Finding a basis for R(A): The r nonâ€“zero rows of B form a basis for R(A)
and hence row rank A = r.
For we have seen earlier that R(A) = R(B). Also
R(B)
=
âŸ¨B1âˆ—, . . . , Bmâˆ—âŸ©
=
âŸ¨B1âˆ—, . . . , Brâˆ—, 0 . . . , 0âŸ©
=
âŸ¨B1âˆ—, . . . , Brâˆ—âŸ©.
The linear independence of the nonâ€“zero rows of B is proved as follows: Let
the leading entries of rows 1, . . . , r of B occur in columns c1, . . . , cr. Suppose
that
x1B1âˆ—+ Â· Â· Â· + xrBrâˆ—= 0.
Then equating components c1, . . . , cr of both sides of the last equation, gives
x1 = 0, . . . , xr = 0, in view of the fact that B is in reduced rowâ€“ echelon
form.
Finding a basis for C(A): The r columns Aâˆ—c1, . . . , Aâˆ—cr form a basis for
C(A) and hence column rank A = r. For it is clear that columns c1, . . . , cr
of B form the leftâ€“toâ€“right basis for C(B) and consequently from parts (b)
and (c) of Theorem 3.3.5, it follows that columns c1, . . . , cr of A form the
leftâ€“toâ€“right basis for C(A).

3.5. RANK AND NULLITY OF A MATRIX
65
Finding a basis for N(A): For notational simplicity, let us suppose that c1 =
1, . . . , cr = r. Then B has the form
B =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
1
0
Â· Â· Â·
0
b1r+1
Â· Â· Â·
b1n
0
1
Â· Â· Â·
0
b2r+1
Â· Â· Â·
b2n
...
...
Â· Â· Â·
...
...
Â· Â· Â·
...
0
0
Â· Â· Â·
1
brr+1
Â· Â· Â·
brn
0
0
Â· Â· Â·
0
0
Â· Â· Â·
0
...
...
Â· Â· Â·
...
...
Â· Â· Â·
...
0
0
Â· Â· Â·
0
0
Â· Â· Â·
0
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
.
Then N(B) and hence N(A) are determined by the equations
x1
=
(âˆ’b1r+1)xr+1 + Â· Â· Â· + (âˆ’b1n)xn
...
xr
=
(âˆ’brr+1)xr+1 + Â· Â· Â· + (âˆ’brn)xn,
where xr+1, . . . , xn are arbitrary elements of F. Hence the general vector X
in N(A) is given by
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
x1
...
xr
xr+1
...
xn
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
=
xr+1
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
âˆ’b1r+1
...
âˆ’brr+1
1
...
0
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
+ Â· Â· Â· + xn
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
âˆ’bn
...
âˆ’brn
0
...
1
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
(3.2)
=
xr+1X1 + Â· Â· Â· + xnXnâˆ’r.
Hence N(A) is spanned by X1, . . . , Xnâˆ’r, as xr+1, . . . , xn are arbitrary. Also
X1, . . . , Xnâˆ’r are linearly independent. For equating the right hand side of
equation 3.2 to 0 and then equating components r + 1, . . . , n of both sides
of the resulting equation, gives xr+1 = 0, . . . , xn = 0.
Consequently X1, . . . , Xnâˆ’r form a basis for N(A).
Theorem 3.5.1 now follows. For we have
row rank A
=
dim R(A) = r
column rank A
=
dim C(A) = r.
Hence
row rank A = column rank A.

66
CHAPTER 3. SUBSPACES
Also
column rank A + nullity A = r + dim N(A) = r + (n âˆ’r) = n.
DEFINITION 3.5.2 The common value of column rank A and row rank A
is called the rank of A and is denoted by rank A.
EXAMPLE 3.5.1 Given that the reduced rowâ€“echelon form of
A =
ï£®
ï£°
1
1
5
1
4
2
âˆ’1
1
2
2
3
0
6
0
âˆ’3
ï£¹
ï£»
equal to
B =
ï£®
ï£°
1
0
2
0
âˆ’1
0
1
3
0
2
0
0
0
1
3
ï£¹
ï£»,
ï¬nd bases for R(A), C(A) and N(A).
Solution. [1, 0, 2, 0, âˆ’1], [0, 1, 3, 0, 2] and [0, 0, 0, 1, 3] form a basis for
R(A). Also
Aâˆ—1 =
ï£®
ï£°
1
2
3
ï£¹
ï£», Aâˆ—2 =
ï£®
ï£°
1
âˆ’1
0
ï£¹
ï£», Aâˆ—4 =
ï£®
ï£°
1
2
0
ï£¹
ï£»
form a basis for C(A).
Finally N(A) is given by
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
x1
x2
x3
x4
x5
ï£¹
ï£ºï£ºï£ºï£ºï£»
=
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
âˆ’2x3 + x5
âˆ’3x3 âˆ’2x5
x3
âˆ’3x5
x5
ï£¹
ï£ºï£ºï£ºï£ºï£»
= x3
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
âˆ’2
âˆ’3
1
0
0
ï£¹
ï£ºï£ºï£ºï£ºï£»
+ x5
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
1
âˆ’2
0
âˆ’3
1
ï£¹
ï£ºï£ºï£ºï£ºï£»
= x3X1 + x5X2,
where x3 and x5 are arbitrary. Hence X1 and X2 form a basis for N(A).
Here rank A = 3 and nullity A = 2.
EXAMPLE 3.5.2 Let A =
Â· 1
2
2
4
Â¸
. Then B =
Â· 1
2
0
0
Â¸
is the reduced
rowâ€“echelon form of A.

3.6. PROBLEMS
67
Hence [1, 2] is a basis for R(A) and
Â· 1
2
Â¸
is a basis for C(A). Also N(A)
is given by the equation x1 = âˆ’2x2, where x2 is arbitrary. Then
Â· x1
x2
Â¸
=
Â· âˆ’2x2
x2
Â¸
= x2
Â· âˆ’2
1
Â¸
and hence
Â· âˆ’2
1
Â¸
is a basis for N(A).
Here rank A = 1 and nullity A = 1.
EXAMPLE 3.5.3 Let A =
Â· 1
2
3
4
Â¸
. Then B =
Â· 1
0
0
1
Â¸
is the reduced
rowâ€“echelon form of A.
Hence [1, 0], [0, 1] form a basis for R(A) while [1, 3], [2, 4] form a basis
for C(A). Also N(A) = {0}.
Here rank A = 2 and nullity A = 0.
We conclude this introduction to vector spaces with a result of great
theoretical importance.
THEOREM 3.5.2 Every linearly independent family of vectors in a sub-
space S can be extended to a basis of S.
Proof. Suppose S has basis X1, . . . , Xm and that Y1, . . . , Yr is a linearly
independent family of vectors in S. Then
S = âŸ¨X1, . . . , XmâŸ©= âŸ¨Y1, . . . , Yr, X1, . . . , XmâŸ©,
as each of Y1, . . . , Yr is a linear combination of X1, . . . , Xm.
Then applying the leftâ€“toâ€“right algorithm to the second spanning family
for S will yield a basis for S which includes Y1, . . . , Yr.
3.6
PROBLEMS
1. Which of the following subsets of R2 are subspaces?
(a) [x, y] satisfying x = 2y;
(b) [x, y] satisfying x = 2y and 2x = y;
(c) [x, y] satisfying x = 2y + 1;
(d) [x, y] satisfying xy = 0;

68
CHAPTER 3. SUBSPACES
(e) [x, y] satisfying x â‰¥0 and y â‰¥0.
[Answer: (a) and (b).]
2. If X, Y, Z are vectors in Rn, prove that
âŸ¨X, Y, ZâŸ©= âŸ¨X + Y, X + Z, Y + ZâŸ©.
3. Determine if X1 =
ï£®
ï£¯ï£¯ï£°
1
0
1
2
ï£¹
ï£ºï£ºï£», X2 =
ï£®
ï£¯ï£¯ï£°
0
1
1
2
ï£¹
ï£ºï£ºï£»and X3 =
ï£®
ï£¯ï£¯ï£°
1
1
1
3
ï£¹
ï£ºï£ºï£»are linearly
independent in R4.
4. For which real numbers Î» are the following vectors linearly independent
in R3?
X1 =
ï£®
ï£°
Î»
âˆ’1
âˆ’1
ï£¹
ï£»,
X2 =
ï£®
ï£°
âˆ’1
Î»
âˆ’1
ï£¹
ï£»,
X3 =
ï£®
ï£°
âˆ’1
âˆ’1
Î»
ï£¹
ï£».
5. Find bases for the row, column and null spaces of the following matrix
over Q:
A =
ï£®
ï£¯ï£¯ï£°
1
1
2
0
1
2
2
5
0
3
0
0
0
1
3
8
11
19
0
11
ï£¹
ï£ºï£ºï£».
6. Find bases for the row, column and null spaces of the following matrix
over Z2:
A =
ï£®
ï£¯ï£¯ï£°
1
0
1
0
1
0
1
0
1
1
1
1
1
1
0
0
0
1
1
0
ï£¹
ï£ºï£ºï£».
7. Find bases for the row, column and null spaces of the following matrix
over Z5:
A =
ï£®
ï£¯ï£¯ï£°
1
1
2
0
1
3
2
1
4
0
3
2
0
0
0
1
3
0
3
0
2
4
3
2
ï£¹
ï£ºï£ºï£».

3.6. PROBLEMS
69
8. Find bases for the row, column and null spaces of the matrix A deï¬ned
in section 1.6, Problem 17. (Note: In this question, F is a ï¬eld of four
elements.)
9. If X1, . . . , Xm form a basis for a subspace S, prove that
X1, X1 + X2, . . . , X1 + Â· Â· Â· + Xm
also form a basis for S.
10. Let A =
Â· a
b
c
1
1
1
Â¸
. Find conditions on a, b, c such that (a) rank A =
1; (b) rank A = 2.
[Answer: (a) a = b = c; (b) at least two of a, b, c are distinct.]
11. Let S be a subspace of F n with dim S = m. If X1, . . . , Xm are vectors
in S with the property that S = âŸ¨X1, . . . , XmâŸ©, prove that X1 . . . , Xm
form a basis for S.
12. Find a basis for the subspace S of R3 deï¬ned by the equation
x + 2y + 3z = 0.
Verify that Y1 = [âˆ’1, âˆ’1, 1]t âˆˆS and ï¬nd a basis for S which includes
Y1.
13. Let X1, . . . , Xm be vectors in F n. If Xi = Xj, where i < j, prove that
X1, . . . Xm are linearly dependent.
14. Let X1, . . . , Xm+1 be vectors in F n. Prove that
dim âŸ¨X1, . . . , Xm+1âŸ©= dim âŸ¨X1, . . . , XmâŸ©
if Xm+1 is a linear combination of X1, . . . , Xm, but
dim âŸ¨X1, . . . , Xm+1âŸ©= dim âŸ¨X1, . . . , XmâŸ©+ 1
if Xm+1 is not a linear combination of X1, . . . , Xm.
Deduce that the system of linear equations AX = B is consistent, if
and only if
rank [A|B] = rank A.

70
CHAPTER 3. SUBSPACES
15. Let a1, . . . , an be elements of F, not all zero. Prove that the set of
vectors [x1, . . . , xn]t where x1, . . . , xn satisfy
a1x1 + Â· Â· Â· + anxn = 0
is a subspace of F n with dimension equal to n âˆ’1.
16. Prove Lemma 3.2.1, Theorem 3.2.1, Corollary 3.2.1 and Theorem 3.3.2.
17. Let R and S be subspaces of F n, with R âŠ†S. Prove that
dim R â‰¤dim S
and that equality implies R = S. (This is a very useful way of proving
equality of subspaces.)
18. Let R and S be subspaces of F n. If R âˆªS is a subspace of F n, prove
that R âŠ†S or S âŠ†R.
19. Let X1, . . . , Xr be a basis for a subspace S. Prove that all bases for S
are given by the family Y1, . . . , Yr, where
Yi =
r
X
j=1
aijXj,
and where A = [aij] âˆˆMrÃ—r(F) is a nonâ€“singular matrix.

Chapter 4
DETERMINANTS
DEFINITION 4.0.1 If A =
Â· a11
a12
a21
a22
Â¸
, we deï¬ne the determinant of
A, (also denoted by det A,) to be the scalar
det A = a11a22 âˆ’a12a21.
The notation
Â¯Â¯Â¯Â¯
a11
a12
a21
a22
Â¯Â¯Â¯Â¯ is also used for the determinant of A.
If A is a real matrix, there is a geometrical interpretation of det A.
If
P = (x1, y1) and Q = (x2, y2) are points in the plane, forming a triangle
with the origin O = (0, 0), then apart from sign, 1
2
Â¯Â¯Â¯Â¯
x1
y1
x2
y2
Â¯Â¯Â¯Â¯ is the area
of the triangle OPQ. For, using polar coordinates, let x1 = r1 cos Î¸1 and
y1 = r1 sin Î¸1, where r1 = OP and Î¸1 is the angle made by the ray
-
OP with
the positive xâ€“axis. Then triangle OPQ has area 1
2OP Â· OQ sin Î±, where
Î± = âˆ POQ. If triangle OPQ has antiâ€“clockwise orientation, then the ray
-
OQ makes angle Î¸2 = Î¸1 + Î± with the positive xâ€“axis. (See Figure 4.1.)
Also x2 = r2 cos Î¸2 and y2 = r2 sin Î¸2. Hence
Area OPQ
=
1
2OP Â· OQ sin Î±
=
1
2OP Â· OQ sin (Î¸2 âˆ’Î¸1)
=
1
2OP Â· OQ(sin Î¸2 cos Î¸1 âˆ’cos Î¸2 sin Î¸1)
=
1
2(OQ sin Î¸2 Â· OP cos Î¸1 âˆ’OQ cos Î¸2 Â· OP sin Î¸1)
71

72
CHAPTER 4. DETERMINANTS
-
6
x
y
Â©Â©Â©Â©Â©Â©Â©Â©
Q
P
O
Î¸1
Î±
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢@
@
@
@
Figure 4.1: Area of triangle OPQ.
=
1
2(y2x1 âˆ’x2y1)
=
1
2
Â¯Â¯Â¯Â¯
x1
y1
x2
y2
Â¯Â¯Â¯Â¯ .
Similarly, if triangle OPQ has clockwise orientation, then its area equals
âˆ’1
2
Â¯Â¯Â¯Â¯
x1
y1
x2
y2
Â¯Â¯Â¯Â¯.
For a general triangle P1P2P3, with Pi = (xi, yi), i = 1, 2, 3, we can
take P1 as the origin. Then the above formula gives
1
2
Â¯Â¯Â¯Â¯
x2 âˆ’x1
y2 âˆ’y1
x3 âˆ’x1
y3 âˆ’y1
Â¯Â¯Â¯Â¯
or
âˆ’1
2
Â¯Â¯Â¯Â¯
x2 âˆ’x1
y2 âˆ’y1
x3 âˆ’x1
y3 âˆ’y1
Â¯Â¯Â¯Â¯ ,
according as vertices P1P2P3 are antiâ€“clockwise or clockwise oriented.
We now give a recursive deï¬nition of the determinant of an nÃ—n matrix
A = [aij], n â‰¥3.
DEFINITION 4.0.2 (Minor) Let Mij(A) (or simply Mij if there is no
ambiguity) denote the determinant of the (n âˆ’1) Ã— (n âˆ’1) submatrix of A
formed by deleting the iâ€“th row and jâ€“th column of A. (Mij(A) is called
the (i, j) minor of A.)
Assume that the determinant function has been deï¬ned for matrices of
size (nâˆ’1)Ã—(nâˆ’1). Then det A is deï¬ned by the soâ€“called ï¬rstâ€“row Laplace

73
expansion:
det A
=
a11M11(A) âˆ’a12M12(A) + . . . + (âˆ’1)1+nM1n(A)
=
n
X
j=1
(âˆ’1)1+ja1jM1j(A).
For example, if A = [aij] is a 3 Ã— 3 matrix, the Laplace expansion gives
det A = a11M11(A) âˆ’a12M12(A) + a13M13(A)
=
a11
Â¯Â¯Â¯Â¯
a22
a23
a32
a33
Â¯Â¯Â¯Â¯ âˆ’a12
Â¯Â¯Â¯Â¯
a21
a23
a31
a33
Â¯Â¯Â¯Â¯ + a13
Â¯Â¯Â¯Â¯
a21
a22
a31
a32
Â¯Â¯Â¯Â¯
=
a11(a22a33 âˆ’a23a32) âˆ’a12(a21a33 âˆ’a23a31) + a13(a21a32 âˆ’a22a31)
=
a11a22a33 âˆ’a11a23a32 âˆ’a12a21a33 + a12a23a31 + a13a21a32 âˆ’a13a22a31.
(The recursive deï¬nition also works for 2 Ã— 2 determinants, if we deï¬ne the
determinant of a 1 Ã— 1 matrix [t] to be the scalar t:
det A = a11M11(A) âˆ’a12M12(A) = a11a22 âˆ’a12a21.)
EXAMPLE 4.0.1 If P1P2P3 is a triangle with Pi = (xi, yi), i = 1, 2, 3,
then the area of triangle P1P2P3 is
1
2
Â¯Â¯Â¯Â¯Â¯Â¯
x1
y1
1
x2
y2
1
x3
y3
1
Â¯Â¯Â¯Â¯Â¯Â¯
or
âˆ’1
2
Â¯Â¯Â¯Â¯Â¯Â¯
x1
y1
1
x2
y2
1
x3
y3
1
Â¯Â¯Â¯Â¯Â¯Â¯
,
according as the orientation of P1P2P3 is antiâ€“clockwise or clockwise.
For from the deï¬nition of 3 Ã— 3 determinants, we have
1
2
Â¯Â¯Â¯Â¯Â¯Â¯
x1
y1
1
x2
y2
1
x3
y3
1
Â¯Â¯Â¯Â¯Â¯Â¯
=
1
2
Âµ
x1
Â¯Â¯Â¯Â¯
y2
1
y3
1
Â¯Â¯Â¯Â¯ âˆ’y1
Â¯Â¯Â¯Â¯
x2
1
x3
1
Â¯Â¯Â¯Â¯ +
Â¯Â¯Â¯Â¯
x2
y2
x3
y3
Â¯Â¯Â¯Â¯
Â¶
=
1
2
Â¯Â¯Â¯Â¯
x2 âˆ’x1
y2 âˆ’y1
x3 âˆ’x1
y3 âˆ’y1
Â¯Â¯Â¯Â¯ .
One property of determinants that follows immediately from the deï¬ni-
tion is the following:
THEOREM 4.0.1 If a row of a matrix is zero, then the value of the de-
terminant is zero.

74
CHAPTER 4. DETERMINANTS
(The corresponding result for columns also holds, but here a simple proof
by induction is needed.)
One of the simplest determinants to evaluate is that of a lower triangular
matrix.
THEOREM 4.0.2 Let A = [aij], where aij = 0 if i < j. Then
det A = a11a22 . . . ann.
(4.1)
An important special case is when A is a diagonal matrix.
If A =diag (a1, . . . , an) then det A = a1 . . . an. In particular, for a scalar
matrix tIn, we have det (tIn) = tn.
Proof. Use induction on the size n of the matrix.
The result is true for n = 2. Now let n > 2 and assume the result true
for matrices of size n âˆ’1. If A is n Ã— n, then expanding det A along row 1
gives
det A
=
a11
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
a22
0
. . .
0
a32
a33
. . .
0
...
an1
an2
. . .
ann
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
=
a11(a22 . . . ann)
by the induction hypothesis.
If A is upper triangular, equation 4.1 remains true and the proof is again
an exercise in induction, with the slight diï¬€erence that the column version
of theorem 4.0.1 is needed.
REMARK 4.0.1 It can be shown that the expanded form of the determi-
nant of an n Ã— n matrix A consists of n! signed products Â±a1i1a2i2 . . . anin,
where (i1, i2, . . . , in) is a permutation of (1, 2, . . . , n), the sign being 1 or
âˆ’1, according as the number of inversions of (i1, i2, . . . , in) is even or odd.
An inversion occurs when ir > is but r < s. (The proof is not easy and is
omitted.)
The deï¬nition of the determinant of an n Ã— n matrix was given in terms
of the ï¬rstâ€“row expansion.
The next theorem says that we can expand
the determinant along any row or column. (The proof is not easy and is
omitted.)

75
THEOREM 4.0.3
det A =
n
X
j=1
(âˆ’1)i+jaijMij(A)
for i = 1, . . . , n (the soâ€“called iâ€“th row expansion) and
det A =
n
X
i=1
(âˆ’1)i+jaijMij(A)
for j = 1, . . . , n (the soâ€“called jâ€“th column expansion).
REMARK 4.0.2 The expression (âˆ’1)i+j obeys the chessâ€“board pattern
of signs:
ï£®
ï£¯ï£¯ï£¯ï£°
+
âˆ’
+
. . .
âˆ’
+
âˆ’
. . .
+
âˆ’
+
. . .
...
ï£¹
ï£ºï£ºï£ºï£».
The following theorems can be proved by straightforward inductions on
the size of the matrix:
THEOREM 4.0.4 A matrix and its transpose have equal determinants;
that is
det At = det A.
THEOREM 4.0.5 If two rows of a matrix are equal, the determinant is
zero. Similarly for columns.
THEOREM 4.0.6 If two rows of a matrix are interchanged, the determi-
nant changes sign.
EXAMPLE 4.0.2 If P1 = (x1, y1) and P2 = (x2, y2) are distinct points,
then the line through P1 and P2 has equation
Â¯Â¯Â¯Â¯Â¯Â¯
x
y
1
x1
y1
1
x2
y2
1
Â¯Â¯Â¯Â¯Â¯Â¯
= 0.

76
CHAPTER 4. DETERMINANTS
For, expanding the determinant along row 1, the equation becomes
ax + by + c = 0,
where
a =
Â¯Â¯Â¯Â¯
y1
1
y2
1
Â¯Â¯Â¯Â¯ = y1 âˆ’y2 and b = âˆ’
Â¯Â¯Â¯Â¯
x1
1
x2
1
Â¯Â¯Â¯Â¯ = x2 âˆ’x1.
This represents a line, as not both a and b can be zero. Also this line passes
through Pi, i = 1, 2. For the determinant has its ï¬rst and iâ€“th rows equal
if x = xi and y = yi and is consequently zero.
There is a corresponding formula in threeâ€“dimensional geometry.
If
P1, P2, P3 are nonâ€“collinear points in threeâ€“dimensional space, with Pi =
(xi, yi, zi), i = 1, 2, 3, then the equation
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
x
y
z
1
x1
y1
z1
1
x2
y2
z2
1
x3
y3
z3
1
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
= 0
represents the plane through P1, P2, P3. For, expanding the determinant
along row 1, the equation becomes ax + by + cz + d = 0, where
a =
Â¯Â¯Â¯Â¯Â¯Â¯
y1
z1
1
y2
z2
1
y3
z3
1
Â¯Â¯Â¯Â¯Â¯Â¯
, b = âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯
x1
z1
1
x2
z2
1
x3
z3
1
Â¯Â¯Â¯Â¯Â¯Â¯
, c =
Â¯Â¯Â¯Â¯Â¯Â¯
x1
y1
1
x2
y2
1
x3
y3
1
Â¯Â¯Â¯Â¯Â¯Â¯
.
As we shall see in chapter 6, this represents a plane if at least one of a, b, c
is nonâ€“zero.
However, apart from sign and a factor
1
2, the determinant
expressions for a, b, c give the values of the areas of projections of triangle
P1P2P3 on the (y, z), (x, z) and (x, y) planes, respectively. Geometrically,
it is then clear that at least one of a, b, c is nonâ€“zero. It is also possible to
give an algebraic proof of this fact.
Finally, the plane passes through Pi, i = 1, 2, 3 as the determinant has
its ï¬rst and iâ€“th rows equal if x = xi, y = yi, z = zi and is consequently
zero. We now work towards proving that a matrix is nonâ€“singular if its
determinant is nonâ€“zero.
DEFINITION 4.0.3 (Cofactor) The (i, j) cofactor of A, denoted by
Cij(A) (or Cij if there is no ambiguity) is deï¬ned by
Cij(A) = (âˆ’1)i+jMij(A).

77
REMARK 4.0.3 It is important to notice that Cij(A), like Mij(A), does
not depend on aij. Use will be made of this observation presently.
In terms of the cofactor notation, Theorem 4.0.3 takes the form
THEOREM 4.0.7
det A =
n
X
j=1
aijCij(A)
for i = 1, . . . , n and
det A =
n
X
i=1
aijCij(A)
for j = 1, . . . , n.
Another result involving cofactors is
THEOREM 4.0.8 Let A be an n Ã— n matrix. Then
(a)
n
X
j=1
aijCkj(A) = 0
if i Ì¸= k.
Also
(b)
n
X
i=1
aijCik(A) = 0
if j Ì¸= k.
Proof.
If A is nÃ—n and i Ì¸= k, let B be the matrix obtained from A by replacing
row k by row i. Then det B = 0 as B has two identical rows.
Now expand det B along row k. We get
0 = det B
=
n
X
j=1
bkjCkj(B)
=
n
X
j=1
aijCkj(A),
in view of Remark 4.0.3.

78
CHAPTER 4. DETERMINANTS
DEFINITION 4.0.4 (Adjoint) If A = [aij] is an n Ã— n matrix, the ad-
joint of A, denoted by adj A, is the transpose of the matrix of cofactors.
Hence
adj A =
ï£®
ï£¯ï£¯ï£¯ï£°
C11
C21
Â· Â· Â·
Cn1
C12
C22
Â· Â· Â·
Cn2
...
...
C1n
C2n
Â· Â· Â·
Cnn
ï£¹
ï£ºï£ºï£ºï£».
Theorems 4.0.7 and 4.0.8 may be combined to give
THEOREM 4.0.9 Let A be an n Ã— n matrix. Then
A(adj A) = (det A)In = (adj A)A.
Proof.
(A adj A)ik
=
n
X
j=1
aij(adj A)jk
=
n
X
j=1
aijCkj(A)
=
Î´ikdet A
=
((det A)In)ik.
Hence A(adj A) = (det A)In. The other equation is proved similarly.
COROLLARY 4.0.1 (Formula for the inverse) If det A Ì¸= 0, then A
is nonâ€“singular and
Aâˆ’1 =
1
det Aadj A.
EXAMPLE 4.0.3 The matrix
A =
ï£®
ï£°
1
2
3
4
5
6
8
8
9
ï£¹
ï£»
is nonâ€“singular. For
det A
=
Â¯Â¯Â¯Â¯
5
6
8
9
Â¯Â¯Â¯Â¯ âˆ’2
Â¯Â¯Â¯Â¯
4
6
8
9
Â¯Â¯Â¯Â¯ + 3
Â¯Â¯Â¯Â¯
4
5
8
8
Â¯Â¯Â¯Â¯
=
âˆ’3 + 24 âˆ’24
=
âˆ’3 Ì¸= 0.

79
Also
Aâˆ’1
=
1
âˆ’3
ï£®
ï£°
C11
C21
C31
C12
C22
C32
C13
C23
C33
ï£¹
ï£»
=
âˆ’1
3
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
Â¯Â¯Â¯Â¯
5
6
8
9
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
2
3
8
9
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
2
3
5
6
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
4
6
8
9
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
1
3
8
9
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
1
3
4
6
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
4
5
8
8
Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯
1
2
8
8
Â¯Â¯Â¯Â¯
Â¯Â¯Â¯Â¯
1
2
4
5
Â¯Â¯Â¯Â¯
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
=
âˆ’1
3
ï£®
ï£°
âˆ’3
6
âˆ’3
12
âˆ’15
6
âˆ’8
8
âˆ’3
ï£¹
ï£».
The following theorem is useful for simplifying and numerically evaluating
a determinant. Proofs are obtained by expanding along the corresponding
row or column.
THEOREM 4.0.10 The determinant is a linear function of each row and
column.
For example
(a)
Â¯Â¯Â¯Â¯Â¯Â¯
a11 + aâ€²
11
a12 + aâ€²
12
a13 + aâ€²
13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
a11
a12
a13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
+
Â¯Â¯Â¯Â¯Â¯Â¯
aâ€²
11
aâ€²
12
aâ€²
13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
(b)
Â¯Â¯Â¯Â¯Â¯Â¯
ta11
ta12
ta13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
= t
Â¯Â¯Â¯Â¯Â¯Â¯
a11
a12
a13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
.
COROLLARY 4.0.2 If a multiple of a row is added to another row, the
value of the determinant is unchanged. Similarly for columns.
Proof.
We illustrate with a 3 Ã— 3 example, but the proof is really quite
general.
Â¯Â¯Â¯Â¯Â¯Â¯
a11 + ta21
a12 + ta22
a13 + ta23
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
a11
a12
a13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
+
Â¯Â¯Â¯Â¯Â¯Â¯
ta21
ta22
ta23
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯

80
CHAPTER 4. DETERMINANTS
=
Â¯Â¯Â¯Â¯Â¯Â¯
a11
a12
a13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
+ t
Â¯Â¯Â¯Â¯Â¯Â¯
a21
a22
a23
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
a11
a12
a13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
+ t Ã— 0
=
Â¯Â¯Â¯Â¯Â¯Â¯
a11
a12
a13
a21
a22
a23
a31
a32
a33
Â¯Â¯Â¯Â¯Â¯Â¯
.
To evaluate a determinant numerically, it is advisable to reduce the matrix
to rowâ€“echelon form, recording any sign changes caused by row interchanges,
together with any factors taken out of a row, as in the following examples.
EXAMPLE 4.0.4 Evaluate the determinant
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
3
4
5
6
8
8
9
Â¯Â¯Â¯Â¯Â¯Â¯
.
Solution. Using row operations R2 â†’R2 âˆ’4R1 and R3 â†’R3 âˆ’8R1 and
then expanding along the ï¬rst column, gives
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
3
4
5
6
8
8
9
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
1
2
3
0
âˆ’3
âˆ’6
0
âˆ’8
âˆ’15
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯
âˆ’3
âˆ’6
âˆ’8
âˆ’15
Â¯Â¯Â¯Â¯
=
âˆ’3
Â¯Â¯Â¯Â¯
1
2
âˆ’8
âˆ’15
Â¯Â¯Â¯Â¯ = âˆ’3
Â¯Â¯Â¯Â¯
1
2
0
1
Â¯Â¯Â¯Â¯ = âˆ’3.
EXAMPLE 4.0.5 Evaluate the determinant
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
2
1
3
1
4
5
7
6
1
2
1
1
3
4
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
.
Solution.
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
2
1
3
1
4
5
7
6
1
2
1
1
3
4
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
2
1
0
âˆ’2
âˆ’2
2
0
âˆ’1
âˆ’13
âˆ’5
0
0
1
3
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯

81
=
âˆ’2
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
2
1
0
1
1
âˆ’1
0
âˆ’1
âˆ’13
âˆ’5
0
0
1
3
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
=
âˆ’2
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
2
1
0
1
1
âˆ’1
0
0
âˆ’12
âˆ’6
0
0
1
3
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
=
2
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
2
1
0
1
1
âˆ’1
0
0
1
3
0
0
âˆ’12
âˆ’6
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
=
2
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
2
1
0
1
1
âˆ’1
0
0
1
3
0
0
0
30
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
= 60.
EXAMPLE 4.0.6 (Vandermonde determinant) Prove that
Â¯Â¯Â¯Â¯Â¯Â¯
1
1
1
a
b
c
a2
b2
c2
Â¯Â¯Â¯Â¯Â¯Â¯
= (b âˆ’a)(c âˆ’a)(c âˆ’b).
Solution. Subtracting column 1 from columns 2 and 3 , then expanding
along row 1, gives
Â¯Â¯Â¯Â¯Â¯Â¯
1
1
1
a
b
c
a2
b2
c2
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
1
0
0
a
b âˆ’a
c âˆ’a
a2
b2 âˆ’a2
c2 âˆ’a2
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯
b âˆ’a
c âˆ’a
b2 âˆ’a2
c2 âˆ’a2
Â¯Â¯Â¯Â¯
=
(b âˆ’a)(c âˆ’a)
Â¯Â¯Â¯Â¯
1
1
b + a
c + a
Â¯Â¯Â¯Â¯ = (b âˆ’a)(c âˆ’a)(c âˆ’b).
REMARK 4.0.4 From theorems 4.0.6, 4.0.10 and corollary 4.0.2, we de-
duce
(a) det (EijA) = âˆ’det A,
(b) det (Ei(t)A) = t det A, if t Ì¸= 0,

82
CHAPTER 4. DETERMINANTS
(c) det (Eij(t)A) =det A.
It follows that if A is rowâ€“equivalent to B, then det B = c det A, where c Ì¸= 0.
Hence det B Ì¸= 0 â‡”det A Ì¸= 0 and det B = 0 â‡”det A = 0. Consequently
from theorem 2.5.8 and remark 2.5.7, we have the following important result:
THEOREM 4.0.11 Let A be an n Ã— n matrix. Then
(i) A is nonâ€“singular if and only if det A Ì¸= 0;
(ii) A is singular if and only if det A = 0;
(iii) the homogeneous system AX = 0 has a nonâ€“trivial solution if and
only if det A = 0.
EXAMPLE 4.0.7 Find the rational numbers a for which the following
homogeneous system has a nonâ€“trivial solution and solve the system for
these values of a:
x âˆ’2y + 3z
=
0
ax + 3y + 2z
=
0
6x + y + az
=
0.
Solution. The coeï¬ƒcient determinant of the system is
âˆ†=
Â¯Â¯Â¯Â¯Â¯Â¯
1
âˆ’2
3
a
3
2
6
1
a
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
1
âˆ’2
3
0
3 + 2a
2 âˆ’3a
0
13
a âˆ’18
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯
3 + 2a
2 âˆ’3a
13
a âˆ’18
Â¯Â¯Â¯Â¯
=
(3 + 2a)(a âˆ’18) âˆ’13(2 âˆ’3a)
=
2a2 + 6a âˆ’80 = 2(a + 8)(a âˆ’5).
So âˆ†= 0 â‡”a = âˆ’8 or a = 5 and these values of a are the only values for
which the given homogeneous system has a nonâ€“trivial solution.
If a = âˆ’8, the coeï¬ƒcient matrix has reduced rowâ€“echelon form equal to
ï£®
ï£°
1
0
âˆ’1
0
1
âˆ’2
0
0
0
ï£¹
ï£»

83
and so the complete solution is x = z, y = 2z, with z arbitrary. If a = 5,
the coeï¬ƒcient matrix has reduced rowâ€“echelon form equal to
ï£®
ï£°
1
0
1
0
1
âˆ’1
0
0
0
ï£¹
ï£»
and so the complete solution is x = âˆ’z, y = z, with z arbitrary.
EXAMPLE 4.0.8 Find the values of t for which the following system is
consistent and solve the system in each case:
x + y
=
1
tx + y
=
t
(1 + t)x + 2y
=
3.
Solution. Suppose that the given system has a solution (x0, y0). Then the
following homogeneous system
x + y + z
=
0
tx + y + tz
=
0
(1 + t)x + 2y + 3z
=
0
will have a nonâ€“trivial solution
x = x0,
y = y0,
z = âˆ’1.
Hence the coeï¬ƒcient determinant âˆ†is zero. However
âˆ†=
Â¯Â¯Â¯Â¯Â¯Â¯
1
1
1
t
1
t
1 + t
2
3
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
1
0
0
t
1 âˆ’t
0
1 + t
1 âˆ’t
2 âˆ’t
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯
1 âˆ’t
0
1 âˆ’t
2 âˆ’t
Â¯Â¯Â¯Â¯ = (1âˆ’t)(2âˆ’t).
Hence t = 1 or t = 2. If t = 1, the given system becomes
x + y
=
1
x + y
=
1
2x + 2y
=
3
which is clearly inconsistent. If t = 2, the given system becomes
x + y
=
1
2x + y
=
2
3x + 2y
=
3

84
CHAPTER 4. DETERMINANTS
which has the unique solution x = 1, y = 0.
To ï¬nish this section, we present an old (1750) method of solving a
system of n equations in n unknowns called Cramerâ€™s rule . The method is
not used in practice. However it has a theoretical use as it reveals explicitly
how the solution depends on the coeï¬ƒcients of the augmented matrix.
THEOREM 4.0.12 (Cramerâ€™s rule) The system of n linear equations
in n unknowns x1, . . . , xn
a11x1 + a12x2 + Â· Â· Â· + a1nxn
=
b1
a21x1 + a22x2 + Â· Â· Â· + a2nxn
=
b2
...
an1x1 + an2x2 + Â· Â· Â· + annxn
=
bn
has a unique solution if âˆ†= det [aij] Ì¸= 0, namely
x1 = âˆ†1
âˆ†, x2 = âˆ†2
âˆ†, . . . , xn = âˆ†n
âˆ†,
where âˆ†i is the determinant of the matrix formed by replacing the iâ€“th
column of the coeï¬ƒcient matrix A by the entries b1, b2, . . . , bn.
Proof. Suppose the coeï¬ƒcient determinant âˆ†Ì¸= 0. Then by corollary 4.0.1,
Aâˆ’1 exists and is given by Aâˆ’1 =
1
âˆ†adj A and the system has the unique
solution
ï£®
ï£¯ï£¯ï£¯ï£°
x1
x2
...
xn
ï£¹
ï£ºï£ºï£ºï£»= Aâˆ’1
ï£®
ï£¯ï£¯ï£¯ï£°
b1
b2
...
bn
ï£¹
ï£ºï£ºï£ºï£»
=
1
âˆ†
ï£®
ï£¯ï£¯ï£¯ï£°
C11
C21
Â· Â· Â·
Cn1
C12
C22
Â· Â· Â·
Cn2
...
...
C1n
C2n
Â· Â· Â·
Cnn
ï£¹
ï£ºï£ºï£ºï£»
ï£®
ï£¯ï£¯ï£¯ï£°
b1
b2
...
bn
ï£¹
ï£ºï£ºï£ºï£»
=
1
âˆ†
ï£®
ï£¯ï£¯ï£¯ï£°
b1C11 + b2C21 + . . . + bnCn1
b2C12 + b2C22 + . . . + bnCn2
...
bnC1n + b2C2n + . . . + bnCnn
ï£¹
ï£ºï£ºï£ºï£».
However the iâ€“th component of the last vector is the expansion of âˆ†i along
column i. Hence
ï£®
ï£¯ï£¯ï£¯ï£°
x1
x2
...
xn
ï£¹
ï£ºï£ºï£ºï£»= 1
âˆ†
ï£®
ï£¯ï£¯ï£¯ï£°
âˆ†1
âˆ†2
...
âˆ†n
ï£¹
ï£ºï£ºï£ºï£»=
ï£®
ï£¯ï£¯ï£¯ï£°
âˆ†1/âˆ†
âˆ†2/âˆ†
...
âˆ†n/âˆ†
ï£¹
ï£ºï£ºï£ºï£».

4.1. PROBLEMS
85
4.1
PROBLEMS
.
1. If the points Pi = (xi, yi), i = 1, 2, 3, 4 form a quadrilateral with ver-
tices in antiâ€“clockwise orientation, prove that the area of the quadri-
lateral equals
1
2
ÂµÂ¯Â¯Â¯Â¯
x1
x2
y1
y2
Â¯Â¯Â¯Â¯ +
Â¯Â¯Â¯Â¯
x2
x3
y2
y3
Â¯Â¯Â¯Â¯ +
Â¯Â¯Â¯Â¯
x3
x4
y3
y4
Â¯Â¯Â¯Â¯ +
Â¯Â¯Â¯Â¯
x4
x1
y4
y1
Â¯Â¯Â¯Â¯
Â¶
.
(This formula generalizes to a simple polygon and is known as the
Surveyorâ€™s formula.)
2. Prove that the following identity holds by expressing the leftâ€“hand
side as the sum of 8 determinants:
Â¯Â¯Â¯Â¯Â¯Â¯
a + x
b + y
c + z
x + u
y + v
z + w
u + a
v + b
w + c
Â¯Â¯Â¯Â¯Â¯Â¯
= 2
Â¯Â¯Â¯Â¯Â¯Â¯
a
b
c
x
y
z
u
v
w
Â¯Â¯Â¯Â¯Â¯Â¯
.
3. Prove that
Â¯Â¯Â¯Â¯Â¯Â¯
n2
(n + 1)2
(n + 2)2
(n + 1)2
(n + 2)2
(n + 3)2
(n + 2)2
(n + 3)2
(n + 4)2
Â¯Â¯Â¯Â¯Â¯Â¯
= âˆ’8.
4. Evaluate the following determinants:
(a)
Â¯Â¯Â¯Â¯Â¯Â¯
246
427
327
1014
543
443
âˆ’342
721
621
Â¯Â¯Â¯Â¯Â¯Â¯
(b)
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
2
3
4
âˆ’2
1
âˆ’4
3
3
âˆ’4
âˆ’1
2
4
3
âˆ’2
âˆ’1
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
.
[Answers: (a) âˆ’29400000; (b) 900.]
5. Compute the inverse of the matrix
A =
ï£®
ï£°
1
0
âˆ’2
3
1
4
5
2
âˆ’3
ï£¹
ï£»
by ï¬rst computing the adjoint matrix.
[Answer: Aâˆ’1 = âˆ’1
13
ï£®
ï£°
âˆ’11
âˆ’4
2
29
7
âˆ’10
1
âˆ’2
1
ï£¹
ï£».]

86
CHAPTER 4. DETERMINANTS
6. Prove that the following identities hold:
(i)
Â¯Â¯Â¯Â¯Â¯Â¯
2a
2b
b âˆ’c
2b
2a
a + c
a + b
a + b
b
Â¯Â¯Â¯Â¯Â¯Â¯
=
âˆ’2(a âˆ’b)2(a + b),
(ii)
Â¯Â¯Â¯Â¯Â¯Â¯
b + c
b
c
c
c + a
a
b
a
a + b
Â¯Â¯Â¯Â¯Â¯Â¯
=
2a(b2 + c2).
7. Let Pi = (xi, yi), i = 1, 2, 3. If x1, x2, x3 are distinct, prove that there
is precisely one curve of the form y = ax2 + bx + c passing through
P1, P2 and P3.
8. Let
A =
ï£®
ï£°
1
1
âˆ’1
2
3
k
1
k
3
ï£¹
ï£».
Find the values of k for which det A = 0 and hence, or otherwise,
determine the value of k for which the following system has more than
one solution:
x + y âˆ’z
=
1
2x + 3y + kz
=
3
x + ky + 3z
=
2.
Solve the system for this value of k and determine the solution for
which x2 + y2 + z2 has least value.
[Answer: k = 2; x = 10/21, y = 13/21, z = 2/21.]
9. By considering the coeï¬ƒcient determinant, ï¬nd all rational numbers a
and b for which the following system has (i) no solutions, (ii) exactly
one solution, (iii) inï¬nitely many solutions:
x âˆ’2y + bz
=
3
ax +
2z
=
2
5x + 2y
=
1.
Solve the system in case (iii).
[Answer: (i) ab = 12 and a Ì¸= 3, no solution; ab Ì¸= 12, unique solution;
a = 3, b = 4, inï¬nitely many solutions; x = âˆ’2
3z + 2
3, y = 5
3z âˆ’7
6, with
z arbitrary.]

4.1. PROBLEMS
87
10. Express the determinant of the matrix
B =
ï£®
ï£¯ï£¯ï£°
1
1
2
1
1
2
3
4
2
4
7
2t + 6
2
2
6 âˆ’t
t
ï£¹
ï£ºï£ºï£»
as as polynomial in t and hence determine the rational values of t for
which Bâˆ’1 exists.
[Answer: det B = (t âˆ’2)(2t âˆ’1); t Ì¸= 2 and t Ì¸= 1
2.]
11. If A is a 3 Ã— 3 matrix over a ï¬eld and det A Ì¸= 0, prove that
(i)
det (adj A)
=
(det A)2,
(ii)
(adj A)âˆ’1
=
1
det AA = adj (Aâˆ’1).
12. Suppose that A is a real 3 Ã— 3 matrix such that AtA = I3.
(i) Prove that At(A âˆ’I3) = âˆ’(A âˆ’I3)t.
(ii) Prove that det A = Â±1.
(iii) Use (i) to prove that if det A = 1, then det (A âˆ’I3) = 0.
13. If A is a square matrix such that one column is a linear combination of
the remaining columns, prove that det A = 0. Prove that the converse
also holds.
14. Use Cramerâ€™s rule to solve the system
âˆ’2x + 3y âˆ’z
=
1
x + 2y âˆ’z
=
4
âˆ’2x âˆ’y + z
=
âˆ’3.
[Answer: x = 2, y = 3, z = 4.]
15. Use remark 4.0.4 to deduce that
det Eij = âˆ’1,
det Ei(t) = t,
det Eij(t) = 1
and use theorem 2.5.8 and induction, to prove that
det (BA) = det B det A,
if B is nonâ€“singular. Also prove that the formula holds when B is
singular.

88
CHAPTER 4. DETERMINANTS
16. Prove that
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
a + b + c
a + b
a
a
a + b
a + b + c
a
a
a
a
a + b + c
a + b
a
a
a + b
a + b + c
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
= c2(2b+c)(4a+2b+c).
17. Prove that
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1 + u1
u1
u1
u1
u2
1 + u2
u2
u2
u3
u3
1 + u3
u3
u4
u4
u4
1 + u4
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
= 1 + u1 + u2 + u3 + u4.
18. Let A âˆˆMnÃ—n(F). If At = âˆ’A, prove that det A = 0 if n is odd and
1 + 1 Ì¸= 0 in F.
19. Prove that
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
1
1
1
r
1
1
1
r
r
1
1
r
r
r
1
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
= (1 âˆ’r)3.
20. Express the determinant
Â¯Â¯Â¯Â¯Â¯Â¯
1
a2 âˆ’bc
a4
1
b2 âˆ’ca
b4
1
c2 âˆ’ab
c4
Â¯Â¯Â¯Â¯Â¯Â¯
as the product of one quadratic and four linear factors.
[Answer: (b âˆ’a)(c âˆ’a)(c âˆ’b)(a + b + c)(b2 + bc + c2 + ac + ab + a2).]

Chapter 5
COMPLEX NUMBERS
5.1
Constructing the complex numbers
One way of introducing the ï¬eld C of complex numbers is via the arithmetic
of 2 Ã— 2 matrices.
DEFINITION 5.1.1 A complex number is a matrix of the form
Â· x
âˆ’y
y
x
Â¸
,
where x and y are real numbers.
Complex numbers of the form
Â· x
0
0
x
Â¸
are scalar matrices and are called
real complex numbers and are denoted by the symbol {x}.
The real complex numbers {x} and {y} are respectively called the real
part and imaginary part of the complex number
Â· x
âˆ’y
y
x
Â¸
.
The complex number
Â· 0
âˆ’1
1
0
Â¸
is denoted by the symbol i.
We have the identities
Â· x
âˆ’y
y
x
Â¸
=
Â· x
0
0
x
Â¸
+
Â· 0
âˆ’y
y
0
Â¸
=
Â· x
0
0
x
Â¸
+
Â· 0
âˆ’1
1
0
Â¸ Â· y
0
0
y
Â¸
=
{x} + i{y},
i2 =
Â· 0
âˆ’1
1
0
Â¸ Â· 0
âˆ’1
1
0
Â¸
=
Â· âˆ’1
0
0
âˆ’1
Â¸
= {âˆ’1}.
89

90
CHAPTER 5. COMPLEX NUMBERS
Complex numbers of the form i{y}, where y is a nonâ€“zero real number, are
called imaginary numbers.
If two complex numbers are equal, we can equate their real and imaginary
parts:
{x1} + i{y1} = {x2} + i{y2} â‡’x1 = x2 and y1 = y2,
if x1, x2, y1, y2 are real numbers. Noting that {0} + i{0} = {0}, gives the
useful special case is
{x} + i{y} = {0} â‡’x = 0 and y = 0,
if x and y are real numbers.
The sum and product of two real complex numbers are also real complex
numbers:
{x} + {y} = {x + y},
{x}{y} = {xy}.
Also, as real complex numbers are scalar matrices, their arithmetic is very
simple.
They form a ï¬eld under the operations of matrix addition and
multiplication. The additive identity is {0}, the additive inverse of {x} is
{âˆ’x}, the multiplicative identity is {1} and the multiplicative inverse of {x}
is {xâˆ’1}. Consequently
{x} âˆ’{y} = {x} + (âˆ’{y}) = {x} + {âˆ’y} = {x âˆ’y},
{x}
{y} = {x}{y}âˆ’1 = {x}{yâˆ’1} = {xyâˆ’1} =
Â½x
y
Â¾
.
It is customary to blur the distinction between the real complex number
{x} and the real number x and write {x} as x. Thus we write the complex
number {x} + i{y} simply as x + iy.
More generally, the sum of two complex numbers is a complex number:
(x1 + iy1) + (x2 + iy2) = (x1 + x2) + i(y1 + y2);
(5.1)
and (using the fact that scalar matrices commute with all matrices under
matrix multiplication and {âˆ’1}A = âˆ’A if A is a matrix), the product of
two complex numbers is a complex number:
(x1 + iy1)(x2 + iy2) = x1(x2 + iy2) + (iy1)(x2 + iy2)
= x1x2 + x1(iy2) + (iy1)x2 + (iy1)(iy2)
= x1x2 + ix1y2 + iy1x2 + i2y1y2
= (x1x2 + {âˆ’1}y1y2) + i(x1y2 + y1x2)
= (x1x2 âˆ’y1y2) + i(x1y2 + y1x2),
(5.2)

5.2. CALCULATING WITH COMPLEX NUMBERS
91
The set C of complex numbers forms a ï¬eld under the operations of
matrix addition and multiplication. The additive identity is 0, the additive
inverse of x + iy is the complex number (âˆ’x) + i(âˆ’y), the multiplicative
identity is 1 and the multiplicative inverse of the nonâ€“zero complex number
x + iy is the complex number u + iv, where
u =
x
x2 + y2 and v =
âˆ’y
x2 + y2 .
(If x + iy Ì¸= 0, then x Ì¸= 0 or y Ì¸= 0, so x2 + y2 Ì¸= 0.)
From equations 5.1 and 5.2, we observe that addition and multiplication
of complex numbers is performed just as for real numbers, replacing i2 by
âˆ’1, whenever it occurs.
A useful identity satisï¬ed by complex numbers is
r2 + s2 = (r + is)(r âˆ’is).
This leads to a method of expressing the ratio of two complex numbers in
the form x + iy, where x and y are real complex numbers.
x1 + iy1
x2 + iy2
=
(x1 + iy1)(x2 âˆ’iy2)
(x2 + iy2)(x2 âˆ’iy2)
=
(x1x2 + y1y2) + i(âˆ’x1y2 + y1x2)
x2
2 + y2
2
.
The process is known as rationalization of the denominator.
5.2
Calculating with complex numbers
We can now do all the standard linear algebra calculations over the ï¬eld of
complex numbers â€“ ï¬nd the reduced rowâ€“echelon form of an matrix whose el-
ements are complex numbers, solve systems of linear equations, ï¬nd inverses
and calculate determinants.
For example,
Â¯Â¯Â¯Â¯
1 + i
2 âˆ’i
7
8 âˆ’2i
Â¯Â¯Â¯Â¯
=
(1 + i)(8 âˆ’2i) âˆ’7(2 âˆ’i)
=
(8 âˆ’2i) + i(8 âˆ’2i) âˆ’14 + 7i
=
âˆ’4 + 13i Ì¸= 0.

92
CHAPTER 5. COMPLEX NUMBERS
Then by Cramerâ€™s rule, the linear system
(1 + i)z + (2 âˆ’i)w
=
2 + 7i
7z + (8 âˆ’2i)w
=
4 âˆ’9i
has the unique solution
z
=
Â¯Â¯Â¯Â¯
2 + 7i
2 âˆ’i
4 âˆ’9i
8 âˆ’2i
Â¯Â¯Â¯Â¯
âˆ’4 + 13i
=
(2 + 7i)(8 âˆ’2i) âˆ’(4 âˆ’9i)(2 âˆ’i)
âˆ’4 + 13i
=
2(8 âˆ’2i) + (7i)(8 âˆ’2i) âˆ’{(4(2 âˆ’i) âˆ’9i(2 âˆ’i)}
âˆ’4 + 13i
=
16 âˆ’4i + 56i âˆ’14i2 âˆ’{8 âˆ’4i âˆ’18i + 9i2}
âˆ’4 + 13i
=
31 + 74i
âˆ’4 + 13i
=
(31 + 74i)(âˆ’4 âˆ’13i)
(âˆ’4)2 + 132
=
838 âˆ’699i
(âˆ’4)2 + 132
=
838
185 âˆ’699
185i
and similarly w = âˆ’698
185 + 229
185i.
An important property enjoyed by complex numbers is that every com-
plex number has a square root:
THEOREM 5.2.1
If w is a nonâ€“zero complex number, then the equation z2 = w has a so-
lution z âˆˆC.
Proof. Let w = a + ib, a, b âˆˆR.
Case 1. Suppose b = 0. Then if a > 0, z = âˆša is a solution, while if
a < 0, iâˆšâˆ’a is a solution.
Case 2. Suppose b Ì¸= 0. Let z = x + iy, x, y âˆˆR. Then the equation
z2 = w becomes
(x + iy)2 = x2 âˆ’y2 + 2xyi = a + ib,

5.2. CALCULATING WITH COMPLEX NUMBERS
93
so equating real and imaginary parts gives
x2 âˆ’y2 = a
and
2xy = b.
Hence x Ì¸= 0 and y = b/(2x). Consequently
x2 âˆ’
Âµ b
2x
Â¶
2 = a,
so 4x4 âˆ’4ax2 âˆ’b2 = 0 and 4(x2)2 âˆ’4a(x2) âˆ’b2 = 0. Hence
x2 = 4a Â±
âˆš
16a2 + 16b2
8
= a Â±
âˆš
a2 + b2
2
.
However x2 > 0, so we must take the + sign, as a âˆ’
âˆš
a2 + b2 < 0. Hence
x2 = a +
âˆš
a2 + b2
2
,
x = Â±
s
a +
âˆš
a2 + b2
2
.
Then y is determined by y = b/(2x).
EXAMPLE 5.2.1 Solve the equation z2 = 1 + i.
Solution. Put z = x + iy. Then the equation becomes
(x + iy)2 = x2 âˆ’y2 + 2xyi = 1 + i,
so equating real and imaginary parts gives
x2 âˆ’y2 = 1 and 2xy = 1.
Hence x Ì¸= 0 and y = b/(2x). Consequently
x2 âˆ’
Âµ 1
2x
Â¶
2 = 1,
so 4x4 âˆ’4x2 âˆ’1 = 0. Hence
x2 = 4 Â± âˆš16 + 16
8
= 1 Â±
âˆš
2
2
.
Hence
x2 = 1 +
âˆš
2
2
and
x = Â±
s
1 +
âˆš
2
2
.

94
CHAPTER 5. COMPLEX NUMBERS
Then
y = 1
2x = Â±
1
âˆš
2
p
1 +
âˆš
2
.
Hence the solutions are
z = Â±
ï£«
ï£­
s
1 +
âˆš
2
2
+
i
âˆš
2
p
1 +
âˆš
2
ï£¶
ï£¸.
EXAMPLE 5.2.2 Solve the equation z2 + (
âˆš
3 + i)z + 1 = 0.
Solution. Because every complex number has a square root, the familiar
formula
z = âˆ’b Â±
âˆš
b2 âˆ’4ac
2a
for the solution of the general quadratic equation az2 + bz + c = 0 can be
used, where now a(Ì¸= 0), b, c âˆˆC. Hence
z
=
âˆ’(
âˆš
3 + i) Â±
q
(
âˆš
3 + i)2 âˆ’4
2
=
âˆ’(
âˆš
3 + i) Â±
q
(3 + 2
âˆš
3i âˆ’1) âˆ’4
2
=
âˆ’(
âˆš
3 + i) Â±
p
âˆ’2 + 2
âˆš
3i
2
.
Now we have to solve w2 = âˆ’2 + 2
âˆš
3i.
Put w = x + iy.
Then w2 =
x2 âˆ’y2 + 2xyi = âˆ’2 + 2
âˆš
3i and equating real and imaginary parts gives
x2 âˆ’y2 = âˆ’2 and 2xy = 2
âˆš
3. Hence y =
âˆš
3/x and so x2 âˆ’3/x2 = âˆ’2. So
x4 + 2x2 âˆ’3 = 0 and (x2 + 3)(x2 âˆ’1) = 0. Hence x2 âˆ’1 = 0 and x = Â±1.
Then y = Â±
âˆš
3. Hence (1 +
âˆš
3i)2 = âˆ’2 + 2
âˆš
3i and the formula for z now
becomes
z
=
âˆ’
âˆš
3 âˆ’i Â± (1 +
âˆš
3i)
2
=
1 âˆ’
âˆš
3 + (1 +
âˆš
3)i
2
or
âˆ’1 âˆ’
âˆš
3 âˆ’(1 +
âˆš
3)i
2
.
EXAMPLE 5.2.3 Find the cube roots of 1.

5.3. GEOMETRIC REPRESENTATION OF C
95
Solution.
We have to solve the equation z3 = 1, or z3 âˆ’1 = 0.
Now
z3 âˆ’1 = (z âˆ’1)(z2 + z + 1). So z3 âˆ’1 = 0 â‡’z âˆ’1 = 0 or z2 + z + 1 = 0.
But
z2 + z + 1 = 0 â‡’z = âˆ’1 Â±
âˆš
12 âˆ’4
2
= âˆ’1 Â±
âˆš
3i
2
.
So there are 3 cube roots of 1, namely 1 and (âˆ’1 Â±
âˆš
3i)/2.
We state the next theorem without proof.
It states that every nonâ€“
constant polynomial with complex number coeï¬ƒcients has a root in the
ï¬eld of complex numbers.
THEOREM 5.2.2 (Gauss) If f(z) = anzn + anâˆ’1znâˆ’1 + Â· Â· Â· + a1z + a0,
where an Ì¸= 0 and n â‰¥1, then f(z) = 0 for some z âˆˆC.
It follows that in view of the factor theorem, which states that if a âˆˆF is
a root of a polynomial f(z) with coeï¬ƒcients from a ï¬eld F, then z âˆ’a is a
factor of f(z), that is f(z) = (z âˆ’a)g(z), where the coeï¬ƒcients of g(z) also
belong to F. By repeated application of this result, we can factorize any
polynomial with complex coeï¬ƒcients into a product of linear factors with
complex coeï¬ƒcients:
f(z) = an(z âˆ’z1)(z âˆ’z2) Â· Â· Â· (z âˆ’zn).
There are available a number of computational algorithms for ï¬nding good
approximations to the roots of a polynomial with complex coeï¬ƒcients.
5.3
Geometric representation of C
Complex numbers can be represented as points in the plane, using the cor-
respondence x + iy â†”(x, y). The representation is known as the Argand
diagram or complex plane.
The real complex numbers lie on the xâ€“axis,
which is then called the real axis, while the imaginary numbers lie on the
yâ€“axis, which is known as the imaginary axis. The complex numbers with
positive imaginary part lie in the upper half plane, while those with negative
imaginary part lie in the lower half plane.
Because of the equation
(x1 + iy1) + (x2 + iy2) = (x1 + x2) + i(y1 + y2),
complex numbers add vectorially, using the parallellogram law. Similarly,
the complex number z1 âˆ’z2 can be represented by the vector from (x2, y2)
to (x1, y1), where z1 = x1 + iy1 and z2 = x2 + iy2. (See Figure 5.1.)

96
CHAPTER 5. COMPLEX NUMBERS
-

6
?
z1 âˆ’z2
z1 + z2
z1
z2
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

Â©Â©Â©Â©Â©Â©Â©Â©
*
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
@
@
@
@
R
@
@
@
@
R
Â©Â©Â©Â©Â©Â©Â©Â©
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Figure 5.1: Complex addition and subraction.
The geometrical representation of complex numbers can be very useful
when complex number methods are used to investigate properties of triangles
and circles. It is very important in the branch of calculus known as Complex
Function theory, where geometric methods play an important role.
We mention that the line through two distinct points P1 = (x1, y1) and
P2 = (x2, y2) has the form z = (1 âˆ’t)z1 + tz2, t âˆˆR, where z = x + iy is
any point on the line and zi = xi + iyi, i = 1, 2. For the line has parametric
equations
x = (1 âˆ’t)x1 + tx2,
y = (1 âˆ’t)y1 + ty2
and these can be combined into a single equation z = (1 âˆ’t)z1 + tz2.
Circles have various equation representations in terms of complex num-
bers, as will be seen later.
5.4
Complex conjugate
DEFINITION 5.4.1 (Complex conjugate) If z = x + iy, the complex
conjugate of z is the complex number deï¬ned by z = x âˆ’iy. Geometrically,
the complex conjugate of z is obtained by reï¬‚ecting z in the real axis (see
Figure 5.2).
The following properties of the complex conjugate are easy to verify:

5.4. COMPLEX CONJUGATE
97
-

6
?
x
y
z
z

>
ZZZZ
~
Figure 5.2: The complex conjugate of z: z.
1. z1 + z2 = z1 + z2;
2. âˆ’z = âˆ’z.
3. z1 âˆ’z2 = z1 âˆ’z2;
4. z1z2 = z1 z2;
5. (1/z) = 1/z;
6. (z1/z2) = z1/z2;
7. z is real if and only if z = z;
8. With the standard convention that the real and imaginary parts are
denoted by Re z and Im z, we have
Re z = z + z
2
,
Im z = z âˆ’z
2i
;
9. If z = x + iy, then zz = x2 + y2.
THEOREM 5.4.1 If f(z) is a polynomial with real coeï¬ƒcients, then its
nonâ€“real roots occur in complexâ€“conjugate pairs, i.e.
if f(z) = 0, then
f(z) = 0.
Proof.
Suppose f(z) = anzn + anâˆ’1znâˆ’1 + Â· Â· Â· + a1z + a0 = 0, where
an, . . . , a0 are real. Then
0 = 0 = f(z)
=
anzn + anâˆ’1znâˆ’1 + Â· Â· Â· + a1z + a0
=
an zn + anâˆ’1 znâˆ’1 + Â· Â· Â· + a1 z + a0
=
anzn + anâˆ’1znâˆ’1 + Â· Â· Â· + a1z + a0
=
f(z).

98
CHAPTER 5. COMPLEX NUMBERS
EXAMPLE 5.4.1 Discuss the position of the roots of the equation
z4 = âˆ’1
in the complex plane.
Solution. The equation z4 = âˆ’1 has real coeï¬ƒcients and so its roots come
in complex conjugate pairs. Also if z is a root, so is âˆ’z. Also there are
clearly no real roots and no imaginary roots. So there must be one root w
in the ï¬rst quadrant, with all remaining roots being given by w, âˆ’w and
âˆ’w. In fact, as we shall soon see, the roots lie evenly spaced on the unit
circle.
The following theorem is useful in deciding if a polynomial f(z) has a
multiple root a; that is if (z âˆ’a)m divides f(z) for some m â‰¥2. (The proof
is left as an exercise.)
THEOREM 5.4.2 If f(z) = (z âˆ’a)mg(z), where m â‰¥2 and g(z) is a
polynomial, then f â€²(a) = 0 and the polynomial and its derivative have a
common root.
From theorem 5.4.1 we obtain a result which is very useful in the explicit
integration of rational functions (i.e. ratios of polynomials) with real coeï¬ƒ-
cients.
THEOREM 5.4.3 If f(z) is a nonâ€“constant polynomial with real coeï¬ƒ-
cients, then f(z) can be factorized as a product of real linear factors and
real quadratic factors.
Proof. In general f(z) will have r real roots z1, . . . , zr and 2s nonâ€“real
roots zr+1, zr+1, . . . , zr+s, zr+s, occurring in complexâ€“conjugate pairs by
theorem 5.4.1. Then if an is the coeï¬ƒcient of highest degree in f(z), we
have the factorization
f(z)
=
an(z âˆ’z1) Â· Â· Â· (z âˆ’zr) Ã—
Ã—(z âˆ’zr+1)(z âˆ’zr+1) Â· Â· Â· (z âˆ’zr+s)(z âˆ’zr+s).
We then use the following identity for j = r + 1, . . . , r + s which in turn
shows that paired terms give rise to real quadratic factors:
(z âˆ’zj)(z âˆ’zj)
=
z2 âˆ’(zj + zj)z + zjzj
=
z2 âˆ’2Re zj + (x2
j + y2
j ),
where zj = xj + iyj.
A wellâ€“known example of such a factorization is the following:

5.5. MODULUS OF A COMPLEX NUMBER
99
-

6
?
|z|
x
y
z

>
Figure 5.3: The modulus of z: |z|.
EXAMPLE 5.4.2 Find a factorization of z4+1 into real linear and quadratic
factors.
Solution. Clearly there are no real roots. Also we have the preliminary
factorization z4 + 1 = (z2 âˆ’i)(z2 + i). Now the roots of z2 âˆ’i are easily
veriï¬ed to be Â±(1 + i)/
âˆš
2, so the roots of z2 + i must be Â±(1 âˆ’i)/
âˆš
2.
In other words the roots are w = (1 + i)/
âˆš
2 and w, âˆ’w, âˆ’w. Grouping
conjugateâ€“complex terms gives the factorization
z4 + 1
=
(z âˆ’w)(z âˆ’w)(z + w)(z + w)
=
(z2 âˆ’2zRe w + ww)(z2 + 2zRe w + ww)
=
(z2 âˆ’
âˆš
2z + 1)(z2 +
âˆš
2z + 1).
5.5
Modulus of a complex number
DEFINITION 5.5.1 (Modulus) If z = x + iy, the modulus of z is the
nonâ€“negative real number |z| deï¬ned by |z| =
p
x2 + y2. Geometrically, the
modulus of z is the distance from z to 0 (see Figure 5.3).
More generally, |z1âˆ’z2| is the distance between z1 and z2 in the complex
plane. For
|z1 âˆ’z2| = |(x1 + iy1) âˆ’(x2 + iy2)|
=
|(x1 âˆ’x2) + i(y1 âˆ’y2)|
=
p
(x1 âˆ’x2)2 + (y1 âˆ’y2)2.
The following properties of the modulus are easy to verify, using the identity
|z|2 = zz:
(i)
|z1z2| = |z1||z2|;

100
CHAPTER 5. COMPLEX NUMBERS
(ii)
|zâˆ’1| = |z|âˆ’1;
(iii)
Â¯Â¯Â¯Â¯
z1
z2
Â¯Â¯Â¯Â¯ = |z1|
|z2|.
For example, to prove (i):
|z1z2|2
=
(z1z2)z1z2 = (z1z2)z1 z2
=
(z1z1)(z2z2) = |z1|2|z2|2 = (|z1||z2|)2.
Hence |z1z2| = |z1||z2|.
EXAMPLE 5.5.1 Find |z| when z =
(1 + i)4
(1 + 6i)(2 âˆ’7i).
Solution.
|z|
=
|1 + i|4
|1 + 6i||2 âˆ’7i|
=
(
âˆš
12 + 12)4
âˆš
12 + 62p
22 + (âˆ’7)2
=
4
âˆš
37
âˆš
53.
THEOREM 5.5.1 (Ratio formulae) If z lies on the line through z1 and
z2:
z = (1 âˆ’t)z1 + tz2,
t âˆˆR,
we have the useful ratio formulae:
(i)
Â¯Â¯Â¯Â¯
z âˆ’z1
z âˆ’z2
Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯
t
1 âˆ’t
Â¯Â¯Â¯Â¯
if z Ì¸= z2,
(ii)
Â¯Â¯Â¯Â¯
z âˆ’z1
z1 âˆ’z2
Â¯Â¯Â¯Â¯
=
|t|.
Circle equations. The equation |z âˆ’z0| = r, where z0 âˆˆC and r >
0, represents the circle centre z0 and radius r. For example the equation
|z âˆ’(1 + 2i)| = 3 represents the circle (x âˆ’1)2 + (y âˆ’2)2 = 9.
Another useful circle equation is the circle of Apollonius :
Â¯Â¯Â¯Â¯
z âˆ’a
z âˆ’b
Â¯Â¯Â¯Â¯ = Î»,

5.5. MODULUS OF A COMPLEX NUMBER
101
-

6
?
x
y
Figure 5.4: Apollonius circles: |z+2i|
|zâˆ’2i| = 1
4, 3
8, 1
2, 5
8; 4
1, 8
3, 2
1, 8
5.
where a and b are distinct complex numbers and Î» is a positive real number,
Î» Ì¸= 1. (If Î» = 1, the above equation represents the perpendicular bisector
of the segment joining a and b.)
An algebraic proof that the above equation represents a circle, runs as
follows. We use the following identities:
(i)
|z âˆ’a|2
=
|z|2 âˆ’2Re (za) + |a|2
(ii)
Re (z1 Â± z2)
=
Re z1 Â± Re z2
(iii)
Re (tz)
=
tRe z if t âˆˆR.
We have
Â¯Â¯Â¯Â¯
z âˆ’a
z âˆ’b
Â¯Â¯Â¯Â¯ = Î» â‡”|z âˆ’a|2 = Î»2|z âˆ’b|2
â‡”
|z|2 âˆ’2Re {za} + |a|2 = Î»2(|z|2 âˆ’2Re {zb} + |b|2)
â‡”
(1 âˆ’Î»2)|z|2 âˆ’2Re {z(a âˆ’Î»2b)} = Î»2|b|2 âˆ’|a|2
â‡”
|z|2 âˆ’2Re
Â½
z
Âµa âˆ’Î»2b
1 âˆ’Î»2
Â¶Â¾
= Î»2|b|2 âˆ’|a|2
1 âˆ’Î»2
â‡”
|z|2 âˆ’2Re
Â½
z
Âµa âˆ’Î»2b
1 âˆ’Î»2
Â¶Â¾
+
Â¯Â¯Â¯Â¯
a âˆ’Î»2b
1 âˆ’Î»2
Â¯Â¯Â¯Â¯
2
= Î»2|b|2 âˆ’|a|2
1 âˆ’Î»2
+
Â¯Â¯Â¯Â¯
a âˆ’Î»2b
1 âˆ’Î»2
Â¯Â¯Â¯Â¯
2
.

102
CHAPTER 5. COMPLEX NUMBERS
Now it is easily veriï¬ed that
|a âˆ’Î»2b|2 + (1 âˆ’Î»2)(Î»2|b|2 âˆ’|a|2) = Î»2|a âˆ’b|2.
So we obtain
Â¯Â¯Â¯Â¯
z âˆ’a
z âˆ’b
Â¯Â¯Â¯Â¯ = Î»
â‡”
Â¯Â¯Â¯Â¯z âˆ’
Âµa âˆ’Î»2b
1 âˆ’Î»2
Â¶Â¯Â¯Â¯Â¯
2
= Î»2|a âˆ’b|2
|1 âˆ’Î»2|2
â‡”
Â¯Â¯Â¯Â¯z âˆ’
Âµa âˆ’Î»2b
1 âˆ’Î»2
Â¶Â¯Â¯Â¯Â¯ = Î»|a âˆ’b|
|1 âˆ’Î»2|.
The last equation represents a circle centre z0, radius r, where
z0 = a âˆ’Î»2b
1 âˆ’Î»2
and
r = Î»|a âˆ’b|
|1 âˆ’Î»2|.
There are two special points on the circle of Apollonius, the points z1 and
z2 deï¬ned by
z1 âˆ’a
z1 âˆ’b = Î»
and
z2 âˆ’a
z2 âˆ’b = âˆ’Î»,
or
z1 = a âˆ’Î»b
1 âˆ’Î»
and
z2 = a + Î»b
1 + Î» .
(5.3)
It is easy to verify that z1 and z2 are distinct points on the line through a
and b and that z0 = z1+z2
2
. Hence the circle of Apollonius is the circle based
on the segment z1, z2 as diameter.
EXAMPLE 5.5.2 Find the centre and radius of the circle
|z âˆ’1 âˆ’i| = 2|z âˆ’5 âˆ’2i|.
Solution. Method 1. Proceed algebraically and simplify the equation
|x + iy âˆ’1 âˆ’i| = 2|x + iy âˆ’5 âˆ’2i|
or
|x âˆ’1 + i(y âˆ’1)| = 2|x âˆ’5 + i(y âˆ’2)|.
Squaring both sides gives
(x âˆ’1)2 + (y âˆ’1)2 = 4((x âˆ’5)2 + (y âˆ’2)2),
which reduces to the circle equation
x2 + y2 âˆ’38
3 x âˆ’14
3 y + 38 = 0.

5.6. ARGUMENT OF A COMPLEX NUMBER
103
Completing the square gives
(x âˆ’19
3 )2 + (y âˆ’7
3)2 =
Âµ19
3
Â¶2
+
Âµ7
3
Â¶2
âˆ’38 = 68
9 ,
so the centre is ( 19
3 , 7
3) and the radius is
q
68
9 .
Method 2.
Calculate the diametrical points z1 and z2 deï¬ned above by
equations 5.3:
z1 âˆ’1 âˆ’i
=
2(z1 âˆ’5 âˆ’2i)
z2 âˆ’1 âˆ’i
=
âˆ’2(z2 âˆ’5 âˆ’2i).
We ï¬nd z1 = 9 + 3i and z2 = (11 + 5i)/3. Hence the centre z0 is given by
z0 = z1 + z2
2
= 19
3 + 7
3i
and the radius r is given by
r = |z1 âˆ’z0| =
Â¯Â¯Â¯Â¯
Âµ19
3 + 7
3i
Â¶
âˆ’(9 + 3i)
Â¯Â¯Â¯Â¯ =
Â¯Â¯Â¯Â¯âˆ’8
3 âˆ’2
3i
Â¯Â¯Â¯Â¯ =
âˆš
68
3 .
5.6
Argument of a complex number
Let z = x + iy be a nonâ€“zero complex number, r = |z| =
p
x2 + y2. Then
we have x = r cos Î¸, y = r sin Î¸, where Î¸ is the angle made by z with the
positive xâ€“axis. So Î¸ is unique up to addition of a multiple of 2Ï€ radians.
DEFINITION 5.6.1 (Argument) Any number Î¸ satisfying the above
pair of equations is called an argument of z and is denoted by arg z. The
particular argument of z lying in the range âˆ’Ï€ < Î¸ â‰¤Ï€ is called the principal
argument of z and is denoted by Arg z (see Figure 5.5).
We have z = r cos Î¸ + ir sin Î¸ = r(cos Î¸ + i sin Î¸) and this representation
of z is called the polar representation or modulusâ€“argument form of z.
EXAMPLE 5.6.1 Arg 1 = 0, Arg (âˆ’1) = Ï€, Arg i = Ï€
2 , Arg (âˆ’i) = âˆ’Ï€
2 .
We note that y/x = tan Î¸ if x Ì¸= 0, so Î¸ is determined by this equation up
to a multiple of Ï€. In fact
Arg z = tanâˆ’1 y
x + kÏ€,

104
CHAPTER 5. COMPLEX NUMBERS
-

6
?

>
x
y
r
z
Î¸
Figure 5.5: The argument of z: arg z = Î¸.
where k = 0 if x > 0; k = 1 if x < 0, y > 0; k = âˆ’1 if x < 0, y < 0.
To determine Arg z graphically, it is simplest to draw the triangle formed
by the points 0, x, z on the complex plane, mark in the positive acute angle
Î± between the rays 0, x and 0, z and determine Arg z geometrically, using
the fact that Î± = tanâˆ’1(|y|/|x|), as in the following examples:
EXAMPLE 5.6.2 Determine the principal argument of z for the followig
complex numbers:
z = 4 + 3i, âˆ’4 + 3i, âˆ’4 âˆ’3i, 4 âˆ’3i.
Solution. Referring to Figure 5.6, we see that Arg z has the values
Î±, Ï€ âˆ’Î±, âˆ’Ï€ + Î±, âˆ’Î±,
where Î± = tanâˆ’1 3
4.
An important property of the argument of a complex number states that
the sum of the arguments of two nonâ€“zero complex numbers is an argument
of their product:
THEOREM 5.6.1 If Î¸1 and Î¸2 are arguments of z1 and z2, then Î¸1 + Î¸2
is an argument of z1z2.
Proof. Let z1 and z2 have polar representations z1 = r1(cos Î¸1 + i sin Î¸1)
and z2 = r2(cos Î¸2 + i sin Î¸2). Then
z1z2
=
r1(cos Î¸1 + i sin Î¸1)r2(cos Î¸2 + i sin Î¸2)
=
r1r2(cos Î¸1 cos Î¸2 âˆ’sin Î¸1 sin Î¸2 + i(cos Î¸1 sin Î¸2 + sin Î¸1 cos Î¸2))
=
r1r2(cos (Î¸1 + Î¸2) + i sin (Î¸1 + Î¸2)),

5.6. ARGUMENT OF A COMPLEX NUMBER
105
-

6
?
x
y
4 + 3i
Î±

>
-

6
?
x
y
âˆ’4 + 3i
Î±Z
Z
Z
Z
}
-

6
?
x
y
âˆ’4 âˆ’3i
Î±



=
-

6
?
x
y
4 âˆ’3i
Î±
ZZZZ
~
Figure 5.6: Argument examples.
which is the polar representation of z1z2, as r1r2 = |z1||z2| = |z1z2|. Hence
Î¸1 + Î¸2 is an argument of z1z2.
An easy induction gives the following generalization to a product of n
complex numbers:
COROLLARY 5.6.1 If Î¸1, . . . , Î¸n are arguments for z1, . . . , zn respectively,
then Î¸1 + Â· Â· Â· + Î¸n is an argument for z1 Â· Â· Â· zn.
Taking Î¸1 = Â· Â· Â· = Î¸n = Î¸ in the previous corollary gives
COROLLARY 5.6.2 If Î¸ is an argument of z, then nÎ¸ is an argument for
zn.
THEOREM 5.6.2 If Î¸ is an argument of the nonâ€“zero complex number
z, then âˆ’Î¸ is an argument of zâˆ’1.
Proof. Let Î¸ be an argument of z. Then z = r(cos Î¸+i sin Î¸), where r = |z|.
Hence
zâˆ’1
=
râˆ’1(cos Î¸ + i sin Î¸)âˆ’1
=
râˆ’1(cos Î¸ âˆ’i sin Î¸)
=
râˆ’1(cos(âˆ’Î¸) + i sin(âˆ’Î¸)).

106
CHAPTER 5. COMPLEX NUMBERS
Now râˆ’1 = |z|âˆ’1 = |zâˆ’1|, so âˆ’Î¸ is an argument of zâˆ’1.
COROLLARY 5.6.3 If Î¸1 and Î¸2 are arguments of z1 and z2, then Î¸1 âˆ’Î¸2
is an argument of z1/z2.
In terms of principal arguments, we have the following equations:
(i)
Arg (z1z2)
=
Arg z1+Arg z2 + 2k1Ï€,
(ii)
Arg (zâˆ’1)
=
âˆ’Arg z + 2k2Ï€,
(iii)
Arg (z1/z2)
=
Arg z1âˆ’Arg z2 + 2k3Ï€,
(iv)
Arg (z1 Â· Â· Â· zn)
=
Arg z1 + Â· Â· Â· +Arg zn + 2k4Ï€,
(v)
Arg (zn)
=
n Arg z + 2k5Ï€,
where k1, k2, k3, k4, k5 are integers.
In numerical examples, we can write (i), for example, as
Arg (z1z2) â‰¡Arg z1 + Arg z2.
EXAMPLE 5.6.3 Find the modulus and principal argument of
z =
Ãƒâˆš
3 + i
1 + i
!17
and hence express z in modulusâ€“argument form.
Solution. |z| = |
âˆš
3 + i|17
|1 + i|17
=
217
(
âˆš
2)17 = 217/2.
Arg z
â‰¡
17Arg
Ãƒâˆš
3 + i
1 + i
!
=
17(Arg (
âˆš
3 + i) âˆ’Arg (1 + i))
=
17
Â³Ï€
6 âˆ’Ï€
4
Â´
= âˆ’17Ï€
12
.
Hence Arg z =
Â¡ âˆ’17Ï€
12
Â¢
+ 2kÏ€, where k is an integer. We see that k = 1 and
hence Arg z = 7Ï€
12 . Consequently z = 217/2 Â¡
cos 7Ï€
12 + i sin 7Ï€
12
Â¢
.
DEFINITION 5.6.2 If Î¸ is a real number, then we deï¬ne eiÎ¸ by
eiÎ¸ = cos Î¸ + i sin Î¸.
More generally, if z = x + iy, then we deï¬ne ez by
ez = exeiy.

5.7. DE MOIVREâ€™S THEOREM
107
For example,
e
iÏ€
2 = i, eiÏ€ = âˆ’1, eâˆ’iÏ€
2 = âˆ’i.
The following properties of the complex exponential function are left as
exercises:
THEOREM 5.6.3
(i)
ez1ez2
=
ez1+z2,
(ii)
ez1 Â· Â· Â· ezn
=
ez1+Â·Â·Â·+zn,
(iii)
ez
Ì¸=
0,
(iv)
(ez)âˆ’1
=
eâˆ’z,
(v)
ez1/ez2
=
ez1âˆ’z2,
(vi)
ez
=
ez.
THEOREM 5.6.4 The equation
ez = 1
has the complete solution z = 2kÏ€i, k âˆˆZ.
Proof. First we observe that
e2kÏ€i = cos (2kÏ€) + i sin (2kÏ€) = 1.
Conversely, suppose ez = 1, z = x + iy. Then ex(cos y + i sin y) = 1. Hence
ex cos y = 1 and ex sin y = 0. Hence sin y = 0 and so y = nÏ€, n âˆˆZ. Then
ex cos (nÏ€) = 1, so ex(âˆ’1)n = 1, from which follows (âˆ’1)n = 1 as ex > 0.
Hence n = 2k, k âˆˆZ and ex = 1. Hence x = 0 and z = 2kÏ€i.
5.7
De Moivreâ€™s theorem
The next theorem has many uses and is a special case of theorem 5.6.3(ii).
Alternatively it can be proved directly by induction on n.
THEOREM 5.7.1 (De Moivre) If n is a positive integer, then
(cos Î¸ + i sin Î¸)n = cos nÎ¸ + i sin nÎ¸.
As a ï¬rst application, we consider the equation zn = 1.
THEOREM 5.7.2 The equation zn = 1 has n distinct solutions, namely
the complex numbers Î¶k = e
2kÏ€i
n , k = 0, 1, . . . , n âˆ’1.
These lie equally
spaced on the unit circle |z| = 1 and are obtained by starting at 1, moving
round the circle antiâ€“clockwise, incrementing the argument in steps of 2Ï€
n .
(See Figure 5.7)
We notice that the roots are the powers of the special root Î¶ = e
2Ï€i
n .

108
CHAPTER 5. COMPLEX NUMBERS
-
-
Î¶0
Î¶1
Î¶2
Î¶nâˆ’1
6






7
Â©Â©Â©Â©Â©Â©Â©Â©
*
HHHHHHHH
j
2Ï€/n
2Ï€/n
2Ï€/n
|z| = 1
Figure 5.7: The nth roots of unity.
Proof. With Î¶k deï¬ned as above,
Î¶n
k =
Â³
e
2kÏ€i
n
Â´n
= e
2kÏ€i
n n = 1,
by De Moivreâ€™s theorem. However |Î¶k| = 1 and arg Î¶k = 2kÏ€
n , so the com-
plex numbers Î¶k, k = 0, 1, . . . , n âˆ’1, lie equally spaced on the unit circle.
Consequently these numbers must be precisely all the roots of zn âˆ’1. For
the polynomial zn âˆ’1, being of degree n over a ï¬eld, can have at most n
distinct roots in that ï¬eld.
The more general equation zn = a, where a âˆˆ, C, a Ì¸= 0, can be reduced
to the previous case:
Let Î± be argument of z, so that a = |a|eiÎ±. Then if w = |a|1/ne
iÎ±
n , we
have
wn
=
Â³
|a|1/ne
iÎ±
n
Â´n
=
(|a|1/n)n Â³
e
iÎ±
n
Â´n
=
|a|eiÎ± = a.
So w is a particular solution. Substituting for a in the original equation,
we get zn = wn, or (z/w)n = 1. Hence the complete solution is z/w =

5.7. DE MOIVREâ€™S THEOREM
109
-
z0
z1
znâˆ’1
6
PPPPPPPPP
q
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢Â¢
Â©Â©Â©Â©Â©Â©Â©Â©
Â©
*
Î±
2Ï€/n
|z| = (|a|)1/n
Figure 5.8: The roots of zn = a.
e
2kÏ€i
n , k = 0, 1, . . . , n âˆ’1, or
zk = |a|1/ne
iÎ±
n e
2kÏ€i
n
= |a|1/ne
i(Î±+2kÏ€)
n
,
(5.4)
k = 0, 1, . . . , n âˆ’1. So the roots are equally spaced on the circle
|z| = |a|1/n
and are generated from the special solution having argument equal to (arg a)/n,
by incrementing the argument in steps of 2Ï€/n. (See Figure 5.8.)
EXAMPLE 5.7.1 Factorize the polynomial z5 âˆ’1 as a product of real
linear and quadratic factors.
Solution. The roots are 1, e
2Ï€i
5 , e
âˆ’2Ï€i
5 , e
4Ï€i
5 , e
âˆ’4Ï€i
5 , using the fact that nonâ€“
real roots come in conjugateâ€“complex pairs. Hence
z5 âˆ’1 = (z âˆ’1)(z âˆ’e
2Ï€i
5 )(z âˆ’e
âˆ’2Ï€i
5 )(z âˆ’e
4Ï€i
5 )(z âˆ’e
âˆ’4Ï€i
5
).
Now
(z âˆ’e
2Ï€i
5 )(z âˆ’e
âˆ’2Ï€i
5 )
=
z2 âˆ’z(e
2Ï€i
5 + e
âˆ’2Ï€i
5 ) + 1
=
z2 âˆ’2z cos 2Ï€
5 + 1.

110
CHAPTER 5. COMPLEX NUMBERS
Similarly
(z âˆ’e
4Ï€i
5 )(z âˆ’e
âˆ’4Ï€i
5 ) = z2 âˆ’2z cos 4Ï€
5 + 1.
This gives the desired factorization.
EXAMPLE 5.7.2 Solve z3 = i.
Solution. |i| = 1 and Arg i = Ï€
2 = Î±. So by equation 5.4, the solutions are
zk = |i|1/3e
i(Î±+2kÏ€)
3
, k = 0, 1, 2.
First, k = 0 gives
z0 = e
iÏ€
6 = cos Ï€
6 + i sin Ï€
6 =
âˆš
3
2 + i
2.
Next, k = 1 gives
z1 = e
5Ï€i
6 = cos 5Ï€
6 + i sin 5Ï€
6 = âˆ’
âˆš
3
2
+ i
2.
Finally, k = 2 gives
z1 = e
9Ï€i
6 = cos 9Ï€
6 + i sin 9Ï€
6 = âˆ’i.
We ï¬nish this chapter with two more examples of De Moivreâ€™s theorem.
EXAMPLE 5.7.3 If
C
=
1 + cos Î¸ + Â· Â· Â· + cos (n âˆ’1)Î¸,
S
=
sin Î¸ + Â· Â· Â· + sin (n âˆ’1)Î¸,
prove that
C = sin nÎ¸
2
sin Î¸
2
cos (nâˆ’1)Î¸
2
and S = sin nÎ¸
2
sin Î¸
2
sin (nâˆ’1)Î¸
2
,
if Î¸ Ì¸= 2kÏ€, k âˆˆZ.

5.8. PROBLEMS
111
Solution.
C + iS
=
1 + (cos Î¸ + i sin Î¸) + Â· Â· Â· + (cos (n âˆ’1)Î¸ + i sin (n âˆ’1)Î¸)
=
1 + eiÎ¸ + Â· Â· Â· + ei(nâˆ’1)Î¸
=
1 + z + Â· Â· Â· + znâˆ’1, where z = eiÎ¸
=
1 âˆ’zn
1 âˆ’z , if z Ì¸= 1, i.e. Î¸ Ì¸= 2kÏ€,
=
1 âˆ’einÎ¸
1 âˆ’eiÎ¸ = e
inÎ¸
2 (e
âˆ’inÎ¸
2
âˆ’e
inÎ¸
2 )
e
iÎ¸
2 (e
âˆ’iÎ¸
2 âˆ’e
iÎ¸
2 )
=
ei(nâˆ’1) Î¸
2 sin nÎ¸
2
sin Î¸
2
=
(cos (n âˆ’1) Î¸
2 + i sin (n âˆ’1) Î¸
2)sin nÎ¸
2
sin Î¸
2
.
The result follows by equating real and imaginary parts.
EXAMPLE 5.7.4 Express cos nÎ¸ and sin nÎ¸ in terms of cos Î¸ and sin Î¸,
using the equation cos nÎ¸ + sin nÎ¸ = (cos Î¸ + i sin Î¸)n.
Solution. The binomial theorem gives
(cos Î¸ + i sin Î¸)n = cosn Î¸ +
Â¡n
1
Â¢
cosnâˆ’1 Î¸(i sin Î¸) +
Â¡n
2
Â¢
cosnâˆ’2 Î¸(i sin Î¸)2 + Â· Â· Â·
+ (i sin Î¸)n.
Equating real and imaginary parts gives
cos nÎ¸ = cosn Î¸ âˆ’
Â¡n
2
Â¢
cosnâˆ’2 Î¸ sin2 Î¸ + Â· Â· Â·
sin nÎ¸ =
Â¡n
1
Â¢
cosnâˆ’1 Î¸ sin Î¸ âˆ’
Â¡n
3
Â¢
cosnâˆ’3 Î¸ sin3 Î¸ + Â· Â· Â· .
5.8
PROBLEMS
1. Express the following complex numbers in the form x + iy, x, y real:
(i) (âˆ’3 + i)(14 âˆ’2i); (ii) 2 + 3i
1 âˆ’4i; (iii) (1 + 2i)2
1 âˆ’i
.
[Answers: (i) âˆ’40 + 20i; (ii) âˆ’10
17 + 11
17i; (iii) âˆ’7
2 + i
2.]
2. Solve the following equations:

112
CHAPTER 5. COMPLEX NUMBERS
(i)
iz + (2 âˆ’10i)z
=
3z + 2i,
(ii)
(1 + i)z + (2 âˆ’i)w
=
âˆ’3i
(1 + 2i)z + (3 + i)w
=
2 + 2i.
[Answers:(i) z = âˆ’9
41 âˆ’
i
41; (ii) z = âˆ’1 + 5i, w = 19
5 âˆ’8i
5 .]
3. Express 1 + (1 + i) + (1 + i)2 + . . . + (1 + i)99 in the form x + iy, x, y
real. [Answer: (1 + 250)i.]
4. Solve the equations: (i) z2 = âˆ’8 âˆ’6i;
(ii) z2 âˆ’(3 + i)z + 4 + 3i = 0.
[Answers: (i) z = Â±(1 âˆ’3i);
(ii) z = 2 âˆ’i, 1 + 2i.]
5. Find the modulus and principal argument of each of the following
complex numbers:
(i) 4 + i;
(ii) âˆ’3
2 âˆ’i
2;
(iii) âˆ’1 + 2i;
(iv) 1
2(âˆ’1 + i
âˆš
3).
[Answers:
(i)
âˆš
17, tanâˆ’1 1
4; (ii)
âˆš
10
2 , âˆ’Ï€ + tanâˆ’1 1
3; (iii)
âˆš
5, Ï€ âˆ’
tanâˆ’1 2.]
6. Express the following complex numbers in modulus-argument form:
(i) z = (1 + i)(1 + i
âˆš
3)(
âˆš
3 âˆ’i).
(ii) z = (1 + i)5(1 âˆ’i
âˆš
3)5
(
âˆš
3 + i)4
.
[Answers:
(i) z = 4
âˆš
2(cos 5Ï€
12 + i sin 5Ï€
12 );
(ii) z = 27/2(cos 11Ï€
12 + i sin 11Ï€
12 ).]
7.
(i) If z = 2(cos Ï€
4 +i sin Ï€
4 ) and w = 3(cos Ï€
6 +i sin Ï€
6 ), ï¬nd the polar
form of
(a) zw; (b) z
w; (c) w
z ; (d) z5
w2 .
(ii) Express the following complex numbers in the form x + iy:
(a) (1 + i)12; (b)
Â³
1âˆ’i
âˆš
2
Â´âˆ’6
.
[Answers: (i): (a) 6(cos 5Ï€
12 + i sin 5Ï€
12 );
(b) 2
3(cos
Ï€
12 + i sin
Ï€
12);
(c) 3
2(cos âˆ’Ï€
12 + i sin âˆ’Ï€
12);
(d) 32
9 (cos 11Ï€
12 + i sin 11Ï€
12 );
(ii): (a) âˆ’64;
(b) âˆ’i.]

5.8. PROBLEMS
113
8. Solve the equations:
(i) z2 = 1 + i
âˆš
3; (ii) z4 = i; (iii) z3 = âˆ’8i; (iv) z4 = 2 âˆ’2i.
[Answers: (i) z = Â± (
âˆš
3+i)
âˆš
2
; (ii) ik(cos Ï€
8 + i sin Ï€
8 ), k = 0, 1, 2, 3; (iii)
z = 2i, âˆ’
âˆš
3âˆ’i,
âˆš
3âˆ’i; (iv) z = ik2
3
8 (cos
Ï€
16 âˆ’i sin
Ï€
16), k = 0, 1, 2, 3.]
9. Find the reduced rowâ€“echelon form of the complex matrix
ï£®
ï£°
2 + i
âˆ’1 + 2i
2
1 + i
âˆ’1 + i
1
1 + 2i
âˆ’2 + i
1 + i
ï£¹
ï£».
[Answer:
ï£®
ï£°
1
i
0
0
0
1
0
0
0
ï£¹
ï£».]
10.
(i) Prove that the line equation lx + my = n is equivalent to
pz + pz = 2n,
where p = l + im.
(ii) Use (ii) to deduce that reï¬‚ection in the straight line
pz + pz = n
is described by the equation
pw + pz = n.
[Hint: The complex number l + im is perpendicular to the given
line.]
(iii) Prove that the line |zâˆ’a| = |zâˆ’b| may be written as pz+pz = n,
where p = b âˆ’a and n = |b|2 âˆ’|a|2. Deduce that if z lies on the
Apollonius circle |zâˆ’a|
|zâˆ’b| = Î», then w, the reï¬‚ection of z in the line
|z âˆ’a| = |z âˆ’b|, lies on the Apollonius circle |zâˆ’a|
|zâˆ’b| = 1
Î».
11. Let a and b be distinct complex numbers and 0 < Î± < Ï€.
(i) Prove that each of the following sets in the complex plane rep-
resents a circular arc and sketch the circular arcs on the same
diagram:

114
CHAPTER 5. COMPLEX NUMBERS
Arg z âˆ’a
z âˆ’b = Î±, âˆ’Î±, Ï€ âˆ’Î±, Î± âˆ’Ï€.
Also show that Arg z âˆ’a
z âˆ’b = Ï€ represents the line segment joining
a and b, while Arg z âˆ’a
z âˆ’b = 0 represents the remaining portion of
the line through a and b.
(ii) Use (i) to prove that four distinct points z1, z2, z3, z4 are con-
cyclic or collinear, if and only if the crossâ€“ratio
z4 âˆ’z1
z4 âˆ’z2
/z3 âˆ’z1
z3 âˆ’z2
is real.
(iii) Use (ii) to derive Ptolemyâ€™s Theorem: Four distinct points A, B, C, D
are concyclic or collinear, if and only if one of the following holds:
AB Â· CD + BC Â· AD
=
AC Â· BD
BD Â· AC + AD Â· BC
=
AB Â· CD
BD Â· AC + AB Â· CD
=
AD Â· BC.

Chapter 6
EIGENVALUES AND
EIGENVECTORS
6.1
Motivation
We motivate the chapter on eigenvalues by discussing the equation
ax2 + 2hxy + by2 = c,
where not all of a, h, b are zero. The expression ax2 + 2hxy + by2 is called
a quadratic form in x and y and we have the identity
ax2 + 2hxy + by2 =
Â£
x
y
Â¤ Â· a
h
h
b
Â¸ Â· x
y
Â¸
= XtAX,
where X =
Â· x
y
Â¸
and A =
Â· a
h
h
b
Â¸
. A is called the matrix of the quadratic
form.
We now rotate the x, y axes anticlockwise through Î¸ radians to new
x1, y1 axes. The equations describing the rotation of axes are derived as
follows:
Let P have coordinates (x, y) relative to the x, y axes and coordinates
(x1, y1) relative to the x1, y1 axes. Then referring to Figure 6.1:
115

116
CHAPTER 6. EIGENVALUES AND EIGENVECTORS
-

6
?
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

@
@
@
@
@
@
@
@
I
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡
Â¡

@
@
@
@
@
@
@
@
R









@
@
@
x
y
x1
y1
P
Q
R
O
Î±
Î¸
Figure 6.1: Rotating the axes.
x
=
OQ = OP cos (Î¸ + Î±)
=
OP(cos Î¸ cos Î± âˆ’sin Î¸ sin Î±)
=
(OP cos Î±) cos Î¸ âˆ’(OP sin Î±) sin Î¸
=
OR cos Î¸ âˆ’PR sin Î¸
=
x1 cos Î¸ âˆ’y1 sin Î¸.
Similarly y = x1 sin Î¸ + y1 cos Î¸.
We can combine these transformation equations into the single matrix
equation:
Â· x
y
Â¸
=
Â· cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¸ Â· x1
y1
Â¸
,
or X = PY , where X =
Â· x
y
Â¸
, Y =
Â· x1
y1
Â¸
and P =
Â· cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¸
.
We note that the columns of P give the directions of the positive x1 and y1
axes. Also P is an orthogonal matrix â€“ we have PP t = I2 and so P âˆ’1 = P t.
The matrix P has the special property that det P = 1.
A matrix of the type P =
Â· cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¸
is called a rotation matrix.
We shall show soon that any 2 Ã— 2 real orthogonal matrix with determinant

6.1. MOTIVATION
117
equal to 1 is a rotation matrix.
We can also solve for the new coordinates in terms of the old ones:
Â· x1
y1
Â¸
= Y = P tX =
Â·
cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸
Â¸ Â· x
y
Â¸
,
so x1 = x cos Î¸ + y sin Î¸ and y1 = âˆ’x sin Î¸ + y cos Î¸. Then
XtAX = (PY )tA(PY ) = Y t(P tAP)Y.
Now suppose, as we later show, that it is possible to choose an angle Î¸ so
that P tAP is a diagonal matrix, say diag(Î»1, Î»2). Then
XtAX =
Â£
x1
y1
Â¤ Â· Î»1
0
0
Î»2
Â¸ Â· x1
y1
Â¸
= Î»1x2
1 + Î»2y2
1
(6.1)
and relative to the new axes, the equation ax2 + 2hxy + by2 = c becomes
Î»1x2
1 + Î»2y2
1 = c, which is quite easy to sketch. This curve is symmetrical
about the x1 and y1 axes, with P1 and P2, the respective columns of P,
giving the directions of the axes of symmetry.
Also it can be veriï¬ed that P1 and P2 satisfy the equations
AP1 = Î»1P1 and AP2 = Î»2P2.
These equations force a restriction on Î»1 and Î»2. For if P1 =
Â· u1
v1
Â¸
, the
ï¬rst equation becomes
Â· a
h
h
b
Â¸ Â· u1
v1
Â¸
= Î»1
Â· u1
v1
Â¸
or
Â· a âˆ’Î»1
h
h
b âˆ’Î»1
Â¸ Â· u1
v1
Â¸
=
Â· 0
0
Â¸
.
Hence we are dealing with a homogeneous system of two linear equations in
two unknowns, having a nonâ€“trivial solution (u1, v1). Hence
Â¯Â¯Â¯Â¯
a âˆ’Î»1
h
h
b âˆ’Î»1
Â¯Â¯Â¯Â¯ = 0.
Similarly, Î»2 satisï¬es the same equation.
In expanded form, Î»1 and Î»2
satisfy
Î»2 âˆ’(a + b)Î» + ab âˆ’h2 = 0.
This equation has real roots
Î» = a + b Â±
p
(a + b)2 âˆ’4(ab âˆ’h2)
2
= a + b Â±
p
(a âˆ’b)2 + 4h2
2
(6.2)
(The roots are distinct if a Ì¸= b or h Ì¸= 0. The case a = b and h = 0 needs
no investigation, as it gives an equation of a circle.)
The equation Î»2 âˆ’(a+b)Î»+abâˆ’h2 = 0 is called the eigenvalue equation
of the matrix A.

118
CHAPTER 6. EIGENVALUES AND EIGENVECTORS
6.2
Deï¬nitions and examples
DEFINITION 6.2.1 (Eigenvalue, eigenvector)
Let A be a complex square matrix. Then if Î» is a complex number and
X a nonâ€“zero complex column vector satisfying AX = Î»X, we call X an
eigenvector of A, while Î» is called an eigenvalue of A. We also say that X
is an eigenvector corresponding to the eigenvalue Î».
So in the above example P1 and P2 are eigenvectors corresponding to Î»1
and Î»2, respectively.
We shall give an algorithm which starts from the
eigenvalues of A =
Â· a
h
h
b
Â¸
and constructs a rotation matrix P such that
P tAP is diagonal.
As noted above, if Î» is an eigenvalue of an n Ã— n matrix A, with
corresponding eigenvector X, then (A âˆ’Î»In)X = 0, with X Ì¸= 0, so
det (A âˆ’Î»In) = 0 and there are at most n distinct eigenvalues of A.
Conversely if det (A âˆ’Î»In) = 0, then (A âˆ’Î»In)X = 0 has a nonâ€“trivial
solution X and so Î» is an eigenvalue of A with X a corresponding eigenvector.
DEFINITION 6.2.2 (Characteristic equation, polynomial)
The equation det (A âˆ’Î»In) = 0 is called the characteristic equation of A,
while the polynomial det (A âˆ’Î»In) is called the characteristic polynomial of
A. The characteristic polynomial of A is often denoted by chA(Î»).
Hence the eigenvalues of A are the roots of the characteristic polynomial
of A.
For a 2 Ã— 2 matrix A =
Â· a
b
c
d
Â¸
, it is easily veriï¬ed that the character-
istic polynomial is Î»2 âˆ’(trace A)Î» + det A, where trace A = a + d is the sum
of the diagonal elements of A.
EXAMPLE 6.2.1 Find the eigenvalues of A =
Â· 2
1
1
2
Â¸
and ï¬nd all eigen-
vectors.
Solution. The characteristic equation of A is Î»2 âˆ’4Î» + 3 = 0, or
(Î» âˆ’1)(Î» âˆ’3) = 0.
Hence Î» = 1 or 3. The eigenvector equation (A âˆ’Î»In)X = 0 reduces to
Â· 2 âˆ’Î»
1
1
2 âˆ’Î»
Â¸ Â· x
y
Â¸
=
Â· 0
0
Â¸
,

6.2. DEFINITIONS AND EXAMPLES
119
or
(2 âˆ’Î»)x + y
=
0
x + (2 âˆ’Î»)y
=
0.
Taking Î» = 1 gives
x + y
=
0
x + y
=
0,
which has solution x = âˆ’y, y arbitrary.
Consequently the eigenvectors
corresponding to Î» = 1 are the vectors
Â· âˆ’y
y
Â¸
, with y Ì¸= 0.
Taking Î» = 3 gives
âˆ’x + y
=
0
x âˆ’y
=
0,
which has solution x = y, y arbitrary. Consequently the eigenvectors corre-
sponding to Î» = 3 are the vectors
Â· y
y
Â¸
, with y Ì¸= 0.
Our next result has wide applicability:
THEOREM 6.2.1 Let A be a 2 Ã— 2 matrix having distinct eigenvalues Î»1
and Î»2 and corresponding eigenvectors X1 and X2. Let P be the matrix
whose columns are X1 and X2, respectively. Then P is nonâ€“singular and
P âˆ’1AP =
Â· Î»1
0
0
Î»2
Â¸
.
Proof. Suppose AX1 = Î»1X1 and AX2 = Î»2X2. We show that the system
of homogeneous equations
xX1 + yX2 = 0
has only the trivial solution.
Then by theorem 2.5.10 the matrix P =
[X1|X2] is nonâ€“singular. So assume
xX1 + yX2 = 0.
(6.3)
Then A(xX1 + yX2) = A0 = 0, so x(AX1) + y(AX2) = 0. Hence
xÎ»1X1 + yÎ»2X2 = 0.
(6.4)

120
CHAPTER 6. EIGENVALUES AND EIGENVECTORS
Multiplying equation 6.3 by Î»1 and subtracting from equation 6.4 gives
(Î»2 âˆ’Î»1)yX2 = 0.
Hence y = 0, as (Î»2âˆ’Î»1) Ì¸= 0 and X2 Ì¸= 0. Then from equation 6.3, xX1 = 0
and hence x = 0.
Then the equations AX1 = Î»1X1 and AX2 = Î»2X2 give
AP = A[X1|X2] = [AX1|AX2]
=
[Î»1X1|Î»2X2]
=
[X1|X2]
Â· Î»1
0
0
Î»2
Â¸
= P
Â· Î»1
0
0
Î»2
Â¸
,
so
P âˆ’1AP =
Â· Î»1
0
0
Î»2
Â¸
.
EXAMPLE 6.2.2 Let A =
Â· 2
1
1
2
Â¸
be the matrix of example 6.2.1. Then
X1 =
Â· âˆ’1
1
Â¸
and X2 =
Â· 1
1
Â¸
are eigenvectors corresponding to eigenvalues
1 and 3, respectively. Hence if P =
Â· âˆ’1
1
1
1
Â¸
, we have
P âˆ’1AP =
Â· 1
0
0
3
Â¸
.
There are two immediate applications of theorem 6.2.1. The ï¬rst is to the
calculation of An: If P âˆ’1AP = diag (Î»1, Î»2), then A = Pdiag (Î»1, Î»2)P âˆ’1
and
An =
Âµ
P
Â· Î»1
0
0
Î»2
Â¸
P âˆ’1
Â¶n
= P
Â· Î»1
0
0
Î»2
Â¸n
P âˆ’1 = P
Â· Î»n
1
0
0
Î»n
2
Â¸
P âˆ’1.
The second application is to solving a system of linear diï¬€erential equations
dx
dt
=
ax + by
dy
dt
=
cx + dy,
where A =
Â· a
b
c
d
Â¸
is a matrix of real or complex numbers and x and y
are functions of t. The system can be written in matrix form as Ë™X = AX,
where
X =
Â· x
y
Â¸
and Ë™X =
Â· Ë™x
Ë™y
Â¸
=
Â· dx
dt
dy
dt
Â¸
.

6.2. DEFINITIONS AND EXAMPLES
121
We make the substitution X = PY , where Y =
Â· x1
y1
Â¸
. Then x1 and y1
are also functions of t and
Ë™X = P Ë™Y = AX = A(PY ), so Ë™Y = (P âˆ’1AP)Y =
Â· Î»1
0
0
Î»2
Â¸
Y.
Hence Ë™x1 = Î»1x1 and Ë™y1 = Î»2y1.
These diï¬€erential equations are wellâ€“known to have the solutions x1 =
x1(0)eÎ»1t and y1 = y1(0)eÎ»2t, where x1(0) is the value of x1 when t = 0.
[If dx
dt = kx, where k is a constant, then
d
dt
Â³
eâˆ’ktx
Â´
= âˆ’keâˆ’ktx + eâˆ’kt dx
dt = âˆ’keâˆ’ktx + eâˆ’ktkx = 0.
Hence eâˆ’ktx is constant, so eâˆ’ktx = eâˆ’k0x(0) = x(0). Hence x = x(0)ekt.]
However
Â· x1(0)
y1(0)
Â¸
= P âˆ’1
Â· x(0)
y(0)
Â¸
, so this determines x1(0) and y1(0) in
terms of x(0) and y(0). Hence ultimately x and y are determined as explicit
functions of t, using the equation X = PY .
EXAMPLE 6.2.3 Let A =
Â· 2
âˆ’3
4
âˆ’5
Â¸
.
Use the eigenvalue method to
derive an explicit formula for An and also solve the system of diï¬€erential
equations
dx
dt
=
2x âˆ’3y
dy
dt
=
4x âˆ’5y,
given x = 7 and y = 13 when t = 0.
Solution. The characteristic polynomial of A is Î»2+3Î»+2 which has distinct
roots Î»1 = âˆ’1 and Î»2 = âˆ’2. We ï¬nd corresponding eigenvectors X1 =
Â· 1
1
Â¸
and X2 =
Â· 3
4
Â¸
. Hence if P =
Â· 1
3
1
4
Â¸
, we have P âˆ’1AP = diag (âˆ’1, âˆ’2).
Hence
An
=
Â¡
Pdiag (âˆ’1, âˆ’2)P âˆ’1Â¢n = Pdiag ((âˆ’1)n, (âˆ’2)n)P âˆ’1
=
Â· 1
3
1
4
Â¸ Â· (âˆ’1)n
0
0
(âˆ’2)n
Â¸ Â·
4
âˆ’3
âˆ’1
1
Â¸

122
CHAPTER 6. EIGENVALUES AND EIGENVECTORS
=
(âˆ’1)n
Â· 1
3
1
4
Â¸ Â· 1
0
0
2n
Â¸ Â·
4
âˆ’3
âˆ’1
1
Â¸
=
(âˆ’1)n
Â· 1
3 Ã— 2n
1
4 Ã— 2n
Â¸ Â·
4
âˆ’3
âˆ’1
1
Â¸
=
(âˆ’1)n
Â· 4 âˆ’3 Ã— 2n
âˆ’3 + 3 Ã— 2n
4 âˆ’4 Ã— 2n
âˆ’3 + 4 Ã— 2n
Â¸
.
To solve the diï¬€erential equation system, make the substitution X =
PY . Then x = x1 + 3y1, y = x1 + 4y1. The system then becomes
Ë™x1
=
âˆ’x1
Ë™y1
=
âˆ’2y1,
so x1 = x1(0)eâˆ’t, y1 = y1(0)eâˆ’2t. Now
Â· x1(0)
y1(0)
Â¸
= P âˆ’1
Â· x(0)
y(0)
Â¸
=
Â·
4
âˆ’3
âˆ’1
1
Â¸ Â· 7
13
Â¸
=
Â· âˆ’11
6
Â¸
,
so x1 = âˆ’11eâˆ’t and y1 = 6eâˆ’2t. Hence x = âˆ’11eâˆ’t + 3(6eâˆ’2t) = âˆ’11eâˆ’t +
18eâˆ’2t, y = âˆ’11eâˆ’t + 4(6eâˆ’2t) = âˆ’11eâˆ’t + 24eâˆ’2t.
For a more complicated example we solve a system of inhomogeneous
recurrence relations.
EXAMPLE 6.2.4 Solve the system of recurrence relations
xn+1
=
2xn âˆ’yn âˆ’1
yn+1
=
âˆ’xn + 2yn + 2,
given that x0 = 0 and y0 = âˆ’1.
Solution. The system can be written in matrix form as
Xn+1 = AXn + B,
where
A =
Â·
2
âˆ’1
âˆ’1
2
Â¸
and B =
Â· âˆ’1
2
Â¸
.
It is then an easy induction to prove that
Xn = AnX0 + (Anâˆ’1 + Â· Â· Â· + A + I2)B.
(6.5)

6.2. DEFINITIONS AND EXAMPLES
123
Also it is easy to verify by the eigenvalue method that
An = 1
2
Â· 1 + 3n
1 âˆ’3n
1 âˆ’3n
1 + 3n
Â¸
= 1
2U + 3n
2 V,
where U =
Â· 1
1
1
1
Â¸
and V =
Â·
1
âˆ’1
âˆ’1
1
Â¸
. Hence
Anâˆ’1 + Â· Â· Â· + A + I2
=
n
2 U + (3nâˆ’1 + Â· Â· Â· + 3 + 1)
2
V
=
n
2 U + (3nâˆ’1 âˆ’1)
4
V.
Then equation 6.5 gives
Xn =
Âµ1
2U + 3n
2 V
Â¶ Â·
0
âˆ’1
Â¸
+
Âµn
2 U + (3nâˆ’1 âˆ’1)
4
V
Â¶ Â· âˆ’1
2
Â¸
,
which simpliï¬es to
Â· xn
yn
Â¸
=
Â· (2n + 1 âˆ’3n)/4
(2n âˆ’5 + 3n)/4
Â¸
.
Hence xn = (2n âˆ’1 + 3n)/4 and yn = (2n âˆ’5 + 3n)/4.
REMARK 6.2.1 If (A âˆ’I2)âˆ’1 existed (that is, if det (A âˆ’I2) Ì¸= 0, or
equivalently, if 1 is not an eigenvalue of A), then we could have used the
formula
Anâˆ’1 + Â· Â· Â· + A + I2 = (An âˆ’I2)(A âˆ’I2)âˆ’1.
(6.6)
However the eigenvalues of A are 1 and 3 in the above problem, so formula 6.6
cannot be used there.
Our discussion of eigenvalues and eigenvectors has been limited to 2 Ã— 2
matrices. The discussion is more complicated for matrices of size greater
than two and is best left to a second course in linear algebra. Nevertheless
the following result is a useful generalization of theorem 6.2.1. The reader
is referred to [28, page 350] for a proof.
THEOREM 6.2.2 Let A be an n Ã— n matrix having distinct eigenvalues
Î»1, . . . , Î»n and corresponding eigenvectors X1, . . . , Xn. Let P be the matrix
whose columns are respectively X1, . . . , Xn. Then P is nonâ€“singular and
P âˆ’1AP =
ï£®
ï£¯ï£¯ï£¯ï£°
Î»1
0
Â· Â· Â·
0
0
Î»2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
Î»n
ï£¹
ï£ºï£ºï£ºï£».

124
CHAPTER 6. EIGENVALUES AND EIGENVECTORS
Another useful result which covers the case where there are multiple eigen-
values is the following (The reader is referred to [28, pages 351â€“352] for a
proof):
THEOREM 6.2.3 Suppose the characteristic polynomial of A has the fac-
torization
det (Î»In âˆ’A) = (Î» âˆ’c1)n1 Â· Â· Â· (Î» âˆ’ct)nt,
where c1, . . . , ct are the distinct eigenvalues of A.
Suppose that for i =
1, . . . , t, we have nullity (ciInâˆ’A) = ni. For each i, choose a basis Xi1, . . . , Xini
for the eigenspace N(ciIn âˆ’A). Then the matrix
P = [X11| Â· Â· Â· |X1n1| Â· Â· Â· |Xt1| Â· Â· Â· |Xtnt]
is nonâ€“singular and P âˆ’1AP is the following diagonal matrix
P âˆ’1AP =
ï£®
ï£¯ï£¯ï£¯ï£°
c1In1
0
Â· Â· Â·
0
0
c2In2
Â· Â· Â·
0
...
...
...
...
0
0
Â· Â· Â·
ctInt
ï£¹
ï£ºï£ºï£ºï£».
(The notation means that on the diagonal there are n1 elements c1, followed
by n2 elements c2,. . . , nt elements ct.)
6.3
PROBLEMS
1. Let A =
Â· 4
âˆ’3
1
0
Â¸
. Find a nonâ€“singular matrix P such that P âˆ’1AP =
diag (1, 3) and hence prove that
An = 3n âˆ’1
2
A + 3 âˆ’3n
2
I2.
2. If A =
Â· 0.6
0.8
0.4
0.2
Â¸
, prove that An tends to a limiting matrix
Â· 2/3
2/3
1/3
1/3
Â¸
as n â†’âˆ.

6.3. PROBLEMS
125
3. Solve the system of diï¬€erential equations
dx
dt
=
3x âˆ’2y
dy
dt
=
5x âˆ’4y,
given x = 13 and y = 22 when t = 0.
[Answer: x = 7et + 6eâˆ’2t, y = 7et + 15eâˆ’2t.]
4. Solve the system of recurrence relations
xn+1
=
3xn âˆ’yn
yn+1
=
âˆ’xn + 3yn,
given that x0 = 1 and y0 = 2.
[Answer: xn = 2nâˆ’1(3 âˆ’2n), yn = 2nâˆ’1(3 + 2n).]
5. Let A =
Â· a
b
c
d
Â¸
be a real or complex matrix with distinct eigenvalues
Î»1, Î»2 and corresponding eigenvectors X1, X2. Also let P = [X1|X2].
(a) Prove that the system of recurrence relations
xn+1
=
axn + byn
yn+1
=
cxn + dyn
has the solution
Â· xn
yn
Â¸
= Î±Î»n
1X1 + Î²Î»n
2X2,
where Î± and Î² are determined by the equation
Â· Î±
Î²
Â¸
= P âˆ’1
Â· x0
y0
Â¸
.
(b) Prove that the system of diï¬€erential equations
dx
dt
=
ax + by
dy
dt
=
cx + dy
has the solution
Â· x
y
Â¸
= Î±eÎ»1tX1 + Î²eÎ»2tX2,

126
CHAPTER 6. EIGENVALUES AND EIGENVECTORS
where Î± and Î² are determined by the equation
Â· Î±
Î²
Â¸
= P âˆ’1
Â· x(0)
y(0)
Â¸
.
6. Let A =
Â· a11
a12
a21
a22
Â¸
be a real matrix with nonâ€“real eigenvalues Î» =
a + ib and Î» = a âˆ’ib, with corresponding eigenvectors X = U + iV
and X = U âˆ’iV , where U and V are real vectors. Also let P be the
real matrix deï¬ned by P = [U|V ]. Finally let a + ib = reiÎ¸, where
r > 0 and Î¸ is real.
(a) Prove that
AU
=
aU âˆ’bV
AV
=
bU + aV.
(b) Deduce that
P âˆ’1AP =
Â·
a
b
âˆ’b
a
Â¸
.
(c) Prove that the system of recurrence relations
xn+1
=
a11xn + a12yn
yn+1
=
a21xn + a22yn
has the solution
Â· xn
yn
Â¸
= rn{(Î±U + Î²V ) cos nÎ¸ + (Î²U âˆ’Î±V ) sin nÎ¸},
where Î± and Î² are determined by the equation
Â· Î±
Î²
Â¸
= P âˆ’1
Â· x0
y0
Â¸
.
(d) Prove that the system of diï¬€erential equations
dx
dt
=
ax + by
dy
dt
=
cx + dy

6.3. PROBLEMS
127
has the solution
Â· x
y
Â¸
= eat{(Î±U + Î²V ) cos bt + (Î²U âˆ’Î±V ) sin bt},
where Î± and Î² are determined by the equation
Â· Î±
Î²
Â¸
= P âˆ’1
Â· x(0)
y(0)
Â¸
.
[Hint: Let
Â· x
y
Â¸
= P
Â· x1
y1
Â¸
. Also let z = x1 + iy1. Prove that
Ë™z = (a âˆ’ib)z
and deduce that
x1 + iy1 = eat(Î± + iÎ²)(cos bt + i sin bt).
Then equate real and imaginary parts to solve for x1, y1 and
hence x, y.]
7. (The case of repeated eigenvalues.) Let A =
Â· a
b
c
d
Â¸
and suppose
that the characteristic polynomial of A, Î»2 âˆ’(a + d)Î» + (ad âˆ’bc), has
a repeated root Î±. Also assume that A Ì¸= Î±I2. Let B = A âˆ’Î±I2.
(i) Prove that (a âˆ’d)2 + 4bc = 0.
(ii) Prove that B2 = 0.
(iii) Prove that BX2 Ì¸= 0 for some vector X2; indeed, show that X2
can be taken to be
Â· 1
0
Â¸
or
Â· 0
1
Â¸
.
(iv) Let X1 = BX2. Prove that P = [X1|X2] is nonâ€“singular,
AX1 = Î±X1 and AX2 = Î±X2 + X1
and deduce that
P âˆ’1AP =
Â· Î±
1
0
Î±
Â¸
.
8. Use the previous result to solve system of the diï¬€erential equations
dx
dt
=
4x âˆ’y
dy
dt
=
4x + 8y,

128
CHAPTER 6. EIGENVALUES AND EIGENVECTORS
given that x = 1 = y when t = 0.
[To solve the diï¬€erential equation
dx
dt âˆ’kx = f(t),
k a constant,
multiply throughout by eâˆ’kt, thereby converting the leftâ€“hand side to
dx
dt (eâˆ’ktx).]
[Answer: x = (1 âˆ’3t)e6t, y = (1 + 6t)e6t.]
9. Let
A =
ï£®
ï£°
1/2
1/2
0
1/4
1/4
1/2
1/4
1/4
1/2
ï£¹
ï£».
(a) Verify that det (Î»I3 âˆ’A), the characteristic polynomial of A, is
given by
(Î» âˆ’1)Î»(Î» âˆ’1
4).
(b) Find a nonâ€“singular matrix P such that P âˆ’1AP = diag (1, 0, 1
4).
(c) Prove that
An = 1
3
ï£®
ï£°
1
1
1
1
1
1
1
1
1
ï£¹
ï£»+
1
3 Â· 4n
ï£®
ï£°
2
2
âˆ’4
âˆ’1
âˆ’1
2
âˆ’1
âˆ’1
2
ï£¹
ï£»
if n â‰¥1.
10. Let
A =
ï£®
ï£°
5
2
âˆ’2
2
5
âˆ’2
âˆ’2
âˆ’2
5
ï£¹
ï£».
(a) Verify that det (Î»I3 âˆ’A), the characteristic polynomial of A, is
given by
(Î» âˆ’3)2(Î» âˆ’9).
(b) Find a nonâ€“singular matrix P such that P âˆ’1AP = diag (3, 3, 9).

Chapter 7
Identifying second degree
equations
7.1
The eigenvalue method
In this section we apply eigenvalue methods to determine the geometrical
nature of the second degree equation
ax2 + 2hxy + by2 + 2gx + 2fy + c = 0,
(7.1)
where not all of a, h, b are zero.
Let A =
Â· a
h
h
b
Â¸
be the matrix of the quadratic form ax2 +2hxy +by2.
We saw in section 6.1, equation 6.2 that A has real eigenvalues Î»1 and Î»2,
given by
Î»1 = a + b âˆ’
p
(a âˆ’b)2 + 4h2
2
, Î»2 = a + b +
p
(a âˆ’b)2 + 4h2
2
.
We show that it is always possible to rotate the x, y axes to x1, x2 axes whose
positive directions are determined by eigenvectors X1 and X2 corresponding
to Î»1and Î»2 in such a way that relative to the x1, y1 axes, equation 7.1 takes
the form
aâ€²x2 + bâ€²y2 + 2gâ€²x + 2f â€²y + c = 0.
(7.2)
Then by completing the square and suitably translating the x1, y1 axes,
to new x2, y2 axes, equation 7.2 can be reduced to one of several standard
forms, each of which is easy to sketch. We need some preliminary deï¬nitions.
129

130
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
DEFINITION 7.1.1 (Orthogonal matrix) An n Ã— n real matrix P is
called orthogonal if
P tP = In.
It follows that if P is orthogonal, then det P = Â±1. For
det (P tP) = det P t det P = ( det P)2,
so (det P)2 = det In = 1. Hence det P = Â±1.
If P is an orthogonal matrix with det P = 1, then P is called a proper
orthogonal matrix.
THEOREM 7.1.1 If P is a 2 Ã— 2 orthogonal matrix with det P = 1, then
P =
Â· cos Î¸
âˆ’sin Î¸
sin Î¸
cos Î¸
Â¸
for some Î¸.
REMARK 7.1.1 Hence, by the discusssion at the beginning of Chapter
6, if P is a proper orthogonal matrix, the coordinate transformation
Â· x
y
Â¸
= P
Â· x1
y1
Â¸
represents a rotation of the axes, with new x1 and y1 axes given by the
repective columns of P.
Proof. Suppose that P tP = I2, where âˆ†= det P = 1. Let
P =
Â· a
b
c
d
Â¸
.
Then the equation
P t = P âˆ’1 = 1
âˆ†adj P
gives
Â· a
c
b
d
Â¸
=
Â·
d
âˆ’b
âˆ’c
a
Â¸
Hence a = d, b = âˆ’c and so
P =
Â· a
âˆ’c
c
a
Â¸
,
where a2 + c2 = 1.
But then the point (a, c) lies on the unit circle, so
a = cos Î¸ and c = sin Î¸, where Î¸ is uniquely determined up to multiples of
2Ï€.

7.1. THE EIGENVALUE METHOD
131
DEFINITION 7.1.2 (Dot product). If X =
Â· a
b
Â¸
and Y =
Â· c
d
Â¸
, then
X Â· Y , the dot product of X and Y , is deï¬ned by
X Â· Y = ac + bd.
The dot product has the following properties:
(i) X Â· (Y + Z) = X Â· Y + X Â· Z;
(ii) X Â· Y = Y Â· X;
(iii) (tX) Â· Y = t(X Â· Y );
(iv) X Â· X = a2 + b2 if X =
Â· a
b
Â¸
;
(v) X Â· Y = XtY .
The length of X is deï¬ned by
||X|| =
p
a2 + b2 = (X Â· X)1/2.
We see that ||X|| is the distance between the origin O = (0, 0) and the point
(a, b).
THEOREM 7.1.2 (Geometrical interpretation of the dot product)
Let A = (x1, y1) and B = (x2, y2) be points, each distinct from the origin
O = (0, 0). Then if X =
Â· x1
y1
Â¸
and Y =
Â· x2
y2
Â¸
, we have
X Â· Y = OA Â· OB cos Î¸,
where Î¸ is the angle between the rays OA and OB.
Proof. By the cosine law applied to triangle OAB, we have
AB2 = OA2 + OB2 âˆ’2OA Â· OB cos Î¸.
(7.3)
Now AB2 = (x2 âˆ’x1)2 + (y2 âˆ’y1)2, OA2 = x2
1 + y2
1, OB2 = x2
2 + y2
2.
Substituting in equation 7.3 then gives
(x2 âˆ’x1)2 + (y2 âˆ’y1)2 = (x2
1 + y2
1) + (x2
2 + y2
2) âˆ’2OA Â· OB cos Î¸,

132
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
which simpliï¬es to give
OA Â· OB cos Î¸ = x1x2 + y1y2 = X Â· Y.
It follows from theorem 7.1.2 that if A = (x1, y1) and B = (x2, y2) are
points distinct from O = (0, 0) and X =
Â· x1
y1
Â¸
and Y =
Â· x2
y2
Â¸
, then
X Â· Y = 0 means that the rays OA and OB are perpendicular. This is the
reason for the following deï¬nition:
DEFINITION 7.1.3 (Orthogonal vectors) Vectors X and Y are called
orthogonal if
X Â· Y = 0.
There is also a connection with orthogonal matrices:
THEOREM 7.1.3 Let P be a 2 Ã— 2 real matrix. Then P is an orthogonal
matrix if and only if the columns of P are orthogonal and have unit length.
Proof. P is orthogonal if and only if P tP = I2. Now if P = [X1|X2], the
matrix P tP is an important matrix called the Gram matrix of the column
vectors X1 and X2. It is easy to prove that
P tP = [Xi Â· Xj] =
Â· X1 Â· X1
X1 Â· X2
X2 Â· X1
X2 Â· X2
Â¸
.
Hence the equation P tP = I2 is equivalent to
Â· X1 Â· X1
X1 Â· X2
X2 Â· X1
X2 Â· X2
Â¸
=
Â· 1
0
0
1
Â¸
,
or, equating corresponding elements of both sides:
X1 Â· X1 = 1, X1 Â· X2 = 0, X2 Â· X2 = 1,
which says that the columns of P are orthogonal and of unit length.
The next theorem describes a fundamental property of real symmetric
matrices and the proof generalizes to symmetric matrices of any size.
THEOREM 7.1.4 If X1 and X2 are eigenvectors corresponding to distinct
eigenvalues Î»1 and Î»2 of a real symmetric matrix A, then X1 and X2 are
orthogonal vectors.

7.1. THE EIGENVALUE METHOD
133
Proof. Suppose
AX1 = Î»1X1, AX2 = Î»2X2,
(7.4)
where X1 and X2 are nonâ€“zero column vectors, At = A and Î»1 Ì¸= Î»2.
We have to prove that Xt
1X2 = 0. From equation 7.4,
Xt
2AX1 = Î»1Xt
2X1
(7.5)
and
Xt
1AX2 = Î»2Xt
1X2.
(7.6)
From equation 7.5, taking transposes,
(Xt
2AX1)t = (Î»1Xt
2X1)t
so
Xt
1AtX2 = Î»1Xt
1X2.
Hence
Xt
1AX2 = Î»1Xt
1X2.
(7.7)
Finally, subtracting equation 7.6 from equation 7.7, we have
(Î»1 âˆ’Î»2)Xt
1X2 = 0
and hence, since Î»1 Ì¸= Î»2,
Xt
1X2 = 0.
THEOREM 7.1.5 Let A be a real 2 Ã— 2 symmetric matrix with distinct
eigenvalues Î»1 and Î»2. Then a proper orthogonal 2Ã—2 matrix P exists such
that
P tAP = diag (Î»1, Î»2).
Also the rotation of axes
Â· x
y
Â¸
= P
Â· x1
y1
Â¸
â€œdiagonalizesâ€ the quadratic form corresponding to A:
XtAX = Î»1x2
1 + Î»2y2
1.

134
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
Proof. Let X1 and X2 be eigenvectors corresponding to Î»1 and Î»2. Then
by theorem 7.1.4, X1 and X2 are orthogonal. By dividing X1 and X2 by
their lengths (i.e. normalizing X1 and X2) if necessary, we can assume that
X1 and X2 have unit length. Then by theorem 7.1.1, P = [X1|X2] is an
orthogonal matrix. By replacing X1 by âˆ’X1, if necessary, we can assume
that det P = 1. Then by theorem 6.2.1, we have
P tAP = P âˆ’1AP =
Â· Î»1
0
0
Î»2
Â¸
.
Also under the rotation X = PY ,
XtAX
=
(PY )tA(PY ) = Y t(P tAP)Y = Y t diag (Î»1, Î»2)Y
=
Î»1x2
1 + Î»2y2
1.
EXAMPLE 7.1.1 Let A be the symmetric matrix
A =
Â· 12
âˆ’6
âˆ’6
7
Â¸
.
Find a proper orthogonal matrix P such that P tAP is diagonal.
Solution. The characteristic equation of A is Î»2 âˆ’19Î» + 48 = 0, or
(Î» âˆ’16)(Î» âˆ’3) = 0.
Hence A has distinct eigenvalues Î»1 = 16 and Î»2 = 3. We ï¬nd corresponding
eigenvectors
X1 =
Â· âˆ’3
2
Â¸
and X2 =
Â· 2
3
Â¸
.
Now ||X1|| = ||X2|| =
âˆš
13. So we take
X1 =
1
âˆš
13
Â· âˆ’3
2
Â¸
and X2 =
1
âˆš
13
Â· 2
3
Â¸
.
Then if P = [X1|X2], the proof of theorem 7.1.5 shows that
P tAP =
Â· 16
0
0
3
Â¸
.
However det P = âˆ’1, so replacing X1 by âˆ’X1 will give det P = 1.

7.1. THE EIGENVALUE METHOD
135
y2
x2
2
4
-2
-4
2
4
-2
-4
x
y
Figure 7.1: 12x2 âˆ’12xy + 7y2 + 60x âˆ’38y + 31 = 0.
REMARK 7.1.2 (A shortcut) Once we have determined one eigenvec-
tor X1 =
Â· a
b
Â¸
, the other can be taken to be
Â· âˆ’b
a
Â¸
, as these vectors are
always orthogonal. Also P = [X1|X2] will have det P = a2 + b2 > 0.
We now apply the above ideas to determine the geometric nature of
second degree equations in x and y.
EXAMPLE 7.1.2 Sketch the curve determined by the equation
12x2 âˆ’12xy + 7y2 + 60x âˆ’38y + 31 = 0.
Solution. With P taken to be the proper orthogonal matrix deï¬ned in the
previous example by
P =
Â·
3/
âˆš
13
2/
âˆš
13
âˆ’2/
âˆš
13
3/
âˆš
13
Â¸
,
then as theorem 7.1.1 predicts, P is a rotation matrix and the transformation
X =
Â· x
y
Â¸
= PY = P
Â· x1
y1
Â¸

136
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
or more explicitly
x = 3x1 + 2y1
âˆš
13
, y = âˆ’2x1 + 3y1
âˆš
13
,
(7.8)
will rotate the x, y axes to positions given by the respective columns of P.
(More generally, we can always arrange for the x1 axis to point either into
the ï¬rst or fourth quadrant.)
Now A =
Â· 12
âˆ’6
âˆ’6
7
Â¸
is the matrix of the quadratic form
12x2 âˆ’12xy + 7y2,
so we have, by Theorem 7.1.5
12x2 âˆ’12xy + 7y2 = 16x2
1 + 3y2
1.
Then under the rotation X = PY , our original quadratic equation becomes
16x2
1 + 3y2
1 + 60
âˆš
13(3x1 + 2y1) âˆ’38
âˆš
13(âˆ’2x1 + 3y1) + 31 = 0,
or
16x2
1 + 3y2
1 + 256
âˆš
13x1 +
6
âˆš
13y1 + 31 = 0.
Now complete the square in x1 and y1:
16
Âµ
x2
1 + 16
âˆš
13x1
Â¶
+ 3
Âµ
y2
1 +
2
âˆš
13y1
Â¶
+ 31 = 0,
16
Âµ
x1 +
8
âˆš
13
Â¶2
+ 3
Âµ
y1 +
1
âˆš
13
Â¶2
=
16
Âµ 8
âˆš
13
Â¶2
+ 3
Âµ 1
âˆš
13
Â¶2
âˆ’31
=
48.
(7.9)
Then if we perform a translation of axes to the new origin (x1, y1) =
(âˆ’
8
âˆš
13, âˆ’
1
âˆš
13):
x2 = x1 +
8
âˆš
13, y2 = y1 +
1
âˆš
13,
equation 7.9 reduces to
16x2
2 + 3y2
2 = 48,
or
x2
2
3 + y2
2
16 = 1.

7.1. THE EIGENVALUE METHOD
137
x
y
Figure 7.2: x2
a2 + y2
b2 = 1, 0 < b < a: an ellipse.
This equation is now in one of the standard forms listed below as Figure 7.2
and is that of a whose centre is at (x2, y2) = (0, 0) and whose axes of
symmetry lie along the x2, y2 axes. In terms of the original x, y coordinates,
we ï¬nd that the centre is (x, y) = (âˆ’2, 1). Also Y = P tX, so equations 7.8
can be solved to give
x1 = 3x1 âˆ’2y1
âˆš
13
, y1 = 2x1 + 3y1
âˆš
13
.
Hence the y2â€“axis is given by
0 = x2
=
x1 +
8
âˆš
13
=
3x âˆ’2y
âˆš
13
+
8
âˆš
13,
or 3x âˆ’2y + 8 = 0. Similarly the x2 axis is given by 2x + 3y + 1 = 0.
This ellipse is sketched in Figure 7.1.
Figures 7.2, 7.3, 7.4 and 7.5 are a collection of standard second degree
equations: Figure 7.2 is an ellipse; Figures 7.3 are hyperbolas (in both these
examples, the asymptotes are the lines y = Â± b
ax); Figures 7.4 and 7.5
represent parabolas.
EXAMPLE 7.1.3 Sketch y2 âˆ’4x âˆ’10y âˆ’7 = 0.

138
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
x
y
x
y
Figure 7.3: (i) x2
a2 âˆ’y2
b2 = 1;
(ii) x2
a2 âˆ’y2
b2 = âˆ’1, 0 < b, 0 < a.
x
y
x
y
Figure 7.4: (i) y2 = 4ax, a > 0;
(ii) y2 = 4ax, a < 0.

7.1. THE EIGENVALUE METHOD
139
x
y
x
y
Figure 7.5: (iii) x2 = 4ay, a > 0;
(iv) x2 = 4ay, a < 0.
Solution. Complete the square:
y2 âˆ’10y + 25 âˆ’4x âˆ’32
=
0
(y âˆ’5)2 = 4x + 32
=
4(x + 8),
or y2
1 = 4x1, under the translation of axes x1 = x + 8, y1 = y âˆ’5. Hence we
get a parabola with vertex at the new origin (x1, y1) = (0, 0), i.e. (x, y) =
(âˆ’8, 5).
The parabola is sketched in Figure 7.6.
EXAMPLE 7.1.4 Sketch the curve x2 âˆ’4xy + 4y2 + 5y âˆ’9 = 0.
Solution. We have x2 âˆ’4xy + 4y2 = XtAX, where
A =
Â·
1
âˆ’2
âˆ’2
4
Â¸
.
The characteristic equation of A is Î»2âˆ’5Î» = 0, so A has distinct eigenvalues
Î»1 = 5 and Î»2 = 0. We ï¬nd corresponding unit length eigenvectors
X1 =
1
âˆš
5
Â·
1
âˆ’2
Â¸
, X2 =
1
âˆš
5
Â· 2
1
Â¸
.
Then P = [X1|X2] is a proper orthogonal matrix and under the rotation of
axes X = PY , or
x
=
x1 + 2y1
âˆš
5
y
=
âˆ’2x1 + y1
âˆš
5
,

140
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
x1
y1
4
8
12
-4
-8
4
8
12
-4
-8
x
y
Figure 7.6: y2 âˆ’4x âˆ’10y âˆ’7 = 0.
we have
x2 âˆ’4xy + 4y2 = Î»1x2
1 + Î»2y2
1 = 5x2
1.
The original quadratic equation becomes
5x2
1 +
âˆš
5
âˆš
5(âˆ’2x1 + y1) âˆ’9
=
0
5(x2
1 âˆ’2
âˆš
5x1) +
âˆš
5y1 âˆ’9
=
0
5(x1 âˆ’1
âˆš
5)2 = 10 âˆ’
âˆš
5y1
=
âˆš
5(y1 âˆ’2
âˆš
5),
or 5x2
2 = âˆ’1
âˆš
5y2, where the x1, y1 axes have been translated to x2, y2 axes
using the transformation
x2 = x1 âˆ’1
âˆš
5,
y2 = y1 âˆ’2
âˆš
5.
Hence the vertex of the parabola is at (x2, y2) = (0, 0), i.e.
(x1, y1) =
( 1
âˆš
5, 2
âˆš
5), or (x, y) = ( 21
5 , 8
5). The axis of symmetry of the parabola is the
line x2 = 0, i.e. x1 = 1/
âˆš
5. Using the rotation equations in the form
x1
=
x âˆ’2y
âˆš
5

7.2. A CLASSIFICATION ALGORITHM
141
x2
y2
2
4
-2
-4
2
4
-2
-4
x
y
Figure 7.7: x2 âˆ’4xy + 4y2 + 5y âˆ’9 = 0.
y1
=
2x + y
âˆš
5
,
we have
x âˆ’2y
âˆš
5
=
1
âˆš
5,
or
x âˆ’2y = 1.
The parabola is sketched in Figure 7.7.
7.2
A classiï¬cation algorithm
There are several possible degenerate cases that can arise from the general
second degree equation. For example x2 +y2 = 0 represents the point (0, 0);
x2 + y2 = âˆ’1 deï¬nes the empty set, as does x2 = âˆ’1 or y2 = âˆ’1; x2 = 0
deï¬nes the line x = 0; (x + y)2 = 0 deï¬nes the line x + y = 0; x2 âˆ’y2 = 0
deï¬nes the lines x âˆ’y = 0, x + y = 0; x2 = 1 deï¬nes the parallel lines
x = Â±1; (x + y)2 = 1 likewise deï¬nes two parallel lines x + y = Â±1.
We state without proof a complete classiï¬cation 1 of the various cases
1This classiï¬cation forms the basis of a computer program which was used to produce
the diagrams in this chapter. I am grateful to Peter Adams for his programming assistance.

142
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
that can possibly arise for the general second degree equation
ax2 + 2hxy + by2 + 2gx + 2fy + c = 0.
(7.10)
It turns out to be more convenient to ï¬rst perform a suitable translation of
axes, before rotating the axes. Let
âˆ†=
Â¯Â¯Â¯Â¯Â¯Â¯
a
h
g
h
b
f
g
f
c
Â¯Â¯Â¯Â¯Â¯Â¯
,
C = ab âˆ’h2, A = bc âˆ’f 2, B = ca âˆ’g2.
If C Ì¸= 0, let
Î± =
âˆ’
Â¯Â¯Â¯Â¯
g
h
f
b
Â¯Â¯Â¯Â¯
C
,
Î² =
âˆ’
Â¯Â¯Â¯Â¯
a
g
h
f
Â¯Â¯Â¯Â¯
C
.
(7.11)
CASE 1. âˆ†= 0.
(1.1) C Ì¸= 0. Translate axes to the new origin (Î±, Î²), where Î± and Î² are
given by equations 7.11:
x = x1 + Î±,
y = y1 + Î².
Then equation 7.10 reduces to
ax2
1 + 2hx1y1 + by2
1 = 0.
(a) C > 0: Single point (x, y) = (Î±, Î²).
(b) C < 0: Two nonâ€“parallel lines intersecting in (x, y) = (Î±, Î²).
The lines are
y âˆ’Î²
x âˆ’Î±
=
âˆ’h Â±
âˆš
âˆ’C
b
if b Ì¸= 0,
x = Î±
and
y âˆ’Î²
x âˆ’Î± = âˆ’a
2h,
if b = 0.
(1.2) C = 0.
(a) h = 0.
(i) a = g = 0.
(A) A > 0: Empty set.
(B) A = 0: Single line y = âˆ’f/b.

7.2. A CLASSIFICATION ALGORITHM
143
(C) A < 0: Two parallel lines
y = âˆ’f Â±
âˆš
âˆ’A
b
(ii) b = f = 0.
(A) B > 0: Empty set.
(B) B = 0: Single line x = âˆ’g/a.
(C) B < 0: Two parallel lines
x = âˆ’g Â±
âˆš
âˆ’B
a
(b) h Ì¸= 0.
(i) B > 0: Empty set.
(ii) B = 0: Single line ax + hy = âˆ’g.
(iii) B < 0: Two parallel lines
ax + hy = âˆ’g Â±
âˆš
âˆ’B.
CASE 2. âˆ†Ì¸= 0.
(2.1) C Ì¸= 0. Translate axes to the new origin (Î±, Î²), where Î± and Î² are
given by equations 7.11:
x = x1 + Î±,
y = y1 + Î².
Equation 7.10 becomes
ax2
1 + 2hx1y1 + by2
1 = âˆ’âˆ†
C .
(7.12)
CASE 2.1(i) h = 0. Equation 7.12 becomes ax2
1 + by2
1 = âˆ’âˆ†
C .
(a) C < 0: Hyperbola.
(b) C > 0 and aâˆ†> 0: Empty set.
(c) C > 0 and aâˆ†< 0.
(i) a = b: Circle, centre (Î±, Î²), radius
q
g2+f2âˆ’ac
a
.
(ii) a Ì¸= b: Ellipse.

144
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
CASE 2.1(ii) h Ì¸= 0.
Rotate the (x1, y1) axes with the new positive x2â€“axis in the direction
of
[(b âˆ’a + R)/2, âˆ’h],
where R =
p
(a âˆ’b)2 + 4h2.
Then equation 7.12 becomes
Î»1x2
2 + Î»2y2
2 = âˆ’âˆ†
C .
(7.13)
where
Î»1 = (a + b âˆ’R)/2, Î»2 = (a + b + R)/2,
Here Î»1Î»2 = C.
(a) C < 0: Hyperbola.
Here Î»2 > 0 > Î»1 and equation 7.13 becomes
x2
2
u2 âˆ’y2
2
v2 = âˆ’âˆ†
|âˆ†| ,
where
u =
s
|âˆ†|
CÎ»1
, v =
s
|âˆ†|
âˆ’CÎ»2
.
(b) C > 0 and aâˆ†> 0: Empty set.
(c) C > 0 and aâˆ†< 0: Ellipse.
Here Î»1, Î»2, a, b have the same sign and Î»1 Ì¸= Î»2 and equa-
tion 7.13 becomes
x2
2
u2 + y2
2
v2 = 1,
where
u =
r
âˆ†
âˆ’CÎ»1
, v =
r
âˆ†
âˆ’CÎ»2
.
(2.1) C = 0.
(a) h = 0.
(i) a = 0: Then b Ì¸= 0 and g Ì¸= 0. Parabola with vertex
Âµâˆ’A
2gb , âˆ’f
b
Â¶
.

7.2. A CLASSIFICATION ALGORITHM
145
Translate axes to (x1, y1) axes:
y2
1 = âˆ’2g
b x1.
(ii) b = 0: Then a Ì¸= 0 and f Ì¸= 0. Parabola with vertex
Âµ
âˆ’g
a, âˆ’B
2fa
Â¶
.
Translate axes to (x1, y1) axes:
x2
1 = âˆ’2f
a y1.
(b) h Ì¸= 0: Parabola. Let
k = ga + bf
a + b .
The vertex of the parabola is
Âµ(2akf âˆ’hk2 âˆ’hac)
d
, a(k2 + ac âˆ’2kg)
d
Â¶
.
Now translate to the vertex as the new origin, then rotate to
(x2, y2) axes with the positive x2â€“axis along [sa, âˆ’sh], where
s = sign (a).
(The positive x2â€“axis points into the ï¬rst or fourth quadrant.)
Then the parabola has equation
x2
2 =
âˆ’2st
âˆš
a2 + h2 y2,
where t = (af âˆ’gh)/(a + b).
REMARK 7.2.1 If âˆ†= 0, it is not necessary to rotate the axes. Instead
it is always possible to translate the axes suitably so that the coeï¬ƒcients of
the terms of the ï¬rst degree vanish.
EXAMPLE 7.2.1 Identify the curve
2x2 + xy âˆ’y2 + 6y âˆ’8 = 0.
(7.14)

146
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
Solution. Here
âˆ†=
Â¯Â¯Â¯Â¯Â¯Â¯
2
1
2
0
1
2
âˆ’1
3
0
3
âˆ’8
Â¯Â¯Â¯Â¯Â¯Â¯
= 0.
Let x = x1 + Î±, y = y1 + Î² and substitute in equation 7.14 to get
2(x1 + Î±)2 + (x1 + Î±)(y1 + Î²) âˆ’(y1 + Î²)2 + 4(y1 + Î²) âˆ’8 = 0.
(7.15)
Then equating the coeï¬ƒcients of x1 and y1 to 0 gives
4Î± + Î²
=
0
Î± + 2Î² + 4
=
0,
which has the unique solution Î± = âˆ’2
3, Î² = 8
3. Then equation 7.15 simpliï¬es
to
2x2
1 + x1y1 âˆ’y2
1 = 0 = (2x1 âˆ’y1)(x1 + y1),
so relative to the x1, y1 coordinates, equation 7.14 describes two lines: 2x1âˆ’
y1 = 0 or x1 + y1 = 0. In terms of the original x, y coordinates, these lines
become 2(x + 2
3) âˆ’(y âˆ’8
3) = 0 and (x + 2
3) + (y âˆ’8
3) = 0, i.e. 2x âˆ’y + 4 = 0
and x + y âˆ’2 = 0, which intersect in the point
(x, y) = (Î±, Î²) = (âˆ’2
3, 8
3).
EXAMPLE 7.2.2 Identify the curve
x2 + 2xy + y2 + +2x + 2y + 1 = 0.
(7.16)
Solution. Here
âˆ†=
Â¯Â¯Â¯Â¯Â¯Â¯
1
1
1
1
1
1
1
1
1
Â¯Â¯Â¯Â¯Â¯Â¯
= 0.
Let x = x1 + Î±, y = y1 + Î² and substitute in equation 7.16 to get
(x1+Î±)2+2(x1+Î±)(y1+Î²)+(y1+Î²)2+2(x1+Î±)+2(y1+Î²)+1 = 0. (7.17)
Then equating the coeï¬ƒcients of x1 and y1 to 0 gives the same equation
2Î± + 2Î² + 2 = 0.
Take Î± = 0, Î² = âˆ’1. Then equation 7.17 simpliï¬es to
x2
1 + 2x1y1 + y2
1 = 0 = (x1 + y1)2,
and in terms of x, y coordinates, equation 7.16 becomes
(x + y + 1)2 = 0, or x + y + 1 = 0.

7.3. PROBLEMS
147
7.3
PROBLEMS
1. Sketch the curves
(i) x2 âˆ’8x + 8y + 8 = 0;
(ii) y2 âˆ’12x + 2y + 25 = 0.
2. Sketch the hyperbola
4xy âˆ’3y2 = 8
and ï¬nd the equations of the asymptotes.
[Answer: y = 0 and y = 4
3x.]
3. Sketch the ellipse
8x2 âˆ’4xy + 5y2 = 36
and ï¬nd the equations of the axes of symmetry.
[Answer: y = 2x and x = âˆ’2y.]
4. Sketch the conics deï¬ned by the following equations. Find the centre
when the conic is an ellipse or hyperbola, asymptotes if an hyperbola,
the vertex and axis of symmetry if a parabola:
(i) 4x2 âˆ’9y2 âˆ’24x âˆ’36y âˆ’36 = 0;
(ii) 5x2 âˆ’4xy + 8y2 + 4
âˆš
5x âˆ’16
âˆš
5y + 4 = 0;
(iii) 4x2 + y2 âˆ’4xy âˆ’10y âˆ’19 = 0;
(iv) 77x2 + 78xy âˆ’27y2 + 70x âˆ’30y + 29 = 0.
[Answers: (i) hyperbola, centre (3, âˆ’2), asymptotes 2x âˆ’3y âˆ’12 =
0, 2x + 3y = 0;
(ii) ellipse, centre (0,
âˆš
5);
(iii) parabola, vertex (âˆ’7
5, âˆ’9
5), axis of symmetry 2x âˆ’y + 1 = 0;
(iv) hyperbola, centre (âˆ’1
10,
7
10), asymptotes 7x + 9y + 7 = 0 and
11x âˆ’3y âˆ’1 = 0.]
5. Identify the lines determined by the equations:
(i) 2x2 + y2 + 3xy âˆ’5x âˆ’4y + 3 = 0;

148
CHAPTER 7. IDENTIFYING SECOND DEGREE EQUATIONS
(ii) 9x2 + y2 âˆ’6xy + 6x âˆ’2y + 1 = 0;
(iii) x2 + 4xy + 4y2 âˆ’x âˆ’2y âˆ’2 = 0.
[Answers: (i) 2x + y âˆ’3 = 0 and x + y âˆ’1 = 0; (ii) 3x âˆ’y + 1 = 0;
(iii) x + 2y + 1 = 0 and x + 2y âˆ’2 = 0.]

Chapter 8
THREEâ€“DIMENSIONAL
GEOMETRY
8.1
Introduction
In this chapter we present a vectorâ€“algebra approach to threeâ€“dimensional
geometry. The aim is to present standard properties of lines and planes,
with minimum use of complicated threeâ€“dimensional diagrams such as those
involving similar triangles. We summarize the chapter:
Points are deï¬ned as ordered triples of real numbers and the distance
between points P1 = (x1, y1, z1) and P2 = (x2, y2, z2) is deï¬ned by the
formula
P1P2 =
p
(x2 âˆ’x1)2 + (y2 âˆ’y1)2 + (z2 âˆ’z1)2.
Directed line segments
-
AB are introduced as threeâ€“dimensional column
vectors: If A = (x1, y1, z1) and B = (x2, y2, z2), then
-
AB=
ï£®
ï£°
x2 âˆ’x1
y2 âˆ’y1
z2 âˆ’z1
ï£¹
ï£».
If P is a point, we let P =
-
OP and call P the position vector of P.
With suitable deï¬nitions of lines, parallel lines, there are important ge-
ometrical interpretations of equality, addition and scalar multiplication of
vectors.
(i) Equality of vectors: Suppose A, B, C, D are distinct points such that
no three are collinear. Then
-
AB=
-
CD if and only if
-
AB âˆ¥
-
CD and
-
AC âˆ¥
-
BD (See Figure 8.1.)
149

150
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
Â¡
Â¡
Â¡
Â¡

-
A
Â¡
Â¡
Â¡
Â¡

C
QQQQQQ
D
QQQQQQ
B
-
AB=
-
CD,
-
AC=
-
BD
-
AB +
-
AC=
-
AD
Figure 8.1: Equality and addition of vectors.
(ii) Addition of vectors obeys the parallelogram law: Let A, B, C be nonâ€“
collinear. Then
-
AB +
-
AC=
-
AD,
where D is the point such that
-
AB âˆ¥
-
CD and
-
AC âˆ¥
-
BD. (See Fig-
ure 8.1.)
(iii) Scalar multiplication of vectors: Let
-
AP= t
-
AB, where A and B are
distinct points. Then P is on the line AB,
AP
AB = |t|
and
(a) P = A if t = 0, P = B if t = 1;
(b) P is between A and B if 0 < t < 1;
(c) B is between A and P if 1 < t;
(d) A is between P and B if t < 0.
(See Figure 8.2.)

8.1. INTRODUCTION
151
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
@
@
@
@
@
R
@
@
@
R P
A
B
-
AP= t
-
AB, 0 < t < 1
Figure 8.2: Scalar multiplication of vectors.
The dot product XÂ·Y of vectors X =
ï£®
ï£°
a1
b1
c1
ï£¹
ï£»and Y =
ï£®
ï£°
a2
b2
c2
ï£¹
ï£», is deï¬ned
by
X Â· Y = a1a2 + b1b2 + c1c2.
The length ||X|| of a vector X is deï¬ned by
||X|| = (X Â· X)1/2
and the Cauchyâ€“Schwarz inequality holds:
|X Â· Y | â‰¤||X|| Â· ||Y ||.
The triangle inequality for vector length now follows as a simple deduction:
||X + Y || â‰¤||X|| + ||Y ||.
Using the equation
AB = ||
-
AB ||,
we deduce the corresponding familiar triangle inequality for distance:
AB â‰¤AC + CB.

152
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
The angle Î¸ between two nonâ€“zero vectors X and Y is then deï¬ned by
cos Î¸ =
X Â· Y
||X|| Â· ||Y ||,
0 â‰¤Î¸ â‰¤Ï€.
This deï¬nition makes sense. For by the Cauchyâ€“Schwarz inequality,
âˆ’1 â‰¤
X Â· Y
||X|| Â· ||Y || â‰¤1.
Vectors X and Y are said to be perpendicular or orthogonal if X Â·Y = 0.
Vectors of unit length are called unit vectors. The vectors
i =
ï£®
ï£°
1
0
0
ï£¹
ï£»,
j =
ï£®
ï£°
0
1
0
ï£¹
ï£»,
k =
ï£®
ï£°
0
0
1
ï£¹
ï£»
are unit vectors and every vector is a linear combination of i, j and k:
ï£®
ï£°
a
b
c
ï£¹
ï£»= ai + bj + ck.
Nonâ€“zero vectors X and Y are parallel or proportional if the angle be-
tween X and Y equals 0 or Ï€; equivalently if X = tY for some real number
t. Vectors X and Y are then said to have the same or opposite direction,
according as t > 0 or t < 0.
We are then led to study straight lines. If A and B are distinct points,
it is easy to show that AP + PB = AB holds if and only if
-
AP = t
-
AB, where 0 â‰¤t â‰¤1.
A line is deï¬ned as a set consisting of all points P satisfying
P = P0 + tX,
t âˆˆR
or equivalently
-
P0P= tX,
for some ï¬xed point P0 and ï¬xed nonâ€“zero vector X called a direction vector
for the line.
Equivalently, in terms of coordinates,
x = x0 + ta, y = y0 + tb, z = z0 + tc,
where P0 = (x0, y0, z0) and not all of a, b, c are zero.

8.1. INTRODUCTION
153
There is then one and only one line passing passing through two distinct
points A and B. It consists of the points P satisfying
-
AP = t
-
AB,
where t is a real number.
The crossâ€“product XÃ—Y provides us with a vector which is perpendicular
to both X and Y . It is deï¬ned in terms of the components of X and Y :
Let X = a1i + b1j + c1k and Y = a2i + b2j + c2k. Then
X Ã— Y = ai + bj + ck,
where
a =
Â¯Â¯Â¯Â¯
b1
c1
b2
c2
Â¯Â¯Â¯Â¯ ,
b = âˆ’
Â¯Â¯Â¯Â¯
a1
c1
a2
c2
Â¯Â¯Â¯Â¯ ,
c =
Â¯Â¯Â¯Â¯
a1
b1
a2
b2
Â¯Â¯Â¯Â¯ .
The crossâ€“product enables us to derive elegant formulae for the distance
from a point to a line, the area of a triangle and the distance between two
skew lines.
Finally we turn to the geometrical concept of a plane in threeâ€“dimensional
space.
A plane is a set of points P satisfying an equation of the form
P = P0 + sX + tY, s, t âˆˆR,
(8.1)
where X and Y are nonâ€“zero, nonâ€“parallel vectors.
In terms of coordinates, equation 8.1 takes the form
x
=
x0 + sa1 + ta2
y
=
y0 + sb1 + tb2
z
=
z0 + sc1 + tc2,
where P0 = (x0, y0, z0).
There is then one and only one plane passing passing through three
nonâ€“collinear points A, B, C. It consists of the points P satisfying
-
AP = s
-
AB +t
-
AC,
where s and t are real numbers.
The crossâ€“product enables us to derive a concise equation for the plane
through three nonâ€“collinear points A, B, C, namely
-
AP Â·(
-
AB Ã—
-
AC) = 0.

154
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
When expanded, this equation has the form
ax + by + cz = d,
where ai + bj + ck is a nonâ€“zero vector which is perpendicular to
-
P1P2 for
all points P1, P2 lying in the plane. Any vector with this property is said to
be a normal to the plane.
It is then easy to prove that two planes with nonâ€“parallel normal vectors
must intersect in a line.
We conclude the chapter by deriving a formula for the distance from a
point to a plane.
8.2
Threeâ€“dimensional space
DEFINITION 8.2.1 Threeâ€“dimensional space is the set E3 of ordered
triples (x, y, z), where x, y, z are real numbers. The triple (x, y, z) is called
a point P in E3 and we write P = (x, y, z). The numbers x, y, z are called,
respectively, the x, y, z coordinates of P.
The coordinate axes are the sets of points:
{(x, 0, 0)}
(xâ€“axis), {(0, y, 0)}
(yâ€“axis), {(0, 0, z)}
(zâ€“axis).
The only point common to all three axes is the origin O = (0, 0, 0).
The coordinate planes are the sets of points:
{(x, y, 0)}
(xyâ€“plane), {(0, y, z)}
(yzâ€“plane), {(x, 0, z)}
(xzâ€“plane).
The positive octant consists of the points (x, y, z), where x > 0, y >
0, z > 0.
We think of the points (x, y, z) with z > 0 as lying above the xyâ€“plane,
and those with z < 0 as lying beneath the xyâ€“plane. A point P = (x, y, z)
will be represented as in Figure 8.3. The point illustrated lies in the positive
octant.
DEFINITION 8.2.2 The distance P1P2 between points P1 = (x1, y1, z1)
and P2 = (x2, y2, z2) is deï¬ned by the formula
P1P2 =
p
(x2 âˆ’x1)2 + (y2 âˆ’y1)2 + (z2 âˆ’z1)2.
For example, if P = (x, y, z),
OP =
p
x2 + y2 + z2.

8.2. THREEâ€“DIMENSIONAL SPACE
155
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O QQQQQQ (x, y, 0)
(x, 0, 0)
Â¢
Â¢
Â¢
Â¢
(0, y, 0)
Q
Q
Q
Q
Q
Q
P = (x, y, z)
(0, 0, z)
P = (x, y, z)
Figure 8.3: Representation of three-dimensional space.
-
y









 x
(x2, 0, 0)
(x1, 0, 0)
(0, y1, 0)
(0, y2, 0)
(0, 0, z1)
(0, 0, z2)
6
z
b
b
b
b
b
b
b
b
b
b
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
b
b
b
b
b
b
b
b
b
b
T
T
T
T
T





T
T
T
T
T
A
B
Figure 8.4: The vector
-
AB.

156
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
DEFINITION 8.2.3 If A = (x1, y1, z1) and B = (x2, y2, z2) we deï¬ne
the symbol
-
AB to be the column vector
-
AB=
ï£®
ï£°
x2 âˆ’x1
y2 âˆ’y1
z2 âˆ’z1
ï£¹
ï£».
We let P =
-
OP and call P the position vector of P.
The components of
-
AB are the coordinates of B when the axes are
translated to A as origin of coordinates.
We think of
-
AB as being represented by the directed line segment from
A to B and think of it as an arrow whose tail is at A and whose head is at
B. (See Figure 8.4.)
Some mathematicians think of
-
AB as representing the translation of
space which takes A into B.
The following simple properties of
-
AB are easily veriï¬ed and correspond
to how we intuitively think of directed line segments:
(i)
-
AB= 0 â‡”A = B;
(ii)
-
BA= âˆ’
-
AB;
(iii)
-
AB +
-
BC=
-
AC (the triangle law);
(iv)
-
BC=
-
AC âˆ’
-
AB= C âˆ’B;
(v) if X is a vector and A a point, there is exactly one point B such that
-
AB= X, namely that deï¬ned by B = A + X.
To derive properties of the distance function and the vector function
-
P1P2, we need to introduce the dot product of two vectors in R3.
8.3
Dot product
DEFINITION 8.3.1 If X =
ï£®
ï£°
a1
b1
c1
ï£¹
ï£»and Y =
ï£®
ï£°
a2
b2
c2
ï£¹
ï£», then X Â· Y , the
dot product of X and Y , is deï¬ned by
X Â· Y = a1a2 + b1b2 + c1c2.

8.3. DOT PRODUCT
157
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
-
6
6

>




=
A
B
A
B
v =
-
AB
âˆ’v =
-
BA
Figure 8.5: The negative of a vector.
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
-
6
6

>

>

>

:
XXXX
z
A
B
C
D
A
B
C
(a)
(b)
-
AB=
-
CD
-
AC=
-
AB +
-
BC
-
BC=
-
AC âˆ’
-
AB
Figure 8.6: (a) Equality of vectors; (b) Addition and subtraction of vectors.
The dot product has the following properties:
(i) X Â· (Y + Z) = X Â· Y + X Â· Z;
(ii) X Â· Y = Y Â· X;
(iii) (tX) Â· Y = t(X Â· Y );
(iv) X Â· X = a2 + b2 + c2 if X =
ï£®
ï£°
a
b
c
ï£¹
ï£»;
(v) X Â· Y = XtY ;
(vi) X Â· X = 0 if and only if X = 0.
The length of X is deï¬ned by
||X|| =
p
a2 + b2 + c2 = (X Â· X)1/2.
We see that ||P|| = OP and more generally ||
-
P1P2 || = P1P2, the
distance between P1 and P2.

158
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
QQQQQQ
,
,
,
,
,
,
Â¢
Â¢
Â¢

i
-
j
6
-
6
ck
k
QQQQQQ
s ai + bj
ai
Â¢
Â¢
Â¢
Â¢
bj
P = ai + bj + ck
Figure 8.7: Position vector as a linear combination of i, j and k.
Vectors having unit length are called unit vectors.
The vectors
i =
ï£®
ï£°
1
0
0
ï£¹
ï£»,
j =
ï£®
ï£°
0
1
0
ï£¹
ï£»,
k =
ï£®
ï£°
0
0
1
ï£¹
ï£»
are unit vectors. Every vector is a linear combination of i, j and k:
ï£®
ï£°
a
b
c
ï£¹
ï£»= ai + bj + ck.
(See Figure 8.7.)
It is easy to prove that
||tX|| = |t| Â· ||X||,
if t is a real number. Hence if X is a nonâ€“zero vector, the vectors
Â±
1
||X||X
are unit vectors.
A useful property of the length of a vector is
||X Â± Y ||2 = ||X||2 Â± 2X Â· Y + ||Y ||2.
(8.2)

8.3. DOT PRODUCT
159
The following important property of the dot product is widely used in
mathematics:
THEOREM 8.3.1 (The Cauchyâ€“Schwarz inequality)
If X and Y are vectors in R3, then
|X Â· Y | â‰¤||X|| Â· ||Y ||.
(8.3)
Moreover if X Ì¸= 0 and Y Ì¸= 0, then
X Â· Y = ||X|| Â· ||Y ||
â‡”
Y = tX, t > 0,
X Â· Y = âˆ’||X|| Â· ||Y ||
â‡”
Y = tX, t < 0.
Proof. If X = 0, then inequality 8.3 is trivially true. So assume X Ì¸= 0.
Now if t is any real number, by equation 8.2,
0 â‰¤||tX âˆ’Y ||2
=
||tX||2 âˆ’2(tX) Â· Y + ||Y ||2
=
t2||X||2 âˆ’2(X Â· Y )t + ||Y ||2
=
at2 âˆ’2bt + c,
where a = ||X||2 > 0, b = X Â· Y, c = ||Y ||2.
Hence
a(t2 âˆ’2b
a t + c
a)
â‰¥0
Âµ
t âˆ’b
a
Â¶2
+ ca âˆ’b2
a2
â‰¥0
.
Substituting t = b/a in the last inequality then gives
ac âˆ’b2
a2
â‰¥0,
so
|b| â‰¤âˆšac = âˆšaâˆšc
and hence inequality 8.3 follows.
To discuss equality in the Cauchyâ€“Schwarz inequality, assume X Ì¸= 0
and Y Ì¸= 0.
Then if X Â· Y = ||X|| Â· ||Y ||, we have for all t
||tX âˆ’Y ||2
=
t2||X||2 âˆ’2tX Â· Y + ||Y ||2
=
t2||X||2 âˆ’2t||X|| Â· ||Y || + ||Y ||2
=
||tX âˆ’Y ||2.

160
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Taking t = ||X||/||Y || then gives ||tX âˆ’Y ||2 = 0 and hence tX âˆ’Y = 0.
Hence Y = tX, where t > 0. The case X Â· Y = âˆ’||X|| Â· ||Y || is proved
similarly.
COROLLARY 8.3.1 (The triangle inequality for vectors)
If X and Y are vectors, then
||X + Y || â‰¤||X|| + ||Y ||.
(8.4)
Moreover if X Ì¸= 0 and Y Ì¸= 0, then equality occurs in inequality 8.4 if and
only if Y = tX, where t > 0.
Proof.
||X + Y ||2
=
||X||2 + 2X Â· Y + ||Y ||2
â‰¤
||X||2 + 2||X|| Â· ||Y || + ||Y ||2
=
(||X|| + ||Y ||)2
and inequality 8.4 follows.
If ||X + Y || = ||X|| + ||Y ||, then the above proof shows that
X Â· Y = ||X|| Â· ||Y ||.
Hence if X Ì¸= 0 and Y Ì¸= 0, the ï¬rst case of equality in the Cauchyâ€“Schwarz
inequality shows that Y = tX with t > 0.
The triangle inequality for vectors gives rise to a corresponding inequality
for the distance function:
THEOREM 8.3.2 (The triangle inequality for distance)
If A, B, C are points, then
AC â‰¤AB + BC.
(8.5)
Moreover if B Ì¸= A and B Ì¸= C, then equality occurs in inequality 8.5 if and
only if
-
AB= r
-
AC, where 0 < r < 1.
Proof.
AC = ||
-
AC ||
=
||
-
AB +
-
BC ||
â‰¤
||
-
AB || + ||
-
BC ||
=
AB + BC.

8.4. LINES
161
Moreover if equality occurs in inequality 8.5 and B Ì¸= A and B Ì¸= C, then
X =
-
ABÌ¸= 0 and Y =
-
BCÌ¸= 0 and the equation AC = AB + BC becomes
||X + Y || = ||X|| + ||Y ||. Hence the case of equality in the vector triangle
inequality gives
Y =
-
BC= tX = t
-
AB, where t > 0.
Then
-
BC
=
-
AC âˆ’
-
AB= t
-
AB
-
AC
=
(1 + t)
-
AB
-
AB
=
r
-
AC,
where r = 1/(t + 1) satisï¬es 0 < r < 1.
8.4
Lines
DEFINITION 8.4.1 A line in E3 is the set L(P0, X) consisting of all
points P satisfying
P = P0 + tX,
t âˆˆR
or equivalently
-
P0P= tX,
(8.6)
for some ï¬xed point P0 and ï¬xed nonâ€“zero vector X. (See Figure 8.8.)
Equivalently, in terms of coordinates, equation 8.6 becomes
x = x0 + ta, y = y0 + tb, z = z0 + tc,
where not all of a, b, c are zero.
The following familiar property of straight lines is easily veriï¬ed.
THEOREM 8.4.1 If A and B are distinct points, there is one and only
one line containing A and B, namely L(A,
-
AB) or more explicitly the line
deï¬ned by
-
AP = t
-
AB, or equivalently, in terms of position vectors:
P = (1 âˆ’t)A + tB
or
P = A + t
-
AB .
(8.7)
Equations 8.7 may be expressed in terms of coordinates: if A = (x1, y1, z1)
and B = (x2, y2, z2), then
x = (1 âˆ’t)x1 + tx2, y = (1 âˆ’t)y1 + ty2, z = (1 âˆ’t)z1 + tz2.

162
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
P
P0
@
@
@
R
@
@
@
@
@
R
C
D
@
@
@
@
@
@
@
@
@
@
@
@
@
-
P0P= t
-
CD
Figure 8.8: Representation of a line.
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O Â©Â©Â©Â©Â©Â©
* B
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¡
Â¡
Â¡
Â¡Â¡

A
P
@
@
@
@
@
@
@
@
@
@
@
@
@
P = A + t
-
AB, 0 < t < 1
Figure 8.9: The line segment AB.

8.4. LINES
163
There is an important geometric signiï¬cance in the number t of the above
equation of the line through A and B. The proof is left as an exercise:
THEOREM 8.4.2 (Joachimsthalâ€™s ratio formulae)
If t is the parameter occurring in theorem 8.4.1, then
(i)
|t| = AP
AB ;
(ii)
Â¯Â¯Â¯Â¯
t
1 âˆ’t
Â¯Â¯Â¯Â¯ = AP
PB
if P Ì¸= B.
Also
(iii) P is between A and B if 0 < t < 1;
(iv) B is between A and P if 1 < t;
(v) A is between P and B if t < 0.
(See Figure 8.9.)
For example, t = 1
2 gives the midâ€“point P of the segment AB:
P = 1
2(A + B).
EXAMPLE 8.4.1 L is the line AB, where A = (âˆ’4, 3, 1), B = (1, 1, 0);
M is the line CD, where C = (2, 0, 2), D = (âˆ’1, 3, âˆ’2); N is the line EF,
where E = (1, 4, 7), F = (âˆ’4, âˆ’3, âˆ’13). Find which pairs of lines intersect
and also the points of intersection.
Solution. In fact only L and N intersect, in the point (âˆ’2
3, 5
3, 1
3). For
example, to determine if L and N meet, we start with vector equations for
L and N:
P = A + t
-
AB,
Q = E + s
-
EF,
equate P and Q and solve for s and t:
(âˆ’4i + 3j + k) + t(5i âˆ’2j âˆ’k) = (i + 4j + 7k) + s(âˆ’5i âˆ’7j âˆ’20k),
which on simplifying, gives
5t + 5s
=
5
âˆ’2t + 7s
=
1
âˆ’t + 20s
=
6
This system has the unique solution t = 2
3, s = 1
3 and this determines a
corresponding point P where the lines meet, namely P = (âˆ’2
3, 5
3, 1
3).
The same method yields inconsistent systems when applied to the other
pairs of lines.

164
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
EXAMPLE 8.4.2 If A = (5, 0, 7) and B = (2, âˆ’3, 6), ï¬nd the points P
on the line AB which satisfy AP/PB = 3.
Solution. Use the formulae
P = A + t
-
AB
and
Â¯Â¯Â¯Â¯
t
1 âˆ’t
Â¯Â¯Â¯Â¯ = AP
PB = 3.
Then
t
1 âˆ’t = 3 or âˆ’3,
so t = 3
4 or t = 3
2. The corresponding points are ( 11
4 , 9
4, 25
4 ) and ( 1
2, 9
2, 11
2 ).
DEFINITION 8.4.2 Let X and Y be nonâ€“zero vectors. Then X is parallel
or proportional to Y if X = tY for some t âˆˆR. We write Xâˆ¥Y if X is parallel
to Y . If X = tY , we say that X and Y have the same or opposite direction,
according as t > 0 or t < 0.
DEFINITION 8.4.3 if A and B are distinct points on a line L, the nonâ€“
zero vector
-
AB is called a direction vector for L.
It is easy to prove that any two direction vectors for a line are parallel.
DEFINITION 8.4.4 Let L and M be lines having direction vectors X
and Y , respectively. Then L is parallel to M if X is parallel to Y . Clearly
any line is parallel to itself.
It is easy to prove that the line through a given point A and parallel to a
given line CD has an equation P = A + t
-
CD.
THEOREM 8.4.3 Let X = a1i + b1j + c1k and Y = a2i + b2j + c2k be
nonâ€“zero vectors. Then X is parallel to Y if and only if
Â¯Â¯Â¯Â¯
a1
b1
a2
b2
Â¯Â¯Â¯Â¯ =
Â¯Â¯Â¯Â¯
b1
c1
b2
c2
Â¯Â¯Â¯Â¯ =
Â¯Â¯Â¯Â¯
a1
c1
a2
c2
Â¯Â¯Â¯Â¯ = 0.
(8.8)
Proof. The case of equality in the Cauchyâ€“Schwarz inequality (theorem 8.3.1)
shows that X and Y are parallel if and only if
|X Â· Y | = ||X|| Â· ||Y ||.
Squaring gives the equivalent equality
(a1a2 + b1b2 + c1c2)2 = (a2
1 + b2
1 + c2
1)(a2
2 + b2
2 + c2
2),

8.4. LINES
165
which simpliï¬es to
(a1b2 âˆ’a2b1)2 + (b1c2 âˆ’b2c1)2 + (a1c2 âˆ’a2c1)2 = 0,
which is equivalent to
a1b2 âˆ’a2b1 = 0, b1c2 âˆ’b2c1 = 0, a1c2 âˆ’a2c1 = 0,
which is equation 8.8.
Equality of geometrical vectors has a fundamental geometrical interpre-
tation:
THEOREM 8.4.4 Suppose A, B, C, D are distinct points such that no
three are collinear. Then
-
AB=
-
CD if and only if
-
AB âˆ¥
-
CD and
-
AC âˆ¥
-
BD
(See Figure 8.1.)
Proof. If
-
AB=
-
CD then
B âˆ’A
=
D âˆ’C,
C âˆ’A
=
D âˆ’B
and so
-
AC=
-
BD. Hence
-
AB âˆ¥
-
CD and
-
AC âˆ¥
-
BD.
Conversely, suppose that
-
AB âˆ¥
-
CD and
-
AC âˆ¥
-
BD. Then
-
AB= s
-
CD
and
-
AC= t
-
BD,
or
B âˆ’A = s(D âˆ’C)
and
C âˆ’A = tD âˆ’B.
We have to prove s = 1 or equivalently, t = 1.
Now subtracting the second equation above from the ï¬rst, gives
B âˆ’C = s(D âˆ’C) âˆ’t(D âˆ’B),
so
(1 âˆ’t)B = (1 âˆ’s)C + (s âˆ’t)D.
If t Ì¸= 1, then
B = 1 âˆ’s
1 âˆ’t C + s âˆ’t
1 âˆ’tD
and B would lie on the line CD. Hence t = 1.

166
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
8.5
The angle between two vectors
DEFINITION 8.5.1 Let X and Y be nonâ€“zero vectors. Then the angle
between X and Y is the unique value of Î¸ deï¬ned by
cos Î¸ =
X Â· Y
||X|| Â· ||Y ||,
0 â‰¤Î¸ â‰¤Ï€.
REMARK 8.5.1 By Cauchyâ€™s inequality, we have
âˆ’1 â‰¤
X Â· Y
||X|| Â· ||Y || â‰¤1,
so the above equation does deï¬ne an angle Î¸.
In terms of components, if X = [a1, b1, c1]t and Y = [a2, b2, c2]t, then
cos Î¸ =
a1a2 + b1b2 + c1c2
p
a2
1 + b2
1 + c2
1
p
a2
2 + b2
2 + c2
2
.
(8.9)
The next result is the well-known cosine rule for a triangle.
THEOREM 8.5.1 (Cosine rule) If A, B, C are points with A Ì¸= B and
A Ì¸= C, then the angle Î¸ between vectors
-
AB and
-
AC satiï¬es
cos Î¸ = AB2 + AC2 âˆ’BC2
2AB Â· AC
,
(8.10)
or equivalently
BC2 = AB2 + AC2 âˆ’2AB Â· AC cos Î¸.
(See Figure 8.10.)
Proof. Let A = (x1, y1, z1), B = (x2, y2, z2), C = (x3, y3, z3). Then
-
AB
=
a1i + b1j + c1k
-
AC
=
a2i + b2j + c2k
-
BC
=
(a2 âˆ’a1)i + (b2 âˆ’b1)j + (c2 âˆ’c1)k,
where
ai = xi+1 âˆ’x1, bi = yi+1 âˆ’y1, ci = zi+1 âˆ’z1, i = 1, 2.

8.5. THE ANGLE BETWEEN TWO VECTORS
167
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
Â¡
Â¡
Â¡
Â¡
A
C
C
C
C
C
C
C
C
C
Q
Q
Q
Q
Q
Q
B
Î¸
cos Î¸ = AB2+AC2âˆ’BC2
2ABÂ·AC
Figure 8.10: The cosine rule for a triangle.
Now by equation 8.9,
cos Î¸ = a1a2 + b1b2 + c1c2
AB Â· AC
.
Also
AB2 + AC2 âˆ’BC2
=
(a2
1 + b2
1 + c2
1) + (a2
2 + b2
2 + c2
2)
âˆ’((a2 âˆ’a1)2 + (b2 âˆ’b1)2 + (c2 âˆ’c1)2)
=
2a1a2 + 2b1b2 + c1c2.
Equation 8.10 now follows, since
-
AB Â·
-
AC= a1a2 + b1b2 + c1c2.
EXAMPLE 8.5.1 Let A = (2, 1, 0), B = (3, 2, 0), C = (5, 0, 1). Find
the angle Î¸ between vectors
-
AB and
-
AC.
Solution.
cos Î¸ =
-
AB Â·
-
AC
AB Â· AC .
Now
-
AB= i + j
and
-
AC= 3i âˆ’j + k.

168
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
Â¡
QQ
A
C
C
C
C
C
C
C
C
C
Q
Q
Q
Q
Q
Q
Â¡
Â¡
Â¡
Â¡
B
AB2 + AC2 = BC2
Figure 8.11: Pythagorasâ€™ theorem for a rightâ€“angled triangle.
Hence
cos Î¸ =
1 Ã— 3 + 1 Ã— (âˆ’1) + 0 Ã— 1
âˆš
12 + 12 + 02p
32 + (âˆ’1)2 + 12 =
2
âˆš
2
âˆš
11 =
âˆš
2
âˆš
11.
Hence Î¸ = cosâˆ’1
âˆš
2
âˆš
11.
DEFINITION 8.5.2 If X and Y are vectors satisfying X Â· Y = 0, we say
X is orthogonal or perpendicular to Y .
REMARK 8.5.2 If A, B, C are points forming a triangle and
-
AB is or-
thogonal to
-
AC, then the angle Î¸ between
-
AB and
-
AC satisï¬es cos Î¸ = 0
and hence Î¸ = Ï€
2 and the triangle is rightâ€“angled at A.
Then we have Pythagorasâ€™ theorem:
BC2 = AB2 + AC2.
(8.11)
We also note that BC â‰¥AB and BC â‰¥AC follow from equation 8.11. (See
Figure 8.11.)
EXAMPLE 8.5.2 Let A = (2, 9, 8), B = (6, 4, âˆ’2), C = (7, 15, 7). Show
that
-
AB and
-
AC are perpendicular and ï¬nd the point D such that ABDC
forms a rectangle.

8.5. THE ANGLE BETWEEN TWO VECTORS
169
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
C
C
C
C
C
C
C
C
Â¡
@
     
@
@
@
@
@
@
@
@
@
@
@
C
A
B
P
Â¡
Â¡
Â¡
Figure 8.12: Distance from a point to a line.
Solution.
-
AB Â·
-
AC= (4i âˆ’5j âˆ’10k) Â· (5i + 6j âˆ’k) = 20 âˆ’30 + 10 = 0.
Hence
-
AB and
-
AC are perpendicular. Also, the required fourth point D
clearly has to satisfy the equation
-
BD=
-
AC, or equivalently D âˆ’B =
-
AC .
Hence
D = B+
-
AC= (6i + 4j âˆ’2k) + (5i + 6j âˆ’k) = 11i + 10j âˆ’3k,
so D = (11, 10, âˆ’3).
THEOREM 8.5.2 (Distance from a point to a line) If C is a point
and L is the line through A and B, then there is exactly one point P on L
such that
-
CP is perpendicular to
-
AB, namely
P = A + t
-
AB,
t =
-
AC Â·
-
AB
AB2
.
(8.12)
Moreover if Q is any point on L, then CQ â‰¥CP and hence P is the point
on L closest to C.

170
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
The shortest distance CP is given by
CP =
q
AC2AB2 âˆ’(
-
AC Â·
-
AB)2
AB
.
(8.13)
(See Figure 8.12.)
Proof. Let P = A + t
-
AB and assume that
-
CP is perpendicular to
-
AB.
Then
-
CP Â·
-
AB
=
0
(P âˆ’C)Â·
-
AB
=
0
(A + t
-
AB âˆ’C)Â·
-
AB
=
0
(
-
CA +t
-
AB)Â·
-
AB
=
0
-
CA Â·
-
AB +t(
-
AB Â·
-
AB)
=
0
âˆ’
-
AC Â·
-
AB +t(
-
AB Â·
-
AB)
=
0,
so equation 8.12 follows.
The inequality CQ â‰¥CP, where Q is any point on L, is a consequence
of Pythagorasâ€™ theorem.
Finally, as
-
CP and
-
PA are perpendicular, Pythagorasâ€™ theorem gives
CP 2
=
AC2 âˆ’PA2
=
AC2 âˆ’||t
-
AB ||2
=
AC2 âˆ’t2AB2
=
AC2 âˆ’
ï£«
ï£­
-
AC Â·
-
AB
AB2
ï£¶
ï£¸
2
AB2
=
AC2AB2 âˆ’(
-
AC Â·
-
AB)2
AB2
,
as required.
EXAMPLE 8.5.3 The closest point on the line through A = (1, 2, 1) and
B = (2, âˆ’1, 3) to the origin is P = ( 17
14, 19
14, 20
14) and the corresponding
shortest distance equals
5
14
âˆš
42.
Another application of theorem 8.5.2 is to the projection of a line segment
on another line:

8.5. THE ANGLE BETWEEN TWO VECTORS
171
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
@
@
@
@
@
@
@
@
@
@
@
B
A
Â¡
Â¡
Â¡
Â¡
Â¡
@
@
@
@""""""""""
PPPPPPPPP
C2
C1
P2
P1
@
Â¡
"
"
Figure 8.13: Projecting the segment C1C2 onto the line AB.
THEOREM 8.5.3 (The projection of a line segment onto a line)
Let C1, C2 be points and P1, P2 be the feet of the perpendiculars from
C1 and C2 to the line AB. Then
P1P2 = |
-
C1C2 Â·Ë†n|,
where
Ë†n =
1
AB
-
AB .
Also
C1C2 â‰¥P1P2.
(8.14)
(See Figure 8.13.)
Proof. Using equations 8.12, we have
P1 = A + t1
-
AB,
P2 = A + t2
-
AB,
where
t1 =
-
AC1 Â·
-
AB
AB2
,
t2 =
-
AC2 Â·
-
AB
AB2
.
Hence
-
P1P2
=
(A + t2
-
AB) âˆ’(A + t1
-
AB)
=
(t2 âˆ’t1)
-
AB,

172
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
so
P1P2
=
||
-
P1P2 || = |t2 âˆ’t1|AB
=
Â¯Â¯Â¯Â¯Â¯Â¯
-
AC2 Â·
-
AB
AB2
âˆ’
-
AC1 Â·
-
AB
AB2
Â¯Â¯Â¯Â¯Â¯Â¯
AB
=
Â¯Â¯Â¯Â¯
-
C1C2 Â·
-
AB
Â¯Â¯Â¯Â¯
AB2
AB
=
Â¯Â¯Â¯Â¯
-
C1C2 Â·Ë†n
Â¯Â¯Â¯Â¯ ,
where Ë†n is the unit vector
Ë†n =
1
AB
-
AB .
Inequality 8.14 then follows from the Cauchyâ€“Schwarz inequality 8.3.
DEFINITION 8.5.3 Two nonâ€“intersecting lines are called skew if they
have nonâ€“parallel direction vectors.
Theorem 8.5.3 has an application to the problem of showing that two skew
lines have a shortest distance between them.
(The reader is referred to
problem 16 at the end of the chapter.)
Before we turn to the study of planes, it is convenient to introduce the
crossâ€“product of two vectors.
8.6
The crossâ€“product of two vectors
DEFINITION 8.6.1 Let X = a1i + b1j + c1k and Y = a2i + b2j + c2k.
Then X Ã— Y , the crossâ€“product of X and Y , is deï¬ned by
X Ã— Y = ai + bj + ck,
where
a =
Â¯Â¯Â¯Â¯
b1
c1
b2
c2
Â¯Â¯Â¯Â¯ ,
b = âˆ’
Â¯Â¯Â¯Â¯
a1
c1
a2
c2
Â¯Â¯Â¯Â¯ ,
c =
Â¯Â¯Â¯Â¯
a1
b1
a2
b2
Â¯Â¯Â¯Â¯ .
The vector crossâ€“product has the following properties which follow from
properties of 2 Ã— 2 and 3 Ã— 3 determinants:
(i) i Ã— j = k,
j Ã— k = i,
k Ã— i = j;

8.6. THE CROSSâ€“PRODUCT OF TWO VECTORS
173
(ii) X Ã— X = 0;
(iii) Y Ã— X = âˆ’X Ã— Y ;
(iv) X Ã— (Y + Z) = X Ã— Y + X Ã— Z;
(v) (tX) Ã— Y = t(X Ã— Y );
(vi) (Scalar triple product formula) if Z = a3i + b3j + c3k, then
X Â· (Y Ã— Z) =
Â¯Â¯Â¯Â¯Â¯Â¯
a1
b1
c1
a2
b2
c2
a3
b3
c3
Â¯Â¯Â¯Â¯Â¯Â¯
= (X Ã— Y ) Â· Z;
(vii) X Â· (X Ã— Y ) = 0 = Y Â· (X Ã— Y );
(viii) ||X Ã— Y || =
p
||X||2||Y ||2 âˆ’(X Â· Y )2;
(ix) if X and Y are nonâ€“zero vectors and Î¸ is the angle between X and Y ,
then
||X Ã— Y || = ||X|| Â· ||Y || sin Î¸.
(See Figure 8.14.)
From theorem 8.4.3 and the deï¬nition of crossâ€“product, it follows that
nonâ€“zero vectors X and Y are parallel if and only if X Ã— Y = 0; hence by
(vii), the crossâ€“product of two nonâ€“parallel, nonâ€“zero vectors X and Y , is
a nonâ€“zero vector perpendicular to both X and Y .
LEMMA 8.6.1 Let X and Y be nonâ€“zero, nonâ€“parallel vectors.
(i) Z is a linear combination of X and Y , if and only if Z is perpendicular
to X Ã— Y ;
(ii) Z is perpendicular to X and Y , if and only if Z is parallel to X Ã— Y .
Proof. Let X and Y be nonâ€“zero, nonâ€“parallel vectors. Then
X Ã— Y Ì¸= 0.
Then if X Ã— Y = ai + bj + ck, we have
det [X Ã— Y |X|Y ]t =
Â¯Â¯Â¯Â¯Â¯Â¯
a
b
c
a1
b1
c1
a2
b2
c2
Â¯Â¯Â¯Â¯Â¯Â¯
= (X Ã— Y ) Â· (X Ã— Y ) > 0.

174
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
@
@
@
@
I
Â¡
Â¡
Â¡
Âª
-
Î¸
Â¡
@
@
X
Y
X Ã— Y
Figure 8.14: The vector crossâ€“product.
Hence the matrix [X Ã— Y |X|Y ] is nonâ€“singular. Consequently the linear
system
r(X Ã— Y ) + sX + tY = Z
(8.15)
has a unique solution r, s, t.
(i) Suppose Z = sX + tY . Then
Z Â· (X Ã— Y ) = sX Â· (X Ã— Y ) + tY Â· (X Ã— Y ) = s0 + t0 = 0.
Conversely, suppose that
Z Â· (X Ã— Y ) = 0.
(8.16)
Now from equation 8.15, r, s, t exist satisfying
Z = r(X Ã— Y ) + sX + tY.
Then equation 8.16 gives
0
=
(r(X Ã— Y ) + sX + tY ) Â· (X Ã— Y )
=
r||X Ã— Y ||2 + sX Â· (X Ã— Y ) + tY Â· (Y Ã— X)
=
r||X Ã— Y ||2.
Hence r = 0 and Z = sX + tY , as required.
(ii) Suppose Z = Î»(X Ã— Y ). Then clearly Z is perpendicular to X and Y .

8.6. THE CROSSâ€“PRODUCT OF TWO VECTORS
175
Conversely suppose that Z is perpendicular to X and Y .
Now from equation 8.15, r, s, t exist satisfying
Z = r(X Ã— Y ) + sX + tY.
Then
sX Â· X + tX Â· Y
=
X Â· Z = 0
sY Â· X + tY Â· Y
=
Y Â· Z = 0,
from which it follows that
(sX + tY ) Â· (sX + tY ) = 0.
Hence sX + tY = 0 and so s = 0, t = 0. Consequently Z = r(X Ã— Y ), as
required.
The crossâ€“product gives a compact formula for the distance from a point
to a line, as well as the area of a triangle.
THEOREM 8.6.1 (Area of a triangle)
If A, B, C are distinct nonâ€“collinear points, then
(i) the distance d from C to the line AB is given by
d = ||
-
AB Ã—
-
AC ||
AB
,
(8.17)
(ii) the area of the triangle ABC equals
||
-
AB Ã—
-
AC ||
2
= ||A Ã— B + B Ã— C + C Ã— A||
2
.
(8.18)
Proof. The area âˆ†of triangle ABC is given by
âˆ†= AB Â· CP
2
,
where P is the foot of the perpendicular from C to the line AB. Now by
formula 8.13, we have
CP
=
q
AC2 Â· AB2 âˆ’(
-
AC Â·
-
AB)2
AB
=
||
-
AB Ã—
-
AC ||
AB
,

176
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
which, by property (viii) of the crossâ€“product, gives formula 8.17.
The
second formula of equation 8.18 follows from the equations
-
AB Ã—
-
AC
=
(B âˆ’A) Ã— (C âˆ’A)
=
{(B âˆ’A) Ã— C} âˆ’{(C âˆ’A) Ã— A}
=
{(B Ã— C âˆ’A Ã— C)} âˆ’{(B Ã— A âˆ’A Ã— A)}
=
B Ã— C âˆ’A Ã— C âˆ’B Ã— A
=
B Ã— C + C Ã— A + A Ã— B,
as required.
8.7
Planes
DEFINITION 8.7.1 A plane is a set of points P satisfying an equation
of the form
P = P0 + sX + tY, s, t âˆˆR,
(8.19)
where X and Y are nonâ€“zero, nonâ€“parallel vectors.
For example, the xyâ€“plane consists of the points P = (x, y, 0) and corre-
sponds to the plane equation
P = xi + yj = O + xi + yj.
In terms of coordinates, equation 8.19 takes the form
x
=
x0 + sa1 + ta2
y
=
y0 + sb1 + tb2
z
=
z0 + sc1 + tc2,
where P0 = (x0, y0, z0) and (a1, b1, c1) and (a2, b2, c2) are nonâ€“zero and
nonâ€“proportional.
THEOREM 8.7.1 Let A, B, C be three nonâ€“collinear points. Then there
is one and only one plane through these points, namely the plane given by
the equation
P = A + s
-
AB +t
-
AC,
(8.20)
or equivalently
-
AP= s
-
AB +t
-
AC .
(8.21)
(See Figure 8.15.)

8.7. PLANES
177
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O 
:





Â©Â©Â©Â©
*
-
PPPPPP
q
PPPPPP
Â©Â©Â©Â©
*
q
A
C
B
Bâ€²
Câ€²
P
-
ABâ€²= s
-
AB,
-
ACâ€²= t
-
AC
-
AP= s
-
AB +t
-
AC
Figure 8.15: Vector equation for the plane ABC.
Proof.
First note that equation 8.20 is indeed the equation of a plane
through A, B and C, as
-
AB and
-
AC are nonâ€“zero and nonâ€“parallel and
(s, t) = (0, 0), (1, 0) and (0, 1) give P = A, B and C, respectively. Call
this plane P.
Conversely, suppose P = P0 + sX + tY is the equation of a plane Q
passing through A, B, C. Then A = P0 + s0X + t0Y , so the equation for
Q may be written
P = A + (s âˆ’s0)X + (t âˆ’t0)Y = A + sâ€²X + tâ€²Y ;
so in eï¬€ect we can take P0 = A in the equation of Q. Then the fact that B
and C lie on Q gives equations
B = A + s1X + t1Y,
C = A + s2X + t2Y,
or
-
AB= s1X + t1Y,
-
AC= s2X + t2Y.
(8.22)
Then equations 8.22 and equation 8.20 show that
P âŠ†Q.
Conversely, it is straightforward to show that because
-
AB and
-
AC are not
parallel, we have
Â¯Â¯Â¯Â¯
s1
t1
s2
t2
Â¯Â¯Â¯Â¯ Ì¸= 0.

178
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
@
@
@
@
@
I
Â¤
Â¤
Â¤
Â¤
Â¡
Â¡
Â¡
Â¡

HHHHHH
j
@
Â¡
A
B
C
P
D
-
AD=
-
AB Ã—
-
AC
-
AD Â·
-
AP = 0
Figure 8.16: Normal equation of the plane ABC.
Hence equations 8.22 can be solved for X and Y as linear combinations of
-
AB and
-
AC, allowing us to deduce that
Q âŠ†P.
Hence
Q = P.
THEOREM 8.7.2 (Normal equation for a plane) Let
A = (x1, y1, z1), B = (x2, y2, z2), C = (x3, y3, z3)
be three nonâ€“collinear points. Then the plane through A, B, C is given by
-
AP Â·(
-
AB Ã—
-
AC) = 0,
(8.23)
or equivalently,
Â¯Â¯Â¯Â¯Â¯Â¯
x âˆ’x1
y âˆ’y1
z âˆ’z1
x2 âˆ’x1
y2 âˆ’y1
z2 âˆ’z1
x3 âˆ’x1
y3 âˆ’y1
z3 âˆ’z1
Â¯Â¯Â¯Â¯Â¯Â¯
= 0,
(8.24)
where P = (x, y, z). (See Figure 8.16.)

8.7. PLANES
179
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢


A
A
A
AK ai + bj + ck
ax + by + cz = d
Figure 8.17: The plane ax + by + cz = d.
REMARK 8.7.1 Equation 8.24 can be written in more symmetrical form
as
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
x
y
z
1
x1
y1
z1
1
x2
y2
z2
1
x3
y3
z3
1
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
= 0.
(8.25)
Proof. Let P be the plane through A, B, C. Then by equation 8.21, we
have P âˆˆP if and only if
-
AP is a linear combination of
-
AB and
-
AC and so
by lemma 8.6.1(i), using the fact that
-
AB Ã—
-
ACÌ¸= 0 here, if and only if
-
AP
is perpendicular to
-
AB Ã—
-
AC. This gives equation 8.23.
Equation 8.24 is the scalar triple product version of equation 8.23, taking
into account the equations
-
AP
=
(x âˆ’x1)i + (y âˆ’y1)j + (z âˆ’z1)k,
-
AB
=
(x2 âˆ’x1)i + (y2 âˆ’y1)j + (z2 âˆ’z1)k,
-
AC
=
(x3 âˆ’x1)i + (y3 âˆ’y1)j + (z3 âˆ’z1)k.
REMARK 8.7.2 Equation 8.24 gives rise to a linear equation in x, y and
z:
ax + by + cz = d,

180
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
where ai + bj + ck Ì¸= 0. For
Â¯Â¯Â¯Â¯Â¯Â¯
x âˆ’x1
y âˆ’y1
z âˆ’z1
x2 âˆ’x1
y2 âˆ’y1
z2 âˆ’z1
x3 âˆ’x1
y3 âˆ’y1
z3 âˆ’z1
Â¯Â¯Â¯Â¯Â¯Â¯
=
Â¯Â¯Â¯Â¯Â¯Â¯
x
y
z
x2 âˆ’x1
y2 âˆ’y1
z2 âˆ’z1
x3 âˆ’x1
y3 âˆ’y1
z3 âˆ’z1
Â¯Â¯Â¯Â¯Â¯Â¯
âˆ’
Â¯Â¯Â¯Â¯Â¯Â¯
x1
y1
z1
x2 âˆ’x1
y2 âˆ’y1
z2 âˆ’z1
x3 âˆ’x1
y3 âˆ’y1
z3 âˆ’z1
Â¯Â¯Â¯Â¯Â¯Â¯
(8.26)
and expanding the ï¬rst determinant on the rightâ€“hand side of equation 8.26
along row 1 gives an expression
ax + by + cz
where
a =
Â¯Â¯Â¯Â¯
y2 âˆ’y1
z2 âˆ’z1
y3 âˆ’y1
z3 âˆ’z1
Â¯Â¯Â¯Â¯ , b = âˆ’
Â¯Â¯Â¯Â¯
x2 âˆ’x1
z2 âˆ’z1
x3 âˆ’x1
z3 âˆ’z1
Â¯Â¯Â¯Â¯ , c =
Â¯Â¯Â¯Â¯
x2 âˆ’x1
y2 âˆ’y1
x3 âˆ’x1
y3 âˆ’y1
Â¯Â¯Â¯Â¯ .
But a, b, c are the components of
-
AB Ã—
-
AC, which in turn is nonâ€“zero, as
A, B, C are nonâ€“collinear here.
Conversely if ai + bj + ck Ì¸= 0, the equation
ax + by + cz = d
does indeed represent a plane. For if say a Ì¸= 0, the equation can be solved
for x in terms of y and z:
ï£®
ï£°
x
y
z
ï£¹
ï£»=
ï£®
ï£°
âˆ’d
a
0
0
ï£¹
ï£»+ y
ï£®
ï£°
âˆ’b
a
1
0
ï£¹
ï£»+ z
ï£®
ï£°
âˆ’c
a
0
1
ï£¹
ï£»,
which gives the plane
P = P0 + yX + zY,
where P0 = (âˆ’d
a, 0, 0) and X = âˆ’b
ai + j and Y = âˆ’c
ai + k are evidently
nonâ€“parallel vectors.
REMARK 8.7.3 The plane equation ax+by +cz = d is called the normal
form, as it is easy to prove that if P1 and P2 are two points in the plane,
then ai + bj + ck is perpendicular to
-
P1P2. Any nonâ€“zero vector with this
property is called a normal to the plane. (See Figure 8.17.)

8.7. PLANES
181
By lemma 8.6.1(ii), it follows that every vector X normal to a plane
through three nonâ€“collinear points A, B, C is parallel to
-
AB Ã—
-
AC, since
X is perpendicular to
-
AB and
-
AC.
EXAMPLE 8.7.1 Show that the planes
x + y âˆ’2z = 1
and
x + 3y âˆ’z = 4
intersect in a line and ï¬nd the distance from the point C = (1, 0, 1) to this
line.
Solution. Solving the two equations simultaneously gives
x = âˆ’1
2 + 5
2z,
y = 3
2 âˆ’1
2z,
(8.27)
where z is arbitrary. Hence
xi + yj + zk = âˆ’1
2i âˆ’3
2j + z(5
2i âˆ’1
2j + k),
which is the equation of a line L through A = (âˆ’1
2, âˆ’3
2, 0) and having
direction vector 5
2i âˆ’1
2j + k.
We can now proceed in one of three ways to ï¬nd the closest point on L
to A.
One way is to use equation 8.17 with B deï¬ned by
-
AB= 5
2i âˆ’1
2j + k.
Another method minimizes the distance CP, where P ranges over L.
A third way is to ï¬nd an equation for the plane through C, having
5
2i âˆ’1
2j + k as a normal. Such a plane has equation
5x âˆ’y + 2z = d,
where d is found by substituting the coordinates of C in the last equation.
d = 5 Ã— 1 âˆ’0 + 2 Ã— 1 = 7.
We now ï¬nd the point P where the plane intersects the line L. Then
-
CP
will be perpendicular to L and CP will be the required shortest distance
from C to L. We ï¬nd using equations 8.27 that
5(âˆ’1
2 + 5
2z) âˆ’(3
2 âˆ’1
2z) + 2z = 7,

182
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢


A
A
A
AK
a1i + b1j + c1k
a2i + b2j + c2k
a1x + b1y + c1z = d1
a2x + b2y + c2z = d2
XXXXXXXX
XXXXXXXX
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
L
Figure 8.18: Line of intersection of two planes.
so z = 11
15. Hence P = ( 4
3, 17
15, 11
15).
It is clear that through a given line and a point not on that line, there
passes exactly one plane. If the line is given as the intersection of two planes,
each in normal form, there is a simple way of ï¬nding an equation for this
plane. More explicitly we have the following result:
THEOREM 8.7.3 Suppose the planes
a1x + b1y + c1z
=
d1
(8.28)
a2x + b2y + c2z
=
d2
(8.29)
have nonâ€“parallel normals. Then the planes intersect in a line L.
Moreover the equation
Î»(a1x + b1y + c1z âˆ’d1) + Âµ(a2x + b2y + c2z âˆ’d2) = 0,
(8.30)
where Î» and Âµ are not both zero, gives all planes through L.
(See Figure 8.18.)
Proof. Assume that the normals a1i + b1j + c1k and a2i + b2j + c2k are
nonâ€“parallel. Then by theorem 8.4.3, not all of
âˆ†1 =
Â¯Â¯Â¯Â¯
a1
b1
a2
b2
Â¯Â¯Â¯Â¯ ,
âˆ†2 =
Â¯Â¯Â¯Â¯
b1
c1
b2
c2
Â¯Â¯Â¯Â¯ ,
âˆ†3 =
Â¯Â¯Â¯Â¯
a1
c1
a2
c2
Â¯Â¯Â¯Â¯
(8.31)

8.7. PLANES
183
are zero. If say âˆ†1 Ì¸= 0, we can solve equations 8.28 and 8.29 for x and y in
terms of z, as we did in the previous example, to show that the intersection
forms a line L.
We next have to check that if Î» and Âµ are not both zero, then equa-
tion 8.30 represents a plane. (Whatever set of points equation 8.30 repre-
sents, this set certainly contains L.)
(Î»a1 + Âµa2)x + (Î»b1 + Âµb2)y + (Î»c1 + Âµc2)z âˆ’(Î»d1 + Âµd2) = 0.
Then we clearly cannot have all the coeï¬ƒcients
Î»a1 + Âµa2,
Î»b1 + Âµb2,
Î»c1 + Âµc2
zero, as otherwise the vectors a1i + b1j + c1k and a2i + b2j + c2k would be
parallel.
Finally, if P is a plane containing L, let P0 = (x0, y0, z0) be a point not
on L. Then if we deï¬ne Î» and Âµ by
Î» = âˆ’(a2x0 + b2y0 + c2z0 âˆ’d2),
Âµ = a1x0 + b1y0 + c1z0 âˆ’d1,
then at least one of Î» and Âµ is nonâ€“zero. Then the coordinates of P0 satisfy
equation 8.30, which therefore represents a plane passing through L and P0
and hence identical with P.
EXAMPLE 8.7.2 Find an equation for the plane through P0 = (1, 0, 1)
and passing through the line of intersection of the planes
x + y âˆ’2z = 1
and
x + 3y âˆ’z = 4.
Solution. The required plane has the form
Î»(x + y âˆ’2z âˆ’1) + Âµ(x + 3y âˆ’z âˆ’4) = 0,
where not both of Î» and Âµ are zero. Substituting the coordinates of P0 into
this equation gives
âˆ’2Î» + Âµ(âˆ’4) = 0,
Î» = âˆ’2Âµ.
So the required equation is
âˆ’2Âµ(x + y âˆ’2z âˆ’1) + Âµ(x + 3y âˆ’z âˆ’4) = 0,
or
âˆ’x + y + 3z âˆ’2 = 0.
Our ï¬nal result is a formula for the distance from a point to a plane.

184
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
-
6
y
z
x
O
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢
Â¢


A
A
A
AK
A
A
A
A
A
A
A
AK
ai + bj + ck
ax + by + cz = d
P0
P
Figure 8.19: Distance from a point P0 to the plane ax + by + cz = d.
THEOREM 8.7.4 (Distance from a point to a plane)
Let P0 = (x0, y0, z0) and P be the plane
ax + by + cz = d.
(8.32)
Then there is a unique point P on P such that
-
P0P is normal to P. Morever
P0P = |ax0 + by0 + cz0 âˆ’d|
âˆš
a2 + b2 + c2
(See Figure 8.19.)
Proof. The line through P0 normal to P is given by
P = P0 + t(ai + bj + ck),
or in terms of coordinates
x = x0 + at,
y = y0 + bt,
z = z0 + ct.
Substituting these formulae in equation 8.32 gives
a(x0 + at) + b(y0 + bt) + c(z0 + ct)
=
d
t(a2 + b2 + c2)
=
âˆ’(ax0 + by0 + cz0 âˆ’d),
so
t = âˆ’
Âµax0 + by0 + cz0 âˆ’d
a2 + b2 + c2
Â¶
.

8.8. PROBLEMS
185
Then
P0P = ||
-
P0P ||
=
||t(ai + bj + ck)||
=
|t|
p
a2 + b2 + c2
=
|ax0 + by0 + cz0 âˆ’d|
a2 + b2 + c2
p
a2 + b2 + c2
=
|ax0 + by0 + cz0 âˆ’d|
âˆš
a2 + b2 + c2
.
Other interesting geometrical facts about lines and planes are left to the
problems at the end of this chapter.
8.8
PROBLEMS
.
1. Find the point where the line through A = (3, âˆ’2, 7) and B =
(13, 3, âˆ’8) meets the xzâ€“plane.
[Ans: (7, 0, 1).]
2. Let A, B, C be nonâ€“collinear points.
If E is the midâ€“point of the
segment BC and F is the point on the segment EA satisfying AF
EF = 2,
prove that
F = 1
3(A + B + C).
(F is called the centroid of triangle ABC.)
3. Prove that the points (2, 1, 4), (1, âˆ’1, 2), (3, 3, 6) are collinear.
4. If A = (2, 3, âˆ’1) and B = (3, 7, 4), ï¬nd the points P on the line AB
satisfying PA/PB = 2/5.
[Ans:
Â¡ 16
7 , 29
7 , 3
7
Â¢
and
Â¡ 4
3, 1
3, âˆ’13
3
Â¢
.]
5. Let M be the line through A = (1, 2, 3) parallel to the line joining
B = (âˆ’2, 2, 0) and C = (4, âˆ’1, 7). Also N is the line joining E =
(1, âˆ’1, 8) and F = (10, âˆ’1, 11). Prove that M and N intersect and
ï¬nd the point of intersection.
[Ans: (7, âˆ’1, 10).]

186
CHAPTER 8. THREEâ€“DIMENSIONAL GEOMETRY
6. Prove that the triangle formed by the points (âˆ’3, 5, 6), (âˆ’2, 7, 9) and
(2, 1, 7) is a 30o, 60o, 90o triangle.
7. Find the point on the line AB closest to the origin, where A =
(âˆ’2, 1, 3) and B = (1, 2, 4). Also ï¬nd this shortest distance.
[Ans:
Â¡
âˆ’16
11, 13
11, 35
11
Â¢
and
q
150
11 .]
8. A line N is determined by the two planes
x + y âˆ’2z = 1,
and
x + 3y âˆ’z = 4.
Find the point P on N closest to the point C = (1, 0, 1) and ï¬nd the
distance PC.
[Ans:
Â¡ 4
3, 17
15, 11
15
Â¢
and
âˆš
330
15 .]
9. Find a linear equation describing the plane perpendicular to the line
of intersection of the planes x + y âˆ’2z = 4 and 3x âˆ’2y + z = 1 and
which passes through (6, 0, 2).
[Ans: 3x + 7y + 5z = 28.]
10. Find the length of the projection of the segment AB on the line L,
where A = (1, 2, 3), B = (5, âˆ’2, 6) and L is the line CD, where
C = (7, 1, 9) and D = (âˆ’1, 5, 8).
[Ans: 17
3 .]
11. Find a linear equation for the plane through A = (3, âˆ’1, 2), perpen-
dicular to the line L joining B = (2, 1, 4) and C = (âˆ’3, âˆ’1, 7). Also
ï¬nd the point of intersection of L and the plane and hence determine
the distance from A to L. [Ans: 5x+2yâˆ’3z = 7,
Â¡ 111
38 , 52
38, 131
38
Â¢
,
q
293
38 .]
12. If P is a point inside the triangle ABC, prove that
P = rA + sB + tC,
where r + s + t = 1 and r > 0, s > 0, t > 0.
13. If B is the point where the perpendicular from A = (6, âˆ’1, 11) meets
the plane 3x + 4y + 5z = 10, ï¬nd B and the distance AB.
[Ans: B =
Â¡ 123
50 , âˆ’286
50 , 255
50
Â¢
and AB =
59
âˆš
50.]

8.8. PROBLEMS
187
14. Prove that the triangle with vertices (âˆ’3, 0, 2), (6, 1, 4), (âˆ’5, 1, 0)
has area 1
2
âˆš
333.
15. Find an equation for the plane through (2, 1, 4), (1, âˆ’1, 2), (4, âˆ’1, 1).
[Ans: 2x âˆ’7y + 6z = 21.]
16. Lines L and M are nonâ€“parallel in 3â€“dimensional space and are given
by equations
P = A + sX,
Q = B + tY.
(i) Prove that there is precisely one pair of points P and Q such that
-
PQ is perpendicular to X and Y .
(ii) Explain why PQ is the shortest distance between lines L and M.
Also prove that
PQ = | (X Ã— Y )Â·
-
AB|
âˆ¥X Ã— Y âˆ¥
.
17. If L is the line through A = (1, 2, 1) and C = (3, âˆ’1, 2), while M
is the line through B = (1, 0, 2) and D = (2, 1, 3), prove that the
shortest distance between L and M equals
13
âˆš
62.
18. Prove that the volume of the tetrahedron formed by four nonâ€“coplanar
points Ai = (xi, yi, zi), 1 â‰¤i â‰¤4, is equal to
1
6 | (
-
A1A2 Ã—
-
A1A3)Â·
-
A1A4|,
which in turn equals the absolute value of the determinant
1
6
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
1
x1
y1
z1
1
x2
y2
z2
1
x3
y3
z3
1
x4
y4
z4
Â¯Â¯Â¯Â¯Â¯Â¯Â¯Â¯
.
19. The points A = (1, 1, 5), B = (2, 2, 1), C = (1, âˆ’2, 2) and D =
(âˆ’2, 1, 2) are the vertices of a tetrahedron. Find the equation of the
line through A perpendicular to the face BCD and the distance of A
from this face. Also ï¬nd the shortest distance between the skew lines
AD and BC.
[Ans: P = (1 + t)(i + j + 5k); 2
âˆš
3; 3.]

188

Chapter 9
FURTHER READING
Matrix theory has many applications to science, mathematics, economics
and engineering.
Some of these applications can be found in the books
[2, 3, 4, 5, 11, 13, 16, 20, 26, 28].
For the numerical side of matrix theory, [6] is recommended. Its bibliography
is also useful as a source of further references.
For applications to:
1. Graph theory, see [7, 13];
2. Coding theory, see [8, 15];
3. Game theory, see [13];
4. Statistics, see [9];
5. Economics, see [10];
6. Biological systems, see [12];
7. Markov nonâ€“negative matrices, see [11, 13, 14, 17];
8. The general equation of the second degree in three variables, see [18];
9. Aï¬ƒne and projective geometry, see [19, 21, 22];
10. Computer graphics, see [23, 24].
189

190

Bibliography
[1] B. Noble. Applied Linear Algebra, 1969. Prentice Hall, NJ.
[2] B. Noble and J.W. Daniel. Applied Linear Algebra, third edition, 1988.
Prentice Hall, NJ.
[3] R.P. Yantis and R.J. Painter. Elementary Matrix Algebra with Appli-
cation, second edition, 1977. Prindle, Weber and Schmidt, Inc. Boston,
Massachusetts.
[4] T.J. Fletcher. Linear Algebra through its Applications, 1972. Van Nos-
trand Reinhold Company, New York.
[5] A.R. Magid. Applied Matrix Models, 1984. John Wiley and Sons, New
York.
[6] D.R. Hill and C.B. Moler. Experiments in Computational Matrix Alge-
bra, 1988. Random House, New York.
[7] N. Deo. Graph Theory with Applications to Engineering and Computer
Science, 1974. Prenticeâ€“Hall, N. J.
[8] V. Pless. Introduction to the Theory of Errorâ€“Correcting Codes, 1982.
John Wiley and Sons, New York.
[9] F.A.
Graybill.
Matrices
with
Applications
in
Statistics,
1983.
Wadsworth, Belmont Ca.
[10] A.C. Chiang. Fundamental Methods of Mathematical Economics, sec-
ond edition, 1974. McGrawâ€“Hill Book Company, New York.
[11] N.J. Pullman. Matrix Theory and its Applications, 1976. Marcel Dekker
Inc. New York.
191

[12] J.M. Geramita and N.J. Pullman. An Introduction to the Application
of Nonnegative Matrices to Biological Systems, 1984. Queenâ€™s Papers
in Pure and Applied Mathematics 68. Queenâ€™s University, Kingston,
Canada.
[13] M. Pearl. Matrix Theory and Finite Mathematics, 1973. McGrawâ€“Hill
Book Company, New York.
[14] J.G. Kemeny and J.L. Snell. Finite Markov Chains, 1967. Van Nostrand
Reinhold, N.J.
[15] E.R. Berlekamp. Algebraic Coding Theory, 1968. McGrawâ€“Hill Book
Company, New York.
[16] G. Strang. Linear Algebra and its Applications, 1988. Harcourt Brace
Jovanovich, San Diego.
[17] H. Minc. Nonnegative Matrices, 1988. John Wiley and Sons, New York.
[18] G.C. Preston and A.R. Lovaglia. Modern Analytic Geometry, 1971.
Harper and Row, New York.
[19] J.A. Murtha and E.R. Willard. Linear Algebra and Geometry, 1969.
Holt, Rinehart and Winston, Inc. New York.
[20] L.A. Pipes. Matrix Methods for Engineering, 1963. Prenticeâ€“Hall, Inc.
N. J.
[21] D. Gans. Transformations and Geometries, 1969. Appletonâ€“Centuryâ€“
Crofts, New York.
[22] J.N. Kapur. Transformation Geometry, 1976. Aï¬ƒliated Eastâ€“West
Press, New Delhi.
[23] G.C. Reid. Postscript Language Tutorial and Cookbook, 1988. Addisonâ€“
Wesley Publishing Company, New York.
[24] D. Hearn and M.P. Baker. Computer Graphics, 1989. Prenticeâ€“Hall,
Inc. N. J.
[25] C.G. Cullen. Linear Algebra with Applications, 1988. Scott, Foresman
and Company, Glenview, Illinois.
[26] R.E. Larson and B.H. Edwards. Elementary Linear Algebra, 1988. D.C.
Heath and Company, Lexington, Massachusetts Toronto.
192

[27] N. Magnenatâ€“Thalman and D. Thalmann. Stateâ€“ofâ€“theâ€“artâ€“in Com-
puter Animation, 1989. Springerâ€“Verlag Tokyo.
[28] W.K. Nicholson. Elementary Linear Algebra, 1990. PWSâ€“Kent, Boston.
193

Index
2 Ã— 2 determinant, 71
algorithm, Gauss-Jordan, 8
angle between vectors, 166
asymptotes, 137
basis, left-to-right algorithm, 62
Cauchy-Schwarz inequality, 159
centroid, 185
column space, 56
complex number, 89
complex number, imaginary num-
ber, 90
complex number, imaginary part,
89
complex number, rationalization,
91
complex number, real, 89
complex number, real part, 89
complex numbers, Apolloniusâ€™ cir-
cle, 100
complex numbers, Argand diagram,
95
complex numbers, argument, 103
complex numbers, complex conju-
gate, 96
complex numbers, complex expo-
nential, 107
complex numbers, complex plane,
95
complex numbers, cross-ratio, 114
complex numbers, De Moivre, 107
complex numbers, lower half plane,
95
complex numbers, modulus, 99
complex numbers, modulus-argument
form, 103
complex numbers, polar represen-
tation, 103
complex numbers, ratio formulae,
100
complex numbers, square root, 92
complex numbers, upper half plane,
95
coordinate axes, 154
coordinate planes, 154
cosine rule, 166
determinant, 38
determinant, cofactor, 76
determinant, diagonal matrix, 74
determinant, Laplace expansion, 73
determinant, lower triangular, 74
determinant, minor, 72
determinant, recursive deï¬nition,
72
determinant, scalar matrix, 74
determinant, Surveyorâ€™s formula,
85
determinant, upper triangular, 74
diï¬€erential equations, 120
direction of a vector, 164
distance, 154
distance to a plane, 184
194

dot product, 131, 156
eigenvalue, 118
eigenvalues, characteristic equation,
118
eigenvector, 118
ellipse, 137
equation, linear, 1
equations, consistent system of, 1,
11
equations, Cramerâ€™s rule, 39
equations, dependent unknowns, 11
equations, homogeneous system of,
16
equations, homogeneous, nonâ€“trivial
solution, 16
equations, homogeneous, trivial so-
lution, 16
equations, inconsistent system of,
1
equations, independent unknowns,
11
equations, system of linear, 1
factor theorem, 95
ï¬eld, 3
ï¬eld, additive inverse, 4
ï¬eld, multiplicative inverse, 4
Gaussâ€™ theorem, 95
hyperbola, 137
imaginary axis, 95
independence, left-to-right test, 59
inversion, 74
Joachimsthal, 163
least squares, 47
least squares, normal equations, 47
least squares, residuals, 47
length of a vector, 131, 157
linear combination, 17
linear dependence, 58
linear equations, Cramerâ€™s rule, 84
linear transformation, 27
linearly independent, 41
mathematical induction, 31
matrices, rowâ€“equivalence of, 7
matrix, 23
matrix, addition, 23
matrix, additive inverse, 24
matrix, adjoint, 78
matrix, augmented, 2
matrix, coeï¬ƒcient, 26
matrix, coeï¬ƒcient , 2
matrix, diagonal, 49
matrix, elementary row, 41
matrix, elementary row operations,
7
matrix, equality, 23
matrix, Gram, 132
matrix, identity, 31
matrix, inverse, 36
matrix, invertible, 36
matrix, Markov, 53
matrix, nonâ€“singular, 36
matrix, nonâ€“singular diagonal, 49
matrix, orthogonal , 130
matrix, power, 31
matrix, product, 25
matrix, proper orthogonal, 130
matrix, reduced rowâ€“echelon form,
6
matrix, row-echelon form, 6
matrix, scalar multiple, 24
matrix, singular, 36
matrix, skewâ€“symmetric, 46
matrix, subtraction, 24
matrix, symmetric, 46
195

matrix, transpose, 45
matrix, unit vectors, 28
matrix, zero, 24
modular addition, 4
modular multiplication, 4
normal form, 180
orthogonal matrix, 116
orthogonal vectors, 168
parabola, 137
parallel lines, 164
parallelogram law, 150
perpendicular vectors, 168
plane, 176
plane through 3 points, 176, 178
position vector, 156
positive octant, 154
projection on a line, 171
rank, 66
real axis, 95
recurrence relations, 32
reï¬‚ection equations, 29
rotation equations, 28
row space, 56
scalar multiplication of vectors, 150
scalar triple product, 173
skew lines, 172
subspace, 55
subspace, basis, 61
subspace, dimension, 63
subspace, generated, 56
subspace, null space, 55
Threeâ€“dimensional space, 154
triangle inequality, 160
unit vectors, 158
vector cross-product, 172
vector equality, 149, 165
vector, column, 27
vector, of constants, 26
vector, of unknowns, 26
vectors, parallel vectors, 164
196


Advances in Intelligent Systems and Computing 665
Georgi Stojanov
Andrea Kulakov    Editors 
ICT 
Innovations 
2016
Cognitive Functions and Next
Generation ICT Systems
www.ebook3000.com

Advances in Intelligent Systems and Computing
Volume 665
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Advances in Intelligent Systems and Computing” contains publications on theory,
applications, and design methods of Intelligent Systems and Intelligent Computing. Virtually
all disciplines such as engineering, natural sciences, computer and information science, ICT,
economics, business, e-commerce, environment, healthcare, life science are covered. The list
of topics spans all the areas of modern intelligent systems and computing.
The publications within “Advances in Intelligent Systems and Computing” are primarily
textbooks and proceedings of important conferences, symposia and congresses. They cover
signiﬁcant recent developments in the ﬁeld, both of a foundational and applicable character.
An important characteristic feature of the series is the short publication time and world-wide
distribution. This permits a rapid and broad dissemination of research results.
Advisory Board
Chairman
Nikhil R. Pal, Indian Statistical Institute, Kolkata, India
e-mail: nikhil@isical.ac.in
Members
Rafael Bello Perez, Universidad Central “Marta Abreu” de Las Villas, Santa Clara, Cuba
e-mail: rbellop@uclv.edu.cu
Emilio S. Corchado, University of Salamanca, Salamanca, Spain
e-mail: escorchado@usal.es
Hani Hagras, University of Essex, Colchester, UK
e-mail: hani@essex.ac.uk
László T. Kóczy, Széchenyi István University, Győr, Hungary
e-mail: koczy@sze.hu
Vladik Kreinovich, University of Texas at El Paso, El Paso, USA
e-mail: vladik@utep.edu
Chin-Teng Lin, National Chiao Tung University, Hsinchu, Taiwan
e-mail: ctlin@mail.nctu.edu.tw
Jie Lu, University of Technology, Sydney, Australia
e-mail: Jie.Lu@uts.edu.au
Patricia Melin, Tijuana Institute of Technology, Tijuana, Mexico
e-mail: epmelin@hafsamx.org
Nadia Nedjah, State University of Rio de Janeiro, Rio de Janeiro, Brazil
e-mail: nadia@eng.uerj.br
Ngoc Thanh Nguyen, Wroclaw University of Technology, Wroclaw, Poland
e-mail: Ngoc-Thanh.Nguyen@pwr.edu.pl
Jun Wang, The Chinese University of Hong Kong, Shatin, Hong Kong
e-mail: jwang@mae.cuhk.edu.hk
More information about this series at http://www.springer.com/series/11156
www.ebook3000.com

Georgi Stojanov
• Andrea Kulakov
Editors
ICT Innovations 2016
Cognitive Functions and Next Generation
ICT Systems
123

Editors
Georgi Stojanov
Paris
France
Andrea Kulakov
Skopje
Macedonia
ISSN 2194-5357
ISSN 2194-5365
(electronic)
Advances in Intelligent Systems and Computing
ISBN 978-3-319-68854-1
ISBN 978-3-319-68855-8
(eBook)
https://doi.org/10.1007/978-3-319-68855-8
Library of Congress Control Number: 2017955723
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
www.ebook3000.com

Preface
The ICT Innovations conference is the primary scientiﬁc gathering event of the
Macedonian
Association
of
Information
and
Communication
Technologies
(ICT-ACT), whose mission is the advancement of ICT technologies. The confer-
ence provides a platform for academics, professionals, and practitioners to interact
and share their research ﬁndings related to basic and applied research in ICT.
The ICT Innovations 2016 conference gathered 186 authors from 19 countries
reporting their scientiﬁc work and novel solutions in ICT. Only 20 papers were
selected for this edition by the International Program Committee. The committee
itself consists of 111 members from 37 countries, chosen for the scientiﬁc excel-
lence in their speciﬁc ﬁelds. Additional 19 short papers can be accessed in the Web
proceedings of the conference. The ﬁrst four papers in this edition are by the invited
speakers at the conference.
ICT Innovations 2016 was held in Ohrid, at Metropol Hotel, in the period of
September 5–7, 2016. The special conference topic was “Cognitive Functions and
Next Generation ICT Systems.” The conference also focused on variety of ICT
ﬁelds: Ambient Intelligence, Artiﬁcial Intelligence, Assistive technologies, Big Data
Analytics, Bioinformatics, Cloud Computing, Cognitive Systems, Collaborative
Environments, Data Mining, Digital Signal & Image Processing, E-health,
Embedded Systems, Emerging Mobile Technologies, Internet of Things, Machine
Learning, Machine Translation, Machine Vision, Natural Language Processing,
Pattern Recognition, Personalized Adaptive Technologies, Personalized Medicine,
Robotics & Automation, Security & Cryptography, Speech Recognition &
Synthesis,
Visualization,
Virtual
Reality,
Wearable
Technologies,
Wireless
Communication & Sensor Networks, and related.
We would like to express sincere gratitude to the invited speakers for their
inspirational talks, to the authors for submitting their works to this conference, and
to the reviewers for sharing their experience during the selection process. Special
thanks go to Bojana Koteska, Monika Simjanoska, Eftim Zdravevski, and Petre
Lameski for their technical support during the conference and for their assistance in
the preparation of the conference proceedings.
September 2016
Georgi Stojanov
Andrea Kulakovv

Organization
ICT Innovations 2016 was organized by the Macedonian Society of Information
and Communication Technologies (ICT-ACT), co-organized together with the
Faculty of Computer Science and Engineering, University Ss. Cyril and Methodius,
Macedonia.
Conference and Program Chairs
Georgi Stojanov
The American University of Paris, France
Andrea Kulakov
University Ss. Cyril and Methodius, Macedonia
Program Committee
Achkoski Jugoslav
Military Academy “General Mihailo Apostolski,”
Macedonia
Ackovska Nevena
University Ss. Cyril and Methodius, Macedonia
Ahmed Gaber Suliman
Mohamed
Universiti Tunku Abdul Rahman, Malaysia
Alti Adel
University of Setif, Algeria
Alzaid Hani
King Abdulaziz City for Science and
Technology, Saudi Arabia
Antovski Ljupcho
University Ss. Cyril and Methodius, Macedonia
Armenski Goce
University Ss. Cyril and Methodius, Macedonia
Astsatryan Hrachya
National Academy of Sciences of Armenia,
Armenia
Baicheva Tsonka
Institute of Mathematics and Informatics,
Bulgaria
Bakeva Verica
University Ss. Cyril and Methodius, Macedonia
Bao Vo Nguyen Quoc
Posts and Telecommunications Institute of
Technology, Vietnam
vii
www.ebook3000.com

Basnarkov Lasko
University Ss. Cyril and Methodius, Macedonia
Bela Genge
Petru Maior University of Tg Mures, Romania
Beus-Dukic Ljerka
University of Westminster, UK
Boggia Gennaro
DEI - Politecnico di Bari, Italy
Borštnar Kljajić Mirjana
University of Maribor, Slovenia
Bosnacki Dragan
Eindhoven University of Technology,
the Netherlands
Burrull Francesc
Universidad Politecnica de Cartagena, Spain
Chatvichienchai Somchai
University of Nagasaki, Japan
Chen Qiu
Kogakuin University, Japan
Chorbev Ivan
University Ss. Cyril and Methodius, Macedonia
Chung Ping-Tsai
Long Island University, USA
Cico Betim
South East European University, Macedonia
Dahiya Deepak
Ansal University Gurgaon, India
Davcev Danco
University Ss. Cyril and Methodius, Macedonia
De Nicola Antonio
ENEA, Italy
Devedzic Goran
University of Kragujevac, Serbia
Dimitrova Vesna
University Ss. Cyril and Methodius, Macedonia
Dimitrovski Ivica
University Ss. Cyril and Methodius, Macedonia
Drlik Martin
Constantine the Philosopher University in Nitra,
Slovakia
Filiposka Sonja
University Ss. Cyril and Methodius, Macedonia
Frasheri Neki
Polytechnic University of Tirana, Albania
Fujinami Kaori
Tokyo University of Agriculture and Technology,
Japan
Gavrilov Andrey
Novosibirsk State Technical University, Russia
Gievska-Krilu Sonja
University Ss. Cyril and Methodius, Macedonia
Gjorgjevik Dejan
University Ss. Cyril and Methodius, Macedonia
Gligoroski Danilo
Norwegian University of Science
and Technology, Norway
Gusev Marjan
University Ss. Cyril and Methodius, Macedonia
Hadzi-Velkov Zoran
University Ss. Cyril and Methodius, Macedonia
Hao Tianyong
Guangdong University of Foreign Studies, China
Hsieh Fu-Shiung
Chaoyang University of Technology, Taiwan
Huraj Ladislav
University of Ss. Cyril and Methodius in Trnava,
Czechia
Ilarri Sergio
University of Zaragoza, Spain
Jakimovski Boro
University Ss. Cyril and Methodius, Macedonia
Janev Valentina
Institute Mihajlo Pupin, Serbia
Jecheva Veselina
Burgas Free University, Bulgaria
Kalajdziski Slobodan
University Ss. Cyril and Methodius, Macedonia
Kaloyanova Kalinka
University of Soﬁa, Bulgaria
Koceski Saso
University Goce Delcev, Macedonia
Kon-Popovska Margita
University Ss. Cyril and Methodius, Macedonia
Kraljevski Ivan
Voice INTER Connect, Germany
viii
Organization

Kulkarni Siddhivinayak
Federation University Australia, Australia
Kumar Singh Brajesh
RBS College, India
Kurti Arianit
Linnaeus University, Sweden
Lazarova-Molnar Sanja
University of Southern Denmark, Denmark
Lim Hwee-San
Universiti Sains Malaysia, Malaysia
Loshkovska Suzana
University Ss. Cyril and Methodius, Macedonia
Madevska Bogdanova Ana
University Ss. Cyril and Methodius, Macedonia
Madzarov Gjorgji
University Ss. Cyril and Methodius, Macedonia
Marina Ninoslav
University St Paul the Apostle, Macedonia
Markovski Smile
University Ss. Cyril and Methodius, Macedonia
Matovski Darko
University of Southampton, UK
Michalak Marcin
Silesian University of Technology, Poland
Mihova Marija
University Ss. Cyril and Methodius, Macedonia
Mileva Aleksandra
University Goce Delcev, Macedonia
Mishev Anastas
University Ss. Cyril and Methodius, Macedonia
Mishkovski Igor
University Ss. Cyril and Methodius, Macedonia
Mitreski Kosta
University Ss. Cyril and Methodius, Macedonia
Mitrevski Pece
University St. Clement of Ohrid, Macedonia
Mohammed Ammar
Cairo University, Egypt
Nancovska Serbec Irena
University of Ljubljana, Slovenia
Nicolau Viorel
Dunarea de Jos University of Galati, Romania
Nicolin Alexandru
National Institute of Physics and Nuclear
Engineering, Romania
Pantano Eleonora
Middlesex University, UK
Patel Shushma
South Bank University, UK
Peachey Paul
University of South Wales, UK
Petkovic Predrag
University of Niš, Serbia
Pirlo Giuseppe
Bari University, Italy
Pop Florin
University Politehnica of Bucharest, Romania
Radevski Vladimir
South East European University, Macedonia
Senthil Kumar A.V.
Hindusthan College of Arts and Science, India
Serova Elena G.
St. Petersburg State Economic University, Russia
Siládi Vladimír
Matej Bel University, Slovakia
Silva Manuel
Instituto Superior de Engenharia do Porto,
Portugal
Sokolova Ana
University of Salzburg, Austria
Spasov Dejan
University Ss. Cyril and Methodius, Macedonia
Stojanovic Igor
University Goce Delcev, Macedonia
Stoyanov Stanimir
University of Plovdiv “Paisii Hilendarski,”
Bulgaria
Subramaniam
Chandrasekaran
Kumaraguru College of Technology, India
Sun Chang-Ai
University of Science and Technology Beijing,
China
Thepade Sudeep
Pune University, India
Organization
ix
www.ebook3000.com

Trajanov Dimitar
University Ss. Cyril and Methodius, Macedonia
Trajkovic Ljiljana
Simon Fraser University, Canada
Trajkovik Vladimir
University Ss. Cyril and Methodius, Macedonia
Treesatayapun Chidentree
Cinvestav-Saltillo, Mexico
Trung Huynh Hieu
Industrial University of Ho Chi Minh City,
Vietnam
Velinov Goran
University Ss. Cyril and Methodius, Macedonia
Vrdoljak Boris
University of Zagreb, Croatia
Wibowo Santoso
CQUniversity, Australia
Xu Shuxiang
University of Tasmania, Australia
Yang Chao-Tung
Tunghai University, Taiwan
Yue Wuyi
Konan University, Japan
Zdravev Zoran
University Goce Delcev, Macedonia
Zdravkova Katerina
University Ss. Cyril and Methodius, Macedonia
Zeng Xiangyan
Fort Valley State University, USA
Organizing Committee
Cveta Martinovska
University Goce Delcev, Macedonia
Gjorgji Madzarov
University Ss. Cyril and Methodius, Macedonia
Elena Vlahu-Georgievska
University St. Clement of Ohrid, Macedonia
Azir Aliu
South East European University, Macedonia
Ustijana Reckoska Shikoska
University St Paul the Apostole, Macedonia
Technical Committee
Bojana Koteska
University Ss. Cyril and Methodius, Macedonia
Monika Simjanoska
University Ss. Cyril and Methodius, Macedonia
Eftim Zdravevski
University Ss. Cyril and Methodius, Macedonia
Petre Lameski
University Ss. Cyril and Methodius, Macedonia
x
Organization

Contents
Invited Keynote Papers
Towards Multimodal Affective Stimulation: Interaction
Between Visual, Auditory and Haptic Modalities . . . . . . . . . . . . . . . . . .
3
Bipin Indurkhya
Cognitive and Emotive Robotics: Artiﬁcial Brain Computing
Cognitive Actions and Emotive Evaluations, Since 1981 . . . . . . . . . . . .
9
Stevo Bozinovski
Metaphors of Creativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
Tony Veale
Socially Intelligent Robots, the Next Generation of Consumer
Robots and the Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
Amit Kumar Pandey
Proceeding Papers
Hand Gesture Recognition Using Deep Convolutional
Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
Gjorgji Strezoski, Dario Stojanovski, Ivica Dimitrovski,
and Gjorgji Madjarov
Computer-Based Statistical Description of Phonetical Balance
for Romanian Utterances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
A. Cocioceanu, T. Ivănoaica, A.I. Nicolin, and M.C. Raportaru
Distributed Private Key Generator for ID-Based Public
Key Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
Pance Ribarski and Ljupcho Antovski
xi
www.ebook3000.com

Relation Between Statistical Tests for Pseudo-Random Number
Generators and Diaphony as a Measure of Uniform Distribution
of Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
Sashe Gjorgjievski, Verica Bakeva, and Vesna Dimitrievska Ristovska
Pattern Recognition of a Digital ECG . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Marjan Gusev, Aleksandar Ristovski, and Ana Guseva
Performance Evaluation of FIR and IIR Filtering of ECG Signals . . . .
103
Aleksandar Milchevski and Marjan Gusev
Inﬂuence of Fuzzy Tolerance Metrics on Classiﬁcation and Regression
Tasks for Fuzzy-Rough Nearest Neighbour Algorithms . . . . . . . . . . . . .
113
Andreja Naumoski, Georgina Mirceva, and Petre Lameski
On the Kalman Filter Approach for Localization of Mobile Robots . . .
123
Kristijan Petrovski, Stole Jovanovski, Miroslav Mirchev,
and Lasko Basnarkov
Evaluation of Automatically Generated Conceptual Database
Model Based on Business Process Model: Controlled Experiment . . . . .
134
Danijela Banjac, Drazen Brdjanin, Goran Banjac, and Slavko Maric
Analysis of Protein Interaction Network for Colorectal Cancer . . . . . . .
146
Zlate Ristovski, Kire Trivodaliev, and Slobodan Kalajdziski
Using Sentiment Analysis of Twitter Data for Determining
Popularity of City Locations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
Nikola Dinkić, Nikola Džaković, Jugoslav Joković, Leonid Stoimenov,
and Aleksandra Đukić
Internet Addiction: Evaluating the Psychometric Properties
of the IAT in Macedonia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
Martin Mihajlov and Aleksandar Stojmenski
Health Care Domain Mobile Reminder for Taking
Prescribed Medications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
Eleonora Milić, Dragan Janković, and Aleksandar Milenković
Enhancing Text-Based Relatedness Measures with Semantic
Web Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
Ana Gjorgjevikj, Riste Stojanov, and Dimitar Trajanov
Power Consumption Analysis of Application Layer Protocols
for the Internet of Things . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
Aleksandar Velinov and Aleksandra Mileva
Improving Medical Cases Retrieval Using an Online Fact Database . . .
203
Ivan Kitanovski, Katarina Trojacanec, Ivica Dimitrovski,
and Suzana Loshkovska
xii
Contents

Improving Scalability of Web Applications by Utilizing
Asynchronous I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
Gjorgji Rankovski and Ivan Chorbev
Relevance Re-ranking Through Proximity Based Term
Frequency Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
S. Sathya Bama, M.S. Irfan Ahmed, and A. Saravanan
Information System for Biosensors Data Exchange in Healthcare . . . . .
230
Monika Simjanoska, Bojana Koteska, Magdalena Kostoska,
Ana Madevska Bogdanova, Nevena Ackovska, and Vladimir Trajkovikj
An Automatic Tracking System for Natural Hazard Events
with Satellite Remote Sensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
Assen Tchorbadjieff
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
Contents
xiii
www.ebook3000.com

Invited Keynote Papers

Towards Multimodal Aﬀective Stimulation:
Interaction Between Visual, Auditory and
Haptic Modalities
Bipin Indurkhya(B)
Jagiellonian University, Cracow, Poland
bipin.indurkhya@uj.edu
1
Aﬀective Computing
Aﬀective computing is concerned with designing and implementing emotionally
intelligent machines. Three major subareas of research in this ﬁeld are: (1) sens-
ing the emotional state of the user, (2) expressing or displaying emotional states
in a robot or an avatar, and (3) manipulating the users emotional state. For
example, Picard [1] has designed a system to infer emotional state of the user
based on their facial expressions. This system is designed to help people with
autism to assess emotional state of the person with whom they are communi-
cating. Once the emotional state of the user is detected, it can also be use as
an interface to aﬀect the state of the system. For example, Lee et al. [2] have
designed an emotionally reactive TV, which uses a soft ball as an emotion-based
interface. The user can squeeze the ball or throw it, and the TV responds accord-
ingly.
The architecture of an aﬀective system is based on extracting the emotional
state of the user based on their facial expressions, prosody of their speech,
gestures, postures, biometric signals such as galvanic skin response, and then
responding to the user in an appropriate manner. For example, a social robot,
on detecting that the user is feeling sad, might try to cheer them up. An emotion-
ally intelligent tutor would detect stress in the student, and will present another
example to illustrate the concept being learnt.
2
Aﬀect Generation
Techniques for manipulating the users emotional state are studied under aﬀect
generation, where the focus is on synthesizing stimuli to induce an emotional
state in the user. This is a new and emerging research area under aﬀective
computing. Aﬀect generation can be used for entertainment, but also to design
systems to support human decision-making. For example, a study by Dazinger
et al. [3] has shown that the judges decisions are aﬀected by when they had
their last meal. This suggests a simple non-technical aﬀect generation system of
providing refreshments at regular intervals.
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 1
www.ebook3000.com

4
B. Indurkhya
There are many techniques for aﬀect generation. We could use images, sounds
including music, or haptic stimulation, which consist of applying vibrations to
some part of the body. Though the use of images and auditory stimuli for aﬀect
generation has been extensively studies, there is little research on how haptic
stimulations aﬀect emotional state of the user.
Some applications have been developed for communicating information
through haptic stimulations. For example, Jayant et al. [4] implemented a
VBraille system for a mobile phone, where the braille pattern is communicated
to a blind user through vibrations. Tomohiro and Sugiyama [5] have developed a
haptic feedback system to help navigate a blind person to their destination. More
recently, such technologies have been deployed in systems for surgical training
(haptic feedback provides the feeling of human tissue) and 3-d computer-aided
design systems (provide the tactile feeling for the object being designed).
Some of these techniques have also been applied for aﬀect generation by hap-
tic stimulation. For instance, Tsetserukou et al. [6] implemented a system that
applies tactile stimulation in the chest area to facilitate aﬀective communication
in Second Life.
Given that visual, auditory, and haptic stimuli can carry aﬀective informa-
tion, a research question is how these modalities interact together, and if we
can use multimodal stimuli to implement aﬀect generation more eﬀectively than
either modality alone.
3
Multisensory Integration of Percepts
Several studies have explored how humans integrate sensory information from
multiple modalities. Most of these studies use the conﬂict paradigm, where con-
ﬂicting inputs are provided in two diﬀerent modalities. For example, the McGurk
aﬀect demonstrates that syllable people hear depends on the mouth movement
they see (McGurk and McDonald [7]). Similarly, the ventriloquist eﬀect illus-
trates that when the visual and auditory stimuli provide conﬂicting information
about the direction of sound, visual stimulus usually dominates (Alais and Burr
[8]). In another study, Shams et al. have found that when a ﬂash appears on the
screen only once but accompanied by two beeps, people usually report seeing
two ﬂashes, thereby demonstrating the dominance of auditory mode over visual.
Indeed, Yokosawa and Kanaya [9] have pointed out that the interaction between
visual and auditory modality is rather complex and depends on the congruency
between the two modalities.
Similarly, the complexity of interaction between the visual and the tactile
stimuli is demonstrated by the rubber-hand eﬀect. The participant watches a
rubber hand being stroked, while having her or his own hand being stroked
simultaneously (but out of sight). It takes only a few seconds for the participant
to attribute body ownership and awareness to the rubber hand to such an extent
that when the rubber hand is hit or stabbed, they feel as if their own hand was
hit or stabbed (Botvinick and Cohen [10]).
Interaction between the audio and tactile stimuli is demonstrated by the
parchment skin illusion (Jousmki and Hari [11]). Participants rub their hands

Towards Multimodal Aﬀective Stimulation
5
together and this sound is recorded. Then they rub their hands together again,
while listening to the playback of the previously recorded sound in which the
high frequency component is ampliﬁed. Most of the participants then report
feeling as if their hands are dry like parchment paper.
Based on various such studies, it has been suggested that multisensory inte-
gration is a complex mechanism involving top-down and bottom-up processes,
and incorporating statistical optimization of the available information (Ernst
and Banks [12]; Tsakiris and Haggard [13]).
Multimodal integration has also been studied for aﬀective eﬀects. Fagel [14]
found that when facial expressions and speech prosody provide conﬂicting emo-
tions, sometimes emotion manifested in one of the channels dominates, but some-
times a new emotion emerges from the interaction. For example, sad video and
happy audio might be interpreted as angry, and happy video and sad audio might
be interpreted as depressed. (See also Li et al. [15])
4
Integration of Visual-Haptic Stimuli
In our past research (Akshita et al. [16]), we did a study on the interaction
of emotional cues presented in visual and haptic modalities. Our goal was to
use the insights from this study to create multimodal aﬀective interfaces that
are more eﬀective than the usual monomodal interface. For example, when you
are viewing an image or a video, your emotional experience could be enhanced
by adding accompanying haptic stimulation via the vibrations from a phone, a
smartwatch, or some other such device.
Our study was carried out in four steps: (1) create an aﬀective visual dataset;
(2) create an aﬀective haptic dataset; (3) understand aﬀective visual-haptic inter-
action; and (4) explore this interaction in terms of haptic features. We used
the cicumplex model of emotion, which represents emotional response in a two-
dimensional space of valence and arousal (Larsen and Diener [17]). Arousal repre-
sents the intensity of emotional response, and valence represents its pleasantness.
So, for example, bordeom has low arousal and negative valence; serenity has low
arousal and positive valence; happiness has high arousal and positive valance;
and anger has high arousal and negative valence.
In the ﬁrst step, we used the International Aﬀective Picture System dataset
called IAPS (Lang et al. [18]), which is a well-cited dataset in emotion research
containing 1192 images with diﬀerent valence and arousal ratings. We veriﬁed the
validity of the IAPS for Indian participants by taking a representative sample of
120 images, and measuring their valence and arousal ratings with eight Indian
participants. We found no statistical diﬀerence between our measured ratings
and those in the IAPS dataset, which meant that we can assume that the IAPS
ratings apply to Indian participants as well.
In the second step, we constructed a haptic dataset along the four dimensions
of intensity, frequency, waveform and rhythm. First we sampled each dimension
based on the limits of human perception and the haptic device, which was the
ViviTouchTM actuator with a frequency range of 0 to 200 Hz. This generated a
www.ebook3000.com

6
B. Indurkhya
set of 390 stimuli. Next, we pruned the dataset by performing a discrimination
test to ensure that the participants can distinguish between pairs of neighbouring
stimuli. This gave us (10 rhythm patterns) × (3 waveforms: sinusoidal, sawtooth,
pulse) × (3 frequencies: 55 Hz, 110 Hz, 165 Hz) × (2 intensity levels: 20 dB, 35 dB)
= 180 stimuli. We presented these stimuli to 16 participants in two sessions, and
asked them to assign each stimulus to one of the nine emotional groupings based
on its valence and arousal. This gave us our haptic dataset of 180 stimuli, each
with its valence and arounsal ratings.
In the third step, we studied how visual and haptic modalities interact
together. For this, we created four sets of supporting stimuli, where visual and
hapic stimuli cohere with each other: so low valance low valance, low arousal
low arousal, high valence high valence, and high arousal high arousal. Similarly,
we also created four sets of conﬂicting stimuli: low valence high valence, high
valence low valence, low arousal high arousal, high arousal low arousal. In each
set, there were ﬁve stimuli. Nineteen participants took part in this study.
In analyzing the results, we compared the aﬀective response of each com-
bined visual-haptic stimulus with the responses of the visual-only and haptic-
only situation. For the case of valence-supporting set, we found no signiﬁcant
diﬀerence among visual only, haptic only and visual-haptic combined situations.
For valence-conﬂicting set, we found that visual-haptic ratings were similar to
visual-only ratings. For the arousal, we found that for both support and conﬂict
situations, the visual-haptic arousal was signiﬁcantly higher than the visual-
only arousal. This shows that adding haptic stimulation enhances the arousal of
a visual stimulus.
In the fourth step, we explored in more detail which features of the haptic
stimuli have the most inﬂuence on the arousal value. We experimented with 24
participants in two groups. We found that waveform and rhythm do not have
any signiﬁcant eﬀect on the increase of the arousal value, but intensity and
frequency do aﬀect the arousal value signiﬁcantly. This suggests that to enhance
the arousal value of a visual stimulus, we need to focus on the frequency and
intensity of the accompanying haptic stimulation.
5
Integration of Audio-Haptic Stimuli
In a related work, we conducted experiments to study the interaction between
auditory and haptic modalities. For this study, we used the International Aﬀec-
tive Digitized Sounds (IADS) system (Bradley and Lang 2007), which is a
dataset containing 167 diﬀerent environmental sounds with associated valence
and arousal ratings. Nineteen participants took part in this experiment. Similar
to the visual-haptic case, we created four sets of supporting stimuli, and four
sets of conﬂicting stimuli and measured the aﬀective response for each stimulus.
Then we compared the aﬀective response of each combined audio-haptic stim-
ulus with the responses of the auditory-only and haptic-only situation. We found
that in the conﬂicting case, the auditory-haptic valence is similar to auditory-
only valence. In the supporting case, there is a more complex interaction:

Towards Multimodal Aﬀective Stimulation
7
combining high-valence audio and high-valence haptic stimuli results in an
auditory-haptic stimulus that has a lower valence, which seems counterintu-
itive and needs further investigation. For arousal, in most cases, the presence
of haptic stimuli increases the arousal of the auditory stimuli. However, in case
of conﬂicting arousal, when the auditory arousal is high, there is no signiﬁcant
diﬀerence between auditory-haptic and auditory-only arousal.
6
Conclusion and Future Work
Our preliminary research shows that adding haptic stimulation increases the
arousal response of the accompanying visual or auditory stimulus. However, this
needs to be investigated further by using other haptic devices, and other parts
of the body like the wrist (for smartwatches). Haptics is a relatively unexplored
modality for commuincating information and emotions. But several researchers
are now studying characteristics of haptic processing, and experimenting with
novel devices and interfaces. We can expect many innovations in this area in the
near future.
Another interesting research direction is to explore if the aﬀective value of
a visual or auditory stimulus can be predicted from its low-level features. For
example, if we look at an image from the IAPS dataset, is it possible to predict
its aﬀective value by processing the image and extracting various low-level (and
high-level as well) features from it. If such techniques can be developed and
validated, then we will have a way to quickly assign an aﬀective value to an
image, and generate an appropriate accompanying haptic stimulation.
It would also be useful to explore techniques that dampen the arousal
response, for they may be used to lessen the eﬀect of viewing disturbing and
traumatic images.
As we were not able to ﬁnd any way to inﬂuence the valance response of a
visual or auditory stimulus, further studies are needed to explore other ways to
manipulate valence.
Our brain naturally integrates inputs from diﬀerent modalities to construct
an object representation that we consider as real. Studying these integration
mechanisms and incorporating them into virtual reality and mixed reality sys-
tems remains a challenge for the future technology.
References
1. Picard, R.W.: Future aﬀective technology for autism and emotion communication.
Philos. Trans. Roy. Soc. B. Biol. Sci. 364(1535), 3575–3584 (2009)
2. Lee, C.H.J., Chang, C., Chung, H., Dickie, C., Selker, T.: Emotionally reactive
television. In: Proceedings of the 12th International Conference on Intelligent User
Interfaces, pp. 329–332. ACM (2007)
3. Danziger, S., Levav, J., Avnaim-Pesso, L.: Extraneous factors in judicial decisions.
Proc. Nat. Acad. Sci. 108(17), 6889–6892 (2011)
www.ebook3000.com

8
B. Indurkhya
4. Jayant, C., Acuario, C., Johnson, W., Hollier, J., Ladner, R.: V-braille: haptic
braille perception using a touch-screen and vibration on mobile phones. In: Pro-
ceedings of the 12th international ACM SIGACCESS conference on Computers
and accessibility, pp. 295–296. ACM (2010)
5. Amemiya, T., Sugiyama, H.: Design of a haptic direction indicator for visually
impaired people in emergency situations. Comput. Helping People Spec. Needs pp.
1141–1144 (2008)
6. Tsetserukou, D., Neviarouskaya, A., Prendinger, H., Ishizuka, M., Tachi, S.:
iFeel IM: innovative real-time communication system with rich emotional and hap-
tic channels. In: CHI 2010 Extended Abstracts on Human Factors in Computing
Systems, pp. 3031–3036. ACM (2010)
7. McGurk, H., MacDonald, J.: Hearing lips and seeing voices (1976)
8. Alais, D., Burr, D.: The ventriloquist eﬀect results from near-optimal bimodal
integration. Curr. Biol. 14(3), 257–262 (2004)
9. Yokosawa, K., Kanaya, S.: Ventriloquism and audio-visual integration of voice and
face. Brain and Nerve= Shinkei Kenkyu no Shinpo 64(7), 771–777 (2012)
10. Botvinick, M., Cohen, J.: Rubber hands’ feel’touch that eyes see. Nature
391(6669), 756 (1998)
11. Jousm¨aki, V., Hari, R.: Parchment-skin illusion: sound-biased touch. Curr. Biol.
8(6), R190–R191 (1998)
12. Ernst, M.O., Banks, M.S.: Humans integrate visual and haptic information in a
statistically optimal fashion. Nature 415(6870), 429–433 (2002)
13. Tsakiris, M., Haggard, P.: The rubber hand illusion revisited: visuotactile inte-
gration and self-attribution. J. Exp. Psychol. Hum. Percept. Perform. 31(1), 80
(2005)
14. Fagel, S.: Emotional McGurk eﬀect. In: Proceedings of the International Conference
on Speech Prosody, Dresden, vol. 1. Citeseer (2006)
15. Li, A., Dang, J.: A cross-cultural investigation on emotion expression under vocal
and facial conﬂict-also an observation on emotional McGurk eﬀect. In: Interna-
tional Symposium on Biomechanical and Physiological Modeling and Speech Sci-
ence, Kanazawa, I, pp. 37–50 (2009)
16. Alagarai Sampath, H., Indurkhya, B., Lee, E., Bae, Y., et al.: Towards multimodal
aﬀective feedback: interaction between visual and haptic modalities. In: Proceed-
ings of the 33rd Annual ACM Conference on Human Factors in Computing Sys-
tems, pp. 2043–2052. ACM (2015)
17. Larsen, R.J., Diener, E.: Promises and problems with the circumplex model of
emotion (1992)
18. Lang, P.J., Bradley, M.M., Cuthbert, B.N.: International aﬀective picture system
(IAPS): technical manual and aﬀective ratings. NIMH Center for the Study of
Emotion and Attention, pp. 39–58 (1997)

Cognitive and Emotive Robotics: Artiﬁcial
Brain Computing Cognitive Actions
and Emotive Evaluations, Since 1981
Stevo Bozinovski1,2(&)
1 Mathematics and Computer Science Department,
South Carolina State University, Orangeburg, USA
sbozinovski@scsu.edu
2 Faculty of Computer Science and Engineering,
Sts Cyril and Methodius University, Skopje, Macedonia
Abstract. This plenary keynote paper marks 35-th anniversary of description
of the ﬁrst artiﬁcial brain, a neural network which in addition of computing
cognitive actions, computes emotive evaluations of the consequence of those
actions. It was designed in 1981, as part of an effort within the Adaptive Net-
works Group of the COINS Department of University of Massachusetts in
Amherst to solve the problems of (1) designing a delayed reinforcement learning
mechanism for artiﬁcial neural networks and (2) designing a self learning system
which will learn without external reinforcement of any kind. This paper
describes steps which led toward solution of those problems. The proposed
artiﬁcial brain, named Crossbar Adaptive Array, can be viewed as a model of
cognition-emotion interaction in biological brains, and can be used in building
brains for cognitive and emotive robotics.
Keywords: Emotional robots  Computer simulation of the brain  Cognition-
emotion interaction  Secondary reinforcement  Emotion backpropagation 
Adaptive Networks Group  Crossbar Adaptive Array  Artiﬁcial brain
1
Introduction
Contemporary robotics devotes a signiﬁcant attention to emotional expressions pro-
duced by robots. There are already robots on the market such as Nao, Pepper, and
Romeo [1], which are able to engage in emotion colored conversation with humans. It is
important step forward in Artiﬁcial Intelligence (AI), which started by neglecting
importance of emotion and focusing mostly on cognition. There is also an effort of
simulation of brain functioning [2], including real connections between brain neurons
[3]. Those efforts need a model of the brain functioning which includes emotions among
other activities of the brain.
The ﬁrst AI paper which mentioned interaction with cognition, emotion, and
motivation was written in 1967 by Simon [4]. The ﬁrst paper that proposes a working
model, both in mathematics and simulation, including emotions and cognition was
written in 1981 [5–7]. The paper introduced a neural network named Crossbar Adaptive
Array (CAA), which can compute in crossbar fashion, in the same memory structure,
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_2
www.ebook3000.com

both cognitive decisions about actions and behaviors, and feelings from consequences
of those actions. CAA was the ﬁrst artiﬁcial brain.
Now, on the 35-th anniversary, this paper describes and comments on that work,
the context which led to the work, as well as consequences, follow ups of that work.
After this introduction, the paper starts with the pioneering events of development of
Artiﬁcial Intelligence in Macedonia and knowledge on abstract automata graphs as well
as self-organizing systems, which provided ground for designing the CAA. The third
chapter describes people and events, who inﬂuenced the work described here to be
carried out within the Adaptive Networks Group (ANW) of the University of Mas-
sachusetts at Amherst. The fourth chapter describes the 1980/81 ANW group. The ﬁfth
chapter describes the research challenges posed in front of ANW group in 1981. The
sixth chapter describes the two approaches and the two architectures developed inside
ANW group to solve the challenges. The seventh chapter points out the 1981 results.
The eight chapter comments on follow-up works related to the CAA architecture,
carried out by members of the 1981 ANW group.
2
Abstract Automata, Chess Playing Programs,
Neural Networks
In 1968, the ﬁrst paper written in Macedonian language [8], started development of
Computer Sciences, including Artiﬁcial Intelligence in Macedonia [9]. The paper was
presented as a lecture at the Seminar on Cybernetics organized by professor Gorgi
Cupona. It was based on the third chapter of the book of Glushkov [10, 11].
The concept of automaton is related to artiﬁcial neural networks from the early
works on automata theory (e.g. [12]). The attached concept of state and automaton
graph (e.g. [13]) makes it basis for Computer Sciences, including Artiﬁcial Intelligence.
The concept of state apace is one of the bases for disembodied intelligence while the
concept of agent (automaton interacting with its environment) is crucial for embodied
AI. Using automata to model Turing Machines, agents, and neural networks is
described for example in [9].
In 1981, development of CAA architecture was based on introducing emotionally
colored states [5–7] in the representation of the problem, solution of which was the
proposed CAA architecture. Example of emotionally colored graph of an automaton is
given in Fig. 1.
λ
1`
1 
1
0
0
 1
0 
0 
0,1
0,1
Fig. 1. Emotional graph of an automaton.
10
S. Bozinovski

This automaton recognizes the language {1011}, a problem stated in the lectures on
theory of compotation given by Moll and Arbib in 1980 and 1981 at the COINS
department at UMass/Amherst. The author of this text attended the lectures and learned
from lecture notes which later become a book [14]. In the lectures and the book the
goal (accepting) state was denoted by a double circle which is usual in the theory of
automata and formal languages. In Fig. 1 the accepting (goal) state is emotionally
colored with emoticon
, while the “no-hope” (trap) state which in [14] has no special
denotation, here is represented by emoticon
. Now the author of this book is teaching
theory of computation at South Carolina State University, and emotionally colored
graphs are a tool in teaching formal language acceptors and other problems related to
automata and search spaces.
In mentioning the pioneering steps in development of AI in Macedonia, let us
mention that the ﬁrst AI computer program by a Macedonian author was written by the
author of this text and it was a chess endgame with two kings and a rock. It was written
in Fortran for IBM 1130 computer, in 1969. It had a main program named SAH which
was doing interaction with a human player over the console typewriter. On demand by
the human (call DATSW), it was drawing the current position on the system plotter.
There were three subroutines: POTEZ subroutine evaluated the move of the human
player for correctness, KODER encoded the move written in standard chess notation
(e.g. KE6) into internal representation and vice-versa, and POZIC analyzed the position
and decided the program move. There was one function, MATIR, which decided
whether the position is a checkmate.
The ﬁrst neural network program by a Macedonian author was written 1970 [15]
and was simulating a perceptron according to the book [11]. The ﬁrst paper published
in a journal was about a neural network simulating Pavlov’s classical conditioning.
[16]. The ﬁrst paper presenting a realization of a perceptron using digital circuits
appeared in 1976 [17] showing realization of a perceptron by digital circuits, by an
array or counters. By 1978, a perceptron was simulated which was able to learn to
recognize 26 letter of Latin alphabet.
3
Minsky, Klopf, Arbib
Here we will mention three people relevant for the 1981 event described in this paper.
In 1969 Marvin Minsky and Seymour Papert wrote a book about perceptrons [18].
They mentioned some issues related to ability of perceptrons to recognize patterns.
Minsky was very inﬂuential, partly because he was a MIT professor, partly due to his
book on computation [19], where the treated abstract automata as neural elements, and
partly because of his inﬂuential papers. The book was interpreted by some people that it
says basically that the neural networks approach toward artiﬁcial intelligence is not a
promising one. The result was that NSF stopped funding artiﬁcial neural networks
(ANN) research, and in USA at the end of 1979 there were no groups working on ANN
ﬁnanced by NSF.
Klopf worked with Avionics Laboratory, Wright Patterson Air Force Base
(WPAFB) in Dayton, Ohio [20]. He proposed that Air Force should ﬁnance a research
on goal seeking systems from goal seeking components. In 1977 a group was formed at
Cognitive and Emotive Robotics: Artiﬁcial Brain
11
www.ebook3000.com

the Computer and Information Science (COINS) Department at the University of
Massachusetts at Amherst, which will work on the topic.
Arbib wrote a book named “Brain, Machine, and Mathematics” [21], which was
published also in Russian language in 1968 [22]. In 1969 he published a book on
abstract automata [23]. The author of this text became familiar with the books of
Arbib’s [22] and Minsky’s [18] as a student of Electrical Engineering Department in
Zagreb, being regular visitor of the Foreign Language Bookstore (popular name
“Russian Bookstore”). In 1978 the author of this text met Arbib at the symposium
Informatica 1978 in Bled. After that, obtaining a Fulbright scholarship, the author of
this text went to University of Massachusetts in Amherst. It is interesting that in the
time when in USA research on artiﬁcial neural networks did not receive federal
funding, a researcher from Macedonia joined a group funded by US military. It was
result of the momentum gained in 1968 when the author of this text for the ﬁrst time
has seen the chapter about self-organizing system in the Glushkov’s book [11].
4
Adaptive Networks Group (ANW), 1980–1981
The members of Adaptive Networks Group in 1980–1981 were Klopf (WPAFB Project
Ofﬁcer), Nico Spinelli (Principal Investigator), Arbib (advisor) Barto (postdoc), and the
graduate students Sutton, Anderson, Jack Porterﬁeld, Ted Selker, and Bozinovski. The
group worked on neural networks with focus on reinforcement learning [24, 25]. By
1981 two very important papers were published. One [26] was about establishing
relation
between
Rescorla-Wagner
theory
of
classical
conditioning
[27]
and
Widrow-Hoff optimization procedure [28]. The other [29] was a great demonstration of
possibility offered by reinforcement learning, a solution of a landmark learning task.
5
ANW Challenges, 1981
In 1981 Sutton pointed out a challenge of delayed reinforcement learning which was
never solved with a neural network. The problem was how to build a neural network
agent which will be able to learn in cases when reinforcement does not appear after
every action of the agent, but rather occasionally, for example at the goal state. There
are many examples of such problem, one of them being the game of chess where a
player performs a move M but it will not receive evaluation (positive or negative
reinforcement) until many moves later; when by backpropagation reasoning it might be
possible to say that the move M was good or bad. Sutton stated an instance of such a
problem, a maze learning instance, where a path should be learned in a maze, but
reinforcement (e.g. food) is only available at the goal state of the maze. The other
instance was pointed out by Anderson, and it was the well known pole balancing task.
The task can be described as: given dynamics of a cart-pole system, and using input
force, balance the pole such that it remains in vertical position. So the challenge was to
build a neural network which will be able to learn in cases of delayed reinforcement.
12
S. Bozinovski

In 1981 Bozinovski pointed the challenge of self-learning, learning without any
external reinforcement, neither advice giving teacher nor a scalar (punish/reward) type
of reinforcement. He also took a challenge to build such a system.
6
Two Approaches Toward the Challenges
There were two approaches and two architectures developed to solve the delayed
reinforcement learning problem. The ﬁrst one was Actor/Critic (A/C) architecture by
Barto and Sutton, experiments for which were carried out by Sutton; the approach was
based on external reinforcement. The second one was Crossbar Adaptive Array
(CAA) architecture by Bozinovski who also carried out the experimental work; the
approach was self learning, learning without any external reinforcement. For both
approaches, the dynamics of cart-pole system was carried out by Anderson, following
the description given in [30]. Figure 2 show both approach architectures.
The challenged graphs on which the A/C and CAA architectures were tested are
shown in Fig. 3. Example of a challenge maze for the A/C architecture is shown in
Fig. 3a. Sutton has chosen as challenge mazes from animal learning, theory, with one
positively reinforcing state, the goal state. Here the goal state is represented as double
a. A/C architecture
b. CAA architecture 
Fig. 2. The architectures used to solve the delayed reinforcement learning problem.
Cognitive and Emotive Robotics: Artiﬁcial Brain
13
www.ebook3000.com

circled, as in automata theory. Figure 3b shows a maze which was challenge for the
CAA architecture. Bozinovski has chosen to model the mazes from the 1980
VAX/VMS computer game “Dungeons and dragons”. Figure 3b shows such a maze
which is simpliﬁcation of the dungeon named Telengard. It had one goal state but also
three states that should be learned to avoid, and one teleportation state going through
which CAA also felt positive reinforcement although it is not a goal state. The
non-neutral states were emotionally colored. From the very beginning, emotionally
colored states were part of the CAA design.
6.1
Crucial Concept of Actor/Critic Architecture: Prediction
The basic idea of the A/C architecture was the concept of prediction of reinforcement.
A predictor module, named adaptive critic element (ACE) will be built, which will be
able to receive a primary reinforcement r from the environment, either occasionally or
only once, and will be able to supply an internal reinforcement r^ to the element ASE
(associative search element) after every action performed by ASE. Basically ASE was a
standard reinforcement learning module which receives reinforcement after each
action, where reinforcement is produced by a system other than environment, in this
case the system ACE. The search is achieved by introducing Gaussian noise. Learning
rules are high order second order and even higher with introduced eligibility traces. The
two memory structures are of same dimension. Inputs x to both ACE and ASE ele-
ments have eligibility traces. The equations are rather complex and of higher than the
ﬁrst order. The detailed description is given in [31].
6.2
Crucial Concept of Crossbar Adaptive Array Architecture: Emotion
As Fig. 2b shows, the CAA architecture has a system of state evaluation. That concept
introduced 1981 [7] was a new concept in reinforcement learning research. At that time it
was not known to the ANW Group that such a concept was already used in Dynamic
Programming [32]. Eight years later, in 1989, Dynamic Programming will be explicitly
a. A/C challenge 
b. CAA challenge
Fig. 3. Challenging environments for delayed reinforcement learning architectures
14
S. Bozinovski

related to reinforcement learning by Watkins [33]. However by introducing state eval-
uation concept the 1981 CAA implicitly related reinforcement learning to Dynamic
Programming.
The 1981 CAA also related the state evaluation to feeling and emotion. Evaluation
of an internal consequence state was felt by the CAA agent. Introducing a single
memory structure which computes both the learned actions and the feelings due to the
consequence of those actions, made the CAA architecture the ﬁrst artiﬁcial brain. It
modeled the cortex (cognitive) part as well as limbic system (emotive) part of the brain.
As opposite the A/C architecture contained only the cognitive part of the brain
(computing learned action as well as prediction of that action).
The search process is deﬁned by a variable named searching strategy r. It can be a
random walk, in which case is modeled as a uniform (rather than Gaussian) distribu-
tion. It models curiosity of the system.
There is only one memory structure of the same size as one of the A/C memory
structures. It computes in a crossbar fashion both actions (behaviors) and emotional
evaluations. The learning process is described with a single, ﬁrst order equation.
waj ¼ waj þ vk
ð1:1Þ
which, written as difference equation is
waj tð Þ ¼ waj t  1
ð
Þ þ xj t  1
ð
Þya t  1
ð
Þv tð Þ
ð1:2Þ
Here waj is memory value associated with performing action a in situation j, and vk
is emotion evaluation obtained from the consequence situation k.
The CAA crossbar learning procedure is given by a pseudocode describing a step
of the procedure
(1) state j: compute action a biasing on w*j, sent it to the environment
i: the environment gives back the state k
(2) state k: compute emotion vk using wk. then compute overall emotion v
(3) state j: increment waj using v (backpropagate emotion and learn)
(4) change state: j = k; goto 1
The pseudocode way of description given in CAA later become a became a
standard way of describing a learning procedure in a reinforcement learning research.
6.3
CAA Secondary Reinforcement Learning (Emotion
Backpropagation)
The emotion backpropagation process used in CAA follows the concept of secondary
reinforcement, known in psychology (e.g. [34]): A primary external reinforcement
r creates an internal state s in a brain, after which the state s becomes reinforcement
itself (secondary reinforcer). A brain then tends to achieve state s expecting from there
to reach reinforcement r. It is actually an emotion backpropagation mechanism.
As Fig. 4 shows, a state, j, performs an action, a. It will transition the system to the
emotionally colored state k, and that emotion will be backpropagated to the action a. The
Cognitive and Emotive Robotics: Artiﬁcial Brain
15
www.ebook3000.com

action will be updated in value such that the chance of being chosen next time is
increased. Now the action a is motivationally colored. It contains motivation (or
intention) to be chosen, because it will bring the agent to a positive emotion. This
backpropagation chain of emotionally colored states and motivation colored actions is
the basis of the CAA secondary reinforcement learning an emotion backpropagation. Let
us note that CAA offered a ﬁrst model which mathematically connects all the three
general functions of the brain: the cognitive (knowing and understanding), the conative
(intentions, dispositions), and the emotive (emotions) e.g. [35].
Fig. 4. The CAA secondary reinforcement (emotion backpropagation) mechanism.
16
S. Bozinovski

6.4
CAA Self-Learning Solution
The other ANW challenge, proof of possibility of a self learning system was also
solved by the CAA approach. The solution was in introducing a genetic environment in
addition to behavioral environment, where from the CAA architecture inherits, by way
of genetic strings (chromosomes), the initial, intrinsic emotions. That was something
new in neural network research. Example of such intrinsic emotion is “if you feel cold,
that is dangerous situation, try to avoid it”. The concept of “cold” is related to “danger”
by genetics, it is not learned, because it is basic survival need. Once initial emotions are
deﬁned by genetics, then using the emotion backpropagation mechanism, an agent is
able to learn a behavioral policy in an environment. The CAA also is able to exporting
the memory after learning, as a chromosome, following the principle of Lamarckian
evolution.
7
1981 Results
In 1981 the only architecture that solved the delayed reinforcement learning challenge
was the CAA architecture. It also solved the self learning challenge. The solution was
presented in front of ANW group [5] and as overhead presentation for the COINS
department under title “Adaptive Arrays”. Separate presentation was given on pole
balancing problem [36] in which it was also reported on parallel programming with
inter-process communication by which the problem was solved: two processes CAA
(written by Bozinovski) and ENVIRONMENT which contained the cart-pole dynamics
(written by Anderson) were running on separate terminals and interchanging infor-
mation
via
VAX/VMS
system
mailboxes,
The
system
software
routine
for
inter-process communication was written by Bob Heller. That was a pioneering work
on application of parallel programming in Artiﬁcial Intelligence.
The CAA architecture also solved the problem of self-learning, i.e. learning without
external reinforcement. For that, it introduced genetic environment and intrinsic
emotions that are genetically given, as well as genetic strings, chromosomes, which are
inherited from genetic environment to be primary reinforcers.
8
After 1981: Relevant Works
The CAA architecture was described in 1982 in an abstract [6] and as a paper [7]. The
A/C approach showed the solution of pole balancing problem in 1983 [31]; it is
interesting that in that paper the terms “delayed reinforcement learning” and “sec-
ondary reinforcement learning” were not used, although those were key terms of the
challenge of the ANW group in 1981.
In 1986 a book appeared [37] after which it was clear that the neural network
research should become part of the mainstream Machine Learning and Artiﬁcial
Intelligence. However in 1987 there was a European meeting on Machine Learning
where there was still a view that neural nets are not part of mainstream Machine
Leaning. Some works of the participants were published, and some were presented as
Cognitive and Emotive Robotics: Artiﬁcial Brain
17
www.ebook3000.com

position papers with no publication. Bozinovski presented a position paper of CAA
research, in front of the audience in which was Watkins, who had a presentation on
search and cross-validation and his paper was published [38].
Two years later in 1989 Watkins presented method of learning from delayed
rewards named Q-learning method [33]. The work showed solution of maze learning
instance of the delayed reinforcement learning, under name “learning with delayed
rewards”. In 1990 the work was shown together with Suttoin and Barto [39]. The eight
years earlier CAA memory was reintroduced, and named Q-table, where Q-elements Q
(a,s) and exactly the same and with the same meaning in a crossbar CAA memory. The
learning rule, named Q-learning was ﬁrst order, conﬁrming the CAA solution, as
difference from higher order learning rules used in A/C architecture. Relation between
Q learning and CAA crossbar learning is evaluated in two ways. One is that CAA
learning is a fore-runner of Q learning [40]. We argue that Q learning is an eight years
later extension of CAA crossbar learning with addition of a forgetting parameter.
Later the CAA approach was developed into a Consequence Driven Systems
theory, which models, among other concepts, personality, motivation, emotion,
curiosity, and learning (e.g. [41]).
The self learning approach by CAA in 1981 was reintroduced in 2004 [42], where
the concept of intrinsic motivation was used rather than the concept of emotion. The
“classical RL system” [43] was redrawn to represent a self-learning system with no
external reinforcement, as shown 22 years earlier [7].
9
Conclusion
In the time of increased interest of robotics industry toward emotion-showing robots,
we point out that robotics has still a challenge in dealing with emotion-cognition
interaction which is a phenomenon in a human brains. This paper points toward the ﬁrst
operational, mathematical and programming model of emotion-cognition interaction,
which was proposed in 1981 as part of research carried out by ANW group of
University of Massachusetts at Amherst. The paper also points toward some follow ups
of the efforts invested by the ANW group in 1981.
Acknowledgement. The author was able to study the Glushkov’s book due to 1968 ideas of
Gorgi Cupona for development of computer sciences in Macedonia. The author would like to
thank Michael Arbib and Nico Spinelli who enabled working with the 1980/81 ANW group. The
author would also like to thank Andrew Barto for allowing the author to join the Adaptive
Networks Group again in 1995/96.
References
1. Softbank Robotics: Robots. http://www.ald.softbankrobotics.com
2. Furber, S.: To build a brain. IEEE Spectr. 49(8), 45–49 (2012)
3. Strickland, E.: A wiring of the brain. IEEE Spectr. 50(5), 12–14 (2013)
4. Simon, H.: Motivational and emotional controls of cognition. Psychol. Rev. 74, 29–39
(1967)
18
S. Bozinovski

5. Bozinovski, S.: Crossbar Adaptive Array. ANW Group, Computer and Information Science
Department, University of Massachusetts at Amherst, 25 November 1981
6. Bozinovski, S.: A self-learning system using secondary reinforcement. In: Abstracts of the
Sixth European Meeting on Cybernetics and Systems Research - 1982, Abstract 87. Austrian
Society of Cybernetic Studies, Vienna (1982)
7. Bozinovski, S.: A self-learning system using secondary reinforcement. In: Trappl, R. (ed.)
Cybernetics and Systems Research, pp. 397–402. North-Holland, New York (1982)
8. Bozinovski, S.: Abstract Automata. In: Cupona, G. (ed.) First Seminar on Cybernetics,
Mathematical Institute with Numeric Center, Skopje (1968) (in Macedonian)
9. Bozinovski, S.: Professor Cupona and the ﬁrst lecture on abstract automata in Macedonia.
Math. Maced. 8, 79–94 (2010)
10. Glushkov, V.: Introduction to Cybernetics. Academy of Sciences of Ukrainian SSR, Kiev
(1964). (in Russian)
11. Glushkov, V.: Introduction to Cybernetics. Belgrade University, Belgrade (1967). (Trans-
lated to Serbian)
12. Kleene, S.: Representation of events in neural nets and abstract automata. In: Shannon, C.,
McCarthy, J. (eds.) Automata Studies, pp. 3–42. Princeton University Press, Princeton
(1956)
13. Moore, E.: Gedanken-experiments on sequential machines. In: Shannon, C., McCarthy,
J. (eds.) Automata Studies, pp. 129–156. Princeton University Press, Princeton (1956)
14. Arbib, M.A., Kfoury, A.J., Moll, R.N.: A Basis of Theoretical Computer Science. Springer,
New York (1981)
15. Bozinovski, S.: Perceptron problematics: teaching in pattern recognition. Paper for the
First-of-May student research competition. University of Zagreb (1970). (in Croatian)
16. Bozinovski, S.: Introduction to psychocybernetics. Rev. Psychol. 5, 115–128 (1975).
Zagreb. (in Croatian)
17. Bozinovski, S., Fulgosi, A.: The inﬂuence of similarity and the transfer of training on the
perceptron training. In: Proceedings of Symposium Informatica, 3-12-1-5, Bled (1976). (in
Croatian)
18. Minsky, M., Papert, S.: Perceptrons: an Introduction to Computational Geometry. MIT
Press, Cambridge (1969)
19. Minsky, M.: Computation: Finite and Inﬁnite Machines. Prentice-Hall, Upper Saddle River
(1967)
20. Klopf, H.: Brain functioning and adaptive systems. Air Force Technical Report, AFCRL
72-0164 (1972)
21. Arbib, M.: Brains, Machines, and Mathematics. McGraw-Hill, New York (1965)
22. Arbib, M.: Brain, Machine, and Mathematics. Nauka, Moskow (1968). (in Russian)
23. Arbib, M.: Theories of Abstract Automata. Prentice-Hall, Upper Saddle River (1969)
24. Mendel, J., McLaren, R.: Reinforcement learning control and pattern recognition systems.
In: Mendel, J., Fu, K.-S. (eds.) Learning and Pattern Recognition Systems, pp. 287–318.
Academic Press, Gurugram (1970)
25. Widrow, B., Gupta, N., Maitra, S.: Punish/Reward: learning with a critic in adaptive
threshold systems. IEEE Trans. Syst. Man Cybern. 3(5), 455–465 (1973)
26. Sutton, R., Barto, A.: Toward a modern theory of adaptive networks: expectation and
prediction. Psychol. Rev. 88, 135–170 (1981)
27. Rescorla, R.A., Wagner, A.R.: A theory of Pavlovian conditioning: variations of the
effectiveness of reinforcement and non-reinforcement. In: Blake, A., Procasy, C. (eds.)
Classical Conditioning II: Current Research and Theory. Appleton-Century-Crofts, New
York (1972)
Cognitive and Emotive Robotics: Artiﬁcial Brain
19
www.ebook3000.com

28. Widrow, B., Hoff, G.: Adaptive switching circuits. IRE WESCON Conv. Rec. pp. 96–104
(1960)
29. Barto, A.G., Sutton, R.S., Brouwer, P.S.: Associative search network: a reinforcement
learning associative memory. Biol. Cybern. 40, 201–211 (1981)
30. Cannon, R.: Dynamics of Physical Systems. McGraw Hill, New York (1967)
31. Barto, A., Sutton, R., Anderson, C.: Neuronlike elements that can solve difﬁcult learning
control problems. IEEE Trans. Syst. Man Cybern. 13, 834–846 (1983)
32. Bellman, R.: Dynamic Programming. Princeton University Press, Princeton (1957)
33. Watkins, C.: Learning from delayed rewards. Ph.D. thesis, Kings College, Cambridge,
England (1989)
34. Keller, F., Shoenfeld, W.: Principles of Psychology. Appleton-Century-Crofts, New York
(1950)
35. Bozinovski, S.: Robotics and Intelligent Manufacturing System. Gocmar Press, New York
(1997). (in Macedonian)
36. Bozinovski, S.: Inverted pendulum learning control. ANW Memo, Computer Science
Department, University of Massachusetts, Amherst, 10 December 1981
37. Rumelhart,
D., McClelland,
J., The PDP Group: Parallel Distributed Processing:
Explorations in the Microstructure of Cognition. MIT Press (1986)
38. Watkins, C.: Combining cross-validation and search. In: Bratko, I., Lavrac, N. (eds.)
Progress in Machine Learning, Proceedings of the 2nd European Working Session on
Learning, pp. 79–87, Sigma (1987)
39. Barto, A., Sutton, R., Watkins, C.: Learning and sequential decision making. In: Gabriel, M.,
Moore, J. (eds.) Learning and Computational Neuroscience: Foundations of Adaptive
Networks, pp. 539–602. MIT Press, Cambridge (1990)
40. Barto, A.: Reinforcement learning. In: Omidvar, O., Elliot, D. (eds.) Neural Systems for
Control, pp. 7–30. Academic Press, Gurugram (1997)
41. Bozinovski, S.: Motivation and emotion in anticipatory behavior of consequence driven
systems. In: Butz, M., Sigaud, O., Gerard, P. (eds.) Proceedings of the Workshop on
Adaptive Behavior in Anticipatory Learning Systems, Edinburg, pp. 100–119 (2002)
42. Barto, A., Singh, S., Chentanez, N.: Intrinsically motivated learning of hierarchical
collections of skills. In: Proceedings of the 3rd International Conference on Developmental
Learning (ICDL 2004), LaJolla, CA (2004)
43. Sutton, R., Barto, A.: Reinforcement Learning. MIT Press, Cambridge (1998)
20
S. Bozinovski

Metaphors of Creativity
Tony Veale(B)
University College Dublin, Dublin, Ireland
tony.veale@ucd.ie
1
Introduction
What is creativity? This question is far more likely to elicit an anecdote, an
aphorism or a metaphor than it is a literal deﬁnition. Creativity is an elusive
phenomenon to study, made all the more vexing by our fundamental inability
to pin it down in formal terms. As a category of human behaviour, creativ-
ity appears to resist all attempts at classical categorization via necessary and
suﬃcient features. Indeed, it may well the case be that there exists no single
creativity mechanism, and that all instances of creative behaviour are best cor-
ralled into a meaningful synthesis only by a system of family-resemblances. For
example, recognizing that diﬀerent solutions might be considered creative in dif-
ferent ways, Newell and Simon [17] suggest four diﬀerent criteria for identifying
a creative answer or solution:
1. The answer has novelty and usefulness (either for the individual or society)
2. The answer demands that we reject ideas we had previously accepted
3. The answer results from intense motivation and persistence
4. The answer comes from clarifying a problem that was originally vague.
A given solution may be considered highly creative yet satisfy just one or
more of these criteria. Nonetheless, no criterion seems to hit the mark. For
instance, consider (1) in the context of linguistic creativity. In the course of an
average day, most speakers of a language will utter some statements that are so
speciﬁc to their ephemeral contexts that they will never have been uttered before
(or likely to be ever again); since these utterances serve some communicative
function, they are thus both novel and useful, yet they can hardly be considered
creative (except in the unconventional, generative-linguistics sense of Chomsky
[1], which describes to the ability of language users to generate an inﬁnitude of
diﬀerent sentences). It appears then that the meaning of novelty in (1) already
presupposes a notion of creativity. Likewise, the rejection of Habeas Corpus as a
legal concept in time of war (or terrorist threat) is not as creative as (2) would
suggest, for true creativity in such a context would ﬁnd a way of preserving a
treasured belief in troubling situations. Meanwhile, the intense motivation and
persistence of (3) is as much a characteristic of tenacious plodders as it is of
creative individuals, while for some problems, pedants are every bit as capable
as insightful thinkers of providing the clarity called for in (4).
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 3
www.ebook3000.com

22
T. Veale
While these criteria are ﬂawed, note how each criterion simply recapitulates
the meaning of a conventional metaphor of creativity. For instance, (1) is con-
sonant with the view that creative solutions are fresh and innovative, perhaps
even ground breaking; (2) suggests that one must think outside the box and
reject conventional categories and labels; (3) suggests that to be creative, one
must expend copious amounts of mental energy in tenaciously exploring a par-
ticular avenue or wide-ranging conceptual space; and (4) espouses the common
belief that creativity requires illumination and insight. These metaphors allude
to a wealth of intuitions that cannot be summarized in the neat bullet points
of (1) thru (4), and one cannot but feel that Newell, Shaw and Simons attempt
at a literal exposition of the facts seriously short-changes these metaphors. A
more considered exploration of the underlying metaphors, to perhaps achieve a
synthesis at both the source and target levels of description, may yield a tighter
and less fragmentary perspective on creativity.
Languages like English have very few non-metaphoric terms for creativity,
which suggests that it is a concept largely understood in metaphoric terms.
This metaphoric basis may well explain why creativity is so diﬃcult to deﬁne
formally, since formal deﬁnitions typically require literal precision. However, the
lack of a literal substrate should not obviate our search for formal insight, since
it has become a ﬁrmly established tenet of research in Cognitive Linguistics that
metaphor constitutes much more than ripples on the surface of natural language,
but a conceptual phenomenon that reveals the deep currents and undertows of
thought itself (e.g., see Lakoﬀand Johnson [2]; Johnson [3]; Martin [4]; Veale and
Keane [5]; Barnden [6], Gibbs [7]; Veale [8]; Veale et al. [9]). The study of our
most widely used metaphors thus opens a window onto the conceptual processes
that underpin not just language but other aspects of cognition, illuminating the
mechanisms we use for reasoning about time, space, culture and society. The
success of the metaphor programme in Cognitive Linguistics suggests that the
metaphors we use to talk about a particular concept of cognitive interest can
sometimes be as revealing as any formal theory. This is especially so when the
concept in question is culturally entrenched and possesses a rich but polymorphic
basis in folk wisdom. Creativity appears to be one such concept, for it makes
its presence known in many contexts (e.g., linguistic, artistic, scientiﬁc, humour)
and many guises (e.g., jokes, theories, products, solutions and pranks) that it
easily deﬁes a reductionist or essentialist account of its workings.
If the latter is indeed the case, we have good reason to believe that a diverse,
non-essentialist account of creativity can be constructed by surveying the folk
beliefs of creative agents themselves, as revealed through the metaphors they
employ to describe this much-valued ability. This belief is supported by the fact
that models of creative behaviour have traditionally been informed by speciﬁc
metaphors of mind and intelligence, though some theoretically-motivated (and
theory-motivating) metaphors may appear less conventional than others. For
instance, Koestler [10] roots his theory of creativity in the notion of bisociation,
which he deﬁnes as an operation over incongruous “mental matrices”. While the
notion of a mental matrix lacks formal structure in Koestler’s writing, he exploits

Metaphors of Creativity
23
this vagueness to oﬀer a visual, geometric metaphor for bisociative combination.
From this metaphoric perspective, Koestler’s matrices become planar surfaces,
so that bisociation can be said to occur at the unexpected juncture of orthogonal
planes. In more contemporary terms, bisociation can be seen as an inﬂuential
precursor to Fauconnier and Turner’s [11,12] theory of Conceptual Integration or
Blended Spaces, which, as the name suggests, is similarly grounded in a spatial
metaphor of conceptual organization.
Linguistic metaphors for creative behaviour have a diverse presence in Eng-
lish, though as we shall argue, the most common metaphors can most natu-
rally be clustered into four families: metaphors of space; metaphors of vision;
metaphors of force; and metaphors of boundary realignment. In the sections
that follow, we consider each of these families of metaphor in turn, and con-
sider the extent to which each family oﬀers real insight into a cognitive model
of creativity. Our goal is to identify a metaphor schema that can be genuinely
informative on the topic of creativity, providing theoretical useful insights into
the nature of both the processes and representations needed to model creative
thought.
2
Spatial Metaphors
This family of metaphors views creative behaviour as a process of exploration
in a conceptual space, through which a variety of alternate pathways may be
pursued. These metaphors are extensions of conventional spatial metaphors for
rational, problem-solving behaviour. For instance, it is commonplace in English
to think of problem solving as involving some or all of the following: “ﬁnding a
solution”, “hitting a dead-end”, “backtracking”, “pursuing a particular avenue”
and “reaching a goal”. Since problem-solving is a purposeful cognitive activity,
this spatial perspective can thus be seen as an extension of the conventional
metaphor Purposeful Action Is A Journey.
The resulting metaphor of Problem-Solving Is Search has been more inﬂuen-
tial than any other in shaping the agenda for research in problem solving and
creativity (e.g., see Heaton [13]), not least because it allows one to extend the
underlying spatial metaphor in a way that best exploits the processing capa-
bilities of computers (see Veale and O’Donoghue [14]). In Newell et al. [15]
employed the speciﬁc extension Solutions Are Points In A Search Space to give
formal structure to the intuition behind the conventional spatial metaphors. A
solution space is a collection of connected points, each of which represent a can-
didate solution to a problem, though most points will represent solutions that
are either incomplete, impracticable or far from ideal. From this perspective,
problem-solving is a process of mental path-ﬁnding from a start state a point in
solution space that represents just the information that one can glean from the
original problem speciﬁcation to a desired goal state through a series of inter-
mediate points or stages. The intrinsic connectivity of a solution space means
that one can navigate the space much as one would navigate a real landscape, by
using cues from the immediate neighbourhood (such as the metaphoric equiva-
lent of gradient) as well as educated guesses to establish a path from the start
www.ebook3000.com

24
T. Veale
state to the goal state. Problem-solving behaviour can be considered rational
only if it leads one to navigate the solution space of a problem in a systematic
manner. For instance, strategies that cause one to visit the same points twice, or
which lead one to wander aimlessly in circles can hardly be called intelligent or
creative. Ultimately, however, the intelligence implied by a particular solution is
very much a function of the path constructed (e.g., how short? how obvious? How
much conceptual distance does it cover?), and as such, the Problem-Solving Is
Search metaphor can be seen as extending the Source-Path-Goal image schema
described by Johnson [3].
Johnsons Source-Path-Goal schema has additional implications for a spatial
perspective on creativity. For example, a natural by-product of this schema is the
Problem-Solving Is Locomotion metaphor, in which the imagination is viewed
as a space that can be traversed by diﬀerent locomotive means of varying relia-
bility and speed. Conventional wisdom thus conceives of creative individuals as
“quick-witted” ﬂiers who undertake wide-ranging “ﬂights of the imagination”.
When not ﬂying, creative agents make speedy progress through solution space
by “mental leaps”, while less creative individuals take “baby steps”. Creative
leaps are subsequently explained to these plodders in the form of a guided walk
through.
It is the “mental agility” of creative individuals that allows them to reach
parts of the solution space that previously went unexplored by other, more con-
ventional means. It is thus commonplace to describe creative thinkers as “explor-
ers” and “pioneers” who open up “new ﬁelds” of inquiry. Creative endeavour is
often idealized as a search for an elusive goal, a quest for true knowledge to ﬁnd
the holy-grail of a particular ﬁeld. “Nimble-minded” searchers reach virgin areas
of a conceptual space by “leaps of the imagination” that bypass intermediate
points in the space and allow thinkers to seemingly pursue multiple alternate
paths at once. Since these virgin areas lack maps, they represent the “perilous
and uncharted waters” of creative discovery.
3
Visual Metaphors
Mental leaps allow some individuals to travel considerable distances within a
solution space, and this distance is one conventional barometer of creative abil-
ity. This spatial quantity has a visual correlate in another family of metaphors
which view creativity as an ability to “see” across large distances. Thus, creative
thinkers possess “foresight” and are suﬃciently “far-seeing” to perceive the “far-
ranging” consequences of their actions. Such “visionaries” typically “see” solu-
tions fully formed, without having to plod through the intermediate points in
solution space from start to goal states. Like locomotive speed and agility, visual
metaphors of creativity imply that creative behaviour is a natural endowment
or gift, an ability that one exercises without necessarily understanding. Thinkers
with “mental acuity” can “see the value” of unorthodox solutions, while people
who refuse to see the products of a creative process are derided as “blinkered”,
“blind”, “lacking in imagination” or as suﬀering from “tunnel-vision”.

Metaphors of Creativity
25
Visual metaphors of creativity can be thought of as extensions of the more
general metaphor schema Creativity Is Perception. Thus, it is commonly said
that creative individuals perceive patterns and connections between objects that
are not visible to lesser thinkers. For instance, chess grandmasters achieve vic-
tory not by scouring the chess search space faster than their opponents - as a
computers might play chess - but by perceiving the relevant patterns that exist
between pieces on the board. It is often said that creative thinkers “feel” their
way to a solution using intuition rather than reasoning, and such thinkers are
called “highly perceptive”.
Nonetheless, the visual perspective on creativity has become lexically
entrenched in English through the words “insight” and “imagination”, while
the suddenness of creative thought, the essential quickness that is spatially cap-
tured by “mental leap”, is captured by the image-based expression “ﬂashes of
insight”.
4
Force Metaphors
While insight may occur in “ﬂashes”, it is also commonly said that “inspiration
strikes”. The latter typiﬁes a family of metaphors that view creativity as a
natural force. Broadly speaking, force metaphors can be sub-categorized into
four types: natural forces, supernatural forces, mechanical forces and biological
forces.
Metaphors based on natural forces tend to view creativity as powerful and
unpredictable as the most intense forms of weather. Creativity can be seen as
lightning that, in the case of inspiration, can “strike” without warning. This
unpredictability is tempered somewhat in one particular variation that describes
a reliable source of creativity, such as an individual or research team with a com-
pelling track record, as “bottled lightning”. Those groups that cannot harness
creative lightning in this way can nonetheless attempt to recreate the weather
conditions that give to it, through the popular group activity of “brainstorm-
ing”. The goal of most think-tanks and research-groups is to facilitate those work
processes that allow creativity to be harnessed as a form of energy. It is common
then in these contexts to talk of “creative energy” that forms a “creative ﬂow”
that can be “tapped into”.
If brainstorming seems like a corporate take on the rain-dance, perhaps it
is because supernatural metaphors express a compatible view of creativity as
a “divine force” that can be invoked with the appropriate rituals, or one that
inhabits only certain individuals as the result of a “divine gift”. Given the great
power that is associated with creative ability, those thinkers that can harness
it eﬀectively are commonly described as “wizards” or “magicians”, while cre-
ativity itself becomes a kind of “magic”. Perhaps this latter view also excuses
our traditional inability to formally deﬁne creativity, since magic and science are
mutually incompatible pursuits.
Complementing this magical view of creativity is a sub-family of force meta-
phors that view creativity as a mechanical phenomenon that can be harnessed for
www.ebook3000.com

26
T. Veale
engineering purposes. Thus, one commonly reads of the “engines of innovation”
and “creative dynamos” that drive technological progress. In this view, creativity
is a driving mechanism forever pushing forward, one that can be “fuelled” or,
to use a popular steam engine metaphor, “stoked”. Such engines need the right
operating conditions to produce the necessary output, so anything that staunches
the creative ﬂow - like some forms of intellectual property law - are said to “put
the brakes on creativity”.
Tempering the hard edge of these mechanical metaphors is another sub-
family that instead emphasises the biological life-force of creativity. From this
perspective, creativity is a fuel that ﬂows in the blood, hence the common phrase
“to get the creative juices ﬂowing”. A creative imagination is often called a
“fertile imagination”, while those who generate many new ideas possess a form
of intellectual “fecundity”. Fertility is also implicit in the idioms “germ of an
idea”, “planting the seeds” and “giving birth to a theory”, while ideas whose
creativity is not borne out on closer analysis are allowed to “wither on the
vine”. This view that Ideas-Are-Fruits (of a fertile imagination) is in turn a
specialization of the schema Ideas-are-Food, for just as foods are a source of
physical energy, ideas are a source of mental energy, where the most innovative
and creative ideas are deemed fresh and the most substantial and intellectually
nourishing are described as meaty.
5
Boundary Metaphors
The spatial perspective on creativity suggests that creative individuals are
capable of far-ranging exploration of a conceptual space, while more mundane
thinkers are fundamentally limited in the scope of their inquiries. This notion
of limit or boundary is consonant with the view of mundane thinkers as “blink-
ered”, “blind” and “narrow-minded”. Indeed, creativity is popularly conceived
as a process of “pushing boundaries”, “pushing the envelope” or “broadening
horizons”, while a creative product, if suﬃciently innovative, is often considered
“ground-breaking”, “a breakthrough” or a “break-out success”. It seems then
that, in addition to the Source-Path-Goal schema, descriptions of creativity also
show evidence of being deeply inﬂuenced by the Container schema of Johnson
[3]. This schema structures our knowledge of containers, real and abstract, by
positing a boundaries that separate the inside of a container and its contents
from the outside world. By shifting, blurring, transcending or re-orienting these
boundaries, a creative individual can dramatically alter the representation of a
given domain or problem space, to reveal solutions that could not previously be
perceived.
Since containers are multiply bounded objects, container metaphors express
a particular view of creativity as a process that transcends speciﬁc categories,
paradigms or frames of reasoning. Categorization is, after all, a process of bound-
ary imposition, wherein a given object is granted category membership only if
it lies within the established boundaries of the category. Creative processes like
metaphor are eﬀective at moving these boundaries to accommodate new and

Metaphors of Creativity
27
strikingly diﬀerent members. Since categorization is often performed on the basis
of visual cues, some key boundary metaphors are also visual metaphors. A blur-
ring of boundary lines allows one to “see the big picture”. In contrast, an ability
to see boundaries that are not obviously present is also useful to the creative
individual, who is able to “see the wood for the trees”. As seen through the Con-
tainer schema, conventional frames of reference become convenient boxes that
constrain habitual thought processes and allow us to make quick, if mundane,
inferences. These boxes also carry labels that describe their contents, but an
open-minded individual can look past conventional labels and is able to “think
outside the box”. Two-dimensional variations of container metaphors are also
common currency when talking about creativity. Thus, inspiration is often said
to arrive “from left ﬁeld”, while unconventional creative thinkers are given licence
to “colour outside the lines”. A radical shift in the positioning of these lines has
come to be called a “paradigm shift” (see Kuhn [16]).
A paradigm-shift not only transcends conventional boundaries, but re-orients
them, frequently resulting in a radical re-orientation of attitudes and beliefs
within a given ﬁeld of research. In recent times, a creative product that causes
an “upheaval” of an entire ﬁeld or market has come to be called a “breakout”
technology that in turn establishes new directions in research. If suﬃciently
radical, a creative disruption is deemed a “revolution” which, following the literal
sense of the word, “over-turns” prevailing wisdom by turning a widely-head
belief or theory “on its head”. In an extension of the problem-solving view of
creativity, it is popularly conceived that creativity allows quick-witted agents
to prevail against diﬃcult odds. A creative agent can prevail in an adversarial
setting by “turning the tables” on a more-literal minded opponent, in eﬀect
turning the opponents weapons, traps or strategies (whether physical or verbal)
against himself, causing them to backﬁre.
6
Analysis
Of the metaphor families surveyed here, spatial metaphors undoubtedly exer-
cise the strongest inﬂuence on theories of intelligence and creative behaviour,
succeeding more than any other family of metaphor in framing the creativity
debate amongst theorists. Of course, such metaphors as “to ﬁnd a solution”
have long been entrenched in Western languages like English, but their theo-
retical value was most potently harnessed in the 1950s by the work of Newell
and Simon (see Newell et al. [15]; Newell and Simon [17,18]), whose concept of
a Solution Space was to establish the dominant vocabulary for AI and Creativ-
ity research until the present day. More recently, this particular metaphor of a
solution space or search space, and the view of intelligence as rational search
that it engenders, has given birth to a variety of more elaborate variations, such
as Boden’s [19,20] Conceptual Space perspective. Not all creative activities are
equally innovative or ground-breaking, so Boden identiﬁes two modalities of cre-
ative activity, exploratory and transformational. Exploratory creativity is the
more mundane of the two modalities, and involves a process of navigation in a
www.ebook3000.com

28
T. Veale
Conceptual Space to locate solutions that exhibit both novelty and utility rela-
tive to some value system. Transformational creativity, a much rarer and intense
form of creative endeavour, actually transforms the Conceptual Space itself, by
modifying the rules or axioms that deﬁne the space. Perkins [21] is not so inter-
ested in transforming the Conceptual Space as identifying topographic aspects
of the space that are more or less conducive to creativity activity. Among the
colourful topographical region types that Perkins identiﬁes are Klondike spaces,
Oasis zones and clueless plateaux.
In cognitive linguistic terms, the utility of spatial metaphors in creativity
research arises from the temporal perspective they oﬀer, since they essentially
provide a serial scan of the creative process (see Langacker [22]). That is, one
can tease out from these metaphors a sequence of actions and states that are
implicated in creative patterns of thought. In contrast, visual metaphors appear
to oﬀer just a summary scan of the creative process as an irreducible mental
function, one perceived as a natural ability (sight) whose particular mechanics
such as image projection onto the retina, transmission via the optic nerve, etc. are
irrelevant to the workings of the metaphor. Nonetheless, visual metaphors can be
seen as corollaries of a spatial perspective on creativity. To eﬀectively traverse
a conceptual space, one needs visual acuity to recognize relevant landmarks,
perceive and avoid obstacles and to look ahead to a goal that may be too distant
to see for less creative individuals. In turn, this level of visual acuity implies that
a conceptual space is suﬃciently illuminated for these landmarks and obstacles
to be perceived against their conceptual backdrop.
This illumination is often provided by a metaphorical source that oﬀers the
most romantic perspective on creativity of any of the families we have consid-
ered. Lightning is sudden, unpredictable and very powerful, precisely the kind of
illumination that romanticists associate with the creative mind-set. This basis in
romanticism notwithstanding, the Creativity as A Force schema is not entirely
devoid of computational resonance, and is tangentially related to some notable
attempts to frame an algorithmic perspective on creativity. For instance, the
ﬂuid analogy work of Hofstadter [23] employs a metaphorical notion of temper-
ature to reﬂect the amount of creative energy in a dynamical system. A hot
system is one in creative ferment, where greater slippage is allowed between
disconnected states that are not, strictly speaking, linked by a deductive chain
of reasoning. A hot system is thus more likely to make potentially unsound, if
ultimately valuable, mental leaps across a conceptual space. In contrast, a cold
system is one that permits only sound, rule-based exploration of the conceptual
space. Since a consistently hot system is too unsound to be useful, and a consis-
tently cold system is too rigid to be innovative, temperature (and thus creative
energy) must be rationed judiciously. It is best if the temperature starts high, to
allow early exploratory steps to have more divergent consequences. Temperature
should then gradually drop over time, to lock these early, creative gambits into
a progressively sounder and more convergent chain of processing. This notion of
temperature a physical metaphor widely used in computer science in a problem-
solving technique called simulated annealing essentially controls the amount of

Metaphors of Creativity
29
freedom exhibited by a creative system. It is a particularly useful notion to adopt
when transition between states in a conceptual space is probabilistically deﬁned,
so that temperature becomes an important parameter of the probability distrib-
ution function. In such cases, a high temperature can enable a transition that is
deemed to have an otherwise low probability of being useful, or one that appears
to momentarily take the exploration process in a direction away from its goal.
In non-probabilistic systems, however, it is harder to ﬁnd a concrete role for
temperature.
These three families of Space, Vision and Force do not so much oﬀer diﬀerent,
even contradictory, descriptions of creativity, but actually describe complemen-
tary aspects of a complex phenomenon that together oﬀer a more coherent view
of creativity than either can do in isolation. Consider spatial metaphors of cre-
ativity: while conveying a valuable and informative process-level view of creativ-
ity as a form of conceptual exploration, this spatial perspective is hardly more
informative than the spatial view of intelligence that underlies symbolic Artiﬁcial
Intelligence. However, the important diﬀerence between both spatial perspec-
tives is precisely that which is conveyed by metaphors of force and vision: cre-
ative exploration is often swift enough to suggest extraordinary powers of mental
locomotion, and perceptive enough in its identiﬁcation of novel and unconven-
tional pathways to suggest an almost divine form of inspiration (where solutions
appear, like lightning, to arrive out of the blue). Metaphors of force and vision
are thus necessary adjuncts to a spatial conception of creativity. Unfortunately,
necessity does not imply informativeness, for these adjuncts merely reﬂect how
the end product of creativity is perceived by an observer (sudden, swift and
electric), and lack any genuine representational or algorithmic insight.
We need to look elsewhere then for such insights. Fortunately, boundary
metaphors make strong representational commitments to source elements such
as boxes, boundaries, lines, ﬁelds and tables that can be given speciﬁc conceptual
interpretations that are, in turn, suggestive of speciﬁc cognitive processes. For
instance, boxes are frames of reference, categories or labels; lines and boundaries
are rules and constraints; ﬁelds and paradigms are domains, or more speciﬁcally,
mental spaces; and tables are strategies. Boundary metaphors thus suggest rep-
resentational and algorithmic aspects of a computational theory of creativity,
and may provide the missing ingredient that is lacking in a synthesis of space,
force and vision metaphors. Nonetheless, boundary metaphors must be made
to work for us in a theoretical sense, to provide true hypothesis-forming and
explanatory power rather than simply adding another layer of poetic rhetoric to
an already over-romanticized ﬁeld of inquiry.
Toward this end, the Trajector-Landmark schema of Langacker [22,24] pro-
vides a basic theoretical ground against which boundary metaphors of creativity
can more formally be analysed, within a cognitive-linguistic framework. In this
schema, a trajector concept is conceived to hold a spatial, possibly locomotive,
relationship with a landmark concept. Interpreted relative to a spatial model
of creative exploration, the creative process (the explorer) itself becomes the
trajector, moving through a space shaped by conceptual boundaries. This space
www.ebook3000.com

30
T. Veale
will be punctuated by landmarks, conceptual structures which include not just
boundaries, but signiﬁcant gradients, way-points and signposts (e.g., recall the
oasis zones and clueless plateaux of Perkins [21]). For a conventional AI system,
boundaries deﬁne the limits of exploration and serve as obstacles that are to be
avoided (by look-ahead or by backtracking). In contrast, boundary metaphors of
creativity imply that a creative system may be capable of dismantling or mov-
ing a troublesome landmark, to reconceptualize it in a less troublesome form
or place. However, we must be wary here to avoid the phenomenological trap
of the vision and force metaphors earlier: it is all very well to say that creative
individuals can transcend boundaries, to think outside the box, but we need to
understand this transcendence in genuine representational and processing terms.
We can begin to reach such an understanding by noting that to dismantle
a landmark concept, one must shift ones attention from the whole to its parts,
eﬀectively reversing or undoing the individuation process that ﬁrst brought the
landmark into focus relative to the conceptual background. Likewise, we note
that the reconstruction of a landmark, in a more conducive form or in a dif-
ferent area of the conceptual space, requires a subsequent shift back and a
re-individuation of the landmark, from its parts (perhaps newly construed or
aligned) to form a new whole. Each of the boundary metaphors we have consid-
ered implies such a dramatic shift of focus, either from the inside to the outside
of a container, from the right-side to the left-side of a boundary or from the
up-side to the down-side of an axis or division. We might ask then, what spe-
ciﬁc cognitive mechanisms or well-understood phenomena exhibit such a radical
shift of attention, and whether such a mechanism or phenomenon might play an
explanatory role in a theory of creativity?. One phenomenon that appears to ﬁt
this description rather snugly is ﬁgure-ground reversal. Perhaps it is this rela-
tively well-understood cognitive phenomenon that boundary metaphors allude
to? If so, the implications for conceptual representation and processing are direct
and computationally informative.
7
Figure-Ground Reversal
Figure-ground reversal (henceforth FGR) is a phenomenon most commonly asso-
ciated with visual perception, and indeed, few readers will be unfamiliar with
the gestalt switches required to process such trick images as the Necker cube
(see Fineman [25]). Nonetheless, FGR is not a phenomenon that is necessarily
restricted to image processing, and may apply to any representation in which
a ﬁgure-ground distinction is present, as in the cognitive representation of con-
ceptual structure (e.g., see Langacker [22,24]). Ultimately, not all conceptual
features are created equal, and in a given conceptual representation some will
conventionally be considered more core or prototype-deﬁning than others (see
Lakoﬀ[26]). Sowa [27] refers to this perspective as the egg-yolk theory of mean-
ing, where those features deemed core will occupy the yolk of the concept, while
those that play a more peripheral role occupy the egg white. In general then,
FGR is a possibility in any conceptual structure in which salience can be redis-
tributed from primary to secondary features of the representation, that is, from

Metaphors of Creativity
31
those conceptual elements that are marked, privileged, prototype-deﬁning or
otherwise highlighted (the egg yolk), to those that are not (the egg white). For
instance, the primary features of a Chair concern its role as a seat or resting
place, while its secondary features concern its physical structure (e.g., wooden,
rigid, legged, etc.). For most examples of a category or theory what we might
call the unexceptional cases the primary and secondary features are in harmony,
to the extent that the latter can serve as reliable proxies for the former (Gendler
[28]). This correlation should not be surprising, since for example, it is the rigid
and legged structure of a chair that allows it to serve its primary role as a seat.
However, in exceptional cases the secondary features diverge from the primary
features and this otherwise reliable correlation breaks down, e.g., in the case of
beanbag chairs. To the extent that exceptional cases are useful cases (e.g., insofar
as beanbags are useful chairs), their novelty marks them as creative examples of
a theory or category.
7.1
Thought Experiments
Gendler [28,29], whose topic of interest is thought experimentation (see also
Mach [30]; Kuhn [16]), claims that all thought experiments (or Gedanken exper-
iments) essentially involve the construction of exceptional cases, since it is excep-
tional cases that best expose the limits of existing categories and theories. For
instance, the Aristotelian theory of falling objects which claims that all objects
fall at a speed proportional to their weight is seen as illogical once we pon-
der the Galilean case of a composite object comprising two stones, one large
and one small, connected by a rope. The primary feature here is weight, while
the secondary (apparently dependent) feature is speed. Galileos exceptional case
exposes a break between weight and speed, since the composite object should fall
both faster than the heavier stone alone (because the composite is heavier still),
and slower (because the light stone would act as a drag on the heavy stone).
Following Gendler, the creativity needed for a successful thought experiment is
precisely the creativity needed to formulate an appropriate exceptional case.
Not every exceptional case will be creative, just as that not every novel
artefact is useful or every divergence from the norm is proﬁtable. In thought
experiments, one explicitly aims to create, via an exceptional case, a perspicu-
ous representation in which backgrounded instinctive knowledge about a domain
of experience is dredged from the level of unarticulated intuition onto the plane
of ideas (Mach [30]). Heaton [13] deﬁnes a perspicuous representation (in the
sense of Wittgenstein) as one that brings about a Gestalt switch by highlight-
ing a new aspect of our use of words. Perspicuity at a representational level
seems to be what visual metaphors of creativity describe as illumination and
what Newell and Simon [17] label as clarity. Insights about exceptionality and
thought experiments (from Mach and Gendler) seem broadly applicable then
to creative reasoning in general, for what is creativity but the generation of
exceptional objects? After all, jokes are exceptional narratives, metaphors are
exceptional comparisons, works of art are exceptional artifacts, and so on. It may
seem that we have traded one vague term, creativity, for another, exceptional,
www.ebook3000.com

32
T. Veale
but Gendlers distinction between primary and secondary features oﬀers a means
of grounding a deﬁnition of exceptionality in terms of a speciﬁc representation
of features (i.e., Lakoﬀs radial categories, or Sowas egg-yolk theories of mean-
ing), and a speciﬁc process for manipulating the salience of these features (i.e.,
FGR). In this view, exceptional examples, either of a theory or a category, should
emphasise secondary features at the expense of primary features, severing the
fragile dependency between both levels of representation to introduce a greater
degree of creative freedom in how the category or theory is used.
7.2
Trickery
Figure-ground reversal appears to play a central role in a diverse range of excep-
tional social constructions, from pranks, swindles, jokes and magical illusions
to trickery of an intellectual variety, in scientiﬁc discovery and formal reason-
ing. Consider pranks, swindles and magic tricks ﬁrst. In each of these activi-
ties, ones attention is deceptively drawn to a highlighted sequence of ultimately
meaningless gestures, while actions of real signiﬁcance are executed in the back-
ground. Magic illusions, jokes and pranks are, after all, humorous tricks in which
an audience willingly participates, while swindles are tricks (often magical in
appearance) where the goal is to not the entertainment of the audience but the
enrichment of the performer. The arsenal of sleight-of-hand skills (such as misdi-
rection, palming and card forcing) employed by professional tricksters, whether
magicians or swindlers, are instances of ﬁgure-ground reversal that keep the key
actions of a trick hidden in the background, while those actions that are either
peripheral, unnecessary or unduly attention-grabbing (such as the ﬁnancial car-
rot dangled by conﬁdence tricksters) are foregrounded as eye-catching ﬂourishes.
In the case of stage magic, the initial FGR must be followed by a second act
of reversal that dramatically foregrounds and makes visible the unseen eﬀects
of the sleight of hand, as when a rabbit is pulled from what was, apparently,
an empty hat. In the case of a criminal swindle of the bait and switch variety,
the unfortunate dupe is left to perform this second FGR for himself at a later
time, e.g., upon discovering that a briefcase apparently ﬁlled with $100 dollar
bills is in fact ﬁlled with old newspapers. This discovery, incongruous at ﬁrst, is
resolved only when the dupe recognizes the swindle for what it is, in much the
same way that many theorists consider humour to be the result of a process of
incongruity resolution (e.g., see Attardo et al. [31], Veale [32]; and see Ritchie
[33] for an excellent survey). Figure-ground-reversal is not a speciﬁc tool of the
trickster, like palming, forcing or misdirection, but a general mode of operation,
and it is vital that we not confuse the strategy for the tactic.
7.3
Jokes
Figure ground reversal manifests itself in humour in a variety of guises, from the
explicitly obvious to the sublimely implicit. Instances of the ﬁrst kind include
jokes in which participants explicitly manipulate the physical ground of a sce-
nario rather than the ﬁgure itself (e.g., twisting an entire room just to screw

Metaphors of Creativity
33
in a lightbulb). Jokes that employ FGR as an explicit physical device in this
way will all invariably seem alike, as if instantiating the same narrative template
or Ur-skeleton (see Hofstadter and Gabora [34]). Instances of the second kind
do not ﬁt into standard templates, and are, unsurprisingly, harder to diagnose,
producing humour that is much less formulaic.
Consider the joke Tim hasnt played water polo since that tragic day when his
horse drowned. There are two crucial points to note about this joke. First, there
is nothing in the text that forces a humorous interpretation, for the most charita-
ble interpretation simply concerns a sportsman with multiple interests who gives
up all sports on the death of a beloved horse. Second, though the joke can be said
to hinge on the semantic conﬂicts inherent in the juxtaposition of Water(-Sports)
and Polo to create the blended concept Water-Polo (e.g., land/water, wet/dry,
animate(horse)/inanimate(canoe), etc.), these conﬂicts in themselves do not pro-
vide the humour nor do they necessitate a humorous resolution, for otherwise,
the blend Water-Polo would itself be humorous. Like thought experiments, jokes
exploit unarticulated knowledge to imagine exceptional circumstances that bring
this knowledge into the foreground. It is the way this knowledge is framed and
exploited, rather than the knowledge itself (which already exists; see Mach [30]),
that leads to insight in the case of Gedanken experiments and humour in the
case of jokes.
Blends like Water-Polo obey a variety of optimality principles (see Fauconnier
and Turner [12]) two of which, unpacking and web, are of direct relevance to this
joke. These principles eﬀectively enable elements of the input space Polo, such
as Horse, to remain accessible (with some cognitive eﬀort) after the blend, even
though these elements are not projected into the blend space itself. In furnishing
a link between Water-Polo and Horse, the blend thus enables the listener to
bind the backgrounded Horse of Water-Polo to the foregrounded horse of the
narrative. Listeners willingly jump then to the improbable construal that the
horse drowns while playing polo in a swimming pool, even though more probable
construals exist, simply because they can, for listeners opportunistically seek out
humorous interpretations of a text whenever they are licensed by context (see
Veale [32] for an elaboration of this opportunistic view of humour). Overall
then, the joke employs an FGR to re-emphasise the concept of a Polo Horse in
the context of Water-Sports, while simultaneously backgrounding the concept
Canoe, allowing one to imagine water-polo played with horses instead of canoes.
This in turn yields a coherent explanation for the drowning of the horse and the
protagonists subsequent disavowal of his sport.
We note that while the General Theory of Verbal Humour (or GTVH) as
described in Attardo et al. [31], provides an explicit logical mechanism for
resolving jokes that employ ﬁgure-ground reversal, this mechanism precludes
the extremely subtle form of FGR described above, and addresses instead the
Ur-jokes in which rooms are rotated to twist in a light-bulb and so on. The
signiﬁcant and deep role that FGR has to play in humour is still largely unac-
knowledged in the GTVH.
www.ebook3000.com

34
T. Veale
7.4
Formal Reasoning
Creativity of an intellectual variety, as found in science, mathematics and phi-
losophy, is also frequently categorized as a benign form of trickery, in which a
theorist is said to employ a particular trick, twist or reversal. Philosophers in
particular are known to set traps for their readers, to catch them in the contra-
dictions that lurk beneath lazy or habitual thought processes. Consider formal
reasoning, which concerns the truth-values of conjectures within a logical sys-
tem and how they may be ascertained by a sound process of proof. This process
either converts conjectures into theorems, or rejects them as falsehoods, via a
proof that is ultimately grounded in a system of axioms and postulated truths.
In ﬁgure-ground terms, conjectures serve as the highlighted ﬁgures in the proof
process, since these are the objects of inquiry on which our attention is primarily
focussed. In contrast, axioms and postulates serve as the background elements
relative to which the truth of falsity of conjectures is explored. As beﬁtting
their postulated, axiomatic status, the truth of these background beliefs is never
questioned, at least in conventional logic systems.
Now, FGR can occur in a formal setting in two diﬀerent ways. In the ﬁrst
case, an axiom is foregrounded with the revisable status of a conjecture, so
that its truth can be openly questioned. In the second, a conjecture is given
axiomatic status and moved back into the postulated ground. As an example of
the ﬁrst case, consider fuzzy logic, a creative variation on classical propositional
logic in which the axiomatic law of excluded middle is denied. This rejection
allows fuzzy propositions to be quantiﬁably true and false at the same time. In a
similar fashion, new geometries can be constructed by denying axiomatic status
to a postulate of Euclidean geometry. For instance, by rejecting the parallel
postulate, one can construct new geometries in which parallel lines are allowed
to intersect, as is the case in Riemannian geometry. The second case of FGR
eﬀectively describes a reasoning process called proof by contradiction or reductio
ad absurdum. Here, the negated form of a given conjecture is added as an axiom
to the formal system in which its truth is to be ascertained. It is assumed that
the formal system is logically sound prior to this addition, and so does not
contain any contradictions. Any contradiction that can thus be generated in
the new system must thus be caused by the addition of this new axiom. Since
negation of the conjecture leads to a contradiction, the original conjecture must
be true. One of the oldest and most elegant uses of this procedure is Euclid’s
demonstration of the inﬁnitude of prime numbers. By assuming as axiomatic the
existence of a highest prime number, Euclid’s proof then demonstrates how one
can construct an even higher prime number. This contradiction is then resolved
by negating the original conjecture and concluding that no ﬁnite number can be
the highest prime, ergo, the set of primes must be inﬁnite.
7.5
Levels of Reversal
Figure-ground reversal is an equally valuable strategy in more empirical sci-
entiﬁc endeavours, such as scientiﬁc discovery and, in particular, pharmaceuti-
cal development. Perhaps the most famous example of the latter concerns the

Metaphors of Creativity
35
development of Viagra, a drug originally formulated by Pﬁzer to treat high
blood-pressure. Early clinical trials proved unsatisfactory and were cancelled,
yet when participants in the trials were asked to return their unused pills, few
complied. Further investigation revealed the drug to have interesting side-eﬀects
that proved to be much more marketable than the drugs intended primary eﬀect.
Products like Viagra provide clear examples of the role of FGR in serendip-
itous scientiﬁc discovery. While every drug has a range of eﬀects on the human
body, the primary eﬀect (the ﬁgure) is that which is marketed by the pharmaceu-
tical company. In contrast, any secondary eﬀects are relegated to the background
for marketing purposes, to literally appear as small-print on the back of the pack-
aging. However, these secondary eﬀects may sometimes be seen as primary, as
with Aspirin, which is now aggressively marketed as a clot-busting blood thinner
rather than as a pain-killer. In the case of Viagra the ﬁgure-ground reversal is
more symmetrical: the primary eﬀect becomes an unfortunate side-eﬀect (users
are warned of sudden dangerous drops in blood pressure), while an unintended
but serendipitous sexual side-eﬀect has become the primary selling point of the
drug.
This kind of serendipity in the pharmaceutical industry suggests that the
secondary characteristics of an object - those which do not satisfy the a priori
goals of an agent - may have more value than its primary characteristics. Of
course, it is important that “value” is construed as broadly as possible. For
instance, if value is construed as “eﬃcacy as a blood pressure treatment”, then
Viagra has low value, However, if the broader construal of value “usefulness in
meeting a widespread demand” is employed, Viagra has considerably more value
to Pﬁzer and its customers.
Serendipitous discovery thus involves two levels of FGR, tactical and strate-
gic. In the case of Viagra, the tactical level describes the primary and secondary
eﬀects of the drug itself. In contrast, the strategic level describes the primary and
secondary goals of the scientists: the scientiﬁc goal of “ﬁnd an eﬃcacious treat-
ment for high blood pressure” serves as ﬁgure to the business-oriented ground
goal of “ﬁnd an eﬀective treatment that will generate substantial revenue for the
company”. Both levels occur in lock-step when creative agents conclude that a
ﬁgure-ground reversal at the tactical level can be supportive of a ﬁgure-ground
reversal at the strategic level.
8
Synthesis
Metaphors of boundary transcendence, and thus our intuitions about FGR and
exceptional cases, are clearly coherent with a metaphoric view of creativity as
agile exploration in a bounded conceptual space. But it is equally important,
however, that the computational interpretations of these metaphors are also
mutually coherent within a single computational framework. To see why this is
indeed the case, we need to return to Newell, Simon and Shaws spatial metaphors
of Problem-Solving Is Search and Solutions Are Points In A Search Space.
Recall that a search space can be envisioned as an interconnected system
of states - descriptions of that subset of the world relevant to the problem at
www.ebook3000.com

36
T. Veale
hand - where those states that diﬀer by the application of a single problem-
solving operation are directly connected. Problem-solving becomes a process
of exploratory path-ﬁnding in this space, to identify a pathway of successive
problem-solving operations (a meta-connection of sorts) that connects the given
start state to a satisfactory goal state. For deterministic problems, there exists a
deﬁnitive rule-based procedure for choosing the appropriate operation to apply
at each state in the search space. For instance, the behaviour of the dealer in
Blackjack is entirely determined by the rules of the game, thereby precluding any
creative or unpredictable behaviour by the dealer. In contrast, the behaviour of
poker players is not similarly determined. It follows that creativity can only
occur when solving problems that are eﬀectively non-deterministic, involving
large search spaces for which there exists no practical, non-speculative procedure
for identifying satisfactory pathways. In creative exploration, many of the states
in a search space may yield useful and novel candidate solutions, and thus, we
expect FGR to be a non-deterministic process that can divergently generate a
potentially large number of valid reconﬁgurations of a conceptual structure (see
Guilford [35] for a discussion of divergent problem solving, and Finke et al. [36]
for a discussion of fecund generation within an exploratory model of creativity).
The application of FGR to a conceptual representation of primary and sec-
ondary features is not a deterministic process, but a process that occurs within
a large search space in which many diﬀerent states might be considered valid
goals. Imagine, for the sake of simplicity, that the default representation of a
concept C is given by an ordered tuple of features of the form
<F1, F2, F3, Fn, fn + 1, fn + 2, fn + 3, fm>
where Fi denotes a primary or foregrounded feature and fj denotes a secondary
or background feature. By features we mean more than atomic symbols; Fi and
fj may denote modiﬁable parameters with a range of possible values, or even
conceptual sub-structures in their own right that recursively contain their own
sub-features. The tuple perspective is thus rich enough to represent blended con-
cepts like Water-Polo or the conceptual structures representing an interpretation
of a given joke text.
There are two basic operations that can be applied to the elements of a tuple
to eﬀect an FGR:
Fi →fi
(feature backgrounding)
fi →Fi
(feature foregrounding)
For a concept representation with m features, if we assume that each feature
can hold one of n diﬀerent values, then every variation of values in this repre-
sentation will correspond to one of nm diﬀerent tuples. Furthermore, since any
combination of features in a tuple can either be foregrounded or backgrounded,
there are 2m diﬀerent ﬁgure-ground conﬁgurations of a given tuple. The corre-
sponding search space deﬁned by these tuples will thus comprise nm2m unique

Metaphors of Creativity
37
states. Recall that FGR, as realized through the operations of feature back-
grounding and feature foregrounding described above, is just one of potentially
many transition mechanisms through which a conceptual explorer can move from
state to state and traverse this search space. FGR should thus be seen as a mech-
anism that applies selectively within the context of a larger exploration process,
to transcend boundaries, to sidestep obstacles or to ﬁnd short-cuts when con-
ventional transition mechanisms appear futile. In other words, only part of the
exploration of a conceptual space need be exceptional? for the whole exploration
to be judged as creative and to produce exceptional outputs.
We can also imagine a series of conventional, non-FGR operations of the
form:
Fi →#value
(assignment to a foregrounded feature)
The speciﬁc range of allowable values will of course depend on the type of the
feature. However, by not providing a transition type of the form fi →#value,
we make the assumption that only foregrounded features can be modiﬁed. With-
out FGR, any feature that is initially backgrounded will thus remain constant
throughout the exploration process.
A sequence of FGR operations of the form fi →Fi might therefore enable
a subsequent sequence of non-FGR operations of the form Fi →#value that
would not otherwise be possible, and these assignments might then be locked in
via another sequence of operations of the form Fi →fi. The state visited in this
way might require a much longer sequence of non-FGR operations to reach, or
might not even be reachable at all without FGR. While this application of FGR
might not appear to be transformative in the strong sense of Boden [19,20] it
is state transforming rather than space transforming the eﬀect may be equally
dramatic, allowing an exploratory process to perform a mental leap between
distant points of the conceptual space. Nonetheless, let us suppose that some
elements of a tuple, ﬁand fj say, are so conventionally de-emphasised that they
are eﬀectively viewed as constants. In such a case, ﬁand fj serve no useful role
within the tuple, so much so that one conventionally imagines the tuple without
them. To such a thinker, the search space is dramatically reduced; it now contains
a more manageable nm−22m−2 states rather than nm2m (a reduction by a factor
of 4n2), but one of the nm2m −nm−22m−2 lost states may be a truly creative
solution, or may comprise a necessary intermediate state on the path leading to
a creative solution. However, now imagine a creative individual who foregrounds
these eﬀectively hidden elements ﬁand fj in order to modify their contents. Such
an act of FGR may well appear to observers as the addition of two new elements
to the tuple, which in turn will appear to transform the conceptual space deﬁned
by possible conﬁgurations of this tuple. In this way, FGR, if applied to elements of
a representation made moribund by convention and habitual thought processes,
can appear as the radical transformation of a space which Boden [19] ascribes
to the most original and creative of individuals.
www.ebook3000.com

38
T. Veale
In this synthesis, which combines exploration within a conceptual space with
FGR as a possible transition mechanism between distant states, the following
metaphoric perspectives on creativity become computationally realizable:
1. Creativity involves mental leaps. A mental leap is an FGR-based transition
between states in a conceptual space that would otherwise require a substan-
tial sequence of non-FGR transitions to achieve. Alternately, the conceptual
distance (as measured by relative position in a taxonomy) between two states
that can be connected by the application of FGR may be large enough to be
called a leap.
2. Creativity requires mental acuity. The selective foregrounding or background-
ing of features implies a perceptiveness that is not typically associated with
conventional deductive processes. It allows a thinker to construct a perspicu-
ous representation that makes clear any tacit knowledge underlying a domain.
Mental acuity is especially relevant to the manipulation of backgrounded fea-
tures, since their lack of emphasis should make them harder to perceive.
3. Creativity requires mental agility. Figure-ground reversal may allow a tran-
sition out of a state that might otherwise be considered a dead-end and from
which an otherwise deductive process might be forced to backtrack. In this
way, FGR allows an exploratory process to transcend boundaries and side-
step obstacles.
4. Creativity is the result of a sudden mental force. An FGR-based mental leap so
shortens the pathway between starting state and goal state that the search is
deemed swift and the result sudden. The swiftness of the process is attributed
to greater locomotive force in the traversal of the search space.
To summarize, then, this synthesis views creativity as arising from the explo-
ration of a conceptual space, but one that involves some degree of discontinuity
in traversal, ranging from a clever twist to a mental leap or the radical transfor-
mation of the space itself, all facilitated by the use of ﬁgure-ground reversal.
9
Conclusions
In trying to corral diﬀerent folk intuitions about the meaning of creativity into
a single, coherent framework, one is reminded of Wittgensteins remark: One is
often bewitched by a word. For example, by the word know (Wittgenstein [37]).
Understanding how and when to use a given word, whether knowledge or creativ-
ity, does not imply a genuine understanding of the cognitive mechanism to which
such a word may allude, any more than one can base a theory of knowledge or
creativity on the dictionary deﬁnition of such a word. Following Wittgenstein,
one must question whether the seductive, often shamming power of words actu-
ally creates the illusion that creativity is a genuine mental phenomenon or that
people can be genuinely creative, any more than people can ever truly know a
subject and possess genuine knowledge. Wittgensteins therapeutic view of phi-
losophy might thus lead us to reject the very idea of creativity as a special kind
of intellectual function, and instead recognize this idea as simply a delusion of

Metaphors of Creativity
39
language. As he claims in the Philosophical Investigations: Philosophy is a battle
against the bewitchment of our intelligence by means of language (Wittgenstein
[38]). Here Wittgenstein appears to suggest that language is both the source of
the bewitchment as well as its remedy (see Heaton [13]). Thus, if the notion
of creative processing is cognitively real, then language, and the linguistic intu-
itions that underpin the use of words like creativity, may oﬀer the truest insights
into its function; in other words, let the use of words teach you their meaning
(Wittgenstein, ibid).
We began this paper with a consideration of ﬁve diﬀerent, and apparently
antithetical, families of creativity metaphor. Rather like the clich of the blind
men who each examine a diﬀerent part of an elephant only to arrive at radi-
cally diﬀerent conclusions, these metaphors appear to suggest radically diﬀer-
ent views on the workings of creativity, yet they ultimately describe the same
underlying mechanism. Through an analysis of the source-domain implications
of these metaphors, and a subsequent computational synthesis of their target-
domain correspondences, we have shown that these families are complementary,
and can be uniﬁed into a coherent, theory-shaping whole.
References
1. Lees, R.B., Chomsky, N.: Syntactic structures. Language 33(3 Part 1), 375–408
(1957)
2. Lakoﬀ, G., Johnson, M.: Metaphors We Live By. Chicago University Press, Chicago
(1980)
3. Johnson, M.: The Body in the Mind: The Bodily Basis of Imagination, Reason,
and Meaning. Chicago University Press, Chicago (1987)
4. Martin, J.H.: A Computational Model of Metaphor Interpretation. Academic Press
Professional Inc., Cambridge (1990)
5. Veale, T., Keane, M.T.: Conceptual scaﬀolding: a spatially founded meaning rep-
resentation for metaphor comprehension. Comput. Intell. 8(3), 494–519 (1992)
6. Barnden, J.A.: Belief in metaphor: taking commonsense psychology seriously1.
Comput. Intell. 8(3), 520–552 (1992)
7. Gibbs, R.W.: The Poetics of Mind: Figurative Thought, Language, and Under-
standing. Cambridge University Press, Cambridge (1994)
8. Veale, T.: Exploding the Creativity Myth: The Computational Foundations of
Linguistic Creativity. Bloomsbury Academic, London (2012)
9. Veale, T., Shutova, E., Klebanov, B.B.: Metaphor: a computational perspective.
Synth. Lect. Hum. Lang. Technol. 9(1), 1–160 (2016)
10. Koestler, A.: The Act of Creation. Macmillan, New York (1964)
11. Fauconnier, G., Turner, M.: Conceptual integration networks. Cogn. Sci. 22(2),
133–187 (1998)
12. Fauconnier, G., Turner, M.: The Way We Think: Conceptual Blending and the
Mind’s Hidden Complexities. Basic Books, New York (2002)
13. Heaton, J.M.: Wittgenstein and Psychoanalysis. Totem Books, Flint (2000)
14. Veale, T., O’Donoghue, D., Keane, M.T.: Computation and blending. Cogn. Lin-
guist. 11(3/4), 253–282 (2000)
15. Newell, A., Shaw, J., Simon, H.A.: Preliminary description of general problem
solving program-I (GPS-I). WP7, Carnegie Institute of Technology, Pittsburgh,
PA (1957)
www.ebook3000.com

40
T. Veale
16. Kuhn, T.S., Hawkins, D.: The structure of scientiﬁc revolutions. Am. J. Phys.
31(7), 554–555 (1963)
17. Newell, A., Simon, H.A.: GPS, a program that simulates human thought. In:
Feigenbaum, E.A., Feldman, J. (eds.) Computers and Thought, pp. 279–293. R.
Oldenbourg KG (1963)
18. Newell, A., Simon, H.A., et al.: Human Problem Solving, vol. 104. Prentice-Hall,
Englewood Cliﬀs (1972)
19. Boden, M.A.: The Creative Mind: Myths and Mechanisms. Weidenfeld and Nichol-
son, London (2004)
20. Boden, M.A.: Computer Models of Creativity, pp. 351–372. Cambridge University
Press, Cambridge (1998)
21. Perkins, D.N.: Archimedes’ Bathtub: The Art and Logic of Breakthrough Thinking.
WW Norton & Company, New York (2000)
22. Langacker, R.: Concept, Image, and Symbol: The Cognitive Basis of Grammar.
Mouton de Gruyter, Berlin, New York (1991)
23. Hofstadter, D.R.: Fluid Concepts and Creative Analogies: Computer Models of the
Fundamental Mechanisms of Thought. Basic Books, New York (1995)
24. Langacker, R.W.: Foundations of Cognitive Grammar: Theoretical Prerequisites,
vol. 1. Stanford University Press, Palo Alto (1987)
25. Fineman, M., Fineman, M.B.: The Nature of Visual Illusion. Courier Corporation,
North Chelmsford (1996)
26. Lakoﬀ, G.: Women, Fire and Dangerous Things: What Concepts Reveal about the
Mind. Chicago University Press, Chicago (1987)
27. Sowa, J.F., et al.: Knowledge Representation: Logical, Philosophical, and Compu-
tational Foundations, vol. 13. MIT Press, Cambridge (2000)
28. Gendler, T.S.: Thought Experiment: On the Powers and Limits of Imaginary Cases.
Garland Publishing, London (2000)
29. Gendler, T.S.: Galileo and the indispensability of scientiﬁc thought experiment.
Br. J. Philos. Sci. 49(3), 397–424 (1998)
30. Mach, E.: The science of mechanics, trans. TJ McCormack, new introduction by
K. Menger, 6th edn. Open Court, La Salle (1960)
31. Attardo, S., Hempelmann, C.F., Di Maio, S.: Script oppositions and logical mech-
anisms: modeling incongruities and their resolutions. Humor: Int. J. Humor Res.
15(1), 3–46 (2002)
32. Veale, T.: Incongruity in humor: root cause or epiphenomenon? Humor: Int. J.
Humor Res. 17(4), 419–428 (2004)
33. Ritchie, G.: Developing the incongruity-resolution theory. Technical report, Edin-
burgh Research Archive, Scotland (1999)
34. Hofstadter, D., Gabora, L.: Synopsis of the workshop on humor and cognition.
Humor: Int. J. Humor Res. 2–4, 417–440 (1989)
35. Guilford, J.P.: The Nature of Human Intelligence. McGraw-Hill, New York (1967)
36. Finke, R.A., Ward, T.B., Smith, S.M.: Creative Cognition: Theory, Research, and
Applications. MIT Press, Cambridge (1992)
37. von Wright, G.H.: Wittgenstein on certainty. In: Problems in the Theory of Knowl-
edge/Probl`emes de la Th´eorie de la Connaissance, pp. 47–60. Springer (1972)
38. Malcolm, N.: Wittgenstein’s philosophical investigations. Philos. Rev. 63(4), 530–
559 (1954)

Socially Intelligent Robots, the Next Generation
of Consumer Robots and the Challenges
Amit Kumar Pandey1,2(B)
1 SoftBank Robotics, Paris, France
2 euRobotics Topic Group on Socially Intelligent Robots and Societal Applications
(SIRo-SA), Paris, France
akpandey@softbankrobotics.com
https://www.ald.softbankrobotics.com/en, http://www.amitkpandey.com
1
New Wave of Social Robots and the Need of Social
Intelligence
We are evolving, so as our society, lifestyle and the needs. AI and ICT have been
with us for decades, and now penetrating more in our day-to-day life, so as the
robots. But, where are all these converging together? Towards creating a smarter
eco-system of living, where robots will coexist with us in harmony, for a smarter,
healthier, safer and happier life. Such robots have enormous potential to play
essential roles in our everyday life, such as in scenarios like companionship, child-
care, educational, special educational, edutainment, healthcare, and co-worker.
The question is how? The answer will be the Social Intelligence (SI) of the
robots. SI of such consumer Robots will be the key technology and the next big
R&D challenge at the cross-section of robotics, AI and cognition. SI will enable
such robots to behave in socially expected and accepted manners. However,
there are still various technological and scientiﬁc constraints that end-users,
robot providers and other involved stakeholders face, which prevent social robots
from entering the market with full potential. There is a great need to create
common ground and shared understanding about social robots, its potentials
and applications, and the barriers. For the long term success of such robots,
there is a need for these robots to behave in socially accepted and expected
ways. In this regard, connectivity, learning and collective intelligence are some
of the key building blocks (Fig. 1).
2
Social Intelligence and Robot Learning from Human
Social robots have a new range of applications. Such robots are supposed to
be working in the human environment, which is diﬀerent from the natural or
industrial setups, as shown in the ﬁgure below (Fig. 2).
The fact that there are humans in the environment creates a new set of R&D
challenges to be solved, for such robots to serve, assist, accompany people and
behave in the manner accepted and appreciated by people and does not create
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_4
www.ebook3000.com

42
A.K. Pandey
Fig. 1. Social robots.
Fig. 2. High-level spectrum of operation environment for robots. Human-awareness
and social awareness are the key factors for social robots.
any negative feeling of surprise or anger. These robots should explicitly consider
the presence of human in all their planning and decision making strategies,
whether it is for motion, manipulation, interaction or learning. This prompts new
research challenges and the need of a coherent theories, models and architectures.
Development of the key cognitive abilities and basic socially intelligent behaviors
of the robots, raise new challenges, which cannot be handled appropriately by
simple adaptation of state of the art robotics planning, control and decision
making techniques of the robots designed to work in non-human environment.

Socially Intelligent Robots, the Next Generation of Consumer Robots
43
Human-awareness and social awareness are the key factors. Among other factors,
a Socially Intelligent Robot should be able to “behaves by taking into account
the socio-cultural norms and expectations, and be able to reason about the
exchange of the favours and the supports the interacting agents (the humans
and the robot) as well as the environment can provide”.
In [1], a layered approach for embodiment of social intelligence of robots
have been proposed. It identiﬁes some of the socio-cognitive abilities and building
blocks leading to more complex behaviors. Perspective taking [2,3], i.e. reasoning
from other perspective and aﬀordances analysis [4], i.e. what can be done with
particular object, are some of such basic abilities towards achieving social intel-
ligence of the robot. Social learning is another mechanism to acquire knowledge
through day-to-day integration and a powerful tool towards building complex
social intelligence [5–9].
Another questions to look into, in the context of social robots at home and
at public places, are the ethical, social and legal aspects. In [10], a system is
presented capable of learning to maintain privacy, but on the other side also
illustrates the cases of potential dilemma of Privacy vs. Ethics for a Socially
Intelligent Robot (related video [11]).
3
SoftBank Robotics, Developing Robots for Well Being
of Society and Smarter Living
We at SoftBank Robotics believe that robots will play a key role in everyday life,
and that robots will co-exist with us, leading to a smarter, healthier and happier
life. Robots of all shapes and sizes are now entering into our day to day life,
whether at the shopping mall, hospital, museum, railway station, elderly care
facility, school or even within the home. These robots are increasingly being
deployed to learn and assist humans in many diﬀerent ways. The social intelli-
gence of the robots, along with the ability to connect and communicate with the
rest of the ecosystem, will be the paramount, and only possible as new and rele-
vant applications are created for these social and humanoid robots. As a leader
in the burgeoning robotics space, SoftBank Robotics is committed to developing
the best robotic experiences for the consumer. By reinforcing the potential for
humanoid robots, the experience of interacting with an intelligent, connected
form factor becomes second nature for society and more quickly accepted into
everyday life. In this regard, there is also a great need to bridge the two com-
munities: Robotics and ICT.
For example, the Pepper robot is currently deployed in thousands of homes
and public places; Romeo, a research prototype, aims to be daily life companion
for people needing physical and cognitive assistance; NAO is supporting both
research and youth education as a teaching tool for STEM programs and sup-
porting special education programs for children with ASD (Fig. 3).
www.ebook3000.com

44
A.K. Pandey
Fig. 3. Romeo, Pepper and NAO.
4
Some Key European Union Projects and Activities,
Breaking the Scientiﬁc and Technological Barriers
SoftBank Robotics is currently leading the way for social robots in the soci-
ety through its active involvement and contribution in many European Union
projects. For example, the MuMMER [12] project, putting Pepper robot in mall.
The project aims to improve the social interaction and engagement capabili-
ties of the robot and incorporate some social navigation skills. The L2ToR [13]
projects aiming to explore the role of social robot to teach second language to
children, whereas the DREAM [14] project aims to explore the role of a social
robot in robot-assisted therapy for children with Autism Spectrum Disorder
(ASD). The recently started CARESSES [15] project, aims to make the robot
culturally-intelligent to assist and accompany elderly people in more connecting

Socially Intelligent Robots, the Next Generation of Consumer Robots
45
and trustworthy manner. Romeo2 [16] projects explores the role of humanoid
robot for everyday companion for elderly people and to provide physical and
cognitive support.
There is also a huge ongoing eﬀort in Europe through partnership of euRo-
botics AISBL [17] (an international non-proﬁt association for all stakeholders in
European robotics) and European Commission, which aims to shape the future of
robotics in Europe through SPARC [18] (one of the largest civilian-funded robot-
ics innovation programme in the world) for Horizon 2020 (H2020). Within this
mechanism, there are various topic groups [19,20], which provides input for the
Multi-Annual Roadmap (MAR) and Strategic Research Agenda (SAR). Some
of these topic groups are also working on the aspects of social robots, e.g. the
topic groups on Socially Intelligent Robots and Societal Applications (SIRo-SA)
(coordinated by Amit Kumar Pandey), Natural Interaction with Social Robots
(coordinated by Vanessa Evers, M. Chetouani, Kerstin Dautenhahn), AI and
Cognition in Robotics (coordinated by Alessandro Saﬃotti, Markus Vincze), etc.
5
Conclusion
A very exciting, smarter and fun future of Human-Robot coexistence awaits.
Intelligent Service Robots in Smart ICT and IoT world are the Next Big things.
For this, we need to move towards a new kind of Intelligence: Social Intelligence,
which needs expertise from diverse domains. The key is to equip the robots with
the capabilities to reason about Humans and Human-Centered Environment. All
these also requires a new paradigm of Evaluation and Benchmarking for Social
Intelligence of robots.
Being so divers, there is a place to play the role by diﬀerent people, and
a multi-disciplinary eﬀort will be needed. We are at the beginning of Personal
Service Robots, a long way to go and many questions have to be addressed by
larger communities, e.g. social, legal, privacy, ethics related aspects of socially
intelligent robots.
References
1. Pandey, A.K., et al.: Towards socially intelligent robots in human centered envi-
ronment. PhD thesis, Toulouse, INSA (2012)
2. Pandey, A.K., Alami, R.: Mightability maps: a perceptual level decisional
framework for co-operative and competitive human-robot interaction. In: 2010
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pp. 5842–5848. IEEE (2010)
3. Pandey, A.K., Alami, R.: Taskability graph: towards analyzing eﬀort based agent-
agent aﬀordances. In: 2012 IEEE RO-MAN, pp. 791–796. IEEE (2012)
4. Pandey, A.K., Alami, R.: Aﬀordance graph: a framework to encode perspective tak-
ing and eﬀort based aﬀordances for day-to-day human-robot interaction. In: 2013
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
pp. 2180–2187. IEEE (2013)
www.ebook3000.com

46
A.K. Pandey
5. Pandey, A.K., Gelin, R.: Human robot interaction can boost robot’s aﬀordance
learning: a proof of concept. In: 2015 International Conference on Advanced Robot-
ics (ICAR), pp. 642–648. IEEE (2015)
6. Pandey, A.K., Ali, M., Alami, R.: Towards a task-aware proactive sociable robot
based on multi-state perspective-taking. Int. J. Soc. Robot. 5(2), 215–236 (2013)
7. Pandey, A.K.: Learning to be proactive. https://youtu.be/lw0tem5hStE
8. Pandey, A.K.: Learning task semantics. https://youtu.be/O7ZSNEYDuWQ
9. Pandey, A.K.: Learning aﬀordances and activities. https://youtu.be/Vnofhx4b6Us
10. Pandey, A.K., Gelin, R., Ruocco, M., Monforte, M., Siciliano, B.: When a social
robot might learn to support potentially immoral behaviors on the name of pri-
vacy: the dilemma of privacy vs. ethics for a socially intelligent robot. In: Privacy-
Sensitive Robotics 2017. HRI (2017)
11. Pandey, A.K.: Learning privacy vs. ethics for a socially intelligent robot. https://
youtu.be/Udwf-9iwmvY
12. MUMMER. http://mummer-project.eu
13. L2TOR. http://www.l2tor.eu
14. DREAM. http://www.dream2020.eu
15. CARESSESROBOT. http://www.caressesrobot.org
16. ROMEO. http://projetromeo.com/en
17. EU-Robotics. https://www.eu-robotics.net
18. SPARC-Robotics. http://www.sparc-robotics.net/
19. EU-Robotics. https://www.eu-robotics.net/eurobotics/topic-groups-/index.html
20. EU-Robotics.
https://www.eu-robotics.net/cms/upload/topic groups/
list-Topic-groups-2017-01.pdf

Proceeding Papers
www.ebook3000.com

Hand Gesture Recognition Using Deep
Convolutional Neural Networks
Gjorgji Strezoski(B), Dario Stojanovski, Ivica Dimitrovski,
and Gjorgji Madjarov
Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University,
Rugjer Boshkovikj 16, 1000 Skopje, Macedonia
{gjorgji.strezoski,dario.stojanovski,ivica.dimitrovski,
gjorgji.madjarov}@finki.ukim.mk
Abstract. Hand gesture recognition is the process of recognizing mean-
ingful expressions of form and motion by a human involving only the
hands. There are plenty of applications where hand gesture recognition
can be applied for improving control, accessibility, communication and
learning. In the work presented in this paper we conducted experiments
with diﬀerent types of convolutional neural networks, including our own
proprietary model. The performance of each model was evaluated on the
Marcel dataset providing relevant insight as to how diﬀerent architectures
inﬂuence performance. Best results were obtained using the GoogLeNet
approach featuring the Inception architecture, followed by our propri-
etary model and the VGG model.
Keywords: Gesture recognition · Computer vision · Convolutional
neural networks · Deep learning · Inception architecture · GoogLeNet
1
Introduction
Hand gestures provide a separate complementary modality to speech for
expressing ones ideas. Information associated with hand gestures in a con-
versation is degree, discourse structure, spatial and temporal structure. The
approaches present can be mainly divided into Data-Glove based and Vision
based approaches [1]. These two approaches are fundamentally diﬀerent due to
the diﬀerent nature of the sensory data collected. The Data-Glove based app-
roach collects data from sensors attached to a glove mounted on the hand of
the user. Using this methodology only necessary information is gathered, which
minimizes the need of data preprocessing and reduces the amount of junk data.
Nevertheless, using a Data-Glove in real life scenarios is often infeasible and
can present diﬀerent issues like connectivity, sensor sensitivity and many other
hardware related problems [2].
Vision based approaches on the other hand oﬀer the convenience of hard-
ware simplicity - they only require a camera or some sort fo scanner. This type
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 5

50
G. Strezoski et al.
of approach complements biological human vision by artiﬁcially describing the
visual ﬁeld. While this type of approach is way cheaper than its Data-Glove
counterpart, it produces a large body fo data that need to be carefully processed
in order to get only the necessary information. Having in mind that to tackle
this problem the recognition system needs to be insensitive to lighting condi-
tions, background invariant and also subject and camera independent [3]. Also
a challenging part of the hand gesture recognition problem is the fact that these
systems need to provide real-time interaction. While this does not aﬀect the
model training directly it implies that later classiﬁcation needs to be conducted
in a manner of milliseconds.
Given the constraints presented by the nature of hand gesture recognition,
a generally invariant approach is required which will retain consistent perfor-
mance in various conditions. In recent years deep learning has stepped up the
game when it comes to computer vision problems. Deep learning approaches
have shown to be superior in various computer vision challenges on multiple
topics. This spike in the performance of these models is partially due to the
recent advances in GPU design and architectures. GPUs are parallel in nature
and are especially well adjusted for training these types of models. While there is
a vast variety of deep architectures, research has shown that Convolution Neural
Networks (CNNs) are most applicable to computer vision problems. This com-
patibility rests on the biological similarity of convolutional neural networks with
the vision part of the human brain [4]. Having said that, humans have the most
sophisticated vision system, which similarly to convolutional neural networks
consists of hierarchically distributed layers of neurons which act as processing
units. Parameter sharing between neurons from diﬀerent levels in the structure
yield diﬀerent connection patterns with diﬀerent connection weights, which in
turn concludes the process with classiﬁcation.
Since these types of architectures have gained popularity during the past
few years, the industry leaders like Google, Nvidia, Microsoft, Deep Mind, IBM,
Clarifai and others have developed their own architectures designed to tackle
diverse problems. Most of these architectures are available for personal and aca-
demic purposes under open licenses ergo researchers and professionals alike can
modify the code, adjust the model and ﬁne-tune the existing parameters. There
is also a vast academic community which continuously pushes performance lim-
its of deep models. Berkeley developed Caﬀe which is one of the best performing
deep learning framework and Oxfords Visual Geometry Group introduced state
of the art performance with a weakly supervised deep detection architecture.
Similarly to these advances Microsoft released its deep learning ﬂagship CNTK,
Google released TensorFlow and Nvidia released the cuDNN framework which
optimizes GPU operations for maximum performance.
Having given a brief introduction into the ﬁeld, our research contributions to
this area are three fold:
• We evaluate several plain and pre-trained convolutional neural networks on
diﬀerent datasets and compare their performance.
www.ebook3000.com

Hand Gesture Recognition Using Deep Convolutional Neural Networks
51
• We train a robust deep model for hand gesture recognition with high accuracy
rate.
• We report good performance in a temporal manner with just 2 ms classiﬁca-
tion time on the fully trained model, making it a real-time functional model.
The rest of the paper is organized as follows. Section 2 outlines current
approaches on hand gesture recognition, with emphasis deep learning methods
and state of the art performance. Section 3 presents the details of the experi-
mental scheme and an overview of the pre-trained and plain models used in our
work. We present and elaborate on the performance achieved using every app-
roach and provide insight on the ﬁndings of our research in Sect. 4. Finally, we
conclude our work and discuss future development in Sect. 5.
2
Related Work
Hand gestures are a fundamental part in human-human communication [2].
The eﬃciency of information transfer using this technique of communication
is remarkable, therefore it has sparked ideas for utilization in the area of human-
computer interaction. For this to be possible the computer needs to recognize the
gesture shown to it by the person controlling it. That is the sole process of hand
gesture recognition. In a classical manner, the most common approach to solving
these types of problems is applying feature extraction techniques. A particular
technique is matching the image of the hand a predeﬁned template [5]. Template
matching has shown to be ineﬀective due to the high variety of environments,
hand forms and variations of diﬀerent gestures. Other classical approaches fea-
turing diﬀerent feature extractors have the ﬂaw of not being ﬂexible enough
to changing datasets and alternating conditions. In these cases the robustness
and invariance of the approaches with deep convolutional neural networks makes
them ideal candidates for these types of problems.
As we mentioned before, deep learning methods have been used to solve a
diverse ﬁeld of computer vision problems in recent years. When it comes to prob-
lems that are representable via images, the parallel nature of convolutional neural
networks allows them to elegantly apply to the matrix representation of the data.
Additionally, multi-column deep CNNs that employ multiple parallel networks
have been shown to improve recognition rates of single networks by 30–80% for
various image classiﬁcation tasks [6]. Neverova et al. [7] successfully combined
RGBD data from the hand region with upper-body skeletal motion data using
convolutional neural networks (CNNs) for recognizing 20 Italian sign language
gestures. However, their technique was intended for gestures performed indoors
only. Barros et al. [8] designed a Multichannel Convolutional Neural Network
(MCNN) which allows hand gesture recognition with implicit feature extraction
in the architecture itself. They report state of the art results on two dataset
containing images of static hand gestures. The ﬁrst dataset was generated using
a robot in laboratory conditions, mimicking real world scenarios with four types
of hand gestures. As a secondary dataset, they evaluated their system on data
containing ten diﬀerent hand gestures made in real, uncontrolled environments.

52
G. Strezoski et al.
Ohn-Bar and Trivedi evaluated various handcrafted spatio-temporal features
and classiﬁers for in-car hand-gesture recognition with RGBD data [3]. They
reported the best performance with a combination of histogram of gradient
(HOG) features and an SVM classiﬁer. Molchanov et al. fused information of
hand gestures from depth, color and radar sensors and jointly trained a convolu-
tional neural network with it. They demonstrated successful classiﬁcation results
for varying lighting conditions and environments [6]. In turn, the before men-
tioned eﬀorts provided the necessary background for conducting our experiments
and motivated our work.
3
Experimental Design
Recent advances in the design of models with deep architectures, especially con-
volutional networks have paved the way for a vast number of diﬀerent CNN
architectures designed to handle all sorts of data. Following the work in [6–11]
we decided to test the best performing models in some of the most challeng-
ing visual classiﬁcation tasks like the ImageNet Large Scale Visual Recognition
Challenge (ILSVRC), on hand gesture recognition. Furthermore, we propose our
own CNN designed with robustness and eﬃciency in mind.
3.1
Dataset
For the purpose of training and testing our model we used the Marcel dataset
which consists of 6 hand signs (A, B, C, FIVE, POINT, V) performed by 24
persons on three diﬀerent types backgrounds. Diﬀerent people and backgrounds
were used in order to increase diversity and information contained within the
dataset. In terms of background, the images in the Marcel dataset were recorded
in front of an uniform light background, uniform dark background and a com-
plex background [12]. Because of the diﬀerent people included in the creation
of this dataset there are also variabilities in hand shape and sizes. This dataset
results in a total of 4937 train images and 675 test images. For the testing and
validation of the diﬀerent models performance we used ﬁve fold cross-validation.
The distribution of images in each of the classes in both the training and testing
set are shown in Table 1.
3.2
Data Augmentation
Because the deep architectures that we trained in our experiments require a large
mass of data to train properly, we used data augmentation on the images in the
dataset. This was done in order to gain quantity while still introducing some
novelty in terms of information to our dataset. Our augmentation consists of
horizontal mirroring of every image in the training set, eﬀectively doubling the
size of the dataset [13]. Horizontal mirroring data augmentation is labeled label-
safe in this type of images. Additionally we trained our models using a gray-scale
representation, thus removing the color factor. Samples of this dataset on a light
plain background are illustrated in Fig. 1
www.ebook3000.com

Hand Gesture Recognition Using Deep Convolutional Neural Networks
53
Table 1. Number of images in each class
Train Test Total
A
1331
99
1430
B
489
104
593
C
573
116
689
FIVE
655
138
793
POINT 1396
121
1517
V
436
97
533
Fig. 1. Samples of the Marcel dataset on plain background
3.3
GoogLeNet
GoogLeNet is a deep convolutional neural network designed by Google featur-
ing their popular Inception architecture. This architecture not only allows for
approximation of a optimal local sparse structure by readily available dense com-
ponents but also reduces data dimensionality [9] wherever the computational
requirements would increase rapidly. GoogLeNet is the particular incarnation of
the Inception architecture that had the lowest error rate in the ILSVRC 2014
challenge.
We trained this model on the Marcel training dataset for 30 epochs with
a batch size of 16 images and a 20% step degradation function. The initial
learning rate was setup to 0.001 and because of the mixed presence of diﬀerent
backgrounds (complex, plain light, plain dark). Also we subtracted the total
RGB mean of the complete dataset before data iteration. Additionally all of
the images were maximized to a size of 256 × 256px to ﬁt the receptive ﬁeld of
the CNN. Whenever the original proportions of the input image could not be
maintained, cropping was introduced to the central region of the image. The
total training time of this model was 2 h on a NVidia GTX 980 Ti.

54
G. Strezoski et al.
3.4
AlexNet
Following its success on the ILSVRC-2010 challenge and relatively simple struc-
ture with just 5 convolutional layers and 3 fully connected layers, AlexNet pro-
vides the simplicity and eﬃciency of a shallow model combined with the predic-
tive performance of a deep model [10].
We trained this model for 20 epochs using an initial learning rate of 0.001
and a batch size of 16 images. In order to encourage faster learning we applied
an exponential learning rate degradation function with a gamma factor od 0.02.
In this case we also subtracted the mean ﬁle from the input images in both the
training and testing phase. The total training time of this model was 1 h on a
NVidia GTX 980 Ti.
3.5
LeNet
The LeNet model is speciﬁcally designed for handwritten and machine printed
character recognition. Having in mind the similarity of the characters with hand
gesture contours, whether printed or written, given the suitable preprocessing
this model should perform well. This model features 7 layers (not counting the
input layer) and a receptive ﬁeld of 28 × 28px. This receptive ﬁeld yields the
need of a region-of-interest (ROI) selector or a smart cropping mechanism, in
order to ﬁt the images into the input space of the model. After cropping the
images to their central section where the gesture is usually contained using PIL
in Python, a resize function scaled the images to 28 × 28 pixels.
For this experiment we trained the LeNet model for 35 epochs with a initial
learning rate od 0.01 and a step degradataion function of 25% step frequency.
3.6
VGG Net
The Visual Geometry Group model is described as a very deep convolutional
neural network [11] that has a ﬁxed input size of 224 × 224px RGB image. As a
preprocessing step in the training process of this network we subtracted the mean
RGB value, computed on the training set, from the input images. This model
features a small receptive ﬁeld and convolutional ﬁlters with 3 × 3px dimen-
sions. The stack of convolutional layers also contain spatial pooling layers with a
2 × 2px window, that passes the data with stride 2 [11]. After the convolutional
stack there is a series of three fully connected layers. The last fully connected
layer performs the classiﬁcation over the 6 classes.
We trained this model for 35 epochs with a batch size set to 128. Because this
network is deeper than most other architectures it takes less epoch to converge, so
during training we noticed convergence around the 17 epoch. Learning started
with a base learning rate of 0.001, which degraded with a step degradation
function 33%, 3 times on a regular interval.
www.ebook3000.com

Hand Gesture Recognition Using Deep Convolutional Neural Networks
55
3.7
Custom Model
Our custom model was originally designed for pixel based segmentation of
images. Since the process of segmentation essentially rests on pure classiﬁcation,
small corrections to the kernel and ﬁlter sizes of the architecture allowed for this
architecture to achieve relatively good performance in this task. This model has
13 layers that contain 5 convolutional and 5 pooling layers [13]. Before the soft-
max classiﬁer there is a fully connected layer aggregating the convolved features
generated to this point. Finally the input layer creates a 194 × 194px receptive
ﬁeld.
Generally, the advantage of this model is the smaller kernels and higher num-
ber of neurons per layer. This approach provided a better compromise between
training times and level of details. The smaller kernels are the culprit when it
comes to longer training times but the also capture smaller details than larger
ones would. Because training is slow the overﬁtting stadium of the training
period comes at a later stage.
Table 2. Layer conﬁguration in custom model
Type
Units
Kernel
0
Input
194 × 194 N/A
1
Convolutional
192 × 192 4 × 4
2
Max pooling
96 × 96
2 × 2
3
Convolutional
92 × 92
4 × 4
4
Max pooling
46 × 46
2 × 2
5
Convolutional
42 × 42
5 × 5
6
Max pooling
21 × 21
2 × 2
7
Convolutional
18 × 18
4 × 4
8
Max pooling
9 × 9
2 × 2
9
Convolutional
6 × 6
5 × 5
10 Max pooling
3 × 3
2 × 2
11 Fully connected 600
1
12 Softmax
6
1
This model was implemented in Berkeley’s Caﬀe framework using the Python
wrapper. Each of the neurons contained in this network relies on the Rectiﬁed
Linear Unit activation ﬁrs introduced in [10]. In the custom model classiﬁcation
is performed using a Softmax classiﬁer with 6 output neurons (one for each
class). Table 2 shows the models conﬁguration layer by layer with unit numbers
and kernel sizes. We performed experiments with adding various dropout layer
throughout the architecture for gaining sparseness and speeding up the training
process. These modiﬁcations to the architecture yielded worse performance in

56
G. Strezoski et al.
terms of accuracy so dropout was excluded from the ﬁnal design. A conclusion
that can be drawn from this is that the increased number of neurons per layer is
in fact helping with performance (due to smaller kernels) and holds no redundant
information.
As with the previous models, this model was trained no more than 25 epochs
in its best run. For increased performance (and reduced speed) we started train-
ing this model with an initial learning rate of 0.002 and degraded this rate using
a 20% step degradation function.
4
Results and Discussion
In this particular research, it was important to evaluate performance in both
accuracy and operation timing due to the potential of applying this types of
models in a real-time control scenario. In terms of accuracy, the GoogLeNet
model performed best with a Top-1 classiﬁcation accuracy of 78.22% and Top-3
classiﬁcation accuracy of 90.41%. While it is the best model in term of accuracy,
because of its depth and complexity, classiﬁcation and training times are the
longest with 4 min per epoch in a training setting and 2.8 ms propagation time
of a test image from the input layer to the end of the network.
Table 3. Accuracy scores for each models best run
Top-1
Top-3
GoogLeNet
78.22% 90.41%
AlexNet
42.18% 60.9%
Custom model 64.17% 84.32%
LeNet
28%
47.19%
VGG model
64.19% 83.33%
Table 4. Average classiﬁcation and training epoch duration on a GPU
Training epoch (min) Classiﬁcation per image (ms)
GoogLeNet
4
2.8
AlexNet
2
1
Custom model 2.5
0.5
LeNet
0.4
0.6
VGG model
5
2
The VGG model and our custom model have similar accuracies in both the
Top-1 and Top-3 categories, with 64.19% and 83.33% for the VGG model and
www.ebook3000.com

Hand Gesture Recognition Using Deep Convolutional Neural Networks
57
64.17% and 84.32% for our custom model respectfully. The AlexNet and LeNet
models performed signiﬁcantly worse than the previous three models as shown
in Tables 3 and 4.
When it comes to classiﬁcation and training duration, as expected the most
simple and shallow architectures provide the best timings in both areas. The
LeNet model performed best in terms of timing with 0.4 min per epoch in the
training setting and 0.6 milliseconds for predicting the class of a single image.
Nevertheless, its good performance on the timing evaluation, the bad classiﬁ-
cation accuracy makes this model unsuitable for any kind of real time control.
High error rate in classiﬁcation would make the system unreliable. On the other
hand the GoogLeNet model was the slowest of all tested models with 4 min per
training epoch and almost three milliseconds per classiﬁcation of a single image.
Our custom model and the VGG model had the best ratio of classiﬁcation accu-
racy and timing making them most suitable for real time use. The GoogLeNet
model would also perform well in this type of setting but it would require more
expensive hardware (GPU) and plenty of optimizations to gain in responsive-
ness. Responsiveness is a major concern in systems control and human-robot
interaction.
5
Conclusion and Future Work
Regarding future work in this direction, we are exploring methodologies for
improving our own model and its predictive power. Additionally, because of
the short classiﬁcation process duration, application of this model in a real-time
setting is part of our future work as well. This is possible with XBox Kinect
sensor technology, providing stable and consistent data feed to the model. Using
the XBox Kinect sensor we would also be able to generate our own datasets for
retraining and improving classiﬁcation rates.
An interesting future approach would be the development of a continuous
training of the model so that with each correct classiﬁcation we would adjust
the weights and activations of network it self. Basically that would enable the
model to get progressively better with time, without the need of a separate
training phase.
Acknowledgments. We would like to acknowledge the support of the European Com-
mission through the project MAESTRA Learning from Massive, Incompletely anno-
tated, and Structured Data (Grant number ICT-2013-612944).
References
1. Shastry, K.R., Ravindran, M., Srikanth, M., Lakshmikhanth, N., et al.: Survey on
various gesture recognition techniques for interfacing machines based on ambient
intelligence. arXiv preprint arXiv:1012.0084 (2010)
2. Singer, M.A., Goldin-Meadow, S.: Children learn when their teacher’s gestures and
speech diﬀer. Psychol. Sci. 16(2), 85–89 (2005)

58
G. Strezoski et al.
3. Ohn-Bar, E., Trivedi, M.M.: Hand gesture recognition in real time for automotive
interfaces: a multimodal vision-based approach and evaluations. IEEE Trans. Intell.
Transp. Syst. 15(6), 2368–2377 (2014)
4. Strezoski, G., Stojanovski, D., Dimitrovski, I., Madjarov, G.: Content based image
retrieval for large medical image corpus. In: Hybrid Artiﬁcial Intelligent Systems,
pp. 714–725. Springer, Heidelberg (2015)
5. Bilal, S., Akmeliawati, R., El Salami, M.J., Shaﬁe, A.A.: Vision-based hand pos-
ture detection and recognition for sign languagea study. In: 2011 4th International
Conference on Mechatronics (ICOM), pp. 1–6. IEEE (2011)
6. Molchanov, P., Gupta, S., Kim, K., Kautz, J.: Hand gesture recognition with 3d
convolutional neural networks. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops, pp. 1–7 (2015)
7. Neverova, N., Wolf, C., Taylor, G.W., Nebout, F.: Multi-scale deep learning for
gesture detection and localization. In: Computer Vision-ECCV 2014 Workshops,
pp. 474–490. Springer, Heidelberg (2014)
8. Barros, P., Magg, S., Weber, C., Wermter, S.: A multichannel convolutional neural
network for hand posture recognition. In: Artiﬁcial Neural Networks and Machine
Learning–ICANN 2014, pp. 403–410. Springer, Heidelberg (2014)
9. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9
(2015)
10. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in neural information processing systems,
pp. 1097–1105 (2012)
11. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
12. Marcel, S., Bernier, O., Viallet, J.E., Collobert, D.: Hand gesture recognition using
input-output hidden Markov models. In: FG, p. 456. IEEE (2000)
13. Strezoski, G., Stojanovski, D., Dimitrovski, I., Madjarov, G.: Deep learning and
support vector machine for eﬀective plant identiﬁcation. In: Proceedings of ICT
Innovations 2015 Conference Web Proceedings, pp. 221–233 (2015)
www.ebook3000.com

Computer-Based Statistical Description
of Phonetical Balance for Romanian Utterances
A. Cocioceanu1, T. Iv˘anoaica1,2, A.I. Nicolin1,2(B), and M.C. Raportaru1
1 Department of Computational Physics and Information Technologies,
Horia Hulubei National Institute for Physics and Nuclear Engineering,
Reactorului 30, Magurele, Romania
alexandru.nicolin@nipne.ro
2 University POLITEHNICA of Bucharest,
Splaiul Independentei 313, Bucharest, Romania
Abstract. Motivated by the advent of security solutions which rely on
voice biometrics, we revisit by means of extensive computer-based inves-
tigations the concept of phonetical balance for Romanian utterances.
We show that the standard distribution of phonems oﬀers only a par-
tial description of the phonetics of the language and that more detailed
statistical indicators are needed. To this end, we introduce a simple indi-
cator that measures vowel-consonant (or consonant-vowel) sequences and
analyze the distribution of consonant clusters for Romanian words. Our
results show that the distribution of consonant clusters is scale-free-like
(akin to the distribution of words and phrases in large texts) and that
large clusters of vowels or consonants are infrequent. This, in turn, indi-
cates that utterances consisting of words which are statistically unrep-
resentative with respect to the previous indicators are good candidates
for benchmarking the eﬃcency of voice biometrics solutions.
1
Introduction
One of the important changes imposed by the Digital Era concerns the way
in which we secure and have access to our assets. Traditionally understood
as a physical object that belongs to the owner, the key that grants access to
one’s assets has gradually shifted towards something the owner knows and, more
recently, towards who the owner is. In fact, the etymology of the word shows the
key as a metal piece for opening locks (via Middle English keie cognate with the
Middle Low German keie, which means lance or spear), emphasizing the physi-
cal nature of the object. The Digital Era, however, has transformed the key into
a piece of information, something that (only) the owner knows, usually in the
form of an alphanumeric sequence that the owner provides before accessing some
digital resources, such as ﬁnancial data in e-banking systems, medical ﬁles on
e-health platforms, personal data on cloud storage provides, etc. [1]. In a way,
the plethora of digital security solutions, in particular the software ones, such as
the hashing methods used to conceal the digital keys, the mechanisms and algo-
rithms used to tunnel and wrap them, the certiﬁcation systems used to secure
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 6

60
A. Cocioceanu et al.
the data transfers, etc., mask the central position that the alphanumeric digital
keys currently have in the information ecosystem. Moreover, although numerous
hybrid authentication methods exist, e.g., the two-factor authentication, where
the addition of a new recognition method is improving the authenticity of the
owner’s identity [2], we observe that one component of this scheme remains
invariant: the (tried and tested) alphanumeric key.
Naturally, the assets themselves have shifted towards the digital realm as well,
as have many of our activities. In fact, digital technologies have substantially
changed the way we socialize and entertain ourselves, the way we work and
learn, and so on, and we now have a new discipline, namely digital sociology,
solely dedicated to the way these technologies are impacting our everyday life [3].
Finally, the past decades have placed more emphasis on (implicitly auto-
mated) biometrics security solutions, which allow the owner of some assets to
access them not by using something that he possesses or something that he
knows, as done in the past, but rather by identifying himself as the owner [4].
The term biometrics itself is deﬁned as “automated recognition of individuals
based on their behavioral and biological characteristics” (see ISO/IEC JTC1
SC37), the most common biometric traits used for authentication of users being
the ﬁngerprint, the face, the iris, the palm-print, the retina, and the voice. In
fact, any human physiological and/or behavioral characteristic can be used as
a biometric characteristic, as long as it satisﬁes a series of requirements such
universality, i.e., every person using the system should possess the trait, unique-
ness, i.e., the trait should be suﬃciently diﬀerent for individuals in the relevant
population such that they can be distinguished from one another, etc., but at
the moment the voice is one of the most used traits [4].
As part of building a voice-biometrics identiﬁcation system which is largely
text-independent (i.e., no pass phrase) and shows little sensitivity to ambiental
noise [5], we revisit by means of extensive computer-based investigations the con-
cept of phonetical balance for Romanian utterances. The goal of our investigation
is to have statistical descriptors of the phonology of the Romanian language that
will be helpful in the development stages of the aforementioned voice-biometrics
identiﬁcation system. To this end, we go beyond the standard distribution of
phonems and analyze the distribution of consonant clusters for Romanian words
to identify the most important ones. Moreover, we propose a simple indicator
that measures vowel-consonant sequences to show that large clusters of vowels
or consonants are infrequent.
2
Distribution of Phonems
While the mathematics behind the distribution of phonems in a given text is
relatively simple, the main technical challenge comes from ﬁnding a set of texts,
usually very large, that are representative for the language under scrutiny. In the
case of Romanian language [6] the text was acquired using the Web-as-resource or
Web-as-corpus approach (considering as sources mainly online Romanian news-
papers and transcripts of the discussions in the European parliament) [7], which
www.ebook3000.com

Computer-Based Statistical Description of Phonetical Balance
61
produced more than 9 million phrases, the largest Romanian plain text corpus
to date [7]. The results of this analysis clearly show that the 34 phonems iden-
tiﬁed are qualitatively diﬀerent, some of them being very common, while others
somewhat infrequent. In fact, the ﬁrst six phonems correspond to more than
50% of the entire phonem usage, while the last six phonems have an occurrence
frequency in between 0.27% and 0.03%.
The aforementioned distribution of phonems (complemented with related
results in Ref. [8]) gives an accurate global description of the Romanian language
which can be now compared with languages (such as English and French) for
which such statistical descriptions have a long history. We note, however, that
in the case of smaller texts this statistical description is insuﬃcient, as two
small texts of similar if not identical distribution of phonems may be, in fact,
substantially diﬀerent from a phonetic point of view. As an elementary example
we note here two simple Romanian sentences, namely S1: “Oaia e proast˘a” (in
English: The sheep is dumb) and S2: “Ia ta e poroas˘a” (in English: Your blouse
is porous), which consist of the same phonems but diﬀer considerably. The ﬁrst
sentence, for instance, has a series of four subsequent vowels (namely “oaia e”)
and a consonant cluster of two letters (namely “st”), while the second has no
consonant cluster and the longest series of vowels is of length two (namely “ae”
and “oa”).
In the language of statistical physics, if the statistical ensemble is very large,
than the distribution of phonems becomes the key descriptor of the text under
scrutiny, as all other features (say, vowel and consonant clusters, distribution
of words, and so on) average out. For short texts, however, the distribution of
phonems should be complemented with additional information about the vowel-
consonant sequences and the types of consonant clusters.
3
Distribution of Consonant Clusters
Motivated by the previous example we embarked on a detailed statistical study of
the Romanian vocabulary using the database of Dexonline [9], which is an open
source collection of the main dictionaries of the Romanian language. For our
analysis we used a set of more than 90.000 words, which roughly correspond to
the lexicon in Ref. [10], which is the main dictionary of the Romanian language.
Through a thorough statistical analysis we identiﬁed all two-consonant clus-
ters, independent of their position in a word, and ranked them according their
occurrence race. To simplify the analysis, the results for the letter s include also
the letter s,, while those for the letter t also include the letter t,. The bubble-plot
in Fig. 1 shows the main consonant clusters, indicating their occurrence through
the size of the bubble. The main message of the plot is that there are a few
frequent consonant clusters (such as “st”, “nt”, “tr”, “pr”, etc.) which appear in
5% to 10% of all words in the Romanian vocabulary and numerous other infre-
quent clusters (such as “lb”, “sf”, etc.). A rapid inspection of the plot shows
the tendency to have consonant clusters using the letters in the second half of
the consonant series. Moreover, there is a clear assymetry with respect to the

62
A. Cocioceanu et al.
b
c
d
f
g
l
m
n
p
r
s
t
b
c
d
f
g
l
m
n
p
r
s
t
Fig. 1. Bubble plot of the main two-letters consonant clusters observed in Romanian
words. The size of the bubble is proportional to the number of words in which the
cluster appears.
ﬁrst diagonal, meaning that the frequency rates of a given cluster and its inverse
are substantially diﬀerent. To understand this property let us look at the “tr”
cluster which is more frequent than the “rt” cluster, or, more clearly at the “st”
and “ts” ones. The ﬁrst order of the consonants, “st”, corresponds to one of the
most important Romanian cluster, while “ts” is inexistent.
What is more interesting is that the distribution of the consonant clus-
ters follows a free-scale-like distribution. Taking P(k) as the probability that a
given cluster appears in k words, we observed (see Fig. 2, the upper panel) that
P(k) ≈k−γ where γ ≈3.2. This statistical behavior shows a striking resem-
blance to the so-called Zipf law which states that the frequency of a given word
is inversely proportional to its rank in the frequency table [11]. A classical textual
econometrics study on the Brown Corpus of American English showed “the” as
the most frequent word in the vocabulary (with an occurrence rate of almost
7%), “of” as the second most frequent word with an occurrence rate of roughly
3.5%, etc. Similarly, it has been shown that the distribution of word sequences
(the so-called n-gramas) follows the same pattern, provided the reference texts
are large enough [12]. The key feature of the Zipf law is that on a log-log plot
the distribution is linear with a negative slope, which is similar to what we
noticed in Fig. 2 (the upper panel). In our case “st” was the most frequent con-
sonant cluster with a frequency rate of 9.4%, “nt” was the second most frequent
cluster (8.5% frequency rate), “tr” was the third one (7% frequency rate), etc.
This distribution is typical to many systems, ranging from social networks (such
as the collaboration of movie actors in ﬁlms and the co-authorship of papers),
the internet, the protein-protein interactio, etc. [13]. While Zipf’s law has been
veriﬁed in numerous contexts the mechanisms behind it remain largely elusive,
despite numerous models which capture some of its features. Zipf, for example,
www.ebook3000.com

Computer-Based Statistical Description of Phonetical Balance
63
# of words in which a cluster appears
2000
3000
4000
5000
6000
7000
8000
9000
Probability
10-3
10-2
10-1
100
# of words in which a cluster appears
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
Probability
0
0.2
0.4
0.6
0.8
1
Fig. 2. (Upper panel) Distribution of the probability that a given cluster appears in
a given number of Romanian words. Please note that the plot is log-log. The dashed
line shows the linear ﬁt (in log-log axes) of the function and corresponds to a slope of
−3.2. (Lower panel) The same distribution as in the upper panel on a graph without
log-log axes. Please notice the rapid decay of the distribution for a number of words
larger than 2000.
understood the law through the principle of least eﬀort, which has been often
revisited by means of advanced mathematical models [14], while others consider
the preferential attachment mechanism which basically says that the speakers
tend to use some words more often than others [15].
It is tempting to see the correlation between the frequency rates of the con-
sonant clusters and their etymology, but such an analysis is of little insight,
as the Romanian dictionaries only record the language from which a given
words entered into the Romanian vocabulary, and not the language of origin
[16]. Finally, we note that the distribution of three- and four-letters consonant
clusters brings only a small correction to the aformentioned statistics and that
a detailed study will be reported elsewhere.
4
Phonetical Balance
The previous discussion on consonant clusters brings some clariﬁcation to the
phonology of Romanian language, but an instrument is needed to quantify the
vowel-consonant sequences. To this end, we introduce for each word in the vocab-
ulary the function

64
A. Cocioceanu et al.
m
-1
-0.5
0
0.5
1
# words
×104
0
1
2
3
4
Fig. 3. Distribution of the number of Romanian words as function of the parameter
m introduced in Eq. (1). Please note that because of the low number of bins in the
histogram it appears, quite erroneously (see Fig. 4), to have a continuous transition
from one set of m-values to another. The plot is, however, extremely useful because it
allows an easy assessment of the average value of m, which is around −0.5.
m =
1
n −1
n−1

j
l(j)l(j + 1),
(1)
where n is the number of letters of the word and l is a boolean function equals
to 1 if the j-th letter is a vowel and −1 if the j-th letter is a consonant. Please
note that a normalization factor equal to 1/(n −1) has been introduced in the
deﬁnition of m such that from the values of m one can directly compare words
of diﬀerent lengths. For words consisting of perfect sequences of vowels and
consonants such as “calamitate” (in English: calamity), “repetare” (in English:
repetition), “sare” (in English: salt), etc., the m-function is numerically equal to
−1, while for the (admittedly fewer) words consisting almost entirely of vowels
or consonants, such as “oaie” (in English: sheep) and “ou˘a” (in English: egs) on
the vowel side and “strˆamb” (in English: crooked) and “prompt” (in English:
prompt) on the consonant side, the m-function is always larger than 0. In fact
for “oaie” and “ou˘a” the m-function is equal to exactly +1 but this is valid for
a very short list of words.
In Figs. 3 and 4 we show the distribution of the number of Romanian words
as function of the parameter m, using a histogram plot with diﬀerent number of
bins, to show that the majority of words corresponds to negative values of m (m,
the average value of m being, in fact, very close to −0.5), thereby indicating that
most Romanian words are structured as (slightly) imperfect vowel-consonant
sequences. Figure 4, in particular, shows extremely clearly that there is a large
set of words, around 18.5% of the investigated vocabulary, which corresponds to
m = −1.
www.ebook3000.com

Computer-Based Statistical Description of Phonetical Balance
65
m
-1
-0.5
0
0.5
1
# words
×104
0
0.5
1
1.5
2
Fig. 4. Detailed distribution of the number of Romanian words as function of the
parameter m introduced in Eq. (1). Please note the isolated cluster close to m = −1
and the bunch in between m = −0.8 and m = 0. From the graph one can easily assess
that there are around 17.500 words with an m-value very close to m = −1.
This shows, incidentally, that the ﬁrst sentence discussed in Sect. 2 consists
of words from the tail of the distribution of m (though the consonant cluster
is one of the very frequent ones), while the second sentence consists of words
from the bulk part of the distribution. Let us also note that approximately 26%
of the Romanian words correspond to m < −0.75. Finally, let us mention that
it is very tempting to compute the global m value considering the reported
frequencies of Romanian words [17], but the computations were done before the
advent of the computer and the validity of some of the reported frequencies has
been questioned [16]. We can, however, use Eq. (1) for entire sentences, just like
for words, and obtain mS1 = 3/11 for the ﬁrst sentence and mS2 = −5/11,
for the second one, which indicates that the ﬁrst sentence is less representative
in a statistical sense with respect to the vowel-consonant (or consonant-vowel)
sequences than the second one.
5
Conclusions
In this paper we have analyzed by computational means the phonetical balance
of Romanian words and introduced two indicators that go beyond the stan-
dard distribution of phonems. We have shown that the distribution of consonant
clusters in Romanian words obeys a scale-free-like distribution and that large
clusters of vowels or consonants are infrequent. The distribution of consonant
clusters is similar to the well-known Zipf law that gives the distribution of words

66
A. Cocioceanu et al.
and short sentences in that it shows that there are a few very frequent con-
sonant clusters and numerous others which are considerably less frequent. Our
results suggest that a reliable voice-based biometrics solution should be partic-
ularly benchmarked against utterances which consist of words with infrequent
consonant clusters and words with positive m-values, as their statistical unrep-
resentativeness makes them good candidates for identifying the ﬂaws of a given
biometrics solution.
As a natural extension of this work we intend to reﬁne the current results
by taking into account the position of a consonant cluster with respect to the
syllables of a word. Moreover, future research should be focused on a global
indicator (such as the Shannon entropy) which considers not only the relations
between the nearest neighbours letters, but also long-range in-word correlations
between letters and clusters.
Acknowledgement. The work was supported by the Horizon 2020 SpeechXRays
project. This project has received funding from the European Union’s Horizon 2020
research and innovation programme under grant agreement No. 653586. The authors
thank Dan Caragea and Mihai B˘arbulescu for insightful discussions.
References
1. Bonneau, J., Herley, C., van Oorschot, P.C., Stajano, F.: Passwords and the evo-
lution of imperfect authentication. Commun. ACM 58, 78–87 (2015)
2. See, for example, Apple’s Touch ID, the biometric ﬁngerprint plus PIN code, the
Google 2-Step Veriﬁcation, and the user telephone number plus user password
3. Lupton, D.: Digital Sociology. Routledge, Abingdon (2014)
4. Jain, A.K., Flynn, P., Ross, A.A. (eds.): Hanbook of Biometrics. Springer, New
York (2008)
5. See, in particular, the SpeechXRays project (http://www.speechxrays.com) which
“will develop and test in real-life environments a user recognition platform based
on voice acoustics analysis and audio-visual identity veriﬁcation” which uses “text-
independent speaker identiﬁcation (no pass phrase)” and “low sensitivity to sur-
rounding noise” (as retrived June 2016). The testing will be done in Greek and
Romanian (2016)
6. See a detailed description of the Romanian language on its Wikipedia entry at
https://en.wikipedia.org/wiki/Romanian language
7. Stanescu, M., Cucu, H., Buzo, A., Burileanu, C.: ASR for low-resourced languages:
building a phonetically balanced Romanian speech corpus. In: 20th European Sig-
nal Processing Conference (EUSIPCO 2012) (2012). ISSN 2076-1465
8. Stanescu, M., Buzo, A., Cucu, H., Burileanu, C.: Statistical phonetic analysis of
the Romanian language for speech recognition and synthesis tasks. In: 54th Inter-
national Symposium ELMAR (2012)
9. The Dexonline dictionary is publicly available at https://dexonline.ro and consists
of more than ﬁfty distinct dictionaries (see the complete list at https://dexonline.
ro/surse) which have a very broad coverage. The dictionaries go from general-use
prescriptive dictionaries and thesauruses to topical and orthographic dictionaries
10. Explanatory Dictionary of the Romanian Language (in Romanian), Romanian
Academy, “Iorgu Iordan” Liguistic Institute, Editura Univers Enciclopedic, 2nd
Edn. (1996)
www.ebook3000.com

Computer-Based Statistical Description of Phonetical Balance
67
11. Zipf, G.K.: Human Behavior and the Principle of Least Eﬀort. An Introduction to
Human Ecology. Addison-Wesley Press, Boston (1949)
12. Ha, L.Q., Sicilia-Garcia, E.I., Ming, J., Smith, F.J.: Extension of Zipf’s law to words
and phrases. In: Proceedings of 19th International Conference on Computational
Linguistics - COLING (2012)
13. Barabasi, A.-L., Reka, A.: Emergence of scaling in random networks. Science 286,
509–512 (1999)
14. i Cancho, R.F., Sole, R.V.: Least eﬀort and the origins of scaling in human lan-
guage. Proc. Nat. Acad. Sci. 100, 1788–1791 (2003)
15. Lin, R., Ma, Q.D.Y., Bian, C.: Scaling laws in human speech, decreasing emergence
of new words and a generalized model (2015). arXiv:1412.4846v2
16. Dinu,
M.:
Personalitatea
Limbii
Romˆane,
Cartea
Romˆaneasc˘a
(1996).
(in
Romanian)
17. Juilland, A., Edwards, P., Juilland, I.: Frequency Dictionary of Romanian Words.
Mouton & Comp., Clearwater (1965)

Distributed Private Key Generator for ID-Based
Public Key Infrastructure
Pance Ribarski(B) and Ljupcho Antovski
Faculty of Computer Sciences and Engineering, Ss. Cyril and Methodius University,
Skopje, Macedonia
{pance.ribarski,ljupcho.antovski}@finki.ukim.mk
Abstract. We recognize the need of certiﬁcateless PKI to reduce the
step of obtaining the public key. This leads to ID-Based cryptography
where we have PKI with full power to generate private keys for any iden-
tity. We solve this problem by implementing distributed key generation
to form a group of players which will act as private key generator for
ID-Based PKI. The implementation is done on the Android platform,
showing the possibilities of running PKI on cheap and widely available
hardware.
Keywords: Distributed key generation · ID-based cryptography · PKI ·
Implementation · Android
1
Introduction
Ever since 1984 when Shamir [15] introduced the notion of identity-based public
key encryption (IBE), this new scheme was fully adopted and researched through
the years. Shamir’s paper was only the introduction and left open questions for
future research on the topic. His work only covered ID-Based signatures, not full
ID-Based cryptography. Papers followed immediately in 1987 [6,16], in 1989 [17]
and in 1991 [12]. However, all these schemes were unsatisfactory. The ﬁrst fully
functional IBE scheme was proposed by Boneh and Franklin [3] in 20011.
Shortly, in IBE scheme the public key is a well-known string that is tied to
the identity of the client. This string can be the client’s email, telephone number,
website, or some other string that is well-known in the protocol where IBE is
used. This leads to the notion of certiﬁcateless public key crypto (certiﬁcateless
PKC). Obviously, the positive thing here is removing the need for obtaining cer-
tiﬁcates from public key infrastructure (PKI). But, the downside is the extreme
power given to the private key generator (PKG) of the ID-Based PKI. In this
paper we introduce a solution to this problem by implementing real-world dis-
tributed key generation scheme (DKG scheme) on the Android platform. We are
showing that everyday mobile devices can be used as distributed PKG in cer-
tiﬁcateless PKI. Afterwards, the given solution can be plugged in any protocol
that employs ID-Based PKC in the mobile world.
1 Please refer to [10] for more extensive overview on IBE schemes and papers.
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_7
www.ebook3000.com

Distributed Private Key Generator for ID-Based Public Key Infrastructure
69
Related Eork. There has been a lot of work towards distributed key generation.
One of the earliest works on distributed key generation is Pedersen’s veriﬁed
secret sharing in 1992 [13]. His work shows how to compute on shared secrets,
thus opening the way for distributed key generation. Following his work, Gennaro
published papers in 1997 and 1998 on the topics of VSS and distributed key
generation for discrete-log cryptosystems [8,9]. After them, there are papers
starting to investigate asynchronous secret sharing and DKG systemts [4]. These
papers are followed by papers with real implementation of DKG systems for
the internet [11]. Afterwards, some authors started using bivariate functions
in VSS schemes for more eﬃcient distributed key generation schemes [1,18].
Our work is based on the research of Pedersen on veriﬁable secret sharing and
Gennaro’s papers for discrete-log distributed key generation schemes. Our aim
is to implement a real-world distributed key generation algorithm in ID-Based
PKC scenario, having in mind mobile devices.
Organization of this Paper. The Sect. 2 introduces secret sharing theory, the
notion of secure secret sharing and secret sharing without honest dealer. This
section also covers the introduction of ID-Based cryptography and the open
problem we are trying to solve. The Sect. 3 gives solution to the open problem
with ID-Based single center of power PKG. It outlines the protocol, the PKG
initialization and the key extraction protocol. Finally, the Sect. 4 gives ﬁnal words
on the topic.
2
Basics
2.1
Secret Sharing
The notion of secret sharing means dividing a secret information into pieces and
giving those pieces to players in the secret sharing protocol. After the sharing
process, every player has a piece of the secret information without having any
knowledge about the secret. When enough players gather together, the secret
information can be reconstructed. If less than enough players gather, the secret
information can not be reconstructed. This neccessary number of players is called
a threshold value.
The roles in the secret sharing protocol are a Dealer and Players. The Dealer
has the secret information, divides it into pieces and shares the pieces to the
Players. During the process of dividing the secret, the Dealer decides on the
threshold value and the number of Players. Because the Dealer has the secret in
the beginning, it is considered that he/she is honest, and further more, forgets
the secret immediatelly after the dealing. When the number of gathered play-
ers is greater or equal than the threshold value, then the secret information is
reconstructed by combining their pieces.
Deﬁnition 1. A secret scharing scheme (t, n) is the process of sharing a secret
S into pieces S1, S2, . . . , Sn in a way that:

70
P. Ribarski and L. Antovski
– Any t gathered pieces can reconstruct S;
– Any t −1 or less pieces can not give away any information about S.
The secret sharing scheme consists of two phases:
– Sharing phase: the Dealer divides the secret S and shares piece Si to Player
Pi, i = 1, . . . , n;
– Reconstruction phase: all players gather their pieces, at least the threshold
number of pieces will reconstruct S.
This is called a secret sharing scheme with n players and threshold value t.
The secret sharing was invented in 1979 by Shamir [14] and by Blakley [2]
independantly from each other. The concept is used where sensitive data is
used and they should not be visible all the time. This refers to, for example,
cryptographic keys, secret documents, PIN numbers and other similar types
of data. Using the secret sharing scheme (see Deﬁntion 1), we can split this
sensitive data across more data centers (or databases). In this way, not one (or
any number lower than the threshold value) data center can access the data.
Only when enough centers gather (the threshold value), the sensitive data can
be reconstructed and used.
Adi Shamir’s Secret Sharing. The secret sharing scheme from Shamir [14]
gives a way to share a secret with threshold value t. The scheme is using the fact
that exactly t points are enough to deﬁne a polynom of degree t −1. Knowing
this, we can deﬁne the Shamir’s scheme for secret sharing.
Let n be the number of players in the secret sharing protocol. Let s be
the secret that needs to be shared with the players in the execution of the
protocol. Let t be the threshold value, i.e. the lowest number of players needed
for reconstruction of the secret. This is called a (n, t) secret sharing scheme. The
Dealer of the secret deﬁnes the polynom f(x) as follows:
f(x) = s + a1x + a2x2 + a3x3 + . . . + at−1xt−1
(1)
The coeﬃcients a1, a2, a3, . . . , at−1 are random integers, and the zero coeﬃ-
cient is the secret s that needs to be shared. The Dealer calculates pieces of the
secret for each Player Pi, i = 1, . . . , n as (i, f(i)), i = 1, . . . , n. The Dealer sends
the pair (i, f(i)) to the player Pi. The data transfer must be secure, nobody
must gain access to the data for the Player Pi except Pi.
Since exactly t points are enough for deﬁning a polynom of degree t −1, we
need at least t points (t pairs given to the Players) to reconstruct f(x). We do
this by using Lagrange interpolation of polynomials. The interpolated polynom
over the given points is the linear combination:
L(x) =
t

j=1
f(ij)lj(x)
(2)
www.ebook3000.com

Distributed Private Key Generator for ID-Based Public Key Infrastructure
71
where lj(x) is the Lagrange base polynom:
lj(x) =

1≤m≤t, m̸=j
x −im
ij −im
, j = 1, 2, . . . , t
(3)
By solving L(x) we actually get interpolated f(x). But, the interest in the
interpolation should be diverted only to the zero coeﬃcient - the secret s. There
is no need to calculate the other coeﬃcients in f(x), those are random integers
that are not connected to the secret. To calculate only the zero coeﬃcient we
have the following:
lj(0) = (−1)t−1

1≤m≤t, m̸=j
im
ij −im
, j = 1, 2, . . . , t
(4)
Veriﬁed Secret Sharing (VSS). The secret sharing scheme allows sharing a
secret to players in the protocol. In this simple case (as the Shamir’s scheme),
the players are supposed to be honest and play along correctly in the protocol.
In scenarios where we have a malicious dealer which does not want the secret to
be reconstructed, the dealer can make deliberate mistakes in the sharing phase.
In these scenarios we want to the use the so-called veriﬁed secret sharing. It was
ﬁrst introduced in 1985 by Chor et al. [5].
To implement a VSS we need an extension in the sharing phase. In this phase,
the players can make more rounds of communication to verify the received pieces
of the secret information. If the dealer is malicious and gives wrong pieces of the
secret information, this phase can detect that and stop the execution of the
protocol. If all the shared pieces pass this veriﬁcation test, then we can calculate
the secret if enough players give their pieces.
Feldman VSS. The Feldman veriﬁed secret sharing scheme [7] is an upgrade
over the Shamir secret sharing scheme [14]. Feldman used the homomorphic
property to get a veriﬁed secret sharing scheme. Just like in the Shamir’s
scheme, we deﬁne n, s and t to be the number of players in the protocol,
the secret which has to be shared and the threshold value of the number of
players. Then, the Dealer deﬁnes the polynomial f(x) as in Eq. 1 where the
coeﬃcients a1, a2, a3, . . . , at−1 are random integers and the zero coeﬃcient s
is the secret. Then, the Dealer calculates the pieces for the players as pairs
(i, f(i)), i = 1, . . . , n. Additionally, the Dealer calculates the commitments to
the coeﬃcients of f(x) and the evaluation in i: c0 = gs, c1 = ga1,. . . , ct−1 = gat−1
and di = gf(i).
After this, the Dealer sends the point (i, f(i)), the commitments ci and d to
the Player Pi. The Player Pi is now in position to verify that f(i) is really the
evaluation of f(x) in i by calculating:
c0ci
1 . . . cit−1
t−1 =
gs(ga1)i . . . (gat−1)it−1 =

72
P. Ribarski and L. Antovski
gsga1i . . . gat−1it−1 =
gs+a1i+...+at−1it−1 =
gf(i) = di
By using the homomorphic property of the exponents in the group G it is pos-
sible to get a veriﬁcation scheme. The diﬀerence between the Pedersen’s scheme
and the Shamir’s scheme is that here the Dealer sends additional t elements
from the group G, the Dealer makes additional t exponentiations and the Player
Pi computes additional t exponentiations and t multiplications of the elements
from G.
Sharing Without an Honest Dealer. Shamir’s and Feldman’s schemes
require an honest Dealer. Some protocols require secret sharing scheme where
the Dealer does not have to be honest. These schemes are usualy employed in
DKG protocols. Pedersen ﬁrst presented this kind of scheme in 1992 [13]. The
additional property over the previous secret sharing scheme allows use of this
scheme in protocols where you can not trust any entity to know the secret in the
phase of sharing. In this scheme nobody knows the secret S until the reconstruc-
tion phase. In Pedersen’s secret sharing scheme [13] there is actually execution
of n parallel secret sharing schemes without an honest dealer. Every Player Pi
shares their secret Si to the other players. Every Player gets n −1 other pieces
of shared secrets. The shared secret S across the group of Players is some arith-
metic operation over all the Si pieces (for example, sum of all Si). If the Players
have to do some operation with S (for example extract a key in ID-Based cryp-
tography), each Pi does operation over their own Si to contribute to the ﬁnal
result.
2.2
ID-Based Cryptography
In public crypto every player owns private and public key. The private key should
be secured and the public key is shared with everybody. When Player A wants
to send message to Player B, he/she encrypts it with the public key of the Player
B. When Player B wants to decrypt the message, he/she uses the private key of
Player B. If the Player A wants to prove the authenticity of the message, he/she
signs it with the private key of the Player A, and when Player B wants to check
whether the message is from Player A, they use the public key of Player A.
In the above scenarios we see that both players have to obtain the public keys
from each other in some way. If the players communicate often, then the price of
obtaining keys is negligible. But, if the protocols assume short communication
between players that interchange all the time, then the price of obtaining the
public keys becomes substantial.
In scenarios where the players are all part from some organization, then
this organization creates an entity which helps managing the keys. This entity
is called Public Key Infrastructure (further in the text PKI). Having a PKI,
players are enabled players to obtain public keys from other players easily and
www.ebook3000.com

Distributed Private Key Generator for ID-Based Public Key Infrastructure
73
in a secure way. In the PKI infrastructure there is a Certiﬁcate Authority that
signs the public key of the members of the PKI. The signatures and the public
key that are tightly coupled to speciﬁed player are called a certiﬁcate. If there
is a breach in the private key for some member, then the PKI will issue a new
certiﬁcate with the new public key for the member, i.e. revoke the old key. The
cryptography that is based on PKI is called a certiﬁcate-based cryptography.
The ID-Based cryptography is tackling the problems of certiﬁcates. By intro-
ducing an identiﬁcation as public key, the ID-Based cryptography eliminates the
need for certiﬁcates. Therefore, it is also called certiﬁcateless cryptography. The
elimination of certiﬁcates is a big advantage for ID-Based cryptography. But,
the design of the PKI infrastructure for ID-Based cryptography opens another
problem. The Private Key Generator (further in the text as PKG), is the entity
that generates private keys for given identities. This means that the PKG has the
power to generate private key for any identity. The PKG has substantially more
power than the Certiﬁcate Authority in regular PKI. Solution to this problem is
given in [3] and states that the master private key of the PKG should be divided
amongst more entities in such way that generating a private key for given iden-
tity would require all of them or certain threshold value of them. This paper
oﬀers implemented solution to divide the center of power in the DKG by using
secret sharing without honest dealer for sharing the master key of the DKG.
3
Implementation
We implemented DKG for ID-Based PKG, eﬀectively rendering a secure certiﬁ-
cateless API. First we deﬁne the following:
Deﬁnition 2. Secure PKI for ID-Based cryptography represents a system for
Private Key Generator with the following characteristics:
– The system consists of n players with t threshold value,
– There is no player that has the power to generate private key for any given
identity,
– Only t or more players can generate private key for a given identity.
1 Generate fi(x):
- Choose at random ai,j, j = 0, ..., t −1 the coeﬃcients of fi(x) where the
zero coeﬃcient is the s ecret of the DKG Player Pi
2 Calculate the commitments:
- Calculate the commitments of the coeﬃcients of fi(x):
ci,j = g publicsi, j = 0, ..., t −1
3 Evaluate the polynomial:
- Evaluate the polynomial for the other players
evali = fi(j), j = 1, ..., n, j ̸= i
Scheme 3.1. Initialization step for DKG Player Pi

74
P. Ribarski and L. Antovski
1 Share the commitments:
- DKG Player Pi prepares commitments ci,j, j = 0, ..., t −1 for
DKG Player Pj, j = 1, ..., n, j ̸= i
2 Share the evaluations:
- DKG Player Pi prepares the evaluation of the polynom fi(j) for DKG Player
Pj, j = 1, ..., n
3 Send the message:
- DKG Player Pi sends the message commandReceivePieces to every
DKG Player Pj, j = 1, ..., n, j ̸= i
Scheme 3.2. DKG Player Pi is sharing the commitments and evaluations
The proposed system uses elliptic curves and pairings over elliptic curves.
We use the pairing-friendly Barreto-Naehrig family of curves. In the following
subsections we deﬁne the protocol, the setup of the system and the extraction
of keys.
Protocol. The proposed protocol assumes synchronious communication where
the channels between the players are secure and untappable. The roles in the
protocol are DKG Player and Client. The DKG Players are the PKG body of
the PKI, and the Clients are users of the PKI which need to get private key from
the PKG.
Setup. In the beginning all the DKG Players establish communication chan-
nels with each other. The setup is initiated from a random DKG Player, called
a Leader. The ﬁrst task of the Leader is to setup the elliptic curve and its prop-
erties, the pairing type, the three pairing groups G1, G2 and G3, the number n
of DKG Players and the threshold value t of the PKG. Then, the Leader sends
message commandInit to all the other DKG Players and all the players execute
the initialization step (scheme 3.1). After the initialization step ﬁnishes, all the
DKG Players send back the message commandInitFinished to the Leader.
After receiving n −1 messages commandInitFinished, the Leader starts the
second phase of the setup protocol. In this phase, the Leader sends the message
commandSharePieces to all DKG Players. Every DKG Player upon receiving
this message starts sharing the commitments and evaluations of their polynom
(scheme 3.2).
After the DKG Player Pi receives a message commandReceivePieces saves
the commitments and evaluation from Pj, j = 1, . . . , n, j ̸= i. After saving, the
DKG Player Pi sends the message commandSharePiecesFinished to the Leader.
After receiving n −1 messages commandSharePiecesFinished, the Leader starts
the third phase - veriﬁcation by sending the message commandVerifyCommit-
mentsAndEvals to all DKG Players. After the DKG Player Pi receives this mes-
sage, they start the procedure for veriﬁcation (scheme 3.3).
www.ebook3000.com

Distributed Private Key Generator for ID-Based Public Key Infrastructure
75
1 Calculate the commitment to the group secret:
- DKG Player Pi calculates g pow s public = n
j=1 cj,0
2 Veriﬁcation of commitments and the evaluation of f(x) from
DKG Player Pj, j = 1, ..., n, j ̸= i:
- DKG Player Pi checks if the following holds
cj,0cj,1...cj,t−1
?= gfj(i), j = 1, ..., n, j ̸= i
3 Sending success message:
- If all the veriﬁcations are in order, DKG Player Pi send the message
commandVerifyCommitmentsAndEvalsFinished to the Leader
Scheme 3.3. DKG Player Pi veriﬁes received commitments and evaluations
1 Leader sends (n −1) messages to the other playeres:
- byte: 1
- Integer: 2
- G1: 1
2 Other (n −1) players send one message to the Leader:
- byte: 1
Scheme 3.4. Messages sent in the ﬁrst phase: initialization of DKG Players. Here we
see the content of each sent message in terms of type of the variables.
After the Leader receives n −1 messages commandVerifyCommitmentsAndE-
valsFinished, ﬁnishes the setup protocol. This means that the DKG Players suc-
cessfully created a PKG group and can start issuing private keys.
The price represented in bandwidth and calculations in the ﬁrst phase of the
setup are given in the following schemes.
Key Extraction. When the setup protocol is executed, the DKG Players suc-
cessfully create a group which can act as PKG. The key extraction phase begins
with the Client asking any DKG Player for the public parameters of the PKG.
1 All players calculate:
- scalar doubling: 2n + 1
- G1 scalar multiplication: 2n(k −1) + k + 1
- scalar addition: n(k −1)
2 Leader calculates
- G1 serialization: 1
3 Other (n −1) players calculate
- G1 deserialization: 1
Scheme 3.5. Arithmetic operations in the ﬁrst phase: initialization of DKG Players.
Here we see the arithmetic operations each player performs.

76
P. Ribarski and L. Antovski
1 The Leader sends (n −1) messages to the other players:
- byte: 1
2 All players send one message to all other players
(totalling n(n −1) messages):
- byte: 1
- G1: k + 1
- Scalar: 1
3 All players send one message to the Leader
(totalling n −1 messages):
- byte: 1
Scheme 3.6. Messages sent in the second phase: sharing of pieces. Here we see the
content of each sent message in terms of type of the variables.
1 All players calculate:
- G1 serizalization: k + 1
- Scalar serialization: k
Scheme 3.7. Arithmetic operations in the second phase: sharing of pieces. Here we
see the arithmetic operations each player performs.
1 The Leader sends (n −1) messages to the other players:
- byte: 1
2 All players send one message to the Leader
(totalling n −1 messages):
- byte: 1
Scheme 3.8. Messages sent in the third phase: veriﬁcation. Here we see the content
of each sent message in terms of type of the variables.
1 All players calculate:
- G1 exponentiation: 1
- Scalar doubling: 1
- G1 doubling: 1
- Scalar exponentiation: k
- G1 scalar multiplication: n + k
- G1 multiplication: k
- Scalar addition: k
- G1 equality: 1
Scheme 3.9. Arithmetic operations in the third phase: veriﬁcation. Here we see the
arithmetic operations each player performs.
www.ebook3000.com

Distributed Private Key Generator for ID-Based Public Key Infrastructure
77
1 Asking public information about the PKI:
- The User send a message commandSendPlayersList to DKG Player
- DKG Player receives commandSendPlayersList and sends
commandReceivePlayersList together with the public parameters about
the PKI
2 Asking for private key pieces
- The User sends a message commandClientGetSumOfEvals to all
DKG Players together with their identiﬁcator id
- Every DKG Player Pi calculates and sends privi = idsi,0 and g publicsi,0
with the message commandClientReceiveSumOfEvals
3 Verify the received key pieces:
- When the Client receives commandClientReceiveSumOfEvals
they verify privi = idsi,0 by calculating
e1 = Pairing(g public, privi) and e2 = Pairing(g publicsi,0, id)
and checking whether e1
?= e2. If the veriﬁcation is successful, the piece privi
from DKG PLayer Pi is accepted
4 Veriﬁcation and private key creation:
- When the User veriﬁes n pieces, they
calculate the private key did = n
i=1 privi
- The calculated private key is veriﬁed by calculating
e1 = Pairing(g public, did) and e2 = Pairing(g pow s public, id) and
veriﬁying e1
?= e2. If the veriﬁcation is successful, the private
key did is accepted for the identity id issued
by the PKI
Scheme 3.10. Key extraction algorithm
1 The Client sends n messages to all DKG Players:
- byte: 1
- String: 1
2 All DKG Players send one message to the Client(totalling n messages):
- byte: 1
- G2: 1
Scheme 3.11. Messages sent while executing key extraction protocol. Here we see the
content of each sent message in terms of type of the variables.
These parameters can be access on some kind of a bulletin board too. After
getting the public parameters, the Client establishes communication channels
with the DKG Players. Then the Client and the DKG Players follow the key
extraction algorithm (scheme 3.10).
The price of the key extraction algorithm is represented by the schemes
depicting bandwidth (scheme 3.11) and arithmetic operations (scheme 3.12).

78
P. Ribarski and L. Antovski
1 All DKG Players calculate:
- SHA1 hash h1(byte[])−> G2: 1
- G2 scalar multiplication: 1
- G2 serialization: 1
2 The Client calculates:
- G2 deserialization: 1
- pairing: 2(n + 1)
- G3 equality: n + 1
- G2 multiplication: n
Scheme 3.12. Arithmetic operations while executing key extraction protocol. Here we
see the the arithmetic operations each player performs.
4
Conclusion
This paper tackles the open problem of central power in the PKG in ID-Based
PKC. By using distributed key generation techniques, we successfully removed
this central power from only one entity, and divided the power to generate secret
keys to a group of entitites. If there is a malicious entity in the group, he/she will
not be able to generate private keys for arbitrary identities. Further more, by
implementing this protocol on the Android platform, we gave a real-world exam-
ple that low-cost hardware can be used for PKI infrastructures. This solution
can be easily plugged in any ID-Based PKC protocol which requires distributed
PKG. We give detailed information about all the bandwidth and computation
need for running the protocol. Implementors can easily decide if this solution
ﬁts in their protocols, or in their production solutions.
This work was partially ﬁnanced by the Faculty of Computer Science and
Engineering at the “Ss. Cyril and Methodius University” within the project
SEG.
References
1. Backes, M., Kate, A., Patra, A.: Computational veriﬁable secret sharing revisited.
In: Proceedings of 17th International Conference on The Theory and Application
of Cryptology and Information Security, ASIACRYPT 2011, pp. 590–609. Springer,
Berlin (2011). http://dx.doi.org/10.1007/978-3-642-25385-0 32
2. Blakley, G.: Safeguarding cryptographic keys. In: Proceedings of 1979 AFIPS
National Computer Conference, pp. 313–317. AFIPS Press, Monval (1979)
3. Boneh, D., Franklin, M.K.: Identity-based encryption from the weil pairing. In:
Proceedings of 21st Annual International Cryptology Conference on Advances in
Cryptology, CRYPTO 2001, pp. 213–229. Springer, London (2001). http://dl.acm.
org/citation.cfm?id=646766.704155
4. Cachin, C., Kursawe, K., Lysyanskaya, A., Strobl, R.: Asynchronous veriﬁable
secret sharing and proactive cryptosystems. In: Proceedings of 9th ACM Confer-
ence on Computer and Communications Security, CCS 2002, pp. 88–97. ACM,
New York (2002). http://doi.acm.org/10.1145/586110.586124
www.ebook3000.com

Distributed Private Key Generator for ID-Based Public Key Infrastructure
79
5. Chor, B., Goldwasser, S., Micali, S., Awerbuch, B.: Veriﬁable secret sharing and
achieving simultaneity in the presence of faults. In: Proceedings of 26th Annual
Symposium on Foundations of Computer Science, SFCS 1985, pp. 383–395. IEEE
Computer Society, Washington, DC (1985). http://dx.doi.org/10.1109/SFCS.1985.
64
6. Desmedt, Y., Quisquater, J.J.: Public-key systems based on the diﬃculty of tam-
pering (is there a diﬀerence between DES and RSA?). In: Proceedings on Advances
in cryptology—CRYPTO 1986, pp. 111–117. Springer, London (1987). http://dl.
acm.org/citation.cfm?id=36664.36673
7. Feldman, P.: A practical scheme for non-interactive veriﬁable secret sharing. In:
Proceedings of 28th Annual Symposium on Foundations of Computer Science,
SFCS 1987, pp. 427–438. IEEE Computer Society, Washington, DC (1987). http://
dx.doi.org/10.1109/SFCS.1987.4
8. Gennaro, R., Jarecki, S., Krawczyk, H., Rabin, T.: Secure distributed key genera-
tion for discrete-log based cryptosystems. J. Cryptol. 20(1), 51–83 (2007). http://
dx.doi.org/10.1007/s00145-006-0347-3
9. Gennaro, R., Rabin, M.O., Rabin, T.: Simpliﬁed VSS and fast-track multiparty
computations with applications to threshold cryptography. In: Proceedings of 17th
Annual ACM Symposium on Principles of Distributed Computing, PODC 1998,
pp. 101–111. ACM, New York (1998). http://doi.acm.org/10.1145/277697.277716
10. Joye, M., Neven, G.: Identity-Based Cryptography. Cryptology and Information
Security Series, vol. 2. IOS Press, Amsterdam (2008)
11. Kate, A., Goldberg, I.: Distributed key generation for the internet. In: 29th IEEE
International Conference on Distributed Computing Systems, ICDCS 2009, pp.
119–128, June 2009
12. Maurer, U.M., Yacobi, Y.: Non-interactive public-key cryptography. In: Proceed-
ings of 10th Annual International Conference on Theory and Application of Cryp-
tographic Techniques, EUROCRYPT 1991, pp. 498–507. Springer, Berlin (1991).
http://dl.acm.org/citation.cfm?id=1754868.1754924
13. Pedersen, T.: Non-interactive and information-theoretic secure veriﬁable secret
sharing. In: Feigenbaum, J. (ed.) Advances in Cryptology CRYPTO 1991. Lec-
ture Notes in Computer Science, vol. 576, pp. 129–140. Springer, Berlin (1992).
http://dx.doi.org/10.1007/3-540-46766-1 9
14. Shamir, A.: How to share a secret. Commun. ACM 22(11), 612–613 (1979). http://
doi.acm.org/10.1145/359168.359176
15. Shamir, A.: Identity-based cryptosystems and signature schemes. In: Proceedings
of CRYPTO 1984 on Advances in Cryptology, pp. 47–53. Springer-Verlag New
York, Inc., New York (1985). http://dl.acm.org/citation.cfm?id=19478.19483
16. Tanaka, H.: A realization scheme for the identity-based cryptosystem. In: A Con-
ference on the Theory and Applications of Cryptographic Techniques on Advances
in Cryptology, CRYPTO 1987, pp. 340–349. Springer, London (1988). http://dl.
acm.org/citation.cfm?id=646752.704736
17. Tsujii, S., Itoh, T.: An ID-based cryptosystem based on the discrete logarithm
problem. IEEE J. Sel. Areas Commun. 7(4), 467–473 (1989). http://dx.doi.org/
10.1109/49.17709
18. Wu, Q., Chen, H., Li, Z., Jia, C.: On a practical distributed key generation scheme
based on bivariate polynomials. In: 2011 7th International Conference on Wire-
less Communications, Networking and Mobile Computing (WiCOM), pp. 1–4,
September 2011

Relation Between Statistical Tests
for Pseudo-Random Number Generators
and Diaphony as a Measure of Uniform
Distribution of Sequences
Sashe Gjorgjievski, Verica Bakeva(&),
and Vesna Dimitrievska Ristovska
Faculty of Computer Science and Engineering,
University “Ss Cyril and Methodius”,
P.O. Box 393, Skopje, Republic of Macedonia
sasepp007@yahoo.com, {verica.bakeva,
vesna.dimitrievska.ristovska}@ﬁnki.ukim.mk
Abstract. In this paper we investigate the relation between statistical tests for
pseudo-random number generators and the diaphony as a measure of uniform
distribution of sequences. In order to ﬁnd some relations many experiments are
done. For these experiments we use generators and tests from Diehard battery.
For all generated sequences, the diaphony is calculated and the tests from
Diehard battery are done. Also, we made experiments using two deterministic
sequences: the sequence of Van der Corput and the sequence of equidistant
points.
Keywords: Uniformity  Randomness  Statistical tests  Diaphony  Diehard
battery
1
Introduction
Pseudo-random number generator (PRNG) is a device producing a sequence of
numbers s1; s2; . . . with a given distribution which is supposed to be uniform, where
s1; s2; . . . are elements of a given set of numbers. A sequence of random numbers
s1,s2,… must have two important properties: uniformity (i.e. they are equally probable
everywhere) and independence (i.e. the current value of a random variable has no
relation with the previous values). In fact, in practice, we cannot design a perfect
PRNG, since the way we are building the device is not a random one, which affects the
uniformity and independency of the produced sequences. That is why the word
“pseudo’’ is used and we have to measure the randomness of the obtained sequences.
There are a lot of tests for such measurements and all of them are measuring the
difference between the generated pseudo-random sequences and the theoretically
supposed ideal random sequence. We say that a PRNG is passing a test if the random
sequences produced by that PRNG are passing the test with a probability near to 1. We
can classify PRNGs depending of the tests they have passed. So, for obtaining a better
classiﬁcation we should have many different tests.
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_8
www.ebook3000.com

On the other side, a diaphony is a measure for uniform distribution of a given
sequence. The ideal uniformly distributed sequence has the diaphony equal to 0. Since
the formula for computing of diaphony includes limit, practically, it is not possible to
compute the exact value of the diaphony, but we can approximate it with a desired
accuracy.
Our idea is to ﬁnd a relation between statistical tests for goodness of PRNG and
diaphony of the obtained sequences. The main question is: “Is it possible to replace
statistical tests for randomness with the diaphony, if its value is in an interval [0,e), for
previous given small number e?” In order to answer this question we made many
experiments using generators and statistical tests from Diehard battery [1].
This paper is organized as follows. In Sect. 2 we give a short explanation for two
number generators used in Diehard battery: “Mother of all generators” and “31-bit
generator”. In Sect. 3 we present statistical tests from Diehard battery. The deﬁnition of
diaphony is given in Sect. 4. In the same section we present Van der Corput sequence
as approximately ideal uniform sequence. In Sect. 5, the experimental results are given.
At the end, we give some conclusions.
2
Generators of Pseudo Random Numbers
In our experiments, we used 16 pseudo random generators introduces by George
Marsaglia, but in this paper we present the results of two chosen generators: “Mother of
all generators” and “31-bit generator”. We chose these two generators since the ﬁrst
one is “good” generator (almost all sequences passed all statistical tests) and the second
one is “not so good” (there are sequences which do not pass all tests). Further on, we
will give a brief description of these two generators.
• Mother of all Generators
This generator creates a binary ﬁle with 11 megabytes of 32-bit integers. This is
achieved by using the following “multiply with carry” generator [1]:
xn ¼ 2111111111  xn4 þ 1492  xn3 þ 1776  xn2 þ 5115  xn1 þ carry mod 232:
The advantage of this generator is that it has very large period which is 2160. The
carry part in the next step of the iteration process is the numerator when the module is
b = 232. For example, if the linear combination with the current 4 x-values x1, x2, x3, x4
and the carry gives the result: 73  b þ 2245, then the new x will be 2245, while the
new carry will be 73.
The biggest advantage of this special “multiply with carry” generator is that it
allows use of modules 216 and 232 in that way that avoids the trailing bits disadvantages
common for congruential sequences for such modules.
When concretely executing this program, we will be prompt to enter 4 initial values
(seeds) of integer numbers for x1, x2, x3, x4 and the carry. Then as described above, the
new values of x and c are being produced, and so on. Thus, a binary ﬁle of integers is
being created, which further will be used to check whether the generated sequence
passes the Diehard statistical tests.
Relation Between Statistical Tests for PRNG and Diaphony
81

• 31-bit Generator
The 31-bit generator creates a binary ﬁle with 11 megabytes of 31-bit integers. In the
literature, this generator is also known by the name “RAN2” generator. This generator
is a combination of two linear congruential generators. The generated random numbers,
are then additionally randomized with shufﬂing [4].
After that the generated 31-bit random numbers are being shifted to the left for 1
bit, which means that the leading beats are fulﬁlled [1]. From the other side this means
that the last bit of all integer numbers generated with this PRNG will be equal to 0. By
saying that, we should expect that all Diehard statistical tests which use 32-bits from
left to right will show spectacular failures.
This generator is very simple for use, since we should enter only 1 initial seed
(integer number). After that the rest of 31-bit integers are being produced and written in
a binary ﬁle. This ﬁle is used for further analysis to check whether the produced
sequence passes the statistical tests in Diehard battery.
3
Diehard Tests
Diehard tests are a battery of statistical tests for measuring the quality of a random
number generator. It contains 16 statistical tests. This battery was developed by George
Marsaglia over several years and ﬁrst published in 1995. It is a complete, thorough and
comprehensive set of statistical tests for PRNG [2]. This set of statistical tests serves as
some kind of litmus for checking and eventual certiﬁcation of PRNG. If a PRNG
passes Diehard statistical tests, then it can be used in more serious scientiﬁc researches.
Further on, we give an overview of these tests [1, 3].
• Birthday Spacings Test
For this test we choose m birthdays in a year of n days. The spacing between the
birthdays needs to be listed. If the number of values that occurred more than once in the
list is the variable J, than it has asymptotical Poisson distribution with mean m3= 4n
ð
Þ.
By doing a lot of experiments it is received that the values of n have to be very
large so the results could be compared with the expected Poisson distribution with
mean m3/(4n). The default setup uses n ¼ 224 and m ¼ 29, so that enables us to take the
Poisson with k ¼ 227
226 ¼ 2 as the concrete distribution for J. For a sample of 500 J’s a
chi-square test is performed to provide a p value. The ﬁrst test uses the ﬁrst bits from 1–
24 counting from left to right from the integers in the speciﬁed ﬁle and the ﬁle is closed
and reopened. The same is done with the bits from 2–25 to provide birthdays, and the
ﬁle is closed and reopened again. And so on until the last sequence of bits from 9–32.
Each set of bits provides a p-value, so we have nine p-values. At the end a
Kolmogorov – Smirnov (K – S) test is taken for the acquired 9 p-values.
• Overlapping 5-Permutation Test (OPERM-5)
The overlapping 5 permutation test gets its name since it takes 5! possible orderings of
ﬁve consecutive integers. These tests are overlapping m-tuple tests for which elements
of the overlapping 5-tuples are not independent, or even successive states of a Markov
82
S. Gjorgjievski et al.
www.ebook3000.com

chain. Let s1; s2; . . . be uniform variates produced by a RNG. Each of the overlapping
5-tuples (s1, s2, s3, s4, s5), (s2, s3, s4, s5, s6), (s3, s4, s5, s6, s7), is in one of 120 possible
states:
S1 : x1\x2\x3\x4\x5;
S2 : x1\x2\x3\x5\x4;
S3 : x1\x2\x4\x3\x5; . . .;
S120 : x5\x4\x3\x2\x1:
This test actually uses 1 000 000 sets of 5 integers. It is obvious that these sets
overlap between each other (some integers belong to more than 2 sets). Because each
numbers provide a state it comes that thousands of different states are being observed,
resulting with counting of the number of occurrences of each state. Than the quadratic
form in the weak inverse 120  120 covariance matrix is being made, equivalent to the
likelihood ratio test which expects that the counts will have normal distribution.
In our experiments, the test is performed twice.
• Binary Rank Tests
Binary rank tests use some of the characteristics of the matrixes and their ranks. N-
dimensional cube is taken using the columns of a matrix as axes. If the rank of the
matrix is the same as the size of the matrix, then we can get to whatever point in the N-
dimensional cube. The actual values of the ranks are being compared with the ranks
previously calculated. This is made by performing a chi-squared test to compare how
well the sample ﬁts the expected distribution.
• 31  31 Binary Matrix
A 31  31 binary matrix is generated formed of 31 integers in each row. 31 bits are
being used (only the last bit is not taken) of those 31 integers. If the rank of the matrix
is r then 0  r  31. But in practice, ranks are rarely less than 28, so if that is the
case those kinds of ranks are being combined with those for rank 28. After that a
sample of 40 000 matrixes is taken and then a chi-squared test is performed to calculate
the actual value of the ranks which can be 31, 30, 29 and  28. If 40 000 matrixes are
taken as a sample it is clear that 40 000 ∙31 = 1 240 000 integers are used in this test.
• 32  32 Binary Matrix
A 32  32 binary matrix is random generated formed of 32 integers in each row. If the
rank of the matrix is r then 0  r  32. But in practice ranks are rarely less than 29,
so if that is the case those kinds of ranks are being combined with those for rank 29.
After that a sample of 40 000 matrixes is taken and then a chi-squared test is performed
to calculate the actual value of the ranks which can be 32, 31, 30 and  29. If 40 000
matrixes are taken as a sample it is clear that 40 000 ∙32 = 1 280 000 integers are used
in this test.
• 6  8 Binary Matrix
A 6  8 binary matrix is generated from 6 random integers and 8 bits from those
integers. If the rank of the matrix is r then 0  r  6. But in practice ranks are rarely
less than 4, so if that is the case those kinds of ranks are being combined with those for
Relation Between Statistical Tests for PRNG and Diaphony
83

rank 4. After that a sample of 100 000 matrixes is taken and then a chi-squared test is
performed to calculate the actual value of the ranks which can be 6, 5 and  4. If 100
000 matrixes are taken as a sample it is clear that 100 000 ∙8 = 800 000 integers are
used in this test.
In our experiments, the test is performed 25 times. At the end a Kolmogorov-
Smirnov (K-S) test for 25 obtained p-values is performed to check whether they all
together are uniformly distributed at [0,1).
• Bitstream Test
The ﬁle that is tested is considered as a stream of bits. So called 20 letter words are
taken, overlapping between each other. The ﬁrst word is from the 1st to 20th bit, then the
second from 2nd to 21st bit etc. Then the number of the missing 20 letter words is
counted in a string of 221 overlapping words. Their number is expected to be normally
distributed with mean 144 909 and sigma 428. It leads to a uniform [0, 1] p-value.
In our experiments, the test is repeated 20 times and we perform a Kolmogorov –
Smirnov test on the obtained 20 p-values.
• The Tests OPSO, OQSO and DNA
• OPSO Overlapping Pairs Sparse Occupancy Test
In this test a 2-letter words from an alphabet of 1024 letters are being considered. Each
of the 2 letters are determined by a speciﬁed 10 bits from a 32 bit integers in the
sequence to be tested. With OPSO, 221 overlapping words are being generated and then
the number of the missing words (i.e. 2-letter words which do not appear in the entire
sequence) is counted. This number is almost normally distributed with mean 141909
and a standard deviation sigma 290. It leads to a uniform [0, 1] p-value. The test is
executed 23 times, ﬁrst using the bits from 1 to 10, then 2–11, 3–12, …, 23–32 bits of
the 221 þ 1 “keystrokes”.
• OQSO Overlapping Quadruples Sparse Occupancy Test
Similar as in OPSO, in this test a 4-letter words from an alphabet of 32 letters are being
considered. Each of the 4 letters are determined by a speciﬁed 5 bits from a 32 bit
integers. With OQSO 221 overlapping words are being generated and then the number
of the missing words is counted. This number is almost normally distributed with mean
141 909 and a standard deviation sigma 295 determined by simulation. Again, p-value
is uniformly distributed on [0,1]. The test is executed 28 times, ﬁrst using the bits from
1 to 5, then 2–6, 3–7, …, 28–32 bits of the 221 + 3 “keystrokes”.
• DNA
Like the previous 2 tests, in this test a 10-letter words from an alphabet of 4 letters {C,
G, A, T} are being considered. Each of the 10 letters are determined by a speciﬁed 2
bits from a 32 bit integers. With DNA 221 overlapping words are being generated and
then the number of the missing words in the whole sequence is counted. This number is
almost normally distributed with mean 141 909 and a standard deviation sigma 339
determined by simulation. As previous, p-value is uniformly distributed on [0,1]. The
test is executed 31 times, ﬁrst using the bits from 1 to 2, then 2–3, 3–4, …, 31–32 bits
of the 221 + 9 “keystrokes”.
84
S. Gjorgjievski et al.
www.ebook3000.com

• Count the 1’s Test on a Stream of Bytes
In this test as its name suggests the number of 1’s in a stream of bytes is counted. Each
byte can contain from 0 to 8 1’s with different probabilities:
1
256 ; 8
256 ; 28
256 ; 56
256 ; 70
256 ;
56
256 ; 28
256 ; 8
256 ; 1
256. The stream of bytes provides a string of overlapping 5-letter words.
Each letter takes value A, B, C, D, E. The letters are determined by the number of 1’s,
in that byte: 0, 1, or 2 ! A, 3 ! B, 4 ! C, 5 ! D, and 6, 7 or 8 ! E. So, which
letter will be taken depends from the number of 1’s in the stream. The number of 5
letters overlapping words is 55. From a string of 256 000 ﬁve letter words, frequencies
for each word are being counted. The quadratic form in the weak inverse of the
covariance matrix of the cell counts provides a chi-square test (the ordinary Pearson
sums of (OBS-EXP)2/EXP on counts for 5- and 4-letter cell counts). The test returns 2
p-values for both 5- and 4-letter cell counts.
• Count the 1’s Test for Speciﬁc Bytes
In this test as its name suggests the number of 1’s in speciﬁc bytes from each 32 integer
are being counted. From each integer, a speciﬁc byte is chosen, say the left-most: bits 1
to 8. Each byte can contain from 0 to 8 1’s with different probabilities:
1
256 ; 8
256 ;
28
256 ; 56
256 ; 70
256 ; 56
256 ; 28
256 ; 8
256 ; 1
256 : The letters are determined by the number of 1’s, in that
byte: 0, 1, or 2 ! A, 3 ! B, 4 ! C, 5 ! D, and 6, 7 or 8 ! E. So, which letter will
be taken depends from the number of 1’s in that byte. So the words of 5 letters are
being formed from the speciﬁed bytes from successive integers. The number of 5 letters
overlapping words (each “letter” taking values A, B, C, D, E) is 55. From a string of 256
000 ﬁve letter words frequencies for each word are being counted. The quadratic form
in the weak inverse of the covariance matrix of the cell counts provides a chi-square
test (the ordinary Pearson sums of (OBS-EXP)2/EXP on counts for 5- and 4-letter cell
counts). The test is executed 25 times, ﬁrst using ﬁrst byte (bits from 1 to 8), then
second byte (bits 2-9), …, 25th byte (bits 25-32) and the corresponding p-values of
Pearson chi-square tests are found.
• Parking Test
In this test we park a car (it is a circle with radius 1) in a square of side 100 (so the
square is with 100  100 size). Then we do the same with the second, third car and so
on. If a crash occurs when we try to park a car, the process for that particular car is
repeated from the beginning choosing different random location for parking. The
number of successfully parked cars is being counted after 12 000 attempts. This
number has approximately normal distribution with the average of 3 523 with sigma
21.9. At the end a Kolmogorov-Smirnov (K-S) test for 10 obtained p-values is per-
formed to check whether they all together are uniformly distributed at [0,1).
• Minimum Distance Test
Again we take a square but now with a side of 10 000 choosing 8 000 random points in
it. If we denote the minimum distance between nðn1Þ
2
pairs of random points with d, and
if the points are independent and uniformly distributed, then d2 should be exponentially
distributed with mean 0.995. Then 1  exp ðd2=0:995Þ should also be uniform on
Relation Between Statistical Tests for PRNG and Diaphony
85

[0,1) and a Kolmogorov-Smirnov test on the resulting uniform values serves as a test of
uniformity for random points in the square. The Kolmogorov-Smirnov test is based on
the full set of 100 random choices of 8000 points in the 10 000  10 000 square:
• 3D Spheres Test
Here we take a cube with side 10 000 and choose 4 000 random points in it. At each
point, center a sphere large enough to reach the next closest point. Then the distribution
of the volume of the smallest such a sphere is found and it is approximately expo-
nentially distributed with mean 120p
3 : Then the cube of radius r3 is also exponential with
mean 30 (obtained by extensive simulation). With this test, we generated 20 times by
4 000 such spheres. Next, using the transformation 1  exp ðr3=30Þ; each minimum
cube of radius r3 lead to a uniform distributed variable on [0, 1). Then a K-S test is
done on the 20 p-values.
• Squeeze Test
In this test random integers are ﬂoated to get uniform distributions on [0, 1). Starting
with k ¼ 232, the test ﬁnds J, the number of iterations necessary to reduce k to 0 using
the reduction k = ⌊kU⌋, where U is a random uniform. A sample of 100 000 J’s is used
for v2-test of the cell frequencies.
• Overlapping Sums Test
Let m  100 be a ﬁxed integer. Take a sequence of independent and identically
distributed U(0, 1) random variables U1; U2; . . . and form the overlapping sums
S1 ¼ U1 þ U2 þ . . . þ Um; S2 ¼ U2 þ U3 þ . . . þ Um þ 1, and so on. The random
variables Si; i ¼ 1; 2; . . ., m are virtually normal with a covariance matrix which is easy
to calculate. Clearly, E Si
ð Þ ¼ m=2, and D Si
ð Þ ¼ m=12, i ¼ 1; 2; . . ., m. Furthermore, if
1  i \ j  m, then Si and Sj have a sum S of m  j þ i uniform values in common
with
X ¼ SiS; S,
and
Y ¼ SkS
being
mutually
independent.
Therefore,
cov Si; Sj


¼ ðm  j þ iÞ=12.
Thus, if C denotes the m  m covariance matrix of the Si’s, the matrix 12C is Toeplitz
with diagonals m, m – 1, …, 1. A cholesky factorization yields C = VVT, where V is
lower triangular. Since V−1, the inverse of a lower triangular matrix is easily computed,
we can convert the vector S of Si’s to independent normal variables via the linear
transformation X = V−1S which can be tested for normality or uniformity after converting
to uniforms via the normal cumulative distribution function. After 10 times applying of
K-S test, another K-S test is performed on the obtained 10 p-values. The combination of
the two Kolmogorov – Smirnov (K-S) tests expands the size of the detected circuits.
• Runs Test
The RUNS test counts the number of runs up and run downs in a sequence of 10 000
uniform variables [0,1) acquired by ﬂoating the 32-bit integers from the speciﬁed ﬁle.
Because the covariance matrix for the runs up and runs down is known, a chi-square
test may be carried out for quadratic forms in a weak inverse of the matrix in order to
get a p-value. Performing this 10 times the p-values are obtained, and then for these 10
p-values a K-S test is executed. After that the whole test is performed again.
86
S. Gjorgjievski et al.
www.ebook3000.com

• Craps Test
This test is somehow connected with the Craps game. The test plays n  200 000
games of craps and counts the number of wins and the number of throws necessary to
end each game. The number of wins should be very close to normal with mean np and
variance np(1 −p) where p = 244/495. Throws necessary to complete the game can
vary from 1 to ∞, but all throws  21 are lumped together. A v2-test is made on the
number-of-throws cell counts. Each 32-bit integer from the test ﬁle provides the value
for the throw of a die, by ﬂoating to [0,1), multiplying by 6 and taking 1 plus the
integer part of the result.
Note that the most of the tests in DIEHARD return a p-value, which should be
uniform on [0,1) if the input ﬁle contains truly independent random bits. Those
p-values are obtained by p = F(X), where F is the assumed cumulative distribution
function of the sample (random variable X) – often normal. But that assumed F is just
an asymptotic approximation, for which the ﬁt will be worst in the tails. Therefore
p < 0.025 or p > 0.975 means that the RNG has “failed the test at the 0.05 level”.
4
Diaphony and Van der Corput Sequence
Let s  1 be a ﬁxed integer and n ¼ ð~xiÞi  0 be an arbitrary sequence of points in
[0,1)s. For each integer N  1 and an arbitrary subinterval J of [0,1)s with a volume
l(J), we denote by Aðn; J; NÞ the number of the points ~xn of the sequence n whose
indices n satisfy the inequalities 0  n  N −1 and belong to the interval J. The
sequence n is called uniformly distributed in [0,1)s if the equality:
lim
N!/
Aðn; J; NÞ
N
¼ lðJÞ
ð1Þ
holds for every subinterval J of [0,1)s.
The diaphony is a quantitative measure for uniform distribution of sequences in
[0,1)s. Zinterhof [5] uses the trigonometric functional system s to introduce the
“classical” diaphony
s ¼ fe~mð~xÞ ¼ expð2pi\~m;~x [ Þ : ~m ¼ ðm1; m2; . . .; msÞg 2 Zs;
ð2Þ
~x ¼ ðx1; x2; . . .; xsÞg2½0; 1Þs; where \~m;~x [ is the inner product between the vectors
~m and ~x: The classical diaphony is deﬁned as follows.
Deﬁnition 1 [5]. For each integer N  1 the classical diaphony FN(s, n) of the ﬁrst
N elements of the sequence n ¼ ð~xiÞi  0 of points in [0,1)s is deﬁned as:
FNðs; nÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
~h2Zs;~h6¼~0
R2ð~hÞ 1
N
X
N1
n¼0
expð2pi\~h;~xn [ Þ


2
v
u
u
u
t
;
ð3Þ
Relation Between Statistical Tests for PRNG and Diaphony
87

where
for
a
vector ~h ¼ ðh1; . . .; hsÞeZs,
the
coefﬁcient
Rð~hÞ
is
deﬁned
by
Rð~hÞ ¼ Q
s
j¼1
maxð1; jhjjÞ:
In this paper we take one-dimensional version of the diaphony.
The following theorem gives necessary and sufﬁcient condition for uniformity of a
sequence n ¼ ð~xiÞi  0.
Theorem 1 [5]. The sequence n ¼ ð~xiÞi  0 of points in [0,1)s is uniformly distributed
if and only if the limit equality is fulﬁlled.
lim
N!1 FNðs; nÞ ¼ 0
ð4Þ
At the end, we give the deﬁnition of the sequence of Van der Corput since this
sequence is a typical example of “very good” uniformly distributed sequence and as
such it is used in our investigation.
Deﬁnition 2 [6]. Let b  2 is an integer number and let R = (ri)i  0 is a sequence
of permutations on the set {0,1, …, b −1}. If an arbitrary integer number i  0 has its
representation in a number system with a base b:
i ¼
X
s
j¼0
ajðiÞb j
ð5Þ
where j  0, aj(i) ∊{0,1, …, b −1}, then the generalized sequence of Van der Corput
is deﬁned as:
SR
b ðiÞ ¼
X
s
j¼0
rj ajðiÞ


bj1
ð6Þ
Actually the so called sequence of Halton (which ﬁrst was deﬁned in the 1960 by
the American scientist Halton) is practically used in this paper so we will also give its
deﬁnition. Now if we set Ʃ = I in the previous deﬁnition where I stands for the
sequence of identities, we will get the sequence of Halton.
Deﬁnition 3 [7].
Let b  2 is an integer number. If an arbitrary integer number
i  0 has its representation in a number system with a base b given by (5) then the
sequence of Halton is deﬁned as:
SI
bðiÞ ¼
X
s
j¼0
ajðiÞbj1:
ð7Þ
88
S. Gjorgjievski et al.
www.ebook3000.com

5
Experiments
We made many experiments using the generators from the Diehard battery. There are
16 generators integrated in this packet and we generated 25 random sequences by a
generator. Here, we present only the results for 5 sequences for the following two
generators: “Mother of all generators” (Table 1) and “31-bit-generator” (the so called
RAN2 generator) (Table 2). In the tables, we give p-values for Birthday Spacing test,
OPERM-5 test, 31  31 Binary matrix, 32  32 Binary matrix, 6  8 Binary matrix
test, Count the 1’s Test for stream of bytes, Parking, Minimum Distance, 3DSphere,
Squeeze, Overlapping Sums, Run and Craps test. The bolded values of p’s mean that
the test fails. For the other tests (which are performed more than once), we give the
number of passed tests over the total number of tests. These tests are: Bitstream, OPSO,
OQSO, DNA and Count the 1’s for speciﬁc bytes.
In the third column, we present the p-value for the standard Pearson test for the
uniform distribution. Note that, here p-value has the normal meaning. If p > 0.05, we
conclude that the test passes at the 0.05 level of signiﬁcance.
And, in the second column we give the corresponding value of diaphony (see the
formula (3)). In order to apply this formula we have to divide all elements in the
sequences with the maximum element to obtain sequences with elements in [0,1).
Analyzing the results in Table 1, we can conclude that the diaphony for all ﬁve
sequences is very close to 0 and all of them pass the Pearson v2-test. This conﬁrm that
the sequences generated using “Mother of all generators” are uniformly distributed.
Also, these sequences pass almost all other tests. This means that the sequences are
random ones.
On the other side, from results in Table 2, the conclusions for diaphony and
Pearson v2-test are similar as in the previous case. Namely, the diaphony for all ﬁve
sequences is very close to 0 and all of them pass the Pearson v2-test. But, many of test
for randomness fail. All ﬁve sequences did not pass Birthday test, 32  32 Binary
matrix test, Count the 1’s Test on a stream of bytes, all 20 Bitstream tests. Also, 3 (from
5) sequences fail for 6  8 Binary matrix test, one sequence fails one Run test and
other sequence fails one Overlapping 5-Permutation Test.
From the results in these two tables we can conclude that there is a relation between
the diaphony and Pearson v2-test. This is expected since the both of them measure the
uniformity of a sequence. But, we cannot ﬁnd any relation between the diaphony and
other tests for randomness.
In order to conﬁrm this, we made experiments performing the Diehard battery tests
for the sequence of Van der Corput and the sequence of equidistant points. Both of
them are not random sequences. They are strongly deterministic. The concrete value of
diaphony for Van der Corput sequence (with 2 867 190 elements) is 1.75  10−6 and
for the sequence of equidistant points (with 2 867 190 elements) is 5.94  10−15. Also,
p-values of Pearson v2-test for the both sequences are 1 which means that the both
sequences are uniformly distributed. But, the both sequences did not pass any other
tests for Diehard battery. The Squeeze, Overlapping Sums, Run and Craps test did not
give any results. Note that for applying of the tests in Diehard battery, all elements from
Relation Between Statistical Tests for PRNG and Diaphony
89

Table 1. Results for “Mather of all generators”
Mother of all 
generator
Diaphony 
Pearson χ^2  
Birth-Test
OPERM-5
Binary-31
Binary-32
Binary6x8
Bitstream
OPSO
OQSO
DNA
Count Bytes
Count Stream
Parking
Minim.D
3DSpheres
Sqeeze
O-SUM
Runs Test
Craps Test
p-value
p-value
p-value
p-value
p-value
p-value
No. of passed tests /
total no. of tests
p-value
p-value
p-value
p-value
p-value
p-value
p-value
p-value
Seq. 1
0.001
647
0.834
0.737
0.550
0.237
0.779
0.325
0.075
19/20
20/23
27/2
8
31/31
22/25
0.680
0.076
0.648
0.256
0.173
0.710
0.063
0.101
0.538
0.530
0.101
0.002
Seq. 2
0.000
804
0.962
0.390
0.047
0.000
0.540
0.499
0.687
18/20
19/23
26/2
8
30/31
23/25
0.239
0.167
0.374
0.913
0.211
0.711
0.576
0.492
0.783
0.361
0.431
0.34
9
0.72
1
Seq. 3
0.001
098
0.799
0.335
0.886
0.098
0.903
0.722
0.269
20/20
23/23
27/2
8
28/31
23/25
0.516
0.301
0.302
0.533
0.431
0.887
0.799
0.799
0.083
0.477
0.924
0.09
7
0.55
1
Seq. 4
0.001
228
0.121
0.680
0.367
0.653
0.334
0.476
0.087
18/20
23/23
28/2
8
28/31
20/25
0.928
0.319
0.747
0.718
0.662
0.752
0.634
0.480
0.065
0.774
0.537
0.02
8
0.48
8
Seq. 5
0.001
094
0.471
0.698
0.313
0.067
0.570
0.596
0.246
20/20
19/23
25/2
8
30/31
22/25
0.505
0.398
0.855
0.733
0.643
0.973
0.599
0.481
0.405
0.610
0.409
0.83
2
0.24
5
0.092
90
S. Gjorgjievski et al.
www.ebook3000.com

Table 2. Results for “31-bit-generator”
31 bit generator
Diaphony 
Pearson χ^2 
Birth-Test
OPERM-5
Binary-31
Binary-32
Binary6x8
Bitstream
OPSO
OQSO
DNA
Count Bytes
Count-Stream
Parking
Minim.D
3DSpheres
Sqeeze
O-SUM
Runs Test
Craps Test
p-value
p-value
p-value
p-value
p-value
p-value
No. of passed tests /
total no. of tests
p-value
p-value
p-value
p-value
p-value
p-value
p-value
p-value
0.000739
0.673
0.997
0.330
0.990
0.538
1.000
0.975
0/20
22/23
26/28
27/31
22/25
1.000
1.000
0.871
0.136
0.538
0.491
0.569
0.475
0.572
0.466
0.335
0.256
0.120
0.001195
0.176
0.998
0.861
0.766
0.538
1.000
0.992
0/20
20/23
27/28
28/31
23/25
1.000
1.000
0.382
0.645
0.371
0.790
0.801
0.045
0.267
0.980
0.073
0.674
0.294
0.000856
0.833
0.996
0.973
0.315
0.862
1.000
0.917
0/20
20/23
27/28
28/31
21/25
1.000
1.000
0.232
0.379
0.682
0.110
0.918
0.619
0.727
0.571
0.059
0.079
0.890
0.001037
0.527
0.999
0.569
0.103
0.925
1.000
0.960
0/20
21/23
26/28
29/31
20/25
1.000
1.000
0.710
0.630
0.387
0.301
0.857
0.771
0.311
0.375
0.324
0.815
0.525
0.000667
0.575
0.997
0.665
0.399
0.847
1.000
0.992
0/20
21/23
26/28
26/31
22/25
1.000
1.000
0.417
0.722
0.599
0.047
0.943
0.911
0.911
0.174
0.215
0.154
0.410
Seq. 5
Seq. 4
Seq. 3
Seq. 2
Seq.1
Relation Between Statistical Tests for PRNG and Diaphony
91

these sequences (which are in [0,1)) are multiplied with the maximum element, to
obtain sequences of integers ready for use as input in program for Diehard tests.
6
Conclusions
In this paper we investigate the relation between statistical tests for pseudo-random
number generators and the diaphony as a measure of uniform distribution of sequences.
Although, we expected that we will ﬁnd some relation between the both categories, it is
not the case. This kind of diaphony is only a measure of uniformity and cannot be used
for measuring the randomness of sequences. We found relation only between the
diaphony and Pearson v2-test which is expected. So, statistical tests are tools for
checking sequence randomness (uniformity and independence), and the diaphony is a
measure only for uniformity of sequences. These two measures are not equivalent.
Acknowledgements. This work was partially ﬁnanced by the Faculty of Computer Science and
Engineering at the “Ss. Cyril and Methodius” University in Skopje.
References
1. http://stat.fsu.edu/pub/diehard/
2. Marsaglia, G., Tsang, W.W.: Some difﬁcult-to-pass tests of randomness. J. Stat. Softw. 7(3),
1–9 (2002)
3. Narasimhan, B.: JDiehard: an implementation of Diehard in Java. In: Hornik, K., Leisch, F.
(eds.) DSC 2001, Proceedings of the 2nd International Workshop on Distributed Statistical
Computing, Vienna, Austria, pp. 1–9 (2001)
4. Zeeb, C.N., Burns, P.J.: Random number generator recommendation. Colorado State
University, Department of Mechanical Engineering, Fort Collins, CO
5. Zinterhof, P.: Uber einige Abschatzungen bei der approximation von Funktionen mit
Gleichverteilungsmethoden. S. B. Akad. Wiss. Math.-Naturw. Klase Abt.II 185, 121–132
(1976)
6. Faure, H.: Discrépances de suites associées a un systéme de numeration (an dimension un).
Bull. Soc. Math. Fr. 109, 143–182 (1981)
7. Halton, J.H.: On the efﬁciency of certain quasi – random sequences of points in evaluating
multi – dimensional integrals. Numer. Math. 2, 84–90 (1960)
92
S. Gjorgjievski et al.
www.ebook3000.com

Pattern Recognition of a Digital ECG
Marjan Gusev1(B), Aleksandar Ristovski2, and Ana Guseva2
1 FCSE, Ss. Cyril and Methodious University, Skopje, Macedonia
marjan.gushev@finki.ukim.mk
2 Innovation Dooel, Skopje, Macedonia
{aleksandar.ristovski,ana.gusheva}@innovation.com.mk
Abstract. The process of assisted ECG diagnosing mimics the way a
medic would act upon. Such a process inevitably comprises the feature
extraction step, when the standard ECG signal components: the QRS
complex, the P wave and T wave are detected. Using a pattern recog-
nition algorithm for the purpose is one of the available options. In this
article, the pattern recognition approach for the feature extraction rou-
tine is explained by analysis of consecutive steps and its eﬀectiveness is
discussed in comparison to other means of QRS complex detection.
Keywords: Pattern
recognition ·
QRS
detection ·
Performance
engineering
1
Introduction
Although the digitalization of the ECG signal is not a new concept to the tech-
nological advancement, its beneﬁts are recently taking into practice. Thus, the
digitalized recordings are much more feasible to archiving data relative to hard
copied recordings, which is why contemporary instruments are more often fea-
turing a digital output, at the very least, supplementary to the standard output.
Regardless the cause, the process of data interpretation by a virtual AI agent
corresponds to the process used by cardiologists. An ECG recording consists of a
QRS complex, preceded by a P wave and followed by a T wave [4]. The depiction
of these components is inevitably the ﬁrst step of the analysis, irrelevant to
whether a medic or an AI agent is conducting the evaluation. The form of a
QRS complex in Lead I, along with the other morphological components, are
shown in Fig. 1.
The process of detecting the components can vary. Each approach has its own
pros and cons, but, what is of greatest importance is the eﬀectiveness in success-
fully matching the actual component constellation. When evaluating eﬀective-
ness, a certain degree of disparity is tolerable. However, the degree of disparity
depends ﬁrmly on the properties of the statistical population that is chosen.
The superiority of the approach is related to having high eﬀectiveness when
processing a problem set that consists of statistical samples that are untypical if
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 9

94
M. Gusev et al.
Fig. 1. A QRS complex preceded by a P wave and followed by a T wave
compared to ECG recordings of a statistical population with normatively normal
ECG recordings, i.e. a population with low rate of divergent properties.
Considering that an approach is evidently eﬀective, it needs to be able to
detect a wide range of cardiac irregularities. By using pattern recognition, not
only a considerable rate of component detection is achieved, but the number of
irregularities that can be detected is prominent.
2
Background
There are twelve standard leads of ECG. Each of them features a unique rep-
resentation of the signal components and it is not always that a component is
present in the lead recording. This is so not due to the individual properties of
the subject recorded, but rather has to do with the general physiology of the
human body [3]. In addition, each signal component takes diﬀerent form depend-
ing on which lead it has been registered [6]. That is why the normative criterion
correspondent to a certain lead is analyzed when devising a pattern.
The pattern recognition used in the discussed approach so far has been solely
applied for detection of QRS complexes in lead I of the ECG signal. In this case,
after the positioning of the QRS complexes has been established, the P waves and
T waves can be located much more easily by conducting a search in the interval
between two neighboring QRS complexes. However, the pattern recognition can
be done for other leads as well, by using a pattern built suitably for a speciﬁc
lead and the signal component that needs be detected.
There are normative clauses that deﬁne a QRS signal in lead I. These include
the normative duration: between 0.06 s and 0.10 s - standard value for adults and
same for all leads, and the normative amplitude: 500 to 2000 µV for lead I [3].
It is relevant to say that the amplitudes’ values are devised as relative diﬀerence
between the R peaks’ amplitude and the Q and S peaks’ amplitudes, and not as
relative diﬀerence between the R peaks amplitudes and zero.
While the duration of the QRS complex is an acknowledged standard, the
amplitude is aﬀected by a greater number of factors. These include the size
www.ebook3000.com

Pattern Recognition of a Digital ECG
95
of the ventricular chambers - the larger the chamber, the larger the voltage,
and the proximity of precordial (chest) electrodes to the ventricular chambers -
the closer, the larger the relative Q-R and R-S voltage diﬀerences. Additionally,
there are other factors that need be taken into account: certain cardiac conditions
can cause the R peak not to be centrally in-between the Q and S peaks. Such
predicaments strongly indicate that it is of high interest to construct a pattern
which is individual to a patient.
The form of the pattern has a “V” shape - although the QRS complex has
an inverted “V” shape, it needs be turned upside down due to the inversion that
occurs during the pattern check. Pragmatically, a pattern cannot be deﬁned as
a single one dimensional line when used on one dimensional ECG data, and it
must have a surface area, i.e. be planar. That is achieved by having a pair of
one dimensional border lines: linear lower boundary and linear upper boundary.
This two linear components never intersect to form a closed planar geometric
form, meaning that there is a gap between them so that the linear ECG signal
can enter and exit the pattern, as presented in Fig. 2.
Fig. 2. A QRS component (red) entering the pattern surface area (blue)
The linear ECG signal spans across two axes: the horizontal axis with time as
its dimension, and the vertical axis with the amplitude value as its dimension.
The digitalized ECG signal consists of an array of samples: the index of the
element represents the instance of time while the value of the array element
represents the amplitude value at the corresponding time.
3
Building a Pattern
In respect to the temporal normative clause, the upper boundary has a length
equal to the maximum duration of the QRS complex, whereas the lower bound-
ary equals the minimum duration of the QRS complex. Just as the ECG signal
takes form of an array of amplitudes, both the upper and lower pattern boundary
need to have the exact same digital form representation. The parameter sam-
pling time/sampling frequency has a key role in devising the size of the pattern

96
M. Gusev et al.
Algorithm 1. Construct QRS Pattern
1: procedure Calculate Pattern Length
2:
if usingSampleTime then
3:
numberOfElements ←maxLength / sampTime
4:
else
5:
if usingSamplingFrequency then
6:
numberOfElements ←maxLength * sampFreq
7:
if usingSamplingFrequency%2 = 0 then
8:
numberOfElements ←numberOfElements - 1
in time, that is, the number of elements in the array that the boundaries are
represented by (numberOfElements). To calculate the number of elements of the
array which represents the upper boundary, the maximum length of the QRS
complex in time (maxLength) is divided by the sample time (sampTime), i.e.
multiplied by the sampling frequency (sampFreq). Because the pattern needs to
have a central element for the R peak, if the array length is not an odd number,
the length is trimmed by one (Algorithm 1).
Although the length of the lower boundary corresponds to the minimum
length of the QRS complex duration, it is feasible to use a lower pattern bound-
ary with the same length as the higher pattern boundary length so that the
implementation of the algorithm that does the pattern check is made simpler.
That is why in order to keep the pattern functionality, the elements in range
[0, (maximum length −minimum length)/2] and [maximum length/2 + minimum
length/2, maximum length] have a practically unattainable value. This modiﬁ-
cation is shown on Fig. 3.
Fig. 3. A pattern with the unattainable values of the lower and upper boundaries
Both the amplitudes of the ﬁrst elements and last elements of the pattern
arrays have a value of zero. The amplitude value of the middle element rep-
resenting the R peak of the upper boundary is the value of the maximum R
peak amplitude −2000 µV, while the amplitude value of the middle element
www.ebook3000.com

Pattern Recognition of a Digital ECG
97
representing the R peak of the lower boundary is the value of the minimum R
peak amplitude −500 µV. The inclination from the ﬁrst element towards the
peak and the inclination from the peak towards the last element is linear, mean-
ing that the absolute diﬀerence between neighboring elements in the array is
constant. This incremental (ampincrement), i.e. decremental value (ampDecre-
ment) is calculated for the upper boundary of the pattern, proportionally to the
maximum amplitude value and the number of elements in the interval [0, max-
imum length/2 −ﬂatPeakRadius] (1). The variable ﬂatPeakRadius depicts the
length of the optionally ﬂattened peak of the pattern, since having a ﬂattened
peak at times proved to increase the eﬀectiveness of the QRS detection. The
variable middleIndex represents half the length of the pattern, with pattern ele-
ments’ indexing starting from 0. For the lower pattern boundary, the incremen-
tal/decremental values are proportional to the minimum amplitude value and
the number of elements in the interval [(maximum length −minimum length)/2,
maximum length/2], whilst the optional ﬂattened peak radius is unnecessary.
amplIncrement = −amplDecrement =
−maxAmplitude
middleIndex −flatPeakRadius
(1)
For the construction of personalized patterns, besides changing the maximum
and minimum amplitudes, along with the maximum and minimum lengths of the
QRS complex, several other changes can greatly increase the pattern eﬀective-
ness: applying an amplitude oﬀset to both the upper and lower boundary, and
applying a ﬂat peak to the R segment of the pattern. What the former modiﬁ-
cation does is it adds or subtracts a constant value to each of the array element
values, shifting the pattern along the amplitude axis. The latter modiﬁcation
gives a series of elements within a certain radius from the middle array element
a uniﬁed amplitude value - the maximum and minimum amplitude values for
the upper and lower boundaries accordingly. This modiﬁcation causes change in
the incremental/decremental value of the neighbor elements.
4
Using a Prebuilt Pattern for ECG Analysis
The algorithm input data consists of the digital ECG signal and the sampling
time/sampling rate, while the algorithm output consists of a set of detected signal
components. According to the needs, the output data is an array containing
either the components’ locations in the signal (timestamps, i.e. the indices of
the signal array elements) along with the amplitude values, or either only the
components’ locations. Regardless the lead that is being processed or the signal
component targeted for detection, the entire algorithm implementation stays the
same.
What makes the approach interesting is the fact that the voltage relative to
zero is irrelevant. What is taken into account is the relative amplitude diﬀerence
between the amplitude of an array element and the amplitudes of the elements
in the range [t-pattern length, t), where t is the time stamp of an element,
belonging to the interval [(pattern length)/2, ECG signal length].

98
M. Gusev et al.
Algorithm 2. Find QRS Complexes
1: procedure Check for pattern match
2:
for i = patternLength; i < ecgLength; i++ do
3:
QRSComplex ←True
4:
j ←patternLength −1
5:
while j > 0 AND QRSComplex do
6:
Diff ←ecgSignal[i] −ecgSignal[i −j]
7:
QRSComplex ←(Diff > lowPattern[patternLength −j −1])
AND (Diff > highPattern[patternLength −j −1])
8:
j ←j −1
For each of the elements of the array representing the digital ECG signal
in the interval [(pattern length)/2, ECG signal length] a pattern check is done
(the variable ecgLength depicts the number of elements of the array representing
the signal). That is achieved by calculating the relative diﬀerence with each of
the previous elements of the array, whilst the number of the previous elements
corresponds with the number of elements in the pattern (patternLength). For
each relative distance calculated, the diﬀerence needs to be lower then a corre-
sponding element value of the upper boundary of a pattern and greater then a
corresponding element value of the lower boundary of a pattern. The boolean
variable QRSComplex gets the value of an individual check and serves as an
indicator for the check of a single QRS complex. As soon as the check fails, the
search continues by looking for a next QRS complex. (Algorithm 2).
Because of how the diﬀerence of values is calculated and the order in which
the check is done, the original signal is being inverted both in terms of the
amplitude and in temporal terms. The double inversion is shown in Fig. 4.
Fig. 4. A personalized pattern with the original (green) and inverted signal (red)
The proposed approach can detect an R pattern that fully lies somewhere
in the ECG recording. In order to utilize the volume of data processed, i.e. to
enable detection of components that are partially part of the recording (signal
www.ebook3000.com

Pattern Recognition of a Digital ECG
99
components that are partially present at the begging and the end of the ECG
recording), a partial pattern check is in favor. However, in order for the partial
pattern recognition to work, it is necessary for the local maximum of the partial
signal component be present in the ECG recording. Taken that the element
currently processed lies within the interval [(pattern length)/2, pattern length]
(at the beginning of the recording) a pattern check is done for the elements down
to the ﬁrst element in the recording. At the end of the recording the same check
is done with some slight modiﬁcations in the algorithm.
5
Evaluation and Discussion
Before the QRS detection phase is to occur, the records are submitted to meta-
data format uniﬁcation, signal ﬁltration and normalization, as suggested in previ-
ous related work of the authors [11]. The ﬁltering phase deals with the removal of
baseline wander, caused mostly by perspiration, respiration and body movement
by using high-pass ﬁltering, while the high frequency noise such as power-line
interference, electromyography (EMG) noise, and instrumentation noise is dealt
with by using low-pass ﬁltering [2,13]. A bandpass ﬁlter with low-pass frequency
of 40 Hz and high-pass frequency of 0.5 Hz can be implemented by a variety of
ﬁlters, such as FIR ﬁlter (realized by the ARMA ﬁltering method. An IIR ﬁlter
(realized as Butterworth ﬁlter) achieves superior performance, but still lacks the
precision when compared to DWT or similar techniques [9].
Although the duration of the ECG signal components tends to stay within
corespondent marginal values in case of non-pathological cardiac conditions and
a number of pathological ones, the amplitude variations are substantial. The
pattern recognition approach used has taken care of the relative vertical ampli-
tude shift, the diﬀerences in vertical scaling cause certain issues during the QRS
detection phase, such as inadequate eﬀectiveness rate, or considerable rate of
false positives, due to P or T waves taken for QRS complexes. Therefore, simple
linear scaling transforms suﬃciently scale up the signals’ amplitudes.
To maximize the eﬀectiveness of the QRS complex detection and to minimize
the false positive rate, for each of the databases tested a batch analysis was per-
formed, to derive an optimal pattern for a particular database. However, since
a single pattern has been used per database, the eﬃciency is not comparable
to the eﬃciency of approaches that deal with arrhythmia classiﬁcation. Conse-
quently, the eﬃciency is signiﬁcantly higher for databases that do not contain
morphologically abnormal QRS complexes.
Three databases published by Physionet [5] were chosen as a subject to the
conducted analyses: the MIT-BIH Arrhythmia Database [10], the MIT-BIH Nor-
mal Sinus Rhythm Database and the ECG-ID Database [7]. For each database,
several parameters such as number of True Positives (TP), the number of False
Negative (FN) and False Positives (FP) constitute the eﬀectivity outliners: Sen-
sitivity (2), Speciﬁcity (3) and Accuracy (4).
Sensitivity =
TP
TP + FN
(2)

100
M. Gusev et al.
Specificity =
TP
TP + FP
(3)
Accuracy =
TP
TP + FP + FN
(4)
The MIT-BIH Arrhythmia Database is commonly used for arrhythmia clas-
siﬁcation benchmarking, set to include a variety of rare but clinically impor-
tant phenomena that would not be well-represented by a small random sample
of Holter recordings. Although the goal of the work does not include achiev-
ing high eﬃciency in the QRS detection for such databases, it is important to
demonstrate how the ﬁndings match up to advanced detection and classiﬁcation
algorithms. This database consists of 48 records, belonging to 47 subjects, stud-
ied by the BIH Arrhythmia Laboratory between 1975 and 1979. Each record
last little over 30 min, all of which have been accompanied with annotations
(temporally annotated QRS complexes).
Out of these 48 recordings, 39 recordings were included in the batch analysis.
The records that were omitted are the following: records 102, 104, 107, 717 due
to the use of Pacemaker, which distorts the QRS complex morphology in a way
that it cannot pass the pattern check; the patients that correspond with the
records 108, 114, 200 and 207 have been under the therapy of Digoxin, which
is known for causing potent ventricular ectopies; record 214 due to inadequate
signal normalization (in the beginning of the signal, a few strong ventricular
ectopies are present that consequently interfere with the normalization range).
The 39 recordings that were part of the batch analysis account 92,633 anno-
tations. The optimal pattern reached true positive rate, i.e. sensitivity rate of
95.94%, or 88,875/92,633 correctly annotated beats. The number of false posi-
tives is 3,300, delivering speciﬁcity rate of 96.41%. The accuracy rate is 92.64%.
The Normal Sinus Rhythm Database, as the name implies, consists of records
of people with no signiﬁcant arrhythmic behavior. The database counts 18
records. However, ﬁve of the records feature generic ECG pattern that is presum-
ably artiﬁcially generated, most likely, a malfunction in the Physionet system.
The records of this database account more then 2 days continuous recording.
Therefore, only the ﬁrst 30 min are taken into account for the batch analysis.
The 13 recordings that were part of the batch analysis account 31,740 anno-
tations. The optimal pattern reached true positive rate, i.e. sensitivity rate of
99.89%, or 31,690/31,740 correctly annotated beats. The number of false posi-
tives is 47, delivering speciﬁcity rate of 99.85%. The accuracy rate is 99.87%.
Finally, the ECG-ID Database contains 310 records obtained from 90 peo-
ple, collected periodically for over 6 months. The database was consolidated in
order to conduct research in the ﬁeld of biometric recognition via typical ECG
characteristics. This database features a statistical population representing typ-
ical samples, i.e. rate of arrhythmia very close to the average rate of arrhythmia
occurrence. Unfortunately, only the ﬁrst 10 beats of each recording have been
annotated.
www.ebook3000.com

Pattern Recognition of a Digital ECG
101
The 310 recordings that were part of the batch analysis account 3,102 anno-
tations. The optimal pattern reached true positive rate, i.e. sensitivity rate of
95.23%, or 2,954/3,102 correctly annotated beats. The number of false positives
is 175, delivering speciﬁcity rate of 94.40%. The accuracy rate is 90.14%.
It was anticipated that the pattern recognition approach would excell at
eﬀectivity when subjected on normal sinus rhythm signal. On the other hand,
the conducted analyses from the MIT-BIH Database and the ECG-ID Database
might as well seem satisfactory given the fact that the single pattern - pattern
recognition approach is not meant to suite speciﬁc scenarios where substantial
aberration in the signal morphology is present. However, such eﬀectiveness indi-
cators do not stand reliable given the important and crucial impact they could
have when a patients’ well being is of concern.
Related work in the ﬁeld of ECG QRS detection is notably focused on the
detection of abnormal hearth behavior, which is to be expected. Hence, most of
the work published deals with classiﬁcation of QRS complexes according to their
morphological, and closely related, physiological distinctions. Most of the QRS
detection methods involve wavelet transforms [8] and neural networks for the
QRS complex classiﬁcation [14]. However, there are also other, rather atypical
approaches to the matter, such as mathematical morphologies [12] and deriva-
tives [1].
Regarding the eﬀectiveness in the QRS detection methods, Martinez et al. [8]
made a summarization of the QRS detection results of a set of algorithms that are
appraised as state of the art algorithms in the ﬁeld. The asseblage features QRS
sensitivity in the range of 98.30%–99.89%, for the MIT-BIH Database. These
algorithms rely on wavelet transforms since the variety of ﬁltering techniques
gives an increase to the sensitivity.
6
Conclusion
Although the pattern recognition method might appeal unconventional in com-
parison to the other QRS detection methods, the onset results are promising. It
has exceled at sensitivity and speciﬁcity in the detection of QRS complexes of a
normal sinus rhythm, which was conﬁrmed by the results of the Physionet NSR
Database analysis.
What is of great importance to the eﬀectiveness of an algorithm which deals
with any type of QRS detection method, are the supplementary signal ﬁlters.
However, it is very important to stay low on complexity and demand for system
resources.
Although this version of the pattern recognition detection is not implemented
as a real-time streaming algorithm, conventional to be used by mobile devices,
the algorithm design would allow such an implementation with an ease, hence,
expanding the algorithm applicability. However, the eventual goal of any feature
work would be the increase of eﬀectivity. Hence, the algorithm design would eas-
ily allow is the simultaneous recognition of multiple patterns, which will intro-
duce the functionally of heart beat classiﬁcation and will engage a potential for
machine learning and fuzzy logic.

102
M. Gusev et al.
References
1. Arzeno, N.M., Deng, Z.D., Poon, C.S.: Analysis of ﬁrst-derivative based QRS detec-
tion algorithms. IEEE Trans. Biomed. Eng. 55(2), 478–484 (2008)
2. Blanco-Velasco, M., Weng, B., Barner, K.E.: ECG signal denoising and baseline
wander correction based on the empirical mode decomposition. Comput. Biol. Med.
38(1), 1–13 (2008)
3. Burns, N.: Cardiovascular physiology. Retrieved from School of Medicine, Trinity
College, Dublin (2013)
4. Garcia, T.B., et al.: 12-Lead ECG: The Art of Interpretation. Jones & Bartlett
Publishers, Burlington (2013)
5. Goldberger, A.L., Amaral, L.A., Glass, L., Hausdorﬀ, J.M., Ivanov, P.C., Mark,
R.G., Mietus, J.E., Moody, G.B., Peng, C.K., Stanley, H.E.: Physiobank, phys-
iotoolkit, and physionet components of a new research resource for complex phys-
iologic signals. Circulation 101(23), e215–e220 (2000)
6. Katz, L.N., Pick, A.: Clinical Electrocardiography. Lea & Febiger, Philadelphia
(1956)
7. Lugovaya, T.: Biometric human identiﬁcation based on electrocardiogram. Mas-
ter’s thesis, Faculty of Computing Technologies and Informatics, Electrotechnical
University LETI, Saint-Petersburg, Russian Federation (2005)
8. Mart´ınez, J.P., Almeida, R., Olmos, S., Rocha, A.P., Laguna, P.: A wavelet-based
ECG delineator: evaluation on standard databases. IEEE Trans. Biomed. Eng.
51(4), 570–581 (2004)
9. Milchevski, A., Gusev, M.: Performance evaluation of FIR and IIR ﬁltering of ECG
signals. In: ICT Innovations 2016. Advances in Intelligent Systems and Computing,
AISC, Springer series (2016, in press)
10. Moody, G.B., Mark, R.G.: The impact of the MIT-BIH arrhythmia database. IEEE
Eng. Med. Biol. Mag. 20(3), 45–50 (2001)
11. Ristovski, A., Guseva, A., Gusev, M., Ristov, S.: Visualization in the ECG QRS
detection algorithms. In: MIPRO, Proceedings of 39th International Convention
on ICT, IEEE Conference Proceedings, pp. 218–223 (2016)
12. Trahanias, P.: An approach to QRS complex detection using mathematical mor-
phology. IEEE Trans. Biomed. Eng. 40(2), 201–205 (1993)
13. Van Alste, J., Schilder, T.: Removal of base-line wander and power-line interference
from the ECG by an eﬃcient FIR ﬁlter with a reduced number of taps. IEEE Trans.
Biomed. Eng. 12, 1052–1060 (1985)
14. Xue, Q., Hu, Y.H., Tompkins, W.J.: Neural-network-based adaptive matched ﬁl-
tering for QRS detection. IEEE Trans. Biomed. Eng. 39(4), 317–329 (1992)
www.ebook3000.com

Performance Evaluation of FIR and IIR
Filtering of ECG Signals
Aleksandar Milchevski(B) and Marjan Gusev
FCSE, Ss. Cyril and Methodious University,
Rugjer Boshkovikj 16, 1000 Skopje, Macedonia
milchevski@gmail.com, marjan.gushev@finki.ukim.mk
Abstract. When a wearable ECG sensor transmits signals to a mobile
device, the mobile applications needs to be very eﬃcient and save the
limited mobile phone resources. This motivates us to ﬁnd an algorithm
implementation that is not computationally intensive, but still very eﬃ-
cient in denoising the ECG signal. The use of a window-based design
Finite Impulse Response (FIR) and Inﬁnite Impulse Response (IIR) ﬁl-
ters are analysed in this paper. Several ﬁlters have been designed and
the computational eﬃciency have been analysed both theoretically and
experimentally. The results show that the designed IIR outperforms the
FIR ﬁlter achieving a better computational eﬃciency with a minimal
distortion of the ECG signal.
Keywords: DSP · Performance · Speedup
1
Introduction
The electrocardiogram (ECG) has been one of the most used biomedical signals
since the beginning of the twentieth century. The information obtained from the
ECG can be used as an indication in the diagnoses of several medical conditions,
such us myocardial infarction [10,11], pulmonary embolism [3], epileptic seizures
[14], arrhythmia [8], etc. An important precondition in all of these procedures is
the successful acquisition and denoising of the ECG.
A method for removal of the baseline wander and power-line interference
from the ECG can use a Finite Impulse Response (FIR) ﬁlter with a reduced
number of taps in order to increase the eﬃciency. For example, a highpass ﬁl-
ter with a cut-oﬀfrequency of 0.7 Hz was used for the baseline wander removal
[13]. A notch Inﬁnite Impulse Response (IIR) ﬁlter can also ﬁlter out the power
line interference in the recording of the ECG [9]. Another approach for ECG
denoising is with the use of the Discrete Wavelet Transform (DWT). One such
method uses DWT for simultaneous denoising and feature extraction [2]. Other
techniques used for the ECG denoising include: Principal Component Analysis
(PCA) [7], Independent Component Analysis (ICA) [1], Empirical Mode Decom-
position (EMD) [12].
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 10

104
A. Milchevski and M. Gusev
A lot of issues challenged the programming of wearable ECG sensors and their
implementation in combination of smart mobile devices and cloud computing
[4]. To realize an eﬃcient implementation of ECG denoising by devices with a
small computational complexity, we analyse the use of a window-based design
FIR ﬁlter and a Butterworth IIR ﬁlter. Several ﬁlters have been designed and
the execution time has been analysed both theoretically and experimentally.
Filtering design issues are explained in Sect. 2 and the complexity is analyzed
in Sect. 3. Section 4 presents the testing methodology and evaluation of results.
Finally, the conclusions are presented in Sect. 5.
2
Filter Design for FIR and IIR Denoising of the ECG
The goal of this research is to compare FIR and IIR ﬁlters for denoising the
ECG signal. The initial assumption is to design ﬁlters that have almost equal
performance characteristics and then to analyze the computational complexity
of the both approaches.
The windowing design method is a simple method to design a FIR ﬁlter. The
procedure begins by modelling the ideal frequency response of the ﬁlter and then
by computing the impulse response. However, the computed impulse response is
not ﬁnite, hence, there is a need for windowing operation in order to obtain a
ﬁnite impulse response. The choice of the window is a trade-oﬀbetween the size
of the main lobe inﬂuencing the transition width and the peaks of the side lobes
inﬂuencing the ripples in the non-transition bands.
The Butterworth IIR ﬁlter is one of the vastly used IIR ﬁlters. The magnitude
response of the ﬁlter is maximally ﬂat i.e. it is a monotonic function. Compared
to the other well known IIR ﬁlters (such us the Chebyshev and elliptic ﬁlters)
requires a higher order to obtain the same design speciﬁcations. However, it has a
more linear phase characteristic. Although originally designed as an analog ﬁlter
it can be digitized using a bilinear transform (1), setting a relation between the
analogue and digital frequencies, marked by ωa and ωd correspondingly, where
T is the sampling period.
ωa = 2
T tanωdT
2
(1)
The design approaches to realize similar performance characteristics of the
designed FIR and IIR ﬁlters is not an easy task, since it is hard to set the order
of the ﬁlters so they are equally good, because both ﬁlters have positive prop-
erties which the other ﬁlter does not. For example, the designed FIR ﬁlter has
a linear phase characteristic and the designed Butterworth ﬁlter has maximally
ﬂat magnitude response.
Therefore, we have set a goal to design ﬁlters that will have an equal width
of the transition band, the passband ripple, and the stopband attenuation. The
design process should determine the ﬁlter order M and the ﬁlter kernel coeﬃ-
cients.
The transition width W of a FIR ﬁlter depends on the ﬁlter order MF IR
and the type of window used. For the Hamming window, the transition width
www.ebook3000.com

Performance Evaluation of FIR and IIR Filtering of ECG Signals
105
expressed in Hz is approximated with the Eq. (2), where Fs is the sampling
frequency.
W =
3.3
MF IR
Fs
(2)
Considering that the transition width is distributed equally from the critical
frequency Fc (set to 40 Hz in this case) the normalized passband and stopband
frequencies, correspondingly marked by θp and θs, can be calculated by (3) and
(4) correspondingly.
θp = (40 −W/2)(2/Fs)
(3)
θs = (40 + W/2)(2/Fs)
(4)
After determination of the stopband and passband frequencies, the order of
the digital Butterworth ﬁlter can be found with the Eq. (5), where Rs is the
stopband attenuation and Rp the passband ripple.
MIIR =
⎡
⎢⎢⎢
log10[(10−0.1Rs −1)(10−0.1Rp −1)]
2log10
tg( π
2 θs)
tg( π
2 θp)
⎤
⎥⎥⎥
(5)
To eliminate the high-frequency noise in the ECG signal, we designed a low-
pass FIR ﬁlter using the Hamming window with length of 101 element, using
M = 100 and calculating the convolution coeﬃcients w[i] for i = 0, . . . , M by
w[i] = 0.54 −0.46cos(2πi/M).
The cut-oﬀfrequency is set to 40 Hz. The minimum stopband attenuation for
the Hamming window is −53 dB, and the transition width (3.3/M)500 Hz, deter-
mining the transition band for the designed ﬁlter (31.75, 48.25) Hz, as presented
in Fig. 1.
Fig. 1. Amplitude and phase response of the designed lowpass ﬁlters
A low-pass Butterworth IIR ﬁlter is also designed to eliminate the high-
frequency noise with design parameters presented in Table 1.
Figure 2 shows the output of the low-pass FIR ﬁlter. The blue line is the input
in the ﬁlter (original ECG signal) and the red line in the ﬁgure is the output
from the ﬁlter. It can be seen that the noise initiated by a power supply has
been successfully removed. The Butterworth IIR ﬁlter shows the same results of
removing the high-frequency noise in the ECG signal.

106
A. Milchevski and M. Gusev
Table 1. Design parameters of the lowpass Butterworth ﬁlter
Passband corner frequency fp = 30 Hz
Stopband corner frequency fs = 50 Hz
Passband ripple
Rp = 0.1 dB
Stopband attenuations
Rs = 30 dB
Fig. 2. Results by applying a low-pass ﬁlter
A high pass ﬁlter can be used to eliminate the low-frequency noise initiated
by a physical movement or breathing, represented by low frequencies. Similarly
to the previous section we design two ﬁlters for this task: a FIR ﬁlter using
Hamming window and a digital Butterworth IIR ﬁlter. The ﬁlters are evaluated
on an ECG that contains baseline drift artifacts.
A high pass ﬁlter with a narrow transition band is required in order to pre-
serve the details and characteristic features from the ECG as much as possible.
In order to achieve this, the ﬁlter order must be increased signiﬁcantly compared
to the previous section. For this case, we design a FIR ﬁlter with an order of
1000 and a cutoﬀfrequency of 0.5 Hz. The design parameters for the fourth order
Butterworth IIR ﬁlter are shown in Table 2.
Table 2. Design parameters of the highpass Butterworth ﬁlter
Passband corner frequency fp = 0.2 Hz
Stopband corner frequency fs = 0.8 Hz
Passband ripple
Rp = 0.1 dB
Stopband attenuations
Rs = 30 dB
Figure 3 shows the results obtained when an ECG signal is processed by a
high-pass ﬁlter. The analysis shows that the baseline drift is removed successfully
with both designed ﬁlters.
However, a distortion of the results is present when applying the IIR But-
terworth ﬁlter due to non-linear phase, as presented by the yellow line in Fig. 4
when an ECG signal (blue line) is processed.
www.ebook3000.com

Performance Evaluation of FIR and IIR Filtering of ECG Signals
107
Fig. 3. Results by applying a high-pass ﬁlter
Fig. 4. A distortion by the non-linear phase and its elimination on a high-pass and
band-pass Butterworth ﬁlter, respectively
The solution for this problem in the case of the high-pass IIR ﬁlter is the
use of the zero-phase digital ﬁltering. The technique consists of two steps: in
the forward step the input signal is ﬁltered with the designed ﬁlter and in the
backward step the obtained result is ﬂipped in time prior to the ﬁltering with
the same ﬁlter [5]. This procedure squares the amplitude response of the ﬁlter
and zeros the phase response, which cancels out the distortions due to the non-
linear phase of the designed ﬁlter, as presented by the red line in Fig. 4. The
negative side is that the number of operations using this technique is doubled
and additional requirements are set, such as to buﬀer the whole input and output
streams.
The bandpass ﬁlter combines the properties of the two previously designed
ﬁlters i.e. it should remove both low-frequency and high-frequency noise. Again,
two ﬁlters are designed: a FIR ﬁlter using Hamming window and a digital Butter-
worth IIR ﬁlter. The order of the combined FIR ﬁlter is equal to the higher value
of 1000, and the two critical frequencies are 0.5 Hz and 40 Hz. The design para-
meters for the Butterworth ﬁlter are combined from Tables 1 and 2. The order
of the designed Butterworth ﬁlter is 10. It should be noted that an attempt was
made to design a Butterworth ﬁlter with a narrower transition band and smaller
ripples, however, the obtained ﬁlter was unstable.
Figure 5 presents the results of processing an ECG signal with the designed
bandpass FIR ﬁlters. The results show that the baseline drift and the high-
frequency noise are removed successfully.

108
A. Milchevski and M. Gusev
Fig. 5. Results by applying a bandpass ﬁlter to an ECG signal
3
Programming Issues and Complexity Analysis
One sample of the output of an M-th order FIR ﬁlter deﬁned by a transfer
function in (6) is calculated and can be programmed by (7).
H(z) = b0 + b1z−1 + b2z−2 + . . . + bMz−M
(6)
y[n] = b0x[n] + b1x[n −1] + . . . + bMx[n −M]
(7)
The number of arithmetic operations is easy to obtain i.e. the number of
multiplications is M + 1 and the number of additions is M, with a total of
2M + 1 operations. The number of memory accesses per sample is 2M + 1 to
access the ﬁlter kernel coeﬃcients and a corresponding segment of the input
stream, plus the access to the processed output.
A programming solution P1 that aims at keeping low memory buﬀers uses
buﬀers of only last M input stream samples and M ﬁlter coeﬃcients. Although
this solution requires smaller memory requirements, still it generates additional
M rotation operations per sample, to keep the buﬀer elements of the input stream
aligned to the last processed sample. Another solution (P2) will be to buﬀer
the whole input data stream, requiring a high memory capacity, but without
memory rotation operations. A solution that uses a circular buﬀer was reported
in the case of a wavelet ﬁlter [6] can also be used in this case leading to a
programming solution P3 without extended memory capacity requirements and
additional memory rotation operations.
The number of arithmetic operations for the IIR ﬁlter can be obtained by a
transfer function (8) and calculated by (9).
H(z) = b0 + b1z−1 + b2z−2 + . . . + bMz−M
1 + a1z−1 + a2z−2 + . . . + aMz−M
(8)
y[n] = b0x[n] + b1x[n −1] + . . . + bMx[n −M]
(9)
−a1x[n −1] + . . . + aMx[n −M]
Therefore, the number of multiplications is equal to M+(M+1) = 2M+1, the
number of additions and subtractions is M, thus, the total number of arithmetic
www.ebook3000.com

Performance Evaluation of FIR and IIR Filtering of ECG Signals
109
operations equals to 4M +1. Additionally, a total of 4M +2 memory accesses are
required to access the ﬁlter kernel coeﬃcients and the corresponding segments
of the input stream samples and previous outputs.
To eliminate the distortion introduced by the high-pass ﬁltering, the number
of operations is doubled due to the use of the zero phase ﬁltering technique,
ending with a total of 2(4M + 1) = 8M + 2 arithmetic operations. In addition,
the sequence has to be inverted prior to the second operation, and then once
again at the end, which asks for two more runs of the whole procedure to scan
the input stream. If expressed as a number of operations per sample, this will
need 2 more memory rotation operations per sample. Thus, the total number of
memory accesses per sample is equal to 8M + 6.
The zero phase technique makes an additional requirement, such as the neces-
sity to store the whole input and output streams. Similar to the previous analysis
of programming approaches, one can use the P1 approach to saving the mem-
ory buﬀers and use only 4 buﬀers with a length of M for the input and output
streams and to keep the ﬁlter kernel coeﬃcients. This solution can be realized
to process the input stream and write data to the output ﬁle, then to invert the
ﬁle and process once again and ﬁnally to invert the result ﬁle. In this case, the
solution requires rotation of 2M buﬀer elements to keep the updated input and
output data elements aligned to the last processed sample.
The second programming solution P2 requires big memory buﬀers to store
the input and output streams. However, it does not need to rotate the buﬀer
elements and does not need to write the ﬁle and invert the ﬁle. The third solution
P3 uses a circular buﬀer and does not need memory rotation operations, but still
needs to keep the output stream elements in the memory to enable the zero phase
elimination with the reverse calculation.
4
Performance Evaluation
The memory accesses, storage requirements, and cache behaviour complicate the
overall analysis and an experimental analysis was used for further clariﬁcation.
The performance of the designed ﬁlters has been tested with a real imple-
mentation with an input signal with a length of 10000 up to 500000 samples,
where the sampling frequency is 500 Hz. The designed FIR ﬁlter has order of
M = 1000 elements and the Butterworth IIR ﬁlter M = 10. The experiments
were performed on a computer with 4 core processor with 1.2 GHz and 4 GB
RAM. The execution code was compiled for one core without compiler opti-
mization to extract the essential behavior of the algorithm.
The experiments were conducted for the following programming solutions:
– P1 - small buﬀer requirements with corresponding memory rotation opera-
tions,
– P2 - high buﬀer requirements without memory rotation operations, and
– P3 - using a circular memory buﬀers.

110
A. Milchevski and M. Gusev
Fig. 6. Speedup of the P2/P3 solutions over the P1 solution for IIR and FIR ﬁlters
Fig. 7. Speedup of the designed IIR over FIR ﬁlter
We have provided at least ﬁve test runs for each input signal and calculated
the corresponding average response time.
The speedup comparing two diﬀerent solutions is calculated as a ratio of
the corresponding default response time and the new analyzed solution. We will
present speedup comparison of diﬀerent solutions for the same ﬁlter and also a
speedup of the IIR ﬁlter over the FIR ﬁlter.
Note that the time response of the programming approach P3 is almost the
same as the P2, with diﬀerence that can be neglected. Since the P3 requires
smaller memory buﬀers, we will only compare the P3 solution over the P1.
Figure 6 presents the time performance speedup of obtained programming
solutions P3 (or P2) over the P1. The average speedup of the P3 solution over
the P1 solution is 1.373 for the IIR ﬁlter and 2.672 for the FIR ﬁlter. A higher
speedup is obtained in the case of FIR ﬁlter since the ﬁlter length is bigger and
more memory rotate operations are required. The solution that uses a circu-
lar buﬀer outperforms the conventional solution, not just in reduced memory
requirements, but also in response time performance.
The speedup of the IIR ﬁlter over the FIR ﬁlter for the same transition band,
passband ripple and stopband attenuation is presented in Fig. 7. An average
speedup value is 4 and slightly rises as the ﬁlter length is higher.
www.ebook3000.com

Performance Evaluation of FIR and IIR Filtering of ECG Signals
111
The presented analysis shows that the designed Butterworth ﬁlter is compu-
tationally more eﬃcient compared to the designed FIR ﬁlter. Another positive
side of the Butterworth ﬁlter is that it has maximally ﬂat magnitude response
which the window based designed FIR does not. Although the zero phase elim-
ination required double the number of computations, still it was more eﬃcient.
5
Conclusion and Future Work
The analyzed FIR and Butterworth ﬁlters are computationally eﬃcient, but they
lack a precision when compared to DWT or similar techniques. For example, the
denoising techniques based on DWT are better suited for non-stationary signals,
such as the ECG, and also provide a better starting point for further feature
extraction.
However, the simplicity and eﬃcient algorithm realization with small memory
requirements and processing power are the main features motivating us to use
them in the development of a mobile cloud application.
The theoretical analysis and experiments prove that the Butterworth IIR
ﬁlter outperforms the FIR ﬁlter in obtaining a ﬂat amplitude characteristic and
also the programming solution obtains smaller response times compared to the
FIR ﬁlter. In addition, a software solution that uses a circular buﬀer outperforms
the conventional approach of using classical memory buﬀers.
The provided analysis is useful to be applied to mobile devices in the IoT
world for healthcare, where the essential approach is toward saving the battery
life, constrained by limited memory capacity and processing power.
References
1. Barros, A.K., Mansour, A., Ohnishi, N.: Removing artifacts from electrocardio-
graphic signals using independent components analysis. Neurocomputing 22(1),
173–186 (1998)
2. Faezipour, M., Tiwari, T.M., Saeed, A., Nourani, M., Tamil, L.S.: Wavelet-based
denoising and beat detection of ECG signal. In: IEEE/NIH Life Science Systems
and Applications Workshop, LiSSA 2009, pp. 100–103. IEEE (2009)
3. Geibel, A., Zehender, M., Kasper, W., Olschewski, M., Klima, C., Konstantinides,
S.: Prognostic value of the ECG on admission in patients with acute major pul-
monary embolism. Eur. Respir. J. 25(5), 843–848 (2005)
4. Gusev, M., Stojmenski, A., Chorbev, I.: Challenges for development of a mobile
application for ECG detection and feature extraction. J. Emerg. Res. Solut. ICT
1(2) (2016, in Press )
5. Gustafsson, F.: Determining the initial states in forward-backward ﬁltering. IEEE
Trans. Signal Process. 44(4), 988–992 (1996)
6. Milchevski, A., Gusev, M.: Improved pipelined wavelet implementation for ﬁltering
ECG signals. Technical report 7/2016, University Sts Cyril and Methodius, FCSE,
LiiT (2016)
7. Moody, G.B., Mark, R.G.: QRS morphology representation and noise estimation
using the Karhunen-Loeve transform. In: Proceedings of the Computers in Cardi-
ology 1989, pp. 269–272. IEEE (1989)

112
A. Milchevski and M. Gusev
8. Owis, M.I., Abou-Zied, A.H., Youssef, A.B.M., Kadah, Y.M.: Study of features
based on nonlinear dynamical modeling in ECG arrhythmia detection and classi-
ﬁcation. IEEE Trans. Biomed. Eng. 49(7), 733–736 (2002)
9. Pei, S.C., Tseng, C.C.: IIR multiple notch ﬁlter design based on allpass ﬁlter. IEEE
Trans. Circuits Syst. II: Analog Digit. Signal Process. 44(2), 133–136 (1997)
10. Schr¨oder, R.: Prognostic impact of early ST-segment resolution in acute ST-
elevation myocardial infarction. Circulation 110(21), e506–e510 (2004)
11. Schroder, R., Wegscheider, K., Schroder, K., Dissmann, R., Meyer-Sabellek, W.,
Group, I.T., et al.: Extent of early ST segment elevation resolution: a strong pre-
dictor of outcome in patients with acute myocardial infarction and a sensitive
measure to compare thrombolytic regimens. A substudy of the international joint
eﬃcacy comparison of thrombolytics (INJECT) trial. J. Am. Coll. Cardiol. 26(7),
1657–1664 (1995)
12. Taigang, H., Cliﬀord, G., Tarassanko, L.: Application of ICA in removing artefacts
from the ECG. Neural Process. Lett. 10(1), 1–5 (2004)
13. Van Alste, J., Schilder, T.: Removal of base-line wander and power-line interference
from the ECG by an eﬃcient FIR ﬁlter with a reduced number of taps. IEEE Trans.
Biomed. Eng. BME–32(12), 1052–1060 (1985)
14. Zijlmans, M., Flanagan, D., Gotman, J.: Heart rate changes and ECG abnormali-
ties during epileptic seizures: prevalence and deﬁnition of an objective clinical sign.
Epilepsia 43(8), 847–854 (2002)
www.ebook3000.com

Inﬂuence of Fuzzy Tolerance Metrics
on Classiﬁcation and Regression Tasks for
Fuzzy-Rough Nearest Neighbour Algorithms
Andreja Naumoski(&), Georgina Mirceva, and Petre Lameski
Faculty of Computers Science and Engineering,
Ss. Cyril and Methodius University in Skopje, Skopje, Macedonia
{andreja.naumoski,georgina.mirceva,
petre.lameski}@ﬁnki.ukim.mk
Abstract. In this paper, we investigate the inﬂuence of the fuzzy tolerance
relationship (fuzzy similarity metrics) on two fuzzy and two fuzzy-rough nearest
neighbour algorithms for both classiﬁcation and regression tasks. The fuzzy
similarity metric plays a major role in construction of the lower and upper
approximations of decision classes, and therefore has high inﬂuence on the
accuracy of the algorithm. The experimental results evaluated on the four
approaches show the difﬁculty to estimate a single metric that will be good in all
cases. Moreover, the choice of similarity metric on some datasets has not
inﬂuence at all. This require further investigation, not only with similarity
metrics, but also for evaluating the algorithms with different T-norms and
implicators.
Keywords: Fuzzy tolerance relationship  Fuzzy rough sets  k-nearest
neighbour  Classiﬁcation  Regression
1
Introduction
From the set of well-used machine learning algorithms, the k-nearest neighbour (k-NN)
is among the simplest algorithms. It is a non-parametric, instance-based learning, or
lazy learning algorithm, that can be used for both classiﬁcation and regression tasks. If
k-NN is used for classiﬁcation, the output is a class membership derived by a majority
voting of its neighbours. In classiﬁcation task, the class membership is assigned to the
class that is most common among the k nearest neighbours, while in regression the
output is average values of the k neighbours. An extension of the k-NN algorithm to
fuzzy set theory (k-FNN) was introduced in [1]. This extension allows partial mem-
bership of an object to different classes and the algorithm is aware of the relative
closeness of the test instance. Nonetheless, this improved extension is not without
problems, as the author argued in [2], the k-FNN is not able to deal with the insufﬁcient
knowledge that is part of each real-time problem. The problem lies in need of satisfying
the rule that all predicted membership degrees of various classes need to sum up to 1.
Consequently, the algorithm makes clear-cut predictions, by removing every training
pattern from the test object, and therefore no suitable neighbour is found. The authors
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_11

in [2] tried to solve this problem by introducing fuzzy-rough ownership function (this
algorithm is referred as k-FRNNOn throughout this paper), which does not refer to the
main ingredients of the rough set theory: lower and upper approximations. Therefore,
in [3] the authors proposed alternative algorithms to cope with this problem. These
approaches, named FRNN and VQNN in this paper, uses test objects, that are nearest
neighbours, to construct the lower and upper approximation of each decision class [3].
Later it computes the membership of the test object to these approximations [3].
The two algorithms in [3] are very ﬂexible, and can be deﬁned with different
fuzzy-rough approximations, including implicator/T-norm combination
[4] and
vaguely quantiﬁed rough set (VQRS) model [3, 5]. That’s why the aim of this paper is
to investigate the inﬂuence of four fuzzy similarity metrics on the AUC-ROC metric
(for classiﬁcation task) and Relative Root Mean Square Error (RRMSE) metric (for
regression task) using various datasets with four k-FNN algorithms. Furthermore, the
overall results between the fuzzy k-NN and fuzzy-rough k-NN are compared.
The rest of the paper is organised as follows. Section 2 provides necessary details
for fuzzy rough set theory, while Sect. 3 is concerned with the deﬁnition of the fuzzy
and fuzzy-rough k-NN algorithms. Section 4 outlines the experimental approach and
presents experimentation results on a series of classiﬁcation and regression problems.
The paper is concluded in Sect. 5.
2
Deﬁnition of Fuzzy Rough Sets
The deﬁnition of the fuzzy rough set theory is closely related to the properties of both
fuzzy and rough set theory. First, the rough set theory [6] provides understating how
the knowledge can be extracted from a particular domain, so it’s able to retain the
information content while reducing the amount of knowledge involved [3]. The
fuzzy-rough set theory provides advantages for both rough and fuzzy sets. In order to
join the advantages of indiscernibility and the fuzzy uncertainty, in this section deﬁne
some key elements that are necessary for rough sets.
In order to solve classiﬁcation or regression problems, let’s consider simple
information system (U, A), where U is a non-empty set of ﬁnite objects (the universe of
discourse) and A is a non-empty ﬁnite set of attributes. Furthermore, for machine
learning task, the set A is union between C (the set of conditional attributes) and d (the
decision or class attribute). The relationship between the elements of the information
system (U, A) is deﬁned as U ! Va for every a 2 A, where Va is the set of values that
attribute a may take. According the rough set theory, with any B  A there is an
associated equivalence relation RB:
RB ¼ fðx; yÞ 2 U2j8a 2 B; aðxÞ ¼ aðyÞg:
ð1Þ
If (x, y) 2 RB, then x and y are indiscernible by attributes from B. “The equivalence
classes of the B-indiscernibility relation are denoted [x]B. If A  U, then A can be
approximated using the information contained within B by constructing the B-lower
and B-upper approximations of A” [3]:
114
A. Naumoski et al.
www.ebook3000.com

RB;A ¼ fx 2 Uj½xBAg
ð2Þ
RB;A ¼ fx 2 Uj½xB \ A 6¼ 0g:
ð3Þ
The tuple (RB;A; RB:A) is called a rough set. However, using this deﬁnition above,
the methods can only operate when dataset contains only discretised values. Therefore,
it is necessary to perform a discretization step before analysing the dataset. In this step,
applying fuzzy theory is intuitive and logical step of events, and also ﬂexible. By
applying fuzzy theory, we apply the means of fuzzy relations R in U, so U ! [0, 1], so
the algorithm assigns to each couple of objects a degree of similarity. In order this to
work, it is assumed that R is at least fuzzy tolerance relations satisfying two conditions:
R (x, x) = 1 and R (x, y) = R (y, x) for x and y in U. Given y in U, its forest Ry is deﬁned
by Ry (x) = R (x, y) for every x in U [3]. So now, we can deﬁne the fuzzy lower and
upper approximations of A by R. There are several ways to construct the fuzzy upper
and the lower approximations, but one general deﬁnition [5, 7] is following:
RAðxÞ ¼ inf
y2U
IðRðx; yÞ; AðyÞÞ
ð4Þ
RAðxÞ ¼ sup
y2U
TðRðx; yÞ; AðyÞÞ:
ð5Þ
In Eqs. 4 and 5, I is an implicator and T is a T-norm. In order to get the traditional
lower and upper approximations, we need to set A as a crisp set and R is an equivalence
relation in U. An implicator I is a [0, 1]2 ! [0, 1] mapping that is decreasing in its ﬁrst
and increasing in its second argument, satisfying I (0, 0) = I (0, 1) = I (1, 1) = 1 and
I (1, 0) = 0. On the other hand, T-norm T is an increasing, commutative, associative
[0, 1]2 ! [0, 1] mapping that satisﬁes T (x, 1) = x for x in [0, 1].
Deﬁned in this way, the fuzzy rough set approximations are very sensitive to noise
values, which means a small change in a single object can result in changes of
approximations. This is due to the use of sup and inf, which generalize the existential
and universal quantiﬁer [3]. In context of classiﬁcation and regression tasks, this will
have effect on the accuracy of the predictive model. Therefore, another algorithm
proposed in [3], the concept of quantiﬁed rough sets (VQRS) is introduced in [3]. This
algorithm uses linguistic quantiﬁers, as opposed to the traditional T-norms and
implicators, to decide to what extend an object belongs to the lower and upper
approximation. In this algorithm the couple (Q1, Q2) represents the fuzzy quantiﬁers
that model of linguistic quantiﬁers, then the lower and upper approximation of A by
R are deﬁned as follows:
RQ1ðyÞ ¼ Q1ðjRy \ Aj
Ry
Þ ¼ Q1ð
P
x2X
minðRðx; yÞ; AðyÞÞ
P
x2X
Rðx; yÞ
Þ
ð6Þ
Inﬂuence of Fuzzy Tolerance Metrics on Classiﬁcation and Regression Tasks
115

RQ2ðyÞ ¼ Q2ðjRy \ Aj
Ry
Þ ¼ Q2ð
P
x2X
minðRðx; yÞ; AðyÞÞ
P
x2X
Rðx; yÞ
Þ;
ð7Þ
where the fuzzy set intersection is deﬁned by the min T-norm and the fuzzy set
cardinality by the sigma-count operation [3]. If we make comparison between the fuzzy
lower and upper approximations, the VQRS approximations do not extend the classical
rough set and they can be still fuzzy.
3
The Algorithms
3.1
k-Fuzzy Nearest Neighbour Classiﬁcation (k-FNN and k-FRNNOn)
The ﬁrst fuzzy k-NN (k-FNN) algorithm based on fuzzy theory proposed in [1], have
purpose to classify test objects based on their similarity for a given number of
neighbours together with the training objects in combination with the neighbours’
membership degrees whatever they are crisp or fuzzy relative to the class labels. In
order this to work, extension of the function needed to classify object y that belongs to
class C is computed as:
C0ðyÞ ¼
X
x2N
Rðx; yÞCðxÞ:
ð8Þ
In this formula N is a set of the K nearest neighbours of y, and R (x, y) is a similarity
between x and y that has values in the interval [0,1]. There is great difference on how
R (x, y) is calculated in crisp, fuzzy and fuzzy-rough sets. If we consider a classical or
crisp k-NN, the function R (x, y) is deﬁned as:
Rðx; yÞ ¼
y  x
k
k2=ðm1Þ
P
j2N
y  jk2=ðm1Þ

;
ð9Þ
where || y −x || denotes the Euclidean norm, and m is a parameter that controls the
overall weighting of the similarity. If we take a closer look on the relationship between
the variables in this formula, it is easy to note that the degree of closeness of neighbours
will inﬂuence the impact that their class membership has on the process of obtaining
the membership of the test object. The ﬁrst attempt to merge this algorithm with the
concepts from the fuzzy rough set theory was presented in [4, 7]. The authors from
these papers, constructed fuzzy-rough ownership function in order to handle both
“fuzzy uncertainty” and the “rough uncertainty”. The fuzzy-rough ownership function
for object y and class C is deﬁned as:
sCðyÞ ¼
P
x2U
Rðx; yÞCðxÞ
jUj
;
ð10Þ
116
A. Naumoski et al.
www.ebook3000.com

while the fuzzy relationship R is given as:
Rðx; yÞ ¼ exp
X
a2C
U
2 P
x2U
aðyÞ  aðxÞk
k
2=ðm1Þ
0
B
@
1
C
A
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Band
ðaðyÞ  aðxÞÞ2=ðm1Þ
0
B
B
B
B
B
B
@
1
C
C
C
C
C
C
A
;
ð11Þ
where m controls the weighting of the similarity (as in k-FNN) and Band is a parameter
that decides the bandwidth of the membership. In terms of applying this in k-NN
classiﬁcation problem, Eq. 11 will consider the conﬁdence that y will be classiﬁed in
C. This algorithm will be referred as k-FRNNOn in this paper.
Compared with the k-FNN algorithm, k-FRNNOn algorithm takes into account all
training objects, and not limiting his search space only on the set of neighbours. By
doing this, the algorithm does not require the number of neighbours to be considered. It
is obvious that the very distant training objects will not inﬂuence the outcome, which is
very different from the case of k-FNN. It is important to note that this algorithm doesn’t
uses lower and upper approximations to determinate the class membership [2].
3.2
k-Fuzzy-Rough Nearest Neighbour (FRNN and VQNN) Algorithms
The authors in [3] proposed two algorithms that combines the fuzzy-rough approxi-
mations with the ideas of the k-FNN approach, i.e. the FRNN and k-FNN algorithm
with VQRS approximations (VQNN). According [3], the algorithm uses lower and
upper approximations of the decision class that is calculated by means of the nearest
neighbour of a test object y. This give to the algorithm a good clue to predict the
membership of the test object to that class. For further details, the reader can read paper
[3]. In order these two algorithms to perform a crisp classiﬁcation, the algorithms
output the decision class with the resulting best fuzzy lower and upper approximation
memberships. In this way both algorithms utilize the information for the fuzzy lower
and upper membership approximations to determinate the class membership [3].
Because these algorithms depend from the choice of the fuzzy tolerance relationship as
any k-FNN algorithm, a general way of constructing R given by [3] is deﬁned as
follows:
Rðx; yÞ ¼ min
a2C Raðx; yÞ;
ð12Þ
which Ra(x, y) is the degree to which objects x and y are similar for attribute a. In this
paper, we investigate the inﬂuence of four possible Ra(x, y) (fuzzy tolerance relation-
ship) deﬁnitions:
Raðx; yÞ ¼ 1  jaðxÞ  aðyÞj
amax  amin
ð13Þ
Inﬂuence of Fuzzy Tolerance Metrics on Classiﬁcation and Regression Tasks
117

Raðx; yÞ ¼ expð ðaðxÞ  aðyÞÞ2
2r2
a
Þ
ð14Þ
Raðx; yÞ ¼ maxðminððaðyÞ  ðaðxÞ  raÞÞ
ðaðxÞ  ðaðxÞ  raÞÞ ; ððaðxÞ þ raÞ  aðyÞÞ
ððaðxÞ þ raÞ  aðxÞÞÞ; 0Þ
ð15Þ
Raðx; yÞ ¼ maxð0; minð1; b  a  aðxÞ  aðyÞ
ra
ÞÞ; a ¼ 0:5; b ¼ 1:0;
ð16Þ
where r2
a is the variance of attribute a, and amax and amin are the maximal and minimal
occurring value of that attribute. The four similarity fuzzy metrics will be further tested
and we will examine their inﬂuence on the four algorithms. Each fuzzy tolerance
relationship will be referred with the equation number, i.e. similarity 14 will be referred
as Eq. 14.
4
Experimentation
4.1
Experimental Setup
This section presents the experimental setup of the fuzzy and fuzzy-rough classiﬁcation
algorithms k-FNN, k-FRNNOn, FRNN and VQNN evaluation, over sixteen benchmark
datasets from [8, 9]. Datasets WatersClass (C4–C8) are used to experimentally evaluate
the inﬂuence of the methods on classiﬁcation tasks, while the Datasets Waterreg (R1–
R8) have been derived from [10], in order to show the inﬂuence of the similarity metric
in regression tasks. Detail description for each dataset can be found in Table 1. All
datasets contain real value attributes.
Since, we don’t study the effect of the number of k nearest neighbours, but the
effect of the similarity metrics, we leave the default values for each algorithm as they
are proposed in their original papers. For the k-FNN and k-FRNNO, we also leave the
Table 1. Characteristics of the datasets used for experimental evaluation
Dataset for classiﬁcation Classiﬁcation
Datasets for regression Regression
Objects Attributes
Objects Attributes
Glass (C1)
214
10
Waterreg1 (R1)
218
17
Ionosphere (C2)
351
35
Waterreg2 (R2)
218
17
Vote (C3)
435
17
Waterreg3 (R3)
218
17
Waveform (C4)
5000
41
Waterreg4 (R4)
218
17
Waterclass1 (C5)
218
22
Waterreg5 (R5)
218
17
Waterclass2 (C6)
218
112
Waterreg6 (R6)
218
17
Waterclass3 (C7)
57
22
Waterreg7 (R7)
218
17
Waterclass4 (C8)
57
96
Waterreg8 (R8)
218
17
118
A. Naumoski et al.
www.ebook3000.com

default value of m to 2. As we mentioned, for FRNN approach we used the Algebraic
T-norm (T (x, y) = x * y) and the KD implicator (I (x, y) = max (1 −x, y)), while the
VQNN approach as deﬁned in [3] is used with the following settings: Ql = Q (0.1; 0.6)
and Q2 = Q (0.2; 1.0), according Eq. 17.
Qða;bÞðxÞ ¼
0;
x  a
2ðxaÞ2
ðbaÞ2 ;
a  x  a þ b
2
1  2ðxaÞ2
ðbaÞ2 ;
a þ b
2
 x  b
1;
b  x
8
>
>
>
>
<
>
>
>
>
:
:
ð17Þ
Because k-FNN algorithm has no option for similarity metric, that’s why we give
results only for one experiment. The presentation of the experiments is divided in two
separate tables for both classiﬁcation and regression tasks.
4.2
Experimental Results
In this section, the results from the classiﬁcation experiments are presented. In the
tables, all results are given for each algorithm in the following form: MethodSimilarity
Metric. The evaluation metric used to analyse the results is the AUC-ROC metric for
classiﬁcation task, and Relative Root Means Square Error (RRMSE) for regression
task.
In Table 2, the experimental results depict the inﬂuence of each similarity metric
for each algorithm. If we compare the results obtained by each of the algorithms, it is
noticeable that FRNN and VQNN algorithms achieved best AUC-ROC accuracy.
Looking in the results achieved between fuzzy k-NN and fuzzy-rough k-NN algo-
rithms, the results are tie. A more detail investigation shows that between k-FRNNOn
Table 2. Results for AUC-ROC for classiﬁcation task using the train dataset for testing. Bolded
results show the best result for each method, while underlined results show the best result per
dataset.
Classical
C1
C2
C3
C4
C5
C6
C7
C8
k-FNN
1
1
0.950 1
0.887 0.887 0.966 0.966
k-FRNNOn14 0.429 0.487 0
0.203 0.443 0.493 0.384 0.412
k-FRNNOn15 0.434 0.587 0
0.199 0.413 0.538 0.352 0.399
k-FRNNOn16 0.358 0.513 0
0.222 0.440 0.483 0.357 0.500
k-FRNNOn17 0.429 0.487 0
0.203 0.443 0.493 0.384 0.421
FRNN14
0.949 1
0.941 0.999 0.743 0.849 0.812 0.963
FRNN15
0.955 1
0.941 1
0.753 0.884 0.814 0.979
FRNN16
0.998 1
0.941 1
0.854 0.935 0.952 0.991
FRNN17
0.949 1
0.941 0.999 0.783 0.849 0.812 0.963
VQNN14
0.954 0.992 0.993 0.980 0.764 0.839 0.741 0.856
VQNN15
0.944 0.991 0.993 0.979 0.741 0.870 0.703 0.902
VQNN16
0.971 0.987 0.993 0.990 0.800 0.909 0.757 0.951
VQNN17
0.954 0.992 0.993 0.980 0.764 0.839 0.741 0.856
Inﬂuence of Fuzzy Tolerance Metrics on Classiﬁcation and Regression Tasks
119

algorithm and the rest of the algorithms there is not a huge difference in the accuracy,
except for C2 and C8 datasets. In most cases, various similarity metrics achieved
highest train accuracy. Therefore, it is difﬁcult to tell what is the best similarity metric
for this algorithm. However, similarity metric 16 obtained best results compared with
the rest of the metrics for the FRNN algorithm and in some cases for the VQNN
algorithm.
Regarding the overall test AUC-ROC results, the situation is different here, where
the fuzzy-rough algorithms outperformed the fuzzy k-NN approach in all datasets.
Comparing the inﬂuence of the similarity metric for each of the fuzzy-rough approa-
ches, it is obvious that Similarity 16 metric is oppressed by the Similarity metrics 14
and 17. However, this is a general rule, but not an absolute rule, where we can see in
some cases that other similarity metric achieves relatively good results. It is interesting
to note, on both training and test experiments, some datasets are immune to the
inﬂuence of the similarity metric. In our case the dataset C3 exhibits this behaviour.
Regarding the regression task, we analysed 8 datasets, and the experimental results
for RRMSE by using the train and test dataset for testing are presented in Tables 4 and
5, respectively.
The overall comparison of the results given in Table 4 shows slightly better results
for the fuzzy-rough algorithms compared to the fuzzy algorithms. Comparing each
similarity metric, the results reveal that the metric 14 and 17 achieve better results using
the FRNN algorithm, while metric 15 using the VQNN algorithm.
Same as in Table 3, the algorithms using fuzzy-rough theory obtained better results
than the fuzzy algorithms. Comparing the similarity metric, FRNN algorithm achieve
better results using metric 14 and 17, while no pattern was noticed using VQNN
algorithm. In both cases k-FNNOn algorithm obtained same results in both training and
test experiments, and the similarity metric does not have inﬂuence.
Table 3. Results for AUC-ROC for classiﬁcation task using the test dataset for testing. Bolded
results show the best result for each method, while underlined results show the best result per
dataset.
Classical
C1
C2
C3
C4
C5
C6
C7
C8
k-FNN
0.777 0.794 0.843 0.858 0.497 0.492 0.547 0.561
k-FRNNOn14 0.417 0.484 0
0.203 0.443 0.530 0.462 0.473
k-FRNNOn15 0.430 0.584 0
0.200 0.441 0.559 0.437 0.457
k-FRNNOn16 0.360 0.505 0
0.222 0.431 0.512 0.448 0.530
k-FRNNOn17 0.417 0.484 0
0.203 0.443 0.520 0.462 0.473
FRNN14
0.886 0.952 0.888 0.904 0.551 0.535 0.569 0.642
FRNN15
0.873 0.946 0.888 0.894 0.577 0.521 0.567 0.606
FRNN16
0.892 0.946 0.888 0.846 0.567 0.510 0.559 0.566
FRNN17
0.886 0.952 0.888 0.904 0.551 0.535 0.569 0.642
VQNN14
0.889 0.938 0.981 0.950 0.555 0.555 0.516 0.615
VQNN15
0.887 0.938 0.981 0.948 0.555 0.523 0.514 0.565
VQNN16
0.906 0.925 0.981 0.930 0.542 0.513 0.525 0.618
VQNN17
0.886 0.938 0.981 0.950 0.555 0.555 0.516 0.615
120
A. Naumoski et al.
www.ebook3000.com

5
Conclusion
In this paper, we investigated the inﬂuence of the fuzzy similarity metric (fuzzy tol-
erance relationship) on four approaches; two of them are based on fuzzy techniques
(k-FNN and k-FNNOn), and the other two techniques are based on fuzzy-rough
approach (FRNN and VQNN). Both classiﬁcation and regression experimental results
show that in some cases it is hard to determinate the right fuzzy similarity metric, and
Table 4. Results for RRMSE for regression task using the train dataset for testing. Bolded results
show the best result for each method, while underlined results show the best result per dataset.
Classical
C1
C2
C3
C4
C5
C6
C7
C8
k-FNN
0.721 0.692 0.845 0.856 0.981 0.979 0.886 0.891
k-FRNNOn14 0.390 0.542 0.846 0.595 0.913 0.340 0.422 0.992
k-FRNNOn15 0.390 0.542 0.846 0.595 0.913 0.340 0.422 0.992
k-FRNNOn16 0.390 0.542 0.846 0.595 0.913 0.340 0.422 0.992
k-FRNNOn17 0.390 0.542 0.846 0.595 0.913 0.340 0.422 0.992
FRNN14
0.662 0.622 0.834 0.855 0.280 0.346 0.898 0.873
FRNN15
0.610 0.575 0.776 0.796 0.945 0.946 0.852 0.855
FRNN16
0.491 0.461 0.643 0.669 0.973 0.946 0.921 0.869
FRNN17
0.662 0.622 0.834 0.855 0.279 0.346 0.898 0.873
VQNN14
0.708 0.692 0.849 0.873 0.886 0.877 0.872 0.905
VQNN15
0.719 0.695 0.846 0.882 0.915 0.898 0.801 0.908
VQNN16
0.623 0.619 0.764 0.845 0.423 0.133 0.503 0.462
VQNN17
0.708 0.692 0.849 0.873 0.886 0.877 0.872 0.905
Table 5. Results for RRMSE for regression task using the test dataset for testing. Bolded results
show the best result for each method, while underlined results show the best result per dataset.
Classical
C1
C2
C3
C4
C5
C6
C7
C8
k-FNN
0.789 0.761 0.945 0.960 0.017 0.021 0.932 0.934
k-FRNNOn14 0.384 0.535 0.843 0.592 0.902 0.140 0.396 0.958
k-FRNNOn15 0.384 0.535 0.843 0.592 0.902 0.140 0.396 0.958
k-FRNNOn16 0.384 0.535 0.843 0.592 0.902 0.140 0.396 0.958
k-FRNNOn17 0.384 0.535 0.843 0.592 0.902 0.140 0.396 0.958
FRNN14
0.770 0.736 0.937 0.955 0.118 0.160 0.989 0.954
FRNN15
0.759 0.722 0.933 0.958 0.977 0.125 0.976 0.935
FRNN16
0.759 0.726 0.934 0.953 0.489 0.533 0.278 0.121
FRNN17
0.770 0.735 0.937 0.955 0.118 0.160 0.989 0.954
VQNN14
0.785 0.761 0.930 0.945 0.976 0.963 0.927 0.941
VQNN15
0.811 0.778 0.939 0.969 0.975 0.964 0.842 0.893
VQNN16
0.811 0.791 0.940 0.987 0.642 0.369 0.569 0.478
VQNN17
0.785 0.761 0.930 0.945 0.976 0.963 0.927 0.941
Inﬂuence of Fuzzy Tolerance Metrics on Classiﬁcation and Regression Tasks
121

on some datasets the change of the metric has no effect. There is no major difference
between the results for classiﬁcation and regression task, but compared globally, the
results from the test set are better using fuzzy-rough approach than the fuzzy approach
alone.
Finally, as a future work we plan to investigate the impact of averaging the lower
and/or upper approximations, and to investigate the inﬂuence of T-norm and T-conorm
in the process of task at hand.
Acknowledgement. This work was partially ﬁnanced by the Faculty of Computer Science and
Engineering at the Ss. Cyril and Methodius University in Skopje.
References
1. Keller, J.M., Gray, M.R., Givens, J.A.: A fuzzy K-nearest neighbour algorithm. IEEE Trans.
Syst. Man Cybernet. 15(4), 580–585 (1985)
2. Wang, X., Yang, J., Teng, X., Peng, N.: Fuzzy-rough set based nearest neighbour clustering
classiﬁcation algorithm. In: Wang, L., Jin, Y. (eds.) FSKD 2005. LNCS (LNAI), vol. 3613,
pp. 370–373. Springer, Heidelberg (2005)
3. Jensen, R., Cornelis, C.: A new approach to fuzzy-rough nearest neighbour classiﬁcation. In:
Rough Sets and Current Trends in Computing, 310–319. Springer, Heidelberg (2008)
4. Sarkar, M.: Fuzzy-rough nearest neighbour’s algorithm. Fuzzy Sets Syst. 158, 2123–2152
(2007)
5. Cornelis, C., De Cock, M., Radzikowska, A.M.: Vaguely quantiﬁed rough sets. In: An, A.,
Stefanowski, J., Ramanna, S., Butz, C.J., Pedrycz, W., Wang, G. (eds.) RSFDGrC 2007.
LNCS (LNAI), vol. 4482, pp. 87–94. Springer, Heidelberg (2007)
6. Radzikowska, A.M., Kerre, E.E.: A comparative study of fuzzy rough sets. Fuzzy Sets Syst.
126(2), 137–155 (2002)
7. Wang, X., Yang, J., Teng, X., Peng N.: Fuzzy-rough set based nearest neighbour clustering
classiﬁcation algorithm. In: Lecture Notes in Computer Science, vol. 3613, pp. 370–373
(2005)
8. Blake, C.L., Merz, C.J.: UCI Repository of machine learning databases. University of
California, Irvine (1998). http://archive.ics.uci.edu/ml/
9. Naumoski, A., Mircev, M.: Novel fuzzy measure of similarity for fuzzy-rough feature
selection. In: Loskoska, S. Koceski, S. (eds.) ICT Innovations 2015, pp. 11–21 (2015). ISSN
1857-7288
10. Naumoski, A.: Multi-target modelling of diatoms diversity indices in Lake Prespa. Appl.
Ecol. Environ. Res. 10(4), 521–529 (2012)
122
A. Naumoski et al.
www.ebook3000.com

On the Kalman Filter Approach for Localization
of Mobile Robots
Kristijan Petrovski(B), Stole Jovanovski, Miroslav Mirchev,
and Lasko Basnarkov
Faculty of Computer Science and Engineering, Ss. Cyril and Methodius University,
Rudjer Boshkovikj 16, P.O. 393,
1000 Skopje, Republic of Macedonia
{petrovski.kristijan.1,jovanovski.stole}@students.finki.ukim.mk,
{miroslav.mirchev,lasko.basnarkov}@finki.ukim.mk
http://finki.ukim.mk/
Abstract. In this work we analyze robot motion given from the UTIAS
Multi-Robot Dataset. The dataset contains recordings of robots wan-
dering in a conﬁned environment with randomly spaced static land-
marks. After some preprocessing of the data, an algorithm based on
the Extended Kalman Filter is developed to determine the positions of
robots at every instant of time using the positions of the landmarks. The
algorithm takes into account the asynchronous time steps and the sparse
measurement data to develop its estimates. These estimates are then
compared with the groundtruth data provided in the same dataset. Fur-
thermore several methods of noise estimation are tested, which improve
the error of the estimate for some robots.
Keywords: Robot localization · Extended Kalman Filter · Noise esti-
mation · Real-world data
1
Introduction
In many research areas there are similar types of problems requiring some kind
of localization in space, such as in robotics [1,2], wireless sensor networks [3,4],
vehicle [5] and wildlife tracking [6] etc. Plenty of diﬀerent approaches have been
developed for solving these problems among which the Extended Kalman Filter
(EKF) described in [7] has been one of the most employed, particularly in robot
localization [1]. A good introduction to the Extended Kalman Filter is given in
[8]. The EKF has been obtained by extending the applicability of the classical
Kalman ﬁlter [9] to problems with a nonlinear model or measurement function.
All varieties of the Kalman ﬁlter belong to the group of Bayesian approaches
for localization, such as particle ﬁlters and multi-hypothesis tracking, surveyed
in [10]. Although localization problems have been widely addressed there are
still many aspects, particularly those which are application speciﬁc, that can be
further analyzed.
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 12

124
K. Petrovski et al.
An implementation of EKF applied on a real dataset of several robots measur-
ing distances to several reference points is presented in [11]. There the authors
develop a general method that utilizes a decentralized robot network, where
robots communicate state estimates of objects in the environment for purposes
of localization. They obtain the same results as with a centralized network with
the added beneﬁt that the robots can use the Markov property to reduce mem-
ory, without taking into account the knowledge of other robots.
In this paper we provide a case study of the problem of localization using the
Extended Kalman Filter applied on the same dataset as above, focusing on single
robot localization. Due to the nature of data provided, several aspects of the
problem are studied such as tackling asynchronous measurements at arbitrary
timestamps and running the EKF in steps of varying duration. Methods that
estimate the measurement and process noise are characterized, while the inherent
bias of the measurement data is examined in detail and then corrected, as in [11].
These ﬁndings could be helpful in a more appropriate application of the EKF
for localization problems and its further reﬁnements.
The organization of the paper is the following. First, in Sect. 2 a presenta-
tion is given of the dataset used in the experimentation. Section 3 contains a
description of the application of the localization method (EKF), while Sect. 4
shows techniques of determining the characteristics of noise from the data in
order to incorporate them in the EKF. Section 5 presents an explanation of how
the data is preprocessed and oﬀers some numerical results for localization based
on several parameters. Finally, in Sect. 6 we give some conclusions and discuss
possible directions for future work.
2
Data Description
This paper uses the UTIAS Multi-Robot Cooperative Localization and Mapping
Dataset [12]. The dataset is created by 9 runs of separate experiments. In each
experiment 5 robots move randomly in a conﬁned environment for a certain
amount of time. They can perceive each other and 10 stationary landmarks
for the purpose of localization. While moving, each robot collects groundtruth,
control and measurement data:
– the groundtruth data records the robot’s position and orientation (x, y, θ),
and the landmark’s positions (x, y) in the laboratory reference system. This
data is obtained with a 10-camera Vicon motion system that collects data
every 0.01 s on average with an accuracy of 10−3 m.
– the control data is composed of records of the robot’s forward and angular
speed (v, ω). For each robot they are issued roughly every 0.015 s.
– the measurement data consists of estimates of distances from the landmarks
and other robots and the angle at which they are perceived (z = (d, θ)), as
seen from the reference system located at the robot where the measurements
are taken. These readings are sparser occurring every 0.2 s on average.
www.ebook3000.com

On the Kalman Filter Approach for Localization of Mobile Robots
125
Each of the 9 experiments has a diﬀerent running time and diﬀerent land-
mark positions. In some of them a certain number of obstacles are placed in the
environment.
3
EKF Algorithm
The Kalman ﬁlter is a sequential Bayesian inference method generally applied
in systems evolving in time, where the state estimate at the next moment is
obtained in two steps. In the ﬁrst step, referred as prediction, a state estimation
is based on some model of the dynamics of the system. The second step is
correction, because the estimate is improved using some measurements. The
literature on the Kalman ﬁlter is abundant and the authors refer the novice
reader to [8].
This section simply applies the Kalman ﬁltering procedure given in the same
work, by skipping the derivation and only focusing on the points that are typical
for this work. This analysis uses the Extended Discrete Kalman Filter since the
measurements are non-linear functions of the state variables. To create a model
that produces trajectories comparable with the groundtruth, linear approxima-
tion can be applied, and then the following model of motion can be used
x−
k+1 = xk + vk cos θnΔtk,
y−
k+1 = yk + vk sin θnΔtk,
θ−
k+1 = θk + ωkΔtk,
(1)
where the minus in the superscript means that the value is predicted, subscripts
denote the number of iteration, while Δtk is the time interval between two
consecutive measurements, which as was said before is not constant. To shorten
the notation one can put the state variables in a column vector x = (x, y, θ)T
and thus obtain a simple version of the evolution model xk+1 = f(xk). The error
of the estimate is the diﬀerence between the state vector and the corresponding
vector obtained from the groundtruth data xg = (xg, yg, θg)T
e = x −xg.
(2)
Then, the prediction error covariance matrix of the Kalman ﬁlter-P, which is
the expectation of the product eeT evolves according to
P−
k+1 = APkAT + Qk,
(3)
where Qk is the model error covariance matrix and A is the system dynamics
matrix
A =
⎡
⎣
1 0 −vk sin θkΔtk
0 1 vn cos θkΔtk
0 0
1
⎤
⎦.
(4)

126
K. Petrovski et al.
When the measurements arrive, the predicted state can be improved, which
according to the second step in Kalman ﬁltering is a linear combination of the
predicted state and the diﬀerence between the measurements and their estimates
xk+1 = x−
k+1 + Kk

zk+1 −Hkx−
k+1

.
(5)
In the last equation the measurements at moment k +1 are packed in the vector
zk+1, their estimates are the product Hkx−
k+1, while Kk is the Kalman gain
matrix that optimizes the correction. The matrix elements of Hk relate the
measurement zi with the state variables xj with Hi,j =
∂zi
∂xj . As given above,
the measurements for a certain robot are distances from it to some landmark or
other robot and the angle at which the other robot or landmark is estimated to
be seen. Focusing on a robot denoted with index r, the distance to the landmark
l is
dr,l =

(xr −xl)2 + (yr −yl)2,
(6)
while in robot’s coordinate system the landmark is estimated to be seen at angle
θr,l = arctan
	 yl −yr
xl −xr

−θr,
(7)
where the last expression can be obtained with simple analytic geometry. The
landmarks are assumed to have known positions (xl, yl). Then, the matrix ele-
ments Hr,l for all robots r and for all measurements to the landmarks l can
be obtained with straightforward calculus. The only non-zero elements are the
following
∂dr,l
∂xr
= xr −xl
dr,l
,
∂dr,l
∂yr
= yr −yl
dr,l
,
∂θl
∂xr
=
yl −yr
(xl −xr)2 ·
1
1 +

yl−yr
xl−xr
2 ,
∂θl
∂yr
= −
1
xl −xr
·
1
1 +

yl−yr
xl−xr
2 ,
∂θl
∂θr
= −1.
(8)
According to the Kalman ﬁlter theory the optimal Kalman gain at iteration k is
Kk = P−
k+1HT
k

HkP−
k+1HT
k + Rk+1
−1 ,
(9)
where Rk+1 is the measurement error covariance matrix.
The Extended Kalman Filter expects discrete concurrent control and mea-
surement data, where each control reading is used to obtain a priori state esti-
mate (Eq. (1)) and the concurrent measurement reading is used to obtain a pos-
teriori state estimate (Eq. (5)). For the purpose of this dataset a constant time
www.ebook3000.com

On the Kalman Filter Approach for Localization of Mobile Robots
127
step could not be used as the data readings occur asynchronously. Furthermore,
as the robots do not receive a measurement simultaneously with each control
datum, the robot’s position can not be updated until it receives a measurement
reading. This means that the robot’s a priori state estimate is updated asyn-
chronously as new control data come in, and when it obtains a measurement
datum, that is used that to get a posteriori state estimate. As the control data
arrives much more frequently, one can assume that for a measurement datum
that updates the a priori state, there was a previous control datum shortly before
it i.e. the control and measurement readings occurred at roughly the same time.
4
Noise Estimation Procedures
As is given, the Kalman Filter has unspeciﬁed values for the process and measure-
ment noise in Eqs. (3) and (9), respectively. These are unique to the environment
and the robot’s hardware, but as they are not available for the dataset a method
needs to be developed that estimates them.
4.1
Estimating the Measurement Noise
The measurement noise quantiﬁes the uncertainty of the robot’s abilities to dis-
cern targets using various sensors. For this problem it can be calculated as the
covariance matrix of the diﬀerence ez = z−ˆz between the measurement estimate
of a relative target’s state ˆz obtained from the dataset, and the actual relative
target’s state z, shown in [8].
Using relative range and bearing (d, θ) from the measurement data and the
robot’s actual groundtruth state (x, y, θ), estimated absolute target states can
be calculated. The diﬀerence between these estimates and the actual absolute
target states can be used for measurement noise computation (i.e. the diﬀerence
between the positions the robot perceives the targets and the positions at which
they are actually located). The covariance matrix of these diﬀerences can be
used in Eq. (9).
However, one can expect that the distance noise level depends on the angle
at which the robot located the target (for example if a robot located the target
at an obtuse angle it might cause a bigger distance error, than if the target was
centred in its ﬁeld of view). This hypothesis is conﬁrmed in Fig. 1, that shows the
distance measurement error depending on the measurement angle. The scattered
points correspond to all measurements for all robots for the ﬁrst 8 experiments.1
As expected the results show that the distribution of the distance measurement
error is correlated with the measurement angle, with smaller distance errors
occurring for measurements near the centre axis of the robot, and bigger distance
errors occurring at the edges of the robot’s ﬁeld of view, resembling a parabolic
curve.
For this reason a method is proposed, which takes care to change the dis-
tance measurement noise estimate depending on the measurement angle. First
1 The 9-th experiment is excluded, as it has diﬀerent environment conditions.

128
K. Petrovski et al.
Fig. 1. Distance error versus the measurement angle. Every robot roughly has a ﬁeld
of view spanning in the range of −0.6 to 0.6 radians. For the purposes of this plot one
percent of extreme values were removed.
the measurement data is segmented into 150 equal width bins, based on the
measurement angle, then for each bin the average and variation of the errors is
calculated. To further smooth the data a second degree polynomial line2 is ﬁtted
over these values, and then ﬁnally these smoothed values are used for the estima-
tion. The obtained results are shown in Fig. 2. As a result, for each measurement
that belongs to a particular bin the EKF can use separate values. The variation
per each bin is used as the covariance noise estimate in Eq. (9). Furthermore,
the Kalman ﬁlter expects the noise to have a mean of 0, while in Fig. 1 the mea-
surement distance error is shown to be positively biased. For this reason Eq. (5)
is amended, with the addition of the bin average of the measurement distance
error, which causes the measurement distances to be centred around zero. The
results obtained from applying this method are shown in Sect. 5.
4.2
Estimating the Process Noise
The process noise represents the diﬀerence between the system evolution accord-
ing to the model and its real counterpart. In Kalman ﬁlter theory it is quantiﬁed
with the covariance matrix of the error e = xk −ˆxk between the actual state of
the robot xk and its version obtained by evolving the previous actual state for
one step by the model ˆxk = f(xk−1).
To estimate the process noise one can run the whole experiment and calculate
the average error between the robot’s estimated state (1) obtained using the
2 We use a regression ridge model for smoothing.
www.ebook3000.com

On the Kalman Filter Approach for Localization of Mobile Robots
129
Fig. 2. This ﬁgure shows the average and variance of the measurement distance error
from all the bins based on measurement angle, additionally a polynomial line of degree
2 is drawn to smooth the data.
above model, and the true groundtruth value (x, y, θ) provided by the dataset,
which should give a good estimate for the actual process noise.
This can be achieved by iterating through the control data. Every control
reading is applied to the groundtruth robot position at the moment3 of the
previous control. Subsequently, the diﬀerence is calculated between that state
and the groundtruth robot state at the time of the new control. These diﬀerences
serve as a mean to estimate the error between the system evolution derived using
control readings, and the actual robot motion calculated using groundtruth data.
The process noise estimate is then computed as the covariance matrix of these
diﬀerences (used in Eq. (3)). Moreover, there are several scopes of data that
one can use to compute the covariance. The estimate can be obtained using
diﬀerences from data readings of a single robot or more broadly data readings
from the whole experiment or even diﬀerences coming from the entire dataset
(this is further discussed in Sect. 5).
5
Results
The dataset contains many robots that are purported to move randomly during
their experiment. However after some analysis one can ﬁnd that some robots got
‘stuck’ and stopped moving for a period of time.
There are several robots with similar movement in the dataset. For these
robots the state estimate error remains constant for a long period, which does
not produce realistic results when evaluating error statistics.
In order to gain meaningful results, we consider only the robots that kept
moving randomly for the whole duration of their experiment. In addition the
9-th experiment that contained obstacles in the environment is discarded from
the analysis, as it had diﬀerent environmental conditions from the other 8 exper-
iments.
3 Linear interpolation on the groundtruth data is used, so there are estimates of the
robot state at any moment.

130
K. Petrovski et al.
The analysis of this paper is focused on a single robot trying to localize
itself inside its environment, using measurements coming only from landmarks.
A general algorithm is developed, which allows experimentation with diﬀerent
robots from the dataset. We then study several scenarios:
– For the process noise estimate (p.n.) one can use diﬀerent scopes to estimate
the noise:
• using data from each robot run, to estimate the process noise in its run
• using data averaged over the whole experiment, i.e. all 5 robots that
operated concurrently in the environment
• using data averaged over the whole dataset, i.e. all 8 experiments
The variance of error of the needed scope of data is used in Eq. (3).
– The measurement noise estimate (m.n.) can also be calculated using diﬀerent
scopes for estimation. The variance of the needed scope of data is used in
Eq. (9). Otherwise, the algorithm can also use the equal width bin method as
explained in Subsect. 4.1.
– In this dataset robots perceive their environment by periodically taking pic-
tures and then processing them to obtain relative range and bearing to recog-
nized targets (each object in the environment has a bar-code that uniquely
identiﬁes it [12]). This means that at some distinct time a single robot poten-
tially has measurements to several landmarks. For the case where a robot has
concurrent landmarks measurements, the algorithm can choose to include all
of them, or just use a single landmark measurement, in Eq. (5).
For each of these scenarios the algorithm provides an error calculated from
the average diﬀerence between the estimated robot’s state and the actual
groundtruth robot state. Since the groundtruth and control data from the
dataset are not synchronous, the state estimations occur at diﬀerent moments
than the groundtruth data. To obtain a valid error estimate, the algorithm uses
linear interpolation to compute groundtruth data for the moments when the
estimations are calculated. We then focus on the results coming from the whole
dataset and results when including only the top 5 robots of the dataset (the
robots which are best localized by the algorithm). For each error estimate the
algorithm calculates the absolute average error, and the standard deviation of
the error. The results are summarized in Tables 1 and 2.
The tables demonstrate a fairly big diﬀerence between the top 5 robots and
the rest. Because of this a further direction of research might be into visualizing
the robot motion and determining reasons why these robots are localized better.
Concerning the measurement noise, the equal width bin method performs
good on the dataset, mainly because it takes into account both the changing
variance based on the measurement angle, and the unsymmetrical measurement
distance error. The method is worse when using smaller scopes of data for esti-
mation. However it is unrealistic that one can obtain speciﬁc estimates when
operating in a real environment. Moreover, the method actually performs best
when focusing on the top robots of the dataset (Table 2). Further analysis could
www.ebook3000.com

On the Kalman Filter Approach for Localization of Mobile Robots
131
Table 1. Error statistics (absolute average and standard deviation) for diﬀerent para-
meters, described at the beginning of this section. This data comes from the ﬁrst 8
experiments in the dataset.
Diﬀerent scenarios
Avg.; sd. of position
er.[m]
Avg.; sd. of angle
er.[rad]
p.n. from whole dataset
0.1506; 0.3798
0.1177; 0.2599
p.n. from one experiment
0.1513; 0.3805
0.1179; 0.2602
p.n. from single robot
0.1512; 0.3799
0.1180; 0.2603
m.n. with equal width bins 0.1586; 0.4384
0.1455; 0.3471
m.n. from single robot
0.1506; 0.3798
0.1177; 0.2599
m.n. from one experiment
0.1521; 0.3752
0.1180; 0.2570
m.n. from whole dataset
0.1652; 0.3971
0.1416; 0.3246
Multiple landmarks
0.1506; 0.3798
0.1177; 0.2599
Single landmark
0.1598; 0.3947
0.1225; 0.2696
Table 2. Error statistics (absolute average and standard deviation) for diﬀerent para-
meters, described at the beginning of this section. This data comes from the 5 most
precisely localized robots in the dataset.
Diﬀerent scenarios
Avg.; sd. of position
er.[m]
Avg.; sd. of angle
er.[rad]
p.n. from whole dataset
0.0471; 0.0632
0.0540; 0.0833
p.n. from one experiment
0.0484; 0.0646
0.0561; 0.0855
p.n. from single robot
0.0490; 0.0658
0.0572; 0.0866
m.n. with equal width bins 0.0471; 0.0632
0.0540; 0.0833
m.n. from single robot
0.0558; 0.0752
0.0417; 0.0752
m.n. from one experiment
0.0607; 0.0801
0.0442; 0.0767
m.n. from whole dataset
0.0706; 0.0881
0.0603; 0.0876
Multiple landmarks
0.0471; 0.0632
0.0540; 0.0833
Single landmark
0.0477; 0.0635
0.0560; 0.0846
be directed into ﬁnding methods that use diﬀerent segmentation techniques and
better smooth the data in the diﬀerent segments.
For the landmark option, as expected, the algorithm gets better results when
using multiple landmarks. This is especially true when focusing on the whole
dataset, however, for the top 5 robots this option does not make a big change in
the error. More investigation is needed into the choice of landmark measurements
used, perhaps only including the closest and most centred landmarks will give
better results.

132
K. Petrovski et al.
6
Conclusions and Future Work
This paper presents a study on localization of robots based on real data. As
it can be expected with such data there are certain problems pertaining to
synchronizing the input data and uncertain robot’s movements. However the
results demonstrate several ﬁndings.
The error is very much dependent on the quality of the robot’s motion
through the environment with the best 5 localizations of robots achieving around
3 times better results, as compared to all the other robots in the dataset (Tables 1
and 2). When the robots view multiple targets and keep perceiving landmarks
for a longer time they perform better. To achieve even better results, aspects
such as the unsymmetrical measurement distance and the changing measurement
variance need to be included in the computation.
This paper provides a solid foundation for further analysis in cooperative
multi-robot localization, which can be performed using the same dataset as it
also contains robot-to-robot measurements. Cooperative multi-robot approaches
have been described in [13,14] using distributed EKF. In such a multi-robot
scenario robot-to-robot and robot-to-landmark measurements need to be com-
bined, which have diﬀerent characteristics as the former accumulates error on its
multi-hop path, while the latter is prone only to single hop measurement noise.
Therefore, the localization could be performed better using a modiﬁed version of
the EKF that allows weights to be given to the diﬀerent types of measurements.
References
1. Leonard, J.J., Durrant-Whyte, H.F.: Mobile robot localization by tracking geomet-
ric beacons. IEEE Trans. Robot. Autom. 7(3), 376–382 (1991)
2. Thrun, S., Fox, D., Burgard, W., Dellaert, F.: Robust Monte Carlo localization for
mobile robots. Artif. Intell. 128(1–2), 99–141 (2001)
3. Niculescu, D.: Positioning in ad hoc sensor networks. IEEE Netw. 18(4), 24–29
(2004)
4. Mao, G., Fidan, B., Anderson, B.D.O.: Wireless sensor network localization tech-
niques. Comput. Netw. 51(10), 2529–2553 (2007)
5. Li, H., Nashashibi, F.: Cooperative multi-vehicle localization using split covariance
intersection ﬁlter. IEEE Intell. Transp. Syst. Mag. 5(2), 33–44 (2013)
6. Juang, P., Oki, H., Wang, Y., Martonosi, M., Peh, L.S., Rubenstein, D.: Energy-
eﬃcient computing for wildlife tracking: design tradeoﬀs and early experiences with
Zebranet. ACM Sigplan Not. 37(10), 96–107 (2002)
7. Smith, G.L., Schmidt, S.F., McGee, L.A.: Application of statistical ﬁlter theory
to the optimal estimation of position and velocity on board a circumlunar vehicle.
National Aeronautics and Space Administration (1962)
8. Welch, G., Bishop, G.: An introduction to the Kalman ﬁlter. University of North
Carolina, Department of Computer Science (2006)
9. Kalman, R.: A new approach to linear ﬁltering and prediction problems. J. Fluids
Eng. 82(1), 35–45 (1960)
10. Fox, D., Hightower, J., Liao, L., Schulz, D., Borriello, G.: Bayesian ﬁltering for
location estimation. IEEE Pervasive Comput. 3, 24–33 (2003)
www.ebook3000.com

On the Kalman Filter Approach for Localization of Mobile Robots
133
11. Leung, K.Y.K.: Cooperative localization and mapping in sparsely-communicating
robot networks. Ph.D. thesis, University of Toronto (2012)
12. Leung, K.Y., Halpern, Y., Barfoot, T.D., Liu, H.H.: The UTIAS multi-robot coop-
erative localization and mapping dataset. Int. J. Robot. Res. 30(8), 969–974 (2011)
13. Roumeliotis, S.I., Bekey, G.A.: Distributed multirobot localization. IEEE Trans.
Robot. Autom. 18(5), 781–795 (2002)
14. Madhavan, R., Fregene, K., Parker, L.E.: Distributed cooperative outdoor multi-
robot localization and mapping. Auton. Robots 17(1), 23–39 (2004)

Evaluation of Automatically Generated
Conceptual Database Model Based on Business
Process Model: Controlled Experiment
Danijela Banjac(B), Drazen Brdjanin, Goran Banjac, and Slavko Maric
Faculty of Electrical Engineering, University of Banja Luka, Patre 5,
78000 Banja Luka, Bosnia and Herzegovina
{danijela.banjac,bdrazen,goran.banjac,ms}@etfbl.net
Abstract. This paper presents the results of the controlled experiment
that we have conducted in order to evaluate an approach to automated
design of the initial conceptual database model based on the collaborative
business process model. The source business process model is represented
by BPMN, while the target conceptual model is represented by UML
class diagram. The results of the experiment imply that the approach
enables generation of the target conceptual model with a high percent-
age of completeness (>85%) and precision (>85%), which conﬁrms the
results of the initial case-study based evaluation.
Keywords: BPMN · Collaborative business process model · Conceptual
database model · Evaluation · Experiment · Model-driven · UML
1
Introduction
Data modeling is a very important part of information system design. Data
models are used to deﬁne and analyze data representing the essence of any
information system. The process of data modeling involves professional data
modelers, as well as business experts, and it is not straightforward, meaning
it often requires many iterations before designing the ﬁnal model. Therefore,
automatic generation of data models is of great interest and it has been the
subject of research for many years. Although the idea of model-driven design of
data models is more than 25 years old, the survey [1] shows that only a small
number of papers present the implemented automatic model-driven generator of
the data model and the corresponding evaluation results.
In this paper, we present the results of the controlled experiment that we
have conducted in order to evaluate an approach to automated synthesis of the
conceptual database model (CDM) based on the collaborative business process
model (BPM). The source BPM is represented by BPMN [2], while the target
CDM is represented by UML [3] class diagram. The initial case-study based eval-
uation of the proposed approach [4] implies that the approach and implemented
generator enable generation of the target model with very high precision. In order
to evaluate the approach more extensively, we have conducted four experiments
with undergraduate students at the University of Banja Luka (UoBL).
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 13
www.ebook3000.com

Evaluation of Automatically Generated Conceptual Database Model
135
The paper is structured as follows. After the introduction, the second section
presents the related work. The third section presents a short overview of the
evaluated approach to automatic synthesis of the initial CDM based on the
collaborative BPM. The fourth section describes the experiments. The results
are analyzed in the ﬁfth section. The ﬁnal section concludes the paper.
2
Related Work
The survey [1] shows that the current approaches to model-driven synthesis of
data models (MDSDM) can be classiﬁed as: function-oriented, process-oriented,
communication-oriented, and goal-oriented. In most papers, a process-oriented
model (POM) is used as the source model. The survey [1] shows that the semantic
capacity of POMs has still not been suﬃciently identiﬁed to enable automatic
synthesis of the complete target data model.
The BPMN is used in [4–18] as the starting point for MDSDM. There are
two QVT [19]-based proposals [8,11], but with modest achievements in the auto-
mated generation of analysis level class diagrams, and there are also several
proposals [6,7,10,14,16] for semi-automated generation. A MDSDM based on
BPMN is also considered in [12,13,16], but without implementation. The large
majority of all proposals are based on an incomplete source model i.e. single dia-
gram (a real model contains a ﬁnite set of diagrams). Only [18] considers a set of
interrelated BPMs, but with no explicit rules and implementation. This overview
does not contain approaches based on other POMs, although there is a paper
[20] considering a ﬁnite set of interrelated BPMs. An overview of BPMN-based
MDSDM approaches is given in Fig. 1.
The formal rules for automated CDM synthesis based on BPMN are pre-
sented in [4], and partially in [15,18]. Other papers consider only guidelines that
do not enable automated CDM synthesis. In this paper, we evaluate an approach
[4] that enables automated CDM synthesis.
There are no papers presenting experiments for evaluation of automatically
generated CDMs based on POMs. There are two papers [4,20] presenting case-
study based evaluation of POM-based approaches. There is also a paper [21]
presenting a controlled experiment which compares two techniques for deriving
conceptual models from requirements models, but none of them is a POM.
Fig. 1. Overview of BPMN-based MDSDM approaches

136
D. Banjac et al.
3
Overview of Evaluated Approach for Automatic CDM
Synthesis Based on Collaborative BPM
The semantic capacity of the collaborative BPM represented by BPMN, and the
corresponding rules for automatic CDM synthesis are presented in [2]. Due to
the paper length limitation, in this section we give only a short illustration of
these rules in Fig. 2.
Fig. 2. Mapping of BPM concepts into CDM concepts
www.ebook3000.com

Evaluation of Automatically Generated Conceptual Database Model
137
4
Experiment Design
This section describes the design of the experiments, including the experimental
context, variables, as well as the subjects, settings and assignments for each
experiment.
4.1
Experimental Context
The fourth-year (240 ECTS1) undergraduate study program Computing and
Informatics at the UoBL Faculty of Electrical Engineering (FEE) oﬀers a course
entitled Object-Oriented Design and Programming (OODP) in the 8th semester.
Through this course, students acquire the necessary knowledge and practical
skills in object-oriented analysis and design of software systems. The course is
elective and 24 students (3rd and 4th-year students) took it in 2014/15 aca-
demic year. Prior to taking this course, the students took several courses on
programming and database modeling, in which they studied data structures and
algorithms, object-oriented programming, database design, software engineering,
information systems, UML, etc.
Before taking these experiments, we had to train students in order to achieve
approximately the same level of knowledge. The introductory lessons included
a short reminder of database modeling by using UML class diagrams. Since the
students had little prior knowledge of BPMN, they had to learn BPMN concepts
and notation before the experimental tasks. They were also trained to use the
Topcased platform for modeling. The training lasted four weeks and it included
both theoretical and practical classes.
Given their prior knowledge about database modeling and object-oriented
design and knowledge gained in the OODP course, the students were appropriate
participants in the experiments. The students had four assignments during the
semester and every assignment was done individually.
4.2
Variables
We identiﬁed the following types of variables, according to [22].
Response Variables. The quantitative outcome of an experiment is referred
to as a response variable. The response variable of an experiment must reﬂect
the data that is collected from experiments so it can be used in the analysis.
Appropriate measures and metrics need to be used in order to acquire the values
of the response variables. There are no metrics and measures for the quantitative
evaluation of automatically generated CDMs based on POMs, but there are some
metrics and measures for the evaluation of automatic CDM generators based on
natural language processing (NLP). Some of NLP metrics and measures can be
used to perform the quantitative evaluation of the implemented generator. We
1 European Credit Transfer and Accumulation System.

138
D. Banjac et al.
have adopted some of them that were introduced in [23,24] and later adjusted
in [25].
The most commonly used metrics for quantitative evaluation of automatically
generated CDMs are: Ngenerated (the total number of automatically generated
concepts), Ncorrect (the number of correctly generated concepts that can be kept
in the target model), Nincorrect (the number of incorrectly generated concepts
that cannot be kept in the target model), Nexcessive (the number of excessively
generated concepts that should not be kept in the target model), and Nmissing
(the number of missing concepts that should be in the target model, but are not
generated). In this experiment we use recall and precision as response variables
used for evaluation of automatically generated CDMs.
Recall represents the percentage of the target CDM that is automatically
generated (the estimated percentage of automatically generated concepts in the
total number of concepts in the target model). It may be deﬁned as
Recall =
Ncorrect
Ncorrect + Nmissing
· 100%.
(1)
Precision represents the percentage of correctly generated concepts in an
automatically generated model (the percentage of correctly generated classes
and the percentage of correctly generated associations). It may be deﬁned as
Precision =
Ncorrect
Ncorrect + Nincorrect
· 100%.
(2)
Parameters. Qualitative or quantitative characteristics that are invariable in
the experimentation process are called parameters. Such characteristics are to
be ﬁxed so they do not inﬂuence the results of the experiment. We established
the following:
– Modeling language used for modeling BPMs was BPMN.
– Modeling language used for modeling CDMs was UML class diagram.
– Modeling tool. Topcased with BPMN Modeler plugin is used as a tool for
modeling BPMs (BPMN) and CDMs (UML class diagram).
– Business processes modeling training. All students participated in the same
BPMN training, because they had very little prior knowledge of BPMN.
– Conceptual database modeling training. Although the students had experience
in database modeling using Information Engineering (IE) notation and tools,
all students participated in the same UML database modeling training.
4.3
Subjects, Settings and Assignments
Experiment #1. The goal of Experiment #1 was to compare an automat-
ically generated CDM with a manually designed CDM for the same business
system. In this experiment, students had to create manually a DM based on a
given collaborative BPM. The source model used in this experiment was a BPM
www.ebook3000.com

Evaluation of Automatically Generated Conceptual Database Model
139
representing Order processing. Due to the space limitations, we are not able to
provide ﬁgures representing source models nor automatically generated CDMs
(models can be obtained from the authors).
Eight students participated in this experiment, but only six models were
considered for evaluation because two models were incomplete. The time for
modeling was limited to 90 min. The students were in the same laboratory during
the experiment and there was also a teacher present in order to preserve the
validity of the experiment and, if necessary, to help the students to understand
the source model.
The students’ individual work resulted in diﬀerent CDMs for the same BPM,
making the comparison of automatically generated and manually designed CDMs
very challenging. We tried to ﬁnd an equal or equivalent concept in manually
designed CDMs for each concept in the automatically generated model. If an
equal or equivalent concept was found in a manually designed CDM, it was con-
sidered as suitable and it was retained in the target CDM without any change.
If an equal or equivalent concept was not found in a manually designed CDM,
it was necessary to decide whether the given concept is excessive or it could
be retained in the target CDM. Although some concepts may be considered
as redundant, designers sometimes introduce redundant associations to achieve
better performance in data retrieval. It is easier for the designer to remove some
excessive concept from an automatically generated CDM that is correctly gener-
ated, than to add some new concept. Missing concepts are the relevant concepts
that were introduced in the manually designed CDMs but are missing from the
automatically generated CDM.
Experiment #2. Setup of Experiment #2 was similar to Experiment #1.
The purpose of Experiment #2 was also to compare an automatically generated
CDM and manually created CDMs. In the experiment, the students had to
create CDMs based on the given collaborative BPM representing E-mail voting.
The source model is taken from the BPMN speciﬁcation [26] and it illustrates
resolving issues through e-mail votes. The source model is very complex and
it provides examples for many of BPMN features. Like in Experiment #1, all
students had the same source model.
Fourteen students participated and six manually created CDMs were con-
sidered for evaluation. Other models were evaluated as incorrect. The main
diﬀerence between Experiments #1 and #2, in addition to the source model
complexity, is that Experiment #2 was done as homework (students had more
time to do it). The automatically generated and manually designed models were
compared in the same way as in Experiment #1.
Experiment #3. Experiment #3 was diﬀerent compared to the ﬁrst two.
In this experiment, the students analyzed the CDM which was automatically
generated based on the E-mail voting BPM. The goal of the experiment was to
compare the results of the initial evaluation [4,27] and the results of the students’
evaluation. The students evaluated the CDM from the database designer’s point

140
D. Banjac et al.
of view. For each concept (class or association) in the generated CDM, they had
to determine whether the concept should be retained unchanged, or it should
be retained with corrections (if it was generated incorrectly) or it should not be
contained in the model (if it is redundant).
The students evaluated the same generated model within a 60-min time
frame. They were in the same laboratory during the experiment and the teach-
ers were also present in order to preserve the validity of the experiment and to
help the students to understand the model. During the experiment, the students
also had the source BPM (E-mail voting) at their disposal. We did not accept
all evaluations, because some of them were incorrect. Thirteen students partic-
ipated in the experiment, but only six models were considered for evaluation,
since the source BPM was too complicated for some students.
Experiment #4. Experiment #4 was completely diﬀerent from previously
described experiments. The goal of this experiment was to test the generator
[4,27] on a set of diﬀerent real BPMs. Each student had to create his/her own
collaborative BPM representing a real business process and use that model as the
source model to create manually a target CDM. The students had to choose and
model diﬀerent processes. Fifteen students participated in the experiment and
seven models were considered for evaluation. The experiment was also done as
homework. We used the generator to create target CDMs based on the students’
collaborative BPMs, which we compared with the manually designed CDMs in
the same way as in Experiments #1 and #2.
5
Results Analysis and Discussion
Table 1 shows the results of the initial quantitative analysis of the automatically
generated CDM [4,27], as well as the results of comparing the automatically
generated and manually designed CDMs obtained in Experiment #1. The initial
case-study based results show that the generated CDM has very high recall and
precision (both over 90%). All generated classes could be retained in the target
CDM. There are only several generated associations that could not be retained
in the target CDM because of partially incorrect cardinalities. The main reason
for this are some control patterns that are presently not covered by the rules.
The results of comparing the automatically generated and manually designed
CDMs show that the objectiﬁed evaluation does not signiﬁcantly diﬀer from the
initial evaluation, i.e. the initial results were conﬁrmed in Experiment #1.
Table 2 shows the results of the initial quantitative analysis of the automat-
ically generated CDM [4,27], as well as the result of comparing the automati-
cally generated and manually designed CDMs obtained in Experiment #2. The
obtained values in Experiment #2 are lower than in Experiment #1 – especially
precision for associations, but it is still very high (>75%). The main reason
for lower precision in Experiment #2 is the source model complexity. Taking
into account the high complexity of the source model (four complex loops, two
sub-processes, etc.), recall and precision are still very high.
www.ebook3000.com

Evaluation of Automatically Generated Conceptual Database Model
141
The results of the students’ analysis of the automatically generated model in
Experiment #3 are shown in Table 3 (left part). The students’ results conﬁrmed
the initial case-study based results shown in Table 2. Precision is almost the same
as the initial precision, but recall is slightly lower. Recall is lower because the
students evaluated that there were some classes and associations missing from
the generated CDM. There are some classes (and corresponding associations)
that could not be generated based on just one collaborative BPM, because the
corresponding objects do not exist in the source model, but we presume that
some of the missing classes could be generated based on a set of source BPMs.
Table 1. Results obtained in Experiment #1
Concepts
Case-study based [4,27]
Experiment #1
Recall [%] Precision [%] Recall [%] Precision [%]
Classes
100
100
96.43
100.00
Associations
97
97
88.35
92.78
Table 2. Results obtained in Experiment #2
Concepts
Case-study based [4,27]
Experiment #2
Recall [%] Precision [%] Recall [%] Precision [%]
Classes
100
100
95.56
100.00
Associations
97
86
83.54
77.76
Table 3. Results obtained in Experiments #3 and #4
Concepts
Experiment #3
Experiment #4
Recall [%] Precision [%] Recall [%] Precision [%]
Classes
94.45
100.00
98.09
100.00
Associations 87.47
89.98
91.86
85.79
The results obtained in Experiment #4 are shown in Table 3 (right part).
Recall and precision for classes are almost 100%. There are a few classes miss-
ing, as previously explained. The analysis of models shows that there are more
classes manually designed than those automatically generated, mostly because
the students used generalizations/specializations.
Table 3 also shows the results of Experiment #4 for associations. We observed
that there were more automatically generated associations than those manually
designed. Some of these associations are excessive, but it is easier for designers
to remove excessive associations than to add new associations. In particular, we
especially analyzed the source models with lower results for automatically gen-
erated CDMs. We concluded that lower results were obtained when the source

142
D. Banjac et al.
collaborative BPMs had many decisions/merges, because those control patterns
are presently not covered by the transformation rules. We also found out more
about the process ﬂow inﬂuence on the end multiplicities of the generated asso-
ciations. These ﬁndings will also be used in the future to improve the rules and
generator.
Table 4 presents the summary results for all experiments. The results imply
that: (i) generated classes are completely suitable and they could be retained in
the target CDM without any change, (ii) there are some associations that could
not be retained in the target CDM (because of partially incorrect cardinality),
and (iii) there are some classes and associations missing in the generated CDM.
Table 4. Summary results for experiments
Experiments Classes
Associations
Recall [%] Precision [%] Recall [%] Precision [%]
#1
96.43
100.00
88.35
92.78
#2
95.56
100.00
83.54
77.76
#3
94.45
100.00
87.47
89.98
#4
98.09
100.00
91.86
85.79
Mean:
96.13
100.00
87.81
86.58
5.1
Threats to Validity
This section describes potential threats to the validity of the experiments.
Measuring Quality. According to Conway’s Law [28], independent work of
several database designers will result in the creation of several diﬀerent CDMs
for the same system. According to Date [29], the problem of ﬁnding the logi-
cal design that is incontestably the right one is still a rather intractable prob-
lem. Therefore, the objectivity of comparing an automatically generated CDM
with some manually designed CDM for the same system is questionable since
the degree of compliance will diﬀer from one case to another. With respect to
that, and in order to obtain some reliable evaluation of automatically generated
CDMs, qualitative evaluation was carried out from the database designer’s point
of view. Each generated concept was analyzed and it was determined whether it
was suitable and correctly generated to be retained in the design of the target
CDM. All models were reviewed by (at least) three teachers from FEE.
Conclusion Validity. Although all students started the training with the same
level of competence in BPM design, it turned out that they acquired diﬀerent
levels of modeling competence during the training. Thus, we considered only
those models that were evaluated as valid and complete.
www.ebook3000.com

Evaluation of Automatically Generated Conceptual Database Model
143
We also held a training for database modeling using UML class diagrams,
since the students had diﬀerent levels of knowledge and skills in modeling using
IE. Therefore, only valid and complete models were used in the experiments.
Since the two experiments were designed as homework, the students had to
present their homework in front of the class. It turned out that some of them
were copying assignments from each other, so their models were excluded.
We started the experiment assuming that students were not familiar with
the transformation rules. It turned out that two students had come across some
papers describing the transformation rules, and consequently their models were
not considered.
6
Conclusions and Future Work
Previously we analyzed the semantic potential of the collaborative BPMN for
automated CDM generation, and, based on that analysis, we deﬁned the formal
rules for automated CDM design [4]. An ATL [30]-based generator [4,27] was
implemented based on these formal rules. The initial case-study based evalua-
tion of the automatically generated CDMs implied that the generator is able to
generate very high percentage of the target CDM with very high precision.
Controlled experiments within FEE were conducted in order to evaluate the
approach more extensively. The results of the experiments were analyzed and
used for evaluation of the automatically generated CDMs. This paper has pre-
sented the experiments conducted, as well as the results of the evaluation of the
approach.
These experiments with students conﬁrmed the results of the initial case-
study based evaluation. The results of the experiments imply that the speciﬁed
transformation rules cover the automated generation of the majority of concepts
of the target CDM. Recall and precision for classes are very high (>95%) and
they are slightly lower for associations (>85%). Therefore, considering the initial
results [4] and the results of the experiments presented in this paper, we conclude
that the proposed approach can be used in practice.
Based on these experiments, we have further identiﬁed how the process ﬂow
aﬀects the association end multiplicities, and that knowledge will be used to
further improve the generator. Of course, further identiﬁcation of the semantic
capacity of BPMs for automated CDM design will be part of the future work, as
well as the evaluation of automatically generated CDMs based on more source
models and with participants more experienced in database design.
References
1. Brdjanin, D., Maric, S.: Model-driven techniques for data model synthesis. Elec-
tronics 17(2), 130–136 (2013)
2. OMG: Business Process Model and Notation (BPMN), v2.0. OMG (2011)
3. OMG: Uniﬁed Modeling Language (OMG UML), v2.5. OMG (2015)

144
D. Banjac et al.
4. Brdjanin, D., Banjac, G., Maric, S.: Automated synthesis of initial conceptual
database model based on collaborative business process model. In: Bogdanova,
M.A., Gjorgjevikj, D. (eds.) ICT Innovations 2014: World of Data. AISC, vol. 311,
pp. 145–156. Springer International Publishing, Cham (2015)
5. Rungworawut, W., Senivongse, T.: From business world to software world: deriving
class diagrams from business process models. In: Proceedings of the 5th WSEAS
International Conference on Aplied Informatics and Communications, WSEAS, pp.
233–238 (2005)
6. Rungworawut, W., Senivongse, T.: Using ontology search in the design of class
diagram from business process model. PWASET 12, 165–170 (2006)
7. Brambilla, M., Cabot, J., Comai, S.: Automatic generation of workﬂow-extended
domain models. In: Engels, G., et al. (eds.) MoDELS 2007. LNCS, vol. 4735, pp.
375–389. Springer, Heidelberg (2007)
8. Rodriguez, A., Fernandez-Medina, E., Piattini, M.: Towards obtaining analysis-
level class and use case diagrams from business process models. In: Song, I.Y., et al.
(eds.) ER Workshops 2008. LNCS, vol. 5232, pp. 103–112. Springer, Heidelberg
(2008)
9. de la Vara, J.L., Fortuna, M.H., Sanchez, J., Werner, C.M.L., Borges, M.R.S.: A
requirements enegineering approach for data modelling of process-aware informa-
tion systems. In: Abramowicz, W. (ed.) BIS 2009. LNBIP, vol. 21, pp. 133–144.
Springer, Heidelberg (2009)
10. Brambilla, M., Cabot, J., Comai, S.: Extending conceptual schemas with business
process information. Adv. Softw. Eng. 2010 (2010). Article ID 525121
11. Rodriguez, A., Garcia-Rodriguez de Guzman, I., Fernandez-Medina, E., Piattini,
M.: Semi-formal transformation of secure business processes into analysis class and
use case models: an MDA approach. Inf. Softw. Technol. 52(9), 945–971 (2010)
12. Zhang, J., Feng, P., Wu, Z., Yu, D., Chen, K.: Activity based CIM modeling and
transformation for business process systems. Int. J. Softw. Eng. Knowl. Eng. 20(3),
289–309 (2010)
13. Nikiforova, O., Pavlova, N.: Application of BPMN instead of GRAPES for two-
hemisphere model driven approach. In: Grundspenkis, J., et al. (eds.) ADBIS 2009
Workshops. LNCS, vol. 5968, pp. 185–192. Springer, Heidelberg (2010)
14. de la Vara, J.L.: Business process-based requirements speciﬁcation and object-
oriented conceptual modelling of information systems. Ph.D. thesis, Valencia Poly-
technic University (2011)
15. Cruz, E.F., Machado, R.J., Santos, M.Y.: From business process modeling to data
model: a systematic approach. In: Proceedings of QUATIC 2012, pp. 205–210.
IEEE (2012)
16. Drozdov´a, M., Mokryˇs, M., Kardoˇs, M., Kurillov´a, Z., Pap´an, J.: Change of para-
digm for development of software support for elearning. In: Proceedings of ICETA
2012, pp. 81–84. IEEE (2012)
17. Rhazali, Y., Hadi, Y., Mouloudi, A.: Transformation method CIM to PIM: from
business processes models deﬁned in BPMN to use case and class models deﬁned
in UML. Int. J. Comput. Inf. Syst. Control Eng. 8(8), 1334–1338 (2014)
18. Cruz, E.F., Machado, R.J., Santos, M.Y.: Deriving a data model from a set of
interrelated business process models. In: Proceedings of ICEIS 2015, pp. 49–59
(2015)
19. OMG: MOF 2.0 Query/View/Transformation Speciﬁcation, v1.0. OMG (2008)
20. Brdjanin, D., Maric, S.: Towards the automated business model-driven conceptual
database design. In: Morzy, T., Harder, T., Wrembel, R. (eds.) Advances in Data-
www.ebook3000.com

Evaluation of Automatically Generated Conceptual Database Model
145
bases and Information Systems. AISC, vol. 186, pp. 31–43. Springer, Heidelberg
(2012)
21. Espana, S., Ruiz, M., Gonzalez, A.: Systematic derivation of conceptual models
from requirements models: a controlled experiment. In: Proceedings of RCIS 2012,
pp. 1–12. IEEE (2012)
22. Juristo, N., Moreno, A.: Basics of Software Engineering Experimentation. Springer,
Heidelberg (2001)
23. Harmain, H., Gaizauskas, R.: CM-Builder: a natural language-based CASE tool
for object-oriented analysis. Autom. Softw. Eng. 10(2), 157–181 (2003)
24. Omar, N., Hanna, P., McKevitt, P.: Heuristics-based entity-relationship modelling
through natural language processing. In: Proceedings of AICS 2004, pp. 302–313
(2004)
25. Brdjanin, D., Maric, S.: An approach to automated conceptual database design
based on the UML activity diagram. Comput. Sci. Inf. Syst. 9(1), 249–283 (2012)
26. OMG: BPMN 2.0 by Example, v.1.0. OMG (2010)
27. Banjac, G.: Automated synthesis of conceptual database model based on collabo-
rative business process model. Master thesis, University of Banja Luka (2014)
28. Conway, M.: How do committees invent? Datamation 14(4), 28–31 (1968)
29. Date, C.: An Introduction to Database Systems, 8th edn. Addison-Wesley, Boston
(2003)
30. Jouault, F., Allilaire, F., Bezivin, J., Kurtev, I.: ATL: a model transformation tool.
Sci. Comput. Program. 72(1–2), 31–39 (2008)

Analysis of Protein Interaction Network
for Colorectal Cancer
Zlate Ristovski(B), Kire Trivodaliev, and Slobodan Kalajdziski
Faculty of Computer Science and Engineering,
Ss. Cyril and Methodius University, Skopje, Macedonia
ristovskizlate@gmail.com,
{kire.trivodaliev,slobodan.kalajdziski}@finki.ukim.mk
http://www.finki.ukim.mk
Abstract. In this paper we create and analyze a protein-protein inter-
action network (PPIN) of colorectal cancer (CRC). First we identify
proteins that are related to the CRC (set of seed proteins). Using this
set we generate the CRC PPIN with the help of Cytoscape. We analyze
this PPIN in a twofold manner. We ﬁrst extract important topological
features for proteins in the network which we use to determine CRC
essential proteins. Next we perform a modular analysis by discovering
CRC signiﬁcant functional terms through the process of GO enrichment
within densely connected subgroups (clusters) of the PPIN. The mod-
ular analysis results in a mapping from the CRC signiﬁcant terms to
CRC signiﬁcant proteins. Finally, we combine the topological and mod-
ular evidence for the proteins in the CRC PPIN, exclude the initial seed
proteins and obtain a list of proteins that could be taken as possible
bio-markers for CRC.
Keywords: Colorectal cancer · Protein-protein interaction network ·
Network analysis · Gene Ontology · Clustering · Cytoscape
1
Introduction
Colorectal cancer is a very complex disease where many proteins, genes and
molecular functions are involved. Usually proteins are performing common bio-
logical functions, but when some of the proteins change their biological function
then a disease, such as cancer, will evolve in the organism [1].
In the past few decades, the research in the ﬁeld of system and molecular
biology induced a good knowledge about functions and molecular characteris-
tics of the proteins. This knowledge is stored in, and later used from protein
sequence databases, the most prominent one being UniProt [2]. Proteins hardly
ever act in isolation from other proteins, i.e. proteins interact, forming protein
complexes. Protein-protein interactions are commonly represented in the form
of a network, where individual proteins and their interactions correspond to
nodes and edges, accordingly [3]. The advent of high-throughput technologies
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 14
www.ebook3000.com

Analysis of Protein Interaction Network for Colorectal Cancer
147
has increased the volume of protein interaction data, thus enabling high-curated
biological networks, and with the existing knowledge about networks and the
tools and techniques for their analysis we are one step closer to fully under-
stand the complex mechanisms behind protein interactions, and making valid
hypothesis for biomarkers and/or pathways in cancer.
Computational approaches that use the protein-protein interaction network
(PPIN) to identify important proteins involved in cancer and metastasis of can-
cer have been the focus of previous research. Wachi et al. have discovered high
centrality in diﬀerent genes of lung cancer [4]. Rhodes et al. conducted an inte-
grated analysis of cancer by using a sequence of DNA segments using the high–
throughput approach [5]. Another study has used concepts from system biology
to improve the prognosis of lung cancer [6]. The authors of the later research
analyze the molecular interactions that are unregulated for a given type of cancer
using a large set of sequences of DNA segments. Other cancer research include
topological analysis of PPINs [7], and PPIN based candidate gene prediction [8].
All these papers show the importance of computational methods for reach-
ing conclusions and understanding cancer related PPINs. However, most of the
papers have used very limited level of network analysis of the PPIN. To get a
better picture of the protein interactions in colorectal cancer (CRC), a system-
atic and thorough analysis is needed of all the cancer network related features
through the use of the available tools and algorithms for network analysis.
The aim of this paper is to create and analyze the PPIN of colorectal cancer
(CRC). By applying diﬀerent network analysis and GO enrichment analysis, we
will identify the proteins that are “most important” for CRC. If we exclude the
proteins that are proved to be CRC-related, we can deﬁne possible bio-marker
CRC proteins. These proteins should be further analyzed to be conﬁrmed as valid
bio-markers. The rest of the paper is organized as follows: in the second section
we provide the overview of the used methodology (generation of colorectal cancer
PPIN and its network analysis); in the third section we provide the obtained
results with a discussion, and ﬁnally the forth section concludes the paper.
2
Research Methods
The steps for creating and analyzing the CRC PPIN are shown on Fig. 1. As can
be seen, the ﬁrst step is the acquisition of the most important CRC proteins. This
set of proteins is passed on the second step, where we generate the CRC PPIN.
The creation of the CRC PPIN is done by merging data from diﬀerent protein-
protein interaction databases. The generated CRC PPIN is represented in the
form of a graph, where a node correspond to a protein, while an edge depicts
the corresponding interaction between the connected pair of nodes (proteins).
The third step is the most important, and the most complex one. This includes
the CRC PPIN analysis and the deduction of meaningful results i.e. proteins
that should be used as possible bio-markers for CRC. The CRC PPIN analysis
is done in a two-fold manner. First, by using topological analysis we extract
some important features for all nodes of the graph and identify target nodes by

148
Z. Ristovski et al.
Fig. 1. Step-by-step methodology for creating and analyzing the CRC PPIN
calculating a summary score based on the topological features. Next we do a
modular analysis that produces the graph organization (the partitioning of the
CRC PPIN) in meaningful clusters. Gene Ontology (GO) enrichment is used to
assign biological meaning to the clusters resulting from the modular analysis.
The signiﬁcant over-representation of some biological characteristics elicits the
target nodes in a cluster. The list of identiﬁed target nodes (GO terms) is used
to deﬁne the CRC-related proteins. By excluding the seed proteins we obtain
the ﬁnal result.
2.1
Identifying Seed Proteins
The seed proteins are the proteins with the highest level of interest in the PPIN.
In the case of CRC, these proteins are chosen from COSMIC (Catalogue of
somatic mutations in cancer), which is a database of somatically acquired muta-
tions that are found in human cancer [9]. The data in COSMIC is curated from
the scientiﬁc literature and from large-scale experiments. We have converted
all protein symbols from the COSMIC database into NCBI’s Entrez IDs [10],
because this type of protein identiﬁcation is supported by majority of tools. The
obtained seed proteins for colorectal cancer (20 total) are given in Table 1.
2.2
Colorectal Cancer Protein-Protein Interaction Network
2.2.1
CRC PPIN Generation
For the CRC PPIN generation we used the PSICQUIC plugin for Cytoscape
[11]. Cytoscape is a Java based visualization and analysis tool, one of the most
www.ebook3000.com

Analysis of Protein Interaction Network for Colorectal Cancer
149
Table 1. CRC seed proteins (Entrez ID - Protein symbol)
7157-TP53
4089-SMAD4
8289-ARID1A
8085-KMT2D
324-APC
5290-PIK3CA
5727-PTCH1
999-CDH1
3845-KRAS 55294-FBXW7 673-BRAF
8295-TRRAP
472-ATM
4763-NF1
1387-CREBBP 58508-MLL3
5728-PTEN 5925-RB1
3875-KIT
4292-MLH1
commonly used with biological networks. The PSICQUIC plugin takes a list of
seed proteins as input, and searches the available protein interaction databases
for proteins that interact with the ones in the seed list. At the time we conducted
the experiments, the tool was using the following interaction databases: BIND
[12], BindingDB [13], and BioGrid [14].
The resulting network has a total of 1672 nodes and 6032 edges. However,
in this network there are proteins from diﬀerent species, which is due to the
fact that the BIND database stores information about interactions, molecular
complexes and pathways from diﬀerent species (human, mouse, rat). Therefore,
we need to ﬁlter the network nodes to exclude all non-human proteins. We used
the NCBI taxonomy embedded in the Entrez IDs of the proteins to select only the
human proteins (the taxonomy ID of human proteins is 9606), and we produced
the ﬁnal network of 1500 nodes and 5837 edges.
2.2.2
Topological Analysis
A network is said to be scale-free if the number of links originating from a given
node exhibits a power law distribution, which in simple words means that several
of the nodes in the network are highly connected (hubs or essential nodes) and
most of the nodes are connected with one or several neighbors [15]. Scale-free
networks are particularly resistant to random node removal but are extremely
sensitive to the targeted removal of hubs. This information should be taken into
account in the process of identifying target nodes for the CRC PPIN. There are
diﬀerent topological features that can encode a node’s essentiality in the sense
as deﬁned here, of which we use degree, betweeness centrality and maximum
neighborhood component of nodes.
We can deﬁne a PPIN graph with G = (V, E), where V is the node set, and
E is the undirected edges set. The degree Deg(v) of the node v ∈V refers to the
number of edges connected to the node. If we denote with N(v) the set of nodes
adjacent to node v i.e. its neighbors, we can deﬁne Deg(v) as Deg(v) = |N(v)|.
The betweenness centrality BC(v) is a metric which calculates average num-
ber of shortest paths that pass through the node v. It can be calculated by using
the following expression:
BC(v) =

s̸=t̸=v∈C(v)
σst(v)
σst
(1)

150
Z. Ristovski et al.
where s, t, v ∈V , C(v) is the connected component that contains node v, σst is
the number of shortest paths from node s to node t that pass through node v,
and σst is the total number of shortest paths from node s to node t.
Let GNV = (N(v), ENV ) be the subgraph of G induced by the neighborhood
set N(v) of node v. The maximum neighborhood component score of node v,
MNC(v), is deﬁned to be the size of the maximum (largest) connected compo-
nent in GNV . The MNC-score is computed by using the expression:
MNC(v) = |MC(GNV )||V (MC(v))|
(2)
where MC(GNV ) is the maximum connected component of GNV .
Fig. 2. Proteins with highest cumulative topological score
Figure 2 shows the proteins with the highest cumulative topological score,
calculated as the sum of the normalized values of Deg(v), BC(v), and MNC(v),
for each v ∈V . As can be seen, the topological features clearly distinguish the
seed proteins (those proteins marked with the asterisk-sign), which is to be
expected since the seed proteins are the ones that are known to be esential for
CRC. Our focus is on the non-seed proteins in these “high scoring” nodes and
these are our target proteins, i.e. the ones that might be esential to CRC.
2.2.3
Modular Analysis
The modular structure of cancer related PPINs is of great importance since it
gives an insight of the molecular complexes and signaling pathways involved
in the disease and thus oﬀers the possibility of creating hypotheses about the
causes. We use the Molecular complex detection (MCODE) [16] and Markov
cluster algorithm (MCL) [17] as the most common and widely used clustering
www.ebook3000.com

Analysis of Protein Interaction Network for Colorectal Cancer
151
algorithms, which are initially designed for detecting functional modules or pro-
tein complexes, i.e. groups of densely interconnected proteins that are loosely
connected with other such groups. These modules are in line with a property
that disease-related clusters (disease genes causing similar diseases) exhibit an
increased tendency for their protein products to interact with each other.
Molecular Complex Detection (MCODE)
Molecular Complex Detection (MCODE) detects densely connected regions in
PPINs as protein complexes. The algorithm is based on node weighting by local
neighborhood density and outward traversal from a locally dense seed protein to
isolate densely connected regions. To identify modules in our CRC PPIN we
used the MCODE plugin for Cytoscape.
Applied to our CRC PPIN, MCODE found seven putative complexes, but
only two of them are important. A network complex is said to be important if
its score is greater than one and has a decent number of nodes and edges. Let
C = (Vc, Ec) be a complex (subgraph), then the score of C is calculated as the
product of the density and the number of vertices in the complex (Dc ∗|Vc|) [16].
The ﬁrst important complex in our CRC PPIN (Complex1) consists of 6
proteins (ATM, TRRAP, RB1, TP53 and CREBBP belong to the seed proteins
set, while MDM2 is a non-seed protein). While, the second important complex
(Complex2), consists of 4 proteins (NF1 and SMAD4 are the seed proteins, while
SMAD2 and SMAD3 are non-seed proteins).
After obtaining the two complexes, we performed GO enrichment analysis in
order to identify important biological functions and processes related to these
clusters. The GO enrichment analysis is done by using the ClueGO plugin [18] in
Cytoscape. ClueGO uses kappa statistics to link the terms present in a network.
It generates a dynamical network structure for the terms (reﬂecting the relation-
ships between the terms based on the similarity of their associated proteins), and
takes into consideration the protein list of interest in its initial run.
Grouping of terms into functional groups in ClueGO is based on the sim-
ilarity distances measure, as described in the paper for DAVID tool [19]. As
DAVID tool suggests, the terms that share global functional annotation proﬁles
are functionally very similar to each other. The analysis with ClueGO on the two
complexes found with MCODE, Complex1 and Complex2, produced a networks
of GO’s terms (biological processes, functions and pathways) where terms that
share similar functions are grouped together and the most important term in
every group is the term with smallest p-value.
After calculating the most important terms in ClueGO, the most important
terms for Complex1 are: “regulation and cellular response to heat” (associated
proteins: ATM, CREBBP), “p53 Signaling Pathway” (ATM, MDM2, TP53) and
“Cell Cycle: G1/S Check Point” (ATM, CREBBP, MDM2), while for the Com-
plex2, the most important terms are: “I-SMAD binding” (SMAD2, SMAD4),
“Transforming growth factor beta receptor, pathway-speciﬁc cytoplasmic medi-
ator activity” (SMAD2, SMAD3), “Transforming growth factor beta receptor,
cytoplasmic mediator activity” (SMAD2, SMAD3, SMAD4) and “Transforming
growth factor beta2 production” (SMAD3, SMAD4).

152
Z. Ristovski et al.
Markov Cluster Algorithm (MCL)
The Markov cluster algorithm (MCL) simulates a ﬂow on the graph by calculat-
ing successive powers of the associated adjacency matrix. It iteratively enhances
the contrast between regions of strong or weak ﬂow in the graph. The process
converges towards a partition of the graph, with a set of high-ﬂow regions (the
clusters) separated by boundaries with no ﬂow. By applying MCL on our CRC
PPIN we got 17 clusters, as presented on the Fig. 3.
Fig. 3. MCL Clusters (yellow nodes are the seed proteins)
If we take a closer look at the clusters presented on the Fig. 3, the most
interesting clusters for GO enrichment analysis will be the ﬁrst three clusters (the
only ones that have more than one seed protein). The GO enrichment analysis is
done by using the BiNGO tool [20] in Cytoscape. This tool calculates the p-values
for each GO term associated to the proteins in the cluster, and the term with
the smallest p-value is the most important. For the Cluster1 the most important
term is: “Negative regulation of neuroblast proliferation” (associated proteins:
NF1, TP53), for the Cluster2: “Gamma-catenin binding” (APC, CDH1), while
for the Cluster3: “TFC beta signaling pathway” (CREBBP, SMAD4).
3
Results and Discussion
The GO terms that we found via the modular analysis of the CRC PPIN can
be used to identify proteins that are possible bio-markers. From the GO terms
and pathways we can ﬁnd all proteins that are associated with these GO terms.
Table 2 lists the GO term and the associated proteins for each of the GO terms
and pathways, together with associated p-value for every GO term.
But not all of these proteins are important for us. To ﬁlter this list, ﬁrst we
select only the proteins (shown on Table 3) that are part of more than two GO
terms, or the proteins that have a high cumulative topological score (Fig. 2).
www.ebook3000.com

Analysis of Protein Interaction Network for Colorectal Cancer
153
Table 2. Top 10 important GO terms and pathways
GO term
Associated proteins
p-value
10−6
Regulation and cellular response to
heat
YWHAE, BAG1, HSP90AB1, TPR,
HSP90AA1, MAPK1, MTOR, SIRT1,
POM121, EP300, HSPA8, ATM,
DNAJB1, NUP54, DNAJB6, ATR,
RPA2, NUP214, MAPK3, RPA1,
NUPL2, NUPL1, NUP98, RANBP2,
NUP153
52
p53 signaling pathway
ATM, CHEK2, ATR, MDM2, MDM4,
TP53, CKK4, CCNE1
1.2
Cell cycle: G1/S check point
SMAD3, SMAD4, ATR, SKP2,
CDKN1A, TP53, GSK3B, CDK4,
CDK6, CDK2
12
I-SMAD binding
SMURF1, SMAD1, SMAD2, SMAD4,
AXIN1, SMAD7, SMAD6, TGFBR1
2.2
Transforming growth factor beta
receptor, pathway-speciﬁc
cytoplasmic mediator activity
SMAD1, SMAD2, SMAD3, SMAD5,
SMAD93
0.4
Transforming growth factor beta
receptor, cytoplasmic mediator
activity
SMAD1, SMAD2, SMAD3, SMAD4,
SMAD5, SMAD7, SMAD6
0.0055
Transforming growth factor beta2
production
SMAD3, SMAD4
1.1
Negative regulation of neuroblast
proliferation
NF1, TP53
0.41
Gamma-catenin binding
LEF1, CTNNA1, APC, CDH1, FZR1
1.8
TGF beta signaling pathway
TGFBRAP1, CDH1, SMAD2,
SMAD3, SMAD4, APC
0.15
Table 3. List of proteins and number of times each protein appears in selected list of
GO terms (proteins in bold are candidate bio-markers)
472-ATM
4 4087-SMAD2
4 4088-SMAD3 5 324-APC
3
545-ATR
3 4092-SMAD7 2 4089-SMAD4 6 999-CDH1
4
4193-MDM2 2 4091-SMAD6
2 7157-TP53
5 4090-SMAD5 2
4086-SMAD1
3
Then, we excluded the seed proteins and the proteins with functions not related
to cancer. This ﬁnal list of proteins (proteins in boldface from Table 3) could be
taken as possible list of bio-marker candidates.

154
Z. Ristovski et al.
From the public accessible databases we found that these proteins (ATR,
MDM2, SMAD5 and SMAD7) and their mutations are involved in several bio-
logical and molecular processes, for example, cell communication, cell shape and
apoptosis. Also they are involved in the pathways of carcinogenesis, transduc-
tion, and metastasis. These four proteins and their mutations may play impor-
tant roles in CRC PPIN, and they may serve as possible CRC bio-markers. Also,
these proteins may have the key impact in design of some new drugs for CRC
cancer. However, further analysis, investigations and experiments are needed if
we want these hypothesis to be conﬁrmed or rejected, because all these ﬁndings
came from experimental data gathered from diﬀerent databases.
4
Conclusion
In this paper we have presented a methodology for creation and analysis of
protein-protein interaction network (PPIN) of colorectal cancer (CRC). We start
with the list of CRC seed proteins and with the help of Cytoscape we have gen-
erated a CRC PPIN. With the help of well-known topological network character-
istics we have identiﬁed non-seed proteins that are characteristic to CRC. The
modular network analysis, based on MCODE and MCL algorithms, generates
PPIN complexes that are additionally analyzed with the help of GO enrichment
analysis. The identiﬁed “most important” GO terms deﬁned the list of “impor-
tant” proteins. By excluding the seed proteins we have obtained a list of four
proteins (ATR, MDM2, SMAD5 and SMAD7) that could be taken as possible
bio-markers for CRC. These proteins and their mutations may play important
roles in CRC PPIN, and may serve as possible CRC bio-markers. But, these com-
putationally obtained results should be taken with care and must be explored
more deeply to check if these proteins have inﬂuence to CRC and its metastasis.
Acknowledgement. The work in this paper was partially ﬁnanced by the Faculty
of Computer Science and Engineering, Ss. Cyril and Methodius University, Skopje, as
part of the “Analysis of nutrigenomic data” project.
References
1. Kreeger, P.K., Lauﬀenburger, D.A.: Cancer systems biology: a network modeling
perspective. Carcinogenesis 31(1), 2–8 (2010)
2. Consortium, U., et al.: UniProt: a hub for protein information. Nucleic Acids Res.
gku989 (2014)
3. Alberghina, L., H¨ofer, T., Vanoni, M.: Molecular networks and system-level prop-
erties. J. Biotechnol. 144(3), 224–233 (2009)
4. Wachi, S., Yoneda, K., Wu, R.: Interactome-transcriptome analysis reveals the high
centrality of genes diﬀerentially expressed in lung cancer tissues. Bioinformatics
21(23), 4205–4208 (2005)
5. Rhodes, D.R., Chinnaiyan, A.M.: Integrative analysis of the cancer transcriptome.
Nat. Genet. 37, S31–S37 (2005)
www.ebook3000.com

Analysis of Protein Interaction Network for Colorectal Cancer
155
6. Mani, K.M., Lefebvre, C., Wang, K., Lim, W.K., Basso, K., Dalla-Favera, R.,
Califano, A.: A systems biology approach to prediction of oncogenes and molecular
perturbation targets in B-cell lymphomas. Mol. Syst. Biol. 4(1), 169 (2008)
7. Jonsson, P.F., Bates, P.A.: Global topological features of cancer proteins in the
human interactome. Bioinformatics 22(18), 2291–2297 (2006)
8. Aragues, R., Sander, C., Oliva, B.: Predicting cancer involvement of genes from
heterogeneous data. BMC Bioinform. 9(1), 1 (2008)
9. Forbes, S.A., Bindal, N., Bamford, S., Cole, C., Kok, C.Y., Beare, D., Jia, M.,
Shepherd, R., Leung, K., Menzies, A., et al.: COSMIC: mining complete cancer
genomes in the catalogue of somatic mutations in cancer. Nucleic Acids Res. gkq929
(2010)
10. Maglott, D., Ostell, J., Pruitt, K.D., Tatusova, T.: Entrez gene: gene-centered
information at NCBI. Nucleic Acids Res. 33(Suppl. 1), D54–D58 (2005)
11. Aranda, B., Blankenburg, H., Kerrien, S., Brinkman, F.S., Ceol, A., Chautard, E.,
Dana, J.M., De Las Rivas, J., Dumousseau, M., Galeota, E., et al.: PSICQUIC
and PSISCORE: accessing and scoring molecular interactions. Nat. Methods 8(7),
528–529 (2011)
12. Bader, G.D., Betel, D., Hogue, C.W.: BIND: the biomolecular interaction network
database. Nucleic Acids Res. 31(1), 248–250 (2003)
13. Liu, T., Lin, Y., Wen, X., Jorissen, R.N., Gilson, M.K.: BindingDB: a web-
accessible database of experimentally determined protein-ligand binding aﬃnities.
Nucleic Acids Res. 35(Suppl. 1), D198–D201 (2007)
14. Stark, C., Breitkreutz, B.J., Reguly, T., Boucher, L., Breitkreutz, A., Tyers,
M.: BioGRID: a general repository for interaction datasets. Nucleic Acids Res.
34(Suppl. 1), D535–D539 (2006)
15. Barab´asi, A.L., Albert, R.: Emergence of scaling in random networks. Science
286(5439), 509–512 (1999)
16. Bader, G.D., Hogue, C.W.: An automated method for ﬁnding molecular complexes
in large protein interaction networks. BMC Bioinform. 4(1), 1 (2003)
17. Morris, J.H., Apeltsin, L., Newman, A.M., Baumbach, J., Wittkop, T., Su, G.,
Bader, G.D., Ferrin, T.E.: clusterMaker: a multi-algorithm clustering plugin for
cytoscape. BMC Bioinform. 12(1), 1 (2011)
18. Bindea, G., Mlecnik, B., Hackl, H., Charoentong, P., Tosolini, M., Kirilovsky, A.,
Fridman, W.H., Pag`es, F., Trajanoski, Z., Galon, J.: ClueGO: a cytoscape plug-in
to decipher functionally grouped gene ontology and pathway annotation networks.
Bioinformatics 25(8), 1091–1093 (2009)
19. Alvord, G., Roayaei, J., Stephens, R., Baseler, M.W., Lane, H.C., Lempicki, R.A.:
The david gene functional classiﬁcation tool: a novel biological module-centric
algorithm to functionally analyze large gene lists. Genome Biol. 8(9), 183 (2007)
20. Maere, S., Heymans, K., Kuiper, M.: BiNGO: a cytoscape plugin to assess over-
representation of gene ontology categories in biological networks. Bioinformatics
21(16), 3448–3449 (2005)

Using Sentiment Analysis of Twitter Data
for Determining Popularity of City Locations
Nikola Dinkić1(&), Nikola Džaković1, Jugoslav Joković1,
Leonid Stoimenov1, and Aleksandra Đukić2
1 Faculty of Electronic Engineering, University of Niš, Niš, Serbia
{dinkicnikola,ndzakovic}@elfak.rs, {Jugoslav.Jokovic,
Leonid.Stoimenov}@elfak.ni.ac.rs
2 Faculty of Architecture, University of Belgrade, Belgrade, Serbia
adjukic@rcub.bg.ac.rs
Abstract. The paper considers mining and analyzing data generated by Twitter
social network, regarding content classiﬁcation, language determination and
sentiment analysis of tweets. Analyzes are based on geospatial tweets collected
in timespan of four months within region Vračar in Belgrade, Serbia. All of
collected data is ﬁrst being preprocessed, ﬁltered and classiﬁed by given criteria,
by using “Twitter search engine” (TSE) application, that has been upgraded in
order to detect tweet language and execute sentiment analysis of the tweets
written in English. This type of analysis can be used for determining popularity
of city locations of interest and public spaces in general.
Keywords: Natural language processing  Sentiment analysis and opinion
mining  Geospatial data  Twitter social network
1
Introduction
The synergy between technology and built environment generate urban culture with
digital streams. On the other hand, cities and their open public spaces are reﬂections of
users changing needs. Furthermore, the future of urban space depends on the role of
information and communication technologies (ICT) and importance of their networks
should be reconsidered since they have become indispensable ingredients of urban life
[1]. ICT provides the overlapping of real and virtual spaces and allows creative par-
ticipation of users.
With the explosive growth of social media (e.g., reviews, forum discussions, blogs,
micro-blogs, Twitter, comments, and postings in social network sites) on the Web,
individuals and organizations are increasingly using the content in these media for
decision making. Many big corporations have also built their own in-house decision
making solutions, e.g., Microsoft, Google, Hewlett-Packard, SAP, and SAS. Although
linguistics and natural language processing have a long history, research about people
opinions and sentiments in social media is actual in last ten years.
This paper will present and analyze the connections which are established and
intensiﬁed between users and open public spaces via Twitter. Twitter is one the most
popular data sources for research [2] because of its open network allowing access to
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_15
www.ebook3000.com

information published through the platform. Twitter is a novel microblogging service
launched in 2006 with more than 310 million monthly active users. On Twitter, every
user can publish short messages with up to 140 characters, so-called “tweets”, which
are visible on a public message board of the website or through third-party applications.
The public timeline conveying the tweets of all users worldwide is an extensive
real-time information stream of more than one million messages per hour. The original
idea behind microblogging was to provide personal status updates. However, these
days, postings cover every imaginable topic, ranging from political news to product
information in a variety of formats, e.g., short sentences, links to websites, and direct
messages to other users.
The method that was used in analysis is the method of mapping users on the social
maps (via social networks). It was based on a software application TSE [3]. The aim
was tracking and measuring the intensity of users in the monitored territory, testing the
latest behavioral patterns of them as well as tracing the “positive routes”. The obtained
results have enabled the determination of the image of the open public spaces perceived
by the users, as well as the potential of the analyzed area for the formation of transverse
and longitudinal pedestrian ﬂows that could help improving networking of open public
spaces.
2
Related Work
Natural language processing (NLP) is a ﬁeld of computer science, artiﬁcial intelligence,
and computational linguistics concerned with the interactions between computers and
human (natural) languages. NLP is an area of research and application that explores
how computers can be used to understand and manipulate natural language text or
speech to do useful things. NLP researchers aim to gather knowledge on how human
beings understand and use language so that appropriate tools and techniques can be
developed to make computer systems understand and manipulate natural languages to
perform desired tasks.
This paper is focused on sentiment analysis, also called opinion mining. It is the
ﬁeld of NLP that analyzes people opinions, sentiments, evaluations, appraisals, atti-
tudes, and emotions towards entities such as products, services, organizations, indi-
viduals, issues, events, topics, and their attributes. It represents a large problem space.
While in industry, the term sentiment analysis is more commonly used, but in aca-
demia, both sentiment analysis and opinion mining are frequently employed. They
basically represent the same ﬁeld of study.
Sentiment analysis and opinion mining mainly focuses on opinions, which express
or imply positive or negative sentiments. In paper [4], Hu and Liu proposed a
lexicon-based algorithm for aspect level sentiment classiﬁcation, but the method can
determine the sentiment orientation of a sentence as well. It was based on a sentiment
lexicon generated using a bootstrapping strategy with some given positive and negative
sentiment word seeds and the synonyms and antonyms relations in WordNet.
The sentiment orientation of a sentence was determined by summing up the ori-
entation scores of all sentiment words in the sentence. A positive word was given the
sentiment score of +1 and a negative word was given the sentiment score of −1. In [5]
Using Sentiment Analysis of Twitter Data for Determining Popularity
157

the relationships between the NFL betting line and public opinions in blogs and Twitter
were studied. In [6] Twitter sentiment was linked with public opinion polls. In [7]
Twitter sentiment was also applied to predict election results. In [8] Twitter data, movie
reviews and blogs were used to predict box-ofﬁce revenues for movies. In [9] Twitter
moods were used to predict the stock market. In [10] they tracked opinions about
movies on Twitter and predicted box-ofﬁce revenues with very accurate results. They
simply used their opinion parser system to analyze positive and negative opinions
about each movie with no additional algorithms.
In general, sentiment analysis has been investigated mainly at three levels:
• Document level: The task at this level is to classify whether a whole opinion
document expresses a positive or negative sentiment [11]. For example, given a
product review, the system determines whether the review expresses an overall
positive or negative opinion about the product. This task is commonly known as
document-level sentiment classiﬁcation. This level of analysis assumes that each
document expresses opinions on a single entity (e.g., a single product). Thus, it is
not applicable to documents which evaluate or compare multiple entities.
• Sentence level: The task at this level goes to the sentences and determines whether
each sentence expressed a positive, negative, or neutral opinion. Neutral usually
means no opinion. This level of analysis is closely related to subjectivity classiﬁ-
cation, which distinguishes sentences (called objective sentences) that express
factual information from sentences (called subjective sentences) that express sub-
jective views and opinions.
• Entity and Aspect level: Both the document level and the sentence level analyzes
do not discover what exactly people liked and did not like. Instead of looking at
language constructs (documents, paragraphs, sentences, clauses or phrases), aspect
level directly looks at the opinion itself. It is based on the idea that an opinion
consists of a sentiment (positive or negative) and a target (of opinion). An opinion
without its target being identiﬁed is of limited use. Realizing the importance of
opinion targets also helps us understand the sentiment analysis problem better. The
goal of this level of analysis is to discover sentiments on entities and/or their
aspects.
3
Classiﬁcation of Geospatial Data
Twitter search engine [3] application (TSE) allows gathering, mining and storing of
geospatial data produced on Twitter social network. This paper describes new features
of application TSE and its ability to process and analyze data from this social network.
Also in order to illustrate functionality of this application, this paper shows results of
analysis of data collected from tweets for Vračar region in Belgrade, Serbia.
In addition to the collection and storage of data, TSE offers visualization and analysis,
but also it can execute complex queries over stored data. This queries use special
geospatial functions that are built within MySQL database. These functions represent
correlation between two objects that are deﬁned by geospatial points. This application
also offers users to draw polygons on Google map, in order to deﬁne boundaries of their
158
N. Dinkić et al.
www.ebook3000.com

analysis. Note, that all polygons must be within regions for which data is collected.
Polygon drawing is done by using Google Maps JavaScript API. Web application TSE
also has the ability to detect the language and perform sentiment analysis.
The most important indicators of sentiments are sentiment words, also called
opinion words. These words are commonly used to express positive or negative sen-
timents. For example, good, wonderful, and amazing are positive sentiment words, and
bad, poor, and terrible are negative sentiment words. Sentiment words and phrases are
instrumental to sentiment analysis for obvious reasons. A list of such words and
phrases is called a sentiment lexicon (or opinion lexicon). Although sentiment words
and phrases are important for sentiment analysis, only using them is far from sufﬁcient.
The problem is much more complex. In other words, we can say that sentiment lexicon
is necessary but not sufﬁcient for sentiment analysis of complex texts. Since, tweets are
generally short and informal, and use many Internet slangs and emoticons, they are
easier to analyze due to the length limit because the authors are usually straight for-
ward, and immediately get right to the point. Thus, it is often easier to achieve high
sentiment analysis accuracy. Reviews are also easier because they are highly focused
with little irrelevant information. Because of that for sentiment analysis, this paper uses
lexicon-based algorithm to determine the sentiment orientation of a sentence.
Researchers have proposed many approaches to compile sentiment words. Three
main approaches are: manual approach, dictionary-based approach, and corpus-based
approach. [4] used a dictionary to compile sentiment words. This is an obvious
approach because most dictionaries (e.g., WordNet) list synonyms and antonyms for
each word. Thus, a simple technique in this approach is to use a few seed sentiment
words to bootstrap based on the synonym and antonym structure of a dictionary.
Speciﬁcally, this method works as follows: A small set of sentiment words (seeds) with
known positive or negative orientations is ﬁrst collected manually, which is very easy.
The algorithm then grows this set by searching in the WordNet or another online
dictionary for their synonyms and antonyms. The newly found words are added to the
seed list. The next iteration begins. The iterative process ends when no more new
words can be found. Their dictionary was compiled over many years starting from their
ﬁrst paper. Original dictionary consists of 4783 negative and 2006 positive words, but
to the original dictionary were added emoticons, which are now very popular, and can
also show if it is a positive or negative opinion. Tweets can be classiﬁed as a positive or
negative depending on which group of words they contain. This gives similar results as
simply counting positive and negative words, since Twitter messages are so short
(about 11 words). Since the area of Serbia belongs to the world top by multilingualism,
it is necessary to detect only the tweets that are in English. TSE uses web service
“Language detection API” [12] for language detection. Language detection API has the
ability to detect 160 different languages and offers 5000 requests for free on daily basis.
4
Data Processing and Analysis
The analysis of geospatial data requires data to be in the speciﬁed format so that
geospatial queries can be executed. However, since all information obtained from the
Twitter REST API is in JSON format, before any analysis it is necessary to perform
Using Sentiment Analysis of Twitter Data for Determining Popularity
159

transformation of geo-information to speciﬁc format. This process of transformation of
the original data to geospatial data types represents the pre-processing, and this is the
ﬁrst step in this analysis.
In order to illustrate possibilities of TSE application, tweets collected over a period
of four months (December 2015–March 2016) for region Vračar in Belgrade, were
analyzed and results of this analysis are shown in this paper. This space is deﬁned by
the corresponding polygon on the map, as shown in Fig. 4. The execution of geospatial
queries for the given polygon was obtained the cumulative data presented in Table 1.
In addition to the cumulative analysis, TSE allows ﬁltering data based on the
analysis in speciﬁed time intervals, in other words this means classiﬁcation by months
of the year, days of the week or otherwise deﬁned time intervals. Figure 1 shows the
distribution of tweets by month, which are shared in the studied area. It can be con-
cluded that the Twitter users in the previous four-month period were the most active
during the March 2016.
Table 1. Cumulative ﬁgures for region Vračar
Type of analysis
Value
Number of tweets
3002
Number of users
603
Number of followers
1079721
Number of friends
421780
Number of retweets
35
Number of likes
490
Number of applications
8
Fig. 1. Tweet count by months
160
N. Dinkić et al.
www.ebook3000.com

The results of data classiﬁcation by days of the week are shown in Fig. 2. Based on
them, we can conclude that the users were most active on Saturdays.
In addition to analysis of data for a given time, the type of content of tweet itself
could classify the collected tweets. Figure 3 represents the percentage representation of
content of analyzed tweets.
Fig. 2. Classiﬁcation of tweets by days of the week
Fig. 3. Classiﬁcation of tweets by content
Using Sentiment Analysis of Twitter Data for Determining Popularity
161

Based on the distribution of tweets per application used to post it, it can be concluded,
which content type has a tweet, without the need for a deeper analysis. For example, all
tweets, which are sent by Instagram, must contain a picture. From the analysis results, it
can be concluded that each Twitter user from community of Vračar on average posted
4.97 tweets, the users were most active during March, the largest number of tweets has
been posted on Saturday and the most popular application is Instagram with 44.5%,
following Foursquare with 34.9%, which indicates high attractiveness of the area.
The geospatial tweets classiﬁed by content are shown in Fig. 4. Tweets are dis-
played on the map with markers in different colors depending on the content of the
tweet, picture (red), check in (green) and only text (blue).
Analysis of all tweets detected 12 different languages: Serbian, English, German,
French, Japanese, Thai, Russian, Turkish, Scottish, Bulgarian, Spanish and Portuguese,
of which the most common are English with 67.4% and Serbian with 30.6%.
Based on sentiment analysis of content posted in region Vračar, tweets were
divided into three groups, tweets which contain positive words, negative words and
which contain both positive and negative. Since on Twitter it is a very popular to use
hashtags (#), tweet content can be divided into two groups, text and hashtags. The
results of sentiment analysis of tweets that contain either positive or negative words are
shown in Table 2.
Fig. 4. Geospatial tweets for region Vračar classiﬁed by content
Table 2. Sentiment analysis of tweets
Hashtag(#) Text Both
Positive
55
155
200
Negative
9
43
52
Complex
5
10
14
P
69
208
266
162
N. Dinkić et al.
www.ebook3000.com

Sentiment analysis can be executed only on tweets that contain text (including
hashtags). Application detected 266 text tweets to belong to one of three groups
(positive, negative, and complex) and this distribution chart is shown in Fig. 5. From
266 classiﬁed text tweets, 75.2% of them carried a positive message. The map of the
tweets, which are classiﬁed as positive is shown in the Fig. 6.
Fig. 6. Distribution of positive tweets
Fig. 5. Tweets orientation
Using Sentiment Analysis of Twitter Data for Determining Popularity
163

5
Conclusion
Generally, Twitter social network turned out to be a great basis for analysis of public
space popularity. Its API provides a lot of publicly available information about tweets,
but also about the users, which is the most important thing for every successful
research. New feature of TSE application for language detection conﬁrms the fact that
Serbia belongs to the world top by multilingualism, which indicates that Vračar is very
popular for foreign tourists. Sentiment analysis also shows that attraction sites of this
region leave a positive impression on tourists who come to visit them. All analyses
shown in this paper represent only a portion of possibilities that TSE application can
offer and all of them can be used for creating better urban plans, in terms of (re)design
of public spaces. These analyses can be used to quantify popularity of locations of
interest and public spaces in general, as well as to determine correlations between
locations.
References
1. Pigg, K.E., Crank, L.D.: Building community social capital the potential and promise of
information and communications technologies. J. Commun. Inf. 1(1), 58–73 (2004)
2. dos Santos, A.D.P., Wives, L.K., Alvares, L.O.: Location-Based Events Detection on
Micro-Blogs (2012)
3. Nikola, D., Dinkić, N., Joković, J., Stoimenov, L.: Web application for mining, storing,
processing and geo-analysis data from Twitter social network, YU INFO, Kopaonik, Srbija,
March 2016
4. Hu, M., Liu, B.: Mining opinion features in customer reviews. In: American Association for
Artiﬁcial Intelligence (2004)
5. Hong, Y., Skiena, S.: The wisdom of bookies? Sentiment analysis versus the NFL point
spread. In: Proceedings of the Fourth International AAAI Conference on Weblogs and Social
Media (2010)
6. O’Connor, B.: From tweets to polls: linking text sentiment to public opinion time series. In:
Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media
(2010)
7. Tumasjan, A.: Predicting elections with Twitter: what 140 characters reveal about political
sentiment. In: Proceedings of the Fourth International AAAI Conference on Weblogs and
Social Media (2010)
8. Asur,
S.,
Huberman,
B.A.:
Predicting
the
future
with
social
media.
In:
2010
IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent
Technology (WI-IAT) (2010)
9. Bollena, J., Mao, H., Zeng, X.-J.: Twitter mood predicts the stock market. J. Comput. Sci. 2,
1–8 (2011)
10. Liu, B.: Sentiment Analysis and Opinion Mining (2012)
11. Pang, B., Lee, L., Vaithyanathan, S.: Thumbs up?: Sentiment classiﬁcation using machine
learning techniques. In: Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia (2002)
12. Language detection API. https://detectlanguage.com/. Accessed 15 Apr 2016
164
N. Dinkić et al.
www.ebook3000.com

Internet Addiction: Evaluating
the Psychometric Properties of the IAT
in Macedonia
Martin Mihajlov1(&) and Aleksandar Stojmenski2
1 Faculty of Economics, Ss. Cyril and Methodius University, Skopje, Macedonia
martin@eccf.ukim.edu.mk
2 Faculty of Computer Science and Engineering,
Ss. Cyril and Methodius University, Skopje, Macedonia
aleksandar.stojmenski@ﬁnki.ukim.mk
Abstract. In this digital age there is a growing importance in global research of
Internet addiction. The purpose of this research study is to establish a valid
instrument for measuring the addictive use of Internet. We investigated the psy-
chometric properties of the established Young’s Internet Addiction Test (IAT) on
a Macedonian population. Since its development, there has been a number of
culture-speciﬁc validation studies of the IAT, but never in cultures from south-east
Europe. In a sample of 322 undergraduate participants, exploratory factor analysis
(EFA) determined the presence of a three factor structure of the IAT with 17 items.
The three factors, “Withdrawal and social problems”, “Time management” and
“Failure and neglect” explained 38.43% of the total variance with good internal
consistency for the IAT (Cronbach’s a = .89) and good reliability for the factors
(.65–.84). Hence, this version of the IAT is a valid instrument with sound psy-
chometric properties for measuring Internet addiction in a sample of participants
from south-east Europe and may be used for further research.
Keywords: Internet addiction  Internet Addiction Test (IAT)  Macedonia
1
Introduction
The terminology, deﬁnition and theoretical basis of addictive Internet use has been and
still is the subject of scientiﬁc debates. The term most widely used in the literature,
“Internet addiction” was deﬁned by Young [1] as “an impulse-control disorder without
intoxication that has symptoms such as preoccupation with the Internet, problem with
control over Internet use, lower mood, excessive amount of time spent online, lower
performance at school or work, deteriorating physical health, jeopardized relationships
with family or friends, and lying about the Internet use”. Besides “Internet addiction”
there are other terms in the literature including problematic Internet use [2], Internet
abuse [3], pathological Internet use [4], and compulsive Internet use [5].
With over 46.4% of the population online [6] the Internet has become an essential
tool in all aspects of everyday life. The increasing number of Internet users is followed
by an increase in the number of hours spent online, mostly due to the anytime-
anywhere access enabled by mobile devices with an almost constant Internet
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_16

connection. The increased accessibility of the Internet also increases the potential for
addictive use, especially among young people who generally ﬁnd life without the
Internet almost impossible. With a few notable exceptions [7], the research on Internet
addiction has mostly been restricted to selected developed countries. Hence, for the
purpose of advancing the understanding of this phenomenon it is necessary to evaluate
its properties in less-developed countries, like Macedonia. With only provisional access
in the beginning of this century when Internet connectivity was available to only 2.5%
of the population, the number of Internet users in Macedonia has grown rapidly to
almost 70% in 2016 [8]. According to the latest report by the State Statistical Ofﬁce, in
2015 approximately 89.3% of the population aged 15–24 used the Internet daily,
whereas 65.8% of these users accessed the Internet via a mobile device (SSO, 2015).
When it comes to measuring Internet addiction, according to Laconi et al. [9], there
are 45 tools, which measure and assess addictive Internet use through scales, interviews
or diagnostic criteria. According to the same research, only 17 tools have had their
psychometric properties evaluated more than once. The development of these tools
includes different approaches such as adaptation of DSM-IV criteria regarding sub-
stance abuse and dependence or pathological gambling [10, 11], cognitive-behavioral
modeling [2, 12], or behavioral addiction modeling [13].
From these tools, the Internet Addiction Test [13] is one of the earliest constructs for
evaluating addictive online behavior. With widely studied psychometric properties, this
test has been translated into many languages and is the most popular evaluation tool.
Regarding the properties of the test, its internal consistency is reported as high, ranging
from Cronbach’s a = .85 to a = .93 [14, 15] while the temporal reliability of the test is
reported as good, ranging from r = .73 to r = .85 [16, 17]. However, the factorial
structure of the test has signiﬁcantly varied across studies and samples ranging from
one-dimensional [18, 19, 20], two-dimensional [21], three [15], four [16], ﬁve [22], and
six-dimensional solutions [16]. A sample overview of factors is presented in Table 1.
Considering the high variance of the factor structure between studies it is necessary to
validate the test before it can be applied to a new culture. Instead of using Internet
addiction as an overall concept, examining the factor structure and dimensionality of the
IAT will provide us with a greater detail of the culture-speciﬁc nature of addictive use.
Table 1. Factor extraction overview in analysis of other cultures
No
Factors
Language
Reference
1
/
French
[19]
2
Dependent use and excessive use
English
[26]
3
Psychological/emotional conﬂict, time-management
problems, and mood modiﬁcation
English
[25]
3
Withdrawal & social problems, time management &
performance, and reality substitute
Chinese
[10]
5
Lack of control, neglect of duty, problematic use, social
relationship disruption, and email privacy
Malay
[23]
(continued)
166
M. Mihajlov and A. Stojmenski
www.ebook3000.com

Addictive Internet use can greatly inﬂuence the everyday life of the individual.
Nevertheless, due to a signiﬁcant lack of research the impact of addictive Internet to the
personal life of individuals in South-East Europe is unknown. Hence, as a ﬁrst step
towards developing this ﬁeld of research in this paper we examine and validate the IAT
within the cultural context of Macedonia.
2
Method
2.1
Participants
A total of 322 undergraduate students from three different departments participated in
this study. As we excluded 5 responses as unengaged entries (SD < 0.3), the ﬁnal
dataset consisted of responses from 317 participants. The age of the participants varied
between 19 and 25 (M = 20.79, SD = 1.269) and the gender distribution was rather
even with 49.21% female (n = 156, M = 20.71, SD = 1.042), and 50.79% male
(n = 161, M = 20.85, SD = 1.435) participants. Based on their ﬁeld of study 37.2%
(n = 118) of the participants were in economics, 49.2% (n = 156) in computer science
and 13.6% (n = 43) in civil engineering.
2.2
Instrument/Measures
The Internet Addiction Test is a questionnaire designed for self-evaluation of Internet
addiction. As an expanded version of the Internet Addiction Diagnostic Questionnaire
(IADQ) [22] it acts as a self-report instrument used to measure an individual’s Internet
use from the perspective of psychological symptoms and behaviors. The questionnaire
consists of 20 items rated on a 5-point Likert scale, ranging from 1 (rarely) to 5
(always). The minimum obtainable score is set at 20 and the maximum at 100, with a
higher score indicating a greater level of Internet addiction. Based on the severity score
participants can be classiﬁed as:
• minimal users (20–39), the average online users with complete control over their
Internet use;
• moderate users (40–69), the users who experience occasional or frequent problems
due to their Internet use;
• excessive users (70–100), the users with signiﬁcant problems caused by Internet use.
Table 1. (continued)
No
Factors
Language
Reference
6
Salience, excessive use, neglect work, anticipation, lack of
control, and neglect social life
English
[17]
6
Compromised social quality of life, compromised
individual quality of life, compensatory usage of the
Internet, compromised academic/working careers,
compromised time control, and excitatory usage of the
Internet
Italian
[24]
Internet Addiction: Evaluating the Psychometric Properties of the IAT
167

2.3
Procedure/Analysis
A web link to an online questionnaire was provided to all participants. Following a
declaration of informed consent, the volunteering participants completed the IAT
questionnaire, along with basic demographic data (age, gender, ﬁeld of study) at a time
of their leisure.
After the preliminary evaluation of the collected data we proceeded with the sta-
tistical analysis in three phases using SPSS. First we examined the response distribu-
tions of all IAT items to estimate their internal consistency through inter-item
correlations. We then proceeded with the exploratory factor analysis (EFA) using
principal axis factoring as an extraction method since the Likert-scale data had an
expected non-normal distribution.
3
Results
3.1
Preliminary Analysis
The distribution of responses were not excessively skewed or kuriotic implying suf-
ﬁcient variance on all items. The correlation matrix contained no negative values
indicating that all of the items measured the same characteristic, and had 182 (95.79%)
signiﬁcant inter-item coefﬁcients with an average r = 0.254. The items of the IAT also
had a good internal consistency with a Cronbach’s alpha value of 0.87.
The suitability of data for factor analysis was examined with measures of sampling
adequacy. A substantial number of coefﬁcients (35.79%) had a value of 0.3 or above
with the determinant of the R-matrix at .004 indicating no multicolinearity or singu-
larity problems. The Kaiser–Meyer–Olkin measure (KMO) indicated a value of .890,
exceeding the minimum value of .600, while all KMO values for individual items were
greater than .78, well above the acceptable limit of .5. Additionally, the Bartlett’s Test
of Sphericity was statistically signiﬁcant (v2 = 1735.021, df = 190, p < .0001), sup-
porting the factorability of the correlation matrix.
3.2
Factor Analysis
The initial exploratory factor analysis was run to obtain factors with eigenvalues greater
than Kaiser’s criterion of 1.00. The results of the factor extraction returned ﬁve factors
explaining 40.41% of the variance. Considering that the individual communalities after
extraction were less than .7, and the average communality was 0.404, the retention of
all the extracted factors was highly unlikely. The ambiguous scree plot showed
inﬂexions that would justify the retention of 2 to 4 factors. Additionally, the factor
structure after rotation revealed several items with initial loadings below 0.3 for all
factors. Hence, we iterated the SPSS analysis by removing different items in each
iteration and evaluating the state of the resulting factors. The optimal factor extraction
occurred when we removed items q4, q7 and q10 either due to low communalities or
low factor loadings (< 0.3). The reduced 17-item rotated solution returned three factors
explaining 38.43% of the variance (Table 2). We identiﬁed a cross-loading on item 14
between Factor 1 and Factor 2, whereas the item was grouped on Factor 2 because of
168
M. Mihajlov and A. Stojmenski
www.ebook3000.com

its larger loading and conceptual ﬁt. Hence, Factor 1 comprises of items 3, 5, 9, 11, 12,
13, 15, 18, 19 and 20, Factor 2 comprises of items 1, 2, 14 and 16 and Factor 3
comprises of items 6, 8 and 17. Factor 1 can be classiﬁed as “Withdrawal and social
problems”, factor 2 as Time management and factor 3 as “Failure and neglect”.
The convergent validity of the IAT results were examined by evaluation inter-factor
correlations (Table 3). The strongest correlation was between the “Withdrawal and
social problems” factor and the “Failure and neglect” factor (r = 0.477). While the
correlations between “Withdrawal and social problems” and “Time management” and
“Failure and neglect” and “Time management” were similar with r = 0.323 and
r = 0.325 respectively.
Table 2. Reduced IAT pattern matrix
F1
F2
F3
Item 03
0.42
Item 05
0.32
Item 09
0.47
Item 11
0.47
Item 12
0.61
Item 13
0.64
Item 15
0.67
Item 18
0.46
Item 19
0.57
Item 20
0.80
Item 1
0.62
Item 2
0.37
Item 14
(.31) 0.36
Item 16
0.45
Item 6
0.61
Item 8
0.60
Item 17
0.32
Eigenvalue
4.52
2.09 2.74
Variance explained 28.67
6.03 3.74
Cronbach’s a
0.84
0.66 0.65
Table 3. Reducet factor correlation matrix
F1 - Withdrawal and
social problems
F2 - Time
management
F3 - Failure and
neglect
F1 - Withdrawal and
social problems
1.000
.323
.477
F2 - Time management
.323
1.000
.325
F3 - Failure and neglect
.477
.325
1.000
Internet Addiction: Evaluating the Psychometric Properties of the IAT
169

Regarding reliability, we calculated the Cronbach’s a for all factors. The “With-
drawal and social problems” subscale of the IAT had a good reliability with Cronbach’s
a = 0.84. Conversely, the “Time management” subscale had an acceptable reliability
with Cronbach’s a = 0.71, while the “Failure and neglect” subscale had a questionable
reliability with for Cronbach’s a = 0.65.
4
IAT Analysis
Using the IAT cutoff score system, a total of 82 participants (25.87%) were classiﬁed as
minimal users, the majority, 215 participants (67.82%), were classiﬁed as moderate
users, and only 20 participants (6.31%) were classiﬁed as excessive users. Based on
gender the distribution of participants was similar for minimal users (45.1% male,
54.9% female) and moderate users (51.2% male, 48.8% female), while there was a
difference in excessive users (70% male, 30% female). Nevertheless, a one-way
ANOVA revealed no signiﬁcant effect of gender on the level of Internet addiction, F (1,
315) = 3.22, p = 0.74 > 0.5. Additionally, we tested for differences in Internet
addiction based on ﬁeld of study and the one-way ANOVA also revealed no signiﬁcant
effect, F (1, 315) = 2.477, p = 0.86 > 0.5.
The mean score of the IAT score for all participants was 48.55 (SD = 12.71).
Disregarding the participants gender, the mean score for minimal users was 33.65
(SD = 40.08), for moderate users was 61.67 (SD = 7.979), and for excessive users
was76.15 (SD = 5.153). A one-way ANOVA revealed no signiﬁcant difference
between participants of different genders for the three factors deﬁned in the EFA.
5
Discussion
The results presented in the previous section present the psychometric properties of the
IAT for a Macedonian population. The factor analysis suggested the existence of three
factors to explain this phenomenon: the ﬁrst relates to withdrawal from ofﬂine activities
that may or may not include other people, the second relates to the inability to manage
time spent online the third relates neglect of academic and professional activities as
well as the failure to avoid the use of Internet in general.
Although initially designed as a single-dimension construct, IAT has repeatedly
shown a multidimensional structure in most studies. Our three factor solution is par-
tially in accordance with previous investigations, most notably [10, 15] with whom it
shares two of the extracted factors, “Withdrawal and social problems” and “Time
management”. The same two factors are also present in [27] 5-factor solution, while
time management is also a part of [24] 6-factor construct.
The lower factor loadings of IAT items and the exclusion of items in general
suggests that the IAT should be reﬁned in accordance with technological advancement
and changes in social life [7], which have made signiﬁcant differences in how people
interact with online environments. Speciﬁcally, item 7 related to the frequency of
checking email has been surpassed by the use of a myriad of messaging and com-
munication apps.
170
M. Mihajlov and A. Stojmenski
www.ebook3000.com

The limitations of this study follow several directions which suggest avenues for
future studies. The results from the sample might not be generalized on a wider pop-
ulation without a larger longitudinal study. Additionally, by not including another
instrument for comparison we couldn’t determine the convergent and discriminant
validity of the IAT. The IAT itself is limited as it doesn’t consider the use of mobile
devices nor the context of Internet. Furthermore, the IAT generally lacks temporal
stability as it ignores the transient nature of Internet use. Nevertheless, despite these
limitations, the ﬁtting factor structure along with the internal consistency and reliability
conﬁrm the IAT is a valid instrument for measuring Internet addiction in different
cultures.
References
1. Young, K.S.: Internet addiction: symptoms, evaluation and treatment. Innovations Clin.
Pract.: Source B. 17, 19–31 (1999)
2. Davis, R.A., Flett, G.L., Besser, A.: Validation of a new scale for measuring problematic
internet use: implications for pre-employment screening. CyberPsychol. Behav. 5, 331–345
(2002)
3. Morahan-Martin, J.: Internet abuse addiction? disorder? symptom? alternative explanations?
Soc. Sci. Comput. Rev. 23, 39–48 (2005)
4. Niemz, K., Grifﬁths, M., Banyard, P.: Prevalence of pathological internet use among
university students and correlations with self-esteem, the General Health Questionnaire
(GHQ), and disinhibition. CyberPsychol. Behav. 8, 562–570 (2005)
5. Meerkerk, G.-J., Eijnden, R.J.V.D., Garretsen, H.F.: Predicting compulsive internet use: it’s
all about sex! CyberPsychol. Behav. 9, 95–103 (2006)
6. Internet World Stats. World Internet Users and 2015 Population Stats (Accessed 2015).
http://www.internetworldstats.com/stats.htm
7. Karim, A.R., Nigar, N.: The Internet addiction test: assessing its psychometric properties in
Bangladeshi culture. Asian J. Psychiatry 10, 75–83 (2014)
8. Internet Live Stats, Republic of Macedonia Internet Users. (Accessed 2016). http://www.
internetlivestats.com/internet-users/macedonia/
9. Laconi, S., Rodgers, R.F., Chabrol, H.: The measurement of internet addiction: a critical
review of existing scales and their psychometric properties. Comput. Hum. Behav. 41, 190–
202 (2014)
10. Chang, M.K., Law, S.P.M.: Factor structure for young’s Internet Addiction Test: a
conﬁrmatory study. Comput. Hum. Behav. 24, 2597–2619 (2008)
11. Chen, S., Weng, L., Su, Y., Wu, H., Yang, P.: Development of a Chinese internet addiction
scale and its psychometric study. Chin. J. Psychol. 45, 279 (2003)
12. Meerkerk, G.-J., Van Den Eijnden, R.J., Vermulst, A.A., Garretsen, H.F.: The compulsive
internet use scale (CIUS): some psychometric properties. Cyberpsychol. behav. 12, 1–6
(2009)
13. Young, K.S.: Internet addiction: the emergence of a new clinical disorder. CyberPsychol.
Behav. 1, 237–244 (1998)
14. Conti, M.A., Jardim, A.P., Hearst, N., Cordás, T.A., Tavares, H., de Abreu, C.N.: Evaluation
of semantic equivalence and internal consistency of a Portuguese version of the Internet
Addiction Test (IAT). Arch. Clin. Psychiatry 39, 106–110 (2012). (São Paulo)
Internet Addiction: Evaluating the Psychometric Properties of the IAT
171

15. Lai, C.-M., Mak, K.-K., Watanabe, H., Ang, R.P., Pang, J.S., Ho, R.C.: Psychometric
properties of the Internet Addiction Test in Chinese adolescents. J. Pediatr. Psychol. 38,
794–807 (2013)
16. Lee, K., Lee, H.-K., Gyeong, H., Yu, B., Song, Y.-M., Kim, D.: Reliability and validity of
the Korean version of the internet addiction test among college students. J. Korean Med. Sci.
28, 763–768 (2013)
17. Widyanto, L., McMurran, M.: The psychometric properties of the Internet Addiction Test.
CyberPsychol. Behav. 7, 443–450 (2004)
18. Hawi, N.S.: Arabic validation of the internet addiction test. Cyberpsychol. Behav. Soc.
Networking 16, 200–204 (2013)
19. Khazaal, Y., Billieux, J., Thorens, G., Khan, R., Louati, Y., Scarlatti, E., Theintz, F.,
Lederrey, J., Van Der Linden, M., Zullino, D.: French validation of the internet addiction
test. CyberPsychol. Behav. 11, 703–706 (2008)
20. Panayides, P., Walker, M.J.: Evaluation of the psychometric properties of the internet
Addiction Test (IAT) in a sample of Cypriot high school students: the Rasch measurement
perspective. Eur. J. Psychol. 8, 327–351 (2012)
21. Pawlikowski, M., Altstötter-Gleich, C., Brand, M.: Validation and psychometric properties
of a short version of Young’s Internet Addiction Test. Comput. Hum. Behav. 29, 1212–1223
(2013)
22. Young, K.S.: Psychology of computer use: XL. Addictive use of the internet: a case that
breaks the stereotype. Psychol. R. 79, 899–902 (1996)
23. Guan, N.C., Isa, S.M., Hashim, A.H., Pillai, S.K., Singh, M.K.H.: Validity of the malay
version of the Internet Addiction Test a study on a group of medical students in Malaysia.
Asia Pac. J. Public Health 27, NP2210–NP2219 (2015)
24. Ferraro, G., Caci, B., D’amico, A., Blasi, M.D.: Internet addiction disorder: an Italian study.
CyberPsychol. Behav. 10, 170–175 (2006)
25. Widyanto, L., Grifﬁths, M.D., Brunsden, V.: A psychometric comparison of the Internet
Addiction Test, the Internet-Related Problem Scale, and self-diagnosis. Cyberpsychol.
Behav. Soc. Networking 14, 141–149 (2011)
26. Jelenchick, L.A., Becker, T., Moreno, M.A.: Assessing the psychometric properties of the
Internet Addiction Test (IAT) in US college students. Psychiatry Res. 196, 296–301 (2012)
27. Kesici, S., Sahin, I.: Turkish adaptation study of Internet Addiction Scale. Cyberpsychol.
Behav. Soc. Networking. 13, 185–189 (2010)
172
M. Mihajlov and A. Stojmenski
www.ebook3000.com

Health Care Domain Mobile Reminder
for Taking Prescribed Medications
Eleonora Milić(&), Dragan Janković, and Aleksandar Milenković
Faculty of Electronic Engineering, University of Niš, Niš, Serbia
eleonora.milic@elfak.rs, {dragan.jankovic,
aleksandar.milenkovic}@elfak.ni.ac.rs
Abstract. Nowadays, majority of people as a main problem for their poor health
and bad psycho-physical condition states the lack of time. Commitments, dynamic
and stressful way of life lead to people being negligent of themselves. Thus, their
health condition is damaged. The big problem are acute patients, but also the
patients who have a chronic disease and who should take prescribed medications
regularly, and who, for some reason, are prevented or have forgotten to take the
prescribed dosage of the medicaments. Due to the inadequate taking of the ther-
apy, the time for patient’s recovery is signiﬁcantly prolonged or the existing
problem is not relieved (with chronic patients). PersonalMedicationReminder is
an Android application that downloads the prescriptions from the server of a health
care institution or allows the user to insert the over-the-counter drugs (without
prescription therapy). The application allows patients to create reminders and
receive notiﬁcations which would inform them about the time for the next
receiving of the therapy. The application downloads the prescriptions from the
electronic medical record of the patient from the medical information system
MEDIS.NET which is used in health institutions for primary health care in
Republic of Serbia [1, 2]. The problems that occurred during the application
development are also shown in the paper. In the conclusion are stated the direc-
tions of the further research and improvement of the mobile application.
Keywords: mHealth  Personal medical reminder  Health care  Medicine
reminder
1
Introduction
Today in the domain of the health care of patients the biggest mistakes happen because
the obligations and the everyday life lead people to the lack of care for themselves and
they do not follow the doctors’ orders. Patients are people of a different proﬁle. No
matter if they are students, businessmen, programmers, housewives, retirees or chil-
dren, they all can be equally careless about their health. Life, as is lead today, is full of
responsibilities and stress, which makes people prone to a variety of diseases. In desire
to maintain health using modern technologies we are getting a chance to, by relying on
our smartphones, stay healthy and in good shape.
In health care the important problem is that the patients often forget to take the right
amount of medicines in the right time. It is important to mention that, for example, taking
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_17

antibiotics is time dependent since if the medicine is not taken in the right time, its effect is
annulled. The situation is similar with some other medications. In order not to depend on
other people to remind them about taking the medicines, patients need to adopt taking the
medicines as a part of their routine, while not allowing anything to distract them or to
make them forget about the medicines. A certain percentage of people thus use tech-
nologies like the reminder, most frequently the alarm on their smartphones.
As a solution of the mentioned problem is imposed the idea of the Android appli-
cation which goal would be to remind the patient to take the right medicine and the right
amount, as well as to warn them that the pills or capsules are running out and that the
patient should visit the chosen doctor and get the new prescription. The prescriptions
must be valid and taken from the medical information system of the health institution
where the user is being treated. MEDIS.NET is a medical information system developed
for the needs of the institutions of primary health care. It is used in more than 25
institutions of that type in the Republic of Serbia [1, 2]. PersonalMedicationReminder
application downloads prescriptions for the certain patient from the electronic medical
record from MEDIS.NET medical information system. Patients should be enabled to,
besides getting the predeﬁned values, customize on their own the number of tablets per
box, the period of taking the medicine, recommended dose (for certain medications that
are taken as needed or tablets such as vitamins or diet supplements is already made
possible to independently change the default dose or period recommended by the
doctor), as well as to set the alarm for the next medicine for all the prescriptions that the
application gets from the server. Also, for over-the-counter medicines, patient should be
enabled to input all the important data for the medicine bought at the drugstore and to set
the alarm for it. It is also necessary to make possible the permanent deletion of the
prescription at any moment if the user wants that, or to temporarily turn the reminder off
for a medicine taken as needed. In this way, the possibility of taking the wrong dose of a
certain medicine or not taking it can be brought to a minimum.
In this paper is presented the Android application in which, by incorporating the
electronic reminder with an alarm, the solution to the described problem was imple-
mented. While some applications are developed in such a way that for their use they
need to be connected to an additional hardware, for using the PersonalPillsReminder
application it is enough for the user to stick to the prescriptions and medicines with a
simple interaction with the application and minimal time spent. Patients do not have to
remember the time and the amount of the medicine that they are to consume. Instead of
them, the smart mobile application, in which the alarms are set for the time when the
right dosage of the right prescribed medicine should be taken, is taking care of that.
Authentication for the access to the application and the server where the medical record
of the patient is stored is done through the personal identiﬁcation number of the insured
(LBO) and the PIN which are stored in the database on the server. All the commu-
nication with the server is protected by using the HTTPS protocol. When the alarm
starts, the notiﬁcations are shown to the user on the display of the smartphone until the
he conﬁrms taking the prescribed dose of the medicine. The number of the remaining
tablets for each medicine is stored separately. The system is developed to provide the
simple navigation and user interface.
Throughout this work will be presented the earlier researches and conceptual
description of the solutions. The architecture and the design of the Android application
174
E. Milić et al.
www.ebook3000.com

PersonalMedicationReminder and its components were implemented using Eclipse
development environment and Restful web service [2, 3]. After the evaluation of the
proposed solution, the directions of the further researches are shown and the conclusion
was made.
2
Earlier Research
Applications that represent a kind of medical reminders, according to the functionality
they offer, are sorted into three main categories:
• SMR (Simple Medication Reminders): applications that offer basic functionality of
remembering prescriptions, notifying the user, setting the alarm, choosing the type
of notiﬁcations and accompanying sound effects, postponing the alarm, etc.
• AMR (Advance medication reminders): provides additional options like the time
zone change support, overdose protection, different description, instructions, etc.
• MMA (Medication management apps): basically AMR applications that support
multiple user accounts. They enable users to store important information, contacts
of the chosen doctors, reminders for the future checkups, etc. [4].
The usage of the applications in the ﬁeld of health care and protection is increasing
every day. Some of them need additional hardware as sensors, RFID tags or motion
detectors [5]. Many attempts to reduce the administrative mistakes in treating patients
were focused on the development of the medical dispensers. Below are given the
examples of the applications which are results of the earlier researches.
MyMediHealth application is the reminder for children. It is performed on the
smartphones and it enables conﬁguring reminders for notifying the user about the
medicine that is scheduled at that time [6, 7].
Wedjat represented an attempt of the integration of the support in the health care
and wireless computing. The application was conceptualized so that is prevents patients
from making mistakes caused by administration. Main functionalities are: reminder of
the necessary daily intake of the medicines, instructions related to the allowed intake of
the medicaments and storing the data about the consumed medicines [8].
Med Minder application is a simple and free solution envisioned as a planer appli-
cation which main ﬂaw was that it took to much manual settings thus too much time [9].
Medical Reminder and Healthcare. Android application’s goal is to remind the
patients with the ringing of the alarm to take their dose of the medicine. Through this
application they could see the contacts of the doctors and hospitals, in order to schedule
an appointment. It is possible to set the date, the time of the alarm and the instructions
for multiple medicines at the same time. Besides the alarm, the user would get the
notiﬁcation via SMS or e-mail [10].
Medication Reminder System is also an Android application for patients. It auto-
matically sets the time when the patient should take the next dose of the medicine.
Those data the application takes from the prescription of the patient and in that way
makes the possibility for mistakes that the patient can make by setting the time by
himself smaller [11].
Health Care Domain Mobile Reminder for Taking Prescribed Medications
175

Majority of the applications on the market offer reminders that are based on timers.
Some of these applications allow only one notiﬁcation for a certain medicine a day, and
do not support time intervals such as reminders every few hours or every few days.
Small number of applications has the option of postponing the alarm for certain
medications, which is bad if the user does not react immediately, because he can forget
to take the medicine. The weak point of the current systems is that the user must input
all the medications he uses alone, which takes a lot of time. There is no service that
stores the original prescriptions by medical institutions. There are no notiﬁcations for
the patient that his medicines will run out in a certain number of days and that he
should visit the doctor for the new prescription. Because of the manual input of the
prescriptions there is a big possibility for mistakes which could result in big problems
in patient’s treatment. Bigger problem present the reminders that were notifying the
user to take the medicine at a certain time, but without the doctor prescribed dose and
periods, which again can cause a signiﬁcant damage for the patient.
3
Proposed Solution
Studying the previous solutions and the needs of the users, a set of functionalities that
would in the most adequate way be made into an Android application was listed.
Application would on its own download the prescriptions from the server of the
medical institution or let the user input the medications that are bought over-the-
counter. It would make possible for the user to get the prescribed therapy (one medicine
per prescription) and to create the events through which it would notify the user that it’s
time to take the therapy.
For starting the application, it would be necessary to authenticate the patient by
inputting the LBO (personal identiﬁcation number of the insured) and a PIN (which is
provided by the doctor). It should be made possible (conﬁgurable) to memorize the
credentials in the memory of the telephone so that the users would not have to input
them every time, if they want to.
From the system of the health institution, it is needed to deliver to the mobile
application the exact date, as well as the prescriptions for the authenticated user. Each
prescription should have the diagnosis, medicine (code, name, strength, unit of mea-
sure) and the frequency of taking the medicine. Each medicine has a certain amount of
tablets/capsules per package (e.g. 20 capsules of a certain antibiotic in the package).
That number is different between different producers, so, the patient should input
the number of tablets in the package taken from the pharmacy each time they get the
new prescription. The user should be reminded that he had consumed the whole
package a few days earlier so that he would have enough time to visit the doctor for
further treatment (this option should be conﬁgurable, e.g. notify 1 day earlier, 7 days
earlier). The user would be able to change the number of the remaining tablets/capsules
in the package (lost one tablet, the package in gotten in the pharmacy contained 20
instead of 30 tablets, etc.)
The problem would be if the user does not take the medicine in the scheduled time
(e.g. the user does not have the medicine with him, does not see the notiﬁcation in time,
sees but forgets to take the medicine). Depending on the medicine, it can be taken
176
E. Milić et al.
www.ebook3000.com

immediately, or taken as scheduled. If the medicine should be taken as soon as pos-
sible, than it is necessary to change the time for the next taking of the medicine (if the
medicine is taken every 8 h, than it is necessary to take the medicine from that moment
on every 8 h). There are some medicines (e.g. vitamins) that can be taken at any time of
the day, but it is important to take them that day. Then, there are medicines that are
taken according to a scheme (e.g. 2 tablets in the morning, 1 tablet in the afternoon,
0.5 mg in the evening, and a different scheme for each day). The doctor can tell the
patient to take the medicine every six hours, but after two days to take it every 8 h if he
feels better. For that reason, the user should be given the option to change the fre-
quency of taking the medicine (the default frequency should be suggested which the
user would be able to change as needed).
The important request is that the notiﬁcation about the therapy must be delivered
always to the patient, even if there is the power or the internet connection outage. The
time of the next taking of the therapy shouldn’t be dependent on the time zone.
When the application is started for the ﬁrst time, the service for downloading the
active prescriptions would be summoned if the user chooses to turn on the reminders. It
is necessary to deﬁne the time after which the application would download the pre-
scriptions from the server by itself. For example, the application would once a day
download the status from the database of the medical information system. That option
would be conﬁgurable (e.g. every 8 h, or every 24 h, etc.). It would be good to enable
the option for the user to reload the prescriptions on request.
There should also be the option to delete the prescription from the application. The
user would be able to change at any time: the frequency of taking the medicine, number
of tablets in the package, in how many hours should he take the medicine, the total
number of tablets per package, how many days earlier should he be notiﬁed that he
needs the new prescription and to download new prescriptions not older than the
conﬁgured number of days.
The background service would take care about the time when the alarm should be
activated and for which prescription, as well as when the alarm should remind the user
when to get the new prescription. It would start automatically when starting the phone.
The data about the prescriptions that are downloaded via the service, time for taking the
next medicine, and everything else relevant for the prescription would be stored in the
phone’s memory and reached from there, as long as the application reloads the list
searching for the new prescriptions from the server.
Frequency in taking therapy can be: once a day every other day, once a week, once
a month, once a year, as needed and according to a scheme. When the prescriptions are
downloaded for the ﬁrst time, the user is immediately notiﬁed to input the number of
tablets/capsules for the new prescriptions and to activate the alarms. The user should
decide whether he would take the therapy according to the deﬁned scheme (the
determined number of hours) or he would adjust the time (input the time in hours or
minutes). The same is applicable to the amount of the medicine used.
When the alarm starts when it is time for taking the medicine on an Android
telephone a notiﬁcation would appear (sound, vibration and light). If the user ignores
the notiﬁcation it would repeat every 15 min. When the patient opens the notiﬁcation
and clicks on it, a prescription would open and the user would have two options: to
conﬁrm taking the medicine or to postpone the alarm for some time. After a certain
Health Care Domain Mobile Reminder for Taking Prescribed Medications
177

time of taking the therapy when the number of tablets would be smaller than the
conﬁgured number of days, the user would be notiﬁed to schedule a new doctor’s
appointment. In each moment the user would be able to delete each prescription
individually, to turn off the notiﬁcations for it or to delete all the prescriptions from the
list. When the prescription is marked for deletion, it will remain on the list for another
48 h if the user wants to undo the deletion, and after that it will be deleted from the
application. No matter if the prescription was downloaded from the database of the
medical information system of the health institution or the user input the data by
himself, the alarms and the notiﬁcations will function just the same.
4
Implementation and Solution Architecture
The client application was developed for Android API 3.0 and higher. The develop-
ment environment used for this purpose was Eclipse Juno 4.2 and Java programming
language.
A client application PersonalMedicationReminder, which communicates with the
server of the medical information system via Restfull web service for downloading
prescriptions, was developed for Android devices. Basic functionalities illustrated in
the example of the demo application from mHealth (abbreviation for the term used to
describe the usage of mobile devices and services for the practice of medicine and
public health) ﬁeld are:
• The service starts upon starting the phone.
• LBO and PIN can be but do not have to be stored in the phone’s memory,
depending on whether the user wants to input them every time he checks or updates
the prescriptions in the application.
• Once the LBO is input and the prescriptions are downloaded, the user cannot input a
different LBO before the previous reminders are deleted.
• All the prescriptions are remembered in the phone’s memory.
• The service performs checks for new prescriptions not older than the date set by the
user.
• Frequency of updating the prescriptions and notiﬁcations about the lack of
medicines the user conﬁgures in the application.
• The alarm and the notiﬁcations for the medicines function independently from the
Internet connection.
• The user gets a predeﬁned period for taking the medicine which can be customized.
It is necessary to input the number of tablets in the package to start the reminder for
the medicine.
• If the battery on the phone is drained, and the telephone is turned off in the moment
when the notiﬁcation about the therapy should appear, the user will be notiﬁed
about it as soon as the phone is turned back on.
• When the user clicks on the notiﬁcation the application is started, and the user can
postpone the alarm or conﬁrm that he took the medicine and set the alarm for the
next taking of the therapy.
178
E. Milić et al.
www.ebook3000.com

• The prescription can be marked for deletion and will be permanently deleted from
the list after 48 h. During that period, the user can restore the marked prescription.
• When the user is left with less than, for example, 7 days of medication supply (the
predeﬁned number of days in the application), he is notiﬁed that he needs a new
prescription. The user also gets a similar notiﬁcation when the whole package is
consumed (Fig. 1).
The Fig. 2 shows the organization and the architecture of the system. In the
architecture there is: a service that works in the background, a layer which deserializes
the data from the server, processes them and stores them in the application and a U/I
component for setting the changeable parameters of the service and for the interaction
of the user with the application.
Fig. 1. PersonalMedicalReminder-screen with prescription data
Fig. 2. The system architecture.
Health Care Domain Mobile Reminder for Taking Prescribed Medications
179

The background service starts automatically after turning the Android device on
and for the customizable parameters in the stated cases, it sets a predeﬁned value that is
stored in the telephone memory. If there is a need for the change of those parameters, it
was made possible for the user to input the wanted values, and the changes would be
stored in the memory of the device. All the values in the application are stored in the.
xml ﬁle, in the application folder. It takes care of the reminders related to each separate
prescription, activates the alarms and the reminders and sends them further to the user’s
interface.
The advanced level, i.e. the level for gathering, deserialization and storing of the
data from the prescriptions accepts the given data and classiﬁes them into predeﬁned
objects for storing the values used in the application.
The user interface helps in signing in the user for the server of the medical insti-
tution, shows the prescriptions and the notiﬁcation, but it also makes it possible for the
user to set the parameters used in the application.
Pictures of the display of the application are shown below.
One of the shortcomings which can be seen in the very beginning is that the
reminder must be manually set for each medicine. Since there is no data about the
number of tablets/capsules per package on the server of the medical institution, the user
must input that number by himself and then start the reminder and the alarm.
One more shortcoming is that the database permanently stores all the prescriptions
and does not delete them when the patient ﬁnishes the therapy. Due to this, the
application must take care about it by itself which prescriptions it would show and
which prescriptions it would consider expired. After each taking of the therapy, the
number of tablets in the prescription is automatically decreased. The moment when that
number is zero, the reminder for the medication is turned off. The prescription stays
visible until the user deletes it or until it is older than three months. After three months
the service automatically removes the prescription from the list.
5
Conclusion
Methods that are currently used for reminding patients to follow the therapy can be
demanding in terms of time and resources, and are based on either the help of the
family and friends or on paying the person who would periodically check the patient
and take care of him. The automatic planner would enable lowering the amount of
mistakes that can happen for previously stated reasons and at the same time it would
lead to the quality improvement of the care since it would decrease the amount of time
that the patient or the medical stuff would have to spend.
For the development of the application that will be used by majority of people,
Android platform is seen as the best solution. Android makes the development of the
application easy since Java gives a whole set of libraries. Of course, it is possible to use
Java on the other embedded Linux platforms, but there such a wide variety of user APIs
does not exist.
In this paper is presented a developed Android mobile application which is simple
for use and that helps patients to follow the prescribed therapies. The service is done as
a background service and it contacts the sever as often and the user wants it (parameter
180
E. Milić et al.
www.ebook3000.com

is possible to set in the mobile application). The prescriptions are stored in the tele-
phone’s memory, while taking very little memory space due to the fact that the data are
texts.
Further steps in improving the application would be adding the option for
scheduling the appointments with the chosen doctor through the mobile application
which will save a lot of time to the user. The default value of the tablets per package
could be conﬁgurable on the level of application, so that the reminder could be started
automatically without waiting for the patient to input the number of tablets for a newly
downloaded prescription. For the mobile application to be available to all the patients
for the three major smartphone operation systems versions for the iOS and Windows
Phone mobile platforms will be developed.
References
1. Medis.NET-Health Care Information System (2016). http://www.elfak.ni.ac.rs/downloads/
projekti/tehnicka-resenja/katalog/medis-net.pdf
2. Janković, D., Milenković, A., Rajković, P., Stanković, T., Marković, I., Cvetković, S.,
Vučković, D., Pešić, S.: Medicinski informacioni sistem MEDIS.NET. Laboratorija za
medicinsku informatiku, Elektronski fakultet Niš (2010). http://www.elfak.ni.ac.rs/phptest/
new/html/nauka/tehnicka_resenja/resenja/0259.html
3. Eclipse (2016). https://eclipse.org/
4. Representatial State Transfer (2016). https://en.wikipedia.org/wiki/Representational_state_
transfer
5. Stawarz, K., Cox, A.L., Blandford, A.: Don’t forget your pill! designing effective
medicationreminder apps that support users’ daily routines. In: CHI 2014 Proceedings of the
SIGCHI
Conference
on
Human
Factors
in
Computing
Systems
(2014).
ISBN
978-1-4503-2473-1
6. Motion Sensors (2016). http://developer.android.com/guide/topics/sensors/sensors_motion.
html
7. Harris, C.E., Davison, C.L., Culpepper, D.K., Scott, P., Johnson, K.B.: MyMediHealth –
designing a next generation system for child-centered medication management. J. Biomed.
Inf. 43(5), 27–31 (2011)
8. Wang, M.Y., Tsai, P.H., Liu, J.W. S., Zao, J.K.: Wedjat: a mobile phone based medication
reminder and monitor. In: 2009 Bioinformatics and BioEngineering, BIBE 2009 (2009).
ISBN 978-0-7695-3656-9
9. Med Minder (2016). https://play.google.com/store/apps/details?id=com.garland.medmind
erfree&feature
10. Ameta, D., Mudaliar, K., Patel, P.: Medication reminder and healthcare – an android
application. Int. J. Managing Public Sect. Inf. Commun. Technol. (IJMPICT) 6(2) (2015)
11. Bhadane, A., Sapna, K., Ishwari, B., Pallavi, P., Achaliya, P.N.: An android based medication
reminder system based on OCR using ANN. In: IJCA Proceedings on International
Conference on Recent Trends in Engineering and Technology 2013 (2013)
Health Care Domain Mobile Reminder for Taking Prescribed Medications
181

Enhancing Text-Based Relatedness Measures
with Semantic Web Data
Ana Gjorgjevikj(B), Riste Stojanov, and Dimitar Trajanov
Faculty of Computer Science and Engineering,
Ss. Cyril and Methodius in Skopje, Skopje, Macedonia
ana.gorgevic@gmail.com, {riste.stojanov,dimitar.trajanov}@finki.ukim.mk
Abstract. Entity relatedness measures quantify the amount of associa-
tion between two entities, such as people, places or events, and are fun-
damental part of many Natural Language Processing and Information
Retrieval applications. Calculating entity relatedness requires access to
entity speciﬁc information, so a very common practice is to use Wikipedia
or its Semantic Web representations as source of knowledge. This paper
explores which of the diﬀerent semantic relationships that associate two
entities in DBpedia are good indicators of their relatedness and could
be used to enhance some of the standard text-based relatedness mea-
sures. The ultimate goal is learning a well performing relatedness calcu-
lation method that does not require vast amount of preprocessing, but is
applicable in cases when entities lack either textual context or semantic
relationships. The KORE entity relatedness dataset was used for learn-
ing a convenient and well performing method for measuring relatedness
and its evaluation.
Keywords: Entity relatedness · Semantic relatedness · Entity ranking ·
DBpedia
1
Introduction
The rapidly growing amount of textual data, encoding knowledge in an unstruc-
tured manner, imposes an urgent need for techniques that enable its eﬃcient
analysis and exploration. Much of the research in the areas of Natural Language
Processing (NLP), Information Extraction (IE) and Information Retrieval (IR) is
focused on extracting some structure from the textual data that can lead to eas-
ier information access. With the expansion of the structured knowledge available
as part of the Semantic Web, this knowledge is frequently used for addressing the
aforementioned issues and making the textual data more machine understand-
able. Research areas such as Entity Disambiguation and Linking, semantic search
and ranking are particularly focused on leveraging the semantic knowledge in
solving NLP and IR tasks.
One of the core components of the algorithms exploiting semantic data in
NLP and IR tasks is the calculation of semantic relatedness between entities.
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 18
www.ebook3000.com

Enhancing Text-Based Relatedness Measures with Semantic Web Data
183
Semantic relatedness can be deﬁned as a measure of the amount of association
between two entities, considering any kind of relationships between them. The
terms semantic relatedness and semantic similarity frequently appear together in
literature and in some cases are even used interchangeably [1]. However, they rep-
resent diﬀerent concepts, i.e. semantic similarity is a quantiﬁcation of how much
two concepts are alike [2] and a special case of relatedness [3]. It is usually limited
to hyponymy/hyperonymy relations in a taxonomy [4], whereas two concepts can
be related in other ways than similarity, for example through meronymy or func-
tional relations. Reasoning about semantic relatedness usually requires under-
standing of the entities context and larger amount of common-sense and domain-
speciﬁc knowledge, so a simple hyponymy/hyperonymy hierarchy is insuﬃcient.
The Semantic Web [5] provides standardized representation of the human
knowledge in a computer understandable format, particularly useful for reason-
ing about semantic relatedness of two entities. In the past decade large amounts
of digital corpora have been transformed into semantic representations, increas-
ing the amount of well structured and interconnected data available. The DBpe-
dia dataset1 is one of the oldest and most widely used, extracting knowledge
from Wikipedia2 and making it available following the Semantic Web and Linked
Data principles [6]. Many research works have already shown the usefulness of
Wikipedia in NLP and IR tasks due to its common properties with the lexical
resources, as well as the unique features coming from the world and domain
speciﬁc knowledge it encodes [7]. DBpedia exhibits those properties in a more
structured manner, making them even more useful.
Even though the semantic datasets seem as a promising source of relatedness
information, only a small number of recent works [8,9] exploit them, while most
of the state-of-the-art methods are based on entities textual descriptions solely
or the Wikipedia hyperlink structure. In this paper we propose a relatedness
calculation method that is based on the data available in DBpedia and machine
learning techniques. A set of features that are based on DBpedia taxonomical
and ontological structure are deﬁned, some of which novel and not evaluated in
the previous works and some already proven as eﬀective when applied to the
Wikipedia data. Considering the fact that not all entities have well linked repre-
sentations in DBpedia, the feature set is further extended with features purely
based on the entities textual context. The relatedness function was evaluated on
the entity ranking task where set of entities is ranked based on their relatedness
to one main entity. This work focuses on Named Entities only.
2
Related Work
The early research related to measuring semantic relatedness and similarity has
been focused on general terms or concepts, and therefore most of the approaches
have exploited statistical techniques or lexical resources like WordNet [3,10,11].
The extension of the task to more speciﬁc concepts and entities that do not
1 http://wiki.dbpedia.org/.
2 https://www.wikipedia.org/.

184
A. Gjorgjevikj et al.
exist in lexical resources has imposed a need for domain-speciﬁc knowledge and
popularized the use of datasets like Wikipedia. Strube and Ponzetto [4] were
one of the ﬁrst to use Wikipedia as source of information for term relatedness
through adjustment of the known WordNet based measures to the Wikipedia
hierarchical category structure. Gabrilovich and Markovitch [12] proposed the
Explicit Semantic Analysis (ESA) approach in which terms are represented as
vectors of concepts (articles) from Wikipedia. The relevance of a concept for a
term is based on the TF-IDF measure and the term occurrence frequency across
the Wikipedia articles. Terms are compared using measures speciﬁc to the vector-
space model. Milne and Witten [13] deﬁne a relatedness measure that adjusts
the Normalized Google Distance formula to the Wikipedia hyperlink structure,
which has become one of the most frequently used relatedness measures in entity
extraction systems.
Hoﬀart et al. [14] deﬁne a set of relatedness measures based on the entities
textual context (single-word keywords and multi-word keyphrases mined from
text describing the entities). The keywords and keyphrases are weighted with
regard to the entity using a variation of the TF-IDF weighting and compared
through a set of diﬀerent measures. Aggarwal and Buitelaar [15] base their work
on the ESA approach and improve its accuracy. While ESA takes into consider-
ation all mentions of a word in a Wikipedia article when calculating the TF-IDF
score, this new approach considers only the links created by humans. Ceccarelli
et al. [16] present a method for learning to rank entities based on their related-
ness to another entity. Through deﬁnition of a set of 27 features, mainly based
on the Wikipedia link structure, they model the relatedness calculation problem
as a ranking problem. Schuhmacher and Ponzetto [8] calculate entity relatedness
using the semantic relations from DBpedia in two ways, ﬁrst by calculating the
cheapest paths between the two entities in a sub-graph derived from DBpedia,
with previously learned link weights, and through comparison of the weighted
semantic graph representations of the entities abstracts using the Graph Edit
Distance formula. Hulpu¸s et al. [9] present a graph-based relatedness measure as
well, where graph edges are assigned a weight called exclusivity and the relat-
edness is calculated as a sum of the paths with highest weights between the
entities.
The approach presented in this paper measures the relatedness between
Named Entities through a combination of text-based and structure-based fea-
tures, i.e. takes into consideration both the entities textual descriptions and
their semantic relations. The approach is similar to the one presented in [16]
and adjusts few of its best performing features to DBpedia, while adding new
ones speciﬁc to the DBpedia semantics. The entity relatedness model is diﬀerent
than the one in [16], where the authors try to directly learn an entity ranking
model through learning to rank algorithms, while we try to learn a model that
predicts a continuous-valued relatedness. None of the other related approaches
uses machine learning to model the entity relatedness and only few make use of
semantically structured data [8,9].
www.ebook3000.com

Enhancing Text-Based Relatedness Measures with Semantic Web Data
185
3
Entity Relatedness Calculation
This section describes the methodology used to learn a non-linear function for
calculation of relatedness between two Named Entities from a set of features. It
starts with a general overview of the data and feature sets, while ending with
description of the machine learning algorithm used to learn the function.
3.1
Data Preparation
From the few cross-domain knowledge bases available as part of the Seman-
tic Web (e.g. DBpedia, Yago,3 Freebase4) DBpedia was selected for the pur-
pose of this work. From the English version of DBpedia 20145 the extended
abstracts of the DBpedia resources, corresponding to the ﬁrst paragraph of the
Wikipedia article, were used as source of entities textual context. The article cat-
egories dataset, associating the DBpedia resources to the categories they belong
to through the dct:subject6 relation, and the SKOS categories dataset, which
contains information about the hierarchical and associative relations between
the Wikipedia categories represented using the SKOS vocabulary,7 were used as
sources of relatedness information, together with the properties of the DBpedia
resources and the DBpedia ontology. The datasets were preprocessed for removal
of the unneeded data that may lead to wrong results (e.g. administrative cate-
gories) and the preprocessed datasets were imported in a local instance of the
Openlink Virtuoso server, making them available for use by the application mod-
ules.
3.2
Feature Selection
For the purpose of relatedness calculation between two entities, the entity pairs
were described through a set of features that quantify the amount of their asso-
ciation from diﬀerent aspects. We started with a set of 16 features, summarized
in Table 1, out of which 9 most informative ones were selected. An attempt
was made to make distinction between features that are more likely to indicate
semantic similarity and those more likely to indicate semantic relatedness, con-
sidering the similarity as a special case of relatedness. In the rest of this section
the features are described into more details.
There are only two features that use the entities textual descriptions, rep-
resented through their extended abstracts in DBpedia, while all the other are
structure-based. The text-based features (1 and 2 in Table 1) rely on well-known
string comparison measures, the Jaccard similarity coeﬃcient and the TF-IDF
based cosine similarity. The Jaccard similarity coeﬃcient is used for measuring
3 http://www.mpi-inf.mpg.de/yago/.
4 https://www.freebase.com/.
5 http://wiki.dbpedia.org/services-resources/datasets/dbpedia-data-set-2014.
6 http://purl.org/dc/terms/subject.
7 http://www.w3.org/2004/02/skos/.

186
A. Gjorgjevikj et al.
Table 1. Summary of the features considered for entity relatedness calculation
Feature
Description
1
Jabstr(e1, e2)
Jabstr(e1, e2) =
|tokens(e1)  tokens(e2)|
|tokens(e1)  tokens(e2)|
2
cosabstr(e1, e2)
Cosine similarity between entities extended
abstracts
3
|catl=1(e1)  catl=1(e2)|
Number of common categories on level one
4
|catl=2(e1)  catl=2(e2)|
Number of common categories on level two
5
|cls(e1)  cls(e2)|
Number of common ontology classes
6
|p(ci, cj)|, ci ∈cat(e1), cj ∈
cat(e2), len(p) = 1
Number of skos:related links between the entities
categories
7
|p(ti, tj)|, ti ∈cls(e1), tj ∈
cls(e2), len(p) = 1
Number of object properties deﬁned between the
DBpedia ontology classes which the entities
belong to
8
|p(e1, e2)|, len(p) = 1
Number of paths with length one between the
entities
9
|p(e1, e2)|, len(p) = 2
Number of paths with length two between the
entities (e1 →x →e2 and e1 ←x ←e2)
10 |inlinks(e1)  inlinks(e2)|
Number of entities to which both e1 and e2 point
(e1 ←x →e2)
11 |outlinks(e1)  outlinks(e2)|
Number of entities pointing to both e1 and e2
(e1 →x ←e2)
12 P(e)
Probability of link to entity e equal to the
number of entities linking to e divided by total
number of entities in DBpedia (4,584,616 in the
English DBpedia 2014). Calculated for each of
the two entities
13 P(e1|e2)
P(e1|e2) = |inlinks(e1)  inlinks(e2)|
|inlinks(e2)|
14 Jinlinks(e1, e2)
Jaccard similarity coeﬃcient based on the
number of entities linking to e1 and e2
Jinlinks(e1, e2) = |inlinks(e1)  inlinks(e2)|
|inlinks(e1)  inlinks(e2)|
15 friend(e1, e2)
Equals to 1 if e1 has link to e2, or
friend(e1, e2) = |outlinks(e1)  inlinks(e2)|
|outlinks(e1)|
otherwise
16 avgFrnd(e1, e2)
avgFrnd(e1, e2) = friend(e1,e2)+friend(e2,e1)
2
similarity between two sets by dividing the size of their intersection by the size
of their union. The abstracts similarity was calculated on a token level, i.e. the
two texts are represented as sets of tokens. For calculation of the cosine simi-
larity, the abstracts were represented as vectors of tokens in an n-dimensional
vector space, where n is the total number of tokens in the texts. The value of
each dimension is the TF-IDF score of the token for the speciﬁc text. Since it is
a corpus-based measure, when calculating it we took into consideration all the
www.ebook3000.com

Enhancing Text-Based Relatedness Measures with Semantic Web Data
187
entities that we wanted to rank with regard to the main entity. Although called
similarity measures, these two measures are actually based on the shared words
between the texts which can indicate relatedness as well.
Features 3–5 can be considered as similarity indicators since they are based on
taxonomical structures. The ﬁrst two are deﬁned with regard to the DBpedia cat-
egory taxonomy, considering the membership of the entities in categories through
dct:subject relation and the skos:broader relation between the categories. The
third one considers the membership of the entities in ontology classes through
the rdf:type8 relation. Only the membership into the most speciﬁc classes of
the ontology was considered, since the classes at the higher levels can be too
generic. Two additional features (6 and 7) capture the number of skos:related
links between the categories which the two entities belong to and the object
properties deﬁned between the DBpedia ontology classes of the entities (object
properties that have them as rdfs:domain9 or rdfs:range10). Although all the
properties deﬁned between two ontology classes might not be applicable to all
their instances, this feature can still contain some relatedness information useful
in cases when relations between the instances are deﬁcient. Features 8–11 are
based on the paths (object properties) between the two entities. The ﬁrst one
counts the paths with length equal to one between the two entities, while the sec-
ond one counts the paths with length two, starting from one of the entities and
ending into the other. Two features that consider cases in which the two entities
point to a common entity or a common entity points to them both were deﬁned
as well. Five additional features (12–16) were taken from [16] and adjusted to
work with the object properties in DBpedia. In the original work, the authors
calculate their values using the Wikipedia hyperlink structure, i.e. the hyperlinks
between the appropriate articles. In this work the object properties between the
DBpedia entities were used.
The initial set of features was subjected to a feature selection process that
eliminated those that have little value for relatedness calculation. The usefulness
of each feature was evaluated on a subset of an existing dataset designed specif-
ically for evaluation of entity relatedness and ranking functions, i.e. the KORE
dataset [14]. The dataset, consisting of 420 Named Entity pairs, was divided into
two subsets, one used as a training set for learning the relatedness function and
one as a test set. The KORE dataset consists of 21 seed entities for which an addi-
tional 20 candidate entities linked from the seeds’ Wikipedia articles have been
selected and ranked manually. Since the candidate entity ranking with respect
to the seed entity is a key aspect of the evaluation process, during the creation
of the training and test subsets the 420 entity pairs were not divided randomly,
but the separation was done on the seed entity level. Out of the 21 seed entities,
6 entities were randomly chosen for the training subset and the other 15 were
left as a test subset. All 20 candidate entities for the chosen 6 seed entities were
included in the training subset, resulting with 120 entity pairs, and the candidate
8 http://www.w3.org/1999/02/22-rdf-syntax-ns#type.
9 http://www.w3.org/2000/01/rdf-schema#domain.
10 http://www.w3.org/2000/01/rdf-schema#range.

188
A. Gjorgjevikj et al.
entities of the remaining 15 seed entities were included in the test subset. The
training subset contains around 30% of the whole KORE dataset, whereas the
test subset around 70%. For better validation, the same process was repeated
ten times, resulting into ten diﬀerent combinations of training and test entities.
The nine features that were selected through the feature selection process are
the abstracts Jaccard similarity (feature 1 from Table 1), the abstracts cosine
similarity (feature 2), the number of paths with length one (feature 8), the prob-
abilities P(e1) and P(e2) (feature 12), the conditional probabilities P(e1|e2) and
P(e2|e1) (feature 13), Jaccard coeﬃcient based on inlinks (feature 14) and the
average friendship (feature 16). Further details are presented in Sect. 4.2.
3.3
Learning a Relatedness Function
After the feature selection process, the next phase was learning a relatedness
function that outputs a continuous relatedness value for an entity pair. Since our
training subset is relatively limited in size, more complex prediction models such
as neural networks were considered to achieve better results. Through experi-
ments with diﬀerent conﬁgurations of feedforward neural networks, a neural
network consisting of four layers, i.e. one input layer with one input neuron for
each feature, two hidden layers, each with four neurons and an output layer with
one neuron was designed. The sigmoid function is used as activation function of
the neurons in the hidden layers, whereas the output node is linear. The training
of the neural networks was done using the machine learning software Weka [17].
The trained networks were evaluated the on the appropriate test subsets and
the achieved performance is presented into details in the next section.
4
Evaluation
This section describes the evaluation of the learned relatedness calculation model
and provides analysis of the results achieved in the diﬀerent phases.
4.1
Evaluation Dataset and Methodology
For the purpose of learning and evaluating the relatedness calculation model,
few available datasets were considered, out of which the KORE entity related-
ness dataset was selected as the most appropriate. The other considered datasets
either do not make clear distinction between the concepts of similarity and relat-
edness or deal with general terms and not with Named Entities. The KORE
entity relatedness dataset is speciﬁcally designed for evaluation of Named Entity
relatedness metrics. It consists of entities belonging to four domains - IT com-
panies, Hollywood celebrities, video games and television series and in contrast
to the other available datasets that specify numeric scores to the term pairs, the
KORE dataset assigns the entity pairs relative ranking judgments [14]. Since
the KORE dataset provides only the names of the entities and not an URI,
these names were mapped to the appropriate DBpedia entities. As described
www.ebook3000.com

Enhancing Text-Based Relatedness Measures with Semantic Web Data
189
in Sect. 3, 30% of the data or 6 randomly chosen seed entities and their candi-
date entities were used to learn a function for relatedness calculation, and the
remaining 15 seed entities were used for evaluation. For each of the 15 seed enti-
ties and their candidates, a relatedness score was calculated using the learned
relatedness function and these scores were used to produce a relative ranking
of the 20 candidates. This ranking was compared with the gold standard using
the Spearman’s rank correlation coeﬃcient ρ. This coeﬃcient is used to test the
association between two ranked variables and has a value between −1 and 1,
indicating negative, no or positive correlation. This coeﬃcient was calculated
separately for each seed entity and then averaged for the whole test dataset. As
described in Sect. 3, ten random training/test datasets were created and used to
determine if there is any signiﬁcant deviation between the achieved ρ coeﬃcient
with the learned relatedness functions.
4.2
Evaluation Results
In the feature selection phase highest correlation coeﬃcient ρ was achieved by
the abstracts cosine similarity feature (0.573), the Jaccard similarity (0.568) and
the number of paths with length one (0.554). Based on these scores, nine features
were selected as input for the neural network. The maximum Spearman’s rank
correlation coeﬃcient achieved in the 10 repetitions of the relatedness function
learning and evaluation process was 0.725, with a low standard deviation between
the repetitions equal to 0.014.
For the purpose of further evaluation, an additional set of 40 named entities
from diﬀerent domains in Wikipedia (companies, places, scientists, actors, nov-
els, television series and etc.) was created. The entities (Wikipedia articles) that
were linked from the Wikipedia articles of the main entities were extracted as
candidate entities and ranked with regard to the main entity using the learned
relatedness functions. The produced relatedness scores and rankings were then
manually evaluated and few examples are shown in Table 2. It contains three
main entities - Oracle Corporation,11 Tim Berners-Lee12 and Mont Blanc,13
together with 15 candidate entities ranked in descending order by their relat-
edness to the main entity. The presented rankings were produced with the best
performing relatedness function.
Below we summarize the results achieved by the related work on the KORE
dataset, mentioning only the best results reported in the referenced papers. The
best performing methods on the KORE dataset are the KPCS method presented
in [14] which is a variation of the base KORE method and performs comparison
of the entities textual context (ρ = 0.698), as well as the methods presented
in [15], which compare entities representations in a vector space derived from
Wikipedia (ρ = 0.781). The methods based on structured data contained in
semantic knowledge bases have slightly worse results than the previous meth-
ods, with ρ equal to 0.624 [8] and 0.63 [9]. The relatedness calculation method
11 https://en.wikipedia.org/wiki/Oracle Corporation.
12 https://en.wikipedia.org/wiki/Tim Berners-Lee.
13 https://en.wikipedia.org/wiki/Mont Blanc.

190
A. Gjorgjevikj et al.
Table 2. Example entities from the extended evaluation dataset
Oracle Corporation
Tim Berners-Lee
Mont Blanc
1
Oracle Applications
W3C
Mont Blanc massif
2
Safra A. Catz
World Wide Web
Haute-Savoie
3
Oracle Enterprise Manager
Web Foundation
Italy
4
Jeﬀrey O. Henley
Robert Cailliau
Graian Alps
5
Oracle Fusion Middleware
Semantic Web
France
6
Ed Oates
Royal Academy of Engineering
Aosta Valley
7
Mark Hurd
Douglas Engelbart
Jacques Balmat
8
Bob Miner
Nigel Shadbolt
Courmayeur
9
Oracle Database
Royal Society
Aiguille du Midi
10
Larry Ellison
Order of the British Empire
Chamonix
11
Sun Microsystems
Order of Merit
Michel-Gabriel Paccard
12
Redwood Shores, California
MIT
Mount Everest
13
Java(programming language)
Plessey
Mont Blanc Tunnel
14
MySQL
University of Southampton
Switzerland
15
Oracle Application Express
England
Mont Blanc du Tacul
presented in this paper achieves satisfactory results, while being relatively sim-
ple and only requiring access to the DBpedia data for the entities of interest.
There is no need for considering a broader set of data nor extensive preprocessing
of the whole Wikipedia/DBpedia dataset. The approach can be easily extended
with entities textual data from outside DBpedia or relations from other datasets.
The main contribution of this paper is the demonstration that through use of
more complex machine learning algorithms good entity ranking results can be
achieved even with simple features. It proves the usefulness of structural data
available in DBpedia for enhancement of the relatedness measures based on text
comparison.
5
Conclusion
This paper presents a method for measuring relatedness of Named Entities exist-
ing in a semantic knowledge base, learned from a set of simple features. The
semantic relatedness measures are crucial part of more complex tasks like Named
Entity linking or document retrieval, together with other computationally-
intensive algorithms, which was the reason why simplicity was one of the main
goals of this work. The presented approach uses the data available in DBpedia to
enhance the well-known text-based relatedness measures. Through combination
of simple text and structure-based features, satisfactory results were achieved,
proving the usefulness of the semantic data for relatedness calculation, while
addressing the potential problem with lack of semantic relations for certain enti-
ties. The approach does not require any signiﬁcant preprocessing of the data
coming from DBpedia nor the entities textual context. Considering the limited
www.ebook3000.com

Enhancing Text-Based Relatedness Measures with Semantic Web Data
191
size and domain coverage of the KORE dataset, further evaluations on more
extensive datasets are planned, as well as utilization of more complex machine
learning techniques, which makes this work a base for a more extensive future
research on eﬀective ways to exploit Semantic Web data for relatedness calcula-
tion.
References
1. Budanitsky, A., Hirst, G.: Evaluating wordnet-based measures of lexical semantic
relatedness. Comput. Linguist. 32(1), 13–47 (2006)
2. Pedersen, T., Patwardhan, S., Michelizzi, J.: Wordnet:: Similarity: measuring the
relatedness of concepts. In: Demonstration papers at HLT-NAACL 2004, pp. 38–
41. Association for Computational Linguistics (2004)
3. Resnik, P.: Using information content to evaluate semantic similarity in a taxon-
omy. arXiv preprint cmp-lg/9511007 (1995)
4. Strube, M., Ponzetto, S.P.: WikiRelate! computing semantic relatedness using
Wikipedia. In: AAAI, vol. 6, pp. 1419–1424 (2006)
5. Berners-Lee, T., Hendler, J., Lassila, O., et al.: The semantic web. Sci. Am. 284(5),
28–37 (2001)
6. Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes, P.N.,
Hellmann, S., Morsey, M., van Kleef, P., Auer, S., et al.: DBpedia-a large-scale,
multilingual knowledge base extracted from Wikipedia. Semant. Web J. 5, 1–29
(2014)
7. Medelyan, O., Milne, D., Legg, C., Witten, I.H.: Mining meaning from Wikipedia.
Int. J. Hum.-Comput. Stud. 67(9), 716–754 (2009)
8. Schuhmacher, M., Ponzetto, S.P.: Knowledge-based graph document modeling. In:
Proceedings of the 7th ACM International Conference on Web Search and Data
Mining, pp. 543–552. ACM (2014)
9. Hulpu¸s, I., Prangnawarat, N., Hayes, C.: Path-based semantic relatedness on linked
data and its use to word and entity disambiguation. In: The Semantic Web–ISWC
2015, pp. 442–457. Springer (2015)
10. Wu, Z., Palmer, M.: Verbs semantics and lexical selection. In: Proceedings of the
32nd Annual Meeting on Association for Computational Linguistics, pp. 133–138.
Association for Computational Linguistics (1994)
11. Leacock, C., Chodorow, M.: Combining local context and Wordnet similarity for
word sense identiﬁcation. In: WordNet: An Electronic Lexical Database, vol. 49,
no. 2, pp. 265–283 (1998)
12. Gabrilovich, E., Markovitch, S.: Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. IJCAI, vol. 7, pp. 1606–1611 (2007)
13. Witten, I., Milne, D.: An eﬀective, low-cost measure of semantic relatedness
obtained from Wikipedia links. In: Proceeding of AAAI Workshop on Wikipedia
and Artiﬁcial Intelligence: An Evolving Synergy, Chicago, USA, pp. 25–30. AAAI
Press (2008)
14. Hoﬀart, J., Seufert, S., Nguyen, D.B., Theobald, M., Weikum, G.: KORE:
keyphrase overlap relatedness for entity disambiguation. In: Proceedings of the
21st ACM International Conference on Information and Knowledge Management,
pp. 545–554. ACM (2012)
15. Aggarwal, N., Buitelaar, P.: Wikipedia-based distributional semantics for entity
relatedness. In: 2014 AAAI Fall Symposium Series (2014)

192
A. Gjorgjevikj et al.
16. Ceccarelli, D., Lucchese, C., Orlando, S., Perego, R., Trani, S.: Learning related-
ness measures for entity linking. In: Proceedings of the 22nd ACM International
Conference on Conference on Information & Knowledge Management, pp. 139–148.
ACM (2013)
17. Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: The
WEKA data mining software: an update. ACM SIGKDD Explor. Newsl. 11(1),
10–18 (2009)
www.ebook3000.com

Power Consumption Analysis of Application
Layer Protocols for the Internet of Things
Aleksandar Velinov(&) and Aleksandra Mileva
Faculty of Computer Sciences, Shtip, Republic of Macedonia
aleksandar.210106@student.ugd.edu.mk,
aleksandra.mileva@ugd.edu.mk
Abstract. In this paper, we present power consumption analysis of application
layer protocols CoAP, MQTT and XMPP for the Internet of Things. With this
modern concept of the future will be connected all devices which can be con-
nected. Sensors, home appliances, vehicles, mobile devices are just some of the
physical objects that will be affected. Here especially may be mentioned the
sensory devices. These are devices used to detect events and changes in the
environment by generating appropriate output. Many of them are with limited
performances, memory and battery. They are often placed in inaccessible areas.
Therefore, the power consumption is very important, and which of the protocols
used for the Internet of Things provide greater energy savings. According to the
test results, MQTT and CoAP provide major energy savings, unlike XMPP
which consumes more power. For all protocols, the most energy is spent in a
state RX, while the least is spent in a state LPM.
Keywords: Internet of Things  CoAP  MQTT  XMPP
1
Introduction
The term Internet of Things (IoT) was ﬁrst introduced by British entrepreneur Kevin
Ashton back in 1999. With the discovery, industrial IoT passed a long way into the
past, but it did not have the form and momentum currently present in the IoT space [1].
Internet of Things is a computer concept that represent the connection of various
types of devices. Using the Internet and modern communication networks they can
identify and exchange data with each other. IoT predicts that thousands of end devices
with sensing, actuating, processing and communication capabilities will be able to
connect to the Internet. Unlike the past, when mostly mobile technologies were
globally connected via the internet, now most types of electronic equipment can be
connected. This trend is expected to accelerate in the coming years due to the reduction
in hardware costs as well as the maturity of Internet technology [2]. These devices can
be connected directly using mobile technologies such as 2G, 3G, LTE (4G), and the
latest 5G. The connection can be established by using different types of wireless
technologies such as: Zigbee (based on IEEE 802.15.4 standard), Wi-Fi (based on
IEEE 802.11 standard), 6LowPAN over Zigbee (IPv6 over Low Power Personal Area
Networks), or Bluetooth (based on IEEE 802.15.1) [3].
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_19

According to analysts there will be nearly 26 billion devices on the IoT by 2020.
Cisco estimates that 50 billion devices and objects will be connected to the internet by
2020. Other vendors have slightly different predictions. Irrespective of these differ-
ences, it is very clear that the number of devices/things connected would be in the range
of billions, not millions [1].
IoT has standardized suite of protocols at every layer of the protocol stack (Fig. 1).
IEEE 802.15.4 protocol operates at the network/link layer. IPv6/6LoWPAN operates at
the Internet layer. The transport layer uses UDP. At this level can also be used TCP
(Table 1). The top layer is application layer. This is the layer at which take place all
communications between devices to access other networks, the public Internet, and
ﬁnal applications. This way allows online servers to update to the latest values of end
devices but also enables the transfer of commands from applications to end devices.
Here, we use multiple protocols such as: CoAP, MQTT and XMPP. These protocols
will be the topic of our research.
After Introduction Section, in Sect. 2 we will present the scenario for measuring the
power consumption of application layer protocols for the Internet of Things. Here, we
will provide an overview of the protocols and their architecture. We will introduce the
operating system for IoT, and the tools that come with it. The main results are placed in
the Sect. 3, where a brief overview of the analysis on various protocols is given,
followed by discussions and ﬁnal conclusions.
Fig. 1. Standardized protocols for the Internet of Things
Table 1. Differences between the application layer protocols for the Internet of Things
Protocol Transport QoS options Architecture
Security
CoAP
UDP
Yes
Request/response
DTLS
MQTT
TCP
Yes
Publish/subscribe
TLS/SSL
XMPP
TCP
No
Request/response, publish/subscribe TLS/SSL
194
A. Velinov and A. Mileva
www.ebook3000.com

1.1
Previous Work
To our knowledge, there are no many papers and researches relating to power con-
sumption of application layer protocols for the IoT. Son Han used the Contiki operating
system for IoT, and tools Powertrace and Energest to assess power consumption [4].
He also gives an overview of analyzes carried out using the CM5000 wireless sensor
module and a simpliﬁed version of Powertrace [5]. Dunkels et al. gave a solution for
implementation of energy efﬁcient CoAP protocol [6]. They used more sensor modules
in their research, but they did not elaborate the energy consumptions in different time
intervals at different stages on module.
2
Scenario
We will perform measurements for three IoT application layer protocols: CoAP,
MQTT and XMPP. These protocols have different architecture, and if we use in
practice, we have to know it.
The Constrained Application Protocol (CoAP) is a specialized web transfer pro-
tocol for use with constrained nodes and constrained networks. The nodes often have
8-bit microcontrollers with small amounts of RAM and ROM, while constrained
networks such as IPv6 over Low-Power Wireless Personal Area Networks often have
high packet error rates and a typical throughput of 10 s of kbit/s. CoAP provides a
request/response interaction model between application endpoints, supports built-in
discovery of services and resources, and includes key concepts of the Web such as
URIs and Internet media types. CoAP is designed to easily interface with HTTP for
integration with the Web while meeting specialized requirements such as multicast
support, very low overhead, and simplicity for constrained environments [7].
Message Queue Telemetry Transport (MQTT) protocol was discovered by IBM
and targets lightweight Machine to Machine (M2M) communications. It is an asyn-
chronous publish/subscribe protocol that runs on top of the TCP stack. This type of
protocol meet better the IoT requirements then request/response since clients do not
have to request updates thus, the network bandwidth is decreasing and the need for
using computational resources is dropping. In MQTT there is a Broker (server) that
contains topics. A client can be a publisher that sends information to the broker at a
speciﬁc topic or subscriber that receives automatic messages every time there is a new
update of the topic [3].
Extensible Messaging and Presence Protocol (XMPP) is a protocol that is based on
XML (Extensible Markup Language) and intended for instant messaging and online
presence detection. It functions between or among servers, and facilitates near-real-time
operation. The protocol may allow Internet users to send instant messages to anyone
else on the Internet, regardless of differences in operating systems and browsers. In our
examples we will use Contiki OS [8]. It is operating system for IoT. The distribution of
Contiki known as Instant Contiki is Ubuntu Linux virtual machine that runs in the
VMWare Player and contains all the tools, compilers and simulators needed to develop
applications for IoT. We will use the latest version of Instant Contiki 3.0 which offers
Power Consumption Analysis of Application Layer Protocols for the IoT
195

improvements over its older versions. One of the improvements in the latest version is
implementation of the MQTT protocol.
In our research for power consumption we will use network simulator Cooja [12]. It
allows simulation on small and large networks of Contiki sensor modules. The modules
can be emulated at the hardware level. It is slower but always true examination of the
properties of the system, or the less detailed level, which is faster and allows simulation
of large networks.
For our scenario, in our Cooja simulation we will use the Z1 Zolertia wireless
sensor module (WSN) [9]. Z1 is equipped with the second generation of MSP430F
2617 low power microcontroller which owns 16-bit RISC CPU with 16 MHz clock
speed, 8 KB RAM, 92 KB ﬂash memory. In our Cooja simulation, sensor nodes have
8 MHz clock speed, and we will take that into account in the calculations of energy
consumption. This module includes the CC2420 transceiver, which is suitable for
802.15.4, and operates at 2.4 GHz with an effective data rate of 250 Kbps. For CoAP
protocol, we will use Z1 mote on the client and on the server side. We will measure
power consumption on the client and on the server side, and in cases where we have 1–
5 clients that access the server sensor module or without clients. Doing so will
determine the changes in power consumption in this case. For the other two protocols
MQTT and XMPP, we will use Z1 only on client side since the implementation of
MQTT broker and XMPP server is a web oriented. We will use RPL border router to
establish a communication. The main goal of a router is to connect one network to
another.
In the analysis of the CoAP protocol, the connection is established between client
and server. First the client creates a connection to the server and then periodically sends
data to any of the predeﬁned URL for service discovery. The selection is made ran-
domly using random function.
For the MQTT protocol, Mosquitto broker is installed on the local machine, which
we will use to connect Z1 module from Cooja. On one side we have client with
installed client program on Z1 module. To connect Z1 to the broker we have used a
RPL border router which is also installed on another Z1 module [10]. The router opens
a port to establish a connection with the broker using tunslip6. This tool is activated
through the terminal in Instant Contiki directory:/home/user/contiki-3.0/tools. Com-
munication is done by running this command (while we are still in the same directory):
sudo./tunslip6 aaaa::1/64 -L -a localhost -p 60002
60002 is a port that is open by RPL border router module in the Cooja simulation.
The program that will be used is written in the C programming language as all others
Contiki OS programs. With the execution of the program multiple messages will be
published on the topic (topic_name) of mosquitto broker. This is done using the
following function:
mqtt_publish(“topic_name”, “message”, 0, 1);
On the local machine we have client that is subscribed to all topics of the broker.
For XMPP, we will use Prosody XMPP server that is installed on the local
machine, and Pidgin client. XMPP client accessing the chat room in Pidgin and it prints
196
A. Velinov and A. Mileva
www.ebook3000.com

predeﬁned text that was previously adjusted in the program which is installed as
ﬁrmware of the wireless sensor module in Cooja:
xmpp_send_muc_msg(“contiki rocks”,
“‘room@conference.localhost’”);
Klauck developed XMPP client for Contiki OS that is optimized and supports chat
for multiple users (Multi user chat) [11].
3
Analysis of Power Consumption
By executing the application in Cooja, in the mote output we can see module power
consumption in several stages of the module as: ALL_CPU, ALL_LPM, ALL_TX and
ALL_RX. ALL_CPU is the total (high) CPU (CPU in active mode), in the form of
number of ticks. ALL_LPM is the total number of ticks in state LPM (CPU in Low
Power Mode). ALL_TX and ALL_RX are the total number of clock ticks in state TX
(Transmit) and RX (Receive), respectively. These outputs are enabled by using the tool
called Powertrace which is part of Instant Contiki. To use it we have to add this in the
program Makeﬁle:
APP += powertrace
While in the code of the program we have to add this:
#include “powertrace.h”
powertrace_start(CLOCK_SECOND * 10);
The last expression shows that Powertrace will print the results every 10 s. We can
change it depending on the needs. In our research we use 10 s as time interval.
3.1
Power Consumption Calculations
We calculated the power consumption by using the following formula:
Power consumption ¼ Energest Value  Current  Voltage
RTIMER SECOND  Runtime
ð1Þ
Energest_Value represents the difference between number of ticks of the clock
(spent in a particular state as CPU, LPM, TX or RX) between the two time intervals.
These are values obtained from Powertrace marked with the preﬁx ALL. We got a
value of Current for CPU, LPM, TX and RX from the Z1 Datasheet. The Voltage for
Z1 module is 3 V. RTIMER_SECOND is 32768. Runtime is the time interval in which
we perform measurements. In our case it is 10 s. The total time interval is 110 s, i.e.
00:00.000 to 01:50.000 in Cooja simulation.
Power Consumption Analysis of Application Layer Protocols for the IoT
197

3.2
Results and Discussions
The results obtained are presented in tables and graphs. In the graphs, time intervals are
represented on the x-coordinate, while power consumptions (in mW) are represented on
the y-coordinate.
Figure 2 shows the power consumption of CoAP client (Z1 module) at all time
intervals. Based on the average capacity of AA battery that is 2500 mAh, and the
nominal voltage of 1.5 V, we can estimate the battery life of wireless sensor module
(Z1 module). As we can see from the graph on Fig. 2, the highest consumption of
energy is approximately 1.523 mW (60 s.). The lowest power consumption is
approximately 0.8299 mW (30 s.). In other time intervals the power consumption is
similar, and it is approximately 0.8 mW. For CoAP client the average power con-
sumption of the test interval is 0.90796957 mW (Fig. 2).
The battery life of the Z1 module using CoAP protocol would be [4]:
2500mAh  1:5V  2
ð
Þ= 0:90796957mW  24h  365days
ð
Þ ¼ 0:942943917 years ð2Þ
Figures 3, 4, 5, 6, 7 and 8 show the consumption in cases when CoAP server has 1–
5 clients, or without clients. In Table 2 we can see the battery life of the module which
is used as CoAP server, in cases where we have from 1 to 5 clients, or if we do not have
clients. According to our analysis the battery life is reduced if multiple clients accessing
the server. The average power consumption of CoAP server with 1 client is approxi-
mately 0.843 mW. As we can see from the graphs, the average power consumption
increases by approximately 0.3 mW, with each added client.
Figure 9 shows the power consumption of MQTT client (Z1 module) at all time
intervals. As we can see in the graph on Fig. 9, the highest power consumption of
MQTT protocol is approximately 1.25 mW (10 s). The lowest power consumption is
approximately 0.45 mW (90 s), The average power consumption derived from our
research for MQTT protocol is 0.779582753 mW, therefore the battery life of the
module using this protocol would be approximately 1.098234126 years.
Fig. 2. Power consumption for CoAP client
198
A. Velinov and A. Mileva
www.ebook3000.com

Fig. 3. Power consumption for CoAP server that has 1 client
Fig. 4. Power consumption for CoAP server that has 2 clients
Fig. 5. Power consumption for CoAP server that has 3 clients
Power Consumption Analysis of Application Layer Protocols for the IoT
199

Fig. 6. Power consumption for CoAP server that has 4 clients
Fig. 7. Power consumption for CoAP server that has 5 clients
Fig. 8. Power consumption for CoAP server without clients
200
A. Velinov and A. Mileva
www.ebook3000.com

Figure 10 shows the power consumption of XMPP client. As we can see in the
graph on Fig. 10, the highest power consumption of XMPP protocol is approximately
56.9765 mW
(40 s),
while
the
lowest
power
consumption
is
approximately
56.5576 mW (90 s). According to our research, average power consumption is
approximately 56.73892177 mW. The battery life according to these data would be
0.015089543 years. XMPP is the largest consumer of energy in comparison with the
other two protocols on application layer for the IoT.
Table 2. Power consumption and battery life for CoAP server
Power consumption (mW) Lifetime (year)
1 client
0.84297777
1.015642896
2 clients
1.205370361
0.710291551
3 clients
1.569899757
0.545362454
4 clients
1.907529235
0.448834213
5 clients
2.245291651
0.381315444
Without clients 0.493677441
1.734258674
Fig. 9. Power consumption for MQTT client
Fig. 10. Power consumption for XMPP client
Power Consumption Analysis of Application Layer Protocols for the IoT
201

For all protocols, the most energy is spent in a state RX, while the least is spent in a
state LPM.
4
Conclusion
From the analysis of the application layer protocols for the IoT, we can conclude that
the average power consumption varies between CoAP, MQTT and XMPP protocol.
The average power consumption depends on several states: CPU, LPM, TX and RX.
According to the test results, we can conclude that CoAP and MQTT protocols provide
major power savings, which is not the case with the XMPP protocol.
References
1. Emerging open and standard protocol stack for internet of things. http://www.slideshare.net/
aniruddha.chakrabarti/mphasis-digital-pov-emerging-open-standard-protocol-stack-for-iot
2. Gehrmann, C., Selander, G., Seitz, L.: Authorization framework for the internet-of-things.
In: 2013 IEEE 14th International Symposium and Workshops, pp. 1–6, Madrid (2013)
3. Gallego, F.V., Zarate, J.A., Chatzimisios, P., Karagiannis, V.: A survey on application layer
protocols for the internet of things. Trans. IoT Cloud Comput. 3(1), 11–17 (2015)
4. Contiki, O.S.: Using powertrace and energest power proﬁle to estimate power consumption.
http://thingschat.blogspot.mk/2015/04/contiki-os-using-powertrace-and.html
5. Sample data for powertrace using CM5000 motes. https://github.com/sonhan/contiki-
sonhan/
6. Dunkels, A., Kovatsch, M., Duquennoy, S.: A low-power CoAP for Contiki. In: MASS 2011
Proceedings of 2011 IEEE 8th International Conference on Mobile Ad-Hoc and Sensor
Systems, pp. 855–860. IEEE Computer Society, Washington, DC, USA (2011)
7. Bormann, C., Hartke, K., Shelby, Z.: The constrained application protocol (CoAP), internet
engineering task force (IETF), Universitaet Bremen TZI, Germany (2014)
8. Contiki: The open source OS for the internet of things. http://www.contiki-os.org/
9. Z1 Datasheet. http://zolertia.sourceforge.net/wiki/images/e/e8/Z1_RevC_Datasheet.pdf
10. RPL border router. http://anrg.usc.edu/contiki/index.php/RPL_Border_Router
11. Kirsche, M., Klauck, R.: Chatty things - making the internet of things readily usable for the
masses with XMPP. In: Proceedings of 8th International Conference on Collaborative
Computing: Networking, Applications and Worksharing, Pittsburgh, Pennsylvania, USA,
pp. 60–69, (2012)
12. Get started with Contiki, http://www.contiki-os.org/start.html
202
A. Velinov and A. Mileva
www.ebook3000.com

Improving Medical Cases Retrieval Using
an Online Fact Database
Ivan Kitanovski(B), Katarina Trojacanec, Ivica Dimitrovski,
and Suzana Loshkovska
Faculty of Computer Science and Engineering,
University “Ss. Cyril and Methodius”, Skopje, Macedonia
{ivan.kitanovski,katarina.trojacanec,ivica.dimitrovski,
suzana.loshkovska}@finki.ukim.mk
http://www.finki.ukim.mk
Abstract. This paper presents an approach for retrieval of medical cases
using a novel query expansion method. The approach relies purely on
the text data in the medical cases. The cases are indexed with Terrier
IR search engine based on their text content including the caption of the
ﬁgure contained within them. Furthermore, in the retrieval phase there
is an input consisted of a long text query in a narrative form. The input
query is expanded by using on-line fact databases, such as Freebase, with
the aim that this will add more terms relevant to the concepts mentioned
in the text. The goal is to provide a way of query expansion, so that the
query is more deﬁned, which should provide more narrowed and precise
results in the retrieval. The retrieval is done with the BM25 weighting
model. Our approach shows that expanding the input text query in this
fashion can provide a boost in the retrieval performance.
Keywords: Medical articles retrieval · Query expansion · Pubmed arti-
cles · Terrier IR · Freebase
1
Introduction
The rapid advances in technology are in-part resulting in tools which we use
everyday and help make our lives easier. These tools depend on the data collec-
tions, which are growing constantly. Thorough research on such data is needed,
since these tools can range from simple applications to complex decision mak-
ing systems. The data that these tools use can appear in multiple forms like
text, image, web pages, video clips etc. The value that it brings depends on the
context of the situation where it is used.
Our main focus of interest in this paper is how to use the data in the medical
context. Finding the right data related to a medical question is a very complex
task, but an important one as it can help the physician with the current case
he is working on or provide valuable insight for a researcher or student. In this
sense we focus on a information retrieval system, which is a vital tool in the
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 20

204
I. Kitanovski et al.
medical context, if we take into consideration that there are 12400 diﬀerent
categories of medical conditions [1]. More precisely, we focus on retrieval of
medical articles (cases), since they contain a lot of biomedical information and
empower healthcare experts by allowing them to ﬁnd related data or explore
cases with similar symptoms or conditions [2] in medical information repositories.
Already, there are many systems which allow such retrieval services: Pubget [3],
Pubmed [4], eTBLAST [5], etc. PubMed, for instance, is a search engine created
for easier access to the Medline database [6], which contains over 22 million
articles. Currently, Pubmed (as well the other systems) work based on keyword
search on diﬀerent ﬁelds such as the main text, author, and date. But, according
to [7] a more real-life scenario would be where the queries are narrative and
where the user can explain his current condition in more details.
In this paper we propose a query expansion method for text-based retrieval
of medical articles. The method uses a medical knowledge tool to detect medical
terms in a given query and uses a generic on-line fact database to provide a
broader explanation of the detected terms. Then, the expanded terms are added
to the original query. The idea behind this approach is that on-line fact databases
maintained by a large community of users with diverse backgrounds should help
provide better retrieval results.
The rest of the paper is organized as follows: Sect. 2 holds the related work.
The prosed method for query expansion is presented in Sect. 3. The experimental
setup and evaluation measures are detailed in Sect. 4. Section 5 contains the
experimental results and discussion. The concluding remarks and planned future
work is presented in Sect. 6.
2
Related Work
Medical case retrieval is an ongoing challenge in the medical information systems
domain. There have been many attempts at tackling the issue, usually based on
proven and existing search engine platforms.
Herrera et al. [8] based their platform on the Lucene search engine. The
proposed approach took into consideration the entire article content for indexing.
The approach concatenated the title, abstract and the text content of the article
and built and index based on that. This is a standard retrieval approach and it
is important to note that this performed well only on the 2012 version of the
ImageCLEF dataset.
Vahid et al. [9] use the Terrier IR search engine as a platform on top which
they built their retrieval system. In this system, the entire text content of the
article is taken into consideration and it is preprocessed in the following man-
ner: special signs and stop words are removed, the stemming is applied. In the
retrieval stage they apply TF-IDF weighting model, which is one of the most
frequently used models in IR systems. The approach is stable and provides con-
sistent results across diﬀerent versions of the dataset.
Simpson et al. [10] use the Essie search engine as a retrieval platform. Their
approach is based on UMLS concept expansion, which is a built-in feature in
www.ebook3000.com

Improving Medical Cases Retrieval Using an Online Fact Database
205
Essie. The approach provided good results for text-based image retrieval tests,
but poor on the case-based retrieval ones.
There are many more approaches [11,12] based on existing search engines,
but Lucene and Terrier IR are the ones which are most frequently used, since
they are generic and easily accessible. One of the best results were reported with
the Terrier IR, also, it has eﬃcient search methods for large-scale document
collections. Hence, based on that and our previous work [13] in the ﬁeld, we
decided to use it in our experiments.
The other aspect of our research is related to query expansion. Query expan-
sion has proven useful in generic information retrieval systems by making queries
match to more relevant documents that might not contain the exact terms. The
eﬀect of this is more visible in short queries [14]. Typical query expansion tech-
niques [15] take into consideration statistical correlation, synonyms, the knowl-
edge of morphology of words and the use of dictionaries to improve the retrieval
results.
Voorhees [16] found that automatic query expansion using WordNet can
lead to degradation in performance, while hand picking the concepts improves
ambiguous queries.
Hersh et al. [17] found that adding MeSH terms to the query is a simple yet
eﬀective approach that leads to better retrieval performance.
An interesting text-based retrieval framework is presented in [18]. The
authors use the Indri search engine as a retrieval platform for their framework.
The retrieval phase is consisted of using unigram language model with Dirichlet
prior smoothing. The novel part of this approach in the way they apply query
expansion. The framework uses an external bibliographic source i.e. Medline.
Each query is ﬁrst executed against the Medline database and the top n doc-
uments are retrieved. Then, the MeSG terms of the retrieved documents are
added to the query and the expanded query is executed against the ImageCLEF
database. This approach is, to our knowledge, the best reported on the dataset,
but not for all versions, so we believe there is room for improvement.
The work done in query expansion is diverse and with varying results. Hence,
most of the approaches focus on expansion by adding medical terms to the query.
Hence, we focus on providing a automated way of query expansion for med-
ical contact, but with publicly available generic and domain-speciﬁc knowledge
databases.
3
Query Expansion with On-Line Fact Database
The diagram of the proposed approach is presented on Fig. 1. The approach is
separated in two stages i.e. an on-line and oﬀ-line stage.
The oﬀ-line stage deals with the indexing structure of the data. It is consisted
of preprocessing and indexing of the medical cases. Preprocessing is a procedure
where the cases have to be processed so that they are cleaned up for creating a
more eﬃcient index. First, the special signs are removed from the cases. After-
wards, all the stop words are removed. These are words which have no semantic

206
I. Kitanovski et al.
meaning, but appear frequently in the text and add unnecessary noise. Token
normalization is the next step. At this step all words are converted to lower case.
The ﬁnal step of the preprocessing is called stemming. At this stage, the words
are reduced to their root form. After the preprocessing is done a inverted index
is created using single-pass indexing.
Query expansion
Medical cases
Preprocessing 
Text query
Inverted index
Preprocessing
Final results
oﬀ-line phase
on-line phase
Retrieval
Medical Knowledge Tool
Fact Database
Fig. 1. Diagram of retrieval of medical cases with the proposed query expansion method
In the on-line stage the user provides the input in the form of a narrative text
query. First, the query is preprocessed in the same manner as the medical cases.
The next phase in the on-line stage is the query expansion. In this phase, we use
a medical knowledge tool to detect the medical terms in the query. Afterwards,
we query the detected medical terms on the fact database and use the synonyms
to expand the query. In the end, we use the expanded query to search against the
inverted index using a weighting model and provide the results. The weighting
model is a mathematical model which computes a numeric score, which gives
an estimate of how similar a given query is to a given article. The more similar
the items, the higher the number of the computed score. The score is calculated
for each query against each article and the articles are sorted in a descending
manner grouped by the query and the ﬁnal result is provided.
4
Experimental Setup and Evaluation
4.1
Dataset
To evaluate our proposed approach, we referred to the ImageCLEF 2013 [12]
data collection. It contains text and visual data. There is a speciﬁc subset in
it which is designed with the purpose to evaluate medical case-based retrieval
algorithms. It is consisted of 74 654 medical articles (cases), which are predomi-
nantly journal articles from PubMed/Medline database. The articles are stored
in a XML format presented on Fig. 2. Each article is consisted of several parts:
title, abstract, text content of the articles (referred as full-text) and text cap-
tions for the images depicted in the article. We have expanded the articles by
incorporating their MeSH heading terms from the PubMed description of the
articles.
www.ebook3000.com

Improving Medical Cases Retrieval Using an Online Fact Database
207
ArƟcle
Title
Abstract
Fulltext
Figures
Figure 1
Figure 2
Figure N
...
CapƟon
CapƟon
CapƟon
Fig. 2. Diagram of the XML structure of a medical article
The data collection also provides 35 queries which should be used to case-
based retrieval approaches. All queries are short narrative case descriptions,
each containing 2–3 image for allowing multimodal or content-based retrieval
experiments. Our primary focus is on text-based retrieval, so in our experiments
we only used the text part of the queries. A small subset of the provided queries
is presented below:
– Query 1. A 56-year-old woman with Hepatitis C, now with abdominal pain
and jaundice. Abdominal MRI shows T1 and T2 hyperintense mass in the
left lobe of the liver which is enhanced in the arterial phase.
– Query 2. A 50-year-old man with severe right ﬂank pain and hematuria.
Renal ultrasound shows a markedly echogenic lesion with a posterior acoustic
shadow measuring about 8x10mm in the right kidney.
– Query 3. A 49-year-old woman with a prolapsed mass in the opening of her
urethra. Pelvic CT shows a heterogeneously enhanced mass on the female
urethra. Pathology shows ramifying papillae, high nuclear/cytoplasmic ratio,
and brisk mitotic activity.
4.2
Retrieval Details
For the experimental part of approach we used a open source search engine plat-
form. We referred to Terrier IR [19] as a comprehensive, ﬂexible and transparent
platform for research and experimentation in text retrieval. We used Terrier IR
to preprocess, index the articles and retrieve them in the later stage. In the
preprocessing stage, we use Terrier’s built-in tokenizer for English and the stan-
dardized list of stop-words for stop-words removal. Whereas, for stemming we
used Porter stemmer [20], which is the most frequently used stemmer for English.
In the retrieval stage we needed a weighting model to perform the retrieval and
here we referred to BM25 [21] as it is reported as of the most appropriate models
for this type of retrieval problems [22]. The query expansion method consisted
of two stages: 1. medical term detection, 2. expansion using a fact database. For
medical term detection we used the Medical Text Index (MTI) which is a part of
MetaMap developed and maintained by the National Library of Medicine [23].

208
I. Kitanovski et al.
MTI accepts large pieces of text (up to 10000 words) and returns descriptions for
each term (or phrase) detected in the text. Each detect term/phrase is assigned a
score, which represents it’s importance in the text. We treated all terms/phrases
above the term“Homo sapiens” which we treated as noise, since it appears in
all queries. After the medical term detection stage, we query the detected terms
against an on-line fact database. In our experiments we used Freebase [24], which
is a collaboratively created database for structuring human knowledge. For each
query, we used it’s medical terms to query Freebase and add the Freebase syn-
onyms for the provided term to the query.
4.3
Evaluation Measures
For evaluation metrics, we used the standardized ImageCLEF evaluation metrics:
– Mean Average Precision (MAP) - the mean of the average precision scores
for each query
– Precision at ﬁrst 10 (P10) - precision of the ﬁrst (top) 10 returned articles
(cases)
– Precision at ﬁrst 30 (P30) - precision of the ﬁrst (top) 30 returned articles
(cases).
According to ImageCLEF [12] practices MAP is processed over the 1000 top
returned cases per query.
5
Results and Discussion
The goal of our research was to answer the question: Can query expansion using
on-line fact database increase the retrieval over standard retrieval?
For that purpose we created two types of experiments. The ﬁrst experiment
is the standard (baseline) retrieval, where we perform the retrieval using the
original query, without any additional modiﬁcation. The second experiment is
the retrieval using the query expansion method we proposed. The results from
our experiments are presented in Table 1.
Table 1. Results from the two experiments performed.
MAP
P10
P30
Baseline
0.2004 0.2029 0.1476
Freebase expansion 0.2179 0.2086 0.1695
The presented results show that using query expansion from on-line fact
databases can increase the retrieval performance. There is even an increase in
P10 and P30 metrics. These are key metrics as they measure the top returned
www.ebook3000.com

Improving Medical Cases Retrieval Using an Online Fact Database
209
results, which is what users interact with mostly. Overall, the reason for the
increase in performance is due to the fact that the terms used for the expansion
i.e. the freebase synonyms for the medical terms are more descriptive than the
original medical terms in the query in the sense that they are more widely
described.
6
Conclusion
In this paper we presented a novel method for query expansion in text-based
retrieval of medical articles. Our experiment showed that the presented method
provided a boost in retrieval performance. The proposed method relies on a
medical term detection tool and an on-line fact database. The key feature
is expanding with queries with terms extracted from a generic database of
knowledge. The logic behind this is that it brings more descriptive and non-
medical terms into the queries, which broadens the queries and allows for better
performance.
In the future we plan on integrating on-line medical retrieval systems, com-
bine their datasets and retrieval features into our retrieval platform. The goal is
to use standardized techniques for medical articles retrieval and boost them by
applying them with our expansion methods.
Acknowledgments. This work is partially supported by the Faculty of Computer
Science and Engineering, Skopje, Macedonia as a part of the project “Scalable Photo
Annotation”.
References
1. Dye, C., Reeder, J.C., Terry, R.F.: Research for universal health coverage. World
Health Organization (2013)
2. Mour˜ao, A., Martins, F., Magalh˜aes, J.: Multimodal medical information retrieval
with unsupervised rank fusion. Comput. Med. Imaging Graph. 39, 35–45 (2015)
3. Pubget. http://www.nlm.nih.gov/pubs/factsheets/medline.html. Accessed 9 June
2016
4. Bodenreider, O.: The uniﬁed medical language system (umls): integrating biomed-
ical terminology. Nucleic Acids Res. 32(suppl 1), D267–D270 (2004)
5. Errami, M., Wren, J.D., Hicks, J.M., Garner, H.R.: eTBLAST: a web server to
identify expert reviewers, appropriate journals and similar publications. Nucleic
Acids Res. 35(suppl 2), W12–W15 (2007)
6. Pubget. http://pubget.com. Accessed 9 May 2015
7. Peters, C.: Cross language evaluation forum. D-Lib (2000)
8. de Herrera, A.G.S., Markonis, D., Eggel, I., M¨uller, H.: The medgift group in
imageclefmed 2012. In: CLEF (Online Working Notes/Labs/Workshop) (2012)
9. Vahid, A.H., Alpkocak, A., Hamed, R.G., Caylan, N., Ozturkmenoglu, O.: Demir
at imageclefmed 2012: inter–modality and intra–modality integrated combination
retrieval. Working Notes CLEF vol. 2012 (2012)

210
I. Kitanovski et al.
10. Simpson, M.S., You, D., Rahman, M.M., Demner-Fushman, D., Antani, S.K.,
Thoma, G.R.: ITI’s participation in the 2013 medical track of imageclef. In: CLEF
(Online Working Notes/Labs/Workshop) (2013)
11. Vanegas, J.A., Caicedo, J.C., Camargo, J.E., Ramos-Poll´an, R., Gonz´alez, F.A.:
Bioingenium at imageclef 2012: textual and visual indexing for medical images. In:
CLEF (Online Working Notes/Labs/Workshop) (2012)
12. de Herrera, A.G.S., Kalpathy-Cramer, J., Demner-Fushman, D., Antani, S., M¨uller,
H.: Overview of the imageclef 2013 medical tasks. Working Notes of CLEF (2013)
13. Kitanovski, I., Dimitrovski, I., Loskovska, S.: FCSE at imageclef 2012: eval-
uating techniques for medical image retrieval. In: CLEF (Online Working
Notes/Labs/Workshop) (2012)
14. Navigli, R., Velardi, P.: An analysis of ontology-based query expansion strategies.
In: Proceedings of the 14th European Conference on Machine Learning, Workshop
on Adaptive Text Extraction and Mining, Cavtat-Dubrovnik, Croatia, pp. 42–49.
Citeseer (2003)
15. Carpineto, C., Romano, G.: A survey of automatic query expansion in information
retrieval. ACM Comput. Surv. (CSUR) 44(1), 1 (2012)
16. Voorhees, E.M.: Query expansion using lexical-semantic relations. In: SIGIR94,
pp. 61–69. Springer (1994)
17. Hersh, W., Price, S., Donohoe, L.: Assessing thesaurus-based query expansion using
the umls metathesaurus. In: Proceedings of the AMIA Symposium, p. 344. Amer-
ican Medical Informatics Association (2000)
18. Choi, S., Lee, J., Choi, J.: Snumedinfo at imageclef 2013: medical retrieval task.
In: CLEF 2013 Evaluation Labs and Workshop, Online Working Notes (2013)
19. Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, C., Lioma, C.: Terrier:
a high performance and scalable information retrieval platform. In: Proceedings of
the OSIR Workshop, pp. 18–25. Citeseer (2006)
20. Macdonald, C., Plachouras, V., He, B., Lioma, C., Ounis, I.: University of glasgow
at webclef 2005: experiments in per-ﬁeld normalisation and language speciﬁc stem-
ming. In: Accessing Multilingual Information Repositories, pp. 898–907. Springer
(2006)
21. Amati, G., Van Rijsbergen, C.J.: Probabilistic models of information retrieval
based on measuring the divergence from randomness. ACM Trans. Inf. Syst.
(TOIS) 20(4), 357–389 (2002)
22. Kitanovski, I., Dimitrovski, I., Loskovska, S.: FCSE at medical tasks of imageclef
2013. In: CLEF (Working Notes) (2013)
23. Aronson, A.R.: Eﬀective mapping of biomedical text to the UMLS metathesaurus:
the metamap program. In: Proceedings of the AMIA Symposium, p. 17. American
Medical Informatics Association (2001)
24. Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J.: Freebase: a collabo-
ratively created graph database for structuring human knowledge. In: Proceedings
of the 2008 ACM SIGMOD International Conference on Management of Data, pp.
1247–1250. ACM (2008)
www.ebook3000.com

Improving Scalability of Web Applications
by Utilizing Asynchronous I/O
Gjorgji Rankovski(&) and Ivan Chorbev
Faculty of Computer Science and Engineering,
University of Ss. Cyril and Methodius,
Rugjer Boskovikj 16, P.O. Box 393, 1000 Skopje, Republic of Macedonia
gorgi.rankovski@gmail.com, ivan.chorbev@ﬁnki.ukim.mk
Abstract. The focus of the paper is the use of asynchronous I/O calls in web
applications to improve their scalability, by increasing the number of requests
per second that it can process and decreasing the average response time of the
system. Popular development frameworks have always included only blocking
I/O APIs in their base, making asynchronous I/O methods hard to implement
and maintain. Signiﬁcant effort has been made in recent years to enrich these
frameworks with better syntax for asynchronous API to improve developers’
experience and encourage its use. Such improvement in .NET’s syntax is put to
the test in this paper and the results are presented and evaluated.
Keywords: Distributed system Web application Web service Asynchronous
programming  I/O  Load test
1
Introduction
Web applications that are publically available to wide audiences have a requirement to
handle growing amount of trafﬁc, or have the potential to be enlarged in order to adapt
to that growth. This scalability property of web applications is one of the most
important issues in modern web development. Today’s users are unpredictable in the
means of accessing web applications; from the classic approach using a web browser
on a desktop computer, through a variety of smart devices available on the market in
many shapes. Real-time communication and push notiﬁcations are another important
characteristic of this modern era where users expect to receive messages without their
action.
To adapt to these users’ needs, web developers have been in constant search for
new frameworks and paradigms that will enable development of highly-scalable sys-
tems in a safe and easy way. As a result, Node.js has emerged as a new technology for
developing web applications and was quickly accepted by developer communities and
companies. Embracing JavaScript’s event system, Node.js has made asynchronous I/O
a ﬁrst class citizen in its framework, which makes programming of highly scalable
servers both easy and safe [6].
The approach in traditional web development frameworks is based on threads.
Namely, when an incoming request reaches the web server, the main process that
handles the request spans a new thread and allocates the request to it. That thread is busy
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_21

with processing the request for its entire life cycle, and it is unable to handle a different
request in the meantime. To make things worse, traditional frameworks have syn-
chronous API for I/O operations, which means that the thread processing a HTTP
request will be blocked when an I/O access is required to process it. This inherently
makes the web application’s scalability limited to the number of threads that the web
server can create both time and resource wise. Event-based approach in Node.js is thread
agnostic and uses the natural asynchronous API of operating systems for I/O access,
making it suitable for web applications composed of distributed components [7].
Even though event systems are solving the performance problem in high concur-
rency systems very well, they do not make the thread-based servers obsolete [1]. The
focus of this paper is analysis of the transforming of a legacy web application,
developed in ASP.NET, which uses blocking I/O, into an architecture that uses
asynchronous I/O to access components it is dependent on. Results from tests made
against endpoints of the application before and after the transformations are being
presented in this paper as well. The results have shown that with threads, similar or
higher performance can be achieved. Related issues with threads are not in the para-
digm itself, but a simpler programming model was missing in the traditional frame-
works. Developers of these frameworks have been trying to come up with a simpler
model and have achieved that by close integration between the compiler and the thread
system. Speciﬁcally of interest in this paper is the async/await pair of keywords
introduced in .NET version 4.5.
2
Previous Research
The event-based approach has been utilized for asynchronous approach in I/O access in
many papers for developing complex distributed systems [1–5]. Mostly, their work is
focused on combining the thread approach and events together into a hybrid solution,
resulting in a library for an existing platform like Java or .NET. Some of the topics
covered in these papers are:
– Missing callback syntax for asynchronous events in the underlying platform [2, 4].
– New mental model needed for programmers to write chains of callbacks [2, 3].
– In [3] the authors point out that high allocation of memory and CPU time during
thread’s context switching is the main reason event-based solutions perform better.
– Library solution for improving the syntax for writing asynchronous code [4].
– A proposed mechanism for dynamic stack growth is available in [5] to avoid large
stacks for managing thread state.
A library-based solution can help improve the development with an asynchronous
I/O. However, a true effort is needed by the stakeholders of existing frameworks to
make a widely accepted solution. As mentioned in these papers, solutions to scalability
issues of thread-based servers lie in the tight integration between the compiler and the
thread system. Such tight integration is introduced in .NET 4.5 with the new pair of
keywords (async/await). Their utilization for improving scalability of a legacy system
is presented in this paper.
212
G. Rankovski and I. Chorbev
www.ebook3000.com

3
Proof of Concept
Before starting with the transformation process, a proof of concept was needed in order
to demonstrate the power of the improved syntax for asynchronous programming in .
NET, and its underlying mechanism. One of the biggest gains is in the server-side
programming frameworks, like ASP.NET, which represents a thread-based server.
When a request comes in the system, a thread from the thread pool is assigned to that
request and is responsible for generating a response to the requesting client. Before
version 4.5, all of the I/O libraries in .NET consisted of synchronous APIs, such as
reading from a ﬁle, accessing a database or other services over the network. An
asynchronous approach was used in rare cases, mostly because support was missing
from the framework and writing a wrapper around the synchronous API requires
experience, as it is considered unsafe and hard to maintain. Therefore, when access to
an I/O system is needed, a blocking approach is used, which makes the thread pro-
cessing the request to be in a blocking state, basically not doing anything, but unable to
accept different requests in the meantime. A busy web application can easily end up
without available threads in the thread pool, making incoming requests wait longer
period in the queue before processing them. This effectively increases the average
response time of the system, and the approach in this case is to scale the system
horizontally, by creating new instances of the web application on a separate server.
Operating costs are rising, even though the full potential of the system is not used.
By introducing the async/await pair of keywords, developers are encouraged to use
asynchronous code when making I/O calls. By using the asynchronous API, the thread is
not blocked when I/O call is in progress, but it is being released in the thread pool,
available to accept new requests. When the I/O call ﬁnishes, a new thread from the pool is
assigned to continue executing the request, therefore using the full potential of the system.
To conﬁrm these claims, the following web application has been put to load tests:
In Fig. 1 two methods are presented. The “Sync” function calls the Sleep method
on the current thread, which blocks the executing thread for 2 s and then returns a result
to the client. This way, synchronous API is emulated for an I/O call that lasts 2 s. The
“Async” method on the other hand, implements the new async/awaits pair of keywords
and emulates an asynchronous I/O call that lasts 2 s. It is using the Task. Delay
public string Sync()
{ 
Thread.Sleep(2000);
return "Hello, Sync!";
} 
public async Task<string> Async()
{ 
await Task.Delay(2000);
return "Hello, Async!";
} 
Fig. 1. Emulation of an I/O call for both synchronous and asynchronous approach
Improving Scalability of Web Applications by Utilizing Asynchronous I/O
213

method, which registers a timer with the operating system and releases the current
thread. The thread is free to go back to the thread pool and accept more requests, while
the operating system doesn’t use any thread for timer implementation. When the timer
ticks, a new thread from the thread pool continues the execution of the program and
returns a result to the client. The code after the Task. Delay method call is effectively an
event handler that executes on the timer tick, but the syntax is quite simpliﬁed, because
there’s no extra function or variables declared to handle the callback.
These two tests have been put to load tests in a controlled environment. The web
application hosting these methods is published on an IIS server conﬁgured to have only
10 threads available in the pool. Each method has been tested with 1 min of constant
user load with three setups – 10, 11 and 20 users making constant request to the method.
As it can be seen in Table 1, performances degrade when using blocking I/O
methods as number of users increases. The number of requests that can be executed
remains the same (4.41–4.37) after increasing requests over the thread pool limit, but
the average response time decreases signiﬁcantly. When 11 users are making simul-
taneous requests, the 11th user will have to wait for 2 s for a thread to be available to
accept their request, which means that their request will last 4 s. That effectively
increases the average response time to 2.51 s. As expected, when the number of
requests doubles the number of threads in the pool, the average response time doubles
as well, from 2.06 s to 4.58. The system doesn’t scale well in this situation.
In Table 2, results from the asynchronous methods are presented. The number of
requests that can be accepted always meets the number of requests that are going in the
application (total tests and requests/second indicators). The average response time is
always constants, no matter how many users are simultaneously accessing the system.
This means that the system scales very well with the growing number of users. Scal-
ability is not limited by the number of threads that can be created inside the thread pool,
and horizontal scale is avoided.
Table 1. Load test results against synchronous method
Indicator
10 constant users
11 constant users
20 constant users
Total tests
249
225
223
Requests/second
4.88
4.41
4.37
Average request
time (seconds)
2.06
2.51
4.58
Table 2. Load test results against asynchronous method
Indicator
10 constant users
11 constant users
20 constant users
Total tests
240
264
480
Requests/second
5
5.5
10
Average request
time (seconds)
2.01
2.01
2.01
214
G. Rankovski and I. Chorbev
www.ebook3000.com

4
Transforming a Legacy Application into a Hybrid
Approach
Results from proof of concept tests are encouraging to apply asynchronous I/O calls to
an existing web application that is built with the traditional approach of blocking I/O
methods. The web application in question is a Video on Demand solution, hosted on
Microsoft Azure cloud platform, and is already in production stage, used by real users.
The video on demand service’s overall architecture is presented on Fig. 2. The web
application is hosted on a web server and represents an entry point into the system. This
system represents a distributed system, composed from 4 independent components.
Beside the web server, an SQL Server is used for persisting relational data, a cache
server is used for storing customer’s authentication sessions and elastic search server is
used for textual search of available contents. All these components can be scaled and
tested independently of each other, and each of them has its own function in the system.
To generate a response to requests that come in the system, the web application needs
to make at least two I/O calls: one against the Redis cache, to validate user’s
authentication claims, and one against the Database, to compose data for the response.
Fig. 2. Logical overview of the system
Improving Scalability of Web Applications by Utilizing Asynchronous I/O
215

The biggest challenge during the transformation of the system was the fact that an
asynchronous code uses or it is being used by another asynchronous code. With other
words, the asynchronous code spans across the whole architecture of the web appli-
cation, from the entry point to the exit point of the system. In a way, the asynchronous
code has infectious effect, because it insists the code that’s in its immediate surrounding
be asynchronous as well. Transformation of the system has been done in steps, for each
web endpoint separately. The asynchronous and synchronous code can live together in
the same space, as long as they are not mixed in the same HTTP call.
5
Test Results
5.1
Tests Conﬁguration
To test the performances of a system, it needs to be simulated in an environment that
mirrors the environment where the system is running. The two architectures of the
system have been published to a similar environment as the live system in the
Microsoft Azure cloud infrastructure, but with scaled down performances. The point is
to test differences between the two approaches, so both of them are set up in the same
hardware environments. The tests are created to target 7 system endpoints, which
actually are 7 web services that are available publically. They have been chosen with
criteria to include multiple I/O calls and that they are one of the most used web
services.
Tests are developed using the Visual Studio Load Testing environment and they are
executed by agents in the cloud, located near the physical location of the servers. This
removes the unpredictability of the internet connection and the delay that can show up
because of long distance between the environment that executes the tests and the
environment of the simulated system. Two test scenarios are developed with the fol-
lowing conﬁguration:
1. Test scenario with a constant load of the system. This test simulates 25 users
making constant requests for a period of 1 min.
2. Test scenario with increasing number of requests by using steps. The test starts with
10 users and every 10 s, it adds 20 more users to the execution. This test also lasts
for 1 min, where the maximum number of users (100) reaches at the 50th second of
the test.
Both tests are conﬁgured to run for one minute. Beside the regular run time, there are
warm-up and cool-down periods of 3 s. During the warm-up period, results are not
included in the test results and this period is used for the application to warm up, by
establishing connections to the I/O systems. The cool-down period is used to include
results of requests initiated during the regular run time, but that have not received
response during that time. No requests are initiated during cool-down, but requests that
end in this period are included in the ﬁnal results.
Since there are 7 endpoints under tests, to avoid any randomness, tests are con-
ﬁgured such that each virtual user makes requests to the endpoints in the same order,
without think time between the requests.
216
G. Rankovski and I. Chorbev
www.ebook3000.com

5.2
Evaluation
Both tests are run against each architecture for 5 times and the results presented here
are the best results from each testing.
From the summary results under constant load (Table 3), signiﬁcant improvements
can be noticed when using asynchronous I/O calls. One of the important indicators is
the number of requests that can be executed per seconds, and with this hardware setup,
the synchronous architecture can perform 8.5 requests per second, where the asyn-
chronous architecture gives 13.5. Also, the average response time is much better,
giving 1.42 s with the asynchronous approach, and 2.35 s for the synchronous
approach. We can conclude that asynchronous I/O calls make the following
improvements:
– Increase of 60% in number of requests per second
– Decrease of 40% in average response time
In Table 4, summary results from the step increasing load test show that in this
scenario, asynchronous approach for I/O calls is again giving signiﬁcant improvements,
with 46% increase in number of requests per seconds and 44% decrease in average
response time.
Table 3. Summary results from constant load test
Indicator
Synchronous Approach
Asynchronous approach
Total tests
72
110
Tests/second
1.2
1.83
Average 
test 
cycle 
(seconds)
14.3
9.68
Average requests/second
8.5
13.4
Average 
response 
time 
(seconds)
2.35
1.42
Table 4. Summary results from step increasing load test
Indicator
Synchronous Approach
Asynchronous approach
Total tests
47
108
Tests/second
0.78
1.80
Average test cycle
(seconds)
14.8 
13.8
Average requests/second
9.2
13.5
Average response time
(seconds)
4.09
2.31
Improving Scalability of Web Applications by Utilizing Asynchronous I/O
217

6
Conclusion
If we compare results from Tables 3 and 4 for the same architecture, we can notice that
performances degrade signiﬁcantly for the synchronous approach. The system can
accept fewer requests per second, which effectively decreases the average response
time for the whole system. For the asynchronous approach, a constant performance is
showing up for number of requests that can be accepted per second. An increase in
average response time is showing up, which means that one of the components is
degrading in performance due to the increasing number of requests. But, the scalability
limit of the web application is removed thanks to the transformation of the legacy code
with asynchronous I/O calls.
References
1. von Behren, R., Condit, J., Brewer, E.: Why events are a bad idea (for high-concurrency
servers). Computer Science Division, University of California at Berkeley (2003)
2. Cugola, G., Di Nitto, E., Fuggetta, A.: Exploiting an event-based infrastructure to develop
complex distributed systems. CEFRIEL – Politecnico di Milano (1998)
3. Haller, P., Odersky, M.: Scala actors: unifying thread-based and event- based programming.
EPFL, Switzerland (2009)
4. Cunningham, R., Kohler, E.: Making events less slippery with eel. University of California,
Los Angeles (2005)
5. Haller, P., Odersky, M.: Event-based programming without inversion of control. EPFL,
Switzerland (2006)
6. Hughes-Croucher, T., Wilson, M.: Node: Up and Running. O’Reilly, Sebastopol (2014)
7. Wilson, J.R.: Node.js the Right Way. The Pragmatic Programmers (2013)
8. Henderson, C.: Building Scalable Web Sites. O’Reilly, Sebastopol (2006)
9. Chrysanthakopoulos, G., Singh, S.: An asynchronous messaging library for c#. In:
Proceedings of Workshop on Synchronization and Concurrency in Object-Oriented
Languages (SCOOL), OOPSLA (2005)
10. von Behren, R., Condit, J., Zhou, F., Necula, G.C., Brewer, E.: Capriccio: scalable threads
for internet services. In: 19th ACM Symposium on Operating Systems Principles, Bolton
Landing, Lake George, New York, October 2003
11. Ousterhout, J.K.: Why threads are a bad idea (for most purposes). Presentation at the 1996
USENIX Annual Technical Conference, January 1996
12. Cleary, S.: Async programming – introduction to async/await on ASP.NET. MSDN
Magazine (2014)
13. Ammann, P., Offutt, J.: Introduction to Software Testing. Cambridge University Press,
Cambridge (2008)
14. Henderson, C.: Building Scalable Web Sites: Building, Scaling and Optimizing the Next
Generation of Web Applications. O’Reilly, Sebastopol (2006)
15. Redkar, T., Guidici, T.: Windows Azure Platform. Apress, New York City (2011)
218
G. Rankovski and I. Chorbev
www.ebook3000.com

Relevance Re-ranking Through Proximity
Based Term Frequency Model
S. Sathya Bama1(&), M.S. Irfan Ahmed2, and A. Saravanan1
1 Sri Krishna College of Technology, Coimbatore 641042, Tamil Nadu, India
ssathya21@gmail.com, a.saravanan21@gmail.com
2 Nehru Institute of Engineering and Technology,
Coimbatore, Tamil Nadu, India
msirfan@gmail.com
Abstract. In this internet era, people rely on the most signiﬁcant tool called
search engine for retrieving attractive information from the web. Also, the rapid
growth in the usage of the web increases the volume of data on the web, due to
which most of the documents retrieved by the search engine is overwhelmed
with inappropriate and redundant information called outliers. This not only
increases the result space, but also roots in wasting the user’s time and effort that
makes them to surf uninteresting data. Consequently, a method is essential for
the web user community to remove uninteresting information and to present the
interesting data in an organized manner based on their request. Web content
outlier mining is promising research area that serves these features to the web
users. In this research work, proximity based term frequency model has been
developed for retrieving the appropriate information and for reﬁning the quality
of the results offered by the search engine. Experimental results indicate that
proximity based term frequency model improves the performance in terms of
relevancy re-ranking of the retrieved documents.
Keywords: Relevance ranking  Search engine  Term frequency  Proximity 
Web content outlier
1
Introduction
Due to digital evolution, the web is offering a massive amount of information for the
people to perform their transaction over the web. As a result, a growing number of
people are currently using the Internet to ascertain the exact information, to commu-
nicate or to share their knowledge and their views through social networking. Among
all these activities, web search is the leading application on the internet. It is a type of
information retrieval where the web users demand fast and appropriate response. Thus,
search engine came into existence. Search engines are the most familiar and popular
tool among people that allows them to search interesting information from the web.
The search engine uses a variety of algorithms to make the search effective and to help
the users in accessing desired information. Conversely, web search is effective only if
people aware of the information what they are seeking for. But in many cases, people
are unaware of the information; however, they need interesting and relevant content in
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8_22

a faithful manner. Thus, the key problem in information retrieval is providing the user
with interesting and relevant information when they do not have any awareness about
information need [1].
Search engine employs methods and techniques of web content mining which is the
process of extracting useful information from the contents of web documents. Even
though, the result produced by the search engine is not considered as effective since it
contains irrelevant and redundant information called as outliers. Conventional outlier
mining algorithms have been devised to ﬁnd rare patterns that well suit numeric data
sets. These algorithms cannot be used directly for web data since it contains different
types of data like text, number, image etc. There are three types of web outliers; web
content outliers, web structure outliers, and web usage outliers [2]. This paper primarily
focuses on the problem of identifying outliers from web contents called web content
outlier mining. A web content outlier is described as a web document with different
contents compared to similar documents taken from the same category.
2
Outlier Mining
According to Hawkins, Outlier is deﬁned as an observation that deviates too much from
other observations that it arouses uncertainties that it was generated by a different
mechanism from other observations [3]. The exact deﬁnition of an outlier depends on
the context. Deﬁnitions fall roughly into following categories [4, 5]: (i) Distribution
based, (ii) Depth based (iii) Distance based (iv) Deviation based (v) Density based.
Distribution-based methods originate from statistics. Here normal data objects
follow a known given distribution and occur in a high probability region of this model
where as an outlier deviates strongly from underlying distribution. The outliers deviate
more than 3 times the standard deviation from the mean [5–7]. One of the statistical
approach drawbacks is it requires knowledge about parameters of the data set, such as
the data distribution. However, in many cases, the data distribution may not be known
[8]. General idea of depth based method is that the search for outliers is made at the
border of the data space but independent of the statistical distributions. The Basic
assumption here is Outliers are located at the border of the data space and Normal
objects are in the center of the data space [5].
Distance based methods was originally proposed by Knorr [9]. He deﬁnes outlier as
an observation that is dmin distance away from p percentage of observations in the
dataset. The problem is then ﬁnding appropriate and such that outliers would be cor-
rectly detected with a small number of false detections. This process usually needs
domain knowledge [10].
Deviation based method identiﬁes outliers by examining the main characteristics of
objects in a group. Objects that “deviate” from this description are considered outliers.
Hence, in this approach the term deviations are typically used to refer to outliers [8,
11]. Density based was proposed by Breunig [12]. The density around a point is
compared with the density around its local neighbors. The density around a normal data
object is similar to the density around its neighbors where as it differs for outliers.
220
S. Sathya Bama et al.
www.ebook3000.com

3
Measuring Proximity for Outlier Mining
A proximity measure explores the actual closeness of terms in a given segment of text.
In IR proximity can be deﬁned as the similarity of query-terms within a sample of text.
The underlying hypothesis of this research paper is that documents in which query
terms appear closer together are more useful to the user since it improves the degree of
relevance. A complete proximity function should include the relationships between all
query-terms. Thus, increase in the closeness of two query terms increases the document
score. This score is subject to the distance between the terms in a document.
This paper provides a framework for re-ranking the retrieved documents based on
the relevancy score that includes proximity based term frequency of all the words in the
given query called keywords and term frequency of other words. A pair-wise proximity
measure is used in which a pair of terms is given as an input for the proximity function.
To calculate the proximity based term frequency, position vectors for each keyword in
the retrieved documents have to be deﬁned which includes the relative position of the
terms in that document.
4
Related Work
Finding outliers in web is difﬁcult than ﬁnding outliers in numeric data due to the
dynamic and diverse structure of the web. While searching information from the web,
since user expects the accurate data in less time, implementing the model that present
the accurate data quickly by removing irrelevant and redundant data is a very big
challenge to the research community. Most of the existing algorithms concentrate on
numeric data and for structured data.
Agyemang [13] developed the method of ﬁnding outliers on the web using full
word matching assuming the existence of domain dictionary. The above authors
developed the work with n-gram techniques for partial matching of strings with domain
dictionary [2, 13]. Agyemang et al. enhanced the same work without domain dic-
tionary. Based on the above ideas, they prolonged the work by presenting HyCOQ
which a hybrid algorithm that draws from the power of n-gram based and word based
system [14, 15]. Poonkuzhali et al. presented a mathematical approach based on signed
and rectangular representation [16] and a mathematical approach based on correlation
analysis [17] have been developed to detect and remove the redundancy between
unstructured web documents. Another statistical approach has been proposed by
Poonkuzhali et al., in which the keywords and other words are given with equal
weights [18]. Sathya Bama et al., introduced a weighted approach [19]. For many of the
above methods the dictionary has to be compiled for every domain as a pre-processing
step which consumes lot of time and effort.
There have been many recent attempts to incorporate proximity into IR models
[20–24]. The relatedness of terms in a semantic sense can be mapped to a proximity
measure [23]. Ronan has presented 12 measures related to proximity either implicitly or
explicitly. Work has been presented [25] that makes use of a fuzzy set theoretic
measure of proximity in a Boolean model of retrieval. This framework is elegant and
presents results for a number of different system parameter settings. Previous research
Relevance Re-ranking Through Proximity Based Term Frequency Model
221

[26] has used a window or passage method to determine proximity within a certain
threshold. The work shows that proximity can increase performance on small collec-
tions. Proximity information is incorporated into an existing ad hoc retrieval function to
improve the performance of short queries [27]. They create a proximity function at the
sentence level, whereby if two query terms appear within the same sentence the doc-
ument score will be increased. More recently some approaches have been successful in
employing proximity into a number of keyword based retrieval functions [23].
Introduction and analysis of a novel approach for determining the “goodness” of a
span of terms in a document [28]. The integration of term proximity in to probabilistic
model BM25 using pseudo-term frequency instead of term frequency has been intro-
duced in [29]. When considering only top retrieved documents, term proximity
information can lead to improved retrieval effectiveness [30]. The structured nature of
web documents may also contribute to the signiﬁcant improvements that can be
obtained through the use of phrases and term proximity information in web retrieval
[31]. Recent work by Song et al. indicates that using ﬂexible proximity terms within an
information retrieval model such as BM25 results in improved retrieval effectiveness
[32]. This paper aims to create a proximity function which deals with pair of adjacent
query-terms.
5
PTF – A Framework for Web Content Outlier
The architecture of the proposed model is given in Fig. 1.
Fig. 1. Architecture of proximity based term frequency model
222
S. Sathya Bama et al.
www.ebook3000.com

Consider a sample document D. The set of terms in the document D is assumed as
TD = {a b c d a b a c a b d e} and the set of integer position for each term in the
document D can be given as PD = {1 2 3 4 5 6 7 8 9 10 11 12}. Let the Query Term
Q = {a b c}. Thus to calculate the proximity based term frequency for the keywords in
the document, non-query terms can be discarded since most of document holds min-
imum number of query terms, maximum number of non-query terms and computing
proximity score for non-query terms incorporate the generation of large number of term
pairs in which an enormous amount time has been involved. Thus the content of the
document D can be represented as TD = {a b c x a b a c a b x x} where ‘x’ denotes the
non-query terms.
The position vector for each query term a, b, c in the document D can be given as
PD(a) = {1, 5, 7, 9}, PD(b) = {2, 6, 10}, PD(c) = {3, 8} respectively. Thus the fre-
quency of all the query terms a, b, c in the document D is fD(a) = 4, fD(b) = 3 and
fD(c) = 2. Also for Relevance Score the frequency of other terms will also be calculated
to normalize the relevance score. fD(rt) = 3 where fD(rt) is the frequency of remaining
terms in the document D. Intuitively, two factors affect the proximity score is the
distance between terms in term-pair and the order of occurrence of the terms in
term-pairs.
The term-pairs involving adjacent terms for the given query Q can be generated.
Then the distance between all the adjacent term pair ti and tj can calculated based on the
position vectors of the terms in keywords in the document. Thus for the above query
the adjacent term pairs can be generated as ab, bc. Therefore, the distance between a
term-pair can be computed as in Eq. (1).
dis ti; tj


¼ PD tj
 
 PD ti
ð Þ


ð1Þ
Here dis(ti, tj) is the distance between two terms and PD(ti) is the position of term ti
in document D. The term proximity measure can be used in the calculation of relevance
score by satisfying the unique constraint as in Condition 1.
Condition 1: The relevance/proximity score should decrease as the distance between
the query terms increases.
The Proximity based Term Frequency can be computed using Eq. (2).
PTFðtiÞ ¼ f ðtiÞ þ
Xn
j¼1;j6¼i TPðti; tjÞ
ð2Þ
where PTF(t) is the proximity based frequency measure, f(t) is the frequency of the
term t in the document and n is the number of term-pairs for the given query. The
proximity function TP() (Term Proximity) has been deﬁned as a function that takes the
input as the term-pair and this ensures the unique constraint given in Condition 1 and it
is given in Eq. (3).
TP ti; tj


¼
1
min dis ti; tj




ð3Þ
Relevance Re-ranking Through Proximity Based Term Frequency Model
223

When more than one tj occurs near ti, the one with the smallest distance can be used to
calculate proximity function.
Thus PTF(t) will be calculated for each term in the keyword. Then this new
proximity based term frequency of a keyword is combined with the term frequency of
other words in the document to calculate the relevance score. The relevance score can
be calculated with the below formula in Eq. (4).
RS D
ð Þr¼
fD ti
ð Þ þ Pn
j¼1;j6¼i TPD ti; tj


h
i
þ b  fD rt
ð Þ
ð
Þ
ND
ð4Þ
RS(D) is the relevance score of a document D, for all the document in the retrieved
set of documents. ND is the total number of terms in the document D. b is the scaling
factor and it is set as 0.5, since non-query terms has less weightage when compared to
keywords. Then the documents are arranged in the descending order of the relevance
score. If more than one document contains the same relevance score, then it can be
compared to ﬁnd out the redundant documents.
6
Experimental Results
An experimental analysis has been made for the proposed model. The experiment is
conducted for the user query “Recent Research in Web Content Mining” against a
Google search engine. The top 10 documents which are listed in Table 1 are retrieved
and it becomes the input for the proposed approach which is then pre-processed by
removing stop words. The Keywords are extracted from the user query.
Initially the term frequency and position vectors for all the keywords are generated
for all the retrieved documents. Then based on the position vector proximity measure
for all the keywords are calculated. Finally term frequencies for non-query terms are
also calculated. Then the relevance score can be calculated as in Table 2 with the above
formula. Finally the documents are arranged in descending order of their relevance
Table 1. List of input documents
Did
Retrieved documents
D1
www.cs.uic.edu/*liub/publications/editorial.pdf
D2
dmr.cs.umn.edu/Papers/P2004_4.pdf
D3
www.ijarcsse.com/docs/papers/Volume_3/11_November2013/V3I11-0352.pdf
D4
www.ijcsit.com/docs/Volume%205/vol5issue03/ijcsit20140503316.pdf
D5
ebiquity.umbc.edu/_ﬁle_directory_/papers/214.pdf
D6
esatjournals.org/Volumes/IJRET/2014V03/I03/IJRET20140303009.pdf
D7
www.kdd.org/sites/default/ﬁles/issues/2-1-2000-06/kosala.pdf
D8
citeseerx.ist.psu.edu/viewdoc/download?doi10.1.1.258.8941&rep=rep1&type=pdf
D9
www.upet.ro/annals/economics/pdf/2012/part1/Dinuca-Ciobanu.pdf
D10 arxiv.org/pdf/cs/0011033.pdf
224
S. Sathya Bama et al.
www.ebook3000.com

score. Since the relevance score for the documents D7 and D10 are same, the common
terms for the two documents can be counted. If the number of common words is same
as the number of words in the documents, then one of the documents is redundant
which can be removed.
The precision value is calculated at each position and the values always result
between 0 and 1. The precision for the proposed method is compared with the Google
rank and existing method given in [19]. The graph is shown in Fig. 2. Thus, the
proposed method gives better performance in terms of relevancy and it always lies
between 0 and 1.
The proposed method is compared with existing methods against the labelled
dataset having 125 relevant documents and 25 outlier documents. The experiment is
performed by varying the number of relevant documents and outliers and the results are
shown in Table 3. The number of outliers detected, percentage of Precision rate,
Table 2. Document score for the given query
Did
Frequency of
keywords
Term proximity
of keywords
Frequency of
remaining terms
Total
frequency
Relevance
score
D1
163
1116.90
2176
2339
0.5472
D2
99
249.59
485
584
0.5969
D3
187
572.75
1052
1239
0.6132
D4
129
448.31
848
977
0.5909
D5
210
699.95
1375
1585
0.5741
D6
128
306.09
565
693
0.6264
D7
347
1126.82
2181
2528
0.583
D8
187
333.04
584
771
0.6745
D9
55
296.26
584
639
0.5497
D10
347
1126.82
2181
2528
0.583
0
0.2
0.4
0.6
0.8
1
p@1
p@2
p@3
p@4
p@5
p@6
p@7
p@8
p@9
p@10
Precisions 
Google Rank
Weighted Approach
Proximity based Term Frequency
Fig. 2. Precision comparison at each position
Relevance Re-ranking Through Proximity Based Term Frequency Model
225

percentage of false rate and execution time for the proposed method is compared along
with the existing correlation analysis [17], Weighted Approach [19] and N-Gram
approach [14] are shown in the table. The graph for the above results are shown in the
Figs. 3 and 4.
From the results shown in Fig. 3, the accuracy for weighted term frequency
approach is better than weighted correlation and N-gram methods. However, proximity
based approach provides even better precision rate. Also, as in Fig. 4, the false rate is
very for proximity based approach when compared with other approaches.
Table 3. Comparative study for proposed methods
Query #
1
2
3
4
5
Relevant documents
25
50 75
100 125
Actual outliers
5
10 15
20
25
Proximity based approach Outliers detected
5
9 14
18
22
Precision (%)
100 90 93.33 90
88
False rate (%)
0 10 6.67
10
12
Execution time (Secs)
5
8 12
17
21
Correlation analysis
Outliers detected
3
7 12
16
21
Precision (%)
60 70 80
80
84
False rate (%)
40 30 20
20
16
Execution time (Secs)
10 16 23
31
39
Weighted approach
Outliers detected
4
8 13
17
22
Precision (%)
80 80 86.67 85
88
False rate (%)
20 20 13.33 15
12
Execution time (Secs)
4
7 12
15
19
N-gram approach
Outliers detected
2
7 11
15
19
Precision (%)
40 70 73.33 75
76
False rate (%)
60 30 26.67 25
24
Execution time (Secs)
12 17 25
32
41
100
90
93.33
90
88
60
70
80
80
84
80
80
86.687
85
88
40
70
73.33
75
76
Q1
Q2
Q3
Q4
Q5
Precision (%)
Query Number
Proximity based Approach
Correlation Analysis
Weighted Approach
N-Gram Approach
Fig. 3. Comparison of accuracy for proposed methods
226
S. Sathya Bama et al.
www.ebook3000.com

The comparison on execution time is given in Fig. 5. However, the execution time
is less for weighted approach, but the drawback in this method of is that it has to
compile and maintain domain dictionary. Thus the proposed method provides better
result when compared to others.
7
Conclusion and Future Work
The enormous growth of information sources available on the World Wide Web has
forced the web mining researchers to develop new and effective algorithms and tools to
identify relevant information without duplicates. In this paper proximity based term
frequency model (PTF) has been proposed that computes the position based relevance
score. This method reduces the time and space since the compilation of domain dic-
tionary has been eliminated. However, this work focuses on outlier mining in web
documents especially for text. Experimental results show that the proposed model
outperforms well than the exiting methods. Future work aims at mining outliers in web
documents that not only includes text but also images and either multimedia data. Also,
other mathematical tools can be used to improve the results further.
0 
10
6.67
10
12
40
30
20
20
16
20
20
13.33
15
12
60
30
26.67
25
24
Q1
Q2
Q3
Q4
Q5
False Rate (%)
Query Number
False Rate Comparison
Proximity based Approach
Correlation Analysis
Weighted Approach
N-Gram Approach
Fig. 4. Comparison of false rate for proposed methods
0
10
20
30
40
50
Q1
Q2
Q3
Q4
Q5
Time in Seconds
Query Number
Proximity based Approach
Correlation Analysis
Weighted Approach
N-Gram Approach
Fig. 5. Comparison of execution time
Relevance Re-ranking Through Proximity Based Term Frequency Model
227

References
1. Mele, I.: Web usage mining for enhancing search-result delivery and helping users to ﬁnd
interesting web content. In: Proceedings of the Sixth ACM International Conference on Web
Search and Data Mining. ACM (2013)
2. Agyemang, M., Barker, K., Alhajj, S.R.: Framework for mining web content outliers. In:
Proceedings of 19th ACM Symposium on Applied Computing, Nicosia, Cyprus, pp. 590–
594 (2004a)
3. Hawkins, D.M.: Identiﬁcation of Outliers, vol. 11. Chapman and Hall, London (1980)
4. Jin, W., Tung, A.K., Han, J.: Mining top-n local outliers in large databases. In: Proceedings
of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 293–298. ACM (2001)
5. Kriegel, H.P., Kröger, P., Zimek, A.: Outlier detection techniques. In: Tutorial at the 13th
Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining (2009)
6. Freedman, D., Pisani, R., Purves, R.: Statistics. W.W. Norton & Company, Inc., New York
(1978)
7. Hautamäki, V., Kärkkäinen, I., Fränti, P.: Outlier detection using k-nearest neighbour graph.
In: ICPR, vol. 3, pp. 430–433 (2004)
8. Han, J., Kamber, M.: Data Mining Concepts and Techniques. Morgan Kaufmann,
Burlington (2001)
9. Knorr, E.M., Ng, R.T.: Algorithms for mining distance based outliers in large datasets. In:
Proceedings of the International Conference on Very Large Data Bases, pp. 392–403 (1998)
10. Knorr, E.M., Ng, R.T., Tucakov, V.: Distance-based outliers: algorithms and applications.
Int. J. Very Large Data Bases 8(3–4), 237–253 (2000)
11. Bakar, Z.A., Mohemad, R., Ahmad, A., Deris, M.M.: A comparative study for outlier
detection techniques in data mining. In: 2006 IEEE Conference on Cybernetics and
Intelligent Systems, pp. 1–6. IEEE (2006)
12. Breunig, M.M., Kriegel, H.P., Ng, R.T., Sander, J.: LOF: identifying density-based local
outliers. ACM Sigmod Rec. 29(2), 93–104 (2000)
13. Agyemang, M., Barker, K., Alhajj, R.: Framework for mining web content outliers. In:
Proceedings of the 2004 ACM Symposium on Applied Computing, pp. 590–594. ACM
(2004b)
14. Agyemang, M., Barker, K., Alhajj, R.S.: Mining web content outliers using structure
oriented weighting techniques and N-grams. In: Proceedings of the 2005 ACM Symposium
on Applied Computing, pp. 482–487. ACM (2005a)
15. Agyemang, M., Barker, K., Alhajj, R.: Hybrid approach to web content outlier mining
without query vector. In: Data Warehousing and Knowledge Discovery, pp. 285–294.
Springer, Heidelberg (2005b)
16. Poonkuzhali, G., Uma, G.V., Sarukesi, K.: Detection and removal of redundant web content
through rectangular and signed approach. Int. J. Eng. Sci. Technol. 2(9), 4026–4032 (2010)
17. Poonkuzhali, G., Kishore Kumar, R., Kripa Keshav, R., Sudhakar, P., Sarukesi, K.:
Correlation based method to detect and remove redundant web document. In: Advanced
Materials Research, vol. 171, pp. 543–546 (2011a)
18. Poonkuzhali, G., Kumar, R.K., Keshav, R.K., Thiagarajan, K., Sarukesi, K.: Statistical
approach for improving the quality of search results. In: Proceedings of the 10th WSEAS
International Conference on Applied Computer and Applied Computational Science, pp. 89–
93. World Scientiﬁc and Engineering Academy and Society (WSEAS) (2011b)
19. Sathya Bama, S., Irfan Ahmed, M.S., Saravanan, A.: Enhancing the search engine results
through web content ranking. Int. J. Appl. Eng. Res. 10(5), 13625–13635 (2015)
228
S. Sathya Bama et al.
www.ebook3000.com

20. Bhatia, M.P.S, Kumar Khalid, A.: Contextual proximity based term-weighting for improved
web information retrieval. In: KSEM, pp. 267–278 (2007)
21. Bai, J., Chang, Y., Cui, H., Zheng, Z., Sun, G., Li, X.: Investigation of partial query
proximity in web search. In: Proceeding of the 17th International Conference on World Wide
Web, WWW 2008, pp. 1183–1184. ACM (2008)
22. Na, S.-H., Kim, J., Kang, I.-S., Lee, J.-H.: Exploiting proximity feature in bigram language
model for information retrieval. In: Proceedings of the 31st Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2008,
pp. 821–822. ACM (2008)
23. Tao, T., Zhai, C.X.: An exploration of proximity measures in information retrieval. In:
Proceedings of the 30th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR 2007, pp. 295–302. ACM (2007)
24. Cummins, R., O’Riordan, C.: Learning in a pair-wise term-term proximity framework for
information retrieval. In: Proceedings of the 32nd International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 251–258. ACM (2009)
25. Beigbeder, M., Mercier, A.: An information retrieval model using the fuzzy proximity
degree of term occurrences. In: Proceedings of the 2005 ACM Symposium on Applied
Computing, pp. 1018–1022. ACM (2005)
26. Possas, B., Ziviani, N., Meira, Jr., W.: Enhancing the set-based model using proximity
information. In: String Processing and Information Retrieval, pp. 104–116. Springer,
Heidelberg (2002)
27. Rasolofo, Y., Savoy, J.: Term proximity scoring for keyword-based retrieval systems,
pp. 207–218. Springer, Heidelberg (2003a)
28. Svore, K.M., Kanani, P.H., Khan, N.: How good is a span of terms?: exploiting proximity to
improve web retrieval. In: Proceedings of the 33rd International ACM SIGIR Conference on
Research and Development in Information Retrieval, pp. 154–161. ACM (2010)
29. Song, R., Taylor, M.J., Wen, J.R., Hon, H.W., Yu, Y.: Viewing term proximity from a
different perspective. In: Advances in Information Retrieval, pp. 346–357. Springer,
Heidelberg (2008)
30. Rasolofo, Y., Savoy, J.: Term proximity scoring for keyword-based retrieval systems. In:
ECIR, p. 207 (2003b)
31. Mishne, G., de Rijke, M.: Boosting web retrieval through query operations. In: ECIR,
pp. 502–516 (2005)
32. Song, R., Yu, L., Wen, J., Hon, H.: A proximity probabilistic model for information
retrieval. Technical report, Microsoft Research (2011)
Relevance Re-ranking Through Proximity Based Term Frequency Model
229

Information System for Biosensors Data
Exchange in Healthcare
Monika Simjanoska, Bojana Koteska(B), Magdalena Kostoska,
Ana Madevska Bogdanova, Nevena Ackovska, and Vladimir Trajkovikj
Faculty of Computer Science and Engineering, University Ss. Cyril and Methodius,
Rugjer Boskovikj 16, 1000 Skopje, Macedonia
{monika.simjanoska,bojana.koteska,magdalena.kostoska,
ana.madevska.bogdanova,nevena.ackovska,vladimir.trajkovik}@finki.ukim.mk
Abstract. This paper presents a novel cloud information system (BIO-
HIS - Biosensors Healthcare Information System) whose main goal is
to enable vital data exchange obtained from biosensors. BIOHIS corre-
sponds to the requirements of the existing protocols for MRMI (medical
response to major incidents). This system aims to ease and improve the
data exchange between the diﬀerent institutions involved in the MRMI
protocols. BIOHIS is one step closer to the interoperable data ﬂow among
the various medical system interfaces, ensuring structured data capture.
The system is intended to use cloud leverages as inﬁnite storage, com-
puting capacities and full time authorized access.
Keywords: Biosensors · Information system · Medical protocols
1
Introduction
The purpose of biosensors inclusion in people’s everyday life is to increase
the overall quality of life, but also to improve the quality of response to life-
threatening situations. The biosensors technology has potential to aid the med-
ical protocols that deal with both civilian and military disasters.
The beneﬁts of introducing biosensors in the existing medical protocols is
multi-fold, especially during the triage process where the manual measurements
are substituted with automatic vital data gathering [1]. Biosensors provide mul-
tiple triage processes to run in parallel which speeds up process of prioritization
in case of multiple injuries. It enables the reduction of the number of medical
persons needed for performing the measurements at the location of the incident
and the inclusion of volunteers with no medical experience.
Our information system, as a centralized cloud solution, provides continu-
ous monitoring and transfer of patients’ data. In order to enable the contin-
uous data ﬂow among the diﬀerent entities involved in the medical protocols,
we must ensure structured data environment. Having combined the vital data
together with the rest of patient data as demographic records, lab analysis, clin-
icians notes, etc., we can ease the transfer of the vitals to whoever involved in
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 23
www.ebook3000.com

Information System for Biosensors Data Exchange in Healthcare
231
the MRMI. Bringing the right information in the right hands at the right time
requires cautious design of an information system that will meet the medical pro-
tocol requirements. In this paper we describe our BIOHIS architecture and data
structure in details and the beneﬁts from the implementation of such a system.
Our system enables eﬃcient resource allocation of medical facility capacities and
ambulance vehicles, depending on triage decision. The biosensor digital transfer
of vital parameters improves the coordination and communication between hos-
pitals in a case of losing any patients’ information. The early and fast detection
of patient state with the biosensors also reduces the number of possible deaths.
The paper is organized as follows: in the next Sect. 2, we give an overview of
the existing solutions such as electronic health records. In Sect. 3, we describe the
BIOHIS architecture. Section 4 provides the data structure model of our system.
Diﬀerent possible scenarios are provided in Sect. 5. The system constraints are
experimentally explained in Sect. 6 and conclusive remarks are given in the last
Sect. 7.
2
Related Work
Data can be useful only if gathered in a meaningful, digital form. The necessity
of evidence is enormous due to the fact of the exponential data increase - the
predictions say up to 25,000PB of data by 2020 [2]. Data should be constantly
gathered and analyzed upon which actions will be triggered. This problem still
remains a challenge in the real-life context [3].
Electronic Health Record (EHR) is an initiative to collect all kind of patients
data, whether it is demographics data, clinicians notes, medications, laboratory
data, vital signs reports, etc. A complete documentation is expected to provide
easy access by diﬀerent medical institutions, deliver more reliable health-care and
support evidence-based practice. Each clinical system has unique and dynamic
work ﬂow and a customized EHR system adapted to the local user requirements.
A standardized EHR format is very hard to achieve, thus interoperable standards
are developed for data exchange between diﬀerent EHR formats. The goal is to
access a structured data template, automatically populate the template from
existing EHR data, store and transmit the completed template to the appropriate
organizations or researchers [4].
There are several distinguished EHR standards. Virtual Data Warehouse
(VDW) is a virtual setting of parallel databases that have been constructed
by extracting data directly from the local EHR systems. It is a method for
standardizing and pooling EHR data for multi-site research [5].
Health Level Seven (HL7) makes possible the exchange of demographics and
other textual information. It is not a type of software, instead it provides a
speciﬁcation for making systems interoperable. In HL7’s strategies description
the interoperability is deﬁned in three diﬀerent contexts that aﬀect how the
software is designed, the data is stored and used [6].
Developing a standard that maintains the semantic meaning is really hard
to achieve. Systematized Nomenclature of Medicine Clinical Terms - SNOMED

232
M. Simjanoska et al.
CT in a period of 40 years has developed from a pathology-speciﬁc nomenclature
into a logic-based health care terminology [7].
As previously discussed, there are several recently developed healthcare sys-
tems that implement EHR standards.
A customized healthcare service by means of wellness clothing which includes
digital yarns and biosensors is presented in [8]. It’s goal is to acquire, analyze,
and present bio-engineering data including ECG, respiration, acceleration, and
body temperature.
In [9], the authors propose a framework to collect patients’ data in real time,
perform monitoring, and propose medical and/or life style engagements. The
framework integrates mobile technologies to collect and communicate vital data
from a patient’s wearable biosensors. The data are stored in the Cloud and are
available to physicians, paramedics, etc.
An intelligent home-based platform is implemented in [10]. The platform
involves an open-platform-based intelligent medicine box, intelligent pharma-
ceutical packaging with communication capability enabled by passive radio-
frequency identiﬁcation and a ﬂexible and wearable bio-medical sensor device.
3
BIOHIS Architecture
In this section we present a novel cloud information system, BIOHIS, whose
main goal is to enable vital data exchange obtained from biosensors.
BIOHIS consists of several component: biosensors, local electronic devices
for communication with the biosensors, cloud server, connectivity to the cloud
server (diﬀerent links) and real-time monitoring of the stored data by medical
personnel. Figure 1 presents the BIOHIS system overview.
Sensor
Sensor
Sensor
Sensor
Bluetooth or cord
Bluetooth or cord
Bluetooth or cord
Bluetooth or cord
Internet
Cloud 
server
Bluetooth
Radio Data Link
Fig. 1. BIOHIS system overview
We consider two possible cases for data transmission presented in Fig. 1:
– Data can be transmitted to the cloud by Internet, or
– Data can be transmitted to the cloud via High Frequency (HF) radios.
www.ebook3000.com

Information System for Biosensors Data Exchange in Healthcare
233
In general case, where Internet is available, the data is transmitted to the cloud
and then distributed to whoever needs them. In cases where Internet is not
available, HF radios can also be used for data transfer.
4
BIOHIS Data Model
The data model of BIOHIS system is shown in Fig. 2. Each system user has an
opportunity to access data for multiple hospitals that he/she is authorized for.
Some users have a permission to manage data for diﬀerent hospitals. The data for
the patients are stored in the database by using the centralized cloud solution for
our system on the battleﬁeld or at the location of a major incident. The measured
vital parameters from the biosensors are stored in the table “Vital Parameters”
(ECG, respiratory rate, oxygen rate, temperature and blood pressure). These
data are measured periodically and an authorized user can see the history of
vital parameters value for a patient.
Before the triage process begins, a new ID is generated for each injured
person. Software automatically proposes a new “personId” which is also attached
to the injured person. At the place of the incident, the medical person who does
the triage inserts only the information about injured body parts and medication,
and later, in the hospital, the rest of the information is fulﬁlled on the basis on
the person’s ID. The triage process also includes identiﬁcation and marking of
the injured body parts which are stored in the table “InjuredBodyParts”. The
level of injury is calculated by using the Glasgow Coma Score (GCS), and Triage
Sort Score and data are inserted in the table “LevelofInjury”. Concerning the
medications that can be given to a injured person at the place of the incident or
later at the hospital, one can use the table “Medication” - a history of medication
records are stored at this table. After the triage process, if needed, the patient
is sent to a hospital with free capacity to accommodate.
5
Implementation Scenarios
The protocols for medical response to major incidents is comprised of: triage,
medical response before hospital treatment, transport to hospital, hospital
response, communication, command and coordination. The eﬀective medical
response to major incidents does not depend only on these components, but
also on the way how they function together.
5.1
Civil Disasters
In a case of a civil mass disaster, the main hospital communicates with the
dispatch center for sending ambulance vehicles at the location of the incident.
The dispatch center is responsible for coordination with other hospitals and
inspection of the capacity to accommodate patients. The data obtained from
the patients triage are sent to the main hospital by HF radios. This practice

234
M. Simjanoska et al.
User
userID
PK
name
country
username
password
Hospital
hospitalID
PK
hospitalName
hospitalLocaƟon
PaƟent
M
N
paƟentID
PK
paƟentName
M
1
freeAccomodaƟ
onBeds
VitalParameters
ECG
respiratoryRate
oxigenRate
temparature
bloodPresure
M
1
InjuredBodyParts
bodyPartName
ibpID
PK
1
M
vpID
PK
ƟmeDateInsert
LevelofInjury
loiID
PK
levelName
ƟmeDateInsert
1
M
MedicaƟon
medicaƟonID
PK
medicaƟonName
ammount
ƟmeDateInsert
Enrollment
enrollmentID
PK
ƟmeDateEnroll
ment
roomNumber
1
M
daysSupply
ƟmeDateInsert
1
M
Fig. 2. BIOHIS data model
usually results in errors and misunderstandings in the coordination, communi-
cation and issued commands, which is actually the weakest link of the saving
protocol implementation. Failure of sending data is the main reason for inclusion
of the biosensors in the triage process which includes monitoring of the patient
vital data signs and determining the severity of injuries. The advantage of using
biosensors is the data transfer to local electronic devices via Bluetooth technol-
ogy. The measured data are stored to local devices and this is especially useful
when Internet connection is temporary lost or not existing. Figure 3 shows an
architecture of a general protocol for medical response with biosensors incorpo-
rated. Additionally, depending on the available data communications, the data
from the local electronic devices can be transmitted to the centralized cloud
application in real time and then distributed to main hospital, communication
center and other hospitals which are authorized to view these data.
5.2
Military Environment
Figure 4 presents the biosensors usage in military environments. The place of
incident is the battleﬁeld where specially trained ﬁrst aid responders come to
the wounded soldiers and perform the triage procedure. Since the Internet is
not available on the battleﬁeld, the data are transmitted via Bluetooth to the
www.ebook3000.com

Information System for Biosensors Data Exchange in Healthcare
235
Sensor
Sensor
Bluetooth
ConnecƟon
Cloud 
server
Fig. 3. Biosensors incorporation in civil environment
Sensor
Bluetooth
ConnecƟon
Cloud 
server
Sensor
Fig. 4. Biosensors incorporation in military environment

236
M. Simjanoska et al.
mobile terminals. The temporary stored data are synced to the cloud as soon
as there is Internet connection established. On the batteﬁeld, only the data for
the injured body parts and medications are inserted. The entities involved in
the saving protocol are the military transport ambulance/helicopter, the impro-
vised hospital tent, and the remote hospital. All of them can access the central
database, retrieve the patients’ data collected from the battleﬁeld, during the
transport and from the hospital tent. The advantages of the digital capture of
patients’ vitals are the early preparation for appropriate intervention, and the
history availability for future treatment.
6
Experiments and System Constraint Results
The system constraints are imposed due to diﬀerent use cases and communica-
tion technologies. One of the most important constraint is the size of data and
the ability to transfer the data to the target environment using the available
communication.
In order to predict and model the data size created by the sensors we have
created laboratory environment where we have conducted numerous experiments
using diﬀerent sensors [11] and observed the data size per minute:
1. 50 SPO2 sensor readings at 2 and 1000 Hz rate each;
2. 50 Airﬂow sensor readings at 1000 Hz rate;
3. 100 ECG sensor readings at 125, 200, 500 and 1000 Hz rate each.
Table 1 shows the obtained results. During the experiment we have established
that wearing the standard airﬂow sensor won’t be practical, but these data can
be extracted from ECG measurements. Given Table 1, we can conclude that total
size of the data used for vital signals (per person) is 40 KB/min at 125 Hz reading
rate and 310 KB/min at 1000 Hz reading rate. From these measurements, we can
conclude that for an hour of observation and sensor readings for 10 persons, the
total size of generated data will be 24 MB at 125 Hz reading rate and 186 MB at
1000 Hz reading rate. This gives us the initial data storage capacity we need to
provide in order to ensure that our system is eﬃcient.
Another important constraint is the data transfer rate which depends on
the communication technologies. Depending on the scenario used, the HF radio
and Bluetooth communication deﬁne the tightest constraints. Table 2 gives data
rate transfer of these technologies. Given the slowest data rate in this table (HF
radios), the 125 Hz sample rate will be satisfactory to transfer sensor data for 10
persons.
We have also created model of data size given the vital data parameters
used in standard triage protocols as well as their ranges. The result for each
parameter, as well as the total data size are shown in Table 3. Given each reading
is 6B and 125 Hz sample rate is used, the total size is 750 B/s and the total size
per minute is 45 KB. Assuming usage of data delimitation doubles the value, we
obtain 90 KB per minute. With all parameters included in every reading we are
still within the calculated 192 KB/min bandwidth per person (obtained by the
www.ebook3000.com

Information System for Biosensors Data Exchange in Healthcare
237
Table 1. Laboratory measured sensor data size
Sensors included
Data size per minute
SPO2 sensor readings at 2 Hz rate
2 KB/min
SPO2 sensor readings at 1000 Hz rate
130 KB/min
Airﬂow sensor readings at 1000 Hz rate 40 KB/min
ECG sensor readings at 125 Hz rate
30 KB/min
ECG sensor readings at 200 Hz rate
50 KB/min
ECG sensor readings at 500 Hz rate
110 KB/min
ECG sensor readings at 1000 Hz rate
180 KB/min
Table 2. Communication technologies data rate
Technology
Data rate
Bandwidth of HF military radios currently in use
in Macedonian army
256 KB/s = 1920 KB/min
Bandwidth of HF military radios currently in
process of acquisition in Macedonian army
1 MB/s
Bluetooth 2.0
3 MB/s
Bluetooth 3.0
24 MB/s
Bluetooth 4.0
24 MB/s
Table 3. Triage vital data parameters size
Vital data parameter Data range
Data size
Pulse rate
60 to 100 beats
1B per reading
Respiratory rate
12–20 breaths per minute
Less than 1B per reading
Blood pressure
2 values normally in range
between 80 and 180
2B per reading
Temperature
Small integer
Less than 1B per reading
Total
6B
lowest communication rate HF radio scenario where 10 persons wear sensors).
We measured ECG, SO2 and airﬂow in order to determine data size per minute
and the data rate given the technology that will be used. The experiment has
shown that if we use 125 Hz sample rate, we can transfer biosensor data for 10
persons.
7
Conclusion
In this paper we propose a biosensors healthcare information system (BIOHIS)
as centralized cloud solution which provides continuous monitoring and transfer

238
M. Simjanoska et al.
of patients’ data. The incorporation of biosensors technology replaces the man-
ual measurements with automatic vital data gathering, enables parallel triage
process and reduces the number of medical persons needed for performing the
measurements. The digital transfer of patients vital parameters also improves
the coordination and communication between hospitals in a case of losing any
patients’ information, provides early and fast detection of patient state and early
preparation for appropriate future intervention.
We also describe the BIOHIS architecture and data structure. The data
model replaces the current EHR system practices to store the clinical and admin-
istrative data in unstructured ﬁelds. It enables the interchange of vital data to
gather combined with the rest of patient data as lab analysis, clinicians notes,
etc. among the diﬀerent entities involved in the medical protocols.
Our biosensors healthcare information system can be applied both to civil
disasters and medical environment scenarios. When Internet is available, the
data are transmitted to the cloud and then distributed to whoever needs them.
If Internet is not available, HF radios can be used for data transfer. In both
cases, the measured vital data are transferred and stored to the local electronic
devices using the Bluetooth technology. This is especially useful when Internet
connection is temporary lost or not existing.
We identiﬁed the data size and transfer technologies availability as biggest
system constraints. We have conducted laboratory experiments in order to pre-
dict and model the data size created by the biosensors. We performed numerous
measuring activities using diﬀerent biosensors and observed the data size per
minute which gives us the initial data storage capacity we need to provide. Tak-
ing into consideration the data transfer rate which depends on the communica-
tion technologies, we concluded that the 125 Hz sample rate will be satisfactory
to transfer sensor data for 10 persons. We have also created a model of data size
given the vital data parameters used in standard triage protocols.
Acknowledgment. This paper is supported by SIARS, NATO multi-year project
NATO.EAP.SFPP 984753.
References
1. Madevska Bogdanova, A., Simjanoska, M., Ackovska, N., Kostoska, M., Koteska,
B., Tashkoski, M.: Biosensors technology in massive civil disasters. In: 23th Inter-
national Symposium on Emergency Medicine, Slovenian Society for Emergency
Medicine, pp. 355–359 (2016)
2. Healthcare IT Connect (HITC): Big Data Infographic (2012)
3. Caulﬁeld, B.M., Donnelly, S.C.: What is connected health and why will it change
your practice? QJM 106(8), 703–707 (2013)
4. S&I Framework: Structured Data Capture
5. Ross, T.R., Ng, D., Brown, J.S., Pardee, R., Hornbrook, M.C., Hart, G., Steiner,
J.F.: The HMO research network virtual data warehouse: a public data model to
support collaboration. EGEMS 2(1) (2014)
6. HL7: Health Level Seven International (2016)
www.ebook3000.com

Information System for Biosensors Data Exchange in Healthcare
239
7. Cornet, R., de Keizer, N.: Forty years of SNOMED: a literature review. BMC Med.
Inf. Decis. Making 8(Suppl. 1), S2 (2008)
8. Lee, T.G., Lee, S.H.: Dynamic bio-sensing process design in mobile wellness infor-
mation system for smart healthcare. Wirel. Pers. Commun. 86(1), 201–215 (2016)
9. Benharref, A., Serhani, M.A.: Novel cloud and SOA-based framework for E-health
monitoring using wireless biosensors. IEEE J. Biomed. Health Inf. 18(1), 46–55
(2014)
10. Yang, G., Xie, L., M¨antysalo, M., Zhou, X., Pang, Z., Da Xu, L., Kao-Walter, S.,
Chen, Q., Zheng, L.R.: A health-iot platform based on the integration of intelligent
packaging, unobtrusive bio-sensor, and intelligent medicine box. IEEE Trans. Ind.
Inf. 10(4), 2180–2191 (2014)
11. Kostoska, M., Koteska, B., Simjanoska, M., Tashkoski, M., Ackovska, N., Madevska
Bogdanova, A., Golubovski, R.: eHealth platform prototype for real-time biosensor
data transfer. In: 13th International Conference on Informatics and Information
Technologies (2016, in print)

An Automatic Tracking System for Natural
Hazard Events with Satellite Remote Sensing
Assen Tchorbadjieﬀ(B)
Institute of Mathematics and Informatics, Bulgarian Academy of Science,
Acad. Georgi Bonchev Str., Block 8, 1113 Soﬁa, Bulgaria
atchorbadjieff@math.bas.bg
Abstract. The atmosphere satellite data for atmosphere parameters
are the most important source of information for monitoring of areas
without or with very rare environment research facilities. With grow-
ing dynamic of Climate change, the detailed observation, research and
risk management is with a vital importance for nations in regions as the
South-East Europe. Due to insuﬃcient ground based research infrastruc-
ture and qualiﬁed personal, the satellites are main source of reliable data
of atmosphere process. The presented paper describes the basic available
functionalities of a system for automatic atmosphere events location and
transport prediction based on available open data form NASA satellites.
Keywords: Satellites · Computational physics · Natural events ·
Spatial-temporal data
1
Introduction
The diﬀerent developments due to climate change cause very strong impact on
mankind living behavior. Most of them are connected to atmosphere processes,
which are the most imminent and dynamic in impact. The range of their inﬂuence
is from brief to long term climate changes and from short haze through causing
serious damage ﬂoods to long term drought and desert spread. The ﬁrst big
serious event that was observed in contemporary history is The Year Without a
Summer in 1816 when massive eruption of Mount Tambora a year earlier caused
global cooling. Since then the interest in study of atmosphere and its impact
have been growing. As a part of this eﬀorts, with technical revolution many
new scientiﬁc instruments was involved in the ground and space observations of
atmosphere.
The most perspective and very important sources of scientiﬁc data in last
decades came from space satellites. Moreover, with their fast technical improve-
ments the possibilities for monitoring are expanded. In addition to classical mete-
orology observations, the monitoring of air quality, aerosols particles and volcano
eruptions, wildlife ﬁres and Dust storms are enabled. With acquired data and its
appropriately proceeding with computer programs it is possible to detect and
c
⃝Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8 24
www.ebook3000.com

An Automatic Tracking System
241
distinguish diﬀerent sources of pollution. A very good demonstration of eﬀec-
tiveness and usefulness of such type of analysis is the latest advanced research
of regional SO2 and NO2 pollution changes from 2005 to 2015 with data from
AURA OMI satellite [1].
The importance and usefulness of data from satellites are even more extended
in areas and territories where ground based observatories are very rare and not
well equipped. The similar region is Balkan peninsula, where only one permanent
observation facility is available. It is Basic Environmental observatory at Mous-
sala, the highest place in the Balkans [2]. However, the region is an area that
is very strongly exposed to environmental changes, caused either from anthro-
pogenic and natural factors. The region is exposed on the path of Sahara dust air
masses transport [3,4], there are many local pollutants, the devastating wildﬁres
are usual events during the late summer. Despite the fact that satellites data
are not enough detailed mainly because of lack of permanent measurements [4],
the available satellite data are important for region monitoring.
2
Automatic Detection System
2.1
Software Conﬁguration
The specially dedicated software for observation of pollutants and extreme events
detection and location is under development. For development are selected the
fully functional tools for statistical computing based on R [5]. The R environ-
ment enables the usage of all possible resources of contemporary computational
science and meteorology multiprocess and cluster computations, automatic data
acquisition, parsing and segmentation, air transport simulation and ﬁnally- the
machine learning based on all possible available statistical tools. For current
version, the system is only used for training based on historical data observa-
tions. However, the development of R and technical infrastructure allows further
functionality expansion of current system to real time analytical tool.
The model is designed to run on distributed computing infrastructure due
to design requirements of large memory and computational power. The cur-
rent results are computed on high-performance grid computing infrastructure
at Institute of Information and Communication Technologies BAS in Soﬁa [6].
The raw satellite data, available in HDF format, are acquired by standard wget
procedure. Then, the specially developed script in R processes and extracts the
required data subsets from HDF format to more convenient for processing in R
data formats. For advanced computing the High-Performance and Parallel Com-
puting functionalities in R are used [7]. With them the access to every diﬀerent
satellite parameter is autonomous and could be processed next to parametric
dependent analytic tools. The general view of the infrastructure is shown in
(Fig. 1).
The data analysis system relies mainly on quality of NASA published data.
The ﬁrst step in data processing is data extraction based on available meta-
data for validity and measurement errors. This process is run by diﬀerent
multiprocessing data drivers, each one of them enables autonomy of diﬀerent

242
A. Tchorbadjieﬀ
Fig. 1. The system relies mainly on NASA satellite open data system. The required
data is downloaded via Internet to high-performance grid computing system for com-
putation. The computations are observable and produced results are accessible through
direct link.
parameters observation. The R computational environment enhances the cus-
tomization of quick and reliable statistical analysis with acquired data. The most
possible outcomes from these analyses are air pollutant detection and large sci-
entiﬁc information for research. Then, for the further data analysis of air mass
transport, the most important information could be acquired from air mass back-
ward and forward in time trajectories. For modeling air mass transport the sys-
tem relies on NOAA HYSPLIT model dedicated for atmospheric trajectory and
dispersion calculations [8]. For automatic computations of required trajectories
and their plot in R are selected the functionalities of Openair R library [9]. The
software library is distributed with a number of R scripts, which allows complete
automatic raw meteo data acquisition from NOAA and proceeding of obtained
data to trajectory computations with custom selected parameters in both direc-
tions, backward and forward in time. The acquired results are preprocessed and
stored in shared system directory and could be accessed by diﬀerent systems
analytical drivers (Fig. 2).
2.2
Spatio-Temporal Design
The preprocessed trajectory records enable discovering of number of possible
sources or predict the destination of air pollution immediately after its detection
from satellite based on HYSPLIT trajectories. For this purpose, the statistical
analysis of merged information of satellite data and computed particle transport
trajectories is assumed to locate the most probable source and/or destination
of pollutant. This combination of remote sensing data with trajectory creates
4-dimensional spatial-temporal space. The 2-dimensional spatial domain S (lat-
itudes and longitudes) and temporal domain T (time of measurements) deﬁne
3-dimensional spatio-temporal sample z = (z(s1, t1), ..., z(sn, tn)), with locations
www.ebook3000.com

An Automatic Tracking System
243
Fig. 2. The automatic system consists of assembly of preprocessing, statistical and
trajectory computation software scripts and tools. The system management and syn-
chronization between diﬀerent parts are relied on conﬁguration ﬁle.
(s1, t1), ..., (sn, tn) ⊆R2 × R [10,11]. For statistical analysis of spatial and tem-
poral dispersion of pollutant particles are used spatial-temporal covariance for
spatial distance h and temporal distance u: [10,11]:
Cst(h, u) = cov(Z(s, t), Z(¯s, ¯t))
(1)
for (s, t), (¯s, ¯t) ∈SxT and ||s−¯s|| < h; ||t−¯t|| < u. Respectively, spatial-temporal
variograms are equal to [11]:
γst(h, u) = Cs,t(0, 0) −Cs,t(h, u)
(2)
Having computed in parallel of multiple diﬀerent in initial altitude trajecto-
ries from the same spatial-temporal location, the vertical 3-rd spatial dimension
is yielded in comparison between them.
The implemented solution is based on output results from particle transport
trajectories. They are with time resolution of 1 h. However, the satellites could
not scan observed place from same stationarity less than once a day. Due to
this constrains the diﬀerent implementations are assumed for approximation of
distances between trajectories and coordinates of remotely scanned cites. The
coincident between trajectory and satellite data is assumed to occur when the
region where both, the coordinates of the latitude and longitude of trajectory
Wt(Δi) and these of the remote scan Ws(Δi), are in the same location window
during equal time interval Δi:

|Wt(Δi) −Ws(Δi)| <= ϵ
(3)
With ϵ is denoted the maximum allowed distance. For practical analysis
three diﬀerent equal values for location with ϵ = (0.5, 1, 1.5) and time with
Δi = 3; 6; 12 h are assumed.

244
A. Tchorbadjieﬀ
3
Results and Demonstrations
For numerical demonstration the explanatory information from Sahara dust
events (SDE) over Balkan region during the Spring 2013 is used. The data are
acquired from NASA Aqua/AIRS satellite as dust score index, which is com-
puted from several diﬀerent infrared channels on board of satellite [12]. The
score values above 360 indicate presence of mineral dust in atmosphere. How-
ever, this score should not be assumed as being a quantitative measure since
this satellite is not trained on the Balkan Peninsula territory. Thus, the dust
scores with ﬂag for land uncertainties equal to 1 over observed territories may
be considered only as a qualitative conﬁrmation of SDE detection, rather than
a quantitative measure. The wrongness due to clouds were excluded because of
very high error [12].
3.1
Quantitative Results
The selected SDEs are three mineral outbursts which are detected above Balkans
on May 28th, May 19–20th and on May 16–17th [4]. The observed coordinates
are for the area above the territory of Republic of Macedonia. The dust scores
conﬁrm mineral dust arrival with relatively low values in between 360 and 370
during the midnight on May 18th. The next available observations for SDE are
from midnight of May 20th, where the score range is between 438 and 502.
The last SDE on May 29th and May 30th are conﬁrmed with day and night
observations with values between 406 and 511. The visual maps available on
NASA Worldview [14], as well as raw data directly extracted from processing
software, show that the main area for the ﬁrst event is mainly on South-East
direction and detected particles most probably are dispersed from the main
front. However, the two bigger events which occurred later completely covers
the territory of Republic of Macedonia.
For simplicity, the computed trajectories begin from place with averaged
coordinates for multiple observations during the same hour above territory of
Republic of Macedonia. But the software originally is designed to obtain coor-
dinates directly from satellite data output. The number of selected levels about
ground levels are also optional. Due to atmosphere models, the higher altitude
yields the longer directories and investigated travel distances are longer. The
demonstrative trajectories with starting point from Skopje with 4000 and 100 m
above ground level (m AGL) with time of beginning during the SDE of May
19–20th are generated and shown in Fig. 3.
The results from comparison between the events show correlation between
intensity and number of coincidences in the trajectories and satellite data over
all diﬀerent distance intervals. Because the detected particles at 18 May 2013
are from SDE which is not directly associated with the territory of Republic
of Macedonia, the number of coincidences are very lower than the direct SDE
events from May 19–20th and May 29. The magnitude of diﬀerences for these
numbers could be more than 7 times for 36 h backward trajectories. This may be
explained with the nature of particle transport. Because the event from May 18th
www.ebook3000.com

An Automatic Tracking System
245
Fig. 3. The graphics show automatically generated backward trajectories with OpenAir
library [9] in R for HYSPLIT model. The starting point is the region of Skopje in
between 0000UTC at May 19th to 0000UTC at May 21st. The starting altitude for
upper graphic is 4000 m agl and 100 m agl for lower. The backward paths are 96 h long.
is mainly associated with dispersed local transport from diﬀerent source of
mineral plume, the coincidences are missed during backward trajectories of
air-masses. The only detected reliable coincidences except those above mainly
observed territory are at nearby region above Albania and their Mediterranean
coasts, which is more likely to show availability of similar dispersed particles.
However, the coincidences during the next two events are available over the
backward part of air-masses, which conﬁrm hypothesis of direct transport.
Besides, the comparison between number of coincidences in trajectories and
satellite data conﬁrms the expectations of tradeoﬀs between lower precision
in spatial-temporal domains and the area of covered territory. The data from
tests show that when the model is run with larger window for coincidences the

246
A. Tchorbadjieﬀ
observed area is also larger, conversely to the case with more narrow observa-
tions sample space (Fig. 4). Moreover, as data from both ﬁgures show that lack
of results during backward transport is possible outcome in case of small spatial-
temporal window of coincidences. The most acceptable explanation for this are
uncertainties in trajectory models and time synchronization between them and
satellite measurements.
2013−05−26 19:00:00
2013−05−27 05:50:00
2013−05−27 16:40:00
2013−05−28 03:30:00
2013−05−28 14:20:00
2013−05−29 01:10:00
[16,115]
(115,214]
(214,313]
(313,412]
(412,511]
2013−05−26 22:00:00
2013−05−27 13:30:00
2013−05−28 05:00:00
2013−05−28 20:30:00
[16,115]
(115,214]
(214,313]
(313,412]
(412,511]
Fig. 4. The graphics show the number of registered satellite data and their Dust score
with time window Δ = 6 (left) and Δ = 3 (right) hours for 72 h long trajectory with
starting time at 29 May 2013 1200 UTC at 1000 m. AGL
The data from Table 1 show the diﬀerent pattern of computational output as
a result of the initial trajectory altitude. When the altitude is higher the longer
distance range is covered but the counts of coincidence are smaller. This could
be noticed at the Fig. 3 where long time backward trajectory paths reach the
Atlantic Ocean, the place where mineral dust is not observed. Conversely, when
the initial altitude is lower, the covered area for the same observed time window
is smaller. This allow to investigate smaller area. Following this dependency
the initial trajectory altitude is parameter dependent. In case of Sahara dust
transport the higher altitudes yields better results. But when satellite data is
used for local pollutants, as forest ﬁres for example, the very small values of
altitude, like 10 or 100 m agl for example, should be considered.
3.2
Spatial-Temporal Statistics
The geostatistical analysis conﬁrms the intuitive expectations and quantitative
results. As the variograms show, that computations with higher initial altitudes
are more suitable for long range rural transport in comparison to these with lower
ones. As the data from Table 1 show, the lower altitude trajectory missed the
source and only intercept the plume transport above Greece and Mediterranean.
These results yield random spatial-temporal distribution in diurnal basis (Fig. 5).
Conversely, the variogram produced from data form transport in higher altitudes
(2000m AGL starting point) shows clear trend in both domains - time and
www.ebook3000.com

An Automatic Tracking System
247
Table 1. The most distant area of registered Sahara dust plume during the SDE event
from May 29th 2013 with computed coincidences for 36 h long backward trajectories
for altitudes of 1000 and 2000 m above ground level.
Trajectory start time
dd-mm-yyyy hh
ϵ
Lat/Long Initial altitude of
2000 m agl
Initial altitude of
1000 m agl
29-05 2013 0900
0.5 Lat
21.16087
21.37168
Long
29.50668
36.076
29-05 2013 1200
0.5 Lat
20.86322
21.13143
Long
31.58606
36.4857
29-05 2013 1500
0.5 Lat
20.2287
20.86723
Long
31.558
36.68876
29-05 2013 1800
0.5 Lat
16.07134
20.4431
Long
38.3999
37.0137
location (Fig. 6). This trend follows intensity decrease and dispersion of dust
plume from event source place, measured with lower number and values of index
score.
However, the variograms show very strong Nugget Eﬀect - very high initial
values of γst(h = 0, u = 0) [13]. This result is due to noise from measurement
error and microscale variation in used dust score data. They are produced mainly
because dust score index records are used without any ﬁltering procedure and
their values are in the range between 1 to 511. Thus, for model improvements,
the functionalities must be expanded further with methods for ﬁltering, robust
estimation of particle dispersion and spatial prediction (kriging) [13].
distance (m)
time lag (days)
gamma
20000
25000
30000
35000
40000
45000
Fig. 5. 3D diurnal spatial-temporal variograms for a model with resolutions ϵ = 0.5
and Δi = 6 h for SDE registered on May 29th 2013 1200 UTC. The initial trajectory
resolution is 1000 m AGL (left). Backward trajectories are computed for 48 h.

248
A. Tchorbadjieﬀ
distance (m)
time lag (days)
gamma
10000
15000
20000
25000
30000
2013−05−28 12:05:13
2013−05−28 19:45:13
2013−05−29 03:25:13
[393,416.6]
(416.6,440.2]
(440.2,463.8]
(463.8,487.4]
(487.4,511]
Fig. 6. 3D diurnal spatial-temporal variogram (left) and spatial-temporal distribution
of Dust Index (right) for a model with resolutions ϵ = 0.5 and Δi = 6 h for SDE
registered on May 29th 2013 1200 UTC. The initial trajectory resolution is 2000 m.
AGL. Backward trajectories are computed for 48 h.
4
Conclusions
The results in this paper show the basic available functionalities of working model
of satellite observations of atmospheric processes. The very simple tests are
shown and its validity is conﬁrmed with very well registered Sahara Dust Events
ground measurements from BEO observatory in Moussala. However, ﬁrstly the
model requires more detailed calibration for its complete functionality with all
possible other parameters. For this purpose the detailed statistical veriﬁcation
and training with large data, obtained from satellites and ground measured qual-
ity data from diﬀerent transnational sources are needed. Secondly, the model
requires additional development for almost real-time functionality and possible
prediction features. Having prediction enabled, the system could ﬁx data gaps
of irregular satellite scans and enable possible event expectation in area even
before its detection.
References
1. Krotkov, N., et al.: Aura OMI observations of regional SO2 and NO2 pollution
changes from 2005 to 2015. Atmos. Chem. Phys. 16, 4605–4629 (2016)
2. Angelov, C., et al.: BEO Moussala a new facility for complex environment studies.
In: Sustainable Development in Mountain Regions, pp. 123–139 (2011)
3. Pey, J., et al.: African dust outbreaks over the Mediterranean Basin during 2001–
2011: PM10 concentrations, phenomenology and trends, and its relation with syn-
optic and mesoscale meteorology. Atmos. Chem. Phys. 13, 1395–1410 (2013)
4. Tchorbadjieﬀ, A., et al.: Sahara dust events over south-western Bulgaria during
the late spring of 2013. Comptes rendus de lAcadmie bulgare des Sciences 68(10),
1229–1234 (2013)
5. R-Project. https://www.r-project.org/
6. Atanassov, E., Gurov, T., Karaivanova, A., Ivanovska, S., Durchova, M., Georgiev,
D., Dimitrov, D.: Tuning for scalability on hybrid HPC cluster. In: Mathematics
in Industry, pp. 64–77. Cambridge Scholar Publishing (2014)
www.ebook3000.com

An Automatic Tracking System
249
7. Ostrouchov, G., Chen, W.-C., Schmidt, D., Patel, P.: Programming with Big Data
in R (2012). http://r-pbd.org/
8. Stein, A., Draxler, R., Rolph, G., Stunder, J., Cohen, M., Ngan, F.: NOAAs HYS-
PLIT atmospheric transport and dispersion modeling system. Bull. Am. Meteorol.
Soc. 96(12), 2059–2077 (2015)
9. Carslaw, D.C., Ropkins, K.: Openair-an R package for air quality data analysis.
Environ. Model Softw. 27–28, 52–61 (2012)
10. De Iaco, S., Myers, D.E., Posa, D.: Space-time analysis using a general product-sum
model. Stat. Probab. Lett. 52(1), 21–28 (2001)
11. Graler, B., Pebesma, E., Heuvelink, G.: Spatio-temporal interpolation using gstat.
R J. 8(1), 204–218 (2016)
12. DeSouza-Machado, S.G., et al.: Infrared retrievals of dust using AIRS: comparisons
of optical depths and heights derived for a North African dust storm to other
collocated EOS A-Train and surface observations. J. Geophys. Res. 115(D15),
CiteID D15201 (2010)
13. Cressie, N.: Statistics for Spatial Data. Wiley, New York (1993)
14. NASA Earth Data. https://earthdata.nasa.gov

Author Index
A
Ackovska, Nevena, 230
Antovski, Ljupcho, 68
B
Bakeva, Verica, 80
Banjac, Danijela, 134
Banjac, Goran, 134
Basnarkov, Lasko, 123
Bogdanova, Ana Madevska, 230
Bozinovski, Stevo, 9
Brdjanin, Drazen, 134
C
Chorbev, Ivan, 211
Cocioceanu, A., 59
D
Dimitrovski, Ivica, 49, 203
Dinkić, Nikola, 156
Džaković, Nikola, 156
Đukić, Aleksandra, 156
G
Gjorgjevikj, Ana, 182
Gjorgjievski, Sashe, 80
Gusev, Marjan, 93, 103
Guseva, Ana, 93
I
Indurkhya, Bipin, 3
Irfan Ahmed, M.S., 219
Ivănoaica, T., 59
J
Janković, Dragan, 173
Joković, Jugoslav, 156
Jovanovski, Stole, 123
K
Kalajdziski, Slobodan, 146
Kitanovski, Ivan, 203
Kostoska, Magdalena, 230
Koteska, Bojana, 230
L
Lameski, Petre, 113
Loshkovska, Suzana, 203
M
Madjarov, Gjorgji, 49
Maric, Slavko, 134
Mihajlov, Martin, 165
Milchevski, Aleksandar, 103
Milenković, Aleksandar, 173
Mileva, Aleksandra, 193
Milić, Eleonora, 173
Mirceva, Georgina, 113
Mirchev, Miroslav, 123
N
Naumoski, Andreja, 113
Nicolin, A.I., 59
P
Pandey, Amit Kumar, 41
Petrovski, Kristijan, 123
© Springer International Publishing AG 2018
G. Stojanov and A. Kulakov (eds.), ICT Innovations 2016,
Advances in Intelligent Systems and Computing 665,
https://doi.org/10.1007/978-3-319-68855-8
251
www.ebook3000.com

R
Rankovski, Gjorgji, 211
Raportaru, M.C., 59
Ribarski, Pance, 68
Ristovska, Vesna Dimitrievska, 80
Ristovski, Aleksandar, 93
Ristovski, Zlate, 146
S
Saravanan, A., 219
Sathya Bama, S., 219
Simjanoska, Monika, 230
Stoimenov, Leonid, 156
Stojanov, Riste, 182
Stojanovski, Dario, 49
Stojmenski, Aleksandar, 165
Strezoski, Gjorgji, 49
T
Tchorbadjieff, Assen, 240
Trajanov, Dimitar, 182
Trajkovikj, Vladimir, 230
Trivodaliev, Kire, 146
Trojacanec, Katarina, 203
V
Veale, Tony, 21
Velinov, Aleksandar, 193
252
Author Index


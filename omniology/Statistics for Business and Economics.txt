Marcelo Fernandes
Statistics for Business and
Economics
Download free books at

2 
Marcelo Fernandes
Statistics for Business and
Economics
Download free eBooks at bookboon.com

3 
Statistics for Business and Economics
¬© 2009 Marcelo Fernandes & Ventus Publishing ApS
ISBN 978-87-7681-481-6
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
4 
Contents
Contents
1.  
Introduction 
1.1  
Gathering data
1.2  
Data handling 
1.3  
Probability and statistical inference 
2.  
Data description 
2.1  
Data distribution 
2.2  
Typical values 
2.3  
Measures of dispersion 
3.  
Basic principles of probability 
3.1  
Set theory 
3.2  
From set theory to probability 
4.  
Probability distributions 
4.1  
Random variable 
4.2  
Random vectors and joint distributions
4.3  
Marginal distributions 
4.4  
Conditional density function 
4.5  
Independent random variables
4.6  
Expected value, moments, and co-moments
6
7
8
9
11
11
13
15
18
18
19
36
36
53
56
57
58
60
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Statistics for Business and Economics
 
Contents
5 
4.7  
Discrete distributions 
4.8  
Continuous distributions 
5.  
Random sampling 
5.1  
Sample statistics 
5.2  
Large-sample theory
6.  
Point and interval estimation 
6.1  
Point estimation 
6.2  
Interval estimation 
7.  
Hypothesis testing 
7.1  
Rejection region for sample means 
7.2  
Size, level, and power of a test 
7.3  
Interpreting p-values
7.4  
Likelihood-based tests
74
87
95
99
102
107
108
121
127
131
136
141
142
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Statistics for Business and Economics
 
6 
Chapter 1
Introduction
This compendium aims at providing a comprehensive overview of the main topics that ap-
pear in any well-structured course sequence in statistics for business and economics at the
undergraduate and MBA levels. The idea is to supplement either formal or informal statistic
textbooks such as, e.g., ‚ÄúBasic Statistical Ideas for Managers‚Äù by D.K. Hildebrand and R.L.
Ott and ‚ÄúThe Practice of Business Statistics: Using Data for Decisions‚Äù by D.S. Moore,
G.P. McCabe, W.M. Duckworth and S.L. Sclove, with a summary of theory as well as with
a couple of extra examples. In what follows, we set the road map for this compendium by
describing the main steps of statistical analysis.
Introduction
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Statistics for Business and Economics
 
7 
Statistics is the science and art of making sense of both quantitative and qualitative data.
Statistical thinking now dominates almost every Ô¨Åeld in science, including social sciences such
as business, economics, management, and marketing. It is virtually impossible to avoid data
analysis if we wish to monitor and improve the quality of products and processes within a
business organization. This means that economists and managers have to deal almost daily
with data gathering, management, and analysis.
1.1
Gathering data
Collecting data involves two key decisions. The Ô¨Årst refers to what to measure. Unfortu-
nately, it is not necessarily the case that the easiest-to-measure variable is the most relevant
for the speciÔ¨Åc problem in hand. The second relates to how to obtain the data. Sometimes
gathering data is costless, e.g., a simple matter of internet downloading. However, there are
many situations in which one must take a more active approach and construct a data set
from scratch.
Data gathering normally involves either sampling or experimentation. Albeit the latter
is less common in social sciences, one should always have in mind that there is no need for a
lab to run an experiment. There is pretty of room for experimentation within organizations.
And we are not speaking exclusively about research and development. For instance, we could
envision a sales competition to test how salespeople react to diÔ¨Äerent levels of performance
incentives. This is just one example of a key driver to improve quality of products and
processes.
Sampling is a much more natural approach in social sciences. It is easy to appreciate
that it is sometimes too costly, if not impossible, to gather universal data and hence it makes
sense to restrict attention to a representative sample of the population. For instance, while
census data are available only every 5 or 10 years due to the enormous cost/eÔ¨Äort that it
involves, there are several household and business surveys at the annual, quarterly, monthly,
and sometimes even weekly frequency.
Introduction
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
8 
1.2
Data handling
Raw data are normally not very useful in that we must normally do some data manipulation
before carrying out any piece of statistical analysis. Summarizing the data is the primary
tool for this end.
It allows us not only to assess how reliable the data are, but also to
understand the main features of the data. Accordingly, it is the Ô¨Årst step of any sensible
data analysis.
Summarizing data is not only about number crunching. Actually, the Ô¨Årst task to trans-
form numbers into valuable information is invariably to graphically represent the data. A
couple of simple graphs do wonders in describing the most salient features of the data. For
example, pie charts are essential to answer questions relating to proportions and fractions.
For instance, the riskiness of a portfolio typically depends on how much investment there
is in the risk-free asset relative to the overall investment in risky assets such as those in
the equity, commodities, and bond markets. Similarly, it is paramount to map the source
of problems resulting in a warranty claim so as to ensure that design and production man-
agers focus their improvement eÔ¨Äorts on the right components of the product or production
process.
The second step is to Ô¨Ånd the typical values of the data. It is important to know, for
example, what is the average income of the households in a given residential neighborhood if
you wish to open a high-end restaurant there. Averages are not suÔ¨Écient though, for interest
may sometimes lie on atypical values. It is very important to understand the probability
of rare events in risk management. The insurance industry is much more concerned with
extreme (rare) events than with averages.
The next step is to examine the variation in the data. For instance, one of the main
tenets of modern Ô¨Ånance relates to the risk-return tradeoÔ¨Ä, where we normally gauge the
riskiness of a portfolio by looking at how much the returns vary in magnitude relative to
their average value. In quality control, we may improve the process by raising the average
Introduction
Download free eBooks at bookboon.com

 
9 
quality of the Ô¨Ånal product as well as by reducing the quality variability. Understanding
variability is also key to any statistical thinking in that it allows us to assess whether the
variation we observe in the data is due to something other than random variation.
The Ô¨Ånal step is to assess whether there is any abnormal pattern in the data. For instance,
it is interesting to examine nor only whether the data are symmetric around some value but
also how likely it is to observe unusually high values that are relatively distant from the bulk
of data.
1.3
Probability and statistical inference
It is very diÔ¨Écult to get data for the whole population. It is very often the case that it is
too costly to gather a complete data set about a subset of characteristics in a population,
either because of economic reasons or because of the computational burden. For instance, it
is impossible for a Ô¨Årm that produces millions and millions of nails every day to check each
one of their nails for quality control. This means that, in most instances, we will have to
examine data coming from a sample of the population.
Statistics for Business and Economics
Introduction
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Statistics for Business and Economics
 
10 
As a sample is just a glimpse of the entire population, it will entail some degree of uncer-
tainty to the statistical problem. To ensure that we are able to deal with this uncertainty, it
is very important to sample the data from its population in a random manner, otherwise
some sort of selection bias might arise in the resulting data sample. For instance, if you wish
to assess the performance of the hedge fund industry, it does not suÔ¨Éce to collect data about
living hedge funds. We must also collect data on extinct funds for otherwise our database
will be biased towards successful hedge funds. This sort of selection bias is also known as
survivorship bias.
The random nature of a sample is what makes data variability so important. Probability
theory essentially aims to study how this sampling variation aÔ¨Äects statistical inference,
improving our understanding how reliable our inference is. In addition, inference theory is
one of the main quality-control tools in that it allows to assess whether a salient pattern
in data is indeed genuine beyond reasonable random variation. For instance, some equity
fund managers boast to have positive returns for a number of consecutive periods as if this
would entail unrefutable evidence of genuine stock-picking ability. However, in a universe of
thousands and thousands of equity funds, it is more than natural that, due to sheer luck,
a few will enjoy several periods of positive returns even if the stock returns are symmetric
around zero, taking positive and negative values with equal likelihood.
Introduction
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
11 
Chapter 2
Data description
The Ô¨Årst step of data analysis is to summarize the data by drawing plots and charts as well
as by computing some descriptive statistics. These tools essentially aim to provide a better
understanding of how frequent the distinct data values are, and of how much variability
there is around a typical value in the data.
2.1
Data distribution
It is well known that a picture tells more than a million words. The same applies to any
serious data analysis for graphs are certainly among the best and most convenient data
descriptors. We start with a very simple, though extremely useful, type of data plot that
reveals the frequency at which any given data value (or interval) appears in the sample. A
frequency table reports the number of times that a given observation occurs or, if based
on relative terms, the frequency of that value divided by the number of observations in the
sample.
Example
A Ô¨Årm in the transformation industry classiÔ¨Åes the individuals at managerial
positions according to their university degree. There are currently 1 accountant, 3 adminis-
trators, 4 economists, 7 engineers, 2 lawyers, and 1 physicist. The corresponding frequency
table is as follows.
Data description
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
12 
degree
accounting
business
economics
engineering
law
physics
value
1
2
3
4
5
6
counts
1
3
4
7
2
1
relative frequency
1/18
1/6
2/9
7/18
1/9
1/18
Note that the degree subject that a manager holds is of a qualitative nature, and so it is not
particularly meaningful if one associates a number to each one of these degrees. The above
table does so in the row reading ‚Äòvalue‚Äô according to the alphabetical order, for instance.
The corresponding plot for this type of categorical data is the bar chart. Figure 2.1 plots
a bar chart using the degrees data in the above example. This is the easiest way to identify
particular shapes of the distribution of values, especially concerning data dispersion. Least
data concentration occurs if the envelope of the bars forms a rectangle in that every data
value appears at approximately the same frequency.
Data description
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Statistics for Business and Economics
 
13 
In statistical quality control, one very often employs bar charts to illustrate the reasons
for quality failures (in order of importance, i.e., frequency). These bar charts (also known
as Pareto charts in this particular case) are indeed very popular for highlighting the natural
focus points for quality improvement.
Bar charts are clearly designed to describe the distribution of categorical data. In a similar
vein, histograms are the easiest graphical tool for assessing the distribution of quantitative
data. It is often the case that one must Ô¨Årst group the data into intervals before plotting a
histogram. In contrast to bar charts, histogram bins are contiguous, respecting some sort of
scale.
0
1
2
3
4
5
6
7
8
accounting
business
economics
engineering
law
physics
Figure 2.1: Bar chart of managers‚Äô degree subjects
2.2
Typical values
There are three popular measures of central tendency: mode, mean, and median. The mode
refers to the most frequent observation in the sample. If a variable may take a large number
of values, it is then convenient to group the data into intervals. In this instance, we deÔ¨Åne the
Data description
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
14 
mode as the midpoint of the most frequent interval. Even though the mode is a very intuitive
measure of central tendency, it is very sensitive to changes, even if only marginal, in data
values or in the interval deÔ¨Ånition. The mean is the most commonly-used type of average
and so it is often referred to simply as the average. The mean of a set of numbers is the sum
of all of the elements in the set divided by the number of elements: i.e., ¬ØXN = 1
N
N
i=1 Xi. If
the set is a statistical population, then we call it a population mean or expected value. If the
data set is a sample of the population, we call the resulting statistic a sample mean. Finally,
we deÔ¨Åne the median as the number separating the higher half of a sample/population from
the lower half. We can compute the median of a Ô¨Ånite set of numbers by sorting all the
observations from lowest value to highest value and picking the middle one.
Example
Consider a sample of MBA graduates, whose Ô¨Årst salaries (in $1,000 per annum)
after graduating were as follows.
75
86
86
87
89
95
95
95
95
95
96
96
96
97
97
97
97
98
98
99
99
99
99
100
100
100
105
110
110
110
115
120
122
125
132
135
140
150
150
160
165
170
172
175
185
190
200
250
250
300
The mean salary is about $126,140 per annum, whereas the median Ô¨Ågure is exactly $100,000
and the mode amounts to $95,000. Now, if one groups the data into 8 evenly distributed
bins between the minimum and maximum values, both the median and mode converge to
same value of about $91,000 (i.e., the midpoint of the second bin).
The mean value plays a major role in statistics. Although the median has several ad-
vantages over the mean, the latter is easier to manipulate for it involves a simple linear
combination of the data rather than a non-diÔ¨Äerentiable function of the data as the median.
In statistical quality control, for instance, it is very common to display a means chart (also
known as x-bar chart), which essentially plots the mean of a variable through time. We
Data description
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
15 
say that a process is in statistical control if the means vary randomly but in a stable fash-
ion, whereas it is out of statistical control if the plot shows either a dramatic variation or
systematic changes.
2.3
Measures of dispersion
While measures of central tendency are useful to understand what are the typical values
of the data, measures of dispersion are important to describe the scatter of the data or,
equivalently, data variability with respect to the central tendency. Two distinct samples
may have the same mean or median, but diÔ¨Äerent levels of variability, or vice-versa.
A
proper description of data set should always include both of these characteristics. There are
various measures of dispersion, each with its own set of advantages and disadvantages.
We Ô¨Årst deÔ¨Åne the sample range as the diÔ¨Äerence between the largest and smallest values
in the sample. This is one of the simplest measures of variability to calculate. However, it
depends only on the most extreme values of the sample, and hence it is very sensitive to
outliers and atypical observations. In addition, it also provides no information whatsoever
about the distribution of the remaining data points. To circumvent this problem, we may
think of computing the interquartile range by taking the diÔ¨Äerence between the third and Ô¨Årst
quartiles of the distribution (i.e., subtracting the 25th percentile from the 75th percentile).
This is not only a pretty good indicator of the spread in the center region of the data, but
it is also much more resistant to extreme values than the sample range.
We now turn our attention to the median absolute deviation, which renders a more
comprehensive alternative to the interquartile range by incorporating at least partially the
information from all data points in the sample. We compute the median absolute deviation
by means of md |Xi ‚àímd(X)|, where md(¬∑) denotes the median operator, yielding a very
robust measure of dispersion to aberrant values in the sample. Finally, the most popular
measure of dependence is the sample standard deviation as deÔ¨Åned by the square root of
the sample variance: i.e., sN =

1
N‚àí1
N
i=1

Xi ‚àí¬ØXN
2, where ¬ØXN is the sample mean.
Data description
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
16 
The main advantage of variance-based measures of dispersion is that they are functions of
a sample mean. In particular, the sample variance is the sample mean of the square of the
deviations relative to the sample mean.
Example
Consider the sample of MBA graduates from the previous example.
The
variance of their Ô¨Årst salary after graduating is about $2,288,400,000 per annum, whereas
the standard deviation is $47,837.
The range is much larger, amounting to 300, 000 ‚àí
75, 000 = 225, 000 per annum. The huge diÔ¨Äerence between these two measures of dispersion
suggests the presence of extreme values in the data. The fact that the interquartile range is
150,000+150,000
2
‚àí96,000+96,000
2
= 54, 000‚Äîand hence closer the the standard deviation‚Äîseems
to corroborate this interpretation. Finally, the median absolute deviation of the sample is
only 10,000 indicating that the aberrant values of the sample are among the largest (rather
than smallest) values.
Data description
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Statistics for Business and Economics
 
17 
In statistical quality control, it is also useful to plot some measures of dispersion over
time. The most common are the R and S charts, which respectively depict how the range
and the standard deviation vary over time. The standard deviation is also informative in a
means chart for the interval [mean value ¬± two standard deviations] contains about 95% of
the data if their histogram is approximately bell-shaped (symmetric with a single peak). An
alternative is to plot control limits at the mean value ¬± three standard deviations, which
should include all of the data inside. These procedures are very useful in that they reduce
the likelihood of a manager to go Ô¨Åre-Ô¨Åghting every short-term variation in the means chart.
Only variations that are very likely to reÔ¨Çect something out of control will fall outside the
control limits.
A well-designed statistical quality-control system should take both means and dispersion
charts into account for it is possible to improve on quality by reducing variability and/or
by increasing average quality. For instance, a chef that reduces cooking time on average by
5 minutes, with 90% of the dishes arriving 10 minutes earlier and 10% arriving 40 minutes
later, will probably not make the owner of the restaurant very happy.
Data description
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
18 
Chapter 3
Basic principles of probability
3.1
Set theory
There are two fundamental sets, namely, the universe U and the empty set ‚àÖ. We say they
are fundamental because ‚àÖ‚äÜA ‚äÜU for every set A.
Taking the diÔ¨Äerence between sets A and B yields a set whose elements are in A but
not in B: A ‚àíB = {x | x ‚ààA and x /‚ààB}.
Note that A ‚àíB is not necessarily the
same as B ‚àíA. The union of A and B results in a set whose elements are in A or in B:
A‚à™B = {x | x ‚ààA or x ‚ààB}. Naturally, if an element x belongs to both A and B, then it is
also in the union A ‚à™B. In turn, the intersection of A and B individuates only the elements
that both sets share in common: A ‚à©B = {x | x ‚ààA and x ‚ààB}. Last but not least, the
complement ¬ØA of A deÔ¨Ånes a set with all elements in the universe that are not in A, that is
to say, ¬ØA = U ‚àíA = {x | x /‚ààA}.
Example
Suppose that you roll a die and take note of the resulting value. The universe
is the set with all possible values, namely, U = {1, 2, 3, 4, 5, 6}. Consider the following two
sets: A = {1, 2, 3, 4} and B = {2, 4, 6}. It then follows that A ‚àíB = {1, 3}, B ‚àíA = {6},
A ‚à™B = {1, 2, 3, 4, 6}, and A ‚à©B = {2, 4}.
If A and B are complementing sets, i.e., A = ¬ØB, then A‚àíB = A, B ‚àíA = B, A‚à™B = U,
and A ‚à©B = ‚àÖ. Figure 3.1 illustrates how one may represent sets using a Venn diagram.
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
Figure 3.1: Venn diagram representing sets A (oval in blue and purple) and B (oval in red
and purple) within the universe (rectangle box). The intersection A ‚à©B of A and B is in
purple, whereas the overall area in color (i.e., red, blue, and purple) corresponds to the union
set A ‚à™B. The complement of A consists of the areas in grey and red, whereas the areas in
grey and blue deÔ¨Åne the complement of B.
Properties
The union and intersection operators are symmetric in that A ‚à™B = B ‚à™A
and A ‚à©B = B ‚à©A. They are also transitive in that (A ‚à™B) ‚à™C = A ‚à™(B ‚à™C) and
(A ‚à©B) ‚à©C = A ‚à©(B ‚à©C).
From the above properties, it is straightforward to show that the following identities hold:
(I1) A ‚à™(B ‚à©C) = (A ‚à™B) ‚à©(A ‚à™C), (I2) A ‚à©(B ‚à™C) = (A ‚à©B) ‚à™(A ‚à©C), (I3) A ‚à©‚àÖ= ‚àÖ,
(I4) A ‚à™‚àÖ= A, (I5) A ‚à©B = ¬ØA ‚à™¬ØB, (I6) A ‚à™B = ¬ØA ‚à©¬ØB, and (I7) A = A.
3.2
From set theory to probability
The probability counterpart for the universe in set theory is the sample space S. Similarly,
probability focus on events, which are subsets of possible outcomes in the sample space.
Example
Suppose we wish to compute the probability of getting an even value in a die
roll. The sample space is the universe of possible outcomes S = {1, 2, 3, 4, 5, 6}, whereas the
event of interest corresponds to the set {2, 4, 6}.
19 
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
20 
To combine events, we employ the same rules as for sets. Accordingly, the event A ‚à™B
occurs if and only if we observe an outcome that belongs to A or to B, whereas the event
A‚à©B occurs if and only if both A and B happen. It is also straightforward to combine more
than two events in that ‚à™n
i=1Ai occurs if and only if at least one of the events Ai happens,
whereas ‚à©n
i=1Ai holds if and only if every event Ai occur for i = 1, . . . , n. In the same vein,
the event ¬ØA occurs if and only if we do not observe any outcome that belongs to the event
A. Finally, we say that two events are mutually exclusive if A ‚à©B = ‚àÖ, that is to say, they
never occur at the same time. Mutually exclusive events are analogous to mutually exclusive
sets in that their intersection is null.
Basic principles of probability
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
21 
3.2.1
Relative frequency
Suppose we repeat a given experiment n times and count how many times, say nA and
nB, the events A and B occur, respectively. It then follows that the relative frequency of
event A is fA = nA/n, whereas it is fB = nB/n for event B. In addition, if events A and
B are mutually exclusive (i.e., A ‚à©B = ‚àÖ), then the relative frequency of C = A ‚à™B is
fC = (nA + nB)/n = fA + fB.
The relative frequency of any event is always between zero and one. Zero corresponds
to an event that never occurs, whereas a relative frequency of one means that we always
observe that particular event. The relative frequency is very important for the fundamental
law of statistics (also known as the Glivenko-Cantelli theorem) says that, as the number of
experiments n grows to inÔ¨Ånity, it converges to the probability of the event: fA ‚ÜíPr(A).
Chapter 5 discusses this convergence in more details.
Example
The Glivenko-Cantelli theorem is the principle underlying many sport compe-
titions. The NBA play-oÔ¨Äs are a good example. To ensure that the team with the best odds
succeed, the playoÔ¨Äs are such that a team must win a given number of games against the
same adversary before qualifying to the next round.
3.2.2
Event probability
It now remains to deÔ¨Åne what we exactly mean with the notion of probability. We associate
a real number to the probability of observing the event A, denoted by Pr(A), satisfying the
following properties:
P1
0 ‚â§Pr(A) ‚â§1;
P2
Pr(S) = 1;
P3
Pr(A ‚à™B) = Pr(A) + Pr(B) if A ‚à©B = ‚àÖ;
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
22 
P4
Pr(‚à™n
i=1Ai) = n
i=1 Pr(Ai) if the collection of events {Ai, i = 1, . . . , n} is pairwise
mutually exclusive even if n ‚Üí‚àû.
It is easy to see that P4 follows immediately from P3 if we restrict attention to a Ô¨Ånite
number of experiments (n < ‚àû). From properties P1 to P4, it is possible to derive some
important results concerning the diÔ¨Äerent ways we may combine events.
Result
It follows from P1 to P4 that
(a) Pr(‚àÖ) = 0,
(b) Pr( ¬ØA) = 1 ‚àíPr(A),
(c) Pr(A ‚à™B) = Pr(A) + Pr(B) ‚àíPr(A ‚à©B), and
(d) Pr(A) ‚â§Pr(B) if A ‚äÜB.
Proof: (a) By deÔ¨Ånition, the probability of event A is the same as the probability of the
union of A and ‚àÖ, viz. Pr(A) = Pr(A ‚à™‚àÖ). However, A and ‚àÖare mutually exclusive events
in that A‚à©‚àÖ= ‚àÖ, implying that Pr(A) = Pr(A)+Pr(‚àÖ) by P3. (b) By deÔ¨Ånition, A‚à™¬ØA = S
and A ‚à©¬ØA = ‚àÖ, and so Pr(S) = Pr(A ‚à™¬ØA) = Pr(A) + Pr( ¬ØA) = 1 by P2 and P3. (c) It
is straightforward to observe that A ‚à™B = A ‚à™(B ‚à©¬ØA) and that A ‚à©(B ‚à©¬ØA) = ‚àÖfor the
event within parentheses consists of all outcomes in B that are not in A. It thus ensues that
Pr(A ‚à™B) = Pr

A ‚à™(B ‚à©¬ØA)

= Pr(A) + Pr(B ‚à©¬ØA). We now decompose the event B into
outcomes that belong and not belong to A: B = (A ‚à©B) ‚à™(B ‚à©¬ØA). There is no intersection
between these two terms, hence Pr(B)‚àíPr(A‚à©B) = Pr(B‚à©¬ØA), yielding the result. (d) The
previous decomposition reduces to B = A ‚à™(B ‚à©¬ØA) given that A ‚à©B = A. It then follows
that Pr(B) = Pr(A) + Pr(B ‚à©¬ØA) ‚â§Pr(A) in view that any probability is nonnegative.
‚ñ†
3.2.3
Finite sample space
A Ô¨Ånite sample space must have only a Ô¨Ånite number of elements, say, {a1, a2, . . . , an}. Let
pj denote the probability of observing the corresponding event {aj}, for j = 1, . . . , n. It is
easy to appreciate that 0 ‚â§pj ‚â§1 for all j = 1, . . . , n and that n
j=1 pj = 1 given that the
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
23 
events (a1, . . . , an) span the whole sample space. As the latter are also mutually exclusive,
it follows that Pr(A) = pj1 + . . . , +pjk = k
r=1 pjr for A = {aj1, . . . , ajk}, with 1 ‚â§k ‚â§n.
Example:
The sample space corresponding to the value we obtain by throwing a die is
{1, 2, 3, 4, 5, 6} and the probability pj of observing any value j ‚àà{1, . . . , 6} is equal to 1/6.
In general, if every element in the sample space is equiprobable, then the probability of
observing a given event is equal to the ratio between the number of elements in the event
and the number of elements in the sample space.
Examples
(1) Suppose the interest lies on the event of observing a value above 4 in a die throw. There
are only two values in the sample space that satisfy this condition, namely, {5, 6}, and hence
the probability of this event is 2/6 = 1/3.
(2)
Consider now Ô¨Çipping twice a coin and recording the heads and tails. The resulting
sample space is {HH, HT, TH, TT}. As the elements of the sample space are equiprobable,
the probability of observing only one head is
#{HT,TH}
#{HH,HT,TH,TT} = 2/4 = 1/2.
These examples suggest that the most straightforward manner to compute the proba-
bility of a given event is to run experiments in which the elements of the sample space are
equiprobable. Needless to say, it is not always very easy to contrive such experiments. We
illustrate this issue with another example.
Example:
Suppose one takes a nail from a box containing nails of three diÔ¨Äerent sizes.
It is typically easier to grab a larger nail than a small one and hence such an experiment
would not yield equiprobable outcomes. However, the alternative experiment in which we
Ô¨Årst numerate the nails and then draw randomly a number to decide which nail to take
would lead to equiprobable results.
Basic principles of probability
Download free eBooks at bookboon.com

 
24 
3.2.4
Back to the basics: Learning how to count
The last example of the previous section illustrates a situation in which it is straightforward
to redesign the experiment so as to induce equiprobable outcomes. Life is tough, though,
and such an instance is the exception rather than the rule. For instance, a very common
problem in quality control is to infer from a small random sample the probability of observing
a given number of defective goods within a lot. This is evidently a situation that does not
automatically lead to equiprobable outcomes given the sequential nature of the experiment.
To deal with such a situation, we must Ô¨Årst learn how to count the possible outcomes using
some tools of combinatorics.
Multiplication
Consider that an experiment consists of a sequence of two procedures,
say, A and B. Let nA and nB denote the number of ways in which one can execute A and B,
respectively. It then follows that there is n = nA nB ways of executing such an experiment.
In general, if the experiment consists of a sequence of k procedures, then one may run it in
n = k
i=1 ni diÔ¨Äerent ways.
Law for Computing Students
Basic principles of probability
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Statistics for Business and Economics
 
25 
Addition
Suppose now that the experiment involves k procedures in parallel (rather
than in sequence). This means that we either execute the procedure 1 or the procedure 2
or . . . or the procedure k. If ni denotes the number of ways that one may carry out the
procedure i ‚àà{1, . . . , k}, then there are n = n1 + ¬∑ ¬∑ ¬∑ + nk = k
i=1 ni ways of running such
an experiment.
Permutation
Suppose now that we have a set of n diÔ¨Äerent elements and we wish to
know the number of sequences we can construct containing each element once, and only once.
Note that the concept of sequence is distinct from that of a set, in that order of appear-
ance matters. For instance, the sample space {a, b, c} allows for the following permutations
(abc, acb, bac, bca, cab, cba). In general, there are n! = n‚àí1
j=0(n ‚àíj) possible permutations
out of n elements because there are n options for the Ô¨Årst element of the sequence, but only
n ‚àí1 options for the second element, n ‚àí2 options for the third element and so on until we
have only one remaining option for the last element of the sequence. There is also a more
general meaning for permutation in combinatorics for which we form sequences of k diÔ¨Äerent
elements from a set of n elements. This means that we have n options for the Ô¨Årst element
of the sequence, but then n ‚àí1 options for the second element and so on until we have only
n‚àík+1 options for the last element of the sequence. It thus follows that we have n!/(n‚àík)!
permutations of k out of n elements in this broader sense.
Combination
This is a notion that only diÔ¨Äers from permutation in that ordering does
not matter. This means that we just wish to know how many subsets of k elements we can
construct out of a set of n elements. For instance, it is possible to form the following subsets
with two elements of {a, b, c, d}: {a, b}, {a, c}, {a, d}, {b, c}, {b, d}, and {c, d}. Note that
{b, a} does not count because it is exactly the same subset as {a, b}. This suggests that, in
general, the number of combinations is inferior to the number of permutations because one
must count only one of the sequences that employ the same elements but with a diÔ¨Äerent
ordering. In view that there are n!/(n ‚àír)! permutations of k out of n elements and k! ways
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
26 
to choose the ordering of these k elements, the number of possible combinations of k out of
n elements is
	n
k

=
n!
(n ‚àík)! k!.
Before we revisit the original quality control example, it is convenient to illustrate the
use of the above combinatoric tools through another example.
Example:
Suppose there is a syndicate with 5 engineers and 3 economists. How many
committees of 3 people one can form with exactly 2 engineers? Well, we must form commit-
tees of 2 engineers and 1 economist. There are
5
2

ways of choosing 2 out of 5 engineers,
whereas there are
3
1

ways of choosing 1 out of 3 economists. Altogether, this means that
one can form
5
2
3
1

= 30 committees with 2 engineers and 1 economist out of a group of 5
engineers and 3 economists.
We are now ready to reconsider the quality control problem of inferring the number of
defective goods within a lot. Suppose, for instance, that a lot has n objects of which nd are
defective and that we draw a sample of k elements of which kd are defective. We Ô¨Årst note
that there are
n
k

ways of choosing k elements from a lot of n goods, whereas there are
nd
kd

ways of combining kd defective goods from a total of nd defective goods within the lot as
well as
n‚àínd
k‚àíkd

ways of choosing (k ‚àíkd) elements out of the (n ‚àínd) non-defective goods
within the lot. Accordingly, the probability of observing kd defective goods within a sample
of k goods is
 k
kd
n‚àínd
k‚àíkd

n
k

(3.1)
if there are nd defective goods within a lot of n objects.
3.2.5
Conditional probability
We denote by Pr(A|B) the probability of event A given that we have already observed event
B. Intuitively, conditioning on the realization of a given event has the eÔ¨Äect of reducing the
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
27 
sample space from S to the sample space spanned by B.
Examples
(1) Suppose that we throw a die twice. In the Ô¨Årst throw, we observe a value equal to 6
and we wish to know what is the probability of observing a value of 2 in the second throw.
In this instance, the fact that we have observed a value of 6 in the Ô¨Årst throw has no impact
in the value we will observe in the second throw for the two events are independent. This
means that the Ô¨Årst value brings about no information about the second throw and hence
the probability of observing a value of 2 in the second throw given that we have observed
a value of 6 in the Ô¨Årst throw remains the same as before, that is to say, the probability of
observing a value of 2: 1/6.
Basic principles of probability
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Statistics for Business and Economics
 
28 
(2)
Next, consider A = {(x1, x2)|x1 + x2 = 10} = {(5, 5), (6, 4), (4, 6)} and B =
{(x1, x2)|x1 > x2} = {(2, 1), (3, 2), (3, 1), ¬∑ ¬∑ ¬∑ , (6, 5)}.
The probability of A is Pr(A) =
3/36 = 1/12, whereas the probability of B is Pr(B) = 15/36. In addition, the probabil-
ity of observing both A and B is Pr(A ‚à©B) = 1/36. It thus turns out that the probability
of observing A given B is Pr(A|B) = 1/15 = Pr(A ‚à©B)/Pr(B), whereas the probability of
observing B given A is Pr(B|A) = 1/3 = Pr(A ‚à©B)/Pr(A).
It is obviously not by chance that, in general, Pr(A|B) = Pr(A‚à©B)/Pr(B), for Pr(B) > 0.
By conditioning on event B we are restricting the sample space to B and hence we must
consider the probability of observing both A and B and then normalize by the measure of
event B. It is as if we were computing the relative frequency at which the event A occurs
given the outcomes that are possible within event B. This notion makes sense even if we
consider unconditional events. Indeed, the unconditional probability of A is the conditional
probability of A given the sample space S, i.e., Pr(A|S) = Pr(A ‚à©S)/Pr(S) = Pr(A).
Finally, it is also interesting to note that we may decompose the probability of A ‚à©B into a
conditional probability and a marginal probability, namely, Pr(A ‚à©B) = Pr(A|B) Pr(B) =
Pr(B|A) Pr(A).
Example:
Suppose that a computer lab has 4 new and 2 old desktops running Windows
as well as 3 new and 1 old desktops running Linux. What is the probability of a student
to randomly sit in front of a desktop running Windows? What is the likelihood that this
particular desktop is new given that it runs Windows? Well, there are 10 computers in the
lab of which 6 run Windows. This means that the answer of the Ô¨Årst question is 3/5, whereas
Pr(new|Windows) = Pr(new ‚à©Windows)/Pr(Windows) = (4/10)/(6/10) = 2/3.
Figure 3.2.5 illustrates a situation in which the events A and B are mutually exclusive and
hence A ‚à©B = ‚àÖ. In this instance, the probability of both events occurring is obviously zero
and so are both conditional probabilities, i.e., Pr(A‚à©B) = 0 ‚áíPr(A|B) = Pr(B|A) = 0. In
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
29 
contrast, Figure 3.3 depicts another polar case: A ‚äÇB. Now, Pr(A ‚à©B) = Pr(A), whereas
Pr(A|B) = Pr(A)/Pr(B) and Pr(B|A) = 1.
Decomposing a joint probability into the product of a conditional probability and of a
marginal probability is a very useful tool, especially if one combines it with partitions of the
sample space. Let B1, . . . , Bk denote a partition of the sample space S, that is to say,
(a)
Bi ‚à©Bj = ‚àÖ,
1 ‚â§i Ã∏= j ‚â§k
(b)
‚à™k
i=1Bi = S
(c)
Pr(Bi) > 0,
1 ‚â§i ‚â§k.
This partition yields the decomposition A = (A ‚à©B1) ‚à™(A ‚à©B2) ‚à™. . . ‚à™(A ‚à©Bk) for any
event A ‚ààS. The nice thing about partitions is that they are mutually exclusive and hence
(A ‚à©Bi) ‚à©(A ‚à©Bj) = ‚àÖfor any 1 ‚â§i Ã∏= j ‚â§k. This means that Pr(A) = k
i=1 Pr(A ‚à©Bi) =
i=1
k
Pr(A|Bi) Pr(Bi).
Figure 3.2: Venn diagram representing two mutually exclusive events A (oval in blue) and
B (oval in red) within the sample space (rectangle box).
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
Figure 3.3: Venn diagram representing events A (oval in purple) and B (oval in red and
purple) within the sample space (rectangle box) such that A ‚à©B = A.
For instance, if we deÔ¨Åne the sample space by the possible outcomes of a die throw, we
may think of several distinct partitions as, for example,
(a)
Bi = {i}
for i = 1, . . . , 6
(b)
B1 = {1, 3, 5}, B2 = {2, 4, 6}
(c)
B1 = {1, 2}, B2 = {3, 4, 5}, B3 = {6}.
Example:
Consider a lot of 100 frying pans of which 20 are defective. DeÔ¨Åne the events A =
{Ô¨Årst frying pan is defective} and B = {second frying pan is defective} within a context of
sequential sampling without reposition.
The probability of observing event B naturally
depends on whether the Ô¨Årst frying pan is defective or not. Now, there are only two possible
outcomes in that the Ô¨Årst frying pan is either defective or not. This suggests a very simple
partition of the sample space based on A and ¬ØA, giving way to Pr(B) = Pr(B|A) Pr(A) +
Pr(B| ¬ØA) Pr( ¬ØA). In particular, Pr(B|A) = 19/99 for there are only 19 defective frying pans
left among the remaining 99 frying pans if A is true. Similarly, Pr(B| ¬ØA) = 20/99, whereas
Pr(A) = 1/5 and Pr( ¬ØA) = 1‚àíPr(A) = 4/5. We thus conclude that Pr(B) = 19
99
1
5 + 20
99
4
5 = 1
5.
30 
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
31 
In some instances, we cannot observe some events, and hence we must infer whether
they are true or false given the available information. For instance, if you are in a building
with no windows and someone arrives completely soaked with a broken umbrella, it sounds
reasonable to infer that it is raining outside even if you cannot directly observe the weather.
The Bayes rule formalizes how one should conduct such an inference based on conditional
probabilities:
Pr(Bi|A) =
Pr(A|Bi) Pr(Bi)
k
j=1 Pr(A|Bj) Pr(Bj)
i = 1, ¬∑ ¬∑ ¬∑ , k
where B1, . . . , Bk is a partition of the sample space.
In the example above, we cannot
observe whether it is raining, but we may partition the sample space (i.e., weather) into
B = {it is raining} and ¬ØB = {it is not raining}, and then calculate the probability of B given
that we observe event A = {someone arrives completely soaked with a broken umbrella}.
Basic principles of probability
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
32 
The Bayes rule has innumerable applications in business, economics and Ô¨Ånance. For
instance, imagine you are the market maker for a given stock and that there are both
informed and uninformed traders in the market. In contrast to informed traders, you do not
know whether news are good or bad and hence you must infer it from the trades you observe
in order to adjust your bid and ask quotes accordingly. If you observe much more traders
buying than selling, then you will assign a higher probability to good news. If traders are
selling much more than buying, then the likelihood of bad news rises. The Bayes rule is the
mechanism at which you learn whether news are good or bad by looking at trades.
3.2.6
Independent events
Consider for a moment two mutually exclusive events A and B. Knowing about A gives
loads of information about the likelihood of event B. In particular, if A occurs, we know for
sure that event B did not occur. More formally, the conditional probability of B given that
we observe A is Pr(B|A) = Pr(A ‚à©B)/Pr(A) = 0 given that A ‚à©B = ‚àÖ(see Figure 3.2.5).
We thus conclude that A and B are dependent events given that knowing about one entails
complete information about the other. Following this reasoning, it makes sense to associate
independence with lack of information content. We thus say that A and B are independent
events if and only if Pr(A|B) = Pr(A).
The latter condition means that Pr(A ‚à©B) =
Pr(A|B) Pr(B) = Pr(A) Pr(B), which in turn is equivalent to say that Pr(B|A) = Pr(B)
given that Pr(A ‚à©B) = Pr(B|A) Pr(A) as well. Intuitively, if A and B are independent, the
probability of observing A (or B) does not depend on whether B (or A has occurred) and
hence conditioning on the sample space (i.e., looking at the unconditional distribution) or
on the event B makes no diÔ¨Äerence.
Example:
Consider a lot of 10,000 pipes of which 10% comes with some sort of in-
dentation. Suppose we randomly draw two pipes from the lot and deÔ¨Åne the events A1 =
{Ô¨Årst pipe is in perfect conditions} and A2 = {second pipe is in perfect conditions}. If sam-
pling is with reposition, then events A1 and A2 are independent and so Pr(A1 ‚à©A2) =
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
33 
Pr(A1) Pr(A2) = (0.9)2 = 0.81. However, if sampling is without reposition, then Pr(A1 ‚à©
A2) = Pr(A2|A1) Pr(A1) = 0.9 8,999
9,999, which is very marginally diÔ¨Äerent from 0.81.
This example illustrates well a situation in which the events are not entirely independent,
though assuming independence would simplify a lot the computation of the joint probability
at the expenses of a very marginal cost due to the large sample. This is just to say that
sometimes it pays oÔ¨Äto assume independence between events even if we know that, in
theory, they are not utterly independent.
Problem set
Exercise 1.
Show that
Pr(A ‚à™B ‚à™C) = Pr(A) + Pr(B) + Pr(C)
‚àíPr(A ‚à©B) ‚àíPr(A ‚à©C) ‚àíPr(B ‚à©C)
+ Pr(A ‚à©B ‚à©C).
Solution
We employ a similar decomposition to the one in the proof of (c). In particular,
(A ‚à™B) ‚à™C = (A ‚à™B) ‚à™(C ‚à©A ‚à™B). As the intersection is null,
Pr(A ‚à™B ‚à™C) = Pr(A ‚à™B) + Pr(C ‚à©A ‚à™B).
We now decompose the event C into outcomes that belong and not belong to A ‚à™B:
C =

C ‚à©(A ‚à™B)

‚à™

C ‚à©A ‚à™B

,
yielding Pr

C ‚à©A ‚à™B

= Pr(C) ‚àíPr

C ‚à©(A ‚à™B)

. So far, we have that
Pr(A ‚à™B ‚à™C) = Pr(A ‚à™B) + Pr(C) ‚àíPr

C ‚à©(A ‚à™B)

= Pr(A) + Pr(B) + Pr(C) ‚àíPr(A ‚à©B) ‚àíPr

C ‚à©(A ‚à™B)

.
It remains to show that the last term equals to Pr(A ‚à©C) + Pr(B ‚à©C) ‚àíPr(A ‚à©B ‚à©C).
To appreciate this, it suÔ¨Éces to see that C ‚à©(A ‚à™B) = (A ‚à©C) ‚à™(B ‚à©C), which gives way
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
34 
to Pr

C ‚à©(A ‚à™B)

= Pr(A ‚à©C) + Pr(B ‚à©C) ‚àíPr

(A ‚à©C) ‚à©(B ‚à©C)

by P3. The last
term is obviously equivalent to Pr(A ‚à©B ‚à©C), completing the proof.
‚ñ†
Exercise 2.
Consider two events A e B. Show that the probability that only one of these
events occurs is Pr(A ‚à™B) ‚àíPr(A ‚à©B).
Solution
Let C denote the event in which we observe only one event between A or
B. It then consists of every possible outcome that it is in A ‚à™B and not in A ‚à©B. It
is straightforward to appreciate from a Venn diagram that C = (A ‚à™B) ‚àí(A ‚à©B) =
(A ‚à™B) ‚à©A ‚à©B = (A ‚à©¬ØB) ‚à™( ¬ØA ‚à©B). The last representation is the easiest to manipulate
for it involves mutually exclusive events. In particular, it follows immediately from (c) that
Pr(C) = Pr(A) + Pr(B) ‚àí2 Pr(A ‚à™B) = Pr(A ‚à™B) ‚àíPr(A ‚à©B).
‚ñ†
Exercise 3.
There are three plants that produce a given screw: A, B, and C. Plant
A produces the double of screws than B and C, whose productions are at par. In addition,
quality control is better at plants A and B in that only 2% of the screws they produce
are defective as opposed to 4% in plant C. Suppose that we sample one screw from the
warehouse that collects all screws produced by A, B, and C. What is the probability that
the screw is defective? What is the probability that the defective screw is from plant A?
Solution:
Let A = {screw comes from plant A}, B = {screw comes from plant B},
C = {screw comes from plant C}, and D = {screw is defective}. Given that A‚Äôs production
is twofold, it follows that Pr(A) = 1/2 and that Pr(B) = Pr(C) = 1/4. We now decompose
the event D according to whether the screw comes from A, B, or C.
The latter forms
a partition because if a screw comes from a given plant it cannot come from any other
plant. In addition, there are only plants A, B, and C producing this particular screw. The
decomposition yields
Pr(D) = Pr(D|A) Pr(A) + Pr(D|B) Pr(B) + Pr(D|C) Pr(C)
= 0.02 1
2 + 0.02 1
4 + 0.04 1
4 = 0.025.
Basic principles of probability
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
35 
To answer the second question, we must apply the Bayes rule for we do not observe whether
the screw comes from a given plant, but we do know whether it is defective or not. So,
the conditional probability that the screw is from A given that it is defective is Pr(A|D) =
0.02√ó1/2
0.02√ó1/2+0.02√ó1/4+0.04√ó1/4 =
0.01
0.01+0.005+0.01 = 2/5.
‚ñ†
Basic principles of probability
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Statistics for Business and Economics
 
36 
Chapter 4
Probability distributions
4.1
Random variable
Dealing with events and sample spaces is very intuitive, but it is not very easy to keep track
of things if the sample space is large. That is why we next introduce the notion of random
variable, which entails a much easier approach to probability theory.
DeÔ¨Ånition:
X(s) is a random variable if X(¬∑) is a function that assigns a real value to
every element s in the sample space S.
Example:
Suppose we Ô¨Çip twice a coin and deÔ¨Åne the sample space as the sequence of
heads and tails, that is to say, S = {HH, HT, TH, TT}. Let X denote a random variable
equal to the number of heads:
X(s) =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
0, if s ‚àà{TT}
1, if s ‚àà{HT, TH}
2, if s ‚àà{HH}.
Note that every element in the sample space corresponds to exactly one value of the random
variable, though the latter may assume the same value for diÔ¨Äerent elements of the sample
space.
4.1.1
Discrete random variable
If X is a discrete random variable, then it takes only a countable number of values. This
means that, in practice, we may consider a list of possible outcomes x1, . . . , xn (even if n ‚Üí
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
37 
‚àû) for any discrete random variable X. Denoting the probability of observing a particular
value by pi ‚â°p(xi) = Pr(X = xi), it follows that pi ‚â•0 for i = 1 . . . , n and that n
i=1 pi = 1.
The function p(¬∑) is known as the probability distribution function of the discrete random
variable. For instance, a random variable following a discrete uniform probability function
pi = 1/n, with n Ô¨Ånite, is the random-variable counterpart of equiprobable events. Figure
4.1 displays the probability distribution function of a discrete uniform random variable over
the set {1, 2, . . . , 10}.
Example:
Suppose that a mutual fund buys and holds a given stock as long as price
changes are nonnegative. Let stock prices follow a random walk such that the probability
of observing a negative price change is 2/5. DeÔ¨Åne the sample space S and the random
variable N according to the number of periods that are necessary to observe the mutual
fund unwinding its position: S = {1, 01, 001, 0001, . . .} and N = {1, 2, 3, 4, . . .}. It is easy
to see that N = n if and only if we observe a negative price change in the nth period after
(n‚àí1) periods of nonnegative returns. In addition, the random walk hypothesis implies that
returns are independent over time, and so
Pr(N = n) =
	3
5

n‚àí1 2
5
n = 1, 2, . . .
Just as a sanity check, let‚Äôs test whether the above probability function sums up to one if
we consider every possible outcome:
‚àû

n=1
Pr(N = n) = 2
5
	
1 + 3
5 + 9
25 + ¬∑ ¬∑ ¬∑

= 2
5
1
1 ‚àí3/5 = 1.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
38 
Figure 4.1: The left and right axes correspond to the probability distribution function and
cumulative probability distribution function of a uniform distribution over {1, 2, . . . , 10},
respectively.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
39 
Binomial distribution
A Bernoulli essay is the simplest and most intuitive of all probability distribution functions.
It restricts attention to a binary random variable that takes value one with probability p,
otherwise it is equal to zero (with probability 1 ‚àíp). Consider now a random variable that
sums up the values of n independent Bernoulli essays. The probability distribution function
of such a variable is by deÔ¨Ånition binomial.
Example:
Suppose that a production line results in defective products with probability
0.20. A random draw of three products leads to a sample space given by
S = {DDD, DDN, DND, NDD, NND, NDN, DNN, NNN},
where D and N refer to defective and non-defective products, respectively. The ordering
does not matter much in most situations and so the typical random variable of interest is
the number of defective goods X ‚àà{0, 1, 2, 3}. The probability distribution function of X
then is p0 = 0.83, p1 = 3 √ó 0.2 √ó 0.82, p2 = 3 √ó 0.8 √ó 0.22, and p3 = 0.23.
In the example above, it is readily seen from the sample space that there is only one
manner to obtain either three defective goods or three non-defective goods. In contrast,
there are three diÔ¨Äerent ways to observe either one or two defective products due to the fact
that the ordering does not matter. It is precisely the latter that explains why the binomial
distribution function involves the combinatoric tool of combination.
DeÔ¨Ånition:
Consider an experiment in which the event A occurs with probability p =
Pr(A) and so Pr( ¬ØA) = 1 ‚àíp. Run such an experiment independently n times. The resulting
sample space is S = {all sequences a1, . . . , an}, where ai is either A or ¬ØA for i = 1, . . . , n. The
random variable X that counts the number of times that the event A occurs has a binomial
distribution function B(n, p) with parameters n (namely, the number of independent essays)
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
40 
and p (namely, the probability of event A). The binomial distribution is such that
px = Pr(X = x) =
	n
x

px (1 ‚àíp)n‚àíx,
x = 0, 1, ¬∑ ¬∑ ¬∑ , n.
(4.1)
To show that (4.2) is a probability distribution function, it suÔ¨Éces to conÔ¨Årm that it
sums up to one given that px > 0. As expected,
n

x=0
Pr(X = x) =
n

x=0
	n
x

px (1 ‚àíp)n‚àíx
= [p + (1 ‚àíp)]n = 1,
where the last equality comes from Newton‚Äôs binomial expansion (hence the name of the
distribution).
Problem set
Exercise 1.
Paolo Maldini challenges BuÔ¨Äon for a series of 20 penalty kicks. In the Ô¨Årst
10 penalty kicks, Maldini scores with probability 4/5. However, as from the 11th attempt,
Maldini‚Äôs age kicks in and the probability of scoring reduces to 1/2. Assuming that the
outcomes are independent among themselves, compute the probability that Maldini scores
exactly k goals.
Solution:
Each penalty kick corresponds to a Bernoulli essay with probability p1 = 4/5
of success in the Ô¨Årst 10 attempts and p2 = 1/2 from then on up to the 20th penalty kick.
We thus split the problem into scoring k1 goals in the Ô¨Årst 10 attempts and k ‚àík1 goals in
the second 10 penalty kicks. The former leads to a binomial distribution B(10, 4/5), whereas
the latter to a binomial B(10, 1/2). Putting together gives way to
	10
k1

pk1
1 (1 ‚àíp1)10‚àík1 √ó
	
10
k ‚àík1

pk‚àík1
2
(1 ‚àíp1)10‚àík+k1
	10
k1

0.8k1 0.210‚àík1 √ó
	
10
k ‚àík1

0.510.
It now remains to sum up the ways at which Maldini can score k goals by scoring exactly
k1 goals in the Ô¨Årst 10 attempts. To this end, we must Ô¨Årst consider whether k > 0 or not,
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
41 
yielding a probability of scoring k penalty kicks of
Pr(X = k) =
min{k,10}

k1=max{0,k‚àí10}
	10
k1

0.8k1 0.210‚àík1
	
10
k ‚àík1

0.510.
We sum from max{0, k ‚àí10} because if k > 10 then Maldini should score at least K ‚àí10
in each series of 10 attempts. Similarly, we sum up to min{k, 10} because if k < 10, then
Maldini cannot score more than k goals in each series of penalty kicks.
‚ñ†
Exercise 2.
Consider a random variable X ‚àà{0, 1, 2, . . .} such that
Pr(X = t) = (1 ‚àíŒ±)Œ±t,
t = 0, 1, 2, ¬∑ ¬∑ ¬∑
(a) For which values of Œ± (4.5) indeed is a probability distribution function?
(b) Show that for any positive integers s and t, Pr(X > s + t|X > s) = Pr(X ‚â•t).
Solution:
(a) It follows from ‚àû
t=0 Pr(X = t) = 1 that (1 ‚àíŒ±) ‚àû
t=0 Œ±t = 1. The latter involves an
inÔ¨Ånite sum of a geometric progression, which converges to (1‚àíŒ±) ‚àû
t=0 Œ±t = (1‚àíŒ±)
1
1‚àíŒ± = 1
only if Œ± belongs to the unit interval.
(b) It follows from the fact that s + t ‚â•s > 0 that
Pr(X > s + t|X > s) = Pr(X > s + t)
Pr(X > s)
=
‚àû
r=s+t+1(1 ‚àíŒ±)Œ±r
‚àû
r=s+1(1 ‚àíŒ±)Œ±r
=
‚àû
r=s+t+1 Œ±r
‚àû
r=s+1 Œ±r
= (1 ‚àíŒ±)‚àí1Œ±s+t+1
(1 ‚àíŒ±)‚àí1Œ±s+1
= Œ±t.
To complete the proof, it suÔ¨Éces to show that Pr(X ‚â•t) = Œ±t. That is indeed the case
because
Pr(X ‚â•t) =
‚àû

r=t
(1 ‚àíŒ±)Œ±r = (1 ‚àíŒ±)
‚àû

r=t
Œ±r
= (1 ‚àíŒ±)
Œ±t
1 ‚àíŒ± = Œ±t,
where the penultimate equality comes from the fact that the inÔ¨Ånite sum of a geometric
progression is equal to the Ô¨Årst term of the progression divided by one minus the quotient
of the progression.
‚ñ†
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
42 
4.1.2
Cumulative probability distribution function
The goal not always lies on computing a pointwise probability. We are very often interested
in understanding how likely it is to observe a range of values, e.g., the probability of observing
X ‚â§x. This motivates us to deÔ¨Åne the cumulative distribution function as
FX(x) = Pr(X ‚â§x) =

j
p(xj),
‚àÄxj ‚â§x.
Note that FX is a nondecreasing step function in x given that FX(x1) ‚â§FX(x2) if x1 ‚â§x2.
In addition, the cumulative distribution function belongs to the unit interval given that
limx‚Üí‚àí‚àûFX(x) = 0 and limx‚Üí‚àûFX(x) = 1. Finally, if X ‚àà{x1, x2, . . . |x1 < x2 < . . .}, then
Pr(X = xn) = Pr(X ‚â§xn) ‚àíPr(X ‚â§xn‚àí1) = FX(xn) ‚àíFX(xn‚àí1).
Example:
Let X ‚àà{x1, x2, x3} with p(x1) = 1
3, p(x2) = 1
6, and p(x3) = 1
2. The cumulative
distribution function then reads
FX(x) =
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
0,
if ‚àí‚àû< x < x1
1/3 if x1 ‚â§x < x2
1/2 if x2 ‚â§x < x3
1
if x3 ‚â§x < ‚àû.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
43 
4.1.3
Continuous random variable
We say that a random variable is continuous if the probability of observing any particular
value in the real line is zero. Accordingly, the notion of probability distribution function is
meaningless and we have to come up with something a bit diÔ¨Äerent, though with a similar
interpretation. The analog of the probability distribution function for continuous random
variables is the probability density function. To understand the latter, we Ô¨Årst note that,
within the context of continuous random variables, it only makes sense to talk about the
probability of observing a value within a given interval. The probability density function
then measures the mass of probability of an inÔ¨Ånitesimal interval, that is to say, Pr(x < X <
x + Œîx) for a very small Œîx > 0. In the following deÔ¨Ånition, we formalize such a notion.
DeÔ¨Ånition:
The probability density function fX(¬∑) of a random variable X is such that
(a) fX(x) ‚â•0,
‚àÄx ‚ààR
(b)
 +‚àû
‚àí‚àûfX(x) dx = 1
(c) Pr(a ‚â§X ‚â§b) =
 b
a fX(x) dx for ‚àí‚àû< a < b < +‚àû.
Note that condition (a) corresponds to the restriction that the probability distribution
function is positive for every element in the sample space, whereas (b) is analogous to the
imposition that the probability distribution function sums up to one if evaluated at every
element in the sample space. Finally, (c) reÔ¨Çects the fact that the probability of observing a
particular value is zero given that Pr(X = x0) =
 x0
x0 fX(x) dx = 0. Of course, the fact that
an event A has probability zero does not mean that it is impossible to observe it. It just
means that it is improbable. For instance, imagine that we are measuring how much time
one takes to run 100 meters. The probability of observing a value of precisely 10 seconds is
zero ex-ante, for there is a continuum of values around 10 seconds, though it could well take
exactly 10 seconds ex-post. A corollary is that
Pr(a ‚â§X ‚â§b) = Pr(a < X ‚â§b) = Pr(a < X < b) = Pr(a ‚â§X < b).
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
44 
Examples
(1) Let a random variable satisfy the following probability density function
fX(x) =
‚éß
‚é®
‚é©
2x, if 0 < x < 1
0,
otherwise.
We Ô¨Årst note that fX(x) ‚â•0 for any value of x ‚ààR and that
 +‚àû
‚àí‚àû
fX(x) dx =
 1
0
fX(x) dx =
 1
0
2x dx = x21
0 = 1.
In addition, if we wish to compute the probability of observing a given interval, say, x ‚â§1/2,
then it follows that
Pr
	
X ‚â§1
2

=
 1/2
0
2x dx = x21/2
0
= 1
4.
Finally, we may also compute the conditional probability of observing a value within an
interval given that we know it belongs to a larger interval. For instance,
Pr
	
X ‚â§1
2

1
3 ‚â§X ‚â§2
3

= Pr
 1
3 ‚â§x ‚â§1
2

Pr
1
3 ‚â§x ‚â§2
3
 =
x2|1/2
1/3
x2|2/3
1/3
= 5
12.
(2) Let X denote a random variable with density function fX(x) =
‚éß
‚é®
‚é©
Œ± x‚àí3, if 1 ‚â§x ‚â§3
0
otherwise.
It is easy to appreciate that fX(x) ‚â•0 for all x ‚ààR as long as Œ± ‚â•0. In addition, to ensure
that it integrates up to one over the real line, Œ± must equal 9/4 given that
 3
1
Œ± x‚àí3 dx = ‚àíŒ±
2 x2

3
1 = 4 Œ±
9 .
As before, the interest sometimes lies on calculating the probability of observing X within
a given interval. The cumulative distribution function of a continuous random variable is
FX(x) = Pr(X ‚â§x) =
 x
‚àí‚àû
fX(u) du.
The cumulative distribution function is a nondecreasing continuous function given that
FX(x1) ‚â§FX(x2) if x1 ‚â§x2. The continuity is in contrast with the step-like feature in
the case of discrete random variables.
It is as if the height of the steps shrink to zero
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
45 
as the random variables moves from a discrete to a continuous nature, so that FX be-
comes a continuous function. Given that we are now treating with a continuous random
variable, the sum of the pointwise probabilities becomes the integral of the density func-
tion.
As before, FX is such that limx‚Üí‚àí‚àûFX(x) = 0 and limx‚Üí‚àûFX(x) = 1.
In ad-
dition, Pr(x0 ‚â§X ‚â§x1) = Pr(X ‚â§x1) ‚àíPr(X ‚â§x0) = FX(x1) ‚àíFX(x0) for any
‚àí‚àû< x0 ‚â§x1 < ‚àû.
Finally, it also follows from the deÔ¨Ånition of an integral that
fX(x) =
d
dxFX(x) = F ‚Ä≤
X(x) for every x ‚ààR.
Examples
(1) Let X denote a random variable with cumulative distribution function
FX(x) =
‚éß
‚é®
‚é©
1 ‚àíe‚àíx, if x ‚â•0
0,
if x < 0.
It is evident that limx‚Üí‚àí‚àûFX(x) = 0 and that limx‚Üí‚àûFX(x) = 1, whereas diÔ¨Äerentiating
the cumulative distribution function gives way to fX(x) = F ‚Ä≤
X(x) =
‚éß
‚é®
‚é©
e‚àíx, if x ‚â•0
0,
if x < 0.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master‚Äôs Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master‚Äôs programmes
‚Ä¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
‚Ä¢ 1st place: MSc International Business
‚Ä¢ 1st place: MSc Financial Economics
‚Ä¢ 2nd place: MSc Management of Learning
‚Ä¢ 2nd place: MSc Economics
‚Ä¢ 2nd place: MSc Econometrics and Operations Research
‚Ä¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‚ÄòBeste Studies‚Äô ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Statistics for Business and Economics
 
46 
(2) Let X denote an exponential random variable with density function
fX(x) =
‚éß
‚é®
‚é©
Œª e‚àíŒª x, if x ‚â•0
0,
if x < 0.
Note that the previous example is a particular case of the exponential distribution with
Œª = 1. Now, consider the probability of observing X within the interval (t, t + 1):
Pr(t < X < t + 1) = FX(t + 1) ‚àíFX(t) =
 t+1
t
Œª e‚àíŒª x dx
= ‚àíe‚àíŒª xt+1
t
= e‚àíŒª t(1 ‚àíe‚àíŒª).
Letting Œ± = e‚àíŒª then yields Pr(t < X < t + 1) = (1 ‚àíŒ±) Œ±t, which is the probability
distribution function of the memoryless discrete random variable of the second exercise of
the problem set in Section 4.1.
(3) Let X denote a random variable with density function
fX(x) =
‚éß
‚é®
‚é©
6 x (1 ‚àíx), if 0 ‚â§x ‚â§1
0
otherwise.
As before, it is easy to see that fX(x) ‚â•0 for every x ‚ààR and that
 1
0 6 x (1 ‚àíx) dx =
(3 x2 ‚àí2 x3)|1
0 = 3 ‚àí2 = 1. As for the cumulative distribution function, integrating the
density function up to x yields
FX(x) =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
0,
if x < 0
x2 (3 ‚àí2x) if 0 ‚â§x ‚â§1
1
if x > 1.
We may employ the latter to compute the probability of observing a value within an interval
as well as a conditional probability as, e.g.,
Pr
	
X ‚â§1
2

1
3 < X < 2
3

=
(3x2 ‚àí2x3)|1/2
1/3
(3x2 ‚àí2x3)|2/3
1/3
=
3/4 ‚àí1/4 ‚àí1/3 + 2/27
4/3 ‚àí16/27 ‚àí1/3 + 2/27 = 10
13.
Uniform distribution
The simplest continuous distribution function is the uniform. It essentially dictates that
intervals of same length are equiprobable within the support of the random variable, say
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
47 
[Œ±, Œ≤]. The corresponding density function is
fX(x) =
‚éß
‚é®
‚é©.1/(Œ≤ ‚àíŒ±) if Œ± ‚â§x ‚â§Œ≤
0
otherwise,
which implies that Pr(a ‚â§X ‚â§b) = b‚àía
Œ≤‚àíŒ± = FX(b)‚àíFX(a) with ‚àí‚àû< Œ± ‚â§a ‚â§b ‚â§Œ≤ < ‚àû.
Finally, the cumulative distribution function reads
FX(x) =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
0
if ‚àí‚àû< x < Œ±
x/(Œ≤ ‚àíŒ±) if Œ± ‚â§x ‚â§Œ≤
1
if Œ≤ < x ‚â§‚àû
Example:
Let X denote a random variable that is uniformly distributed in the unit
interval. The density function then is
fX(x) =
‚éß
‚é®
‚é©
1
2
if 0 ‚â§x ‚â§1
0 otherwise,
whereas the cumulative distribution function is
FX(x) =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
0 if ‚àí‚àû< x < 0
x if 0 ‚â§x ‚â§1
1 if 1 < x < ‚àû.
There are several applications for the uniform distribution.
For instance, if we wish
to generate a random variable from a particular distribution, we may always start with a
uniform distribution and then transform it to obtain the desired distribution. This is possible
because, for any random variable X with cumulative distribution function FX, FX(X) has
a uniform distribution in the unit interval. More advanced applications include resampling
techniques (e.g., bootstrap) and prior distributions in Bayesian analysis.
4.1.4
Functions of random variables
Consider a random variable X :
s ‚ÜíX(s) = x for any s ‚ààS and a transformation
H : x ‚ÜíH(x) = y that maps a realization x of the random variable X into a real value
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
48 
y ‚ààR.
Instead of transforming the realization x, one may also transform the random
variable X, giving way to another random variable Y = H(X) :
s ‚ÜíH[X(s)]. Given
that the randomness of Y is completely due to X, it is possible to compute the probability
distribution of Y if we know the probability distribution function of X.
Example:
Let Y = H(X) = 2X +1 with X following a standard exponential distribution
fX(x) = e‚àíx 1(x > 0), where 1(A) is an indicator function that takes value one if A is true,
zero otherwise. Given that X is positive, the support of Y is given by the interval [1, ‚àû). It
then follows that
FY (y) = Pr(Y ‚â§y) = Pr(2X + 1 ‚â§y) = Pr
	
X ‚â§y ‚àí1
2

= FX
	y ‚àí1
2

=
 (y‚àí1)/2
0
e‚àíx dx =

‚àíe‚àíx(y‚àí1)/2
0
= 1 ‚àíe(1‚àíy)/2.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
49 
In general, we deal with transformations of a random variable according to whether the
latter is discrete or random. If X is a discrete random variable, then it is easy to appreciate
that a random variable Y = H(X) will also be discrete regardless of the transformation H.
Letting X ‚àà{x1, ¬∑ ¬∑ ¬∑ , xn, ¬∑ ¬∑ ¬∑ } yields Y ‚àà{y1 = H(x1), ¬∑ ¬∑ ¬∑ , yn = H(xn), ¬∑ ¬∑ ¬∑ }. In addition,
Pr(Y = yi) = Pr(X = xi) if the transformation H is such that each y corresponds to a
unique value x.
Example:
Let X ‚àà{‚àí1, 0, 1} with Pr(X = ‚àí1) = 1/3, Pr(X = 0) = 1/2, and Pr(X =
1) = 1/6. If Y = X2, then Pr(Y = 0) = 1/2 and Pr(Y = 1) = 1/2.
Letting xik denote the values of X such that H(xik) = yi for every k ‚àà{1, 2, . . .} leads
to Pr(Y = yi) = ‚àû
k=1 Pr(X = xik).
Example:
Let X ‚àà{1, 2, . . . , n, . . .} with Pr(X = n) = 2‚àín and let
Y =
‚éß
‚é®
‚é©
1
if X is even,
‚àí1 if X is odd.
It thus follows that Pr(Y = 1) = 1
4 + 1
16 + ¬∑ ¬∑ ¬∑ =
1/4
1‚àí1/4 = 1
3 and hence Pr(Y = ‚àí1) = 2
3.
If X is a continuous random variable, then Y = H(X) is not necessarily continuous
for discreteness can arise depending on the transformation H. Naturally, any continuous
transformation preserves the continuity of the random variables.
Example:
Let X denote a random variable in the real line and
Y =
‚éß
‚é®
‚é©
‚àí1 if X < 0,
1
if X ‚â•0.
In this instance, Pr(Y = yi) =

A fX(x) dx, where A denotes the event about X that
corresponds to {Y = yi}, that is to say, A is either the negative real line or the nonnegative
real line.
In general, to derive the probabilistic structure of Y given X, we must Ô¨Årst compute the
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
50 
distribution function FY (y) = Pr(Y ‚â§y) by means of the events in X that correspond to
{Y ‚â§y} and then diÔ¨Äerentiate with respect to y to obtain the density function fY . Finally,
we must also determine the support of Y by seeking the values of y for which fY (y) > 0.
Example:
Let X denote a continuous random variable with density function
fX(x) =
‚éß
‚é®
‚é©
2x if 0 < x < 1,
0
otherwise.
(4.2)
Letting Y = H(X) = 3X + 1 then yields
FY (y) = Pr(Y ‚â§y) = Pr(3X + 1 ‚â§y) = Pr
	
X ‚â§y ‚àí1
3

=
 (y‚àí1)/3
0
fX(x) dx =
 (y‚àí1)/3
0
2x dx = x2(y‚àí1)/3
0
=
	y ‚àí1
3

2
= (1 ‚àíy)2/9.
The density function then is fY (y) = F ‚Ä≤
Y (y) = 2
9 (y ‚àí1), whereas the support of Y is given
by the interval (1, 4) for y = 3x + 1 with 0 < x < 1 to ensure that both fX and fY are
bounded away from zero.
As an alternative, if H is diÔ¨Äerentiable and strictly monotone, we could also determine
FY by noting that x = H‚àí1(y) and hence
{H(X) ‚â§y} ‚àº
‚éß
‚é®
‚é©
{X ‚â§H‚àí1(y)} if H is strictly increasing
{X ‚â•H‚àí1(y)} if H is strictly decreasing.
It then suÔ¨Éces to appreciate that
FY (y) = Pr(Y ‚â§y) = Pr [H(X) ‚â§y] = Pr

H ‚â∑H‚àí1(y)

=
‚éß
‚é®
‚é©
1 ‚àíFX [H‚àí1(y)] if H is strictly decreasing,
FX [H‚àí1(y)]
if H is strictly increasing.
As for the density function,
fY (y) = d
dyFY (y) =

dFX
dx
dx
dy
 = fX(x)

dx
dy
 ,
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
51 
where x = H‚àí1(y).
Examples
(1) Let us revisit the previous example in which the density function of X is given by (4.2)
and Y = H(X) = 3X + 1. The cumulative distribution function of Y is FY (y) = Pr(Y ‚â§
y) = F [(y ‚àí1)/3], whereas the density function is fY (y) = fX(x)
 dx
dy
, with x = (y ‚àí1)/3.
It then follows that fY (y) = 2
9 (y ‚àí1). Just as a sanity check, we next check whether the
latter integrates to one over the support of Y . If x ‚àà(0, 1), then Y = 3X + 1 ‚àà(1, 4) and so
 4
1
fY (y) dy =

y2/9 ‚àí2y/9
4
1 = 16
9 ‚àí8
9 ‚àí1
9 + 2
9 = 1.
(2) Letting now Y = H(X) = e‚àíX yields
FY (y) = Pr(Y ‚â§y) = Pr(e‚àíX ‚â§y) = Pr(X ‚â•‚àíln y)
=
 1
‚àíln y
2x dx =

x21
‚àíln y = 1 ‚àí(‚àíln y)2.
DiÔ¨Äerentiating with respect to y then leads to fY (y) = F ‚Ä≤
Y (y) = ‚àí(2 ln y)/y. As for the
support, we conÔ¨Årm that Y ‚àà(1/e, 1) by showing that
 1
1/e ‚àí2 ln y
y
dy = 1.
Needless to
say, applying the alternative methods results in the same expressions for the cumulative
distribution and density functions. Indeed,
FY (y) = Pr(Y ‚â§y) = Pr(X ‚â•‚àíln y) = 1 ‚àíFX(‚àíln y),
whereas fY (y) = fX(x)
 dx
dy
 = ‚àí(2 ln y)/y given that x = ‚àíln y.
(3) Let X denote a random variable with density function
fX(x) =
‚éß
‚é®
‚é©
1/2 if ‚àí1 < x < 1
0
otherwise.
The cumulative distribution function of Y = X2 then is
FY (y) = Pr(Y ‚â§y) = Pr(X2 ‚â§y) = Pr(‚àí‚àöy ‚â§X ‚â§‚àöy) = FX(‚àöy) ‚àíFX(‚àí‚àöy),
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
52 
whereas the density function is
fY (y) = F ‚Ä≤
Y (y) = fX(‚àöy)
2‚àöy)
‚àífX(‚àí‚àöy)
‚àí2‚àöy
=
1
2‚àöy [fX(‚àöy) + fX(‚àí‚àöy)]
=
1
2‚àöy
	1
2 + 1
2

=
1
2‚àöy
with a support given by the unit interval.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Statistics for Business and Economics
 
53 
4.2
Random vectors and joint distributions
A random vector X = (X1, . . . , Xn) is a vector of, say n, random variables. There are
three types of random vectors: continuous, discrete, and mixed. The latter is essentially
a vector including both continuous and discrete random variables, and hence we will fo-
cus only on continuous and discrete random vectors. In what follows, we will consider a
bivariate random vector (X, Y ), though extending the discussion to n-dimensional random
vectors is straightforward. The joint probability distribution function of a discrete bivari-
ate random vector (X, Y ) is p(x, y) = Pr(X = x, Y = y), where X ‚àà{x1, . . . , xn, . . .} and
Y = {y1, . . . , yn, . . .}. Similarly, the joint density function of a continuous bivariate random
vector (X, Y ) is fXY (x, y) ‚àºPr (x ‚â§X ‚â§x + Œîx, y ‚â§Y ‚â§y + Œîy) for small enough Œîx
and Œîy. As before, the density function is such that fXY (x, y) ‚â•0 for every (x, y) ‚ààR2 and
that
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àûfXY (x, y) dx dy = 1.
Examples
(1) Suppose there are two shoemakers in a shop. The Ô¨Årst does at best 5 shoes in a given
month, whereas the second takes more time to make a shoe and hence does at most 3 shoes
per month. Let X ‚àà{0, 1, . . . , 5} and Y ‚àà{0, 1, . . . , 3} denote the number of shoes by the
Ô¨Årst and second shoemakers in a given month, with probabilities given by
X = 0
1
2
3
4
5
Y = 0
0
0.01
0.03
0.05
0.07
0.09
1
0.01
0.02
0.04
0.05
0.06
0.08
2
0.01
0.03
0.05
0.05
0.05
0.06
3
0.01
0.02
0.04
0.06
0.06
0.05
If the interest lies on the event B = {X > Y }, for instance, it then suÔ¨Éces to sum up the
probabilities that appear in boldface, resulting in Pr(B) = 3/4.
(2) The shop owner is a bit concerned with the amount of leather each shoemaker employs
per month. Let X and Y now denote the quantity of leather the Ô¨Årst and second shoemakers
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
54 
spend, respectively. The shop owner is so miser that he gauges with inÔ¨Ånite precision how
much leather the shoemakers use, and so we may assume that (X, Y ) is a continuous bivariate
random variable. In addition, the joint density function is given by
fXY (x, y) =
‚éß
‚é®
‚é©
Œ± if 5 ‚â§x ‚â§10 and 4 ‚â§y ‚â§9
0 otherwise.
We Ô¨Årst compute Œ± by integrating the density function over R2:
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
fXY (x, y) dx dy = 1
 9
4
 10
5
Œ± dx dy =
 9
4
(Œ±x)|10
5 dy =
 9
4
5Œ± dy
= (5Œ±y)|9
4 = 25Œ± = 1,
implying that Œ± = 1/25. Next, let‚Äôs compute how likely is the event B = {X > Y } that the
Ô¨Årst shoemaker employs more leather than the second shoemaker:
Pr(B) = 1 ‚àíPr(X ‚â§Y ) = 1 ‚àí
 9
5
 y
5
1
25 dx dy
= 1 ‚àí1
25
 9
5
(y ‚àí5) dy = 17
25.
The Ô¨Årst integral is over the interval [5, 9] because it is impossible to observe Y ‚â•X if
4 ‚â§Y < 5 given that X ‚â•5, whereas the second integral is over the interval [5, y] because
X cannot exceed the value that we observe for Y given that Y ‚â•X.
(3) Let (X, Y ) denote a bivariate random variable with joint density function
fXY (x, y) =
‚éß
‚é®
‚é©
x2 + xy/3 if ) < x < 1 and 0 < y < 2
0
otherwise.
We Ô¨Årst show that the above density function integrates to one:
 1
0
 2
0
	
x2 + 1
3 xy

dy dx =
 1
0
	
x2y + 1
6 xy2


2
0
dx
=
 1
0
	
2x2 + 2
3 x

dx =
	2
3 x3 + 1
3 x2


1
0
= 2
3 + 1
3 = 1.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
55 
We next illustrate how to compute the likelihood of an event that involves both X and Y ,
say, the probability of observing {X + Y ‚â§1}:
Pr(X + Y ‚â§1) = Pr(Y ‚â§1 ‚àíX)
=
 1
0
 1‚àíx
0
	
x2 + 1
3 xy

dy dx =
 1
0
	
x2y + 1
6 xy2


1‚àíx
0
dx
=
 1
0

x2(1 ‚àíx) + 1
6 x(1 ‚àíx)2

dx =
 1
0

x2 ‚àíx3 + 1
6 (x ‚àí2x2 + x3)

dx
=
 1
0
	2
3 x2 ‚àí5
6 x3 + 1
6 x

dx =
	2
9 x3 ‚àí5
24 x4 + 1
12 x


1
0
= 2
9 ‚àí5
24 + 1
12 = 16 ‚àí15 + 6
72
= 7
72.
In general, it follows that the joint probability distribution function of a continuous
random vector X = (X1, . . . , Xn) is given by
FX(x) = Pr(X ‚â§x) =
 x1
‚àí‚àû
¬∑ ¬∑ ¬∑
 xn
‚àí‚àû
fX(x1, . . . , xn) dx1 ¬∑ ¬∑ ¬∑ dxn,
with x = (x1, . . . , xn), whereas the joint density function is
fX(x) =
‚àÇ
‚àÇx‚Ä≤ FX(x) =
‚àÇn
‚àÇx1 ¬∑ ¬∑ ¬∑ ‚àÇxn
FX(x1, . . . , xn).
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
56 
4.3
Marginal distributions
Knowing the joint distribution function FXY of (X, Y ) also implies the knowledge of the
marginal distributions FX and FY of X and Y , respectively. After all, it contains all in-
formation about the probabilistic structure of both X and Y .
To extract the marginal
distributions of X and Y , it suÔ¨Éces to ‚Äòintegrate‚Äô the other random variable out of the joint
distribution. We employ quotation marks because integrating out a discrete random vari-
able, say Y , corresponds to summing the joint probability for all possible values of Y given
that any of these values may occur. Letting Bj = {X = x, Y = yj} for j = 1, 2, . . . then
yields p(x) = Pr(X = x) = Pr(B1 or ¬∑ ¬∑ ¬∑ or Bn or ¬∑ ¬∑ ¬∑ ) = ‚àû
j=1 p(x, yj), given that these
events are all mutually exclusive.
As for continuous random variables, the marginal density function of X is
fX(x) ‚àºPr(x ‚â§X ‚â§x + Œîx)
for a very small Œîx > 0
= Pr(x ‚â§X ‚â§x + Œîx, ‚àí‚àû< Y < ‚àû) =
 ‚àû
‚àí‚àû
fXY (x, y) dy,
and hence
Pr(a < X < b) = Pr(a < X < b, ‚àí‚àû< Y < ‚àû) =
 b
a
 ‚àû
‚àí‚àû
fXY (x, y) dy dx.
Examples
(1) Let (X, Y ) denote a bivariate random vector with joint density given by
fXY (x, y) =
‚éß
‚é®
‚é©
2(x + y ‚àí2xy) if 0 < x, y < 1
0
otherwise.
We Ô¨Årst conÔ¨Årm that (4.3) indeed is a density function by showing that it integrates to one:
 1
0
 1
0
2(x + y ‚àí2xy) dx dy =
 1
0

x2 + 2xy ‚àí2x2y
1
0 dy
=
 1
0
dy = 1.
It is evident from the above derivations that the marginal distributions of X and Y are both
uniform in the unit interval, even though their joint distribution is not uniform.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
57 
(2) Let (X, Y ) denote a uniform random variable in the rectangle [Œ±X, Œ≤X] √ó [Œ±Y , Œ≤Y ]. The
joint density function then is
fXY (x, y) =
‚éß
‚é®
‚é©
1
(Œ≤X‚àíŒ±X)(Œ≤Y ‚àíŒ±Y )
if Œ±X < x < Œ≤X, Œ±Y < y < Œ≤Y
0
otherwise.
Integrating X out yields a uniform density function in the interval [Œ±Y , Œ≤Y ] for Y , whereas
integrating the latter out gives way to a uniform distribution function for X in the interval
[Œ±X, Œ≤X].
(3) Suppose now that (X, Y ) is uniform in B = {(x, y) : 0 < x < 1, x2 < y < x}. It follows
from the fact that the area of B is
 1
0 (x ‚àíx2) dx = 1
6 that
fXY (x, y) =
‚éß
‚é®
‚é©
6 if (x, y) ‚ààB
0 if (x, y) /‚ààB,
otherwise the joint density would integrate to something diÔ¨Äerent from one. The marginal
density of X then is fX(x) =
 x
x2 6 dy = 6 x(1 ‚àíx) for 0 ‚â§x ‚â§1, whereas the marginal of Y
is fY (y) =
 ‚àöy
y
6 dx = 6(‚àöy ‚àíy) for 0 ‚â§y ‚â§1.
The above examples illustrate that a uniform joint distribution does not ensure uniform
marginals, just as uniform marginals do not imply a uniform joint distribution.
4.4
Conditional density function
In the case of discrete random variables, by deÔ¨Ånition, it suÔ¨Éces to compute the ratio between
the probability of observing both events and the probability of observing the conditioning
event, so that p(x|y) = p(x, y)/p(y) if p(y) > 0 and p(y|x) = p(x, y)/p(x) if p(x) > 0. Note
that the conditional probability meets all of the conditions for a probability distribution
function, namely, it is nonnegative for all values of x and sums up to one given that
‚àû

i=1
p(xi|y) =
‚àû

i=1
p(xi, y)
p(y)
=
1
p(y)
‚àû

i=1
p(xi, y) = p(y)
p(y) = 1.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
58 
Example:
Let us revisit the example of the two shoemakers and compute the prob-
ability of the Ô¨Årst shoemaker to produce two shoes in a given month given that the second
shoemaker has also produced two shoes.
By the deÔ¨Ånition of conditional probability, it
suÔ¨Éces to compute the ratio between the probability of observing both events and the prob-
ability of observing the conditioning event, so that
Pr(X = 2|Y = 2) = Pr(X = 2, Y = 2)
Pr(Y = 2)
= 0.05
0.25 = 1
5.
As for continuous random variables, it is easy to see that the same applies to density
functions given that they proxy for the probability of observing the random variable within an
interval of inÔ¨Ånitesimal length. It thus ensues that fX|Y (x|y) = fXY (x, y)/fY (y) if fY (y) > 0
and that fY |X(y|x) = fXY (x, y)/fX(x) if fX(x) > 0. As before, the conditional density
function indeed is a density function in that fX|Y (x|y) ‚â•0 for every x ‚ààR and
 ‚àû
‚àí‚àû
fX|Y (x|y) dx =
 ‚àû
‚àí‚àû
fXY (x, y)
fY (y)
dx
=
1
fY (y)
 ‚àû
‚àí‚àû
fXY (x, y) dx = fY (y)
fY (y) = 1.
Example:
Let (X, Y ) denote a random vector with joint density function given by
fXY (x, y) =
‚éß
‚é®
‚é©
x2 + 1
3 xy if (x, y) ‚àà[0, 1] √ó [0, 2]
0
otherwise.
The conditional density of X given Y then is
fX|Y (x|y) = fXY (x, y)
fY (y)
=
x2 + 1
3 xy
 1
0

x2 + 1
3 xy

dx
=
x2 + 1
3 xy
1
3 x3 + 1
6 x2y
1
0
= x2 + 1
3 xy
1
3 + y
6
= 6x2 + 2xy
2 + y
,
for 0 ‚â§x ‚â§1 and 0 ‚â§y ‚â§2.
4.5
Independent random variables
There is a nice correspondence between independent events and independent random vari-
ables. The condition that Pr(A ‚à©B) = Pr(A)Pr(B) translates into p(x, y) = p(x)p(y) if
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
59 
X and Y are discrete random variables and into fXY (x, y) = fX(x)fY (y) if X and Y are
continuous random variables. That is to say, independence ensues if and only if the joint
probability/density is the product of the marginals. In addition, we can also deÔ¨Åne indepen-
dence by means of conditional probabilities/densities in that independence holds if and only
if (a) p(x|y) = p(x, y)/p(y) = p(x) and p(y|x) = p(x, y)/p(x) = p(y) if X and Y are discrete;
and (b) fX|Y (x|y) = fXY (x, y)/fY (y) = fX(x) and fY |X(y|x) = fXY (x, y)/fX(x) = fY (y) if
X and Y are continuous.
Examples
(1) Let X and Y denote the time it takes to observe a transaction for two stocks, with joint
density function given by
fXY (x, y) =
‚éß
‚é®
‚é©
exp [‚àí(x + y)] if x ‚â•0, y ‚â•0
0
otherwise.
It is easy to see that fXY (x, y) = fX(x)fY (y), with fX(x) = e‚àíx for x > 0 and fY (y) = e‚àíy
for y > 0, and hence X and Y are independent random variables.
(2) Let now X and Y denote random variables with joint density given by fXY (x, y) = 8xy
for 0 ‚â§x ‚â§y ‚â§1. Although it is easy to decompose fXY into the product of functions that
depend exclusively on either X or Y , there is no way to get rid of the dependence in the
support. The fact that X is always inferior to Y is what makes the two random variables
dependent.
Finally, it is interesting to observe more closely how the link between independent events
and independent random variables works. Let A and B denote independent events concerning
the random variables X and Y , respectively. Equivalence follows from the fact that
Pr(A ‚à©B) =
 
A‚à©B
fXY (x, y) dx dy =
 
A‚à©B
fX(x)fY (y) dx dy
=

A
fX(x) dx

BfY (y) dy = Pr(A)Pr(B).
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
60 
4.6
Expected value, moments, and co-moments
Another way to characterize a distribution is through its moments. The Ô¨Årst moment refers to
the expected value of the distribution, whereas the second moment relates to the dispersion of
the distribution. There is also room for higher-order moments in that there are distributions
with an inÔ¨Ånite number of moments. For instance, the third moment tells us whether the
distribution is asymmetric around the expected value, whilst the fourth moment determines
how thick the tails of the distribution are, that is to say, how likely it is to observe an extreme
realization regardless of whether to the right or to the left of the expected value.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet‚Äôs 
electricity needs. Already today, SKF‚Äôs innovative know-
how is crucial to running a large proportion of the 
world‚Äôs wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Statistics for Business and Economics
 
61 
4.6.1
Expected value
The Ô¨Årst moment of a distribution is known as expected value or population mean. As the
name implies, it entails a typical value for the distribution. Before formalizing the notion of
expected value, it is convenient to start with an example to motivate the discussion.
Example:
Gimli proposes a game to his friend Legolas with prizes in gold pieces as in the
table below. In addition, if Legolas decides to play, he must pay beforehand one gold piece
to Gimli.
die throw
1
2
3
4
5
6
Legolas‚Äô payoÔ¨Ä
-3
-2
-1
1
2
5
Legolas‚Äô expected payoÔ¨Äthen is the average payoÔ¨Äminus the entry costs, that is to say,
E(Legolas‚Äô payoÔ¨Ä) = 1
6 (5 + 2 + 1 ‚àí1 ‚àí2 ‚àí3) ‚àí1 = ‚àí2
3. This means that the game is not
very fair to Legolas given that, on average, he will have to pay 2/3 of a gold piece to Gimli.
To make the game fair, Legolas would have to bargain down the entry cost to 1/3.
In the above example, it is easy to compute the expected value because the outcomes
of a die throw are equiprobable and hence it suÔ¨Éces to compute the arithmetic mean of
the possible outcomes.
In general, the values that the random variable can assume are
not equiprobable and hence we must weigh by their mass of probability. We do that by
applying the expectation operator E(¬∑) to the random variable of interest. In the case of
discrete random variables, the expectation operator is such that E(X) = ‚àû
i=1 xip(xi). If
the latter series does not converge to some Ô¨Ånite value, then we say that the distribution has
no expected value.
Examples
(1)
Let X ‚àºB(n, p), so that Pr(X = x) =
n
x

px (1 ‚àíp)n‚àíx. The expected value of a
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
62 
binomial distribution is
E(X) =
n

x=0
x Pr(X = x) =
n

x=0
x
	n
x

px (1 ‚àíp)n‚àíx
=
n

x=1
x
	n
x

px (1 ‚àíp)n‚àíx =
n

x=1
x
n!
x!(n ‚àíx)! px (1 ‚àíp)n‚àíx
=
n

x=1
n!
(x ‚àí1)!(n ‚àíx)! px (1 ‚àíp)n‚àíx.
Letting k = x ‚àí1 then yields
E(X) = n
n‚àí1

k=0
	n ‚àí1
k

pk+1 (1 ‚àíp)n‚àík‚àí1
= np
n‚àí1

k=0
	n ‚àí1
k

pk(1 ‚àíp)n‚àí1‚àík = np
given that
n‚àí1
k

pk(1 ‚àíp)n‚àí1‚àík is the probability distribution function of a B(n ‚àí1, p). Note
that the expected value of the binomial distribution corresponds to the absolute frequency
at which we observe a value equal to one.
(2)
Suppose that a given trading strategy normally entails a weekly return S, though it
may proÔ¨Åt less if there is a sequence of bad news. More precisely, the return reduces to
0 < R < S if there are up to two bad news, and to L < 0 if there are three or more
bad news over the week. The probability of observing a bad news is 1/20 in any given
trading day of the week. Let now X ‚àà{L, R, S} and B ‚àà{1, 2, 3, 4, 5} denote the weekly
return of this trading strategy and the number of bad news over the week, respectively.
The latter is a binomial random variable with probability distribution function given by
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
63 
Pr(B = k) =
5
k

(1/20)k (19/20)n‚àík, and hence the expected value of X is
E(X) = Pr(B = 0) S + [Pr(B = 1) + Pr(B = 2)] R + Pr(B ‚â•3) L
= (19/20)5 S +

5(1/20)(19/20)4 + 10(1/20)2(19/20)3
R
+

10(1/20)3(19/20)2 + 5(1/20)4(19/20) + (1/20)5
L
= (19/20)5S + 5(1/20)(19/20)3 [19/20 + 2(1/20)] R
+ (1/20)3 
10(19/20)2 + 5(1/20)(19/20) + (1/20)2
L
= 0.7737809 S + 0.2250609 R + 0.0011581 L.
Note that the weights assigned to each outcome of the weekly return sum up to one for they
correspond to their probability of occurring.
As for continuous random variables, the expectation operator is such that E(X) =
 ‚àû
‚àí‚àûxfX(x) dx. As before, if the function g(x) = xfX(x) is not integrable, then the dis-
tribution features no expected value.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
64 
Examples
(1) Let X denote a uniform random variable in the interval [Œ±, Œ≤]. In view that the density
function is
fX(x) =
‚éß
‚é®
‚é©
1
Œ≤‚àíŒ±
if Œ± < x < Œ≤
0
otherwise
it follows that
E(X) =
 Œ≤
Œ±
x
Œ≤ ‚àíŒ± dx =
	 x2/2
Œ≤ ‚àíŒ±


Œ≤
Œ±
= Œ≤2 ‚àíŒ±2
2(Œ≤ ‚àíŒ±) = (Œ≤ ‚àíŒ±)(Œ≤ + Œ±)
2(Œ≤ ‚àíŒ±)
= Œ≤ + Œ±
2
.
This makes sense for the uniform continuous distribution is analogous to the case of equiprob-
able events and hence it suÔ¨Éces to take the arithmetic mean of the lower and upper limits
of the support.
(2) Let X denote a random variable with density function given by
fX(x) =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
x/225
if 0 ‚â§x ‚â§15
(30 ‚àíx)/225 if 15 ‚â§x ‚â§30
0
otherwise.
The expected value then is
E(X) =
 15
0
x2
225 dx +
 30
15
(30 ‚àíx)x
225
dx =
1
225
	x3
3


15
0
+
	
15x2 ‚àíx3
3


30
15

= 1
152
	153
3 + 15 √ó 302 ‚àí153 ‚àí303
3 + 153
3

= 5 + 60 ‚àí15 ‚àí40 + 5 = 15.
This result is pretty intuitive given that the density function looks like a symmetric triangle
with peak at 15.
Suppose now we wish to derive the expected value of Y = H(X).
The expectation
operator is such that
E(Y ) =
‚éß
‚é®
‚é©
‚àû
i=1 yip(yi)
if discrete
 ‚àû
‚àí‚àûyfY (y) dy if continuous
=
‚éß
‚é®
‚é©
‚àû
i=1 H(xi)p(xi)
if discrete
 ‚àû
‚àí‚àûH(x)fX(x) dx if continuous.
This means that there is no need to derive the distribution of Y to compute its expected
value. It suÔ¨Éces to compute the expectation of H(X) given the density function of X.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
65 
Example:
In some situations, the interest lies on the magnitude of the random variable
regardless of the sign it takes. Suppose, for instance, that X has a double exponential density
given by
fX(x) =
‚éß
‚é®
‚é©
1
2 ex
if x ‚â§0
1
2 e‚àíx if x ‚â•0,
which is symmetric around zero. Now, the expected value of Y = |X| is
E(Y ) =
 ‚àû
‚àí‚àû
|x| fX(x) dx = 1
2
 0
‚àí‚àû
|x| ex dx +
 ‚àû
0
|x| ex dx

= 1
2
 0
‚àí‚àû
(‚àíx)ex dx +
 ‚àû
0
xe‚àíx dx

=
 ‚àû
0
xe‚àíx dx = 1,
where the last equality follows from integration by parts. Alternatively, one can compute
the expected value of Y by Ô¨Årst deriving its distribution. In particular,
FY (y) = Pr(Y ‚â§y) = Pr(|X| ‚â§y) = Pr(‚àíy ‚â§X ‚â§y) = 2 Pr(0 ‚â§X ‚â§y)
= 2
 y
0
fX(x) dx = 2
 y
0
e‚àíx dx =

‚àíe‚àíxy
0 = 1 ‚àíe‚àíy,
giving way to fY (y) = e‚àíy for y ‚â•0 and to E(Y ) =
 ‚àû
0 yfY (y) dy =
 ‚àû
0 ye‚àíy dy = 1.
It is possible to compute the expected value of a function of a random vector, say Z =
H(X, Y ), along the same lines. More precisely,
E(Z) =
‚éß
‚é®
‚é©
‚àû
i=1 zip(zi)
if discrete
 ‚àû
‚àí‚àûzfZ(z) dz if continuous
=
‚éß
‚é®
‚é©
‚àû
i=1 H(xi, yi)p(xi, yi)
if discrete
 ‚àû
‚àí‚àûH(x, y)fXY (x, y) dx dy if continuous,
which avoids the derivation of the probability/density function of Z. Apart from that, the
expectation operator has two other interesting properties. First, it is a linear operator in
that E(aX +b) = a n
i=1 E(Xi)+b for any Ô¨Åxed constants a and b, if X = n
i=1 Xi. Second,
if the random variables are independent, then the expectation of their product is equal to
the product of their expectations.
This means that, if X and Y are independent, then
E(XY ) = E(X)E(Y ). The examples below employ these properties to derive expectations.
Examples
(1)
A binomial distribution B(n, p) results from the sum of the outcomes of a sequence
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
66 
of n independent Bernoulli essays with probability p. Let Yi denote the outcome of each of
these Bernoulli essays (i = 1, . . . , n), taking value one with probability p, otherwise zero.
The expected value of X then is E(X) = E(Y1 + Y2 + ¬∑ ¬∑ ¬∑ + Yn) = n
i=1 E(Yi) = np.
(2)
Let D denote the weekly demand for apple crumbles, with probability distribution
function pn = Pr(D = n). Let C denote the cost of baking a unit of apple crumble and E
the cost of keeping one apple crumble. Suppose that we sell each apple crumble at a price
P and that our initial stock is of N apple crumbles. It then follows that our proÔ¨Åt Œ† in a
given week is a random variable given by
Œ† =
‚éß
‚é®
‚é©
N(P ‚àíC)
if D ‚â•N
DP ‚àíNC ‚àí(N ‚àíD)E if D < N
because in the latter case we produce N apple crumbles, sell D and then stock the ones that
we are not able to sell during the week. This means that
Œ† =
‚éß
‚é®
‚é©
N(P ‚àíC)
with probability Pr(D ‚â•N) = 1 ‚àíPr(D < N)
D(P + E) ‚àíN(C + E) with probability Pr(D < N)
and hence
E(Œ†) = N(P ‚àíC) Pr(D ‚â•N) + E [D(P + E) ‚àíN(C + E)|D < N] Pr(D < N)
= N(P ‚àíC)
‚àû

n=N+1
pn + (P + E)
N

n=0
n pn ‚àíN(C + E)
N

n=0
pn
= N(P ‚àíC)

1 ‚àí
N

n=0
pn

+ (P + E)
N

n=0
n pn ‚àíN(C + E)
N

n=0
pn
= N(P ‚àíC) + (P + E)
N

n=0
n pn ‚àí[N(C + E) + N(P ‚àíC)]
N

n=0
pn
= N(P ‚àíC) + (P + E)
N

n=0
n pn ‚àíN(P + E)
N

n=0
pn
= N(P ‚àíC) ‚àí(P + E)
N

n=0
(N ‚àín)pn.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
67 
If, for instance, Pr(D = n) =
1
10 for n ‚àà{0, 1, . . . , 9}, it then ensues that
E(Œ†) = N(P ‚àíC) ‚àí(P + E)
N

n=0
N ‚àín
10
= N(P ‚àíC) ‚àíP + E
10

N(N + 1) ‚àí
N

n=0
n

= N(P ‚àíC) ‚àíP + E
10

N(N + 1) ‚àíN(N + 1)
2

= N(P ‚àíC) ‚àíP + E
10
N(N + 1)
2
= N(P ‚àíC) ‚àíN(N + 1)(P + E)
20
.
The above naturally only holds if N ‚â§9, which makes sense given that it seems unreasonable
to start the week with more than the maximum demand for apple crumbles.
The expectation operator is a linear operator and hence it is extremely easy to deal
with aÔ¨Éne functions of a random variable.
Although we have already shown that it is
also straightforward to handle non-aÔ¨Éne functions of random variables, it is important to
note that E[g(X)] Ã∏= g[E(X)] in general. For instance, the next example illustrates that
E(X2) ‚â•[E(X)]2 for any random variable X.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
68 
Example:
Let X denote a random variable that takes value either 1 or -1 with probability
1/2. Given that it is symmetric around zero, the expected value of X is obviously zero and
hence [E(X)]2 = 0. In contrast, the expected value of its square exceeds zero given that
E(X2) = (1 + 1)1/2 = 1.
4.6.2
Variance, covariance, and correlation
The variance measures the amount of variation and dispersion of a random variable around
the mean value. This information is paramount to any problem of statistical inference. In
Ô¨Ånance, we are not only interested in the expected return of an investment, but also in how
risky it is. Most people prefer an investment that entails a return of 10% for sure than
one with a return of either 30% or -20% with probability 1/2 despite the fact that both
investments have en expected return of 10%. The most basic measure of risk is given by the
variance, which gauges the average magnitude of the deviations with respect to the mean
value by means of a quadratic transformation: var(X) = E [X ‚àíE(X)]2.
We could of course employ the absolute value rather than the square to measure the
magnitude of the deviation. The advantage of using squares is that it is diÔ¨Äerentiable as
opposed to the absolute value and that it is very easy to compute by means of the expectation
operator. The drawback is that taking squares potentiates the extreme values, if any, in the
data. Another disadvantage of the variance is that it is not in the same unit of the random
variable.
This is however easy to remedy in that we can always consider the standard
deviation of the random variable, namely, the square root of the variance, so as to recover
the original unit of measurement.
The variance is also known as the second centered moment of the distribution. We say it
is centered because we are looking at deviations around the Ô¨Årst moment (i.e., the expected
value). It relates to the second moment because of the focus on the second power of the
random variable. It is interesting to note that we can also write the variance as a function
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
69 
of the Ô¨Årst two (uncentered) moments of the distribution given that
var(X) = E [X ‚àíE(X)]2 = E

X2 ‚àí2XE(X) + [E(X)]2
= E(X2) ‚àí2 E(X)E(X) + [E(X)]2 = E(X2) ‚àí[E(X)]2.
The properties of the expectation operator are very helpful to compute the second uncentered
moment of the distribution in view that it is not necessary to Ô¨Ånd the distribution of the
square of the random variable. Indeed,
E(X2) =
‚éß
‚é®
‚é©
‚àû
i=1 x2
i p(xi)
if X is discrete
 ‚àû
‚àí‚àûx2fX(x) dx if X is continuous
The properties of the expectation operator also imply a couple of properties for the
variance. First, the variance of an aÔ¨Éne function of X is proportional to the variance of
X.
In particular, var(aX + b) = a2 var(X) for any Ô¨Åxed constants a and b given that
both the second moment and the square of the Ô¨Årst moment will depend on the square of
the slope coeÔ¨Écient a2 and the fact that we take deviations with respect to the expected
value will take care of the intercept b. Second, in the event that X and Y are independent
random variables, the variance of the sum is equal to the sum of the variances, that is to
say, var(X + Y ) = var(X) + var(Y ). In what follows, we demonstrate both properties by
showing that var(aX + b + cY ) = a2 var(X) + c2 var(Y ) if X and Y are independent:
var(aX + b + cY ) = E [aX + b + cY ‚àíE(aX + b + cY )]2
= E [aX + b + cY ‚àía E(X) ‚àíb ‚àíc E(Y )]2
= E {a[X ‚àíE(X)] + c[Y ‚àíE(Y )]}2
= E

a2 [X ‚àíE(X)]2 + 2ac [X ‚àíE(X)][Y ‚àíE(Y )] + c2 [Y ‚àíE(Y )]2
= a2 var(X) + c2 var(Y ) + 2ac E[X ‚àíE(X)][Y ‚àíE(Y )].
To show that the last term is equal to zero, it suÔ¨Éces to appreciate that
E[X ‚àíE(X)][Y ‚àíE(Y )] = E(XY ) ‚àí2 E(X)E(Y ) + E(X)E(Y ) = 0
(4.3)
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
70 
given that independence between X and Y implies that the expectation of the product is
the product of the expectations.
Equation (4.3) suggests a simple measure of dependence between two random variables
based on how their deviations relative to the mean co-vary. Bearing that in mind, we deÔ¨Åne
the covariance between X and Y as
cov(X, Y ) = E[X ‚àíE(X)][Y ‚àíE(Y )] = E(XY ) ‚àíE(X)E(Y ).
The intuition is simple. If the deviations of X ‚àíE(X) tend to have the same sign as the
deviations Y ‚àíE(Y ), we then say that X and Y co-move together and hence their covariance
is positive. In contrast, the covariance is negative if the deviations tend to have opposite
signs.
It is possible to show that the covariance is a measure of linear dependence and
hence independence implies zero covariance (or, equivalently, orthogonality), though zero
covariance does not necessarily imply independence.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT‚Ä¶
     RUN FASTER.
          RUN LONGER..
                RUN EASIER‚Ä¶
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Statistics for Business and Economics
 
71 
Another drawback of the covariance is that it has a strange unit. It is in units of X times
units of Y , compromising a bit interpretability. The easiest remedy for that is to standardize
the deviations of X and Y with respect to their means by their standard deviation, giving
way to the correlation:
corr(X, Y ) =
cov(X, Y )

var(X) var(Y )
.
The latter has no unit in contrast to the covariance.
In addition, standardizing by the
standard deviation is also convenient for it makes the two deviations comparable.
The
correlation also has some very nice properties. First, it is at most one in magnitude, that is,
‚àí1 ‚â§corr(X, Y ) ‚â§1. Second, as the covariance, the correlation is equal to zero if and only
if X and Y are orthogonal (or, equivalently, linearly independent). Third, as in the variance
and the covariance, the correlation is also based on expectations and hence we can employ
all of the apparatus that comes with the expectation operator.
Examples
(1)
A survey classify the degree of customers‚Äô satisfaction, say X, into a scale from 0
to 10. The answers to the survey indicate a symmetric probability distribution given by
p0 = p10 = 0.05, p1 = p2 = p8 = p9 = 0.15, p3 = . . . = p7 = 0.06. The expected degree of
satisfaction then is E(X) = (1 + 2 + 8 + 9) √ó 0.15 + (3 + 4 + 5 + 6 + 7) √ó 0.06 + 10 √ó 0.05 = 5,
which makes sense given that the distribution is symmetric around 5. We next compute the
second moment of the customers‚Äô satisfaction, namely,
E(X2) = (1 + 4 + 64 + 81) √ó 0.15 + (9 + 16 + 25 + 36 + 49) √ó 0.06 + 100 √ó 0.05 = 35.6,
implying a variance of var(X) = E(X2) ‚àí[E(X)]2 = 35.6 ‚àí25 = 10.6 and a standard
deviation of 3.25.
(2) Let X denote a binomial random variable B(n, p) with an expected value of E(X) = np.
Instead of computing the second moment directly from E(X2) = n
x=0 x2n
p

px(1 ‚àíp)n‚àíx,
we will take advantage of the deÔ¨Ånition of the binomial distribution as a sequence of n
independent Bernoulli essays with probability p. Let Yi, with i ‚àà{1, . . . , n}, denote these
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
72 
Bernoulli essays. It then follows that var(X) = var (n
i=1 Yi) = n
i=1 var(Yi) given that all
the covariances are zero due to the independence between the Bernoulli essays. Now, the
variance of Yi is var(Yi) = E(Y 2
i )‚àí[E(Yi)]2 = p‚àíp2 = p (1‚àíp) and hence var(X) = n p (1‚àíp).
(3) Let X denote a uniform random variable in the interval [Œ±, Œ≤]: i.e., X ‚àºU(Œ±, Œ≤). We
know that the expected value of X is E(X) = (Œ± + Œ≤)/2, whereas the second moment is
E(X2) =
 Œ≤
Œ±
x2
Œ≤ ‚àíŒ± dx =

x3
3(Œ≤ ‚àíŒ±)

Œ≤
Œ±
= Œ≤3 ‚àíŒ±3
3(Œ≤ ‚àíŒ±).
The variance of X then reads
var(X) = Œ≤3 ‚àíŒ±3
3(Œ≤ ‚àíŒ±) ‚àí(Œ± + Œ≤)2
4
= 4(Œ≤3 ‚àíŒ±3) ‚àí3(Œ≤ ‚àíŒ±)(Œ± + Œ≤)2
12(Œ≤ ‚àíŒ±)
= 4 Œ≤3 ‚àí4 Œ±3 ‚àí3 Œ≤3 + 3 Œ±3 ‚àí6 Œ± Œ≤2 + 6 Œ±2 Œ≤ ‚àí3 Œ±2 Œ≤ + 3 Œ± Œ≤2
12(Œ≤ ‚àíŒ±)
= Œ≤3 ‚àíŒ±3 ‚àí3 Œ± Œ≤2 + 3 Œ±2 Œ≤
12(Œ≤ ‚àíŒ±)
= (Œ≤ ‚àíŒ±)3
12(Œ≤ ‚àíŒ±) = (Œ≤ ‚àíŒ±)2
12
,
which makes sense in that, as a measure of dispersion, it depends on the square (Euclidean)
distance between the support bounds.
(4) Let (X, Y ) denote a bivariate random variable with density function
fXY (x, y) =
‚éß
‚é®
‚é©
2 if 0 ‚â§x < y ‚â§1
0 otherwise.
The marginal densities then are fX(x) =
 1
x 2 dy = 2(1 ‚àíx) for 0 ‚â§x ‚â§1 and fY (y) =
 y
0 2 dx = 2y for 0 ‚â§y ‚â§1, with expected values of E(X) =
 1
0 2x(1 ‚àíx) dx =
1
3 and
E(Y ) =
 1
0 2y2 dy = 2
3, respectively. As for the moments of second order, we start with the
covariance, namely,
cov(X, Y ) = E(XY ) ‚àíE(X) E(Y ) =
 1
0
 y
0
2xy dx dy ‚àí1
3
2
3 = 1
4 ‚àí2
9 = 1
36.
As for the variances, it follows that
var(X) = E(X2) ‚àí[E(X)]2 =
 1
0
2x2(1 ‚àíx) dx ‚àí1
9 = 1
6 ‚àí1
9 = 1
18
var(Y ) = E(Y 2) ‚àí[E(Y )]2 =
 1
0
2y3 dy ‚àí4
9 = 1
2 ‚àí4
9 = 1
18,
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
73 
and hence the correlation between X and Y amounts to 1/2.
The covariance is such that cov(aX + b, cY + d) = accov(X, Y ). The intercepts b and d
have no impact in the covariance because it deals with deviations with respect to the mean
value and the latter obviously shifts with b and d as much as X and Y , respectively. Further,
the correlation is completely invariant to any aÔ¨Éne transformation in that corr(aX +b, cY +
d) = corr(X, Y ). The extra level of robustness stems from the fact that we standardize the
deviations by the standard deviation, which changes with a and c in the same proportion as
X and Y , respectively.
4.6.3
Higher-order moments
In general, we deÔ¨Åne the kth uncentered moment of a distribution as
Œºk = E(Xk) =
‚éß
‚é®
‚é©
‚àû
i=1 xk
i p(xi)
if X is discrete
 ‚àû
‚àí‚àûxkfX(x) dx if X is continuous.
Similarly, we deÔ¨Åne the centered moments as ¬ØŒºk = E[X ‚àíE(X)]k. However, in most sit-
uations, we prefer to standardize the random variable not only by subtracting the mean,
but also by dividing by the standard deviation, so as to obtain a quantity that is compara-
ble across diÔ¨Äerent random variables. For instance, we deÔ¨Åne skewness and kurtosis as the
standardized third and fourth moments, respectively:
sk(X) = E

X ‚àíE(X)

var(X)
3
and
k(X) = E

X ‚àíE(X)

var(X)
4
.
The former gauges how asymmetric is the distribution relative to its mean value, whereas
the latter measures how thick the left and right tails are.
For instance, it is very well
documented that stock returns display negative skewness and very high kurtosis, reÔ¨Çecting
the fact that extreme negative returns are more frequent than extreme positive returns. In
contrast, changes in exchange rates are typically symmetric around zero, though they also
exhibit very high kurtosis implying thick tails.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
74 
4.7
Discrete distributions
In this section, we brieÔ¨Çy review the discrete distributions we have seen in the previous sec-
tions and introduce a couple of other distribution functions that are often useful in practice.
4.7.1
Binomial
Consider a sequence of n independent experiments in which the event A may occur with
probability p = Pr(A). The resulting sample space is S = {all sequences a1, . . . , an}, where
ai is either A or ¬ØA for i = 1, . . . , n. The random variable X that counts the number of
times that the event A occurs has a binomial distribution function B(n, p) with parameters
n (namely, the number of independent essays) and p (namely, the probability of event A).
The binomial distribution is such that
Pr(X = x) =
	n
x

px (1 ‚àíp)n‚àíx,
x = 0, 1, ¬∑ ¬∑ ¬∑ , n.
(4.4)
The expected value of a binomial distribution is np, whereas the variance is np(1‚àíp). Figure
4.2 depicts the probability distribution function and cumulative distribution function of a
binomial random variable with n = 10 and p = 0.25.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
75 
Figure 4.2: The left and right axes correspond to the probability distribution function and
cumulative probability distribution function of a binomial random variable with n = 10 and
p = 0.25, respectively.
There are two common violations of the binomial law. The Ô¨Årst comes in the form of
dependent Bernoulli essays, whereas the second stems from Bernoulli essays with diÔ¨Äerent
probabilities. An example of the latter is the Exercise 1 in Section 4.1.1. As for the former,
dependence in the essays may entail a very strong impact in the probability distribution
function, much more than changing probabilities. Suppose, for instance, the Bernoulli essays
exhibit positive dependence. The sum of their values X = Y1 + Y2 + . . . + Yn will then have
a variance of var(X) = n
i=1 var(Yi) + 2 n
i=2 Cov(Y1, Yi). The Ô¨Årst term is equal to the
variance of the binomial distribution np(1 ‚àíp), which will be dominated by the second
term given that we are summing up n positive covariances. So, under positive/negative
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
76 
dependence, the binomial distribution would under/overestimate the true variance of X. In
contrast, changing probabilities will always overestimate the true variance of X as the Ô¨Årst
(very extreme!) example illustrates.
Examples
(1) Let X = Y1 + . . . + Y20, where Y1, . . . , Y10 are always equal to one and Y11, . . . , Y20 are
always equal to zero. The true variance of X is zero, though we would estimate a probability
of 1/2 under the assumption that X is binomial and hence a variance of 5.
(2) Let X = 50
i=1 Yi+100
i=51 Yi, where the Ô¨Årst and second terms form binomial distributions
B(50, 0.3) and B(50, 0.7), respectively. Assuming a binomial distribution with the average
probability of p = 0.5 yields an expected value of 50 and a variance of 25. Now, we know
from Exercise 1 in Section 4.1.1 that the true probability distribution function of X is
Pr(X = k) =
min(k,50)

k1=max(0,k‚àí50)
	50
k1

0.3k10.750‚àík1
	
50
k ‚àík1

0.7k‚àík10.350‚àík+k1
=
min(k,50)

k1=max(0,k‚àí50)
	50
k1

	
50
k ‚àík1

0.350‚àík+2k10.750+k‚àí2k1,
implying a variance of about 21.
The binomial distribution has a number of applications in practice. The most fruitful
Ô¨Ånancial application is the binomial tree model of asset returns for derivatives pricing. The
simplest binomial tree assumes that stock returns are independent over time taking value
either Œî or ‚àíŒî with probability 1/2. Running such a model for a large number of periods
yield the same solution for the price of a derivative as the Black-Scholes model. This is
not surprising given that a binomial distribution B(n, p) converges to a normal distribution
N

np, np(1 ‚àíp)

as the number of periods n increases (see Figure 4.3). To make the model
more realistic, the most advanced versions of the binomial tree may include time-varying
probabilities and/or dependence over time, which of course contradict the assumptions of
the binomial distribution.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
77 
Figure 4.3: The probability distribution function of a binomial random variable resembles
more and more the symmetric bell shape of a normal distribution as the number of essays
increases. The plots refer to binomial distributions with p = 0.25 and n ‚àà{10, 15, 20}.
Before applying the binomial distribution, we must make sure that the assumptions of
constant probability and independence hold. For instance, if we perform a survey by phone
between 16:00 and 20:00, the binomial distribution is probably not a good idea given that
it is very likely that the audience changes with the time of the day, especially before and
after working hours. Similarly, we cannot assume a binomial distribution if the interest lies
on the number of stocks with negative returns in a given week. These are not independent
events in that there are common factors that aÔ¨Äect diÔ¨Äerent stocks at the same time (e.g.,
the energy sector depends heavily on the price of oil).
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
78 
4.7.2
Hypergeometric
The hypergeometric distribution arises in situations in which we draw a sample of n units
from a population of size N consisting of two distinct groups of size N1 and N ‚àíN1, with
n < min(N1, N2), and we deÔ¨Åne X as the number of units in either one of the groups, say
the Ô¨Årst group. We know from Section 3.2.4 that the probability of any event A is given by
the ratio of the number of possible outcomes in A to the total number of possible outcomes.
Accordingly,
Pr(X = x) =
N1
x
N‚àíN1
n‚àíx

N
n

with 0 ‚â§x ‚â§n
given that there are
N1
x

ways of choosing x from the N1 units of the Ô¨Årst group,
N‚àíN1
n‚àíx

ways
of choosing the remaining n ‚àíx units from the second group, and
N
n

ways of choosing a
sample of n units from a population of size N. Figure 4.4 displays the probability distribution
function and cumulative distribution function of a hypergeometric random variable with
N = 50, N1 = 25, and n = 10.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Statistics for Business and Economics
 
79 
Figure 4.4: The left and right axes respectively correspond to the probability distribution
function and cumulative probability distribution function of a hypergeometric random vari-
able with N = 50, N1 = 25, and n = 10.
The hypergeometric distribution has an expected value of E(X) = nN1/N and variance
of var(X) = n N1
N

1 ‚àíN1
N
 N‚àín
N‚àí1. If the population size is much larger than the sample size
(i.e., N ‚â´n), the hypergeometric converges to a binomial distribution with probability
p = N1/N. Although the binomial approximation entails exactly the same expected value,
the variance diÔ¨Äers by a small-sample correction factor
N‚àín
N‚àí1.
This happens because the
only diÔ¨Äerence between the binomial and hypergeometric distributions is that the binomial
samples with reposition, whereas the hypergeometric samples without reposition and hence
the probability changes in a very particular way as we keep sampling the population. See
the quality control problem in the end of Section 3.2.4, for instance.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
80 
Example:
Consider a population of 87 Ô¨Ånancial analysts in which 13 are from one of the
largest Ô¨Ånancial institutions in the world. Suppose we wish to form a committee with 10
Ô¨Ånancial analysts, but there is a concern that too many could come from this big institution.
A rough estimate based on the binomial distribution for the probability of observing two
Ô¨Ånancial analysts from the above Ô¨Ånancial institution is
Pr(X = 2)
B‚àº
	10
2

	13
87

2	74
87

8
‚àº= 0.275,
whereas the true probability given by the hypergeometric is
Pr(X = 2) =
13
2
74
8

87
10

‚àº= 0.294.
4.7.3
Geometric
The setup of the geometric distribution is similar to that of the binomial.
We conduct
independent essays with probability p of success. In contrast to the binomial, the interest
lies on how many essays are necessary to observe the next success. This means that we Ô¨Åx
the number of success to one and let instead the number of essays to vary randomly. Letting
X denote a geometric random variable yields
Pr(X = x) = p(1 ‚àíp)x‚àí1
with x = 1, 2, 3, ¬∑ ¬∑ ¬∑
The name of the distribution comes from the fact that the cumulative probability distribution
function depends on the sum of a geometric progression. Figure 4.5 plots the probability
distribution function and cumulative distribution function of a geometric random variable
with p = 0.25.
Summing all possible outcomes yields
‚àû

x=1
Pr(X = x) =
‚àû

x=1
p(1 ‚àíp)x‚àí1 =
p
1 ‚àí(1 ‚àíp) = 1.
Along the same lines, it is possible to show that the expected value and variance of the
geometric distribution are respectively 1/p and (1‚àíp)/p2. In addition, Exercise 2 of Section
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
81 
4.1.1 shows that a geometric random variable has no memory and hence the link with the
exponential distribution in (4.8).
Figure 4.5: The left and right axes correspond to the probability distribution function and
cumulative probability distribution function of a geometric random variable with p = 0.25,
respectively.
4.7.4
Negative binomial
The last variation of the binomial distribution is the negative binomial. It has this name for
it inverts the problem of the binomial distribution in that X denotes the number of Bernoulli
essays that are necessary to observe k successes. If you wish, the negative binomial extends
the geometric distribution in that we wait for k rather than only one success to occur. The
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
82 
probability distribution function of the negative binomial is
Pr(X = x)
	x ‚àí1
k ‚àí1

pk(1 ‚àíp)x‚àík
with x ‚â•k.
Note that we employ the combination
x‚àí1
k‚àí1

because we know that the last essay must
result in a success. A negative binomial variate has an expected value of k/p, with variance
of k(1 ‚àíp)/p2.
Figure 4.6 displays the probability distribution function and cumulative
distribution function of a negative binomial random variable with p = 0.25 and k = 3.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Statistics for Business and Economics
 
83 
Figure 4.6: The left and right axes correspond to the probability distribution function and
cumulative probability distribution function of a negative binomial random variable with
p = 0.25 and k = 3, respectively.
4.7.5
Poisson
The Poisson distribution provides the simplest way to model events that occur at random over
time. It assumes that there is a constant arrival rate within a time interval and that events
are independent over time. There are a handful of applications for the Poisson distribution
in practice.
Electricity providers could well employ a Poisson distribution to model the
occurrence of electrical tempests in the areas they serve (though storms could exhibit spatial
dependence). Call centers typically assume a Poisson distribution for the number of phone
calls they receive within a given interval of time (though there could exist an underlying event
triggering many calls at approximately the same time, see example below). Commercial
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
84 
banks normally employ a Poisson distribution to model how many clients will default on
their loan payments within a given month (though it is hard to argue for independence in
periods of Ô¨Ånancial distress).
Let X denote the number of events that occur within a given time interval. We say that
X has a Poisson distribution with arrival rate Œª if its probability distribution function is
given by
Pr(X = x) = e‚àíŒªŒªx
x!
,
with x = 0, 1, 2, ¬∑ ¬∑ ¬∑
The arrival rate of Œª is relative to the time interval of reference. For instance, if we are
dealing with an arrival rate Œª per minute, then we expect 5Œª events within a 5-minute time
interval. It turns out that both the expected value and variance of a Poisson are equal to
Œª. Figure 4.7 portraits the probability distribution function and cumulative distribution
function of a Poisson random variable with an arrival rate of lambda = 5.
Example:
To make the emergency call center more eÔ¨Écient, we must model the number of
incoming telephone calls to the emergency number so as to better understand the likelihood
of events such as more than 10 phone calls within a 5-minute time interval.
We would
presumably expect that the number of incoming calls between 18:00 and 19:00 is larger than
the number of phone calls between 04:00 and 05:00. This implies that the arrival rate of
emergency calls is diÔ¨Äerent depending on the time of the day and hence we have to apply
diÔ¨Äerent Poisson distributions for diÔ¨Äerent time periods.
Although the Poisson distribution seems very diÔ¨Äerent from the binomial distribution,
there is a close link between them. To see why, let‚Äôs think about we would model the type
of situation that calls for a Poisson distribution by means of a binomial distribution. The
Ô¨Årst step is to split the time interval of reference into n very short subintervals of equal
length. The idea is to have small enough subintervals so as to ensure that the probability
of observing more than one event within a subinterval is negligible in comparison with the
probability of at most one occurrence. In other words, there is either 0 or 1 event in each
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
85 
subinterval. This paves the way to the use of a binomial distribution given that we can now
model this sequence of subintervals as a sequence of Bernoulli trials.
It remains to decide upon the probability p of observing the event, which should somehow
relate to the arrival rate Œª of the Poisson distribution. We know that, on average, there are
Œª Œît events within a time interval of length Œît. Splitting Œît into n subintervals of time
yields on average np events within Œît. It now suÔ¨Éces to equate the expected number of
events under the binomial assumption with the arrival rate of the Poisson distribution to
obtain p = Œª Œît
n . Next, we consider a random variable X that counts the number of events
within a time interval of length, say, Œît = 1.
Figure 4.7: The left and right axes correspond to the probability distribution function and
cumulative probability distribution function of a Poisson with an arrival rate of Œª = 5,
respectively.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
86 
We Ô¨Årst compute from the binomial distribution the probability of not observing any
event: Pr(X = 0) = (1 ‚àíp)n =

1 ‚àíŒª
n
n. However, we may think of imposing n ‚Üí‚àûso as
to ensure that the subintervals are small enough. This leads to Pr(X = 0) = e‚àíŒª. We next
compute a recursive relation for the probability distribution function under the binomial
assumption, namely,
Pr(X = k) =
	n
k

pk(1 ‚àíp)n‚àík
=
n!
k!(n ‚àík)! pk‚àí1(1 ‚àíp)n‚àík+1
p
1 ‚àíp
=
	
n
k ‚àí1

pk‚àí1(1 ‚àíp)n‚àík+1 n ‚àík + 1
k
p
1 ‚àíp
= (n ‚àík + 1)p
k(1 ‚àíp)
Pr(X = k ‚àí1)
= Œª ‚àí(k ‚àí1)p
k(1 ‚àíp)
Pr(X = k ‚àí1)
given that p = Œª/n. Taking limits (n ‚Üí‚àûor, equivalently, p ‚Üí0) then yields Pr(X = k) =
Œª
k Pr(X = k ‚àí1). In particular, Pr(X = 1) = Œª e‚àíŒª for k = 1, Pr(X = 2) = Œª2
2 e‚àíŒª for k = 2,
and so on. In general, Pr(X = k) = Œªk
k! e‚àíŒª just as in the Poisson distribution.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
87 
The above discussion about the intimate link between the binomial and Poisson distri-
butions is interesting because it motivates why so many credit institutions employ a Poisson
distribution to model the arrival rate of credit defaults. Although the binomial distribution
is theoretically more suitable, it is diÔ¨Écult to handle the probability distribution function of
a binomial random variable if n is too large. As the probability of a credit default is typically
very small (at least in a developed economy and in periods of normal activity), this is the
ideal setup for a Poisson approximation of the binomial distribution.
4.8
Continuous distributions
In this section, we brieÔ¨Çy review the few continuous distributions that we have seen in the
previous sections. In addition, we introduce a series of other continuous distributions, most
of them deriving from the normal distribution. Also known as the Gaussian distribution,
the latter is the most important distribution in statistics not only because it is often a good
assumption in practice, but also because it naturally arises in theory to approximate the
distribution of the sample mean of any random variable (regardless of its distribution). This
last result is known as the central limit theorem, which we will study in Section 5.2.
4.8.1
Uniform
This is the continuous counterpart of equiprobable events in that any interval of a given
length within the support of the distribution will have exactly the same probability. More
formally, let X denote a uniform random variable in the interval [Œ±, Œ≤], with density function
fX(x) =
‚éß
‚é®
‚é©
1
Œ≤‚àíŒ±, if Œ± ‚â§x ‚â§Œ≤
0,
otherwise.
We have already shown that the expected value of a uniform random variable is given by
the average between the upper and lower limits of the support, i.e., E(X) = Œ±+Œ≤
2 , whereas
the variance is var(X) = (Œ≤‚àíŒ±)2
12
.
Example:
Suppose that the delay of a given tram is uniformly distributed between 0 and
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
88 
20 minutes during winter. This means that the probability of observing a delay of at least
8 minutes is
Pr(X ‚â•8) = 1 ‚àíPr(X ‚â§8) = 1 ‚àíFX(8)
=
 20
8
1
20 dx = 1 ‚àí
 8
0
1
20 dx
= 1 ‚àí8
20 = 12
20 = 3/5.
4.8.2
Exponential
The exponential distribution arises in a setting very similar to the one of the Poisson distri-
bution. The diÔ¨Äerence is that the interest now lies on the time spell we must wait to observe
the next event. As in the Poisson context, we assume that events are independent over time
and the arrival rate is constant. Applications abound in quality control, including fatigue
and reliability analysis. In Ô¨Ånance, there is a new strand of the literature that aims to model
the time between trades so as to better understand market activity. In addition, it is also
interesting to observe how much time it takes to observe a change in prices for it conveys
information about market volatility. Finally, labor economists are keen on carrying out du-
ration analyses so as to study unemployment spells and time to promotions. In general, the
exponential distribution plays a major role in duration analysis regardless of whether the
duration has an economic, Ô¨Ånancial or quality-control interpretation.
The density function of an exponential variate is fX(x) =
1
Œª e‚àíŒªx with x > 0 (zero
otherwise), whereas the survival function SX(x) = 1 ‚àíFX(x) = Pr(X > x) is given by
Pr(X > x) =
 ‚àû
x
fX(t) dt = 1 ‚àí
 x
0
1
Œª e‚àít/Œª dt = e‚àíx/Œª.
This naturally means that the cumulative distribution function is FX(x) = 1‚àíe‚àíx/Œª, though
it is more common in duration analysis to talk about the survival function. Both the expected
value and the standard deviation of the exponential distribution are equal to Œª, which reminds
us again of the Poisson distribution whose expected value is equal to the variance. Figure
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
89 
4.8 portraits the probability density and distribution functions of a standard exponential
random variable (i.e., Œª = 1).
As aforementioned, the exponential distribution somewhat resembles the geometric dis-
tribution in that it features no memory given that
Pr(X > s + t|X > s) = Pr(X > s + t)
Pr(X > s)
= e‚àí(s+t)/Œª
e‚àís/Œª
= e‚àít/Œª = Pr(X > t).
The probability of waiting another interval of time of at least t is constant regardless of how
much we have been waiting for.
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
probability density function
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
cumulative density function
Figure 4.8: The left and right axes respectively correspond to the probability density and dis-
tribution functions of a standard exponential random variable, that is to say, an exponential
with Œª = 1.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
90 
4.8.3
Normal and related distributions
The normal (or Gaussian) is the fundamental distribution in statistics not only because it
naturally appears in a wide array of situation, but also because of the central limit theorems
that ensure it provides a good approximation in large samples for the sample mean of almost
any random variable. The normal distribution is also easy to manipulate for a number of
reasons. First, it is completely characterized by its mean and variance, which we denote by
Œº and œÉ2, respectively. Second, in contrast to the distributions we have seen so far, there is
no connection between the mean and variance of a Gaussian variate. This confers an extra
level of Ô¨Çexibility to the normal distribution, explaining why it seems to work well in all sorts
of situations. Third, the normal distribution is close under aÔ¨Éne transformations in that a
linear combination of Gaussian random variables is also Gaussian.
Probability distributions
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360¬∞
thinking.
¬© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Statistics for Business and Economics
 
91 
The density function of a normal random variable is given by
fX(x) =
1
‚àö
2 œÄ œÉ2 exp

‚àí1
2
	x ‚àíŒº
œÉ

2
with ‚àí‚àû< x < ‚àû.
We denote a normal random variable with mean Œº and variance œÉ2 by X ‚àºN(Œº, œÉ2).
There is no closed-form solution for the cumulative distribution function of a normal random
variable and hence we must tabulate it. Naturally, it would be impossible to evaluate the
distribution function of X ‚àºN(Œº, œÉ2) for every value in the real line and for every mean-
variance combination.
Figure 4.9: The left and right axes correspond to the probability density and distribution
functions of a standard normal random variable, respectively.
To circumvent this problem, we tabulate only the standard normal distribution N(0, 1),
which has zero mean and unit interval, given that we can obtain any other normal distribution
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
92 
by means of a simple aÔ¨Éne transformation: Z =
X‚àíŒº
œÉ
‚àºN(0, 1) if X ‚àºN(Œº, œÉ2). The
standard normal distribution and density play such a major role in statistics that we denote
them by Œ¶(¬∑) and œÜ(¬∑), respectively. The latter is œÜ(z) =
1
‚àö
2œÄ exp

‚àí1
2 z2
for z ‚ààR. Figure
4.9 displays the probability density and distribution functions of a standard normal random
variable.
The normal distribution is symmetric and hence all odd centered moments are equal
to zero.
In contrast, the variance of the normal distribution determines the magnitude
of every even centered moment. For instance, the kurtosis of the normal distribution is
k(X) = E
X‚àíŒº
œÉ
4 = E(Z4) = 3 and that‚Äôs why some people refer to k(X) ‚àí3 as excess
kurtosis. Figure 4.10 shows precisely how increasing the dispersion aÔ¨Äects the shape of a
normal density function and hence the probability of observing extreme realizations.
Figure 4.10: The probability density function of a normal random variable with zero mean
and standard deviation œÉ ‚àà{1, 2, 3}.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
93 
Figure 4.11: The Ô¨Årst panel plots the probability density and distribution functions of a
chi-square random variable in the left and right axes, respectively. The second panel shows
how the shape of the chi-square density changes with the degrees of freedom.
The normal distribution gives way to a number of interesting distributions depending
on how we transform it. In what follows, we discuss two distributions that derive from the
Gaussian distribution and are of particular interest in the context of statistical inference.
The Ô¨Årst is the chi-square distribution, which consists of the sum of a number of squared
independent standard normal random variables. Let Zi ‚àºN(0, 1) denote a sequence of
mutually independent standard normal distributions for i = 1, . . . , N. It then follows that
œá2
N = N
i=1 Z2
i is a chi-square distribution with N degrees of freedom. The mean of œá2
N is N,
whereas its variance is twofold amounting to 2N. The chi-square is important in a number
of situations. For instance, the chi-square distribution arises in a very natural manner if
we wish to compute the probability that a standard normal random variable belongs to
a symmetric interval around zero. Figure 4.11 not only plots the probability density and
distribution functions of a chi-square random variable, but also shows how it changes with
the degrees of freedom.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
94 
Figure 4.12: The Ô¨Årst panel plots the probability density and distribution functions of a
t-student random variable in the left and right axes, respectively. The second panel shows
how the shape of t-student density varies with the degrees of freedom.
The second is the t-student distribution, which stems from a ratio of a standard normal
distribution to the square root of an independent chi-square distribution divided by its
degrees of freedom. In particular, we denote a t-student with N degrees of freedom by tN =
Z0
‚àö
1
N
N
i=1 Z2
i , where Zi‚Äôs are independent standard normal distributions for i = 0, 1, . . . , N.
The t-student is symmetric around the origin and hence has mean zero. In turn, the variance
of a t-student random variable is N/(N ‚àí2), with N denoting the degrees of freedom. It is
easy to see that the variance is ill-deÔ¨Åned if there are not enough degrees of freedom in that
N ‚â§2 implies a negative variance. This is true in general for the t-student distribution in
that the kth moment exists if and only if the degrees of freedom exceed k. We will see later
that the t-student distribution is paramount to hypothesis testing under the assumption of
normality.
Figure 4.12 depicts the probability density and distribution functions of a t-student ran-
dom variable with 3 degrees of freedom in the Ô¨Årst panel, whereas the second panel illustrates
how the shape of its density changes with the degrees of freedom. In particular, it is notice-
able that the t-student converges to a standard normal distribution as the degrees of freedom
increase.
Probability distributions
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
95 
Chapter 5
Random sampling
They way we collect the data is extremely important because, ideally, we would like to end
up with a random sample. The latter consists of independent and identically distributed
(iid) observations from the population. This is the ideal setup for two reasons. If data are
independent, then it is easy to characterize the joint distribution because it is equivalent to
the product of the marginals. In addition, if the data come from a common distribution,
the marginals are all identical, thereby depending on the same vector Œ∏ of parameters. We
formalize these ideas using the joint density function of a random sample X = (X1, . . . , XN),
namely, fX(x1, . . . , xN) = N
i=1 fXi(xi) = N
i=1 fX(xi; Œ∏). The Ô¨Årst equality follows from in-
dependence, whereas the second ensues from the fact that the elements of the random sample
are identically distributed. We typically represent a random sample by Xi ‚àºiid fX(¬∑; Œ∏) for
i = 1, . . . , N.
Data collection is not easy. It is indeed quite diÔ¨Écult to design a data sampling procedure
that is free of any bias.
The most common problems in business and economics relate
to censorship, selection, survivorship, and no-answer biases. Censorship bias takes place
whenever we cannot observe data within a given interval. For instance, if there are price
limits in a stock exchange, we cannot observe stock prices either above the upper limit or
below the lower limit given by the maximum daily oscillation. In such a context, censoring
may occur for, even if the equilibrium price moves above or below the limits, we observe at
most a price at one of the limits. The similar problem arises in exchange-rate target zones.
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
96 
The only diÔ¨Äerence is that a successful speculative attack against the currency may cause
the break of the target zone (i.e., price limits). A very diÔ¨Äerent situation in which censorship
plays a major role is in labor economics. For instance, if we are measuring how much time it
takes for an individual to obtain a promotion, our data set will invariably include individuals
who haven‚Äôt been promoted yet. So, the most we can say is that the time to promotion of
these individuals is larger than the number of periods we have been observing them in our
sample.
Selection bias occurs in the event that the sampling procedure is such that the data
tend to come from a speciÔ¨Åc group within the population. For instance, in most developing
countries, women must decide whether they join the work force or stay at home full time
taking care of the house chores, whereas men rarely have such an option. This means that we
cannot directly compare the salary of male and female workers. They are diÔ¨Äerent not only
in gender, but also because female workers have taken a previous decision to join the labor
market, whereas men didn‚Äôt. Taking such a decision shows some degree of ambition and
determination that is probably correlated with productivity and hence with salary. That is
why a simple comparison of wage diÔ¨Äerentials will normally underestimate the discrimination
against women in the labor market. The same reasoning applies to immigrants, as well. The
simple fact that they have taken a previous decision to migrate, while others in the same
situation didn‚Äôt, indicates that they form a diÔ¨Äerent group (perhaps more ambitious, focused,
and determined). In Ô¨Ånance, selection bias may also aÔ¨Äect asset returns through a liquidity
(rather than competition) channel. Illiquid assets by deÔ¨Ånition trade less frequently than
liquid assets and hence they are much more likely to exhibit price staleness. If transaction
prices do not change, then the return is zero. However, zero returns are not really reÔ¨Çecting
the true change in the value of the asset in that they are merely an artifact due to illiquidity.
This means that we cannot treat zero returns and nonzero returns in the same way.
Survivorship bias arises whenever we are looking at a sample of individuals/Ô¨Årms/units
resulting from some sort of competition. Although it is much more natural to think about
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
97 
survivorship bias in biology, where evolution establishes intense competition among diÔ¨Äerent
genes, examples abound in economics, Ô¨Ånance, and management. For instance, hedge funds
that perform poorly end up managing less funds that those doing well. As most indices
are weighted by assets under management, they tend to reÔ¨Çect more the performance of
the hedge funds that perform well over time, overestimating the overall performance in the
industry. In fact, hedge funds that perform systematically poorly end up closing their doors,
thereby disappearing from databases at a certain point in time. Survivorship bias then arises
very strongly if we collect data by sampling the returns of all funds that currently exist since
some date in the past. To avoid such a bias, we must Ô¨Årst choose the starting data and then
collect the data of all funds were operational since then. In this way, the data set would
include not only the successful funds that are still in action, but also those that did do very
well and ceased to exist.
The no-answer bias is very common in surveys. People who have strong opinions, es-
pecially negative, are typically much more inclined to answer a survey. That‚Äôs why most
lecturers are very keen to publicize teaching evaluation surveys to students. If they don‚Äôt,
it is very likely that the answers will have a negative bias for students that do not have
many criticisms will presumably not bother to answer the survey as much as the students
who are not happy. In work environments, we could well argue that workaholics tend not to
respond work-unrelated surveys (e.g., menu of the eatery) for they prefer to dedicate their
time to more productive tasks (it is also very likely that they bring their own sandwich from
home to spend less time in lunch breaks!). This means that work-unrelated surveys will not
represent entirely the views of the population in the Ô¨Årm due to the lack of answers from
the workaholics.
Example:
Suppose a commercial bank would like to work out how many days on average
it takes to process a cheque. The cost of sampling all cheques is prohibitively high and hence
the person in charge decides to draw a sample of 1,000 cheques. The snag is how to draw a
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
98 
random sample given that we cannot simply tag all cheques with a number and then draw
from a discrete uniform distribution. One solution is to draw every mth cheque that the
bank processes until we observe 1,000 cheques. This type of sampling is not entirely random
in that we will never observe two consecutive cheques (from the same Ô¨Årm, perhaps!) in the
sample, but it should do the trick reasonably well.
The above example illustrates well the fact that random sampling is an abstract notion.
In some situations, it is virtually impossible to draw a completely random sample, and hence
we must do with samples that are ‚Äú random enough‚Äù. The next series of examples are more
concrete in that they establish alternative procedures to draw a random sample in all sort
of setups.
Random sampling
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Statistics for Business and Economics
 
99 
Examples
(1) Suppose a marketing Ô¨Årm wishes to interview 1,000 households in a town. One solution
is to draw from a discrete uniform distribution random numbers that identify households
given their addresses (or post codes). The interviewer then visit the address between 15:00
and 18:00 and, if no one answer, that we eliminate that address and replace by another
drawn at random from the same discrete uniform distribution.
(2) Suppose a bookstore in the university campus wishes to evaluate the stock of textbooks
before the beginning of the term. We may draw a random number that identiÔ¨Åes a given
location in the shelves and then check the textbook in that location as well as the 50 closest
textbooks.
(3)
A manager would like to assess the performance of the cleaning staÔ¨Ä. Her assistant
comes up with two assessment strategies. The Ô¨Årst involves inspecting 15 oÔ¨Éces completely
at random, whereas the second strategy chooses one oÔ¨Éce at random from each of the three
Ô¨Çoors of the building and then inspect them as well as the 9 oÔ¨Éces closest to each of them.
The time to completion is the same for both strategies despite the fact the second strategy
inspects the double of oÔ¨Éces. The manager decides for the Ô¨Årst strategy for it really entails a
random sample. She rightly explains to her assistant that, even though the second strategy
seems more eÔ¨Écient at Ô¨Årst glance, it is less convenient for it would not generate a random
sample. The reason is simple. The allocation of the cleaning staÔ¨Äis typically by wing and
Ô¨Çoor, and hence, by restricting attention to a given area of the Ô¨Çoor (i.e., the neighborhood
of the sampled oÔ¨Éce), she would risk to assess the work of a particular subset of janitors
rather than the overall performance of the cleaning staÔ¨Ä.
5.1
Sample statistics
Let X = (X1, . . . , XN) denote a random sample with density function fX(¬∑; Œ∏). We don‚Äôt
know the true value of the vector Œ∏ of population parameters and so the goal is to infer
it from the realization of the random sample, say, x = (x1, . . . , xn). Note that x is just
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
100 
one possible realization for the random sample X with probability mass given by the joint
density function evaluate at x, namely, fX(x; Œ∏) = N
i=1 fX(xi; Œ∏).
Example:
Let the random variable X come from a discrete uniform distribution that takes
integer values between 1 and 6. This means that the sample space is {1, 2, 3, 4, 5, 6}, whose
elements are drawn with probability 1/6. Suppose now that we take three random samples
of three observations, say (1, 1, 4), (2, 4, 5), and (2, 3, 2). Their sample means are respectively
2, 11/3, and 7/3, though the expected value of X is (1 + 2 + 3 + 4 + 5 + 6)/6 = 7/2.
What we wish to illustrate with the above example is that the sample mean is also a
random variable, whose distribution depends on the distribution from which we draw the
random sample. Accordingly, the value we observe for the sample mean varies with the
sample we actually draw. Each diÔ¨Äerent sample yields a distinct value for the sample mean.
Needless to say, this holds for any function g(X) = g(X1, . . . , XN) of the sample, which
we call sample statistic. To conduct inference, we must always bear in mind that a sample
statistic is random and hence we must determine its distribution, which we call sampling
distribution.
Example:
Let X = (X1, . . . , XN) denote a random sample from a normal distribution
with mean Œº and variance œÉ2, i.e., Xi ‚àºiid N(Œº, œÉ2). The sample mean ¬ØXN = 1
N
N
i=1 Xi is
a random variable with expected value
E( ¬ØXN) = E

1
N
N

i=1
Xi

= 1
N
N

i=1
E(Xi) = 1
N
N

i=1
Œº = 1
N (NŒº) = Œº
and variance
var( ¬ØXN) = var

1
N
N

i=1
Xi

= 1
N 2
N

i=1
var(Xi) = 1
N 2
N

i=1
œÉ2 = 1
N 2 (NœÉ2) = œÉ2/N.
Finally, as the sample mean is a linear combination of normal random variables, it is also
normal. We thus conclude that ¬ØXN ‚àºN(Œº, œÉ2/N), which diÔ¨Äers from the normal distribution
from which we draw the sample.
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
101 
In the next section, we will show that, as long as the sample size is large enough, normality
is often a very good approximation for most sampling distributions, even if the distribution
from which we draw the sample is not normal (and even unknown).
The next example
concludes this section by depicting such a situation.
Example:
The number of transactions on the Macau stock market is on average of 62,000
trades per week with a standard deviation of 7,000. Suppose now that we take note of the
number of transactions per week within a year. The expected value of the sample mean is
E( ¬ØXN) = 62, 000 trades per week, with a variance of var( ¬ØXN) = 7, 0002/52 = 942, 307.69
given that there are 52 weeks in a year. The normal approximation for the sample mean
distribution yields a probability of observing a sample mean at least two standard deviations
away from its mean, i.e., Pr

¬Ø
XN‚àí62,000
942,307.69
 > 2

, of about 5%.
Random sampling
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Statistics for Business and Economics
 
102 
5.2
Large-sample theory
In this section, we will discuss how to approximate the sampling distribution of a statistic in
general. In particular, we will talk about asymptotic approximations in that we will let the
sample size grow to inÔ¨Ånity. Although these results hold only in the limit, we will see that
they often provide a good guidance to the behavior of most statistics as long as the sample
size is large enough. What is ‚Äòlarge enough‚Äô will of course depend on the task at hand. If we
employ asymptotic results to approximate the distribution of the mean of a random sample
coming from a uniform distribution, large enough could mean even 5 observations.
We Ô¨Årst discuss what we mean by limit theory in the context of random variables. In
particular, we establish several modes of convergence for sequences of random variables.
Next, we establish some convergence results for sample means given that they play a major
role in statistics. There are two asymptotic results that permeate almost any problem of
statistical inference: Laws of large numbers say that sample means converge (in some sense
that we will precise in the next section) to the population mean, whereas central limit
theorems single out the conditions under which we can approximate the distribution of a
sample mean with a normal distribution.
5.2.1
Modes of convergence
Let X1, X2, . . . denote a sequence of random variables, which we denote simply by XN despite
of the abuse of notation. We say that XN converges in probability to a constant a, which
we denote by XN
p
‚àí‚Üía, if limN‚Üí‚àûPr(|XN ‚àía| < œµ) = 1 for any œµ > 0. We call a the
probability limit of XN, which we denote by plimN‚Üí‚àûXN = a. The probability limit is a
natural generalization of the mathematical notion of a limit. A stronger mode of convergence
follows if we switch the order of the limit and probability operators. Indeed, it is much
more stringent to impose that Pr (limN‚Üí‚àûXN = a) = 1 for it constrains the function that
the random variables in the sequence XN use to map the sample space to the real line.
This is what we call almost sure convergence or, equivalently, convergence with probability
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
103 
one, which we denote by XN
a.s.
‚àí‚Üía.
Mean squared convergence denotes a situation in
which a sequence XN of random variables converge in mean square to a constant a in that
limN‚Üí‚àûE(XN ‚àía)2 = 0. We then say that a is the mean square limit of the sequence XN
and write XN
m.s.
‚àí‚Üía.
These modes of convergence admit various extensions. For instance, we can consider the
convergence to a random variable by writing that XN
fmc
‚àí‚ÜíX if and only if XN ‚àíX
fmc
‚àí‚Üí0,
where fmc ‚àà{p, a.s., m.s} denotes your favorite mode of convergence.
In addition, we
can also think of sequences of random vectors (or matrices) rather than sequences of scalar
random variables.
In this event, it suÔ¨Éces to apply your favorite mode of convergence
element-wise.
The modes of convergence we have seen so far are pretty strong in that the sequence of
random variables in the limit becomes degenerate given that it converges to a simple constant
(without any form of randomness). In contrast, convergence in distribution deals with limit-
ing distributions. We say that XN converges in distribution to X if limN‚Üí‚àûFXN(x) = FX(x)
for any x ‚ààR. We call FX the asymptotic (or limiting) distribution of XN and denote this
mode of convergence by XN
d
‚àí‚ÜíX or XN
d
‚àí‚ÜíFX.
Convergence in distribution is the weakest of the convergence deÔ¨Ånitions in that we con-
strain only the distribution of the random variable and not the values it take. In particular,
it is possible to show that
XN
a.s.
‚àí‚ÜíX
XN
m.s.
‚àí‚ÜíX
‚é´
‚é¨
‚é≠
‚áí
XN
p
‚àí‚ÜíX
‚áí
XN
d
‚àí‚ÜíX.
As before, we can also extend the notion of convergence in distribution to a multivariate
setting (i.e., to random vectors).
Finally, we conclude this section by showing how to manipulate and combine the diÔ¨Äerent
modes of convergence. We Ô¨Årst note that g(XN)
p
‚àí‚Üíg(X) if XN
p
‚àí‚ÜíX and that g(XN)
d
‚àí‚Üí
g(X) if XN
d
‚àí‚ÜíX provided that g(¬∑) is a continuous function. This pair of results is known
as the continuous mapping theorem. The Slutsky theorem is one of the various applications
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
104 
of the continuous mapping theorem, ensuring not only that XN +YN
d
‚àí‚ÜíX +a if XN
d
‚àí‚ÜíX
and YN
p
‚àí‚Üía, but also that XNYN
p
‚àí‚Üí0 if XN
d
‚àí‚ÜíX and YN
p
‚àí‚Üí0. Finally, it is easy to
see that AN XN
d
‚àí‚ÜíA X if XN
d
‚àí‚ÜíX and AN
p
‚àí‚ÜíA, as well.
Example:
Let XN denote a k-dimensional random vector such that XN
p
‚àí‚ÜíŒº and
‚àö
N(XN ‚àíŒº)
d
‚àí‚ÜíZ, where Œº is a vector of constants and Z is a random vector with
some known distribution. Let now Œ±(¬∑) : Rk ‚ÜíRr denote a multivariate function with
continuous Ô¨Årst-order derivatives. It then follows from a simple Ô¨Årst-order Taylor expansion
that
‚àö
N

Œ±(XN) ‚àíŒ±(Œº)

d
‚àí‚ÜíA Z, where A = ‚àÇŒ±(Œº)
‚àÇŒº‚Ä≤ . To appreciate why, note that the
Ô¨Årst-order Taylor expansion is Œ±(XN) = Œ±(Œº)+ ‚àÇŒ±(Œº‚àó)
‚àÇŒº‚Ä≤
(XN ‚àíŒº) with Œº‚àó= ŒªŒº+(1‚àíŒª)XN
for some Œª in the unit interval. Multiplying both sides by
‚àö
N then yields the result given
that Œº‚àó
p
‚àí‚ÜíŒº for any Œª provided that XN
p
‚àí‚ÜíŒº.
The above example illustrates the delta method, which consists of a useful tool to derive
the asymptotic distribution of a known function of a statistic. It is interesting to note that,
even though the example assumes that XN converges in probability to a vector of constants,
it suÔ¨Éces to multiply by
‚àö
N to avoid the convergence in probability and obtain convergence
in distribution. The next section discusses more thoroughly the conditions under which this
may happen in the context of sample means.
5.2.2
Limit theory for sample means
We can write most statistics of interest as sample means. For instance, the sample variance
is the sample mean of the squared deviations with respect to the mean value in the sample.
This means that we can learn a lot about the asymptotic behavior of most statistics if
we understand well what happens with a generic sample mean in large samples. In what
follows, we will discuss two sorts of asymptotic results: Laws of large numbers (LLN) handle
convergence in probability and almost sure convergence, whereas central limit theorems
(CLT) deal with convergence in distribution for sample means.
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
105 
We start with Chebyshev‚Äôs weak law of large numbers, which posits that ¬ØXN ‚â°1
N
N
i=1 Xi
p
‚àí‚Üí
Œº if limN‚Üí‚àûE( ¬ØXN) = Œº and limN‚Üí‚àûvar( ¬ØXN) = 0. It is easy to see why this result hold by
noting that these moment conditions essentially ensure convergence in mean square, which
in turn implies convergence in probability. Also, the above result does not require a random
sampling given that we are not necessarily imposing that E(Xi) is constant for i = 1, . . . , N.
If we compute the sample mean of a random sample, it then suÔ¨Éces to assume that E |X| < ‚àû
to obtain ¬ØXN
p
‚àí‚ÜíŒº ‚â°E(X). We will refer to this result as Khintchine‚Äôs weak law of large
numbers for random samples. By imposing a slightly more stringent condition, it is also
possible to show that the sample mean almost surely converge to the true mean of a random
sample (also known as Kolmogorov‚Äôs strong law of large numbers).
Finally, we tackle the asymptotic distribution of a sample mean by means of Lindeberg-
L¬¥evy central limit theorem, which says that
‚àö
N( ¬ØXN ‚àíŒº)
d
‚àí‚ÜíN(0, œÉ2) as long as Xi is iid
with E |Xi|2 < ‚àû. To sum up, a sample mean ¬ØXN of iid random variables with Ô¨Ånite mean
and variance is such that ¬ØXN
a.s.
‚àí‚ÜíŒº (due to LLN) and
‚àö
N( ¬ØXN ‚àíŒº)
d
‚àí‚ÜíN(0, œÉ2) (due to
CLT).
There is a whole bunch of diÔ¨Äerent laws of large numbers and central limit theorems that
deal with all sorts of settings. Extensions include, among others, LLN and CLT for random
variables that are dependent and/or non-identically distributed as well as for random vectors
and matrices.
Although there is not much gain in showing/memorizing all the diÔ¨Äerent
conditions under which we can derive either a LLN or a CLT, it is important to know that
such results exist for a wide array of situations.
Example:
Let us revisit the last example of Section 5.1. Assuming that the number of
transactions per week on the Macau stock exchange is iid over time, with some Ô¨Ånite mean
and variance, ensures that the normal approximation for the distribution of the sample
mean will work well in practice. However, it is sort of risky to assume that the number of
transactions per week is independent over time. A simple inspection of a time series plot
Random sampling
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
106 
for such a series would typically indicate that market activity cluster over time. Although
this does not aÔ¨Äect the application of Chebyshev‚Äôs weak LLN, we must consider a CLT
that allows for dependence over time. There are indeed several central limit theorems that
relax the assumption of independence (which we will not review here given that they impose
primitive conditions that restrict the dependence between observations in quite complicated
manners).
Random sampling
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Statistics for Business and Economics
 
107 
Chapter 6
Point and interval estimation
An estimator is a statistic that we employ to estimate an unknown population parameter
such as, for example, a population mean. It is also a random variable in that its value
depends on the particular realization of the sample. A parameter estimate then is the value
that we observe for an estimator given the sample. The next example illustrates the fact
that we can always think of many diÔ¨Äerent estimators for a given quantity.
Example:
Let X denote a Gaussian random variable with mean 1/3 and unknown
variance, that is, X ‚àºN(1/3, œÉ2). Suppose that we draw a random sample of 5 observations
with values: x1 = 1
3, x2 = 1
4, x3 = 1
2, x4 = 1
3, and x5 = 2
9. We could estimate the population
mean by any of the following estimators:
1. !Œº1 = X1, which produces an estimate of 1
3;
2. !Œº2 = X1+X4
2
, which entails an estimate of 1
3;
3. !Œº3 = X2, giving way to an estimate of 1
4; and
4. !Œº4 = ¬ØX5 = 1
5
5
i=1 Xi, leading to an estimate of
59
180.
Needless to say, the list above is not exhaustive at all and many other estimators exist
for the population mean. That‚Äôs exactly why we must come up with some criteria to choose
between estimators. Sections 6.1.1 to 6.1.3 discuss some intuitive criteria, whereas Sections
6.1.4 and 6.1.5 describe the two most popular estimation methods in statistics. Regardless of
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
108 
the method we employ, the usual estimation procedure involves three steps. First, we must
draw/observe a random sample from the population of interest. Second, we must calculate
a point estimate of the parameter. By point estimation, we mean assigning a unique value
to the estimate as opposed to an interval of possible values as in interval estimation. Third,
we must compute a measure of variability for the estimator that accounts for the sampling
variation in the data. This often takes the form of computing conÔ¨Ådence intervals.
In what follows, we Ô¨Årst discuss point estimation and then turn attention to the more
interesting problem of interval estimation. We say interval estimation is more interesting for
it allows us to bridge the two main strands of statistical inference, namely, estimation and
hypothesis testing.
6.1
Point estimation
We denote by !Œ∏N a point estimator of Œ∏ based on a sample of N observations, though we will
sometimes omit the dependence on the sample size to simplify notation. We next deÔ¨Åne in
a more formal manner what we mean by point estimator.
Consider a sample X(N) = (X1, . . . , XN) of N random variables that we denote by Xi
for i = 1, . . . , N. We know that a statistic is a function of the sample X(N) and we deÔ¨Åne
a point estimator as a statistic !Œ∏N ‚â°!Œ∏(X(N)) ‚â°!Œ∏(X1, . . . , XN) that we employ to infer the
value of a parameter Œ∏ of the joint distribution of X(N) = (X1, . . . , XN). The parameter
estimate is the realization of the estimator, that is to say, !Œ∏N ‚â°!Œ∏(x1, . . . , xN). Note the
abuse of notation in that we refer to both estimator and estimate as !Œ∏N.
There are several possible estimators for any parameter Œ∏ and hence we will discuss in
what follows the sort of properties we would like our estimator to hold. In particular, we will
start with the deÔ¨Ånition of the mean squared error of a given estimator so as to motivate
the discussion about unbiasedness, consistency, and eÔ¨Éciency.
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
109 
6.1.1
Mean squared error
As there are many candidate estimators for a given population parameter, it seems paramount
to rank them through some measure of precision. The most popular measure is the mean
squared error, which gauges the average distance of the estimator to the true parameter
value by means of a quadratic distance.
For a sample X(N), the error of the estimator
!Œ∏N ‚â°!Œ∏(X(N)) is given by !Œ∏N ‚àíŒ∏. Note that the estimation error depends not only on the
estimator, but also on the particular sample we observe. DiÔ¨Äerent samples will give distinct
point estimates for the population parameter of interest.
DeÔ¨Ånition:
The mean squared error of !Œ∏N is MSE(!Œ∏N, Œ∏) = E(!Œ∏N ‚àíŒ∏)2.
The intuition for a measure such as the MSE is straightforward. We square the estima-
tion error before taking averages in order to avoid negative values canceling out with positive
values. It thus measure how far, on average, the set of estimates are from the population pa-
rameter of interest. The nicest thing about the mean squared error is that we can decompose
it into two readily interpretable components:
MSE(!Œ∏N, Œ∏) = E(!Œ∏N ‚àíŒ∏)2 = E
"
!Œ∏N ‚àíE(!Œ∏N) + E(!Œ∏N) ‚àíŒ∏
#2
= E
$"
!Œ∏N ‚àíE(!Œ∏N)
#2
+
"
E(!Œ∏N) ‚àíŒ∏
#2
‚àí2
"
!Œ∏N ‚àíE(!Œ∏N)
# "
E(!Œ∏N) ‚àíŒ∏
#%
= E
"
!Œ∏N ‚àíE(!Œ∏N)
#2
+ E
"
E(!Œ∏N) ‚àíŒ∏
#2
‚àí2 E
"
!Œ∏N ‚àíE(!Œ∏N)
# "
E(!Œ∏N) ‚àíŒ∏
#
= var(!Œ∏N) +
"
E(!Œ∏N) ‚àíŒ∏
#2
‚àí2 E
"
!Œ∏N ‚àíE(!Œ∏N)
# "
E(!Œ∏N) ‚àíŒ∏
#
= var(!Œ∏N) +
"
E(!Œ∏N) ‚àíŒ∏
#2
.
This decomposition clariÔ¨Åes that the mean squared error is the sum of the variance of the
estimator and the squared bias of the estimator. The variance indicates how far, on average,
the set of estimates are from their expected value and hence it is a measure of precision of
the estimator. We denote the square root of the variance of an estimator by standard error
(rather than standard deviation). In contrast, we deÔ¨Åne the bias of !Œ∏N as E(!Œ∏N) ‚àíŒ∏, that
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
110 
is to say, the diÔ¨Äerence between the average estimate and Œ∏ or, equivalently, the average
estimation error. The bias thus measures how accurate is the estimator. We will later show
that there is a bias-variance tradeoÔ¨Ä, which translates into a trade-oÔ¨Äbetween accuracy and
precision.
We say that !Œ∏ is an unbiased estimator of Œ∏ if and only if E(!Œ∏N) = Œ∏. The bias is a
property of the estimator, not of the estimate.
Ideally, we would like to have the most
accurate and precise estimator. The next deÔ¨Ånition formalizes this idea by setting up the
MSE criterion for choosing between estimators.
DeÔ¨Ånition:
Let Œò denote the parameter space that collects all possible values for the
population parameter Œ∏. Consider two estimators !Œ∏1 and !Œ∏2 for Œ∏. We say that !Œ∏1 is more
eÔ¨Écient than !Œ∏2 if MSE(!Œ∏1, Œ∏) ‚â§MSE(!Œ∏2, Œ∏) for every Œ∏ ‚ààŒò and MSE(!Œ∏1, Œ∏) < MSE(!Œ∏2, Œ∏)
for at least one value of Œ∏ ‚ààŒò.
Point and interval estimation
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
‚Ä¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
‚Ä¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
‚Ä¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Statistics for Business and Economics
 
111 
Example:
Let X(N) = (X1, . . . , XN) denote a random sample of iid N(Œº, œÉ2), whose
parameters we estimate by means of !ŒºN =
1
N
N
i=1 Xi and ÀúœÉ2
N =
1
N‚àí1
N
i=1(Xi ‚àí!ŒºN)2,
respectively. Both estimators are unbiased in that E(!ŒºN) = 1
N
N
i=1 E(Xi) = Œº and that
E(ÀúœÉ2
N) = E

1
N ‚àí1
N

i=1
(Xi ‚àí!ŒºN)2

= E

1
N ‚àí1
N

i=1

X2
i ‚àí2Xi!ŒºN + !Œº2
N


= E

1
N ‚àí1
 N

i=1
X2
i ‚àí2 !ŒºN
N

i=1
Xi +
N

i=1
!Œº2
N

= E

1
N ‚àí1
 N

i=1
X2
i ‚àí2 !ŒºN (N !ŒºN) + N !Œº2
N

= E

1
N ‚àí1
 N

i=1
X2
i ‚àíN !Œº2
N

=
1
N ‚àí1

E
 N

i=1
X2
i

‚àíN E(!Œº2
N)

=
1
N ‚àí1

N E(X2
i ) ‚àíN E(!Œº2
N)

=
N
N ‚àí1

(Œº2 + œÉ2) ‚àí
	
Œº2 + œÉ2
N


=
N
N ‚àí1
	
1 ‚àí1
N

œÉ2 = œÉ2
as the second uncentered moment of any random variable is the sum of the square of the
Ô¨Årst moment and the variance, i.e., E(X2) = [E(X)]2 + var(X). See below for the derivation
of the variance of !ŒºN. Unbiasedness means that the mean squared error of !ŒºN and ÀúœÉ2
N are
just their variance. The variance of !Œº is
E (!ŒºN ‚àíŒº)2 = E

1
N
N

i=1
Xi ‚àíŒº
2
= E

1
N
N

i=1
(Xi ‚àíŒº)
2
= E

1
N 2

1‚â§i,j‚â§N
(Xi ‚àíŒº)(Xj ‚àíŒº)

= 1
N 2

1‚â§i,j‚â§N
E[(Xi ‚àíŒº)(Xj ‚àíŒº)]
= 1
N 2
 N

i=1
var(Xi) + 2

1‚â§i<j‚â§n
cov(Xi, Xj)

= œÉ2
N ,
given that independence ensures that cov(Xi, Xj) = 0 for all 1 ‚â§i Ã∏= j ‚â§N. It is also
possible to show that, under normality, the variance of ÀúœÉ2
N is given by
E

!œÉ2
N ‚àíœÉ22 =
2
N ‚àí1 œÉ4.
We require normality for it ensures that the fourth moment depends only on œÉ2. Consider
now the following alternative variance estimator: !œÉ2
N =
1
N
N
i=1(Xi ‚àí!ŒºN)2. This estimator
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
112 
is obviously biased in that E(!œÉ2
N) = E
 N‚àí1
N
ÀúœÉ2
N

= N‚àí1
N œÉ2. As for the variance, we employ
the same trick to show that
var(!œÉ2
N) = var
	N ‚àí1
N
ÀúœÉ2
N

=
	N ‚àí1
N

2
var(ÀúœÉ2
N)
=
	N ‚àí1
N

2
2
N ‚àí1 œÉ4 = 2(N ‚àí1)
N 2
œÉ4.
This means that the mean squared error of !œÉ2
N is
MSE(!œÉ2
N, œÉ2) = 2(N ‚àí1)
N 2
œÉ4 +
	N ‚àí1
N
œÉ2 ‚àíœÉ2

2
=
2(N ‚àí1)
N 2
+ 1
N 2

œÉ4
=
	2(N ‚àí1) + 1
N 2

œÉ4 = 2N ‚àí1
N 2
œÉ4,
which is strictly inferior to the mean squared error of ÀúœÉ2
N. In the MSE sense, it then follows
that !œÉ2
N is superior to ÀúœÉ2
N as an estimator of œÉ2.
6.1.2
Unbiasedness
Although it seems very reasonable to compare estimators purely on the basis of the mean
squared error, it is very often the case that there exists no best estimator. The reason is that
the class of all possible estimators is too large. One way to make the selection of estimators
tractable is to restrict the search to a speciÔ¨Åc class of estimators. A natural choice is the
collection of all unbiased estimators.
DeÔ¨Ånition:
An estimator !Œ∏N is a best unbiased estimator of Œ∏ if E(!Œ∏N) = Œ∏ for all Œ∏ ‚ààŒò
and var(!Œ∏N) ‚â§var(&Œ∏N) for any other unbiased estimator &Œ∏N such that E(&Œ∏N) = Œ∏.
The above deÔ¨Ånition does not help us much in the sense that it does not provides much
information about the best unbiased estimator. The next result adds to this discussion by
establishing a lower bound for the variance of any estimator. It is known as the Cram¬¥er-Rao
inequality.
Theorem:
Let X(N) = (X1, . . . , XN) denote a random vector with joint probability
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
113 
density function f(X(N); Œ∏). Consider an estimator !Œ∏N of Œ∏ such that
‚àÇ
‚àÇŒ∏ E(!Œ∏N) =
 ‚àû
‚àí‚àû
‚àÇ
‚àÇŒ∏
"
!Œ∏Nf(X(N); Œ∏)
#
dX(N)
and var(!Œ∏) < ‚àû. It then follows that
var(!Œ∏) ‚â•
‚àÇ
‚àÇŒ∏ E(!Œ∏)
E
"
‚àÇ
‚àÇŒ∏ ln f(X(N); Œ∏)
#2.
It is easy to see that the numerator of the right-hand side of the above inequality is equal
to one if the estimator is unbiased, whereas the denominator depends on the expected value
of the Ô¨Årst derivative of the logarithm of the joint density function. We call the latter the
score function, which will play a key role in Section 6.1.5.
6.1.3
Consistency
Although it makes sense to impose unbiasedness, there are biased estimators that achieve
better mean squared errors as we have seen in the example of Section 6.1.1.
The most
important is to nail the parameter Œ∏ down as the sample size increases. In that example,
for instance, ÀúœÉ2
N is asymptotically unbiased in that the bias shrinks to zero as the sample
size N goes to inÔ¨Ånity. As the Nobel prize winner Clive Granger once said, ‚ÄúIf you cannot
get it right as the sample size grows to inÔ¨Ånity, you shouldn‚Äôt be in the business‚Äù. The next
deÔ¨Ånition formalizes this notion by introducing the concept of consistent estimators.
DeÔ¨Ånition:
Let Œò denote the parameter space and {!Œ∏N; N ‚â•1} denote a sequence of
estimators of the population parameter Œ∏ ‚ààŒò indexed by the sample size N. In particular,
let !Œ∏N denote an estimator based on the Ô¨Årst N observations of a sample (X1, X2, . . .) from
a given probability distribution f(x; Œ∏). The sequence !Œ∏N is (weakly) consistent on Œò if and
only if, for all Œ∏ ‚ààŒò and for all Œµ > 0, it holds that limN‚Üí‚àûPr(|!Œ∏N ‚àíŒ∏| ‚â•Œµ) = 0.
So, a consistent estimator converges in probability to the true value of the population
parameter. There is no problem in using a diÔ¨Äerent notion of convergence. If we are talking
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
114 
about convergence in mean square then we say the estimator is consistent in mean square.
Similarly, a strongly consistent estimator converges almost surely to the true value of the
parameter.
Example:
Suppose that X1, X2, . . . is a sequence of random variables drawn from a
N(Œº, œÉ2) distribution. To estimate Œº based on the Ô¨Årst N observations, we usually use the
sample mean !Œ∏N = (X1 + . . . + XN)/N, which is an unbiased estimator with variance of
œÉ2/N. Given that linear combinations of normal variates are also normal, !Œ∏n is normal with
mean Œº and variance œÉ2/N and hence
‚àö
N (!Œ∏N ‚àíŒº)/œÉ has a standard normal distribution.
It then follows that
Pr(!Œ∏N ‚àíŒº ‚â•Œµ) = Pr

‚àö
N
!Œ∏N ‚àíŒº
œÉ
‚â•
‚àö
N Œµ
œÉ

= 1 ‚àíŒ¶
‚àö
N Œµ
œÉ

‚Üí0
as N tends to inÔ¨Ånity, for any Ô¨Åxed Œµ > 0. Similarly, Pr(!Œ∏N ‚àíŒº ‚â§‚àíŒµ) ‚Üí0. Therefore, the
sequence !Œ∏ of sample mean is consistent for the population mean Œº.
Point and interval estimation
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
115 
In the above example, it is easy to prove consistency due to the normality assumption.
In general problem, it is not easy to Ô¨Ånd the exact distribution of !Œ∏N and hence establishing
consistency in a direct manner becomes intractable. That‚Äôs why most proofs of consistency
are based on the Chebychev inequality. Applying the latter to an estimator !Œ∏N yields
Pr(|!Œ∏N ‚àíŒ∏| ‚â•Œµ) ‚â§E|!Œ∏N ‚àíŒ∏|2
œµ2
.
As a consequence, it suÔ¨Éces to show that the mean squared error of !Œ∏N converges to zero (or,
equivalently, that !Œ∏N is asymptotically unbiased and its variance shrinks to zero). Another
useful result to prove consistency is the continuous mapping theorem, which dictates that,
if !Œ∏N is consistent for Œ∏, then g(!Œ∏N) is consistent for g(Œ∏) if g(¬∑) is a continuous real-valued
function.
In the next sections, we discuss two methods for deriving consistent estimators of a
population parameter. The Ô¨Årst is the method of moments, which impose assumptions only
on the moments of the distribution. This means that it does not require us to know the
joint distribution of the data, only the moments. In contrast, the second method requires
the speciÔ¨Åcation of the joint distribution and, as such, it takes advantage of the whole
probabilistic structure to derive eÔ¨Écient estimators for the population parameters.
6.1.4
Method of moments
The method of moments relies on the very simple idea of matching sample moments with
their population counterparts. Let (X1, . . . , XN) denote a random sample with marginal
probability density functions given by f(Xi; Œ∏) with Œ∏ = (Œ∏1, . . . , Œ∏k)‚Ä≤. Assume that the Ô¨Årst
k moments of Xi exist, that is to say, E(Xk
i ) ‚â°Œºk(Œ∏) < ‚àû. The moments naturally depend
on the parameter vector Œ∏ = (Œ∏1, . . . , Œ∏k)‚Ä≤, and so we can equate the Ô¨Årst k sample moments
to the corresponding k population moments to solve for (Œ∏1, . . . , Œ∏k):
1
N
N

i=1
Xi = Œº1(Œ∏), 1
N
N

i=1
X2
i = Œº2(Œ∏), . . . , 1
N
N

i=1
Xk
i = Œºk(Œ∏).
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
116 
The above forms a system of k equations with k incognita, and hence it suÔ¨Éces to solve for
(Œ∏1, . . . , Œ∏k) as a function of the sample moments.
Examples
(1) Consider a random sample (X1, . . . , XN) from a normal distribution with mean Œº and
variance œÉ2. Using the method of moments, we equate the Ô¨Årst two sample and population
moments giving way to !ŒºN =
1
N
N
i=1 Xi and
1
N
N
i=1 X2
i = !Œº2
N + !œÉ2
N. The latter leads to
!œÉ2
N = 1
N
N
i=1 X2
i ‚àí!Œº2
N = 1
N
N
i=1(Xi ‚àí!ŒºN)2.
(2) Suppose that (X1, . . . , XN) is a random sample drawn from an exponential distribution
with parameter Œª. The methods of moments would suggest employing the sample mean as
an estimator of Œª.
(3) Suppose that (X1, . . . , XN) is a random sample from a negative binomial distribution
with parameters k and p. The methods of moments lands us with
1
N
N
i=1 Xi = k/p and
1
N
N
i=1 X2
i = k(1 ‚àíp)/p2. Solving for k and p then yields
ÀÜkN =

1
N
N
i=1 Xi
2
1
N
N
i=1 Xi(1 + Xi)
and
ÀÜpN =
1
N
N
i=1 Xi
1
N
N
i=1 Xi(1 + Xi)
.
6.1.5
Maximum likelihood
The method of moments uses only a subset of the joint distribution moments and hence
it cannot be as eÔ¨Écient as an estimator that exploits the whole information given by the
joint distribution. The price to pay for the latter is that it is easier to misspecify the joint
distribution than a couple of moments. For instance, economic theory speaks only about
expectations (and hence moments), without much to say about distributions (unless as a
simplifying assumption to make the model tractable).
Consider a ransom sample (X1, . . . , XN) drawn from a probability density function given
by f(Xi; Œ∏), where Œ∏ = (Œ∏1, . . . , Œ∏k)‚Ä≤. We deÔ¨Åne the likelihood function as
L(Œ∏; X) =
N
'
i=1
f(Xi; Œ∏).
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
117 
In other words, the likelihood function is equivalent to the joint density of the data but for
the argument. While the joint density is a function of the random variables (X1, . . . , XN)
given a parameter vector Œ∏, the likelihood inverts the problem and considers a function of
the parameter vector Œ∏ given the sample we observe (X1, . . . , XN).
The maximum likelihood (ML) estimator !Œ∏N then searches for the parameter value that
maximize the probability of observing the sample (X1, . . . , XN), resulting in
!Œ∏N = argmax
Œ∏‚ààŒò
L(Œ∏; X) = argmax
Œ∏‚ààŒò
N
'
i=1
f(Xi; Œ∏).
In view that monotone transformation do not alter the maximization problem, we prefer to
take the logarithm of the likelihood function so as to end up with a sum of log-densities for
it is always easier to manipulate sums rather than products. This yields
!Œ∏N = argmax
Œ∏‚ààŒò
ln L(Œ∏; X) = argmax
Œ∏‚ààŒò
N

i=1
ln f(Xi; Œ∏).
So, to Ô¨Ånd the ML estimator, we must equate the score vector
‚àÇ
‚àÇŒ∏‚Ä≤ ln f(X; Œ∏) to zero.
The maximum likelihood method entails a number of interesting properties for the esti-
mator. First, it is invariant to parameter transformations in that, if !Œ∏ is the ML estimator of
Œ∏, then the ML estimator of g(Œ∏) is g(!Œ∏) for any function g of Œ∏. Second, the ML estimator
is very easy to deal with in large samples for it is asymptotically normal. Third, under cer-
tain weak regularity conditions, the ML estimator is asymptotically unbiased and eÔ¨Écient
in that it achieves the lower bound given by the Cram¬¥er-Rao inequality as the sample size
goes to inÔ¨Ånity.
Examples
(1) Consider a random sample (X1, . . . , XN) from a binomial model with probability p. The
likelihood is L(p; X) = N
i=1
 n
xi

pxi(1 ‚àíp)N‚àíxi and hence the score function is
‚àÇ
‚àÇp ln L(p; X) =
N
i=1 Xi
p
‚àíN ‚àíN
i=1 Xi
1 ‚àíp
.
Equating the score function to zero yields ÀÜpN = 1
N
N
i=1 Xi.
(2) Suppose that (X1, . . . , XN) is a random sample drawn from a Poisson distribution with
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
118 
arrival rate Œª. The Ô¨Årst-order condition for the maximization of the log-likelihood function
then is
‚àÇ
‚àÇŒª ln L(Œª; X)

Œª=ŒªN
= ‚àÇ
‚àÇŒª
N

i=1
ln
	e‚àíŒªŒªXi
Xi!


Œª=ŒªN
= ‚àÇ
‚àÇŒª
N

i=1
[‚àíŒª + Xi ln Œª ‚àíln(Xi!)]

Œª=ŒªN
= ‚àÇ
‚àÇŒª

‚àíNŒª + ln Œª
N

i=1
Xi ‚àí
N

i=1
ln(Xi!)

Œª=ŒªN
=
N
i=1 Xi
!ŒªN
‚àíN = 0,
giving way to !ŒªN = 1
N
N
i=1 Xi as the maximum likelihood estimator.
(3) Suppose that (X1, . . . , XN) is a random sample from a normal distribution with mean
Œº and variance œÉ2. The Ô¨Årst-order conditions are
‚àÇ
‚àÇŒº ln L(Œº, œÉ2; X) =
1
œÉ2
N
i=1(Xi ‚àíŒº) and
‚àÇ
‚àÇœÉ2 ln L(Œº, œÉ2; X) = ‚àíN
2œÉ2 +
1
2œÉ4
N
i=1(Xi ‚àíŒº)2 yielding the following maximum likelihood
estimators for the mean and variance: !ŒºN =
1
N
N
i=1 Xi and !œÉ2
N =
1
N
N
i=1(Xi ‚àí!ŒºN)2,
respectively.
Point and interval estimation
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
‚ÄúThe perfect start 
of a successful, 
international career.‚Äù
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Statistics for Business and Economics
 
119 
To show that the maximum likelihood estimator is consistent and eÔ¨Écient, we must
impose some regularity conditions. First, maximization of the likelihood function is over a
compact parameter space Œò ‚äÇRk and the true parameter vector Œ∏ is in the interior of the
parameter space. Second, the average log-likelihood function sN(Œ∏) ‚â°
1
N
N
i=1 ln L(Œ∏; Xi)
converges almost surely to its expected value for all possible parameter values, that is to say,
sN(Œ∏)
a.s.
‚àí‚ÜíEŒ∏0[sN(Œ∏)] ‚â°s‚àû(Œ∏, Œ∏0) for every Œ∏ ‚ààŒò, where the expectation is taken over the
joint distribution function evaluated at the true parameter value Œ∏0 ‚ààŒò. Third, sN(Œ∏) is
continuous in Œ∏ ‚ààŒò, and hence the sample holds for EŒ∏0[sN(Œ∏)]. Fourth, the latter has a
unique maximum in Œ∏ ‚ààŒò.
We are now ready to show that !Œ∏N converges almost surely to the true value Œ∏0 of
the population parameter vector. We start by noting that !Œ∏N for sure exists given that a
continuous function always has a maximum in a compact set. Second, for any Œ∏ Ã∏= Œ∏0, it
follows that EŒ∏0 [ln L(Œ∏; X) ‚àíln L(Œ∏0; X)] ‚â§ln EŒ∏0 [L(Œ∏; X)/L(Œ∏0; X)] due to the Jensen‚Äôs
inequality as the logarithmic function is concave. However,
EŒ∏0 [L(Œ∏; X)/L(Œ∏0; X)] =
 ‚àû
‚àí‚àû
L(Œ∏; x)
L(Œ∏0; x) L(Œ∏0; x) dx = 1
and hence EŒ∏0 [ln L(Œ∏; X) ‚àíln L(Œ∏0; X)] ‚â§0. We now Ô¨Årst divide both sides by N to yield
EŒ∏0 [sN(Œ∏) ‚àísN(Œ∏0)] ‚â§0 and then take limits to obtain s‚àû(Œ∏, Œ∏0) ‚â§s‚àû(Œ∏0, Œ∏0) almost surely
by the uniform convergence assumption. Further, the identiÔ¨Åcation assumption ensures that
the inequality is strict if Œ∏ Ã∏= Œ∏0, whereas s‚àû(!Œ∏N, Œ∏0) ‚â•s‚àû(Œ∏0, Œ∏0) by construction given that
!Œ∏N maximizes the average log-likelihood. Altogether, this means that !Œ∏N
a.s.
‚àí‚ÜíŒ∏0, proving
strong consistency.
The weak consistency of the maximum likelihood estimator is much easier in that it
suÔ¨Éces to show that the mean of the ML estimator converges to the true value of the
parameter, while its variance shrinks to zero. The regularity conditions we must impose to
ensure weak consistency (i.e., convergence in probability) are indeed much milder than what
we assume to achieve strong consistency (i.e., almost sure convergence). In what follows, we
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
120 
derive the asymptotic mean and variance of the ML estimator and then derive its asymptotic
normality under the assumption that sN(Œ∏) is twice continuously diÔ¨Äerentiable.
We Ô¨Årst take the derivative of sN(Œ∏) with respect to Œ∏ to Ô¨Ånd the score function, which
we then equate to zero to Ô¨Ånd maximum of the log-likelihood function, namely,
‚àÇ
‚àÇŒ∏‚Ä≤ sN(Œ∏) = 1
N
N

i=1
‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; Xi) = 0.
Although we denote the score function by
‚àÇ
‚àÇŒ∏‚Ä≤ sN(Œ∏), note that it depends on X and hence
it is also a random vector with the same dimension as the vector Œ∏ of parameters. The Ô¨Årst
step of the proof is to show that the score function is on average zero for any Œ∏ ‚ààŒò. This
is indeed the case as long as
‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏; Xi) is bounded, and so we can switch the order of
diÔ¨Äerentiation and integration
EŒ∏
 ‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; Xi)

=
 ‚àû
‚àí‚àû
‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi) L(Œ∏; xi) dxi =
 ‚àû
‚àí‚àû
‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏; xi)
L(Œ∏; xi)
L(Œ∏; xi) dxi
=
 ‚àû
‚àí‚àû
‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏; xi) dxi = ‚àÇ
‚àÇŒ∏‚Ä≤
 ‚àû
‚àí‚àû
L(Œ∏; xi) dxi = ‚àÇ
‚àÇŒ∏‚Ä≤ 1 = 0.
We next apply a Taylor expansion to the score function evaluated at the ML estimator,
which is equal to zero given the Ô¨Årst-order condition:
0 = ‚àÇ
‚àÇŒ∏‚Ä≤ L(!Œ∏N; X) = ‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏0; X) +
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ L(Œ∏‚àó; X) (!Œ∏N ‚àíŒ∏0),
where Œ∏‚àó‚àà[Œ∏0, !Œ∏N]. It is straightforward to show that HŒ∏‚àó‚â°
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ L(Œ∏‚àó; X) is invertible
and thus
‚àö
N (!Œ∏N ‚àíŒ∏0) = ‚àíH‚àí1
Œ∏‚àó
‚àö
N
‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏0; X). Note that HŒ∏‚àó= 1
N
N
i=1
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ L(Œ∏‚àó; Xi)
and hence it does satisfy a strong law of large numbers provided that the variance of
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ L(Œ∏‚àó; Xi) is Ô¨Ånite. In addition, we know that the ML estimator is strongly consistent in
that !Œ∏ converges almost surely to Œ∏0, implying that Œ∏‚àóconverges as well to Œ∏0. Altogether,
this means that the random matrix HŒ∏‚àóconverges to EŒ∏0
"
1
N
N
i=1
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ L(Œ∏0; Xi)
#
< ‚àû
almost surely.
It now remains to study the asymptotic behavior of
‚àö
N
‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏0; X). In view that we
have already seen that the latter has mean zero for any Œ∏ ‚ààŒò, it is reasonable to assume there
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
121 
is a central limit theorem that applies. Letting I‚àû(Œ∏0) ‚â°limN‚Üí‚àûvar
‚àö
N
‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏0; X)

then yields I‚àí1
‚àû(Œ∏0)
‚àö
N
‚àÇ
‚àÇŒ∏‚Ä≤ L(Œ∏0; X)
d
‚àí‚ÜíN(0, Ik), where Ik is a k-dimensional identity ma-
trix and I‚àû(Œ∏0) is known as the information matrix.1 Combining all of the above ingredients
gives way to
‚àö
N(!Œ∏N ‚àíŒ∏0)
d
‚àí‚ÜíN

0, H‚àí1
‚àû(Œ∏0)I‚àû(Œ∏0)H‚àí1
‚àû(Œ∏0)

.
To demonstrate that the variance of the ML estimator achieves the Cram¬¥er-Rao lower bound
and hence it is eÔ¨Écient, it suÔ¨Éces to show that I‚àû(Œ∏0) = ‚àíH‚àí1
‚àû(Œ∏0). To do so, we Ô¨Årst
diÔ¨Äerentiate both sides of
 ‚àû
‚àí‚àû
‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi) L(Œ∏; xi) dxi = 0 with respect to Œ∏, yielding
 ‚àû
‚àí‚àû
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi) L(Œ∏; xi) dxi +
 ‚àû
‚àí‚àû
‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi) ‚àÇ
‚àÇŒ∏ L(Œ∏; xi) dxi = 0
‚áíEŒ∏

‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi)

+ EŒ∏
$ ‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi)
  ‚àÇ
‚àÇŒ∏ ln L(Œ∏; xi)
%
= 0
‚áíEŒ∏

1
N
N

i=1
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi)

= ‚àí1
N
N

i=1
EŒ∏
$ ‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; xi)
  ‚àÇ
‚àÇŒ∏ ln L(Œ∏; xi)
%
‚áíEŒ∏ [H(Œ∏)] = ‚àíEŒ∏
$
N
 ‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; X)
  ‚àÇ
‚àÇŒ∏ ln L(Œ∏; X)
%
given that the scores
‚àÇ
‚àÇŒ∏ ln L(Œ∏; xi) and
‚àÇ
‚àÇŒ∏ ln L(Œ∏; xj) are uncorrelated for 1 ‚â§i Ã∏= j ‚â§N.
Finally, taking limits yield the result.
6.2
Interval estimation
It is obviously useful to know the expected value of a random variable, but ideally we must
also have some idea of variability. For instance, it is always good news to hear that one of
our investments is giving on average 10% of return per month. However, it is even more
comforting to know that it has an expected return of 10% ¬± 1%. The idea is to derive a
1 Although we have spoken about joint distributions, we have not explicitly introduced any multivariate
distribution. A multivariate normal distribution depends on a vector of means Œº and a covariance matrix Œ£.
The elements of Œº correspond to the individuals means, whereas the elements in the main diagonal of Œ£ refer
to the individual variances. The oÔ¨Ä-diagonal elements of Œ£ denote the covariance between the components
of the random vector. So, if a random vector X = (X1, . . . , Xk) is multivariate normal with a covariance
matrix given by the k-dimensional identity matrix Ik, then Xi and Xj are independent for any i Ã∏= j. In
general, if X ‚àºN(Œº, Œ£), then a vector of linear combinations of the elements of X is also multivariate
normal, namely, AX ‚àºN(AŒº, AŒ£A‚Ä≤).
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
122 
measure of precision from the sampling distribution of the sample mean (or any other point
estimator). In this way, we may provide a range of values within which we expect the mean
of the distribution to belong rather than give only a point estimate.
Examples
(1) Let X ‚àºN(Œº, œÉ2) with œÉ2 known. We estimate Œº by means of the sample mean ¬ØXN,
whose sampling distribution is ¬ØXN ‚àºN(Œº, œÉ2/N). To establish a conÔ¨Ådence interval around
the sample mean in which the true value of Œº belongs with 95% of probability, we Ô¨Årst note
that
‚àö
N( ¬ØXN ‚àíŒº)/œÉ is standard normal and hence Pr
‚àö
N
 ¬ØXN ‚àíŒº
 /œÉ ‚â§1.96

= 95%. It
then follows that
Pr
	‚àö
N

¬ØXN ‚àíŒº
œÉ
 ‚â§1.96

= Pr
	
¬ØXN ‚àí1.96
œÉ
‚àö
N
‚â§Œº ‚â§¬ØXN + 1, 96 œÉ
‚àö
N

= 0, 95
This result holds exactly only because of normality and because we know the value of the
variance. In large samples, the above conÔ¨Ådence interval nevertheless provides a good ap-
proximation because the central limit theorem and the weak law of large numbers ensure that
the sampling distribution converges to a normal distribution, whereas the sample variance
converges in probability to the true variance as N ‚Üí‚àû, respectively.
(2) Let X = (X1, . . . , Xn) denote a random sample of Bernoulli essays that take value one
with probability p, zero otherwise. To estimate the probability p, a natural estimator is the
relative frequency !pN = 1
N
N
i=1 Xi, which is unbiased given that
E(!pN) = 1
N
N

i=1
E(Xi) = 1
N (Np) = p.
This means that the mean squared error of the relative frequency corresponds to its variance,
which is given by
E (!pN ‚àíp)2 = E

1
N
N

i=1
(Xi ‚àíp)
2
= 1
N E(X ‚àíp)2 = 1
N var(X) = 1
N p (1 ‚àíp).
Note that the variance shrinks to zero as N ‚Üí‚àû, conÔ¨Årming that the relative frequency is
a consistent estimator in that it converges in probability to the probability p. The central
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
123 
limit theorem says that the binomial converges to a normal distribution as the sample size
grows. Given that N
i=1 Xi has by deÔ¨Ånition a binomial distribution with expected mean
Np and variance Np(1 ‚àíp), it follows that !pN weakly converges to a normal distribution
with mean p and variance p (1‚àíp)/N. Accordingly, the probability p belongs to the interval
"
!pN ‚àí1.96

!pN(1 ‚àí!pN)/N, !pN + 1.96

!pN(1 ‚àí!pN)/N
#
with 95% of conÔ¨Ådence. As before, we are using not only the central limit theorem to justify
asymptotic normality, but also the law of large numbers to ensure that we estimate the
variance consistently by plugging in !pN in lieu of p.
Point and interval estimation
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
‚ñ∂‚ñ∂enroll by September 30th, 2014 and 
‚ñ∂‚ñ∂save up to 16% on the tuition!
‚ñ∂‚ñ∂pay in 10 installments / 2 years
‚ñ∂‚ñ∂Interactive Online education
‚ñ∂‚ñ∂visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Statistics for Business and Economics
 
124 
It is interesting to review what happens under normality just to Ô¨Åx some ideas. So,
let Xi ‚àºiid N(Œº, œÉ2) for i = 1, . . . , N. We know that the sample mean ¬ØXN has expected
value Œº and variance œÉ2/N. In addition, as it is a linear combination of normal variates,
¬ØXN ‚àºN(Œº, œÉ2/N) or, equivalently,
‚àö
N ( ¬ØX ‚àíŒº)/œÉ ‚àºN(0, 1). The snag is that, in general,
the standard deviation œÉ is unknown and hence we can at best replace it with a consistent
estimator.
For instance, the maximum likelihood estimator of the variance of a normal
distribution is
!œÉ2
N = 1
N
N

i=1
(Xi ‚àí¬ØXN)2 = 1
N
N

i=1

X2
i ‚àí2Xi ¬ØXN + ¬ØX2
N

= 1
N
N

i=1
X2
i ‚àí2 ¬ØXN
1
N
N

i=1
Xi + ¬ØX2
N = 1
N
N

i=1
X2
i ‚àí2 ¬ØX2
N + ¬ØX2
N
= 1
N
N

i=1
X2
i ‚àí¬ØX2
N,
with an expected value of
E(!œÉ2
N) = E

1
N
N

i=1
X2
i ‚àí¬ØX2
N

= E(X2) ‚àíE( ¬ØX2
N)
= Œº2 + œÉ2 ‚àíŒº2 ‚àíœÉ2/N = œÉ2(1 ‚àí1/N) = N ‚àí1
N
œÉ2 N‚Üë‚àû
‚àí‚ÜíœÉ2.
Although it is asymptotically unbiased, the ML estimator is biased in small samples. As we
know, to Ô¨Ånd an unbiased estimator for the variance of a normal distribution, it suÔ¨Éces to
multiply the ML estimator by a factor of N/(N ‚àí1), giving way to
ÀúœÉ2
N =
N
N ‚àí1 !œÉ2
N =
1
N ‚àí1
N

i=1
(Xi ‚àí¬ØXN)2.
Intuitively, we subtract one from the denominator because we have already lost one degree
of freedom due to the estimation of the mean.
The nice thing about assuming normality is that it allows us to say something about the
distribution of ÀúœÉ2
N and hence about the exact distribution of the sample mean. To appreciate
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
125 
that, we Ô¨Årst note that
ÀúœÉ2
N =
1
N ‚àí1
N

i=1
(Xi ‚àí¬ØXN)2 =
1
N ‚àí1
N

i=1

(Xi ‚àíŒº) ‚àí( ¬ØXN ‚àíŒº)
2
=
1
N ‚àí1
N

i=1

(Xi ‚àíŒº)2 ‚àí2(Xi ‚àíŒº)( ¬ØXN ‚àíŒº) + ( ¬ØXN ‚àíŒº)2
=
1
N ‚àí1
 N

i=1
(Xi ‚àíŒº)2 ‚àí2
N

i=1
(Xi ‚àíŒº)( ¬ØXN ‚àíŒº) +
N

i=1
( ¬ØXN ‚àíŒº)2

=
1
N ‚àí1
 N

i=1
(Xi ‚àíŒº)2 ‚àí2N( ¬ØXN ‚àíŒº)2 + N( ¬ØXN ‚àíŒº)2

=
1
N ‚àí1
 N

i=1
(Xi ‚àíŒº)2 ‚àíN( ¬ØXN ‚àíŒº)2

=
1
N ‚àí1
N

i=1
(Xi ‚àíŒº)2 ‚àí
N
N ‚àí1( ¬ØXN ‚àíŒº)2.
Dividing both sides by the variance then yields
(N ‚àí1) ÀúœÉ2
N
œÉ2 =
N

i=1
	Xi ‚àíŒº
œÉ

2
‚àíN
	 ¬ØXN ‚àíŒº
œÉ

2
=
N

i=1
	Xi ‚àíŒº
œÉ

2
‚àí
	 ¬ØXN ‚àíŒº
œÉ/
‚àö
N

2
.
Both terms within brackets refer to squared standard normal random variables and hence
have chi-square distributions. Moreover, it is possible to show that (N ‚àí1) ÀúœÉ2
N/œÉ2 is a chi-
square random variable with N ‚àí1 degrees of freedom. Bearing that in mind, we now turn
attention to
‚àö
N
¬ØXN ‚àíŒº
ÀúœÉN
=
‚àö
N( ¬ØXN ‚àíŒº)/œÉ
ÀúœÉN/œÉ
,
whose distribution is t-student with N ‚àí1 degrees of freedom given that the numerator is
standard normal and the denominator is a chi-square divided by its degrees of freedom.2
Altogether, this means that, under normality, we can construct an exact conÔ¨Ådence interval
for the mean value of the distribution. In particular, it follows that
Pr
	
‚àö
N
¬ØXN ‚àíŒº
ÀúœÉN
 ‚â§tN‚àí1(1 ‚àíŒ±/2)

= 1 ‚àíŒ±,
2 It is also possible to show that, under normality, the random variables in the numerator and denominator
are independent.
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
126 
where tN‚àí1(1‚àíŒ±/2) is the (1‚àíŒ±/2) percentile of a t-student with N ‚àí1 degrees of freedom.
For instance, to construct a 95% conÔ¨Ådence interval, we set Œ± to 5% and hence we must look
at the 97.5% percentile of the t-student. As the latter distribution is symmetric, the 97.5%
percentile is equal to the absolute value of the 2.5% percentile, so that Œº will belong to the
interval
"
¬ØXN + tN‚àí1(Œ±/2)ÀúœÉN/
‚àö
N, ¬ØXN + tN‚àí1(1 ‚àíŒ±/2)ÀúœÉN/
‚àö
N
#
with a probability of 95%.
If normality does not hold, then we must in general employ the central limit theorem and
the law of large numbers to justify the asymptotic approximation of the conÔ¨Ådence interval
based on the normal distribution. Let Xi ‚àºiid FX(Œº, œÉ2) for i = 1, . . . , N. Standardizing
the sample mean then yields
‚àö
N
¬ØXN ‚àíŒº
ÀúœÉN
=
‚àö
N
¬ØXN ‚àíŒº
œÉ
œÉ
ÀúœÉN
d
‚àí‚ÜíN(0, 1)
given that the central limit theorem dictates that the Ô¨Årst term of the right-hand side of the
equality is asymptotically standard normal and consistency ensures that the second term
converges in probability to one as N ‚Üí‚àû. Actually, the same result follows for any other
consistent variance estimator. This means that Œº ‚àà[ ¬ØXN +zŒ±/2 ÀúœÉN/
‚àö
N, ¬ØXN +z1‚àíŒ±/2 ÀúœÉN/
‚àö
N]
with probability 1 ‚àíŒ±, where zŒ±/2 and z1‚àíŒ±/2 are the Œ±/2 and (1 ‚àíŒ±/2) percentiles of the
standard normal distribution. As before, due to the symmetry of the normal distribution, it
turns out that zŒ¥ = ‚àíz1‚àíŒ¥ for any 0 ‚â§Œ¥ ‚â§1.
Point and interval estimation
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
127 
Chapter 7
Hypothesis testing
We know from previous chapters that, whenever we try to infer a quantity, the resulting
estimate varies with the sample. The random nature of any sample statistic is such that
we should always think twice before interpreting a result. For instance, we might wonder
whether a sample mean of 11 conÔ¨Årms or not a hypothesized value of 10 for the population
mean. The answer of course lies on the typical variation of the data. If the standard deviation
is very small, say 0.01, then a sample mean of 11 is pretty far away from 10. We probably
would conclude diÔ¨Äerently if the standard deviation were large, say 5. Building conÔ¨Ådence
intervals is just a Ô¨Årst step to answer this type of questions. SigniÔ¨Åcance (or hypothesis)
testing provides a much more general tool for this sort of task. In particular, it allows us to
check on how strong the statistical evidence is in favor of or against a hypothesis about the
data (e.g., whether the true mean is 10 given that the sample mean is 11).
SigniÔ¨Åcance testing starts with a partition of the probability space into two regions: the
null hypothesis H0 and the alternative hypothesis H1. The former consists of all events for
which the relation of interest holds, whereas the alternative hypothesis is simply the negation
of the null. The idea is to observe the data seeking for evidence against the null hypothesis.
Note that a statistical test can at best contradict the null hypothesis. However, failing to
reject the null hypothesis does not necessarily mean that we should accept it, just that we do
not have enough material to reject it. The testing strategy then is to develop a statistic that
should reÔ¨Çect the relation of interest as hypothesized by the null. This means that we will
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
128 
have to derive the sampling distribution of some test statistic conditioning on the fact that
the null holds. To make this task easier, we normally deÔ¨Åne the null as the simplest case. For
instance, it is much easier to compute the distribution for the sample mean by setting the
population mean to 10 rather than considering any value diÔ¨Äerent from 10. In general, the
alternative hypothesis typically reÔ¨Çects a change/impact in the process/population, while
the null hypothesis indicates the absence of a change or impact.
Examples
(1) Suppose we wish to test whether the true population mean Œº exceeds 15. We can then
deÔ¨Åne the null hypothesis as H0 : Œº > 15 and the alternative hypothesis as H1 : Œº ‚â§15.
This is a directional test given that we are attempting to evince deviations with respect to a
particular direction. We could thus consider a statistical test that rejects the sample mean if
it belongs to a given interval. Intuitively, to determine the latter, we should appreciate that
it does not suÔ¨Éce to observe a sample mean below 15 to reject the null. The reason is simple.
As a consistent estimator, the sample mean should provide us a value in the neighborhood
of the true mean. If the latter is 15.1, for instance, then it is likely that we will observe a
sample mean below 15 even though the null hypothesis hold.
(2)
Suppose now the interest lies on testing whether Œº = 15.
We then deÔ¨Åne the null
hypothesis as H0 : Œº = 15 given that it is easier to derive the distribution of the sample mean
if we know the true value of the population mean. In contrast to the previous example, testing
H0 involves no direction in that we should observe both positive and negative deviations
with respect to 15 (instead of only negative) in our attempt to reject the null hypothesis. As
before, because of sampling variation, we should consider a test in which we reject the null
if the sample mean is either too large or too small relative to 15. Needless to say, we should
deÔ¨Åne ‚Äòtoo large‚Äô and ‚Äòtoo small‚Äô according to the distribution of the sample mean just as we
have done for conÔ¨Ådence intervals in the previous chapter.
To understand whether the value we observe for a sample statistic is plausible or not,
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
129 
given the amount of randomness we specify in the null hypothesis, we must hinge our analysis
on the distribution of that sample statistic under the null. For instance, most people would
not reject the null hypotheses in the above example if the sample mean were 14.9999, though
most would reject the null in the second example if observing a sample mean of 500. As
we mention in the second example, to Ô¨Ånd the appropriate rejection region, we must follow
a procedure very similar to that we have used in the previous chapter to obtain conÔ¨Ådence
intervals. Indeed, the Ô¨Årst step in the derivation of a statistical testing procedure is to obtain
a rejection region by Ô¨Åxing a signiÔ¨Åcance level for the test. Just as a conÔ¨Ådence interval of
95% will on average miss the true value of the parameter 5% of the times, a test with 95%
signiÔ¨Åcance level will incorrectly reject the null hypothesis at most 5% of the times.
Hypothesis testing
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
130 
The main diÔ¨Äerence between conÔ¨Ådence intervals and tests is that there is only one type
of error in the former, i.e., missing the true value of the parameter. In contrast, there are
two sources of errors within hypothesis testing. We can either commit an error of type I or
an error of type II. The Ô¨Årst arises if we reject the null hypothesis even though it is true,
whereas the second refers to the event of failing to reject a false null. To sum up, if we denote
by R the event of rejecting the null hypothesis H0, then Pr(type I error) = Pr(R | H0 is true)
and Pr(type II error) = Pr( ¬ØR | H0 is false) with ¬ØR denoting the complement event of R and
hence a non-rejection of the null.
Note that we treat these errors in a very asymmetric fashion in that we Ô¨Åx only the
maximum tolerable probability of a type I error, e.g., Pr(type I error) ‚â§Œ±% if the signiÔ¨Å-
cance level is of (1 ‚àíŒ±)%. It turns out that it is impossible to control both errors at the
same type and hence the best we can do is to Ô¨Ånd a statistical procedure that minimizes the
chances of an error of type II given a Ô¨Åxed probability of an error of type I. Alternatively, we
could think of doing the contrary, that is to say, Ô¨Åxing the probability of a type II error and
then obtain the testing procedure that minimizes the likelihood of a type I error. Although
there is no logical reason to do so (i.e., Ô¨Åxing the error of type I), there is a moral reason
(who said moral does not play a role in science?) as advocated by two of the most eminent
statisticians of all times, namely, Jerzy Neyman (1894-1981) and Egon Pearson (1895-1980).
The motivation for their idea of Ô¨Åxing the type I error rests on a jury trial. Most people
would agree with Neyman and Pearson that it is preferable to free a guilty criminal than to
put an innocent in jail. We thus Ô¨Åx the probability of committing a type I error for it is
more damaging than the type II error.
Example
We may see a pregnancy test as a statistical procedure that decides whether
there is enough evidence supporting pregnancy. As statistical procedure can at best reject
the null hypothesis, this means that the null of a pregnancy test is actually the absence of
pregnancy. Most people would agree that a false negative is potentially more damaging than
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
131 
a false positive. This is in line with the Neyman-Pearson solution in that a false positive
corresponds to a type I error (rejecting a true null), whereas a false negative refers to a type
II error (failing to reject a false null).
In what follows, we Ô¨Årst show how to derive the rejection region for sample means. We
then introduce the concepts of size, level and power of a test, which derive from the type
I and type II errors. Next, we introduce the notion of p-value, which somehow gauges the
strength of the evidence against the null hypothesis. Computing p-values is an alternative
to setting ex-ante the signiÔ¨Åcance level of the test and hence a rejection region. Finally,
in the lasts section, we discuss hypothesis testing in a more general fashion by assuming a
likelihood approach.
7.1
Rejection region for sample means
We motivate this section with a simple example. A pharmaceutical lab is running clinic
trials to assess whether a new medicine to control the levels of cholesterol indeed works
better than the current medicine in the market. The clinical trials consider two groups of
100 patients. Group A takes the new medicine, whereas group B are subject to the standard
treatment.
To evaluate the relative performance of the new medicine, the lab measures
the diÔ¨Äerence in the cholesterol decrease between groups A and B (in percentage points).
The null hypothesis of interest is that, on average, there is no diÔ¨Äerence between the two
treatments: H0 : ŒºA‚àíB = 0.
The most natural estimator for the mean diÔ¨Äerence between the results of groups A and B
is the diÔ¨Äerence between their sample means (or, equivalently, the sample mean diÔ¨Äerence).
We must now ask ourselves whether a sample diÔ¨Äerence of, say, 1.06 is plausible given a
standard deviation of, say, 2 under the null of zero mean diÔ¨Äerence. The idea is very similar
to what we do if we wish to construct a conÔ¨Ådence interval.
The central limit theorem
ensures that we can approximate the sampling distribution of the sample mean diÔ¨Äerence in
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
132 
large samples by a standard Gaussian distribution if we subtract its mean and divide by its
standard deviation. The null hypothesis says that the mean diÔ¨Äerence is zero, while saying
nothing about the standard deviation. The standard error of the sample mean diÔ¨Äerence is
the standard deviation divided by the square root of the sample size, so that it amounts to
1/5. Letting zq denote the qth percentile of a standard normal distribution then yields
Pr
	
!ŒºA‚àíB ‚àí0
1/5
 > z1‚àíŒ±/2
 H0

= Pr

|!ŒºA‚àíB| > z1‚àíŒ±/2
5
 H0

‚àº= Œ±.
This suggests rejecting at the 5% signiÔ¨Åcance level if |!ŒºA‚àíB| > 1
5 z1‚àíŒ±/2 = 1.96
5
= 0.392. The
sample mean diÔ¨Äerence of 1.06 is obviously superior to 0.392 and hence we conclude that
the two treatments entail diÔ¨Äerent performances.
Hypothesis testing
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Statistics for Business and Economics
 
133 
The above test is two-sided in that it looks whether both positive and negative deviations
are large. The fact that the sample mean diÔ¨Äerence is positive also indicates that the new
medicine performs relatively better. To conÔ¨Årm that, we must test a directional null hypothe-
sis by means of one-sided tests. So, if we change the null to H0 : ŒºA‚àíB > 0, only large negative
deviations will contradict the null hypothesis. It then follow from Pr

!ŒºA‚àíB < zŒ±
5
 H0
 ‚àº= Œ±
that the rejection region is (‚àí‚àû, zŒ±
5 ]. Note that the Œ±th percentile of the standard normal
distribution is negative (zŒ± < 0) for any Œ± < 1/2. In contrast, if we deÔ¨Åne the null hypoth-
esis as H0 : ŒºA‚àíB < 0, then we would have to worry only about large positive sample mean
diÔ¨Äerences, yielding [ z1‚àíŒ±
5 , ‚àû) as a rejection region given that Pr

!ŒºA‚àíB > z1‚àíŒ±
5
 H0
 ‚àº= Œ±.
Figure 7.1 illustrates the main standard normal percentiles for testing purposes.
Let us now consider the general two-sided case in which we wish to test the null that the
population mean of a random sample X = (X1, . . . , XN) is equal to Œº0. In what follows, we
consider three diÔ¨Äerent setups. The Ô¨Årst and second settings respectively assume normality
with known and unknown variances, whereas the third imposes no speciÔ¨Åc distribution for
the data. Regardless of the data distribution, the sample mean is on average equal to Œº0
under the null hypothesis, with variance œÉ2/N.
Normality, known variance:
If Xi ‚àºiid N(Œº, œÉ2) with a known variance œÉ2, it follows
that
¬Ø
XN‚àíŒº0
œÉ/
‚àö
N is standard normal and hence the rejection region at the Œ± signiÔ¨Åcance level is
(‚àí‚àû, Œº0 +
œÉ
‚àö
N zŒ±/2] ‚à™[Œº0 +
œÉ
‚àö
N z1‚àíŒ±/2, ‚àû). Note that the above intervals are symmetric in
that z1‚àíŒ±/2 = ‚àízŒ±/2 for any 0 < Œ± < 1.
Normality, unknown variance:
If Xi ‚àºiid N(Œº, œÉ2) with a unknown variance œÉ2,
it follows that
¬Ø
XN‚àíŒº0
œÉ/
‚àö
N is t-student with N ‚àí1 degrees of freedom. The rejection region at
the Œ± signiÔ¨Åcance level is (‚àí‚àû, Œº0 +
œÉ
‚àö
N t(Œ±/2)
N‚àí1 ] ‚à™[Œº0 +
œÉ
‚àö
N t(Œ±/2)
N‚àí1 , ‚àû), where t(q)
d
denote the
qth percentile of the t-student with d degrees of freedom. Note that the above intervals
are symmetric given that t(1‚àíŒ±/2)
d
= ‚àít(Œ±/2)
d
for any 0 < Œ± < 1 regardless of the number of
degrees of freedom.
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
134 
-2.57
-1.96 -1.645
-1
0
1
1.645
1.96
2.57
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
probability density function
Figure 7.1: The area in yellow corresponds to 1% of the probability mass of a standard
normal distribution (i.e., 0.5% in the lower tail plus 0.5% in the upper tail), whereas the
area in yellow and blue responds for 5% of the probability mass (i.e., 2.5% in the lower tail
plus 2.5% in the upper tail). Finally, the area in pink entails another 2.5% of the probability
mass in each of the tails, so that the area in yellow, blue and pink amounts to 10% of the
probability mass of a standard normal distribution.
Unknown distribution:
If Xi ‚àºiid fX(Œº, œÉ2) with a unknown variance œÉ2, it follows
from the central limit theorem that
¬Ø
XN‚àíŒº0
œÉ/
‚àö
N converges in distribution to a standard normal
as the sample size grows. The asymptotic rejection region at the Œ± signiÔ¨Åcance level then is
(‚àí‚àû, Œº0 +
œÉ
‚àö
N zŒ±/2] ‚à™[Œº0 +
œÉ
‚àö
N z1‚àíŒ±/2, ‚àû).
Adapting the above rejection intervals to one-sided tests is pretty straightforward. It
suÔ¨Éces to take the interval of interest and replace Œ±/2 by Œ± in the percentile of the sampling
distribution. For instance, under normality and unknown variance, the rejection region for
the one-sided test for H0 : Œº > Œº0 against H1 : Œº ‚â§Œº0 is [Œº0 +
œÉ
‚àö
N t(1‚àíŒ±)
N‚àí1 , ‚àû), whereas we
would reject H0 : Œº < Œº0 if the test statistic falls within (‚àí‚àû, Œº0 +
œÉ
‚àö
N t(Œ±)
N‚àí1]. The critical
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
135 
value that deÔ¨Ånes the rejection region for the one-sided test depends on Œº0 just as the two-
sided test, despite the fact we deÔ¨Åne the null hypothesis through an inequality rather than
equality. By conditioning the test statistic on Œº0, we are essentially taking the least favorable
situation within the alternative hypothesis, that is to say, the value of Œº ‚ààH1 that is closest
to the null hypothesis. It turns out that considering the least favorable situation within the
alternative hypothesis alleviates the probability of committing a type II error.
Example:
A barista prepares a sequence of 16 espressos, taking note of how much time it
takes to pour the best possible expresso. As the sample mean time amounts to 26 seconds,
the barista concludes that the expresso machine is too fast and decides to Ô¨Åne-tune it in order
to increase the preparation time by 2 seconds. The quality-control manager disagrees with
the barista for the following reasons. First, the sample size is too small. Second, the sample
is not entirely random given that the barista prepares the espressos in a straight sequence.
Third, the barista is not accounting for the randomness of the data. The sample standard
deviation is indeed quite palpable, at 6 seconds. Fourth, it could be more interesting to
Ô¨Åne-tune the expresso machine so as to reduce the variability of the preparation time rather
than to increase the mean time. To substantiate her argument, the quality-control manager
tests whether the mean time is equal to 28 within a random sample context. The diÔ¨Äerence
between the hypothesized and sample means is equal to 2 seconds. The following large-
sample approximation then holds
Pr
	‚àö
16

¬ØX16 ‚àí28
6
 > z1‚àíŒ±/2
 H0 : Œº = 28

‚àº= 1 ‚àíŒ¶(z1‚àíŒ±/2) + Œ¶(‚àíz1‚àíŒ±/2)
= 1 ‚àíŒ¶(z1‚àíŒ±/2) + Œ¶(zŒ±/2)
= 1 ‚àí(1 ‚àíŒ±/2) + Œ±/2 = Œ±,
where Œ¶(¬∑) denotes the cumulative distribution function of a standard normal. Setting Œ± to
5% then yields a test that rejects the null H0 : Œº = 28 if the sample mean does not belong
to the interval

28 ¬± 3√ó1.96
2

. This is not the case here as the sample mean is 26 seconds and
so there is no statistical reason to believe that the expresso machine requires adjustment.
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
136 
7.2
Size, level, and power of a test
In this section, we extend the discussion to a more general setting in which we are interested
in a parameter Œ∏ of the distribution (not necessarily the mean). As before, the derivation of
a testing procedure involves two major steps. The Ô¨Årst is to obtain a test statistic that is
able to distinguish the null from the alternative hypothesis. For instance, if we are interested
in the arrival rate of a Poisson distribution, it is then natural to focus either on the sample
mean or on the sample variance.1 The second is to derive the rejection region for the test
statistic. The rejection region depends of course on the level of signiÔ¨Åcance Œ±, which denotes
the upper limit for the probability of committing a type I error. A similar concept is given by
the (exact/asymptotic) size of a test, which corresponds to the (exact/limiting) probability
of observing a type I error.
In general, we are only able to compute the size of a test if both null and alternative
hypotheses are simple, that is to say, they involve only one value for the parameter vector
H0 : Œ∏ = Œ∏0 against H1 : Œ∏ = Œ∏1.
Unfortunately, most situations refer to at least one
composite hypothesis, e.g., H0 : Œ∏ = Œ∏0 against H1 : Œ∏ < Œ∏0 or H0 : Œ∏ = Œ∏0 against H1 : Œ∏ > Œ∏0
or H0 : Œ∏ = Œ∏0 against H1 : Œ∏ Ã∏= Œ∏1 or H0 : Œ∏ ‚â•Œ∏0 against H1 : Œ∏ < Œ∏0 or H0 : Œ∏ ‚â§Œ∏0 against
H1 : Œ∏ > Œ∏0. Note that it does not make much sense to think about a situation in which the
null hypothesis is composite and the alternative is simple. It is always easier to derive the
distribution of the test statistic for a given value of the parameter (rather than an interval),
and so it would payoÔ¨Äto invert the hypotheses.
Well, both level and size relate to the type I error. To make it fair, we will now deÔ¨Åne a
concept that derives from the probability of committing a type II error. The power of a test
is the probability of correctly rejecting the null hypothesis, namely,
Pr(R | H0 is false) = 1 ‚àíPr( ¬ØR | H0 is false) = 1 ‚àíPr(type II error)
So, we should attempt to obtain the most powerful test as possible if we wish to minimize
1 Recall that if X is a Poisson with arrival rate Œª, then E(X) = var(X) = Œª.
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
137 
the likelihood of having a type II error. In general, the power of a test is a function of the
value of the parameter vector under the alternative. The power function degenerates to a
constant only in the event of a simple alternative hypothesis, viz. H1 : Œ∏ = Œ∏1. To work out
the logic of the derivation of the power, let‚Äôs revisit the barista example from the previous
section.
Example:
Suppose that it actually takes on average 24 seconds for pouring a perfect
expresso. In the previous section, we have computed a large-sample approximation under
the null for the distribution of the sample mean. We now derive the asymptotic power of
the means test at the Œ± level of signiÔ¨Åcance conditioning on Œº = 24.
Hypothesis testing
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master‚Äôs Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master‚Äôs programmes
‚Ä¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
‚Ä¢ 1st place: MSc International Business
‚Ä¢ 1st place: MSc Financial Economics
‚Ä¢ 2nd place: MSc Management of Learning
‚Ä¢ 2nd place: MSc Economics
‚Ä¢ 2nd place: MSc Econometrics and Operations Research
‚Ä¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‚ÄòBeste Studies‚Äô ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Statistics for Business and Economics
 
138 
The probability of falling into the rejection region is
Pr
	‚àö
16

¬ØX16 ‚àí28
6
 > 1.96
 Œº = 24

= 1 ‚àíPr
	
2

¬ØX16 ‚àí28
3
 ‚â§1.96
 Œº = 24

= 1 ‚àíPr

28 ‚àí2.94 ‚â§¬ØX16 ‚â§28 + 2.94
 Œº = 24

= 1 ‚àíPr

25.06 ‚â§¬ØX16 ‚â§30.94
 Œº = 24

= 1 ‚àíPr
	‚àö
16 25.06 ‚àí24
6
‚â§
‚àö
16
¬ØX16 ‚àí24
6
‚â§
‚àö
16 30.94 ‚àí24
6
 Œº = 24

‚àº= 1 ‚àí

Œ¶
	6.94
3/2

‚àíŒ¶
	1.06
3/2


= 1 ‚àí0.999998142 + 0.760113176 = 0.760115034.
Note that this power Ô¨Ågure holds only asymptotically for we are taking the normal approxi-
mation for the unknown distribution of the sample mean.
In general, to compute the (asymptotic) power function of a two-sided means test, it
suÔ¨Éces to appreciate that the probability of rejecting the null for Œº = Œº1 Ã∏= Œº0 is
Pr
	‚àö
N

¬ØXN ‚àíŒº0
!œÉN
 > z1‚àíŒ±/2
 Œº = Œº1

= 1 ‚àíPr
	‚àö
N

¬ØXN ‚àíŒº0
!œÉN
 ‚â§z1‚àíŒ±/2
 Œº = Œº1

= 1 ‚àíPr
	
‚àíz1‚àíŒ±/2 ‚â§
‚àö
N
¬ØXN ‚àíŒº0
!œÉN
‚â§z1‚àíŒ±/2
 Œº = Œº1

= 1 ‚àíPr
	
Œº0 ‚àíz1‚àíŒ±/2
!œÉN
‚àö
N
‚â§¬ØXN ‚â§Œº0 + z1‚àíŒ±/2
!œÉN
‚àö
N
 Œº = Œº1

= 1 ‚àíPr
	
Œº0 ‚àíŒº1 ‚àíz1‚àíŒ±/2
!œÉN
‚àö
N
‚â§¬ØXN ‚àíŒº1 ‚â§Œº0 ‚àíŒº1 + z1‚àíŒ±/2
!œÉN
‚àö
N
 Œº = Œº1

= 1 ‚àíPr
	‚àö
N Œº0 ‚àíŒº1
!œÉN
‚àíz1‚àíŒ±/2 ‚â§
‚àö
N
¬ØXN ‚àíŒº1
!œÉN
‚â§
‚àö
N Œº0 ‚àíŒº1
!œÉN
+ z1‚àíŒ±/2
 Œº = Œº1

‚àº= 1 ‚àíŒ¶
	‚àö
N Œº0 ‚àíŒº1
!œÉN
+ z1‚àíŒ±/2

+ Œ¶
	‚àö
N Œº0 ‚àíŒº1
!œÉN
‚àíz1‚àíŒ±/2

.
Note that the power function converges to one as the sample size increases provided that
Œº Ã∏= Œº1 because both cumulative distribution functions converge to the same value (namely,
¬±1 depending on whether Œº0 ‚â∑Œº1).
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
139 
It is straightforward to deal with one-sided tests as well.
For instance, for a means
test of H0 : Œº = Œº0 against H1 : Œº > Œº0, the test statistic is
¬Ø
XN‚àíŒº0
œÉN/
‚àö
N with an asymptotic
critical value given by the (1 ‚àíŒ±)th percentile of the standard normal distribution given
that Pr
 ¬Ø
XN‚àíŒº0
œÉN/
‚àö
N > z1‚àíŒ±

‚àº= Œ± under the null hypothesis. Letting Œº1 > Œº0 denote a mean
value under the alternative yields a power of
Pr
	‚àö
N
¬ØXN ‚àíŒº0
!œÉN
> z1‚àíŒ±
 Œº = Œº1

= 1 ‚àíPr
	
¬ØXN ‚â§Œº0 + z1‚àíŒ±
!œÉN
‚àö
N
 Œº = Œº1

= 1 ‚àíPr
	
¬ØXN ‚àíŒº1 ‚â§Œº0 ‚àíŒº1 + z1‚àíŒ±
!œÉN
‚àö
N
 Œº = Œº1

= 1 ‚àíPr
	 ¬ØXN ‚àíŒº1
!œÉN/
‚àö
N
‚â§Œº0 ‚àíŒº1
!œÉN/
‚àö
N
+ z1‚àíŒ±
 Œº = Œº1

‚àº= 1 ‚àíŒ¶
	‚àö
N Œº0 ‚àíŒº1
!œÉN
+ z1‚àíŒ±

.
As before, power converges to one as the sample size increases. This property is known
as consistency. We say a test is consistent if it has asymptotic unit power for any Ô¨Åxed
alternative.
In the previous chapter, we have seen that it is typically very diÔ¨Écult to obtain eÔ¨Écient
estimators if we do not restrict attention to a speciÔ¨Åc class (e.g., class of unbiased estimators).
The same problem arises if we wish to derive a uniformly most powerful test at a certain
signiÔ¨Åcance level. Unless we conÔ¨Åne attention to simple null and alternative hypotheses, it
is not possible to derive optimal tests without imposing further restrictions. To appreciate
why, it suÔ¨Éces to imagine a situation in which we wish to test H0 : Œ∏ = Œ∏0 against H1 : Œ∏ Ã∏= Œ∏0.
It is easy to see that the one-sided test for H0 : Œ∏ = Œ∏0 against H0 : Œ∏ > Œ∏0 is more powerful
than the two-sided test if Œ∏ = Œ∏1 > Œ∏0, just as the one-sided test for H0 : Œ∏ = Œ∏0 against
H0 : Œ∏ < Œ∏0 is more powerful than the two-sided test if Œ∏ = Œ∏1 < Œ∏0.
Figure 7.2 illustrates this fact by plotting the power functions of one-sided tests for
H0 : Œ∏ = Œ∏0 against either H1 : Œ∏ < Œ∏0 or H1 : Œ∏ > Œ∏0 at the Œ± and Œ±/2 level of signiÔ¨Åcance.
The power of the one-sided tests are inferior to their levels of signiÔ¨Åcance for values of Œ∏ that
strongly contradict the alternative hypothesis (e.g., large positive values for H1 : Œ∏ < Œ∏0).
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
140 
0
Œ±/2
Œ±
power
Œ∏0
Œ∏
H1 : Œ∏ < Œ∏0, level Œ±/2
H1 : Œ∏ < Œ∏0, level Œ±
H1 : Œ∏ > Œ∏0, level Œ±/2
H1 : Œ∏ > Œ∏0, level Œ±
Figure 7.2: Power functions of one-sided tests for H0 : Œ∏ = Œ∏0
This is natural, though not acceptable for a test of H0 : Œ∏ = Œ∏0, because these tests are not
designed to look at deviations from the null in both directions. That‚Äôs exactly why we prefer
to restrict attention to unbiased tests, that is to say, tests whose power are always above
size. Applying such a criterion to the above situation clariÔ¨Åes why most people would prefer
the two-sided test instead of one of the one-sided tests. To obtain the power function of a
two-sided test of H0 : Œ∏ = Œ∏0, it suÔ¨Éces to sum up the power function of the one-sided tests
at Œ±/2 signiÔ¨Åcance level against H1 : Œ∏ > Œ∏0 and H1 : Œ∏ < Œ∏0.
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
141 
7.3
Interpreting p-values
The Neyman-Pearson paradigm leads to a dichotomy in the context of hypothesis testing
in that we can either reject or not the null hypothesis given a certain signiÔ¨Åcance level.
We would expect however that there are rejections and rejections. How far a test statistic
extends into the rejection region should intuitively convey some information about the weight
of the sample evidence against the null hypothesis. To measure how much evidence we have
against the null, we employ the concept of p-value, which refers to the probability under the
null that the value of the test statistic is at least as extreme as the one we actually observe
in the sample. Smaller p-values correspond to more conclusive sample evidence given that
we impose the null. In other words, the p-value is the smallest signiÔ¨Åcance level at which we
would reject the null hypothesis given the observed value of the test statistic.
Computing p-values is like taking the opposite route we take to derive a rejection region.
To obtain the latter, we Ô¨Åx the level of signiÔ¨Åcance Œ± in the computation of the critical values.
To Ô¨Ånd a p-value of an one-sided test, we compute the tail probability of the test statistic by
evaluating the corresponding distribution at the sample statistic. As for two-sided tests, we
must just multiply the one-sided p-value by two if the sampling distribution is symmetric.
The main diÔ¨Äerence between the level of signiÔ¨Åcance and the p-value is that the latter is a
function of the sample, whereas we the former is a Ô¨Åxed probability that we choose ex-ante.
For instance, the p-value of an asymptotic means test is
Pr
	‚àö
N
¬ØXN ‚àíŒº0
!œÉN
>
‚àö
N ¬ØxN ‚àíŒº0
!œÉN

= 1 ‚àíŒ¶
	‚àö
N ¬ØxN ‚àíŒº0
!œÉN

if the alternative hypothesis is H1 : Œº > Œº0, whereas it is
Pr
	‚àö
N
¬ØXN ‚àíŒº0
!œÉN
<
‚àö
N ¬ØxN ‚àíŒº0
!œÉN

= Œ¶
	‚àö
N ¬ØxN ‚àíŒº0
!œÉN

for H1 : Œº < Œº0. As for two-sided tests, the p-value reads
2 Pr
	‚àö
N
¬ØXN ‚àíŒº0
!œÉN
>
‚àö
N

¬ØxN ‚àíŒº0
!œÉN


= 2

1 ‚àíŒ¶
	‚àö
N

¬ØxN ‚àíŒº0
!œÉN



Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
142 
for H1 : Œº Ã∏= Œº0. To better understand how we compute p-values, let‚Äôs revisit the barista
example one more time.
Example:
Under the null distribution that it takes on average 28 seconds for pouring
a perfect expresso, the asymptotic normal approximation for the distribution of the sample
mean implies the following p-value for a sample mean of 26 seconds:
2 Pr
	‚àö
16
¬ØX16 ‚àí28
6
>
‚àö
16

¬Øx16 ‚àí28
6

 H0 : Œº = 28

‚àº= 2

1 ‚àíŒ¶
	
4

26 ‚àí28
6



= 2[1 ‚àíŒ¶(4/3)] = 2(1 ‚àí0.90878878)
= 0.18242244.
This means that we cannot reject the null hypothesis at the usual levels of signiÔ¨Åcance (i.e.,
1%, 5% and 10%). We must be ready to consider a level of signiÔ¨Åcance of about 18.25% if
we really wish to reject the null.
Before concluding this section, it is useful to talk about what p-value is not about. First,
it is not about the probability that the null hypothesis is true. We could never produce
such a probability. We compute the p-value under the null and hence it cannot say anything
about how likely the null hypothesis is. In addition, it does not make any sense to compute
the probability of a hypothesis given that the latter is not a random variable. Second, a large
p-value does not necessarily imply that the null is true. It just means that we don‚Äôt have
enough evidence to reject it. Third, the p-value does not say anything about the magnitude
of the deviation with respect to the null hypothesis. To sum up, the p-value entails the
conÔ¨Ådence that we may have in the null hypothesis to explain the result we actually observe
in the sample.
7.4
Likelihood-based tests
The discussion in Section suggests that it is very often the case there is no uniformly most
powerful test for a given set of null and alternative hypotheses. It turns nonetheless out that
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
143 
likelihood-based tests typically yield very powerful tests in a wide array of situations. In
particular, if it exists, a uniformly most powerful (unbiased) test is very often equivalent to a
likelihood-based test. This means that likelihood methods entail not only eÔ¨Écient estimators,
but also a framework to build satisfactory tests.
Let Œ∏ ‚ààŒò ‚äÇRk denote a k-dimensional parameter vector of which the likelihood L(Œ∏; X)
is a function. Consider the problem of testing the composite null hypothesis H0 : Œ∏ ‚ààŒò0
against the composite alternative hypothesis H0 : Œ∏ ‚ààŒò ‚àíŒò0. We now deÔ¨Åne the likelihood
ratio as
Œª(X) ‚â°maxŒ∏‚ààŒò0 L(Œ∏; X)
maxŒ∏‚ààŒò L(Œ∏; X) = L(!Œ∏
(0)
N ; X)
L(!Œ∏N; X)
,
where !Œ∏
(0)
N and !Œ∏N are the restricted and unrestricted maximum likelihood estimators, re-
spectively. The restricted optimization means that we search for the parameter vector that
maximizes the log-likelihood function only within the null parameter space Œò0, whereas the
unrestricted optimization yields the usual ML estimator of Œ∏.
Hypothesis testing
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics for Business and Economics
 
144 
The intuition for a likelihood-ratio test is very simple. In the event that the null hypoth-
esis is true, the unrestricted optimization will (in the limit as N ‚Üí‚àû) yield a value for
the parameter vector within Œò0 and hence the log-likelihood ratio will take a unit value. If
the null is false, then the unrestricted optimization will yield a value for Œ∏ ‚ààŒò ‚àíŒò0 and
hence the ratio will take a value below one. This suggests a rejection region of the form
{X : Œª(X) ‚â§CŒ±} for some constant 0 ‚â§CŒ± ‚â§1 that depends on the signiÔ¨Åcance level Œ±.
Example:
Let X denote a random sample from a normal distribution with mean Œº and
variance œÉ2.
Suppose that the interest lies on testing the null hypothesis H0 : Œº = Œº0
against the alternative H1 : Œº Ã∏= Œº0 by means of likelihood methods.
As the (unre-
stricted) likelihood function is (2œÄœÉ2)‚àíN/2 exp
"
1
2œÉ2
N
i=1(Xi ‚àíŒº)2#
, the (unrestricted) max-
imum likelihood estimators for Œº and œÉ2 are the sample mean ¬ØXN and sample variance !œÉ2
N.
In contrast, conÔ¨Åning attention to the null hypothesis yields a restricted likelihood func-
tion of (2œÄœÉ2)‚àíN/2 exp
"
1
2œÉ2
N
i=1(Xi ‚àíŒº0)2#
with restricted ML estimators given by Œº0 and
ÀúœÉ2
N = 1
N
N
i=1(Xi ‚àíŒº0)2. It then follows that the likelihood ratio is
Œª(X) =
(2œÄÀúœÉ2
N)‚àíN/2 exp
"
1
2ÀúœÉ2
N
N
i=1(Xi ‚àíŒº0)2#
(2œÄ!œÉ2
N)‚àíN/2 exp
"
1
2œÉ2
N
N
i=1(Xi ‚àí¬ØXN)2
#
=
"
1
N
N
i=1(Xi ‚àíŒº0)2#‚àíN/2
exp
"
N
N
i=1(Xi‚àíŒº0)2
2
N
i=1(Xi‚àíŒº0)2
#
"
1
N
N
i=1(Xi ‚àí¬ØXN)2
#‚àíN/2
exp
"
N
N
i=1(Xi‚àí¬Ø
XN)2
2
N
i=1(Xi‚àí¬Ø
XN)2
#
=
"N
i=1(Xi ‚àíŒº0)2#‚àíN/2
exp(N/2)
"N
i=1(Xi ‚àí¬ØXN)2
#‚àíN/2
exp(N/2)
=
 N
i=1(Xi ‚àíŒº0)2
N
i=1(Xi ‚àí¬ØXN)2
‚àíN/2
.
To compute the critical value kŒ± of the rejection region, we must Ô¨Årst derive the distribution
of Œª(X) under the null distribution. This may look like a daunting task, but it is actually
straightforward for we can write the numerator of the fraction as
N

i=1
(Xi ‚àíŒº0)2 =
N

i=1
(Xi ‚àí¬ØXN + ¬ØXN ‚àíŒº0)2 =
N

i=1
(Xi ‚àí¬ØXN)2 ‚àíN( ¬ØXN ‚àíŒº0)2,
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
145 
which implies that
Œª(X) =

1 +
N( ¬ØXN ‚àíŒº0)2
N
i=1(Xi ‚àí¬ØXN)2
‚àíN/2
.
Well, now it suÔ¨Éces to appreciate that the likelihood ratio is a monotone decreasing function
of
‚àö
N( ¬ØXN ‚àíŒº0)/sN given that the fraction within brackets is the square of the latter divided
by N ‚àí1. It then follows from
‚àö
N( ¬ØXN ‚àíŒº0)/sN ‚àºtN‚àí1 that a rejection region of the form
{X :

‚àö
N( ¬ØXN ‚àíŒº0)/sN
 ‚â•tN‚àí1(1‚àíŒ±/2)}, where tN‚àí1(1‚àíŒ±/2) is the (1‚àíŒ±/2)th percentile
of a t-student distribution with N ‚àí1 degrees of freedom, yields a test with a signiÔ¨Åcance
level of Œ±.
The above example shows that it is possible to compute the rejection rate of a likelihood
ratio test by looking at whether it depends exclusively on a statistic with a known sampling
distribution. In general, however, it is very diÔ¨Écult to derive the exact sampling distribution
of the likelihood ratio and so we must employ asymptotic approximations.
Assume, for
instance, that X = (X1, . . . , XN) is a random sample from a distribution FŒ∏ and that we
wish to test H0 : Œ∏ = Œ∏0 against H0 : Œ∏ Ã∏= Œ∏0. The fact that the unrestricted ML estimator is
consistent under both the null and alternative hypotheses ensures that ln L(Œ∏0; X) admits a
Taylor expansion around !Œ∏N:
ln L(Œ∏0; X) = ln L(!Œ∏N; X) + ‚àÇ
‚àÇŒ∏ ln L(!Œ∏N; X)(Œ∏0 ‚àí!Œ∏N) + 1
2
‚àÇ2
‚àÇŒ∏2 ln L(!Œ∏N; X)(Œ∏0 ‚àí!Œ∏N)2
+ 1
6
‚àÇ3
‚àÇŒ∏3 ln L(Œ∏‚àó; X)(Œ∏0 ‚àí!Œ∏N)3,
where Œ∏‚àó= ŒªŒ∏0 + (1 ‚àíŒª)!Œ∏N for 0 ‚â§Œª ‚â§1. The deÔ¨Ånition of the ML estimator is such
that the Ô¨Årst derivative of the log-likelihood function is zero, whereas the fact that !Œ∏N is a
‚àö
N‚àíconsistent estimator ensures that the last term of the expansion converges to zero at
a very fast rate. It then follows that
‚àí2 ln Œª(X) = 2

ln L(!Œ∏N; X) ‚àíln L(Œ∏0; X

‚àº= ‚àí‚àÇ2
‚àÇŒ∏2L(!Œ∏N; X)(!Œ∏N ‚àíŒ∏0)2.
(7.1)
Now, we know that under the null
‚àö
N(!Œ∏N ‚àíŒ∏0) weakly converges to a normal distribution
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
146 
with mean zero and variance given by the inverse of the information matrix
I‚àû(Œ∏0) ‚â°‚àílim
N‚Üí‚àû
1
N
‚àÇ2
‚àÇŒ∏2L(!Œ∏N; X).
This means that LR = ‚àí2 ln Œª(X) is asymptotically chi-square with one degree of freedom
for the right-hand side of (7.1) is the square of a standard normal variate. This suggests
that a test that rejects the null hypothesis if the likelihood ratio LR ‚â•œá2
1(1 ‚àíŒ±), where
the latter denotes the (1 ‚àíŒ±)th percentile of the chi-square distribution with one degree of
freedom, is asymptotically of level Œ±.
Example:
Let Xi ‚àºiid Poisson(Œª) for i = 1, . . . , N and deÔ¨Åne the null and alternative
hypotheses as H0 : Œª = Œª0 and H1 : Œª Ã∏= Œª0, respectively. The likelihood ratio then is
LR = ‚àí2 ln Œª(X) = ‚àí2 ln exp(‚àíNŒª0) Œª
N
i=1 Xi
0
exp(‚àíN!ŒªN) !Œª
N
i=1 Xi
N
= ‚àí2N
"
(Œª0 ‚àí!ŒªN) ‚àí!ŒªN ln(Œª0/!ŒªN)
#
d
‚àí‚Üíœá2
1,
where !ŒªN = 1
N
N
i=1 Xi is the ML estimator of the Poisson arrival rate.
Hypothesis testing
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Statistics for Business and Economics
 
147 
We next extend this result to a more general setting as well as derive two additional
likelihood-based tests that are asymptotically equivalent to the likelihood ratio test. We
start by establishing some notation. Let Œò0 = {Œ∏ : R(Œ∏) = 0, Œ∏ ‚ààŒò}, where R(Œ∏) = 0
represents a system of r nonlinear equations concerning Œ∏. For instance, we could think of
testing whether Œ∏1 + Œ∏2 = 1 and Œ∏3 = . . . = Œ∏k = 0, giving way to a system of r = k ‚àí1
restrictions of the form R(Œ∏) = (Œ∏1 + Œ∏2 ‚àí1, Œ∏3, . . . , Œ∏k)‚Ä≤ = 0. Recall that the unrestricted
maximum likelihood estimator !Œ∏N is such that
‚àö
N(!Œ∏N ‚àíŒ∏)
d
‚àí‚ÜíN(0, I‚àí1
‚àû(Œ∏)) and that
the score function is such that
1
‚àö
N
‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; X)
d
‚àí‚ÜíN(0, I‚àû(Œ∏)), where I‚àû(Œ∏) is the
information matrix. In contrast, the restricted maximum likelihood estimator ÀúŒ∏N maximizes
the log-likelihood function subject to R(Œ∏) = 0 (and so it does not equate the score function
to zero for it has to account for the Lagrange multiplier term).
Along the same lines as before, the likelihood ratio is
LR = ‚àí2 ln Œª(X) = 2

ln L(!Œ∏N; X) ‚àíln L(ÀúŒ∏N; X

‚àº= (!Œ∏N ‚àíÀúŒ∏N)‚Ä≤

‚àí
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤L(!Œ∏N; X)

(!Œ∏N ‚àíÀúŒ∏N)
(7.2)
given that, under the null, a Taylor expansion is admissible for both estimators are consistent
and hence close to each other. Now, it is possible to show that, under the null, the asymptotic
variance of
‚àö
N(!Œ∏N ‚àíÀúŒ∏N) is
lim
N‚Üí‚àû

‚àí1
N
‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤L(!Œ∏N; X)
‚àí1
.
This implies that the right-hand side of (7.2) converges in distribution to a chi-square with
r degrees of freedom. To appreciate why, it suÔ¨Éces to observe that !Œ∏N and ÀúŒ∏N respectively
estimate k and k ‚àír free parameters, so that their diÔ¨Äerence concerns only r elements.
Figure 7.3 shows that the likelihood ratio test gauges the diÔ¨Äerence between the criterion
function that we maximize either with or without constraints. It also illustrate two alter-
native routes to assess whether the data is consistent with the constraints in the parameter
space. The Ô¨Årst is to measure the diÔ¨Äerence between the restricted and unrestricted ML
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
148 
estimators or, equivalently, to evaluate whether the unrestricted ML estimator satisÔ¨Åes the
restriction in the null hypothesis. This testing strategy gives way to what we call Wald tests.
The second route is to evaluate whether the score function of the constrained ML estimator
is close to zero. The motivation lies on the fact that, in the limit, it is completely costless to
impose a true null. This translates into a Lagrange multiplier in the vicinity of zero, so that
the Ô¨Årst-order condition reduces to equating the score function to zero. Lagrange multiplier
tests rely then on measuring how diÔ¨Äerent from zero is the score function evaluated at the
constrained ML estimator.
ln L(Œ∏, X)
ln L(!Œ∏N, X)
ln L(ÀúŒ∏N, X)
ÀúŒ∏N
!Œ∏N
Œ∏
Figure 7.3: Likelihood-based tests based on unrestricted and restricted ML estimators (!Œ∏N
and ÀúŒ∏N, respectively).
The log-likelihood test measures the diÔ¨Äerence between the con-
strained and unconstrained log-likelihood functions, whereas the Wald test gauges the dif-
ference between the unrestricted and restricted ML estimators. The Lagrange multiplier
test assesses the magnitude of the constrained score function by focusing on the slope of
the green line. The zero slope of the red line reÔ¨Çects the fact that the unconstrained score
function is equal to zero by deÔ¨Ånition.
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
149 
We Ô¨Årst show how to compute Wald tests and then discuss Lagrange multiplier tests. As
usual, we will derive the necessary asymptotic theory by means of Taylor expansions. Wald
tests are about whether the unconstrained ML estimator meets the restrictions in the null
hypothesis and so we start with a Taylor expansion of R(Œ∏) around !Œ∏N, namely,
R(Œ∏) ‚àº= R(!Œ∏N) + RŒ∏(Œ∏ ‚àí!Œ∏N)
with RŒ∏ = ‚àÇ
‚àÇŒ∏‚Ä≤R(Œ∏).
It is now evident that
‚àö
N[R(!Œ∏N) ‚àíR(Œ∏)] will converge to a multivariate normal distri-
bution with mean zero and covariance matrix given by RŒ∏ I‚àí1
‚àû(Œ∏) R‚Ä≤
Œ∏.2 Well, if the null is
true, we expect that the (unrestricted) ML estimator will approximately satisfy the system
of nonlinear restrictions in that R(!Œ∏N) ‚àº= 0.
This suggests gauging whether the mag-
nitude of R(!Œ∏N) deviates from zero signiÔ¨Åcantly as a way of testing H0 against H1.
In
particular, we know that
‚àö
NR(!Œ∏N)
d
‚àí‚ÜíN(0, RŒ∏ I‚àí1
‚àû(Œ∏) R‚Ä≤
Œ∏) under the null and hence
it suÔ¨Éces to take a quadratic form of
‚àö
NR(!Œ∏N) normalized by its covariance matrix to
end up with an asymptotically chi-square distribution with r degrees of freedom, namely,
W ‚â°N R(!Œ∏N)‚Ä≤[RŒ∏ I‚àí1
‚àû(Œ∏) R‚Ä≤
Œ∏]‚àí1R(!Œ∏N)
d
‚àí‚Üíœár. Note that by taking a quadratic form we
automatically avoid negative and positive deviations from zero to cancel out. The asymp-
totic Wald test then rejects the null at the Œ± signiÔ¨Åcance level if W ‚â•œá2
r(1 ‚àíŒ±), where
the latter denotes the (1 ‚àíŒ±)th percentile of the chi-square distribution with r degrees of
freedom.
Example:
Let Xi ‚àºiid B(1, p) for i = 1, . . . , N. DeÔ¨Åne the null and alternative hypotheses
as H0 : p = p0 and H1 : p Ã∏= p0, respectively.
The unconstrained maximum likelihood
estimator of p is the sample mean !pN = N
i=1 Xi, whose variance is p(1 ‚àíp)/N. Applying a
central limit theorem then yields
W = N (!pN ‚àíp0)2
!pN(1 ‚àí!pN)
d
‚àí‚Üíœá2
1
suggesting us to reject the null at the Œ± signiÔ¨Åcance level if W ‚â•œá2
1(1 ‚àíŒ±).
2 See Footnote 1 in Section 6.1.5 for a very brief discussion about the multivariate normal distribution.
Hypothesis testing
Download free eBooks at bookboon.com

Statistics for Business and Economics
 
150 
We now turn our attention to the Lagrange multiplier test. The score function
‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; X)
is on average zero for any Œ∏ ‚ààŒò and hence it is zero also for any Œ∏ ‚ààŒò0. In addition, the
variance of the score function is under the null equal to
var
	 ‚àÇ
‚àÇŒ∏‚Ä≤ ln L(Œ∏; X)
 Œ∏ ‚ààŒò0

= ‚àíE

‚àÇ
‚àÇŒ∏‚àÇŒ∏‚Ä≤ ln L(Œ∏; X)
 Œ∏ ‚ààŒò0

‚â°IN(Œ∏),
which in the limit coincides with the information matrix I‚àû(Œ∏). It thus follows that
LM =
‚àÇ
‚àÇŒ∏‚Ä≤ ln L(ÀúŒ∏N; X)‚Ä≤I‚àí1
N (ÀúŒ∏N) ‚àÇ
‚àÇŒ∏‚Ä≤ ln L(ÀúŒ∏N; X)
d
‚àí‚Üíœá2
r
and hence we must reject the null hypothesis if LM ‚â•œá2
r(1‚àíŒ±) to obtain an asymptotic test
of level Œ±. Note that the chi-square distribution has r degrees of freedom even though ÀúŒ∏N
has k ‚àír free parameters. This is because the score of the k ‚àír free parameters must equate
to zero, remaining only r dimensions for the score function to vary (i.e., those aÔ¨Äected by
the restrictions).
Example:
Let‚Äôs revisit the previous example in which X = (X1, . . . , XN) with Xi ‚àº
iid B(1, p) for i = 1, . . . , N. The LM test statistic for H0 : p = p0 against H1 : p Ã∏= p0 then is
LM = N (!pN ‚àíp0)2
p0(1 ‚àíp0)
d
‚àí‚Üíœá2
1
given that the score function evaluated at p0 is (!pN ‚àíp0)/[p0(1‚àíp0)/N] and the corresponding
information matrix is N/[p0/(1 ‚àíp0)]. We would thus reject the null if LM ‚â•œá2
1(1 ‚àíŒ±) to
obtain an asymptotic test at the Œ± level of signiÔ¨Åcance.
In the above example, it is evident that the Wald and LM tests are asymptotically
equivalent for the diÔ¨Äerence between their denominators shrink to zero under the null as the
sample mean !pN converges almost surely to p0. This asymptotic equivalence actually holds
in general, linking not only Wald and LM tests but also likelihood ratio tests. This should
come with no surprise given that the three statistics intuitively carry the same information
as it is easily seen in Figure 7.3.
Hypothesis testing
Download free eBooks at bookboon.com

